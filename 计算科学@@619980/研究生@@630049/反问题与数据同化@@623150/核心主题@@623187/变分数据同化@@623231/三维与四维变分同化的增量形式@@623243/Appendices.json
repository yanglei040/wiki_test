{"hands_on_practices": [{"introduction": "增量四维变分同化（4D-Var）的核心在于迭代求解一系列二次代价函数，这个过程发生在其“内循环”中。本实践旨在比较两种主流算法——预条件共轭梯度法（PCG）和有限内存Broyden–Fletcher–Goldfarb–Shanno（L-BFGS）算法——在解决这一核心问题时的表现。通过这项练习，您将深入理解这些基础求解器的实现细节，并探索一个关键的实际问题：在跨越“外循环”时，代价函数曲率的变化如何影响L-BFGS算法的效率，以及是否需要重置其内存以保持收敛性 [@problem_id:3390407]。", "problem": "考虑在四维变分同化（Four-Dimensional Variational Assimilation (4D-Var)）的内循环中出现的增量二次子问题，该问题在外线性化循环中被重复求解。内问题是最小化严格凸二次泛函\n$$\nJ(\\delta x) = \\tfrac{1}{2}\\,\\delta x^{\\top} \\mathbf{H}\\,\\delta x - \\mathbf{g}^{\\top}\\,\\delta x,\n$$\n其中 $\\mathbf{H}\\in\\mathbb{R}^{n\\times n}$ 是对称正定矩阵，$\\mathbf{g}\\in\\mathbb{R}^{n}$ 是一个给定向量。在增量公式中，$\\mathbf{H}$ 可以解释为由背景项和观测项组装的 Hessian 矩阵的高斯-牛顿近似，例如\n$$\n\\mathbf{H} = \\mathbf{B}^{-1} + \\sum_{t} \\mathbf{M}_{t}^{\\top} \\mathbf{R}_{t}^{-1} \\mathbf{M}_{t},\n$$\n其中 $\\mathbf{B}\\in\\mathbb{R}^{n\\times n}$ 是对称正定的背景协方差，$\\mathbf{R}_{t}\\in\\mathbb{R}^{m_t\\times m_t}$ 是时间 $t$ 的对称正定观测误差协方差，而 $\\mathbf{M}_{t}\\in\\mathbb{R}^{m_t\\times n}$ 是时间 $t$ 的线性化模型-观测算子。外循环的更新会改变线性化，因此 $\\mathbf{H}$ 在不同的外迭代之间可能会变化。\n\n最小化子 $\\delta x^{\\ast}$ 由一阶最优性条件所刻画\n$$\n\\nabla J(\\delta x) = \\mathbf{H}\\,\\delta x - \\mathbf{g} = \\mathbf{0},\n$$\n因此 $\\delta x^{\\ast}$ 求解线性系统\n$$\n\\mathbf{H}\\,\\delta x = \\mathbf{g}.\n$$\n通常采用两类算法：\n- 预条件共轭梯度（PCG），它利用对称正定系统的 Krylov 子空间结构，并使用对称正定预条件子 $\\mathbf{M}\\approx \\mathbf{H}$。\n- 有限内存 Broyden–Fletcher–Goldfarb–Shanno (L-BFGS)，它维护少量过去的准牛顿曲率对来近似 $\\mathbf{H}^{-1}$，并沿着准牛顿方向执行线搜索。\n\n在增量变分同化中，实践者有时会将 L-BFGS 内存跨外循环携带，以加速收敛。然而，如果 $\\mathbf{H}$ 的曲率在不同外循环之间发生变化，存储的曲率对可能与新的 $\\mathbf{H}$ 不一致，从而降低效率。\n\n从上述定义和对称正定矩阵的性质出发，实现一个程序，该程序：\n1. 使用 PCG 求解内问题，预条件子为等于 $\\mathbf{H}$ 对角线的对角矩阵。当相对残差 $\\|\\mathbf{g} - \\mathbf{H}\\,\\delta x\\|_{2}/\\|\\mathbf{g}\\|_{2}$ 小于 $10^{-8}$ 或达到最大 500 次迭代时终止。\n2. 使用 L-BFGS 求解内问题，内存大小为 $m$，并沿着准牛顿搜索方向采用精确步长。对于二次目标函数，沿搜索方向 $\\mathbf{p}$ 的精确步长为\n$$\n\\alpha = -\\frac{\\nabla J(\\delta x)^{\\top}\\mathbf{p}}{\\mathbf{p}^{\\top}\\mathbf{H}\\,\\mathbf{p}}.\n$$\n当满足相同的相对残差标准或达到 500 次迭代后终止。使用标准的双循环递归，初始逆 Hessian 矩阵缩放设置为\n$$\n\\gamma = \\frac{\\mathbf{s}_{k-1}^{\\top}\\mathbf{y}_{k-1}}{\\mathbf{y}_{k-1}^{\\top}\\mathbf{y}_{k-1}}\n$$\n当内存可用时，否则 $\\gamma=1$。在接受曲率对时，强制执行曲率条件 $\\mathbf{s}^{\\top}\\mathbf{y} > 10^{-12}$。\n3. 模拟两个外循环。在第一个外循环中，从 $\\delta x_{0}=\\mathbf{0}$ 开始求解内问题直至收敛。在第二个外循环中，根据测试用例的定义更改 $\\mathbf{H}$，从前一个解作为初始猜测开始，再次求解。每个测试用例执行三次第二循环运行：\n   - PCG 基准。\n   - L-BFGS，不重置地从第一个循环继承内存。\n   - L-BFGS，采用重置策略，即如果在第二个循环开始时\n     $$\n     \\Delta = \\frac{\\|\\mathbf{H}_{\\text{new}} - \\mathbf{H}_{\\text{old}}\\|_{F}}{\\|\\mathbf{H}_{\\text{old}}\\|_{F}} > \\tau,\n     $$\n     则丢弃内存，其中 $\\|\\cdot\\|_{F}$ 是 Frobenius 范数，$\\tau$ 是一个阈值参数。否则，保留内存。使用 $\\tau=0.2$。\n\n您的实现必须是完全确定性和自洽的。角度（如果存在）必须以弧度为单位。\n\n测试套件。使用以下测试用例，维度 $n=30$，内存大小 $m=10$，容差 $10^{-8}$，最大迭代次数 $500$。对于所有情况，使用 $\\mathbf{g}=\\mathbf{1}\\in\\mathbb{R}^{30}$。\n- 情况 1（理想情况，无曲率变化）：$\\mathbf{H}_{1}=\\mathrm{diag}(1,2,\\dots,30)$，$\\mathbf{H}_{2}=\\mathbf{H}_{1}$。\n- 情况 2（带各向异性的特征向量旋转）：令 $\\mathbf{D}=\\mathrm{diag}(\\lambda_{i})$，其中 $\\lambda_{i}$ 在 $10^{-2}$ 和 $10^{2}$ 之间对数均匀分布于 30 个点。定义一个正交矩阵 $\\mathbf{Q}$ 为在坐标平面 $(1,2)$, $(3,4)$, $(5,6)$, $(7,8)$, $(9,10)$, $(11,12)$, $(13,14)$, $(15,16)$, $(17,18)$, $(19,20)$, $(21,22)$, $(23,24)$, $(25,26)$, $(27,28)$, $(29,30)$（此描述中索引从 1 开始）中进行角度为 $\\pi/3$ 的平面旋转的乘积。设置 $\\mathbf{H}_{1}=\\mathbf{D}$ 和 $\\mathbf{H}_{2}=\\mathbf{Q}^{\\top}\\mathbf{D}\\,\\mathbf{Q}$。\n- 情况 3（无旋转的特征值缩放）：$\\mathbf{H}_{1}=\\mathrm{diag}(\\lambda_{i})$，其中 $\\lambda_{i}$ 在 $10^{0}$ 和 $10^{3}$ 之间对数均匀分布于 30 个点，以及 $\\mathbf{H}_{2}=\\mathrm{diag}(\\mu_{i})$，其中 $\\mu_{i}$ 在 $10^{-4}$ 和 $10^{4}$ 之间对数均匀分布于 30 个点。\n\n对于每个测试用例，报告在第二个外循环中为满足终止标准所进行的迭代次数，分别针对：\n- PCG 基准，\n- 带继承内存的 L-BFGS，\n- 带重置策略的 L-BFGS。\n\n最终输出格式。您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果，按测试用例和方法排序，即\n$$\n[\\text{pcg}_{1},\\text{lbfgs\\_carry}_{1},\\text{lbfgs\\_reset}_{1},\\text{pcg}_{2},\\text{lbfgs\\_carry}_{2},\\text{lbfgs\\_reset}_{2},\\text{pcg}_{3},\\text{lbfgs\\_carry}_{3},\\text{lbfgs\\_reset}_{3}],\n$$\n其中每个条目都是一个整数迭代次数。", "solution": "用户提供的问题是有效的。它在科学上基于数值线性代数和优化，特别是在变分数据同化的背景下。该问题是适定的，所有必要的参数、算法和测试用例都已明确定义。它是客观且自洽的。因此，将提供一个解决方案。\n\n该问题要求实现和比较两种标准的迭代方法，即预条件共轭梯度（PCG）和有限内存 Broyden–Fletcher–Goldfarb–Shanno (L-BFGS)，用于解决在增量 4D-Var 中出现的一系列二次优化问题。核心任务是求解线性系统 $\\mathbf{H}\\,\\delta x = \\mathbf{g}$ 以得到 $\\delta x$，其中 $\\mathbf{H}$ 是一个对称正定（SPD）矩阵。\n\n该模拟仿真了数据同化系统的两个“外循环”。在第一个循环中，求解一个初始问题，其矩阵为 $\\mathbf{H}_1$。在第二个循环中，问题发生变化（矩阵变为 $\\mathbf{H}_2$），我们从第一个循环的解开始重新求解。此设置旨在测试迭代方法对问题曲率变化的适应能力，曲率由 Hessian 矩阵 $\\mathbf{H}$ 表示。\n\n分析重点关注第二个外循环的三种策略：\n1.  **PCG 基准**：一个带有对角预条件子的标准 PCG 方法。PCG 为新问题从头开始构建其搜索空间（一个 Krylov 子空间），因此其性能仅取决于 $\\mathbf{H}_2$ 的性质和初始猜测。\n2.  **带内存继承的 L-BFGS**：L-BFGS 算法使用先前迭代的曲率信息（步长向量 $\\mathbf{s}$ 和梯度差向量 $\\mathbf{y}$ 的配对）来构建逆 Hessian 矩阵的近似。此策略将第一个循环（使用 $\\mathbf{H}_1$）的曲率对内存继承到第二个循环（使用 $\\mathbf{H}_2$）。如果 $\\mathbf{H}_2$ 与 $\\mathbf{H}_1$ 显著不同，这些旧的曲率信息（$\\mathbf{y} \\approx \\mathbf{H}_1 \\mathbf{s}$）可能与新问题（需要 $\\mathbf{y} \\approx \\mathbf{H}_2 \\mathbf{s}$）不一致，可能导致收敛性下降。\n3.  **带重置策略的 L-BFGS**：这是一种自适应策略。它使用 Frobenius 范数来衡量 $\\mathbf{H}_1$ 和 $\\mathbf{H}_2$ 之间的变化。如果相对变化超过阈值 $\\tau$，则丢弃（重置）L-BFGS 内存，以防止使用不一致的曲率。否则，内存被继承。\n\n实现包括三个主要部分：\n1.  一个用于 PCG 算法的函数。\n2.  一个用于 L-BFGS 算法的函数。\n3.  一个主驱动脚本，用于按规定设置三个测试用例，为每个用例运行双循环模拟，并记录第二个循环中三种策略的迭代次数。\n\n让我们详细说明每种算法和测试用例的实现。\n\n**PCG 实现**\nPCG 算法求解 $\\mathbf{H}\\delta x = \\mathbf{g}$，其中 $\\mathbf{H}$ 为 SPD 矩阵。\n- **预条件子**：按照规定，使用对角预条件子 $\\mathbf{M} = \\mathrm{diag}(\\mathbf{H})$。其逆矩阵就是 $\\mathbf{M}^{-1} = \\mathrm{diag}(1/H_{ii})$。\n- **初始化**：$\\delta x_0$ 是初始猜测，$\\mathbf{r}_0 = \\mathbf{g} - \\mathbf{H}\\delta x_0$，$\\mathbf{z}_0 = \\mathbf{M}^{-1}\\mathbf{r}_0$，$\\mathbf{p}_0 = \\mathbf{z}_0$。\n- **迭代**：使用标准的 PCG 递推关系来更新 $\\delta x_k, \\mathbf{r}_k, \\mathbf{z}_k, \\mathbf{p}_k$。\n- **终止**：当相对残差 $\\|\\mathbf{r}_k\\|_2 / \\|\\mathbf{g}\\|_2$ 低于容差 $10^{-8}$ 或达到 500 次迭代后，循环停止。\n\n**L-BFGS 实现**\nL-BFGS 算法最小化二次泛函 $J(\\delta x) = \\frac{1}{2}\\delta x^{\\top} \\mathbf{H} \\delta x - \\mathbf{g}^{\\top} \\delta x$。\n- **搜索方向**：搜索方向 $\\mathbf{p}_k$ 使用标准的双循环递归计算，该递归近似了逆 Hessian 矩阵与梯度的乘积，即 $\\mathbf{p}_k = -\\mathbf{H}_k^{-1} \\nabla J(\\delta x_k)$。\n- **内存**：存储固定数量的过去曲率对 $(\\mathbf{s}_i, \\mathbf{y}_i)$，其中 $\\mathbf{s}_i=\\delta x_{i+1}-\\delta x_i$ 且 $\\mathbf{y}_i=\\nabla J(\\delta x_{i+1}) - \\nabla J(\\delta x_i)$。此内存的大小为 $m=10$。仅当满足曲率条件 $\\mathbf{s}_i^\\top \\mathbf{y}_i > 10^{-12}$ 时，才会存储一对。\n- **初始 Hessian 缩放**：初始逆 Hessian 近似 $\\mathbf{H}_k^0$ 是一个缩放的单位矩阵，$\\mathbf{H}_k^0 = \\gamma_k \\mathbf{I}$，缩放因子为 $\\gamma_k = (\\mathbf{s}_{k-1}^\\top \\mathbf{y}_{k-1}) / (\\mathbf{y}_{k-1}^\\top \\mathbf{y}_{k-1})$，如果没有可用内存，则 $\\gamma_k=1$。\n- **线搜索**：对于二次目标函数，使用能最小化 $J(\\delta x_k + \\alpha_k \\mathbf{p}_k)$ 的精确步长 $\\alpha_k$：$\\alpha_k = -(\\nabla J(\\delta x_k)^\\top\\mathbf{p}_k)/(\\mathbf{p}_k^\\top\\mathbf{H}\\mathbf{p}_k)$。\n- **终止**：当相对梯度范数 $\\|\\nabla J(\\delta x_k)\\|_2 / \\|\\mathbf{g}\\|_2$ 低于 $10^{-8}$ 或达到 500 次迭代后，循环停止。\n\n**测试用例设置**\n所有情况均使用维度 $n=30$ 和右侧向量 $\\mathbf{g} = \\mathbf{1}$。\n- **情况 1**：$\\mathbf{H}_1$ 和 $\\mathbf{H}_2$ 是相同的对角矩阵，$\\mathbf{H}_1 = \\mathbf{H}_2 = \\mathrm{diag}(1, 2, ..., 30)$。预计没有曲率变化。\n- **情况 2**：$\\mathbf{H}_1 = \\mathbf{D}$，一个对角矩阵，其特征值在 $10^{-2}$ 到 $10^{2}$ 之间对数均匀分布。$\\mathbf{H}_2 = \\mathbf{Q}^\\top \\mathbf{D} \\mathbf{Q}$，其中 $\\mathbf{Q}$ 是一个表示旋转组合的正交矩阵。此情况测试在保持特征值不变的情况下旋转特征系统的效果。\n- **情况 3**：$\\mathbf{H}_1$ 和 $\\mathbf{H}_2$ 均为对角矩阵，但具有不同的特征值分布。$\\mathbf{H}_1$ 的特征值在 $10^0$ 到 $10^3$ 之间对数分布，而 $\\mathbf{H}_2$ 的特征值在 $10^{-4}$ 到 $10^4$ 之间对数分布。此情况测试显著特征值缩放的效果。\n\n最终程序遵循此逻辑，系统地执行每个测试用例，并收集第二个外循环的迭代次数，以生成所需的输出。", "answer": "```python\nimport numpy as np\nfrom collections import deque\n\ndef pcg(H: np.ndarray, g: np.ndarray, x0: np.ndarray, tol: float, max_iter: int) -> tuple[np.ndarray, int]:\n    \"\"\"\n    Solves the linear system Hx=g for a symmetric positive definite matrix H\n    using the Preconditioned Conjugate Gradient (PCG) method with a diagonal preconditioner.\n\n    Args:\n        H: The system matrix (n x n).\n        g: The right-hand side vector (n).\n        x0: The initial guess for the solution (n).\n        tol: The relative residual tolerance for convergence.\n        max_iter: The maximum number of iterations.\n\n    Returns:\n        A tuple containing the solution vector x and the number of iterations performed.\n    \"\"\"\n    x = np.copy(x0)\n    r = g - H @ x\n    \n    g_norm = np.linalg.norm(g)\n    if g_norm == 0:\n        return x, 0\n\n    if np.linalg.norm(r) / g_norm  tol:\n        return x, 0\n\n    diag_H = np.diag(H)\n    z = r / diag_H\n    p = z.copy()\n    rs_old = np.dot(r, z)\n\n    if rs_old == 0:\n        return x, 0\n\n    for i in range(max_iter):\n        Hp = H @ p\n        if np.dot(p, Hp) == 0: break\n        alpha = rs_old / np.dot(p, Hp)\n        \n        x += alpha * p\n        r -= alpha * Hp\n\n        if np.linalg.norm(r) / g_norm  tol:\n            return x, i + 1\n\n        z = r / diag_H\n        rs_new = np.dot(r, z)\n\n        if rs_old == 0: break\n        beta = rs_new / rs_old\n        p = z + beta * p\n        rs_old = rs_new\n        \n    return x, max_iter\n\ndef lbfgs(H: np.ndarray, g: np.ndarray, x0: np.ndarray, m: int, tol: float, max_iter: int, \n          initial_memory: tuple[deque, deque] | None = None) - tuple[np.ndarray, int, tuple[deque, deque]]:\n    \"\"\"\n    Minimizes the quadratic functional J(x) = 0.5*x'Hx - g'x using the L-BFGS algorithm.\n\n    Args:\n        H: The Hessian matrix (n x n).\n        g: The linear term vector (n).\n        x0: The initial guess for the minimizer (n).\n        m: The memory size for L-BFGS.\n        tol: The relative gradient norm tolerance for convergence.\n        max_iter: The maximum number of iterations.\n        initial_memory: An optional tuple of deques (s_hist, y_hist) to start with.\n\n    Returns:\n        A tuple containing the solution vector x, the number of iterations,\n        and the final L-BFGS memory (s_hist, y_hist).\n    \"\"\"\n    x = np.copy(x0)\n    \n    if initial_memory is None:\n        s_hist = deque(maxlen=m)\n        y_hist = deque(maxlen=m)\n    else:\n        s_hist, y_hist = initial_memory\n\n    g_norm = np.linalg.norm(g)\n    if g_norm == 0:\n        return x, 0, (s_hist, y_hist)\n        \n    grad = H @ x - g\n    if np.linalg.norm(grad) / g_norm  tol:\n        return x, 0, (s_hist, y_hist)\n\n    for i in range(max_iter):\n        q = grad.copy()\n        \n        alphas = []\n        rhos = []\n        for s_i, y_i in zip(reversed(s_hist), reversed(y_hist)):\n            rho_i = 1.0 / np.dot(s_i, y_i)\n            rhos.append(rho_i)\n            alpha_i = rho_i * np.dot(s_i, q)\n            alphas.append(alpha_i)\n            q -= alpha_i * y_i\n\n        if len(y_hist)  0:\n            s_k_minus_1 = s_hist[-1]\n            y_k_minus_1 = y_hist[-1]\n            ykyk = np.dot(y_k_minus_1, y_k_minus_1)\n            gamma = np.dot(s_k_minus_1, y_k_minus_1) / ykyk if ykyk != 0 else 1.0\n        else:\n            gamma = 1.0\n        \n        r = gamma * q\n        \n        for s_i, y_i, alpha_i, rho_i in zip(s_hist, y_hist, reversed(alphas), reversed(rhos)):\n            beta = rho_i * np.dot(y_i, r)\n            r += s_i * (alpha_i - beta)\n            \n        p = -r\n\n        grad_dot_p = np.dot(grad, p)\n        p_H_p = np.dot(p, H @ p)\n        \n        if p_H_p = 0: break\n        \n        alpha_step = -grad_dot_p / p_H_p\n\n        s_new = alpha_step * p\n        x_new = x + s_new\n        grad_new = H @ x_new - g\n        y_new = grad_new - grad\n\n        if np.dot(s_new, y_new)  1e-12:\n            s_hist.append(s_new)\n            y_hist.append(y_new)\n\n        x = x_new\n        grad = grad_new\n\n        if np.linalg.norm(grad) / g_norm  tol:\n            return x, i + 1, (s_hist, y_hist)\n            \n    return x, max_iter, (s_hist, y_hist)\n\n\ndef run_case(H1, H2, n, m_mem, tol, max_iter, tau):\n    \"\"\"\n    Runs the two-loop simulation for a given test case.\n    \"\"\"\n    g = np.ones(n)\n    x0 = np.zeros(n)\n\n    # Outer Loop 1\n    x_pcg1, _ = pcg(H1, g, x0, tol, max_iter)\n    x_lbfgs1, _, memory1 = lbfgs(H1, g, x0, m_mem, tol, max_iter)\n\n    # Outer Loop 2\n    _, iters_pcg2 = pcg(H2, g, x_pcg1, tol, max_iter)\n    \n    memory1_carry = (memory1[0].copy(), memory1[1].copy())\n    _, iters_lbfgs_carry, _ = lbfgs(H2, g, x_lbfgs1, m_mem, tol, max_iter, initial_memory=memory1_carry)\n\n    h1_norm_f = np.linalg.norm(H1, 'fro')\n    delta = np.linalg.norm(H2 - H1, 'fro') / h1_norm_f if h1_norm_f > 0 else np.linalg.norm(H2, 'fro')\n\n    mem_to_use = None\n    if delta = tau:\n        mem_to_use = (memory1[0].copy(), memory1[1].copy())\n    \n    _, iters_lbfgs_reset, _ = lbfgs(H2, g, x_lbfgs1, m_mem, tol, max_iter, initial_memory=mem_to_use)\n    \n    return [iters_pcg2, iters_lbfgs_carry, iters_lbfgs_reset]\n\ndef solve():\n    \"\"\"\n    Sets up and runs the test suite, then prints the results.\n    \"\"\"\n    n = 30\n    m_mem = 10\n    tol = 1e-8\n    max_iter = 500\n    tau = 0.2\n    \n    all_results = []\n    \n    # Case 1: Happy path, no curvature change\n    H1_c1 = np.diag(np.arange(1.0, n + 1.0))\n    H2_c1 = H1_c1\n    all_results.extend(run_case(H1_c1, H2_c1, n, m_mem, tol, max_iter, tau))\n\n    # Case 2: Eigenvector rotation with anisotropy\n    D = np.diag(np.logspace(-2, 2, n))\n    Q = np.eye(n)\n    theta = np.pi / 3\n    c, s = np.cos(theta), np.sin(theta)\n    for i in range(0, n, 2):\n        j = i + 1\n        Q[i, i] = c\n        Q[j, j] = c\n        Q[i, j] = -s\n        Q[j, i] = s\n    H1_c2 = D\n    H2_c2 = Q.T @ D @ Q\n    all_results.extend(run_case(H1_c2, H2_c2, n, m_mem, tol, max_iter, tau))\n\n    # Case 3: Eigenvalue rescaling without rotation\n    H1_c3 = np.diag(np.logspace(0, 3, n))\n    H2_c3 = np.diag(np.logspace(-4, 4, n))\n    all_results.extend(run_case(H1_c3, H2_c3, n, m_mem, tol, max_iter, tau))\n\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n\n```", "id": "3390407"}, {"introduction": "在实际的数据同化系统中，内循环求解器的收敛速度往往是决定计算效率的关键瓶颈。本练习将介绍预处理技术，这是一种通过改善线性系统的条件数来显著加速迭代求解器（如PCG）收敛的关键方法。您将设计并实现一个基于截断奇异值分解（SVD）的高效预条件子，它特别适用于背景误差协方差具有低秩结构的情形，例如在集合同化方法中。这项实践将向您展示如何利用问题的内在结构来大幅提升算法性能 [@problem_id:3390394]。", "problem": "考虑在线性化设置下的三维和四维变分同化的增量格式。背景场增量表示为 $\\delta x \\in \\mathbb{R}^{n}$，观测算子由线性映射 $H \\in \\mathbb{R}^{m \\times n}$ 表示，背景误差协方差表示为 $B \\in \\mathbb{R}^{n \\times n}$，观测误差协方差表示为 $R \\in \\mathbb{R}^{m \\times m}$。观测新息向量为 $d \\in \\mathbb{R}^{m}$。关于拉格朗日乘子变量 $\\lambda \\in \\mathbb{R}^{m}$ 的观测空间法方程为\n$$\n\\left(H B H^\\top + R\\right)\\lambda = d.\n$$\n在围绕背景场进行线性化的增量变分同化中，$B$ 和 $R$ 是对称正定的，$H$ 在其像空间上是满秩的，而 $d$ 由线性化新息给出。\n\n在观测空间中设计一个实验，其中矩阵 $H B H^\\top$ 是低秩的。这通过将 $B$ 构造为低秩矩阵来实现，即通过分解 $B = G G^\\top$（其中 $G \\in \\mathbb{R}^{n \\times r}$ 且 $r \\ll n$），使得 $H B H^\\top = \\left(H G\\right)\\left(H G\\right)^\\top$ 的秩最多为 $r$。使用对角观测误差协方差 $R = \\sigma^2 I_m$，其中 $\\sigma^2  0$。\n\n实现预处理共轭梯度法 (PCG) 来求解观测空间中的对称正定线性系统\n$$\n\\left(H B H^\\top + R\\right)\\lambda = d\n$$\n并应用一个由低秩因子 $\\left(H G\\right) \\in \\mathbb{R}^{m \\times r}$ 定义的截断奇异值分解 (SVD) 预处理器。令 $\\left(H G\\right) = U \\Sigma V^\\top$ 为其奇异值分解，其中 $U \\in \\mathbb{R}^{m \\times r}$ 的列是标准正交的，$\\Sigma \\in \\mathbb{R}^{r \\times r}$ 是包含非负奇异值的对角矩阵，$V \\in \\mathbb{R}^{r \\times r}$ 是正交矩阵。对称矩阵 $H B H^\\top$ 的特征分解则为 $U \\Lambda U^\\top$，其中 $\\Lambda = \\Sigma^2$。对于截断秩 $k$（其中 $0 \\leq k \\leq r$），将预处理器 $M^{-1}_k : \\mathbb{R}^{m} \\to \\mathbb{R}^{m}$ 的作用定义为\n$$\nM^{-1}_k z = \\frac{1}{\\sigma^2} z + U_k \\,\\mathrm{diag}\\left(\\left\\{\\frac{1}{\\sigma^2 + \\lambda_i} - \\frac{1}{\\sigma^2}\\right\\}_{i=1}^{k}\\right) U_k^\\top z,\n$$\n其中 $U_k \\in \\mathbb{R}^{m \\times k}$ 包含 $U$ 的前 $k$ 列，对应于 $H B H^\\top$ 的 $k$ 个最大特征值 $\\{\\lambda_i\\}_{i=1}^{k}$。\n\n从最小二乘变分同化代价函数、其线性化和法方程等基本原理出发，相应地设计算法解。使用无矩阵算子实现带有上述预处理器的 PCG 算法：\n- 对于任何 $v \\in \\mathbb{R}^{m}$，通过低秩分解将 $\\left(H B H^\\top\\right)v$ 计算为 $\\left(H G\\right)\\left(H G\\right)^\\top v$，并加上 $R v = \\sigma^2 v$。\n- 对于任何 $z \\in \\mathbb{R}^{m}$，根据截断 SVD 公式应用 $M^{-1}_k z$。\n\n使用以下固定的实验设置，该设置在科学上合理且在数值上是良态的：\n- 观测维数 $m = 150$。\n- 状态维数 $n = 300$。\n- 背景协方差的秩 $r = 20$，通过 $B = G G^\\top$ 实现，其中 $G \\in \\mathbb{R}^{n \\times r}$ 的元素为独立的标准正态分布。\n- 观测算子 $H \\in \\mathbb{R}^{m \\times n}$ 的元素为独立的标准正态分布，并按行进行缩放以使欧几里得范数为单位1，以避免因尺度问题导致的病态。\n- 观测误差协方差 $R = \\sigma^2 I_m$，其中 $\\sigma^2 = 1.0$。\n- 右端项 $d \\in \\mathbb{R}^{m}$ 的元素为独立的标准正态分布。\n- 固定随机数生成器种子以确保可复现性。\n\n实现 PCG 算法，初始迭代值为 $\\lambda_0 = 0$，相对残差容差 $\\varepsilon = 10^{-8}$（定义为 $\\lVert r_k \\rVert_2 / \\lVert d \\rVert_2  \\varepsilon$，其中 $r_k$ 是第 $k$ 次迭代的残差），最大迭代次数为 $K_{\\max} = 500$。\n\n通过对截断秩 $k \\in \\{0, 5, 10, 20, 40\\}$ 运行 PCG 求解器来定义测试套件。$k = 0$ 和 $k = r$ 的情况分别涵盖了无预处理和全秩预处理的边界条件，而 $k  r$ 的情况必须通过有效地截断到 $k=r$ 来处理。\n\n对于每个测试用例，可量化的答案是达到指定容差所需的 PCG 迭代次数的整数值（如果未达到容差，则为 $K_{\\max}$）。你的程序应产生单行输出，其中包含一个逗号分隔的列表，用方括号括起来，并按测试套件中秩的顺序排列结果，例如 $[n_0,n_1,n_2,n_3,n_4]$，其中 $n_i$ 是指定顺序中第 $i$ 个测试用例的迭代次数。", "solution": "问题陈述是计算科学领域内一个有效且良态的练习，特别是在反演问题和数据同化领域。它要求实现并分析一种预处理共轭梯度 (PCG) 算法，用于求解增量变分同化中产生的观测空间法方程。该问题具有科学依据，形式上明确，并且内部一致。\n\n问题的核心是求解线性系统：\n$$\n\\left(H B H^\\top + R\\right)\\lambda = d\n$$\n其中系统矩阵，我们记为 $A = H B H^\\top + R$，是对称正定 (SPD) 的。这一性质由问题的设定保证：$B$ 是对称半正定的 ($B = G G^\\top$)，而 $R$ 是对称正定的 ($R = \\sigma^2 I_m$ 且 $\\sigma^2  0$)。对于大型、稀疏或结构化的 SPD 系统，PCG 方法是首选算法。\n\n问题为协方差矩阵指定了一种在大规模应用中常见的特殊结构。背景误差协方差 $B$ 被建模为一个低秩矩阵，$B = G G^\\top$，其中 $G \\in \\mathbb{R}^{n \\times r}$ 且秩 $r$ 远小于状态维数 $n$。这是基于集合的协方差模型的特点。观测误差协方差 $R$ 是对角的，$R = \\sigma^2 I_m$，这对应于不相关观测误差的假设。\n\n在这些结构下，系统矩阵变为 $A = (H G)(H G)^\\top + \\sigma^2 I_m$。令 $K = H G \\in \\mathbb{R}^{m \\times r}$。矩阵 $A$ 现在表示为一个缩放单位矩阵的低秩更新：\n$$\nA = K K^\\top + \\sigma^2 I_m\n$$\n利用这种结构来设计一个高效的预处理器。矩阵 $K K^\\top$ 是对称半正定的，秩最多为 $r$。其特征分解可以从 $K$ 的奇异值分解 (SVD) 高效计算得出。令 $K$ 的 SVD 为 $K = U \\Sigma V^\\top$，其中 $U \\in \\mathbb{R}^{m \\times r}$ 的列是标准正交的，$\\Sigma \\in \\mathbb{R}^{r \\times r}$ 是奇异值 $s_i$ 构成的对角矩阵，$V \\in \\mathbb{R}^{r \\times r}$ 是一个正交矩阵。$K K^\\top$ 的特征分解则为：\n$$\nK K^\\top = (U \\Sigma V^\\top)(U \\Sigma V^\\top)^\\top = U \\Sigma V^\\top V \\Sigma^\\top U^\\top = U \\Sigma^2 U^\\top = U \\Lambda U^\\top\n$$\n其中 $\\Lambda = \\Sigma^2$ 是一个对角矩阵，其元素 $\\lambda_i = s_i^2$ 是 $K K^\\top$ 的非零特征值。$U$ 的列是相应的特征向量。\n\n系统矩阵为 $A = U \\Lambda U^\\top + \\sigma^2 I_m$。一个好的预处理器 $M$ 应该近似于 $A$，并且其逆 $M^{-1}$ 应该易于计算。问题基于 $K K^\\top$ 的截断特征分解定义了一个预处理器 $M_k$：\n$$\nM_k = U_k \\Lambda_k U_k^\\top + \\sigma^2 I_m\n$$\n其中 $U_k$ 包含 $U$ 的前 $k$ 列（对应于 $k$ 个最大特征值的特征向量），$\\Lambda_k$ 包含前 $k$ 个最大的特征值。该矩阵的逆 $M_k^{-1}$ 可以使用 Sherman-Morrison-Woodbury 公式解析求得。问题给出了 $M_k^{-1}$ 对向量 $z$ 的作用如下：\n$$\nM_k^{-1} z = \\frac{1}{\\sigma^2} z + U_k \\cdot \\mathrm{diag}\\left(\\left\\{\\frac{1}{\\sigma^2 + \\lambda_i} - \\frac{1}{\\sigma^2}\\right\\}_{i=1}^{k}\\right) \\cdot U_k^\\top z\n$$\n该公式允许以无矩阵的方式应用预处理器。当 $k=0$ 时，第二项消失，预处理器简化为按 $1/\\sigma^2$ 进行缩放，这等同于在按 $\\sigma^2$ 缩放的系统上使用标准的共轭梯度法。随着 $k$ 增加到接近 $r$，$M_k$ 成为 $A$ 的一个更好的近似，预计 PCG 方法会以更少的迭代次数收敛。对于 $k \\geq r$，预处理器成为 $A$ 的精确逆（在此公式中），除去数值精度限制，PCG 应该在一次迭代中收敛。\n\n将遵循以下原则实现该解法：\n1.  **问题设置**：根据指定的维度和统计分布生成矩阵 $H$、$G$ 和向量 $d$，使用固定的随机种子以保证可复现性。$H$ 的行被归一化。\n2.  **算子和预处理器构建**：\n    *   形成矩阵 $K = H G$。\n    *   计算 $K$ 的 SVD 以获得 $U$、$\\Sigma$ 和 $V$。$K K^\\top$ 的特征值是 $\\Sigma$ 中奇异值的平方。\n    *   实现一个表示矩阵向量积 $A v = (K K^\\top + \\sigma^2 I_m) v$ 的函数。这通过 $K(K^\\top v) + \\sigma^2 v$ 来完成，以避免显式形成 $m \\times m$ 的矩阵 $K K^\\top$。\n    *   实现一个函数工厂，该工厂给定一个截断秩 $k$，返回一个用于应用预处理器 $M_k^{-1} z$ 的函数，该函数使用所提供的公式。此函数将处理 $k$ 超过秩 $r$ 的情况。\n3.  **PCG 实现**：从基本原理实现 PCG 算法。该算法通过沿相对于系统矩阵 $A$ 共轭并通过预处理器改进的方向搜索来迭代地优化解。循环持续进行，直到相对残差范数低于容差 $\\varepsilon = 10^{-8}$ 或达到最大迭代次数 $K_{\\max}=500$。\n4.  **实验执行**：对每个截断秩 $k \\in \\{0, 5, 10, 20, 40\\}$ 运行 PCG 求解器，并记录收敛所需的迭代次数。结果将展示基于 SVD 的预处理器作为截断秩 $k$ 的函数的有效性。\n\n最终输出将是这些迭代次数的列表，并按要求格式化。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the variational assimilation problem using PCG with a truncated SVD preconditioner.\n    \"\"\"\n    # Fixed experimental setup\n    m = 150  # Observation dimension\n    n = 300  # State dimension\n    r = 20   # Background covariance rank\n    sigma2 = 1.0\n    seed = 42\n\n    # PCG parameters\n    tol = 1e-8\n    max_iter = 500\n\n    # Test suite\n    truncation_ranks = [0, 5, 10, 20, 40]\n\n    # --- 1. Problem Generation ---\n    rng = np.random.default_rng(seed)\n\n    G = rng.standard_normal((n, r))\n    H_raw = rng.standard_normal((m, n))\n    d = rng.standard_normal(m)\n\n    # Normalize rows of H to have unit Euclidean norm\n    H_norms = np.linalg.norm(H_raw, axis=1, keepdims=True)\n    # Avoid division by zero, though highly unlikely with this setup\n    H_norms[H_norms == 0] = 1.0\n    H = H_raw / H_norms\n\n    # --- 2. Operator and Preconditioner Construction ---\n    # Form the low-rank factor in observation space\n    HG = H @ G\n\n    # Compute SVD of HG for the preconditioner\n    # U has orthonormal columns, s has singular values in descending order\n    U, s, _ = np.linalg.svd(HG, full_matrices=False)\n    \n    # Eigenvalues of HBH^T are squares of singular values of HG\n    lambdas = s**2\n    \n    # Define the matrix-vector product for A = HBH^T + R = (HG)(HG)^T + sigma^2*I\n    def apply_A(v):\n        \"\"\"Matrix-free application of A = (HG)(HG)^T + sigma^2 * I.\"\"\"\n        return HG @ (HG.T @ v) + sigma2 * v\n\n    def get_preconditioner_op(k, U_svd, lambdas_svd, sigma2_val, r_max):\n        \"\"\"\n        Factory for the preconditioner application function M_k^{-1}.\n        \"\"\"\n        # Effective rank cannot exceed the actual rank of HBH^T\n        k_eff = min(k, r_max)\n\n        if k_eff == 0:\n            # For k=0, M is sigma^2*I, so M^{-1} is (1/sigma^2)*I\n            return lambda z: (1.0 / sigma2_val) * z\n\n        # Truncate to the top k_eff components\n        Uk = U_svd[:, :k_eff]\n        lambdas_k = lambdas_svd[:k_eff]\n        \n        # Pre-compute the diagonal for the SMW-derived formula\n        diag_vals = 1.0 / (sigma2_val + lambdas_k) - 1.0 / sigma2_val\n        \n        def apply_M_inv(z):\n            \"\"\"Matrix-free application of the preconditioner M_k^{-1}.\"\"\"\n            # Term 1: (1/sigma^2) * z\n            term1 = (1.0 / sigma2_val) * z\n            # Term 2: U_k * D_k * U_k^T * z\n            U_T_z = Uk.T @ z\n            term2 = Uk @ (diag_vals * U_T_z)\n            return term1 + term2\n            \n        return apply_M_inv\n\n    def pcg(A_op, d_vec, M_inv_op, tolerance, max_iterations):\n        \"\"\"\n        Implementation of the Preconditioned Conjugate Gradient method.\n        \"\"\"\n        x = np.zeros_like(d_vec)\n        r = d_vec.copy()\n        \n        norm_d = np.linalg.norm(d_vec)\n        if norm_d == 0:\n            return 0  # Trivial solution\n            \n        # Check initial residual\n        if np.linalg.norm(r) / norm_d  tolerance:\n            return 0\n\n        z = M_inv_op(r)\n        p = z.copy()\n        rs_old = r @ z\n\n        for i in range(1, max_iterations + 1):\n            Ap = A_op(p)\n            alpha = rs_old / (p @ Ap)\n            \n            x += alpha * p\n            r -= alpha * Ap\n            \n            if np.linalg.norm(r) / norm_d  tolerance:\n                return i\n            \n            z = M_inv_op(r)\n            rs_new = r @ z\n            \n            # Fletcher-Reeves update for beta\n            beta = rs_new / rs_old\n            p = z + beta * p\n            rs_old = rs_new\n            \n        return max_iterations\n\n    # --- 3. Experiment Execution ---\n    results = []\n    for k in truncation_ranks:\n        # Get the specific preconditioner for the current rank k\n        apply_M_inv = get_preconditioner_op(k, U, lambdas, sigma2, r)\n        \n        # Run PCG and store the number of iterations\n        iterations = pcg(apply_A, d, apply_M_inv, tol, max_iter)\n        results.append(iterations)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3390394"}, {"introduction": "标准的二次代价函数假设观测误差服从高斯分布，但这在现实世界中常常不成立，因为观测数据可能包含离群值或遵循非高斯误差分布。本练习将引导您超越这一经典假设，通过引入鲁棒代价函数（如Huber或学生t分布）来处理这些复杂情况。您将通过实现迭代重加权最小二乘（IRLS）算法，来解决由此产生的非线性优化问题，这充分展示了变分框架在适应更复杂统计假设方面的灵活性 [@problem_id:3390445]。", "problem": "考虑在存在稳健观测项和非高斯误差的情况下，对状态向量进行增量变分同化。设背景场状态表示为 $x_b \\in \\mathbb{R}^n$，分析场求取为 $x_a = x_b + \\delta x$，其中 $\\delta x \\in \\mathbb{R}^n$ 是增量。假设背景误差为高斯分布，其协方差 $B \\in \\mathbb{R}^{n \\times n}$ 是对称正定矩阵。观测可以是在单个时间点（三维变分同化，3D-Var），也可以是跨越多个时间点并伴有线性模式演变（四维变分同化，4D-Var）。观测算子在背景场周围进行线性化，对于 3D-Var 产生一个矩阵 $H \\in \\mathbb{R}^{m \\times n}$，对于 4D-Var 产生时间索引对 $(H_k, M_{0 \\rightarrow k})$，其中 $M_{0 \\rightarrow k} \\in \\mathbb{R}^{n \\times n}$ 是从初始时间到时间 $k$ 的切线性模式。\n\n定义新息向量 $d \\in \\mathbb{R}^m$ 为观测与背景场投影状态之间的差异，并将观测误差尺度收集到 $\\sigma \\in \\mathbb{R}^m$ 中，其各项均为严格正值。待最小化的稳健增量成本函数形式如下\n$$\nJ(\\delta x) = \\frac{1}{2}\\, \\delta x^\\top B^{-1} \\delta x + \\sum_{i=1}^m \\rho\\!\\left(\\frac{(G\\,\\delta x - d)_i}{\\sigma_i}\\right),\n$$\n其中 $G \\in \\mathbb{R}^{m \\times n}$ 表示从 $\\delta x$ 到观测空间的线性化映射（对于 $3\\text{D-Var}$，$G = H$；对于 $4\\text{D-Var}$，$G$ 堆叠了所有观测时间的 $H_k M_{0 \\rightarrow k}$），$(\\cdot)_i$ 表示第 $i$ 个分量，$\\rho$ 是一个捕获非高斯误差特征的稳健罚函数。假设 $\\rho$ 是凸或伪凸的，并且几乎处处可微，其一阶导数有良好定义，除非可能在个别点上需要使用极限导数。\n\n从最大后验估计和线性化的基本原理出发，推导一个有理论依据的迭代算法来最小化 $J(\\delta x)$，该算法不依赖于非高斯观测模型的闭式解公式。您的算法必须与增量格式一致，遵循 $4\\text{D-Var}$ 的线性化动力学，并至少能处理以下稳健罚函数：二次（高斯）、Huber、伪-Huber 和 Student-$t$，每种都带有指定的稳健性参数。当增量变化足够小或达到最大迭代次数时，算法必须终止，并应返回计算出的增量 $\\delta x$。\n\n在一个程序中实现该算法，并将其应用于以下测试套件。在所有情况下，将背景场状态 $x_b$ 取为零向量，因此新息等于观测向量。所有数值必须完全按照给定值使用。所有线性代数对象均以标准矩阵和向量表示法表示。\n\n测试用例 1 (3D-Var, 对单个离群值稳健):\n- 状态维数 $n = 2$。\n- 背景协方差 $B = \\mathrm{diag}([1.0, 1.0])$。\n- 观测算子 $H = I_{2 \\times 2}$。\n- 新息 $d = \\begin{bmatrix} 1.0 \\\\ 10.0 \\end{bmatrix}$。\n- 观测尺度 $\\sigma = \\begin{bmatrix} 1.0 \\\\ 1.0 \\end{bmatrix}$。\n- 稳健罚函数：Huber，阈值参数 $\\delta = 1.0$。\n\n测试用例 2 (3D-Var, 具有小残差的近高斯情况):\n- 状态维数 $n = 2$。\n- 背景协方差 $B = \\mathrm{diag}([1.0, 1.0])$。\n- 观测算子 $H = I_{2 \\times 2}$。\n- 新息 $d = \\begin{bmatrix} 0.1 \\\\ -0.1 \\end{bmatrix}$。\n- 观测尺度 $\\sigma = \\begin{bmatrix} 1.0 \\\\ 1.0 \\end{bmatrix}$。\n- 稳健罚函数：高斯二次型。\n\n测试用例 3 ($4\\text{D-Var}$, 具有重尾观测误差和单个大离群值的标量状态):\n- 状态维数 $n = 1$。\n- 背景协方差 $B = [1.0]$。\n- 线性模式 $x_{k+1} = a\\, x_k$，其中 $a = 0.9$，时间为 $k = 0, 1, 2, 3$。\n- 在时间 $k = 1, 2, 3, 4$ 的观测为 $y = \\begin{bmatrix} 1.0 \\\\ 1.0 \\\\ 20.0 \\\\ 1.0 \\end{bmatrix}$；由于 $x_b = 0$，新息等于 $d = y$。\n- 观测算子 $H_k = [1]$（对所有时间）。\n- 观测尺度 $\\sigma = \\begin{bmatrix} 1.0 \\\\ 1.0 \\\\ 1.0 \\\\ 1.0 \\end{bmatrix}$。\n- 稳健罚函数：Student-$t$，自由度参数 $\\nu = 3.0$。\n\n测试用例 4 ($4\\text{D-Var}$, 具有更强离群值和通过伪-Huber 实现的类$\\ell_1$稳健罚函数的标量状态):\n- 状态维数 $n = 1$。\n- 背景协方差 $B = [1.0]$。\n- 线性模式 $x_{k+1} = a\\, x_k$，其中 $a = 0.95$，时间为 $k = 0, 1, 2, 3$。\n- 在时间 $k = 1, 2, 3, 4$ 的观测为 $y = \\begin{bmatrix} 2.0 \\\\ -2.0 \\\\ 50.0 \\\\ 2.0 \\end{bmatrix}$；由于 $x_b = 0$，新息等于 $d = y$。\n- 观测算子 $H_k = [1]$（对所有时间）。\n- 观测尺度 $\\sigma = \\begin{bmatrix} 1.0 \\\\ 1.0 \\\\ 1.0 \\\\ 1.0 \\end{bmatrix}$。\n- 稳健罚函数：伪-Huber，参数 $\\delta = 0.1$。\n\n您的程序应为每个测试用例计算增量 $\\delta x$，并生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个元素本身是一个浮点数列表，代表相应测试用例的 $\\delta x$ 的分量。例如，输出的形式应为 $\\left[[\\cdot,\\cdot],[\\cdot,\\cdot],[\\cdot],[\\cdot]\\right]$。所有数值答案都必须以浮点数形式报告。此问题不涉及任何物理单位或角度单位。", "solution": "根据既定准则对问题进行验证。\n\n### 步骤 1：提取已知条件\n- **状态向量**：$x \\in \\mathbb{R}^n$\n- **背景场状态**：$x_b \\in \\mathbb{R}^n$\n- **分析场状态**：$x_a = x_b + \\delta x$\n- **状态增量**：$\\delta x \\in \\mathbb{R}^n$\n- **背景误差协方差**：$B \\in \\mathbb{R}^{n \\times n}$，对称正定。\n- **观测算子**：\n    - 对于 3D-Var：$H \\in \\mathbb{R}^{m \\times n}$。\n    - 对于 4D-Var：时间索引对 $(H_k, M_{0 \\rightarrow k})$，其中 $M_{0 \\rightarrow k} \\in \\mathbb{R}^{n \\times n}$ 是切线性模式。\n- **新息向量**：$d \\in \\mathbb{R}^m$，观测与背景场投影状态之间的差异。\n- **观测误差尺度**：$\\sigma \\in \\mathbb{R}^m$，各项均为严格正值。\n- **增量成本函数**：\n$$\nJ(\\delta x) = \\frac{1}{2}\\, \\delta x^\\top B^{-1} \\delta x + \\sum_{i=1}^m \\rho\\!\\left(\\frac{(G\\,\\delta x - d)_i}{\\sigma_i}\\right)\n$$\n- **线性化观测映射**：$G \\in \\mathbb{R}^{m \\times n}$。对于 3D-Var，$G = H$。对于 4D-Var，$G$ 堆叠了 $H_k M_{0 \\rightarrow k}$。\n- **稳健罚函数**：$\\rho$，凸或伪凸，几乎处处可微。\n- **特殊条件**：背景场状态 $x_b$ 为零向量，意味着新息向量 $d$ 等于观测向量 $y$。\n- **终止条件**：当增量变化足够小或达到最大迭代次数时，迭代算法终止。\n- **要求的稳健罚函数**：高斯（二次）、Huber、伪-Huber、Student-$t$。\n\n**测试用例：**\n1.  **3D-Var, Huber**：$n=2$, $B = \\mathrm{diag}([1.0, 1.0])$, $H = I_{2 \\times 2}$, $d = [1.0, 10.0]^\\top$, $\\sigma = [1.0, 1.0]^\\top$, Huber 罚函数，$\\delta = 1.0$。\n2.  **3D-Var, 高斯**：$n=2$, $B = \\mathrm{diag}([1.0, 1.0])$, $H = I_{2 \\times 2}$, $d = [0.1, -0.1]^\\top$, $\\sigma = [1.0, 1.0]^\\top$, 高斯罚函数。\n3.  **4D-Var, Student-t**：$n=1$, $B = [1.0]$, 线性模式 $x_{k+1} = a x_k$，$a = 0.9$。在 $k=1,2,3,4$ 时的观测为 $y = [1.0, 1.0, 20.0, 1.0]^\\top$。$H_k = [1]$（对所有 $k$）。$\\sigma = [1.0, 1.0, 1.0, 1.0]^\\top$。Student-$t$ 罚函数，$\\nu = 3.0$。\n4.  **4D-Var, 伪-Huber**：$n=1$, $B = [1.0]$, 线性模式 $x_{k+1} = a x_k$，$a = 0.95$。在 $k=1,2,3,4$ 时的观测为 $y = [2.0, -2.0, 50.0, 2.0]^\\top$。$H_k = [1]$（对所有 $k$）。$\\sigma = [1.0, 1.0, 1.0, 1.0]^\\top$。伪-Huber 罚函数，$\\delta = 0.1$。\n\n### 步骤 2：使用提取的已知条件进行验证\n- **科学依据**：该问题描述了使用稳健统计的增量变分同化。这是数据同化中的一个标准且高级的课题，用于气象学和海洋学等领域处理非高斯观测误差。成本函数公式和指定的罚函数在文献中都是成熟的。\n- **适定性**：该问题是适定的。成本函数是一个严格凸的二次项 ($\\frac{1}{2}\\delta x^\\top B^{-1} \\delta x$) 和一个凸或伪凸项的和，因此其本身是凸或伪凸的，这确保了最小值的存在。对于指定的罚函数，预期存在唯一最小值。所有测试用例的参数都已完全指定。\n- **客观性**：该问题使用精确的数学和科学语言陈述，没有歧义或主观性。\n\n该问题不存在任何无效性缺陷：\n1.  它在科学和数学上是合理的。\n2.  它可以直接形式化，并且是变分同化主题的核心。\n3.  每个测试用例的设置是完整且一致的。\n4.  条件和数据在计算模型上下文中被数值化指定并且物理上是合理的。\n5.  问题结构是标准的，并能导出一个定义明确的解。\n6.  问题并非微不足道；它需要推导和实现一个非平凡的迭代算法（IRLS）来处理一般的非二次罚函数。\n7.  结果是可数值验证的。\n\n### 步骤 3：结论与行动\n该问题是**有效的**。将提供一个合理的解答。\n\n### 求解推导\n\n目标是找到最小化成本函数的状态增量 $\\delta x$：\n$$\nJ(\\delta x) = \\frac{1}{2}\\, \\delta x^\\top B^{-1} \\delta x + \\sum_{i=1}^m \\rho\\!\\left(\\frac{(G\\,\\delta x - d)_i}{\\sigma_i}\\right)\n$$\n此最小化问题等价于寻找状态的最大后验（MAP）估计，其假设是状态服从高斯先验分布（由背景项表示），且观测误差分布的负对数与罚函数 $\\rho$ 成正比。\n\n成本函数 $J(\\delta x)$ 由一个二次背景项和一个非二次观测项组成。因此，$J(\\delta x)$ 通常不是二次函数，其最小值无法通过求解单个线性系统得到。需要一个迭代过程。处理此类问题的一个有理论依据且广泛使用的方法是迭代重加权最小二乘（IRLS）算法。\n\nIRLS 的核心原理是用一系列二次函数来逼近非二次成本函数。在每次迭代 $k$ 中，观测罚函数项 $\\sum_i \\rho(r_i)$（其中 $r_i = ((G\\,\\delta x - d)_i/\\sigma_i)$）被一个加权二次项所近似。$\\rho(r_i)$ 在 $r_i=0$ 附近的二阶泰勒展开是 $\\rho(0) + \\rho'(0)r_i + \\frac{1}{2}\\rho''(0)r_i^2$。这启发我们将 $\\rho(r_i)$ 近似为 $\\frac{1}{2} w_i r_i^2$，其中权重 $w_i$ 取决于当前的残差。一个稳健的权重选择，它将 IRLS 与 Gauss-Newton 优化方法联系起来，由下式给出：\n$$\nw(r) = \\frac{\\rho'(r)}{r}\n$$\n对于 $r \\to 0$，只要 $\\rho'(0)=0$，该权重就趋近于 $\\rho''(0)$。对于给定的罚函数，这个极限是良定义的。\n\n在第 $k+1$ 次迭代中，使用根据前一估计 $\\delta x_k$ 计算出的权重 $w_i^{(k)}$，成本函数被一个纯二次函数 $J_{k+1}(\\delta x)$ 近似：\n$$\nJ_{k+1}(\\delta x) = \\frac{1}{2}\\, \\delta x^\\top B^{-1} \\delta x + \\frac{1}{2} \\sum_{i=1}^m w_i^{(k)} \\left( \\frac{(G\\,\\delta x - d)_i}{\\sigma_i} \\right)^2\n$$\n这可以写成矩阵形式：\n$$\nJ_{k+1}(\\delta x) = \\frac{1}{2}\\, \\delta x^\\top B^{-1} \\delta x + \\frac{1}{2} (G\\,\\delta x - d)^\\top R_k^{-1} (G\\,\\delta x - d)\n$$\n其中 $R_k^{-1}$ 是当前迭代的有效逆观测误差方差的对角矩阵。其对角元素为 $(R_k^{-1})_{ii} = w_i^{(k)} / \\sigma_i^2$。\n\n为了找到二次函数 $J_{k+1}(\\delta x)$ 的最小值，我们将其关于 $\\delta x$ 的梯度设为零：\n$$\n\\nabla_{\\delta x} J_{k+1}(\\delta x) = B^{-1} \\delta x + G^\\top R_k^{-1} (G\\,\\delta x - d) = 0\n$$\n重新整理各项以求解 $\\delta x$（它将成为下一次迭代的估计值 $\\delta x_{k+1}$），得到以下线性系统：\n$$\n(B^{-1} + G^\\top R_k^{-1} G) \\delta x_{k+1} = G^\\top R_k^{-1} d\n$$\n这个方程构成了迭代算法的核心。\n\n### 稳健罚函数和权重\n\n权重函数 $w(u) = \\rho'(u)/u$ 是为每个所需的罚函数推导的，其中 $u$ 代表一个归一化的残差。\n\n1.  **高斯（二次）**：$\\rho(u) = \\frac{1}{2} u^2$\n    - $\\rho'(u) = u$\n    - $w(u) = \\frac{u}{u} = 1$。权重是恒定的，因此标准线性最小二乘解在一次迭代中即可得到。\n\n2.  **Huber**：$\\rho(u) = \\begin{cases} \\frac{1}{2}u^2  |u| \\le \\delta \\\\ \\delta|u| - \\frac{1}{2}\\delta^2  |u|  \\delta \\end{cases}$\n    - $\\rho'(u) = \\begin{cases} u  |u| \\le \\delta \\\\ \\delta \\cdot \\mathrm{sgn}(u)  |u|  \\delta \\end{cases}$\n    - $w(u) = \\frac{\\rho'(u)}{u} = \\begin{cases} 1  |u| \\le \\delta \\\\ \\delta/|u|  |u|  \\delta \\end{cases}$\n\n3.  **伪-Huber**：$\\rho(u) = \\delta^2 \\left(\\sqrt{1 + (u/\\delta)^2} - 1\\right)$\n    - $\\rho'(u) = \\frac{u}{\\sqrt{1 + (u/\\delta)^2}}$\n    - $w(u) = \\frac{\\rho'(u)}{u} = \\frac{1}{\\sqrt{1 + (u/\\delta)^2}}$\n\n4.  **Student-t**：$\\rho(u) = \\frac{\\nu+1}{2} \\log\\left(1 + \\frac{u^2}{\\nu}\\right)$\n    - $\\rho'(u) = \\frac{(\\nu+1)u}{\\nu + u^2}$\n    - $w(u) = \\frac{\\rho'(u)}{u} = \\frac{\\nu+1}{\\nu + u^2}$\n\n### 算法摘要\n寻找 $\\delta x$ 的迭代算法如下：\n1.  **初始化**：设置迭代计数器 $k = 0$ 并初始化增量，例如 $\\delta x_0 = 0$。\n2.  **迭代**：对于 $k=0, 1, 2, \\dots$ 直到收敛或达到最大迭代次数：\n    a. 使用当前增量 $\\delta x_k$ 计算归一化残差 $r^{(k)}$：$r_i^{(k)} = (G\\,\\delta x_k - d)_i / \\sigma_i$。\n    b. 根据所选的罚函数和残差 $r_i^{(k)}$ 计算权重 $w_i^{(k)}$。对于 $r_i^{(k)} \\approx 0$，使用权重函数的极限值以避免除以零。\n    c. 形成有效逆观测误差协方差矩阵 $R_k^{-1}$，它是一个对角矩阵，其元素为 $(R_k^{-1})_{ii} = w_i^{(k)} / \\sigma_i^2$。\n    d. 构造矩阵 $A = B^{-1} + G^\\top R_k^{-1} G$ 和向量 $b = G^\\top R_k^{-1} d$。\n    e. 求解线性系统 $A \\delta x_{k+1} = b$ 以找到更新后的增量 $\\delta x_{k+1}$。\n    f. 通过比较 $\\delta x_{k+1}$ 和 $\\delta x_k$ 来检查收敛性。例如，如果 $\\|\\delta x_{k+1} - \\delta x_k\\|  \\epsilon$（对于一个小的容差 $\\epsilon$），则终止。\n3.  **返回**：最终计算出的增量 $\\delta x$。\n\n该算法同时适用于 3D-Var 和 4D-Var，唯一的区别在于矩阵 $G$ 的构造方式。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef get_weights(u, penalty_type, params, epsilon=1e-9):\n    \"\"\"\n    Calculates the weights for the IRLS algorithm for different robust penalties.\n    w(u) = rho'(u) / u.\n    \n    Args:\n        u (np.ndarray): The normalized residuals.\n        penalty_type (str): The name of the penalty function.\n        params (dict): Parameters for the penalty function.\n        epsilon (float): Small number to handle division by zero.\n\n    Returns:\n        np.ndarray: The array of weights.\n    \"\"\"\n    if penalty_type == 'gaussian':\n        return np.ones_like(u)\n    \n    elif penalty_type == 'huber':\n        delta = params['delta']\n        abs_u = np.abs(u)\n        weights = np.ones_like(u)\n        mask = abs_u > delta\n        weights[mask] = delta / abs_u[mask]\n        return weights\n        \n    elif penalty_type == 'pseudo_huber':\n        delta = params['delta']\n        return 1.0 / np.sqrt(1.0 + (u / delta)**2)\n        \n    elif penalty_type == 'student_t':\n        nu = params['nu']\n        return (nu + 1) / (nu + u**2)\n        \n    else:\n        raise ValueError(f\"Unknown penalty type: {penalty_type}\")\n\ndef solve_irls(B, G, d, sigma, penalty_type, penalty_params, max_iter=100, tol=1e-8):\n    \"\"\"\n    Solves the variational assimilation problem using Iteratively Reweighted Least Squares.\n\n    Args:\n        B (np.ndarray): Background error covariance matrix.\n        G (np.ndarray): Linearized observation operator.\n        d (np.ndarray): Innovation vector.\n        sigma (np.ndarray): Observation error scales.\n        penalty_type (str): Type of robust penalty ('gaussian', 'huber', etc.).\n        penalty_params (dict): Parameters for the penalty function.\n        max_iter (int): Maximum number of iterations.\n        tol (float): Convergence tolerance.\n\n    Returns:\n        np.ndarray: The computed state increment delta_x.\n    \"\"\"\n    n = B.shape[0]\n    delta_x = np.zeros(n)\n    \n    B_inv = np.linalg.inv(B)\n    \n    for k in range(max_iter):\n        prev_delta_x = delta_x.copy()\n        \n        # 1. Calculate normalized residuals\n        residuals = G @ delta_x - d\n        normalized_residuals = residuals / sigma\n        \n        # 2. Calculate weights\n        weights = get_weights(normalized_residuals, penalty_type, penalty_params)\n        \n        # 3. Form effective observation error inverse covariance\n        R_inv_eff = np.diag(weights / (sigma**2))\n        \n        # 4. Construct and solve the linear system\n        # (B_inv + G.T @ R_inv_eff @ G) @ delta_x = G.T @ R_inv_eff @ d\n        A = B_inv + G.T @ R_inv_eff @ G\n        b = G.T @ R_inv_eff @ d\n        \n        delta_x = np.linalg.solve(A, b)\n        \n        # 5. Check for convergence\n        if np.linalg.norm(delta_x - prev_delta_x)  tol:\n            break\n            \n    return delta_x\n\ndef solve():\n    \"\"\"\n    Main function to define and solve the test cases.\n    \"\"\"\n    test_cases = [\n        # Case 1: 3D-Var, Huber\n        {\n            'type': '3d_var',\n            'n': 2,\n            'B': np.diag([1.0, 1.0]),\n            'H': np.eye(2),\n            'd': np.array([1.0, 10.0]),\n            'sigma': np.array([1.0, 1.0]),\n            'penalty_type': 'huber',\n            'penalty_params': {'delta': 1.0}\n        },\n        # Case 2: 3D-Var, Gaussian\n        {\n            'type': '3d_var',\n            'n': 2,\n            'B': np.diag([1.0, 1.0]),\n            'H': np.eye(2),\n            'd': np.array([0.1, -0.1]),\n            'sigma': np.array([1.0, 1.0]),\n            'penalty_type': 'gaussian',\n            'penalty_params': {}\n        },\n        # Case 3: 4D-Var, Student-t\n        {\n            'type': '4d_var',\n            'n': 1,\n            'B': np.array([[1.0]]),\n            'a': 0.9,\n            'obs_times': np.array([1, 2, 3, 4]),\n            'H_k': np.array([[1.0]]),\n            'd': np.array([1.0, 1.0, 20.0, 1.0]),\n            'sigma': np.array([1.0, 1.0, 1.0, 1.0]),\n            'penalty_type': 'student_t',\n            'penalty_params': {'nu': 3.0}\n        },\n        # Case 4: 4D-Var, pseudo-Huber\n        {\n            'type': '4d_var',\n            'n': 1,\n            'B': np.array([[1.0]]),\n            'a': 0.95,\n            'obs_times': np.array([1, 2, 3, 4]),\n            'H_k': np.array([[1.0]]),\n            'd': np.array([2.0, -2.0, 50.0, 2.0]),\n            'sigma': np.array([1.0, 1.0, 1.0, 1.0]),\n            'penalty_type': 'pseudo_huber',\n            'penalty_params': {'delta': 0.1}\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        if case['type'] == '3d_var':\n            G = case['H']\n        elif case['type'] == '4d_var':\n            a = case['a']\n            obs_times = case['obs_times']\n            # M_0_k for scalar case is a^k\n            G = np.array([a**k for k in obs_times]).reshape(-1, 1) @ case['H_k']\n        \n        delta_x = solve_irls(\n            case['B'], G, case['d'], case['sigma'],\n            case['penalty_type'], case['penalty_params']\n        )\n        results.append(list(delta_x))\n\n    def format_list(l):\n        return '[' + ','.join(f\"{x:.15g}\" for x in l) + ']'\n    \n    all_results_str = ','.join([format_list(r) for r in results])\n    final_output = f\"[{all_results_str}]\"\n    \n    print(final_output)\n\nsolve()\n```", "id": "3390445"}]}