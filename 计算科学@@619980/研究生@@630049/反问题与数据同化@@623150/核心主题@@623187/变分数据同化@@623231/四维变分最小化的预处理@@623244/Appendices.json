{"hands_on_practices": [{"introduction": "预处理旨在改善四维变分（4D-Var）问题中 Hessian 矩阵的条件数，从而加速迭代求解器的收敛。对角缩放是一种简单但有效的预处理形式，它通过对控制变量和观测进行重新缩放来调整问题的几何结构。\n\n本练习提供了一个具体的动手计算，让您能直接观察到用于背景和观测误差的缩放矩阵如何改变高斯-牛顿（Gauss-Newton）Hessian 矩阵 [@problem_id:3412592]。通过应用格什戈林圆盘定理（Gershgorin circle theorem），您将学习一种实用技术来估计所得预处理矩阵的谱半径，这对于评估预处理器的有效性至关重要。", "problem": "考虑在线性化动力学和观测算子下的增量四维变分(4D-Var)资料同化框架。在通过背景预处理得到的控制变量空间中，Hessian矩阵的Gauss–Newton近似由一个对称正定矩阵表示。为研究对角预处理的效果，假设使用以下对角尺度化近似：一个背景尺度化矩阵 $\\tilde{L}$ 以及在每个时间 $k$ 的观测误差尺度化 $\\tilde{R}_{k}^{-1/2}$。那么，在时间 $k$ 的尺度化线性观测映射为\n$$\n\\tilde{J}_{k} \\;=\\; \\tilde{R}_{k}^{-1/2}\\, H_{k}\\, M_{0,k}\\, \\tilde{L},\n$$\n从而近似的控制空间Hessian矩阵为\n$$\nH_{v} \\;=\\; I \\;+\\; \\sum_{k} \\tilde{J}_{k}^{\\top}\\tilde{J}_{k}.\n$$\n\n设状态维数为 $n=3$，并考虑两个观测时间 $k=1,2$。假设从时间0到时间$k$的切线性模型算子、观测算子和对角尺度化如下：\n$$\nM_{0,1} \\;=\\; \\begin{pmatrix} 1  0.4  0 \\\\ 0  1  0.2 \\\\ 0  0  1 \\end{pmatrix}, \\quad\nM_{0,2} \\;=\\; \\begin{pmatrix} 1  -0.2  0.1 \\\\ 0  1  0.3 \\\\ 0  0  1 \\end{pmatrix},\n$$\n$$\nH_{1} \\;=\\; \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\end{pmatrix}, \\quad\nH_{2} \\;=\\; \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\end{pmatrix},\n$$\n$$\n\\tilde{L} \\;=\\; \\operatorname{diag}(2,\\,1,\\,3), \\quad\n\\tilde{R}_{1}^{-1/2} \\;=\\; \\operatorname{diag}(2,\\,2), \\quad\n\\tilde{R}_{2}^{-1/2} \\;=\\; \\operatorname{diag}\\!\\left(\\tfrac{5}{2},\\,\\tfrac{5}{3}\\right).\n$$\n\n从标准的增量4D-Var代价函数及其在控制空间中的Gauss–Newton Hessian矩阵出发，推导由上述选择所隐含的矩阵 $H_{v}$。然后，使用Gershgorin圆盘定理，确定 $H_{v}$ 谱半径的最紧可能上界，即对角线元素与非对角线元素绝对值行和之和在各行中的最大值。将最终答案表示为一个精确的有理数。无需四舍五入，最终结果中不应包含任何单位。", "solution": "在线性化设定下，控制变量空间中的增量四维变分(4D-Var)代价函数可以写为\n$$\nJ(v) \\;=\\; \\tfrac{1}{2}\\,\\|v\\|^{2} \\;+\\; \\tfrac{1}{2}\\sum_{k}\\big\\| \\tilde{R}_{k}^{-1/2}\\, H_{k}\\, M_{0,k}\\, \\tilde{L}\\, v \\big\\|^{2},\n$$\n其中 $v$ 是控制变量，$\\tilde{L}$ 和 $\\tilde{R}_{k}^{-1/2}$ 是对角尺度化近似。Hessian矩阵 $H_{v}$ 的Gauss–Newton近似则为\n$$\nH_{v} \\;=\\; I \\;+\\; \\sum_{k} \\tilde{J}_{k}^{\\top}\\tilde{J}_{k}, \\quad \\text{其中} \\quad \\tilde{J}_{k} \\;=\\; \\tilde{R}_{k}^{-1/2}\\, H_{k}\\, M_{0,k}\\, \\tilde{L}.\n$$\n\n我们首先计算 $\\tilde{J}_{1}$ 和 $\\tilde{J}_{2}$。\n\n通过对 $M_{0,1}$ 的列进行尺度变换来计算 $M_{0,1}\\tilde{L}$：\n$$\nM_{0,1}\\tilde{L} \\;=\\; \\begin{pmatrix} 2  0.4  0 \\\\ 0  1  0.6 \\\\ 0  0  3 \\end{pmatrix}.\n$$\n应用 $H_{1}$（选择前两行）：\n$$\nH_{1}M_{0,1}\\tilde{L} \\;=\\; \\begin{pmatrix} 2  0.4  0 \\\\ 0  1  0.6 \\end{pmatrix}.\n$$\n左乘 $\\tilde{R}_{1}^{-1/2} = \\operatorname{diag}(2,2)$：\n$$\n\\tilde{J}_{1} \\;=\\; \\begin{pmatrix} 4  0.8  0 \\\\ 0  2  1.2 \\end{pmatrix} \\;=\\; \\begin{pmatrix} 4  \\tfrac{4}{5}  0 \\\\ 0  2  \\tfrac{6}{5} \\end{pmatrix}.\n$$\n\n类似地，计算 $M_{0,2}\\tilde{L}$：\n$$\nM_{0,2}\\tilde{L} \\;=\\; \\begin{pmatrix} 2  -0.2  0.3 \\\\ 0  1  0.9 \\\\ 0  0  3 \\end{pmatrix}.\n$$\n应用 $H_{2}$：\n$$\nH_{2}M_{0,2}\\tilde{L} \\;=\\; \\begin{pmatrix} 2  -0.2  0.3 \\\\ 0  1  0.9 \\end{pmatrix}.\n$$\n左乘 $\\tilde{R}_{2}^{-1/2} = \\operatorname{diag}\\!\\left(\\tfrac{5}{2},\\,\\tfrac{5}{3}\\right)$：\n$$\n\\tilde{J}_{2} \\;=\\; \\begin{pmatrix} 5  -\\tfrac{1}{2}  \\tfrac{3}{4} \\\\ 0  \\tfrac{5}{3}  \\tfrac{3}{2} \\end{pmatrix}.\n$$\n\n接下来，计算贡献项 $\\tilde{J}_{k}^{\\top}\\tilde{J}_{k}$。\n\n对于 $\\tilde{J}_{1}$：\n$$\n\\tilde{J}_{1}^{\\top}\\tilde{J}_{1} \\;=\\; \\begin{pmatrix}\n16  \\tfrac{16}{5}  0 \\\\\n\\tfrac{16}{5}  \\tfrac{116}{25}  \\tfrac{12}{5} \\\\\n0  \\tfrac{12}{5}  \\tfrac{36}{25}\n\\end{pmatrix}.\n$$\n\n对于 $\\tilde{J}_{2}$：\n$$\n\\tilde{J}_{2}^{\\top}\\tilde{J}_{2} \\;=\\; \\begin{pmatrix}\n25  -\\tfrac{5}{2}  \\tfrac{15}{4} \\\\\n-\\tfrac{5}{2}  \\tfrac{109}{36}  \\tfrac{17}{8} \\\\\n\\tfrac{15}{4}  \\tfrac{17}{8}  \\tfrac{45}{16}\n\\end{pmatrix}.\n$$\n\n将两者相加并加上单位矩阵以得到 $H_{v}$：\n$$\n\\sum_{k}\\tilde{J}_{k}^{\\top}\\tilde{J}_{k} \\;=\\; \\begin{pmatrix}\n41  \\tfrac{7}{10}  \\tfrac{15}{4} \\\\\n\\tfrac{7}{10}  \\tfrac{6901}{900}  \\tfrac{181}{40} \\\\\n\\tfrac{15}{4}  \\tfrac{181}{40}  \\tfrac{1701}{400}\n\\end{pmatrix},\n$$\n$$\nH_{v} \\;=\\; I \\;+\\; \\sum_{k}\\tilde{J}_{k}^{\\top}\\tilde{J}_{k} \\;=\\; \\begin{pmatrix}\n42  \\tfrac{7}{10}  \\tfrac{15}{4} \\\\\n\\tfrac{7}{10}  \\tfrac{7801}{900}  \\tfrac{181}{40} \\\\\n\\tfrac{15}{4}  \\tfrac{181}{40}  \\tfrac{2101}{400}\n\\end{pmatrix}.\n$$\n\n根据Gershgorin圆盘定理，$H_{v}$ 的每个特征值都位于至少一个以 $a_{ii}$ 为圆心、半径为 $r_{i} = \\sum_{j\\neq i} |a_{ij}|$ 的圆盘内。谱半径（最大特征值）的上界为 $a_{ii} + r_{i}$ 在所有 $i$ 中的最大值。我们计算非对角线元素绝对值的行和：\n$$\nr_{1} \\;=\\; \\left|\\tfrac{7}{10}\\right| + \\left|\\tfrac{15}{4}\\right| \\;=\\; \\tfrac{7}{10} + \\tfrac{15}{4} \\;=\\; \\tfrac{89}{20},\n$$\n$$\nr_{2} \\;=\\; \\left|\\tfrac{7}{10}\\right| + \\left|\\tfrac{181}{40}\\right| \\;=\\; \\tfrac{7}{10} + \\tfrac{181}{40} \\;=\\; \\tfrac{209}{40},\n$$\n$$\nr_{3} \\;=\\; \\left|\\tfrac{15}{4}\\right| + \\left|\\tfrac{181}{40}\\right| \\;=\\; \\tfrac{15}{4} + \\tfrac{181}{40} \\;=\\; \\tfrac{331}{40}.\n$$\n\n现在为每一行计算 $a_{ii} + r_{i}$：\n$$\na_{11} + r_{1} \\;=\\; 42 + \\tfrac{89}{20} \\;=\\; \\tfrac{840}{20} + \\tfrac{89}{20} \\;=\\; \\tfrac{929}{20},\n$$\n$$\na_{22} + r_{2} \\;=\\; \\tfrac{7801}{900} + \\tfrac{209}{40} \\;=\\; \\tfrac{15602}{1800} + \\tfrac{9405}{1800} \\;=\\; \\tfrac{25007}{1800},\n$$\n$$\na_{33} + r_{3} \\;=\\; \\tfrac{2101}{400} + \\tfrac{331}{40} \\;=\\; \\tfrac{2101}{400} + \\tfrac{3310}{400} \\;=\\; \\tfrac{5411}{400}.\n$$\n\n基于Gershgorin定理的谱半径最紧上界是这三个量中的最大值。比较可知，\n$$\n\\max\\!\\left\\{ \\tfrac{929}{20}, \\tfrac{25007}{1800}, \\tfrac{5411}{400} \\right\\} \\;=\\; \\tfrac{929}{20}.\n$$\n\n因此，在给定的对角尺度化下，$H_{v}$ 最大特征值的Gershgorin上界是 $\\tfrac{929}{20}$。", "answer": "$$\\boxed{\\tfrac{929}{20}}$$", "id": "3412592"}, {"introduction": "观测空间预处理的一个关键组成部分是“白化”(whitening)新息（innovations），这依赖于观测误差协方差矩阵 $R_k$ 被正确定模的假设。如果该假设成立，经过白化处理的新息应表现为标准正态分布的随机向量。\n\n本练习将理论付诸实践，要求您实现一个基本的统计诊断工具：卡方检验 [@problem_id:3412579]。您将编写代码来评估归一化新息的统计行为是否符合预期，从而能够量化地检测观测误差是否存在尺度偏差（即模型对观测是过于自信还是不够自信）。这对于任何使用真实世界数据同化系统的人来说都是一项关键技能。", "problem": "考虑一个四维变分 (4D-Var) 数据同化设定，其观测时间序列由索引 $k \\in \\{1,\\dots, K\\}$ 标记。在每个时刻，有一个状态估计 $x_k \\in \\mathbb{R}^{n_k}$、一个观测向量 $y_k \\in \\mathbb{R}^{m_k}$、一个线性观测算子 $H_k \\in \\mathbb{R}^{m_k \\times n_k}$，以及一个对称正定的观测误差协方差矩阵 $R_k \\in \\mathbb{R}^{m_k \\times m_k}$。定义新息为 $d_k = y_k - H_k x_k$。四维变分 (4D-Var) 数据同化中的观测空间预处理使用归一化新息 $z_k = R_k^{-1/2} d_k$，其中 $R_k^{-1/2}$ 表示满足 $R_k^{-1/2} R_k R_k^{-1/2} = I$ 的唯一对称逆平方根。\n\n一项用于观测空间预处理和缩放的基本统计诊断断言，在正确的观测误差建模和无偏的预报误差下，序列 $\\{z_k\\}$ 的行为应近似于独立的标准正态向量，即 $z_k \\sim \\mathcal{N}(0, I_{m_k})$。因此，范数平方和 $S = \\sum_{k=1}^K \\|z_k\\|_2^2$（在理想假设下）近似服从自由度为 $\\nu = \\sum_{k=1}^K m_k$ 的卡方分布。\n\n您的任务是实现一个程序，该程序针对一个同化窗口内的几个测试案例，计算归一化新息，并以 $\\alpha = 0.05$ 的显著性水平进行双边卡方一致性检验，以检测不良的观测缩放。如果 $S$ 落在卡方接受区间 $[\\chi^2_{\\nu}^{-1}(\\alpha/2), \\chi^2_{\\nu}^{-1}(1-\\alpha/2)]$ 之外，则该检验必须将案例判定为缩放不良。此外，还需报告标准化偏差 $Z = (S - \\nu) / \\sqrt{2 \\nu}$ 作为描述性统计量。\n\n通过特征值分解计算 $R_k^{-1/2}$：如果 $R_k = Q_k \\Lambda_k Q_k^\\top$，其中 $\\Lambda_k = \\mathrm{diag}(\\lambda_{k,1},\\dots,\\lambda_{k,m_k})$ 且 $\\lambda_{k,i} > 0$，则 $R_k^{-1/2} = Q_k \\Lambda_k^{-1/2} Q_k^\\top$，其中 $\\Lambda_k^{-1/2} = \\mathrm{diag}(\\lambda_{k,1}^{-1/2},\\dots,\\lambda_{k,m_k}^{-1/2})$。\n\n实现以下测试套件。在所有案例中，假设一个案例内各时间点之间是独立的。\n\n- 案例 A（缩放良好，维度恒定）：\n  - 窗口长度 $K = 5$，对所有 $k$ 维度为 $m_k = 2$。\n  - 对所有 $k$，$H_k = I_2$ 且 $x_k = [0, 0]^\\top$。\n  - 对所有 $k$，使用 $R_k$ 等于常数矩阵 $R = \\mathrm{diag}(1.0, 4.0)$。\n  - 观测值 $y_k$ 如下：\n    - $y_1 = [0.3, -2.4]^\\top$\n    - $y_2 = [-0.7, 1.0]^\\top$\n    - $y_3 = [1.1, 0.0]^\\top$\n    - $y_4 = [0.0, -1.2]^\\top$\n    - $y_5 = [-1.3, 1.8]^\\top$\n\n- 案例 B（观测过于自信，新息与案例 A 相同但缩放错误）：\n  - $K$、$m_k$、$H_k$、$x_k$ 和 $y_k$ 与案例 A 相同。\n  - 对所有 $k$，使用 $R_k = 0.25 \\times \\mathrm{diag}(1.0, 4.0)$。\n\n- 案例 C（观测不够自信，新息与案例 A 相同但缩放错误）：\n  - $K$、$m_k$、$H_k$、$x_k$ 和 $y_k$ 与案例 A 相同。\n  - 对所有 $k$，使用 $R_k = 4.0 \\times \\mathrm{diag}(1.0, 4.0)$。\n\n- 案例 D（窗口内维度变化，缩放良好）：\n  - 窗口长度 $K = 4$，维度为 $m_1 = 2$，$m_2 = 1$，$m_3 = 2$，$m_4 = 1$。\n  - 观测算子和状态：\n    - $H_1 = I_2$, $x_1 = [0, 0]^\\top$\n    - $H_2 = I_1$, $x_2 = [0]^\\top$\n    - $H_3 = I_2$, $x_3 = [0, 0]^\\top$\n    - $H_4 = I_1$, $x_4 = [0]^\\top$\n  - 观测误差协方差 $R_k$ 和观测值 $y_k$：\n    - $R_1 = \\mathrm{diag}(1.0, 9.0)$, $y_1 = [0.5, -1.5]^\\top$\n    - $R_2 = [4.0]$, $y_2 = [1.4]$\n    - $R_3 = \\mathrm{diag}(0.5, 2.0)$, $y_3 = [-0.70710678, 0.42426407]^\\top$\n    - $R_4 = [1.0]$, $y_4 = [0.0]$\n\n每个案例的算法规范：\n- 对每个时间 $k$，计算新息 $d_k = y_k - H_k x_k$。\n- 通过对称特征值分解计算 $R_k^{-1/2}$，并构建归一化新息 $z_k = R_k^{-1/2} d_k$。\n- 累加 $S \\leftarrow S + \\|z_k\\|_2^2$ 和 $\\nu \\leftarrow \\nu + m_k$。\n- 窗口结束后，计算 $Z = (S - \\nu) / \\sqrt{2 \\nu}$。\n- 使用显著性水平 $\\alpha = 0.05$，计算接受区间的边界 $a = \\chi^2_{\\nu}^{-1}(\\alpha/2)$ 和 $b = \\chi^2_{\\nu}^{-1}(1 - \\alpha/2)$。如果 $S  a$ 或 $S > b$，则将标志设为 $1$，否则设为 $0$。\n\n最终输出规范：\n- 对每个案例，返回一个双元素列表 $[Z, \\mathrm{flag}]$，其中 $Z$ 四舍五入到六位小数，$\\mathrm{flag} \\in \\{0, 1\\}$ 是在 $\\alpha = 0.05$ 水平下卡方一致性检验的判定结果。\n- 您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果。例如，包含四个案例的一行应类似于 $[[Z_1,\\mathrm{flag}_1],[Z_2,\\mathrm{flag}_2],[Z_3,\\mathrm{flag}_3],[Z_4,\\mathrm{flag}_4]]$，不含多余文本。\n\n本问题陈述中的所有数学符号和数字均指其在线性代数和统计学中的通常含义。不涉及物理单位或角度，也不需要百分比。确保所有计算在内部是一致的、确定性的，并且可从所提供的数据中复现。", "solution": "该问题要求在一个4D-Var数据同化背景下，为一系列新息实现一个卡方一致性检验。每个测试案例的关键步骤如下：\n1.  **遍历同化窗口**：对于每个时间步 $k=1, \\dots, K$，我们处理可用的观测数据。\n2.  **计算新息**：新息（或观测减预报残差）是 $d_k = y_k - H_k x_k$。在所有指定的测试案例中，状态估计 $x_k$ 都是零向量，这简化为 $d_k=y_k$。\n3.  **归一化新息**：使用观测误差协方差矩阵 $R_k$ 对新息进行归一化。这是预处理和统计分析中的关键一步。归一化新息为 $z_k = R_k^{-1/2} d_k$。\n    矩阵 $R_k^{-1/2}$ 是 $R_k$ 的唯一对称逆平方根。问题指定通过特征值分解来计算它。对于一个对称矩阵 $R_k$，其分解为 $R_k = Q_k \\Lambda_k Q_k^\\top$，其中 $Q_k$ 是一个由特征向量构成的正交矩阵，$\\Lambda_k$ 是一个由特征值构成的对角矩阵。那么逆平方根由 $R_k^{-1/2} = Q_k \\Lambda_k^{-1/2} Q_k^\\top$ 给出。由于问题中所有的 $R_k$ 矩阵都是对角矩阵，它们的特征向量构成标准基，所以 $Q_k = I$。因此，$R_k^{-1/2}$ 只是一个对角矩阵，其对角线元素是 $R_k$ 对角线元素的倒数平方根。为了通用性和遵守问题规范，我们实现了完整的特征值分解方法。\n4.  **计算检验统计量S**：在理想假设下，即预报误差无偏且观测误差被 $R_k$ 正确建模，归一化新息 $z_k$ 是来自标准正态分布的独立随机向量，即 $z_k \\sim \\mathcal{N}(0, I_{m_k})$。这样一个向量的欧几里得范数的平方 $\\|z_k\\|_2^2$ 服从自由度为 $m_k$ 的卡方分布，即 $\\|z_k\\|_2^2 \\sim \\chi^2(m_k)$。这些独立的卡方变量之和 $S = \\sum_{k=1}^K \\|z_k\\|_2^2$ 服从一个卡方分布，其自由度等于各个自由度之和，即 $\\nu = \\sum_{k=1}^K m_k$。\n5.  **计算描述性统计量Z**：对于一个卡方变量 $S \\sim \\chi^2(\\nu)$，其均值为 $\\nu$，方差为 $2\\nu$。标准化偏差 $Z = (S - \\mathbb{E}[S]) / \\sqrt{\\mathrm{Var}(S)} = (S - \\nu) / \\sqrt{2\\nu}$ 衡量了观测到的 $S$ 距离其期望值有多少个标准差。\n6.  **执行假设检验**：我们检验观测缩放是正确的这一原假设。在 $\\alpha = 0.05$ 的显著性水平下使用双边检验。我们从 $\\chi^2(\\nu)$ 分布的逆累积分布函数中找到临界值 $a = \\chi^2_{\\nu}^{-1}(\\alpha/2)$ 和 $b = \\chi^2_{\\nu}^{-1}(1-\\alpha/2)$。如果计算出的统计量 $S$ 落在接受域 $[a, b]$ 之外，我们拒绝原假设，并得出观测缩放不良的结论。这通过将一个标志设置为 $1$ 来表示。否则，标志为 $0$。\n\n**案例 A：缩放良好**\n$K=5$，$m_k=2$，$\\nu=10$。$R_k=\\mathrm{diag}(1,4)$。\n$S = \\sum_{k=1}^5 \\|z_k\\|^2 = 1.53 + 0.74 + 1.21 + 0.36 + 2.50 = 6.34$。\n$Z = (6.34 - 10)/\\sqrt{20} \\approx -0.818382$。\n$\\chi^2(10)$ 的 $95\\%$ 接受区间是 $[\\chi^2_{10}(0.025), \\chi^2_{10}(0.975)] \\approx [3.247, 20.483]$。\n由于 $S=6.34$ 在此区间内，标志为 $0$。\n\n**案例 B：过于自信（R 过小）**\n$R_k = 0.25 \\times \\mathrm{diag}(1,4)$。这意味着我们声称观测误差比实际要小。\n$R_k^{-1/2}$ 是案例 A 的两倍，因此 $z_k$ 是其两倍大。$\\|z_k\\|^2$ 是其四倍大。\n$S = 4 \\times 6.34 = 25.36$。$\\nu=10$。\n$Z = (25.36 - 10)/\\sqrt{20} \\approx 3.434046$。\n由于 $S=25.36 > 20.483$，标志为 $1$。\n\n**案例 C：不够自信（R 过大）**\n$R_k = 4.0 \\times \\mathrm{diag}(1,4)$。这意味着我们声称观测误差比实际要大。\n$R_k^{-1/2}$ 是案例 A 的一半，因此 $\\|z_k\\|^2$ 是其四分之一大。\n$S = 0.25 \\times 6.34 = 1.585$。$\\nu=10$。\n$Z = (1.585 - 10)/\\sqrt{20} \\approx -1.881646$。\n由于 $S=1.585  3.247$，标志为 $1$。\n\n**案例 D：维度变化，缩放良好**\n$K=4$，$m_k = \\{2,1,2,1\\}$，$\\nu = 6$。\n$S = \\|z_1\\|^2 + \\|z_2\\|^2 + \\|z_3\\|^2 + \\|z_4\\|^2 = 0.5 + 0.49 + 1.09 + 0.0 = 2.08$。\n$Z = (2.08 - 6)/\\sqrt{12} \\approx -1.131593$。\n$\\chi^2(6)$ 的 $95\\%$ 接受区间是 $[\\chi^2_{6}(0.025), \\chi^2_{6}(0.975)] \\approx [1.237, 14.449]$。\n由于 $S=2.08$ 在此区间内，标志为 $0$。", "answer": "```python\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef solve():\n    \"\"\"\n    Computes statistical diagnostics for 4D-Var observation scaling for several test cases.\n    \"\"\"\n    \n    # Define common data for Cases A, B, C\n    common_y_list = [\n        np.array([0.3, -2.4]),\n        np.array([-0.7, 1.0]),\n        np.array([1.1, 0.0]),\n        np.array([0.0, -1.2]),\n        np.array([-1.3, 1.8])\n    ]\n    common_R_base = np.diag([1.0, 4.0])\n\n    # Define the complete test suite\n    test_cases = [\n        # Case A: Well-scaled\n        {\n            \"K\": 5, \"m_list\": [2] * 5, \"x_list\": [np.zeros(2)] * 5, \"H_list\": [np.eye(2)] * 5,\n            \"y_list\": common_y_list, \"R_list\": [common_R_base] * 5,\n        },\n        # Case B: Overconfident observations\n        {\n            \"K\": 5, \"m_list\": [2] * 5, \"x_list\": [np.zeros(2)] * 5, \"H_list\": [np.eye(2)] * 5,\n            \"y_list\": common_y_list, \"R_list\": [0.25 * common_R_base] * 5,\n        },\n        # Case C: Underconfident observations\n        {\n            \"K\": 5, \"m_list\": [2] * 5, \"x_list\": [np.zeros(2)] * 5, \"H_list\": [np.eye(2)] * 5,\n            \"y_list\": common_y_list, \"R_list\": [4.0 * common_R_base] * 5,\n        },\n        # Case D: Varying dimension, well-scaled\n        {\n            \"K\": 4, \"m_list\": [2, 1, 2, 1],\n            \"x_list\": [np.zeros(2), np.zeros(1), np.zeros(2), np.zeros(1)],\n            \"H_list\": [np.eye(2), np.eye(1), np.eye(2), np.eye(1)],\n            \"y_list\": [\n                np.array([0.5, -1.5]),\n                np.array([1.4]),\n                np.array([-0.70710678, 0.42426407]),\n                np.array([0.0])\n            ],\n            \"R_list\": [\n                np.diag([1.0, 9.0]),\n                np.array([[4.0]]),\n                np.diag([0.5, 2.0]),\n                np.array([[1.0]])\n            ],\n        }\n    ]\n\n    results = []\n    alpha = 0.05\n\n    for case in test_cases:\n        S = 0.0\n        nu = 0\n        \n        for k in range(case[\"K\"]):\n            # Extract data for time step k\n            mk = case[\"m_list\"][k]\n            xk = case[\"x_list\"][k]\n            Hk = case[\"H_list\"][k]\n            yk = case[\"y_list\"][k]\n            Rk = case[\"R_list\"][k]\n            \n            # Step 1: Compute innovation\n            dk = yk - Hk @ xk\n            \n            # Step 2: Compute R_k^{-1/2} via eigenvalue decomposition\n            eigvals, eigvecs = np.linalg.eigh(Rk)\n            # Assuming Rk is positive definite as per problem statement\n            Rk_inv_sqrt = eigvecs @ np.diag(1.0 / np.sqrt(eigvals)) @ eigvecs.T\n\n            # Step 3: Compute normalized innovation\n            zk = Rk_inv_sqrt @ dk\n            \n            # Step 4: Accumulate S and nu\n            S += np.sum(zk**2)\n            nu += mk\n\n        # Step 5: Compute standardized deviation Z\n        # Handle nu=0 case, though not expected for this problem set.\n        Z = (S - nu) / np.sqrt(2 * nu) if nu > 0 else 0.0\n        \n        # Step 6: Compute chi-square acceptance interval\n        lower_bound = chi2.ppf(alpha / 2.0, nu)\n        upper_bound = chi2.ppf(1.0 - alpha / 2.0, nu)\n        \n        # Step 7: Perform the test and set the flag\n        flag = 0 if (lower_bound = S = upper_bound) else 1\n        \n        results.append([round(Z, 6), flag])\n\n    # Format the final output string exactly as specified in the problem description example.\n    # The example [[Z_1,flag_1],[Z_2,flag_2],...] implies no spaces.\n    results_str_parts = []\n    for z_val, flag_val in results:\n        results_str_parts.append(f\"[{z_val},{flag_val}]\")\n    \n    final_output = f\"[{','.join(results_str_parts)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3412579"}, {"introduction": "虽然简单的对角缩放很有用，但先进的预处理技术通常涉及构建一个能够捕捉复杂误差结构的精密背景误差协方差矩阵 $B$。混合模型融合了静态（气候）和基于集合的动态信息，是这一领域的前沿方法，它旨在利用两种方法的优势。\n\n在这个综合性的高级练习中，您将实现一个交叉验证框架，以确定混合背景协方差模型的最优混合权重 [@problem_id:3412516]。这项实践不仅能巩固您对背景预处理如何构建的理解，还将向您介绍数据驱动的调优方法，这些方法对于构建高性能的数据同化系统至关重要。", "problem": "考虑一个用于线性、确定性动力系统的四维变分（4D-Var）数据同化问题。状态向量为 $x_t \\in \\mathbb{R}^n$，动力学模型为 $x_{t+1} = M x_t$，观测为 $y_t = H x_t + \\eta_t$，其中 $\\eta_t \\sim \\mathcal{N}(0, R)$ 是独立同分布的高斯观测噪声。初始条件 $x_0$ 的背景（先验）是均值为 $x_b = 0$、协方差为 $B$ 的高斯分布。长度为 $T$ 的时间窗内的4D-Var代价函数为\n$$\nJ(x_0) = (x_0 - x_b)^\\top B^{-1} (x_0 - x_b) + \\sum_{t=0}^{T-1} (y_t - H M^t x_0)^\\top R^{-1} (y_t - H M^t x_0).\n$$\n在预处理4D-Var中，通常会寻找 $B^{-1}$ 的一个近似，以改善正规方程矩阵的条件数。我们考虑一种混合集合-变分近似方法，该方法在精度空间（协方差矩阵的逆）中融合了气候协方差与集合异常信息。设 $B_c \\in \\mathbb{R}^{n \\times n}$ 表示一个已知且严格正定的气候协方差。设 $A \\in \\mathbb{R}^{n \\times m}$ 表示一个集合异常矩阵，其列由从气候分布中提取的中心化样本给出，并按 $1/\\sqrt{m-1}$ 缩放，使得 $B_e = A A^\\top$ 是一个经验集合协方差。为了数值稳定性，定义一个正则化的经验协方差 $B_e^\\epsilon = B_e + \\epsilon I_n$，其中 $\\epsilon  0$ 是一个很小的标量。对于混合权重 $w \\in [0,1]$，定义混合精度矩阵\n$$\nB^{-1}(w) = w B_c^{-1} + (1 - w) \\left(B_e^\\epsilon\\right)^{-1}.\n$$\n你必须通过交叉验证确定最优混合权重 $w^\\star$，方法是最小化一个验证损失，该损失衡量了由训练观测计算出的估计值 $x_0$ 预测预留观测的效果。\n\n你的任务是编写一个完整的程序，该程序：\n- 将 $B_c$ 构建为一个一维索引上的平稳协方差，其元素为 $(B_c)_{ij} = \\sigma_b^2 \\exp\\left(-\\frac{|i-j|}{L}\\right)$，适用于索引 $i,j \\in \\{1, \\dots, n\\}$，其中方差为 $\\sigma_b^2$，相关长度尺度为 $L$。\n- 通过从 $\\mathcal{N}(0, B_c)$ 中抽取 $m$ 个独立样本来生成集合异常，形成样本矩阵，通过减去样本均值进行中心化，并按 $1/\\sqrt{m-1}$ 进行缩放，以获得 $A$，使得 $B_e = A A^\\top$。\n- 构建一个稳定的线性动力学矩阵 $M$ 为 $M = Q \\operatorname{diag}(d) Q^\\top$，其中 $Q$ 是标准正交矩阵，$d \\in \\mathbb{R}^n$ 的元素在 $(0,1)$ 区间内，这使得 $M$ 可对角化且谱半径小于1。\n- 对于每个同化窗口，生成一个真实初始条件 $x_0^{\\text{true}} \\sim \\mathcal{N}(0, B_c)$，模拟轨迹 $x_t = M^t x_0^{\\text{true}}$，并生成观测 $y_t = H x_t + \\eta_t$，其中 $\\eta_t \\sim \\mathcal{N}(0, R)$ 且 $R = \\sigma_o^2 I_p$。\n- 在每个测试案例中进行交叉验证：将每个窗口的观测划分为训练时间 $t \\in \\{0,1,\\dots,T-2\\}$ 和一个验证时间 $t = T-1$。对于给定的 $w$，计算训练4D-Var代价（背景项使用 $B^{-1}(w)$）的最小化器 $x_0^\\star(w)$，并评估验证损失\n$$\n\\ell(w) = \\left(y_{T-1} - H M^{T-1} x_0^\\star(w)\\right)^\\top R^{-1} \\left(y_{T-1} - H M^{T-1} x_0^\\star(w)\\right).\n$$\n交叉验证得分 $S(w)$ 是测试案例中所有窗口的 $\\ell(w)$ 的平均值。最优混合权重是 $w^\\star = \\arg\\min_{w \\in \\mathcal{W}} S(w)$，在预定义的网格 $\\mathcal{W}$ 上求得。\n- 使用以下针对训练时间的'正规方程'来计算 $x_0^\\star(w)$：\n$$\n\\left(B^{-1}(w) + \\sum_{t=0}^{T-2} (M^t)^\\top H^\\top R^{-1} H M^t\\right) x_0^\\star(w) = \\sum_{t=0}^{T-2} (M^t)^\\top H^\\top R^{-1} y_t,\n$$\n其中 $x_b = 0$。\n\n预处理由 $B^{-1}(w)$ 表示；你可以选择通过Cholesky分解将 $B^{-1}(w)$ 分解为 $P(w)^\\top P(w)$，但所要求的估计直接使用正规方程。\n\n定义线性观测算子 $H \\in \\mathbb{R}^{p \\times n}$ 为 $H = [I_p, 0_{p \\times (n-p)}]$，即它提取状态的前 $p$ 个分量。使用以下测试套件，并固定随机种子以确保可复现性，并在混合权重网格 $\\mathcal{W} = \\{0.00, 0.05, 0.10, \\dots, 1.00\\}$ 上进行搜索：\n\n- 测试案例 1 (均衡的集合大小):\n  - 状态维度: $n = 20$。\n  - 观测维度: $p = 10$。\n  - 窗口长度: $T = 5$。\n  - 集合大小: $m = 8$。\n  - 气候方差: $\\sigma_b^2 = 1.0$。\n  - 相关长度: $L = 3.0$。\n  - 观测方差: $\\sigma_o^2 = 0.01$。\n  - 正则化: $\\epsilon = 10^{-3}$。\n  - 窗口数量: $K = 4$。\n  - 种子: 动力学 $Q$ 种子 = 123，异常种子 = 101，窗口种子 = 201。\n\n- 测试案例 2 (小集合大小):\n  - $n = 20$, $p = 10$, $T = 5$, $m = 4$。\n  - $\\sigma_b^2 = 1.0$, $L = 3.0$。\n  - $\\sigma_o^2 = 0.01$, $\\epsilon = 10^{-3}$。\n  - $K = 4$。\n  - 种子: 动力学 $Q$ 种子 = 124，异常种子 = 102，窗口种子 = 202。\n\n- 测试案例 3 (大集合大小):\n  - $n = 20$, $p = 10$, $T = 5$, $m = 18$。\n  - $\\sigma_b^2 = 1.0$, $L = 3.0$。\n  - $\\sigma_o^2 = 0.01$, $\\epsilon = 10^{-3}$。\n  - $K = 4$。\n  - 种子: 动力学 $Q$ 种子 = 125，异常种子 = 103，窗口种子 = 203。\n\n实现细节：\n- 通过抽取一个随机高斯矩阵，应用QR分解得到 $Q$，并将对角元素 $d_i$ 均匀分布在 $[0.80, 0.95]$ 区间内来构建 $M$。使用 $M = Q \\operatorname{diag}(d) Q^\\top$。\n- 对于每个测试案例和每个折叠 $k \\in \\{1,\\dots,K\\}$，使用指定的窗口种子加上偏移量 $k$ 来抽取 $x_0^{\\text{true}}$ 和 $\\eta_t$，以保证可复现性。\n- 通过重复相乘计算 $t = 0, 1, \\dots, T-1$ 的矩阵幂 $M^t$。\n\n你的程序应生成单行输出，其中包含三个最优混合权重，每个测试案例一个，形式为用方括号括起来的逗号分隔列表（例如，`[w1,w2,w3]`）。每个 $w_i$ 必须是从交叉验证找到的网格 $\\mathcal{W}$ 中选择的 $[0,1]$ 区间内的实数。不涉及物理单位，也不出现角度。给定种子，输出必须是可复现的。", "solution": "用户希望解决一个使用混合背景误差协方差模型的四维变分（4D-Var）数据同化问题。目标是通过最小化交叉验证得分，找到用于混合气候协方差 $B_c$ 和基于集合的协方差 $B_e$ 的最优混合权重 $w^\\star$。\n\n### 方法与原理\n\n该问题是完全自包含的，并为数值实验提供了完整的规范。我将遵循指定的程序，包括模型设置、数据生成和交叉验证，以确定三个不同测试案例的最优混合权重 $w^\\star$。该解决方案需要仔细实现线性代数运算、使用指定种子以确保可复现性的随机数生成，以及在可能的权重网格上进行系统搜索。\n\n#### 1. 模型分量构建\n\n对于每个测试案例，我将构建4D-Var系统的静态分量。\n\n*   **气候背景协方差 ($B_c$)**：该矩阵基于提供的平稳指数衰减模型构建：\n    $$\n    (B_c)_{ij} = \\sigma_b^2 \\exp\\left(-\\frac{|i-j|}{L}\\right)\n    $$\n    其中 $i, j \\in \\{0, \\dots, n-1\\}$，$\\sigma_b^2$ 是背景方差，$L$ 是相关长度尺度。该矩阵是对称正定的。它的逆矩阵 $B_c^{-1}$ 和其Cholesky因子 $L_c$（其中 $B_c = L_c L_c^\\top$）将被预先计算。\n\n*   **集合背景协方差 ($B_e^\\epsilon$)**：通过从气候分布 $\\mathcal{N}(0, B_c)$ 中抽取样本来生成一个包含 $m$ 个成员的集合。这些样本经过中心化和缩放，形成一个异常矩阵 $A \\in \\mathbb{R}^{n \\times m}$。经验协方差为 $B_e = A A^\\top$。为了数值稳定性，它被正则化为 $B_e^\\epsilon = B_e + \\epsilon I_n$。由于 $B_e$ 是半正定的且 $\\epsilon  0$，因此 $B_e^\\epsilon$ 是严格正定的。其逆矩阵 $(B_e^\\epsilon)^{-1}$ 将被预先计算。\n\n*   **动力学矩阵 ($M$)**：一个稳定的线性算子 $M$ 构建为 $M = Q \\operatorname{diag}(d) Q^\\top$。标准正交矩阵 $Q$ 从一个随机高斯矩阵的QR分解中获得，对角矩阵 $\\operatorname{diag}(d)$ 包含被指定在区间 $[0.80, 0.95]$ 内的特征值，确保谱半径小于1。该矩阵的幂 $M^t$（对于 $t \\in \\{0, \\dots, T-1\\}$）将被预先计算。\n\n*   **观测模型 ($H, R$)**：观测算子 $H$ 提取状态向量的前 $p$ 个分量。观测误差协方差是 $R = \\sigma_o^2 I_p$，这是一个简单的对角矩阵。其逆矩阵是 $R^{-1} = (1/\\sigma_o^2) I_p$。\n\n#### 2. 交叉验证程序\n\n任务的核心是找到最优权重 $w^\\star$ 的交叉验证循环。对于 $K$ 个交叉验证折叠（窗口）中的每一个，以及来自网格 $\\mathcal{W} = \\{0.00, 0.05, \\dots, 1.00\\}$ 的每个候选权重 $w$：\n\n*   **数据生成**：从 $\\mathcal{N}(0, B_c)$ 中抽取一个“真实”初始状态 $x_0^{\\text{true}}$。使用 $x_t = M^t x_0^{\\text{true}}$ 演化状态轨迹。通过应用观测算子并添加高斯噪声来生成观测值 $y_t$：$y_t = H x_t + \\eta_t$，其中 $\\eta_t \\sim \\mathcal{N}(0, R)$。\n\n*   **4D-Var最小化**：对观测进行划分。时间 $t \\in \\{0, \\dots, T-2\\}$ 用于训练，时间 $t = T-1$ 保留用于验证。通过求解对应于训练代价函数的正规方程来找到初始状态估计 $x_0^\\star(w)$：\n    $$\n    \\left(B^{-1}(w) + \\sum_{t=0}^{T-2} (M^t)^\\top H^\\top R^{-1} H M^t\\right) x_0^\\star(w) = \\sum_{t=0}^{T-2} (M^t)^\\top H^\\top R^{-1} y_t\n    $$\n    其中混合背景精度矩阵为 $B^{-1}(w) = w B_c^{-1} + (1-w)(B_e^\\epsilon)^{-1}$。左侧矩阵（Hessian矩阵）保证是可逆的。\n\n*   **验证损失计算**：估计值 $x_0^\\star(w)$ 的质量通过其预测在 $t = T-1$ 处的预留观测的能力来评估。验证损失是预测误差的平方马氏距离：\n    $$\n    \\ell(w) = \\left(y_{T-1} - H M^{T-1} x_0^\\star(w)\\right)^\\top R^{-1} \\left(y_{T-1} - H M^{T-1} x_0^\\star(w)\\right)\n    $$\n\n*   **得分聚合**：对于每个 $w$，将损失 $\\ell(w)$ 在 $K$ 个折叠上求和。最终的交叉验证得分 $S(w)$ 是平均损失。\n\n#### 3. 最优权重选择\n\n在计算了所有 $w \\in \\mathcal{W}$ 的 $S(w)$ 之后，测试案例的最优混合权重 $w^\\star$ 被选为最小化得分的那个：\n$$\nw^\\star = \\arg\\min_{w \\in \\mathcal{W}} S(w)\n$$\n对三个测试案例中的每一个都重复此过程，这些案例在集合大小 $m$ 和随机种子上有所不同，以探究最优混合如何随集合信息的质量而变化。最终输出是这三个最优权重的列表。\n\n实现将依赖 `numpy` 进行所有数值计算，并遵守指定的随机种子以确保可复现性。", "answer": "```python\nimport numpy as np\n\ndef run_test_case(params):\n    \"\"\"\n    Executes a single test case for 4D-Var cross-validation.\n\n    This function sets up the model components, runs K-fold cross-validation\n    to find the optimal blending weight for the hybrid background covariance,\n    and returns the optimal weight.\n    \"\"\"\n    # Unpack test case parameters\n    n, p, T, m, sigma_b_sq, L, sigma_o_sq, epsilon, K, q_seed, anom_seed, win_seed = params\n    \n    # Define blending weight grid\n    w_grid = np.linspace(0.0, 1.0, 21)\n\n    # --- Setup phase (fixed for the test case) ---\n\n    # Initialize RNGs for reproducibility\n    rng_q = np.random.default_rng(q_seed)\n    rng_anom = np.random.default_rng(anom_seed)\n\n    # 1. Construct climatological background covariance Bc and its inverse\n    indices = np.arange(n)\n    dist_matrix = np.abs(indices[:, np.newaxis] - indices[np.newaxis, :])\n    Bc = sigma_b_sq * np.exp(-dist_matrix / L)\n    Bc_inv = np.linalg.inv(Bc)\n    try:\n        Lc = np.linalg.cholesky(Bc)  # For generating samples a N(0, Bc)\n    except np.linalg.LinAlgError:\n        # Fallback for matrices that might have slight numerical precision issues\n        # although the exponential covariance is positive definite.\n        evals, evecs = np.linalg.eigh(Bc)\n        Lc = evecs @ np.diag(np.sqrt(np.maximum(evals, 1e-15)))\n\n    # 2. Construct ensemble-based covariance Be_eps and its inverse\n    # Draw m samples from N(0, Bc)\n    z_samples = rng_anom.standard_normal(size=(n, m))\n    samples = Lc @ z_samples\n    # Center the samples\n    sample_mean = samples.mean(axis=1, keepdims=True)\n    centered_samples = samples - sample_mean\n    # Form anomaly matrix A and empirical covariance Be\n    anomalies = centered_samples / np.sqrt(m - 1)\n    Be = anomalies @ anomalies.T\n    # Regularize and invert\n    Be_eps = Be + epsilon * np.eye(n)\n    Be_eps_inv = np.linalg.inv(Be_eps)\n\n    # 3. Construct dynamics matrix M and its powers\n    random_mat = rng_q.standard_normal(size=(n, n))\n    Q, _ = np.linalg.qr(random_mat)\n    d = np.linspace(0.80, 0.95, n)\n    M = Q @ np.diag(d) @ Q.T\n    M_powers = [np.linalg.matrix_power(M, t) for t in range(T)]\n\n    # 4. Construct observation operator H and error covariance inverse R_inv\n    H = np.zeros((p, n))\n    H[:, :p] = np.eye(p)\n    R_inv = (1.0 / sigma_o_sq) * np.eye(p)\n\n    # 5. Pre-compute the observation-related term of the Hessian (independent of w and k)\n    # Hess_obs_term = sum_{t=0}^{T-2} (M^t)^T H^T R^{-1} H M^t\n    Hess_obs_term = np.zeros((n, n))\n    for t in range(T - 1):  # Sum up to T-2\n        HMt = H @ M_powers[t]\n        Hess_obs_term += HMt.T @ R_inv @ HMt\n\n    # --- Cross-validation phase ---\n    \n    total_losses_per_w = np.zeros(len(w_grid))\n\n    for k in range(K):\n        # Set up RNG for this window/fold for reproducible data generation\n        rng_win = np.random.default_rng(win_seed + k)\n        \n        # Generate true state trajectory and noisy observations for this fold\n        x0_true = Lc @ rng_win.standard_normal(size=n)\n        \n        y_obs = np.zeros((T, p))\n        for t in range(T):\n            xt_true = M_powers[t] @ x0_true\n            obs_noise = rng_win.standard_normal(size=p) * np.sqrt(sigma_o_sq)\n            y_obs[t, :] = H @ xt_true + obs_noise\n            \n        # Compute the observation-related term of the normal equation's RHS\n        # RHS_obs_term = sum_{t=0}^{T-2} (M^t)^T H^T R^{-1} y_t\n        RHS_obs_term = np.zeros(n)\n        for t in range(T - 1):  # Sum up to T-2\n            HMt = H @ M_powers[t]\n            RHS_obs_term += HMt.T @ R_inv @ y_obs[t]\n            \n        # Loop over all candidate blending weights\n        for i, w in enumerate(w_grid):\n            # Assemble the hybrid precision matrix B_inv(w)\n            B_inv_w = w * Bc_inv + (1 - w) * Be_eps_inv\n            \n            # Assemble the full Hessian for the normal equations\n            Hessian = Hess_obs_term + B_inv_w\n            \n            # The RHS only has the observation term as x_b=0\n            RHS = RHS_obs_term\n            \n            # Solve for the initial condition estimate x0_star(w)\n            x0_star = np.linalg.solve(Hessian, RHS)\n            \n            # Compute validation loss using the withheld observation at t=T-1\n            y_pred_val = H @ M_powers[T - 1] @ x0_star\n            err_val = y_obs[T - 1] - y_pred_val\n            \n            # validation loss: err^T R^{-1} err\n            loss = err_val.T @ R_inv @ err_val\n            \n            # Accumulate the loss for this weight\n            total_losses_per_w[i] += loss\n            \n    # Average the losses over K folds to get the final CV score S(w)\n    avg_losses = total_losses_per_w / K\n    \n    # Find the weight w_star that minimizes the average loss\n    best_w_idx = np.argmin(avg_losses)\n    w_star = w_grid[best_w_idx]\n    \n    return w_star\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case 1 (balanced ensemble size)\n        (20, 10, 5, 8, 1.0, 3.0, 0.01, 1e-3, 4, 123, 101, 201),\n        # Test Case 2 (small ensemble size)\n        (20, 10, 5, 4, 1.0, 3.0, 0.01, 1e-3, 4, 124, 102, 202),\n        # Test Case 3 (large ensemble size)\n        (20, 10, 5, 18, 1.0, 3.0, 0.01, 1e-3, 4, 125, 103, 203),\n    ]\n\n    results = []\n    for case in test_cases:\n        # Run the cross-validation for one case and get the optimal weight.\n        w_star = run_test_case(case)\n        results.append(w_star)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3412516"}]}