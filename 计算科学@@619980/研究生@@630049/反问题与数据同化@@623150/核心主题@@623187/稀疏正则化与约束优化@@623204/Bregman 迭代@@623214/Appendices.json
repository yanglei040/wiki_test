{"hands_on_practices": [{"introduction": "在应用布雷格曼迭代解决复杂问题之前，掌握其核心组成部分——布雷格曼距离至关重要。本练习提供了一个具体的实践机会，用于计算全变分 (Total Variation, TV) 泛函的布雷格曼距离，而 TV 泛函是图像处理领域的基石。通过逐步应用 TV、其亚梯度以及布雷格曼距离的定义，您将对迭代方法背后的几何原理建立起基础性的理解 [@problem_id:3369778]。", "problem": "考虑长度为 $n=5$ 的离散一维信号，其中 $u,v \\in \\mathbb{R}^{5}$ 由下式给出\n$$\nu=\\begin{pmatrix}2\\\\0\\\\4\\\\2\\\\1\\end{pmatrix}, \\qquad v=\\begin{pmatrix}0\\\\2\\\\5\\\\1\\\\3\\end{pmatrix}.\n$$\n将离散信号 $w \\in \\mathbb{R}^{5}$ 的各向异性全变分 (TV) 定义为凸泛函\n$$\n\\mathrm{TV}(w)=\\sum_{i=1}^{4} |w_{i+1}-w_{i}|.\n$$\n令 $D:\\mathbb{R}^{5}\\to\\mathbb{R}^{4}$ 为前向有限差分算子，其定义为 $(Dw)_{i}=w_{i+1}-w_{i}$，对于 $i=1,2,3,4$。对于一个凸泛函 $J$，关于次梯度 $p \\in \\partial J(v)$ 的 Bregman 距离定义为\n$$\nD_{J}^{p}(u,v)=J(u)-J(v)-\\langle p, u-v\\rangle,\n$$\n其中 $\\langle \\cdot,\\cdot\\rangle$ 表示标准欧几里得内积。使用通过次梯度链式法则对 $\\mathrm{TV}$ 次微分的刻画：存在 $s\\in\\mathbb{R}^{4}$ 满足 $s_{i}\\in \\operatorname{sign}\\big((Dv)_{i}\\big)$，使得 $p=D^{\\top}s$，其中 $D^{\\top}$ 是 $D$ 在欧几里得内积下的伴随算子。这里，如果 $t>0$，则 $\\operatorname{sign}(t)=1$；如果 $t<0$，则 $\\operatorname{sign}(t)=-1$；且 $\\operatorname{sign}(0)=[-1,1]$。\n\n使用对 $i=1,2,3,4$ 选择 $s_{i}=\\operatorname{sign}\\big((Dv)_{i}\\big)$ 以及 $p=D^{\\top}s$ 来计算 Bregman 距离 $D_{\\mathrm{TV}}^{p}(u,v)$。你的最终答案必须是一个实数。无需四舍五入。", "solution": "问题要求计算对于给定的离散信号 $u,v \\in \\mathbb{R}^{5}$ 的 Bregman 距离 $D_{\\mathrm{TV}}^{p}(u,v)$。关于凸泛函 $J$ 和次梯度 $p \\in \\partial J(v)$ 的 Bregman 距离公式由下式给出：\n$$\nD_{J}^{p}(u,v) = J(u) - J(v) - \\langle p, u-v \\rangle\n$$\n在本问题中，泛函 $J$ 是各向异性全变分，$J=\\mathrm{TV}$。因此，我们需要计算：\n$$\nD_{\\mathrm{TV}}^{p}(u,v) = \\mathrm{TV}(u) - \\mathrm{TV}(v) - \\langle p, u-v \\rangle\n$$\n我们将分别计算每一项。给定的向量为 $u=\\begin{pmatrix}2\\\\0\\\\4\\\\2\\\\1\\end{pmatrix}$ 和 $v=\\begin{pmatrix}0\\\\2\\\\5\\\\1\\\\3\\end{pmatrix}$。\n\n首先，我们计算 $u$ 的全变分。其定义为 $\\mathrm{TV}(w) = \\sum_{i=1}^{4} |w_{i+1}-w_{i}|$。\n对于 $u$，差值为：\n$u_2 - u_1 = 0-2 = -2$\n$u_3 - u_2 = 4-0 = 4$\n$u_4 - u_3 = 2-4 = -2$\n$u_5 - u_4 = 1-2 = -1$\n$u$ 的全变分是这些差值的绝对值之和：\n$$\n\\mathrm{TV}(u) = |-2| + |4| + |-2| + |-1| = 2 + 4 + 2 + 1 = 9\n$$\n\n其次，我们计算 $v$ 的全变分。对于 $v$，差值为：\n$v_2 - v_1 = 2-0 = 2$\n$v_3 - v_2 = 5-2 = 3$\n$v_4 - v_3 = 1-5 = -4$\n$v_5 - v_4 = 3-1 = 2$\n$v$ 的全变分为：\n$$\n\\mathrm{TV}(v) = |2| + |3| + |-4| + |2| = 2 + 3 + 4 + 2 = 11\n$$\n\n第三，我们计算次梯度 $p \\in \\partial \\mathrm{TV}(v)$。问题说明 $p = D^{\\top}s$，其中 $s \\in \\mathbb{R}^4$ 由特定选择 $s_{i}=\\operatorname{sign}\\big((Dv)_{i}\\big)$ 决定。算子 $D:\\mathbb{R}^{5}\\to\\mathbb{R}^{4}$ 是前向有限差分算子，$(Dw)_i = w_{i+1}-w_i$。\n向量 $Dv$ 包含我们为计算 $\\mathrm{TV}(v)$ 而得出的差值：\n$$\nDv = \\begin{pmatrix} v_2 - v_1 \\\\ v_3 - v_2 \\\\ v_4 - v_3 \\\\ v_5 - v_4 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 3 \\\\ -4 \\\\ 2 \\end{pmatrix}\n$$\n现在，我们通过取 $Dv$ 每个分量的符号来找到向量 $s$。符号函数的定义为：对于 $t>0$，$\\operatorname{sign}(t)=1$；对于 $t<0$，$\\operatorname{sign}(t)=-1$。由于 $Dv$ 的所有分量都不为零，我们得到唯一的 $s$：\n$$\ns = \\begin{pmatrix} \\operatorname{sign}(2) \\\\ \\operatorname{sign}(3) \\\\ \\operatorname{sign}(-4) \\\\ \\operatorname{sign}(2) \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\\\ -1 \\\\ 1 \\end{pmatrix}\n$$\n接下来，我们确定伴随算子 $D^{\\top}:\\mathbb{R}^4 \\to \\mathbb{R}^5$ 的作用。伴随算子的定义性质是，对于所有 $w \\in \\mathbb{R}^5, x \\in \\mathbb{R}^4$，都有 $\\langle Dw, x \\rangle = \\langle w, D^{\\top}x \\rangle$。\n$$\n\\langle Dw, x \\rangle = \\sum_{i=1}^{4} (w_{i+1}-w_i)x_i = \\sum_{i=1}^{4} w_{i+1}x_i - \\sum_{i=1}^{4} w_i x_i\n$$\n通过重新索引并按 $w_i$ 对项进行分组，我们得到：\n$$\n\\langle Dw, x \\rangle = -w_1 x_1 + w_2(x_1-x_2) + w_3(x_2-x_3) + w_4(x_3-x_4) + w_5 x_4\n$$\n由此，我们可以确定 $D^{\\top}x$ 的分量：\n$(D^{\\top}x)_1 = -x_1$\n$(D^{\\top}x)_2 = x_1 - x_2$\n$(D^{\\top}x)_3 = x_2 - x_3$\n$(D^{\\top}x)_4 = x_3 - x_4$\n$(D^{\\top}x)_5 = x_4$\n将此应用于我们的向量 $s = (1, 1, -1, 1)^{\\top}$，我们得到次梯度 $p=D^{\\top}s$：\n$p_1 = -s_1 = -1$\n$p_2 = s_1 - s_2 = 1 - 1 = 0$\n$p_3 = s_2 - s_3 = 1 - (-1) = 2$\n$p_4 = s_3 - s_4 = -1 - 1 = -2$\n$p_5 = s_4 = 1$\n因此，次梯度为 $p = \\begin{pmatrix} -1 \\\\ 0 \\\\ 2 \\\\ -2 \\\\ 1 \\end{pmatrix}$。\n\n第四，我们计算内积 $\\langle p, u-v \\rangle$。我们首先求出向量差 $u-v$：\n$$\nu-v = \\begin{pmatrix} 2-0 \\\\ 0-2 \\\\ 4-5 \\\\ 2-1 \\\\ 1-3 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -2 \\\\ -1 \\\\ 1 \\\\ -2 \\end{pmatrix}\n$$\n现在我们计算内积：\n$$\n\\langle p, u-v \\rangle = p_1(u_1-v_1) + p_2(u_2-v_2) + p_3(u_3-v_3) + p_4(u_4-v_4) + p_5(u_5-v_5)\n$$\n$$\n\\langle p, u-v \\rangle = (-1)(2) + (0)(-2) + (2)(-1) + (-2)(1) + (1)(-2) = -2 + 0 - 2 - 2 - 2 = -8\n$$\n\n最后，我们将所有计算出的值代回 Bregman 距离公式中：\n$$\nD_{\\mathrm{TV}}^{p}(u,v) = \\mathrm{TV}(u) - \\mathrm{TV}(v) - \\langle p, u-v \\rangle = 9 - 11 - (-8)\n$$\n$$\nD_{\\mathrm{TV}}^{p}(u,v) = -2 - (-8) = -2 + 8 = 6\n$$\n得到的 Bregman 距离是一个非负值，这与它作为凸泛函的定义所预期的一致。", "answer": "$$\n\\boxed{6}\n$$", "id": "3369778"}, {"introduction": "理解了布雷格曼距离之后，让我们在一个简化的场景中观察布雷格曼迭代的实际作用。本练习将一个基本的数据同化场景建模为一个标量优化问题，从而清晰地揭示算法的内在机制。通过执行几次迭代，您将直接观察到遵循先验知识与满足新观测约束之间的权衡，并理解算法参数如何影响这种平衡 [@problem_id:3369784]。", "problem": "考虑一个一维静态数据同化问题。未知状态是一个标量 $x \\in \\mathbb{R}$。观测算子是一个标量 $H \\in \\mathbb{R}$，观测值是一个标量 $y \\in \\mathbb{R}$。状态的先验信息由一个凸二次泛函 $\\phi(x) = \\tfrac{\\alpha}{2}(x - x_b)^2$ 编码，其中 $\\alpha \\in \\mathbb{R}_{>0}$ 是先验权重，$x_b \\in \\mathbb{R}$ 是先验均值。该同化问题被构建为在等式约束 $H x = y$ 下最小化 $\\phi(x)$ 的凸优化问题。\n\n从凸分析和等式约束优化的第一性原理出发，推导两步 Bregman 迭代格式，该格式在尊重先验 $\\phi(x)$ 的同时强制执行约束 $H x = y$。从最小化受线性等式约束的凸泛函 $\\phi(x)$ 的 Bregman 迭代的基本定义开始，并使用对二阶可微凸函数有效的微分法则，来获得 $x$ 的标量更新式以及对偶变量的相应更新式。除这些基础知识外，不得使用或假设任何专门的快捷公式。将 $x^{(0)}$ 定义为 $\\phi(x)$ 的最小化子，并根据 $x^{(0)}$ 处的次梯度来一致地初始化对偶变量。\n\n对每个测试用例，从 $x^{(0)}$ 和初始对偶变量开始，数值上精确执行两次 Bregman 迭代。对于每个测试用例，在第二次迭代后报告两个量：\n- 绝对约束残差 $|H x^{(2)} - y|$，\n- 绝对先验偏差 $|x^{(2)} - x_b|$。\n\n这两个量共同说明了两次迭代后在约束满足和先验依从性之间的权衡。\n\n不涉及物理单位。报告无角度的标量值。最终的数值输出必须表示为十进制数。\n\n您的程序必须生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果。外层列表的每个元素对应一个测试用例，并且本身必须是按上述顺序排列的双元素列表。例如，输出格式必须为 $[[r_1,d_1],[r_2,d_2],\\dots]$ 的形式，其中每个 $r_i$ 和 $d_i$ 都是十进制数。将每个十进制数四舍五入到小数点后六位。\n\n使用以下涵盖不同场景的测试套件：\n- 测试用例 1 (基准): $H = 1.0$, $y = 3.0$, $x_b = 0.0$, $\\alpha = 1.0$, $\\lambda = 1.0$。\n- 测试用例 2 (强先验): $H = 1.0$, $y = 3.0$, $x_b = 0.0$, $\\alpha = 100.0$, $\\lambda = 1.0$。\n- 测试用例 3 (强约束惩罚): $H = 1.0$, $y = 3.0$, $x_b = 0.0$, $\\alpha = 1.0$, $\\lambda = 100.0$。\n- 测试用例 4 (弱敏感度): $H = 0.1$, $y = 3.0$, $x_b = 0.0$, $\\alpha = 1.0$, $\\lambda = 1.0$。\n- 测试用例 5 (先验等于观测): $H = 1.0$, $y = 2.0$, $x_b = 2.0$, $\\alpha = 1.0$, $\\lambda = 1.0$。\n\n在所有情况下，$\\lambda \\in \\mathbb{R}_{>0}$ 是 Bregman 迭代中使用的惩罚参数。您的实现必须是确定性的和自包含的；它不能读取任何输入。单行输出必须是指定格式的精确数值列表，每个数字保留六位小数。", "solution": "我们从最小化受线性等式约束的凸泛函 $\\phi(x)$ 的凸优化问题开始。其基本设置为\n$$\n\\min_{x \\in \\mathbb{R}} \\; \\phi(x) \\quad \\text{subject to} \\quad H x = y,\n$$\n其中 $\\phi(x) = \\tfrac{\\alpha}{2} (x - x_b)^2$，$\\alpha \\in \\mathbb{R}_{>0}$ 和 $x_b \\in \\mathbb{R}$ 是给定的。观测算子 $H \\in \\mathbb{R}$ 和观测值 $y \\in \\mathbb{R}$ 定义了约束条件。函数 $\\phi$ 是严格凸且二阶连续可微的，因此其梯度对所有 $x$ 都有明确定义。\n\n用于此问题的 Bregman 迭代格式通过迭代求解以下子问题来更新状态变量 $x$ 和一个与次梯度相关的对偶变量 $p$：\n$$\nx^{(k+1)} = \\arg\\min_x \\left\\{ \\phi(x) - p^{(k)}x + \\frac{\\lambda}{2}(Hx-y)^2 \\right\\}\n$$\n$$\np^{(k+1)} = p^{(k)} - \\lambda H(Hx^{(k+1)}-y)\n$$\n其中 $\\lambda \\in \\mathbb{R}_{>0}$ 是一个惩罚参数。\n\n$x$ 的子问题是无约束的二次最小化。我们通过将其对 $x$ 的导数设为零来求解：\n$$\n\\frac{d}{dx} \\left( \\frac{\\alpha}{2}(x - x_b)^2 - p^{(k)}x + \\frac{\\lambda}{2}(Hx-y)^2 \\right) = 0\n$$\n$$\n\\alpha(x - x_b) - p^{(k)} + \\lambda H(Hx-y) = 0\n$$\n重新整理各项以求解 $x$：\n$$\n(\\alpha + \\lambda H^2)x = \\alpha x_b + p^{(k)} + \\lambda H y\n$$\n这给出了 $x$ 的显式更新公式：\n$$\nx^{(k+1)} = \\frac{\\alpha x_b + p^{(k)} + \\lambda H y}{\\alpha + \\lambda H^2}\n$$\n\n算法的初始化如下：$x^{(0)}$ 被选择为先验 $\\phi(x)$ 的无约束最小化子，即 $x^{(0)} = x_b$。初始对偶变量 $p^{(0)}$ 是 $\\phi$ 在 $x^{(0)}$ 处的次梯度（即梯度），所以 $p^{(0)} = \\nabla \\phi(x^{(0)}) = \\alpha(x^{(0)}-x_b) = 0$。\n\n我们将此算法精确执行两次迭代，然后计算最终的指标。\n\n- 测试用例 1: $H = 1.0, y = 3.0, x_b = 0.0, \\alpha = 1.0, \\lambda = 1.0$。\n  $x^{(0)}=0, p^{(0)}=0$。\n  $x^{(1)} = \\frac{1(0)+0+1(1)(3)}{1+1(1^2)} = 1.5$。\n  $p^{(1)} = 0 - 1(1)(1.5-3) = 1.5$。\n  $x^{(2)} = \\frac{1(0)+1.5+1(1)(3)}{1+1(1^2)} = 2.25$。\n  指标: $|1(2.25)-3|=0.75$, $|2.25-0|=2.25$。\n\n- 测试用例 2: $H = 1.0, y = 3.0, x_b = 0.0, \\alpha = 100.0, \\lambda = 1.0$。\n  $x^{(0)}=0, p^{(0)}=0$。\n  $x^{(1)} = \\frac{100(0)+0+1(1)(3)}{100+1(1^2)} = 3/101 \\approx 0.029703$。\n  $p^{(1)} = 0 - 1(1)(3/101-3) = 300/101 \\approx 2.970297$。\n  $x^{(2)} = \\frac{100(0)+300/101+3}{101} = \\frac{603/101}{101} = 603/10201 \\approx 0.059112$。\n  指标: $|1(603/10201)-3| = |-30000/10201| \\approx 2.940888$, $|603/10201-0| \\approx 0.059112$。\n\n- 测试用例 3: $H = 1.0, y = 3.0, x_b = 0.0, \\alpha = 1.0, \\lambda = 100.0$。\n  $x^{(0)}=0, p^{(0)}=0$。\n  $x^{(1)} = \\frac{1(0)+0+100(1)(3)}{1+100(1^2)} = 300/101 \\approx 2.970297$。\n  $p^{(1)} = 0 - 100(1)(300/101-3) = 300/101 \\approx 2.970297$。\n  $x^{(2)} = \\frac{1(0)+300/101+300}{101} = \\frac{30600/101}{101} = 30600/10201 \\approx 3.000696$。\n  指标: $|1(30600/10201)-3| = |-3/10201| \\approx 0.000294$, $|30600/10201-0| \\approx 3.000696$。\n\n- 测试用例 4: $H = 0.1, y = 3.0, x_b = 0.0, \\alpha = 1.0, \\lambda = 1.0$。\n  $x^{(0)}=0, p^{(0)}=0$。\n  $x^{(1)} = \\frac{1(0)+0+1(0.1)(3)}{1+1(0.1^2)} = 0.3/1.01 \\approx 0.297030$。\n  $p^{(1)} = 0 - 1(0.1)(0.1(0.3/1.01)-3) = -0.1(-3/1.01) = 0.3/1.01 \\approx 0.297030$。\n  $x^{(2)} = \\frac{1(0)+0.3/1.01+0.3}{1.01} = \\frac{0.603/1.01}{1.01} = 0.603/1.0201 \\approx 0.591119$。\n  指标: $|0.1(0.603/1.0201)-3| = |-2.940888...| \\approx 2.940888$, $|0.603/1.0201-0| \\approx 0.591119$。\n\n- 测试用例 5: $H = 1.0, y = 2.0, x_b = 2.0, \\alpha = 1.0, \\lambda = 1.0$。\n  $x^{(0)}=2, p^{(0)}=0$。\n  $x^{(1)} = \\frac{1(2)+0+1(1)(2)}{1+1(1^2)} = 4/2 = 2$。\n  $p^{(1)} = 0 - 1(1)(2-2) = 0$。\n  $x^{(2)} = \\frac{1(2)+0+2}{2} = 2$。\n  指标: $|1(2)-2|=0.0$, $|2-2|=0.0$。\n\n这些结果证实了定性的权衡关系：更强的约束惩罚（大 $\\lambda$）或更高的敏感度（大 $H$）会迅速减小约束残差，但可能代价是在仅两步之后与先验的偏差更大；而强先验（大 $\\alpha$）会限制与 $x_b$ 的偏差，但在相同的迭代预算内会使约束满足变慢。当先验和观测一致时，两个指标都为零，迭代保持在先验均值处。\n\n该实现完全遵循推导出的标量更新式，并以指定的输出格式返回要求的小数点后六位的指标。", "answer": "```python\nimport numpy as np\n\ndef bregman_two_steps(H, y, x_b, alpha, lam):\n    \"\"\"\n    Performs two Bregman iterations for the 1D convex prior problem.\n    \"\"\"\n    # Initialization\n    x = float(x_b)\n    p = 0.0\n\n    # Perform two iterations\n    for _ in range(2):\n        # x-update\n        numerator = alpha * x_b + p + lam * H * y\n        denominator = alpha + lam * H**2\n        x = numerator / denominator\n        \n        # p-update\n        residual = H * x - y\n        p = p - lam * H * residual\n\n    # Calculate final metrics\n    final_residual = H * x - y\n    residual_abs = abs(final_residual)\n    prior_dev_abs = abs(x - x_b)\n    \n    return residual_abs, prior_dev_abs\n\ndef solve():\n    # Define the test cases: (H, y, x_b, alpha, lam)\n    test_cases = [\n        (1.0, 3.0, 0.0, 1.0, 1.0),      # Test case 1 (baseline)\n        (1.0, 3.0, 0.0, 100.0, 1.0),    # Test case 2 (strong prior)\n        (1.0, 3.0, 0.0, 1.0, 100.0),    # Test case 3 (strong constraint penalty)\n        (0.1, 3.0, 0.0, 1.0, 1.0),      # Test case 4 (weak sensitivity)\n        (1.0, 2.0, 2.0, 1.0, 1.0),      # Test case 5 (prior equals observation)\n    ]\n\n    results = []\n    for H, y, x_b, alpha, lam in test_cases:\n        res_abs, dev_abs = bregman_two_steps(H, y, x_b, alpha, lam)\n        # Round to six decimal places\n        res_abs_rounded = round(res_abs, 6)\n        dev_abs_rounded = round(dev_abs, 6)\n        results.append([f\"{res_abs_rounded:.6f}\", f\"{dev_abs_rounded:.6f}\"])\n\n    # Format the output exactly as a single line list of lists string\n    # e.g., [[r1,d1],[r2,d2]]\n    inner_strings = [f\"[{r},{d}]\" for r, d in results]\n    output_string = f\"[{','.join(inner_strings)}]\"\n    \n    print(output_string)\n\n# Execute the solution\n# Expected output: [[0.750000,2.250000],[2.940888,0.059112],[0.000294,3.000696],[2.940888,0.591119],[0.000000,0.000000]]\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3369784"}, {"introduction": "在掌握了基础知识之后，我们现在来解决一个实际且强大的应用：使用分裂布雷格曼方法进行全变分 (TV) 正则化图像去噪。这个实践任务涉及实现该算法的完整一次迭代，该算法等价于交替方向乘子法 (ADMM)。您将学习如何将一个复杂问题分解为多个更简单的子问题——一个涉及求解线性系统，另一个则是一个简单的收缩操作——这是现代大规模优化中的核心技术 [@problem_id:3369799]。", "problem": "在逆问题和数据同化的框架内，考虑一个单位正向算子的一维全变分 (TV) 降噪问题。设 $A$ 表示正向算子，并假设 $A = I$ (单位算子)。给定一个数据向量 $g \\in \\mathbb{R}^n$，我们将 Rudin–Osher–Fatemi TV 降噪目标设定为最小化数据保真项和 TV 正则化项之和，即一个泛函，其在候选信号 $u \\in \\mathbb{R}^n$ 处的值为数据失配平方与 $u$ 的离散梯度的 $\\ell_1$-范数之和，并由正则化参数进行缩放。离散梯度使用周期性边界条件下的有限差分定义。\n\n通过分量法则定义有限差分算子 $D : \\mathbb{R}^n \\to \\mathbb{R}^n$：\n$$(D u)_i = u_{(i+1) \\bmod n} - u_i \\quad \\text{对于 } i \\in \\{0,1,\\dots,n-1\\}。$$\n假设使用 Split Bregman 方法来解决这个降噪问题。在 Split Bregman 算法中，迭代状态包括主变量 $u \\in \\mathbb{R}^n$、强制 $d \\approx D u$ 的分裂变量 $d \\in \\mathbb{R}^n$ 以及 Bregman 变量 $b \\in \\mathbb{R}^n$。迭代使用一个惩罚参数 $\\lambda > 0$ 和一个 TV 正则化参数 $\\mu > 0$。\n\n从初始状态 $u^{0} = 0$, $d^{0} = 0$, 和 $b^{0} = 0$ (即 $\\mathbb{R}^n$ 中的零向量) 开始，实现一次完整的 Split Bregman 迭代。使用基于增广拉格朗日观点的原理推导来计算第一次迭代中 $u$ 的更新和 $d$ 的更新。离散算子必须严格按照规定实现，所有计算必须以双精度浮点算术进行。\n\n您的程序必须为每个指定的测试用例生成由单次 Split Bregman 迭代产生的向量 $u^{1}$ 和 $d^{1}$。在程序输出中，将向量表示为浮点数列表。\n\n使用以下测试套件，它涵盖了典型行为、近常数数据、小问题规模和高惩罚机制：\n\n- 测试用例 1: $n = 6$, $g = [\\,1.0,\\,1.2,\\,0.9,\\,-0.5,\\,-0.4,\\,0.0\\,]$, $\\mu = 0.6$, $\\lambda = 2.0$.\n- 测试用例 2: $n = 6$, $g = [\\,2.0,\\,2.0,\\,2.0,\\,2.0,\\,2.0,\\,2.0\\,]$, $\\mu = 1.5$, $\\lambda = 1.0$.\n- 测试用例 3: $n = 3$, $g = [\\,0.0,\\,10.0,\\,-10.0\\,]$, $\\mu = 0.4$, $\\lambda = 0.5$.\n- 测试用例 4: $n = 8$, $g = [\\,0.0,\\,0.5,\\,1.0,\\,1.5,\\,1.25,\\,0.8,\\,0.3,\\,-0.2\\,]$, $\\mu = 2.0$, $\\lambda = 3.0$.\n\n对于每个测试用例，使用上面定义的周期性边界条件下的有限差分算子 $D$ 进行一次完整的 Split Bregman 迭代，计算 $u^{1}$ 和 $d^{1}$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果。该列表的第 $i$ 个元素本身必须是一个包含两个列表的列表，其中第一个内部列表是第 $i$ 个测试用例的 $u^{1}$，第二个内部列表是 $d^{1}$。例如，输出格式应为\n$$[\\, [\\,[u^{1}_{(1)}],\\,[d^{1}_{(1)}]\\,],\\,[\\,[u^{1}_{(2)}],\\,[d^{1}_{(2)}]\\,],\\,\\dots\\, ]$$\n打印输出中不含空格。", "solution": "该问题是有效且适定的。它描述了 Split Bregman 算法在全变分 (TV) 降噪问题中的一个标准应用，提供了所有必要的定义、参数和初始条件。\n\n一维 Rudin–Osher–Fatemi (ROF) TV 降噪问题旨在寻找一个信号 $u \\in \\mathbb{R}^n$，该信号对于给定的含噪信号 $g \\in \\mathbb{R}^n$ 最小化以下目标泛函：\n$$ E(u) = \\frac{1}{2} \\|u-g\\|_2^2 + \\mu \\|Du\\|_1 $$\n这里，$\\mu > 0$ 是控制降噪强度的正则化参数。算子 $D: \\mathbb{R}^n \\to \\mathbb{R}^n$ 是离散梯度，在周期性边界条件下定义为 $(Du)_i = u_{(i+1) \\bmod n} - u_i$。\n\n$\\ell_1$-范数项 $\\|Du\\|_1$ 是不可微的，这使得直接最小化变得困难。Split Bregman 方法通过引入一个分裂变量 $d \\in \\mathbb{R}^n$ 来解决这个问题，并将问题转化为一个约束优化问题：\n$$ \\min_{u,d} \\frac{1}{2} \\|u-g\\|_2^2 + \\mu \\|d\\|_1 \\quad \\text{满足} \\quad d = Du $$\n\n这个约束问题通过最小化一个相关的增广拉格朗日量来解决，从而得到一个迭代格式。Split Bregman 迭代以交替方式更新主变量 $u$、分裂变量 $d$ 和 Bregman (对偶) 变量 $b \\in \\mathbb{R}^n$。对于迭代 $k+1$，迭代格式由以下步骤给出：\n1.  更新 $u$:\n    $$ u^{k+1} = \\arg\\min_u \\left\\{ \\frac{1}{2} \\|u-g\\|_2^2 + \\frac{\\lambda}{2} \\|Du - d^k + b^k\\|_2^2 \\right\\} $$\n2.  更新 $d$:\n    $$ d^{k+1} = \\arg\\min_d \\left\\{ \\mu \\|d\\|_1 + \\frac{\\lambda}{2} \\|Du^{k+1} - d + b^k\\|_2^2 \\right\\} $$\n3.  更新 $b$:\n    $$ b^{k+1} = b^k + (Du^{k+1} - d^{k+1}) $$\n参数 $\\lambda > 0$ 是对约束 $d=Du$ 的一个惩罚参数。\n\n我们的任务是计算一次完整的迭代，得到 $u^1$ 和 $d^1$，从初始状态 $u^0 = 0$、$d^0 = 0$ 和 $b^0 = 0$ 开始。\n\n**步骤 1：$u^1$ 的 $u$-子问题**\n\n我们将 $k=0$ 以及初始条件 $d^0=0$ 和 $b^0=0$ 代入 $u$ 的更新规则中：\n$$ u^1 = \\arg\\min_u \\left\\{ \\frac{1}{2} \\|u-g\\|_2^2 + \\frac{\\lambda}{2} \\|Du - 0 + 0\\|_2^2 \\right\\} = \\arg\\min_u \\left\\{ \\frac{1}{2} \\|u-g\\|_2^2 + \\frac{\\lambda}{2} \\|Du\\|_2^2 \\right\\} $$\n这是一个二次最小化问题，可以通过将目标函数关于 $u$ 的梯度设为零来求解。$\\frac{1}{2}\\|u-g\\|_2^2$ 的梯度是 $u-g$。$\\frac{\\lambda}{2}\\|Du\\|_2^2 = \\frac{\\lambda}{2} u^T D^T D u$ 的梯度是 $\\lambda D^T D u$。将梯度之和设为零得到正规方程：\n$$ (u^1 - g) + \\lambda D^T D u^1 = 0 $$\n$$ (I + \\lambda D^T D) u^1 = g $$\n其中 $I$ 是 $n \\times n$ 单位矩阵，$D^T$ 是 $D$ 的转置。矩阵 $C = I + \\lambda D^T D$ 是循环矩阵，因为 $D$ (带周期性边界的前向差分) 是一个循环矩阵。循环线性系统可以在傅里叶域中高效求解。设 $\\mathcal{F}$ 表示离散傅里叶变换 (DFT)。对系统应用 DFT 得到：\n$$ \\mathcal{F}((I + \\lambda D^T D) u^1) = \\mathcal{F}(g) $$\n$$ \\mathcal{F}(I + \\lambda D^T D) \\mathcal{F}(u^1) = \\mathcal{F}(g) $$\n项 $\\mathcal{F}(I + \\lambda D^T D)$ 代表循环矩阵 $C$ 的特征值。这些特征值可以通过对 $C$ 的第一行进行 DFT 来找到。算子 $D^T D$ 是带周期性边界的一维离散拉普拉斯算子的负数，即 $(D^T D u)_i = -u_{(i+1) \\bmod n} + 2u_i - u_{(i-1) \\bmod n}$。因此，$C$ 的第一行是 $c = [1+2\\lambda, -\\lambda, 0, \\dots, 0, -\\lambda]$。\n然后 $u^1$ 的 DFT 为：\n$$ \\mathcal{F}(u^1)_k = \\frac{\\mathcal{F}(g)_k}{ \\mathcal{F}(c)_k } $$\n最后，我们通过应用逆 DFT 得到 $u^1$：\n$$ u^1 = \\mathcal{F}^{-1}\\left( \\frac{\\mathcal{F}(g)}{\\mathcal{F}(c)} \\right) $$\n\n**步骤 2：$d^1$ 的 $d$-子问题**\n\n接下来，我们使用新计算出的 $u^1$ 和之前的 $b^0=0$ 来计算 $d^1$：\n$$ d^1 = \\arg\\min_d \\left\\{ \\mu \\|d\\|_1 + \\frac{\\lambda}{2} \\|Du^1 - d + 0\\|_2^2 \\right\\} = \\arg\\min_d \\left\\{ \\mu \\|d\\|_1 + \\frac{\\lambda}{2} \\|d - Du^1\\|_2^2 \\right\\} $$\n这个问题是可分离的，意味着我们可以独立求解每个分量 $d_i$：\n$$ d^1_i = \\arg\\min_{d_i} \\left\\{ \\mu |d_i| + \\frac{\\lambda}{2} (d_i - (Du^1)_i)^2 \\right\\} $$\n这是凸优化中的一个标准问题，其解由软阈值算子给出。设 $v_i = (Du^1)_i$，阈值为 $T = \\mu/\\lambda$。解为：\n$$ d^1_i = \\text{shrink}(v_i, T) = \\text{sign}(v_i) \\max(|v_i| - T, 0) $$\n对所有分量 $i=0, \\dots, n-1$ 计算此式，即可得到向量 $d^1$。\n\n**一次迭代的算法总结**\n给定 $n$、$g$、$\\mu$ 和 $\\lambda$：\n1.  构造循环矩阵 $C = I + \\lambda D^T D$ 的第一行：$c = [1+2\\lambda, -\\lambda, 0, \\dots, 0, -\\lambda]$。\n2.  计算 $c$ 的 DFT 以获得 $C$ 的特征值，记为 $\\hat{c} = \\mathcal{F}(c)$。\n3.  计算数据向量 $g$ 的 DFT，记为 $\\hat{g} = \\mathcal{F}(g)$。\n4.  通过逐元素相除计算解 $u^1$ 的 DFT：$\\hat{u}^1_k = \\hat{g}_k / \\hat{c}_k$。\n5.  通过对 $\\hat{u}^1$ 进行逆 DFT 计算 $u^1$：$u^1 = \\mathcal{F}^{-1}(\\hat{u}^1)$。\n6.  计算 $u^1$ 的离散梯度：$v = Du^1$。\n7.  对 $v$ 应用阈值为 $T=\\mu/\\lambda$ 的软阈值算子以获得 $d^1$。\n所有计算都使用双精度浮点算术进行。", "answer": "```python\nimport numpy as np\nimport json\n\ndef run_split_bregman_iteration(n: int, g: np.ndarray, mu: float, lambda_param: float) -> tuple[list[float], list[float]]:\n    \"\"\"\n    Computes one full Split Bregman iteration for 1D TV denoising.\n\n    Args:\n        n (int): The size of the signal.\n        g (np.ndarray): The observed (noisy) signal vector.\n        mu (float): The TV regularization parameter.\n        lambda_param (float): The Bregman penalty parameter.\n\n    Returns:\n        tuple[list[float], list[float]]: A tuple containing the updated vectors u^1 and d^1 as lists.\n    \"\"\"\n    # Step 1: Compute u^1 by solving (I + lambda * D^T D)u = g\n    # This is a circulant system, solved efficiently via FFT.\n\n    # First, construct the first row of the circulant matrix C = I + lambda * D^T * D\n    c_first_row = np.zeros(n, dtype=np.float64)\n    c_first_row[0] = 1.0 + 2.0 * lambda_param\n    c_first_row[1] = -lambda_param\n    c_first_row[n-1] = -lambda_param\n\n    # Eigenvalues of C are the FFT of its first row.\n    c_eig = np.fft.fft(c_first_row)\n\n    # FFT of the data vector g\n    g_hat = np.fft.fft(g)\n\n    # Solve for u^1 in the Fourier domain\n    u1_hat = g_hat / c_eig\n\n    # Inverse FFT to get u^1 in the spatial domain.\n    # The result should be real; take the real part to discard negligible imaginary components due to floating point error.\n    u1 = np.fft.ifft(u1_hat).real\n\n    # Step 2: Compute d^1 using the soft-shrinkage operator\n    # d^1 = shrink(Du^1, mu/lambda)\n\n    # Compute the discrete gradient Du^1 with periodic boundary conditions\n    # (Du)_i = u_{i+1} - u_i\n    v = np.roll(u1, -1) - u1\n\n    # Apply the soft-shrinkage operator\n    threshold = mu / lambda_param\n    d1 = np.sign(v) * np.maximum(np.abs(v) - threshold, 0.0)\n\n    return u1.tolist(), d1.tolist()\n\ndef solve():\n    \"\"\"\n    Runs the Split Bregman iteration for the test cases specified in the problem and prints the results.\n    \"\"\"\n    test_cases = [\n        # (n, g, mu, lambda)\n        (6, [1.0, 1.2, 0.9, -0.5, -0.4, 0.0], 0.6, 2.0),\n        (6, [2.0, 2.0, 2.0, 2.0, 2.0, 2.0], 1.5, 1.0),\n        (3, [0.0, 10.0, -10.0], 0.4, 0.5),\n        (8, [0.0, 0.5, 1.0, 1.5, 1.25, 0.8, 0.3, -0.2], 2.0, 3.0),\n    ]\n\n    results = []\n    for n, g_list, mu, lambda_param in test_cases:\n        g_np = np.array(g_list, dtype=np.float64)\n        u1, d1 = run_split_bregman_iteration(n, g_np, mu, lambda_param)\n        results.append([u1, d1])\n\n    # The output format requires a json-like string with no spaces.\n    # json.dumps with custom separators is the most reliable way to achieve this.\n    print(json.dumps(results, separators=(',', ':')))\n\nsolve()\n```", "id": "3369799"}]}