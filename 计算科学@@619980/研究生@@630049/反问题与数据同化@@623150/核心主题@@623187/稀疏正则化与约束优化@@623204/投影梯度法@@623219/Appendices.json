{"hands_on_practices": [{"introduction": "投影梯度法的核心在于其投影步骤，即将无约束的梯度更新映射回可行集。对于如概率单纯形这样的重要约束集，设计一个高效的投影算子至关重要。本练习 [@problem_id:3414855] 将指导你从卡鲁什-库恩-塔克 (KKT) 最优性条件出发，推导并实现一个用于计算到概率单纯形上的欧氏投影的高效算法，从而让你深入理解约束优化中的基本原理。", "problem": "考虑在闭凸集 $C \\subset \\mathbb{R}^n$ 上的欧几里得投影，对于任意 $x \\in \\mathbb{R}^n$，该投影定义为 $P_C(x) = \\arg\\min_{z \\in C} \\frac{1}{2}\\|z - x\\|_2^2$。在许多反问题和数据同化应用中，决策变量表示 $n$ 个状态上的概率分布，并且必须位于概率单纯形 $C = \\Delta = \\{z \\in \\mathbb{R}^n : z_i \\ge 0,\\ \\sum_{i=1}^n z_i = 1\\}$ 中。投影 $P_\\Delta(x)$ 在投影梯度法中用于在梯度下降步骤后强制满足可行性。从凸优化的原理和带约束的二次最小化的最优性条件出发，推导一个能在 $O(n\\log n)$ 时间内计算任意 $x \\in \\mathbb{R}^n$ 的 $P_\\Delta(x)$ 的算法。该推导必须从 Karush–Kuhn–Tucker (KKT) 最优性条件开始，并通过证明计算出的点满足最优性的充要条件来论证算法的正确性。\n\n你的程序必须实现所得到的 $O(n\\log n)$ 算法，并将其应用于以下 $x \\in \\mathbb{R}^n$ 的输入测试套件：\n- 测试用例 1：$x = [0.2, -0.1, 0.4, 2.0, 0.3]$。\n- 测试用例 2：$x = [0.1, 0.2, 0.3, 0.4, 0.0]$。\n- 测试用例 3：$x = [-1.0, -2.0, -3.0]$。\n- 测试用例 4：$x = [5.0]$。\n- 测试用例 5：$x = [0.5, 0.5, 0.5, 0.5]$。\n- 测试用例 6：$x = [0.0, -0.5, 0.0, 0.7, 0.8]$。\n\n对于每个测试用例，使用你的 $O(n\\log n)$ 算法计算投影向量 $P_\\Delta(x)$。每个结果必须是一个浮点数列表。你的程序应生成单行输出，其中包含用方括号括起来的逗号分隔的结果列表（例如，$[result1,result2,result3]$，其中每个 $resultk$ 是第 $k$ 个测试用例的投影向量）。此问题不涉及物理单位或角度单位。输出是投影向量本身，可量化为浮点数列表。该测试套件涵盖了一般情况、已是可行点的情况、所有分量为负的情况、标量边界情况、相同正分量的情况，以及包含零和负数的混合情况，从而测试其正确性、可行性和边界条件。", "solution": "问题要求推导并实现一个高效算法，用以计算向量 $x \\in \\mathbb{R}^n$ 在概率单纯形 $\\Delta$ 上的欧几里得投影。概率单纯形是集合 $\\Delta = \\{z \\in \\mathbb{R}^n : \\sum_{i=1}^n z_i = 1, z_i \\ge 0 \\text{ for all } i\\}$。投影 $P_\\Delta(x)$ 是以下带约束的二次优化问题的唯一解 $z^*$：\n$$\n\\text{minimize} \\quad f(z) = \\frac{1}{2}\\|z - x\\|_2^2 = \\frac{1}{2}\\sum_{i=1}^n (z_i - x_i)^2\n$$\n$$\n\\text{subject to} \\quad \\sum_{i=1}^n z_i = 1 \\quad \\text{and} \\quad z_i \\ge 0 \\quad \\text{for } i=1, \\dots, n.\n$$\n这是一个凸优化问题，因为目标函数是严格凸的，并且可行集 $\\Delta$ 是凸的。因此，Karush-Kuhn-Tucker (KKT) 条件是确定最优性的充要条件。我们首先构造该问题的拉格朗日函数。\n\n拉格朗日函数 $L(z, \\lambda, \\nu)$ 由下式给出：\n$$\nL(z, \\lambda, \\nu) = \\frac{1}{2}\\sum_{i=1}^n (z_i - x_i)^2 - \\sum_{i=1}^n \\lambda_i z_i - \\nu \\left( \\sum_{i=1}^n z_i - 1 \\right)\n$$\n其中 $\\lambda_i \\ge 0$ 是非负约束 $z_i \\ge 0$ 的拉格朗日乘子，$\\nu$ 是等式约束 $\\sum_{i=1}^n z_i = 1$ 的拉格朗日乘子。最优点 $z^*$ 的 KKT 条件如下：\n\n1.  **平稳性 (Stationarity)**：拉格朗日函数关于 $z$ 的梯度在 $z^*$ 处必须为零：\n    $$\n    \\frac{\\partial L}{\\partial z_i}\\bigg|_{z=z^*} = (z_i^* - x_i) - \\lambda_i - \\nu = 0 \\quad \\implies \\quad z_i^* = x_i + \\lambda_i + \\nu\n    $$\n\n2.  **原始可行性 (Primal Feasibility)**：解 $z^*$ 必须位于可行集 $\\Delta$ 中：\n    $$\n    \\sum_{i=1}^n z_i^* = 1 \\quad \\text{and} \\quad z_i^* \\ge 0 \\quad \\text{for all } i\n    $$\n\n3.  **对偶可行性 (Dual Feasibility)**：不等式约束的乘子必须为非负：\n    $$\n    \\lambda_i \\ge 0 \\quad \\text{for all } i\n    $$\n\n4.  **互补松弛性 (Complementary Slackness)**：\n    $$\n    \\lambda_i z_i^* = 0 \\quad \\text{for all } i\n    $$\n\n根据互补松弛性条件，对于每个分量 $i$，要么 $\\lambda_i = 0$，要么 $z_i^* = 0$。\n-   如果 $z_i^* > 0$，互补松弛性意味着 $\\lambda_i = 0$。此时平稳性条件简化为 $z_i^* = x_i + \\nu$。由于 $z_i^* > 0$，我们有 $x_i+\\nu > 0$。\n-   如果 $z_i^* = 0$，则 $\\lambda_i \\ge 0$。平稳性条件给出 $0 = x_i + \\lambda_i + \\nu$，即 $\\lambda_i = -x_i - \\nu$。对偶可行性条件 $\\lambda_i \\ge 0$ 意味着 $-x_i - \\nu \\ge 0$，即 $x_i + \\nu \\le 0$。\n\n让我们引入一个单一的阈值参数 $\\theta = -\\nu$。这样 $z_i^*$ 的条件可以被统一：\n-   如果 $x_i - \\theta > 0$，则 $z_i^* > 0$ 且 $\\lambda_i=0$，因此 $z_i^* = x_i - \\theta$。\n-   如果 $x_i - \\theta \\le 0$，则 $z_i^* = 0$。\n\n这可以对所有 $i$ 简洁地表示为：\n$$\nz_i^* = \\max(0, x_i - \\theta) = (x_i - \\theta)_+\n$$\n问题现在简化为找到标量阈值 $\\theta$ 的正确值。我们可以通过强制满足原始可行性约束 $\\sum_{i=1}^n z_i^* = 1$ 来确定 $\\theta$：\n$$\n\\sum_{i=1}^n \\max(0, x_i - \\theta) = 1\n$$\n令 $g(\\theta) = \\sum_{i=1}^n \\max(0, x_i - \\theta)$。该函数是连续的、分段线性的且单调不增。我们需要找到方程 $g(\\theta) = 1$ 的根 $\\theta$。直接求解析解很困难，但我们可以根据 $g(\\theta)$ 的性质设计一个高效的算法。\n\n考虑将 $x$ 的分量按降序排序：$u_1 \\ge u_2 \\ge \\dots \\ge u_n$。随着 $\\theta$ 的减小，项 $\\max(0, u_i - \\theta)$ 会按此顺序变为非零。令 $\\rho$ 为最终解 $z^*$ 中正分量的数量。这些将对应于 $x$ 的 $\\rho$ 个最大分量。因此，对于对应于 $u_1, \\dots, u_\\rho$ 的索引 $i$，我们有 $z_i^* > 0$，而对于对应于 $u_{\\rho+1}, \\dots, u_n$ 的索引，我们有 $z_i^* = 0$。这意味着阈值 $\\theta$ 必须满足 $u_\\rho - \\theta > 0$ 和 $u_{\\rho+1} - \\theta \\le 0$，即 $u_{\\rho+1} \\le \\theta  u_\\rho$。\n\n对于这个假设的 $\\rho$，和约束变为：\n$$\n\\sum_{j=1}^\\rho (u_j - \\theta) + \\sum_{j=\\rho+1}^n 0 = 1\n$$\n$$\n\\left(\\sum_{j=1}^\\rho u_j\\right) - \\rho\\theta = 1\n$$\n求解 $\\theta$ 可为每个可能的 $\\rho$ 产生一个候选阈值：\n$$\n\\theta_\\rho = \\frac{1}{\\rho}\\left(\\sum_{j=1}^\\rho u_j - 1\\right)\n$$\n正确的 $\\rho$ 是使该 $\\theta_\\rho$ 与假设 $u_{\\rho+1} \\le \\theta_\\rho  u_\\rho$ 一致的那个。我们只需要找到满足条件 $u_\\rho > \\theta_\\rho$ 的最大索引 $\\rho \\in \\{1, \\dots, n\\}$。令该索引为 $\\rho^*$。\n条件 $u_\\rho > \\theta_\\rho$ 等价于 $\\rho u_\\rho > \\sum_{j=1}^\\rho u_j - 1$。\n让我们证明，如果 $\\rho^*$ 是满足此条件的最大索引，那么第二个条件 $u_{\\rho^*+1} \\le \\theta_{\\rho^*}$ 也成立。使用反证法，假设 $u_{\\rho^*+1} > \\theta_{\\rho^*}$。这意味着 $(\\rho^*+1) u_{\\rho^*+1} > \\rho^* u_{\\rho^*+1} > \\rho^* \\theta_{\\rho^*}$。同时，$(\\sum_{j=1}^{\\rho^*} u_j - 1) + u_{\\rho^*+1} = \\rho^*\\theta_{\\rho^*} + u_{\\rho^*+1}$。\n考虑 $\\rho^*+1$ 的条件：\n$$(\\rho^*+1)u_{\\rho^*+1} > \\sum_{j=1}^{\\rho^*+1} u_j - 1 = \\left(\\sum_{j=1}^{\\rho^*} u_j - 1\\right) + u_{\\rho^*+1} = \\rho^*\\theta_{\\rho^*} + u_{\\rho^*+1}$$\n这简化为 $\\rho^* u_{\\rho^*+1} > \\rho^* \\theta_{\\rho^*}$，即 $u_{\\rho^*+1} > \\theta_{\\rho^*}$，这正是我们的假设。这意味着如果 $u_{\\rho^*+1} > \\theta_{\\rho^*}$，那么 $\\rho^*+1$ 也会满足测试条件，这与 $\\rho^*$ 是满足该条件的最大索引相矛盾。因此，必然有 $u_{\\rho^*+1} \\le \\theta_{\\rho^*}$。\n\n这引出了以下 $O(n\\log n)$ 算法：\n1.  将输入向量 $x$ 按降序排序得到向量 $u$。此步骤耗时 $O(n\\log n)$。\n2.  找到满足 $u_j > \\frac{1}{j}(\\sum_{i=1}^j u_i - 1)$ 的最大索引 $j \\in \\{1, \\dots, n\\}$，该值即为 $\\rho$。这可以通过从 $j=1$ 到 $n$ 迭代并维护 $u_i$ 的一个累加和，在 $O(n)$ 时间内完成。\n3.  利用这个 $\\rho$，计算最终的阈值 $\\theta = \\frac{1}{\\rho}(\\sum_{i=1}^\\rho u_i - 1)$。这需要用到上一步计算出的和，耗时 $O(1)$。\n4.  使用原始向量 $x$ 和阈值 $\\theta$ 计算投影 $z^*$：$z_i^* = \\max(0, x_i - \\theta)$，对 $i=1, \\dots, n$。此步骤耗时 $O(n)$。\n\n主要步骤是初始排序，因此总时间复杂度为 $O(n\\log n)$。通过这种构造方式，可以保证所得向量 $z^*$ 满足 KKT 条件，因此是唯一的.​​最优解。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    It computes the projection of several vectors onto the probability simplex\n    and prints the results in the specified format.\n    \"\"\"\n\n    def project_to_simplex(x: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Computes the Euclidean projection of a vector x onto the probability simplex.\n\n        The algorithm takes O(n log n) time due to the sorting step. The\n        derivation is based on the Karush-Kuhn-Tucker (KKT) conditions for\n        the constrained quadratic optimization problem.\n\n        Args:\n            x: A numpy array representing the vector to be projected.\n\n        Returns:\n            A numpy array representing the projected vector.\n        \"\"\"\n        n = x.shape[0]\n        \n        # If the vector is already in the simplex, return it.\n        # This is an optional optimization, the main algorithm handles this case correctly.\n        if np.sum(x) == 1 and np.all(x >= 0):\n            return x\n\n        # Sort the vector x in descending order.\n        u = np.sort(x)[::-1]\n\n        # Compute the cumulative sum of the sorted vector.\n        cssv = np.cumsum(u)\n        \n        # Find the largest rho such that u_rho > (1/rho) * (sum_{i=1}^{rho} u_i - 1).\n        # This is done in a vectorized way for efficiency.\n        # The equation is rearranged to avoid division inside the loop:\n        # rho * u_rho > sum_{i=1}^{rho} u_i - 1\n        indices = np.arange(1, n + 1)\n        condition = u * indices > cssv - 1\n        \n        # The last index where the condition is true gives us the correct rho.\n        # np.where returns a tuple of arrays, we need the first one.\n        # The set of indices satisfying the condition is never empty for n>=1.\n        rho_idx = np.where(condition)[0][-1]\n        rho = rho_idx + 1\n\n        # Compute the threshold theta using the found rho.\n        theta = (cssv[rho_idx] - 1) / rho\n\n        # Compute the projection by applying the threshold.\n        # z_i = max(x_i - theta, 0)\n        z = np.maximum(x - theta, 0)\n\n        return z\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        np.array([0.2, -0.1, 0.4, 2.0, 0.3]),\n        np.array([0.1, 0.2, 0.3, 0.4, 0.0]),\n        np.array([-1.0, -2.0, -3.0]),\n        np.array([5.0]),\n        np.array([0.5, 0.5, 0.5, 0.5]),\n        np.array([0.0, -0.5, 0.0, 0.7, 0.8]),\n    ]\n\n    results = []\n    for case in test_cases:\n        projected_vector = project_to_simplex(case)\n        # Convert the result to a list of floats for correct string formatting.\n        results.append(str(projected_vector.tolist()))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3414855"}, {"introduction": "在掌握了投影算子的实现后，保证投影梯度法收敛的关键在于选择一个合适的步长 $\\alpha$。步长的选择直接影响算法的稳定性和收敛速度，而这通常依赖于目标函数梯度的利普希茨常数 $L$。本练习 [@problem_id:3414873] 将引导你通过理论推导和数值方法（幂迭代法）来估计 $L$ 的上下界，并利用这些界来选择一个确保算法稳定收敛的“安全”步长，这是解决实际问题中的一项核心技能。", "problem": "考虑在线性反问题和数据同化中出现的凸二次规划问题：在箱式约束下最小化Tikhonov正则化的最小二乘目标，\n$$\n\\min_{x \\in \\mathbb{R}^n} \\; J(x) = \\frac{1}{2}\\,\\|A x - b\\|_2^2 + \\frac{\\gamma}{2}\\,\\|x\\|_2^2 \\quad \\text{subject to} \\quad \\ell \\le x \\le u,\n$$\n其中 $A \\in \\mathbb{R}^{m \\times n}$，$b \\in \\mathbb{R}^m$，$\\gamma \\ge 0$，且不等式是分量级的，$\\ell, u \\in \\mathbb{R}^n$ 且对所有 $i$ 都有 $\\ell_i \\le u_i$。$J$ 的梯度是Lipschitz连续的，常数为 $L = \\|Q\\|_2$，其中 $Q = A^\\top A + \\gamma I_n$ 且 $\\|\\cdot\\|_2$ 表示谱范数。投影梯度法迭代如下\n$$\nx^{k+1} = \\Pi_{[\\ell,u]}\\Big(x^k - \\alpha \\,\\nabla J(x^k)\\Big),\n$$\n其中 $\\Pi_{[\\ell,u]}$ 是到箱体 $[\\ell,u]$ 上的欧几里得投影，$\\alpha  0$ 是步长。对于凸目标上的投影梯度法，一个标准全局收敛保证要求选择 $\\alpha \\in (0, 2/L)$，而一个保证单调下降的选择是 $\\alpha \\in (0, 1/L]$。\n\n任务：\n- 从谱范数和次乘法矩阵范数的定义出发，推导一个关于 $L = \\|Q\\|_2$ 的可计算上界 $\\overline{L}$，该上界不要求计算 $Q$ 的特征值。您的推导必须只使用范数和线性算子的基本性质，并且不得假定超出这些性质的公式。您可以假设能够调用一个计算 $Q$ 的矩阵-向量乘积的例程，但不一定能直接访问 $Q$ 本身的条目。\n- 使用幂迭代法作用于 $Q$，推导一个关于 $L$ 的可计算数值下界 $\\underline{L}$，该下界随着迭代次数的增加而收敛到 $L$。您的推导必须从对称半正定矩阵谱范数的变分刻画和瑞利商的定义开始，并且不得先验地假定任何特定的收敛率。\n- 基于以上内容，解释如何使用一个可证明的上界 $\\overline{L}$ 为投影梯度法选择一个严格安全的步长 $\\alpha$，以及如何在算法设计期间使用来自幂迭代的 $\\underline{L}$ 来评估 $\\overline{L}$ 的保守性。\n\n然后，实现一个程序，该程序：\n- 对每个测试用例，仅使用允许的操作（矩阵-向量乘积和基本矩阵范数）计算一个严格上界 $\\overline{L}$ 和一个数值下界 $\\underline{L}$。\n- 使用 $\\alpha = 1/\\overline{L}$ 运行投影梯度法直到收敛，使用停止准则 $\\|x^{k+1} - x^k\\|_2 \\le \\varepsilon$ 或最大迭代次数 $K_{\\max}$，以先发生者为准。\n- 为每个测试用例报告三个浮点数：下界 $\\underline{L}$、上界 $\\overline{L}$ 以及在返回点 $x^{\\star}$ 处的最终目标值 $J(x^{\\star})$。\n\n您必须使用的基本原理：\n- 对称半正定矩阵 $Q$ 的变分刻画：$L = \\|Q\\|_2 = \\max_{\\|v\\|_2=1} v^\\top Q v$。\n- 对于 $v \\ne 0$ 的瑞利商 $R_Q(v) = \\frac{v^\\top Q v}{v^\\top v}$。\n- 相容矩阵范数的次乘法性和范数控制关系，包括 $\\|M\\|_2 \\le \\sqrt{\\|M\\|_1 \\|M\\|_\\infty}$ 和 $\\|X+Y\\| \\le \\|X\\| + \\|Y\\|$（对于任何相容的矩阵范数 $\\|\\cdot\\|$）。\n- 欧几里得投影到闭凸集上的非扩张性以及投影梯度法在凸目标上的基本性质。\n\n测试套件：\n- 案例 1：\n  - $A = \\begin{bmatrix} 3  1  0 \\\\ 1  4  1 \\\\ 0  1  2 \\end{bmatrix}$, $b = \\begin{bmatrix} 1 \\\\ 0 \\\\ -1 \\end{bmatrix}$, $\\gamma = 0.2$。\n  - $\\ell = \\begin{bmatrix} -0.5 \\\\ -0.5 \\\\ -0.5 \\end{bmatrix}$, $u = \\begin{bmatrix} 2 \\\\ 2 \\\\ 2 \\end{bmatrix}$, $x^0 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$。\n- 案例 2：\n  - $A = \\begin{bmatrix} 1  2  3 \\\\ 2  4  6 \\end{bmatrix}$, $b = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}$, $\\gamma = 0$。\n  - $\\ell = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$, $u = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$, $x^0 = \\begin{bmatrix} 0.3 \\\\ 0.3 \\\\ 0.3 \\end{bmatrix}$。\n- 案例 3：\n  - $A = \\begin{bmatrix} 2  0  -1 \\\\ 0  1  1 \\\\ 1  -1  0 \\\\ 0  2  3 \\end{bmatrix}$, $b = \\begin{bmatrix} 1 \\\\ -1 \\\\ 0 \\\\ 2 \\end{bmatrix}$, $\\gamma = 3.0$。\n  - $\\ell = \\begin{bmatrix} -0.1 \\\\ -0.1 \\\\ -0.1 \\end{bmatrix}$, $u = \\begin{bmatrix} 0.1 \\\\ 0.1 \\\\ 0.1 \\end{bmatrix}$, $x^0 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$。\n\n在所有案例中使用的算法参数：\n- 幂迭代最大迭代次数 $K_{\\mathrm{pow}} = 200$，容差 $\\varepsilon_{\\mathrm{pow}} = 10^{-10}$。\n- 投影梯度最大迭代次数 $K_{\\max} = 20000$，容差 $\\varepsilon = 10^{-9}$。\n- 对于上界，仅使用可证明的有效不等式。例如，您可以使用 $\\|Q\\|_2 \\le \\sqrt{\\|Q\\|_1 \\|Q\\|_\\infty}$ 以及 $Q = A^\\top A + \\gamma I_n$ 的结构来推断 $\\|Q\\|_2 \\le \\|A\\|_2^2 + \\gamma \\le \\|A\\|_1 \\|A\\|_\\infty + \\gamma$。您必须选择一个严格的 $\\overline{L}$，并可以取多个严格上界的最小值以降低保守性。\n\n编程和输出要求：\n- 您的程序必须为每个测试用例计算通过对 $Q$ 进行幂迭代得到的最终瑞利商作为数值下界 $\\underline{L}$，按前述方法选择的严格上界 $\\overline{L}$，以及在使用步长 $\\alpha = 1/\\overline{L}$ 运行投影梯度法后的最终目标值 $J(x^\\star)$。\n- 您的程序应生成一行输出，其中包含用方括号括起来的逗号分隔结果列表。该列表必须按顺序包含案例1、案例2和案例3的三元组 $\\left(\\underline{L}, \\overline{L}, J(x^\\star)\\right)$，并展平为一个单一列表。每个浮点数必须四舍五入到6位小数。例如，输出格式必须严格为\n$[\\underline{L}_1,\\overline{L}_1,J_1,\\underline{L}_2,\\overline{L}_2,J_2,\\underline{L}_3,\\overline{L}_3,J_3]$。", "solution": "这个问题是适定的，并且有科学依据。它提出了一个在线性反问题和数据同化中出现的标准凸优化问题，即在箱式约束下最小化Tikhonov正则化的最小二乘泛函。所有数据、参数和目标都已明确定义，构成了一个在数值优化和科学计算领域的有效问题。我们着手进行求解。\n\n待最小化的目标函数是\n$$\nJ(x) = \\frac{1}{2}\\,\\|A x - b\\|_2^2 + \\frac{\\gamma}{2}\\,\\|x\\|_2^2\n$$\n受分量级约束 $\\ell \\le x \\le u$ 的限制。$J(x)$ 的梯度是\n$$\n\\nabla J(x) = A^\\top(Ax - b) + \\gamma x = (A^\\top A + \\gamma I_n)x - A^\\top b.\n$$\n设Hessian矩阵为 $Q = A^\\top A + \\gamma I_n$。梯度可以写成 $\\nabla J(x) = Qx - c$，其中 $c = A^\\top b$。由于 $Q$ 是常数， $J(x)$ 的Hessian矩阵是 $\\nabla^2 J(x) = Q$。矩阵 $A^\\top A$ 总是半正定的。由于 $\\gamma \\ge 0$，矩阵 $Q$ 是对称且半正定的。如果 $\\gamma  0$ 或 $A$ 具有满列秩，则 $Q$ 是正定的，且 $J(x)$ 是严格凸的。\n\n梯度的Lipschitz常数 $L$ 是Hessian矩阵的谱范数（最大特征值，因为 $Q$ 是对称半正定的）：$L = \\|Q\\|_2 = \\lambda_{\\max}(Q)$。\n\n**可计算上界 $\\overline{L}$ 的推导**\n\n目标是找到 $L = \\|Q\\|_2$ 的一个上界 $\\overline{L}$，该上界不需要进行特征值分解。我们使用矩阵范数的基本性质。\n\n对于任何矩阵 $M \\in \\mathbb{R}^{n \\times n}$，一个已知的联系谱范数与$1$-范数和$\\infty$-范数的不等式是 $\\|M\\|_2 \\le \\sqrt{\\|M\\|_1 \\|M\\|_\\infty}$。$1$-范数是最大绝对列和，$\\infty$-范数是最大绝对行和。\n$$\n\\|M\\|_1 = \\max_{1 \\le j \\le n} \\sum_{i=1}^n |M_{ij}|, \\quad \\|M\\|_\\infty = \\max_{1 \\le i \\le n} \\sum_{j=1}^n |M_{ij}|.\n$$\n我们的Hessian矩阵是 $Q = A^\\top A + \\gamma I_n$。由于 $A^\\top A$ 是对称的，$\\gamma I_n$ 也是对称的，因此 $Q$ 是对称的。对于任何对称矩阵 $M$，我们有 $\\|M\\|_1 = \\|M\\|_\\infty$。\n将此应用于 $Q$，不等式变为 $\\|Q\\|_2 \\le \\sqrt{\\|Q\\|_1 \\|Q\\|_1} = \\|Q\\|_1$。\n因此，一个有效且可计算的 $L$ 的上界是 $\\overline{L} = \\|Q\\|_1$。\n$$\nL = \\|Q\\|_2 \\le \\|Q\\|_1 = \\max_{j} \\sum_{i=1}^n |Q_{ij}|.\n$$\n由于问题提供了 $A$ 的显式矩阵，我们可以构造 $Q = A^\\top A + \\gamma I_n$ 并计算其 $1$-范数。\n\n另一个界可以通过使用范数的三角不等式和次乘法性质推导出来：\n$$\nL = \\|Q\\|_2 = \\|A^\\top A + \\gamma I_n\\|_2 \\le \\|A^\\top A\\|_2 + \\|\\gamma I_n\\|_2.\n$$\n我们有 $\\|\\gamma I_n\\|_2 = \\gamma$（对于 $\\gamma \\ge 0$）。此外，$\\|A^\\top A\\|_2 \\le \\|A^\\top\\|_2 \\|A\\|_2$。由于 $\\|A^\\top\\|_2 = \\|A\\|_2$，这简化为 $\\|A^\\top A\\|_2 \\le \\|A\\|_2^2$。这给出了 $L \\le \\|A\\|_2^2 + \\gamma$。虽然这正确，但直接计算 $\\|A\\|_2$ 本身就是一个特征值问题。我们可以进一步界定 $\\|A\\|_2 \\le \\sqrt{\\|A\\|_1 \\|A\\|_\\infty}$。这产生了界 $L \\le \\|A\\|_1 \\|A\\|_\\infty + \\gamma$。通过$1$-范数的次可加性和次乘法性可以证明 $\\|Q\\|_1 = \\|A^\\top A + \\gamma I_n\\|_1 \\le \\|A^\\top A\\|_1 + \\|\\gamma I_n\\|_1 \\le \\|A^\\top\\|_1 \\|A\\|_1 + \\gamma = \\|A\\|_\\infty \\|A\\|_1 + \\gamma$。这表明界 $\\|Q\\|_1$ 通常比 $\\|A\\|_1\\|A\\|_\\infty + \\gamma$ 更紧或与之相等。因此，我们将选择 $\\overline{L} = \\|Q\\|_1$ 作为我们的严格且可计算的上界。\n\n**可计算下界 $\\underline{L}$ 的推导**\n\nLipschitz常数 $L$ 是对称半正定矩阵 $Q$ 的最大特征值。最大特征值的变分刻画由下式给出\n$$\nL = \\lambda_{\\max}(Q) = \\max_{v \\in \\mathbb{R}^n, v \\ne 0} \\frac{v^\\top Q v}{v^\\top v}.\n$$\n表达式 $R_Q(v) = \\frac{v^\\top Q v}{v^\\top v}$ 是瑞利商。这个刻画立即意味着对于任何非零向量 $v \\in \\mathbb{R}^n$，瑞利商 $R_Q(v)$ 是 $L$ 的一个下界：$R_Q(v) \\le L$。\n\n为了找到一个紧密的下界，我们需要找到一个向量 $v$，它能很好地逼近与 $\\lambda_{\\max}(Q)$ 对应的特征向量。幂迭代法就是为此目的而设计的。该算法生成一个收敛到主特征向量的向量序列。\n\n过程如下：\n1. 从一个初始向量 $v^0$ 开始，满足 $\\|v^0\\|_2 = 1$（例如，一个随机生成的向量）。\n2. 对于 $k = 0, 1, 2, \\dots$：\n    $$\n    w^{k+1} = Q v^k\n    $$\n    $$\n    v^{k+1} = \\frac{w^{k+1}}{\\|w^{k+1}\\|_2}\n    $$\n3. 向量序列 $\\{v^k\\}$ 收敛到与最大模特征值相关的特征向量。由于 $Q$ 是半正定的，这个特征值就是 $\\lambda_{\\max}(Q) = L$。\n4. 在每次迭代 $k$ 时，瑞利商 $R_Q(v^k)$ 提供了 $L$ 的一个下界。当 $k \\to \\infty$ 时，$v^k$ 与主特征向量对齐，并且 $R_Q(v^k)$ 收敛到 $L$。可以证明，序列 $\\{R_Q(v^k)\\}$ 是非递减的。\n\n因此，通过运行足够多步的幂迭代，最终的瑞利商 $\\underline{L} = R_Q(v^{\\text{final}})$ 提供了一个高质量的 $L$ 的数值下界。\n\n**步长选择与保守性评估**\n\n投影梯度法迭代为 $x^{k+1} = \\Pi_{[\\ell,u]}(x^k - \\alpha \\nabla J(x^k))$。对于一个具有 $L$-Lipschitz连续梯度的凸目标函数 $J$，如果步长 $\\alpha$ 选择在区间 $\\alpha \\in (0, 2/L)$ 内，则保证收敛到一个最小值点。选择 $\\alpha \\in (0, 1/L]$ 还能额外保证目标函数值是非递增的，即 $J(x^{k+1}) \\le J(x^k)$，这是稳定收敛的一个理想属性。\n\n$L$ 的确切值通常是未知的。然而，我们已经推导出了一个可证明的上界 $\\overline{L}$，使得 $L \\le \\overline{L}$。通过选择步长 $\\alpha = 1/\\overline{L}$，我们确保了 $0  \\alpha \\le 1/L$。因此，这个选择是“严格安全的”，因为它保证落在确保单调收敛的区间内。\n\n梯度法的收敛速度受到步长的影响。过小的步长可能导致收敛非常缓慢。理想的步长接近稳定范围的上限，因此基于 $L$ 的选择（例如 $1/L$）比基于一个松散上界 $\\overline{L} \\gg L$ 的选择要好。\n\n从幂迭代获得的下界 $\\underline{L}$ 提供了对 $L$ 真实值的出色近似，即 $\\underline{L} \\approx L$。我们可以通过计算比率 $\\overline{L} / \\underline{L}$ 来评估我们的上界 $\\overline{L}$ 的质量，或“保守性”。\n- 如果 $\\overline{L} / \\underline{L} \\approx 1$，则上界是紧密的，步长 $\\alpha = 1/\\overline{L}$ 接近最优。\n- 如果 $\\overline{L} / \\underline{L} \\gg 1$，则上界是松散的，步长过于保守，这可能会显著减慢投影梯度算法的收敛速度。这种评估可以为寻求更紧的 $\\overline{L}$ 界或采用自适应步长策略（如线搜索）的决策提供信息。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the constrained quadratic optimization problem using the projected gradient method\n    for three distinct test cases. For each case, it computes lower and upper bounds for the\n    Lipschitz constant of the gradient and the final objective value.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"A\": np.array([[3, 1, 0], [1, 4, 1], [0, 1, 2]], dtype=np.float64),\n            \"b\": np.array([1, 0, -1], dtype=np.float64),\n            \"gamma\": 0.2,\n            \"l\": np.array([-0.5, -0.5, -0.5], dtype=np.float64),\n            \"u\": np.array([2, 2, 2], dtype=np.float64),\n            \"x0\": np.array([0, 0, 0], dtype=np.float64),\n        },\n        {\n            \"A\": np.array([[1, 2, 3], [2, 4, 6]], dtype=np.float64),\n            \"b\": np.array([1, 2], dtype=np.float64),\n            \"gamma\": 0.0,\n            \"l\": np.array([0, 0, 0], dtype=np.float64),\n            \"u\": np.array([1, 1, 1], dtype=np.float64),\n            \"x0\": np.array([0.3, 0.3, 0.3], dtype=np.float64),\n        },\n        {\n            \"A\": np.array([[2, 0, -1], [0, 1, 1], [1, -1, 0], [0, 2, 3]], dtype=np.float64),\n            \"b\": np.array([1, -1, 0, 2], dtype=np.float64),\n            \"gamma\": 3.0,\n            \"l\": np.array([-0.1, -0.1, -0.1], dtype=np.float64),\n            \"u\": np.array([0.1, 0.1, 0.1], dtype=np.float64),\n            \"x0\": np.array([0, 0, 0], dtype=np.float64),\n        },\n    ]\n\n    # Algorithmic Parameters\n    K_pow = 200\n    eps_pow = 1e-10\n    K_max = 20000\n    eps_pgd = 1e-9\n\n    results = []\n\n    for case in test_cases:\n        A = case[\"A\"]\n        b = case[\"b\"]\n        gamma = case[\"gamma\"]\n        l_bound = case[\"l\"]\n        u_bound = case[\"u\"]\n        x = case[\"x0\"].copy()\n        \n        m, n = A.shape\n        \n        # Form the Hessian Q = A^T A + gamma * I\n        Q = A.T @ A + gamma * np.eye(n)\n        \n        # --- 1. Compute rigorous upper bound L_upper ---\n        # L_upper = ||Q||_1, which is the maximum absolute column sum.\n        # For a symmetric matrix Q, ||Q||_1 = ||Q||_inf.\n        L_upper = np.max(np.sum(np.abs(Q), axis=0))\n\n        # --- 2. Compute numerical lower bound L_lower using Power Iteration ---\n        # Start with a random vector\n        np.random.seed(0) # for reproducibility\n        v = np.random.rand(n)\n        v = v / np.linalg.norm(v)\n        \n        L_lower_val = 0.0\n        for _ in range(K_pow):\n            w = Q @ v\n            v_new = w / np.linalg.norm(w)\n            # Check for convergence\n            if np.linalg.norm(v_new - v)  eps_pow:\n                v = v_new\n                break\n            v = v_new\n        \n        # Final Rayleigh quotient is the lower bound\n        L_lower = v.T @ (Q @ v)\n\n        # --- 3. Run Projected Gradient Method ---\n        alpha = 1.0 / L_upper\n        c = A.T @ b\n\n        for _ in range(K_max):\n            grad_J = Q @ x - c\n            x_unconstrained = x - alpha * grad_J\n            x_next = np.clip(x_unconstrained, l_bound, u_bound)\n            \n            # Check stopping criterion\n            if np.linalg.norm(x_next - x)  eps_pgd:\n                x = x_next\n                break\n            x = x_next\n\n        x_star = x\n\n        # --- 4. Compute final objective value J(x*) ---\n        residual_norm_sq = np.linalg.norm(A @ x_star - b)**2\n        regularizer_norm_sq = np.linalg.norm(x_star)**2\n        J_final = 0.5 * residual_norm_sq + (gamma / 2.0) * regularizer_norm_sq\n        \n        results.extend([L_lower, L_upper, J_final])\n\n    # Format the final output\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3414873"}, {"introduction": "虽然投影梯度法是处理约束问题的强大工具，但它并非唯一选择。通过与其他方法进行比较，可以更深刻地理解其特性和适用场景。本练习 [@problem_id:3414810] 要求你将投影梯度下降法 (PGD) 与处理正定约束的另一种经典方法——对数障碍法——进行对比。通过实现这两种算法并分析它们在边界附近的不同行为，你将学会评估不同算法策略在解决同一问题时的实际效果和微妙差异。", "problem": "考虑一个具有有界正值状态的线性逆问题。设 $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ 是一个已知的满列秩线性观测算子，$\\mathbf{b} \\in \\mathbb{R}^{m}$ 是给定的测量值。目标是通过最小化最小二乘失配来从数据中估计状态向量 $\\mathbf{x} \\in \\mathbb{R}^{n}$，并满足严格为正的约束，即最小化目标函数 $J(\\mathbf{x}) = \\tfrac{1}{2}\\lVert \\mathbf{A}\\mathbf{x} - \\mathbf{b} \\rVert_2^2$，约束条件为 $\\mathbf{x} \\in \\mathbb{R}_{++}^{n}$，其中 $\\mathbb{R}_{++}^{n}$ 表示所有分量都严格为正的向量集合。这个严格为正的约束模拟了数据同化中的先验知识，即由 $\\mathbf{x}$ 表示的物理量（如浓度或密度）必须为正。\n\n将对两种算法策略进行研究和比较：\n\n1. 投影梯度下降法 (Projected Gradient Descent, PGD)：迭代形式为 $\\mathbf{x}^{k+1} = \\mathcal{P}_{\\mathbb{R}_{+}^{n}}\\!\\left(\\mathbf{x}^{k} - \\alpha \\nabla J(\\mathbf{x}^{k})\\right)$，其中 $\\nabla J(\\mathbf{x})$ 是 $J$ 的梯度，$\\mathcal{P}_{\\mathbb{R}_{+}^{n}}$ 表示到非负象限 $\\mathbb{R}_{+}^{n} = \\{\\mathbf{x} \\in \\mathbb{R}^{n} : x_i \\ge 0, \\ \\forall i\\}$ 的正交投影。该方法通过投影显式地施加非负性约束。众所周知，对于步长参数 $\\alpha \\in (0, 1/L)$（其中 $L$ 是梯度 $\\nabla J$ 的任意 Lipschitz 常数）的选择，对于凸函数 $J$，迭代序列在目标函数上呈下降趋势。\n\n2. 用于严格正性的对数障碍代理：定义障碍增广目标函数 $J_{\\mu}(\\mathbf{x}) = \\tfrac{1}{2}\\lVert \\mathbf{A}\\mathbf{x} - \\mathbf{b} \\rVert_2^2 - \\mu \\sum_{i=1}^{n} \\log(x_i)$，其中 $\\mu  0$ 是一个障碍参数。该代理函数通过惩罚趋近边界 $x_i \\downarrow 0$ 的行为来隐式地施加严格正性约束 $\\mathbf{x} \\in \\mathbb{R}_{++}^{n}$。梯度下降迭代的形式为 $\\mathbf{x}^{k+1} = \\mathbf{x}^{k} - \\alpha \\nabla J_{\\mu}(\\mathbf{x}^{k})$，其中选择步长 $\\alpha  0$ 以保持正性和下降性。在边界 $x_i \\downarrow 0$ 附近，由于 $-\\log(x_i)$ 导数的发散，障碍项主导其行为。\n\n仅从以下基本事实出发：最小二乘目标函数 $J(\\mathbf{x}) = \\tfrac{1}{2}\\lVert \\mathbf{A}\\mathbf{x} - \\mathbf{b} \\rVert_2^2$ 的梯度为 $\\nabla J(\\mathbf{x}) = \\mathbf{A}^{\\top}(\\mathbf{A}\\mathbf{x} - \\mathbf{b})$，$\\nabla J$ 的 Lipschitz 常数由谱范数的平方 $L = \\lVert \\mathbf{A} \\rVert_2^2$ 界定，到非负象限的正交投影 $\\mathcal{P}_{\\mathbb{R}_{+}^{n}}$ 是逐分量作用的，以及 $\\log(x)$ 的导数是 $1/x$，请完成以下任务：\n\n- 推导此问题的 PGD 更新映射的显式形式，确保步长的选择使用梯度的有效 Lipschitz 界，并且投影逐分量地强制执行非负性。\n\n- 推导障碍增广目标函数 $J_{\\mu}(\\mathbf{x})$ 梯度的显式形式，并设计一个与梯度下降迭代兼容的、保持正性且强制下降的步长选择规则。该规则必须保证如果 $\\mathbf{x}^{k} \\in \\mathbb{R}_{++}^{n}$，则 $\\mathbf{x}^{k+1} \\in \\mathbb{R}_{++}^{n}$，同时确保 $J_{\\mu}$ 的目标函数值下降。完成此任务时，除了上述基本事实外，不得使用任何快捷公式。\n\n- 实现这两种算法直至收敛，收敛停止规则基于连续迭代的差值。两种方法都使用具有严格正分量的固定初始猜测值。对于 PGD，显式地实现到 $\\mathbb{R}_{+}^{n}$ 的投影。对于障碍法，确保在每次迭代中都执行保持正性的步长规则。\n\n- 在以下测试用例集上比较两种方法的近边界行为。此问题中没有物理单位；所有量都是无量纲的。\n\n测试集（每个案例由 $(\\mathbf{A}, \\mathbf{b}, \\mu, \\mathbf{x}^{0}, \\text{tolerances})$ 指定）：\n1. 案例 1（一维，边界主导）：$n = 1$，$\\mathbf{A} = [1]$，$\\mathbf{b} = [10^{-6}]$，$\\mu = 10^{-6}$，初始猜测值 $\\mathbf{x}^{0} = [10^{-3}]$，停止容差 $\\epsilon = 10^{-12}$，最大迭代次数 $N_{\\max} = 20000$。\n2. 案例 2（二维，跨坐标的病态条件）：$n = 2$，$\\mathbf{A} = \\mathrm{diag}(1, 10^{-2})$，$\\mathbf{b} = [10^{-3}, 10^{-5}]^{\\top}$，$\\mu = 10^{-5}$，初始猜测值 $\\mathbf{x}^{0} = [10^{-3}, 10^{-3}]^{\\top}$，停止容差 $\\epsilon = 10^{-12}$，最大迭代次数 $N_{\\max} = 20000$。\n3. 案例 3（十维，卷积模糊且真实解中存在零）：$n = 10$，$\\mathbf{A} \\in \\mathbb{R}^{10 \\times 10}$ 是与核 $\\mathbf{h} = [0.25, 0.5, 0.25]$ 相关的 Toeplitz 卷积矩阵，在边界处进行零填充，即 $(\\mathbf{A}\\mathbf{x})_i = 0.25 x_{i-1} + 0.5 x_i + 0.25 x_{i+1}$，其中 $x_0 = x_{n+1} = 0$，且 $\\mathbf{b} = \\mathbf{A}\\mathbf{x}_{\\mathrm{true}}$，其中 $\\mathbf{x}_{\\mathrm{true}} = [0.0, 0.1, 0.0, 0.2, 0.0, 0.05, 0.0, 0.1, 0.0, 0.0]^{\\top}$。附加噪声可忽略不计，为保证可复现性可取为零。取 $\\mu = 10^{-6}$，初始猜测值 $\\mathbf{x}^{0} = 10^{-3}\\cdot \\mathbf{1}$（所有分量均为 $10^{-3}$ 的向量），停止容差 $\\epsilon = 10^{-12}$，最大迭代次数 $N_{\\max} = 50000$。\n\n对于每个案例，您的程序必须生成三个量化近边界行为的标量：\n- 最终 PGD 解的最小分量值，$\\min_i x^{\\star}_{\\mathrm{PGD}, i}$。\n- 最终障碍法解的最小分量值，$\\min_i x^{\\star}_{\\mathrm{bar}, i}$。\n- PGD 解与障碍法解的最终最小二乘失配之差，$J(\\mathbf{x}^{\\star}_{\\mathrm{PGD}}) - J(\\mathbf{x}^{\\star}_{\\mathrm{bar}})$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含九个结果（每个案例三个，按案例1、案例2、案例3的顺序列出上述结果），结果为逗号分隔的列表，并用方括号括起来（例如，“[r1,r2,r3,r4,r5,r6,r7,r8,r9]”）。必须只打印这一行。不涉及角度；无需单位转换。所有输出均为无量纲浮点数。", "solution": "所提出的问题是逆问题数值优化领域中一个有效、适定且有科学依据的任务。它涉及在正性约束下最小化一个凸二次目标函数，这是数据同化和其他科学领域的标准问题。所有提供的信息都是自洽的、数学上一致的，并且足以用于推导和实现指定的算法。我们开始进行求解。\n\n核心问题是找到一个状态向量 $\\mathbf{x}$，它能最小化最小二乘目标函数：\n$$\nJ(\\mathbf{x}) = \\frac{1}{2}\\lVert \\mathbf{A}\\mathbf{x} - \\mathbf{b} \\rVert_2^2\n$$\n约束条件是 $\\mathbf{x}$ 的所有分量都严格为正，即 $\\mathbf{x} \\in \\mathbb{R}_{++}^{n}$，其中 $(\\mathbf{x})_i = x_i  0$ 对所有 $i=1, \\dots, n$ 成立。由于最小化子可能位于该集合的边界上，我们实际上是在其闭包 $\\mathbf{x} \\in \\mathbb{R}_{+}^{n}$（其中 $x_i \\ge 0$）中寻找解。\n\n### 投影梯度下降法 (Projected Gradient Descent, PGD)\n\n投影梯度下降法通过首先沿负梯度方向迈出一步，然后将结果投影到可行集上来迭代地优化估计值 $\\mathbf{x}^k$。\n\n**1. 梯度推导：**\n目标函数 $J(\\mathbf{x})$ 的梯度作为一个基本事实被给出：\n$$\n\\nabla J(\\mathbf{x}) = \\mathbf{A}^{\\top}(\\mathbf{A}\\mathbf{x} - \\mathbf{b})\n$$\n\n**2. PGD 更新规则：**\n迭代更新由 $\\mathbf{x}^{k+1} = \\mathcal{P}_{\\mathbb{R}_{+}^{n}}\\!\\left(\\mathbf{x}^{k} - \\alpha \\nabla J(\\mathbf{x}^{k})\\right)$ 给出。这包括两个步骤：\n- 梯度下降步：$\\mathbf{y}^{k+1} = \\mathbf{x}^{k} - \\alpha \\nabla J(\\mathbf{x}^{k})$。\n- 投影步：$\\mathbf{x}^{k+1} = \\mathcal{P}_{\\mathbb{R}_{+}^{n}}(\\mathbf{y}^{k+1})$。\n\n**3. 步长 $\\alpha$：**\n为保证梯度下降的收敛，步长 $\\alpha$ 必须满足 $\\alpha \\in (0, 2/L)$，其中 $L$ 是梯度 $\\nabla J(\\mathbf{x})$ 的 Lipschitz 常数。问题提供了一个更严格的充分条件 $\\alpha \\in (0, 1/L)$，以及 Lipschitz 常数的一个有效界 $L = \\lVert \\mathbf{A} \\rVert_2^2 = \\sigma_{\\max}^2(\\mathbf{A})$，其中 $\\sigma_{\\max}(\\mathbf{A})$ 是 $\\mathbf{A}$ 的最大奇异值。可以从这个区间选择一个固定的步长 $\\alpha$，例如，$\\alpha = c / \\lVert \\mathbf{A} \\rVert_2^2$，其中常数 $c \\in (0, 1)$，比如 $c=0.99$。\n\n**4. 投影算子 $\\mathcal{P}_{\\mathbb{R}_{+}^{n}}$：**\n到非负象限 $\\mathbb{R}_{+}^{n}$ 的正交投影是逐分量作用的。对于任何向量 $\\mathbf{y} \\in \\mathbb{R}^n$，其投影 $\\mathbf{x} = \\mathcal{P}_{\\mathbb{R}_{+}^{n}}(\\mathbf{y})$ 定义为：\n$$\nx_i = (\\mathbf{x})_i = \\max(y_i, 0) \\quad \\text{for } i=1, \\dots, n\n$$\n\n**5. 显式 PGD 更新映射：**\n结合这些元素，从 $\\mathbf{x}^k$ 到 $\\mathbf{x}^{k+1}$ 的显式更新映射是：\n$$\n\\mathbf{x}^{k+1} = \\max\\left(\\mathbf{x}^k - \\alpha \\left( \\mathbf{A}^{\\top}(\\mathbf{A}\\mathbf{x}^k - \\mathbf{b}) \\right), \\mathbf{0}\\right)\n$$\n其中 $\\max$ 函数是逐分量应用的。该算法在 $\\mathbb{R}_{+}^{n}$ 中寻找解。如果真正的最小化子具有零分量，该方法能够通过将迭代的相应分量精确地设置为 $0$ 来找到它们。\n\n### 对数障碍法\n\n对数障碍法是一种内点法，它通过一个惩罚项将正性约束整合到目标函数中。这创建了一个代理目标函数 $J_{\\mu}(\\mathbf{x})$，该函数在没有显式约束的情况下进行最小化，但其结构阻止了迭代到达可行集的边界。\n\n**1. 障碍增广目标函数：**\n代理目标函数定义为：\n$$\nJ_{\\mu}(\\mathbf{x}) = J(\\mathbf{x}) - \\mu \\sum_{i=1}^{n} \\log(x_i) = \\frac{1}{2}\\lVert \\mathbf{A}\\mathbf{x} - \\mathbf{b} \\rVert_2^2 - \\mu \\sum_{i=1}^{n} \\log(x_i)\n$$\n其中 $\\mu  0$ 是障碍参数。当 $x_i \\to 0^+$ 时，项 $-\\mu \\log(x_i)$ 发散到 $+\\infty$，从而惩罚接近边界的迭代。\n\n**2. 障碍目标函数的梯度：**\n利用梯度算子的线性和给定的基本事实，我们推导出 $\\nabla J_{\\mu}(\\mathbf{x})$：\n$$\n\\nabla J_{\\mu}(\\mathbf{x}) = \\nabla \\left(\\frac{1}{2}\\lVert \\mathbf{A}\\mathbf{x} - \\mathbf{b} \\rVert_2^2\\right) - \\nabla \\left(\\mu \\sum_{i=1}^{n} \\log(x_i)\\right)\n$$\n对数障碍项的梯度是：\n$$\n\\nabla \\left(\\mu \\sum_{i=1}^{n} \\log(x_i)\\right) = \\mu \\begin{bmatrix} \\partial/\\partial x_1 \\\\ \\vdots \\\\ \\partial/\\partial x_n \\end{bmatrix} \\sum_{j=1}^{n} \\log(x_j) = \\mu \\begin{bmatrix} 1/x_1 \\\\ \\vdots \\\\ 1/x_n \\end{bmatrix} = \\mu \\mathbf{x}^{\\circ -1}\n$$\n其中 $\\mathbf{x}^{\\circ -1}$ 表示 $\\mathbf{x}$ 的逐分量逆。\n因此，完整的梯度是：\n$$\n\\nabla J_{\\mu}(\\mathbf{x}) = \\mathbf{A}^{\\top}(\\mathbf{A}\\mathbf{x} - \\mathbf{b}) - \\mu \\mathbf{x}^{\\circ -1}\n$$\n\n**3. 保持正性且强制下降的步长规则：**\n更新是一个标准的梯度下降步，$\\mathbf{x}^{k+1} = \\mathbf{x}^k - \\alpha_k \\nabla J_{\\mu}(\\mathbf{x}^k)$。我们必须设计一个选择步长 $\\alpha_k  0$ 的规则，以保证正性（对所有 $i$ 都有 $x_i^{k+1}  0$）和下降性（$J_{\\mu}(\\mathbf{x}^{k+1})  J_{\\mu}(\\mathbf{x}^k)$）。\n\n- **保持正性：** 对于每个分量 $i$，我们要求 $x_i^{k+1} = x_i^k - \\alpha_k (\\nabla J_{\\mu}(\\mathbf{x}^k))_i  0$。令 $\\mathbf{g}_{\\mu}^k = \\nabla J_{\\mu}(\\mathbf{x}^k)$。\n  如果 $(g_{\\mu}^k)_i \\le 0$，则第 $i$ 个分量是不减的，条件对任何 $\\alpha_k  0$ 都成立。\n  如果 $(g_{\\mu}^k)_i  0$，我们必须有 $\\alpha_k  x_i^k / (g_{\\mu}^k)_i$。\n  为了同时满足所有分量，步长必须受这些比率的最小值的限制：\n  $$\n  \\alpha_{\\text{max}} = \\min_{i \\text{ s.t. } (g_{\\mu}^k)_i  0} \\left\\{ \\frac{x_i^k}{(g_{\\mu}^k)_i} \\right\\}\n  $$\n  任何步长 $\\alpha_k \\in (0, \\alpha_{\\text{max}})$ 都将确保 $\\mathbf{x}^{k+1} \\in \\mathbb{R}_{++}^n$。\n\n- **强制下降：** 固定步长不够稳健，因为 $\\nabla J_{\\mu}$ 的 Lipschitz 常数依赖于 $\\mathbf{x}$，并且在边界附近可能任意大。回溯线搜索是一种标准且有效的方法。规则如下：\n  1. 设置回溯参数：一个充分下降常数 $c \\in (0, 1)$（例如，$c=10^{-4}$）和一个收缩因子 $\\tau \\in (0, 1)$（例如，$\\tau=0.5$）。\n  2. 计算搜索方向 $\\mathbf{d}^k = -\\mathbf{g}_{\\mu}^k = -\\nabla J_{\\mu}(\\mathbf{x}^k)$。\n  3. 设置一个初始试探步长 $\\alpha$。一个安全且积极的选择是最大保持正性步长的一部分，例如，$\\alpha = \\beta \\alpha_{\\text{max}}$，其中 $\\beta \\in (0, 1)$（例如，$\\beta=0.99$）。\n  4. 当 Armijo 条件不满足时：\n      $$\n      J_{\\mu}(\\mathbf{x}^k + \\alpha \\mathbf{d}^k) > J_{\\mu}(\\mathbf{x}^k) + c \\alpha (\\mathbf{g}_{\\mu}^k)^{\\top} \\mathbf{d}^k\n      $$\n      这简化为：\n      $$\n      J_{\\mu}(\\mathbf{x}^k - \\alpha \\mathbf{g}_{\\mu}^k) > J_{\\mu}(\\mathbf{x}^k) - c \\alpha \\lVert \\mathbf{g}_{\\mu}^k \\rVert_2^2\n      $$\n      缩小步长：$\\alpha \\leftarrow \\tau \\alpha$。\n  5. 设置 $\\alpha_k = \\alpha$。\n\n此过程保证了每一步都保持严格正性，并减小障碍目标函数 $J_{\\mu}$ 的值。与 PGD 不同，障碍法将始终产生具有严格正分量的解，对于在 $J$ 的真实最小化子中为零的分量，当 $\\mu \\to 0$ 时，这些分量会趋近于零。\n\n### 近边界行为比较\n\n- **PGD：** 投影算子 $\\max(\\cdot, 0)$ 在 $0$ 点是不可微的，但它允许算法将解向量 $\\mathbf{x}^{\\star}_{\\mathrm{PGD}}$ 的分量精确地放置在可行集的边界上，即 $x_i^{\\star} = 0$。当已知真实的物理状态具有零值（例如，某些区域的浓度或密度为零）时，这是有利的。\n\n- **对数障碍法：** 对数障碍项 $-\\mu \\log(x_i)$ 产生一个梯度分量 $-\\mu/x_i$，当 $x_i \\to 0^+$ 时，该分量无限增大。这起到一种排斥力的作用，阻止任何分量变为零。解 $\\mathbf{x}^{\\star}_{\\mathrm{bar}}$ 将始终位于严格内部 $\\mathbb{R}_{++}^n$。$\\mathbf{x}^{\\star}_{\\mathrm{bar}}$ 的最小分量将是小的正值，其大小取决于障碍参数 $\\mu$。因此，在 $\\mathbb{R}_{+}^{n}$ 上最小化的 $J(\\mathbf{x}^{\\star}_{\\mathrm{PGD}})$ 通常会小于或等于 $J(\\mathbf{x}^{\\star}_{\\mathrm{bar}})$，后者是最小化一个扰动目标函数 $J_\\mu$ 的结果。", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import toeplitz\n\ndef solve_pgd(A, b, x0, tol, n_max):\n    \"\"\"\n    Solves the non-negatively constrained least squares problem using Projected Gradient Descent (PGD).\n    min 0.5 * ||Ax - b||^2 s.t. x >= 0.\n    \"\"\"\n    x = np.copy(x0)\n    AtA = A.T @ A\n    Atb = A.T @ b\n    \n    # Lipschitz constant of the gradient is the max eigenvalue of A^T A, which is ||A||_2^2\n    try:\n        # np.linalg.norm(A, 2) can be slow for large matrices, but fine for these cases.\n        L = np.linalg.norm(A, 2)**2\n        if L == 0:  # Handle zero matrix case\n            L = 1.0\n    except np.linalg.LinAlgError:\n        L = np.linalg.norm(AtA, 2) # Fallback if norm(A, 2) fails\n\n    alpha = 0.99 / L  # Fixed step size satisfying alpha  1/L\n\n    for _ in range(n_max):\n        grad = AtA @ x - Atb\n        x_new = x - alpha * grad\n        x_new = np.maximum(x_new, 0) # Projection step\n        \n        if np.linalg.norm(x_new - x)  tol:\n            x = x_new\n            break\n        x = x_new\n        \n    return x\n\ndef solve_barrier(A, b, mu, x0, tol, n_max):\n    \"\"\"\n    Solves the strictly positive least squares problem using a log-barrier method.\n    min 0.5 * ||Ax - b||^2 - mu * sum(log(x_i)).\n    \"\"\"\n    x = np.copy(x0)\n    AtA = A.T @ A\n    Atb = A.T @ b\n\n    # Backtracking line search parameters\n    c1 = 1e-4\n    tau = 0.5\n\n    for k in range(n_max):\n        # Prevent x components from being exactly zero or negative due to numerical error\n        x[x = 0] = np.finfo(float).eps\n\n        # Calculate gradient of barrier objective J_mu\n        grad_mu = (AtA @ x - Atb) - mu / x\n        \n        # Calculate max step size to preserve positivity\n        pos_grad_indices = grad_mu > 0\n        if not np.any(pos_grad_indices):\n            alpha_max_pos = 1.0 # All gradient components non-positive, can take a large step\n        else:\n            alpha_max_pos = np.min(x[pos_grad_indices] / grad_mu[pos_grad_indices])\n        \n        alpha = 0.99 * alpha_max_pos\n\n        # Perform backtracking line search\n        J_mu_k = 0.5 * np.sum((A @ x - b)**2) - mu * np.sum(np.log(x))\n        descent_term = c1 * alpha * (grad_mu @ grad_mu)\n\n        while True:\n            x_new = x - alpha * grad_mu\n            \n            # Ensure new iterate is strictly positive for log evaluation\n            if np.any(x_new = 0):\n                alpha *= tau\n                descent_term *= tau\n                if alpha  1e-20: # Step size too small\n                     break\n                continue\n\n            J_mu_new = 0.5 * np.sum((A @ x_new - b)**2) - mu * np.sum(np.log(x_new))\n            \n            if J_mu_new = J_mu_k - c1 * alpha * np.dot(grad_mu, grad_mu): # Armijo condition check\n                break\n            \n            alpha *= tau\n            if alpha  1e-20: # Step size too small\n                x_new = x\n                break\n\n        if np.linalg.norm(x_new - x)  tol:\n            x = x_new\n            break\n        x = x_new\n    \n    return x\n\ndef calculate_metrics(A, b, x_pgd, x_bar):\n    \"\"\"Calculates the three required performance metrics.\"\"\"\n    min_pgd = np.min(x_pgd)\n    min_bar = np.min(x_bar)\n    \n    j_pgd = 0.5 * np.linalg.norm(A @ x_pgd - b)**2\n    j_bar = 0.5 * np.linalg.norm(A @ x_bar - b)**2\n    \n    diff_j = j_pgd - j_bar\n    return min_pgd, min_bar, diff_j\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    # Test Suite (A, b, mu, x0, tol, n_max)\n    case1_A = np.array([[1.0]])\n    case1_b = np.array([1e-6])\n    case1_mu = 1e-6\n    case1_x0 = np.array([1e-3])\n    case1_tol = 1e-12\n    case1_nmax = 20000\n\n    case2_A = np.diag([1.0, 1e-2])\n    case2_b = np.array([1e-3, 1e-5])\n    case2_mu = 1e-5\n    case2_x0 = np.array([1e-3, 1e-3])\n    case2_tol = 1e-12\n    case2_nmax = 20000\n\n    n3 = 10\n    c3 = np.zeros(n3)\n    c3[0] = 0.5\n    c3[1] = 0.25\n    case3_A = toeplitz(c3)\n    case3_x_true = np.array([0.0, 0.1, 0.0, 0.2, 0.0, 0.05, 0.0, 0.1, 0.0, 0.0])\n    case3_b = case3_A @ case3_x_true\n    case3_mu = 1e-6\n    case3_x0 = 1e-3 * np.ones(n3)\n    case3_tol = 1e-12\n    case3_nmax = 50000\n    \n    test_cases = [\n        (case1_A, case1_b, case1_mu, case1_x0, case1_tol, case1_nmax),\n        (case2_A, case2_b, case2_mu, case2_x0, case2_tol, case2_nmax),\n        (case3_A, case3_b, case3_mu, case3_x0, case3_tol, case3_nmax),\n    ]\n\n    results = []\n    for A, b, mu, x0, tol, n_max in test_cases:\n        x_pgd = solve_pgd(A, b, x0, tol, n_max)\n        x_bar = solve_barrier(A, b, mu, x0, tol, n_max)\n        \n        metrics = calculate_metrics(A, b, x_pgd, x_bar)\n        results.extend(metrics)\n\n    # Format output as specified\n    print(f\"[{','.join(f'{v:.15e}' for v in results)}]\")\n\nsolve()\n```", "id": "3414810"}]}