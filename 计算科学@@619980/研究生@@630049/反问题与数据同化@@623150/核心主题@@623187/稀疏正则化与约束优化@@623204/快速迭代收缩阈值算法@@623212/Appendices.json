{"hands_on_practices": [{"introduction": "FISTA 算法名称中的“收缩-阈值”正来源于其核心的近端映射（proximal mapping）步骤。这个练习将该关键部分独立出来，也就是著名的软阈值（soft-thresholding）算子。在将它整合到完整的算法之前，先通过这个练习来熟练掌握其计算方法，是理解 FISTA 工作原理的第一步。[@problem_id:3446917]", "problem": "考虑复合凸优化问题，即最小化函数 $g(x) = f(x) + \\lambda \\|x\\|_{1}$，其中 $f(x) = \\frac{1}{2}\\|A x - b\\|_{2}^{2}$ 且 $\\|x\\|_{1} = \\sum_{i} |x_{i}|$。在快速迭代收缩阈值算法 (FISTA) 中，一次迭代使用缩放后的 $\\ell_{1}$-范数的近端映射。假设当前的一个梯度步已产生向量 $z \\in \\mathbb{R}^{7}$，需要计算近端点\n$$\nx = \\operatorname{prox}_{\\tau \\lambda \\|\\cdot\\|_{1}}(z),\n$$\n其定义为函数\n$$\nx \\mapsto \\tau \\lambda \\|x\\|_{1} + \\frac{1}{2}\\|x - z\\|_{2}^{2}\n$$\n的唯一最小化子。\n设给定数据为\n$$\nz = \\left(1, -\\frac{1}{4}, \\frac{2}{5}, -\\frac{2}{5}, \\frac{7}{15}, -\\frac{9}{10}, 0\\right), \\quad \\lambda = \\frac{3}{5}, \\quad \\tau = \\frac{2}{3}.\n$$\n计算精确向量 $x = \\operatorname{prox}_{\\tau \\lambda \\|\\cdot\\|_{1}}(z)$，结果表示为最简有理数形式。将最终答案以单个行向量的形式给出。不要近似；无需四舍五入。", "solution": "问题要求计算近端点 $x = \\operatorname{prox}_{\\tau \\lambda \\|\\cdot\\|_{1}}(z)$，其定义为以下优化问题的唯一最小化子：\n$$\n\\min_{x \\in \\mathbb{R}^{7}} \\left( \\tau \\lambda \\|x\\|_{1} + \\frac{1}{2}\\|x - z\\|_{2}^{2} \\right)\n$$\n目标函数可以根据向量 $x = (x_1, \\dots, x_7)$ 和 $z = (z_1, \\dots, z_7)$ 的分量写出：\n$$\n\\min_{x_1, \\dots, x_7} \\left( \\tau \\lambda \\sum_{i=1}^{7} |x_i| + \\frac{1}{2} \\sum_{i=1}^{7} (x_i - z_i)^2 \\right)\n$$\n这个目标函数是可分的，意味着它可以分解为多个函数的和，每个函数仅依赖于一个分量 $x_i$。因此，我们可以对每个分量 $x_i$ 独立地最小化该函数：\n$$\nx_i = \\arg\\min_{u \\in \\mathbb{R}} \\left( \\tau \\lambda |u| + \\frac{1}{2}(u - z_i)^2 \\right) \\quad \\text{for } i = 1, \\dots, 7\n$$\n这是著名的软阈值算子。令 $\\alpha = \\tau\\lambda$。一维问题\n$$\n\\min_{u} \\left( \\alpha|u| + \\frac{1}{2}(u - v)^2 \\right)\n$$\n的解由软阈值函数 $S_{\\alpha}(v)$ 给出，其闭式解为：\n$$\nS_{\\alpha}(v) = \\operatorname{sign}(v) \\max(|v| - \\alpha, 0)\n$$\n这可以分段表示为：\n$$\nS_{\\alpha}(v) = \\begin{cases} v - \\alpha & \\text{if } v > \\alpha \\\\ 0 & \\text{if } |v| \\le \\alpha \\\\ v + \\alpha & \\text{if } v < -\\alpha \\end{cases}\n$$\n首先，我们必须使用给定的 $\\lambda$ 和 $\\tau$ 值计算阈值参数 $\\alpha$：\n$$\n\\lambda = \\frac{3}{5}, \\quad \\tau = \\frac{2}{3}\n$$\n阈值 $\\alpha$ 为：\n$$\n\\alpha = \\tau \\lambda = \\left(\\frac{2}{3}\\right) \\left(\\frac{3}{5}\\right) = \\frac{6}{15} = \\frac{2}{5}\n$$\n现在，我们将软阈值算子 $S_{2/5}$ 应用于向量 $z$ 的每个分量：\n$$\nz = \\left(1, -\\frac{1}{4}, \\frac{2}{5}, -\\frac{2}{5}, \\frac{7}{15}, -\\frac{9}{10}, 0\\right)\n$$\n我们来计算每个分量 $x_i = S_{2/5}(z_i)$：\n\n对于 $i=1$：$z_1 = 1$。阈值为 $\\alpha = \\frac{2}{5}$。由于 $z_1 = 1 > \\frac{2}{5}$，我们有：\n$x_1 = z_1 - \\alpha = 1 - \\frac{2}{5} = \\frac{5}{5} - \\frac{2}{5} = \\frac{3}{5}$。\n\n对于 $i=2$：$z_2 = -\\frac{1}{4}$。我们将 $|z_2| = \\frac{1}{4}$ 与 $\\alpha = \\frac{2}{5}$ 比较。换算成公分母 $20$，我们有 $\\frac{1}{4} = \\frac{5}{20}$ 和 $\\frac{2}{5} = \\frac{8}{20}$。由于 $\\frac{5}{20}  \\frac{8}{20}$，即 $|z_2|  \\alpha$。因此：\n$x_2 = 0$。\n\n对于 $i=3$：$z_3 = \\frac{2}{5}$。我们有 $|z_3| = \\frac{2}{5}$，这等于阈值 $\\alpha = \\frac{2}{5}$。由于 $|z_3| \\le \\alpha$：\n$x_3 = 0$。\n\n对于 $i=4$：$z_4 = -\\frac{2}{5}$。我们有 $|z_4| = \\frac{2}{5}$，这等于阈值 $\\alpha = \\frac{2}{5}$。由于 $|z_4| \\le \\alpha$：\n$x_4 = 0$。\n\n对于 $i=5$：$z_5 = \\frac{7}{15}$。我们将 $z_5$ 与 $\\alpha = \\frac{2}{5}$ 比较。换算成公分母 $15$，我们有 $\\frac{2}{5} = \\frac{6}{15}$。由于 $z_5 = \\frac{7}{15}  \\frac{6}{15} = \\alpha$，我们有：\n$x_5 = z_5 - \\alpha = \\frac{7}{15} - \\frac{2}{5} = \\frac{7}{15} - \\frac{6}{15} = \\frac{1}{15}$。\n\n对于 $i=6$：$z_6 = -\\frac{9}{10}$。我们将 $z_6$ 与 $-\\alpha = -\\frac{2}{5}$ 比较。换算成公分母 $10$，我们有 $-\\frac{2}{5} = -\\frac{4}{10}$。由于 $z_6 = -\\frac{9}{10}  -\\frac{4}{10} = -\\alpha$，我们有：\n$x_6 = z_6 + \\alpha = -\\frac{9}{10} + \\frac{2}{5} = -\\frac{9}{10} + \\frac{4}{10} = -\\frac{5}{10} = -\\frac{1}{2}$。\n\n对于 $i=7$：$z_7 = 0$。我们有 $|z_7| = 0$，并且 $0 \\le \\frac{2}{5} = \\alpha$。因此：\n$x_7 = 0$。\n\n将这些分量组合起来，我们得到结果向量 $x$：\n$$\nx = \\left(\\frac{3}{5}, 0, 0, 0, \\frac{1}{15}, -\\frac{1}{2}, 0\\right)\n$$", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{3}{5}  0  0  0  \\frac{1}{15}  -\\frac{1}{2}  0 \\end{pmatrix}}\n$$", "id": "3446917"}, {"introduction": "掌握了近端算子后，我们便可以将它与梯度下降和 Nesterov 加速步骤结合起来。这个练习将引导你手动完成一个完整的 FISTA 迭代过程。通过这个具体的计算，你将清楚地看到算法的各个部分——外推、梯度计算、梯度步和近端步——是如何协同工作以产生下一个更优的解的。[@problem_id:3446895]", "problem": "考虑一维的最小绝对收缩和选择算子 (LASSO) 问题，其目标是最小化复合函数 $f(x) = g(x) + h(x)$，其中 $g(x) = \\frac{1}{2}\\|Ax - b\\|_{2}^{2}$ 且 $h(x) = \\lambda |x|$。令测量矩阵为标量 $A = a$，其中 $a = 2$，令 $b = 3$，且 $\\lambda = 1$。快速迭代收缩阈值算法 (FISTA) 的初始迭代值为 $x^{k-1} = 0$，$x^{k} = 1$，加速参数为 $t_{k} = 2$。使用以下具有科学依据的基础：\n\n- 光滑项 $g$ 的梯度定义为 $\\nabla g(x) = A^{\\top}(Ax - b)$。\n- 梯度 $\\nabla g$ 的 Lipschitz 常数 $L$ 是 $A$ 的谱范数的平方，在一维情况下为 $L = a^{2}$。\n- $h$ 的近端算子（参数为 $\\tau  0$）定义为 $\\operatorname{prox}_{\\tau h}(z) = \\arg\\min_{x}\\left\\{\\frac{1}{2}(x - z)^{2} + \\tau \\lambda |x|\\right\\}$。\n- FISTA 的经典 Nesterov 加速序列满足隐式关系 $t_{k+1}^{2} - t_{k+1} = t_{k}^{2}$。\n\n对此 LASSO 实例执行一次完整的 FISTA 迭代，步骤如下：首先根据上述隐式关系确定 $t_{k+1}$；然后使用 $x^{k}$、$x^{k-1}$、$t_{k}$ 和 $t_{k+1}$ 构成外推点 $y^{k}$；计算梯度 $\\nabla g(y^{k})$；执行梯度步 $z^{k} = y^{k} - \\frac{1}{L}\\nabla g(y^{k})$；并应用参数为 $\\tau = \\frac{1}{L}$ 的近端算子得到 $x^{k+1} = \\operatorname{prox}_{\\frac{1}{L}h}(z^{k})$。按 $y^{k}$、$\\nabla g(y^{k})$、$x^{k+1}$ 和 $t_{k+1}$ 的顺序，精确（不进行四舍五入）地表示这四个量的最终结果。", "solution": "用户要求我使用单次 FISTA 算法迭代来解决一个一维 LASSO 问题。\n\n### 步骤 1：提取已知条件\n问题是最小化函数 $f(x) = g(x) + h(x)$，其中：\n- 光滑部分是 $g(x) = \\frac{1}{2}\\|Ax - b\\|_{2}^{2}$。\n- 非光滑部分是 $h(x) = \\lambda |x|$。\n- 标量测量矩阵：$A = a = 2$。\n- 标量测量值：$b = 3$。\n- 正则化参数：$\\lambda = 1$。\n- 初始迭代值：$x^{k-1} = 0$, $x^{k} = 1$。\n- 初始加速参数：$t_{k} = 2$。\n\n问题为快速迭代收缩阈值算法 (FISTA) 提供了以下定义和更新规则：\n- 光滑项的梯度：$\\nabla g(x) = A^{\\top}(Ax - b)$。\n- 梯度的 Lipschitz 常数：$L = a^{2}$。\n- 近端算子：$\\operatorname{prox}_{\\tau h}(z) = \\arg\\min_{x}\\left\\{\\frac{1}{2}(x - z)^{2} + \\tau \\lambda |x|\\right\\}$。\n- 加速参数更新：$t_{k+1}^{2} - t_{k+1} = t_{k}^{2}$。\n- FISTA 迭代步骤：\n  1. 确定 $t_{k+1}$。\n  2. 构成外推点 $y^{k} = x^{k} + \\frac{t_{k}-1}{t_{k+1}}(x^{k}-x^{k-1})$。\n  3. 计算梯度 $\\nabla g(y^{k})$。\n  4. 执行梯度步 $z^{k} = y^{k} - \\frac{1}{L}\\nabla g(y^{k})$。\n  5. 应用近端算子 $x^{k+1} = \\operatorname{prox}_{\\frac{1}{L}h}(z^{k})$。\n\n最终输出应为四个量 $y^{k}$、$\\nabla g(y^{k})$、$x^{k+1}$ 和 $t_{k+1}$。\n\n### 步骤 2：使用提取的已知条件进行验证\n- **科学依据：** 问题描述了将 FISTA 算法应用于一维 LASSO 问题的一次迭代。FISTA 是优化和信号处理领域中一个成熟且科学可靠的算法。所有定义（梯度、Lipschitz 常数、近端算子）在此背景下都是标准的。该设置是一个教科书式的例子。\n- **适定性：** 提供了所有必要的常数、初始条件和更新规则。该过程是确定性的，并能得出下一次迭代的唯一解。\n- **客观性：** 问题陈述是精确的、定量的，并且没有任何主观性语言。\n\n该问题是自洽的、一致的，并具有科学依据。它没有表现出验证标准中列出的任何缺陷。\n\n### 步骤 3：结论与行动\n问题有效。我将继续进行求解。\n\n### 解题过程\n目标是执行一次完整的 FISTA 迭代。我们将遵循规定的步骤。\n\n首先，我们计算必要的常数。标量测量矩阵为 $A=a=2$，因此梯度 $\\nabla g$ 的 Lipschitz 常数为：\n$$L = a^{2} = 2^{2} = 4$$\n在梯度步和近端步中使用的步长是 $\\frac{1}{L} = \\frac{1}{4}$。\n\nFISTA 迭代的步骤如下：\n\n**1. 确定 $t_{k+1}$**\n加速参数 $t_{k+1}$ 由隐式关系 $t_{k+1}^{2} - t_{k+1} = t_{k}^{2}$ 确定。给定 $t_{k} = 2$。\n$$t_{k+1}^{2} - t_{k+1} = 2^{2} = 4$$\n这是一个关于 $t_{k+1}$ 的二次方程：\n$$t_{k+1}^{2} - t_{k+1} - 4 = 0$$\n使用二次方程求根公式 $t_{k+1} = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$，其中 $a=1$，$b=-1$，$c=-4$：\n$$t_{k+1} = \\frac{-(-1) \\pm \\sqrt{(-1)^{2} - 4(1)(-4)}}{2(1)} = \\frac{1 \\pm \\sqrt{1 + 16}}{2} = \\frac{1 \\pm \\sqrt{17}}{2}$$\nFISTA 中的加速参数序列是正数，因此我们取正根：\n$$t_{k+1} = \\frac{1 + \\sqrt{17}}{2}$$\n\n**2. 构成外推点 $y^{k}$**\nFISTA 中的外推步骤由下式给出：\n$$y^{k} = x^{k} + \\frac{t_{k}-1}{t_{k+1}}(x^{k}-x^{k-1})$$\n代入给定值 $x^{k}=1$、$x^{k-1}=0$、$t_{k}=2$ 以及我们计算出的 $t_{k+1}$：\n$$y^{k} = 1 + \\frac{2-1}{\\frac{1 + \\sqrt{17}}{2}}(1 - 0) = 1 + \\frac{1}{\\frac{1 + \\sqrt{17}}{2}} = 1 + \\frac{2}{1 + \\sqrt{17}}$$\n为简化起见，我们将分数的分母有理化：\n$$\\frac{2}{1 + \\sqrt{17}} = \\frac{2(1 - \\sqrt{17})}{(1 + \\sqrt{17})(1 - \\sqrt{17})} = \\frac{2(1 - \\sqrt{17})}{1 - 17} = \\frac{2(1 - \\sqrt{17})}{-16} = \\frac{\\sqrt{17} - 1}{8}$$\n现在，我们求 $y^{k}$：\n$$y^{k} = 1 + \\frac{\\sqrt{17} - 1}{8} = \\frac{8}{8} + \\frac{\\sqrt{17} - 1}{8} = \\frac{7 + \\sqrt{17}}{8}$$\n\n**3. 计算梯度 $\\nabla g(y^{k})$**\n对于一维情况，梯度 $\\nabla g(x) = A^{\\top}(Ax - b)$ 变为 $\\nabla g(x) = a(ax - b)$。\n当 $a=2$ 且 $b=3$ 时，我们有：\n$$\\nabla g(x) = 2(2x - 3)$$\n我们在点 $y^{k} = \\frac{7 + \\sqrt{17}}{8}$ 处计算该梯度：\n$$\\nabla g(y^{k}) = 2\\left(2\\left(\\frac{7 + \\sqrt{17}}{8}\\right) - 3\\right) = 2\\left(\\frac{7 + \\sqrt{17}}{4} - 3\\right)$$\n$$\\nabla g(y^{k}) = 2\\left(\\frac{7 + \\sqrt{17} - 12}{4}\\right) = 2\\left(\\frac{\\sqrt{17} - 5}{4}\\right) = \\frac{\\sqrt{17} - 5}{2}$$\n\n**4. 执行梯度步以获得 $z^{k}$**\n梯度下降步骤为：\n$$z^{k} = y^{k} - \\frac{1}{L}\\nabla g(y^{k})$$\n代入我们计算出的值：\n$$z^{k} = \\frac{7 + \\sqrt{17}}{8} - \\frac{1}{4}\\left(\\frac{\\sqrt{17} - 5}{2}\\right) = \\frac{7 + \\sqrt{17}}{8} - \\frac{\\sqrt{17} - 5}{8}$$\n$$z^{k} = \\frac{(7 + \\sqrt{17}) - (\\sqrt{17} - 5)}{8} = \\frac{7 + \\sqrt{17} - \\sqrt{17} + 5}{8} = \\frac{12}{8} = \\frac{3}{2}$$\n\n**5. 应用近端算子以获得 $x^{k+1}$**\n下一个迭代值 $x^{k+1}$ 是通过将 $h(x) = \\lambda |x|$ 的近端算子应用于 $z^{k}$ 来找到的。该算子被称为软阈值算子：\n$$x^{k+1} = \\operatorname{prox}_{\\frac{1}{L}h}(z^{k}) = \\text{soft}\\left(z^{k}, \\frac{\\lambda}{L}\\right)$$\n软阈值函数定义为 $\\text{soft}(z, \\theta) = \\operatorname{sign}(z)\\max(|z| - \\theta, 0)$。\n阈值为 $\\theta = \\frac{\\lambda}{L} = \\frac{1}{4}$。\n我们将其应用于 $z^{k} = \\frac{3}{2}$：\n$$x^{k+1} = \\text{soft}\\left(\\frac{3}{2}, \\frac{1}{4}\\right)$$\n由于 $z^{k} = \\frac{3}{2}  0$，其符号为正。我们检查 $|z^{k}|$ 是否大于阈值：\n$$|\\frac{3}{2}| = \\frac{3}{2} = \\frac{6}{4}  \\frac{1}{4}$$\n条件满足，因此我们执行收缩：\n$$x^{k+1} = \\frac{3}{2} - \\frac{1}{4} = \\frac{6}{4} - \\frac{1}{4} = \\frac{5}{4}$$\n\n所要求的四个量是 $y^{k}$、$\\nabla g(y^{k})$、$x^{k+1}$ 和 $t_{k+1}$。\n- $y^{k} = \\frac{7 + \\sqrt{17}}{8}$\n- $\\nabla g(y^{k}) = \\frac{\\sqrt{17} - 5}{2}$\n- $x^{k+1} = \\frac{5}{4}$\n- $t_{k+1} = \\frac{1 + \\sqrt{17}}{2}$\n\n这些将作为最终答案排列在一个行矩阵中。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{7 + \\sqrt{17}}{8}  \\frac{\\sqrt{17} - 5}{2}  \\frac{5}{4}  \\frac{1 + \\sqrt{17}}{2}\n\\end{pmatrix}\n}\n$$", "id": "3446895"}, {"introduction": "从理论到实践的最后一步是实现算法。这个综合性练习要求你编写完整的 FISTA 算法，并包含一个至关重要的实用技巧：回溯线搜索（backtracking line search）。当梯度项的 Lipschitz 常数未知时，该技巧是必不可少的。通过将算法逻辑转化为可执行的代码，你将能最深刻地巩固对 FISTA 的理解。[@problem_id:3446933]", "problem": "要求您实现一个基于凸分析的数值方法，以求解一系列合成的稀疏重构实例。考虑最小绝对收缩和选择算子 (LASSO) 的目标函数\n$$\n\\min_{x \\in \\mathbb{R}^n} F(x) \\equiv g(x) + h(x),\n$$\n其中光滑部分为\n$$\ng(x) \\equiv \\tfrac{1}{2}\\|A x - b\\|_2^2\n$$\n其梯度为\n$$\n\\nabla g(x) = A^\\top (A x - b),\n$$\n非光滑正则化项为缩放的L1范数\n$$\nh(x) \\equiv \\lambda \\|x\\|_1.\n$$\n$g$ 的梯度是全局 Lipschitz 连续的，存在某个常数 $L^\\star \\in (0,\\infty)$ 满足\n$$\n\\|\\nabla g(x) - \\nabla g(y)\\|_2 \\le L^\\star \\|x - y\\|_2\n$$\n对于所有 $x,y \\in \\mathbb{R}^n$。在此设定下，带回溯的快速迭代收缩阈值算法 (FISTA) 使用以下基本原理：\n- 由 $\\nabla g$ 的 Lipschitz 连续性所蕴含的二次上界，该上界为接受候选步长提供了充分下降条件的依据。\n- 缩放L1范数的近端算子，\n$$\n\\operatorname{prox}_{\\tau \\|\\cdot\\|_1}(v) = \\arg\\min_{x \\in \\mathbb{R}^n}\\left\\{\\tfrac{1}{2}\\|x - v\\|_2^2 + \\tau \\|x\\|_1\\right\\},\n$$\n即软阈值映射。\n- Nesterov 加速序列，其中 $t_1 = 1$，并根据 $t_k$ 构建外推。\n\n您的任务是：\n- 从上述基础出发，根据 $\\nabla g$ 的 Lipschitz 属性推导用于回溯的充分下降条件，以验证在当前外推点附近关于 $g$ 的局部二次上界模型。\n- 假设 Lipschitz 常数 $L^\\star$ 未知，实现用于 LASSO 目标函数 $F(x)$ 的带回溯的快速迭代收缩阈值算法 (FISTA)。以 $x_1 = 0$，$y_1 = x_1$ 和 $t_1 = 1$ 进行初始化。在每次外层迭代 $k \\in \\{1,2,\\dots\\}$ 中，执行回溯线搜索，该搜索从当前估计值 $L_k$ 开始，并将 $L_k$ 乘以一个因子 $\\eta  1$，直到满足充分下降条件。使用与 $h$ 相关的近端映射，根据外推点和当前的 $L_k$ 形成近端梯度候选点。使用 $t_{k+1} = \\tfrac{1 + \\sqrt{1 + 4 t_k^2}}{2}$ 和常规的外推方法来维持标准的 FISTA 加速更新。\n- 每当回溯线搜索在接受迭代 $k$ 之前将 $L_k$ 乘以 $\\eta$ 时，记为一次“$L_k$ 的增加”。记录在前 $10$ 次外层迭代期间（即对于 $k \\in \\{1,\\dots,10\\}$）发生的此类增加的总次数。\n\n不涉及物理单位。所有角度（如有）均假定为弧度。所有答案必须是实数值。\n\n测试套件和要求的输出：\n实现您的程序以运行以下三个合成测试用例。在每个用例中，通过 $b = A x_{\\mathrm{true}}$ 从一个植入向量 $x_{\\mathrm{true}}$ 生成 $b$（无噪声）。对于每个用例，运行带回溯的 FISTA 算法正好 $10$ 次外层迭代，并报告在这 $10$ 次迭代中发生的 $L_k$ 增加的总次数。\n\n- 用例 1（理想路径，适度回溯）：\n  - $A \\in \\mathbb{R}^{6 \\times 10}$:\n    $$\n    A = \\begin{bmatrix}\n    1  0  0  0  0  0  0.5  -0.2  0.3  0.1 \\\\\n    0  1  0  0  0  0  -0.1  0.4  0.2  -0.3 \\\\\n    0  0  1  0  0  0  0.3  -0.4  0.1  0.2 \\\\\n    0  0  0  1  0  0  -0.2  0.1  0.5  -0.1 \\\\\n    0  0  0  0  1  0  0.4  0.3  -0.2  0.2 \\\\\n    0  0  0  0  0  1  -0.3  0.2  0.1  0.4\n    \\end{bmatrix}.\n    $$\n  - $x_{\\mathrm{true}} \\in \\mathbb{R}^{10}$，其元素为\n    $$\n    x_{\\mathrm{true}} = \\begin{bmatrix} 0  1.2  0  -0.7  0  0  0  0  2.0  0 \\end{bmatrix}^\\top.\n    $$\n  - $b = A x_{\\mathrm{true}}$.\n  - $\\lambda = 0.05$，初始 $L_1 = 0.1$，回溯因子 $\\eta = 2.0$。\n\n- 用例 2（边界情况，无需回溯）：\n  - 与用例 1 中相同的 $A$ 和 $x_{\\mathrm{true}}$，其中 $b = A x_{\\mathrm{true}}$。\n  - $\\lambda = 0.05$，初始 $L_1 = 100.0$，回溯因子 $\\eta = 2.0$。\n\n- 用例 3（边缘情况，由于 $\\eta$ 较小和算子范数较大而导致多次小幅增加）：\n  - $A \\in \\mathbb{R}^{8 \\times 12}$:\n    $$\n    A = \\begin{bmatrix}\n    3.0  -2.0  1.0  0.0  1.0  -1.0  2.0  -3.0  1.5  -0.5  0.5  -1.5 \\\\\n    0.0  1.0  -1.0  2.0  -2.0  1.0  -1.5  2.5  -1.0  0.5  -0.5  1.0 \\\\\n    1.5  -1.0  0.5  -0.5  1.0  -1.5  2.0  -2.0  1.0  -1.0  0.5  -0.5 \\\\\n    -1.0  2.0  -2.0  1.0  0.0  1.0  -2.5  3.0  -1.5  1.0  -1.0  0.5 \\\\\n    2.0  -1.0  1.5  -1.0  2.0  -2.0  1.0  -1.0  0.5  -0.5  1.0  -1.5 \\\\\n    -2.0  1.0  -0.5  1.5  -1.0  2.0  -1.0  1.0  -0.5  0.5  -1.0  1.0 \\\\\n    1.0  0.5  -1.5  2.0  -2.5  1.5  -1.0  2.0  -1.0  0.5  -0.5  1.0 \\\\\n    -1.5  2.5  -1.0  0.5  1.0  -1.0  2.0  -3.0  1.5  -1.0  1.0  -0.5\n    \\end{bmatrix}.\n    $$\n  - $x_{\\mathrm{true}} \\in \\mathbb{R}^{12}$，其元素为\n    $$\n    x_{\\mathrm{true}} = \\begin{bmatrix} 0  0  2.0  -1.0  0  0  0  3.0  0  0  0  0.5 \\end{bmatrix}^\\top.\n    $$\n  - $b = A x_{\\mathrm{true}}$.\n  - $\\lambda = 0.1$，初始 $L_1 = 1.0$，回溯因子 $\\eta = 1.1$。\n\n对于每个用例，运行带回溯的 FISTA 算法正好 10 次外层迭代，并统计在这 10 次迭代中 $L_k$ 被乘以 $\\eta$ 的总次数。您的程序应产生单行输出，其中包含用方括号括起来的逗号分隔列表的结果（例如，$\\left[\\text{结果}_1,\\text{结果}_2,\\text{结果}_3\\right]$）。每个结果必须是表示其对应测试用例计数的整数。不应打印任何其他文本。", "solution": "该问题要求实现带有回溯线搜索的快速迭代收缩阈值算法 (FISTA) 来解决 LASSO 优化问题。主要任务是计算在固定迭代次数内，回溯过程中 Lipschitz 常数估计值增加的次数。\n\nLASSO 目标函数由 $F(x) = g(x) + h(x)$ 给出，其中 $g(x) = \\frac{1}{2}\\|A x - b\\|_2^2$ 是光滑的数据保真项，$h(x) = \\lambda \\|x\\|_1$ 是非光滑的稀疏性诱导正则化项。\n\n首先，我们推导回溯线搜索中使用的充分下降条件。光滑项的梯度为 $\\nabla g(x) = A^\\top (A x - b)$。题目给出 $\\nabla g$ 是 Lipschitz 连续的，常数为 $L^\\star$，这意味着对于任何 $L \\ge L^\\star$，以下不等式成立：\n$$\ng(z) \\le g(y) + \\langle \\nabla g(y), z - y \\rangle + \\frac{L}{2} \\|z - y\\|_2^2\n$$\n这个不等式被称为下降引理，它为函数 $g$ 在点 $y$ 附近的点 $z$ 处提供了一个二次上界。\n\nFISTA 是一种近端梯度方法，它将标准的近端梯度步与 Nesterov 风格的加速相结合。在每次迭代 $k$ 中，给定一个外推点 $y_k$，算法通过最小化 $F(x)$ 在 $y_k$ 周围的二次近似来寻找新点 $x_k$。这种最小化导出了近端梯度更新：\n$$\nx_k = \\operatorname{prox}_{h/L}(y_k - \\frac{1}{L}\\nabla g(y_k))\n$$\n其中 $L$ 是 Lipschitz 常数的估计值，$\\operatorname{prox}_{\\tau h}(\\cdot)$ 是函数 $\\tau h$ 的近端算子。对于 $h(x) = \\lambda \\|x\\|_1$，近端步涉及软阈值算子 $S_{\\tau}(v)_i = \\operatorname{sign}(v_i)\\max(|v_i|-\\tau, 0)$，因此：\n$$\nx_k = S_{\\lambda/L}\\left(y_k - \\frac{1}{L}A^\\top(Ay_k - b)\\right)\n$$\n由于真实的 Lipschitz 常数 $L^\\star$ 未知，因此采用回溯线搜索来在每次迭代 $k$ 中找到一个合适的步长 $1/L_k$。从对 $L_k$ 的一个估计值（例如，前一次迭代的值 $L_{k-1}$）开始，我们检查二次上界是否对我们的候选点 $x_k$ 成立。通过将 $z=x_k$ 和 $y=y_k$ 代入下降引理，可以直接导出充分下降条件。我们要求选择的 $L_k$ 满足：\n$$\ng(x_k) \\le g(y_k) + \\langle \\nabla g(y_k), x_k - y_k \\rangle + \\frac{L_k}{2} \\|x_k - y_k\\|_2^2\n$$\n如果此条件不满足，则估计值 $L_k$ 太小。我们将其乘以一个因子 $\\eta  1$（即 $L_k \\leftarrow \\eta L_k$），用新的 $L_k$ 重新计算候选点 $x_k$，然后再次检查该条件。重复此过程直到条件满足。每次乘以 $\\eta$ 都构成一次我们必须计数的“增加”。\n\n完整的带回溯的 FISTA 算法流程如下：\n\n1.  **初始化**：给定 Lipschitz 常数的初始猜测值 $L_0$，回溯因子 $\\eta  1$。设置 $x_0 = 0$，$y_1 = x_0$ 和 $t_1 = 1$。令总增加次数 $C = 0$。\n\n2.  **迭代**：对于 $k = 1, 2, \\ldots, 10$：\n    a. **回溯线搜索**：\n        i. 从一个试验 Lipschitz 常数开始，例如 $L_{trial} = L_{k-1}$。\n        ii. 计算候选点 $x_{k, trial} = S_{\\lambda/L_{trial}}\\left(y_k - \\frac{1}{L_{trial}}\\nabla g(y_k)\\right)$。\n        iii. 检查充分下降条件：如果 $g(x_{k, trial})  g(y_k) + \\langle \\nabla g(y_k), x_{k, trial} - y_k \\rangle + \\frac{L_{trial}}{2} \\|x_{k, trial} - y_k\\|_2^2$，则更新 $L_{trial} \\leftarrow \\eta L_{trial}$，计数器加一 $C \\leftarrow C+1$，并返回到步骤 (ii)。\n        iv. 一旦条件满足，设置 $L_k = L_{trial}$ 和 $x_k = x_{k, trial}$。\n\n    b. **加速步**：更新动量项：\n        i. $t_{k+1} = \\frac{1 + \\sqrt{1 + 4t_k^2}}{2}$。\n        ii. $y_{k+1} = x_k + \\frac{t_k - 1}{t_{k+1}}(x_k - x_{k-1})$。\n\n3.  在 $10$ 次迭代后，计数器 $C$ 的最终值就是给定测试用例的结果。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Runs FISTA with backtracking on a suite of test cases and reports the\n    total number of backtracking steps (increases of L).\n    \"\"\"\n\n    test_cases = [\n        {\n            \"name\": \"Case 1\",\n            \"A\": np.array([\n                [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, -0.2, 0.3, 0.1],\n                [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, -0.1, 0.4, 0.2, -0.3],\n                [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.3, -0.4, 0.1, 0.2],\n                [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, -0.2, 0.1, 0.5, -0.1],\n                [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, 0.3, -0.2, 0.2],\n                [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, -0.3, 0.2, 0.1, 0.4]\n            ]),\n            \"x_true\": np.array([0.0, 1.2, 0.0, -0.7, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0]),\n            \"lam\": 0.05,\n            \"L_initial\": 0.1,\n            \"eta\": 2.0\n        },\n        {\n            \"name\": \"Case 2\",\n            \"A\": np.array([\n                [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, -0.2, 0.3, 0.1],\n                [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, -0.1, 0.4, 0.2, -0.3],\n                [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.3, -0.4, 0.1, 0.2],\n                [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, -0.2, 0.1, 0.5, -0.1],\n                [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, 0.3, -0.2, 0.2],\n                [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, -0.3, 0.2, 0.1, 0.4]\n            ]),\n            \"x_true\": np.array([0.0, 1.2, 0.0, -0.7, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0]),\n            \"lam\": 0.05,\n            \"L_initial\": 100.0,\n            \"eta\": 2.0\n        },\n        {\n            \"name\": \"Case 3\",\n            \"A\": np.array([\n                [3.0, -2.0, 1.0, 0.0, 1.0, -1.0, 2.0, -3.0, 1.5, -0.5, 0.5, -1.5],\n                [0.0, 1.0, -1.0, 2.0, -2.0, 1.0, -1.5, 2.5, -1.0, 0.5, -0.5, 1.0],\n                [1.5, -1.0, 0.5, -0.5, 1.0, -1.5, 2.0, -2.0, 1.0, -1.0, 0.5, -0.5],\n                [-1.0, 2.0, -2.0, 1.0, 0.0, 1.0, -2.5, 3.0, -1.5, 1.0, -1.0, 0.5],\n                [2.0, -1.0, 1.5, -1.0, 2.0, -2.0, 1.0, -1.0, 0.5, -0.5, 1.0, -1.5],\n                [-2.0, 1.0, -0.5, 1.5, -1.0, 2.0, -1.0, 1.0, -0.5, 0.5, -1.0, 1.0],\n                [1.0, 0.5, -1.5, 2.0, -2.5, 1.5, -1.0, 2.0, -1.0, 0.5, -0.5, 1.0],\n                [-1.5, 2.5, -1.0, 0.5, 1.0, -1.0, 2.0, -3.0, 1.5, -1.0, 1.0, -0.5]\n            ]),\n            \"x_true\": np.array([0.0, 0.0, 2.0, -1.0, 0.0, 0.0, 0.0, 3.0, 0.0, 0.0, 0.0, 0.5]),\n            \"lam\": 0.1,\n            \"L_initial\": 1.0,\n            \"eta\": 1.1\n        }\n    ]\n\n    results = []\n\n    def soft_threshold(v, tau):\n        return np.sign(v) * np.maximum(np.abs(v) - tau, 0.0)\n\n    for case in test_cases:\n        A = case[\"A\"]\n        x_true = case[\"x_true\"]\n        lam = case[\"lam\"]\n        L_initial = case[\"L_initial\"]\n        eta = case[\"eta\"]\n        n_iters = 10\n\n        b = A @ x_true\n        n = A.shape[1]\n        \n        # Following standard FISTA notation: x_0, y_1, t_1\n        x_k = np.zeros(n)      # Corresponds to x_{k} in iteration k\n        x_km1 = np.zeros(n)    # Corresponds to x_{k-1} in iteration k\n        y_k = x_k              # y_1 = x_0\n        t_k = 1.0              # t_1\n        L = L_initial          # L_0\n        total_increases = 0\n\n        # Loop for k = 1, ..., 10\n        for _ in range(1, n_iters + 1):\n            L_inner = L\n            while True:\n                grad_g_y = A.T @ (A @ y_k - b)\n                x_k_candidate = soft_threshold(y_k - (1.0 / L_inner) * grad_g_y, lam / L_inner)\n\n                g_x = 0.5 * np.linalg.norm(A @ x_k_candidate - b)**2\n                g_y = 0.5 * np.linalg.norm(A @ y_k - b)**2\n                rhs = g_y + np.dot(grad_g_y, x_k_candidate - y_k) + \\\n                      (L_inner / 2.0) * np.linalg.norm(x_k_candidate - y_k)**2\n\n                if g_x = rhs:\n                    L = L_inner\n                    break\n                else:\n                    L_inner *= eta\n                    total_increases += 1\n            \n            # Found a valid L and x_k_candidate (which is now x_k)\n            x_km1 = x_k\n            x_k = x_k_candidate\n            \n            # Update momentum terms\n            t_kp1 = (1.0 + np.sqrt(1.0 + 4.0 * t_k**2)) / 2.0\n            y_kp1 = x_k + ((t_k - 1.0) / t_kp1) * (x_k - x_km1)\n            \n            # Update state for next iteration\n            t_k = t_kp1\n            y_k = y_kp1\n\n        results.append(total_increases)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3446933"}]}