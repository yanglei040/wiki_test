{"hands_on_practices": [{"introduction": "本练习将带您从基础上了解贝叶斯推断的核心机制。通过对一个简单高斯模型后验分布的解析推导，您将具体理解先验知识如何被观测数据更新，并洞察后验均值和方差是如何体现先验与似然之间由精度加权的折衷。[@problem_id:3544172]", "problem": "考虑在有效场论 (EFT) 中，核子-核子势的接触相互作用中出现的一个无量纲低能常数 $\\,\\theta\\,$ 的贝叶斯校准。假设单个由模拟器预测的可观测量，例如结合能残差，被建模为对 $\\,\\theta\\,$ 的线性响应，并带有加性高斯噪声：\n$$\ny \\mid \\theta \\sim \\mathcal{N}(a\\,\\theta,\\ \\sigma^{2}),\n$$\n其中 $\\,y\\,$ 是观测到的残差，$\\,a\\,$ 是来自线性化模拟器的已知灵敏度系数，$\\,\\sigma^{2}\\,$ 是模拟器和实验不确定度组合的已知方差。$\\,\\theta\\,$ 的先验是高斯分布，\n$$\n\\theta \\sim \\mathcal{N}(\\mu_{0},\\ \\tau^{2}),\n$$\n其先验均值 $\\,\\mu_{0}\\,$ 和先验方差 $\\,\\tau^{2}\\,$ 已知。在此次校准中，所有量 $\\,\\theta,\\ \\mu_{0},\\ a,\\ y\\,$ 均被视为无量纲。\n\n你将比较两个仅在先验宽度上有所不同的校准：一个使用先验方差 $\\,\\tau_{1}^{2}\\,$，另一个使用 $\\,\\tau_{2}^{2}\\,$，两者共享相同的先验均值 $\\,\\mu_{0}\\,$。从贝叶斯定理以及高斯似然和高斯先验的定义出发，推导每种情况下的后验分布，然后计算：\n\n1. 将先验方差从 $\\,\\tau_{1}^{2}\\,$ 更改为 $\\,\\tau_{2}^{2}\\,$ 所引起的后验均值的确切位移：\n$$\n\\Delta \\mu_{\\mathrm{post}} \\equiv \\mu_{\\mathrm{post}}(\\tau_{2}^{2}) - \\mu_{\\mathrm{post}}(\\tau_{1}^{2}).\n$$\n\n2. 后验方差的确切比率：\n$$\n\\rho \\equiv \\frac{\\tau_{\\mathrm{post}}^{2}(\\tau_{2}^{2})}{\\tau_{\\mathrm{post}}^{2}(\\tau_{1}^{2})}.\n$$\n\n将 $\\,\\Delta \\mu_{\\mathrm{post}}\\,$ 和 $\\,\\rho\\,$ 表示为 $\\,\\mu_{0},\\,a,\\,y,\\,\\sigma,\\,\\tau_{1}^{2},\\,\\tau_{2}^{2}\\,$ 的闭式解析函数。无需进行数值计算。请将你的最终答案以单个行向量的形式给出，其中按顺序包含 $\\,\\Delta \\mu_{\\mathrm{post}}\\,$ 和 $\\,\\rho\\,$。由于在此设置中 $\\,\\theta\\,$ 及所有相关量均为无量纲，因此不需要物理单位。无需四舍五入；请给出精确的解析表达式。", "solution": "该问题提法恰当且有科学依据，基于使用共轭先验的贝叶斯推断的标准原理。任务是当参数 $\\,\\theta\\,$ 的高斯先验方差改变时，推导其后验均值的变化以及后验方差的比率。\n\n首先，我们必须推导 $\\,\\theta\\,$ 的后验分布的一般形式。根据贝叶斯定理，后验概率密度函数 (PDF) $p(\\theta \\mid y)$ 与似然函数 $p(y \\mid \\theta)$ 和先验概率密度函数 $p(\\theta)$ 的乘积成正比：\n$$\np(\\theta \\mid y) \\propto p(y \\mid \\theta) \\, p(\\theta)\n$$\n问题陈述似然函数是高斯分布：\n$$\np(y \\mid \\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(y - a\\theta)^2}{2\\sigma^2} \\right)\n$$\n$\\,\\theta\\,$ 的先验也是高斯分布：\n$$\np(\\theta) = \\frac{1}{\\sqrt{2\\pi\\tau^2}} \\exp\\left( -\\frac{(\\theta - \\mu_0)^2}{2\\tau^2} \\right)\n$$\n将这两个分布相乘，并省略常数归一化因子，我们得到未归一化的后验分布：\n$$\np(\\theta \\mid y) \\propto \\exp\\left( -\\frac{(y - a\\theta)^2}{2\\sigma^2} \\right) \\exp\\left( -\\frac{(\\theta - \\mu_0)^2}{2\\tau^2} \\right)\n$$\n$$\np(\\theta \\mid y) \\propto \\exp\\left( -\\frac{1}{2} \\left[ \\frac{(y - a\\theta)^2}{\\sigma^2} + \\frac{(\\theta - \\mu_0)^2}{\\tau^2} \\right] \\right)\n$$\n为了确定后验分布的形式，我们对指数部分中的 $\\,\\theta\\,$ 进行配方。令方括号中的项为 $\\,Q(\\theta)\\,$：\n$$\nQ(\\theta) = \\frac{y^2 - 2ay\\theta + a^2\\theta^2}{\\sigma^2} + \\frac{\\theta^2 - 2\\mu_0\\theta + \\mu_0^2}{\\tau^2}\n$$\n我们收集包含 $\\,\\theta^2\\,$ 和 $\\,\\theta\\,$ 的项：\n$$\nQ(\\theta) = \\theta^2 \\left( \\frac{a^2}{\\sigma^2} + \\frac{1}{\\tau^2} \\right) - 2\\theta \\left( \\frac{ay}{\\sigma^2} + \\frac{\\mu_0}{\\tau^2} \\right) + \\mathrm{const.}\n$$\n其中“const.”包含不依赖于 $\\,\\theta\\,$ 的项。$\\,\\theta\\,$ 的后验分布是形式为 $\\,\\mathcal{N}(\\mu_{\\mathrm{post}}, \\tau_{\\mathrm{post}}^2)\\,$ 的高斯分布，其概率密度函数与 $\\,\\exp\\left( -\\frac{(\\theta - \\mu_{\\mathrm{post}})^2}{2\\tau_{\\mathrm{post}}^2} \\right)\\,$ 成正比。指数部分可以写为：\n$$\n-\\frac{1}{2\\tau_{\\mathrm{post}}^2} (\\theta^2 - 2\\mu_{\\mathrm{post}}\\theta + \\mu_{\\mathrm{post}}^2) = -\\frac{1}{2} \\left[ \\theta^2 \\left(\\frac{1}{\\tau_{\\mathrm{post}}^2}\\right) - 2\\theta \\left(\\frac{\\mu_{\\mathrm{post}}}{\\tau_{\\mathrm{post}}^2}\\right) + \\mathrm{const.} \\right]\n$$\n通过比较 $\\,Q(\\theta)\\,$ 中 $\\,\\theta^2\\,$ 项的系数和标准高斯形式，我们确定了后验方差的倒数，即后验精度：\n$$\n\\frac{1}{\\tau_{\\mathrm{post}}^2} = \\frac{a^2}{\\sigma^2} + \\frac{1}{\\tau^2} = \\frac{a^2\\tau^2 + \\sigma^2}{\\sigma^2\\tau^2}\n$$\n由此，后验方差 $\\,\\tau_{\\mathrm{post}}^2\\,$ 为：\n$$\n\\tau_{\\mathrm{post}}^2 = \\frac{\\sigma^2\\tau^2}{a^2\\tau^2 + \\sigma^2}\n$$\n通过比较 $\\,\\theta\\,$ 项的系数，我们找到后验均值 $\\,\\mu_{\\mathrm{post}}\\,$：\n$$\n\\frac{\\mu_{\\mathrm{post}}}{\\tau_{\\mathrm{post}}^2} = \\frac{ay}{\\sigma^2} + \\frac{\\mu_0}{\\tau^2}\n$$\n$$\n\\mu_{\\mathrm{post}} = \\tau_{\\mathrm{post}}^2 \\left( \\frac{ay}{\\sigma^2} + \\frac{\\mu_0}{\\tau^2} \\right) = \\left( \\frac{\\sigma^2\\tau^2}{a^2\\tau^2 + \\sigma^2} \\right) \\left( \\frac{ay\\tau^2 + \\mu_0\\sigma^2}{\\sigma^2\\tau^2} \\right)\n$$\n$$\n\\mu_{\\mathrm{post}} = \\frac{ay\\tau^2 + \\mu_0\\sigma^2}{a^2\\tau^2 + \\sigma^2}\n$$\n现在我们将这些一般结果应用于指定的两种情况。\n\n情况1：先验方差 $\\,\\tau_1^2\\,$。\n$$\n\\mu_{\\mathrm{post}}(\\tau_1^2) = \\frac{ay\\tau_1^2 + \\mu_0\\sigma^2}{a^2\\tau_1^2 + \\sigma^2}\n$$\n$$\n\\tau_{\\mathrm{post}}^2(\\tau_1^2) = \\frac{\\sigma^2\\tau_1^2}{a^2\\tau_1^2 + \\sigma^2}\n$$\n\n情况2：先验方差 $\\,\\tau_2^2\\,$。\n$$\n\\mu_{\\mathrm{post}}(\\tau_2^2) = \\frac{ay\\tau_2^2 + \\mu_0\\sigma^2}{a^2\\tau_2^2 + \\sigma^2}\n$$\n$$\n\\tau_{\\mathrm{post}}^2(\\tau_2^2) = \\frac{\\sigma^2\\tau_2^2}{a^2\\tau_2^2 + \\sigma^2}\n$$\n\n1.  计算后验均值的位移，$\\,\\Delta \\mu_{\\mathrm{post}} = \\mu_{\\mathrm{post}}(\\tau_2^2) - \\mu_{\\mathrm{post}}(\\tau_1^2)\\,$：\n$$\n\\Delta \\mu_{\\mathrm{post}} = \\frac{ay\\tau_2^2 + \\mu_0\\sigma^2}{a^2\\tau_2^2 + \\sigma^2} - \\frac{ay\\tau_1^2 + \\mu_0\\sigma^2}{a^2\\tau_1^2 + \\sigma^2}\n$$\n我们通分得到：\n$$\n\\Delta \\mu_{\\mathrm{post}} = \\frac{(ay\\tau_2^2 + \\mu_0\\sigma^2)(a^2\\tau_1^2 + \\sigma^2) - (ay\\tau_1^2 + \\mu_0\\sigma^2)(a^2\\tau_2^2 + \\sigma^2)}{(a^2\\tau_1^2 + \\sigma^2)(a^2\\tau_2^2 + \\sigma^2)}\n$$\n展开分子：\n$$\n(a^3y\\tau_1^2\\tau_2^2 + ay\\sigma^2\\tau_2^2 + a^2\\mu_0\\sigma^2\\tau_1^2 + \\mu_0\\sigma^4) - (a^3y\\tau_1^2\\tau_2^2 + ay\\sigma^2\\tau_1^2 + a^2\\mu_0\\sigma^2\\tau_2^2 + \\mu_0\\sigma^4)\n$$\n项 $\\,a^3y\\tau_1^2\\tau_2^2\\,$ 和 $\\,\\mu_0\\sigma^4\\,$ 消掉了。我们剩下：\n$$\nay\\sigma^2\\tau_2^2 + a^2\\mu_0\\sigma^2\\tau_1^2 - ay\\sigma^2\\tau_1^2 - a^2\\mu_0\\sigma^2\\tau_2^2 = ay\\sigma^2(\\tau_2^2 - \\tau_1^2) - a^2\\mu_0\\sigma^2(\\tau_2^2 - \\tau_1^2)\n$$\n提取公因式得到：\n$$\na\\sigma^2(y - a\\mu_0)(\\tau_2^2 - \\tau_1^2)\n$$\n因此，后验均值的位移是：\n$$\n\\Delta \\mu_{\\mathrm{post}} = \\frac{a\\sigma^2(y - a\\mu_0)(\\tau_2^2 - \\tau_1^2)}{(a^2\\tau_1^2 + \\sigma^2)(a^2\\tau_2^2 + \\sigma^2)}\n$$\n\n2.  计算后验方差的比率，$\\,\\rho = \\frac{\\tau_{\\mathrm{post}}^2(\\tau_2^2)}{\\tau_{\\mathrm{post}}^2(\\tau_1^2)}\\,$：\n$$\n\\rho = \\frac{\\frac{\\sigma^2\\tau_2^2}{a^2\\tau_2^2 + \\sigma^2}}{\\frac{\\sigma^2\\tau_1^2}{a^2\\tau_1^2 + \\sigma^2}}\n$$\n每个分数分子中的 $\\,\\sigma^2\\,$ 项消掉了：\n$$\n\\rho = \\frac{\\tau_2^2}{a^2\\tau_2^2 + \\sigma^2} \\cdot \\frac{a^2\\tau_1^2 + \\sigma^2}{\\tau_1^2}\n$$\n重新整理得到比率的最终表达式：\n$$\n\\rho = \\frac{\\tau_2^2(a^2\\tau_1^2 + \\sigma^2)}{\\tau_1^2(a^2\\tau_2^2 + \\sigma^2)}\n$$\n所求的两个量，$\\,\\Delta \\mu_{\\mathrm{post}}\\,$ 和 $\\,\\rho\\,$，现在都已表示为给定参数的闭式解析函数。最终答案以行向量的形式呈现。", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{a\\sigma^2(y - a\\mu_0)(\\tau_2^2 - \\tau_1^2)}{(a^2\\tau_1^2 + \\sigma^2)(a^2\\tau_2^2 + \\sigma^2)}  \\frac{\\tau_2^2 (a^2\\tau_1^2 + \\sigma^2)}{\\tau_1^2 (a^2\\tau_2^2 + \\sigma^2)} \\end{pmatrix}}\n$$", "id": "3544172"}, {"introduction": "在单参数更新的基础上，本练习引入了利用费雪信息矩阵 ($F$) 进行局部参数可辨识性分析的关键概念。您将为一个线性化模型推导 $F$ 矩阵，并学习如何通过其特征值来揭示哪些参数组合被数据良好约束（“刚性”），哪些则约束不佳（“松弛”），这对于设计有效的实验和构建稳健的模型至关重要。[@problem_id:3544124]", "problem": "在核相互作用参数的贝叶斯校准中，考虑一个模型预测向量 $f(\\theta) \\in \\mathbb{R}^{n}$，它表示 $n$ 个可观测量，是 $p$ 个参数 $\\theta \\in \\mathbb{R}^{p}$ 的函数。假设数据模型假定存在加性高斯噪声，其均值为零，协方差 $C \\in \\mathbb{R}^{n \\times n}$ 已知且为正定矩阵，并据此构建似然函数。在一个名义点 $\\theta_{0}$ 附近，通过 $f(\\theta) \\approx f(\\theta_{0}) + J(\\theta - \\theta_{0})$ 将模型线性化，其中 $J \\in \\mathbb{R}^{n \\times p}$ 是雅可比矩阵，其元素为在 $\\theta_{0}$ 处计算的 $J_{i\\alpha} = \\partial f_{i} / \\partial \\theta_{\\alpha}$。\n\n任务 A：从高斯似然和 Fisher 信息矩阵 $F$（定义为负对数似然的 Hessian 矩阵的期望）的定义出发，推导线性化模型的 Fisher 信息矩阵，用 $J$ 和 $C$ 表示。您的推导必须从高斯分布误差的似然函数的基本定义和 Fisher 信息的定义开始，并且不得假定任何预先给定的 $F$ 的封闭形式。\n\n任务 B：根据您的推导解释，$F$ 的特征值如何量化 $\\theta$ 各分量在 $\\theta_{0}$ 附近的局部可辨识性，即可辨识性是指 $\\theta$ 的小扰动是否能在模型预测中产生相对于噪声水平而言可区分的变化。\n\n任务 C：对于一个具体的校准案例，将 $p = 2$ 个无量纲相互作用参数校准到 $n = 3$ 个无量纲可观测量（这些量通过适当的归一化构建，使得所有雅可比矩阵元素均为无量纲，且 $C$ 是无量纲误差的协方差），假设在 $\\theta_{0}$ 处的雅可比矩阵和观测误差协方差为\n$$\nJ \\;=\\; \\begin{pmatrix}\n0.8  -0.2 \\\\\n0.5  0.3 \\\\\n-0.1  0.6\n\\end{pmatrix},\n\\qquad\nC \\;=\\; \\operatorname{diag}\\!\\big(0.04,\\, 0.01,\\, 0.09\\big).\n$$\n计算此线性化模型的 Fisher 信息矩阵，然后计算定义为 $\\det(F)$ 的 D-最优性度量。提供 $\\det(F)$ 的最终数值，四舍五入到四位有效数字。由于参数和可观测量已被无量纲化，所要求的数值答案是无量纲的。最终答案必须是指定的单个实数。", "solution": "该问题是有效的，因为它在科学上基于统计推断和计算物理学，信息完备、问题适定，且陈述客观。我们开始解答。\n\n问题分为三个任务。我们将按顺序进行解答。\n\n### 任务 A：Fisher 信息矩阵的推导\n\n目标是为带有高斯噪声的线性化模型推导 Fisher 信息矩阵 $F$。$F$ 的定义是负对数似然的 Hessian 矩阵的期望。\n\n令数据向量为 $d \\in \\mathbb{R}^{n}$。数据模型由 $d = f(\\theta) + \\epsilon$ 给出，其中 $\\theta \\in \\mathbb{R}^{p}$ 是参数向量，$\\epsilon$ 是一个随机噪声向量。问题指出，噪声是均值为零、协方差矩阵 $C \\in \\mathbb{R}^{n \\times n}$ 已知且正定的高斯噪声。因此，$\\epsilon \\sim \\mathcal{N}(0, C)$。\n\n在给定参数 $\\theta$ 的情况下，数据 $d$ 的概率密度，即似然函数 $\\mathcal{L}(\\theta|d)$，由多元高斯分布给出：\n$$\n\\mathcal{L}(\\theta|d) = p(d|\\theta) = \\frac{1}{\\sqrt{(2\\pi)^{n} \\det(C)}} \\exp\\left( -\\frac{1}{2} (d - f(\\theta))^{T} C^{-1} (d - f(\\theta)) \\right)\n$$\n对数似然 $\\ln\\mathcal{L}(\\theta|d)$ 为：\n$$\n\\ln\\mathcal{L}(\\theta|d) = -\\frac{n}{2} \\ln(2\\pi) - \\frac{1}{2} \\ln(\\det(C)) - \\frac{1}{2} (d - f(\\theta))^{T} C^{-1} (d - f(\\theta))\n$$\n负对数似然，我们记为 $\\mathcal{NLL}(\\theta)$，通过对上式取负得到。不依赖于 $\\theta$ 的项可以归为一个常数。\n$$\n\\mathcal{NLL}(\\theta) = \\text{const} + \\frac{1}{2} (d - f(\\theta))^{T} C^{-1} (d - f(\\theta))\n$$\n问题指定在名义点 $\\theta_{0}$ 附近使用 $f(\\theta)$ 的线性化模型：\n$$\nf(\\theta) \\approx f(\\theta_{0}) + J(\\theta - \\theta_{0})\n$$\n其中 $J$ 是在 $\\theta_{0}$ 处计算的雅可比矩阵。将此代入 $\\mathcal{NLL}(\\theta)$ 的表达式中：\n$$\nd - f(\\theta) \\approx d - [f(\\theta_{0}) + J(\\theta - \\theta_{0})] = [d - f(\\theta_{0})] - J(\\theta - \\theta_{0})\n$$\n令名义点处的残差为 $r_{0} = d - f(\\theta_{0})$。则 $d - f(\\theta) \\approx r_{0} - J(\\theta - \\theta_{0})$。\n线性化模型的负对数似然为：\n$$\n\\mathcal{NLL}(\\theta) \\approx \\text{const} + \\frac{1}{2} (r_{0} - J(\\theta - \\theta_{0}))^{T} C^{-1} (r_{0} - J(\\theta - \\theta_{0}))\n$$\nFisher 信息矩阵 $F$ 是一个 $p \\times p$ 矩阵，其元素为 $F_{\\alpha\\beta} = E_d \\left[ \\frac{\\partial^2 \\mathcal{NLL}}{\\partial \\theta_{\\alpha} \\partial \\theta_{\\beta}} \\right]$。首先，我们计算 $\\mathcal{NLL}(\\theta)$ 的 Hessian 矩阵 $H_{\\mathcal{NLL}}$。为此，我们先求梯度 $\\nabla_{\\theta} \\mathcal{NLL}$。\n令 $v(\\theta) = r_{0} - J(\\theta - \\theta_{0})$。二次项为 $\\frac{1}{2} v(\\theta)^{T} C^{-1} v(\\theta)$。这个二次型关于分量 $\\theta_{\\alpha}$ 的导数为：\n$$\n\\frac{\\partial}{\\partial \\theta_{\\alpha}} \\left( \\frac{1}{2} v^{T} C^{-1} v \\right) = \\left( \\frac{\\partial v}{\\partial \\theta_{\\alpha}} \\right)^{T} C^{-1} v\n$$\n向量 $v(\\theta)$ 关于 $\\theta_{\\alpha}$ 的偏导数是 $v$ 关于 $\\theta$ 的雅可比矩阵的第 $\\alpha$ 列，即 $-J$。所以，$\\frac{\\partial v_i}{\\partial \\theta_\\alpha} = -J_{i\\alpha}$。这意味着导数向量是 $-J$ 的第 $\\alpha$ 列，记为 $(-J)_{:,\\alpha}$。\n$$\n\\left( \\frac{\\partial v}{\\partial \\theta_{\\alpha}} \\right) = (-J)_{:,\\alpha}\n$$\n所以梯度分量为：\n$$\n\\frac{\\partial \\mathcal{NLL}}{\\partial \\theta_{\\alpha}} = ((-J)_{:,\\alpha})^{T} C^{-1} v = ((-J)^{T}C^{-1}v)_{\\alpha}\n$$\n完整的梯度向量为：\n$$\n\\nabla_{\\theta} \\mathcal{NLL} = (-J)^{T} C^{-1} v(\\theta) = -J^{T} C^{-1} (r_{0} - J(\\theta - \\theta_{0}))\n$$\n现在我们通过对梯度关于 $\\theta_{\\beta}$ 求导来计算 Hessian 矩阵：\n$$\n\\frac{\\partial^2 \\mathcal{NLL}}{\\partial \\theta_{\\beta} \\partial \\theta_{\\alpha}} = \\frac{\\partial}{\\partial \\theta_{\\beta}} \\left[ -J^{T} C^{-1} r_{0} + J^{T} C^{-1} J (\\theta - \\theta_{0}) \\right]_{\\alpha}\n$$\n项 $-J^{T} C^{-1} r_{0}$ 关于 $\\theta$ 是常数。第二项是矩阵 $J^{T} C^{-1} J$ 乘以向量 $(\\theta - \\theta_{0})$。\n$$\n\\frac{\\partial}{\\partial \\theta_{\\beta}} \\left[ \\sum_{\\gamma=1}^{p} (J^{T} C^{-1} J)_{\\alpha\\gamma} (\\theta_{\\gamma} - (\\theta_{0})_{\\gamma}) \\right] = (J^{T} C^{-1} J)_{\\alpha\\beta}\n$$\n这对所有的 $\\alpha, \\beta$ 都成立。因此，Hessian 矩阵为：\n$$\nH_{\\mathcal{NLL}}(\\theta) = J^{T} C^{-1} J\n$$\n线性化模型的一个关键特征是其 Hessian 矩阵是常数，既不依赖于 $\\theta$ 也不依赖于数据 $d$（包含在 $r_0$ 中）。\n最后一步是对数据 $d$ 取期望。由于 Hessian 矩阵不依赖于 $d$，其期望就是该矩阵本身。\n$$\nF = E_d[H_{\\mathcal{NLL}}(\\theta)] = E_d[J^{T} C^{-1} J] = J^{T} C^{-1} J\n$$\n任务 A 的推导到此完成。\n\n### 任务 B：F 特征值的解释\n\nFisher 信息矩阵 $F = J^{T} C^{-1} J$ 量化了可观测数据提供的关于未知参数 $\\theta$ 的信息量。$F$ 的特征值从参数可辨识性的角度为这些信息提供了一个具体的几何解释。\n\n考虑参数在名义点 $\\theta_{0}$ 附近的一个小扰动 $\\delta\\theta$。这会导致模型预测发生变化，$\\delta f = f(\\theta_0 + \\delta\\theta) - f(\\theta_0) \\approx J \\delta\\theta$。这一变化的显著性必须相对于观测噪声进行评估，而观测噪声由协方差矩阵 $C$ 表征。马氏距离的平方 $\\delta f^{T} C^{-1} \\delta f$ 衡量了变化量 $\\delta f$ 在数据空间中与原点的距离的平方，并由逆协方差加权。它是一个无量纲的度量，衡量了变化 $\\delta f$ 与典型噪声涨落的可区分程度。\n\n我们可以通过考虑二次型 $\\delta\\theta^{T} F \\delta\\theta$ 将其与 $F$ 直接联系起来：\n$$\n\\delta\\theta^{T} F \\delta\\theta = \\delta\\theta^{T} (J^{T} C^{-1} J) \\delta\\theta = (J \\delta\\theta)^{T} C^{-1} (J \\delta\\theta) = \\delta f^{T} C^{-1} \\delta f\n$$\n这表明 $\\delta\\theta^{T} F \\delta\\theta$ 正是模型输出变化的统计“距离”或可区分性的平方。\n\n现在，设 $F$ 有特征分解 $F = V \\Lambda V^{T}$，其中 $V$ 是一个正交矩阵，其列 $v_k$ 是 $F$ 的特征向量，$\\Lambda$ 是由相应特征值 $\\lambda_k$ 构成的对角矩阵。特征向量 $\\{v_k\\}$ 构成了参数空间的一组标准正交基。\n\n如果我们选择一个沿着特定特征向量方向的扰动，$\\delta\\theta = \\epsilon v_k$（其中 $\\epsilon$ 是一个小的量值），可区分性度量变为：\n$$\n\\delta\\theta^{T} F \\delta\\theta = (\\epsilon v_k)^{T} F (\\epsilon v_k) = \\epsilon^{2} v_k^{T} F v_k = \\epsilon^{2} v_k^{T} (\\lambda_k v_k) = \\epsilon^{2} \\lambda_k \\|v_k\\|^2 = \\epsilon^{2} \\lambda_k\n$$\n因此，特征值 $\\lambda_k$ 是模型输出变化的可区分性，对应于参数在特征向量 $v_k$ 方向上单位扰动平方所引起的变化。\n- **大的特征值 $\\lambda_k$** 意味着即使在由 $v_k$ 定义的参数组合方向上施加一个小的扰动，也会导致模型预测产生大的、统计上显著的变化。这种参数组合被称为“刚性的”或被数据**良好辨识的**。数据对这个方向上的变化是敏感的。\n- **小的特征值 $\\lambda_k$** 意味着即使在由 $v_k$ 定义的参数组合方向上施加一个大的扰动，也只会在模型预测中产生小的、统计上不显著的变化，这种变化很难与噪声区分开来。这种参数组合是“松弛的”或**辨识不良的**。数据几乎不能提供信息来约束这种参数组合。\n\n总之，$F$ 的特征值量化了不同参数线性组合的局部可辨识性，而特征向量定义了这些组合的方向。特征值谱揭示了参数敏感度的一个层级结构。\n\n### 任务 C：数值计算\n\n给定雅可比矩阵 $J$ 和协方差矩阵 $C$：\n$$\nJ = \\begin{pmatrix}\n0.8  -0.2 \\\\\n0.5  0.3 \\\\\n-0.1  0.6\n\\end{pmatrix},\n\\qquad\nC = \\begin{pmatrix}\n0.04  0  0 \\\\\n0  0.01  0 \\\\\n0  0  0.09\n\\end{pmatrix}\n$$\n首先，我们计算协方差矩阵的逆 $C^{-1}$。由于 $C$ 是对角矩阵，$C^{-1}$ 也是对角矩阵，其对角线上的元素是 $C$ 对应元素的倒数。\n$$\nC^{-1} = \\begin{pmatrix}\n1/0.04  0  0 \\\\\n0  1/0.01  0 \\\\\n0  0  1/0.09\n\\end{pmatrix} = \\begin{pmatrix}\n25  0  0 \\\\\n0  100  0 \\\\\n0  0  100/9\n\\end{pmatrix}\n$$\n接下来，我们使用任务 A 中的公式 $F = J^{T} C^{-1} J$ 计算 Fisher 信息矩阵。\n$J$ 的转置是：\n$$\nJ^{T} = \\begin{pmatrix}\n0.8  0.5  -0.1 \\\\\n-0.2  0.3  0.6\n\\end{pmatrix}\n$$\n我们首先计算乘积 $J^{T} C^{-1}$：\n$$\nJ^{T} C^{-1} = \\begin{pmatrix}\n0.8  0.5  -0.1 \\\\\n-0.2  0.3  0.6\n\\end{pmatrix}\n\\begin{pmatrix}\n25  0  0 \\\\\n0  100  0 \\\\\n0  0  100/9\n\\end{pmatrix}\n= \\begin{pmatrix}\n(0.8)(25)  (0.5)(100)  (-0.1)(100/9) \\\\\n(-0.2)(25)  (0.3)(100)  (0.6)(100/9)\n\\end{pmatrix}\n$$\n$$\nJ^{T} C^{-1} = \\begin{pmatrix}\n20  50  -10/9 \\\\\n-5  30  60/9\n\\end{pmatrix} = \\begin{pmatrix}\n20  50  -10/9 \\\\\n-5  30  20/3\n\\end{pmatrix}\n$$\n现在我们将此结果乘以 $J$ 得到 $F$：\n$$\nF = (J^{T} C^{-1}) J = \\begin{pmatrix}\n20  50  -10/9 \\\\\n-5  30  20/3\n\\end{pmatrix}\n\\begin{pmatrix}\n0.8  -0.2 \\\\\n0.5  0.3 \\\\\n-0.1  0.6\n\\end{pmatrix}\n$$\n$2 \\times 2$ 矩阵 $F$ 的元素是：\n$$\nF_{11} = (20)(0.8) + (50)(0.5) + (-10/9)(-0.1) = 16 + 25 + 1/9 = 41 + 1/9 = \\frac{369}{9} + \\frac{1}{9} = \\frac{370}{9}\n$$\n$$\nF_{12} = (20)(-0.2) + (50)(0.3) + (-10/9)(0.6) = -4 + 15 - 6/9 = 11 - 2/3 = \\frac{33}{3} - \\frac{2}{3} = \\frac{31}{3}\n$$\n$$\nF_{21} = (-5)(0.8) + (30)(0.5) + (20/3)(-0.1) = -4 + 15 - 2/3 = 11 - 2/3 = \\frac{31}{3}\n$$\n正如 Fisher 矩阵所预期的，$F_{12} = F_{21}$。\n$$\nF_{22} = (-5)(-0.2) + (30)(0.3) + (20/3)(0.6) = 1 + 9 + 12/3 = 10 + 4 = 14\n$$\n所以，Fisher 信息矩阵为：\n$$\nF = \\begin{pmatrix}\n370/9  31/3 \\\\\n31/3  14\n\\end{pmatrix}\n$$\nD-最优性度量是 $F$ 的行列式 $\\det(F)$。\n$$\n\\det(F) = F_{11} F_{22} - F_{12} F_{21} = \\left(\\frac{370}{9}\\right)(14) - \\left(\\frac{31}{3}\\right)^2\n$$\n$$\n\\det(F) = \\frac{370 \\times 14}{9} - \\frac{31^2}{9} = \\frac{5180}{9} - \\frac{961}{9} = \\frac{5180 - 961}{9} = \\frac{4219}{9}\n$$\n为了得到最终的数值，我们进行除法运算：\n$$\n\\det(F) = \\frac{4219}{9} \\approx 468.777...\n$$\n四舍五入到四位有效数字，我们得到 $468.8$。", "answer": "$$\n\\boxed{468.8}\n$$", "id": "3544124"}, {"introduction": "这项综合性练习将引导您从参数估计进阶到模型选择，这是贝叶斯框架下科学方法论的基石。您将实现一个完整的计算流程来计算贝叶斯因子 ($B_{AB}$)，该因子用于定量比较两种竞争的有效场论模型的证据。本练习旨在提升您运用数据不仅拟合模型，更能利用数值稳定的线性代数技术在不同理论模型之间做出判决的能力。[@problem_id:3544169]", "problem": "考虑一个用于手性有效场论（chiral EFT）校准的线性化代理，其截断阶为 $n$。在该模型中，收集在向量 $y \\in \\mathbb{R}^m$ 中的 $m$ 个测量值被建模为 $y \\approx X \\theta + \\varepsilon_{\\mathrm{exp}} + \\varepsilon_{\\mathrm{tr}}$。此处，$X \\in \\mathbb{R}^{m \\times d}$ 是关于 $d$ 个低能常数（LEC）$\\theta \\in \\mathbb{R}^d$ 的灵敏度设计矩阵。实验噪声 $\\varepsilon_{\\mathrm{exp}}$ 是一个零均值高斯分布，其协方差为对角矩阵 $\\Sigma_{\\mathrm{exp}} = \\mathrm{diag}(s_1^2,\\dots,s_m^2)$。截断误差 $\\varepsilon_{\\mathrm{tr}}$ 被建模为一个零均值高斯分布，其方差与阶数相关，为 $\\sigma_n^2 I_m$，其中 $\\sigma_n^2 = c^2 Q^{2(n+1)}$，$c0$ 是一个依赖于方案的尺度参数，$Q \\in (0,1)$ 是一个展开参数。假设 LEC 的先验分布为高斯分布，$\\theta \\sim \\mathcal{N}(0,\\tau^2 I_d)$，其中 $\\tau^20$。在这些假设下，一个指定了 $(n,Q,c)$ 的模型 $\\mathcal{M}$ 的边缘似然（贝叶斯证据）通过从高斯似然和先验中积分掉 $\\theta$ 得到。两个截断方案 $\\mathcal{M}_A$ 和 $\\mathcal{M}_B$ 之间的贝叶斯因子为 $B_{AB} = \\dfrac{p(y \\mid \\mathcal{M}_A)}{p(y \\mid \\mathcal{M}_B)}$。请使用基本的贝叶斯原理（贝叶斯定理、高斯积分和线性高斯模型）推导出一个算法。该算法在给定 $(y,X,\\{s_i^2\\}_{i=1}^m,\\tau^2)$ 以及两个方案的规格 $(n,Q_A,c_A)$ 和 $(n,Q_B,c_B)$ 的情况下，能利用线性代数精确且稳健地计算贝叶斯因子的自然对数 $\\ln B_{AB}$。\n\n你的任务是实现一个程序，为以下每个测试用例计算 $\\ln B_{AB}$。所有量都是无量纲的，不需要物理单位。你必须使用以下经过充分检验的数学事实作为基础：(i) 多元高斯积分的分布性质，(ii) 独立高斯随机变量求和的法则，(iii) 高斯变量的线性变换，以及 (iv) 贝叶斯因子定义为边缘似然的比值。除了从这些原理推导出的公式外，不要假设任何捷径公式。\n\n对于每个测试用例，模型证据必须从边缘分布 $y \\sim \\mathcal{N}(0, C)$ 构建，其协方差为 $C = \\Sigma_{\\mathrm{exp}} + \\sigma_n^2 I_m + X (\\tau^2 I_d) X^\\top$。证据的自然对数必须使用基于 $C$ 的 Cholesky 分解的数值稳定方法进行评估。\n\n测试套件的输入规格：\n- 案例 1 (正常路径)：$m = 3$， $d = 2$， $y = [1.05, 0.95, 1.10]$，$X = \\begin{bmatrix}1.0  0.2 \\\\ 1.0  -0.1 \\\\ 1.0  0.3\\end{bmatrix}$，实验方差 $\\{s_i^2\\} = [0.0025, 0.0025, 0.0025]$，$\\tau^2 = 1.0$，$n = 2$，$(Q_A,c_A) = (0.3, 1.0)$，$(Q_B,c_B) = (0.5, 1.0)$。\n- 案例 2 (低 $Q$ 边界情况)：$m = 2$， $d = 1$， $y = [0.02, -0.01]$，$X = \\begin{bmatrix}1.0 \\\\ 1.0\\end{bmatrix}$，实验方差 $\\{s_i^2\\} = [0.0001, 0.0001]$，$\\tau^2 = 0.1$，$n = 3$，$(Q_A,c_A) = (0.05, 1.0)$，$(Q_B,c_B) = (0.1, 1.0)$。\n- 案例 3 (高不确定性情况)：$m = 4$， $d = 3$， $y = [0.5, 0.7, 0.6, 0.9]$，$X = \\begin{bmatrix}1.0  0.3  -0.2 \\\\ 1.0  -0.1  0.4 \\\\ 1.0  0.2  0.1 \\\\ 1.0  -0.3  0.2\\end{bmatrix}$，实验方差 $\\{s_i^2\\} = [0.04, 0.04, 0.01, 0.09]$，$\\tau^2 = 0.5$，$n = 1$，$(Q_A,c_A) = (0.6, 1.0)$，$(Q_B,c_B) = (0.8, 1.0)$。\n- 案例 4 (固定 $Q$ 值但预因子不同)：$m = 3$， $d = 1$， $y = [2.0, 1.8, 2.2]$，$X = \\begin{bmatrix}1.0 \\\\ 0.9 \\\\ 1.1\\end{bmatrix}$，实验方差 $\\{s_i^2\\} = [0.25, 0.16, 0.36]$，$\\tau^2 = 2.0$，$n = 0$，$(Q_A,c_A) = (0.7, 1.0)$，$(Q_B,c_B) = (0.7, 2.0)$。\n\n程序要求：\n- 为每个方案实现 $\\sigma_n^2 = c^2 Q^{2(n+1)}$ 的计算。\n- 构建 $C = \\Sigma_{\\mathrm{exp}} + \\sigma_n^2 I_m + X (\\tau^2 I_d) X^\\top$ 并使用 $C$ 的 Cholesky 分解，通过零均值高斯的数值稳定闭合形式来评估 $\\ln p(y \\mid \\mathcal{M})$：$\\ln p(y \\mid \\mathcal{M}) = -\\tfrac{1}{2}\\left( m \\ln(2\\pi) + \\ln \\det C + y^\\top C^{-1} y \\right)$。\n- 对每个案例返回 $\\ln B_{AB} = \\ln p(y \\mid \\mathcal{M}_A) - \\ln p(y \\mid \\mathcal{M}_B)$。\n\n最终输出格式：\n- 你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如 $[r_1,r_2,r_3,r_4]$），其中每个 $r_i$ 是第 $i$ 个测试用例的 $\\ln B_{AB}$ 值，四舍五入到六位小数。", "solution": "该问题要求计算用于手性有效场论（EFT）校准的两个竞争理论模型 $\\mathcal{M}_A$ 和 $\\mathcal{M}_B$ 之间的贝叶斯因子的自然对数 $\\ln B_{AB}$。解决方案必须从贝叶斯统计学的基本原理推导得出，并使用数值稳健的线性代数技术实现。\n\n收集在向量 $y \\in \\mathbb{R}^m$ 中的 $m$ 个可观测量的统计模型由以下线性关系给出：\n$$\ny \\approx X \\theta + \\varepsilon_{\\mathrm{exp}} + \\varepsilon_{\\mathrm{tr}}\n$$\n此处，$X \\in \\mathbb{R}^{m \\times d}$ 是设计矩阵，表示可观测量对向量 $\\theta \\in \\mathbb{R}^d$ 中包含的 $d$ 个低能常数（LEC）的灵敏度。模型各组成部分由以下分布假设定义：\n1.  LEC 的先验分布是零均值高斯分布：$p(\\theta) = \\mathcal{N}(\\theta \\mid 0, \\tau^2 I_d)$，其中 $\\tau^2  0$ 是先验方差，$I_d$ 是 $d \\times d$ 单位矩阵。\n2.  实验误差 $\\varepsilon_{\\mathrm{exp}}$ 被建模为零均值高斯分布，其协方差矩阵为对角矩阵：$\\varepsilon_{\\mathrm{exp}} \\sim \\mathcal{N}(0, \\Sigma_{\\mathrm{exp}})$，其中 $\\Sigma_{\\mathrm{exp}} = \\mathrm{diag}(s_1^2, \\dots, s_m^2)$。\n3.  理论截断误差 $\\varepsilon_{\\mathrm{tr}}$ 被建模为零均值高斯分布，其协方差矩阵为 $\\sigma_n^2 I_m$，其中 $I_m$ 是 $m \\times m$ 单位矩阵。方差 $\\sigma_n^2$ 依赖于 EFT 截断阶 $n$、展开参数 $Q \\in (0,1)$ 和尺度参数 $c0$，其公式为 $\\sigma_n^2 = c^2 Q^{2(n+1)}$。\n\n一个具体的模型，记作 $\\mathcal{M}$，由三元组 $(n, Q, c)$ 定义。贝叶斯因子 $B_{AB}$ 是两个模型 $\\mathcal{M}_A = (n, Q_A, c_A)$ 和 $\\mathcal{M}_B = (n, Q_B, c_B)$ 的边缘似然（或证据）之比：\n$$\nB_{AB} = \\frac{p(y \\mid \\mathcal{M}_A)}{p(y \\mid \\mathcal{M}_B)}\n$$\n我们的目标是计算其自然对数，$\\ln B_{AB} = \\ln p(y \\mid \\mathcal{M}_A) - \\ln p(y \\mid \\mathcal{M}_B)$。\n\n推导过程首先确定边缘似然 $p(y \\mid \\mathcal{M})$。这通过从联合分布中积分掉参数向量 $\\theta$ 来完成：\n$$\np(y \\mid \\mathcal{M}) = \\int p(y \\mid \\theta, \\mathcal{M}) p(\\theta \\mid \\mathcal{M}) \\, d\\theta\n$$\n根据模型结构，向量 $y$ 可以表示为三个独立随机向量之和：$X\\theta$、$\\varepsilon_{\\mathrm{exp}}$ 和 $\\varepsilon_{\\mathrm{tr}}$。根据高斯分布的性质，高斯变量的线性变换仍然是高斯分布。因此，项 $X\\theta$ 是一个零均值高斯随机向量，其均值为 $\\mathbb{E}[X\\theta] = X \\mathbb{E}[\\theta] = 0$，协方差为 $\\mathrm{Cov}(X\\theta) = X \\mathrm{Cov}(\\theta) X^\\top = X(\\tau^2 I_d)X^\\top = \\tau^2 X X^\\top$。\n\n独立高斯随机向量的和也是一个高斯随机向量。和的均值是均值的和，和的协方差是协方差的和。因此，$y$ 的边缘分布是高斯分布，$y \\sim \\mathcal{N}(0, C)$，其总协方差矩阵 $C$ 由下式给出：\n$$\nC = \\mathrm{Cov}(X\\theta) + \\mathrm{Cov}(\\varepsilon_{\\mathrm{exp}}) + \\mathrm{Cov}(\\varepsilon_{\\mathrm{tr}}) = \\tau^2 X X^\\top + \\Sigma_{\\mathrm{exp}} + \\sigma_n^2 I_m\n$$\n此推导验证了问题中所述的边缘数据分布。这个零均值多元高斯分布的概率密度函数为：\n$$\np(y \\mid \\mathcal{M}) = \\frac{1}{\\sqrt{(2\\pi)^m \\det(C)}} \\exp\\left(-\\frac{1}{2} y^\\top C^{-1} y\\right)\n$$\n取自然对数得到对数边缘似然：\n$$\n\\ln p(y \\mid \\mathcal{M}) = -\\frac{1}{2} \\left( m \\ln(2\\pi) + \\ln(\\det(C)) + y^\\top C^{-1} y \\right)\n$$\n为了以数值稳定的方式计算该量，我们避免直接计算逆矩阵 $C^{-1}$ 和行列式 $\\det(C)$，因为这些计算容易出现数值下溢/上溢和不准确性。协方差矩阵 $C$ 是对称正定的，因为它是半正定矩阵和正定矩阵的和。因此，它允许唯一的 Cholesky 分解 $C = L L^\\top$，其中 $L$ 是一个对角线元素为正的下三角矩阵。\n\n这种分解方法可以稳定地计算所需的项：\n1.  对数行列式项：$\\ln(\\det(C)) = \\ln(\\det(L L^\\top)) = \\ln(\\det(L)^2) = 2 \\ln(\\det(L))$。由于三角矩阵的行列式是其对角元素的乘积，即 $\\det(L) = \\prod_{i=1}^m L_{ii}$，我们有 $\\ln(\\det(C)) = 2 \\sum_{i=1}^m \\ln(L_{ii})$。这种方法避免了计算许多小数值或大数值乘积时相关的数值问题。\n2.  二次型项 $y^\\top C^{-1} y$：我们可以将其写为 $y^\\top (L L^\\top)^{-1} y = y^\\top (L^\\top)^{-1} L^{-1} y$。令 $z = L^{-1} y$。二次型变为 $z^\\top z = \\|z\\|_2^2$。向量 $z$ 可以通过求解下三角系统 $L z = y$ 得到，这通过前向代入法实现，是一种计算高效且稳定的操作。\n\n计算单个模型 $\\mathcal{M}$ 的对数证据的算法如下：\n1.  给定模型参数 $(n,Q,c)$，计算截断方差 $\\sigma_n^2 = c^2 Q^{2(n+1)}$。\n2.  构建总协方差矩阵 $C = \\tau^2 X X^\\top + \\mathrm{diag}(\\{s_i^2\\}) + \\sigma_n^2 I_m$。\n3.  计算 Cholesky 因子 $L$，使得 $C = L L^\\top$。\n4.  计算对数行列式为 $2 \\sum_i \\ln(L_{ii})$。\n5.  使用前向代入法求解 $L z = y$ 得到 $z$。\n6.  计算二次型为 $z^\\top z$。\n7.  结合这些项以求得 $\\ln p(y \\mid \\mathcal{M})$。\n\n为了求得对数贝叶斯因子 $\\ln B_{AB}$，此算法将对模型 $\\mathcal{M}_A$（参数为 $(Q_A, c_A)$）和模型 $\\mathcal{M}_B$（参数为 $(Q_B, c_B)$）分别执行，然后将结果相减：\n$$\n\\ln B_{AB} = \\ln p(y \\mid \\mathcal{M}_A) - \\ln p(y \\mid \\mathcal{M}_B)\n$$\n对每个提供的测试用例实施此程序，以得出最终结果。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Solves the problem for all test cases specified.\n    \"\"\"\n\n    test_cases = [\n        # Case 1 (happy path)\n        {\n            \"y\": np.array([1.05, 0.95, 1.10]),\n            \"X\": np.array([[1.0, 0.2], [1.0, -0.1], [1.0, 0.3]]),\n            \"s_sq\": np.array([0.0025, 0.0025, 0.0025]),\n            \"tau_sq\": 1.0,\n            \"n\": 2,\n            \"model_A\": {\"Q\": 0.3, \"c\": 1.0},\n            \"model_B\": {\"Q\": 0.5, \"c\": 1.0},\n        },\n        # Case 2 (low-Q edge)\n        {\n            \"y\": np.array([0.02, -0.01]),\n            \"X\": np.array([[1.0], [1.0]]),\n            \"s_sq\": np.array([0.0001, 0.0001]),\n            \"tau_sq\": 0.1,\n            \"n\": 3,\n            \"model_A\": {\"Q\": 0.05, \"c\": 1.0},\n            \"model_B\": {\"Q\": 0.1, \"c\": 1.0},\n        },\n        # Case 3 (high-uncertainty regime)\n        {\n            \"y\": np.array([0.5, 0.7, 0.6, 0.9]),\n            \"X\": np.array([[1.0, 0.3, -0.2], [1.0, -0.1, 0.4], [1.0, 0.2, 0.1], [1.0, -0.3, 0.2]]),\n            \"s_sq\": np.array([0.04, 0.04, 0.01, 0.09]),\n            \"tau_sq\": 0.5,\n            \"n\": 1,\n            \"model_A\": {\"Q\": 0.6, \"c\": 1.0},\n            \"model_B\": {\"Q\": 0.8, \"c\": 1.0},\n        },\n        # Case 4 (different prefactors at fixed Q)\n        {\n            \"y\": np.array([2.0, 1.8, 2.2]),\n            \"X\": np.array([[1.0], [0.9], [1.1]]),\n            \"s_sq\": np.array([0.25, 0.16, 0.36]),\n            \"tau_sq\": 2.0,\n            \"n\": 0,\n            \"model_A\": {\"Q\": 0.7, \"c\": 1.0},\n            \"model_B\": {\"Q\": 0.7, \"c\": 2.0},\n        }\n    ]\n\n    def compute_log_evidence(y, X, s_sq, tau_sq, n, Q, c):\n        \"\"\"\n        Computes the log-marginal likelihood for a given model.\n        \n        Args:\n            y (np.ndarray): Measurement vector.\n            X (np.ndarray): Design matrix.\n            s_sq (np.ndarray): Vector of experimental variances.\n            tau_sq (float): Prior variance for LECs.\n            n (int): Truncation order.\n            Q (float): Expansion parameter.\n            c (float): Scheme-dependent scale.\n        \n        Returns:\n            float: The natural logarithm of the marginal likelihood.\n        \"\"\"\n        m = X.shape[0]\n\n        # 1. Calculate truncation error variance\n        sigma_n_sq = c**2 * Q**(2 * (n + 1))\n\n        # 2. Construct the total covariance matrix C\n        Sigma_exp = np.diag(s_sq)\n        C = tau_sq * (X @ X.T) + Sigma_exp + sigma_n_sq * np.identity(m)\n\n        # 3. Compute Cholesky factorization: C = L L^T\n        try:\n            L = linalg.cholesky(C, lower=True)\n        except linalg.LinAlgError:\n            # Fallback for numerically challenging cases, though not expected here.\n            # This would indicate the matrix is not positive definite.\n            return -np.inf\n\n        # 4. Calculate log-determinant: log(det(C)) = 2 * sum(log(diag(L)))\n        log_det_C = 2 * np.sum(np.log(np.diag(L)))\n\n        # 5. Solve L z = y for z using forward substitution\n        z = linalg.solve_triangular(L, y, lower=True)\n\n        # 6. Calculate quadratic form: y^T C^{-1} y = z^T z\n        quad_form = np.dot(z, z)\n\n        # 7. Combine terms for the log-evidence\n        log_p = -0.5 * (m * np.log(2 * np.pi) + log_det_C + quad_form)\n        \n        return log_p\n\n    results = []\n    for case in test_cases:\n        y = case[\"y\"]\n        X = case[\"X\"]\n        s_sq = case[\"s_sq\"]\n        tau_sq = case[\"tau_sq\"]\n        n = case[\"n\"]\n        \n        # Unpack model parameters\n        params_A = case[\"model_A\"]\n        params_B = case[\"model_B\"]\n\n        # Compute log-evidence for model A\n        log_p_A = compute_log_evidence(y, X, s_sq, tau_sq, n, params_A[\"Q\"], params_A[\"c\"])\n        \n        # Compute log-evidence for model B\n        log_p_B = compute_log_evidence(y, X, s_sq, tau_sq, n, params_B[\"Q\"], params_B[\"c\"])\n        \n        # Compute log Bayes factor\n        log_B_AB = log_p_A - log_p_B\n        \n        results.append(round(log_B_AB, 6))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3544169"}]}