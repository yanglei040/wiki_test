{"hands_on_practices": [{"introduction": "本练习将信息论的基本概念应用于一个简化的生物信号转导模型。通过从香农熵和互信息的基本定义出发，您将推导出一个经典的离散噪声信道——二元对称信道——中的信息传输量[@problem_id:3320002]。这个实践旨在巩固您对熵、条件熵和互信息之间关系的理解，并阐明噪声如何影响生物系统中的信息保真度。", "problem": "细胞中的一个二元信号转导通路将上游受体状态 $X \\in \\{0,1\\}$ 向下游传递给一个转录报告基因 $Y \\in \\{0,1\\}$。受体状态 $X$ 代表配体的缺失（$0$）或存在（$1$），在所考虑的环境中，配体缺失或存在的可能性相同，因此 $P(X=0) = P(X=1) = \\frac{1}{2}$。分子读出受到独立的翻转噪声 $N \\sim \\mathrm{Bernoulli}(\\epsilon)$ 的干扰，该噪声代表随机的错误磷酸化事件，因此报告基因遵循 $Y = X \\oplus N$，其中 $\\oplus$ 表示二进制异或。假设 $X$ 和 $N$ 是独立的。\n\n从香农熵和互信息的基本定义出发，且不使用任何预先推导出的捷径，推导受体状态 $X$ 和报告基因 $Y$ 之间的互信息 $I(X;Y)$ 对于此二元对称关系的闭式解析表达式，以 $\\epsilon$ 表示。最终值以比特为单位表示。最终答案必须是关于 $\\epsilon$ 的单个解析表达式。", "solution": "该问题陈述在科学上是合理的、定义明确且完整的。它描述了一个经典的二元对称信道，这是信息论中的一个基本模型，恰当地应用于嘈杂信号转导通路的生物学背景。所有必要的参数和关系都已定义，允许从基本原理进行唯一且有意义的推导。因此，将提供一个解答。\n\n目标是推导受体状态 $X$ 和报告基因状态 $Y$ 之间的互信息 $I(X;Y)$。问题指定推导必须从香农熵和互信息的基本定义开始，并且最终结果必须以比特表示。\n\n互信息 $I(X;Y)$ 可以用几种等价的方式来定义。我们将使用以下定义：\n$$I(X;Y) = H(Y) - H(Y|X)$$\n其中 $H(Y)$ 是报告基因状态 $Y$ 的熵，$H(Y|X)$ 是在给定受体状态 $X$ 的条件下 $Y$ 的条件熵。由于结果要求以比特为单位，熵计算中对数的底将是 $2$。\n\n首先，我们计算条件熵 $H(Y|X)$。根据定义：\n$$H(Y|X) = -\\sum_{x \\in \\{0,1\\}} \\sum_{y \\in \\{0,1\\}} P(X=x, Y=y) \\log_2(P(Y=y|X=x))$$\n这可以重写为：\n$$H(Y|X) = \\sum_{x \\in \\{0,1\\}} P(X=x) \\left( -\\sum_{y \\in \\{0,1\\}} P(Y=y|X=x) \\log_2(P(Y=y|X=x)) \\right)$$\n$$H(Y|X) = \\sum_{x \\in \\{0,1\\}} P(X=x) H(Y|X=x)$$\n\n我们需要确定条件概率 $P(Y|X)$。问题陈述为 $Y = X \\oplus N$，其中 $N \\sim \\mathrm{Bernoulli}(\\epsilon)$。这意味着 $P(N=1) = \\epsilon$ 和 $P(N=0) = 1-\\epsilon$。变量 $X$ 和 $N$ 是独立的。符号 $\\oplus$ 表示模2加法（异或）。\n\n情况1：$X=0$。\n输出为 $Y = 0 \\oplus N = N$。\n- $P(Y=0|X=0) = P(N=0) = 1-\\epsilon$\n- $P(Y=1|X=0) = P(N=1) = \\epsilon$\n\n情况2：$X=1$。\n输出为 $Y = 1 \\oplus N$。\n- 如果 $N=1$，则 $Y=0$。因此，$P(Y=0|X=1) = P(N=1) = \\epsilon$。\n- 如果 $N=0$，则 $Y=1$。因此，$P(Y=1|X=1) = P(N=0) = 1-\\epsilon$。\n\n现在我们可以计算条件熵 $H(Y|X=x)$：\n对于 $X=0$：\n$$H(Y|X=0) = -\\left( P(Y=0|X=0)\\log_2(P(Y=0|X=0)) + P(Y=1|X=0)\\log_2(P(Y=1|X=0)) \\right)$$\n$$H(Y|X=0) = -\\left( (1-\\epsilon)\\log_2(1-\\epsilon) + \\epsilon\\log_2(\\epsilon) \\right)$$\n\n对于 $X=1$：\n$$H(Y|X=1) = -\\left( P(Y=0|X=1)\\log_2(P(Y=0|X=1)) + P(Y=1|X=1)\\log_2(P(Y=1|X=1)) \\right)$$\n$$H(Y|X=1) = -\\left( \\epsilon\\log_2(\\epsilon) + (1-\\epsilon)\\log_2(1-\\epsilon) \\right)$$\n\n两个条件熵是相同的。我们把这个量表示为二元熵函数，$H_b(\\epsilon)$：\n$$H_b(\\epsilon) = -(\\epsilon \\log_2(\\epsilon) + (1-\\epsilon)\\log_2(1-\\epsilon))$$\n请注意，根据约定，$\\lim_{p \\to 0^+} p \\log_b(p) = 0$。\n\n现在，我们使用给定的先验概率 $P(X=0) = P(X=1) = \\frac{1}{2}$ 来计算总条件熵 $H(Y|X)$：\n$$H(Y|X) = P(X=0)H(Y|X=0) + P(X=1)H(Y|X=1)$$\n$$H(Y|X) = \\frac{1}{2} H_b(\\epsilon) + \\frac{1}{2} H_b(\\epsilon) = H_b(\\epsilon)$$\n$$H(Y|X) = -(\\epsilon \\log_2(\\epsilon) + (1-\\epsilon)\\log_2(1-\\epsilon))$$\n\n接下来，我们计算输出的熵 $H(Y)$。其定义为：\n$$H(Y) = -\\sum_{y \\in \\{0,1\\}} P(Y=y) \\log_2(P(Y=y))$$\n我们必须首先找到 $Y$ 的边缘概率分布。我们使用全概率定律：\n$$P(Y=y) = \\sum_{x \\in \\{0,1\\}} P(Y=y|X=x) P(X=x)$$\n\n对于 $Y=0$：\n$$P(Y=0) = P(Y=0|X=0)P(X=0) + P(Y=0|X=1)P(X=1)$$\n$$P(Y=0) = (1-\\epsilon) \\cdot \\frac{1}{2} + \\epsilon \\cdot \\frac{1}{2} = \\frac{1-\\epsilon+\\epsilon}{2} = \\frac{1}{2}$$\n\n对于 $Y=1$：\n$$P(Y=1) = P(Y=1|X=0)P(X=0) + P(Y=1|X=1)P(X=1)$$\n$$P(Y=1) = \\epsilon \\cdot \\frac{1}{2} + (1-\\epsilon) \\cdot \\frac{1}{2} = \\frac{\\epsilon+1-\\epsilon}{2} = \\frac{1}{2}$$\n\n输出分布是均匀的，即 $P(Y=0) = P(Y=1) = \\frac{1}{2}$。现在我们计算 $H(Y)$：\n$$H(Y) = -\\left( P(Y=0)\\log_2(P(Y=0)) + P(Y=1)\\log_2(P(Y=1)) \\right)$$\n$$H(Y) = -\\left( \\frac{1}{2}\\log_2\\left(\\frac{1}{2}\\right) + \\frac{1}{2}\\log_2\\left(\\frac{1}{2}\\right) \\right)$$\n$$H(Y) = -\\log_2\\left(\\frac{1}{2}\\right) = -(-\\log_2(2)) = \\log_2(2) = 1 \\text{ 比特}$$\n\n最后，我们将 $H(Y)$ 和 $H(Y|X)$ 的结果代入互信息的定义中：\n$$I(X;Y) = H(Y) - H(Y|X)$$\n$$I(X;Y) = 1 - \\left( -(\\epsilon \\log_2(\\epsilon) + (1-\\epsilon)\\log_2(1-\\epsilon)) \\right)$$\n$$I(X;Y) = 1 + \\epsilon \\log_2(\\epsilon) + (1-\\epsilon)\\log_2(1-\\epsilon)$$\n\n该表达式表示受体状态和报告基因状态之间的互信息，单位为比特，是噪声概率 $\\epsilon$ 的函数。", "answer": "$$ \\boxed{1 + \\epsilon \\log_2(\\epsilon) + (1-\\epsilon)\\log_2(1-\\epsilon)} $$", "id": "3320002"}, {"introduction": "在掌握了离散变量之后，我们将把互信息的概念扩展到连续变量领域，这对于分析定量的生物学测量至关重要。本练习模拟了一个具有连续输出的生化检测，其中真实的生物信号被高斯噪声所干扰 [@problem_id:3320034]。通过推导，您将揭示互信息与一个关键实验参数——信噪比（$SNR$）——之间的直接联系，从而理解测量质量如何决定我们从数据中提取生物学洞见的上限。", "problem": "一项单细胞生化报告基因检测通过测量经过对数变换和跨重复样本标准化后的荧光强度来追踪一个转录因子的活性。设细胞间的潜在标准化生物信号被建模为一个连续随机变量 $X$，且 $X \\sim \\mathcal{N}(0,\\sigma_{X}^{2})$，这代表了真实报告基因活性的细胞间变异。设测量噪声被建模为一个独立的连续随机变量 $N$，且 $N \\sim \\mathcal{N}(0,\\sigma_{N}^{2})$，其捕捉了仪器噪声和背景噪声。观测到的测量值为 $Y = X + N$。\n\n仅使用信息论中微分熵和互信息的基本定义，推导生物信号 $X$ 和观测值 $Y$ 之间互信息 $I(X;Y)$ 的闭式表达式，并以自然单位（奈特）表示。然后，在参数值为 $\\sigma_{X}^{2} = 2.0$ 和 $\\sigma_{N}^{2} = 0.5$ 的情况下计算互信息，并以奈特为单位给出最终数值。\n\n此外，请在该检测的背景下将信噪比 (SNR) 解释为比率 $\\sigma_{X}^{2}/\\sigma_{N}^{2}$，并说明它是如何控制从 $X$ 到 $Y$ 的信息传递的。\n\n将 $I(X;Y)$ 的最终数值答案以奈特表示，并四舍五入到 $4$ 位有效数字。", "solution": "该问题是有效的，因为它具有科学依据、问题明确且客观。它提出了一个来自信号处理和信息论的标准模型，并将其应用于一个真实的生物学背景。所有必要信息都已提供，以便从基本原理中推导出唯一解。\n\n两个连续随机变量 $X$ 和 $Y$ 之间的互信息 $I(X;Y)$ 是根据它们的微分熵来定义的。以自然单位（奈特）表示，其定义为：\n$$I(X;Y) = h(Y) - h(Y|X)$$\n其中 $h(Y)$ 是随机变量 $Y$ 的微分熵，$h(Y|X)$ 是在给定 $X$ 的条件下 $Y$ 的条件微分熵。我们必须推导这两项的表达式。\n\n首先，我们确定观测值 $Y$ 的分布。问题陈述 $Y = X + N$，其中 $X$ 和 $N$ 是独立的随机变量。\n信号 $X$ 服从高斯分布，$X \\sim \\mathcal{N}(0, \\sigma_{X}^{2})$。\n噪声 $N$ 也服从高斯分布，$N \\sim \\mathcal{N}(0, \\sigma_{N}^{2})$。\n两个独立高斯随机变量之和仍然是高斯随机变量。和的均值是均值之和，和的方差是方差之和。\n$Y$ 的均值为：\n$$E[Y] = E[X + N] = E[X] + E[N] = 0 + 0 = 0$$\n$Y$ 的方差为：\n$$\\text{Var}(Y) = \\text{Var(X + N)} = \\text{Var}(X) + \\text{Var}(N) = \\sigma_{X}^{2} + \\sigma_{N}^{2}$$\n因此，观测值 $Y$ 的分布为 $Y \\sim \\mathcal{N}(0, \\sigma_{X}^{2} + \\sigma_{N}^{2})$。\n\n对于服从高斯分布 $\\mathcal{N}(\\mu, \\sigma^2)$ 的连续随机变量 $Z$，其微分熵 $h(Z)$ 由以下公式给出：\n$$h(Z) = \\frac{1}{2} \\ln(2\\pi e \\sigma^2)$$\n将此公式应用于 $Y$，我们得到其微分熵：\n$$h(Y) = \\frac{1}{2} \\ln(2\\pi e (\\sigma_{X}^{2} + \\sigma_{N}^{2}))$$\n\n接下来，我们计算条件微分熵 $h(Y|X)$。这是在给定 $X$ 特定值的条件下 $Y$ 的熵的期望值。\n$$h(Y|X) = \\int p(x) h(Y|X=x) \\,dx$$\n当已知 $X$ 有一个特定值 $x$ 时，观测值 $Y$ 由 $Y = x + N$ 给出。由于在这种情况下 $x$ 是一个常数， $Y$ 中唯一的随机性来源是噪声 $N$。因此，在给定 $X=x$ 的条件下 $Y$ 的分布是一个高斯分布，其均值为 $E[x+N] = x+E[N] = x$，方差为 $\\text{Var}(x+N) = \\text{Var}(N) = \\sigma_{N}^{2}$。因此，$Y|X=x \\sim \\mathcal{N}(x, \\sigma_{N}^{2})$。\n这个条件分布的微分熵是：\n$$h(Y|X=x) = \\frac{1}{2} \\ln(2\\pi e \\sigma_{N}^{2})$$\n关键在于，这个条件熵的表达式不依赖于 $x$ 的具体值。因此，它在所有可能的 $x$ 值上的期望就是这个表达式本身：\n$$h(Y|X) = E_X[h(Y|X=x)] = \\frac{1}{2} \\ln(2\\pi e \\sigma_{N}^{2})$$\n\n现在我们可以通过将 $h(Y)$ 和 $h(Y|X)$ 的表达式代入基本定义中来计算互信息 $I(X;Y)$：\n$$I(X;Y) = h(Y) - h(Y|X) = \\frac{1}{2} \\ln(2\\pi e (\\sigma_{X}^{2} + \\sigma_{N}^{2})) - \\frac{1}{2} \\ln(2\\pi e \\sigma_{N}^{2})$$\n使用对数恒等式 $\\ln(a) - \\ln(b) = \\ln(a/b)$，我们简化该表达式：\n$$I(X;Y) = \\frac{1}{2} \\left[ \\ln\\left(\\frac{2\\pi e (\\sigma_{X}^{2} + \\sigma_{N}^{2})}{2\\pi e \\sigma_{N}^{2}}\\right) \\right]$$\n$$I(X;Y) = \\frac{1}{2} \\ln\\left(\\frac{\\sigma_{X}^{2} + \\sigma_{N}^{2}}{\\sigma_{N}^{2}}\\right)$$\n$$I(X;Y) = \\frac{1}{2} \\ln\\left(1 + \\frac{\\sigma_{X}^{2}}{\\sigma_{N}^{2}}\\right)$$\n这就是信号 $X$ 和观测值 $Y$ 之间互信息的闭式表达式，单位为奈特。\n\n问题将信噪比 (SNR) 定义为信号方差与噪声方差之比：$\\text{SNR} = \\sigma_{X}^{2}/\\sigma_{N}^{2}$。将此代入我们推导的表达式中得到：\n$$I(X;Y) = \\frac{1}{2} \\ln(1 + \\text{SNR})$$\n这个方程提供了关于 SNR 如何控制信息传递的直接解释。互信息 $I(X;Y)$ 量化了通过进行测量 $Y$ 所获得的关于生物信号 $X$ 的不确定性的减少量。\n如果 $\\text{SNR} \\to 0$ (即 $\\sigma_{N}^{2} \\gg \\sigma_{X}^{2}$)，那么 $I(X;Y) \\to \\frac{1}{2} \\ln(1) = 0$。这表示噪声淹没了信号，测量几乎不提供任何关于真实生物状态的信息。\n如果 $\\text{SNR} \\to \\infty$ (即 $\\sigma_{N}^{2} \\to 0$)，那么 $I(X;Y) \\to \\infty$。这代表了一个无噪声信道，其中测量值 $Y$ 完美地反映了信号 $X$，允许传递任意大量的信息。因此，SNR 是决定测量系统解析潜在生物学变异能力的关键参数。\n\n最后，我们针对给定的参数值计算互信息：$\\sigma_{X}^{2} = 2.0$ 和 $\\sigma_{N}^{2} = 0.5$。\n首先，计算 SNR：\n$$\\text{SNR} = \\frac{\\sigma_{X}^{2}}{\\sigma_{N}^{2}} = \\frac{2.0}{0.5} = 4.0$$\n现在，将该值代入互信息的表达式中：\n$$I(X;Y) = \\frac{1}{2} \\ln(1 + 4.0) = \\frac{1}{2} \\ln(5)$$\n使用自然对数计算：\n$$I(X;Y) \\approx \\frac{1}{2} \\times 1.6094379... \\approx 0.80471895...$$\n四舍五入到 $4$ 位有效数字，互信息为 $0.8047$ 奈特。", "answer": "$$\\boxed{0.8047}$$", "id": "3320034"}, {"introduction": "前面的练习假设我们已知概率分布，但这在实际研究中很少见。这个高级实践将带您从理论走向应用，解决如何从有限的真实数据中估算互信息的关键问题[@problem_id:3320085]。您将学习并应用一种强大的非参数方法——基于k近邻的KSG估计器，并探讨其对于数据变换的稳健性，这是利用组学数据推断基因调控网络等任务的核心技术。", "problem": "考虑两个实值随机变量 $X$ 和 $Y$，它们代表在 $N$ 个生物样本中测量的两个基因的标准化基因表达水平。对于连续变量，互信息 $I(X;Y)$ 根据第一性原理通过微分熵定义为 $I(X;Y) = H(X) + H(Y) - H(X,Y)$，其中 $H(Z) = -\\int f_Z(z) \\ln f_Z(z) \\, dz$ 且 $f_Z$ 表示 $Z$ 的概率密度函数。在计算系统生物学中，一种广泛用于从有限样本估计 $I(X;Y)$ 的非参数方法是基于 k-最近邻计数和距离。\n\n你的任务是：\n- 从上述定义和使用 k-最近邻进行局部密度估计的原理出发，介绍用于互信息 $I(X;Y)$ 的 Kraskov–Stögbauer–Grassberger (KSG) 估计器，并推导其核心公式，该公式用共享邻域的边际计数取代了显式的局部半径项。明确论证该估计器是如何通过在相同的联合 k-球半径下耦合联合邻域和边际邻域而产生的。\n- 使用推导出的估计器，为以下 $N = 6$ 组配对观测值的数据集计算互信息估计值：\n$(x_1,y_1) = (0.05, 0.05)$，$(x_2,y_2) = (0.31, 0.31)$，$(x_3,y_3) = (0.90, 0.90)$，$(x_4,y_4) = (1.57, 1.57)$，$(x_5,y_5) = (2.33, 2.33)$，$(x_6,y_6) = (3.80, 3.80)$。\n对该估计器采用以下约定：在联合空间中使用最大范数，即 $\\ell_{\\infty}$ 范数，距离为 $d_{\\infty}((x_i,y_i),(x_j,y_j)) = \\max\\{|x_j - x_i|, |y_j - y_i|\\}$；设 $k = 1$；对每个索引 $i$，将联合半径 $\\epsilon_i$ 定义为到联合空间中第 $k$ 个最近邻的距离；将边际计数 $n_x(i)$ 和 $n_y(i)$ 分别定义为满足 $|x_j - x_i|  \\epsilon_i$ 和 $|y_j - y_i|  \\epsilon_i$ 的样本数 $j \\neq i$（严格不等式）；使用自然对数，因此结果以奈特 (nats) 表示。\n- 从概念上简要讨论为什么该估计器对组学数据预处理中常用的严格单调变换（例如，$y \\mapsto g(y)$，其中 $g$ 严格递增）具有鲁棒性，并指出实践中可能出现的任何注意事项。\n\n将你最终的数值估计四舍五入到 $4$ 位有效数字。互信息以奈特 (nats) 表示。", "solution": "该问题需要分三部分作答：推导用于互信息的 Kraskov–Stögbauer–Grassberger (KSG) 估计器，将其应用于一个特定数据集，以及对其对单调变换的鲁棒性进行概念性讨论。\n\n### 第1部分：KSG 估计器的推导\n\n两个连续随机变量 $X$ 和 $Y$ 之间的互信息 $I(X;Y)$ 根据它们的微分熵定义为：\n$$I(X;Y) = H(X) + H(Y) - H(X,Y)$$\n其中 $H(Z) = -\\int f_Z(z) \\ln f_Z(z) \\, dz$ 是具有概率密度函数（PDF）$f_Z(z)$ 的随机变量 $Z$ 的微分熵。\n\n由于估计概率密度函数 $f_X$、$f_Y$ 和 $f_{XY}$ 的困难，从有限样本集 $\\{(x_i, y_i)\\}_{i=1}^N$ 直接估计该量具有挑战性。KSG 方法提供了一种巧妙的解决方案，它使用 k-最近邻 (k-NN) 距离来估计熵，并使得系统性偏差得以抵消。\n\n对于来自 $N$ 个样本的 $d$ 维随机变量 $Z$，其微分熵的 Kozachenko-Leonenko (KL) 估计器由下式给出：\n$$ \\hat{H}(Z) = \\psi(N) - \\psi(k) + \\ln(c_d) + \\frac{d}{N} \\sum_{i=1}^N \\ln(\\rho_i(k)) $$\n其中 $\\psi(\\cdot)$ 是双伽玛函数，$k$ 是所考虑的邻居数量，$\\rho_i(k)$ 是从样本 $z_i$ 到其第 $k$ 个最近邻的距离，$c_d$ 是所选范数下 $d$ 维单位球的体积。\n\n将 KL 估计器简单地应用于互信息定义中的每一项，将涉及在三个不同的空间（$X$ 的一维空间、$Y$ 的一维空间以及 $(X,Y)$ 的二维联合空间）中寻找 k-NN 距离，从而导致三组不同的距离。这会引入无法抵消的偏差。\n\nKSG 估计器的核心思想是通过为每个点使用在联合空间中确定的单一距离尺度来耦合估计过程。具体来说，对于每个点 $z_i=(x_i, y_i)$：\n1. 我们在联合 $(X,Y)$ 空间中使用选定的范数（这里是最大范数，$\\ell_{\\infty}$）确定到其第 $k$ 个最近邻的距离 $\\epsilon_i$。这在 $(x_i, y_i)$ 周围定义了一个局部区域（一个边长为 $2\\epsilon_i$ 的正方形），该区域恰好包含 $k$ 个其他点。\n2. 我们不直接在边际空间中寻找 k-NN，而是将这个联合空间区域投影到边际坐标轴上。这在 $x_i$ 和 $y_i$ 周围定义了长度为 $2\\epsilon_i$ 的区间。\n3. 然后，我们计算其边际坐标落入这些区间内的点的数量。设 $n_x(i)$ 为满足 $|x_j - x_i|  \\epsilon_i$ 的点 $j \\neq i$ 的数量，设 $n_y(i)$ 为满足 $|y_j - y_i|  \\epsilon_i$ 的点 $j \\neq i$ 的数量。\n\n熵估计器针对此方案进行了调整。联合熵估计器保持标准的 KL 形式，因为我们在这个空间中固定了 $k$ 个邻居：\n$$ \\hat{H}(X,Y) = \\psi(N) - \\psi(k) + \\ln(c_2) + \\frac{2}{N} \\sum_{i=1}^N \\ln(\\epsilon_i) $$\n其中 $d=2$。\n\n对于边际熵，邻居的数量不再固定为 $k$，而是对每个点 $i$ 变化为 $n_x(i)$ 和 $n_y(i)$。对于可变数量的邻居，熵估计器变成了对邻居计数的双伽玛函数的平均值。在 $x_i$ 周围半径为 $\\epsilon_i$ 的边际球中的总点数是 $n_x(i)+1$（包括点 $i$ 本身）。边际熵的估计器是：\n$$ \\hat{H}(X) \\approx \\psi(N) - \\frac{1}{N}\\sum_{i=1}^N \\psi(n_x(i)+1) + \\ln(c_1) + \\frac{1}{N}\\sum_{i=1}^N \\ln(\\epsilon_i) $$\n$$ \\hat{H}(Y) \\approx \\psi(N) - \\frac{1}{N}\\sum_{i=1}^N \\psi(n_y(i)+1) + \\ln(c_1) + \\frac{1}{N}\\sum_{i=1}^N \\ln(\\epsilon_i) $$\n这里，半径 $\\epsilon_i$ 是在联合空间中确定的同一个半径。\n\n将这些代入互信息公式 $\\hat{I}(X;Y) = \\hat{H}(X) + \\hat{H}(Y) - \\hat{H}(X,Y)$：\n$$ \\hat{I}(X;Y) = \\left( \\psi(N) - \\langle\\psi(n_x+1)\\rangle \\!+\\! \\ln(c_1) \\!+\\! \\langle\\ln\\epsilon\\rangle \\right) \\!+\\! \\left( \\psi(N) - \\langle\\psi(n_y+1)\\rangle \\!+\\! \\ln(c_1) \\!+\\! \\langle\\ln\\epsilon\\rangle \\right) \\!-\\! \\left( \\psi(N) - \\psi(k) \\!+\\! \\ln(c_2) \\!+\\! 2\\langle\\ln\\epsilon\\rangle \\right) $$\n其中 $\\langle \\cdot \\rangle$ 表示对所有 $N$ 个点的平均。\n\n涉及半径平均对数的项 $\\langle\\ln\\epsilon\\rangle$ 被抵消：$\\langle\\ln\\epsilon\\rangle + \\langle\\ln\\epsilon\\rangle - 2\\langle\\ln\\epsilon\\rangle = 0$。\n涉及单位球体积的项也被抵消。对于最大范数，一个 $d$ 维单位球的体积是 $c_d = 2^d$。因此，$c_1=2$ 且 $c_2=4$，所以 $\\ln(c_2) = \\ln(4) = 2\\ln(2) = 2\\ln(c_1)$。体积项也相互抵消：$\\ln(c_1) + \\ln(c_1) - \\ln(c_2) = 0$。\n\nKSG 互信息估计器的最终表达式非常简洁，因为它仅依赖于邻居计数，并且不含显式的度量相关项：\n$$ \\hat{I}(X;Y) = \\psi(N) + \\psi(k) - \\frac{1}{N} \\sum_{i=1}^N \\left[ \\psi(n_x(i)+1) + \\psi(n_y(i)+1) \\right] $$\n这个公式用局部邻居计数取代了对局部半径的显式估计，这是 KSG 方法鲁棒性的核心。\n\n### 第2部分：对给定数据集的计算\n\n我们给定 $N=6$ 个数据点，并且必须使用 $k=1$ 和最大（$\\ell_{\\infty}$）范数。\n数据点是：$P_1(0.05, 0.05)$、$P_2(0.31, 0.31)$、$P_3(0.90, 0.90)$、$P_4(1.57, 1.57)$、$P_5(2.33, 2.33)$、$P_6(3.80, 3.80)$。\n数据点落在直线 $y=x$ 上。因此，最大范数距离简化为 $d_{\\infty}(P_i, P_j) = \\max\\{|x_j - x_i|, |y_j - y_i|\\} = |x_j - x_i|$。\n\n首先，我们为每个点 $i$ 找到到第 $k=1$ 个最近邻的距离 $\\epsilon_i$。\n- 对于 $P_1(0.05, 0.05)$：最近的点是 $P_2$。$\\epsilon_1 = |0.31 - 0.05| = 0.26$。\n- 对于 $P_2(0.31, 0.31)$：最近的点是 $P_1$。$\\epsilon_2 = |0.05 - 0.31| = 0.26$。\n- 对于 $P_3(0.90, 0.90)$：最近的点是 $P_2$。$\\epsilon_3 = |0.31 - 0.90| = 0.59$。\n- 对于 $P_4(1.57, 1.57)$：最近的点是 $P_3$。$\\epsilon_4 = |0.90 - 1.57| = 0.67$。\n- 对于 $P_5(2.33, 2.33)$：最近的点是 $P_4$。$\\epsilon_5 = |1.57 - 2.33| = 0.76$。\n- 对于 $P_6(3.80, 3.80)$：最近的点是 $P_5$。$\\epsilon_6 = |2.33 - 3.80| = 1.47$。\n\n接下来，我们计算边际计数 $n_x(i)$ 和 $n_y(i)$。因为对所有 $j$ 都有 $y_j=x_j$，所以很明显对所有 $i$ 都有 $n_x(i) = n_y(i)$。我们需要找到满足 $|x_j - x_i|  \\epsilon_i$ 的点 $j \\neq i$ 的数量。\n- 对于 $i=1$：$\\epsilon_1 = 0.26$。最近的点 $P_2$ 的距离是 $|x_2 - x_1| = 0.26$。因为不等式是严格的（``），所以没有点满足该条件。因此，$n_x(1)=0$。\n- 对于 $i=2$：$\\epsilon_2 = 0.26$。最近的点 $P_1$ 的距离是 $|x_1 - x_2| = 0.26$。同样，由于严格不等式，$n_x(2)=0$。\n- 对于所有其他点 $i=3, 4, 5, 6$，第 $k=1$ 个最近邻是排序列表中的相邻点，其距离恰好是 $\\epsilon_i$。没有其他点比最近邻更近。因此，对于所有 $i \\in \\{1, 2, 3, 4, 5, 6\\}$，我们有 $n_x(i)=0$ 和 $n_y(i)=0$。\n\n现在我们将这些值代入估计器公式：\n$$ \\hat{I}(X;Y) = \\psi(6) + \\psi(1) - \\frac{1}{6} \\sum_{i=1}^6 \\left[ \\psi(0+1) + \\psi(0+1) \\right] $$\n$$ \\hat{I}(X;Y) = \\psi(6) + \\psi(1) - \\frac{1}{6} \\sum_{i=1}^6 \\left[ 2\\psi(1) \\right] $$\n$$ \\hat{I}(X;Y) = \\psi(6) + \\psi(1) - \\frac{1}{6} (6 \\times 2\\psi(1)) $$\n$$ \\hat{I}(X;Y) = \\psi(6) + \\psi(1) - 2\\psi(1) = \\psi(6) - \\psi(1) $$\n\n双伽玛函数与欧拉-马斯刻若尼常数 $\\gamma$ 和调和数 $H_n$ 相关。具体来说，$\\psi(1) = -\\gamma$，对于整数 $m > 1$，$\\psi(m) = H_{m-1} - \\gamma$。\n$H_5 = 1 + \\frac{1}{2} + \\frac{1}{3} + \\frac{1}{4} + \\frac{1}{5} = \\frac{60+30+20+15+12}{60} = \\frac{137}{60}$。\n所以，$\\psi(6) = H_5 - \\gamma = \\frac{137}{60} - \\gamma$。\n\n将这些代入我们对 $\\hat{I}(X;Y)$ 的表达式中：\n$$ \\hat{I}(X;Y) = \\left(\\frac{137}{60} - \\gamma\\right) - (-\\gamma) = \\frac{137}{60} $$\n作为小数，$\\frac{137}{60} \\approx 2.28333...$。四舍五入到 4 位有效数字，互信息估计值为 $2.283$ 奈特。\n\n### 第3部分：关于对单调变换的鲁棒性的讨论\n\n根据定义，互信息 $I(X;Y)$ 在对变量进行独立的、严格单调（因此可逆）的变换下是不变的，即对于严格单调函数 $g$ 和 $h$，有 $I(g(X); h(Y)) = I(X;Y)$。这一性质对于分析生物数据至关重要，因为在实验过程或预处理（例如，对数缩放、分位数归一化）中，测量值常常会经过非线性但单调的变换。\n\nKSG 估计器被认为对此类变换具有鲁棒性，这是由于其基于排序、无度量的公式。上面的推导表明，$\\hat{I}(X;Y)$ 的最终公式不依赖于绝对距离 $\\epsilon_i$ 或特定的度量选择（因为体积项 $c_d$ 被抵消了）。它仅依赖于整数邻居计数 $k$、$N$、$n_x(i)$ 和 $n_y(i)$。这些计数由点的局部排序决定。单调变换会扭曲空间，但保留了沿每个轴的数据的一维顺序。对于数据密集的大样本，这种扭曲对局部邻域有平滑的影响，邻居的排序结构往往得以保留，从而得到稳定的互信息估计。与基于分箱（直方图）的方法相比，这是一个显著的优势，因为后者对数据相对于分箱边界的缩放和变换方式高度敏感。\n\n然而，这种鲁棒性存在重要的注意事项，特别是对于有限样本：\n1.  **实践中的非不变性**：对于有限的 $N$，KSG 估计器通常*不是*严格不变的。非线性变换 $x' = g(x)$ 会改变联合空间中的相对距离。在 $(X,Y)$ 空间中是第 $k$ 个最近邻的点，在 $(X',Y)$ 空间中可能不再是第 $k$ 个最近邻。这会改变定义半径 $\\epsilon_i$ 的邻居的身份，进而可能改变计数 $n_x(i)$ 和 $n_y(i)$，导致不同的互信息估计值。\n2.  **对范数的依赖性**：不变性的程度也取决于所使用的范数。例如，在最大范数下，估计器仅在缩放因子相等（即 $|a|=|c|$）时，才对独立的仿射变换 $x \\to ax+b$, $y \\to cy+d$ 保持不变。每个轴上不同的缩放因子将改变最近邻搜索的几何形状。\n3.  **有限样本效应**：鲁棒性是一种渐近性质，对于大的 $N$ 表现更好。对于小而稀疏的数据集，如此问题中的数据集，少数最近邻身份的改变可能对最终估计产生更显著的影响。\n\n总之，KSG 估计器的鲁棒性源于其抵消了度量相关项的公式，转而依赖于局部邻居计数。虽然对于有限样本并非严格不变，但这种设计使其在抵抗单调数据变换方面比许多其他估计器稳定得多，这是计算生物学中一个非常理想的特性。", "answer": "$$\\boxed{2.283}$$", "id": "3320085"}]}