{"hands_on_practices": [{"introduction": "要真正掌握图神经网络，我们必须从其核心计算机制入手。本练习将引导你手动完成一次完整的消息传递迭代，包括消息的计算、聚合和节点状态的更新。通过这个详细的计算过程，你将直观地理解GNN节点是如何通过汇集其邻居节点的信息来学习和更新自身表示的，从而为后续更复杂的应用打下坚实的基础。", "problem": "考虑一个包含 $3$ 个基因 $G_1$、$G_2$ 和 $G_3$ 的有向基因调控网络。每条有向边都标有一个调控基序：激活或抑制。节点特征表示基础转录倾向。您将执行一次消息传递神经网络 (Message Passing Neural Network, MPNN) 的迭代 ($t=0 \\to t=1$)，其中每条边的消息函数 $\\psi$ 是线性的，每个节点的更新函数 $\\phi$ 是门控循环单元 (Gated Recurrent Unit, GRU)。使用消息传递神经网络 (MPNN) 的标准求和聚合定义以及标准的门控循环单元 (GRU) 方程。隐藏状态维度为 $d_h=2$，节点特征维度为 $d_x=1$，边特征维度为 $d_e=2$（基序的独热编码：激活为 $[1,0]$，抑制为 $[0,1]$）。\n\n网络结构：\n- 有向边：$G_1 \\to G_2$ (激活)，$G_3 \\to G_2$ (激活)，$G_2 \\to G_1$ (抑制)，$G_2 \\to G_3$ (抑制)。\n- 节点特征 $x_i$：$x_1 = 0.3$，$x_2 = -0.1$，$x_3 = 0.5$。\n- 初始隐藏状态 $h_i^{(0)}$：$h_1^{(0)} = [0.2, -0.1]$，$h_2^{(0)} = [0.0, 0.3]$，$h_3^{(0)} = [-0.2, 0.1]$。\n- 对于每条边 $j \\to i$，激活的边特征 $e_{j \\to i}$ 为 $[1,0]$，抑制的为 $[0,1]$。\n\n消息函数 $\\psi$ 的输入为拼接向量 $[h_j^{(0)}; x_j; e_{j \\to i}] \\in \\mathbb{R}^{5}$，通过一个共享线性映射输出 $m_{j \\to i} \\in \\mathbb{R}^{2}$，其参数为\n$$\nW_m = \n\\begin{pmatrix}\n0.5  -0.3  0.2  0.1  -0.1 \\\\\n-0.4  0.6  -0.2  0.05  0.15\n\\end{pmatrix},\n\\quad\nb_m = \n\\begin{pmatrix}\n0.01 \\\\\n-0.02\n\\end{pmatrix}.\n$$\n\n通过求和聚合到节点 $G_2$ 的传入消息：$M_2 = \\sum_{j \\in \\mathcal{N}_{\\text{in}}(2)} m_{j \\to 2}$，其中 $\\mathcal{N}_{\\text{in}}(2) = \\{1, 3\\}$。\n\n更新函数 $\\phi$ 是一个 GRU，其输入为 $M_2 \\in \\mathbb{R}^{2}$ 和前一个隐藏状态 $h_2^{(0)} \\in \\mathbb{R}^{2}$。GRU 参数（在各节点间共享）如下：\n$$\nW_z = \n\\begin{pmatrix}\n0.2  -0.1 \\\\\n0.05  0.3\n\\end{pmatrix},\\quad\nU_z = \n\\begin{pmatrix}\n-0.2  0.4 \\\\\n0.1  -0.3\n\\end{pmatrix},\\quad\nb_z = \n\\begin{pmatrix}\n0.01 \\\\\n-0.02\n\\end{pmatrix},\n$$\n$$\nW_r = \n\\begin{pmatrix}\n-0.1  0.2 \\\\\n0.3  -0.05\n\\end{pmatrix},\\quad\nU_r = \n\\begin{pmatrix}\n0.25  -0.15 \\\\\n-0.2  0.35\n\\end{pmatrix},\\quad\nb_r = \n\\begin{pmatrix}\n0.0 \\\\\n0.01\n\\end{pmatrix},\n$$\n$$\nW_h = \n\\begin{pmatrix}\n0.3  0.1 \\\\\n-0.2  0.25\n\\end{pmatrix},\\quad\nU_h = \n\\begin{pmatrix}\n0.5  -0.3 \\\\\n0.2  0.4\n\\end{pmatrix},\\quad\nb_h = \n\\begin{pmatrix}\n0.0 \\\\\n0.0\n\\end{pmatrix}.\n$$\n\n执行一次完整的 MPNN 迭代，计算节点 $G_2$ 更新后的隐藏状态 $h_2^{(1)}$。将 $h_2^{(1)}$ 的第一个分量作为实数报告。将您的答案四舍五入到四位有效数字。不需要单位。", "solution": "用户需要找到节点 $G_2$ 在一次消息传递神经网络 (MPNN) 迭代后更新的隐藏状态 $h_2^{(1)}$ 的第一个分量。该过程涉及三个主要步骤：消息计算、消息聚合以及使用门控循环单元 (GRU) 进行状态更新。\n\n### 步骤 1：问题验证\n根据指定标准验证问题陈述。\n\n**提取的已知条件：**\n- **网络结构：** 一个包含 $3$ 个节点 ($G_1, G_2, G_3$) 的有向图，边为 $G_1 \\to G_2$ (激活)，$G_3 \\to G_2$ (激活)，$G_2 \\to G_1$ (抑制)，$G_2 \\to G_3$ (抑制)。\n- **节点特征：** $x_1 = 0.3$，$x_2 = -0.1$，$x_3 = 0.5$。这些是标量 ($d_x=1$)。\n- **初始隐藏状态：** $h_1^{(0)} = \\begin{pmatrix} 0.2 \\\\ -0.1 \\end{pmatrix}$，$h_2^{(0)} = \\begin{pmatrix} 0.0 \\\\ 0.3 \\end{pmatrix}$，$h_3^{(0)} = \\begin{pmatrix} -0.2 \\\\ 0.1 \\end{pmatrix}$。隐藏状态维度为 $d_h=2$。\n- **边特征：** 独热编码，维度 $d_e=2$。激活为 $e_{act} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$，抑制为 $e_{inh} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$。\n- **消息函数 ($\\psi$)：** $m_{j \\to i} = W_m [h_j^{(0)}; x_j; e_{j \\to i}] + b_m$。输入分别是 $\\mathbb{R}^{2}$、$\\mathbb{R}^{1}$ 和 $\\mathbb{R}^{2}$ 中向量的拼接，形成一个 $\\mathbb{R}^{5}$ 中的向量。输出是消息 $m_{j \\to i} \\in \\mathbb{R}^{2}$。\n   - $W_m = \\begin{pmatrix} 0.5  -0.3  0.2  0.1  -0.1 \\\\ -0.4  0.6  -0.2  0.05  0.15 \\end{pmatrix}$，$b_m = \\begin{pmatrix} 0.01 \\\\ -0.02 \\end{pmatrix}$。\n- **聚合函数：** 对节点 $G_2$ 的传入消息求和。向 $G_2$ 发送消息的邻居集合为 $\\mathcal{N}_{\\text{in}}(2) = \\{1, 3\\}$。聚合后的消息为 $M_2 = \\sum_{j \\in \\mathcal{N}_{\\text{in}}(2)} m_{j \\to 2}$。\n- **更新函数 ($\\phi$)：** 一个 GRU，输入为 $M_2$ 和前一个隐藏状态 $h_2^{(0)}$。\n   - $W_z = \\begin{pmatrix} 0.2  -0.1 \\\\ 0.05  0.3 \\end{pmatrix}, U_z = \\begin{pmatrix} -0.2  0.4 \\\\ 0.1  -0.3 \\end{pmatrix}, b_z = \\begin{pmatrix} 0.01 \\\\ -0.02 \\end{pmatrix}$\n   - $W_r = \\begin{pmatrix} -0.1  0.2 \\\\ 0.3  -0.05 \\end{pmatrix}, U_r = \\begin{pmatrix} 0.25  -0.15 \\\\ -0.2  0.35 \\end{pmatrix}, b_r = \\begin{pmatrix} 0.0 \\\\ 0.01 \\end{pmatrix}$\n   - $W_h = \\begin{pmatrix} 0.3  0.1 \\\\ -0.2  0.25 \\end{pmatrix}, U_h = \\begin{pmatrix} 0.5  -0.3 \\\\ 0.2  0.4 \\end{pmatrix}, b_h = \\begin{pmatrix} 0.0 \\\\ 0.0 \\end{pmatrix}$\n- **任务：** 计算 $h_2^{(1)}$ 的第一个分量，并四舍五入到四位有效数字。\n\n**验证结论：**\n问题是**有效的**。它在科学上基于图神经网络的原理及其在计算生物学中的应用。问题陈述清晰，所有必要的参数、初始条件和函数都有明确定义。所有矩阵和向量的维度都是一致的。问题是客观的，没有歧义。这是一个基于所提供定义的直接计算任务。\n\n### 步骤 2：求解计算\n\n计算分三个阶段进行：消息生成、聚合和状态更新。\n\n**阶段 I：消息生成**\n我们需要计算来自 $G_2$ 的邻居节点 $G_1$ 和 $G_3$ 的消息。\n\n1.  **从 $G_1$ 到 $G_2$ 的消息 ($m_{1 \\to 2}$):**\n    边 $G_1 \\to G_2$ 是激活，因此 $e_{1 \\to 2} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$。\n    消息函数的输入向量是拼接后的 $[h_1^{(0)}; x_1; e_{1 \\to 2}]$:\n    $$ v_1 = \\begin{pmatrix} 0.2 \\\\ -0.1 \\\\ 0.3 \\\\ 1 \\\\ 0 \\end{pmatrix} $$\n    消息为 $m_{1 \\to 2} = W_m v_1 + b_m$:\n    $$ m_{1 \\to 2} = \\begin{pmatrix} 0.5  -0.3  0.2  0.1  -0.1 \\\\ -0.4  0.6  -0.2  0.05  0.15 \\end{pmatrix} \\begin{pmatrix} 0.2 \\\\ -0.1 \\\\ 0.3 \\\\ 1 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0.01 \\\\ -0.02 \\end{pmatrix} $$\n    $$ m_{1 \\to 2} = \\begin{pmatrix} (0.5)(0.2) + (-0.3)(-0.1) + (0.2)(0.3) + (0.1)(1) + (-0.1)(0) \\\\ (-0.4)(0.2) + (0.6)(-0.1) + (-0.2)(0.3) + (0.05)(1) + (0.15)(0) \\end{pmatrix} + \\begin{pmatrix} 0.01 \\\\ -0.02 \\end{pmatrix} $$\n    $$ m_{1 \\to 2} = \\begin{pmatrix} 0.1 + 0.03 + 0.06 + 0.1 \\\\ -0.08 - 0.06 - 0.06 + 0.05 \\end{pmatrix} + \\begin{pmatrix} 0.01 \\\\ -0.02 \\end{pmatrix} = \\begin{pmatrix} 0.29 \\\\ -0.15 \\end{pmatrix} + \\begin{pmatrix} 0.01 \\\\ -0.02 \\end{pmatrix} = \\begin{pmatrix} 0.30 \\\\ -0.17 \\end{pmatrix} $$\n\n2.  **从 $G_3$ 到 $G_2$ 的消息 ($m_{3 \\to 2}$):**\n    边 $G_3 \\to G_2$ 是激活，因此 $e_{3 \\to 2} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$。\n    输入向量是 $[h_3^{(0)}; x_3; e_{3 \\to 2}]$:\n    $$ v_3 = \\begin{pmatrix} -0.2 \\\\ 0.1 \\\\ 0.5 \\\\ 1 \\\\ 0 \\end{pmatrix} $$\n    消息为 $m_{3 \\to 2} = W_m v_3 + b_m$:\n    $$ m_{3 \\to 2} = \\begin{pmatrix} 0.5  -0.3  0.2  0.1  -0.1 \\\\ -0.4  0.6  -0.2  0.05  0.15 \\end{pmatrix} \\begin{pmatrix} -0.2 \\\\ 0.1 \\\\ 0.5 \\\\ 1 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0.01 \\\\ -0.02 \\end{pmatrix} $$\n    $$ m_{3 \\to 2} = \\begin{pmatrix} (0.5)(-0.2) + (-0.3)(0.1) + (0.2)(0.5) + (0.1)(1) + (-0.1)(0) \\\\ (-0.4)(-0.2) + (0.6)(0.1) + (-0.2)(0.5) + (0.05)(1) + (0.15)(0) \\end{pmatrix} + \\begin{pmatrix} 0.01 \\\\ -0.02 \\end{pmatrix} $$\n    $$ m_{3 \\to 2} = \\begin{pmatrix} -0.1 - 0.03 + 0.1 + 0.1 \\\\ 0.08 + 0.06 - 0.1 + 0.05 \\end{pmatrix} + \\begin{pmatrix} 0.01 \\\\ -0.02 \\end{pmatrix} = \\begin{pmatrix} 0.07 \\\\ 0.09 \\end{pmatrix} + \\begin{pmatrix} 0.01 \\\\ -0.02 \\end{pmatrix} = \\begin{pmatrix} 0.08 \\\\ 0.07 \\end{pmatrix} $$\n\n**阶段 II：消息聚合**\n节点 $G_2$ 的聚合消息是传入消息的总和：\n$$ M_2 = m_{1 \\to 2} + m_{3 \\to 2} = \\begin{pmatrix} 0.30 \\\\ -0.17 \\end{pmatrix} + \\begin{pmatrix} 0.08 \\\\ 0.07 \\end{pmatrix} = \\begin{pmatrix} 0.38 \\\\ -0.10 \\end{pmatrix} $$\n\n**阶段 III：状态更新 (GRU)**\n更新函数是一个 GRU，输入为 $M_2$，前一状态为 $h_2^{(0)}$。设 $\\sigma(x) = 1/(1+\\exp(-x))$ 为 sigmoid 函数，$\\odot$ 为逐元素乘积。\n\n1.  **更新门 ($z_2$):** $z_2 = \\sigma(W_z M_2 + U_z h_2^{(0)} + b_z)$\n    $$ W_z M_2 = \\begin{pmatrix} 0.2  -0.1 \\\\ 0.05  0.3 \\end{pmatrix} \\begin{pmatrix} 0.38 \\\\ -0.10 \\end{pmatrix} = \\begin{pmatrix} 0.076+0.01 \\\\ 0.019-0.03 \\end{pmatrix} = \\begin{pmatrix} 0.086 \\\\ -0.011 \\end{pmatrix} $$\n    $$ U_z h_2^{(0)} = \\begin{pmatrix} -0.2  0.4 \\\\ 0.1  -0.3 \\end{pmatrix} \\begin{pmatrix} 0.0 \\\\ 0.3 \\end{pmatrix} = \\begin{pmatrix} 0.12 \\\\ -0.09 \\end{pmatrix} $$\n    $$ z_2 = \\sigma\\left( \\begin{pmatrix} 0.086 \\\\ -0.011 \\end{pmatrix} + \\begin{pmatrix} 0.12 \\\\ -0.09 \\end{pmatrix} + \\begin{pmatrix} 0.01 \\\\ -0.02 \\end{pmatrix} \\right) = \\sigma\\left( \\begin{pmatrix} 0.216 \\\\ -0.121 \\end{pmatrix} \\right) = \\begin{pmatrix} 0.553793 \\\\ 0.469799 \\end{pmatrix} $$\n\n2.  **重置门 ($r_2$):** $r_2 = \\sigma(W_r M_2 + U_r h_2^{(0)} + b_r)$\n    $$ W_r M_2 = \\begin{pmatrix} -0.1  0.2 \\\\ 0.3  -0.05 \\end{pmatrix} \\begin{pmatrix} 0.38 \\\\ -0.10 \\end{pmatrix} = \\begin{pmatrix} -0.038-0.02 \\\\ 0.114+0.005 \\end{pmatrix} = \\begin{pmatrix} -0.058 \\\\ 0.119 \\end{pmatrix} $$\n    $$ U_r h_2^{(0)} = \\begin{pmatrix} 0.25  -0.15 \\\\ -0.2  0.35 \\end{pmatrix} \\begin{pmatrix} 0.0 \\\\ 0.3 \\end{pmatrix} = \\begin{pmatrix} -0.045 \\\\ 0.105 \\end{pmatrix} $$\n    $$ r_2 = \\sigma\\left( \\begin{pmatrix} -0.058 \\\\ 0.119 \\end{pmatrix} + \\begin{pmatrix} -0.045 \\\\ 0.105 \\end{pmatrix} + \\begin{pmatrix} 0.0 \\\\ 0.01 \\end{pmatrix} \\right) = \\sigma\\left( \\begin{pmatrix} -0.103 \\\\ 0.234 \\end{pmatrix} \\right) = \\begin{pmatrix} 0.474278 \\\\ 0.558234 \\end{pmatrix} $$\n\n3.  **候选隐藏状态 ($\\tilde{h}_2^{(1)}$):** $\\tilde{h}_2^{(1)} = \\tanh(W_h M_2 + U_h (r_2 \\odot h_2^{(0)}) + b_h)$\n    $$ r_2 \\odot h_2^{(0)} = \\begin{pmatrix} 0.474278 \\\\ 0.558234 \\end{pmatrix} \\odot \\begin{pmatrix} 0.0 \\\\ 0.3 \\end{pmatrix} = \\begin{pmatrix} 0.0 \\\\ 0.167470 \\end{pmatrix} $$\n    $$ W_h M_2 = \\begin{pmatrix} 0.3  0.1 \\\\ -0.2  0.25 \\end{pmatrix} \\begin{pmatrix} 0.38 \\\\ -0.10 \\end{pmatrix} = \\begin{pmatrix} 0.114-0.01 \\\\ -0.076-0.025 \\end{pmatrix} = \\begin{pmatrix} 0.104 \\\\ -0.101 \\end{pmatrix} $$\n    $$ U_h (r_2 \\odot h_2^{(0)}) = \\begin{pmatrix} 0.5  -0.3 \\\\ 0.2  0.4 \\end{pmatrix} \\begin{pmatrix} 0.0 \\\\ 0.167470 \\end{pmatrix} = \\begin{pmatrix} -0.050241 \\\\ 0.066988 \\end{pmatrix} $$\n    $$ \\tilde{h}_2^{(1)} = \\tanh\\left( \\begin{pmatrix} 0.104 \\\\ -0.101 \\end{pmatrix} + \\begin{pmatrix} -0.050241 \\\\ 0.066988 \\end{pmatrix} + \\begin{pmatrix} 0.0 \\\\ 0.0 \\end{pmatrix} \\right) = \\tanh\\left( \\begin{pmatrix} 0.053759 \\\\ -0.034012 \\end{pmatrix} \\right) = \\begin{pmatrix} 0.053717 \\\\ -0.033996 \\end{pmatrix} $$\n\n4.  **新隐藏状态 ($h_2^{(1)}$):** $h_2^{(1)} = (1 - z_2) \\odot h_2^{(0)} + z_2 \\odot \\tilde{h}_2^{(1)}$\n    $$ 1 - z_2 = \\begin{pmatrix} 1 - 0.553793 \\\\ 1 - 0.469799 \\end{pmatrix} = \\begin{pmatrix} 0.446207 \\\\ 0.530201 \\end{pmatrix} $$\n    $$ (1 - z_2) \\odot h_2^{(0)} = \\begin{pmatrix} 0.446207 \\\\ 0.530201 \\end{pmatrix} \\odot \\begin{pmatrix} 0.0 \\\\ 0.3 \\end{pmatrix} = \\begin{pmatrix} 0.0 \\\\ 0.159060 \\end{pmatrix} $$\n    $$ z_2 \\odot \\tilde{h}_2^{(1)} = \\begin{pmatrix} 0.553793 \\\\ 0.469799 \\end{pmatrix} \\odot \\begin{pmatrix} 0.053717 \\\\ -0.033996 \\end{pmatrix} = \\begin{pmatrix} 0.029749 \\\\ -0.015971 \\end{pmatrix} $$\n    $$ h_2^{(1)} = \\begin{pmatrix} 0.0 \\\\ 0.159060 \\end{pmatrix} + \\begin{pmatrix} 0.029749 \\\\ -0.015971 \\end{pmatrix} = \\begin{pmatrix} 0.029749 \\\\ 0.143089 \\end{pmatrix} $$\n\n更新后的隐藏状态 $h_2^{(1)}$ 的第一个分量是 $0.029749$。\n四舍五入到四位有效数字得到 $0.02975$。", "answer": "$$\n\\boxed{0.02975}\n$$", "id": "3317106"}, {"introduction": "在将GNN应用于如细胞状态分类等关键生物学任务时，我们不仅关心模型的预测结果，更关心其预测的可靠性。本实践练习将带你探讨一个至关重要的问题：如何量化GNN预测的不确定性，并识别出那些模型可能出错的“分布外”（Out-of-Distribution）样本。你将学习并实现基于预测熵和能量分数的标准方法，并设计一种“拒绝预测”策略，这对于在实际高风险场景中安全部署模型至关重要。", "problem": "给定在一个细胞-细胞相互作用图上为细胞状态分类任务训练的图神经网络 (GNN) 的节点级输出。目标是使用预测熵和能量分数来检测预测不可靠的分布外 (OOD) 细胞状态，并实施满足目标覆盖率约束的弃权策略，以实现安全部署。您必须从第一性原理出发，编写一个完整、可运行的程序来计算所要求的指标。\n\n推导和实现的基本依据：使用以下经过充分检验的事实和定义。对于一个有限的标签集，GNN 节点分类器会为每个节点导出标签上的一个分类预测分布。该分类预测分布是通过对未归一化的分数 (logits) 进行归一化得到的。关于分类预测的不确定性可以通过该分类分布的香农熵来量化。在基于能量的模型和统计力学中，在温度为 T 的情况下，一个构型对应的能量是负温度乘以配分函数的对数；对于 logits，这对应于负温度乘以由逆温度缩放后的 logits 的指数和的对数。受试者工作特征 (Receiver Operating Characteristic) 分析比较正例和负例之间的分数分布，曲线下面积（Area Under the Curve）是一个随机选择的正例分数高于一个随机选择的负例分数的概率，其中平局情况计为一半。\n\n您的程序必须为下面的每个测试用例实现以下任务：\n\n- 每个节点的输入包含一个长度为 $C$ 的未归一化 logit 向量和一个二进制指示符，用于表示该节点是 OOD (1) 还是分布内 (ID, 0)。所有对数均以 $e$ 为底，所有温度均为正实数。\n- 对于每个节点，从 logits 计算经过 softmax 归一化的分类预测分布，然后计算该分类分布的香农熵作为预测不确定性。\n- 对于每个节点，在给定温度下计算其能量分数，该分数对应于与 logits 和温度相关联的吉布斯分布的配分函数的对数的负温度倍。\n- 将更高的不确定性和更高的能量视为指示更高的 OOD 可能性。对于每种评分方法（熵和能量），计算针对 OOD 标签的受试者工作特征曲线下面积 (AUROC)，其中平局情况计为一半。\n- 为每种评分方法实施一种弃权策略，通过选择分数最低的 $k$ 个节点进行保留（发布预测）来达到目标覆盖率 $c \\in (0,1]$，其中对于 $N$ 个节点，$k = \\lfloor c N \\rfloor$，并对余下的节点弃权。报告实现的覆盖率（将等于 $k/N$）和弃权集中的 OOD 召回率，其定义为弃权集中的 OOD 节点数除以 OOD 节点总数。如果在截止点附近出现分数平局，则在按分数排序后，通过索引顺序来确定性地打破平局。\n\n测试套件。对于每个测试用例，给定一个形状为 $N \\times C$ 的 logits 矩阵，一个长度为 $N$ 的 OOD 标签向量，一个温度 $T$，以及一个目标覆盖率 $c$。\n\n- 测试用例 1：$N=8$, $C=3$, $T=1.0$, $c=0.6$。\n  Logits 矩阵\n  $$\n  \\begin{bmatrix}\n  5.0  0.5  -0.5 \\\\\n  4.0  1.0  0.0 \\\\\n  3.5  0.2  0.1 \\\\\n  2.0  2.0  0.0 \\\\\n  3.2  -0.1  0.0 \\\\\n  0.1  0.0  -0.1 \\\\\n  0.5  0.5  0.5 \\\\\n  0.0  -0.2  0.1\n  \\end{bmatrix}\n  $$\n  OOD 标签\n  $$\n  [\\,0,\\,0,\\,0,\\,0,\\,0,\\,1,\\,1,\\,1\\,].\n  $$\n\n- 测试用例 2：$N=7$, $C=4$, $T=2.0$, $c=0.5$。\n  Logits 矩阵\n  $$\n  \\begin{bmatrix}\n  8.0  0.0  0.0  0.0 \\\\\n  0.0  7.5  0.0  0.0 \\\\\n  0.0  0.0  0.0  0.0 \\\\\n  0.1  -0.1  0.0  0.0 \\\\\n  2.2  2.1  2.0  1.9 \\\\\n  -0.1  0.0  0.1  0.0 \\\\\n  4.0  3.9  -10.0  -9.0\n  \\end{bmatrix}\n  $$\n  OOD 标签\n  $$\n  [\\,0,\\,0,\\,1,\\,1,\\,0,\\,1,\\,0\\,].\n  $$\n\n- 测试用例 3：$N=6$, $C=2$, $T=1.5$, $c=0.67$。\n  Logits 矩阵\n  $$\n  \\begin{bmatrix}\n  2.0  1.9 \\\\\n  0.0  0.0 \\\\\n  0.1  -0.1 \\\\\n  1.0  0.95 \\\\\n  0.5  0.49 \\\\\n  0.2  0.21\n  \\end{bmatrix}\n  $$\n  OOD 标签\n  $$\n  [\\,0,\\,1,\\,1,\\,0,\\,0,\\,1\\,].\n  $$\n\n输出规范。对于每个测试用例，按顺序计算以下五个量：\n- 使用预测熵作为 OOD 分数的 AUROC。\n- 使用能量作为 OOD 分数的 AUROC。\n- 在给定 $c$ 下，基于熵的弃权策略所达到的覆盖率。\n- 基于熵的弃权策略下，弃权集中的 OOD 召回率。\n- 基于能量的弃权策略下，弃权集中的 OOD 召回率。\n\n您的程序应生成单行输出，其中包含所有测试用例的串联结果，形式为一个扁平的浮点数列表，每个浮点数精确到小数点后六位，用方括号括起来，并用逗号分隔，按测试用例 1、2、3 的顺序排列。例如，输出格式必须为\n$$\n[\\;v_{1,1},\\,v_{1,2},\\,v_{1,3},\\,v_{1,4},\\,v_{1,5},\\,v_{2,1},\\,\\dots,\\,v_{3,5}\\;],\n$$\n其中 $v_{i,j}$ 表示测试用例 $i$ 的第 $j$ 个量。", "solution": "我们使用概率论、信息论和统计力学中的定义来形式化该问题，然后设计一个忠于这些原理的算法。\n\n一个用于 $C$ 个细胞状态类别的 GNN 节点分类器，会为每个节点生成一个未归一化的分数向量 (logits) $\\mathbf{z} \\in \\mathbb{R}^C$。标签上的分类预测分布 $\\mathbf{p} \\in \\Delta^{C-1}$ 是通过对 $\\mathbf{z}$ 进行归一化得到的，使其分量和为 1 且保持为正。将未归一化的对数概率映射到概率的唯一平滑归一化映射是由 softmax 变换给出的，它是通过其配分函数归一化的指数映射。数值稳定的计算使用一个常数偏移量，由于概率对于 $\\mathbf{z}$ 的加法平移具有不变性，该偏移量在归一化过程中会被抵消。\n\n预测不确定性可以通过分类分布的香non熵来量化。对于一个分类分布 $\\mathbf{p} = (p_1,\\dots,p_C)$，香non熵由信息论的公理定义为满足连续性、在均匀分布下取最大值以及递归性的唯一泛函（在相差一个乘法常数的情况下）。使用自然对数，以自然单位表示的香non熵为\n$$\nH(\\mathbf{p}) \\;=\\; -\\sum_{k=1}^C p_k \\,\\log p_k,\n$$\n并约定根据连续性 $0 \\log 0 = 0$。对于一个尖锐的 $\\mathbf{p}$ 分布，$H(\\mathbf{p})$ 很小；而对于均匀的 $\\mathbf{p}$ 分布，$H(\\mathbf{p})$ 很大。\n\n基于能量的推理将 logits 解释为构型的未归一化的负能量。在温度 $T>0$ 时，标签上的吉布斯分布的概率与 $\\exp(z_k/T)$ 成正比。相关的配分函数是\n$$\nZ(\\mathbf{z},T) \\;=\\; \\sum_{k=1}^C \\exp\\!\\left(\\frac{z_k}{T}\\right).\n$$\n与未归一化状态相关的能量是负温度乘以对数配分函数，\n$$\nE(\\mathbf{z};T) \\;=\\; -T \\,\\log Z(\\mathbf{z},T) \\;=\\; -T \\,\\log \\!\\left( \\sum_{k=1}^C \\exp\\!\\left(\\frac{z_k}{T}\\right) \\right).\n$$\n大而置信度高的 logits 会使对数内的和变大，从而减小 $E(\\mathbf{z};T)$（使其更负）；而平坦或低幅值的 logits 会使 $Z$ 较小，从而增大 $E(\\mathbf{z};T)$（使其更不负）。因此，更高的 $E(\\mathbf{z};T)$ 表示更高的不确定性和更大的 OOD 可能性。\n\n为了将不确定性分数与 OOD 标签进行比较，我们采用受试者工作特征框架。定义一个实值分数 $s \\in \\mathbb{R}$，其中越大的 $s$ 值应表示越高的 OOD 可能性。给定一组分数 $\\{s_i\\}_{i=1}^N$ 和二元标签 $\\{\\ell_i\\}_{i=1}^N$（其中 $\\ell_i \\in \\{0,1\\}$），我们将 $\\ell_i=1$ 解释为 OOD（正例），$\\ell_i=0$ 解释为 ID（负例）。受试者工作特征曲线下面积 (AUROC) 是一个随机抽样的正例比一个随机抽样的负例得分严格高的概率，再加上平局概率的一半。具体来说，令 $\\mathcal{P}=\\{i:\\ell_i=1\\}$ 和 $\\mathcal{N}=\\{j:\\ell_j=0\\}$，其大小分别为 $|\\mathcal{P}|=n_1$ 和 $|\\mathcal{N}|=n_0$，则 AUROC 为\n$$\n\\mathrm{AUROC} \\;=\\; \\frac{1}{n_1 n_0}\\sum_{i\\in\\mathcal{P}} \\sum_{j\\in\\mathcal{N}} \\left[ \\mathbf{1}\\{ s_i > s_j \\} + \\tfrac{1}{2}\\,\\mathbf{1}\\{ s_i = s_j \\} \\right],\n$$\n其中 $\\mathbf{1}\\{\\cdot\\}$ 表示指示函数。这个表达式等价于由 $n_1 n_0$ 归一化的 Mann–Whitney U 统计量，并能正确处理平局情况。\n\n对于弃权，我们寻求一种安全的部署策略，该策略在满足覆盖率约束的同时，对不确定的节点不进行预测。设覆盖率 $c \\in (0,1]$ 表示发布预测的节点比例。给定节点 $i$ 的不确定性分数 $u_i$（其中较大的 $u_i$ 表示较高的不确定性），在硬覆盖率约束下，为最小化保留预测的总体不确定性，最优选择是选取具有最小 $u_i$ 的 $k = \\lfloor cN \\rfloor$ 个节点。我们将弃权集定义为其补集。实现的覆盖率为 $k/N$。为衡量 OOD 过滤效果，定义弃权集内的 OOD 召回率为\n$$\n\\mathrm{Recall}_{\\mathrm{OOD}} \\;=\\; \\frac{\\left|\\{\\,i \\in \\text{abstained} : \\ell_i=1\\,\\}\\right|}{\\left|\\{\\,i : \\ell_i=1\\,\\}\\right|},\n$$\n即被弃权策略正确捕获的 OOD 节点所占的比例。如果在截止点附近出现平局，一个确定性且可复现的选择是按 $(u_i, i)$（先按分数，再按索引）进行排序，以确保稳定性。\n\n每个测试用例的算法步骤如下：\n- 对于每个具有 logits $\\mathbf{z}_i \\in \\mathbb{R}^C$ 的节点 $i \\in \\{1,\\dots,N\\}$，使用指数变换并通过总和进行归一化来计算分类预测分布 $\\mathbf{p}_i$。为保证数值稳定性，在进行指数运算前减去 $m_i = \\max_k z_{ik}$，然后进行归一化，由于 softmax 对加法常数的不变性，这不会改变 $\\mathbf{p}_i$。\n- 计算预测熵 $h_i = H(\\mathbf{p}_i) = -\\sum_{k=1}^C p_{ik} \\log p_{ik}$，在数值实现中为 $p_{ik}$ 设置一个小的正数下界，以避免计算 $\\log 0$。\n- 计算能量 $e_i = E(\\mathbf{z}_i;T) = -T \\log \\sum_{k=1}^C \\exp(z_{ik}/T)$。为保证数值稳定性，计算 $a_{ik} = z_{ik}/T$，减去 $b_i=\\max_k a_{ik}$，计算 $\\log\\sum_k \\exp(a_{ik}-b_i)$，然后加上 $b_i$，最后乘以 $-T$。\n- 形成两种 OOD 分数：$s^{(H)}_i = h_i$ 和 $s^{(E)}_i = e_i$，两者的方向都设定为值越大表示 OOD 的可能性越大。\n- 通过上述的成对定义，计算每个分数相对于 OOD 标签的 $\\mathrm{AUROC}$。\n- 对于覆盖率为 $c$ 的弃权，设置 $k=\\lfloor cN \\rfloor$。对于基于熵的弃权，按 $(s^{(H)}_i, i)$ 升序对节点进行排序，并保留前 $k$ 个节点；对其余节点进行弃权。对于基于能量的弃权，类似地按 $(s^{(E)}_i, i)$ 升序排序。计算每种方法实现的覆盖率 $k/N$ 和弃权集中的 OOD 召回率。\n\n边缘情况和正确性考量：\n- 当 $cN1$ 时，$\\lfloor cN \\rfloor$ 确保至少可以保留 0 个节点；在我们的测试套件中，$c$ 和 $N$ 的选择使得 $k \\ge 1$。实现的覆盖率恰好是 $k/N$，可能由于向下取整而略低于 $c$。\n- 使用成对比较的 AUROC 定义对平局情况具有鲁棒性；当所有分数在不同标签间都相等时，$\\mathrm{AUROC} = 0.5$。\n- 对于均匀分布的 $\\mathbf{p}_i$，熵达到最大值，这与将高熵视为类 OOD 的处理方式一致。随着 logits 变得平坦，能量增加（变得更不负），这与使用更高的能量作为更像 OOD 来分离 OOD 的方法一致。\n\n我们现在在指定的测试用例上实现这个算法。对于每个用例，我们按顺序计算五个量：使用熵的 AUROC，使用能量的 AUROC，基于熵的弃权策略下实现的覆盖率，基于熵的弃权策略下弃权集中的 OOD 召回率，以及基于能量的弃权策略下弃权集中的 OOD 召回率。最终输出将测试用例 1、2 和 3 的这 5 个量串联起来，打印成一个扁平列表，其中每个值都格式化为精确到小数点后 6 位。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef softmax_rows(logits: np.ndarray) -> np.ndarray:\n    # Numerically stable softmax over last axis\n    z = logits\n    m = np.max(z, axis=1, keepdims=True)\n    exp = np.exp(z - m)\n    sum_exp = np.sum(exp, axis=1, keepdims=True)\n    return exp / sum_exp\n\ndef predictive_entropy(probs: np.ndarray) -> np.ndarray:\n    # Shannon entropy with natural log; clip to avoid log(0)\n    eps = 1e-12\n    p = np.clip(probs, eps, 1.0)\n    return -np.sum(p * np.log(p), axis=1)\n\ndef energy_scores(logits: np.ndarray, T: float) -> np.ndarray:\n    # E = -T * log sum_k exp(z_k / T), computed stably\n    a = logits / T\n    m = np.max(a, axis=1, keepdims=True)\n    lse = m + np.log(np.sum(np.exp(a - m), axis=1, keepdims=True))\n    E = -T * lse\n    return E.ravel()\n\ndef auc_pairwise(scores: np.ndarray, labels: np.ndarray) -> float:\n    # Labels: 1 for OOD (positive), 0 for ID (negative)\n    pos = scores[labels == 1]\n    neg = scores[labels == 0]\n    n_pos = pos.size\n    n_neg = neg.size\n    if n_pos == 0 or n_neg == 0:\n        return float('nan')\n    # Pairwise comparisons\n    # Use broadcasting cautiously due to sizes; here small N, safe\n    diff = pos[:, None] - neg[None, :]\n    gt = (diff > 0).sum()\n    eq = (diff == 0).sum()\n    auc = (gt + 0.5 * eq) / (n_pos * n_neg)\n    return float(auc)\n\ndef abstain_with_coverage(scores: np.ndarray, labels: np.ndarray, coverage: float):\n    # Keep k = floor(c * N) with lowest scores (lowest uncertainty), abstain on the rest.\n    N = scores.size\n    k = int(np.floor(coverage * N + 1e-12))\n    # Deterministic tie-breaking by index: stable sort on (score, index)\n    idx = np.arange(N)\n    order = np.lexsort((idx, scores))  # sort by scores asc, then idx asc\n    keep_idx = order[:k]\n    abstain_idx = order[k:]\n    achieved_coverage = k / N\n    # OOD recall in abstained set\n    if np.sum(labels == 1) == 0:\n        ood_recall = 0.0\n    else:\n        ood_in_abstained = np.sum(labels[abstain_idx] == 1)\n        total_ood = np.sum(labels == 1)\n        ood_recall = ood_in_abstained / total_ood\n    return achieved_coverage, ood_recall\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"logits\": np.array([\n                [5.0, 0.5, -0.5],\n                [4.0, 1.0, 0.0],\n                [3.5, 0.2, 0.1],\n                [2.0, 2.0, 0.0],\n                [3.2, -0.1, 0.0],\n                [0.1, 0.0, -0.1],\n                [0.5, 0.5, 0.5],\n                [0.0, -0.2, 0.1]\n            ], dtype=float),\n            \"ood\": np.array([0,0,0,0,0,1,1,1], dtype=int),\n            \"T\": 1.0,\n            \"coverage\": 0.6\n        },\n        {\n            \"logits\": np.array([\n                [8.0, 0.0, 0.0, 0.0],\n                [0.0, 7.5, 0.0, 0.0],\n                [0.0, 0.0, 0.0, 0.0],\n                [0.1, -0.1, 0.0, 0.0],\n                [2.2, 2.1, 2.0, 1.9],\n                [-0.1, 0.0, 0.1, 0.0],\n                [4.0, 3.9, -10.0, -9.0]\n            ], dtype=float),\n            \"ood\": np.array([0,0,1,1,0,1,0], dtype=int),\n            \"T\": 2.0,\n            \"coverage\": 0.5\n        },\n        {\n            \"logits\": np.array([\n                [2.0, 1.9],\n                [0.0, 0.0],\n                [0.1, -0.1],\n                [1.0, 0.95],\n                [0.5, 0.49],\n                [0.2, 0.21]\n            ], dtype=float),\n            \"ood\": np.array([0,1,1,0,0,1], dtype=int),\n            \"T\": 1.5,\n            \"coverage\": 0.67\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        logits = case[\"logits\"]\n        labels = case[\"ood\"].astype(int)\n        T = float(case[\"T\"])\n        coverage = float(case[\"coverage\"])\n\n        probs = softmax_rows(logits)\n        ent = predictive_entropy(probs)\n        eng = energy_scores(logits, T)\n\n        # AUROC with entropy and energy (higher means more OOD-like)\n        auroc_ent = auc_pairwise(ent, labels)\n        auroc_eng = auc_pairwise(eng, labels)\n\n        # Abstention with coverage using entropy scores\n        cov_ent, recall_ent = abstain_with_coverage(ent, labels, coverage)\n        # Abstention with coverage using energy scores\n        cov_eng, recall_eng = abstain_with_coverage(eng, labels, coverage)\n\n        # Append results in the specified order: AUROC_ent, AUROC_eng, cov_ent, recall_ent, recall_eng\n        results.extend([\n            f\"{auroc_ent:.6f}\",\n            f\"{auroc_eng:.6f}\",\n            f\"{cov_ent:.6f}\",\n            f\"{recall_ent:.6f}\",\n            f\"{recall_eng:.6f}\"\n        ])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3317151"}, {"introduction": "图神经网络在处理长距离依赖时，尤其是在模拟如信号通路等链式结构时，常常会遇到“信息过压缩”（oversquashing）问题，即来自远处节点的信息在图的瓶颈处被稀释或丢失。本项高级实践将向你展示如何运用离散几何中的深刻概念——Ollivier-Ricci曲率，来诊断网络中的这些信息瓶颈。更进一步，你将实施一种曲率引导的“图重接线”策略，通过智能地添加捷径来改善信息流，并最终评估这种方法对提升GNN在生物信号通路预测任务中性能的实际效果。", "problem": "生物信号网络可以表示为一个无向简单图 $G = (V,E)$，其顶点 $V$ 表示信号实体（例如，蛋白质），其边 $E$ 编码了潜在的相互作用或信息流。在图神经网络（GNN, Graph Neural Network）的消息传递范式中，顶点 $i \\in V$ 上的标量特征 $x^{(t)}_i$ 通过聚合邻居信息逐层更新；一种广泛使用的基本规则是平均邻居的特征。当许多远端源必须通过图的狭窄区域汇集以到达目标时，信息可能被压缩到一个低维瓶颈中，这种现象被称为过度挤压（oversquashing）。图上的离散里奇曲率（Discrete Ricci curvature）为此类瓶颈提供了一个原则性的几何指标：强的负曲率通常标志着局部邻域之间的瓶颈传输。在本问题中，您将通过离散曲率诊断过度挤压现象，通过一种曲率引导的重接线（rewiring）方法来缓解它（该方法通过添加边来改善负曲率边附近的局部连通性），并评估该方法在配体扰动下对通路激活预测的影响。\n\n使用以下基本框架：\n\n- 信号图 $G = (V,E)$ 是有限、无向和无权的。顶点 $i$ 的邻居集是 $N(i)$，度是 $d_i = |N(i)|$，顶点 $u$ 和 $v$ 之间的最短路径距离表示为 $D(u,v)$。\n- 对于每条边 $(i,j) \\in E$，分别在 $N(i)$ 和 $N(j)$ 上定义均匀邻居测度 $m_i$ 和 $m_j$，即对于 $u \\in N(i)$，$m_i(u) = 1/d_i$；对于 $v \\in N(j)$，$m_j(v) = 1/d_j$。闲置参数为 $0$ 时，边 $(i,j)$ 的Ollivier-Ricci曲率（ORC, Ollivier-Ricci curvature）为\n$$\n\\kappa_{ij} = 1 - W_1(m_i, m_j),\n$$\n其中 $W_1(m_i, m_j)$ 是在离散分布 $m_i$ 和 $m_j$ 之间，使用基度量 $D(\\cdot,\\cdot)$ 计算的1-瓦瑟斯坦距离（1-Wasserstein distance）（也称推土机距离，Earth Mover's Distance, EMD）。因为对于相邻顶点 $D(i,j) = 1$，所以分母为 $1$ 并被省略。\n- 标量特征的单层邻居平均消息传递更新定义为\n$$\nx^{(t+1)}_i = \\sigma\\!\\left(\\frac{x^{(t)}_i + \\sum_{j \\in N(i)} x^{(t)}_j}{d_i + 1}\\right),\n$$\n其中 $\\sigma(z) = \\max(0,z)$ 是整流线性单元（rectified linear unit）非线性函数。此更新在平均中包含一个自环，并迭代 $T$ 层。\n- 在源顶点 $S \\subset V$ 处施加强度为 $\\{\\lambda_s\\}_{s \\in S}$ 的配体扰动时，指定目标顶点 $v^\\star$ 处的通路激活的真实值代理（ground-truth proxy）由指数衰减的路径影响给出\n$$\ny^\\text{true} = \\sum_{s \\in S} \\lambda_s \\, \\beta^{D(s, v^\\star)},\n$$\n其中衰减参数 $\\beta \\in (0,1)$ 是在重接线前的原始图上测量的。预测的激活值为 $y^\\text{pred} = x^{(T)}_{v^\\star}$，通过消息传递规则获得。\n\n您必须实现一个程序，对每个测试用例，纯粹地按算法执行以下所有步骤：\n\n$1.$ 构建指定的信号图 $G$ 并计算所有顶点对之间的最短路径距离 $D(u,v)$。\n\n$2.$ 对于每条边 $(i,j) \\in E$，通过使用基度量 $D(\\cdot,\\cdot)$ 的线性规划，精确求解定义了 $W_1(m_i, m_j)$ 的离散最优传输问题，从而计算Ollivier-Ricci曲率 $\\kappa_{ij}$。\n\n$3.$ 曲率引导的重接线：对于给定的阈值 $\\tau$，识别出曲率 $\\kappa_{ij}  \\tau$ 的边集。通过在这些负曲率边周围闭合三角形来形成一个候选新边集，具体来说，考虑图中尚不存在的边 $(u,j)$（其中 $u \\in N(i) \\setminus \\{j\\}$）和边 $(i,v)$（其中 $v \\in N(j) \\setminus \\{i\\}$）。按其端点之间的距离 $D(\\cdot,\\cdot)$ 降序（以优先选择更长的捷径），然后按其端点的度之和 $d_u + d_v$ 升序（以避免枢纽节点）对候选边进行排序，并添加最多 $K$ 条新边以形成重接线后的图 $\\tilde{G}$。\n\n$4.$ 在原始图 $G$ 上，使用给定的衰减参数 $\\beta$ 和源 $S$ 处的配体强度 $\\{\\lambda_s\\}$，计算目标 $v^\\star$ 处的真实激活值 $y^\\text{true}$。\n\n$5.$ 使用 $T$ 层的邻居平均消息传递更新，在图 $G$ 上计算预测激活值 $y^\\text{pred}_\\text{before}$，在图 $\\tilde{G}$ 上计算 $y^\\text{pred}_\\text{after}$。初始特征为：对于 $s \\in S$，$x^{(0)}_s = \\lambda_s$；对于 $i \\notin S$，$x^{(0)}_i = 0$。\n\n$6.$ 将改进报告为绝对误差的有符号差，\n$$\n\\Delta = \\left|y^\\text{pred}_\\text{after} - y^\\text{true}\\right| - \\left|y^\\text{pred}_\\text{before} - y^\\text{true}\\right|.\n$$\n负的 $\\Delta$ 表示重接线后有所改进。\n\n您的程序必须处理以下测试套件，该套件旨在探测典型、瓶颈和已连接良好的情况：\n\n- 测试用例 $1$（理想路径，具有少数源的长链）：\n  - 图：一个由 $12$ 个顶点（标记为 $0,1,\\dots,11$）组成的简单链，边为 $(i,i+1)$，其中 $i \\in \\{0,\\dots,10\\}$。\n  - 源和强度：$S = \\{0,1,2\\}$，强度为 $\\{\\lambda_0,\\lambda_1,\\lambda_2\\} = \\{1.0, 0.6, 0.4\\}$。\n  - 目标：$v^\\star = 11$。\n  - 衰减：$\\beta = 0.8$。\n  - 消息传递层数：$T = 12$。\n  - 曲率阈值和重接线预算：$\\tau = -0.1$，$K = 4$。\n\n- 测试用例 $2$（具有多个源的瓶颈级联）：\n  - 图：一个由 $16$ 个顶点（标记为 $0,1,\\dots,15$）组成的简单链，边为 $(i,i+1)$，其中 $i \\in \\{0,\\dots,14\\}$。\n  - 源和强度：$S = \\{0,1,2,3,4\\}$，强度为 $\\{\\lambda_0,\\lambda_1,\\lambda_2,\\lambda_3,\\lambda_4\\} = \\{1.0, 0.7, 0.5, 0.3, 0.2\\}$。\n  - 目标：$v^\\star = 15$。\n  - 衰减：$\\beta = 0.85$。\n  - 消息传递层数：$T = 16$。\n  - 曲率阈值和重接线预算：$\\tau = -0.05$，$K = 6$。\n\n- 测试用例 $3$（局部结构已连接良好，预期变化最小）：\n  - 图：一个由 $8$ 个顶点（标记为 $0,1,\\dots,7$）组成的链，边为 $(i,i+1)$，其中 $i \\in \\{0,\\dots,6\\}$，以及一条额外的边 $(3,5)$，在 $\\{3,4,5\\}$ 上形成一个三角形。\n  - 源和强度：$S = \\{0,1\\}$，强度为 $\\{\\lambda_0,\\lambda_1\\} = \\{1.0, 0.5\\}$。\n  - 目标：$v^\\star = 7$。\n  - 衰减：$\\beta = 0.9$。\n  - 消息传递层数：$T = 8$。\n  - 曲率阈值和重接线预算：$\\tau = -0.2$，$K = 3$。\n\n您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔的结果列表，按测试用例的顺序排列，例如 $[r_1,r_2,r_3]$，其中每个 $r_i$ 是测试用例 $i$ 的 $\\Delta$ 的浮点值。不涉及物理单位、角度或百分比；所有输出都是以十进制形式表示的无单位实数。", "solution": "用户要求实现一个六步计算流程，以分析和重接线一个生物信号网络的图表示。问题的核心涉及使用Ollivier-Ricci曲率（ORC）诊断网络瓶颈，通过曲率引导的边添加过程缓解这些瓶颈，并评估这种重接线对图神经网络（GNN）预测任务的影响。\n\n该解决方案通过为每个测试用例按算法执行六个指定步骤中的每一步来进行。\n\n**步骤1：图表示与所有顶点对之间的最短路径**\n\n信号网络是一个无向、无权的图 $G=(V, E)$。它可以使用邻接表来表示，这对于测试套件中提供的稀疏图是高效的。为了计算瓦瑟斯坦距离的基度量和真实激活值，我们需要所有顶点对 $(u,v) \\in V \\times V$ 之间的最短路径距离 $D(u,v)$。对于此处给定的小型无权图，Floyd-Warshall算法提供了一种计算所有顶点对之间最短路径的直接方法。该算法初始化一个距离矩阵，其中对于边 $(i,j) \\in E$，$D(i,i)=0$，$D(i,j)=1$，否则 $D(i,j)=\\infty$。然后它通过中间顶点迭代地松弛这些距离：\n$$\nD(i,j) = \\min(D(i,j), D(i,k) + D(k,j))\n$$\n对于所有中间顶点 $k \\in V$。\n\n**步骤2：Ollivier-Ricci曲率（ORC）计算**\n\n边 $(i,j) \\in E$ 的ORC定义为 $\\kappa_{ij} = 1 - W_1(m_i, m_j)$，其中 $W_1$ 是两个概率分布 $m_i$ 和 $m_j$ 之间的1-瓦瑟斯坦距离（或推土机距离）。分布 $m_i$ 是顶点 $i$ 的邻域 $N(i)$ 上的一个均匀测度，为每个邻居 $u \\in N(i)$ 分配 $1/d_i$ 的概率质量，其中 $d_i = |N(i)|$ 是 $i$ 的度。分布 $m_j$ 的定义类似。\n\n瓦瑟斯坦距离 $W_1(m_i, m_j)$ 是一个最优传输问题的解，该问题可以被表述为一个线性规划（LP）。我们寻求一个传输方案 $\\pi = (\\pi_{uv})$，它能最小化总传输成本，其中将一个单位质量从邻居 $u \\in N(i)$ 移动到邻居 $v \\in N(j)$ 的成本是图距离 $D(u,v)$。\n\n该线性规划表述为：\n$$\n\\text{最小化} \\quad \\sum_{u \\in N(i)} \\sum_{v \\in N(j)} D(u,v) \\pi_{uv}\n$$\n约束条件如下：\n1.  对于所有 $u \\in N(i), v \\in N(j)$，$\\pi_{uv} \\ge 0$ (非负性)。\n2.  对于所有 $u \\in N(i)$，$\\sum_{v' \\in N(j)} \\pi_{uv'} = m_i(u) = 1/d_i$ (源处的质量守恒)。\n3.  对于所有 $v \\in N(j)$，$\\sum_{u' \\in N(i)} \\pi_{u'v} = m_j(v) = 1/d_j$ (目标处的质量守恒)。\n\n这个线性规划可以使用标准的科学计算库（如 `scipy.optimize.linprog`）来求解。目标函数的最终最小值即为 $W_1(m_i, m_j)$。对图 $G$ 中的每条边都执行此计算。\n\n对于测试用例1和2中的简单链状图，任何边的曲率 $\\kappa_{ij}$ 都恰好为 $0$。这是无限一维晶格的一个已知结果，并且对这些有限链也成立。例如，在链内部的边 $(i, i+1)$ 上，$N(i) = \\{i-1, i+1\\}$ 且 $N(i+1) = \\{i, i+2\\}$。最优传输方案将质量从 $i-1$ 移动到 $i$，从 $i+1$ 移动到 $i+2$，得出 $W_1=1$，因此 $\\kappa = 1-1=0$。\n\n**步骤3：曲率引导的重接线**\n\n过度挤压与负曲率边相关，这些边充当瓶颈。重接线策略旨在缓解这些瓶颈。首先，对于给定的阈值 $\\tau$，我们识别出边集 $E_{neg} = \\{(i,j) \\in E \\mid \\kappa_{ij}  \\tau\\}$。\n\n对于每个负曲率边 $(i,j) \\in E_{neg}$，我们生成一个候选边集以添加到图中。这些候选边在边 $(i,j)$ 周围“闭合三角形”，形成捷径。候选边的形式为 $(u,j)$（其中 $u \\in N(i) \\setminus \\{j\\}$）和 $(i,v)$（其中 $v \\in N(j) \\setminus \\{i\\}$），前提是它们尚未在 $E$ 中。\n\n收集所有唯一的候选边，然后进行排序。主要排序标准是端点之间的最短路径距离 $D(u,v)$，按降序排列，以优先选择长程捷径。次要标准是端点原始度数之和 $d_u+d_v$，按升序排列，以倾向于连接中心性较低的节点。从这个排序列表中选出前 $K$ 条边添加到 $G$ 中，创建重接线后的图 $\\tilde{G} = (V, E \\cup E_{new})$。\n\n**步骤4：真实激活值计算**\n\n真实值 $y^\\text{true}$ 作为GNN预测的基准。它在原始图 $G$ 上计算，是来自源顶点 $S \\subset V$ 的影响的加权和：\n$$\ny^\\text{true} = \\sum_{s \\in S} \\lambda_s \\beta^{D(s, v^\\star)}\n$$\n其中 $\\{\\lambda_s\\}$ 是源处的信号强度，$v^\\star$ 是目标顶点，$\\beta \\in (0,1)$ 是衰减因子，$D(s, v^\\star)$ 是步骤1中得到的最短路径距离。\n\n**步骤5：通过GNN消息传递预测激活值**\n\n目标顶点处的预测激活值是通过模拟一个简单的GNN $T$ 层得到的。初始特征向量 $x^{(0)}$ 设置为：对于源顶点 $s \\in S$，$x^{(0)}_s = \\lambda_s$，否则为 $0$。然后对 $t = 0, \\dots, T-1$ 迭代更新特征：\n$$\nx^{(t+1)}_i = \\sigma\\!\\left(\\frac{x^{(t)}_i + \\sum_{j \\in N(i)} x^{(t)}_j}{d_i + 1}\\right)\n$$\n其中 $\\sigma(z) = \\max(0,z)$ 是ReLU激活函数。分母中包含一个自环。此模拟执行两次：一次在原始图 $G$ 上以获得 $y^\\text{pred}_\\text{before} = x^{(T)}_{v^\\star}$，一次在重接线后的图 $\\tilde{G}$ 上以获得 $y^\\text{pred}_\\text{after} = x^{(T)}_{v^\\star}$。请注意，第二次模拟的邻域 $N(i)$ 和度 $d_i$ 必须基于 $\\tilde{G}$。\n\n**步骤6：报告改进**\n\n重接线带来的改进通过绝对预测误差的变化来量化：\n$$\n\\Delta = \\left|y^\\text{pred}_\\text{after} - y^\\text{true}\\right| - \\left|y^\\text{pred}_\\text{before} - y^\\text{true}\\right|\n$$\n$\\Delta$ 的负值表示重接线过程通过减少相对于真实值的误差，改进了GNN的预测。最终输出是每个测试用例的这些 $\\Delta$ 值的列表。", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import linprog\n\nclass GraphSignalAnalyzer:\n    \"\"\"\n    Implements the full analysis pipeline for a single graph test case.\n    \"\"\"\n    def __init__(self, num_vertices, edges, sources, lambdas, v_star, beta, T, tau, K):\n        self.num_vertices = num_vertices\n        self.edges = frozenset(tuple(sorted(edge)) for edge in edges)\n        self.sources = sources\n        self.lambdas = lambdas\n        self.v_star = v_star\n        self.beta = beta\n        self.T = T\n        self.tau = tau\n        self.K = K\n\n        # Step 1: Build graph structure and compute all-pairs shortest paths\n        self.adj = {i: [] for i in range(num_vertices)}\n        for u, v in self.edges:\n            self.adj[u].append(v)\n            self.adj[v].append(u)\n        self.degrees = {i: len(self.adj[i]) for i in range(num_vertices)}\n        self.dists = self._compute_apsp()\n\n    def _compute_apsp(self):\n        \"\"\"Computes all-pairs shortest paths using the Floyd-Warshall algorithm.\"\"\"\n        d = np.full((self.num_vertices, self.num_vertices), np.inf)\n        np.fill_diagonal(d, 0)\n        for u, v in self.edges:\n            d[u, v] = 1\n            d[v, u] = 1\n        \n        for k in range(self.num_vertices):\n            for i in range(self.num_vertices):\n                for j in range(self.num_vertices):\n                    d[i, j] = min(d[i, j], d[i, k] + d[k, j])\n        return d\n\n    def _compute_orc(self, i, j):\n        \"\"\"Computes Ollivier-Ricci curvature for an edge (i, j) via linear programming.\"\"\"\n        Ni = self.adj[i]\n        Nj = self.adj[j]\n        di, dj = len(Ni), len(Nj)\n\n        if di == 0 or dj == 0:\n            return 1.0\n\n        # Create LP problem for 1-Wasserstein distance.\n        cost = np.array([self.dists[u, v] for u in Ni for v in Nj])\n        \n        A_eq = np.zeros((di + dj, di * dj))\n        b_eq = np.concatenate([np.full(di, 1/di), np.full(dj, 1/dj)])\n        \n        for row in range(di): # Source constraints\n            for col in range(dj):\n                A_eq[row, row * dj + col] = 1\n        \n        for row in range(dj): # Target constraints\n            for col in range(di):\n                A_eq[di + row, col * dj + row] = 1\n        \n        res = linprog(c=cost, A_eq=A_eq, b_eq=b_eq, bounds=(0, None), method='highs')\n        \n        if not res.success:\n            return np.nan # Should not happen in this problem\n\n        wasserstein_dist = res.fun\n        return 1.0 - wasserstein_dist\n\n    def _run_gnn(self, adj, T, initial_features):\n        \"\"\"Simulates the GNN message passing for T layers.\"\"\"\n        x = initial_features.copy()\n        degrees = {i: len(adj[i]) for i in range(self.num_vertices)}\n        \n        for _ in range(T):\n            x_next = np.zeros_like(x)\n            for i in range(self.num_vertices):\n                neighbor_sum = sum(x[j] for j in adj[i])\n                if degrees[i] == -1: # Problem statement uses d_i + 1\n                    agg = 0\n                else:\n                    agg = (x[i] + neighbor_sum) / (degrees[i] + 1)\n                x_next[i] = max(0, agg)\n            x = x_next\n        \n        return x[self.v_star]\n\n    def run_analysis(self):\n        \"\"\"Executes the entire 6-step pipeline and returns the improvement score delta.\"\"\"\n        # Step 2: Compute ORC for all edges\n        curvatures = {edge: self._compute_orc(edge[0], edge[1]) for edge in self.edges}\n\n        # Step 3: Curvature-guided rewiring\n        neg_curv_edges = [edge for edge, k in curvatures.items() if k  self.tau]\n        \n        candidate_edges = set()\n        for i, j in neg_curv_edges:\n            for u in self.adj[i]:\n                if u != j and tuple(sorted((u, j))) not in self.edges:\n                    candidate_edges.add(tuple(sorted((u, j))))\n            for v in self.adj[j]:\n                if v != i and tuple(sorted((i, v))) not in self.edges:\n                    candidate_edges.add(tuple(sorted((i, v))))\n\n        sorted_candidates = sorted(\n            list(candidate_edges),\n            key=lambda e: (-self.dists[e[0], e[1]], self.degrees[e[0]] + self.degrees[e[1]], e[0], e[1])\n        )\n        \n        new_edges = sorted_candidates[:self.K]\n        \n        tilde_adj = {i: list(self.adj[i]) for i in range(self.num_vertices)}\n        for u, v in new_edges:\n            tilde_adj[u].append(v)\n            tilde_adj[v].append(u)\n\n        # Step 4: Compute ground truth activation\n        y_true = sum(\n            self.lambdas[s_idx] * (self.beta ** self.dists[s, self.v_star])\n            for s_idx, s in enumerate(self.sources)\n        )\n\n        # Step 5: Compute predicted activations\n        initial_features = np.zeros(self.num_vertices)\n        for s_idx, s in enumerate(self.sources):\n            initial_features[s] = self.lambdas[s_idx]\n            \n        y_pred_before = self._run_gnn(self.adj, self.T, initial_features)\n        y_pred_after = self._run_gnn(tilde_adj, self.T, initial_features)\n        \n        # Step 6: Report improvement\n        error_before = abs(y_pred_before - y_true)\n        error_after = abs(y_pred_after - y_true)\n        delta = error_after - error_before\n        \n        return delta\n\n\ndef solve():\n    \"\"\"\n    Initializes and runs the analysis for each test case specified in the problem.\n    \"\"\"\n    test_cases = [\n        # Test case 1\n        {\n            \"num_vertices\": 12, \"edges\": [(i, i + 1) for i in range(11)],\n            \"sources\": [0, 1, 2], \"lambdas\": [1.0, 0.6, 0.4],\n            \"v_star\": 11, \"beta\": 0.8, \"T\": 12, \"tau\": -0.1, \"K\": 4\n        },\n        # Test case 2\n        {\n            \"num_vertices\": 16, \"edges\": [(i, i + 1) for i in range(15)],\n            \"sources\": [0, 1, 2, 3, 4], \"lambdas\": [1.0, 0.7, 0.5, 0.3, 0.2],\n            \"v_star\": 15, \"beta\": 0.85, \"T\": 16, \"tau\": -0.05, \"K\": 6\n        },\n        # Test case 3\n        {\n            \"num_vertices\": 8, \"edges\": [(i, i + 1) for i in range(7)] + [(3, 5)],\n            \"sources\": [0, 1], \"lambdas\": [1.0, 0.5],\n            \"v_star\": 7, \"beta\": 0.9, \"T\": 8, \"tau\": -0.2, \"K\": 3\n        }\n    ]\n\n    results = []\n    for params in test_cases:\n        analyzer = GraphSignalAnalyzer(**params)\n        delta = analyzer.run_analysis()\n        results.append(delta)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3317174"}]}