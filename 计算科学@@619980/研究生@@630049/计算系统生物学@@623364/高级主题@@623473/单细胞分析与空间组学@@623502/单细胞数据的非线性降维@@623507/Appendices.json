{"hands_on_practices": [{"introduction": "在对单细胞数据进行非线性降维之前，预处理是一个至关重要的步骤，它能显著影响最终的可视化和聚类结果。本练习将通过编程实践，量化一个常见的预处理步骤——z-score标准化——如何改变高斯亲和矩阵的谱（即特征值集合），这个矩阵是许多流形学习算法（如谱聚类和Laplacian Eigenmaps）的核心。通过这个练习，你将深入理解数据缩放对捕捉细胞间几何关系的底层影响，并学会处理实际数据中可能出现的边缘情况，例如无方差基因。[@problem_id:3334321]", "problem": "考虑一个单细胞转录组数据集，它表示为一个包含 $n$ 个细胞和 $p$ 个基因的矩阵，其中每个细胞 $i$ 是一个点 $x_i \\in \\mathbb{R}^p$。对于任意两个细胞 $i$ 和 $j$，高斯亲和矩阵的元素定义为 $W_{ij} = \\exp\\left(-\\|x_i - x_j\\|_2^2 / (2 \\sigma^2)\\right)$，其中 $\\| \\cdot \\|_2$ 表示欧几里得范数，$\\sigma > 0$ 是核带宽参数。对每个基因进行 Z-score 标准化意味着减去该基因在所有细胞中的均值，然后除以其标准差，从而得到标准化点 $z_i \\in \\mathbb{R}^p$；如果一个基因在所有细胞中的方差为零，则其所有细胞的标准化值均定义为 $0$。使用原始数据 $\\{x_i\\}$ 构建高斯亲和矩阵 $W^{(0)}$，并使用标准化数据 $\\{z_i\\}$ 构建矩阵 $W^{(z)}$。亲和矩阵的谱（spectrum）指的是其特征值的多重集。对于像 $W^{(0)}$ 或 $W^{(z)}$ 这样的对称亲和矩阵，所有特征值都是实数。对于给定的数据集和带宽，将谱变化幅度定义为\n$$\nS = \\left\\| \\lambda\\!\\left(W^{(z)}\\right) - \\lambda\\!\\left(W^{(0)}\\right) \\right\\|_2,\n$$\n其中 $\\lambda(\\cdot)$ 表示按降序排列的特征值向量，$\\|\\cdot\\|_2$ 是欧几里得范数。\n\n仅根据 Z-score 标准化、欧几里得距离、高斯核的定义，以及关于实对称矩阵及其特征值的基本事实，设计并实现一个程序，为下面指定的每个测试用例计算 $S$。您的程序必须使用所提供的精确数据集和带宽，对零方差基因应用零方差 Z-score 标准化规则，计算亲和矩阵，获取其特征值，按降序排序，并按规定计算 $S$。每个报告的 $S$ 值必须四舍五入到 $6$ 位小数。\n\n测试套件：\n- 测试用例 1（正常路径，中等带宽）：使用数据集 $X^{(A)} \\in \\mathbb{R}^{5 \\times 3}$，其行向量为 $x_1 = (2.0, 5.0, 8.0)$、$x_2 = (3.0, 6.0, 7.5)$、$x_3 = (4.0, 6.5, 7.0)$、$x_4 = (5.0, 7.0, 6.0)$、$x_5 = (7.0, 7.5, 5.0)$，带宽 $\\sigma = 1.0$。\n- 测试用例 2（小带宽边界）：使用与上述相同的数据集 $X^{(A)}$，带宽 $\\sigma = 0.1$。\n- 测试用例 3（大带宽边界）：使用与上述相同的数据集 $X^{(A)}$，带宽 $\\sigma = 100.0$。\n- 测试用例 4（零方差基因边界情况）：使用数据集 $X^{(B)} \\in \\mathbb{R}^{5 \\times 3}$，其行向量为 $x_1 = (0.0, 0.0, 10.0)$、$x_2 = (10.0, 0.0, 10.0)$、$x_3 = (20.0, 0.0, 10.0)$、$x_4 = (30.0, 0.0, 10.0)$、$x_5 = (40.0, 0.0, 10.0)$，带宽 $\\sigma = 5.0$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，格式为 $[S_1,S_2,S_3,S_4]$，其中 $S_k$ 是测试用例 $k$ 经四舍五入的谱变化幅度。输出是无量纲的实数，且必须是四舍五入到 $6$ 位小数的浮点数。", "solution": "任务是量化 Z-score 标准化对源自单细胞数据的高斯亲和矩阵的特征谱的影响。我们获得了数据转换、亲和矩阵构建以及谱变化度量标准的精确数学定义。该问题是一个基于线性代数、统计学和计算生物学原理的、定义明确的计算练习。\n\n整个过程包括几个不同且连续的步骤：\n1. 对于给定的数据集 $X$ 和带宽 $\\sigma$，构建高斯亲和矩阵 $W^{(0)}$。\n2. 计算并排序 $W^{(0)}$ 的特征值。\n3. 对数据集 $X$ 应用 Z-score 标准化以获得新数据集 $Z$，并按规定处理零方差基因。\n4. 使用标准化数据 $Z$ 和相同的带宽 $\\sigma$ 构建高斯亲和矩阵 $W^{(z)}$。\n5. 计算并排序 $W^{(z)}$ 的特征值。\n6. 计算两个排序后的特征值向量之间的欧几里得距离，以求得谱变化幅度 $S$。\n\n我们现在将详细阐述每个步骤的数学和概念基础。\n\n一个单细胞数据集表示为一个矩阵 $X \\in \\mathbb{R}^{n \\times p}$，其中 $n$ 是细胞数量，$p$ 是基因数量。每个细胞 $i$ 对应一个行向量 $x_i \\in \\mathbb{R}^p$。\n\n**1. Z-score 标准化**\n\nZ-score 标准化是一种常见的预处理技术，用于通过将特征（基因）转换到共同尺度上使其具有可比性。它将每个基因的表达值重新缩放，使其均值为 $0$，标准差为 $1$。\n\n对于每个基因 $j \\in \\{1, \\dots, p\\}$，我们首先计算其在所有 $n$ 个细胞中的均值 $\\mu_j$ 和总体标准差 $s_j$：\n$$\n\\mu_j = \\frac{1}{n} \\sum_{i=1}^n x_{ij}\n$$\n$$\ns_j = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (x_{ij} - \\mu_j)^2}\n$$\n原始数据点 $x_i = (x_{i1}, \\dots, x_{ip})$ 被转换为标准化点 $z_i = (z_{i1}, \\dots, z_{ip})$。如果一个基因 $j$ 的方差不为零（即 $s_j > 0$），其对细胞 $i$ 的标准化值为：\n$$\nz_{ij} = \\frac{x_{ij} - \\mu_j}{s_j}\n$$\n问题指定了一个关键的边界情况：如果一个基因 $j$ 的方差为零（$s_j = 0$），则其对所有细胞的标准化值均定义为 $0$，即对于所有 $i \\in \\{1, \\dots, n\\}$ 都有 $z_{ij} = 0$。该规则可以防止除以零，并有效地从标准化空间中的距离计算中移除了恒定表达基因的贡献。\n\n**2. 高斯亲和矩阵构建**\n\n细胞之间的关系由一个亲和矩阵捕获，该矩阵测量成对的相似性。高斯核是用于此目的的标准选择。两个细胞 $i$ 和 $j$（其数据向量为 $u_i$ 和 $u_j$，其中 $u$ 可以是原始数据 $x$ 或标准化数据 $z$）之间的亲和力定义为：\n$$\nW_{ij} = \\exp\\left(-\\frac{\\|u_i - u_j\\|_2^2}{2 \\sigma^2}\\right)\n$$\n这里，$\\|u_i - u_j\\|_2^2$ 是两个细胞向量之间的平方欧几里得距离，$\\sigma > 0$ 是核带宽参数，它控制高斯的宽度，从而决定邻域的尺度。较小的 $\\sigma$ 会导致更局部、更稀疏的相似性概念，而较大的 $\\sigma$ 则考虑更广泛的关系。\n\n我们构建两个这样的矩阵：\n- $W^{(0)}$，使用原始数据点 $\\{x_i\\}$。\n- $W^{(z)}$，使用 Z-score 标准化后的数据点 $\\{z_i\\}$。\n\n根据构造，这些矩阵是实对称的（$W_{ij} = W_{ji}$），因为欧几里得距离是对称的。对角线元素始终为 $W_{ii} = \\exp(0) = 1$。\n\n**3. 谱分析**\n\n矩阵的谱是其特征值的多重集。线性代数的一个基本定理指出，任何实对称矩阵，例如我们的亲和矩阵 $W^{(0)}$ 和 $W^{(z)}$，都是可对角化的，并且只有实特征值。这些特征值为理解由亲和矩阵所代表的数据图的结构提供了深刻的见解。\n\n我们计算 $W^{(0)}$ 和 $W^{(z)}$ 的特征值。设这些特征值的多重集为 $\\{\\lambda_k^{(0)}\\}_{k=1}^n$ 和 $\\{\\lambda_k^{(z)}\\}_{k=1}^n$。为了进行有意义的比较，我们按降序对它们进行排序，以形成特征值向量：\n$$\n\\lambda\\left(W^{(0)}\\right) = \\left(\\lambda_1^{(0)}, \\lambda_2^{(0)}, \\dots, \\lambda_n^{(0)}\\right) \\text{ 其中 } \\lambda_1^{(0)} \\ge \\lambda_2^{(0)} \\ge \\dots \\ge \\lambda_n^{(0)}\n$$\n$$\n\\lambda\\left(W^{(z)}\\right) = \\left(\\lambda_1^{(z)}, \\lambda_2^{(z)}, \\dots, \\lambda_n^{(z)}\\right) \\text{ 其中 } \\lambda_1^{(z)} \\ge \\lambda_2^{(z)} \\ge \\dots \\ge \\lambda_n^{(z)}\n$$\n\n**4. 谱变化幅度计算**\n\n最后一步是量化由 Z-score 转换引起的谱的总变化。问题将谱变化幅度 $S$ 定义为两个排序后的特征值向量之差的欧几里得（$L_2$）范数：\n$$\nS = \\left\\| \\lambda\\!\\left(W^{(z)}\\right) - \\lambda\\!\\left(W^{(0)}\\right) \\right\\|_2 = \\sqrt{\\sum_{k=1}^n \\left(\\lambda_k^{(z)} - \\lambda_k^{(0)}\\right)^2}\n$$\n该度量提供了一个单一的标量值，总结了 Z-score 标准化在多大程度上改变了由高斯亲和矩阵的谱所捕获的数据的几何结构。\n\n实现将针对每个测试用例精确遵循这些步骤，使用稳健的数值库来执行矩阵和向量运算。", "answer": "```python\nimport numpy as np\nfrom scipy.spatial.distance import pdist, squareform\n\ndef calculate_spectral_change(X: np.ndarray, sigma: float) -> float:\n    \"\"\"\n    Computes the spectral change magnitude S for a given dataset and bandwidth.\n\n    The function performs the following steps:\n    1. Computes the Gaussian affinity matrix W^(0) from the original data X.\n    2. Computes the sorted eigenvalues of W^(0).\n    3. Standardizes (z-scores) the data X to get Z, handling zero-variance genes.\n    4. Computes the Gaussian affinity matrix W^(z) from the standardized data Z.\n    5. Computes the sorted eigenvalues of W^(z).\n    6. Calculates the Euclidean norm of the difference between the two eigenvalue vectors.\n\n    Args:\n        X: A numpy array of shape (n, p) representing n cells and p genes.\n        sigma: The Gaussian kernel bandwidth parameter.\n\n    Returns:\n        The spectral change magnitude S.\n    \"\"\"\n    # 1. Compute affinity matrix W^(0) from original data X\n    # The 'sqeuclidean' metric computes the squared Euclidean distance.\n    dist_sq_0 = squareform(pdist(X, 'sqeuclidean'))\n    W0 = np.exp(-dist_sq_0 / (2 * sigma**2))\n\n    # 2. Compute sorted eigenvalues of W^(0)\n    # np.linalg.eigh is specialized for Hermitian (symmetric for real matrices)\n    # and returns eigenvalues in ascending order. We reverse them.\n    eigvals_0 = np.linalg.eigh(W0)[0][::-1]\n\n    # 3. Z-score the data X to get Z\n    # We use population standard deviation (ddof=0 is the default in np.std)\n    means = np.mean(X, axis=0)\n    stds = np.std(X, axis=0)\n\n    # Initialize standardized matrix with zeros. This correctly handles the\n    # zero-variance case where standardized values must be 0.\n    Z = np.zeros_like(X, dtype=float)\n    \n    # Create a boolean mask for columns with non-zero standard deviation\n    non_zero_std_mask = stds > 1e-15 # Use a small tolerance for floating point safety\n    \n    # Apply standardization only to columns with non-zero standard deviation\n    if np.any(non_zero_std_mask):\n        Z[:, non_zero_std_mask] = (X[:, non_zero_std_mask] - means[non_zero_std_mask]) / stds[non_zero_std_mask]\n\n    # 4. Compute affinity matrix W^(z) from standardized data Z\n    dist_sq_z = squareform(pdist(Z, 'sqeuclidean'))\n    Wz = np.exp(-dist_sq_z / (2 * sigma**2))\n    \n    # 5. Compute sorted eigenvalues of W^(z)\n    eigvals_z = np.linalg.eigh(Wz)[0][::-1]\n\n    # 6. Calculate spectral change magnitude S\n    S = np.linalg.norm(eigvals_z - eigvals_0)\n\n    return S\n\ndef solve():\n    \"\"\"\n    Main function to run the specified test cases and print the results.\n    \"\"\"\n    # Test case 1: Happy path, moderate bandwidth\n    X_A = np.array([\n        [2.0, 5.0, 8.0],\n        [3.0, 6.0, 7.5],\n        [4.0, 6.5, 7.0],\n        [5.0, 7.0, 6.0],\n        [7.0, 7.5, 5.0]\n    ])\n    sigma_1 = 1.0\n\n    # Test case 2: Small bandwidth boundary\n    sigma_2 = 0.1\n\n    # Test case 3: Large bandwidth boundary\n    sigma_3 = 100.0\n\n    # Test case 4: Zero-variance gene edge case\n    X_B = np.array([\n        [0.0, 0.0, 10.0],\n        [10.0, 0.0, 10.0],\n        [20.0, 0.0, 10.0],\n        [30.0, 0.0, 10.0],\n        [40.0, 0.0, 10.0]\n    ])\n    sigma_4 = 5.0\n\n    test_cases = [\n        (X_A, sigma_1),\n        (X_A, sigma_2),\n        (X_A, sigma_3),\n        (X_B, sigma_4)\n    ]\n\n    results = []\n    for X, sigma in test_cases:\n        S = calculate_spectral_change(X, sigma)\n        # Format the result to 6 decimal places as a string\n        results.append(f\"{S:.6f}\")\n\n    # Print the final output in the specified format\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3334321"}, {"introduction": "随着单细胞技术的发展，我们面临的数据集规模越来越大，细胞数量动辄数十万甚至上百万。这使得计算效率成为算法设计的核心瓶颈，而不再是次要问题。本练习聚焦于非线性降维算法中一个关键的实际问题：如何高效地存储和处理捕捉细胞局部邻域关系的大型稀疏亲和矩阵。通过对密集存储和稀疏存储策略的内存占用进行第一性原理计算，你将具体地认识到稀疏表示对于处理大规模单细胞数据的不可或缺性。[@problem_id:3334326]", "problem": "在应用于单细胞转录组数据的非线性降维中，诸如t-分布随机邻域嵌入 (t-SNE) 和均匀流形逼近与投影 (UMAP) 等算法会构建一个$k$-最近邻图来捕捉细胞间的局部关系。假设数据集包含 $N$ 个细胞。定义一个权重矩阵 $W \\in \\mathbb{R}^{N \\times N}$，其中 $W_{ij}$ 编码了细胞 $i$ 和细胞 $j$ 之间的相似性；以及一个高维亲和矩阵 $P \\in \\mathbb{R}^{N \\times N}$，其中 $P_{ij}$ 表示细胞 $i$ 和细胞 $j$ 之间的类概率亲和度。在实践中，为了反映$k$-最近邻图的局部性约束，$W$ 和 $P$ 的每一行仅包含与该行细胞的最近邻相对应的 $k$ 个非零项，所有其他项均为零，从而产生稀疏矩阵。\n\n您将比较在嵌入过程中同时存储 $W$ 和 $P$ 的两种策略：\n1. 密集存储：将 $W$ 和 $P$ 均存储为完整的 $N \\times N$ 数组，元素为64位浮点值。\n2. 稀疏存储：使用压缩稀疏行 (CSR) 格式存储 $W$ 和 $P$，每个非零项存储为64位浮点值，其列索引存储为32位整数，行指针数组（`indptr`）为长度为 $N+1$ 的32位整数数组。假设每行恰好有 $k$ 个非零项。\n\n从稀疏表示和内存计算的基本原理出发，说明稀疏表示相对于密集存储如何减少内存占用。然后，对于 $N = 100{,}000$ 个细胞和 $k = 15$ 的情况，计算使用稀疏 CSR 表示法存储 $W$ 和 $P$ 两个矩阵与使用密集存储相比，总共节省了多少内存。最终的内存节省量以吉字节（GB）为单位表示，使用 $1 \\,\\mathrm{GB} = 10^{9}$ 字节的约定。将您的答案四舍五入到四位有效数字。", "solution": "根据既定标准对该问题进行验证。\n\n### 步骤 1：提取已知信息\n- 数据集大小：$N$ 个细胞。\n- 矩阵：权重矩阵 $W \\in \\mathbb{R}^{N \\times N}$ 和亲和矩阵 $P \\in \\mathbb{R}^{N \\times N}$。\n- 稀疏性：$W$ 和 $P$ 的每行恰好有 $k$ 个非零项。\n- 存储策略 1 (密集)：$W$ 和 $P$ 均存储为完整的 $N \\times N$ 数组，元素为64位浮点值。\n- 存储策略 2 (稀疏 CSR)：\n    - 格式：压缩稀疏行 (CSR)。\n    - 非零值：64位浮点数。\n    - 列索引：32位整数。\n    - 行指针数组 (`indptr`)：长度为 $N+1$ 的数组，使用32位整数。\n- 数值：\n    - $N = 100,000$。\n    - $k = 15$。\n- 单位换算：$1 \\,\\mathrm{GB} = 10^{9}$ 字节。\n- 输出要求：总内存节省量，以 GB 为单位，四舍五入到四位有效数字。\n\n### 步骤 2：使用提取的已知信息进行验证\n该问题具有科学依据，因为它描述了在使用 t-SNE 和 UMAP 等既有算法分析大规模单细胞数据时一个标准且关键的计算考量。使用稀疏矩阵表示来处理 $k$-最近邻图的概念是这些方法可扩展性的基础。该问题提法得当，提供了执行计算所需的所有必要参数（$N$、$k$、数据类型的比特深度）。语言客观、精确。该问题不违反任何无效标准；它是完整的、一致的、现实的且可形式化的。\n\n### 步骤 3：结论与行动\n该问题有效。将提供完整解答。\n\n### 解题推导\n该问题要求比较存储两个 $N \\times N$ 矩阵 $W$ 和 $P$ 时，密集存储与稀疏存储的内存使用情况，然后进行具体的数值计算。我们首先从基本原理出发，推导每种存储策略的内存成本。\n\n首先，我们定义基本数据类型以字节为单位的内存大小。\n- 一个64位浮点值需要 $\\frac{64}{8} = 8$ 字节。\n- 一个32位整数需要 $\\frac{32}{8} = 4$ 字节。\n\n**1. 密集存储内存计算**\n\n在密集存储格式中，矩阵的每个元素都被显式存储，无论其值是多少。\n- 一个 $N \\times N$ 矩阵包含 $N^2$ 个元素。\n- 每个元素是一个64位浮点数，占用 $8$ 字节。\n- 存储单个密集矩阵所需的总内存 $M_{\\text{dense, single}}$ 为：\n$$ M_{\\text{dense, single}} = N^2 \\times 8 \\text{ 字节} $$\n- 因为我们需要存储 $W$ 和 $P$ 两个矩阵，所以密集存储的总内存 $M_{\\text{dense, total}}$ 为：\n$$ M_{\\text{dense, total}} = 2 \\times M_{\\text{dense, single}} = 2 \\times 8 N^2 = 16 N^2 \\text{ 字节} $$\n\n**2. 稀疏存储 (CSR) 内存计算**\n\n压缩稀疏行 (CSR) 格式仅存储非零元素及其位置。它由三个数组组成：\n- `data`：存储非零元素的值。\n- `indices`：存储 `data` 中每个值对应的列索引。\n- `indptr`：一个大小为 $N+1$ 的数组，其中 `indptr[i]` 指向 `data` 和 `indices` 数组中第 `i` 行的起始位置。\n\n问题陈述， $N$ 行中的每一行都恰好有 $k$ 个非零项。\n- 一个矩阵中的非零项总数为 $N \\times k$。\n- `data` 数组的内存（存储 $Nk$ 个浮点值）：\n$$ M_{\\text{data}} = (N \\times k) \\times 8 \\text{ 字节} $$\n- `indices` 数组的内存（存储 $Nk$ 个整型列索引）：\n$$ M_{\\text{indices}} = (N \\times k) \\times 4 \\text{ 字节} $$\n- `indptr` 数组的内存（存储 $N+1$ 个整型指针）：\n$$ M_{\\text{indptr}} = (N+1) \\times 4 \\text{ 字节} $$\n- 单个矩阵以 CSR 格式存储的总内存 $M_{\\text{sparse, single}}$ 是这些部分的总和：\n$$ M_{\\text{sparse, single}} = M_{\\text{data}} + M_{\\text{indices}} + M_{\\text{indptr}} = 8Nk + 4Nk + 4(N+1) = 12Nk + 4N + 4 \\text{ 字节} $$\n- 对于 $W$ 和 $P$ 两个矩阵，稀疏存储的总内存 $M_{\\text{sparse, total}}$ 为：\n$$ M_{\\text{sparse, total}} = 2 \\times M_{\\text{sparse, single}} = 2(12Nk + 4N + 4) = 24Nk + 8N + 8 \\text{ 字节} $$\n\n**3. 概念比较与稀疏存储的合理性**\n\n当 $M_{\\text{sparse, single}}  M_{\\text{dense, single}}$ 时，稀疏存储会减少内存占用。该不等式为：\n$$ 12Nk + 4N + 4  8N^2 $$\n对于大的 $N$，低阶项 $4N$ 和 $4$ 可以忽略不计。该条件近似为：\n$$ 12Nk  8N^2 \\implies 12k  8N \\implies k  \\frac{2}{3}N $$\n在单细胞分析中，最近邻的数量 $k$（通常为 $10$-$100$）远小于细胞数量 $N$（通常为 $10^4$-$10^6$）。$k \\ll N$ 的条件确保了 $k  \\frac{2}{3}N$ 以多个数量级的优势成立。密集存储的内存需求随 $N$ 呈二次方关系（即 $O(N^2)$）增长，而稀疏存储则随 $N$ 呈线性关系（即 $O(Nk)$）增长。这种扩展性的差异使得密集存储对于大型数据集在计算上是不可行的，而稀疏存储则仍然可行。\n\n**4. 内存节省量的数值计算**\n\n总内存节省量 $\\Delta M$ 是总密集存储内存与总稀疏存储内存之差。\n$$ \\Delta M = M_{\\text{dense, total}} - M_{\\text{sparse, total}} $$\n$$ \\Delta M = 16N^2 - (24Nk + 8N + 8) \\text{ 字节} $$\n\n现在，我们代入给定值 $N = 100,000 = 10^5$ 和 $k=15$。\n\n- 计算总密集内存：\n$$ M_{\\text{dense, total}} = 16 \\times (10^5)^2 = 16 \\times 10^{10} \\text{ 字节} $$\n\n- 计算总稀疏内存：\n$$ M_{\\text{sparse, total}} = 24 \\times (10^5) \\times 15 + 8 \\times (10^5) + 8 $$\n$$ M_{\\text{sparse, total}} = 360 \\times 10^5 + 8 \\times 10^5 + 8 $$\n$$ M_{\\text{sparse, total}} = (360 + 8) \\times 10^5 + 8 = 368 \\times 10^5 + 8 = 36,800,000 + 8 = 36,800,008 \\text{ 字节} $$\n\n- 以字节为单位计算内存节省量：\n$$ \\Delta M = 160,000,000,000 - 36,800,008 = 159,963,199,992 \\text{ 字节} $$\n\n最后，我们将节省的内存转换为吉字节 (GB)，使用 $1 \\,\\mathrm{GB} = 10^{9}$ 字节的约定。\n$$ \\Delta M_{\\text{GB}} = \\frac{159,963,199,992}{10^9} = 159.963199992 \\,\\mathrm{GB} $$\n\n题目要求将答案四舍五入到四位有效数字。数值 $159.963...$ 四舍五入到四位有效数字为 $160.0$。在这种情况下，末尾的零是有效数字。", "answer": "$$\\boxed{160.0}$$", "id": "3334326"}, {"introduction": "单细胞研究的前沿正在从分析单一数据模态转向整合多种来源的信息，以获得对生物系统更全面的理解。本练习将引导你进入这一前沿领域，通过设计并实现一个算法，将单细胞RNA测序（scRNA-seq）数据与空间转录组学数据进行对齐。你将从头开始构建一个联合损失函数 $\\mathcal{L}$，它能同时优化嵌入空间以匹配基因表达相似性和物理空间邻近性，并推导其梯度，最终通过梯度下降法解决这个多模态整合问题。这个实践将为你提供一个将核心降维思想扩展到复杂、真实世界研究场景的范例。[@problem_id:3334340]", "problem": "给定在同一组单细胞上测量的两种模态：基于液滴的单细胞RNA测序 (scRNA-seq) 和空间转录组学。scRNA-seq 模态为每个细胞提供了高维表达谱，而空间模态为每个细胞在组织中定位提供了二维坐标。目标是通过优化成对距离的组合损失，推导并实现一种基于原理的非线性降维方法，该方法将 scRNA-seq 数据的嵌入与空间距离对齐。\n\n从以下基本定义和经过充分检验的事实出发：\n\n- 分子生物学中心法则 (CDMB) 断言，遗传信息从 DNA 流向 RNA，再到蛋白质。对于单细胞 RNA 测序，信使 RNA 的丰度是细胞状态的一个信息丰富的代表。这为使用表达谱之间的成对距离作为细胞状态空间结构描述符提供了理论依据。我们将限制在标准化特征上计算的成对欧几里得距离，作为一种广泛接受的基线描述符。\n\n- 对于 $\\mathbb{R}^m$ 中的任意点集 $\\{x_i\\}_{i=1}^N$，欧几里得距离定义为 $d(x_i,x_j) = \\|x_i - x_j\\|_2$，其中 $\\|\\cdot\\|_2$ 表示 $\\ell_2$ 范数。对于 $\\mathbb{R}^2$ 中的空间坐标 $\\{s_i\\}_{i=1}^N$，空间距离为 $d(s_i,s_j) = \\|s_i - s_j\\|_2$。\n\n- 在度量多维缩放 (MDS) 中，通过最小化一个应力目标函数来学习 $\\mathbb{R}^p$ 中的嵌入 $\\{y_i\\}_{i=1}^N$，该目标函数惩罚嵌入距离与目标距离矩阵之间的偏差。在这里，我们扩展这一原则，通过使用惩罚的凸组合，同时与两个目标距离矩阵对齐，一个来自 scRNA-seq 表达，另一个来自空间坐标。\n\n制定并实现以下对齐目标。令 $D^{(\\text{expr})}_{ij}$ 表示从 scRNA-seq 表达谱计算的欧几里得距离，令 $D^{(\\text{spatial})}_{ij}$ 表示从空间坐标计算的欧几里得距离。设嵌入为 $Y \\in \\mathbb{R}^{N \\times p}$，其行为 $y_i \\in \\mathbb{R}^p$，成对嵌入距离为 $R_{ij} = \\|y_i - y_j\\|_2$。定义组合损失\n$$\n\\mathcal{L}(Y;\\alpha) \\;=\\; \\sum_{1 \\le i  j \\le N} \\left[ \\alpha \\left(R_{ij} - D^{(\\text{expr})}_{ij}\\right)^2 \\;+\\; (1 - \\alpha)\\left(R_{ij} - D^{(\\text{spatial})}_{ij}\\right)^2 \\right],\n$$\n其中 $\\alpha \\in [0,1]$ 是一个模态权重，它在纯表达驱动的对齐 ($\\alpha = 1$) 和纯空间驱动的对齐 ($\\alpha = 0$) 之间进行插值。为确保跨模态距离的可通约性，您必须将每个目标距离矩阵除以其所有非零上三角项的均值来进行归一化。\n\n您的任务：\n\n- 从欧几里得距离的定义和微分链式法则出发，根据第一性原理推导 $\\mathcal{L}(Y;\\alpha)$ 相对于嵌入坐标 $Y$ 的梯度。通过极限论证来处理 $R_{ij} = 0$ 的情况，以产生数值稳定的更新。\n\n- 设计一个通过对 $Y$ 进行迭代梯度下降来最小化 $\\mathcal{L}(Y;\\alpha)$ 的算法，并满足以下设计约束和理由：\n  - 使用空间坐标初始化 $Y$，并对其进行缩放，使得非零上三角嵌入距离的均值与 $1$ 匹配，这是归一化后目标距离的均值。\n  - 在每次迭代中将 $Y$ 重新中心化，使其在所有细胞间的均值为零，以避免漂移。\n  - 使用一个固定的步长，该步长经选择以确保为提供的测试套件收敛，并在达到固定的迭代次数或在过去 $20$ 次迭代中 $\\mathcal{L}(Y;\\alpha)$ 的绝对变化低于一个小阈值时停止。\n  - 通过将任何除以 $R_{ij}$ 的操作替换为除以 $\\max(R_{ij}, \\varepsilon)$ (其中 $\\varepsilon  0$ 是一个小数) 来确保数值稳定性。\n\n- 在一个完整的、无需输入即可运行的程序中实现该算法，并为指定的测试套件生成所需的输出。\n\n测试套件规范：\n\n对于所有测试用例，设细胞数量 $N = 6$，嵌入维度 $p = 2$。空间坐标和 scRNA-seq 表达特征对每个测试用例都是固定的，如下所示。\n\n对于测试用例 1 到 3，使用相同的空间坐标和表达特征：\n\n空间坐标矩阵 $S \\in \\mathbb{R}^{6 \\times 2}$:\n$$\nS \\;=\\; \\begin{bmatrix}\n0.0  0.0 \\\\\n1.0  0.0 \\\\\n2.0  0.0 \\\\\n0.0  1.0 \\\\\n1.0  1.0 \\\\\n2.0  1.0\n\\end{bmatrix}.\n$$\n\n表达特征矩阵 $X \\in \\mathbb{R}^{6 \\times 3}$，其行定义为\n$$\nx\\_i^{(1)} \\;=\\; x\\_i^{(\\text{spatial-x})} \\;+\\; \\delta\\_i^{(1)}, \\quad\nx\\_i^{(2)} \\;=\\; x\\_i^{(\\text{spatial-y})} \\;+\\; \\delta\\_i^{(2)}, \\quad\nx\\_i^{(3)} \\;=\\; 0.5\\, x\\_i^{(\\text{spatial-x})} \\;+\\; 0.5\\, x\\_i^{(\\text{spatial-y})} \\;+\\; \\delta\\_i^{(3)},\n$$\n其中空间坐标是 $S$ 对应行的条目，确定性扰动为\n$$\n\\delta^{(1)} \\;=\\; \\begin{bmatrix} 0.00  0.05  -0.05  0.025  -0.025  0.00 \\end{bmatrix}^\\top, \\quad\n\\delta^{(2)} \\;=\\; \\begin{bmatrix} 0.01  -0.01  0.015  -0.015  0.00  0.02 \\end{bmatrix}^\\top, \\quad\n\\delta^{(3)} \\;=\\; \\begin{bmatrix} 0.02  0.00  -0.02  0.03  -0.03  0.01 \\end{bmatrix}^\\top.\n$$\n测试用例 1：使用 $\\alpha = 0.5$。\n\n测试用例 2：使用 $\\alpha = 1.0$。\n\n测试用例 3：使用 $\\alpha = 0.0$。\n\n对于测试用例 4，使用相同的空间坐标 $S$，但将表达特征乘以因子 $3.0$，即使用 $X^{(\\text{scaled})} = 3.0 \\cdot X$，并设置 $\\alpha = 0.3$。\n\n对于测试用例 5，修改空间坐标，将最后一个点与第五个点重复，以使一对点的空间距离为零：\n$$\nS^{(\\text{dup})} \\;=\\; \\begin{bmatrix}\n0.0  0.0 \\\\\n1.0  0.0 \\\\\n2.0  0.0 \\\\\n0.0  1.0 \\\\\n1.0  1.0 \\\\\n1.0  1.0\n\\end{bmatrix},\n$$\n使用原始表达特征 $X$，并设置 $\\alpha = 0.5$。\n\n距离计算和归一化：\n\n- 对于每个测试用例，将 $D^{(\\text{expr})}$ 计算为相关表达矩阵（$X$ 或 $X^{(\\text{scaled})}$）行的成对欧几里得距离，将 $D^{(\\text{spatial})}$ 计算为相关空间矩阵（$S$ 或 $S^{(\\text{dup})}$）行的成对欧几里得距离。\n\n- 将 $D^{(\\text{expr})}$ 和 $D^{(\\text{spatial})}$ 分别除以其非零上三角项的均值进行归一化，使得归一化后非零距离的均值为 $1$。\n\n算法超参数：\n\n- 使用一个小的正常数 $\\varepsilon = 10^{-8}$ 以保证除以 $R_{ij}$ 时的数值稳定性。\n\n- 使用固定步长 $\\eta = 0.05$。\n\n- 最多使用 $1000$ 次迭代，如果过去 $20$ 次迭代中 $\\mathcal{L}(Y;\\alpha)$ 的绝对变化低于 $10^{-9}$，则提前停止。\n\n最终输出格式：\n\n您的程序应生成一行输出，其中包含五个测试用例的结果，形式为方括号内以逗号分隔的列表，按测试用例 1 到 5 的顺序排列，每个结果是 $\\mathcal{L}(Y;\\alpha)$ 的最终值，为浮点数（无量纲单位）。例如，输出必须是以下形式\n$$\n[\\ell\\_1,\\ell\\_2,\\ell\\_3,\\ell\\_4,\\ell\\_5],\n$$\n其中 $\\ell\\_k$ 表示测试用例 $k$ 的最终损失。", "solution": "将嵌入与单细胞表达数据和空间坐标对齐的问题，被表述为最小化一个组合的类应力损失函数。解决方案需要推导此损失函数的梯度，并实现一个迭代梯度下降算法来找到最优的低维嵌入。\n\n### 1. 梯度推导\n\n组合损失函数 $\\mathcal{L}(Y;\\alpha)$ 定义为所有唯一细胞对 $(i,j)$（其中 $1 \\le i  j \\le N$）的和：\n$$\n\\mathcal{L}(Y;\\alpha) \\;=\\; \\sum_{1 \\le i  j \\le N} \\left[ \\alpha \\left(R_{ij} - D^{(\\text{expr})}_{ij}\\right)^2 \\;+\\; (1 - \\alpha)\\left(R_{ij} - D^{(\\text{spatial})}_{ij}\\right)^2 \\right]\n$$\n此处，$Y \\in \\mathbb{R}^{N \\times p}$ 是嵌入坐标矩阵，其行为 $y_i \\in \\mathbb{R}^p$，$R_{ij} = \\|y_i - y_j\\|_2$ 是嵌入中点 $i$ 和 $j$ 之间的欧几里得距离，$D^{(\\text{expr})}_{ij}$ 和 $D^{(\\text{spatial})}_{ij}$ 分别是来自表达和空间模态的目标距离，$\\alpha \\in [0,1]$ 是一个加权参数。为简化起见，我们定义一个加权目标距离 $W_{ij} = \\alpha D^{(\\text{expr})}_{ij} + (1 - \\alpha)D^{(\\text{spatial})}_{ij}$。单个对 $(i,j)$ 的损失项可以通过展开平方重写为：\n$$\n\\mathcal{L}_{ij} = \\alpha(R_{ij}^2 - 2R_{ij}D^{(\\text{expr})}_{ij} + (D^{(\\text{expr})}_{ij})^2) + (1-\\alpha)(R_{ij}^2 - 2R_{ij}D^{(\\text{spatial})}_{ij} + (D^{(\\text{spatial})}_{ij})^2)\n$$\n$$\n\\mathcal{L}_{ij} = R_{ij}^2 - 2R_{ij}(\\alpha D^{(\\text{expr})}_{ij} + (1-\\alpha)D^{(\\text{spatial})}_{ij}) + \\text{const} = R_{ij}^2 - 2R_{ij}W_{ij} + \\text{const}\n$$\n总损失为 $\\mathcal{L} = \\sum_{1 \\le i  j \\le N} \\mathcal{L}_{ij}$。我们需要计算关于单个细胞 $k$ 坐标的梯度，即向量 $\\frac{\\partial \\mathcal{L}}{\\partial y_k}$。坐标向量 $y_k$ 出现在任何 $i=k$ 或 $j=k$ 的项 $\\mathcal{L}_{ij}$ 中。$y_k$ 的总梯度是这些项的偏导数之和：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial y_k} = \\sum_{j > k} \\frac{\\partial \\mathcal{L}_{kj}}{\\partial y_k} + \\sum_{i  k} \\frac{\\partial \\mathcal{L}_{ik}}{\\partial y_k}\n$$\n我们应用链式法则，$\\frac{\\partial \\mathcal{L}_{ij}}{\\partial y_k} = \\frac{\\partial \\mathcal{L}_{ij}}{\\partial R_{ij}} \\frac{\\partial R_{ij}}{\\partial y_k}$。\n\n首先，我们求 $\\mathcal{L}_{ij}$ 对 $R_{ij}$ 的导数：\n$$\n\\frac{\\partial \\mathcal{L}_{ij}}{\\partial R_{ij}} = 2\\alpha(R_{ij} - D^{(\\text{expr})}_{ij}) + 2(1-\\alpha)(R_{ij} - D^{(\\text{spatial})}_{ij}) = 2(R_{ij} - W_{ij})\n$$\n接着，我们求 $R_{ij} = \\|y_i - y_j\\|_2$ 对 $y_i$ 和 $y_j$ 的导数。使用欧几里得范数的标准导数：\n$$\n\\frac{\\partial R_{ij}}{\\partial y_i} = \\frac{y_i - y_j}{\\|y_i - y_j\\|_2} = \\frac{y_i - y_j}{R_{ij}} \\quad \\text{和} \\quad \\frac{\\partial R_{ij}}{\\partial y_j} = \\frac{y_j - y_i}{\\|y_i - y_j\\|_2} = -\\frac{y_i - y_j}{R_{ij}}\n$$\n对于任何 $k \\ne i, j$，$\\frac{\\partial R_{ij}}{\\partial y_k}$ 是零向量。\n\n现在我们组合这些结果。对于一个项 $\\mathcal{L}_{kj}$ 且 $j>k$：\n$$\n\\frac{\\partial \\mathcal{L}_{kj}}{\\partial y_k} = \\frac{\\partial \\mathcal{L}_{kj}}{\\partial R_{kj}} \\frac{\\partial R_{kj}}{\\partial y_k} = 2(R_{kj} - W_{kj}) \\frac{y_k - y_j}{R_{kj}} = 2\\left(1 - \\frac{W_{kj}}{R_{kj}}\\right)(y_k - y_j)\n$$\n对于一个项 $\\mathcal{L}_{ik}$ 且 $i  k$：\n$$\n\\frac{\\partial \\mathcal{L}_{ik}}{\\partial y_k} = \\frac{\\partial \\mathcal{L}_{ik}}{\\partial R_{ik}} \\frac{\\partial R_{ik}}{\\partial y_k} = 2(R_{ik} - W_{ik}) \\frac{y_k - y_i}{R_{ik}} = 2\\left(1 - \\frac{W_{ik}}{R_{ik}}\\right)(y_k - y_i)\n$$\n由于 $R_{ik} = R_{ki}$ 和 $W_{ik} = W_{ki}$，$\\frac{\\partial \\mathcal{L}_{ik}}{\\partial y_k}$ 的形式与 $\\frac{\\partial \\mathcal{L}_{kj}}{\\partial y_k}$ 相同。因此，总梯度为：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial y_k} = \\sum_{j \\ne k} 2\\left(1 - \\frac{W_{kj}}{R_{kj}}\\right)(y_k - y_j)\n$$\n当 $R_{kj} \\to 0$ 时，如果 $W_{kj} > 0$，则梯度项会发散。通过极限论证，当 $y_k \\to y_j$ 时，向量 $y_k - y_j$ 趋于零，而 $1/R_{kj}$ 项趋于无穷大。然而，如果 $W_{kj}=0$，则梯度为 $2(y_k - y_j) \\to 0$。如果 $W_{kj} > 0$，则梯度发散，表明存在一个强排斥力，防止点重合。在数值实现中，我们将分母 $R_{kj}$ 替换为 $\\max(R_{kj}, \\varepsilon)$ 来避免除以零，其中 $\\varepsilon$ 是一个小的正数。\n\n### 2. 算法设计\n\n该算法将通过梯度下降来最小化 $\\mathcal{L}$。\n\n1.  **数据预处理**：\n    a. 计算 $N \\times N$ 的距离矩阵 $D^{(\\text{expr})}$ 和 $D^{(\\text{spatial})}$。\n    b. 对于每个矩阵，计算其非零上三角项的均值。\n    c. 将每个矩阵除以其各自的均值进行归一化。\n2.  **初始化**：\n    a. 将嵌入 $Y$ 初始化为空间坐标 $S$。\n    b. 计算初始嵌入 $Y$ 的非零上三角距离的均值。\n    c. 用这个均值缩放 $Y$，使其均值为 $1$。\n3.  **迭代优化**：\n    a. 对于固定次数的迭代或直到收敛：\n    b. **中心化**：从 $Y$ 的每一行减去 $Y$ 的列均值，即 $Y \\leftarrow Y - \\bar{Y}$。\n    c. **计算梯度**：对于每个细胞 $k$，使用上面推导的公式计算梯度 $\\frac{\\partial \\mathcal{L}}{\\partial y_k}$。这可以向量化以提高效率。令 $C_{kj} = 2(1 - W_{kj}/\\max(R_{kj}, \\varepsilon))$。那么 $\\frac{\\partial \\mathcal{L}}{\\partial y_k} = \\sum_{j \\ne k} C_{kj}(y_k - y_j)$。这可以写成图拉普拉斯算子的形式。令 $L$ 为拉普拉斯矩阵，其中 $L_{kk} = \\sum_{j \\ne k} C_{kj}$ 且 $L_{kj} = -C_{kj}$ for $k \\ne j$。则梯度矩阵 $\\nabla_Y \\mathcal{L}$ 的第 $k$ 行为 $(LY)_k$。\n    d. **更新**：使用固定步长 $\\eta$ 更新嵌入：$Y \\leftarrow Y - \\eta \\nabla_Y \\mathcal{L}$。\n    e. **收敛检查**：计算当前损失 $\\mathcal{L}$。如果与前 $20$ 次迭代的损失相比，绝对变化小于阈值，则停止。\n4.  **输出**：返回最终的损失值 $\\mathcal{L}$。\n\n该设计符合所有约束条件，并为解决优化问题提供了一个稳健的框架。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.spatial.distance import pdist, squareform\n\n# Hyperparameters as specified in the problem statement\nEPS = 1e-8\nSTEP_SIZE = 0.05\nMAX_ITER = 1000\nSTOP_WINDOW = 20\nSTOP_TOL = 1e-9\n\ndef solve():\n    \"\"\"\n    Main function to orchestrate the test cases and print the final result.\n    \"\"\"\n\n    def run_optimization(alpha, S_in, X_in):\n        \"\"\"\n        Performs the gradient descent optimization for a single test case.\n        \"\"\"\n        # 1. Preprocessing: compute and normalize distance matrices\n        D_spatial = squareform(pdist(S_in, 'euclidean'))\n        D_expr = squareform(pdist(X_in, 'euclidean'))\n\n        # Normalize by mean of non-zero upper-triangular entries.\n        D_spatial_ut = pdist(S_in, 'euclidean')\n        mean_spatial = D_spatial_ut[D_spatial_ut > 0].mean() if np.any(D_spatial_ut > 0) else 0.\n        D_spatial_norm = D_spatial / mean_spatial if mean_spatial > 0 else D_spatial\n\n        D_expr_ut = pdist(X_in, 'euclidean')\n        mean_expr = D_expr_ut[D_expr_ut > 0].mean() if np.any(D_expr_ut > 0) else 0.\n        D_expr_norm = D_expr / mean_expr if mean_expr > 0 else D_expr\n        \n        # Weighted target distance matrix\n        W = alpha * D_expr_norm + (1 - alpha) * D_spatial_norm\n        \n        # 2. Initialization\n        Y = S_in.copy().astype(np.float64) # Use float64 for precision\n\n        # Scale Y so that the mean of its nonzero upper-triangular distances is 1.\n        R_init_ut = pdist(Y, 'euclidean')\n        mean_R_init = R_init_ut[R_init_ut > 0].mean() if np.any(R_init_ut > 0) else 0.\n        if mean_R_init > 0:\n            Y /= mean_R_init\n            \n        # 3. Iterative Optimization\n        loss_history = []\n        \n        for i in range(MAX_ITER):\n            # Recenter at each iteration to prevent drift\n            Y -= Y.mean(axis=0)\n\n            # Compute current embedding distances\n            R = squareform(pdist(Y, 'euclidean'))\n\n            # Compute loss\n            loss_term1 = alpha * np.square(R - D_expr_norm)\n            loss_term2 = (1 - alpha) * np.square(R - D_spatial_norm)\n            loss = np.sum(np.triu(loss_term1 + loss_term2, k=1))\n            loss_history.append(loss)\n\n            # Check for early stopping\n            if i >= STOP_WINDOW:\n                if abs(loss_history[-1] - loss_history[-STOP_WINDOW])  STOP_TOL:\n                    break\n            \n            # Compute gradient using the vectorized approach\n            R_safe = np.maximum(R, EPS)\n            ratio_matrix = W / R_safe\n            np.fill_diagonal(ratio_matrix, 0)\n            \n            C_matrix = 2 * (1 - ratio_matrix)\n            np.fill_diagonal(C_matrix, 0)\n                    \n            L_lap = np.diag(C_matrix.sum(axis=1)) - C_matrix\n            \n            grad = L_lap @ Y\n            \n            # Update embedding coordinates\n            Y -= STEP_SIZE * grad\n\n        # Final loss calculation after optimization loop\n        R_final = squareform(pdist(Y, 'euclidean'))\n        loss_term1_final = alpha * np.square(R_final - D_expr_norm)\n        loss_term2_final = (1 - alpha) * np.square(R_final - D_spatial_norm)\n        final_loss = np.sum(np.triu(loss_term1_final + loss_term2_final, k=1))\n        \n        return final_loss\n\n    # Define the test cases from the problem statement.\n    S = np.array([\n        [0.0, 0.0], [1.0, 0.0], [2.0, 0.0],\n        [0.0, 1.0], [1.0, 1.0], [2.0, 1.0]\n    ])\n\n    delta1 = np.array([0.00, 0.05, -0.05, 0.025, -0.025, 0.00])\n    delta2 = np.array([0.01, -0.01, 0.015, -0.015, 0.00, 0.02])\n    delta3 = np.array([0.02, 0.00, -0.02, 0.03, -0.03, 0.01])\n    \n    X = np.zeros((6, 3))\n    X[:, 0] = S[:, 0] + delta1\n    X[:, 1] = S[:, 1] + delta2\n    X[:, 2] = 0.5 * S[:, 0] + 0.5 * S[:, 1] + delta3\n\n    X_scaled = 3.0 * X\n    S_dup = np.array([\n        [0.0, 0.0], [1.0, 0.0], [2.0, 0.0],\n        [0.0, 1.0], [1.0, 1.0], [1.0, 1.0]\n    ])\n\n    test_cases = [\n        {'alpha': 0.5, 'S': S, 'X': X},\n        {'alpha': 1.0, 'S': S, 'X': X},\n        {'alpha': 0.0, 'S': S, 'X': X},\n        {'alpha': 0.3, 'S': S, 'X': X_scaled},\n        {'alpha': 0.5, 'S': S_dup, 'X': X}\n    ]\n\n    results = []\n    for case in test_cases:\n        final_loss = run_optimization(case['alpha'], case['S'], case['X'])\n        results.append(final_loss)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.12f}' for r in results)}]\")\n\nsolve()\n```", "id": "3334340"}]}