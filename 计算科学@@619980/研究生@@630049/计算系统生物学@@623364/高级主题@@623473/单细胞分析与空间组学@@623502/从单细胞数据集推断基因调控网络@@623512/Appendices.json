{"hands_on_practices": [{"introduction": "在推断复杂的调控网络之前，我们必须首先正确地处理原始的单细胞数据。标准化和数据转换是消除技术噪音和稳定方差的基础步骤，对于确保下游分析的可靠性至关重要。\n\n这项实践 ([@problem_id:3314516]) 提供了两种基本数据转换技术的动手经验：对数正态化和正则化对数转换。通过比较它们对基因-基因相关性的影响，你将更深入地理解数据预处理选择如何从根本上影响网络分析的结果，为后续的推断任务奠定坚实的基础。", "problem": "给定单细胞核糖核酸（RNA）测序的计数数据，其形式为一个非负整数矩阵，其中包含多个基因在多个细胞中的计数值，以及一个表示每个细胞测序深度的正值大小因子向量。您的任务是实现计数数据的两种变换：一种是带伪计数的对数归一化，另一种是方差稳定的正则化对数变换。然后，量化正则化对指定基因对之间的皮尔逊积矩相关系数的影响。\n\n请从以下单细胞基因调控网络分析的基本基础开始：\n- 分子生物学的中心法则指出，脱氧核糖核酸（DNA）转录为核糖核酸（RNA），再翻译成蛋白质；基因表达通过每个细胞中每个基因的核糖核酸转录本计数来量化。\n- 单细胞核糖核酸测序计数通常能很好地用离散计数数据来建模，通常使用负二项分布。测序深度是原始计数的混淆因素，因此除以每个细胞的大小因子是一个广泛接受的归一化步骤。\n- 自然对数可以减小乘法尺度，并在与正伪计数一起使用时部分稳定计数数据的方差，以避免出现未定义的$\\log(0)$。\n- 正则化，例如岭式收缩，通过将偏差向中心趋势收缩来减小方差。\n- 皮尔逊积矩相关系数衡量两个变量在样本间的线性关联。\n\n设$C \\in \\mathbb{N}_0^{G \\times N}$表示计数矩阵，其中有$G$个基因和$N$个细胞，设$s \\in \\mathbb{R}_+^{N}$表示正值大小因子向量。对于一个正伪计数$\\alpha \\in \\mathbb{R}_+$，对数归一化表达矩阵$L \\in \\mathbb{R}^{G \\times N}$的定义为逐项计算：\n$$\nL_{ij} = \\log\\left(\\frac{C_{ij}}{s_j} + \\alpha\\right),\n$$\n其中$\\log$表示自然对数。为了构建方差稳定的正则化对数变换，使用从归一化平均表达量派生的基因特异性岭参数，将基因层面的对数表达偏差向基因层面的均值收缩。具体来说，设归一化（非对数）表达量为$X_{ij} = \\frac{C_{ij}}{s_j}$，基因层面的归一化均值为$\\bar{X}_i = \\frac{1}{N}\\sum_{j=1}^{N} X_{ij}$。对于一个正则化尺度$\\beta \\in \\mathbb{R}_+$和一个用于避免除以零的小正常数$\\varepsilon \\in \\mathbb{R}_+$，定义基因特异性正则化强度为：\n$$\n\\lambda_i = \\frac{\\beta}{\\bar{X}_i + \\varepsilon}.\n$$\n设基因层面的对数均值为$\\mu_i = \\frac{1}{N} \\sum_{j=1}^{N} L_{ij}$。正则化对数变换$R \\in \\mathbb{R}^{G \\times N}$则通过收缩围绕$\\mu_i$的偏差来逐项定义：\n$$\nR_{ij} = \\mu_i + \\frac{1}{1+\\lambda_i}\\left(L_{ij} - \\mu_i\\right).\n$$\n\n对于任何基因对$(a,b)$，其中$a \\in \\{0,1,\\dots,G-1\\}$和$b \\in \\{0,1,\\dots,G-1\\}$，对于变换$T \\in \\{L,R\\}$，定义其在细胞间的皮尔逊积矩相关系数为：\n$$\n\\rho_T(a,b) = \\frac{\\sum_{j=1}^{N} \\left(T_{a j} - \\bar{T}_a\\right)\\left(T_{b j} - \\bar{T}_b\\right)}{\\sqrt{\\sum_{j=1}^{N} \\left(T_{a j} - \\bar{T}_a\\right)^2}\\sqrt{\\sum_{j=1}^{N} \\left(T_{b j} - \\bar{T}_b\\right)^2}},\n$$\n其中$\\bar{T}_a = \\frac{1}{N}\\sum_{j=1}^{N} T_{a j}$且$\\bar{T}_b = \\frac{1}{N}\\sum_{j=1}^{N} T_{b j}$。如果任一基因在某种变换下方差为零（即分母为零），则定义该变换下的$\\rho_T(a,b)$为$0$。\n\n通过计算每个指定基因对$(a,b)$的差值来评估方差稳定化如何改变相关性：\n$$\n\\Delta(a,b) = \\rho_R(a,b) - \\rho_L(a,b).\n$$\n\n实现一个程序，对于一小组带有附带参数的计数矩阵测试套件，计算每个测试用例中指定基因对的$\\Delta(a,b)$的算术平均值。最终输出必须将所有提供的测试用例的结果汇总为单行，形式为用方括号括起来的逗号分隔列表。\n\n使用以下测试套件：\n\n- 测试用例1（一般情况，具有中等计数、一些零值和异构的大小因子）：\n$$\nC^{(1)} = \\begin{bmatrix}\n12  & 0  & 5  & 20\\\\\n0  & 2  & 0  & 8\\\\\n30  & 25  & 0  & 10\n\\end{bmatrix}, \\quad\ns^{(1)} = [0.9, 1.1, 0.8, 1.3], \\quad\n\\alpha^{(1)} = 0.5, \\quad\n\\beta^{(1)} = 1.0, \\quad\n\\varepsilon^{(1)} = 10^{-6}, \\quad\nP^{(1)} = \\{(0,1), (0,2)\\}.\n$$\n\n- 测试用例2（边界情况，具有大量零值和相等的大小因子）：\n$$\nC^{(2)} = \\begin{bmatrix}\n0  & 0  & 0  & 1 & 0\\\\\n3  & 0  & 0  & 0 & 2\n\\end{bmatrix}, \\quad\ns^{(2)} = [1.0, 1.0, 1.0, 1.0, 1.0], \\quad\n\\alpha^{(2)} = 10^{-3}, \\quad\n\\beta^{(2)} = 2.0, \\quad\n\\varepsilon^{(2)} = 10^{-6}, \\quad\nP^{(2)} = \\{(0,1)\\}.\n$$\n\n- 测试用例3（边缘情况，具有极端的大小因子异构性）：\n$$\nC^{(3)} = \\begin{bmatrix}\n100 & 200 & 300\\\\\n5   & 10  & 15\\\\\n0   & 0   & 1\n\\end{bmatrix}, \\quad\ns^{(3)} = [0.5, 1.0, 5.0], \\quad\n\\alpha^{(3)} = 1.0, \\quad\n\\beta^{(3)} = 0.5, \\quad\n\\varepsilon^{(3)} = 10^{-3}, \\quad\nP^{(3)} = \\{(0,1), (1,2)\\}.\n$$\n\n您的程序必须：\n- 对于每个测试用例，计算$L$、$R$，然后对测试用例$k$的所有$(a,b) \\in P^{(k)}$计算$\\Delta(a,b)$，最后计算在$P^{(k)}$上的算术平均值。\n- 生成单行输出，其中包含按顺序$k = 1, 2, 3$排列的测试用例的平均差值，格式为逗号分隔的列表并用方括号括起（例如, $[x_1,x_2,x_3]$）。不涉及单位；所有计算都是无量纲的实数。", "solution": "该问题要求在单细胞转录组学中实现并比较两种常见的数据变换方法：一种是标准的对数归一化，另一种是方差稳定的正则化对数变换。目标是量化正则化对基因对之间皮尔逊相关性的影响。该过程基于单细胞数据分析的既定原则，即原始计数需针对技术性因素（如测序深度）进行调整，并通过变换来稳定方差，以便于进行下游分析，如基因调控网络推断。\n\n每个测试用例的处理流程如下：\n\n首先，我们处理不同细胞间测序深度可变所带来的混淆效应。原始计数矩阵$C \\in \\mathbb{N}_0^{G \\times N}$（其中$G$为基因数，$N$为细胞数）通过每个细胞的大小因子$s \\in \\mathbb{R}_+^{N}$进行归一化。这会产生归一化表达矩阵$X \\in \\mathbb{R}_{\\ge 0}^{G \\times N}$，其元素为$X_{ij} = \\frac{C_{ij}}{s_j}$。这一步将每个细胞的计数置于可比较的尺度上。\n\n其次，我们应用标准的对数归一化。基因表达数据通常跨越几个数量级，并表现出均值-方差关系，即更高的计数具有更高的方差。应用自然对数可以压缩此范围并减轻异方差性。为处理零计数的问题（因为$\\log(0)$未定义），会添加一个正伪计数$\\alpha \\in \\mathbb{R}_+$。因此，对数归一化表达矩阵$L \\in \\mathbb{R}^{G \\times N}$通过逐项计算得出：\n$$\nL_{ij} = \\log\\left(X_{ij} + \\alpha\\right) = \\log\\left(\\frac{C_{ij}}{s_j} + \\alpha\\right)\n$$\n\n第三，我们构建方差稳定的正则化对数变换。虽然对数变换有所帮助，但它并不能完美地稳定方差。正则化或称收缩，是一种通过将表达值拉向中心趋势来进一步减小方差的技术。在这里，我们实现了一种基因特异性的岭式收缩。每个基因$i$的收缩强度（表示为$\\lambda_i$）被设计为与其平均归一化表达量$\\bar{X}_i = \\frac{1}{N}\\sum_{j=1}^{N} X_{ij}$成反比。其动机是观察到低表达基因往往具有更高的相对方差（即噪声更大），因此能从更强的正则化中受益。对于给定的正则化尺度$\\beta \\in \\mathbb{R}_+$和一个用于防止除以零的小常数$\\varepsilon \\in \\mathbb{R}_+$，正则化强度为：\n$$\n\\lambda_i = \\frac{\\beta}{\\bar{X}_i + \\varepsilon}\n$$\n然后，通过将对数表达偏差$(L_{ij} - \\mu_i)$向基因层面的对数均值$\\mu_i = \\frac{1}{N} \\sum_{j=1}^{N} L_{ij}$收缩，来获得正则化矩阵$R \\in \\mathbb{R}^{G \\times N}$。正则化值$R_{ij}$的计算公式为：\n$$\nR_{ij} = \\mu_i + \\frac{1}{1+\\lambda_i}\\left(L_{ij} - \\mu_i\\right)\n$$\n项$\\frac{1}{1+\\lambda_i}$充当收缩因子。当一个基因的平均表达量$\\bar{X}_i$较低时，$\\lambda_i$较大，使得收缩因子变小，从而将值$L_{ij}$强烈地拉向均值$\\mu_i$。相反，对于高表达基因，$\\lambda_i$较小，其值仅受到弱正则化。\n\n第四，我们使用皮尔逊积矩相关系数$\\rho$来量化指定基因对$(a,b)$之间的线性关联。这对对数归一化数据（$L$）和正则化数据（$R$）都进行计算。对于给定的变换矩阵$T \\in \\{L, R\\}$，基因$a$和基因$b$之间的相关性为：\n$$\n\\rho_T(a,b) = \\frac{\\sum_{j=1}^{N} \\left(T_{a j} - \\bar{T}_a\\right)\\left(T_{b j} - \\bar{T}_b\\right)}{\\sqrt{\\sum_{j=1}^{N} \\left(T_{a j} - \\bar{T}_a\\right)^2}\\sqrt{\\sum_{j=1}^{N} \\left(T_{b j} - \\bar{T}_b\\right)^2}}\n$$\n其中$\\bar{T}_a$和$\\bar{T}_b$是变换后表达值在细胞间的基因层面均值。根据规定，如果一对基因中任一基因的表达在细胞间方差为零（即其表达是恒定的），则分母变为零，相关性$\\rho_T(a,b)$被定义为$0$。\n\n最后，为了评估正则化的影响，我们计算相关系数的差值$\\Delta(a,b) = \\rho_R(a,b) - \\rho_L(a,b)$，该计算针对测试用例$k$中指定集合$P^{(k)}$的每个基因对$(a,b)$进行。每个测试用例的最终结果是这些差值在$P^{(k)}$中所有基因对上的算术平均值。实现将对每个提供的测试用例执行这些步骤，并将平均差值汇总到单个输出列表中。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the full workflow for all test cases and prints the final result.\n    \"\"\"\n    \n    # Test case 1\n    C1 = np.array([[12, 0, 5, 20],\n                   [0, 2, 0, 8],\n                   [30, 25, 0, 10]], dtype=np.float64)\n    s1 = np.array([0.9, 1.1, 0.8, 1.3], dtype=np.float64)\n    alpha1 = 0.5\n    beta1 = 1.0\n    epsilon1 = 1e-6\n    P1 = [(0, 1), (0, 2)]\n\n    # Test case 2\n    C2 = np.array([[0, 0, 0, 1, 0],\n                   [3, 0, 0, 0, 2]], dtype=np.float64)\n    s2 = np.array([1.0, 1.0, 1.0, 1.0, 1.0], dtype=np.float64)\n    alpha2 = 1e-3\n    beta2 = 2.0\n    epsilon2 = 1e-6\n    P2 = [(0, 1)]\n\n    # Test case 3\n    C3 = np.array([[100, 200, 300],\n                   [5, 10, 15],\n                   [0, 0, 1]], dtype=np.float64)\n    s3 = np.array([0.5, 1.0, 5.0], dtype=np.float64)\n    alpha3 = 1.0\n    beta3 = 0.5\n    epsilon3 = 1e-3\n    P3 = [(0, 1), (1, 2)]\n\n    test_cases = [\n        (C1, s1, alpha1, beta1, epsilon1, P1),\n        (C2, s2, alpha2, beta2, epsilon2, P2),\n        (C3, s3, alpha3, beta3, epsilon3, P3),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        C, s, alpha, beta, epsilon, P = case\n        \n        # Step 1: Normalize by size factors\n        X = C / s\n        \n        # Step 2: Log-normalize with pseudocount\n        L = np.log(X + alpha)\n        \n        # Step 3: Calculate parameters for regularization\n        # Gene-wise normalized mean\n        X_bar = np.mean(X, axis=1)\n        \n        # Gene-specific regularization strength\n        lambda_i = beta / (X_bar + epsilon)\n        \n        # Gene-wise log mean\n        mu_i = np.mean(L, axis=1)\n        \n        # Step 4: Apply regularized log transform\n        # Reshape for broadcasting:\n        # L has shape (G, N)\n        # mu_i and lambda_i have shape (G,). Reshape to (G, 1) to operate on each row.\n        mu_i_reshaped = mu_i.reshape(-1, 1)\n        lambda_i_reshaped = lambda_i.reshape(-1, 1)\n        \n        shrinkage_factor = 1.0 / (1.0 + lambda_i_reshaped)\n        deviations = L - mu_i_reshaped\n        R = mu_i_reshaped + shrinkage_factor * deviations\n        \n        # Step 5: Compute correlation differences\n        \n        def pearson_corr(v_a, v_b):\n            \"\"\"\n            Computes Pearson correlation, returning 0 if variance of either vector is 0.\n            \"\"\"\n            # Using a small tolerance for floating point comparisons\n            if np.isclose(np.var(v_a), 0.0) or np.isclose(np.var(v_b), 0.0):\n                return 0.0\n            \n            # np.corrcoef calculates the correlation matrix. We need the off-diagonal element.\n            # It handles the mean-centering and normalization internally.\n            corr_matrix = np.corrcoef(v_a, v_b)\n            return corr_matrix[0, 1]\n\n        deltas = []\n        for a, b in P:\n            # Correlation on log-normalized data\n            rho_L = pearson_corr(L[a, :], L[b, :])\n            \n            # Correlation on regularized data\n            rho_R = pearson_corr(R[a, :], R[b, :])\n            \n            # Difference\n            delta = rho_R - rho_L\n            deltas.append(delta)\n            \n        # Step 6: Calculate mean difference for the test case\n        mean_delta = np.mean(deltas)\n        results.append(mean_delta)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3314516"}, {"introduction": "一类主要的基因调控网络推断方法，即基于约束的算法，通过系统性地检验基因间的条件独立性来重建网络。Peter-Clark (PC) 算法是该方法的经典范例，为从观测数据中推断因果结构提供了一个清晰的逻辑框架。\n\n在这个练习中 ([@problem_id:3314528])，你将从零开始实现 PC 算法，包括执行条件独立性检验和基于 V-结构确定边的方向。这项实践将巩固你对图模型如何将数据中的统计独立性与底层网络拓扑联系起来的理解，其中包含了马尔可夫等价类这一关键概念。", "problem": "给定离散化的单细胞基因表达状态，表示为整数值数组，其中每一列对应一个基因，每一行对应一个细胞。您的任务是，从基本原理出发，实现一个基于约束的 Peter-Clark (PC) 算法，该算法从数据中的条件独立性学习一个完全部分有向无环图 (CPDAG)。您必须为每个数据集量化学习到的 CPDAG 中定向边的数量与保持无向的边的数量。算法规范必须源于条件独立性、有向无环图 (DAG) 中的图分离以及离散多项模型中独立性的似然检验等基本定义。\n\n您必须使用的基本依据：\n- 条件独立性的定义：随机变量 $X$ 和 $Y$ 在给定集合 $Z$ 的条件下是条件独立的，当且仅当对于所有具有非零概率的值，$p(x,y \\mid z) = p(x \\mid z)p(y \\mid z)$。\n- 有向无环图 (DAG) 中的图分离（也称为 $d$-分离）：由 DAG 结构所蕴含的条件独立关系。\n- 多项分布计数的似然原理：在（条件）独立性的原假设下，列联表中的对数似然比统计量在适当的正则性条件下，遵循一个渐近卡方分布，其自由度由自由参数数量减去约束数量决定。\n\n您的程序必须实现：\n1) 骨架发现：从变量上的完全无向图开始，当存在一个大小为 $\\lvert S \\rvert \\leq k_{\\max}$ 的条件集 $S$，使得似然比检验在水平 $\\alpha$ 下未能拒绝 $X \\perp Y \\mid S$ 的原假设时，迭代地移除变量 $X$ 和 $Y$ 之间的无向边。为每个移除的对 $\\{X,Y\\}$ 记录一个这样的分离集 $S$。\n2) V-结构定向：对于任何三元组 $X - Z - Y$，其中 $X$ 和 $Y$ 在骨架中不相邻，且 $Z$ 不在为 $\\{X,Y\\}$ 记录的分离集中，则定向为 $X \\to Z \\leftarrow Y$。\n3) Meek 的定向传播（重复应用直至无变化）：如果存在 $A \\to B - C$ 且 $A$ 和 $C$ 不相邻，则将 $B - C$ 定向为 $B \\to C$。\n\n独立性检验必须使用根据经验计数构建的离散列联表的似然比统计量来实现。对于给定集合 $Z$ 的条件检验，将所有出现在数据中的 $Z$ 的配置的各层特定贡献进行聚合。使用渐近卡方参考分布，其自由度等于在所有层上，每个测试对的非空水平数减一的乘积之和。当得到的总自由度为 $0$ 时，将该检验视为无信息量的，并为了算法的目的接受条件独立（即，取 $p$-值为 $1$）。使用显著性水平 $\\alpha$ 来决定独立性，即 $p$-值 $\\ge \\alpha$。\n\n对于每个数据集，假设学习到的 CPDAG 有一个有向边集和一个无向边集。每个数据集报告两个整数：定向边的数量（如果 $i \\to j$ 或 $j \\to i$ 中只有一个存在，则将对 $\\{i,j\\}$ 计为定向）和无向边的数量（如果 $i$ 和 $j$ 之间存在无向邻接，则将对计为一次）。没有邻接的对不计入。\n\n测试套件。在以下三个测试案例上实现您的算法。在所有案例中，使用显著性水平 $\\alpha = 0.001$ 和最大条件集大小 $k_{\\max} = 2$。\n\n- 案例 1（四个基因，带有一个碰撞子和一个下游目标）。变量为 $A$、 $B$、 $C$、 $D$，每个变量取值于 $\\{0,1\\}$。确定性地构造 $C$ 为异或 $C = A \\oplus B$ 以创建碰撞子 $A \\to C \\leftarrow B$。构造 $D$ 为 $C$ 的一个带噪声的子节点：对于 $(A,B)$ 的每种配置，复制 $5$ 个样本，使得 $C$ 由 $A \\oplus B$ 固定，并在其中 $4$ 个样本中设置 $D=C$，在 $1$ 个样本中设置 $D=1-C$。将此块完全相同地重复 $50$ 次，以产生 $1000$ 个样本。预期的结构是 $A \\to C \\leftarrow B$ 和 $C \\to D$。\n- 案例 2（三个独立基因）。变量为 $A$、 $B$、 $C$，取值于 $\\{0,1\\}$。通过等概率地枚举所有 $2^3=8$ 种配置来构建数据集，每种配置重复 $50$ 次，以获得 $400$ 个样本。预期的结构没有边。\n- 案例 3（三个基因组成的带噪声的链）。变量为 $A$、 $B$、 $C$，取值于 $\\{0,1\\}$。按如下方式构建一个包含 $200$ 个样本的数据集：对于 $A \\in \\{0,1\\}$ 的每个值，产生 $100$ 个样本。在这 $100$ 个样本中，有 $90$ 个设置 $B=A$，有 $10$ 个设置 $B=1-A$。对于每个这样的 $(A,B)$ 记录，通过在 $9$ 行中设置 $C=B$ 和在 $1$ 行中设置 $C=1-B$ 来扩展为 $10$ 行。预期的结构是 $A \\to B \\to C$。\n\n输入格式。没有外部输入；您的程序必须嵌入这些数据集。\n\n输出格式。您的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表，列表的每个元素是一个双元素整数列表 $[o,u]$，代表一个测试案例，其中 $o$ 等于定向边的数量， $u$ 等于无向边的数量。例如，输出必须类似于 $[[o_1,u_1],[o_2,u_2],[o_3,u_3]]$，不含空格。\n\n您的实现必须是自包含的，仅依赖于上述数学原理，并且确定性地运行。不涉及物理单位或角度。所有数值答案均为整数。测试套件涵盖了一个带下游定向传播的碰撞子案例、一个完全独立的边界案例，以及一个链式案例，其中条件独立性移除了一个长程边，但由于缺少 V-结构而留下了一个马尔可夫等价类，保持无向。", "solution": "我们从核心定义开始。对于离散随机变量，$X \\perp Y \\mid Z$ 意味着对于所有具有正概率的值，$p(x,y \\mid z) = p(x \\mid z)p(y \\mid z)$。在多项抽样模型下，用于检验独立性与一般依赖性的对数似然比统计量比较了联合分布可分解为边缘分布的约束模型与无约束模型。对于无条件检验，似然比统计量等于观测计数乘以观测计数与独立性假设下期望计数之比的对数的两倍，并在所有单元格上求和。渐近地，该统计量服从 $\\chi^2$ 分布，自由度为 $(r_X-1)(r_Y-1)$，其中 $r_X$ 和 $r_Y$ 是样本中出现的 $X$ 和 $Y$ 的类别数量。对于给定离散集合 $Z$ 的条件检验，相同的逻辑应用于 $Z$ 的每个层内；总统计量是各层统计量之和，总自由度是各层 $(r_{X|z}-1)(r_{Y|z}-1)$ 之和，使用的是每层中的非空水平数。如果自由度为 $0$，则该统计量不提供拒绝独立性的信息，并且为了算法的目的，接受原假设（将 $p$-值设为 $1$）。\n\nPeter-Clark (PC) 算法利用条件独立性约束来发现有向无环图 (DAG) 的马尔可夫等价类，表示为一个完全部分有向无环图 (CPDAG)。步骤如下：\n- 骨架发现：在变量上初始化完全无向图。对于 $s=0,1,\\dots,k_{\\max}$，对每个相邻对 $(X,Y)$，遍历 $X$ 的邻居中（不包括 $Y$）所有大小为 $s$ 的子集 $S$。如果在显著性水平 $\\alpha$ 下 $X \\perp Y \\mid S$，则移除边 $X-Y$ 并记录 $S$ 作为 $\\{X,Y\\}$ 的分离集。这操作化了全局马尔可夫性质：如果一条路径被 $S$ 阻断，则条件分布分解，使得 $X$ 和 $Y$ 变得独立；反之，如果没有限定大小的分离集，则保留邻接关系。\n- V-结构定向：一个三元组 $X-Z-Y$，其中 $X$ 和 $Y$ 在骨架中不相邻但都与 $Z$ 相邻，这表明要么是一个碰撞子 $X \\to Z \\leftarrow Y$，要么是一个非碰撞子，其中 $Z$ 属于 $X$ 和 $Y$ 之间的一个分离集。如果 $Z$ 未被记录在 $\\{X,Y\\}$ 的任何分离集中，那么为了满足观测到的边际分离 $X \\perp Y$ 而不以 $Z$ 为条件，必须是 $Z$ 是一个碰撞子；否则，非碰撞子模式将意味着 $X$ 和 $Y$ 之间存在依赖关系。因此，定向为 $X \\to Z \\leftarrow Y$。\n- 定向传播（Meek 规则）：如果存在 $A \\to B - C$ 且 $A$ 和 $C$ 不相邻，则将 $B - C$ 定向为 $B \\to C$。这避免了在 $B$ 处创建未被分离集支持的无屏蔽碰撞子，并保持了非循环性和等价类约束。重复应用此规则，直到无法推导出更多定向。\n\n独立性检验设计。我们直接从数据数组构建经验列联表。对于无条件检验，我们形成关于 $X$ 和 $Y$ 类别的计数 $n_{ij}$，计算行总和与列总和，计算独立性假设下的期望计数 $E_{ij} = (n_{i\\cdot} n_{\\cdot j})/n$，并计算对数似然比统计量 $G^2 = 2\\sum_{i,j:\\, n_{ij}>0} n_{ij} \\log(n_{ij}/E_{ij})$。对于条件检验，我们遍历 $Z$ 的每个观测到的配置 $z$，形成限制在 $Z=z$ 的行上的 $X \\times Y$ 列联表，并对各层的特定 $G^2_z$ 和自由度 $(r_{X|z}-1)(r_{Y|z}-1)$ 求和。$p$-值是 $\\Pr(\\chi^2_{\\text{df}} \\ge G^2)$。\n\n算法在测试套件上的应用：\n- 案例 1：数据精确编码了 $C = A \\oplus B$，$D$ 是 $C$ 的一个带噪声的子节点，概率为 $p(D=C \\mid C)=0.8$。在骨架发现阶段：$A$ 和 $B$ 边际独立，因此 $A-B$ 被移除，分离集 $S=\\varnothing$。对 $(A,C)$ 和 $(B,C)$ 保持相邻，因为存在直接依赖，不能被不包括端点的条件集所阻断。对 $(A,D)$ 和 $(B,D)$ 边际上是依赖的，因为存在通过 $C$ 的路径，但在给定 $C$ 的条件下变为条件独立（根据 DAG 的局部马尔可夫性质以及 $D$ 仅依赖于 $C$ 的构造），因此边 $A-D$ 和 $B-D$ 被移除，分离集 $S=\\{C\\}$。对 $(C,D)$ 保持相邻。V-结构定向将 $A \\to C \\leftarrow B$ 定向，因为 $C$ 不在 $\\{A,B\\}$ 的分离集中。然后 Meek 规则使用 $A \\to C - D$ 以及 $A$ 和 $D$ 不相邻（对 $B$ 也类似）来将 $C - D$ 定向为 $C \\to D$。该 CPDAG 有 $3$ 条定向边和 $0$ 条无向边。\n- 案例 2：$(A,B,C)$ 上的均匀乘积分布意味着所有变量对都无条件独立，因此骨架在 $s=0$ 时丢弃了所有边。不存在定向。该 CPDAG 有 $0$ 条定向边和 $0$ 条无向边。\n- 案例 3：数据与因子分解 $p(A)p(B \\mid A)p(C \\mid B)$ 一致，每一步都有高保真复制但严格存在正变异性。边际上，由于因果链或路径上诱导的依赖，所有对都是依赖的。然而，$A \\perp C \\mid B$ 从因子分解中精确成立，因此边 $A-C$ 被移除，分离集为 $\\{B\\}$。不存在无屏蔽碰撞子，因为 $A$ 和 $C$ 被 $B$ 分离，且 $B$ 在 $\\{A,C\\}$ 的分离集中。没有碰撞子，Meek 规则不会强制进行定向；因此，$(A,B)$ 和 $(B,C)$ 在 CPDAG 中保持无向，对应于链 $A \\to B \\to C$ 和 $A \\leftarrow B \\leftarrow C$ 的马尔可夫等价类。该 CPDAG 有 $0$ 条定向边和 $2$ 条无向边。\n\n因此，当严格按照规定实现该程序时，三个案例的输出是每个案例一个双元素整数列表。最终程序必须打印一行格式为 $[[o_1,u_1],[o_2,u_2],[o_3,u_3]]$ 的内容，以汇总测试套件的结果。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import chi2\nfrom itertools import combinations, product\n\ndef gtest_conditional_independence(data, x_col, y_col, z_cols):\n    \"\"\"\n    Compute conditional likelihood-ratio G^2 statistic for testing\n    X ⟂ Y | Z using empirical counts.\n\n    Parameters:\n        data: np.ndarray of shape (n_samples, n_vars), integer categories starting at 0\n        x_col, y_col: int, column indices for X and Y\n        z_cols: list of ints, column indices for conditioning set Z (possibly empty)\n\n    Returns:\n        stat: float, total G^2 statistic\n        df: int, total degrees of freedom\n        p_value: float, chi-squared survival function at stat with df\n    \"\"\"\n    x = data[:, x_col]\n    y = data[:, y_col]\n\n    # Determine the set of strata over Z\n    if len(z_cols) == 0:\n        strata = [()]\n        z_values = np.empty((data.shape[0], 0), dtype=int)\n    else:\n        z_values = data[:, z_cols]\n        # Unique rows for Z to iterate strata\n        # Convert rows to tuples\n        strata = list({tuple(row) for row in z_values})\n\n    total_stat = 0.0\n    total_df = 0\n\n    # Precompute possible levels for X and Y across the dataset\n    # Use observed levels to build tables\n    x_levels = np.unique(x)\n    y_levels = np.unique(y)\n    rx = len(x_levels)\n    ry = len(y_levels)\n    x_level_index = {val: idx for idx, val in enumerate(x_levels)}\n    y_level_index = {val: idx for idx, val in enumerate(y_levels)}\n\n    for z_val in strata:\n        if len(z_cols) == 0:\n            mask = np.ones(data.shape[0], dtype=bool)\n        else:\n            # Build mask for this stratum\n            z_arr = np.array(z_val, dtype=int)\n            mask = np.all(z_values == z_arr, axis=1)\n        # Extract stratum samples\n        x_s = x[mask]\n        y_s = y[mask]\n        n_s = x_s.shape[0]\n        if n_s == 0:\n            continue\n\n        # Build contingency table for this stratum\n        table = np.zeros((rx, ry), dtype=float)\n        # Fill counts\n        for xi, yi in zip(x_s, y_s):\n            ix = x_level_index[xi]\n            iy = y_level_index[yi]\n            table[ix, iy] += 1.0\n\n        # Compute row sums, col sums, total\n        row_sums = table.sum(axis=1)\n        col_sums = table.sum(axis=0)\n        total = table.sum()\n        if total == 0:\n            continue\n\n        # Degrees of freedom for this stratum = (r_x_pos - 1)*(r_y_pos - 1)\n        r_x_pos = int(np.sum(row_sums > 0))\n        r_y_pos = int(np.sum(col_sums > 0))\n        df_stratum = max(0, (r_x_pos - 1) * (r_y_pos - 1))\n        if df_stratum == 0:\n            # No contribution to statistic (degenerate stratum)\n            continue\n\n        # Expected counts under independence for this stratum\n        expected = np.outer(row_sums, col_sums) / total\n        # G-test contribution: 2 * sum_{i,j: n_ij > 0} n_ij * log(n_ij / E_ij)\n        # Avoid invalid log by masking\n        with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n            positive_mask = table > 0\n            ratio = np.zeros_like(table)\n            ratio[positive_mask] = table[positive_mask] / expected[positive_mask]\n            # Terms with expected zero should not happen if df_stratum > 0\n            contrib = np.zeros_like(table)\n            # Only valid where expected > 0 and observed > 0\n            valid = positive_mask  (expected > 0)\n            contrib[valid] = table[valid] * np.log(ratio[valid])\n            g2 = 2.0 * np.sum(contrib)\n\n        total_stat += g2\n        total_df += df_stratum\n\n    if total_df == 0:\n        # Treat as independent (uninformative) for algorithm\n        p_value = 1.0\n    else:\n        p_value = chi2.sf(total_stat, df=total_df)\n    return total_stat, total_df, p_value\n\ndef pc_algorithm(data, alpha=0.001, max_k=2):\n    \"\"\"\n    Constraint-based PC algorithm to learn a CPDAG from discrete data.\n\n    Parameters:\n        data: np.ndarray of shape (n_samples, n_vars)\n        alpha: significance level for independence tests\n        max_k: maximum size of conditioning set\n\n    Returns:\n        undirected: np.ndarray bool matrix of undirected adjacencies (symmetric)\n        directed: np.ndarray bool matrix for directed edges (directed[i,j]=True if i->j)\n    \"\"\"\n    n_vars = data.shape[1]\n    # Initialize complete undirected graph\n    undirected = np.zeros((n_vars, n_vars), dtype=bool)\n    for i in range(n_vars):\n        for j in range(i + 1, n_vars):\n            undirected[i, j] = True\n            undirected[j, i] = True\n\n    # Separation sets: map pair (i,j) with i", "id": "3314528"}, {"introduction": "通过整合多种数据类型，基因调控网络推断的准确性可以得到显著提升。实现这一目标的一个强大方法是采用贝叶斯框架，其中一种数据模态（如染色质可及性）的先验知识可以为另一种模态（如基因表达）的分析提供信息。\n\n这项实践 ([@problem_id:3314530]) 将指导你构建一个贝叶斯模型，该模型将源自 scATAC-seq 的模体（motif）得分作为先验，并与基于 scRNA-seq 的似然函数相结合。通过计算调控边的后验概率，你将学习一种有原则且被广泛应用的多组学数据整合策略，从而更接近计算系统生物学研究的前沿。", "problem": "给定单个转录因子 (TF) $j$ 和一组由 $i$ 索引的候选靶基因，考虑从单细胞分析中推断有向调控边 $j \\to i$ 的问题。我们将结合使用测序的单细胞转座酶可及性染色质分析 (scATAC-seq) 的基序可及性得分与单细胞信使核糖核酸 (mRNA) 计数，为每条边形成贝叶斯后验概率。假设以下基本原理和事实：(1) 用于假设检验的贝叶斯定理，(2) 负二项 (NB) 分布是经过充分检验的单细胞计数数据模型，以及 (3) 源自染色质可及性的基序得分是关于 TF-基因调控潜力的信息性先验。\n\n定义和建模假设：\n- 令 $m_{j \\to i} \\in \\mathbb{R}$ 表示在基因 $i$ 的启动子或增强子区域，TF $j$ 的 scATAC-seq 衍生基序可及性得分。\n- 定义边 $j \\to i$ 存在的先验概率为\n$$\n\\pi_{j \\to i} = \\sigma\\!\\left(\\alpha + \\beta \\, m_{j \\to i}\\right) = \\frac{1}{1 + \\exp\\!\\left( -(\\alpha + \\beta \\, m_{j \\to i}) \\right)},\n$$\n其中 $\\sigma(\\cdot)$ 是逻辑斯蒂函数，$\\alpha, \\beta \\in \\mathbb{R}$ 是固定的超参数。\n- 令 $y_{i c} \\in \\{0,1,2,\\dots\\}$ 表示在细胞 $c$ 中基因 $i$ 的观测单细胞 mRNA 计数，并令 $s_c \\in \\mathbb{R}_{0}$ 表示已知的每个细胞的暴露度（例如，文库大小归一化因子）。\n- 令 $x_{j c} \\in \\mathbb{R}$ 表示每个细胞 $c$ 的 TF $j$ 的活性代理。为实施一个双组模型，定义一个阈值 $\\tau \\in \\mathbb{R}$ 和一个组标签 $g(c) \\in \\{0,1\\}$，使得当 $x_{j c} \\ge \\tau$ 时 $g(c) = 1$，否则 $g(c) = 0$。\n- 在无边假设 $H_0$ 下，假设一个负二项模型，其具有单一均值参数 $\\theta \\in \\mathbb{R}_{\\ge 0}$ 和共享的离散度 $k \\in \\mathbb{R}_{0}$：\n$$\ny_{i c} \\sim \\mathrm{NB}\\left(\\mu_c = s_c \\theta, \\; k\\right).\n$$\n- 在有边假设 $H_1$ 下，假设一个负二项模型，其具有特定于组的均值参数 $\\theta_0, \\theta_1 \\in \\mathbb{R}_{\\ge 0}$ 和相同的离散度 $k$：\n$$\ny_{i c} \\sim \\mathrm{NB}\\left(\\mu_c = s_c \\, \\theta_{g(c)}, \\; k\\right).\n$$\n- 使用均值为 $\\mu$ 和离散度（大小）为 $k$ 的负二项概率质量函数的以下参数化形式：\n$$\n\\Pr(Y=y \\mid \\mu, k) = \\binom{y + k - 1}{y} \\left(\\frac{k}{k + \\mu}\\right)^{k} \\left(\\frac{\\mu}{k + \\mu}\\right)^y,\n$$\n对于 $y \\in \\{0,1,2,\\dots\\}$，$\\mu \\in \\mathbb{R}_{\\ge 0}$ 和 $k \\in \\mathbb{R}_{0}$。该参数化意味着 $\\mathbb{E}[Y]=\\mu$ 和 $\\mathrm{Var}(Y) = \\mu + \\mu^2/k$。\n- 为了进行模型比较，通过代入最大似然估计 (MLEs) 来近似每个假设下的边际似然。对于一组观测值 $\\{y_{i c}\\}$ 和细胞特异性均值 $\\mu_c$，其对数似然（在不考虑与 $\\mu$ 无关的加性常数的情况下）为\n$$\n\\ell(\\{\\y_{i c}\\} \\mid \\{\\mu_c\\}, k) = \\sum_{c} \\left[ y_{i c} \\log(\\mu_c) - (y_{i c} + k) \\log(k + \\mu_c) \\right].\n$$\n- $H_0$ 和 $H_1$ 下的 MLEs 是通过最大化各自的对数似然来定义的；在 $H_0$ 下，$\\mu_c = s_c \\theta$；在 $H_1$ 下，$\\mu_c = s_c \\theta_{g(c)}$。对于组 $G \\subseteq \\{c\\}$，MLEs 的导数条件是\n$$\n\\frac{\\partial \\ell}{\\partial \\theta} = \\sum_{c \\in G} \\left[ \\frac{y_{i c}}{\\theta} - \\frac{(y_{i c} + k) s_c}{k + s_c \\theta} \\right] = 0,\n$$\n类似地，二阶导数可用于确保凹性并用于 Newton 更新。在 $\\sum_{c \\in G} y_{i c} = 0$ 的边界情况下，MLE 出现在边界 $\\theta = 0$ 处。\n\n推断目标和计算：\n- 定义贝叶斯因子为\n$$\n\\mathrm{BF}_{j \\to i} = \\exp\\!\\left( \\ell_{H_1}^{\\star} - \\ell_{H_0}^{\\star} \\right),\n$$\n其中 $\\ell_{H_0}^{\\star}$ 和 $\\ell_{H_1}^{\\star}$ 分别是使用上述 MLEs 在 $H_0$ 和 $H_1$ 下最大化的对数似然。\n- 使用贝叶斯定理，计算边 $j \\to i$ 的后验概率：\n$$\n\\Pr(E_{j \\to i}=1 \\mid \\text{data}) = \\frac{\\pi_{j \\to i} \\, \\mathrm{BF}_{j \\to i}}{\\pi_{j \\to i} \\, \\mathrm{BF}_{j \\to i} + (1 - \\pi_{j \\to i})}.\n$$\n\n测试套件和要求的输出：\n实现一个程序，针对以下固定输入，将四个候选边的后验概率输出为单行，格式为方括号内由逗号分隔的列表。不涉及物理单位；所有输出必须是十进制浮点数。\n\n所有边共享的全局参数：\n- 细胞数 $C = 8$。\n- 每个细胞的暴露度 $s = [1.00, 0.90, 1.10, 0.95, 1.05, 1.20, 0.80, 1.00]$。\n- TF 活性 $x_{j} = [-0.50, -0.20, 0.10, 0.30, 0.70, 1.20, -1.00, 0.50]$。\n- 阈值 $\\tau = 0.30$，使得当 $x_{j c} \\ge 0.30$ 时 $g(c) = 1$，否则 $g(c) = 0$。\n- 离散度 $k = 2.0$。\n- 先验逻辑斯蒂参数 $\\alpha = -2.0, \\beta = 1.0$。\n\n每个边的测试用例：\n- 边 1：基序得分 $m_{j \\to 1} = 2.5$；计数 $y_{1} = [2, 1, 1, 4, 5, 6, 0, 3]$。\n- 边 2：基序得分 $m_{j \\to 2} = 0.1$；计数 $y_{2} = [3, 2, 2, 3, 3, 2, 1, 3]$。\n- 边 3：基序得分 $m_{j \\to 3} = 6.0$；计数 $y_{3} = [0, 0, 0, 0, 0, 0, 0, 0]$。\n- 边 4：基序得分 $m_{j \\to 4} = 0.0$；计数 $y_{4} = [5, 4, 3, 1, 1, 0, 6, 2]$。\n\n覆盖范围基本原理：\n- 边 1 代表高活性组中计数较高的情况，在有足够先验支持的情况下，预期会支持 $H_1$。\n- 边 2 代表各组间计数相似的情况，预期会产生接近 1 的贝叶斯因子，并让先验起主导作用。\n- 边 3 代表全零情况，这是一个边界条件，此时两个假设具有相同的似然，后验概率等于先验概率。\n- 边 4 代表低活性组中计数较高的情况，这对有向分组提出了挑战，测试了当 $g(c)=1$ 的组未显示增加时的稳健性。\n\n最终输出格式：\n您的程序应生成一行输出，其中包含一个由方括号括起来的逗号分隔列表（例如，“[result1,result2,result3,result4]”），其中每个结果是按 $i=1,2,3,4$ 顺序排列的边的 $\\Pr(E_{j \\to i}=1 \\mid \\text{data})$。", "solution": "目标是计算四个候选基因调控边（表示为 $j \\to i$）的贝叶斯后验概率。该概率 $\\Pr(E_{j \\to i}=1 \\mid \\text{data})$ 综合了源自染色质可及性的先验知识与来自单细胞基因表达计数的证据。该框架基于贝叶斯定理，并使用负二项模型处理计数数据。\n\n对每条边 $j \\to i$ 的总体过程涉及以下几个计算步骤：\n\n1.  **细胞分组**：首先根据转录因子 (TF) 活性代理 $x_{jc}$ 将细胞划分为两组。如果一个细胞 $c$ 的 TF 活性达到或超过阈值 $\\tau$，即 $x_{jc} \\ge \\tau$ 时，该细胞被分配到组 1（$g(c)=1$），否则分配到组 0。\n    给定 TF 活性 $x_j = [-0.50, -0.20, 0.10, 0.30, 0.70, 1.20, -1.00, 0.50]$ 和阈值 $\\tau = 0.30$，细胞索引 $\\{0, 1, \\dots, 7\\}$ 的划分如下：\n    -   组 0（低 TF 活性，$g(c)=0$）：索引为 $\\{0, 1, 2, 6\\}$ 的细胞。\n    -   组 1（高 TF 活性，$g(c)=1$）：索引为 $\\{3, 4, 5, 7\\}$ 的细胞。\n\n2.  **先验概率计算**：一条边 $j \\to i$ 的先验概率 $\\pi_{j \\to i}$ 由 scATAC-seq 基序可及性得分 $m_{j \\to i}$ 决定。这通过逻辑斯蒂函数进行形式化：\n    $$\n    \\pi_{j \\to i} = \\sigma(\\alpha + \\beta \\, m_{j \\to i}) = \\frac{1}{1 + \\exp(-(\\alpha + \\beta \\, m_{j \\to i}))}\n    $$\n    使用给定的超参数 $\\alpha = -2.0$ 和 $\\beta = 1.0$，根据四个测试用例各自的基序得分计算此先验概率。\n\n3.  **最大似然估计 (MLE)**：为了评估来自基因表达数据的证据，我们使用基于似然的方法比较两个假设。这需要找到负二项 (NB) 分布均值参数的最大似然估计 (MLEs)。\n    -   在零假设 $H_0$（无边）下，所有细胞共享一个单一的均值参数 $\\theta$。细胞特异性均值计数为 $\\mu_c = s_c \\theta$。\n    -   在备择假设 $H_1$（有边）下，细胞具有特定于组的均值参数 $\\theta_0$ 和 $\\theta_1$。细胞特异性均值计数为 $\\mu_c = s_c \\theta_{g(c)}$。\n    \n    参数 $\\theta$ 的 MLE（对于给定的细胞组）是通过求解方程 $\\frac{\\partial \\ell}{\\partial \\theta} = 0$ 得到的，其中 $\\ell$ 是对数似然。需要求解的方程是：\n    $$\n    \\sum_{c \\in G} \\left[ \\frac{y_{ic}}{\\theta} - \\frac{(y_{ic} + k) s_c}{k + s_c \\theta} \\right] = 0\n    $$\n    这个非线性方程需要数值求解。一个合适的方法是 Newton-Raphson 法，它使用 $\\theta_{\\text{new}} = \\theta_{\\text{old}} - f(\\theta_{\\text{old}})/f'(\\theta_{\\text{old}})$ 形式的迭代更新，其中 $f(\\theta) = \\frac{\\partial \\ell}{\\partial \\theta}$。$\\theta$ 的一个合适的初始猜测是矩估计法，$\\theta_0 = (\\sum y_c) / (\\sum s_c)$。\n    \n    当一个组中的所有计数都为零（$\\sum_{c \\in G} y_{ic} = 0$）时，会出现一种特殊情况。在这种情况下，似然在边界处最大化，得到 MLE 为 $\\theta=0$。\n\n4.  **对数似然计算**：一旦找到 MLEs（$H_0$ 下的 $\\theta^{\\star}$；$H_1$ 下的 $\\theta_0^{\\star}$ 和 $\\theta_1^{\\star}$），就计算最大化的对数似然。对数似然函数（忽略与 $\\mu$ 无关的常数项）是：\n    $$\n    \\ell^{\\star}(\\{\\y_{ic}\\} \\mid \\{\\mu_c^{\\star}\\}, k) = \\sum_{c} \\left[ y_{ic} \\log(\\mu_c^{\\star}) - (y_{ic} + k) \\log(k + \\mu_c^{\\star}) \\right]\n    $$\n    对于 $H_0$，这得到 $\\ell_{H_0}^{\\star}$。对于 $H_1$，分别计算每个组的对数似然然后求和：$\\ell_{H_1}^{\\star} = \\ell_{G_0}^{\\star} + \\ell_{G_1}^{\\star}$。在 $\\theta^{\\star}=0$（且所有对应的 $y_{ic}=0$）的特殊情况下，大小为 $|G|$ 的组的对数似然简化为 $-|G| k \\log(k)$。\n\n5.  **贝叶斯因子计算**：贝叶斯因子 $\\mathrm{BF}_{j \\to i}$ 量化了数据支持 $H_1$ 相对于 $H_0$ 的证据。它是边际似然的比值，此处通过在 MLEs 处评估的似然来近似：\n    $$\n    \\mathrm{BF}_{j \\to i} = \\frac{\\Pr(\\text{data} \\mid H_1)}{\\Pr(\\text{data} \\mid H_0)} \\approx \\exp(\\ell_{H_1}^{\\star} - \\ell_{H_0}^{\\star})\n    $$\n\n6.  **后验概率计算**：最后，使用贝叶斯定理将先验概率和贝叶斯因子结合起来，得到边的后验概率：\n    $$\n    \\Pr(E_{j \\to i}=1 \\mid \\text{data}) = \\frac{\\Pr(E_{j \\to i}=1) \\Pr(\\text{data} \\mid E_{j \\to i}=1)}{\\Pr(\\text{data})} = \\frac{\\pi_{j \\to i} \\, \\mathrm{BF}_{j \\to i}}{\\pi_{j \\to i} \\, \\mathrm{BF}_{j \\to i} + (1 - \\pi_{j \\to i})}\n    $$\n    对测试套件中指定的四条边中的每一条都执行此计算。实现将系统地应用这些步骤，以生成最终的后验概率向量。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the Bayesian posterior probability for gene regulatory edges\n    based on scATAC-seq priors and scRNA-seq likelihoods.\n    \"\"\"\n\n    # Global parameters shared across all edges\n    s_all = np.array([1.00, 0.90, 1.10, 0.95, 1.05, 1.20, 0.80, 1.00])\n    x_j = np.array([-0.50, -0.20, 0.10, 0.30, 0.70, 1.20, -1.00, 0.50])\n    tau = 0.30\n    k = 2.0\n    alpha = -2.0\n    beta = 1.0\n\n    # Per-edge test cases\n    test_cases = [\n        # (motif_score, counts_array)\n        (2.5, np.array([2, 1, 1, 4, 5, 6, 0, 3])),\n        (0.1, np.array([3, 2, 2, 3, 3, 2, 1, 3])),\n        (6.0, np.array([0, 0, 0, 0, 0, 0, 0, 0])),\n        (0.0, np.array([5, 4, 3, 1, 1, 0, 6, 2])),\n    ]\n\n    # Pre-compute cell groupings\n    g = (x_j >= tau).astype(int)\n    g0_indices = np.where(g == 0)[0]\n    g1_indices = np.where(g == 1)[0]\n    s_g0 = s_all[g0_indices]\n    s_g1 = s_all[g1_indices]\n\n    def mle_theta(y, s, k, tol=1e-9, max_iter=100):\n        \"\"\"\n        Finds the Maximum Likelihood Estimate for the NB parameter theta\n        using Newton-Raphson iteration.\n        \"\"\"\n        if np.sum(y) == 0.0:\n            return 0.0\n        \n        # Initial guess using method of moments\n        theta = np.sum(y) / np.sum(s)\n        if theta == 0:  # Fallback for unusual inputs\n            theta = 1.0\n\n        for _ in range(max_iter):\n            # Use a safe value for theta to avoid division by zero or log(0)\n            theta_safe = max(theta, 1e-12)\n            \n            mu = s * theta_safe\n            \n            # First derivative of log-likelihood (the function to find root of)\n            f_val = np.sum(y / theta_safe - (y + k) * s / (k + mu))\n            \n            if abs(f_val)  tol:\n                break\n            \n            # Second derivative of log-likelihood\n            df_val = np.sum(-y / theta_safe**2 + (y + k) * s**2 / (k + mu)**2)\n            \n            # Avoid division by zero if derivative is flat\n            if abs(df_val)  1e-12:\n                break\n\n            # Newton-Raphson step\n            step = f_val / df_val\n            theta = theta - step\n        \n        # Ensure theta is positive\n        return max(theta, 1e-12)\n\n    def log_likelihood(y, s, theta, k):\n        \"\"\"\n        Calculates the log-likelihood for the NB model given parameters.\n        \"\"\"\n        if theta  1e-12:\n            if np.sum(y) > 0:\n                return -np.inf\n            # If all y_i are 0 and theta is 0\n            return -len(y) * k * np.log(k)\n        \n        mu = s * theta\n        \n        # The term y*log(mu) can be written as y*(log(s)+log(theta)).\n        # If y_i is 0, the term is 0. If y_i > 0, mu > 0, so log is safe.\n        # We need to handle the case where y_i > 0 but mu_i is 0 (which\n        # shouldn't happen if theta > 0).\n        # A vectorized approach that avoids 0*log(0) = nan:\n        term1 = np.where(y > 0, y * np.log(mu), 0)\n        term2 = (y + k) * np.log(k + mu)\n        \n        return np.sum(term1 - term2)\n\n    results = []\n    for m, y_all in test_cases:\n        # 1. Calculate prior probability\n        pi = 1.0 / (1.0 + np.exp(-(alpha + beta * m)))\n        \n        # 2. Calculate maximized log-likelihood under H0 (one group)\n        theta_h0 = mle_theta(y_all, s_all, k)\n        ll_h0 = log_likelihood(y_all, s_all, theta_h0, k)\n        \n        # 3. Calculate maximized log-likelihood under H1 (two groups)\n        y_g0 = y_all[g0_indices]\n        y_g1 = y_all[g1_indices]\n        \n        theta_g0 = mle_theta(y_g0, s_g0, k)\n        ll_g0 = log_likelihood(y_g0, s_g0, theta_g0, k)\n        \n        theta_g1 = mle_theta(y_g1, s_g1, k)\n        ll_g1 = log_likelihood(y_g1, s_g1, theta_g1, k)\n        \n        ll_h1 = ll_g0 + ll_g1\n        \n        # 4. Calculate Bayes Factor\n        # Check for -inf likelihoods, which can occur if MLE is 0\n        # for a group with non-zero counts (a scenario our code avoids, but\n        # is good practice to handle robustly).\n        if ll_h1 == -np.inf:\n            bf = 0.0\n        else:\n            bf = np.exp(ll_h1 - ll_h0)\n        \n        # 5. Calculate posterior probability\n        if np.isinf(bf):\n            posterior = 1.0\n        else:\n            denominator = pi * bf + (1.0 - pi)\n            if denominator == 0:\n                posterior = 0.0\n            else:\n                posterior = (pi * bf) / denominator\n        \n        results.append(posterior)\n\n    # Format the final output\n    print(f\"[{','.join(f'{r:.10f}' for r in results)}]\")\n\nsolve()\n```", "id": "3314530"}]}