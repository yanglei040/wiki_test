{"hands_on_practices": [{"introduction": "本练习是一项基础性训练，要求你从第一性原理出发，推导正态分布的均值和方差的最大似然估计。通过完成此推导，你将巩固对似然函数、得分方程和费雪信息矩阵等核心概念的理解，这些都是统计推断的基石。[@problem_id:3322901]", "problem": "在一次定量转录组学分析中，假设对单个基因在 $n$ 个技术重复中的信使核糖核酸（mRNA）丰度的重复测量值被建模为独立同分布（i.i.d.），即 $y_{1},\\dots,y_{n} \\sim \\mathcal{N}(\\mu,\\sigma^{2})$，其中 $\\mu$ 是未知的平均丰度，$\\sigma^{2}$ 是未知的测量方差。运用最大似然估计和最小二乘推断的原理，从第一性原理出发，由正态分布的概率密度函数和独立性假设开始构建似然函数。然后：\n- 通过对对数似然函数关于每个参数求导，推导出 $\\mu$ 和 $\\sigma^{2}$ 的得分方程。\n- 求解得分方程，以数据形式表示 $\\mu$ 和 $\\sigma^{2}$ 的最大似然估计（MLEs）。\n- 计算观测到的费雪信息矩阵，定义为对数似然函数关于 $(\\mu,\\sigma^{2})$ 的负海森矩阵，并在最大似然估计值处求值。\n\n为确保得到一个封闭形式的单一最终答案，请将观测到的费雪信息矩阵在最大似然估计值处的行列式作为最终结果报告。最终答案应以符号表达式表示；无需进行数值舍入。不需要物理单位。使用记号 $\\hat{\\mu}$ 和 $\\hat{\\sigma}^{2}$ 分别表示 $\\mu$ 和 $\\sigma^{2}$ 的最大似然估计，用 $n$ 表示重复次数。", "solution": "用户提供了一个问题陈述，在尝试解决之前需要对其进行验证。\n\n### 步骤 1：提取已知条件\n- 数据是 $n$ 个独立同分布（i.i.d.）的测量值，$y_1, \\dots, y_n$。\n- 数据来自正态分布：$y_i \\sim \\mathcal{N}(\\mu, \\sigma^2)$。\n- 分布的参数，即均值 $\\mu$ 和方差 $\\sigma^2$，是未知的。\n- 任务是：\n    1. 从对数似然推导 $\\mu$ 和 $\\sigma^2$ 的得分方程。\n    2. 求解这些方程以找到最大似然估计（MLEs），记为 $\\hat{\\mu}$ 和 $\\hat{\\sigma}^2$。\n    3. 计算观测到的费雪信息矩阵，其定义为在最大似然估计值处求值的对数似然的负海森矩阵。\n- 最终需要报告的结果是这个观测到的费雪信息矩阵的行列式，用 $n$ 和 $\\hat{\\sigma}^2$ 的符号形式表示。\n\n### 步骤 2：使用提取的已知条件进行验证\n- **科学上成立：**该问题牢固地植根于统计推断的基本原理，特别是最大似然估计。正态分布是科学领域统计建模的基石，而似然、得分函数和费雪信息等概念是估计理论与实践的核心。定量转录组学的应用背景是恰当的。\n- **适定性：**该问题提供了一个明确的目标和所有必要的信息来实现它。步骤逻辑清晰，导向一个唯一的、明确定义的数学表达式。\n- **客观性：**该问题使用精确、无歧义的数学和统计术语进行陈述。它不包含任何主观或基于意见的内容。\n\n### 步骤 3：结论与行动\n该问题有效。它在科学上是合理的，问题是适定的、客观的，并且需要一个标准的、尽管详细的从第一性原理出发的推导。我现在将继续进行解答。\n\n***\n\n解答过程首先构建对数似然函数，然后找到其临界点以确定最大似然估计，最后计算最大值点的曲率以找到观测到的费雪信息。\n\n来自正态分布 $\\mathcal{N}(\\mu, \\sigma^2)$ 的单个观测值 $y_i$ 的概率密度函数（PDF）是：\n$$ f(y_i | \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - \\mu)^2}{2\\sigma^2}\\right) $$\n鉴于观测值 $y_1, \\dots, y_n$ 是独立同分布的，似然函数 $L(\\mu, \\sigma^2_ | \\mathbf{y})$ 是各个概率密度函数的乘积：\n$$ L(\\mu, \\sigma^2) = \\prod_{i=1}^{n} f(y_i | \\mu, \\sigma^2) = \\prod_{i=1}^{n} \\frac{1}{(2\\pi\\sigma^2)^{1/2}} \\exp\\left(-\\frac{(y_i - \\mu)^2}{2\\sigma^2}\\right) $$\n$$ L(\\mu, \\sigma^2) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - \\mu)^2\\right) $$\n处理对数似然函数 $\\ell(\\mu, \\sigma^2) = \\ln(L(\\mu, \\sigma^2))$ 更为方便：\n$$ \\ell(\\mu, \\sigma^2) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - \\mu)^2 $$\n\n**1. 推导得分方程**\n得分方程是通过将对数似然函数关于参数 $\\mu$ 和 $\\sigma^2$ 的一阶偏导数设为零得到的。\n\n关于 $\\mu$ 的偏导数是：\n$$ \\frac{\\partial \\ell}{\\partial \\mu} = \\frac{\\partial}{\\partial \\mu} \\left( -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - \\mu)^2 \\right) $$\n$$ \\frac{\\partial \\ell}{\\partial \\mu} = -\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} 2(y_i - \\mu)(-1) = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (y_i - \\mu) $$\n第一个得分方程是 $\\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (y_i - \\mu) = 0$。\n\n关于 $\\sigma^2$ 的偏导数是：\n$$ \\frac{\\partial \\ell}{\\partial (\\sigma^2)} = \\frac{\\partial}{\\partial (\\sigma^2)} \\left( -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - \\mu)^2 \\right) $$\n$$ \\frac{\\partial \\ell}{\\partial (\\sigma^2)} = -\\frac{n}{2\\sigma^2} + \\frac{1}{2(\\sigma^2)^2} \\sum_{i=1}^{n} (y_i - \\mu)^2 $$\n第二个得分方程是 $-\\frac{n}{2\\sigma^2} + \\frac{1}{2(\\sigma^2)^2} \\sum_{i=1}^{n} (y_i - \\mu)^2 = 0$。\n\n**2. 求解最大似然估计（MLEs）**\n为了找到最大似然估计 $\\hat{\\mu}$ 和 $\\hat{\\sigma}^2$，我们求解得分方程。从第一个方程得出：\n$$ \\frac{1}{\\hat{\\sigma}^2} \\sum_{i=1}^{n} (y_i - \\hat{\\mu}) = 0 \\implies \\sum_{i=1}^{n} y_i - n\\hat{\\mu} = 0 $$\n$$ \\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^{n} y_i $$\n均值的最大似然估计是样本均值。\n\n将 $\\hat{\\mu}$ 代入第二个得分方程：\n$$ -\\frac{n}{2\\hat{\\sigma}^2} + \\frac{1}{2(\\hat{\\sigma}^2)^2} \\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2 = 0 $$\n两边乘以 $2(\\hat{\\sigma}^2)^2$ 得：\n$$ -n\\hat{\\sigma}^2 + \\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2 = 0 $$\n$$ \\hat{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2 $$\n方差的最大似然估计是分母为 $n$ 的样本方差。\n\n**3. 计算观测到的费雪信息矩阵**\n观测到的费雪信息矩阵 $I(\\mu, \\sigma^2)$ 是对数似然函数的海森矩阵 $\\mathbf{H}$ 的负值。海森矩阵的元素是 $\\ell$ 的二阶偏导数。\n$$ \\mathbf{H} = \\begin{pmatrix} \\frac{\\partial^2 \\ell}{\\partial \\mu^2}  \\frac{\\partial^2 \\ell}{\\partial \\mu \\partial (\\sigma^2)} \\\\ \\frac{\\partial^2 \\ell}{\\partial (\\sigma^2) \\partial \\mu}  \\frac{\\partial^2 \\ell}{\\partial (\\sigma^2)^2} \\end{pmatrix} $$\n我们计算每个元素：\n$$ \\frac{\\partial^2 \\ell}{\\partial \\mu^2} = \\frac{\\partial}{\\partial \\mu} \\left( \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (y_i - \\mu) \\right) = -\\frac{n}{\\sigma^2} $$\n$$ \\frac{\\partial^2 \\ell}{\\partial (\\sigma^2) \\partial \\mu} = \\frac{\\partial}{\\partial (\\sigma^2)} \\left( \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (y_i - \\mu) \\right) = -\\frac{1}{(\\sigma^2)^2} \\sum_{i=1}^{n} (y_i - \\mu) $$\n$$ \\frac{\\partial^2 \\ell}{\\partial (\\sigma^2)^2} = \\frac{\\partial}{\\partial (\\sigma^2)} \\left( -\\frac{n}{2\\sigma^2} + \\frac{1}{2(\\sigma^2)^2} \\sum_{i=1}^{n} (y_i - \\mu)^2 \\right) = \\frac{n}{2(\\sigma^2)^2} - \\frac{1}{(\\sigma^2)^3} \\sum_{i=1}^{n} (y_i - \\mu)^2 $$\n接下来，我们在最大似然估计值 $(\\hat{\\mu}, \\hat{\\sigma}^2)$ 处计算海森矩阵。\n$$ \\left. \\frac{\\partial^2 \\ell}{\\partial \\mu^2} \\right|_{(\\hat{\\mu}, \\hat{\\sigma}^2)} = -\\frac{n}{\\hat{\\sigma}^2} $$\n非对角项变为零，因为 $\\sum_{i=1}^{n} (y_i - \\hat{\\mu}) = 0$：\n$$ \\left. \\frac{\\partial^2 \\ell}{\\partial (\\sigma^2) \\partial \\mu} \\right|_{(\\hat{\\mu}, \\hat{\\sigma}^2)} = -\\frac{1}{(\\hat{\\sigma}^2)^2} \\sum_{i=1}^{n} (y_i - \\hat{\\mu}) = 0 $$\n对于最后一个元素，我们代入 $\\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2 = n\\hat{\\sigma}^2$：\n$$ \\left. \\frac{\\partial^2 \\ell}{\\partial (\\sigma^2)^2} \\right|_{(\\hat{\\mu}, \\hat{\\sigma}^2)} = \\frac{n}{2(\\hat{\\sigma}^2)^2} - \\frac{1}{(\\hat{\\sigma}^2)^3} \\sum_{i=1}^{n} (y_i - \\hat{\\mu})^2 = \\frac{n}{2(\\hat{\\sigma}^2)^2} - \\frac{n\\hat{\\sigma}^2}{(\\hat{\\sigma}^2)^3} = \\frac{n}{2(\\hat{\\sigma}^2)^2} - \\frac{n}{(\\hat{\\sigma}^2)^2} = -\\frac{n}{2(\\hat{\\sigma}^2)^2} $$\n在最大似然估计值处计算的海森矩阵是：\n$$ \\mathbf{H}(\\hat{\\mu}, \\hat{\\sigma}^2) = \\begin{pmatrix} -\\frac{n}{\\hat{\\sigma}^2}  0 \\\\ 0  -\\frac{n}{2(\\hat{\\sigma}^2)^2} \\end{pmatrix} $$\n观测到的费雪信息矩阵是 $I(\\hat{\\mu}, \\hat{\\sigma}^2) = -\\mathbf{H}(\\hat{\\mu}, \\hat{\\sigma}^2)$：\n$$ I(\\hat{\\mu}, \\hat{\\sigma}^2) = \\begin{pmatrix} \\frac{n}{\\hat{\\sigma}^2}  0 \\\\ 0  \\frac{n}{2(\\hat{\\sigma}^2)^2} \\end{pmatrix} $$\n最后，我们计算观测到的费雪信息矩阵的行列式。由于该矩阵是对角矩阵，其行列式是对角元素的乘积：\n$$ \\det(I(\\hat{\\mu}, \\hat{\\sigma}^2)) = \\left(\\frac{n}{\\hat{\\sigma}^2}\\right) \\left(\\frac{n}{2(\\hat{\\sigma}^2)^2}\\right) = \\frac{n^2}{2(\\hat{\\sigma}^2)^3} $$", "answer": "$$\\boxed{\\frac{n^2}{2(\\hat{\\sigma}^2)^{3}}}$$", "id": "3322901"}, {"introduction": "真实的生物系统通常由非线性模型描述，其参数无法直接求得解析解。本练习将最小二乘估计原理应用于经典的米氏动力学模型，演示了如何构建高斯-牛顿算法进行迭代式参数拟合。你将推导雅可比矩阵和正规方程，从而获得解决非线性回归问题的实践经验。[@problem_id:3322866]", "problem": "在一个计算系统生物学研究中，您正在对遵循米氏动力学（Michaelis–Menten kinetics）的酶催化反应的初始反应速率测量值进行建模。对于底物浓度 $\\{x_i\\}_{i=1}^{n}$ 和测得的初始速率 $\\{y_i\\}_{i=1}^{n}$，假设观测模型为\n$$\ny_i \\;=\\; \\frac{V_{\\max}\\, x_i}{K_m + x_i} \\;+\\; \\epsilon_i,\n$$\n其中 $\\epsilon_i$ 是均值为 $0$、方差为 $\\sigma^2$ 的独立同分布 (i.i.d.) 高斯噪声项，且 $V_{\\max} > 0, K_m > 0$。在高斯噪声假设下，最大似然估计量与最小化残差平方和的非线性最小二乘估计量一致。令第 $i$ 个数据的残差为\n$$\nr_i(\\theta) \\;=\\; y_i \\;-\\; f(x_i;\\theta), \\quad \\text{with} \\quad \\theta \\equiv (V_{\\max}, K_m), \\quad f(x;\\theta) \\equiv \\frac{V_{\\max} x}{K_m + x}.\n$$\n考虑使用高斯-牛顿法求解非线性最小二乘问题。从高斯噪声的最大似然第一性原理以及雅可比矩阵作为模型关于参数的一阶导数矩阵的定义出发，推导：\n- 单个观测值 $x_i$ 的雅可比行向量，即 $f(x_i;\\theta)$ 关于 $(V_{\\max}, K_m)$ 的偏导数的 $1 \\times 2$ 向量；以及\n- 参数增量 $\\Delta \\theta$ 的显式高斯-牛顿法向方程，用关于 $i$ 的求和形式表示，通过对称 $2 \\times 2$ 矩阵 $\\sum_{i=1}^{n} J_i(\\theta)^{\\top} J_i(\\theta)$ 和 $2 \\times 1$ 向量 $\\sum_{i=1}^{n} J_i(\\theta)^{\\top} r_i(\\theta)$ 的分量来书写，其中 $J_i(\\theta)$ 表示在 $x_i$ 处的雅可比行向量。\n\n将您的最终答案以一个包含三个条目的单行矩阵形式报告：第一，用 $x_i$、$V_{\\max}$ 和 $K_m$ 表示的雅可比行向量；第二，以关于 $i$ 的显式求和形式写出的高斯-牛顿法向矩阵的 $2 \\times 2$ 对称矩阵；第三，以残差 $r_i(\\theta) = y_i - f(x_i;\\theta)$ 的形式、用关于 $i$ 的显式求和写出的 $2 \\times 1$ 右侧向量。最终答案中不要包含等号。无需进行数值计算，也无需四舍五入。以符号形式表示最终答案。", "solution": "用户提供了一个有效的问题陈述。任务是推导应用于米氏动力学模型的高斯-牛顿法的法向方程的各个组成部分。该问题在酶动力学和统计推断方面具有科学依据，问题提法清晰，并提供了所有必要的定义。我将开始推导。\n\n该问题涉及对米氏模型函数参数 $\\theta = (V_{\\max}, K_m)$ 的估计\n$$\nf(x; \\theta) = \\frac{V_{\\max} x}{K_m + x}\n$$\n高斯-牛顿法通过求解称为法向方程的线性系统，为参数向量提供迭代更新 $\\Delta\\theta$：\n$$\n\\left( \\sum_{i=1}^{n} J_i(\\theta)^{\\top} J_i(\\theta) \\right) \\Delta \\theta = \\sum_{i=1}^{n} J_i(\\theta)^{\\top} r_i(\\theta)\n$$\n其中 $J_i(\\theta)$ 是第 $i$ 个观测值的雅可比行向量，$r_i(\\theta)$ 是相应的残差。我们的目标是推导雅可比行向量 $J_i(\\theta)$、矩阵 $\\sum_{i=1}^{n} J_i(\\theta)^{\\top} J_i(\\theta)$ 和向量 $\\sum_{i=1}^{n} J_i(\\theta)^{\\top} r_i(\\theta)$ 的解析表达式。\n\n首先，我们推导单个观测值 $x_i$ 的雅可比行向量。雅可比矩阵 $J_i(\\theta)$ 是一个 $1 \\times 2$ 的行向量，包含模型函数 $f(x_i; \\theta)$ 关于参数 $V_{\\max}$ 和 $K_m$ 的偏导数：\n$$\nJ_i(\\theta) = \\begin{pmatrix} \\frac{\\partial f(x_i; \\theta)}{\\partial V_{\\max}}  \\frac{\\partial f(x_i; \\theta)}{\\partial K_m} \\end{pmatrix}\n$$\n我们分别计算每个偏导数。关于 $V_{\\max}$ 的导数是：\n$$\n\\frac{\\partial f}{\\partial V_{\\max}} = \\frac{\\partial}{\\partial V_{\\max}} \\left( \\frac{V_{\\max} x_i}{K_m + x_i} \\right) = \\frac{x_i}{K_m + x_i}\n$$\n关于 $K_m$ 的导数使用商法则求得：\n$$\n\\frac{\\partial f}{\\partial K_m} = \\frac{\\partial}{\\partial K_m} \\left( \\frac{V_{\\max} x_i}{K_m + x_i} \\right) = \\frac{0 \\cdot (K_m + x_i) - (V_{\\max} x_i) \\cdot \\frac{\\partial}{\\partial K_m}(K_m + x_i)}{(K_m + x_i)^2} = \\frac{-V_{\\max} x_i}{(K_m + x_i)^2}\n$$\n结合这些结果，第 $i$ 个测量值的雅可比行向量是：\n$$\nJ_i(\\theta) = \\begin{pmatrix} \\frac{x_i}{K_m + x_i}  - \\frac{V_{\\max} x_i}{(K_m + x_i)^2} \\end{pmatrix}\n$$\n\n接下来，我们推导法向方程的 $2 \\times 2$ 对称矩阵，它是雅可比行向量与其转置的外积之和：$\\sum_{i=1}^{n} J_i(\\theta)^{\\top} J_i(\\theta)$。对于单个观测值 $i$，外积 $J_i(\\theta)^{\\top} J_i(\\theta)$ 为：\n$$\nJ_i(\\theta)^{\\top} J_i(\\theta) = \\begin{pmatrix} \\frac{x_i}{K_m + x_i} \\\\ - \\frac{V_{\\max} x_i}{(K_m + x_i)^2} \\end{pmatrix} \\begin{pmatrix} \\frac{x_i}{K_m + x_i}  - \\frac{V_{\\max} x_i}{(K_m + x_i)^2} \\end{pmatrix}\n= \\begin{pmatrix} \\left(\\frac{x_i}{K_m + x_i}\\right)^2  -\\frac{V_{\\max} x_i^2}{(K_m + x_i)^3} \\\\ -\\frac{V_{\\max} x_i^2}{(K_m + x_i)^3}  \\left(\\frac{V_{\\max} x_i}{(K_m + x_i)^2}\\right)^2 \\end{pmatrix}\n$$\n将这些矩阵对所有观测值 $i=1, \\dots, n$ 求和，得到高斯-牛顿法向矩阵：\n$$\n\\sum_{i=1}^{n} J_i(\\theta)^{\\top} J_i(\\theta) = \\begin{pmatrix}\n\\sum_{i=1}^{n} \\frac{x_i^2}{(K_m + x_i)^2}  - \\sum_{i=1}^{n} \\frac{V_{\\max} x_i^2}{(K_m + x_i)^3} \\\\\n- \\sum_{i=1}^{n} \\frac{V_{\\max} x_i^2}{(K_m + x_i)^3}  \\sum_{i=1}^{n} \\frac{V_{\\max}^2 x_i^2}{(K_m + x_i)^4}\n\\end{pmatrix}\n$$\n\n最后，我们推导法向方程的 $2 \\times 1$ 右侧向量 $\\sum_{i=1}^{n} J_i(\\theta)^{\\top} r_i(\\theta)$，其中 $r_i(\\theta) = y_i - f(x_i; \\theta)$ 是第 $i$ 个观测值的残差。该向量是雅可比转置向量的和，每个向量都乘以相应的标量残差：\n$$\n\\sum_{i=1}^{n} J_i(\\theta)^{\\top} r_i(\\theta) = \\sum_{i=1}^{n} \\begin{pmatrix} \\frac{x_i}{K_m + x_i} \\\\ - \\frac{V_{\\max} x_i}{(K_m + x_i)^2} \\end{pmatrix} r_i(\\theta) = \\begin{pmatrix}\n\\sum_{i=1}^{n} r_i(\\theta) \\frac{x_i}{K_m + x_i} \\\\\n- \\sum_{i=1}^{n} r_i(\\theta) \\frac{V_{\\max} x_i}{(K_m + x_i)^2}\n\\end{pmatrix}\n$$\n这些推导出的分量构成了针对此特定参数估计问题的高斯-牛顿算法一次迭代所必需的元素。", "answer": "$$\n\\boxed{\\begin{pmatrix} \\begin{pmatrix} \\frac{x_i}{K_m + x_i}  - \\frac{V_{\\max} x_i}{(K_m + x_i)^2} \\end{pmatrix}  \\begin{pmatrix} \\sum_{i=1}^{n} \\frac{x_i^2}{(K_m + x_i)^2}  - \\sum_{i=1}^{n} \\frac{V_{\\max} x_i^2}{(K_m + x_i)^3} \\\\ - \\sum_{i=1}^{n} \\frac{V_{\\max} x_i^2}{(K_m + x_i)^3}  \\sum_{i=1}^{n} \\frac{V_{\\max}^2 x_i^2}{(K_m + x_i)^4} \\end{pmatrix}  \\begin{pmatrix} \\sum_{i=1}^{n} r_i(\\theta) \\frac{x_i}{K_m + x_i} \\\\ - \\sum_{i=1}^{n} r_i(\\theta) \\frac{V_{\\max} x_i}{(K_m + x_i)^2} \\end{pmatrix} \\end{pmatrix}}\n$$", "id": "3322866"}, {"introduction": "标准的高斯噪声假设使得参数估计对离群值高度敏感，而离群值在高通量生物数据中普遍存在。本练习通过使用重尾的学生t分布替代高斯误差模型，来探索稳健回归的概念。通过推导和比较这两种模型的影响函数，你将理解为何以及如何选择模型来抵御离群值的扭曲效应，这是分析真实世界实验数据的一项关键技能。[@problem_id:3322854]", "problem": "一个代谢组学实验室量化了不同实验条件下的代谢物丰度，并拟合了一个参数回归模型，以通过实验协变量预测测量响应。设数据为 $\\{(x_{i}, y_{i})\\}_{i=1}^{n}$，包含 $n$ 个独立样本，其中 $y_{i}$ 是测量的对数丰度，$x_{i}$ 是协变量向量。该模型假定\n$y_{i} = f(x_{i}; \\theta) + \\varepsilon_{i}$，\n其中 $f(x_{i}; \\theta)$ 是一个平滑预测函数，$\\varepsilon_{i}$ 是独立的、均值为零的误差。在实践中，由于偶发的离子抑制或未对准事件，代谢组学数据可能包含离群值，这促使我们使用重尾噪声模型。\n\n假设以下两种误差模型，它们具有一个共同的尺度参数 $\\sigma > 0$：\n\n- Student-$t$ 模型：$\\varepsilon_{i} \\sim t_{\\nu}(0, \\sigma)$，自由度为 $\\nu > 0$，其密度为\n$$\np_{\\nu,\\sigma}(\\varepsilon) = \\frac{\\Gamma\\!\\left(\\frac{\\nu+1}{2}\\right)}{\\Gamma\\!\\left(\\frac{\\nu}{2}\\right)\\sqrt{\\nu \\pi}\\,\\sigma} \\left(1 + \\frac{\\varepsilon^{2}}{\\nu \\sigma^{2}}\\right)^{-\\frac{\\nu+1}{2}}.\n$$\n\n- 高斯模型：$\\varepsilon_{i} \\sim \\mathcal{N}(0, \\sigma^{2})$，其密度为\n$$\np_{\\sigma}(\\varepsilon) = \\frac{1}{\\sqrt{2\\pi}\\,\\sigma} \\exp\\!\\left(-\\frac{\\varepsilon^{2}}{2\\sigma^{2}}\\right).\n$$\n\n令样本 $i$ 的残差为 $r_{i}(\\theta) = y_{i} - f(x_{i}; \\theta)$。\n\n任务：\n\n1. 利用误差的独立性以及给定的 Student-$t$ 模型的密度，推导出作为 $\\theta$ 和 $\\sigma$ 的函数的负对数似然，并用残差 $r_{i}(\\theta)$ 来表示。你可以省略不依赖于 $\\theta$ 或 $\\sigma$ 的加性项。\n\n2. 在稳健回归中，对估计方程的影响由单个样本的负对数似然相对于残差的导数决定，即函数 $\\psi(r) = \\frac{d}{dr}\\rho(r)$，其中 $\\rho(r)$ 是单个样本的负对数似然贡献。推导出以下模型对应的 $\\psi$-函数：\n   - 参数为 $\\nu$ 和 $\\sigma$ 的 Student-$t$ 误差模型，\n   - 尺度参数为 $\\sigma$ 的高斯误差模型。\n\n3. 考虑代谢组学数据集中存在一小部分离群值。为了比较稳健性，定义比率\n$$\n\\Xi(r; \\nu, \\sigma) = \\frac{\\psi_{\\text{Student-}t}(r)}{\\psi_{\\text{Gaussian}}(r)}.\n$$\n提供 $\\Xi(r; \\nu, \\sigma)$ 的单个闭式解析表达式，用 $r$、$\\nu$ 和 $\\sigma$ 表示。\n\n请给出 $\\Xi(r; \\nu, \\sigma)$ 的简化表达式作为你的最终答案。不需要进行数值计算。不要包含任何单位。所有其他推导都是为了支持最终的表达式。最终答案必须是单个解析表达式。", "solution": "该问题被验证为具有科学依据、问题提出得当且客观。这些任务涉及统计建模和稳健回归中的标准推导，基于正确的数学定义。我们着手进行解答。\n\n解答通过依次解决三个任务来组织。\n\n### 任务1：Student-$t$ 模型的负对数似然\n\n数据的模型为 $y_{i} = f(x_{i}; \\theta) + \\varepsilon_{i}$，其中误差 $\\varepsilon_{i}$ 被假定为根据 Student-$t$ 分布独立同分布，即 $\\varepsilon_{i} \\sim t_{\\nu}(0, \\sigma)$。第 $i$ 个样本的残差是 $r_{i}(\\theta) = y_{i} - f(x_{i}; \\theta)$，它对应于未观测到的误差 $\\varepsilon_{i}$。\n\n每个误差 $\\varepsilon_i = r_i(\\theta)$ 的概率密度函数由下式给出：\n$$\np_{\\nu,\\sigma}(r_i(\\theta)) = \\frac{\\Gamma\\!\\left(\\frac{\\nu+1}{2}\\right)}{\\Gamma\\!\\left(\\frac{\\nu}{2}\\right)\\sqrt{\\nu \\pi}\\,\\sigma} \\left(1 + \\frac{r_i(\\theta)^{2}}{\\nu \\sigma^{2}}\\right)^{-\\frac{\\nu+1}{2}}.\n$$\n由于误差的独立性，整个数据集的似然函数 $L(\\theta, \\sigma)$ 是单个概率密度的乘积：\n$$\nL(\\theta, \\sigma) = \\prod_{i=1}^{n} p_{\\nu,\\sigma}(r_i(\\theta)) = \\prod_{i=1}^{n} \\left[ \\frac{\\Gamma\\!\\left(\\frac{\\nu+1}{2}\\right)}{\\Gamma\\!\\left(\\frac{\\nu}{2}\\right)\\sqrt{\\nu \\pi}\\,\\sigma} \\left(1 + \\frac{r_i(\\theta)^{2}}{\\nu \\sigma^{2}}\\right)^{-\\frac{\\nu+1}{2}} \\right].\n$$\n对数似然 $\\ell(\\theta, \\sigma) = \\ln(L(\\theta, \\sigma))$ 是单个密度的对数之和：\n$$\n\\ell(\\theta, \\sigma) = \\sum_{i=1}^{n} \\ln \\left[ p_{\\nu,\\sigma}(r_i(\\theta)) \\right] = \\sum_{i=1}^{n} \\left[ \\ln\\left(\\frac{\\Gamma\\!\\left(\\frac{\\nu+1}{2}\\right)}{\\Gamma\\!\\left(\\frac{\\nu}{2}\\right)\\sqrt{\\nu \\pi}}\\right) - \\ln(\\sigma) - \\frac{\\nu+1}{2} \\ln\\left(1 + \\frac{r_i(\\theta)^{2}}{\\nu \\sigma^{2}}\\right) \\right].\n$$\n这可以展开为：\n$$\n\\ell(\\theta, \\sigma) = n \\ln\\left(\\frac{\\Gamma\\!\\left(\\frac{\\nu+1}{2}\\right)}{\\Gamma\\!\\left(\\frac{\\nu}{2}\\right)\\sqrt{\\nu \\pi}}\\right) - n \\ln(\\sigma) - \\frac{\\nu+1}{2} \\sum_{i=1}^{n} \\ln\\left(1 + \\frac{r_i(\\theta)^{2}}{\\nu \\sigma^{2}}\\right).\n$$\n负对数似然 $-\\ell(\\theta, \\sigma)$ 为：\n$$\n-\\ell(\\theta, \\sigma) = -n \\ln\\left(\\frac{\\Gamma\\!\\left(\\frac{\\nu+1}{2}\\right)}{\\Gamma\\!\\left(\\frac{\\nu}{2}\\right)\\sqrt{\\nu \\pi}}\\right) + n \\ln(\\sigma) + \\frac{\\nu+1}{2} \\sum_{i=1}^{n} \\ln\\left(1 + \\frac{r_i(\\theta)^{2}}{\\nu \\sigma^{2}}\\right).\n$$\n按照指示，我们省略不依赖于参数 $\\theta$ 或 $\\sigma$ 的加性项。参数 $\\nu$ 被认为是固定的。因此，项 $-n \\ln\\left(\\frac{\\Gamma\\!\\left(\\frac{\\nu+1}{2}\\right)}{\\Gamma\\!\\left(\\frac{\\nu}{2}\\right)\\sqrt{\\nu \\pi}}\\right)$ 被省略。我们旨在最小化的负对数似然的最终表达式为：\n$$\n\\mathcal{L}_{\\text{Student-}t}(\\theta, \\sigma) = n \\ln(\\sigma) + \\frac{\\nu+1}{2} \\sum_{i=1}^{n} \\ln\\left(1 + \\frac{r_i(\\theta)^{2}}{\\nu \\sigma^{2}}\\right).\n$$\n\n### 任务2：$\\psi$-函数的推导\n\n函数 $\\psi(r)$ 被定义为单个样本的负对数似然贡献 $\\rho(r)$ 相对于残差 $r$ 的导数。\n\n**对于 Student-$t$ 模型：**\n依赖于残差 $r$ 的单个样本对负对数似然的贡献表示为 $\\rho_{\\text{Student-}t}(r)$。从上面推导出的完整负对数似然中，我们通过取求和内的项并将 $r_i(\\theta)$ 视为 $r$ 来识别 $\\rho(r)$。相对于 $r$ 是常数的加性项（如 $\\ln(\\sigma)$）被省略，因为它们的导数为零。\n$$\n\\rho_{\\text{Student-}t}(r) = \\frac{\\nu+1}{2} \\ln\\left(1 + \\frac{r^{2}}{\\nu \\sigma^{2}}\\right).\n$$\n现在，我们计算导数 $\\psi_{\\text{Student-}t}(r) = \\frac{d}{dr}\\rho_{\\text{Student-}t}(r)$:\n$$\n\\psi_{\\text{Student-}t}(r) = \\frac{d}{dr} \\left[ \\frac{\\nu+1}{2} \\ln\\left(1 + \\frac{r^{2}}{\\nu \\sigma^{2}}\\right) \\right].\n$$\n使用链式法则求导，我们得到：\n$$\n\\psi_{\\text{Student-}t}(r) = \\frac{\\nu+1}{2} \\cdot \\frac{1}{1 + \\frac{r^{2}}{\\nu \\sigma^{2}}} \\cdot \\frac{d}{dr}\\left(\\frac{r^{2}}{\\nu \\sigma^{2}}\\right) = \\frac{\\nu+1}{2} \\cdot \\frac{1}{1 + \\frac{r^{2}}{\\nu \\sigma^{2}}} \\cdot \\frac{2r}{\\nu \\sigma^{2}}.\n$$\n化简该表达式：\n$$\n\\psi_{\\text{Student-}t}(r) = (\\nu+1) \\cdot \\frac{r}{\\nu \\sigma^{2} \\left(1 + \\frac{r^{2}}{\\nu \\sigma^{2}}\\right)} = \\frac{(\\nu+1)r}{\\nu \\sigma^{2} + r^{2}}.\n$$\n\n**对于高斯模型：**\n概率密度函数为 $p_{\\sigma}(\\varepsilon) = \\frac{1}{\\sqrt{2\\pi}\\,\\sigma} \\exp\\!\\left(-\\frac{\\varepsilon^{2}}{2\\sigma^{2}}\\right)$。\n对于一个残差 $r$，单个样本的负对数似然为：\n$$\n-\\ln(p_{\\sigma}(r)) = -\\ln\\left(\\frac{1}{\\sqrt{2\\pi}\\,\\sigma}\\right) - \\left(-\\frac{r^{2}}{2\\sigma^{2}}\\right) = \\ln(\\sqrt{2\\pi}) + \\ln(\\sigma) + \\frac{r^{2}}{2\\sigma^{2}}.\n$$\n依赖于残差 $r$ 的项是 $\\rho_{\\text{Gaussian}}(r)$：\n$$\n\\rho_{\\text{Gaussian}}(r) = \\frac{r^{2}}{2\\sigma^{2}}.\n$$\n这对应于普通最小二乘目标函数，按 $1/(2\\sigma^2)$ 进行了缩放。\n现在，我们计算导数 $\\psi_{\\text{Gaussian}}(r) = \\frac{d}{dr}\\rho_{\\text{Gaussian}}(r)$:\n$$\n\\psi_{\\text{Gaussian}}(r) = \\frac{d}{dr}\\left(\\frac{r^{2}}{2\\sigma^{2}}\\right) = \\frac{2r}{2\\sigma^{2}} = \\frac{r}{\\sigma^{2}}.\n$$\n\n### 任务3：比率 $\\Xi(r; \\nu, \\sigma)$\n\n比率 $\\Xi(r; \\nu, \\sigma)$ 被定义为两个 $\\psi$-函数的商：\n$$\n\\Xi(r; \\nu, \\sigma) = \\frac{\\psi_{\\text{Student-}t}(r)}{\\psi_{\\text{Gaussian}}(r)}.\n$$\n代入任务2中推导的表达式：\n$$\n\\Xi(r; \\nu, \\sigma) = \\frac{\\frac{(\\nu+1)r}{\\nu \\sigma^{2} + r^{2}}}{\\frac{r}{\\sigma^{2}}}.\n$$\n假设 $r \\neq 0$，我们可以通过乘以分母的倒数来化简此表达式：\n$$\n\\Xi(r; \\nu, \\sigma) = \\frac{(\\nu+1)r}{\\nu \\sigma^{2} + r^{2}} \\cdot \\frac{\\sigma^{2}}{r}.\n$$\n从分子和分母中消去项 $r$ 得到最终的闭式表达式：\n$$\n\\Xi(r; \\nu, \\sigma) = \\frac{(\\nu+1)\\sigma^{2}}{\\nu \\sigma^{2} + r^{2}}.\n$$\n这个比率表示 Student-$t$ 模型相对于高斯模型赋给残差 $r$ 的权重。对于大的残差（$|r| \\to \\infty$），$\\Xi \\to 0$，这通过降低离群值的影响展示了 Student-$t$ 模型的稳健性。对于小的残差（$r \\to 0$），$\\Xi \\to \\frac{\\nu+1}{\\nu}$，这表现出与高斯模型类似的行为，但带有一个缩放因子。", "answer": "$$\n\\boxed{\\frac{(\\nu+1)\\sigma^{2}}{\\nu \\sigma^{2} + r^{2}}}\n$$", "id": "3322854"}]}