{"hands_on_practices": [{"introduction": "现代差异基因表达分析的核心在于精确的统计建模。由于RNA测序（RNA-seq）数据普遍存在超出泊松分布预期的“过度离散”现象，负二项（Negative Binomial, NB）分布成为了捕获原始计数数据变异性的理想选择。本练习将引导你从第一性原理出发，为基因表达计数数据拟合一个负二项广义线性模型（GLM）。通过亲自推导并实现基于最大似然估计的迭代重加权最小二乘（IRLS）算法，你将能深刻理解DESeq2和edgeR等主流分析工具的底层工作机制，从而超越“黑箱”式的使用，真正掌握效应量评估的核心技术。[@problem_id:3301611]", "problem": "在计算系统生物学中，给定一个基于计数的基因表达场景，用于差异基因表达分析和效应量评估。基本假设如下：(i) 分子生物学的中心法则意味着基因表达水平可以通过读取计数进行量化；(ii) 当存在超出泊松模型的过度离散时，计数数据可以通过负二项分布很好地建模；(iii) 广义线性模型（GLM, Generalized Linear Model）提供了一个原则性框架，通过一个连接函数将协变量与分布的均值关联起来；(iv) 最大似然估计（MLE, Maximum Likelihood Estimation）通过在指定的概率模型下最大化观测数据的似然来提供参数估计。\n\n对于由 $g$ 索引的每个基因，以及由 $i = 1, \\dots, n$ 索引的每个样本，原始计数 $y_{g i}$ 被建模为负二项随机变量，其均值参数为 $\\mu_{g i}$，基因特异性离散度参数 $\\phi_g$ 已知。方差函数为 $V(y_{g i}) = \\mu_{g i} + \\phi_g \\mu_{g i}^2$。带有对数连接函数的广义线性模型规定\n$$\n\\log \\mu_{g i} = \\alpha_g + x_i^\\top \\beta_g,\n$$\n其中 $\\alpha_g$ 是一个截距，捕捉基因 $g$ 的基线表达水平，$x_i$ 是样本 $i$ 的已知协变量向量，$\\beta_g$ 是待估计的效应量参数向量（差异表达效应）。对于每个基因，离散度 $\\phi_g$ 是已知且固定的。您的任务是在指定的模型假设下，通过最大似然法估计 $\\beta_g$。\n\n从上述基本定义出发，推导一个算法，该算法仅基于负二项对数似然和带有对数连接的广义线性模型结构来计算 $\\beta_g$（以及冗余截距 $\\alpha_g$）的最大似然估计。该算法应基于第一性原理：使用最大似然估计框架，推导必要的得分方程，并设计一个能收敛到最大似然估计的迭代方法。根据在最大似然估计处得到的观测信息，计算 $\\beta_g$ 每个分量的沃尔德标准误。将 $\\beta_g$ 解释为均值尺度上的对数效应量。\n\n将推导出的算法实现为一个完整的、可运行的程序。对于每个提供的测试案例基因，该程序输出第一个协变量分量（不包括截距）的估计效应量及其估计标准误。本问题中没有物理单位。所有答案都应以浮点数形式提供。\n\n使用以下测试套件，其中包含 $n = 6$ 个样本和单个协变量 $x_i$，指示处理组 ($x_i = 1$) 与对照组 ($x_i = 0$):\n\n- 协变量 $x_i$（对于 $i = 1, \\dots, 6$）：$\\{0, 0, 0, 1, 1, 1\\}$。\n\n- 基因 $g = 1$：计数 $y_{1 i}$ 为 $\\{20, 24, 22, 40, 45, 43\\}$，离散度 $\\phi_1 = 0.1$。\n\n- 基因 $g = 2$：计数 $y_{2 i}$ 为 $\\{0, 1, 0, 3, 2, 1\\}$，离散度 $\\phi_2 = 1.0$。\n\n- 基因 $g = 3$：计数 $y_{3 i}$ 为 $\\{100, 110, 90, 95, 105, 85\\}$，离散度 $\\phi_3 = 0.5$。\n\n- 基因 $g = 4$：计数 $y_{4 i}$ 为 $\\{5, 6, 4, 5, 6, 4\\}$，离散度 $\\phi_4 = 10^{-6}$。\n\n对于每个基因，计算单元素 $\\beta_g$ 的最大似然估计及其沃尔德标准误。您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。对于每个基因，按 $g = 1, 2, 3, 4$ 的顺序，包含一个双元素列表 $[\\widehat{\\beta}_g, \\mathrm{SE}(\\widehat{\\beta}_g)]$，两个值均四舍五入到六位小数。例如，整体输出格式必须是\n$$\n[\\,[\\widehat{\\beta}_1,\\mathrm{SE}(\\widehat{\\beta}_1)],\\,[\\widehat{\\beta}_2,\\mathrm{SE}(\\widehat{\\beta}_2)],\\,[\\widehat{\\beta}_3,\\mathrm{SE}(\\widehat{\\beta}_3)],\\,[\\widehat{\\beta}_4,\\mathrm{SE}(\\widehat{\\beta}_4)]\\,],\n$$\n打印为单行，无任何附加文本。", "solution": "用户提供的问题是计算系统生物学中一个提法明确且具有科学依据的任务，具体涉及对计数数据的广义线性模型（GLM）中的参数进行估计。该问题是有效的，因为它是自洽的、客观的，并且基于已建立的差异基因表达分析统计原理。因此，我们可以着手提供一个完整的解决方案。\n\n目标是推导并实现一个算法，以在负二项（NB）广义线性模型中找到效应量参数 $\\beta_g$ 的最大似然估计（MLE）。对于每个基因 $g$，对于样本 $i=1, \\dots, n$ 观测到的计数 $y_{gi}$ 被建模为：\n$$\ny_{gi} \\sim \\text{NB}(\\mu_{gi}, \\phi_g)\n$$\n均值为 $\\mu_{gi}$，离散度 $\\phi_g$ 是每个基因已知且固定的参数。方差由 $V(y_{gi}) = \\mu_{gi} + \\phi_g \\mu_{gi}^2$ 给出。均值 $\\mu_{gi}$ 通过一个对数连接函数与样本特异性协变量 $x_i$ 相关联：\n$$\n\\log(\\mu_{gi}) = \\eta_{gi} = \\alpha_g + x_i^\\top \\beta_g\n$$\n这里，$\\eta_{gi}$ 是线性预测器，$\\alpha_g$ 是截距参数，$\\beta_g$ 是效应量参数向量。令 $\\theta_g = (\\alpha_g, \\beta_g^\\top)^\\top$ 为待估计的完整参数向量。一个基因的设计矩阵 $X$ 的行为 $(1, x_i^\\top)$，因此 $\\eta_g = X \\theta_g$。为简化起见，在接下来的推导中我们将省略基因索引 $g$。\n\n**1. 对数似然函数**\n\n一个均值为 $\\mu$、离散度为 $\\phi$（其中尺寸参数 $r = 1/\\phi$）的负二项随机变量 $Y$ 的概率质量函数可以参数化为：\n$$\nP(Y=y | \\mu, \\phi) = \\frac{\\Gamma(y + 1/\\phi)}{\\Gamma(y+1)\\Gamma(1/\\phi)} \\left( \\frac{\\mu}{1/\\phi + \\mu} \\right)^y \\left( \\frac{1/\\phi}{1/\\phi + \\mu} \\right)^{1/\\phi}\n$$\n单个观测值 $y_i$ 的对数似然为：\n$$\n\\ell(\\theta | y_i) = \\log\\Gamma(y_i + 1/\\phi) - \\log\\Gamma(y_i+1) - \\log\\Gamma(1/\\phi) + y_i \\log(\\mu_i) + \\frac{1}{\\phi} \\log(1/\\phi) - (y_i + 1/\\phi) \\log(\\mu_i + 1/\\phi)\n$$\n忽略不依赖于参数 $\\theta$（也即不依赖于 $\\mu_i$）的项，对数似然的相关部分是：\n$$\n\\ell(\\theta | y_i) \\propto y_i \\log(\\mu_i) - (y_i + 1/\\phi) \\log(\\mu_i + 1/\\phi)\n$$\n对于 $n$ 个独立观测值的总对数似然是所有样本的总和：\n$$\nL(\\theta) = \\sum_{i=1}^n \\ell(\\theta | y_i) = \\sum_{i=1}^n \\left[ y_i \\log(\\mu_i) - (y_i + 1/\\phi) \\log(\\mu_i + 1/\\phi) \\right]\n$$\n\n**2. 得分方程和费雪信息**\n\n为了找到最大似然估计，我们必须解得分方程 $U(\\theta) = \\nabla_\\theta L(\\theta) = 0$。我们使用链式法则：$\\frac{\\partial L}{\\partial \\theta_j} = \\sum_i \\frac{\\partial \\ell_i}{\\partial \\mu_i} \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\frac{\\partial \\eta_i}{\\partial \\theta_j}$。\n\n各部分为：\n- $\\frac{\\partial \\eta_i}{\\partial \\theta_j} = X_{ij}$，其中 $X_{ij}$ 是设计矩阵中样本 $i$ 和参数 $j$ 对应的元素。\n- 使用对数连接 $\\eta_i = \\log(\\mu_i)$，我们有 $\\mu_i = e^{\\eta_i}$，所以 $\\frac{\\partial \\mu_i}{\\partial \\eta_i} = e^{\\eta_i} = \\mu_i$。\n- $\\frac{\\partial \\ell_i}{\\partial \\mu_i} = \\frac{y_i}{\\mu_i} - \\frac{y_i + 1/\\phi}{\\mu_i + 1/\\phi} = \\frac{y_i(\\mu_i + 1/\\phi) - \\mu_i(y_i + 1/\\phi)}{\\mu_i(\\mu_i + 1/\\phi)} = \\frac{y_i/\\phi - \\mu_i/\\phi}{\\mu_i(\\mu_i + 1/\\phi)} = \\frac{y_i - \\mu_i}{\\mu_i(1 + \\phi \\mu_i)}$。我们注意到，问题中给出的方差函数 $V(y_{gi}) = \\mu_{gi} + \\phi_g \\mu_{gi}^2$ 正是对应于此对数似然形式的方差函数。因此，对似然函数求导是正确的。\n\n组合各部分得到得分贡献：\n$$\n\\frac{\\partial \\ell_i}{\\partial \\theta_j} = \\left( \\frac{y_i - \\mu_i}{\\mu_i(1 + \\phi \\mu_i)} \\right) (\\mu_i) (X_{ij}) = \\frac{y_i - \\mu_i}{1 + \\phi \\mu_i} X_{ij}\n$$\n得分向量 $U(\\theta)$ 的分量为 $U_j(\\theta) = \\sum_{i=1}^n \\frac{y_i - \\mu_i}{1 + \\phi \\mu_i} X_{ij}$。用矩阵表示法，即为 $U(\\theta) = X^\\top W_s (y - \\mu)$，其中 $W_s$ 是一个对角矩阵，其元素为 $(W_s)_{ii} = (1+\\phi\\mu_i)^{-1}$。这些方程在 $\\theta$ 上是非线性的，需要迭代求解。\n\n**3. 费雪评分法和迭代重加权最小二乘法（IRLS）**\n\n求解广义线性模型得分方程的标准方法是费雪评分法，这是一种基于牛顿-拉弗森算法的迭代过程。更新规则为：\n$$\n\\theta^{(t+1)} = \\theta^{(t)} + I(\\theta^{(t)})^{-1} U(\\theta^{(t)})\n$$\n其中 $I(\\theta)$ 是费雪信息矩阵，由 $I(\\theta) = -E[H(\\theta)]$ 给出，其中 $H(\\theta)$ 是海森矩阵（$\\nabla_\\theta^2 L(\\theta)$）。广义线性模型的费雪信息矩阵的第 $(j,k)$ 个元素为：\n$$\nI_{jk}(\\theta) = \\sum_{i=1}^n w_i X_{ij} X_{ik} \\quad \\text{其中} \\quad w_i = \\frac{1}{V(\\mu_i)}\\left(\\frac{\\partial \\mu_i}{\\partial \\eta_i}\\right)^2\n$$\n对于我们的负二项广义线性模型（带对数连接）：\n- $V(\\mu_i) = \\mu_i + \\phi \\mu_i^2 = \\mu_i(1 + \\phi\\mu_i)$\n- $\\frac{\\partial \\mu_i}{\\partial \\eta_i} = \\mu_i$\n因此，权重为：\n$$\nw_i = \\frac{1}{\\mu_i(1 + \\phi\\mu_i)} (\\mu_i)^2 = \\frac{\\mu_i}{1 + \\phi\\mu_i}\n$$\n费雪信息矩阵为 $I(\\theta) = X^\\top W X$，其中 $W$ 是权重 $w_i$ 的对角矩阵。\n\n费雪评分法更新可以重排为一个迭代重加权最小二乘（IRLS）问题。定义一个“工作响应”向量 $z$：\n$$\nz_i = \\eta_i + (y_i - \\mu_i) \\frac{\\partial \\eta_i}{\\partial \\mu_i} = \\eta_i + \\frac{y_i - \\mu_i}{\\mu_i}\n$$\n$\\theta$ 的更新则通过求解加权最小二乘问题得到：\n$$\n\\theta^{(t+1)} = (X^\\top W^{(t)} X)^{-1} X^\\top W^{(t)} z^{(t)}\n$$\n其中右侧的量均在当前估计 $\\theta^{(t)}$ 处计算。\n\nIRLS 算法流程如下：\n1. 初始化参数向量 $\\theta^{(0)}$。一个合理的起点是 $\\beta^{(0)}=0$ 和 $\\alpha^{(0)}=\\log(\\bar{y})$。\n2. 对 $t=0, 1, 2, \\dots$ 进行迭代，直至收敛：\n    a. 计算线性预测器：$\\eta^{(t)} = X \\theta^{(t)}$。\n    b. 计算均值向量：$\\mu^{(t)} = \\exp(\\eta^{(t)})$。\n    c. 计算对角权重矩阵 $W^{(t)}$，其权重为 $w_i^{(t)} = \\frac{\\mu_i^{(t)}}{1 + \\phi\\mu_i^{(t)}}$。\n    d. 计算工作响应向量 $z^{(t)}$，其分量为 $z_i^{(t)} = \\eta_i^{(t)} + \\frac{y_i - \\mu_i^{(t)}}{\\mu_i^{(t)}}$。\n    e. 通过解线性系统更新参数：$\\theta^{(t+1)} = (X^\\top W^{(t)} X)^{-1} X^\\top W^{(t)} z^{(t)}$。\n3. 当 $\\theta$ 的变化，例如 $||\\theta^{(t+1)} - \\theta^{(t)}||^2$，小于一个小的容差时，迭代停止。\n\n**4. 标准误**\n\n收敛到最大似然估计 $\\hat{\\theta}$ 后，估计量的渐近协方差矩阵由在最大似然估计处计算的费雪信息矩阵的逆矩阵给出：\n$$\n\\text{Cov}(\\hat{\\theta}) = I(\\hat{\\theta})^{-1} = (X^\\top \\hat{W} X)^{-1}\n$$\n其中 $\\hat{W}$ 是使用最终估计均值 $\\hat{\\mu}$ 计算的权重矩阵。此协方差矩阵的对角元素是参数估计量的估计方差。第 $j$ 个参数估计 $\\hat{\\theta}_j$ 的沃尔德标准误是第 $j$ 个对角元素的平方根：\n$$\n\\text{SE}(\\hat{\\theta}_j) = \\sqrt{(\\text{Cov}(\\hat{\\theta}))_{jj}}\n$$\n对于本问题，$\\theta = (\\alpha, \\beta)^\\top$，所以我们关心的是 $\\hat{\\beta}$ 和 $\\text{SE}(\\hat{\\beta})$，它们对应于 $\\hat{\\theta}$ 的第二个元素和协方差矩阵 $(2,2)$ 元素的平方根。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef fit_nb_glm(y_counts, covariates, phi, max_iter=100, tol=1e-8):\n    \"\"\"\n    Fits a Negative Binomial Generalized Linear Model using IRLS.\n\n    Args:\n        y_counts (np.ndarray): Vector of observed counts.\n        covariates (np.ndarray): Vector of covariates for the non-intercept term.\n        phi (float): Known dispersion parameter.\n        max_iter (int): Maximum number of iterations for IRLS.\n        tol (float): Convergence tolerance.\n\n    Returns:\n        tuple: A tuple containing:\n            - beta_hat (float): The estimated coefficient for the covariate.\n            - se_beta (float): The standard error of the estimated coefficient.\n    \"\"\"\n    # Design matrix X should include an intercept column\n    n_samples = len(y_counts)\n    X = np.ones((n_samples, 2))\n    X[:, 1] = covariates\n\n    # Initial parameter estimates\n    # A safe starting point for the intercept if all y_counts are 0\n    mean_y = np.mean(y_counts)\n    if mean_y == 0:\n        alpha = -10.0  # Heuristic for log(small_number)\n    else:\n        alpha = np.log(mean_y)\n    beta = 0.0\n    theta = np.array([alpha, beta])\n\n    for i in range(max_iter):\n        # 1. Linear predictor and mean\n        eta = X @ theta\n        mu = np.exp(eta)\n\n        # 2. Weights for Fisher Information matrix\n        # w_i = mu_i / (1 + phi * mu_i)\n        weights = mu / (1.0 + phi * mu)\n        W = np.diag(weights)\n\n        # 3. Working response\n        # z_i = eta_i + (y_i - mu_i) / mu_i\n        # Handle mu_i close to zero to avoid division issues.\n        # If mu is very small, the weight is also very small, so the contribution\n        # to the weighted least squares is minimal. We can add a small epsilon.\n        mu_safe = np.maximum(mu, 1e-10)\n        z = eta + (y_counts - mu) / mu_safe\n\n        # 4. Update parameters via weighted least squares\n        # theta_new = (X^T W X)^-1 X^T W z\n        # Solving the linear system is more stable than inverting.\n        XT_W = X.T @ W\n        XT_W_X = XT_W @ X\n        XT_W_z = XT_W @ z\n        \n        try:\n            # Check if matrix is invertible\n            if np.linalg.det(XT_W_X) == 0:\n                # This can happen with perfect separation or other data issues\n                # Return NaN to indicate failure to converge\n                return np.nan, np.nan\n            theta_new = np.linalg.solve(XT_W_X, XT_W_z)\n        except np.linalg.LinAlgError:\n            # Fails to converge, likely due to singular matrix\n            return np.nan, np.nan\n\n        # 5. Check for convergence\n        diff = np.sum((theta_new - theta) ** 2)\n        theta = theta_new\n        if diff  tol:\n            break\n\n    # After convergence, calculate standard errors\n    # Cov(theta_hat) = (X^T W X)^-1\n    final_eta = X @ theta\n    final_mu = np.exp(final_eta)\n    final_weights = final_mu / (1.0 + phi * final_mu)\n    final_W = np.diag(final_weights)\n    \n    try:\n        cov_matrix = np.linalg.inv(X.T @ final_W @ X)\n        se_beta = np.sqrt(cov_matrix[1, 1])\n    except np.linalg.LinAlgError:\n        se_beta = np.nan\n\n    beta_hat = theta[1]\n    \n    return beta_hat, se_beta\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Structure: (counts, dispersion)\n    test_cases = [\n        (np.array([20, 24, 22, 40, 45, 43]), 0.1),\n        (np.array([0, 1, 0, 3, 2, 1]), 1.0),\n        (np.array([100, 110, 90, 95, 105, 85]), 0.5),\n        (np.array([5, 6, 4, 5, 6, 4]), 1e-6),\n    ]\n\n    # Covariate vector is the same for all cases.\n    covariates = np.array([0, 0, 0, 1, 1, 1])\n\n    results = []\n    for y_counts, phi in test_cases:\n        beta_hat, se_beta = fit_nb_glm(y_counts, covariates, phi)\n        results.append([beta_hat, se_beta])\n\n    # Format the output string as required.\n    # [[beta1,SE1],[beta2,SE2],...]\n    results_str_list = []\n    for beta, se in results:\n        # Round to six decimal places for output\n        beta_str = f\"{beta:.6f}\"\n        se_str = f\"{se:.6f}\"\n        results_str_list.append(f\"[{beta_str},{se_str}]\")\n    \n    final_output = f\"[{','.join(results_str_list)}]\"\n\n    # Final print statement in the exact required format.\n    print(final_output)\n\nsolve()\n```", "id": "3301611"}, {"introduction": "在应用复杂的统计模型之前，对原始测序计数进行归一化是至关重要的一步，它旨在校正样本间因测序深度等因素引起的技术偏差。然而，不同的归一化方法可能对最终的分析结果产生显著影响。本练习将让你亲手实现三种广泛使用的归一化策略：TMM（Trimmed Mean of M-values）、上四分位数（Upper-Quartile）和DESeq的中位数比率法。通过比较不同方法得到的效应量估计值，你将量化归一化选择对分析结论的敏感性，从而认识到在生物信息学流程中进行敏感性分析的必要性。[@problem_id:3301660]", "problem": "给定代表一组基因在两种条件下跨生物学重复的核糖核酸测序 (RNA-seq) 基因表达的计数矩阵，以及研究基因水平效应量估计对标准化方法选择的敏感性的要求。该任务涉及三种广泛使用的标准化方案：M-值的修剪均值 (TMM)、上四分位数以及在差异表达测序 (DESeq) 中使用的比率中位数大小因子。计算目标是估计每个基因在每种标准化下的效应量，计算方法之间效应量的两两差异，并识别对标准化选择最敏感的基因。\n\n基本基础和假设：测序计数是源自分子取样的非负整数，具有样本特异性的乘法偏差。假设有 $G$ 个基因，由 $g \\in \\{0,\\dots,G-1\\}$ 索引，以及 $S$ 个样本，由 $i \\in \\{0,\\dots,S-1\\}$ 索引。设 $x_{g i}$ 表示样本 $i$ 中基因 $g$ 的观测计数。设 $L_i = \\sum_{g=0}^{G-1} x_{g i}$ 表示样本 $i$ 的文库大小。每个样本属于一个条件 $c_i \\in \\{\\text{A}, \\text{B}\\}$。对于标准化，定义样本特异性大小因子 $s_i$，使得标准化后的计数为 $y_{g i} = x_{g i} / s_i$。条件间的效应量将通过 $y_{g i}$ 的组水平汇总统计量的对数比率来评估。\n\n要实现的标准化方法：\n- M-值的修剪均值 (TMM)：M-值的修剪均值 (TMM) 通过移除极端基因并计算相对表达量对数比率的均值来估计相对缩放偏移。选择文库大小 $L_r$ 最接近 $\\operatorname{median}\\{L_0,\\dots,L_{S-1}\\}$ 的样本作为参考样本 $r$。对于每个非参考样本 $i \\neq r$，定义每个基因的量 $p_{g i} = x_{g i} / L_i$ 和 $p_{g r} = x_{g r} / L_r$，然后对于 $x_{g i}  0$ 和 $x_{g r}  0$ 的基因，定义 $M_{g i} = \\log_2\\!\\left(\\frac{p_{g i}}{p_{g r}}\\right)$ 和 $A_{g i} = \\frac{1}{2}\\log_2\\!\\left(p_{g i} p_{g r}\\right)$。通过修剪 $M_{g i}$ 的下 $p_M$ 和上 $p_M$ 分位数以及 $A_{g i}$ 的下 $p_A$ 和上 $p_A$ 分位数来排除基因，然后计算保留基因的修剪均值 $\\overline{M}_i$。对于 $i \\neq r$，设 $s_r = 1$ 和 $s_i = 2^{\\overline{M}_i}$。如果对于样本 $i$，修剪后没有基因剩下，则设 $s_i = 1$。\n- 上四分位数大小因子：对于每个样本 $i$，计算其上四分位数 $U_i$ 为 $\\{x_{g i} : x_{g i}  0\\}$ 的第 $75$ 百分位数。定义 $s_i = U_i$。如果一个样本没有正数计数，则设 $s_i = 1$。\n- DESeq 比率中位数大小因子：对于在所有样本中都有严格正数计数（即，对于所有 $i$，$x_{g i}  0$）的每个基因 $g$，计算其几何平均值 $G_g = \\exp\\!\\left(\\frac{1}{S}\\sum_{i=0}^{S-1} \\log x_{g i}\\right)$。对于每个样本 $i$，对所有这些基因 $g$ 形成比率 $r_{g i} = x_{g i}/G_g$，并将 $s_i$ 定义为 $\\operatorname{median}\\{r_{g i}\\}_g$。如果没有基因在所有样本中都有严格正数计数，则退回到文库大小缩放，$s_i = L_i / \\operatorname{median}\\{L_0,\\dots,L_{S-1}\\}$。\n\n效应量定义：对于产生大小因子 $s^{(m)}_i$ 的标准化方法 $m$，定义标准化计数 $y^{(m)}_{g i} = x_{g i} / s^{(m)}_i$。对于固定的伪计数 $c  0$，定义条件 $\\text{A}$ 内的组均值为 $\\mu^{(m)}_{g,\\text{A}} = \\frac{1}{|I_{\\text{A}}|}\\sum_{i \\in I_{\\text{A}}} y^{(m)}_{g i}$，条件 $\\text{B}$ 内的组均值为 $\\mu^{(m)}_{g,\\text{B}} = \\frac{1}{|I_{\\text{B}}|}\\sum_{i \\in I_{\\text{B}}} y^{(m)}_{g i}$，其中 $I_{\\text{A}}$ 和 $I_{\\text{B}}$ 分别是条件 $\\text{A}$ 和 $\\text{B}$ 中样本的索引集。效应量为 $ES^{(m)}_g = \\log_2\\!\\left(\\frac{\\mu^{(m)}_{g,\\text{A}} + c}{\\mu^{(m)}_{g,\\text{B}} + c}\\right)$。\n\n敏感性分析：对于一对方法 $(m_1,m_2)$，定义 $\\Delta ES_g(m_1,m_2) = ES^{(m_1)}_g - ES^{(m_2)}_g$。定义敏感性得分 $S_g = \\max\\{|\\Delta ES_g(\\text{TMM},\\text{UQ})|, |\\Delta ES_g(\\text{TMM},\\text{DESeq})|, |\\Delta ES_g(\\text{UQ},\\text{DESeq})|\\}$。对于一个阈值 $\\tau  0$，如果 $S_g \\ge \\tau$，则一个基因被认为是敏感的。\n\n您的程序必须实现这三种标准化方法，为每个基因和方法计算 $ES^{(m)}_g$，为所有方法对计算 $\\Delta ES_g$，计算 $S_g$，并为每个提供的测试用例返回被分类为敏感基因的从零开始的索引。如果任何中间量需要修剪或集合排除，请遵守上述定义。如果任何标准化方法遇到退化情况（例如，没有有效基因或百分位数未定义），请应用所描述的明确备用方案。\n\n测试套件（三个案例；每个案例指定一个形状为 $G \\times S$ 的整数计数矩阵、组成员关系、伪计数 $c$、修剪参数 $p_M$ 和 $p_A$，以及敏感性阈值 $\\tau$）。对于所有案例，使用从零开始的基因索引和从零开始的样本索引。\n\n- 案例 $1$（正常路径，变化的文库大小，混合差异表达）：\n    - $G = 8$ 个基因和 $S = 6$ 个样本的计数 $x_{g i}$（行是基因 $g = 0,\\dots,7$；列是样本 $i = 0,\\dots,5$）：\n      - 基因 $0$：$[50,55,52,49,53,50]$\n      - 基因 $1$：$[100,95,105,280,300,320]$\n      - 基因 $2$：$[200,210,190,80,75,85]$\n      - 基因 $3$：$[0,5,0,30,35,40]$\n      - 基因 $4$：$[400,420,410,390,380,400]$\n      - 基因 $5$：$[10,12,11,9,8,10]$\n      - 基因 $6$：$[8000,8200,7800,7000,7200,7100]$\n      - 基因 $7$：$[2,1,0,3,4,0]$\n    - 条件索引集：$I_{\\text{A}} = \\{0,1,2\\}$， $I_{\\text{B}} = \\{3,4,5\\}$。\n    - 伪计数：$c = 0.5$。\n    - TMM 修剪分数：$p_M = 0.3$, $p_A = 0.05$。\n    - 敏感性阈值：$\\tau = 0.3$。\n\n- 案例 $2$（边界情况，大量零值，极端的文库大小不平衡，如果需要，可能会触发 DESeq 的备用方案）：\n    - $G = 6$ 个基因和 $S = 6$ 个样本的计数 $x_{g i}$：\n      - 基因 $0$：$[0,0,0,0,0,0]$\n      - 基因 $1$：$[5,0,2,0,5,2]$\n      - 基因 $2$：$[0,0,50,0,0,150]$\n      - 基因 $3$：$[20,2000,25,10,900,20]$\n      - 基因 $4$：$[30,1500,20,20,800,30]$\n      - 基因 $5$：$[10,1000,15,5,700,10]$\n    - 条件索引集：$I_{\\text{A}} = \\{0,1,2\\}$， $I_{\\text{B}} = \\{3,4,5\\}$。\n    - 伪计数：$c = 0.5$。\n    - TMM 修剪分数：$p_M = 0.3$, $p_A = 0.05$。\n    - 敏感性阈值：$\\tau = 0.5$。\n\n- 案例 $3$（边缘情况，完全平衡的计数；所有方法应一致，导致零敏感性）：\n    - $G = 5$ 个基因和 $S = 6$ 个样本的计数 $x_{g i}$：\n      - 基因 $0$：$[100,100,100,100,100,100]$\n      - 基因 $1$：$[100,100,100,100,100,100]$\n      - 基因 $2$：$[100,100,100,100,100,100]$\n      - 基因 $3$：$[100,100,100,100,100,100]$\n      - 基因 $4$：$[100,100,100,100,100,100]$\n    - 条件索引集：$I_{\\text{A}} = \\{0,1,2\\}$， $I_{\\text{B}} = \\{3,4,5\\}$。\n    - 伪计数：$c = 0.5$。\n    - TMM 修剪分数：$p_M = 0.3$, $p_A = 0.05$。\n    - 敏感性阈值：$\\tau = 0.01$。\n\n要求的最终输出格式：您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，其中每个元素本身就是一个用方括号括起来、无空格、逗号分隔的从零开始的基因索引列表。例如，一个有效的输出可能看起来像 $[[0,2,5],[1],[0,3]]$；如果一个案例没有敏感基因，则为该案例输出一个空列表 $[]$。", "solution": "该问题被认为是有效的，因为它在科学上基于计算系统生物学的既定原则，具有精确的定义和备用方案，是适定的，并且内部一致。任务是实现三种不同的 RNA-seq 标准化方法——M-值的修剪均值 (TMM)、上四分位数 (UQ) 和来自 DESeq 的比率中位数方案——并量化基因水平效应量估计对方法选择的敏感性。\n\n设给定的计数矩阵为 $X$，其元素为 $x_{g i}$，其中基因 $g \\in \\{0, \\dots, G-1\\}$，样本 $i \\in \\{0, \\dots, S-1\\}$。样本 $i$ 的文库大小为 $L_i = \\sum_{g=0}^{G-1} x_{g i}$。每个样本 $i$ 属于两种条件之一，表示为 $\\text{A}$ 和 $\\text{B}$，具有相应的样本索引集 $I_{\\text{A}}$ 和 $I_{\\text{B}}$。分析的核心是为三种标准化方法中的每一种计算样本特异性的大小因子 $s_i$，然后用这些因子来获得标准化计数 $y_{g i} = x_{g i} / s_i$。\n\n第一种标准化方法是M-值的修剪均值 (TMM)。该方法相对于一个选定的参考样本来估计大小因子。参考样本（索引为 $r$）被选为文库大小 $L_r$ 与所有文库大小的中位数 $\\operatorname{median}\\{L_0, \\dots, L_{S-1}\\}$ 距离最小的那个样本。根据定义，$s_r = 1$。对于所有其他样本 $i$，我们计算每个基因的表达比例的对数比率 $M_{g i} = \\log_2(\\frac{p_{g i}}{p_{g r}})$ 和平均表达强度 $A_{g i} = \\frac{1}{2}\\log_2(p_{g i} p_{g r})$，其中 $p_{g i} = x_{g i}/L_i$ 和 $p_{g r} = x_{g r}/L_r$。这些计算仅限于在样本 $i$ 和参考样本 $r$ 中都具有正计数的基因。通过修剪 $M_{gi}$ 和 $A_{gi}$ 的值来识别一组稳健的基因。具体来说，如果一个基因的 $M_{gi}$ 值落在该样本所有 $M_{gi}$ 值的下部或上部 $p_M$ 分数内，或者其 $A_{gi}$ 值落在所有 $A_{gi}$ 值的下部或上部 $p_A$ 分数内，则该基因被排除。然后，大小因子 $s_i$ 被计算为 $s_i = 2^{\\overline{M}_i}$，其中 $\\overline{M}_i$ 是保留（未修剪）基因的 $M_{gi}$ 值的平均值。如果在修剪后没有基因剩下，我们使用备用方案 $s_i = 1$。\n\n第二种方法是上四分位数 (UQ) 标准化。这是一种更简单的方法，其中每个样本 $i$ 的大小因子 $s_i$ 被设置为其正数计数的第 $75$ 百分位数，即 $s_i = U_i = \\text{percentile}(\\{x_{g i} : x_{g i}  0\\}, 75)$。该方法对少数极高表达的基因具有稳健性。如果一个样本不包含正数计数，其大小因子被设置为默认值 $s_i = 1$。\n\n第三种方法是比率中位数法，这是 DESeq 软件包的一个特点。该方法首先通过计算每个基因 $g$ 在所有样本中的计数的几何平均值来建立一个伪参考样本：$G_g = \\exp(\\frac{1}{S}\\sum_{i=0}^{S-1} \\log x_{g i})$。此计算仅限于在每个样本中都具有严格正数计数的基因。然后，对于每个样本 $i$，大小因子 $s_i$ 被确定为比率 $r_{g i} = x_{g i} / G_g$ 的中位数，该中位数是在用于计算几何平均值的基因集上计算的。定义了一个关键的备用方案：如果没有一个基因在所有样本中都具有正数计数，则通过文库大小标准化来计算大小因子，$s_i = L_i / \\operatorname{median}\\{L_0, \\dots, L_{S-1}\\}$。\n\n一旦为每种方法 $m \\in \\{\\text{TMM}, \\text{UQ}, \\text{DESeq}\\}$ 计算出大小因子 $s^{(m)}_i$，我们就可以确定每个基因的效应量。标准化计数为 $y^{(m)}_{g i} = x_{g i} / s^{(m)}_i$。在每个条件下为每个基因 $g$ 计算这些标准化计数的算术平均值：$\\mu^{(m)}_{g,\\text{A}} = \\frac{1}{|I_{\\text{A}}|}\\sum_{i \\in I_{\\text{A}}} y^{(m)}_{g i}$ 和 $\\mu^{(m)}_{g,\\text{B}} = \\frac{1}{|I_{\\text{B}}|}\\sum_{i \\in I_{\\text{B}}} y^{(m)}_{g i}$。添加一个伪计数 $c  0$ 以处理零值并稳定低计数基因的对数比率。基因 $g$ 在方法 $m$ 下的效应量是以2为底的倍数变化对数：$ES^{(m)}_g = \\log_2((\\mu^{(m)}_{g,\\text{A}} + c) / (\\mu^{(m)}_{g,\\text{B}} + c))$。\n\n最后，为了评估对标准化方法的敏感性，我们为每个基因计算效应量的两两差异，$\\Delta ES_g(m_1,m_2) = ES^{(m_1)}_g - ES^{(m_2)}_g$。基因 $g$ 的总体敏感性被定义为三种方法对之间的最大绝对差异：$S_g = \\max\\{|\\Delta ES_g(\\text{TMM},\\text{UQ})|, |\\Delta ES_g(\\text{TMM},\\text{DESeq})|, |\\Delta ES_g(\\text{UQ},\\text{DESeq})|\\}$。如果一个基因的敏感性得分 $S_g$ 达到或超过给定阈值 $\\tau$，则该基因被分类为敏感基因。最终输出包含每个测试用例的这些敏感基因的索引。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to process all test cases for normalization sensitivity analysis.\n    \"\"\"\n\n    test_cases = [\n        # Case 1 (happy path)\n        {\n            \"counts\": np.array([\n                [50, 55, 52, 49, 53, 50],\n                [100, 95, 105, 280, 300, 320],\n                [200, 210, 190, 80, 75, 85],\n                [0, 5, 0, 30, 35, 40],\n                [400, 420, 410, 390, 380, 400],\n                [10, 12, 11, 9, 8, 10],\n                [8000, 8200, 7800, 7000, 7200, 7100],\n                [2, 1, 0, 3, 4, 0]\n            ], dtype=np.int64),\n            \"cond_a_indices\": [0, 1, 2],\n            \"cond_b_indices\": [3, 4, 5],\n            \"pseudocount\": 0.5,\n            \"p_m\": 0.3,\n            \"p_a\": 0.05,\n            \"tau\": 0.3\n        },\n        # Case 2 (boundary case, many zeros)\n        {\n            \"counts\": np.array([\n                [0, 0, 0, 0, 0, 0],\n                [5, 0, 2, 0, 5, 2],\n                [0, 0, 50, 0, 0, 150],\n                [20, 2000, 25, 10, 900, 20],\n                [30, 1500, 20, 20, 800, 30],\n                [10, 1000, 15, 5, 700, 10]\n            ], dtype=np.int64),\n            \"cond_a_indices\": [0, 1, 2],\n            \"cond_b_indices\": [3, 4, 5],\n            \"pseudocount\": 0.5,\n            \"p_m\": 0.3,\n            \"p_a\": 0.05,\n            \"tau\": 0.5\n        },\n        # Case 3 (edge case, balanced counts)\n        {\n            \"counts\": np.array([\n                [100, 100, 100, 100, 100, 100],\n                [100, 100, 100, 100, 100, 100],\n                [100, 100, 100, 100, 100, 100],\n                [100, 100, 100, 100, 100, 100],\n                [100, 100, 100, 100, 100, 100]\n            ], dtype=np.int64),\n            \"cond_a_indices\": [0, 1, 2],\n            \"cond_b_indices\": [3, 4, 5],\n            \"pseudocount\": 0.5,\n            \"p_m\": 0.3,\n            \"p_a\": 0.05,\n            \"tau\": 0.01\n        }\n    ]\n\n    all_results = []\n    for params in test_cases:\n        counts = params[\"counts\"]\n        cond_a_indices = params[\"cond_a_indices\"]\n        cond_b_indices = params[\"cond_b_indices\"]\n        pseudocount = params[\"pseudocount\"]\n        p_m = params[\"p_m\"]\n        p_a = params[\"p_a\"]\n        tau = params[\"tau\"]\n\n        # Run normalizations\n        s_tmm = normalize_tmm(counts, p_m, p_a)\n        s_uq = normalize_uq(counts)\n        s_deseq = normalize_deseq(counts)\n\n        # Calculate effect sizes\n        es_tmm = calculate_es(counts, s_tmm, cond_a_indices, cond_b_indices, pseudocount)\n        es_uq = calculate_es(counts, s_uq, cond_a_indices, cond_b_indices, pseudocount)\n        es_deseq = calculate_es(counts, s_deseq, cond_a_indices, cond_b_indices, pseudocount)\n\n        # Sensitivity analysis\n        delta_es_tmm_uq = np.abs(es_tmm - es_uq)\n        delta_es_tmm_deseq = np.abs(es_tmm - es_deseq)\n        delta_es_uq_deseq = np.abs(es_uq - es_deseq)\n        \n        sensitivity_scores = np.maximum.reduce([delta_es_tmm_uq, delta_es_tmm_deseq, delta_es_uq_deseq])\n        \n        sensitive_genes = np.where(sensitivity_scores >= tau)[0].tolist()\n        all_results.append(sensitive_genes)\n\n    formatted_results = [f\"[{','.join(map(str, res))}]\" for res in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\ndef normalize_tmm(counts, p_m, p_a):\n    num_genes, num_samples = counts.shape\n    lib_sizes = np.sum(counts, axis=0)\n\n    # Handle all zero lib sizes to avoid division by zero\n    lib_sizes[lib_sizes == 0] = 1\n\n    median_lib_size = np.median(lib_sizes)\n    dists = np.abs(lib_sizes - median_lib_size)\n    ref_idx = np.argmin(dists)\n\n    size_factors = np.ones(num_samples)\n\n    for i in range(num_samples):\n        if i == ref_idx:\n            continue\n\n        x_i = counts[:, i]\n        x_r = counts[:, ref_idx]\n        \n        # Filter for genes with positive counts in both samples\n        valid_genes_mask = (x_i > 0)  (x_r > 0)\n        \n        if not np.any(valid_genes_mask):\n            size_factors[i] = 1.0\n            continue\n            \n        x_i_filt = x_i[valid_genes_mask]\n        x_r_filt = x_r[valid_genes_mask]\n        \n        lib_i = lib_sizes[i]\n        lib_r = lib_sizes[ref_idx]\n\n        p_i = x_i_filt / lib_i\n        p_r = x_r_filt / lib_r\n\n        m_values = np.log2(p_i / p_r)\n        a_values = 0.5 * np.log2(p_i * p_r)\n        \n        # Trim M and A values\n        m_low = np.percentile(m_values, 100 * p_m, interpolation='linear')\n        m_high = np.percentile(m_values, 100 * (1 - p_m), interpolation='linear')\n\n        a_low = np.percentile(a_values, 100 * p_a, interpolation='linear')\n        a_high = np.percentile(a_values, 100 * (1 - p_a), interpolation='linear')\n        \n        retained_mask = (m_values >= m_low)  (m_values = m_high)  \\\n                        (a_values >= a_low)  (a_values = a_high)\n        \n        retained_m_values = m_values[retained_mask]\n\n        if len(retained_m_values) == 0:\n            size_factors[i] = 1.0\n        else:\n            trimmed_mean_m = np.mean(retained_m_values)\n            size_factors[i] = 2**trimmed_mean_m\n            \n    return size_factors\n\ndef normalize_uq(counts):\n    num_samples = counts.shape[1]\n    size_factors = np.ones(num_samples)\n\n    for i in range(num_samples):\n        sample_counts = counts[:, i]\n        positive_counts = sample_counts[sample_counts > 0]\n        \n        if len(positive_counts) == 0:\n            size_factors[i] = 1.0\n        else:\n            uq = np.percentile(positive_counts, 75, interpolation='linear')\n            size_factors[i] = uq if uq > 0 else 1.0\n\n    return size_factors\n\ndef normalize_deseq(counts):\n    num_genes, num_samples = counts.shape\n    \n    # Find genes with strictly positive counts across all samples\n    positive_across_all_mask = np.all(counts > 0, axis=1)\n\n    if not np.any(positive_across_all_mask):\n        # Fallback: library size normalization\n        lib_sizes = np.sum(counts, axis=0)\n        # Avoid division by zero for all-zero library or median of zero\n        lib_sizes[lib_sizes == 0] = 1\n        median_lib = np.median(lib_sizes)\n        if median_lib == 0:\n            median_lib = 1\n        return lib_sizes / median_lib\n\n    # Main path\n    counts_for_geo_mean = counts[positive_across_all_mask, :]\n    \n    with np.errstate(divide='ignore'):\n        log_counts = np.log(counts_for_geo_mean)\n    \n    geo_means = np.exp(np.mean(log_counts, axis=1))\n\n    ratios = counts_for_geo_mean / geo_means[:, np.newaxis]\n    size_factors = np.median(ratios, axis=0)\n    \n    return size_factors\n\ndef calculate_es(counts, size_factors, cond_a_indices, cond_b_indices, pseudocount):\n    # Normalize counts\n    norm_counts = counts / size_factors[np.newaxis, :]\n    \n    # Group means\n    mean_a = np.mean(norm_counts[:, cond_a_indices], axis=1)\n    mean_b = np.mean(norm_counts[:, cond_b_indices], axis=1)\n    \n    # Effect size\n    ratio = (mean_a + pseudocount) / (mean_b + pseudocount)\n    effect_sizes = np.log2(ratio)\n    \n    return effect_sizes\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3301660"}, {"introduction": "在基因表达研究中，非生物学因素导致的系统性偏差，如实验室间的“批次效应”，常常会掩盖真实的生物学信号，对结果的可重复性构成严峻挑战。本练习介绍了一种利用外部“峰入”（spike-in）标准品来校正此类偏差的强大技术。你将利用这些已知倍比变化的合成RNA分子，估计并校正由不同实验室引入的系统性测量偏差，从而提升效应量估计的准确性和跨实验可比性。这项实践将锻炼你整合多来源数据、确保研究结果稳健性的关键能力。[@problem_id:3301688]", "problem": "给定您在两个独立实验室中对信使RNA (mRNA) 基因和外部spike-in标准品进行的重复计数测量。在每个实验室中，都对一个对照条件和一个处理条件进行了测定。请假设以下适用于差异基因表达分析和效应量评估的基础模型和定义：\n\n- 基因的对数倍数变化 (Log Fold Change, LFC) 定义为 $LFC = \\log_{2}\\left(\\dfrac{\\bar{T} + \\epsilon}{\\bar{C} + \\epsilon}\\right)$，其中 $\\bar{T}$ 是处理组重复样本的算术平均值，$\\bar{C}$ 是对照组重复样本的算术平均值，$\\epsilon$ 是一个小的正伪计数，以确保在出现零计数时数值的稳定性。\n- Spike-in标准品是以处理组和对照组之间已知的比例添加的合成分子。它们的真实对数倍数变化（表示为 $LFC^{true}$）是先验已知的。\n- 实验室特异性的乘性尺度偏差被建模为 $LFC^{obs} \\approx \\kappa \\cdot LFC^{true}$（针对spike-in），其中 $LFC^{obs}$ 是观测到的spike-in对数倍数变化，$\\kappa$ 是一个未知的、实验室特异性的缩放因子。\n- 校准规则为 $LFC^{cal} = \\dfrac{LFC^{obs}}{\\hat{\\kappa}}$，其中 $\\hat{\\kappa}$ 是从spike-in中导出的 $\\kappa$ 的估计值。\n\n从这个基础出发，为每个测试案例实施以下步骤：\n\n1. 对每个实验室：\n   a. 使用给定的重复计数值和指定的 $\\epsilon$，为每个spike-in计算 $LFC^{obs}$。\n   b. 通过最小化 $\\sum_{i}\\left(LFC^{obs}_{i} - \\hat{\\kappa} \\cdot LFC^{true}_{i}\\right)^{2}$ 的通过原点回归模型，使用最小二乘法估计 $\\hat{\\kappa}$。这会得到\n   $$\\hat{\\kappa} = \\dfrac{\\sum_{i} LFC^{true}_{i} \\cdot LFC^{obs}_{i}}{\\sum_{i} \\left(LFC^{true}_{i}\\right)^{2}}.$$\n   c. 计算每个基因的 $LFC^{obs}$。\n   d. 通过 $LFC^{cal} = \\dfrac{LFC^{obs}}{\\hat{\\kappa}}$ 校准基因的对数倍数变化。\n\n2. 在每个实验室内，分别按 $LFC^{obs}$ 和 $LFC^{cal}$ 的绝对值大小对基因进行排序。将排序函数定义为按绝对值降序排序，若绝对值相同，则按基因名称的字典升序排序。确定校准后，实验室内是否有任何基因排序发生变化。变化指示符应为布尔值。\n\n3. 使用皮尔逊相关系数，量化校准前后基因对数倍数变化（带符号）的跨实验室可重复性。设 $x_{j}$ 为实验室A中基因的 $LFC^{obs}$ 向量， $y_{j}$ 为实验室B中对应相同基因和顺序的向量。计算\n   $$r_{\\text{pre}} = \\dfrac{\\operatorname{Cov}(x, y)}{\\sigma_{x}\\sigma_{y}}$$\n   和\n   $$r_{\\text{post}} = \\dfrac{\\operatorname{Cov}\\left(x^{cal}, y^{cal}\\right)}{\\sigma_{x^{cal}}\\sigma_{y^{cal}}},$$\n   其中 $\\operatorname{Cov}$ 表示协方差，$\\sigma$ 表示标准差。将改善量报告为 $\\Delta r = r_{\\text{post}} - r_{\\text{pre}}$。\n\n4. 您的程序应为下面提供的测试套件实现上述计算。伪计数必须为 $\\epsilon = 0.5$。无需报告物理单位，也不涉及角度。所有小数输出必须为十进制形式。\n\n测试套件（三个案例）：\n\n- 案例1（理想情况：实验室间存在显著偏差差异，无零计数）：\n  - 实验室：Lab1 和 Lab2\n  - 基因与重复样本：\n    - 对照组重复样本（各实验室共用）：\n      - $G1$: $[190, 210]$, $G2$: $[175, 185]$, $G3$: $[145, 155]$, $G4$: $[215, 225]$, $G5$: $[95, 105]$, $G6$: $[295, 305]$, $G7$: $[75, 85]$, $G8$: $[390, 410]$\n    - Lab1处理组重复样本：\n      - $G1$: $[850, 865]$, $G2$: $[470, 480]$, $G3$: $[90, 95]$, $G4$: $[215, 225]$, $G5$: $[30, 32]$, $G6$: $[580, 595]$, $G7$: $[11, 12]$, $G8$: $[530, 540]$\n    - Lab2处理组重复样本：\n      - $G1$: $[455, 465]$, $G2$: $[310, 315]$, $G3$: $[112, 115]$, $G4$: $[215, 225]$, $G5$: $[50, 53]$, $G6$: $[440, 445]$, $G7$: $[26, 27]$, $G8$: $[470, 475]$\n  - Spike-in：\n    - 对照组重复样本（各实验室共用）：$S1$: $[98, 102]$, $S2$: $[98, 102]$, $S3$: $[98, 102]$\n    - Lab1处理组重复样本：$S1$: $[38, 38]$, $S2$: $[98, 102]$, $S3$: $[260, 268]$\n    - Lab2处理组重复样本：$S1$: $[57, 58]$, $S2$: $[98, 102]$, $S3$: $[173, 175]$\n    - Spike-in真实对数倍数变化：$LFC^{true}(S1) = -1$, $LFC^{true}(S2) = 0$, $LFC^{true}(S3) = +1$\n\n- 案例2（边界条件：轻微偏差差异，效应不显著）：\n  - 实验室：LabA 和 LabB\n  - 基因与重复样本：\n    - 对照组重复样本（各实验室共用）：\n      - $G1$: $[145, 155]$, $G2$: $[190, 210]$, $G3$: $[175, 185]$, $G4$: $[95, 105]$, $G5$: $[215, 225]$\n    - LabA处理组重复样本：\n      - $G1$: $[214, 218]$, $G2$: $[137, 140]$, $G3$: $[178, 182]$, $G4$: $[205, 208]$, $G5$: $[105, 108]$\n    - LabB处理组重复样本：\n      - $G1$: $[207, 210]$, $G2$: $[142, 145]$, $G3$: $[177, 183]$, $G4$: $[192, 195]$, $G5$: $[113, 115]$\n  - Spike-in：\n    - 对照组重复样本（各实验室共用）：$S1$: $[98, 102]$, $S2$: $[98, 102]$, $S3$: $[98, 102]$\n    - LabA处理组重复样本：$S1$: $[48, 49]$, $S2$: $[98, 102]$, $S3$: $[206, 209]$\n    - LabB处理组重复样本：$S1$: $[51, 53]$, $S2$: $[98, 102]$, $S3$: $[192, 194]$\n    - Spike-in真实对数倍数变化：$LFC^{true}(S1) = -1$, $LFC^{true}(S2) = 0$, $LFC^{true}(S3) = +1$\n\n- 案例3（边缘案例：一个重复样本中存在零计数，偏差差异较大）：\n  - 实验室：LabA 和 LabB\n  - 基因与重复样本：\n    - LabA对照组重复样本：\n      - $G1$: $[115, 125]$, $G2$: $[58, 62]$, $G3$: $[195, 205]$, $G4$: $[78, 82]$, $G5$: $[148, 152]$\n    - LabA处理组重复样本：\n      - $G1$: $[292, 298]$, $G2$: $[8, 10]$, $G3$: $[260, 265]$, $G4$: $[42, 44]$, $G5$: $[148, 152]$\n    - LabB对照组重复样本：\n      - $G1$: $[115, 125]$, $G2$: $[0, 60]$, $G3$: $[195, 205]$, $G4$: $[78, 82]$, $G5$: $[148, 152]$\n    - LabB处理组重复样本：\n      - $G1$: $[193, 197]$, $G2$: $[11, 12]$, $G3$: $[230, 234]$, $G4$: $[56, 58]$, $G5$: $[148, 152]$\n  - Spike-in：\n    - 对照组重复样本（各实验室共用）：$S1$: $[98, 102]$, $S2$: $[98, 102]$, $S3$: $[98, 102]$\n    - LabA处理组重复样本：$S1$: $[40, 41]$, $S2$: $[98, 102]$, $S3$: $[244, 248]$\n    - LabB处理组重复样本：$S1$: $[61, 62]$, $S2$: $[98, 102]$, $S3$: $[161, 163]$\n    - Spike-in真实对数倍数变化：$LFC^{true}(S1) = -1$, $LFC^{true}(S2) = 0$, $LFC^{true}(S3) = +1$\n\n你的程序必须输出单行内容，其中包含按顺序列出的三个案例的结果。对于每个案例，返回一个包含两个元素的列表 $[\\Delta r, \\text{changed}]$，其中 $\\Delta r$ 是一个浮点数（皮尔逊相关系数的改善量，$r_{\\text{post}} - r_{\\text{pre}}$），$\\text{changed}$ 是一个布尔值，指示在任一实验室内，基于绝对LFC的基因排序是否因校准而改变（该值是案例中两个实验室结果的聚合）。最终输出格式必须是形如 \"[[delta_r_case1,changed_case1],[delta_r_case2,changed_case2],[delta_r_case3,changed_case3]]\" 的单行字符串。", "solution": "问题陈述已经过仔细审查，并被确认为有效。它在科学上基于计算系统生物学的原理，特别是关于差异基因表达数据的归一化。该问题定义明确，提供了一套完整且一致的定义、数据和计算步骤。所有术语都得到了正式定义，且目标是可量化的。\n\n任务是实现一个计算工作流程，使用外部spike-in标准品来分析和校正mRNA计数数据中的实验室间偏差。这涉及几个步骤：计算对数倍数变化 (LFCs)，估计实验室特异性的偏差缩放因子 ($\\kappa$)，校准观测到的基因LFCs，并评估此校准对基因排序和跨实验室可重复性的影响。\n\n基础常数和公式如下：\n- 用于处理零计数的伪计数为 $\\epsilon = 0.5$。\n- 每个基因或spike-in的观测对数倍数变化 ($LFC^{obs}$) 定义为：\n$$LFC = \\log_{2}\\left(\\dfrac{\\bar{T} + \\epsilon}{\\bar{C} + \\epsilon}\\right)$$\n其中 $\\bar{T}$ 和 $\\bar{C}$ 分别是处理组和对照组重复样本计数的算术平均值。\n\n分析的核心按指定的三个阶段进行。\n\n**1. 实验室内数据处理与校准**\n\n对测试案例中提供的每个实验室数据集，应用以下程序：\n\na. **Spike-in LFC计算**：使用每个spike-in标准品的对照组和处理组重复计数值及上述公式，计算其 $LFC^{obs}$。\n\nb. **偏差因子估计 ($\\hat{\\kappa}$)**：问题假定观测到的spike-in LFCs ($LFC^{obs}_i$) 与其已知的真实LFCs ($LFC^{true}_i$) 之间存在线性关系，模型为 $LFC^{obs}_i \\approx \\kappa \\cdot LFC^{true}_i$。实验室特异性缩放因子 $\\kappa$ 通过通过原点的回归最小二乘法进行估计。目标是找到最小化残差平方和 $\\sum_{i}\\left(LFC^{obs}_{i} - \\hat{\\kappa} \\cdot LFC^{true}_{i}\\right)^{2}$ 的 $\\hat{\\kappa}$。该最小化问题的解由下式给出：\n$$\\hat{\\kappa} = \\dfrac{\\sum_{i} LFC^{true}_{i} \\cdot LFC^{obs}_{i}}{\\sum_{i} \\left(LFC^{true}_{i}\\right)^{2}}$$\n这个估计量 $\\hat{\\kappa}$ 代表了实验室测量过程在对数倍数变化尺度上的系统性乘性偏差。由于提供的 $LFC^{true}$ 值为 $[-1, 0, 1]$，平方和为 $2$，因此分母不为零。\n\nc. **基因LFC计算**：使用每个内源基因各自的重复计数值，计算其 $LFC^{obs}$。\n\nd. **基因LFC校准**：校准观测到的基因LFCs以纠正估计的偏差。校准后的LFC ($LFC^{cal}$) 通过将观测到的LFC除以估计的缩放因子计算得出：\n$$LFC^{cal} = \\dfrac{LFC^{obs}}{\\hat{\\kappa}}$$\n此过程将观测到的效应重新缩放，使其达到在无偏差测量系统（其中$\\kappa=1$）中应有的水平。\n\n**2. 实验室内排序稳定性分析**\n\n为了评估校准对单个实验室内基因表达变化解释的影响，比较了校准前后基因的相对顺序。基因根据其LFC值的绝对值大小进行排序，因为这个指标常用于优先选择基因进行深入研究。具体的排序规则是：\n- 主排序键：按LFC绝对值 ($|LFC|$) 降序排列。\n- 次排序键（决胜局）：按基因名称的字典升序排列。\n\n为每个实验室生成两个基因名称的排序列表：一个使用 $|LFC^{obs}|$，另一个使用 $|LFC^{cal}|$。比较这两个列表。如果它们不完全相同，则表明校准过程改变了至少一个基因的相对重要性，该实验室的变化布尔标志被设为真。对于每个测试案例，最终的变化指示符是两个实验室标志的逻辑或运算结果。\n\n**3. 跨实验室可重复性评估**\n\n归一化的一个关键目标是提高独立实验之间结果的一致性。这通过测量校准前后基因LFCs的跨实验室一致性来量化。为此，使用皮尔逊相关系数。\n\n设 $x$ 和 $y$ 分别为来自实验室A和实验室B的所有基因的 $LFC^{obs}$ 向量，并保持一致的基因顺序。校准前的可重复性 $r_{\\text{pre}}$ 为：\n$$r_{\\text{pre}} = \\dfrac{\\operatorname{Cov}(x, y)}{\\sigma_{x}\\sigma_{y}}$$\n其中 $\\operatorname{Cov}(x, y)$ 是 $x$ 和 $y$ 的协方差，$\\sigma_{x}$ 和 $\\sigma_{y}$ 是它们的标准差。\n\n类似地，设 $x^{cal}$ 和 $y^{cal}$ 为 $LFC^{cal}$ 值的向量。校准后的可重复性 $r_{\\text{post}}$ 为：\n$$r_{\\text{post}} = \\dfrac{\\operatorname{Cov}\\left(x^{cal}, y^{cal}\\right)}{\\sigma_{x^{cal}}\\sigma_{y^{cal}}}$$\n\n可重复性的改善量报告为差值 $\\Delta r = r_{\\text{post}} - r_{\\text{pre}}$。一个正的 $\\Delta r$ 表示spike-in校准使得两个实验室的基因LFC测量结果更加一致。\n\n所描述的方法将被系统地应用于三个测试案例中的每一个，以生成所需的输出。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to orchestrate the analysis for all test cases.\n    \"\"\"\n    epsilon = 0.5\n\n    # Define the test cases from the problem statement.\n    gene_control_c1 = np.array([[190, 210], [175, 185], [145, 155], [215, 225], [95, 105], [295, 305], [75, 85], [390, 410]], dtype=float)\n    spike_control_c1 = np.array([[98, 102], [98, 102], [98, 102]], dtype=float)\n\n    gene_control_c2 = np.array([[145, 155], [190, 210], [175, 185], [95, 105], [215, 225]], dtype=float)\n    spike_control_c2 = np.array([[98, 102], [98, 102], [98, 102]], dtype=float)\n\n    test_cases = [\n        { # Case 1\n            \"gene_names\": [f\"G{i}\" for i in range(1, 9)],\n            \"spike_lfc_true\": np.array([-1.0, 0.0, 1.0]),\n            \"labs\": {\n                \"Lab1\": {\n                    \"gene_c\": gene_control_c1,\n                    \"gene_t\": np.array([[850, 865], [470, 480], [90, 95], [215, 225], [30, 32], [580, 595], [11, 12], [530, 540]], dtype=float),\n                    \"spike_c\": spike_control_c1,\n                    \"spike_t\": np.array([[38, 38], [98, 102], [260, 268]], dtype=float)\n                },\n                \"Lab2\": {\n                    \"gene_c\": gene_control_c1,\n                    \"gene_t\": np.array([[455, 465], [310, 315], [112, 115], [215, 225], [50, 53], [440, 445], [26, 27], [470, 475]], dtype=float),\n                    \"spike_c\": spike_control_c1,\n                    \"spike_t\": np.array([[57, 58], [98, 102], [173, 175]], dtype=float)\n                }\n            }\n        },\n        { # Case 2\n            \"gene_names\": [f\"G{i}\" for i in range(1, 6)],\n            \"spike_lfc_true\": np.array([-1.0, 0.0, 1.0]),\n            \"labs\": {\n                \"LabA\": {\n                    \"gene_c\": gene_control_c2,\n                    \"gene_t\": np.array([[214, 218], [137, 140], [178, 182], [205, 208], [105, 108]], dtype=float),\n                    \"spike_c\": spike_control_c2,\n                    \"spike_t\": np.array([[48, 49], [98, 102], [206, 209]], dtype=float)\n                },\n                \"LabB\": {\n                    \"gene_c\": gene_control_c2,\n                    \"gene_t\": np.array([[207, 210], [142, 145], [177, 183], [192, 195], [113, 115]], dtype=float),\n                    \"spike_c\": spike_control_c2,\n                    \"spike_t\": np.array([[51, 53], [98, 102], [192, 194]], dtype=float)\n                }\n            }\n        },\n        { # Case 3\n            \"gene_names\": [f\"G{i}\" for i in range(1, 6)],\n            \"spike_lfc_true\": np.array([-1.0, 0.0, 1.0]),\n            \"labs\": {\n                \"LabA\": {\n                    \"gene_c\": np.array([[115, 125], [58, 62], [195, 205], [78, 82], [148, 152]], dtype=float),\n                    \"gene_t\": np.array([[292, 298], [8, 10], [260, 265], [42, 44], [148, 152]], dtype=float),\n                    \"spike_c\": np.array([[98, 102], [98, 102], [98, 102]], dtype=float),\n                    \"spike_t\": np.array([[40, 41], [98, 102], [244, 248]], dtype=float)\n                },\n                \"LabB\": {\n                    \"gene_c\": np.array([[115, 125], [0, 60], [195, 205], [78, 82], [148, 152]], dtype=float),\n                    \"gene_t\": np.array([[193, 197], [11, 12], [230, 234], [56, 58], [148, 152]], dtype=float),\n                    \"spike_c\": np.array([[98, 102], [98, 102], [98, 102]], dtype=float),\n                    \"spike_t\": np.array([[61, 62], [98, 102], [161, 163]], dtype=float)\n                }\n            }\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        case_result = _process_case(case, epsilon)\n        results.append(case_result)\n\n    # The required output format is a \"stringified\" list of lists with no spaces.\n    print(str(results).replace(' ', ''))\n\ndef _calculate_lfc(treatment_reps, control_reps, epsilon):\n    \"\"\"Computes log2 fold change with a pseudocount.\"\"\"\n    mean_t = np.mean(treatment_reps, axis=1)\n    mean_c = np.mean(control_reps, axis=1)\n    # Add epsilon to numerator and denominator to prevent log(0) and division by zero.\n    return np.log2((mean_t + epsilon) / (mean_c + epsilon))\n\ndef _process_lab(gene_names, lab_data, spike_lfc_true, epsilon):\n    \"\"\"Processes data for a single lab to get LFCs and ranking change.\"\"\"\n    # Step 1a: Compute observed LFC for spike-ins\n    spike_lfc_obs = _calculate_lfc(lab_data[\"spike_t\"], lab_data[\"spike_c\"], epsilon)\n    \n    # Step 1b: Estimate kappa\n    kappa_num = np.sum(spike_lfc_true * spike_lfc_obs)\n    kappa_den = np.sum(spike_lfc_true**2)\n    kappa_hat = kappa_num / kappa_den\n    \n    # Step 1c: Compute observed LFC for genes\n    gene_lfc_obs = _calculate_lfc(lab_data[\"gene_t\"], lab_data[\"gene_c\"], epsilon)\n    \n    # Step 1d: Calibrate gene LFCs\n    gene_lfc_cal = gene_lfc_obs / kappa_hat\n    \n    # Step 2: Determine if gene ranking changes\n    # Rank by descending absolute value, tie-break by ascending gene name\n    lfc_obs_with_names = list(zip(gene_names, gene_lfc_obs))\n    lfc_cal_with_names = list(zip(gene_names, gene_lfc_cal))\n    \n    # Create the ranking key: a tuple of (-abs(LFC), gene_name)\n    rank_obs = sorted(lfc_obs_with_names, key=lambda x: (-abs(x[1]), x[0]))\n    rank_cal = sorted(lfc_cal_with_names, key=lambda x: (-abs(x[1]), x[0]))\n    \n    # Extract just the gene names to check for ranking changes\n    ranked_names_obs = [item[0] for item in rank_obs]\n    ranked_names_cal = [item[0] for item in rank_cal]\n    \n    ranking_changed = (ranked_names_obs != ranked_names_cal)\n    \n    return gene_lfc_obs, gene_lfc_cal, ranking_changed\n\ndef _process_case(case_data, epsilon):\n    \"\"\"Processes a full test case with two labs.\"\"\"\n    lab_names = list(case_data[\"labs\"].keys())\n    lab_a_name, lab_b_name = lab_names[0], lab_names[1]\n\n    gene_names = case_data[\"gene_names\"]\n    spike_lfc_true = case_data[\"spike_lfc_true\"]\n\n    data_a = case_data[\"labs\"][lab_a_name]\n    lfc_obs_a, lfc_cal_a, changed_a = _process_lab(gene_names, data_a, spike_lfc_true, epsilon)\n    \n    data_b = case_data[\"labs\"][lab_b_name]\n    lfc_obs_b, lfc_cal_b, changed_b = _process_lab(gene_names, data_b, spike_lfc_true, epsilon)\n    \n    # Step 2 (final): Aggregate ranking change flag\n    case_changed = changed_a or changed_b\n    \n    # Step 3: Quantify cross-lab reproducibility improvement\n    # Using np.corrcoef which returns a 2x2 matrix. The value at [0,1] is the Pearson r.\n    r_pre = np.corrcoef(lfc_obs_a, lfc_obs_b)[0, 1]\n    r_post = np.corrcoef(lfc_cal_a, lfc_cal_b)[0, 1]\n    \n    delta_r = r_post - r_pre\n    \n    return [delta_r, case_changed]\n\nsolve()\n\n```", "id": "3301688"}]}