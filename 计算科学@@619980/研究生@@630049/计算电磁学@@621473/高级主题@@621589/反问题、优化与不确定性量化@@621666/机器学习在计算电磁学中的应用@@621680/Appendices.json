{"hands_on_practices": [{"introduction": "在将机器学习应用于基于物理的逆问题时，一个核心挑战是高效地计算损失函数相对于物理参数的梯度。本练习将引导您掌握伴随状态法（adjoint-state method），这是一种用于计算此类梯度的强大而优雅的数学工具。通过推导亥姆霍兹方程（Helmholtz equation）的伴随问题，您将深入理解如何将数据失配与控制物理系统的偏微分方程参数联系起来，这是构建和训练物理信息神经网络（PINN）及其他梯度优化模型的关键技能。[@problem_id:3327841]", "problem": "考虑一个反散射设置，其中有界域 $ \\Omega \\subset \\mathbb{R}^{3} $ 内空间变化的电容率 $ \\epsilon(\\mathbf{r}) $ 将通过一个由数据失配最小化驱动的机器学习模型来推断。计算物理正演模型是针对角频率为 $ \\omega $ 的时谐场 $ u(\\mathbf{r}) $ 的标量亥姆霍兹方程，其中磁导率 $ \\mu $ 和电容率 $ \\epsilon(\\mathbf{r}) $ 均为实值正数。对于每个由 $ m = 1, \\dots, N_{s} $ 索引的实验，正演场 $ u_{m}(\\mathbf{r}) $ 满足\n$$\n\\left(\\nabla^{2} + \\omega^{2} \\mu \\epsilon(\\mathbf{r})\\right) u_{m}(\\mathbf{r}) = s_{m}(\\mathbf{r}) \\quad \\text{in } \\Omega,\n$$\n其中 $ s_{m}(\\mathbf{r}) $ 是一个已知的源分布。假设在 $ \\partial \\Omega $ 上有均匀吸收边界条件，该条件强制施加一个适定的辐射条件，并使伴随态推导中的边界项消失。测量值在接收器位置 $ \\{\\mathbf{r}_{j}\\}_{j=1}^{N_{r}} $ 通过一个线性采样算子 $ \\mathcal{C} $ 进行采集，该算子定义为\n$$\n\\mathcal{C} u_{m} = \\left(u_{m}(\\mathbf{r}_{1}), \\dots, u_{m}(\\mathbf{r}_{N_{r}})\\right)^{\\top} \\in \\mathbb{C}^{N_{r}}.\n$$\n给定复值观测数据向量 $ \\mathbf{d}_{m} \\in \\mathbb{C}^{N_{r}} $，数据失配泛函为\n$$\nJ(\\epsilon) = \\frac{1}{2} \\sum_{m=1}^{N_{s}} \\left\\| \\mathcal{C} u_{m} - \\mathbf{d}_{m} \\right\\|_{2}^{2},\n$$\n其中 $ \\|\\cdot\\|_{2} $ 表示由 $ \\mathbb{C}^{N_{r}} $ 上的埃尔米特内积导出的欧几里得范数。在机器学习框架中，训练目标是关于场参数 $ \\epsilon(\\mathbf{r}) $ 最小化 $ J(\\epsilon) $。\n\n使用伴随态方法，在 $ \\mu $ 和 $ \\omega $ 是固定实数的假设下，推导泛函梯度 $ \\nabla_{\\epsilon} J(\\mathbf{r}) $ 关于 $ \\epsilon(\\mathbf{r}) $ 的表达式。在推导过程中，从标量亥姆霍兹方程和 $ J(\\epsilon) $ 的定义出发，并且只使用与给定边界条件一致的分部积分和伴随算子恒等式。明确写出正演和伴随场方程，包括它们的右端项。将最终梯度表示为关于正演场 $ u_{m} $ 和伴随场 $ p_{m} $ 的单一闭式解析表达式。\n\n你的最终答案必须是单一的闭式解析表达式。最终答案中不要包含单位。如果引入任何角度，必须以弧度为单位。最终的解析表达式不需要进行舍入。", "solution": "该问题要求推导数据失配泛函 $J(\\epsilon)$ 关于空间变化的电容率 $ \\epsilon(\\mathbf{r}) $ 的泛函梯度。伴随态方法是为此推导指定的技术。泛函梯度，记为 $ \\nabla_{\\epsilon} J(\\mathbf{r}) $，由泛函 $J$ 的一阶变分定义，即对于一个微小扰动 $ \\delta\\epsilon(\\mathbf{r}) $，$J$ 的变化由下式给出：\n$$ \\delta J = J(\\epsilon + \\delta\\epsilon) - J(\\epsilon) = \\int_{\\Omega} \\nabla_{\\epsilon} J(\\mathbf{r}) \\, \\delta\\epsilon(\\mathbf{r}) \\, d\\mathbf{r} + O(\\|\\delta\\epsilon \\|^2) $$\n我们的目标是通过将扰动 $ \\delta\\epsilon $ 与其引起的变化 $ \\delta J $ 相关联，来找到 $ \\nabla_{\\epsilon} J(\\mathbf{r}) $ 的表达式。\n\n失配泛函 $J(\\epsilon)$ 由下式给出：\n$$ J(\\epsilon) = \\frac{1}{2} \\sum_{m=1}^{N_{s}} \\left\\| \\mathcal{C} u_{m} - \\mathbf{d}_{m} \\right\\|_{2}^{2} $$\n其中 $u_m$ 通过标量亥姆霍兹方程隐式地依赖于 $\\epsilon$。该范数是 $ \\mathbb{C}^{N_{r}} $ 上的欧几里得范数，由埃尔米特内积 $ \\|\\mathbf{v}\\|_{2}^{2} = \\mathbf{v}^{H}\\mathbf{v} $ 导出。因此，我们可以将 $J(\\epsilon)$ 写为：\n$$ J(\\epsilon) = \\frac{1}{2} \\sum_{m=1}^{N_{s}} (\\mathcal{C} u_{m} - \\mathbf{d}_{m})^{H} (\\mathcal{C} u_{m} - \\mathbf{d}_{m}) $$\n\n首先，我们计算 $J$ 关于场 $ u_m $ 的一阶变分。一个微小的扰动 $ \\epsilon \\rightarrow \\epsilon + \\delta\\epsilon $ 会引起场的扰动 $ u_m \\rightarrow u_m + \\delta u_m $。$J$ 中相应的一阶变分为：\n$$ \\delta J = \\frac{1}{2} \\sum_{m=1}^{N_s} \\left[ (\\mathcal{C}\\delta u_m)^H (\\mathcal{C}u_m - \\mathbf{d}_m) + (\\mathcal{C}u_m - \\mathbf{d}_m)^H (\\mathcal{C}\\delta u_m) \\right] $$\n方括号中的两项互为复共轭。因此，它们的和为 $2 \\text{Re} [(\\mathcal{C}u_m - \\mathbf{d}_m)^H (\\mathcal{C}\\delta u_m)] $。\n$$ \\delta J = \\sum_{m=1}^{N_s} \\text{Re} \\left[ (\\mathcal{C}u_m - \\mathbf{d}_m)^H (\\mathcal{C}\\delta u_m) \\right] $$\n这个表达式可以用测量算子 $ \\mathcal{C} $ 的定义来写。令 $ (\\mathcal{C}u_m)_j = u_m(\\mathbf{r}_j) $。令 $ (\\mathbf{d}_m)_j = d_{mj} $。\n$$ \\delta J = \\sum_{m=1}^{N_s} \\text{Re} \\left[ \\sum_{j=1}^{N_r} \\overline{(u_m(\\mathbf{r}_j) - d_{mj})} \\, \\delta u_m(\\mathbf{r}_j) \\right] $$\n\n接下来，我们将场扰动 $ \\delta u_m $ 与参数扰动 $ \\delta\\epsilon $ 联系起来。正演场 $ u_m $ 满足状态方程：\n$$ L_\\epsilon u_m(\\mathbf{r}) \\equiv \\left(\\nabla^{2} + \\omega^{2} \\mu \\epsilon(\\mathbf{r})\\right) u_{m}(\\mathbf{r}) = s_{m}(\\mathbf{r}) $$\n受扰场 $ u_m + \\delta u_m $ 满足具有受扰参数 $ \\epsilon + \\delta\\epsilon $ 的相同方程：\n$$ \\left(\\nabla^{2} + \\omega^{2} \\mu (\\epsilon(\\mathbf{r}) + \\delta\\epsilon(\\mathbf{r}))\\right) (u_{m}(\\mathbf{r}) + \\delta u_m(\\mathbf{r})) = s_{m}(\\mathbf{r}) $$\n展开此方程并只保留 $ \\delta\\epsilon $ 和 $ \\delta u_m $ 的一阶项：\n$$ (\\nabla^{2} + \\omega^{2} \\mu \\epsilon) u_m + (\\nabla^{2} + \\omega^{2} \\mu \\epsilon) \\delta u_m + (\\omega^{2} \\mu \\delta\\epsilon) u_m + O(\\delta\\epsilon \\delta u_m) = s_m $$\n减去原始状态方程 $ (\\nabla^{2} + \\omega^{2} \\mu \\epsilon) u_m = s_m $，我们得到线性化正演方程，也称为灵敏度方程：\n$$ L_\\epsilon(\\delta u_m) = (\\nabla^{2} + \\omega^{2} \\mu \\epsilon) \\delta u_m = -\\omega^2 \\mu \\delta\\epsilon \\, u_m $$\n\n现在我们引入伴随态方法，从 $ \\delta J $ 的表达式中消去 $ \\delta u_m $。我们为每个实验 $m$ 定义一个伴随场 $ p_m(\\mathbf{r}) $。我们使用复值函数的标准 $ L^2(\\Omega) $ 内积，$ \\langle f, g \\rangle = \\int_{\\Omega} f(\\mathbf{r})\\overline{g(\\mathbf{r})} \\, d\\mathbf{r} $。算子 $ A $ 的伴随算子记为 $ A^\\dagger $，并满足 $ \\langle A f, g \\rangle = \\langle f, A^\\dagger g \\rangle $。算子 $ L_\\epsilon = \\nabla^2 + \\omega^2\\mu\\epsilon $ 是自伴的，即 $ L_\\epsilon^\\dagger = L_\\epsilon $，因为 $ \\omega, \\mu, \\epsilon $ 都是实数，并且问题陈述保证了分部积分产生的边界项会消失。\n\n我们可以通过引入狄拉克delta函数，使用在 $ \\Omega $ 上的积分来表示 $ \\delta J $：\n$$ \\delta J = \\sum_{m=1}^{N_s} \\text{Re} \\left[ \\int_{\\Omega} \\delta u_m(\\mathbf{r}) \\left( \\sum_{j=1}^{N_r} \\overline{(u_m(\\mathbf{r}_j) - d_{mj})} \\delta(\\mathbf{r} - \\mathbf{r}_j) \\right) d\\mathbf{r} \\right] $$\n这可以以内积的形式写为：\n$$ \\delta J = \\sum_{m=1}^{N_s} \\text{Re} \\langle \\delta u_m, q_m \\rangle $$\n其中 $ q_m(\\mathbf{r}) $ 是第 $m$ 个实验的伴随源：\n$$ q_m(\\mathbf{r}) = \\sum_{j=1}^{N_r} \\overline{(u_m(\\mathbf{r}_j) - d_{mj})} \\delta(\\mathbf{r} - \\mathbf{r}_j) $$\n根据灵敏度方程，我们可以写出 $ \\delta u_m = -L_\\epsilon^{-1}(\\omega^2 \\mu \\delta\\epsilon \\, u_m) $。将此代入 $ \\delta J $ 的表达式：\n$$ \\delta J = \\sum_{m=1}^{N_s} \\text{Re} \\langle -L_\\epsilon^{-1}(\\omega^2 \\mu \\delta\\epsilon \\, u_m), q_m \\rangle $$\n利用伴随算子的性质 $ \\langle A f, g \\rangle = \\langle f, A^\\dagger g \\rangle $，我们将 $ L_\\epsilon^{-1} $ 移到内积的另一边：\n$$ \\delta J = \\sum_{m=1}^{N_s} \\text{Re} \\langle \\omega^2 \\mu \\delta\\epsilon \\, u_m, -(L_\\epsilon^{-1})^\\dagger q_m \\rangle $$\n我们现在将伴随场 $ p_m $ 定义为 $ p_m = (L_\\epsilon^{-1})^\\dagger q_m $ 的解。这等价于求解伴随方程 $ L_\\epsilon^\\dagger p_m = q_m $。由于 $ L_\\epsilon^\\dagger = L_\\epsilon $，伴随问题为：\n$$ L_\\epsilon p_m(\\mathbf{r}) = (\\nabla^{2} + \\omega^{2} \\mu \\epsilon(\\mathbf{r})) p_{m}(\\mathbf{r}) = q_m(\\mathbf{r}) $$\n将 $ p_m $ 代回 $ \\delta J $ 的表达式：\n$$ \\delta J = \\sum_{m=1}^{N_s} \\text{Re} \\langle \\omega^2 \\mu \\delta\\epsilon \\, u_m, -p_m \\rangle $$\n$$ \\delta J = \\sum_{m=1}^{N_s} \\text{Re} \\left[ \\int_{\\Omega} (\\omega^2 \\mu \\delta\\epsilon \\, u_m) \\overline{(-p_m)} \\, d\\mathbf{r} \\right] $$\n由于 $ \\omega, \\mu $ 是实常数且 $ \\delta\\epsilon $ 是实扰动，我们可以将它们从实部算子中提出来：\n$$ \\delta J = \\sum_{m=1}^{N_s} \\left( -\\omega^2 \\mu \\int_{\\Omega} \\delta\\epsilon(\\mathbf{r}) \\, \\text{Re} \\left[ u_m(\\mathbf{r}) \\overline{p_m(\\mathbf{r})} \\right] \\, d\\mathbf{r} \\right) $$\n交换求和与积分的顺序：\n$$ \\delta J = \\int_{\\Omega} \\left( - \\omega^2 \\mu \\sum_{m=1}^{N_s} \\text{Re} \\left[ u_m(\\mathbf{r}) \\overline{p_m(\\mathbf{r})} \\right] \\right) \\delta\\epsilon(\\mathbf{r}) \\, d\\mathbf{r} $$\n通过将其与定义 $ \\delta J = \\int_{\\Omega} \\nabla_{\\epsilon} J(\\mathbf{r}) \\, \\delta\\epsilon(\\mathbf{r}) \\, d\\mathbf{r} $ 进行比较，我们可以确定泛函梯度。\n\n所需的要素是：\n正演场方程为，对于 $ m=1, \\dots, N_s $：\n$$ \\left(\\nabla^{2} + \\omega^{2} \\mu \\epsilon(\\mathbf{r})\\right) u_{m}(\\mathbf{r}) = s_{m}(\\mathbf{r}) \\quad \\text{in } \\Omega $$\n伴随场方程为，对于 $ m=1, \\dots, N_s $：\n$$ \\left(\\nabla^{2} + \\omega^{2} \\mu \\epsilon(\\mathbf{r})\\right) p_{m}(\\mathbf{r}) = \\sum_{j=1}^{N_r} \\overline{(u_{m}(\\mathbf{r}_{j}) - d_{mj})} \\delta(\\mathbf{r} - \\mathbf{r}_{j}) \\quad \\text{in } \\Omega $$\n泛函梯度的最终表达式为：\n$$ \\nabla_{\\epsilon} J(\\mathbf{r}) = - \\omega^2 \\mu \\sum_{m=1}^{N_s} \\text{Re} \\left[ u_m(\\mathbf{r}) \\overline{p_m(\\mathbf{r})} \\right] $$\n实部也可以写成 $ \\text{Re}[u_m \\overline{p_m}] = \\text{Re}[u_m] \\text{Re}[p_m] + \\text{Im}[u_m] \\text{Im}[p_m] $。该表达式以其最紧凑的形式给出。", "answer": "$$ \\boxed{- \\omega^{2} \\mu \\sum_{m=1}^{N_{s}} \\text{Re} \\left[ u_{m}(\\mathbf{r}) \\overline{p_{m}(\\mathbf{r})} \\right]} $$", "id": "3327841"}, {"introduction": "传统的电磁仿真有时会产生不符合物理规律的“伪模”（spurious modes），这给结果的解读带来了困扰。本实践练习将指导您构建一个机器学习分类器，充当一个智能诊断工具，自动识别这些伪解。您将通过编程实现从数据生成到模型训练的全过程，利用无散度约束（divergence-free constraint）和模式正交性等基本物理原理来设计特征，从而教会模型区分真实的物理模式与数值计算产生的伪影。[@problem_id:3327863]", "problem": "您的任务是设计并实现一个完整的、可运行的程序。该程序使用从散度约束和模式正交性中派生的特征，训练一个二元分类器来检测计算电磁学中离散旋度-旋度本征问题里的伪本征模，然后提出自动化的网格修复建议。分类器必须在一个由程序自身生成的合成数据集上进行训练，该数据集基于符合物理原理的离散磁场模式构建方法。整个程序必须是自包含的，并为提供的测试套件生成一个单行输出，汇总所有结果。\n\n从电磁学定律出发，在数学上推导用于分类的特征。请使用以下基本依据：\n- 适用于宏观介质的麦克斯韦方程组，包括磁通量密度 $\\mathbf{B}$ 的无散度约束 $\\nabla \\cdot \\mathbf{B} = 0$，以及电场 $\\mathbf{E}$ 的无源旋度-旋度本征问题\n$$\n\\nabla \\times \\left( \\mu^{-1} \\nabla \\times \\mathbf{E} \\right) = \\lambda \\varepsilon \\mathbf{E},\n$$\n在一个单连通域中具有适当的边界条件，其中 $\\mu$ 是磁导率，$\\varepsilon$ 是介电常数，$\\lambda$ 是本征值。\n- 在均匀介质中，不同本征模在与自伴旋度-旋度算子相关的能量内积下具有正交性，这意味着对于不同的本征函数 $\\mathbf{E}_i$ 和 $\\mathbf{E}_j$\n$$\n\\int_{\\Omega} \\varepsilon \\mathbf{E}_i \\cdot \\mathbf{E}_j d\\Omega = 0.\n$$\n- 在离散设置中，伪模式的出现通常是由于违反了精确序列性质，从而导致非无散场或模式重复；因此，基于原理的特征应该量化散度和（非）正交性。\n\n您的程序必须：\n1. 在均匀矩形网格上，构建一个离散二维磁场模式 $\\mathbf{B}(x,y) = (B_x(x,y), B_y(x,y))$ 的合成数据集。生成两种类型的模式：\n   - 从保证网格上 $\\nabla \\cdot \\mathbf{B} \\approx 0$ 的解析模式构建的、物理上合理的无散度模式。\n   - 作为标量势 $\\phi(x,y)$ 的梯度构建的伪模式，即 $\\mathbf{B} = \\nabla \\phi$，通常情况下其散度 $\\nabla \\cdot \\mathbf{B} = \\nabla^2 \\phi \\neq 0$。\n2. 对于给定均匀网格（网格间距为 $h_x$ 和 $h_y$）上的任意离散模式对 $(\\mathbf{B}^{(1)}, \\mathbf{B}^{(2)})$，为每个模式计算以下特征：\n   - 相对散度大小\n     $$\n     R_{\\mathrm{div}} = \\frac{\\left\\| \\nabla \\cdot \\mathbf{B} \\right\\|_{L^2}}{\\left\\| \\mathbf{B} \\right\\|_{L^2} + \\epsilon},\n     $$\n     其中 $\\epsilon > 0$ 是一个小的稳定化常数，$L^2$ 范数使用面积元 $h_x h_y$ 进行离散化。使用离散中心差分计算内部点，并使用一致的单边差分计算边界点，以近似 $\\partial B_x / \\partial x$ 和 $\\partial B_y / \\partial y$。\n   - 两种模式之间的非正交性指数\n     $$\n     R_{\\mathrm{nonorth}}^{(i)} = \\frac{\\left| \\langle \\mathbf{B}^{(i)}, \\mathbf{B}^{(j)} \\rangle \\right|}{\\left\\| \\mathbf{B}^{(i)} \\right\\|_{L^2} \\left\\| \\mathbf{B}^{(j)} \\right\\|_{L^2} + \\epsilon}, \\quad i \\neq j,\n     $$\n     使用离散内积 $\\langle \\mathbf{U}, \\mathbf{V} \\rangle = \\sum (U_x V_x + U_y V_y) h_x h_y$。\n   - 网格各向异性质量度量\n     $$\n     Q_{\\mathrm{aspect}} = \\max\\left( \\frac{h_x}{h_y}, \\frac{h_y}{h_x} \\right).\n     $$\n3. 使用带 $L^2$ 正则化的逻辑回归训练一个二元分类器，根据特征向量\n   $$\n   \\mathbf{x} = [ R_{\\mathrm{div}}, R_{\\mathrm{nonorth}}, Q_{\\mathrm{aspect}} ],\n   $$\n   使用该合成数据集来预测一个模式是伪模式（$1$）还是物理模式（$0$）。在训练之前，使用训练集的均值和标准差对特征进行标准化。分类器必须在程序中直接实现（不使用外部机器学习库），通过对带有 $L^2$ 惩罚项的交叉熵损失函数应用批量梯度下降法进行训练。为了保证可复现性，请固定随机种子。\n4. 对于每个测试用例，计算两种模式的特征，应用训练好的分类器以获得预测标签（布尔值），然后根据以下基于规则的映射，为该用例提出一个单一的自动化网格修复建议代码：\n   - 如果 $Q_{\\mathrm{aspect}} > 3.0$，建议代码为 $1$（建议进行局部各向同性细化以减少各向异性）。\n   - 否则，如果任一模式被预测为伪模式并且其 $R_{\\mathrm{div}} > 0.15$，建议代码为 $2$（建议采用散度协调离散化或添加散度惩罚项）。\n   - 否则，如果 $\\max(R_{\\mathrm{nonorth}}^{(1)}, R_{\\mathrm{nonorth}}^{(2)}) > 0.95$，建议代码为 $3$（建议进行正交化或改进本征求解器设置，以分离近似重复的模式）。\n   - 否则建议代码为 $0$（无需修复）。\n5. 以确定性的方式实现整个流程，并生成一个单行输出，该输出包含测试套件的结果列表，其中每个测试用例的结果是一个形如\n   `[标签^(1), 标签^(2), 建议代码]`\n   的列表，其中两个标签是布尔值，建议代码是整数。\n\n程序中的所有量都是无量纲的。解析函数中出现的角度，单位为弧度。最终输出行必须是每个测试用例列表组成的 Python 列表的精确字符串表示，例如\n`[[True, False, 2], [False, False, 0]]`。\n\n测试套件：\n提供四个科学上有意义的用例，用于检验分类器和建议逻辑的不同方面。所有测试在每个方向上均使用 $n_x = 64$, $n_y = 64$ 个网格点。设域的大小为 $L_x$ 和 $L_y$，因此网格间距为 $h_x = L_x / (n_x - 1)$ 和 $h_y = L_y / (n_y - 1)$。\n- 用例 $1$（良好解析的物理模式，理想情况）：$L_x = 1.0, L_y = 1.0$。模式 $1$ 是无散度的，波数为 $k = 3$。模式 $2$ 是无散度的，波数为 $k = 5$。\n- 用例 $2$（一个伪梯度模式）：$L_x = 1.0, L_y = 1.0$。模式 $1$ 是一个来自标量势 $\\phi(x,y) = \\sin(10 x)\\sin(10 y)$ 的梯度场。模式 $2$ 是无散度的，波数为 $k = 4$。\n- 用例 $3$（各向异性网格质量问题）：$L_x = 1.0, L_y = 5.0$。模式 $1$ 是无散度的，波数为 $k = 4$。模式 $2$ 是无散度的，波数为 $k = 6$。\n- 用例 $4$（近似重复的模式，高非正交性）：$L_x = 1.0, L_y = 1.0$。模式 $1$ 是无散度的，波数为 $k = 8$。模式 $2$ 等于模式 $1$ 再加上一个大小为 $10^{-3}$ 的加性噪声到每个分量上。\n\n您的程序应生成一个单行输出，其中包含用方括号括起来的、以逗号分隔的结果列表，每个测试用例的结果本身是一个列表 `[标签^(1), 标签^(2), 建议代码]`，例如：\n`[[False, False, 0], [True, False, 2], [False, False, 1], [False, False, 3]]`。\n\n实现约束：\n- 仅使用 Python 语言运行时和执行环境中指定的库。不允许使用外部机器学习库。\n- 在程序中固定随机数生成器种子，以确保确定性行为。\n- 训练数据集必须包含至少 $200$ 个模式，从上述解析构造中抽取，并包含均衡的物理模式和伪模式样本。", "solution": "本问题的解决方案是编写一个 Python 程序，该程序可实现一个完整的诊断流程：生成合成数据、提取基于物理原理的特征、训练一个二元分类器，并基于分类结果和特征值提出自动化修复建议。以下是该程序实现步骤的详细说明。\n\n### 1. 合成数据生成\n为了训练分类器，程序首先生成一个带标签的数据集。这些数据模仿了计算电磁学本征问题中的两种模式：\n- **物理模式 (Physical Modes)**：这些模式在物理上是合理的，必须满足磁场的无散度条件 $\\nabla \\cdot \\mathbf{B} = 0$。为了在离散网格上确保这一点，程序使用了一个二维流函数 $\\psi(x, y)$。通过将磁场定义为 $\\mathbf{B} = (\\partial_y\\psi, -\\partial_x\\psi)$，其散度 $\\nabla \\cdot \\mathbf{B} = \\partial_x(\\partial_y\\psi) - \\partial_y(\\partial_x\\psi)$ 根据克莱罗定理恒为零。在代码中，`generate_physical_mode` 函数利用正弦函数作为流函数来创建这些无散度场。\n- **伪模式 (Spurious Modes)**：这些模式代表了数值计算中常见的非物理“伪影”。它们通常表现为梯度场，即 $\\mathbf{B} = \\nabla\\phi$，其中 $\\phi$ 是一个标量势。这样的场通常不满足无散度条件，因为 $\\nabla \\cdot \\mathbf{B} = \\nabla^2\\phi$，这通常不为零。`generate_spurious_mode` 函数通过计算一个标量势的梯度来生成这些模式。\n\n程序通过随机选择模式类型（物理或伪模式）和相关参数（如波数），生成一个包含 200 个样本的均衡训练集。\n\n### 2. 特征计算\n对于每个生成的模式，程序计算一个三维特征向量，用于捕捉其物理属性：\n- **相对散度大小 ($R_{\\mathrm{div}}$)**：该特征直接量化了模式对无散度条件的违反程度。一个完美的物理模式其散度应为零，而伪模式通常具有显著的非零散度。程序使用 `numpy.gradient` 计算离散散度，并用场的 $L^2$ 范数进行归一化，使其与模式的幅度无关。\n- **非正交性指数 ($R_{\\mathrm{nonorth}}$)**：该特征衡量一对模式之间的正交性。物理上不同的本征模应该是正交的。一个接近 1 的高值表示两个模式线性相关，可能暗示着数值求解器未能分离近似简并的模式。\n- **网格各向异性 ($Q_{\\mathrm{aspect}}$)**：这是一个网格质量度量，定义为网格单元在 x 和 y 方向上尺寸的最大比率。高各向异性会降低数值精度，是产生伪解的一个潜在因素。\n\n这些特征的计算在 `compute_features` 函数中实现。\n\n### 3. 分类器训练\n程序从零开始实现一个带 L2 正则化的逻辑回归分类器 (`LogisticRegressor` 类)，而不依赖外部机器学习库。\n- **模型**：一个模式被分类为伪模式的概率由 Sigmoid 函数 $\\sigma(\\mathbf{w}^T \\mathbf{x} + b)$ 建模。\n- **训练**：通过最小化正则化的二元交叉熵损失函数来学习权重 $\\mathbf{w}$ 和偏置 $b$。优化过程使用批量梯度下降法，在整个训练集上迭代计算梯度并更新参数。\n- **标准化**：在训练之前，使用训练集的均值和标准差对特征进行标准化处理。这能确保所有特征具有相似的尺度，从而改善梯度下降的收敛性能和稳定性。\n\n### 4. 测试与自动化修复建议\n训练完成后，分类器被用于分析给定的四个测试用例。\n1.  **特征提取与预测**：对于每个测试用例中的模式对，程序计算其特征向量，并使用训练阶段得到的均值和标准差对其进行标准化。然后，训练好的分类器预测每个模式是物理模式（`False`）还是伪模式（`True`）。\n2.  **修复逻辑**：最后，根据问题中定义的一组分层规则，程序生成一个修复建议代码：\n    - 代码 `1`：如果网格各向异性过高 ($Q_{\\mathrm{aspect}} > 3.0$)\n    - 代码 `2`：如果任一模式被预测为伪模式且其相对散度过大 ($R_{\\mathrm{div}} > 0.15$)\n    - 代码 `3`：如果两个模式高度相关（非正交性指数 $R_{\\mathrm{nonorth}} > 0.95$）\n    - 代码 `0`：如果以上条件均不满足，则表示无需修复。\n\n这个基于规则的系统将分类器的诊断结果转化为一个可操作的建议，为改进数值模拟设置提供指导。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to orchestrate the entire pipeline: dataset generation,\n    feature computation, classifier training, and testing.\n    \"\"\"\n    # Fix the random seed for reproducibility\n    np.random.seed(42)\n\n    # --- Global Constants ---\n    STABILIZATION_EPS = 1e-9\n    TRAINING_SAMPLES = 200  # Will be created from 100 pairs\n    GRID_SIZE_TRAIN = 32\n    \n    # Logistic Regression Hyperparameters\n    LEARNING_RATE = 0.1\n    ITERATIONS = 2000\n    REG_PARAM = 0.01\n\n    # --- Helper Functions for Grid and Mode Generation ---\n    def make_grid(nx, ny, Lx, Ly):\n        \"\"\"Creates a 2D mesh grid.\"\"\"\n        x = np.linspace(0, Lx, nx)\n        y = np.linspace(0, Ly, ny)\n        X, Y = np.meshgrid(x, y)\n        hx = Lx / (nx - 1)\n        hy = Ly / (ny - 1)\n        return X, Y, hx, hy\n\n    def generate_physical_mode(k_val, X, Y):\n        \"\"\"Generates a divergence-free mode from a stream function.\"\"\"\n        # psi = sin(kx*x) * sin(ky*y)\n        # Bx = d(psi)/dy, By = -d(psi)/dx\n        kx = k_val\n        ky = k_val\n        psi = np.sin(kx * X) * np.sin(ky * Y)\n        Bx = ky * np.sin(kx * X) * np.cos(ky * Y)\n        By = -kx * np.cos(kx * X) * np.sin(ky * Y)\n        return Bx, By\n\n    def generate_spurious_mode(phi_func, X, Y):\n        \"\"\"Generates a spurious gradient-field mode from a scalar potential.\"\"\"\n        # B = grad(phi)\n        # Bx = d(phi)/dx, By = d(phi)/dy\n        phi = phi_func(X, Y)\n        # np.gradient returns derivatives along axis 0 (y), then axis 1 (x)\n        gy, gx = np.gradient(phi, Y[0,1]-Y[0,0], X[1,0]-X[0,0])\n        return gx, gy\n\n    # --- Feature Computation Functions ---\n    def compute_divergence(Bx, By, hx, hy):\n        \"\"\"Computes the divergence of a 2D vector field using np.gradient.\"\"\"\n        _, dBx_dx = np.gradient(Bx, hy, hx)\n        dBy_dy, _ = np.gradient(By, hy, hx)\n        return dBx_dx + dBy_dy\n\n    def compute_l2_norm(B_comp, hx, hy):\n        \"\"\"Computes the discrete L2 norm of a vector field.\"\"\"\n        Bx, By = B_comp\n        integrand = Bx**2 + By**2\n        return np.sqrt(np.sum(integrand) * hx * hy)\n\n    def compute_inner_product(B1_comp, B2_comp, hx, hy):\n        \"\"\"Computes the discrete inner product of two vector fields.\"\"\"\n        B1x, B1y = B1_comp\n        B2x, B2y = B2_comp\n        integrand = B1x * B2x + B1y * B2y\n        return np.sum(integrand) * hx * hy\n\n    def compute_features(B1_comp, B2_comp, hx, hy):\n        \"\"\"Computes the full feature set for a pair of modes.\"\"\"\n        # Feature 3: Mesh Anisotropy\n        Q_aspect = max(hx / hy, hy / hx)\n\n        # Compute norms for both modes\n        norm_B1 = compute_l2_norm(B1_comp, hx, hy)\n        norm_B2 = compute_l2_norm(B2_comp, hx, hy)\n\n        # Feature 1: Relative Divergence for each mode\n        div_B1 = compute_divergence(B1_comp[0], B1_comp[1], hx, hy)\n        norm_div_B1 = np.sqrt(np.sum(div_B1**2) * hx * hy)\n        R_div1 = norm_div_B1 / (norm_B1 + STABILIZATION_EPS)\n\n        div_B2 = compute_divergence(B2_comp[0], B2_comp[1], hx, hy)\n        norm_div_B2 = np.sqrt(np.sum(div_B2**2) * hx * hy)\n        R_div2 = norm_div_B2 / (norm_B2 + STABILIZATION_EPS)\n\n        # Feature 2: Non-Orthogonality\n        inner_prod = compute_inner_product(B1_comp, B2_comp, hx, hy)\n        R_nonorth = abs(inner_prod) / ((norm_B1 * norm_B2) + STABILIZATION_EPS)\n        \n        features1 = np.array([R_div1, R_nonorth, Q_aspect])\n        features2 = np.array([R_div2, R_nonorth, Q_aspect])\n\n        return features1, features2\n\n    # --- Logistic Regression Classifier (from scratch) ---\n    class LogisticRegressor:\n        def __init__(self, lr, n_iter, lambda_):\n            self.lr = lr\n            self.n_iter = n_iter\n            self.lambda_ = lambda_\n            self.weights = None\n            self.bias = None\n\n        def _sigmoid(self, z):\n            # Clip to avoid overflow in exp\n            z = np.clip(z, -500, 500)\n            return 1 / (1 + np.exp(-z))\n\n        def fit(self, X, y):\n            n_samples, n_features = X.shape\n            self.weights = np.zeros(n_features)\n            self.bias = 0\n\n            # Batch Gradient Descent\n            for _ in range(self.n_iter):\n                linear_model = np.dot(X, self.weights) + self.bias\n                y_predicted = self._sigmoid(linear_model)\n\n                # Compute gradients\n                dw = (1 / n_samples) * (np.dot(X.T, (y_predicted - y)) + self.lambda_ * self.weights)\n                db = (1 / n_samples) * np.sum(y_predicted - y)\n\n                # Update weights and bias\n                self.weights -= self.lr * dw\n                self.bias -= self.lr * db\n\n        def predict_proba(self, X):\n            linear_model = np.dot(X, self.weights) + self.bias\n            return self._sigmoid(linear_model)\n\n        def predict(self, X, threshold=0.5):\n            probas = self.predict_proba(X)\n            return probas >= threshold\n\n    # --- 1. Generate Training Data ---\n    num_pairs = TRAINING_SAMPLES // 2\n    X_train_list = []\n    y_train_list = []\n\n    for i in range(num_pairs):\n        # Randomize mesh\n        nx, ny = GRID_SIZE_TRAIN, GRID_SIZE_TRAIN\n        Lx = np.random.uniform(1.0, 5.0)\n        Ly = np.random.uniform(1.0, 5.0)\n        X, Y, hx, hy = make_grid(nx, ny, Lx, Ly)\n\n        # Generate a pair of modes, each randomly physical or spurious\n        modes = []\n        labels = []\n        for _ in range(2):\n            if np.random.rand() > 0.5: # Physical\n                k = np.random.uniform(2.0, 10.0)\n                modes.append(generate_physical_mode(k, X, Y))\n                labels.append(0) # 0 for physical\n            else: # Spurious\n                k = np.random.uniform(2.0, 10.0)\n                modes.append(generate_spurious_mode(lambda x,y: np.sin(k*x)*np.cos(k*y), X, Y))\n                labels.append(1) # 1 for spurious\n\n        # Compute features for the pair\n        features1, features2 = compute_features(modes[0], modes[1], hx, hy)\n        \n        X_train_list.append(features1)\n        y_train_list.append(labels[0])\n        X_train_list.append(features2)\n        y_train_list.append(labels[1])\n        \n    X_train = np.array(X_train_list)\n    y_train = np.array(y_train_list)\n\n    # --- 2. Standardize Features and Train Classifier ---\n    mean = X_train.mean(axis=0)\n    std = X_train.std(axis=0)\n    std[std == 0] = 1 # Avoid division by zero if a feature is constant\n    X_train_std = (X_train - mean) / std\n\n    classifier = LogisticRegressor(lr=LEARNING_RATE, n_iter=ITERATIONS, lambda_=REG_PARAM)\n    classifier.fit(X_train_std, y_train)\n\n    # --- 3. Process Test Suite ---\n    test_cases_params = [\n        {'Lx': 1.0, 'Ly': 1.0, 'mode1_type': 'physical', 'mode1_k': 3, 'mode2_type': 'physical', 'mode2_k': 5},\n        {'Lx': 1.0, 'Ly': 1.0, 'mode1_type': 'spurious', 'mode1_phi': lambda x,y: np.sin(10*x)*np.sin(10*y), 'mode2_type': 'physical', 'mode2_k': 4},\n        {'Lx': 1.0, 'Ly': 5.0, 'mode1_type': 'physical', 'mode1_k': 4, 'mode2_type': 'physical', 'mode2_k': 6},\n        {'Lx': 1.0, 'Ly': 1.0, 'mode1_type': 'physical', 'mode1_k': 8, 'mode2_type': 'duplicate', 'mode2_k': 8}\n    ]\n    nx_test, ny_test = 64, 64\n    results = []\n\n    for case in test_cases_params:\n        X, Y, hx, hy = make_grid(nx_test, ny_test, case['Lx'], case['Ly'])\n        \n        # Generate modes for the test case\n        B1, B2 = None, None\n        \n        if case['mode1_type'] == 'physical':\n            B1 = generate_physical_mode(case['mode1_k'], X, Y)\n        elif case['mode1_type'] == 'spurious':\n            B1 = generate_spurious_mode(case['mode1_phi'], X, Y)\n\n        if case['mode2_type'] == 'physical':\n            B2 = generate_physical_mode(case['mode2_k'], X, Y)\n        elif case['mode2_type'] == 'duplicate':\n            B_base = generate_physical_mode(case['mode2_k'], X, Y)\n            noise_mag = 1e-3\n            noise_x = noise_mag * (2 * np.random.rand(nx_test, ny_test) - 1)\n            noise_y = noise_mag * (2 * np.random.rand(nx_test, ny_test) - 1)\n            B2 = (B_base[0] + noise_x, B_base[1] + noise_y)\n        \n        # Compute features\n        features1, features2 = compute_features(B1, B2, hx, hy)\n        \n        # Standardize features using training set stats\n        test_features = np.array([features1, features2])\n        test_features_std = (test_features - mean) / std\n        \n        # Get predictions\n        predictions = classifier.predict(test_features_std)\n        label1, label2 = bool(predictions[0]), bool(predictions[1])\n        \n        # Apply repair suggestion logic\n        Q_aspect = features1[2] # same for both\n        R_div1, R_div2 = features1[0], features2[0]\n        R_nonorth = features1[1]\n        \n        suggestion_code = 0 # Default: no repair\n        if Q_aspect > 3.0:\n            suggestion_code = 1\n        elif (label1 and R_div1 > 0.15) or (label2 and R_div2 > 0.15):\n            suggestion_code = 2\n        elif R_nonorth > 0.95:\n            suggestion_code = 3\n\n        results.append([label1, label2, suggestion_code])\n\n    # --- 4. Print Final Output ---\n    print(results)\n\n\nsolve()\n```", "id": "3327863"}, {"introduction": "将物理先验知识融入神经网络架构是计算电磁学中机器学习应用的前沿方向。本练习将揭示图神经网络（GNN）与有限元法（FEM）之间的深刻联系，指导您设计一个其消息传递机制精确模仿有限元“卷曲-卷曲”（curl-curl）算子装配过程的 GNN 层。通过这个实践，您将学习如何构建一个不再是“黑箱”的、其结构本身就蕴含了物理离散化信息的神经网络，为开发更具可解释性和泛化能力的物理感知代理模型打下基础。[@problem_id:3327879]", "problem": "你需要设计并实现一个单层图神经网络，该网络精确模拟了使用最低阶边自由度的二维离散化中旋度-旋度双线性形式的有限元法组装过程。从计算电磁学中的一个有效基本出发点开始，推导出消息传递规则。然后在图中编码完美电导体（PEC）边界条件，以便在更新过程中将边界边钳制为零。最后，实现一个完整的程序，该程序为给定的小网格和几组参数计算该层的输出，并以指定格式打印结果。\n\n从以弱形式写出的时谐麦克斯韦旋度-旋度算子开始：给定一个矢量测试函数 $v$ 和一个电场 $E$，双线性形式为\n$$\na(E,v) = \\int_{\\Omega} \\mu^{-1} \\left( \\nabla \\times E \\right) \\cdot \\left( \\nabla \\times v \\right) d\\Omega,\n$$\n其中 $\\mu$ 是磁导率，$\\Omega$ 是计算域。在最低阶基于边的离散化（与Sobolev空间 $H(\\mathrm{curl})$ 兼容）中，将一个自由度与每个有向边关联起来，作为 $E$ 的切向分量沿该边的线积分。在一个由具有分片常数 $\\mu$ 和面积 $A$ 的单元组成的网格上，定义每个单元 $k$ 的离散旋度为\n$$\nc_k(x) = \\frac{1}{A_k} \\sum_{e \\in \\partial k} s_{k,e} x_e,\n$$\n其中 $x_e$ 是边自由度（在此问题中为无量纲量），$A_k$ 是单元 $k$ 的面积，$s_{k,e} \\in \\{+1,-1\\}$ 是通过将边 $e$ 的全局方向与所选的单元 $k$ 边界的逆时针方向进行比较而得出的符号。对于单元级常数 $\\mu_k$，组装的全局刚度（旋度-旋度）矩阵 $K$ 具有以下结构\n$$\nK = C^{\\top} \\mathrm{diag}\\!\\left( \\frac{\\mu_1^{-1}}{A_1}, \\ldots, \\frac{\\mu_K^{-1}}{A_K} \\right) C,\n$$\n其中 $C \\in \\mathbb{R}^{K \\times E}$ 是带符号的单元-边关联矩阵，其元素为 $C_{k,e} = s_{k,e}$，$K$ 是单元数，$E$ 是边数。换句话说，双线性形式简化为\n$$\na(x,y) = x^{\\top} K y.\n$$\n\n构建一个图 $\\mathcal{G}$，其中每个节点对应一个有向网格边，如果两个节点共同界定至少一个单元，则连接它们。该单层图神经网络必须实现消息传递\n$$\ny = \\left( \\mathbf{1} - m \\right) \\odot \\left( K x \\right),\n$$\n其中 $x \\in \\mathbb{R}^E$ 是输入边特征向量（无量纲），$y \\in \\mathbb{R}^E$ 是输出，$m \\in \\{0,1\\}^E$ 是一个编码完美电导体边界条件的二元边界掩码（对于边界边，$m_e = 1$；对于内部边，$m_e = 0$），$\\odot$ 表示逐元素乘法。这通过将边界节点钳制为零来强制执行狄利克雷条件。\n\n使用以下固定的网格、几何形状和方向：\n- 域被细分为 $2$ 个轴对齐的单位正方形单元：左侧单元 $k=0$，面积 $A_0 = 1$；右侧单元 $k=1$，面积 $A_1 = 1$。\n- 唯一的有向边按如下方式索引和定向，所有长度均为 $1$（无量纲单位）：\n  - $e_0$：左下边，从 $(0,0)$ 到 $(1,0)$（全局方向沿 $+x$）。\n  - $e_1$：右下边，从 $(1,0)$ 到 $(2,0)$（全局方向沿 $+x$）。\n  - $e_2$：中间垂直边，从 $(1,0)$ 到 $(1,1)$（全局方向沿 $+y$）。\n  - $e_4$：左上边，从 $(0,1)$ 到 $(1,1)$（全局方向沿 $+x$）。\n  - $e_5$：右上边，从 $(1,1)$ 到 $(2,1)$（全局方向沿 $+x$）。\n  - $e_6$：左边界垂直边，从 $(0,0)$ 到 $(0,1)$（全局方向沿 $+y$）。\n  - $e_7$：右边界垂直边，从 $(2,0)$ 到 $(2,1)$（全局方向沿 $+y$）。\n- 每个单元的逆时针边界遍历导出了带符号的关联矩阵 $C \\in \\mathbb{R}^{2 \\times 7}$，其非零元素为\n  - 对于左侧单元 $k=0$：$C_{0,0} = +1$, $C_{0,2} = +1$, $C_{0,4} = -1$, $C_{0,6} = -1$。\n  - 对于右侧单元 $k=1$：$C_{1,1} = +1$, $C_{1,7} = +1$, $C_{1,5} = -1$, $C_{1,2} = -1$。\n$C$ 的所有其他元素均为 $0$。\n\n定义边界掩码 $m \\in \\{0,1\\}^7$，对于边界边 $m_e = 1$，对于内部边 $m_e = 0$。在此网格中，$e_2$ 是唯一的内部边，因此选择\n$$\nm = [1, 1, 0, 1, 1, 1, 1],\n$$\n按边索引 $[e_0, e_1, e_2, e_4, e_5, e_6, e_7]$ 排序。\n\n你的程序必须：\n- 根据 $C$ 和 $(\\mu_0, \\mu_1)$，使用公式 $K = C^{\\top} \\mathrm{diag}\\!\\left( \\mu_0^{-1}/A_0, \\mu_1^{-1}/A_1 \\right) C$ 组装 $K$。\n- 计算单层输出 $y = \\left( \\mathbf{1} - m \\right) \\odot \\left( K x \\right)$。\n- 使用以下涵盖多种情况的测试套件：\n  $1.$ 正常情况：$\\mu_0 = 1$，$\\mu_1 = 1$，$x = [1, 2, -1, 0.5, -0.5, 0, 0]$。\n  $2.$ 材料对比：$\\mu_0 = 0.5$，$\\mu_1 = 2$，$x = [-0.25, 0.75, 1, -1.25, 0, 0.25, -0.25]$。\n  $3.$ 边界驱动激励：$\\mu_0 = 1$，$\\mu_1 = 1$，$x = [0, 0, 0, 0, 0, 3, -3]$。\n\n在此问题中，所有量都是无量纲的。对于每个测试用例，返回完整的输出向量 $y \\in \\mathbb{R}^7$，形式为浮点数列表，四舍五入到六位小数。\n\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，其中每个条目对应一个测试用例，并且其本身是一个包含其 $7$ 个四舍五入输出的逗号分隔列表，并用方括号括起来。例如，输出格式必须为\n`[[y_1,0, y_1,1, ..., y_1,6], [y_2,0, ..., y_2,6], [y_3,0, ..., y_3,6]]`。", "solution": "本问题的目标是实现一个单层图神经网络（GNN）操作，该操作在数学上等价于有限元法（FEM）中旋度-旋度算子的离散化。我们将按照以下步骤完成计算：构建所需的矩阵，组装全局刚度矩阵 $K$，然后对每个测试用例计算输出向量 $y$。\n\n### 1. 矩阵构建\n首先，我们根据问题描述中的几何信息和边索引 $[e_0, e_1, e_2, e_4, e_5, e_6, e_7]$（对应于数组索引 $0$ 到 $6$）来构建矩阵。\n\n- **带符号的关联矩阵 $C \\in \\mathbb{R}^{2 \\times 7}$**:\n该矩阵将边（列）映射到单元（行）。$C_{k,e}$ 为 $+1$ 或 $-1$ 取决于边 $e$ 的方向与单元 $k$ 的逆时针边界遍历方向是否一致。\n- 单元 0 (左): $e_0$ (+1), $e_2$ (+1), $e_4$ (-1), $e_6$ (-1)\n- 单元 1 (右): $e_1$ (+1), $e_7$ (+1), $e_5$ (-1), $e_2$ (-1)\n由此，$C$ 矩阵为：\n$$\nC = \\begin{pmatrix}\n1 & 0 & 1 & -1 & 0 & -1 & 0 \\\\\n0 & 1 & -1 & 0 & -1 & 0 & 1\n\\end{pmatrix}\n$$\n\n- **边界掩码 $m$**:\n该向量用于识别边界边。问题中指出 $e_2$ 是唯一的内部边。\n$$ m = [1, 1, 0, 1, 1, 1, 1] $$\n因此，用于选择内部边的向量 $(\\mathbf{1}-m)$ 为：\n$$ \\mathbf{1} - m = [0, 0, 1, 0, 0, 0, 0] $$\n这表明只有与内部边 $e_2$ 相关的输出才不为零。\n\n### 2. 计算过程\n对于每个测试用例，我们计算 $y = (\\mathbf{1} - m) \\odot (Kx)$，其中 $K = C^\\top D C$。\n\n#### 测试用例 1:\n- 参数: $\\mu_0 = 1, \\mu_1 = 1$, $A_0 = 1, A_1 = 1$\n- 输入: $x = [1, 2, -1, 0.5, -0.5, 0, 0]^\\top$\n1.  **构建 $D$**: $D = \\mathrm{diag}(\\mu_0^{-1}, \\mu_1^{-1}) = \\mathrm{diag}(1, 1) = I_{2\\times2}$ (单位矩阵)。\n2.  **组装 $K$**: $K = C^\\top I C = C^\\top C$。\n3.  **计算 $Kx$**: 这可以分步完成，即消息传递的“聚合-传播”过程。\n    - 聚合 (边到单元): $z = Cx = \\begin{pmatrix} 1-1-0.5-0 \\\\ 2-(-1)-(-0.5) \\end{pmatrix} = \\begin{pmatrix} -0.5 \\\\ 3.5 \\end{pmatrix}$\n    - 变换: $w = Dz = I z = z = \\begin{pmatrix} -0.5 \\\\ 3.5 \\end{pmatrix}$\n    - 传播 (单元到边): $Kx = C^\\top w = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & -1 \\\\ -1 & 0 \\\\ 0 & -1 \\\\ -1 & 0 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} -0.5 \\\\ 3.5 \\end{pmatrix} = \\begin{pmatrix} -0.5 \\\\ 3.5 \\\\ -0.5 - 3.5 \\\\ -(-0.5) \\\\ -3.5 \\\\ -(-0.5) \\\\ 3.5 \\end{pmatrix} = \\begin{pmatrix} -0.5 \\\\ 3.5 \\\\ -4.0 \\\\ 0.5 \\\\ -3.5 \\\\ 0.5 \\\\ 3.5 \\end{pmatrix}$\n4.  **应用边界条件**: $y = (\\mathbf{1}-m) \\odot Kx = [0, 0, 1, 0, 0, 0, 0] \\odot [-0.5, 3.5, -4.0, 0.5, -3.5, 0.5, 3.5]^\\top = [0, 0, -4.0, 0, 0, 0, 0]^\\top$\n\n#### 测试用例 2:\n- 参数: $\\mu_0 = 0.5, \\mu_1 = 2$, $A_0 = 1, A_1 = 1$\n- 输入: $x = [-0.25, 0.75, 1, -1.25, 0, 0.25, -0.25]^\\top$\n1.  **构建 $D$**: $D = \\mathrm{diag}(0.5^{-1}, 2^{-1}) = \\mathrm{diag}(2, 0.5)$。\n2.  **计算 $Kx = C^\\top D C x$**:\n    - 聚合: $z = Cx = \\begin{pmatrix} -0.25+1-(-1.25)-0.25 \\\\ 0.75-1-0-(-0.25) \\end{pmatrix} = \\begin{pmatrix} 1.75 \\\\ -0.5 \\end{pmatrix}$\n    - 变换: $w = Dz = \\begin{pmatrix} 2 \\times 1.75 \\\\ 0.5 \\times (-0.5) \\end{pmatrix} = \\begin{pmatrix} 3.5 \\\\ -0.25 \\end{pmatrix}$\n    - 传播: $Kx = C^\\top w = \\begin{pmatrix} 3.5 \\\\ -0.25 \\\\ 3.5 - (-0.25) \\\\ -3.5 \\\\ -(-0.25) \\\\ -3.5 \\\\ -0.25 \\end{pmatrix} = \\begin{pmatrix} 3.5 \\\\ -0.25 \\\\ 3.75 \\\\ -3.5 \\\\ 0.25 \\\\ -3.5 \\\\ -0.25 \\end{pmatrix}$\n3.  **应用边界条件**: $y = (\\mathbf{1}-m) \\odot Kx = [0, 0, 3.75, 0, 0, 0, 0]^\\top$\n\n#### 测试用例 3:\n- 参数: $\\mu_0 = 1, \\mu_1 = 1$, $A_0 = 1, A_1 = 1$\n- 输入: $x = [0, 0, 0, 0, 0, 3, -3]^\\top$\n1.  **构建 $D$**: $D = I_{2\\times2}$。\n2.  **计算 $Kx = C^\\top C x$**:\n    - 聚合: $z = Cx = \\begin{pmatrix} 0+0-0-3 \\\\ 0-0-0-3 \\end{pmatrix} = \\begin{pmatrix} -3 \\\\ -3 \\end{pmatrix}$\n    - 变换: $w = z = \\begin{pmatrix} -3 \\\\ -3 \\end{pmatrix}$\n    - 传播: $Kx = C^\\top w = \\begin{pmatrix} -3 \\\\ -3 \\\\ -3 - (-3) \\\\ -(-3) \\\\ -(-3) \\\\ -(-3) \\\\ -3 \\end{pmatrix} = \\begin{pmatrix} -3 \\\\ -3 \\\\ 0 \\\\ 3 \\\\ 3 \\\\ 3 \\\\ -3 \\end{pmatrix}$\n3.  **应用边界条件**: $y = (\\mathbf{1}-m) \\odot Kx = [0, 0, 0.0, 0, 0, 0, 0]^\\top$\n\n这些计算结果将由提供的 Python 代码实现，并以指定的格式输出。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the output of a single-layer graph-neural-network-like operator\n    that mirrors the FEM assembly of a curl-curl bilinear form.\n    \"\"\"\n    # Define the mesh and problem geometry.\n    # The cell-edge incidence matrix C is 2x7.\n    # Rows correspond to cells k=0 (left) and k=1 (right).\n    # Columns correspond to edges ordered as [e0, e1, e2, e4, e5, e6, e7].\n    C = np.array([\n        [1, 0, 1, -1, 0, -1, 0],  # Cell k=0\n        [0, 1, -1, 0, -1, 0, 1]   # Cell k=1\n    ], dtype=np.float64)\n\n    # Cell areas.\n    A0, A1 = 1.0, 1.0\n\n    # Boundary mask m: 1 for boundary edges, 0 for interior edges.\n    # The unique interior edge is e2, at index 2.\n    m = np.array([1, 1, 0, 1, 1, 1, 1], dtype=np.float64)\n\n    # Selector for interior edges, (1 - m).\n    interior_selector = 1.0 - m\n\n    # Define the test cases.\n    test_cases = [\n        {\n            \"mu0\": 1.0, \"mu1\": 1.0,\n            \"x\": np.array([1.0, 2.0, -1.0, 0.5, -0.5, 0.0, 0.0], dtype=np.float64)\n        },\n        {\n            \"mu0\": 0.5, \"mu1\": 2.0,\n            \"x\": np.array([-0.25, 0.75, 1.0, -1.25, 0.0, 0.25, -0.25], dtype=np.float64)\n        },\n        {\n            \"mu0\": 1.0, \"mu1\": 1.0,\n            \"x\": np.array([0.0, 0.0, 0.0, 0.0, 0.0, 3.0, -3.0], dtype=np.float64)\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        mu0, mu1 = case[\"mu0\"], case[\"mu1\"]\n        x = case[\"x\"]\n\n        # 1. Construct the diagonal matrix D of material/geometric properties.\n        D = np.diag([1.0 / (mu0 * A0), 1.0 / (mu1 * A1)])\n\n        # 2. Assemble the stiffness matrix K = C^T * D * C.\n        K = C.T @ D @ C\n        \n        # 3. Apply the linear operator: Kx.\n        Kx = K @ x\n\n        # 4. Apply the boundary condition by elementwise-multiplying with the\n        #    interior edge selector.\n        y = interior_selector * Kx\n\n        # 5. Round the results to six decimal places and format as a list.\n        y_rounded = [round(val, 6) for val in y]\n        results.append(y_rounded)\n\n    # Final print statement in the exact required format.\n    # The format is a string representation of a list of lists.\n    # str() on a list automatically adds spaces, e.g., '[1.0, 2.0]'.\n    # A manual format is used here to avoid spaces after commas.\n    print(f\"[{','.join(str(res).replace(' ', '') for res in results)}]\")\n\nsolve()\n```", "id": "3327879"}]}