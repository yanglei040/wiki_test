{"hands_on_practices": [{"introduction": "自适应MCMC方法的核心是学习一个高效的提议分布。本练习将探讨一个根本问题：对于高斯目标分布，理想的提议协方差矩阵应该具备何种结构和尺度？通过仿射不变性论证和扩散极限启发式，你将推导出最优提议协方差应与目标协方差成正比，并确定在最大化采样器效率下的最优缩放因子。[@problem_id:3353620]", "problem": "考虑 Haario–Saksman–Tamminen 自适应 Metropolis (AM) 算法，该算法的目标是一个协方差矩阵为 $\\Sigma_{\\star}$ 的 $d$ 维零均值高斯分布，即目标密度 $\\pi(x)$ 对于 $x\\in\\mathbb{R}^{d}$ 正比于 $\\exp\\!\\big(-\\tfrac{1}{2} x^{\\top}\\Sigma_{\\star}^{-1}x\\big)$。在每次迭代中，AM 算法使用一个形式为 $q(x,\\cdot)=\\mathcal{N}\\!\\big(x,\\;S\\big)$ 的对称高斯随机游走提议，其中 $S$ 是一个随时间自适应调整的正定提议协方差矩阵。\n\n从 Metropolis–Hastings 算法的基本性质和高斯目标在线性变换下的仿射不变性出发，论证（无需引用任何特定的最优尺度缩放公式）$S$ 的渐近最优结构是 $\\Sigma_{\\star}$ 的一个标量倍数，即 $S=s_{d}^{2}\\Sigma_{\\star}$，其中标量 $s_{d}>0$ 取决于维度 $d$。然后，使用高维随机游走 Metropolis (RWM) 的扩散极限启发式方法，推导接受概率作为重缩放步长参数的函数的极限形式，并通过最大化期望平方跳跃距离，确定最优标量 $s_{d}$ 作为 $d$ 的显式函数。\n\n你的最终答案必须是 $s_{d}$ 关于 $d$ 的单个闭式解析表达式。将数值前置因子四舍五入至三位有效数字。本问题不涉及物理单位。", "solution": "该问题要求关于针对高斯目标的自适应 Metropolis (AM) 算法的两个主要结果。首先，利用仿射不变性论证来推断提议协方差矩阵 $S$ 的最优结构。其次，在高维极限下推导该协方差矩阵的最优尺度缩放因子。\n\n### 第一部分：提议协方差的最优结构\n\n目标密度为 $\\pi(x) \\propto \\exp(-\\frac{1}{2} x^{\\top}\\Sigma_{\\star}^{-1}x)$，其中 $x \\in \\mathbb{R}^d$。提议是一个高斯随机游走，$x' = x + \\xi$，其中 $\\xi \\sim \\mathcal{N}(0, S)$。Metropolis-Hastings 接受概率为 $\\alpha(x, x') = \\min\\left(1, \\frac{\\pi(x')}{\\pi(x)}\\right)$。\n\n我们援引仿射不变性原理。采样算法的效率不应依赖于状态空间 $\\mathbb{R}^d$ 的基的选择。考虑一个任意的可逆线性变换 $y = Ax$，其中 $A$ 是一个 $d \\times d$ 矩阵。\n如果 $x \\sim \\mathcal{N}(0, \\Sigma_{\\star})$，则变换后的变量 $y$ 服从高斯分布 $y \\sim \\mathcal{N}(0, A\\Sigma_{\\star}A^{\\top})$。$y$ 的目标密度为 $\\pi_y(y) \\propto \\exp(-\\frac{1}{2} y^{\\top}(A\\Sigma_{\\star}A^{\\top})^{-1}y)$。\n\n在 $y$ 空间中与 $x$ 空间中相对应的提议机制是 $y' = Ax' = A(x+\\xi) = y + A\\xi$。$y$ 空间中的提议增量是 $\\xi_y = A\\xi$。由于 $\\xi \\sim \\mathcal{N}(0, S)$，变换后的增量服从 $\\xi_y \\sim \\mathcal{N}(0, ASA^{\\top})$。因此，$y$ 空间中的提议协方差是 $S_y = ASA^{\\top}$。\n\n原始链的接受概率是目标密度对数比的函数：\n$$ \\ln\\left(\\frac{\\pi(x')}{\\pi(x)}\\right) = -\\frac{1}{2}\\left( (x+\\xi)^{\\top}\\Sigma_{\\star}^{-1}(x+\\xi) - x^{\\top}\\Sigma_{\\star}^{-1}x \\right) = -x^{\\top}\\Sigma_{\\star}^{-1}\\xi - \\frac{1}{2}\\xi^{\\top}\\Sigma_{\\star}^{-1}\\xi $$\n变换后链的接受概率取决于类似的数量：\n$$ \\ln\\left(\\frac{\\pi_y(y')}{\\pi_y(y)}\\right) = -y^{\\top}(A\\Sigma_{\\star}A^{\\top})^{-1}\\xi_y - \\frac{1}{2}\\xi_y^{\\top}(A\\Sigma_{\\star}A^{\\top})^{-1}\\xi_y $$\n代入 $y = Ax$，$\\xi_y = A\\xi$ 和 $(A\\Sigma_{\\star}A^{\\top})^{-1} = (A^{\\top})^{-1}\\Sigma_{\\star}^{-1}A^{-1}$：\n$$ \\ln\\left(\\frac{\\pi_y(y')}{\\pi_y(y)}\\right) = -(Ax)^{\\top}(A^{\\top})^{-1}\\Sigma_{\\star}^{-1}A^{-1}(A\\xi) - \\frac{1}{2}(A\\xi)^{\\top}(A^{\\top})^{-1}\\Sigma_{\\star}^{-1}A^{-1}(A\\xi) $$\n$$ = -x^{\\top}A^{\\top}(A^{\\top})^{-1}\\Sigma_{\\star}^{-1}A^{-1}A\\xi - \\frac{1}{2}\\xi^{\\top}A^{\\top}(A^{\\top})^{-1}\\Sigma_{\\star}^{-1}A^{-1}A\\xi $$\n$$ = -x^{\\top}\\Sigma_{\\star}^{-1}\\xi - \\frac{1}{2}\\xi^{\\top}\\Sigma_{\\star}^{-1}\\xi = \\ln\\left(\\frac{\\pi(x')}{\\pi(x)}\\right) $$\n接受概率是相同的。这意味着算法的性能指标（如接受率和自相关时间）在仿射变换下是不变的，只要提议协方差相应地变换为 $S \\to ASA^{\\top}$。\n\n对于一个学习提议协方差 $S$ 的自适应算法，很自然地会要求所学到的最优协方差尊重这种不变性。在目标分布不相关且在所有方向上都具有单位方差的坐标系中，即各向同性 (isotropic) 的情况下，问题变得最简单。我们可以通过选择一个变换矩阵 $A$ 来实现这一点，使得 $A\\Sigma_{\\star}A^{\\top} = I_d$，即 $d \\times d$ 的单位矩阵。例如，我们可以使用白化变换 $A = \\Sigma_{\\star}^{-1/2}$。在这个白化空间中，目标是 $\\mathcal{N}(0, I_d)$。\n\n对于一个各向同性的目标，没有理由偏好某个方向。任何对状态空间的有效探索也应该是各向同性的。因此，白化空间中的最优提议协方差必须与单位矩阵成正比：$S_y = c I_d$，其中标量 $c > 0$。\n\n将这个最优提议变换回原始的 $x$ 空间，得到最优 $S$ 的结构：\n$S_y = ASA^{\\top} \\implies S = A^{-1} S_y (A^{\\top})^{-1}$\n使用 $A = \\Sigma_{\\star}^{-1/2}$（所以 $A^{-1} = \\Sigma_{\\star}^{1/2}$）和 $S_y = c I_d$：\n$$ S = \\Sigma_{\\star}^{1/2} (c I_d) (\\Sigma_{\\star}^{-1/2})^{\\top} = c \\Sigma_{\\star}^{1/2} (\\Sigma_{\\star}^{1/2})^{\\top} = c \\Sigma_{\\star} $$\n因此，渐近最优的提议协方差矩阵 $S$ 必须是目标协方差矩阵 $\\Sigma_{\\star}$ 的一个标量倍数。我们将这个正的标量比例常数表示为 $s_d^2$，所以 $S = s_d^2 \\Sigma_{\\star}$。\n\n### 第二部分：最优尺度缩放因子 $s_d$\n\n基于上述论证，我们可以在标准化空间中分析算法的性能，其中目标是 $\\pi(y) \\propto \\exp(-\\frac{1}{2}y^{\\top}y)$，提议是 $\\mathcal{N}(y, s_d^2 I_d)$，其结果将适用于一般情况。为简化起见，本节我们使用 $x$ 代替 $y$。提议是 $x' = x + \\xi$，其中 $\\xi \\sim \\mathcal{N}(0, s_d^2 I_d)$。\n\n我们使用高维 ($d \\to \\infty$) 扩散极限启发式方法。为使接受率保持非零，步长必须趋于零。我们使用尺度缩放拟设 $s_d = \\lambda / \\sqrt{d}$，其中常数 $\\lambda > 0$。因此，提议为 $\\xi \\sim \\mathcal{N}(0, (\\lambda^2/d)I_d)$。\n\n对数目标的变为 $\\Delta E(x, \\xi) = \\frac{1}{2}\\|x\\|^2 - \\frac{1}{2}\\|x'\\|^2 = -x^{\\top}\\xi - \\frac{1}{2}\\|\\xi\\|^2$。接受概率为 $\\alpha(x, x') = \\min(1, \\exp(\\Delta E))$。我们来分析 $\\Delta E$ 中的两项：\n1.  根据大数定律，当 $d \\to \\infty$ 时，$\\|\\xi\\|^2 = \\sum_{i=1}^d \\xi_i^2$ 收敛于其期望值：\n    $$ E[\\|\\xi\\|^2] = E\\left[\\sum_{i=1}^d \\xi_i^2\\right] = d \\cdot E[\\xi_1^2] = d \\cdot \\text{Var}(\\xi_1) = d \\cdot \\frac{\\lambda^2}{d} = \\lambda^2 $$\n    所以，$\\|\\xi\\|^2 \\to \\lambda^2$。\n2.  $x^{\\top}\\xi = \\sum_{i=1}^d x_i \\xi_i$ 这一项是独立同分布 (i.i.d.) 项的和，其中 $x_i \\sim \\mathcal{N}(0, 1)$ （因为 $x$ 来自平稳分布）且 $\\xi_i \\sim \\mathcal{N}(0, \\lambda^2/d)$。每一项的均值为 $E[x_i\\xi_i] = E[x_i]E[\\xi_i] = 0$，方差为 $\\text{Var}(x_i\\xi_i) = E[x_i^2]E[\\xi_i^2] = 1 \\cdot (\\lambda^2/d) = \\lambda^2/d$。\n    该和的方差是 $\\text{Var}(x^{\\top}\\xi) = d \\cdot (\\lambda^2/d) = \\lambda^2$。根据中心极限定理，$x^{\\top}\\xi$ 在分布上收敛于一个正态随机变量 $Z \\sim \\mathcal{N}(0, \\lambda^2)$。\n\n在 $d\\to\\infty$ 的极限下，接受概率成为 $\\lambda$ 的一个确定性函数，这是通过对随机分量的极限分布进行平均得到的：\n$$ \\alpha(\\lambda) = E_Z[\\min(1, \\exp(-Z - \\frac{1}{2}\\lambda^2))] $$\n令 $Y = -Z - \\frac{1}{2}\\lambda^2$。由于 $Z \\sim \\mathcal{N}(0, \\lambda^2)$，所以 $Y \\sim \\mathcal{N}(-\\frac{1}{2}\\lambda^2, \\lambda^2)$。期望为\n$$ \\alpha(\\lambda) = \\int_{-\\infty}^{\\infty} \\min(1, e^y) p(y) dy = \\int_{-\\infty}^{0} e^y p(y) dy + \\int_{0}^{\\infty} p(y) dy $$\n其中 $p(y)$ 是 $Y$ 的密度。令 $\\Phi$ 为标准正态累积分布函数 (CDF)。第二项是 $P(Y > 0) = P\\left(\\mathcal{N}(-\\frac{1}{2}\\lambda^2, \\lambda^2) > 0\\right) = P\\left(\\mathcal{N}(0, 1) > \\frac{\\lambda}{2}\\right) = 1 - \\Phi(\\lambda/2) = \\Phi(-\\lambda/2)$。\n可以证明，第一个积分的计算结果也是 $\\Phi(-\\lambda/2)$。因此，极限接受率为\n$$ \\alpha(\\lambda) = 2\\Phi(-\\lambda/2) $$\n\n为了找到最优的 $\\lambda$，我们最大化采样器的效率，该效率与每次迭代的期望平方跳跃距离 (ESJD) 成正比。跳跃是 $x_{n+1}-x_n$，在接受时为 $\\xi$，在拒绝时为 $0$。\n$$ \\text{ESJD} = E[\\|x_{n+1}-x_n\\|^2] = E[\\alpha(x_n, x_n') \\|\\xi\\|^2] $$\n在高维极限下，接受概率和提议步长的范数渐近独立。\n$$ \\text{ESJD}(\\lambda) \\approx E[\\alpha(x_n, x_n')] E[\\|\\xi\\|^2] = \\alpha(\\lambda) E[\\|\\xi\\|^2] $$\n如前所示，$E[\\|\\xi\\|^2] = \\lambda^2$。所以我们要最大化函数 $f(\\lambda) = \\lambda^2 \\alpha(\\lambda) = 2\\lambda^2 \\Phi(-\\lambda/2)$。我们通过将其关于 $\\lambda$ 的导数设为零来找到最大值：\n$$ \\frac{df}{d\\lambda} = \\frac{d}{d\\lambda} \\left[2\\lambda^2 \\Phi(-\\lambda/2)\\right] = 4\\lambda\\Phi(-\\lambda/2) + 2\\lambda^2 \\frac{d}{d\\lambda}\\Phi(-\\lambda/2) = 0 $$\n使用链式法则，并令 $\\phi$ 为标准正态概率密度函数 (PDF)，$\\frac{d}{d\\lambda}\\Phi(-\\lambda/2) = \\phi(-\\lambda/2) \\cdot (-\\frac{1}{2}) = -\\frac{1}{2}\\phi(\\lambda/2)$，因为 $\\phi$ 是一个偶函数。\n$$ 4\\lambda\\Phi(-\\lambda/2) - \\lambda^2\\phi(\\lambda/2) = 0 $$\n对于 $\\lambda > 0$，我们可以除以 $\\lambda$：\n$$ 4\\Phi(-\\lambda/2) = \\lambda\\phi(\\lambda/2) $$\n这是一个关于最优 $\\lambda$ 的超越方程，我们将其标记为 $\\lambda_{opt}$。该方程无法用初等函数求解。数值解给出 $\\lambda_{opt} \\approx 2.379$。问题要求四舍五入到三位有效数字，所以 $\\lambda_{opt} \\approx 2.38$。\n\n那么，最优标量 $s_d$ 由我们的尺度缩放拟设给出：\n$$ s_d = \\frac{\\lambda_{opt}}{\\sqrt{d}} \\approx \\frac{2.38}{\\sqrt{d}} $$\n这给出了 $s_d$ 对维度 $d$ 的显式函数依赖关系。", "answer": "$$ \\boxed{\\frac{2.38}{\\sqrt{d}}} $$", "id": "3353620"}, {"introduction": "理解提议步长的缩放法则是掌握自适应Metropolis算法的关键。本练习要求你从第一性原理出发，推导随机游走Metropolis算法在平稳状态下的平均接受率，并分析其在高维极限下的行为。通过这个过程，你将深刻理解为何提议方差按 $1/d$ 比例缩放对于在维度 $d$ 趋于无穷时维持一个恒定且不为零的接受率至关重要。[@problem_id:3353654]", "problem": "考虑一个$d$维随机游走 Metropolis 算法，其目标分布为一个多元正态分布 $\\pi = \\mathcal{N}(0, \\Sigma_{\\star})$，提议分布为对称的 $q(\\cdot \\mid x) = \\mathcal{N}(x, s^{2}\\Sigma_{\\star})$，其中 $s > 0$ 是一个标量缩放参数。在平稳状态下，设 $x \\sim \\pi$ 且 $y \\sim q(\\cdot \\mid x)$；接受概率为 $\\alpha(x,y) = \\min\\{1, \\pi(y)/\\pi(x)\\}$。仅使用 Metropolis-Hastings 接受概率的定义和多元正态分布的性质，从第一性原理出发，推导平稳平均接受率\n$$\n\\alpha_{d}(s) \\equiv \\mathbb{E}[\\alpha(x,y)]\n$$\n的一个精确一维条件表示，该表示应为一个关于标准正态向量欧几里得范数的期望。然后，在高维标度 $s = \\ell/\\sqrt{d}$（其中 $\\ell \\in (0,\\infty)$ 为固定值）下，使用大数定律论证来计算极限平稳接受率\n$$\n\\lim_{d \\to \\infty} \\alpha_{d}\\!\\left(\\frac{\\ell}{\\sqrt{d}}\\right)\n$$\n得到一个关于 $\\ell$ 的闭式解析表达式。最后，简要解释这个极限表达式如何证明 Haario–Saksman–Tamminen 自适应 Metropolis 算法中标度选择 $s^{2} \\approx c^{2}/d$ 的合理性，其中链的经验协方差 $\\widehat{\\Sigma}$ 取代了 $\\Sigma_{\\star}$。\n\n请以关于 $\\ell$ 的单个解析表达式的形式提供最终答案。无需四舍五入。", "solution": "该问题陈述被评估为有效，因为它在科学上基于马尔可夫链蒙特卡洛方法的理论，特别是随机游走 Metropolis 算法。问题是适定的，所有必要的定义和条件都已给出，足以推导出一个唯一且有意义的解。语言客观明确。该问题是 MCMC 算法分析中的一个标准理论练习，并与其所述主题——Haario-Saksman-Tamminen 自适应 Metropolis 算法直接相关。\n\n我们开始推导。\n\n目标分布是一个 $d$ 维多元正态分布 $\\pi = \\mathcal{N}(0, \\Sigma_{\\star})$。其在点 $z \\in \\mathbb{R}^d$ 处的概率密度函数 (PDF) 由下式给出\n$$\n\\pi(z) = \\frac{1}{(2\\pi)^{d/2} \\det(\\Sigma_{\\star})^{1/2}} \\exp\\left(-\\frac{1}{2} z^\\top \\Sigma_{\\star}^{-1} z\\right)\n$$\n提议分布是一个对称随机游走，$q(y \\mid x) = \\mathcal{N}(x, s^2 \\Sigma_{\\star})$。这意味着提议状态 $y$ 是通过 $y = x + \\epsilon$ 生成的，其中扰动 $\\epsilon$ 从 $\\mathcal{N}(0, s^2 \\Sigma_{\\star})$ 中抽取。\n\nMetropolis-Hastings 接受概率为 $\\alpha(x,y) = \\min\\{1, R(x,y)\\}$，对于对称提议，比率 $R(x,y)$ 由下式给出\n$$\nR(x,y) = \\frac{\\pi(y)}{\\pi(x)}\n$$\n代入目标分布的 PDF，我们得到\n$$\nR(x,y) = \\frac{\\exp\\left(-\\frac{1}{2} y^\\top \\Sigma_{\\star}^{-1} y\\right)}{\\exp\\left(-\\frac{1}{2} x^\\top \\Sigma_{\\star}^{-1} x\\right)} = \\exp\\left(-\\frac{1}{2} \\left( y^\\top \\Sigma_{\\star}^{-1} y - x^\\top \\Sigma_{\\star}^{-1} x \\right) \\right)\n$$\n我们将 $y = x + \\epsilon$ 代入指数中的二次型：\n\\begin{align*}\ny^\\top \\Sigma_{\\star}^{-1} y - x^\\top \\Sigma_{\\star}^{-1} x = (x+\\epsilon)^\\top \\Sigma_{\\star}^{-1} (x+\\epsilon) - x^\\top \\Sigma_{\\star}^{-1} x \\\\\n= (x^\\top + \\epsilon^\\top) \\Sigma_{\\star}^{-1} (x+\\epsilon) - x^\\top \\Sigma_{\\star}^{-1} x \\\\\n= x^\\top \\Sigma_{\\star}^{-1} x + 2 x^\\top \\Sigma_{\\star}^{-1} \\epsilon + \\epsilon^\\top \\Sigma_{\\star}^{-1} \\epsilon - x^\\top \\Sigma_{\\star}^{-1} x \\\\\n= 2 x^\\top \\Sigma_{\\star}^{-1} \\epsilon + \\epsilon^\\top \\Sigma_{\\star}^{-1} \\epsilon\n\\end{align*}\n这里我们使用了 $\\Sigma_{\\star}^{-1}$ 的对称性。该比率变为\n$$\nR(x,y) = \\exp\\left(-x^\\top \\Sigma_{\\star}^{-1} \\epsilon - \\frac{1}{2} \\epsilon^\\top \\Sigma_{\\star}^{-1} \\epsilon\\right)\n$$\n平均平稳接受率是 $\\alpha_d(s) = \\mathbb{E}[\\alpha(x,y)]$，其中期望是针对 $x$ 的平稳分布以及给定 $x$ 时 $y$ 的提议分布来计算的。这意味着 $x \\sim \\mathcal{N}(0, \\Sigma_{\\star})$ 并且独立地，$\\epsilon \\sim \\mathcal{N}(0, s^2 \\Sigma_{\\star})$。\n\n为了简化表达式，我们进行变量替换。令 $A = \\Sigma_{\\star}^{-1/2}$ 为逆协方差矩阵的对称平方根。定义变换后的变量：\n$$\n\\tilde{x} = A x \\quad \\text{and} \\quad \\tilde{\\epsilon} = A \\epsilon\n$$\n如果 $x \\sim \\mathcal{N}(0, \\Sigma_{\\star})$，则 $\\tilde{x} \\sim \\mathcal{N}(0, A \\Sigma_{\\star} A^\\top) = \\mathcal{N}(0, I_d)$，这是一个标准的 $d$ 维正态分布。\n如果 $\\epsilon \\sim \\mathcal{N}(0, s^2 \\Sigma_{\\star})$，则 $\\tilde{\\epsilon} \\sim \\mathcal{N}(0, A (s^2 \\Sigma_{\\star}) A^\\top) = \\mathcal{N}(0, s^2 I_d)$。这意味着我们可以写成 $\\tilde{\\epsilon} = s Z$，其中 $Z \\sim \\mathcal{N}(0, I_d)$ 是一个标准的 $d$ 维正态向量，且独立于 $\\tilde{x}$。\n\n指数中的项现在可以重写为：\n$$\nx^\\top \\Sigma_{\\star}^{-1} \\epsilon = (A x)^\\top (A \\epsilon) = \\tilde{x}^\\top \\tilde{\\epsilon} = s \\tilde{x}^\\top Z\n$$\n$$\n\\epsilon^\\top \\Sigma_{\\star}^{-1} \\epsilon = (A \\epsilon)^\\top (A \\epsilon) = \\tilde{\\epsilon}^\\top \\tilde{\\epsilon} = s^2 Z^\\top Z = s^2 \\|Z\\|^2\n$$\n比率 $R$ 现在取决于两个独立标准正态向量 $\\tilde{x}$ 和 $Z$：\n$$\nR = \\exp\\left(-s \\tilde{x}^\\top Z - \\frac{1}{2} s^2 \\|Z\\|^2\\right)\n$$\n期望现在是关于 $\\tilde{x}, Z \\sim \\mathcal{N}(0, I_d)$ 计算的。我们来分析 $\\tilde{x}^\\top Z$ 这一项。以 $Z$ 为条件，这是独立标准正态随机变量的线性组合，即 $\\sum_{i=1}^d Z_i \\tilde{x}_i$。因此，它是一个正态随机变量，其均值为 $\\mathbb{E}[\\tilde{x}^\\top Z \\mid Z] = 0$，方差为 $\\text{Var}(\\tilde{x}^\\top Z \\mid Z) = Z^\\top \\text{Var}(\\tilde{x}) Z = Z^\\top I_d Z = \\|Z\\|^2$。\n因此，我们可以写成 $\\tilde{x}^\\top Z = \\|Z\\| W$，其中 $W \\sim \\mathcal{N}(0,1)$ 是一个标准正态变量，独立于 $Z$。\n\n令 $V = s \\|Z\\|$。该比率变为 $V$ 和 $W$ 的函数：\n$$\nR = \\exp(-V W - V^2/2)\n$$\n以 $Z$（因此以 $V$）为条件的接受概率是 $\\mathbb{E}_W[\\min\\{1, R\\}]$。我们来计算这个内层期望：\n\\begin{align*}\n\\mathbb{E}_W[\\min\\{1, \\exp(-V W - V^2/2)\\}] = \\int_{-\\infty}^{\\infty} \\min\\{1, \\exp(-V w - V^2/2)\\} \\frac{1}{\\sqrt{2\\pi}} \\exp(-w^2/2) \\, dw \\\\\n= \\int_{-V/2}^{\\infty} \\exp(-V w - V^2/2) \\frac{1}{\\sqrt{2\\pi}} \\exp(-w^2/2) \\, dw + \\int_{-\\infty}^{-V/2} 1 \\cdot \\frac{1}{\\sqrt{2\\pi}} \\exp(-w^2/2) \\, dw\n\\end{align*}\n第二个积分就是概率 $\\mathbb{P}(W \\le -V/2) = \\Phi(-V/2)$，其中 $\\Phi$ 是标准正态分布的累积分布函数（CDF）。\n对于第一个积分，我们合并指数部分：\n$$\n-Vw - \\frac{V^2}{2} - \\frac{w^2}{2} = -\\frac{1}{2}(w^2 + 2Vw + V^2) = -\\frac{1}{2}(w+V)^2\n$$\n积分为 $\\int_{-V/2}^{\\infty} \\frac{1}{\\sqrt{2\\pi}} \\exp(-\\frac{1}{2}(w+V)^2) \\, dw$。令 $u = w+V$，则 $du = dw$。积分下限变为 $u = -V/2 + V = V/2$。该积分为：\n$$\n\\int_{V/2}^{\\infty} \\frac{1}{\\sqrt{2\\pi}} \\exp(-u^2/2) \\, du = \\mathbb{P}(U > V/2) = 1 - \\Phi(V/2) = \\Phi(-V/2)\n$$\n其中 $U \\sim \\mathcal{N}(0,1)$。\n因此，以 $Z$ 为条件的接受概率是 $\\Phi(-V/2) + \\Phi(-V/2) = 2 \\Phi(-V/2)$。将 $V = s\\|Z\\|$ 代回，我们得到 $2\\Phi(-s\\|Z\\|/2)$。\n\n为了求得无条件的平均接受率 $\\alpha_d(s)$，我们对 $Z \\sim \\mathcal{N}(0, I_d)$ 取期望。这就得到了所要求的一维条件表示：\n$$\n\\alpha_d(s) = \\mathbb{E}_{Z \\sim \\mathcal{N}(0, I_d)} [2 \\Phi(-s\\|Z\\|/2)]\n$$\n\n接下来，我们计算在高维标度 $s = \\ell/\\sqrt{d}$（其中 $\\ell \\in (0, \\infty)$ 为固定常数）下的极限。\n$$\n\\lim_{d \\to \\infty} \\alpha_d\\left(\\frac{\\ell}{\\sqrt{d}}\\right) = \\lim_{d \\to \\infty} \\mathbb{E} \\left[ 2 \\Phi\\left(-\\frac{\\ell}{\\sqrt{d}} \\frac{\\|Z\\|}{2}\\right) \\right] = 2 \\lim_{d \\to \\infty} \\mathbb{E} \\left[ \\Phi\\left(-\\frac{\\ell}{2} \\frac{\\|Z\\|}{\\sqrt{d}}\\right) \\right]\n$$\n我们来分析 $\\|Z\\|/\\sqrt{d}$ 这一项。我们有 $\\|Z\\|^2 = \\sum_{i=1}^d Z_i^2$，其中 $Z_i$ 是独立同分布的 $\\mathcal{N}(0,1)$ 随机变量。根据大数定律，样本均值依概率收敛于真实均值：\n$$\n\\frac{1}{d} \\|Z\\|^2 = \\frac{1}{d} \\sum_{i=1}^d Z_i^2 \\xrightarrow{p} \\mathbb{E}[Z_1^2] \\quad \\text{as } d \\to \\infty\n$$\n期望 $\\mathbb{E}[Z_1^2] = \\text{Var}(Z_1) + (\\mathbb{E}[Z_1])^2 = 1 + 0^2 = 1$。\n因此，$\\|Z\\|^2/d \\xrightarrow{p} 1$。由于平方根函数 $g(x) = \\sqrt{x}$ 在 $x > 0$ 上是连续的，根据连续映射定理，\n$$\n\\frac{\\|Z\\|}{\\sqrt{d}} = \\sqrt{\\frac{\\|Z\\|^2}{d}} \\xrightarrow{p} \\sqrt{1} = 1 \\quad \\text{as } d \\to \\infty\n$$\n因此，$\\Phi$ 函数的自变量 $-\\frac{\\ell}{2} \\frac{\\|Z\\|}{\\sqrt{d}}$ 依概率收敛到 $-\\ell/2$。\n函数 $\\Phi(x)$ 是连续且有界的（介于 0 和 1 之间）。因此，被积函数 $\\Phi(\\cdot)$ 也是有界的。根据有界收敛定理（控制收敛定理的一个特例），我们可以交换极限和期望的顺序：\n$$\n\\lim_{d \\to \\infty} \\mathbb{E} \\left[ \\Phi\\left(-\\frac{\\ell}{2} \\frac{\\|Z\\|}{\\sqrt{d}}\\right) \\right] = \\mathbb{E} \\left[ \\lim_{d \\to \\infty, p} \\Phi\\left(-\\frac{\\ell}{2} \\frac{\\|Z\\|}{\\sqrt{d}}\\right) \\right] = \\mathbb{E} \\left[ \\Phi\\left(-\\frac{\\ell}{2}\\right) \\right]\n$$\n由于 $-\\ell/2$ 是一个常数，期望就是其本身的值，即 $\\Phi(-\\ell/2)$。\n因此，极限平稳接受率为：\n$$\n\\lim_{d \\to \\infty} \\alpha_d\\left(\\frac{\\ell}{\\sqrt{d}}\\right) = 2 \\Phi(-\\ell/2)\n$$\n\n最后，我们解释自适应 Metropolis (AM) 算法中标度选择的理由。推导出的极限 $\\alpha(\\ell) = 2 \\Phi(-\\ell/2)$ 表明，为了在维度 $d \\to \\infty$ 时保持一个非平凡的（即不为 0 或 1）常数接受率，提议分布的标度参数 $s$ 必须是 $d^{-1/2}$ 阶的。具体来说，提议方差 $s^2 \\Sigma_{\\star}$ 必须按照 $s^2 \\propto 1/d$ 进行缩放。如果 $s$ 是常数，接受率将收敛到 0；如果 $s$ 的缩放速度快于 $d^{-1/2}$（例如，$1/d$），接受率将收敛到 1。\n\n对于包括本问题在内的一类特定问题，理论表明，最优极限接受率约为 0.234，此时采样器的效率最高。通过将我们的极限表达式设为这个值，我们可以找到最优的 $\\ell$：\n$$\n2 \\Phi(-\\ell/2) = 0.234 \\implies \\Phi(-\\ell/2) = 0.117\n$$\n使用标准正态分布的逆累积分布函数，$\\Phi^{-1}(0.117) \\approx -1.19$。这得到 $-\\ell/2 \\approx -1.19$，或 $\\ell \\approx 2.38$。\n相应的最优提议方差标度为 $s^2 = \\ell^2/d \\approx (2.38)^2/d$。\n\nHaario-Saksman-Tamminen AM 算法使用提议分布 $q(y \\mid x) = \\mathcal{N}(x, s_d^2 \\widehat{\\Sigma})$，其中 $\\widehat{\\Sigma}$ 是马尔可夫链历史的经验协方差，作为未知真实协方差 $\\Sigma_{\\star}$ 的自适应替代品。标度因子 $s_d^2$ 被设置为 $c^2/d$，其中常数 $c$ 是基于此理论结果选择的。标准选择是 $c = 2.38$（或通常四舍五入为 2.4）。这种标度确保了当算法探索高维空间时，它能保持一个稳定且高效的接受率，防止链陷入停滞（接受率太低）或混合过慢（接受率太高）。该理论推导为这一关键的算法设计选择提供了根本性的依据。", "answer": "$$\n\\boxed{2 \\Phi\\left(-\\frac{\\ell}{2}\\right)}\n$$", "id": "3353654"}, {"introduction": "理论上优美的算法在实践中往往面临计算成本的挑战，在高维空间中尤其如此。本练习将你的注意力从统计理论转向计算效率，要求你为AM算法的每次迭代建立一个精确的计算成本模型。你将分析并量化诸如周期性协方差更新和低秩近似等策略如何有效降低计算负担，从而深入理解在大规模问题中实现算法可扩展性所需做的权衡。[@problem_id:3353639]", "problem": "考虑由 Haario、Saksman 和 Tamminen 提出的自适应 Metropolis (AM) 算法，这是一种马尔可夫链蒙特卡洛 (MCMC) 方法，其目标分布位于 $\\mathbb{R}^{d}$ 上，在第 $n$ 次迭代时的状态为 $X_{n} \\in \\mathbb{R}^{d}$。AM 算法的提议形式为 $Y_{n} = X_{n} + s L_{n} Z_{n}$，其中 $s > 0$ 是一个标量，$Z_{n} \\sim \\mathcal{N}(0, I_{d})$，$L_{n}$ 是一个 Cholesky 因子，使得 $C_{n} = L_{n} L_{n}^{\\top}$ 近似于目标协方差。协方差更新基于链的经验协方差和一个 Robbins-Monro 步长 $\\eta_{n} \\in (0,1)$，并带有一个小的对角正则化项。设均值更新为 $ \\mu_{n+1} = \\mu_{n} + \\eta_{n} (X_{n} - \\mu_{n})$，协方差更新为\n$$\nC_{n+1} = (1 - \\eta_{n}) C_{n} + \\eta_{n} \\left( (X_{n} - \\mu_{n})(X_{n} - \\mu_{n})^{\\top} \\right) + \\eta_{n} \\epsilon I_{d},\n$$\n其中 $\\epsilon > 0$ 是一个很小的常数。假设一个标准的浮点运算成本模型，其中一个稠密的 $d \\times d$ 矩阵与向量的乘法成本为 $2 d^{2}$ flops，一个下三角 $d \\times d$ 矩阵与向量的乘法成本恰好为 $d^{2}$ flops，两个 $d$ 维向量的稠密外积成本为 $d^{2}$ flops，一个 $d \\times d$ 矩阵的加法或缩放成本为 $d^{2}$ flops，一个 $d$ 维向量的加法或缩放成本为 $d$ flops，一个稠密的对称正定 $d \\times d$ 矩阵的 Cholesky 分解成本为 $\\frac{1}{3} d^{3}$ flops。忽略随机数生成成本和目标密度评估成本；只关注提议生成和自适应中的线性代数运算量。\n\n提出了两种降低成本的策略：\n\n- 周期性协方差分解：并非每次迭代都重新计算 $L_{n}$，而是每 $m \\in \\mathbb{N}$ 次迭代重新计算一次 $C_{n}$ 的完整 Cholesky 分解，并在其间重用最近的 $L_{n}$。将 Cholesky 分解的成本摊销到 $m$ 次迭代中，视为每次迭代 $\\frac{1}{3} d^{3} / m$ flops。\n\n- 低秩协方差近似：维持一个近似 $C_{n} \\approx D_{n} + U_{n} U_{n}^{\\top}$，其中 $D_{n} \\in \\mathbb{R}^{d \\times d}$ 是对角矩阵，$U_{n} \\in \\mathbb{R}^{d \\times r}$ 的秩为 $r$，且 $1 \\le r \\ll d$。对于提议生成，抽取 $Z_{r} \\sim \\mathcal{N}(0, I_{r})$ 和 $Z_{d} \\sim \\mathcal{N}(0, I_{d})$，然后计算 $Y_{n} = X_{n} + s \\left( U_{n} Z_{r} + \\sqrt{D_{n}} \\odot Z_{d} \\right)$，其中 $\\odot$ 表示按元素乘法，$\\sqrt{D_{n}}$ 表示对角线元素的按元素平方根。对这种低秩提议使用以下运算次数：计算 $U_{n} Z_{r}$ 成本为 $2 d r$ flops，计算 $\\sqrt{D_{n}} \\odot Z_{d}$ 成本为 $d$ flops，向量加法和缩放增加 $3 d$ flops，因此总提议生成成本为 $2 d r + 4 d$ flops。对于低秩协方差更新，假设每次迭代的步骤和成本如下：以 $d$ flops 计算 $v_{n} = X_{n} - \\mu_{n}$，以 $2 d$ flops 更新 $\\mu_{n+1} = \\mu_{n} + \\eta_{n} v_{n}$，从秩-1 项的对角线以 $2 d$ flops 更新 $D_{n+1}$，并通过秩-$r$ 增量投影和扩充以 $2 d r + r^{2}$ flops 更新 $U_{n+1}$。因此，每次迭代的总低秩自适应成本为 $2 d r + r^{2} + 5 d$ flops。继续每 $m$ 次迭代周期性地对 $C_{n}$ 进行完整的 Cholesky 分解以防止长期漂移，因此上述的摊销周期性成本 $\\frac{1}{3} d^{3} / m$ 仍然适用。\n\n从这些基本定义和成本模型出发，推导出当两种策略（每次迭代都进行低秩提议和协方差更新，以及每 $m$ 次迭代进行一次完整的 Cholesky 重新计算）联合使用时，每次迭代的摊销浮点运算次数的闭式表达式，记为 $C_{\\mathrm{lr}}(d, m, r)$。将你的最终答案表示为关于 $d$、$m$ 和 $r$ 的单一解析表达式。不要简化为大-$\\mathcal{O}$ 记法；给出在所述 flops 模型下的精确表达式。不需要进行数值取整，答案中也不需要报告单位。", "solution": "自适应 Metropolis 算法通过从一个高斯分布中抽样来构建提议，该高斯分布的协方差与当前估计的协方差 $C_{n}$ 成正比。每次迭代的计算负担来自三个方面：使用协方差的一个因子生成提议、均值和协方差的自适应更新，以及偶尔对协方差进行重新分解以获得新的 Cholesky 因子。为了推导同时使用周期性分解和低秩协方差近似时每次迭代的摊销浮点运算次数的闭式表达式，我们将每个部分的贡献相加。\n\n在满秩设置下，基本的 AM 提议为 $Y_{n} = X_{n} + s L_{n} Z_{n}$，其中 $C_{n} = L_{n} L_{n}^{\\top}$ 且 $Z_{n} \\sim \\mathcal{N}(0, I_{d})$。一个下三角 $d \\times d$ 矩阵与一个 $d$ 维向量相乘的成本恰好是 $d^{2}$ flops（有 $d(d+1)/2$ 次乘法和 $d(d-1)/2$ 次加法），而缩放和相加 $d$ 维向量需要 $2 d$ flops，因此满秩提议采样的成本为 $d^{2} + 2 d$ flops。均值更新 $\\mu_{n+1} = \\mu_{n} + \\eta_{n} (X_{n} - \\mu_{n})$ 是一个向量操作，计算 $v_{n} = X_{n} - \\mu_{n}$ 需要 $d$ flops，缩放和加法需要 $2 d$ flops，总计 $3 d$ flops。协方差更新\n$$\nC_{n+1} = (1 - \\eta_{n}) C_{n} + \\eta_{n} \\left( v_{n} v_{n}^{\\top} \\right) + \\eta_{n} \\epsilon I_{d}\n$$\n需要以下运算：计算外积 $v_{n} v_{n}^{\\top}$ 需要 $d^{2}$ flops，将 $C_{n}$ 乘以 $(1 - \\eta_{n})$ 需要 $d^{2}$ flops，将两个 $d \\times d$ 矩阵相加需要 $d^{2}$ flops，以及加上对角正则化项需要 $d$ flops。加上用于均值更新的 $3 d$ flops，满秩情况下每次迭代的总自适应成本为 $3 d^{2} + 3 d$ flops。如果每次迭代都重新计算 Cholesky 分解 $L_{n}$，那么每次迭代的成本还将包括用于 Cholesky 分解的 $\\frac{1}{3} d^{3}$ flops；然而，如果每 $m$ 次迭代进行一次周期性分解，则每次迭代的摊销 Cholesky 成本为 $\\frac{1}{3} d^{3} / m$ flops。\n\n我们现在分析低秩策略。协方差近似为 $C_{n} \\approx D_{n} + U_{n} U_{n}^{\\top}$，其中 $D_{n}$ 是对角矩阵，$U_{n}$ 有 $r$ 列。提议使用两个独立的标准正态分布 $Z_{r} \\sim \\mathcal{N}(0, I_{r})$ 和 $Z_{d} \\sim \\mathcal{N}(0, I_{d})$，并计算\n$$\nY_{n} = X_{n} + s \\left( U_{n} Z_{r} + \\sqrt{D_{n}} \\odot Z_{d} \\right).\n$$\n计算 $U_{n} Z_{r}$（一个稠密的 $d \\times r$ 矩阵乘以一个 $r$ 维向量）的成本是 $2 d r$ flops；$\\sqrt{D_{n}} \\odot Z_{d}$ 是一个对角缩放，成本为 $d$ flops；三个 $d$ 维向量操作（两个分量的相加、乘以 $s$ 进行缩放，以及加到 $X_{n}$ 上）成本为 $3 d$ flops。因此，在低秩近似下，每次迭代的提议生成成本为 $2 d r + 4 d$ flops。\n\n对于低秩协方差更新，我们像之前一样保留了 $3 d$ flops 的均值更新成本，其中计算 $v_{n} = X_{n} - \\mu_{n}$ 需要 $d$ flops，缩放和加法需要 $2 d$ flops。对角矩阵 $D_{n}$ 捕获了低秩部分未表示的方差；用秩-1 项 $\\eta_{n} v_{n} v_{n}^{\\top}$ 的对角线更新 $D_{n}$ 需要对 $d$ 个条目进行元素级操作，模型化为 $2 d$ flops（一次用于计算 $v_{n} \\odot v_{n}$，一次用于缩放和加法）。更新因子 $U_{n}$ 以反映协方差的非对角结构，可以通过一个增量秩-$r$ 过程来完成（例如，将 $v_{n}$ 投影到 $U_{n}$ 的当前张成空间上，然后进行修正和截断），其主要成本是一个 $d \\times r$ 矩阵与一个 $r$ 维向量的乘法以及相关的较小的 $r \\times r$ 操作。我们将其模型化为每次迭代 $2 d r + r^{2}$ flops。将这些部分相加，每次迭代的低秩自适应成本为\n$$\n\\text{adapt}_{\\mathrm{lr}}(d, r) = d + 2 d + 2 d + (2 d r + r^{2}) = 2 d r + r^{2} + 5 d.\n$$\n\n为了防止在低秩近似中累积漂移，我们仍然每 $m$ 次迭代对当前的 $C_{n}$ 进行周期性的完整 Cholesky 分解。这种周期性分解的摊销成本为每次迭代 $\\frac{1}{3} d^{3} / m$ flops。\n\n结合联合策略的三个组成部分——低秩提议生成、低秩自适应和摊销的周期性完整 Cholesky 分解——每次迭代的总摊销浮点运算次数为\n$$\nC_{\\mathrm{lr}}(d, m, r) = \\underbrace{(2 d r + 4 d)}_{\\text{提议}} + \\underbrace{(2 d r + r^{2} + 5 d)}_{\\text{自适应}} + \\underbrace{\\frac{1}{3} \\frac{d^{3}}{m}}_{\\text{摊销的Cholesky}}.\n$$\n化简后，\n$$\nC_{\\mathrm{lr}}(d, m, r) = 4 d r + r^{2} + 9 d + \\frac{1}{3} \\frac{d^{3}}{m}.\n$$\n\n这是在使用秩为 $r$ 的低秩提议和协方差更新，并结合每 $m$ 次迭代进行一次周期性完整 Cholesky 重新计算时，在所述 flops 模型下的闭式摊销的每次迭代浮点运算次数。它明确了其中的权衡：降低 $r$ 会减少 $4 d r + r^{2}$ 项，增加 $m$ 会减少 $\\frac{1}{3} \\frac{d^{3}}{m}$ 项，而线性项 $9 d$ 反映了每次迭代中不可避免的 $d$ 维向量操作。", "answer": "$$\\boxed{4 d r + r^{2} + 9 d + \\frac{d^{3}}{3 m}}$$", "id": "3353639"}]}