{"hands_on_practices": [{"introduction": "重采样是粒子滤波的基石，用以防止样本退化。本练习将深入探讨一种高效的重采样方法——残差重采样（residual resampling）的统计特性。通过证明其条件无偏性，您将更深刻地理解重采样方法的有效性，及其如何保证蒙特卡洛估计的准确性 [@problem_id:3308535]。", "problem": "考虑一个离散时间状态空间模型，其潜过程为 $\\{X_{t}\\}_{t \\geq 0}$，观测为 $\\{Y_{t}\\}_{t \\geq 1}$，分别由转移密度 $f(x_{t} \\mid x_{t-1})$ 和观测似然 $g(y_{t} \\mid x_{t})$ 给出。令 $y_{1:t}$ 表示截至时间 $t$ 观测到的固定数据。假设一种序贯蒙特卡洛（SMC）方法，也称为粒子滤波，在时间 $t$ 产生一个加权粒子系统 $\\{(x_{t}^{i}, \\tilde{w}_{t}^{i})\\}_{i=1}^{N}$ 来近似滤波分布 $p(x_{t} \\mid y_{1:t})$，其中 $\\tilde{w}_{t}^{i} \\geq 0$ 且 $\\sum_{i=1}^{N} \\tilde{w}_{t}^{i} = 1$。\n\n要求您使用残差重采样，为滤波期望 $\\int \\varphi(x) p(x \\mid y_{1:t}) \\, dx$ 构建一个关于重采样引入的随机性的条件无偏估计量。残差重采样的操作如下：对于 $i \\in \\{1,\\dots,N\\}$，定义整数分配 $n_{i} = \\lfloor N \\tilde{w}_{t}^{i} \\rfloor$，令 $R = N - \\sum_{i=1}^{N} n_{i}$，并计算归一化残差 $r_{i} = \\frac{N \\tilde{w}_{t}^{i} - n_{i}}{R}$（如果 $R > 0$），否则 $r_{i} = 0$。为索引 $i$ 构建 $n_{i}$ 个确定性副本，然后从 $\\{1,\\dots,N\\}$ 上的分类分布（概率为 $\\{r_{i}\\}_{i=1}^{N}$）中独立抽取剩余的 $R$ 个祖先索引。将 $N$ 个祖先索引的完整集合表示为 $A_{1:N}$，并将基于重采样的滤波期望估计量定义为\n$$\n\\widehat{I}_{\\mathrm{res}} \\;=\\; \\frac{1}{N} \\sum_{j=1}^{N} \\varphi\\!\\big(x_{t}^{A_{j}}\\big).\n$$\n\n任务：\n- 仅使用残差重采样的定义和期望的线性性质，证明估计量 $\\widehat{I}_{\\mathrm{res}}$ 关于经验加权测度是条件无偏的；即证明\n$$\n\\mathbb{E}\\!\\left[ \\widehat{I}_{\\mathrm{res}} \\,\\middle|\\, x_{t}^{1:N}, \\tilde{w}_{t}^{1:N} \\right]\n\\;=\\; \\sum_{i=1}^{N} \\tilde{w}_{t}^{i} \\,\\varphi(x_{t}^{i}).\n$$\n清楚地解释为什么这构成了经验加权期望的无偏估计量，并讨论它在何种意义上为 $\\int \\varphi(x) p(x \\mid y_{1:t}) \\, dx$ 提供了一个估计量。\n\n- 推导在权重 $\\{\\tilde{w}_{t}^{i}\\}_{i=1}^{N}$ 条件下，由残差重采样产生的确定性副本的期望数量的表达式，然后对具有以下权重向量的 $N$ 个粒子进行求值：\n$$\nN \\;=\\; 200, \\quad \\big(\\tilde{w}_{t}^{1},\\dots,\\tilde{w}_{t}^{6}\\big) \\;=\\; \\big(0.041,\\, 0.157,\\, 0.233,\\, 0.089,\\, 0.271,\\, 0.209\\big).\n$$\n\n您的最终答案必须是一个单一的数字，等于在指定 $N$ 和权重下残差重采样的确定性副本的期望数量。不需要四舍五入，最终答案中不应报告单位。", "solution": "该问题陈述清晰，在随机模拟领域具有科学依据，并为获得唯一解提供了所有必要信息。我们可以开始解题。\n\n第一个任务是证明残差重采样估计量 $\\widehat{I}_{\\mathrm{res}}$ 对于经验加权期望是条件无偏的。该估计量定义为\n$$\n\\widehat{I}_{\\mathrm{res}} = \\frac{1}{N} \\sum_{j=1}^{N} \\varphi\\big(x_{t}^{A_{j}}\\big)\n$$\n其中 $\\{A_{j}\\}_{j=1}^{N}$ 是通过残差重采样算法选择的祖先索引集合。我们需要证明\n$$\n\\mathbb{E}\\!\\left[ \\widehat{I}_{\\mathrm{res}} \\,\\middle|\\, x_{t}^{1:N}, \\tilde{w}_{t}^{1:N} \\right] \\;=\\; \\sum_{i=1}^{N} \\tilde{w}_{t}^{i} \\,\\varphi(x_{t}^{i}).\n$$\n该期望以粒子位置 $\\{x_{t}^{i}\\}_{i=1}^{N}$ 及其相关的归一化权重 $\\{\\tilde{w}_{t}^{i}\\}_{i=1}^{N}$ 为条件。在此条件下，这些量被视为固定常数。唯一的随机性来源是祖先索引 $\\{A_j\\}$ 的选择。\n\n令 $K_i$ 为表示原始粒子索引 $i$ 在重采样步骤中被选为祖先次数的随机变量。祖先索引的集合 $\\{A_j\\}_{j=1}^N$ 是一个多重集，其中索引 $i$ 出现 $K_i$ 次。因此，我们可以将估计量中的和重写为：\n$$\n\\sum_{j=1}^{N} \\varphi\\big(x_{t}^{A_{j}}\\big) = \\sum_{i=1}^{N} K_i \\varphi(x_{t}^{i}).\n$$\n于是，估计量为 $\\widehat{I}_{\\mathrm{res}} = \\frac{1}{N} \\sum_{i=1}^{N} K_i \\varphi(x_{t}^{i})$。\n\n利用期望的线性性质，我们有：\n$$\n\\mathbb{E}\\!\\left[ \\widehat{I}_{\\mathrm{res}} \\,\\middle|\\, x_{t}^{1:N}, \\tilde{w}_{t}^{1:N} \\right] = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{E}\\!\\left[ K_i \\,\\middle|\\, \\tilde{w}_{t}^{1:N} \\right] \\varphi(x_{t}^{i}).\n$$\n我们的目标是计算 $K_i$ 的条件期望。残差重采样过程将 $K_i$ 定义为一个确定性部分和一个随机部分之和。\n确定性部分是整数副本的数量，$n_i = \\lfloor N \\tilde{w}_{t}^{i} \\rfloor$。\n随机部分来自于抽取剩余的 $R = N - \\sum_{k=1}^{N} n_k$ 个粒子。这 $R$ 个粒子是从 $\\{1, \\dots, N\\}$ 上的一个分类分布中独立抽取的，其概率为 $\\{r_i\\}_{i=1}^N$，其中当 $R > 0$ 时，$r_i = \\frac{N \\tilde{w}_{t}^{i} - n_{i}}{R}$。\n令 $K_i^{\\mathrm{rand}}$ 为在这 $R$ 次随机抽取中索引 $i$ 被选中的次数。那么，$K_i = n_i + K_i^{\\mathrm{rand}}$。\n随机变量 $K_i^{\\mathrm{rand}}$ 服从二项分布，$K_i^{\\mathrm{rand}} \\sim \\mathrm{Binomial}(R, r_i)$，因为它是在 $R$ 次独立的伯努利试验中“成功”（抽中索引 $i$）的次数，每次试验的成功概率为 $r_i$。\n\n因此，$K_i$ 的条件期望是：\n$$\n\\mathbb{E}\\!\\left[ K_i \\,\\middle|\\, \\tilde{w}_{t}^{1:N} \\right] = \\mathbb{E}\\!\\left[ n_i + K_i^{\\mathrm{rand}} \\,\\middle|\\, \\tilde{w}_{t}^{1:N} \\right] = n_i + \\mathbb{E}\\!\\left[ K_i^{\\mathrm{rand}} \\,\\middle|\\, \\tilde{w}_{t}^{1:N} \\right].\n$$\n二项随机变量 $\\mathrm{Binomial}(n,p)$ 的期望是 $np$。因此，对于 $R > 0$：\n$$\n\\mathbb{E}\\!\\left[ K_i^{\\mathrm{rand}} \\,\\middle|\\, \\tilde{w}_{t}^{1:N} \\right] = R \\cdot r_i = R \\cdot \\frac{N \\tilde{w}_{t}^{i} - n_{i}}{R} = N \\tilde{w}_{t}^{i} - n_{i}.\n$$\n如果 $R=0$，则 $N = \\sum_{k=1}^N n_k = \\sum_{k=1}^N \\lfloor N \\tilde{w}_t^k \\rfloor$。由于 $\\sum_{k=1}^N N \\tilde{w}_t^k = N$，这意味着 $\\sum_{k=1}^N (N \\tilde{w}_t^k - \\lfloor N \\tilde{w}_t^k \\rfloor) = 0$。因为和中的每一项都是非负的，所以对于所有的 $k$，我们必须有 $N \\tilde{w}_t^k - \\lfloor N \\tilde{w}_t^k \\rfloor = 0$。这意味着对于所有的 $k$，$N \\tilde{w}_t^k$ 都是整数，所以 $n_k = N \\tilde{w}_t^k$。在这种情况下，$N \\tilde{w}_{t}^{i} - n_{i} = 0$。同时，当 $R=0$ 时，没有进行随机抽取，所以 $K_i^{\\mathrm{rand}} = 0$，其期望为 $0$。因此，公式 $\\mathbb{E}[K_i^{\\mathrm{rand}}] = N \\tilde{w}_{t}^{i} - n_{i}$ 在 $R=0$ 时也成立。\n\n将此结果代入 $\\mathbb{E}[K_i]$ 的表达式中：\n$$\n\\mathbb{E}\\!\\left[ K_i \\,\\middle|\\, \\tilde{w}_{t}^{1:N} \\right] = n_i + (N \\tilde{w}_{t}^{i} - n_{i}) = N \\tilde{w}_{t}^{i}.\n$$\n最后，我们将其代入估计量期望值的表达式中：\n$$\n\\mathbb{E}\\!\\left[ \\widehat{I}_{\\mathrm{res}} \\,\\middle|\\, x_{t}^{1:N}, \\tilde{w}_{t}^{1:N} \\right] = \\frac{1}{N} \\sum_{i=1}^{N} (N \\tilde{w}_{t}^{i}) \\varphi(x_{t}^{i}) = \\sum_{i=1}^{N} \\tilde{w}_{t}^{i} \\varphi(x_{t}^{i}).\n$$\n证明完毕。\n\n这个结果意味着 $\\widehat{I}_{\\mathrm{res}}$ 是经验期望 $\\sum_{i=1}^{N} \\tilde{w}_{t}^{i} \\varphi(x_{t}^{i})$ 的一个无偏估计量，其中无偏性是仅相对于重采样步骤引入的随机性而言的。数量 $\\sum_{i=1}^{N} \\tilde{w}_{t}^{i} \\varphi(x_{t}^{i})$ 是基于重采样前的加权粒子系统对真实滤波期望 $I_t = \\int \\varphi(x) p(x \\mid y_{1:t}) \\, dx$ 的标准蒙特卡洛近似。对于有限数量的粒子 $N$，这个经验期望通常是 $I_t$ 的一个有偏估计量。然而，它是一个相合估计量，意味着在适当的正则性条件下，当 $N \\to \\infty$ 时，它收敛于 $I_t$。因此，$\\widehat{I}_{\\mathrm{res}}$ 为真实滤波期望 $I_t$ 提供了一个估计量，其意义在于它是一个 $I_t$ 的相合估计量的条件无偏估计量。\n\n第二个任务是推导确定性副本期望数量的表达式并进行求值。\n粒子 $i$ 的确定性副本数量定义为 $n_i = \\lfloor N \\tilde{w}_{t}^{i} \\rfloor$。确定性副本的总数是所有粒子的总和，$N_{\\mathrm{det}} = \\sum_{i=1}^{N} n_i$。\n问题要求的是在权重 $\\{\\tilde{w}_{t}^{i}\\}_{i=1}^N$ 条件下，确定性副本的期望数量。由于权重是给定的，数量 $N_{\\mathrm{det}}$ 是一个固定的确定性值。一个常数的期望就是它本身。\n因此，表达式为：\n$$\n\\mathbb{E}\\!\\left[ N_{\\mathrm{det}} \\,\\middle|\\, \\tilde{w}_{t}^{1:N} \\right] = \\sum_{i=1}^{N} \\lfloor N \\tilde{w}_{t}^{i} \\rfloor.\n$$\n给定 $N = 200$ 和 6 个粒子的非零权重：$\\tilde{w}_{t}^{1}=0.041$，$\\tilde{w}_{t}^{2}=0.157$，$\\tilde{w}_{t}^{3}=0.233$，$\\tilde{w}_{t}^{4}=0.089$，$\\tilde{w}_{t}^{5}=0.271$，$\\tilde{w}_{t}^{6}=0.209$。对于所有 $i>6$，$\\tilde{w}_{t}^{i}=0$。\n\n我们为这 6 个粒子中的每一个计算 $N \\tilde{w}_{t}^{i}$：\n- $n_1 = \\lfloor 200 \\times 0.041 \\rfloor = \\lfloor 8.2 \\rfloor = 8$\n- $n_2 = \\lfloor 200 \\times 0.157 \\rfloor = \\lfloor 31.4 \\rfloor = 31$\n- $n_3 = \\lfloor 200 \\times 0.233 \\rfloor = \\lfloor 46.6 \\rfloor = 46$\n- $n_4 = \\lfloor 200 \\times 0.089 \\rfloor = \\lfloor 17.8 \\rfloor = 17$\n- $n_5 = \\lfloor 200 \\times 0.271 \\rfloor = \\lfloor 54.2 \\rfloor = 54$\n- $n_6 = \\lfloor 200 \\times 0.209 \\rfloor = \\lfloor 41.8 \\rfloor = 41$\n\n对于 $i > 6$，$\\tilde{w}_{t}^{i} = 0$，所以 $n_i = \\lfloor 200 \\times 0 \\rfloor = 0$。\n确定性副本的总数是这些值的和：\n$$\n\\sum_{i=1}^{200} n_i = n_1 + n_2 + n_3 + n_4 + n_5 + n_6 + \\sum_{i=7}^{200} n_i\n$$\n$$\n= 8 + 31 + 46 + 17 + 54 + 41 + 0 = 197.\n$$\n在给定权重条件下，确定性副本的期望数量为 197。", "answer": "$$\n\\boxed{197}\n$$", "id": "3308535"}, {"introduction": "一个可靠的滤波器必须是稳定的，这意味着其估计最终应独立于初始的先验假设。本练习旨在研究导致隐马尔可夫模型（Hidden Markov Models）中滤波器稳定或不稳定的条件。通过编程模拟，您将学会诊断由模型结构引起的失效模式，并测试如“退火”（tempering）等正则化技术，以恢复滤波器关键的“遗忘”特性 [@problem_id:3308531]。", "problem": "考虑一个有限状态隐马尔可夫模型 (HMM)，其隐状态过程为 $\\{X_t\\}_{t \\geq 1}$，取值于 $\\{1,2,\\dots,K\\}$，观测过程为 $\\{Y_t\\}_{t \\geq 1}$，由转移核 $p(x_t \\mid x_{t-1})$ 和观测似然 $p(y_t \\mid x_t)$ 指定。给定观测 $y_{1:t}$ 和 $X_1$ 的先验分布 $\\mu$，在时间 $t$ 的滤波分布表示为 $\\pi_t^\\mu$，其中 $\\pi_t^\\mu(i) = \\mathbb{P}(X_t = i \\mid y_{1:t})$。两个滤波分布之间的距离由全变差 (TV) 范数度量，对于离散分布定义为\n$$\n\\left\\lVert \\pi_t^\\mu - \\pi_t^\\nu \\right\\rVert_{\\mathrm{TV}} = \\frac{1}{2} \\sum_{i=1}^K \\left| \\pi_t^\\mu(i) - \\pi_t^\\nu(i) \\right|.\n$$\n根据马尔可夫链和条件概率的基本定义，有限状态 HMM 的滤波递归通过预测后更新得到：一步预测为 $\\alpha_{t \\mid t-1}^\\mu(j) = \\sum_{i=1}^K \\pi_{t-1}^\\mu(i) \\, p(j \\mid i)$，更新步骤为\n$$\n\\pi_t^\\mu(j) = \\frac{\\alpha_{t \\mid t-1}^\\mu(j) \\, g_j(y_t)}{\\sum_{k=1}^K \\alpha_{t \\mid t-1}^\\mu(k) \\, g_k(y_t)},\n$$\n其中 $g_j(y_t) = p(y_t \\mid X_t = j)$。滤波器稳定性指的是对于任意两个先验 $\\mu$ 和 $\\nu$，$\\left\\lVert \\pi_t^\\mu - \\pi_t^\\nu \\right\\rVert_{\\mathrm{TV}}$ 随着 $t$ 的增加而衰减。已知实现遗忘的一个充分机制是转移核中存在混合性以及信息丰富的观测，而失效模式包括非连通动态和不可辨识或对称的似然。\n\n您的任务是编写一个完整、可运行的程序，该程序：\n- 实现有限状态 HMM 的滤波递归，并计算最终时刻的 $\\left\\lVert \\pi_t^\\mu - \\pi_t^\\nu \\right\\rVert_{\\mathrm{TV}}$。\n- 为每个测试用例从指定的 HMM 模拟一条观测路径，以驱动滤波器。\n- 通过检查最终 TV 距离是否低于稳定性阈值来诊断非遗忘机制。\n- 通过均匀混合修改转移核并可选地对似然进行退火，提出并测试一种基于退火的修复方法。\n\n退火修复定义如下。给定一个转移矩阵 $P$，其元素为 $P_{ij} = p(j \\mid i)$，定义一个退火转移矩阵\n$$\n\\widetilde{P} = (1 - \\varepsilon) P + \\varepsilon U,\n$$\n其中 $U$ 是一个 $K \\times K$ 矩阵，所有行都等于 $\\{1,\\dots,K\\}$ 上的均匀分布，$\\varepsilon \\in [0,1]$。给定似然函数 $g_j(y)$，使用参数 $\\beta \\in (0,1]$ 的似然退火在更新步骤中使用 $g_j(y)^\\beta$：\n$$\n\\pi_t^{\\mu,\\mathrm{temp}}(j) = \\frac{\\alpha_{t \\mid t-1}^\\mu(j) \\, g_j(y_t)^\\beta}{\\sum_{k=1}^K \\alpha_{t \\mid t-1}^\\mu(k) \\, g_k(y_t)^\\beta}.\n$$\n\n您必须为以下三个参数集的测试套件实现上述功能，每个参数集描述了 $(K, P, \\text{发射参数}, T, \\mu, \\nu, \\varepsilon, \\beta)$，其中观测是具有状态依赖均值和标准差的高斯分布。在所有情况下，观测路径长度为 $T$，状态 $j$ 的高斯似然使用均值 $m_j$ 和标准差 $s_j$：\n$$\ng_j(y) = \\frac{1}{\\sqrt{2\\pi} s_j} \\exp\\left( -\\frac{(y - m_j)^2}{2 s_j^2} \\right).\n$$\n所有随机模拟必须是可复现的。\n\n测试套件：\n- 情况 1 (稳定的混合性和信息丰富的观测)：\n  - $K = 2$,\n  - $P = \\begin{pmatrix} 0.9  0.1 \\\\ 0.1  0.9 \\end{pmatrix}$,\n  - 均值 $m = (0, 3)$,\n  - 标准差 $s = (1, 1)$,\n  - $T = 50$,\n  - 先验 $\\mu = (0.99, 0.01)$ 和 $\\nu = (0.01, 0.99)$,\n  - 退火参数 $\\varepsilon = 0.02$, $\\beta = 0.7$。\n- 情况 2 (对抗性：单位动态和无信息量的观测)：\n  - $K = 2$,\n  - $P = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}$,\n  - 均值 $m = (0, 0)$,\n  - 标准差 $s = (5, 5)$,\n  - $T = 50$,\n  - 先验 $\\mu = (0.99, 0.01)$ 和 $\\nu = (0.01, 0.99)$,\n  - 退火参数 $\\varepsilon = 0.05$, $\\beta = 0.7$。\n- 情况 3 (对抗性：非连通块和跨块不可辨识)：\n  - $K = 4$,\n  - $P = \\begin{pmatrix} 0.95  0.05  0  0 \\\\ 0.05  0.95  0  0 \\\\ 0  0  0.95  0.05 \\\\ 0  0  0.05  0.95 \\end{pmatrix}$,\n  - 均值 $m = (0, 0, 0, 0)$,\n  - 标准差 $s = (1, 1, 1, 1)$,\n  - $T = 50$,\n  - 先验 $\\mu = (0.5, 0.5, 0, 0)$ 和 $\\nu = (0, 0, 0.5, 0.5)$,\n  - 退火参数 $\\varepsilon = 0.02$, $\\beta = 0.7$。\n\n对每种情况，执行以下操作：\n1. 使用固定的随机种子和固定的初始隐状态分布（在 $\\{1,\\dots,K\\}$ 上均匀分布），从指定的 $P$ 和高斯发射分布模拟一条观测路径 $\\{Y_t\\}_{t=1}^T$。\n2. 使用相同的观测，从未退火模型（$\\varepsilon = 0$, $\\beta = 1$）分别从 $\\mu$ 和 $\\nu$ 开始运行两个滤波器。计算在时间 $T$ 的最终 TV 距离：\n   $$\n   D_{\\mathrm{std}} = \\left\\lVert \\pi_T^\\mu - \\pi_T^\\nu \\right\\rVert_{\\mathrm{TV}}.\n   $$\n3. 使用相同的观测，从退火模型（使用指定的 $\\varepsilon$ 和 $\\beta$）分别从 $\\mu$ 和 $\\nu$ 开始运行两个退火滤波器。计算在时间 $T$ 的最终 TV 距离：\n   $$\n   D_{\\mathrm{temp}} = \\left\\lVert \\pi_T^{\\mu,\\mathrm{temp}} - \\pi_T^{\\nu,\\mathrm{temp}} \\right\\rVert_{\\mathrm{TV}}.\n   $$\n4. 对于稳定性阈值 $\\tau = 0.1$，生成布尔值\n   $$\n   S_{\\mathrm{std}} = \\mathbf{1}\\{ D_{\\mathrm{std}} \\leq \\tau \\}, \\quad S_{\\mathrm{temp}} = \\mathbf{1}\\{ D_{\\mathrm{temp}} \\leq \\tau \\}.\n   $$\n\n您的程序应生成单行输出，其中包含三个测试用例的结果，格式为方括号括起来的逗号分隔列表。每个用例的结果必须是列表\n$$\n\\left[ D_{\\mathrm{std}}, D_{\\mathrm{temp}}, S_{\\mathrm{std}}, S_{\\mathrm{temp}} \\right],\n$$\n其中 $D_{\\mathrm{std}}$ 和 $D_{\\mathrm{temp}}$ 是浮点数，$S_{\\mathrm{std}}$ 和 $S_{\\mathrm{temp}}$ 是布尔值。因此，最终打印的行必须如下所示\n$$\n\\left[ [d_{1,\\mathrm{std}}, d_{1,\\mathrm{temp}}, s_{1,\\mathrm{std}}, s_{1,\\mathrm{temp}}], [d_{2,\\mathrm{std}}, d_{2,\\mathrm{temp}}, s_{2,\\mathrm{std}}, s_{2,\\mathrm{temp}}], [d_{3,\\mathrm{std}}, d_{3,\\mathrm{temp}}, s_{3,\\mathrm{std}}, s_{3,\\mathrm{temp}}] \\right].\n$$\n此问题不涉及物理单位；所有输出均为无量纲的数值。", "solution": "问题陈述经过了仔细验证，并被确定为科学上合理、良置、客观和完整的。它构成了计算统计学中的一个标准练习，特别是关于有限状态隐马尔可夫模型 (HMM) 滤波器的稳定性属性。因此，我们可以着手提供一个形式化的解决方案。\n\n该问题要求对离散时间、有限状态 HMM 的滤波器稳定性进行分析。滤波器的稳定性，或称遗忘特性，指的是其“遗忘”其初始状态的能力，使得以不同先验分布开始的两个滤波器随时间收敛到相同的后验分布。收敛程度使用全变差 (TV) 距离来衡量。我们将实现 HMM 滤波递归，模拟观测数据，并研究稳定性失效的场景。此外，我们将实现并测试一种基于退火的正则化技术，旨在恢复稳定性。\n\nHMM 由以下部分定义：\n1. 一个隐状态过程 $\\{X_t\\}_{t \\ge 1}$，在一个有限集合 $\\{1, 2, \\dots, K\\}$ 中取值。状态的演化由一个时齐马尔可夫链控制，其转移概率矩阵为 $K \\times K$ 的 $P$，其中 $P_{ij} = p(X_t=j \\mid X_{t-1}=i)$。\n2. 一个观测过程 $\\{Y_t\\}_{t \\ge 1}$。在每个时间 $t$，观测值 $Y_t$是从一个仅依赖于当前隐状态 $X_t$ 的分布中抽取的。条件概率密度函数表示为 $g_j(y) = p(y \\mid X_t=j)$。在此问题中，发射是高斯的：$Y_t \\mid \\{X_t=j\\} \\sim \\mathcal{N}(m_j, s_j^2)$。\n3. 一个在 $\\{1, 2, \\dots, K\\}$ 上的初始状态分布 $\\mu$。\n\n为解决此问题，我们将实现以下组件。\n\n**1. HMM 数据模拟**\n对于每个测试用例，我们必须首先生成一个单一、可复现的观测序列 $\\{y_t\\}_{t=1}^T$。这通过首先模拟潜在状态序列 $\\{x_t\\}_{t=1}^T$ 然后模拟相应的观测来实现。\n- **状态模拟**：初始状态 $x_1$ 从指定的 $\\{1, \\dots, K\\}$ 上的均匀分布中抽取。对于 $t=2, \\dots, T$ 的后续状态 $x_t$，从由转移矩阵 $P$ 的第 $x_{t-1}$ 行定义的分类分布中抽取。\n- **观测模拟**：对于模拟路径中的每个状态 $x_t=j$，相应的观测 $y_t$ 从高斯发射分布 $\\mathcal{N}(m_j, s_j^2)$ 中抽取。\n- **可复现性**：所有用于模拟的随机数均使用以固定种子初始化的伪随机数生成器生成，确保整个过程是确定性和可复现的。我们将使用种子 $0$。\n\n**2. 贝叶斯滤波递归**\n分析的核心是滤波算法，它计算在时间 $t$ 时隐状态的后验分布，$\\pi_t(j) = \\mathbb{P}(X_t=j \\mid y_{1:t})$，给定截至时间 $t$ 的观测序列。对于每个时间 $t=1, \\dots, T$，递归分两步进行：\n\n- **预测**：时间 $t$ 状态的先验分布是基于时间 $t-1$ 的后验计算得出的。在向量表示法中，如果 $\\pi_{t-1}$ 是时间 $t-1$ 的后验概率行向量，则预测分布 $\\alpha_{t|t-1}$ 由下式给出：\n    $$\n    \\alpha_{t|t-1} = \\pi_{t-1} P\n    $$\n- **更新**：预测的分布使用新的观测 $y_t$ 通过贝叶斯法则进行更新。更新后的后验 $\\pi_t$ 为：\n    $$\n    \\pi_t(j) = \\frac{\\alpha_{t|t-1}(j) \\, g_j(y_t)}{\\sum_{k=1}^K \\alpha_{t|t-1}(k) \\, g_k(y_t)}\n    $$\n为了防止重复乘以小概率值导致的数值下溢，计算在对数域中进行。更新步骤变成了对数概率的相加，然后使用 `log-sum-exp` 操作进行归一化。\n\n**3. 滤波器稳定性与全变差距离**\n滤波器稳定性是指对于任意两个初始先验 $\\mu$ 和 $\\nu$，相应的滤波分布 $\\pi_t^\\mu$ 和 $\\pi_t^\\nu$ 随着 $t \\to \\infty$ 收敛的性质。这种收敛由全变差 (TV) 距离度量：\n$$\nD_t = \\left\\lVert \\pi_t^\\mu - \\pi_t^\\nu \\right\\rVert_{\\mathrm{TV}} = \\frac{1}{2} \\sum_{i=1}^K \\left| \\pi_t^\\mu(i) - \\pi_t^\\nu(i) \\right|\n$$\n如果 $\\lim_{t \\to \\infty} D_t = 0$，则认为滤波器是稳定的。对于此问题，稳定性是在有限时间 $T$ 与阈值 $\\tau = 0.1$ 进行评估的。\n\n**4. 作为正则化的退火**\n该问题提出了两种滤波器稳定性的常见失效模式，我们用退火来解决：\n\n- **转移退火**：为对抗非连通或慢混合动态，转移矩阵 $P$ 被替换为一个扰动版本 $\\widetilde{P}$：\n    $$\n    \\widetilde{P} = (1 - \\varepsilon) P + \\varepsilon U\n    $$\n    其中 $U$ 是一个所有行都等于 $\\{1, \\dots, K\\}$ 上均匀分布的矩阵，$\\varepsilon \\in (0,1]$ 是一个小的混合参数。这确保了得到的马尔可夫链是遍历的，保证了唯一的平稳分布，并强制所有状态之间进行混合。\n\n- **似然退火**：为了调节观测的影响，特别是当它们无信息量或具有误导性时，似然函数 $g_j(y)$ 被提升到 $\\beta \\in (0,1]$ 的幂：\n    $$\n    \\pi_t(j) \\propto \\alpha_{t|t-1}(j) \\, g_j(y_t)^\\beta\n    $$\n    在对数域中，这对应于将对数似然乘以 $\\beta$。$\\beta  1$ 的值会“扁平化”似然，使滤波器更多地依赖于动态而不是数据。\n\n**5. 测试用例分析**\n三个测试用例旨在展示特定的行为：\n\n- **情况 1 (稳定)：** 转移矩阵是混合的，且发射分布是良好分离的。我们预期标准滤波器是稳定的，即 $D_{\\mathrm{std}} \\le \\tau=0.1$。退火滤波器也将是稳定的。\n\n- **情况 2 (对抗性)：** 转移矩阵是单位矩阵，意味着状态永不改变（零混合）。两种状态的发射分布相同，意味着观测不携带关于状态的任何信息。我们预期标准滤波器将无法忘记其初始先验，导致 $D_{\\mathrm{std}} \\gg \\tau$。使用 $\\varepsilon > 0$ 的转移退火引入了混合性，即使观测无信息量，也将导致后验收敛到 $\\widetilde{P}$ 的平稳分布。我们预期 $D_{\\mathrm{temp}} \\le \\tau$。\n\n- **情况 3 (对抗性)：** 转移矩阵是块对角的，创建了两个不连通的状态类。先验 $\\mu$ 和 $\\nu$ 将它们的所有质量放在不同的块中。所有状态的发射分布是相同的。标准滤波器将无法在块之间移动概率质量，从而保留了初始的分离。我们预期 $D_{\\mathrm{std}} \\approx 1$。转移退火连接了这些块，从而实现了收敛和稳定性。我们预期 $D_{\\mathrm{temp}} \\le \\tau$。\n\n实现将首先为每个案例模拟数据，然后针对两个指定的先验 $\\mu$ 和 $\\nu$ 运行标准滤波器和退火滤波器，以计算最终的 TV 距离和稳定性标志。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\nfrom scipy.special import logsumexp\n\ndef solve():\n    \"\"\"\n    Main function to run the HMM filtering analysis for the test suite.\n    \"\"\"\n    \n    # Test suite from the problem statement.\n    # Each case: (K, P, m, s, T, mu, nu, epsilon, beta)\n    test_cases = [\n        # Case 1: Stable mixing and informative observations\n        (\n            2,\n            np.array([[0.9, 0.1], [0.1, 0.9]]),\n            np.array([0.0, 3.0]),\n            np.array([1.0, 1.0]),\n            50,\n            np.array([0.99, 0.01]),\n            np.array([0.01, 0.99]),\n            0.02,\n            0.7\n        ),\n        # Case 2: Adversarial - identity dynamics and uninformative observations\n        (\n            2,\n            np.array([[1.0, 0.0], [0.0, 1.0]]),\n            np.array([0.0, 0.0]),\n            np.array([5.0, 5.0]),\n            50,\n            np.array([0.99, 0.01]),\n            np.array([0.01, 0.99]),\n            0.05,\n            0.7\n        ),\n        # Case 3: Adversarial - non-communicating blocks and non-identifiable likelihood\n        (\n            4,\n            np.array([\n                [0.95, 0.05, 0.0, 0.0],\n                [0.05, 0.95, 0.0, 0.0],\n                [0.0, 0.0, 0.95, 0.05],\n                [0.0, 0.0, 0.05, 0.95]\n            ]),\n            np.array([0.0, 0.0, 0.0, 0.0]),\n            np.array([1.0, 1.0, 1.0, 1.0]),\n            50,\n            np.array([0.5, 0.5, 0.0, 0.0]),\n            np.array([0.0, 0.0, 0.5, 0.5]),\n            0.02,\n            0.7\n        )\n    ]\n\n    # Global settings for simulation and analysis\n    stability_threshold = 0.1\n    # Use a fixed seed for reproducible simulations\n    rng = np.random.default_rng(0)\n    \n    final_results = []\n\n    for case in test_cases:\n        K, P, m, s, T, mu, nu, epsilon, beta = case\n\n        # 1. Simulate one observation path from the HMM\n        x_path = np.zeros(T, dtype=int)\n        y_path = np.zeros(T)\n        \n        # Initial state drawn uniformly\n        x_path[0] = rng.choice(K) \n        \n        for t in range(T):\n            if t > 0:\n                x_path[t] = rng.choice(K, p=P[x_path[t-1], :])\n            # Draw observation from Gaussian emission\n            y_path[t] = rng.normal(loc=m[x_path[t]], scale=s[x_path[t]])\n\n        # 2. Run standard filters and compute TV distance\n        pi_T_mu_std = run_filter(y_path, T, K, P, m, s, mu, beta=1.0)\n        pi_T_nu_std = run_filter(y_path, T, K, P, m, s, nu, beta=1.0)\n        D_std = 0.5 * np.sum(np.abs(pi_T_mu_std - pi_T_nu_std))\n        S_std = D_std = stability_threshold\n\n        # 3. Run tempered filters and compute TV distance\n        U = np.full((K, K), 1.0 / K)\n        P_tempered = (1.0 - epsilon) * P + epsilon * U\n        \n        pi_T_mu_temp = run_filter(y_path, T, K, P_tempered, m, s, mu, beta)\n        pi_T_nu_temp = run_filter(y_path, T, K, P_tempered, m, s, nu, beta)\n        D_temp = 0.5 * np.sum(np.abs(pi_T_mu_temp - pi_T_nu_temp))\n        S_temp = D_temp = stability_threshold\n\n        final_results.append([D_std, D_temp, S_std, S_temp])\n\n    # Final print statement in the exact required format.\n    # Convert boolean to lower case string 'true'/'false' as often expected in JSON-like formats.\n    # However, the problem doesn't specify, and Python's str(bool) is 'True'/'False'. Let's stick to that.\n    def format_results(case_results):\n        d_std, d_temp, s_std, s_temp = case_results\n        return f\"[{d_std}, {d_temp}, {str(s_std)}, {str(s_temp)}]\"\n\n    print(f\"[\" + \",\".join(map(str, final_results)) + \"]\")\n\n\ndef run_filter(y_path, T, K, P, m, s, prior, beta):\n    \"\"\"\n    Runs the HMM filtering recursion.\n\n    Args:\n        y_path (np.ndarray): The sequence of observations.\n        T (int): The length of the observation sequence.\n        K (int): The number of hidden states.\n        P (np.ndarray): The transition matrix.\n        m (np.ndarray): The means of the Gaussian emissions.\n        s (np.ndarray): The standard deviations of the Gaussian emissions.\n        prior (np.ndarray): The initial prior distribution over states.\n        beta (float): The likelihood tempering parameter.\n\n    Returns:\n        np.ndarray: The final filtering distribution pi_T.\n    \"\"\"\n    # Initialize with the log of the prior distribution\n    # A small constant is added to the prior to avoid log(0) for deterministic priors.\n    log_pi = np.log(prior + 1e-100) \n\n    for t in range(T):\n        # Prediction step\n        # Convert back to probability space for matrix multiplication\n        pi = np.exp(log_pi)\n        alpha_pred = pi @ P\n\n        # Update step (in log-space for numerical stability)\n        # Handle cases where predicted probability is 0\n        log_alpha_pred = np.log(alpha_pred, where=alpha_pred > 0, out=np.full_like(alpha_pred, -np.inf))\n        \n        # Calculate log-likelihoods for the current observation y_t\n        log_likelihoods = np.array([\n            norm.logpdf(y_path[t], loc=m[j], scale=s[j]) for j in range(K)\n        ])\n\n        # Compute unnormalized log posterior\n        unnormalized_log_post = log_alpha_pred + beta * log_likelihoods\n        \n        # Normalize using log-sum-exp trick\n        log_norm_const = logsumexp(unnormalized_log_post)\n        log_pi = unnormalized_log_post - log_norm_const\n\n    # Return the final filtering distribution in probability space\n    return np.exp(log_pi)\n\nsolve()\n```", "id": "3308531"}, {"introduction": "虽然重采样是必要的，但其频率却是一个关键的权衡：过于频繁会引入路径退化，过于稀少则导致权重退化，两者都会影响平滑估计的整体精度。这项高级练习将指导您通过经验性方法校准由有效样本量（Effective Sample Size, ESS）阈值控制的重采样频率。通过实现近似的SMC平滑器和精确的RTS卡尔曼平滑器，您将通过最小化均方误差来优化这一关键超参数，这个过程真实反映了开发高性能状态空间模型估计器时所面临的实际挑战 [@problem_id:3308538]。", "problem": "您将研究一个标量线性高斯状态空间模型，并设计一个序贯蒙特卡洛（SMC）平滑估计器，其重采样由有效样本量（ESS）阈值触发。您的任务是从基本原理出发，为平滑泛函推导中心极限定理（CLT），然后校准ESS阈值，以最小化平滑估计器随时间的期望均方误差（MSE）。最后，您将实现一个完整的、可运行的程序，该程序使用固定的测试套件对阈值进行经验性校准，并产生所需的输出。\n\n该问题的基本基础是状态空间模型中滤波和平滑的Feynman–Kac表示、序贯蒙特卡洛（SMC）中重要性采样和重采样的定义，以及混合和正则性条件下的经典中心极限定理（CLT）结果。\n\n模型和定义：\n- 考虑一个具有隐藏状态 $x_t$ 和观测值 $y_t$ 的标量线性高斯状态空间模型。\n- 初始状态为 $x_0 \\sim \\mathcal{N}(\\mu_0, P_0)$，状态动态为 $x_t = a x_{t-1} + \\eta_t$，其中 $\\eta_t \\sim \\mathcal{N}(0,q)$，观测模型为 $y_t = c x_t + \\epsilon_t$，其中 $\\epsilon_t \\sim \\mathcal{N}(0,r)$。\n- 设观测序列为 $y_{1:T} = (y_1, \\dots, y_T)$，对于一个固定的时间范围 $T$。\n- 将时间 $t$ 的平滑目标泛函定义为 $\\varphi_t = \\mathbb{E}[g(x_t) \\mid y_{1:T}]$，对于一个可测函数 $g$。在本问题中，使用 $g(x) = x$，因此 $\\varphi_t = \\mathbb{E}[x_t \\mid y_{1:T}]$。\n- SMC平滑估计器 $\\hat{\\varphi}_t^{(N)}$ 是由一个包含 $N$ 个粒子的自助粒子滤波器构建的，该滤波器使用由ESS阈值触发的系统重采样，并在粒子集上进行前向滤波-后向平滑。\n\n序贯蒙特卡洛设置和ESS触发的重采样：\n- 在每个时间 $t$，自助提议分布为 $x_t^{(i)} \\sim p(x_t \\mid x_{t-1}^{(a^{(i)})})$，其中 $p(x_t \\mid x_{t-1}) = \\mathcal{N}(a x_{t-1}, q)$，祖先索引 $a^{(i)}$ 通过重采样（当触发时）选择。\n- 在时间 $t$ 的未归一化权重为 $w_t^{(i)} \\propto p(y_t \\mid x_t^{(i)})$，其中 $p(y_t \\mid x_t)=\\mathcal{N}(c x_t, r)$。归一化权重为 $\\tilde{w}_t^{(i)} = w_t^{(i)}\\big/\\sum_{j=1}^N w_t^{(j)}$。\n- 在时间 $t$ 的有效样本量（ESS）定义为 $\\mathrm{ESS}_t = \\left(\\sum_{i=1}^N \\tilde{w}_t^{(i)}\\right)^2 \\big/ \\sum_{i=1}^N \\left(\\tilde{w}_t^{(i)}\\right)^2 = 1 \\big/ \\sum_{i=1}^N \\left(\\tilde{w}_t^{(i)}\\right)^2$。如果 $\\mathrm{ESS}_t  \\tau N$，则触发重采样，其中 $\\tau \\in (0,1]$ 是ESS阈值。\n- 使用系统重采样。重采样后，将权重重置为 $\\tilde{w}_t^{(i)} = 1/N$。\n\n在粒子集上进行平滑：\n- 使用由马尔可夫转移所蕴含的后向核，在粒子集上执行前向滤波-后向平滑。定义 $\\kappa_{t}(x_t^{(i)}, x_{t+1}^{(j)}) = p(x_{t+1}^{(j)} \\mid x_t^{(i)})$，对于标量高斯模型，$p(x_{t+1} \\mid x_t) = \\mathcal{N}(a x_t, q)$。在实现中，为了数值稳定性和归一化不变性，仅使用指数部分并省略常数因子，即 $\\exp\\left(-\\frac{1}{2 q} \\left(x_{t+1}^{(j)} - a x_t^{(i)}\\right)^2\\right)$。\n- 用 $\\beta_T^{(i)} = 1$ 对所有 $i$ 初始化后向消息。对于 $t = T-1, T-2, \\dots, 1$，更新 $\\beta_t^{(i)} = \\sum_{j=1}^N \\beta_{t+1}^{(j)} \\tilde{w}_{t+1}^{(j)} \\kappa_t(x_t^{(i)}, x_{t+1}^{(j)})$。\n- 时间 $t$ 的平滑权重为 $\\tilde{w}_{t,\\mathrm{sm}}^{(i)} \\propto \\tilde{w}_t^{(i)} \\beta_t^{(i)}$，并归一化以使总和为1。\n- 平滑估计器为 $\\hat{\\varphi}_t^{(N)} = \\sum_{i=1}^N \\tilde{w}_{t,\\mathrm{sm}}^{(i)} g(x_t^{(i)})$，对于 $t=1,\\dots,T-1$，而对于 $t=T$，设置 $\\hat{\\varphi}_T^{(N)} = \\sum_{i=1}^N \\tilde{w}_T^{(i)} g(x_T^{(i)})$。\n\nCLT目标与校准目标：\n- 在Feynman–Kac模型与重采样的标准混合和正则性假设下，您将为每个固定时间 $t$ 推导出CLT $\\sqrt{N}\\left(\\hat{\\varphi}_t^{(N)} - \\varphi_t\\right) \\Rightarrow \\mathcal{N}(0,\\sigma_t^2)$。渐近方差 $\\sigma_t^2$ 是局部贡献的总和，这些贡献取决于 $g(x_t)$ 在平滑分布下的变异性，以及通过后向核在时间上传播的变异性。\n- 使用CLT，论证校准 $\\tau$ 以控制有效样本量和谱系退化，从而最小化期望均方误差 $\\mathrm{MSE} = \\frac{1}{T} \\sum_{t=1}^T \\mathbb{E}\\left[\\left(\\hat{\\varphi}_t^{(N)} - \\varphi_t\\right)^2\\right]$。\n\n程序要求：\n- 实现一个完整的程序，对于每个测试用例，从指定的模型参数生成合成数据，使用Rauch–Tung–Striebel（RTS）卡尔曼平滑器计算精确的平滑均值 $\\varphi_t$，对一组 $\\tau$ 候选值运行SMC平滑器，估计时间平均MSE，并输出最小化估计MSE的 $\\tau$。当出现平局时，在具有最小MSE的候选项中选择最小的 $\\tau$。\n- 如上所述，使用恒等函数 $g(x) = x$。\n- 当 $\\mathrm{ESS}_t  \\tau N$ 时，使用系统重采样。\n- 对于每个测试用例，在指定数量的重复实验中对时间平均MSE进行平均，以近似目标中的期望。\n\n测试套件：\n- 案例1：参数 $(\\mu_0,P_0,a,q,c,r,T,N,R)$ 设置为 $(0.0,1.0,0.9,1.0,1.0,0.5,40,200,12)$，$\\tau$ 候选值为 $[0.3, 0.5, 0.7, 0.9]$。\n- 案例2：参数 $(\\mu_0,P_0,a,q,c,r,T,N,R)$ 设置为 $(0.0,5.0,1.0,1.0,1.0,2.0,60,150,12)$，$\\tau$ 候选值为 $[0.2, 0.4, 0.6, 0.8, 1.0]$。\n- 案例3：参数 $(\\mu_0,P_0,a,q,c,r,T,N,R)$ 设置为 $(0.0,1.0,0.5,0.5,1.0,0.1,50,120,12)$，$\\tau$ 候选值为 $[0.1, 0.3, 0.5, 0.7]$。\n\n最终输出格式：\n- 您的程序应产生单行输出，其中包含三个测试用例选择的阈值，格式为逗号分隔的浮点数列表，四舍五-入到两位小数，并用方括号括起来。例如，如果选择的阈值是 $\\tau_1 = 0.5$、$\\tau_2 = 0.6$ 和 $\\tau_3 = 0.3$，则输出必须是 `[0.50,0.60,0.30]`。\n\n重要约束：\n- 最终程序必须完全自包含，不需要用户输入，并且只能使用下面指定的Python标准库、NumPy和SciPy。本问题中不出现角度或物理单位，因此除了上述定义外，不需要任何单位说明。", "solution": "该问题已经过验证，并被确定为计算统计学和随机模拟领域中一个定义明确、具有科学依据且完整的问题。它要求实现并经验性地比较标准算法（Rauch-Tung-Striebel平滑器、序贯蒙特卡洛平滑器），以解决一个实际的参数校准任务。所有必要的模型、参数和目标都已明确定义。\n\n目标是经验性地确定序贯蒙特卡洛（SMC）平滑器中用于重采样的最优有效样本量（ESS）阈值，记为 $\\tau$。最优性定义为最小化平滑状态估计的时间平均均方误差（MSE）。我们给定一个标量线性高斯状态空间模型：\n- 状态动态：$x_t = a x_{t-1} + \\eta_t$，其中 $\\eta_t \\sim \\mathcal{N}(0,q)$\n- 观测模型：$y_t = c x_t + \\epsilon_t$，其中 $\\epsilon_t \\sim \\mathcal{N}(0,r)$\n\n估计的目标是每个时间步 $t \\in \\{1, \\dots, T\\}$ 的状态的平滑均值 $\\varphi_t = \\mathbb{E}[x_t \\mid y_{1:T}]$。\n\n**理论动机与基于原则的设计**\n\n$\\varphi_t$ 的估计器记为 $\\hat{\\varphi}_t^{(N)}$，它是使用一个包含 $N$ 个粒子的SMC平滑器构建的。在适当的正则性和混合条件下，SMC估计器的中心极限定理（CLT）成立。对于固定的时间 $t$ 并且当粒子数 $N \\to \\infty$ 时，估计误差的分布收敛于：\n$$\n\\sqrt{N}\\left(\\hat{\\varphi}_t^{(N)} - \\varphi_t\\right) \\Rightarrow \\mathcal{N}(0,\\sigma_t^2)\n$$\n其中“$\\Rightarrow$”表示依分布收敛。该CLT意味着，对于一个大而有限的 $N$，均方误差（MSE）可以近似为：\n$$\n\\mathrm{MSE}_t = \\mathbb{E}\\left[\\left(\\hat{\\varphi}_t^{(N)} - \\varphi_t\\right)^2\\right] \\approx \\frac{\\sigma_t^2}{N}\n$$\n渐近方差 $\\sigma_t^2$ 是一个复杂的量，它依赖于粒子滤波器的整个历史，包括随机重采样步骤。我们的目标是选择重采样阈值 $\\tau$ 以最小化时间平均MSE，即 $\\frac{1}{T} \\sum_{t=1}^T \\mathrm{MSE}_t$，这近似等价于最小化时间平均渐近方差。\n\n$\\tau \\in (0,1]$ 的选择控制着重采样的频率，并涉及到一个关键的权衡：\n1.  **不频繁重采样（低 $\\tau$）**：如果很少执行重采样，粒子权重可能会变得高度倾斜，即一个或少数几个粒子的权重接近1，而其余粒子的权重接近0。这种现象被称为权重退化，它会增加SMC估计器的方差，因为对估计有贡献的有效粒子数非常少。ESS定义为 $\\mathrm{ESS}_t = 1 / \\sum_{i=1}^N (\\tilde{w}_t^{(i)})^2$，是衡量这种退化程度的指标。\n2.  **频繁重采样（高 $\\tau$）**：如果几乎在每个步骤都进行重采样（例如，$\\tau$ 接近1），则可以减轻权重退化。然而，这会引入另一个问题：路径退化。频繁的重采样会导致粒子集中许多粒子共享一个共同的近期祖先。粒子轨迹多样性的这种丧失同样会增加平滑估计的方差，特别是对于较早的时间步，因为来自未来的信息可供反向传播的独特祖先路径更少。\n\n因此，存在一个最优的 $\\tau$ 值，它在减少权重方差和增加谱系方差之间取得了平衡。由于总方差作为 $\\tau$ 的函数的解析表达式是难以处理的，我们采用一种基于经验和模拟的方法进行校准。\n\n**算法与实现策略**\n\n我们的策略涉及一个嵌套循环结构，以经验性地评估测试套件中提供的每个候选 $\\tau$。\n\n1.  **外层循环**：遍历每个候选值 $\\tau$。\n2.  **中层循环（重复实验）**：对于每个 $\\tau$，运行 $R$ 次独立的模拟（重复实验），以获得稳定的MSE估计值。\n3.  **内层循环（单次重复实验的逻辑）**：\n    a.  **数据生成**：从线性高斯模型中合成一个真实状态序列 $\\{x_t\\}_{t=0}^T$ 和一个对应的观测序列 $\\{y_t\\}_{t=1}^T$。\n    b.  **基准真相计算**：给定的模型是线性和高斯的，因此精确的后验平滑分布是高斯的，并且可以解析计算。我们使用标准的 **Rauch-Tung-Striebel (RTS) 平滑器** 来计算真实的平滑均值 $\\varphi_t = \\mathbb{E}[x_t \\mid y_{1:T}]$。RTS算法包括一个前向传递（卡尔曼滤波器）来计算滤波估计 $\\mathbb{E}[x_t \\mid y_{1:t}]$ 和一个后向传递来将这些估计与未来信息结合。\n    c.  **SMC估计**：我们实现指定的SMC平滑器来计算估计值 $\\hat{\\varphi}_t^{(N)}$。这涉及两个主要阶段：\n        i.  **前向传递（粒子滤波器）**：一个自助粒子滤波器从 $t=1$ 推进到 $T$。在每个步骤中，粒子根据状态动态进行传播，然后根据观测的似然进行加权。计算ESS，如果它低于阈值 $\\tau N$，则执行 **系统重采样**。对于后续的平滑步骤至关重要的是，存储每个时间步*在任何重采样之前*的完整粒子集及其权重。\n        ii. **后向传递（FFBS）**：执行一个 **前向滤波-后向平滑（FFBS）** 算法。从终端消息 $\\beta_T^{(i)} = 1$ 开始，我们从 $t=T-1$ 向后迭代到 $t=1$，根据递归式 $\\beta_t^{(i)} = \\sum_{j=1}^N \\beta_{t+1}^{(j)} \\tilde{w}_{t+1}^{(j)} \\kappa_t(x_t^{(i)}, x_{t+1}^{(j)})$ 更新后向消息 $\\beta_t^{(i)}$，其中 $\\kappa_t$ 是状态转移密度。这个递归是在存储的重采样前的粒子集上执行的。然后，使用与前向权重 $\\tilde{w}_t^{(i)}$ 和后向消息 $\\beta_t^{(i)}$ 的乘积成比例的权重来计算平滑估计 $\\hat{\\varphi}_t^{(N)}$。\n    d.  **MSE计算**：该次重复实验的时间平均MSE计算为 $\\frac{1}{T}\\sum_{t=1}^T (\\hat{\\varphi}_t^{(N)} - \\varphi_t)^2$。\n\n4.  **选择**：在完成给定 $\\tau$ 的所有重复实验后，记录平均MSE。最后，选择导致最低平均MSE的 $\\tau$ 作为该测试用例的最优值，若出现平局，则选择最小的 $\\tau$。对三个测试用例中的每一个都重复此整个过程。\n\n这种严谨的、基于模拟的方法允许通过在问题中定义的特定模型类别上优化期望的性能指标（MSE），来直接校准超参数 $\\tau$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the final results.\n    \"\"\"\n    # Set a global random seed for complete reproducibility of the experiment.\n    np.random.seed(42)\n    \n    test_cases = [\n        {\n            'mu0': 0.0, 'P0': 1.0, 'a': 0.9, 'q': 1.0, 'c': 1.0, 'r': 0.5, \n            'T': 40, 'N': 200, 'R': 12, 'taus': [0.3, 0.5, 0.7, 0.9]\n        },\n        {\n            'mu0': 0.0, 'P0': 5.0, 'a': 1.0, 'q': 1.0, 'c': 1.0, 'r': 2.0, \n            'T': 60, 'N': 150, 'R': 12, 'taus': [0.2, 0.4, 0.6, 0.8, 1.0]\n        },\n        {\n            'mu0': 0.0, 'P0': 1.0, 'a': 0.5, 'q': 0.5, 'c': 1.0, 'r': 0.1, \n            'T': 50, 'N': 120, 'R': 12, 'taus': [0.1, 0.3, 0.5, 0.7]\n        }\n    ]\n\n    final_results = []\n    for params in test_cases:\n        best_tau = calibrate_tau_for_case(params)\n        final_results.append(f\"{best_tau:.2f}\")\n\n    print(f\"[{','.join(final_results)}]\")\n\ndef calibrate_tau_for_case(params):\n    \"\"\"\n    For a single test case, finds the optimal tau from a list of candidates.\n    \"\"\"\n    taus = params['taus']\n    R = params['R']\n    \n    mse_by_tau = {}\n    for tau in taus:\n        replicate_mses = []\n        for _ in range(R):\n            x_true, y_obs = generate_data(params)\n            \n            # Ground truth via RTS Kalman Smoother\n            phi_true, _ = rts_smoother(y_obs, params)\n            \n            # SMC estimation\n            phi_hat = smc_smoother(y_obs, params, tau)\n            \n            # Time-averaged MSE for this replicate (t=1..T)\n            mse = np.mean((phi_hat[1:] - phi_true[1:])**2) \n            replicate_mses.append(mse)\n        \n        mse_by_tau[tau] = np.mean(replicate_mses)\n\n    # Find best tau (min MSE, with tie-breaking by choosing smallest tau)\n    min_mse = float('inf')\n    best_tau = -1.0\n    # Iterate through taus in their given (sorted) order.\n    for tau in taus:\n        if mse_by_tau[tau]  min_mse:\n            min_mse = mse_by_tau[tau]\n            best_tau = tau\n            \n    return best_tau\n    \ndef generate_data(params):\n    \"\"\"\n    Generates synthetic data from the linear Gaussian state-space model.\n    \"\"\"\n    T, mu0, P0, a, q, c, r = params['T'], params['mu0'], params['P0'], params['a'], params['q'], params['c'], params['r']\n    x = np.zeros(T + 1)\n    y = np.zeros(T + 1) # y_1 to y_T stored at indices 1..T\n    \n    x[0] = np.random.normal(mu0, np.sqrt(P0))\n    for t in range(1, T + 1):\n        x[t] = a * x[t-1] + np.random.normal(0, np.sqrt(q))\n        y[t] = c * x[t] + np.random.normal(0, np.sqrt(r))\n    return x, y\n\ndef rts_smoother(y_obs, params):\n    \"\"\"\n    Computes exact smoothed means and covariances using the RTS smoother.\n    \"\"\"\n    T, mu0, P0, a, q, c, r = params['T'], params['mu0'], params['P0'], params['a'], params['q'], params['c'], params['r']\n    \n    x_filt = np.zeros(T + 1)\n    P_filt = np.zeros(T + 1)\n    x_pred = np.zeros(T + 1)\n    P_pred = np.zeros(T + 1)\n\n    x_filt[0], P_filt[0] = mu0, P0\n    \n    # Kalman Filter (forward pass)\n    for t in range(1, T + 1):\n        x_pred[t] = a * x_filt[t-1]\n        P_pred[t] = a**2 * P_filt[t-1] + q\n        \n        S = c**2 * P_pred[t] + r\n        K = (P_pred[t] * c) / S\n        x_filt[t] = x_pred[t] + K * (y_obs[t] - c * x_pred[t])\n        P_filt[t] = (1 - K * c) * P_pred[t]\n        \n    x_smooth = np.zeros(T + 1)\n    P_smooth = np.zeros(T + 1)\n    \n    x_smooth[T], P_smooth[T] = x_filt[T], P_filt[T]\n    \n    # RTS (backward pass)\n    for t in range(T - 1, -1, -1):\n        J = (P_filt[t] * a) / P_pred[t+1]\n        x_smooth[t] = x_filt[t] + J * (x_smooth[t+1] - x_pred[t+1])\n        P_smooth[t] = P_filt[t] + J**2 * (P_smooth[t+1] - P_pred[t+1])\n\n    return x_smooth, P_smooth\n\ndef systematic_resample_indices(weights, N):\n    \"\"\"\n    Performs systematic resampling and returns the indices of resampled particles.\n    \"\"\"\n    c = np.cumsum(weights)\n    u0 = np.random.uniform(0, 1/N)\n    u = u0 + np.arange(N) / N\n    return np.searchsorted(c, u)\n\ndef smc_smoother(y_obs, params, tau):\n    \"\"\"\n    Performs SMC smoothing with ESS-triggered systematic resampling and FFBS.\n    \"\"\"\n    T, N, mu0, P0, a, q, c, r = params['T'], params['N'], params['mu0'], params['P0'], params['a'], params['q'], params['c'], params['r']\n    \n    all_particles = np.zeros((T + 1, N))\n    all_weights = np.zeros((T + 1, N))\n    \n    # At time t=0, we have the prior particles\n    current_particles = np.random.normal(mu0, np.sqrt(P0), size=N)\n    all_particles[0, :] = current_particles\n    all_weights[0, :] = 1.0 / N\n\n    # Forward filter pass\n    for t in range(1, T + 1):\n        # Resampling is based on t-1 weights to get ancestors for time t particles\n        ess_prev = 1.0 / np.sum(all_weights[t-1, :]**2)\n        if ess_prev  tau * N:\n            indices = systematic_resample_indices(all_weights[t-1, :], N)\n            ancestor_particles = all_particles[t-1, :][indices]\n        else:\n            ancestor_particles = all_particles[t-1, :]\n\n        # Propagate from chosen ancestors\n        propagated_particles = a * ancestor_particles + np.random.normal(0, np.sqrt(q), size=N)\n        \n        log_w = -0.5 * ((y_obs[t] - c * propagated_particles)**2) / r\n        log_w -= np.max(log_w) # for numerical stability\n        w = np.exp(log_w)\n        normalized_weights = w / np.sum(w)\n\n        all_particles[t, :] = propagated_particles\n        all_weights[t, :] = normalized_weights\n    \n    # Backward smoothing pass (FFBS)\n    smoothed_means = np.zeros(T + 1)\n    smoothed_means[T] = np.sum(all_weights[T, :] * all_particles[T, :])\n\n    beta = np.ones(N)\n    for t in range(T - 1, 0, -1):\n        # Calculate backward kernel matrix between particle sets at t and t+1\n        diff_sq = (all_particles[t+1, :][np.newaxis, :] - a * all_particles[t, :][:, np.newaxis])**2\n        kernel = np.exp(-0.5 * diff_sq / q)\n        \n        weighted_beta_next = beta * all_weights[t+1, :]\n        beta = np.dot(kernel, weighted_beta_next)\n        \n        w_sm = all_weights[t, :] * beta\n        w_sm_sum = np.sum(w_sm)\n        if w_sm_sum > 1e-100: # Handle potential underflow\n            w_sm /= w_sm_sum\n        else:\n            w_sm = np.ones(N) / N # Reset if all weights are zero\n            \n        smoothed_means[t] = np.sum(w_sm * all_particles[t, :])\n        \n    return smoothed_means\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3308538"}]}