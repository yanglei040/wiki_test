{"hands_on_practices": [{"introduction": "在随机近似中，降低估计的方差是加速收敛和提高稳定性的关键。本练习提供了一个机会，通过分析证明一种简单而强大的技术——公共随机数（CRN）——如何显著降低 Kiefer-Wolfowitz 梯度估计器的方差。通过这个推导，您将亲身体会到巧妙的模拟设计对算法性能的深刻影响 ([@problem_id:3348680])。", "problem": "考虑一个随机目标，其均值响应函数为 $f(\\theta) = \\mathbb{E}[Y(\\theta, \\xi)]$，该函数在给定标量参数 $\\theta \\in \\mathbb{R}$ 的一个邻域内是可微的。在 Kiefer-Wolfowitz (KW) 有限差分法中，梯度通过使用在扰动点 $\\theta \\pm \\delta$（其中 $\\delta  0$ 是一个小数）处的两个含噪声的观测值，经由对称差分来估计。定义一步 KW 梯度估计量\n$$\n\\widehat{g}(\\theta; \\delta) = \\frac{Y(\\theta + \\delta) - Y(\\theta - \\delta)}{2 \\delta}.\n$$\n假设观测值具有以下噪声结构：对于任意固定的 $\\theta$ 和 $\\delta$，记\n$$\nY(\\theta \\pm \\delta) = f(\\theta \\pm \\delta) + \\varepsilon_{\\pm},\n$$\n其中 $\\mathbb{E}[\\varepsilon_{\\pm}] = 0$，$\\operatorname{Var}(\\varepsilon_{\\pm}) = \\sigma^{2}$（对于某个 $\\sigma^{2} \\in (0, \\infty)$），并且在使用公共随机数 (CRN) 时，随机数对 $(\\varepsilon_{+}, \\varepsilon_{-})$ 的相关系数为 $\\rho \\in [-1, 1]$。在不使用 CRN 时，该随机数对是独立的，对应于 $\\rho = 0$。在整个问题中，$f(\\cdot)$ 是确定性的，唯一的随机性来源是 $(\\varepsilon_{+}, \\varepsilon_{-})$。\n\n仅使用期望的线性性质以及方差和协方差的定义，完成以下任务：\n\n- 在使用 CRN 的情况下，推导 $\\operatorname{Cov}(Y(\\theta + \\delta), Y(\\theta - \\delta))$，并用 $\\sigma^{2}$ 和 $\\rho$ 表示。\n\n- 分别推导在使用 CRN 和独立情况下的 $\\operatorname{Var}(\\widehat{g}(\\theta; \\delta))$。\n\n- 定义方差影响比 $R(\\rho)$ 为\n$$\nR(\\rho) \\equiv \\frac{\\operatorname{Var}(\\widehat{g}(\\theta; \\delta) \\text{ under CRN})}{\\operatorname{Var}(\\widehat{g}(\\theta; \\delta) \\text{ under independence})}.\n$$\n计算 $R(\\rho)$ 作为 $\\rho$ 的函数的闭式解析表达式。\n\n给出 $R(\\rho)$ 的表达式作为你的最终答案。不需要进行数值舍入。", "solution": "首先验证问题，以确保其具有科学依据、是良定的和客观的。\n\n### 步骤1：提取已知条件\n-   目标函数均值：$f(\\theta) = \\mathbb{E}[Y(\\theta, \\xi)]$\n-   参数：$\\theta \\in \\mathbb{R}$\n-   Kiefer-Wolfowitz 梯度估计量：$\\widehat{g}(\\theta; \\delta) = \\frac{Y(\\theta + \\delta) - Y(\\theta - \\delta)}{2 \\delta}$，其中 $\\delta  0$。\n-   观测噪声模型：$Y(\\theta \\pm \\delta) = f(\\theta \\pm \\delta) + \\varepsilon_{\\pm}$。\n-   噪声性质：$\\mathbb{E}[\\varepsilon_{\\pm}] = 0$，$\\operatorname{Var}(\\varepsilon_{\\pm}) = \\sigma^{2}$，其中 $\\sigma^{2} \\in (0, \\infty)$。\n-   使用公共随机数 (CRN) 时的相关系数：$\\rho = \\operatorname{Corr}(\\varepsilon_{+}, \\varepsilon_{-}) \\in [-1, 1]$。\n-   独立情况：随机数对 $(\\varepsilon_{+}, \\varepsilon_{-})$ 独立，对应于 $\\rho = 0$。\n-   函数 $f(\\cdot)$ 是确定性的。\n-   唯一的随机性来源是噪声对 $(\\varepsilon_{+}, \\varepsilon_{-})$。\n-   方差影响比定义：$R(\\rho) \\equiv \\frac{\\operatorname{Var}(\\widehat{g}(\\theta; \\delta) \\text{ under CRN})}{\\operatorname{Var}(\\widehat{g}(\\theta; \\delta) \\text{ under independence})}$。\n\n### 步骤2：使用提取的已知条件进行验证\n该问题是随机模拟和优化领域的一个标准理论练习，特别关注 Kiefer-Wolfowitz 有限差分梯度估计量的方差。\n-   **科学依据**：该问题基于概率论的既定原理（期望、方差、协方差、相关系数）及其在随机算法分析中的应用。Kiefer-Wolfowitz 方法是随机逼近的基石。\n-   **良定性**：问题陈述清晰，提供了推导所需量值的所有必要定义和假设。问题具体，并导向一个唯一的解析解。\n-   **客观性**：语言精确且数学化，不包含任何主观或模棱两可的陈述。\n\n该问题被认为是有效的，因为它没有违反任何指定的无效性标准。我们可以继续进行求解。\n\n---\n\n解答过程根据问题陈述的要求分三部分进行推导。\n\n### 第1部分：推导 $\\operatorname{Cov}(Y(\\theta + \\delta), Y(\\theta - \\delta))$\n我们被要求在 CRN 假设下推导两个含噪声观测值 $Y(\\theta + \\delta)$ 和 $Y(\\theta - \\delta)$ 之间的协方差。两个随机变量 $X$ 和 $Z$ 之间的协方差定义为 $\\operatorname{Cov}(X, Z) = \\mathbb{E}[(X - \\mathbb{E}[X])(Z - \\mathbb{E}[Z])]$。\n\n令 $X = Y(\\theta + \\delta)$，$Z = Y(\\theta - \\delta)$。首先，我们计算它们的期望。利用期望的线性性质和给定属性：\n$$\n\\mathbb{E}[Y(\\theta + \\delta)] = \\mathbb{E}[f(\\theta + \\delta) + \\varepsilon_{+}] = \\mathbb{E}[f(\\theta + \\delta)] + \\mathbb{E}[\\varepsilon_{+}]\n$$\n由于 $f(\\cdot)$ 是一个确定性函数，$\\mathbb{E}[f(\\theta + \\delta)] = f(\\theta + \\delta)$。我们已知 $\\mathbb{E}[\\varepsilon_{+}] = 0$。因此，\n$$\n\\mathbb{E}[Y(\\theta + \\delta)] = f(\\theta + \\delta)\n$$\n类似地，对于 $Y(\\theta - \\delta)$：\n$$\n\\mathbb{E}[Y(\\theta - \\delta)] = \\mathbb{E}[f(\\theta - \\delta) + \\varepsilon_{-}] = f(\\theta - \\delta) + \\mathbb{E}[\\varepsilon_{-}] = f(\\theta - \\delta)\n$$\n现在我们可以应用协方差的定义：\n$$\n\\operatorname{Cov}(Y(\\theta + \\delta), Y(\\theta - \\delta)) = \\mathbb{E}[(Y(\\theta + \\delta) - \\mathbb{E}[Y(\\theta + \\delta)])(Y(\\theta - \\delta) - \\mathbb{E}[Y(\\theta - \\delta)])]\n$$\n代入观测值及其均值的表达式：\n$$\n= \\mathbb{E}[((f(\\theta + \\delta) + \\varepsilon_{+}) - f(\\theta + \\delta))((f(\\theta - \\delta) + \\varepsilon_{-}) - f(\\theta - \\delta))]\n$$\n$$\n= \\mathbb{E}[\\varepsilon_{+} \\varepsilon_{-}]\n$$\n根据定义，噪声项的协方差为 $\\operatorname{Cov}(\\varepsilon_{+}, \\varepsilon_{-}) = \\mathbb{E}[\\varepsilon_{+} \\varepsilon_{-}] - \\mathbb{E}[\\varepsilon_{+}]\\mathbb{E}[\\varepsilon_{-}]$。由于 $\\mathbb{E}[\\varepsilon_{+}] = 0$ 且 $\\mathbb{E}[\\varepsilon_{-}] = 0$，这简化为 $\\operatorname{Cov}(\\varepsilon_{+}, \\varepsilon_{-}) = \\mathbb{E}[\\varepsilon_{+} \\varepsilon_{-}]$。\n\n相关系数 $\\rho$ 定义为 $\\rho = \\operatorname{Corr}(\\varepsilon_{+}, \\varepsilon_{-}) = \\frac{\\operatorname{Cov}(\\varepsilon_{+}, \\varepsilon_{-})}{\\sqrt{\\operatorname{Var}(\\varepsilon_{+})\\operatorname{Var}(\\varepsilon_{-})}}$。\n我们已知 $\\operatorname{Var}(\\varepsilon_{+}) = \\sigma^2$ 和 $\\operatorname{Var}(\\varepsilon_{-}) = \\sigma^2$。将这些代入相关系数公式：\n$$\n\\rho = \\frac{\\operatorname{Cov}(\\varepsilon_{+}, \\varepsilon_{-})}{\\sqrt{\\sigma^2 \\cdot \\sigma^2}} = \\frac{\\operatorname{Cov}(\\varepsilon_{+}, \\varepsilon_{-})}{\\sigma^2}\n$$\n由此，我们得到 $\\operatorname{Cov}(\\varepsilon_{+}, \\varepsilon_{-}) = \\rho \\sigma^2$。因此，\n$$\n\\operatorname{Cov}(Y(\\theta + \\delta), Y(\\theta - \\delta)) = \\rho \\sigma^2\n$$\n\n### 第2部分：推导 $\\operatorname{Var}(\\widehat{g}(\\theta; \\delta))$\n我们需要分别推导在 CRN 和独立情况下梯度估计量 $\\widehat{g}(\\theta; \\delta)$ 的方差。我们使用随机变量线性组合的方差性质：$\\operatorname{Var}(aX + bZ) = a^2\\operatorname{Var}(X) + b^2\\operatorname{Var}(Z) + 2ab\\operatorname{Cov}(X, Z)$。\n\n估计量为 $\\widehat{g}(\\theta; \\delta) = \\frac{1}{2\\delta} Y(\\theta + \\delta) - \\frac{1}{2\\delta} Y(\\theta - \\delta)$。\n这里，$a = \\frac{1}{2\\delta}$，$b = -\\frac{1}{2\\delta}$，$X = Y(\\theta + \\delta)$，$Z = Y(\\theta - \\delta)$。方差为：\n$$\n\\operatorname{Var}(\\widehat{g}(\\theta; \\delta)) = \\left(\\frac{1}{2\\delta}\\right)^2 \\operatorname{Var}(Y(\\theta + \\delta)) + \\left(-\\frac{1}{2\\delta}\\right)^2 \\operatorname{Var}(Y(\\theta - \\delta)) + 2\\left(\\frac{1}{2\\delta}\\right)\\left(-\\frac{1}{2\\delta}\\right) \\operatorname{Cov}(Y(\\theta + \\delta), Y(\\theta - \\delta))\n$$\n$$\n= \\frac{1}{4\\delta^2} \\left[ \\operatorname{Var}(Y(\\theta + \\delta)) + \\operatorname{Var}(Y(\\theta - \\delta)) - 2\\operatorname{Cov}(Y(\\theta + \\delta), Y(\\theta - \\delta)) \\right]\n$$\n首先，我们求出单个观测值的方差：\n$$\n\\operatorname{Var}(Y(\\theta + \\delta)) = \\operatorname{Var}(f(\\theta + \\delta) + \\varepsilon_{+}) = \\operatorname{Var}(\\varepsilon_{+}) = \\sigma^2\n$$\n因为 $f(\\theta + \\delta)$ 是一个确定性常数。类似地，$\\operatorname{Var}(Y(\\theta - \\delta)) = \\sigma^2$。\n\n**情况1：使用 CRN**\n我们将第1部分推导出的方差和协方差（$\\operatorname{Cov}(Y(\\theta + \\delta), Y(\\theta - \\delta)) = \\rho \\sigma^2$）代入 $\\operatorname{Var}(\\widehat{g}(\\theta; \\delta))$ 的表达式中：\n$$\n\\operatorname{Var}(\\widehat{g}(\\theta; \\delta) \\text{ under CRN}) = \\frac{1}{4\\delta^2} [\\sigma^2 + \\sigma^2 - 2(\\rho \\sigma^2)] = \\frac{2\\sigma^2 - 2\\rho\\sigma^2}{4\\delta^2} = \\frac{2\\sigma^2(1 - \\rho)}{4\\delta^2}\n$$\n$$\n\\operatorname{Var}(\\widehat{g}(\\theta; \\delta) \\text{ under CRN}) = \\frac{\\sigma^2(1-\\rho)}{2\\delta^2}\n$$\n\n**情况2：独立**\n$\\varepsilon_{+}$ 和 $\\varepsilon_{-}$ 的独立性等价于设置它们的相关系数 $\\rho = 0$。在这种情况下，它们的协方差为零，因此 $\\operatorname{Cov}(Y(\\theta + \\delta), Y(\\theta - \\delta)) = 0$。我们可以通过在 CRN 公式中设置 $\\rho=0$ 来获得结果：\n$$\n\\operatorname{Var}(\\widehat{g}(\\theta; \\delta) \\text{ under independence}) = \\frac{\\sigma^2(1-0)}{2\\delta^2} = \\frac{\\sigma^2}{2\\delta^2}\n$$\n或者，将 $\\operatorname{Cov}(\\cdot)=0$ 直接代入一般方差公式：\n$$\n\\operatorname{Var}(\\widehat{g}(\\theta; \\delta) \\text{ under independence}) = \\frac{1}{4\\delta^2} [\\sigma^2 + \\sigma^2 - 0] = \\frac{2\\sigma^2}{4\\delta^2} = \\frac{\\sigma^2}{2\\delta^2}\n$$\n\n### 第3部分：计算方差影响比 $R(\\rho)$\n方差影响比 $R(\\rho)$ 定义为使用 CRN 时估计量的方差与独立情况下其方差的比率。\n$$\nR(\\rho) = \\frac{\\operatorname{Var}(\\widehat{g}(\\theta; \\delta) \\text{ under CRN})}{\\operatorname{Var}(\\widehat{g}(\\theta; \\delta) \\text{ under independence})}\n$$\n使用第2部分中推导的表达式：\n$$\nR(\\rho) = \\frac{\\frac{\\sigma^2(1-\\rho)}{2\\delta^2}}{\\frac{\\sigma^2}{2\\delta^2}}\n$$\n只要给定的条件 $\\sigma^2  0$ 和 $\\delta  0$ 成立，分子和分母中的项 $\\frac{\\sigma^2}{2\\delta^2}$ 就会消去。剩下：\n$$\nR(\\rho) = 1 - \\rho\n$$\n这个表达式量化了使用 CRN 的效果。正相关 ($\\rho  0$) 会减小梯度估计的方差，这是在这种情况下使用 CRN 的主要动机。负相关 ($\\rho  0$) 则会增大方差。", "answer": "$$\n\\boxed{1 - \\rho}\n$$", "id": "3348680"}, {"introduction": "在梯度不可用的情况下，Kiefer-Wolfowitz 算法依赖于有限差分来估计梯度，而估计器的选择直接影响算法效率。本练习将引导您比较单边和双边（中心）有限差分方法的优劣 ([@problem_id:3348722])。通过分析它们的偏差、方差和计算成本，您将学会在算法设计的核心问题——偏差-方差权衡——中做出明智的决策。", "problem": "考虑用于最小化一个充分光滑的目标函数 $f:\\mathbb{R}^d\\to\\mathbb{R}$ 的 Kiefer-Wolfowitz 随机近似方案，该函数只能通过带噪声的观测得到。对于任意查询点 $x\\in\\mathbb{R}^d$，我们观测到 $Y(x)=f(x)+\\varepsilon(x)$，其中 $\\mathbb{E}[\\varepsilon(x)]=0$ 且 $\\mathrm{Var}(\\varepsilon(x))=\\sigma^2$，并且对于不同的查询点 $x\\neq x'$，$\\varepsilon(x)$ 和 $\\varepsilon(x')$ 是独立的。设 $e_i$ 表示 $\\mathbb{R}^d$ 中的第 $i$ 个标准基向量。由有限差分构建的、沿着坐标 $i$ 的随机梯度估计量由单边（前向）差分定义为\n$$\n\\widehat{g}_i^{\\mathrm{one}}(x,\\delta)=\\frac{Y(x+\\delta e_i)-Y(x)}{\\delta},\n$$\n以及双边（中心）差分定义为\n$$\n\\widehat{g}_i^{\\mathrm{two}}(x,\\delta)=\\frac{Y(x+\\delta e_i)-Y(x-\\delta e_i)}{2\\delta},\n$$\n其中 $\\delta0$ 是一个小的扰动大小。假设 $f$ 在 $x$ 的一个邻域内具有连续三阶偏导数。仅使用这些估计量的定义、$f$ 的泰勒展开以及独立噪声的期望和方差的基本性质，比较当 $\\delta\\to 0$ 时 $\\widehat{g}_i^{\\mathrm{one}}$ 和 $\\widehat{g}_i^{\\mathrm{two}}$ 的偏差阶数和方差缩放，并确定在 Kiefer-Wolfowitz 随机近似中，考虑到统计精度和维度 $d$ 下每次迭代所需的函数求值次数，何时双边方案更优。\n\n以下哪个陈述是正确的？\n\nA. 在所述的光滑性和独立性假设下，$\\widehat{g}_i^{\\mathrm{one}}$ 的偏差阶数为 $O(\\delta)$，方差缩放为 $2\\sigma^2/\\delta^2$，而 $\\widehat{g}_i^{\\mathrm{two}}$ 的偏差阶数为 $O(\\delta^2)$，方差缩放为 $\\sigma^2/(2\\delta^2)$。当 $f$ 足够光滑，两个扰动之间的噪声是独立的或可以使其正相关以进一步减小方差，并且维度 $d$ 适中以至于每个坐标的额外求值是可以接受时，双边差分更优。\n\nB. 单边和双边差分都具有 $O(\\delta^2)$ 阶的偏差；然而，双边差分的方差缩放为 $2\\sigma^2/\\delta^2$，而单边差分的方差缩放为 $\\sigma^2/(2\\delta^2)$。仅当维度 $d$ 非常大时，双边差分才更优。\n\nC. 单边差分的偏差阶数为 $O(\\delta)$，方差缩放为 $2\\sigma^2/\\delta^2$，而双边差分的偏差阶数为 $O(\\delta^2)$ 但方差缩放为 $4\\sigma^2/\\delta^2$，因此只要噪声是独立的，单边方案就更优。\n\nD. 在 Kiefer-Wolfowitz 设置中，双边差分总是更优，因为对称性消除了噪声，产生的方差不依赖于 $\\delta$；此外，单边差分的偏差阶数为 $O(\\delta)$，双边差分的偏差阶数也是 $O(\\delta)$，因此无论维度 $d$ 如何，方差优势都占主导地位。", "solution": "该问题陈述是随机近似方法分析中的一个有效练习。它具有科学依据、提法恰当且客观，为进行严格推导提供了所有必要信息。\n\n题目要求我们在 Kiefer-Wolfowitz 随机近似算法的背景下，比较用于梯度的单边和双边有限差分估计量。比较将基于这些估计量的偏差和方差，以及每次迭代的计算成本。真实梯度分量表示为 $\\partial_i f(x) = \\frac{\\partial f}{\\partial x_i}(x)$。\n\n目标函数 $f$ 假设具有连续三阶偏导数。这使我们能够使用泰勒定理。对于一个小的标量 $\\delta  0$ 和标准基向量 $e_i$，我们在点 $x \\in \\mathbb{R}^d$ 附近有以下展开式：\n$$\nf(x+\\delta e_i) = f(x) + \\delta \\partial_i f(x) + \\frac{\\delta^2}{2!} \\partial_{ii}^2 f(x) + \\frac{\\delta^3}{3!} \\partial_{iii}^3 f(x) + O(\\delta^4)\n$$\n$$\nf(x-\\delta e_i) = f(x) - \\delta \\partial_i f(x) + \\frac{\\delta^2}{2!} \\partial_{ii}^2 f(x) - \\frac{\\delta^3}{3!} \\partial_{iii}^3 f(x) + O(\\delta^4)\n$$\n其中 $\\partial_{ii}^2 f(x)$ 和 $\\partial_{iii}^3 f(x)$ 分别是 $f$ 关于第 $i$ 个坐标的二阶和三阶偏导数，在 $x$ 处求值。在任意点 $z$ 的噪声 $\\varepsilon(z)$ 满足 $\\mathbb{E}[\\varepsilon(z)]=0$ 和 $\\mathrm{Var}(\\varepsilon(z))=\\sigma^2$。对于不同的点 $z_1 \\neq z_2$，噪声项 $\\varepsilon(z_1)$ 和 $\\varepsilon(z_2)$ 是独立的。\n\n**1. 单边估计量的分析**\n\n单边估计量定义为：\n$$\n\\widehat{g}_i^{\\mathrm{one}}(x,\\delta)=\\frac{Y(x+\\delta e_i)-Y(x)}{\\delta} = \\frac{f(x+\\delta e_i)+\\varepsilon(x+\\delta e_i)-f(x)-\\varepsilon(x)}{\\delta}\n$$\n\n**$\\widehat{g}_i^{\\mathrm{one}}$ 的偏差：**\n该估计量的期望是：\n$$\n\\mathbb{E}[\\widehat{g}_i^{\\mathrm{one}}(x,\\delta)] = \\frac{\\mathbb{E}[f(x+\\delta e_i)+\\varepsilon(x+\\delta e_i)] - \\mathbb{E}[f(x)+\\varepsilon(x)]}{\\delta}\n$$\n由于函数值 $f(\\cdot)$ 是确定性的且 $\\mathbb{E}[\\varepsilon(\\cdot)] = 0$，这可以简化为：\n$$\n\\mathbb{E}[\\widehat{g}_i^{\\mathrm{one}}(x,\\delta)] = \\frac{f(x+\\delta e_i) - f(x)}{\\delta}\n$$\n代入 $f(x+\\delta e_i)$ 的泰勒展开式：\n$$\n\\mathbb{E}[\\widehat{g}_i^{\\mathrm{one}}(x,\\delta)] = \\frac{(f(x) + \\delta \\partial_i f(x) + \\frac{\\delta^2}{2} \\partial_{ii}^2 f(x) + O(\\delta^3)) - f(x)}{\\delta} = \\partial_i f(x) + \\frac{\\delta}{2} \\partial_{ii}^2 f(x) + O(\\delta^2)\n$$\n偏差是期望与真实值之间的差：\n$$\n\\mathrm{Bias}(\\widehat{g}_i^{\\mathrm{one}}) = \\mathbb{E}[\\widehat{g}_i^{\\mathrm{one}}(x,\\delta)] - \\partial_i f(x) = \\frac{\\delta}{2} \\partial_{ii}^2 f(x) + O(\\delta^2)\n$$\n因此，单边估计量的偏差阶数为 $O(\\delta)$。\n\n**$\\widehat{g}_i^{\\mathrm{one}}$ 的方差：**\n$$\n\\mathrm{Var}(\\widehat{g}_i^{\\mathrm{one}}(x,\\delta)) = \\mathrm{Var}\\left(\\frac{\\varepsilon(x+\\delta e_i) - \\varepsilon(x)}{\\delta}\\right) = \\frac{1}{\\delta^2} \\mathrm{Var}(\\varepsilon(x+\\delta e_i) - \\varepsilon(x))\n$$\n由于查询点 $x+\\delta e_i$ 和 $x$ 是不同的，噪声项是独立的。对于独立随机变量 $A$ 和 $B$，有 $\\mathrm{Var}(A-B) = \\mathrm{Var}(A) + \\mathrm{Var}(B)$。\n$$\n\\mathrm{Var}(\\widehat{g}_i^{\\mathrm{one}}(x,\\delta)) = \\frac{1}{\\delta^2} (\\mathrm{Var}(\\varepsilon(x+\\delta e_i)) + \\mathrm{Var}(\\varepsilon(x))) = \\frac{1}{\\delta^2}(\\sigma^2 + \\sigma^2) = \\frac{2\\sigma^2}{\\delta^2}\n$$\n方差缩放为 $2\\sigma^2/\\delta^2$。\n\n**2. 双边估计量的分析**\n\n双边（或中心）估计量定义为：\n$$\n\\widehat{g}_i^{\\mathrm{two}}(x,\\delta)=\\frac{Y(x+\\delta e_i)-Y(x-\\delta e_i)}{2\\delta} = \\frac{f(x+\\delta e_i)+\\varepsilon(x+\\delta e_i)- (f(x-\\delta e_i)+\\varepsilon(x-\\delta e_i))}{2\\delta}\n$$\n\n**$\\widehat{g}_i^{\\mathrm{two}}$ 的偏差：**\n$$\n\\mathbb{E}[\\widehat{g}_i^{\\mathrm{two}}(x,\\delta)] = \\frac{f(x+\\delta e_i) - f(x-\\delta e_i)}{2\\delta}\n$$\n代入 $f(x+\\delta e_i)$ 和 $f(x-\\delta e_i)$ 的泰勒展开式：\n$$\nf(x+\\delta e_i) - f(x-\\delta e_i) = \\left(f(x) + \\delta \\partial_i f(x) + \\frac{\\delta^2}{2} \\partial_{ii}^2 f(x) + \\frac{\\delta^3}{6} \\partial_{iii}^3 f(x)\\right) - \\left(f(x) - \\delta \\partial_i f(x) + \\frac{\\delta^2}{2} \\partial_{ii}^2 f(x) - \\frac{\\delta^3}{6} \\partial_{iii}^3 f(x)\\right) + O(\\delta^5)\n$$\n$\\delta$ 的偶次幂项相互抵消：\n$$\nf(x+\\delta e_i) - f(x-\\delta e_i) = 2\\delta \\partial_i f(x) + \\frac{2\\delta^3}{6} \\partial_{iii}^3 f(x) + O(\\delta^5) = 2\\delta \\partial_i f(x) + \\frac{\\delta^3}{3} \\partial_{iii}^3 f(x) + O(\\delta^5)\n$$\n除以 $2\\delta$：\n$$\n\\mathbb{E}[\\widehat{g}_i^{\\mathrm{two}}(x,\\delta)] = \\partial_i f(x) + \\frac{\\delta^2}{6} \\partial_{iii}^3 f(x) + O(\\delta^4)\n$$\n偏差为：\n$$\n\\mathrm{Bias}(\\widehat{g}_i^{\\mathrm{two}}) = \\mathbb{E}[\\widehat{g}_i^{\\mathrm{two}}(x,\\delta)] - \\partial_i f(x) = \\frac{\\delta^2}{6} \\partial_{iii}^3 f(x) + O(\\delta^4)\n$$\n因此，双边估计量的偏差阶数为 $O(\\delta^2)$。这比单边估计量有显著的改进。\n\n**$\\widehat{g}_i^{\\mathrm{two}}$ 的方差：**\n$$\n\\mathrm{Var}(\\widehat{g}_i^{\\mathrm{two}}(x,\\delta)) = \\mathrm{Var}\\left(\\frac{\\varepsilon(x+\\delta e_i) - \\varepsilon(x-\\delta e_i)}{2\\delta}\\right) = \\frac{1}{4\\delta^2} \\mathrm{Var}(\\varepsilon(x+\\delta e_i) - \\varepsilon(x-\\delta e_i))\n$$\n查询点 $x+\\delta e_i$ 和 $x-\\delta e_i$ 是不同的，因此噪声项是独立的。\n$$\n\\mathrm{Var}(\\widehat{g}_i^{\\mathrm{two}}(x,\\delta)) = \\frac{1}{4\\delta^2} (\\mathrm{Var}(\\varepsilon(x+\\delta e_i)) + \\mathrm{Var}(\\varepsilon(x-\\delta e_i))) = \\frac{1}{4\\delta^2}(\\sigma^2 + \\sigma^2) = \\frac{2\\sigma^2}{4\\delta^2} = \\frac{\\sigma^2}{2\\delta^2}\n$$\n方差缩放为 $\\sigma^2/(2\\delta^2)$。这比单边估计量的方差小 4 倍。\n\n**3. 比较与结论**\n\n| 估计量                     | 偏差阶数        | 方差缩放                      | 每次迭代函数求值次数（$d$ 维）   |\n|----------------------------|-----------------|-------------------------------|--------------------------------|\n| $\\widehat{g}^{\\mathrm{one}}$   | $O(\\delta)$     | $\\frac{2\\sigma^2}{\\delta^2}$  | $d+1$                          |\n| $\\widehat{g}^{\\mathrm{two}}$   | $O(\\delta^2)$   | $\\frac{\\sigma^2}{2\\delta^2}$  | $2d$                           |\n\n双边估计量在偏差（$O(\\delta^2)$ 阶 vs. $O(\\delta)$ 阶）和方差（低 4 倍）方面都更优越。这种更高的统计精度通常会导致 Kiefer-Wolfowitz 算法更快的收敛。其权衡在于计算成本：双边方案每次迭代需要 $2d$ 次函数求值，而单边方案需要 $d+1$ 次。对于大的 $d$，这大约是成本增加了 2 倍。因此，当双边方案的优越统计特性超过其较高的计算成本时，它才是更可取的。这通常发生在函数求值不是极其昂贵，且维度 $d$ 没有大到使 $2d$ 的成本不可接受的情况下（即 $d$ 适中）。\n\n**选项评估：**\n\n*   **A**: 此选项正确地陈述了：$\\widehat{g}_i^{\\mathrm{one}}$ 偏差为 $O(\\delta)$，方差为 $2\\sigma^2/\\delta^2$；$\\widehat{g}_i^{\\mathrm{two}}$ 偏差为 $O(\\delta^2)$，方差为 $\\sigma^2/(2\\delta^2)$。它正确地指出了其更优的条件：$f$ 足够光滑（这使得 $O(\\delta^2)$ 偏差成为可能），维度 $d$ 适中（平衡统计增益与计算成本）。它还正确地指出，使用公共随机数（引入正相关）可以进一步减小双边估计量的方差，使其更具吸引力。这个陈述完全准确。\n*   **B**: 此选项错误地声称两种估计量的偏差阶数均为 $O(\\delta^2)$，并错误地交换了方差表达式。它在事实上是错误的。\n*   **C**: 此选项正确地陈述了单边估计量的性质和双边估计量的偏差。然而，它错误地指出双边估计量的方差为 $4\\sigma^2/\\delta^2$，这与正确值相差 8 倍。\n*   **D**: 此选项有根本性缺陷。它错误地声称双边估计量的方差与 $\\delta$ 无关，并错误地指出其偏差为 $O(\\delta)$。\n\n基于严格的推导，选项 A 为两种估计量的性质和实际考虑因素提供了完整且正确的总结。\n\n**结论：**\n- A：**正确**。偏差阶数、方差缩放以及对权衡的讨论都是准确的。\n- B：**不正确**。单边估计量的偏差不正确，方差项被交换了。\n- C：**不正确**。双边估计量的方差不正确。\n- D：**不正确**。双边估计量的偏差和方差性质都不正确。", "answer": "$$\\boxed{A}$$", "id": "3348722"}, {"introduction": "理论知识的最终检验在于实践应用。本练习将引导您从纸笔分析走向代码实现，挑战您构建 Robbins-Monro 和 Kiefer-Wolfowitz 两种算法来解决最大似然估计问题 ([@problem_id:3348700])。通过实现这些经典的随机近似方法，您将巩固对递归更新过程的理解，并直面参数约束和超参数选择等实际问题。", "problem": "考虑一项任务：对于参数模型，仅使用从独立同分布数据或小批量（minibatch）数据中构建的无偏噪声评估，通过随机近似来计算最大似然参数。其基础是最大似然估计的定义属性（即它是得分方程的解），以及使用无偏噪声观测来寻找根或最大化器的一般随机近似递归。\n\n您需要实现两种一维随机近似过程：\n\n- 用于求解得分方程的 Robbins-Monro 过程。\n- 用于在无法直接访问梯度的情况下最大化带噪声目标的 Kiefer-Wolfowitz 过程。\n\n从以下核心定义和事实开始：\n- 对于一个参数为 $\\theta$ 的模型，基于数据 $x_{1},\\dots,x_{n}$ 的对数似然为 $\\ell(\\theta)=\\sum_{i=1}^{n}\\log f(x_{i};\\theta)$。在正则性条件下，最大似然估计 $\\hat{\\theta}$ 求解得分方程 $\\frac{1}{n}\\sum_{i=1}^{n}\\frac{\\partial}{\\partial\\theta}\\log f(x_{i};\\theta)=0$。\n- Robbins-Monro 过程寻找 $\\theta^{\\star}$ 使得 $h(\\theta^{\\star})=0$，它使用递归式 $\\theta_{k+1}=\\theta_{k}+\\alpha_{k}Z_{k}$，其中 $\\mathbb{E}[Z_{k}\\mid\\theta_{k}]=\\pm h(\\theta_{k})$（符号根据 $h$ 的单调性一致选择），步长序列 $(\\alpha_{k})$ 满足 $\\sum_{k=1}^{\\infty}\\alpha_{k}=\\infty$ 和 $\\sum_{k=1}^{\\infty}\\alpha_{k}^{2}  \\infty$。\n- Kiefer-Wolfowitz 过程对目标函数 $J(\\theta)$ 执行无梯度最大化，通过采样带噪声的函数评估并使用对称有限差分梯度估计 $G_{k}=\\frac{\\widehat{J}(\\theta_{k}+c_{k})-\\widehat{J}(\\theta_{k}-c_{k})}{2c_{k}}$，其中扰动序列 $(c_{k})$ 递减至 $0$，步长 $(\\alpha_{k})$ 满足与上述相同的条件。这里 $\\widehat{J}$ 表示对 $J$ 的一次无偏噪声评估。\n\n实现以下两个模型和过程：\n\n- 模型 A（指数分布率参数）：观测值 $x_{1},\\dots,x_{n}$ 是从率参数为 $\\theta$ 的指数分布中独立抽取的样本（密度函数为 $f(x;\\theta)=\\theta e^{-\\theta x}$，其中 $x\\geq 0$）。单次观测的得分是 $s(\\theta;x)=\\frac{\\partial}{\\partial\\theta}\\log f(x;\\theta)$，其在数据集上的平均值定义了一个函数，该函数的零点对应于最大似然估计。每次迭代使用一个随机采样的数据点来执行 Robbins-Monro 递归。为确保科学真实性和数值稳定性，通过将更新投影到一个固定的可行区间 $\\Theta=[\\varepsilon,M]$（其中 $\\varepsilon0$ 很小， $M0$ 很大）来强制执行参数约束 $\\theta0$。\n\n- 模型 B（正态分布均值）：观测值 $x_{1},\\dots,x_{n}$ 是从均值为 $\\mu$、单位方差的正态分布中独立抽取的样本。目标是每次观测的平均对数似然 $J(\\mu)=\\frac{1}{n}\\sum_{i=1}^{n}\\log f(x_{i};\\mu)$，其中 $f(x;\\mu)=\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}(x-\\mu)^{2}\\right)$。每次迭代使用在随机采样的大小固定的 minibatch 上计算的对称有限差分来执行 Kiefer-Wolfowitz 递归。\n\n为了保证可复现性和科学清晰度，在两个模型中，数据都必须在程序内部从一个已知的真实参数值模拟生成。所有角度（如果出现）都将以弧度为单位指定，但此处没有出现角度。没有出现物理单位。数值步长和参数必须遵守上述条件。所选的步长序列必须为 $\\alpha_{k}=\\frac{a_{0}}{k+1}$ 形式，其中 $a_{0}$ 为正常数；Kiefer-Wolfowitz 的扰动序列必须为 $c_{k}=\\frac{c_{0}}{(k+1)^{\\beta}}$ 形式，其中 $0  \\beta \\leq 1$。\n\n您的程序必须实现以下测试套件。对于每个测试用例，模拟数据集，按指定的迭代次数运行规定的过程，并返回最终的参数估计值。\n\n- 测试用例 1（Robbins-Monro，理想情况，指数分布率参数）：\n  - 真实率参数 $\\theta^{\\star}=2.0$，样本量 $n=800$。\n  - 初始化 $\\theta_{0}=0.4$，步长基数 $a_{0}=0.7$。\n  - 迭代次数 $K=3000$，投影区间 $\\Theta=[10^{-6},20.0]$。\n  - 随机种子 $2025$。\n\n- 测试用例 2（Robbins-Monro，边界接近零的率参数，指数分布率参数）：\n  - 真实率参数 $\\theta^{\\star}=0.25$，样本量 $n=400$。\n  - 初始化 $\\theta_{0}=0.05$，步长基数 $a_{0}=0.5$。\n  - 迭代次数 $K=4000$，投影区间 $\\Theta=[10^{-6},20.0]$。\n  - 随机种子 $1001$。\n\n- 测试用例 3（Kiefer-Wolfowitz，理想情况，正态分布均值）：\n  - 真实均值 $\\mu^{\\star}=1.5$，样本量 $n=1200$。\n  - 初始化 $\\mu_{0}=-2.0$，步长基数 $a_{0}=0.8$，扰动基数 $c_{0}=0.3$，指数 $\\beta=\\frac{1}{3}$。\n  - 迭代次数 $K=2500$，minibatch 大小 $m=50$。\n  - 随机种子 $77$。\n\n- 测试用例 4（Kiefer-Wolfowitz，高噪声，小样本，正态分布均值）：\n  - 真实均值 $\\mu^{\\star}=0.0$，样本量 $n=60$。\n  - 初始化 $\\mu_{0}=3.0$，步长基数 $a_{0}=1.0$，扰动基数 $c_{0}=0.5$，指数 $\\beta=\\frac{1}{3}$。\n  - 迭代次数 $K=3000$，minibatch 大小 $m=10$。\n  - 随机种子 $42$。\n\n对于模型 A 中的 Robbins-Monro 递归，每次迭代使用一个随机选择的观测值来构造带噪声的得分项。对于模型 B 中的 Kiefer-Wolfowitz 递归，每次迭代对两次扰动使用相同的 minibatch 来形成对称有限差分估计。在所有情况下，使用由所提供的种子控制的独立随机性。\n\n您的程序应生成单行输出，其中包含四个测试用例的最终估计值，形式为用方括号括起来的逗号分隔列表（例如，$[\\text{result}_{1},\\text{result}_{2},\\text{result}_{3},\\text{result}_{4}]$）。每个 $\\text{result}_{i}$ 必须是代表该测试用例最终参数估计值的浮点数。", "solution": "当前任务是实现并测试两种随机近似算法，即 Robbins-Monro 过程和 Kiefer-Wolfowitz 过程，用于为两种不同的参数模型计算最大似然参数估计。\n\n首先对问题进行验证，确认其科学上合理、定义明确、客观且完整。所有必要的数据、模型和算法参数均已提供，其所依据的统计和数学原理是标准的且陈述正确。\n\n解决方案包含两个主要部分：\n1.  实现 Robbins-Monro (RM) 算法，以找到指数分布得分方程的根。\n2.  实现 Kiefer-Wolfowitz (KW) 算法，以最大化正态分布的对数似然函数。\n\n**模型 A：用于指数分布率参数的 Robbins-Monro 算法**\n\n数据观测值 $x_{i}$ 从率参数为 $\\theta$ 的指数分布中抽取，其概率密度函数为 $f(x;\\theta)=\\theta e^{-\\theta x}$（$x \\geq 0$）。$\\theta$ 的最大似然估计 (MLE) 通过求解得分方程（平均得分为零）得到。单次观测的对数似然为 $\\log f(x;\\theta) = \\log\\theta - \\theta x$。\n\n对于单个观测值 $x$，得分函数是对数似然函数关于参数 $\\theta$ 的导数：\n$$\ns(\\theta;x) = \\frac{\\partial}{\\partial\\theta}\\log f(x;\\theta) = \\frac{1}{\\theta} - x\n$$\nRM 算法旨在找到期望得分函数 $h(\\theta) = \\mathbb{E}[s(\\theta;X)]$ 的根 $\\theta^{\\star}$。其中 $X$ 是一个来自真实分布（参数为 $\\theta_{true}$）的随机变量，其期望为 $\\mathbb{E}[X] = 1/\\theta_{true}$。因此，\n$$\nh(\\theta) = \\mathbb{E}\\left[\\frac{1}{\\theta} - X\\right] = \\frac{1}{\\theta} - \\mathbb{E}[X] = \\frac{1}{\\theta} - \\frac{1}{\\theta_{true}}\n$$\n$h(\\theta)$ 的根确实是 $\\theta_{true}$。RM 算法提供了一种使用 $h(\\theta)$ 的带噪声样本来迭代寻找这个根的方法。导数 $h'(\\theta) = -1/\\theta^2$ 为负，因此函数是单调递减的。对于一个递减函数，确保向根收敛的 RM 更新规则是：\n$$\n\\theta_{k+1} = \\theta_{k} + \\alpha_k (\\text{h}(\\theta_k) \\text{的带噪声观测})\n$$\n在每次迭代 $k$ 中，我们使用一个随机采样的数据点 $x_{i_k}$ 来形成 $h(\\theta_k)$ 的一个带噪声估计，即得分 $s(\\theta_k; x_{i_k}) = \\frac{1}{\\theta_k} - x_{i_k}$。完整的更新规则是：\n$$\n\\theta_{k+1} = \\theta_{k} + \\alpha_k \\left(\\frac{1}{\\theta_k} - x_{i_k}\\right)\n$$\n步长序列由 $\\alpha_k = \\frac{a_0}{k+1}$ 给出，它满足所需条件 $\\sum_{k=0}^{\\infty}\\alpha_k = \\infty$ 和 $\\sum_{k=0}^{\\infty}\\alpha_k^2  \\infty$。为维持约束 $\\theta0$ 并确保数值稳定性，更新后的估计值被投影到一个可行区间 $\\Theta = [\\varepsilon, M]$ 上。\n\n**模型 B：用于正态分布均值参数的 Kiefer-Wolfowitz 算法**\n\n数据观测值 $x_{i}$ 来自均值为 $\\mu$、单位方差（$1$）的正态分布，其概率密度函数为 $f(x;\\mu) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{1}{2}(x-\\mu)^2\\right)$。任务是最大化平均对数似然函数 $J(\\mu) = \\mathbb{E}[\\log f(X;\\mu)]$。\n\nKW 算法是一种无梯度优化方法，适用于我们只能获得目标函数带噪声评估的情况。它执行一个随机梯度上升更新：\n$$\n\\mu_{k+1} = \\mu_k + \\alpha_k G_k\n$$\n其中 $G_k$ 是梯度 $\\nabla J(\\mu_k) = J'(\\mu_k)$ 的一个估计。这个梯度使用基于目标函数 $\\widehat{J}$ 的带噪声评估的对称有限差分方案来近似。设 $\\widehat{J}(\\mu, \\text{batch})$ 是在一个小批量数据上计算的平均对数似然。在迭代 $k$ 时的梯度估计是：\n$$\nG_k = \\frac{\\widehat{J}(\\mu_k+c_k, \\text{batch}_k) - \\widehat{J}(\\mu_k-c_k, \\text{batch}_k)}{2c_k}\n$$\n步长序列是 $\\alpha_k = \\frac{a_0}{k+1}$，扰动序列是 $c_k = \\frac{c_0}{(k+1)^\\beta}$，其中 $0  \\beta \\leq 1$。对于每次迭代 $k$，从完整数据集中抽取一个大小为 $m$ 的小批量。同一个小批量用于计算 $\\widehat{J}(\\mu_k+c_k, \\cdot)$ 和 $\\widehat{J}(\\mu_k-c_k, \\cdot)$，这是一种方差缩减技术。对于一个小批量 $\\{x_j\\}_{j=1}^m$，带噪声的目标函数为：\n$$\n\\widehat{J}(\\mu, \\text{batch}) = \\frac{1}{m} \\sum_{j=1}^{m} \\left(-\\frac{1}{2}\\log(2\\pi) - \\frac{1}{2}(x_j - \\mu)^2\\right)\n$$\n由于项 $-\\frac{1}{2}\\log(2\\pi)$ 相对于 $\\mu$ 是常数，它在有限差分中被抵消，因此我们可以使用简化的目标函数 $-\\frac{1}{2m}\\sum_{j=1}^{m} (x_j-\\mu)^2$。$G_k$ 的计算简化为 $\\frac{1}{m}\\sum_{j=1}^m(x_j-\\mu_k)$，这是真实梯度 $J'(\\mu_k) = \\mathbb{E}[X-\\mu_k] = \\mu_{true} - \\mu_k$ 的一个无偏估计。\n\n实现过程是建立这两种算法，并使用为四个测试用例分别提供的特定参数来运行它们。使用固定的随机种子确保了模拟数据和算法随机路径的可复现性。", "answer": "```python\nimport numpy as np\n\ndef robbins_monro_exp(theta_star, n, theta0, a0, K, projection_interval, seed):\n    \"\"\"\n    Implements the Robbins-Monro procedure for the exponential rate parameter.\n    \n    Args:\n        theta_star (float): Ground truth rate parameter for data simulation.\n        n (int): Total sample size.\n        theta0 (float): Initial guess for the parameter.\n        a0 (float): Base step-size parameter.\n        K (int): Number of iterations.\n        projection_interval (tuple): (min, max) for parameter projection.\n        seed (int): Random seed for reproducibility.\n\n    Returns:\n        float: The final estimate of the parameter theta.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # Simulate data from the exponential distribution\n    # numpy.random.exponential takes scale beta = 1/lambda (rate)\n    scale = 1.0 / theta_star\n    data = rng.exponential(scale=scale, size=n)\n    \n    theta = theta0\n    eps, M = projection_interval\n    \n    for k in range(K):\n        # Step size sequence\n        alpha_k = a0 / (k + 1.0)\n        \n        # Select one random data point\n        x_i = rng.choice(data)\n        \n        # RM update for the score s(theta; x) = 1/theta - x\n        # Update rule is theta_{k+1} = theta_k + alpha_k * (1/theta_k - x_i)\n        # This corresponds to a stochastic gradient ascent on the log of the likelihood\n        # whose gradient is h(theta)=E[1/theta - X] which is decreasing.\n        # So we use theta_{k+1} = theta_k + alpha_k * observation_of_h(theta_k)\n        # The update direction needs to be correct.\n        # h(theta) = 1/theta - 1/theta_star. It's a decreasing function.\n        # To find its root, we need to use theta_k+1 = theta_k + alpha_k * (1/theta_k - x_i)\n        # Check: if theta_k  theta_star, 1/theta_k  1/theta_star, E[update]  0. Correct.\n        update_val = (1.0 / theta) - x_i\n        theta = theta + alpha_k * update_val\n        \n        # Project back to the feasible interval\n        theta = np.clip(theta, eps, M)\n        \n    return theta\n\ndef kiefer_wolfowitz_norm(mu_star, n, mu0, a0, c0, beta, K, m, seed):\n    \"\"\"\n    Implements the Kiefer-Wolfowitz procedure for the normal mean parameter.\n\n    Args:\n        mu_star (float): Ground truth mean for data simulation.\n        n (int): Total sample size.\n        mu0 (float): Initial guess for the parameter.\n        a0 (float): Base step-size parameter.\n        c0 (float): Base perturbation size parameter.\n        beta (float): Exponent for perturbation sequence decay.\n        K (int): Number of iterations.\n        m (int): Minibatch size.\n        seed (int): Random seed for reproducibility.\n\n    Returns:\n        float: The final estimate of the parameter mu.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # Simulate data from the normal distribution\n    data = rng.normal(loc=mu_star, scale=1.0, size=n)\n    \n    mu = mu0\n    \n    def noisy_objective(mu_val, batch):\n        # Objective is log-likelihood, constant terms can be dropped\n        return -0.5 * np.mean((batch - mu_val)**2)\n\n    for k in range(K):\n        # Step size and perturbation sequences\n        alpha_k = a0 / (k + 1.0)\n        c_k = c0 / ((k + 1.0)**beta)\n        \n        # Sample a minibatch with replacement\n        minibatch = rng.choice(data, size=m, replace=True)\n        \n        # Evaluate objective at perturbed points\n        J_plus = noisy_objective(mu + c_k, minibatch)\n        J_minus = noisy_objective(mu - c_k, minibatch)\n        \n        # Finite-difference gradient estimate\n        G_k = (J_plus - J_minus) / (2.0 * c_k)\n        \n        # KW update (stochastic gradient ascent)\n        mu = mu + alpha_k * G_k\n        \n    return mu\n\ndef solve():\n    \"\"\"\n    Runs the test suite and prints the final results.\n    \"\"\"\n    test_cases = [\n        # Test Case 1 (Robbins-Monro)\n        {'type': 'RM', 'params': {'theta_star': 2.0, 'n': 800, 'theta0': 0.4, 'a0': 0.7, 'K': 3000, 'projection_interval': (1e-6, 20.0), 'seed': 2025}},\n        # Test Case 2 (Robbins-Monro)\n        {'type': 'RM', 'params': {'theta_star': 0.25, 'n': 400, 'theta0': 0.05, 'a0': 0.5, 'K': 4000, 'projection_interval': (1e-6, 20.0), 'seed': 1001}},\n        # Test Case 3 (Kiefer-Wolfowitz)\n        {'type': 'KW', 'params': {'mu_star': 1.5, 'n': 1200, 'mu0': -2.0, 'a0': 0.8, 'c0': 0.3, 'beta': 1/3, 'K': 2500, 'm': 50, 'seed': 77}},\n        # Test Case 4 (Kiefer-Wolfowitz)\n        {'type': 'KW', 'params': {'mu_star': 0.0, 'n': 60, 'mu0': 3.0, 'a0': 1.0, 'c0': 0.5, 'beta': 1/3, 'K': 3000, 'm': 10, 'seed': 42}},\n    ]\n\n    results = []\n    for case in test_cases:\n        if case['type'] == 'RM':\n            result = robbins_monro_exp(**case['params'])\n        elif case['type'] == 'KW':\n            result = kiefer_wolfowitz_norm(**case['params'])\n        results.append(result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3348700"}]}