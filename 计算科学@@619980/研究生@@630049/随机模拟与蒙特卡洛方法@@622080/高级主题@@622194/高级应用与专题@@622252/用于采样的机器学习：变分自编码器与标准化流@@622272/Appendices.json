{"hands_on_practices": [{"introduction": "归一化流 (Normalizing Flows) 通过一系列可逆变换，将简单的基础分布（如高斯分布）转化为复杂的目标分布。这个练习将带你深入剖析构成流模型的关键部件——耦合层 (coupling layer)。通过亲手计算一个真实非体积保持 (RealNVP) 变换的正向与反向过程，并推导其雅可比行列式的对数，你将直观地理解模型是如何改变数据分布同时精确追踪其概率密度变化的 [@problem_id:3318886]。", "problem": "考虑一个来自归一化流的二维实数非体积保持 (RNVP) 耦合变换，它在 $\\mathbb{R}^{2}$ 上的定义如下。设输入为 $z = (z_{1}, z_{2})$，输出为 $x = (x_{1}, x_{2})$，其耦合规则为\n$$\nx_{1} = z_{1}, \\quad x_{2} = z_{2} \\cdot \\exp\\big(s(z_{1})\\big) + t(z_{1}),\n$$\n其中尺度函数 $s(\\cdot)$ 和平移函数 $t(\\cdot)$ 由下式指定\n$$\ns(u) = \\ln\\big(1 + u^{2}\\big), \\qquad t(u) = \\sin(u) + u^{3}.\n$$\n假设在可微双射下，概率密度遵循变量替换原理和标准雅可比行列式法则。对于特定输入\n$$\nz = \\left(1, \\frac{1}{2}\\right),\n$$\n执行以下任务：\n- 计算前向变换 $x = f(z)$。\n- 计算在上述得到的前向象 $x$ 处的逆变换 $z = f^{-1}(x)$。\n- 计算在 $z$ 处前向变换的雅可比矩阵行列式绝对值的对数，该值量化了映射下的局部对数密度变化。\n\n将您的最终结果表示为一个单行矩阵，按顺序包含五个量 $(x_{1}, x_{2}, z_{1}^{\\mathrm{inv}}, z_{2}^{\\mathrm{inv}}, \\Delta)$，其中 $\\Delta$ 表示在 $z$ 处前向变换的雅可比行列式绝对值的对数。不需要四舍五入，您的最终答案必须是精确值。", "solution": "用户提供了一个关于归一化流领域中实数非体积保持 (RNVP) 耦合变换的问题陈述。我将首先验证该问题陈述。\n\n### 问题验证\n\n**步骤1：提取已知条件**\n-   **变换**：一个二维实数非体积保持 (RNVP) 耦合变换 $f: \\mathbb{R}^{2} \\to \\mathbb{R}^{2}$，将 $z = (z_{1}, z_{2})$ 映射到 $x = (x_{1}, x_{2})$。\n-   **变换规则**：\n    $$\n    x_{1} = z_{1}\n    $$\n    $$\n    x_{2} = z_{2} \\cdot \\exp\\big(s(z_{1})\\big) + t(z_{1})\n    $$\n-   **尺度函数**：$s(u) = \\ln\\big(1 + u^{2}\\big)$。\n-   **平移函数**：$t(u) = \\sin(u) + u^{3}$。\n-   **输入点**：$z = \\left(1, \\frac{1}{2}\\right)$。\n-   **要求计算**：\n    1.  在给定输入点处的前向变换 $x = f(z)$。\n    2.  在计算出的象 $x$ 处求值的逆变换 $z^{\\mathrm{inv}} = f^{-1}(x)$。\n    3.  在给定输入点处求值的前向变换雅可比行列式绝对值的对数，$\\Delta = \\ln|\\det(J_{f}(z))|$。\n-   **输出格式**：一个包含五个量 $(x_{1}, x_{2}, z_{1}^{\\mathrm{inv}}, z_{2}^{\\mathrm{inv}}, \\Delta)$ 的单行矩阵。\n\n**步骤2：使用提取的已知条件进行验证**\n-   **科学依据**：该问题基于归一化流的既定数学原理，特别是 RNVP 架构。变换、函数和雅可比计算是该机器学习领域的标准组成部分，并且在数学上是合理的。\n-   **适定性**：该变换是一个微分同胚。对于所有 $u \\in \\mathbb{R}$，尺度函数 $s(u)$ 和平移函数 $t(u)$ 都是无限可微的。该变换是可逆的，因为项 $\\exp(s(z_{1})) = \\exp(\\ln(1+z_1^2)) = 1+z_1^2$ 是严格为正的，从而允许对 $z_{2}$ 有唯一解。该问题定义清晰，并有唯一、稳定的解。\n-   **客观性**：问题陈述纯粹是数学性的和客观的，没有主观或含糊不清的语言。\n\n**步骤3：结论与行动**\n该问题在数学上是一致的、适定的，并有科学依据。因此，它是**有效的**。我将继续进行解答。\n\n### 解题过程\n\n解答需要根据所提供的 RNVP 变换执行三个不同的计算。\n\n**1. 前向变换 $x = f(z)$**\n\n给定输入点 $z = (z_{1}, z_{2}) = (1, \\frac{1}{2})$。\n变换定义为：\n$$\nx_{1} = z_{1}\n$$\n$$\nx_{2} = z_{2} \\cdot \\exp\\big(s(z_{1})\\big) + t(z_{1})\n$$\n首先，我们在 $z_{1} = 1$ 处计算尺度函数 $s(z_{1})$ 和平移函数 $t(z_{1})$ 的值。\n$$\ns(1) = \\ln(1 + 1^{2}) = \\ln(2)\n$$\n$$\nt(1) = \\sin(1) + 1^{3} = \\sin(1) + 1\n$$\n现在，我们可以计算 $x_{1}$ 和 $x_{2}$。\n$$\nx_{1} = z_{1} = 1\n$$\n$$\nx_{2} = z_{2} \\cdot \\exp(s(1)) + t(1) = \\frac{1}{2} \\cdot \\exp(\\ln(2)) + (\\sin(1) + 1)\n$$\n由于 $\\exp(\\ln(2)) = 2$，上式简化为：\n$$\nx_{2} = \\frac{1}{2} \\cdot 2 + \\sin(1) + 1 = 1 + \\sin(1) + 1 = 2 + \\sin(1)\n$$\n因此，前向变换得到的点是 $x = (x_{1}, x_{2}) = (1, 2 + \\sin(1))$。\n\n**2. 逆变换 $z^{\\mathrm{inv}} = f^{-1}(x)$**\n\n为了找到逆变换 $f^{-1}$，我们必须用 $x_{1}$ 和 $x_{2}$ 来求解前向变换方程中的 $z_{1}$ 和 $z_{2}$。\n从 $x_{1} = z_{1}$，我们立即得到：\n$$\nz_{1} = x_{1}\n$$\n我们将此代入第二个方程：\n$$\nx_{2} = z_{2} \\cdot \\exp(s(x_{1})) + t(x_{1})\n$$\n现在，我们求解 $z_{2}$：\n$$\nx_{2} - t(x_{1}) = z_{2} \\cdot \\exp(s(x_{1}))\n$$\n$$\nz_{2} = \\frac{x_{2} - t(x_{1})}{\\exp(s(x_{1}))} = \\big(x_{2} - t(x_{1})\\big) \\exp(-s(x_{1}))\n$$\n所以逆变换 $f^{-1}$ 由下式给出：\n$$\nz_{1} = x_{1}\n$$\n$$\nz_{2} = \\big(x_{2} - t(x_{1})\\big) \\exp(-s(x_{1}))\n$$\n我们在上一步计算出的点 $x = (1, 2 + \\sin(1))$ 处计算这个逆变换。设结果为 $z^{\\mathrm{inv}} = (z_{1}^{\\mathrm{inv}}, z_{2}^{\\mathrm{inv}})$。\n$$\nz_{1}^{\\mathrm{inv}} = x_{1} = 1\n$$\n对于 $z_{2}^{\\mathrm{inv}}$，我们首先在 $x_{1} = 1$ 处计算 $s(x_{1})$ 和 $t(x_{1})$：\n$$\ns(1) = \\ln(1+1^{2}) = \\ln(2)\n$$\n$$\nt(1) = \\sin(1) + 1^{3} = \\sin(1) + 1\n$$\n现在将这些代入 $z_{2}$ 的表达式中：\n$$\nz_{2}^{\\mathrm{inv}} = \\big((2 + \\sin(1)) - (\\sin(1) + 1)\\big) \\exp(-\\ln(2))\n$$\n$$\nz_{2}^{\\mathrm{inv}} = (2 + \\sin(1) - \\sin(1) - 1) \\cdot \\frac{1}{\\exp(\\ln(2))}\n$$\n$$\nz_{2}^{\\mathrm{inv}} = (1) \\cdot \\frac{1}{2} = \\frac{1}{2}\n$$\n正如预期的那样，将逆变换应用于 $z$ 的象会返回原始点。所以，$z^{\\mathrm{inv}} = (z_{1}^{\\mathrm{inv}}, z_{2}^{\\mathrm{inv}}) = (1, \\frac{1}{2})$。\n\n**3. 雅可比行列式绝对值的对数, $\\Delta$**\n\n前向变换是 $x = f(z)$，其中 $x_{1} = z_{1}$ 和 $x_{2} = z_{2} \\exp(s(z_{1})) + t(z_{1})$。雅可比矩阵 $J_{f}(z)$ 由下式给出：\n$$\nJ_{f}(z) = \\begin{pmatrix} \\frac{\\partial x_{1}}{\\partial z_{1}} & \\frac{\\partial x_{1}}{\\partial z_{2}} \\\\ \\frac{\\partial x_{2}}{\\partial z_{1}} & \\frac{\\partial x_{2}}{\\partial z_{2}} \\end{pmatrix}\n$$\n我们计算偏导数：\n$$\n\\frac{\\partial x_{1}}{\\partial z_{1}} = 1\n$$\n$$\n\\frac{\\partial x_{1}}{\\partial z_{2}} = 0\n$$\n$$\n\\frac{\\partial x_{2}}{\\partial z_{1}} = \\frac{\\partial}{\\partial z_{1}} \\left( z_{2} \\exp(s(z_{1})) + t(z_{1}) \\right) = z_{2} \\exp(s(z_{1})) s'(z_{1}) + t'(z_{1})\n$$\n$$\n\\frac{\\partial x_{2}}{\\partial z_{2}} = \\frac{\\partial}{\\partial z_{2}} \\left( z_{2} \\exp(s(z_{1})) + t(z_{1}) \\right) = \\exp(s(z_{1}))\n$$\n雅可比矩阵是：\n$$\nJ_{f}(z) = \\begin{pmatrix} 1 & 0 \\\\ z_{2} \\exp(s(z_{1})) s'(z_{1}) + t'(z_{1}) & \\exp(s(z_{1})) \\end{pmatrix}\n$$\n这是一个下三角矩阵。三角矩阵的行列式是其对角元素的乘积。\n$$\n\\det(J_{f}(z)) = 1 \\cdot \\exp(s(z_{1})) = \\exp(s(z_{1}))\n$$\n行列式绝对值的对数 $\\Delta$ 是：\n$$\n\\Delta = \\ln |\\det(J_{f}(z))| = \\ln |\\exp(s(z_{1}))|\n$$\n由于 $\\exp(\\cdot)$ 总是正的，绝对值是多余的。\n$$\n\\Delta = \\ln(\\exp(s(z_{1}))) = s(z_{1})\n$$\n这是 RNVP 耦合层的一个特性，它使得雅可比行列式的计算非常高效。我们在输入点 $z = (1, \\frac{1}{2})$ 处计算 $\\Delta$，它只依赖于 $z_{1}=1$。\n$$\n\\Delta = s(1) = \\ln(1 + 1^{2}) = \\ln(2)\n$$\n\n**最终整合**\n\n现在我们将五个计算出的量 $(x_{1}, x_{2}, z_{1}^{\\mathrm{inv}}, z_{2}^{\\mathrm{inv}}, \\Delta)$ 组合成一个单行矩阵：\n-   $x_{1} = 1$\n-   $x_{2} = 2 + \\sin(1)$\n-   $z_{1}^{\\mathrm{inv}} = 1$\n-   $z_{2}^{\\mathrm{inv}} = \\frac{1}{2}$\n-   $\\Delta = \\ln(2)$\n\n得到的向量是 $(1, 2 + \\sin(1), 1, \\frac{1}{2}, \\ln(2))$。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1 & 2 + \\sin(1) & 1 & \\frac{1}{2} & \\ln(2)\n\\end{pmatrix}\n}\n$$", "id": "3318886"}, {"introduction": "变分自编码器 (VAE) 的训练核心在于平衡数据重构质量与对潜空间的正则化，这两者体现在证据下界 (ELBO) 目标函数中。本练习设计了一个特殊的思想实验，其中解码器完全忽略了潜变量 $z$ 的信息，旨在揭示一种被称为“后验坍塌” (posterior collapse) 的常见训练难题 [@problem_id:3318904]。通过推导最优编码器参数并计算其对应的信息互量，你将从量化的角度理解模型为何会选择“偷懒”放弃使用潜变量，从而对 VAE 的内在机制有更深刻的认识。", "problem": "考虑一个一维变分自编码器（Variational Autoencoder, VAE），即一个具有标量潜变量 $z \\in \\mathbb{R}$ 和标量观测值 $x \\in \\mathbb{R}$ 的变分自编码器。设数据生成分布为 $p_{\\mathrm{data}}(x)$，这是一个均值为零、方差为 $\\sigma_{d}^{2}$ 的高斯分布，即 $p_{\\mathrm{data}}(x)$ 为 $\\mathcal{N}(0,\\sigma_{d}^{2})$。潜变量的先验分布为 $p(z)$，这是一个标准正态分布 $\\mathcal{N}(0,1)$。指定解码器忽略 $z$ 并输出 $p_{\\theta}(x \\mid z) = \\mathcal{N}(0,\\sigma_{x}^{2})$，该输出与 $z$ 无关。编码器被限制在高斯族 $q_{\\phi}(z \\mid x) = \\mathcal{N}(\\alpha x,\\sigma_{q}^{2})$ 中，其参数为 $\\phi=(\\alpha,\\sigma_{q}^{2})$。\n\n从第一性原理出发，使用证据下界（ELBO）作为边际对数似然下界的定义以及 Kullback–Leibler 散度的定义，推导在 $p_{\\mathrm{data}}(x)$下的期望 ELBO，将其表示为 $\\alpha$ 和 $\\sigma_{q}^{2}$ 的函数，然后确定使该期望 ELBO 最大化的编码器参数 $(\\alpha^{\\star},\\sigma_{q}^{2\\star})$。接下来，从互信息的定义出发，计算在联合分布 $q(x,z) = p_{\\mathrm{data}}(x)\\, q_{\\phi}(z \\mid x)$ 下的互信息 $\\mathbb{I}_{q}(x;z)$，并在最优化参数 $(\\alpha^{\\star},\\sigma_{q}^{2\\star})$ 处求值。将最大化的期望 ELBO 和在最优点处的互信息组成一个对，以行矩阵的形式报告。用 $\\sigma_{x}^{2}$ 和 $\\sigma_{d}^{2}$ 的闭式形式表示你的最终答案。无需四舍五入。", "solution": "用户想要解决一个关于一维变分自编码器（VAE）的问题。\n\n### 第1步：提取已知条件\n已知条件如下：\n- 数据生成分布：$p_{\\mathrm{data}}(x) = \\mathcal{N}(x | 0, \\sigma_{d}^{2})$。\n- 潜变量先验分布：$p(z) = \\mathcal{N}(z | 0, 1)$。\n- 解码器（似然）分布：$p_{\\theta}(x \\mid z) = \\mathcal{N}(x \\mid 0, \\sigma_{x}^{2})$。\n- 编码器（变分后验）分布：$q_{\\phi}(z \\mid x) = \\mathcal{N}(z \\mid \\alpha x, \\sigma_{q}^{2})$。\n- 待优化的参数为 $\\phi = (\\alpha, \\sigma_{q}^{2})$。\n\n### 第2步：使用提取的已知条件进行验证\n该问题具有科学依据，使用了机器学习、概率论和信息论中的标准定义和原理。这是一个目标明确、定义良好的理论练习。所有术语都得到了精确定义，前提条件自洽且一致。该设定虽然经过简化（例如，解码器忽略潜变量），但在数学上是有效的，旨在检验基本概念。该问题并非微不足道，需要多步推导。\n\n### 第3步：结论与行动\n问题有效。我们现在开始进行求解。\n\n### 期望证据下界（ELBO）的推导\n\n单个数据点 $x$ 的证据下界（ELBO）定义为：\n$$\n\\mathrm{ELBO}(x) = \\mathbb{E}_{z \\sim q_{\\phi}(z \\mid x)}[\\ln p_{\\theta}(x \\mid z)] - \\mathrm{KL}(q_{\\phi}(z \\mid x) \\| p(z))\n$$\n我们将分别计算这两项。\n\n第一项是期望对数似然，也称为重构项。解码器为 $p_{\\theta}(x \\mid z) = \\mathcal{N}(x \\mid 0, \\sigma_{x}^{2})$。其概率密度函数为：\n$$\np_{\\theta}(x \\mid z) = \\frac{1}{\\sqrt{2\\pi\\sigma_{x}^{2}}} \\exp\\left(-\\frac{x^2}{2\\sigma_{x}^{2}}\\right)\n$$\n其自然对数为：\n$$\n\\ln p_{\\theta}(x \\mid z) = -\\frac{1}{2}\\ln(2\\pi\\sigma_{x}^{2}) - \\frac{x^2}{2\\sigma_{x}^{2}}\n$$\n由于该表达式不依赖于潜变量 $z$，因此其关于 $q_{\\phi}(z \\mid x)$ 的期望就是表达式本身：\n$$\n\\mathbb{E}_{z \\sim q_{\\phi}(z \\mid x)}[\\ln p_{\\theta}(x \\mid z)] = -\\frac{1}{2}\\ln(2\\pi\\sigma_{x}^{2}) - \\frac{x^2}{2\\sigma_{x}^{2}}\n$$\n\n第二项是编码器分布 $q_{\\phi}(z \\mid x)$ 与先验分布 $p(z)$ 之间的 Kullback–Leibler (KL) 散度。我们已知 $q_{\\phi}(z \\mid x) = \\mathcal{N}(z \\mid \\alpha x, \\sigma_{q}^{2})$ 和 $p(z) = \\mathcal{N}(z \\mid 0, 1)$。两个一元正态分布 $\\mathcal{N}(\\mu_1, \\sigma_1^2)$ 和 $\\mathcal{N}(\\mu_2, \\sigma_2^2)$ 之间的KL散度为：\n$$\n\\mathrm{KL}(\\mathcal{N}(\\mu_1, \\sigma_1^2) \\| \\mathcal{N}(\\mu_2, \\sigma_2^2)) = \\ln\\frac{\\sigma_2}{\\sigma_1} + \\frac{\\sigma_1^2 + (\\mu_1-\\mu_2)^2}{2\\sigma_2^2} - \\frac{1}{2}\n$$\n在我们的例子中，$\\mu_1 = \\alpha x$，$\\sigma_1^2 = \\sigma_q^2$，$\\mu_2 = 0$，$\\sigma_2^2 = 1$。将这些值代入公式：\n$$\n\\mathrm{KL}(q_{\\phi}(z \\mid x) \\| p(z)) = \\ln\\frac{1}{\\sigma_q} + \\frac{\\sigma_q^2 + (\\alpha x - 0)^2}{2 \\cdot 1^2} - \\frac{1}{2}\n$$\n$$\n= -\\frac{1}{2}\\ln(\\sigma_q^2) + \\frac{\\sigma_q^2 + \\alpha^2 x^2}{2} - \\frac{1}{2}\n$$\n\n结合这两项，单个点 $x$ 的ELBO为：\n$$\n\\mathrm{ELBO}(x) = \\left(-\\frac{1}{2}\\ln(2\\pi\\sigma_{x}^{2}) - \\frac{x^2}{2\\sigma_{x}^{2}}\\right) - \\left(-\\frac{1}{2}\\ln(\\sigma_q^2) + \\frac{\\sigma_q^2 + \\alpha^2 x^2}{2} - \\frac{1}{2}\\right)\n$$\n$$\n\\mathrm{ELBO}(x) = -\\frac{1}{2}\\ln(2\\pi\\sigma_{x}^{2}) - \\frac{x^2}{2\\sigma_{x}^{2}} + \\frac{1}{2}\\ln(\\sigma_q^2) - \\frac{\\sigma_q^2}{2} - \\frac{\\alpha^2 x^2}{2} + \\frac{1}{2}\n$$\n\n问题要求在数据分布 $p_{\\mathrm{data}}(x) = \\mathcal{N}(x \\mid 0, \\sigma_d^2)$ 下的期望ELBO。我们将其表示为 $\\mathcal{L}(\\alpha, \\sigma_q^2) = \\mathbb{E}_{x \\sim p_{\\mathrm{data}}(x)}[\\mathrm{ELBO}(x)]$。我们对依赖于 $x$ 的项求期望：\n$$\n\\mathcal{L}(\\alpha, \\sigma_q^2) = \\mathbb{E}_{x \\sim \\mathcal{N}(0,\\sigma_d^2)}\\left[ -\\frac{1}{2}\\ln(2\\pi\\sigma_{x}^{2}) - \\frac{x^2}{2\\sigma_{x}^{2}} + \\frac{1}{2}\\ln(\\sigma_q^2) - \\frac{\\sigma_q^2}{2} - \\frac{\\alpha^2 x^2}{2} + \\frac{1}{2} \\right]\n$$\n利用期望的线性性质，并注意到对于 $x \\sim \\mathcal{N}(0, \\sigma_d^2)$，我们有 $\\mathbb{E}[x^2] = \\mathrm{Var}(x) + (\\mathbb{E}[x])^2 = \\sigma_d^2 + 0^2 = \\sigma_d^2$：\n$$\n\\mathcal{L}(\\alpha, \\sigma_q^2) = -\\frac{1}{2}\\ln(2\\pi\\sigma_{x}^{2}) - \\frac{\\mathbb{E}[x^2]}{2\\sigma_{x}^{2}} + \\frac{1}{2}\\ln(\\sigma_q^2) - \\frac{\\sigma_q^2}{2} - \\frac{\\alpha^2 \\mathbb{E}[x^2]}{2} + \\frac{1}{2}\n$$\n$$\n\\mathcal{L}(\\alpha, \\sigma_q^2) = -\\frac{1}{2}\\ln(2\\pi\\sigma_{x}^{2}) - \\frac{\\sigma_d^2}{2\\sigma_{x}^{2}} - \\frac{\\alpha^2 \\sigma_d^2}{2} + \\frac{1}{2}\\ln(\\sigma_q^2) - \\frac{\\sigma_q^2}{2} + \\frac{1}{2}\n$$\n\n### 期望ELBO的最大化\n\n为了找到最优参数 $(\\alpha^{\\star}, \\sigma_q^{2\\star})$，我们通过对 $\\mathcal{L}(\\alpha, \\sigma_q^2)$ 分别求关于 $\\alpha$ 和 $\\sigma_q^2$ 的偏导数并令其为零来最大化该函数。不涉及 $\\alpha$ 或 $\\sigma_q^2$ 的项在优化过程中是常数。\n\n该函数可以对 $\\alpha$ 和 $\\sigma_q^2$ 分解：\n$$\n\\mathcal{L}(\\alpha, \\sigma_q^2) = \\left( -\\frac{\\alpha^2 \\sigma_d^2}{2} \\right) + \\left( \\frac{1}{2}\\ln(\\sigma_q^2) - \\frac{\\sigma_q^2}{2} \\right) + \\text{常数}\n$$\n对于 $\\alpha$：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\alpha} = - \\alpha \\sigma_d^2\n$$\n令其为零，$- \\alpha \\sigma_d^2 = 0$。由于 $\\sigma_d^2 > 0$，这意味着 $\\alpha^{\\star} = 0$。二阶导数 $\\frac{\\partial^2 \\mathcal{L}}{\\partial \\alpha^2} = -\\sigma_d^2  0$，证实了这是一个最大值点。\n\n对于 $\\sigma_q^2$：令 $v = \\sigma_q^2$。我们优化 $f(v) = \\frac{1}{2}\\ln(v) - \\frac{v}{2}$ 这一项。\n$$\n\\frac{\\partial f}{\\partial v} = \\frac{1}{2v} - \\frac{1}{2}\n$$\n令其为零，$\\frac{1}{2v} - \\frac{1}{2} = 0 \\implies v = 1$。因此，$\\sigma_q^{2\\star} = 1$。二阶导数 $\\frac{\\partial^2 f}{\\partial v^2} = -\\frac{1}{2v^2}$，对于 $v>0$ 其值小于0，证实了这是一个最大值点。\n\n因此，最优编码器参数为 $(\\alpha^{\\star}, \\sigma_q^{2\\star}) = (0, 1)$。\n\n### 最大化的期望ELBO值\n\n我们将这些最优参数代回到 $\\mathcal{L}(\\alpha, \\sigma_q^2)$ 的表达式中：\n$$\n\\mathcal{L}^{\\star} = \\mathcal{L}(0, 1) = -\\frac{1}{2}\\ln(2\\pi\\sigma_{x}^{2}) - \\frac{\\sigma_d^2}{2\\sigma_{x}^{2}} - \\frac{0^2 \\sigma_d^2}{2} + \\frac{1}{2}\\ln(1) - \\frac{1}{2} + \\frac{1}{2}\n$$\n$$\n\\mathcal{L}^{\\star} = -\\frac{1}{2}\\ln(2\\pi\\sigma_{x}^{2}) - \\frac{\\sigma_d^2}{2\\sigma_{x}^{2}} - 0 + 0 - \\frac{1}{2} + \\frac{1}{2}\n$$\n$$\n\\mathcal{L}^{\\star} = -\\frac{1}{2}\\ln(2\\pi\\sigma_{x}^{2}) - \\frac{\\sigma_{d}^{2}}{2\\sigma_{x}^{2}}\n$$\n\n### 互信息\n\n在联合分布 $q(x,z) = p_{\\mathrm{data}}(x)q_{\\phi}(z \\mid x)$ 下，$x$ 和 $z$ 之间的互信息由下式给出：\n$$\n\\mathbb{I}_q(x;z) = \\mathbb{E}_{x \\sim p_{\\mathrm{data}}(x)}[\\mathrm{KL}(q_\\phi(z \\mid x) \\| p(z))]\n$$\n这正是我们为 ELBO 计算的 KL 散度项的期望。我们需要在最优化参数 $(\\alpha^{\\star}, \\sigma_q^{2\\star}) = (0, 1)$ 处计算该值。\n\n在这些参数下，编码器变为：\n$$\nq_{\\phi^{\\star}}(z \\mid x) = \\mathcal{N}(z \\mid 0 \\cdot x, 1) = \\mathcal{N}(z \\mid 0, 1)\n$$\n这与先验分布 $p(z)$ 完全相同。因此，对于任何 $x$ 值，KL散度为：\n$$\n\\mathrm{KL}(q_{\\phi^{\\star}}(z \\mid x) \\| p(z)) = \\mathrm{KL}(\\mathcal{N}(z \\mid 0, 1) \\| \\mathcal{N}(z \\mid 0, 1)) = 0\n$$\n由于对所有 $x$ 而言KL散度都为0，其在 $p_{\\mathrm{data}}(x)$ 上的期望也为0：\n$$\n\\mathbb{I}_q(x;z)|_{\\phi=\\phi^{\\star}} = \\mathbb{E}_{x \\sim p_{\\mathrm{data}}(x)}[0] = 0\n$$\n这个结果是预料之中的。最优编码器 $q_{\\phi^{\\star}}(z \\mid x)$ 完全忽略了输入 $x$，并输出一个与先验分布相同的分布。这意味着在模型 $q(x,z)$ 下，潜变量 $z$ 与观测值 $x$ 是独立的，因此它们的互信息为零。这种现象被称为后验坍塌（posterior collapse）。\n\n### 最终结果\n\n该对由最大化的期望ELBO和在最优点处的互信息组成。\n- 最大化的期望ELBO：$\\mathcal{L}^{\\star} = -\\frac{1}{2}\\ln(2\\pi\\sigma_{x}^{2}) - \\frac{\\sigma_{d}^{2}}{2\\sigma_{x}^{2}}$\n- 互信息：$\\mathbb{I}_q(x;z)|_{\\phi=\\phi^{\\star}} = 0$", "answer": "$$\n\\boxed{\\begin{pmatrix} -\\frac{1}{2}\\ln(2\\pi\\sigma_{x}^{2}) - \\frac{\\sigma_{d}^{2}}{2\\sigma_{x}^{2}}  0 \\end{pmatrix}}\n$$", "id": "3318904"}, {"introduction": "建立一个生成模型只是第一步，评估其性能同样至关重要。本练习为你提供了一组来自学习模型的典型评估输出，让你模拟评估真实场景中的模型表现 [@problem_id:3318928]。你需要计算三个关键指标：衡量似然的“每维比特数” (bits-per-dimension)、评估重要性采样效率的“有效样本量” (Effective Sample Size)，以及检验模型校准度的卡方统计量。完成这个练习将使你掌握评估和比较不同生成模型的实用技能。", "problem": "您正在评估一个由归一化流 (Normalizing Flow, NF) 模型生成的学习提议分布，该分布针对一个一维合成目标；同时也在评估一个用于对留出数据进行评分的基于流的密度模型。对于一个大小为 $N = 6$、数据维度为 $D = 1$ 的留出集，您被提供了以下信息：\n\n1. 在学习到的密度模型下的自然对数似然，$\\{\\ln p_{\\theta}(x_i)\\}_{i=1}^{N}$：\n$\\{-1.20,\\,-0.80,\\,-1.50,\\,-1.10,\\,-0.95,\\,-1.30\\}$。\n\n2. 来自自归一化重要性采样的未归一化对数重要性权重，其提议分布为 $q$，未归一化目标为 $\\tilde{p}$。这些权重由 $a_i = \\ln \\tilde{p}(x_i) - \\ln q(x_i)$ 给出，其中 $i=1,\\dots,N$：\n$\\{-0.2,\\,0.0,\\,0.1,\\,-0.5,\\,0.3,\\,-0.1\\}$。\n\n3. 对于相同的 $N$ 个测试点，在学习到的密度模型下的概率积分变换 (Probability Integral Transform, PIT) 值，定义为 $u_i = F_{\\theta}(x_i)$，其中 $F_{\\theta}$ 是 $p_{\\theta}$ 的累积分布函数：\n$\\{0.12,\\,0.47,\\,0.68,\\,0.83,\\,0.24,\\,0.59\\}$。\n\n仅从适用于随机模拟和采样机器学习的核心定义出发：\n\n- 每维比特数 (bits-per-dimension, bpd) 是在整个数据集上以比特为单位衡量的每维度平均信息量，其中一个数据点 $x$ 在模型 $p_{\\theta}$ 下的信息量是其似然的负底为2的对数。\n- 对于自归一化重要性采样，有效样本量 (Effective Sample Size, ESS) 是根据归一化重要性权重 $\\{w_i\\}$ 定义的，其中 $w_i = \\tilde{w}_i / \\sum_{j=1}^{N} \\tilde{w}_j$ 且 $\\tilde{w}_i \\propto \\tilde{p}(x_i)/q(x_i)$。\n- 可以通过比较 $\\{u_i\\}$ 的经验分布与 $[0,1]$ 上的均匀分布来构建一个基于概率积分变换 (PIT) 的校准差异。使用皮尔逊卡方差异，在 $[0,1]$ 上设置 $B=4$ 个等宽的区间，并将观测计数与均匀期望进行比较。\n\n计算以下三个量：\n\n1. 在 $N$ 个留出点上的每维比特数。\n2. 根据给定的未归一化对数重要性权重计算的有效样本量。\n3. 使用 $B=4$ 个等宽直方图区间的基于 PIT 的皮尔逊卡方差异。\n\n以行向量 $\\big[\\text{bpd},\\,\\text{ESS},\\,\\chi^2\\big]$ 的形式报告您的答案，其中每个条目四舍五入到四位有效数字。不需要单位。", "solution": "该问题要求计算三个不同的指标来评估一个学习到的模型：每维比特数 (bpd)、有效样本量 (ESS) 和一个基于概率积分变换 (PIT) 的皮尔逊卡方统计量。该问题是适定的、有科学依据的，并为获得唯一解提供了所有必要的数据和定义。\n\n提供的数据如下：\n- 样本量：$N=6$。\n- 数据维度：$D=1$。\n- 自然对数似然：$\\{\\ln p_{\\theta}(x_i)\\}_{i=1}^{6} = \\{-1.20,\\,-0.80,\\,-1.50,\\,-1.10,\\,-0.95,\\,-1.30\\}$。\n- 未归一化对数重要性权重：$\\{a_i\\}_{i=1}^{6} = \\{-0.2,\\,0.0,\\,0.1,\\,-0.5,\\,0.3,\\,-0.1\\}$。\n- 概率积分变换 (PIT) 值：$\\{u_i\\}_{i=1}^{6} = \\{0.12,\\,0.47,\\,0.68,\\,0.83,\\,0.24,\\,0.59\\}$。\n\n我们将依次计算这三个量。\n\n1.  **每维比特数 (bpd)**\n\n每维比特数 (bpd) 定义为以2为底的平均负对数似然，并按维度数进行归一化。公式为：\n$$ \\text{bpd} = \\frac{1}{D} \\left\\langle -\\log_2 p_{\\theta}(x) \\right\\rangle = -\\frac{1}{DN} \\sum_{i=1}^{N} \\log_2 p_{\\theta}(x_i) $$\n我们已知自然对数似然 $\\ln p_{\\theta}(x_i)$。我们使用对数的换底公式 $\\log_2(y) = \\frac{\\ln(y)}{\\ln(2)}$，以给定的值来表示 bpd：\n$$ \\text{bpd} = -\\frac{1}{DN \\ln(2)} \\sum_{i=1}^{N} \\ln p_{\\theta}(x_i) $$\n首先，我们计算给定的对数似然之和：\n$$ \\sum_{i=1}^{6} \\ln p_{\\theta}(x_i) = (-1.20) + (-0.80) + (-1.50) + (-1.10) + (-0.95) + (-1.30) = -6.85 $$\n现在，我们将 $D=1$，$N=6$ 和总和代入 bpd 公式：\n$$ \\text{bpd} = -\\frac{1}{(1)(6) \\ln(2)} (-6.85) = \\frac{6.85}{6 \\ln(2)} $$\n使用近似值 $\\ln(2) \\approx 0.693147$：\n$$ \\text{bpd} \\approx \\frac{6.85}{6 \\times 0.693147} \\approx \\frac{6.85}{4.158882} \\approx 1.64708 $$\n四舍五入到四位有效数字，我们得到 $\\text{bpd} \\approx 1.647$。\n\n2.  **有效样本量 (ESS)**\n\n自归一化重要性采样的有效样本量 (ESS) 由以下公式给出：\n$$ \\text{ESS} = \\frac{1}{\\sum_{i=1}^{N} w_i^2} $$\n其中 $w_i$ 是归一化的重要性权重。归一化权重由未归一化权重 $\\tilde{w}_i$ 计算得出，公式为 $w_i = \\frac{\\tilde{w}_i}{\\sum_{j=1}^{N} \\tilde{w}_j}$。\n问题提供了未归一化的对数重要性权重，$a_i = \\ln \\tilde{w}_i$（相差一个常数）。因此，我们可以取 $\\tilde{w}_i = \\exp(a_i)$。对数权重集合为 $\\{a_i\\} = \\{-0.2,\\,0.0,\\,0.1,\\,-0.5,\\,0.3,\\,-0.1\\}$。\n未归一化的权重是：\n$$ \\{\\tilde{w}_i\\}_{i=1}^6 = \\{\\exp(-0.2),\\, \\exp(0.0),\\, \\exp(0.1),\\, \\exp(-0.5),\\, \\exp(0.3),\\, \\exp(-0.1)\\} $$\n一种数值稳定的计算 ESS 的方法是使用公式 $\\text{ESS} = \\frac{(\\sum \\tilde{w}_i)^2}{\\sum \\tilde{w}_i^2}$。\n首先，我们计算未归一化权重之和：\n$$ \\sum_{i=1}^6 \\tilde{w}_i = \\exp(-0.2) + 1 + \\exp(0.1) + \\exp(-0.5) + \\exp(0.3) + \\exp(-0.1) $$\n$$ \\sum_{i=1}^6 \\tilde{w}_i \\approx 0.81873 + 1.0 + 1.10517 + 0.60653 + 1.34986 + 0.90484 = 5.78513 $$\n接下来，我们计算未归一化权重的平方和，即 $\\sum \\exp(2a_i)$：\n$$ \\sum_{i=1}^6 \\tilde{w}_i^2 = \\exp(-0.4) + \\exp(0.0) + \\exp(0.2) + \\exp(-1.0) + \\exp(0.6) + \\exp(-0.2) $$\n$$ \\sum_{i=1}^6 \\tilde{w}_i^2 \\approx 0.67032 + 1.0 + 1.22140 + 0.36788 + 1.82212 + 0.81873 = 5.90045 $$\n现在我们可以计算 ESS：\n$$ \\text{ESS} = \\frac{(\\sum \\tilde{w}_i)^2}{\\sum \\tilde{w}_i^2} \\approx \\frac{(5.78513)^2}{5.90045} \\approx \\frac{33.4677}{5.90045} \\approx 5.67202 $$\n四舍五入到四位有效数字，我们得到 $\\text{ESS} \\approx 5.672$。\n\n3.  **基于 PIT 的皮尔逊卡方差异 ($\\chi^2$)**\n\n如果模型是完美校准的，PIT 值 $\\{u_i\\}$ 应服从区间 $[0,1]$ 上的均匀分布。我们使用皮尔逊卡方拟合优度检验来检验这个假设。我们在 $[0,1]$ 上使用 $B=4$ 个等宽的区间。\n这些区间是：$[0, 0.25)$, $[0.25, 0.50)$, $[0.50, 0.75)$ 和 $[0.75, 1.0]$。\n样本总数为 $N=6$。每个区间 $j$ 的期望计数 $E_j$ 是 $E_j = N/B = 6/4 = 1.5$。\n给定的 PIT 值为 $\\{u_i\\} = \\{0.12,\\,0.47,\\,0.68,\\,0.83,\\,0.24,\\,0.59\\}$。\n我们将这些值分到各个区间中，以找到观测计数 $O_j$：\n- 区间 1: $[0, 0.25) \\rightarrow$ 值 $0.12, 0.24$。所以，$O_1 = 2$。\n- 区间 2: $[0.25, 0.50) \\rightarrow$ 值 $0.47$。所以，$O_2 = 1$。\n- 区间 3: $[0.50, 0.75) \\rightarrow$ 值 $0.68, 0.59$。所以，$O_3 = 2$。\n- 区间 4: $[0.75, 1.0] \\rightarrow$ 值 $0.83$。所以，$O_4 = 1$。\n观测计数为 $\\{O_j\\} = \\{2, 1, 2, 1\\}$。总和为 $2+1+2+1=6=N$。\n皮尔逊卡方统计量的计算公式为：\n$$ \\chi^2 = \\sum_{j=1}^{B} \\frac{(O_j - E_j)^2}{E_j} $$\n代入数值：\n$$ \\chi^2 = \\frac{(2 - 1.5)^2}{1.5} + \\frac{(1 - 1.5)^2}{1.5} + \\frac{(2 - 1.5)^2}{1.5} + \\frac{(1 - 1.5)^2}{1.5} $$\n$$ \\chi^2 = \\frac{(0.5)^2}{1.5} + \\frac{(-0.5)^2}{1.5} + \\frac{(0.5)^2}{1.5} + \\frac{(-0.5)^2}{1.5} $$\n$$ \\chi^2 = \\frac{0.25}{1.5} + \\frac{0.25}{1.5} + \\frac{0.25}{1.5} + \\frac{0.25}{1.5} = 4 \\times \\frac{0.25}{1.5} = \\frac{1}{1.5} = \\frac{2}{3} $$\n$$ \\chi^2 = \\frac{2}{3} \\approx 0.66666\\dots $$\n四舍五入到四位有效数字，我们得到 $\\chi^2 \\approx 0.6667$。\n\n计算出的三个量，四舍五入到四位有效数字，分别是：\n- $\\text{bpd} = 1.647$\n- $\\text{ESS} = 5.672$\n- $\\chi^2 = 0.6667$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1.647  5.672  0.6667\n\\end{pmatrix}\n}\n$$", "id": "3318928"}]}