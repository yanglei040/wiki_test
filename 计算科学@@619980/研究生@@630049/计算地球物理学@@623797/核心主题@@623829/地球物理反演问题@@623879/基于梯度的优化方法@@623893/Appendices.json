{"hands_on_practices": [{"introduction": "地球物理反演问题在本质上通常是病态的，这意味着微小的数据噪声可能导致解出现巨大的偏差。Tikhonov 正则化是稳定该问题的一种经典方法。本练习将通过数值实验，探索正则化参数 $\\lambda$ 如何控制数据拟合与解稳定性之间的权衡，并揭示其如何直接影响梯度下降法的收敛性 [@problem_id:3601017]。", "problem": "考虑一个在计算地球物理学中常见的线性反问题，其中模型参数向量 $\\mathbf{m} \\in \\mathbb{R}^n$ 通过一个正演算子 $\\mathbf{G} \\in \\mathbb{R}^{p \\times n}$ 与观测数据 $\\mathbf{d} \\in \\mathbb{R}^p$ 相关联。一个带有 Tikhonov 正则化的标准二次目标函数定义为\n$$\nJ(\\mathbf{m}) = \\frac{1}{2}\\left\\|\\mathbf{C}_d^{-\\frac{1}{2}}(\\mathbf{G}\\mathbf{m} - \\mathbf{d})\\right\\|_2^2 + \\frac{\\lambda}{2}\\left\\|\\mathbf{L}\\mathbf{m}\\right\\|_2^2,\n$$\n其中 $\\mathbf{C}_d \\in \\mathbb{R}^{p \\times p}$ 是一个对称正定的数据协方差矩阵，$\\mathbf{L} \\in \\mathbb{R}^{q \\times n}$ 是一个正则化算子，$\\lambda > 0$ 是平衡数据拟合差与正则化的权衡参数。$J(\\mathbf{m})$ 的梯度为\n$$\n\\nabla J(\\mathbf{m}) = \\mathbf{G}^\\top\\mathbf{C}_d^{-1}(\\mathbf{G}\\mathbf{m} - \\mathbf{d}) + \\lambda \\mathbf{L}^\\top \\mathbf{L}\\mathbf{m},\n$$\n并且，对于线性的 $\\mathbf{G}$ 和二次正则化，Hessian 矩阵是一个对称正定矩阵\n$$\n\\mathbf{H}(\\lambda) = \\mathbf{G}^\\top\\mathbf{C}_d^{-1}\\mathbf{G} + \\lambda \\mathbf{L}^\\top \\mathbf{L}.\n$$\n$\\mathbf{H}(\\lambda)$ 的谱条件由条件数来量化\n$$\n\\kappa(\\mathbf{H}(\\lambda)) = \\frac{\\lambda_{\\max}(\\mathbf{H}(\\lambda))}{\\lambda_{\\min}(\\mathbf{H}(\\lambda))},\n$$\n其中 $\\lambda_{\\max}$ 和 $\\lambda_{\\min}$ 分别表示最大和最小特征值。在基于梯度的方法中，例如具有固定步长 $\\tau > 0$ 的最速下降法，其更新公式为\n$$\n\\mathbf{m}_{k+1} = \\mathbf{m}_k - \\tau \\nabla J(\\mathbf{m}_k),\n$$\n对于二次目标函数，其收缩因子由 $\\mathbf{H}(\\lambda)$ 的谱通过对每个特征值 $\\lambda_i(\\mathbf{H}(\\lambda))$ 的项 $|1 - \\tau \\lambda_i(\\mathbf{H}(\\lambda))|$ 控制，这直接影响每次迭代的收敛速率。过小的 $\\lambda$ 会因 $\\mathbf{G}^\\top\\mathbf{C}_d^{-1}\\mathbf{G}$ 的小特征值而导致病态条件，从而在固定 $\\tau$ 的情况下减慢收敛速度；而过大的 $\\lambda$ 会增加曲率，如果 $\\tau$ 不相应减小，则可能在固定 $\\tau$ 的情况下导致不稳定或发散。\n\n你的任务是编写一个程序，该程序 (i) 分析权衡参数 $\\lambda$ 对一个简单病态线性算子的 $\\mathbf{H}(\\lambda)$ 条件数的影响，以及 (ii) 通过一个一维恢复问题，说明当使用固定步长梯度下降法时，过大或过小的 $\\lambda$ 如何改变收敛行为，包括步长和迭代次数。\n\n你必须使用以下配置：\n\n- 条件数分析：令 $\\mathbf{G} = \\mathrm{diag}(1, 10^{-3}) \\in \\mathbb{R}^{2 \\times 2}$，$\\mathbf{C}_d = \\mathbf{I}_2$，以及 $\\mathbf{L} = \\mathbf{I}_2$。那么\n$$\n\\mathbf{H}(\\lambda) = \\mathbf{G}^\\top \\mathbf{G} + \\lambda \\mathbf{I}_2 = \\mathrm{diag}(1 + \\lambda, 10^{-6} + \\lambda),\n$$\n并且条件数为\n$$\n\\kappa(\\mathbf{H}(\\lambda)) = \\frac{1 + \\lambda}{10^{-6} + \\lambda}.\n$$\n\n- 一维恢复：考虑标量参数 $m \\in \\mathbb{R}$，正演算子 $g \\in \\mathbb{R}$，数据协方差 $c_d \\in \\mathbb{R}$，以及正则化算子 $l = 1$。使用 $g = 10^{-3}$，$c_d = 1$，并用 $m_{\\text{true}} = 2$ 生成无噪声数据 $d = g m_{\\text{true}}$。考虑目标函数\n$$\nJ(m) = \\frac{1}{2}(g m - d)^2 + \\frac{\\lambda}{2} m^2,\n$$\n其梯度和 Hessian 矩阵为\n$$\n\\nabla J(m) = g(g m - d) + \\lambda m, \\quad H(\\lambda) = g^2 + \\lambda.\n$$\n对于步长 $\\tau = 0.5$ 的固定步长梯度下降法，更新公式为\n$$\nm_{k+1} = m_k - \\tau\\left[g(g m_k - d) + \\lambda m_k\\right],\n$$\n从 $m_0 = 0$ 开始。精确极小值点为\n$$\nm^\\star(\\lambda) = \\frac{g d}{g^2 + \\lambda} = \\frac{g^2 m_{\\text{true}}}{g^2 + \\lambda}.\n$$\n使用相对误差停止准则 $|m_k - m^\\star(\\lambda)|/|m^\\star(\\lambda)|  10^{-8}$ 和最大 $10^{4}$ 次迭代。对每个 $\\lambda$，计算：\n- 来自上述 $2 \\times 2$ 配置的条件数 $\\kappa(\\mathbf{H}(\\lambda))$，\n- 达到容差所需的迭代次数（如果方法非收缩或在最大迭代次数内未能达到容差，则使用 $-1$），\n- 最终迭代值 $m_{\\text{final}}$（停止时的估计值，或者如果提早检测到发散，则为 5 次迭代后的值），\n- 前 5 个更新步长的平均幅度，定义为 $\\frac{1}{5}\\sum_{k=0}^{4}|m_{k+1} - m_k|$（如果在停止前迭代次数少于 5 次，则对可用步长取平均），\n- 一个布尔值，指示方法是否在给定的容差和迭代上限下收敛。\n\n测试集：\n使用以下权衡参数值：\n$$\n\\lambda \\in \\left\\{0, 10^{-6}, 10^{-2}, 1, 10\\right\\}.\n$$\n\n最终输出格式：\n你的程序应生成单行输出，包含测试集的结果，格式为一个逗号分隔的列表，并用方括号括起来，其中每个条目对应一个 $\\lambda$，并且本身也是一个列表：\n`[[\\lambda, \\kappa, iters, m_final, avg_first5_step, converged], ...]`\n所有条目必须是如上指定的数值或布尔类型，并且顺序必须与测试集中 $\\lambda$ 的顺序匹配。", "solution": "该问题要求分析 Tikhonov 正则化线性最小二乘问题，重点关注正则化参数 $\\lambda$ 的作用。我们将首先验证问题的提法，然后进行原则性的求解。该问题具有科学依据，是适定的，并且所有必要的参数和方程都已提供且一致。因此，它是有效的。\n\n问题的核心在于最小化目标函数：\n$$\nJ(\\mathbf{m}) = \\frac{1}{2}\\left\\|\\mathbf{C}_d^{-\\frac{1}{2}}(\\mathbf{G}\\mathbf{m} - \\mathbf{d})\\right\\|_2^2 + \\frac{\\lambda}{2}\\left\\|\\mathbf{L}\\mathbf{m}\\right\\|_2^2\n$$\n第一项衡量预测数据 $\\mathbf{G}\\mathbf{m}$ 与观测数据 $\\mathbf{d}$ 之间的拟合差，由数据协方差矩阵 $\\mathbf{C}_d$ 的逆进行加权。第二项是正则化（或惩罚）项，用于惩罚不期望的模型特征，由算子 $\\mathbf{L}$ 控制。权衡参数 $\\lambda  0$ 平衡了拟合数据与满足正则化约束之间的重要性。\n\n对于基于梯度的优化方法，收敛速率与目标函数的 Hessian 矩阵的性质密切相关。梯度下降的更新步骤由下式给出：\n$$\n\\mathbf{m}_{k+1} = \\mathbf{m}_k - \\tau \\nabla J(\\mathbf{m}_k)\n$$\n其中 $\\tau$ 是步长。对于二次目标函数，误差根据 Hessian 矩阵 $\\mathbf{H}$ 的特征值 $\\lambda_i$ 进行传播。收敛性由收缩因子决定，对于每个特征分量，收缩因子为 $|1 - \\tau\\lambda_i(\\mathbf{H})|$。为了使方法收敛，该因子对于所有特征值都必须小于 1，这要求 $0  \\tau  2/\\lambda_{\\max}(\\mathbf{H})$。总体的收敛速率由最慢收敛的分量决定，该分量对应于 $\\max_i |1 - \\tau\\lambda_i(\\mathbf{H})|$。最优固定步长 $\\tau_{\\text{opt}} = 2/(\\lambda_{\\min} + \\lambda_{\\max})$ 产生的收敛速率由条件数 $\\kappa(\\mathbf{H}) = \\lambda_{\\max}/\\lambda_{\\min}$ 决定。条件数越大，意味着收敛速率越慢。\n\n该问题的 Hessian 矩阵为：\n$$\n\\mathbf{H}(\\lambda) = \\mathbf{G}^\\top\\mathbf{C}_d^{-1}\\mathbf{G} + \\lambda \\mathbf{L}^\\top \\mathbf{L}\n$$\n参数 $\\lambda$ 直接影响 Hessian 矩阵的谱，从而影响其条件数。\n\n**1. 条件数分析**\n\n我们给定一个简化的 $2 \\times 2$ 案例来分析 $\\lambda$ 对条件数的影响：$\\mathbf{G} = \\mathrm{diag}(1, 10^{-3})$，$\\mathbf{C}_d = \\mathbf{I}_2$ 和 $\\mathbf{L} = \\mathbf{I}_2$。Hessian 矩阵变为：\n$$\n\\mathbf{H}(\\lambda) = \\mathbf{G}^\\top \\mathbf{G} + \\lambda \\mathbf{I}_2 = \\mathrm{diag}(1^2, (10^{-3})^2) + \\lambda \\mathrm{diag}(1, 1) = \\mathrm{diag}(1 + \\lambda, 10^{-6} + \\lambda)\n$$\n特征值是对角线元素：$\\lambda_{\\max} = 1 + \\lambda$ 和 $\\lambda_{\\min} = 10^{-6} + \\lambda$。条件数为：\n$$\n\\kappa(\\mathbf{H}(\\lambda)) = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}} = \\frac{1 + \\lambda}{10^{-6} + \\lambda}\n$$\n- 如果 $\\lambda \\to 0$，则 $\\kappa(\\mathbf{H}(\\lambda)) \\to 1/10^{-6} = 10^6$。Hessian 矩阵是严重病态的，这预示着梯度方法的收敛会很慢。\n- 如果 $\\lambda \\to \\infty$，则 $\\kappa(\\mathbf{H}(\\lambda)) \\to \\lambda/\\lambda = 1$。Hessian 矩阵变为完全良态的。然而，一个非常大的 $\\lambda$ 会过度惩罚模型范数，导致解 $m^\\star(\\lambda)$ 严重偏向于零，可能无法很好地拟合数据。\n\n**2. 一维恢复与收敛行为**\n\n我们现在研究一个标量问题（$n=p=1$），其中 $g = 10^{-3}$，$c_d = 1$，$l=1$，$m_{\\text{true}} = 2$，数据 $d = g m_{\\text{true}} = 2 \\times 10^{-3}$。Hessian 矩阵是一个标量 $H(\\lambda) = g^2 + \\lambda = 10^{-6} + \\lambda$。步长为 $\\tau = 0.5$ 的梯度下降更新的收缩因子为：\n$$\n\\rho(\\lambda) = |1 - \\tau H(\\lambda)| = |1 - 0.5(10^{-6} + \\lambda)|\n$$\n收敛要求 $\\rho(\\lambda)  1$。\n- 对于小的 $\\lambda$（例如 $\\lambda=0$ 或 $\\lambda=10^{-6}$），$H(\\lambda)$ 非常小。收缩因子 $\\rho(\\lambda)$ 仅略小于 1。这导致收敛极其缓慢。\n- 对于中等的 $\\lambda$（例如 $\\lambda=1$），$H(\\lambda) \\approx 1$。收缩因子为 $\\rho(1) \\approx |1 - 0.5(1)| = 0.5$。这是一个健康的收缩因子，预示着快速收敛。这对应于一个良态问题，其中步长对于曲率是合适的。\n- 对于大的 $\\lambda$（例如 $\\lambda=10$），$H(\\lambda) \\approx 10$。步长 $\\tau = 0.5$ 对于目标函数增大的曲率来说太大了。对 $\\tau$ 的收敛条件是 $0  \\tau  2/H(\\lambda)$。对于 $\\lambda=10$，这意味着 $\\tau  2/10.000001 \\approx 0.2$。由于我们使用固定步长 $\\tau=0.5$，这个条件被违反了。收缩因子变为 $\\rho(10) \\approx |1 - 0.5(10)| = 4  1$。该方法将会发散。\n\n程序将为指定的 $\\lambda$ 值实现梯度下降迭代，并计算所要求的指标，这些指标预计将与此理论分析一致。\n- $\\lambda = 0, 10^{-6}$：高条件数，收敛非常慢（会达到最大迭代次数）。\n- $\\lambda = 10^{-2}, 1$：较低的条件数，观察到更快的收敛。\n- $\\lambda = 10$：由于步长相对于问题的曲率过大而导致发散。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Analyzes the effect of the Tikhonov regularization parameter lambda on the\n    conditioning and convergence of a gradient descent method for a linear\n    inverse problem.\n    \"\"\"\n    # Test suite of trade-off parameters\n    test_lambdas = [0.0, 1e-6, 1e-2, 1.0, 10.0]\n\n    # Parameters for the 1D recovery problem\n    g = 1e-3\n    m_true = 2.0\n    d = g * m_true\n    tau = 0.5\n    m0 = 0.0\n    max_iter = 10000\n    tol = 1e-8\n\n    final_results = []\n\n    for lam in test_lambdas:\n        # (i) Conditioning analysis from the 2x2 configuration\n        kappa = (1.0 + lam) / (1e-6 + lam)\n\n        # (ii) One-dimensional recovery problem\n        # Hessian and contraction factor\n        H_lambda = g**2 + lam\n        rho = abs(1.0 - tau * H_lambda)\n        \n        # Exact minimizer\n        m_star_num = g**2 * m_true\n        m_star_den = g**2 + lam\n        m_star = m_star_num / m_star_den if m_star_den != 0 else float('inf')\n\n        # Initialize iteration variables\n        m = m0\n        iters = 0\n        converged = False\n        steps = []\n        \n        # Handle the theoretically divergent case\n        if rho >= 1.0:\n            # As per the problem, run for 5 iterations to get m_final and avg step\n            loop_limit = 5\n            for k in range(loop_limit):\n                grad = (g**2 + lam) * m - g * d\n                m_next = m - tau * grad\n                steps.append(abs(m_next - m))\n                m = m_next\n            \n            m_final = m\n            avg_first5_step = np.mean(steps)\n            iters = -1\n            converged = False\n            \n            final_results.append([lam, kappa, iters, m_final, avg_first5_step, converged])\n            continue\n\n        # Handle convergent or slowly converging cases\n        for k in range(max_iter):\n            grad = (g**2 + lam) * m - g * d\n            m_next = m - tau * grad\n            \n            # Store step magnitude for the first 5 steps\n            if k  5:\n                steps.append(abs(m_next - m))\n            \n            m = m_next\n            iters += 1\n            \n            # Check for convergence using the relative error criterion\n            # The case m_star == 0 is not possible for lam >= 0 but this is robust\n            relative_error = abs(m - m_star) / abs(m_star) if m_star != 0.0 else abs(m)\n            if relative_error  tol:\n                converged = True\n                break\n        \n        m_final = m\n        # If the loop finished due to max_iter, it did not converge\n        if not converged:\n            iters = -1\n        \n        # Calculate average of available step magnitudes\n        avg_first5_step = np.mean(steps) if steps else 0.0\n        \n        final_results.append([lam, kappa, iters, m_final, avg_first5_step, converged])\n\n    # Format the output as a single string\n    output_str = \"[\" + ','.join(map(str, final_results)) + \"]\"\n    print(output_str)\n\nsolve()\n```", "id": "3601017"}, {"introduction": "在前一个练习的基础上，我们知道即使有正则化，病态问题对于标准梯度下降法来说仍然是一个挑战，因为不同参数方向上的曲率差异巨大。本练习将展示 Adagrad、RMSProp 和 Adam 等自适应优化器的强大能力。这些方法能为每个参数动态调整学习率，从而在这种“各向异性”的目标函数曲面上实现更快的收敛 [@problem_id:3601039]。", "problem": "考虑一个双参数线性化逆问题，该问题是计算地球物理学中小扰动声速反演的典型代表。设未知参数矢量为 $m = [v_1, v_2]^\\top$，其中 $v_1$ 和 $v_2$ 是 P波速度，单位为米/秒 ($\\mathrm{m/s}$)。假设合成观测数据由一个作用于 $m$ 的线性正演算子 $G \\in \\mathbb{R}^{2 \\times 2}$ 生成，并带有协方差为单位矩阵的零均值高斯噪声。在此噪声模型下，最大似然估计可简化为最小化以下最小二乘目标函数\n$$\nJ(m) = \\frac{1}{2} \\left\\| G m - d_{\\mathrm{obs}} \\right\\|_2^2,\n$$\n其中 $d_{\\mathrm{obs}} \\in \\mathbb{R}^2$ 是观测数据。为了测试，我们将使用无噪声的合成数据 $d_{\\mathrm{obs}} = G m^\\star$，其中 $m^\\star$ 是预设的真实解（ground-truth），因此全局最小值点即为 $m^\\star$。算子 $G$ 将被选为对角矩阵，以通过其奇异值来控制曲率的各向异性。曲率的巨大差异是由于 $J(m)$ 的Hessian矩阵为 $H = G^\\top G$，其特征值是 $G$ 对角线元素的平方；当这些元素存在数量级差异时，就会产生强烈的各向异性。\n\n您的任务是实现四种基于梯度的优化方法，从给定的初始猜测 $m^{(0)}$ 出发来最小化 $J(m)$：梯度下降法 (Gradient Descent, GD)、自适应梯度算法 (Adaptive Gradient, Adagrad)、均方根传播 (Root Mean Square Propagation, RMSProp) 和自适应矩估计 (Adaptive Moment Estimation, Adam)。对于每种方法，精确运行 $10$ 次迭代。对于每次迭代 $t = 1, 2, \\ldots, 10$，计算欧几里得误差的平方\n$$\nE^{(t)} = \\left\\| m^{(t)} - m^\\star \\right\\|_2^2,\n$$\n并报告每种方法的序列 $\\{E^{(t)}\\}_{t=1}^{10}$。所有误差值 $E^{(t)}$ 必须以 $(\\mathrm{m/s})^2$ 为单位进行报告。\n\n使用下面指定的超参数，根据其标准更新定义来实现这些方法。除了实现这些成熟方法所必需的公式外，不要使用任何快捷公式。从第一性原理推导 $J(m)$ 的梯度。所有方法都使用相同的梯度。每个测试案例都给出了优化器的学习率和其他超参数。不允许使用线搜索或额外的启发式方法。\n\n测试套件和超参数：\n- 设 $G = \\mathrm{diag}(g_1, g_2)$，其中 $g_1, g_2 > 0$，且 $d_{\\mathrm{obs}} = G m^\\star$。\n- 定义三个测试案例如下：\n    1. 测试案例 A (理想情况，强但非极端的各向异性)：\n       - $g_1 = 1000$, $g_2 = 1$。\n       - $m^\\star = [2000.0, 1500.0]^\\top$ $(\\mathrm{m/s})$。\n       - $m^{(0)} = [3000.0, 500.0]^\\top$ $(\\mathrm{m/s})$。\n       - GD 学习率 $\\alpha_{\\mathrm{GD}} = 10^{-6}$。\n       - Adagrad 学习率 $\\alpha_{\\mathrm{Ada}} = 10^{-1}$，数值稳定器 $\\epsilon = 10^{-8}$。\n       - RMSProp 学习率 $\\alpha_{\\mathrm{RMS}} = 10^{-3}$，衰减参数 $\\rho = 0.9$，稳定器 $\\epsilon = 10^{-8}$。\n       - Adam 学习率 $\\alpha_{\\mathrm{Adam}} = 10^{-3}$，一阶矩衰减 $\\beta_1 = 0.9$，二阶矩衰减 $\\beta_2 = 0.999$，稳定器 $\\epsilon = 10^{-8}$。\n    2. 测试案例 B (具有极端各向异性的边缘情况)：\n       - $g_1 = 10000$, $g_2 = 0.1$。\n       - $m^\\star = [2200.0, 1400.0]^\\top$ $(\\mathrm{m/s})$。\n       - $m^{(0)} = [1800.0, 2000.0]^\\top$ $(\\mathrm{m/s})$。\n       - GD 学习率 $\\alpha_{\\mathrm{GD}} = 10^{-10}$。\n       - Adagrad 学习率 $\\alpha_{\\mathrm{Ada}} = 10^{-2}$，$\\epsilon = 10^{-8}$。\n       - RMSProp 学习率 $\\alpha_{\\mathrm{RMS}} = 10^{-3}$，$\\rho = 0.9$，$\\epsilon = 10^{-8}$。\n       - Adam 学习率 $\\alpha_{\\mathrm{Adam}} = 10^{-3}$，$\\beta_1 = 0.9$, $\\beta_2 = 0.999$, $\\epsilon = 10^{-8}$。\n    3. 测试案例 C (边界条件：在解处初始化)：\n       - $g_1 = 1000$, $g_2 = 1$。\n       - $m^\\star = [2500.0, 1000.0]^\\top$ $(\\mathrm{m/s})$。\n       - $m^{(0)} = [2500.0, 1000.0]^\\top$ $(\\mathrm{m/s})$。\n       - GD 学习率 $\\alpha_{\\mathrm{GD}} = 10^{-6}$。\n       - Adagrad 学习率 $\\alpha_{\\mathrm{Ada}} = 10^{-1}$，$\\epsilon = 10^{-8}$。\n       - RMSProp 学习率 $\\alpha_{\\mathrm{RMS}} = 10^{-3}$，$\\rho = 0.9$, $\\epsilon = 10^{-8}$。\n       - Adam 学习率 $\\alpha_{\\mathrm{Adam}} = 10^{-3}$，$\\beta_1 = 0.9$, $\\beta_2 = 0.999$, $\\epsilon = 10^{-8}$。\n\n要求的最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的、以逗号分隔的结果列表。\n- 将每个测试案例表示为一个包含四个内部列表的列表，顺序为 $[\\text{GD}, \\text{Adagrad}, \\text{RMSProp}, \\text{Adam}]$。\n- 每个内部列表必须包含 $10$ 个浮点数值（即序列 $\\{E^{(t)}\\}_{t=1}^{10}$，单位为 $(\\mathrm{m/s})^2$），不含单位字符串或任何附加文本。\n- 因此，总输出是一个长度为 $3$ 的列表，每个条目对应一个测试案例。例如，格式化模式为\n`[[[e_{1,GD}^{(1)},...,e_{1,GD}^{(10)}], [e_{1,Ada}^{(1)},...,e_{1,Ada}^{(10)}], [e_{1,RMS}^{(1)},...,e_{1,RMS}^{(10)}], [e_{1,Adam}^{(1)},...,e_{1,Adam}^{(10)}]], ...]`\n其中每个 $e_{k,\\cdot}^{(t)}$ 是一个以 $(\\mathrm{m/s})^2$ 为单位的浮点数，$k \\in \\{1,2,3\\}$ 是测试案例的索引。", "solution": "问题陈述被评估为**有效**。这是一个在计算地球物理学和数值优化领域内定义明确、有科学依据且客观的问题。\n\n- **给定条件**：\n    - 参数矢量：$m = [v_1, v_2]^\\top$，其中 $v_1, v_2$ 是 P波速度，单位为 $\\mathrm{m/s}$。\n    - 目标函数：$J(m) = \\frac{1}{2} \\left\\| G m - d_{\\mathrm{obs}} \\right\\|_2^2$。\n    - 正演算子：$G = \\mathrm{diag}(g_1, g_2) \\in \\mathbb{R}^{2 \\times 2}$，其中 $g_1, g_2 > 0$。\n    - 观测数据：$d_{\\mathrm{obs}} = G m^\\star$，其中 $m^\\star$ 是真实解参数矢量。\n    - 误差度量：$E^{(t)} = \\left\\| m^{(t)} - m^\\star \\right\\|_2^2$，单位为 $(\\mathrm{m/s})^2$。\n    - 每种方法的迭代次数：精确为 $10$ 次。\n    - 优化方法：梯度下降法 (GD)、Adagrad、RMSProp、Adam。\n    - 测试案例 A：\n        - $g_1 = 1000$, $g_2 = 1$。\n        - $m^\\star = [2000.0, 1500.0]^\\top$。\n        - $m^{(0)} = [3000.0, 500.0]^\\top$。\n        - 超参数：$\\alpha_{\\mathrm{GD}} = 10^{-6}$；$\\alpha_{\\mathrm{Ada}} = 10^{-1}, \\epsilon = 10^{-8}$；$\\alpha_{\\mathrm{RMS}} = 10^{-3}, \\rho = 0.9, \\epsilon = 10^{-8}$；$\\alpha_{\\mathrm{Adam}} = 10^{-3}, \\beta_1 = 0.9, \\beta_2 = 0.999, \\epsilon = 10^{-8}$。\n    - 测试案例 B：\n        - $g_1 = 10000$, $g_2 = 0.1$。\n        - $m^\\star = [2200.0, 1400.0]^\\top$。\n        - $m^{(0)} = [1800.0, 2000.0]^\\top$。\n        - 超参数：$\\alpha_{\\mathrm{GD}} = 10^{-10}$；$\\alpha_{\\mathrm{Ada}} = 10^{-2}, \\epsilon = 10^{-8}$；$\\alpha_{\\mathrm{RMS}} = 10^{-3}, \\rho = 0.9, \\epsilon = 10^{-8}$；$\\alpha_{\\mathrm{Adam}} = 10^{-3}, \\beta_1 = 0.9, \\beta_2 = 0.999, \\epsilon = 10^{-8}$。\n    - 测试案例 C：\n        - $g_1 = 1000$, $g_2 = 1$。\n        - $m^\\star = [2500.0, 1000.0]^\\top$。\n        - $m^{(0)} = [2500.0, 1000.0]^\\top$。\n        - 超参数：与测试案例 A 相同。\n\n- **验证**：\n    - **科学依据**：该问题使用了逆问题中常见的标准线性最小二乘公式，并基于已建立的梯度优化算法。它没有科学上的缺陷。\n    - **适定性**：该问题是适定的。对于每个测试案例，所有需要的数据、初始条件和参数都已提供，从而可以得到唯一且可计算的解序列。使用无噪声的合成数据确保了真实解 $m^\\star$ 是 $J(m)$ 的唯一最小值点。\n    - **客观性**：该问题使用精确的数学和数值术语进行陈述，没有任何主观性或歧义。\n\n该问题有效，可以进行求解。\n\n### 基于原理的求解\n\n问题的核心是使用四种不同的基于梯度的优化算法来最小化最小二乘目标函数 $J(m)$。所有这些方法都依赖于目标函数的梯度，我们必须首先推导该梯度。\n\n_1. 梯度推导_\n\n目标函数为\n$$\nJ(m) = \\frac{1}{2} \\| G m - d_{\\mathrm{obs}} \\|_2^2\n$$\n展开平方欧几里得范数可得：\n$$\nJ(m) = \\frac{1}{2} (G m - d_{\\mathrm{obs}})^\\top (G m - d_{\\mathrm{obs}}) = \\frac{1}{2} (m^\\top G^\\top G m - 2 m^\\top G^\\top d_{\\mathrm{obs}} + d_{\\mathrm{obs}}^\\top d_{\\mathrm{obs}})\n$$\n为了求得梯度 $\\nabla_m J(m)$，我们对矢量 $m$ 进行微分：\n$$\n\\nabla_m J(m) = \\frac{1}{2} (2 G^\\top G m - 2 G^\\top d_{\\mathrm{obs}})\n$$\n这可以简化为梯度的表达式，该表达式将被所有四种优化方法使用：\n$$\n\\nabla_m J(m) = G^\\top(G m - d_{\\mathrm{obs}})\n$$\n由于 $d_{\\mathrm{obs}} = G m^\\star$，我们也可以将梯度写为 $\\nabla_m J(m) = G^\\top G (m - m^\\star)$。这证实了梯度为零当且仅当 $m = m^\\star$，因为 $G^\\top G$ 是可逆的 ($g_1, g_2  0$)。\n\n_2. 优化算法_\n\n所有算法都遵循迭代更新方案。从一个初始猜测 $m^{(0)}$ 开始，它们生成一个估计序列 $m^{(1)}, m^{(2)}, \\ldots, m^{(10)}$。对于每次迭代 $t \\in \\{1, 2, \\ldots, 10\\}$，我们计算梯度 $g^{(t)} = \\nabla_m J(m^{(t-1)})$，然后更新模型参数以获得 $m^{(t)}$。\n\n_a. 梯度下降法 (GD)_\n\n这是最简单的基于梯度的方法。其更新规则是在负梯度方向上迈出一步，步长由学习率 $\\alpha$ 缩放。\n$$\nm^{(t)} = m^{(t-1)} - \\alpha_{\\mathrm{GD}} \\, g^{(t)}\n$$\n\n_b. 自适应梯度算法 (Adagrad)_\n\nAdagrad为每个参数自适应地调整学习率，将其缩放为与所有过去梯度的平方和的平方根成反比。它对于梯度稀疏或参数间梯度尺度变化大的问题很有效。\n- 初始化梯度累积矢量：$S^{(0)} = \\mathbf{0}$。\n- 在第 $t$ 次迭代时：\n    $$\n    S^{(t)} = S^{(t-1)} + g^{(t)} \\odot g^{(t)}\n    $$\n    $$\n    m^{(t)} = m^{(t-1)} - \\frac{\\alpha_{\\mathrm{Ada}}}{\\sqrt{S^{(t)}} + \\epsilon} \\odot g^{(t)}\n    $$\n这里，$\\odot$ 表示逐元素乘法，除法也是逐元素的。小常数 $\\epsilon$ 用于防止除以零。\n\n_c. 均方根传播 (RMSProp)_\n\nRMSProp 修改了 Adagrad，以防止学习率单调递减。它使用平方梯度的指数衰减平均值，从而赋予近期梯度更大的权重。\n- 初始化平方梯度的移动平均值：$v^{(0)} = \\mathbf{0}$。\n- 在第 $t$ 次迭代时：\n    $$\n    v^{(t)} = \\rho \\, v^{(t-1)} + (1-\\rho) \\, g^{(t)} \\odot g^{(t)}\n    $$\n    $$\n    m^{(t)} = m^{(t-1)} - \\frac{\\alpha_{\\mathrm{RMS}}}{\\sqrt{v^{(t)}} + \\epsilon} \\odot g^{(t)}\n    $$\n参数 $\\rho$ 是移动平均的衰减率。\n\n_d. 自适应矩估计 (Adam)_\n\nAdam 结合了 RMSProp（基于梯度的二阶矩自适应调整各参数的学习率）和动量（使用过去梯度的一阶矩，即指数衰减平均值）的思想。\n- 初始化一阶矩矢量：$p^{(0)} = \\mathbf{0}$。\n- 初始化二阶矩矢量：$v^{(0)} = \\mathbf{0}$。\n- 在第 $t$ 次迭代时：\n    $$\n    p^{(t)} = \\beta_1 \\, p^{(t-1)} + (1-\\beta_1) \\, g^{(t)}\n    $$\n    $$\n    v^{(t)} = \\beta_2 \\, v^{(t-1)} + (1-\\beta_2) \\, g^{(t)} \\odot g^{(t)}\n    $$\n    对矩进行初始化偏差校正：\n    $$\n    \\hat{p}^{(t)} = \\frac{p^{(t)}}{1 - \\beta_1^t}\n    $$\n    $$\n    \\hat{v}^{(t)} = \\frac{v^{(t)}}{1 - \\beta_2^t}\n    $$\n    然后参数更新为：\n    $$\n    m^{(t)} = m^{(t-1)} - \\alpha_{\\mathrm{Adam}} \\frac{\\hat{p}^{(t)}}{\\sqrt{\\hat{v}^{(t)}} + \\epsilon}\n    $$\n这里，$\\beta_1$ 和 $\\beta_2$ 分别是一阶和二阶矩估计的衰减率。\n\n_3. 计算过程_\n\n对三个测试案例中的每一个，执行以下过程：\n1. 初始化参数 $g_1, g_2, m^\\star, m^{(0)}$ 以及四种优化方法中每一种的超参数。\n2. 构建矩阵 $G = \\mathrm{diag}(g_1, g_2)$ 并计算合成数据 $d_{\\mathrm{obs}} = G m^\\star$。\n3. 对于四种方法（GD、Adagrad、RMSProp、Adam）中的每一种：\n    a. 将参数矢量 $m$ 初始化为 $m^{(0)}$，并将任何方法特定的状态变量（例如，$S, v, p$）初始化为零矢量。\n    b. 创建一个列表来存储平方误差序列 $\\{E^{(t)}\\}_{t=1}^{10}$。\n    c. 对 $t$ 从 $1$ 到 $10$ 进行循环：\n        i. 使用当前参数矢量 $m$ 计算梯度 $g^{(t)} = G^\\top(G m - d_{\\mathrm{obs}})$。\n        ii. 使用特定算法的更新规则将参数矢量 $m$ 更新为一个新的矢量 $m_{new}$。这个 $m_{new}$ 成为 $m^{(t)}$。\n        iii. 计算平方误差 $E^{(t)} = \\|m^{(t)} - m^\\star\\|_2^2$。\n        iv. 将 $E^{(t)}$ 存储在误差列表中。\n        v. 用 $m_{new}$ 替换当前参数矢量 $m$，用于下一次迭代。\n4. 将一个测试案例中每种方法收集到的误差列表进行汇总，最后，将所有三个测试案例的结果格式化为所需的字符串输出。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and evaluates four gradient-based optimization methods (GD, Adagrad, RMSProp, Adam) \n    on a 2D linearized inverse problem for three distinct test cases.\n    \"\"\"\n    \n    test_cases = [\n        # Test Case A\n        {\n            \"g\": np.array([1000.0, 1.0]),\n            \"m_star\": np.array([2000.0, 1500.0]),\n            \"m0\": np.array([3000.0, 500.0]),\n            \"hyperparams\": {\n                \"gd\": {\"alpha\": 1e-6},\n                \"adagrad\": {\"alpha\": 1e-1, \"epsilon\": 1e-8},\n                \"rmsprop\": {\"alpha\": 1e-3, \"rho\": 0.9, \"epsilon\": 1e-8},\n                \"adam\": {\"alpha\": 1e-3, \"beta1\": 0.9, \"beta2\": 0.999, \"epsilon\": 1e-8},\n            }\n        },\n        # Test Case B\n        {\n            \"g\": np.array([10000.0, 0.1]),\n            \"m_star\": np.array([2200.0, 1400.0]),\n            \"m0\": np.array([1800.0, 2000.0]),\n            \"hyperparams\": {\n                \"gd\": {\"alpha\": 1e-10},\n                \"adagrad\": {\"alpha\": 1e-2, \"epsilon\": 1e-8},\n                \"rmsprop\": {\"alpha\": 1e-3, \"rho\": 0.9, \"epsilon\": 1e-8},\n                \"adam\": {\"alpha\": 1e-3, \"beta1\": 0.9, \"beta2\": 0.999, \"epsilon\": 1e-8},\n            }\n        },\n        # Test Case C\n        {\n            \"g\": np.array([1000.0, 1.0]),\n            \"m_star\": np.array([2500.0, 1000.0]),\n            \"m0\": np.array([2500.0, 1000.0]),\n            \"hyperparams\": {\n                \"gd\": {\"alpha\": 1e-6},\n                \"adagrad\": {\"alpha\": 1e-1, \"epsilon\": 1e-8},\n                \"rmsprop\": {\"alpha\": 1e-3, \"rho\": 0.9, \"epsilon\": 1e-8},\n                \"adam\": {\"alpha\": 1e-3, \"beta1\": 0.9, \"beta2\": 0.999, \"epsilon\": 1e-8},\n            }\n        }\n    ]\n\n    num_iterations = 10\n    \n    def gradient_J(m, G, d_obs):\n        return G.T @ (G @ m - d_obs)\n\n    def run_gd(m0, m_star, G, d_obs, hyperparams):\n        errors = []\n        m = m0.copy()\n        alpha = hyperparams['alpha']\n        for _ in range(num_iterations):\n            grad = gradient_J(m, G, d_obs)\n            m -= alpha * grad\n            errors.append(np.sum((m - m_star)**2))\n        return errors\n\n    def run_adagrad(m0, m_star, G, d_obs, hyperparams):\n        errors = []\n        m = m0.copy()\n        alpha = hyperparams['alpha']\n        epsilon = hyperparams['epsilon']\n        S = np.zeros_like(m0)\n        for _ in range(num_iterations):\n            grad = gradient_J(m, G, d_obs)\n            S += grad**2\n            m -= (alpha / (np.sqrt(S) + epsilon)) * grad\n            errors.append(np.sum((m - m_star)**2))\n        return errors\n\n    def run_rmsprop(m0, m_star, G, d_obs, hyperparams):\n        errors = []\n        m = m0.copy()\n        alpha = hyperparams['alpha']\n        rho = hyperparams['rho']\n        epsilon = hyperparams['epsilon']\n        v = np.zeros_like(m0)\n        for _ in range(num_iterations):\n            grad = gradient_J(m, G, d_obs)\n            v = rho * v + (1 - rho) * grad**2\n            m -= (alpha / (np.sqrt(v) + epsilon)) * grad\n            errors.append(np.sum((m - m_star)**2))\n        return errors\n\n    def run_adam(m0, m_star, G, d_obs, hyperparams):\n        errors = []\n        m = m0.copy()\n        alpha = hyperparams['alpha']\n        beta1 = hyperparams['beta1']\n        beta2 = hyperparams['beta2']\n        epsilon = hyperparams['epsilon']\n        p = np.zeros_like(m0)\n        v = np.zeros_like(m0)\n        for t in range(1, num_iterations + 1):\n            grad = gradient_J(m, G, d_obs)\n            p = beta1 * p + (1 - beta1) * grad\n            v = beta2 * v + (1 - beta2) * grad**2\n            p_hat = p / (1 - beta1**t)\n            v_hat = v / (1 - beta2**t)\n            m -= alpha * p_hat / (np.sqrt(v_hat) + epsilon)\n            errors.append(np.sum((m - m_star)**2))\n        return errors\n\n    all_results = []\n    for case in test_cases:\n        G = np.diag(case['g'])\n        m_star = case['m_star']\n        m0 = case['m0']\n        d_obs = G @ m_star\n        hyperparams = case['hyperparams']\n        \n        case_results = []\n        \n        # GD\n        case_results.append(run_gd(m0, m_star, G, d_obs, hyperparams['gd']))\n        # Adagrad\n        case_results.append(run_adagrad(m0, m_star, G, d_obs, hyperparams['adagrad']))\n        # RMSProp\n        case_results.append(run_rmsprop(m0, m_star, G, d_obs, hyperparams['rmsprop']))\n        # Adam\n        case_results.append(run_adam(m0, m_star, G, d_obs, hyperparams['adam']))\n        \n        all_results.append(case_results)\n\n    # Format the output string as specified\n    case_strings = []\n    for case_result in all_results:\n        method_strings = []\n        for method_errors in case_result:\n            # Use a general formatter that avoids scientific notation for smaller numbers\n            # but handles large ones. str() is usually sufficient.\n            error_str = '[' + ','.join(map(str, method_errors)) + ']'\n            method_strings.append(error_str)\n        case_strings.append('[' + ','.join(method_strings) + ']')\n    \n    final_output = '[' + ','.join(case_strings) + ']'\n    print(final_output)\n\nsolve()\n```", "id": "3601039"}, {"introduction": "在见证了自适应方法的有效性之后，最后一个练习提出了一个关键问题：我们应如何将这些算法应用于具有不同物理单位的参数（如速度和密度）？这个概念性练习利用量纲分析揭示了，直接地应用这些算法可能在物理上是无意义的。它强调了在进行优化前，通过无量纲化或恰当的预处理来确保优化步骤在物理上保持一致性和尺度不变性的重要性，这是建立稳健科学模型的核心原则 [@problem_id:3601067]。", "problem": "考虑一个计算地球物理学中的波形反演设置，其中模型场是空间变化的地震波速 $v(\\mathbf{x})$（单位为 $[\\mathrm{m}/\\mathrm{s}]$）和质量密度 $\\rho(\\mathbf{x})$（单位为 $[\\mathrm{kg}/\\mathrm{m}^3]$），定义在一个三维域 $\\Omega \\subset \\mathbb{R}^3$ 上，其体积元 $d\\mathbf{x}$ 的单位为 $[\\mathrm{m}^3]$。在接收点位置 $\\mathbf{x}_r$ 的预测压力数据 $p(t,\\mathbf{x}_r)$ 的单位是帕斯卡 $[\\mathrm{Pa}]$。每个接收点和时间的数据残差定义为 $r(t,\\mathbf{x}_r;m) = p(t,\\mathbf{x}_r;m) - d(t,\\mathbf{x}_r)$，其中 $d(t,\\mathbf{x}_r)$ 是观测到的压力。失配泛函由加权最小二乘形式定义\n$$\nJ(m) = \\frac{1}{2}\\sum_{r}\\int_{0}^{T} \\left( W_d\\, r(t,\\mathbf{x}_r;m) \\right)^2 \\, dt,\n$$\n其中 $W_d$ 是一个数据加权算子，其选择使得 $W_d\\, r$ 是无量纲的（例如，$W_d = C_d^{-1/2}$，其中 $C_d$ 是一个单位为 $[\\mathrm{Pa}^2]$ 的数据协方差算子）。根据构造，$J(m)$ 是无量纲的。关于模型参数的梯度定义为在空间上的标准 $L^2(\\Omega)$ 内积下，Gateaux 导数的 Riesz 表示。具体来说，扰动 $\\delta v$ 和 $\\delta \\rho$ 产生变分\n$$\n\\delta J = \\int_{\\Omega} g_v(\\mathbf{x}) \\, \\delta v(\\mathbf{x}) \\, d\\mathbf{x} + \\int_{\\Omega} g_{\\rho}(\\mathbf{x}) \\, \\delta \\rho(\\mathbf{x}) \\, d\\mathbf{x},\n$$\n该变分是无量纲的。一个带有逐参数自适应性的基于梯度的更新写为\n$$\nv_{k+1}(\\mathbf{x}) = v_k(\\mathbf{x}) - \\alpha_v \\, g_v(\\mathbf{x}), \\quad \\rho_{k+1}(\\mathbf{x}) = \\rho_k(\\mathbf{x}) - \\alpha_{\\rho} \\, g_{\\rho}(\\mathbf{x}),\n$$\n其中 $\\alpha_v$ 和 $\\alpha_{\\rho}$ 是标量步长，对于不同参数可以不同。假设正演算子和伴随状态法与上述定义一致，并产生 $g_v$ 和 $g_{\\rho}$，它们在空间 $L^2$ 内积下实现了所述的 Gateaux 导数。\n\n仅使用基于上述定义的量纲分析，并且不引用任何特定的波动方程，回答以下问题：\n\n- 确定当内积为 $\\int_{\\Omega}(\\cdot)\\, d\\mathbf{x}$ 时，由 $\\delta J$ 是无量纲的要求所蕴含的 $g_v$ 和 $g_{\\rho}$ 的物理单位。\n- 从这些单位推断，为了使更新在原始（有量纲的）参数 $v$ 和 $\\rho$ 上物理一致，$\\alpha_v$ 和 $\\alpha_{\\rho}$ 需要具备什么单位。\n- 基于此推理，论证逐参数自适应性（即选择 $\\alpha_v \\neq \\alpha_{\\rho}$）是否在适当缩放的失配函数中保留了正确的无量纲梯度步长，以及在什么条件下这种自适应性可以产生无量纲、尺度不变的更新。\n\n哪个选项最能概括正确的结论？\n\nA. 在一个经过适当无量纲化的失配函数中，只有当更新在无量纲变量上进行时（例如，使用参考尺度 $v_0$ 和 $\\rho_0$ 的 $\\tilde v = v/v_0$ 和 $\\tilde \\rho = \\rho/\\rho_0$），或者等效地，当 $\\alpha_v$ 和 $\\alpha_{\\rho}$ 被赋予补偿性的物理单位以使得 $\\alpha_v g_v$ 和 $\\alpha_{\\rho} g_{\\rho}$ 的单位分别为 $[\\mathrm{m}/\\mathrm{s}]$ 和 $[\\mathrm{kg}/\\mathrm{m}^3]$ 时，逐参数自适应性才能保留无量纲的梯度步长。在原始变量上使用无单位的 $\\alpha_v$ 和 $\\alpha_{\\rho}$ 草率地应用自适应性，并不能保证更新是无量纲、尺度不变的。\n\nB. 因为失配函数 $J(m)$ 是无量纲的，所以梯度 $g_v$ 和 $g_{\\rho}$ 也是无量纲的，因此任何对无量纲逐参数自适应因子的选择都会自动保留正确的无量纲梯度步长。\n\nC. 根据梯度的定义，梯度 $g_v$ 的单位是 $[\\mathrm{m}/\\mathrm{s}]$，$g_{\\rho}$ 的单位是 $[\\mathrm{kg}/\\mathrm{m}^3]$，因此将单个无量纲步长应用于两个参数总是能保留物理尺度。\n\nD. 在空间 $L^2(\\Omega)$ 内积下，$g_v$ 的单位是 $[\\mathrm{s}/\\mathrm{m}^4]$，$g_{\\rho}$ 的单位是 $[1/\\mathrm{kg}]$，但是选择自适应步长 $\\alpha_v \\propto 1/\\|g_v\\|$ 和 $\\alpha_{\\rho} \\propto 1/\\|g_{\\rho}\\|$ 会自动消除单位，使得更新无论参数化如何都变为无单位且不变的。", "solution": "用户想要对一个关于波形反演中基于梯度的优化中的量纲分析问题进行严格的验证和求解。\n\n### 第一步：提取已知条件\n-   模型参数：地震波速 $v(\\mathbf{x})$，单位为 $[\\mathrm{m}/\\mathrm{s}]$；质量密度 $\\rho(\\mathbf{x})$，单位为 $[\\mathrm{kg}/\\mathrm{m}^3]$。\n-   域：三维域 $\\Omega \\subset \\mathbb{R}^3$。\n-   体积元：$d\\mathbf{x}$，单位为 $[\\mathrm{m}^3]$。\n-   预测数据：$p(t,\\mathbf{x}_r;m)$，单位为帕斯卡 $[\\mathrm{Pa}]$。\n-   观测数据：$d(t,\\mathbf{x}_r)$，单位为帕斯卡 $[\\mathrm{Pa}]$。\n-   数据残差：$r(t,\\mathbf{x}_r;m) = p(t,\\mathbf{x}_r;m) - d(t,\\mathbf{x}_r)$。\n-   失配泛函：$J(m) = \\frac{1}{2}\\sum_{r}\\int_{0}^{T} \\left( W_d\\, r(t,\\mathbf{x}_r;m) \\right)^2 \\, dt$。\n-   数据加权算子 $W_d$：定义为使得 $W_d\\, r$ 无量纲。\n-   失配泛函 $J(m)$ 是无量纲的。\n-   失配泛函的变分：$\\delta J = \\int_{\\Omega} g_v(\\mathbf{x}) \\, \\delta v(\\mathbf{x}) \\, d\\mathbf{x} + \\int_{\\Omega} g_{\\rho}(\\mathbf{x}) \\, \\delta \\rho(\\mathbf{x}) \\, d\\mathbf{x}$。\n-   变分 $\\delta J$ 是无量纲的。\n-   梯度定义：梯度 $(g_v, g_{\\rho})$ 是在标准 $L^2(\\Omega)$ 内积下的 Gateaux 导数的 Riesz 表示。\n-   基于梯度的更新：$v_{k+1}(\\mathbf{x}) = v_k(\\mathbf{x}) - \\alpha_v \\, g_v(\\mathbf{x})$ 和 $\\rho_{k+1}(\\mathbf{x}) = \\rho_k(\\mathbf{x}) - \\alpha_{\\rho} \\, g_{\\rho}(\\mathbf{x})$。\n-   步长：$\\alpha_v$ 和 $\\alpha_{\\rho}$ 是标量步长。\n\n### 第二步：使用提取的已知条件进行验证\n问题陈述描述了一个多参数反问题的标准设置，特别是在地球物理学中的全波形反演。失配泛函、其变分（Gateaux 导数）以及基于梯度的更新规则的定义都是规范的。所提出的问题是基于这些定义的一个严格的量纲分析练习。\n\n-   **科学上成立：** 该问题坚实地建立在变分法和优化的既定数学和物理原理之上，并应用于基于波的反演。该设置是真实世界问题的一个简化但概念上准确的表示。\n-   **良态的：** 该问题提供了足够的信息来确定所涉及量的单位，并分析更新规则的量纲一致性。从给定的方程可以推导出一个唯一的结论。\n-   **客观性：** 该问题以精确、技术性的语言陈述，没有主观性或歧义。\n-   **完整性和一致性：** 分析所需的所有定义都已提供。在术语上存在轻微的张力：$\\delta J$的显式公式（它将 $g_v$ 和 $g_\\rho$ 定义为积分核）与它们是在“标准 $L^2$ 内积”下的 Riesz 表示的陈述之间（对于具有不同单位的多参数空间，这在量纲上是有问题的）。然而，严格的分析必须从给定的显式数学表达式出发。$g_v$ 和 $g_\\rho$ 这两个量的主要定义是它们在 $\\delta J$ 积分表达式中的作用。这正是它们在实践中通过伴随状态法推导出来的方式。因此，严格遵守所提供的方程，该问题是可解的。\n\n### 第三步：判断与行动\n该问题是有效的。关于内积定义的内部张力可以通过优先使用 $\\delta J$ 的显式公式作为梯度核 $g_v$ 和 $g_{\\rho}$ 的定义来解决，这也反映了计算实践。我将继续进行详细的求解。\n\n### 推导\n\n该问题需要一个基于量纲一致性的三部分分析。\n\n**第一部分：确定 $g_v$ 和 $g_{\\rho}$ 的物理单位。**\n\n分析从失配泛函的变分 $\\delta J$ 的定义开始：\n$$\n\\delta J = \\int_{\\Omega} g_v(\\mathbf{x}) \\, \\delta v(\\mathbf{x}) \\, d\\mathbf{x} + \\int_{\\Omega} g_{\\rho}(\\mathbf{x}) \\, \\delta \\rho(\\mathbf{x}) \\, d\\mathbf{x}\n$$\n我们已知 $J(m)$ 是无量纲的，因此其变分 $\\delta J$ 也是无量纲的。让一个量 $Q$ 的单位表示为 $[Q]$。因此，$[\\delta J] = 1$。\n\n为了使右侧的和是无量纲的，每个积分项也必须是无量纲的（假设它们没有被定义为具有相等但相反的非无量纲单位，这在物理上是不合理的）。让我们分析第一项：\n$$\n\\left[ \\int_{\\Omega} g_v(\\mathbf{x}) \\, \\delta v(\\mathbf{x}) \\, d\\mathbf{x} \\right] = 1\n$$\n积分的单位是被积函数单位与测度单位的乘积。\n$$\n[g_v] \\cdot [\\delta v] \\cdot [d\\mathbf{x}] = 1\n$$\n从问题陈述中，我们有速度扰动的单位 $[\\delta v] = [v] = [\\mathrm{m}/\\mathrm{s}]$ 和体积元的单位 $[d\\mathbf{x}] = [\\mathrm{m}^3]$。代入这些单位得到：\n$$\n[g_v] \\cdot \\left[\\frac{\\mathrm{m}}{\\mathrm{s}}\\right] \\cdot [\\mathrm{m}^3] = 1\n$$\n$$\n[g_v] \\cdot \\left[\\frac{\\mathrm{m}^4}{\\mathrm{s}}\\right] = 1\n$$\n解出 $g_v$ 的单位：\n$$\n[g_v] = \\frac{1}{[\\mathrm{m}^4/\\mathrm{s}]} = \\left[\\frac{\\mathrm{s}}{\\mathrm{m}^4}\\right]\n$$\n同样，对于涉及密度的第二项：\n$$\n\\left[ \\int_{\\Omega} g_{\\rho}(\\mathbf{x}) \\, \\delta \\rho(\\mathbf{x}) \\, d\\mathbf{x} \\right] = 1\n$$\n$$\n[g_{\\rho}] \\cdot [\\delta \\rho] \\cdot [d\\mathbf{x}] = 1\n$$\n使用密度扰动的单位 $[\\delta \\rho] = [\\rho] = [\\mathrm{kg}/\\mathrm{m}^3]$ 和体积元的单位 $[d\\mathbf{x}] = [\\mathrm{m}^3]$：\n$$\n[g_{\\rho}] \\cdot \\left[\\frac{\\mathrm{kg}}{\\mathrm{m}^3}\\right] \\cdot [\\mathrm{m}^3] = 1\n$$\n$$\n[g_{\\rho}] \\cdot [\\mathrm{kg}] = 1\n$$\n解出 $g_{\\rho}$ 的单位：\n$$\n[g_{\\rho}] = \\frac{1}{[\\mathrm{kg}]} = [\\mathrm{kg}^{-1}]\n$$\n\n**第二部分：推断 $\\alpha_v$ 和 $\\alpha_{\\rho}$ 所需的单位。**\n\n基于梯度的更新由以下公式给出：\n$$\nv_{k+1}(\\mathbf{x}) = v_k(\\mathbf{x}) - \\alpha_v \\, g_v(\\mathbf{x})\n$$\n$$\n\\rho_{k+1}(\\mathbf{x}) = \\rho_k(\\mathbf{x}) - \\alpha_{\\rho} \\, g_{\\rho}(\\mathbf{x})\n$$\n为了使这些方程在物理上保持一致，每个方程中的所有项必须具有相同的单位（量纲齐次性原则）。对于速度更新：\n$$\n[v_{k+1}] = [v_k] = [\\alpha_v \\, g_v(\\mathbf{x})]\n$$\n我们知道 $[v_k] = [\\mathrm{m}/\\mathrm{s}]$，并且我们求得 $[g_v] = [\\mathrm{s}/\\mathrm{m}^4]$。因此：\n$$\n\\left[\\frac{\\mathrm{m}}{\\mathrm{s}}\\right] = [\\alpha_v] \\cdot \\left[\\frac{\\mathrm{s}}{\\mathrm{m}^4}\\right]\n$$\n解出 $\\alpha_v$ 的单位：\n$$\n[\\alpha_v] = \\frac{[\\mathrm{m}/\\mathrm{s}]}{[\\mathrm{s}/\\mathrm{m}^4]} = \\left[\\frac{\\mathrm{m}^5}{\\mathrm{s}^2}\\right]\n$$\n对于密度更新：\n$$\n[\\rho_{k+1}] = [\\rho_k] = [\\alpha_{\\rho} \\, g_{\\rho}(\\mathbf{x})]\n$$\n我们知道 $[\\rho_k] = [\\mathrm{kg}/\\mathrm{m}^3]$，并且我们求得 $[g_{\\rho}] = [\\mathrm{kg}^{-1}]$。因此：\n$$\n\\left[\\frac{\\mathrm{kg}}{\\mathrm{m}^3}\\right] = [\\alpha_{\\rho}] \\cdot [\\mathrm{kg}^{-1}]\n$$\n解出 $\\alpha_{\\rho}$ 的单位：\n$$\n[\\alpha_{\\rho}] = \\frac{[\\mathrm{kg}/\\mathrm{m}^3]}{[\\mathrm{kg}^{-1}]} = \\left[\\frac{\\mathrm{kg}^2}{\\mathrm{m}^3}\\right]\n$$\n这表明步长 $\\alpha_v$ 和 $\\alpha_{\\rho}$ 必须是有量纲的量，并且它们具有不同的物理单位。\n\n**第三部分：关于逐参数自适应性的论证。**\n\n逐参数自适应性，如在 Adam 或 RMSProp 等方法中实现的，通常根据每个参数的梯度大小历史来计算其自适应缩放因子。这些因子通常被视为调节全局学习率的无量纲标量。\n\n上述分析表明，如果使用原始物理参数（$v, \\rho$）进行计算，更新规则中的“步长”$\\alpha_v$ 和 $\\alpha_\\rho$ 不是简单的标量，而必须携带特定的、不同的物理量纲，以确保更新量 $\\delta v = -\\alpha_v g_v$ 和 $\\delta \\rho = -\\alpha_\\rho g_\\rho$ 分别具有正确的速度和密度单位。一个生成无量纲自适应因子 $\\alpha_v$ 和 $\\alpha_\\rho$ 的自适应算法的草率应用将导致量纲不一致的更新。比较 $g_v$ 和 $g_{\\rho}$ 的大小（例如，$\\|g_v\\|$ 与 $\\|g_{\\rho}\\|$）也是没有物理意义的，因为它们具有不相关的单位。这样的算法将不是尺度不变的；改变基本单位（例如，从米到公里）会改变优化的物理结果。\n\n为了正确实现自适应性，必须确保量纲一致性和尺度不变性。有两种主要方法可以实现这一点：\n\n1.  **无量纲化：** 定义无量纲的模型参数，例如 $\\tilde{v}(\\mathbf{x}) = v(\\mathbf{x})/v_0$ 和 $\\tilde{\\rho}(\\mathbf{x}) = \\rho(\\mathbf{x})/\\rho_0$，其中 $v_0$ 和 $\\rho_0$ 是特征参考值。然后在 $\\tilde{v}$ 和 $\\tilde{\\rho}$ 上执行优化。关于这些无量纲参数的梯度将具有可直接比较的单位，并且可以有意义地应用自适应的无量纲步长。在模型空间中产生的步长 $(\\delta \\tilde{v}, \\delta \\tilde{\\rho})$ 是无量纲的。\n\n2.  **量纲预处理：** 继续使用原始参数 $(v, \\rho)$，但要认识到 $\\alpha_v$ 和 $\\alpha_\\rho$ 不是步长，而是一个预处理算子的体现。也就是说，更新是 $\\delta m = -\\alpha P g$，其中 $P$ 是一个预处理矩阵/算子，$\\alpha$ 是一个无量纲的步长。问题中的“步长”对应于 $P$ 的条目：$\\alpha_v = \\alpha P_v$ 和 $\\alpha_\\rho = \\alpha P_\\rho$。这里，$P_v$ 和 $P_\\rho$ 必须携带适当的单位（分别为 $[\\mathrm{m}^5/\\mathrm{s}^2]$ 和 $[\\mathrm{kg}^2/\\mathrm{m}^3]$）以使总更新在量纲上正确。在这种情况下，自适应性意味着设计适当的有量纲的预处理器，这是一个比简单应用无量纲因子更高级的概念。\n\n两种方法都得出相同的结论：在原始物理参数上草率地应用带有无量纲步长的逐参数自适应性是错误的。更新要么必须在无量纲变量上执行，要么自适应的“步长”必须被正确地构造为有量纲的算子。\n\n### 逐选项分析\n\n**A. 在一个经过适当无量纲化的失配函数中，只有当更新在无量纲变量上进行时（例如，使用参考尺度 $v_0$ 和 $\\rho_0$ 的 $\\tilde v = v/v_0$ 和 $\\tilde \\rho = \\rho/\\rho_0$），或者等效地，当 $\\alpha_v$ 和 $\\alpha_{\\rho}$ 被赋予补偿性的物理单位以使得 $\\alpha_v g_v$ 和 $\\alpha_{\\rho} g_{\\rho}$ 的单位分别为 $[\\mathrm{m}/\\mathrm{s}]$ 和 $[\\mathrm{kg}/\\mathrm{m}^3]$ 时，逐参数自适应性才能保留无量纲的梯度步长。在原始变量上使用无单位的 $\\alpha_v$ 和 $\\alpha_{\\rho}$ 草率地应用自适应性，并不能保证更新是无量纲、尺度不变的。**\n此选项正确地描述了两种有效的方法：无量纲化或使用有量纲的步长（预处理器）。它正确地指出了有量纲步长的目的是确保模型更新 $\\alpha_v g_v$ 和 $\\alpha_\\rho g_\\rho$ 具有正确的物理单位。它也正确地陈述了草率应用方法是有缺陷的。\n**结论：正确。**\n\n**B. 因为失配函数 $J(m)$ 是无量纲的，所以梯度 $g_v$ 和 $g_{\\rho}$ 也是无量纲的，因此任何对无量纲逐参数自适应因子的选择都会自动保留正确的无量纲梯度步长。**\n前提是错误的。如上所述， $J(m)$ 的无量纲性质要求梯度核 $g_v$ 和 $g_{\\rho}$ 必须是有量纲的，以补偿定义积分中模型参数和体积元的量纲。由于前提是错误的，结论是无根据的。\n**结论：不正确。**\n\n**C. 根据梯度的定义，梯度 $g_v$ 的单位是 $[\\mathrm{m}/\\mathrm{s}]$，$g_{\\rho}$ 的单位是 $[\\mathrm{kg}/\\mathrm{m}^3]$，因此将单个无量纲步长应用于两个参数总是能保留物理尺度。**\n此选项错误地假设梯度核 $g_v, g_\\rho$（由 $\\delta J$ 的积分定义）必须与模型参数具有相同的单位。这仅在梯度是相对于特定的、物理加权的内积定义时才成立，而所提供的 $\\delta J$ 公式并非如此。根据给定的方程，单位是 $[g_v] = [\\mathrm{s}/\\mathrm{m}^4]$ 和 $[g_\\rho] = [\\mathrm{kg}^{-1}]$。因此，前提是不正确的。\n**结论：不正确。**\n\n**D. 在空间 $L^2(\\Omega)$ 内积下，$g_v$ 的单位是 $[\\mathrm{s}/\\mathrm{m}^4]$，$g_{\\rho}$ 的单位是 $[1/\\mathrm{kg}]$，但是选择自适应步长 $\\alpha_v \\propto 1/\\|g_v\\|$ 和 $\\alpha_{\\rho} \\propto 1/\\|g_{\\rho}\\|$ 会自动消除单位，使得更新无论参数化如何都变为无单位且不变的。**\n该陈述的第一部分正确地指出了从问题的 $\\delta J$ 定义中推导出的 $g_v$ 和 $g_\\rho$ 的单位。然而，第二部分是不正确的。与 $1/\\|g_v\\|$ 成比例的步长并不会“自动消除单位”以使更新在量纲上正确。如思考过程中所示，像 $\\alpha_v \\propto 1/\\|g_v\\|_{L^2}$ 这样的选择会导致更新 $\\delta v = \\alpha_v g_v$ 具有不正确的物理单位。更新并不是变得“无单位”；目标是使更新量 $\\delta v$ 与 $v$ 具有相同的单位。此选项曲解了对自适应步长进行量纲分析的结果。\n**结论：不正确。**", "answer": "$$\\boxed{A}$$", "id": "3601067"}]}