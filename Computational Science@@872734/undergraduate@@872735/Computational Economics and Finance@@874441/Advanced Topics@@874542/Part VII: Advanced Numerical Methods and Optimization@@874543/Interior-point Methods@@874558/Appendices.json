{"hands_on_practices": [{"introduction": "Interior-Point Methods iteratively refine a solution by taking a series of Newton steps toward the optimum of a barrier-penalized problem. This exercise will give you first-hand experience with the core calculation of a single IPM iteration by having you compute the Newton step and the associated Newton decrement, a crucial measure of progress. By working through a portfolio optimization problem with an entropy objective, you will solidify your understanding of the mechanics behind the central path [@problem_id:2402656].", "problem": "Consider a portfolio allocation vector $w \\in \\mathbb{R}^3_{++}$ over $n=3$ assets with expected returns vector $r \\in \\mathbb{R}^3$. The objective is to maximize the Shannon entropy of the portfolio, given by $-\\sum_{i=1}^3 w_i \\ln(w_i)$, subject to a budget constraint and a target expected return constraint. Equivalently, this can be posed as the minimization problem\n$\\min_{w \\in \\mathbb{R}^3_{++}} \\sum_{i=1}^3 w_i \\ln(w_i)$\nsubject to the linear equality constraints $\\boldsymbol{1}^{\\top}w = 1$ and $r^{\\top}w = \\mu$, where $\\ln(\\cdot)$ denotes the natural logarithm. Consider the Interior-Point Method (IPM) with a barrier scaling parameter $t0$ applied to the equality-constrained minimization of $t \\sum_{i=1}^3 w_i \\ln(w_i)$.\n\nAt the current iterate, you are given:\n- $r = \\begin{pmatrix}0.02 \\\\ 0.01 \\\\ 0.03\\end{pmatrix}$,\n- $\\mu = 0.02$,\n- $w^{(0)} = \\begin{pmatrix}0.4 \\\\ 0.4 \\\\ 0.2\\end{pmatrix}$,\n- $t = 10$,\n- the current dual multipliers for the constraints are $\\nu^{(0)} = \\begin{pmatrix}0 \\\\ 0\\end{pmatrix}$.\n\nUsing first principles from constrained optimization and the definition of the equality-constrained Newton step for the $t$-scaled problem (that is, by forming the Lagrangian with the linear constraints, writing down the first-order optimality conditions, and linearizing them at the given iterate), compute the Newton decrement squared\n$\\lambda^2 = \\Delta w^{\\top} \\nabla^2\\!\\big(t \\sum_{i=1}^3 w_i \\ln(w_i)\\big)\\, \\Delta w$\nat the given iterate $(w^{(0)}, \\nu^{(0)})$, where $\\Delta w$ is the equality-constrained Newton step direction for the $t$-scaled problem at $(w^{(0)}, \\nu^{(0)})$. Round your answer to four significant figures.", "solution": "The problem asks us to compute the Newton decrement squared, $\\lambda^2$, for an equality-constrained optimization problem. The objective function is $f_t(w) = t \\sum_{i=1}^3 w_i \\ln(w_i)$ with $t=10$. The constraints are linear, $Aw=b$.\n\nFirst, we define the components.\nThe objective function:\n$$ f_t(w) = 10 \\sum_{i=1}^3 w_i \\ln(w_i) $$\nThe constraints are $\\boldsymbol{1}^{\\top}w = 1$ and $\\boldsymbol{r}^{\\top}w = \\mu$. We define the constraint matrix $A$ and vector $b$:\n$$ A = \\begin{pmatrix} \\boldsymbol{1}^\\top \\\\ \\boldsymbol{r}^\\top \\end{pmatrix} = \\begin{pmatrix} 1  1  1 \\\\ 0.02  0.01  0.03 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 1 \\\\ 0.02 \\end{pmatrix} $$\nThe Newton step $(\\Delta w, \\Delta\\nu)$ is found by solving the linearized KKT conditions at the current iterate $(w^{(0)}, \\nu^{(0)})$. Since $w^{(0)}$ is not feasible, the system is:\n$$ \\begin{pmatrix} \\nabla^2 f_t(w^{(0)})  A^\\top \\\\ A  0 \\end{pmatrix} \\begin{pmatrix} \\Delta w \\\\ \\Delta\\nu \\end{pmatrix} = \\begin{pmatrix} -\\nabla f_t(w^{(0)}) - A^\\top\\nu^{(0)} \\\\ b - A w^{(0)} \\end{pmatrix} $$\nGiven $\\nu^{(0)} = \\boldsymbol{0}$, this simplifies to:\n$$ \\begin{pmatrix} H  A^\\top \\\\ A  0 \\end{pmatrix} \\begin{pmatrix} \\Delta w \\\\ \\Delta\\nu \\end{pmatrix} = \\begin{pmatrix} -g \\\\ r_{\\text{prim}} \\end{pmatrix} $$\nwhere $H = \\nabla^2 f_t(w^{(0)})$, $g = \\nabla f_t(w^{(0)})$, and $r_{\\text{prim}} = b - A w^{(0)}$. We compute the components at the iterate $w^{(0)} = \\begin{pmatrix} 0.4 \\\\ 0.4 \\\\ 0.2 \\end{pmatrix}$.\n\nThe gradient of $f_t(w)$ is $\\nabla f_t(w) = t (\\boldsymbol{1} + \\ln(w))$, where $\\ln(w)$ is applied element-wise.\n$$ g = \\nabla f_t(w^{(0)}) = 10 \\begin{pmatrix} 1 + \\ln(0.4) \\\\ 1 + \\ln(0.4) \\\\ 1 + \\ln(0.2) \\end{pmatrix} \\approx \\begin{pmatrix} 10 \\times (1 - 0.91629) \\\\ 10 \\times (1 - 0.91629) \\\\ 10 \\times (1 - 1.60944) \\end{pmatrix} = \\begin{pmatrix} 0.8371 \\\\ 0.8371 \\\\ -6.0944 \\end{pmatrix} $$\nThe Hessian of $f_t(w)$ is a diagonal matrix $\\nabla^2 f_t(w) = t \\cdot \\text{diag}(1/w_1, \\dots, 1/w_n)$.\n$$ H = \\nabla^2 f_t(w^{(0)}) = 10 \\cdot \\text{diag}\\left(\\frac{1}{0.4}, \\frac{1}{0.4}, \\frac{1}{0.2}\\right) = \\text{diag}(25, 25, 50) $$\nThe inverse Hessian is $H^{-1} = \\text{diag}(1/25, 1/25, 1/50) = \\text{diag}(0.04, 0.04, 0.02)$.\n\nThe constraint residual is:\n$$ r_{\\text{prim}} = b - A w^{(0)} = \\begin{pmatrix} 1 \\\\ 0.02 \\end{pmatrix} - \\begin{pmatrix} 1  1  1 \\\\ 0.02  0.01  0.03 \\end{pmatrix} \\begin{pmatrix} 0.4 \\\\ 0.4 \\\\ 0.2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0.02 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0.018 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0.002 \\end{pmatrix} $$\nFrom the KKT system, we can first solve for $\\Delta\\nu$.\n$$ A \\Delta w = r_{\\text{prim}} $$\n$$ H \\Delta w + A^\\top \\Delta\\nu = -g \\implies \\Delta w = -H^{-1}(g + A^\\top \\Delta\\nu) $$\nSubstituting $\\Delta w$ into the first equation gives:\n$$ A(-H^{-1}(g + A^\\top \\Delta\\nu)) = r_{\\text{prim}} \\implies (A H^{-1} A^\\top) \\Delta\\nu = -A H^{-1} g - r_{\\text{prim}} $$\nWe compute the matrix $A H^{-1} A^\\top$:\n$$ A H^{-1} A^\\top = \\begin{pmatrix} 1  1  1 \\\\ 0.02  0.01  0.03 \\end{pmatrix} \\begin{pmatrix} 0.04  0  0 \\\\ 0  0.04  0 \\\\ 0  0  0.02 \\end{pmatrix} \\begin{pmatrix} 1  0.02 \\\\ 1  0.01 \\\\ 1  0.03 \\end{pmatrix} = \\begin{pmatrix} 0.1  0.0018 \\\\ 0.0018  0.000038 \\end{pmatrix} $$\nAnd the vector $A H^{-1} g$:\n$$ A H^{-1} g = \\begin{pmatrix} 0.04  0.04  0.02 \\\\ 0.0008  0.0004  0.0006 \\end{pmatrix} \\begin{pmatrix} 0.8371 \\\\ 0.8371 \\\\ -6.0944 \\end{pmatrix} \\approx \\begin{pmatrix} -0.05492 \\\\ -0.002652 \\end{pmatrix} $$\nThe system for $\\Delta\\nu$ is:\n$$ \\begin{pmatrix} 0.1  0.0018 \\\\ 0.0018  0.000038 \\end{pmatrix} \\Delta\\nu = -\\begin{pmatrix} -0.05492 \\\\ -0.002652 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 0.002 \\end{pmatrix} = \\begin{pmatrix} 0.05492 \\\\ 0.000652 \\end{pmatrix} $$\nSolving this $2 \\times 2$ system yields $\\Delta\\nu \\approx \\begin{pmatrix} 1.6304 \\\\ -60.066 \\end{pmatrix}$.\n\nNow, we compute the Newton step $\\Delta w$:\n$$ \\Delta w = -H^{-1}(g + A^\\top \\Delta\\nu) $$\n$$ A^\\top \\Delta\\nu \\approx \\begin{pmatrix} 1  0.02 \\\\ 1  0.01 \\\\ 1  0.03 \\end{pmatrix} \\begin{pmatrix} 1.6304 \\\\ -60.066 \\end{pmatrix} = \\begin{pmatrix} 1.6304 - 1.20132 \\\\ 1.6304 - 0.60066 \\\\ 1.6304 - 1.80198 \\end{pmatrix} = \\begin{pmatrix} 0.42908 \\\\ 1.02974 \\\\ -0.17158 \\end{pmatrix} $$\n$$ g + A^\\top \\Delta\\nu \\approx \\begin{pmatrix} 0.8371 \\\\ 0.8371 \\\\ -6.0944 \\end{pmatrix} + \\begin{pmatrix} 0.42908 \\\\ 1.02974 \\\\ -0.17158 \\end{pmatrix} = \\begin{pmatrix} 1.26618 \\\\ 1.86684 \\\\ -6.26598 \\end{pmatrix} $$\n$$ \\Delta w = -\\begin{pmatrix} 0.04 \\\\ 0.04 \\\\ 0.02 \\end{pmatrix} \\odot \\begin{pmatrix} 1.26618 \\\\ 1.86684 \\\\ -6.26598 \\end{pmatrix} = \\begin{pmatrix} -0.0506472 \\\\ -0.0746736 \\\\ 0.1253196 \\end{pmatrix} $$\nFinally, we compute the Newton decrement squared, $\\lambda^2$, using its definition:\n$$ \\lambda^2 = \\Delta w^{\\top} H \\Delta w $$\n$$ \\lambda^2 \\approx \\begin{pmatrix} -0.0506472  -0.0746736  0.1253196 \\end{pmatrix} \\begin{pmatrix} 25  0  0 \\\\ 0  25  0 \\\\ 0  0  50 \\end{pmatrix} \\begin{pmatrix} -0.0506472 \\\\ -0.0746736 \\\\ 0.1253196 \\end{pmatrix} $$\n$$ \\lambda^2 \\approx 25(-0.0506472)^2 + 25(-0.0746736)^2 + 50(0.1253196)^2 $$\n$$ \\lambda^2 \\approx 25(0.00256514) + 25(0.00557615) + 50(0.0157050) $$\n$$ \\lambda^2 \\approx 0.0641285 + 0.13940375 + 0.78525 $$\n$$ \\lambda^2 \\approx 0.98878225 $$\nRounding the result to four significant figures gives $0.9888$.", "answer": "$$\\boxed{0.9888}$$", "id": "2402656"}, {"introduction": "A powerful feature of many optimization algorithms, including IPMs, is their ability to determine when a problem has no solution. Instead of simply failing, they can produce a mathematical proof, known as a certificate of infeasibility. This practice demonstrates how to construct a Farkas certificate from the dual variables provided by an IPM that has declared a portfolio selection problem infeasible, connecting the algorithm's output to the fundamental theory of linear inequalities [@problem_id:2402685].", "problem": "Consider the long-only portfolio feasibility problem: find $\\boldsymbol{x} \\in \\mathbb{R}^{3}$ such that $\\boldsymbol{x} \\succeq \\mathbf{0}$ and $A \\boldsymbol{x} = \\boldsymbol{b}$, where\n$$\nA \\;=\\; \\begin{pmatrix}\n1  1  1 \\\\\n0.10  0.20  0.15\n\\end{pmatrix},\n\\qquad\n\\boldsymbol{b} \\;=\\; \\begin{pmatrix}\n1 \\\\\n0.22\n\\end{pmatrix}.\n$$\nThe first row $1 \\cdot x_{1} + 1 \\cdot x_{2} + 1 \\cdot x_{3} = 1$ is the budget constraint, and the second row $0.10 x_{1} + 0.20 x_{2} + 0.15 x_{3} = 0.22$ is a target expected return requirement. An interior-point method applied to the homogeneous self-dual embedding of this feasibility problem terminates declaring infeasibility and returns a final dual iterate approximately\n$$\n\\tilde{\\boldsymbol{y}} \\;=\\; \\begin{pmatrix} 0.1997 \\\\ -1.0030 \\end{pmatrix}.\n$$\nUsing the sign information in $\\tilde{\\boldsymbol{y}}$ to fix a normalization, construct a Farkas certificate of infeasibility $\\boldsymbol{y} \\in \\mathbb{R}^{2}$ satisfying $A^{\\top} \\boldsymbol{y} \\succeq \\mathbf{0}$ and $\\boldsymbol{b}^{\\top} \\boldsymbol{y}  0$, with the normalization $y_{2} = -1$ and with $y_{1}$ chosen as small as possible subject to these inequalities. Compute the scalar $\\boldsymbol{b}^{\\top} \\boldsymbol{y}$. No rounding is required; provide an exact value.", "solution": "The problem requires the construction of a Farkas certificate of infeasibility for a given linear system. The feasibility problem is to find a vector $\\boldsymbol{x} \\in \\mathbb{R}^{3}$ such that $A \\boldsymbol{x} = \\boldsymbol{b}$ and $\\boldsymbol{x} \\succeq \\mathbf{0}$. The provided matrices are\n$$\nA \\;=\\; \\begin{pmatrix}\n1  1  1 \\\\\n0.10  0.20  0.15\n\\end{pmatrix},\n\\qquad\n\\boldsymbol{b} \\;=\\; \\begin{pmatrix}\n1 \\\\\n0.22\n\\end{pmatrix}.\n$$\nAccording to Farkas's Lemma (a theorem of the alternative), the system $A \\boldsymbol{x} = \\boldsymbol{b}, \\boldsymbol{x} \\succeq \\mathbf{0}$ is infeasible if and only if there exists a vector $\\boldsymbol{y} \\in \\mathbb{R}^{2}$ satisfying the two conditions:\n1. $A^{\\top} \\boldsymbol{y} \\succeq \\mathbf{0}$\n2. $\\boldsymbol{b}^{\\top} \\boldsymbol{y}  0$\nSuch a vector $\\boldsymbol{y}$ is called a Farkas certificate of infeasibility. The problem instructs us to find such a certificate.\n\nFirst, we write the transpose of matrix $A$:\n$$\nA^{\\top} \\;=\\; \\begin{pmatrix}\n1  0.10 \\\\\n1  0.20 \\\\\n1  0.15\n\\end{pmatrix}\n$$\nLet the certificate vector be $\\boldsymbol{y} = \\begin{pmatrix} y_{1} \\\\ y_{2} \\end{pmatrix}$. The condition $A^{\\top} \\boldsymbol{y} \\succeq \\mathbf{0}$ expands into a system of three linear inequalities:\n$$\n\\begin{pmatrix}\n1  0.10 \\\\\n1  0.20 \\\\\n1  0.15\n\\end{pmatrix}\n\\begin{pmatrix} y_{1} \\\\ y_{2} \\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\ny_{1} + 0.10 y_{2} \\\\\ny_{1} + 0.20 y_{2} \\\\\ny_{1} + 0.15 y_{2}\n\\end{pmatrix}\n\\;\\succeq\\;\n\\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nThis gives us:\n(i) $y_{1} + 0.10 y_{2} \\geq 0$\n(ii) $y_{1} + 0.20 y_{2} \\geq 0$\n(iii) $y_{1} + 0.15 y_{2} \\geq 0$\n\nThe second condition, $\\boldsymbol{b}^{\\top} \\boldsymbol{y}  0$, becomes:\n$$\n\\boldsymbol{b}^{\\top} \\boldsymbol{y} \\;=\\; \\begin{pmatrix} 1  0.22 \\end{pmatrix} \\begin{pmatrix} y_{1} \\\\ y_{2} \\end{pmatrix} \\;=\\; y_{1} + 0.22 y_{2}  0\n$$\n\nThe problem specifies a normalization for the certificate, $y_{2} = -1$. This normalization is consistent with the sign structure of the approximate dual iterate $\\tilde{\\boldsymbol{y}}$ provided. Substituting $y_{2} = -1$ into our system of inequalities:\n(i) $y_{1} + 0.10(-1) \\geq 0 \\implies y_{1} \\geq 0.10$\n(ii) $y_{1} + 0.20(-1) \\geq 0 \\implies y_{1} \\geq 0.20$\n(iii) $y_{1} + 0.15(-1) \\geq 0 \\implies y_{1} \\geq 0.15$\n(iv) $y_{1} + 0.22(-1)  0 \\implies y_{1}  0.22$\n\nTo satisfy inequalities (i), (ii), and (iii) simultaneously, $y_{1}$ must be greater than or equal to the maximum of the lower bounds:\n$$\ny_{1} \\geq \\max(0.10, 0.20, 0.15) = 0.20\n$$\nCombining this with inequality (iv), we obtain the full condition on $y_{1}$:\n$$\n0.20 \\leq y_{1}  0.22\n$$\nThe problem demands that we choose $y_{1}$ to be as small as possible. The set of valid values for $y_{1}$ is the interval $[0.20, 0.22)$. The minimum value in this set is its lower bound, $y_{1} = 0.20$.\n\nWith this selection, our Farkas certificate is:\n$$\n\\boldsymbol{y} = \\begin{pmatrix} y_{1} \\\\ y_{2} \\end{pmatrix} = \\begin{pmatrix} 0.20 \\\\ -1 \\end{pmatrix}\n$$\nWe must verify that this vector satisfies all conditions.\n$A^{\\top}\\boldsymbol{y} = \\begin{pmatrix} 0.20 + 0.10(-1) \\\\ 0.20 + 0.20(-1) \\\\ 0.20 + 0.15(-1) \\end{pmatrix} = \\begin{pmatrix} 0.10 \\\\ 0 \\\\ 0.05 \\end{pmatrix}$, which is component-wise greater than or equal to $\\mathbf{0}$.\n$\\boldsymbol{b}^{\\top}\\boldsymbol{y} = 0.20 + 0.22(-1) = 0.20 - 0.22 = -0.02$, which is strictly less than $0$.\nBoth conditions are satisfied.\n\nThe final task is to compute the scalar $\\boldsymbol{b}^{\\top} \\boldsymbol{y}$. We have already calculated this value.\n$$\n\\boldsymbol{b}^{\\top} \\boldsymbol{y} = 0.20 - 0.22 = -0.02\n$$\nThe value $-0.02$ is exact.", "answer": "$$\\boxed{-0.02}$$", "id": "2402685"}, {"introduction": "Moving from theory to practice, this final exercise challenges you to implement a complete primal-dual interior-point solver for a classic quadratic portfolio optimization problem. You will learn a critical technique for large-scale optimization: solving the internal linear systems using an iterative method (Conjugate Gradient) that avoids explicit matrix inversion, relying only on matrix-vector products. This hands-on coding problem bridges the gap between textbook algorithms and efficient, practical software [@problem_id:2402731].", "problem": "You are asked to write a complete, runnable program that computes optimal long-only portfolio weights for a set of quadratic programs defined by mean-variance objectives with a single full-investment constraint. The program must solve each quadratic program by only using products of the Hessian matrix with vectors and must not compute or use any explicit matrix inverse of the Hessian.\n\nThe optimization problem for a given test case is: minimize the objective\n$$\n\\frac{1}{2}\\,\\boldsymbol{x}^{\\top} Q\\,\\boldsymbol{x} - \\boldsymbol{\\mu}^{\\top}\\boldsymbol{x}\n$$\nsubject to the constraints\n$$\n\\boldsymbol{1}^{\\top}\\boldsymbol{x} = 1,\\quad \\boldsymbol{x} \\succeq \\boldsymbol{0},\n$$\nwhere $Q \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite, $\\boldsymbol{\\mu} \\in \\mathbb{R}^{n}$ is the expected return vector, $\\boldsymbol{1}$ is the all-ones vector in $\\mathbb{R}^{n}$, and $\\boldsymbol{0}$ is the zero vector in $\\mathbb{R}^{n}$. The decision variable is $\\boldsymbol{x} \\in \\mathbb{R}^{n}$. All computations are to be done in purely numerical terms; no physical units apply.\n\nYour program must produce, for each test case, the optimal weight vector $\\boldsymbol{x}^{\\star}$ as a list of real numbers, each rounded to six decimal places. The constraint $\\boldsymbol{x} \\succeq \\boldsymbol{0}$ enforces long-only positions. The full-investment constraint is $\\boldsymbol{1}^{\\top}\\boldsymbol{x} = 1$.\n\nThe test suite consists of the following three cases. In each case, $A = \\boldsymbol{1}^{\\top}$ and $b = 1$ define the single equality constraint. All numeric entries below are exact.\n\n- Test case $1$ ($n=4$):\n  - Hessian\n    $$\n    Q_1 =\n    \\begin{bmatrix}\n    0.10  0.02  0.04  0.00 \\\\\n    0.02  0.08  0.01  0.00 \\\\\n    0.04  0.01  0.09  0.00 \\\\\n    0.00  0.00  0.00  0.05\n    \\end{bmatrix}\n    $$\n  - Expected returns\n    $$\n    \\boldsymbol{\\mu}_1 = \\begin{bmatrix} 0.12 \\\\ 0.10 \\\\ 0.07 \\\\ 0.03 \\end{bmatrix}\n    $$\n\n- Test case $2$ ($n=3$):\n  - Hessian\n    $$\n    Q_2 =\n    \\begin{bmatrix}\n    0.20  0.15  0.15 \\\\\n    0.15  0.30  0.25 \\\\\n    0.15  0.25  0.50\n    \\end{bmatrix}\n    $$\n  - Expected returns\n    $$\n    \\boldsymbol{\\mu}_2 = \\begin{bmatrix} 0.18 \\\\ 0.02 \\\\ 0.01 \\end{bmatrix}\n    $$\n\n- Test case $3$ ($n=3$):\n  - Hessian\n    $$\n    Q_3 = 0.02 \\cdot\n    \\begin{bmatrix}\n    1.0  0.999  0.9995 \\\\\n    0.999  1.0  0.9992 \\\\\n    0.9995  0.9992  1.0\n    \\end{bmatrix}\n    +\n    10^{-6}\\, I_3\n    $$\n    where $I_3$ is the $3 \\times 3$ identity matrix.\n  - Expected returns\n    $$\n    \\boldsymbol{\\mu}_3 = \\begin{bmatrix} 0.050 \\\\ 0.051 \\\\ 0.049 \\end{bmatrix}\n    $$\n\nYour program must:\n- Enforce the constraints exactly within numerical tolerance.\n- Use only matrix-vector products involving $Q$; do not form or use any explicit inverse of $Q$ or of any derived matrix.\n- Return the optimizer $\\boldsymbol{x}^{\\star}$ for each test case, rounded to six decimal places.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a list of the rounded weights for the corresponding test case, for example,\n$$\n[ [x_{1,1},x_{1,2},\\dots], [x_{2,1},x_{2,2},\\dots], [x_{3,1},x_{3,2},\\dots] ].\n$$\nNo spaces are required, and each entry must be printed with exactly six digits after the decimal point.\n\nDesign for coverage:\n- The first case is a standard instance with moderate correlations.\n- The second case encourages boundary solutions where some optimal weights may be numerically zero.\n- The third case is nearly collinear and tests numerical stability.\n\nThe expected answer for each test case is the list of optimal weights as real numbers. Each list must consist of exactly $n$ floating-point numbers.", "solution": "The problem presented is a convex quadratic program (QP) concerning optimal portfolio allocation under a mean-variance framework. The objective is to minimize a function representing a trade-off between portfolio risk (variance) and return, subject to a full-investment constraint and a long-only (non-negativity) constraint on the portfolio weights $\\boldsymbol{x}$.\n\nThe optimization problem is formally stated as:\n$$\n\\begin{aligned}\n\\text{minimize} \\quad  f(\\boldsymbol{x}) = \\frac{1}{2}\\,\\boldsymbol{x}^{\\top} Q\\,\\boldsymbol{x} - \\boldsymbol{\\mu}^{\\top}\\boldsymbol{x} \\\\\n\\text{subject to} \\quad  \\boldsymbol{1}^{\\top}\\boldsymbol{x} = 1 \\\\\n \\boldsymbol{x} \\succeq \\boldsymbol{0}\n\\end{aligned}\n$$\nwhere $Q$ is a symmetric positive definite covariance matrix, $\\boldsymbol{\\mu}$ is the vector of expected returns, and $\\boldsymbol{x}$ is the vector of portfolio weights. The condition that $Q$ is positive definite ensures that the objective function $f(\\boldsymbol{x})$ is strictly convex. The feasible region, defined by the linear equality and non-negativity constraints, is a simplex, which is a closed and convex set. The minimization of a strictly convex function over a non-empty, closed, convex set guarantees the existence of a unique optimal solution $\\boldsymbol{x}^{\\star}$.\n\nThe optimality of a solution is fully characterized by the Karush-Kuhn-Tucker (KKT) conditions. The Lagrangian for this problem is:\n$$\n\\mathcal{L}(\\boldsymbol{x}, \\lambda, \\boldsymbol{s}) = \\frac{1}{2}\\boldsymbol{x}^{\\top} Q \\boldsymbol{x} - \\boldsymbol{\\mu}^{\\top} \\boldsymbol{x} - \\lambda(\\boldsymbol{1}^{\\top}\\boldsymbol{x} - 1) - \\boldsymbol{s}^{\\top}\\boldsymbol{x}\n$$\nwhere $\\lambda \\in \\mathbb{R}$ is the Lagrange multiplier for the equality constraint and $\\boldsymbol{s} \\in \\mathbb{R}^n$ is the vector of Lagrange multipliers for the non-negativity constraints. The KKT conditions for optimality are:\n1.  **Stationarity:** $\\nabla_{\\boldsymbol{x}} \\mathcal{L} = Q\\boldsymbol{x} - \\boldsymbol{\\mu} - \\lambda\\boldsymbol{1} - \\boldsymbol{s} = \\boldsymbol{0}$\n2.  **Primal Feasibility:** $\\boldsymbol{1}^{\\top}\\boldsymbol{x} = 1$, $\\boldsymbol{x} \\succeq \\boldsymbol{0}$\n3.  **Dual Feasibility:** $\\boldsymbol{s} \\succeq \\boldsymbol{0}$\n4.  **Complementary Slackness:** $x_i s_i = 0$ for all $i \\in \\{1, \\dots, n\\}$\n\nTo solve this system of equations and inequalities, we employ a primal-dual interior-point method (IPM). This class of algorithms is highly effective for QPs and is well-suited to the problem's constraints, particularly the restriction against forming or using explicit matrix inverses. IPMs iteratively solve a sequence of perturbed KKT systems, approaching the optimal solution from the interior of the feasible region.\n\nThe complementary slackness conditions $x_i s_i = 0$ are replaced by a perturbed version, $x_i s_i = \\tau$, where $\\tau  0$ is a barrier parameter that is driven to zero as the iterations proceed. The core of the IPM is to apply Newton's method to solve this perturbed KKT system for the search directions $(\\Delta\\boldsymbol{x}, \\Delta\\lambda, \\Delta\\boldsymbol{s})$. This leads to a linear system at each iteration.\n\nLet the current iterate be $(\\boldsymbol{x}, \\lambda, \\boldsymbol{s})$. The primal and dual residuals are:\n$$\n\\begin{aligned}\nr_p = \\boldsymbol{1}^{\\top}\\boldsymbol{x} - 1 \\\\\n\\boldsymbol{r}_d = Q\\boldsymbol{x} - \\boldsymbol{\\mu} - \\lambda\\boldsymbol{1} - \\boldsymbol{s}\n\\end{aligned}\n$$\nThe Newton system for the search directions $(\\Delta\\boldsymbol{x}, \\Delta\\lambda, \\Delta\\boldsymbol{s})$ is:\n$$\n\\begin{bmatrix}\nQ  \\boldsymbol{1}  I \\\\\n\\boldsymbol{1}^{\\top}  0  \\boldsymbol{0}^{\\top} \\\\\nS  \\boldsymbol{0}  X\n\\end{bmatrix}\n\\begin{bmatrix}\n\\Delta\\boldsymbol{x} \\\\\n-\\Delta\\lambda \\\\\n-\\Delta\\boldsymbol{s}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n-\\boldsymbol{r}_d \\\\\n-r_p \\\\\n-X\\boldsymbol{s} + \\sigma \\mu_{gap} \\boldsymbol{1}\n\\end{bmatrix}\n$$\nwhere $X = \\text{diag}(\\boldsymbol{x})$, $S = \\text{diag}(\\boldsymbol{s})$, $\\mu_{gap} = (\\boldsymbol{x}^{\\top}\\boldsymbol{s})/n$ is the duality gap, and $\\sigma \\in [0, 1]$ is a centering parameter.\n\nTo adhere to the constraint of not forming or inverting matrices, we do not solve this system directly. Instead, we eliminate $\\Delta\\boldsymbol{s}$ to obtain a smaller, equivalent system known as the augmented system or normal equations:\n$$\n\\begin{bmatrix}\nQ + X^{-1}S  \\boldsymbol{1} \\\\\n\\boldsymbol{1}^{\\top}  0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\Delta\\boldsymbol{x} \\\\\n\\Delta\\lambda\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\boldsymbol{b}_1 \\\\\nb_2\n\\end{bmatrix}\n$$\nHere, the matrix $H_{bar} = Q + X^{-1}S$ is symmetric and positive definite, as $Q$ is positive definite and $X^{-1}S$ is a diagonal matrix with positive entries (since $\\boldsymbol{x}  \\boldsymbol{0}$ and $\\boldsymbol{s}  \\boldsymbol{0}$ in the interior).\n\nTo solve this system for $(\\Delta\\boldsymbol{x}, \\Delta\\lambda)$ without inverting $H_{bar}$, we use the Schur complement method. We solve for $\\Delta\\boldsymbol{x}$ in terms of $\\Delta\\lambda$ from the first block equation: $\\Delta\\boldsymbol{x} = H_{bar}^{-1}(\\boldsymbol{b}_1 - \\Delta\\lambda\\boldsymbol{1})$. Substituting this into the second block equation gives a scalar equation for $\\Delta\\lambda$:\n$$\n\\Delta\\lambda = \\frac{\\boldsymbol{1}^{\\top}H_{bar}^{-1}\\boldsymbol{b}_1 - b_2}{\\boldsymbol{1}^{\\top}H_{bar}^{-1}\\boldsymbol{1}}\n$$\nComputing $\\Delta\\lambda$ requires the vectors $H_{bar}^{-1}\\boldsymbol{b}_1$ and $H_{bar}^{-1}\\boldsymbol{1}$. These are obtained by solving the linear systems $H_{bar}\\boldsymbol{y}_1 = \\boldsymbol{b}_1$ and $H_{bar}\\boldsymbol{y}_2 = \\boldsymbol{1}$. Crucially, we use the Conjugate Gradient (CG) method to solve these systems. CG is an iterative algorithm that finds the solution using only matrix-vector products of the form $H_{bar}\\boldsymbol{v} = Q\\boldsymbol{v} + (X^{-1}S)\\boldsymbol{v}$. This operation relies only on matrix-vector products with the original Hessian $Q$, thereby satisfying all problem constraints.\n\nOnce $\\Delta\\lambda$ and $\\Delta\\boldsymbol{x}$ are found, $\\Delta\\boldsymbol{s}$ is recovered. A line search is then performed to determine a step size $\\alpha \\in (0, 1]$ that maintains the positivity of $\\boldsymbol{x}$ and $\\boldsymbol{s}$. The iterates are updated:\n$$\n\\boldsymbol{x} \\leftarrow \\boldsymbol{x} + \\alpha \\Delta\\boldsymbol{x}, \\quad \\lambda \\leftarrow \\lambda + \\alpha \\Delta\\lambda, \\quad \\boldsymbol{s} \\leftarrow \\boldsymbol{s} + \\alpha \\Delta\\boldsymbol{s}\n$$\nThis process is repeated until the primal residuals, dual residuals, and the duality gap are all below a specified tolerance, at which point the algorithm has converged to the optimal solution $\\boldsymbol{x}^{\\star}$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef conjugate_gradient(matvec_func, b, x0=None, tol=1e-10, max_iter=1000):\n    \"\"\"\n    Solves the symmetric positive-definite system Ax = b using the Conjugate Gradient method.\n    'matvec_func' is a function that computes the product A@v.\n    \"\"\"\n    n = len(b)\n    if x0 is None:\n        x = np.zeros(n)\n    else:\n        x = x0.copy()\n\n    r = b - matvec_func(x)\n    p = r.copy()\n    rs_old = np.dot(r, r)\n\n    if np.sqrt(rs_old)  tol:\n        return x\n\n    for _ in range(max_iter):\n        Ap = matvec_func(p)\n        alpha = rs_old / np.dot(p, Ap)\n        x += alpha * p\n        r -= alpha * Ap\n        rs_new = np.dot(r, r)\n\n        if np.sqrt(rs_new)  tol:\n            break\n\n        p = r + (rs_new / rs_old) * p\n        rs_old = rs_new\n\n    return x\n\ndef solve_portfolio_optimization(Q, mu, max_iter=50, tol_outer=1e-9, tol_inner=1e-12, eta=0.995, sigma=0.1):\n    \"\"\"\n    Solves the quadratic program using a primal-dual interior-point method.\n    The method iteratively solves the KKT conditions using Newton's method,\n    with an inner loop using Conjugate Gradient to solve linear systems\n    without explicit matrix inversion.\n    \"\"\"\n    n = len(mu)\n    \n    # Initialization\n    x = np.ones(n) / n\n    lam = 0.0\n    s = np.ones(n)\n\n    ones_n = np.ones(n)\n\n    for _ in range(max_iter):\n        # Calculate residuals and duality gap\n        res_p = np.sum(x) - 1.0\n        res_d = Q @ x - mu - lam * ones_n - s\n        gap = np.dot(s, x) / n\n\n        # Convergence check\n        norm_res_d = np.linalg.norm(res_d)\n        if norm_res_d  tol_outer and abs(res_p)  tol_outer and gap  tol_outer:\n            break\n\n        # Define matrix-vector product for the CG solver's system matrix H_bar = Q + X^-1 * S\n        s_over_x = s / x\n        def h_bar_mv(v):\n            return Q @ v + s_over_x * v\n\n        # Set up the right-hand sides for the reduced Newton system\n        rhs_dx_eq = -res_d - s + (sigma * gap) / x\n        rhs_lam_eq = -res_p\n\n        # Solve the linear systems using Conjugate Gradient\n        # System 1: H_bar * y1 = rhs_dx_eq\n        # System 2: H_bar * y2 = 1\n        y1 = conjugate_gradient(h_bar_mv, rhs_dx_eq, tol=tol_inner, max_iter=2*n)\n        y2 = conjugate_gradient(h_bar_mv, ones_n, tol=tol_inner, max_iter=2*n)\n\n        # Compute search directions for lambda, x, and s\n        dlam = (rhs_lam_eq - np.sum(y1)) / np.sum(y2)\n        dx = y1 + dlam * y2\n        ds = (-s * x - s * dx + sigma * gap) / x\n        \n        # Line search for step sizes to maintain positivity\n        alpha_p = 1.0\n        if np.any(dx  0):\n            alpha_p = min(1.0, np.min(-x[dx  0] / dx[dx  0]))\n            \n        alpha_s = 1.0\n        if np.any(ds  0):\n            alpha_s = min(1.0, np.min(-s[ds  0] / ds[ds  0]))\n        \n        alpha = min(alpha_p, alpha_s) * eta\n\n        # Update iterates\n        x += alpha * dx\n        s += alpha * ds\n        lam += alpha * dlam\n\n    # Post-processing for clean output\n    x[x  tol_outer] = 0.0\n    x /= np.sum(x)\n    \n    return x\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and run the solver.\n    \"\"\"\n    # Test case 1\n    Q1 = np.array([\n        [0.10, 0.02, 0.04, 0.00],\n        [0.02, 0.08, 0.01, 0.00],\n        [0.04, 0.01, 0.09, 0.00],\n        [0.00, 0.00, 0.00, 0.05]\n    ])\n    mu1 = np.array([0.12, 0.10, 0.07, 0.03])\n\n    # Test case 2\n    Q2 = np.array([\n        [0.20, 0.15, 0.15],\n        [0.15, 0.30, 0.25],\n        [0.15, 0.25, 0.50]\n    ])\n    mu2 = np.array([0.18, 0.02, 0.01])\n\n    # Test case 3\n    Q3_base = 0.02 * np.array([\n        [1.0,    0.999,  0.9995],\n        [0.999,  1.0,    0.9992],\n        [0.9995, 0.9992, 1.0]\n    ])\n    Q3 = Q3_base + 1e-6 * np.identity(3)\n    mu3 = np.array([0.050, 0.051, 0.049])\n\n    test_cases = [\n        (Q1, mu1),\n        (Q2, mu2),\n        (Q3, mu3),\n    ]\n\n    results = []\n    for Q, mu in test_cases:\n        x_star = solve_portfolio_optimization(Q, mu)\n        results.append(x_star)\n\n    # Format the final output string exactly as required\n    def format_list(arr):\n        return '[' + ','.join([f'{x:.6f}' for x in arr]) + ']'\n        \n    formatted_strings = [format_list(res) for res in results]\n    print(f\"[[{format_list(results[0])},{format_list(results[1])},{format_list(results[2])}]]\")\n\n# The expected answer is a JSON-formatted list of lists of lists.\n# However, the problem asks for a single line of comma-separated lists.\n# Let me adjust the output format to match the problem description's example.\n# [[x1,x2,...], [y1,y2,...], [z1,z2,...]]\n\ndef solve_and_print():\n    # Test case 1\n    Q1 = np.array([[0.10,0.02,0.04,0.00],[0.02,0.08,0.01,0.00],[0.04,0.01,0.09,0.00],[0.00,0.00,0.00,0.05]])\n    mu1 = np.array([0.12,0.10,0.07,0.03])\n    # Test case 2\n    Q2 = np.array([[0.20,0.15,0.15],[0.15,0.30,0.25],[0.15,0.25,0.50]])\n    mu2 = np.array([0.18,0.02,0.01])\n    # Test case 3\n    Q3 = 0.02 * np.array([[1.0,0.999,0.9995],[0.999,1.0,0.9992],[0.9995,0.9992,1.0]]) + 1e-6 * np.identity(3)\n    mu3 = np.array([0.050,0.051,0.049])\n\n    x1 = solve_portfolio_optimization(Q1, mu1)\n    x2 = solve_portfolio_optimization(Q2, mu2)\n    x3 = solve_portfolio_optimization(Q3, mu3)\n    \n    res_str = \"[\"\n    res_str += \"[\" + \",\".join([f\"{val:.6f}\" for val in x1]) + \"],\"\n    res_str += \"[\" + \",\".join([f\"{val:.6f}\" for val in x2]) + \"],\"\n    res_str += \"[\" + \",\".join([f\"{val:.6f}\" for val in x3]) + \"]\"\n    res_str += \"]\"\n    \n    print(res_str)\n\nsolve_and_print()\n```", "id": "2402731"}]}