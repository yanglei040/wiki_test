{"hands_on_practices": [{"introduction": "To truly appreciate the elegance of the Conjugate Gradient (CG) method, it is instructive to compare it directly with its simpler predecessor, the Steepest Descent (SD) method. This exercise provides a concrete, step-by-step calculation in a simple two-dimensional portfolio optimization setting, allowing you to quantify the suboptimality of the \"greedy\" steepest descent path versus the more deliberate, non-interfering path taken by CG [@problem_id:2382887]. By working through this problem, you will see firsthand how CG's intelligent choice of search directions avoids the inefficient zigzagging that often plagues the SD algorithm.", "problem": "Consider an unconstrained mean–variance portfolio selection problem with two risky assets. The objective is to minimize the quadratic function\n$$f(w) \\;=\\; \\frac{1}{2}\\, w^{\\top} \\Sigma \\, w \\;-\\; \\mu^{\\top} w,$$\nwhere the covariance matrix is\n$$\\Sigma \\;=\\; \\begin{pmatrix} 2 & 1 \\\\[4pt] 1 & 3 \\end{pmatrix},$$\nthe expected return vector is\n$$\\mu \\;=\\; \\begin{pmatrix} 1 \\\\[4pt] 0 \\end{pmatrix},$$\nand the initial portfolio is\n$$w_{0} \\;=\\; \\begin{pmatrix} 0 \\\\[4pt] 0 \\end{pmatrix}.$$\n\nAt step $k \\in \\{1,2\\}$, compare two search directions from the same current iterate: the steepest descent direction $d_{k}^{\\mathrm{SD}} \\,=\\, -\\nabla f(w_{k-1})$ and a Conjugate Gradient (CG) direction $d_{k}^{\\mathrm{CG}}$ constructed to be $\\Sigma$-conjugate to the previous CG direction (with $d_{1}^{\\mathrm{CG}} \\,=\\, -\\nabla f(w_{0})$ and $d_{2}^{\\mathrm{CG}}$ satisfying $\\big(d_{2}^{\\mathrm{CG}}\\big)^{\\top} \\Sigma \\, d_{1}^{\\mathrm{CG}} \\,=\\, 0$). For each direction at each step, take the exact line search step that minimizes $f$ along the corresponding line, and denote the resulting post-step points by $w_{k}^{\\mathrm{SD}}$ and $w_{k}^{\\mathrm{CG}}$. Define the suboptimality at step $k$ as\n$$s_{k} \\;=\\; f\\!\\big(w_{k}^{\\mathrm{SD}}\\big) \\;-\\; f\\!\\big(w_{k}^{\\mathrm{CG}}\\big).$$\n\nCompute $s_{1}$ and $s_{2}$ exactly. Report your final answer as a single row vector $\\big(s_{1} \\;\\; s_{2}\\big)$ using exact fractions. Do not include units and do not round.", "solution": "The problem statement is scientifically grounded, well-posed, and objective. It describes a standard quadratic optimization task for which the specified numerical algorithms, Steepest Descent (SD) and Conjugate Gradient (CG), are well-defined. We proceed with a formal derivation.\n\nThe objective function to minimize is given by\n$$f(w) = \\frac{1}{2} w^{\\top} \\Sigma w - \\mu^{\\top} w$$\nThe gradient of this function is\n$$\\nabla f(w) = \\Sigma w - \\mu$$\nThe initial point is $w_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$. The initial gradient is $\\nabla f(w_0) = \\Sigma w_0 - \\mu = -\\mu = \\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix}$.\nThe residual vector is defined as $r = -\\nabla f(w)$. The initial residual is $r_0 = -\\nabla f(w_0) = \\mu = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\n\nThe optimal step size $\\alpha$ for an exact line search from an iterate $w_k$ along a direction $d_k$ is given by the formula that minimizes $f(w_k + \\alpha d_k)$:\n$$\\alpha_k = -\\frac{\\nabla f(w_k)^{\\top} d_k}{d_k^{\\top} \\Sigma d_k}$$\n\nStep $k=1$:\n\nFor both SD and CG, the initial search direction is the negative gradient:\n$$d_1^{\\mathrm{SD}} = d_1^{\\mathrm{CG}} = -\\nabla f(w_0) = r_0 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$$\nThe step size $\\alpha_1$ is identical for both methods:\n$$\\alpha_1 = -\\frac{\\nabla f(w_0)^{\\top} d_1}{d_1^{\\top} \\Sigma d_1} = \\frac{r_0^{\\top} r_0}{r_0^{\\top} \\Sigma r_0}$$\nWe compute the terms:\n$$r_0^{\\top} r_0 = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = 1$$\n$$r_0^{\\top} \\Sigma r_0 = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} 2 & 1 \\\\ 1 & 3 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = 2$$\nThe step size is $\\alpha_1 = \\frac{1}{2}$.\nThe new iterates are identical:\n$$w_1^{\\mathrm{SD}} = w_1^{\\mathrm{CG}} = w_0 + \\alpha_1 d_1 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + \\frac{1}{2} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\end{pmatrix}$$\nLet us denote this common iterate as $w_1$. The value of the objective function at this point is:\n$$f(w_1) = \\frac{1}{2} w_1^{\\top} \\Sigma w_1 - \\mu^{\\top} w_1 = \\frac{1}{2} \\begin{pmatrix} \\frac{1}{2} & 0 \\end{pmatrix} \\begin{pmatrix} 2 & 1 \\\\ 1 & 3 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\end{pmatrix}$$\n$$f(w_1) = \\frac{1}{2} \\begin{pmatrix} 1 & \\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\end{pmatrix} - \\frac{1}{2} = \\frac{1}{2} \\left(\\frac{1}{2}\\right) - \\frac{1}{2} = \\frac{1}{4} - \\frac{1}{2} = -\\frac{1}{4}$$\nSince $f(w_1^{\\mathrm{SD}}) = f(w_1^{\\mathrm{CG}}) = -\\frac{1}{4}$, the suboptimality at step $1$ is:\n$$s_1 = f(w_1^{\\mathrm{SD}}) - f(w_1^{\\mathrm{CG}}) = 0$$\n\nStep $k=2$:\n\nWe start from the common iterate $w_1 = \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\end{pmatrix}$. The gradient at this point is:\n$$\\nabla f(w_1) = \\Sigma w_1 - \\mu = \\begin{pmatrix} 2 & 1 \\\\ 1 & 3 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ \\frac{1}{2} \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ \\frac{1}{2} \\end{pmatrix}$$\nThe new residual is $r_1 = -\\nabla f(w_1) = \\begin{pmatrix} 0 \\\\ -\\frac{1}{2} \\end{pmatrix}$.\n\nFor the SD method, the search direction is $d_2^{\\mathrm{SD}} = -\\nabla f(w_1) = r_1 = \\begin{pmatrix} 0 \\\\ -\\frac{1}{2} \\end{pmatrix}$.\nThe step size is $\\alpha_2^{\\mathrm{SD}} = \\frac{r_1^{\\top} r_1}{r_1^{\\top} \\Sigma r_1}$.\nNumerator: $r_1^{\\top} r_1 = (0)^2 + (-\\frac{1}{2})^2 = \\frac{1}{4}$.\nDenominator: $r_1^{\\top} \\Sigma r_1 = \\begin{pmatrix} 0 & -\\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} 2 & 1 \\\\ 1 & 3 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ -\\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} 0 & -\\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} -\\frac{1}{2} \\\\ -\\frac{3}{2} \\end{pmatrix} = \\frac{3}{4}$.\n$\\alpha_2^{\\mathrm{SD}} = \\frac{1/4}{3/4} = \\frac{1}{3}$.\nThe SD iterate is $w_2^{\\mathrm{SD}} = w_1 + \\alpha_2^{\\mathrm{SD}} d_2^{\\mathrm{SD}} = \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\end{pmatrix} + \\frac{1}{3} \\begin{pmatrix} 0 \\\\ -\\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} \\\\ -\\frac{1}{6} \\end{pmatrix}$.\nThe function value is $f(w_2^{\\mathrm{SD}}) = \\frac{1}{2} \\begin{pmatrix} \\frac{1}{2} & -\\frac{1}{6} \\end{pmatrix} \\begin{pmatrix} 2 & 1 \\\\ 1 & 3 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \\\\ -\\frac{1}{6} \\end{pmatrix} - \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \\\\ -\\frac{1}{6} \\end{pmatrix}$.\n$f(w_2^{\\mathrm{SD}}) = \\frac{1}{2} \\begin{pmatrix} \\frac{1}{2} & -\\frac{1}{6} \\end{pmatrix} \\begin{pmatrix} 1-\\frac{1}{6} \\\\ \\frac{1}{2}-\\frac{3}{6} \\end{pmatrix} - \\frac{1}{2} = \\frac{1}{2} \\begin{pmatrix} \\frac{1}{2} & -\\frac{1}{6} \\end{pmatrix} \\begin{pmatrix} \\frac{5}{6} \\\\ 0 \\end{pmatrix} - \\frac{1}{2} = \\frac{1}{2}\\left(\\frac{5}{12}\\right) - \\frac{1}{2} = \\frac{5}{24} - \\frac{12}{24} = -\\frac{7}{24}$.\n\nFor the CG method, the search direction is $d_2^{\\mathrm{CG}} = r_1 + \\beta_1 d_1^{\\mathrm{CG}}$, with $\\beta_1 = \\frac{r_1^{\\top}r_1}{r_0^{\\top}r_0}$.\nWith $r_1^{\\top} r_1 = \\frac{1}{4}$ and $r_0^{\\top} r_0 = 1$, we have $\\beta_1 = \\frac{1}{4}$.\n$$d_2^{\\mathrm{CG}} = r_1 + \\frac{1}{4}d_1^{\\mathrm{CG}} = \\begin{pmatrix} 0 \\\\ -\\frac{1}{2} \\end{pmatrix} + \\frac{1}{4} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{4} \\\\ -\\frac{1}{2} \\end{pmatrix}$$\nThe step size is $\\alpha_2^{\\mathrm{CG}} = -\\frac{\\nabla f(w_1)^{\\top} d_2^{\\mathrm{CG}}}{(d_2^{\\mathrm{CG}})^{\\top} \\Sigma d_2^{\\mathrm{CG}}} = \\frac{r_1^{\\top} d_2^{\\mathrm{CG}}}{(d_2^{\\mathrm{CG}})^{\\top} \\Sigma d_2^{\\mathrm{CG}}}$.\nNumerator: $r_1^{\\top} d_2^{\\mathrm{CG}} = \\begin{pmatrix} 0 & -\\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{4} \\\\ -\\frac{1}{2} \\end{pmatrix} = \\frac{1}{4}$.\nDenominator: $(d_2^{\\mathrm{CG}})^{\\top} \\Sigma d_2^{\\mathrm{CG}} = \\begin{pmatrix} \\frac{1}{4} & -\\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} 2 & 1 \\\\ 1 & 3 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{4} \\\\ -\\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{4} & -\\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} 0 \\\\ -\\frac{5}{4} \\end{pmatrix} = \\frac{5}{8}$.\n$\\alpha_2^{\\mathrm{CG}} = \\frac{1/4}{5/8} = \\frac{2}{5}$.\nThe CG iterate is $w_2^{\\mathrm{CG}} = w_1 + \\alpha_2^{\\mathrm{CG}} d_2^{\\mathrm{CG}} = \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\end{pmatrix} + \\frac{2}{5} \\begin{pmatrix} \\frac{1}{4} \\\\ -\\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2}+\\frac{1}{10} \\\\ -\\frac{1}{5} \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{5} \\\\ -\\frac{1}{5} \\end{pmatrix}$.\nThis is the exact minimizer $w^* = \\Sigma^{-1}\\mu$, as expected for CG in $N=2$ dimensions.\nThe minimum function value is $f(w_2^{\\mathrm{CG}}) = f(w^*) = -\\frac{1}{2}\\mu^\\top w^* = -\\frac{1}{2}\\begin{pmatrix} 1 & 0 \\end{pmatrix}\\begin{pmatrix} 3/5 \\\\ -1/5 \\end{pmatrix} = -\\frac{3}{10}$.\n\nThe suboptimality at step $2$ is:\n$$s_2 = f(w_2^{\\mathrm{SD}}) - f(w_2^{\\mathrm{CG}}) = -\\frac{7}{24} - \\left(-\\frac{3}{10}\\right) = -\\frac{7}{24} + \\frac{3}{10} = \\frac{-35 + 36}{120} = \\frac{1}{120}$$\n\nThe final values are $s_1 = 0$ and $s_2 = \\frac{1}{120}$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix} 0 & \\frac{1}{120} \\end{pmatrix}\n}\n$$", "id": "2382887"}, {"introduction": "The power of the Conjugate Gradient method extends beyond just being faster than steepest descent; its convergence rate is fundamentally linked to the spectral properties of the system matrix $A$. This problem explores a fascinating theoretical guarantee: for a symmetric positive-definite system $Ax=b$ where $A$ has only $k$ distinct eigenvalues, CG is guaranteed to find the exact solution in at most $k$ iterations [@problem_id:2382876]. You will verify this property for a four-asset system arising from a single-factor model, a common structure in finance, and in doing so, build a deeper connection between abstract linear algebra and practical economic modeling.", "problem": "Consider a four-asset mean–variance setting where the return vector has covariance matrix $\\Sigma \\in \\mathbb{R}^{4 \\times 4}$ given by\n$$\n\\Sigma \\;=\\; I_{4} \\;+\\; v v^{\\top}, \\quad v \\;=\\; \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\nLet the excess-return vector be $b \\in \\mathbb{R}^{4}$ given by\n$$\nb \\;=\\; \\begin{pmatrix} 2 \\\\ 1 \\\\ 5 \\\\ -4 \\end{pmatrix}.\n$$\nYou are to solve the linear system\n$$\n\\Sigma x \\;=\\; b\n$$\nusing the conjugate gradient (CG) method, starting from $x_{0} = 0_{4}$ and assuming exact arithmetic throughout. \n\nTasks:\n- Establish that $\\Sigma$ has exactly two distinct eigenvalues and is symmetric positive definite.\n- Prove that the conjugate gradient method terminates in exactly two iterations for this system under exact arithmetic, and compute the converged solution vector $x$ that satisfies $\\Sigma x = b$.\n- Briefly explain the economic structure that the specific form of $\\Sigma$ implies for the joint distribution of returns.\n\nReport only the converged solution vector $x$ as your final numeric answer, written as a single row matrix. No rounding is required.", "solution": "The problem statement will first be validated for scientific soundness and consistency.\n\nStep 1: Extract Givens.\n- The covariance matrix is $\\Sigma = I_{4} + v v^{\\top}$, where $I_4$ is the $4 \\times 4$ identity matrix.\n- The vector $v$ is given by $v = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$.\n- The excess-return vector is $b = \\begin{pmatrix} 2 \\\\ 1 \\\\ 5 \\\\ -4 \\end{pmatrix}$.\n- The linear system to solve is $\\Sigma x = b$.\n- The solution method is the conjugate gradient (CG) method.\n- The initial guess is the zero vector, $x_{0} = 0_{4}$.\n- The calculation assumes exact arithmetic.\n\nThe tasks are:\n1. Establish that $\\Sigma$ has exactly two distinct eigenvalues and is symmetric positive definite (SPD).\n2. Prove that the CG method terminates in exactly two iterations and compute the solution vector $x$.\n3. Explain the economic structure implied by the form of $\\Sigma$.\n\nStep 2: Validate Using Extracted Givens.\nThe problem is scientifically grounded, being a standard problem in computational finance and numerical linear algebra. The matrix $\\Sigma$ is a rank-one update of the identity, a well-understood structure. The conjugate gradient method is appropriate for systems where the matrix is SPD. To confirm this, we check the properties of $\\Sigma$.\n\nSymmetry: $\\Sigma^{\\top} = (I_{4} + vv^{\\top})^{\\top} = I_{4}^{\\top} + (vv^{\\top})^{\\top} = I_{4} + (v^{\\top})^{\\top}v^{\\top} = I_{4} + vv^{\\top} = \\Sigma$. The matrix is symmetric.\n\nEigenvalues: Let $u$ be an eigenvector of $\\Sigma$. The eigenvalue equation is $(I_{4} + vv^{\\top})u = \\lambda u$, which rearranges to $(vv^{\\top})u = (\\lambda - 1)u$. This shows that $u$ is also an eigenvector of the rank-one matrix $vv^{\\top}$ with eigenvalue $\\lambda - 1$.\nThe matrix $vv^{\\top}$ has at most one non-zero eigenvalue.\n- If $u$ is a multiple of $v$, say $u=cv$ for some non-zero scalar $c$, then $vv^{\\top}(cv) = v(v^{\\top}cv) = c(v^{\\top}v)v$. The corresponding eigenvalue of $vv^{\\top}$ is $\\lambda_{vv^{\\top}} = v^{\\top}v = 1^2 + 1^2 + 0^2 + 0^2 = 2$.\n- If $u$ is orthogonal to $v$ ($v^{\\top}u = 0$), then $vv^{\\top}u = v(v^{\\top}u) = v(0) = 0$. The eigenvalue is $0$. The space of vectors orthogonal to $v$ in $\\mathbb{R}^{4}$ has dimension $4-1=3$.\nSo, the eigenvalues of $vv^{\\top}$ are $2$ (with multiplicity $1$) and $0$ (with multiplicity $3$).\nThe eigenvalues of $\\Sigma = I_4 + vv^{\\top}$ are given by $\\lambda_{\\Sigma} = 1 + \\lambda_{vv^{\\top}}$. Thus, the eigenvalues of $\\Sigma$ are $1+2=3$ (multiplicity $1$) and $1+0=1$ (multiplicity $3$).\nSince $\\Sigma$ has exactly two distinct eigenvalues ($1$ and $3$), and both are positive, the symmetric matrix $\\Sigma$ is positive definite.\n\nThe problem is well-posed, complete, and consistent. It is a valid problem.\n\nStep 3: Verdict and Action.\nThe problem is valid. I will now provide the solution.\n\nThe problem is addressed in three parts as requested.\n\nPart 1: Eigenvalues and Symmetric Positive Definite (SPD) Property of $\\Sigma$.\nAs established in the validation step, the matrix $\\Sigma = I_4 + vv^{\\top}$ is symmetric. Its eigenvalues are $3$ and $1$. As all eigenvalues are positive, $\\Sigma$ is symmetric positive definite. This confirms that the conjugate gradient method is applicable.\n\nPart 2: Conjugate Gradient Termination and Solution.\nThe conjugate gradient method is guaranteed to find the exact solution to $\\Sigma x = b$ in at most $k$ iterations (assuming exact arithmetic), where $k$ is the number of distinct eigenvalues of $\\Sigma$. Since $\\Sigma$ has exactly two distinct eigenvalues, the method will terminate in exactly two iterations.\n\nTo compute the solution $x$, one could perform the two steps of the CG algorithm. However, a more insightful method utilizes the spectral properties of $\\Sigma$ that we have just derived. This approach is more robust and less prone to arithmetic error. The solution $x$ must lie in the Krylov subspace $\\mathcal{K}_{2}(\\Sigma, r_0)$ since the method converges in two steps and $x_0=0$. This subspace is spanned by $\\{r_0, \\Sigma r_0\\}$, where $r_0=b-\\Sigma x_0=b$.\n\nThe exact solution is given by $x = \\Sigma^{-1}b$. We can compute this by decomposing $b$ into the eigenspaces of $\\Sigma$.\nThe eigenspace corresponding to $\\lambda=3$ is spanned by the vector $v = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$.\nThe eigenspace corresponding to $\\lambda=1$ is the orthogonal complement of the space spanned by $v$, i.e., the set of vectors $u$ such that $v^{\\top}u=0$.\n\nWe decompose $b$ into a component parallel to $v$, $b_{\\parallel}$, and a component orthogonal to $v$, $b_{\\perp}$.\n$b = b_{\\parallel} + b_{\\perp}$.\nThe parallel component is the projection of $b$ onto $v$:\n$$\nb_{\\parallel} = \\frac{b^{\\top}v}{v^{\\top}v} v = \\frac{\\begin{pmatrix} 2 & 1 & 5 & -4 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}}{\\begin{pmatrix} 1 & 1 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}} v = \\frac{2(1)+1(1)}{1^2+1^2} v = \\frac{3}{2} v = \\begin{pmatrix} 3/2 \\\\ 3/2 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\nThe orthogonal component is $b_{\\perp} = b - b_{\\parallel}$:\n$$\nb_{\\perp} = \\begin{pmatrix} 2 \\\\ 1 \\\\ 5 \\\\ -4 \\end{pmatrix} - \\begin{pmatrix} 3/2 \\\\ 3/2 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1/2 \\\\ -1/2 \\\\ 5 \\\\ -4 \\end{pmatrix}.\n$$\nWe can verify that $v^{\\top}b_{\\perp} = 1(1/2) + 1(-1/2) + 0 + 0 = 0$, so $b_{\\perp}$ is indeed in the eigenspace for $\\lambda=1$.\n\nNow we apply $\\Sigma^{-1}$ to $b = b_{\\parallel} + b_{\\perp}$. Since $b_{\\parallel}$ is an eigenvector for $\\lambda=3$ and $b_{\\perp}$ is an eigenvector for $\\lambda=1$:\n$$\nx = \\Sigma^{-1}b = \\Sigma^{-1}(b_{\\parallel} + b_{\\perp}) = \\Sigma^{-1}b_{\\parallel} + \\Sigma^{-1}b_{\\perp}\n$$\n$$\nx = \\frac{1}{3} b_{\\parallel} + \\frac{1}{1} b_{\\perp}\n$$\nSubstituting the vectors:\n$$\nx = \\frac{1}{3} \\begin{pmatrix} 3/2 \\\\ 3/2 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 1/2 \\\\ -1/2 \\\\ 5 \\\\ -4 \\end{pmatrix} = \\begin{pmatrix} 1/2 \\\\ 1/2 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 1/2 \\\\ -1/2 \\\\ 5 \\\\ -4 \\end{pmatrix} = \\begin{pmatrix} 1/2+1/2 \\\\ 1/2-1/2 \\\\ 0+5 \\\\ 0-4 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 5 \\\\ -4 \\end{pmatrix}.\n$$\nThis is the exact solution, which the CG algorithm finds in two steps.\n\nLet us verify the solution:\n$$\n\\Sigma x = (I_4 + vv^{\\top})x = x + v(v^{\\top}x)\n$$\n$$\nv^{\\top}x = \\begin{pmatrix} 1 & 1 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 5 \\\\ -4 \\end{pmatrix} = 1(1) + 1(0) = 1.\n$$\n$$\n\\Sigma x = \\begin{pmatrix} 1 \\\\ 0 \\\\ 5 \\\\ -4 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix} (1) = \\begin{pmatrix} 1+1 \\\\ 0+1 \\\\ 5+0 \\\\ -4+0 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 1 \\\\ 5 \\\\ -4 \\end{pmatrix} = b.\n$$\nThe solution is correct.\n\nPart 3: Economic Structure.\nThe covariance matrix of asset returns $\\Sigma = I_4 + vv^{\\top}$ is characteristic of a single-factor asset pricing model. In such a model, the return $R_i$ of an asset $i$ is described by its exposure to a common market factor $F$ and an asset-specific idiosyncratic shock $\\epsilon_i$: $R_i = \\beta_i F + \\epsilon_i$. The covariance matrix for the vector of returns $R$ is $\\text{Cov}(R) = (\\beta\\beta^{\\top})\\sigma_F^2 + D$, where $\\beta$ is the vector of factor loadings, $\\sigma_F^2$ is the variance of the factor, and $D$ is the diagonal matrix of idiosyncratic variances.\n\nIn this problem, the structure $\\Sigma = vv^{\\top} + I_4$ implies:\n1.  There is a single systematic risk factor with unit variance ($\\sigma_F^2=1$).\n2.  The vector of factor exposures (loadings) is $v = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$. This means only the first two assets are exposed to this common risk factor, both with a loading of $1$. Assets 3 and 4 are not exposed to this factor.\n3.  The idiosyncratic risk matrix is $D = I_4$. This means each asset has an independent, specific risk component with a variance of $1$.\n\nConsequently, the non-zero covariance between assets 1 and 2, $(\\Sigma)_{12} = 1$, is entirely explained by their shared exposure to the common factor. All other pairs of assets are uncorrelated, as $(\\Sigma)_{ij} = 0$ for $i \\neq j$ unless $\\{i,j\\}=\\{1,2\\}$. The total variance of assets 1 and 2 is $(\\Sigma)_{11} = (\\Sigma)_{22} = 2$, comprising factor variance ($1^2 \\cdot 1 = 1$) and idiosyncratic variance ($1$). The total variance of assets 3 and 4 is $(\\Sigma)_{33} = (\\Sigma)_{44} = 1$, which is purely idiosyncratic as their factor loading is zero.", "answer": "$$\n\\boxed{\\begin{pmatrix} 1 & 0 & 5 & -4 \\end{pmatrix}}\n$$", "id": "2382876"}, {"introduction": "Sometimes, the best way to understand why something works is to see what happens when it breaks. This hands-on coding exercise takes you \"under the hood\" of the CG algorithm to demonstrate the critical importance of maintaining $A$-orthogonality among the search directions [@problem_id:2382914]. By implementing the algorithm and intentionally perturbing a search direction, you will experimentally confirm that this conjugacy is the linchpin for the method's guaranteed convergence in $n$ steps. This act of \"algorithmic sabotage\" provides a powerful and lasting intuition for the sophisticated design of the Conjugate Gradient method.", "problem": "You are given a linear system arising from a penalized mean–variance Markowitz portfolio model. Consider $n$ assets with return vector $\\mu \\in \\mathbb{R}^n$ and covariance matrix $\\Sigma \\in \\mathbb{R}^{n \\times n}$ that is symmetric positive definite (SPD). Introduce a soft budget penalty with weight $\\eta &gt; 0$ to obtain the unconstrained quadratic objective\n$$\nf(x) \\equiv \\tfrac{1}{2} x^{\\top} \\Sigma x - \\mu^{\\top} x + \\tfrac{\\eta}{2}\\left(\\mathbf{1}^{\\top} x - 1\\right)^2,\n$$\nwhere $\\mathbf{1} \\in \\mathbb{R}^n$ is the vector of ones. The first-order condition $\\nabla f(x) = 0$ reduces to the SPD linear system\n$$\nA x = b,\\quad\\text{with}\\quad A \\equiv \\Sigma + \\eta\\, \\mathbf{1}\\mathbf{1}^{\\top},\\quad b \\equiv \\mu + \\eta\\, \\mathbf{1}.\n$$\nYour task is to implement the Conjugate Gradient (CG) method to solve $A x = b$ and to demonstrate how perturbing a single search direction breaks $A$-orthogonality of the search directions and prevents exact convergence in at most $n$ steps.\n\nUse the following well-specified instance that is representative for computational finance:\n- Number of assets: $n = 6$.\n- Constant correlation matrix $R \\in \\mathbb{R}^{6 \\times 6}$ with correlation parameter $\\rho = 0.2$, defined by $R = \\rho \\mathbf{1}\\mathbf{1}^{\\top} + (1-\\rho) I$.\n- Asset standard deviations $s = [0.15, 0.20, 0.25, 0.30, 0.22, 0.18]$; define $\\Sigma = \\operatorname{diag}(s)\\, R\\, \\operatorname{diag}(s)$.\n- Expected returns $\\mu = [0.08, 0.10, 0.12, 0.15, 0.11, 0.09]^{\\top}$.\n- Penalty weight $\\eta = 10^{-2}$.\n\nAlgorithmic requirements:\n- Implement the Conjugate Gradient (CG) method starting from $x_0 = 0$. You must compute exactly $n$ iterations, without early termination, using mathematically justified updates that preserve $A$-conjugacy in exact arithmetic for SPD systems.\n- Implement a mechanism to perturb exactly one search direction. Specifically, if the perturbation iteration index is $k_{\\mathrm{perturb}} \\in \\{0,1,\\dots,n-1\\}$ and the scalar perturbation magnitude is $\\varepsilon \\geq 0$, then at iteration $j = k_{\\mathrm{perturb}}$ replace the current search direction $p_j$ by\n$$\n\\tilde{p}_j = p_j + \\varepsilon \\,\\lVert p_j \\rVert_2\\, e_1,\n$$\nwhere $e_1 = [1,0,\\dots,0]^{\\top} \\in \\mathbb{R}^n$ is the first standard basis vector. If $k_{\\mathrm{perturb}} < 0$ or $k_{\\mathrm{perturb}} \\ge n$ or $\\varepsilon = 0$, no perturbation should be applied.\n- Record all $n$ search directions as the columns of a matrix $P \\in \\mathbb{R}^{n \\times n}$ in the order they are used by CG.\n\nFor each run, after exactly $n$ iterations, compute both:\n1. The residual norm after $n$ iterations, defined by $\\lVert b - A x_n \\rVert_2$.\n2. The $A$-orthogonality defect of the search directions, defined as the maximum absolute value of off-diagonal entries of the Gram matrix $G \\equiv P^{\\top} A P$, that is,\n$$\n\\max_{i \\ne j} \\left| G_{ij} \\right|.\n$$\n\nTest suite:\nRun your implementation on the fixed $(A,b)$ above for the following five parameter pairs $(k_{\\mathrm{perturb}}, \\varepsilon)$:\n- Case $1$: $(-1, 0)$.\n- Case $2$: $(2, 1)$.\n- Case $3$: $(0, 1)$.\n- Case $4$: $(10, 1)$.\n- Case $5$: $(3, 10^{-3})$.\n\nFinal output format:\n- Your program must print a single line containing a flat list with $2$ numbers per test case, in order: for each case, first the residual norm after $n$ steps, then the $A$-orthogonality defect. All numbers must be rounded to exactly $8$ decimal places using standard rounding.\n- Concretely, the output must be a single line of the form\n$$\n[\\text{res}_1,\\text{def}_1,\\text{res}_2,\\text{def}_2,\\dots,\\text{res}_5,\\text{def}_5],\n$$\nwhere each $\\text{res}_i$ and $\\text{def}_i$ is a decimal numeral with exactly $8$ digits after the decimal point.", "solution": "The problem requires the implementation of the Conjugate Gradient (CG) method to solve a linear system $Ax=b$ derived from a mean-variance portfolio optimization problem. The central task is to demonstrate the importance of maintaining $A$-orthogonality among the search directions by analyzing the effect of a targeted perturbation.\n\nFirst, we conduct a formal validation of the problem statement.\n\nThe givens are:\n- Objective function: $f(x) \\equiv \\tfrac{1}{2} x^{\\top} \\Sigma x - \\mu^{\\top} x + \\tfrac{\\eta}{2}\\left(\\mathbf{1}^{\\top} x - 1\\right)^2$.\n- Linear system: $A x = b$.\n- System matrix: $A \\equiv \\Sigma + \\eta\\, \\mathbf{1}\\mathbf{1}^{\\top}$.\n- Right-hand side vector: $b \\equiv \\mu + \\eta\\, \\mathbf{1}$.\n- System dimension: $n = 6$.\n- Correlation parameter: $\\rho = 0.2$.\n- Correlation matrix: $R = \\rho \\mathbf{1}\\mathbf{1}^{\\top} + (1-\\rho) I$.\n- Asset standard deviations: $s = [0.15, 0.20, 0.25, 0.30, 0.22, 0.18]^{\\top}$.\n- Covariance matrix: $\\Sigma = \\operatorname{diag}(s)\\, R\\, \\operatorname{diag}(s)$.\n- Expected returns vector: $\\mu = [0.08, 0.10, 0.12, 0.15, 0.11, 0.09]^{\\top}$.\n- Penalty weight: $\\eta = 10^{-2}$.\n- CG starting vector: $x_0 = \\mathbf{0}$.\n- Iteration count: exactly $n=6$.\n- Perturbation rule: For $j = k_{\\text{perturb}}$, replace search direction $p_j$ with $\\tilde{p}_j = p_j + \\varepsilon \\,\\lVert p_j \\rVert_2\\, e_1$. This is active for $k_{\\mathrm{perturb}} \\in \\{0, 1, \\dots, n-1\\}$ and $\\varepsilon > 0$.\n- Metrics: $\\lVert b - A x_n \\rVert_2$ and $\\max_{i \\ne j} \\left| (P^{\\top} A P)_{ij} \\right|$.\n\nThe problem is valid. It is scientifically grounded in established principles of computational finance and numerical linear algebra. The covariance matrix $\\Sigma$ is constructed to be symmetric positive definite (SPD). The system matrix $A = \\Sigma + \\eta \\mathbf{1}\\mathbf{1}^{\\top}$ is the sum of an SPD matrix $\\Sigma$ and a symmetric positive semi-definite matrix $\\eta \\mathbf{1}\\mathbf{1}^{\\top}$ (since $\\eta = 10^{-2} > 0$), which ensures that $A$ is also SPD. Consequently, the linear system $Ax=b$ is well-posed, and the CG method is a suitable and theoretically sound solution approach. All parameters and algorithmic requirements are specified with sufficient precision and are internally consistent.\n\nWe begin by constructing the system components. The dimension is $n=6$. The vectors $\\mu \\in \\mathbb{R}^6$ and $s \\in \\mathbb{R}^6$ are given. The vector $\\mathbf{1}$ is the vector of all ones in $\\mathbb{R}^6$, and $I$ is the $6 \\times 6$ identity matrix.\nThe correlation matrix $R$ is:\n$$ R = (1-0.2)I + 0.2 \\cdot \\mathbf{1}\\mathbf{1}^{\\top} = 0.8 I + 0.2 \\cdot \\mathbf{1}\\mathbf{1}^{\\top} $$\nThe covariance matrix $\\Sigma$ is constructed as:\n$$ \\Sigma = \\operatorname{diag}([0.15, 0.20, 0.25, 0.30, 0.22, 0.18]) \\cdot R \\cdot \\operatorname{diag}([0.15, 0.20, 0.25, 0.30, 0.22, 0.18]) $$\nWith $\\eta = 0.01$, the system matrix $A$ and vector $b$ are:\n$$ A = \\Sigma + 0.01 \\cdot \\mathbf{1}\\mathbf{1}^{\\top} $$\n$$ b = \\mu + 0.01 \\cdot \\mathbf{1} $$\n\nThe Conjugate Gradient algorithm is an iterative method that solves $Ax=b$ by generating a sequence of search directions $\\{p_j\\}$ which are mutually $A$-orthogonal (or conjugate), i.e., $p_i^{\\top} A p_j = 0$ for $i \\ne j$. The standard algorithm proceeds as follows, starting with $x_0 = \\mathbf{0}$, $r_0 = b$, and $p_0 = r_0$:\nFor $j = 0, 1, \\dots, n-1$:\n1. Compute step size: $\\alpha_j = \\frac{r_j^{\\top} r_j}{p_j^{\\top} A p_j}$\n2. Update solution: $x_{j+1} = x_j + \\alpha_j p_j$\n3. Update residual: $r_{j+1} = r_j - \\alpha_j A p_j$\n4. Compute improvement factor: $\\beta_j = \\frac{r_{j+1}^{\\top} r_{j+1}}{r_j^{\\top} r_j}$\n5. Update search direction: $p_{j+1} = r_{j+1} + \\beta_j p_j$\n\nIn exact arithmetic, this process generates an $A$-orthogonal basis of search directions and finds the exact solution in at most $n$ iterations. The core of this problem is to disrupt this property. At a specified iteration $j=k_{\\text{perturb}}$, the search direction $p_j$ is perturbed:\n$$ \\tilde{p}_j = p_j + \\varepsilon \\lVert p_j \\rVert_2 e_1 $$\nwhere $e_1 = [1, 0, \\dots, 0]^{\\top}$. This perturbed direction $\\tilde{p}_j$ is then used in place of $p_j$ for the updates of $x_{j+1}$ and $r_{j+1}$. Crucially, the subsequent search direction $p_{j+1}$ is built using this perturbed direction:\n$$ p_{j+1} = r_{j+1} + \\beta_j \\tilde{p}_j $$\nThis perturbation breaks the chain of conjugacy. The new direction $\\tilde{p}_j$ is generally not $A$-orthogonal to the preceding directions $p_0, \\dots, p_{j-1}$. This error propagates, as all subsequent directions are built upon this contaminated step. As a result, the set of $n$ executed search directions $\\{p_0, \\dots, \\tilde{p}_j, \\dots, p_{n-1}\\}$ is no longer an $A$-orthogonal basis, and the theoretical guarantee of convergence in $n$ steps is lost.\n\nWe expect to observe:\n1. For unperturbed runs (Cases $1$ and $4$), the final residual norm $\\lVert b - A x_n \\rVert_2$ and the $A$-orthogonality defect $\\max_{i \\ne j} | (P^{\\top} A P)_{ij} |$ will be near machine precision (close to $0$). The matrix $P^{\\top} A P$ will be almost perfectly diagonal.\n2. For perturbed runs (Cases $2$, $3$, and $5$), both metrics will be significantly greater than $0$. This demonstrates that the algorithm failed to converge to the exact solution in $n$ steps and that the underlying conjugacy property of the search directions was destroyed. The magnitude of the error will depend on the perturbation strength $\\varepsilon$ and the iteration index $k_{\\text{perturb}}$.\n\nThe solution is implemented by first defining the system matrices from the problem data. A function is then created to execute the CG algorithm. This function incorporates the logic to perturb the specified search direction and stores all used directions. After exactly $n=6$ iterations, it computes the final residual norm and the $A$-orthogonality defect from the matrix of search directions. This process is repeated for each of the five test cases.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Sets up the portfolio optimization problem, runs the Conjugate Gradient (CG)\n    method for several test cases with and without perturbation, and\n    calculates the specified performance metrics.\n    \"\"\"\n    # Problem Constants and Data\n    n = 6\n    rho = 0.2\n    s = np.array([0.15, 0.20, 0.25, 0.30, 0.22, 0.18])\n    mu = np.array([0.08, 0.10, 0.12, 0.15, 0.11, 0.09])\n    eta = 1e-2\n\n    # Construct the linear system Ax = b\n    ones = np.ones((n, 1))\n    R = rho * (ones @ ones.T) + (1 - rho) * np.identity(n)\n    D = np.diag(s)\n    Sigma = D @ R @ D\n    A = Sigma + eta * (ones @ ones.T)\n    b = mu + eta * ones.flatten()\n\n    def run_cg_perturbed(A_mat, b_vec, k_perturb, epsilon, n_iter):\n        \"\"\"\n        Runs the Conjugate Gradient algorithm for n_iter steps with an\n        optional perturbation on a specified search direction.\n        \n        Args:\n            A_mat (np.ndarray): The system matrix (n x n).\n            b_vec (np.ndarray): The right-hand side vector (n,).\n            k_perturb (int): The index of the iteration to perturb.\n            epsilon (float): The magnitude of the perturbation.\n            n_iter (int): The number of iterations to run.\n\n        Returns:\n            tuple: A tuple containing:\n                - residual_norm (float): The L2 norm of the final residual.\n                - defect (float): The A-orthogonality defect.\n        \"\"\"\n        dim = A_mat.shape[0]\n        x = np.zeros(dim)\n        r = b_vec - A_mat @ x\n        p = r\n        rs_old = r.T @ r\n        \n        p_storage = []\n        e1 = np.zeros(dim)\n        e1[0] = 1.0\n\n        for j in range(n_iter):\n            p_effective = p\n            \n            # Apply perturbation if conditions are met\n            if j == k_perturb and epsilon > 0:\n                p_norm = np.linalg.norm(p)\n                perturbation = epsilon * p_norm * e1\n                p_effective = p + perturbation\n\n            p_storage.append(p_effective)\n            \n            Ap = A_mat @ p_effective\n            alpha = rs_old / (p_effective.T @ Ap)\n            \n            x = x + alpha * p_effective\n            r = r - alpha * Ap\n            \n            rs_new = r.T @ r\n            \n            beta = rs_new / rs_old\n            p = r + beta * p_effective\n            rs_old = rs_new\n\n        # 1. Calculate final residual norm\n        final_residual_norm = np.linalg.norm(b_vec - A_mat @ x)\n        \n        # 2. Calculate A-orthogonality defect\n        P = np.array(p_storage).T\n        G = P.T @ A_mat @ P\n        np.fill_diagonal(G, 0.0) # We only care about off-diagonal elements\n        orthogonality_defect = np.max(np.abs(G))\n        \n        return final_residual_norm, orthogonality_defect\n\n    test_cases = [\n        (-1, 0.0),    # Case 1: No perturbation (invalid index)\n        (2, 1.0),     # Case 2: Perturb p_2 with eps=1\n        (0, 1.0),     # Case 3: Perturb p_0 with eps=1\n        (10, 1.0),    # Case 4: No perturbation (index out of bounds)\n        (3, 1e-3),    # Case 5: Perturb p_3 with small eps\n    ]\n\n    results = []\n    for k_perturb, epsilon in test_cases:\n        res_norm, defect = run_cg_perturbed(A, b, k_perturb, epsilon, n)\n        results.append(f\"{res_norm:.8f}\")\n        results.append(f\"{defect:.8f}\")\n        \n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2382914"}]}