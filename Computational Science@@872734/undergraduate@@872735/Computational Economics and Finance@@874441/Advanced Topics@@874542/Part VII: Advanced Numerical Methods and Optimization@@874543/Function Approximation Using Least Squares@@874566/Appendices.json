{"hands_on_practices": [{"introduction": "This first exercise provides a complete hands-on workflow, from modeling to analysis, using the famous Laffer curve as a case study. You will not only fit polynomials to noisy data but also learn to use K-fold cross-validation to select the appropriate model complexity, a crucial skill for avoiding overfitting. This practice [@problem_id:2395010] culminates in using your fitted model to perform an optimization and find the estimated revenue-maximizing tax rate, demonstrating how function approximation directly informs economic policy questions.", "problem": "Consider an economy where the government collects tax revenue as a function of the average tax rate. Let the tax rate be denoted by $t \\in [0,1]$ (expressed as a decimal, not a percentage), and suppose we observe noisy samples of tax revenue at different tax rates generated by a structural form. Your task is to approximate the unknown Laffer curve (tax revenue as a function of the tax rate) by fitting either a quadratic or cubic polynomial using Ordinary Least Squares (OLS), and then use the fitted approximation to compute the revenue-maximizing tax rate on the unit interval.\n\nStart from the following fundamental base:\n- Given sample pairs $\\{(t_i, y_i)\\}_{i=1}^n$, a polynomial of degree $d$ is $p_d(t) = \\sum_{j=0}^{d} \\beta_j t^j$. Ordinary Least Squares (OLS) estimates coefficients by minimizing the sum of squared residuals $\\sum_{i=1}^{n} (y_i - p_d(t_i))^2$.\n- Model selection can be performed by $K$-fold cross-validation that evaluates out-of-sample prediction error and chooses the degree $d$ with the lowest average validation error.\n\nYour program must:\n1. For each test case, simulate data $(t_i, y_i)$ as follows. Draw $n$ independent tax rates $t_i \\sim \\text{Uniform}[0,1]$. Generate noise $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$. Define the true revenue function by $R_{\\text{true}}(t) = A \\cdot t \\cdot (1 - t)^k$. Then set $y_i = R_{\\text{true}}(t_i) + \\varepsilon_i$. All randomness must be seeded deterministically as specified in the test suite.\n2. For each candidate degree $d \\in \\{2,3\\}$, fit $p_d(t)$ by OLS using the simulated data.\n3. Use $K$-fold cross-validation with $K=5$ folds (randomly shuffled using the given seed) to select the degree $d$ with the lowest average validation mean squared error. In case of a tie, prefer the lower degree.\n4. Refit the chosen degree on the full dataset to obtain final coefficients $\\hat{\\beta}_j$.\n5. Compute the estimated revenue-maximizing tax rate $\\hat{t}^\\star$ by maximizing the fitted polynomial $p_d(t)$ over the closed interval $[0,1]$. That is, find all real critical points by solving $p_d'(t)=0$ that lie in $[0,1]$, evaluate $p_d(t)$ at these points and at the endpoints $t=0$ and $t=1$, and set $\\hat{t}^\\star$ equal to the maximizer. Then compute the estimated maximum revenue $\\hat{R}^\\star = p_d(\\hat{t}^\\star)$.\n6. Angles do not appear in this problem. Express all tax rates as decimals in $[0,1]$ and all revenues as real numbers without percentage signs.\n\nUse the following test suite. For each case, use the provided seed for all random operations (both data generation and fold shuffling), draw $n$ points, use noise standard deviation $\\sigma$, and parameters $A$ and $k$ for the data-generating process $R_{\\text{true}}(t) = A \\cdot t \\cdot (1 - t)^k$:\n- Case $1$: seed $42$, $n=40$, $\\sigma=0.005$, $A=1.0$, $k=1$.\n- Case $2$: seed $123$, $n=50$, $\\sigma=0.01$, $A=1.0$, $k=2$.\n- Case $3$: seed $7$, $n=25$, $\\sigma=0.05$, $A=1.0$, $k=2$.\n\nFinal output format:\n- Your program should produce a single line of output containing a Python-style list of three sublists, one per test case, in the same order as above. Each sublist must be of the form $[d,\\ \\hat{t}^\\star,\\ \\hat{R}^\\star]$, where $d$ is the chosen degree (an integer in $\\{2,3\\}$), and $\\hat{t}^\\star$ and $\\hat{R}^\\star$ are rounded to exactly six decimal places. For example: $[[2,0.500000,0.250000],[3,0.333333,0.197531],[2,0.480000,0.210000]]$.", "solution": "The problem is subjected to validation against the specified criteria.\n\n**Step 1: Extract Givens**\n- **Domain**: Tax rate $t \\in [0,1]$.\n- **Data**: Sample pairs $\\{(t_i, y_i)\\}_{i=1}^n$.\n- **Model**: Polynomial of degree $d$, $p_d(t) = \\sum_{j=0}^{d} \\beta_j t^j$.\n- **Estimation Method**: Ordinary Least Squares (OLS) by minimizing the sum of squared residuals $\\sum_{i=1}^{n} (y_i - p_d(t_i))^2$.\n- **Model Selection**: $K$-fold cross-validation with $K=5$. The degree $d$ with the lowest average validation error is chosen. Candidate degrees are $d \\in \\{2,3\\}$. Ties are broken by choosing the lower degree.\n- **Data Generating Process**:\n    - $t_i \\sim \\text{Uniform}[0,1]$.\n    - Noise $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$.\n    - True revenue function $R_{\\text{true}}(t) = A \\cdot t \\cdot (1 - t)^k$.\n    - Observed revenue sample $y_i = R_{\\text{true}}(t_i) + \\varepsilon_i$.\n- **Optimization**: Find the revenue-maximizing tax rate $\\hat{t}^\\star$ by maximizing the fitted polynomial $p_d(t)$ over the interval $[0,1]$. The estimated maximum revenue is $\\hat{R}^\\star = p_d(\\hat{t}^\\star)$.\n- **Test Cases**:\n    - Case $1$: seed=$42$, $n=40$, $\\sigma=0.005$, $A=1.0$, $k=1$.\n    - Case $2$: seed=$123$, $n=50$, $\\sigma=0.01$, $A=1.0$, $k=2$.\n    - Case $3$: seed=$7$, $n=25$, $\\sigma=0.05$, $A=1.0$, $k=2$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Grounding**: The problem is scientifically grounded. It presents a standard task in econometrics and statistics: function approximation from noisy data. The use of polynomial regression, Ordinary Least Squares, and cross-validation for model selection are fundamental and well-established methods. The Laffer curve is a valid concept in economics, and the chosen functional form is a mathematically tractable model.\n- **Well-Posedness**: The problem is well-posed. All necessary parameters, data generation processes, and algorithmic steps are explicitly defined. The objectives are clear and lead to a unique computational result for each test case, given the deterministic seeds.\n- **Objectivity**: The problem statement is objective, using precise mathematical and statistical language, and is free of subjective or ambiguous terminology.\n\n**Step 3: Verdict and Action**\nThe problem is valid. It is a well-defined computational exercise in applied statistics. A solution will be provided.\n\n---\n\nThe problem requires us to approximate an unknown function, representing the tax revenue as a function of the tax rate, from a set of noisy observations. We will use polynomial regression and select the optimal polynomial degree from a candidate set $\\{2, 3\\}$ using $K$-fold cross-validation. Subsequently, we will find the tax rate that maximizes the estimated revenue function over the valid domain $[0,1]$.\n\n**1. Data Generation**\nFor each test case, we are given a seed for the pseudo-random number generator, the number of samples $n$, the noise standard deviation $\\sigma$, and parameters $A$ and $k$ for the true revenue function. The data points $(t_i, y_i)$ for $i=1, \\dots, n$ are generated as follows:\n- The tax rates $t_i$ are drawn from a uniform distribution on the interval $[0, 1]$, i.e., $t_i \\sim \\text{U}[0, 1]$.\n- The corresponding true revenue is calculated using $R_{\\text{true}}(t_i) = A \\cdot t_i \\cdot (1 - t_i)^k$.\n- A noise term $\\varepsilon_i$ is drawn from a normal distribution with mean $0$ and variance $\\sigma^2$, i.e., $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$.\n- The observed revenue is $y_i = R_{\\text{true}}(t_i) + \\varepsilon_i$.\n\n**2. Polynomial Regression via Ordinary Least Squares (OLS)**\nWe seek to approximate the true function with a polynomial of degree $d$, $p_d(t) = \\sum_{j=0}^{d} \\beta_j t^j$. The coefficients $\\boldsymbol{\\beta} = [\\beta_0, \\beta_1, \\dots, \\beta_d]^T$ are estimated by minimizing the sum of squared residuals (SSR):\n$$\n\\text{SSR}(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} (y_i - p_d(t_i))^2\n$$\nThis is a linear least squares problem. Let $\\mathbf{y} = [y_1, \\dots, y_n]^T$ be the vector of observed revenues. Let $\\mathbf{X}$ be the $n \\times (d+1)$ design matrix, which is a Vandermonde matrix for polynomial fitting:\n$$\n\\mathbf{X} =\n\\begin{pmatrix}\n1 & t_1 & t_1^2 & \\dots & t_1^d \\\\\n1 & t_2 & t_2^2 & \\dots & t_2^d \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & t_n & t_n^2 & \\dots & t_n^d\n\\end{pmatrix}\n$$\nThe problem is to minimize $||\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}||_2^2$. The solution $\\hat{\\boldsymbol{\\beta}}$ is given by the normal equations:\n$$\n(\\mathbf{X}^T \\mathbf{X})\\hat{\\boldsymbol{\\beta}} = \\mathbf{X}^T \\mathbf{y}\n$$\nAssuming $\\mathbf{X}^T \\mathbf{X}$ is invertible, the OLS estimate is $\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}$. Computationally, this is solved efficiently using methods like QR decomposition, which is what numerical libraries typically implement.\n\n**3. Model Selection via $K$-Fold Cross-Validation**\nTo choose between a quadratic ($d=2$) and a cubic ($d=3$) model, we use $K$-fold cross-validation with $K=5$. This technique provides an estimate of the model's out-of-sample prediction error, helping to prevent overfitting.\nThe procedure is as follows for each candidate degree $d$:\n1. The dataset of $n$ samples is randomly shuffled and partitioned into $K=5$ disjoint subsets (folds) of nearly equal size.\n2. For each fold $k \\in \\{1, 2, 3, 4, 5\\}$:\n   a. The $k$-th fold is designated as the validation set. The remaining $K-1$ folds are combined to form the training set.\n   b. A polynomial of degree $d$ is fitted using OLS on the training set, yielding coefficients $\\hat{\\boldsymbol{\\beta}}^{(k)}$.\n   c. The mean squared error (MSE) is computed on the validation set: $MSE_k = \\frac{1}{|N_k|} \\sum_{i \\in \\text{fold } k} (y_i - p_d(t_i; \\hat{\\boldsymbol{\\beta}}^{(k)}))^2$, where $|N_k|$ is the number of samples in fold $k$.\n3. The cross-validation score for degree $d$ is the average of these MSEs: $CV(d) = \\frac{1}{K} \\sum_{k=1}^{K} MSE_k$.\nThe optimal degree, $d^\\star$, is chosen to be the one that minimizes this score: $d^\\star = \\arg\\min_{d \\in \\{2,3\\}} CV(d)$. If $CV(2) = CV(3)$, we select the simpler model, $d^\\star=2$.\n\n**4. Optimization of the Chosen Model**\nOnce the optimal degree $d^\\star$ is selected, the polynomial model is re-fitted using the entire dataset to obtain the final coefficient vector $\\hat{\\boldsymbol{\\beta}}$. We denote the final estimated revenue function as $p_{d^\\star}(t)$.\n\nThe next step is to find the tax rate $\\hat{t}^\\star$ that maximizes this function on the closed interval $[0, 1]$. By the Extreme Value Theorem, a continuous function on a closed interval must attain its maximum and minimum. The maximizer must be either at the endpoints of the interval ($t=0$ or $t=1$) or at an interior critical point where the derivative $p'_{d^\\star}(t)$ is zero.\n\nThe derivative of the polynomial is $p'_{d^\\star}(t) = \\sum_{j=1}^{d^\\star} j \\hat{\\beta}_j t^{j-1}$.\n- If $d^\\star=2$, the derivative is $p'_2(t) = \\hat{\\beta}_1 + 2\\hat{\\beta}_2 t$. Setting this to zero gives one critical point: $t_c = -\\frac{\\hat{\\beta}_1}{2\\hat{\\beta}_2}$.\n- If $d^\\star=3$, the derivative is $p'_3(t) = \\hat{\\beta}_1 + 2\\hat{\\beta}_2 t + 3\\hat{\\beta}_3 t^2$. This is a quadratic equation. Its roots can be found using the quadratic formula, yielding up to two real critical points.\n\nThe set of candidate values for the maximizer $\\hat{t}^\\star$ consists of the endpoints $\\{0, 1\\}$ and any real critical points that fall within the interval $[0, 1]$. We evaluate $p_{d^\\star}(t)$ at each of these candidate points. The value of $t$ that yields the highest revenue is the estimated optimal tax rate, $\\hat{t}^\\star$.\n$$\n\\hat{t}^\\star = \\arg\\max_{t \\in \\text{candidates}} p_{d^\\star}(t)\n$$\nThe estimated maximum revenue is then $\\hat{R}^\\star = p_{d^\\star}(\\hat{t}^\\star)$. This procedure is followed for each test case to produce the final results.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Laffer curve approximation problem for a suite of test cases.\n    \"\"\"\n    test_cases = [\n        # (seed, n, sigma, A, k)\n        (42, 40, 0.005, 1.0, 1),\n        (123, 50, 0.01, 1.0, 2),\n        (7, 25, 0.05, 1.0, 2),\n    ]\n\n    all_results = []\n    \n    for seed, n, sigma, A, k_param in test_cases:\n        # Set all sources of randomness for reproducibility\n        rng = np.random.default_rng(seed)\n\n        # 1. Simulate data\n        t = rng.uniform(0, 1, n)\n        true_revenue = A * t * (1 - t)**k_param\n        noise = rng.normal(0, sigma, n)\n        y = true_revenue + noise\n\n        # 2. & 3. Model selection using 5-fold cross-validation\n        K = 5\n        indices = np.arange(n)\n        rng.shuffle(indices)\n        folds = np.array_split(indices, K)\n        \n        candidate_degrees = [2, 3]\n        cv_errors = {}\n\n        for d in candidate_degrees:\n            fold_mses = []\n            for k_fold_idx in range(K):\n                val_indices = folds[k_fold_idx]\n                train_indices = np.concatenate([folds[i] for i in range(K) if i != k_fold_idx])\n\n                t_train, y_train = t[train_indices], y[train_indices]\n                t_val, y_val = t[val_indices], y[val_indices]\n\n                # Fit model on training data\n                X_train = np.vander(t_train, d + 1, increasing=True)\n                coeffs, _, _, _ = np.linalg.lstsq(X_train, y_train, rcond=None)\n\n                # Evaluate on validation data\n                X_val = np.vander(t_val, d + 1, increasing=True)\n                y_pred_val = X_val @ coeffs\n                mse = np.mean((y_val - y_pred_val)**2)\n                fold_mses.append(mse)\n            \n            cv_errors[d] = np.mean(fold_mses)\n\n        # Select the best degree, preferring the lower degree in case of a tie\n        if cv_errors[2] <= cv_errors[3]:\n            d_star = 2\n        else:\n            d_star = 3\n\n        # 4. Refit the chosen model on the full dataset\n        X_full = np.vander(t, d_star + 1, increasing=True)\n        # beta has shape (d_star + 1,) where beta[j] is the coefficient for t^j\n        beta, _, _, _ = np.linalg.lstsq(X_full, y, rcond=None)\n\n        # 5. Compute the estimated revenue-maximizing tax rate\n        \n        # Derivative coefficients: p'(t) = sum_{j=1 to d} j * beta[j] * t^(j-1)\n        # np.roots expects coefficients in descending power order\n        # For p'(t) = c_0*t^(d-1) + ... + c_{d-1}, coeffs are [c_0, ..., c_{d-1}]\n        # c_m = (m+1)*beta[m+1] in our notation.\n        # So for np.roots, we need [(d_star)*beta[d_star], (d_star-1)*beta[d_star-1], ..., 1*beta[1]]\n        deriv_coeffs = [j * beta[j] for j in range(d_star, 0, -1)]\n        critical_points = np.roots(deriv_coeffs)\n        \n        # Candidate points for the maximum on [0, 1]\n        candidate_t = [0.0, 1.0]\n        for root in critical_points:\n            # Filter for real roots within the interval [0, 1]\n            if np.isreal(root) and 0.0 <= root.real <= 1.0:\n                candidate_t.append(root.real)\n        \n        candidate_t = np.unique(candidate_t)\n\n        # Evaluate the polynomial at candidate points to find the maximum\n        # np.polyval expects coefficients in descending power order [beta_d, ..., beta_0]\n        poly_coeffs_desc = beta[::-1]\n        candidate_R = np.polyval(poly_coeffs_desc, candidate_t)\n        \n        max_idx = np.argmax(candidate_R)\n        t_star = candidate_t[max_idx]\n        R_star = candidate_R[max_idx]\n\n        all_results.append(\n            f\"[{d_star},{t_star:.6f},{R_star:.6f}]\"\n        )\n        \n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```", "id": "2395010"}, {"introduction": "In many financial applications, we work with functions that are analytically complex or computationally slow, such as the Black-Scholes option pricing formula. This practice [@problem_id:2394969] introduces a powerful motivation for approximation: replacing a complicated function with a simple polynomial. By doing so, subsequent operations like differentiation become trivial, allowing you to efficiently estimate an option's risk sensitivities (its \"Greeks\"), a fundamental task in financial risk management.", "problem": "You are given the task of approximating the price function of a European call option as a univariate function of the underlying asset level, and then using the approximation to compute sensitivities (\"Greeks\") by differentiation. For a European call with underlying level $S$, strike $K$, continuously compounded risk-free rate $r$ (expressed as a decimal), volatility $\\sigma$ (expressed as a decimal), and time to maturity $T$ (in years), the price under the Black–Scholes model is\n$$\nC(S) \\;=\\; S\\,\\Phi(d_1) \\;-\\; K\\,e^{-r T}\\,\\Phi(d_2),\n$$\nwhere\n$$\nd_1 \\;=\\; \\frac{\\ln(S/K) + \\left(r + \\tfrac{1}{2}\\sigma^2\\right) T}{\\sigma\\sqrt{T}}, \n\\qquad\nd_2 \\;=\\; d_1 \\;-\\; \\sigma\\sqrt{T},\n$$\n$\\Phi(\\cdot)$ is the cumulative distribution function of the standard normal distribution, and $\\ln(\\cdot)$ denotes the natural logarithm. The option Delta and Gamma are, respectively, the first and second partial derivatives of the price with respect to $S$:\n$$\n\\Delta(S) \\;=\\; \\frac{\\partial C}{\\partial S}(S), \n\\qquad\n\\Gamma(S) \\;=\\; \\frac{\\partial^2 C}{\\partial S^2}(S).\n$$\nUnder the Black–Scholes model, the analytical formulas are\n$$\n\\Delta_{\\mathrm{BS}}(S) \\;=\\; \\Phi(d_1), \n\\qquad\n\\Gamma_{\\mathrm{BS}}(S) \\;=\\; \\frac{\\phi(d_1)}{S\\,\\sigma\\,\\sqrt{T}},\n$$\nwhere $\\phi(\\cdot)$ is the standard normal probability density function.\n\nYour task is to, for each specified parameter set, construct a least-squares polynomial approximation of degree $m$ to the pricing function $C(S)$ using sample points on a prescribed interval for $S$, then compute the approximated Delta and Gamma at a specified evaluation point by differentiating the polynomial once and twice with respect to $S$, and finally report the absolute errors of these approximations relative to the Black–Scholes analytical Delta and Gamma.\n\nFor each test case:\n- Construct the equispaced grid $\\{S_i\\}_{i=1}^n$ on the closed interval $[S_{\\min}, S_{\\max}]$ with $n$ points.\n- Evaluate the exact price $y_i = C(S_i)$ at each grid point using the Black–Scholes formula above.\n- Let $\\mathcal{P}_m$ denote the set of polynomials in $S$ of degree at most $m$. Compute the least-squares polynomial approximant $\\hat{C}_m \\in \\mathcal{P}_m$ that minimizes $\\sum_{i=1}^n \\left(y_i - \\hat{C}_m(S_i)\\right)^2$ over the coefficients of $\\hat{C}_m$.\n- Differentiate $\\hat{C}_m$ with respect to $S$ to obtain $\\widehat{\\Delta}_m(S) = \\frac{\\mathrm{d}}{\\mathrm{d}S}\\hat{C}_m(S)$ and $\\widehat{\\Gamma}_m(S) = \\frac{\\mathrm{d}^2}{\\mathrm{d}S^2}\\hat{C}_m(S)$.\n- At the evaluation point $S_0$, compute the absolute errors $|\\widehat{\\Delta}_m(S_0) - \\Delta_{\\mathrm{BS}}(S_0)|$ and $|\\widehat{\\Gamma}_m(S_0) - \\Gamma_{\\mathrm{BS}}(S_0)|$.\n\nDesign details:\n- All rates and volatilities must be treated as decimals (for example, an annualized rate of $2\\%$ is entered and used as $0.02$).\n- No physical units are involved. There are no angles in this problem. Do not express any answers using a percentage sign.\n- The final output for the program must be a single line containing a comma-separated list of results enclosed in square brackets. Each test case contributes one inner list of two floats, in the order $[\\text{DeltaError}, \\text{GammaError}]$, each rounded to six decimal places. For example, an output with two test cases should look like $[[0.000123,0.045678],[0.000010,0.000200]]$.\n\nTest suite (each bullet specifies $(K, r, \\sigma, T, S_{\\min}, S_{\\max}, n, m, S_0)$):\n- Case A: $(100,\\, 0.02,\\, 0.2,\\, 0.5,\\, 50,\\, 150,\\, 101,\\, 5,\\, 100)$.\n- Case B: $(100,\\, 0.01,\\, 0.25,\\, 1.0,\\, 80,\\, 200,\\, 121,\\, 6,\\, 180)$.\n- Case C: $(100,\\, 0.03,\\, 0.05,\\, 1.0,\\, 80,\\, 120,\\, 81,\\, 5,\\, 100)$.\n- Case D: $(100,\\, 0.00,\\, 0.3,\\, 0.01,\\, 80,\\, 120,\\, 81,\\, 5,\\, 100)$.\n- Case E: $(120,\\, 0.02,\\, 0.2,\\, 1.0,\\, 60,\\, 120,\\, 121,\\, 5,\\, 60)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the inner list $[\\text{DeltaError}, \\text{GammaError}]$ for the corresponding test case in the order A through E, with each float rounded to six decimal places.", "solution": "The problem as stated has been subjected to rigorous validation. It requires the numerical approximation of a European call option's price function and its first two derivatives—Delta and Gamma—using a polynomial least-squares fit. The theoretical underpinnings are the Black-Scholes model, a foundational concept in mathematical finance, and the method of least squares, a standard tool in numerical analysis. All parameters, constraints, and objectives are specified with sufficient clarity and precision. The test cases provided are well within the domain of validity for the model and the numerical methods. No scientific or logical contradictions, ambiguities, or missing information have been identified. The problem is therefore deemed to be valid, well-posed, and scientifically grounded. We may proceed to construct the solution.\n\nThe objective is to approximate the Black-Scholes price function $C(S)$ for a European call option as a polynomial in the underlying asset price $S$. The price is given by\n$$\nC(S) = S\\,\\Phi(d_1) - K\\,e^{-r T}\\,\\Phi(d_2)\n$$\nwhere $K$ is the strike price, $r$ is the risk-free rate, $T$ is the time to maturity, $\\sigma$ is the volatility, and $\\Phi(\\cdot)$ is the cumulative distribution function (CDF) of the standard normal distribution. The terms $d_1$ and $d_2$ are defined as\n$$\nd_1 = \\frac{\\ln(S/K) + \\left(r + \\frac{1}{2}\\sigma^2\\right) T}{\\sigma\\sqrt{T}}, \\quad d_2 = d_1 - \\sigma\\sqrt{T}\n$$\nThe problem requires us to generate a set of $n$ sample points $(S_i, y_i)$ where the grid $\\{S_i\\}_{i=1}^n$ is equispaced on the interval $[S_{\\min}, S_{\\max}]$, and $y_i = C(S_i)$. We then seek a polynomial $\\hat{C}_m(S)$ of degree at most $m$,\n$$\n\\hat{C}_m(S) = \\sum_{j=0}^{m} p_j S^{m-j}\n$$\nthat minimizes the sum of squared residuals, $L = \\sum_{i=1}^n (y_i - \\hat{C}_m(S_i))^2$. This is a classical linear least-squares problem. The vector of coefficients $\\mathbf{p} = [p_0, p_1, \\dots, p_m]^T$ can be found by solving the normal equations $(\\mathbf{X}^T \\mathbf{X})\\mathbf{p} = \\mathbf{X}^T \\mathbf{y}$, where $\\mathbf{y}$ is the vector of prices $y_i$ and $\\mathbf{X}$ is the design matrix with entries $X_{ij} = S_i^{m-j}$. Numerically-stable algorithms, such as those based on QR decomposition, are typically employed to solve this system.\n\nOnce the coefficients of the approximating polynomial $\\hat{C}_m(S)$ are determined, we can readily compute its derivatives to approximate the option's sensitivities, known as \"Greeks.\" The Delta, $\\Delta = \\frac{\\partial C}{\\partial S}$, is the first derivative with respect to $S$, and the Gamma, $\\Gamma = \\frac{\\partial^2 C}{\\partial S^2}$, is the second derivative. The polynomial approximations for these are:\n$$\n\\widehat{\\Delta}_m(S) = \\frac{\\mathrm{d}}{\\mathrm{d}S}\\hat{C}_m(S) = \\sum_{j=0}^{m-1} (m-j) p_j S^{m-j-1}\n$$\n$$\n\\widehat{\\Gamma}_m(S) = \\frac{\\mathrm{d}^2}{\\mathrm{d}S^2}\\hat{C}_m(S) = \\sum_{j=0}^{m-2} (m-j)(m-j-1) p_j S^{m-j-2}\n$$\nThese approximate Greeks are then evaluated at a specified point $S_0$.\n\nThe accuracy of these approximations is assessed by comparing them against the analytical formulas for Delta and Gamma derived from the Black-Scholes model:\n$$\n\\Delta_{\\mathrm{BS}}(S) = \\Phi(d_1)\n$$\n$$\n\\Gamma_{\\mathrm{BS}}(S) = \\frac{\\phi(d_1)}{S\\,\\sigma\\,\\sqrt{T}}\n$$\nwhere $\\phi(\\cdot)$ is the probability density function (PDF) of the standard normal distribution. The absolute errors are calculated at the evaluation point $S_0$:\n$$\n\\text{DeltaError} = |\\widehat{\\Delta}_m(S_0) - \\Delta_{\\mathrm{BS}}(S_0)|\n$$\n$$\n\\text{GammaError} = |\\widehat{\\Gamma}_m(S_0) - \\Gamma_{\\mathrm{BS}}(S_0)|\n$$\nThe computational procedure for each test case involves these steps: generating the grid of asset prices, calculating the exact option prices at these points, fitting the polynomial, differentiating it, evaluating the approximate and analytical Greeks at $S_0$, and finally computing the absolute errors. Standard numerical libraries will be used for the normal distribution functions and for the polynomial fitting and differentiation, ensuring both accuracy and numerical stability.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\n# language: Python\n# version: 3.12\n# libraries:\n#     - name: numpy\n#       version: 1.23.5\n#     - name: scipy\n#       version: 1.11.4\n\ndef black_scholes_call(S, K, T, r, sigma):\n    \"\"\"\n    Calculates the Black-Scholes price for a European call option.\n    Note: S can be a numpy array.\n    \"\"\"\n    # Ensure S is a float array to avoid potential type issues\n    S = np.asarray(S, dtype=float)\n    \n    # Handle the case where S is very close to zero\n    S[S < 1e-9] = 1e-9\n\n    d1 = (np.log(S / K) + (r + 0.5 * sigma**2) * T) / (sigma * np.sqrt(T))\n    d2 = d1 - sigma * np.sqrt(T)\n    price = S * norm.cdf(d1) - K * np.exp(-r * T) * norm.cdf(d2)\n    return price\n\ndef black_scholes_delta(S, K, T, r, sigma):\n    \"\"\"\n    Calculates the analytical Black-Scholes Delta for a European call option.\n    \"\"\"\n    S = float(S)\n    if S < 1e-9:\n        S = 1e-9\n    d1 = (np.log(S / K) + (r + 0.5 * sigma**2) * T) / (sigma * np.sqrt(T))\n    return norm.cdf(d1)\n\ndef black_scholes_gamma(S, K, T, r, sigma):\n    \"\"\"\n    Calculates the analytical Black-Scholes Gamma for a European call option.\n    \"\"\"\n    S = float(S)\n    if S < 1e-9:\n        S = 1e-9\n    d1 = (np.log(S / K) + (r + 0.5 * sigma**2) * T) / (sigma * np.sqrt(T))\n    pdf_d1 = norm.pdf(d1)\n    return pdf_d1 / (S * sigma * np.sqrt(T))\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and generate the final output.\n    \"\"\"\n    # Test cases: (K, r, sigma, T, S_min, S_max, n, m, S0)\n    test_cases = [\n        (100, 0.02, 0.2, 0.5, 50, 150, 101, 5, 100),\n        (100, 0.01, 0.25, 1.0, 80, 200, 121, 6, 180),\n        (100, 0.03, 0.05, 1.0, 80, 120, 81, 5, 100),\n        (100, 0.00, 0.3, 0.01, 80, 120, 81, 5, 100),\n        (120, 0.02, 0.2, 1.0, 60, 120, 121, 5, 60),\n    ]\n\n    results = []\n    for case in test_cases:\n        K, r, sigma, T, S_min, S_max, n, m, S0 = case\n\n        # 1. Construct the grid and evaluate exact prices\n        S_grid = np.linspace(S_min, S_max, n)\n        y_grid = black_scholes_call(S_grid, K, T, r, sigma)\n\n        # 2. Compute the least-squares polynomial approximant\n        # np.polyfit returns coefficients in descending order of power\n        coeffs = np.polyfit(S_grid, y_grid, m)\n        \n        # 3. Differentiate the polynomial to get Greeks approximations\n        # np.poly1d creates a polynomial object from coefficients\n        C_poly = np.poly1d(coeffs)\n        Delta_poly = C_poly.deriv(1)\n        Gamma_poly = C_poly.deriv(2)\n\n        # 4. Evaluate approximated Greeks at the evaluation point S0\n        delta_approx = Delta_poly(S0)\n        gamma_approx = Gamma_poly(S0)\n\n        # 5. Evaluate analytical Greeks at S0\n        delta_exact = black_scholes_delta(S0, K, T, r, sigma)\n        gamma_exact = black_scholes_gamma(S0, K, T, r, sigma)\n\n        # 6. Compute absolute errors\n        delta_error = abs(delta_approx - delta_exact)\n        gamma_error = abs(gamma_approx - gamma_exact)\n        \n        results.append([delta_error, gamma_error])\n\n    # Final print statement in the exact required format.\n    formatted_results = [f'[{d_err:.6f},{g_err:.6f}]' for d_err, g_err in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2394969"}, {"introduction": "Our final practice ventures into the heart of computational economics, where function approximation becomes an essential tool for solving models that lack analytical solutions. Here, you will tackle a Cournot competition model by approximating the firms' complex best-response functions with polynomials. This exercise [@problem_id:2394952] demonstrates how these approximations can then be used within an iterative algorithm to find the market's approximate Nash equilibrium, highlighting a sophisticated technique used at the frontier of economic modeling.", "problem": "Consider a static Cournot competition with a finite number of firms. Each firm chooses a nonnegative quantity to maximize its profit taking the quantities of other firms as given. Let the inverse demand function be given by $P(Q) = \\alpha - \\beta Q + \\dfrac{\\gamma}{1 + Q^2}$ where $Q = \\sum_{i=1}^{N} q_i$ is the aggregate quantity, $q_i \\ge 0$ is firm $i$'s quantity, and $\\alpha, \\beta, \\gamma$ are parameters. Firm $i$ has a cost function $C_i(q_i) = c_i q_i + \\dfrac{\\delta}{2} q_i^2$ with parameters $c_i$ and $\\delta > 0$. The profit of firm $i$ is $\\pi_i(q_i, q_{-i}) = \\big(P(Q) - c_i\\big) q_i - \\dfrac{\\delta}{2} q_i^2$. The first-order condition for an interior best response of firm $i$ given the sum of competitors' outputs $S_i = \\sum_{j \\ne i} q_j$ is\n$$F_i(q; S_i) \\equiv P(S_i + q) + q \\cdot P'(S_i + q) - c_i - \\delta q = 0,$$\nwith $q \\ge 0$. Assume that $\\delta$ and $\\beta$ are such that a unique nonnegative best response exists for each $S_i \\ge 0$.\n\nYou are asked to approximate the vector of firms' best-response functions $\\{BR_i(S)\\}_{i=1}^N$ by polynomials using least squares, and then to compute a fixed point of the approximate best-response mapping. The design is as follows, grounded in fundamental optimization and least squares approximation principles:\n\n- Fundamental base:\n  - Cournot best response satisfies the first-order condition $F_i(q; S_i) = 0$ for an interior optimum, combined with the nonnegativity constraint $q \\ge 0$. For the given $P(Q)$, $P'(Q)$ and $P''(Q)$ are well-defined and continuous.\n  - Least squares approximation fits a function $\\hat{f}(x)$ from a chosen linear basis by minimizing the sum of squared residuals between observed targets and model predictions.\n\n- Algorithm to implement:\n  1. For each firm $i$, choose a polynomial basis $\\{\\phi_k(S)\\}_{k=0}^K$ with degree at most $K = 3$, i.e., $\\phi_0(S) = 1$, $\\phi_1(S) = S$, $\\phi_2(S) = S^2$, $\\phi_3(S) = S^3$.\n  2. Construct a training grid of $M = 200$ evenly spaced values for $S$ on $[0, S_{\\max}]$, where\n     $$S_{\\max} = N \\cdot \\max\\Big\\{0, \\frac{\\alpha - \\min_i c_i}{\\beta (N + 1) + \\delta} \\Big\\} + \\kappa,$$\n     with $\\kappa = 5$. This $S_{\\max}$ is a conservative bound motivated by a linear-demand symmetric benchmark, inflated by a positive margin $\\kappa$ to cover nonlinear effects. All mathematical symbols used here were previously defined.\n  3. For each $S$ on the grid and each firm $i$, compute the exact best response $q_i^{\\star}(S)$ by solving $F_i(q; S) = 0$ for $q \\ge 0$. If $F_i(0; S) \\le 0$, set $q_i^{\\star}(S) = 0$; otherwise, find the unique root in $(0, \\infty)$ using a robust bracketing method.\n  4. Fit the polynomial coefficients $\\theta_i \\in \\mathbb{R}^{K+1}$ by ridge-regularized least squares (with ridge parameter $\\lambda = 10^{-8}$) to minimize $\\sum_{m=1}^M \\big(q_i^{\\star}(S_m) - \\sum_{k=0}^K \\theta_{i,k} \\phi_k(S_m)\\big)^2 + \\lambda \\|\\theta_i\\|_2^2$. Denote the fitted approximation by $\\widehat{BR}_i(S) = \\sum_{k=0}^K \\theta_{i,k} \\phi_k(S)$.\n  5. Compute an approximate Nash equilibrium by iterating the approximate best responses as a fixed-point iteration on quantities. Initialize $q^{(0)} = \\mathbf{0} \\in \\mathbb{R}^N$. For iteration $t = 0, 1, 2, \\dots$, compute for each $i$:\n     - $S_i^{(t)} = \\sum_{j \\ne i} q_j^{(t)}$,\n     - raw update $\\tilde{q}_i^{(t+1)} = \\max\\{0, \\widehat{BR}_i(S_i^{(t)})\\}$,\n     - damped update $q_i^{(t+1)} = (1 - \\omega) q_i^{(t)} + \\omega \\tilde{q}_i^{(t+1)}$ with damping weight $\\omega = 0.7$.\n     Stop when $\\|q^{(t+1)} - q^{(t)}\\|_{\\infty} \\le \\text{tol}$ with tolerance $\\text{tol} = 10^{-10}$ or when a maximum of $T_{\\max} = 10000$ iterations is reached. Report $Q^{\\star} = \\sum_{i=1}^N q_i^{(t)}$ at termination.\n  6. All computations must be done in floating-point arithmetic. There are no physical units or angles in this problem.\n\nImplement a program that follows the above design exactly and produces the approximate equilibrium aggregate quantity $Q^{\\star}$ for each parameter set in the test suite below. For each test case, the number of firms $N$, and the parameter arrays are specified. The inverse demand parameters are $(\\alpha,\\beta,\\gamma)$ and cost parameters are $\\delta$ and $(c_1,\\dots,c_N)$.\n\nTest suite:\n- Case A (symmetric duopoly, linear demand):\n  - $N = 2$,\n  - $(\\alpha, \\beta, \\gamma) = (10, 1, 0)$,\n  - $\\delta = 1$,\n  - $(c_1, c_2) = (2, 2)$.\n- Case B (symmetric triopoly, mildly nonlinear demand):\n  - $N = 3$,\n  - $(\\alpha, \\beta, \\gamma) = (12, 1.2, 2.5)$,\n  - $\\delta = 0.8$,\n  - $(c_1, c_2, c_3) = (1.5, 1.5, 1.5)$.\n- Case C (asymmetric duopoly, nonlinear demand):\n  - $N = 2$,\n  - $(\\alpha, \\beta, \\gamma) = (8, 1.5, 1)$,\n  - $\\delta = 2.5$,\n  - $(c_1, c_2) = (2, 3)$.\n- Case D (symmetric triopoly, linear demand with high curvature in cost):\n  - $N = 3$,\n  - $(\\alpha, \\beta, \\gamma) = (6, 2, 0)$,\n  - $\\delta = 3$,\n  - $(c_1, c_2, c_3) = (1, 1, 1)$.\n\nProgram output specification:\n- For each case, compute the approximate equilibrium aggregate quantity $Q^{\\star}$ as described.\n- The program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order [Case A, Case B, Case C, Case D]. Each entry must be a floating-point number rounded to exactly six digits after the decimal point.", "solution": "The problem statement is evaluated for validity prior to attempting a solution.\n\n### Step 1: Extract Givens\n\n- **Model**: Static Cournot competition with $N$ firms.\n- **Inverse Demand Function**: $P(Q) = \\alpha - \\beta Q + \\dfrac{\\gamma}{1 + Q^2}$, where $Q = \\sum_{i=1}^{N} q_i$.\n- **Cost Function for Firm $i$**: $C_i(q_i) = c_i q_i + \\dfrac{\\delta}{2} q_i^2$, with $\\delta > 0$.\n- **Profit Function for Firm $i$**: $\\pi_i(q_i, q_{-i}) = \\big(P(Q) - c_i\\big) q_i - \\dfrac{\\delta}{2} q_i^2$.\n- **First-Order Condition (FOC)**: $F_i(q; S_i) \\equiv P(S_i + q) + q \\cdot P'(S_i + q) - c_i - \\delta q = 0$ for an interior solution $q > 0$, where $S_i = \\sum_{j \\ne i} q_j$.\n- **Assumption**: A unique non-negative best response exists for each $S_i \\ge 0$.\n\n- **Algorithm for Approximation and Equilibrium Computation**:\n    1.  **Basis Functions**: Polynomials up to degree $K=3$: $\\{\\phi_k(S) = S^k\\}_{k=0}^3$.\n    2.  **Training Grid**: $M=200$ evenly spaced points for $S$ on $[0, S_{\\max}]$.\n    3.  **Grid Upper Bound**: $S_{\\max} = N \\cdot \\max\\Big\\{0, \\frac{\\alpha - \\min_i c_i}{\\beta (N + 1) + \\delta} \\Big\\} + \\kappa$, with $\\kappa = 5$.\n    4.  **Best Response Calculation**: For each $S$ on the grid, solve for $q_i^{\\star}(S)$. If $F_i(0; S) \\le 0$, then $q_i^{\\star}(S) = 0$. Otherwise, solve $F_i(q; S) = 0$ for $q > 0$.\n    5.  **Approximation Method**: Ridge-regularized least squares with ridge parameter $\\lambda = 10^{-8}$ to find coefficients $\\theta_i$ for the approximate best-response function $\\widehat{BR}_i(S) = \\sum_{k=0}^K \\theta_{i,k} \\phi_k(S)$.\n    6.  **Fixed-Point Iteration**:\n        -   Initialization: $q^{(0)} = \\mathbf{0}$.\n        -   Update Rule: $q_i^{(t+1)} = (1 - \\omega) q_i^{(t)} + \\omega \\cdot \\max\\{0, \\widehat{BR}_i(\\sum_{j \\ne i} q_j^{(t)})\\}$ with damping $\\omega = 0.7$.\n        -   Termination: $\\|q^{(t+1)} - q^{(t)}\\|_{\\infty} \\le \\text{tol} = 10^{-10}$ or $T_{\\max} = 10000$ iterations.\n    7.  **Final Output**: $Q^{\\star} = \\sum_{i=1}^N q_i^{(t)}$ at termination.\n\n- **Test Suite**: Four cases (A, B, C, D) are specified with parameters for $N$, $(\\alpha, \\beta, \\gamma)$, $\\delta$, and $(c_1, \\dots, c_N)$.\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem is subjected to a critical review based on the extracted information.\n\n-   **Scientifically Grounded**: The problem is based on the Cournot model of oligopoly, a cornerstone of microeconomic theory. The functions for demand and cost are standard forms. The use of first-order conditions for optimization, least squares for function approximation, and fixed-point iteration for equilibrium finding are all fundamental and well-established methods in economics and computational science. The problem is scientifically sound.\n-   **Well-Posed**: The problem describes a particular numerical algorithm and asks for its implementation and output. Each step of the algorithm is precisely defined, from the construction of the training grid to the parameters of the iterative solver. The assumption of a unique best response simplifies the underlying economic model, ensuring that the target function for approximation is well-defined. The computational task is therefore well-posed.\n-   **Objective**: The problem is stated using precise mathematical language and algorithmic specifications. It is free of subjective claims, ambiguity, or opinion.\n\nThe problem does not exhibit any of the flaws listed in the validation criteria. It is a standard, formalizable problem in computational economics.\n\n### Step 3: Verdict and Action\n\nThe problem statement is **valid**. A solution will be constructed according to the specified algorithm.\n\n### Solution Derivation\n\nThe process involves three main stages: generating the true best-response data, approximating these data with polynomials, and then finding the fixed point of the system of approximate best-response functions.\n\n**1. Best-Response Function Characterization**\n\nThe best response of firm $i$ is the quantity $q_i$ that maximizes its profit, given the total quantity of its competitors, $S_i$. The first-order condition for an interior maximum is $F_i(q; S_i) = 0$. We must first write this function explicitly.\nThe inverse demand is $P(Q) = \\alpha - \\beta Q + \\gamma(1 + Q^2)^{-1}$.\nIts derivative is $P'(Q) = -\\beta - 2\\gamma Q(1 + Q^2)^{-2}$.\nSubstituting these into the FOC, where $Q = S_i + q$:\n$$F_i(q; S_i) = \\left( \\alpha - \\beta(S_i+q) + \\frac{\\gamma}{1+(S_i+q)^2} \\right) + q \\left( -\\beta - \\frac{2\\gamma(S_i+q)}{(1+(S_i+q)^2)^2} \\right) - c_i - \\delta q = 0$$\nThis can be simplified to:\n$$F_i(q; S_i) = (\\alpha - c_i - \\beta S_i) - (\\delta + 2\\beta)q + \\frac{\\gamma}{1+(S_i+q)^2} - \\frac{2\\gamma q(S_i+q)}{(1+(S_i+q)^2)^2} = 0$$\nThe best response $q_i^{\\star}(S_i)$ is the solution to this equation. A corner solution $q_i^{\\star}(S_i)=0$ occurs if the marginal profit at zero output is non-positive. This condition is checked by evaluating $F_i(0; S_i)$:\n$$F_i(0; S_i) = P(S_i) - c_i = \\alpha - \\beta S_i + \\frac{\\gamma}{1+S_i^2} - c_i$$\nIf $F_i(0; S_i) \\le 0$, firm $i$ produces nothing. Otherwise, a positive quantity $q > 0$ is produced, and it must satisfy $F_i(q; S_i) = 0$. The problem guarantees a unique such solution, which will be found using a numerical root-finding algorithm.\n\n**2. Polynomial Approximation**\n\nWe approximate the true (but computationally expensive) best-response function $q_i^{\\star}(S_i)$ with a polynomial $\\widehat{BR}_i(S_i)$ of degree $K=3$.\n$$\\widehat{BR}_i(S) = \\sum_{k=0}^{3} \\theta_{i,k} S^k = \\theta_{i,0} + \\theta_{i,1} S + \\theta_{i,2} S^2 + \\theta_{i,3} S^3$$\nThe coefficients $\\theta_i = [\\theta_{i,0}, \\theta_{i,1}, \\theta_{i,2}, \\theta_{i,3}]^T$ are determined by minimizing the sum of squared errors on a training set, with ridge regularization.\n\nFirst, we generate the training data. A grid of $M=200$ points $\\{S_m\\}_{m=1}^M$ is created over the interval $[0, S_{\\max}]$, where $S_{\\max}$ is a conservatively estimated upper bound for the relevant range of competitor outputs. For each $S_m$ and each firm $i$, we compute the corresponding \"true\" best response $y_{i,m} = q_i^{\\star}(S_m)$ by solving the FOC as described above.\n\nSecond, we set up the least squares problem. Let $y_i$ be the vector of $M$ target values $\\{y_{i,m}\\}$. Let $X$ be the $M \\times (K+1)$ design matrix, where $X_{mk} = (S_m)^k$ for $m=1, \\dots, M$ and $k=0, \\dots, K$. The ridge regression problem is to find $\\theta_i$ that minimizes:\n$$L(\\theta_i) = \\| y_i - X \\theta_i \\|_2^2 + \\lambda \\| \\theta_i \\|_2^2$$\nThe solution to this minimization problem is given by the normal equations:\n$$\\theta_i = (X^T X + \\lambda I)^{-1} X^T y_i$$\nwhere $I$ is the $(K+1) \\times (K+1)$ identity matrix and $\\lambda = 10^{-8}$. This procedure is repeated for each firm $i=1, \\dots, N$.\n\n**3. Fixed-Point Iteration for Nash Equilibrium**\n\nA Nash equilibrium is a profile of quantities $q^{\\star} = (q_1^{\\star}, \\dots, q_N^{\\star})$ such that for every firm $i$, $q_i^{\\star}$ is a best response to the other firms' quantities $S_i^{\\star} = \\sum_{j \\ne i} q_j^{\\star}$. This is a fixed point of the best-response mapping. We seek an *approximate* Nash equilibrium by finding a fixed point of the system of *approximate* best-response functions $\\{\\widehat{BR}_i\\}$.\n\nWe employ a damped fixed-point iteration (also known as a Jacobi iteration with successive over-relaxation, though here it is under-relaxation as $\\omega < 1$):\n- Start with an initial guess, $q^{(0)} = \\mathbf{0}$.\n- For each iteration $t=0, 1, \\dots$:\n    - For each firm $i=1, \\dots, N$, compute the sum of other firms' current quantities: $S_i^{(t)} = \\sum_{j \\ne i} q_j^{(t)}$.\n    - Compute a \"raw\" update based on the approximate best response: $\\tilde{q}_i^{(t+1)} = \\max\\{0, \\widehat{BR}_i(S_i^{(t)})\\}$. The $\\max\\{0, \\cdot\\}$ operation enforces the non-negativity constraint on quantities.\n    - Apply damping to get the final update for this iteration: $q_i^{(t+1)} = (1 - \\omega) q_i^{(t)} + \\omega \\tilde{q}_i^{(t+1)}$.\n- This process is repeated until the change between successive iterates is sufficiently small, i.e., $\\max_i |q_i^{(t+1)} - q_i^{(t)}| \\le \\text{tol}$, or a maximum number of iterations $T_{\\max}$ is reached.\n\nThe final result for each test case is the aggregate equilibrium quantity, $Q^{\\star} = \\sum_{i=1}^N q_i$, where $q_i$ are the components of the converged quantity vector. The implementation will follow these steps precisely for each provided test case.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import brentq\n\ndef solve():\n    \"\"\"\n    Implements the full algorithm to find the approximate Cournot-Nash equilibrium\n    for a set of test cases.\n    \"\"\"\n    \n    # --- Algorithm parameters from the problem statement ---\n    K = 3  # Degree of polynomial approximation\n    M = 200  # Number of grid points for S\n    KAPPA = 5.0  # Buffer for S_max calculation\n    LAMBDA = 1e-8  # Ridge regression parameter\n    OMEGA = 0.7  # Damping weight for fixed-point iteration\n    TOL = 1e-10  # Convergence tolerance\n    T_MAX = 10000 # Maximum number of iterations\n\n    # --- Test suite from the problem statement ---\n    test_cases = [\n        {\n            \"name\": \"Case A\",\n            \"N\": 2,\n            \"demand_params\": (10.0, 1.0, 0.0),\n            \"cost_params\": {\"delta\": 1.0, \"c\": np.array([2.0, 2.0])},\n        },\n        {\n            \"name\": \"Case B\",\n            \"N\": 3,\n            \"demand_params\": (12.0, 1.2, 2.5),\n            \"cost_params\": {\"delta\": 0.8, \"c\": np.array([1.5, 1.5, 1.5])},\n        },\n        {\n            \"name\": \"Case C\",\n            \"N\": 2,\n            \"demand_params\": (8.0, 1.5, 1.0),\n            \"cost_params\": {\"delta\": 2.5, \"c\": np.array([2.0, 3.0])},\n        },\n        {\n            \"name\": \"Case D\",\n            \"N\": 3,\n            \"demand_params\": (6.0, 2.0, 0.0),\n            \"cost_params\": {\"delta\": 3.0, \"c\": np.array([1.0, 1.0, 1.0])},\n        },\n    ]\n\n    results = []\n\n    for case in test_cases:\n        N = case[\"N\"]\n        alpha, beta, gamma = case[\"demand_params\"]\n        delta = case[\"cost_params\"][\"delta\"]\n        c_vec = case[\"cost_params\"][\"c\"]\n\n        # --- Define model functions ---\n        def P(Q):\n            return alpha - beta * Q + gamma / (1.0 + Q**2)\n\n        def P_prime(Q):\n            return -beta - (2.0 * gamma * Q) / (1.0 + Q**2)**2\n\n        def F_i(q, S, c_i, delta_local):\n            Q = S + q\n            return P(Q) + q * P_prime(Q) - c_i - delta_local * q\n\n        # --- Stage 1: Function Approximation ---\n\n        # 1.1. Construct training grid for S\n        # Denominator should not be non-positive; beta and delta are positive.\n        s_max_frac_num = alpha - np.min(c_vec)\n        s_max_frac_den = beta * (N + 1) + delta\n        \n        # Guard against non-positive denominator, though problem constraints prevent it\n        if s_max_frac_den <= 0:\n            s_max_base = 0\n        else:\n            s_max_base = N * max(0.0, s_max_frac_num / s_max_frac_den)\n\n        S_max = s_max_base + KAPPA\n        S_grid = np.linspace(0, S_max, M)\n\n        all_coeffs = []\n        for i in range(N):\n            c_i = c_vec[i]\n\n            # 1.2. Compute exact best response q_i*(S) on the grid\n            q_star_values = np.zeros(M)\n            for j, S_val in enumerate(S_grid):\n                # Check for corner solution\n                if F_i(0, S_val, c_i, delta) <= 0:\n                    q_star_values[j] = 0.0\n                else:\n                    # Find root for interior solution\n                    # The bracket [0, S_max] is robust for these problems.\n                    # A small epsilon is added to the lower bound to avoid issues if F(0) is numerically zero\n                    # brentq requires the function values at the endpoints to have different signs.\n                    # We know F(0) > 0. F(q) -> -inf for large q.\n                    upper_bound = S_max\n                    try:\n                        q_star_values[j] = brentq(F_i, 0, upper_bound, args=(S_val, c_i, delta))\n                    except ValueError:\n                        # If S_max is not a sufficient upper bound, extend it.\n                        while F_i(upper_bound, S_val, c_i, delta) > 0:\n                            upper_bound *= 2\n                        q_star_values[j] = brentq(F_i, 0, upper_bound, args=(S_val, c_i, delta))\n\n\n            # 1.3. Fit polynomial via Ridge-regularized Least Squares\n            X = np.vander(S_grid, K + 1, increasing=True)\n            y = q_star_values\n            \n            # Solve (X.T @ X + lambda * I) @ theta = X.T @ y\n            lhs = X.T @ X + LAMBDA * np.identity(K + 1)\n            rhs = X.T @ y\n            coeffs = np.linalg.solve(lhs, rhs)\n            all_coeffs.append(coeffs)\n\n        # --- Stage 2: Fixed-Point Iteration ---\n        q_current = np.zeros(N)\n        for t in range(T_MAX):\n            q_previous = q_current.copy()\n            \n            total_q = np.sum(q_current)\n            \n            # This is a vectorized way to compute S_i for all i\n            S_vec = total_q - q_current\n            \n            q_tilde = np.zeros(N)\n            for i in range(N):\n                poly_coeffs = all_coeffs[i]\n                # np.polynomial.polynomial.polyval expects coeffs [c0, c1, c2, ...]\n                br_val = np.polynomial.polynomial.polyval(S_vec[i], poly_coeffs)\n                q_tilde[i] = max(0.0, br_val)\n\n            # Damped update\n            q_current = (1.0 - OMEGA) * q_previous + OMEGA * q_tilde\n\n            # Check for convergence\n            if np.max(np.abs(q_current - q_previous)) < TOL:\n                break\n        \n        Q_star = np.sum(q_current)\n        results.append(Q_star)\n\n    # --- Final Output Formatting ---\n    print(f\"[{','.join(f'{q:.6f}' for q in results)}]\")\n\nsolve()\n```", "id": "2394952"}]}