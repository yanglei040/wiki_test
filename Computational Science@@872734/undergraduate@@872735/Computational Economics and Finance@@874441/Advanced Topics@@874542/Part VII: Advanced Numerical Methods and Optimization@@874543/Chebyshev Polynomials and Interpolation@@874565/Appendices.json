{"hands_on_practices": [{"introduction": "To begin, let's explore the fundamental mechanism of Chebyshev interpolation with a direct calculation. While polynomial interpolation over equally spaced points can lead to large errors (Runge's phenomenon), choosing nodes at the roots of Chebyshev polynomials provides a remarkably accurate approximation. This foundational exercise [@problem_id:2187288] demonstrates this principle by having you approximate the function $f(x) = x^3$ with a polynomial of a lower degree, revealing the near-optimal properties of this method.", "problem": "In numerical analysis, interpolation using Chebyshev nodes is a superior alternative to using equally spaced points, especially for avoiding Runge's phenomenon. This method chooses interpolation points that are the roots of Chebyshev polynomials, which are clustered near the endpoints of the interval.\n\nConsider the function $f(x) = x^3$ over the interval $[-1, 1]$. Your task is to find the unique polynomial of degree at most 2, let's call it $P_2(x)$, that interpolates $f(x)$ at the three Chebyshev nodes on this interval. These nodes are defined as the roots of the degree-3 Chebyshev polynomial of the first kind, $T_3(x)$.\n\nExpress your answer for $P_2(x)$ as a simplified polynomial in terms of $x$.", "solution": "We are given $f(x)=x^{3}$ on $[-1,1]$ and must find the unique polynomial $P_{2}(x)$ of degree at most $2$ that interpolates $f$ at the three Chebyshev nodes on $[-1,1]$, defined as the roots of $T_{3}(x)$, the degree-$3$ Chebyshev polynomial of the first kind.\n\nUse the identity $T_{3}(x)=\\cos\\!\\big(3\\arccos x\\big)=4x^{3}-3x$. The roots of $T_{3}$ are the three Chebyshev nodes:\n$$\nT_{3}(x)=4x^{3}-3x=x\\big(4x^{2}-3\\big)=0\n\\quad\\Longrightarrow\\quad\nx\\in\\left\\{0,\\;\\pm\\frac{\\sqrt{3}}{2}\\right\\}.\n$$\nLet $P_{2}(x)=A x^{2}+B x+C$. The interpolation conditions are $P_{2}(\\xi)=\\xi^{3}$ at each node $\\xi\\in\\left\\{0,\\pm\\frac{\\sqrt{3}}{2}\\right\\}$.\n\n1) At $x=0$:\n$$\nP_{2}(0)=C=0 \\quad\\Longrightarrow\\quad C=0.\n$$\n\n2) Let $a=\\frac{\\sqrt{3}}{2}$. At $x=a$ and $x=-a$:\n$$\nP_{2}(a)=A a^{2}+B a=a^{3},\\qquad P_{2}(-a)=A a^{2}-B a=-a^{3}.\n$$\nAdding these two equations eliminates $B$:\n$$\n\\big(A a^{2}+B a\\big)+\\big(A a^{2}-B a\\big)=a^{3}+(-a^{3})\n\\;\\Longrightarrow\\;\n2A a^{2}=0\n\\;\\Longrightarrow\\;\nA=0\n\\quad(\\text{since }a\\neq 0).\n$$\nSubstitute $A=0$ into $A a^{2}+B a=a^{3}$ to solve for $B$:\n$$\nB a=a^{3}\\;\\Longrightarrow\\;B=a^{2}.\n$$\nWith $a=\\frac{\\sqrt{3}}{2}$, we have\n$$\na^{2}=\\left(\\frac{\\sqrt{3}}{2}\\right)^{2}=\\frac{3}{4}.\n$$\nTherefore,\n$$\nP_{2}(x)=B x=\\frac{3}{4}\\,x.\n$$\nA quick check: $P_{2}(0)=0=f(0)$, and $P_{2}(\\pm a)=\\frac{3}{4}(\\pm a)=\\pm a\\cdot\\frac{3}{4}=\\pm a^{3}=f(\\pm a)$, so the interpolation conditions are satisfied at all three Chebyshev nodes.\n\nHence, the unique interpolating polynomial of degree at most $2$ is $P_{2}(x)=\\frac{3}{4}x$.", "answer": "$$\\boxed{\\frac{3}{4}x}$$", "id": "2187288"}, {"introduction": "One of the most powerful features of polynomial approximations is that they are analytic, meaning we can easily compute their derivatives. This practice [@problem_id:2379334] applies this concept to a core task in computational finance: calculating the \"Greeks\" of an option, which measure the sensitivity of its price to changes in underlying variables. By analytically differentiating a Chebyshev polynomial that approximates an option's price, you will gain hands-on experience in using these methods to build practical tools for risk analysis.", "problem": "A European call option price as a function of the spot price $S$ and volatility $\\sigma$ is approximated over the domain $S \\in [80, 120]$ and $\\sigma \\in [0.1, 0.4]$ by a tensor-product Chebyshev polynomial of the first kind of degree $2$ in $S$ and degree $1$ in $\\sigma$. Let the affine maps to the Chebyshev domain be\n$$\nx(S) \\equiv \\frac{2 S - (S_{\\max} + S_{\\min})}{S_{\\max} - S_{\\min}}, \\quad y(\\sigma) \\equiv \\frac{2 \\sigma - (\\sigma_{\\max} + \\sigma_{\\min})}{\\sigma_{\\max} - \\sigma_{\\min}},\n$$\nwith $S_{\\min} = 80$, $S_{\\max} = 120$, $\\sigma_{\\min} = 0.1$, and $\\sigma_{\\max} = 0.4$. The approximation is\n$$\n\\widehat{P}(S,\\sigma) = \\sum_{i=0}^{2}\\sum_{j=0}^{1} a_{ij}\\, T_{i}(x(S))\\, T_{j}(y(\\sigma)),\n$$\nwhere $T_{n}$ denotes the Chebyshev polynomial of the first kind, with $T_{0}(u) = 1$, $T_{1}(u) = u$, and $T_{2}(u) = 2u^{2} - 1$. The coefficients are\n$$\na_{00} = 10, \\quad a_{10} = 5, \\quad a_{20} = 2, \\quad a_{01} = 8, \\quad a_{11} = -3, \\quad a_{21} = 1.\n$$\n\nCompute the optionâ€™s Delta, Gamma, and Vega of the approximation at $S = 105$ and $\\sigma = 0.2$, where Delta is $\\frac{\\partial \\widehat{P}}{\\partial S}$, Gamma is $\\frac{\\partial^{2} \\widehat{P}}{\\partial S^{2}}$, and Vega is $\\frac{\\partial \\widehat{P}}{\\partial \\sigma}$. Express your final answer as a single row matrix in the order $\\big(\\Delta, \\Gamma, \\text{Vega}\\big)$. Do not round your answer.", "solution": "The problem requires the computation of three partial derivatives of an approximate option price function, $\\widehat{P}(S,\\sigma)$, with respect to the spot price $S$ and volatility $\\sigma$. These derivatives are known in finance as Delta ($\\Delta$), Gamma ($\\Gamma$), and Vega. The analysis proceeds as follows.\n\nFirst, the problem statement is validated.\nStep 1: Extract Givens.\n- The approximation for the option price is $\\widehat{P}(S,\\sigma) = \\sum_{i=0}^{2}\\sum_{j=0}^{1} a_{ij}\\, T_{i}(x(S))\\, T_{j}(y(\\sigma))$.\n- The domain is $S \\in [S_{\\min}, S_{\\max}]$ with $S_{\\min} = 80$ and $S_{\\max} = 120$, and $\\sigma \\in [\\sigma_{\\min}, \\sigma_{\\max}]$ with $\\sigma_{\\min} = 0.1$ and $\\sigma_{\\max} = 0.4$.\n- The affine maps to the canonical Chebyshev domain $[-1, 1]$ are:\n$$\nx(S) = \\frac{2 S - (S_{\\max} + S_{\\min})}{S_{\\max} - S_{\\min}} \\quad \\text{and} \\quad y(\\sigma) = \\frac{2 \\sigma - (\\sigma_{\\max} + \\sigma_{\\min})}{\\sigma_{\\max} - \\sigma_{\\min}}\n$$\n- The Chebyshev polynomials of the first kind are provided: $T_{0}(u) = 1$, $T_{1}(u) = u$, and $T_{2}(u) = 2u^{2} - 1$.\n- The coefficients are given as: $a_{00} = 10$, $a_{10} = 5$, $a_{20} = 2$, $a_{01} = 8$, $a_{11} = -3$, and $a_{21} = 1$.\n- The quantities to be computed are $\\Delta = \\frac{\\partial \\widehat{P}}{\\partial S}$, $\\Gamma = \\frac{\\partial^{2} \\widehat{P}}{\\partial S^{2}}$, and Vega = $\\frac{\\partial \\widehat{P}}{\\partial \\sigma}$.\n- The evaluation point is $(S, \\sigma) = (105, 0.2)$.\n\nStep 2: Validate Using Extracted Givens.\nThe problem is scientifically grounded, well-posed, objective, and complete. It presents a standard application of multivariate polynomial approximation and differentiation from the field of computational finance. No scientific principles are violated, all necessary data is provided, and the terms are defined unambiguously.\n\nStep 3: Verdict and Action.\nThe problem is valid. A solution will be derived.\n\nThe derivatives of the affine maps are computed first.\nFor $x(S)$:\n$$\nx(S) = \\frac{2S - (120 + 80)}{120 - 80} = \\frac{2S - 200}{40} = \\frac{S - 100}{20}\n$$\nThe first and second derivatives with respect to $S$ are:\n$$\n\\frac{dx}{dS} = \\frac{1}{20} \\quad \\text{and} \\quad \\frac{d^{2}x}{dS^{2}} = 0\n$$\nFor $y(\\sigma)$:\n$$\ny(\\sigma) = \\frac{2\\sigma - (0.4 + 0.1)}{0.4 - 0.1} = \\frac{2\\sigma - 0.5}{0.3}\n$$\nThe first derivative with respect to $\\sigma$ is:\n$$\n\\frac{dy}{d\\sigma} = \\frac{2}{0.3} = \\frac{20}{3}\n$$\nThe full expression for $\\widehat{P}$ is:\n$$\n\\widehat{P}(S,\\sigma) = \\sum_{j=0}^{1} T_{j}(y) \\left( \\sum_{i=0}^{2} a_{ij} T_{i}(x) \\right) = T_{0}(y) \\sum_{i=0}^{2} a_{i0} T_{i}(x) + T_{1}(y) \\sum_{i=0}^{2} a_{i1} T_{i}(x)\n$$\nSubstituting $T_0(y)=1$ and $T_1(y)=y$:\n$$\n\\widehat{P}(S,\\sigma) = (a_{00} T_{0}(x) + a_{10} T_{1}(x) + a_{20} T_{2}(x)) + y (a_{01} T_{0}(x) + a_{11} T_{1}(x) + a_{21} T_{2}(x))\n$$\n\nTo compute the derivatives with respect to $S$ and $\\sigma$, we require the derivatives of the Chebyshev polynomials:\n$T'_{0}(u) = 0$, $T'_{1}(u) = 1$, $T'_{2}(u) = 4u$.\n$T''_{0}(u) = 0$, $T''_{1}(u) = 0$, $T''_{2}(u) = 4$.\n\nNow, we compute Delta, Gamma, and Vega using the chain rule.\n\n1.  Delta ($\\Delta = \\frac{\\partial \\widehat{P}}{\\partial S}$):\n$$\n\\Delta = \\frac{\\partial \\widehat{P}}{\\partial S} = \\frac{\\partial \\widehat{P}}{\\partial x} \\frac{dx}{dS}\n$$\nThe partial derivative with respect to $x$ is:\n$$\n\\frac{\\partial \\widehat{P}}{\\partial x} = (a_{10} T'_{1}(x) + a_{20} T'_{2}(x)) + y (a_{11} T'_{1}(x) + a_{21} T'_{2}(x)) = (a_{10} + 4a_{20}x) + y (a_{11} + 4a_{21}x)\n$$\nSo, Delta is:\n$$\n\\Delta = \\left[ (a_{10} + 4a_{20}x) + y (a_{11} + 4a_{21}x) \\right] \\frac{dx}{dS}\n$$\n\n2.  Gamma ($\\Gamma = \\frac{\\partial^{2} \\widehat{P}}{\\partial S^{2}}$):\n$$\n\\Gamma = \\frac{\\partial}{\\partial S} \\left( \\frac{\\partial \\widehat{P}}{\\partial S} \\right) = \\frac{\\partial}{\\partial S} \\left( \\frac{\\partial \\widehat{P}}{\\partial x} \\frac{dx}{dS} \\right) = \\frac{\\partial}{\\partial S} \\left( \\frac{\\partial \\widehat{P}}{\\partial x} \\right) \\frac{dx}{dS} + \\frac{\\partial \\widehat{P}}{\\partial x} \\frac{d^{2}x}{dS^{2}}\n$$\nSince $\\frac{d^{2}x}{dS^{2}} = 0$, the expression simplifies to:\n$$\n\\Gamma = \\left( \\frac{\\partial}{\\partial x} \\left( \\frac{\\partial \\widehat{P}}{\\partial x} \\right) \\frac{dx}{dS} \\right) \\frac{dx}{dS} = \\frac{\\partial^{2} \\widehat{P}}{\\partial x^{2}} \\left( \\frac{dx}{dS} \\right)^{2}\n$$\nThe second partial derivative with respect to $x$ is:\n$$\n\\frac{\\partial^{2} \\widehat{P}}{\\partial x^{2}} = \\frac{\\partial}{\\partial x} \\left[ (a_{10} + 4a_{20}x) + y (a_{11} + 4a_{21}x) \\right] = 4a_{20} + 4a_{21}y\n$$\nSo, Gamma is:\n$$\n\\Gamma = (4a_{20} + 4a_{21}y) \\left( \\frac{dx}{dS} \\right)^{2}\n$$\n\n3.  Vega ($\\text{Vega} = \\frac{\\partial \\widehat{P}}{\\partial \\sigma}$):\n$$\n\\text{Vega} = \\frac{\\partial \\widehat{P}}{\\partial \\sigma} = \\frac{\\partial \\widehat{P}}{\\partial y} \\frac{dy}{d\\sigma}\n$$\nThe partial derivative with respect to $y$ is:\n$$\n\\frac{\\partial \\widehat{P}}{\\partial y} = a_{01} T_{0}(x) + a_{11} T_{1}(x) + a_{21} T_{2}(x) = a_{01} + a_{11} x + a_{21}(2x^{2} - 1)\n$$\nSo, Vega is:\n$$\n\\text{Vega} = \\left[ (a_{01} - a_{21}) + a_{11}x + 2a_{21}x^{2} \\right] \\frac{dy}{d\\sigma}\n$$\nFinally, we evaluate these expressions at the point $(S, \\sigma) = (105, 0.2)$.\nFirst, calculate $x$ and $y$:\n$$\nx(105) = \\frac{105 - 100}{20} = \\frac{5}{20} = \\frac{1}{4}\n$$\n$$\ny(0.2) = \\frac{2(0.2) - 0.5}{0.3} = \\frac{0.4 - 0.5}{0.3} = \\frac{-0.1}{0.3} = -\\frac{1}{3}\n$$\nSubstitute the values of the coefficients, $x$, $y$, and the derivatives of the maps.\n\nFor Delta:\n$$\n\\Delta = \\left[ (5 + 4(2)\\frac{1}{4}) + (-\\frac{1}{3}) ((-3) + 4(1)\\frac{1}{4}) \\right] \\left(\\frac{1}{20}\\right)\n$$\n$$\n\\Delta = \\left[ (5 + 2) + (-\\frac{1}{3}) (-3 + 1) \\right] \\left(\\frac{1}{20}\\right) = \\left[ 7 + (-\\frac{1}{3})(-2) \\right] \\left(\\frac{1}{20}\\right)\n$$\n$$\n\\Delta = \\left[ 7 + \\frac{2}{3} \\right] \\left(\\frac{1}{20}\\right) = \\left( \\frac{21+2}{3} \\right) \\left(\\frac{1}{20}\\right) = \\frac{23}{3} \\cdot \\frac{1}{20} = \\frac{23}{60}\n$$\n\nFor Gamma:\n$$\n\\Gamma = (4(2) + 4(1)(-\\frac{1}{3})) \\left( \\frac{1}{20} \\right)^{2} = (8 - \\frac{4}{3}) \\left( \\frac{1}{400} \\right)\n$$\n$$\n\\Gamma = \\left( \\frac{24-4}{3} \\right) \\left( \\frac{1}{400} \\right) = \\frac{20}{3} \\cdot \\frac{1}{400} = \\frac{1}{3 \\cdot 20} = \\frac{1}{60}\n$$\n\nFor Vega:\n$$\n\\text{Vega} = \\left[ (8 - 1) + (-3)(\\frac{1}{4}) + 2(1)(\\frac{1}{4})^{2} \\right] \\left( \\frac{20}{3} \\right)\n$$\n$$\n\\text{Vega} = \\left[ 7 - \\frac{3}{4} + 2(\\frac{1}{16}) \\right] \\left( \\frac{20}{3} \\right) = \\left[ 7 - \\frac{3}{4} + \\frac{1}{8} \\right] \\left( \\frac{20}{3} \\right)\n$$\n$$\n\\text{Vega} = \\left[ \\frac{56 - 6 + 1}{8} \\right] \\left( \\frac{20}{3} \\right) = \\left( \\frac{51}{8} \\right) \\left( \\frac{20}{3} \\right) = \\frac{17 \\cdot 5}{2} = \\frac{85}{2}\n$$\nThe final results are $\\Delta = \\frac{23}{60}$, $\\Gamma = \\frac{1}{60}$, and Vega $= \\frac{85}{2}$.", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{23}{60} & \\frac{1}{60} & \\frac{85}{2} \\end{pmatrix}}$$", "id": "2379334"}, {"introduction": "In practical applications, a critical question is how to choose the degree of the approximating polynomial to balance accuracy and computational cost. This coding exercise [@problem_id:2379358] addresses this by guiding you to implement an algorithm that automatically selects the polynomial degree. The method works by monitoring the decay rate of the Chebyshev coefficients, a property directly linked to the smoothness of the function being approximated, providing an essential skill for building robust and efficient numerical models.", "problem": "Consider the problem of approximating a bounded real-valued function $f$ on the interval $[-1,1]$ by a truncated Chebyshev series of the first kind. Let $\\{T_n(x)\\}_{n \\ge 0}$ denote the Chebyshev polynomials of the first kind, defined by $T_n(x) = \\cos(n \\arccos x)$ for $x \\in [-1,1]$ with angles measured in radians. The Chebyshev series of $f$ is given by\n$$\nf(x) \\sim \\frac{1}{2} c_0 + \\sum_{n=1}^{\\infty} c_n T_n(x),\n$$\nwhere the Chebyshev coefficients $\\{c_n\\}_{n \\ge 0}$ are defined by\n$$\nc_n = \\frac{2}{\\pi} \\int_{-1}^{1} \\frac{f(x)\\, T_n(x)}{\\sqrt{1-x^2}} \\, dx\n= \\frac{2}{\\pi} \\int_{0}^{\\pi} f(\\cos \\theta)\\, \\cos(n \\theta)\\, d\\theta,\n$$\nwith the convention that the truncated approximation of degree $N$ is\n$$\np_N(x) = \\frac{1}{2} c_0 + \\sum_{n=1}^{N} c_n T_n(x).\n$$\n\nYour task is to write a complete, runnable program that, for each test case described below, computes a data-driven degree selection $N^\\star$ by monitoring the decay of the Chebyshev coefficients. The decision rule is defined purely in terms of the coefficients $\\{c_n\\}$ as follows. Given a nonnegative integer $N_{\\max}$, a tolerance $\\varepsilon \\in (0,1)$, and a positive integer window length $q$, define $K = N_{\\max} + q$. Compute numerical approximations to the coefficients $\\{c_n\\}_{n=0}^{K}$. Then select\n$$\nN^\\star = \\min \\left\\{ N \\in \\{0,1,\\dots,N_{\\max}-q\\} \\,:\\, \n\\max_{n \\in \\{N+1,\\dots,N+q\\}} |c_n| \\le \\varepsilon \\cdot \\max_{m \\in \\{0,\\dots,N\\}} |c_m| \\right\\},\n$$\nand if the set is empty, set $N^\\star = N_{\\max}$. All trigonometric function arguments are in radians. Your numerical method for approximating the integrals must be consistent with the above definitions, and the same method must be applied uniformly across all cases.\n\nThis criterion is intended for automated basis selection in global polynomial approximations commonly used in computational economics and finance (for example, for value functions, policy functions, and pricing kernels), while controlling projection error through the decay of Chebyshev coefficients.\n\nTest Suite. For each test, the function $f$, the tolerance $\\varepsilon$, the window length $q$, and the cap $N_{\\max}$ are specified:\n\n- Test A (analytic, rapidly decaying coefficients):\n  - $f(x) = e^{x}$,\n  - $\\varepsilon = 10^{-12}$,\n  - $q = 5$,\n  - $N_{\\max} = 200$.\n\n- Test B (analytic, rational function):\n  - $f(x) = \\dfrac{1}{1 + 25 x^2}$,\n  - $\\varepsilon = 10^{-10}$,\n  - $q = 5$,\n  - $N_{\\max} = 200$.\n\n- Test C (nonsmooth at $x=0$):\n  - $f(x) = |x|$,\n  - $\\varepsilon = 10^{-3}$,\n  - $q = 5$,\n  - $N_{\\max} = 200$.\n\n- Test D (constant function, boundary case):\n  - $f(x) = 3$,\n  - $\\varepsilon = 10^{-14}$,\n  - $q = 5$,\n  - $N_{\\max} = 50$.\n\n- Test E (single-mode Chebyshev polynomial, exact structure):\n  - $f(x) = \\cos\\!\\big(50 \\arccos x\\big)$,\n  - $\\varepsilon = 10^{-12}$,\n  - $q = 3$,\n  - $N_{\\max} = 200$.\n\n- Test F (stringent tolerance with small cap, no satisfactory window before cap):\n  - $f(x) = |x|$,\n  - $\\varepsilon = 10^{-6}$,\n  - $q = 5$,\n  - $N_{\\max} = 30$.\n\nFinal Output Format. Your program should produce a single line of output containing the six selected degrees aggregated into a single comma-separated list enclosed in square brackets, with the results in the order A, B, C, D, E, F. For example, the output must look like\n$$\n[\\text{result}_A,\\text{result}_B,\\text{result}_C,\\text{result}_D,\\text{result}_E,\\text{result}_F]\n$$\nwhere each entry is an integer.", "solution": "The supplied problem statement is subjected to validation before a solution is attempted.\n\n### Step 1: Extract Givens\n- **Function to Approximate:** A bounded real-valued function $f$ on the interval $[-1,1]$.\n- **Basis:** Chebyshev polynomials of the first kind, $T_n(x) = \\cos(n \\arccos x)$.\n- **Chebyshev Series:** An expansion of the form $f(x) \\sim \\frac{1}{2} c_0 + \\sum_{n=1}^{\\infty} c_n T_n(x)$.\n- **Chebyshev Coefficients:** The coefficients $\\{c_n\\}$ are defined by the integral $c_n = \\frac{2}{\\pi} \\int_{0}^{\\pi} f(\\cos \\theta)\\, \\cos(n \\theta)\\, d\\theta$.\n- **Approximation:** A truncated series of degree $N$, denoted $p_N(x) = \\frac{1}{2} c_0 + \\sum_{n=1}^{N} c_n T_n(x)$.\n- **Objective:** For each test case, find an optimal degree $N^\\star$ based on a coefficient decay rule.\n- **Parameters:**\n    - Maximum degree considered: $N_{\\max}$.\n    - Tolerance: $\\varepsilon \\in (0,1)$.\n    - Window length: $q \\in \\mathbb{Z}^+$.\n- **Decision Rule:** Compute numerical approximations of coefficients $\\{c_n\\}_{n=0}^{K}$ where $K = N_{\\max} + q$. Then find $N^\\star$ as:\n$$\nN^\\star = \\min \\left\\{ N \\in \\{0,1,\\dots,N_{\\max}-q\\} \\,:\\, \n\\max_{n \\in \\{N+1,\\dots,N+q\\}} |c_n| \\le \\varepsilon \\cdot \\max_{m \\in \\{0,\\dots,N\\}} |c_m| \\right\\}.\n$$\nIf the set is empty, $N^\\star = N_{\\max}$.\n- **Numerical Method:** Must be consistent with the definitions and applied uniformly.\n- **Test Suite:** Six test cases (A-F) are provided, each with a specific function $f$, and parameters $\\varepsilon$, $q$, and $N_{\\max}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed against the required criteria:\n- **Scientifically Grounded:** The problem is rooted in the classical theory of approximation by orthogonal polynomials, a cornerstone of numerical analysis. Chebyshev polynomial approximations are standard, effective tools used widely in scientific computing, including in computational economics and finance as stated. The foundation is scientifically sound.\n- **Well-Posed:** The decision rule for $N^\\star$ is explicit and unambiguous. For any set of computed coefficients, the search space for $N$ is finite ($\\{0, \\dots, N_{\\max}-q\\}$), and the existence of a minimum is guaranteed. The fallback assignment $N^\\star = N_{\\max}$ ensures a unique solution is always found. The problem is well-posed.\n- **Objective:** The problem is stated using precise mathematical definitions and objective criteria. There are no subjective or ambiguous terms.\n\nThe problem formulation is free of scientific unsoundness, ambiguity, and contradiction. All data required for a computational solution are provided. The problem is therefore deemed **valid**.\n\n### Step 3: Verdict and Action\nThe problem is valid. A reasoned solution will be provided.\n\n### Solution and Algorithm Design\nThe core of the problem is to compute numerical approximations of the Chebyshev coefficients $\\{c_n\\}$ and then apply the specified decision rule.\n\n**1. Numerical Approximation of Coefficients**\n\nThe coefficients $c_n$ are defined by an integral. A highly accurate and efficient method for approximating these coefficients is the pseudospectral or Chebyshev projection method. This involves interpolating the function $f(x)$ at a set of specific nodes and then computing the coefficients of the resulting interpolating polynomial. These coefficients serve as excellent approximations of the true series coefficients, especially for smooth functions, where they converge spectrally.\n\nLet $K = N_{\\max} + q$. We will construct a polynomial interpolant of degree $K$. This requires $K+1$ interpolation points. The optimal choice for stability and accuracy are the Chebyshev-Gauss-Lobatto nodes, which are the extrema of the Chebyshev polynomial $T_K(x)$ on the interval $[-1,1]$. These nodes are given by:\n$$\nx_j = \\cos\\left(\\frac{j \\pi}{K}\\right), \\quad j = 0, 1, \\dots, K.\n$$\nLet the polynomial that interpolates $f(x)$ at these nodes be $p_K(x) = \\sum_{n=0}^{K} a_n T_n(x)$. The coefficients $\\{a_n\\}$ are our numerical approximations. They can be computed efficiently using a Discrete Cosine Transform (DCT), typically of Type-I. The `numpy.polynomial.chebyshev.Chebyshev.fit` method provides a robust, standard implementation for this task. Given the nodes $x_j$ and function values $f(x_j)$, it returns the coefficient vector $[a_0, a_1, \\dots, a_K]$.\n\n**2. Consistency with Problem Definitions**\n\nThe problem defines the approximation as $p_N(x) = \\frac{1}{2} c_0 + \\sum_{n=1}^{N} c_n T_n(x)$, while the standard `numpy` basis is $\\{T_n(x)\\}_{n \\ge 0}$. The coefficients are related by $a_0 = c_0/2$ and $a_n = c_n$ for $n \\ge 1$. The decision rule is phrased in terms of $\\{c_n\\}$. Therefore, after obtaining the coefficients $\\{a_n\\}$ from the fitting procedure, we must convert them to $\\{c_n\\}$ before applying the rule:\n$$\nc_0 = 2a_0, \\quad c_n = a_n \\text{ for } n \\ge 1.\n$$\nThese computed coefficients, which we will call $\\hat{c}_n$ to denote they are approximations, will be used in the decision rule.\n\n**3. Implementing the Decision Rule**\n\nWith the coefficient vector $\\{\\hat{c}_n\\}_{n=0}^K$ available, we implement the search for $N^\\star$. The algorithm proceeds as follows:\n- Initialize the result to the default value, $N^\\star = N_{\\max}$.\n- Iterate through candidate values $N$ from $0$ to $N_{\\max} - q$.\n- For each $N$, compute two quantities:\n    - $M_{\\text{hist}} = \\max_{m \\in \\{0, \\dots, N\\}} |\\hat{c}_m|$. This represents the magnitude of the largest coefficient seen up to degree $N$.\n    - $M_{\\text{win}} = \\max_{n \\in \\{N+1, \\dots, N+q\\}} |\\hat{c}_n|$. This is the magnitude of the largest coefficient in the subsequent window of size $q$.\n- Check if the condition $M_{\\text{win}} \\le \\varepsilon \\cdot M_{\\text{hist}}$ is satisfied.\n    - This condition is robustly implemented as a multiplication, avoiding division-by-zero issues if $M_{\\text{hist}}$ is zero. If $M_{\\text{hist}}=0$, the condition holds only if $M_{\\text{win}}=0$, which is a mathematically sound interpretation.\n- If the condition is met, we have found a satisfactory degree. Since the problem asks for the minimum such $N$, we set $N^\\star = N$ and terminate the search immediately.\n- If the loop completes without the condition ever being met, the initial value $N^\\star = N_{\\max}$ is retained.\n\nThis procedure is applied uniformly to each test case specified in the problem statement. For example, in Test E, where $f(x) = T_{50}(x)$, the true coefficients are $c_{50}=2$ (in our $\\{a_n\\}$ basis, $a_{50}=1$) and all others are zero. The numerical procedure will yield $\\hat{c}_{50} \\approx 2$ and other $\\hat{c}_n$ near machine precision. The rule will fail for $N < 50$ because either $M_{\\text{hist}}$ is small and $M_{\\text{win}}$ contains the large coefficient, or both are small but do not satisfy the tolerance. For $N=50$, $M_{\\text{hist}} = |\\hat{c}_{50}| \\approx 2$ and $M_{\\text{win}}$ will be near machine precision. The condition will be satisfied, correctly identifying $N^\\star=50$. In Test F, the slow decay of coefficients for $f(x)=|x|$ combined with a stringent tolerance $\\varepsilon$ and small $N_{\\max}$ will likely cause the loop to complete without finding a suitable $N$, resulting in the default $N^\\star=N_{max}$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom numpy.polynomial import chebyshev as cheb\n\ndef solve():\n    \"\"\"\n    Solves for the optimal Chebyshev truncation degree N* for a series of test cases.\n    \"\"\"\n\n    def compute_n_star(f, epsilon, q, n_max):\n        \"\"\"\n        Computes the data-driven degree selection N* for a given function and parameters.\n\n        Args:\n            f (callable): The function to approximate, defined on [-1, 1].\n            epsilon (float): The tolerance for the coefficient decay condition.\n            q (int): The window length for checking coefficient decay.\n            n_max (int): The maximum degree to consider.\n\n        Returns:\n            int: The selected degree N*.\n        \"\"\"\n        # The total number of coefficients to compute is K+1, where K = n_max + q.\n        # This corresponds to an interpolating polynomial of degree K.\n        K = n_max + q\n\n        # Generate K+1 Chebyshev-Gauss-Lobatto nodes, which are the extrema of T_K(x).\n        # np.polynomial.chebyshev.chebpts1(K + 1) generates x_j = cos(j*pi/K) for j=0,...,K.\n        nodes = cheb.chebpts1(K + 1)\n\n        # Evaluate the function at the nodes.\n        # Use np.clip to guard against floating-point errors in arccos for values just outside [-1, 1].\n        f_vals = f(nodes)\n\n        # Compute coefficients of the interpolating polynomial of degree K.\n        # Chebyshev.fit solves a least-squares problem, which for Chebyshev nodes\n        # and deg=K is equivalent to interpolation. It returns coefficients {a_n}.\n        poly_fit = cheb.Chebyshev.fit(nodes, f_vals, deg=K)\n        a_coeffs = poly_fit.coef\n\n        # The problem defines the series with a 1/2 factor for the first term,\n        # i.e., p_N(x) = 0.5*c_0 + sum_{n=1 to N} c_n*T_n(x).\n        # The numpy basis is p_N(x) = sum_{n=0 to N} a_n*T_n(x).\n        # The relationship is a_0 = c_0/2 and a_n = c_n for n > 0.\n        # We convert our computed {a_n} to {c_n} for the decision rule.\n        c_coeffs = a_coeffs.copy()\n        if len(c_coeffs) > 0:\n            c_coeffs[0] *= 2.0\n\n        # Apply the decision rule to find N*\n        n_star = n_max\n        for N in range(n_max - q + 1):\n            # max|c_m| for m in {0, ..., N}\n            # Slice is [0, N], so we need N+1 elements.\n            max_m = np.max(np.abs(c_coeffs[0 : N + 1]))\n            \n            # max|c_n| for n in {N+1, ..., N+q}\n            # Slice is [N+1, N+q], so N+q-N = q elements.\n            max_n = np.max(np.abs(c_coeffs[N + 1 : N + q + 1]))\n\n            # The condition is stated in multiplicative form to avoid division by zero.\n            if max_n <= epsilon * max_m:\n                n_star = N\n                break  # Found the minimum N, so we can stop.\n        \n        return n_star\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test A (analytic, rapidly decaying coefficients)\n        {'f': lambda x: np.exp(x), 'eps': 1e-12, 'q': 5, 'n_max': 200},\n        # Test B (analytic, rational function)\n        {'f': lambda x: 1.0 / (1.0 + 25.0 * x**2), 'eps': 1e-10, 'q': 5, 'n_max': 200},\n        # Test C (nonsmooth at x=0)\n        {'f': lambda x: np.abs(x), 'eps': 1e-3, 'q': 5, 'n_max': 200},\n        # Test D (constant function, boundary case)\n        {'f': lambda x: np.full_like(x, 3.0), 'eps': 1e-14, 'q': 5, 'n_max': 50},\n        # Test E (single-mode Chebyshev polynomial, exact structure)\n        {'f': lambda x: np.cos(50.0 * np.arccos(np.clip(x, -1.0, 1.0))), 'eps': 1e-12, 'q': 3, 'n_max': 200},\n        # Test F (stringent tolerance with small cap, no satisfactory window before cap)\n        {'f': lambda x: np.abs(x), 'eps': 1e-6, 'q': 5, 'n_max': 30},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = compute_n_star(case['f'], case['eps'], case['q'], case['n_max'])\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2379358"}]}