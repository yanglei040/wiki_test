{"hands_on_practices": [{"introduction": "Before writing your own optimization code, it is crucial to understand what can go wrong. This diagnostic challenge [@problem_id:2414722] places you in the role of a quantitative analyst debugging a faulty implementation of Newton's method. By examining the iteration-by-iteration logs, you will learn to spot violations of the core principles that guarantee convergence, such as the sufficient decrease condition, and develop the critical skills needed to diagnose and fix real-world code.", "problem": "A financial institution estimates a binary default probability model by minimizing the negative log-likelihood of a logistic specification using Newton's method with backtracking line search that is intended to enforce Armijo sufficient decrease. The parameter vector is denoted by $\\beta \\in \\mathbb{R}^p$, the objective by $f(\\beta)$, the gradient by $\\nabla f(\\beta)$, and the Hessian by $\\nabla^2 f(\\beta)$. The implementation prints the following diagnostics.\n\nAt iteration $k=0$:\n- $f(\\beta_0) = 120.35$.\n- $\\lVert \\nabla f(\\beta_0) \\rVert_2 = 85.7$.\n- The minimum and maximum eigenvalues of $\\nabla^2 f(\\beta_0)$ are $\\lambda_{\\min} = 3.1$ and $\\lambda_{\\max} = 214.2$.\n- The linear system for the Newton direction is solved, and the reported inner product is $\\nabla f(\\beta_0)^\\top s_0 = -28.9$.\n- The backtracking line search with parameter $c_1 = 10^{-4}$ accepts $t_0 = 1$ with the message “Armijo satisfied,” and the code reports $f(\\beta_0 + t_0 s_0) = 134.7$.\n\nAt iteration $k=1$:\n- $f(\\beta_1) = 134.7$.\n- $\\lVert \\nabla f(\\beta_1) \\rVert_2 = 92.4$.\n- The minimum eigenvalue of $\\nabla^2 f(\\beta_1)$ is reported as $\\lambda_{\\min} = 2.7$.\n- The computed direction satisfies $\\nabla f(\\beta_1)^\\top s_1 = -31.4$.\n- The line search again accepts $t_1 = 1$ with the message “Armijo satisfied,” and $f(\\beta_1 + t_1 s_1) = 150.2$.\n\nAssume the negative log-likelihood of the logistic model is correctly specified and twice continuously differentiable, and that the dataset is of moderate scale and conditioning, so that numerical linear algebra is stable when the Hessian is positive definite.\n\nBased on these diagnostics and standard properties of Newton's method for unconstrained minimization with Armijo backtracking, which implementation issue is most consistent with the observed non-convergence?\n\nA. The Hessian was assembled without cross-partial terms, making it indefinite and producing non-descent directions.\n\nB. The Newton system was solved with the wrong sign, effectively using $s_k = \\nabla^2 f(\\beta_k)^{-1} \\nabla f(\\beta_k)$, which is an ascent direction.\n\nC. The Armijo sufficient decrease test in the line search was implemented with the inequality reversed, so steps that increase $f$ are erroneously accepted.\n\nD. The algorithm’s stopping criterion on $\\lVert \\nabla f(\\beta_k) \\rVert_2$ is too strict, causing it to continue iterating instead of terminating near a minimizer.", "solution": "The user has provided a problem statement describing the output of a faulty implementation of Newton's method for minimizing a negative log-likelihood function. I must validate the statement and then diagnose the most likely implementation error based on the provided data.\n\n**Problem Validation**\n\nFirst, I will extract the given information and validate the problem statement.\n\n**Givens:**\n-   **Algorithm**: Newton's method for minimizing an objective function $f(\\beta)$.\n-   **Update Rule**: $\\beta_{k+1} = \\beta_k + t_k s_k$, where $s_k$ is the Newton direction.\n-   **Newton Direction**: $s_k$ is the solution to $\\nabla^2 f(\\beta_k) s_k = - \\nabla f(\\beta_k)$.\n-   **Line Search**: Backtracking to enforce the Armijo sufficient decrease condition with parameter $c_1 = 10^{-4}$.\n-   **Armijo Condition**: $f(\\beta_k + t_k s_k) \\le f(\\beta_k) + c_1 t_k \\nabla f(\\beta_k)^\\top s_k$.\n-   **Assumptions**: The function $f$ is the twice continuously differentiable negative log-likelihood of a logistic model. Numerical stability is assumed when the Hessian is positive definite.\n\n**Data at Iteration $k=0$:**\n-   $f(\\beta_0) = 120.35$\n-   $\\lVert \\nabla f(\\beta_0) \\rVert_2 = 85.7$\n-   Eigenvalues of $\\nabla^2 f(\\beta_0)$: $\\lambda_{\\min} = 3.1$, $\\lambda_{\\max} = 214.2$.\n-   $\\nabla f(\\beta_0)^\\top s_0 = -28.9$\n-   Accepted step size: $t_0 = 1$.\n-   Resulting function value: $f(\\beta_1) = f(\\beta_0 + t_0 s_0) = 134.7$.\n-   Algorithm message: \"Armijo satisfied\".\n\n**Data at Iteration $k=1$:**\n-   $f(\\beta_1) = 134.7$\n-   $\\lVert \\nabla f(\\beta_1) \\rVert_2 = 92.4$\n-   Minimum eigenvalue of $\\nabla^2 f(\\beta_1)$: $\\lambda_{\\min} = 2.7$.\n-   $\\nabla f(\\beta_1)^\\top s_1 = -31.4$\n-   Accepted step size: $t_1 = 1$.\n-   Resulting function value: $f(\\beta_2) = f(\\beta_1 + t_1 s_1) = 150.2$.\n-   Algorithm message: \"Armijo satisfied\".\n\n**Validation Analysis:**\nThe problem statement is scientifically grounded in numerical optimization and statistics. The provided data appears contradictory to the stated goal of the algorithm (minimization via Armijo rule), but this is the basis of the diagnostic question, not a flaw in the problem statement itself. The problem is well-posed, objective, and contains sufficient information for a logical diagnosis. For a logistic regression negative log-likelihood, the Hessian matrix is guaranteed to be positive semi-definite, and for a non-degenerate dataset, positive definite. The given positive eigenvalues ($\\lambda_{\\min} = 3.1$ and $\\lambda_{\\min} = 2.7$) are consistent with this property. The problem is therefore valid.\n\n**Derivation of Solution**\n\nThe objective of the algorithm is to minimize $f(\\beta)$. This requires that the function value decreases at each step, i.e., $f(\\beta_{k+1}) < f(\\beta_k)$. The Armijo condition is designed to guarantee this, and even more, to ensure a \"sufficient\" decrease.\n\nLet us analyze the Armijo condition at iteration $k=0$:\nThe condition is $f(\\beta_0 + t_0 s_0) \\le f(\\beta_0) + c_1 t_0 \\nabla f(\\beta_0)^\\top s_0$.\nSubstituting the given values:\n-   Left-hand side (LHS): $f(\\beta_0 + s_0) = 134.7$.\n-   Right-hand side (RHS): $f(\\beta_0) + c_1 t_0 \\nabla f(\\beta_0)^\\top s_0 = 120.35 + (10^{-4})(1)(-28.9) = 120.35 - 0.00289 = 120.34711$.\n\nThe correct Armijo condition requires $134.7 \\le 120.34711$, which is false. Despite this, the algorithm reports \"Armijo satisfied\". Furthermore, the function value has increased: $f(\\beta_1) = 134.7 > f(\\beta_0) = 120.35$. This is a failure for a minimization algorithm.\n\nLet us perform the same analysis for iteration $k=1$:\nThe condition is $f(\\beta_1 + t_1 s_1) \\le f(\\beta_1) + c_1 t_1 \\nabla f(\\beta_1)^\\top s_1$.\nSubstituting the given values:\n-   LHS: $f(\\beta_1 + s_1) = 150.2$.\n-   RHS: $f(\\beta_1) + c_1 t_1 \\nabla f(\\beta_1)^\\top s_1 = 134.7 + (10^{-4})(1)(-31.4) = 134.7 - 0.00314 = 134.69686$.\n\nThe correct Armijo condition requires $150.2 \\le 134.69686$, which is also false. Again, the algorithm erroneously reports \"Armijo satisfied\" and the function value increases: $f(\\beta_2) = 150.2 > f(\\beta_1) = 134.7$.\n\nThe algorithm is systematically failing. It is taking steps that increase the objective function and moving away from a minimum, as also indicated by the increasing gradient norm ($\\lVert \\nabla f(\\beta_1) \\rVert_2 > \\lVert \\nabla f(\\beta_0) \\rVert_2$). The only way \"Armijo satisfied\" could be printed while the function increases is if the logical test for the condition is implemented incorrectly.\n\nLet us consider the possibility of a reversed inequality in the implementation:\n$f(\\beta_k + t_k s_k) \\ge f(\\beta_k) + c_1 t_k \\nabla f(\\beta_k)^\\top s_k$.\n\n-   At $k=0$, this would test if $134.7 \\ge 120.34711$. This is true.\n-   At $k=1$, this would test if $150.2 \\ge 134.69686$. This is also true.\n\nThis reversed inequality perfectly explains why a step size of $t=1$ is accepted in both iterations despite leading to a substantial increase in the objective function.\n\n**Option-by-Option Analysis**\n\nA. The Hessian was assembled without cross-partial terms, making it indefinite and producing non-descent directions.\n**Analysis**: This claim is contradicted by the data. The minimum eigenvalues of the Hessian at both iterations are given as $\\lambda_{\\min} = 3.1$ and $\\lambda_{\\min} = 2.7$, respectively. Since the minimum eigenvalues are positive, the Hessians are positive definite, not indefinite. Furthermore, the computed directions are descent directions because the inner products with the gradient, $\\nabla f(\\beta_0)^\\top s_0 = -28.9$ and $\\nabla f(\\beta_1)^\\top s_1 = -31.4$, are negative.\n**Verdict**: Incorrect.\n\nB. The Newton system was solved with the wrong sign, effectively using $s_k = \\nabla^2 f(\\beta_k)^{-1} \\nabla f(\\beta_k)$, which is an ascent direction.\n**Analysis**: If this were the case, the search direction $s_k$ would be an ascent direction. For a positive definite Hessian $\\nabla^2 f(\\beta_k)$, the directional derivative $\\nabla f(\\beta_k)^\\top s_k$ would be $\\nabla f(\\beta_k)^\\top [\\nabla^2 f(\\beta_k)]^{-1} \\nabla f(\\beta_k) > 0$. However, the provided data shows $\\nabla f(\\beta_k)^\\top s_k < 0$ for both iterations. Thus, the computed directions are descent directions, not ascent directions.\n**Verdict**: Incorrect.\n\nC. The Armijo sufficient decrease test in the line search was implemented with the inequality reversed, so steps that increase $f$ are erroneously accepted.\n**Analysis**: As demonstrated in the derivation above, if the Armijo condition check was implemented as $f(\\beta_{k+1}) \\ge f(\\beta_k) + c_1 t_k \\nabla f(\\beta_k)^\\top s_k$, the algorithm would accept the steps $t_0=1$ and $t_1=1$, and report \"Armijo satisfied\" at each iteration. This hypothesis is fully consistent with the observed increase in the function value ($120.35 \\to 134.7 \\to 150.2$) and matches all numerical data provided.\n**Verdict**: Correct.\n\nD. The algorithm’s stopping criterion on $\\lVert \\nabla f(\\beta_k) \\rVert_2$ is too strict, causing it to continue iterating instead of terminating near a minimizer.\n**Analysis**: The nature of the stopping criterion is irrelevant to the erroneous behavior observed *within* each iteration. The core problem is that the algorithm is diverging (function value and gradient norm are increasing), not that it is converging slowly and failing to stop. A strict stopping criterion does not cause divergence; it merely means the algorithm runs longer if it is converging. The observed behavior is a fundamental failure of the optimization step itself.\n**Verdict**: Incorrect.", "answer": "$$\\boxed{C}$$", "id": "2414722"}, {"introduction": "Having diagnosed a broken algorithm, your next step is to build one that works reliably. This practice [@problem_id:2414720] guides you through the implementation of a safeguarded Newton's method, a cornerstone of practical optimization. You will learn to incorporate a fallback to the steepest descent direction and implement a backtracking line search, creating a robust solver that handles non-convex regions and ensures convergence.", "problem": "Consider the unconstrained minimization of smooth real-valued functions under an update rule that enforces a sufficient decrease condition at every iteration. Let $f:\\mathbb{R}^n \\to \\mathbb{R}$ be twice continuously differentiable. At an iterate $x_k \\in \\mathbb{R}^n$, define the gradient $\\nabla f(x_k)$ and the Hessian matrix $\\nabla^2 f(x_k)$. A candidate update $x_{k+1} = x_k + \\alpha_k p_k$ must satisfy the Armijo sufficient decrease condition with constants $c_1 \\in (0,1)$ and $\\rho \\in (0,1)$, namely\n$$\nf(x_k + \\alpha_k p_k) \\le f(x_k) + c_1 \\alpha_k \\nabla f(x_k)^\\top p_k,\n$$\nwhere $\\alpha_k$ is chosen by backtracking of the form $\\alpha_k \\in \\{ \\rho^j : j \\in \\{0,1,2,\\ldots\\} \\}$, starting from $\\alpha_k = 1$. For a given $x_k$, first attempt to select $p_k$ as the solution to the linear system\n$$\n\\nabla^2 f(x_k)\\, p_k = - \\nabla f(x_k).\n$$\nIf this candidate direction fails to produce a step length $\\alpha_k$ in the backtracking sequence that satisfies the sufficient decrease condition, then reject that direction and instead select $p_k = -\\nabla f(x_k)$, and determine $\\alpha_k$ via the same backtracking sufficient decrease condition. The iteration terminates when $\\|\\nabla f(x_k)\\|_2 \\le \\varepsilon$ or when a maximum number of $K$ iterations has been performed.\n\nImplement this iterative scheme with the following fixed constants: $c_1 = 10^{-4}$, $\\rho = 0.5$, tolerance $\\varepsilon = 10^{-8}$, backtracking limit of $M = 50$ reductions per iteration, and maximum iterations $K = 50$. If the linear system is singular or ill-posed at some $x_k$, treat the direction defined by $\\nabla^2 f(x_k)\\, p_k = - \\nabla f(x_k)$ as failed and proceed with $p_k = -\\nabla f(x_k)$ as above. All computations are to be performed in real arithmetic without any external data input.\n\nApply this scheme to the following test suite. In all cases, use the Euclidean norm $\\|\\cdot\\|_2$ for the termination criterion, and start the backtracking at step size $\\alpha_k = 1$ at each iteration. Report the minimizer estimate returned by the termination condition for each case. Express every reported number as a decimal rounded to exactly six digits after the decimal point.\n\nTest case A (binary choice negative log-likelihood in two parameters). Let the parameter be $x = (b_0, b_1)^\\top \\in \\mathbb{R}^2$. Let the data be $\\{(x_i,y_i)\\}_{i=1}^5$ with\n$$\n(x_1,y_1) = (-2,0),\\quad (x_2,y_2) = (-1,0),\\quad (x_3,y_3) = (0,1),\\quad (x_4,y_4) = (1,1),\\quad (x_5,y_5) = (2,0).\n$$\nDefine the negative log-likelihood\n$$\nf(x) = \\sum_{i=1}^5 \\left[ \\log\\!\\left(1 + e^{b_0 + b_1 x_i}\\right) - y_i\\,(b_0 + b_1 x_i) \\right].\n$$\nUse the initial point $x_0 = (0,0)^\\top$.\n\nTest case B (ill-conditioned quadratic). Let $x \\in \\mathbb{R}^2$. Define\n$$\nf(x) = \\tfrac{1}{2} x^\\top Q x - b^\\top x,\\quad Q = \\begin{bmatrix} 10^{-6} & 0 \\\\ 0 & 1 \\end{bmatrix},\\quad b = \\begin{bmatrix} 10^{-6} \\\\ 1 \\end{bmatrix}.\n$$\nUse the initial point $x_0 = (10,-10)^\\top$.\n\nTest case C (nonconvex one-dimensional quartic). Let $x \\in \\mathbb{R}$. Define\n$$\nf(x) = x^4 - 3 x^2 + x.\n$$\nUse the initial point $x_0 = 0.2$.\n\nOutput specification. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where the result for each test case appears in order as:\n- For Test case A: a list with two numbers $[b_0^\\star,b_1^\\star]$,\n- For Test case B: a list with two numbers $[x_1^\\star,x_2^\\star]$,\n- For Test case C: a single number $x^\\star$,\nwith every number rounded to exactly six digits after the decimal point. For example, the format must be\n$$\n\\big[ [b_0^\\star,b_1^\\star], [x_1^\\star,x_2^\\star], x^\\star \\big].\n$$\nNo spaces are permitted in the printed line. All numbers should be expressed as plain decimals.", "solution": "The problem presented is a well-defined task in numerical optimization. It requires the implementation of a hybrid iterative algorithm combining Newton's method with the steepest descent method, augmented by a backtracking line search to ensure sufficient decrease at each step. The problem is scientifically sound, resting on established principles of nonlinear optimization, and provides all necessary parameters, initial conditions, and objective functions for three distinct test cases. The problem is therefore deemed valid.\n\nThe algorithm to be implemented is an unconstrained minimization procedure for a twice continuously differentiable function $f: \\mathbb{R}^n \\to \\mathbb{R}$. Given an iterate $x_k$, the next iterate $x_{k+1}$ is found by $x_{k+1} = x_k + \\alpha_k p_k$, where $p_k$ is the search direction and $\\alpha_k$ is the step length.\n\nThe core of the algorithm is the choice of the search direction $p_k$. The primary choice is the Newton direction, obtained by solving the linear system:\n$$\n\\nabla^2 f(x_k) p_k = - \\nabla f(x_k)\n$$\nwhere $\\nabla f(x_k)$ is the gradient and $\\nabla^2 f(x_k)$ is the Hessian of $f$ at $x_k$. This direction is optimal for quadratic functions and provides rapid local convergence (quadratic) near a minimum where the Hessian is positive definite. However, the Newton direction may be undefined if the Hessian is singular, or it may not be a descent direction if the Hessian is not positive definite. A direction $p_k$ is a descent direction if $\\nabla f(x_k)^\\top p_k < 0$. For the Newton direction, $\\nabla f(x_k)^\\top p_k = -\\nabla f(x_k)^\\top (\\nabla^2 f(x_k))^{-1} \\nabla f(x_k)$, which is negative only if $\\nabla^2 f(x_k)$ is positive definite. If the Newton direction is not a descent direction or fails to yield a suitable step length, the algorithm must switch to a robust alternative.\n\nThe fallback direction is the steepest descent direction, $p_k = -\\nabla f(x_k)$. This direction is guaranteed to be a descent direction as long as the gradient is non-zero, since $\\nabla f(x_k)^\\top p_k = -\\nabla f(x_k)^\\top \\nabla f(x_k) = -\\|\\nabla f(x_k)\\|_2^2 < 0$. While its convergence is generally slower (linear), it ensures progress from any point where $\\nabla f(x_k) \\neq 0$.\n\nThe step length $\\alpha_k$ is determined by a backtracking line search. Starting with a trial step $\\alpha = 1$, the step is successively reduced by a factor $\\rho \\in (0,1)$ until the Armijo sufficient decrease condition is met:\n$$\nf(x_k + \\alpha p_k) \\le f(x_k) + c_1 \\alpha \\nabla f(x_k)^\\top p_k\n$$\nfor a constant $c_1 \\in (0,1)$. The problem sets $c_1 = 10^{-4}$ and $\\rho = 0.5$. A limit of $M=50$ backtracking steps is imposed.\n\nThe overall iterative procedure is as follows:\nFor $k = 0, 1, 2, \\ldots, K-1$:\n1. Compute the gradient $g_k = \\nabla f(x_k)$. Terminate if $\\|g_k\\|_2 \\le \\varepsilon$.\n2. Attempt to compute the Newton direction $p_{\\text{Newton}}$. This attempt fails if $\\nabla^2f(x_k)$ is singular, or if the resulting direction is not one of descent ($\\nabla f(x_k)^\\top p_{\\text{Newton}} \\ge 0$).\n3. If the Newton direction is valid, perform a backtracking line search. If a step length $\\alpha_k$ is found within $M$ reductions, the update is performed with $p_k = p_{\\text{Newton}}$ and $\\alpha_k$.\n4. If the Newton direction step fails for any reason (singularity, not a descent direction, or backtracking failure), the algorithm reverts to the steepest descent direction $p_k = -\\nabla f(x_k)$ and performs another backtracking line search to find $\\alpha_k$.\n5. Update the iterate: $x_{k+1} = x_k + \\alpha_k p_k$.\nThe process terminates if the gradient norm is below a tolerance $\\varepsilon=10^{-8}$ or if the maximum number of iterations $K=50$ is reached.\n\nThe implementation will apply this logic to three test cases. For each case, we must provide the analytical forms of the objective function $f(x)$, its gradient $\\nabla f(x)$, and its Hessian matrix $\\nabla^2 f(x)$.\n\n**Test Case A: Binary Choice Negative Log-Likelihood**\nThe objective function is the negative log-likelihood for a logistic regression model.\n$f(b_0, b_1) = \\sum_{i=1}^5 \\left[ \\log(1 + e^{b_0 + b_1 x_i}) - y_i(b_0 + b_1 x_i) \\right]$.\nLet $u_i(b) = b_0 + b_1 x_i$. The sigmoid function is $\\sigma(u) = 1/(1+e^{-u})$.\nThe gradient components are:\n$$\n\\frac{\\partial f}{\\partial b_0} = \\sum_{i=1}^5 (\\sigma(u_i) - y_i)\n$$\n$$\n\\frac{\\partial f}{\\partial b_1} = \\sum_{i=1}^5 x_i (\\sigma(u_i) - y_i)\n$$\nThe Hessian components are (using $\\sigma'(u) = \\sigma(u)(1-\\sigma(u))$):\n$$\n\\frac{\\partial^2 f}{\\partial b_0^2} = \\sum_{i=1}^5 \\sigma(u_i)(1-\\sigma(u_i))\n$$\n$$\n\\frac{\\partial^2 f}{\\partial b_1 \\partial b_0} = \\sum_{i=1}^5 x_i \\sigma(u_i)(1-\\sigma(u_i))\n$$\n$$\n\\frac{\\partial^2 f}{\\partial b_1^2} = \\sum_{i=1}^5 x_i^2 \\sigma(u_i)(1-\\sigma(u_i))\n$$\nThis Hessian is always positive semi-definite, ensuring that the Newton direction (if defined) is a descent direction.\n\n**Test Case B: Ill-Conditioned Quadratic**\nThe objective function is $f(x) = \\tfrac{1}{2} x^\\top Q x - b^\\top x$.\nThe gradient is $\\nabla f(x) = Qx - b$.\nThe Hessian is constant: $\\nabla^2 f(x) = Q$.\nSince $Q$ is a constant, positive definite matrix, Newton's method with a full step ($\\alpha=1$) will find the exact minimum $x^*=Q^{-1}b$ in a single iteration.\n\n**Test Case C: Nonconvex One-Dimensional Quartic**\nThe objective function is $f(x) = x^4 - 3x^2 + x$.\nThe gradient (derivative) is $f'(x) = 4x^3 - 6x + 1$.\nThe Hessian (second derivative) is $f''(x) = 12x^2 - 6$.\nAt the initial point $x_0 = 0.2$, the Hessian is $f''(0.2) = 12(0.04) - 6 = -5.52$, which is negative. Therefore, the Newton direction at $x_0$ will be an ascent direction, and the algorithm must fall back to steepest descent for the first iteration.\n\nThe following Python code implements the described algorithm and applies it to the three test cases, formatting the output as specified.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the optimization for all test cases and print results.\n    \"\"\"\n    # Fixed constants for the algorithm\n    C1 = 1e-4\n    RHO = 0.5\n    EPS = 1e-8\n    MAX_ITER = 50\n    BT_LIMIT = 50\n\n    def optimizer(f, grad_f, hess_f, x0):\n        \"\"\"\n        Implements the hybrid Newton/Gradient-Descent optimization algorithm.\n        \"\"\"\n        x = np.array(x0, dtype=float)\n\n        for _ in range(MAX_ITER):\n            g = grad_f(x)\n            if np.linalg.norm(g) <= EPS:\n                return x\n\n            p = None\n            alpha = None\n            newton_step_succeeded = False\n\n            # --- Attempt Newton's Method ---\n            try:\n                H = hess_f(x)\n                p_newton = np.linalg.solve(H, -g)\n\n                # Newton direction must be a descent direction\n                if np.dot(g, p_newton) < 0:\n                    # Backtracking line search for Newton's direction\n                    alpha_bt = 1.0\n                    fk = f(x)\n                    g_dot_p = np.dot(g, p_newton)\n                    for _ in range(BT_LIMIT):\n                        if f(x + alpha_bt * p_newton) <= fk + C1 * alpha_bt * g_dot_p:\n                            p = p_newton\n                            alpha = alpha_bt\n                            newton_step_succeeded = True\n                            break\n                        alpha_bt *= RHO\n            except np.linalg.LinAlgError:\n                # Hessian is singular or ill-posed\n                pass\n\n            # --- Fallback to Steepest Descent ---\n            if not newton_step_succeeded:\n                p_sd = -g\n                \n                # Backtracking line search for steepest descent\n                alpha_bt = 1.0\n                fk = f(x)\n                g_dot_p = np.dot(g, p_sd)\n                for _ in range(BT_LIMIT):\n                    if f(x + alpha_bt * p_sd) <= fk + C1 * alpha_bt * g_dot_p:\n                        p = p_sd\n                        alpha = alpha_bt\n                        break\n                    alpha_bt *= RHO\n            \n            # If no step could be found (highly unlikely for steepest descent)\n            if p is None or alpha is None:\n                return x\n\n            # Update x\n            x = x + alpha * p\n        \n        return x\n\n    # --- Test Case A: Logistic Regression ---\n    x_data_A = np.array([-2.0, -1.0, 0.0, 1.0, 2.0])\n    y_data_A = np.array([0.0, 0.0, 1.0, 1.0, 0.0])\n    x0_A = np.array([0.0, 0.0])\n\n    def f_A(b):\n        u = b[0] + b[1] * x_data_A\n        # Use np.logaddexp for numerical stability: log(1+exp(x)) = logaddexp(0,x)\n        return np.sum(np.logaddexp(0, u) - y_data_A * u)\n\n    def grad_f_A(b):\n        u = b[0] + b[1] * x_data_A\n        # Sigmoid function: 1 / (1 + exp(-u))\n        sigma = 1 / (1 + np.exp(-u))\n        diff = sigma - y_data_A\n        return np.array([np.sum(diff), np.sum(diff * x_data_A)])\n\n    def hess_f_A(b):\n        u = b[0] + b[1] * x_data_A\n        sigma = 1 / (1 + np.exp(-u))\n        weights = sigma * (1 - sigma)\n        h00 = np.sum(weights)\n        h01 = np.sum(weights * x_data_A)\n        h11 = np.sum(weights * x_data_A**2)\n        return np.array([[h00, h01], [h01, h11]])\n\n    result_A = optimizer(f_A, grad_f_A, hess_f_A, x0_A)\n\n    # --- Test Case B: Ill-Conditioned Quadratic ---\n    Q_B = np.array([[1e-6, 0.0], [0.0, 1.0]])\n    b_vec_B = np.array([1e-6, 1.0])\n    x0_B = np.array([10.0, -10.0])\n\n    def f_B(x):\n        return 0.5 * x.T @ Q_B @ x - b_vec_B.T @ x\n    \n    def grad_f_B(x):\n        return Q_B @ x - b_vec_B\n    \n    def hess_f_B(x):\n        return Q_B\n\n    result_B = optimizer(f_B, grad_f_B, hess_f_B, x0_B)\n\n    # --- Test Case C: Nonconvex Quartic ---\n    x0_C = np.array([0.2])\n\n    def f_C(x):\n        xv = x[0]\n        return xv**4 - 3 * xv**2 + xv\n    \n    def grad_f_C(x):\n        xv = x[0]\n        return np.array([4 * xv**3 - 6 * xv + 1])\n    \n    def hess_f_C(x):\n        xv = x[0]\n        return np.array([[12 * xv**2 - 6]])\n    \n    result_C = optimizer(f_C, grad_f_C, hess_f_C, x0_C)\n\n    # --- Format Output ---\n    results_list = [result_A, result_B, result_C]\n    formatted_parts = []\n    for res in results_list:\n        if res.size > 1:\n            num_strs = [f\"{val:.6f}\" for val in res]\n            formatted_parts.append(f\"[{','.join(num_strs)}]\")\n        else:\n            formatted_parts.append(f\"{res.item():.6f}\")\n\n    final_output = f\"[{','.join(formatted_parts)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "2414720"}, {"introduction": "The final step in mastering a numerical method is learning to apply it efficiently at scale. This exercise [@problem_id:2414738] introduces a large-scale optimization problem derived from a spatial economic model, where the Hessian matrix is sparse. You will learn to leverage this sparsity to create a computationally efficient implementation, demonstrating how to scale Newton's method to solve the kind of complex, high-dimensional problems encountered in modern economic modeling.", "problem": "Consider the minimization of a strictly convex objective that arises from a discretized spatial economic potential on a two-dimensional grid. Let the grid have $N_x$ rows and $N_y$ columns, and let $N = N_x N_y$. Index locations by $i \\in \\{0,1,\\dots,N-1\\}$. The mapping between a grid coordinate $(r,c)$ with $r \\in \\{0,1,\\dots,N_x-1\\}$ and $c \\in \\{0,1,\\dots,N_y-1\\}$ and the vector index is $i = r N_y + c$. Define the set of undirected edges $\\mathcal{E}$ by $4$-neighborhood adjacency: nodes $(r,c)$ and $(r',c')$ are adjacent if and only if $|r-r'|+|c-c'|=1$. Let the graph Laplacian matrix be $L \\in \\mathbb{R}^{N \\times N}$, defined by $L_{ii} = \\deg(i)$ and $L_{ij} = -1$ if $(i,j)\\in \\mathcal{E}$ and $L_{ij}=0$ otherwise. For given parameters $a_i>0$, $b_i \\in \\mathbb{R}$, $c_i \\ge 0$, and $k \\ge 0$, define the objective function\n$$\nf(x) \\;=\\; \\frac{1}{2}\\sum_{i=0}^{N-1} a_i \\left(x_i - b_i\\right)^2 \\;+\\; \\frac{k}{2}\\sum_{(i,j)\\in \\mathcal{E}} \\left(x_i - x_j\\right)^2 \\;+\\; \\sum_{i=0}^{N-1} c_i e^{x_i},\n$$\nfor $x \\in \\mathbb{R}^N$. All angles used within trigonometric functions below are in radians.\n\nYour task is to write a program that, for each parameter set in the test suite below, computes a minimizer $x^\\star \\in \\mathbb{R}^N$ of $f(x)$ to high accuracy and returns the achieved objective value $f(x^\\star)$. The implementation must be numerically stable for all given cases and must exploit the sparsity structure implied by the $4$-neighborhood grid. Use the following universal initialization and stopping criteria across all cases:\n- Initialization: $x^{(0)} = 0 \\in \\mathbb{R}^N$.\n- Stopping criteria: terminate when either the infinity norm of the gradient satisfies $\\|\\nabla f(x)\\|_\\infty \\le 10^{-8}$, or when an appropriate second-order decrement proxy indicates near-stationarity at a level comparable to $10^{-12}$, or after at most $50$ iterations. The step size selection must ensure descent of $f(x)$.\n\nTest suite of parameter sets to cover multiple regimes:\n- Case A (moderate coupling, nonlinear local terms): $N_x=4$, $N_y=4$, $k=0.5$. For $i \\in \\{0,\\dots,N-1\\}$, let $a_i = 2.0 + 0.1\\,(i+1)$, $b_i = 0.2 \\sin(i+1)$, $c_i = 0.3\\,\\bigl((i \\bmod 3)+1\\bigr)$.\n- Case B (no coupling; diagonal Hessian structure): $N_x=5$, $N_y=5$, $k=0.0$. For $i \\in \\{0,\\dots,N-1\\}$, let $a_i = 1.5 + 0.05\\,(i+1)$, $b_i = -0.1 \\cos\\bigl(2\\,(i+1)\\bigr)$, $c_i = 0.2\\,\\bigl(1 + (i \\bmod 2)\\bigr)$.\n- Case C (purely quadratic with strong coupling): $N_x=8$, $N_y=8$, $k=1.0$. For $i \\in \\{0,\\dots,N-1\\}$, let $a_i = 3.0$, $b_i = 0.1\\,\\bigl((i \\bmod 7)-3\\bigr)$, $c_i = 0.0$.\n- Case D (larger grid, moderate coupling, mild nonlinearity): $N_x=20$, $N_y=20$, $k=0.2$. For $i \\in \\{0,\\dots,N-1\\}$, let $a_i = 1.0 + 0.001\\,(i+1)$, $b_i = 0.05 \\sin\\bigl(0.1\\,(i+1)\\bigr)$, $c_i = 0.05 + 0.05\\,\\frac{(i \\bmod 5)}{5}$.\n\nAll exponentials, sines, and cosines are to be interpreted as the standard real-valued functions. There are no physical units in this problem. Each case must be solved independently from the shared initialization $x^{(0)}=0$.\n\nRequired final output format: your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Specifically, output $[F_A,F_B,F_C,F_D]$, where $F_\\bullet = f(x^\\star)$ for the corresponding case, each rounded to exactly $6$ digits after the decimal point (use standard rounding to nearest, with ties to the nearest even representable decimal if your language defines it). For example, the format must be exactly like $[1.234567,8.765432,0.000001,2.500000]$ with no spaces anywhere in the line.", "solution": "The problem requires the minimization of a multidimensional objective function $f(x)$ for a vector $x \\in \\mathbb{R}^N$. Adhering to scientific protocol, we first validate the problem statement.\n\nThe objective function is stated as\n$$\nf(x) \\;=\\; \\frac{1}{2}\\sum_{i=0}^{N-1} a_i \\left(x_i - b_i\\right)^2 \\;+\\; \\frac{k}{2}\\sum_{(i,j)\\in \\mathcal{E}} \\left(x_i - x_j\\right)^2 \\;+\\; \\sum_{i=0}^{N-1} c_i e^{x_i}.\n$$\nThe parameters are specified with constraints $a_i > 0$, $b_i \\in \\mathbb{R}$, $c_i \\ge 0$, and $k \\ge 0$. The set $\\mathcal{E}$ represents edges in a graph with a standard $4$-neighborhood structure. The definitions are mathematically precise and consistent. The problem is a standard unconstrained nonlinear optimization problem, grounded in established mathematical principles often found in computational economics, physics, and image analysis. The problem is therefore scientifically sound and well-posed.\n\nTo select a suitable minimization algorithm, we must analyze the properties of $f(x)$. The function is twice continuously differentiable. Its convexity is determined by the definiteness of its Hessian matrix, $\\nabla^2 f(x)$.\n\nLet us express the function in a compact matrix-vector form. Let $A = \\text{diag}(a_0, \\dots, a_{N-1})$ be the diagonal matrix of $a_i$ coefficients and $L$ be the graph Laplacian matrix. The quadratic term $\\frac{1}{2}\\sum_{(i,j)\\in \\mathcal{E}} (x_i - x_j)^2$ is equivalent to $\\frac{1}{2} x^T L x$, a standard result from spectral graph theory. The function can thus be written as\n$$\nf(x) = \\frac{1}{2}(x-b)^T A (x-b) + \\frac{k}{2} x^T L x + \\sum_{i=0}^{N-1} c_i e^{x_i}.\n$$\nThe gradient of $f(x)$ is the vector $\\nabla f(x) \\in \\mathbb{R}^N$ given by\n$$\n\\nabla f(x) = A(x-b) + kLx + G(x),\n$$\nwhere $G(x)$ is a vector with components $(G(x))_i = c_i e^{x_i}$.\nThe Hessian matrix $\\nabla^2 f(x) \\in \\mathbb{R}^{N \\times N}$ is\n$$\n\\nabla^2 f(x) = A + kL + \\text{diag}(c_0e^{x_0}, \\dots, c_{N-1}e^{x_{N-1}}).\n$$\nWe analyze the definiteness of this Hessian to ascertain the convexity of $f(x)$:\n1. The matrix $A$ is diagonal with entries $a_i > 0$, so it is a positive definite matrix.\n2. The graph Laplacian $L$ is a known positive semi-definite matrix. Given $k \\ge 0$, the matrix $kL$ is also positive semi-definite.\n3. The matrix $\\text{diag}(c_i e^{x_i})$ is diagonal. Since $c_i \\ge 0$ and $e^{x_i} > 0$ for all $x_i \\in \\mathbb{R}$, its diagonal entries are non-negative, making it a positive semi-definite matrix.\n\nThe Hessian $\\nabla^2 f(x)$ is the sum of a positive definite matrix ($A$) and two positive semi-definite matrices. Such a sum is invariably positive definite. A positive definite Hessian for all $x \\in \\mathbb{R}^N$ confirms that $f(x)$ is a strictly convex function. A strictly convex and coercive function (coercivity is guaranteed by the quadratic terms involving $a_i > 0$) on $\\mathbb{R}^N$ has a unique global minimizer, $x^\\star$.\n\nFor a strictly convex, twice-differentiable function where the gradient and Hessian are analytically available, Newton's method is the superior choice for its characteristic quadratic rate of convergence near the minimizer. The iterative procedure begins from an initial point $x^{(0)}$ and generates a sequence of iterates $x^{(k)}$. The Newton direction, $\\Delta x^{(k)}$, is found by solving the linear system\n$$\n\\nabla^2 f(x^{(k)}) \\Delta x^{(k)} = -\\nabla f(x^{(k)}).\n$$\nA pure Newton step $(x^{(k+1)} = x^{(k)} + \\Delta x^{(k)})$ does not guarantee convergence from a distant starting point. Therefore, we must incorporate a line search to determine a step size $\\alpha_k \\in (0, 1]$ that ensures decrease in the objective function. We employ a backtracking line search that satisfies the Armijo condition:\n$$\nf(x^{(k)} + \\alpha_k \\Delta x^{(k)}) \\le f(x^{(k)}) + \\sigma \\alpha_k (\\nabla f(x^{(k)}))^T \\Delta x^{(k)},\n$$\nfor a small constant $\\sigma \\in (0, 0.5)$, such as $\\sigma = 10^{-4}$. The full update rule is then $x^{(k+1)} = x^{(k)} + \\alpha_k \\Delta x^{(k)}$.\n\nA critical aspect of the implementation is the efficient solution of the Newton system. The Hessian matrix $\\nabla^2 f(x)$ is sparse, comprising the sum of two diagonal matrices and the sparse Laplacian $L$. For a $4$-neighborhood grid, each row of the Hessian has at most $5$ non-zero entries. This sparsity is to be exploited by using sparse matrix data structures and a tailored sparse linear solver.\n\nThe algorithm is implemented as follows:\n1. Initialize iteration counter $k=0$ and the solution vector $x^{(0)} = 0 \\in \\mathbb{R}^N$ as specified.\n2. For $k$ from $0$ up to a maximum of $49$:\n    a. Compute the gradient vector $g = \\nabla f(x^{(k)})$ and the sparse Hessian matrix $H = \\nabla^2 f(x^{(k)})$.\n    b. Check the first stopping criterion: if the infinity norm of the gradient satisfies $\\|g\\|_\\infty \\le 10^{-8}$, the process terminates.\n    c. Solve the sparse linear system $H \\Delta x = -g$ for the Newton direction $\\Delta x$.\n    d. Check the second-order stopping criterion. The half Newton decrement, $\\lambda^2/2 = -\\frac{1}{2} g^T \\Delta x$, is a measure of proximity to the minimum. If $-\\frac{1}{2} g^T \\Delta x \\le 10^{-12}$, the solution is considered converged, and we take the final step $x^{(k+1)} = x^{(k)} + \\Delta x$ and terminate.\n    e. Starting with $\\alpha=1$, perform a backtracking line search to find a step size $\\alpha_k$ satisfying the Armijo condition.\n    f. Update the solution: $x^{(k+1)} = x^{(k)} + \\alpha_k \\Delta x$.\n3. If the loop completes without convergence, the procedure stops at the maximum iteration count.\n\nThis complete algorithm is applied to each test case. The final value of the objective function $f(x^\\star)$ at the determined minimizer $x^\\star$ is then computed and reported.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.sparse import diags, lil_matrix\nfrom scipy.sparse.linalg import spsolve\n\ndef solve():\n    \"\"\"\n    Main solver function that iterates through test cases and prints the final results.\n    \"\"\"\n\n    def build_laplacian(Nx, Ny):\n        \"\"\"\n        Constructs the sparse graph Laplacian matrix for a 2D grid with 4-neighborhood.\n        \n        Args:\n            Nx (int): Number of rows in the grid.\n            Ny (int): Number of columns in the grid.\n            \n        Returns:\n            scipy.sparse.csr_matrix: The sparse Laplacian matrix L.\n        \"\"\"\n        N = Nx * Ny\n        # Use LIL format for efficient incremental construction of the sparse matrix.\n        L = lil_matrix((N, N))\n        for r in range(Nx):\n            for c in range(Ny):\n                i = r * Ny + c\n                degree = 0\n                # Neighbor up\n                if r > 0:\n                    j = (r - 1) * Ny + c\n                    L[i, j] = -1\n                    degree += 1\n                # Neighbor down\n                if r < Nx - 1:\n                    j = (r + 1) * Ny + c\n                    L[i, j] = -1\n                    degree += 1\n                # Neighbor left\n                if c > 0:\n                    j = r * Ny + (c - 1)\n                    L[i, j] = -1\n                    degree += 1\n                # Neighbor right\n                if c < Ny - 1:\n                    j = r * Ny + (c + 1)\n                    L[i, j] = -1\n                    degree += 1\n                L[i, i] = degree\n        # Convert to CSR format for fast matrix-vector products.\n        return L.tocsr()\n\n    def objective_function(x, a, b, c, k, L_sparse):\n        \"\"\"Calculates the value of the objective function f(x).\"\"\"\n        term1 = 0.5 * np.sum(a * (x - b)**2)\n        # The term (k/2) * x.T @ L @ x is computationally correct for the sum expression\n        term2 = 0.5 * k * (x.T @ (L_sparse @ x))\n        term3 = np.sum(c * np.exp(x))\n        return term1 + term2 + term3\n\n    def solve_one_case(Nx, Ny, k_param, a_vec, b_vec, c_vec):\n        \"\"\"\n        Solves the minimization problem for a single set of parameters using Newton's method.\n        \"\"\"\n        N = Nx * Ny\n        L_sparse = build_laplacian(Nx, Ny)\n        A_sparse = diags(a_vec, 0, format='csr')\n\n        x = np.zeros(N)\n        \n        # Newton's method parameters\n        max_iter = 50\n        grad_tol = 1e-8\n        decrement_tol = 1e-12\n        \n        # Line search parameters\n        beta = 0.5\n        sigma = 1e-4\n\n        for _ in range(max_iter):\n            exp_x = np.exp(x)\n            c_exp_x = c_vec * exp_x\n            \n            # Gradient: g = A(x-b) + k*L*x + c*exp(x)\n            g = A_sparse @ (x - b_vec) + k_param * (L_sparse @ x) + c_exp_x\n            \n            if np.max(np.abs(g)) <= grad_tol:\n                break\n\n            # Hessian: H = A + k*L + diag(c*exp(x))\n            H_diag = diags(c_exp_x, 0, format='csr')\n            H = A_sparse + k_param * L_sparse + H_diag\n\n            # Solve the Newton system: H*dx = -g.\n            # Convert to CSC format for efficiency with spsolve.\n            dx = spsolve(H.tocsc(), -g)\n            \n            newton_decrement_half = -0.5 * g.T @ dx\n            if newton_decrement_half <= decrement_tol:\n                x = x + dx  # Take full final step\n                break\n            \n            # Backtracking line search\n            alpha = 1.0\n            f_x = objective_function(x, a_vec, b_vec, c_vec, k_param, L_sparse)\n            slope = g.T @ dx\n\n            while True:\n                x_new = x + alpha * dx\n                f_x_new = objective_function(x_new, a_vec, b_vec, c_vec, k_param, L_sparse)\n                if f_x_new <= f_x + sigma * alpha * slope:\n                    break\n                alpha *= beta\n                if alpha < 1e-12: # Safety break\n                    break\n            \n            x = x + alpha * dx\n        \n        return objective_function(x, a_vec, b_vec, c_vec, k_param, L_sparse)\n\n    test_cases = [\n        # Case A\n        {'Nx': 4, 'Ny': 4, 'k': 0.5, \n         'a_func': lambda i: 2.0 + 0.1 * (i + 1),\n         'b_func': lambda i: 0.2 * np.sin(i + 1),\n         'c_func': lambda i: 0.3 * ((i % 3) + 1)},\n        # Case B\n        {'Nx': 5, 'Ny': 5, 'k': 0.0,\n         'a_func': lambda i: 1.5 + 0.05 * (i + 1),\n         'b_func': lambda i: -0.1 * np.cos(2 * (i + 1)),\n         'c_func': lambda i: 0.2 * (1 + (i % 2))},\n        # Case C\n        {'Nx': 8, 'Ny': 8, 'k': 1.0,\n         'a_func': lambda i: 3.0,\n         'b_func': lambda i: 0.1 * ((i % 7) - 3),\n         'c_func': lambda i: 0.0},\n        # Case D\n        {'Nx': 20, 'Ny': 20, 'k': 0.2,\n         'a_func': lambda i: 1.0 + 0.001 * (i + 1),\n         'b_func': lambda i: 0.05 * np.sin(0.1 * (i + 1)),\n         'c_func': lambda i: 0.05 + 0.05 * ((i % 5) / 5)}\n    ]\n\n    results = []\n    for case in test_cases:\n        Nx, Ny = case['Nx'], case['Ny']\n        N = Nx * Ny\n        i_vals = np.arange(N)\n        \n        a = case['a_func'](i_vals)\n        if np.isscalar(a): a = np.full(N, a, dtype=float)\n\n        b = case['b_func'](i_vals)\n        if np.isscalar(b): b = np.full(N, b, dtype=float)\n\n        c = case['c_func'](i_vals)\n        if np.isscalar(c): c = np.full(N, c, dtype=float)\n        \n        f_val = solve_one_case(Nx, Ny, case['k'], a, b, c)\n        results.append(f\"{f_val:.6f}\")\n        \n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2414738"}]}