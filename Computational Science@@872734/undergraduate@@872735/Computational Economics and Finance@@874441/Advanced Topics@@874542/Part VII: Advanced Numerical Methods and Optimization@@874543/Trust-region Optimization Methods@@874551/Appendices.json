{"hands_on_practices": [{"introduction": "At the heart of every trust-region algorithm lies the subproblem: minimizing a simple quadratic model of our objective function within a \"trust radius\" where the model is deemed reliable. This first practice grounds you in this fundamental task in a clear, one-dimensional setting [@problem_id:2224504]. By solving it, you will develop a concrete understanding of how the optimal step is determined by comparing the unconstrained model minimizer to the trust-region boundary.", "problem": "In the context of unconstrained optimization, trust-region methods iteratively approximate a complex function with a simpler model function $m(p)$ around the current point. The next step $p$ is then determined by solving the trust-region subproblem, which involves minimizing this model within a \"trust region\" of radius $\\Delta  0$, where the model is believed to be a reliable approximation of the original function. The subproblem is formally stated as:\n$$\n\\min_{p} m(p) \\quad \\text{subject to} \\quad \\|p\\| \\le \\Delta\n$$\nConsider a one-dimensional optimization scenario where the model function for a step $p \\in \\mathbb{R}$ is a quadratic given by:\n$$\nm(p) = g p + \\frac{1}{2} H p^2 + c\n$$\nwith a gradient term $g = 2$, a Hessian term $H = 6$, and an arbitrary constant $c$. The step is constrained by a trust-region radius of $\\Delta = 0.1$.\n\nDetermine the optimal step $p$ that solves this one-dimensional trust-region subproblem. Provide your answer as an exact decimal number.", "solution": "We must minimize the quadratic model $m(p) = g p + \\frac{1}{2} H p^{2} + c$ subject to the trust-region constraint $|p| \\le \\Delta$, with given values $g=2$, $H=6$, and $\\Delta=0.1$. The constant $c$ does not affect the minimizer and can be ignored.\n\nConsider the Lagrangian for the trust-region subproblem:\n$$\n\\mathcal{L}(p,\\lambda) = g p + \\frac{1}{2} H p^{2} + \\lambda \\left(p^{2} - \\Delta^{2}\\right),\n$$\nwith $\\lambda \\ge 0$. The Karush-Kuhn-Tucker conditions are:\n1. Stationarity: \n$$\n\\frac{\\partial \\mathcal{L}}{\\partial p} = g + H p + 2 \\lambda p = 0.\n$$\n2. Primal feasibility: $|p| \\le \\Delta$.\n3. Dual feasibility: $\\lambda \\ge 0$.\n4. Complementary slackness: $\\lambda \\left(p^{2} - \\Delta^{2}\\right) = 0$.\n\nCase 1 (interior solution): If $|p|  \\Delta$, then $\\lambda = 0$ and stationarity gives\n$$\ng + H p = 0 \\quad \\Rightarrow \\quad p_{u} = -\\frac{g}{H}.\n$$\nFor $g=2$ and $H=6$, this gives $p_{u} = -\\frac{2}{6} = -\\frac{1}{3}$. Check feasibility: $|p_{u}| = \\frac{1}{3}  0.1 = \\Delta$, so the interior solution is infeasible.\n\nCase 2 (boundary solution): Then $p^{2} = \\Delta^{2}$, so $p = \\pm \\Delta$. Stationarity becomes\n$$\ng + H p + 2 \\lambda p = 0 \\quad \\Rightarrow \\quad (H + 2 \\lambda) p = -g.\n$$\nSince $H + 2 \\lambda \\ge 0$, the sign of $p$ must match the sign of $-g$. With $g = 2  0$, we must take $p = -\\Delta = -0.1$. To verify dual feasibility, solve for $\\lambda$:\n$$\n(H + 2 \\lambda)(-\\Delta) = -g \\quad \\Rightarrow \\quad (H + 2 \\lambda)\\Delta = g \\quad \\Rightarrow \\quad 2 \\lambda = \\frac{g}{\\Delta} - H.\n$$\nSubstituting $g=2$, $\\Delta=0.1$, and $H=6$ gives\n$$\n2 \\lambda = \\frac{2}{0.1} - 6 = 20 - 6 = 14 \\quad \\Rightarrow \\quad \\lambda = 7 \\ge 0,\n$$\nwhich satisfies dual feasibility and complementary slackness.\n\nTherefore, the optimal trust-region step is the boundary step in the negative gradient direction:\n$$\np^{\\star} = -\\Delta = -0.1.\n$$", "answer": "$$\\boxed{-0.1}$$", "id": "2224504"}, {"introduction": "Having mastered the basic subproblem, we now explore a scenario that highlights a key advantage of trust-region methods: navigating non-convex landscapes. This exercise demonstrates how the method intelligently handles an indefinite Hessian at a saddle point, a common challenge in complex optimization problems [@problem_id:2444737]. You will see how the algorithm leverages directions of negative curvature to escape the saddle and continue making progress, a capability that simpler gradient-based methods often lack.", "problem": "Consider the unconstrained minimization of the quadratic objective in two variables,\n$$f(x) = -x_{1}^{2} + x_{2}^{2}, \\quad x = \\begin{pmatrix} x_{1} \\\\ x_{2} \\end{pmatrix} \\in \\mathbb{R}^{2}.$$\nAt the current iterate\n$$x_{k} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix},$$\na trust-region method with Euclidean norm and trust-region radius $\\Delta  0$ forms the quadratic model\n$$m_{k}(s) = f(x_{k}) + \\nabla f(x_{k})^{\\top} s + \\tfrac{1}{2}\\, s^{\\top} \\nabla^{2} f(x_{k})\\, s,$$\nand computes the exact trust-region step by solving\n$$\\min_{s \\in \\mathbb{R}^{2}} \\; m_{k}(s) \\quad \\text{subject to} \\quad \\|s\\|_{2} \\le \\Delta.$$\nAmong all global minimizers, adopt the convention that the chosen step has a nonnegative first component.\n\nDetermine the exact trust-region step $s_{k}$ as a function of $\\Delta$. Provide your answer as a $1 \\times 2$ row vector. No numerical rounding is required.", "solution": "We begin from the definitions of gradient and Hessian for the quadratic objective. The objective is\n$$f(x) = -x_{1}^{2} + x_{2}^{2},$$\nso its gradient and Hessian are, for any $x \\in \\mathbb{R}^{2}$,\n$$\\nabla f(x) = \\begin{pmatrix} -2 x_{1} \\\\ 2 x_{2} \\end{pmatrix}, \\qquad \\nabla^{2} f(x) = \\begin{pmatrix} -2  0 \\\\ 0  2 \\end{pmatrix}.$$\nAt the trust-region center $x_{k} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$, the gradient is\n$$\\nabla f(x_{k}) = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}.$$\nThe quadratic model at $x_{k}$ is therefore\n$$m_{k}(s) = f(x_{k}) + \\nabla f(x_{k})^{\\top} s + \\tfrac{1}{2}\\, s^{\\top} \\nabla^{2} f(x_{k})\\, s = f(0) + \\tfrac{1}{2}\\, s^{\\top} H s,$$\nwhere we denote $H \\coloneqq \\nabla^{2} f(x_{k}) = \\begin{pmatrix} -2  0 \\\\ 0  2 \\end{pmatrix}$. The constant term $f(0)$ is irrelevant for the minimizer of $m_{k}$, so the trust-region subproblem reduces to\n$$\\min_{\\|s\\|_{2} \\le \\Delta} \\; \\tfrac{1}{2}\\, s^{\\top} H s.$$\nBecause $H$ is symmetric and indefinite, we analyze its eigenstructure. The eigenpairs of $H$ are\n$$(\\lambda_{1}, v_{1}) = (-2, \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}), \\qquad (\\lambda_{2}, v_{2}) = (2, \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}).$$\nFor any nonzero $s$, write $s = r u$ with $r = \\|s\\|_{2}$ and $\\|u\\|_{2} = 1$. Then\n$$\\tfrac{1}{2}\\, s^{\\top} H s = \\tfrac{1}{2}\\, r^{2}\\, u^{\\top} H u.$$\nFor fixed $r$, minimizing over unit vectors $u$ is equivalent to minimizing the Rayleigh quotient $u^{\\top} H u$, which is minimized by any eigenvector associated with the smallest eigenvalue of $H$. Since the smallest eigenvalue is $\\lambda_{\\min} = -2$ with eigenvectors $\\pm v_{1}$, the minimizers on the sphere $\\|s\\|_{2} = r$ are $s = r (\\pm v_{1})$.\n\nWithin the trust region $\\|s\\|_{2} \\le \\Delta$, the objective value decreases as $r$ increases along a direction of negative curvature, so the constrained minimizer lies on the boundary with $r = \\Delta$ and direction $u$ equal to an eigenvector for $\\lambda_{\\min}$. Imposing the selection convention that the first component of $s$ be nonnegative chooses $u = v_{1} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ rather than $-v_{1}$. Hence, the exact trust-region step is\n$$s_{k} = \\Delta \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}.$$\nThis step aligns with the direction of negative curvature of the model and thus moves along the unstable manifold of the saddle point, demonstrating how the trust-region method escapes the saddle along negative curvature.\n\nExpressed as a $1 \\times 2$ row vector, the step is\n$$\\begin{pmatrix} \\Delta  0 \\end{pmatrix}.$$", "answer": "$$\\boxed{\\begin{pmatrix}\\Delta  0\\end{pmatrix}}$$", "id": "2444737"}, {"introduction": "The power of a trust-region method lies not just in solving the subproblem, but also in its ability to adapt. The size of the trust region is not fixed; it expands or contracts based on the quality of the quadratic model's predictions. This practice focuses on this crucial self-correcting mechanism by examining a case where a poor model leads to a bad step [@problem_id:2444749]. By calculating the acceptance ratio and applying the update rule, you will understand how the algorithm maintains robustness by intelligently managing its \"trust\" in the model.", "problem": "Consider a calibration subproblem that arises in Maximum Likelihood Estimation (MLE) for a binary choice model in computational finance. Let the per-observation negative log-likelihood for a Bernoulli outcome be modeled as a function of a single scalar index $x$ by\n$$\nf(x) \\;=\\; \\ln\\!\\big(1+\\exp(x)\\big) \\;-\\; \\bar{y}\\,x,\n$$\nwhere $\\bar{y}\\in(0,1)$ is the sample mean of observed outcomes. At trust-region iteration $k$, suppose the current iterate is $x_k = 0$, and the sample mean is $\\bar{y} = 0.51$. The trust-region quadratic model about $x_k$ is\n$$\nm_k(p) \\;=\\; f(x_k) \\;+\\; g_k\\,p \\;+\\; \\tfrac{1}{2}\\,B_k\\,p^2,\n$$\nwith gradient $g_k = \\nabla f(x_k)$ and a symmetric Hessian approximation $B_k$. Assume $B_k = 0.01$, which severely underestimates the true curvature at $x_k$. The trust-region radius is $\\Delta_k = 1$, and the step used is the Cauchy point along the negative gradient direction, namely in one dimension\n$$\np_k \\;=\\; \\Delta_k\\,\\mathrm{sign}(-g_k).\n$$\nDefine the trust-region acceptance ratio\n$$\n\\rho_k \\;=\\; \\frac{f(x_k) - f(x_k + p_k)}{m_k(0) - m_k(p_k)}.\n$$\nThe trust-region radius update policy is\n- if $\\rho_k  0$, set $\\Delta_{k+1} = 0.1\\,\\Delta_k$,\n- if $0 \\le \\rho_k  0.25$, set $\\Delta_{k+1} = 0.5\\,\\Delta_k$,\n- if $\\rho_k \\ge 0.25$, set $\\Delta_{k+1} = 2\\,\\Delta_k$.\n\nCompute $\\Delta_{k+1}$. Express your final answer as a pure number. No rounding is required.", "solution": "The problem requires the computation of the next trust-region radius, $\\Delta_{k+1}$, based on the current iteration's data and a specified update rule. This requires the calculation of the trust-region acceptance ratio, $\\rho_k$.\n\nThe acceptance ratio is defined as:\n$$\n\\rho_k \\;=\\; \\frac{f(x_k) - f(x_k + p_k)}{m_k(0) - m_k(p_k)}\n$$\nThis ratio compares the actual reduction in the objective function, $ared_k = f(x_k) - f(x_k + p_k)$, to the predicted reduction from the quadratic model, $pred_k = m_k(0) - m_k(p_k)$. We must compute these two quantities.\n\nFirst, let us determine the quantities related to the current iterate $x_k=0$. The objective function is given by:\n$$\nf(x) \\;=\\; \\ln(1+\\exp(x)) - \\bar{y}x\n$$\nWith the sample mean $\\bar{y} = 0.51$, the function is:\n$$\nf(x) \\;=\\; \\ln(1+\\exp(x)) - 0.51x\n$$\nAt the current iterate $x_k=0$, the function value is:\n$$\nf(x_k) = f(0) = \\ln(1+\\exp(0)) - 0.51(0) = \\ln(1+1) = \\ln(2)\n$$\nTo find the step $p_k$, we first compute the gradient $g_k = \\nabla f(x_k) = f'(x_k)$. The derivative of $f(x)$ is:\n$$\nf'(x) = \\frac{d}{dx} \\left( \\ln(1+\\exp(x)) - 0.51x \\right) = \\frac{\\exp(x)}{1+\\exp(x)} - 0.51\n$$\nEvaluating the gradient at $x_k=0$:\n$$\ng_k = f'(0) = \\frac{\\exp(0)}{1+\\exp(0)} - 0.51 = \\frac{1}{1+1} - 0.51 = 0.5 - 0.51 = -0.01\n$$\nThe step $p_k$ is given as the Cauchy point, which for this one-dimensional problem is specified as $p_k = \\Delta_k\\,\\mathrm{sign}(-g_k)$. With the trust-region radius $\\Delta_k=1$:\n$$\np_k = 1 \\cdot \\mathrm{sign}(-(-0.01)) = \\mathrm{sign}(0.01) = 1\n$$\nThe new point is $x_k + p_k = 0 + 1 = 1$.\n\nNow, we compute the actual reduction, $ared_k = f(x_k) - f(x_k + p_k)$. We have $f(x_k) = \\ln(2)$. The function value at the new point is:\n$$\nf(x_k + p_k) = f(1) = \\ln(1+\\exp(1)) - 0.51(1) = \\ln(1+e) - 0.51\n$$\nThe actual reduction is:\n$$\nared_k = \\ln(2) - \\left( \\ln(1+e) - 0.51 \\right) = \\ln(2) - \\ln(1+e) + 0.51\n$$\nNext, we compute the predicted reduction, $pred_k = m_k(0) - m_k(p_k)$. The quadratic model is:\n$$\nm_k(p) = f(x_k) + g_k p + \\frac{1}{2} B_k p^2\n$$\nAt $p=0$, we have $m_k(0) = f(x_k)$. The predicted reduction is therefore:\n$$\npred_k = m_k(0) - m_k(p_k) = f(x_k) - \\left( f(x_k) + g_k p_k + \\frac{1}{2} B_k p_k^2 \\right) = -g_k p_k - \\frac{1}{2} B_k p_k^2\n$$\nUsing the values $g_k = -0.01$, $p_k=1$, and the given Hessian approximation $B_k=0.01$:\n$$\npred_k = -(-0.01)(1) - \\frac{1}{2}(0.01)(1)^2 = 0.01 - 0.005 = 0.005\n$$\nThe acceptance ratio is:\n$$\n\\rho_k = \\frac{ared_k}{pred_k} = \\frac{\\ln(2) - \\ln(1+e) + 0.51}{0.005}\n$$\nTo apply the trust-region radius update rule, we must determine which range $\\rho_k$ falls into. This requires determining the sign of the numerator, $N = \\ln(2) - \\ln(1+e) + 0.51$.\nThe denominator $0.005$ is positive. The sign of $\\rho_k$ is the sign of $N$.\nWe can rewrite the numerator as $N = \\ln\\left(\\frac{2}{1+e}\\right) + 0.51$.\nLet us establish bounds for the terms. We know $e \\approx 2.718$.\nThus, $1+e \\approx 3.718$.\nThe term $\\ln(2) \\approx 0.693$.\nThe term $\\ln(1+e) = \\ln(1+\\exp(1))  \\ln(1+2.7) = \\ln(3.7)$. Since $e  3.7  e^2$, we have $1 = \\ln(e)  \\ln(3.7)  2$. More precisely, $\\ln(3.7) \\approx 1.308$.\nSo, $\\ln(2) - \\ln(1+e)$ is approximately $0.693 - 1.308 = -0.615$.\nThe numerator is $N \\approx -0.615 + 0.51 = -0.105$. The numerator is negative.\nA more formal argument: We wish to prove $f(1)  f(0)$, which is equivalent to $ared_k  0$. This means we must show $\\ln(1+e) - 0.51  \\ln(2)$, or $\\ln\\left(\\frac{1+e}{2}\\right)  0.51$. This is equivalent to $\\frac{1+e}{2}  \\exp(0.51)$.\nUsing the known bounds $2.71  e  2.72$, we have $3.71  1+e  3.72$, so $1.855  \\frac{1+e}{2}  1.86$.\nFor the other side, using the inequality $e^x  \\frac{1}{1-x}$ for $x \\in (0,1)$, we have $\\exp(0.51)  \\frac{1}{1-0.51} = \\frac{1}{0.49} \\approx 2.04$. This bound is not tight enough.\nUsing $e^x  1+x$, we have $e^{0.51}  1.51$.\nLet us use a tighter bound from Taylor series: $e^x \\approx 1+x+\\frac{x^2}{2}$.\n$e^{0.51} \\approx 1 + 0.51 + \\frac{0.51^2}{2} = 1.51 + \\frac{0.2601}{2} = 1.51+0.13005 = 1.64005$.\nThe value of $\\frac{1+e}{2} \\approx 1.859$ is clearly greater than $1.64$.\nHence, $f(1)  f(0)$, which implies that the actual reduction $ared_k = f(0)-f(1)$ is negative.\nSince $ared_k  0$ and $pred_k = 0.005  0$, the ratio $\\rho_k = \\frac{ared_k}{pred_k}$ is negative.\n\nThe trust-region radius update policy is:\n- if $\\rho_k  0$, set $\\Delta_{k+1} = 0.1\\,\\Delta_k$,\n- if $0 \\le \\rho_k  0.25$, set $\\Delta_{k+1} = 0.5\\,\\Delta_k$,\n- if $\\rho_k \\ge 0.25$, set $\\Delta_{k+1} = 2\\,\\Delta_k$.\n\nAs we have established that $\\rho_k  0$, the first rule applies.\nGiven $\\Delta_k = 1$, the new radius is:\n$$\n\\Delta_{k+1} = 0.1 \\cdot \\Delta_k = 0.1 \\cdot 1 = 0.1\n$$\nThe problem is now solved. The next trust-region radius is $0.1$.", "answer": "$$\n\\boxed{0.1}\n$$", "id": "2444749"}]}