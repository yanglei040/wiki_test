{"hands_on_practices": [{"introduction": "Gradient descent is often visualized as a ball rolling downhill, but on a complex landscape with multiple valleys, the starting point is everything. This exercise explores the critical concept of non-convexity, where a tiny difference in initialization can determine whether the algorithm finds the true global minimum or gets permanently stuck in a suboptimal one. Through this thought experiment, you will gain a deep appreciation for the local nature of gradient-based methods and their sensitivity to initial conditions [@problem_id:2375265].", "problem": "A researcher in computational finance is calibrating a scalar preference parameter $\\theta$ in a representative agent asset-pricing model by minimizing a stylized nonconvex loss that captures misspecification and a weak empirical tilt. The loss function is\n$$\nL(\\theta) \\;=\\; (\\theta^{2} - 1)^{2} \\;-\\; 0.2\\,\\theta.\n$$\nThe researcher applies standard gradient descent with a constant step size $\\alpha$, using the update\n$$\n\\theta_{t+1} \\;=\\; \\theta_{t} \\;-\\; \\alpha\\,\\nabla L(\\theta_{t}).\n$$\nConsider $\\alpha = 0.1$ and the two initializations $\\theta_{0}^{(-)} = -0.051$ and $\\theta_{0}^{(+)} = -0.049$, which differ by $0.002$. Which of the following best characterizes the asymptotic behavior of the two gradient descent runs and identifies whether a suboptimal minimum is reached?\n\nA. Both $\\theta_{0}^{(-)}$ and $\\theta_{0}^{(+)}$ converge to the left local minimum near $\\theta \\approx -1$, which is the global minimum.\n\nB. $\\theta_{0}^{(-)}$ converges to the right local minimum near $\\theta \\approx 1$ (global), while $\\theta_{0}^{(+)}$ converges to the left local minimum near $\\theta \\approx -1$ (suboptimal).\n\nC. $\\theta_{0}^{(-)}$ converges to the left local minimum near $\\theta \\approx -1$ (suboptimal), while $\\theta_{0}^{(+)}$ converges to the right local minimum near $\\theta \\approx 1$ (global).\n\nD. Both sequences diverge to $\\pm\\infty$ because $\\alpha = 0.1$ is too large for stability on this loss.", "solution": "The first duty in any scientific inquiry is to validate the problem statement. A flawed premise leads to a meaningless conclusion.\n\n### Step 1: Extract Givens\n\nThe problem provides the following information:\n- Loss function: $L(\\theta) = (\\theta^{2} - 1)^{2} - 0.2\\,\\theta$.\n- Gradient descent update rule: $\\theta_{t+1} = \\theta_{t} - \\alpha\\,\\nabla L(\\theta_{t})$.\n- Step size: $\\alpha = 0.1$.\n- Initial conditions: $\\theta_{0}^{(-)} = -0.051$ and $\\theta_{0}^{(+)} = -0.049$.\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem is analyzed against the required criteria for validity.\n- **Scientifically Grounded**: The problem describes the minimization of a scalar polynomial function using the standard gradient descent algorithm. This is a fundamental and well-understood problem in numerical optimization and calculus. The function is nonconvex, which presents a common challenge in optimization, making the problem non-trivial and relevant. The context of \"computational finance\" is merely a setting; the core problem is purely mathematical. It is scientifically sound.\n- **Well-Posed**: The loss function is explicitly defined. The update rule, step size, and initial conditions are given as precise numerical values. The question asks for the asymptotic behavior of the sequences, which is a deterministic outcome of the given setup. The problem is well-posed.\n- **Objective**: The problem is stated with objective and precise mathematical language. There are no subjective or ambiguous terms.\n\nThe problem statement does not violate any criteria for validity. It is a standard, self-contained, and solvable mathematical problem.\n\n### Step 3: Verdict and Action\n\nThe problem statement is **valid**. We proceed to the solution.\n\n### Derivation of Solution\n\nThe solution requires an analysis of the topology of the loss function $L(\\theta)$ and the dynamics of the gradient descent algorithm.\n\nFirst, we must identify the critical points of the loss function by finding the roots of its gradient. The loss function is $L(\\theta) = \\theta^4 - 2\\theta^2 + 1 - 0.2\\theta$.\nThe gradient (first derivative) is:\n$$\n\\nabla L(\\theta) = L'(\\theta) = \\frac{d}{d\\theta} \\left( \\theta^4 - 2\\theta^2 + 1 - 0.2\\theta \\right) = 4\\theta^3 - 4\\theta - 0.2\n$$\nThe critical points $\\theta^*$ are the solutions to $L'(\\theta^*) = 0$:\n$$\n4(\\theta^*)^3 - 4\\theta^* - 0.2 = 0\n$$\nThis is a cubic equation. To understand its roots, we observe that for the unperturbed equation $4\\theta^3 - 4\\theta = 0$, the roots are $\\theta = 0, \\pm 1$. The term $-0.2$ is a small perturbation.\n- Near $\\theta = 0$, $L'(\\theta) \\approx -4\\theta - 0.2$. Setting this to $0$ gives $-4\\theta \\approx 0.2$, so $\\theta \\approx -0.05$.\n- Near $\\theta = 1$, let $\\theta = 1+\\epsilon$. $L'(1+\\epsilon) = 4(1+\\epsilon)^3 - 4(1+\\epsilon) - 0.2 \\approx 4(1+3\\epsilon) - 4(1+\\epsilon) - 0.2 = 8\\epsilon - 0.2$. Setting this to $0$ gives $8\\epsilon \\approx 0.2$, so $\\epsilon \\approx 0.025$, yielding a root near $\\theta \\approx 1.025$.\n- Near $\\theta = -1$, let $\\theta = -1+\\epsilon$. $L'(-1+\\epsilon) = 4(-1+\\epsilon)^3 - 4(-1+\\epsilon) - 0.2 \\approx 4(-1+3\\epsilon) - 4(-1+\\epsilon) - 0.2 = 8\\epsilon - 0.2$. Setting this to $0$ gives $8\\epsilon \\approx 0.2$, so $\\epsilon \\approx 0.025$, yielding a root near $\\theta \\approx -1 + 0.025 = -0.975$.\n\nSo, we have three critical points, approximately $\\theta_1^* \\approx -0.975$, $\\theta_2^* \\approx -0.05$, and $\\theta_3^* \\approx 1.025$.\n\nNext, we classify these critical points using the second derivative test. The second derivative is:\n$$\nL''(\\theta) = \\frac{d}{d\\theta} (4\\theta^3 - 4\\theta - 0.2) = 12\\theta^2 - 4\n$$\n- At $\\theta_1^* \\approx -0.975$: $L''(-0.975) = 12(-0.975)^2 - 4 > 12(0.9)^2 - 4 = 12(0.81) - 4 = 9.72 - 4 > 0$. This is a local minimum.\n- At $\\theta_2^* \\approx -0.05$: $L''(-0.05) = 12(-0.05)^2 - 4 = 12(0.0025) - 4 = 0.03 - 4  0$. This is a local maximum.\n- At $\\theta_3^* \\approx 1.025$: $L''(1.025) = 12(1.025)^2 - 4 > 12(1)^2 - 4 = 8  0$. This is a local minimum.\n\nWe have two local minima and one local maximum. To determine the global minimum, we must compare the values of $L(\\theta)$ at the two minima.\n- $L(\\theta_1^*) \\approx L(-0.975) = ((-0.975)^2 - 1)^2 - 0.2(-0.975) \\approx (0.95 - 1)^2 + 0.195 = 0.0025 + 0.195 = 0.1975$.\n- $L(\\theta_3^*) \\approx L(1.025) = ((1.025)^2 - 1)^2 - 0.2(1.025) \\approx (1.05 - 1)^2 - 0.205 = 0.0025 - 0.205 = -0.2025$.\nSince $L(\\theta_3^*)  L(\\theta_1^*)$, the local minimum near $\\theta \\approx 1.025$ is the **global minimum**, and the local minimum near $\\theta \\approx -0.975$ is a **suboptimal local minimum**.\n\nThe local maximum at $\\theta_2^* \\approx -0.05$ acts as a separatrix, or a \"watershed,\" for the gradient flow. Initial points to the left of this maximum will descend into the left basin of attraction, while points to the right will descend into the right basin.\n\nThe initializations are $\\theta_{0}^{(-)} = -0.051$ and $\\theta_{0}^{(+)} = -0.049$. These are strategically placed on either side of the local maximum at $\\theta_2^* \\approx -0.05$.\n- For $\\theta_{0}^{(-)} = -0.051$: This point is slightly to the left of the local maximum. The gradient $L'(\\theta)$ must be positive in the interval immediately to the left of a local maximum. Let's verify: $L'(-0.051) = 4(-0.051)^3 - 4(-0.051) - 0.2 \\approx 4(-0.00013) + 0.204 - 0.2 = -0.00052 + 0.004  0$.\nThe gradient descent update is $\\theta_{1}^{(-)} = \\theta_{0}^{(-)} - \\alpha L'(\\theta_{0}^{(-)})$. Since $L'(\\theta_{0}^{(-)})  0$, the update will subtract a positive quantity, moving the parameter to the left: $\\theta_{1}^{(-)}  \\theta_{0}^{(-)}$. This trajectory will converge to the left local minimum, which is suboptimal.\n- For $\\theta_{0}^{(+)} = -0.049$: This point is slightly to the right of the local maximum. The gradient $L'(\\theta)$ must be negative in the interval immediately to the right of a local maximum. Let's verify: $L'(-0.049) = 4(-0.049)^3 - 4(-0.049) - 0.2 \\approx 4(-0.00012) + 0.196 - 0.2 = -0.00048 - 0.004  0$.\nThe update is $\\theta_{1}^{(+)} = \\theta_{0}^{(+)} - \\alpha L'(\\theta_{0}^{(+)})$. Since $L'(\\theta_{0}^{(+)})  0$, the update will add a positive quantity, moving the parameter to the right: $\\theta_{1}^{(+)}  \\theta_{0}^{(+)}$. This trajectory will converge to the right local minimum, which is the global minimum.\n\nFinally, we assess the claim of divergence. For gradient descent to converge to a minimum $\\theta^*$, a sufficient condition on the step size is $0  \\alpha  2/L''(\\theta^*)$.\n- At the left minimum ($\\theta_1^*$), $L''(\\theta_1^*) \\approx 7.4$. The condition is $\\alpha  2/7.4 \\approx 0.27$. Our $\\alpha = 0.1$ satisfies this.\n- At the right minimum ($\\theta_3^*$), $L''(\\theta_3^*) \\approx 8.6$. The condition is $\\alpha  2/8.6 \\approx 0.23$. Our $\\alpha = 0.1$ satisfies this as well.\nThe step size $\\alpha = 0.1$ is sufficiently small to ensure convergence, so the sequences will not diverge.\n\n### Option-by-Option Analysis\n\nA. **Both $\\theta_{0}^{(-)}$ and $\\theta_{0}^{(+)}$ converge to the left local minimum near $\\theta \\approx -1$, which is the global minimum.**\nThe initial points are on opposite sides of a local maximum and thus fall into different basins of attraction. They do not converge to the same minimum. Furthermore, the left minimum is suboptimal, not global.\nVerdict: **Incorrect**.\n\nB. **$\\theta_{0}^{(-)}$ converges to the right local minimum near $\\theta \\approx 1$ (global), while $\\theta_{0}^{(+)}$ converges to the left local minimum near $\\theta \\approx -1$ (suboptimal).**\nThis is the reverse of our derived behavior. $\\theta_{0}^{(-)}$ is pushed left, and $\\theta_{0}^{(+)}$ is pushed right.\nVerdict: **Incorrect**.\n\nC. **$\\theta_{0}^{(-)}$ converges to the left local minimum near $\\theta \\approx -1$ (suboptimal), while $\\theta_{0}^{(+)}$ converges to the right local minimum near $\\theta \\approx 1$ (global).**\nThis matches our analysis precisely. $\\theta_{0}^{(-)} = -0.051$ lies in the basin of attraction of the suboptimal left minimum. $\\theta_{0}^{(+)} = -0.049$ lies in the basin of attraction of the global right minimum.\nVerdict: **Correct**.\n\nD. **Both sequences diverge to $\\pm\\infty$ because $\\alpha = 0.1$ is too large for stability on this loss.**\nOur analysis of the step size condition shows that $\\alpha = 0.1$ is well within the stable range for convergence to either minimum. The claim of divergence is false.\nVerdict: **Incorrect**.", "answer": "$$\\boxed{C}$$", "id": "2375265"}, {"introduction": "The mathematical guarantee that gradient descent makes progress relies on the smoothness of the objective function. This practice challenges that assumption by introducing a function with a sudden jump, a common feature in models with fixed costs or regulatory triggers. You will investigate how a standard gradient update, while locally \"correct,\" can step across this discontinuity and paradoxically increase the loss, highlighting a fundamental limitation you must be aware of when modeling real-world problems [@problem_id:2375204].", "problem": "Consider a one-parameter optimization problem motivated by a regulatory capital surcharge in finance. Let the objective be the function $J(\\theta) = (\\theta - a)^{2} + \\lambda \\,\\mathbf{1}\\{\\theta \\ge \\tau\\}$, where $\\theta \\in \\mathbb{R}$ is a scalar decision variable (e.g., a leverage ratio), $a \\in \\mathbb{R}$ is an unconstrained target, $\\lambda \\in \\mathbb{R}_{+}$ is a fixed surcharge that is incurred once the ratio reaches the regulatory threshold, and $\\mathbf{1}\\{\\cdot\\}$ is the indicator function. Take $a = 1$, $\\lambda = 0.5$, and $\\tau = 0.8$. The algorithm applies standard gradient descent: for any iterate $\\theta_{k}$ at which $\\nabla J(\\theta_{k})$ exists, the next iterate is $\\theta_{k+1} = \\theta_{k} - \\alpha \\,\\nabla J(\\theta_{k})$, with constant step size $\\alpha = 0.2$, starting from $\\theta_{0} = 0.75$.\n\nWhich statement about the first update and its effect on the objective value is correct?\n\nA. The update yields $\\theta_{1} = 0.85 \\ge \\tau$, and $J(\\theta_{1})  J(\\theta_{0})$ due to activation of the surcharge at $\\theta \\ge \\tau$.\n\nB. The update yields $\\theta_{1} = 0.85 \\ge \\tau$, and $J(\\theta_{1})  J(\\theta_{0})$ because any sufficiently small step in gradient descent decreases the objective even with discontinuities.\n\nC. The update cannot be computed because $\\nabla J(\\theta_{0})$ does not exist.\n\nD. The update yields $\\theta_{1} = 0.65  \\tau$, and $J(\\theta_{1})  J(\\theta_{0})$ because the step remains below the threshold.", "solution": "The validity of the problem statement must be established before any attempt at a solution.\n\n### Step 1: Extract Givens\nThe problem provides the following data and definitions:\n- Objective function: $J(\\theta) = (\\theta - a)^{2} + \\lambda \\,\\mathbf{1}\\{\\theta \\ge \\tau\\}$\n- Decision variable: $\\theta \\in \\mathbb{R}$\n- Target parameter: $a = 1$\n- Surcharge parameter: $\\lambda = 0.5$, with $\\lambda \\in \\mathbb{R}_{+}$\n- Threshold parameter: $\\tau = 0.8$\n- Indicator function: $\\mathbf{1}\\{\\cdot\\}$ is defined as $1$ if the condition inside is true, and $0$ otherwise.\n- Algorithm: Standard gradient descent, $\\theta_{k+1} = \\theta_{k} - \\alpha \\,\\nabla J(\\theta_{k})$, for any iterate $\\theta_{k}$ where the gradient $\\nabla J(\\theta_{k})$ exists.\n- Step size: $\\alpha = 0.2$ (constant)\n- Initial point: $\\theta_{0} = 0.75$\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed for validity against established scientific and mathematical principles.\n\n1.  **Scientific Grounding**: The problem describes a simple one-dimensional optimization task. The objective function is a sum of a standard quadratic loss, $(\\theta - a)^2$, and a discontinuous penalty term, $\\lambda \\,\\mathbf{1}\\{\\theta \\ge \\tau\\}$. This structure is a valid and common simplified model in areas like economics, finance, and engineering to represent fixed costs or regulatory penalties that are triggered at a specific threshold. The application of gradient descent to such a function is a standard topic in non-smooth optimization. The problem is mathematically and conceptually sound.\n\n2.  **Well-Posedness**: The problem is well-posed. All parameters ($a, \\lambda, \\tau, \\alpha, \\theta_{0}$) are specified, and the task is clearly defined: compute the first iterate $\\theta_{1}$ and compare the objective value $J(\\theta_{1})$ with $J(\\theta_{0})$. The function $J(\\theta)$ is non-differentiable at exactly one point, $\\theta = \\tau$. The initial point $\\theta_{0} = 0.75$ is not the point of non-differentiability, so the gradient at the start, $\\nabla J(\\theta_{0})$, is well-defined. This allows for the computation of a single, unique update.\n\n3.  **Objectivity**: The problem is stated using precise mathematical language without ambiguity or subjective claims.\n\nThe problem statement has no scientific or mathematical flaws, contains all necessary information, and poses a clear, answerable question. It is therefore valid.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will proceed with the derivation and solution.\n\n### Derivation\nFirst, we substitute the specified parameter values into the objective function:\n$$ J(\\theta) = (\\theta - 1)^{2} + 0.5 \\cdot \\mathbf{1}\\{\\theta \\ge 0.8\\} $$\nThe initial point is $\\theta_{0} = 0.75$. We evaluate the objective function at this point. Since $\\theta_{0} = 0.75  0.8$, the indicator function $\\mathbf{1}\\{0.75 \\ge 0.8\\}$ evaluates to $0$.\n$$ J(\\theta_{0}) = J(0.75) = (0.75 - 1)^{2} + 0.5 \\cdot 0 = (-0.25)^{2} = 0.0625 $$\n\nNext, we must compute the gradient of $J(\\theta)$ to perform the gradient descent update. The function $J(\\theta)$ is differentiable everywhere except at the point of discontinuity, $\\theta = \\tau = 0.8$.\nFor any $\\theta  0.8$, the function is locally equivalent to $J(\\theta) = (\\theta - 1)^{2}$. The gradient (which is a simple derivative in this scalar case) is:\n$$ \\nabla J(\\theta) = \\frac{d}{d\\theta}(\\theta - 1)^{2} = 2(\\theta - 1) $$\nFor any $\\theta  0.8$, the function is locally equivalent to $J(\\theta) = (\\theta - 1)^{2} + 0.5$. The gradient is:\n$$ \\nabla J(\\theta) = \\frac{d}{d\\theta}((\\theta - 1)^{2} + 0.5) = 2(\\theta - 1) $$\nNote that the gradient is the same on both sides of the discontinuity. The gradient does not exist at $\\theta = 0.8$.\n\nOur starting point is $\\theta_{0} = 0.75$, which is less than $0.8$. Therefore, the gradient at $\\theta_0$ is well-defined.\n$$ \\nabla J(\\theta_{0}) = \\nabla J(0.75) = 2(0.75 - 1) = 2(-0.25) = -0.5 $$\n\nThe gradient descent update rule is $\\theta_{1} = \\theta_{0} - \\alpha \\nabla J(\\theta_{0})$. With $\\alpha = 0.2$:\n$$ \\theta_{1} = 0.75 - (0.2) \\cdot (-0.5) = 0.75 + 0.1 = 0.85 $$\n\nNow, we evaluate the objective function at the new iterate, $\\theta_{1} = 0.85$. Since $\\theta_{1} = 0.85 \\ge 0.8$, the indicator function $\\mathbf{1}\\{0.85 \\ge 0.8\\}$ evaluates to $1$.\n$$ J(\\theta_{1}) = J(0.85) = (0.85 - 1)^{2} + 0.5 \\cdot 1 = (-0.15)^{2} + 0.5 = 0.0225 + 0.5 = 0.5225 $$\n\nFinally, we compare the objective values:\n$$ J(\\theta_{1}) = 0.5225 $$\n$$ J(\\theta_{0}) = 0.0625 $$\nWe observe that $J(\\theta_{1})  J(\\theta_{0})$. The objective function value has increased after the first step. This is a classic example of how standard gradient descent can fail on non-smooth or non-convex problems, particularly when a step crosses a discontinuity. The update, based on the local gradient, was in a \"correct\" direction for the quadratic part of the function, but it inadvertently stepped into a region with a large penalty, causing the total objective value to increase.\n\n### Option-by-Option Analysis\n\n**A. The update yields $\\theta_{1} = 0.85 \\ge \\tau$, and $J(\\theta_{1})  J(\\theta_{0})$ due to activation of the surcharge at $\\theta \\ge \\tau$.**\nOur calculation shows that the update yields $\\theta_{1} = 0.85$. Given $\\tau = 0.8$, it is true that $\\theta_{1} \\ge \\tau$. Our calculation also shows $J(\\theta_{1}) = 0.5225$ and $J(\\theta_{0}) = 0.0625$, so $J(\\theta_{1})  J(\\theta_{0})$ is true. The reason provided, \"activation of the surcharge\", correctly identifies that crossing the threshold $\\tau$ added the penalty term $\\lambda=0.5$ to the objective, which was the dominant factor in the increase of $J$.\nVerdict: **Correct**.\n\n**B. The update yields $\\theta_{1} = 0.85 \\ge \\tau$, and $J(\\theta_{1})  J(\\theta_{0})$ because any sufficiently small step in gradient descent decreases the objective even with discontinuities.**\nThe first part of the statement, $\\theta_{1} = 0.85$, is correct. However, the second part, $J(\\theta_{1})  J(\\theta_{0})$, is false, as our calculation shows the opposite. The reasoning provided is fundamentally incorrect; gradient descent is guaranteed to decrease the objective function for a sufficiently small step size only for (at least) continuous and typically smooth functions. For discontinuous functions, this guarantee is lost, as demonstrated by this very problem.\nVerdict: **Incorrect**.\n\n**C. The update cannot be computed because $\\nabla J(\\theta_{0})$ does not exist.**\nThis statement is false. The gradient of $J(\\theta)$ is undefined only at the single point $\\theta = \\tau = 0.8$. The initial point is $\\theta_{0} = 0.75$. At this point, the function is locally smooth and its gradient exists. We calculated it to be $\\nabla J(\\theta_{0}) = -0.5$. Therefore, the update can be, and was, computed.\nVerdict: **Incorrect**.\n\n**D. The update yields $\\theta_{1} = 0.65  \\tau$, and $J(\\theta_{1})  J(\\theta_{0})$ because the step remains below the threshold.**\nThe claimed update result is $\\theta_{1} = 0.65$. Our calculation shows $\\theta_{1} = 0.85$. The value $0.65$ would be obtained by an incorrect update rule: $\\theta_{0} + \\alpha \\nabla J(\\theta_0) = 0.75 + 0.2(-0.5) = 0.75 - 0.1 = 0.65$. This corresponds to gradient *ascent*, not descent, in terms of the function's value if not for the discontinuity. The premise of the statement is based on a calculation error.\nVerdict: **Incorrect**.", "answer": "$$\\boxed{A}$$", "id": "2375204"}, {"introduction": "After exploring potential pitfalls, we now apply gradient descent to a core problem in financial engineering where it performs beautifully. Many risk management tasks can be formulated as convex optimization problems, which are the ideal setting for gradient descent. In this hands-on coding practice, you will implement the algorithm to calculate an optimal hedging ratio for an airline's fuel costs, turning an abstract mathematical tool into a practical solution for managing financial risk [@problem_id:2375264].", "problem": "An airline seeks to hedge its jet fuel cost exposure by trading oil futures contracts. Let a time index be denoted by $t \\in \\{1, \\dots, T\\}$. For each period $t$, let $s_t$ denote the observed change in the airlineâ€™s jet fuel cost per unit volume (for example, a change in United States dollars (USD) per gallon), and let $f_t$ denote the corresponding change in the futures price per contract-unit over the same period. The airline chooses a scalar hedging ratio $h \\in \\mathbb{R}$, interpreted as the number of futures contract-units per unit of fuel exposure, to reduce fluctuations in the hedged cost change $x_t(h) = s_t - h f_t$. To discourage excessively large positions, the airline incurs a small quadratic position penalty with coefficient $\\gamma \\ge 0$. The objective is to choose $h$ that minimizes the penalized sample mean squared hedged change\n$$\nJ(h) = \\frac{1}{T} \\sum_{t=1}^{T} \\left(s_t - h f_t\\right)^2 + \\gamma h^2.\n$$\nStarting from a given initial guess $h_0 \\in \\mathbb{R}$, use basic gradient descent with a fixed step size $\\alpha  0$ to minimize $J(h)$. At iteration $k$, update $h_{k+1} = h_k - \\alpha \\, \\nabla J(h_k)$, and terminate when the absolute change in iterates satisfies $|h_{k+1} - h_k|  \\varepsilon$ or when a maximum of $N$ iterations is reached. Your implementation must:\n- Derive the gradient $\\nabla J(h)$ from first principles based on the definition of $J(h)$.\n- Ensure numerical robustness by using the provided tolerance $\\varepsilon$ and iteration cap $N$.\n- Return the final iterate as the estimated optimal hedging ratio $h^\\star$.\n\nImplement a program that computes $h^\\star$ for each of the following test cases. For each case, $s$ and $f$ are given as ordered lists of real numbers, and you are provided with $\\gamma$, $\\alpha$, $h_0$, $\\varepsilon$, and $N$.\n\nTest Suite:\n- Case $1$:\n  - $s = [\\,\\$1.0,\\,-\\$0.5,\\,\\$0.75,\\,-\\$1.25,\\,\\$0.6,\\,-\\$0.8\\,]$\n  - $f = [\\,\\$0.9,\\,-\\$0.4,\\,\\$0.8,\\,-\\$1.1,\\,\\$0.5,\\,-\\$0.7\\,]$\n  - $\\gamma = 0.01$, $\\alpha = 0.05$, $h_0 = 0.0$, $\\varepsilon = 10^{-12}$, $N = 100000$\n- Case $2$:\n  - $s = [\\,\\$1.0,\\,-\\$1.0,\\,\\$1.0,\\,-\\$1.0,\\,\\$0.5,\\,-\\$0.5\\,]$\n  - $f = [\\,-\\$0.8,\\,\\$0.8,\\,-\\$0.7,\\,\\$0.7,\\,-\\$0.4,\\,\\$0.4\\,]$\n  - $\\gamma = 0.02$, $\\alpha = 0.05$, $h_0 = 0.0$, $\\varepsilon = 10^{-12}$, $N = 100000$\n- Case $3$ (weak hedging instrument; tests regularization):\n  - $s = [\\,\\$1.0,\\,\\$0.5,\\,-\\$0.5,\\,-\\$1.0,\\,\\$0.3,\\,-\\$0.2\\,]$\n  - $f = [\\,\\$0.1,\\,-\\$0.1,\\,\\$0.1,\\,-\\$0.1,\\,\\$0.05,\\,-\\$0.05\\,]$\n  - $\\gamma = 0.5$, $\\alpha = 0.02$, $h_0 = 0.0$, $\\varepsilon = 10^{-12}$, $N = 100000$\n- Case $4$ (longer sample, negligible penalty):\n  - $s = [\\,\\$0.2,\\,-\\$0.1,\\,\\$0.3,\\,-\\$0.25,\\,\\$0.15,\\,-\\$0.05,\\,\\$0.4,\\,-\\$0.35\\,]$\n  - $f = [\\,\\$0.18,\\,-\\$0.12,\\,\\$0.28,\\,-\\$0.22,\\,\\$0.14,\\,-\\$0.06,\\,\\$0.39,\\,-\\$0.33\\,]$\n  - $\\gamma = 0.0$, $\\alpha = 0.08$, $h_0 = 0.0$, $\\varepsilon = 10^{-12}$, $N = 100000$\n\nAll monetary quantities are in United States dollars (USD) but the output hedging ratios are unitless. Your program should produce a single line of output containing the four estimated hedging ratios as a comma-separated list enclosed in square brackets, each rounded to exactly $6$ decimal places (for example, $[1.234567,-0.123456,0.000001,2.000000]$). No other text should be printed.", "solution": "The problem as stated is subjected to validation.\n\n**Step 1: Extract Givens**\n\nThe objective is to find the hedging ratio $h \\in \\mathbb{R}$ that minimizes the following penalized cost function:\n$$\nJ(h) = \\frac{1}{T} \\sum_{t=1}^{T} \\left(s_t - h f_t\\right)^2 + \\gamma h^2\n$$\nwhere:\n- $t \\in \\{1, \\dots, T\\}$ is the time index.\n- $s_t$ is the change in jet fuel cost per unit volume.\n- $f_t$ is the change in the futures price per contract-unit.\n- $\\gamma \\ge 0$ is the quadratic position penalty coefficient.\n\nThe optimization is to be performed using basic gradient descent with the update rule:\n$$\nh_{k+1} = h_k - \\alpha \\, \\nabla J(h_k)\n$$\nwhere:\n- $k$ is the iteration index.\n- $h_k$ is the estimate of the hedging ratio at iteration $k$.\n- $\\alpha  0$ is a fixed step size.\n- $\\nabla J(h_k)$ is the gradient of $J(h)$ evaluated at $h_k$.\n\nThe algorithm starts from an initial guess $h_0$ and terminates under one of two conditions:\n1.  Convergence: $|h_{k+1} - h_k|  \\varepsilon$, where $\\varepsilon$ is a specified tolerance.\n2.  Maximum iterations: The number of iterations reaches a maximum value $N$.\n\nThe provided test cases are:\n- **Case 1**:\n  - $s = [1.0, -0.5, 0.75, -1.25, 0.6, -0.8]$\n  - $f = [0.9, -0.4, 0.8, -1.1, 0.5, -0.7]$\n  - $\\gamma = 0.01$, $\\alpha = 0.05$, $h_0 = 0.0$, $\\varepsilon = 10^{-12}$, $N = 100000$\n- **Case 2**:\n  - $s = [1.0, -1.0, 1.0, -1.0, 0.5, -0.5]$\n  - $f = [-0.8, 0.8, -0.7, 0.7, -0.4, 0.4]$\n  - $\\gamma = 0.02$, $\\alpha = 0.05$, $h_0 = 0.0$, $\\varepsilon = 10^{-12}$, $N = 100000$\n- **Case 3**:\n  - $s = [1.0, 0.5, -0.5, -1.0, 0.3, -0.2]$\n  - $f = [0.1, -0.1, 0.1, -0.1, 0.05, -0.05]$\n  - $\\gamma = 0.5$, $\\alpha = 0.02$, $h_0 = 0.0$, $\\varepsilon = 10^{-12}$, $N = 100000$\n- **Case 4**:\n  - $s = [0.2, -0.1, 0.3, -0.25, 0.15, -0.05, 0.4, -0.35]$\n  - $f = [0.18, -0.12, 0.28, -0.22, 0.14, -0.06, 0.39, -0.33]$\n  - $\\gamma = 0.0$, $\\alpha = 0.08$, $h_0 = 0.0$, $\\varepsilon = 10^{-12}$, $N = 100000$\n\n**Step 2: Validate Using Extracted Givens**\n\nThe problem is evaluated against the validation criteria.\n- **Scientifically Grounded**: The problem is an application of regularized linear regression, a fundamental technique in statistics and machine learning, to a standard problem in financial engineering (hedging). The objective function is a form of Ridge regression, and gradient descent is a canonical method for its optimization. The formulation is scientifically and mathematically sound.\n- **Well-Posed**: The objective function $J(h)$ is a quadratic function of $h$. Specifically, $J(h) = A h^2 - B h + C$, where the coefficient of the quadratic term is $A = (\\frac{1}{T} \\sum_{t=1}^T f_t^2) + \\gamma$. Since $\\gamma \\ge 0$ and $f_t^2 \\ge 0$, the coefficient $A$ is strictly positive as long as not all $f_t$ are zero or $\\gamma  0$. In all test cases, the vector $f$ is not a zero vector, thus $A  0$. This means $J(h)$ is a strictly convex function and possesses a unique global minimum. The problem is therefore well-posed.\n- **Objective**: The problem is stated in precise mathematical terms, devoid of ambiguity, subjectivity, or opinion.\n- **Completeness**: All required data and parameters ($s, f, \\gamma, \\alpha, h_0, \\varepsilon, N$) are provided for each test case. The problem is self-contained.\n\n**Step 3: Verdict and Action**\n\nThe problem is deemed **valid**. A solution will be furnished.\n\n**Solution Derivation**\n\nThe core of the gradient descent algorithm is the computation of the gradient of the objective function, $\\nabla J(h)$. We derive this from first principles by differentiating $J(h)$ with respect to $h$.\n\nThe objective function is:\n$$\nJ(h) = \\frac{1}{T} \\sum_{t=1}^{T} (s_t - h f_t)^2 + \\gamma h^2\n$$\n\nThe gradient $\\nabla J(h)$ is the ordinary derivative $\\frac{dJ}{dh}$:\n$$\n\\nabla J(h) = \\frac{d}{dh} \\left( \\frac{1}{T} \\sum_{t=1}^{T} (s_t - h f_t)^2 + \\gamma h^2 \\right)\n$$\n\nBy linearity of differentiation, we can differentiate term by term:\n$$\n\\nabla J(h) = \\frac{1}{T} \\sum_{t=1}^{T} \\frac{d}{dh} (s_t - h f_t)^2 + \\frac{d}{dh} (\\gamma h^2)\n$$\n\nApplying the chain rule to the first term and the power rule to the second:\n$$\n\\frac{d}{dh} (s_t - h f_t)^2 = 2 (s_t - h f_t) \\cdot \\frac{d}{dh}(s_t - h f_t) = 2 (s_t - h f_t) (-f_t) = -2s_t f_t + 2h f_t^2\n$$\n$$\n\\frac{d}{dh} (\\gamma h^2) = 2 \\gamma h\n$$\n\nSubstituting these back into the expression for the gradient:\n$$\n\\nabla J(h) = \\frac{1}{T} \\sum_{t=1}^{T} (-2s_t f_t + 2h f_t^2) + 2 \\gamma h\n$$\n\nWe can separate the terms inside the summation:\n$$\n\\nabla J(h) = \\frac{1}{T} \\left( \\sum_{t=1}^{T} (-2s_t f_t) + \\sum_{t=1}^{T} (2h f_t^2) \\right) + 2 \\gamma h\n$$\n$$\n\\nabla J(h) = -\\frac{2}{T} \\sum_{t=1}^{T} s_t f_t + \\frac{2h}{T} \\sum_{t=1}^{T} f_t^2 + 2 \\gamma h\n$$\n\nFactoring out $2h$:\n$$\n\\nabla J(h) = 2h \\left( \\frac{1}{T} \\sum_{t=1}^{T} f_t^2 + \\gamma \\right) - \\frac{2}{T} \\sum_{t=1}^{T} s_t f_t\n$$\nThis is the analytical expression for the gradient. This expression will be used in the iterative update rule.\n\nThe algorithm proceeds as follows:\n1.  Initialize $h_k$ with $h_0$.\n2.  For $k$ from $0$ to $N-1$:\n    a. Compute the gradient $\\nabla J(h_k)$ using the derived formula. To do this efficiently, we first pre-compute the sample moments $\\frac{1}{T} \\sum s_t f_t$ and $\\frac{1}{T} \\sum f_t^2$.\n    b. Update the estimate: $h_{k+1} = h_k - \\alpha \\nabla J(h_k)$.\n    c. Check for convergence: if $|h_{k+1} - h_k|  \\varepsilon$, the algorithm has converged. The final estimate is $h^\\star = h_{k+1}$. Terminate.\n3.  If the loop completes without meeting the convergence criterion, the algorithm terminates due to reaching the maximum number of iterations. The final estimate is $h^\\star = h_N$.\n\nThis procedure will be implemented for each test case to find the optimal hedging ratio $h^\\star$. The provided step sizes $\\alpha$ are small enough to ensure stable convergence toward the unique minimum of the convex objective function.\nThe analytical solution for the minimum, found by setting $\\nabla J(h) = 0$, is\n$h^\\star = \\frac{\\sum s_t f_t}{\\sum f_t^2 + T\\gamma}$, which serves as a validation for the convergence of the iterative algorithm.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for the optimal hedging ratio using gradient descent for multiple test cases.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"s\": [1.0, -0.5, 0.75, -1.25, 0.6, -0.8],\n            \"f\": [0.9, -0.4, 0.8, -1.1, 0.5, -0.7],\n            \"gamma\": 0.01,\n            \"alpha\": 0.05,\n            \"h0\": 0.0,\n            \"epsilon\": 1e-12,\n            \"N\": 100000,\n        },\n        {\n            \"s\": [1.0, -1.0, 1.0, -1.0, 0.5, -0.5],\n            \"f\": [-0.8, 0.8, -0.7, 0.7, -0.4, 0.4],\n            \"gamma\": 0.02,\n            \"alpha\": 0.05,\n            \"h0\": 0.0,\n            \"epsilon\": 1e-12,\n            \"N\": 100000,\n        },\n        {\n            \"s\": [1.0, 0.5, -0.5, -1.0, 0.3, -0.2],\n            \"f\": [0.1, -0.1, 0.1, -0.1, 0.05, -0.05],\n            \"gamma\": 0.5,\n            \"alpha\": 0.02,\n            \"h0\": 0.0,\n            \"epsilon\": 1e-12,\n            \"N\": 100000,\n        },\n        {\n            \"s\": [0.2, -0.1, 0.3, -0.25, 0.15, -0.05, 0.4, -0.35],\n            \"f\": [0.18, -0.12, 0.28, -0.22, 0.14, -0.06, 0.39, -0.33],\n            \"gamma\": 0.0,\n            \"alpha\": 0.08,\n            \"h0\": 0.0,\n            \"epsilon\": 1e-12,\n            \"N\": 100000,\n        },\n    ]\n\n    def compute_h_star(s_data, f_data, gamma, alpha, h0, epsilon, N):\n        \"\"\"\n        Computes the optimal hedging ratio h* using gradient descent.\n        \"\"\"\n        s = np.array(s_data)\n        f = np.array(f_data)\n        T = len(s)\n\n        # Pre-compute sample moments for efficiency\n        # E[f^2] = (1/T) * sum(f_t^2)\n        mean_f_sq = np.sum(f**2) / T\n        # E[sf] = (1/T) * sum(s_t * f_t)\n        mean_s_f = np.sum(s * f) / T\n        \n        h_k = h0\n\n        for _ in range(N):\n            # Gradient: grad_J(h) = 2 * (h * (E[f^2] + gamma) - E[sf])\n            grad_J = 2 * (h_k * (mean_f_sq + gamma) - mean_s_f)\n            \n            h_k_plus_1 = h_k - alpha * grad_J\n            \n            if np.abs(h_k_plus_1 - h_k)  epsilon:\n                return h_k_plus_1\n            \n            h_k = h_k_plus_1\n            \n        return h_k\n\n    results = []\n    for case in test_cases:\n        h_star = compute_h_star(\n            case[\"s\"],\n            case[\"f\"],\n            case[\"gamma\"],\n            case[\"alpha\"],\n            case[\"h0\"],\n            case[\"epsilon\"],\n            case[\"N\"],\n        )\n        results.append(h_star)\n\n    # Format the final output as a comma-separated list of strings,\n    # with each number rounded to 6 decimal places, enclosed in brackets.\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n\n```", "id": "2375264"}]}