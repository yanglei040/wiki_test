{"hands_on_practices": [{"introduction": "The best way to understand the power of sparse grids is to build one. This first exercise guides you through implementing a sparse grid interpolant for a smooth, five-dimensional function that is designed to be highly non-additive, meaning its variables are strongly coupled. By tracking the approximation error as you increase the grid level, you will gain hands-on experience with the combination technique and see firsthand how sparse grids can effectively approximate complex, smooth functions in higher dimensions where full tensor grids would be computationally infeasible [@problem_id:2399817].", "problem": "Let $d = 5$ and consider the compact hypercube domain $[0,1]^5$. Define the smooth, highly non-additive function $f : [0,1]^5 \\to \\mathbb{R}$ by\n$$\nf(x_1,x_2,x_3,x_4,x_5) = \\exp(0.5\\,x_1 x_2 + 0.25\\,x_3)\\,\\cos\\Big(\\pi\\big(x_1 x_3 + 2.0\\,x_2 x_4\\big)\\Big) + \\sin\\Big(2\\pi\\big(x_4 x_5 + 0.1\\,x_1\\big)\\Big) + 0.05\\,x_1 x_2 x_3 x_4 x_5,\n$$\nwhere all trigonometric arguments are in radians.\n\nYour task is to approximate $f$ using a five-dimensional sparse-grid interpolant on $[0,1]^5$ at three approximation levels $\\ell \\in \\{1,2,3\\}$, and to quantify approximation quality on a fixed evaluation set. For each level $\\ell$, compute the maximum absolute pointwise error of the interpolant relative to $f$ evaluated on the following $15$ test points:\n$$\n\\begin{aligned}\n(\\,0.5,\\,0.5,\\,0.5,\\,0.5,\\,0.5\\,),\\quad (\\,0,\\,0,\\,0,\\,0,\\,0\\,),\\quad (\\,1,\\,1,\\,1,\\,1,\\,1\\,),\\quad (\\,0,\\,1,\\,0,\\,1,\\,0\\,),\\quad (\\,1,\\,0,\\,1,\\,0,\\,1\\,),\\\\\n(\\,0.25,\\,0.75,\\,0.33,\\,0.67,\\,0.5\\,),\\quad (\\,0.9,\\,0.1,\\,0.2,\\,0.8,\\,0.3\\,),\\quad (\\,0.1,\\,0.9,\\,0.8,\\,0.2,\\,0.7\\,),\\\\\n(\\,0.3,\\,0.3,\\,0.7,\\,0.7,\\,0.2\\,),\\quad (\\,0.6,\\,0.4,\\,0.2,\\,0.9,\\,0.1\\,),\\quad (\\,0.15,\\,0.85,\\,0.55,\\,0.45,\\,0.35\\,),\\\\\n(\\,0.8,\\,0.2,\\,0.6,\\,0.4,\\,0.5\\,),\\quad (\\,0.2,\\,0.8,\\,0.4,\\,0.6,\\,0.9\\,),\\quad (\\,0.05,\\,0.95,\\,0.25,\\,0.75,\\,0.35\\,),\\quad (\\,0.4,\\,0.4,\\,0.4,\\,0.4,\\,0.4\\,).\n\\end{aligned}\n$$\n\nThe test suite consists of the three levels $\\ell \\in \\{1,2,3\\}$. For each $\\ell$, the required answer is a real number equal to the maximum absolute error over the above test set. Your program must produce a single line of output containing these three results as a comma-separated list enclosed in square brackets, in the order $\\ell = 1$, $\\ell = 2$, $\\ell = 3$, with each real number rounded to $6$ decimal places. For example, an output of the required form is\n$$\n[e_1, e_2, e_3],\n$$\nwhere $e_\\ell$ denotes the maximum absolute error at level $\\ell$ rounded to $6$ decimal places.\n\nNo external input is provided to the program, and no physical units are involved. All angles in trigonometric functions are in radians. The final output must be precisely one line in the specified format.", "solution": "The problem statement is subjected to validation and is found to be valid. It is scientifically grounded in the established field of numerical analysis, specifically high-dimensional function approximation. The problem is well-posed, objective, and provides all necessary information to construct a unique, verifiable solution. The task is to implement a standard numerical algorithm, the sparse grid interpolation method, and to evaluate its performance.\n\nThe solution proceeds as follows. The core of the task is to approximate a given $d=5$ dimensional function $f: [0,1]^5 \\to \\mathbb{R}$ using a sparse grid interpolant, denoted $\\mathcal{A}_{\\ell,d}f$, for approximation levels $\\ell \\in \\{1, 2, 3\\}$. The accuracy of this interpolant is then assessed by computing the maximum absolute error on a prescribed set of $15$ test points.\n\nA sparse grid interpolant is constructed using the combination technique, which combines several tensor-product interpolants built on anisotropic grids. This approach mitigates the curse of dimensionality by judiciously selecting grid configurations that contribute most to the accuracy of the approximation. For a function $f$ defined on a $d$-dimensional domain, the sparse grid interpolant of level $\\ell$ is given by the formula:\n$$\n\\mathcal{A}_{\\ell,d}f(x) = \\sum_{q=0}^{\\min(\\ell, d-1)} (-1)^q \\binom{d-1}{q} \\sum_{|\\mathbf{i}|_1 = \\ell-q} \\mathcal{I}^{\\mathbf{i}}f(x)\n$$\nwhere:\n- $\\mathbf{i} = (i_1, i_2, \\dots, i_d)$ is a multi-index of non-negative integers representing the level of the 1D interpolant in each dimension.\n- $|\\mathbf{i}|_1 = i_1 + i_2 + \\dots + i_d$ is the sum of the levels.\n- $\\mathcal{I}^{\\mathbf{i}}f(x)$ is the tensor-product interpolant constructed from 1D interpolants of levels $i_1, \\dots, i_d$.\n- $\\binom{n}{k}$ is the binomial coefficient.\n\nThe construction of $\\mathcal{I}^{\\mathbf{i}}f(x)$ relies on a choice of 1D interpolation rule. For its superior stability and accuracy properties, we use Lagrange interpolation on Clenshaw-Curtis nodes. The 1D Clenshaw-Curtis grid of level $i \\ge 0$ on the interval $[0,1]$ is defined as:\n- For level $i=0$: $m_0=1$ point at $x_0=0.5$.\n- For level $i > 0$: $m_i=2^i+1$ points, given by $x_k = \\frac{1}{2}\\left(1 - \\cos\\left(\\frac{k\\pi}{m_i-1}\\right)\\right)$ for $k=0, 1, \\dots, m_i-1$.\n\nThe tensor-product interpolant $\\mathcal{I}^{\\mathbf{i}}f(x)$ for a point $x=(x_1, \\dots, x_d)$ is computed as:\n$$\n\\mathcal{I}^{\\mathbf{i}}f(x) = \\sum_{p \\in \\mathcal{G}_{\\mathbf{i}}} f(p) \\prod_{j=1}^{d} L_{i_j, p_j}(x_j)\n$$\nHere, $\\mathcal{G}_{\\mathbf{i}} = X_{i_1} \\times X_{i_2} \\times \\dots \\times X_{i_d}$ is the tensor-product grid, where $X_{i_j}$ is the set of 1D nodes for level $i_j$. The term $L_{i_j, p_j}(x_j)$ denotes the 1D Lagrange basis polynomial for the node $p_j$ in dimension $j$, evaluated at coordinate $x_j$.\n\nThe computational algorithm is structured as follows:\n1.  For each approximation level $\\ell \\in \\{1, 2, 3\\}$, iterate through the $15$ test points provided.\n2.  For each test point $x_{test}$, calculate the value of the sparse grid interpolant $\\mathcal{A}_{\\ell,d}f(x_{test})$ using the combination formula. This involves:\n    a.  Iterating through $q$ from $0$ to $\\min(\\ell, d-1)$.\n    b.  Generating all multi-indices $\\mathbf{i}$ such that $|\\mathbf{i}|_1 = \\ell-q$.\n    c.  For each multi-index $\\mathbf{i}$, computing the value of the corresponding tensor-product interpolant $\\mathcal{I}^{\\mathbf{i}}f(x_{test})$.\n    d.  Summing these values weighted by the combination coefficients $(-1)^q \\binom{d-1}{q}$.\n3.  To optimize performance, evaluations of the function $f$ at grid points are cached, as the same grid points are used across many tensor-product interpolants. Recursive and intermediate calculations are also memoized where appropriate.\n4.  For each test point, calculate the absolute error $|\\,f(x_{test}) - \\mathcal{A}_{\\ell,d}f(x_{test})\\,|$.\n5.  The maximum of these absolute errors over all $15$ test points is recorded as the result for level $\\ell$.\n6.  The final output is a list of these maximum errors for $\\ell=1, 2, 3$, formatted to $6$ decimal places as required.", "answer": "```python\nimport numpy as np\nfrom scipy.special import comb\nfrom itertools import product\nfrom functools import lru_cache\n\n# The target function to be approximated, as defined in the problem.\ndef f_definition(x):\n    \"\"\"The function f, vectorized for efficient evaluation on NumPy arrays.\"\"\"\n    x1, x2, x3, x4, x5 = x\n    term1 = np.exp(0.5 * x1 * x2 + 0.25 * x3) * np.cos(np.pi * (x1 * x3 + 2.0 * x2 * x4))\n    term2 = np.sin(2 * np.pi * (x4 * x5 + 0.1 * x1))\n    term3 = 0.05 * x1 * x2 * x3 * x4 * x5\n    return term1 + term2 + term3\n\n# Caching for function evaluations. The key is a tuple representation of the point.\nfunc_eval_cache = {}\n\ndef f_cached(p_tuple):\n    \"\"\"A wrapper for f_definition that uses a cache to avoid re-computation.\"\"\"\n    if p_tuple not in func_eval_cache:\n        func_eval_cache[p_tuple] = f_definition(np.array(p_tuple))\n    return func_eval_cache[p_tuple]\n\n@lru_cache(maxsize=None)\ndef get_1d_nodes(level):\n    \"\"\"\n    Computes 1D Clenshaw-Curtis nodes on [0,1] for a given hierarchical level.\n    Level 0 corresponds to a single point at the domain center.\n    Level i > 0 corresponds to 2^i+1 points. The results are cached.\n    \"\"\"\n    if level == 0:\n        return (0.5,)  # Return as tuple for hashability\n    num_points = 2**level + 1\n    k = np.arange(num_points)\n    nodes = 0.5 * (1.0 - np.cos(k * np.pi / (num_points - 1)))\n    return tuple(nodes)\n\n@lru_cache(maxsize=None)\ndef generate_multi_indices(d, s):\n    \"\"\"\n    Generates all d-dimensional multi-indices i with |i|_1 = s.\n    The recursive generation is memoized for efficiency.\n    \"\"\"\n    if d == 1:\n        return [(s,)]\n    indices = []\n    for i in range(s + 1):\n        # Recursively find indices for the remaining dimensions and sum\n        sub_indices = generate_multi_indices(d - 1, s - i)\n        for sub in sub_indices:\n            indices.append((i,) + sub)\n    return indices\n\ndef get_1d_lagrange_basis_eval(t, nodes_tuple, k):\n    \"\"\"\n    Evaluates the k-th 1D Lagrange basis polynomial at point t.\n    The nodes are passed as a tuple to enable potential caching if this function were wrapped.\n    \"\"\"\n    nodes = np.array(nodes_tuple)\n    xk = nodes[k]\n\n    if np.isclose(t, xk):\n        return 1.0\n\n    numerator = 1.0\n    denominator = 1.0\n    for j, xj in enumerate(nodes):\n        if j != k:\n            numerator *= (t - xj)\n            denominator *= (xk - xj)\n    \n    # This prevents division by zero if t is close to another node,\n    # making the numerator near-zero.\n    if np.isclose(denominator, 0.0):\n        return 0.0\n        \n    return numerator / denominator\n\ndef solve():\n    \"\"\"\n    Main solver function to perform sparse grid interpolation and error calculation.\n    \"\"\"\n    D = 5\n    LEVELS = [1, 2, 3]\n    TEST_POINTS = np.array([\n        [0.5, 0.5, 0.5, 0.5, 0.5], [0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 1.0, 1.0],\n        [0.0, 1.0, 0.0, 1.0, 0.0], [1.0, 0.0, 1.0, 0.0, 1.0],\n        [0.25, 0.75, 0.33, 0.67, 0.5], [0.9, 0.1, 0.2, 0.8, 0.3], [0.1, 0.9, 0.8, 0.2, 0.7],\n        [0.3, 0.3, 0.7, 0.7, 0.2], [0.6, 0.4, 0.2, 0.9, 0.1], [0.15, 0.85, 0.55, 0.45, 0.35],\n        [0.8, 0.2, 0.6, 0.4, 0.5], [0.2, 0.8, 0.4, 0.6, 0.9], [0.05, 0.95, 0.25, 0.75, 0.35],\n        [0.4, 0.4, 0.4, 0.4, 0.4]\n    ])\n\n    final_errors = []\n\n    for l_level in LEVELS:\n        max_level_error = 0.0\n        \n        # Cache for tensor product interpolation results, reset for each test point.\n        tensor_interp_cache = {}\n\n        def get_tensor_interp_val(x_test_tuple, mi_tuple):\n            \"\"\"Computes or retrieves from cache the tensor product interpolant value.\"\"\"\n            if mi_tuple in tensor_interp_cache:\n                return tensor_interp_cache[mi_tuple]\n\n            nodes_per_dim = [get_1d_nodes(mi) for mi in mi_tuple]\n            node_indices_ranges = [range(len(nodes)) for nodes in nodes_per_dim]\n            \n            total_sum = 0.0\n            for index_tuple in product(*node_indices_ranges):\n                \n                grid_point = tuple(nodes_per_dim[i][index_tuple[i]] for i in range(D))\n                func_val = f_cached(grid_point)\n                \n                basis_product = 1.0\n                for i in range(D):\n                    basis_val = get_1d_lagrange_basis_eval(x_test_tuple[i], nodes_per_dim[i], index_tuple[i])\n                    basis_product *= basis_val\n                \n                total_sum += func_val * basis_product\n            \n            tensor_interp_cache[mi_tuple] = total_sum\n            return total_sum\n\n        for x_test_point in TEST_POINTS:\n            x_test_tuple = tuple(x_test_point)\n            tensor_interp_cache.clear()\n            \n            interpolant_val = 0.0\n            for q in range(min(l_level, D - 1) + 1):\n                comb_coeff = ((-1)**q) * comb(D - 1, q, exact=True)\n                if comb_coeff == 0:\n                    continue\n                \n                s = l_level - q\n                multi_indices = generate_multi_indices(D, s)\n                \n                term_sum = 0.0\n                for mi_tuple in multi_indices:\n                    term_sum += get_tensor_interp_val(x_test_tuple, mi_tuple)\n                \n                interpolant_val += comb_coeff * term_sum\n\n            exact_val = f_definition(x_test_point)\n            error = abs(exact_val - interpolant_val)\n            if error  max_level_error:\n                max_level_error = error\n        \n        final_errors.append(f\"{max_level_error:.6f}\")\n\n    print(f\"[{','.join(final_errors)}]\")\n\nsolve()\n```", "id": "2399817"}, {"introduction": "In many economic models, not all state variables are created equal; some have a much larger impact on the outcome than others. This practice builds on the standard sparse grid by introducing the concept of anisotropy, which allows us to selectively refine the grid along more important dimensions. You will implement an anisotropic sparse grid to approximate a function where the first two dimensions are deliberately made more significant, and compare its efficiency to a standard, isotropic grid, demonstrating a key technique for optimizing high-dimensional approximations in practice [@problem_id:2432646].", "problem": "You are given the task of approximating a high-dimensional function on a hypercube using an anisotropic sparse grid based on the Smolyak construction. The domain is the unit hypercube $[0,1]^{10}$. Let $d = 10$. For each dimension $i \\in \\{1,\\dots,d\\}$ and each level $\\ell_i \\in \\mathbb{N}$ with $\\ell_i \\ge 1$, define the one-dimensional nested dyadic grid\n$$\nX_{\\ell_i} = \\left\\{ \\frac{j}{2^{\\ell_i - 1}} \\,:\\, j = 0,1,\\dots,2^{\\ell_i - 1} \\right\\} \\subset [0,1].\n$$\nFor a multi-index $\\boldsymbol{\\ell} = (\\ell_1,\\dots,\\ell_d)$, define the full tensor grid\n$$\nG_{\\boldsymbol{\\ell}} = X_{\\ell_1} \\times \\cdots \\times X_{\\ell_d}.\n$$\nLet $U_{\\boldsymbol{\\ell}}$ be the $d$-variate multilinear nodal interpolation operator on $G_{\\boldsymbol{\\ell}}$, defined by tensor products of one-dimensional linear Lagrange basis functions on the dyadic grids.\n\nLet $\\boldsymbol{\\alpha} = (\\alpha_1,\\dots,\\alpha_d)$ be a vector of positive integer anisotropy weights and let $Q \\in \\mathbb{N}$ with $Q \\ge 0$. Define the down-closed index set\n$$\n\\mathcal{I}(\\boldsymbol{\\alpha}, Q) = \\left\\{ \\boldsymbol{\\ell} \\in \\mathbb{N}^d \\,:\\, \\ell_i \\ge 1 \\text{ for all } i, \\ \\sum_{i=1}^d \\alpha_i (\\ell_i - 1) \\le Q \\right\\}.\n$$\nConsider a linear combination of full-grid interpolants\n$$\n\\mathcal{S}_{\\boldsymbol{\\alpha},Q} = \\sum_{\\boldsymbol{\\ell} \\in \\mathcal{I}(\\boldsymbol{\\alpha}, Q)} c_{\\boldsymbol{\\ell}} \\, U_{\\boldsymbol{\\ell}},\n$$\nwith coefficients $\\{c_{\\boldsymbol{\\ell}}\\}$ characterized by the interpolation consistency condition\n$$\n\\sum_{\\boldsymbol{k} \\in \\mathcal{I}(\\boldsymbol{\\alpha}, Q), \\, \\boldsymbol{k} \\ge \\boldsymbol{\\ell}} c_{\\boldsymbol{k}} = 1 \\quad \\text{for every } \\boldsymbol{\\ell} \\in \\mathcal{I}(\\boldsymbol{\\alpha}, Q),\n$$\nwhere $\\boldsymbol{k} \\ge \\boldsymbol{\\ell}$ denotes the componentwise partial order $k_i \\ge \\ell_i$ for all $i$. This condition ensures that $\\mathcal{S}_{\\boldsymbol{\\alpha},Q}$ interpolates the target function at every node in $\\bigcup_{\\boldsymbol{\\ell} \\in \\mathcal{I}(\\boldsymbol{\\alpha}, Q)} G_{\\boldsymbol{\\ell}}$.\n\nYour program must implement $\\mathcal{S}_{\\boldsymbol{\\alpha},Q}$ on $[0,1]^{10}$ and evaluate it at prescribed points. Use the following two target functions, each defined for $\\boldsymbol{x} = (x_1,\\dots,x_{10}) \\in [0,1]^{10}$:\n- $f_1(\\boldsymbol{x}) = \\exp(x_1) + \\sin(2\\pi x_2) + 0.1 \\sum_{i=3}^{10} 0.5^{\\,i} \\, x_i^2$.\n- $f_2(\\boldsymbol{x}) = \\log(1 + 5 x_1) + \\sqrt{1 + x_2} + 0.01 \\sum_{i=3}^{10} x_i$.\nAll trigonometric arguments are to be interpreted in radians. The natural logarithm function is $\\log(\\cdot)$ and the principal square root is $\\sqrt{\\cdot}$.\n\nUse the following evaluation set $\\mathcal{E} = \\{\\boldsymbol{y}^{(k)}\\}_{k=1}^{5} \\subset [0,1]^{10}$ where each $\\boldsymbol{y}^{(k)}$ is explicitly given:\n- $\\boldsymbol{y}^{(1)} = (0.13,\\,0.77,\\,0.50,\\,0.20,\\,0.80,\\,0.33,\\,0.66,\\,0.10,\\,0.90,\\,0.42)$,\n- $\\boldsymbol{y}^{(2)} = (0.31,\\,0.62,\\,0.25,\\,0.75,\\,0.40,\\,0.60,\\,0.20,\\,0.80,\\,0.35,\\,0.65)$,\n- $\\boldsymbol{y}^{(3)} = (0.73,\\,0.27,\\,0.15,\\,0.85,\\,0.55,\\,0.45,\\,0.05,\\,0.95,\\,0.22,\\,0.78)$,\n- $\\boldsymbol{y}^{(4)} = (0.50,\\,0.50,\\,0.10,\\,0.90,\\,0.30,\\,0.70,\\,0.25,\\,0.75,\\,0.40,\\,0.60)$,\n- $\\boldsymbol{y}^{(5)} = (0.21,\\,0.84,\\,0.63,\\,0.37,\\,0.12,\\,0.88,\\,0.47,\\,0.53,\\,0.19,\\,0.81)$.\n\nDefine the anisotropy weights that favor the first two dimensions as\n$$\n\\boldsymbol{\\alpha}^{\\mathrm{aniso}} = (1,\\,1,\\,4,\\,4,\\,4,\\,4,\\,4,\\,4,\\,4,\\,4),\n$$\nand the isotropic weights as\n$$\n\\boldsymbol{\\alpha}^{\\mathrm{iso}} = (1,\\,1,\\,1,\\,1,\\,1,\\,1,\\,1,\\,1,\\,1,\\,1).\n$$\n\nTest Suite. Your program must execute the following four test cases and report one scalar result for each:\n- Test $1$: Use $\\boldsymbol{\\alpha} = \\boldsymbol{\\alpha}^{\\mathrm{aniso}}$ and $Q = 8$ to build $\\mathcal{S}_{\\boldsymbol{\\alpha},Q}$ for $f_1$. Compute the maximum absolute error over $\\mathcal{E}$,\n$$\n\\max_{\\boldsymbol{y} \\in \\mathcal{E}} \\left| \\mathcal{S}_{\\boldsymbol{\\alpha},Q}[f_1](\\boldsymbol{y}) - f_1(\\boldsymbol{y}) \\right|.\n$$\n- Test $2$: Use $\\boldsymbol{\\alpha} = \\boldsymbol{\\alpha}^{\\mathrm{iso}}$ and $Q = 3$ to build $\\mathcal{S}_{\\boldsymbol{\\alpha},Q}$ for $f_1$. Compute the maximum absolute error over $\\mathcal{E}$.\n- Test $3$: Use $\\boldsymbol{\\alpha} = \\boldsymbol{\\alpha}^{\\mathrm{aniso}}$ and $Q = 6$ to build $\\mathcal{S}_{\\boldsymbol{\\alpha},Q}$ for $f_2$. Compute the maximum absolute error over $\\mathcal{E}$.\n- Test $4$: Use $\\boldsymbol{\\alpha} = \\boldsymbol{\\alpha}^{\\mathrm{aniso}}$ and $Q = 8$ to build $\\mathcal{S}_{\\boldsymbol{\\alpha},Q}$ for $f_1$. Evaluate the absolute error at the single point $\\boldsymbol{0} = (0,0,0,0,0,0,0,0,0,0)$,\n$$\n\\left| \\mathcal{S}_{\\boldsymbol{\\alpha},Q}[f_1](\\boldsymbol{0}) - f_1(\\boldsymbol{0}) \\right|.\n$$\n\nFinal Output Format. Your program should produce a single line of output containing the four results in order as a comma-separated list enclosed in square brackets, for example\n$$\n[r1,r2,r3,r4],\n$$\nwhere each rj is a real number (a floating-point value).", "solution": "The user has provided a well-defined problem in numerical analysis, specifically concerning the approximation of high-dimensional functions using anisotropic sparse grids. The problem is scientifically grounded, internally consistent, and contains all necessary information for its resolution. I will therefore proceed with a complete solution.\n\nThe core task is to evaluate the sparse grid interpolant $\\mathcal{S}_{\\boldsymbol{\\alpha},Q}[f]$ at specified points $\\boldsymbol{y} \\in [0,1]^{10}$. The interpolant is defined using the combination technique formulation:\n$$\n\\mathcal{S}_{\\boldsymbol{\\alpha},Q}[f](\\boldsymbol{y}) = \\sum_{\\boldsymbol{\\ell} \\in \\mathcal{I}(\\boldsymbol{\\alpha}, Q)} c_{\\boldsymbol{\\ell}} \\, U_{\\boldsymbol{\\ell}}[f](\\boldsymbol{y})\n$$\nwhere $f$ is the target function, $\\boldsymbol{y}$ is an evaluation point, $\\mathcal{I}(\\boldsymbol{\\alpha}, Q)$ is a down-closed set of multi-indices, $c_{\\boldsymbol{\\ell}}$ are combination coefficients, and $U_{\\boldsymbol{\\ell}}[f](\\boldsymbol{y})$ is the value of a full tensor-product multilinear interpolant.\n\nThe solution is implemented via a series of modular steps for each test case:\n1.  **Generation of the Index Set**: The set $\\mathcal{I}(\\boldsymbol{\\alpha}, Q) = \\left\\{ \\boldsymbol{\\ell} \\in \\mathbb{N}^d \\,:\\, \\ell_i \\ge 1, \\ \\sum_{i=1}^d \\alpha_i (\\ell_i - 1) \\le Q \\right\\}$ is constructed. To do this efficiently, we define level vectors $\\boldsymbol{k} = (\\ell_1-1, \\dots, \\ell_d-1)$ where each $k_i \\ge 0$. The condition becomes $\\sum_{i=1}^d \\alpha_i k_i \\le Q$. A recursive backtracking algorithm is employed to find all valid vectors $\\boldsymbol{k} \\in \\mathbb{N}_0^d$ satisfying this inequality. The corresponding multi-indices $\\boldsymbol{\\ell} = \\boldsymbol{k} + \\mathbf{1}$ form the set $\\mathcal{I}(\\boldsymbol{\\alpha}, Q)$. The set is stored in a hash set for efficient lookups.\n\n2.  **Computation of Combination Coefficients**: The coefficients $c_{\\boldsymbol{\\ell}}$ are determined by the consistency condition that ensures the operator is interpolatory. For a down-closed index set $\\mathcal{I}$, this condition implies a unique solution for $c_{\\boldsymbol{\\ell}}$ given by the inclusion-exclusion principle:\n    $$\n    c_{\\boldsymbol{\\ell}} = \\sum_{J \\subseteq \\{1, \\dots, d\\}} (-1)^{|J|} \\mathbb{I}(\\boldsymbol{\\ell} + \\mathbf{e}_J \\in \\mathcal{I}(\\boldsymbol{\\alpha}, Q))\n    $$\n    where $\\mathbf{e}_J = \\sum_{j \\in J} \\mathbf{e}_j$ and $\\mathbf{e}_j$ is the $j$-th standard basis vector. $\\mathbb{I}(\\cdot)$ is the indicator function, which is $1$ if the condition holds and $0$ otherwise. For each $\\boldsymbol{\\ell} \\in \\mathcal{I}$, we iterate through all $2^d$ subsets $J$ of the dimensions, check if the neighbor index $\\boldsymbol{\\ell}+\\mathbf{e}_J$ is in $\\mathcal{I}$, and sum the signed contributions. For $d=10$, $2^{10} = 1024$ checks per coefficient, which is computationally feasible.\n\n3.  **Evaluation of Full Tensor-Product Interpolants**: The term $U_{\\boldsymbol{\\ell}}[f](\\boldsymbol{y})$ denotes the evaluation of the multilinear interpolant on the full tensor grid $G_{\\boldsymbol{\\ell}} = X_{\\ell_1} \\times \\cdots \\times X_{\\ell_d}$ at point $\\boldsymbol{y}$. This is implemented recursively. The value of the $d$-dimensional interpolant is obtained by performing a one-dimensional linear interpolation in the first dimension, where the values at the two required grid points are themselves computed by $(d-1)$-dimensional interpolation in the remaining variables. This process bottoms out after $d$ steps, requiring function evaluations at the $2^d$ corner points of the hyper-rectangular cell in $G_{\\boldsymbol{\\ell}}$ that encloses $\\boldsymbol{y}$.\n\n4.  **Optimization via Memoization**: A naive implementation would be computationally prohibitive due to redundant calculations. The evaluation of $U_{\\boldsymbol{\\ell}}[f](\\boldsymbol{y})$ for different $\\boldsymbol{\\ell}$ will repeatedly require values of $f$ at the same grid points. To eliminate this redundancy, all function evaluations $f(\\boldsymbol{x})$ are memoized (cached) in a dictionary. When a value for $f$ at a specific grid point $\\boldsymbol{x}$ is needed, the cache is checked first, and the function is evaluated only if the value has not been previously computed.\n\n5.  **Assembly and Error Calculation**: For each test case and each evaluation point $\\boldsymbol{y}$, the final approximation is assembled by summing the contributions $c_{\\boldsymbol{\\ell}} U_{\\boldsymbol{\\ell}}[f](\\boldsymbol{y})$ over all $\\boldsymbol{\\ell} \\in \\mathcal{I}(\\boldsymbol{\\alpha}, Q)$. The absolute error is then computed as $|\\mathcal{S}_{\\boldsymbol{\\alpha},Q}[f](\\boldsymbol{y}) - f(\\boldsymbol{y})|$. The maximum of these errors over the evaluation set $\\mathcal{E}$ is reported as required.\n\nFor Test case $4$, the evaluation point is $\\boldsymbol{y} = \\boldsymbol{0} = (0,\\dots,0)$. The point $\\boldsymbol{0}$ is a member of every grid $G_{\\boldsymbol{\\ell}}$ since the one-dimensional grids $X_{\\ell_i}$ always contain $0$. The sparse grid construction guarantees interpolation at all points in the sparse grid union, $\\mathcal{H}_{\\boldsymbol{\\alpha},Q} = \\bigcup_{\\boldsymbol{\\ell} \\in \\mathcal{I}} G_{\\boldsymbol{\\ell}}$. Since $\\boldsymbol{0} \\in \\mathcal{H}_{\\boldsymbol{\\alpha},Q}$, we must have $\\mathcal{S}_{\\boldsymbol{\\alpha},Q}[f_1](\\boldsymbol{0}) = f_1(\\boldsymbol{0})$, which implies the absolute error is exactly $0$. This serves as an analytical check on the correctness of the implementation.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom itertools import combinations\nimport math\n\ndef solve():\n    \"\"\"\n    Implements and evaluates an anisotropic sparse grid interpolant.\n    \"\"\"\n    d = 10\n\n    # Define target functions\n    def f1(x_tuple):\n        x = np.array(x_tuple)\n        term1 = np.exp(x[0])\n        term2 = np.sin(2 * np.pi * x[1])\n        term3 = 0.1 * np.sum([0.5**(i + 3) * x[i + 2]**2 for i in range(8)])\n        return term1 + term2 + term3\n\n    def f2(x_tuple):\n        x = np.array(x_tuple)\n        term1 = np.log(1 + 5 * x[0])\n        term2 = np.sqrt(1 + x[1])\n        term3 = 0.01 * np.sum(x[2:])\n        return term1 + term2 + term3\n\n    # Define evaluation points\n    evaluation_set = [\n        (0.13, 0.77, 0.50, 0.20, 0.80, 0.33, 0.66, 0.10, 0.90, 0.42),\n        (0.31, 0.62, 0.25, 0.75, 0.40, 0.60, 0.20, 0.80, 0.35, 0.65),\n        (0.73, 0.27, 0.15, 0.85, 0.55, 0.45, 0.05, 0.95, 0.22, 0.78),\n        (0.50, 0.50, 0.10, 0.90, 0.30, 0.70, 0.25, 0.75, 0.40, 0.60),\n        (0.21, 0.84, 0.63, 0.37, 0.12, 0.88, 0.47, 0.53, 0.19, 0.81),\n    ]\n\n    # Define anisotropy weights\n    alpha_aniso = (1, 1, 4, 4, 4, 4, 4, 4, 4, 4)\n    alpha_iso = (1, 1, 1, 1, 1, 1, 1, 1, 1, 1)\n\n    # Define test cases\n    test_cases = [\n        {'f': f1, 'alpha': alpha_aniso, 'Q': 8, 'eval_points': evaluation_set},\n        {'f': f1, 'alpha': alpha_iso,   'Q': 3, 'eval_points': evaluation_set},\n        {'f': f2, 'alpha': alpha_aniso, 'Q': 6, 'eval_points': evaluation_set},\n        {'f': f1, 'alpha': alpha_aniso, 'Q': 8, 'eval_points': [tuple([0.0]*d)]},\n    ]\n\n    memo_indices = {}\n    memo_coeffs = {}\n\n    def generate_indices(alpha, Q):\n        indices = set()\n        k_levels = []\n\n        def find_k(dim_idx, current_sum):\n            if dim_idx == d:\n                indices.add(tuple(k + 1 for k in k_levels))\n                return\n\n            max_k = (Q - current_sum) // alpha[dim_idx]\n            for ki in range(max_k + 1):\n                k_levels.append(ki)\n                find_k(dim_idx + 1, current_sum + alpha[dim_idx] * ki)\n                k_levels.pop()\n        \n        find_k(0, 0)\n        return indices\n\n    def calculate_coeffs(index_set):\n        coeffs = {}\n        e_vectors = np.identity(d, dtype=int)\n        for l_tuple in index_set:\n            l_vec = np.array(l_tuple)\n            c_l = 0\n            for size in range(d + 1):\n                for J in combinations(range(d), size):\n                    l_prime_vec = l_vec.copy()\n                    for j_idx in J:\n                        l_prime_vec[j_idx] += 1\n                    \n                    if tuple(l_prime_vec) in index_set:\n                        c_l += (-1)**size\n            coeffs[l_tuple] = c_l\n        return coeffs\n    \n    def get_interpolant_evaluator(f_func, y_tuple, l_tuple, memo_f):\n        def recursive_eval(dim, partial_point):\n            if dim == d:\n                point = tuple(partial_point)\n                if point not in memo_f:\n                    memo_f[point] = f_func(point)\n                return memo_f[point]\n\n            k = l_tuple[dim]\n            y_i = y_tuple[dim]\n\n            if k == 1:\n                # Grid is {0, 1}\n                weight = y_i\n                if abs(weight)  1e-15: return recursive_eval(dim + 1, partial_point + [0.0])\n                if abs(weight - 1.0)  1e-15: return recursive_eval(dim + 1, partial_point + [1.0])\n                \n                val_left = recursive_eval(dim + 1, partial_point + [0.0])\n                val_right = recursive_eval(dim + 1, partial_point + [1.0])\n                return (1.0 - weight) * val_left + weight * val_right\n            \n            m = 1  (k - 1)  # 2**(k-1)\n            \n            if abs(y_i - 1.0)  1e-15:\n                return recursive_eval(dim + 1, partial_point + [1.0])\n\n            pos = y_i * m\n            j = int(pos)\n            weight = pos - j\n\n            left_coord = j / m\n            \n            if weight  1e-15:\n                return recursive_eval(dim + 1, partial_point + [left_coord])\n\n            right_coord = (j + 1) / m\n            \n            val_left = recursive_eval(dim + 1, partial_point + [left_coord])\n            val_right = recursive_eval(dim + 1, partial_point + [right_coord])\n            return (1.0 - weight) * val_left + weight * val_right\n\n        return recursive_eval(0, [])\n\n    results = []\n    for case in test_cases:\n        f_func = case['f']\n        alpha = case['alpha']\n        Q = case['Q']\n        eval_points = case['eval_points']\n        \n        case_key = (alpha, Q)\n        \n        if case_key in memo_indices:\n            index_set = memo_indices[case_key]\n        else:\n            index_set = generate_indices(alpha, Q)\n            memo_indices[case_key] = index_set\n\n        if case_key in memo_coeffs:\n            coeffs = memo_coeffs[case_key]\n        else:\n            coeffs = calculate_coeffs(index_set)\n            memo_coeffs[case_key] = coeffs\n        \n        memo_f = {}\n        max_abs_error = 0.0\n\n        for y_tuple in eval_points:\n            approx_val = 0.0\n            for l_tuple in index_set:\n                c_l = coeffs[l_tuple]\n                if abs(c_l)  1e-15:\n                    continue\n                \n                u_l_f_y = get_interpolant_evaluator(f_func, y_tuple, l_tuple, memo_f)\n                approx_val += c_l * u_l_f_y\n            \n            true_val = f_func(y_tuple)\n            abs_error = abs(approx_val - true_val)\n            \n            if abs_error  max_abs_error:\n                max_abs_error = abs_error\n        \n        results.append(max_abs_error)\n\n    print(f\"[{','.join(f'{r:.12f}' for r in results)}]\")\n\nsolve()\n```", "id": "2432646"}, {"introduction": "A crucial aspect of mastering any numerical method is understanding its limitations. This final practice presents a fascinating scenario where a sophisticated, high-order global method (Smolyak-Chebyshev interpolation) is outperformed by a simple, local method (piecewise trilinear interpolation). By constructing a function with a sharp \"kink\"—a feature common in economic models with constraints or thresholds—you will discover the critical role that function smoothness plays in the performance of global polynomial approximations [@problem_id:2399802]. This exercise provides a powerful lesson on choosing the right tool for the problem at hand.", "problem": "Consider the compact domain $\\mathcal{D}=[-1,1]^3$ with coordinates $(x,y,z)$. Your task is to construct a specific three-dimensional function $f:\\mathcal{D}\\to\\mathbb{R}$ that exhibits a nondifferentiable kink along a plane so that, for a range of discretization parameters, global polynomial interpolation based on Chebyshev polynomials combined via a Smolyak sparse grid incurs a larger root mean squared error than local piecewise trilinear interpolation on a uniform tensor grid. Work entirely from first principles: define the function, define both approximation operators precisely, evaluate both on a fixed evaluation grid, and compare the resulting errors.\n\nUse the following mathematical definitions.\n\n1) Function class. For a fixed threshold parameter $\\tau\\in\\mathbb{R}$, define\n$$\nf_\\tau(x,y,z)=\\max\\{0, x+y+z-\\tau\\},\\quad (x,y,z)\\in[-1,1]^3.\n$$\nSelect and explicitly use the particular member $f_{\\tau}$ specified in each test case below.\n\n2) One-dimensional Chebyshev–Lobatto nodes and barycentric weights. For a positive integer $m\\ge 1$, define nodes\n$$\nx_j=\\cos\\left(\\frac{\\pi j}{m-1}\\right),\\quad j=0,1,\\dots,m-1,\n$$\nwith the convention that for $m=1$ the single node is $x_0=0$. The associated barycentric weights $\\lambda_j$ are\n$$\n\\lambda_j = (-1)^j\\cdot \\delta_j,\\quad \\delta_j=\\begin{cases}\\frac{1}{2}, \\text{if } j \\in \\{0, m-1\\} \\text{ and } m \\ge 2 \\\\ 1,  \\text{otherwise.}\\end{cases}\n$$\nFor $m=1$, take $\\lambda_0=1$.\n\nGiven distinct nodes $(x_j)_{j=0}^{m-1}$ with barycentric weights $(\\lambda_j)_{j=0}^{m-1}$ and data values $(y_j)_{j=0}^{m-1}$, the one-dimensional barycentric interpolant $U_m[g]$ of a function $g$ is the polynomial of degree at most $m-1$ defined pointwise by\n$$\nU_m[g](x)=\n\\begin{cases}\ng(x_k), \\text{if }x=x_k\\ \\text{for some }k \\\\ \\\\\n\\dfrac{\\displaystyle \\sum_{j=0}^{m-1}\\dfrac{\\lambda_j}{x-x_j}g(x_j)}{\\displaystyle \\sum_{j=0}^{m-1}\\dfrac{\\lambda_j}{x-x_j}}, \\text{otherwise.}\n\\end{cases}\n$$\n\n3) Smolyak sparse-grid Chebyshev interpolant in $d=3$ dimensions. For a one-dimensional level $\\ell\\in\\mathbb{N}$, set\n$$\nm(\\ell)=\\begin{cases}\n1, \\ell=1,\\\\\n2^{\\ell-1}+1, \\ell\\ge 2,\n\\end{cases}\n$$\nand let $U_{m(\\ell)}$ be the one-dimensional interpolant defined above at the Chebyshev–Lobatto nodes of cardinality $m(\\ell)$. For a total level $L\\in\\mathbb{N}$ and $d=3$, define the Smolyak index set\n$$\n\\mathcal{I}_{L}=\\left\\{\\boldsymbol{\\ell}=(\\ell_1,\\ell_2,\\ell_3)\\in\\mathbb{N}^3:\\ \\ell_i\\ge 1,\\ \\ \\ell_1+\\ell_2+\\ell_3\\le L+2\\right\\}.\n$$\nFor $\\boldsymbol{\\ell}\\in\\mathcal{I}_L$, define the combination coefficient\n$$\nw(\\boldsymbol{\\ell};L)=(-1)^{L+2-(\\ell_1+\\ell_2+\\ell_3)}\\binom{2}{L+2-(\\ell_1+\\ell_2+\\ell_3)}.\n$$\nThe Smolyak interpolant $A_L[f]$ is the linear operator\n$$\nA_L[f](x,y,z)=\\sum_{\\boldsymbol{\\ell}\\in\\mathcal{I}_L} w(\\boldsymbol{\\ell};L)\\,\\big(U_{m(\\ell_1)}\\otimes U_{m(\\ell_2)}\\otimes U_{m(\\ell_3)}\\big)[f](x,y,z),\n$$\nwhere $\\otimes$ denotes the tensor-product application of one-dimensional interpolants along each coordinate.\n\n4) Uniform tensor grid and piecewise trilinear interpolant. For an integer $M\\ge 2$, define a uniform grid on $[-1,1]$ with nodes\n$$\n\\xi_j=-1+\\frac{2j}{M-1},\\quad j=0,1,\\dots,M-1.\n$$\nThe piecewise trilinear interpolant $T_M[f]$ is the unique function that is trilinear on each grid cell and matches $f$ at all tensor grid nodes.\n\n5) Discrete root mean squared error (RMSE). For an integer $E\\ge 2$, define an evaluation grid with $E$ nodes per axis\n$$\n\\zeta_k=-1+\\frac{2k}{E-1},\\quad k=0,1,\\dots,E-1.\n$$\nGiven an approximant $\\hat{f}$ to $f$ on $\\mathcal{D}$, define the discrete RMSE\n$$\n\\mathrm{RMSE}(\\hat{f},f)=\\left(\\frac{1}{E^3}\\sum_{i=0}^{E-1}\\sum_{j=0}^{E-1}\\sum_{k=0}^{E-1}\\big(\\hat{f}(\\zeta_i,\\zeta_j,\\zeta_k)-f(\\zeta_i,\\zeta_j,\\zeta_k)\\big)^2\\right)^{1/2}.\n$$\n\nImplement a complete program that does the following for each test case: constructs the specified $f_\\tau$, constructs $A_L[f_\\tau]$ and $T_M[f_\\tau]$ as defined above, evaluates both approximants and $f_\\tau$ on the same evaluation grid, computes the RMSEs, and reports the scalar differences\n$$\n\\Delta=\\mathrm{RMSE}\\big(A_L[f_\\tau],f_\\tau\\big)-\\mathrm{RMSE}\\big(T_M[f_\\tau],f_\\tau\\big).\n$$\nPositive $\\Delta$ values indicate that Chebyshev interpolation on the Smolyak grid performs worse than piecewise trilinear interpolation.\n\nUse the following test suite, which varies the kink location and the discretization levels to probe typical, boundary-adjacent, and lower-level cases:\n\n- Test case $1$: $\\tau=0.0$, $L=4$, $M=25$, $E=31$.\n- Test case $2$: $\\tau=0.9$, $L=5$, $M=29$, $E=31$.\n- Test case $3$: $\\tau=-0.2$, $L=3$, $M=21$, $E=31$.\n\nFinal output format. Your program should produce a single line of output containing the three numerical results $\\Delta$ from the above test cases, as a comma-separated list enclosed in square brackets, with each scalar rounded to six decimal places (for example, $[\\delta_1,\\delta_2,\\delta_3]$).", "solution": "The problem statement has been subjected to rigorous validation and is determined to be valid. It is scientifically grounded in the principles of numerical analysis and approximation theory, well-posed with all necessary definitions and parameters provided, and expressed in objective, formal language. The problem constitutes a standard, verifiable exercise in computational mathematics, specifically comparing the efficacy of a global polynomial approximation method (Smolyak-Chebyshev) against a local piecewise polynomial method (piecewise trilinear) for a function exhibiting a low degree of smoothness. We will therefore proceed with a complete solution.\n\nThe objective is to compute the difference in root mean squared error, $\\Delta$, between two approximation schemes for the function $f_\\tau(x,y,z)=\\max\\{0, x+y+z-\\tau\\}$ on the domain $\\mathcal{D}=[-1,1]^3$. A positive value of $\\Delta$ signifies that the Smolyak-Chebyshev method yields a larger error than the piecewise trilinear method. The function $f_\\tau$ is continuous but not differentiable along the plane defined by $x+y+z-\\tau=0$, where it has a 'kink'. This lack of smoothness is the central feature being tested.\n\nOur procedure is as follows:\n1.  Define the target function $f_\\tau$ and the evaluation grid.\n2.  Construct the piecewise trilinear interpolant $T_M[f_\\tau]$ and evaluate it on the grid.\n3.  Construct the Smolyak sparse-grid interpolant $A_L[f_\\tau]$ and evaluate it on the same grid.\n4.  Compute the discrete root mean squared error (RMSE) for both approximations against the true function.\n5.  Calculate the difference $\\Delta = \\mathrm{RMSE}(A_L[f_\\tau]) - \\mathrm{RMSE}(T_M[f_\\tau])$.\n\n**1. Evaluation Framework**\n\nFor each test case, we are given a parameter $\\tau$ for the function $f_\\tau$, a level $L$ for the Smolyak grid, a grid size $M$ for the trilinear interpolant, and a resolution $E$ for the evaluation grid. The evaluation grid is a uniform tensor-product grid on $\\mathcal{D}=[-1,1]^3$ with $E$ points along each axis, given by nodes $\\zeta_k=-1+\\frac{2k}{E-1}$ for $k=0, \\dots, E-1$. All approximations and the true function will be computed at each of the $E^3$ points of this grid.\n\n**2. Piecewise Trilinear Interpolant $T_M[f_\\tau]$**\n\nThis method relies on a local approximation within each cell of a uniform grid.\nFirst, a uniform tensor grid is defined with $M$ nodes per axis: $\\xi_j=-1+\\frac{2j}{M-1}$ for $j=0, \\dots, M-1$. This partitions the domain $\\mathcal{D}$ into $(M-1)^3$ cubic cells. The function $f_\\tau$ is evaluated at each of the $M^3$ grid nodes.\n\nTo find the value of the interpolant $T_M[f_\\tau]$ at an evaluation point $(x,y,z)$, we first identify the cell $[\\xi_{j_x}, \\xi_{j_x+1}] \\times [\\xi_{j_y}, \\xi_{j_y+1}] \\times [\\xi_{j_z}, \\xi_{j_z+1}]$ that contains the point. The value is then computed by trilinear interpolation using the eight known function values at the corners of this cell. This is implemented efficiently by utilizing the `scipy.ndimage.map_coordinates` function, which performs N-dimensional piecewise linear interpolation. The evaluation coordinates $(\\zeta_i, \\zeta_j, \\zeta_k)$ are mapped to the index-based coordinate system of the $M$-point grid before being passed to the function.\n\n**3. Smolyak Sparse-Grid Interpolant $A_L[f_\\tau]$**\n\nThe Smolyak construction provides a way to build a high-dimensional interpolant from a combination of one-dimensional interpolants, achieving better scaling with dimension than a full tensor product for sufficiently smooth functions. The operator for dimension $d=3$ and level $L$ is:\n$$\nA_L[f] = \\sum_{\\boldsymbol{\\ell}\\in\\mathcal{I}_L} w(\\boldsymbol{\\ell};L)\\,\\big(U_{m(\\ell_1)}\\otimes U_{m(\\ell_2)}\\otimes U_{m(\\ell_3)}\\big)[f]\n$$\nwhere $\\boldsymbol{\\ell}=(\\ell_1,\\ell_2,\\ell_3)$ is a multi-index of one-dimensional levels. The components are:\n-   **Index Set**: $\\mathcal{I}_{L}=\\left\\{\\boldsymbol{\\ell}\\in\\mathbb{N}^3:\\ \\ell_1+\\ell_2+\\ell_3\\le L+2\\right\\}$.\n-   **Combination Coefficients**: $w(\\boldsymbol{\\ell};L)=(-1)^{\\,L+2-|\\boldsymbol{\\ell}|_1}\\binom{2}{\\,L+2-|\\boldsymbol{\\ell}|_1\\,}$, where $|\\boldsymbol{\\ell}|_1=\\ell_1+\\ell_2+\\ell_3$. These coefficients are non-zero only for $L \\le |\\boldsymbol{\\ell}|_1 \\le L+2$.\n-   **One-Dimensional Interpolant**: $U_{m(\\ell)}$ is the barycentric polynomial interpolant at $m(\\ell)$ Chebyshev-Lobatto nodes. The number of nodes is $m(1)=1$ and $m(\\ell)=2^{\\ell-1}+1$ for $\\ell \\ge 2$.\n\nThe evaluation of $A_L[f_\\tau]$ on the evaluation grid proceeds as follows:\nAn accumulator grid, initialized to zeros, stores the final approximation values. We iterate through each multi-index $\\boldsymbol{\\ell}$ in the active index set (where $w(\\boldsymbol{\\ell};L) \\neq 0$). For each $\\boldsymbol{\\ell}$:\na. A sparse tensor-product grid is formed from the $m(\\ell_1)$, $m(\\ell_2)$, and $m(\\ell_3)$ Chebyshev-Lobatto nodes along the $x$, $y$, and $z$ axes, respectively.\nb. The true function $f_\\tau$ is evaluated at all nodes of this sparse grid.\nc. A three-dimensional tensor-product barycentric interpolation is performed from this sparse grid to the dense $E \\times E \\times E$ evaluation grid. This is accomplished by applying the one-dimensional barycentric interpolation formula sequentially along each dimension.\nd. The resulting interpolated values are multiplied by the coefficient $w(\\boldsymbol{\\ell};L)$ and added to the accumulator grid.\n\nThe one-dimensional barycentric interpolation formula is highly effective for Chebyshev points due to its numerical stability. For a set of nodes $(x_j)$, values $(y_j)$, and barycentric weights $(\\lambda_j)$, the interpolant at a point $x$ is given by $\\left(\\sum_j \\frac{\\lambda_j y_j}{x-x_j}\\right) / \\left(\\sum_j \\frac{\\lambda_j}{x-x_j}\\right)$.\n\n**4. Error Calculation and Comparison**\n\nAfter computing the values of both approximations, $A_L[f_\\tau]$ and $T_M[f_\\tau]$, on the evaluation grid, their respective discrete root mean squared errors are calculated:\n$$\n\\mathrm{RMSE}(\\hat{f},f)=\\left(\\frac{1}{E^3}\\sum_{i,j,k=0}^{E-1}\\big(\\hat{f}(\\zeta_i,\\zeta_j,\\zeta_k)-f_\\tau(\\zeta_i,\\zeta_j,\\zeta_k)\\big)^2\\right)^{1/2}\n$$\nThe final result for each test case is the difference $\\Delta = \\mathrm{RMSE}(A_L[f_\\tau]) - \\mathrm{RMSE}(T_M[f_\\tau])$. The function $f_\\tau$ is linear almost everywhere, but its kink violates the smoothness assumptions that guarantee rapid convergence for global polynomial methods. In contrast, piecewise linear methods are local; their error is primarily confined to the cells intersected by the kink. It is therefore anticipated that for the given parameters, $\\Delta$ will be positive, demonstrating a scenario where the local, lower-order method is superior.", "answer": "```python\nimport numpy as np\nfrom scipy.special import comb\nfrom scipy.ndimage import map_coordinates\n\ndef get_cheby_grid(m, cache):\n    \"\"\"\n    Computes or retrieves from cache the Chebyshev-Lobatto nodes and barycentric weights.\n    \"\"\"\n    if m in cache:\n        return cache[m]\n\n    if m == 1:\n        nodes = np.array([0.0])\n        weights = np.array([1.0])\n        cache[m] = (nodes, weights)\n        return nodes, weights\n\n    j = np.arange(m)\n    nodes = np.cos(np.pi * j / (m - 1))\n    \n    weights = (-1.0)**j\n    weights[0] *= 0.5\n    weights[-1] *= 0.5\n    \n    cache[m] = (nodes, weights)\n    return nodes, weights\n\ndef barycentric_interp1d(x_eval, x_nodes, y_nodes, bary_weights):\n    \"\"\"\n    Performs 1D barycentric interpolation on a vector of evaluation points.\n    \"\"\"\n    Ne = x_eval.shape[0]\n    Nm = x_nodes.shape[0]\n    \n    if Nm == 1:\n        return np.full(Ne, y_nodes[0])\n\n    interp_vals = np.zeros(Ne, dtype=np.float64)\n    exact_matches = np.zeros(Ne, dtype=bool)\n\n    # Handle cases where evaluation points are very close to nodes\n    for j in range(Nm):\n        matches = np.isclose(x_eval, x_nodes[j])\n        if np.any(matches):\n            interp_vals[matches] = y_nodes[j]\n            exact_matches |= matches\n\n    # Process points that are not nodes using the vectorized formula\n    non_match_indices = np.where(~exact_matches)[0]\n    if len(non_match_indices) > 0:\n        x_sub_eval = x_eval[non_match_indices]\n        \n        diff = x_sub_eval[:, None] - x_nodes[None, :]\n        temp = bary_weights[None, :] / diff\n        \n        numerator = np.sum(temp * y_nodes[None, :], axis=1)\n        denominator = np.sum(temp, axis=1)\n\n        # Avoid division by zero, although it is unlikely for non-node points\n        result = np.divide(numerator, denominator, \n                           out=np.zeros_like(numerator), \n                           where=denominator != 0)\n        \n        interp_vals[non_match_indices] = result\n\n    return interp_vals\n\ndef compute_smolyak_approx(f, L, eval_grid_1d, cache):\n    \"\"\"\n    Computes the Smolyak sparse grid approximation on the evaluation grid.\n    \"\"\"\n    E = len(eval_grid_1d)\n    approx_values = np.zeros((E, E, E), dtype=np.float64)\n\n    # Generate Smolyak index set and coefficients\n    indices_and_coeffs = []\n    max_level_sum = L + 2\n    for l1 in range(1, max_level_sum + 1):\n        for l2 in range(1, max_level_sum - l1 + 1):\n            for l3 in range(1, max_level_sum - l1 - l2 + 1):\n                level_sum = l1 + l2 + l3\n                k = max_level_sum - level_sum\n                if 0 = k = 2:\n                    w = ((-1)**k) * comb(2, k, exact=True)\n                    if w != 0:\n                        indices_and_coeffs.append(((l1, l2, l3), w))\n\n    m_func = lambda l: 1 if l == 1 else 2**(l - 1) + 1\n\n    for (l1, l2, l3), w in indices_and_coeffs:\n        m1, m2, m3 = m_func(l1), m_func(l2), m_func(l3)\n        \n        nodes1, weights1 = get_cheby_grid(m1, cache)\n        nodes2, weights2 = get_cheby_grid(m2, cache)\n        nodes3, weights3 = get_cheby_grid(m3, cache)\n\n        grid_x, grid_y, grid_z = np.meshgrid(nodes1, nodes2, nodes3, indexing='ij')\n        f_on_grid = f(grid_x, grid_y, grid_z)\n        \n        # Sequentially apply 1D interpolation\n        interp1 = np.zeros((E, m2, m3), dtype=np.float64)\n        for j in range(m2):\n            for k in range(m3):\n                interp1[:, j, k] = barycentric_interp1d(eval_grid_1d, nodes1, f_on_grid[:, j, k], weights1)\n\n        interp2 = np.zeros((E, E, m3), dtype=np.float64)\n        for i in range(E):\n            for k in range(m3):\n                interp2[i, :, k] = barycentric_interp1d(eval_grid_1d, nodes2, interp1[i, :, k], weights2)\n\n        tensor_prod_interp = np.zeros((E, E, E), dtype=np.float64)\n        for i in range(E):\n            for j in range(E):\n                tensor_prod_interp[i, j, :] = barycentric_interp1d(eval_grid_1d, nodes3, interp2[i, j, :], weights3)\n        \n        approx_values += w * tensor_prod_interp\n        \n    return approx_values\n\ndef compute_trilinear_approx(f, M, eval_xx, eval_yy, eval_zz):\n    \"\"\"\n    Computes the piecewise trilinear approximation on the evaluation grid.\n    \"\"\"\n    grid_nodes_1d = np.linspace(-1., 1., M)\n    grid_x, grid_y, grid_z = np.meshgrid(grid_nodes_1d, grid_nodes_1d, grid_nodes_1d, indexing='ij')\n    f_on_uniform_grid = f(grid_x, grid_y, grid_z)\n    \n    # Map physical coordinates of the evaluation grid to the index coordinates of the M-point grid\n    h = 2.0 / (M - 1)\n    coords_x = (eval_xx - (-1)) / h\n    coords_y = (eval_yy - (-1)) / h\n    coords_z = (eval_zz - (-1)) / h\n    \n    coords = np.stack([coords_x, coords_y, coords_z])\n    \n    # Use SciPy's map_coordinates for efficient N-D linear interpolation\n    approx_values = map_coordinates(f_on_uniform_grid, coords, order=1, mode='nearest')\n    \n    return approx_values\n\ndef solve():\n    \"\"\"\n    Main driver function to run test cases and compute the error difference.\n    \"\"\"\n    test_cases = [\n        (0.0, 4, 25, 31),\n        (0.9, 5, 29, 31),\n        (-0.2, 3, 21, 31),\n    ]\n\n    results = []\n    cheby_cache = {}\n\n    for i, (tau, L, M, E) in enumerate(test_cases):\n        f = lambda x, y, z: np.maximum(0., x + y + z - tau)\n        \n        eval_grid_1d = np.linspace(-1., 1., E)\n        eval_xx, eval_yy, eval_zz = np.meshgrid(eval_grid_1d, eval_grid_1d, eval_grid_1d, indexing='ij')\n        \n        f_true_values = f(eval_xx, eval_yy, eval_zz)\n\n        # Smolyak-Chebyshev approximation\n        A_L_values = compute_smolyak_approx(f, L, eval_grid_1d, cheby_cache)\n        rmse_A = np.sqrt(np.mean((A_L_values - f_true_values)**2))\n\n        # Piecewise trilinear approximation\n        T_M_values = compute_trilinear_approx(f, M, eval_xx, eval_yy, eval_zz)\n        rmse_T = np.sqrt(np.mean((T_M_values - f_true_values)**2))\n\n        delta = rmse_A - rmse_T\n        results.append(delta)\n\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2399802"}]}