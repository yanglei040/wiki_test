{"hands_on_practices": [{"introduction": "We begin our hands-on journey with a foundational task in Bayesian analysis: estimating an unknown proportion. This exercise applies the Beta-Binomial conjugate model, a cornerstone of Bayesian statistics, to a modern problem in financial text analysis [@problem_id:2375504]. By quantifying the sentiment of central bank communications, you will practice calculating essential posterior summaries like the mean and credible intervals, building a solid foundation for more complex Bayesian models.", "problem": "You are given a simplified, model-based setup for quantifying the sentiment of central bank communications through the relative frequency of terms categorized as \"dovish\" and \"hawkish.\" For a single document, let $D$ denote the count of \"dovish\" terms and let $H$ denote the count of \"hawkish\" terms extracted by a pre-specified lexicon. Let $N$ denote the total number of categorized terms, so $N = D + H$. Assume that each categorized term is independently generated and that the unknown probability that a categorized term is \"dovish\" is $p \\in (0,1)$. Conditional on $N$, model the count $D$ as a Binomial random variable with parameter $p$. Place a Beta prior on $p$, so $p \\sim \\text{Beta}(a,b)$ with shape parameters $a  0$ and $b  0$.  \n\nDefine the documentâ€™s \"dovishness index\" as the posterior mean of $p$ under the model above. Define an equal-tailed credible interval (CI) for $p$ at level $c \\in (0,1)$ as the interval whose lower and upper endpoints are the $\\alpha/2$ and $1-\\alpha/2$ posterior quantiles of $p$, respectively, where $\\alpha = 1-c$. For classification support, define the posterior probability that the document is more \"dovish than hawkish\" at threshold $\\tau \\in (0,1)$ as $\\mathbb{P}(p  \\tau \\mid D,H,a,b)$. All probability values must be expressed as decimals in $[0,1]$.  \n\nFor each test case below, compute and return the following four quantities for the specified parameters $(D,H,a,b,c,\\tau)$:\n- the posterior mean of $p$,\n- the lower endpoint of the equal-tailed CI at level $c$,\n- the upper endpoint of the equal-tailed CI at level $c$,\n- the posterior probability $\\mathbb{P}(p  \\tau \\mid D,H,a,b)$.\n\nTest suite (each line is one test case $(D,H,a,b,c,\\tau)$):\n- $(D,H,a,b,c,\\tau) = (\\,30,\\,20,\\,2,\\,2,\\,0.95,\\,0.5\\,)$\n- $(D,H,a,b,c,\\tau) = (\\,0,\\,0,\\,1,\\,1,\\,0.9,\\,0.5\\,)$\n- $(D,H,a,b,c,\\tau) = (\\,50,\\,10,\\,0.5,\\,0.5,\\,0.95,\\,0.5\\,)$\n- $(D,H,a,b,c,\\tau) = (\\,1,\\,9,\\,5,\\,1,\\,0.9,\\,0.5\\,)$\n- $(D,H,a,b,c,\\tau) = (\\,100,\\,100,\\,10,\\,10,\\,0.99,\\,0.5\\,)$\n\nYour program should produce a single line of output containing the results as a comma-separated list of lists with no spaces, where each inner list corresponds to one test case and is ordered as $[\\,\\text{posterior\\_mean},\\,\\text{CI\\_lower},\\,\\text{CI\\_upper},\\,\\mathbb{P}(p\\tau)\\,]$. Each decimal must be rounded to $6$ digits after the decimal point. For example, the final output format must look like $[[m_1,l_1,u_1,q_1],[m_2,l_2,u_2,q_2],\\dots]$ with no spaces anywhere.", "solution": "The problem requires the application of Bayesian inference to a standard conjugate model. The core principle is that a Beta distribution is a conjugate prior for the parameter of a Binomial likelihood. This property allows for a closed-form analytical expression for the posterior distribution, which simplifies computation significantly.\n\nLet the prior distribution for the probability $p$ be a Beta distribution with shape parameters $a$ and $b$:\n$$ p \\sim \\text{Beta}(a,b) $$\nThe probability density function (PDF) is given by:\n$$ f(p; a, b) = \\frac{p^{a-1}(1-p)^{b-1}}{B(a,b)} $$\nwhere $B(a,b)$ is the Beta function.\n\nThe observed data consists of $D$ \"dovish\" terms and $H$ \"hawkish\" terms, for a total of $N = D+H$ terms. The likelihood of observing $D$ dovish terms in a sample of size $N$, given the probability $p$, is described by the Binomial distribution:\n$$ D \\mid p, N \\sim \\text{Binomial}(N, p) $$\nThe likelihood function is:\n$$ L(p \\mid D, N) = \\binom{N}{D} p^D (1-p)^{N-D} $$\n\nAccording to Bayes' theorem, the posterior distribution of $p$ is proportional to the product of the likelihood and the prior:\n$$ f(p \\mid D, N, a, b) \\propto L(p \\mid D, N) \\times f(p; a, b) $$\n$$ f(p \\mid D, N, a, b) \\propto \\left( p^D (1-p)^{N-D} \\right) \\times \\left( p^{a-1} (1-p)^{b-1} \\right) $$\nCombining the terms with $p$ and $(1-p)$:\n$$ f(p \\mid D, N, a, b) \\propto p^{D+a-1} (1-p)^{N-D+b-1} $$\nSubstituting $N = D+H$:\n$$ f(p \\mid D, N, a, b) \\propto p^{D+a-1} (1-p)^{H+b-1} $$\nThis expression is the kernel of a Beta distribution with updated parameters $a' = a+D$ and $b' = b+H$. Thus, the posterior distribution is:\n$$ p \\mid D, H, a, b \\sim \\text{Beta}(a', b') = \\text{Beta}(a+D, b+H) $$\n\nWith the posterior distribution identified, we can compute the four required quantities.\n\n1.  **Posterior Mean (Dovishness Index)**: The mean of a $\\text{Beta}(a', b')$ distribution is given by:\n    $$ E[p \\mid D, H, a, b] = \\frac{a'}{a'+b'} = \\frac{a+D}{a+D+b+H} $$\n\n2.  **Equal-Tailed Credible Interval (CI)**: For a confidence level $c$, we set $\\alpha = 1-c$. The equal-tailed CI is defined by the $\\frac{\\alpha}{2}$ and $1-\\frac{\\alpha}{2}$ quantiles of the posterior distribution. Let $F_{a',b'}^{-1}(q)$ be the quantile function (inverse CDF) of the $\\text{Beta}(a', b')$ distribution for a probability $q$.\n    - Lower endpoint: $CI_{\\text{lower}} = F_{a',b'}^{-1}\\left(\\frac{\\alpha}{2}\\right) = F_{a+D, b+H}^{-1}\\left(\\frac{1-c}{2}\\right)$\n    - Upper endpoint: $CI_{\\text{upper}} = F_{a',b'}^{-1}\\left(1 - \\frac{\\alpha}{2}\\right) = F_{a+D, b+H}^{-1}\\left(1 - \\frac{1-c}{2}\\right)$\n\n3.  **Posterior Probability $\\mathbb{P}(p  \\tau)$**: This is the probability that $p$ exceeds a certain threshold $\\tau$, calculated from the posterior distribution. This is equivalent to the survival function (SF), $S(x) = 1 - F(x)$, evaluated at $\\tau$.\n    $$ \\mathbb{P}(p  \\tau \\mid D,H,a,b) = \\int_{\\tau}^{1} f(p \\mid D,H,a,b) \\,dp = 1 - F_{a',b'}(\\tau) = S_{a+D, b+H}(\\tau) $$\n    where $F_{a',b'}$ and $S_{a',b'}$ are the CDF and SF of the $\\text{Beta}(a', b')$ distribution, respectively. Using the survival function is computationally preferred for numerical stability, especially for $\\tau$ near $1$.\n\nThese formulae will be applied to each test case using the provided parameters. The calculations for quantiles and the survival function will be performed using standard numerical libraries.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import beta\n\ndef solve():\n    \"\"\"\n    Computes Bayesian sentiment metrics for a series of test cases.\n\n    For each case, it calculates the posterior mean, credible interval,\n    and a posterior probability based on a Beta-Binomial model.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each tuple is in the format (D, H, a, b, c, tau)\n    test_cases = [\n        (30, 20, 2, 2, 0.95, 0.5),\n        (0, 0, 1, 1, 0.9, 0.5),\n        (50, 10, 0.5, 0.5, 0.95, 0.5),\n        (1, 9, 5, 1, 0.9, 0.5),\n        (100, 100, 10, 10, 0.99, 0.5),\n    ]\n\n    all_results = []\n    for case in test_cases:\n        D, H, a, b, c, tau = case\n\n        # Calculate posterior parameters for the Beta distribution\n        # a_prime = a + D and b_prime = b + H\n        a_prime = a + D\n        b_prime = b + H\n\n        # 1. Calculate the posterior mean\n        posterior_mean = a_prime / (a_prime + b_prime)\n\n        # 2. Calculate the equal-tailed credible interval\n        alpha = 1.0 - c\n        ci_lower = beta.ppf(alpha / 2.0, a_prime, b_prime)\n        ci_upper = beta.ppf(1.0 - alpha / 2.0, a_prime, b_prime)\n\n        # 3. Calculate the posterior probability P(p  tau)\n        # This is the survival function (1 - CDF) of the posterior Beta distribution\n        prob_p_gt_tau = beta.sf(tau, a_prime, b_prime)\n        \n        case_results = [\n            posterior_mean,\n            ci_lower,\n            ci_upper,\n            prob_p_gt_tau\n        ]\n        all_results.append(case_results)\n\n    # Format the final output string according to the specification:\n    # A comma-separated list of lists with no spaces, with values rounded to 6 decimal places.\n    # Example: [[m1,l1,u1,q1],[m2,l2,u2,q2],...]\n    inner_parts = []\n    for res_list in all_results:\n        # Format each number to 6 decimal places\n        formatted_vals = [f\"{val:.6f}\" for val in res_list]\n        # Join numbers with a comma and enclose in brackets\n        inner_parts.append(f\"[{','.join(formatted_vals)}]\")\n    \n    # Join the inner lists with a comma and enclose in brackets for the final string\n    final_output_str = f\"[{','.join(inner_parts)}]\"\n\n    print(final_output_str)\n\nsolve()\n```", "id": "2375504"}, {"introduction": "Beyond estimating parameters for a single model, a key task in science and economics is comparing competing theories. This practice introduces the Bayes factor, a principal tool for Bayesian model comparison that quantifies the evidence provided by the data in favor of one model over another [@problem_id:2375563]. You will implement the calculation of the marginal likelihood for linear models and, critically, investigate how the choice of prior distributions can influence model selection, a subtle but crucial aspect of applied Bayesian work.", "problem": "You are given two competing linear time series models intended as stylized representations of two economic theories for fluctuations in gross domestic product (GDP) growth. The first model, $M_{K}$, represents a simplified Keynesian mechanism with an intercept capturing demand shifts. The second model, $M_{R}$, represents a simplified Real Business Cycle (RBC) mechanism with deeper persistence through two lags and no intercept. For a univariate sequence $\\{y_t\\}_{t=1}^{T}$ of demeaned real GDP growth rates, define the models over the common sample $t=3,\\dots,T$ as follows:\n- Model $M_{K}$ (Keynesian): for $t=3,\\dots,T$, $y_t = \\alpha + \\phi y_{t-1} + \\varepsilon_t$, with $\\varepsilon_t \\sim \\mathcal{N}(0,\\sigma^2)$.\n- Model $M_{R}$ (Real Business Cycle (RBC)): for $t=3,\\dots,T$, $y_t = \\beta_1 y_{t-1} + \\beta_2 y_{t-2} + \\varepsilon_t$, with $\\varepsilon_t \\sim \\mathcal{N}(0,\\sigma^2)$.\n\nFor each model, the parameters and prior distributions are:\n- For $M_{K}$, $\\theta_K = (\\alpha,\\phi,\\sigma^2)$ with prior\n  $\\alpha,\\phi \\mid \\sigma^2 \\sim \\mathcal{N}(m_{0,K}, \\sigma^2 V_{0,K})$ and $\\sigma^2 \\sim \\text{Inverse-Gamma}(a_{0,K}, b_{0,K})$.\n- For $M_{R}$, $\\theta_R = (\\beta_1,\\beta_2,\\sigma^2)$ with prior\n  $\\beta_1,\\beta_2 \\mid \\sigma^2 \\sim \\mathcal{N}(m_{0,R}, \\sigma^2 V_{0,R})$ and $\\sigma^2 \\sim \\text{Inverse-Gamma}(a_{0,R}, b_{0,R})$.\n\nThe Inverse-Gamma prior is parameterized by the density\n$$\np(\\sigma^2) \\;=\\; \\frac{b_0^{a_0}}{\\Gamma(a_0)} \\, (\\sigma^2)^{-(a_0+1)} \\exp\\!\\left(-\\frac{b_0}{\\sigma^2}\\right),\n$$\nwhere $\\Gamma(\\cdot)$ is the Gamma function.\n\nYou must compute, for each dataset and each specified prior configuration, the natural logarithm of the Bayes factor in favor of $M_{K}$ over $M_{R}$, that is,\n$$\n\\log \\text{BF}_{K,R} \\;=\\; \\log p(y \\mid M_{K}) \\;-\\; \\log p(y \\mid M_{R}),\n$$\nwhere $p(y \\mid M)$ denotes the marginal likelihood under model $M$ evaluated on the common sample $t=3,\\dots,T$.\n\nDatasets (each sequence is $y_1,\\dots,y_T$), with $T$ specified implicitly by length:\n- Dataset $1$: $[0.00, 0.10, 0.18, 0.24, 0.29, 0.31, 0.33, 0.35]$.\n- Dataset $2$: $[0.00, 0.30, 0.20, 0.27, 0.18, 0.25, 0.17, 0.24]$.\n- Dataset $3$: $[0.05, 0.02, 0.03, 0.01]$.\n\nTwo distinct prior specifications are to be used:\n\n- Prior specification $\\mathcal{A}$ (diffuse, symmetric across models):\n  - For $M_{K}$: $m_{0,K}^{\\mathcal{A}} = \\begin{bmatrix} 0.00 \\\\ 0.00 \\end{bmatrix}$, $V_{0,K}^{\\mathcal{A}} = \\operatorname{diag}(10000.00, 10000.00)$, $a_{0,K}^{\\mathcal{A}} = 0.01$, $b_{0,K}^{\\mathcal{A}} = 0.01$.\n  - For $M_{R}$: $m_{0,R}^{\\mathcal{A}} = \\begin{bmatrix} 0.00 \\\\ 0.00 \\end{bmatrix}$, $V_{0,R}^{\\mathcal{A}} = \\operatorname{diag}(10000.00, 10000.00)$, $a_{0,R}^{\\mathcal{A}} = 0.01$, $b_{0,R}^{\\mathcal{A}} = 0.01$.\n\n- Prior specification $\\mathcal{B}$ (informative, model-specific):\n  - For $M_{K}$: $m_{0,K}^{\\mathcal{B}} = \\begin{bmatrix} 0.02 \\\\ 0.80 \\end{bmatrix}$, $V_{0,K}^{\\mathcal{B}} = \\operatorname{diag}(0.01, 0.01)$, $a_{0,K}^{\\mathcal{B}} = 2.00$, $b_{0,K}^{\\mathcal{B}} = 0.01$.\n  - For $M_{R}$: $m_{0,R}^{\\mathcal{B}} = \\begin{bmatrix} 0.60 \\\\ 0.20 \\end{bmatrix}$, $V_{0,R}^{\\mathcal{B}} = \\operatorname{diag}(0.01, 0.01)$, $a_{0,R}^{\\mathcal{B}} = 2.00$, $b_{0,R}^{\\mathcal{B}} = 0.01$.\n\nTest Suite and Answer Specification:\n- The common sample for likelihood evaluation is $t=3,\\dots,T$ for both models and all datasets.\n- For each dataset $i \\in \\{1,2,3\\}$ and each prior specification $P \\in \\{\\mathcal{A},\\mathcal{B}\\}$, compute the scalar $\\log \\text{BF}_{K,R}^{(i,P)}$.\n- Your program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the exact order:\n$$\n[\\log \\text{BF}_{K,R}^{(1,\\mathcal{A})}, \\log \\text{BF}_{K,R}^{(1,\\mathcal{B})}, \\log \\text{BF}_{K,R}^{(2,\\mathcal{A})}, \\log \\text{BF}_{K,R}^{(2,\\mathcal{B})}, \\log \\text{BF}_{K,R}^{(3,\\mathcal{A})}, \\log \\text{BF}_{K,R}^{(3,\\mathcal{B})}]\n$$\nAll six outputs must be floating-point numbers. No additional text or formatting is permitted beyond this single line.", "solution": "The objective is to compute the log Bayes Factor $\\log \\text{BF}_{K,R}$, which requires calculating the log marginal likelihood, $\\log p(y \\mid M)$, for each model $M \\in \\{M_K, M_R\\}$.\n\nBoth models fit the framework of a general linear model:\n$$\ny = X\\beta + \\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_N)\n$$\nwhere $y$ is a vector of $N$ observations, $X$ is an $N \\times k$ design matrix, and $\\beta$ is a $k \\times 1$ vector of regression coefficients. The likelihood function is $p(y \\mid \\beta, \\sigma^2) = (2\\pi\\sigma^2)^{-N/2} \\exp(-\\frac{1}{2\\sigma^2}(y-X\\beta)^T(y-X\\beta))$.\n\nThe specified prior is a Normal-Inverse-Gamma distribution:\n$$\np(\\beta, \\sigma^2) = p(\\beta \\mid \\sigma^2) p(\\sigma^2)\n$$\nwhere $\\beta \\mid \\sigma^2 \\sim \\mathcal{N}(m_0, \\sigma^2 V_0)$ and $\\sigma^2 \\sim \\text{Inverse-Gamma}(a_0, b_0)$. This is a conjugate prior for the normal linear model.\n\nThe marginal likelihood $p(y \\mid M)$ is obtained by integrating the product of the likelihood and the prior over all parameters:\n$$\np(y \\mid M) = \\int_0^\\infty \\int_{\\mathbb{R}^k} p(y \\mid \\beta, \\sigma^2) p(\\beta \\mid \\sigma^2) p(\\sigma^2) \\, d\\beta \\, d\\sigma^2\n$$\nFor the specified conjugate setup, this integral has a closed-form solution. The posterior distribution for the parameters is also Normal-Inverse-Gamma, $p(\\beta, \\sigma^2 \\mid y) \\propto p(\\beta, \\sigma^2)p(y \\mid \\beta, \\sigma^2)$, with updated parameters:\n- Posterior precision matrix for $\\beta$: $V_n^{-1} = V_0^{-1} + X^T X$\n- Posterior mean for $\\beta$: $m_n = V_n (V_0^{-1}m_0 + X^T y)$\n- Posterior shape for $\\sigma^2$: $a_n = a_0 + N/2$\n- Posterior rate for $\\sigma^2$: $b_n = b_0 + \\frac{1}{2}(y^T y + m_0^T V_0^{-1} m_0 - m_n^T V_n^{-1} m_n)$\n\nThe marginal likelihood is the normalizing constant of the posterior, given by:\n$$\np(y \\mid M) = \\frac{\\Gamma(a_n)}{\\Gamma(a_0)} \\frac{b_0^{a_0}}{b_n^{a_n}} (2\\pi)^{-N/2} \\frac{|V_n|^{1/2}}{|V_0|^{1/2}}\n$$\nTo prevent numerical underflow and simplify calculations, we work with the natural logarithm of the marginal likelihood:\n$$\n\\log p(y \\mid M) = \\log\\Gamma(a_n) - \\log\\Gamma(a_0) + a_0\\log(b_0) - a_n\\log(b_n) - \\frac{N}{2}\\log(2\\pi) + \\frac{1}{2}(\\log|V_n| - \\log|V_0|)\n$$\nThe log-Gamma function, $\\log\\Gamma(\\cdot)$, is provided by standard scientific libraries. The log-determinants $\\log|V|$ are computed robustly.\n\nFor each dataset and model, we first construct the response vector $y$ and design matrix $X$ using the common sample for $t=3, \\dots, T$. The number of observations is $N = T - 2$.\nThe response vector is $y = [y_3, y_4, \\dots, y_T]^T$.\n\nFor model $M_K (y_t = \\alpha + \\phi y_{t-1} + \\varepsilon_t)$, the parameter vector is $\\beta_K = [\\alpha, \\phi]^T$, and the design matrix is:\n$$\nX_K = \\begin{bmatrix} 1  y_2 \\\\ 1  y_3 \\\\ \\vdots  \\vdots \\\\ 1  y_{T-1} \\end{bmatrix}\n$$\nThis is an $N \\times 2$ matrix.\n\nFor model $M_R (y_t = \\beta_1 y_{t-1} + \\beta_2 y_{t-2} + \\varepsilon_t)$, the parameter vector is $\\beta_R = [\\beta_1, \\beta_2]^T$, and the design matrix is:\n$$\nX_R = \\begin{bmatrix} y_2  y_1 \\\\ y_3  y_2 \\\\ \\vdots  \\vdots \\\\ y_{T-1}  y_{T-2} \\end{bmatrix}\n$$\nThis is also an $N \\times 2$ matrix.\n\nThe procedure for each of the six required computations is as follows:\n1. Select a dataset and a prior specification.\n2. Construct the response vector $y$ and the design matrices $X_K$ and $X_R$.\n3. For model $M_K$, use its corresponding prior hyperparameters ($m_{0,K}, V_{0,K}, a_{0,K}, b_{0,K}$) to calculate the posterior parameters ($V_{n,K}, m_{n,K}, a_{n,K}, b_{n,K}$) and then the log marginal likelihood $\\log p(y \\mid M_K)$ using the formula above.\n4. For model $M_R$, repeat step 3 with its corresponding prior hyperparameters to find $\\log p(y \\mid M_R)$.\n5. Compute the log Bayes factor: $\\log \\text{BF}_{K,R} = \\log p(y \\mid M_K) - \\log p(y \\mid M_R)$.\nThis process is repeated for each dataset-prior pair as specified in the problem statement. The algorithm will be implemented in Python using the `numpy` library for linear algebra and `scipy` for the log-Gamma function.", "answer": "```python\nimport numpy as np\nfrom scipy.special import gammaln\nfrom numpy.linalg import inv, slogdet\n\ndef calculate_log_marginal_likelihood(y_data, X, m0, V0, a0, b0):\n    \"\"\"\n    Calculates the log marginal likelihood for a linear model with a Normal-Inverse-Gamma prior.\n    \n    Args:\n        y_data (np.ndarray): The dependent variable vector (N,).\n        X (np.ndarray): The design matrix (N, k).\n        m0 (np.ndarray): The prior mean vector for beta (k,).\n        V0 (np.ndarray): The prior covariance matrix for beta (k, k).\n        a0 (float): The prior shape parameter for sigma^2.\n        b0 (float): The prior rate parameter for sigma^2.\n\n    Returns:\n        float: The log marginal likelihood.\n    \"\"\"\n    N, k = X.shape\n\n    # Calculation of posterior parameters for beta (Vn, mn)\n    V0_inv = inv(V0)\n    Vn_inv = V0_inv + X.T @ X\n    Vn = inv(Vn_inv)\n    mn = Vn @ (V0_inv @ m0 + X.T @ y_data)\n    \n    # Calculation of posterior parameters for sigma^2 (an, bn)\n    an = a0 + N / 2\n    \n    # Quadratic forms for bn\n    # bn = b0 + 0.5 * (y'y + m0'V0_inv*m0 - mn'Vn_inv*mn)\n    ss_data = y_data.T @ y_data\n    ss_prior = m0.T @ V0_inv @ m0\n    ss_posterior_mean = mn.T @ Vn_inv @ mn\n    bn = b0 + 0.5 * (ss_data + ss_prior - ss_posterior_mean)\n\n    # Log determinant term\n    _, logdet_Vn = slogdet(Vn)\n    _, logdet_V0 = slogdet(V0)\n    log_det_term = 0.5 * (logdet_Vn - logdet_V0)\n\n    # Assemble the log marginal likelihood\n    log_ml = (gammaln(an) - gammaln(a0) +\n              a0 * np.log(b0) - an * np.log(bn) -\n              (N / 2) * np.log(2 * np.pi) +\n              log_det_term)\n\n    return log_ml\n\ndef solve():\n    \"\"\"\n    Solves the problem by computing the log Bayes Factor for each dataset and prior specification.\n    \"\"\"\n    # Define datasets\n    datasets = {\n        1: np.array([0.00, 0.10, 0.18, 0.24, 0.29, 0.31, 0.33, 0.35]),\n        2: np.array([0.00, 0.30, 0.20, 0.27, 0.18, 0.25, 0.17, 0.24]),\n        3: np.array([0.05, 0.02, 0.03, 0.01])\n    }\n\n    # Define prior specifications\n    priors = {\n        'A': {  # Prior Specification A\n            'K': {'m0': np.array([0.0, 0.0]), 'V0': np.diag([10000.0, 10000.0]), 'a0': 0.01, 'b0': 0.01},\n            'R': {'m0': np.array([0.0, 0.0]), 'V0': np.diag([10000.0, 10000.0]), 'a0': 0.01, 'b0': 0.01}\n        },\n        'B': {  # Prior Specification B\n            'K': {'m0': np.array([0.02, 0.80]), 'V0': np.diag([0.01, 0.01]), 'a0': 2.0, 'b0': 0.01},\n            'R': {'m0': np.array([0.60, 0.20]), 'V0': np.diag([0.01, 0.01]), 'a0': 2.0, 'b0': 0.01}\n        }\n    }\n    \n    test_cases = [\n        (1, 'A'), (1, 'B'),\n        (2, 'A'), (2, 'B'),\n        (3, 'A'), (3, 'B')\n    ]\n\n    results = []\n    for d_idx, p_key in test_cases:\n        y_full = datasets[d_idx]\n        T = len(y_full)\n        N = T - 2\n\n        # Common dependent variable for the sample t=3,...,T\n        y_vec = y_full[2:]\n\n        # Construct Design Matrix for Model K (Keynesian)\n        # y_t = alpha + phi*y_{t-1}\n        X_K = np.vstack([np.ones(N), y_full[1:T-1]]).T\n        \n        # Construct Design Matrix for Model R (RBC)\n        # y_t = beta1*y_{t-1} + beta2*y_{t-2}\n        X_R = np.vstack([y_full[1:T-1], y_full[0:T-2]]).T\n        \n        # Get prior hyperparameters for the current case\n        prior_K = priors[p_key]['K']\n        prior_R = priors[p_key]['R']\n\n        # Calculate log marginal likelihood for Model K\n        log_ml_K = calculate_log_marginal_likelihood(y_vec, X_K, \n            prior_K['m0'], prior_K['V0'], prior_K['a0'], prior_K['b0'])\n        \n        # Calculate log marginal likelihood for Model R\n        log_ml_R = calculate_log_marginal_likelihood(y_vec, X_R, \n            prior_R['m0'], prior_R['V0'], prior_R['a0'], prior_R['b0'])\n\n        # Compute the log Bayes Factor in favor of K over R\n        log_bf_KR = log_ml_K - log_ml_R\n        results.append(log_bf_KR)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2375563"}, {"introduction": "We now progress to a sophisticated and practical application in modern macroeconomics and finance: modeling the long-run equilibrium relationships between non-stationary time series. This exercise guides you through the Bayesian estimation of a Cointegrated Vector Autoregression (CVAR) model, using a conjugate Matrix-Normal Inverse-Wishart prior [@problem_id:2375565]. By implementing this advanced model from first principles, you will gain hands-on experience with multivariate Bayesian regression and see how the core principles of conjugate updating scale to handle complex, dynamic systems such as exchange rates.", "problem": "You are asked to design and implement a complete, runnable program that performs Bayesian estimation of a Cointegrated Vector Autoregression (CVAR) model for two log exchange rates under a conjugate prior. The task must be solved from first principles, starting from core definitions of multivariate Gaussian likelihoods and conjugate priors, and without using black-box time-series routines. Every step must be explicitly coded using basic linear algebra.\n\nConsider a bivariate log exchange rate vector $y_t \\in \\mathbb{R}^2$ that follows a Vector Error Correction Model (VECM) with cointegration rank equal to $1$ and no lagged differences:\n$$\n\\Delta y_t = \\alpha \\, \\beta^{\\top} y_{t-1} + \\varepsilon_t, \\quad \\varepsilon_t \\sim \\mathcal{N}(\\mathbf{0}, \\Sigma),\n$$\nwhere $\\alpha \\in \\mathbb{R}^{2 \\times 1}$ is the adjustment (also called loading) vector, $\\beta \\in \\mathbb{R}^{2 \\times 1}$ is the cointegration vector, and $\\Sigma \\in \\mathbb{R}^{2 \\times 2}$ is the positive-definite innovation covariance matrix. The cointegration vector is fixed and known as $\\beta = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}$. The model can be organized as a multivariate linear regression by defining the regressor $x_t = \\beta^{\\top} y_{t-1} \\in \\mathbb{R}$, stacking $T$ observations into the $T \\times 1$ matrix $X$ with entries $x_t$, and stacking $\\Delta y_t$ as the $T \\times 2$ matrix $Y$. Then:\n$$\nY = X B + E,\\quad E \\sim \\mathcal{MN}_{T,2}(0, I_T, \\Sigma),\n$$\nwhere $B \\in \\mathbb{R}^{1 \\times 2}$ is the coefficient matrix, and $\\mathcal{MN}$ denotes the matrix-normal distribution. Note that $\\alpha$ is the transpose of $B$, i.e., $\\alpha = B^{\\top}$.\n\nAssume a conjugate Matrix-Normal Inverse-Wishart (MNIW) prior:\n$$\nB \\mid \\Sigma \\sim \\mathcal{MN}_{1,2}(B_0, V_0, \\Sigma), \\quad \\Sigma \\sim \\mathcal{IW}(S_0, \\nu_0),\n$$\nwith known hyperparameters $B_0 \\in \\mathbb{R}^{1 \\times 2}$, $V_0 \\in \\mathbb{R}^{1 \\times 1}$, $S_0 \\in \\mathbb{R}^{2 \\times 2}$, and $\\nu_0 \\in \\mathbb{R}$. You must derive the posterior distribution of $(B, \\Sigma)$ under this prior and Gaussian likelihood, and compute the posterior mean of $\\alpha = B^{\\top}$.\n\nUse the following hyperparameters for all cases: $B_0 = \\mathbf{0}_{1 \\times 2}$, $V_0 = c I_1$ with $c = 10^6$, $S_0 = s_0 I_2$ with $s_0 = 10^{-4}$, and $\\nu_0 = 4$.\n\nData generation must be performed by simulating from the VECM itself for each test case, using the exact data-generating process:\n$$\n\\begin{aligned}\ny_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\\\\n\\text{for } t = 1, \\ldots, T: \\\\\n\\quad x_t = \\beta^{\\top} y_{t-1}, \\\\\n\\quad \\Delta y_t = \\alpha^{\\text{true}} x_t + \\varepsilon_t,\\quad \\varepsilon_t \\overset{\\text{i.i.d.}}{\\sim} \\mathcal{N}(\\mathbf{0}, \\Sigma^{\\text{true}}), \\\\\n\\quad y_t = y_{t-1} + \\Delta y_t.\n\\end{aligned}\n$$\n\nYou must implement the Bayesian posterior derivation and compute the posterior mean of $\\alpha$ given the simulated $(X, Y)$, under the MNIW prior. Do not rely on any built-in routines for time-series estimation or Bayesian regression. All quantities must be expressed and computed in consistent mathematical terms and dimensions.\n\nTest Suite:\nImplement the following three test cases. For each case, simulate data exactly as specified (including seeds) and then compute the posterior mean of $\\alpha$.\n\n- Case $1$ (happy path; clear error-correction):\n  - $T = 200$,\n  - $\\alpha^{\\text{true}} = \\begin{bmatrix} -0.2 \\\\ 0.15 \\end{bmatrix}$,\n  - $\\Sigma^{\\text{true}} = \\begin{bmatrix} 0.0004  0.0 \\\\ 0.0  0.0009 \\end{bmatrix}$,\n  - random seed $= 123$.\n\n- Case $2$ (correlated shocks; weaker error-correction):\n  - $T = 80$,\n  - $\\alpha^{\\text{true}} = \\begin{bmatrix} -0.05 \\\\ 0.05 \\end{bmatrix}$,\n  - $\\Sigma^{\\text{true}} = \\begin{bmatrix} 0.0009  0.0002 \\\\ 0.0002  0.0009 \\end{bmatrix}$,\n  - random seed $= 456$.\n\n- Case $3$ (edge case; near no cointegration adjustment under the rank-$1$ model):\n  - $T = 50$,\n  - $\\alpha^{\\text{true}} = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix}$,\n  - $\\Sigma^{\\text{true}} = \\begin{bmatrix} 0.0016  0.0 \\\\ 0.0  0.0016 \\end{bmatrix}$,\n  - random seed $= 789$.\n\nYour program should output, in a single line, a flat list containing exactly six floating-point numbers corresponding to the posterior means of the two entries of $\\alpha$ for each case, in the order:\n$[\\widehat{\\alpha}_{1}^{(1)}, \\widehat{\\alpha}_{2}^{(1)}, \\widehat{\\alpha}_{1}^{(2)}, \\widehat{\\alpha}_{2}^{(2)}, \\widehat{\\alpha}_{1}^{(3)}, \\widehat{\\alpha}_{2}^{(3)}]$,\nwhere the superscript $(i)$ indexes the test case. Each floating-point number must be formatted to exactly $6$ decimal places. The final output must be a single line in the exact format:\n\"[a1_case1,a2_case1,a1_case2,a2_case2,a1_case3,a2_case3]\".\n\nConstraints and requirements recap:\n- You must start from core definitions of the multivariate Gaussian likelihood and the Matrix-Normal Inverse-Wishart (MNIW) prior.\n- You must derive the posterior distribution under conjugacy and implement the resulting posterior mean of $\\alpha$.\n- You must simulate the data exactly as specified for each test case.\n- You must not use any external data or non-standard libraries.\n- Your program must produce a single line with a comma-separated list enclosed in square brackets, with exactly $6$ floating-point numbers each to $6$ decimal places, in the specified order.", "solution": "**1. Model Specification**\nThe model for the bivariate log exchange rate vector $y_t \\in \\mathbb{R}^2$ is a Vector Error Correction Model (VECM) of cointegration rank $1$:\n$$\n\\Delta y_t = \\alpha \\beta^\\top y_{t-1} + \\varepsilon_t, \\quad \\varepsilon_t \\sim \\mathcal{N}(\\mathbf{0}, \\Sigma)\n$$\nHere, $\\alpha \\in \\mathbb{R}^{2 \\times 1}$ is the vector of adjustment coefficients, $\\beta = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix} \\in \\mathbb{R}^{2 \\times 1}$ is the known cointegrating vector, and $\\Sigma \\in \\mathbb{R}^{2 \\times 2}$ is the positive-definite covariance matrix of the innovations.\n\nFor estimation purposes, this model is specified as a multivariate linear regression. Let the scalar error correction term be $x_t = \\beta^\\top y_{t-1}$. For a sample of $T$ observations, we define the following matrices:\n- The matrix of dependent variables: $Y = \\begin{bmatrix} (\\Delta y_1)^\\top \\\\ \\vdots \\\\ (\\Delta y_T)^\\top \\end{bmatrix} \\in \\mathbb{R}^{T \\times 2}$\n- The matrix of regressors: $X = \\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_T \\end{bmatrix} \\in \\mathbb{R}^{T \\times 1}$\nThe model can then be expressed as:\n$$\nY = X B + E\n$$\nwhere $B = \\alpha^\\top \\in \\mathbb{R}^{1 \\times 2}$ is the matrix of regression coefficients. The error matrix $E$ has rows that are independent draws from $\\mathcal{N}(\\mathbf{0}, \\Sigma)$, which corresponds to the matrix-normal distribution $E \\sim \\mathcal{MN}_{T,2}(\\mathbf{0}, I_T, \\Sigma)$.\n\n**2. Bayesian Framework: Likelihood and Prior**\nThe likelihood of the data $(Y, X)$ given the parameters $(B, \\Sigma)$ is derived from the distribution of the error term $E$:\n$$\np(Y \\mid X, B, \\Sigma) \\propto |\\Sigma|^{-T/2} \\exp\\left(-\\frac{1}{2} \\mathrm{tr}\\left( (Y - X B)^\\top (Y - X B) \\Sigma^{-1} \\right)\\right)\n$$\nA conjugate Matrix-Normal Inverse-Wishart (MNIW) prior is specified for the parameters $(B, \\Sigma)$:\n$$\np(B, \\Sigma) = p(B \\mid \\Sigma) p(\\Sigma)\n$$\nThe conditional prior for $B$ is matrix-normal, and the marginal prior for $\\Sigma$ is inverse-Wishart:\n$$\nB \\mid \\Sigma \\sim \\mathcal{MN}_{1,2}(B_0, V_0, \\Sigma)\n$$\n$$\n\\Sigma \\sim \\mathcal{IW}(S_0, \\nu_0)\n$$\nThe prior is defined by the hyperparameters $B_0 \\in \\mathbb{R}^{1 \\times 2}$, $V_0 \\in \\mathbb{R}^{1 \\times 1}$ (a positive definite matrix, here a scalar), $S_0 \\in \\mathbb{R}^{2 \\times 2}$ (a positive definite matrix), and $\\nu_0 \\in \\mathbb{R}$ (degrees of freedom, $\\nu_0  2-1=1$).\n\n**3. Posterior Distribution Derivation**\nAccording to Bayes' theorem, the posterior distribution is proportional to the likelihood multiplied by the prior:\n$$\np(B, \\Sigma \\mid Y, X) \\propto p(Y \\mid X, B, \\Sigma) \\, p(B \\mid \\Sigma) \\, p(\\Sigma)\n$$\nGiven the conjugate nature of the prior, the posterior distribution retains the MNIW form:\n$$\n(B, \\Sigma) \\mid Y, X \\sim \\text{MNIW}(B_N, V_N, S_N, \\nu_N)\n$$\nThe posterior hyperparameters $(B_N, V_N, S_N, \\nu_N)$ are updates of the prior hyperparameters based on the data. We derive these by analyzing the kernel of the posterior density. The terms in the exponent of the log-posterior involving $B$ are (ignoring constants):\n$$\n-\\frac{1}{2} \\mathrm{tr}\\left( \\left[ (Y - X B)^\\top (Y - X B) + (B - B_0)^\\top V_0^{-1} (B - B_0) \\right] \\Sigma^{-1} \\right)\n$$\nBy expanding the quadratic forms and completing the square with respect to $B$, we can identify the parameters of the conditional posterior $p(B \\mid \\Sigma, Y, X)$. The relevant terms are:\n$$\nB^\\top (X^\\top X) B - B^\\top(X^\\top Y) - (Y^\\top X)B + B^\\top V_0^{-1} B - B^\\top V_0^{-1} B_0 - B_0^\\top V_0^{-1} B\n$$\n$$\n= B^\\top(V_0^{-1} + X^\\top X)B - B^\\top(V_0^{-1}B_0 + X^\\top Y) - (B_0^\\top V_0^{-1} + Y^\\top X)B\n$$\nThis structure reveals the posterior precision and mean of $B$. The posterior precision matrix (for the rows of $B$) is $V_N^{-1} = V_0^{-1} + X^\\top X$. The posterior mean $B_N$ satisfies $V_N^{-1}B_N = V_0^{-1}B_0 + X^\\top Y$.\nThe final update rules for the parameters governing the posterior of $B$ are:\n$$\nV_N = (V_0^{-1} + X^\\top X)^{-1}\n$$\n$$\nB_N = V_N (V_0^{-1} B_0 + X^\\top Y)\n$$\nThe posterior degrees of freedom for the inverse-Wishart distribution of $\\Sigma$ is updated simply as:\n$$\n\\nu_N = \\nu_0 + T\n$$\nThe posterior scale matrix $S_N$ is not needed for this problem, as we only require the posterior mean of $\\alpha$. The posterior mean of $B$ is $E[B \\mid Y, X] = B_N$. Since $\\alpha = B^\\top$, by linearity of expectation, the posterior mean of $\\alpha$ is:\n$$\n\\widehat{\\alpha} = E[\\alpha \\mid Y, X] = (E[B \\mid Y, X])^\\top = B_N^\\top\n$$\n\n**4. Computational Algorithm**\nThe implementation involves two main parts for each test case.\n\nFirst, **Data Simulation**:\n$1$. Initialize a random number generator with the specified seed.\n$2$. Set the initial vector $y_0 = \\mathbf{0} \\in \\mathbb{R}^{2 \\times 1}$.\n$3$. Loop from $t = 1$ to $T$:\n    a. Compute the error correction term $x_t = \\beta^\\top y_{t-1}$.\n    b. Generate an innovation vector $\\varepsilon_t \\sim \\mathcal{N}(\\mathbf{0}, \\Sigma^{\\text{true}})$.\n    c. Compute the change vector $\\Delta y_t = \\alpha^{\\text{true}} x_t + \\varepsilon_t$.\n    d. Update the state vector: $y_t = y_{t-1} + \\Delta y_t$.\n    e. Store $x_t$ and $(\\Delta y_t)^\\top$.\n$4$. Assemble the stored values into the matrices $X \\in \\mathbb{R}^{T \\times 1}$ and $Y \\in \\mathbb{R}^{T \\times 2}$.\n\nSecond, **Bayesian Estimation**:\n$1$. Specify the prior hyperparameters: $B_0 = \\mathbf{0}_{1 \\times 2}$, $V_0 = c I_1$ with $c = 10^6$, $\\nu_0 = 4$, and $S_0 = s_0 I_2$ with $s_0 = 10^{-4}$.\n$2$. Use the simulated data $(X, Y)$ to compute the summary statistics $X^\\top X$ and $X^\\top Y$.\n$3$. Compute the inverse of the prior precision matrix, $V_0^{-1}$.\n$4$. Calculate the posterior precision parameter $V_N = (V_0^{-1} + X^\\top X)^{-1}$.\n$5$. Calculate the posterior mean parameter $B_N = V_N (V_0^{-1} B_0 + X^\\top Y)$.\n$6$. The final estimate is the posterior mean of $\\alpha$, given by $\\widehat{\\alpha} = B_N^\\top$. The two elements of this vector are the required outputs for the given case.\n\nThis procedure is executed for each of the three test cases, and the results are concatenated.", "answer": "```python\nimport numpy as np\n\ndef generate_data(T, alpha_true, Sigma_true, seed):\n    \"\"\"\n    Simulates data from the specified VECM.\n\n    Args:\n        T (int): Number of time periods to simulate.\n        alpha_true (np.ndarray): True alpha vector (2x1).\n        Sigma_true (np.ndarray): True innovation covariance matrix (2x2).\n        seed (int): Random seed for reproducibility.\n\n    Returns:\n        tuple[np.ndarray, np.ndarray]: A tuple containing the regressor matrix X (Tx1)\n                                       and the dependent variable matrix Y (Tx2).\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    beta = np.array([[1.0], [-1.0]])\n    y_prev = np.zeros((2, 1))\n\n    X_list = []\n    Y_list = []\n\n    for _ in range(T):\n        # Calculate the error correction term\n        x_t = (beta.T @ y_prev).item()  # The result is a scalar\n        \n        # Draw innovation from a multivariate normal distribution\n        epsilon_t = rng.multivariate_normal(np.zeros(2), Sigma_true).reshape(2, 1)\n\n        # Compute delta_y_t using the VECM equation\n        delta_y_t = alpha_true * x_t + epsilon_t\n        \n        # Update the y process\n        y_t = y_prev + delta_y_t\n        \n        # Store data for the regression Y = XB + E\n        X_list.append(x_t)\n        Y_list.append(delta_y_t.flatten())  # Store delta_y_t as a row vector\n        \n        y_prev = y_t\n\n    X = np.array(X_list).reshape(-1, 1)\n    Y = np.array(Y_list)\n    return X, Y\n\ndef estimate_posterior_alpha_mean(X, Y, B0, V0):\n    \"\"\"\n    Computes the posterior mean of alpha under a conjugate MNIW prior.\n\n    Args:\n        X (np.ndarray): Regressor matrix (Tx1).\n        Y (np.ndarray): Dependent variable matrix (Tx2).\n        B0 (np.ndarray): Prior mean for B (1x2).\n        V0 (np.ndarray): Prior covariance for B (rows) (1x1).\n\n    Returns:\n        np.ndarray: The posterior mean of alpha, a flattened array of 2 elements.\n    \"\"\"\n    # Calculate sufficient statistics from data\n    XTX = X.T @ X  # Shape: (1x1)\n    XTY = X.T @ Y  # Shape: (1x2)\n\n    # Calculate inverse of prior variance\n    V0_inv = np.linalg.inv(V0)\n    \n    # Calculate posterior parameters for B using standard update rules\n    VN_inv = V0_inv + XTX\n    VN = np.linalg.inv(VN_inv)\n    \n    # The term V0_inv @ B0 is zero since B0 is zero, but included for completeness\n    BN = VN @ (V0_inv @ B0 + XTY)  # Shape: (1x2)\n    \n    # The posterior mean of alpha is the transpose of BN\n    alpha_hat = BN.T  # Shape: (2x1)\n    \n    return alpha_hat.flatten()\n\ndef solve():\n    \"\"\"\n    Main solver function to run all test cases and print the final result.\n    \"\"\"\n    # Definition of test cases as provided in the problem statement\n    test_cases = [\n        {\n            \"T\": 200,\n            \"alpha_true\": np.array([[-0.2], [0.15]]),\n            \"Sigma_true\": np.array([[0.0004, 0.0], [0.0, 0.0009]]),\n            \"seed\": 123\n        },\n        {\n            \"T\": 80,\n            \"alpha_true\": np.array([[-0.05], [0.05]]),\n            \"Sigma_true\": np.array([[0.0009, 0.0002], [0.0002, 0.0009]]),\n            \"seed\": 456\n        },\n        {\n            \"T\": 50,\n            \"alpha_true\": np.array([[0.0], [0.0]]),\n            \"Sigma_true\": np.array([[0.0016, 0.0], [0.0, 0.0016]]),\n            \"seed\": 789\n        }\n    ]\n\n    # Prior hyperparameters, S0 and nu0 are not needed for the mean of alpha\n    c = 1e6\n    B0 = np.zeros((1, 2))\n    V0 = c * np.identity(1)\n\n    all_results = []\n    for case in test_cases:\n        # 1. Simulate data according to the VECM process\n        X, Y = generate_data(case[\"T\"], case[\"alpha_true\"], case[\"Sigma_true\"], case[\"seed\"])\n        \n        # 2. Compute the posterior mean of alpha\n        alpha_hat = estimate_posterior_alpha_mean(X, Y, B0, V0)\n        \n        # 3. Collect results\n        all_results.extend(alpha_hat)\n\n    # Format the final output string exactly as required\n    formatted_results = [f\"{r:.6f}\" for r in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2375565"}]}