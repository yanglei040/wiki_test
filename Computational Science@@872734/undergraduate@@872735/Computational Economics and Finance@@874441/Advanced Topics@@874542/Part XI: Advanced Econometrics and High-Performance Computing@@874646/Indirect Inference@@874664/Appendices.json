{"hands_on_practices": [{"introduction": "We begin with an intuitive problem that builds a strong foundation for the logic of indirect inference. Imagine a 'plinko' or Galton board, where balls bounce off pegs to produce a final distribution. This exercise [@problem_id:2401820] challenges you to act as a detective, inferring the board's hidden physical properties—the probability $p$ of bouncing right and the step size $d$—just by observing the final positions of the balls. By using a simple Normal distribution as an auxiliary model to summarize the data, you will learn to implement the core mechanism of matching moments to connect an observable outcome to its unobservable generating process.", "problem": "Consider a simplified model of a Galton-style \"plinko\" board used to illustrate stochastic processes. A large number of identical balls are dropped, one at a time, from the top. Each ball encounters $T$ rows of pegs. At each peg, the ball deflects either to the right or to the left. Let $S_{it}\\in\\{-d, +d\\}$ denote the horizontal increment of ball $i$ at row $t$, where $d > 0$ is the fixed horizontal step size. Assume $\\mathbb{P}(S_{it}=+d)=p$ and $\\mathbb{P}(S_{it}=-d)=1-p$, with $0\\le p\\le 1$, independently across balls and rows. The final horizontal position of ball $i$ after $T$ rows is $X_i=\\sum_{t=1}^{T} S_{it}$.\n\nLet $\\theta=(p,d)$ be the unknown structural parameter vector. You observe a dataset of $N_{\\text{obs}}$ independent final positions $\\{X_i\\}_{i=1}^{N_{\\text{obs}}}$ generated by the true $\\theta_0=(p_0,d_0)$. You are to estimate $\\theta$ using Indirect Inference (II), defined as follows. Choose an auxiliary model: a Normal distribution $\\mathcal{N}(\\mu,\\sigma^2)$ whose auxiliary parameters are the sample mean and the sample variance computed from the final positions. Define the auxiliary statistic mapping $m(\\cdot)$ as $m(\\{x_i\\}) = \\big(\\bar{x}, s^2\\big)$, where $\\bar{x}$ is the sample mean and $s^2$ is the sample variance with divisor $N$ (population variance). For a candidate $\\theta$, let $m_{\\text{sim}}(\\theta)$ denote the auxiliary statistics computed from data generated under the structural model with parameter $\\theta$. The II estimator minimizes the quadratic distance\n$$\nQ(\\theta)=\\big(m_{\\text{obs}}-m_{\\text{sim}}(\\theta)\\big)^{\\top} W \\big(m_{\\text{obs}}-m_{\\text{sim}}(\\theta)\\big),\n$$\nwith weight matrix $W=I_2$, where $m_{\\text{obs}}=m(\\{X_i\\}_{i=1}^{N_{\\text{obs}}})$.\n\nYour task is to write a complete program that, for each test case specified below, does all of the following from first principles:\n- Generate the observed dataset $\\{X_i\\}_{i=1}^{N_{\\text{obs}}}$ using the structural model with the given true parameter $\\theta_0$ and the specified random seed.\n- Compute the observed auxiliary statistics $m_{\\text{obs}}$.\n- Compute the Indirect Inference estimate $\\hat{\\theta}=(\\hat{p},\\hat{d})$ as a minimizer of $Q(\\theta)$ over the admissible set $\\{(p,d): 0\\le p\\le 1, \\ d0\\}$.\n- Return $\\hat{\\theta}$ for each test case as specified in the final output format.\n\nAll probabilities must be represented as decimals in $[0,1]$. No physical units are involved. All angles, if any, are irrelevant to this problem. The estimation must be framed strictly in terms of the definitions given above.\n\nTest suite. For each item, you are given the number of rows $T$, the true probability $p_0$, the true step size $d_0$, the observed sample size $N_{\\text{obs}}$, and the random seed to generate the observed data. The random number generator must be initialized with the provided seed for observed data generation. The final sample variance must use the divisor $N_{\\text{obs}}$ (population variance).\n- Test case $1$: $T=20$, $p_0=0.6$, $d_0=1.2$, $N_{\\text{obs}}=50000$, seed $=13579$.\n- Test case $2$: $T=30$, $p_0=0.5$, $d_0=1.0$, $N_{\\text{obs}}=50000$, seed $=24680$.\n- Test case $3$: $T=25$, $p_0=0.9$, $d_0=0.8$, $N_{\\text{obs}}=50000$, seed $=11235$.\n\nFinal output format. Your program should produce a single line of output containing the results as a list of lists, one inner list per test case, in the same order as listed above. Each inner list must contain two decimal numbers $[\\hat{p},\\hat{d}]$ rounded to exactly six digits after the decimal point. For example, the output line must look like:\n[[p1,d1],[p2,d2],[p3,d3]]\nwith no spaces inserted, where each $p_j$ and $d_j$ is a decimal rounded to six places.", "solution": "The problem presented is a valid and well-posed exercise in parameter estimation using the method of Indirect Inference. It is scientifically grounded in probability theory and statistical estimation, and all necessary information for its resolution is provided. We shall proceed with a complete solution.\n\nThe core of the problem is to estimate the structural parameters $\\theta=(p,d)$ of a stochastic process, given a set of observations of the process's final state. The structural model describes a ball moving on a Galton board with $T$ rows. At each row $t \\in \\{1, \\dots, T\\}$, the ball's horizontal position changes by an increment $S_t$, which is $+d$ with probability $p$ and $-d$ with probability $1-p$. The final horizontal position after $T$ rows is $X = \\sum_{t=1}^{T} S_t$.\n\nFirst, we establish the analytical relationship between the structural parameters $\\theta=(p,d)$ and the moments of the final position $X$. Let $K$ be a random variable representing the number of deflections to the right, which follows a Binomial distribution, $K \\sim \\text{Binomial}(T,p)$. The total number of deflections is $T$, so there are $T-K$ deflections to the left. The final position $X$ can be expressed as:\n$$\nX = K \\cdot (+d) + (T-K) \\cdot (-d) = d(K - (T-K)) = d(2K - T)\n$$\nThe mean and variance of the Binomial variable $K$ are $\\mathbb{E}[K] = Tp$ and $\\text{Var}(K) = Tp(1-p)$. Using the properties of expectation and variance, we can find the mean $\\mu_X$ and variance $\\sigma_X^2$ of the final position $X$:\n$$\n\\mu(p, d) = \\mathbb{E}[X] = \\mathbb{E}[d(2K - T)] = d(2\\mathbb{E}[K] - T) = d(2Tp - T) = Td(2p-1)\n$$\n$$\n\\sigma^2(p, d) = \\text{Var}(X) = \\text{Var}(d(2K - T)) = d^2 \\text{Var}(2K) = 4d^2\\text{Var}(K) = 4d^2Tp(1-p)\n$$\nThese two equations form the binding function that maps the structural parameters $\\theta=(p,d)$ to the theoretical moments of the observable data.\n\nThe problem requires estimation by Indirect Inference (II). We are given an auxiliary model, which is a Normal distribution $\\mathcal{N}(\\mu, \\sigma^2)$, and the auxiliary statistics are the sample mean $\\bar{x}$ and sample variance $s^2$ (with divisor $N_{\\text{obs}}$). We have a set of $N_{\\text{obs}}$ observations $\\{X_i\\}_{i=1}^{N_{\\text{obs}}}$ generated from the true parameters $\\theta_0=(p_0,d_0)$. We calculate the observed auxiliary statistic vector $m_{\\text{obs}} = (\\bar{x}_{\\text{obs}}, s^2_{\\text{obs}})$.\n\nThe II estimator $\\hat{\\theta}$ is found by minimizing the objective function:\n$$\nQ(\\theta) = (m_{\\text{obs}} - m_{\\text{sim}}(\\theta))^{\\top} W (m_{\\text{obs}} - m_{\\text{sim}}(\\theta))\n$$\nwhere the weighting matrix is the identity matrix, $W=I_2$. The term $m_{\\text{sim}}(\\theta)$ represents the auxiliary statistics produced by the model with parameter $\\theta$. A crucial point is the evaluation of $m_{\\text{sim}}(\\theta)$. While one could perform a simulation for each value of $\\theta$ during optimization, this would introduce stochasticity into the objective function, complicating the minimization and requiring specification of an inner simulation size, which is not provided. The correct and standard approach in this context is to use the analytical moments derived above, which correspond to the expectation of the auxiliary statistics as the simulation size approaches infinity.\nThus, $m_{\\text{sim}}(\\theta) = (\\mu(p,d), \\sigma^2(p,d))$. The objective function becomes:\n$$\nQ(p,d) = (\\bar{x}_{\\text{obs}} - \\mu(p,d))^2 + (s^2_{\\text{obs}} - \\sigma^2(p,d))^2\n$$\n$$\nQ(p,d) = (\\bar{x}_{\\text{obs}} - Td(2p-1))^2 + (s^2_{\\text{obs}} - 4Td^2p(1-p))^2\n$$\nThe estimation task is to find the values $(\\hat{p}, \\hat{d})$ that minimize this function $Q(p,d)$ subject to the constraints $p \\in [0,1]$ and $d  0$. This is a standard non-linear constrained optimization problem.\n\nThe algorithmic procedure to solve the problem for each test case is as follows:\n1.  **Generate Observed Data**: For a given test case with parameters $T$, $p_0$, $d_0$, $N_{\\text{obs}}$, and a specific random seed, generate $N_{\\text{obs}}$ observations. This is achieved efficiently by first sampling $N_{\\text{obs}}$ values $\\{K_i\\}_{i=1}^{N_{\\text{obs}}}$ from the distribution $\\text{Binomial}(T, p_0)$. Then, the observed final positions are calculated as $X_i = d_0(2K_i - T)$.\n2.  **Compute Observed Statistics**: From the generated dataset $\\{X_i\\}$, calculate the observed auxiliary statistics $m_{\\text{obs}}=(\\bar{x}_{\\text{obs}}, s^2_{\\text{obs}})$. Specifically, $\\bar{x}_{\\text{obs}} = \\frac{1}{N_{\\text{obs}}} \\sum_{i=1}^{N_{\\text{obs}}} X_i$ and $s^2_{\\text{obs}} = \\frac{1}{N_{\\text{obs}}} \\sum_{i=1}^{N_{\\text{obs}}} (X_i - \\bar{x}_{\\text{obs}})^2$.\n3.  **Numerical Optimization**: Minimize the objective function $Q(p,d)$ with respect to $p$ and $d$. We employ a numerical optimization algorithm suitable for constrained problems, such as a quasi-Newton method with box constraints (e.g., L-BFGS-B). The search space is defined by the bounds $p \\in [0,1]$ and $d \\in (0, \\infty)$. A small positive lower bound is used for $d$ for numerical stability.\n4.  **Report Estimate**: The pair of values $(\\hat{p}, \\hat{d})$ that minimizes $Q(p,d)$ constitutes the Indirect Inference estimate for the given test case. The results from all test cases are then collected and formatted as required.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Main function to solve the Indirect Inference estimation problem for all test cases.\n    \"\"\"\n\n    # Test cases as specified in the problem statement.\n    test_cases = [\n        # (T, p0, d0, N_obs, seed)\n        (20, 0.6, 1.2, 50000, 13579),\n        (30, 0.5, 1.0, 50000, 24680),\n        (25, 0.9, 0.8, 50000, 11235),\n    ]\n\n    all_results = []\n\n    for T, p0, d0, N_obs, seed in test_cases:\n        # Step 1: Generate the observed dataset {X_i}\n        # Use a more efficient method based on the Binomial distribution.\n        # k ~ Binomial(T, p) represents the number of right steps.\n        # X = k * (+d) + (T - k) * (-d) = d * (2k - T)\n        rng = np.random.default_rng(seed)\n        k_obs = rng.binomial(T, p0, size=N_obs)\n        x_obs = d0 * (2 * k_obs - T)\n\n        # Step 2: Compute the observed auxiliary statistics m_obs = (mean, variance)\n        mu_obs = np.mean(x_obs)\n        # Use population variance (divisor N), as specified (ddof=0 is default for np.var)\n        var_obs = np.var(x_obs)\n        m_obs = (mu_obs, var_obs)\n        \n        # Step 3: Define the objective function Q(theta) for minimization.\n        # theta is a tuple (p, d).\n        def objective_function(theta, T_val, m_obs_val):\n            p, d = theta\n            mu_obs_val, var_obs_val = m_obs_val\n\n            # Analytical moments from the structural model\n            mu_sim = T_val * d * (2 * p - 1)\n            var_sim = 4 * T_val * (d**2) * p * (1 - p)\n            \n            # Quadratic objective function Q(theta)\n            q_val = (mu_obs_val - mu_sim)**2 + (var_obs_val - var_sim)**2\n            return q_val\n\n        # Step 4: Perform numerical optimization to find the II estimate.\n        # Initial guess for the parameters (p, d)\n        initial_guess = [0.5, 1.0]\n\n        # Bounds for the parameters: 0 = p = 1 and d  0.\n        # Use a small positive number for the lower bound of d for numerical stability.\n        bounds = [(0.0, 1.0), (1e-9, None)]\n\n        # Minimize the objective function. L-BFGS-B is suitable for box constraints.\n        result = minimize(\n            fun=objective_function,\n            x0=initial_guess,\n            args=(T, m_obs),\n            method='L-BFGS-B',\n            bounds=bounds\n        )\n\n        # The optimized parameters are the II estimates\n        p_hat, d_hat = result.x\n        all_results.append([p_hat, d_hat])\n\n    # Final print statement in the exact required format.\n    # Create the list of lists with rounded values.\n    # e.g., [[0.600012, 1.199998], [0.500001, 1.000003], [0.900005, 0.799989]]\n    # Then convert to string and remove spaces to match the output format.\n    final_list_formatted = [[round(p, 6), round(d, 6)] for p, d in all_results]\n    output_str = str(final_list_formatted).replace(\" \", \"\")\n    print(output_str)\n\nsolve()\n\n```", "id": "2401820"}, {"introduction": "Moving from a physical analogy to a more standard econometric context, this practice explores a common scenario where the auxiliary model's parameters are estimated using the Generalized Method of Moments (GMM). You will estimate the persistence parameter $\\theta$ of a structural AR(1) model. This exercise [@problem_id:2401827] introduces the practical challenge of working with an overidentified GMM auxiliary model and demonstrates the use of common random numbers, a crucial variance-reduction technique that enhances the stability of simulation-based estimators. This problem bridges the gap between textbook theory and the robust methods used in empirical research.", "problem": "You are given a structural time series model and an auxiliary model with parameters estimated by Generalized Method of Moments (GMM). Your task is to implement an indirect inference estimator where the auxiliary parameters are obtained by GMM rather than Maximum Likelihood Estimation (MLE), and to produce indirect inference estimates for a specified test suite of data-generating processes and simulation settings.\n\nThe structural model is the autoregressive model of order one with known innovation variance equal to one. For each time index $t$, the process satisfies\n$$\ny_t = \\theta \\, y_{t-1} + \\varepsilon_t,\n$$\nwhere $\\varepsilon_t \\sim \\mathcal{N}(0,1)$ are independent and identically distributed. Use the initial condition $y_0 = 0$ and a burn-in of $B = 200$ periods before collecting the $T$ observations. For simulation of observed and auxiliary-sample paths, the angle unit is not applicable and no physical units are involved.\n\nThe auxiliary model is the linear regression\n$$\ny_t = \\beta_0 + \\beta_1 \\, y_{t-1} + u_t,\n$$\nand the auxiliary parameters $\\beta = (\\beta_0,\\beta_1)^\\top$ are defined via overidentified moment conditions based on the instrument vector\n$$\nz_t = \\begin{bmatrix} 1 \\\\ y_{t-1} \\\\ y_{t-2} \\end{bmatrix}.\n$$\nFor each sample $y_1,\\dots,y_T$, define the sample moment function for $t=3,\\dots,T$ as\n$$\ng_T(\\beta) = \\frac{1}{n} \\sum_{t=3}^T z_t \\, \\left(y_t - \\beta_0 - \\beta_1 \\, y_{t-1}\\right),\n$$\nwhere $n = T-2$. The GMM estimator of $\\beta$ is defined using the identity weighting matrix of dimension $3 \\times 3$ as\n$$\n\\widehat{\\beta}(y_{1:T}) = \\arg \\min_{\\beta \\in \\mathbb{R}^2} \\; g_T(\\beta)^\\top g_T(\\beta).\n$$\n\nThe indirect inference objective for a candidate structural parameter $\\theta$ uses $S$ independent simulated samples of length $T$ generated from the structural model with the same burn-in $B$. Let $\\widehat{\\beta}^{\\text{obs}}$ denote the auxiliary GMM estimate obtained from the observed dataset. For a given $\\theta$, define\n$$\n\\overline{\\beta}(\\theta) = \\frac{1}{S} \\sum_{s=1}^S \\widehat{\\beta}(y^{(s)}_{1:T}(\\theta)),\n$$\nwhere $y^{(s)}_{1:T}(\\theta)$ is the $s$-th simulated sample of length $T$ generated under $\\theta$. The indirect inference criterion is\n$$\nQ(\\theta) = \\left(\\widehat{\\beta}^{\\text{obs}} - \\overline{\\beta}(\\theta)\\right)^\\top W \\left(\\widehat{\\beta}^{\\text{obs}} - \\overline{\\beta}(\\theta)\\right),\n$$\nwith $W = I_2$. For each test case below, restrict $\\theta$ to the discrete search grid\n$$\n\\mathcal{G}_i = \\{-0.95, -0.95 + \\delta_i, \\dots, 0.95\\},\n$$\nwhere $\\delta_i$ is the grid step specified in the test case. The indirect inference estimate is\n$$\n\\widehat{\\theta}_i \\in \\arg \\min_{\\theta \\in \\mathcal{G}_i} Q(\\theta),\n$$\nwith ties broken by selecting the smallest $\\theta$.\n\nSimulation and reproducibility requirements are as follows:\n- For each test case $i$, generate the observed dataset using the true parameter $\\theta^{\\star}_i$, sample size $T_i$, innovation variance equal to $1$, burn-in $B=200$, and the specified observed-data seed $s^{\\text{obs}}_i$.\n- For evaluating $Q(\\theta)$ on the grid for test case $i$, use common random numbers across different $\\theta$ values by pre-generating, for each replication index $s \\in \\{1,\\dots,S_i\\}$, a sequence of $T_i + B$ standard normal innovations using seed $s^{\\text{sim}}_i + s$. Reuse the same pre-generated innovations for all $\\theta \\in \\mathcal{G}_i$.\n\nImplement the above and compute $\\widehat{\\theta}_i$ for the following four test cases:\n- Test case $1$: $T_1 = 300$, $\\theta^{\\star}_1 = 0.6$, $S_1 = 20$, $\\delta_1 = 0.01$, $s^{\\text{obs}}_1 = 1729$, $s^{\\text{sim}}_1 = 20231$.\n- Test case $2$: $T_2 = 300$, $\\theta^{\\star}_2 = 0.9$, $S_2 = 25$, $\\delta_2 = 0.01$, $s^{\\text{obs}}_2 = 1730$, $s^{\\text{sim}}_2 = 20232$.\n- Test case $3$: $T_3 = 150$, $\\theta^{\\star}_3 = 0.0$, $S_3 = 40$, $\\delta_3 = 0.02$, $s^{\\text{obs}}_3 = 1731$, $s^{\\text{sim}}_3 = 20233$.\n- Test case $4$: $T_4 = 300$, $\\theta^{\\star}_4 = -0.5$, $S_4 = 20$, $\\delta_4 = 0.01$, $s^{\\text{obs}}_4 = 1732$, $s^{\\text{sim}}_4 = 20234$.\n\nYour program should produce a single line of output containing the four estimated values $\\widehat{\\theta}_1, \\widehat{\\theta}_2, \\widehat{\\theta}_3, \\widehat{\\theta}_4$ as a comma-separated list enclosed in square brackets, with each value rounded to four decimal places, for example, $[\\widehat{\\theta}_1,\\widehat{\\theta}_2,\\widehat{\\theta}_3,\\widehat{\\theta}_4]$.", "solution": "The problem requires the implementation of an indirect inference estimator for the parameter $\\theta$ of a first-order autoregressive process, AR(1). The estimation procedure is based on an auxiliary model whose parameters are estimated via the Generalized Method of Moments (GMM). The validity of the problem statement is first assessed.\n\n### Step 1: Extract Givens\n\n- **Structural Model**: $y_t = \\theta \\, y_{t-1} + \\varepsilon_t$, where $\\varepsilon_t \\sim \\mathcal{N}(0,1)$ are i.i.d. innovations.\n- **Initial Condition**: $y_0 = 0$.\n- **Simulation Parameters**: Burn-in period $B = 200$. Sample size of $T$ observations collected after burn-in.\n- **Auxiliary Model**: $y_t = \\beta_0 + \\beta_1 \\, y_{t-1} + u_t$, with parameters $\\beta = (\\beta_0,\\beta_1)^\\top$.\n- **Instrument Vector**: $z_t = [1, y_{t-1}, y_{t-2}]^\\top$.\n- **GMM Sample Moment Function**: $g_T(\\beta) = \\frac{1}{n} \\sum_{t=3}^T z_t \\, (y_t - \\beta_0 - \\beta_1 y_{t-1})$, with $n = T-2$.\n- **GMM Estimator**: $\\widehat{\\beta}(y_{1:T}) = \\arg \\min_{\\beta \\in \\mathbb{R}^2} \\; g_T(\\beta)^\\top g_T(\\beta)$. This corresponds to using an identity weighting matrix.\n- **Indirect Inference Criterion**: $Q(\\theta) = (\\widehat{\\beta}^{\\text{obs}} - \\overline{\\beta}(\\theta))^\\top W (\\widehat{\\beta}^{\\text{obs}} - \\overline{\\beta}(\\theta))$, with weighting matrix $W = I_2$.\n- **Simulated Auxiliary Parameters**: $\\overline{\\beta}(\\theta) = \\frac{1}{S} \\sum_{s=1}^S \\widehat{\\beta}(y^{(s)}_{1:T}(\\theta))$, where $y^{(s)}_{1:T}(\\theta)$ are $S$ independent samples generated from the structural model with parameter $\\theta$.\n- **Estimation Grid**: $\\mathcal{G}_i = \\{-0.95, -0.95 + \\delta_i, \\dots, 0.95\\}$.\n- **Indirect Inference Estimator**: $\\widehat{\\theta}_i \\in \\arg \\min_{\\theta \\in \\mathcal{G}_i} Q(\\theta)$, with ties broken by selecting the smallest $\\theta$.\n- **Reproducibility**: Common random numbers are to be used for evaluating $Q(\\theta)$ at different $\\theta$ values. For test case $i$, the observed data is generated with seed $s^{\\text{obs}}_i$. The $S_i$ simulation paths are generated with seeds $s^{\\text{sim}}_i + 1, \\dots, s^{\\text{sim}}_i + S_i$.\n- **Test Cases**:\n    1.  $T_1 = 300$, $\\theta^{\\star}_1 = 0.6$, $S_1 = 20$, $\\delta_1 = 0.01$, $s^{\\text{obs}}_1 = 1729$, $s^{\\text{sim}}_1 = 20231$.\n    2.  $T_2 = 300$, $\\theta^{\\star}_2 = 0.9$, $S_2 = 25$, $\\delta_2 = 0.01$, $s^{\\text{obs}}_2 = 1730$, $s^{\\text{sim}}_2 = 20232$.\n    3.  $T_3 = 150$, $\\theta^{\\star}_3 = 0.0$, $S_3 = 40$, $\\delta_3 = 0.02$, $s^{\\text{obs}}_3 = 1731$, $s^{\\text{sim}}_3 = 20233$.\n    4.  $T_4 = 300$, $\\theta^{\\star}_4 = -0.5$, $S_4 = 20$, $\\delta_4 = 0.01$, $s^{\\text{obs}}_4 = 1732$, $s^{\\text{sim}}_4 = 20234$.\n\n### Step 2: Validate Using Extracted Givens\n\n- **Scientific Groundedness**: The problem is well-grounded in the principles of econometrics and statistics. The AR(1) model, GMM estimation, and indirect inference are all standard and widely used methodologies.\n- **Well-Posedness**: The problem is well-posed. The objective function $Q(\\theta)$ is clearly defined, and its minimization over a discrete grid guarantees the existence of a solution. The specified tie-breaking rule ensures the solution is unique.\n- **Objectivity**: The problem is stated in precise, objective mathematical language, free from any subjective or ambiguous terms.\n- **Completeness and Consistency**: All required models, parameters, seeds, and procedural steps are explicitly provided. The setup is internally consistent and contains no contradictions. The use of an identity weighting matrix for GMM is a valid, though potentially inefficient, choice that leads to a well-defined estimator.\n\n### Step 3: Verdict and Action\n\nThe problem statement is valid, being scientifically sound, well-posed, and self-contained. A reasoned solution will be provided.\n\n### Principle-Based Design of the Solution\n\nThe solution is constructed by implementing the specified indirect inference procedure. This involves several distinct, logical steps: data simulation, auxiliary parameter estimation, evaluation of the indirect inference criterion, and a grid search for the structural parameter.\n\n1.  **Analytical GMM Estimator for Auxiliary Parameters**\n    The auxiliary parameters $\\beta = (\\beta_0, \\beta_1)^\\top$ are estimated by minimizing $J(\\beta) = g_T(\\beta)^\\top g_T(\\beta)$. Let the sample be denoted $y_1, \\dots, y_T$. The moment conditions are evaluated for $t=3, \\dots, T$, comprising $n=T-2$ observations. We can express the problem in matrix form. Let\n    $$\n    y_{\\text{vec}} = \\begin{bmatrix} y_3 \\\\ y_4 \\\\ \\vdots \\\\ y_T \\end{bmatrix}, \\quad\n    X = \\begin{bmatrix} 1  y_2 \\\\ 1  y_3 \\\\ \\vdots  \\vdots \\\\ 1  y_{T-1} \\end{bmatrix}, \\quad\n    Z = \\begin{bmatrix} 1  y_2  y_1 \\\\ 1  y_3  y_2 \\\\ \\vdots  \\vdots  \\vdots \\\\ 1  y_{T-1}  y_{T-2} \\end{bmatrix}.\n    $$\n    The GMM objective function, ignoring the constant factor $1/n^2$, is to minimize $(Z'y_{\\text{vec}} - Z'X\\beta)^\\top (Z'y_{\\text{vec}} - Z'X\\beta)$. This is a linear least squares problem of the form $\\min_{\\beta} \\|b - A\\beta \\|_2^2$, where $A = Z'X$ and $b = Z'y_{\\text{vec}}$. The solution is given by the normal equations: $\\beta = (A^\\top A)^{-1} A^\\top b$. Substituting back, the GMM estimator is\n    $$\n    \\widehat{\\beta} = \\left( (X'Z)(Z'X) \\right)^{-1} (X'Z)(Z'y_{\\text{vec}}).\n    $$\n    This analytical formula is computationally efficient and numerically stable for non-collinear data, and it will be used for estimating $\\widehat{\\beta}$.\n\n2.  **Data Simulation from the Structural Model**\n    A function is required to generate time series data from the structural model $y_t = \\theta y_{t-1} + \\varepsilon_t$. The procedure for generating a single sample of length $T$ is as follows:\n    - Initialize a path of length $B+T+1$ with $y_0 = 0$.\n    - Use a pre-generated sequence of $B+T$ i.i.d. standard normal innovations $\\varepsilon_1, \\dots, \\varepsilon_{B+T}$.\n    - Iterate from $t=1$ to $B+T$: $y_t = \\theta y_{t-1} + \\varepsilon_t$.\n    - The final sample consists of the last $T$ observations, $\\{y_{B+1}, \\dots, y_{B+T}\\}$, effectively discarding the initial $B$ burn-in periods and $y_0$.\n\n3.  **Indirect Inference Procedure**\n    The core of the problem is a four-step process for each test case:\n    - **Step A: \"Observed\" Data and Parameters**. For a given test case $i$ with true parameter $\\theta^\\star_i$, sample size $T_i$, and seed $s^{\\text{obs}}_i$, a single \"observed\" time series $y^{\\text{obs}}$ is generated. The GMM estimator is then applied to this series to obtain the observed auxiliary parameter vector $\\widehat{\\beta}^{\\text{obs}}$.\n    - **Step B: Generation of Common Random Numbers**. For the simulation part, $S_i$ sets of i.i.d. standard normal innovations, each of length $B+T_i$, are pre-generated using seeds $s^{\\text{sim}}_i + 1, \\dots, s^{\\text{sim}}_i + S_i$. These sets of innovations are held fixed (common) for the evaluation of $Q(\\theta)$ across all candidate values of $\\theta$ in the grid $\\mathcal{G}_i$. This is a variance reduction technique that improves the stability of the estimator.\n    - **Step C: Evaluation of the Criterion Function $Q(\\theta)$**. For each candidate $\\theta$ on the grid $\\mathcal{G}_i$, the following is performed:\n        - $S_i$ time series are simulated using the structural model with parameter $\\theta$ and the $S_i$ pre-generated sets of innovations.\n        - For each of the $S_i$ simulated series, the auxiliary parameter vector $\\widehat{\\beta}^{(s)}(\\theta)$ is estimated using the analytical GMM formula.\n        - The average auxiliary parameter vector $\\overline{\\beta}(\\theta) = \\frac{1}{S_i} \\sum_{s=1}^{S_i} \\widehat{\\beta}^{(s)}(\\theta)$ is computed.\n        - The indirect inference criterion is calculated as the squared Euclidean distance: $Q(\\theta) = \\|\\widehat{\\beta}^{\\text{obs}} - \\overline{\\beta}(\\theta)\\|_2^2$.\n    - **Step D: Grid Search**. The value of $Q(\\theta)$ is computed for every $\\theta \\in \\mathcal{G}_i$. The estimate $\\widehat{\\theta}_i$ is the value of $\\theta$ that minimizes $Q(\\theta)$. The specified tie-breaking rule (select the smallest $\\theta$) is naturally handled by iterating through the grid in ascending order and only updating the minimum if a strictly smaller value of $Q(\\theta)$ is found.\n\n4.  **Implementation**\n    An implementation will be developed in Python using the `numpy` library for numerical operations. A main `solve` function will orchestrate the process. Helper functions will be created for:\n    - Simulating an AR(1) process (`generate_ar1`).\n    - Estimating the GMM parameters (`gmm_estimator`).\n    - Running the entire procedure for a single test case (`run_test_case`).\n    The main function will iterate through the four specified test cases, call the case runner, collect the estimated $\\widehat{\\theta}_i$, and format the final output as specified.", "answer": "```python\nimport numpy as np\nfrom scipy import stats\n\ndef generate_ar1(theta, T, B, innovations):\n    \"\"\"\n    Generates a time series from an AR(1) model.\n    \n    Args:\n        theta (float): The AR(1) parameter.\n        T (int): The number of observations in the final sample.\n        B (int): The number of burn-in periods.\n        innovations (np.ndarray): A vector of T+B standard normal innovations.\n\n    Returns:\n        np.ndarray: The simulated time series of length T.\n    \"\"\"\n    path_len = T + B\n    path = np.zeros(path_len + 1)  # path[0] is y_0\n    \n    for t in range(path_len):\n        path[t + 1] = theta * path[t] + innovations[t]\n        \n    return path[B + 1:] # Returns y_{B+1}, ..., y_{B+T} (length T)\n\ndef gmm_estimator(y):\n    \"\"\"\n    Computes the GMM estimator for the auxiliary model.\n    y_t = beta_0 + beta_1 * y_{t-1} + u_t\n    z_t = [1, y_{t-1}, y_{t-2}]\n    \n    Args:\n        y (np.ndarray): The time series data (y_1, ..., y_T), length T.\n        \n    Returns:\n        np.ndarray: The GMM estimate [beta_0_hat, beta_1_hat].\n    \"\"\"\n    T = len(y)\n    n = T - 2 # Number of observations for moment conditions (t=3 to T)\n    \n    if n = 0:\n        raise ValueError(\"Time series is too short for GMM estimation.\")\n\n    # y_vec corresponds to y_t for t in {3..T}\n    y_vec = y[2:T] # shape (n,)\n    \n    # X corresponds to [1, y_{t-1}] for t in {3..T}\n    X = np.zeros((n, 2))\n    X[:, 0] = 1.0\n    X[:, 1] = y[1:T-1] # y_2, ..., y_{T-1}\n    \n    # Z corresponds to [1, y_{t-1}, y_{t-2}] for t in {3..T}\n    Z = np.zeros((n, 3))\n    Z[:, 0] = 1.0\n    Z[:, 1] = y[1:T-1] # y_2, ..., y_{T-1}\n    Z[:, 2] = y[0:T-2] # y_1, ..., y_{T-2}\n    \n    # Analytical solution for GMM with W = I:\n    # beta_hat = ( (X'Z Z'X)^-1 ) * ( X'Z Z'y )\n    XT_Z = X.T @ Z\n    ZT_X = Z.T @ X\n    ZT_y = Z.T @ y_vec\n    \n    # Matrix to be inverted\n    M = XT_Z @ ZT_X\n    \n    try:\n        M_inv = np.linalg.inv(M)\n    except np.linalg.LinAlgError:\n        # Fallback to pseudo-inverse if matrix is singular\n        M_inv = np.linalg.pinv(M)\n        \n    beta_hat = M_inv @ (XT_Z @ ZT_y)\n    \n    return beta_hat\n\ndef run_test_case(params):\n    \"\"\"\n    Performs the full indirect inference estimation for one test case.\n    \n    Args:\n        params (dict): A dictionary with all parameters for the test case.\n        \n    Returns:\n        float: The indirect inference estimate of theta.\n    \"\"\"\n    T, theta_star, S, delta, s_obs, s_sim = params.values()\n    B = 200\n\n    # 1. Generate \"observed\" data and estimate beta_obs\n    rng_obs = np.random.default_rng(s_obs)\n    innovs_obs = rng_obs.standard_normal(T + B)\n    y_obs = generate_ar1(theta_star, T, B, innovs_obs)\n    beta_obs = gmm_estimator(y_obs)\n\n    # 2. Pre-generate common random numbers for simulations\n    sim_innovations = []\n    for s in range(1, S + 1):\n        rng_sim = np.random.default_rng(s_sim + s)\n        sim_innovations.append(rng_sim.standard_normal(T + B))\n\n    # 3. Grid search for theta\n    grid_start = -0.95\n    grid_end = 0.95\n    num_points = int(round((grid_end - grid_start) / delta)) + 1\n    theta_grid = np.linspace(grid_start, grid_end, num_points)\n\n    min_Q = np.inf\n    best_theta = None\n\n    for theta_candidate in theta_grid:\n        beta_sim_list = []\n        for s in range(S):\n            y_sim = generate_ar1(theta_candidate, T, B, sim_innovations[s])\n            beta_sim = gmm_estimator(y_sim)\n            beta_sim_list.append(beta_sim)\n        \n        beta_bar = np.mean(beta_sim_list, axis=0)\n        \n        Q = np.sum((beta_obs - beta_bar)**2)\n        \n        # Tie-breaking: select the smallest theta, so only update on strictly smaller Q\n        if Q  min_Q:\n            min_Q = Q\n            best_theta = theta_candidate\n            \n    return best_theta\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        {'T': 300, 'theta_star': 0.6, 'S': 20, 'delta': 0.01, 's_obs': 1729, 's_sim': 20231},\n        {'T': 300, 'theta_star': 0.9, 'S': 25, 'delta': 0.01, 's_obs': 1730, 's_sim': 20232},\n        {'T': 150, 'theta_star': 0.0, 'S': 40, 'delta': 0.02, 's_obs': 1731, 's_sim': 20233},\n        {'T': 300, 'theta_star': -0.5, 'S': 20, 'delta': 0.01, 's_obs': 1732, 's_sim': 20234},\n    ]\n\n    results = []\n    for case in test_cases:\n        estimated_theta = run_test_case(case)\n        results.append(estimated_theta)\n\n    print(f\"[{','.join(f'{x:.4f}' for x in results)}]\")\n\nsolve()\n```", "id": "2401827"}, {"introduction": "This final practice demonstrates the true power and flexibility of indirect inference by taking you beyond the world of linear stochastic models. Here, you will estimate the parameter of a famous chaotic system, the logistic map, whose output can appear random despite being generated by a deterministic rule. The exercise [@problem_id:2401774] shows that by summarizing the complex dynamics with a simple linear AR(1) auxiliary model, we can successfully recover the underlying structural parameter $r$. This powerfully illustrates that indirect inference is applicable whenever a model can be simulated, making it an invaluable tool for systems where the likelihood function is intractable or unknown.", "problem": "Construct a self-contained program that implements an estimator based on Indirect Inference (II) to recover the structural parameter of a chaotic discrete-time model. The true data generating process (DGP) is defined by the logistic map with a single unknown parameter. The state equation is\n$$\nx_{t+1} = r \\, x_t \\, (1 - x_t), \\quad t = 0,1,2,\\dots,T-1,\n$$\nwith fixed known initial condition $x_0 \\in (0,1)$, and the observation equation is\n$$\ny_t = x_t + \\varepsilon_t, \\quad \\varepsilon_t \\sim \\mathcal{N}(0,\\sigma^2), \\quad t = 0,1,\\dots,T-1,\n$$\nwhere $\\varepsilon_t$ are independent and identically distributed Gaussian disturbances with known standard deviation $\\sigma  0$ (possibly $\\sigma=0$). The only unknown structural parameter is $r \\in \\mathbb{R}$. The objective is to estimate $r$ using Indirect Inference (II) by matching auxiliary statistics computed on the observed data with those computed on simulated data from the structural model.\n\nDefine the auxiliary model as the autoregressive model of order one $AR(1)$ with intercept:\n$$\ny_t = a_0 + a_1 \\, y_{t-1} + u_t, \\quad t=1,2,\\dots,T-1,\n$$\nwhere $u_t$ are residuals with zero mean. For any series $\\{y_t\\}_{t=0}^{T-1}$, define the auxiliary statistic vector\n$$\n\\hat{b}(y) = \\big(\\hat{a}_0(y), \\hat{a}_1(y), \\hat{s}_u(y)\\big),\n$$\nwhere $\\hat{a}_0(y)$ and $\\hat{a}_1(y)$ are the ordinary least squares (OLS) estimators of $a_0$ and $a_1$, and\n$$\n\\hat{s}_u(y) = \\sqrt{\\frac{1}{T-1}\\sum_{t=1}^{T-1} \\hat{u}_t(y)^2},\n$$\nwith $\\hat{u}_t(y)$ the OLS residuals. The Indirect Inference estimator of $r$ is defined as the minimizer of a quadratic distance between the auxiliary statistics computed from the observed data and those computed from simulated data generated by the structural model with candidate parameter values. Let $K \\in \\mathbb{N}$ be the number of independent simulated datasets for averaging. For a candidate parameter $r$, define $K$ simulated datasets $\\{y^{(k)}(r)\\}_{k=1}^K$ using the same sample size $T$, the same initial condition $x_0$, and Gaussian observation noise with the same known $\\sigma$. Define the average simulated auxiliary statistics as\n$$\n\\bar{b}(r) = \\frac{1}{K} \\sum_{k=1}^K \\hat{b}\\!\\left(y^{(k)}(r)\\right).\n$$\nWith the identity weighting matrix $W = I_3$, the criterion function is\n$$\nQ(r) = \\left(\\hat{b}(y^{obs}) - \\bar{b}(r) \\right)^{\\top} W \\left(\\hat{b}(y^{obs}) - \\bar{b}(r) \\right),\n$$\nwhere $y^{obs}$ denotes the observed data series. The Indirect Inference estimator is\n$$\n\\hat{r} \\in \\arg\\min_{r \\in \\mathcal{R}} Q(r),\n$$\nover a prespecified finite grid $\\mathcal{R}$.\n\nYour program must implement this estimator exactly as defined above with the following fixed design elements to ensure determinacy and reproducibility:\n\n- Parameter grid: \n$$\n\\mathcal{R} = \\left\\{ 3.50 + 0.0025 \\, j \\,:\\, j = 0,1,2,\\dots,200 \\right\\}.\n$$\n- Number of simulation replications: $K = 15$.\n- Initial condition: $x_0 = 0.123456789$.\n- Weighting matrix: $W = I_3$.\n- For reproducibility across candidate values $r \\in \\mathcal{R}$, use common random numbers for the simulated datasets: for each test case, generate exactly $K$ independent Gaussian noise sequences of length $T$ from a pseudo-random number generator with a fixed seed $s_{sim}$, and reuse these $K$ sequences for all $r \\in \\mathcal{R}$. For the observed data, generate its Gaussian noise sequence from a pseudo-random number generator with a fixed seed $s_{obs}$. In all cases, draw $\\varepsilon_t$ as independent $\\mathcal{N}(0,\\sigma^2)$ variates. The initial condition $x_0$ must be the same across all simulations.\n\nTest suite. Implement and solve the estimator for each of the following test cases, where the true structural parameter, sample size, noise level, and seeds are specified:\n\n- Case A (general case): $r^{\\star} = 3.8000$, $T = 1000$, $\\sigma = 0.0200$, $s_{obs} = 1729$, $s_{sim} = 2468$.\n- Case B (near onset of chaos, lower noise): $r^{\\star} = 3.5700$, $T = 800$, $\\sigma = 0.0100$, $s_{obs} = 1730$, $s_{sim} = 2469$.\n- Case C (maximal chaos, no measurement noise): $r^{\\star} = 4.0000$, $T = 1200$, $\\sigma = 0.0000$, $s_{obs} = 1731$, $s_{sim} = 2470$.\n- Case D (shorter series, higher noise): $r^{\\star} = 3.9500$, $T = 300$, $\\sigma = 0.0500$, $s_{obs} = 1732$, $s_{sim} = 2471$.\n\nFor each case, generate the observed data $y^{obs}$ by simulating the logistic map with the specified $r^{\\star}$, $x_0$, and $T$, and adding Gaussian noise with the specified $\\sigma$ using the specified $s_{obs}$. Then compute the Indirect Inference estimator $\\hat{r}$ over the grid $\\mathcal{R}$ as defined above, using $K$ simulated datasets per grid point with common random numbers from the specified $s_{sim}$.\n\nFinal output format. Your program should produce a single line of output containing the four estimated values $\\hat{r}$ for Cases A–D, in that order, as a comma-separated list enclosed in square brackets. Each value must be printed as a decimal rounded to exactly six digits after the decimal point. For example, the output format is like \"[rA,rB,rC,rD]\" where $rA$, $rB$, $rC$, and $rD$ are the four rounded estimates. No units are involved and no additional text should be printed.", "solution": "The problem requires constructing an estimator grounded in the definition of Indirect Inference (II) and applying it to a chaotic structural model. The structural DGP is the logistic map with observation noise. The estimator is defined by the following elements:\n\n1. Structural model. The state equation is $x_{t+1} = r \\, x_t \\, (1 - x_t)$ for $t = 0,1,\\dots,T-1$ with $x_0 = 0.123456789$. The observed data are $y_t = x_t + \\varepsilon_t$ with $\\varepsilon_t \\sim \\mathcal{N}(0,\\sigma^2)$ independent for each $t$.\n\n2. Auxiliary model. The auxiliary model is the $AR(1)$ with intercept: $y_t = a_0 + a_1 \\, y_{t-1} + u_t$ for $t = 1,\\dots,T-1$. For any series $y = (y_0,\\dots,y_{T-1})$, define the OLS estimates using the normal equations. Let $Y = (y_1,\\dots,y_{T-1})^{\\top}$ and $X$ be the $(T-1) \\times 2$ matrix with first column of ones and second column $(y_0,\\dots,y_{T-2})^{\\top}$. The OLS estimator is\n$$\n\\hat{\\beta}(y) = \\begin{pmatrix} \\hat{a}_0(y) \\\\ \\hat{a}_1(y) \\end{pmatrix} = (X^{\\top}X)^{-1} X^{\\top} Y,\n$$\nand the residuals are $\\hat{u}(y) = Y - X \\hat{\\beta}(y)$. The residual dispersion statistic is\n$$\n\\hat{s}_u(y) = \\sqrt{ \\frac{1}{T-1} \\sum_{t=1}^{T-1} \\hat{u}_t(y)^2 }.\n$$\nCollect the auxiliary statistics as $\\hat{b}(y) = \\big(\\hat{a}_0(y), \\hat{a}_1(y), \\hat{s}_u(y)\\big)$.\n\n3. Indirect Inference criterion. For a candidate parameter $r$, simulate $K = 15$ datasets $y^{(k)}(r)$ of length $T$ using the structural model with initial state $x_0$ and Gaussian noise with the known $\\sigma$. Compute the average of the auxiliary statistics across simulations,\n$$\n\\bar{b}(r) = \\frac{1}{K} \\sum_{k=1}^K \\hat{b}\\!\\left(y^{(k)}(r)\\right),\n$$\nand define the quadratic distance with identity weighting matrix $W = I_3$ as\n$$\nQ(r) = \\left(\\hat{b}(y^{obs}) - \\bar{b}(r)\\right)^{\\top} \\left(\\hat{b}(y^{obs}) - \\bar{b}(r)\\right).\n$$\nThe estimator is $\\hat{r} \\in \\arg\\min_{r \\in \\mathcal{R}} Q(r)$ over the finite grid $\\mathcal{R} = \\{3.50 + 0.0025 \\, j : j = 0,1,\\dots,200\\}$.\n\n4. Reproducibility and common random numbers. For each test case, fix the observed noise seed $s_{obs}$ to generate the single observed sequence $\\{\\varepsilon_t^{obs}\\}$, and fix the simulation seed $s_{sim}$ to generate exactly $K = 15$ independent Gaussian noise sequences $\\{\\varepsilon_t^{(k)}\\}_{k=1}^K$. Use these same $K$ sequences for all candidate $r \\in \\mathcal{R}$. This implements common random numbers so that $Q(r)$ varies due to the change in the structural dynamics rather than due to changes in simulation noise. The initial condition $x_0 = 0.123456789$ is the same across all simulations.\n\n5. Implementation plan consistent with first principles. For each test case: \n- Generate the observed series $y^{obs}$ by simulating $x_{t+1} = r^{\\star} x_t (1-x_t)$ with $x_0$ and then adding $\\varepsilon_t^{obs} \\sim \\mathcal{N}(0,\\sigma^2)$ for $t = 0,\\dots,T-1$. \n- Compute $\\hat{b}(y^{obs})$ via the OLS formulas above. \n- Generate $K$ independent Gaussian noise sequences with the specified $s_{sim}$, each of length $T$. For each candidate $r \\in \\mathcal{R}$, simulate the structural state path $x(r)$ from the logistic map using $x_0$, form the $K$ simulated datasets $y^{(k)}(r) = x(r) + \\varepsilon^{(k)}$, compute $\\hat{b}\\!\\left(y^{(k)}(r)\\right)$ for each $k$, obtain $\\bar{b}(r)$, evaluate $Q(r)$, and select the minimizer over $\\mathcal{R}$. If there are multiple minimizers due to numerical ties, select the smallest $r$ in $\\mathcal{R}$ that achieves the minimum, which is a well-defined rule.\n\n6. Numerical and statistical considerations. The logistic map for $r \\in [3.50,4.00]$ exhibits complex and often chaotic behavior. Indirect Inference leverages the auxiliary model to compare salient features (intercept, persistence, residual dispersion) between observed and simulated data. Matching these features across a set of simulations with common random numbers attains a criterion that guides the choice of $r$. The finite grid $\\mathcal{R}$ defines a sieve-type approximation to the continuous parameter space; the grid resolution $0.0025$ implies that the attainable accuracy for $\\hat{r}$ is within $0.0025$ in ideal conditions. The presence of measurement noise $\\sigma$ and sample size $T$ influence the variability of the auxiliary statistics and, consequently, the precision of $\\hat{r}$.\n\n7. Output. The final program computes $\\hat{r}$ for the four specified cases, in order A, B, C, D, and prints a single line with a bracketed, comma-separated list of the four estimates rounded to six decimal places, with no additional output.\n\nThis solution directly applies the definition of Indirect Inference by specifying the auxiliary statistics, the simulation framework, and the minimization over the prescribed grid, ensuring that each step is grounded in the mathematical formulation and that the results are reproducible due to fixed seeds and common random numbers.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef simulate_logistic_path(r: float, T: int, x0: float) - np.ndarray:\n    \"\"\"\n    Simulate the logistic map x_{t+1} = r x_t (1 - x_t) for t=0,...,T-2 with x_0 = x0.\n    Returns an array x of length T with x[0]=x0.\n    \"\"\"\n    x = np.empty(T, dtype=float)\n    x[0] = x0\n    for t in range(T - 1):\n        x[t + 1] = r * x[t] * (1.0 - x[t])\n    return x\n\ndef auxiliary_stats_ar1(y: np.ndarray) - np.ndarray:\n    \"\"\"\n    Compute auxiliary statistics from AR(1) with intercept:\n    y_t = a0 + a1 y_{t-1} + u_t, t=1..T-1.\n    Returns [a0_hat, a1_hat, s_u_hat] where s_u_hat = sqrt( (1/(T-1)) sum u_t^2 ).\n    \"\"\"\n    # Ensure 1D float array\n    y = np.asarray(y, dtype=float).ravel()\n    if y.size  2:\n        # Degenerate case: not enough observations; return NaNs (should not occur in this problem)\n        return np.array([np.nan, np.nan, np.nan], dtype=float)\n    y_curr = y[1:]\n    y_lag = y[:-1]\n    # Design matrix with intercept\n    X = np.column_stack((np.ones_like(y_lag), y_lag))\n    # OLS via normal equations\n    XtX = X.T @ X\n    XtY = X.T @ y_curr\n    beta = np.linalg.solve(XtX, XtY)\n    resid = y_curr - X @ beta\n    s_u = np.sqrt(np.mean(resid ** 2))\n    return np.array([beta[0], beta[1], s_u], dtype=float)\n\ndef indirect_inference_estimate_r(\n    r_grid: np.ndarray,\n    K: int,\n    x0: float,\n    T: int,\n    sigma: float,\n    obs_seed: int,\n    sim_seed: int,\n    r_true: float\n) - float:\n    \"\"\"\n    Compute the indirect inference estimate of r over the given grid r_grid.\n    - Generate observed data using r_true, x0, T, sigma with RNG seed obs_seed.\n    - Generate K simulation noise sequences using seed sim_seed (common random numbers).\n    - For each r in r_grid, simulate x path and form K simulated datasets by adding the fixed noise sequences.\n    - Compute auxiliary statistics for observed and average over simulated datasets.\n    - Minimize squared Euclidean distance between observed and average simulated auxiliary stats.\n    Returns the minimizing r (tie broken by smallest r).\n    \"\"\"\n    # Generate observed data\n    x_obs = simulate_logistic_path(r_true, T, x0)\n    rng_obs = np.random.default_rng(obs_seed)\n    eps_obs = rng_obs.normal(loc=0.0, scale=sigma, size=T)\n    y_obs = x_obs + eps_obs\n    b_obs = auxiliary_stats_ar1(y_obs)\n\n    # Pre-generate K noise sequences (common random numbers)\n    rng_sim = np.random.default_rng(sim_seed)\n    eps_bank = rng_sim.normal(loc=0.0, scale=sigma, size=(K, T))\n\n    # For each r, compute Q(r)\n    best_r = None\n    best_Q = np.inf\n\n    # Preallocate buffer for speed\n    # Loop over candidate r\n    for r in r_grid:\n        # Simulate state path once for this r\n        x_sim = simulate_logistic_path(r, T, x0)\n        # Add fixed noise sequences to produce K simulated datasets\n        # Each y_k has shape (T,)\n        Q_components = []\n        b_sum = np.zeros(3, dtype=float)\n        # Loop over K replications\n        for k in range(K):\n            y_k = x_sim + eps_bank[k]\n            b_k = auxiliary_stats_ar1(y_k)\n            b_sum += b_k\n        b_bar = b_sum / K\n        diff = b_obs - b_bar\n        Q_r = float(diff @ diff)  # Identity weighting\n        # Update best\n        if Q_r  best_Q - 1e-18 or (np.isclose(Q_r, best_Q) and (best_r is None or r  best_r)):\n            best_Q = Q_r\n            best_r = r\n\n    return float(best_r)\n\ndef solve():\n    # Fixed design elements\n    x0 = 0.123456789\n    K = 15\n    # Grid: {3.50 + 0.0025 * j, j=0..200}\n    r_grid = 3.50 + 0.0025 * np.arange(201, dtype=float)\n\n    # Define the test cases from the problem statement.\n    # Each tuple: (r_true, T, sigma, s_obs, s_sim)\n    test_cases = [\n        (3.8000, 1000, 0.0200, 1729, 2468),  # Case A\n        (3.5700,  800, 0.0100, 1730, 2469),  # Case B\n        (4.0000, 1200, 0.0000, 1731, 2470),  # Case C\n        (3.9500,  300, 0.0500, 1732, 2471),  # Case D\n    ]\n\n    results = []\n    for r_true, T, sigma, s_obs, s_sim in test_cases:\n        r_hat = indirect_inference_estimate_r(\n            r_grid=r_grid,\n            K=K,\n            x0=x0,\n            T=T,\n            sigma=sigma,\n            obs_seed=s_obs,\n            sim_seed=s_sim,\n            r_true=r_true\n        )\n        # Round to 6 decimals in final output formatting\n        results.append(r_hat)\n\n    # Format each result to exactly 6 decimal places\n    formatted = \",\".join(f\"{val:.6f}\" for val in results)\n    # Final print statement in the exact required format.\n    print(f\"[{formatted}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2401774"}]}