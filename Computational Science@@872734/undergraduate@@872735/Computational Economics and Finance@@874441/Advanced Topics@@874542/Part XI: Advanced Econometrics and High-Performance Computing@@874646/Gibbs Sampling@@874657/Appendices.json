{"hands_on_practices": [{"introduction": "The engine of a Gibbs sampler is its ability to draw from full conditional distributions. Before we can simulate, we must first derive these essential building blocks from the joint distribution. This first exercise provides practice in this fundamental skill, using an intuitive joint distribution that is uniform over a triangular region to make the dependencies between variables clear. [@problem_id:1920313]", "problem": "Consider a pair of continuous random variables, $(X, Y)$, whose joint probability density function (PDF) is uniform over a specific region in the $xy$-plane. The joint PDF is given by $f(x, y) = C$ for $0 < x < y < 1$, and $f(x, y) = 0$ otherwise, where $C$ is the appropriate normalization constant.\n\nAn iterative simulation strategy to generate samples from this joint distribution requires the knowledge of the two full conditional distributions: the distribution of $X$ given a fixed value of $Y=y$, and the distribution of $Y$ given a fixed value of $X=x$.\n\nWhich of the following options correctly describes these two conditional distributions? Let $U(a, b)$ denote the uniform distribution on the interval $(a, b)$.\n\nA. $X|Y=y \\sim U(0, 1)$ and $Y|X=x \\sim U(0, 1)$\n\nB. $X|Y=y \\sim U(0, y)$ and $Y|X=x \\sim U(x, 1)$\n\nC. $X|Y=y \\sim U(y, 1)$ and $Y|X=x \\sim U(0, x)$\n\nD. $X|Y=y \\sim U(0, 1-y)$ and $Y|X=x \\sim U(0, 1-x)$\n\nE. $X|Y=y \\sim U(0, y/2)$ and $Y|X=x \\sim U(x, (1+x)/2)$", "solution": "We are given a joint PDF $f(x,y)=C$ on the region $\\{(x,y): 0<x<y<1\\}$ and $f(x,y)=0$ otherwise, where $C$ is a normalization constant. For deriving conditional distributions, we use the definitions of conditional densities:\n- For fixed $y$, the conditional density of $X$ given $Y=y$ is $f_{X|Y}(x|y)=\\frac{f(x,y)}{f_{Y}(y)}$ on the support where $f(x,y)>0$.\n- For fixed $x$, the conditional density of $Y$ given $X=x$ is $f_{Y|X}(y|x)=\\frac{f(x,y)}{f_{X}(x)}$ on the support where $f(x,y)>0$.\n\nFirst, identify the supports for the conditionals:\n- Given $Y=y$ with $0<y<1$, the admissible $x$ values must satisfy $0<x<y$.\n- Given $X=x$ with $0<x<1$, the admissible $y$ values must satisfy $x<y<1$.\n\nCompute the marginal densities needed for normalization in the conditionals:\n- For $0<y<1$,\n$$\nf_{Y}(y)=\\int_{0}^{y} f(x,y)\\,dx=\\int_{0}^{y} C\\,dx=C\\,y.\n$$\n- For $0<x<1$,\n$$\nf_{X}(x)=\\int_{x}^{1} f(x,y)\\,dy=\\int_{x}^{1} C\\,dy=C\\,(1-x).\n$$\n\nNow derive the conditional densities:\n- For $0<x<y<1$,\n$$\nf_{X|Y}(x|y)=\\frac{f(x,y)}{f_{Y}(y)}=\\frac{C}{C\\,y}=\\frac{1}{y},\n$$\nand $f_{X|Y}(x|y)=0$ otherwise. This is the density of a uniform distribution on $(0,y)$, so $X|Y=y\\sim U(0,y)$.\n\n- For $x<y<1$ with $0<x<1$,\n$$\nf_{Y|X}(y|x)=\\frac{f(x,y)}{f_{X}(x)}=\\frac{C}{C\\,(1-x)}=\\frac{1}{1-x},\n$$\nand $f_{Y|X}(y|x)=0$ otherwise. This is the density of a uniform distribution on $(x,1)$, so $Y|X=x\\sim U(x,1)$.\n\nTherefore, the correct option is $B$.", "answer": "$$\\boxed{B}$$", "id": "1920313"}, {"introduction": "Once the full conditional distributions are known, the next step is to execute the sampling algorithm. This practice makes the abstract process concrete by guiding you through a single, complete iteration of a Gibbs sampler. Using the inverse transform method, you will see exactly how the chain transitions from its current state to a new one, one variable at a time. [@problem_id:1920320]", "problem": "Consider a two-dimensional random vector $(X, Y)$ whose joint probability distribution is defined by the following full conditional distributions:\n- The conditional distribution of $X$ given $Y=y$ is an Exponential distribution with a rate parameter of $y$. The probability density function is given by $p(x|y) = y \\exp(-yx)$ for $x > 0$.\n- The conditional distribution of $Y$ given $X=x$ is a Poisson distribution with a mean parameter of $x$. The probability mass function is given by $p(y=k|x) = \\frac{x^k \\exp(-x)}{k!}$ for $k \\in \\{0, 1, 2, \\dots\\}$.\n\nYou are tasked with performing one full iteration of a Gibbs sampler. Starting from the initial state $(x^{(0)}, y^{(0)}) = (2, 3)$, you will generate a new state $(x^{(1)}, y^{(1)})$. The procedure for the iteration is as follows: first, draw a sample for $x^{(1)}$ from the distribution $p(x|y^{(0)})$, and then, using this new value $x^{(1)}$, draw a sample for $y^{(1)}$ from the distribution $p(y|x^{(1)})$.\n\nTo generate the necessary random variates, you must use the inverse transform sampling method. Use the following random numbers, which are drawn from a Uniform(0,1) distribution:\n- For generating $x^{(1)}$, use the uniform random number $u_x = 0.600$.\n- For generating $y^{(1)}$, use the uniform random number $u_y = 0.750$.\n\nWhat are the numerical values for the new state $(x^{(1)}, y^{(1)})$? The value for $x^{(1)}$ must be rounded to four significant figures.", "solution": "We perform one Gibbs update using inverse transform sampling.\n\n1) Sample $x^{(1)}$ from $p(x \\mid y^{(0)}=3)$. For an Exponential rate $y$, the conditional CDF is\n$$F(x \\mid y)=1-\\exp(-yx), \\quad x>0.$$\nInverse transform uses $u_{x}=F(x \\mid y)$, hence\n$$x^{(1)}=F^{-1}(u_{x})=-\\frac{1}{y^{(0)}}\\ln\\!\\bigl(1-u_{x}\\bigr).$$\nWith $y^{(0)}=3$ and $u_{x}=0.600$,\n$$x^{(1)}=-\\frac{1}{3}\\ln(1-0.600)=-\\frac{1}{3}\\ln(0.4)=\\frac{1}{3}\\ln(2.5)\\approx 0.3054302439.$$\nRounded to four significant figures: $x^{(1)}=0.3054$.\n\n2) Sample $y^{(1)}$ from $p(y \\mid x^{(1)})$ using $u_{y}=0.750$. For a Poisson mean $x$, the pmf is\n$$p(y=k \\mid x)=\\frac{x^{k}\\exp(-x)}{k!}, \\quad k\\in\\{0,1,2,\\dots\\}.$$\nInverse transform for a discrete distribution selects the smallest $k$ such that $F(k \\mid x)=\\sum_{j=0}^{k}p(j \\mid x)\\ge u_{y}$.\n\nWith $x=x^{(1)}=\\frac{1}{3}\\ln(2.5)$, compute\n$$p(0 \\mid x)=\\exp(-x)=\\exp\\!\\Bigl(-\\tfrac{1}{3}\\ln(2.5)\\Bigr)=2.5^{-1/3}\\approx 0.7368.$$\nSince $p(0 \\mid x)=0.73680.750$, continue to $k=1$:\n$$p(1 \\mid x)=x\\exp(-x)=x\\,2.5^{-1/3}\\approx 0.30543\\times 0.7368\\approx 0.2250.$$\nThen\n$$F(1 \\mid x)=p(0 \\mid x)+p(1 \\mid x)\\approx 0.7368+0.2250=0.9618>0.750,$$\nso the smallest $k$ with $F(k \\mid x)\\ge 0.750$ is $k=1$. Therefore $y^{(1)}=1$.\n\nThus, the new state is $(x^{(1)},y^{(1)})=(0.3054,1)$ with $x^{(1)}$ rounded to four significant figures.", "answer": "$$\\boxed{(0.3054, 1)}$$", "id": "1920320"}, {"introduction": "A Gibbs sampler can sometimes be deceptively easy to implement; if you can derive and sample from the full conditionals, the algorithm will run. However, this mechanical execution does not guarantee convergence to a valid target distribution. This final practice explores the critical, and sometimes subtle, concept of posterior propriety, demonstrating a scenario where the Gibbs sampler fails to converge meaningfully because the underlying joint posterior is improper. [@problem_id:2398193]", "problem": "Consider a simple asset-return model commonly used in computational economics and finance. Let daily log-returns $\\{y_i\\}_{i=1}^n$ for a single asset be modeled as independent and identically distributed draws from a Normal distribution with unknown mean $\\mu$ and unknown variance $\\sigma^2$: \n$$\ny_i \\mid \\mu,\\sigma^2 \\sim \\mathcal{N}(\\mu,\\sigma^2), \\quad i=1,\\dots,n.\n$$\nSuppose we observe $n=3$ returns that are not all identical (so that the sample variance is strictly positive). Consider the following improper prior specification:\n$$\np(\\mu) \\propto 1, \\quad p(\\sigma^2) \\propto 1, \\quad \\sigma^2 > 0,\n$$\nand the standard two-block Gibbs sampler that alternates draws from the full conditionals $p(\\mu \\mid \\sigma^2,\\mathbf{y})$ and $p(\\sigma^2 \\mid \\mu,\\mathbf{y})$, where $\\mathbf{y} = (y_1,y_2,y_3)$. Which of the following statements is most accurate?\n\nA. The posterior $p(\\mu,\\sigma^2 \\mid \\mathbf{y})$ is proper for any sample size $n \\ge 1$, so the Gibbs sampler converges to a unique stationary distribution equal to the posterior.\n\nB. For $n=3$, each full conditional distribution is proper, so the Gibbs sampler can be implemented, but the joint posterior $p(\\mu,\\sigma^2 \\mid \\mathbf{y})$ is improper; therefore, the Markov Chain Monte Carlo (MCMC) chain lacks a valid stationary distribution even if it appears to mix.\n\nC. The conditional density $p(\\sigma^2 \\mid \\mu,\\mathbf{y})$ is improper for any $n \\le 3$, so the Gibbs sampler cannot even be implemented; the failure arises from undefined full conditionals.\n\nD. Replacing $p(\\sigma^2) \\propto 1$ with the scale-invariant prior $p(\\log \\sigma^2) \\propto 1$ leaves the propriety status unchanged and thus cannot resolve any lack of posterior propriety at $n=3$.", "solution": "The problem requires an analysis of a Gibbs sampler's validity for a Normal model with unknown mean $\\mu$ and variance $\\sigma^2$, under a specific improper prior specification and a small sample size $n=3$. We must first determine the propriety of the joint posterior distribution and the full conditional distributions.\n\nThe model is specified as:\n- Likelihood: $y_i \\mid \\mu, \\sigma^2 \\sim \\mathcal{N}(\\mu, \\sigma^2)$ for $i=1, \\dots, n$. The likelihood function is $L(\\mu, \\sigma^2; \\mathbf{y}) = \\prod_{i=1}^n (2\\pi\\sigma^2)^{-1/2} \\exp\\left(-\\frac{(y_i-\\mu)^2}{2\\sigma^2}\\right) = (2\\pi)^{-n/2} (\\sigma^2)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n (y_i-\\mu)^2\\right)$.\n- Sample size: $n=3$.\n- Data property: Not all $y_i$ are identical, which ensures the sample variance $s^2 = \\frac{1}{n-1}\\sum_{i=1}^n (y_i - \\bar{y})^2$ is strictly positive.\n- Prior: An improper prior $p(\\mu, \\sigma^2) = p(\\mu)p(\\sigma^2) \\propto 1 \\cdot 1 = 1$ for $\\sigma^2  0$ and $-\\infty  \\mu  \\infty$.\n\nFirst, we derive the joint posterior distribution $p(\\mu, \\sigma^2 \\mid \\mathbf{y})$ using Bayes' theorem:\n$$\np(\\mu, \\sigma^2 \\mid \\mathbf{y}) \\propto L(\\mu, \\sigma^2; \\mathbf{y}) \\cdot p(\\mu, \\sigma^2)\n$$\n$$\np(\\mu, \\sigma^2 \\mid \\mathbf{y}) \\propto (\\sigma^2)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n (y_i-\\mu)^2\\right)\n$$\nWe can rewrite the sum of squares as $\\sum_{i=1}^n (y_i-\\mu)^2 = \\sum_{i=1}^n (y_i - \\bar{y} + \\bar{y} - \\mu)^2 = \\sum_{i=1}^n (y_i-\\bar{y})^2 + n(\\mu - \\bar{y})^2 = (n-1)s^2 + n(\\mu-\\bar{y})^2$. Let $S = (n-1)s^2$. Since $s^2  0$ and $n=3$, $S  0$.\nThe posterior is then:\n$$\np(\\mu, \\sigma^2 \\mid \\mathbf{y}) \\propto (\\sigma^2)^{-n/2} \\exp\\left(-\\frac{(n-1)s^2 + n(\\mu-\\bar{y})^2}{2\\sigma^2}\\right)\n$$\n\nTo check if this posterior is proper, we must verify if its integral over the parameter space is finite:\n$$\n\\int_0^\\infty \\int_{-\\infty}^\\infty p(\\mu, \\sigma^2 \\mid \\mathbf{y}) d\\mu d\\sigma^2  \\infty\n$$\nLet us integrate with respect to $\\mu$ first:\n$$\n\\int_{-\\infty}^\\infty \\exp\\left(-\\frac{n(\\mu-\\bar{y})^2}{2\\sigma^2}\\right) d\\mu\n$$\nThis is the kernel of a Normal distribution for $\\mu$ with mean $\\bar{y}$ and variance $\\sigma^2/n$. The integral is equal to $\\sqrt{2\\pi\\sigma^2/n}$.\nSubstituting this back, the marginal posterior for $\\sigma^2$ is:\n$$\np(\\sigma^2 \\mid \\mathbf{y}) \\propto \\int_0^\\infty (\\sigma^2)^{-n/2} \\exp\\left(-\\frac{(n-1)s^2}{2\\sigma^2}\\right) \\cdot \\sqrt{2\\pi\\sigma^2/n} \\, d\\sigma^2\n$$\n$$\np(\\sigma^2 \\mid \\mathbf{y}) \\propto (\\sigma^2)^{-n/2 + 1/2} \\exp\\left(-\\frac{(n-1)s^2}{2\\sigma^2}\\right) = (\\sigma^2)^{-(n-1)/2} \\exp\\left(-\\frac{(n-1)s^2}{2\\sigma^2}\\right)\n$$\nThis expression is the kernel of an Inverse-Gamma distribution for $\\sigma^2$. An Inverse-Gamma distribution with shape $\\alpha$ and scale $\\beta$, denoted $\\text{IG}(\\alpha, \\beta)$, has a density proportional to $x^{-(\\alpha+1)}e^{-\\beta/x}$. For our distribution, the shape parameter $\\alpha$ must satisfy $\\alpha+1 = (n-1)/2$, which gives $\\alpha = (n-1)/2 - 1 = (n-3)/2$.\nFor the distribution to be proper, the shape parameter must be positive: $\\alpha  0$.\n$$\n\\frac{n-3}{2}  0 \\implies n  3.\n$$\nFor the given problem with $n=3$, the shape parameter is $\\alpha = (3-3)/2 = 0$. The integral of $p(\\sigma^2 \\mid \\mathbf{y})$ diverges logarithmically. Thus, the joint posterior $p(\\mu, \\sigma^2 \\mid \\mathbf{y})$ is **improper** for $n=3$.\n\nNext, we examine the full conditional distributions required for the Gibbs sampler.\n1.  **Full conditional for $\\mu$**: $p(\\mu \\mid \\sigma^2, \\mathbf{y})$\n    $$\n    p(\\mu \\mid \\sigma^2, \\mathbf{y}) \\propto p(\\mu, \\sigma^2 \\mid \\mathbf{y}) \\propto \\exp\\left(-\\frac{n(\\mu-\\bar{y})^2}{2\\sigma^2}\\right)\n    $$\n    This is the kernel of a Normal distribution, $\\mathcal{N}(\\bar{y}, \\sigma^2/n)$. This is a proper distribution for any $n \\ge 1$ and any given $\\sigma^2  0$.\n\n2.  **Full conditional for $\\sigma^2$**: $p(\\sigma^2 \\mid \\mu, \\mathbf{y})$\n    $$\n    p(\\sigma^2 \\mid \\mu, \\mathbf{y}) \\propto p(\\mu, \\sigma^2 \\mid \\mathbf{y}) \\propto (\\sigma^2)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(y_i-\\mu)^2\\right)\n    $$\n    This is the kernel of an Inverse-Gamma distribution for $\\sigma^2$. Let the sum of squares be $Q = \\sum_{i=1}^n(y_i-\\mu)^2  0$. The density is proportional to $(\\sigma^2)^{-n/2} \\exp(-Q/(2\\sigma^2))$. Comparing this to the $\\text{IG}(\\alpha, \\beta)$ kernel $(\\sigma^2)^{-(\\alpha+1)}e^{-\\beta/\\sigma^2}$, we have $\\alpha+1 = n/2$, so the shape parameter is $\\alpha = n/2-1$. For this conditional to be proper, we need $\\alpha  0$, which means $n/2 - 1  0 \\implies n  2$.\n    For the given $n=3$, the shape parameter is $\\alpha = 3/2 - 1 = 1/2  0$. Therefore, the full conditional $p(\\sigma^2 \\mid \\mu, \\mathbf{y})$ is a proper Inverse-Gamma distribution.\n\nSummary for $n=3$:\n- The joint posterior $p(\\mu, \\sigma^2 \\mid \\mathbf{y})$ is **improper**.\n- The full conditional $p(\\mu \\mid \\sigma^2, \\mathbf{y})$ is **proper** (Normal).\n- The full conditional $p(\\sigma^2 \\mid \\mu, \\mathbf{y})$ is **proper** (Inverse-Gamma).\n\nNow we evaluate each statement.\n\nA. The posterior $p(\\mu,\\sigma^2 \\mid \\mathbf{y})$ is proper for any sample size $n \\ge 1$, so the Gibbs sampler converges to a unique stationary distribution equal to the posterior.\nThis statement is false. As derived, the joint posterior is proper only if $n  3$. For $n=3$, it is improper. **Incorrect**.\n\nB. For $n=3$, each full conditional distribution is proper, so the Gibbs sampler can be implemented, but the joint posterior $p(\\mu,\\sigma^2 \\mid \\mathbf{y})$ is improper; therefore, the Markov Chain Monte Carlo (MCMC) chain lacks a valid stationary distribution even if it appears to mix.\nThis statement is a precise summary of our findings. For $n=3$, both full conditionals are indeed proper, meaning one can algorithmically draw from them and run the sampler. However, because the joint posterior is improper, the resulting Markov chain does not converge to a proper probability distribution. Such a chain is not positive recurrent and does not possess a valid stationary distribution. This phenomenon, where conditionals are proper but the joint is not, is a known pathology in MCMC. **Correct**.\n\nC. The conditional density $p(\\sigma^2 \\mid \\mu,\\mathbf{y})$ is improper for any $n \\le 3$, so the Gibbs sampler cannot even be implemented; the failure arises from undefined full conditionals.\nThis is false. We showed that $p(\\sigma^2 \\mid \\mu, \\mathbf{y})$ is proper for $n  2$. Specifically, for $n=3$, it is a proper Inverse-Gamma distribution. The Gibbs sampler *can* be implemented. The failure lies in the nature of the target distribution, not the implementation steps. **Incorrect**.\n\nD. Replacing $p(\\sigma^2) \\propto 1$ with the scale-invariant prior $p(\\log \\sigma^2) \\propto 1$ leaves the propriety status unchanged and thus cannot resolve any lack of posterior propriety at $n=3$.\nThe prior $p(\\log \\sigma^2) \\propto 1$ is the same as $p(\\sigma) \\propto 1/\\sigma$ or, via change of variables, $p(\\sigma^2) \\propto 1/\\sigma^2$. This is the standard Jeffreys' prior for $\\sigma^2$ (with $\\mu$ unknown). Let us re-examine the posterior propriety with this new prior.\n$$\np(\\mu, \\sigma^2 \\mid \\mathbf{y}) \\propto L(\\mu, \\sigma^2; \\mathbf{y}) \\cdot p(\\mu) \\cdot p(\\sigma^2) \\propto (\\sigma^2)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^2}\\sum(y_i-\\mu)^2\\right) \\cdot 1 \\cdot (\\sigma^2)^{-1}\n$$\n$$\np(\\mu, \\sigma^2 \\mid \\mathbf{y}) \\propto (\\sigma^2)^{-(n/2+1)} \\exp\\left(-\\frac{1}{2\\sigma^2}\\sum(y_i-\\mu)^2\\right)\n$$\nFollowing the same integration steps as before, the marginal posterior for $\\sigma^2$ becomes:\n$$\np(\\sigma^2 \\mid \\mathbf{y}) \\propto (\\sigma^2)^{-(n/2+1)+1/2} \\exp\\left(-\\frac{(n-1)s^2}{2\\sigma^2}\\right) = (\\sigma^2)^{-(n+1)/2} \\exp\\left(-\\frac{(n-1)s^2}{2\\sigma^2}\\right)\n$$\nThis is an Inverse-Gamma kernel with shape parameter $\\alpha$ satisfying $\\alpha+1 = (n+1)/2$, which gives $\\alpha = (n-1)/2$. For propriety, we need $\\alpha  0$, which implies $(n-1)/2  0 \\implies n  1$.\nWith this new prior, the posterior is proper for $n=3$ (since $31$). Therefore, changing the prior *does* resolve the lack of propriety. The statement claims the propriety status is \"unchanged\" and the issue \"cannot\" be resolved, both of which are demonstrably false. **Incorrect**.", "answer": "$$\\boxed{B}$$", "id": "2398193"}]}