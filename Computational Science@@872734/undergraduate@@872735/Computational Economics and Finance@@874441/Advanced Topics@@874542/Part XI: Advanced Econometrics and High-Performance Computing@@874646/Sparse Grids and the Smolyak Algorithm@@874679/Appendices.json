{"hands_on_practices": [{"introduction": "This first practice grounds the theory of sparse grids in a tangible and important application from computational finance: pricing a multi-asset basket option. You will directly confront the \"curse of dimensionality\" by comparing the computational cost of a full tensor-product grid against a Smolyak sparse grid. This exercise [@problem_id:2396782] will make it clear why sparse grids are an indispensable tool for tackling high-dimensional integration problems.", "problem": "Consider a European call option written on a basket of $d$ risky assets under the risk-neutral Black–Scholes framework. Let $S_i(0)$ denote the initial price of asset $i$, $r$ the constant risk-free rate, $\\sigma_i$ the volatility of asset $i$, and $T$ the maturity time. Under the risk-neutral measure, the terminal price of asset $i$ is given by\n$$\nS_i(T) = S_i(0)\\,\\exp\\!\\Big( \\big(r - \\tfrac{1}{2}\\sigma_i^2\\big)T + \\sigma_i \\sqrt{T}\\, Z_i \\Big),\n$$\nwhere $Z_i$ are independent standard normal random variables. The basket call option has payoff\n$$\n\\Pi = \\max\\!\\Big(\\frac{1}{d}\\sum_{i=1}^{d} S_i(T) - K,\\, 0\\Big),\n$$\nwith strike $K$. The arbitrage-free price at time $0$ is\n$$\nV_0 = e^{-rT}\\,\\mathbb{E}\\big[\\Pi\\big].\n$$\n\nYour task is to approximate $V_0$ for the case $d=5$ using Gaussian quadrature and to numerically demonstrate the curse of dimensionality by comparing a full tensor-product Gaussian quadrature with a Smolyak sparse grid constructed from the same one-dimensional rule.\n\nYou must proceed from first principles by turning the $d$-dimensional expectation under the standard normal distribution into a Gaussian–Hermite quadrature. Recall the Gaussian–Hermite quadrature rule of order $n$ for one dimension,\n$$\n\\int_{-\\infty}^{\\infty} e^{-x^2} f(x)\\,dx \\approx \\sum_{j=1}^{n} w_j f(x_j),\n$$\nwhere $x_j$ are the nodes and $w_j$ are the weights. Show that for a standard normal random variable $Z$,\n$$\n\\mathbb{E}[g(Z)] = \\frac{1}{\\sqrt{\\pi}}\\int_{-\\infty}^{\\infty} e^{-x^2}\\, g\\!\\big(\\sqrt{2}\\,x\\big)\\,dx,\n$$\nand generalize this to $d$ independent standard normals to justify a $d$-dimensional tensor product of one-dimensional Gaussian–Hermite rules. Use this to produce:\n\n- A $d$-dimensional tensor-product Gaussian–Hermite quadrature with $n$ nodes per dimension (total nodes $n^d$).\n- A Smolyak sparse grid of isotropic level $L$ built from the same one-dimensional Gaussian–Hermite family $\\{Q_\\ell\\}_{\\ell \\ge 1}$, where the one-dimensional order is $m(\\ell) = 2\\ell - 1$. Use the Smolyak combination formula with index set\n$$\n\\mathcal{I}(L,d) = \\big\\{\\boldsymbol{\\ell}\\in \\mathbb{N}^d : 1 \\le \\ell_k, \\ \\ |\\boldsymbol{\\ell}|_1 \\le L + d - 1 \\big\\},\n$$\nand coefficients\n$$\nc(\\boldsymbol{\\ell}) = (-1)^{L + d - 1 - |\\boldsymbol{\\ell}|_1}\\binom{d-1}{L + d - 1 - |\\boldsymbol{\\ell}|_1}.\n$$\nHere $Q_{\\ell}$ denotes the one-dimensional Gaussian–Hermite rule of order $m(\\ell)$, and the $d$-dimensional operator is the tensor product $\\bigotimes_{k=1}^d Q_{\\ell_k}$.\n\nFrom the above foundations, design an algorithm that:\n- Maps the expectation to the Gaussian–Hermite weighted integral via the change of variables $z = \\sqrt{2}\\,x$ in each dimension.\n- Correctly applies the normalization factor $\\pi^{-d/2}$ for $d$ dimensions.\n- Constructs the tensor grid and the Smolyak sparse grid and evaluates the discounted payoff at all required nodes.\n- Counts the number of function evaluations for each method as a proxy for computational cost.\n\nTest Suite. Implement the following two computational test cases and one comparison task:\n\n- Case A (happy path, $d=5$):\n  - Parameters: $S_0 = (100,\\, 90,\\, 110,\\, 95,\\, 105)$, $\\sigma = (0.2,\\, 0.25,\\, 0.15,\\, 0.3,\\, 0.18)$, $r = 0.02$, $T = 1.0$, $K = 100$.\n  - Tensor-product Gaussian–Hermite quadrature with $n=3$ nodes per dimension.\n  - Smolyak sparse grid with isotropic level $L=3$ and one-dimensional order $m(\\ell) = 2\\ell - 1$.\n  - A high-accuracy reference computed by tensor-product Gaussian–Hermite quadrature with $n_{\\text{ref}}=9$ nodes per dimension.\n  - Outputs to compute:\n    - The absolute error of the tensor-product approximation relative to the reference as a float.\n    - The absolute error of the sparse-grid approximation relative to the reference as a float.\n    - The tensor-product node count as an integer.\n    - The sparse-grid node count as an integer, taken as the sum of tensor-product sizes of all constituent terms in the Smolyak combination.\n\n- Case B (boundary condition with zero volatility, $d=5$):\n  - Parameters: same $S_0$, $r$, $T$, $K$ as in Case A, but $\\sigma = (0,\\,0,\\,0,\\,0,\\,0)$.\n  - The exact price is given by the deterministic value\n    $$\n    V_0^{\\text{det}} = e^{-rT}\\,\\max\\!\\Big(\\frac{1}{d}\\sum_{i=1}^{d} S_i(0)\\,e^{rT} - K,\\, 0\\Big) = \\max\\!\\Big(\\frac{1}{d}\\sum_{i=1}^{d} S_i(0) - K e^{-rT},\\, 0\\Big).\n    $$\n  - Verify that both the tensor-product quadrature with $n=3$ and the sparse grid with $L=3$ match $V_0^{\\text{det}}$ within an absolute tolerance of $10^{-10}$. Output a boolean indicating whether both pass.\n\n- Efficiency comparison:\n  - For Case A, output a boolean indicating whether the sparse grid achieves absolute error less than or equal to the tensor-product error while using strictly fewer nodes.\n\nFinal Output Format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list entries must be, in order:\n- The tensor-product absolute error for Case A (float).\n- The sparse-grid absolute error for Case A (float).\n- The tensor-product node count for Case A (integer).\n- The sparse-grid node count for Case A (integer).\n- The boundary condition check for Case B (boolean).\n- The efficiency comparison check for Case A (boolean).\n\nFor example, the output format must be\n$$\n[\\text{err\\_tensor},\\text{err\\_sparse},\\text{nodes\\_tensor},\\text{nodes\\_sparse},\\text{boundary\\_ok},\\text{efficiency\\_ok}],\n$$\nwith the two errors as decimal numbers, the two node counts as integers, and the last two entries as booleans. No additional text should be printed.", "solution": "We begin from the risk-neutral valuation principle for a European derivative: the time-$0$ price $V_0$ of a payoff $\\Pi$ at time $T$ is given by $V_0 = e^{-rT}\\,\\mathbb{E}[\\Pi]$, where the expectation is under the risk-neutral measure. In the Black–Scholes framework with independent standard normals $Z_i \\sim \\mathcal{N}(0,1)$, the terminal asset values are\n$$\nS_i(T) = S_i(0)\\,\\exp\\!\\Big( \\big(r - \\tfrac{1}{2}\\sigma_i^2\\big)T + \\sigma_i \\sqrt{T}\\, Z_i \\Big),\n$$\nand the basket call payoff is\n$$\n\\Pi = \\max\\!\\Big(\\frac{1}{d}\\sum_{i=1}^{d} S_i(T) - K,\\, 0\\Big).\n$$\nThus,\n$$\nV_0 = e^{-rT}\\,\\mathbb{E}\\left[\\max\\!\\Big(\\frac{1}{d}\\sum_{i=1}^{d} S_i(0)\\,\\exp\\!\\big( \\big(r - \\tfrac{1}{2}\\sigma_i^2\\big)T + \\sigma_i \\sqrt{T}\\, Z_i \\big) - K,\\, 0\\Big)\\right].\n$$\n\nTo connect to Gaussian–Hermite quadrature, we transform expectations under the standard normal law to integrals with the Gaussian–Hermite weight. For a single standard normal random variable $Z \\sim \\mathcal{N}(0,1)$ and a suitable test function $g$, we have\n$$\n\\mathbb{E}[g(Z)] = \\int_{-\\infty}^{\\infty} \\frac{1}{\\sqrt{2\\pi}} e^{-z^2/2} g(z)\\,dz.\n$$\nWith the change of variables $z = \\sqrt{2}\\,x$ (so $dz = \\sqrt{2}\\,dx$), one obtains\n$$\n\\mathbb{E}[g(Z)] = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} e^{-(\\sqrt{2}x)^2/2} g(\\sqrt{2}x)\\,\\sqrt{2}\\,dx = \\frac{1}{\\sqrt{\\pi}}\\int_{-\\infty}^{\\infty} e^{-x^2} g(\\sqrt{2}\\,x)\\,dx.\n$$\nIn $d$ dimensions with independent standard normals $\\boldsymbol{Z} = (Z_1,\\dots,Z_d)$, the joint density factorizes, and by applying the change of variables componentwise, we obtain\n$$\n\\mathbb{E}[g(\\boldsymbol{Z})] = \\frac{1}{\\pi^{d/2}} \\int_{\\mathbb{R}^d} e^{-\\|\\boldsymbol{x}\\|_2^2}\\, g(\\sqrt{2}\\,\\boldsymbol{x})\\, d\\boldsymbol{x}.\n$$\nHence a $d$-dimensional Gaussian–Hermite tensor-product quadrature yields\n$$\n\\mathbb{E}[g(\\boldsymbol{Z})] \\approx \\frac{1}{\\pi^{d/2}} \\sum_{j_1=1}^{n}\\cdots \\sum_{j_d=1}^{n} \\Big(\\prod_{k=1}^{d} w_{j_k}\\Big)\\, g\\!\\Big(\\sqrt{2}\\,x_{j_1},\\dots,\\sqrt{2}\\,x_{j_d}\\Big),\n$$\nwhere $(x_{j},w_{j})$ are the one-dimensional Gaussian–Hermite nodes and weights at order $n$, associated with the weight $e^{-x^2}$.\n\nApplying this to the basket payoff, we define\n$$\ng(\\boldsymbol{z}) = \\max\\!\\Big(\\frac{1}{d}\\sum_{i=1}^{d} S_i(0)\\,\\exp\\!\\big( \\big(r - \\tfrac{1}{2}\\sigma_i^2\\big)T + \\sigma_i \\sqrt{T}\\, z_i \\big) - K,\\, 0\\Big),\n$$\nand the price approximation is\n$$\nV_0 \\approx e^{-rT}\\,\\frac{1}{\\pi^{d/2}} \\sum_{j_1=1}^{n}\\cdots \\sum_{j_d=1}^{n} \\Big(\\prod_{k=1}^{d} w_{j_k}\\Big)\\, g\\!\\Big(\\sqrt{2}\\,x_{j_1},\\dots,\\sqrt{2}\\,x_{j_d}\\Big).\n$$\n\nThis tensor-product rule uses $n^d$ function evaluations. For $d=5$, the node count grows as $n^5$, which is a manifestation of the curse of dimensionality: even modest increases in $n$ cause exponential growth in cost.\n\nTo mitigate this, we consider a Smolyak sparse grid constructed from the same one-dimensional Gaussian–Hermite family. Let $Q_{\\ell}$ denote the one-dimensional Gaussian–Hermite quadrature of order $m(\\ell)=2\\ell-1$ for level $\\ell \\in \\mathbb{N}$. For isotropic level $L \\in \\mathbb{N}$ in $d$ dimensions, the Smolyak operator is\n$$\nA(L,d) = \\sum_{\\boldsymbol{\\ell}\\in \\mathcal{I}(L,d)} c(\\boldsymbol{\\ell}) \\bigotimes_{k=1}^{d} Q_{\\ell_k},\n$$\nwith index set\n$$\n\\mathcal{I}(L,d) = \\big\\{\\boldsymbol{\\ell}\\in \\mathbb{N}^d : 1 \\le \\ell_k,\\ \\ |\\boldsymbol{\\ell}|_1 \\le L + d - 1\\big\\},\n$$\nand combination coefficients\n$$\nc(\\boldsymbol{\\ell}) = (-1)^{L + d - 1 - |\\boldsymbol{\\ell}|_1}\\binom{d-1}{L + d - 1 - |\\boldsymbol{\\ell}|_1}.\n$$\nThe resulting sparse grid approximation of the expectation is\n$$\n\\mathbb{E}[g(\\boldsymbol{Z})] \\approx \\frac{1}{\\pi^{d/2}} \\sum_{\\boldsymbol{\\ell}\\in \\mathcal{I}(L,d)} c(\\boldsymbol{\\ell}) \\sum_{j_1=1}^{m(\\ell_1)} \\cdots \\sum_{j_d=1}^{m(\\ell_d)} \\left(\\prod_{k=1}^{d} w_{j_k}^{(\\ell_k)}\\right) g\\!\\Big(\\sqrt{2}\\,x_{j_1}^{(\\ell_1)},\\dots,\\sqrt{2}\\,x_{j_d}^{(\\ell_d)}\\Big),\n$$\nwhere $(x_j^{(\\ell)}, w_j^{(\\ell)})$ are the one-dimensional Gaussian–Hermite nodes and weights at order $m(\\ell)$. Note that this combination utilizes the same one-dimensional rules as the tensor product but mixes tensor products of varying orders through the Smolyak coefficients. The computational cost is the sum over tensor-product sizes $\\prod_{k=1}^d m(\\ell_k)$ for all $\\boldsymbol{\\ell} \\in \\mathcal{I}(L,d)$; this grows more gently in $d$ than $n^d$ for comparable accuracy.\n\nAlgorithmic design:\n\n- Precompute the one-dimensional Gaussian–Hermite nodes and weights for all required orders via a stable generator for orthogonal polynomials. The nodes and weights integrate $e^{-x^2}$ exactly up to degree $2n-1$, and the sum of weights equals $\\sqrt{\\pi}$, ensuring exactness for constant integrands.\n- Implement the tensor-product quadrature by forming a Cartesian product of one-dimensional nodes and weights, applying the $\\sqrt{2}$ scaling to map to standard normals, and multiplying the product weights. Multiply the accumulated sum by $\\pi^{-d/2}$ and $e^{-rT}$.\n- Implement the Smolyak sparse grid by iterating over all multi-indices $\\boldsymbol{\\ell}$ in $\\mathcal{I}(L,d)$, computing the combination coefficient $c(\\boldsymbol{\\ell})$, forming the tensor grid for each $\\boldsymbol{\\ell}$ using $m(\\ell_k)$ points in dimension $k$, evaluating and accumulating the weighted contributions with the same normalization factors. Count the computational work as the sum of tensor sizes across all $\\boldsymbol{\\ell}$.\n- For the boundary case with $\\sigma_i = 0$, the terminal prices are deterministic, $S_i(T) = S_i(0)e^{rT}$, and therefore the exact price is $V_0^{\\text{det}} = \\max\\!\\big(\\frac{1}{d}\\sum_i S_i(0) - K e^{-rT}, 0\\big)$. Because Gaussian–Hermite rules integrate constants exactly, both the tensor and sparse-grid approximations should match this value up to rounding errors.\n\nTest Suite implementation:\n\n- Case A: $d=5$, $S_0=(100,90,110,95,105)$, $\\sigma=(0.2,0.25,0.15,0.3,0.18)$, $r=0.02$, $T=1.0$, $K=100$. Compute:\n  - Reference with tensor $n_{\\text{ref}}=9$.\n  - Tensor with $n=3$; record absolute error and node count $3^5$.\n  - Smolyak with $L=3$ and $m(\\ell)=2\\ell-1$; record absolute error against reference and the sparse-grid node count as the sum of tensor sizes over $\\mathcal{I}(L,d)$.\n- Case B: same as Case A but $\\sigma=\\boldsymbol{0}$. Verify both tensor $n=3$ and sparse $L=3$ are within $10^{-10}$ of $V_0^{\\text{det}}$.\n- Efficiency comparison: For Case A, check whether the sparse grid achieves error less than or equal to the tensor-product error while using strictly fewer nodes.\n\nThe program will produce a single output line:\n$[\\text{err\\_tensor},\\text{err\\_sparse},\\text{nodes\\_tensor},\\text{nodes\\_sparse},\\text{boundary\\_ok},\\text{efficiency\\_ok}]$,\nwith the entries defined as above. This design demonstrates the curse of dimensionality via the $n^5$ scaling for the tensor grid and contrasts it with the Smolyak sparse grid, which, for the same one-dimensional Gaussian–Hermite family, achieves competitive accuracy with substantially fewer function evaluations.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom numpy.polynomial.hermite import hermgauss\nimport math\nfrom itertools import product\n\ndef gh_rule(n):\n    # One-dimensional Gauss-Hermite nodes and weights for weight e^{-x^2}\n    x, w = hermgauss(n)\n    return x, w\n\ndef basket_payoff(Z, S0, sigmas, r, T, K):\n    # Z: shape (N, d)\n    d = Z.shape[1]\n    S0 = np.asarray(S0, dtype=float)\n    sigmas = np.asarray(sigmas, dtype=float)\n    mu = (r - 0.5 * sigmas**2) * T\n    sgsqrtT = sigmas * math.sqrt(T)\n    # exponent per point and dimension\n    exponents = mu + sgsqrtT * Z  # shape (N, d)\n    ST = S0 * np.exp(exponents)   # broadcasting over (N, d)\n    avg = np.mean(ST, axis=1)\n    payoff = np.maximum(avg - K, 0.0)\n    return payoff\n\ndef tensor_gauss_hermite_price(S0, sigmas, r, T, K, n):\n    d = len(S0)\n    x, w = gh_rule(n)\n    # Build tensor grid via meshgrid\n    grids_x = np.meshgrid(*([x]*d), indexing='ij')\n    grids_w = np.meshgrid(*([w]*d), indexing='ij')\n    # Flatten\n    X_cols = [g.ravel() for g in grids_x]  # list length d, each shape (n**d,)\n    W_arrays = [gw.ravel() for gw in grids_w]\n    # Product weights\n    W_prod = np.ones_like(W_arrays[0])\n    for Wa in W_arrays:\n        W_prod *= Wa\n    # Map to standard normals: Z = sqrt(2) * x\n    Z = np.sqrt(2.0) * np.column_stack(X_cols)  # shape (N, d)\n    payoff = basket_payoff(Z, S0, sigmas, r, T, K)\n    # Normalization factor for d-dim expectation\n    factor = math.pi ** (-d / 2.0)\n    expectation = factor * np.sum(W_prod * payoff)\n    price = math.exp(-r * T) * expectation\n    nodes = n ** d\n    return price, nodes\n\ndef generate_multi_indices_sum_d(s, d, min_level=1):\n    # Generate all tuples of length d of positive integers >= min_level with sum s\n    # Use recursive backtracking\n    result = []\n    def backtrack(prefix, remaining, k):\n        if k == d - 1:\n            last = remaining\n            if last >= min_level:\n                result.append(tuple(prefix + [last]))\n            return\n        # Each component at least min_level\n        min_val = min_level\n        # Remaining must allow at least min_level for remaining dims\n        max_val = remaining - (d - k - 1) * min_level\n        for val in range(min_val, max_val + 1):\n            backtrack(prefix + [val], remaining - val, k + 1)\n    backtrack([], s, 0)\n    return result\n\ndef smolyak_sparse_price(S0, sigmas, r, T, K, L):\n    d = len(S0)\n    # Precompute 1D rules for levels 1..(L + d - 1) because indices can reach that in sum, but each component level is bounded by sum\n    # However, for efficiency we build on demand and cache by level\n    rule_cache = {}\n    def one_d_rule_for_level(level):\n        if level not in rule_cache:\n            n = 2 * level - 1\n            rule_cache[level] = gh_rule(n)\n        return rule_cache[level]\n\n    total_nodes_cost = 0\n    factor = math.pi ** (-d / 2.0)\n    acc = 0.0\n    # Iterate sums s from d to L + d\n    for s in range(d, L + d):\n        # Binomial coefficient for all multi-indices with |ell|_1 = s\n        comb_coeff = math.comb(d - 1, L + d - 1 - s)\n        sign = -1 if ((L + d - 1 - s) % 2 == 1) else 1\n        c_s = sign * comb_coeff\n        if c_s == 0:\n            continue\n        # All multi-indices with sum s\n        for ell in generate_multi_indices_sum_d(s, d, min_level=1):\n            # Build per-dimension nodes and weights at levels in ell\n            x_list = []\n            w_list = []\n            n_points_list = []\n            for lev in ell:\n                xk, wk = one_d_rule_for_level(lev)\n                x_list.append(xk)\n                w_list.append(wk)\n                n_points_list.append(len(xk))\n            # Tensor product for this multi-index\n            grids_x = np.meshgrid(*x_list, indexing='ij')\n            grids_w = np.meshgrid(*w_list, indexing='ij')\n            # Flatten\n            X_cols = [g.ravel() for g in grids_x]\n            W_arrays = [gw.ravel() for gw in grids_w]\n            # Product weights\n            W_prod = np.ones_like(W_arrays[0])\n            for Wa in W_arrays:\n                W_prod *= Wa\n            # Map to Z\n            Z = np.sqrt(2.0) * np.column_stack(X_cols)\n            payoff = basket_payoff(Z, S0, sigmas, r, T, K)\n            contrib = factor * np.sum(W_prod * payoff)\n            acc += c_s * contrib\n            # Cost accumulation: number of nodes in this tensor product\n            nodes_this = 1\n            for npt in n_points_list:\n                nodes_this *= npt\n            total_nodes_cost += nodes_this\n    price = math.exp(-r * T) * acc\n    return price, total_nodes_cost\n\ndef solve():\n    # Define the test cases from the problem statement.\n\n    # Case A parameters\n    S0 = [100.0, 90.0, 110.0, 95.0, 105.0]\n    sigmas_A = [0.2, 0.25, 0.15, 0.3, 0.18]\n    r = 0.02\n    T = 1.0\n    K = 100.0\n    d = len(S0)\n\n    # Reference with tensor n_ref=9\n    price_ref, nodes_ref = tensor_gauss_hermite_price(S0, sigmas_A, r, T, K, n=9)\n\n    # Tensor with n=3\n    price_tensor, nodes_tensor = tensor_gauss_hermite_price(S0, sigmas_A, r, T, K, n=3)\n    err_tensor = abs(price_tensor - price_ref)\n\n    # Sparse grid with L=3\n    price_sparse, nodes_sparse = smolyak_sparse_price(S0, sigmas_A, r, T, K, L=3)\n    err_sparse = abs(price_sparse - price_ref)\n\n    # Case B: zero volatility boundary\n    sigmas_B = [0.0, 0.0, 0.0, 0.0, 0.0]\n    # Deterministic price\n    mean_S0 = sum(S0) / d\n    price_det = max(mean_S0 - K * math.exp(-r * T), 0.0)\n\n    price_tensor_B, _ = tensor_gauss_hermite_price(S0, sigmas_B, r, T, K, n=3)\n    price_sparse_B, _ = smolyak_sparse_price(S0, sigmas_B, r, T, K, L=3)\n\n    tol = 1e-10\n    boundary_ok = (abs(price_tensor_B - price_det) = tol) and (abs(price_sparse_B - price_det) = tol)\n\n    # Efficiency comparison for Case A\n    efficiency_ok = (err_sparse = err_tensor) and (nodes_sparse  nodes_tensor)\n\n    results = [\n        float(err_tensor),\n        float(err_sparse),\n        int(nodes_tensor),\n        int(nodes_sparse),\n        bool(boundary_ok),\n        bool(efficiency_ok),\n    ]\n\n    # Final print statement in the exact required format.\n    # Ensure default Python representation for floats and booleans.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2396782"}, {"introduction": "While standard sparse grids offer a huge advantage over tensor products, they assume the function being approximated is equally sensitive to all dimensions. This practice introduces anisotropic grids, which intelligently allocate more computational effort to more \"important\" dimensions. By implementing an anisotropic interpolant [@problem_id:2432646], you will learn how to tailor the grid structure to the specific characteristics of the function, a key skill for solving real-world high-dimensional problems efficiently.", "problem": "You are given the task of approximating a high-dimensional function on a hypercube using an anisotropic sparse grid based on the Smolyak construction. The domain is the unit hypercube $[0,1]^{10}$. Let $d = 10$. For each dimension $i \\in \\{1,\\dots,d\\}$ and each level $\\ell_i \\in \\mathbb{N}$ with $\\ell_i \\ge 1$, define the one-dimensional nested dyadic grid\n$$\nX_{\\ell_i} = \\left\\{ \\frac{j}{2^{\\ell_i - 1}} \\,:\\, j = 0,1,\\dots,2^{\\ell_i - 1} \\right\\} \\subset [0,1].\n$$\nFor a multi-index $\\boldsymbol{\\ell} = (\\ell_1,\\dots,\\ell_d)$, define the full tensor grid\n$$\nG_{\\boldsymbol{\\ell}} = X_{\\ell_1} \\times \\cdots \\times X_{\\ell_d}.\n$$\nLet $U_{\\boldsymbol{\\ell}}$ be the $d$-variate multilinear nodal interpolation operator on $G_{\\boldsymbol{\\ell}}$, defined by tensor products of one-dimensional linear Lagrange basis functions on the dyadic grids.\n\nLet $\\boldsymbol{\\alpha} = (\\alpha_1,\\dots,\\alpha_d)$ be a vector of positive integer anisotropy weights and let $Q \\in \\mathbb{N}$ with $Q \\ge 0$. Define the down-closed index set\n$$\n\\mathcal{I}(\\boldsymbol{\\alpha}, Q) = \\left\\{ \\boldsymbol{\\ell} \\in \\mathbb{N}^d \\,:\\, \\ell_i \\ge 1 \\text{ for all } i, \\ \\sum_{i=1}^d \\alpha_i (\\ell_i - 1) \\le Q \\right\\}.\n$$\nConsider a linear combination of full-grid interpolants\n$$\n\\mathcal{S}_{\\boldsymbol{\\alpha},Q} = \\sum_{\\boldsymbol{\\ell} \\in \\mathcal{I}(\\boldsymbol{\\alpha}, Q)} c_{\\boldsymbol{\\ell}} \\, U_{\\boldsymbol{\\ell}},\n$$\nwith coefficients $\\{c_{\\boldsymbol{\\ell}}\\}$ characterized by the interpolation consistency condition\n$$\n\\sum_{\\boldsymbol{k} \\in \\mathcal{I}(\\boldsymbol{\\alpha}, Q), \\, \\boldsymbol{k} \\ge \\boldsymbol{\\ell}} c_{\\boldsymbol{k}} = 1 \\quad \\text{for every } \\boldsymbol{\\ell} \\in \\mathcal{I}(\\boldsymbol{\\alpha}, Q),\n$$\nwhere $\\boldsymbol{k} \\ge \\boldsymbol{\\ell}$ denotes the componentwise partial order $k_i \\ge \\ell_i$ for all $i$. This condition ensures that $\\mathcal{S}_{\\boldsymbol{\\alpha},Q}$ interpolates the target function at every node in $\\bigcup_{\\boldsymbol{\\ell} \\in \\mathcal{I}(\\boldsymbol{\\alpha}, Q)} G_{\\boldsymbol{\\ell}}$.\n\nYour program must implement $\\mathcal{S}_{\\boldsymbol{\\alpha},Q}$ on $[0,1]^{10}$ and evaluate it at prescribed points. Use the following two target functions, each defined for $\\boldsymbol{x} = (x_1,\\dots,x_{10}) \\in [0,1]^{10}$:\n- $f_1(\\boldsymbol{x}) = \\exp(x_1) + \\sin(2\\pi x_2) + 0.1 \\sum_{i=3}^{10} 0.5^{\\,i} \\, x_i^2$.\n- $f_2(\\boldsymbol{x}) = \\log(1 + 5 x_1) + \\sqrt{1 + x_2} + 0.01 \\sum_{i=3}^{10} x_i$.\nAll trigonometric arguments are to be interpreted in radians. The natural logarithm function is $\\log(\\cdot)$ and the principal square root is $\\sqrt{\\cdot}$.\n\nUse the following evaluation set $\\mathcal{E} = \\{\\boldsymbol{y}^{(k)}\\}_{k=1}^{5} \\subset [0,1]^{10}$ where each $\\boldsymbol{y}^{(k)}$ is explicitly given:\n- $\\boldsymbol{y}^{(1)} = (0.13,\\,0.77,\\,0.50,\\,0.20,\\,0.80,\\,0.33,\\,0.66,\\,0.10,\\,0.90,\\,0.42)$,\n- $\\boldsymbol{y}^{(2)} = (0.31,\\,0.62,\\,0.25,\\,0.75,\\,0.40,\\,0.60,\\,0.20,\\,0.80,\\,0.35,\\,0.65)$,\n- $\\boldsymbol{y}^{(3)} = (0.73,\\,0.27,\\,0.15,\\,0.85,\\,0.55,\\,0.45,\\,0.05,\\,0.95,\\,0.22,\\,0.78)$,\n- $\\boldsymbol{y}^{(4)} = (0.50,\\,0.50,\\,0.10,\\,0.90,\\,0.30,\\,0.70,\\,0.25,\\,0.75,\\,0.40,\\,0.60)$,\n- $\\boldsymbol{y}^{(5)} = (0.21,\\,0.84,\\,0.63,\\,0.37,\\,0.12,\\,0.88,\\,0.47,\\,0.53,\\,0.19,\\,0.81)$.\n\nDefine the anisotropy weights that favor the first two dimensions as\n$$\n\\boldsymbol{\\alpha}^{\\mathrm{aniso}} = (1,\\,1,\\,4,\\,4,\\,4,\\,4,\\,4,\\,4,\\,4,\\,4),\n$$\nand the isotropic weights as\n$$\n\\boldsymbol{\\alpha}^{\\mathrm{iso}} = (1,\\,1,\\,1,\\,1,\\,1,\\,1,\\,1,\\,1,\\,1,\\,1).\n$$\n\nTest Suite. Your program must execute the following four test cases and report one scalar result for each:\n- Test $1$: Use $\\boldsymbol{\\alpha} = \\boldsymbol{\\alpha}^{\\mathrm{aniso}}$ and $Q = 8$ to build $\\mathcal{S}_{\\boldsymbol{\\alpha},Q}$ for $f_1$. Compute the maximum absolute error over $\\mathcal{E}$,\n$$\n\\max_{\\boldsymbol{y} \\in \\mathcal{E}} \\left| \\mathcal{S}_{\\boldsymbol{\\alpha},Q}[f_1](\\boldsymbol{y}) - f_1(\\boldsymbol{y}) \\right|.\n$$\n- Test $2$: Use $\\boldsymbol{\\alpha} = \\boldsymbol{\\alpha}^{\\mathrm{iso}}$ and $Q = 3$ to build $\\mathcal{S}_{\\boldsymbol{\\alpha},Q}$ for $f_1$. Compute the maximum absolute error over $\\mathcal{E}$.\n- Test $3$: Use $\\boldsymbol{\\alpha} = \\boldsymbol{\\alpha}^{\\mathrm{aniso}}$ and $Q = 6$ to build $\\mathcal{S}_{\\boldsymbol{\\alpha},Q}$ for $f_2$. Compute the maximum absolute error over $\\mathcal{E}$.\n- Test $4$: Use $\\boldsymbol{\\alpha} = \\boldsymbol{\\alpha}^{\\mathrm{aniso}}$ and $Q = 8$ to build $\\mathcal{S}_{\\boldsymbol{\\alpha},Q}$ for $f_1$. Evaluate the absolute error at the single point $\\boldsymbol{0} = (0,0,0,0,0,0,0,0,0,0)$,\n$$\n\\left| \\mathcal{S}_{\\boldsymbol{\\alpha},Q}[f_1](\\boldsymbol{0}) - f_1(\\boldsymbol{0}) \\right|.\n$$\n\nFinal Output Format. Your program should produce a single line of output containing the four results in order as a comma-separated list enclosed in square brackets, for example\n$$\n[{\\tt r1},{\\tt r2},{\\tt r3},{\\tt r4}],\n$$\nwhere each ${\\tt rj}$ is a real number (a floating-point value).", "solution": "The user has provided a well-defined problem in numerical analysis, specifically concerning the approximation of high-dimensional functions using anisotropic sparse grids. The problem is scientifically grounded, internally consistent, and contains all necessary information for its resolution. I will therefore proceed with a complete solution.\n\nThe core task is to evaluate the sparse grid interpolant $\\mathcal{S}_{\\boldsymbol{\\alpha},Q}[f]$ at specified points $\\boldsymbol{y} \\in [0,1]^{10}$. The interpolant is defined using the combination technique formulation:\n$$\n\\mathcal{S}_{\\boldsymbol{\\alpha},Q}[f](\\boldsymbol{y}) = \\sum_{\\boldsymbol{\\ell} \\in \\mathcal{I}(\\boldsymbol{\\alpha}, Q)} c_{\\boldsymbol{\\ell}} \\, U_{\\boldsymbol{\\ell}}[f](\\boldsymbol{y})\n$$\nwhere $f$ is the target function, $\\boldsymbol{y}$ is an evaluation point, $\\mathcal{I}(\\boldsymbol{\\alpha}, Q)$ is a down-closed set of multi-indices, $c_{\\boldsymbol{\\ell}}$ are combination coefficients, and $U_{\\boldsymbol{\\ell}}[f](\\boldsymbol{y})$ is the value of a full tensor-product multilinear interpolant.\n\nThe solution is implemented via a series of modular steps for each test case:\n1.  **Generation of the Index Set**: The set $\\mathcal{I}(\\boldsymbol{\\alpha}, Q) = \\left\\{ \\boldsymbol{\\ell} \\in \\mathbb{N}^d \\,:\\, \\ell_i \\ge 1, \\ \\sum_{i=1}^d \\alpha_i (\\ell_i - 1) \\le Q \\right\\}$ is constructed. To do this efficiently, we define level vectors $\\boldsymbol{k} = (\\ell_1-1, \\dots, \\ell_d-1)$ where each $k_i \\ge 0$. The condition becomes $\\sum_{i=1}^d \\alpha_i k_i \\le Q$. A recursive backtracking algorithm is employed to find all valid vectors $\\boldsymbol{k} \\in \\mathbb{N}_0^d$ satisfying this inequality. The corresponding multi-indices $\\boldsymbol{\\ell} = \\boldsymbol{k} + \\mathbf{1}$ form the set $\\mathcal{I}(\\boldsymbol{\\alpha}, Q)$. The set is stored in a hash set for efficient lookups.\n\n2.  **Computation of Combination Coefficients**: The coefficients $c_{\\boldsymbol{\\ell}}$ are determined by the consistency condition that ensures the operator is interpolatory. For a down-closed index set $\\mathcal{I}$, this condition implies a unique solution for $c_{\\boldsymbol{\\ell}}$ given by the inclusion-exclusion principle:\n    $$\n    c_{\\boldsymbol{\\ell}} = \\sum_{J \\subseteq \\{1, \\dots, d\\}} (-1)^{|J|} \\mathbb{I}(\\boldsymbol{\\ell} + \\mathbf{e}_J \\in \\mathcal{I}(\\boldsymbol{\\alpha}, Q))\n    $$\n    where $\\mathbf{e}_J = \\sum_{j \\in J} \\mathbf{e}_j$ and $\\mathbf{e}_j$ is the $j$-th standard basis vector. $\\mathbb{I}(\\cdot)$ is the indicator function, which is $1$ if the condition holds and $0$ otherwise. For each $\\boldsymbol{\\ell} \\in \\mathcal{I}$, we iterate through all $2^d$ subsets $J$ of the dimensions, check if the neighbor index $\\boldsymbol{\\ell}+\\mathbf{e}_J$ is in $\\mathcal{I}$, and sum the signed contributions. For $d=10$, $2^{10} = 1024$ checks per coefficient, which is computationally feasible.\n\n3.  **Evaluation of Full Tensor-Product Interpolants**: The term $U_{\\boldsymbol{\\ell}}[f](\\boldsymbol{y})$ denotes the evaluation of the multilinear interpolant on the full tensor grid $G_{\\boldsymbol{\\ell}} = X_{\\ell_1} \\times \\cdots \\times X_{\\ell_d}$ at point $\\boldsymbol{y}$. This is implemented recursively. The value of the $d$-dimensional interpolant is obtained by performing a one-dimensional linear interpolation in the first dimension, where the values at the two required grid points are themselves computed by $(d-1)$-dimensional interpolation in the remaining variables. This process bottoms out after $d$ steps, requiring function evaluations at the $2^d$ corner points of the hyper-rectangular cell in $G_{\\boldsymbol{\\ell}}$ that encloses $\\boldsymbol{y}$.\n\n4.  **Optimization via Memoization**: A naive implementation would be computationally prohibitive due to redundant calculations. The evaluation of $U_{\\boldsymbol{\\ell}}[f](\\boldsymbol{y})$ for different $\\boldsymbol{\\ell}$ will repeatedly require values of $f$ at the same grid points. To eliminate this redundancy, all function evaluations $f(\\boldsymbol{x})$ are memoized (cached) in a dictionary. When a value for $f$ at a specific grid point $\\boldsymbol{x}$ is needed, the cache is checked first, and the function is evaluated only if the value has not been previously computed.\n\n5.  **Assembly and Error Calculation**: For each test case and each evaluation point $\\boldsymbol{y}$, the final approximation is assembled by summing the contributions $c_{\\boldsymbol{\\ell}} U_{\\boldsymbol{\\ell}}[f](\\boldsymbol{y})$ over all $\\boldsymbol{\\ell} \\in \\mathcal{I}(\\boldsymbol{\\alpha}, Q)$. The absolute error is then computed as $|\\mathcal{S}_{\\boldsymbol{\\alpha},Q}[f](\\boldsymbol{y}) - f(\\boldsymbol{y})|$. The maximum of these errors over the evaluation set $\\mathcal{E}$ is reported as required.\n\nFor Test case $4$, the evaluation point is $\\boldsymbol{y} = \\boldsymbol{0} = (0,\\dots,0)$. The point $\\boldsymbol{0}$ is a member of every grid $G_{\\boldsymbol{\\ell}}$ since the one-dimensional grids $X_{\\ell_i}$ always contain $0$. Thesparse grid construction guarantees interpolation at all points in the sparse grid union, $\\mathcal{H}_{\\boldsymbol{\\alpha},Q} = \\bigcup_{\\boldsymbol{\\ell} \\in \\mathcal{I}} G_{\\boldsymbol{\\ell}}$. Since $\\boldsymbol{0} \\in \\mathcal{H}_{\\boldsymbol{\\alpha},Q}$, we must have $\\mathcal{S}_{\\boldsymbol{\\alpha},Q}[f_1](\\boldsymbol{0}) = f_1(\\boldsymbol{0})$, which implies the absolute error is exactly $0$. This serves as an analytical check on the correctness of the implementation.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom itertools import combinations\nimport math\n\ndef solve():\n    \"\"\"\n    Implements and evaluates an anisotropic sparse grid interpolant.\n    \"\"\"\n    d = 10\n\n    # Define target functions\n    def f1(x_tuple):\n        x = np.array(x_tuple)\n        term1 = np.exp(x[0])\n        term2 = np.sin(2 * np.pi * x[1])\n        term3 = 0.1 * np.sum([0.5**(i + 3) * x[i + 2]**2 for i in range(8)])\n        return term1 + term2 + term3\n\n    def f2(x_tuple):\n        x = np.array(x_tuple)\n        term1 = np.log(1 + 5 * x[0])\n        term2 = np.sqrt(1 + x[1])\n        term3 = 0.01 * np.sum(x[2:])\n        return term1 + term2 + term3\n\n    # Define evaluation points\n    evaluation_set = [\n        (0.13, 0.77, 0.50, 0.20, 0.80, 0.33, 0.66, 0.10, 0.90, 0.42),\n        (0.31, 0.62, 0.25, 0.75, 0.40, 0.60, 0.20, 0.80, 0.35, 0.65),\n        (0.73, 0.27, 0.15, 0.85, 0.55, 0.45, 0.05, 0.95, 0.22, 0.78),\n        (0.50, 0.50, 0.10, 0.90, 0.30, 0.70, 0.25, 0.75, 0.40, 0.60),\n        (0.21, 0.84, 0.63, 0.37, 0.12, 0.88, 0.47, 0.53, 0.19, 0.81),\n    ]\n\n    # Define anisotropy weights\n    alpha_aniso = (1, 1, 4, 4, 4, 4, 4, 4, 4, 4)\n    alpha_iso = (1, 1, 1, 1, 1, 1, 1, 1, 1, 1)\n\n    # Define test cases\n    test_cases = [\n        {'f': f1, 'alpha': alpha_aniso, 'Q': 8, 'eval_points': evaluation_set},\n        {'f': f1, 'alpha': alpha_iso,   'Q': 3, 'eval_points': evaluation_set},\n        {'f': f2, 'alpha': alpha_aniso, 'Q': 6, 'eval_points': evaluation_set},\n        {'f': f1, 'alpha': alpha_aniso, 'Q': 8, 'eval_points': [tuple([0.0]*d)]},\n    ]\n\n    memo_indices = {}\n    memo_coeffs = {}\n\n    def generate_indices(alpha, Q):\n        indices = set()\n        k_levels = []\n\n        def find_k(dim_idx, current_sum):\n            if dim_idx == d:\n                indices.add(tuple(k + 1 for k in k_levels))\n                return\n\n            max_k = (Q - current_sum) // alpha[dim_idx]\n            for ki in range(max_k + 1):\n                k_levels.append(ki)\n                find_k(dim_idx + 1, current_sum + alpha[dim_idx] * ki)\n                k_levels.pop()\n        \n        find_k(0, 0)\n        return indices\n\n    def calculate_coeffs(index_set):\n        coeffs = {}\n        e_vectors = np.identity(d, dtype=int)\n        for l_tuple in index_set:\n            l_vec = np.array(l_tuple)\n            c_l = 0\n            for size in range(d + 1):\n                for J in combinations(range(d), size):\n                    l_prime_vec = l_vec.copy()\n                    for j_idx in J:\n                        l_prime_vec[j_idx] += 1\n                    \n                    if tuple(l_prime_vec) in index_set:\n                        c_l += (-1)**size\n            coeffs[l_tuple] = c_l\n        return coeffs\n    \n    def get_interpolant_evaluator(f_func, y_tuple, l_tuple, memo_f):\n        def recursive_eval(dim, partial_point):\n            if dim == d:\n                point = tuple(partial_point)\n                if point not in memo_f:\n                    memo_f[point] = f_func(point)\n                return memo_f[point]\n\n            k = l_tuple[dim]\n            y_i = y_tuple[dim]\n\n            if k == 1:\n                # Grid is {0, 1}\n                weight = y_i\n                if abs(weight)  1e-15: return recursive_eval(dim + 1, partial_point + [0.0])\n                if abs(weight - 1.0)  1e-15: return recursive_eval(dim + 1, partial_point + [1.0])\n                \n                val_left = recursive_eval(dim + 1, partial_point + [0.0])\n                val_right = recursive_eval(dim + 1, partial_point + [1.0])\n                return (1.0 - weight) * val_left + weight * val_right\n            \n            m = 1  (k - 1)  # 2**(k-1)\n            \n            if abs(y_i - 1.0)  1e-15:\n                return recursive_eval(dim + 1, partial_point + [1.0])\n\n            pos = y_i * m\n            j = int(pos)\n            weight = pos - j\n\n            left_coord = j / m\n            \n            if weight  1e-15:\n                return recursive_eval(dim + 1, partial_point + [left_coord])\n\n            right_coord = (j + 1) / m\n            \n            val_left = recursive_eval(dim + 1, partial_point + [left_coord])\n            val_right = recursive_eval(dim + 1, partial_point + [right_coord])\n            return (1.0 - weight) * val_left + weight * val_right\n\n        return recursive_eval(0, [])\n\n    results = []\n    for case in test_cases:\n        f_func = case['f']\n        alpha = case['alpha']\n        Q = case['Q']\n        eval_points = case['eval_points']\n        \n        case_key = (alpha, Q)\n        \n        if case_key in memo_indices:\n            index_set = memo_indices[case_key]\n        else:\n            index_set = generate_indices(alpha, Q)\n            memo_indices[case_key] = index_set\n\n        if case_key in memo_coeffs:\n            coeffs = memo_coeffs[case_key]\n        else:\n            coeffs = calculate_coeffs(index_set)\n            memo_coeffs[case_key] = coeffs\n        \n        memo_f = {}\n        max_abs_error = 0.0\n\n        for y_tuple in eval_points:\n            approx_val = 0.0\n            for l_tuple in index_set:\n                c_l = coeffs[l_tuple]\n                if abs(c_l)  1e-15:\n                    continue\n                \n                u_l_f_y = get_interpolant_evaluator(f_func, y_tuple, l_tuple, memo_f)\n                approx_val += c_l * u_l_f_y\n            \n            true_val = f_func(y_tuple)\n            abs_error = abs(approx_val - true_val)\n            \n            if abs_error > max_abs_error:\n                max_abs_error = abs_error\n        \n        results.append(max_abs_error)\n\n    print(f\"[{','.join(f'{r:.12f}' for r in results)}]\")\n\nsolve()\n```", "id": "2432646"}, {"introduction": "The most sophisticated sparse grid methods build the grid dynamically based on the function's behavior. This practice guides you through implementing an adaptive algorithm that uses hierarchical surpluses as error indicators to refine the grid precisely where it is needed most. This powerful technique [@problem_id:2432623] is particularly effective for functions with localized features, such as sharp gradients or kinks, and represents the cutting edge of sparse grid methods.", "problem": "You are to design and implement an adaptive sparse grid interpolant based on the Smolyak construction with a hierarchical, piecewise-linear (hat) basis on the hypercube domain $[0,1]^d$. The algorithm must refine the grid adaptively using the magnitude of the hierarchical surplus coefficients as the refinement indicator. Your implementation must be a complete, runnable program that computes specified quantitative outputs for a defined test suite.\n\nThe required algorithmic elements must be derived from the following foundational base:\n- Core definitions of hierarchical bases in one dimension: For level $l \\in \\mathbb{N}$ and odd index $i \\in \\{1,3,\\dots,2^l - 1\\}$, define the 1-dimensional node $x_{l,i} = i / 2^l$ and the hat basis function\n$$\n\\varphi_{l,i}(x) = \\max\\left(1 - 2^l \\left| x - \\frac{i}{2^l} \\right|, 0\\right).\n$$\n- Multidimensional tensor basis: For $d \\in \\mathbb{N}$, multi-level $\\boldsymbol{l} = (l_1,\\dots,l_d)$, and multi-index $\\boldsymbol{i} = (i_1,\\dots,i_d)$ with each $i_j \\in \\{1,3,\\dots,2^{l_j}-1\\}$, the node is $\\boldsymbol{x}_{\\boldsymbol{l},\\boldsymbol{i}} = (i_1 2^{-l_1},\\dots,i_d 2^{-l_d})$ and the basis function factorizes as\n$$\n\\Phi_{\\boldsymbol{l},\\boldsymbol{i}}(\\boldsymbol{x}) = \\prod_{j=1}^d \\varphi_{l_j,i_j}(x_j).\n$$\n- Hierarchical interpolation: Given a set $\\mathcal{A}$ of multi-indices $(\\boldsymbol{l},\\boldsymbol{i})$, the interpolant is\n$$\n\\mathcal{I} f(\\boldsymbol{x}) = \\sum_{(\\boldsymbol{l},\\boldsymbol{i}) \\in \\mathcal{A}} \\alpha_{\\boldsymbol{l},\\boldsymbol{i}} \\, \\Phi_{\\boldsymbol{l},\\boldsymbol{i}}(\\boldsymbol{x}),\n$$\nwhere the hierarchical surplus coefficients are defined recursively by\n$$\n\\alpha_{\\boldsymbol{l},\\boldsymbol{i}} = f\\!\\left(\\boldsymbol{x}_{\\boldsymbol{l},\\boldsymbol{i}}\\right) - \\mathcal{I}_{\\text{prev}} f\\!\\left(\\boldsymbol{x}_{\\boldsymbol{l},\\boldsymbol{i}}\\right),\n$$\nwith $\\mathcal{I}_{\\text{prev}}$ denoting the interpolant formed by all previously added basis functions corresponding to strictly coarser nodes in a downward-closed sense.\n\nYour adaptive algorithm must:\n1. Initialize with the single interior node at level $\\boldsymbol{l} = (1,\\dots,1)$ and index $\\boldsymbol{i} = (1,\\dots,1)$, compute its hierarchical surplus, and insert it into an adaptive queue keyed by the absolute surplus magnitude.\n2. Iteratively select the node in the queue with the largest absolute surplus and refine it by adding its admissible children. A child along coordinate $j$ increases $l_j$ by $1$ and replaces $i_j$ by one of $\\{2 i_j - 1, 2 i_j + 1\\}$ while keeping all other coordinates unchanged. For each newly added child, compute its hierarchical surplus using the current interpolant, and insert the child into the queue.\n3. Terminate when either the maximum absolute surplus over all currently available nodes is below a given tolerance $ \\tau  0$, or a specified maximum number of grid points $N_{\\max}$ has been reached.\n\nUse this algorithm to construct interpolants for the following test suite and compute the maximum absolute interpolation error on a specified validation grid for each case. All angles, where applicable, must be interpreted in radians.\n\nFor each test, define:\n- The dimension $d$.\n- The target function $f : [0,1]^d \\to \\mathbb{R}$.\n- The tolerance $\\tau$ and cap $N_{\\max}$.\n- A validation grid formed as the Cartesian product of $m$ equidistant points per dimension in the open interval $(0,1)$, specifically at $x_k = \\frac{k}{m+1}$ for $k = 1,2,\\dots,m$.\n- The final outputs to be computed: the number of grid points actually used (an integer) and the maximum absolute error on the validation grid (a float), i.e.,\n$$\nE_{\\max} = \\max_{\\boldsymbol{x} \\in \\mathcal{G}_m} \\left| f(\\boldsymbol{x}) - \\mathcal{I} f(\\boldsymbol{x}) \\right|.\n$$\n\nTest suite:\n- Case A (happy path, smooth separable in three dimensions):\n  - $d = 3$.\n  - $f(\\boldsymbol{x}) = \\exp\\!\\left(0.5\\,x_1 - 0.3\\,x_2 + 0.2\\,x_3\\right)$.\n  - $\\tau = 10^{-3}$, $N_{\\max} = 500$.\n  - Validation grid parameter $m = 9$.\n- Case B (anisotropic localized Gaussian bump in two dimensions):\n  - $d = 2$.\n  - $f(\\boldsymbol{x}) = \\exp\\!\\left(-40 \\sum_{j=1}^2 (x_j - c_j)^2\\right)$ with $\\boldsymbol{c} = (0.2, 0.8)$.\n  - $\\tau = 5 \\times 10^{-4}$, $N_{\\max} = 600$.\n  - Validation grid parameter $m = 25$.\n- Case C (one-dimensional non-smooth absolute value kink):\n  - $d = 1$.\n  - $f(x) = |x - 0.3|$.\n  - $\\tau = 2 \\times 10^{-4}$, $N_{\\max} = 300$.\n  - Validation grid parameter $m = 200$.\n- Case D (moderate four-dimensional smooth function with mild oscillation):\n  - $d = 4$.\n  - $f(\\boldsymbol{x}) = \\prod_{j=1}^4 \\left(1 + 0.1\\,x_j\\right) + 0.01 \\sum_{j=1}^4 \\sin(2\\pi x_j)$, with angles in radians.\n  - $\\tau = 2 \\times 10^{-3}$, $N_{\\max} = 400$.\n  - Validation grid parameter $m = 7$.\n\nImplementation constraints and final output format:\n- Your program must be self-contained and must not read any input. It must implement the adaptive sparse grid interpolation algorithm described above and compute, for each test case, the pair $(N, E_{\\max})$, where $N$ is the number of grid points actually used at termination.\n- Your program should produce a single line of output containing all results in a flat, comma-separated Python list as\n  [N_A,E_A,N_B,E_B,N_C,E_C,N_D,E_D]\nwhere $N_\\cdot$ are integers and $E_\\cdot$ are floats. The error values must be reported as decimal numbers (not fractions), and angles, where present, must be interpreted in radians.", "solution": "The problem statement has been validated and is deemed sound. It is scientifically grounded, well-posed, and objective. It presents a clear task: to design and implement an adaptive sparse grid interpolation algorithm based on the Smolyak construction with a piecewise-linear hierarchical basis. The definitions, algorithmic steps, and test cases are specified with sufficient rigor to permit a unique and verifiable solution.\n\nThe objective is to construct an adaptive sparse grid interpolant for a function $f: [0,1]^d \\to \\mathbb{R}$. The domain is the $d$-dimensional hypercube $[0,1]^d$. The interpolation scheme is built upon a hierarchical basis of piecewise-linear \"hat\" functions.\n\nIn one dimension, for a given level $l \\in \\mathbb{N} = \\{1, 2, 3, \\dots\\}$, a set of nodal points is defined at $x_{l,i} = i / 2^l$ for odd indices $i \\in \\{1, 3, \\dots, 2^l - 1\\}$. Associated with each such node is a hat basis function:\n$$\n\\varphi_{l,i}(x) = \\max\\left(1 - 2^l \\left| x - x_{l,i} \\right|, 0\\right)\n$$\nThis basis is defined only on the interior of the interval $(0,1)$, which implies the resulting interpolant will be zero on the boundary.\n\nFor a $d$-dimensional problem, the basis functions are formed by a tensor product construction. A point in the hierarchical grid is identified by a multi-level $\\boldsymbol{l} = (l_1, \\dots, l_d) \\in \\mathbb{N}^d$ and a multi-index $\\boldsymbol{i} = (i_1, \\dots, i_d)$, where each component $i_j$ is an odd integer satisfying $1 \\le i_j \\le 2^{l_j}-1$. The corresponding grid node is $\\boldsymbol{x}_{\\boldsymbol{l},\\boldsymbol{i}} = (x_{l_1,i_1}, \\dots, x_{l_d,i_d})$, and the multidimensional basis function is:\n$$\n\\Phi_{\\boldsymbol{l},\\boldsymbol{i}}(\\boldsymbol{x}) = \\prod_{j=1}^d \\varphi_{l_j,i_j}(x_j)\n$$\n\nThe sparse grid interpolant $\\mathcal{I}f(\\boldsymbol{x})$ is a linear combination of these basis functions for a selected set $\\mathcal{A}$ of multi-indices $(\\boldsymbol{l}, \\boldsymbol{i})$:\n$$\n\\mathcal{I} f(\\boldsymbol{x}) = \\sum_{(\\boldsymbol{l},\\boldsymbol{i}) \\in \\mathcal{A}} \\alpha_{\\boldsymbol{l},\\boldsymbol{i}} \\, \\Phi_{\\boldsymbol{l},\\boldsymbol{i}}(\\boldsymbol{x})\n$$\nThe coefficients $\\alpha_{\\boldsymbol{l},\\boldsymbol{i}}$ are the hierarchical surpluses, defined recursively. For a new point $\\boldsymbol{x}_{\\boldsymbol{l},\\boldsymbol{i}}$ being added to the grid, its surplus is the difference between the true function value and the value of the interpolant constructed from all previously included, coarser points:\n$$\n\\alpha_{\\boldsymbol{l},\\boldsymbol{i}} = f(\\boldsymbol{x}_{\\boldsymbol{l},\\boldsymbol{i}}) - \\sum_{(\\boldsymbol{l}',\\boldsymbol{i}') \\in \\mathcal{A}_{\\text{prev}}} \\alpha_{\\boldsymbol{l}',\\boldsymbol{i}'} \\, \\Phi_{\\boldsymbol{l}',\\boldsymbol{i}'}(\\boldsymbol{x}_{\\boldsymbol{l},\\boldsymbol{i}})\n$$\n\nThe core of the specified task is the adaptive nature of the algorithm. Adaptivity is driven by the magnitude of the hierarchical surpluses, which serve as an error indicator. The algorithm proceeds as follows:\n\n$1$. Initialization: The process begins with a single interior node at the coarsest level, specified by the multi-level $\\boldsymbol{l} = (1, \\dots, 1)$ and multi-index $\\boldsymbol{i} = (1, \\dots, 1)$. This corresponds to the point $\\boldsymbol{x} = (0.5, \\dots, 0.5)$. Its surplus is simply $\\alpha_{\\boldsymbol{l},\\boldsymbol{i}} = f(\\boldsymbol{x}_{\\boldsymbol{l},\\boldsymbol{i}})$, as the initial interpolant is zero. This point, along with its surplus, is added to the grid and to a priority queue, which is ordered by the absolute magnitude of the surpluses.\n\n$2$. Iterative Refinement: The algorithm enters a loop that continues until a termination condition is met. In each iteration:\n   a. Selection: The grid point $(\\boldsymbol{l}_{\\text{p}}, \\boldsymbol{i}_{\\text{p}})$ with the largest absolute surplus $|\\alpha|$ is selected and removed from the priority queue. This is the parent point for refinement.\n   b. Refinement: The grid is refined by generating the admissible children of the parent point. For each dimension $j \\in \\{1, \\dots, d\\}$, two children are generated. A child's multi-level is derived from the parent's by incrementing the level in dimension $j$, i.e., $l'_j = l_j+1$ and $l'_k = l_k$ for $k \\neq j$. The corresponding index $i'_j$ for the new level $l'_j$ is given by $\\{2i_j-1, 2i_j+1\\}$, while $i'_k=i_k$ for $k \\neq j$.\n   c. Update: For each newly generated child point, its hierarchical surplus is calculated using the formula above, where the sum is over all points currently in the grid. The new point and its surplus are added to the grid data structure and inserted into the priority queue.\n\n$3$. Termination: The iterative process halts if either of two conditions is met:\n   a. The maximum absolute surplus in the priority queue falls below a specified tolerance $\\tau$.\n   b. The total number of points in the grid reaches a defined maximum, $N_{\\max}$.\n\nUpon termination, the algorithm has constructed a sparse grid and the corresponding interpolant $\\mathcal{I}f(\\boldsymbol{x})$. The final step is to assess its accuracy. This is done by calculating the maximum absolute error $E_{\\max}$ on a pre-defined validation grid $\\mathcal{G}_m$:\n$$\nE_{\\max} = \\max_{\\boldsymbol{x} \\in \\mathcal{G}_m} \\left| f(\\boldsymbol{x}) - \\mathcal{I} f(\\boldsymbol{x}) \\right|\n$$\nThe validation grid $\\mathcal{G}_m$ is the Cartesian product of $m$ equidistant points in $(0,1)$ for each dimension.\n\nThe implementation will consist of a main driver function that iterates through the specified test cases. For each case, a dedicated solver function will execute the adaptive algorithm. Key data structures will include a dictionary to store the grid points and their associated data (surpluses, coordinates), and a min-heap from Python's `heapq` module to serve as the priority queue. Helper functions will be implemented to evaluate the 1D and multidimensional basis functions, and the overall interpolant. The final output will be the number of points $N$ used in the grid and the computed maximum error $E_{\\max}$ for each test case.", "answer": "```python\nimport numpy as np\nimport heapq\nimport itertools\nimport math\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and run the adaptive sparse grid solver for each.\n    \"\"\"\n\n    # --- Test Case Definitions ---\n    # Case A: Smooth separable function\n    def f_A(x):\n        return np.exp(0.5 * x[0] - 0.3 * x[1] + 0.2 * x[2])\n\n    # Case B: Anisotropic localized Gaussian bump\n    def f_B(x):\n        c = np.array([0.2, 0.8])\n        return np.exp(-40.0 * np.sum((x - c)**2))\n\n    # Case C: Non-smooth absolute value kink\n    def f_C(x):\n        return np.abs(x[0] - 0.3)\n\n    # Case D: Smooth function with mild oscillation\n    def f_D(x):\n        prod_term = np.prod(1.0 + 0.1 * x)\n        sin_term = 0.01 * np.sum(np.sin(2.0 * np.pi * x))\n        return prod_term + sin_term\n        \n    test_cases = [\n        {'d': 3, 'f': f_A, 'tau': 1e-3, 'n_max': 500, 'm': 9},\n        {'d': 2, 'f': f_B, 'tau': 5e-4, 'n_max': 600, 'm': 25},\n        {'d': 1, 'f': f_C, 'tau': 2e-4, 'n_max': 300, 'm': 200},\n        {'d': 4, 'f': f_D, 'tau': 2e-3, 'n_max': 400, 'm': 7}\n    ]\n\n    results = []\n    for case in test_cases:\n        N, E_max = solve_case(case['d'], case['f'], case['tau'], case['n_max'], case['m'])\n        results.extend([N, E_max])\n\n    print(f\"[{','.join(map(str, results))}]\")\n\n\ndef solve_case(d, f, tau, n_max, m):\n    \"\"\"\n    Solves a single adaptive sparse grid interpolation problem.\n    \"\"\"\n    \n    # Memoization caches for basis function calculations\n    phi_1d_cache = {}\n    phi_multi_d_cache = {}\n\n    def phi_1d(x, l, i):\n        \"\"\"Evaluates the 1D hierarchical hat basis function.\"\"\"\n        cache_key = (x, l, i)\n        if cache_key in phi_1d_cache:\n            return phi_1d_cache[cache_key]\n\n        # Using 1  l which is equivalent to 2**l for integers\n        val = max(0.0, 1.0 - abs((1  l) * x - i))\n        phi_1d_cache[cache_key] = val\n        return val\n\n    def phi_multi_d(x_vec, l_vec, i_vec):\n        \"\"\"Evaluates the multidimensional tensor-product basis function.\"\"\"\n        cache_key = (tuple(x_vec), l_vec, i_vec)\n        if cache_key in phi_multi_d_cache:\n            return phi_multi_d_cache[cache_key]\n        \n        prod = 1.0\n        for j in range(d):\n            prod *= phi_1d(x_vec[j], l_vec[j], i_vec[j])\n        phi_multi_d_cache[cache_key] = prod\n        return prod\n\n    def evaluate_interpolant(x_vec, grid_points):\n        \"\"\"Evaluates the sparse grid interpolant at a point x_vec.\"\"\"\n        total = 0.0\n        for (l_vec, i_vec), data in grid_points.items():\n            alpha = data['surplus']\n            basis_val = phi_multi_d(x_vec, l_vec, i_vec)\n            total += alpha * basis_val\n        return total\n\n    grid_points = {}\n    priority_queue = []\n\n    # 1. Initialize with the first point\n    l0 = tuple([1] * d)\n    i0 = tuple([1] * d)\n    \n    x0 = tuple(i / (1  l) for l, i in zip(l0, i0))\n    f_val = f(np.array(x0))\n    alpha0 = f_val  # I_prev is 0\n    \n    grid_points[(l0, i0)] = {'surplus': alpha0, 'coords': x0}\n    heapq.heappush(priority_queue, (-abs(alpha0), l0, i0))\n\n    # 2. Main adaptive loop\n    while priority_queue:\n        if len(grid_points) >= n_max:\n            break\n        \n        neg_abs_alpha_max, _, _ = priority_queue[0]\n        if -neg_abs_alpha_max  tau:\n            break\n\n        # Pop parent point with largest surplus\n        _, l_parent, i_parent = heapq.heappop(priority_queue)\n\n        # Generate and add children\n        for j in range(d):  # Dimension to refine\n            l_child_list = list(l_parent)\n            l_child_list[j] += 1\n            l_child = tuple(l_child_list)\n            \n            i_child_val_1 = 2 * i_parent[j] - 1\n            i_child_val_2 = 2 * i_parent[j] + 1\n            \n            for i_child_val in [i_child_val_1, i_child_val_2]:\n                if len(grid_points) >= n_max:\n                    break\n\n                i_child_list = list(i_parent)\n                i_child_list[j] = i_child_val\n                i_child = tuple(i_child_list)\n\n                if (l_child, i_child) in grid_points:\n                    continue\n                \n                # Calculate surplus for the new child point\n                x_child = tuple(i / (1  l) for l, i in zip(l_child, i_child))\n                f_val_child = f(np.array(x_child))\n                \n                # Clear evaluation caches for new point evaluation\n                phi_1d_cache.clear()\n                phi_multi_d_cache.clear()\n                \n                I_prev_at_child = evaluate_interpolant(x_child, grid_points)\n                alpha_child = f_val_child - I_prev_at_child\n\n                # Add child to grid and priority queue\n                grid_points[(l_child, i_child)] = {'surplus': alpha_child, 'coords': x_child}\n                heapq.heappush(priority_queue, (-abs(alpha_child), l_child, i_child))\n            \n            if len(grid_points) >= n_max:\n                break\n    \n    # Final number of grid points used\n    N = len(grid_points)\n\n    # 3. Error calculation on validation grid\n    axis_pts = (np.arange(1, m + 1, dtype=float)) / (m + 1.0)\n    validation_grid = itertools.product(*([axis_pts] * d))\n    \n    max_error = 0.0\n    for x_val_tuple in validation_grid:\n        x_val = np.array(x_val_tuple)\n        f_true = f(x_val)\n        \n        phi_1d_cache.clear()\n        phi_multi_d_cache.clear()\n        \n        f_interp = evaluate_interpolant(x_val, grid_points)\n        max_error = max(max_error, abs(f_true - f_interp))\n            \n    return N, max_error\n\nif __name__ == \"__main__\":\n    solve()\n\n```", "id": "2432623"}]}