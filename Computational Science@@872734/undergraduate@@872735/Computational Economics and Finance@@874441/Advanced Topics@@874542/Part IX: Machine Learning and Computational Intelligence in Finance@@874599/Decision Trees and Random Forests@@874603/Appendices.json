{"hands_on_practices": [{"introduction": "To truly understand decision trees, we must start with their most fundamental action: the split. This first exercise demystifies the splitting process of a regression tree by framing it as a familiar problem in econometricsâ€”finding a structural break in a time series. By implementing a single-split tree that minimizes the Sum of Squared Errors ($SSE$), you will see how the algorithm partitions data to find the most significant change point, building a deep intuition for the tree's core optimization objective [@problem_id:2386904].", "problem": "You are given a univariate financial time series of decimal log-returns $\\{y_t\\}_{t=0}^{T-1}$, where $T \\in \\mathbb{N}$ and each $y_t \\in \\mathbb{R}$. Consider the piecewise-constant two-regime model with a single structural break at an integer split location $\\tau$, where the left segment indices are $\\{0,1,\\dots,\\tau-1\\}$ and the right segment indices are $\\{\\tau,\\dots,T-1\\}$. For any admissible split $\\tau$, define the left-segment mean $\\mu_L(\\tau)$ and the right-segment mean $\\mu_R(\\tau)$ by\n$$\n\\mu_L(\\tau) \\equiv \\frac{1}{\\tau}\\sum_{t=0}^{\\tau-1} y_t,\\qquad\n\\mu_R(\\tau) \\equiv \\frac{1}{T-\\tau}\\sum_{t=\\tau}^{T-1} y_t,\n$$\nand the total squared error\n$$\n\\mathrm{SSE}(\\tau) \\equiv \\sum_{t=0}^{\\tau-1}\\bigl(y_t - \\mu_L(\\tau)\\bigr)^2 \\;+\\; \\sum_{t=\\tau}^{T-1}\\bigl(y_t - \\mu_R(\\tau)\\bigr)^2.\n$$\nGiven a minimum leaf size $m \\in \\mathbb{N}$, a split $\\tau$ is admissible if it satisfies $m \\le \\tau \\le T - m$. Your task is, for each test case, to compute an integer $\\tau^\\star$ that minimizes $\\mathrm{SSE}(\\tau)$ over all admissible splits. If there are multiple minimizers (including numerical ties), select the smallest such $\\tau^\\star$. If no admissible split exists (i.e., if $T  2m$), define the output for that test case to be $-1$.\n\nAll values of $y_t$ are decimal log-returns and must be treated as dimensionless real numbers (no units). Do not express any result as a percentage; all numerical quantities must be decimals or integers.\n\nTest suite. For each test case, you are given the pair $(\\{y_t\\}_{t=0}^{T-1}, m)$:\n\n- Test 1 (clear mean shift): $T = 10$, $y = [0.000, 0.000, 0.000, 0.000, 0.000, 0.020, 0.020, 0.020, 0.020, 0.020]$, $m=1$.\n- Test 2 (constant series, tie-breaking by smallest split): $T = 6$, $y = [0.010, 0.010, 0.010, 0.010, 0.010, 0.010]$, $m=2$.\n- Test 3 (single extreme observation near the end with minimum leaf size): $T = 8$, $y = [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.100]$, $m=2$.\n- Test 4 (boundary case with exactly one admissible split): $T = 4$, $y = [0.010, 0.020, 0.030, 0.040]$, $m=2$.\n- Test 5 (drift followed by a higher constant mean): $T = 10$, $y = [0.000, 0.010, 0.020, 0.030, 0.040, 0.060, 0.060, 0.060, 0.060, 0.060]$, $m=1$.\n\nRequired final output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the tests above. For example, if the five answers are $\\tau_1^\\star,\\dots,\\tau_5^\\star$, print\n[$\\tau_1^\\star,\\tau_2^\\star,\\tau_3^\\star,\\tau_4^\\star,\\tau_5^\\star$].", "solution": "The posed problem is subjected to validation before a solution is attempted.\n\n### Step 1: Extract Givens\n- **Time series**: A univariate series $\\{y_t\\}_{t=0}^{T-1}$, where $T \\in \\mathbb{N}$ and $y_t \\in \\mathbb{R}$.\n- **Model**: A piecewise-constant two-regime model with a single structural break at an integer split location $\\tau$.\n- **Segmentation**:\n    - Left segment indices: $\\{0, 1, \\dots, \\tau-1\\}$.\n    - Right segment indices: $\\{\\tau, \\dots, T-1\\}$.\n- **Segment Means**:\n    - Left-segment mean: $\\mu_L(\\tau) \\equiv \\frac{1}{\\tau}\\sum_{t=0}^{\\tau-1} y_t$.\n    - Right-segment mean: $\\mu_R(\\tau) \\equiv \\frac{1}{T-\\tau}\\sum_{t=\\tau}^{T-1} y_t$.\n- **Objective Function**: Total Squared Error, $\\mathrm{SSE}(\\tau) \\equiv \\sum_{t=0}^{\\tau-1}\\bigl(y_t - \\mu_L(\\tau)\\bigr)^2 \\;+\\; \\sum_{t=\\tau}^{T-1}\\bigl(y_t - \\mu_R(\\tau)\\bigr)^2$.\n- **Constraints**:\n    - Minimum leaf size: $m \\in \\mathbb{N}$.\n    - Admissible split: A split $\\tau$ is admissible if it satisfies $m \\le \\tau \\le T - m$.\n- **Task**:\n    - Compute the integer $\\tau^\\star$ that minimizes $\\mathrm{SSE}(\\tau)$ over all admissible splits.\n    - Tie-breaking rule: If multiple minimizers exist, select the smallest such $\\tau^\\star$.\n    - No admissible split: If $T  2m$, the output is $-1$.\n- **Test Suite**: Five test cases are provided, each consisting of a pair $(\\{y_t\\}_{t=0}^{T-1}, m)$. The dollar signs in the data are considered decorative and the values are treated as decimal numbers.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed for validity.\n- **Scientifically Grounded**: The problem is a classic example of change-point detection, a fundamental topic in statistics, signal processing, and econometrics. The minimization of the Sum of Squared Errors (SSE) is equivalent to a maximum likelihood estimation for a break in the mean of a sequence of random variables, assuming they are otherwise independent and identically distributed Gaussian variables with a constant variance. This is a well-established and scientifically sound model.\n- **Well-Posed**: The problem is well-posed. The domain of optimization for $\\tau$ is the finite set of integers $\\{m, m+1, \\dots, T-m\\}$. The objective function $\\mathrm{SSE}(\\tau)$ is uniquely defined for each $\\tau$ in this domain. As we are minimizing a real-valued function over a finite set, a minimum is guaranteed to exist. The tie-breaking rule ensures a unique solution is selected. The condition for the non-existence of admissible splits ($T  2m$) is also clearly defined.\n- **Objective**: The problem is stated with precise mathematical definitions and without ambiguity. The goal is clear and the criteria for the solution are explicitly given.\n\n### Step 3: Verdict and Action\nThe problem is valid. It is a well-defined mathematical optimization problem grounded in standard statistical theory. A solution will be provided.\n\n### Solution Derivation\nThe objective is to find $\\tau^\\star = \\arg\\min_{m \\le \\tau \\le T-m} \\mathrm{SSE}(\\tau)$.\n\nA naive approach involves iterating through each admissible split $\\tau$, and for each $\\tau$, calculating the segment means and the SSE by summing over the data points. For a given $\\tau$, calculating $\\mu_L(\\tau)$ and $\\mu_R(\\tau)$ requires $O(\\tau)$ and $O(T-\\tau)$ operations, respectively. Calculating the two sums of squares similarly requires $O(\\tau)$ and $O(T-\\tau)$ operations. The total complexity for a single $\\tau$ is $O(T)$. Since there are $O(T)$ possible values for $\\tau$, the total complexity of this naive method is $O(T^2)$. This is inefficient for large $T$.\n\nA more efficient algorithm can be constructed by recognizing that the SSE can be calculated using a well-known identity for variance: for a set of numbers $\\{x_i\\}_{i=1}^n$, the sum of squared deviations from the mean is $\\sum_{i=1}^n (x_i - \\bar{x})^2 = \\sum_{i=1}^n x_i^2 - n\\bar{x}^2 = \\sum_{i=1}^n x_i^2 - \\frac{1}{n}(\\sum_{i=1}^n x_i)^2$.\n\nApplying this to our two segments:\nThe SSE for the left segment is:\n$$ \\mathrm{SSE}_L(\\tau) = \\sum_{t=0}^{\\tau-1} (y_t - \\mu_L(\\tau))^2 = \\left(\\sum_{t=0}^{\\tau-1} y_t^2\\right) - \\frac{1}{\\tau}\\left(\\sum_{t=0}^{\\tau-1} y_t\\right)^2 $$\nThe SSE for the right segment is:\n$$ \\mathrm{SSE}_R(\\tau) = \\sum_{t=\\tau}^{T-1} (y_t - \\mu_R(\\tau))^2 = \\left(\\sum_{t=\\tau}^{T-1} y_t^2\\right) - \\frac{1}{T-\\tau}\\left(\\sum_{t=\\tau}^{T-1} y_t\\right)^2 $$\nThe total SSE is $\\mathrm{SSE}(\\tau) = \\mathrm{SSE}_L(\\tau) + \\mathrm{SSE}_R(\\tau)$.\n\nTo compute these sums efficiently for all $\\tau$, we can pre-compute the prefix sums (cumulative sums) of the series $y_t$ and its square $y_t^2$. Let us define two auxiliary arrays, $C_1$ and $C_2$, both of size $T+1$:\n$$ C_1[k] = \\sum_{t=0}^{k-1} y_t \\quad \\text{for } k > 0, \\text{ with } C_1[0] = 0 $$\n$$ C_2[k] = \\sum_{t=0}^{k-1} y_t^2 \\quad \\text{for } k > 0, \\text{ with } C_2[0] = 0 $$\nThese prefix sum arrays can be computed in $O(T)$ time.\n\nUsing these arrays, the sums over any segment can be calculated in $O(1)$ time. For a split at $\\tau$:\n- Sum over left segment $[0, \\dots, \\tau-1]$:\n  - $\\sum_{t=0}^{\\tau-1} y_t = C_1[\\tau]$\n  - $\\sum_{t=0}^{\\tau-1} y_t^2 = C_2[\\tau]$\n- Sum over right segment $[\\tau, \\dots, T-1]$:\n  - $\\sum_{t=\\tau}^{T-1} y_t = \\sum_{t=0}^{T-1} y_t - \\sum_{t=0}^{\\tau-1} y_t = C_1[T] - C_1[\\tau]$\n  - $\\sum_{t=\\tau}^{T-1} y_t^2 = \\sum_{t=0}^{T-1} y_t^2 - \\sum_{t=0}^{\\tau-1} y_t^2 = C_2[T] - C_2[\\tau]$\n\nSubstituting these into the SSE formulae:\n$$ \\mathrm{SSE}_L(\\tau) = C_2[\\tau] - \\frac{(C_1[\\tau])^2}{\\tau} $$\n$$ \\mathrm{SSE}_R(\\tau) = (C_2[T] - C_2[\\tau]) - \\frac{(C_1[T] - C_1[\\tau])^2}{T-\\tau} $$\nThe total SSE for a given $\\tau$ can thus be computed in $O(1)$ time after the initial $O(T)$ pre-computation.\n\nThe overall algorithm is as follows:\n1.  First, check if any admissible split exists. If $T  2m$, no split $\\tau$ can satisfy both $\\tau \\ge m$ and $T-\\tau \\ge m$. In this case, the procedure terminates and returns $-1$.\n2.  Compute the prefix sum arrays $C_1$ and $C_2$ for the given time series $\\{y_t\\}$ and $\\{y_t^2\\}$. This takes $O(T)$ time.\n3.  Initialize a variable for the minimum SSE found, $\\mathrm{SSE}_{\\min}$, to a very large value (or infinity), and the best split, $\\tau^\\star$, to a default value (e.g., $-1$ or the first admissible split).\n4.  Iterate through all admissible splits $\\tau$ from $m$ to $T-m$.\n5.  For each $\\tau$, calculate $\\mathrm{SSE}(\\tau)$ in $O(1)$ time using the pre-computed prefix sums and the derived formulae.\n6.  If the calculated $\\mathrm{SSE}(\\tau)$ is strictly less than the current $\\mathrm{SSE}_{\\min}$, update $\\mathrm{SSE}_{\\min} = \\mathrm{SSE}(\\tau)$ and $\\tau^\\star = \\tau$. The use of strict inequality (``) ensures that for ties in SSE, the first (and thus smallest) value of $\\tau$ encountered is retained, satisfying the problem's tie-breaking rule.\n7.  After the loop completes, $\\tau^\\star$ will hold the optimal split point. Return $\\tau^\\star$.\n\nThis algorithm has a total time complexity of $O(T)$, dominated by the prefix sum computation, which is a significant improvement over the naive $O(T^2)$ approach.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for the optimal structural break point in a time series.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test 1: clear mean shift\n        (np.array([0.000, 0.000, 0.000, 0.000, 0.000, 0.020, 0.020, 0.020, 0.020, 0.020]), 1),\n        # Test 2: constant series, tie-breaking by smallest split\n        (np.array([0.010, 0.010, 0.010, 0.010, 0.010, 0.010]), 2),\n        # Test 3: single extreme observation near the end with minimum leaf size\n        (np.array([0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.100]), 2),\n        # Test 4: boundary case with exactly one admissible split\n        (np.array([0.010, 0.020, 0.030, 0.040]), 2),\n        # Test 5: drift followed by a higher constant mean\n        (np.array([0.000, 0.010, 0.020, 0.030, 0.040, 0.060, 0.060, 0.060, 0.060, 0.060]), 1),\n    ]\n\n    results = []\n    \n    for y, m in test_cases:\n        T = len(y)\n        \n        # Step 1: Check for existence of an admissible split.\n        if T  2 * m:\n            results.append(-1)\n            continue\n            \n        # Step 2: Compute prefix sums for y and y^2.\n        # We prepend a 0 to make indexing C[k] = sum_{i=0}^{k-1} y_i natural.\n        y_sq = y**2\n        \n        # C1[k] = sum_{t=0}^{k-1} y_t\n        C1 = np.zeros(T + 1)\n        np.cumsum(y, out=C1[1:])\n\n        # C2[k] = sum_{t=0}^{k-1} y_t^2\n        C2 = np.zeros(T + 1)\n        np.cumsum(y_sq, out=C2[1:])\n        \n        min_sse = np.inf\n        best_tau = -1\n        \n        # Step 3: Iterate through all admissible splits.\n        # The range is m = tau = T - m.\n        for tau in range(m, T - m + 1):\n            # Calculate sums for the left segment [0, ..., tau-1]\n            # The length of the left segment is tau.\n            sum_L = C1[tau]\n            sum_sq_L = C2[tau]\n            \n            # Calculate sums for the right segment [tau, ..., T-1]\n            # The length of the right segment is T - tau.\n            sum_R = C1[T] - C1[tau]\n            sum_sq_R = C2[T] - C2[tau]\n            \n            # Calculate SSE for left and right segments\n            # SSE = sum(y_i^2) - (sum(y_i))^2 / n\n            sse_L = sum_sq_L - (sum_L**2) / tau\n            sse_R = sum_sq_R - (sum_R**2) / (T - tau)\n            \n            current_sse = sse_L + sse_R\n            \n            # Step 4: Update minimum SSE and best tau.\n            # Strict inequality handles the tie-breaking rule (smallest tau).\n            if current_sse  min_sse:\n                min_sse = current_sse\n                best_tau = tau\n        \n        results.append(best_tau)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2386904"}, {"introduction": "Standard decision trees use statistical measures like Gini impurity or entropy, but real-world financial and economic problems often have asymmetric costs. Misclassifying a high-revenue client is not the same as misclassifying a low-revenue one. This practice challenges you to go beyond off-the-shelf criteria and embed economic context directly into the model by implementing a custom loss function for the tree's splitting decisions [@problem_id:2386905]. You will learn to build a tree that optimizes for a specific business objective, a crucial skill in computational finance.", "problem": "You are given a binary classification task grounded in computational economics and finance, where each observation represents a customer, each label indicates whether the customer is high-value, and each observation carries a revenue weight. A split at a node of a binary decision tree is formed by selecting a threshold on a single real-valued feature and assigning a constant class prediction in each resulting leaf to minimize a custom economic loss. For each observation index $i \\in \\{1,\\dots,n\\}$, let the scalar feature be $x_i \\in \\mathbb{R}$, the binary label be $y_i \\in \\{0,1\\}$, and the nonnegative revenue weight be $r_i \\in \\mathbb{R}_{\\ge 0}$. Let $\\alpha \\in \\mathbb{R}_{0}$ and $\\beta \\in \\mathbb{R}_{0}$ be given misclassification cost parameters. For any threshold $\\tau \\in \\mathbb{R}$, define the left region $L(\\tau) = \\{i : x_i  \\tau\\}$ and the right region $R(\\tau) = \\{i : x_i \\ge \\tau\\}$. In a region $S \\in \\{L(\\tau), R(\\tau)\\}$, if the constant prediction is $c \\in \\{0,1\\}$, the misclassification loss contributed by observation $i \\in S$ is $0$ if $c = y_i$, equals $\\alpha \\cdot r_i$ if $y_i = 1$ and $c = 0$, and equals $\\beta$ if $y_i = 0$ and $c = 1$. The leaf prediction $c_S(\\tau)$ must minimize the total loss within $S$. If there is a tie in the leaf between predicting $0$ and $1$, select $c_S(\\tau) = 0$. The total split loss is the sum of the two leaf losses, one from $L(\\tau)$ and one from $R(\\tau)$. Among a finite candidate set of thresholds $\\mathcal{T} \\subset \\mathbb{R}$, select $\\tau^\\star \\in \\mathcal{T}$ that minimizes the total split loss. If multiple thresholds achieve the same minimum total loss, select the numerically smallest $\\tau^\\star$. For each test case below, you must determine $\\tau^\\star$, the corresponding minimized total loss, and the two leaf predictions $c_{L}(\\tau^\\star)$ and $c_{R}(\\tau^\\star)$.\n\nTest Suite. For each case, you are given arrays $x$, $y$, $r$, parameters $\\alpha$, $\\beta$, and candidate thresholds $\\mathcal{T}$. Apply the definitions above exactly.\n\n- Case A (general case):\n  - $x = [1.0, 2.0, 2.5, 3.5, 4.0, 5.0]$\n  - $y = [0, 1, 0, 1, 1, 0]$\n  - $r = [10.0, 200.0, 5.0, 150.0, 300.0, 2.0]$\n  - $\\alpha = 5.0$, $\\beta = 1.0$\n  - $\\mathcal{T} = \\{1.5, 2.25, 3.0, 3.75, 4.5\\}$\n- Case B (boundary and ties; includes an empty leaf and a leaf tie):\n  - $x = [0.0, 0.1, 0.2, 0.3]$\n  - $y = [1, 1, 0, 0]$\n  - $r = [1.0, 1.0, 1.0, 1.0]$\n  - $\\alpha = 1.0$, $\\beta = 1.0$\n  - $\\mathcal{T} = \\{-1.0, 0.15, 1.0\\}$\n- Case C (edge case with a single extremely high-revenue positive):\n  - $x = [10.0, 20.0, 30.0, 40.0, 50.0]$\n  - $y = [0, 0, 1, 0, 0]$\n  - $r = [1.0, 1.0, 1000.0, 1.0, 1.0]$\n  - $\\alpha = 10.0$, $\\beta = 1.0$\n  - $\\mathcal{T} = \\{15.0, 25.0, 35.0, 45.0\\}$\n\nFinal Output Format. Your program must produce a single line containing a list of results, one per test case, in the following order and structure: for each test case output the list $[\\tau^\\star, \\text{total\\_loss}(\\tau^\\star), c_{L}(\\tau^\\star), c_{R}(\\tau^\\star)]$. Aggregate the three per-case lists into a single list and print that list on one line. The per-case entries for $\\tau^\\star$ and $\\text{total\\_loss}(\\tau^\\star)$ must be real numbers; the entries for $c_{L}(\\tau^\\star)$ and $c_{R}(\\tau^\\star)$ must be integers in $\\{0,1\\}$.", "solution": "The problem presented is a well-posed optimization problem in the context of decision tree construction, specifically tailored for a computational economics application. The problem statement is scientifically grounded, self-contained, and logically consistent. No ambiguities or contradictions are present. The task is to find an optimal split threshold $\\tau^\\star$ from a given finite set $\\mathcal{T}$, which minimizes a custom, asymmetric, and weighted economic loss function.\n\nThe procedure is as follows:\nFor each candidate threshold $\\tau$ in the set $\\mathcal{T}$, we must calculate the total split loss. This involves partitioning the data, determining the optimal prediction for each resulting partition (leaf), calculating the loss in each leaf, and summing these losses.\n\nFirst, let us formalize the calculation for a single region $S$, which can be either the left region $L(\\tau) = \\{i : x_i  \\tau\\}$ or the right region $R(\\tau) = \\{i : x_i \\ge \\tau\\}$. A constant prediction $c_S \\in \\{0, 1\\}$ is to be assigned to all observations in this region. The total loss in region $S$ depends on this prediction.\n\nLet $S_0 = \\{i \\in S : y_i = 0\\}$ be the set of indices of negative-class observations in $S$, and $S_1 = \\{i \\in S : y_i = 1\\}$ be the set for positive-class observations.\n\nIf we predict $c_S = 0$, we misclassify all positive-class observations. The loss for each such observation $i \\in S_1$ is $\\alpha \\cdot r_i$. The total loss for predicting $0$ is:\n$$L_0(S) = \\sum_{i \\in S_1} \\alpha \\cdot r_i$$\n\nIf we predict $c_S = 1$, we misclassify all negative-class observations. The loss for each such observation $i \\in S_0$ is $\\beta$. The total loss for predicting $1$ is:\n$$L_1(S) = \\sum_{i \\in S_0} \\beta = \\beta \\cdot |S_0|$$\nwhere $|S_0|$ is the count of negative-class observations in $S$.\n\nThe optimal prediction for the region, $c_S(\\tau)$, is the one that minimizes this loss. According to the problem's tie-breaking rule, if $L_0(S) = L_1(S)$, we must select the prediction $c_S(\\tau) = 0$. Therefore, the decision rule for the prediction in region $S$ is:\n$$\nc_S(\\tau) =\n\\begin{cases}\n1  \\text{if } L_1(S)  L_0(S) \\\\\n0  \\text{if } L_0(S) \\le L_1(S)\n\\end{cases}\n$$\nThe corresponding minimum loss for region $S$ is $\\text{Loss}(S) = \\min(L_0(S), L_1(S))$ if the decision rule is applied correctly. Specifically, $\\text{Loss}(S)$ is $L_1(S)$ if $c_S(\\tau) = 1$, and $L_0(S)$ if $c_S(\\tau) = 0$.\nFor an empty region $S = \\emptyset$, both $S_0$ and $S_1$ are empty. Thus, $L_0(\\emptyset) = 0$ and $L_1(\\emptyset) = 0$. The prediction tie-breaking rule dictates $c_\\emptyset(\\tau)=0$, and the loss is $0$.\n\nThe total split loss for a given threshold $\\tau$ is the sum of the minimum losses from the left and right regions:\n$$\\text{TotalLoss}(\\tau) = \\text{Loss}(L(\\tau)) + \\text{Loss}(R(\\tau))$$\n\nThe overall task is to find the optimal threshold $\\tau^\\star$ from the candidate set $\\mathcal{T}$ that minimizes this total loss:\n$$\\tau^\\star = \\arg\\min_{\\tau \\in \\mathcal{T}} \\text{TotalLoss}(\\tau)$$\nIf multiple thresholds yield the same minimum loss, the problem specifies to select the numerically smallest one.\n\nWe will now apply this procedure to each test case.\n\n**Case A**\n- Data: $x = [1.0, 2.0, 2.5, 3.5, 4.0, 5.0]$, $y = [0, 1, 0, 1, 1, 0]$, $r = [10.0, 200.0, 5.0, 150.0, 300.0, 2.0]$\n- Parameters: $\\alpha = 5.0$, $\\beta = 1.0$\n- Candidate thresholds: $\\mathcal{T} = \\{1.5, 2.25, 3.0, 3.75, 4.5\\}$\n\n1.  **$\\tau = 1.5$**:\n    - $L(1.5)$: indices $\\{1\\}$ ($x_1=1.0$). $S_0=\\{1\\}$, $S_1=\\emptyset$.\n      $L_0(L) = 0$, $L_1(L) = \\beta \\cdot 1 = 1.0$. $L_0 \\le L_1 \\implies c_L=0$, $\\text{Loss}_L = 0.0$.\n    - $R(1.5)$: indices $\\{2,3,4,5,6\\}$. $S_0=\\{3,6\\}$, $S_1=\\{2,4,5\\}$.\n      $L_0(R) = 5.0 \\cdot (200.0 + 150.0 + 300.0) = 3250.0$. $L_1(R) = 1.0 \\cdot 2 = 2.0$.\n      $L_1  L_0 \\implies c_R=1$, $\\text{Loss}_R = 2.0$.\n    - Total Loss = $0.0 + 2.0 = 2.0$.\n\n2.  **$\\tau = 2.25$**:\n    - $L(2.25)$: indices $\\{1,2\\}$. $S_0=\\{1\\}$, $S_1=\\{2\\}$.\n      $L_0(L) = 5.0 \\cdot 200.0 = 1000.0$. $L_1(L) = 1.0 \\cdot 1 = 1.0$. $c_L=1$, $\\text{Loss}_L = 1.0$.\n    - $R(2.25)$: indices $\\{3,4,5,6\\}$. $S_0=\\{3,6\\}$, $S_1=\\{4,5\\}$.\n      $L_0(R) = 5.0 \\cdot (150.0 + 300.0) = 2250.0$. $L_1(R) = 1.0 \\cdot 2 = 2.0$. $c_R=1$, $\\text{Loss}_R = 2.0$.\n    - Total Loss = $1.0 + 2.0 = 3.0$.\n\n3.  **$\\tau = 3.75$**:\n    - $L(3.75)$: indices $\\{1,2,3,4\\}$. $S_0=\\{1,3\\}$, $S_1=\\{2,4\\}$.\n      $L_0(L) = 5.0 \\cdot (200.0 + 150.0) = 1750.0$. $L_1(L) = 1.0 \\cdot 2 = 2.0$. $c_L=1$, $\\text{Loss}_L=2.0$.\n    - $R(3.75)$: indices $\\{5,6\\}$. $S_0=\\{6\\}$, $S_1=\\{5\\}$.\n      $L_0(R) = 5.0 \\cdot 300.0 = 1500.0$. $L_1(R) = 1.0 \\cdot 1 = 1.0$. $c_R=1$, $\\text{Loss}_R=1.0$.\n    - Total Loss = $2.0 + 1.0 = 3.0$.\n\n4.  **$\\tau = 4.5$**:\n    - $L(4.5)$: indices $\\{1,2,3,4,5\\}$. $S_0=\\{1,3\\}$, $S_1=\\{2,4,5\\}$.\n      $L_0(L) = 5.0 \\cdot (200.0+150.0+300.0) = 3250.0$. $L_1(L) = 1.0 \\cdot 2 = 2.0$. $c_L=1$, $\\text{Loss}_L=2.0$.\n    - $R(4.5)$: indices $\\{6\\}$. $S_0=\\{6\\}$, $S_1=\\emptyset$.\n      $L_0(R) = 0$. $L_1(R) = 1.0 \\cdot 1 = 1.0$. $c_R=0$, $\\text{Loss}_R=0.0$.\n    - Total Loss = $2.0 + 0.0 = 2.0$.\n\nThe minimum loss found is $2.0$, which occurs for $\\tau=1.5$ and $\\tau=4.5$. By the tie-breaking rule, we select the smallest threshold.\nResult: $\\tau^\\star = 1.5$, Total Loss = $2.0$, $c_L(\\tau^\\star)=0$, $c_R(\\tau^\\star)=1$.\n\n**Case B**\n- Data: $x = [0.0, 0.1, 0.2, 0.3]$, $y = [1, 1, 0, 0]$, $r = [1.0, 1.0, 1.0, 1.0]$\n- Parameters: $\\alpha = 1.0$, $\\beta = 1.0$\n- Candidate thresholds: $\\mathcal{T} = \\{-1.0, 0.15, 1.0\\}$\n\n1.  **$\\tau = -1.0$**:\n    - $L(-1.0)$: $\\emptyset$. $\\text{Loss}_L = 0.0$, $c_L=0$ (by tie-breaker).\n    - $R(-1.0)$: indices $\\{1,2,3,4\\}$. $S_0=\\{3,4\\}$, $S_1=\\{1,2\\}$.\n      $L_0(R) = 1.0 \\cdot (1.0+1.0) = 2.0$. $L_1(R) = 1.0 \\cdot 2 = 2.0$.\n      $L_0 \\le L_1 \\implies c_R=0$, $\\text{Loss}_R = 2.0$.\n    - Total Loss = $0.0 + 2.0 = 2.0$.\n\n2.  **$\\tau = 0.15$**:\n    - $L(0.15)$: indices $\\{1,2\\}$. $S_0=\\emptyset$, $S_1=\\{1,2\\}$.\n      $L_0(L) = 1.0 \\cdot (1.0+1.0) = 2.0$. $L_1(L) = 1.0 \\cdot 0 = 0$. $c_L=1$, $\\text{Loss}_L=0.0$.\n    - $R(0.15)$: indices $\\{3,4\\}$. $S_0=\\{3,4\\}$, $S_1=\\emptyset$.\n      $L_0(R) = 0$. $L_1(R) = 1.0 \\cdot 2 = 2.0$. $c_R=0$, $\\text{Loss}_R=0.0$.\n    - Total Loss = $0.0 + 0.0 = 0.0$.\n\n3.  **$\\tau = 1.0$**:\n    - $L(1.0)$: indices $\\{1,2,3,4\\}$. Same as $R(-1.0)$. $c_L=0$, $\\text{Loss}_L=2.0$.\n    - $R(1.0)$: $\\emptyset$. $\\text{Loss}_R = 0.0$, $c_R=0$.\n    - Total Loss = $2.0 + 0.0 = 2.0$.\n\nThe minimum loss found is $0.0$, uniquely for $\\tau=0.15$.\nResult: $\\tau^\\star = 0.15$, Total Loss = $0.0$, $c_L(\\tau^\\star)=1$, $c_R(\\tau^\\star)=0$.\n\n**Case C**\n- Data: $x = [10.0, 20.0, 30.0, 40.0, 50.0]$, $y = [0, 0, 1, 0, 0]$, $r = [1.0, 1.0, 1000.0, 1.0, 1.0]$\n- Parameters: $\\alpha = 10.0$, $\\beta = 1.0$\n- Candidate thresholds: $\\mathcal{T} = \\{15.0, 25.0, 35.0, 45.0\\}$\n\n1.  **$\\tau = 15.0$**:\n    - $L(15.0)$: index $\\{1\\}$. $S_0=\\{1\\}$, $S_1=\\emptyset$. $c_L=0$, $\\text{Loss}_L=0.0$.\n    - $R(15.0)$: indices $\\{2,3,4,5\\}$. $S_0=\\{2,4,5\\}$, $S_1=\\{3\\}$.\n      $L_0(R) = 10.0 \\cdot 1000.0 = 10000.0$. $L_1(R) = 1.0 \\cdot 3 = 3.0$. $c_R=1$, $\\text{Loss}_R=3.0$.\n    - Total Loss = $0.0 + 3.0 = 3.0$.\n\n2.  **$\\tau = 25.0$**:\n    - $L(25.0)$: indices $\\{1,2\\}$. $S_0=\\{1,2\\}$, $S_1=\\emptyset$. $c_L=0$, $\\text{Loss}_L=0.0$.\n    - $R(25.0)$: indices $\\{3,4,5\\}$. $S_0=\\{4,5\\}$, $S_1=\\{3\\}$.\n      $L_0(R) = 10.0 \\cdot 1000.0 = 10000.0$. $L_1(R) = 1.0 \\cdot 2 = 2.0$. $c_R=1$, $\\text{Loss}_R=2.0$.\n    - Total Loss = $0.0 + 2.0 = 2.0$.\n\n3.  **$\\tau = 35.0$**:\n    - $L(35.0)$: indices $\\{1,2,3\\}$. $S_0=\\{1,2\\}$, $S_1=\\{3\\}$.\n      $L_0(L) = 10.0 \\cdot 1000.0 = 10000.0$. $L_1(L) = 1.0 \\cdot 2 = 2.0$. $c_L=1$, $\\text{Loss}_L=2.0$.\n    - $R(35.0)$: indices $\\{4,5\\}$. $S_0=\\{4,5\\}$, $S_1=\\emptyset$. $c_R=0$, $\\text{Loss}_R=0.0$.\n    - Total Loss = $2.0 + 0.0 = 2.0$.\n\n4.  **$\\tau = 45.0$**:\n    - $L(45.0)$: indices $\\{1,2,3,4\\}$. $S_0=\\{1,2,4\\}$, $S_1=\\{3\\}$.\n      $L_0(L) = 10.0 \\cdot 1000.0 = 10000.0$. $L_1(L) = 1.0 \\cdot 3 = 3.0$. $c_L=1$, $\\text{Loss}_L=3.0$.\n    - $R(45.0)$: index $\\{5\\}$. $S_0=\\{5\\}$, $S_1=\\emptyset$. $c_R=0$, $\\text{Loss}_R=0.0$.\n    - Total Loss = $3.0 + 0.0 = 3.0$.\n\nThe minimum loss is $2.0$, occurring for $\\tau=25.0$ and $\\tau=35.0$. Selecting the smallest threshold gives $\\tau^\\star=25.0$.\nResult: $\\tau^\\star = 25.0$, Total Loss = $2.0$, $c_L(\\tau^\\star)=0$, $c_R(\\tau^\\star)=1$.", "answer": "```python\nimport numpy as np\n\ndef calculate_leaf_stats(y_leaf, r_leaf, alpha, beta):\n    \"\"\"\n    Calculates the minimum loss and optimal prediction for a single leaf (region).\n\n    Args:\n        y_leaf (np.ndarray): Binary labels {0, 1} for the observations in the leaf.\n        r_leaf (np.ndarray): Revenue weights for the observations in the leaf.\n        alpha (float): Cost parameter for misclassifying a y=1 as 0.\n        beta (float): Cost parameter for misclassifying a y=0 as 1.\n\n    Returns:\n        tuple[float, int]: A tuple containing (minimum_loss, optimal_prediction).\n    \"\"\"\n    # For an empty leaf, loss is 0. Prediction is 0 by tie-breaking rule.\n    if len(y_leaf) == 0:\n        return 0.0, 0\n\n    # Calculate loss if we predict 0 for the entire leaf.\n    # This misclassifies all observations where y=1.\n    mask_y1 = (y_leaf == 1)\n    cost_if_0 = alpha * np.sum(r_leaf[mask_y1])\n\n    # Calculate loss if we predict 1 for the entire leaf.\n    # This misclassifies all observations where y=0.\n    mask_y0 = (y_leaf == 0)\n    cost_if_1 = beta * np.sum(mask_y0)\n\n    # Determine optimal prediction and corresponding loss.\n    # Tie-breaker: if costs are equal, predict 0.\n    if cost_if_1  cost_if_0:\n        return float(cost_if_1), 1\n    else:\n        return float(cost_if_0), 0\n\ndef solve():\n    \"\"\"\n    Solves the decision tree split optimization problem for a suite of test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"x\": np.array([1.0, 2.0, 2.5, 3.5, 4.0, 5.0]),\n            \"y\": np.array([0, 1, 0, 1, 1, 0]),\n            \"r\": np.array([10.0, 200.0, 5.0, 150.0, 300.0, 2.0]),\n            \"alpha\": 5.0, \"beta\": 1.0,\n            \"T_set\": [1.5, 2.25, 3.0, 3.75, 4.5]\n        },\n        {\n            \"x\": np.array([0.0, 0.1, 0.2, 0.3]),\n            \"y\": np.array([1, 1, 0, 0]),\n            \"r\": np.array([1.0, 1.0, 1.0, 1.0]),\n            \"alpha\": 1.0, \"beta\": 1.0,\n            \"T_set\": [-1.0, 0.15, 1.0]\n        },\n        {\n            \"x\": np.array([10.0, 20.0, 30.0, 40.0, 50.0]),\n            \"y\": np.array([0, 0, 1, 0, 0]),\n            \"r\": np.array([1.0, 1.0, 1000.0, 1.0, 1.0]),\n            \"alpha\": 10.0, \"beta\": 1.0,\n            \"T_set\": [15.0, 25.0, 35.0, 45.0]\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        x, y, r, alpha, beta, T_set = case[\"x\"], case[\"y\"], case[\"r\"], case[\"alpha\"], case[\"beta\"], case[\"T_set\"]\n        \n        best_tau = None\n        best_loss = float('inf')\n        best_c_L = -1\n        best_c_R = -1\n\n        for tau in T_set:\n            # Partition the data into left and right regions based on the threshold.\n            left_mask = x  tau\n            right_mask = x >= tau\n\n            # Calculate loss and prediction for the left leaf.\n            loss_L, c_L = calculate_leaf_stats(y[left_mask], r[left_mask], alpha, beta)\n            \n            # Calculate loss and prediction for the right leaf.\n            loss_R, c_R = calculate_leaf_stats(y[right_mask], r[right_mask], alpha, beta)\n\n            total_loss = loss_L + loss_R\n\n            # Check if this threshold provides a better or equal-but-smaller result.\n            if total_loss  best_loss:\n                best_loss = total_loss\n                best_tau = tau\n                best_c_L = c_L\n                best_c_R = c_R\n            elif total_loss == best_loss:\n                # Tie-breaker: choose the numerically smallest threshold.\n                if best_tau is None or tau  best_tau:\n                    best_tau = tau\n                    best_c_L = c_L\n                    best_c_R = c_R\n        \n        # Store the result for the current case.\n        case_result = [float(best_tau), float(best_loss), int(best_c_L), int(best_c_R)]\n        all_results.append(case_result)\n\n    # Print the aggregated results in the specified list-of-lists format.\n    print(all_results)\n\nsolve()\n```", "id": "2386905"}, {"introduction": "Random forests are highly valued for their ability to rank feature importance, but this power comes with a responsibility to guard against subtle data issues. This final exercise is a simulation study that highlights a critical pitfall in applied modeling: target leakage. By generating a synthetic dataset where one feature has access to post-outcome information, you will see firsthand how a random forest can be tricked into assigning high importance to a useless or invalid predictor [@problem_id:2386893]. This practice will hone your skills in critically evaluating model outputs and diagnosing one of the most common sources of failure in real-world projects.", "problem": "You are modeling binary loan default outcomes in a credit portfolio. For each observation $i \\in \\{1,\\dots,n\\}$, let the binary target be $y_i \\in \\{0,1\\}$, where $y_i = 1$ indicates default. You will generate synthetic covariates that are economically interpretable and then add a subtle post-outcome covariate that leaks information about $y_i$. You must then quantify how an ensemble of decision stumps (a one-split decision forest) ranks the covariates by importance and report, for each test case, the zero-based index of the most important covariate. Indices must be reported as integers.\n\nData-generating process:\n- Let the number of base covariates be $p_b = 5$. For each observation $i$, draw a base feature vector $\\mathbf{x}_i \\in \\mathbb{R}^{p_b}$ from a mean-zero multivariate normal distribution with covariance matrix $\\Sigma(\\rho) \\in \\mathbb{R}^{p_b \\times p_b}$ defined by\n$$\n\\Sigma(\\rho) = (1-\\rho) I_{p_b} + \\rho \\mathbf{1}\\mathbf{1}^\\top,\n$$\nwhere $I_{p_b}$ is the identity matrix of size $p_b$ and $\\mathbf{1}$ is the $p_b$-dimensional vector of ones. The scalar $\\rho \\in (-\\frac{1}{p_b-1},1)$ controls the common correlation among base features.\n- Draw an idiosyncratic macro factor $m_i \\sim \\mathcal{N}(0,1)$ independently of $\\mathbf{x}_i$.\n- Define a latent score $s_i$ via a logistic index:\n$$\ns_i = \\beta_0 + \\sum_{j=1}^{p_b} \\beta_j x_{i,j} + \\gamma m_i,\n$$\nwith coefficients fixed at $\\beta_0 = -0.5$, $\\beta_1 = 0.8$, $\\beta_2 = -1.0$, $\\beta_3 = 0.6$, $\\beta_4 = 0.0$, $\\beta_5 = 0.5$, and $\\gamma = 0.7$.\n- Define the default probability $p_i$ by the logistic function:\n$$\np_i = \\frac{1}{1 + e^{-s_i}}.\n$$\n- Draw the binary outcome $y_i \\sim \\text{Bernoulli}(p_i)$ independently across $i$.\n- Define a post-outcome covariate $z_i$ that leaks target information as\n$$\nz_i = \\lambda \\, y_i + \\delta_i,\n$$\nwith $\\delta_i \\sim \\mathcal{N}(0,\\sigma^2)$ independent of everything else. The parameter $\\lambda \\in \\mathbb{R}$ controls the magnitude of leakage, and $\\sigma \\ge 0$ controls the amount of noise obscuring the leakage.\n- For modeling, you will form the feature vector $\\tilde{\\mathbf{x}}_i$ as follows. If the test case flag `include_leak` $= 1$, then set\n$$\n\\tilde{\\mathbf{x}}_i = \\big[x_{i,1}, x_{i,2}, x_{i,3}, x_{i,4}, x_{i,5}, z_i\\big] \\in \\mathbb{R}^{p},\n$$\nwith $p = p_b + 1$ and the leaking covariate at zero-based index $p_b$. If `include_leak` $= 0$, then set\n$$\n\\tilde{\\mathbf{x}}_i = \\big[x_{i,1}, x_{i,2}, x_{i,3}, x_{i,4}, x_{i,5}\\big] \\in \\mathbb{R}^{p},\n$$\nwith $p = p_b$.\n\nModel and importance:\n- Consider an ensemble of $T$ decision stumps (one-split decision trees) with $T = 200$. For each tree $t \\in \\{1,\\dots,T\\}$:\n  - Draw a bootstrap sample of size $n$ by sampling indices from $\\{1,\\dots,n\\}$ with replacement.\n  - Let $m_{\\text{try}} = \\lceil \\sqrt{p} \\rceil$ and select uniformly at random $m_{\\text{try}}$ distinct features from $\\{0,\\dots,p-1\\}$ to be considered for splitting.\n  - For each selected feature $j$, consider splits of the form $x_{j} \\le \\tau$ for thresholds $\\tau$ at midpoints between consecutive sorted observed values of that feature on the bootstrap sample, excluding thresholds that would yield an empty child node. Let $G(S)$ denote the Gini impurity of a set $S$ of binary labels:\n  $$\n  G(S) = 1 - \\sum_{c \\in \\{0,1\\}} \\left(\\frac{n_c}{|S|}\\right)^2,\n  $$\n  where $n_c$ is the count of class $c$ in $S$. For a parent node with label multiset $S$ and a split into left child $S_L$ and right child $S_R$, define the impurity decrease as\n  $$\n  \\Delta G = G(S) - \\left(\\frac{|S_L|}{|S|}G(S_L) + \\frac{|S_R|}{|S|}G(S_R)\\right).\n  $$\n  - Choose the feature $j^\\star$ and threshold $\\tau^\\star$ that maximize $\\Delta G$ among the considered features and thresholds. Grow a decision stump by splitting at $(j^\\star,\\tau^\\star)$.\n  - Attribute the realized impurity decrease $\\Delta G^\\star$ of the chosen split to feature $j^\\star$.\n- Define the importance of feature $j$ as the sum of attributed impurity decreases over the $T$ trees:\n$$\nI_j = \\sum_{t=1}^{T} \\Delta G_t \\cdot \\mathbf{1}\\{j_t^\\star = j\\}.\n$$\n- For each test case, compute the index $j_{\\max} \\in \\{0,\\dots,p-1\\}$ that maximizes $I_j$. In case of ties, take the smallest index achieving the maximum.\n\nTest suite:\nFor reproducibility, use an independent random seed $s$ per test case for both data generation and ensemble construction. Use the following four test cases, each specified by the tuple $(n,\\sigma,\\lambda,\\rho,\\text{include\\_leak}, s)$:\n- Case A: $(3000, 0.1, 0.9, 0.2, 1, 11)$.\n- Case B: $(3000, 0.3, 0.6, 0.2, 1, 12)$.\n- Case C: $(1200, 0.1, 0.9, 0.2, 0, 13)$.\n- Case D: $(3000, 0.6, 0.4, 0.2, 1, 14)$.\n\nRequired program behavior and output:\n- For each test case, generate data according to the process above, train the ensemble described above, compute feature importances $\\{I_j\\}_{j=0}^{p-1}$, and return the zero-based index $j_{\\max}$ of the most important feature.\n- Your program must produce a single line of output containing the four indices as a comma-separated list enclosed in square brackets, in the same order as the test cases, for example [$i_1$,$i_2$,$i_3$,$i_4$]. No additional text should be printed.", "solution": "The problem presented is a valid, well-posed exercise in computational statistics and machine learning, specifically concerning the evaluation of feature importance in tree-based ensemble models. The data-generating process is rigorously defined and scientifically grounded in standard models of financial econometrics. The task is to quantify feature importance using Gini impurity decrease in an ensemble of decision stumps and to identify the most impactful feature, particularly in the presence of a \"leaking\" covariate. The problem is objective, self-contained, and algorithmically specified, permitting a unique, reproducible solution.\n\nThe solution methodology proceeds in two main stages for each test case: data generation and model training with importance calculation. All mathematical entities, including variables, parameters, and numerical values, are represented in LaTeX as required.\n\n**1. Data Generation Process**\n\nFor each of the specified test cases, a synthetic dataset of size $n$ is generated according to the following stochastic process. A random seed $s$ is used to ensure reproducibility.\n\n- **Base Covariates**: A set of $p_b = 5$ base covariates, denoted by the vector $\\mathbf{x}_i \\in \\mathbb{R}^{p_b}$ for each observation $i \\in \\{1, \\dots, n\\}$, is drawn from a multivariate normal distribution $\\mathcal{N}(\\mathbf{0}, \\Sigma(\\rho))$. The covariance matrix $\\Sigma(\\rho)$ is defined as\n$$\n\\Sigma(\\rho) = (1-\\rho) I_{p_b} + \\rho \\mathbf{1}\\mathbf{1}^\\top\n$$\nwhere $I_{p_b}$ is the $p_b \\times p_b$ identity matrix and $\\mathbf{1}$ is a $p_b$-dimensional vector of ones. The parameter $\\rho$ controls the equicorrelation between these base features.\n\n- **Latent Score and Default Probability**: An idiosyncratic factor $m_i \\sim \\mathcal{N}(0,1)$ is drawn independently. A latent score $s_i$ is constructed as a linear combination of the base covariates and the macro factor:\n$$\ns_i = \\beta_0 + \\sum_{j=1}^{p_b} \\beta_j x_{i,j} + \\gamma m_i\n$$\nThe coefficients are fixed at $\\beta_0 = -0.5$, $\\boldsymbol{\\beta}_{1..p_b} = [0.8, -1.0, 0.6, 0.0, 0.5]$, and $\\gamma = 0.7$. Note that the coefficient for $x_{i,4}$ is $\\beta_4 = 0.0$, making this feature uninformative by construction with respect to the latent score. The latent score is then transformed into a probability of default, $p_i$, using the standard logistic function:\n$$\np_i = \\frac{1}{1 + e^{-s_i}}\n$$\n\n- **Binary Outcome**: The binary target variable, $y_i \\in \\{0, 1\\}$, indicating non-default ($0$) or default ($1$), is drawn from a Bernoulli distribution with the generated probability, $y_i \\sim \\text{Bernoulli}(p_i)$.\n\n- **Leaking Covariate**: A post-outcome covariate $z_i$ is generated to simulate information leakage from the target variable. Its definition is:\n$$\nz_i = \\lambda \\, y_i + \\delta_i\n$$\nwhere $\\delta_i$ is a noise term drawn from a normal distribution $\\mathcal{N}(0, \\sigma^2)$. The parameter $\\lambda$ controls the strength of the leakage, and $\\sigma$ controls the noise level. A high ratio of $|\\lambda|$ to $\\sigma$ implies a strong, easily detectable link between $z_i$ and $y_i$.\n\n- **Final Feature Matrix**: The complete feature matrix $\\tilde{\\mathbf{X}}$ is assembled. If the `include_leak` flag is $1$, the feature set is $\\tilde{\\mathbf{x}}_i = [x_{i,1}, \\dots, x_{i,5}, z_i] \\in \\mathbb{R}^{6}$. The leaking feature $z_i$ is located at the last position (zero-based index $5$). If the flag is $0$, only the base covariates are used, $\\tilde{\\mathbf{x}}_i = [x_{i,1}, \\dots, x_{i,5}] \\in \\mathbb{R}^{5}$. The number of features is denoted by $p$.\n\n**2. Feature Importance Quantification**\n\nThe importance of each feature is determined by training an ensemble of $T = 200$ decision stumps. A decision stump is a decision tree with only one split.\n\n- **Ensemble Construction**: For each of the $T$ stumps in the ensemble:\n    1. A bootstrap sample of size $n$ is created by sampling with replacement from the full dataset $(\\tilde{\\mathbf{X}}, \\mathbf{y})$.\n    2. A random subset of $m_{\\text{try}} = \\lceil \\sqrt{p} \\rceil$ distinct features is selected.\n    3. For each selected feature, the optimal split is found. A split is defined by a feature $j$ and a threshold $\\tau$. The quality of the split is measured by the Gini impurity decrease, $\\Delta G$. The Gini impurity of a set of labels $S$ is given by:\n    $$\n    G(S) = 1 - \\sum_{c \\in \\{0,1\\}} \\left(\\frac{n_c}{|S|}\\right)^2\n    $$\n    where $n_c$ is the count of class $c$ in the set $S$. The impurity decrease is the difference between the parent node's impurity and the weighted average of the two child nodes' impurities.\n    4. The feature $j^\\star$ and threshold $\\tau^\\star$ that yield the maximum impurity decrease $\\Delta G^\\star$ are chosen for the stump's split. Potential thresholds are the midpoints of consecutive unique sorted values of the feature in the bootstrap sample.\n\n- **Importance Aggregation**: The importance of a feature $j$, denoted $I_j$, is calculated as the sum of the impurity decreases it is responsible for across all trees in the ensemble:\n$$\nI_j = \\sum_{t=1}^{T} \\Delta G_t \\cdot \\mathbf{1}\\{j_t^\\star = j\\}\n$$\nwhere $\\mathbf{1}\\{\\cdot\\}$ is the indicator function.\n\n- **Result**: Finally, for each test case, the zero-based index of the feature with the highest importance score, $j_{\\max} = \\arg\\max_j I_j$, is identified. Ties are resolved by selecting the smallest index. This index is the output for the case. The procedure is implemented in Python, adhering to the specified libraries and random seeds to ensure verifiable results.", "answer": "```python\nimport numpy as np\nfrom math import ceil, sqrt\n\ndef _gini_impurity(y):\n    \"\"\"Calculates the Gini impurity of a set of binary labels.\"\"\"\n    n_samples = len(y)\n    if n_samples == 0:\n        return 0.0\n    n1_count = np.sum(y)\n    p1 = n1_count / n_samples\n    p0 = 1.0 - p1\n    return 1.0 - (p0**2 + p1**2)\n\ndef _find_best_split(X_boot, y_boot, feature_indices):\n    \"\"\"Finds the best split for a single decision stump.\"\"\"\n    n_samples = len(y_boot)\n    if n_samples = 1:\n        return -1, -1.0\n\n    best_gain = -1.0\n    best_feature = -1\n\n    gini_parent = _gini_impurity(y_boot)\n    n1_total = np.sum(y_boot)\n    \n    for j in feature_indices:\n        feature_values = X_boot[:, j]\n        \n        unique_vals = np.unique(feature_values)\n        if len(unique_vals)  2:\n            continue\n            \n        # Efficiently find the best split for feature j\n        sorted_indices = np.argsort(feature_values)\n        y_sorted = y_boot[sorted_indices]\n        x_sorted = feature_values[sorted_indices]\n        \n        y_cumsum = np.cumsum(y_sorted)\n\n        split_points = np.where(x_sorted[:-1] != x_sorted[1:])[0]\n\n        for i in split_points:\n            n_left = i + 1\n            n_right = n_samples - n_left\n\n            n1_left = y_cumsum[i]\n            n1_right = n1_total - n1_left\n\n            gini_left = 1.0 - ((n1_left / n_left)**2 + ((n_left - n1_left) / n_left)**2)\n            gini_right = 1.0 - ((n1_right / n_right)**2 + ((n_right - n1_right) / n_right)**2)\n            \n            gain = gini_parent - (n_left / n_samples * gini_left + n_right / n_samples * gini_right)\n\n            if gain > best_gain:\n                best_gain = gain\n                best_feature = j\n                \n    return best_feature, best_gain\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # (n, sigma, lambda, rho, include_leak, seed)\n        (3000, 0.1, 0.9, 0.2, 1, 11),\n        (3000, 0.3, 0.6, 0.2, 1, 12),\n        (1200, 0.1, 0.9, 0.2, 0, 13),\n        (3000, 0.6, 0.4, 0.2, 1, 14),\n    ]\n\n    results = []\n\n    for n, sigma, lam, rho, include_leak, seed in test_cases:\n        # Set seed for reproducibility for the entire test case\n        np.random.seed(seed)\n\n        # 1. Data Generation\n        p_b = 5\n        betas = np.array([0.8, -1.0, 0.6, 0.0, 0.5])\n        beta_0 = -0.5\n        gamma = 0.7\n        \n        # Covariance matrix\n        cov_matrix = (1 - rho) * np.identity(p_b) + rho * np.ones((p_b, p_b))\n        \n        # Base features\n        X_base = np.random.multivariate_normal(np.zeros(p_b), cov_matrix, n)\n        \n        # Macro factor\n        m = np.random.randn(n)\n        \n        # Latent score\n        s = beta_0 + X_base @ betas + gamma * m\n        \n        # Default probability\n        p_default = 1.0 / (1.0 + np.exp(-s))\n        \n        # Binary outcome\n        y = np.random.binomial(1, p_default, n)\n        \n        # Assemble final feature matrix\n        if include_leak == 1:\n            delta = np.random.normal(0, sigma, n)\n            z = lam * y + delta\n            X = np.c_[X_base, z]\n        else:\n            X = X_base\n        \n        n_samples, p = X.shape\n\n        # 2. Ensemble Training and Importance Calculation\n        T = 200\n        m_try = ceil(sqrt(p))\n        importances = np.zeros(p)\n        \n        all_indices = np.arange(n_samples)\n\n        for _ in range(T):\n            # Bootstrap sample\n            boot_indices = np.random.choice(all_indices, size=n_samples, replace=True)\n            X_boot, y_boot = X[boot_indices], y[boot_indices]\n            \n            # Feature subsampling\n            feature_indices = np.random.choice(p, size=m_try, replace=False)\n            \n            # Find best split for this stump\n            best_feature, best_gain = _find_best_split(X_boot, y_boot, feature_indices)\n            \n            if best_feature != -1:\n                importances[best_feature] += best_gain\n\n        # 3. Find the most important feature index\n        j_max = np.argmax(importances)\n        results.append(j_max)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2386893"}]}