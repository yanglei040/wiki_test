{"hands_on_practices": [{"introduction": "In economic and financial modeling, it is common for predictor variables to be highly correlated. This exercise challenges you to think critically about how Ridge and Lasso regression behave when faced with perfect multicollinearity, a scenario that reveals their fundamental differences in handling model coefficients [@problem_id:2426293]. Understanding this distinction is crucial for choosing the right model and correctly interpreting its results in practical applications.", "problem": "Consider a linear predictive model in computational finance for an outcome $y \\in \\mathbb{R}^{n}$ (for example, excess market returns) using a predictor matrix $X \\in \\mathbb{R}^{n \\times p}$, with coefficient vector $\\beta \\in \\mathbb{R}^{p}$. Suppose one column $x_{j} \\in \\mathbb{R}^{n}$ is duplicated and appended to $X$ to form an augmented matrix $\\tilde{X} \\in \\mathbb{R}^{n \\times (p+1)}$ in which the $j$-th predictor appears twice as identical columns $x_{j}^{(1)} = x_{j}$ and $x_{j}^{(2)} = x_{j}$. Let $\\lambda  0$ be fixed. Consider the ridge-penalized estimator that minimizes $L_{2}$-regularized least squares and the Least Absolute Shrinkage and Selection Operator (Lasso) estimator that minimizes $L_{1}$-regularized least squares, each fit once using the original $X$ and once using the augmented $\\tilde{X}$.\n\nWhich statement best describes what happens to the solutions when the identical copy of $x_{j}$ is added?\n\nA. For ridge, the optimizer remains unique and assigns equal coefficients to the duplicate columns; the effective shrinkage on the duplicated direction is reduced, so the fitted values generally change relative to the model without duplication. For Lasso, the set of optimal coefficient vectors becomes non-unique across the duplicate coordinates, but the fitted values and the total coefficient $x_{j}$-effect (the sum across the duplicates) are the same as in the model without duplication.\n\nB. Both ridge and Lasso produce unique coefficient vectors and leave the fitted values unchanged by the duplication; coefficients may rescale across the duplicate columns, but predictions are identical to the model without duplication.\n\nC. Ridge loses uniqueness of the coefficients because the design becomes singular, but the fitted values are unchanged; Lasso remains unique due to the $L_{1}$-penalty.\n\nD. Lasso penalizes the duplicated predictor more heavily and therefore shrinks the sum of the duplicate coefficients more than in the model without duplication, changing the fitted values; ridge is unaffected by the duplication because the $L_{2}$-penalty makes the minimizer invariant to identical columns.", "solution": "The problem statement must first be validated for scientific and logical integrity.\n\n**Step 1: Extract Givens**\n- A linear predictive model relates an outcome $y \\in \\mathbb{R}^{n}$ to a predictor matrix $X \\in \\mathbb{R}^{n \\times p}$ via a coefficient vector $\\beta \\in \\mathbb{R}^{p}$.\n- An augmented matrix $\\tilde{X} \\in \\mathbb{R}^{n \\times (p+1)}$ is formed by duplicating a column $x_j \\in \\mathbb{R}^{n}$ from $X$.\n- The duplicated columns are identical: $x_{j}^{(1)} = x_{j}$ and $x_{j}^{(2)} = x_{j}$.\n- A fixed regularization parameter $\\lambda  0$ is used.\n- Two estimators are considered:\n    1. Ridge regression, which minimizes the $L_2$-regularized least squares objective function.\n    2. Lasso, which minimizes the $L_1$-regularized least squares objective function.\n- Each estimator is applied to both the original problem with matrix $X$ and the augmented problem with matrix $\\tilde{X}$.\n- The question asks to describe the effect of this duplication on the solutions (coefficients and fitted values).\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is well-defined within the field of statistical learning and computational finance. It poses a question about the fundamental properties of two standard regularization methods, ridge and Lasso regression, when faced with perfect multicollinearity, a common issue in high-dimensional data.\n\n- **Scientifically Grounded:** The problem is based on the established mathematical theories of ridge and Lasso regression. All concepts are standard in statistics and econometrics. The premise is sound.\n- **Well-Posed:** The problem provides all necessary information to derive the behavior of the estimators. For a fixed $\\lambda  0$, solutions for both ridge and Lasso exist. The question about the uniqueness and characteristics of these solutions is a central part of the analysis, not a flaw in the problem statement.\n- **Objective:** The problem is stated in precise mathematical language, free from subjectivity or ambiguity.\n\nThe problem statement is internally consistent, scientifically valid, and possesses a clear structure for which a definitive, verifiable answer can be derived.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A rigorous derivation of the solution can proceed.\n\n**Derivation of Solution**\n\nLet the original design matrix be $X = [x_1, \\dots, x_j, \\dots, x_p]$. The augmented matrix $\\tilde{X}$ can be written, without loss of generality, by inserting the duplicate of $x_j$ next to the original, as $\\tilde{X} = [x_1, \\dots, x_j, x_j, \\dots, x_p]$. The corresponding coefficient vector is $\\tilde{\\beta} \\in \\mathbb{R}^{p+1}$, which we can partition as $(\\beta_1, \\dots, \\beta_{j-1}, \\beta_{j,1}, \\beta_{j,2}, \\beta_{j+1}, \\dots, \\beta_p)$.\n\n**Analysis of Ridge Regression**\n\nThe ridge estimator for the original problem, $\\hat{\\beta}_{\\text{ridge}}$, minimizes the objective function:\n$$ J(\\beta) = \\|y - X\\beta\\|_2^2 + \\lambda \\|\\beta\\|_2^2 $$\nThe solution is unique for $\\lambda  0$ and is given by $\\hat{\\beta}_{\\text{ridge}} = (X^T X + \\lambda I_p)^{-1} X^T y$.\n\nFor the augmented problem, the ridge estimator $\\tilde{\\hat{\\beta}}_{\\text{ridge}}$ minimizes:\n$$ \\tilde{J}(\\tilde{\\beta}) = \\|y - \\tilde{X}\\tilde{\\beta}\\|_2^2 + \\lambda \\|\\tilde{\\beta}\\|_2^2 $$\nThe Hessian of this objective function is $2(\\tilde{X}^T\\tilde{X} + \\lambda I_{p+1})$. Since $\\tilde{X}^T\\tilde{X}$ is positive semi-definite and $\\lambda I_{p+1}$ is positive definite for $\\lambda  0$, their sum is positive definite. This guarantees that $\\tilde{J}(\\tilde{\\beta})$ is a strictly convex function and has a unique minimizer, $\\tilde{\\hat{\\beta}}_{\\text{ridge}}$.\n\nLet us examine the structure of the solution. The prediction term involving the duplicated columns is $x_j \\beta_{j,1} + x_j \\beta_{j,2} = x_j(\\beta_{j,1} + \\beta_{j,2})$. The penalty term involving these coefficients is $\\lambda(\\beta_{j,1}^2 + \\beta_{j,2}^2)$.\nConsider minimizing the penalty part $\\beta_{j,1}^2 + \\beta_{j,2}^2$ for a fixed sum $S = \\beta_{j,1} + \\beta_{j,2}$. By a standard optimization argument (e.g., Lagrange multipliers), the minimum is achieved when $\\beta_{j,1} = \\beta_{j,2} = S/2$. Thus, the unique solution must have equal coefficients for the identical columns.\n\nNow, let's compare the augmented problem to the original. Let the total coefficient for the $j$-th predictor in the augmented model be $\\beta_j^* = \\beta_{j,1} + \\beta_{j,2}$. Since $\\beta_{j,1} = \\beta_{j,2}$, we have $\\beta_j^* = 2\\beta_{j,1}$, and the penalty term becomes $\\lambda(\\beta_{j,1}^2 + \\beta_{j,2}^2) = \\lambda( (\\beta_j^*/2)^2 + (\\beta_j^*/2)^2 ) = \\lambda \\frac{(\\beta_j^*)^2}{2}$.\nThe augmented problem is therefore equivalent to minimizing:\n$$ \\|y - X\\beta^*\\|_2^2 + \\lambda \\left( \\sum_{k \\neq j} (\\beta_k^*)^2 + \\frac{1}{2}(\\beta_j^*)^2 \\right) $$\nThis is a weighted ridge regression problem where the penalty on the $j$-th coefficient is effectively halved. Since the objective function has changed, the solution vector $\\beta^*$ will in general be different from the original $\\hat{\\beta}_{\\text{ridge}}$. Consequently, the fitted values $\\hat{y} = X\\beta^*$ will also generally change.\n\n**Analysis of Lasso**\n\nThe Lasso estimator for the original problem, $\\hat{\\beta}_{\\text{lasso}}$, minimizes:\n$$ L(\\beta) = \\|y - X\\beta\\|_2^2 + \\lambda \\|\\beta\\|_1 $$\nFor the augmented problem, the Lasso estimator $\\tilde{\\hat{\\beta}}_{\\text{lasso}}$ minimizes:\n$$ \\tilde{L}(\\tilde{\\beta}) = \\|y - \\tilde{X}\\tilde{\\beta}\\|_2^2 + \\lambda \\|\\tilde{\\beta}\\|_1 $$\nThe prediction term is $\\tilde{X}\\tilde{\\beta} = X\\beta^*$, where $\\beta_k^* = \\tilde{\\beta}_k$ for $k \\neq j$ and $\\beta_j^* = \\tilde{\\beta}_{j,1} + \\tilde{\\beta}_{j,2}$. The penalty term is $\\lambda(\\sum_{k \\neq j} |\\tilde{\\beta}_k| + |\\tilde{\\beta}_{j,1}| + |\\tilde{\\beta}_{j,2}|)$.\nLet $\\hat{\\beta}$ be an optimal solution for the original Lasso problem. Any optimal solution $\\tilde{\\hat{\\beta}}$ for the augmented problem must produce the same or lower objective value.\nLet us construct a candidate solution $\\tilde{\\beta}$ from $\\hat{\\beta}$. We set $\\tilde{\\beta}_k = \\hat{\\beta}_k$ for $k \\notin \\{j_1, j_2\\}$ (indices of duplicated columns). We need to choose $\\tilde{\\beta}_{j,1}$ and $\\tilde{\\beta}_{j,2}$ such that $\\tilde{\\beta}_{j,1} + \\tilde{\\beta}_{j,2} = \\hat{\\beta}_j$. With this choice, the residual sum of squares term is identical: $\\|y - \\tilde{X}\\tilde{\\beta}\\|_2^2=\\|y - X\\hat{\\beta}\\|_2^2$.\nThe penalty term becomes $\\lambda(\\sum_{k \\neq j} |\\hat{\\beta}_k| + |\\tilde{\\beta}_{j,1}| + |\\tilde{\\beta}_{j,2}|)$. To match the original problem's objective value, we need $|\\tilde{\\beta}_{j,1}| + |\\tilde{\\beta}_{j,2}| = |\\hat{\\beta}_j|$.\nBy the triangle inequality, $|\\tilde{\\beta}_{j,1}| + |\\tilde{\\beta}_{j,2}| \\ge |\\tilde{\\beta}_{j,1} + \\tilde{\\beta}_{j,2}| = |\\hat{\\beta}_j|$. Equality holds if and only if $\\tilde{\\beta}_{j,1}$ and $\\tilde{\\beta}_{j,2}$ have the same sign (or at least one is zero).\n\nTherefore, any pair $(\\tilde{\\beta}_{j,1}, \\tilde{\\beta}_{j,2})$ satisfying:\n1. $\\tilde{\\beta}_{j,1} + \\tilde{\\beta}_{j,2} = \\hat{\\beta}_j$\n2. $\\text{sign}(\\tilde{\\beta}_{j,1}) = \\text{sign}(\\tilde{\\beta}_{j,2}) = \\text{sign}(\\hat{\\beta}_j)$ (if $\\hat{\\beta}_j \\neq 0$)\nwill result in an optimal solution for the augmented problem. For example, if $\\hat{\\beta}_j  0$, any distribution of $\\hat{\\beta}_j$ into two non-negative parts $\\tilde{\\beta}_{j,1} = \\alpha \\hat{\\beta}_j$ and $\\tilde{\\beta}_{j,2} = (1-\\alpha)\\hat{\\beta}_j$ for $\\alpha \\in [0, 1]$ is valid.\nThis demonstrates that the set of optimal coefficient vectors $\\tilde{\\hat{\\beta}}$ is no longer unique. However, the total effect of the $j$-th predictor, $\\tilde{\\beta}_{j,1} + \\tilde{\\beta}_{j,2}$, is uniquely determined and equals $\\hat{\\beta}_j$. Consequently, the fitted values are also unchanged:\n$\\tilde{\\hat{y}}_{\\text{lasso}} = \\tilde{X}\\tilde{\\hat{\\beta}} = X\\hat{\\beta} = \\hat{y}_{\\text{lasso}}$.\n\n**Option-by-Option Analysis**\n\nA. **For ridge, the optimizer remains unique and assigns equal coefficients to the duplicate columns; the effective shrinkage on the duplicated direction is reduced, so the fitted values generally change relative to the model without duplication. For Lasso, the set of optimal coefficient vectors becomes non-unique across the duplicate coordinates, but the fitted values and the total coefficient $x_{j}$-effect (the sum across the duplicates) are the same as in the model without duplication.**\nThis statement aligns perfectly with the derivations above. For ridge, the solution is unique, coefficients for duplicates are equal, and the change in effective penalty alters the fitted values. For Lasso, the individual coefficients for the duplicates are not unique, but their sum and the fitted values are invariant.\nVerdict: **Correct**.\n\nB. **Both ridge and Lasso produce unique coefficient vectors and leave the fitted values unchanged by the duplication; coefficients may rescale across the duplicate columns, but predictions are identical to the model without duplication.**\nThis is incorrect. Ridge fitted values change, and Lasso coefficient vectors are not unique.\nVerdict: **Incorrect**.\n\nC. **Ridge loses uniqueness of the coefficients because the design becomes singular, but the fitted values are unchanged; Lasso remains unique due to the $L_{1}$-penalty.**\nThis is incorrect. The ridge solution remains unique due to the $L_2$ penalty term ensuring strict convexity. Its fitted values do change. The Lasso solution for the coefficients becomes non-unique.\nVerdict: **Incorrect**.\n\nD. **Lasso penalizes the duplicated predictor more heavily and therefore shrinks the sum of the duplicate coefficients more than in the model without duplication, changing the fitted values; ridge is unaffected by the duplication because the $L_{2}$-penalty makes the minimizer invariant to identical columns.**\nThis is incorrect. The Lasso penalty on the total effect is unchanged, and thus the fitted values are also unchanged. Ridge regression is affected by the duplication, as the effective penalty on the duplicated predictor is reduced.\nVerdict: **Incorrect**.", "answer": "$$\\boxed{A}$$", "id": "2426293"}, {"introduction": "Moving from conceptual understanding to concrete calculation, this exercise demonstrates the distinct behaviors of Ridge and Lasso through a carefully constructed numerical example [@problem_id:2426291]. By working through the calculations, you will directly observe how Lasso's $L_1$ penalty performs variable selection, even arbitrarily, while Ridge's $L_2$ penalty shrinks correlated coefficients together. This practice illuminates the practical trade-offs between achieving a sparse model and maintaining stability in the presence of correlated predictors.", "problem": "An analyst is estimating a linear predictive model for a portfolioâ€™s one-period-ahead excess return using two highly correlated predictors over two time periods, with no intercept term. Let the response vector be $y \\in \\mathbb{R}^{2}$ and the predictor matrix be $X \\in \\mathbb{R}^{2 \\times 2}$ with columns $x_{1}$ and $x_{2}$ given by\n$$\ny=\\begin{pmatrix}1 \\\\ 0\\end{pmatrix}, \\quad x_{1}=\\begin{pmatrix}1 \\\\ 0\\end{pmatrix}, \\quad x_{2}=\\begin{pmatrix}2 \\\\ 1\\end{pmatrix}, \\quad X=\\begin{pmatrix}1  2 \\\\ 0  1\\end{pmatrix}.\n$$\nThe data generating process is $y=x_{1}$ (no noise). Consider two regularized estimators:\n- The Least Absolute Shrinkage and Selection Operator (Lasso) estimator $\\hat{\\beta}^{\\text{LASSO}}(\\lambda)$ defined as any minimizer of\n$$\n\\min_{\\beta \\in \\mathbb{R}^{2}} \\; \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1},\n$$\nwith penalty parameter $\\lambda=\\frac{6}{5}$.\n- The ridge regression estimator $\\hat{\\beta}^{\\text{Ridge}}(\\alpha)$ defined as the unique minimizer of\n$$\n\\min_{\\beta \\in \\mathbb{R}^{2}} \\; \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\frac{\\alpha}{2} \\|\\beta\\|_{2}^{2},\n$$\nwith penalty parameter $\\alpha=\\frac{1}{2}$.\n\nCompute both coefficient vectors exactly and report them as a single row vector $\\big[\\hat{\\beta}_{1}^{\\text{LASSO}}, \\hat{\\beta}_{2}^{\\text{LASSO}}, \\hat{\\beta}_{1}^{\\text{Ridge}}, \\hat{\\beta}_{2}^{\\text{Ridge}}\\big]$. Give exact values; do not round. Your final answer must be this single row vector.", "solution": "The problem requires the computation of two regularized linear regression coefficient vectors: the Ridge estimator and the Lasso estimator. The problem is well-defined, scientifically sound, and all necessary data are provided. We shall solve for each estimator separately.\n\nThe given data are:\nResponse vector $y = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\nPredictor matrix $X = \\begin{pmatrix} 1  2 \\\\ 0  1 \\end{pmatrix}$.\nThe coefficient vector is $\\beta = \\begin{pmatrix} \\beta_1 \\\\ \\beta_2 \\end{pmatrix}$.\n\nFirst, we compute some necessary matrix products.\nThe transpose of $X$ is $X^T = \\begin{pmatrix} 1  0 \\\\ 2  1 \\end{pmatrix}$.\nThe Gram matrix is $X^T X = \\begin{pmatrix} 1  0 \\\\ 2  1 \\end{pmatrix} \\begin{pmatrix} 1  2 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 1  2 \\\\ 2  5 \\end{pmatrix}$.\nThe product $X^T y$ is $X^T y = \\begin{pmatrix} 1  0 \\\\ 2  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$.\n\n**1. Ridge Regression Estimator**\n\nThe ridge regression objective function is given by:\n$$L_{\\text{Ridge}}(\\beta) = \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\frac{\\alpha}{2} \\|\\beta\\|_{2}^{2}$$\nwith the penalty parameter $\\alpha = \\frac{1}{2}$.\nThis objective function is strictly convex and differentiable. The minimizer $\\hat{\\beta}^{\\text{Ridge}}$ is found by setting the gradient with respect to $\\beta$ to zero:\n$$\\nabla_{\\beta} L_{\\text{Ridge}} = -X^T(y - X\\beta) + \\alpha\\beta = 0$$\nRearranging the terms gives the normal equations for ridge regression:\n$$(X^T X + \\alpha I) \\beta = X^T y$$\nwhere $I$ is the $2 \\times 2$ identity matrix. The solution is thus:\n$$\\hat{\\beta}^{\\text{Ridge}} = (X^T X + \\alpha I)^{-1} X^T y$$\nWe substitute the given values:\n$$X^T X + \\alpha I = \\begin{pmatrix} 1  2 \\\\ 2  5 \\end{pmatrix} + \\frac{1}{2} \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 1 + \\frac{1}{2}  2 \\\\ 2  5 + \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{2}  2 \\\\ 2  \\frac{11}{2} \\end{pmatrix}$$\nTo find the inverse of this matrix, we first compute its determinant:\n$$\\det(X^T X + \\alpha I) = \\left(\\frac{3}{2}\\right)\\left(\\frac{11}{2}\\right) - (2)(2) = \\frac{33}{4} - 4 = \\frac{33 - 16}{4} = \\frac{17}{4}$$\nThe inverse is:\n$$(X^T X + \\alpha I)^{-1} = \\frac{1}{\\frac{17}{4}} \\begin{pmatrix} \\frac{11}{2}  -2 \\\\ -2  \\frac{3}{2} \\end{pmatrix} = \\frac{4}{17} \\begin{pmatrix} \\frac{11}{2}  -2 \\\\ -2  \\frac{3}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{22}{17}  -\\frac{8}{17} \\\\ -\\frac{8}{17}  \\frac{6}{17} \\end{pmatrix}$$\nNow we can compute the ridge estimator:\n$$\\hat{\\beta}^{\\text{Ridge}} = \\begin{pmatrix} \\frac{22}{17}  -\\frac{8}{17} \\\\ -\\frac{8}{17}  \\frac{6}{17} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} \\frac{22}{17} - \\frac{16}{17} \\\\ -\\frac{8}{17} + \\frac{12}{17} \\end{pmatrix} = \\begin{pmatrix} \\frac{6}{17} \\\\ \\frac{4}{17} \\end{pmatrix}$$\nThus, $\\hat{\\beta}_{1}^{\\text{Ridge}} = \\frac{6}{17}$ and $\\hat{\\beta}_{2}^{\\text{Ridge}} = \\frac{4}{17}$.\n\n**2. Lasso Estimator**\n\nThe Lasso objective function is:\n$$L_{\\text{Lasso}}(\\beta) = \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1}$$\nwith the penalty parameter $\\lambda = \\frac{6}{5}$. Due to the non-differentiability of the $L_1$-norm, we use subgradient optimality conditions. The minimizer $\\hat{\\beta}^{\\text{LASSO}}$ must satisfy $0 \\in \\partial L_{\\text{Lasso}}(\\hat{\\beta})$.\nThe subgradient condition is $X^T(y - X\\hat{\\beta})_j \\in \\lambda \\cdot \\partial |\\beta_j| |_{\\hat{\\beta}_j}$ for each component $j \\in \\{1, 2\\}$. This can be written as:\n$$\n\\begin{cases}\n(X^T(y - X\\hat{\\beta}))_j = \\lambda \\cdot \\text{sign}(\\hat{\\beta}_j)  \\text{if } \\hat{\\beta}_j \\neq 0 \\\\\n|(X^T(y - X\\hat{\\beta}))_j| \\le \\lambda  \\text{if } \\hat{\\beta}_j = 0\n\\end{cases}\n$$\nLet's compute the vector of correlations with the residuals, $c(\\beta) = X^T(y - X\\beta)$:\n$$c(\\beta) = X^T y - X^T X \\beta = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} - \\begin{pmatrix} 1  2 \\\\ 2  5 \\end{pmatrix}\\begin{pmatrix} \\beta_1 \\\\ \\beta_2 \\end{pmatrix} = \\begin{pmatrix} 1 - \\beta_1 - 2\\beta_2 \\\\ 2 - 2\\beta_1 - 5\\beta_2 \\end{pmatrix}$$\nWe analyze the possible cases for the active set of coefficients.\n\nCase 1: $\\hat{\\beta}_1 = 0$, $\\hat{\\beta}_2 \\ne 0$.\nThe conditions are $|c_1(\\hat{\\beta})| \\le \\lambda$ and $c_2(\\hat{\\beta}) = \\lambda \\cdot \\text{sign}(\\hat{\\beta}_2)$.\nWith $\\hat\\beta_1 = 0$, we have $c_2 = 2 - 5\\hat{\\beta}_2$.\nIf $\\hat{\\beta}_2  0$: $2 - 5\\hat{\\beta}_2 = \\lambda = \\frac{6}{5} \\implies 5\\hat{\\beta}_2 = 2 - \\frac{6}{5} = \\frac{4}{5} \\implies \\hat{\\beta}_2 = \\frac{4}{25}$. This is consistent with $\\hat{\\beta}_2  0$.\nWe must check the condition for $\\hat{\\beta}_1=0$: $|c_1| \\le \\lambda$.\n$|c_1| = |1 - 0 - 2\\hat{\\beta}_2| = |1 - 2(\\frac{4}{25})| = |1 - \\frac{8}{25}| = |\\frac{17}{25}| = \\frac{17}{25}$.\nThe condition is $\\frac{17}{25} \\le \\frac{6}{5}$. Since $\\frac{6}{5} = \\frac{30}{25}$, we have $\\frac{17}{25} \\le \\frac{30}{25}$, which is true.\nSo, $\\hat{\\beta}^{\\text{LASSO}} = \\begin{pmatrix} 0 \\\\ \\frac{4}{25} \\end{pmatrix}$ is a valid solution.\n\nIf $\\hat{\\beta}_2  0$: $2 - 5\\hat{\\beta}_2 = -\\lambda = -\\frac{6}{5} \\implies 5\\hat{\\beta}_2 = 2 + \\frac{6}{5} = \\frac{16}{5} \\implies \\hat{\\beta}_2 = \\frac{16}{25}$. This contradicts the assumption $\\hat{\\beta}_2  0$.\n\nCase 2: $\\hat{\\beta}_1 \\ne 0$, $\\hat{\\beta}_2 = 0$.\nThe conditions are $c_1(\\hat{\\beta}) = \\lambda \\cdot \\text{sign}(\\hat{\\beta}_1)$ and $|c_2(\\hat{\\beta})| \\le \\lambda$.\nWith $\\hat\\beta_2 = 0$, we have $c_1 = 1 - \\hat\\beta_1$.\nIf $\\hat{\\beta}_1  0$: $1 - \\hat{\\beta}_1 = \\lambda = \\frac{6}{5} \\implies \\hat{\\beta}_1 = 1 - \\frac{6}{5} = -\\frac{1}{5}$. Contradiction.\nIf $\\hat{\\beta}_1  0$: $1 - \\hat{\\beta}_1 = -\\lambda = -\\frac{6}{5} \\implies \\hat{\\beta}_1 = 1 + \\frac{6}{5} = \\frac{11}{5}$. Contradiction.\nThis case yields no solution.\n\nCase 3: $\\hat{\\beta}_1 \\ne 0$, $\\hat{\\beta}_2 \\ne 0$.\nThis requires solving $c_j(\\hat{\\beta}) = \\lambda \\cdot \\text{sign}(\\hat{\\beta}_j)$ for $j=1,2$.\n1) $\\hat{\\beta}_1  0, \\hat{\\beta}_2  0$: $\\hat{\\beta}_1 + 2\\hat{\\beta}_2 = 1 - \\frac{6}{5} = -\\frac{1}{5}$. The sum of two positive numbers cannot be negative. Contradiction.\n2) $\\hat{\\beta}_1  0, \\hat{\\beta}_2  0$: $\\hat{\\beta}_1 + 2\\hat{\\beta}_2 = 1 - \\frac{6}{5} = -\\frac{1}{5}$ and $2\\hat{\\beta}_1 + 5\\hat{\\beta}_2 = 2 - (-\\frac{6}{5}) = \\frac{16}{5}$. Solving this system yields $\\hat{\\beta}_2 = \\frac{18}{5}$, which contradicts $\\hat{\\beta}_2  0$.\n3) $\\hat{\\beta}_1  0, \\hat{\\beta}_2  0$: $\\hat{\\beta}_1 + 2\\hat{\\beta}_2 = 1 - (-\\frac{6}{5}) = \\frac{11}{5}$ and $2\\hat{\\beta}_1 + 5\\hat{\\beta}_2 = 2 - \\frac{6}{5} = \\frac{4}{5}$. Solving this system yields $\\hat{\\beta}_2 = -\\frac{18}{5}$, which contradicts $\\hat{\\beta}_2  0$.\n4) $\\hat{\\beta}_1  0, \\hat{\\beta}_2  0$: $\\hat{\\beta}_1 + 2\\hat{\\beta}_2 = 1 - (-\\frac{6}{5}) = \\frac{11}{5}$. The sum of two negative numbers cannot be positive. Contradiction.\nNo solution exists with both coefficients being non-zero.\n\nSince the Lasso objective function is strictly convex (as $X$ has full rank), a unique minimizer must exist. Our analysis of all possible active sets yielded only one valid solution.\nTherefore, the Lasso estimator is $\\hat{\\beta}^{\\text{LASSO}} = \\begin{pmatrix} 0 \\\\ \\frac{4}{25} \\end{pmatrix}$.\nSo, $\\hat{\\beta}_{1}^{\\text{LASSO}} = 0$ and $\\hat{\\beta}_{2}^{\\text{LASSO}} = \\frac{4}{25}$.\n\n**Final Answer Assembly**\nThe problem requests the final answer as a single row vector $\\big[\\hat{\\beta}_{1}^{\\text{LASSO}}, \\hat{\\beta}_{2}^{\\text{LASSO}}, \\hat{\\beta}_{1}^{\\text{Ridge}}, \\hat{\\beta}_{2}^{\\text{Ridge}}\\big]$.\nSubstituting the computed values:\n$$\\begin{pmatrix} 0  \\frac{4}{25}  \\frac{6}{17}  \\frac{4}{17} \\end{pmatrix}$$\nThis is the final result.", "answer": "$$\n\\boxed{\\begin{pmatrix} 0  \\frac{4}{25}  \\frac{6}{17}  \\frac{4}{17} \\end{pmatrix}}\n$$", "id": "2426291"}, {"introduction": "To truly master regularized regression, it is invaluable to implement the underlying optimization algorithm. This practice guides you through building a coordinate descent solver for the Elastic Net model, which combines the $L_1$ and $L_2$ penalties of Lasso and Ridge into a single powerful framework [@problem_id:2426260]. By coding the update rule and validating your solution, you will develop a robust, practical understanding of how these widely used models are fitted to data.", "problem": "You are given the Elastic Net regression problem defined as follows. For a data matrix $X \\in \\mathbb{R}^{n \\times p}$ and a response vector $y \\in \\mathbb{R}^{n}$, consider the optimization problem\n$$\n\\min_{b \\in \\mathbb{R}^p} \\; \\frac{1}{2n} \\lVert y - X b \\rVert_2^2 \\;+\\; \\lambda_1 \\lVert b \\rVert_1 \\;+\\; \\frac{\\lambda_2}{2} \\lVert b \\rVert_2^2,\n$$\nwhere $n$ and $p$ are positive integers, $X$ and $y$ are given, and $\\lambda_1 \\ge 0$, $\\lambda_2 \\ge 0$ are penalty parameters. The task is motivated by model selection and overfitting control in computational economics and finance, where predictors can be highly correlated and the number of variables can be comparable to or larger than the number of observations.\n\nYour program must, for the specified test instances below, compute solutions to the stated optimization problem using a method that updates one coefficient at a time while holding the others fixed, and must demonstrate the hybrid nature of the per-coordinate update in relation to Ridge regression and Least Absolute Shrinkage and Selection Operator (Lasso). The program must produce the outputs prescribed below and must not require any user input.\n\nAll computations are purely numerical with no physical units. Angles and percentages do not appear in this problem.\n\nUse the following test suite consisting of four cases. In every case, $n$ and $p$ are specified by the shapes of $X$ and $y$.\n\n- Case $1$ (Ridge-only consistency on a general design):\n  - Data:\n    $$\n    X_1 = \\begin{bmatrix}\n    1  0  1 \\\\\n    1  1  1 \\\\\n    1  2  1 \\\\\n    1  3  2 \\\\\n    1  4  3 \\\\\n    1  5  5\n    \\end{bmatrix}, \\quad\n    b^{\\mathrm{true}} = \\begin{bmatrix} 0.7 \\\\ 1.3 \\\\ -0.8 \\end{bmatrix}, \\quad\n    y_1 = X_1 b^{\\mathrm{true}}.\n    $$\n  - Penalties: $\\lambda_1 = 0$, $\\lambda_2 = 0.8$.\n  - Required scalar result for this case: a boolean $r_1$ that is true if the solution obtained by your algorithm matches the unique Ridge closed-form solution $b^{\\mathrm{ridge}}$ (defined by solving $(X^\\top X / n + \\lambda_2 I) b = (X^\\top y)/n$) to within an absolute tolerance of $10^{-9}$ in each coefficient, and false otherwise.\n\n- Case $2$ (Lasso-only on an orthonormal design):\n  - Data:\n    $$\n    X_2 = \\begin{bmatrix}\n    2  0  0 \\\\\n    0  2  0 \\\\\n    0  0  2 \\\\\n    0  0  0\n    \\end{bmatrix}, \\quad\n    y_2 = \\begin{bmatrix} 3.2 \\\\ -0.5 \\\\ 1.1 \\\\ 0 \\end{bmatrix}.\n    $$\n    Note that $(1/n) X_2^\\top X_2 = I_3$ with $n = 4$.\n  - Penalties: $\\lambda_1 = 0.6$, $\\lambda_2 = 0$.\n  - Required scalar result for this case: a boolean $r_2$ that is true if the solution obtained by your algorithm matches the known Lasso solution for orthonormal designs, namely the coefficient-wise soft-threshold of the ordinary least squares estimate, to within an absolute tolerance of $10^{-12}$ in each coefficient, and false otherwise.\n\n- Case $3$ (General Elastic Net coefficients on the general design of Case $1$):\n  - Data: $X_3 = X_1$, $y_3 = y_1$.\n  - Penalties: $\\lambda_1 = 0.5$, $\\lambda_2 = 0.3$.\n  - Required results for this case: the three coefficients of the Elastic Net solution $b^{\\mathrm{EN}}$ for this case, each numerically rounded to six decimal places. Denote these as $b^{(3)}_1$, $b^{(3)}_2$, $b^{(3)}_3$.\n\n- Case $4$ (Zero-solution edge case via large $\\ell_1$ penalty):\n  - Data: $X_4 = X_1$, $y_4 = y_1$.\n  - Penalties: $\\lambda_1 = 8.0$, $\\lambda_2 = 0.1$.\n  - Required scalar result for this case: a boolean $r_4$ that is true if the computed Elastic Net solution is the zero vector to within an absolute tolerance of $10^{-10}$ in each coefficient, and false otherwise.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain, in order,\n$$\n\\big[ r_1, \\; r_2, \\; b^{(3)}_1, \\; b^{(3)}_2, \\; b^{(3)}_3, \\; r_4 \\big].\n$$", "solution": "The problem statement is subjected to validation and is found to be valid. It is scientifically grounded in the established theory of regularized linear regression, well-posed, objective, and provides a complete and consistent set of definitions and data for a solvable numerical task.\n\nThe problem asks for the solution to the Elastic Net optimization problem, which is defined by the objective function:\n$$\nL(b) = \\frac{1}{2n} \\lVert y - X b \\rVert_2^2 \\;+\\; \\lambda_1 \\lVert b \\rVert_1 \\;+\\; \\frac{\\lambda_2}{2} \\lVert b \\rVert_2^2\n$$\nHere, $y \\in \\mathbb{R}^{n}$ is the response vector, $X \\in \\mathbb{R}^{n \\times p}$ is the data matrix, $b \\in \\mathbb{R}^{p}$ is the coefficient vector to be optimized, and $\\lambda_1 \\ge 0$, $\\lambda_2 \\ge 0$ are non-negative regularization parameters. The term $\\lVert \\cdot \\rVert_1$ is the $\\ell_1$-norm (sum of absolute values), and $\\lVert \\cdot \\rVert_2$ is the $\\ell_2$-norm (Euclidean norm).\n\nThe problem specifies using a coordinate-wise update method, which is commonly known as coordinate descent. This method iteratively optimizes the objective function with respect to a single coefficient $b_j$ at a time, while keeping all other coefficients $b_{k \\neq j}$ fixed.\n\nTo derive the update rule for a single coefficient $b_j$, we consider the objective function $L(b)$ as a function of $b_j$ only, treating all other components of $b$ as constants.\nLet $r = y - Xb$ be the current residual vector. We can express the sum of squares term as:\n$$\n\\lVert y - X b \\rVert_2^2 = \\left\\lVert y - \\sum_{k=1}^p X_k b_k \\right\\rVert_2^2 = \\left\\lVert \\left(y - \\sum_{k \\neq j} X_k b_k\\right) - X_j b_j \\right\\rVert_2^2\n$$\nLet the partial residual be $r^{(j)} = y - \\sum_{k \\neq j} X_k b_k$. This can be computed from the full residual $r$ and the previous value of $b_j$, denoted $b_j^{\\text{old}}$, as $r^{(j)} = r + X_j b_j^{\\text{old}}$.\nThe objective function, as a function of $b_j$, is:\n$$\nL(b_j) = \\frac{1}{2n} \\lVert r^{(j)} - X_j b_j \\rVert_2^2 + \\lambda_1 |b_j| + \\frac{\\lambda_2}{2} b_j^2 + \\text{constants}\n$$\nExpanding the squared norm, we get:\n$$\nL(b_j) = \\frac{1}{2n} \\left( (r^{(j)})^\\top r^{(j)} - 2b_j (X_j)^\\top r^{(j)} + b_j^2 (X_j)^\\top X_j \\right) + \\lambda_1 |b_j| + \\frac{\\lambda_2}{2} b_j^2 + \\text{constants}\n$$\nTo find the minimum, we take the subgradient of $L(b_j)$ with respect to $b_j$ and set it to $0$. The subgradient $\\partial L(b_j) / \\partial b_j$ is:\n$$\n\\frac{\\partial L(b_j)}{\\partial b_j} = \\frac{1}{n} \\left( - (X_j)^\\top r^{(j)} + b_j (X_j)^\\top X_j \\right) + \\lambda_1 \\text{sgn}(b_j) + \\lambda_2 b_j = 0\n$$\nwhere $\\text{sgn}(b_j)$ is part of the subgradient of $|b_j|$, taking values in $[-1, 1]$ if $b_j=0$.\nLet $\\rho_j = \\frac{1}{n} (X_j)^\\top r^{(j)}$ and $a_j = \\frac{1}{n} (X_j)^\\top X_j$. The equation becomes:\n$$\n-\\rho_j + b_j a_j + \\lambda_2 b_j + \\lambda_1 \\text{sgn}(b_j) = 0 \\implies b_j (a_j + \\lambda_2) = \\rho_j - \\lambda_1 \\text{sgn}(b_j)\n$$\nThis is a one-dimensional LASSO-type problem. The solution is given by the soft-thresholding operator $S(z, \\gamma) = \\text{sgn}(z) \\max(|z|-\\gamma, 0)$. The update for $b_j$ is:\n$$\nb_j \\leftarrow \\frac{S(\\rho_j, \\lambda_1)}{a_j + \\lambda_2}\n$$\nFor computational efficiency, $\\rho_j = \\frac{1}{n} (X_j)^\\top (y - \\sum_{k \\neq j} X_k b_k)$ can be calculated as $\\rho_j = \\frac{1}{n}(X^\\top y)_j - \\sum_{k \\neq j} \\frac{1}{n}(X^\\top X)_{jk} b_k$. By pre-computing the matrices $\\frac{1}{n}X^\\top X$ and $\\frac{1}{n}X^\\top y$, the update for each $b_j$ becomes computationally inexpensive within the iterative loop. The algorithm cycles through all coefficients $j=1, \\dots, p$ repeatedly until the coefficient vector $b$ converges.\n\nThe algorithm will be applied to the four test cases as specified.\n\nCase $1$: Ridge-only consistency.\nWith penalties $\\lambda_1 = 0$ and $\\lambda_2 = 0.8$, the problem reduces to Ridge regression. The update rule simplifies to $b_j \\leftarrow \\rho_j / (a_j + \\lambda_2)$, as $S(\\rho_j, 0) = \\rho_j$. The coordinate descent algorithm is expected to converge to the unique global minimum of the Ridge objective function. This solution is compared to the analytical closed-form solution for Ridge regression, $b^{\\mathrm{ridge}} = ( \\frac{1}{n} X^\\top X + \\lambda_2 I )^{-1} (\\frac{1}{n} X^\\top y)$. A boolean result $r_1$ will be true if the computed coefficients match the analytical solution to within tolerance $10^{-9}$.\n\nCase $2$: Lasso-only on an orthonormal design.\nWith penalties $\\lambda_1 = 0.6$ and $\\lambda_2 = 0$, the problem is Lasso regression. The design matrix $X_2$ has the property that $\\frac{1}{n} X_2^\\top X_2 = I_3$, where $n=4$. For such a design, the objective function decouples across the coefficients, and the exact solution is known to be the soft-thresholded Ordinary Least Squares (OLS) estimate. The OLS estimate is $b^{\\mathrm{OLS}} = (\\frac{1}{n}X_2^\\top X_2)^{-1}(\\frac{1}{n}X_2^\\top y_2) = \\frac{1}{n}X_2^\\top y_2$. The Lasso solution is therefore $b^{\\mathrm{LASSO}}_j = S((b^{\\mathrm{OLS}})_j, \\lambda_1)$. The implemented coordinate descent algorithm is expected to converge to this exact solution. A boolean result $r_2$ will be true if the match is within tolerance $10^{-12}$.\n\nCase $3$: General Elastic Net.\nWith $\\lambda_1 = 0.5$ and $\\lambda_2 = 0.3$, this is a general Elastic Net problem. The full coordinate descent algorithm as derived is applied. The resulting coefficients of the vector $b^{\\mathrm{EN}}$, denoted $b^{(3)}_1, b^{(3)}_2, b^{(3)}_3$, are the required output, rounded to six decimal places.\n\nCase $4$: Zero-solution edge case.\nWith a large $\\ell_1$ penalty $\\lambda_1 = 8.0$ and $\\lambda_2 = 0.1$, it is possible for the solution to be the zero vector, $b=0$. The condition for $b=0$ to be the optimal solution is that the subgradient of the objective function at $b=0$ must contain the zero vector. This condition is $| \\frac{1}{n}(X^\\top y)_j | \\le \\lambda_1$ for all $j=1, \\dots, p$. For the given data and $\\lambda_1 = 8.0$, we compute $\\frac{1}{n} X_1^\\top y_1$ and find that this condition holds for all three components. Therefore, the coordinate descent algorithm is expected to converge to $b=0$. A boolean result $r_4$ will be true if the norm of each computed coefficient is less than the tolerance $10^{-10}$.\n\nThe implementation will proceed by first defining the data for all cases, then executing the coordinate descent solver for each parameter set and performing the specified validation checks or extracting the required coefficient values.", "answer": "```python\nimport numpy as np\n\ndef soft_threshold(rho, lam):\n    \"\"\"Soft-thresholding operator.\"\"\"\n    return np.sign(rho) * np.maximum(np.abs(rho) - lam, 0.)\n\ndef solve_elastic_net(X, y, lambda1, lambda2, max_iter=100000, tol=1e-15):\n    \"\"\"\n    Solves the Elastic Net problem using coordinate descent.\n\n    min_b (1/2n) ||y - Xb||^2_2 + lambda1 ||b||_1 + (lambda2/2) ||b||^2_2\n    \"\"\"\n    n, p = X.shape\n    b = np.zeros(p)\n    \n    # Pre-compute matrices for efficiency\n    XTX = (X.T @ X) / n\n    XTy = (X.T @ y) / n\n    \n    # Denominators for the update rule\n    denominators = np.diag(XTX) + lambda2\n\n    for _ in range(max_iter):\n        b_old_cycle = b.copy()\n        \n        for j in range(p):\n            # Calculate rho_j using pre-computed matrices\n            # rho_j = XTy[j] - (XTX_row_j . b - XTX_jj * b_j)\n            rho_j = XTy[j] - (np.dot(XTX[j, :], b) - XTX[j, j] * b[j])\n            \n            if denominators[j] == 0:\n                 # This case is unlikely with non-zero columns in X or lambda2 > 0\n                b[j] = 0.\n            else:\n                b[j] = soft_threshold(rho_j, lambda1) / denominators[j]\n        \n        # Check for convergence\n        max_change = np.max(np.abs(b - b_old_cycle))\n        if max_change  tol:\n            break\n            \n    return b\n\ndef solve():\n    # --- Define common data ---\n    X1 = np.array([\n        [1.0, 0.0, 1.0],\n        [1.0, 1.0, 1.0],\n        [1.0, 2.0, 1.0],\n        [1.0, 3.0, 2.0],\n        [1.0, 4.0, 3.0],\n        [1.0, 5.0, 5.0]\n    ])\n    b_true = np.array([0.7, 1.3, -0.8])\n    y1 = X1 @ b_true  # As per problem, y1 is defined by this product\n\n    # --- Case 1: Ridge-only consistency ---\n    lambda1_c1, lambda2_c1 = 0.0, 0.8\n    b1_my = solve_elastic_net(X1, y1, lambda1_c1, lambda2_c1)\n    \n    n1, p1 = X1.shape\n    A = (X1.T @ X1) / n1 + lambda2_c1 * np.identity(p1)\n    v = (X1.T @ y1) / n1\n    b1_ridge = np.linalg.solve(A, v)\n    \n    r1 = np.allclose(b1_my, b1_ridge, atol=1e-9)\n\n    # --- Case 2: LASSO-only on an orthonormal design ---\n    X2 = np.array([\n        [2.0, 0.0, 0.0],\n        [0.0, 2.0, 0.0],\n        [0.0, 0.0, 2.0],\n        [0.0, 0.0, 0.0]\n    ])\n    y2 = np.array([3.2, -0.5, 1.1, 0.0])\n    lambda1_c2, lambda2_c2 = 0.6, 0.0\n    \n    b2_my = solve_elastic_net(X2, y2, lambda1_c2, lambda2_c2)\n    \n    n2 = X2.shape[0]\n    b_ols = (X2.T @ y2) / n2\n    b2_lasso = soft_threshold(b_ols, lambda1_c2)\n    \n    r2 = np.allclose(b2_my, b2_lasso, atol=1e-12)\n\n    # --- Case 3: General Elastic Net coefficients ---\n    X3, y3 = X1, y1\n    lambda1_c3, lambda2_c3 = 0.5, 0.3\n    \n    b3_en = solve_elastic_net(X3, y3, lambda1_c3, lambda2_c3)\n    b3_1 = round(b3_en[0], 6)\n    b3_2 = round(b3_en[1], 6)\n    b3_3 = round(b3_en[2], 6)\n\n    # --- Case 4: Zero-solution edge case ---\n    X4, y4 = X1, y1\n    lambda1_c4, lambda2_c4 = 8.0, 0.1\n    \n    b4_en = solve_elastic_net(X4, y4, lambda1_c4, lambda2_c4)\n    r4 = np.allclose(b4_en, np.zeros(X4.shape[1]), atol=1e-10)\n    \n    # --- Final Output Formatting ---\n    results = [\n        str(r1).lower(),\n        str(r2).lower(),\n        f\"{b3_1:.6f}\",\n        f\"{b3_2:.6f}\",\n        f\"{b3_3:.6f}\",\n        str(r4).lower()\n    ]\n    \n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2426260"}]}