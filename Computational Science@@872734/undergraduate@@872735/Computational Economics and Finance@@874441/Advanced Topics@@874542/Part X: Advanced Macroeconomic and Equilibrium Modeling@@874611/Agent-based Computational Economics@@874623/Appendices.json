{"hands_on_practices": [{"introduction": "This exercise expands on the classic El Farol Bar problem, a famous paradigm for studying coordination challenges under uncertainty. You will implement an agent-based model where individuals learn to attend a venue with limited capacity, but with an added layer of complexity: agents can form coalitions that coordinate their actions [@problem_id:2370498]. This practice is valuable for understanding how simple, adaptive learning rules can lead to complex emergent patterns and how meso-level social structures can shape collective behavior.", "problem": "You are asked to implement and analyze an agent-based model of the El Farol Bar problem in which agents may be organized into coalitions that can coordinate their actions with a given probability of successful intra-coalition communication. The model must be simulated for a fixed number of discrete periods, with well-defined agent behavior, coalition activation, and payoff-based learning. All randomness must be made reproducible by explicit seeding of the pseudo-random number generator.\n\nModel definition:\n- There are $N$ agents indexed by $i \\in \\{0,1,\\dots,N-1\\}$ and a bar with capacity $C \\in \\mathbb{N}$. Time proceeds in discrete periods $t=0,1,\\dots,T-1$.\n- Each agent $i$ chooses an action $a_i(t) \\in \\{0,1\\}$ each period, where $a_i(t)=1$ denotes attending and $a_i(t)=0$ denotes staying home. Let the aggregate attendance be $A(t)=\\sum_{i=0}^{N-1} a_i(t)$.\n- Agents are partitioned exogenously into disjoint coalitions $g \\in \\mathcal{G}$ that form a complete partition of the agent set. A coalition $g$ has size $|g| \\ge 1$. Coalitions with $|g|=1$ are singletons. For each coalition $g$ with $|g| \\ge 2$, define its quota $q_g = \\left\\lfloor C \\cdot \\frac{|g|}{N} \\right\\rfloor$. For singletons with $|g|=1$, no coordination is ever performed and no quota is applied.\n- Each agent $i$ maintains two attractions (propensities), $A_i^{\\text{att}}(t)$ for attending and $A_i^{\\text{stay}}(t)$ for staying, initialized at $A_i^{\\text{att}}(0)=0$ and $A_i^{\\text{stay}}(0)=0$. Define parameters $\\rho \\in (0,1)$ (recency) and $\\lambda  0$ (learning step size).\n- Private intention phase: At the beginning of each period $t$, each agent forms a private intention $b_i(t) \\in \\{0,1\\}$ based on the attractions: if $A_i^{\\text{att}}(t)  A_i^{\\text{stay}}(t)$ then $b_i(t)=1$, if $A_i^{\\text{att}}(t)  A_i^{\\text{stay}}(t)$ then $b_i(t)=0$, and if $A_i^{\\text{att}}(t)=A_i^{\\text{stay}}(t)$ then $b_i(t)$ is chosen as a fair Bernoulli coin flip. Ties and all coin flips must use the specified pseudo-random generator and seed for reproducibility.\n- Coalition activation and coordination: For each coalition $g$ with $|g|\\ge 2$, an independent Bernoulli trial with success probability $p_{\\text{comm}} \\in [0,1]$ determines whether $g$ is active at period $t$. If inactive, all members take their private intentions as final actions. If active at period $t$, the coalition observes the vector of private intentions $\\{b_i(t): i \\in g\\}$ and applies the following downward-adjustment rule to produce the final actions $\\{a_i(t): i \\in g\\}$:\n  1. Let $S_g(t)=\\sum_{i \\in g} b_i(t)$ be the number of members intending to attend.\n  2. If $S_g(t) \\le q_g$, then $a_i(t)=b_i(t)$ for all $i \\in g$.\n  3. If $S_g(t)  q_g$, then among the subset $\\{i \\in g: b_i(t)=1\\}$, select exactly $q_g$ members to attend, choosing those with the largest values of $\\Delta_i(t)=A_i^{\\text{att}}(t)-A_i^{\\text{stay}}(t)$; ties in $\\Delta_i(t)$ are broken by choosing the smaller agent index first. Set $a_i(t)=1$ for the selected $q_g$ members and $a_i(t)=0$ for the remaining members of this subset. For agents with $b_i(t)=0$, set $a_i(t)=0$.\n- For any singleton coalition $g$ with $|g|=1$, the final action is $a_i(t)=b_i(t)$ for its sole member $i$.\n- Payoff and learning: After all coalitions resolve, aggregate attendance $A(t)=\\sum_i a_i(t)$ is realized. Each agent’s period payoff $u_i(t)$ is\n  $$u_i(t)=\\begin{cases}\n  1  \\text{if } a_i(t)=1 \\text{ and } A(t) \\le C,\\\\\n  1  \\text{if } a_i(t)=0 \\text{ and } A(t)  C,\\\\\n  0  \\text{otherwise.}\n  \\end{cases}$$\n  Attractions update as follows:\n  $$A_i^{\\text{att}}(t+1)=\\begin{cases}\n  (1-\\rho)\\,A_i^{\\text{att}}(t) + \\lambda\\,u_i(t)  \\text{if } a_i(t)=1,\\\\\n  (1-\\rho)\\,A_i^{\\text{att}}(t)  \\text{if } a_i(t)=0,\n  \\end{cases}$$\n  $$A_i^{\\text{stay}}(t+1)=\\begin{cases}\n  (1-\\rho)\\,A_i^{\\text{stay}}(t) + \\lambda\\,u_i(t)  \\text{if } a_i(t)=0,\\\\\n  (1-\\rho)\\,A_i^{\\text{stay}}(t)  \\text{if } a_i(t)=1.\n  \\end{cases}$$\n\nMeasurement definitions:\n- For a given horizon $T$ and a tail window length $H$ with $1 \\le H \\le T$, define the trailing averages over periods $t \\in \\{T-H, T-H+1, \\dots, T-1\\}$:\n  1. The mean attendance gap $\\bar{G} = \\frac{1}{H}\\sum_{t=T-H}^{T-1} \\left( A(t) - C \\right)$.\n  2. The overcrowding frequency $\\bar{O} = \\frac{1}{H}\\sum_{t=T-H}^{T-1} \\mathbf{1}\\{A(t)  C\\}$, expressed as a decimal in $[0,1]$.\n  3. The average per-period coordination flip share $\\bar{F} = \\frac{1}{H}\\sum_{t=T-H}^{T-1} \\left( \\frac{1}{N} \\sum_{i=0}^{N-1} \\mathbf{1}\\{a_i(t) \\ne b_i(t)\\} \\right)$, i.e., the average fraction of agents whose final action differs from their private intention due to coalition coordination.\n\nRandomness and reproducibility:\n- All randomness, including tie-breaking in intentions and coalition activation, must be generated by a pseudo-random number generator seeded as specified for each test case. Use the given seed exactly as the initialization seed for the random number generator in that test case.\n\nTest suite:\nImplement the simulation and compute $(\\bar{G},\\bar{O},\\bar{F})$ for each of the following four parameter sets. In each case, report the results computed over the last $H$ periods, in the order specified.\n\n- Case $1$: $N=51$, $C=25$, $T=2000$, $H=500$, $\\rho=0.01$, $\\lambda=0.2$, coalition sizes given by the list with $51$ entries all equal to $1$ (every agent is a singleton), $p_{\\text{comm}}=0.5$, seed $=42$.\n- Case $2$: $N=51$, $C=25$, $T=2000$, $H=500$, $\\rho=0.01$, $\\lambda=0.2$, coalition sizes given by the list $[3,3,3,3,3,3,3,30]$ (in order of increasing agent index), $p_{\\text{comm}}=1.0$, seed $=43$.\n- Case $3$: $N=51$, $C=25$, $T=2000$, $H=500$, $\\rho=0.01$, $\\lambda=0.2$, coalition sizes given by the list $[3,3,3,3,3,3,3,30]$, $p_{\\text{comm}}=0.0$, seed $=44$.\n- Case $4$: $N=51$, $C=25$, $T=2000$, $H=500$, $\\rho=0.01$, $\\lambda=0.2$, coalition sizes given by the list with ten entries equal to $5$ followed by one entry equal to $1$ (i.e., $[5,5,5,5,5,5,5,5,5,5,1]$), $p_{\\text{comm}}=0.8$, seed $=45$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case’s result must itself be a list of three floating-point numbers $[\\bar{G},\\bar{O},\\bar{F}]$, in the same order as the test cases above. For example, the overall output format must be\n$[[g_1,o_1,f_1],[g_2,o_2,f_2],[g_3,o_3,f_3],[g_4,o_4,f_4]]$,\nwhere each $g_k$, $o_k$, and $f_k$ is reported as a decimal number.", "solution": "The problem statement has been rigorously validated and is determined to be well-posed, scientifically grounded within the domain of agent-based computational economics, and internally consistent. All parameters, behavioral rules, and an objective evaluation criterion are specified unambiguously. I will therefore proceed with a complete solution.\n\nThe problem requires the implementation of a discrete-time agent-based simulation model. The system state is defined by the attractions of $N$ agents, which evolve over $T$ periods according to their actions and resulting payoffs. The core of the solution is a computational algorithm that precisely executes the model's dynamics. The design is structured into three main phases: initialization, the main simulation loop, and final measurement.\n\nFirst, an initialization phase establishes the simulation environment. This involves setting up the $N$ agents and partitioning them into disjoint coalitions $\\mathcal{G}$ according to the specified list of coalition sizes. For each coalition $g \\in \\mathcal{G}$ of size $|g| \\ge 2$, a coordination quota $q_g = \\lfloor C \\cdot \\frac{|g|}{N} \\rfloor$ is pre-calculated, where $C$ is the bar capacity. Each agent $i$ is initialized with zero attraction for attending and staying, $A_i^{\\text{att}}(0)=0$ and $A_i^{\\text{stay}}(0)=0$. A pseudo-random number generator is seeded as specified for each test case to ensure reproducibility of all stochastic events.\n\nSecond, the main simulation loop iterates from time $t=0$ to $t=T-1$. Each period $t$ comprises a sequence of steps derived directly from the problem definition:\n\n1.  **Private Intention Formation**: Each agent $i$ forms a private intention $b_i(t) \\in \\{0,1\\}$ to either attend ($1$) or stay home ($0$). This decision is based on a comparison of their current attractions. If $A_i^{\\text{att}}(t)  A_i^{\\text{stay}}(t)$, the intention is $b_i(t)=1$. If $A_i^{\\text{att}}(t)  A_i^{\\text{stay}}(t)$, the intention is $b_i(t)=0$. In the case of a tie, $A_i^{\\text{att}}(t) = A_i^{\\text{stay}}(t)$, the intention $b_i(t)$ is determined by a fair coin flip using the seeded random number generator.\n\n2.  **Coalition Coordination**: The final action $a_i(t)$ is determined. For agents in singleton coalitions ($|g|=1$), the final action is always their private intention, $a_i(t) = b_i(t)$. For each non-singleton coalition $g$ ($|g| \\ge 2$), a Bernoulli trial with success probability $p_{\\text{comm}}$ determines if the coalition is active for coordination. If inactive, all members $i \\in g$ set $a_i(t) = b_i(t)$. If active, the sum of intentions $S_g(t) = \\sum_{i \\in g} b_i(t)$ is compared to the coalition's quota $q_g$. If $S_g(t) \\le q_g$, no coordination is needed and $a_i(t)=b_i(t)$ for all $i \\in g$. However, if $S_g(t)  q_g$, a downward-adjustment rule is applied. Among the $S_g(t)$ members intending to attend, exactly $q_g$ are chosen to attend. The selection criterion is based on sorting these members first in descending order of their attraction difference $\\Delta_i(t) = A_i^{\\text{att}}(t) - A_i^{\\text{stay}}(t)$, and then in ascending order of their agent index $i$ to break ties. These $q_g$ agents have their final action set to $a_i(t)=1$. The remaining $S_g(t) - q_g$ members who intended to attend are \"flipped\" to have a final action of $a_i(t)=0$. Agents who initially intended to stay ($b_i(t)=0$) maintain their action $a_i(t)=0$.\n\n3.  **Payoff and Learning**: Once all final actions are determined, the aggregate attendance $A(t) = \\sum_{i=0}^{N-1} a_i(t)$ is computed. Each agent $i$ receives a payoff $u_i(t)=1$ for a \"correct\" decision (attending a non-overcrowded bar, or staying home from an overcrowded one) and $u_i(t)=0$ otherwise. Subsequently, attractions are updated using the specified Roth-Erev learning rule. For an agent $i$ who chose action $a_i(t) \\in \\{0, 1\\}$, the chosen action's attraction is updated via $A_i^{\\text{chosen}}(t+1) = (1-\\rho)A_i^{\\text{chosen}}(t) + \\lambda u_i(t)$, while the unchosen action's attraction is updated via $A_i^{\\text{unchosen}}(t+1) = (1-\\rho)A_i^{\\text{unchosen}}(t)$. Here, $\\rho$ is the recency parameter and $\\lambda$ is the learning step size.\n\nThird, after the simulation completes, the final analysis is performed. The required metrics—mean attendance gap $\\bar{G}$, overcrowding frequency $\\bar{O}$, and average coordination flip share $\\bar{F}$—are calculated by averaging their respective time series over the final $H$ periods of the simulation, from $t=T-H$ to $t=T-1$. The implementation uses vectorized operations provided by the NumPy library for efficiency in computations involving agent state arrays.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_simulation(N, C, T, H, rho, lambda_param, p_comm, coalition_sizes, seed):\n    \"\"\"\n    Implements and simulates the described agent-based El Farol Bar model with coalitions.\n    \"\"\"\n    # Initialize the pseudo-random number generator for reproducibility.\n    rng = np.random.default_rng(seed)\n\n    # 1. Initialization\n    # Construct coalition structure: a list of lists of agent indices.\n    coalitions = []\n    # Store quotas for non-singleton coalitions, indexed by coalition index.\n    quotas = {}\n    agent_idx_counter = 0\n    for g_idx, size in enumerate(coalition_sizes):\n        members = list(range(agent_idx_counter, agent_idx_counter + size))\n        coalitions.append(members)\n        if size = 2:\n            # Per the problem, quota is floor(C * |g| / N).\n            quotas[g_idx] = int(C * size / N)\n        agent_idx_counter += size\n\n    # Agent state variables initialized to 0.\n    attractions = np.zeros(N, dtype=float)\n    stays = np.zeros(N, dtype=float)\n\n    # Data structures for storing tail-end history for final metrics.\n    attendance_history = []\n    flips_history = []\n\n    # 2. Main Simulation Loop\n    for t in range(T):\n        # 2a. Private Intention Phase\n        deltas = attractions - stays\n        intentions = np.zeros(N, dtype=int)\n        intentions[deltas  0] = 1\n        \n        # Resolve ties with a fair coin flip.\n        tie_indices = np.where(deltas == 0)[0]\n        if len(tie_indices)  0:\n            intentions[tie_indices] = rng.integers(0, 2, size=len(tie_indices))\n        \n        # Final actions start as intentions; may be modified by coordination.\n        actions = intentions.copy()\n\n        # 2b. Coalition Coordination Phase\n        for g_idx, g_members in enumerate(coalitions):\n            # Singleton coalitions (|g|=1) do not coordinate.\n            if len(g_members)  2:\n                continue\n\n            # Bernoulli trial for coalition activation.\n            is_active = rng.random()  p_comm\n            if not is_active:\n                continue\n\n            # Active coalition coordination logic.\n            member_intentions = intentions[g_members]\n            S_g = np.sum(member_intentions)\n            q_g = quotas[g_idx]\n\n            if S_g  q_g:\n                # Identify members who intended to attend.\n                intending_member_local_indices = np.where(member_intentions == 1)[0]\n                global_intending_indices = [g_members[i] for i in intending_member_local_indices]\n                \n                # Create a list of candidates to be sorted.\n                candidates = []\n                for agent_idx in global_intending_indices:\n                    candidates.append((deltas[agent_idx], agent_idx))\n                \n                # Sort: 1st key descending delta, 2nd key ascending agent index.\n                candidates.sort(key=lambda x: (-x[0], x[1]))\n                \n                # The agents beyond the quota are 'flipped' to stay home.\n                for _, agent_idx in candidates[q_g:]:\n                    actions[agent_idx] = 0\n\n        # 2c. Payoff and Learning\n        A_t = np.sum(actions)\n        \n        payoffs = np.zeros(N, dtype=float)\n        actions_mask = (actions == 1) # Boolean mask for agents who attended.\n        \n        if A_t = C:  # Bar not crowded: attenders get payoff 1.\n            payoffs[actions_mask] = 1.0\n        else:  # Bar crowded: stayers get payoff 1.\n            payoffs[~actions_mask] = 1.0\n\n        # Update attractions for t+1 using vectorized operations.\n        # First, apply the recency discount to all.\n        attractions = (1 - rho) * attractions\n        stays = (1 - rho) * stays\n\n        # Then, add the reinforcement term for the chosen action.\n        attractions[actions_mask] += lambda_param * payoffs[actions_mask]\n        stays[~actions_mask] += lambda_param * payoffs[~actions_mask]\n\n        # 2d. Data Recording for Tail Averages\n        if t = T - H:\n            attendance_history.append(A_t)\n            flips_history.append(np.sum(actions != intentions))\n\n    # 3. Final Metrics Calculation\n    attendance_history = np.array(attendance_history, dtype=float)\n    flips_history = np.array(flips_history, dtype=float)\n\n    mean_gap = np.mean(attendance_history - C)\n    overcrowding_freq = np.mean(attendance_history  C)\n    mean_flip_share = np.mean(flips_history / N)\n\n    return [mean_gap, overcrowding_freq, mean_flip_share]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: All agents are singletons.\n        {'N': 51, 'C': 25, 'T': 2000, 'H': 500, 'rho': 0.01, 'lambda_param': 0.2, \n         'coalition_sizes': [1] * 51, 'p_comm': 0.5, 'seed': 42},\n        # Case 2: Coalitions, full communication.\n        {'N': 51, 'C': 25, 'T': 2000, 'H': 500, 'rho': 0.01, 'lambda_param': 0.2,\n         'coalition_sizes': [3,3,3,3,3,3,3,30], 'p_comm': 1.0, 'seed': 43},\n        # Case 3: Coalitions, no communication.\n        {'N': 51, 'C': 25, 'T': 2000, 'H': 500, 'rho': 0.01, 'lambda_param': 0.2,\n         'coalition_sizes': [3,3,3,3,3,3,3,30], 'p_comm': 0.0, 'seed': 44},\n        # Case 4: Another coalition structure, partial communication.\n        {'N': 51, 'C': 25, 'T': 2000, 'H': 500, 'rho': 0.01, 'lambda_param': 0.2,\n         'coalition_sizes': [5]*10 + [1], 'p_comm': 0.8, 'seed': 45},\n    ]\n\n    results = []\n    for params in test_cases:\n        result = run_simulation(**params)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(str(r) for r in results)}]\")\n\nsolve()\n```", "id": "2370498"}, {"introduction": "This practice delves into a central theme of political economy: the conflict between value creation and value extraction, often termed production versus rent-seeking. You will simulate a society of agents who must decide how to allocate their effort between a productive activity and a 'rent-seeking' contest to claim a share of the total output [@problem_id:2370544]. By implementing a model with gradient-based learning, you will explore how individual, myopically rational decisions can aggregate into potentially inefficient macroeconomic outcomes, providing a powerful illustration of the 'tragedy of rent-seeking'.", "problem": "You are asked to design and implement an agent-based computational economics (ACE) simulation of a population of agents who repeatedly choose how to allocate a unit of effort between a productive activity and a rent-seeking activity. The model must be constructed from the following foundational components and rules.\n\nFundamental base and core definitions:\n- There are $N$ agents indexed by $i \\in \\{1,\\dots,N\\}$. Time is discrete, indexed by $t \\in \\{0,1,2,\\dots,T-1\\}$.\n- Agent $i$ chooses a fraction $x_{i,t} \\in [0,1]$ of its unit effort to allocate to the productive activity (labeled $K$), and allocates the remaining fraction $r_{i,t} = 1 - x_{i,t}$ to rent-seeking.\n- Each agent’s productive output in period $t$ is $y_{i,t} = A \\cdot x_{i,t}^{\\alpha}$, with $A  0$ and $\\alpha \\in (0,1)$; this is a standard concave production function. Aggregate productive output is $Y_t = \\sum_{j=1}^{N} y_{j,t}$.\n- A fraction $\\tau \\in [0,1]$ of aggregate production is taxed and redistributed to rent-seekers. The rent-seeking pool in period $t$ is $\\tau \\cdot Y_t$. Let $R_t = \\sum_{j=1}^{N} r_{j,t}$ denote aggregate rent-seeking effort. If $R_t  0$, each agent $i$ receives a rent-seeking transfer equal to $\\tau \\cdot Y_t \\cdot \\frac{r_{i,t}}{R_t}$. If $R_t = 0$, the rent-seeking transfer is zero for all agents.\n- Agent $i$’s total period payoff is\n$$\n\\pi_{i,t} \\;=\\; (1 - \\tau)\\, y_{i,t} \\;+\\; \\tau \\, Y_t \\,\\frac{r_{i,t}}{\\max(R_t, \\varepsilon)},\n$$\nwhere $\\varepsilon  0$ is a numerical regularization constant used only to avoid division by zero in computation (see below).\n\nBehavioral adjustment rule (myopic payoff-gradient learning):\n- Agents update their productive allocation via a payoff-monotonic adjustment that respects bounds:\n$$\nx_{i,t+1} \\;=\\; \\mathrm{proj}_{[0,1]}\\!\\Big(x_{i,t} \\;+\\; \\eta \\cdot \\frac{\\partial \\pi_{i,t}}{\\partial x_{i,t}} \\cdot x_{i,t}\\,\\big(1-x_{i,t}\\big)\\Big),\n$$\nwhere $\\eta  0$ is a learning-rate parameter, $\\mathrm{proj}_{[0,1]}(\\cdot)$ denotes projection onto $[0,1]$, and $\\frac{\\partial \\pi_{i,t}}{\\partial x_{i,t}}$ is the within-period marginal effect of $x_{i,t}$ on $\\pi_{i,t}$. The multiplicative factor $x_{i,t}(1-x_{i,t})$ ensures that the update vanishes at the boundaries. A second regularization constant $\\delta_x  0$ must be used only inside derivative computations to avoid singularities at $x_{i,t} = 0$, by replacing $x_{i,t}$ with $\\max(x_{i,t}, \\delta_x)$ wherever $x_{i,t}$ appears inside powers with exponent less than $1$.\n\nDerivation target and implementation details:\n- Starting from the definitions above, compute the derivative $\\frac{\\partial \\pi_{i,t}}{\\partial x_{i,t}}$ by treating $x_{i,t}$ as the only choice variable of agent $i$ at time $t$ (holding other agents’ choices fixed). Use the identities $y_{i,t} = A \\cdot x_{i,t}^{\\alpha}$, $Y_t = \\sum_{j=1}^{N} y_{j,t}$, $r_{i,t} = 1 - x_{i,t}$, and $R_t = \\sum_{j=1}^{N} r_{j,t}$. Your implementation must use the regularized denominator $\\max(R_t,\\varepsilon)$ in all divisions by $R_t$, and must replace $x_{i,t}$ with $\\max(x_{i,t},\\delta_x)$ inside terms that involve $x_{i,t}^{\\alpha - 1}$ to avoid undefined values.\n- Initialize all agents with the same starting allocation $x_{i,0} = x_0 \\in [0,1]$ for all $i$.\n\nNumerical units and reporting:\n- All parameters that are fractions (such as $\\tau$, $x_{i,t}$, and $x_0$) must be represented as decimals in $[0,1]$ and not as percentages.\n- Angles do not appear in this problem.\n- Physical units do not appear in this problem.\n\nTask:\n- Implement a complete, runnable program that simulates the system for $T$ periods for each parameter set in the test suite below. For each parameter set, return a single floating-point number equal to the population average $\\frac{1}{N}\\sum_{i=1}^{N} x_{i,T}$ after $T$ updates (i.e., the final-period average productive allocation). Each reported number must be rounded to exactly $6$ digits after the decimal point.\n\nTest suite (five parameter sets):\n- Test $1$ (happy path): $(N, A, \\alpha, \\tau, \\eta, T, x_0, \\varepsilon, \\delta_x) = (200, 1.0, 0.5, 0.3, 0.05, 1000, 0.5, 10^{-8}, 10^{-8})$.\n- Test $2$ (no redistribution boundary): $(N, A, \\alpha, \\tau, \\eta, T, x_0, \\varepsilon, \\delta_x) = (200, 1.0, 0.5, 0.0, 0.05, 500, 0.2, 10^{-8}, 10^{-8})$.\n- Test $3$ (high redistribution edge): $(N, A, \\alpha, \\tau, \\eta, T, x_0, \\varepsilon, \\delta_x) = (200, 1.0, 0.5, 0.9, 0.02, 1000, 0.8, 10^{-8}, 10^{-8})$.\n- Test $4$ (low productivity case): $(N, A, \\alpha, \\tau, \\eta, T, x_0, \\varepsilon, \\delta_x) = (200, 0.2, 0.5, 0.5, 0.05, 1000, 0.5, 10^{-8}, 10^{-8})$.\n- Test $5$ (nearly linear production): $(N, A, \\alpha, \\tau, \\eta, T, x_0, \\varepsilon, \\delta_x) = (200, 1.0, 0.95, 0.4, 0.02, 1500, 0.5, 10^{-8}, 10^{-8})$.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each number formatted to exactly $6$ digits after the decimal point, in the order of the tests above. For example: $[0.123456,0.654321,0.500000,0.750000,0.333333]$.", "solution": "The user has provided a problem statement describing an agent-based computational economics (ACE) model of production and rent-seeking. The task is to validate this model, and if valid, derive the necessary equations for simulation, implement the simulation, and report the results for a given set of test cases.\n\n### Step 1: Extract Givens\n\n-   **Agents and Time**: $N$ agents, indexed by $i \\in \\{1,\\dots,N\\}$. Discrete time $t \\in \\{0,1,2,\\dots,T-1\\}$.\n-   **Choice Variable**: Agent $i$ chooses $x_{i,t} \\in [0,1]$ (effort on production).\n-   **Rent-Seeking Effort**: $r_{i,t} = 1 - x_{i,t}$.\n-   **Production Function**: $y_{i,t} = A \\cdot x_{i,t}^{\\alpha}$, with parameters $A  0$ and $\\alpha \\in (0,1)$.\n-   **Aggregate Production**: $Y_t = \\sum_{j=1}^{N} y_{j,t}$.\n-   **Rent-Seeking Pool**: $\\tau \\cdot Y_t$, where $\\tau \\in [0,1]$ is the tax/redistribution rate.\n-   **Aggregate Rent-Seeking Effort**: $R_t = \\sum_{j=1}^{N} r_{j,t}$.\n-   **Rent-Seeking Payoff**: $\\tau \\cdot Y_t \\cdot \\frac{r_{i,t}}{R_t}$ if $R_t  0$, and $0$ if $R_t = 0$.\n-   **Total Payoff Function**: $\\pi_{i,t} = (1 - \\tau)\\, y_{i,t} \\;+\\; \\tau \\, Y_t \\,\\frac{r_{i,t}}{\\max(R_t, \\varepsilon)}$, with regularization constant $\\varepsilon  0$.\n-   **Behavioral Update Rule**: $x_{i,t+1} = \\mathrm{proj}_{[0,1]}\\!\\Big(x_{i,t} \\;+\\; \\eta \\cdot \\frac{\\partial \\pi_{i,t}}{\\partial x_{i,t}} \\cdot x_{i,t}\\,\\big(1-x_{i,t}\\big)\\Big)$, where $\\eta  0$ is the learning rate.\n-   **Derivative Regularization**: For derivative computation, $x_{i,t}$ is replaced by $\\max(x_{i,t}, \\delta_x)$ where $x_{i,t}^{\\alpha-1}$ appears, using $\\delta_x  0$.\n-   **Initial Condition**: $x_{i,0} = x_0$ for all $i \\in \\{1, \\dots, N\\}$.\n-   **Task**: For each parameter set, simulate for $T$ periods and report the final average productive allocation $\\frac{1}{N}\\sum_{i=1}^{N} x_{i,T}$, rounded to $6$ decimal places.\n-   **Test Suite**: Five parameter sets $(N, A, \\alpha, \\tau, \\eta, T, x_0, \\varepsilon, \\delta_x)$ are provided.\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem statement is critically evaluated against the validation criteria.\n\n-   **Scientifically Grounded**: The model is a standard representation of a contest or rent-seeking game, widely used in public choice theory and computational economics. The production function is a standard Cobb-Douglas form. The learning rule is a common form of payoff-monotonic dynamics (replicator-like). The model is scientifically sound as a stylized representation of economic trade-offs.\n-   **Well-Posed**: The problem is well-posed for simulation. All parameters, functions, and initial conditions are explicitly defined. The update rule is deterministic. The use of projection $\\mathrm{proj}_{[0,1]}(\\cdot)$ and regularization constants $\\varepsilon$, $\\delta_x$ ensures that the simulation remains within a valid state space and avoids numerical singularities like division by zero or non-real numbers.\n-   **Objective**: The language is formal, precise, and free of subjective or ambiguous terminology.\n\nThe problem does not exhibit any flaws such as scientific unsoundness, non-formalizability, incompleteness, contradiction, or infeasibility. It is a well-defined computational task in the specified domain.\n\n### Step 3: Verdict and Action\n\nThe problem is **valid**. A solution will be developed.\n\n### Derivation of the Payoff Gradient\n\nTo implement the behavioral update rule, we must first compute the partial derivative of agent $i$'s payoff, $\\pi_{i,t}$, with respect to its own choice, $x_{i,t}$, holding the choices of all other agents ($x_{j,t}$ for $j \\neq i$) constant.\n\nThe payoff function for agent $i$ at time $t$ is:\n$$\n\\pi_{i,t} = (1 - \\tau)\\, y_{i,t} + \\tau \\, Y_t \\,\\frac{r_{i,t}}{R_t}\n$$\nWe will differentiate this term by term with respect to $x_{i,t}$. We have the following relationships and their derivatives with respect to $x_{i,t}$:\n-   $y_{i,t} = A x_{i,t}^{\\alpha} \\implies \\frac{\\partial y_{i,t}}{\\partial x_{i,t}} = A \\alpha x_{i,t}^{\\alpha-1}$\n-   $r_{i,t} = 1 - x_{i,t} \\implies \\frac{\\partial r_{i,t}}{\\partial x_{i,t}} = -1$\n-   $Y_t = y_{i,t} + \\sum_{j \\neq i} y_{j,t} \\implies \\frac{\\partial Y_t}{\\partial x_{i,t}} = \\frac{\\partial y_{i,t}}{\\partial x_{i,t}} = A \\alpha x_{i,t}^{\\alpha-1}$\n-   $R_t = r_{i,t} + \\sum_{j \\neq i} r_{j,t} \\implies \\frac{\\partial R_t}{\\partial x_{i,t}} = \\frac{\\partial r_{i,t}}{\\partial x_{i,t}} = -1$\n\nThe derivative of the first term of the payoff function is straightforward:\n$$\n\\frac{\\partial}{\\partial x_{i,t}} \\left [ (1 - \\tau) y_{i,t} \\right ] = (1 - \\tau) \\frac{\\partial y_{i,t}}{\\partial x_{i,t}} = (1 - \\tau) A \\alpha x_{i,t}^{\\alpha-1}\n$$\n\nFor the second term, we use the product rule for differentiation on $\\tau \\left( Y_t \\cdot \\frac{r_{i,t}}{R_t} \\right)$:\n$$\n\\frac{\\partial}{\\partial x_{i,t}} \\left( Y_t \\frac{r_{i,t}}{R_t} \\right) = \\left( \\frac{\\partial Y_t}{\\partial x_{i,t}} \\right) \\left( \\frac{r_{i,t}}{R_t} \\right) + Y_t \\left( \\frac{\\partial}{\\partial x_{i,t}} \\frac{r_{i,t}}{R_t} \\right)\n$$\nWe need the derivative of the share term $\\frac{r_{i,t}}{R_t}$, for which we use the quotient rule:\n$$\n\\frac{\\partial}{\\partial x_{i,t}} \\left( \\frac{r_{i,t}}{R_t} \\right) = \\frac{(\\frac{\\partial r_{i,t}}{\\partial x_{i,t}})R_t - r_{i,t}(\\frac{\\partial R_t}{\\partial x_{i,t}})}{R_t^2} = \\frac{(-1)R_t - r_{i,t}(-1)}{R_t^2} = \\frac{-R_t + r_{i,t}}{R_t^2} = -\\frac{R_t - r_{i,t}}{R_t^2}\n$$\nLet $R_{t,-i} = R_t - r_{i,t} = \\sum_{j \\neq i} r_{j,t}$ be the aggregate rent-seeking effort of all other agents. The derivative of the share term is then $-\\frac{R_{t,-i}}{R_t^2}$.\n\nSubstituting this back into the product rule expression:\n$$\n\\frac{\\partial}{\\partial x_{i,t}} \\left( Y_t \\frac{r_{i,t}}{R_t} \\right) = \\left( A \\alpha x_{i,t}^{\\alpha-1} \\right) \\left( \\frac{r_{i,t}}{R_t} \\right) + Y_t \\left( -\\frac{R_{t,-i}}{R_t^2} \\right)\n$$\n\nCombining all parts, the full derivative of the payoff function is:\n$$\n\\frac{\\partial \\pi_{i,t}}{\\partial x_{i,t}} = (1 - \\tau) A \\alpha x_{i,t}^{\\alpha-1} + \\tau \\left[ A \\alpha x_{i,t}^{\\alpha-1} \\frac{r_{i,t}}{R_t} - \\frac{Y_t R_{t,-i}}{R_t^2} \\right]\n$$\nThis expression provides the marginal return for agent $i$ from shifting a unit of effort from rent-seeking to production.\n\n### Implementation Plan\n\nThe simulation is implemented in Python using the `numpy` library for efficient vector calculations, which is crucial for handling a population of $N=200$ agents over many time steps.\n\n1.  **Initialization**: For each test case, set the parameters $(N, A, \\alpha, \\tau, \\eta, T, x_0, \\varepsilon, \\delta_x)$. A `numpy` array `x` of size $N$ is initialized, with all elements set to $x_0$.\n\n2.  **Simulation Loop**: The simulation proceeds for $T$ time steps. In each step $t$:\n    a.  Calculate current-period values: The vector of rent-seeking efforts `r` is computed as $1-x$. The aggregate rent-seeking effort `R_t` and aggregate production `Y_t` are calculated by summing over the respective agent-level vectors.\n    b.  Apply Regularization: As per the problem statement, the regularized denominator $R_{t,\\text{reg}} = \\max(R_t, \\varepsilon)$ is computed. For the derivative calculation, the choice variable is regularized as $x_{\\text{reg}} = \\max(x, \\delta_x)$ to prevent undefined values in $x^{\\alpha-1}$ when $x=0$.\n    c.  Compute Gradients: The derived formula for $\\frac{\\partial \\pi_{i,t}}{\\partial x_{i,t}}$ is implemented using vectorized operations. All terms, including the marginal productivity $A \\alpha x_{\\text{reg}}^{\\alpha-1}$ and the externality term $R_{t,-i} = R_t - r_i$, are computed as `numpy` arrays, yielding a gradient vector `grad_pi`.\n    d.  Update States: The change in productive allocation, $\\Delta x_t$, is calculated according to the update rule: $\\Delta x_t = \\eta \\cdot \\text{grad\\_pi} \\cdot x \\cdot (1-x)$. The state vector is updated: $x_{t+1} = x_t + \\Delta x_t$.\n    e.  Projection: The new state vector $x_{t+1}$ is projected onto the valid interval $[0,1]$ using `numpy.clip(x, 0.0, 1.0)`, which ensures $x_{i,t+1} \\in [0,1]$ for all agents.\n\n3.  **Result Extraction**: After $T$ iterations, the final population average productive effort, $\\frac{1}{N}\\sum_{i=1}^{N} x_{i,T}$, is calculated.\n\n4.  **Formatting and Output**: The result for each test case is formatted to $6$ decimal places. All results are collected and printed in the specified format: `[result1,result2,...]`.\n\nThis design directly implements the theoretical model and adheres to all specified computational and formatting requirements.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the ACE simulation for all test cases and print results.\n    \"\"\"\n\n    # Test suite (N, A, alpha, tau, eta, T, x0, epsilon, delta_x)\n    test_cases = [\n        (200, 1.0, 0.5, 0.3, 0.05, 1000, 0.5, 1e-8, 1e-8),\n        (200, 1.0, 0.5, 0.0, 0.05, 500, 0.2, 1e-8, 1e-8),\n        (200, 1.0, 0.5, 0.9, 0.02, 1000, 0.8, 1e-8, 1e-8),\n        (200, 0.2, 0.5, 0.5, 0.05, 1000, 0.5, 1e-8, 1e-8),\n        (200, 1.0, 0.95, 0.4, 0.02, 1500, 0.5, 1e-8, 1e-8),\n    ]\n\n    results = []\n    for case in test_cases:\n        N, A, alpha, tau, eta, T, x0, epsilon, delta_x = case\n\n        # Initialize the state vector for productive allocation\n        x = np.full(N, x0, dtype=np.float64)\n\n        # Main simulation loop for T periods\n        for _ in range(T):\n            # 1. Calculate current period aggregate variables\n            r = 1.0 - x\n            R_t = np.sum(r)\n            \n            # Using x directly for production as per y = A*x^alpha\n            y = A * np.power(x, alpha)\n            Y_t = np.sum(y)\n\n            # 2. Implement regularization for derivative calculation\n            R_t_reg = np.maximum(R_t, epsilon)\n            x_reg = np.maximum(x, delta_x)\n\n            # 3. Calculate the payoff gradient for each agent (vectorized)\n            # Marginal productivity: d(y_i)/d(x_i)\n            marginal_y = A * alpha * np.power(x_reg, alpha - 1)\n            \n            # Rent-seeking effort of all other agents: R_t - r_i\n            R_t_minus_i = R_t - r\n\n            # Term 1: Marginal gain from private production\n            grad_term1 = (1.0 - tau) * marginal_y\n\n            # Term 2: Marginal effect through rent-seeking payoff\n            # Sub-term 2a: Effect of increased total production on the rent pool\n            grad_term2a = tau * marginal_y * (r / R_t_reg)\n            # Sub-term 2b: Effect of changing own rent-seeking share\n            grad_term2b = -tau * Y_t * R_t_minus_i / (R_t_reg**2)\n\n            grad_pi = grad_term1 + grad_term2a + grad_term2b\n\n            # 4. Update the state vector based on the learning rule\n            update_term = eta * grad_pi * x * (1.0 - x)\n            x = x + update_term\n\n            # 5. Project the state vector back onto [0, 1]\n            x = np.clip(x, 0.0, 1.0)\n        \n        # After T periods, calculate the average productive allocation\n        final_avg_x = np.mean(x)\n        results.append(f\"{final_avg_x:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2370544"}, {"introduction": "Prediction markets are designed to aggregate dispersed information into a single, collective forecast, but how robust are they to deliberate manipulation? This advanced exercise challenges you to simulate a prediction market built upon a Logarithmic Market Scoring Rule (LMSR), a mechanism widely used in practice [@problem_id:2370584]. You will model Bayesian agents who trade based on private signals, and importantly, introduce 'misinformers' who strategically trade against their own information, allowing you to study the resilience and potential failure of markets as information processors.", "problem": "You are asked to design and implement an Agent-Based Computational Economics (ACE) simulation of a binary-outcome prediction market with a Logarithmic Market Scoring Rule (LMSR) market maker. A continuum of myopic agents arrives sequentially. Each agent observes a private binary signal about the event and trades with the market maker subject to a per-trade budget constraint. With a fixed probability, an agent deliberately spreads misinformation by inverting their private signal before trading.\n\nFundamental base:\n- Agents and signals in Agent-Based Computational Economics (ACE): The event outcome is a binary random variable $Y \\in \\{0,1\\}$ with true probability $\\theta = \\mathbb{P}(Y=1)$, which is unknown to agents. Conditional on $Y$, each agent observes an independent private signal $s \\in \\{0,1\\}$ such that $\\mathbb{P}(s=1 \\mid Y=1) = \\alpha$ and $\\mathbb{P}(s=1 \\mid Y=0) = 1-\\alpha$, where $\\alpha \\in (0,1)$ is a known signal accuracy parameter. Signals are conditionally independent across agents given $Y$.\n- Bayesian updating: If a prior belief is $p = \\mathbb{P}(Y=1)$, then upon observing $s \\in \\{0,1\\}$ the posterior belief $b$ is given by Bayes’ rule,\n  - If $s=1$: $b = \\dfrac{\\alpha p}{\\alpha p + (1-\\alpha)(1-p)}$.\n  - If $s=0$: $b = \\dfrac{(1-\\alpha) p}{(1-\\alpha) p + \\alpha (1-p)}$.\n- Logarithmic Market Scoring Rule (LMSR): The market maker maintains a share vector $\\mathbf{q} = (q_0,q_1)$ and a liquidity parameter $b_L  0$. The cost function is\n  $$C(\\mathbf{q}) = b_L \\log\\left(\\exp\\left(\\frac{q_0}{b_L}\\right) + \\exp\\left(\\frac{q_1}{b_L}\\right)\\right).$$\n  The instantaneous price for outcome $1$ is the gradient component,\n  $$p = \\frac{\\partial C}{\\partial q_1} = \\frac{\\exp\\left(\\frac{q_1}{b_L}\\right)}{\\exp\\left(\\frac{q_0}{b_L}\\right) + \\exp\\left(\\frac{q_1}{b_L}\\right)}.$$\n  In the binary case, the log-odds satisfy\n  $$\\log\\left(\\frac{p}{1-p}\\right) = \\frac{q_1 - q_0}{b_L}.$$\n  Moving the price from $p_{\\text{old}}$ to $p_{\\text{new}}$ by buying only the \"YES\" (outcome $1$) share corresponds to increasing $q_1$ by $\\Delta = b_L\\left(\\log\\left(\\frac{p_{\\text{new}}}{1-p_{\\text{new}}}\\right) - \\log\\left(\\frac{p_{\\text{old}}}{1-p_{\\text{old}}}\\right)\\right)$. Buying only the \"NO\" (outcome $0$) share corresponds to increasing $q_0$ by the analogous positive quantity when $p_{\\text{new}} lt; p_{\\text{old}}$. The monetary cost of any such trade is the difference in the cost function, $\\Delta C = C(\\mathbf{q}_{\\text{new}}) - C(\\mathbf{q}_{\\text{old}})$.\n\nSimulation environment:\n- Initialization: The LMSR starts with price $p_0 = \\pi_0 \\in (0,1)$. This can be realized by setting the initial share vector to any $\\mathbf{q}_0$ with $(q_{1,0}-q_{0,0})/b_L = \\log\\left(\\frac{\\pi_0}{1-\\pi_0}\\right)$. For concreteness, you may take $q_{0,0} = 0$ and $q_{1,0} = b_L \\log\\left(\\frac{\\pi_0}{1-\\pi_0}\\right)$.\n- Arrival and behavior: A sequence of $T$ agents arrives. Before trading, each agent observes the current market price $p$ and treats it as the prior $\\mathbb{P}(Y=1)$. The agent then observes a private signal $s \\in \\{0,1\\}$ drawn from the signal process described above. With probability $p_{\\text{fake}} \\in [0,1]$, the agent deliberately spreads misinformation by inverting the signal, i.e., uses $s' = 1-s$ for their Bayesian update. Otherwise, they use the truthful signal $s' = s$. The agent’s posterior belief $b$ is computed via Bayes’ rule from $p$ and $s'$.\n- Myopic trade with budget: Each agent is risk-neutral and myopic. They attempt to move the market price $p$ toward their belief $b$ by trading with the LMSR market maker, but they face a hard per-trade budget $w  0$. The agent chooses the largest feasible move toward $b$ such that the LMSR cost paid does not exceed $w$. If the cost to fully move the price to $b$ is less than or equal to $w$, the agent moves the price to $b$ exactly. Otherwise, the agent moves partially toward $b$, choosing the unique price on the line between $p$ and $b$ such that the LMSR cost difference equals $w$. You must compute this partial move exactly by solving along the one-dimensional path (buying only \"YES\" when $b \\ge p$ or only \"NO\" when $b lt; p$) and using numerical root-finding on the cost difference if needed.\n- Realization and evaluation: The true outcome $Y$ is drawn once at the start from $\\text{Bernoulli}(\\theta)$ and revealed only at the end. The evaluation metric is the Brier score of the final market probability, defined as $(p_T - Y)^2$, expressed as a decimal number.\n\nRandomness and reproducibility:\n- All randomness must be generated using a pseudorandom number generator with a fixed seed per test case to ensure reproducibility. The outcome $Y$ and all agent signals should be drawn using that seeded generator. There is no other source of randomness.\n\nNumerical details and constraints:\n- Use stable numerical evaluation for the LMSR cost and price (e.g., use a stable log-sum-exp computation). Clamp any probability arguments to lie within $(\\varepsilon,1-\\varepsilon)$ with $\\varepsilon = 10^{-12}$ to avoid division by zero or taking the logarithm of zero. The angle unit is not applicable. There are no physical units in this problem. All probabilities and accuracy parameters must be given as decimals in $[0,1]$.\n- Output formatting: For each test case, produce two floats on the final output line: the final price $p_T$ and the Brier score $(p_T - Y)^2$, each rounded to $6$ decimal places. Aggregate all test case results into a single flat Python-style list printed on one line, e.g., $[p_1,\\text{Brier}_1,p_2,\\text{Brier}_2,\\dots]$.\n\nTest suite:\nImplement your program to run the following four test cases exactly, in this order. For each, use the provided seed for the pseudorandom generator, and print results as specified.\n\n- Case A (happy path, no misinformation):\n  - Seed: $123$\n  - Number of agents: $T = 200$\n  - Liquidity: $b_L = 10.0$\n  - Per-agent budget: $w = 1.0$\n  - Signal accuracy: $\\alpha = 0.7$\n  - Misinformation probability: $p_{\\text{fake}} = 0.0$\n  - True event probability: $\\theta = 0.6$\n  - Initial prior price: $\\pi_0 = 0.5$\n\n- Case B (mixed behavior):\n  - Seed: $456$\n  - Number of agents: $T = 200$\n  - Liquidity: $b_L = 10.0$\n  - Per-agent budget: $w = 1.0$\n  - Signal accuracy: $\\alpha = 0.7$\n  - Misinformation probability: $p_{\\text{fake}} = 0.5$\n  - True event probability: $\\theta = 0.6$\n  - Initial prior price: $\\pi_0 = 0.5$\n\n- Case C (all agents misinform):\n  - Seed: $789$\n  - Number of agents: $T = 200$\n  - Liquidity: $b_L = 10.0$\n  - Per-agent budget: $w = 1.0$\n  - Signal accuracy: $\\alpha = 0.7$\n  - Misinformation probability: $p_{\\text{fake}} = 1.0$\n  - True event probability: $\\theta = 0.6$\n  - Initial prior price: $\\pi_0 = 0.5$\n\n- Case D (uninformative signals boundary):\n  - Seed: $321$\n  - Number of agents: $T = 200$\n  - Liquidity: $b_L = 10.0$\n  - Per-agent budget: $w = 1.0$\n  - Signal accuracy: $\\alpha = 0.5$\n  - Misinformation probability: $p_{\\text{fake}} = 0.7$\n  - True event probability: $\\theta = 0.6$\n  - Initial prior price: $\\pi_0 = 0.5$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[0.512345,0.237890,0.498765,0.251234,0.476543,0.226789,0.500000,0.250000]$). Round each float to $6$ decimal places. No other text should be printed.", "solution": "The problem asks for the implementation of an Agent-Based Computational Economics (ACE) simulation of a binary prediction market. The market is operated by a Logarithmic Market Scoring Rule (LMSR) automated market maker. A sequence of agents interacts with this market. The problem is scientifically well-posed, with all parameters, agent behaviors, and market mechanics clearly defined. It constitutes a standard simulation task in computational finance and economics. I will construct the solution by first detailing the mathematical model and then outlining the simulation algorithm.\n\nThe state of the LMSR market is defined by the quantities of outstanding shares, $\\mathbf{q} = (q_0, q_1)$, for two mutually exclusive outcomes $Y=0$ and $Y=1$. The market price for outcome $1$, denoted by $p$, is derived from the gradient of the LMSR cost function $C(\\mathbf{q})$. A key property of the binary-outcome LMSR is that the price is solely a function of the difference in share quantities, $q_1 - q_0$, and the liquidity parameter $b_L$:\n$$ \\log\\left(\\frac{p}{1-p}\\right) = \\frac{q_1 - q_0}{b_L} $$\nThis implies that the entire state of the market, as far as the price is concerned, can be captured by a single variable, the price $p$ itself. We do not need to track $q_0$ and $q_1$ individually. The simulation starts with an initial price $p_0 = \\pi_0$.\n\nAn agent's interaction with the market is a two-step process: belief formation and trading.\nFirst, an agent, arriving when the market price is $p_k$, treats this price as their prior probability, $\\mathbb{P}(Y=1) = p_k$. They observe a private signal $s \\in \\{0,1\\}$, which is correlated with the true (but unknown) outcome $Y$. The signal's accuracy is given by $\\alpha = \\mathbb{P}(s=1 \\mid Y=1) = \\mathbb{P}(s=0 \\mid Y=0)$. With a specified probability $p_{\\text{fake}}$, the agent acts as a misinformer and uses the inverted signal $s' = 1-s$ for their belief update. Otherwise, the truthful signal $s' = s$ is used. Using $s'$, the agent computes their posterior belief $b$ via Bayes' rule:\n- If $s'=1$: $b = \\dfrac{\\alpha p_k}{\\alpha p_k + (1-\\alpha)(1-p_k)}$\n- If $s'=0$: $b = \\dfrac{(1-\\alpha) p_k}{(1-\\alpha) p_k + \\alpha(1-p_k)}$\n\nSecond, the myopic, risk-neutral agent trades to move the market price from $p_k$ to their posterior belief $b$. The trade is subject to a budget constraint $w$. The cost of a trade that changes the price from $p_{\\text{old}}$ to $p_{\\text{new}}$ can be derived from the LMSR cost function $C(\\mathbf{q}) = b_L \\log\\left(\\exp(q_0/b_L) + \\exp(q_1/b_L)\\right)$.\nA trade that increases the price (i.e., $b  p_k$) involves buying shares of outcome $1$, which only changes $q_1$. The cost of moving the price from $p_k$ to $p_{\\text{new}}$ is $\\Delta C = b_L (\\log(1-p_k) - \\log(1-p_{\\text{new}}))$.\nA trade that decreases the price (i.e., $b  p_k$) involves buying shares of outcome $0$, which only changes $q_0$. The cost is $\\Delta C = b_L (\\log(p_k) - \\log(p_{\\text{new}}))$.\nThese simplified cost formulas, expressed in terms of price, obviate the need to track $q_0$ and $q_1$ and are numerically stable, provided prices are kept away from $0$ and $1$.\n\nThe agent wants to move the price to $b$.\nIf $b  p_k$, the cost for a full move to $b$ is $\\Delta C_{\\text{full}} = b_L (\\log(1-p_k) - \\log(1-b))$.\n- If $\\Delta C_{\\text{full}} \\le w$, the agent makes the full trade, and the new price is $p_{k+1} = b$.\n- If $\\Delta C_{\\text{full}}  w$, the agent spends their entire budget $w$. We must find the new price $p_{k+1}$ such that $w = b_L (\\log(1-p_k) - \\log(1-p_{k+1}))$. This equation can be solved analytically for $p_{k+1}$:\n  $$ p_{k+1} = 1 - (1-p_k) \\exp\\left(-\\frac{w}{b_L}\\right) $$\nIf $b  p_k$, the cost for a full move to $b$ is $\\Delta C_{\\text{full}} = b_L (\\log(p_k) - \\log(b))$.\n- If $\\Delta C_{\\text{full}} \\le w$, the new price is $p_{k+1} = b$.\n- If $\\Delta C_{\\text{full}}  w$, we solve $w = b_L (\\log(p_k) - \\log(p_{k+1}))$. The analytical solution is:\n  $$ p_{k+1} = p_k \\exp\\left(-\\frac{w}{b_L}\\right) $$\nContrary to the problem's suggestion, numerical root-finding is not necessary due to these exact analytical solutions for the budget-constrained trade. If $b=p_k$, no trade occurs and $p_{k+1} = p_k$.\n\nThe simulation proceeds as follows:\n1. Initialize a pseudorandom number generator (RNG) with the given seed.\n2. Determine the true outcome $Y \\in \\{0, 1\\}$ by drawing from a Bernoulli distribution with parameter $\\theta$.\n3. Initialize the market price $p = \\pi_0$.\n4. For each of the $T$ agents:\n    a. Let the current price be $p_k = p$. Clamp $p_k$ to $[\\varepsilon, 1-\\varepsilon]$ for $\\varepsilon=10^{-12}$ to ensure numerical stability.\n    b. Draw a random number to decide if the agent misinforms (based on $p_{\\text{fake}}$).\n    c. Draw the agent's private signal $s \\in \\{0,1\\}$ from the conditional distribution given $Y$ and $\\alpha$.\n    d. Determine the signal to be used, $s'$, based on the misinformation decision.\n    e. Calculate the agent's posterior belief $b$ using Bayes' rule with prior $p_k$ and signal $s'$. Clamp $b$ to $[\\varepsilon, 1-\\varepsilon]$.\n    f. Calculate the new market price $p_{k+1}$ using the budget-constrained trading logic derived above.\n    g. Update the market price: $p = p_{k+1}$.\n5. After all $T$ agents have traded, the final price is $p_T = p$. The simulation's performance is measured by the Brier score, $(p_T - Y)^2$.\n\nThis procedure is deterministic for a given seed and set of parameters, allowing for reproducible results. The implementation will follow this logic for each test case specified. A notable special case is when $\\alpha=0.5$; the signal is uninformative, so an agent's posterior belief $b$ will always equal their prior $p_k$, resulting in no trades and a constant market price.", "answer": "```python\nimport numpy as np\nfrom math import log, exp\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final results.\n    \"\"\"\n    test_cases = [\n        # Case A (happy path, no misinformation)\n        {\"seed\": 123, \"T\": 200, \"bL\": 10.0, \"w\": 1.0, \"alpha\": 0.7, \"p_fake\": 0.0, \"theta\": 0.6, \"pi_0\": 0.5},\n        # Case B (mixed behavior)\n        {\"seed\": 456, \"T\": 200, \"bL\": 10.0, \"w\": 1.0, \"alpha\": 0.7, \"p_fake\": 0.5, \"theta\": 0.6, \"pi_0\": 0.5},\n        # Case C (all agents misinform)\n        {\"seed\": 789, \"T\": 200, \"bL\": 10.0, \"w\": 1.0, \"alpha\": 0.7, \"p_fake\": 1.0, \"theta\": 0.6, \"pi_0\": 0.5},\n        # Case D (uninformative signals boundary)\n        {\"seed\": 321, \"T\": 200, \"bL\": 10.0, \"w\": 1.0, \"alpha\": 0.5, \"p_fake\": 0.7, \"theta\": 0.6, \"pi_0\": 0.5},\n    ]\n\n    results = []\n    for params in test_cases:\n        p_final, brier_score = run_simulation(**params)\n        results.append(f\"{p_final:.6f}\")\n        results.append(f\"{brier_score:.6f}\")\n\n    print(f\"[{','.join(results)}]\")\n\ndef run_simulation(seed, T, bL, w, alpha, p_fake, theta, pi_0):\n    \"\"\"\n    Runs a single ACE simulation for a given set of parameters.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    eps = 1e-12\n\n    # Step 1: Draw the true outcome Y for the event\n    Y = 1 if rng.random()  theta else 0\n\n    # Step 2: Initialize market price\n    p = pi_0\n\n    # Step 3: Sequentially process T agents\n    for _ in range(T):\n        p_current = np.clip(p, eps, 1 - eps)\n\n        # Agent behavior:\n        # 1. Observe signal s based on true outcome Y\n        if Y == 1:\n            signal_s = 1 if rng.random()  alpha else 0\n        else: # Y == 0\n            signal_s = 1 if rng.random()  (1 - alpha) else 0\n\n        # 2. Decide whether to misinform\n        is_misinforming = rng.random()  p_fake\n        signal_s_prime = 1 - signal_s if is_misinforming else signal_s\n\n        # 3. Form posterior belief 'b' using Bayes' rule\n        # The agent uses the current market price p_current as their prior\n        if signal_s_prime == 1:\n            numerator = alpha * p_current\n            denominator = alpha * p_current + (1 - alpha) * (1 - p_current)\n        else: # signal_s_prime == 0\n            numerator = (1 - alpha) * p_current\n            denominator = (1 - alpha) * p_current + alpha * (1 - p_current)\n        \n        # If denominator is zero (due to extreme alpha/p), belief doesn't change\n        b = numerator / denominator if denominator  0 else p_current\n        b = np.clip(b, eps, 1 - eps)\n\n        # Myopic trade logic with budget constraint w:\n        p_next = p_current \n        if b  p_current:\n            # Agent buys \"YES\" shares, wants to move price up to b\n            cost_full_move = bL * (log(1 - p_current) - log(1 - b))\n            if cost_full_move = w:\n                p_next = b\n            else:\n                # Spend full budget w\n                p_next = 1 - (1 - p_current) * exp(-w / bL)\n        elif b  p_current:\n            # Agent buys \"NO\" shares, wants to move price down to b\n            cost_full_move = bL * (log(p_current) - log(b))\n            if cost_full_move = w:\n                p_next = b\n            else:\n                # Spend full budget w\n                p_next = p_current * exp(-w / bL)\n        \n        # Update market price\n        p = p_next\n\n    # Step 4: Calculate final price and Brier score\n    p_final = p\n    brier_score = (p_final - Y)**2\n    \n    return p_final, brier_score\n\nif __name__ == '__main__':\n    solve()\n```", "id": "2370584"}]}