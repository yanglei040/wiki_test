{"hands_on_practices": [{"introduction": "To truly master replicator dynamics, we begin with its most canonical application: the Hawk-Dove game. This exercise [@problem_id:2710640] is a foundational practice in deriving the governing differential equation from a given payoff matrix and analyzing the stability of its equilibria. By working through this problem, you will develop the core analytical skills needed to model how strategies evolve based on their relative success, setting the stage for more complex scenarios.", "problem": "Consider an infinite, well-mixed population engaging in pairwise interactions described by the Hawk–Dove game with the symmetric payoff matrix\n$$\nA \\;=\\; \\begin{pmatrix}\n\\frac{V - C}{2}  V \\\\\n0  \\frac{V}{2}\n\\end{pmatrix},\n$$\nwhere $V>0$ denotes the benefit of the contested resource and $C>0$ denotes the cost of escalation. Let $p \\in [0,1]$ denote the frequency of Hawks in the population and $1-p$ the frequency of Doves. Assume selection dynamics follow the continuous-time replicator equation derived from the definition that a type’s growth rate equals its excess fitness relative to the population mean.\n\nUsing first principles of evolutionary game dynamics, do the following:\n- Derive the one-dimensional ordinary differential equation governing the time evolution of $p$ under replicator dynamics.\n- From this, determine the mixed-strategy equilibrium frequency $p^{\\ast}$ of Hawks that lies strictly between $0$ and $1$ (if it exists), expressed in terms of $V$ and $C$.\n- Linearize the replicator dynamics about $p^{\\ast}$ and compute the eigenvalue of the Jacobian of the scalar flow at $p^{\\ast}$ in closed form. Using its sign, determine the local stability of $p^{\\ast}$ when $VC$.\n\nReport your final answer as a row vector $(p^{\\ast}, \\lambda^{\\ast})$, where $\\lambda^{\\ast}$ is the eigenvalue of the Jacobian of the scalar replicator dynamics at $p^{\\ast}$. No numerical approximation or rounding is required, and no units should be reported in the final answer.", "solution": "The problem will first be validated for scientific soundness, consistency, and completeness before any attempt at a solution is made.\n\n### Step 1: Extract Givens\n- **Population Model**: Infinite, well-mixed population.\n- **Interaction Type**: Pairwise interactions.\n- **Game**: Hawk–Dove game.\n- **Payoff Matrix**: $A = \\begin{pmatrix} \\frac{V - C}{2}  V \\\\ 0  \\frac{V}{2} \\end{pmatrix}$.\n- **Parameters**: $V>0$ (benefit of resource), $C>0$ (cost of escalation).\n- **State Variable**: $p \\in [0,1]$ is the frequency of Hawks; $1-p$ is the frequency of Doves.\n- **Selection Dynamics**: Continuous-time replicator equation, where a type's growth rate equals its excess fitness relative to the mean.\n- **Tasks**:\n    1.  Derive the one-dimensional ordinary differential equation for $p$.\n    2.  Determine the mixed-strategy equilibrium frequency $p^{\\ast} \\in (0,1)$.\n    3.  Linearize dynamics about $p^{\\ast}$, find the eigenvalue $\\lambda^{\\ast}$ of the Jacobian.\n    4.  Determine the local stability of $p^{\\ast}$ for $VC$.\n- **Final Answer Format**: A row vector $(p^{\\ast}, \\lambda^{\\ast})$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the established criteria.\n- **Scientific Groundedness**: The problem is a canonical example from evolutionary game theory. The Hawk-Dove game, the given payoff matrix structure, and the replicator equation are standard, foundational concepts in this field. The setup is scientifically rigorous and factually sound.\n- **Well-Posedness**: The problem is mathematically well-defined. It provides all necessary information (payoff matrix, parameters, dynamic equation form) to derive a unique analytical solution for the requested quantities. The tasks follow a logical progression from model formulation to stability analysis.\n- **Objectivity**: The problem is stated in precise, objective mathematical language, free of any subjectivity, ambiguity, or opinion.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. It is a standard, well-posed problem in mathematical biology that is scientifically sound and objective. I will now proceed with the full derivation and solution.\n\nThe continuous-time replicator equation is derived from the principle that the per-capita growth rate of a strategy is equal to the difference between its fitness and the average fitness of the population. Let $p$ be the frequency of Hawks. The rate of change of $p$ is given by $\\frac{dp}{dt}$. The per-capita rate of change is $\\frac{1}{p}\\frac{dp}{dt}$.\n\nLet $f_H$ and $f_D$ be the expected fitness (payoffs) of a Hawk and a Dove, respectively. A Hawk interacts with another Hawk with probability $p$ and a Dove with probability $1-p$. Similarly for a Dove. The fitness functions are:\n$$f_H(p) = p \\cdot A_{11} + (1-p) \\cdot A_{12} = p\\left(\\frac{V - C}{2}\\right) + (1-p)V$$\n$$f_D(p) = p \\cdot A_{21} + (1-p) \\cdot A_{22} = p(0) + (1-p)\\left(\\frac{V}{2}\\right) = (1-p)\\frac{V}{2}$$\n\nThe mean fitness of the population, $\\bar{f}$, is the weighted average of the individual fitness values:\n$$\\bar{f}(p) = p \\cdot f_H(p) + (1-p) \\cdot f_D(p)$$\n\nAccording to the problem definition, the replicator dynamics for the Hawk frequency $p$ is:\n$$\\frac{1}{p}\\frac{dp}{dt} = f_H(p) - \\bar{f}(p)$$\n$$\\frac{dp}{dt} = p(f_H(p) - \\bar{f}(p))$$\nSubstituting the expression for $\\bar{f}(p)$:\n$$\\frac{dp}{dt} = p(f_H(p) - [p \\cdot f_H(p) + (1-p) \\cdot f_D(p)])$$\n$$\\frac{dp}{dt} = p([1-p]f_H(p) - [1-p]f_D(p))$$\n$$\\frac{dp}{dt} = p(1-p)(f_H(p) - f_D(p))$$\nThis is the general form of the one-dimensional replicator equation.\n\nTo complete the first task, we must compute the difference in fitness, $f_H(p) - f_D(p)$:\n$$f_H(p) - f_D(p) = \\left[p\\left(\\frac{V - C}{2}\\right) + (1-p)V\\right] - \\left[(1-p)\\frac{V}{2}\\right]$$\n$$= p\\frac{V}{2} - p\\frac{C}{2} + V - pV - \\frac{V}{2} + p\\frac{V}{2}$$\n$$= \\left(p\\frac{V}{2} - pV + p\\frac{V}{2}\\right) - p\\frac{C}{2} + \\left(V - \\frac{V}{2}\\right)$$\n$$= (pV - pV) - p\\frac{C}{2} + \\frac{V}{2}$$\n$$= \\frac{V}{2} - p\\frac{C}{2} = \\frac{1}{2}(V - pC)$$\nSubstituting this into the replicator equation gives the specific ordinary differential equation for $p(t)$:\n$$\\frac{dp}{dt} = p(1-p)\\frac{1}{2}(V - pC)$$\n\nThe second task is to find the mixed-strategy equilibrium $p^{\\ast}$ strictly between $0$ and $1$. Equilibria, or fixed points, are found by setting $\\frac{dp}{dt} = 0$.\n$$p(1-p)\\frac{1}{2}(V - pC) = 0$$\nThis equation has three solutions for $p$:\n$1$. $p=0$ (pure Dove population)\n$2$. $p=1$ (pure Hawk population)\n$3$. $V - pC = 0 \\implies p = \\frac{V}{C}$ (mixed population)\nThe problem asks for the internal equilibrium $p^{\\ast} \\in (0,1)$. This corresponds to the third solution.\n$$p^{\\ast} = \\frac{V}{C}$$\nThis equilibrium exists in the interval $(0,1)$ if and only if $0  V  C$, which is consistent with the condition given for the final part of the problem.\n\nThe third task is to linearize the dynamics around $p^{\\ast}$ and find the eigenvalue of the Jacobian. Let $F(p) = \\frac{dp}{dt} = p(1-p)\\frac{1}{2}(V-pC)$. The Jacobian of this scalar flow is its derivative, $J(p) = \\frac{dF}{dp}$. The eigenvalue at the equilibrium $p^{\\ast}$ is $\\lambda^{\\ast} = \\frac{dF}{dp}|_{p=p^{\\ast}}$.\n\nA general property of the replicator equation at an internal fixed point $p^{\\ast}$ is that $f_H(p^{\\ast}) = f_D(p^{\\ast})$. The derivative of $F(p) = p(1-p)(f_H(p)-f_D(p))$ at such a point simplifies. Let $g(p) = f_H(p)-f_D(p)$. Then $F(p) = p(1-p)g(p)$, and $g(p^{\\ast})=0$.\nUsing the product rule:\n$$\\frac{dF}{dp} = (1-2p)g(p) + p(1-p)g'(p)$$\nEvaluating at $p=p^{\\ast}$:\n$$\\lambda^{\\ast} = \\frac{dF}{dp}|_{p=p^{\\ast}} = (1-2p^{\\ast})g(p^{\\ast}) + p^{\\ast}(1-p^{\\ast})g'(p^{\\ast})$$\nSince $g(p^{\\ast})=0$, this simplifies to:\n$$\\lambda^{\\ast} = p^{\\ast}(1-p^{\\ast})g'(p^{\\ast})$$\nWe have $g(p) = \\frac{1}{2}(V-pC)$. Its derivative is:\n$$g'(p) = \\frac{d}{dp}\\left(\\frac{V}{2} - \\frac{pC}{2}\\right) = -\\frac{C}{2}$$\nThis derivative is a constant, so $g'(p^{\\ast}) = -\\frac{C}{2}$.\nSubstituting the expressions for $p^{\\ast}$ and $g'(p^{\\ast})$:\n$$\\lambda^{\\ast} = \\left(\\frac{V}{C}\\right)\\left(1 - \\frac{V}{C}\\right)\\left(-\\frac{C}{2}\\right)$$\n$$\\lambda^{\\ast} = \\left(\\frac{V}{C}\\right)\\left(\\frac{C-V}{C}\\right)\\left(-\\frac{C}{2}\\right)$$\n$$\\lambda^{\\ast} = -\\frac{V(C-V)}{2C}$$\nThis can also be written as:\n$$\\lambda^{\\ast} = \\frac{V(V-C)}{2C}$$\n\nThe final task is to determine the local stability of $p^{\\ast}$ when $V  C$. Local asymptotic stability of a fixed point is determined by the sign of the eigenvalue $\\lambda^{\\ast}$. The fixed point is locally stable if $\\lambda^{\\ast}  0$. We are given $V>0$ and $C>0$.\nThe sign of $\\lambda^{\\ast} = \\frac{V(V-C)}{2C}$ is determined by the sign of the term $(V-C)$, since $V$ and $2C$ are both positive.\nUnder the condition $V  C$, the term $(V-C)$ is negative.\nThus, $\\lambda^{\\ast} = \\frac{(+)(-)}{(+)}  0$.\nSince the eigenvalue is negative, the mixed-strategy equilibrium $p^{\\ast}$ is locally stable. This completes the analysis. The final answer comprises the expressions for $p^{\\ast}$ and $\\lambda^{\\ast}$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{V}{C}  \\frac{V(V-C)}{2C}\n\\end{pmatrix}\n}\n$$", "id": "2710640"}, {"introduction": "Real-world populations are seldom uniform; they often contain agents with different behaviors or learning rules. This practice [@problem_id:2427002] introduces this complexity by modeling a population with both \"adaptive\" agents, who update their strategies, and \"stubborn\" agents, who do not. Your task is to determine the equilibrium state within the adaptive subpopulation, which requires you to carefully recalculate expected payoffs in a heterogeneous environment, a crucial skill for building more realistic economic models.", "problem": "Consider a large well-mixed population engaged in repeated random matching where each agent employs one of $2$ trading strategies: \"Trend\" (denote by $T$) or \"Value\" (denote by $V$). The interaction is a symmetric $2 \\times 2$ game. When a player using strategy $i \\in \\{T,V\\}$ is matched against an opponent using strategy $j \\in \\{T,V\\}$, the player’s one-period expected payoff is the entry of the matrix\n$$\nA \\;=\\; \\begin{pmatrix}\n5  1 \\\\\n3  2\n\\end{pmatrix},\n$$\nwhere the row indicates the player’s strategy and the column indicates the opponent’s strategy. A fraction $s=\\frac{1}{10}$ of the population consists of \"stubborn\" agents who permanently play $T$ and never revise. The remaining fraction $1-s$ of the population are adaptive agents who revise their strategies according to replicator dynamics: a strategy’s share within the adaptive subpopulation increases if and only if its expected payoff exceeds the current average payoff, and decreases if and only if it is below the current average payoff.\n\nLet $p \\in [0,1]$ denote the share of $T$ among adaptive agents. Assume that matching is random across the entire population (including both stubborn and adaptive agents). Under the above assumptions, there exists a unique interior rest point $p^{\\ast} \\in (0,1)$ of the replicator dynamics within the adaptive subpopulation.\n\nCompute $p^{\\ast}$ exactly. Give your answer as an exact fraction; do not round.", "solution": "The problem statement is scrutinized and deemed valid. It is scientifically grounded in the principles of evolutionary game theory, specifically replicator dynamics. The problem is well-posed, providing all necessary parameters and a clear objective. The language is objective and the setup is mathematically consistent and complete. Therefore, a solution will be provided.\n\nLet $s$ be the fraction of \"stubborn\" agents in the population, who always play strategy $T$. We are given $s = \\frac{1}{10}$. The remaining fraction of the population, $1-s$, consists of \"adaptive\" agents. Let $p$ be the fraction of adaptive agents who choose strategy $T$. Consequently, the fraction of adaptive agents who choose strategy $V$ is $1-p$.\n\nThe overall proportion of agents in the entire population playing strategy $T$, denoted by $x_T$, is the sum of the stubborn agents playing $T$ and the adaptive agents playing $T$:\n$$\nx_T = s \\cdot 1 + (1-s) \\cdot p = s + (1-s)p\n$$\nThe overall proportion of agents playing strategy $V$, denoted by $x_V$, consists only of adaptive agents playing $V$:\n$$\nx_V = (1-s)(1-p)\n$$\nNote that $x_T + x_V = s + (1-s)p + (1-s)(1-p) = s + (1-s)(p + 1 - p) = s + 1 - s = 1$, as expected.\n\nThe payoff matrix is given as:\n$$\nA = \\begin{pmatrix} A_{TT}  A_{TV} \\\\ A_{VT}  A_{VV} \\end{pmatrix} = \\begin{pmatrix} 5  1 \\\\ 3  2 \\end{pmatrix}\n$$\nAn adaptive agent's choice of strategy is evaluated against a randomly drawn opponent from the entire population. The expected payoff for an adaptive agent choosing strategy $T$, denoted $U_T$, is:\n$$\nU_T = A_{TT} \\cdot x_T + A_{TV} \\cdot x_V = 5x_T + 1x_V\n$$\nThe expected payoff for an adaptive agent choosing strategy $V$, denoted $U_V$, is:\n$$\nU_V = A_{VT} \\cdot x_T + A_{VV} \\cdot x_V = 3x_T + 2x_V\n$$\nThe replicator dynamics state that the share $p$ of strategy $T$ within the adaptive subpopulation evolves according to the equation:\n$$\n\\dot{p} = p(U_T - \\bar{U})\n$$\nwhere $\\bar{U} = p U_T + (1-p) U_V$ is the average payoff within the adaptive subpopulation.\n\nAn interior rest point $p^{\\ast} \\in (0,1)$ is a state where $\\dot{p}=0$. As $p^{\\ast}$ is interior, $p^{\\ast} \\neq 0$ and $p^{\\ast} \\neq 1$. The condition $\\dot{p}=0$ implies $p(U_T - \\bar{U}) = 0$. Since $p \\neq 0$, we must have $U_T - \\bar{U} = 0$.\nSubstituting the expression for $\\bar{U}$:\n$$\nU_T - [p U_T + (1-p) U_V] = 0\n$$\n$$\n(1-p)U_T - (1-p)U_V = 0\n$$\n$$\n(1-p)(U_T - U_V) = 0\n$$\nSince $p^{\\ast} \\in (0,1)$, we have $1-p^{\\ast} \\neq 0$. Therefore, the condition for an interior rest point simplifies to the equalization of expected payoffs for the two strategies:\n$$\nU_T = U_V\n$$\nSubstituting the expressions for $U_T$ and $U_V$:\n$$\n5x_T + 1x_V = 3x_T + 2x_V\n$$\n$$\n2x_T = x_V\n$$\nNow, we substitute the expressions for $x_T$ and $x_V$ in terms of $p$ and $s$:\n$$\n2(s + (1-s)p) = (1-s)(1-p)\n$$\nWe are seeking the value of $p$, which we will denote $p^{\\ast}$, that satisfies this equilibrium condition. Expanding the equation:\n$$\n2s + 2(1-s)p^{\\ast} = 1-s - (1-s)p^{\\ast}\n$$\nNow, we gather terms involving $p^{\\ast}$ on one side and constant terms on the other:\n$$\n2(1-s)p^{\\ast} + (1-s)p^{\\ast} = 1-s-2s\n$$\n$$\n3(1-s)p^{\\ast} = 1-3s\n$$\nSolving for $p^{\\ast}$:\n$$\np^{\\ast} = \\frac{1-3s}{3(1-s)}\n$$\nThis is the general expression for the interior rest point as a function of the fraction of stubborn agents $s$. The problem specifies $s = \\frac{1}{10}$. Substituting this value:\n$$\n1-s = 1 - \\frac{1}{10} = \\frac{9}{10}\n$$\n$$\n1-3s = 1 - 3\\left(\\frac{1}{10}\\right) = 1 - \\frac{3}{10} = \\frac{7}{10}\n$$\nSubstituting these into the expression for $p^{\\ast}$:\n$$\np^{\\ast} = \\frac{\\frac{7}{10}}{3\\left(\\frac{9}{10}\\right)} = \\frac{\\frac{7}{10}}{\\frac{27}{10}}\n$$\n$$\np^{\\ast} = \\frac{7}{27}\n$$\nThis value lies in the interval $(0,1)$, confirming it is a valid interior rest point. This is the unique interior rest point as requested.", "answer": "$$\n\\boxed{\\frac{7}{27}}\n$$", "id": "2427002"}, {"introduction": "While analytical solutions are powerful, many economic systems are too complex to be solved with pen and paper alone. This hands-on coding exercise [@problem_id:2426991] bridges the gap between theory and application by asking you to simulate the evolution of strategies in a Keynesian \"beauty contest\" game. By implementing the replicator dynamics numerically, you will gain practical experience in tackling high-dimensional strategic problems and observing how population-level behavior emerges from individual-level payoff calculations, a vital skill for any computational economist.", "problem": "Consider a large population of agents repeatedly playing a Keynesian beauty contest game. At each round, every agent commits to a fixed forecasting rule (a \"strategy\") that outputs a scalar guess in the unit interval. The realized target to forecast is proportional to the current population mean guess. The population state is represented by a probability vector $x = (x_0,\\dots,x_K)$ over a finite set of $K+1$ strategies, where $x_i \\ge 0$ and $\\sum_{i=0}^K x_i = 1$. The average guess is $m(x) = \\sum_{i=0}^K x_i g_i$, where $g_i$ is the guess produced by strategy $i$. The game is parameterized by a weight $p \\in (0,1)$ that scales the target as $p \\cdot m(x)$, a baseline anchor $b \\in (0,1]$, and a linear complexity cost coefficient $\\lambda \\ge 0$ that penalizes strategy index. The set of strategies is defined by level-$k$ iterative reasoning with geometric attenuation: for $i \\in \\{0,1,\\dots,K\\}$, the rule is to guess $g_i = b \\, p^i$. The period payoff for strategy $i$ given state $x$ is defined as the negative squared forecast error minus complexity cost, namely\n$$\nu_i(x) = -\\big(g_i - p \\, m(x)\\big)^2 - \\lambda \\, i.\n$$\nPopulation shares evolve according to the continuous-time replicator equation\n$$\n\\dot{x}_i = x_i \\big( u_i(x) - \\bar{u}(x) \\big), \\quad \\text{for } i=0,\\dots,K,\n$$\nwhere $\\bar{u}(x) = \\sum_{j=0}^K x_j u_j(x)$ is the average payoff. The initial condition is the uniform distribution $x_i(0) = \\frac{1}{K+1}$ for all $i$.\n\nYour task is to implement a program that numerically integrates the replicator dynamics to the steady state for several parameter configurations, starting from the given initial condition. Use a time-discretization scheme that preserves the simplex (nonnegativity and unit sum) to numerical tolerance. Convergence should be declared when the maximum absolute change in $x$ over one step is below $10^{-10}$ for at least $100$ consecutive steps, or when a hard cap of simulation steps is reached. Use a fixed time step of $\\Delta t = 0.01$ and a maximum simulated time of $T = 200$ (that is, at most $N = 20000$ steps). If numerical drift violates nonnegativity, project back by clipping at zero and renormalizing to unit sum. After convergence (or at the time cap), report the index $i^\\star$ of the strategy with the largest population share $x_{i^\\star}$; in case of ties, report the smallest such index.\n\nImplement your solution for the following test suite (each tuple is $(p,b,K,\\lambda)$):\n- Case A (happy path): $(\\frac{2}{3}, 1.0, 5, 0.0)$.\n- Case B (cost-dominated boundary): $(0.9, 1.0, 8, 0.05)$.\n- Case C (trade-off interior case): $(0.5, 1.0, 10, 0.02)$.\n\nNotes:\n- All mathematical entities must be treated precisely as defined: $p \\in (0,1)$, $b \\in (0,1]$, $K \\in \\mathbb{N}$ with $K \\ge 1$, and $\\lambda \\ge 0$.\n- Angles do not appear; no angle unit is required.\n- There are no physical units.\n- Percentages are not used; all quantities are real numbers.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for the test suite as a comma-separated list enclosed in square brackets, in the order of cases A, B, C. Each element must be an integer equal to the dominant strategy index $i^\\star$ found by your simulation. For example, an output with three integers must look like $[i_A,i_B,i_C]$ with no spaces.", "solution": "The problem statement is critically examined and found to be valid. It is scientifically grounded in the established theory of evolutionary game theory, specifically using replicator dynamics to model strategy evolution in a Keynesian beauty contest game. The problem is well-posed, providing a complete set of definitions, equations, parameters, and initial conditions required for a unique numerical solution. The language is objective and mathematically precise, with no ambiguities or contradictions.\n\nThe problem requires the numerical integration of a system of continuous-time replicator equations to find the steady-state distribution of strategies. The system describes a population of agents, where the share $x_i$ of the population using strategy $i$ evolves based on its relative performance.\n\nThe state of the system is a probability vector $x = (x_0, \\dots, x_K)$ on the $(K+1)$-dimensional simplex, where $x_i(t)$ is the population share of strategy $i$ at time $t$. The constraints are $x_i \\ge 0$ for all $i \\in \\{0, \\dots, K\\}$ and $\\sum_{i=0}^K x_i = 1$. The initial condition is a uniform distribution, $x_i(0) = \\frac{1}{K+1}$ for all $i$.\n\nThe game involves forecasting a target that depends on the population's average action. The available strategies are indexed by $i \\in \\{0, \\dots, K\\}$. Strategy $i$ corresponds to making a fixed guess $g_i$, defined by level-$k$ reasoning as:\n$$\ng_i = b p^i\n$$\nwhere $b \\in (0,1]$ is a baseline anchor and $p \\in (0,1)$ is a geometric attenuation factor. Given the population state $x$, the average guess is:\n$$\nm(x) = \\sum_{i=0}^K x_i g_i\n$$\nThe target value to be forecasted is $p \\cdot m(x)$.\n\nThe success of a strategy is measured by its payoff, which is the negative of a quadratic loss function (squared forecast error) plus a linear penalty for complexity:\n$$\nu_i(x) = -\\big(g_i - p \\, m(x)\\big)^2 - \\lambda i\n$$\nwhere $\\lambda \\ge 0$ is the coefficient for the complexity cost, which increases with the strategy index $i$.\n\nThe population dynamics are governed by the continuous-time replicator equation:\n$$\n\\dot{x}_i = x_i \\big( u_i(x) - \\bar{u}(x) \\big)\n$$\nwhere $\\bar{u}(x) = \\sum_{j=0}^K x_j u_j(x)$ is the population-average payoff. This equation implies that strategies with above-average payoffs will increase their population share over time, while those with below-average payoffs will decline.\n\nTo solve this system of ordinary differential equations numerically, we must employ a time-discretization scheme. A standard forward Euler method is simple but does not guarantee that the state vector $x$ remains on the simplex. The problem statement suggests a projection method (clipping and renormalization) to handle this. However, a more elegant and robust approach is to use a method that inherently preserves the simplex structure. The exponential update rule, a standard discretization for replicator dynamics, achieves this:\n$$\nx_i(t+\\Delta t) = \\frac{x_i(t) \\exp\\big(\\Delta t \\cdot u_i(x(t))\\big)}{\\sum_{j=0}^K x_j(t) \\exp\\big(\\Delta t \\cdot u_j(x(t))\\big)}\n$$\nThis update rule ensures that if $x_i(t) \\ge 0$ and $\\sum_i x_i(t) = 1$, then $x_i(t+\\Delta t) \\ge 0$ and $\\sum_i x_i(t+\\Delta t) = 1$ are maintained to within numerical precision, fulfilling the problem's requirement.\n\nThe algorithm for each test case is as follows:\n1.  Initialize the parameters $(p, b, K, \\lambda)$ and the numerical constants $\\Delta t = 0.01$, $N_{max} = 20000$, convergence tolerance $\\epsilon = 10^{-10}$, and required consecutive steps for convergence $S_{conv} = 100$.\n2.  Pre-calculate the constant vector of strategy guesses $g = (g_0, \\dots, g_K)$.\n3.  Initialize the state vector $x$ to the uniform distribution: $x_i = \\frac{1}{K+1}$ for all $i$.\n4.  Begin the main simulation loop, iterating from step $n=0$ to $N_{max}-1$.\n    a.  Store the current state vector: $x_{old} \\leftarrow x$.\n    b.  Calculate the average guess $m(x) = x \\cdot g$.\n    c.  Calculate the target value $T = p \\cdot m(x)$.\n    d.  Calculate the payoff vector $u$, where $u_i = -(g_i - T)^2 - \\lambda i$.\n    e.  Update the state vector $x$ using the exponential update rule described above.\n    f.  Check for convergence: Calculate the maximum absolute change, $\\delta = \\max_i |x_i - (x_{old})_i|$. If $\\delta  \\epsilon$, increment a counter for consecutive converged steps. Otherwise, reset the counter.\n    g.  If the counter reaches $S_{conv}$, the system has reached a steady state, and the loop terminates.\n5.  After the loop terminates (either by convergence or by reaching $N_{max}$ steps), identify the index $i^\\star$ of the strategy with the largest population share in the final state vector $x$. In case of a tie, the smallest such index is chosen. This is accomplished by finding the argument of the maximum of the final vector $x$.\n6.  This index $i^\\star$ is the result for the given test case.\nThis procedure is repeated for all specified test cases.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print the results.\n    \"\"\"\n    # Each tuple is (p, b, K, lambda).\n    test_cases = [\n        (2.0/3.0, 1.0, 5, 0.0),    # Case A\n        (0.9, 1.0, 8, 0.05),     # Case B\n        (0.5, 1.0, 10, 0.02),    # Case C\n    ]\n\n    results = []\n    for p, b, K, lambda_val in test_cases:\n        i_star = run_simulation(p, b, K, lambda_val)\n        results.append(i_star)\n\n    # Format the final output as specified.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef run_simulation(p, b, K, lambda_val):\n    \"\"\"\n    Numerically integrates the replicator dynamics for a single parameter configuration.\n\n    Args:\n        p (float): The scaling weight for the target.\n        b (float): The baseline anchor for strategies.\n        K (int): The maximum strategy index (K+1 strategies).\n        lambda_val (float): The complexity cost coefficient.\n\n    Returns:\n        int: The index of the dominant strategy at the steady state.\n    \"\"\"\n    # Numerical integration parameters from the problem description\n    DT = 0.01\n    T_MAX = 200.0\n    N_MAX = int(T_MAX / DT)\n    CONV_TOL = 1e-10\n    CONV_STEPS = 100\n\n    # Initialize strategies and state vector\n    # k_values is an array [0, 1, ..., K]\n    k_values = np.arange(K + 1)\n    # g is the vector of guesses for each strategy\n    g = b * (p ** k_values)\n    # x is the population share vector, initialized to uniform distribution\n    x = np.full(K + 1, 1.0 / (K + 1))\n\n    consecutive_converged_steps = 0\n\n    for _ in range(N_MAX):\n        x_old = x.copy()\n\n        # Step 1: Calculate average guess m(x)\n        m_x = np.dot(x, g)\n\n        # Step 2: Calculate target\n        target = p * m_x\n\n        # Step 3: Calculate payoffs u_i(x) for all strategies\n        forecast_error_sq = (g - target) ** 2\n        complexity_cost = lambda_val * k_values\n        u = -forecast_error_sq - complexity_cost\n\n        # Step 4: Update population shares using the exponential update rule\n        # This scheme inherently preserves the simplex (non-negativity and unit sum).\n        numerators = x * np.exp(DT * u)\n        denominator = np.sum(numerators)\n        \n        # This check is for extreme cases, e.g., underflow of all numerators.\n        # It's unlikely in this problem but is good practice.\n        if denominator > 0:\n            x = numerators / denominator\n        else:\n            # If all population shares vanish, simulation cannot continue.\n            break\n\n        # Step 5: Check for convergence\n        max_change = np.max(np.abs(x - x_old))\n        if max_change  CONV_TOL:\n            consecutive_converged_steps += 1\n        else:\n            consecutive_converged_steps = 0\n\n        if consecutive_converged_steps >= CONV_STEPS:\n            break\n\n    # Determine the dominant strategy after convergence or timeout\n    # np.argmax returns the smallest index in case of a tie.\n    i_star = np.argmax(x)\n\n    return int(i_star)\n\nif __name__ == '__main__':\n    solve()\n```", "id": "2426991"}]}