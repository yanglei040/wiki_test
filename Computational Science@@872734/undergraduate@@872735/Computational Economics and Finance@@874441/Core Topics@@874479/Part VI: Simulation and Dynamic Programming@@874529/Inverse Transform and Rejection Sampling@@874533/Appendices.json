{"hands_on_practices": [{"introduction": "The inverse transform method is a cornerstone of random variate generation, providing a direct bridge from a distribution's cumulative distribution function (CDF) to a sample generation algorithm. This first practice guides you through the complete process for the standard logistic distribution, from analytical derivation to robust numerical implementation and verification [@problem_id:2403666]. By completing this exercise, you will not only build a working sampler but also uncover the fundamental relationship between the logistic distribution and the log-odds (logit) function, a concept vital in many statistical models.", "problem": "You are asked to implement the inverse transform sampling method for the standard logistic distribution and to demonstrate its equivalence to taking the logarithm of the odds ratio (also known as the log-odds or logit). Your implementation must be a complete, runnable program.\n\nStart from the following foundational base:\n\n- If $U$ is a random variable distributed as a continuous uniform distribution on the unit interval, written $U \\sim \\mathrm{Uniform}(0,1)$, and $F$ is a continuous and strictly increasing cumulative distribution function, then the random variable $X = F^{-1}(U)$ has the cumulative distribution function $F$. This is the principle behind inverse transform sampling and follows from the definition of the cumulative distribution function and the monotonicity of $F$.\n- The standard logistic distribution has cumulative distribution function $F(x) = \\dfrac{1}{1 + e^{-x}}$ for all real $x \\in \\mathbb{R}$. Its mean is $0$ and its variance is $\\dfrac{\\pi^2}{3}$.\n\nYour tasks:\n\n1. Using only the base facts above, derive an explicit inverse transform $F^{-1}$ for the standard logistic cumulative distribution function and explain why this inverse equals the log-odds of the corresponding cumulative probability. The log-odds transformation for a probability $p \\in (0,1)$ is defined as $\\log\\!\\left(\\dfrac{p}{1-p}\\right)$, where $\\log$ denotes the natural logarithm.\n2. Implement numerically stable functions for:\n   - The logistic cumulative distribution function $F(x)$.\n   - The logistic inverse transform $F^{-1}(u)$ on $(0,1)$.\n3. Verify the inverse transform and the log-odds relationship across a small test suite that probes typical and edge-case values.\n\nTest suite and required checks:\n\n- Use the following probabilities $u$ to test the inverse transform: $u \\in \\{0.1, 0.5, 0.9, 10^{-12}, 1 - 10^{-12}\\}$. For each $u$ in this set, compute $x = F^{-1}(u)$ and verify numerically that $|F(x) - u| \\leq \\varepsilon$ with tolerance $\\varepsilon = 10^{-10}$. Record a boolean for each $u$ indicating whether the condition holds.\n- Use the following real values $x$ to test the log-odds relationship: $x \\in \\{-6.0, -1.0, 0.0, 2.5, 10.0\\}$. For each $x$ in this set, compute $u = F(x)$ and verify that $\\left|\\log\\!\\left(\\dfrac{u}{1-u}\\right) - x\\right| \\leq \\varepsilon$ with the same tolerance $\\varepsilon = 10^{-10}$. Record a boolean for each $x$ indicating whether the condition holds.\n- Use inverse transform sampling to generate $N = 100000$ independent and identically distributed (IID) draws from the standard logistic distribution by transforming $U_1, \\dots, U_N$ where $U_i \\sim \\mathrm{Uniform}(0,1)$ into $X_i = F^{-1}(U_i)$. Let $\\bar{X}_N$ be the sample mean and $S_N^2$ be the sample variance. Verify the following conditions with tolerances $\\delta_{\\mathrm{mean}} = 0.02$ and $\\delta_{\\mathrm{var}} = 0.05$:\n  - $|\\bar{X}_N - 0| \\leq \\delta_{\\mathrm{mean}}$,\n  - $\\left|S_N^2 - \\dfrac{\\pi^2}{3}\\right| \\leq \\delta_{\\mathrm{var}}$.\n  Record one boolean for the mean check and one boolean for the variance check.\n\nFinal output format:\n\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[true1,true2,...]\"). The order must be:\n  - The five booleans for the inverse-transform checks at $u \\in \\{0.1, 0.5, 0.9, 10^{-12}, 1 - 10^{-12}\\}$ (in that order),\n  - followed by the five booleans for the log-odds checks at $x \\in \\{-6.0, -1.0, 0.0, 2.5, 10.0\\}$ (in that order),\n  - followed by the two booleans for the Monte Carlo checks on the sample mean and variance (in that order).", "solution": "The problem statement has been evaluated and is determined to be **valid**. It is scientifically grounded in established principles of probability theory and statistics, well-posed with clear objectives and constraints, and formulated in objective, unambiguous language. The required tasks are standard exercises in computational statistics. We will proceed with the solution.\n\nThe solution is organized into three parts: the analytical derivation of the inverse transform, a discussion of its numerically stable implementation, and the protocol for its numerical verification.\n\n### 1. Analytical Derivation\n\nThe core of the inverse transform sampling method rests on the inversion of the Cumulative Distribution Function (CDF).\n\n#### 1.1. Inverse of the Standard Logistic CDF\n\nThe standard logistic distribution is defined by its CDF, $F(x)$:\n$$F(x) = \\frac{1}{1 + e^{-x}}$$\nTo apply the inverse transform sampling method, we must find the inverse function, $x = F^{-1}(u)$, by solving for $x$ given a probability $u = F(x)$, where $u \\in (0, 1)$.\n\nWe begin with the equation for the CDF:\n$$u = \\frac{1}{1 + e^{-x}}$$\nWe perform algebraic manipulation to isolate $x$:\n$$1 + e^{-x} = \\frac{1}{u}$$\n$$e^{-x} = \\frac{1}{u} - 1$$\n$$e^{-x} = \\frac{1 - u}{u}$$\nTaking the natural logarithm of both sides yields:\n$$\\ln(e^{-x}) = \\ln\\left(\\frac{1 - u}{u}\\right)$$\n$$-x = \\ln\\left(\\frac{1 - u}{u}\\right)$$\nFinally, solving for $x$:\n$$x = -\\ln\\left(\\frac{1 - u}{u}\\right) = \\ln\\left(\\left(\\frac{1 - u}{u}\\right)^{-1}\\right) = \\ln\\left(\\frac{u}{1 - u}\\right)$$\nThus, the inverse function, or quantile function, is:\n$$F^{-1}(u) = \\ln\\left(\\frac{u}{1 - u}\\right)$$\n\n#### 1.2. Equivalence to the Log-Odds (Logit) Function\n\nThe log-odds, or logit transformation, of a probability $p$ is defined as $\\log\\left(\\frac{p}{1-p}\\right)$. By setting the probability $p$ to be the cumulative probability $u = F(x)$, the log-odds of $u$ becomes:\n$$\\text{logit}(u) = \\log\\left(\\frac{u}{1 - u}\\right)$$\nComparing this with our derived inverse function $F^{-1}(u)$, we see they are identical:\n$$x = F^{-1}(u) = \\log\\left(\\frac{u}{1 - u}\\right) = \\text{logit}(u)$$\nThis demonstrates a fundamental property of the logistic distribution: a random variable $X$ from a standard logistic distribution is equal to the log-odds of its own cumulative probability, $U=F(X)$. This relationship is central to its application in models such as logistic regression.\n\n### 2. Numerical Implementation Strategy\n\nCorrectness in numerical computation requires attention to potential instabilities such as overflow and loss of precision.\n\n#### 2.1. The CDF Function $F(x)$\n\nThe direct implementation of $F(x) = \\frac{1}{1 + e^{-x}}$ is prone to numerical overflow if $x$ is a large negative number, as $e^{-x}$ will become excessively large. A more stable implementation utilizes an alternative but algebraically equivalent form, $F(x) = \\frac{e^x}{1 + e^x}$, which is stable for negative $x$ as $e^x$ approaches $0$. A robust function should therefore branch based on the sign of $x$:\n- For $x \\ge 0$, use $F(x) = \\frac{1}{1 + e^{-x}}$. Here, $e^{-x}$ is between $0$ and $1$, posing no overflow risk.\n- For $x  0$, use $F(x) = \\frac{e^x}{1 + e^x}$. Here, $e^{x}$ is between $0$ and $1$, similarly posing no overflow risk.\nThis ensures stability across the entire real line $\\mathbb{R}$.\n\n#### 2.2. The Inverse CDF Function $F^{-1}(u)$\n\nThe inverse CDF, $F^{-1}(u) = \\ln\\left(\\frac{u}{1 - u}\\right)$, can be implemented as $\\ln(u) - \\ln(1 - u)$. For values of $u$ extremely close to $1$, the term $1-u$ could suffer from catastrophic cancellation if machine precision is insufficient. However, for standard double-precision floating-point arithmetic and the test values provided (e.g., $u = 1 - 10^{-12}$), the calculation of $1-u$ is exact. Therefore, a direct implementation is sufficient and accurate for this problem.\n\n### 3. Verification Protocol\n\nThe problem specifies three sets of tests to verify the correctness of our derivation and implementation.\n\n1.  **Inverse Transform Check**: We will compute $x = F^{-1}(u)$ for a set of probabilities $u$ and verify that $F(x)$ recovers the original $u$ within a specified tolerance $\\varepsilon = 10^{-10}$. This confirms that our implemented functions are indeed inverses of each other.\n\n2.  **Log-Odds Relationship Check**: For a set of values $x$, we will compute $u=F(x)$ and verify that the log-odds of $u$, $\\log\\left(\\frac{u}{1-u}\\right)$, recovers the original $x$ within the tolerance $\\varepsilon = 10^{-10}$. This numerically validates the identity $x = \\text{logit}(F(x))$.\n\n3.  **Monte Carlo Simulation**: We will generate $N=100000$ random variates from the standard logistic distribution using inverse transform sampling on a uniform random number stream. We will then compute the sample mean $\\bar{X}_N$ and sample variance $S_N^2$. These statistics will be compared against the known theoretical mean ($\\mu=0$) and variance ($\\sigma^2 = \\pi^2/3$) of the standard logistic distribution. The adherence to specified tolerances ($\\delta_{\\mathrm{mean}} = 0.02$ and $\\delta_{\\mathrm{var}} = 0.05$) provides empirical evidence that the sampling method correctly generates variates from the target distribution. A fixed random seed will be used to ensure reproducibility of the results.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and verifies the inverse transform sampling for the standard logistic distribution.\n    \"\"\"\n\n    # --- Task 2: Implement numerically stable functions ---\n\n    def logistic_cdf(x: np.ndarray) - np.ndarray:\n        \"\"\"\n        Numerically stable computation of the standard logistic CDF.\n        F(x) = 1 / (1 + exp(-x))\n        \"\"\"\n        x = np.asanyarray(x, dtype=float)\n        res = np.empty_like(x)\n        \n        # For non-negative x, the standard formula is stable.\n        pos_mask = (x = 0)\n        res[pos_mask] = 1.0 / (1.0 + np.exp(-x[pos_mask]))\n        \n        # For negative x, use the equivalent form F(x) = exp(x) / (1 + exp(x)) to avoid overflow.\n        neg_mask = ~pos_mask\n        exp_x_neg = np.exp(x[neg_mask])\n        res[neg_mask] = exp_x_neg / (1.0 + exp_x_neg)\n        \n        return res\n\n    def logistic_inv_cdf(u: np.ndarray) - np.ndarray:\n        \"\"\"\n        Computes the inverse of the standard logistic CDF (quantile function or logit).\n        F_inv(u) = log(u / (1 - u))\n        \"\"\"\n        u = np.asanyarray(u, dtype=float)\n        # The form log(u) - log(1-u) is equivalent and standard.\n        return np.log(u) - np.log(1 - u)\n\n    # --- Task 3: Verify the implementation ---\n\n    results = []\n    \n    # Test Suite 1: Inverse transform check\n    # For each u, compute x = F_inv(u) and check |F(x) - u| = epsilon\n    epsilon = 1e-10\n    u_vals = np.array([0.1, 0.5, 0.9, 1e-12, 1.0 - 1e-12])\n    \n    x_from_u = logistic_inv_cdf(u_vals)\n    u_recalc = logistic_cdf(x_from_u)\n    inv_transform_checks = np.abs(u_recalc - u_vals) = epsilon\n    results.extend(inv_transform_checks)\n\n    # Test Suite 2: Log-odds relationship check\n    # For each x, compute u = F(x) and check |log(u/(1-u)) - x| = epsilon\n    x_vals = np.array([-6.0, -1.0, 0.0, 2.5, 10.0])\n    \n    u_from_x = logistic_cdf(x_vals)\n    # The log-odds is identical to the inverse CDF function.\n    x_recalc = logistic_inv_cdf(u_from_x)\n    log_odds_checks = np.abs(x_recalc - x_vals) = epsilon\n    results.extend(log_odds_checks)\n\n    # Test Suite 3: Monte Carlo simulation check\n    N = 100000\n    delta_mean = 0.02\n    delta_var = 0.05\n    \n    # Use a fixed seed for reproducibility.\n    np.random.seed(42)\n    \n    # Generate N uniform samples from (0, 1)\n    uniform_samples = np.random.uniform(0.0, 1.0, N)\n    \n    # Apply inverse transform sampling\n    logistic_samples = logistic_inv_cdf(uniform_samples)\n    \n    # Calculate sample mean and variance (with ddof=1 for unbiased estimator)\n    sample_mean = np.mean(logistic_samples)\n    sample_var = np.var(logistic_samples, ddof=1)\n    \n    # Theoretical properties of the standard logistic distribution\n    true_mean = 0.0\n    true_var = np.pi**2 / 3.0\n    \n    # Perform checks\n    mean_check = np.abs(sample_mean - true_mean) = delta_mean\n    var_check = np.abs(sample_var - true_var) = delta_var\n    results.extend([mean_check, var_check])\n\n    # Final output formatting\n    # Convert boolean values to lowercase strings \"true\" or \"false\"\n    results_str = [str(b).lower() for b in results]\n    print(f\"[{','.join(results_str)}]\")\n\nsolve()\n```", "id": "2403666"}, {"introduction": "When the inverse CDF is unknown or difficult to compute, rejection sampling offers a powerful and general alternative. This exercise provides hands-on experience with the complete rejection sampling workflow, from generating candidates with a simple uniform proposal to applying the probabilistic acceptance test for a more complex target distribution [@problem_id:2403643]. By implementing the algorithm using a specified pseudo-random number generator, you will also learn the importance of ensuring reproducibility in computational experiments and see how the choice of the envelope constant $M$ affects sampling efficiency.", "problem": "Construct a program that generates independent draws from a probability distribution on the interval $[0,4\\pi]$ with density proportional to $x \\mapsto \\frac{1}{1+\\lvert \\sin(x)\\rvert}$, and computes specified summary statistics for a set of parameter configurations. All angles are in radians. Your program must rely only on the definition of the target distribution and an explicitly specified source of independent Uniform on $[0,1)$ random numbers, without reading any external input.\n\nThe target density is defined by\n$$\nf(x) \\;=\\; c \\cdot \\frac{1}{1+\\lvert \\sin(x)\\rvert}, \\quad x \\in [0,4\\pi],\n$$\nwith $f(x)=0$ outside this interval, and where the normalization constant $c$ is determined uniquely by the requirement $\\int_0^{4\\pi} f(x)\\,dx = 1$.\n\nFor the purpose of generating candidate points and reporting acceptance rates, consider a proposal density $g$ that is uniform on $[0,4\\pi]$, that is,\n$$\ng(x) \\;=\\; \\frac{1}{4\\pi} \\quad \\text{for } x\\in[0,4\\pi], \\quad \\text{and } g(x)=0 \\text{ otherwise},\n$$\ntogether with a positive constant $M$ such that $f(x)\\le M\\,g(x)$ for all $x\\in[0,4\\pi]$.\n\nRandom number generation must use the following Linear Congruential Generator (LCG) to produce a reproducible sequence of Uniform on $[0,1)$ variables. Let the integer state be $(s_n)_{n\\ge 0}$ with\n$$\ns_{n+1} \\;=\\; (a\\, s_n + c_0) \\bmod m,\n$$\nwhere $a=1664525$, $c_0=1013904223$, and $m=2^{32}$. Each Uniform on $[0,1)$ variate is given by $u_n = s_n/m$. For each test case below, initialize $s_0$ to the specified seed value, and use the generated sequence in order.\n\nTest suite:\n- Test case $1$ (boundary-size sample): Use seed $s_0=314159265$, constant $M=\\frac{\\pi}{2}$, and produce $N=1$ accepted draw. Output the single accepted value $x_1$ rounded to $6$ decimal places.\n- Test case $2$ (moderate sample, exact $M$): Use seed $s_0=271828182$, constant $M=\\frac{\\pi}{2}$, and produce $N=1000$ accepted draws. Output two quantities rounded to $6$ decimal places: first, the overall acceptance rate (accepted draws divided by the total number of proposed points), and second, the sample mean of $x\\mapsto \\sin(x)$ over the accepted draws.\n- Test case $3$ (large sample, suboptimal $M$): Use seed $s_0=141421356$, constant $M=\\frac{\\pi}{2}+\\frac{1}{2}$, and produce $N=50000$ accepted draws. Output two quantities rounded to $6$ decimal places: first, the overall acceptance rate, and second, the empirical probability that an accepted draw lies in the interval $[\\pi,2\\pi]$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order\n$$\n[x_1,\\ \\text{acc\\_2},\\ \\overline{\\sin}_2,\\ \\text{acc\\_3},\\ p_{[\\pi,2\\pi],3}],\n$$\nwhere $x_1$ is from test case $1$, $\\text{acc\\_2}$ and $\\overline{\\sin}_2$ are from test case $2$, and $\\text{acc\\_3}$ and $p_{[\\pi,2\\pi],3}$ are from test case $3$. Every numeric item must be rounded to $6$ decimal places, and no spaces are permitted inside the brackets.", "solution": "The problem statement is subjected to validation.\n\n**Step 1: Extract Givens**\n- Target probability density function (PDF): $f(x) = c \\cdot \\frac{1}{1+\\lvert \\sin(x)\\rvert}$ for $x \\in [0,4\\pi]$, and $f(x)=0$ otherwise. The constant $c$ is defined by $\\int_0^{4\\pi} f(x)\\,dx = 1$.\n- Proposal probability density function: $g(x) = \\frac{1}{4\\pi}$ for $x \\in [0,4\\pi]$, and $g(x)=0$ otherwise.\n- Rejection sampling constant: $M$, a positive constant such that $f(x) \\le M\\,g(x)$ for all $x \\in [0,4\\pi]$.\n- Random number source: A Linear Congruential Generator (LCG) defined by $s_{n+1} = (a\\, s_n + c_0) \\bmod m$, with parameters $a=1664525$, $c_0=1013904223$, $m=2^{32}$. Uniform deviates are $u_n = s_n/m$.\n- Test Case 1: $s_0=314159265$, $M=\\frac{\\pi}{2}$, $N=1$. Output is the single accepted value $x_1$.\n- Test Case 2: $s_0=271828182$, $M=\\frac{\\pi}{2}$, $N=1000$. Outputs are the acceptance rate and the sample mean of $\\sin(x)$.\n- Test Case 3: $s_0=141421356$, $M=\\frac{\\pi}{2}+\\frac{1}{2}$, $N=50000$. Outputs are the acceptance rate and the empirical probability that an accepted draw is in $[\\pi,2\\pi]$.\n- Formatting: All numerical outputs must be rounded to $6$ decimal places.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Grounding:** The problem describes the application of rejection sampling, a standard and fundamental algorithm in computational statistics. The target function $x \\mapsto \\frac{1}{1+\\lvert \\sin(x)\\rvert}$ is non-negative and continuous on $[0, 4\\pi]$. Its integral over this domain is finite, ensuring a valid PDF can be constructed. The normalization constant $c$ is determined by finding this integral. The function $|\\sin(x)|$ has a period of $\\pi$, so the integral is $4 \\int_0^\\pi \\frac{dx}{1+\\sin(x)}$. Using the Weierstrass substitution $t = \\tan(x/2)$, this integral evaluates to $2$. Thus, $\\int_0^{4\\pi} \\frac{dx}{1+|\\sin(x)|} = 4 \\times 2 = 8$. The normalization constant is $c=1/8$, and the target PDF is $f(x) = \\frac{1}{8(1+|\\sin x|)}$ on the domain $[0, 4\\pi]$. The problem is scientifically sound.\n\n- **Well-Posedness:** The rejection sampling method requires a constant $M$ such that $f(x) \\le M g(x)$. This inequality is $\\frac{1}{8(1+|\\sin x|)} \\le M \\frac{1}{4\\pi}$, which simplifies to $M \\ge \\frac{\\pi}{2} \\frac{1}{1+|\\sin x|}$. The expression on the right is maximized when $|\\sin x|$ is minimized, i.e., $|\\sin x|=0$. Thus, we must have $M \\ge \\frac{\\pi}{2}$. The problem provides $M=\\frac{\\pi}{2}$ (the optimal value) and $M=\\frac{\\pi}{2}+\\frac{1}{2}$ (a valid suboptimal value), both of which satisfy the condition. The LCG parameters are standard and produce a deterministic, pseudo-random sequence, making the results reproducible. All required parameters ($s_0$, $M$, $N$) for each test case are explicitly provided. The problem is well-posed and self-contained.\n\n- **Objectivity:** The problem is stated using precise mathematical definitions and objective computational tasks. There are no subjective or ambiguous elements.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A reasoned solution will be provided.\n\n**Solution Derivation**\nThe core of this problem is to implement the rejection sampling algorithm. This method generates samples from a target distribution with density $f(x)$ by using a simpler proposal distribution with density $g(x)$. The method requires a constant $M$ such that the scaled proposal density $M g(x)$ serves as an envelope for $f(x)$, i.e., $f(x) \\le M g(x)$ for all $x$.\n\nThe algorithm to generate one sample is as follows:\n1.  Draw a candidate sample $Y$ from the proposal distribution $g(x)$.\n2.  Draw a random number $U$ from the uniform distribution on $[0,1]$.\n3.  If $U \\le \\frac{f(Y)}{M g(Y)}$, accept the candidate $Y$ as a sample from $f(x)$. Otherwise, reject $Y$ and return to step 1.\n\nFor this specific problem:\n- The proposal distribution $g(x)$ is uniform on $[0, 4\\pi]$. A candidate $Y$ can be generated by drawing a uniform variate $U_1 \\sim U(0,1)$ and setting $Y = 4\\pi \\cdot U_1$.\n- The target density is $f(x) = \\frac{1}{8(1+|\\sin x|)}$.\n- The proposal density is $g(x) = \\frac{1}{4\\pi}$.\n- The acceptance condition check involves the ratio:\n$$\n\\frac{f(Y)}{M g(Y)} = \\frac{\\frac{1}{8(1+|\\sin Y|)}}{M \\frac{1}{4\\pi}} = \\frac{4\\pi}{8M} \\cdot \\frac{1}{1+|\\sin Y|} = \\frac{\\pi}{2M} \\cdot \\frac{1}{1+|\\sin Y|}\n$$\nThe sequence of uniform variates required for generating candidates and for the acceptance check must be produced by the specified LCG: $s_{n+1} = (1664525 \\cdot s_n + 1013904223) \\pmod{2^{32}}$, with $u_n = s_n / 2^{32}$.\n\nFor each test case, the procedure is:\n1.  Initialize the LCG with the given seed $s_0$.\n2.  Initialize an empty list for accepted samples and set a proposal counter to $0$.\n3.  Loop until $N$ samples are collected:\n    a. Increment the proposal counter.\n    b. Generate $U_1$ from the LCG to create a candidate $Y = 4\\pi \\cdot U_1$.\n    c. Generate $U_2$ from the LCG for the acceptance test.\n    d. Calculate the threshold $T = \\frac{\\pi}{2M(1+|\\sin Y|)}$.\n    e. If $U_2 \\le T$, add $Y$ to the list of accepted samples.\n4.  After collecting $N$ samples, compute the required statistics.\n\nThe statistics for each case are:\n- Case 1: The value of the single sample, $x_1$.\n- Case 2: The acceptance rate, defined as $\\frac{N}{\\text{total proposals}}$, and the sample mean $\\frac{1}{N} \\sum_{i=1}^{N} \\sin(x_i)$.\n- Case 3: The acceptance rate and the empirical probability $P(\\pi \\le X \\le 2\\pi)$, calculated as $\\frac{1}{N} \\times (\\text{count of samples } x_i \\text{ such that } \\pi \\le x_i \\le 2\\pi)$.\n\nAll final numerical results must be formatted to six decimal places. The implementation will follow this logic precisely to produce the required output.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by implementing rejection sampling for the given\n    distribution and computing the required statistics for three test cases.\n    \"\"\"\n    \n    # LCG parameters are defined as specified in the problem.\n    A_LCG = 1664525\n    C0_LCG = 1013904223\n    M_LCG = 2**32\n\n    def lcg_generator(seed):\n        \"\"\"\n        Creates a generator that yields a sequence of U(0,1) random numbers\n        using the specified Linear Congruential Generator (LCG).\n        \"\"\"\n        state = seed\n        while True:\n            state = (A_LCG * state + C0_LCG) % M_LCG\n            yield state / M_LCG\n\n    def rejection_sampler(lcg, M, N):\n        \"\"\"\n        Generates N samples from the target distribution using rejection sampling.\n\n        Args:\n            lcg (generator): The random number generator.\n            M (float): The constant for the rejection sampling envelope.\n            N (int): The number of samples to generate.\n\n        Returns:\n            tuple: A tuple containing a list of accepted samples and the total\n                   number of proposals made.\n        \"\"\"\n        accepted_samples = []\n        total_proposals = 0\n        \n        # Pre-compute the constant factor for the acceptance threshold calculation\n        acceptance_factor = np.pi / (2.0 * M)\n\n        while len(accepted_samples)  N:\n            total_proposals += 1\n            \n            # Step 1: Generate candidate Y from proposal g(x) = U[0, 4*pi]\n            u1 = next(lcg)\n            y_candidate = u1 * 4.0 * np.pi\n            \n            # Step 2: Generate U for acceptance check from U[0, 1]\n            u2 = next(lcg)\n            \n            # Step 3: Calculate acceptance threshold and check condition\n            # T(y) = f(y) / (M*g(y)) = (pi/(2*M)) * 1/(1+|sin(y)|)\n            threshold = acceptance_factor / (1.0 + np.abs(np.sin(y_candidate)))\n            \n            if u2 = threshold:\n                accepted_samples.append(y_candidate)\n                \n        return accepted_samples, total_proposals\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (seed, M, N)\n        (314159265, np.pi / 2.0, 1),\n        (271828182, np.pi / 2.0, 1000),\n        (141421356, np.pi / 2.0 + 0.5, 50000),\n    ]\n\n    all_results = []\n\n    # --- Test Case 1 ---\n    seed1, M1, N1 = test_cases[0]\n    lcg1 = lcg_generator(seed1)\n    samples1, _ = rejection_sampler(lcg1, M1, N1)\n    x1 = samples1[0]\n    all_results.append(x1)\n\n    # --- Test Case 2 ---\n    seed2, M2, N2 = test_cases[1]\n    lcg2 = lcg_generator(seed2)\n    samples2, total_proposals2 = rejection_sampler(lcg2, M2, N2)\n    \n    # Calculate acceptance rate\n    acc_2 = N2 / total_proposals2\n    all_results.append(acc_2)\n    \n    # Calculate sample mean of sin(x)\n    mean_sin_2 = np.mean([np.sin(x) for x in samples2])\n    all_results.append(mean_sin_2)\n\n    # --- Test Case 3 ---\n    seed3, M3, N3 = test_cases[2]\n    lcg3 = lcg_generator(seed3)\n    samples3, total_proposals3 = rejection_sampler(lcg3, M3, N3)\n    \n    # Calculate acceptance rate\n    acc_3 = N3 / total_proposals3\n    all_results.append(acc_3)\n    \n    # Calculate empirical probability of x in [pi, 2*pi]\n    count_in_interval = sum(1 for x in samples3 if np.pi = x = 2.0 * np.pi)\n    prob_3 = count_in_interval / N3\n    all_results.append(prob_3)\n    \n    # Format all results to 6 decimal places and join them into the final string\n    formatted_results = [f'{r:.6f}' for r in all_results]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2403643"}, {"introduction": "Simulation methods are only as reliable as their most fundamental ingredient: the stream of uniform random numbers. This final practice is a thought experiment that moves beyond implementation to explore what happens when this core component is flawed [@problem_id:2403661]. By analyzing the cascading failures caused by a low-precision random number generator—from biased estimators to the underestimation of tail risk—you will develop a critical perspective on the assumptions underlying Monte Carlo methods and appreciate why high-quality random number generation is a practical necessity.", "problem": "A researcher in computational risk management wishes to simulate a nonnegative loss random variable $X$ with continuous, strictly increasing cumulative distribution function $F(x)$ using the inverse transform method: draw $U \\sim \\mathrm{Uniform}(0,1)$ and set $X = F^{-1}(U)$. However, the only available pseudo-random number generator for $U$ returns values with exactly $2$ decimal places, i.e., $U \\in \\{0.00, 0.01, \\dots, 0.99\\}$ with equal probability $1/100$. The researcher plans to use these simulations to estimate the expected loss $\\mathbb{E}[X]$ and high-quantile risk measures such as the Value at Risk (VaR), where the $\\alpha$-level Value at Risk is defined as $\\text{VaR}_{\\alpha}(X) = \\inf\\{x \\in \\mathbb{R}: F(x) \\ge \\alpha\\}$. Consider also the specific case where $X$ has the exponential distribution with parameter $\\lambda  0$, so that $F(x) = 1 - e^{-\\lambda x}$ for $x \\ge 0$.\n\nSelect all statements that are true.\n\nA. Under the low-precision generator, the simulated values of $X$ are restricted to the grid $x_k = F^{-1}(k/100)$ for $k \\in \\{0,1,\\dots,99\\}$. Consequently, any estimator of $\\text{VaR}_{0.995}(X)$ based on these simulations converges to $x_{99} = F^{-1}(0.99)$ as the number of simulations grows, thereby underestimating the true $\\text{VaR}_{0.995}(X)$ whenever $F^{-1}$ is strictly increasing.\n\nB. Even with the low-precision generator, the sample mean of the inverse-transform simulations is a consistent estimator of $\\mathbb{E}[X]$ as the number of simulations $n \\to \\infty$.\n\nC. In the exponential case with parameter $\\lambda$, the expected value computed from the low-precision inverse-transform simulator equals $(1/100)\\sum_{k=0}^{99} F^{-1}(k/100)$ and is strictly less than $1/\\lambda$.\n\nD. Switching from inverse transform sampling to acceptance-rejection (rejection sampling) with a bounded proposal density resolves the discretization bias caused by the two-decimal-place uniform generator without any other changes.\n\nE. It is possible to recover arbitrarily fine resolution from the two-decimal-place generator by combining multiple independent draws: if $U_j \\in \\{0.00,0.01,\\dots,0.99\\}$ are independent and $D_j = 100\\,U_j \\in \\{0,1,\\dots,99\\}$, then for any positive integer $m$, the variable $V_m = \\sum_{j=1}^{m} D_j\\,100^{-j}$ is uniformly distributed over the grid $\\{0, 1/100^{m}, 2/100^{m}, \\dots, (100^{m}-1)/100^{m}\\}$, enabling inverse transform sampling at resolution $10^{-2m}$ as $m$ increases.\n\nChoose all that apply.", "solution": "The problem statement is subjected to validation.\n\n### Step 1: Extract Givens\n- A nonnegative loss random variable $X$.\n- The cumulative distribution function (CDF) of $X$ is $F(x)$, which is continuous and strictly increasing.\n- Simulation method: inverse transform, $X = F^{-1}(U)$ with $U \\sim \\text{Uniform}(0,1)$.\n- The available pseudo-random number generator for $U$ is discrete, producing values from the set $\\{0.00, 0.01, \\dots, 0.99\\}$ with equal probability $1/100$. Let's denote this discrete random variable as $U_d$. So, $P(U_d = k/100) = 1/100$ for $k \\in \\{0, 1, \\dots, 99\\}$.\n- The goal is to estimate the expected loss $\\mathbb{E}[X]$ and Value at Risk (VaR).\n- The $\\alpha$-level Value at Risk is defined as $\\text{VaR}_{\\alpha}(X) = \\inf\\{x \\in \\mathbb{R}: F(x) \\ge \\alpha\\}$.\n- A specific case is considered: $X$ follows an exponential distribution with parameter $\\lambda  0$, where $F(x) = 1 - e^{-\\lambda x}$ for $x \\ge 0$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed based on the specified criteria.\n- **Scientifically Grounded**: The problem is well-rooted in the principles of computational statistics and quantitative finance. Inverse transform sampling, properties of pseudo-random number generators, expected value, and Value at Risk are standard concepts. The scenario of a limited-precision generator is a practical and educational simplification of real-world computational constraints.\n- **Well-Posed**: The problem is clearly formulated. It describes a specific simulation setup and asks for an evaluation of five distinct statements concerning the properties and consequences of this setup. Each statement is a testable mathematical proposition.\n- **Objective**: The language is precise, technical, and free of any subjectivity or ambiguity.\nThe problem statement does not violate any of the invalidity criteria. It is a self-contained, consistent, and scientifically sound problem.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. A solution will be derived.\n\nThe researcher uses a discrete uniform random variable $U_d$ where $P(U_d = k/100) = 1/100$ for $k \\in \\{0, 1, \\dots, 99\\}$. The simulated loss variable is $X_d = F^{-1}(U_d)$. The possible values for $X_d$ are $x_k = F^{-1}(k/100)$ for $k \\in \\{0, 1, \\dots, 99\\}$.\n\n### Analysis of Option A\nThe statement claims that an estimator of $\\text{VaR}_{0.995}(X)$ based on these simulations will converge to $x_{99} = F^{-1}(0.99)$, underestimating the true $\\text{VaR}_{0.995}(X)$.\n\nThe true Value at Risk at level $\\alpha=0.995$ is $\\text{VaR}_{0.995}(X) = F^{-1}(0.995)$. This is because $F$ is continuous and strictly increasing, so $\\inf\\{x: F(x) \\ge \\alpha\\} = F^{-1}(\\alpha)$.\n\nThe random variable being simulated is $X_d = F^{-1}(U_d)$. The maximum possible value that $U_d$ can take is $0.99$. Since $F^{-1}$ is strictly increasing (because $F$ is), the maximum possible simulated value for the loss is $X_{\\text{max}} = F^{-1}(0.99) = x_{99}$.\n\nAny statistical estimator of a quantile based on a sample from the distribution of $X_d$ cannot produce a value greater than the maximum possible value in that distribution's support. As the number of simulations grows, any consistent empirical quantile estimator will converge to the theoretical quantile of the variable $X_d$.\n\nThe $\\text{VaR}_{0.995}$ of the simulated variable $X_d$ is $\\inf\\{x : P(X_d \\le x) \\ge 0.995\\}$.\n$P(X_d \\le x) = P(F^{-1}(U_d) \\le x) = P(U_d \\le F(x))$.\nWe need to find the smallest $x$ such that $P(U_d \\le F(x)) \\ge 0.995$.\nThe CDF of $U_d$ is a step function. $P(U_d \\le y) = 1$ for $y \\ge 0.99$. To have $P(U_d \\le F(x)) \\ge 0.995$, we must have $P(U_d \\le F(x))=1$. This requires $F(x) \\ge 0.99$.\nSince $F$ is strictly increasing, the smallest $x$ that satisfies $F(x) \\ge 0.99$ is $x = F^{-1}(0.99)$.\nThus, the estimator for the $0.995$-quantile based on simulations of $X_d$ will converge to $F^{-1}(0.99)$.\n\nThe true value is $\\text{VaR}_{0.995}(X) = F^{-1}(0.995)$.\nSince $F^{-1}$ is strictly increasing, $F^{-1}(0.99)  F^{-1}(0.995)$.\nThe simulation-based estimate converges to $F^{-1}(0.99)$, which is a strict underestimation of the true value $F^{-1}(0.995)$. This is a critical failure in risk management, as tail risk is underestimated.\n\nThe statement is **Correct**.\n\n### Analysis of Option B\nThe statement claims the sample mean of the simulations is a consistent estimator of $\\mathbb{E}[X]$.\nThe sample mean is $\\bar{X}_{d,n} = \\frac{1}{n} \\sum_{i=1}^n X_{d,i}$, where $X_{d,i}$ are independent draws of $X_d = F^{-1}(U_d)$.\nBy the Law of Large Numbers, as the number of simulations $n \\to \\infty$, the sample mean converges to the expected value of the simulated variable: $\\bar{X}_{d,n} \\to \\mathbb{E}[X_d]$.\nAn estimator is consistent for a parameter (here, $\\mathbb{E}[X]$) if it converges in probability to that parameter. Thus, we must check if $\\mathbb{E}[X_d] = \\mathbb{E}[X]$.\n\nThe expected value of the simulated variable is:\n$$ \\mathbb{E}[X_d] = \\mathbb{E}[F^{-1}(U_d)] = \\sum_{k=0}^{99} F^{-1}\\left(\\frac{k}{100}\\right) P\\left(U_d = \\frac{k}{100}\\right) = \\frac{1}{100} \\sum_{k=0}^{99} F^{-1}\\left(\\frac{k}{100}\\right) $$\nThe true expected value is:\n$$ \\mathbb{E}[X] = \\mathbb{E}[F^{-1}(U)] = \\int_0^1 F^{-1}(u) \\,du $$\nThe sum for $\\mathbb{E}[X_d]$ is a left Riemann sum approximation of the integral for $\\mathbb{E}[X]$, with partition width $\\Delta u = 1/100$. For a general strictly increasing function $F^{-1}$, this sum is not equal to the integral. For example, if $F^{-1}(u)$ is strictly convex, the left Riemann sum will be strictly less than the integral. If it is strictly concave, it will be strictly greater. In general, $\\mathbb{E}[X_d] \\neq \\mathbb{E}[X]$.\nSince the sample mean converges to $\\mathbb{E}[X_d]$, which is not $\\mathbb{E}[X]$, the estimator has a systematic bias and is not a consistent estimator of $\\mathbb{E}[X]$.\n\nThe statement is **Incorrect**.\n\n### Analysis of Option C\nThis statement specializes to the exponential case, $F(x) = 1 - e^{-\\lambda x}$.\nThe inverse CDF is found by setting $u = F(x) = 1 - e^{-\\lambda x}$, which gives $x = F^{-1}(u) = -\\frac{1}{\\lambda} \\ln(1-u)$.\nThe true expected value is $\\mathbb{E}[X] = 1/\\lambda$.\n\nThe first part of the statement says the expected value from the simulator is $(1/100)\\sum_{k=0}^{99} F^{-1}(k/100)$. As established in the analysis of option B, this is the correct expression for $\\mathbb{E}[X_d]$.\n$$ \\mathbb{E}[X_d] = \\frac{1}{100} \\sum_{k=0}^{99} F^{-1}\\left(\\frac{k}{100}\\right) = \\frac{1}{100} \\sum_{k=0}^{99} \\left(-\\frac{1}{\\lambda} \\ln\\left(1-\\frac{k}{100}\\right)\\right) $$\nThe second part claims this value is strictly less than $1/\\lambda$. This means we must show $\\mathbb{E}[X_d]  \\mathbb{E}[X]$.\nAs noted before, $\\mathbb{E}[X_d]$ is a left Riemann sum for the integral $\\mathbb{E}[X] = \\int_0^1 F^{-1}(u) \\,du$. Let's analyze the convexity of $g(u) = F^{-1}(u) = -\\frac{1}{\\lambda} \\ln(1-u)$.\nThe first derivative is $g'(u) = -\\frac{1}{\\lambda} \\frac{-1}{1-u} = \\frac{1}{\\lambda(1-u)}$.\nThe second derivative is $g''(u) = \\frac{1}{\\lambda} (-1)(1-u)^{-2}(-1) = \\frac{1}{\\lambda(1-u)^2}$.\nSince $\\lambda  0$, we have $g''(u)  0$ for all $u \\in [0,1)$. This means $F^{-1}(u)$ is a strictly convex function on its domain.\nFor a strictly convex function, any left Riemann sum is strictly less than the value of the definite integral over the same interval.\nTherefore,\n$$ \\mathbb{E}[X_d] = \\frac{1}{100} \\sum_{k=0}^{99} F^{-1}\\left(\\frac{k}{100}\\right)  \\int_0^1 F^{-1}(u) \\,du = \\mathbb{E}[X] = \\frac{1}{\\lambda} $$\nBoth parts of the statement are true.\n\nThe statement is **Correct**.\n\n### Analysis of Option D\nThis statement suggests that switching to acceptance-rejection (A-R) sampling would resolve the discretization bias.\nThe standard A-R algorithm requires drawing a candidate sample $Y$ from a proposal distribution $g$ and a uniform random number $W \\sim \\text{Uniform}(0,1)$, then accepting $Y$ if $W \\le \\frac{f(Y)}{Mg(Y)}$, where $f$ is the target density and $M$ is a constant such that $f(x) \\le Mg(x)$.\nThe problem states that the *only* available generator is the discrete one, $U_d$. If we are forced to use $U_d$ in place of $W$ for the acceptance check, the condition becomes $U_d \\le \\frac{f(Y)}{Mg(Y)}$.\nFor a continuous uniform $W$, the probability of acceptance for a given $Y$ is $P(W \\le c) = c$, where $c = \\frac{f(Y)}{Mg(Y)}$. This property is essential for ensuring the accepted samples follow the density $f$.\nFor the discrete generator $U_d$, the probability of acceptance is $P(U_d \\le c) = (\\lfloor 100c \\rfloor + 1)/100$ (for $c  1$). This is a step function and is not equal to $c$. The discretization of the uniform variable corrupts the acceptance probability. The distribution of the accepted samples would therefore not be the target distribution $f(x)$. The discretization bias is not resolved; it is merely transferred to a different part of the simulation algorithm.\nThe statement that the bias is resolved \"without any other changes\" is false.\n\nThe statement is **Incorrect**.\n\n### Analysis of Option E\nThis statement proposes a method to construct a higher-resolution uniform random variable from the low-precision one.\nLet $D_j = 100 U_j$, where $U_j$ are independent draws from the discrete generator. Each $D_j$ is an independent random variable uniformly distributed on the set of integers $\\{0, 1, \\dots, 99\\}$.\nThe proposed variable is $V_m = \\sum_{j=1}^{m} D_j 100^{-j}$.\nLet's analyze $V_m$:\n$$ V_m = \\frac{D_1}{100} + \\frac{D_2}{100^2} + \\dots + \\frac{D_m}{100^m} = \\frac{D_1 100^{m-1} + D_2 100^{m-2} + \\dots + D_m}{100^m} $$\nThe numerator is a random integer constructed from $m$ independent \"digits\" $D_j$ in base $100$. Each sequence of digits $(D_1, \\dots, D_m)$ corresponds to a unique integer value from $0$ (all $D_j=0$) to $99 \\cdot 100^{m-1} + \\dots + 99 = 100^m - 1$.\nSince each $D_j$ is uniform on $\\{0, \\dots, 99\\}$ and they are independent, each possible sequence of $(D_1, \\dots, D_m)$ is equally likely, with probability $(1/100)^m = 1/100^m$.\nThis implies that every integer from $0$ to $100^m-1$ is a possible outcome for the numerator, each with probability $1/100^m$.\nThus, $V_m$ is a random variable uniformly distributed on the grid $\\{0/100^m, 1/100^m, \\dots, (100^m-1)/100^m\\}$.\nThe grid of possible values for $V_m$ has $100^m$ points, and the resolution is $100^{-m} = 10^{-2m}$. As $m$ increases, this discrete uniform distribution provides an increasingly fine approximation to the continuous $\\mathrm{Uniform}(0,1)$ distribution. Using $V_m$ in the inverse transform method, $X=F^{-1}(V_m)$, allows for simulations where the discretization error can be made arbitrarily small by choosing a sufficiently large $m$.\nThis technique is valid and correctly described.\n\nThe statement is **Correct**.", "answer": "$$\\boxed{ACE}$$", "id": "2403661"}]}