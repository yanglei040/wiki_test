{"hands_on_practices": [{"introduction": "This first exercise grounds the abstract concept of Monte Carlo integration in a concrete, physical scenario. By manually calculating an estimate from a small, predefined set of sample points, you will walk through the core mechanics of the method: checking if a sample falls within the integration domain, evaluating the function, and averaging the results. This foundational practice [@problem_id:1376816] is designed to build your intuition for how random sampling can be used to approximate the value of a complex integral.", "problem": "A materials scientist is studying a new alloy created in a cubical mold of side length 1 meter. The coordinate system is aligned with the mold, such that it occupies the region defined by $0 \\le x \\le 1$, $0 \\le y \\le 1$, and $0 \\le z \\le 1$. The concentration of a special hardening agent, $C$, is found to be non-zero only in a specific sub-region of the mold defined by the inequalities $0 \\le z \\le y \\le x \\le 1$. Within this sub-region, the concentration at a point $(x,y,z)$ is described by the function $C(x,y,z) = k x y z$, where $k$ is a constant. Outside this region, the concentration is zero. The total mass of the agent in the mold is given by the integral of the concentration function over the entire volume of the 1-meter-cubed mold.\n\nTo estimate this total mass, an automated measurement system probes the concentration at a set of $N=5$ sample points, which are assumed to be representative of a uniform random sampling within the unit cube. The coordinates of these five points are:\n$P_1 = (0.8, 0.7, 0.2)$\n$P_2 = (0.9, 0.5, 0.6)$\n$P_3 = (0.6, 0.8, 0.3)$\n$P_4 = (0.5, 0.4, 0.3)$\n$P_5 = (0.7, 0.9, 0.8)$\n\nGiven the concentration constant $k = 4.8 \\text{ kg/m}^6$, calculate the numerical estimate for the total mass of the hardening agent in the mold based on this set of five sample points. Express your answer in kilograms (kg) and round to three significant figures.", "solution": "The total mass is the volume integral of the concentration over the unit cube:\n$$\nM=\\iiint_{[0,1]^{3}} C(x,y,z)\\,dV.\n$$\nWith uniform random sampling over a domain of volume $V=1$, the Monte Carlo estimator with $N$ samples $\\{P_{i}\\}_{i=1}^{N}$ is\n$$\n\\widehat{M}=\\frac{V}{N}\\sum_{i=1}^{N} C(P_{i})=\\frac{1}{N}\\sum_{i=1}^{N} C(P_{i}).\n$$\nHere $C(x,y,z)=k\\,x y z$ if $0 \\le z \\le y \\le x \\le 1$ and $C=0$ otherwise. Evaluate the indicator $0 \\le z \\le y \\le x \\le 1$ for each sample:\n- $P_{1}=(0.8,0.7,0.2)$: $0.2 \\le 0.7 \\le 0.8 \\le 1$ is true, so contributes $k\\cdot 0.8\\cdot 0.7\\cdot 0.2=0.112\\,k$.\n- $P_{2}=(0.9,0.5,0.6)$: $0.6 \\le 0.5$ is false, contributes $0$.\n- $P_{3}=(0.6,0.8,0.3)$: $0.8 \\le 0.6$ is false, contributes $0$.\n- $P_{4}=(0.5,0.4,0.3)$: $0.3 \\le 0.4 \\le 0.5 \\le 1$ is true, contributes $k\\cdot 0.5\\cdot 0.4\\cdot 0.3=0.06\\,k$.\n- $P_{5}=(0.7,0.9,0.8)$: $0.9 \\le 0.7$ is false, contributes $0$.\nTherefore\n$$\n\\sum_{i=1}^{5} C(P_{i})=(0.112+0.06)\\,k=0.172\\,k,\n$$\nand\n$$\n\\widehat{M}=\\frac{1}{5}\\cdot 0.172\\,k=0.0344\\,k.\n$$\nSubstituting $k=4.8$ gives\n$$\n\\widehat{M}=0.0344\\times 4.8=0.16512,\n$$\nwhich, rounded to three significant figures, is $0.165$ kilograms.", "answer": "$$\\boxed{0.165}$$", "id": "1376816"}, {"introduction": "Moving from simple integrals to more complex economic models, this practice demonstrates the true power and versatility of Monte Carlo methods. You will estimate the Shapley value, a cornerstone concept in cooperative game theory for fair resource allocation, by simulating the formation of coalitions [@problem_id:2411557]. This problem showcases how Monte Carlo can approximate expected values in high-dimensional spaces—in this case, the space of all possible founder arrival orders—which are intractable to analyze exhaustively, making it an indispensable tool in computational economics.", "problem": "A startup’s founding team decides to allocate equity according to each founder’s marginal contribution in a cooperative production environment formalized by cooperative game theory. Let $N=\\{1,\\dots,n\\}$ be the set of founders and let $v:2^N \\to \\mathbb{R}_{\\ge 0}$ be a characteristic function that maps each coalition $S \\subseteq N$ to a nonnegative, real-valued output. The Shapley value $\\phi_i(v)$ of founder $i \\in N$ is defined as the expected marginal contribution of $i$ to a coalition of predecessors when founders arrive in a uniformly random order. Using Monte Carlo (MC) simulation, approximate the Shapley value for each founder by sampling independent Uniform(“all permutations of $N$”) permutations, computing each sampled marginal contribution, and averaging.\n\nBase principles to use:\n- Cooperative game definition: the value function $v$ assigns a well-defined payoff to each coalition $S \\subseteq N$, with $v(\\varnothing)=0$.\n- Shapley value definition via permutations: for any $i \\in N$, \n$$\n\\phi_i(v)=\\mathbb{E}_{\\pi}\\left[v\\!\\left(P_i^{\\pi}\\cup\\{i\\}\\right)-v\\!\\left(P_i^{\\pi}\\right)\\right],\n$$\nwhere $\\pi$ is a uniformly random permutation of $N$ and $P_i^{\\pi}$ is the set of players preceding $i$ in $\\pi$.\n- Monte Carlo integration (sample average approximation): if $X_1,\\dots,X_M$ are independent and identically distributed (i.i.d.) samples from a distribution with finite mean $\\mu$, then by the Law of Large Numbers the sample average $\\frac{1}{M}\\sum_{m=1}^M X_m$ converges to $\\mu$ as $M \\to \\infty$.\n\nAlgorithmic task:\n- For each test case below, estimate the Shapley value vector $\\phi(v)=(\\phi_1(v),\\dots,\\phi_n(v))$ by drawing $M$ i.i.d. permutations, computing the marginal contribution at each position in each permutation, and averaging. Use the specified pseudo-random seeds for reproducibility. Represent coalitions in any convenient manner (e.g., bit masks). Precompute the value function on all coalitions $S \\subseteq N$ if helpful.\n\nNumerical settings common to all tests:\n- For each test case, draw exactly $M=100000$ i.i.d. permutations uniformly from the $n!$ possible permutations.\n- Use the specified seed $s$ for the pseudo-random number generator at the start of each test case.\n- Report each component of the estimated Shapley value rounded to exactly $6$ decimal places.\n- No physical units are involved. All outputs are to be real numbers.\n\nTest suite:\n- Test A (additive, baseline “happy path”): let $n=3$. Let $a=(0.2,0.3,0.5)$. Define\n$$\nv_A(S)=\\sum_{i\\in S} a_i,\n$$\nwith $v_A(\\varnothing)=0$. Use seed $s_A=314159$.\n- Test B (pairwise synergies with diminishing returns): let $n=4$. Let $a=(0.8,0.6,0.4,0.2)$ and symmetric pairwise synergy matrix $B=(b_{ij})$ with $b_{ii}=0$ and the nonzero entries\n$$\nb_{12}=0.3,\\quad b_{13}=0.05,\\quad b_{23}=0.1,\\quad b_{34}=0.2,\n$$\nwith symmetry $b_{ij}=b_{ji}$ and all other $b_{ij}=0$. Define\n$$\nv_B(S)=\\log\\!\\Big(1+\\sum_{i\\in S} a_i+\\sum_{i<j,\\, i,j\\in S} b_{ij}\\Big),\n$$\nwhere $\\log$ denotes the natural logarithm, and $v_B(\\varnothing)=0$. Use seed $s_B=271828$.\n- Test C (threshold output, edge case with a zero-contribution founder): let $n=4$. Let $a=(0.4,0.35,0.25,0.0)$ and threshold $T=0.9$. Define\n$$\nv_C(S)=\\max\\!\\Big(0,\\sum_{i\\in S} a_i - T\\Big),\n$$\nwith $v_C(\\varnothing)=0$. Use seed $s_C=161803$.\n\nRequired final output format:\nYour program should produce a single line of output that is a JSON-like list of lists. The line must contain three inner lists, one per test case in the order A, B, C. Each inner list must contain the $n$ estimated Shapley values for that test case, each rounded to $6$ decimal places, and written with no spaces. For example, the structural form must be\nouter list: opening bracket, inner lists separated by commas, closing bracket,\ninner list: opening bracket, $n$ real numbers separated by commas, closing bracket.", "solution": "The problem statement is parsed and determined to be valid. It is scientifically grounded in cooperative game theory and numerical analysis, well-posed with all necessary parameters defined, and phrased in objective, formal language. There are no contradictions, ambiguities, or unsound premises. The task is to implement a standard Monte Carlo simulation to estimate Shapley values, a well-defined computational problem.\n\nThe theoretical foundation for the solution is the definition of the Shapley value, $\\phi_i(v)$, for a player $i$ in a cooperative game with characteristic function $v$. The value $\\phi_i(v)$ represents the fair allocation of the total surplus generated by the grand coalition $N$, and it is uniquely characterized by a set of axioms (efficiency, symmetry, null player, and additivity). An equivalent and computationally useful definition is based on an expectation over random orderings of players. Specifically, the Shapley value for player $i$ is the expected marginal contribution of player $i$ to the coalition of players preceding them, averaged over all possible permutations of the set of players $N$. The expectation is taken with respect to a uniform probability distribution over the $n!$ possible permutations.\n\n$$\n\\phi_i(v)=\\mathbb{E}_{\\pi}\\left[v(P_i^{\\pi}\\cup\\{i\\}) - v(P_i^{\\pi})\\right]\n$$\n\nHere, $\\pi$ is a permutation of $N=\\{1, \\dots, n\\}$ chosen uniformly at random, and $P_i^{\\pi}$ is the set of players who appear before player $i$ in the permutation $\\pi$.\n\nDirect enumeration of all $n!$ permutations is computationally infeasible for even moderately large $n$. Therefore, we employ a Monte Carlo method. By the Law of Large Numbers, the expectation $\\mathbb{E}[X]$ of a random variable $X$ can be approximated by the sample mean of a large number of independent and identically distributed (i.i.d.) draws of $X$. In this context, the random variable for player $i$ is their marginal contribution $X_i(\\pi) = v(P_i^{\\pi}\\cup\\{i\\}) - v(P_i^{\\pi})$ for a randomly chosen permutation $\\pi$. We generate a large number, $M$, of i.i.d. random permutations $\\pi_1, \\pi_2, \\dots, \\pi_M$. The Shapley value $\\phi_i(v)$ is then estimated by the sample average:\n\n$$\n\\hat{\\phi}_i(v) = \\frac{1}{M} \\sum_{m=1}^{M} \\left[ v(P_i^{\\pi_m}\\cup\\{i\\}) - v(P_i^{\\pi_m}) \\right]\n$$\n\nThe algorithm proceeds as follows for each test case, which is defined by the number of players $n$, the characteristic function $v$, a pseudo-random seed $s$, and the number of Monte Carlo samples $M=100000$.\n\n1.  **Initialization**: A pseudo-random number generator is initialized with the specified seed $s$ for reproducibility. An array, `total_contributions`, of size $n$ is initialized to all zeros. This array will accumulate the marginal contributions for each player across all sampled permutations.\n\n2.  **Pre-computation of Characteristic Function**: For a small number of players $n$, the number of distinct coalitions, $2^n$, is manageable. To optimize the simulation, we pre-compute the value $v(S)$ for every possible coalition $S \\subseteq N$. Coalitions are efficiently represented using integer bitmasks, where an integer $k$ from $0$ to $2^n-1$ represents a coalition. The $j$-th bit of $k$ is $1$ if player $j$ (using $0$-based indexing) is in the coalition, and $0$ otherwise. A table, `v_values`, of size $2^n$ is populated such that `v_values[k]` stores the value $v(S_k)$, where $S_k$ is the coalition corresponding to the mask $k$.\n\n3.  **Monte Carlo Simulation Loop**: The main loop iterates $M$ times. In each iteration $m=1, \\dots, M$:\n    a. A new random permutation $\\pi_m$ of the players $\\{0, 1, \\dots, n-1\\}$ is generated using the initialized random number generator.\n    b. We iterate through the players in the order specified by $\\pi_m$. A variable, `predecessor_mask`, is maintained, starting at $0$ (representing the empty set $v(\\varnothing)=0$).\n    c. For each player $i$ at position $j$ in the permutation $\\pi_m$, their marginal contribution is calculated. The coalition of predecessors $P_i^{\\pi_m}$ is represented by `predecessor_mask`. The value $v(P_i^{\\pi_m})$ is looked up from the pre-computed table as `v_values[predecessor_mask]`.\n    d. The new coalition $S' = P_i^{\\pi_m} \\cup \\{i\\}$ is formed by setting the $i$-th bit in `predecessor_mask`. The value $v(S')$ is then looked up.\n    e. The marginal contribution, $\\Delta_i = v(S') - v(P_i^{\\pi_m})$, is computed and added to `total_contributions[i]`.\n    f. The `predecessor_mask` is updated to include player $i$ for the next player in the permutation.\n\n4.  **Final Estimation**: After the loop completes, the estimated Shapley value for each player $i$ is calculated by dividing the total accumulated marginal contribution by the number of samples $M$: $\\hat{\\phi}_i(v) = \\text{total\\_contributions}[i] / M$.\n\nThis general algorithm is applied to each of the three test cases, using their specific characteristic functions for the pre-computation step:\n\n-   **Test A**: $n=3$, $a=(0.2,0.3,0.5)$, $v_A(S)=\\sum_{i\\in S} a_i$. This is an additive game, where the analytical Shapley value is $\\phi_i(v_A) = a_i$. The Monte Carlo estimate should be very close to $(0.2, 0.3, 0.5)$.\n\n-   **Test B**: $n=4$, $a=(0.8,0.6,0.4,0.2)$, $B$ is a symmetric synergy matrix. The function $v_B(S)=\\log(1+\\sum_{i\\in S} a_i+\\sum_{i<j,\\, i,j\\in S} b_{ij})$ models individual contributions and pairwise synergies with diminishing returns, captured by the natural logarithm.\n\n-   **Test C**: $n=4$, $a=(0.4,0.35,0.25,0.0)$, $T=0.9$. The function $v_C(S)=\\max(0,\\sum_{i\\in S} a_i - T)$ represents a threshold project. Note that player $4$ (index $3$) has an individual contribution of $a_4=0.0$. Consequently, their marginal contribution is always $v_C(S \\cup \\{4\\}) - v_C(S) = \\max(0, \\sum_{i \\in S} a_i - T) - \\max(0, \\sum_{i \\in S} a_i - T) = 0$. Thus, their analytical Shapley value is $\\phi_4(v_C) = 0.0$, which provides a clear check for the implementation.\n\nThe final results are rounded to $6$ decimal places and formatted as specified.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem by running Monte Carlo simulations for each test case.\n    \"\"\"\n\n    def estimate_shapley_values(n, v_func, M, seed):\n        \"\"\"\n        Estimates Shapley values for a given game using Monte Carlo simulation.\n\n        Args:\n            n (int): Number of players.\n            v_func (function): The characteristic function v(S_mask).\n            M (int): Number of Monte Carlo samples (permutations).\n            seed (int): Seed for the pseudo-random number generator.\n\n        Returns:\n            np.ndarray: An array of estimated Shapley values for each player.\n        \"\"\"\n        rng = np.random.default_rng(seed)\n        \n        # 1. Pre-compute characteristic function values for all 2^n coalitions\n        num_coalitions = 1 << n\n        v_values = np.zeros(num_coalitions, dtype=float)\n        for mask in range(num_coalitions):\n            v_values[mask] = v_func(mask, n)\n\n        # 2. Monte Carlo simulation\n        total_marginal_contributions = np.zeros(n, dtype=float)\n        players = np.arange(n)\n\n        for _ in range(M):\n            # Generate a random permutation of players\n            perm = rng.permutation(players)\n            \n            predecessor_mask = 0\n            for player in perm:\n                # Marginal contribution of 'player' to the coalition of its predecessors\n                # Value of coalition without the player\n                v_predecessors = v_values[predecessor_mask]\n\n                # Value of coalition with the player\n                new_coalition_mask = predecessor_mask | (1 << player)\n                v_with_player = v_values[new_coalition_mask]\n\n                marginal_contribution = v_with_player - v_predecessors\n                total_marginal_contributions[player] += marginal_contribution\n\n                # Add player to the coalition for the next iteration\n                predecessor_mask = new_coalition_mask\n\n        # 3. Average the contributions to get the Shapley value estimates\n        shapley_values = total_marginal_contributions / M\n        return shapley_values\n\n    # Common numerical settings\n    M = 100000\n\n    # Test Case A\n    n_A = 3\n    a_A = np.array([0.2, 0.3, 0.5])\n    seed_A = 314159\n    def v_A(mask, n):\n        val = 0.0\n        for i in range(n):\n            if (mask >> i) & 1:\n                val += a_A[i]\n        return val\n\n    # Test Case B\n    n_B = 4\n    a_B = np.array([0.8, 0.6, 0.4, 0.2])\n    B = np.zeros((n_B, n_B))\n    B[0, 1] = B[1, 0] = 0.3\n    B[0, 2] = B[2, 0] = 0.05\n    B[1, 2] = B[2, 1] = 0.1\n    B[2, 3] = B[3, 2] = 0.2\n    seed_B = 271828\n    def v_B(mask, n):\n        sum_a = 0.0\n        sum_b = 0.0\n        players_in_coalition = []\n        for i in range(n):\n            if (mask >> i) & 1:\n                sum_a += a_B[i]\n                players_in_coalition.append(i)\n        \n        for i in range(len(players_in_coalition)):\n            for j in range(i + 1, len(players_in_coalition)):\n                p1 = players_in_coalition[i]\n                p2 = players_in_coalition[j]\n                sum_b += B[p1, p2]\n                \n        return np.log(1.0 + sum_a + sum_b)\n\n    # Test Case C\n    n_C = 4\n    a_C = np.array([0.4, 0.35, 0.25, 0.0])\n    T_C = 0.9\n    seed_C = 161803\n    def v_C(mask, n):\n        sum_a = 0.0\n        for i in range(n):\n            if (mask >> i) & 1:\n                sum_a += a_C[i]\n        return max(0.0, sum_a - T_C)\n\n    test_cases = [\n        {'n': n_A, 'v_func': v_A, 'seed': seed_A},\n        {'n': n_B, 'v_func': v_B, 'seed': seed_B},\n        {'n': n_C, 'v_func': v_C, 'seed': seed_C},\n    ]\n\n    all_results = []\n    for case in test_cases:\n        shapley_values = estimate_shapley_values(case['n'], case['v_func'], M, case['seed'])\n        \n        # Format results to 6 decimal places\n        formatted_results = [f\"{val:.6f}\" for val in shapley_values]\n        all_results.append(f\"[{','.join(formatted_results)}]\")\n\n    # Print the final output in the required format\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n\n```", "id": "2411557"}, {"introduction": "A powerful tool is only useful when its limitations are understood. This final practice delves into a critical theoretical aspect of Monte Carlo integration: its reliance on the Central Limit Theorem for estimating error. You will investigate a scenario where the integrand has infinite variance, a condition that violates a key assumption of the theorem, and observe how this leads to unreliable confidence intervals [@problem_id:2411534]. This exercise is essential for developing the critical judgment needed to apply Monte Carlo methods responsibly and to diagnose potential issues in your simulations.", "problem": "Consider the family of integrals over the unit interval given by\n$$ I(p) \\equiv \\int_{0}^{1} u^{p} \\, du, $$\nwhere $p$ is a real parameter satisfying $p > -1$ so that the integral exists and is finite. For a random variable $U$ uniformly distributed on $[0,1]$, define the integrand as $f_{p}(U) = U^{p}$. The exact value is\n$$ I(p) = \\frac{1}{p+1}. $$\nFor a given sample size $n \\in \\mathbb{N}$ and number of batches $B \\in \\mathbb{N}$, consider $B$ independent batches, each consisting of $n$ independent draws $U_{1},\\dots,U_{n}$ from the uniform distribution on $[0,1]$. For each batch $b \\in \\{1,\\dots,B\\}$, define the batch sample mean\n$$ \\bar{f}_{b} = \\frac{1}{n} \\sum_{i=1}^{n} f_{p}(U_{i}), $$\nand the unbiased batch sample variance\n$$ s_{b}^{2} = \\frac{1}{n-1} \\sum_{i=1}^{n} \\left( f_{p}(U_{i}) - \\bar{f}_{b} \\right)^{2}, $$\nprovided that $n \\ge 2$. When $n \\ge 2$, define the nominal $95$-level normal-based half-width\n$$ h_{b} = 1.96 \\cdot \\frac{\\sqrt{s_{b}^{2}}}{\\sqrt{n}}. $$\nFor each batch $b$ with $n \\ge 2$, form the nominal interval\n$$ \\left[ \\bar{f}_{b} - h_{b}, \\, \\bar{f}_{b} + h_{b} \\right]. $$\nLet the coverage indicator for batch $b$ be $C_{b} = 1$ if $I(p)$ lies within the interval above and $C_{b} = 0$ otherwise. If $n < 2$, define $C_{b} = 0$ by convention because the sample variance is not defined. Define the empirical coverage fraction across batches as\n$$ \\widehat{\\mathrm{cov}} = \\frac{1}{B} \\sum_{b=1}^{B} C_{b}. $$\nNote that for $p \\le -\\tfrac{1}{2}$, the variance $\\mathbb{V}[f_{p}(U)]$ is infinite since\n$$ \\mathbb{E}\\left[ f_{p}(U)^{2} \\right] = \\int_{0}^{1} u^{2p} \\, du $$\ndiverges, even though $I(p)$ remains finite for all $p > -1$. This situation violates the finite-variance condition typically required for the Central Limit Theorem (CLT), and thus the nominal normal-based error bound may fail.\n\nYour task is to write a complete program that, for each test case, produces the empirical coverage fraction $\\widehat{\\mathrm{cov}}$ as a decimal. Each test case specifies $(p, n, B, \\text{seed})$, where $\\text{seed}$ initializes a pseudorandom number generator to ensure reproducibility. Angles and physical units do not appear; no unit conversions are required. All answers must be expressed as decimals.\n\nTest Suite:\n- Test $1$: $p = -\\tfrac{1}{2}$, $n = 500$, $B = 200$, $\\text{seed} = 17$.\n- Test $2$: $p = -0.6$, $n = 500$, $B = 200$, $\\text{seed} = 19$.\n- Test $3$: $p = \\tfrac{1}{2}$, $n = 500$, $B = 200$, $\\text{seed} = 23$.\n- Test $4$: $p = \\tfrac{1}{2}$, $n = 5000$, $B = 200$, $\\text{seed} = 29$.\n- Test $5$: $p = -\\tfrac{1}{2}$, $n = 5000$, $B = 200$, $\\text{seed} = 31$.\n- Test $6$: $p = \\tfrac{1}{2}$, $n = 1$, $B = 200$, $\\text{seed} = 37$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the six empirical coverage fractions for Tests $1$ through $6$, in order, as a comma-separated list enclosed in square brackets, for example,\n$$ [x_{1},x_{2},x_{3},x_{4},x_{5},x_{6}]. $$", "solution": "The problem statement has been evaluated and is determined to be valid. It is a well-posed problem in computational statistics, grounded in the principles of Monte Carlo integration and hypothesis testing. The definitions are clear, the parameters are fully specified, and the context (exploring the limits of the Central Limit Theorem) is scientifically sound. There are no contradictions, ambiguities, or factual errors.\n\nThe task is to perform a Monte Carlo simulation to compute the empirical coverage fraction of a nominal $95\\%$ confidence interval for the integral $I(p) = \\int_{0}^{1} u^{p} \\, du$. The simulation will examine how the coverage behavior changes with the parameter $p$, which controls the finiteness of the variance of the integrand $f_{p}(U) = U^{p}$, where $U \\sim \\text{Uniform}[0,1]$.\n\nThe exact value of the integral is known to be $I(p) = \\frac{1}{p+1}$ for $p > -1$.\n\nThe algorithm to compute the empirical coverage fraction, $\\widehat{\\mathrm{cov}}$, for a single test case specified by $(p, n, B, \\text{seed})$ is as follows:\n\n1.  Initialize a pseudorandom number generator with the provided integer `seed` to ensure reproducibility.\n2.  Calculate the true value of the integral, which we denote as $I_{true} = \\frac{1}{p+1}$.\n3.  A specific rule is given for the case when the sample size $n$ is less than $2$. If $n < 2$, the sample variance $s_{b}^{2}$ is undefined. By convention, the coverage indicator $C_{b}$ is set to $0$ for all batches. Consequently, the empirical coverage fraction $\\widehat{\\mathrm{cov}}$ is $0$. The calculation for this case terminates here.\n4.  For the primary case where $n \\ge 2$, the simulation proceeds over $B$ independent batches. We will maintain a count, initialized to zero, of the batches for which the confidence interval covers the true value $I_{true}$.\n5.  A highly efficient method involves generating all random numbers required for all $B$ batches at once. We generate a matrix of $B \\times n$ independent random variates, where each element is drawn from the uniform distribution on $[0,1]$. Let this matrix be denoted by $\\mathbf{U}$, with elements $U_{b,i}$ for batch $b \\in \\{1, \\dots, B\\}$ and sample index $i \\in \\{1, \\dots, n\\}$.\n6.  Apply the integrand function $f_{p}(u) = u^{p}$ element-wise to the matrix $\\mathbf{U}$ to obtain a new matrix $\\mathbf{F}$ with elements $F_{b,i} = (U_{b,i})^{p}$. Each row of $\\mathbf{F}$ corresponds to the sample $\\{f_{p}(U_{i})\\}_{i=1}^{n}$ for a single batch.\n7.  For each batch $b$ (i.e., for each row of $\\mathbf{F}$), we compute the required statistics:\n    a. The batch sample mean: $\\bar{f}_{b} = \\frac{1}{n} \\sum_{i=1}^{n} F_{b,i}$. This can be computed for all batches simultaneously by taking the mean across the columns of $\\mathbf{F}$.\n    b. The unbiased batch sample variance: $s_{b}^{2} = \\frac{1}{n-1} \\sum_{i=1}^{n} (F_{b,i} - \\bar{f}_{b})^{2}$. This can also be computed for all batches by taking the sample variance (with one degree of freedom correction, `ddof=1`) across the columns of $\\mathbf{F}$.\n8.  Using these statistics, we calculate the half-width of the nominal $95\\%$ confidence interval for each batch:\n    $$h_{b} = 1.96 \\cdot \\frac{\\sqrt{s_{b}^{2}}}{\\sqrt{n}}$$\n    Note that the value $1.96$ is the approximant for the $0.975$ quantile of the standard normal distribution, $z_{0.025}$.\n9.  For each batch $b$, we determine if the confidence interval $[\\bar{f}_{b} - h_{b}, \\bar{f}_{b} + h_{b}]$ contains the true value $I_{true}$. This condition is equivalent to testing if the absolute error $|\\bar{f}_{b} - I_{true}|$ is less than or equal to the half-width $h_{b}$.\n10. The number of batches for which this condition holds is counted. Let this count be $N_{covered}$.\n11. The empirical coverage fraction is the ratio of the number of covering intervals to the total number of batches:\n    $$\\widehat{\\mathrm{cov}} = \\frac{N_{covered}}{B}$$\nThis procedure is executed for each of the six test cases specified. The theoretical expectation is that for $p > -1/2$, where the variance of $f_{p}(U)$ is finite, the Central Limit Theorem holds, and $\\widehat{\\mathrm{cov}}$ should be close to the nominal level of $0.95$, especially for large $n$. Conversely, for $p \\le -1/2$, the variance is infinite, the theoretical foundation for the normal-based confidence interval is invalid, and we anticipate that $\\widehat{\\mathrm{cov}}$ will deviate significantly from $0.95$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_coverage(p: float, n: int, B: int, seed: int) -> float:\n    \"\"\"\n    Computes the empirical coverage fraction for a given set of parameters.\n\n    Args:\n        p: The exponent parameter for the integrand u^p.\n        n: The sample size for each batch.\n        B: The number of batches.\n        seed: The seed for the random number generator.\n\n    Returns:\n        The empirical coverage fraction as a float.\n    \"\"\"\n    # Per the problem statement, for n < 2, the sample variance is not defined,\n    # and the coverage indicator C_b is 0 by convention. Thus, the total\n    # empirical coverage is 0.\n    if n < 2:\n        return 0.0\n\n    # Initialize the pseudorandom number generator for reproducibility.\n    rng = np.random.default_rng(seed)\n\n    # Calculate the exact value of the integral I(p) = 1/(p+1).\n    I_true = 1.0 / (p + 1.0)\n\n    # Generate all uniform random samples for all B batches at once.\n    # The shape of the resulting array is (B, n).\n    uniform_samples = rng.uniform(size=(B, n))\n\n    # Evaluate the integrand f_p(u) = u^p on the samples.\n    f_p_samples = uniform_samples**p\n\n    # Calculate batch means along axis 1 (across samples in a batch).\n    # The result is an array of shape (B,).\n    batch_means = np.mean(f_p_samples, axis=1)\n\n    # Calculate unbiased batch sample variances along axis 1.\n    # ddof=1 ensures the denominator is (n-1).\n    # The result is an array of shape (B,).\n    batch_variances = np.var(f_p_samples, axis=1, ddof=1)\n\n    # Calculate the half-width of the nominal 95% confidence interval for each batch.\n    # h_b = 1.96 * sqrt(s_b^2 / n)\n    # A small epsilon is not strictly necessary with standard floating point\n    # arithmetic as var >= 0, but good practice to consider. Here, it is omitted.\n    half_widths = 1.96 * np.sqrt(batch_variances / n)\n\n    # Determine for each batch if the true value is within the confidence interval.\n    # This is true if |sample_mean - true_value| <= half_width.\n    # This operation returns a boolean array of shape (B,).\n    is_covered = np.abs(batch_means - I_true) <= half_widths\n\n    # The empirical coverage is the mean of the boolean indicators (True=1, False=0).\n    coverage_fraction = np.mean(is_covered)\n\n    return coverage_fraction\n\ndef solve():\n    \"\"\"\n    Runs the simulation for all test cases and prints the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Format: (p, n, B, seed)\n    test_cases = [\n        (-0.5, 500, 200, 17),    # Test 1\n        (-0.6, 500, 200, 19),    # Test 2\n        (0.5, 500, 200, 23),     # Test 3\n        (0.5, 5000, 200, 29),    # Test 4\n        (-0.5, 5000, 200, 31),   # Test 5\n        (0.5, 1, 200, 37),      # Test 6\n    ]\n\n    results = []\n    for p, n, B, seed in test_cases:\n        # Calculate the empirical coverage for the current test case.\n        result = compute_coverage(p, n, B, seed)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "2411534"}]}