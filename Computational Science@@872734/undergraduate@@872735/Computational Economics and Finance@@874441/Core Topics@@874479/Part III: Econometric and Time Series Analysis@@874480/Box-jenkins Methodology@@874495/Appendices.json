{"hands_on_practices": [{"introduction": "The Box-Jenkins methodology provides a powerful framework for time series analysis, and the first critical step is ensuring the series is stationary. This hands-on coding exercise challenges you to determine the order of integration, denoted $d$ in an $\\text{ARIMA}(p,d,q)$ model, for several simulated economic time series. By implementing a sequence of unit root tests, you will gain practical experience in applying the identification stage of the methodology to prepare data for modeling [@problem_id:2378207].", "problem": "You are given the task of determining the smallest order of differencing that renders a given time series covariance-stationary, following the Box–Jenkins methodology in the context of Autoregressive Integrated Moving Average (ARIMA) modeling. Let $\\{y_t\\}_{t=0}^{T}$ be a real-valued discrete-time stochastic process. Define the backward difference operator $\\Delta$ by $\\Delta y_t = y_t - y_{t-1}$ and its $d$-fold iteration $\\Delta^d y_t$ by applying $\\Delta$ a total of $d$ times. A series is said to be integrated of order $d$ (denoted $I(d)$) if $\\Delta^d y_t$ is covariance-stationary and $\\Delta^{d-1} y_t$ is not, for the smallest nonnegative integer $d$.\n\nYour objective is to write a program that, for each of the following data-generating processes (DGPs), simulates a sample path and determines the smallest integer $d \\in \\{0,1,2\\}$ such that $\\Delta^d y_t$ is covariance-stationary. For economic interpretation, consider the first DGP as a stylized representation of the velocity of money (broad liquidity version, commonly labeled M2 velocity).\n\nAll processes use independent and identically distributed Gaussian innovations $\\{\\varepsilon_t\\}$ with $\\varepsilon_t \\sim \\mathcal{N}(0,\\sigma^2)$, and initial conditions as specified. Unless otherwise stated, take $y_0 = 0$. Use the stated seeds to ensure reproducibility.\n\nTest suite of DGPs to simulate and analyze:\n1. Random walk with drift (stylized M2V):\n   - Dynamics: $y_t = y_{t-1} + c + \\varepsilon_t$ for $t \\in \\{1,\\dots,T\\}$.\n   - Parameters: $T = 500$, $c = 0.002$, $\\sigma = 0.01$, seed $= 1729$.\n2. Stationary autoregressive process of order one with high persistence:\n   - Dynamics: $y_t = \\phi\\, y_{t-1} + \\varepsilon_t$ for $t \\in \\{1,\\dots,T\\}$.\n   - Parameters: $T = 500$, $\\phi = 0.9$, $\\sigma = 0.05$, seed $= 2024$.\n3. Integrated of order two:\n   - Dynamics: $\\Delta^2 y_t = \\varepsilon_t$ for $t \\in \\{1,\\dots,T\\}$ with $y_{-1} = 0$ and $y_0 = 0$.\n   - Equivalent representation: $y_t = 2 y_{t-1} - y_{t-2} + \\varepsilon_t$ for $t \\ge 1$.\n   - Parameters: $T = 500$, $\\sigma = 0.05$, seed $= 7$.\n4. Stationary autoregressive process of order one with moderate persistence:\n   - Dynamics: $y_t = \\phi\\, y_{t-1} + \\varepsilon_t$ for $t \\in \\{1,\\dots,T\\}$.\n   - Parameters: $T = 500$, $\\phi = 0.5$, $\\sigma = 0.05$, seed $= 11$.\n\nYour program must:\n- Simulate one sample path for each DGP using the stated parameters.\n- For each sample path, determine the smallest $d \\in \\{0,1,2\\}$ such that $\\Delta^d y_t$ is covariance-stationary, using a statistically sound and reproducible decision rule grounded in the definition of covariance-stationarity.\n- Produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test suite above. For example, an output like $[1,0,2,0]$ means the first series is assessed as $I(1)$, the second as $I(0)$, the third as $I(2)$, and the fourth as $I(0)$.\n\nThere are no physical units or angles involved. All outputs must be integers. Your program must not require any input from the user and must not read from or write to any files or networks. The final output format must be exactly one line:\n- A list in the form $[\\text{d}_1,\\text{d}_2,\\text{d}_3,\\text{d}_4]$ where each $\\text{d}_i \\in \\{0,1,2\\}$ corresponds to the estimated order of integration for DGP $i$.", "solution": "The problem statement requires the determination of the order of integration, denoted $d$, for several specified discrete-time stochastic processes. The order of integration $d$ is defined as the minimum number of differences required to render a time series covariance-stationary. This is a fundamental task in time series econometrics, particularly within the Box-Jenkins framework for Autoregressive Integrated Moving Average (ARIMA) modeling.\n\nBefore proceeding to a solution, we must ascertain the validity of the problem.\n\n**Problem Validation**\n\n**Step 1: Extract Givens**\n- Task: For four given Data-Generating Processes (DGPs), simulate a sample path and determine the smallest integer $d \\in \\{0,1,2\\}$ such that the $d$-th difference of the series, $\\Delta^d y_t$, is covariance-stationary.\n- Definition: A series is integrated of order $d$, denoted $I(d)$, if $\\Delta^d y_t$ is stationary but $\\Delta^{d-1} y_t$ is not.\n- DGPs:\n    1. Random walk with drift: $y_t = y_{t-1} + c + \\varepsilon_t$, with $T=500$, $c=0.002$, $\\sigma=0.01$, $y_0=0$, seed=$1729$.\n    2. Stationary autoregressive process of order one (AR(1)): $y_t = \\phi y_{t-1} + \\varepsilon_t$, with $T=500$, $\\phi=0.9$, $\\sigma=0.05$, $y_0=0$, seed=$2024$.\n    3. Integrated of order two: $\\Delta^2 y_t = \\varepsilon_t$ (or $y_t = 2y_{t-1} - y_{t-2} + \\varepsilon_t$), with $T=500$, $\\sigma=0.05$, $y_{-1}=0, y_0=0$, seed=$7$.\n    4. Stationary AR(1): $y_t = \\phi y_{t-1} + \\varepsilon_t$, with $T=500$, $\\phi=0.5$, $\\sigma=0.05$, $y_0=0$, seed=$11$.\n- Innovations: $\\varepsilon_t \\sim \\mathcal{N}(0, \\sigma^2)$ are independent and identically distributed (i.i.d.) Gaussian.\n- Methodological Requirement: Use a \"statistically sound and reproducible decision rule\".\n- Output Format: A single line with a list of four integers, `[d1,d2,d3,d4]`.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is based on standard, canonical models from time series analysis (random walk, AR(1) processes) and employs well-defined concepts like covariance-stationarity, differencing, and order of integration. These are central to the specified topic of Box-Jenkins methodology. The premises are factually and mathematically sound.\n- **Well-Posed:** All DGPs are fully specified with parameters, initial conditions, and random seeds, ensuring reproducibility. The objective is clear. The phrase \"statistically sound and reproducible decision rule\" obliges the solver to select an appropriate statistical test, which is a standard expectation in this field. The established procedure is a sequence of unit root tests, such as the Augmented Dickey-Fuller (ADF) test. The problem is therefore well-posed, as a standard methodology exists to find a unique, verifiable solution.\n- **Objective:** The problem is stated in precise mathematical language, free of subjectivity or ambiguity. The DGPs are objective models.\n\n**Step 3: Verdict and Action**\nThe problem is scientifically sound, well-posed, and objective. It contains all necessary information for a unique solution. Therefore, the problem is deemed **valid**. We will now proceed with the solution.\n\n**Principle-Based Solution**\n\nThe core of the problem is to distinguish between stationary and non-stationary time series. A discrete-time stochastic process $\\{y_t\\}$ is (weakly) covariance-stationary if its statistical properties are invariant with respect to time translation. Specifically, for all integers $t$ and $k$:\n1. The mean is constant: $E[y_t] = \\mu$.\n2. The variance is finite and constant: $Var(y_t) = \\sigma_y^2 < \\infty$.\n3. The autocovariance depends only on the lag $k$: $Cov(y_t, y_{t-k}) = \\gamma_k$.\n\nProcesses that violate these properties are non-stationary. A common type of non-stationarity in economic and financial data is the presence of a unit root, where the process has infinite memory and its variance increases over time. A process with $d$ unit roots is said to be integrated of order $d$, denoted $I(d)$. Such a process can be made stationary by applying the backward difference operator $\\Delta$ a total of $d$ times, where $\\Delta y_t = y_t - y_{t-1}$.\n\nTo determine the order of integration $d$, we employ a sequential hypothesis testing procedure using the Augmented Dickey-Fuller (ADF) test. The ADF test is designed to detect the presence of a unit root. It is based on an OLS regression of the form:\n$$ \\Delta y_t = \\alpha + \\beta t + \\gamma y_{t-1} + \\sum_{i=1}^{p} \\delta_i \\Delta y_{t-i} + u_t $$\nThe null hypothesis is that the series has a unit root ($H_0: \\gamma = 0$), against the alternative that it is stationary or trend-stationary ($H_1: \\gamma < 0$). The test statistic is the t-statistic of the estimated coefficient $\\hat{\\gamma}$, which follows a non-standard Dickey-Fuller distribution under the null. We compare this statistic to pre-tabulated critical values. The terms $\\alpha$ (constant), $\\beta t$ (time trend), and the lagged differences $\\Delta y_{t-i}$ are included to respectively account for drift, deterministic trends, and higher-order autoregressive dynamics in the process, ensuring the error term $u_t$ is white noise.\n\nThe choice of including the constant and trend terms defines three main regression models:\n- No constant, no trend ('nc'): $\\Delta y_t = \\gamma y_{t-1} + \\dots$\n- Constant, no trend ('c'): $\\Delta y_t = \\alpha + \\gamma y_{t-1} + \\dots$\n- Constant and trend ('ct'): $\\Delta y_t = \\alpha + \\beta t + \\gamma y_{t-1} + \\dots$\n\nOur algorithmic approach is as follows:\n1.  **Simulate DGPs**: For each of the four test cases, we generate a sample path of length $T+1 = 501$ points ($t=0, \\dots, 500$) using the specified parameters, dynamics, initial conditions, and random seed.\n2.  **Implement ADF Test**: Since the required libraries `numpy` and `scipy` do not include a pre-built ADF test, we implement it from first principles. This involves constructing the dependent variable and the design matrix for the OLS regression, solving for the coefficients using `numpy.linalg.lstsq`, and calculating the t-statistic for the coefficient $\\gamma$. For simplicity and given the nature of the DGPs, we will use a fixed lag order of $p=1$.\n3.  **Sequential Testing Procedure**: For each simulated series, we determine the smallest $d \\in \\{0, 1, 2\\}$ via the following sequence, using a significance level of $5\\%$:\n    -   **Test for $d=0$**: We test the original series $y_t$ for a unit root. To be robust to a potential deterministic trend, we use the 'ct' model. The $5\\%$ critical value is approximately $-3.41$. If the ADF statistic is less than this value, we reject the null hypothesis of a unit root and conclude $d=0$.\n    -   **Test for $d=1$**: If the previous test fails to reject $H_0$, we test the first-differenced series $\\Delta y_t$. This series should no longer have a deterministic trend, but may have a non-zero mean (drift). We use the 'c' model. The $5\\%$ critical value is approximately $-2.86$. If the ADF statistic is less than this value, we conclude $d=1$.\n    -   **Test for $d=2$**: If both previous tests fail to reject, we test the second-differenced series $\\Delta^2 y_t$. This series is expected to be a zero-mean stationary process. We use the 'nc' model. The $5\\%$ critical value is approximately $-1.94$. If the ADF statistic is less than this value, we conclude $d=2$. If this test also fails to reject, we assign $d=2$ as the highest order specified in the problem statement.\n\nThis rigorous, principle-based procedure provides a \"statistically sound and reproducible decision rule\" as demanded by the problem statement.", "answer": "```python\nimport numpy as np\n# from scipy import linalg # numpy.linalg is sufficient and preferred.\n\ndef solve():\n    \"\"\"\n    Simulates four time series processes and determines their order of integration d.\n    \"\"\"\n\n    def adf_test(series: np.ndarray, p: int, regression_type: str) -> float:\n        \"\"\"\n        Performs an Augmented Dickey-Fuller test.\n\n        Args:\n            series: The time series to test.\n            p: The number of lagged differences to include.\n            regression_type: 'nc' (no constant), 'c' (constant), 'ct' (constant, trend).\n\n        Returns:\n            The ADF t-statistic for the lagged level term.\n        \"\"\"\n        n_obs = len(series)\n        dx = np.diff(series)\n\n        # Dependent variable (y_t - y_{t-1})\n        y = dx[p:]\n        n_reg_obs = len(y)\n\n        # Build design matrix X for OLS: y = X*beta + error\n        # Column 0: Lagged level (y_{t-1})\n        X = [series[p:-1]]\n\n        # Columns 1 to p: Lagged differences\n        if p > 0:\n            for i in range(p):\n                # Lag i is dx_{t-i}, index is p-i-1 relative to start of dx\n                X.append(dx[p - i - 1 : n_reg_obs + p - i - 1])\n\n        # Add constant and/or trend\n        if regression_type == 'c':\n            X.append(np.ones(n_reg_obs))\n        elif regression_type == 'ct':\n            X.append(np.ones(n_reg_obs))\n            # Trend starts from time p+1\n            trend = np.arange(p + 1, n_obs)\n            X.append(trend)\n\n        X = np.stack(X, axis=1)\n\n        # Perform OLS using np.linalg.lstsq\n        try:\n            coeffs, residuals_sum_sq, rank, s = np.linalg.lstsq(X, y, rcond=None)\n        except np.linalg.LinAlgError:\n            return 0.0 # Cannot solve, return value that won't reject H0\n        \n        n_regressors = X.shape[1]\n        if rank < n_regressors:\n            return 0.0 # Singular matrix, cannot compute reliably\n\n        # Extract coefficient for the lagged level term\n        gamma_hat = coeffs[0]\n\n        # Calculate standard error of gamma_hat\n        # residuals_sum_sq from lstsq is a single-element array\n        res_var = residuals_sum_sq[0] / (n_reg_obs - n_regressors)\n        xtx_inv = np.linalg.inv(X.T @ X)\n        se_gamma_hat = np.sqrt(res_var * xtx_inv[0, 0])\n\n        if se_gamma_hat == 0:\n            return 0.0\n\n        adf_stat = gamma_hat / se_gamma_hat\n        return adf_stat\n\n    def determine_d(series: np.ndarray) -> int:\n        \"\"\"\n        Determines the order of integration d for a series by sequential ADF tests.\n        \"\"\"\n        # Critical values for a 5% significance level, T~500\n        cv_ct = -3.41  # Model with constant and trend\n        cv_c = -2.86   # Model with constant\n        cv_nc = -1.94  # Model with no constant\n\n        p = 1 # Number of lags for ADF test\n\n        # Test for d=0 (I(0))\n        adf_stat_0 = adf_test(series, p, 'ct')\n        if adf_stat_0 < cv_ct:\n            return 0\n\n        # Test for d=1 (I(1))\n        diff1 = np.diff(series)\n        adf_stat_1 = adf_test(diff1, p, 'c')\n        if adf_stat_1 < cv_c:\n            return 1\n            \n        # Test for d=2 (I(2))\n        diff2 = np.diff(series, n=2)\n        adf_stat_2 = adf_test(diff2, p, 'nc')\n        if adf_stat_2 < cv_nc:\n            return 2\n        \n        # If all tests fail to reject, return highest order as per problem spec\n        return 2\n\n    # --- Test Suite---\n    \n    T = 500\n    test_cases = [\n        {'type': 'rw_drift', 'T': T, 'c': 0.002, 'sigma': 0.01, 'seed': 1729},\n        {'type': 'ar1', 'T': T, 'phi': 0.9, 'sigma': 0.05, 'seed': 2024},\n        {'type': 'i2', 'T': T, 'sigma': 0.05, 'seed': 7},\n        {'type': 'ar1', 'T': T, 'phi': 0.5, 'sigma': 0.05, 'seed': 11},\n    ]\n\n    results = []\n\n    for params in test_cases:\n        np.random.seed(params['seed'])\n        eps = np.random.normal(0, params['sigma'], params['T'] + 1)\n        y = np.zeros(params['T'] + 1)\n\n        if params['type'] == 'rw_drift':\n            # y_t = y_{t-1} + c + eps_t, with y_0 = 0\n            for t in range(1, params['T'] + 1):\n                y[t] = y[t-1] + params['c'] + eps[t]\n        \n        elif params['type'] == 'ar1':\n            # y_t = phi * y_{t-1} + eps_t, with y_0 = 0\n            for t in range(1, params['T'] + 1):\n                y[t] = params['phi'] * y[t-1] + eps[t]\n\n        elif params['type'] == 'i2':\n            # y_t = 2*y_{t-1} - y_{t-2} + eps_t, with y_{-1}=0, y_0=0\n            # Note: y array starts with y_0. y_{-1} is implicitly 0.\n            y[1] = 2 * y[0] - 0 + eps[1] # y_1 = eps_1\n            for t in range(2, params['T'] + 1):\n                y[t] = 2 * y[t-1] - y[t-2] + eps[t]\n        \n        # Determine the order of integration for the simulated series\n        d = determine_d(y)\n        results.append(d)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2378207"}, {"introduction": "Once a time series is made stationary, the next step is to identify the orders of the autoregressive ($p$) and moving average ($q$) components. This practice problem delves into the theoretical heart of identifying an autoregressive process. By algebraically solving the Yule-Walker equations for an $\\text{AR}(2)$ model, you will establish the direct mathematical link between a series' autocorrelation structure and its underlying parameters, providing a deeper understanding of how tools like the PACF work [@problem_id:2378239].", "problem": "In the Box–Jenkins methodology for time series modeling in computational economics and finance, consider a weakly stationary autoregressive process of order two, denoted $\\text{AR}(2)$, used to model a macro-financial series such as deviations of log consumption growth from its mean. Let the process be characterized by parameters $\\phi_1$ and $\\phi_2$, and let $\\rho_k$ denote the autocorrelation at lag $k$. Under weak stationarity, the Yule–Walker equations relate the autocorrelations to the autoregressive parameters. Assume the following relations hold between the first two autocorrelations and the parameters: $\\rho_1 = \\phi_1 + \\phi_2 \\rho_1$ and $\\rho_2 = \\phi_1 \\rho_1 + \\phi_2$. Starting from the AR(2) structure and the definition of the autocovariance and autocorrelation functions, one can establish these relations as equilibrium conditions linking second-order moments to the dynamics. Using only these two relations, solve the system to express $\\phi_1$ and $\\phi_2$ as explicit functions of $\\rho_1$ and $\\rho_2$. Your final answer must be a closed-form analytic expression. If any regularity condition is needed for uniqueness, state it in your reasoning, but do not include it in your final answer. No numerical evaluation is required.", "solution": "The problem posed is to determine the autoregressive parameters, $\\phi_1$ and $\\phi_2$, of a weakly stationary autoregressive process of order two, denoted $\\text{AR}(2)$, as functions of the first two theoretical autocorrelations, $\\rho_1$ and $\\rho_2$. The provided relations are the Yule-Walker equations for lags $k=1$ and $k=2$. These form a system of two linear equations with two unknowns, $\\phi_1$ and $\\phi_2$.\n\nThe given system of equations is:\n$$\n\\rho_1 = \\phi_1 + \\phi_2 \\rho_1 \\quad (1)\n$$\n$$\n\\rho_2 = \\phi_1 \\rho_1 + \\phi_2 \\quad (2)\n$$\n\nTo facilitate solving for $\\phi_1$ and $\\phi_2$, we can rearrange the system into a standard matrix form, $A\\mathbf{x} = \\mathbf{b}$, where $\\mathbf{x} = \\begin{pmatrix} \\phi_1 \\\\ \\phi_2 \\end{pmatrix}$.\nEquation ($1$) can be written as:\n$$\n1 \\cdot \\phi_1 + \\rho_1 \\cdot \\phi_2 = \\rho_1\n$$\nEquation ($2$) can be written as:\n$$\n\\rho_1 \\cdot \\phi_1 + 1 \\cdot \\phi_2 = \\rho_2\n$$\nThe system in matrix form is:\n$$\n\\begin{pmatrix} 1 & \\rho_1 \\\\ \\rho_1 & 1 \\end{pmatrix} \\begin{pmatrix} \\phi_1 \\\\ \\phi_2 \\end{pmatrix} = \\begin{pmatrix} \\rho_1 \\\\ \\rho_2 \\end{pmatrix}\n$$\nThis system can be solved using various methods for linear systems, such as substitution, elimination, or matrix inversion (using Cramer's rule, for instance). We will proceed with algebraic substitution.\n\nFrom equation ($2$), we can express $\\phi_2$ in terms of $\\phi_1$ and the known autocorrelations:\n$$\n\\phi_2 = \\rho_2 - \\phi_1 \\rho_1\n$$\nNow, substitute this expression for $\\phi_2$ into equation ($1$):\n$$\n\\rho_1 = \\phi_1 + (\\rho_2 - \\phi_1 \\rho_1) \\rho_1\n$$\nWe distribute the $\\rho_1$ term on the right-hand side:\n$$\n\\rho_1 = \\phi_1 + \\rho_1 \\rho_2 - \\phi_1 \\rho_1^2\n$$\nNext, we gather all terms containing $\\phi_1$ on one side of the equation and the remaining terms on the other side to solve for $\\phi_1$:\n$$\n\\rho_1 - \\rho_1 \\rho_2 = \\phi_1 - \\phi_1 \\rho_1^2\n$$\nFactor out $\\phi_1$ on the right-hand side and $\\rho_1$ on the left-hand side:\n$$\n\\rho_1 (1 - \\rho_2) = \\phi_1 (1 - \\rho_1^2)\n$$\nTo isolate $\\phi_1$, we must divide by $(1 - \\rho_1^2)$. This step requires a regularity condition for the uniqueness of the solution, namely that the determinant of the coefficient matrix is non-zero. The determinant is $1 \\cdot 1 - \\rho_1 \\cdot \\rho_1 = 1 - \\rho_1^2$. Thus, the condition is $1 - \\rho_1^2 \\neq 0$, which is equivalent to $|\\rho_1| \\neq 1$. For any non-deterministic weakly stationary process, the autocorrelation $|\\rho_k|$ must be strictly less than $1$ for any non-zero lag $k$. Therefore, for a stationary AR($2$) process of interest, we have $|\\rho_1| < 1$, which guarantees the condition is met.\n\nProceeding with the division, we obtain the expression for $\\phi_1$:\n$$\n\\phi_1 = \\frac{\\rho_1 (1 - \\rho_2)}{1 - \\rho_1^2}\n$$\nNow we substitute this expression for $\\phi_1$ back into our equation for $\\phi_2$:\n$$\n\\phi_2 = \\rho_2 - \\rho_1 \\phi_1 = \\rho_2 - \\rho_1 \\left( \\frac{\\rho_1 (1 - \\rho_2)}{1 - \\rho_1^2} \\right)\n$$\nSimplify the expression:\n$$\n\\phi_2 = \\rho_2 - \\frac{\\rho_1^2 (1 - \\rho_2)}{1 - \\rho_1^2}\n$$\nTo combine the terms, we use a common denominator of $(1 - \\rho_1^2)$:\n$$\n\\phi_2 = \\frac{\\rho_2 (1 - \\rho_1^2) - \\rho_1^2 (1 - \\rho_2)}{1 - \\rho_1^2}\n$$\nExpand the numerator:\n$$\n\\phi_2 = \\frac{\\rho_2 - \\rho_2 \\rho_1^2 - \\rho_1^2 + \\rho_1^2 \\rho_2}{1 - \\rho_1^2}\n$$\nThe terms $-\\rho_2 \\rho_1^2$ and $+\\rho_1^2 \\rho_2$ cancel each other out, leading to the final expression for $\\phi_2$:\n$$\n\\phi_2 = \\frac{\\rho_2 - \\rho_1^2}{1 - \\rho_1^2}\n$$\nThus, we have successfully expressed the autoregressive parameters $\\phi_1$ and $\\phi_2$ as explicit functions of the autocorrelations $\\rho_1$ and $\\rho_2$.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{\\rho_1(1 - \\rho_2)}{1 - \\rho_1^2} & \\frac{\\rho_2 - \\rho_1^2}{1 - \\rho_1^2} \\end{pmatrix}}\n$$", "id": "2378239"}, {"introduction": "After identifying and estimating a model, its primary purpose is often forecasting. This exercise puts theory into practice by guiding you through the mechanics of forecasting with a simple but common $\\text{AR}(1)$ model. You will not only compute multi-step-ahead point forecasts but also construct the corresponding prediction intervals, providing a crucial, hands-on lesson in quantifying the uncertainty that grows as we predict further into the future [@problem_id:2378255].", "problem": "Consider the following Autoregressive of order 1 ($\\text{AR}(1)$) model in the Box–Jenkins framework for a univariate, mean-zero, covariance-stationary time series $\\{X_t\\}$:\n$$\nX_t = 0.8\\,X_{t-1} + \\epsilon_t,\n$$\nwhere $\\{\\epsilon_t\\}$ are independent and identically distributed (i.i.d.) Gaussian innovations with $\\epsilon_t \\sim \\mathcal{N}(0,\\sigma_{\\epsilon}^{2})$ and $\\sigma_{\\epsilon}^{2} = 4$. You observe $X_T = 100$. Compute the first three step-ahead point forecasts $X_{T+1|T}$, $X_{T+2|T}$, $X_{T+3|T}$ and, under the Gaussian assumption, the corresponding two-sided $95\\%$ prediction intervals for each forecast. Use the standard normal quantile $z_{0.975} = 1.96$.\n\nAnswer format and rounding requirement:\n- Present your final answer as a single $1 \\times 9$ row matrix in the following order:\n$$\n\\big(X_{T+1|T},\\ \\text{Lower}_{T+1},\\ \\text{Upper}_{T+1},\\ X_{T+2|T},\\ \\text{Lower}_{T+2},\\ \\text{Upper}_{T+2},\\ X_{T+3|T},\\ \\text{Lower}_{T+3},\\ \\text{Upper}_{T+3}\\big).\n$$\n- Round every entry to four significant figures.\n- Do not include any units in your final answer.", "solution": "The problem presented is valid. It is a well-posed problem in time series analysis, grounded in the established Box-Jenkins methodology for autoregressive models. All necessary parameters and conditions are provided, and there are no scientific or logical contradictions. We shall proceed with the derivation of the solution.\n\nThe model is a first-order autoregressive process, $\\text{AR}(1)$, given by:\n$$X_t = \\phi X_{t-1} + \\epsilon_t$$\nThe parameters are specified as $\\phi = 0.8$ and the innovations $\\{\\epsilon_t\\}$ are independent and identically distributed Gaussian random variables with mean $0$ and variance $\\sigma_{\\epsilon}^2 = 4$. The final observed value is $X_T = 100$.\n\nFirst, we compute the point forecasts. The $h$-step-ahead point forecast, denoted $X_{T+h|T}$, is the conditional expectation of $X_{T+h}$ given the information available up to time $T$, which we denote by $\\mathcal{F}_T$.\n$$X_{T+h|T} = E[X_{T+h} | \\mathcal{F}_T]$$\nFor an $\\text{AR}(1)$ model, the recursive nature of the forecast is $X_{T+h|T} = \\phi X_{T+h-1|T}$. By repeated substitution, this yields the general formula for the point forecast:\n$$X_{T+h|T} = \\phi^h X_T$$\n\nWe apply this formula for $h=1, 2, 3$.\nFor $h=1$:\n$$X_{T+1|T} = \\phi X_T = 0.8 \\times 100 = 80$$\nFor $h=2$:\n$$X_{T+2|T} = \\phi^2 X_T = (0.8)^2 \\times 100 = 0.64 \\times 100 = 64$$\nFor $h=3$:\n$$X_{T+3|T} = \\phi^3 X_T = (0.8)^3 \\times 100 = 0.512 \\times 100 = 51.2$$\n\nNext, we construct the $95\\%$ prediction intervals. A two-sided $(1-\\alpha) \\times 100\\%$ prediction interval for $X_{T+h}$ is given by:\n$$X_{T+h|T} \\pm z_{1-\\alpha/2} \\sqrt{\\text{Var}(e_T(h))}$$\nwhere $e_T(h) = X_{T+h} - X_{T+h|T}$ is the $h$-step forecast error, and $z_{1-\\alpha/2}$ is the corresponding quantile of the standard normal distribution. For a $95\\%$ interval, $\\alpha=0.05$, and we are given $z_{0.975} = 1.96$.\n\nThe variance of the forecast error, $\\text{Var}(e_T(h))$, is derived from the moving-average representation of the forecast error. For an AR($1$) process, the $h$-step forecast error is:\n$$e_T(h) = \\epsilon_{T+h} + \\phi \\epsilon_{T+h-1} + \\dots + \\phi^{h-1} \\epsilon_{T+1} = \\sum_{j=0}^{h-1} \\phi^j \\epsilon_{T+h-j}$$\nSince the innovations $\\{\\epsilon_t\\}$ are independent with variance $\\sigma_{\\epsilon}^2$, the variance of the error is:\n$$\\text{Var}(e_T(h)) = \\text{Var}\\left(\\sum_{j=0}^{h-1} \\phi^j \\epsilon_{T+h-j}\\right) = \\sum_{j=0}^{h-1} (\\phi^j)^2 \\text{Var}(\\epsilon_{T+h-j}) = \\sigma_{\\epsilon}^2 \\sum_{j=0}^{h-1} \\phi^{2j}$$\nThis is the sum of a finite geometric series:\n$$\\text{Var}(e_T(h)) = \\sigma_{\\epsilon}^2 \\frac{1 - \\phi^{2h}}{1 - \\phi^2}$$\nWe have $\\sigma_{\\epsilon}^2 = 4$ and $\\phi=0.8$.\n\nFor $h=1$:\nThe forecast error variance is $\\text{Var}(e_T(1)) = \\sigma_{\\epsilon}^2 = 4$.\nThe standard deviation of the error is $\\sqrt{\\text{Var}(e_T(1))} = \\sqrt{4} = 2$.\nThe half-width of the prediction interval is $z_{0.975} \\times 2 = 1.96 \\times 2 = 3.92$.\nThe interval is $80 \\pm 3.92$, which gives:\nLower Bound: $80 - 3.92 = 76.08$\nUpper Bound: $80 + 3.92 = 83.92$\n\nFor $h=2$:\nThe forecast error variance is $\\text{Var}(e_T(2)) = \\sigma_{\\epsilon}^2 (1 + \\phi^2) = 4(1 + (0.8)^2) = 4(1 + 0.64) = 4(1.64) = 6.56$.\nThe standard deviation of the error is $\\sqrt{6.56}$.\nThe half-width of the prediction interval is $z_{0.975} \\times \\sqrt{6.56} = 1.96 \\times \\sqrt{6.56} \\approx 5.02005$.\nThe interval is $64 \\pm 5.02005$, which gives:\nLower Bound: $64 - 5.02005 = 58.97995$\nUpper Bound: $64 + 5.02005 = 69.02005$\n\nFor $h=3$:\nThe forecast error variance is $\\text{Var}(e_T(3)) = \\sigma_{\\epsilon}^2 (1 + \\phi^2 + \\phi^4) = 4(1 + (0.8)^2 + (0.8)^4) = 4(1 + 0.64 + 0.4096) = 4(2.0496) = 8.1984$.\nThe standard deviation of the error is $\\sqrt{8.1984}$.\nThe half-width of the prediction interval is $z_{0.975} \\times \\sqrt{8.1984} = 1.96 \\times \\sqrt{8.1984} \\approx 5.61195$.\nThe interval is $51.2 \\pm 5.61195$, which gives:\nLower Bound: $51.2 - 5.61195 = 45.58805$\nUpper Bound: $51.2 + 5.61195 = 56.81195$\n\nFinally, we round all results to four significant figures as required.\n$X_{T+1|T} = 80.00$\nLower$_{T+1} = 76.08$\nUpper$_{T+1} = 83.92$\n$X_{T+2|T} = 64.00$\nLower$_{T+2} = 58.98$\nUpper$_{T+2} = 69.02$\n$X_{T+3|T} = 51.20$\nLower$_{T+3} = 45.59$\nUpper$_{T+3} = 56.81$\n\nThese values are compiled into the final answer matrix.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n80.00 & 76.08 & 83.92 & 64.00 & 58.98 & 69.02 & 51.20 & 45.59 & 56.81\n\\end{pmatrix}\n}\n$$", "id": "2378255"}]}