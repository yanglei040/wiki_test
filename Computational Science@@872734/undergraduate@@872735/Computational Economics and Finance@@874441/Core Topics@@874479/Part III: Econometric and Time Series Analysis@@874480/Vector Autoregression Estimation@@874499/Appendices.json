{"hands_on_practices": [{"introduction": "This first exercise guides you through a complete computational experiment to compare the forecasting power of different time series models. You will build and test Vector Autoregression (VAR) models of different lag orders against the simple but formidable random walk benchmark. This practice is essential for developing the core skills of model estimation, out-of-sample forecasting, and performance evaluation, which are central to applied econometric work [@problem_id:2447495].", "problem": "You are given the task of building a fully reproducible computational experiment to compare the out-of-sample one-step-ahead forecast performance of three models for multivariate exchange rates: a Vector Autoregression (VAR) of order one, a Vector Autoregression (VAR) of order four, and a random walk in levels. The experiment must be implemented as a complete program.\n\nStart from the following foundational definitions. A $k$-dimensional Vector Autoregression (VAR) of order $p$ is defined by\n$$\n\\mathbf{y}_t = \\mathbf{c} + \\sum_{i=1}^{p} \\mathbf{A}_i \\mathbf{y}_{t-i} + \\mathbf{u}_t,\n$$\nwhere $\\mathbf{y}_t \\in \\mathbb{R}^k$ is the vector of variables, $\\mathbf{c} \\in \\mathbb{R}^k$ is an intercept, $\\mathbf{A}_i \\in \\mathbb{R}^{k \\times k}$ are autoregressive coefficient matrices, and $\\mathbf{u}_t \\sim \\mathcal{N}(\\mathbf{0}, \\boldsymbol{\\Sigma})$ is a zero-mean Gaussian innovation with positive definite covariance matrix $\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{k \\times k}$. The Ordinary Least Squares (OLS) estimator of the parameters in a VAR is obtained by minimizing the sum of squared residuals across all equations, which is equivalent to solving the multivariate least squares problem implied by stacking the regressors and responses.\n\nImplement and evaluate the models as follows.\n\n1. Estimation principle to be implemented. For each model with order $p \\in \\{1,4\\}$, estimate the parameters by Ordinary Least Squares (OLS) using only the training sample. Construct the regressor matrix $\\mathbf{X}$ by stacking an intercept and lagged values $\\{\\mathbf{y}_{t-1}, \\ldots, \\mathbf{y}_{t-p}\\}$ and the response matrix $\\mathbf{Y}$ by stacking $\\mathbf{y}_t$ for $t$ from the end of the training window backward by $p$ lags. Solve the least squares problem to obtain coefficient estimates. Use these fixed estimates to generate one-step-ahead forecasts over the test sample, always conditioning on actual lagged values from the realized series (no re-estimation or updating of parameters during testing).\n\n2. Random walk benchmark. The random walk forecast in levels is defined by $\\widehat{\\mathbf{y}}_{t+1|t} = \\mathbf{y}_t$ for each forecast origin in the test sample.\n\n3. Forecast loss metric. For each model, compute the Root Mean Squared Forecast Error (RMSFE) aggregated across all variables and all out-of-sample forecast origins in the test window:\n$$\n\\mathrm{RMSFE} = \\sqrt{\\frac{1}{H k} \\sum_{h=1}^{H} \\left\\|\\mathbf{y}_{T_{\\text{train}}+h} - \\widehat{\\mathbf{y}}_{T_{\\text{train}}+h|T_{\\text{train}}+h-1}\\right\\|_2^2},\n$$\nwhere $k$ is the dimension of $\\mathbf{y}_t$, $H$ is the number of out-of-sample one-step-ahead forecasts, and $\\|\\cdot\\|_2$ is the Euclidean norm.\n\n4. Data generation. Simulate three independent datasets of artificial log exchange rates, each bivariate with $k=2$, using the data-generating processes (DGPs) below with the specified parameters, training length $T_{\\text{train}}$, test length $T_{\\text{test}}$, and Gaussian innovations with the given covariance matrices. For each dataset, simulate a total length of $T_{\\text{burn}} + T_{\\text{train}} + T_{\\text{test}}$ observations and discard the first $T_{\\text{burn}}$ as burn-in. Use exactly the seeds given for reproducibility. All entries of matrices and vectors are real numbers.\n\n- Test Case $1$ (true VAR($1$), stationary):\n  - Dimension: $k=2$.\n  - Seed: $314159$.\n  - Parameters: $\\mathbf{c} = \\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$,\n    $\\mathbf{A}_1 = \\begin{bmatrix}0.65  0.20 \\\\ -0.10  0.55\\end{bmatrix}$.\n  - Innovation covariance: $\\boldsymbol{\\Sigma} = \\begin{bmatrix}0.010  0.003 \\\\ 0.003  0.015\\end{bmatrix}$.\n  - Sample sizes: $T_{\\text{burn}} = 100$, $T_{\\text{train}} = 300$, $T_{\\text{test}} = 100$.\n\n- Test Case $2$ (true VAR($4$), stationary):\n  - Dimension: $k=2$.\n  - Seed: $271828$.\n  - Parameters: $\\mathbf{c} = \\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$,\n    $\\mathbf{A}_1 = \\begin{bmatrix}0.55  0.00 \\\\ 0.05  0.45\\end{bmatrix}$,\n    $\\mathbf{A}_2 = \\begin{bmatrix}-0.25  0.06 \\\\ 0.00  -0.15\\end{bmatrix}$,\n    $\\mathbf{A}_3 = \\begin{bmatrix}0.12  0.00 \\\\ 0.02  0.10\\end{bmatrix}$,\n    $\\mathbf{A}_4 = \\begin{bmatrix}-0.06  0.00 \\\\ 0.00  -0.04\\end{bmatrix}$.\n  - Innovation covariance: $\\boldsymbol{\\Sigma} = \\begin{bmatrix}0.020  -0.004 \\\\ -0.004  0.012\\end{bmatrix}$.\n  - Sample sizes: $T_{\\text{burn}} = 100$, $T_{\\text{train}} = 300$, $T_{\\text{test}} = 100$.\n\n- Test Case $3$ (true random walk in levels, nonstationary):\n  - Dimension: $k=2$.\n  - Seed: $161803$.\n  - Parameters: $\\mathbf{c} = \\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$,\n    $\\mathbf{A}_1 = \\mathbf{I}_2$ (the $2 \\times 2$ identity), and $\\mathbf{A}_i = \\mathbf{0}$ for all $i \\ge 2$.\n  - Innovation covariance: $\\boldsymbol{\\Sigma} = \\begin{bmatrix}0.005  0.0015 \\\\ 0.0015  0.004\\end{bmatrix}$.\n  - Sample sizes: $T_{\\text{burn}} = 100$, $T_{\\text{train}} = 300$, $T_{\\text{test}} = 100$.\n\n5. Program requirements. Your program must:\n  - Simulate each dataset exactly as specified using the given seeds.\n  - Estimate a VAR($1$) and a VAR($4$) with an intercept by Ordinary Least Squares (OLS) on the training sample only.\n  - Produce one-step-ahead forecasts over the test sample using fixed estimated parameters and actual lagged values.\n  - Compute the Root Mean Squared Forecast Error (RMSFE) for each of the three models (VAR($1$), VAR($4$), random walk) according to the definition above.\n  - For each test case, determine the index of the best model by RMSFE, using the following index convention: $0$ for VAR($1$), $1$ for VAR($4$), and $2$ for random walk.\n\n6. Final output format. Your program should produce a single line of output containing a comma-separated list of three elements (one per test case), enclosed in square brackets. Each element must itself be a list of four values in the order $[\\text{best\\_index}, \\mathrm{RMSFE}_{\\text{VAR}(1)}, \\mathrm{RMSFE}_{\\text{VAR}(4)}, \\mathrm{RMSFE}_{\\text{RW}}]$. Print all RMSFEs as decimal numbers rounded to six digits after the decimal point, and print best indices as integers. For example, the overall output should look like $[[b_1,r_{1,1},r_{4,1},r_{\\mathrm{rw},1}],[b_2,r_{1,2},r_{4,2},r_{\\mathrm{rw},2}],[b_3,r_{1,3},r_{4,3},r_{\\mathrm{rw},3}]]$, where $b_i \\in \\{0,1,2\\}$ and $r_{\\cdot,i}$ are floats rounded to six decimals.\n\nNo physical units or angles are involved in this problem. All numerical answers must be printed exactly as specified in the format above on a single line.", "solution": "The problem statement is valid. It presents a well-defined and self-contained computational experiment in time series econometrics. All parameters, models, and evaluation criteria are specified with sufficient precision to permit a unique and reproducible solution. The underlying principles are standard in the field of computational economics and finance. The task requires the implementation of a simulation study to compare the forecasting performance of Vector Autoregressive (VAR) models against a random walk benchmark.\n\nThe methodology proceeds in four distinct stages for each test case: data generation, model estimation, out-of-sample forecasting, and performance evaluation.\n\n1. Data Generation\nFor each test case, a bivariate time series $\\mathbf{y}_t \\in \\mathbb{R}^2$ of total length $T_{\\text{total}} = T_{\\text{burn}} + T_{\\text{train}} + T_{\\text{test}}$ is simulated from a Vector Autoregressive model of order $p_{\\text{true}}$, given by:\n$$\n\\mathbf{y}_t = \\mathbf{c} + \\sum_{i=1}^{p_{\\text{true}}} \\mathbf{A}_i \\mathbf{y}_{t-i} + \\mathbf{u}_t\n$$\nHere, $\\mathbf{c}$ is the intercept vector, $\\mathbf{A}_i$ are the $k \\times k$ coefficient matrices, and $\\mathbf{u}_t$ is a vector of innovations drawn from a multivariate normal distribution $\\mathcal{N}(\\mathbf{0}, \\boldsymbol{\\Sigma})$. The process is initialized with $\\mathbf{y}_t = \\mathbf{0}$ for $t  p_{\\text{true}}$. A specific seed is used for the random number generator to ensure reproducibility. The first $T_{\\text{burn}} = 100$ observations are discarded to mitigate the influence of initial conditions. The remaining data is split into a training sample of length $T_{\\text{train}} = 300$ and a testing sample of length $T_{\\text{test}} = 100$.\n\n2. Model Estimation\nTwo candidate models, a VAR($1$) and a VAR($4$), are estimated using the training sample. The estimation is performed via Ordinary Least Squares (OLS). For a generic VAR($p$) model, where $p \\in \\{1, 4\\}$, we formulate the system as a multivariate regression:\n$$\n\\mathbf{Y} = \\mathbf{X} \\mathbf{B}^\\top + \\mathbf{U}\n$$\nThe response matrix $\\mathbf{Y}$ is constructed by stacking the observation vectors $\\mathbf{y}_t^\\top$ for $t = p, \\dots, T_{\\text{train}}-1$. It has dimensions $(T_{\\text{train}} - p) \\times k$. The regressor matrix $\\mathbf{X}$ is constructed by stacking the corresponding regressor vectors $\\mathbf{x}_t^\\top = [1, \\mathbf{y}_{t-1}^\\top, \\ldots, \\mathbf{y}_{t-p}^\\top]$ for the same time indices. It has dimensions $(T_{\\text{train}} - p) \\times (1 + kp)$. The matrix $\\mathbf{B} = [\\mathbf{c}, \\mathbf{A}_1, \\ldots, \\mathbf{A}_p]$ contains all model coefficients and has dimensions $k \\times (1 + kp)$. The OLS estimate $\\widehat{\\mathbf{B}}$ is found by solving the normal equations, which can be expressed as:\n$$\n\\widehat{\\mathbf{B}}^\\top = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{Y}\n$$\nNumerically, this is solved using a more stable method like QR decomposition, as implemented in standard linear algebra libraries, which solves for $\\widehat{\\mathbf{B}}^\\top$ in the system $\\mathbf{X} \\widehat{\\mathbf{B}}^\\top = \\mathbf{Y}$. The estimated coefficients $\\widehat{\\mathbf{B}}$ are fixed and used for the entire forecasting exercise.\n\n3. Forecasting\nOne-step-ahead forecasts are generated for the duration of the test sample, from forecast origin $t = T_{\\text{train}}$ to $t = T_{\\text{train}} + T_{\\text{test}} - 1$.\n- For the VAR($p$) models, the forecast for $\\mathbf{y}_{t+1}$ made at time $t$ is:\n$$\n\\widehat{\\mathbf{y}}_{t+1|t} = \\widehat{\\mathbf{c}} + \\sum_{i=1}^{p} \\widehat{\\mathbf{A}}_i \\mathbf{y}_{t+1-i}\n$$\nThis computation uses the fixed estimated coefficients $\\widehat{\\mathbf{c}}$ and $\\widehat{\\mathbf{A}}_i$ from the training phase and the *actual* observed values of the time series for the lagged terms $\\mathbf{y}_{t}, \\mathbf{y}_{t-1}, \\dots$.\n- For the random walk (RW) benchmark model, the forecast is simply the most recent observation:\n$$\n\\widehat{\\mathbf{y}}_{t+1|t} = \\mathbf{y}_t\n$$\n\n4. Evaluation\nThe performance of each model (VAR($1$), VAR($4$), RW) is assessed using the Root Mean Squared Forecast Error (RMSFE). This metric aggregates the forecast errors across all $k$ variables and all $H = T_{\\text{test}}$ forecast horizons in the test sample:\n$$\n\\mathrm{RMSFE} = \\sqrt{\\frac{1}{H k} \\sum_{h=1}^{H} \\left\\|\\mathbf{y}_{T_{\\text{train}}+h} - \\widehat{\\mathbf{y}}_{T_{\\text{train}}+h|T_{\\text{train}}+h-1}\\right\\|_2^2}\n$$\nwhere $\\|\\cdot\\|_2^2$ is the squared Euclidean norm of the vector forecast error. The model with the lowest RMSFE is considered the best for that particular dataset. The program calculates this for each of the three models and identifies the best-performing one by its index: $0$ for VAR($1$), $1$ for VAR($4$), and $2$ for the random walk. The final output is an aggregation of these results for all three specified test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the computational experiment for three test cases.\n    \"\"\"\n    \n    test_cases = [\n        {\n            \"k\": 2,\n            \"seed\": 314159,\n            \"c\": np.array([0.0, 0.0]),\n            \"A_matrices\": [\n                np.array([[0.65, 0.20], [-0.10, 0.55]])\n            ],\n            \"Sigma\": np.array([[0.010, 0.003], [0.003, 0.015]]),\n            \"T_burn\": 100, \"T_train\": 300, \"T_test\": 100,\n        },\n        {\n            \"k\": 2,\n            \"seed\": 271828,\n            \"c\": np.array([0.0, 0.0]),\n            \"A_matrices\": [\n                np.array([[0.55, 0.00], [0.05, 0.45]]),\n                np.array([[-0.25, 0.06], [0.00, -0.15]]),\n                np.array([[0.12, 0.00], [0.02, 0.10]]),\n                np.array([[-0.06, 0.00], [0.00, -0.04]]),\n            ],\n            \"Sigma\": np.array([[0.020, -0.004], [-0.004, 0.012]]),\n            \"T_burn\": 100, \"T_train\": 300, \"T_test\": 100,\n        },\n        {\n            \"k\": 2,\n            \"seed\": 161803,\n            \"c\": np.array([0.0, 0.0]),\n            \"A_matrices\": [np.identity(2)],\n            \"Sigma\": np.array([[0.005, 0.0015], [0.0015, 0.004]]),\n            \"T_burn\": 100, \"T_train\": 300, \"T_test\": 100,\n        }\n    ]\n\n    results_all_cases = []\n\n    for case in test_cases:\n        y_full = _simulate_var(\n            k=case[\"k\"],\n            p_true=len(case[\"A_matrices\"]),\n            c=case[\"c\"],\n            A_matrices=case[\"A_matrices\"],\n            Sigma=case[\"Sigma\"],\n            T_total=case[\"T_burn\"] + case[\"T_train\"] + case[\"T_test\"],\n            seed=case[\"seed\"]\n        )\n        \n        y = y_full[case[\"T_burn\"]:]\n        y_train = y[:case[\"T_train\"]]\n\n        # Estimate VAR(1) and VAR(4) models\n        B_hat_1 = _estimate_var(y_train, p=1)\n        B_hat_4 = _estimate_var(y_train, p=4)\n\n        # Evaluate performance\n        rmsfe_var1 = _forecast_and_evaluate(y, case[\"T_train\"], p=1, B_hat=B_hat_1, model_type='VAR')\n        rmsfe_var4 = _forecast_and_evaluate(y, case[\"T_train\"], p=4, B_hat=B_hat_4, model_type='VAR')\n        rmsfe_rw = _forecast_and_evaluate(y, case[\"T_train\"], p=0, B_hat=None, model_type='RW')\n        \n        rmsfes = [rmsfe_var1, rmsfe_var4, rmsfe_rw]\n        best_index = int(np.argmin(rmsfes))\n\n        # Format results for the current case\n        case_result_str = f'[{best_index},' + ','.join([f'{r:.6f}' for r in rmsfes]) + ']'\n        results_all_cases.append(case_result_str)\n\n    # Final print statement\n    print(f\"[{','.join(results_all_cases)}]\")\n\ndef _simulate_var(k, p_true, c, A_matrices, Sigma, T_total, seed):\n    \"\"\"Simulates data from a VAR(p) process.\"\"\"\n    rng = np.random.default_rng(seed)\n    y = np.zeros((T_total, k))\n    u = rng.multivariate_normal(np.zeros(k), Sigma, size=T_total)\n\n    for t in range(p_true, T_total):\n        y_t = c.copy()\n        for i in range(1, p_true + 1):\n            y_t += A_matrices[i-1] @ y[t-i]\n        y[t] = y_t + u[t]\n    return y\n\ndef _estimate_var(y_train, p):\n    \"\"\"Estimates a VAR(p) model with an intercept using OLS.\"\"\"\n    T_train, k = y_train.shape\n    num_obs = T_train - p\n    \n    Y = y_train[p:]\n    X = np.zeros((num_obs, 1 + k * p))\n    \n    for t in range(p, T_train):\n        regressor_row = [1.0]\n        for i in range(1, p + 1):\n            regressor_row.extend(y_train[t - i])\n        X[t - p, :] = regressor_row\n    \n    B_T, _, _, _ = np.linalg.lstsq(X, Y, rcond=None)\n    \n    return B_T.T\n\ndef _forecast_and_evaluate(y, T_train, p, B_hat, model_type):\n    \"\"\"Generates one-step-ahead forecasts and computes RMSFE.\"\"\"\n    T_total, k = y.shape\n    T_test = T_total - T_train\n    \n    squared_errors_sum = 0.0\n\n    for h in range(T_test):\n        # Forecast origin is t = T_train + h - 1\n        t = T_train + h - 1\n        actual_y = y[t + 1]\n\n        if model_type == 'RW':\n            forecast_y = y[t]\n        elif model_type == 'VAR':\n            x_t = [1.0]\n            for i in range(p):\n                x_t.extend(y[t - i])\n            x_t_vec = np.array(x_t)\n            forecast_y = B_hat @ x_t_vec\n        else:\n            raise ValueError(\"Unknown model_type\")\n\n        forecast_error = actual_y - forecast_y\n        squared_errors_sum += np.sum(forecast_error**2)\n    \n    rmsfe = np.sqrt(squared_errors_sum / (T_test * k))\n    return rmsfe\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2447495"}, {"introduction": "While forecasting is a primary use of VAR models, understanding the dynamic interactions between variables is equally important. This exercise demonstrates the critical consequences of model misspecification, showing how an overly simplistic model can produce a misleading picture of a system's response to shocks. By comparing the true impulse response function (IRF) of a process with that of a misspecified approximation, you will gain a deeper appreciation for the importance of selecting the correct model structure [@problem_id:2447477].", "problem": "You are given a stationary univariate data-generating process defined by the second-order autoregression\n$$ y_t = 0.9\\,y_{t-1} - 0.8\\,y_{t-2} + \\epsilon_t, $$\nwhere the innovations satisfy $$\\epsilon_t \\sim \\text{i.i.d. } \\mathcal{N}(0,1)$$ and are independent across time. Let the initial conditions be $$y_{-1}=0 \\text{ and } y_0=0.$$ Consider approximating the dynamics of this process by a first-order Vector Autoregression (VAR) model (with one variable, which reduces to an autoregression of order one), that is, a model of the form\n$$ y_t = a\\,y_{t-1} + u_t, $$\nwith no intercept and with $u_t$ being a residual term.\n\nYour tasks are the following, to be executed entirely within a single program:\n\n1. Simulation. For each test case, simulate $T+B$ observations from the given second-order autoregression with the specified parameters above, where $B$ is a burn-in length. Discard the first $B$ observations and keep the last $T$ observations for analysis.\n\n2. Estimation. For each retained sample, estimate the first-order Vector Autoregression (VAR(1)) coefficient $a$ by minimizing the sum of squared one-step-ahead prediction errors over the retained sample, i.e., find\n$$ \\hat{a} \\in \\arg\\min_{a \\in \\mathbb{R}} \\sum_{t=1}^{T-1} \\left(y_{t+1} - a\\,y_t\\right)^2. $$\n\n3. Impulse responses. For each test case and for a specified horizon $H \\in \\mathbb{N}$, compute:\n   - The one-standard-deviation impulse response sequence of the true second-order autoregression at horizons $h=1,2,\\dots,H$, defined recursively by the moving-average coefficients $\\{\\psi_h\\}_{h \\ge 0}$ that satisfy\n     $$ \\psi_0 = 1,\\quad \\psi_1 = 0.9,\\quad \\psi_h = 0.9\\,\\psi_{h-1} - 0.8\\,\\psi_{h-2}\\ \\text{for all}\\ h \\ge 2. $$\n     The impulse response at horizon $h$ is $\\psi_h$.\n   - The impulse response sequence of the estimated first-order Vector Autoregression at horizons $h=1,2,\\dots,H$, defined by $\\theta_h = \\hat{a}^h$ with the convention $\\theta_0=1$.\n\n4. Misleading-dynamics metric. For each test case, compute the fraction of horizons in $\\{1,2,\\dots,H\\}$ at which the sign of the estimated VAR(1) impulse response differs from the sign of the true impulse response. Use the sign function $\\operatorname{sgn}:\\mathbb{R}\\to\\{-1,0,1\\}$ given by\n$$ \\operatorname{sgn}(x)=\\begin{cases}1\\text{if }x0,\\\\0\\text{if }x=0,\\\\-1\\text{if }x0.\\end{cases} $$\nLet $m(H)$ denote the count of indices $h \\in \\{1,\\dots,H\\}$ such that $\\operatorname{sgn}(\\theta_h) \\neq \\operatorname{sgn}(\\psi_h)$. The required metric is the fraction\n$$ \\frac{m(H)}{H}. $$\n\n5. Output specification. Your program must output a single line containing a comma-separated list enclosed in square brackets with one floating-point number for each test case, corresponding to the fraction $m(H)/H$, rounded to six decimal places.\n\nSimulation details to be used uniformly in all test cases:\n- Burn-in length $B=500$.\n- Innovation variance equal to $1$.\n- The retained sample size $T$ and horizon $H$ are specified per test case below.\n- All angles, if any arise in reasoning, are not required in the output and do not need units. No physical units are involved.\n\nTest suite. Run your program for the following parameter values:\n- Test case 1: $T=5000$ and $H=12$.\n- Test case 2: $T=100$ and $H=12$.\n- Test case 3: $T=2000$ and $H=1$.\n- Test case 4: $T=2000$ and $H=30$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases listed above, for example, $[r_1,r_2,r_3,r_4]$, where each $r_i$ is the fraction $m(H)/H$ for the corresponding test case, rounded to six decimal places.", "solution": "The problem statement has been validated and is determined to be a valid, scientifically-grounded, and well-posed exercise in computational econometrics. It addresses the consequences of model misspecification by approximating a second-order autoregressive process, AR($2$), with a first-order autoregressive model, AR($1$). All parameters and procedures are specified unambiguously. The stationarity of the true AR($2$) process, $y_t = 0.9\\,y_{t-1} - 0.8\\,y_{t-2} + \\epsilon_t$, is confirmed by analyzing its characteristic polynomial, $\\lambda(z) = 1 - 0.9 z + 0.8 z^2$. The roots of $\\lambda(z)=0$ are $z = \\frac{0.9 \\pm i\\sqrt{2.39}}{1.6}$, which have a modulus of $\\sqrt{1.25}$. Since this modulus is greater than $1$, the roots lie outside the unit circle, confirming that the process is indeed stationary, as stated in the problem.\n\nThe solution requires a Monte Carlo simulation study. The procedure involves four principal steps for each test case: simulation of the true process, estimation of the approximate model's parameters, computation of impulse response functions for both models, and calculation of a metric to quantify the discrepancy in their dynamic properties.\n\n**Step 1: Simulation of the AR(2) Process**\nThe data-generating process is defined by the stochastic difference equation:\n$$ y_t = 0.9\\,y_{t-1} - 0.8\\,y_{t-2} + \\epsilon_t $$\nwhere the innovations $\\epsilon_t$ are independent and identically distributed standard normal random variables, i.e., $\\epsilon_t \\sim \\mathcal{N}(0,1)$. The process is initialized with conditions $y_{-1}=0$ and $y_0=0$. For each test case defined by a sample size $T$ and horizon $H$, a time series of total length $T+B$ is generated, where $B=500$ is a specified burn-in period. The first $B$ observations are discarded to ensure that the analysis is performed on a sample that is representative of the process's stationary distribution, thus mitigating the influence of the arbitrary initial conditions. The simulation proceeds iteratively using the given recurrence relation, starting from $t=1$:\n- $y_1 = 0.9\\,y_0 - 0.8\\,y_{-1} + \\epsilon_1 = \\epsilon_1$\n- $y_2 = 0.9\\,y_1 - 0.8\\,y_0 + \\epsilon_2 = 0.9\\,y_1 + \\epsilon_2$\n- ... and so on for subsequent time steps up to $t = T+B$.\n\n**Step 2: Estimation of the AR(1) Coefficient**\nThe more complex AR($2$) process is to be approximated by a simpler AR($1$) model, which, being a special case of a VAR($1$) with one variable, is given by:\n$$ y_t = a\\,y_{t-1} + u_t $$\nThe coefficient $a$ is estimated using Ordinary Least Squares (OLS) by minimizing the sum of squared one-step-ahead prediction errors on the retained sample. Let the retained sample be denoted as $\\{z_t\\}_{t=1}^T$, where $z_t$ corresponds to the simulated value $y_{B+t}$. The optimization problem is to find $\\hat{a}$ which solves:\n$$ \\hat{a} = \\arg\\min_{a \\in \\mathbb{R}} \\sum_{t=1}^{T-1} (z_{t+1} - a\\,z_t)^2 $$\nThis is a standard regression problem, for which the OLS estimator $\\hat{a}$ has the well-known closed-form solution:\n$$ \\hat{a} = \\frac{\\sum_{t=1}^{T-1} z_t z_{t+1}}{\\sum_{t=1}^{T-1} z_t^2} $$\nThis estimated coefficient, $\\hat{a}$, represents the best linear forecast coefficient based only on the most recent observation.\n\n**Step 3: Impulse Response Function (IRF) Computation**\nAn IRF describes the evolution of the system in response to a single, temporary shock to one of its innovations.\n- **True IRF:** For the true AR($2$) process, the response of $y_{t+h}$ to a unit shock $\\epsilon_t=1$ is given by the coefficient $\\psi_h$ in the Wold moving-average representation $y_t = \\sum_{j=0}^{\\infty} \\psi_j \\epsilon_{t-j}$. These coefficients are computed recursively according to the problem's definition:\n  - $\\psi_0 = 1$\n  - $\\psi_1 = 0.9$\n  - $\\psi_h = 0.9\\,\\psi_{h-1} - 0.8\\,\\psi_{h-2}$ for all integers $h \\ge 2$.\nThe complex roots of the characteristic polynomial imply that the true IRF, $\\{\\psi_h\\}_{h0}$, will be a damped sinusoid, exhibiting oscillations in sign. The sequence of responses $\\{\\psi_h\\}_{h=1}^H$ is computed for each specified horizon $H$.\n\n- **Estimated IRF:** For the estimated AR($1$) model, the impulse response at horizon $h$ is given by $\\theta_h$:\n  - $\\theta_h = \\hat{a}^h$\nThis IRF is a simple geometric sequence. Its behavior is monotonic decay towards zero if $|\\hat{a}|  1$ and $\\hat{a}0$, or oscillating decay if $|\\hat{a}|1$ and $\\hat{a}0$. The sequence $\\{\\theta_h\\}_{h=1}^H$ is computed using the estimated coefficient $\\hat{a}$ from Step 2.\n\n**Step 4: Misleading-Dynamics Metric**\nThe central task is to quantify how often the simplified AR($1$) model provides a qualitatively incorrect forecast of the system's dynamic response. The sign of the impulse response is a fundamental qualitative feature, indicating whether the variable is expected to be above or below its long-run mean following a positive shock. A discrepancy in signs between the true and estimated IRFs reveals a failure of the simpler model to capture the true dynamics. The metric is defined as the fraction of horizons $h \\in \\{1, 2, \\dots, H\\}$ where the signs differ:\n$$ \\text{Metric} = \\frac{m(H)}{H} = \\frac{1}{H} \\sum_{h=1}^{H} \\mathbf{1}_{\\{\\operatorname{sgn}(\\theta_h) \\neq \\operatorname{sgn}(\\psi_h)\\}} $$\nwhere $\\mathbf{1}_{\\{\\cdot\\}}$ denotes the indicator function, which is $1$ if its argument is true and $0$ otherwise, and $\\operatorname{sgn}(\\cdot)$ is the standard sign function. This metric is computed for each test case.\n\nThe algorithmic implementation will precisely follow these four steps. A Python script is developed to automate the entire procedure for the suite of test cases, from data simulation to the final metric calculation, ensuring the output format is strictly followed.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of comparing AR(2) dynamics with an AR(1) approximation.\n    \"\"\"\n\n    # --- Problem Parameters ---\n    test_cases = [\n        (5000, 12),  # T, H for case 1\n        (100, 12),   # T, H for case 2\n        (2000, 1),   # T, H for case 3\n        (2000, 30),  # T, H for case 4\n    ]\n    B = 500  # Burn-in length\n    ar2_coeffs = (0.9, -0.8) # Coefficients for y_{t-1} and y_{t-2}\n    \n    # Store results for each test case\n    results = []\n\n    for T, H in test_cases:\n        # --- 1. Simulation ---\n        # Simulate T+B observations from the AR(2) process.\n        total_len = T + B\n        eps = np.random.normal(loc=0.0, scale=1.0, size=total_len)\n        y = np.zeros(total_len)\n\n        # Initial conditions y_{-1}=0, y_0=0\n        # t=1 (index 0): y[0] = 0.9*y_0 - 0.8*y_{-1} + eps[0] = eps[0]\n        if total_len  0:\n            y[0] = eps[0]\n        \n        # t=2 (index 1): y[1] = 0.9*y[0] - 0.8*y_0 + eps[1] = 0.9*y[0] + eps[1]\n        if total_len  1:\n            y[1] = ar2_coeffs[0] * y[0] + eps[1]\n        \n        # for t=3, .. N (index 2 to N-1)\n        for t in range(2, total_len):\n            y[t] = ar2_coeffs[0] * y[t-1] + ar2_coeffs[1] * y[t-2] + eps[t]\n\n        # Discard burn-in period to get the retained sample\n        y_retained = y[B:]\n\n        # --- 2. Estimation ---\n        # Estimate the AR(1) coefficient 'a' using OLS.\n        # The model is y_{t+1} = a*y_t + u_t\n        # dependent variable: y_retained[1:]\n        # independent variable: y_retained[:-1]\n        y_reg_dep = y_retained[1:]\n        y_reg_indep = y_retained[:-1]\n        \n        # OLS formula: a_hat = (X'Y) / (X'X)\n        numerator = np.dot(y_reg_indep, y_reg_dep)\n        denominator = np.dot(y_reg_indep, y_reg_indep)\n\n        a_hat = 0.0 if denominator == 0 else numerator / denominator\n\n        # --- 3. Impulse Responses ---\n        # True AR(2) IRF\n        psi = np.zeros(H + 1)\n        if H = 0:\n            psi[0] = 1.0\n        if H = 1:\n            psi[1] = ar2_coeffs[0]\n        for h in range(2, H + 1):\n            psi[h] = ar2_coeffs[0] * psi[h-1] + ar2_coeffs[1] * psi[h-2]\n        \n        # We need IRF for horizons h=1, ..., H\n        psi_true = psi[1:]\n\n        # Estimated AR(1) IRF\n        # theta_h = a_hat^h for h=1, ..., H\n        horizons = np.arange(1, H + 1)\n        theta_est = np.power(a_hat, horizons)\n\n        # --- 4. Misleading-Dynamics Metric ---\n        # Count horizons where signs of IRFs differ\n        m_H = 0\n        for h in range(H):\n            if np.sign(theta_est[h]) != np.sign(psi_true[h]):\n                m_H += 1\n        \n        metric = m_H / H if H > 0 else 0.0\n        results.append(metric)\n\n    # --- 5. Output Specification ---\n    # Print results in the required format\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2447477"}, {"introduction": "One of the most tempting, and dangerous, interpretations of VAR models is to infer causality from statistical tests. This practice explores a classic pitfall known as spurious Granger causality, which arises from omitted variable bias. You will simulate a system where two variables are driven by a common latent factor and demonstrate how a VAR model that omits this factor can wrongly conclude that one variable \"causes\" the other, a vital lesson in the careful interpretation of econometric results [@problem_id:2447550].", "problem": "You are given a latent-driven trivariate linear data-generating process designed to illustrate omitted-variable bias in Vector Autoregression (VAR) estimation. A Vector Autoregression (VAR) is a system of linear stochastic difference equations describing the joint dynamics of multiple time series. Consider three processes $\\{Z_t\\}$, $\\{X_t\\}$, and $\\{Y_t\\}$ generated by\n$$Z_t = \\rho_Z Z_{t-1} + u_t,$$\n$$X_t = \\phi_X X_{t-1} + b_X Z_{t-1} + e_t,$$\n$$Y_t = \\phi_Y Y_{t-1} + b_Y Z_{t-1} + v_t,$$\nwhere $u_t$, $e_t$, and $v_t$ are mutually independent, serially independent, mean-zero, Gaussian shocks with variances $\\sigma_u^2$, $\\sigma_e^2$, and $\\sigma_v^2$, respectively. In the full-information trivariate system, $X_t$ does not Granger-cause $Y_t$ because there is no direct lag of $X_t$ entering the law of motion for $Y_t$. Granger causality is defined as follows: process $X_t$ Granger-causes process $Y_t$ if past values of $X_t$ improve the mean squared prediction of $Y_t$ at horizon $1$ beyond the information contained in the past of $Y_t$ and all other relevant processes.\n\nHowever, in practice the latent process $Z_t$ is omitted, and a bivariate VAR is estimated on $(X_t, Y_t)$ only. Your task is to demonstrate how this omission can induce spurious Granger-causal relationships in the estimated bivariate system.\n\nFundamental base:\n- Use the definition of Granger causality in terms of linear predictability, and Ordinary Least Squares (OLS) estimation as the best linear unbiased estimator under the Gauss–Markov conditions. Ordinary Least Squares (OLS) relies on solving the normal equations to minimize the sum of squared residuals.\n- For nested linear models under Gaussian disturbances, the classical F test derived from the ratio of mean squared residual sums provides a valid test for linear exclusion restrictions at finite sample sizes based on the Fisher–Snedecor distribution (F).\n\nInstructions:\n- For each test case below, simulate data from the trivariate system with the specified parameters. Use a burn-in of $B = 300$ observations that are discarded before analysis to mitigate initialization effects. Initialize $Z_0$, $X_0$, and $Y_0$ at $0$.\n- For the observed bivariate data $(X_t, Y_t)$ after burn-in, estimate a bivariate VAR of order $p = 1$ with an intercept by OLS, equation-by-equation.\n- In the $Y$-equation, test the null that $X$ does not Granger-cause $Y$ in the observed bivariate system. This null imposes that all coefficients on the $p$ lags of $X$ in the $Y$-equation are equal to $0$. Construct the standard nested-model F-statistic comparing the unrestricted $Y$-equation (including the lag(s) of $X$) to the restricted $Y$-equation (excluding the lag(s) of $X$), and compute the $p$-value using the cumulative distribution function of the Fisher–Snedecor distribution with the appropriate numerator and denominator degrees of freedom implied by the number of exclusion restrictions and the unrestricted residual degrees of freedom. Reject the null at significance level $\\alpha = 0.05$ if and only if the $p$-value is strictly less than $\\alpha$.\n- For each test case, output a boolean indicating whether the null “$X$ does not Granger-cause $Y$” is rejected in the observed bivariate VAR.\n\nTest suite:\n- Case $1$ (high latent persistence; “happy path” for spurious detection):\n  - $T = 1000$, $p = 1$, $\\alpha = 0.05$, $\\rho_Z = 0.95$, $\\phi_X = 0.2$, $\\phi_Y = 0.2$, $b_X = 1.5$, $b_Y = 1.5$, $\\sigma_u = 0.5$, $\\sigma_e = 0.5$, $\\sigma_v = 0.5$, seed $= 123456$.\n- Case $2$ (latent white noise; boundary with minimal spurious predictability from $X_{t-1}$ to $Y_t$):\n  - $T = 600$, $p = 1$, $\\alpha = 0.05$, $\\rho_Z = 0.0$, $\\phi_X = 0.4$, $\\phi_Y = 0.6$, $b_X = 1.0$, $b_Y = 1.0$, $\\sigma_u = 1.0$, $\\sigma_e = 1.0$, $\\sigma_v = 1.0$, seed $= 20231102$.\n- Case $3$ (small sample; edge case for low power and sampling variability):\n  - $T = 120$, $p = 1$, $\\alpha = 0.05$, $\\rho_Z = 0.9$, $\\phi_X = 0.6$, $\\phi_Y = 0.6$, $b_X = 1.2$, $b_Y = 0.8$, $\\sigma_u = 0.8$, $\\sigma_e = 0.8$, $\\sigma_v = 0.8$, seed $= 7$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the cases listed above, where each entry is a boolean corresponding to whether the null “$X$ does not Granger-cause $Y$” is rejected. For example, an output with three cases should look like $[{\\rm True},{\\rm False},{\\rm True}]$.", "solution": "The problem requires an investigation into the phenomenon of spurious Granger causality that arises from omitted variable bias in a Vector Autoregression (VAR) model. We are provided with a trivariate linear stochastic process consisting of two observed variables, $X_t$ and $Y_t$, and one unobserved (latent) common driver, $Z_t$. The task is to simulate this system under specified parameter sets and test for Granger causality from $X_t$ to $Y_t$ in a misspecified bivariate VAR that omits $Z_t$.\n\nThe data generating process (DGP) is defined by the following system of equations:\n$$Z_t = \\rho_Z Z_{t-1} + u_t$$\n$$X_t = \\phi_X X_{t-1} + b_X Z_{t-1} + e_t$$\n$$Y_t = \\phi_Y Y_{t-1} + b_Y Z_{t-1} + v_t$$\nwhere $u_t$, $e_t$, and $v_t$ are mutually independent, serially uncorrelated Gaussian white noise processes with mean $0$ and variances $\\sigma_u^2$, $\\sigma_e^2$, and $\\sigma_v^2$, respectively.\n\nIn this true structural model, the evolution of $Y_t$ depends only on its own lag, $Y_{t-1}$, and the lag of the latent process, $Z_{t-1}$. There is no term involving a lag of $X_t$ in the equation for $Y_t$. Therefore, in the context of the full trivariate system, $X_t$ does not Granger-cause $Y_t$.\n\nHowever, the econometrician does not observe $Z_t$ and proceeds to estimate a bivariate VAR model of order $p=1$ on the observed data $(X_t, Y_t)$. The equation for $Y_t$ in this misspecified model is:\n$$Y_t = c_Y + \\beta_{Y,1} Y_{t-1} + \\beta_{X,1} X_{t-1} + \\epsilon_t$$\nwhere $\\epsilon_t$ is the new error term. The test for Granger causality from $X_t$ to $Y_t$ is a test of the null hypothesis $H_0: \\beta_{X,1} = 0$.\n\nSpurious Granger causality occurs if we reject this null hypothesis, not because $X_{t-1}$ has genuine predictive content for $Y_t$ (after conditioning on $Y_{t-1}$), but because of the model's misspecification. This is a classic case of omitted variable bias. The Ordinary Least Squares (OLS) estimator for $\\beta_{X,1}$, denoted $\\hat{\\beta}_{X,1}$, will be biased if the included regressor $X_{t-1}$ is correlated with the omitted variable $Z_{t-1}$, which is part of the true error term of the misspecified regression.\n\nThe true model for $Y_t$ can be expressed as $Y_t = \\phi_Y Y_{t-1} + (b_Y Z_{t-1} + v_t)$. When we estimate the bivariate model, the term $b_Y Z_{t-1}$ is omitted and becomes part of the error term $\\epsilon_t$. For the OLS estimator $\\hat{\\beta}_{X,1}$ to be biased, two conditions must be met:\n$1$. The omitted variable $Z_{t-1}$ must be a determinant of $Y_t$. In our DGP, this holds if $b_Y \\neq 0$.\n$2$. The omitted variable $Z_{t-1}$ must be correlated with the included regressor $X_{t-1}$, i.e., $\\mathrm{Cov}(X_{t-1}, Z_{t-1}) \\neq 0$.\n\nLet us analyze this covariance, assuming the processes are stationary. From the DGP, we have $X_{t-1} = \\phi_X X_{t-2} + b_X Z_{t-2} + e_{t-1}$ and $Z_{t-1} = \\rho_Z Z_{t-2} + u_{t-1}$. Taking the covariance of $X_{t-1}$ and $Z_{t-1}$ (and assuming zero means), we find:\n$$\\mathrm{E}[X_{t-1} Z_{t-1}] = \\mathrm{E}[(\\phi_X X_{t-2} + b_X Z_{t-2} + e_{t-1})(\\rho_Z Z_{t-2} + u_{t-1})]$$\nDue to the independence of shocks, this simplifies to:\n$$\\mathrm{E}[X_{t-1} Z_{t-1}] = \\phi_X \\rho_Z \\mathrm{E}[X_{t-2}Z_{t-2}] + b_X \\rho_Z \\mathrm{E}[Z_{t-2}^2]$$\nLet $\\Gamma_{XZ} = \\mathrm{E}[X_t Z_t]$ and $\\gamma_Z(0) = \\mathrm{E}[Z_t^2]$. In steady state, we have $\\Gamma_{XZ} = \\phi_X \\rho_Z \\Gamma_{XZ} + b_X \\rho_Z \\gamma_Z(0)$, which implies $\\Gamma_{XZ} = \\frac{b_X \\rho_Z}{1 - \\phi_X \\rho_Z} \\gamma_Z(0)$.\nSince $\\gamma_Z(0) = \\sigma_u^2 / (1 - \\rho_Z^2)  0$, the covariance $\\mathrm{Cov}(X_{t-1}, Z_{t-1})$ is non-zero if and only if $b_X \\neq 0$ and $\\rho_Z \\neq 0$.\n\nTherefore, spurious Granger causality from $X_t$ to $Y_t$ is expected to arise when $b_X \\neq 0$, $b_Y \\neq 0$, and $\\rho_Z \\neq 0$. The regressor $X_{t-1}$ acts as a proxy for the omitted variable $Z_{t-1}$, and its coefficient $\\beta_{X,1}$ spuriously picks up the influence of $Z_{t-1}$ on $Y_t$. If $\\rho_Z=0$, $Z_t$ is white noise, and $Z_{t-1}$ is uncorrelated with $X_{t-1}$, breaking the mechanism for bias.\n\nThe procedure is to perform a hypothesis test for each case. We generate $T+B$ data points and discard the first $B=300$ for burn-in. On the remaining $T$ observations, we estimate the $Y$-equation of the bivariate VAR(1) using OLS. This requires $T-1$ effective observations. The unrestricted model is $Y_t = c + \\beta_1 Y_{t-1} + \\beta_2 X_{t-1} + \\epsilon_t$, and the restricted model under $H_0: \\beta_2=0$ is $Y_t = c' + \\beta'_1 Y_{t-1} + \\epsilon'_t$.\n\nWe compute the F-statistic using the residual sums of squares ($RSS$) from both regressions:\n$$F = \\frac{(RSS_R - RSS_U) / q}{RSS_U / (N_{reg} - k)}$$\nHere, $RSS_R$ and $RSS_U$ are the RSS for the restricted and unrestricted models, respectively. The number of regression observations is $N_{reg} = T-1$. The number of restrictions is $q=1$. The number of parameters in the unrestricted model is $k=3$ (intercept, one lag of $Y$, one lag of $X$). The degrees of freedom for the F-distribution are $q=1$ for the numerator and $N_{reg} - k = (T-1) - 3 = T-4$ for the denominator.\n\nThe p-value is calculated from the cumulative distribution function (CDF) of the $F_{1, T-4}$ distribution. The null hypothesis is rejected at significance level $\\alpha=0.05$ if the p-value is less than $0.05$.\n\n- Case $1$: High persistence in the latent variable ($\\rho_Z = 0.95$) and a large sample size ($T=1000$). All conditions for spurious causality are strongly met. Rejection of the null is expected.\n- Case $2$: The latent variable is white noise ($\\rho_Z = 0.0$). The mechanism for omitted variable bias is absent. The test should not reject the null, barring a Type I error.\n- Case $3$: High latent persistence ($\\rho_Z = 0.9$) but a small sample size ($T=120$). The bias is present, but the statistical power of the test may be insufficient to detect it. The outcome demonstrates the interplay between bias magnitude and sample size.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import f as f_dist\n\ndef solve():\n    \"\"\"\n    Main function to run test cases for spurious Granger causality.\n    \"\"\"\n    test_cases = [\n        # Case 1 (high latent persistence)\n        {\n            \"T\": 1000, \"p\": 1, \"alpha\": 0.05, \"rho_Z\": 0.95, \"phi_X\": 0.2, \"phi_Y\": 0.2,\n            \"b_X\": 1.5, \"b_Y\": 1.5, \"sigma_u\": 0.5, \"sigma_e\": 0.5, \"sigma_v\": 0.5,\n            \"seed\": 123456\n        },\n        # Case 2 (latent white noise)\n        {\n            \"T\": 600, \"p\": 1, \"alpha\": 0.05, \"rho_Z\": 0.0, \"phi_X\": 0.4, \"phi_Y\": 0.6,\n            \"b_X\": 1.0, \"b_Y\": 1.0, \"sigma_u\": 1.0, \"sigma_e\": 1.0, \"sigma_v\": 1.0,\n            \"seed\": 20231102\n        },\n        # Case 3 (small sample)\n        {\n            \"T\": 120, \"p\": 1, \"alpha\": 0.05, \"rho_Z\": 0.9, \"phi_X\": 0.6, \"phi_Y\": 0.6,\n            \"b_X\": 1.2, \"b_Y\": 0.8, \"sigma_u\": 0.8, \"sigma_e\": 0.8, \"sigma_v\": 0.8,\n            \"seed\": 7\n        }\n    ]\n\n    results = []\n    for case_params in test_cases:\n        result = run_granger_test(case_params)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef run_granger_test(params):\n    \"\"\"\n    Simulates data and performs the Granger causality F-test for a single case.\n\n    Args:\n        params (dict): A dictionary of parameters for the simulation and test.\n\n    Returns:\n        bool: True if the null hypothesis is rejected, False otherwise.\n    \"\"\"\n    # Unpack parameters\n    T = params[\"T\"]\n    p = params[\"p\"]\n    alpha = params[\"alpha\"]\n    rho_Z = params[\"rho_Z\"]\n    phi_X = params[\"phi_X\"]\n    phi_Y = params[\"phi_Y\"]\n    b_X = params[\"b_X\"]\n    b_Y = params[\"b_Y\"]\n    sigma_u = params[\"sigma_u\"]\n    sigma_e = params[\"sigma_e\"]\n    sigma_v = params[\"sigma_v\"]\n    seed = params[\"seed\"]\n    \n    B = 300  # Burn-in period\n    T_total = T + B\n\n    # Set seed for reproducibility\n    np.random.seed(seed)\n\n    # Generate shocks\n    u = np.random.normal(0, sigma_u, T_total)\n    e = np.random.normal(0, sigma_e, T_total)\n    v = np.random.normal(0, sigma_v, T_total)\n\n    # Initialize time series arrays\n    Z = np.zeros(T_total)\n    X = np.zeros(T_total)\n    Y = np.zeros(T_total)\n\n    # Simulate the trivariate system\n    for t in range(1, T_total):\n        Z[t] = rho_Z * Z[t-1] + u[t]\n        X[t] = phi_X * X[t-1] + b_X * Z[t-1] + e[t]\n        Y[t] = phi_Y * Y[t-1] + b_Y * Z[t-1] + v[t]\n\n    # Discard burn-in period\n    X_sample = X[B:]\n    Y_sample = Y[B:]\n\n    # Prepare data for regression (VAR order p=1)\n    # Effective sample size for regression is T-p\n    y_vec = Y_sample[p:]\n    N_reg = len(y_vec)\n\n    # Regressors for the unrestricted model: intercept, Y_lag1, X_lag1\n    X_unrestricted = np.vstack([\n        np.ones(N_reg),\n        Y_sample[p-1:-1],\n        X_sample[p-1:-1]\n    ]).T\n\n    # Regressors for the restricted model: intercept, Y_lag1\n    X_restricted = np.vstack([\n        np.ones(N_reg),\n        Y_sample[p-1:-1]\n    ]).T\n\n    # OLS estimation via np.linalg.lstsq\n    # lstsq returns: coefficients, residuals (sum of squares), rank, singular values\n    # We only need the residual sum of squares (RSS)\n    _, rss_unrestricted, _, _ = np.linalg.lstsq(X_unrestricted, y_vec, rcond=None)\n    _, rss_restricted, _, _ = np.linalg.lstsq(X_restricted, y_vec, rcond=None)\n    \n    # lstsq returns RSS as a one-element array, so we extract the float\n    rss_u = rss_unrestricted[0]\n    rss_r = rss_restricted[0]\n\n    # Compute the F-statistic\n    q = X_unrestricted.shape[1] - X_restricted.shape[1]\n    k_unrestricted = X_unrestricted.shape[1]\n    df_num = q\n    df_den = N_reg - k_unrestricted\n    \n    # Check for df_den  0 to avoid division by zero\n    if df_den = 0:\n        return False # Cannot perform test\n\n    F_statistic = ((rss_r - rss_u) / df_num) / (rss_u / df_den)\n\n    # Compute the p-value using the survival function (1 - CDF)\n    p_value = f_dist.sf(F_statistic, dfn=df_num, dfd=df_den)\n\n    # Reject null if p-value is less than the significance level\n    return p_value  alpha\n\n# Run the simulation and print the results\nsolve()\n```", "id": "2447550"}]}