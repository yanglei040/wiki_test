{"hands_on_practices": [{"introduction": "The foundation of Ordinary Least Squares (OLS) is the assumption of a linear relationship. This practice exercise explores the critical implications of this assumption by challenging you to fit a linear model to data that is perfectly deterministic but non-linear. By doing so, you will discover firsthand how OLS can fail to detect a strong relationship, yielding misleading statistics like a low $R^2$ and a slope coefficient near zero [@problem_id:2417149]. This is a crucial lesson in the importance of data visualization and choosing the correct functional form for your model.", "problem": "Construct a program that demonstrates, by first principles, how a linear Ordinary Least Squares (OLS) regression can fail to detect a deterministic but non-linear relationship. For each test case below, you must generate a synthetic dataset and compute the OLS fit with an intercept, defined as the pair of real numbers $\\left(\\hat{\\beta}_0,\\hat{\\beta}_1\\right)$ that minimize the sum of squared residuals $\\sum_{i=1}^{N}\\left(y_i - \\beta_0 - \\beta_1 x_i\\right)^2$. Then compute the coefficient of determination $R^2$ as $1 - \\dfrac{\\sum_{i=1}^{N}\\hat{\\varepsilon}_i^2}{\\sum_{i=1}^{N}(y_i - \\bar{y})^2}$, where $\\hat{\\varepsilon}_i = y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i$ and $\\bar{y} = \\dfrac{1}{N}\\sum_{i=1}^{N} y_i$. All computations must be performed in exact arithmetic using real numbers, and all generated data are deterministic.\n\nGenerate the following three datasets, each with $N=1001$ evenly spaced points $x_i$ on the specified interval, and the corresponding $y_i$ as specified. There is no randomness in any test:\n\n- Test case A (non-linear periodic, symmetric domain): $x_i$ on $[-\\pi,\\pi]$, $y_i = \\cos(x_i)$.\n- Test case B (non-linear convex, symmetric domain): $x_i$ on $[-2,2]$, $y_i = x_i^2$.\n- Test case C (correctly specified linear benchmark): $x_i$ on $[-5,5]$, $y_i = 3 + 2 x_i$.\n\nYour program must, for each test case, compute and return the pair $\\left(\\hat{\\beta}_1, R^2\\right)$ in this order. The three pairs must be aggregated, in order, into a single line of output as a comma-separated list of three sublists without spaces, with each real number rounded to $6$ decimal places. For example, the required format is\n\"[[b1_A,R2_A],[b1_B,R2_B],[b1_C,R2_C]]\"\nwith all numerals shown to $6$ decimal places.\n\nTest Suite and Answer Specification:\n- The input parameters are fixed as given above; there is no user input.\n- The outputs are three ordered pairs of real numbers, one for each test case: $\\left(\\hat{\\beta}_1, R^2\\right)$ for A, B, and C.\n- The final output must be printed as a single line containing a list of three sublists, each sublist containing two floats rounded to $6$ decimal places, with no spaces.\n\nDesign for coverage:\n- Test case A probes a deterministic, non-linear periodic relationship on a symmetric domain where a linear regression can yield $\\hat{\\beta}_1 \\approx 0$ and $R^2 \\approx 0$ despite perfect functional dependence.\n- Test case B probes a deterministic, non-linear convex relationship on a symmetric domain where the best linear fit is a constant, giving $\\hat{\\beta}_1 = 0$ and $R^2 = 0$.\n- Test case C is a correctly specified linear model that yields $\\hat{\\beta}_1 = 2$ and $R^2 = 1$, serving as a benchmark.", "solution": "We proceed by first principles. The objective is to find the parameters $(\\hat{\\beta}_0, \\hat{\\beta}_1)$ that minimize the sum of squared residuals, $S(\\beta_0, \\beta_1) = \\sum_{i=1}^{N}(y_i - \\beta_0 - \\beta_1 x_i)^2$. To find the minimum, we take the partial derivatives with respect to $\\beta_0$ and $\\beta_1$ and set them to zero. This gives the normal equations:\n$$\n\\frac{\\partial S}{\\partial \\beta_0} = -2 \\sum_{i=1}^{N}(y_i - \\beta_0 - \\beta_1 x_i) = 0 \\implies \\sum y_i - N\\beta_0 - \\beta_1 \\sum x_i = 0\n$$\n$$\n\\frac{\\partial S}{\\partial \\beta_1} = -2 \\sum_{i=1}^{N}x_i(y_i - \\beta_0 - \\beta_1 x_i) = 0 \\implies \\sum x_i y_i - \\beta_0 \\sum x_i - \\beta_1 \\sum x_i^2 = 0\n$$\nSolving this system of linear equations for $\\beta_0$ and $\\beta_1$ yields the OLS estimators, which we denote with a hat:\n$$\n\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^{N}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{N}(x_i - \\bar{x})^2} = \\frac{\\text{Cov}(x, y)}{\\text{Var}(x)}\n$$\n$$\n\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\n$$\nwhere $\\bar{x} = \\frac{1}{N}\\sum x_i$ and $\\bar{y} = \\frac{1}{N}\\sum y_i$.\n\nThe coefficient of determination, $R^2$, measures the proportion of the variance in the dependent variable that is predictable from the independent variable. It is defined as:\n$$\nR^2 = 1 - \\frac{\\text{SSR}}{\\text{TSS}} = 1 - \\frac{\\sum_{i=1}^{N}(y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{N}(y_i - \\bar{y})^2}\n$$\nwhere $\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i$ are the fitted values, SSR is the Sum of Squared Residuals, and TSS is the Total Sum of Squares.\n\nA crucial point for all three test cases is the generation of the independent variable $x$. The $N=1001$ points are evenly spaced on a symmetric interval of the form $[-L, L]$. Since $N$ is odd, the central point is $x_{(N-1)/2} = 0$, and for every point $x_i$ in the dataset, its negative, $-x_i$, is also present. This symmetry implies that the mean of $x$ is exactly zero: $\\bar{x} = 0$.\nThis simplifies the estimators considerably:\n$$\n\\hat{\\beta}_1 = \\frac{\\sum x_i y_i}{\\sum x_i^2}\n$$\n$$\n\\hat{\\beta}_0 = \\bar{y}\n$$\n\nWe now analyze each case using these simplified formulas.\n\n**Test Case A: Non-linear periodic, symmetric domain ($y_i = \\cos(x_i)$ on $[-\\pi, \\pi]$)**\nThe function $y = f(x) = \\cos(x)$ is an even function, meaning $f(-x) = f(x)$. The term in the numerator of $\\hat{\\beta}_1$ is the sum of $g(x_i) = x_i \\cos(x_i)$. The function $g(x)$ is the product of an odd function ($x$) and an even function ($\\cos(x)$), which results in an odd function, $g(-x) = (-x)\\cos(-x) = -x\\cos(x) = -g(x)$.\nThe sum of an odd function over a symmetric set of points centered at zero is exactly zero.\n$$\n\\sum_{i=1}^{N} x_i y_i = \\sum_{i=1}^{N} x_i \\cos(x_i) = 0\n$$\nTherefore, the slope coefficient is zero:\n$$\n\\hat{\\beta}_1 = \\frac{0}{\\sum x_i^2} = 0\n$$\nWith $\\hat{\\beta}_1 = 0$, the regression line is horizontal, $\\hat{y}_i = \\hat{\\beta}_0 = \\bar{y}$. The sum of squared residuals is $SSR = \\sum(y_i - \\bar{y})^2$, which is identical to the total sum of squares, TSS.\n$$\nR^2 = 1 - \\frac{SSR}{TSS} = 1 - \\frac{TSS}{TSS} = 0\n$$\nDespite a perfect deterministic relationship between $x$ and $y$, the linear regression reports no explanatory power whatsoever.\n\n**Test Case B: Non-linear convex, symmetric domain ($y_i = x_i^2$ on $[-2, 2]$)**\nThe function $y = f(x) = x^2$ is also an even function. The analysis is identical to Case A. The product $g(x_i) = x_i y_i = x_i \\cdot x_i^2 = x_i^3$ is an odd function. Summing this function over the symmetric set of points gives zero.\n$$\n\\sum_{i=1}^{N} x_i y_i = \\sum_{i=1}^{N} x_i^3 = 0\n$$\nAgain, this leads to $\\hat{\\beta}_1 = 0$ and consequently $R^2 = 0$. The OLS regression is completely blind to the perfect quadratic relationship. This failure is fundamental and highlights that zero covariance does not imply independence, a mistake often made by beginners. In finance, for example, simple linear models can miss non-linear dependencies such as volatility clustering.\n\n**Test Case C: Correctly specified linear benchmark ($y_i = 3 + 2x_i$ on $[-5, 5]$)**\nThis case serves as a control. The data is generated from a perfect linear model with true parameters $\\beta_0=3$ and $\\beta_1=2$. We expect OLS to recover these parameters exactly.\nUsing the simplified formula for $\\hat{\\beta}_1$:\n$$\n\\hat{\\beta}_1 = \\frac{\\sum x_i y_i}{\\sum x_i^2} = \\frac{\\sum x_i (3 + 2x_i)}{\\sum x_i^2} = \\frac{3\\sum x_i + 2\\sum x_i^2}{\\sum x_i^2}\n$$\nSince $\\sum x_i = N\\bar{x} = 0$, the expression simplifies to:\n$$\n\\hat{\\beta}_1 = \\frac{2\\sum x_i^2}{\\sum x_i^2} = 2\n$$\nThe intercept is $\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x} = \\bar{y} - 2 \\cdot 0 = \\bar{y}$. The mean of $y$ is $\\bar{y} = \\frac{1}{N}\\sum(3+2x_i) = \\frac{1}{N}(3N + 2\\sum x_i) = 3$. So, $\\hat{\\beta}_0 = 3$.\nThe fitted model is $\\hat{y}_i = 3 + 2x_i$, which is identical to the true model. The residuals are $\\hat{\\varepsilon}_i = y_i - \\hat{y}_i = 0$ for all $i$. Thus, $SSR = 0$.\nThe Total Sum of Squares, $TSS = \\sum(y_i - \\bar{y})^2 = \\sum((3+2x_i)-3)^2 = \\sum(2x_i)^2 = 4\\sum x_i^2$. Since not all $x_i$ are zero, $TSS > 0$.\n$$\nR^2 = 1 - \\frac{0}{TSS} = 1\n$$\nAs expected, for a correctly specified linear model without noise, OLS provides a perfect fit.\n\nThe implementation will translate these analytical formulas into code. It will construct the datasets, compute the estimators and $R^2$ for each case, and format the results as specified. Small deviations from the exact theoretical values of $0$, $1$, and $2$ may occur due to floating-point arithmetic, but they will be insignificant at the required precision.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes OLS coefficients for three deterministic datasets to demonstrate\n    the limitations of linear regression for non-linear relationships.\n    \"\"\"\n\n    # Define the problem parameters for each test case.\n    # Each tuple contains: (N, x_min, x_max, y_function, description)\n    test_cases = [\n        (1001, -np.pi, np.pi, lambda x: np.cos(x), \"A\"),\n        (1001, -2.0, 2.0, lambda x: x**2, \"B\"),\n        (1001, -5.0, 5.0, lambda x: 3.0 + 2.0 * x, \"C\"),\n    ]\n\n    results = []\n    for case in test_cases:\n        N, x_min, x_max, y_func, _ = case\n\n        # Generate the deterministic dataset\n        x = np.linspace(x_min, x_max, N)\n        y = y_func(x)\n\n        # Compute means\n        x_mean = np.mean(x)\n        y_mean = np.mean(y)\n\n        # Compute OLS estimator for the slope (beta_1)\n        # beta_1 = Cov(x, y) / Var(x)\n        numerator = np.sum((x - x_mean) * (y - y_mean))\n        denominator = np.sum((x - x_mean)**2)\n        \n        # Avoid division by zero, though not expected here\n        if denominator == 0:\n            beta_1 = 0.0\n        else:\n            beta_1 = numerator / denominator\n\n        # Compute OLS estimator for the intercept (beta_0)\n        beta_0 = y_mean - beta_1 * x_mean\n\n        # Compute the fitted values\n        y_pred = beta_0 + beta_1 * x\n\n        # Compute Sum of Squared Residuals (SSR) and Total Sum of Squares (TSS)\n        ssr = np.sum((y - y_pred)**2)\n        tss = np.sum((y - y_mean)**2)\n\n        # Compute the coefficient of determination (R^2)\n        # If TSS is zero, all y values are the same.\n        # R^2 is undefined, but can be treated as 1 if the model is perfect (ssr=0)\n        # or 0 if it's not (ssr>0). Here, TSS will not be zero.\n        if tss == 0:\n            r_squared = 1.0 if ssr == 0 else 0.0\n        else:\n            r_squared = 1.0 - (ssr / tss)\n\n        results.append([beta_1, r_squared])\n\n    # Format the final output string as per problem specification.\n    # Example: [[b1_A,R2_A],[b1_B,R2_B],[b1_C,R2_C]]\n    # Numbers are rounded to 6 decimal places.\n    sublist_strs = []\n    for res_pair in results:\n        # Using f-string for precise decimal formatting\n        s = f\"[{res_pair[0]:.6f},{res_pair[1]:.6f}]\"\n        sublist_strs.append(s)\n    \n    final_output = f\"[{','.join(sublist_strs)}]\"\n\n    print(final_output)\n\nsolve()\n\n```", "id": "2417149"}, {"introduction": "Perhaps the most critical assumption for causal inference using OLS is that our explanatory variables are uncorrelated with the error term. This exercise uses simulation to demonstrate a classic violation of this assumption: omitted variable bias [@problem_id:2417165]. You will create a world where an unobserved factor like 'ability' influences both 'education' and 'wages', and then see how a regression that omits this factor produces biased estimates of the return to education. This hands-on simulation provides a clear and powerful illustration of why correlation does not equal causation.", "problem": "Consider a data generating process for individual wages in which an econometrician regresses wages on years of education, but an unobserved ability affects both education and wages. Let the true structural model be\n$$\nw_i = \\beta_0 + \\beta_1 \\cdot \\text{edu}_i + \\beta_a \\cdot a_i + u_i,\n$$\nwhere $w_i$ is the wage of individual $i$, $\\text{edu}_i$ is the years of education of individual $i$, $a_i$ is an unobserved ability, and $u_i$ is an idiosyncratic error. Suppose education is determined by\n$$\n\\text{edu}_i = e_0 + \\phi \\cdot a_i + v_i.\n$$\nAssume $a_i \\sim \\mathcal{N}(0,\\sigma_a^2)$, $v_i \\sim \\mathcal{N}(0,\\sigma_v^2)$, and $u_i \\sim \\mathcal{N}(0,\\sigma_u^2)$, with $a_i$, $v_i$, and $u_i$ mutually independent. The econometrician estimates the regression of $w_i$ on $\\text{edu}_i$ with an intercept, omitting $a_i$.\n\nYour task is to write a complete, runnable program that, for a given test suite of parameter values, simulates data according to the process above, estimates the coefficient on $\\text{edu}_i$ from the regression of $w_i$ on $\\text{edu}_i$ with an intercept for each test case, and reports for each case: the estimated coefficient $\\hat{\\beta}_1$, the bias $\\hat{\\beta}_1 - \\beta_1$, and whether the estimate strictly exceeds the true $\\beta_1$.\n\nRandomness and reproducibility: For test case index $i \\in \\{1,2,3,4,5\\}$, use the pseudo-random seed $s_i = s_0 + i$ with base $s_0 = 12345$. Use the specified Normal distributions exactly as stated. All quantities are dimensionless and require no physical units.\n\nTest suite of parameter values to be used:\n- Case $1$ (positive correlation between ability and education; overestimation expected): $N = 100000$, $\\beta_0 = 0$, $\\beta_1 = 0.08$, $\\beta_a = 0.5$, $e_0 = 12$, $\\phi = 0.8$, $\\sigma_a = 1.0$, $\\sigma_v = 1.0$, $\\sigma_u = 1.0$.\n- Case $2$ (no correlation between ability and education; no omitted variable bias in probability limit): $N = 100000$, $\\beta_0 = 0$, $\\beta_1 = 0.08$, $\\beta_a = 0.5$, $e_0 = 12$, $\\phi = 0.0$, $\\sigma_a = 1.0$, $\\sigma_v = 1.0$, $\\sigma_u = 1.0$.\n- Case $3$ (negative correlation between ability and education; underestimation expected): $N = 100000$, $\\beta_0 = 0$, $\\beta_1 = 0.08$, $\\beta_a = 0.5$, $e_0 = 12$, $\\phi = -0.8$, $\\sigma_a = 1.0$, $\\sigma_v = 1.0$, $\\sigma_u = 1.0$.\n- Case $4$ (zero true return to education; spurious positive estimate expected): $N = 100000$, $\\beta_0 = 0$, $\\beta_1 = 0.0$, $\\beta_a = 0.5$, $e_0 = 12$, $\\phi = 0.8$, $\\sigma_a = 1.0$, $\\sigma_v = 1.0$, $\\sigma_u = 1.0$.\n- Case $5$ (boundary where ability has zero variance; omitted variable bias vanishes): $N = 100000$, $\\beta_0 = 0$, $\\beta_1 = 0.08$, $\\beta_a = 0.5$, $e_0 = 12$, $\\phi = 0.8$, $\\sigma_a = 0.0$, $\\sigma_v = 1.0$, $\\sigma_u = 1.0$.\n\nRequired final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each case in the order $1$ through $5$, append, in sequence, the estimated coefficient $\\hat{\\beta}_1$ (a float), the bias $\\hat{\\beta}_1 - \\beta_1$ (a float), and an indicator (a boolean) of whether $\\hat{\\beta}_1$ strictly exceeds $\\beta_1$. Thus the output will be a flat list of length $15$ in the order $[\\hat{\\beta}_1^{(1)}, \\text{bias}^{(1)}, \\text{over}^{(1)}, \\ldots, \\hat{\\beta}_1^{(5)}, \\text{bias}^{(5)}, \\text{over}^{(5)}]$.", "solution": "**Solution Design**\n\nThe core of this problem is the bias that arises in an Ordinary Least Squares (OLS) estimator when a relevant variable is omitted from the regression model, and this omitted variable is correlated with an included regressor.\n\nLet the true model be\n$$w_i = \\beta_0 + \\beta_1 \\cdot \\text{edu}_i + \\beta_a \\cdot a_i + u_i$$\nThe misspecified model estimated by the econometrician is\n$$w_i = \\gamma_0 + \\gamma_1 \\cdot \\text{edu}_i + \\epsilon_i$$\nThe term $\\epsilon_i$ in the estimated model is a composite error term that includes the omitted variable and the original idiosyncratic error: $\\epsilon_i = \\beta_a \\cdot a_i + u_i$.\n\nThe OLS estimator for $\\gamma_1$, which we denote $\\hat{\\beta}_1$, is given by\n$$\\hat{\\beta}_1 = \\frac{\\text{Cov}(\\text{edu}, w)}{\\text{Var}(\\text{edu})}$$\nTo understand the properties of this estimator, we examine its probability limit:\n$$\\text{plim}(\\hat{\\beta}_1) = \\frac{\\text{Cov}(\\text{edu}_i, w_i)}{\\text{Var}(\\text{edu}_i)}$$\nSubstituting the true structural equation for $w_i$:\n$$\\text{plim}(\\hat{\\beta}_1) = \\frac{\\text{Cov}(\\text{edu}_i, \\beta_0 + \\beta_1 \\cdot \\text{edu}_i + \\beta_a \\cdot a_i + u_i)}{\\text{Var}(\\text{edu}_i)}$$\nBy the linearity of covariance and the fact that $\\text{Cov}(\\text{edu}_i, \\beta_0) = 0$:\n$$\\text{plim}(\\hat{\\beta}_1) = \\frac{\\beta_1 \\cdot \\text{Cov}(\\text{edu}_i, \\text{edu}_i) + \\beta_a \\cdot \\text{Cov}(\\text{edu}_i, a_i) + \\text{Cov}(\\text{edu}_i, u_i)}{\\text{Var}(\\text{edu}_i)}$$\nGiven the structural model for education, $\\text{edu}_i = e_0 + \\phi \\cdot a_i + v_i$, and the mutual independence of $a_i, v_i, u_i$, we have $\\text{Cov}(\\text{edu}_i, u_i) = \\text{Cov}(e_0 + \\phi \\cdot a_i + v_i, u_i) = 0$.\nThe expression simplifies to:\n$$\\text{plim}(\\hat{\\beta}_1) = \\beta_1 + \\beta_a \\frac{\\text{Cov}(\\text{edu}_i, a_i)}{\\text{Var}(\\text{edu}_i)}$$\nThe term $\\beta_a \\frac{\\text{Cov}(\\text{edu}_i, a_i)}{\\text{Var}(\\text{edu}_i)}$ represents the asymptotic omitted variable bias. We can derive its components from the given assumptions:\n$$\\text{Cov}(\\text{edu}_i, a_i) = \\text{Cov}(e_0 + \\phi \\cdot a_i + v_i, a_i) = \\phi \\cdot \\text{Var}(a_i) = \\phi \\sigma_a^2$$\n$$\\text{Var}(\\text{edu}_i) = \\text{Var}(e_0 + \\phi \\cdot a_i + v_i) = \\phi^2 \\cdot \\text{Var}(a_i) + \\text{Var}(v_i) = \\phi^2 \\sigma_a^2 + \\sigma_v^2$$\nThus, the asymptotic bias is:\n$$\\text{Bias} = \\beta_a \\frac{\\phi \\sigma_a^2}{\\phi^2 \\sigma_a^2 + \\sigma_v^2}$$\nThe estimator $\\hat{\\beta}_1$ is consistent (i.e., the bias is zero) if and only if $\\beta_a = 0$ or $\\phi = 0$ or $\\sigma_a^2 = 0$. This means the omitted variable either does not affect the dependent variable ($w_i$), or it is uncorrelated with the included regressor ($\\text{edu}_i$), or it has no variance at all.\n\nThe simulation will proceed as follows for each test case:\n$1$. Set the pseudo-random seed to $s_0 + i$ to ensure reproducibility.\n$2$. Generate $N$ draws for the random components $a_i$, $v_i$, and $u_i$ from their specified normal distributions.\n$3$. Construct the observed data vectors for $\\text{edu}$ and $w$ according to their structural equations using the generated random components and the case-specific parameters.\n$4$. Estimate the coefficients of the misspecified model by regressing $w$ on $\\text{edu}$ and an intercept. This is achieved using the standard OLS matrix formula $\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{X}^{\\top}\\mathbf{y}$, where $\\mathbf{y}$ is the vector of $w_i$ values and $\\mathbf{X}$ is the $N \\times 2$ design matrix with a column of ones and a column of $\\text{edu}_i$ values. The estimated coefficient $\\hat{\\beta}_1$ is the second element of the vector $\\hat{\\boldsymbol{\\beta}}$.\n$5$. Calculate the bias as $\\hat{\\beta}_1 - \\beta_1$ and determine if $\\hat{\\beta}_1 > \\beta_1$.\n$6$. Store the three resulting values ($\\hat{\\beta}_1$, bias, over-estimation indicator) for final reporting.\n\nThis procedure will be repeated for all five test cases, demonstrating the theoretical effects of omitted variable bias under different conditions. The large sample size $N=100000$ ensures that the simulated finite-sample estimates will be very close to their theoretical probability limits.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Simulates data to demonstrate omitted variable bias in OLS regression,\n    estimates coefficients for several parameter sets, and reports the results.\n    \"\"\"\n    # Base seed for pseudo-random number generation.\n    base_seed = 12345\n\n    # Test suite of parameter values.\n    # Each tuple contains:\n    # (N, beta_0, beta_1, beta_a, e_0, phi, sigma_a, sigma_v, sigma_u)\n    test_cases = [\n        (100000, 0, 0.08, 0.5, 12, 0.8, 1.0, 1.0, 1.0),\n        (100000, 0, 0.08, 0.5, 12, 0.0, 1.0, 1.0, 1.0),\n        (100000, 0, 0.08, 0.5, 12, -0.8, 1.0, 1.0, 1.0),\n        (100000, 0, 0.0, 0.5, 12, 0.8, 1.0, 1.0, 1.0),\n        (100000, 0, 0.08, 0.5, 12, 0.8, 0.0, 1.0, 1.0),\n    ]\n\n    results = []\n\n    for i, case in enumerate(test_cases, 1):\n        # Unpack parameters for the current case.\n        N, beta_0, beta_1, beta_a, e_0, phi, sigma_a, sigma_v, sigma_u = case\n        \n        # Set the seed for reproducibility for the current test case.\n        seed = base_seed + i\n        rng = np.random.default_rng(seed)\n\n        # 1. Generate the underlying random variables from their distributions.\n        #    a_i ~ N(0, sigma_a^2)\n        #    v_i ~ N(0, sigma_v^2)\n        #    u_i ~ N(0, sigma_u^2)\n        a = rng.normal(loc=0.0, scale=sigma_a, size=N)\n        v = rng.normal(loc=0.0, scale=sigma_v, size=N)\n        u = rng.normal(loc=0.0, scale=sigma_u, size=N)\n\n        # 2. Construct the variables 'edu' and 'wage' based on the structural model.\n        #    edu_i = e_0 + phi * a_i + v_i\n        edu = e_0 + phi * a + v\n        \n        #    w_i = beta_0 + beta_1 * edu_i + beta_a * a_i + u_i\n        wage = beta_0 + beta_1 * edu + beta_a * a + u\n\n        # 3. Perform OLS regression of wage on edu and an intercept.\n        #    We estimate the model: w_i = gamma_0 + gamma_1 * edu_i + error\n        #    The OLS solution is beta_hat = (X'X)^-1 * X'y\n        \n        # Construct the design matrix X with a column of ones for the intercept.\n        X = np.vstack([np.ones(N), edu]).T\n        \n        # Calculate OLS coefficients.\n        try:\n            # Using the standard matrix formula for OLS coefficients.\n            # beta_hat = [gamma_0_hat, gamma_1_hat]\n            XtX = X.T @ X\n            XtY = X.T @ wage\n            beta_hat_vector = np.linalg.inv(XtX) @ XtY\n        except np.linalg.LinAlgError:\n            # In case of singular matrix, use pseudo-inverse\n            beta_hat_vector = np.linalg.pinv(X) @ wage\n\n        estimated_beta_1 = beta_hat_vector[1]\n\n        # 4. Calculate the bias and the overestimation indicator.\n        bias = estimated_beta_1 - beta_1\n        is_over = estimated_beta_1 > beta_1\n\n        # 5. Append results for this case to the master list.\n        results.extend([estimated_beta_1, bias, is_over])\n\n    # Final print statement in the exact required format.\n    # The list is flattened and elements are converted to strings.\n    formatted_results = [f\"{x:.8f}\" if isinstance(x, float) else str(x) for x in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2417165"}, {"introduction": "While OLS estimators remain unbiased in the presence of heteroscedasticity, our statistical inference can be severely misleading. This practice demonstrates the consequences of non-constant error variance and introduces you to the essential tools used in modern econometrics to correct for it [@problem_id:2417150]. By generating data with heteroscedasticity, you will compare the incorrect classical standard errors with their robust counterparts, including White's heteroskedasticity-consistent estimators and those derived from bootstrapping, gaining practical skill in ensuring the validity of your statistical tests.", "problem": "Consider the simple linear regression model with a single regressor and an intercept in a computational economics and finance setting. The response variable $y_i$ is generated from the data-generating process\n$$\ny_i = \\beta_0 + \\beta_1 x_i + u_i,\n$$\nwhere $x_i \\sim \\mathcal{N}(0,1)$ independently and identically distributed across $i$, and the disturbance $u_i$ exhibits heteroskedasticity according to\n$$\nu_i = \\sigma_0 \\sqrt{1 + \\gamma x_i^2}\\,\\varepsilon_i,\\quad \\varepsilon_i \\sim \\mathcal{N}(0,1) \\text{ independently of } x_i.\n$$\nAssume the coefficient of interest is the slope $\\beta_1$. For each test case defined below, generate a sample $\\{(y_i,x_i)\\}_{i=1}^n$ using the specified parameters, and compute three estimates of the standard error of the ordinary least squares (OLS) estimator of $\\beta_1$:\n- the classical homoskedasticity-based OLS standard error,\n- the heteroskedasticity-consistent (HC$0$) standard error (also known as the White-robust standard error),\n- a bootstrap standard error based on nonparametric resampling of observation pairs $(y_i,x_i)$ with replacement using $B$ bootstrap replications.\n\nUse the same fixed random number generator seed $s$ for all random draws within each test case to ensure reproducibility. For all computations, include an intercept in the regression. Express each reported standard error as a real number rounded to six decimal places.\n\nTest suite. Use the following parameter sets; each bullet describes one test case:\n- Case $1$: $n=500$, $\\beta_0=0.7$, $\\beta_1=1.5$, $\\sigma_0=1.0$, $\\gamma=2.0$, $B=400$, $s=17$.\n- Case $2$: $n=500$, $\\beta_0=0.7$, $\\beta_1=1.5$, $\\sigma_0=1.0$, $\\gamma=0.0$, $B=400$, $s=23$.\n- Case $3$: $n=50$, $\\beta_0=0.7$, $\\beta_1=1.5$, $\\sigma_0=1.0$, $\\gamma=2.0$, $B=400$, $s=123$.\n- Case $4$: $n=500$, $\\beta_0=0.7$, $\\beta_1=1.5$, $\\sigma_0=1.0$, $\\gamma=10.0$, $B=400$, $s=2023$.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list of lists enclosed in square brackets. Each inner list corresponds to one test case, in the same order as above, and contains three real numbers in the order [OLS standard error, White-robust standard error, bootstrap standard error], each rounded to six decimal places. For example, the output must have the form\n$[[a_{1,1},a_{1,2},a_{1,3}],[a_{2,1},a_{2,2},a_{2,3}],[a_{3,1},a_{3,2},a_{3,3}],[a_{4,1},a_{4,2},a_{4,3}]]$\nwith no spaces.", "solution": "The model is a simple linear regression:\n$$\ny_i = \\beta_0 + \\beta_1 x_i + u_i\n$$\nfor $i = 1, \\dots, n$. In matrix notation, this is $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{u}$, where $\\mathbf{y}$ is the $n \\times 1$ vector of observations of the response variable, $\\mathbf{X}$ is the $n \\times 2$ design matrix with the first column being a vector of ones and the second column being the observations of the regressor $x_i$, $\\boldsymbol{\\beta} = [\\beta_0, \\beta_1]^\\top$ is the $2 \\times 1$ vector of coefficients, and $\\mathbf{u}$ is the $n \\times 1$ vector of disturbances.\n\nThe OLS estimator for $\\boldsymbol{\\beta}$ is given by:\n$$\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{X}^\\top\\mathbf{y}\n$$\nThe variance-covariance matrix of this estimator, conditional on $\\mathbf{X}$, is:\n$$\n\\text{Var}(\\hat{\\boldsymbol{\\beta}} | \\mathbf{X}) = (\\mathbf{X}^\\top\\mathbf{X})^{-1} \\mathbf{X}^\\top \\boldsymbol{\\Omega} \\mathbf{X} (\\mathbf{X}^\\top\\mathbf{X})^{-1}\n$$\nwhere $\\boldsymbol{\\Omega} = \\text{E}[\\mathbf{u}\\mathbf{u}^\\top | \\mathbf{X}]$. In this problem, the disturbances $u_i$ are independent across observations, so $\\boldsymbol{\\Omega}$ is a diagonal matrix. The heteroskedastic structure $u_i = \\sigma_0 \\sqrt{1 + \\gamma x_i^2}\\,\\varepsilon_i$ with $\\varepsilon_i \\sim \\mathcal{N}(0,1)$ implies that the diagonal elements of $\\boldsymbol{\\Omega}$ are $\\text{Var}(u_i|x_i) = \\sigma_i^2 = \\sigma_0^2(1 + \\gamma x_i^2)$.\n\nThe task is to estimate the standard error of $\\hat{\\beta}_1$, the OLS estimator for the slope coefficient, which is the square root of the second diagonal element of $\\text{Var}(\\hat{\\boldsymbol{\\beta}} | \\mathbf{X})$. We will compute three different estimates for this quantity.\n\n1.  **Classical Homoskedasticity-Based OLS Standard Error**\n\nThis estimator incorrectly assumes that the disturbances are homoskedastic, meaning $\\text{Var}(u_i|x_i) = \\sigma^2$ for all $i$. Under this assumption, $\\boldsymbol{\\Omega} = \\sigma^2\\mathbf{I}_n$, and the variance-covariance matrix simplifies to:\n$$\n\\text{Var}(\\hat{\\boldsymbol{\\beta}} | \\mathbf{X}) = \\sigma^2(\\mathbf{X}^\\top\\mathbf{X})^{-1}\n$$\nThe unknown error variance $\\sigma^2$ is estimated using the OLS residuals, $\\hat{u}_i = y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)$. The unbiased estimator for $\\sigma^2$ is:\n$$\n\\hat{\\sigma}^2 = \\frac{1}{n-k} \\sum_{i=1}^{n} \\hat{u}_i^2\n$$\nwhere $k$ is the number of regressors, which is $2$ in this case (intercept and slope). The estimated classical variance-covariance matrix is $\\widehat{\\text{Var}}_{OLS}(\\hat{\\boldsymbol{\\beta}}) = \\hat{\\sigma}^2(\\mathbf{X}^\\top\\mathbf{X})^{-1}$. The standard error for $\\hat{\\beta}_1$ is the square root of the second diagonal element of this matrix:\n$$\n\\text{SE}_{OLS}(\\hat{\\beta}_1) = \\sqrt{[\\widehat{\\text{Var}}_{OLS}(\\hat{\\boldsymbol{\\beta}})]_{2,2}}\n$$\n\n2.  **Heteroskedasticity-Consistent (HC0) Standard Error**\n\nThis estimator, also known as the White or robust standard error, does not assume homoskedasticity. It provides a consistent estimate of the variance-covariance matrix even when $\\boldsymbol{\\Omega}$ is not a multiple of the identity matrix. It is based on the \"sandwich\" formula for the variance. The middle part of the sandwich, $\\mathbf{S} = \\mathbf{X}^\\top \\boldsymbol{\\Omega} \\mathbf{X}$, is estimated by replacing the unknown variances $\\sigma_i^2$ with the squared OLS residuals $\\hat{u}_i^2$. This gives the HC$0$ estimator for $\\mathbf{S}$:\n$$\n\\hat{\\mathbf{S}}_0 = \\sum_{i=1}^{n} \\hat{u}_i^2 \\mathbf{x}_i \\mathbf{x}_i^\\top\n$$\nwhere $\\mathbf{x}_i = [1, x_i]^\\top$ is the $i$-th row of the design matrix $\\mathbf{X}$. The HC$0$ variance-covariance estimator is then:\n$$\n\\widehat{\\text{Var}}_{HC0}(\\hat{\\boldsymbol{\\beta}}) = (\\mathbf{X}^\\top\\mathbf{X})^{-1} \\hat{\\mathbf{S}}_0 (\\mathbf{X}^\\top\\mathbf{X})^{-1}\n$$\nThe corresponding standard error for $\\hat{\\beta}_1$ is:\n$$\n\\text{SE}_{HC0}(\\hat{\\beta}_1) = \\sqrt{[\\widehat{\\text{Var}}_{HC0}(\\hat{\\boldsymbol{\\beta}})]_{2,2}}\n$$\nWhen $\\gamma > 0$, the errors are truly heteroskedastic, and this estimator is expected to be more accurate than the classical one, especially in large samples. When $\\gamma=0.0$, the errors are homoskedastic, and both estimators should be similar.\n\n3.  **Bootstrap Standard Error**\n\nThe bootstrap provides a nonparametric method for estimating the standard error. We use the \"pairs bootstrap,\" which is appropriate in the presence of heteroskedasticity because it resamples the pairs $(x_i, y_i)$ together, thus preserving the unknown relationship between the regressor and the variance of the error term. The algorithm is as follows:\n    1.  From the original sample $\\{(y_i, x_i)\\}_{i=1}^n$, draw a \"bootstrap sample\" of size $n$ by sampling pairs with replacement.\n    2.  Using this bootstrap sample, compute the OLS estimate of the slope coefficient, denoted $\\hat{\\beta}_{1,b}^*$.\n    3.  Repeat steps $1$ and $2$ for a large number of replications, $B$. This yields a distribution of bootstrap estimates $\\{\\hat{\\beta}_{1,1}^*, \\dots, \\hat{\\beta}_{1,B}^*\\}$.\n    4.  The bootstrap standard error is the sample standard deviation of these $B$ estimates:\n        $$\n        \\text{SE}_{Boot}(\\hat{\\beta}_1) = \\sqrt{\\frac{1}{B-1} \\sum_{b=1}^{B} (\\hat{\\beta}_{1,b}^* - \\bar{\\beta}_1^*)^2}\n        $$\n        where $\\bar{\\beta}_1^* = \\frac{1}{B} \\sum_{b=1}^{B} \\hat{\\beta}_{1,b}^*$. This procedure is computationally intensive but provides a reliable estimate of the sampling variability of $\\hat{\\beta}_1$.\n\nThe implementation will proceed by first generating the data for each test case according to the specified parameters ($n, \\beta_0, \\beta_1, \\sigma_0, \\gamma$) and the random seed $s$. Then, for each generated dataset, the OLS estimates will be computed, followed by the calculation of the three specified standard errors. The results will be rounded and formatted as required.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    It generates data, computes OLS and three types of standard errors,\n    and prints the results in the required format.\n    \"\"\"\n\n    test_cases = [\n        # Case 1: n=500, β0=0.7, β1=1.5, σ0=1.0, γ=2.0, B=400, s=17\n        (500, 0.7, 1.5, 1.0, 2.0, 400, 17),\n        # Case 2: n=500, β0=0.7, β1=1.5, σ0=1.0, γ=0.0, B=400, s=23\n        (500, 0.7, 1.5, 1.0, 0.0, 400, 23),\n        # Case 3: n=50, β0=0.7, β1=1.5, σ0=1.0, γ=2.0, B=400, s=123\n        (50, 0.7, 1.5, 1.0, 2.0, 400, 123),\n        # Case 4: n=500, β0=0.7, β1=1.5, σ0=1.0, γ=10.0, B=400, s=2023\n        (500, 0.7, 1.5, 1.0, 10.0, 400, 2023),\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        n, beta0, beta1, sigma0, gamma, B, s = case\n        \n        # Initialize random number generator for reproducibility\n        rng = np.random.default_rng(s)\n\n        # 1. Data Generation\n        x = rng.normal(0, 1, size=n)\n        epsilon = rng.normal(0, 1, size=n)\n        u = sigma0 * np.sqrt(1 + gamma * x**2) * epsilon\n        y = beta0 + beta1 * x + u\n\n        # 2. OLS Estimation\n        X = np.vstack((np.ones(n), x)).T  # Design matrix [1, x_i]\n        \n        try:\n            # Pre-compute (X'X)^-1\n            inv_XTX = np.linalg.inv(X.T @ X)\n            beta_hat = inv_XTX @ X.T @ y\n        except np.linalg.LinAlgError:\n            # Handle rare case of singular matrix\n            all_results.append([np.nan, np.nan, np.nan])\n            continue\n            \n        residuals = y - X @ beta_hat\n        k = X.shape[1] # Number of regressors (intercept + slope)\n\n        # 3. Compute Standard Errors\n\n        # 3.1. Classical (Homoskedastic) OLS Standard Error\n        sigma2_hat = np.sum(residuals**2) / (n - k)\n        var_cov_ols = sigma2_hat * inv_XTX\n        se_ols = np.sqrt(var_cov_ols[1, 1])\n\n        # 3.2. HC0 (White-Robust) Standard Error\n        # S_0 = sum(u_i^2 * x_i * x_i')\n        S0 = X.T @ np.diag(residuals**2) @ X\n        var_cov_hc0 = inv_XTX @ S0 @ inv_XTX\n        se_hc0 = np.sqrt(var_cov_hc0[1, 1])\n\n        # 3.3. Bootstrap Standard Error (Pairs Bootstrap)\n        bootstrap_betas = np.zeros(B)\n        for b in range(B):\n            # Resample pairs (y_i, x_i) with replacement\n            indices = rng.choice(n, size=n, replace=True)\n            y_star = y[indices]\n            X_star = X[indices]\n            \n            try:\n                # OLS on bootstrap sample\n                inv_XTX_star = np.linalg.inv(X_star.T @ X_star)\n                beta_hat_star = inv_XTX_star @ X_star.T @ y_star\n                bootstrap_betas[b] = beta_hat_star[1]\n            except np.linalg.LinAlgError:\n                bootstrap_betas[b] = np.nan\n        \n        # Standard deviation of bootstrap estimates\n        se_boot = np.nanstd(bootstrap_betas, ddof=1)\n\n        # 4. Store rounded results\n        case_results = [\n            round(se_ols, 6),\n            round(se_hc0, 6),\n            round(se_boot, 6)\n        ]\n        all_results.append(case_results)\n\n    # Format output string to be a list of lists with no spaces\n    formatted_results = [repr(r).replace(' ', '') for r in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2417150"}]}