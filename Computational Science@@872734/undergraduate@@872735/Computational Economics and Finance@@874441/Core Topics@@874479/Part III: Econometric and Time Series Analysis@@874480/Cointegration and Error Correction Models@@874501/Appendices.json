{"hands_on_practices": [{"introduction": "The Engle-Granger two-step method is the foundational procedure for testing for cointegration. This hands-on exercise guides you through its core mechanics, demonstrating how it can distinguish between a genuine long-run equilibrium and a \"spurious\" regression where two unrelated non-stationary series appear correlated simply because they share a common trend. By simulating data under both scenarios, you will gain a concrete understanding of how the stationarity of the regression residuals serves as the key to unlocking the puzzle of cointegration [@problem_id:2380057].", "problem": "You are given a decision problem formulated in purely mathematical terms about long-run equilibrium relations between two stochastic processes representing a firm's quarterly research and development expenditure and its subsequent quarterly revenues with a one-quarter lag. For each parameter set in the provided test suite, construct two sequences $\\{x_t\\}_{t=0}^{T}$ and $\\{y_t\\}_{t=1}^{T}$ according to the following definitions.\n\n1. Data-generating components and notation.\n   - Let $T \\in \\mathbb{N}$ denote the sample size in quarters.\n   - Let $x_0 = 0$. For $t \\in \\{1,2,\\dots,T\\}$, let\n     $$x_t = x_{t-1} + \\varepsilon_t,$$\n     where $\\{\\varepsilon_t\\}_{t=1}^{T}$ are independent and identically distributed Gaussian random variables with mean $0$ and variance $\\sigma_{\\varepsilon}^2$.\n   - Define the lagged input used in the revenue relation as $\\{x_{t-1}\\}_{t=1}^{T}$.\n   - Two alternative specifications for the disturbance in the revenue relation are allowed, parameterized by a type indicator $\\text{residual\\_type} \\in \\{\\text{stationary}, \\text{random\\_walk}\\}$:\n     - If $\\text{residual\\_type} = \\text{stationary}$, define $v_0 = 0$ and for $t \\in \\{1,2,\\dots,T\\}$,\n       $$v_t = \\phi\\, v_{t-1} + \\eta_t,$$\n       where $|\\phi|  1$ and $\\{\\eta_t\\}_{t=1}^{T}$ are independent and identically distributed Gaussian random variables with mean $0$ and variance $\\sigma_{\\eta}^2$. In this case, set\n       $$y_t = c + \\beta\\, x_{t-1} + v_t.$$\n     - If $\\text{residual\\_type} = \\text{random\\_walk}$, define $s_0 = 0$ and for $t \\in \\{1,2,\\dots,T\\}$,\n       $$s_t = s_{t-1} + \\eta_t,$$\n       where $\\{\\eta_t\\}_{t=1}^{T}$ are independent and identically distributed Gaussian random variables with mean $0$ and variance $\\sigma_{\\eta}^2$. In this case, set\n       $$y_t = c + \\beta\\, x_{t-1} + s_t.$$\n\n2. Decision rule to be applied for each parameter set.\n   - Consider the static long-run relation\n     $$y_t = \\alpha + \\beta^{\\ast} x_{t-1} + u_t,$$\n     and define $\\{\\hat{u}_t\\}_{t=1}^{T}$ as the residuals from the Ordinary Least Squares (OLS) regression of $\\{y_t\\}_{t=1}^{T}$ on a constant and $\\{x_{t-1}\\}_{t=1}^{T}$.\n   - Compute the Augmented Dickey-Fuller (ADF) regression without deterministic terms and with one lag of the first difference:\n     $$\\Delta \\hat{u}_t = \\varphi\\, \\hat{u}_{t-1} + \\gamma\\, \\Delta \\hat{u}_{t-1} + \\varepsilon_t^{\\ast}, \\quad t = 3,4,\\dots,T.$$\n     Let $\\tau$ denote the $t$-statistic for the null hypothesis $H_0: \\varphi = 0$ against the one-sided alternative $H_1: \\varphi  0$, computed from this regression.\n   - Use the significance level $\\alpha = 0.05$ with the Engle-Granger residual-based critical value $c_{0.05} = -3.34$ for the model with an intercept in the cointegrating regression and no deterministic terms in the ADF regression. Declare that $\\{y_t\\}$ and $\\{x_{t-1}\\}$ are cointegrated if and only if $\\tau \\le c_{0.05}$.\n\n3. Test suite. For each parameter set $\\theta$, generate the sequences using the above definitions with the given parameters and an independent pseudo-random number generator initialized at the specified seed. The parameters are:\n   - Case A: $\\left(T, \\text{seed}, c, \\beta, \\sigma_{\\varepsilon}, \\sigma_{\\eta}, \\phi, \\text{residual\\_type}\\right) = \\left(400, 101, 0.5, 1.3, 1.0, 0.3, 0.4, \\text{stationary}\\right)$.\n   - Case B: $\\left(T, \\text{seed}, c, \\beta, \\sigma_{\\varepsilon}, \\sigma_{\\eta}, \\phi, \\text{residual\\_type}\\right) = \\left(400, 202, 0.5, 1.3, 1.0, 0.8, 0.0, \\text{random\\_walk}\\right)$.\n   - Case C: $\\left(T, \\text{seed}, c, \\beta, \\sigma_{\\varepsilon}, \\sigma_{\\eta}, \\phi, \\text{residual\\_type}\\right) = \\left(400, 303, -0.2, -0.8, 0.8, 0.5, 0.7, \\text{stationary}\\right)$.\n\n4. Required final output format. Your program should produce a single line of output containing the results as a comma-separated list of booleans enclosed in square brackets, in the order of the test cases above, where each boolean is $\\text{True}$ if cointegration is declared under the rule in item $2$ and $\\text{False}$ otherwise. For example, an output with three cases should look like\n$$[\\text{True},\\text{False},\\text{True}].$$", "solution": "The problem statement presents a valid and well-posed computational exercise in time series econometrics. It requires the implementation of the Engle-Granger two-step procedure for testing cointegration between two simulated time series. The problem is scientifically grounded, formally specified, and all parameters and procedures required for a unique solution are provided for each test case. We shall proceed with the solution.\n\nThe core of the problem is to determine whether a long-run equilibrium relationship, known as cointegration, exists between two stochastic processes, $\\{x_t\\}$ and $\\{y_t\\}$. This is resolved by generating the series according to specified data-generating processes (DGPs) and then applying a statistical hypothesis test. The overall procedure is implemented for each parameter set provided in the test suite.\n\n**Step 1: Data Generation**\n\nFor each test case, we first simulate the time series data. A specific seed is used to initialize a pseudo-random number generator for reproducibility.\nLet $T$ be the sample size. We generate two sequences of independent and identically distributed (i.i.d.) Gaussian random shocks: $\\{\\varepsilon_t\\}_{t=1}^{T} \\sim N(0, \\sigma_{\\varepsilon}^2)$ and $\\{\\eta_t\\}_{t=1}^{T} \\sim N(0, \\sigma_{\\eta}^2)$.\n\nThe first series, $\\{x_t\\}_{t=0}^{T}$, representing RD expenditure, is constructed as a pure random walk:\n$$x_t = x_{t-1} + \\varepsilon_t, \\quad \\text{for } t=1, \\dots, T$$\nwith the initial condition $x_0 = 0$. A process like $\\{x_t\\}$ is non-stationary and is said to be integrated of order one, denoted $I(1)$, because its first difference, $\\Delta x_t = \\varepsilon_t$, is stationary.\n\nThe second series, $\\{y_t\\}_{t=1}^{T}$, representing revenues, is constructed based on a relationship with the lagged expenditure, $\\{x_{t-1}\\}_{t=1}^{T}$, and an error term whose nature depends on the `residual_type` parameter.\n\n- **Case 1: `residual_type` = `stationary`**. The error term $\\{v_t\\}_{t=1}^{T}$ follows a stationary first-order autoregressive (AR(1)) process:\n$$v_t = \\phi v_{t-1} + \\eta_t, \\quad \\text{for } t=1, \\dots, T$$\nwith $v_0 = 0$ and autoregressive parameter $|\\phi|  1$. The revenue series is then:\n$$y_t = c + \\beta x_{t-1} + v_t$$\nIn this case, the linear combination $y_t - \\beta x_{t-1} - c = v_t$ is stationary. By definition, if a linear combination of two $I(1)$ variables is stationary, the variables are cointegrated. Thus, we expect the test to detect cointegration.\n\n- **Case 2: `residual_type` = `random_walk`**. The error term $\\{s_t\\}_{t=1}^{T}$ is itself a random walk:\n$$s_t = s_{t-1} + \\eta_t, \\quad \\text{for } t=1, \\dots, T$$\nwith $s_0 = 0$. The revenue series is:\n$$y_t = c + \\beta x_{t-1} + s_t$$\nHere, the linear combination $y_t - \\beta x_{t-1} - c = s_t$ is a random walk and therefore non-stationary ($I(1)$). The variables are not cointegrated. This is a classic case of a spurious regression, where a high correlation may be observed despite no meaningful long-run relationship.\n\n**Step 2: Engle-Granger Cointegration Test**\n\nThe test proceeds in two stages.\n\n**Stage 1: Estimate the Long-Run Relationship**\nWe estimate the parameters of the hypothesized long-run cointegrating relationship using Ordinary Least Squares (OLS):\n$$y_t = \\alpha + \\beta^{\\ast} x_{t-1} + u_t, \\quad \\text{for } t=1, \\dots, T$$\nWe regress the vector $Y = (y_1, \\dots, y_T)^T$ on a design matrix $X_{ols}$ of dimension $T \\times 2$, where the first column is a vector of ones and the second column is the vector $(x_0, \\dots, x_{T-1})^T$. The OLS coefficient estimates are given by $\\hat{\\theta} = (\\hat{\\alpha}, \\hat{\\beta}^{\\ast})^T = (X_{ols}^T X_{ols})^{-1} X_{ols}^T Y$.\n\nThe residuals from this regression are then computed:\n$$\\hat{u}_t = y_t - (\\hat{\\alpha} + \\hat{\\beta}^{\\ast} x_{t-1}), \\quad \\text{for } t=1, \\dots, T$$\nIf $y_t$ and $x_{t-1}$ are cointegrated, these residuals should be stationary, $I(0)$. If they are not cointegrated, the residuals will be non-stationary, $I(1)$.\n\n**Stage 2: Test Residuals for a Unit Root**\nTo test the stationarity of the residuals $\\{\\hat{u}_t\\}$, we perform an Augmented Dickey-Fuller (ADF) test. The null hypothesis is that the residuals have a unit root (are non-stationary), which implies no cointegration. The alternative is stationarity, which implies cointegration.\n\nThe specific ADF regression model to be estimated is:\n$$\\Delta \\hat{u}_t = \\varphi \\hat{u}_{t-1} + \\gamma \\Delta \\hat{u}_{t-1} + \\varepsilon_t^{\\ast}, \\quad \\text{for } t=3, \\dots, T$$\nThis is an OLS regression of the change in residuals, $\\Delta \\hat{u}_t$, on the lagged level of the residuals, $\\hat{u}_{t-1}$, and the lagged change, $\\Delta \\hat{u}_{t-1}$. The regression is run over $T-2$ observations.\n\nLet $Z$ be the $(T-2) \\times 1$ vector of the dependent variable, $(\\Delta \\hat{u}_3, \\dots, \\Delta \\hat{u}_T)^T$. Let $W$ be the $(T-2) \\times 2$ matrix of independent variables, whose rows are $(\\hat{u}_{t-1}, \\Delta \\hat{u}_{t-1})$ for $t=3, \\dots, T$. The OLS estimates for $(\\varphi, \\gamma)$ are $\\hat{\\psi} = (\\hat{\\varphi}, \\hat{\\gamma})^T = (W^T W)^{-1} W^T Z$.\n\nThe test statistic is the t-statistic for the coefficient $\\varphi$:\n$$\\tau = \\frac{\\hat{\\varphi}}{SE(\\hat{\\varphi})}$$\nwhere $SE(\\hat{\\varphi})$ is the standard error of the estimate $\\hat{\\varphi}$. This is calculated as the square root of the first diagonal element of the coefficient covariance matrix, $\\hat{\\sigma}_{\\varepsilon^{\\ast}}^2 (W^T W)^{-1}$. The variance of the regression-error term $\\varepsilon_t^{\\ast}$ is estimated as:\n$$\\hat{\\sigma}_{\\varepsilon^{\\ast}}^2 = \\frac{1}{T-2-2} \\sum_{t=3}^T (\\varepsilon_t^{\\ast})^2 = \\frac{SSR}{T-4}$$\nwhere $SSR$ is the sum of squared residuals from the ADF regression.\n\n**Step 3: Decision Rule**\n\nThe null hypothesis $H_0: \\varphi = 0$ (unit root in residuals, no cointegration) is tested against the one-sided alternative $H_1: \\varphi  0$ (stationarity, cointegration).\n\nThe calculated t-statistic, $\\tau$, is compared against a specific critical value. Because the test is performed on residuals from a regression of $I(1)$ variables, the distribution of $\\tau$ is non-standard and is known as the Engle-Granger distribution. The problem provides the appropriate critical value for a significance level of $\\alpha = 0.05$, which is $c_{0.05} = -3.34$.\n\nThe decision rule is:\n- If $\\tau \\le -3.34$, reject $H_0$. We conclude that the series are cointegrated. The result for the test case is `True`.\n- If $\\tau  -3.34$, do not reject $H_0$. We conclude there is no evidence of cointegration. The result for the test case is `False`.\n\nThis complete procedure is applied to each of the three test cases to determine the final boolean output.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the Engle-Granger two-step cointegration test for a suite of test cases.\n    \"\"\"\n    test_cases = [\n        # (T, seed, c, beta, sigma_eps, sigma_eta, phi, residual_type)\n        (400, 101, 0.5, 1.3, 1.0, 0.3, 0.4, \"stationary\"),\n        (400, 202, 0.5, 1.3, 1.0, 0.8, 0.0, \"random_walk\"),\n        (400, 303, -0.2, -0.8, 0.8, 0.5, 0.7, \"stationary\"),\n    ]\n\n    results = []\n    critical_value = -3.34\n\n    for case in test_cases:\n        T, seed, c, beta, sigma_eps, sigma_eta, phi, residual_type = case\n\n        # Step 1: Data Generation\n        rng = np.random.default_rng(seed)\n        eps = rng.normal(loc=0.0, scale=sigma_eps, size=T)\n        eta = rng.normal(loc=0.0, scale=sigma_eta, size=T)\n\n        # Generate x_t as a random walk\n        x = np.zeros(T + 1)\n        for t in range(1, T + 1):\n            x[t] = x[t - 1] + eps[t - 1]\n\n        # Generate y_t based on the residual type\n        # In the main vector y, y[t-1] corresponds to the mathematical y_t\n        if residual_type == \"stationary\":\n            v = np.zeros(T + 1)\n            for t in range(1, T + 1):\n                v[t] = phi * v[t-1] + eta[t-1]\n            error_term = v[1:T+1]\n        elif residual_type == \"random_walk\":\n            s = np.zeros(T + 1)\n            for t in range(1, T + 1):\n                s[t] = s[t-1] + eta[t-1]\n            error_term = s[1:T+1]\n        \n        y = c + beta * x[0:T] + error_term\n\n        # Step 2: Engle-Granger Stage 1 - OLS regression to get residuals\n        # Regress y_t on a constant and x_{t-1}\n        X_ols1 = np.vstack((np.ones(T), x[0:T])).T\n        \n        # Using lstsq is numerically more stable than direct matrix inversion\n        coeffs_ols1, _, _, _ = np.linalg.lstsq(X_ols1, y, rcond=None)\n        u_hat = y - X_ols1 @ coeffs_ols1\n\n        # Step 3: Engle-Granger Stage 2 - ADF test on residuals\n        # ADF model: Delta(u_hat)_t = phi*u_hat_{t-1} + gamma*Delta(u_hat)_{t-1} + e_t\n        # for t = 3, ..., T. This gives T-2 observations.\n        delta_u_hat = u_hat[1:] - u_hat[:-1]  # Corresponds to Delta(u_hat)_2, ..., Delta(u_hat)_T\n\n        # Dependent variable: Delta(u_hat)_t for t=3,...,T\n        Y_adf = delta_u_hat[1:]  # Corresponds to Delta(u_hat)_3, ..., Delta(u_hat)_T\n\n        # Independent variables\n        # u_hat_{t-1} for t=3,...,T\n        X_adf_col1 = u_hat[1:-1]\n        # Delta(u_hat)_{t-1} for t=3,...,T\n        X_adf_col2 = delta_u_hat[:-1]\n        \n        X_adf = np.vstack((X_adf_col1, X_adf_col2)).T\n\n        # Perform OLS for the ADF regression\n        coeffs_adf, ssr_adf, _, _ = np.linalg.lstsq(X_adf, Y_adf, rcond=None)\n        phi_hat = coeffs_adf[0]\n\n        # Calculate the t-statistic for phi_hat\n        n_obs_adf = T - 2\n        k_params_adf = 2\n        df = n_obs_adf - k_params_adf\n\n        # Estimate of the regression error variance\n        sigma2_hat_adf = ssr_adf[0] / df\n\n        # Variance-covariance matrix of the ADF coefficients\n        try:\n            cov_matrix = sigma2_hat_adf * np.linalg.inv(X_adf.T @ X_adf)\n            se_phi_hat = np.sqrt(cov_matrix[0, 0])\n            tau_statistic = phi_hat / se_phi_hat\n        except np.linalg.LinAlgError:\n            # Handle cases of singular matrix, which is unlikely but possible\n            # A very large t-stat would not reject the null, which is a safe default\n            tau_statistic = 0.0\n\n        # Step 4: Decision\n        is_cointegrated = tau_statistic = critical_value\n        results.append(is_cointegrated)\n        \n    # Format and print the final output\n    print(f\"[{','.join(str(r) for r in results)}]\")\n\nsolve()\n```", "id": "2380057"}, {"introduction": "While the Engle-Granger test is powerful, a crucial subtlety is that the test statistic does not follow a standard distribution, like the normal or t-distribution. Its distribution depends on the sample size and other properties of the data, making fixed critical values from textbooks potentially unreliable. This practice introduces the bootstrap, a robust computational method to overcome this limitation by generating custom critical values tailored to your specific dataset, thereby greatly improving the reliability of your statistical inference [@problem_id:2380066].", "problem": "You are asked to implement a complete, self-contained program that computes custom, bootstrap-based critical values for the Engle–Granger cointegration test statistic and decides whether to reject the null of no cointegration in several controlled data-generating scenarios. The program must be fully deterministic and produce a single line of output as specified below.\n\nYou must build your solution from the following fundamental base:\n- A stochastic process with a single unit root is integrated of order one, denoted $I(1)$, which means its first difference is stationary.\n- Two $I(1)$ series $\\{x_t\\}$ and $\\{y_t\\}$ are cointegrated if there exists a scalar $\\beta$ such that $y_t - \\beta x_t$ is stationary.\n- The Engle–Granger procedure tests for cointegration by: \n  1) estimating a static cointegrating regression of $y_t$ on $x_t$ via ordinary least squares to obtain residuals $\\hat e_t$,\n  2) testing $\\hat e_t$ for a unit root using an Augmented Dickey–Fuller (ADF) regression without deterministic terms, and using the $t$-statistic on the lagged level coefficient as the Engle–Granger test statistic.\n- The residual-based unit root test in step $2$ has a nonstandard limiting distribution under the null, hence tabulated critical values may not be valid under small samples or under certain short-run dynamics. A bootstrap procedure that simulates the null of no cointegration by generating random walks for both series with appropriately estimated innovation covariance can provide custom critical values adapted to the sample and short-run features.\n\nYour program must adhere to the following computational definitions and requirements:\n- Engle–Granger test statistic:\n  - First, regress $y_t$ on a constant and $x_t$ by ordinary least squares, $y_t = \\alpha + \\beta x_t + \\varepsilon_t$, for $t = 1,\\dots,T$, and obtain residuals $\\hat e_t$.\n  - Then estimate an Augmented Dickey–Fuller regression with no intercept and no trend for the residuals:\n    $$\\Delta \\hat e_t = \\rho \\hat e_{t-1} + \\sum_{i=1}^{k} \\gamma_i \\Delta \\hat e_{t-i} + u_t,$$\n    where $k$ is a fixed nonnegative integer. Compute the ordinary least squares $t$-statistic on the coefficient $\\rho$. This $t$-statistic is the Engle–Granger test statistic $\\tau$.\n- Bootstrap under the null of no cointegration:\n  - Estimate the $2\\times 2$ covariance matrix $\\Sigma$ of the observed first differences $\\Delta x_t$ and $\\Delta y_t$.\n  - For each bootstrap replication $b = 1,\\dots,B$: generate a bivariate Gaussian innovation sequence $\\{(\\varepsilon_t^{x,(b)}, \\varepsilon_t^{y,(b)})\\}_{t=1}^{T-1}$ with mean zero and covariance $\\Sigma$; construct bootstrap random walks\n    $$x_t^{(b)} = x_{t-1}^{(b)} + \\varepsilon_t^{x,(b)}, \\quad y_t^{(b)} = y_{t-1}^{(b)} + \\varepsilon_t^{y,(b)},$$\n    initialized at $x_0^{(b)} = x_0$ and $y_0^{(b)} = y_0$, and compute the Engle–Granger statistic $\\tau^{(b)}$ on $(y^{(b)}, x^{(b)})$ using the same $k$ as above.\n  - The custom critical value at level $a$ is the empirical $a$-quantile of $\\{\\tau^{(b)}\\}_{b=1}^B$. Because the Engle–Granger statistic is typically negative under the alternative, rejection occurs when $\\tau \\le q_a$, where $q_a$ is the $a$-quantile.\n- Use $k = 1$ lag in the Augmented Dickey–Fuller regression for every case.\n- Use $a = 0.05$ for the test size and $B = 299$ bootstrap replications for every case.\n\nData-generating processes and test suite:\n- There are $3$ test cases. For each case you must generate $(y_t, x_t)$ of length $T$ with zero initial values $x_0 = 0$ and $y_0 = 0$ using the specified seed for reproducibility. Your code must use the exact random seeds provided for both data generation and bootstrap replication. For bootstrap, derive its seed deterministically from the case seed by adding the integer $1000003$.\n- Case A (cointegrated):\n  - Parameters: $T = 220$, $\\beta = 1.3$, $\\phi = 0.4$, $\\sigma_x = 1.0$, $\\sigma_u = 0.4$, seed $= 12345$.\n  - Generate $x_t$ as a random walk: $x_t = x_{t-1} + \\varepsilon_t^x$, with $\\varepsilon_t^x \\sim \\mathcal N(0, \\sigma_x^2)$.\n  - Generate a stationary disturbance for the cointegrating relation: $u_t = \\phi u_{t-1} + \\eta_t$, with $\\eta_t \\sim \\mathcal N(0, \\sigma_u^2)$ and $u_0 = 0$.\n  - Define $y_t = \\beta x_t + u_t$.\n- Case B (no cointegration, correlated random walks):\n  - Parameters: $T = 220$, $\\sigma_x = 1.0$, $\\sigma_y = 1.0$, $\\rho = 0.6$, seed $= 54321$.\n  - Generate the vector of first differences $(\\Delta x_t, \\Delta y_t)$ as independent and identically distributed draws from a bivariate normal distribution with mean zero and covariance\n    $$\\begin{pmatrix}\\sigma_x^2  \\rho \\sigma_x \\sigma_y \\\\ \\rho \\sigma_x \\sigma_y  \\sigma_y^2 \\end{pmatrix}.$$\n  - Construct the random walks $x_t = x_{t-1} + \\Delta x_t$ and $y_t = y_{t-1} + \\Delta y_t$.\n- Case C (cointegrated, smaller sample):\n  - Parameters: $T = 80$, $\\beta = 1.0$, $\\phi = 0.5$, $\\sigma_x = 0.8$, $\\sigma_u = 0.6$, seed $= 24680$.\n  - Generate $x_t$, $u_t$, and $y_t$ as in Case A with the given parameters.\n\nYour program must:\n- Implement the Engle–Granger statistic $\\tau$ as defined above with $k = 1$.\n- Implement the bootstrap procedure under the null of no cointegration with $B = 299$ replications and compute the custom $0.05$ critical value for each case.\n- For each test case, return a boolean indicating whether the null is rejected at level $0.05$ using the custom critical value, i.e., return $true$ if $\\tau \\le q_{0.05}$ and $false$ otherwise.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order Case A, Case B, Case C. For example, the output must look like\n  \"[true,false,true]\"\n- All booleans must be in lower-case letters.\n\nAngles are not involved. There are no physical units in this problem. All numerical quantities are pure numbers. All random number generation must use the specified seeds to ensure reproducibility.\n\nYour solution must be implemented as a single, self-contained Python program that prints only the final result line in the exact format described.", "solution": "We begin from core definitions in time series econometrics. A univariate process is integrated of order one, denoted $I(1)$, if its first difference is stationary. Two $I(1)$ processes $\\{x_t\\}$ and $\\{y_t\\}$ are cointegrated if a linear combination $y_t - \\beta x_t$ is stationary. The Engle–Granger procedure operationalizes this definition by first estimating the long-run relation via ordinary least squares and then testing the residuals for a unit root.\n\nPrinciple-based construction of the Engle–Granger statistic:\n- Step $1$ (static cointegrating regression): Estimate $y_t = \\alpha + \\beta x_t + \\varepsilon_t$ by ordinary least squares for $t = 1,\\dots,T$ to obtain fitted parameters and residuals $\\hat e_t = y_t - \\hat \\alpha - \\hat \\beta x_t$.\n- Step $2$ (residual unit root test): Model the residuals with an Augmented Dickey–Fuller regression without deterministic terms, \n  $$\\Delta \\hat e_t = \\rho \\hat e_{t-1} + \\sum_{i=1}^k \\gamma_i \\Delta \\hat e_{t-i} + u_t,$$\n  where $k$ is a fixed augmentation lag to capture short-run serial correlation in the first differences of the residuals. Estimate this regression by ordinary least squares. The Engle–Granger test statistic $\\tau$ is the ordinary least squares $t$-statistic associated with the coefficient $\\rho$ on the lagged level $\\hat e_{t-1}$. A sufficiently negative value of $\\tau$ indicates stationarity of $\\hat e_t$ and thus cointegration.\n\nWhy a bootstrap is required:\n- Under the null hypothesis of no cointegration, the residual $\\hat e_t$ is not a fixed linear transformation of a stationary process; it inherits the unit root from the $I(1)$ regressors, and the distribution of the residual-based unit root statistic is nonstandard, depending in complicated ways on nuisance parameters and finite-sample features. Therefore, tabulated asymptotic critical values may be unreliable in finite samples or in the presence of short-run correlation or endogeneity.\n- A parametric bootstrap can simulate the null hypothesis of no cointegration by generating synthetic pairs $(x_t^{(b)}, y_t^{(b)})$ that are random walks with a covariance structure matched to the observed first differences. By recomputing the Engle–Granger statistic on these bootstrap series, we approximate the sampling distribution of $\\tau$ under the null, conditional on the observed short-run features.\n\nBootstrap design from first principles:\n- Estimate the covariance matrix $\\Sigma$ of the observed first differences $(\\Delta x_t, \\Delta y_t)$ by the unbiased sample covariance, which is a well-tested estimator matching second moments in large samples.\n- Under the null of no cointegration, both $x_t$ and $y_t$ behave as $I(1)$ processes driven by mean-zero innovations. We mimic this by simulating $\\Delta x_t^{(b)}, \\Delta y_t^{(b)}$ as independent and identically distributed Gaussian vectors with mean zero and covariance $\\Sigma$, and then cumulatively summing to construct random walks $x_t^{(b)}$ and $y_t^{(b)}$ initialized at the original $x_0$ and $y_0$ to align levels.\n- For each bootstrap replication $b = 1,\\dots,B$, we compute the Engle–Granger statistic $\\tau^{(b)}$ by repeating the two-step procedure on $(y^{(b)}, x^{(b)})$ with the same augmentation lag $k$.\n\nQuantile-based critical value and rejection rule:\n- Let $\\{\\tau^{(b)}\\}_{b=1}^B$ be the bootstrap statistics. The custom critical value at level $a$ is the empirical $a$-quantile $q_a$ of this set. Because more negative $\\tau$ indicates stronger evidence against the null, the test rejects the null of no cointegration when $\\tau \\le q_a$.\n- We fix $a = 0.05$ and $B = 299$, which balances accuracy and computational cost in an advanced undergraduate setting.\n\nAlgorithmic details of implementation:\n- Ordinary least squares is implemented using linear algebra as follows. For a regression $y = X \\beta + \\varepsilon$, the ordinary least squares estimator is $\\hat \\beta = (X^\\top X)^{-1} X^\\top y$. The residual variance estimator is $\\hat \\sigma^2 = \\frac{1}{n - p} \\sum_{i=1}^n \\hat \\varepsilon_i^2$, where $n$ is the sample size and $p$ is the number of regressors. The covariance matrix of $\\hat \\beta$ is $\\hat \\sigma^2 (X^\\top X)^{-1}$, and the ordinary least squares $t$-statistic for any component is the estimated coefficient divided by its estimated standard error.\n- For the Augmented Dickey–Fuller regression without deterministic terms, the design matrix includes the lagged residual level $\\hat e_{t-1}$ and $k$ lagged first differences $\\Delta \\hat e_{t-1}, \\dots, \\Delta \\hat e_{t-k}$. With $k = 1$, the regressors are $\\hat e_{t-1}$ and $\\Delta \\hat e_{t-1}$, and the response is $\\Delta \\hat e_t$. The effective sample size is $T - 1 - k$ after aligning lags.\n- To ensure reproducibility, we use fixed random seeds for data generation and derive the bootstrap seed for each case by adding $1000003$ to the case seed. We initialize the bootstrap random walks at the observed $x_0$ and $y_0$.\n\nTest suite coverage rationale:\n- Case A is a \"happy path\" with clear cointegration: $T = 220$, $\\beta = 1.3$, moderate stationary deviation $u_t$ with $\\phi = 0.4$ and small variance $\\sigma_u = 0.4$ relative to $\\sigma_x = 1.0$. We expect a negative $\\tau$ sufficiently below the bootstrap critical value, hence rejection.\n- Case B is a null case with no cointegration but contemporaneously correlated random-walk innovations, a known setting where spurious regression is a risk asymptotically mitigated by tailored critical values. The bootstrap that matches the innovation covariance should produce a critical value consistent with the finite-sample behavior, and we generally expect no rejection.\n- Case C is a smaller-sample cointegrated case with $T = 80$, $\\beta = 1.0$, and moderately persistent $u_t$ with $\\phi = 0.5$. Despite the smaller sample, the bootstrap adapts to the finite-sample features and often yields rejection when cointegration is present.\n\nOutput:\n- For each case, compute a boolean indicating whether to reject at level $0.05$ using the custom critical value. The program prints a single line with a list of lower-case booleans in the order Case A, Case B, Case C, e.g., \"[true,false,true]\".\n\nAll mathematical definitions and computations follow from core linear regression and time series principles, without reliance on pre-tabulated critical values. The algorithm respects the residual-based nature of the Engle–Granger test and constructs a scientifically sound bootstrap under the null of no cointegration by simulating bivariate random walks with matched short-run covariance.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Execution environment: Python 3.12, numpy 1.23.5, scipy 1.11.4 (not used below).\nimport numpy as np\n\ndef ols_beta_and_se(X: np.ndarray, y: np.ndarray):\n    \"\"\"\n    Ordinary Least Squares estimation.\n    Returns:\n        beta_hat: (p,) array of coefficients\n        se_beta:  (p,) array of standard errors\n        resid:    (n,) residual vector\n    \"\"\"\n    # Solve for beta using least squares\n    beta_hat, residuals, rank, s = np.linalg.lstsq(X, y, rcond=None)\n    # Compute residuals explicitly for robust se\n    resid = y - X @ beta_hat\n    n, p = X.shape\n    # Degrees of freedom\n    dof = max(n - p, 1)  # guard against division by zero; not expected in tests\n    sigma2 = (resid @ resid) / dof\n    # Compute (X'X)^{-1} using pseudo-inverse for numerical stability\n    XtX = X.T @ X\n    XtX_inv = np.linalg.pinv(XtX)\n    cov_beta = sigma2 * XtX_inv\n    se_beta = np.sqrt(np.diag(cov_beta))\n    return beta_hat, se_beta, resid\n\ndef engle_granger_tau(y: np.ndarray, x: np.ndarray, k: int = 1) - float:\n    \"\"\"\n    Compute the Engle-Granger test statistic (ADF t-stat on residual lag) with k ADF lags, no intercept.\n    Steps:\n      1) OLS of y on [1, x], get residuals e.\n      2) ADF regression: Δe_t = rho * e_{t-1} + sum_{i=1}^k gamma_i Δe_{t-i} + u_t\n         No intercept or trend. Return t-statistic for rho.\n    \"\"\"\n    T = len(y)\n    # Step 1: cointegrating regression with intercept\n    X_coint = np.column_stack([np.ones(T), x])\n    beta_hat, se_beta, resid = ols_beta_and_se(X_coint, y)\n    e = resid\n    # Step 2: ADF without intercept/trend\n    de = e[1:] - e[:-1]  # length T-1\n    if k  0:\n        raise ValueError(\"k must be nonnegative\")\n    m = (T - 1) - k\n    if m = 1:\n        # Not enough data; return NaN or a neutral large value\n        return np.nan\n    y_adf = de[k:]  # length m\n    e_lag = e[:-1][k:]  # length m\n    Z_cols = [e_lag]\n    # add lagged differences Δe_{t-1} ... Δe_{t-k}\n    for j in range(1, k + 1):\n        Z_cols.append(de[k - j:-j])\n    Z = np.column_stack(Z_cols)\n    beta_adf, se_adf, resid_adf = ols_beta_and_se(Z, y_adf)\n    rho_hat = beta_adf[0]\n    se_rho = se_adf[0]\n    # Guard against zero standard error (degenerate); return 0 t-stat if degenerate\n    if se_rho = 0 or not np.isfinite(se_rho):\n        return 0.0\n    t_stat = rho_hat / se_rho\n    return float(t_stat)\n\ndef bootstrap_critical_value(y: np.ndarray, x: np.ndarray, k: int, B: int, alpha: float, seed: int) - float:\n    \"\"\"\n    Parametric bootstrap under the null of no cointegration:\n      - Estimate covariance of [Δx, Δy]\n      - Generate bivariate Gaussian innovations with that covariance\n      - Construct random walks starting at original initial values\n      - Compute Engle-Granger tau for each bootstrap sample\n      - Return empirical alpha-quantile (left tail)\n    \"\"\"\n    T = len(y)\n    dx = x[1:] - x[:-1]\n    dy = y[1:] - y[:-1]\n    # Stack differences to estimate covariance\n    diffs = np.column_stack([dx, dy])\n    # Unbiased covariance estimate\n    Sigma = np.cov(diffs, rowvar=False, ddof=1)\n    # Ensure Sigma is positive semidefinite; numerical guard\n    # If covariance estimation issues, regularize slightly\n    eigvals = np.linalg.eigvalsh(Sigma)\n    if np.min(eigvals)  1e-10:\n        Sigma = Sigma + 1e-8 * np.eye(2)\n    rng = np.random.default_rng(seed)\n    taus = []\n    for b in range(B):\n        # Generate T-1 innovations\n        innov = rng.multivariate_normal(mean=np.zeros(2), cov=Sigma, size=T - 1)\n        x_star = np.empty(T)\n        y_star = np.empty(T)\n        x_star[0] = x[0]\n        y_star[0] = y[0]\n        # Build random walks\n        for t in range(1, T):\n            x_star[t] = x_star[t - 1] + innov[t - 1, 0]\n            y_star[t] = y_star[t - 1] + innov[t - 1, 1]\n        tau_b = engle_granger_tau(y_star, x_star, k=k)\n        taus.append(tau_b)\n    taus = np.array(taus, dtype=float)\n    # Empirical alpha-quantile (left tail)\n    q_alpha = float(np.quantile(taus, alpha, method=\"linear\"))\n    return q_alpha\n\ndef simulate_cointegrated(T: int, seed: int, beta: float, phi: float, sigma_x: float, sigma_u: float):\n    \"\"\"\n    Simulate:\n      x_t = x_{t-1} + eps_x, eps_x ~ N(0, sigma_x^2)\n      u_t = phi * u_{t-1} + eta, eta ~ N(0, sigma_u^2)\n      y_t = beta * x_t + u_t\n    with x_0 = 0, u_0 = 0, y_0 accordingly.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    x = np.empty(T)\n    u = np.empty(T)\n    y = np.empty(T)\n    x[0] = 0.0\n    u[0] = 0.0\n    for t in range(1, T):\n        x[t] = x[t - 1] + rng.normal(0.0, sigma_x)\n        u[t] = phi * u[t - 1] + rng.normal(0.0, sigma_u)\n    y = beta * x + u\n    return y, x\n\ndef simulate_null_rw(T: int, seed: int, sigma_x: float, sigma_y: float, rho: float):\n    \"\"\"\n    Simulate correlated random walks under null:\n      [Δx_t, Δy_t]' ~ iid N(0, Sigma), Sigma set by sigma_x, sigma_y, rho\n      x_0 = y_0 = 0\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    cov_xy = rho * sigma_x * sigma_y\n    Sigma = np.array([[sigma_x ** 2, cov_xy],\n                      [cov_xy, sigma_y ** 2]], dtype=float)\n    # Numerical guard for positive semidefinite Sigma\n    eigvals = np.linalg.eigvalsh(Sigma)\n    if np.min(eigvals)  1e-10:\n        Sigma = Sigma + 1e-8 * np.eye(2)\n    innov = rng.multivariate_normal(mean=np.zeros(2), cov=Sigma, size=T - 1)\n    x = np.empty(T)\n    y = np.empty(T)\n    x[0] = 0.0\n    y[0] = 0.0\n    for t in range(1, T):\n        x[t] = x[t - 1] + innov[t - 1, 0]\n        y[t] = y[t - 1] + innov[t - 1, 1]\n    return y, x\n\ndef decide_reject(y: np.ndarray, x: np.ndarray, k: int, B: int, alpha: float, case_seed: int) - bool:\n    \"\"\"\n    Compute observed tau, bootstrap critical value, and return True if tau = q_alpha.\n    \"\"\"\n    tau_obs = engle_granger_tau(y, x, k=k)\n    # Bootstrap seed derived from case seed\n    boot_seed = case_seed + 1000003\n    q_alpha = bootstrap_critical_value(y, x, k=k, B=B, alpha=alpha, seed=boot_seed)\n    # Reject if observed tau is less than or equal to critical value (left tail)\n    return bool(tau_obs = q_alpha)\n\ndef solve():\n    # Define constants\n    k = 1          # ADF lags\n    B = 299        # Bootstrap replications\n    alpha = 0.05   # Test size\n\n    # Test cases as specified:\n    # Case A: Cointegrated\n    T_A = 220\n    beta_A = 1.3\n    phi_A = 0.4\n    sigma_x_A = 1.0\n    sigma_u_A = 0.4\n    seed_A = 12345\n\n    # Case B: Null (no cointegration), correlated random walks\n    T_B = 220\n    sigma_x_B = 1.0\n    sigma_y_B = 1.0\n    rho_B = 0.6\n    seed_B = 54321\n\n    # Case C: Cointegrated, smaller sample\n    T_C = 80\n    beta_C = 1.0\n    phi_C = 0.5\n    sigma_x_C = 0.8\n    sigma_u_C = 0.6\n    seed_C = 24680\n\n    # Generate data and decisions\n    # Case A\n    yA, xA = simulate_cointegrated(T_A, seed_A, beta_A, phi_A, sigma_x_A, sigma_u_A)\n    reject_A = decide_reject(yA, xA, k=k, B=B, alpha=alpha, case_seed=seed_A)\n\n    # Case B\n    yB, xB = simulate_null_rw(T_B, seed_B, sigma_x_B, sigma_y_B, rho_B)\n    reject_B = decide_reject(yB, xB, k=k, B=B, alpha=alpha, case_seed=seed_B)\n\n    # Case C\n    yC, xC = simulate_cointegrated(T_C, seed_C, beta_C, phi_C, sigma_x_C, sigma_u_C)\n    reject_C = decide_reject(yC, xC, k=k, B=B, alpha=alpha, case_seed=seed_C)\n\n    results = [reject_A, reject_B, reject_C]\n    # Print in exact required format: lower-case booleans\n    def bool_to_str(b): return \"true\" if b else \"false\"\n    print(f\"[{','.join(bool_to_str(r) for r in results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2380066"}, {"introduction": "Identifying a cointegrating relationship is often just the first step; in the real world, long-run economic relationships can change or break down entirely. This exercise moves from *testing for* to *monitoring* the stability of a cointegrating vector over time, a vital task for risk management and model validation. You will implement a recursive estimation procedure that uses one-step-ahead forecast errors to detect when a structural break occurs, providing a practical tool for assessing the ongoing validity of an economic model [@problem_id:2380077].", "problem": "You are given two synthetic time series constructed to follow a long-run relation that may experience a structural change at an unknown time. Let $\\{x_t\\}_{t=0}^{T-1}$ and $\\{y_t\\}_{t=0}^{T-1}$ be defined as follows. For all $t \\in \\{0,1,\\dots,T-1\\}$, let $x_0 = 0$ and for $t \\ge 1$,\n$$\nx_t = x_{t-1} + \\sigma_e \\,\\varepsilon_t,\n$$\nand\n$$\ny_t = \\alpha + \\beta(t)\\, x_t + \\sigma_u \\,\\eta_t,\n$$\nwhere $\\{\\varepsilon_t\\}$ and $\\{\\eta_t\\}$ are independent sequences of independent and identically distributed standard normal random variables, and $\\beta(t)$ is a piecewise constant function\n$$\n\\beta(t) = \\begin{cases}\n\\beta_1,  t \\le T_b,\\\\\n\\beta_2,  t  T_b.\n\\end{cases}\n$$\nThe constant $T_b$ denotes the (potential) break index at which the long-run cointegrating slope changes from $\\beta_1$ to $\\beta_2$. If no break is intended, take $T_b \\ge T$ so that $\\beta(t) = \\beta_1$ for all $t$. The intercept $\\alpha$ is constant.\n\nThe shocks $\\{\\varepsilon_t\\}$ and $\\{\\eta_t\\}$ must be generated deterministically from a specified uniform pseudo-random sequence using a linear congruential generator to ensure reproducibility. Define the multiplicative linear congruential sequence $\\{U_k\\}$ on integers by\n$$\nU_{k} = (a \\cdot U_{k-1}) \\bmod m,\\quad k \\ge 1,\\quad U_0 = s,\n$$\nwith modulus $m = 2^{31}-1$, multiplier $a = 16807$, and integer seed $s \\in \\{1,2,\\dots,m-1\\}$. Map to uniforms $R_k \\in (0,1)$ via $R_k = U_k/m$. Convert to independent standard normal innovations using the Box–Muller transform: for each pair $(R_{2j-1}, R_{2j})$,\n$$\nZ_{2j-1} = \\sqrt{-2\\ln R_{2j-1}}\\,\\cos(2\\pi R_{2j}),\\quad\nZ_{2j} = \\sqrt{-2\\ln R_{2j-1}}\\,\\sin(2\\pi R_{2j}),\n$$\nwhere all angles in the trigonometric functions are in radians. Use the first $T$ normal draws for $\\{\\varepsilon_t\\}$ and the next $T$ normal draws for $\\{\\eta_t\\}$.\n\nFor detection, define a recursive sequence of ordinary least squares (Ordinary Least Squares (OLS)) estimators for the cointegrating relation $y_s = \\alpha + \\beta x_s + \\text{error}$, computed on expanding samples. For each $t \\in \\{T_{\\min}, T_{\\min}+1, \\dots, T-2\\}$, estimate $(\\widehat{\\alpha}_t,\\widehat{\\beta}_t)$ by minimizing $\\sum_{s=0}^{t} (y_s - \\alpha - \\beta x_s)^2$. Equivalently, with $n_t = t+1$, sample means $\\overline{x}_t = \\frac{1}{n_t}\\sum_{s=0}^{t} x_s$ and $\\overline{y}_t = \\frac{1}{n_t}\\sum_{s=0}^{t} y_s$, and centered sums $S_{xx,t} = \\sum_{s=0}^{t} (x_s-\\overline{x}_t)^2$, $S_{xy,t} = \\sum_{s=0}^{t} (x_s-\\overline{x}_t)(y_s-\\overline{y}_t)$,\n$$\n\\widehat{\\beta}_t = \\frac{S_{xy,t}}{S_{xx,t}},\\quad \\widehat{\\alpha}_t = \\overline{y}_t - \\widehat{\\beta}_t\\,\\overline{x}_t.\n$$\nLet the residual standard deviation be\n$$\n\\widehat{\\sigma}_t = \\sqrt{\\frac{1}{n_t - 2}\\sum_{s=0}^{t} \\left(y_s - \\widehat{\\alpha}_t - \\widehat{\\beta}_t x_s\\right)^2},\n$$\ndefined for $n_t \\ge 3$. Form the one-step-ahead standardized forecast error at time $t+1$,\n$$\nz_{t+1} = \\frac{y_{t+1} - \\widehat{\\alpha}_t - \\widehat{\\beta}_t x_{t+1}}{\\widehat{\\sigma}_t}.\n$$\nGiven a threshold $c  0$ and a run length $L \\in \\mathbb{N}$, define the indicator $I_s = \\mathbf{1}\\{|z_s| \\ge c\\}$ for $s \\in \\{T_{\\min}+1, \\dots, T-1\\}$. The detected break index is the smallest index $s^\\star$ such that $I_{s^\\star} = I_{s^\\star+1} = \\dots = I_{s^\\star+L-1} = 1$. If no such index exists, return $-1$. Report the detected index $s^\\star$ in zero-based time indexing.\n\nImplement the above data construction and detection as a complete program for the following test suite of parameter sets, each specified as a tuple\n$$\n(T, T_b, \\alpha, \\beta_1, \\beta_2, \\sigma_e, \\sigma_u, T_{\\min}, c, L, s).\n$$\n- Test case A: $(240, 150, 0.5, 1.0, 1.6, 1.0, 0.05, 40, 3.0, 2, 12345)$.\n- Test case B: $(240, 10^9, 0.5, 1.0, 1.0, 1.0, 0.05, 40, 3.0, 2, 54321)$.\n- Test case C: $(220, 210, 0.3, 0.8, 1.9, 0.9, 0.05, 30, 2.5, 2, 20231107)$.\n- Test case D: $(180, 60, 0.2, 1.2, 1.4, 0.8, 0.1, 25, 2.0, 3, 424242)$.\n\nYour program should compute and return, for each test case, a single integer: either the detected break index $s^\\star$ (in zero-based indexing) or $-1$ if no break is detected under the rule. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[rA,rB,rC,rD]\"), where $rA$, $rB$, $rC$, and $rD$ are the integers corresponding to test cases A, B, C, and D, respectively.", "solution": "The problem presents a well-defined task in computational econometrics: the detection of a structural break in a linear cointegrating relationship between two non-stationary time series. The entire procedure, from data generation to break detection, is specified algorithmically, which permits a unique and verifiable solution. A rigorous analysis proceeds in two stages: first, the deterministic generation of the synthetic data, and second, the application of a recursive monitoring algorithm.\n\nThe foundation of this exercise is a reproducible data generation process. To ensure this, we do not use system-level sources of randomness but instead construct the required random sequences from a specified seed, $s$, using a multiplicative linear congruential generator (LCG). The sequence of integers $\\{U_k\\}$ is given by the recurrence $U_{k} = (a \\cdot U_{k-1}) \\bmod m$ for $k \\ge 1$, with $U_0 = s$. The problem specifies the industry-standard parameters $m = 2^{31}-1$ and $a = 16807$. These integers are mapped to a sequence of uniform pseudo-random numbers $\\{R_k\\}$ in the interval $(0, 1)$ via the transformation $R_k = U_k/m$.\n\nFrom this uniform sequence, we must generate independent standard normal random variates. The Box-Muller transform is specified for this purpose. For each pair of uniform variates $(R_{2j-1}, R_{2j})$, we produce a pair of independent standard normal variates $(Z_{2j-1}, Z_{2j})$:\n$$\nZ_{2j-1} = \\sqrt{-2\\ln R_{2j-1}}\\,\\cos(2\\pi R_{2j})\n$$\n$$\nZ_{2j} = \\sqrt{-2\\ln R_{2j-1}}\\,\\sin(2\\pi R_{2j})\n$$\nThe problem requires a total of $2T$ normal draws to construct the series. The first $T$ draws form the sequence $\\{\\varepsilon_t\\}_{t=0}^{T-1}$, and the subsequent $T$ draws form $\\{\\eta_t\\}_{t=0}^{T-1}$.\n\nWith these shock sequences, the time series are constructed. The series $\\{x_t\\}$ is an I($1$) process, specifically a random walk without drift, starting from $x_0 = 0$:\n$$\nx_t = x_{t-1} + \\sigma_e \\,\\varepsilon_t, \\quad \\text{for } t \\in \\{1, 2, \\dots, T-1\\}.\n$$\nThe series $\\{y_t\\}$ is constructed to be cointegrated with $\\{x_t\\}$, subject to a potential structural break. Its definition is:\n$$\ny_t = \\alpha + \\beta(t)\\, x_t + \\sigma_u \\,\\eta_t, \\quad \\text{for } t \\in \\{0, 1, \\dots, T-1\\}.\n$$\nThe parameter $\\beta(t)$ is a step function that changes its value at the break index $T_b$:\n$$\n\\beta(t) = \\beta_1 \\text{ if } t \\le T_b, \\quad \\text{and} \\quad \\beta(t) = \\beta_2 \\text{ if } t  T_b.\n$$\nA value of $T_b \\ge T$ indicates that no break occurs within the sample period, so $\\beta(t) = \\beta_1$ for all $t$.\n\nThe second stage is the detection of the break point. This is accomplished using a recursive monitoring scheme based on Ordinary Least Squares (OLS) estimation over an expanding data window. For each time index $t$ from a starting point $T_{\\min}$ up to $T-2$, we estimate the coefficients of the static cointegrating regression $y_s = \\alpha + \\beta x_s + \\text{error}$ using all available data from $s=0$ to $s=t$.\n\nTo perform this estimation efficiently, we do not recompute the required sums for each regression. Instead, we maintain running sums of the relevant quantities: $\\sum x_s$, $\\sum y_s$, $\\sum x_s^2$, $\\sum y_s^2$, and $\\sum x_s y_s$. At each step $t$, we update these sums with the new data point $(x_t, y_t)$. Let $n_t = t+1$ be the sample size. The OLS estimates $(\\widehat{\\alpha}_t, \\widehat{\\beta}_t)$ are then calculated using the standard formulae derived from these sums:\n$$\n\\widehat{\\beta}_t = \\frac{\\sum_{s=0}^{t} x_s y_s - n_t \\overline{x}_t \\overline{y}_t}{\\sum_{s=0}^{t} x_s^2 - n_t \\overline{x}_t^2}\n\\quad \\text{and} \\quad\n\\widehat{\\alpha}_t = \\overline{y}_t - \\widehat{\\beta}_t \\overline{x}_t,\n$$\nwhere $\\overline{x}_t$ and $\\overline{y}_t$ are the sample means over $\\{0, \\dots, t\\}$.\n\nThe core of the detection mechanism is the one-step-ahead standardized forecast error, $z_{t+1}$. This statistic measures how surprising the next observation, $(x_{t+1}, y_{t+1})$, is, given the model estimated up to time $t$. It is defined as:\n$$\nz_{t+1} = \\frac{y_{t+1} - (\\widehat{\\alpha}_t + \\widehat{\\beta}_t x_{t+1})}{\\widehat{\\sigma}_t}\n$$\nThe denominator, $\\widehat{\\sigma}_t$, is the estimated standard deviation of the regression residuals, which serves to standardize the forecast error. It is computed as:\n$$\n\\widehat{\\sigma}_t = \\sqrt{\\frac{1}{n_t - 2}\\sum_{s=0}^{t} (y_s - \\widehat{\\alpha}_t - \\widehat{\\beta}_t x_s)^2}\n$$\nThe sum of squared residuals in the numerator can be calculated efficiently as $S_{yy,t} - \\widehat{\\beta}_t S_{xy,t}$, where $S_{yy,t}$ and $S_{xy,t}$ are the centered sum of squares and cross-products.\n\nWhen a structural break occurs, the OLS estimates, which are based on data that may span both pre-break and post-break regimes, become biased. This bias produces systematically poor forecasts for subsequent periods, leading to a sequence of large forecast errors. The detection rule formalizes this intuition. We define an indicator $I_s = 1$ if $|z_s| \\ge c$ and $I_s = 0$ otherwise, for a given threshold $c$. A break is signaled at the first time index $s^\\star$ that initiates a sequence of at least $L$ consecutive large standardized forecast errors, i.e., $I_{s^\\star} = I_{s^\\star+1} = \\dots = I_{s^\\star+L-1} = 1$. The search for $s^\\star$ begins at index $T_{\\min}+1$. If no such sequence is found within the observation window, we conclude no break was detected and report $-1$. The entire procedure is implemented deterministically for each provided parameter set.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    \"\"\"\n    test_cases = [\n        # (T, T_b, alpha, beta_1, beta_2, sigma_e, sigma_u, T_min, c, L, s)\n        (240, 150, 0.5, 1.0, 1.6, 1.0, 0.05, 40, 3.0, 2, 12345),\n        (240, 10**9, 0.5, 1.0, 1.0, 1.0, 0.05, 40, 3.0, 2, 54321),\n        (220, 210, 0.3, 0.8, 1.9, 0.9, 0.05, 30, 2.5, 2, 20231107),\n        (180, 60, 0.2, 1.2, 1.4, 0.8, 0.1, 25, 2.0, 3, 424242),\n    ]\n\n    results = []\n    for params in test_cases:\n        result = _run_case(params)\n        results.append(result)\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef _run_case(params):\n    \"\"\"\n    Processes a single test case.\n    \"\"\"\n    T, T_b, alpha, beta_1, beta_2, sigma_e, sigma_u, T_min, c, L, s = params\n\n    # --- 1. Data Generation ---\n\n    # LCG parameters\n    m = 2**31 - 1\n    a = 16807\n    \n    # Generate 2*T uniform random numbers\n    uniforms = np.zeros(2 * T)\n    U_k = s\n    for k in range(2 * T):\n        U_k = (a * U_k) % m\n        uniforms[k] = U_k / m\n\n    # Generate 2*T standard normal random numbers using Box-Muller transform\n    normals = np.zeros(2 * T)\n    for j in range(T):\n        R1 = uniforms[2*j]\n        R2 = uniforms[2*j+1]\n        \n        log_R1 = np.log(R1)\n        term1 = np.sqrt(-2.0 * log_R1)\n        term2 = 2.0 * np.pi * R2\n\n        normals[2*j] = term1 * np.cos(term2)\n        normals[2*j+1] = term1 * np.sin(term2)\n    \n    epsilons = normals[:T]\n    etas = normals[T:]\n\n    # Generate time series x_t and y_t\n    x = np.zeros(T, dtype=np.float64)\n    y = np.zeros(T, dtype=np.float64)\n    \n    # x_0 = 0 is default\n    for t in range(1, T):\n        x[t] = x[t-1] + sigma_e * epsilons[t]\n\n    for t in range(T):\n        beta_t = beta_1 if t = T_b else beta_2\n        y[t] = alpha + beta_t * x[t] + sigma_u * etas[t]\n\n    # --- 2. Break Detection ---\n    \n    # Array to store indicators of threshold exceedance\n    z_indicators = np.zeros(T, dtype=int)\n    \n    # Initialize running sums for recursive OLS\n    s_x, s_y, s_x2, s_y2, s_xy = 0.0, 0.0, 0.0, 0.0, 0.0\n    \n    # Pre-computation for the initial window up to T_min\n    for t in range(T_min):\n        s_x += x[t]\n        s_y += y[t]\n        s_x2 += x[t]**2\n        s_y2 += y[t]**2\n        s_xy += x[t] * y[t]\n\n    # Recursive estimation and forecast error calculation\n    for t in range(T_min, T - 1):\n        # Update sums with the value at time t\n        s_x += x[t]\n        s_y += y[t]\n        s_x2 += x[t]**2\n        s_y2 += y[t]**2\n        s_xy += x[t] * y[t]\n        \n        n_t = t + 1\n        \n        # Calculate OLS estimates\n        x_bar = s_x / n_t\n        y_bar = s_y / n_t\n        \n        s_xx = s_x2 - n_t * x_bar**2\n        s_yy = s_y2 - n_t * y_bar**2\n        s_xy_t = s_xy - n_t * x_bar * y_bar\n        \n        # Avoid division by zero, though unlikely with a random walk\n        if s_xx == 0:\n            continue\n            \n        beta_hat = s_xy_t / s_xx\n        alpha_hat = y_bar - beta_hat * x_bar\n        \n        # Calculate residual standard deviation\n        ssr = s_yy - beta_hat * s_xy_t\n        # Ensure non-negativity due to potential floating point inaccuracies\n        ssr = max(0, ssr)\n        \n        df = n_t - 2\n        if df = 0:\n            continue\n            \n        sigma_hat_sq = ssr / df\n        \n        if sigma_hat_sq = 0:\n            continue \n        \n        sigma_hat = np.sqrt(sigma_hat_sq)\n        \n        # Calculate standardized one-step-ahead forecast error for time t+1\n        forecast_error = y[t+1] - (alpha_hat + beta_hat * x[t+1])\n        z_t_plus_1 = forecast_error / sigma_hat if sigma_hat  0 else np.inf\n        \n        if np.abs(z_t_plus_1) = c:\n            z_indicators[t + 1] = 1\n\n    # --- 3. Search for Break Index ---\n    \n    # Search for the first run of L consecutive indicators\n    # The search range for the start of the run (s_star)\n    # is from T_min+1 to T-L (inclusive).\n    for s_star in range(T_min + 1, T - L + 1):\n        # Check for a run of length L\n        if np.all(z_indicators[s_star : s_star + L] == 1):\n            return s_star\n\n    return -1\n\nif __name__ == '__main__':\n    solve()\n```", "id": "2380077"}]}