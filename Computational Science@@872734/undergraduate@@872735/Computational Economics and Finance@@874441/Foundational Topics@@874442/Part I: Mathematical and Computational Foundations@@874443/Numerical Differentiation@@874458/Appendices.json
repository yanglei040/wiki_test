{"hands_on_practices": [{"introduction": "Before applying numerical differentiation to complex financial models, it is essential to understand the fundamental tools at our disposal. This exercise directly compares the accuracy of the three most common finite difference methods: the forward, backward, and central difference formulas. By calculating the approximation error for each method on a simple, smooth function, you will gain a clear, quantitative understanding of why the central difference approximation is generally superior and serves as the foundation for more advanced applications. [@problem_id:2191753]", "problem": "Consider the function $f(x) = x \\exp(-x)$. We are interested in approximating its derivative, $f'(x)$, at the point $x_0 = 1$. The quality of an approximation is measured by its absolute error, defined as the absolute difference between the approximated value and the true value of the derivative.\n\nYou are to compare three common finite difference formulas for approximating the derivative:\n1.  **Forward Difference Approximation**: $D_f(x_0, h) = \\frac{f(x_0+h) - f(x_0)}{h}$\n2.  **Backward Difference Approximation**: $D_b(x_0, h) = \\frac{f(x_0) - f(x_0-h)}{h}$\n3.  **Central Difference Approximation**: $D_c(x_0, h) = \\frac{f(x_0+h) - f(x_0-h)}{2h}$\n\nLet $E_f$, $E_b$, and $E_c$ be the absolute errors corresponding to the forward, backward, and central difference approximations, respectively, when using a step size of $h = 0.1$.\n\nCalculate the value of the ratio $R = \\frac{E_f + E_b}{E_c}$. Round your final answer to three significant figures.", "solution": "We begin with $f(x) = x \\exp(-x)$. Its derivative is obtained by the product rule:\n$$\nf'(x) = \\exp(-x) + x \\frac{d}{dx}\\big(\\exp(-x)\\big) = \\exp(-x) - x \\exp(-x) = \\exp(-x)\\,(1 - x).\n$$\nAt $x_{0} = 1$, this gives\n$$\nf'(1) = \\exp(-1)\\,(1 - 1) = 0.\n$$\nHence, for any finite difference approximation $D$, the absolute error at $x_{0}=1$ is simply the absolute value of the approximation:\n$$\nE_{f} = |D_{f}(1,h)|,\\quad E_{b} = |D_{b}(1,h)|,\\quad E_{c} = |D_{c}(1,h)|.\n$$\n\nWith $h = 0.1$, compute the three finite differences using $f(x) = x \\exp(-x)$:\n- Forward difference:\n$$\nD_{f}(1,h) = \\frac{f(1+h) - f(1)}{h} = \\frac{(1.1)\\exp(-1.1) - \\exp(-1)}{0.1}.\n$$\n- Backward difference:\n$$\nD_{b}(1,h) = \\frac{f(1) - f(1-h)}{h} = \\frac{\\exp(-1) - (0.9)\\exp(-0.9)}{0.1}.\n$$\n- Central difference:\n$$\nD_{c}(1,h) = \\frac{f(1+h) - f(1-h)}{2h} = \\frac{(1.1)\\exp(-1.1) - (0.9)\\exp(-0.9)}{0.2}.\n$$\n\nFactor $\\exp(-1)$ by writing $\\exp(-1.1) = \\exp(-1)\\exp(-0.1)$ and $\\exp(-0.9) = \\exp(-1)\\exp(0.1)$. Define $t = \\exp(0.1)$ so that $\\exp(-0.1) = 1/t$. Then\n$$\nD_{f}(1,h) = \\frac{\\exp(-1)}{0.1}\\left(1.1\\,\\frac{1}{t} - 1 \\right),\\quad\nD_{b}(1,h) = \\frac{\\exp(-1)}{0.1}\\left(1 - 0.9\\,t \\right),\n$$\n$$\nD_{c}(1,h) = \\frac{\\exp(-1)}{0.2}\\left(1.1\\,\\frac{1}{t} - 0.9\\,t \\right).\n$$\nLet\n$$\nA = 1.1\\,\\frac{1}{t} - 1,\\quad B = 1 - 0.9\\,t,\\quad C = 1.1\\,\\frac{1}{t} - 0.9\\,t.\n$$\nThen\n$$\nE_{f} = \\frac{\\exp(-1)}{0.1}\\,|A|,\\quad E_{b} = \\frac{\\exp(-1)}{0.1}\\,|B|,\\quad E_{c} = \\frac{\\exp(-1)}{0.2}\\,|C|.\n$$\nTherefore, the ratio simplifies to\n$$\nR = \\frac{E_{f} + E_{b}}{E_{c}} = \\frac{\\frac{\\exp(-1)}{0.1}(|A|+|B|)}{\\frac{\\exp(-1)}{0.2}|C|} = 2\\,\\frac{|A|+|B|}{|C|}.\n$$\n\nNumerically, with $t = \\exp(0.1) \\approx 1.1051701859880927$ and $1/t = \\exp(-0.1) \\approx 0.9048374180359596$,\n$$\nA = 1.1\\cdot 0.9048374180359596 - 1 \\approx -0.00467884016044447,\\quad |A| \\approx 0.00467884016044447,\n$$\n$$\nB = 1 - 0.9\\cdot 1.1051701859880927 \\approx 0.00534683261071661,\\quad |B| \\approx 0.00534683261071661,\n$$\n$$\nC = 1.1\\cdot 0.9048374180359596 - 0.9\\cdot 1.1051701859880927 \\approx 0.000667992450272143,\\quad |C| \\approx 0.000667992450272143.\n$$\nHence,\n$$\nR = 2\\,\\frac{0.00467884016044447 + 0.00534683261071661}{0.000667992450272143} \\approx \\frac{0.02005134554232216}{0.000667992450272143} \\approx 30.0173.\n$$\nRounded to three significant figures,\n$$\nR \\approx 30.0\n$$", "answer": "$$\\boxed{30.0}$$", "id": "2191753"}, {"introduction": "Theoretical models often rely on smooth, infinitely differentiable functions, but many practical applications in finance, such as an option's payoff, involve functions with 'kinks' where the derivative is undefined. This hands-on exercise investigates the behavior of numerical differentiation schemes when applied at such a non-differentiable point. Understanding how left, right, and central differences behave near a kink is critical for correctly implementing and interpreting numerical derivatives of realistic financial payoffs. [@problem_id:2415141]", "problem": "Consider the piecewise-linear payoff function from financial economics defined by the European call option payoff at maturity, given by $f(x) = \\max(x - K, 0)$, where $x$ denotes the underlying price and $K$ is the strike. This function is continuous everywhere but not differentiable at $x = K$. Take $K = 100$. For a point $x$ and a positive step size $h$, define the following three finite difference quotients based on first principles of the derivative as a limit of difference quotients: the left (backward) difference $L(x;h) = \\dfrac{f(x) - f(x - h)}{h}$, the symmetric (central) difference $D(x;h) = \\dfrac{f(x + h) - f(x - h)}{2h}$, and the right (forward) difference $R(x;h) = \\dfrac{f(x + h) - f(x)}{h}$. Your task is to implement a program that evaluates $L(x;h)$, $D(x;h)$, and $R(x;h)$ for each of the test cases below using the specified $f(x)$ and $K$.\n\nUse the following test suite with pairs $(x,h)$:\n- $(x,h) = (90.0, 0.1)$\n- $(x,h) = (110.0, 0.1)$\n- $(x,h) = (100.0, 0.1)$\n- $(x,h) = (99.99, 0.1)$\n- $(x,h) = (100.01, 0.1)$\n- $(x,h) = (100.0, 10^{-6})$\n- $(x,h) = (99.999999, 10^{-8})$\n- $(x,h) = (100.000001, 10^{-8})$\n\nFor each test case, compute and record three floats $L(x;h)$, $D(x;h)$, and $R(x;h)$. Round each reported float to $8$ decimal places. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered by concatenating the triples for the test cases in the order given, i.e., output\n$[L(x_1;h_1), D(x_1;h_1), R(x_1;h_1), L(x_2;h_2), D(x_2;h_2), R(x_2;h_2), \\ldots, L(x_8;h_8), D(x_8;h_8), R(x_8;h_8)]$.\n\nThe final output must be exactly one line in this format. No physical units or angles are involved; all reported quantities are pure numbers. Each individual reported value must be a float rounded to $8$ decimal places.", "solution": "The problem statement is valid. It presents a well-posed computational task rooted in the fundamental principles of numerical analysis and financial mathematics. The problem is self-contained, objective, and scientifically sound. We shall proceed with a rigorous solution.\n\nThe problem requires the evaluation of three finite difference quotients for the European call option payoff function, $f(x)$, defined as:\n$$f(x) = \\max(x - K, 0)$$\nwhere $x$ is the price of the underlying asset and $K$ is the strike price. For this problem, we are given $K = 100$. This function is continuous for all $x \\in \\mathbb{R}$. However, it is not differentiable at the single point $x = K$. The analytical derivative, $f'(x)$, where it exists, is a step function:\n$$\nf'(x) = \n\\begin{cases} \n0 & \\text{if } x < K \\\\\n1 & \\text{if } x > K \n\\end{cases}\n$$\nAt $x=K$, the left-hand derivative is $0$ and the right-hand derivative is $1$.\n\nThe three finite difference schemes to be evaluated are:\n1.  **Left (Backward) Difference**: $L(x;h) = \\dfrac{f(x) - f(x - h)}{h}$\n2.  **Symmetric (Central) Difference**: $D(x;h) = \\dfrac{f(x + h) - f(x - h)}{2h}$\n3.  **Right (Forward) Difference**: $R(x;h) = \\dfrac{f(x + h) - f(x)}{h}$\n\nWe will now compute the triplet $(L(x;h), D(x;h), R(x;h))$ for each of the $8$ test cases. The analysis is organized by the position of the evaluation interval $[x-h, x+h]$ relative to the kink at $K=100$.\n\n**Case Group 1: Evaluation interval lies entirely to the left of the kink ($x+h \\le K$)**\nIn this region, $f(x-h) = f(x) = f(x+h) = 0$.\n- **Test Case 1: $(x,h) = (90.0, 0.1)$**: Here, $x-h=89.9$, $x=90.0$, and $x+h=90.1$. All points are less than $100$.\n  $L = \\frac{0-0}{0.1} = 0$, $D = \\frac{0-0}{0.2} = 0$, $R = \\frac{0-0}{0.1} = 0$.\n  Result: $(0.0, 0.0, 0.0)$.\n- **Test Case 7: $(x,h) = (99.999999, 10^{-8})$**: Here, $x=100-10^{-6}$ and $h=10^{-8}$.\n  $x+h = 100 - 10^{-6} + 10^{-8} = 100 - 0.99 \\times 10^{-6} = 99.99999901 < 100$.\n  All evaluation points are less than or equal to $100$, where the function value is $0$.\n  $L = \\frac{0-0}{10^{-8}} = 0$, $D = \\frac{0-0}{2 \\times 10^{-8}} = 0$, $R = \\frac{0-0}{10^{-8}} = 0$.\n  Result: $(0.0, 0.0, 0.0)$.\n\n**Case Group 2: Evaluation interval lies entirely to the right of the kink ($x-h > K$)**\nIn this region, $f(z) = z - K$, a linear function with slope $1$. The finite difference formulas should recover this slope exactly.\n- **Test Case 2: $(x,h) = (110.0, 0.1)$**: Here, $x-h=109.9 > 100$.\n  $f(x)=10$, $f(x-h)=9.9$, $f(x+h)=10.1$.\n  $L = \\frac{10-9.9}{0.1} = 1$, $D = \\frac{10.1-9.9}{0.2} = 1$, $R = \\frac{10.1-10}{0.1} = 1$.\n  Result: $(1.0, 1.0, 1.0)$.\n- **Test Case 8: $(x,h) = (100.000001, 10^{-8})$**: Here, $x=100+10^{-6}$ and $h=10^{-8}$.\n  $x-h = 100 + 10^{-6} - 10^{-8} = 100 + 0.99 \\times 10^{-6} > 100$.\n  The function is $f(z) = z-100$.\n  $L = \\frac{f(x)-f(x-h)}{h} = \\frac{(x-100)-((x-h)-100)}{h} = \\frac{h}{h} = 1$.\n  $R = \\frac{f(x+h)-f(x)}{h} = \\frac{((x+h)-100)-(x-100)}{h} = \\frac{h}{h} = 1$.\n  $D = \\frac{f(x+h)-f(x-h)}{2h} = \\frac{((x+h)-100)-((x-h)-100)}{2h} = \\frac{2h}{2h} = 1$.\n  Result: $(1.0, 1.0, 1.0)$.\n\n**Case Group 3: Evaluation interval straddles the kink ($x-h < K < x+h$)**\nThis is the most instructive case, as the non-differentiability affects the approximations.\n- **Test Case 3: $(x,h) = (100.0, 0.1)$** and **Test Case 6: $(x,h) = (100.0, 10^{-6})$**:\n  Here, $x=K=100$.\n  $f(x) = f(100) = 0$.\n  $f(x-h) = f(100-h) = 0$ since $100-h < 100$.\n  $f(x+h) = f(100+h) = (100+h)-100 = h$ since $100+h > 100$.\n  $L = \\frac{0-0}{h} = 0$. This approximates the left-hand derivative.\n  $R = \\frac{h-0}{h} = 1$. This approximates the right-hand derivative.\n  $D = \\frac{h-0}{2h} = 0.5$. This is the average of the left and right derivatives.\n  For both cases, the result is: $(0.0, 0.5, 1.0)$.\n\n- **Test Case 4: $(x,h) = (99.99, 0.1)$**: The interval is $[99.89, 100.09]$.\n  $x < K$, so $f(x)=0$. $x-h < K$, so $f(x-h)=0$.\n  $x+h = 100.09 > K$, so $f(x+h) = 100.09 - 100 = 0.09$.\n  $L = \\frac{0-0}{0.1} = 0$.\n  $R = \\frac{0.09-0}{0.1} = 0.9$.\n  $D = \\frac{0.09-0}{2 \\times 0.1} = 0.45$.\n  Result: $(0.0, 0.45, 0.9)$.\n\n- **Test Case 5: $(x,h) = (100.01, 0.1)$**: The interval is $[99.91, 100.11]$.\n  $x-h = 99.91 < K$, so $f(x-h)=0$.\n  $x = 100.01 > K$, so $f(x)=100.01-100=0.01$.\n  $x+h = 100.11 > K$, so $f(x+h)=100.11-100=0.11$.\n  $L = \\frac{0.01-0}{0.1} = 0.1$.\n  $R = \\frac{0.11-0.01}{0.1} = 1$.\n  $D = \\frac{0.11-0}{2 \\times 0.1} = 0.55$.\n  Result: $(0.1, 0.55, 1.0)$.\n\n**Summary of Results**\nThe computed values for $(L, D, R)$ for each test case, prior to final formatting, are:\n1.  $(90.0, 0.1) \\rightarrow (0.0, 0.0, 0.0)$\n2.  $(110.0, 0.1) \\rightarrow (1.0, 1.0, 1.0)$\n3.  $(100.0, 0.1) \\rightarrow (0.0, 0.5, 1.0)$\n4.  $(99.99, 0.1) \\rightarrow (0.0, 0.45, 0.9)$\n5.  $(100.01, 0.1) \\rightarrow (0.1, 0.55, 1.0)$\n6.  $(100.0, 10^{-6}) \\rightarrow (0.0, 0.5, 1.0)$\n7.  $(99.999999, 10^{-8}) \\rightarrow (0.0, 0.0, 0.0)$\n8.  $(100.000001, 10^{-8}) \\rightarrow (1.0, 1.0, 1.0)$\n\nThe following program implements this logic and formats the output as specified.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Evaluates left, central, and right finite difference quotients for the\n    European call option payoff function f(x) = max(x - K, 0).\n    \"\"\"\n    # Define the strike price constant.\n    K = 100.0\n\n    def f(x_val):\n        \"\"\"\n        Calculates the European call option payoff.\n        Args:\n            x_val (float or np.ndarray): The underlying price(s).\n        Returns:\n            float or np.ndarray: The payoff(s).\n        \"\"\"\n        return np.maximum(x_val - K, 0)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (90.0, 0.1),\n        (110.0, 0.1),\n        (100.0, 0.1),\n        (99.99, 0.1),\n        (100.01, 0.1),\n        (100.0, 1e-6),\n        (99.999999, 1e-8),\n        (100.000001, 1e-8),\n    ]\n\n    results = []\n    for x, h in test_cases:\n        # Evaluate the function at the required points.\n        fx = f(x)\n        fx_minus_h = f(x - h)\n        fx_plus_h = f(x + h)\n\n        # Calculate the three finite difference quotients.\n        # L(x;h) = (f(x) - f(x - h)) / h\n        L = (fx - fx_minus_h) / h\n        \n        # D(x;h) = (f(x + h) - f(x - h)) / (2 * h)\n        D = (fx_plus_h - fx_minus_h) / (2 * h)\n        \n        # R(x;h) = (f(x + h) - f(x)) / h\n        R = (fx_plus_h - fx) / h\n\n        # Append the formatted results to the list.\n        # The problem requires rounding each float to 8 decimal places.\n        # Using f-string formatting for precise output.\n        results.append(f\"{L:.8f}\")\n        results.append(f\"{D:.8f}\")\n        results.append(f\"{R:.8f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2415141"}, {"introduction": "The choice of the step size $h$ is perhaps the most critical practical decision in numerical differentiation, involving a trade-off between two competing sources of error. A large $h$ leads to significant truncation error from the approximation itself, while a very small $h$ can lead to catastrophic round-off error due to the limits of floating-point arithmetic. This practice guides you through an empirical investigation to find the optimal step size $h$ that minimizes the total error when calculating Vega, a key option Greek, demonstrating a vital skill for any computational practitioner. [@problem_id:2415200]", "problem": "Consider a European call option on a non-dividend-paying asset priced under the Black–Scholes framework. Let the spot price be $S_0$, the strike be $K$, the continuously compounded risk-free rate be $r$, the time to maturity be $T$, and the volatility be $\\sigma$. The Black–Scholes call price is\n$$\nC(S_0,K,r,T,\\sigma)=S_0\\,\\Phi(d_1)-K e^{-rT}\\,\\Phi(d_2),\n$$\nwhere\n$$\nd_1=\\frac{\\ln\\!\\left(\\frac{S_0}{K}\\right)+\\left(r+\\frac{1}{2}\\sigma^2\\right)T}{\\sigma\\sqrt{T}},\\quad d_2=d_1-\\sigma\\sqrt{T},\n$$\nand $\\Phi(\\cdot)$ is the cumulative distribution function of the standard normal distribution. The analytic Vega of the call is the partial derivative of the price with respect to $\\sigma$ given by\n$$\n\\text{Vega}(S_0,K,r,T,\\sigma)=\\frac{\\partial C}{\\partial \\sigma}=S_0 \\sqrt{T}\\,\\varphi(d_1),\n$$\nwhere $\\varphi(\\cdot)$ is the standard normal probability density function.\n\nDefine a numerical estimator of Vega at a given parameter vector $(S_0,K,r,T,\\sigma)$ by the symmetric difference quotient applied to the volatility,\n$$\n\\widehat{\\text{Vega}}(h)=\\frac{C(S_0,K,r,T,\\sigma+h)-C(S_0,K,r,T,\\sigma-h)}{2h},\n$$\nfor a step size $h>0$ such that $\\sigma-h>0$. For a given $(S_0,K,r,T,\\sigma)$ and a candidate set of step sizes\n$$\n\\mathcal{H}=\\{10^{-12},10^{-11},10^{-10},10^{-9},10^{-8},10^{-7},10^{-6},10^{-5},10^{-4},10^{-3},10^{-2}\\},\n$$\nconsider the admissible subset\n$$\n\\mathcal{H}_{\\text{adm}}=\\{h\\in\\mathcal{H}\\,:\\,0<h<\\sigma/2\\}.\n$$\nDefine the total error at step $h\\in\\mathcal{H}_{\\text{adm}}$ as the absolute error\n$$\nE(h)=\\left|\\widehat{\\text{Vega}}(h)-\\text{Vega}(S_0,K,r,T,\\sigma)\\right|.\n$$\nYour task is to determine, for each specified test case, the step size $h^\\star\\in\\mathcal{H}_{\\text{adm}}$ that empirically minimizes $E(h)$. If there are multiple minimizers, choose the largest $h$ among those that achieve the minimum error.\n\nUse the following test suite of near-the-money cases:\n1. $(S_0,K,r,T,\\sigma)=(100,100,0.01,1.0,0.2)$.\n2. $(S_0,K,r,T,\\sigma)=(100,100,0.01,0.01,0.2)$.\n3. $(S_0,K,r,T,\\sigma)=(100,102,0.02,0.5,0.05)$.\n4. $(S_0,K,r,T,\\sigma)=(100,100,0.03,2.0,0.6)$.\n\nAll quantities are non-dimensional financial parameters, so no physical units are required. Angles are not involved. The final answers for each test case must be returned as floating-point numbers.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[h_1,h_2,h_3,h_4]$), where each $h_i$ is the selected step size $h^\\star$ for test case $i$.", "solution": "The problem statement has been rigorously validated and is deemed valid. It is scientifically grounded in the Black-Scholes option pricing theory and the principles of numerical analysis. It is well-posed, with all necessary parameters, formulae, and a clear, unambiguous objective. The problem is free from contradictions and subjective claims. Therefore, a solution will be presented.\n\nThe task is to find the optimal step size $h^\\star$ for the numerical estimation of an option's Vega. Vega is the first partial derivative of the option price with respect to the volatility, $\\sigma$. The numerical estimation is performed using the symmetric difference quotient. The optimal step size $h^\\star$ is defined as the value from a given discrete set $\\mathcal{H}_{\\text{adm}}$ that minimizes the absolute error between the numerical estimate and the analytical value of Vega.\n\nThe methodology proceeds as follows for each test case defined by the parameter set $(S_0, K, r, T, \\sigma)$:\n\n1.  **Analytic Vega Calculation**: First, the exact value of Vega is computed using the provided analytical formula:\n    $$ \\text{Vega}(S_0,K,r,T,\\sigma) = S_0 \\sqrt{T}\\,\\varphi(d_1) $$\n    where $\\varphi(\\cdot)$ is the probability density function (PDF) of the standard normal distribution, and $d_1$ is given by:\n    $$ d_1=\\frac{\\ln(S_0/K)+\\left(r+\\frac{1}{2}\\sigma^2\\right)T}{\\sigma\\sqrt{T}} $$\n    This analytical value serves as the benchmark against which the numerical estimates are compared.\n\n2.  **Numerical Estimation and Error Calculation**: For each step size $h$ in the admissible set $\\mathcal{H}_{\\text{adm}}$, we compute the numerical approximation of Vega, denoted $\\widehat{\\text{Vega}}(h)$, using the symmetric difference quotient:\n    $$ \\widehat{\\text{Vega}}(h)=\\frac{C(S_0,K,r,T,\\sigma+h)-C(S_0,K,r,T,\\sigma-h)}{2h} $$\n    Here, $C(\\cdot)$ is the Black-Scholes call price function:\n    $$ C(S_0,K,r,T,\\sigma)=S_0\\,\\Phi(d_1)-K e^{-rT}\\,\\Phi(d_2) $$\n    with $\\Phi(\\cdot)$ being the cumulative distribution function (CDF) of the standard normal distribution, and $d_2 = d_1 - \\sigma\\sqrt{T}$. The absolute error for each $h$ is then calculated as:\n    $$ E(h)=\\left|\\widehat{\\text{Vega}}(h)-\\text{Vega}(S_0,K,r,T,\\sigma)\\right| $$\n\n3.  **Optimal Step Size Selection**: The set of candidate step sizes is given by $\\mathcal{H} = \\{10^{-12}, 10^{-11}, \\dots, 10^{-2}\\}$. The admissible set $\\mathcal{H}_{\\text{adm}}$ is defined as $\\{h\\in\\mathcal{H}\\,:\\,0<h<\\sigma/2\\}$. For all provided test cases, the condition $h < \\sigma/2$ is satisfied for all $h \\in \\mathcal{H}$, thus $\\mathcal{H}_{\\text{adm}} = \\mathcal{H}$. We compute the error $E(h)$ for every $h \\in \\mathcal{H}$. We then identify the minimum error, $E_{\\min} = \\min_{h \\in \\mathcal{H}} E(h)$. The optimal step size, $h^\\star$, is the largest value of $h$ from the set of all step sizes that produce this minimum error.\n\nThis process reflects a fundamental trade-off in numerical differentiation. The total error $E(h)$ is a composite of two opposing sources:\n- **Truncation Error**: This error is inherent to the finite difference approximation. For a symmetric difference quotient, it is of order $O(h^2)$. It decreases as $h$ becomes smaller.\n- **Round-off Error**: This error arises from the finite precision of floating-point arithmetic, particularly the catastrophic cancellation when subtracting nearly identical values ($C(\\sigma+h)$ and $C(\\sigma-h)$) and subsequent division by a small $h$. This component is roughly proportional to $\\epsilon_{mach}/h$, where $\\epsilon_{mach}$ is machine epsilon. It increases as $h$ becomes smaller.\n\nThe sum of these two errors results in a total error curve that typically has a U-shape when plotted against $h$ on a logarithmic scale. The empirical search across the discrete set $\\mathcal{H}$ allows us to find the step size that is closest to the bottom of this curve, representing the optimal balance between truncation and round-off errors.\n\nThe implementation will consist of a main function that iterates through each test case. For each case, it will calculate the errors for all candidate step sizes, identify the minimum error, and select the corresponding optimal step size according to the specified tie-breaking rule. The required functions from `scipy.stats.norm` will be used to evaluate the standard normal CDF ($\\Phi$) and PDF ($\\varphi$).", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\n# from scipy import ...\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (100.0, 100.0, 0.01, 1.0, 0.2),\n        (100.0, 100.0, 0.01, 0.01, 0.2),\n        (100.0, 102.0, 0.02, 0.5, 0.05),\n        (100.0, 100.0, 0.03, 2.0, 0.6),\n    ]\n\n    # Candidate set of step sizes H\n    H = [10**-12, 10**-11, 10**-10, 10**-9, 10**-8, 10**-7,\n         10**-6, 10**-5, 10**-4, 10**-3, 10**-2]\n\n    # Function to calculate Black-Scholes call price\n    def black_scholes_call(S0, K, r, T, sigma):\n        # Handle edge cases for sigma and T to avoid mathematical errors\n        if sigma <= 0 or T <= 0:\n            # If T > 0 and sigma = 0, the option price is deterministic.\n            # If T = 0, the option price is its intrinsic value.\n            return max(0.0, S0 - K * np.exp(-r * T))\n            \n        d1 = (np.log(S0 / K) + (r + 0.5 * sigma**2) * T) / (sigma * np.sqrt(T))\n        d2 = d1 - sigma * np.sqrt(T)\n        price = S0 * norm.cdf(d1) - K * np.exp(-r * T) * norm.cdf(d2)\n        return price\n\n    # Function to calculate analytical Vega\n    def analytical_vega(S0, K, r, T, sigma):\n        # Vega is zero if T=0 or sigma=0\n        if sigma <= 0 or T <= 0:\n            return 0.0\n        d1 = (np.log(S0 / K) + (r + 0.5 * sigma**2) * T) / (sigma * np.sqrt(T))\n        vega = S0 * np.sqrt(T) * norm.pdf(d1)\n        return vega\n\n    optimal_h_results = []\n\n    for case in test_cases:\n        S0, K, r, T, sigma = case\n\n        # Determine the admissible set of step sizes.\n        # For all test cases provided, sigma/2 is greater than the largest h_cand in H,\n        # so the admissible set is the full set H.\n        H_adm = [h for h in H if 0 < h < sigma / 2.0]\n\n        # Calculate the true analytical Vega to use as a benchmark\n        true_vega = analytical_vega(S0, K, r, T, sigma)\n\n        errors_with_h = []\n        for h in H_adm:\n            # Calculate numerical Vega using symmetric difference quotient\n            C_plus = black_scholes_call(S0, K, r, T, sigma + h)\n            C_minus = black_scholes_call(S0, K, r, T, sigma - h)\n            numerical_vega = (C_plus - C_minus) / (2.0 * h)\n            \n            # Calculate the absolute error\n            error = abs(numerical_vega - true_vega)\n            errors_with_h.append((error, h))\n\n        # Find the minimum error among all step sizes\n        min_error = min(e for e, h in errors_with_h)\n\n        # Find all step sizes that achieve this minimum error\n        minimizing_hs = [h for e, h in errors_with_h if e == min_error]\n\n        # Select the largest h among the minimizers as per the tie-breaking rule\n        optimal_h = max(minimizing_hs)\n        optimal_h_results.append(optimal_h)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, optimal_h_results))}]\")\n\nsolve()\n```", "id": "2415200"}]}