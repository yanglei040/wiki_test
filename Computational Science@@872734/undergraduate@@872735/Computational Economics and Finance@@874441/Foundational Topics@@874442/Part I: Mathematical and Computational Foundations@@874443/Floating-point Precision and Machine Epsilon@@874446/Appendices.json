{"hands_on_practices": [{"introduction": "This first exercise takes you to the heart of floating-point precision. We will investigate the smallest positive interest rate $r$ that a computer can distinguish from zero when calculating the value of $(1+r)$. This hands-on search for the \"minimum detectable rate\" provides a concrete, financial interpretation of the fundamental concept of machine epsilon, revealing the granular nature of numbers inside a computer [@problem_id:2394219].", "problem": "You are tasked with writing a complete, runnable program that determines how finite-precision arithmetic affects the detectability of small interest rates in single-precision floating point arithmetic. Work within the International Electrotechnical Commission (IEC) and Institute of Electrical and Electronics Engineers (IEEE) Standard for Floating-Point Arithmetic (IEEE 754) model for single precision (base-$2$, rounding to nearest with ties to even) and treat interest rates as pure decimals (no percent signs). In this model, the representable numbers near $1$ are discretely spaced, and an interest rate $r$ is computationally distinguishable from $0$ at unit scale if and only if the single-precision result of adding $r$ to $1$ is strictly greater than $1$. Your objective is to infer the smallest such $r > 0$ robustly, using only floating-point operations that themselves obey single-precision semantics.\n\nFundamental base to use:\n- IEEE 754 single-precision arithmetic uses base-$2$ with a fixed precision and rounds results to the nearest representable value, with ties resolved toward the even significand. You may assume this rounding rule, monotonicity of rounding, and the existence of subnormal numbers as well-tested facts.\n\nConstraints:\n- All additions, multiplications, and divisions intended to reflect single-precision behavior must be performed in single precision. Do not rely on analytical formulas for the answer. Instead, implement an algorithmic search based on the IEEE 754 rounding model to find the minimal $r > 0$ such that the single-precision computation of $1 + r$ is strictly greater than $1$.\n- Express interest rates as decimals (e.g., write $0.05$ for five percent). No units are required.\n\nRequired tasks to implement:\n1. Compute the smallest rate $r > 0$ such that the single-precision result of $1 + r$ is strictly greater than $1$. Use an iterative halving (bisection-style) approach driven entirely by single-precision operations: start from a single-precision $r$ and decrease $r$ until adding half of it no longer increases $1$ in single precision; the last $r$ that still increased $1$ when halved is the desired value.\n2. Verify the boundary condition by checking that with $r_{\\min}$ from step $1$, the single-precision result of $1 + \\frac{r_{\\min}}{2}$ equals $1$.\n3. Cross-check that the computed $r_{\\min}$ matches the single-precision spacing immediately above $1$ obtained from a trusted floating-point metadata source for single precision.\n4. Confirm that adding the smallest positive subnormal single-precision number $s$ to $1$ leaves the single-precision result equal to $1$.\n5. Applied finance check: for a single payoff of magnitude $C = 1$ received at time $T = 360$ (interpreted as months, but with no units required), compute in single precision the difference $\\frac{1}{(1 + r_{\\min})^{T}} - 1$ and return this single number. Report the value as a decimal number (not a percentage).\n\nTest suite and expected outputs:\n- The program must compute the following five outputs in order, aggregating them into a single list:\n  - Case A (happy path): the value of $r_{\\min}$ from task $1$ as a floating-point number.\n  - Case B (boundary condition): a boolean for whether $1 + \\frac{r_{\\min}}{2} = 1$ in single precision.\n  - Case C (consistency): a boolean for whether $r_{\\min}$ equals the single-precision metadata spacing above $1$.\n  - Case D (edge case): a boolean for whether $1 + s = 1$ in single precision for the smallest positive subnormal $s$.\n  - Case E (applied): the value of $\\frac{1}{(1 + r_{\\min})^{360}} - 1$ as a floating-point number, computed entirely in single precision.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result\\_A,result\\_B,result\\_C,result\\_D,result\\_E]$). The list must contain exactly the five values described above, in that order. All floating-point arithmetic relevant to tasks $1$ through $5$ must be performed in single precision, and all comparisons must reflect single-precision results. All outputs must be reported as plain numbers or booleans, without units and without percent signs.", "solution": "The problem requires an investigation into the limitations of finite-precision arithmetic, specifically the IEEE 754 single-precision standard, in the context of financial calculations involving small interest rates. The core task is to determine the smallest positive interest rate $r$, denoted $r_{\\min}$, that is computationally distinguishable from $0$ when added to $1$. This is equivalent to finding the smallest positive single-precision number $r_{\\min}$ such that the floating-point operation $fl(1 + r_{\\min})$ yields a result strictly greater than $1$. We will then use this value to perform several verification checks and a simple financial calculation.\n\nIn the IEEE 754 standard for single-precision floating-point numbers (binary32), a number is represented using a sign bit, an 8-bit biased exponent, and a 23-bit fraction. A normalized number is of the form $(-1)^S \\times 2^{E-127} \\times (1.f)_{2}$, where $S$ is the sign bit, $E$ is the exponent, and $f$ is the fraction.\n\nThe number $1.0$ is represented exactly in this system. Its sign is $0$, its unbiased exponent is $0$ (so the biased exponent $E$ is $127$), and its fraction is all zeros. The significand is implicitly $1.0$. The next larger representable number has the same exponent but increments the last bit of the fraction. This number is $1 + 2^{-23}$. The gap between $1.0$ and the next representable number is therefore $2^{-23}$. This quantity is known as the Unit in the Last Place, or $ulp(1.0)$. Any positive number $x  ulp(1.0)/2 = 2^{-24}$ added to $1.0$ will be rounded back down to $1.0$ by the default round-to-nearest-ties-to-even rule. If $x = 2^{-24}$, a tie occurs, and the result is rounded to the \"even\" significand, which is that of $1.0$. Therefore, the smallest number $r_{\\min}$ such that $fl(1 + r_{\\min}) > 1$ must be $ulp(1.0) = 2^{-23}$.\n\nThe problem mandates an algorithmic discovery of this value, not its analytical derivation.\n\n**Task 1: Algorithmic Computation of $r_{\\min}$**\n\nWe are instructed to find $r_{\\min}$ using an iterative search. The search must find the smallest number $r$ such that $fl(1+r) > 1$. The method described is a power-of-two descent, which we implement by starting with a candidate $r = 1.0$ and repeatedly halving it. The logic is to find the value $r$ for which $fl(1 + r) > 1$ but $fl(1 + r/2) = 1$. This can be implemented with a loop that continues as long as adding half of the current candidate $r$ to $1$ still produces a result greater than $1$. All arithmetic must be performed in single precision.\n\nLet $r_0 = 1.0_{f32}$. We generate a sequence $r_{k+1} = r_k / 2.0_{f32}$. The loop continues as long as $fl(1.0_{f32} + fl(r_k / 2.0_{f32})) > 1.0_{f32}$. When the condition fails, the current $r_k$ is our desired $r_{\\min}$.\n\n**Task 2: Verification of the Boundary Condition**\n\nBy the construction in Task 1, $r_{\\min}$ is the smallest value in the sequence $1, 1/2, 1/4, \\ldots$ for which $fl(1+r_{\\min})>1$. The algorithm terminates precisely when $fl(1 + r_{\\min}/2) = 1$. This check serves to confirm that our algorithm correctly identified the boundary of machine precision relative to $1$. We will compute $fl(1.0_{f32} + fl(r_{\\min} / 2.0_{f32}))$ and verify it equals $1.0_{f32}$.\n\n**Task 3: Cross-check with Floating-Point Metadata**\n\nThe value $r_{\\min}$ we have algorithmically determined should be identical to the fundamental machine constant known as machine epsilon for single precision, which is the distance from $1.0$ to the next larger representable floating-point number. Standard numerical libraries provide access to this value. We will compare our computed $r_{\\min}$ to the value of `numpy.spacing(numpy.float32(1.0))`, which is a trusted source for $ulp(1.0)$. This confirms the correctness of our algorithm and understanding.\n\n**Task 4: Edge Case with Smallest Subnormal Number**\n\nSubnormal (or denormalized) numbers are used to represent values smaller than the smallest normalized number, filling the gap between $0$ and $\\pm 2^{E_{min}}$. The smallest positive single-precision subnormal number, let's call it $s$, is $2^{-149}$. This value is astronomically smaller than $r_{\\min} = 2^{-23}$. When we compute $fl(1.0 + s)$, the result is rounded to the nearest representable number. Since $s \\ll r_{\\min}/2$, the sum $1.0 + s$ is vastly closer to $1.0$ than to the next representable number, $1.0 + r_{\\min}$. Thus, the result of the floating-point addition must be $1.0$. This task verifies our understanding of how extremely small numbers are handled in floating-point addition.\n\n**Task 5: Applied Finance Check**\n\nThis task requires us to compute the effect of this minimal interest rate, $r_{\\min}$, over a long period. We must calculate the value of a financial expression, $D = \\frac{1}{(1 + r_{\\min})^T} - 1$, where the time period $T$ is $360$ (e.g., months in a $30$-year term). All operations must be performed in single precision. This demonstrates that even the smallest computationally detectable interest rate, when compounded, leads to a measurable deviation from a zero-rate scenario. The sequence of operations is:\n1.  Compute $v_1 = fl(1.0_{f32} + r_{\\min})$.\n2.  Compute $v_2 = fl(v_1^{360.0_{f32}})$. This is a repeated multiplication or a library power function, all in single precision.\n3.  Compute $v_3 = fl(1.0_{f32} / v_2)$.\n4.  Compute the final result $D = fl(v_3 - 1.0_{f32})$.\nThe result will be a small negative number, representing the present value discount factor's deviation from $1$ for a cash flow at time $T=360$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Performs a series of tasks related to single-precision floating-point\n    arithmetic and its application in finance, adhering to strict computational\n    and formatting rules.\n    \"\"\"\n\n    # Define single-precision constants to ensure all arithmetic is performed\n    # in the correct domain.\n    one_f32 = np.float32(1.0)\n    two_f32 = np.float32(2.0)\n\n    # --- Task 1: Compute the smallest rate r_min  0 ---\n    # We are looking for the smallest r  0 such that 1 + r  1 in single precision.\n    # The problem specifies a bisection-style search. We start with r=1 and\n    # repeatedly halve it until adding r/2 to 1 no longer produces a result  1.\n    # At that point, the current r is the minimal value, r_min.\n    r = one_f32\n    while (one_f32 + (r / two_f32))  one_f32:\n        r = r / two_f32\n    r_min = r\n    result_A = r_min\n\n    # --- Task 2: Verify the boundary condition ---\n    # Check that adding half of r_min to 1 results in 1, confirming r_min is\n    # the minimal representable increment.\n    is_boundary_correct = (one_f32 + (r_min / two_f32)) == one_f32\n    result_B = is_boundary_correct\n\n    # --- Task 3: Cross-check with floating-point metadata ---\n    # Compare r_min with the value of ULP(1.0) (unit in the last place) for\n    # single precision, also known as machine epsilon relative to 1.\n    # np.spacing(1.0) gives the distance between 1.0 and the next larger float.\n    spacing_at_one = np.spacing(one_f32)\n    is_consistent = (r_min == spacing_at_one)\n    result_C = is_consistent\n\n    # --- Task 4: Confirm edge case with smallest subnormal number ---\n    # The smallest positive single-precision number is np.finfo.tiny.\n    # Adding this to 1 should result in 1 due to rounding.\n    s_subnormal = np.finfo(np.float32).tiny\n    is_subnormal_rounded = (one_f32 + s_subnormal) == one_f32\n    result_D = is_subnormal_rounded\n\n    # --- Task 5: Applied finance check ---\n    # Compute the discount factor deviation over 360 periods using r_min.\n    # All calculations must be performed in single precision.\n    T = np.float32(360.0)\n    one_plus_r_min = one_f32 + r_min\n    # The power operation must also respect single precision.\n    # numpy's ** operator on float32 types maintains float32 precision.\n    compounded_factor = one_plus_r_min ** T\n    inverse_factor = one_f32 / compounded_factor\n    finance_result = inverse_factor - one_f32\n    result_E = finance_result\n\n    # Aggregate results into a list for final output.\n    results = [\n        result_A,\n        result_B,\n        result_C,\n        result_D,\n        result_E\n    ]\n\n    # Final print statement in the exact required format.\n    # The map(str, ...) converts each element, including booleans, to its\n    # string representation ('True', 'False', or the number as a string).\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2394219"}, {"introduction": "Now that we have a feel for the limits of precision, let's explore a common scenario where those limits can lead to incorrect financial valuations. This practice challenges you to value a floating-rate bond with a very small rate margin, demonstrating how a naive implementation can cause the margin's value to vanish due to rounding errors. By comparing this to a rearranged, numerically stable formula, you will learn a crucial technique for preserving accuracy in financial algorithms [@problem_id:2394271].", "problem": "You are asked to write a complete, runnable program that values a floating-rate bond under risk-neutral valuation while exposing the impact of floating-point precision and machine epsilon on the computation. Work from first principles of discounted cash flow under the risk-neutral measure: the present value is the sum over coupon dates of the discounted expected cash flows plus the discounted principal repayment. Use the following fundamental base.\n1. Present value under the risk-neutral measure: the present value equals the sum of cash flows multiplied by corresponding discount factors.\n2. A floating-rate coupon in period $i$ equals $(L_i + m)\\,\\alpha_i\\,N$, where $L_i$ is the given forward rate for period $i$, $m$ is a constant margin added to each reset, $\\alpha_i$ is the accrual fraction of the period, and $N$ is notional.\n3. When discount factors are constructed from simple-compounding forward rates $L_i$ and accruals $\\alpha_i$, they satisfy the recursion $D_0 = 1$ and $D_i = \\dfrac{D_{i-1}}{1 + L_i \\alpha_i}$ for $i = 1,\\dots,T$, where $D_i$ is the discount factor to time $i$ and $T$ is the total number of periods.\n4. The present value is then $PV = N \\left( \\sum_{i=1}^{T} (L_i + m)\\,\\alpha_i\\,D_i + D_T \\right)$, where $D_i$ are obtained from the above recursion and $D_T$ discounts the redemption of principal $N$.\n\nYour task is to implement two numerically distinct computations of the present value:\n- A naive evaluation that directly uses $(L_i + m)$ inside each coupon computation.\n- An algebraically equivalent evaluation that avoids adding a very small number to a much larger number by rearranging terms using identities valid in real arithmetic; in particular, compute the contribution of the margin without first adding it to $L_i$. You must adhere to the same discount factors $D_i$ for both methods.\n\nYour program must execute the following test suite, each test case being fully specified by the notional $N$, the margin $m$, the list or rule for forward rates $\\{L_i\\}_{i=1}^T$, and the list of accrual fractions $\\{\\alpha_i\\}_{i=1}^T$. The discount factors $D_i$ used for valuation in all cases must be derived from the given $L_i$ and $\\alpha_i$ via the recursion $D_0 = 1$, $D_i = \\dfrac{D_{i-1}}{1 + L_i \\alpha_i}$ for $i = 1,\\dots,T$.\n\nTest Case A (happy path, non-negligible margin):\n- $N = 1{,}000{,}000$.\n- $T = 8$ with quarterly accruals: $\\alpha_i = 0.25$ for all $i$.\n- Forward rates: $L = [0.021,\\, 0.022,\\, 0.0235,\\, 0.024,\\, 0.025,\\, 0.026,\\, 0.027,\\, 0.028]$.\n- Margin: $m = 0.0005$.\n\nTest Case B (margin far below the rounding unit near typical rates; illustrates loss due to floating-point precision):\n- $N = 1{,}000{,}000{,}000$.\n- $T = 8$ with quarterly accruals: $\\alpha_i = 0.25$ for all $i$.\n- Forward rates: use the same list as in Test Case A.\n- Margin: $m = 1 \\times 10^{-20}$.\n\nTest Case C (boundary: zero margin):\n- $N = 2{,}000{,}000$.\n- $T = 8$ with quarterly accruals: $\\alpha_i = 0.25$ for all $i$.\n- Forward rates: $L = [0.018,\\, 0.019,\\, 0.0195,\\, 0.020,\\, 0.0205,\\, 0.021,\\, 0.0215,\\, 0.022]$.\n- Margin: $m = 0$.\n\nTest Case D (many periods with small, gradually changing forward rates; tiny but not vanishing margin):\n- $N = 100{,}000{,}000$.\n- $T = 40$ with quarterly accruals: $\\alpha_i = 0.25$ for all $i$.\n- Forward rates given by the rule $L_i = 0.015 + 0.0001\\,i$ for $i = 1,\\dots,40$.\n- Margin: $m = 1 \\times 10^{-12}$.\n\nYour program must, for each test case, compute:\n- The naive present value $PV_{\\text{naive}} = N \\left( \\sum_{i=1}^{T} (L_i + m)\\,\\alpha_i\\,D_i + D_T \\right)$.\n- An alternative present value $PV_{\\text{stable}}$ computed without adding $m$ to $L_i$ inside the coupon term, while respecting algebraic equivalence in exact arithmetic and using the same $D_i$.\n- The absolute difference $\\lvert PV_{\\text{naive}} - PV_{\\text{stable}} \\rvert$.\n\nThere are no physical units in this problem. All rates must be treated as decimals, not as percentages. Angles are not involved. The final output format must be a single line containing a comma-separated list enclosed in square brackets, concatenating for the four test cases the triplets in order: $[PV_{\\text{naive}}^{(A)}, PV_{\\text{stable}}^{(A)}, \\lvert \\cdot \\rvert^{(A)}, PV_{\\text{naive}}^{(B)}, PV_{\\text{stable}}^{(B)}, \\lvert \\cdot \\rvert^{(B)}, PV_{\\text{naive}}^{(C)}, PV_{\\text{stable}}^{(C)}, \\lvert \\cdot \\rvert^{(C)}, PV_{\\text{naive}}^{(D)}, PV_{\\text{stable}}^{(D)}, \\lvert \\cdot \\rvert^{(D)}]$.", "solution": "The core of the problem is the comparison of two computational methods for the present value of a floating-rate bond.\n\nMethod 1: Naive Evaluation\nThe first method is a direct implementation of the provided formula:\n$$\nPV_{\\text{naive}} = N \\left( \\sum_{i=1}^{T} (L_i + m)\\,\\alpha_i\\,D_i + D_T \\right)\n$$\nHere, for each coupon period $i$, the margin $m$ is added to the forward rate $L_i$ before multiplication. When $m$ is significantly smaller than $L_i$, this addition can lead to a loss of floating-point precision. For example, in standard double-precision arithmetic (IEEE $754$), which has approximately $15$â€“$17$ decimal digits of precision, if $L_i \\approx 10^{-2}$ and $m = 10^{-20}$, the sum $L_i + m$ will be computationally indistinguishable from $L_i$. This phenomenon is known as absorption or swamping, where the smaller number is effectively lost.\n\nMethod 2: Stable Evaluation\nTo construct a more stable method, we must rearrange the formula to avoid the direct addition of $m$ to $L_i$. We begin with the same expression for present value and apply algebraic manipulation, which is exact in real arithmetic.\n$$\nPV = N \\left( \\sum_{i=1}^{T} (L_i + m)\\,\\alpha_i\\,D_i + D_T \\right)\n$$\nWe distribute the terms within the summation:\n$$\nPV = N \\left( \\sum_{i=1}^{T} L_i\\alpha_i D_i + \\sum_{i=1}^{T} m\\alpha_i D_i + D_T \\right)\n$$\nThis rearrangement separates the calculation involving $L_i$ from the contribution of the margin $m$. To further simplify and enhance numerical stability, we analyze the term $\\sum_{i=1}^{T} L_i\\alpha_i D_i$. The discount factors are defined by the recursion $D_i = \\frac{D_{i-1}}{1 + L_i \\alpha_i}$. Rearranging this gives $D_i(1 + L_i \\alpha_i) = D_{i-1}$, which leads to $D_i + L_i \\alpha_i D_i = D_{i-1}$. This provides the critical identity:\n$$\nL_i \\alpha_i D_i = D_{i-1} - D_i\n$$\nSubstituting this identity into the summation results in a telescoping series:\n$$\n\\sum_{i=1}^{T} L_i \\alpha_i D_i = \\sum_{i=1}^{T} (D_{i-1} - D_i) = (D_0 - D_1) + (D_1 - D_2) + \\ldots + (D_{T-1} - D_T) = D_0 - D_T\n$$\nBy definition, $D_0 = 1$. Therefore, the sum simplifies to:\n$$\n\\sum_{i=1}^{T} L_i \\alpha_i D_i = 1 - D_T\n$$\nNow, we substitute this result back into the expanded expression for $PV$:\n$$\nPV = N \\left( (1 - D_T) + m \\sum_{i=1}^{T} \\alpha_i D_i + D_T \\right)\n$$\nThe $D_T$ terms cancel, yielding the final, numerically stable formula:\n$$\nPV_{\\text{stable}} = N \\left( 1 + m \\sum_{i=1}^{T} \\alpha_i D_i \\right)\n$$\nThis formula is superior because it completely avoids the addition of $L_i$ and $m$. The total contribution of the margin is calculated separately by first summing the discounted accrual fractions and then multiplying by $m$. This preserves the precision of the minuscule margin term, ensuring its contribution to the final price is not lost due to floating-point arithmetic limitations.\n\nThe implementation will proceed as follows for each test case:\n1.  Establish the parameters $N$, $m$, $T$, and the arrays for $L_i$ and $\\alpha_i$.\n2.  Compute the array of discount factors $D_i$ for $i=0, \\dots, T$ using the specified recursion.\n3.  Calculate $PV_{\\text{naive}}$ using the first formula.\n4.  Calculate $PV_{\\text{stable}}$ using the derived second formula.\n5.  Compute the absolute difference between the two results.\n\nThis procedure will be executed for all four test cases, and the results will be formatted into the required output string.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the floating-rate bond valuation problem for all test cases\n    and prints the results in the specified format.\n    \"\"\"\n\n    def generate_test_cases():\n        \"\"\"\n        Generates and returns the test cases as a list of dictionaries.\n        \"\"\"\n        # Test Case A\n        case_a = {\n            \"N\": 1_000_000.0,\n            \"T\": 8,\n            \"alpha\": np.full(8, 0.25),\n            \"L\": np.array([0.021, 0.022, 0.0235, 0.024, 0.025, 0.026, 0.027, 0.028]),\n            \"m\": 0.0005\n        }\n\n        # Test Case B\n        case_b = {\n            \"N\": 1_000_000_000.0,\n            \"T\": 8,\n            \"alpha\": np.full(8, 0.25),\n            \"L\": np.array([0.021, 0.022, 0.0235, 0.024, 0.025, 0.026, 0.027, 0.028]),\n            \"m\": 1e-20\n        }\n\n        # Test Case C\n        case_c = {\n            \"N\": 2_000_000.0,\n            \"T\": 8,\n            \"alpha\": np.full(8, 0.25),\n            \"L\": np.array([0.018, 0.019, 0.0195, 0.020, 0.0205, 0.021, 0.0215, 0.022]),\n            \"m\": 0.0\n        }\n\n        # Test Case D\n        T_d = 40\n        L_d = np.array([0.015 + 0.0001 * (i + 1) for i in range(T_d)])\n        case_d = {\n            \"N\": 100_000_000.0,\n            \"T\": T_d,\n            \"alpha\": np.full(T_d, 0.25),\n            \"L\": L_d,\n            \"m\": 1e-12\n        }\n\n        return [case_a, case_b, case_c, case_d]\n\n    def calculate_pvs(N, m, L, alpha):\n        \"\"\"\n        Calculates the present value of a floating-rate bond using two different methods.\n\n        Args:\n            N (float): Notional amount.\n            m (float): Margin.\n            L (np.ndarray): Array of forward rates.\n            alpha (np.ndarray): Array of accrual fractions.\n\n        Returns:\n            tuple: A tuple containing (pv_naive, pv_stable, abs_diff).\n        \"\"\"\n        T = len(L)\n        \n        # Calculate discount factors D_i for i=0,...,T\n        # D_0 = 1, D_i = D_{i-1} / (1 + L_i * alpha_i)\n        D = np.empty(T + 1, dtype=np.float64)\n        D[0] = 1.0\n        for i in range(1, T + 1):\n            D[i] = D[i - 1] / (1.0 + L[i - 1] * alpha[i - 1])\n        \n        # We need discount factors D_1, ..., D_T for the sums\n        D_coupon_periods = D[1:]\n\n        # Method 1: Naive PV calculation\n        # PV_naive = N * ( sum_{i=1 to T} (L_i + m) * alpha_i * D_i + D_T )\n        coupon_sum_naive = np.sum((L + m) * alpha * D_coupon_periods)\n        pv_naive = N * (coupon_sum_naive + D[T])\n\n        # Method 2: Stable PV calculation\n        # PV_stable = N * ( 1 + m * sum_{i=1 to T} alpha_i * D_i )\n        margin_sum_stable = np.sum(alpha * D_coupon_periods)\n        pv_stable = N * (1.0 + m * margin_sum_stable)\n\n        # Absolute difference\n        abs_diff = np.abs(pv_naive - pv_stable)\n\n        return pv_naive, pv_stable, abs_diff\n\n    test_cases = generate_test_cases()\n    results = []\n    \n    for case in test_cases:\n        pv_naive, pv_stable, abs_diff = calculate_pvs(\n            N=case[\"N\"], \n            m=case[\"m\"], \n            L=case[\"L\"], \n            alpha=case[\"alpha\"]\n        )\n        results.extend([pv_naive, pv_stable, abs_diff])\n\n    # Format the final output as a comma-separated list in a single line.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2394271"}, {"introduction": "Our final practice shifts focus from the precision of individual operations to the numerical stability of the problem's representation. We will confront the challenge of finding a high-multiplicity root of the polynomial $f(x) = (x-c)^{8}$, a situation that can arise in complex economic models. You will discover how the theoretically simple task of finding the root $x=c$ becomes numerically treacherous when the polynomial is expanded, and you'll compare the performance of standard versus modified numerical methods designed to handle such ill-conditioned problems [@problem_id:2394189].", "problem": "Consider the univariate polynomial $f(x) = (x - c)^{8}$, which has an $8$-fold root at $x = c$. Problems of this type arise in computational economics and finance when repeated optimality conditions or characteristic equations produce high-multiplicity roots under polynomial approximations. In finite-precision arithmetic, repeated roots are known to be numerically delicate. Assume standard double-precision floating-point arithmetic following the Institute of Electrical and Electronics Engineers (IEEE) $754$ standard.\n\nYou must write a complete program that, for each test case, computes the following quantities from first principles:\n\n1. Compute the machine epsilon $\\varepsilon$ defined as the smallest positive floating-point number such that $1 + \\varepsilon > 1$ in the working arithmetic, determined at runtime by halving.\n\n2. Let $f(x) = (x-c)^{8}$ and $f'(x)$ denote its derivative with respect to $x$. Define the iterative map $a_{k+1} = a_{k} - \\dfrac{f(a_{k})}{f'(a_{k})}$ with initial value $a_{0}$ specified by the test case. After $T$ steps, record $E_{\\text{std}} = |a_{T} - c|$. Use $T = 12$.\n\n3. Define the iterative map $b_{k+1} = b_{k} - m \\dfrac{f(b_{k})}{f'(b_{k})}$ with initial value $b_{0} = a_{0}$ and multiplicity $m = 8$. After $S$ steps, record $E_{\\text{mod}} = |b_{S} - c|$. Use $S = 2$.\n\n4. Form the expanded polynomial\n$$\np(x) = \\sum_{k=0}^{8} \\binom{8}{k} (-c)^{8-k} x^{k}\n= x^{8} - 8 c x^{7} + 28 c^{2} x^{6} - 56 c^{3} x^{5} + 70 c^{4} x^{4} - 56 c^{5} x^{3} + 28 c^{6} x^{2} - 8 c^{7} x + c^{8}.\n$$\nUsing standard floating-point linear algebra to compute the eight complex roots $\\{z_{i}\\}_{i=1}^{8}$ of $p(x)$, record\n$$\nR_{\\text{avg}} = \\left| \\dfrac{1}{8} \\sum_{i=1}^{8} z_{i} - c \\right|,\n$$\n$$\nR_{\\text{spread}} = \\max_{1 \\le i \\le 8} |z_{i} - c|,\n$$\nwhere $|\\cdot|$ on complex values denotes the complex modulus.\n\n5. Let $\\delta = \\sqrt{\\varepsilon} \\cdot \\max\\{1, |c|\\}$. Evaluate $f_{\\text{fact}} = ( (c + \\delta) - c )^{8}$ and evaluate $f_{\\text{exp}} = p(c + \\delta)$ using the expanded coefficient representation in item $4$. Record the relative discrepancy\n$$\nC_{\\text{rel}} = \\frac{|f_{\\text{exp}} - f_{\\text{fact}}|}{|f_{\\text{fact}}|}.\n$$\n\nYour program must perform the above computations for each test case in the following test suite:\n- Test $1$: $(c, a_{0}) = (1.0, 2.0)$.\n- Test $2$: $(c, a_{0}) = (10^{-8}, 0.0)$.\n- Test $3$: $(c, a_{0}) = (10^{8}, 0.0)$.\n- Test $4$: $(c, a_{0}) = (10^{16}, 0.0)$.\n\nFor each test case, the required outputs are the five real numbers $[E_{\\text{std}}, E_{\\text{mod}}, R_{\\text{avg}}, R_{\\text{spread}}, C_{\\text{rel}}]$ computed exactly as defined above. Your program should produce a single line of output containing the concatenation of these five-number blocks for the four tests, as a comma-separated list enclosed in square brackets, in test order. Concretely, the final output must be\n$$\n[E_{\\text{std}}^{(1)}, E_{\\text{mod}}^{(1)}, R_{\\text{avg}}^{(1)}, R_{\\text{spread}}^{(1)}, C_{\\text{rel}}^{(1)}, E_{\\text{std}}^{(2)}, E_{\\text{mod}}^{(2)}, R_{\\text{avg}}^{(2)}, R_{\\text{spread}}^{(2)}, C_{\\text{rel}}^{(2)}, E_{\\text{std}}^{(3)}, E_{\\text{mod}}^{(3)}, R_{\\text{avg}}^{(3)}, R_{\\text{spread}}^{(3)}, C_{\\text{rel}}^{(3)}, E_{\\text{std}}^{(4)}, E_{\\text{mod}}^{(4)}, R_{\\text{avg}}^{(4)}, R_{\\text{spread}}^{(4)}, C_{\\text{rel}}^{(4)}].\n$$\n\nAll outputs are real numbers with no units. Angles are not involved. Percentages must not be used; all ratios must be reported as real numbers.", "solution": "The problem statement has been validated and is deemed a valid, well-posed exercise in numerical analysis. It serves as a canonical demonstration of the challenges posed by high-multiplicity roots when using finite-precision, floating-point arithmetic. We will dissect each component of the problem with the required scientific and mathematical rigor.\n\nThe core of the problem is the polynomial $f(x) = (x - c)^{8}$, which possesses a single root at $x=c$ with multiplicity $m=8$. We are tasked with investigating the numerical behavior of various methods for locating this root and evaluating the polynomial itself, under standard IEEE $754$ double-precision arithmetic.\n\n**1. Machine Epsilon ($\\varepsilon$)**\nThe machine epsilon, denoted $\\varepsilon$, is a fundamental characteristic of a floating-point number system. It is defined as the smallest positive number that, when added to $1$, yields a result greater than $1$. In other words, it is the distance from $1.0$ to the next larger representable floating-point number. The problem specifies its determination at runtime via a halving algorithm. We start with a value, for instance $1.0$, and repeatedly divide it by $2$ until $1.0 + (\\varepsilon/2)$ is computationally indistinguishable from $1.0$. The last value for which $1.0 + \\varepsilon > 1.0$ holds is the machine epsilon. For standard double-precision arithmetic, this value is $2^{-52}$, which is approximately $2.22 \\times 10^{-16}$.\n\n**2. Standard Newton-Raphson Iteration ($E_{\\text{std}}$)**\nThe Newton-Raphson method is a root-finding algorithm defined by the iterative map $a_{k+1} = a_k - f(a_k)/f'(a_k)$. For the given polynomial $f(x) = (x-c)^8$, the derivative is $f'(x) = 8(x-c)^7$. The iterative step is therefore:\n$$\na_{k+1} = a_k - \\frac{(a_k-c)^8}{8(a_k-c)^7}\n$$\nIn exact arithmetic, this simplifies to:\n$$\na_{k+1} = a_k - \\frac{a_k-c}{8} = \\frac{7}{8}a_k + \\frac{1}{8}c\n$$\nThis reveals that the error, $e_k = a_k - c$, contracts at each step by a constant factor: $e_{k+1} = a_{k+1} - c = \\frac{7}{8}(a_k - c) = \\frac{7}{8}e_k$. This is characteristic of linear convergence with a rate of $1 - 1/m = 1 - 1/8 = 7/8$, which is notoriously slow. After $T=12$ iterations, the final error $|a_T - c|$ is expected to be $|a_0 - c| \\cdot (7/8)^{12}$. The implementation, however, must use the unsimplified ratio $f(a_k)/f'(a_k)$ to expose potential numerical issues like underflow when $a_k$ is very close to $c$.\n\n**3. Modified Newton-Raphson Iteration ($E_{\\text{mod}}$)**\nTo restore the quadratic convergence of Newton's method in the presence of a known multiplicity $m$, the iteration is modified to $b_{k+1} = b_k - m \\cdot f(b_k)/f'(b_k)$. For $m=8$, this gives:\n$$\nb_{k+1} = b_k - 8 \\cdot \\frac{(b_k-c)^8}{8(b_k-c)^7} = b_k - (b_k - c) = c\n$$\nTheoretically, this method converges in a single step. In practice, due to floating-point inaccuracies, it will converge quadratically to within a small neighborhood of the true root $c$. The specified $S=2$ iterations should be sufficient to drive the error $|b_S-c|$ to a level at or near machine precision.\n\n**4. Expanded Polynomial Root-Finding ($R_{\\text{avg}}$, $R_{\\text{spread}}$)**\nThe problem requires computing the roots of the binomially expanded polynomial:\n$$\np(x) = \\sum_{k=0}^{8} \\binom{8}{k} (-c)^{8-k} x^{k}\n$$\nRepresenting a polynomial by its coefficients is a numerically sensitive task, especially for roots with high multiplicity. The coefficients of $p(x)$ will vary over many orders of magnitude, particularly for large or small $|c|$. When these coefficients are stored in finite precision, they are inherently perturbed. According to the theory of polynomial conditioning, small relative perturbations in the coefficients can lead to large absolute changes in the roots. Consequently, the single analytical root $x=c$ will \"shatter\" into a cluster of eight distinct complex roots $\\{z_i\\}$ scattered around $c$. We are to compute these roots using a standard numerical linear algebra technique (the companion matrix method, as implemented in libraries like `numpy`) and then quantify the result by computing the deviation of their average from $c$ ($R_{\\text{avg}}$) and the maximum deviation in the cluster ($R_{\\text{spread}}$).\n\n**5. Conditioning Analysis ($C_{\\text{rel}}$)**\nThis final part directly probes the numerical conditioning of evaluating the polynomial in its factored form, $f(x) = (x-c)^8$, versus its expanded form, $p(x)$. A perturbation $\\delta = \\sqrt{\\varepsilon} \\cdot \\max\\{1, |c|\\}$ is introduced.\n- The evaluation of the factored form, $f_{\\text{fact}} = ((c+\\delta)-c)^8$, is expected to be numerically stable. The subtraction $(c+\\delta)-c$ should accurately recover $\\delta$, leading to a result close to the true mathematical value of $\\delta^8$.\n- The evaluation of the expanded form, $p(c+\\delta)$, is expected to be numerically unstable. This calculation involves summing many large terms of alternating sign that cancel each other out to produce a very small final result. This phenomenon, known as catastrophic cancellation, will amplify the initial rounding errors in the coefficients and the input value, leading to a computed value $f_{\\text{exp}}$ that can be grossly inaccurate.\nThe relative discrepancy $C_{\\text{rel}} = |f_{\\text{exp}} - f_{\\text{fact}}| / |f_{\\text{fact}}|$ measures the extent of this instability. A large value for $C_{\\text{rel}}$ indicates that the expanded form is ill-conditioned for evaluation near its multiple root. For all test cases, this value is expected to be significantly greater than $1$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the numerical analysis problem for the given test cases.\n    \"\"\"\n\n    test_cases = [\n        # (c, a0)\n        (1.0, 2.0),\n        (1e-8, 0.0),\n        (1e8, 0.0),\n        (1e16, 0.0),\n    ]\n\n    all_results = []\n    \n    # Binomial coefficients B(8,j) for j=0..8\n    # These correspond to B(n,k) for n=8 and k=0, 1, ..., 8\n    binomial_coeffs = np.array([1, 8, 28, 56, 70, 56, 28, 8, 1], dtype=np.float64)\n\n    for c, a0 in test_cases:\n        c_f64 = np.float64(c)\n        a0_f64 = np.float64(a0)\n\n        # 1. Compute machine epsilon\n        eps = np.float64(1.0)\n        while np.float64(1.0) + eps / np.float64(2.0)  np.float64(1.0):\n            eps /= np.float64(2.0)\n\n        # 2. Standard Newton's Method\n        T = 12\n        a_k = a0_f64\n        for _ in range(T):\n            diff = a_k - c_f64\n            # Prevent division by zero if a_k becomes numerically equal to c\n            if diff == 0.0:\n                break\n            f_val = diff**8\n            f_prime_val = 8.0 * (diff**7)\n            # Prevent NaN from 0/0 and halt if derivative is numerically zero\n            if f_prime_val == 0.0:\n                break\n            a_k -= f_val / f_prime_val\n        E_std = np.abs(a_k - c_f64)\n\n        # 3. Modified Newton's Method\n        S = 2\n        m = 8.0\n        b_k = a0_f64\n        for _ in range(S):\n            diff = b_k - c_f64\n            if diff == 0.0:\n                break\n            f_val = diff**8\n            f_prime_val = 8.0 * (diff**7)\n            if f_prime_val == 0.0:\n                break\n            b_k -= m * f_val / f_prime_val\n        E_mod = np.abs(b_k - c_f64)\n\n        # 4. Expanded Polynomial Root Finding\n        # The coefficient for x^k is binom(8,k) * (-c)^(8-k).\n        # numpy.roots needs coefficients for [x^8, x^7, ..., x^0].\n        # The coefficient for x^(8-j) is binom(8, 8-j) * (-c)^j = binom(8,j) * (-c)^j.\n        p_coeffs = np.zeros(9, dtype=np.float64)\n        for j in range(9):\n            p_coeffs[j] = binomial_coeffs[j] * ((-c_f64)**j)\n        \n        roots = np.roots(p_coeffs)\n        \n        R_avg = np.abs(np.mean(roots) - c_f64)\n        R_spread = np.max(np.abs(roots - c_f64))\n\n        # 5. Conditioning Analysis\n        delta = np.sqrt(eps) * np.max([1.0, np.abs(c_f64)])\n        \n        # Factored form evaluation (numerically stable)\n        f_fact = ((c_f64 + delta) - c_f64)**8\n        \n        # Expanded form evaluation using Horner's method (numerically unstable)\n        x_eval = c_f64 + delta\n        # p_coeffs are for x^8, x^7, ..., x^0. np.polyval needs them in this order.\n        # However, our loop for p_coeffs is for powers 8, 7, ..., 0.\n        # It should be p_coeffs[0]x^8 + p_coeffs[1]x^7 + ...\n        # Let's fix the p_coeffs order for numpy.roots\n        # The coefficients are for x^8, x^7, ..., x^0.\n        # The loop generates coefficients for x^8, x^7, ... correctly\n        # Let's recheck polyval usage: it wants p[0] * x**n + p[1] * x**(n-1) + ...\n        # Our p_coeffs[j] is for x^(8-j), so p_coeffs[0] is for x^8, p_coeffs[1] is for x^7 etc.\n        # This seems correct. np.polyval should work.\n        f_exp = np.polyval(p_coeffs, x_eval)\n        \n        # Relative discrepancy\n        if f_fact == 0.0:\n            C_rel = np.inf if f_exp != 0.0 else 0.0\n        else:\n            C_rel = np.abs(f_exp - f_fact) / np.abs(f_fact)\n\n        all_results.extend([E_std, E_mod, R_avg, R_spread, C_rel])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```", "id": "2394189"}]}