{"hands_on_practices": [{"introduction": "This first exercise demonstrates how to translate a classic microeconomic model into a precise, deterministic algorithm. By modeling a rational agent's decision, you will formalize the choice process as a function of its key inputs: the probability of being caught, the potential gain, and the penalty. This practice is fundamental to bridging the gap between economic theory and computational logic, as you will derive the exact decision boundary that separates one action from another [@problem_id:2438853].", "problem": "In a simple microeconomic decision model relevant to computational economics and finance, consider a risk-neutral agent who contemplates a white-collar violation that yields a monetary gain if not caught and a monetary penalty if caught. Let the inputs to the decision algorithm be the triplet $\\left(p, V, C\\right)$, where $p \\in [0,1]$ is the probability of being caught, $V>0$ is the gain realized only if not caught, and $C>0$ is the penalty incurred only if caught. If the agent does not commit the act, the monetary payoff is $0$. If the agent commits the act, with probability $1-p$ the monetary payoff is $V$, and with probability $p$ the monetary payoff is $-C$. The agent is risk neutral and maximizes expected monetary payoff.\n\nModel the agentâ€™s decision as a deterministic algorithm $f$ that maps inputs $\\left(p, V, C\\right)$ to a binary action $f\\left(p,V,C\\right) \\in \\{0,1\\}$, where $f\\left(p,V,C\\right)=1$ denotes choosing to commit and $f\\left(p,V,C\\right)=0$ denotes abstaining. Define the algorithmic decision boundary by the indifference condition between committing and abstaining, and derive the closed-form expression for the maximal detection probability $p^{\\ast}$ (as a function of $V$ and $C$ only) such that $f\\left(p,V,C\\right)=1$ if and only if $p < p^{\\ast}$.\n\nProvide your final answer as a single closed-form analytic expression for $p^{\\ast}$ in terms of $V$ and $C$. No numerical approximation or rounding is required.", "solution": "The problem asks to represent a decision problem as an algorithm and to characterize the decision boundary in terms of a threshold probability $p^{\\ast}$ that depends on $V$ and $C$. The fundamental principle is expected payoff maximization under risk neutrality.\n\nLet the expected payoff from committing be denoted by $\\mathbb{E}\\left[\\Pi_{\\text{commit}}\\right]$ and from abstaining by $\\mathbb{E}\\left[\\Pi_{\\text{abstain}}\\right]$. By the setup, the abstention payoff is $0$, so\n$$\n\\mathbb{E}\\left[\\Pi_{\\text{abstain}}\\right] = 0.\n$$\nIf the agent commits, with probability $1-p$ the payoff is $V$, and with probability $p$ the payoff is $-C$. Therefore, by the law of total expectation,\n$$\n\\mathbb{E}\\left[\\Pi_{\\text{commit}}\\right] = (1-p)\\cdot V + p \\cdot (-C) = V - p\\left(V + C\\right).\n$$\n\nThe algorithm $f\\left(p,V,C\\right)$ returns $1$ (commit) if the expected payoff from committing is at least as large as that from abstaining, and returns $0$ (abstain) otherwise. Thus, the indifference condition defining the boundary between the two actions is\n$$\n\\mathbb{E}\\left[\\Pi_{\\text{commit}}\\right] = \\mathbb{E}\\left[\\Pi_{\\text{abstain}}\\right].\n$$\nSubstituting the expressions above yields\n$$\nV - p\\left(V + C\\right) = 0.\n$$\nSolving for $p$ gives the threshold detection probability $p^{\\ast}$:\n$$\np^{\\ast} = \\frac{V}{V + C}.\n$$\n\nThe algorithmic decision rule can then be stated as: for given inputs $\\left(p,V,C\\right)$ with $V>0$ and $C>0$, choose to commit if and only if $p < p^{\\ast}$, where $p^{\\ast}$ is the maximal detection probability at which the agent is indifferent. The required closed-form expression for $p^{\\ast}$ in terms of $V$ and $C$ is\n$$\np^{\\ast} = \\frac{V}{V + C}.\n$$", "answer": "$$\\boxed{\\frac{V}{V+C}}$$", "id": "2438853"}, {"introduction": "Moving from a single decision to a sequence of choices over time, this problem challenges you to compare two fundamentally different algorithmic strategies for a resource extraction task. You will implement both a simple, myopic 'greedy' algorithm and a globally optimal, forward-looking one based on dynamic programming principles. This comparative analysis is crucial for understanding the economic concept of time preference and quantifying the value of foresight in dynamic optimization problems [@problem_id:2438788].", "problem": "Consider a finite-horizon resource extraction problem motivated by computational economics and finance. A firm owns a mine containing a nonrenewable resource. Time is discrete with $T$ periods indexed by $t \\in \\{1,\\dots,T\\}$. The firm chooses extraction quantities $x_t$ each period subject to a stock constraint and period-specific operational capacity. Let the initial stock be $S \\ge 0$. Let the per-period capacity limits be $u_t \\ge 0$. Let the ore concentration (a multiplicative factor for revenue) be $c_t \\ge 0$. Let the constant output price be $p \\ge 0$. Let the per-period convex extraction cost be quadratic with parameter $a > 0$. Let the intertemporal discount factor be $\\beta \\in (0,1]$. Profits are computed in discounted units.\n\nThe per-period instantaneous net benefit from extracting $x_t$ in period $t$ is given by the concave function\n$$\nb_t(x_t) \\equiv p\\,c_t\\,x_t - \\frac{a}{2}\\,x_t^2,\n$$\nand total discounted net present value (NPV) is\n$$\n\\sum_{t=1}^T \\beta^{t-1}\\, b_t(x_t).\n$$\n\nFeasibility constraints are\n$$\n0 \\le x_t \\le u_t \\quad \\text{for all } t, \\qquad \\sum_{t=1}^T x_t \\le S.\n$$\n\nYour task is to implement and compare two algorithms:\n\n- A greedy algorithm that is myopic: in each period $t$ it chooses $x_t$ to maximize the current period's $b_t(x_t)$ subject only to the instantaneous constraints $0 \\le x_t \\le \\min\\{u_t, s_t\\}$ where $s_t$ is the remaining stock before making the period-$t$ decision. The greedy algorithm ignores any effect on future periods beyond the current stock feasibility. After choosing $x_t$, update the remaining stock by $s_{t+1} = s_t - x_t$ with $s_1 = S$.\n- An optimal dynamic programming (DP) solution based on the Bellman principle of optimality that maximizes the total discounted NPV subject to all constraints. You may compute this optimal solution by any correct algorithm that is equivalent to solving the finite-horizon DP problem.\n\nFor both algorithms, compute the discounted NPV\n$$\nV \\equiv \\sum_{t=1}^T \\beta^{t-1}\\, \\left(p\\,c_t\\,x_t - \\frac{a}{2}\\,x_t^2\\right).\n$$\n\nImplement a program that, for the following test suite, returns for each parameter set a triple consisting of the optimal NPV, the greedy NPV, and an equality indicator defined as $1$ if the absolute difference between the two NPVs is at most $10^{-6}$ and $0$ otherwise. All floating-point outputs must be rounded to $6$ decimals.\n\nTest suite (each case is a tuple $\\left(T,\\ \\beta,\\ p,\\ a,\\ S,\\ \\mathbf{c},\\ \\mathbf{u}\\right)$ where $\\mathbf{c}=(c_1,\\dots,c_T)$ and $\\mathbf{u}=(u_1,\\dots,u_T)$):\n\n- Case $1$: $T=4$, $\\beta=0.95$, $p=2.0$, $a=2.0$, $S=6.0$, $\\mathbf{c}=(1.0,\\ 1.5,\\ 1.2,\\ 1.1)$, $\\mathbf{u}=(5.0,\\ 5.0,\\ 5.0,\\ 5.0)$.\n- Case $2$: $T=3$, $\\beta=0.98$, $p=1.0$, $a=1.0$, $S=3.0$, $\\mathbf{c}=(1.0,\\ 2.0,\\ 5.0)$, $\\mathbf{u}=(10.0,\\ 10.0,\\ 10.0)$.\n- Case $3$: $T=3$, $\\beta=0.5$, $p=1.0$, $a=1.0$, $S=3.0$, $\\mathbf{c}=(5.0,\\ 2.0,\\ 1.0)$, $\\mathbf{u}=(10.0,\\ 10.0,\\ 10.0)$.\n- Case $4$: $T=4$, $\\beta=0.99$, $p=1.0$, $a=1.0$, $S=1.5$, $\\mathbf{c}=(2.0,\\ 2.0,\\ 10.0,\\ 2.0)$, $\\mathbf{u}=(10.0,\\ 10.0,\\ 1.0,\\ 10.0)$.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must be flattened across test cases in the following order: for case $i$ append optimal NPV, then greedy NPV, then the equality indicator (in that order), all rounded as specified. For example, the output must look like\n$$\n[\\text{opt}_1,\\ \\text{greedy}_1,\\ \\text{eq}_1,\\ \\text{opt}_2,\\ \\text{greedy}_2,\\ \\text{eq}_2,\\ \\dots].\n$$", "solution": "The problem statement is first subjected to a rigorous validation procedure.\n\n**Step 1: Extracted Givens**\n- **Time Horizon**: Discrete, $T$ periods, $t \\in \\{1,\\dots,T\\}$.\n- **Choice Variables**: Extraction quantities $x_t$.\n- **Parameters**: Initial stock $S \\ge 0$, per-period capacity $u_t \\ge 0$, ore concentration $c_t \\ge 0$, output price $p \\ge 0$, cost parameter $a > 0$, discount factor $\\beta \\in (0,1]$.\n- **Objective Function**: Maximize total discounted Net Present Value (NPV), $V = \\sum_{t=1}^T \\beta^{t-1}\\, b_t(x_t)$, where the per-period net benefit is $b_t(x_t) \\equiv p\\,c_t\\,x_t - \\frac{a}{2}\\,x_t^2$.\n- **Constraints**:\n    1. Capacity: $0 \\le x_t \\le u_t$ for all $t$.\n    2. Stock: $\\sum_{t=1}^T x_t \\le S$.\n- **Algorithms**:\n    1. **Greedy**: In each period $t$, choose $x_t$ to maximize $b_t(x_t)$ subject to $0 \\le x_t \\le \\min\\{u_t, s_t\\}$, where $s_t$ is the remaining stock.\n    2. **Optimal**: A dynamic programming solution, or equivalent, to maximize the total discounted NPV under all constraints.\n- **Output Requirements**: For each test case, provide a triple: (optimal NPV, greedy NPV, equality indicator). The indicator is $1$ if the absolute difference is $\\le 10^{-6}$, else $0$. Outputs must be rounded to $6$ decimals. Test cases are provided.\n\n**Step 2: Validation**\n- **Scientifically Grounded**: The problem is a canonical finite-horizon resource extraction model, a fundamental topic in computational and resource economics. The model is based on established economic principles. It is valid.\n- **Well-Posed**: The objective function, $\\sum \\beta^{t-1} (p c_t x_t - \\frac{a}{2} x_t^2)$, is strictly concave in the vector $\\mathbf{x} = (x_1, \\dots, x_T)$ because $a>0$ and $\\beta>0$. The feasible set, defined by linear inequalities, is convex and compact. The maximization of a strictly concave function over a non-empty, compact, convex set has a unique solution. The problem is well-posed.\n- **Objective and Complete**: The problem is specified with precise mathematical language and provides all necessary data for the given test cases. It is valid.\n\n**Step 3: Verdict**\nThe problem is valid. We proceed with the solution.\n\nThe problem requires the implementation and comparison of two algorithms for a resource extraction problem.\n\n**Greedy Algorithm**\nThe greedy algorithm is myopic. In each period $t$, it maximizes the current period's benefit $b_t(x_t) = p c_t x_t - \\frac{a}{2} x_t^2$ subject to current constraints only. The function $b_t(x_t)$ is a downward-opening parabola. Its unconstrained maximum is found by setting its derivative with respect to $x_t$ to zero:\n$$\n\\frac{d b_t(x_t)}{d x_t} = p c_t - a x_t = 0 \\implies x_t^* = \\frac{p c_t}{a}\n$$\nThe constraints faced by the greedy decision-maker in period $t$ are the capacity limit $u_t$ and the remaining stock $s_t$. Thus, the choice must satisfy $0 \\le x_t \\le \\min\\{u_t, s_t\\}$. Since $p, c_t, a$ are non-negative, $x_t^* \\ge 0$. The greedy choice $x_t^g$ is therefore the projection of the unconstrained optimum $x_t^*$ onto the feasible interval:\n$$\nx_t^g = \\min\\left(\\frac{p c_t}{a}, u_t, s_t\\right)\n$$\nThe algorithm proceeds by iterating from $t=1$ to $T$, calculating $x_t^g$, and updating the remaining stock $s_{t+1} = s_t - x_t^g$, with initial stock $s_1 = S$. The total NPV is then computed using the resulting extraction path $\\{x_t^g\\}_{t=1}^T$.\n\n**Optimal Algorithm**\nThe problem is a convex optimization problem, specifically a Quadratic Program (QP), since the objective is quadratic and the constraints are linear. While solvable with a generic QP solver, a more efficient method arises from analyzing the Karush-Kuhn-Tucker (KKT) conditions.\n\nThe Lagrangian for the optimization problem is:\n$$\n\\mathcal{L}(\\mathbf{x}, \\lambda, \\boldsymbol{\\mu}, \\boldsymbol{\\nu}) = \\sum_{t=1}^T \\beta^{t-1}\\left(p c_t x_t - \\frac{a}{2} x_t^2\\right) - \\lambda\\left(\\sum_{t=1}^T x_t - S\\right) - \\sum_{t=1}^T \\mu_t(x_t - u_t) - \\sum_{t=1}^T \\nu_t(-x_t)\n$$\nwhere $\\lambda \\ge 0$, $\\mu_t \\ge 0$, and $\\nu_t \\ge 0$ are the Lagrange multipliers for the total stock, per-period upper capacity, and per-period non-negativity constraints, respectively.\n\nThe first-order condition for stationarity with respect to $x_t$ is:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial x_t} = \\beta^{t-1}(p c_t - a x_t) - \\lambda - \\mu_t + \\nu_t = 0\n$$\nThe complementary slackness conditions imply that if $0 < x_t < u_t$, then $\\mu_t = \\nu_t = 0$. In this internal case, the optimal extraction $x_t$ satisfies:\n$$\n\\beta^{t-1}(p c_t - a x_t) = \\lambda \\implies p c_t - a x_t = \\lambda \\beta^{-(t-1)} \\implies x_t = \\frac{p c_t - \\lambda \\beta^{-(t-1)}}{a}\n$$\nThe term $\\lambda$ is the shadow price of the resource stock $S$. The condition states that the discounted marginal profit of extraction must be equal to this shadow price across all periods where extraction is positive and not at a boundary.\nConsidering the boundary conditions ($x_t=0$ or $x_t=u_t$), the optimal extraction $x_t^*$ for a given $\\lambda$ is:\n$$\nx_t^*(\\lambda) = \\max\\left(0, \\min\\left(\\frac{p c_t - \\lambda \\beta^{-(t-1)}}{a}, u_t\\right)\\right)\n$$\nLet $X(\\lambda) = \\sum_{t=1}^T x_t^*(\\lambda)$. The function $X(\\lambda)$ is a continuous and non-increasing function of $\\lambda$. The correct value of $\\lambda$ is determined by the total stock constraint and the complementary slackness condition $\\lambda(\\sum_{t=1}^T x_t^* - S) = 0$.\n\nThe algorithm to find the optimal path is as follows:\n1.  Calculate total extraction assuming the stock constraint is not binding, which corresponds to setting the shadow price $\\lambda=0$. Let this be $X(0) = \\sum_t \\max\\left(0, \\min\\left(\\frac{p c_t}{a}, u_t\\right)\\right)$.\n2.  If $X(0) \\le S$, the resource is not scarce relative to its profitable extraction opportunities. The optimal solution is obtained with $\\lambda^*=0$, and $x_t^* = x_t^*(0)$ for all $t$.\n3.  If $X(0) > S$, the stock constraint is binding, requiring $\\lambda^* > 0$. We must find the unique $\\lambda^* > 0$ that satisfies the market-clearing condition $X(\\lambda^*) = S$. This is a one-dimensional root-finding problem for the equation $g(\\lambda) = X(\\lambda) - S = 0$. A bisection or Newton-family method (e.g., Brent's method) can find $\\lambda^*$ efficiently. The search interval for $\\lambda$ is $[0, \\lambda_{max}]$, where $\\lambda_{max} = \\max_t(p c_t \\beta^{t-1})$ is an upper bound ensuring all $x_t=0$.\n4.  Once $\\lambda^*$ is found, the optimal extraction path $\\{x_t^*(\\lambda^*)\\}_{t=1}^T$ is determined, and the optimal NPV can be computed.\n\nThis KKT-based approach is superior to a generic QP solver for this problem structure. The implementation will use Brent's method for root finding, as it is robust and efficient.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import brentq\n\ndef solve():\n    \"\"\"\n    Solves a series of finite-horizon resource extraction problems using both a greedy\n    and an optimal algorithm, then compares the results.\n    \"\"\"\n\n    test_cases = [\n        (4, 0.95, 2.0, 2.0, 6.0, np.array([1.0, 1.5, 1.2, 1.1]), np.array([5.0, 5.0, 5.0, 5.0])),\n        (3, 0.98, 1.0, 1.0, 3.0, np.array([1.0, 2.0, 5.0]), np.array([10.0, 10.0, 10.0])),\n        (3, 0.5, 1.0, 1.0, 3.0, np.array([5.0, 2.0, 1.0]), np.array([10.0, 10.0, 10.0])),\n        (4, 0.99, 1.0, 1.0, 1.5, np.array([2.0, 2.0, 10.0, 2.0]), np.array([10.0, 10.0, 1.0, 10.0])),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        T, beta, p, a, S, c, u = case\n        \n        # --- Greedy Algorithm ---\n        x_g = np.zeros(T)\n        s_rem = S\n        for t in range(T):\n            x_unconstrained = p * c[t] / a\n            x_g[t] = np.clip(x_unconstrained, 0, min(u[t], s_rem))\n            s_rem -= x_g[t]\n\n        # --- Optimal Algorithm ---\n        betas = beta ** np.arange(T)\n        \n        def get_x_opt(lam, T, p, a, beta, c, u):\n            \"\"\"Computes optimal extraction path for a given lambda.\"\"\"\n            beta_inv_powers = beta ** (-np.arange(T))\n            numerators = p * c - lam * beta_inv_powers\n            x_unbounded = numerators / a\n            return np.clip(x_unbounded, 0, u)\n\n        # Check if resource constraint is binding\n        x_opt_lam0 = get_x_opt(0, T, p, a, beta, c, u)\n        \n        if np.sum(x_opt_lam0) <= S:\n            lam_opt = 0.0\n            x_opt = x_opt_lam0\n        else:\n            # Resource constraint is binding, find lambda > 0\n            # Define the function whose root we want to find\n            def total_extraction_diff(lam):\n                return np.sum(get_x_opt(lam, T, p, a, beta, c, u)) - S\n            \n            # Find a safe upper bound for lambda search\n            # Any lambda larger than this will result in all xt <= 0\n            lam_upper_bound = np.max(p * c * (beta ** np.arange(T))) + 1.0\n\n            try:\n                lam_opt = brentq(total_extraction_diff, 0, lam_upper_bound, xtol=1e-12, rtol=1e-12)\n            except ValueError:\n                # Fallback if signs are not different, though theoretically they should be\n                lam_opt = 0.0 # Should not happen with this problem structure\n            \n            x_opt = get_x_opt(lam_opt, T, p, a, beta, c, u)\n\n        # --- NPV Calculation ---\n        def calculate_npv(x_path, T, beta, p, c, a):\n            npv = 0.0\n            for t in range(T):\n                benefit = p * c[t] * x_path[t] - (a / 2) * x_path[t]**2\n                npv += (beta**t) * benefit\n            return npv\n\n        greedy_npv = calculate_npv(x_g, T, beta, p, c, a)\n        optimal_npv = calculate_npv(x_opt, T, beta, p, c, a)\n\n        # --- Formatting and Appending Results ---\n        optimal_npv_rounded = round(optimal_npv, 6)\n        greedy_npv_rounded = round(greedy_npv, 6)\n        \n        eq_indicator = 1 if abs(optimal_npv - greedy_npv) <= 1e-6 else 0\n        \n        results.extend([f\"{optimal_npv_rounded:.6f}\", f\"{greedy_npv_rounded:.6f}\", eq_indicator])\n\n    # Final print statement\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2438788"}, {"introduction": "Our final practice explores a sophisticated, modern application: using an algorithm to analyze and interact with another automated decision system. You will design an algorithm that finds the most cost-effective way for a rejected loan applicant to change their features to secure approval. This exercise in 'algorithmic recourse' introduces powerful optimization techniques and touches upon critical contemporary issues of fairness and transparency in computational finance [@problem_id:2438838].", "problem": "You are given a formalization of a credit approval decision and an associated notion of a minimally invasive, plausibly edited loan application. Let the feature vector be $\\mathbf{x} \\in \\mathbb{R}^d$, the weight vector be $\\mathbf{w} \\in \\mathbb{R}^d$, and the intercept be $b \\in \\mathbb{R}$. The credit scoring rule is a deterministic classifier that approves an application if and only if $\\mathbf{w}^\\top \\mathbf{x} + b \\ge 0$ and rejects otherwise. An adversarial edit is a vector $\\boldsymbol{\\delta} \\in \\mathbb{R}^d$ producing a modified application $\\mathbf{x} + \\boldsymbol{\\delta}$.\n\nA change is plausible if and only if it satisfies all of the following constraints:\n- For each feature index $i \\in \\{0,1,\\dots,d-1\\}$, the edited value is bounded by $l_i \\le x_i + \\delta_i \\le u_i$, where $\\mathbf{l}, \\mathbf{u} \\in \\mathbb{R}^d$ with $l_i \\le u_i$.\n- For each immutable feature index $i \\in \\mathcal{I} \\subseteq \\{0,1,\\dots,d-1\\}$, the change equals $\\delta_i = 0$.\n- The objective cost of an edit is defined by a nonnegative cost vector $\\mathbf{c} \\in \\mathbb{R}^d_{\\ge 0}$ as the weighted $\\ell_1$ norm $C(\\boldsymbol{\\delta}) = \\sum_{i=0}^{d-1} c_i |\\delta_i|$.\n\nYour task is to design an algorithm that, for a given instance $(\\mathbf{w}, b, \\mathbf{x}, \\mathbf{l}, \\mathbf{u}, \\mathbf{c}, \\mathcal{I})$, computes the minimal cost $C^\\star$ among all plausible edits $\\boldsymbol{\\delta}$ that achieve approval, i.e., that satisfy $\\mathbf{w}^\\top (\\mathbf{x} + \\boldsymbol{\\delta}) + b \\ge 0$. If the original application is already approved, the minimal cost is $0.0$. If no plausible edit can achieve approval, return $-1.0$.\n\nImplement this algorithm as a complete, runnable program that takes no input and evaluates the following fixed test suite. In every case, indices are $0$-based, and all vectors are listed in coordinate order $(0,1,\\dots,d-1)$.\n\nTest Suite (each case specifies $(\\mathbf{w}, b, \\mathbf{x}, \\mathbf{l}, \\mathbf{u}, \\mathbf{c}, \\mathcal{I})$):\n- Case A:\n  - $\\mathbf{w} = (1.0,\\, 0.5,\\, -0.2)$, $b = -0.5$,\n  - $\\mathbf{x} = (0.0,\\, 0.0,\\, 0.0)$,\n  - $\\mathbf{l} = (0.0,\\, -1.0,\\, -1.0)$, $\\mathbf{u} = (2.0,\\, 2.0,\\, 1.0)$,\n  - $\\mathbf{c} = (1.0,\\, 2.0,\\, 1.0)$,\n  - $\\mathcal{I} = \\varnothing$.\n- Case B:\n  - $\\mathbf{w} = (1.0,\\, 1.0)$, $b = -1.0$,\n  - $\\mathbf{x} = (0.6,\\, 0.6)$,\n  - $\\mathbf{l} = (0.0,\\, 0.0)$, $\\mathbf{u} = (1.0,\\, 1.0)$,\n  - $\\mathbf{c} = (1.0,\\, 1.0)$,\n  - $\\mathcal{I} = \\varnothing$.\n- Case C:\n  - $\\mathbf{w} = (0.1,\\, 0.1)$, $b = -5.0$,\n  - $\\mathbf{x} = (0.0,\\, 0.0)$,\n  - $\\mathbf{l} = (0.0,\\, 0.0)$, $\\mathbf{u} = (10.0,\\, 10.0)$,\n  - $\\mathbf{c} = (1.0,\\, 1.0)$,\n  - $\\mathcal{I} = \\varnothing$.\n- Case D:\n  - $\\mathbf{w} = (1.0,\\, 0.5)$, $b = -0.4$,\n  - $\\mathbf{x} = (0.0,\\, 0.0)$,\n  - $\\mathbf{l} = (0.0,\\, 0.0)$, $\\mathbf{u} = (0.1,\\, 2.0)$,\n  - $\\mathbf{c} = (0.1,\\, 10.0)$,\n  - $\\mathcal{I} = \\{0\\}$.\n- Case E:\n  - $\\mathbf{w} = (-1.0)$, $b = -0.4$,\n  - $\\mathbf{x} = (0.0)$,\n  - $\\mathbf{l} = (-1.0)$, $\\mathbf{u} = (1.0)$,\n  - $\\mathbf{c} = (3.0)$,\n  - $\\mathcal{I} = \\varnothing$.\n\nAnswer specification:\n- For each case, compute a single real number equal to the minimal cost $C^\\star$ of a plausible edit that achieves approval, with the convention that if the application is already approved then the answer is $0.0$, and if approval is impossible under the constraints then the answer is $-1.0$.\n- Report each result rounded to $6$ decimal places.\n- Final output format: Your program should produce a single line containing the results for Cases A through E, in that order, as a comma-separated list enclosed in square brackets, with no spaces, for example $[\\alpha,\\beta,\\gamma,\\delta,\\epsilon]$ where each symbol is the rounded result for the corresponding case.", "solution": "The problem as stated is valid. It is a well-posed, scientifically grounded optimization problem that is free of contradictions, ambiguities, and unsubstantiated claims. It falls within the domain of computational finance and algorithmic recourse, which is a subfield of computational economics. I will now provide the formal solution.\n\nThe task is to find the minimum cost $C^\\star$ for an edit vector $\\boldsymbol{\\delta} \\in \\mathbb{R}^d$ to a feature vector $\\mathbf{x} \\in \\mathbb{R}^d$ such that a linear classifier's decision boundary is crossed. The cost function is the weighted $\\ell_1$ norm $C(\\boldsymbol{\\delta}) = \\sum_{i=0}^{d-1} c_i |\\delta_i|$. The optimization problem can be formulated as follows:\n$$\n\\begin{align*}\n\\text{minimize} \\quad & C(\\boldsymbol{\\delta}) = \\sum_{i=0}^{d-1} c_i |\\delta_i| \\\\\n\\text{subject to} \\quad & \\mathbf{w}^\\top (\\mathbf{x} + \\boldsymbol{\\delta}) + b \\ge 0 \\\\\n& l_i \\le x_i + \\delta_i \\le u_i \\quad \\forall i \\in \\{0, 1, \\dots, d-1\\} \\\\\n& \\delta_i = 0 \\quad \\forall i \\in \\mathcal{I}\n\\end{align*}\n$$\n\nFirst, we address the trivial case. If the original application $\\mathbf{x}$ is already approved, the score $S_{\\text{initial}} = \\mathbf{w}^\\top \\mathbf{x} + b$ is non-negative, i.e., $S_{\\text{initial}} \\ge 0$. The minimum cost to achieve approval is trivially $0$, by choosing $\\boldsymbol{\\delta} = \\mathbf{0}$, which is a plausible edit.\n\nIf the application is rejected, $S_{\\text{initial}} < 0$. We must find a $\\boldsymbol{\\delta}$ to satisfy the constraints. The approval constraint can be rewritten as $\\mathbf{w}^\\top \\boldsymbol{\\delta} \\ge -(\\mathbf{w}^\\top \\mathbf{x} + b)$. Let $S_{\\text{needed}} = -S_{\\text{initial}} > 0$. The optimization problem is to achieve a total score increase of at least $S_{\\text{needed}}$ with minimum cost. The constraints on $\\delta_i$ for each mutable feature $i \\notin \\mathcal{I}$ are $l_i - x_i \\le \\delta_i \\le u_i - x_i$.\n\nThis problem has a convex objective function (a weighted $\\ell_1$ norm) and a feasible region defined by linear inequalities (a convex polytope). Therefore, a unique minimum exists if the feasible region is non-empty. The structure of the problem is analogous to the continuous knapsack problem, which admits an exact greedy solution.\n\nThe core of the greedy strategy is to prioritize changes to features that provide the largest increase in score per unit of cost. This \"efficiency\" or inverse marginal cost must be calculated for each feature. To increase the score $\\mathbf{w}^\\top\\boldsymbol{\\delta}$, the sign of $w_i \\delta_i$ must be positive. This implies that $\\text{sign}(\\delta_i) = \\text{sign}(w_i)$ for any feature $i$ that contributes positively.\n\nLet us analyze the potential contribution of each mutable feature $i \\notin \\mathcal{I}$ where $w_i \\neq 0$:\n\\begin{itemize}\n    \\item If $w_i > 0$, we must have $\\delta_i > 0$. The change is bounded by $0 < \\delta_i \\le u_i - x_i$. This is only possible if $u_i > x_i$. The score gain is $w_i \\delta_i$ at a cost of $c_i \\delta_i$. The marginal cost, or cost per unit of score gain, is $c_i / w_i$.\n    \\item If $w_i < 0$, we must have $\\delta_i < 0$. The change is bounded by $l_i - x_i \\le \\delta_i < 0$. This is only possible if $l_i < x_i$. The score gain is $w_i \\delta_i = |w_i| |\\delta_i|$ at a cost of $c_i |\\delta_i|$. The marginal cost is $c_i / |w_i| = c_i / (-w_i)$.\n\\end{itemize}\nIn both cases, for a non-zero cost $c_i > 0$, the marginal cost is $c_i / |w_i|$. If $c_i = 0$ and $w_i \\ne 0$, the marginal cost is $0$, which implies score can be gained for free. Such \"free\" changes should always be made first.\n\nThe algorithm is as follows:\n1.  Calculate the initial score $S_{\\text{initial}} = \\mathbf{w}^\\top \\mathbf{x} + b$. If $S_{\\text{initial}} \\ge 0$, the cost is $0.0$.\n2.  Otherwise, calculate the required score increase $S_{\\text{needed}} = -S_{\\text{initial}}$.\n3.  For each mutable feature $i \\notin \\mathcal{I}$ that can contribute to a score increase (i.e., $w_i > 0$ and $u_i > x_i$, or $w_i < 0$ and $l_i < x_i$), create an \"action\" tuple. Each action represents the potential of one feature and contains its marginal cost ($c_i/|w_i|$), the maximum score gain it can provide, and the cost associated with that maximum gain.\n    \\begin{itemize}\n        \\item For $w_i > 0$: max score gain is $w_i (u_i-x_i)$.\n        \\item For $w_i < 0$: max score gain is $w_i (l_i-x_i)$.\n    \\end{itemize}\n4.  Before proceeding, check for impossibility. Sum the maximum possible score gains from all available actions. If this total is less than $S_{\\text{needed}}$, then achieving approval is impossible, and the result is $-1.0$.\n5.  Sort the actions in ascending order of their marginal cost. This places the most efficient changes first.\n6.  Iterate through the sorted actions. For each action, \"purchase\" as much score gain as needed or as much as is available, whichever is less. The cost incurred for a certain amount of score gain is simply the gain multiplied by the action's marginal cost. Update the remaining score needed and the total accumulated cost.\n7.  Continue until the required score $S_{\\text{needed}}$ has been achieved. The final accumulated cost is the minimal cost $C^\\star$.\n\nThis greedy procedure is guaranteed to find the optimal solution because the marginal cost for each feature's contribution is constant. We are, in effect, filling a \"score knapsack\" of size $S_{\\text{needed}}$ with divisible items (score gains) that have different value-to-weight ratios (inverse marginal costs), and the optimal strategy is to always pick the best ratio first.", "answer": "```python\nimport numpy as np\n\ndef solve_case(w: np.ndarray, b: float, x: np.ndarray, l: np.ndarray, u: np.ndarray, c: np.ndarray, I: set) -> float:\n    \"\"\"\n    Computes the minimal cost for a plausible edit to achieve loan approval.\n    \"\"\"\n    # 1. Calculate initial score\n    initial_score = np.dot(w, x) + b\n    if initial_score >= 0.0:\n        return 0.0\n\n    score_needed = -initial_score\n\n    # 2. Build list of possible actions\n    actions = []\n    d = len(w)\n    for i in range(d):\n        if i in I:\n            continue\n        \n        wi = w[i]\n        ci = c[i]\n        \n        if wi == 0:\n            continue\n\n        if wi > 0:\n            # We need to increase x_i, so delta_i > 0\n            # Change is bounded by [max(0, l_i-x_i), u_i-x_i]\n            # We are interested in positive delta_i, so max change is u_i-x_i\n            max_delta = u[i] - x[i]\n            if max_delta > 0:\n                max_score_gain = wi * max_delta\n                # Handle c_i = 0 case, which means infinite efficiency (zero cost)\n                cost_per_score = ci / wi if ci > 0 else 0.0\n                actions.append({'cost_per_score': cost_per_score, 'max_gain': max_score_gain})\n        \n        elif wi < 0:\n            # We need to decrease x_i, so delta_i < 0\n            # Change is bounded by [l_i-x_i, min(0, u_i-x_i)]\n            # We are interested in negative delta_i, so max change (magnitude) is x_i - l_i\n            max_delta_magnitude = x[i] - l[i]\n            if max_delta_magnitude > 0:\n                # Actual delta is -(x_i - l_i) = l_i - x_i\n                max_score_gain = wi * (l[i] - x[i]) # both negative, product is positive\n                # Similarly handle c_i = 0\n                cost_per_score = ci / abs(wi) if ci > 0 else 0.0\n                actions.append({'cost_per_score': cost_per_score, 'max_gain': max_score_gain})\n    \n    # 3. Check for impossibility\n    max_possible_gain = sum(action['max_gain'] for action in actions)\n    if max_possible_gain < score_needed:\n        return -1.0\n        \n    # 4. Sort actions by cost-effectiveness (lowest cost per score first)\n    actions.sort(key=lambda p: p['cost_per_score'])\n    \n    # 5. Greedily apply actions\n    total_cost = 0.0\n    remaining_score_needed = score_needed\n    \n    for action in actions:\n        if remaining_score_needed <= 0:\n            break\n            \n        gain_from_this_action = min(remaining_score_needed, action['max_gain'])\n        cost_for_this_gain = gain_from_this_action * action['cost_per_score']\n        \n        total_cost += cost_for_this_gain\n        remaining_score_needed -= gain_from_this_action\n        \n    return total_cost\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        # Case A\n        {\n            'w': [1.0, 0.5, -0.2], 'b': -0.5,\n            'x': [0.0, 0.0, 0.0],\n            'l': [0.0, -1.0, -1.0], 'u': [2.0, 2.0, 1.0],\n            'c': [1.0, 2.0, 1.0],\n            'I': []\n        },\n        # Case B\n        {\n            'w': [1.0, 1.0], 'b': -1.0,\n            'x': [0.6, 0.6],\n            'l': [0.0, 0.0], 'u': [1.0, 1.0],\n            'c': [1.0, 1.0],\n            'I': []\n        },\n        # Case C\n        {\n            'w': [0.1, 0.1], 'b': -5.0,\n            'x': [0.0, 0.0],\n            'l': [0.0, 0.0], 'u': [10.0, 10.0],\n            'c': [1.0, 1.0],\n            'I': []\n        },\n        # Case D\n        {\n            'w': [1.0, 0.5], 'b': -0.4,\n            'x': [0.0, 0.0],\n            'l': [0.0, 0.0], 'u': [0.1, 2.0],\n            'c': [0.1, 10.0],\n            'I': [0]\n        },\n        # Case E\n        {\n            'w': [-1.0], 'b': -0.4,\n            'x': [0.0],\n            'l': [-1.0], 'u': [1.0],\n            'c': [3.0],\n            'I': []\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        w = np.array(case['w'])\n        b = case['b']\n        x = np.array(case['x'])\n        l = np.array(case['l'])\n        u = np.array(case['u'])\n        c = np.array(case['c'])\n        I = set(case['I'])\n        \n        result = solve_case(w, b, x, l, u, c, I)\n        results.append(result)\n\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```", "id": "2438838"}]}