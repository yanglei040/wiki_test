{"hands_on_practices": [{"introduction": "To begin our exploration of Polynomial Identity Testing (PIT), let's ground ourselves in a practical verification scenario. This first exercise simulates the task of checking if two different hardware designs compute the same function. By calculating the exact probability that a random test fails to distinguish two genuinely different polynomials, you will gain a concrete understanding of the core trade-off in randomized testing: speed versus the small, but manageable, chance of error [@problem_id:1435754].", "problem": "A software verification engineer is tasked with checking the equivalence of two arithmetic modules designed to perform computations on a pair of input variables, $x$ and $y$. The first module is specified to compute the polynomial $P_1(x, y) = x(x+y)^2$. The second module computes the polynomial $P_2(x, y) = x^2(x+y) + y(x^2+y)$.\n\nTo verify if the two modules are equivalent (i.e., if $P_1$ and $P_2$ represent the same polynomial), the engineer employs a randomized test. The test consists of a single run where a pair of integer inputs, $(r_x, r_y)$, is chosen by selecting $r_x$ and $r_y$ independently and uniformly at random from the set $S = \\{0, 1, 2, \\dots, 99\\}$. The outputs of the two modules for this input pair are then compared.\n\nThe test is considered to have produced a \"false positive\" if the modules are functionally different (meaning $P_1$ and $P_2$ are not the same polynomial), but the specific random inputs $(r_x, r_y)$ chosen for the test happen to yield an identical output, i.e., $P_1(r_x, r_y) = P_2(r_x, r_y)$.\n\nWhat is the exact probability of the test producing a false positive?\n\nA. $\\frac{199}{10000}$\n\nB. $\\frac{3}{100}$\n\nC. $\\frac{1}{50}$\n\nD. $\\frac{3}{10000}$", "solution": "We first expand each polynomial symbolically. For $P_{1}$,\n$$\nP_{1}(x,y)=x(x+y)^{2}=x(x^{2}+2xy+y^{2})=x^{3}+2x^{2}y+xy^{2}.\n$$\nFor $P_{2}$,\n$$\nP_{2}(x,y)=x^{2}(x+y)+y(x^{2}+y)=x^{3}+x^{2}y+x^{2}y+y^{2}=x^{3}+2x^{2}y+y^{2}.\n$$\nTherefore, their difference is\n$$\nP_{1}(x,y)-P_{2}(x,y)=xy^{2}-y^{2}=y^{2}(x-1).\n$$\nThe two modules produce the same output on an input pair $(r_{x},r_{y})$ exactly when $P_{1}(r_{x},r_{y})=P_{2}(r_{x},r_{y})$, i.e., when\n$$\ny^{2}(x-1)=0.\n$$\nOver integers, this occurs if and only if $y=0$ or $x=1$.\n\nSince $r_{x}$ and $r_{y}$ are chosen independently and uniformly from $S=\\{0,1,2,\\dots,99\\}$, the total number of input pairs is $|S|\\cdot|S|=100\\cdot 100=10000$. The number of pairs with $y=0$ is $100$ (any $x\\in S$), the number with $x=1$ is $100$ (any $y\\in S$), and the overlap $(x=1,y=0)$ is counted twice, so by the inclusion-exclusion principle the total number of pairs yielding equality is\n$$\n100+100-1=199.\n$$\nTherefore, the exact probability of a false positive (given the polynomials are not identical) is\n$$\n\\frac{199}{10000},\n$$\nwhich corresponds to option A.", "answer": "$$\\boxed{A}$$", "id": "1435754"}, {"introduction": "The power of randomized PIT comes from choosing test points from a large set, making it hard for a non-zero polynomial to \"hide\" its roots. But what if we used a small, fixed set of points instead? This practice challenges you to think like an adversary and construct a non-zero polynomial that cleverly passes a deterministic test by having roots at all the test points. This exercise highlights the critical weakness of naive testing and powerfully illustrates why the randomness and the size of the evaluation set, as stipulated by the Schwartz-Zippel lemma, are essential for the algorithm's success [@problem_id:1435764].", "problem": "In computational complexity theory, Polynomial Identity Testing (PIT) is the problem of efficiently determining whether a given polynomial is identically equal to the zero polynomial. A common family of algorithms for PIT involves evaluating the polynomial at a set of test points. If all evaluations result in zero, the algorithm concludes that the polynomial is the zero polynomial. However, if the set of test points is too small or not chosen carefully, such an algorithm can be \"fooled\".\n\nConsider a simple randomized PIT algorithm that tests a two-variable polynomial $P(x,y)$ by evaluating it at all points in the set $S = \\{(0,0), (0,1), (1,0), (1,1)\\}$. If $P(x,y)=0$ for all four points in $S$, the algorithm reports that $P$ is the zero polynomial.\n\nYour task is to find a specific non-zero polynomial that fools this algorithm. Find the unique non-zero polynomial $P(x,y)$ that satisfies all three of the following conditions:\n1. It has the general form $P(x,y) = x^2 + A y^2 + B x + C y + D$, where $A, B, C, D$ are rational constants.\n2. It is symmetric with respect to its variables, meaning $P(x,y) = P(y,x)$ for all $x,y$.\n3. It evaluates to zero for all pairs $(x,y)$ in the set $S$.\n\nPresent the resulting polynomial expression for $P(x,y)$ as your final answer.", "solution": "We seek a polynomial of the form $P(x,y) = x^{2} + A y^{2} + B x + C y + D$ with rational constants $A,B,C,D$, which is symmetric, i.e., $P(x,y) = P(y,x)$ for all $x,y$, and vanishes on $S = \\{(0,0),(0,1),(1,0),(1,1)\\}$.\n\nSymmetry under swapping $x$ and $y$ requires matching coefficients between $P(x,y)$ and $P(y,x) = y^{2} + A x^{2} + B y + C x + D$. Equating coefficients for all $x,y$ gives:\n$$A = 1,\\quad C = B.$$\nThus the polynomial reduces to\n$$P(x,y) = x^{2} + y^{2} + B(x + y) + D.$$\n\nImposing the vanishing conditions on $S$:\n- At $(0,0)$: $P(0,0) = D = 0$, hence $D = 0$.\n- At $(1,0)$: $P(1,0) = 1 + B + D = 0 \\Rightarrow 1 + B = 0 \\Rightarrow B = -1$.\n- At $(0,1)$: $P(0,1) = 1 + B + D = 0 \\Rightarrow B = -1$ (consistent).\n- At $(1,1)$: $P(1,1) = 2 + 2B + D = 0 \\Rightarrow 2 + 2(-1) + 0 = 0$ (consistent).\n\nTherefore, the unique coefficients are $A = 1$, $B = -1$, $C = -1$, $D = 0$, yielding\n$$P(x,y) = x^{2} + y^{2} - x - y.$$\nThis polynomial is non-zero, symmetric, and evaluates to zero on all points in $S$, hence it fools the given PIT algorithm.", "answer": "$$\\boxed{x^{2} + y^{2} - x - y}$$", "id": "1435764"}, {"introduction": "While a single random evaluation is fast, its probability of error might be too high for critical applications. This final practice demonstrates how to overcome this limitation through a fundamental technique in randomized algorithms: probability amplification. You will determine the precise number of independent trials required to boost the algorithm's reliability to any desired level, showing how we can make the probability of failure astronomically small. This transforms PIT from a neat theoretical trick into a robust and practical engineering tool [@problem_id:1435768].", "problem": "An engineer is tasked with verifying a complex calculation unit in a new processor. The function computed by this unit can be modeled as a multivariate polynomial $P(x_1, \\dots, x_n)$. The goal is to perform Polynomial Identity Testing (PIT) to check if the unit is malfunctioning and computing the identically zero polynomial, $P \\equiv 0$, instead of the intended non-zero function.\n\nDue to the complexity of the circuit, an exhaustive symbolic verification is computationally infeasible. Instead, the engineer uses a randomized test. The test consists of choosing a random input vector $(r_1, \\dots, r_n)$ and evaluating the polynomial. Each component $r_i$ is chosen independently and uniformly at random from a finite set $S$.\n\nIf $P(r_1, \\dots, r_n) \\neq 0$, the test correctly reports that the polynomial is non-zero. If $P(r_1, \\dots, r_n) = 0$, the test reports that the polynomial is zero. This test has a one-sided error: if the true polynomial is non-zero, there is a chance the test will incorrectly report it as zero by coincidentally picking a root. It is known from the properties of polynomials that for any non-zero polynomial $P$ with a total degree of $d$, the probability of a single test incorrectly reporting \"zero\" is at most $\\frac{d}{|S|}$, where $|S|$ is the size of the set $S$. We can assume that for any practical test, $|S|  d$.\n\nTo increase confidence in the result, the engineer decides to run $k$ independent trials. The overall conclusion is \"zero\" only if all $k$ trials report \"zero\". If any trial reports \"non-zero\", the overall conclusion is \"non-zero\".\n\nDetermine the minimum integer number of trials, $k$, that must be performed to ensure that the probability of incorrectly concluding that a non-zero polynomial is \"zero\" is at most $\\delta$, where $\\delta$ is a small positive value representing the desired error tolerance. Your answer should be an analytical expression in terms of $d$, $|S|$, and $\\delta$. Since $k$ must be an integer, use the ceiling function, $\\lceil \\cdot \\rceil$, in your final expression.", "solution": "Let's analyze the probability of error for the overall procedure. The procedure yields an incorrect result only in one specific case: when the polynomial $P$ is non-zero, but all $k$ trials happen to evaluate to zero.\n\nLet $E$ be the event that the overall procedure incorrectly concludes that a non-zero polynomial is \"zero\". This occurs if and only if all $k$ independent trials fail. Let $F_i$ be the event that the $i$-th trial fails, i.e., it returns \"zero\" for a non-zero polynomial.\n\nThe problem states that the probability of a single trial failing is bounded from above. For any non-zero polynomial of degree $d$, the probability of failure for the $i$-th trial is:\n$$ P(F_i) \\le \\frac{d}{|S|} $$\nLet's denote this upper bound on the single-trial error probability as $p_{err} = \\frac{d}{|S|}$.\n\nThe overall error event $E$ is the intersection of the individual failure events: $E = F_1 \\cap F_2 \\cap \\dots \\cap F_k$. Since the trials are independent, the probability of the overall error is the product of the probabilities of the individual failures:\n$$ P(E) = P(F_1) \\times P(F_2) \\times \\dots \\times P(F_k) $$\nUsing the upper bound for each trial, we can bound the overall error probability:\n$$ P(E) \\le (p_{err})^k = \\left(\\frac{d}{|S|}\\right)^k $$\nWe are given the requirement that this overall error probability must be no more than a specified tolerance $\\delta$. Therefore, we need to find the smallest integer $k$ that satisfies the inequality:\n$$ \\left(\\frac{d}{|S|}\\right)^k \\le \\delta $$\nTo solve for $k$, we can take the natural logarithm of both sides.\n$$ \\ln\\left(\\left(\\frac{d}{|S|}\\right)^k\\right) \\le \\ln(\\delta) $$\nUsing the logarithm property $\\ln(a^b) = b \\ln(a)$, we get:\n$$ k \\ln\\left(\\frac{d}{|S|}\\right) \\le \\ln(\\delta) $$\nNow, we wish to isolate $k$. We need to divide by $\\ln\\left(\\frac{d}{|S|}\\right)$. Since the problem states that for a practical test $|S|  d$, the fraction $\\frac{d}{|S|}$ is less than 1. The natural logarithm of a number between 0 and 1 is negative. When we divide an inequality by a negative number, we must reverse the inequality sign:\n$$ k \\ge \\frac{\\ln(\\delta)}{\\ln\\left(\\frac{d}{|S|}\\right)} $$\nThis expression is correct, but we can rewrite it in a more intuitive form using logarithm properties. Note that $\\ln(a/b) = -\\ln(b/a)$ and $\\ln(x) = -\\ln(1/x)$.\n$$ k \\ge \\frac{-\\ln(1/\\delta)}{-\\ln(|S|/d)} $$\n$$ k \\ge \\frac{\\ln(1/\\delta)}{\\ln(|S|/d)} $$\nThis form is often preferred as both the numerator and the denominator are positive (since $\\delta  1$ implies $1/\\delta  1$, and $|S|/d  1$).\n\nThe question asks for the minimum *integer* number of trials. Since $k$ must be an integer and satisfy this inequality, we must take the ceiling of the right-hand side to find the smallest integer value for $k$.\n$$ k = \\left\\lceil \\frac{\\ln(1/\\delta)}{\\ln(|S|/d)} \\right\\rceil $$\nThis is the minimum number of trials required to meet the desired error tolerance.", "answer": "$$\\boxed{\\left\\lceil \\frac{\\ln(1/\\delta)}{\\ln(|S|/d)} \\right\\rceil}$$", "id": "1435768"}]}