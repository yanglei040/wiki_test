{"hands_on_practices": [{"introduction": "To truly understand Principal Component Analysis, it is essential to work through the mechanics from first principles. This first practice problem moves beyond simply using a software package by asking you to perform an analytical eigendecomposition of a simple, structured covariance matrix. By deriving the eigenvalues and eigenvectors by hand, you will gain a deeper intuition for how PCA identifies directions of variance and how the loading vectors can be interpreted as capturing either the \"average\" behavior of or \"contrasts\" between variables. [@problem_id:3161250]", "problem": "You are given a centered three-variable dataset modeled as a zero-mean random vector $X = (X_{1}, X_{2}, X_{3})^{\\top}$ with a known population covariance matrix\n$$\n\\Sigma \\;=\\; \\begin{pmatrix}\n1 & \\tfrac{1}{2} & \\tfrac{1}{2} \\\\\n\\tfrac{1}{2} & 1 & \\tfrac{1}{2} \\\\\n\\tfrac{1}{2} & \\tfrac{1}{2} & 1\n\\end{pmatrix}.\n$$\nThis describes three standardized measurements that share a common positive association. Using only first principles from the definition of covariance, symmetric matrices, and the characterization of Principal Component Analysis (PCA) as the eigendecomposition of the covariance matrix, carry out the following:\n\n1. Compute the eigenvalues and a corresponding set of orthonormal eigenvectors of $\\Sigma$ analytically.\n2. Define the component loadings as the entries of the unit-norm eigenvectors of $\\Sigma$, and interpret each eigenvector in terms of “average” versus “contrast” across variables, explaining how the signs and relative magnitudes of the loadings encode these notions.\n3. Finally, compute the exact fraction of the total variance explained by the “average” component (the one that puts equal positive weight on all three variables). Express this final fraction in simplest exact form as a rational number. No rounding is required.\n\nState only this final fraction as your final answer. Do not include any units or additional text in your final answer.", "solution": "The problem asks for the fraction of total variance explained by the \"average\" component in a Principal Component Analysis (PCA) of a three-variable system. The system is described by a zero-mean random vector $X = (X_{1}, X_{2}, X_{3})^{\\top}$ with the given population covariance matrix $\\Sigma$.\n\nFirst, we validate the problem statement. The givens are the random vector $X$ and its covariance matrix\n$$\n\\Sigma \\;=\\; \\begin{pmatrix}\n1 & \\tfrac{1}{2} & \\tfrac{1}{2} \\\\\n\\tfrac{1}{2} & 1 & \\tfrac{1}{2} \\\\\n\\tfrac{1}{2} & \\tfrac{1}{2} & 1\n\\end{pmatrix}.\n$$\nThe problem specifies that PCA is to be understood as the eigendecomposition of this covariance matrix. The tasks are to find the eigenvalues and eigenvectors, interpret them, and compute a specific fraction of variance. The problem is scientifically grounded in standard multivariate statistics, well-posed with all necessary information provided, and stated objectively. The matrix $\\Sigma$ is symmetric and, as we will show, positive definite, making it a valid covariance matrix. The problem is therefore valid.\n\nWe proceed with the solution in three parts as requested.\n\nPart 1: Compute eigenvalues and orthonormal eigenvectors of $\\Sigma$.\nThe principal components are the eigenvectors of the covariance matrix $\\Sigma$, and their variances are the corresponding eigenvalues. We find the eigenvalues $\\lambda$ by solving the characteristic equation $\\det(\\Sigma - \\lambda I) = 0$, where $I$ is the $3 \\times 3$ identity matrix.\n$$\n\\det \\begin{pmatrix}\n1-\\lambda & \\tfrac{1}{2} & \\tfrac{1}{2} \\\\\n\\tfrac{1}{2} & 1-\\lambda & \\tfrac{1}{2} \\\\\n\\tfrac{1}{2} & \\tfrac{1}{2} & 1-\\lambda\n\\end{pmatrix} = 0\n$$\nExpanding the determinant, we get:\n$$\n(1-\\lambda)\\left( (1-\\lambda)^2 - \\left(\\frac{1}{2}\\right)^2 \\right) - \\frac{1}{2}\\left( \\frac{1}{2}(1-\\lambda) - \\frac{1}{4} \\right) + \\frac{1}{2}\\left( \\frac{1}{4} - \\frac{1}{2}(1-\\lambda) \\right) = 0\n$$\n$$\n(1-\\lambda)\\left(\\lambda^2 - 2\\lambda + 1 - \\frac{1}{4}\\right) - \\frac{1}{2}\\left(\\frac{1}{4} - \\frac{\\lambda}{2}\\right) + \\frac{1}{2}\\left(-\\frac{1}{4} + \\frac{\\lambda}{2}\\right) = 0\n$$\n$$\n(1-\\lambda)(\\lambda^2 - 2\\lambda + \\frac{3}{4}) - \\frac{1}{8} + \\frac{\\lambda}{4} - \\frac{1}{8} + \\frac{\\lambda}{4} = 0\n$$\n$$\n\\lambda^2 - 2\\lambda + \\frac{3}{4} - \\lambda^3 + 2\\lambda^2 - \\frac{3}{4}\\lambda - \\frac{1}{4} + \\frac{\\lambda}{2} = 0\n$$\n$$\n-\\lambda^3 + 3\\lambda^2 - \\left(2 + \\frac{3}{4} - \\frac{1}{2}\\right)\\lambda + \\left(\\frac{3}{4} - \\frac{1}{4}\\right) = 0\n$$\n$$\n-\\lambda^3 + 3\\lambda^2 - \\frac{9}{4}\\lambda + \\frac{1}{2} = 0\n$$\nMultiplying by $-4$ gives a cleaner polynomial:\n$$\n4\\lambda^3 - 12\\lambda^2 + 9\\lambda - 2 = 0\n$$\nBy inspection or the rational root theorem, we can test potential roots. Let's test $\\lambda=2$: $4(2)^3 - 12(2)^2 + 9(2) - 2 = 32 - 48 + 18 - 2 = 0$. So, $\\lambda=2$ is a root.\nLet's test $\\lambda=1/2$: $4(\\frac{1}{2})^3 - 12(\\frac{1}{2})^2 + 9(\\frac{1}{2}) - 2 = 4(\\frac{1}{8}) - 12(\\frac{1}{4}) + \\frac{9}{2} - 2 = \\frac{1}{2} - 3 + \\frac{9}{2} - 2 = \\frac{10}{2} - 5 = 0$. So, $\\lambda=1/2$ is also a root.\nSince $(\\lambda-2)$ is a factor of the polynomial, we can perform polynomial division, which yields:\n$$\n(\\lambda-2)(4\\lambda^2 - 4\\lambda + 1) = 0\n$$\nThe quadratic factor is a perfect square: $4\\lambda^2 - 4\\lambda + 1 = (2\\lambda - 1)^2$.\nThus, the characteristic equation is $(\\lambda-2)(2\\lambda-1)^2 = 0$.\nThe eigenvalues are $\\lambda_1 = 2$ (with multiplicity $1$) and $\\lambda_2 = \\lambda_3 = \\frac{1}{2}$ (with multiplicity $2$).\n\nNow, we find a set of orthonormal eigenvectors.\nFor $\\lambda_1 = 2$: We solve $(\\Sigma - 2I)v = 0$.\n$$\n\\begin{pmatrix} -1 & \\tfrac{1}{2} & \\tfrac{1}{2} \\\\ \\tfrac{1}{2} & -1 & \\tfrac{1}{2} \\\\ \\tfrac{1}{2} & \\tfrac{1}{2} & -1 \\end{pmatrix} \\begin{pmatrix} v_1 \\\\ v_2 \\\\ v_3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nThis system of linear equations simplifies to $v_1 = v_2 = v_3$. An unnormalized eigenvector is $(1, 1, 1)^{\\top}$. To obtain a unit-norm eigenvector $u_1$, we divide by its norm $\\sqrt{1^2+1^2+1^2} = \\sqrt{3}$.\n$$ u_1 = \\begin{pmatrix} 1/\\sqrt{3} \\\\ 1/\\sqrt{3} \\\\ 1/\\sqrt{3} \\end{pmatrix} $$\n\nFor $\\lambda_2 = \\lambda_3 = \\frac{1}{2}$: We solve $(\\Sigma - \\frac{1}{2}I)v = 0$.\n$$\n\\begin{pmatrix} \\tfrac{1}{2} & \\tfrac{1}{2} & \\tfrac{1}{2} \\\\ \\tfrac{1}{2} & \\tfrac{1}{2} & \\tfrac{1}{2} \\\\ \\tfrac{1}{2} & \\tfrac{1}{2} & \\tfrac{1}{2} \\end{pmatrix} \\begin{pmatrix} v_1 \\\\ v_2 \\\\ v_3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nThis reduces to a single equation $v_1 + v_2 + v_3 = 0$. This describes a two-dimensional eigenspace (a plane) orthogonal to the eigenvector $u_1$, as expected for a symmetric matrix. We need to find an orthonormal basis for this plane.\nLet's choose a simple vector satisfying the condition, for instance, $v' = (1, -1, 0)^{\\top}$. Normalizing it gives our second eigenvector $u_2$:\n$$ u_2 = \\frac{1}{\\sqrt{1^2+(-1)^2+0^2}} \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1/\\sqrt{2} \\\\ -1/\\sqrt{2} \\\\ 0 \\end{pmatrix} $$\nFor the third eigenvector $u_3$, it must be in the same plane ($u_{3,1}+u_{3,2}+u_{3,3}=0$) and also orthogonal to $u_2$ ($u_2 \\cdot u_3 = 0$). The orthogonality condition gives $\\frac{1}{\\sqrt{2}}u_{3,1} - \\frac{1}{\\sqrt{2}}u_{3,2} = 0$, which implies $u_{3,1} = u_{3,2}$. Substituting this into the plane equation gives $2u_{3,1} + u_{3,3} = 0$, or $u_{3,3} = -2u_{3,1}$. Thus, any such vector has the form $(c, c, -2c)^{\\top}$. We normalize it to find $u_3$:\n$$ u_3 = \\frac{1}{\\sqrt{c^2+c^2+(-2c)^2}} \\begin{pmatrix} c \\\\ c \\\\ -2c \\end{pmatrix} = \\frac{1}{\\sqrt{6c^2}} \\begin{pmatrix} c \\\\ c \\\\ -2c \\end{pmatrix} = \\begin{pmatrix} 1/\\sqrt{6} \\\\ 1/\\sqrt{6} \\\\ -2/\\sqrt{6} \\end{pmatrix} $$\n\nPart 2: Interpret the eigenvectors.\nThe eigenvectors are the principal components, and their elements are the \"loadings\".\n- The first principal component, corresponding to $u_1=(1/\\sqrt{3}, 1/\\sqrt{3}, 1/\\sqrt{3})^{\\top}$, has equal positive loadings on all three variables. This component represents the weighted average of the variables, $Z_1 = \\frac{1}{\\sqrt{3}}(X_1+X_2+X_3)$, capturing their tendency to vary together. This is the \"average\" component referred to in the problem.\n- The second and third principal components, $u_2$ and $u_3$, have both positive and negative loadings. They represent \"contrasts\". For example, $u_2=(1/\\sqrt{2}, -1/\\sqrt{2}, 0)^{\\top}$ captures the difference between $X_1$ and $X_2$. $u_3=(1/\\sqrt{6}, 1/\\sqrt{6}, -2/\\sqrt{6})^{\\top}$ captures a contrast between the average of $X_1$ and $X_2$ versus $X_3$.\n\nPart 3: Compute the fraction of total variance explained.\nThe total variance in the dataset is the sum of the variances of the individual variables, which is the trace of the covariance matrix $\\Sigma$.\n$$ \\text{Total Variance} = \\text{tr}(\\Sigma) = 1 + 1 + 1 = 3 $$\nThis sum must also equal the sum of the eigenvalues: $\\sum_{i=1}^3 \\lambda_i = 2 + \\frac{1}{2} + \\frac{1}{2} = 3$, which is consistent.\nThe variance explained by each principal component is its corresponding eigenvalue. The \"average\" component is the first principal component, with variance $\\lambda_1 = 2$.\nThe fraction of the total variance explained by the \"average\" component is the ratio of its variance to the total variance:\n$$ \\text{Fraction of Variance} = \\frac{\\lambda_1}{\\text{tr}(\\Sigma)} = \\frac{2}{3} $$\nThis is an exact rational number in its simplest form.", "answer": "$$\\boxed{\\frac{2}{3}}$$", "id": "3161250"}, {"introduction": "Principal Component Analysis is designed to explain the variance structure of data, not its location in space. This exercise uses a thought experiment to demonstrate why centering your data—subtracting the mean from each variable—is a critical prerequisite for a meaningful analysis. By exploring what happens when a constant, non-varying feature is added to a dataset, you will see how omitting the centering step can lead PCA to identify the data's mean offset from the origin, rather than its true directions of variation. [@problem_id:3161268]", "problem": "Consider a data matrix $X \\in \\mathbb{R}^{n \\times p}$ whose rows index $n$ observations and columns index $p$ variables. You augment $X$ with a constant column to obtain $X' \\in \\mathbb{R}^{n \\times (p+1)}$ defined by $X' = [X \\ \\ \\alpha \\mathbf{1}_n]$, where $\\alpha \\in \\mathbb{R}$ is a fixed constant and $\\mathbf{1}_n \\in \\mathbb{R}^n$ is the vector of ones. Define the column-centering operation as left-multiplication by the centering matrix $C \\in \\mathbb{R}^{n \\times n}$, where $C = I_n - \\frac{1}{n}\\mathbf{1}_n \\mathbf{1}_n^\\top$, so that the centered matrix is $X_c' = C X'$. Principal Components Analysis (PCA) is performed by computing the eigenvectors (loadings) of the sample covariance matrix of the centered data.\n\nStarting from first principles and core definitions:\n- Column centering subtracts, from each column, its sample mean across rows.\n- The sample covariance matrix of the centered data is the symmetric matrix that captures pairwise centered second moments of the variables.\n- PCA loadings are the eigenvectors of the sample covariance matrix, and principal component scores are the projections of the centered data onto these loadings.\n\nUsing these bases, analyze the effect of the added constant column on $X'$ under column centering and discuss the consequences for PCA loadings when centering is omitted. Then select all statements that are true.\n\nA. After column centering via $C = I_n - \\frac{1}{n}\\mathbf{1}_n \\mathbf{1}_n^\\top$, the added constant column becomes the zero vector and does not affect the sample covariance matrix or the principal component loadings.\n\nB. Even without centering, the constant column has zero variance and therefore cannot affect principal components computed from the second moment matrix $X'^\\top X'$.\n\nC. Centering is critical because principal component loadings are defined as eigenvectors of the sample covariance matrix of the centered data; without centering, loadings can be dominated by the mean offset, yielding components that explain location rather than variation.\n\nD. Scaling each column to unit variance (standardization) is sufficient to remove the influence of a constant column even if the data are not centered.\n\nE. Column centering is implemented by right-multiplying $X'$ by $I_{p+1} - \\frac{1}{p+1}\\mathbf{1}_{p+1}\\mathbf{1}_{p+1}^\\top$, so the constant column is preserved after centering because this operation acts on rows, not columns.\n\nSelect all correct options.", "solution": "The problem asks to analyze the effect of adding a constant column to a data matrix on Principal Component Analysis (PCA), both with and without the standard preprocessing step of column centering.\n\nLet's analyze the options:\n\n**A. After column centering via $C = I_n - \\frac{1}{n}\\mathbf{1}_n \\mathbf{1}_n^\\top$, the added constant column becomes the zero vector and does not affect the sample covariance matrix or the principal component loadings.**\n\nThis statement is **true**. The column centering operation subtracts the column's mean from each of its elements. The added column is $\\mathbf{x}_{p+1} = \\alpha \\mathbf{1}_n$, so its mean is $\\alpha$. After centering, each element becomes $\\alpha - \\alpha = 0$. The entire column is thus transformed into a zero vector.\nWhen computing the sample covariance matrix, this zero vector will have zero variance and zero covariance with all other columns. Consequently, the covariance matrix of the augmented data will be a block matrix where the original covariance matrix is preserved, and the new row and column corresponding to the constant feature are all zeros. This adds a trivial principal component with zero variance but does not alter the principal components (loadings and variances) of the original variables.\n\n**B. Even without centering, the constant column has zero variance and therefore cannot affect principal components computed from the second moment matrix $X'^\\top X'$.**\n\nThis statement is **false**. While it is true that a constant column has zero *variance* (which is a centered measure), performing PCA without centering computes eigenvectors of the *second moment matrix* $(X')^\\top X'$, not the covariance matrix. This matrix includes terms that depend on the means of the columns. The added constant column (with value $\\alpha$) will have non-zero cross-product terms with any other column that has a non-zero mean. This coupling alters the entire matrix and its eigenvectors, meaning the constant column absolutely does affect the resulting principal components.\n\n**C. Centering is critical because principal component loadings are defined as eigenvectors of the sample covariance matrix of the centered data; without centering, loadings can be dominated by the mean offset, yielding components that explain location rather than variation.**\n\nThis statement is **true**. It articulates the fundamental motivation for centering data before PCA. PCA's goal is to find directions of maximal *variance*. Variance is, by definition, the spread of data around its mean. By omitting the centering step, the analysis is performed on raw second moments, which conflate the data's variance with its location (mean). For data far from the origin, the direction of the mean vector often becomes the dominant direction of \"variation\" from the origin, and the first principal component will align with it, capturing location instead of the desired covariance structure.\n\n**D. Scaling each column to unit variance (standardization) is sufficient to remove the influence of a constant column even if the data are not centered.**\n\nThis statement is **false**. To scale a column to unit variance, one must divide it by its standard deviation. The standard deviation of a constant column is zero. Division by zero is an undefined operation, so a constant column cannot be scaled to unit variance.\n\n**E. Column centering is implemented by right-multiplying $X'$ by $I_{p+1} - \\frac{1}{p+1}\\mathbf{1}_{p+1}\\mathbf{1}_{p+1}^\\top$, so the constant column is preserved after centering because this operation acts on rows, not columns.**\n\nThis statement is **false**. The problem correctly defines column centering as *left-multiplication* by the $n \\times n$ matrix $C$. Right-multiplying by the $(p+1) \\times (p+1)$ matrix described in the option would perform *row centering* (subtracting the mean of each row from its elements), which is a different operation and not the standard preprocessing for PCA.\n\nTherefore, the only true statements are A and C.", "answer": "$$\\boxed{AC}$$", "id": "3161268"}, {"introduction": "After centering, a practitioner's next question is often whether to also scale the variables to have unit variance. This choice determines whether you perform PCA on the covariance matrix or the correlation matrix, and it has profound consequences. This coding-based exercise investigates a scenario where variables are measured with different levels of error, directly affecting their variances. You will discover how covariance-based PCA can be misled into assigning higher importance to noisier variables, and why using the correlation matrix provides a more robust analysis by placing all variables on an equal footing. [@problem_id:3161286]", "problem": "Consider two observed variables $y_1$ and $y_2$ generated from a common latent signal $z$ and independent measurement errors. Let $z$ be a zero-mean random variable with variance $\\operatorname{Var}(z)=1$, and define $y_1=z+\\epsilon_1$ and $y_2=z+\\epsilon_2$, where $\\epsilon_1$ and $\\epsilon_2$ are zero-mean, mutually independent, and independent of $z$, with variances $\\operatorname{Var}(\\epsilon_1)=\\tau_1^2$ and $\\operatorname{Var}(\\epsilon_2)=\\tau_2^2$. The goal is to analyze how increasing the measurement error variance $\\tau_1^2$ inflates the weight of variable $y_1$ in the first principal component loading under Principal Components Analysis (PCA) computed from the covariance matrix, and why PCA computed from the correlation matrix mitigates this inflation.\n\nUse the following foundational definitions:\n- The covariance matrix $S$ of $(y_1,y_2)$ is given by $S=\\begin{bmatrix}\\operatorname{Var}(y_1)&\\operatorname{Cov}(y_1,y_2)\\\\ \\operatorname{Cov}(y_1,y_2)&\\operatorname{Var}(y_2)\\end{bmatrix}$.\n- The correlation matrix $R$ of $(y_1,y_2)$ is given by $R=D^{-1/2}SD^{-1/2}$, where $D=\\operatorname{diag}(\\operatorname{Var}(y_1),\\operatorname{Var}(y_2))$.\n- Principal Components Analysis (PCA) computes principal directions as the eigenvectors of the chosen scatter matrix (either $S$ for covariance-PCA or $R$ for correlation-PCA). The first principal component loading vector is the unit-norm eigenvector associated with the largest eigenvalue.\n- For a loading vector $v$ corresponding to the first principal component, the loading of variable $y_j$ is the $j$-th entry of $v$. Because eigenvectors are defined up to sign, use the magnitude of the loading, i.e., $\\lvert v_j \\rvert$.\n\nLet $\\tau_2^2=0.5$ be fixed. For each test case, construct $S$ and $R$ using the model above and compute:\n1. The magnitude of the loading for variable $y_1$ in the first principal component under covariance-PCA, denoted $l_1^{\\text{cov}}$.\n2. The magnitude of the loading for variable $y_1$ in the first principal component under correlation-PCA, denoted $l_1^{\\text{cor}}$.\n3. The ratio $r=\\dfrac{l_1^{\\text{cov}}}{l_1^{\\text{cor}}}$.\n\nYour program must implement the calculations exactly in the infinite-sample limit implied by the model, i.e., compute $S$ and $R$ analytically from $(\\tau_1^2,\\tau_2^2)$ without any Monte Carlo sampling.\n\nTest suite:\n- Case A (baseline): $\\tau_1^2=0.0$.\n- Case B (moderate error): $\\tau_1^2=0.5$.\n- Case C (large error): $\\tau_1^2=2.0$.\n- Case D (very large error): $\\tau_1^2=20.0$.\n\nFor each case, output the ratio $r$ as a float. Your program should produce a single line of output containing the four ratios in order as a comma-separated list enclosed in square brackets (e.g., $[r_A,r_B,r_C,r_D]$). There are no physical units or angle units in this problem. The answers are floats and must be computed deterministically from the definitions above.", "solution": "We begin from the generative model definitions and the basic properties of covariance and correlation. The latent signal $z$ has $\\operatorname{Var}(z)=1$, the measurement errors are independent with $\\operatorname{Var}(\\epsilon_1)=\\tau_1^2$ and $\\operatorname{Var}(\\epsilon_2)=\\tau_2^2$, and $z,\\epsilon_1,\\epsilon_2$ are mutually independent. Therefore, by the bilinearity of covariance and independence, we have\n$$\n\\operatorname{Var}(y_1)=\\operatorname{Var}(z)+\\operatorname{Var}(\\epsilon_1)=1+\\tau_1^2,\n\\quad\n\\operatorname{Var}(y_2)=\\operatorname{Var}(z)+\\operatorname{Var}(\\epsilon_2)=1+\\tau_2^2,\n$$\nand\n$$\n\\operatorname{Cov}(y_1,y_2)=\\operatorname{Cov}(z+\\epsilon_1,z+\\epsilon_2)=\\operatorname{Cov}(z,z)+\\operatorname{Cov}(z,\\epsilon_2)+\\operatorname{Cov}(\\epsilon_1,z)+\\operatorname{Cov}(\\epsilon_1,\\epsilon_2)=1+0+0+0=1.\n$$\nThus the covariance matrix $S$ is\n$$\nS=\\begin{bmatrix}1+\\tau_1^2 & 1 \\\\ 1 & 1+\\tau_2^2\\end{bmatrix}.\n$$\nThe correlation matrix $R$ of $(y_1,y_2)$ rescales by the inverse standard deviations of $y_1$ and $y_2$. Let $D=\\operatorname{diag}(1+\\tau_1^2,\\,1+\\tau_2^2)$. Then\n$$\nR=D^{-1/2}SD^{-1/2}\n=\\begin{bmatrix}\n\\frac{1}{\\sqrt{1+\\tau_1^2}} & 0 \\\\\n0 & \\frac{1}{\\sqrt{1+\\tau_2^2}}\n\\end{bmatrix}\n\\begin{bmatrix}\n1+\\tau_1^2 & 1 \\\\\n1 & 1+\\tau_2^2\n\\end{bmatrix}\n\\begin{bmatrix}\n\\frac{1}{\\sqrt{1+\\tau_1^2}} & 0 \\\\\n0 & \\frac{1}{\\sqrt{1+\\tau_2^2}}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 & \\rho \\\\\n\\rho & 1\n\\end{bmatrix},\n$$\nwhere\n$$\n\\rho=\\frac{\\operatorname{Cov}(y_1,y_2)}{\\sqrt{\\operatorname{Var}(y_1)\\operatorname{Var}(y_2)}}\n=\\frac{1}{\\sqrt{(1+\\tau_1^2)(1+\\tau_2^2)}}.\n$$\nPrincipal Components Analysis (PCA) computes orthonormal eigenvectors of the scatter matrix. For covariance-PCA, we use $S$, and for correlation-PCA, we use $R$. The first principal component loading vector is the unit-norm eigenvector associated with the largest eigenvalue. For a loading vector $v$ corresponding to the largest eigenvalue of the chosen matrix, the loading magnitude for variable $y_1$ is $\\lvert v_1\\rvert$.\n\nWe now analyze how increasing $\\tau_1^2$ affects the first principal component under the two choices of scatter matrix.\n\nUnder covariance-PCA, the matrix $S$ has entries $S_{11}=1+\\tau_1^2$, $S_{22}=1+\\tau_2^2$, and $S_{12}=S_{21}=1$. As $\\tau_1^2$ increases while $\\tau_2^2$ remains fixed, $S_{11}$ grows, and $S$ becomes increasingly diagonally dominant in the $(1,1)$ entry relative to $(2,2)$, while the off-diagonal entry $S_{12}$ stays at $1$. Intuitively, the first principal component maximizes the Rayleigh quotient $v^\\top S v$ subject to $\\lVert v\\rVert_2=1$, and as $S_{11}$ grows, directions with larger weight on the first coordinate achieve larger variance. This tilts the first principal eigenvector toward the $y_1$ axis, increasing $\\lvert v_1\\rvert$.\n\nUnder correlation-PCA, the matrix $R$ has unit diagonal entries and off-diagonal entry $\\rho=\\frac{1}{\\sqrt{(1+\\tau_1^2)(1+\\tau_2^2)}}$. Increasing $\\tau_1^2$ does not change the diagonal entries of $R$ (they remain $1$), but decreases $\\rho$ because the numerator stays fixed at $1$ while the denominator increases. The eigenstructure of the symmetric matrix\n$$\nR=\\begin{bmatrix}1 & \\rho \\\\ \\rho & 1\\end{bmatrix}\n$$\nis simple: the eigenvectors are $u_+=\\frac{1}{\\sqrt{2}}\\begin{bmatrix}1 \\\\ 1\\end{bmatrix}$ with eigenvalue $1+\\rho$, and $u_-=\\frac{1}{\\sqrt{2}}\\begin{bmatrix}1 \\\\ -1\\end{bmatrix}$ with eigenvalue $1-\\rho$. For $\\rho\\ge 0$, the largest eigenvalue is $1+\\rho$, and its eigenvector $u_+$ has equal weights on $y_1$ and $y_2$, with loading magnitude $\\lvert (u_+)_1\\rvert=\\frac{1}{\\sqrt{2}}$. Critically, this eigenvector is independent of $\\rho$, so increasing $\\tau_1^2$ does not inflate the loading magnitude for $y_1$ under correlation-PCA; it stays at $\\frac{1}{\\sqrt{2}}$.\n\nAlgorithmically, for each test case:\n1. Set $\\tau_2^2=0.5$ and read the given $\\tau_1^2$.\n2. Construct $S=\\begin{bmatrix}1+\\tau_1^2 & 1 \\\\ 1 & 1+\\tau_2^2\\end{bmatrix}$.\n3. Construct $R=D^{-1/2}SD^{-1/2}$ with $D=\\operatorname{diag}(1+\\tau_1^2,\\,1+\\tau_2^2)$.\n4. Compute the eigenvalues and eigenvectors of $S$, select the eigenvector corresponding to the largest eigenvalue, normalize it to unit norm, and set $l_1^{\\text{cov}}$ to the absolute value of its first entry.\n5. Repeat step 4 for $R$ to obtain $l_1^{\\text{cor}}$.\n6. Compute the ratio $r=\\dfrac{l_1^{\\text{cov}}}{l_1^{\\text{cor}}}$.\n7. Aggregate the four ratios $[r_A,r_B,r_C,r_D]$ into a single printed line.\n\nThis procedure deterministically quantifies the inflation of the weight of variable $y_1$ in the first principal component under covariance-PCA caused by increased measurement error variance $\\tau_1^2$, and demonstrates that correlation-PCA mitigates this effect by standardizing variables, keeping the principal direction symmetric when only two variables with identical diagonal entries are considered. The test suite includes a baseline with no added error, moderate error, large error, and very large error, thereby covering typical behavior and asymptotic tilting under covariance-PCA while showing robustness of correlation-PCA loadings.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef first_pc_loading_var1(matrix: np.ndarray) -> float:\n    \"\"\"\n    Compute the magnitude of the loading for variable 1 (index 0)\n    in the first principal component (largest eigenvalue) for a\n    symmetric 2x2 matrix.\n    \"\"\"\n    # Use eigh for symmetric matrices; returns eigenvalues in ascending order.\n    evals, evecs = np.linalg.eigh(matrix)\n    idx = int(np.argmax(evals))\n    v = evecs[:, idx]\n    # Ensure unit-norm eigenvector (eigh returns normalized eigenvectors).\n    # Take absolute value to remove sign ambiguity.\n    return float(abs(v[0]))\n\ndef build_covariance_matrix(tau1_sq: float, tau2_sq: float) -> np.ndarray:\n    \"\"\"\n    Build the 2x2 covariance matrix S for y1 = z + e1, y2 = z + e2,\n    with Var(z)=1, Var(e1)=tau1_sq, Var(e2)=tau2_sq, and all independent.\n    \"\"\"\n    var1 = 1.0 + tau1_sq\n    var2 = 1.0 + tau2_sq\n    cov12 = 1.0\n    return np.array([[var1, cov12],\n                     [cov12, var2]], dtype=float)\n\ndef build_correlation_matrix_from_cov(S: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Build the correlation matrix R = D^{-1/2} S D^{-1/2},\n    where D = diag(S[0,0], S[1,1]).\n    \"\"\"\n    var1 = S[0, 0]\n    var2 = S[1, 1]\n    inv_sqrt_d = np.array([[1.0 / np.sqrt(var1), 0.0],\n                           [0.0, 1.0 / np.sqrt(var2)]], dtype=float)\n    R = inv_sqrt_d @ S @ inv_sqrt_d\n    return R\n\ndef solve():\n    # Fixed tau2^2 as specified in the problem.\n    tau2_sq = 0.5\n    # Define the test cases for tau1^2.\n    test_cases = [0.0, 0.5, 2.0, 20.0]\n\n    results = []\n    for tau1_sq in test_cases:\n        # Construct covariance matrix S.\n        S = build_covariance_matrix(tau1_sq, tau2_sq)\n        # Construct correlation matrix R.\n        R = build_correlation_matrix_from_cov(S)\n        # Compute loadings for variable 1 under covariance-PCA and correlation-PCA.\n        l1_cov = first_pc_loading_var1(S)\n        l1_cor = first_pc_loading_var1(R)\n        # Compute ratio r = l1_cov / l1_cor.\n        r = l1_cov / l1_cor\n        results.append(r)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3161286"}]}