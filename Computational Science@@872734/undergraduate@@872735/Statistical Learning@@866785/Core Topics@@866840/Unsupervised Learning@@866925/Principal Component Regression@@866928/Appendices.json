{"hands_on_practices": [{"introduction": "A critical step in applying Principal Component Regression (PCR) is selecting the optimal number of components, $k$. Cross-validation provides a robust, data-driven method for this, but a naive implementation can be computationally prohibitive. This exercise [@problem_id:1951651] challenges you to derive an efficient analytical formula for Leave-One-Out Cross-Validation, revealing the elegant mathematical structure of the PCR estimator and its relationship to the \"hat matrix\".", "problem": "A materials engineering team is investigating the properties of a new type of high-entropy alloy. Their goal is to build a predictive model for the alloy's fracture toughness, denoted by $y$, based on a set of $p$ quantitative predictor variables. These predictors, represented by a vector $\\mathbf{x} \\in \\mathbb{R}^p$, include the concentrations of constituent elements and various thermomechanical processing parameters.\n\nThe team has collected a dataset of $n$ alloy samples. Let the $n \\times p$ matrix $\\mathbf{X}$ contain the predictor values for all samples and the $n \\times 1$ vector $\\mathbf{y}$ contain the corresponding fracture toughness measurements. For simplicity, assume that the columns of $\\mathbf{X}$ and the vector $\\mathbf{y}$ have been centered to have a mean of zero.\n\nDue to the nature of alloy design, many of the predictors are expected to be highly correlated. To address this multicollinearity, the team decides to use Principal Component Regression (PCR), a two-stage procedure. First, Principal Component Analysis (PCA) is performed on the predictor matrix $\\mathbf{X}$ to obtain a new set of orthogonal variables called principal components. Second, the response $\\mathbf{y}$ is regressed on a subset of these principal components.\n\nThe PCR model using the first $k$ principal components is defined as follows:\n1. The principal component scores are computed as $\\mathbf{Z}_k = \\mathbf{X}\\mathbf{V}_k$, where $\\mathbf{V}_k$ is the $p \\times k$ matrix whose columns are the first $k$ principal component loading vectors (i.e., the eigenvectors of the sample covariance matrix of $\\mathbf{X}$).\n2. A linear regression of $\\mathbf{y}$ on $\\mathbf{Z}_k$ is performed to get the fitted values $\\hat{\\mathbf{y}}^{(k)}$.\n\nA critical step in PCR is selecting the optimal number of components, $k$. A common method is to choose the $k$ that minimizes the prediction error estimated via cross-validation. The team plans to use Leave-One-Out Cross-Validation (LOOCV). The performance metric is the Predicted Residual Sum of Squares (PRESS), defined as $\\text{PRESS}(k) = \\sum_{i=1}^{n} (y_i - \\hat{y}_{(-i)}^{(k)})^2$, where $\\hat{y}_{(-i)}^{(k)}$ is the prediction for the $i$-th observation from a PCR model with $k$ components that was trained on all data except observation $i$.\n\nWhile this procedure is well-defined, re-fitting the PCR model $n$ times (once for each left-out observation) is computationally inefficient. Your task is to derive an efficient, analytical expression for $\\text{PRESS}(k)$ that avoids this repeated fitting.\n\nExpress the $\\text{PRESS}(k)$ statistic in a form that can be calculated using only a single PCR model fit on the full dataset of $n$ observations. Your final expression should involve the ordinary residuals, $e_i^{(k)} = y_i - \\hat{y}_i^{(k)}$, and the diagonal elements, $h_{ii}^{(k)}$, of the appropriate projection matrix (or \"hat matrix\") associated with the PCR model.", "solution": "The goal is to find a computationally efficient formula for the Predicted Residual Sum of Squares (PRESS) statistic for a Principal Component Regression (PCR) model with $k$ components.\n\nFirst, we recall a general result for any linear model of the form $\\hat{\\mathbf{y}} = \\mathbf{H}\\mathbf{y}$, where $\\mathbf{H}$ is the projection or \"hat\" matrix. The prediction for the $i$-th observation from a model fitted without that observation, $\\hat{y}_{(-i)}$, can be related to the results from the model fitted on all data. The Leave-One-Out Cross-Validation (LOOCV) residual for observation $i$ is given by the well-known formula:\n$$ y_i - \\hat{y}_{(-i)} = \\frac{y_i - \\hat{y}_i}{1 - h_{ii}} $$\nwhere $\\hat{y}_i$ is the fitted value for observation $i$ from the full model, $e_i = y_i - \\hat{y}_i$ is the ordinary residual, and $h_{ii}$ is the $i$-th diagonal element of the hat matrix $\\mathbf{H}$.\n\nThe PRESS statistic is the sum of the squares of these LOOCV residuals:\n$$ \\text{PRESS} = \\sum_{i=1}^{n} (y_i - \\hat{y}_{(-i)})^2 = \\sum_{i=1}^{n} \\left( \\frac{y_i - \\hat{y}_i}{1 - h_{ii}} \\right)^2 = \\sum_{i=1}^{n} \\left( \\frac{e_i}{1-h_{ii}} \\right)^2 $$\nTo apply this to our PCR problem, we need to identify the specific hat matrix $\\mathbf{H}^{(k)}$ for a PCR model with $k$ components and find its diagonal elements $h_{ii}^{(k)}$.\n\nIn PCR, the model is built using the principal component scores, $\\mathbf{Z}_k = \\mathbf{X}\\mathbf{V}_k$, as the predictors. The linear regression of the centered response $\\mathbf{y}$ on $\\mathbf{Z}_k$ yields the estimated coefficients:\n$$ \\hat{\\boldsymbol{\\theta}}_k = (\\mathbf{Z}_k^T \\mathbf{Z}_k)^{-1} \\mathbf{Z}_k^T \\mathbf{y} $$\nThe fitted values $\\hat{\\mathbf{y}}^{(k)}$ are then given by:\n$$ \\hat{\\mathbf{y}}^{(k)} = \\mathbf{Z}_k \\hat{\\boldsymbol{\\theta}}_k = \\mathbf{Z}_k (\\mathbf{Z}_k^T \\mathbf{Z}_k)^{-1} \\mathbf{Z}_k^T \\mathbf{y} $$\nBy comparing this to the general form $\\hat{\\mathbf{y}}^{(k)} = \\mathbf{H}^{(k)} \\mathbf{y}$, we can identify the hat matrix for PCR with $k$ components as:\n$$ \\mathbf{H}^{(k)} = \\mathbf{Z}_k (\\mathbf{Z}_k^T \\mathbf{Z}_k)^{-1} \\mathbf{Z}_k^T $$\nThe columns of the scores matrix $\\mathbf{Z}_k$ are the score vectors $\\mathbf{z}_1, \\mathbf{z}_2, \\dots, \\mathbf{z}_k$. A key property of principal components is that these score vectors are orthogonal to each other: $\\mathbf{z}_j^T \\mathbf{z}_m = 0$ for $j \\neq m$. This means the matrix $\\mathbf{Z}_k^T \\mathbf{Z}_k$ is a $k \\times k$ diagonal matrix:\n$$ \\mathbf{Z}_k^T \\mathbf{Z}_k = \\text{diag}(\\mathbf{z}_1^T\\mathbf{z}_1, \\mathbf{z}_2^T\\mathbf{z}_2, \\dots, \\mathbf{z}_k^T\\mathbf{z}_k) = \\text{diag}(\\|\\mathbf{z}_1\\|^2, \\|\\mathbf{z}_2\\|^2, \\dots, \\|\\mathbf{z}_k\\|^2) $$\nwhere $\\|\\mathbf{z}_j\\|^2 = \\sum_{i=1}^{n} z_{ij}^2$ is the squared Euclidean norm of the $j$-th score vector.\n\nThe inverse matrix is also diagonal:\n$$ (\\mathbf{Z}_k^T \\mathbf{Z}_k)^{-1} = \\text{diag}(\\|\\mathbf{z}_1\\|^{-2}, \\|\\mathbf{z}_2\\|^{-2}, \\dots, \\|\\mathbf{z}_k\\|^{-2}) $$\nSubstituting this back into the expression for $\\mathbf{H}^{(k)}$, we see that the hat matrix is a sum of rank-one projection matrices:\n$$ \\mathbf{H}^{(k)} = \\begin{pmatrix} \\mathbf{z}_1 & \\dots & \\mathbf{z}_k \\end{pmatrix} \\begin{pmatrix} \\|\\mathbf{z}_1\\|^{-2} & & \\\\ & \\ddots & \\\\ & & \\|\\mathbf{z}_k\\|^{-2} \\end{pmatrix} \\begin{pmatrix} \\mathbf{z}_1^T \\\\ \\vdots \\\\ \\mathbf{z}_k^T \\end{pmatrix} = \\sum_{j=1}^{k} \\frac{\\mathbf{z}_j \\mathbf{z}_j^T}{\\|\\mathbf{z}_j\\|^2} $$\nWe are interested in the diagonal elements of this matrix, $h_{ii}^{(k)}$. The $(i,i)$-th element of the outer product matrix $\\mathbf{z}_j \\mathbf{z}_j^T$ is simply $z_{ij}^2$, where $z_{ij}$ is the $i$-th element of the vector $\\mathbf{z}_j$. Therefore, the diagonal elements of $\\mathbf{H}^{(k)}$ are:\n$$ h_{ii}^{(k)} = \\left( \\sum_{j=1}^{k} \\frac{\\mathbf{z}_j \\mathbf{z}_j^T}{\\|\\mathbf{z}_j\\|^2} \\right)_{ii} = \\sum_{j=1}^{k} \\frac{(\\mathbf{z}_j \\mathbf{z}_j^T)_{ii}}{\\|\\mathbf{z}_j\\|^2} = \\sum_{j=1}^{k} \\frac{z_{ij}^2}{\\sum_{l=1}^{n} z_{lj}^2} $$\nNow we have all the pieces. The PRESS statistic for a $k$-component PCR model can be computed by first fitting the model to the full dataset to obtain the ordinary residuals $e_i^{(k)} = y_i - \\hat{y}_i^{(k)}$ and the principal component scores $z_{ij}$. Then, these values are combined using the derived formula.\n\nThe final expression for $\\text{PRESS}(k)$ is:\n$$ \\text{PRESS}(k) = \\sum_{i=1}^{n} \\left( \\frac{y_i - \\hat{y}_i^{(k)}}{1 - h_{ii}^{(k)}} \\right)^2 = \\sum_{i=1}^{n} \\left( \\frac{e_i^{(k)}}{1-h_{ii}^{(k)}} \\right)^2 $$\nThis formula allows for the efficient calculation of the PRESS statistic for any $k$ after performing the PCA and regression just once on the full dataset.", "answer": "$$\\boxed{\\sum_{i=1}^{n} \\left( \\frac{e_i^{(k)}}{1-h_{ii}^{(k)}} \\right)^{2}}$$", "id": "1951651"}, {"introduction": "While cross-validation offers a supervised method for choosing $k$, a common unsupervised heuristic is to select components that explain a high percentage of variance in the predictors. This exercise [@problem_id:3160814] confronts this practice by exploring the critical distinction between a component's variance and its predictive relevance. Through carefully designed simulations, you will discover why a component that explains a large amount of predictor variation may have little to do with the response, highlighting why supervised model selection is often essential.", "problem": "You are given the task of implementing Principal Component Regression (PCR) and comparing two criteria for selecting the number of components $k$: selecting $k$ by a target cumulative variance explained threshold (e.g., $\\tau = 0.95$) versus selecting $k$ by minimizing $K$-fold Cross-Validation (CV) Mean Squared Error (MSE). Your implementation must be grounded in core definitions and well-tested formulas: Principal Component Analysis (PCA), Singular Value Decomposition (SVD), linear regression by least squares, and $K$-fold Cross-Validation.\n\nFundamental base:\n- Principal Component Analysis (PCA) transforms a centered data matrix $X \\in \\mathbb{R}^{n \\times p}$ into orthogonal principal components ordered by variance. If $X$ has thin Singular Value Decomposition (SVD) $X = U S V^\\top$, then the principal axes are the columns of $V$, the singular values are the diagonal entries of $S$, and the sample covariance eigenvalues are proportional to $S^2$. The explained variance ratio of the first $k$ components is the cumulative sum of the leading $k$ entries of $S^2$ divided by the sum of all entries of $S^2$.\n- Principal Component Regression (PCR) fits a linear model of a response $y \\in \\mathbb{R}^n$ using the scores of the first $k$ principal components. To include an intercept, center $X$ and $y$, regress $y$ on the first $k$ principal component score columns by least squares, and then add back the training mean of $y$ to predictions.\n- $K$-fold Cross-Validation (CV) partitions indices $\\{1,\\dots,n\\}$ into $K$ folds, trains on $K-1$ folds and validates on the held-out fold, and averages the validation Mean Squared Error (MSE) across folds. The MSE of predictions $\\hat{y}$ against $y$ is $\\frac{1}{m} \\sum_{i=1}^m (y_i - \\hat{y}_i)^2$ on a validation set of size $m$.\n\nDefinitions and tasks:\n1. Implement PCR as follows:\n   - Center $X$ by subtracting the column means and center $y$ by subtracting its mean on the training set.\n   - Compute the thin SVD $X_{\\text{train}} = U S V^\\top$ on the centered training predictors.\n   - Form principal component scores $Z_{\\text{train}} = X_{\\text{train}} V_{[:,1:k]}$ using the first $k$ columns of $V$.\n   - Fit the coefficients on $Z_{\\text{train}}$ by least squares. Predict on a test set by centering the test $X$ with the training means, projecting onto the same $V_{[:,1:k]}$, and adding back the training mean of $y$ to the centered predictions.\n2. Select $k$ by variance explained: compute the singular values $S$ of the centered full $X$, form $S^2$, and choose the smallest $k$ such that the cumulative ratio of the first $k$ entries of $S^2$ to the sum of all entries of $S^2$ is at least $\\tau$.\n3. Select $k$ by CV MSE: for each $k \\in \\{1,2,\\dots,p\\}$, compute the average $K$-fold CV MSE of PCR with $k$ components, and choose the $k$ that minimizes it. In case of ties, select the smallest $k$.\n4. For each scenario, report the tuple $[k_{\\text{var}}, k_{\\text{cv}}, \\text{MSE}(k_{\\text{var}}), \\text{MSE}(k_{\\text{cv}})]$, where $k_{\\text{var}}$ is the selection by variance explained and $k_{\\text{cv}}$ is the selection by CV MSE. For consistency, round the MSE values to $4$ decimal places.\n\nData generation model:\n- Let $\\boldsymbol{\\lambda} \\in \\mathbb{R}^p$ be a vector of target eigenvalues. Generate samples $x_i \\in \\mathbb{R}^p$ as $x_i = g_i \\operatorname{diag}(\\sqrt{\\boldsymbol{\\lambda}})$, where $g_i \\sim \\mathcal{N}(\\mathbf{0}, I_p)$ independently. This yields data with covariance $\\operatorname{diag}(\\boldsymbol{\\lambda})$ and principal axes equal to the coordinate axes (identity rotation).\n- Define a coefficient vector $\\beta \\in \\mathbb{R}^p$ in the principal component basis by a vector $\\beta_{\\text{rot}} \\in \\mathbb{R}^p$ specifying the contribution of each principal component to the response. Use $\\beta = \\beta_{\\text{rot}}$ directly because the rotation is the identity.\n- Generate the response as $y_i = x_i^\\top \\beta + \\epsilon_i$ with $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma_\\epsilon^2)$ independently.\n\nTest suite:\nImplement the above with $K$-fold Cross-Validation using $K = 5$. Use the identity rotation for principal axes and the following four scenarios. For each scenario, set the random seed for data generation and fold assignment to the specified seed.\n\n- Scenario $1$ (signal aligned with top-variance component):\n  - $n = 200$, $p = 8$\n  - $\\boldsymbol{\\lambda} = (9, 3, 1, 0.5, 0.3, 0.2, 0.1, 0.05)$\n  - $\\beta_{\\text{rot}} = (1, 0, 0, 0, 0, 0, 0, 0)$\n  - $\\sigma_\\epsilon = 0.2$\n  - $\\tau = 0.95$\n  - seed $= 42$\n- Scenario $2$ (signal aligned with lowest-variance component):\n  - $n = 200$, $p = 8$\n  - $\\boldsymbol{\\lambda} = (9, 3, 1, 0.5, 0.3, 0.2, 0.1, 0.05)$\n  - $\\beta_{\\text{rot}} = (0, 0, 0, 0, 0, 0, 0, 1)$\n  - $\\sigma_\\epsilon = 0.2$\n  - $\\tau = 0.95$\n  - seed $= 7$\n- Scenario $3$ (flat variance spectrum, high noise, multi-component signal):\n  - $n = 300$, $p = 12$\n  - $\\boldsymbol{\\lambda} = (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)$\n  - $\\beta_{\\text{rot}} = (1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0)$\n  - $\\sigma_\\epsilon = 1.5$\n  - $\\tau = 0.95$\n  - seed $= 123$\n- Scenario $4$ (threshold at one, low noise, signal on first two components):\n  - $n = 120$, $p = 10$\n  - $\\boldsymbol{\\lambda} = (5, 4, 3, 2, 1.5, 1, 0.8, 0.6, 0.4, 0.2)$\n  - $\\beta_{\\text{rot}} = (1, 1, 0, 0, 0, 0, 0, 0, 0, 0)$\n  - $\\sigma_\\epsilon = 0.1$\n  - $\\tau = 1.0$\n  - seed $= 99$\n\nAngle units are not involved. No physical units are involved. The final outputs are numerical and must be reported as lists of integers and floats as follows.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each scenario contributes one list $[k_{\\text{var}}, k_{\\text{cv}}, \\text{MSE}(k_{\\text{var}}), \\text{MSE}(k_{\\text{cv}})]$, where the MSE entries are rounded to $4$ decimal places. For example, the output should look like $[[1,4,0.1234,0.0567],[\\dots],\\dots]$.", "solution": "The problem requires the implementation of Principal Component Regression (PCR) and a comparison of two distinct methods for selecting the optimal number of principal components, $k$. The first method selects $k$ based on a cumulative explained variance threshold, $\\tau$. The second method selects $k$ by minimizing the Mean Squared Error (MSE) estimated via $K$-fold Cross-Validation (CV). The comparison is performed across four simulated scenarios with specified data generation parameters.\n\n### 1. Data Generation Model\n\nThe data are generated from a linear model. For a specified number of samples $n$ and predictors $p$, the predictor matrix $X \\in \\mathbb{R}^{n \\times p}$ is constructed such that its population covariance matrix is diagonal with entries given by the vector $\\boldsymbol{\\lambda} \\in \\mathbb{R}^p$. Specifically, each row $x_i^\\top$ of $X$ is generated as:\n$$ x_i = G_i \\operatorname{diag}(\\sqrt{\\boldsymbol{\\lambda}}) $$\nwhere $G_i$ is a row vector of $p$ independent samples from the standard normal distribution, $\\mathcal{N}(0, 1)$. This is equivalent to generating a matrix $G \\in \\mathbb{R}^{n \\times p}$ with i.i.d. $\\mathcal{N}(0, 1)$ entries and setting $X = G \\operatorname{diag}(\\sqrt{\\boldsymbol{\\lambda}})$. The population principal axes of this data distribution are the standard basis vectors (an identity rotation).\n\nThe response vector $y \\in \\mathbb{R}^n$ is generated according to the linear model:\n$$ y_i = x_i^\\top \\beta + \\epsilon_i $$\nwhere $\\beta \\in \\mathbb{R}^p$ is the vector of true regression coefficients and $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma_\\epsilon^2)$ are i.i.d. error terms. The coefficient vector $\\beta$ is given directly as $\\beta = \\beta_{\\text{rot}}$, where $\\beta_{\\text{rot}}$ specifies the signal strength along each principal component axis.\n\n### 2. Principal Component Regression (PCR) Algorithm\n\nPCR is a two-stage procedure that first reduces the dimensionality of the predictors using Principal Component Analysis (PCA) and then performs linear regression on the resulting components. The procedure, as specified for a training set $(X_{\\text{train}}, y_{\\text{train}})$ and a test set $X_{\\text{test}}$, is as follows:\n\n1.  **Data Centering**: The training data are centered to have zero mean. The test data are centered using the means from the training data to prevent information leakage.\n    $$ \\bar{x}_{\\text{train}} = \\frac{1}{n_{\\text{train}}} \\sum_{i=1}^{n_{\\text{train}}} x_{\\text{train},i}, \\quad \\bar{y}_{\\text{train}} = \\frac{1}{n_{\\text{train}}} \\sum_{i=1}^{n_{\\text{train}}} y_{\\text{train},i} $$\n    $$ X_{\\text{c,train}} = X_{\\text{train}} - \\mathbf{1}\\bar{x}_{\\text{train}}^\\top, \\quad y_{\\text{c,train}} = y_{\\text{train}} - \\bar{y}_{\\text{train}} $$\n    $$ X_{\\text{c,test}} = X_{\\text{test}} - \\mathbf{1}\\bar{x}_{\\text{train}}^\\top $$\n\n2.  **Principal Component Extraction**: The principal axes are found by computing the Singular Value Decomposition (SVD) of the centered training predictor matrix:\n    $$ X_{\\text{c,train}} = U S V^\\top $$\n    The columns of $V \\in \\mathbb{R}^{p \\times p}$ are the principal axes (or loading vectors).\n\n3.  **Score Projection**: The training data are projected onto the first $k$ principal axes to obtain the score matrix $Z_{\\text{train},k} \\in \\mathbb{R}^{n_{\\text{train}} \\times k}$:\n    $$ Z_{\\text{train},k} = X_{\\text{c,train}} V_k $$\n    where $V_k \\in \\mathbb{R}^{p \\times k}$ contains the first $k$ columns of $V$.\n\n4.  **Regression on Scores**: A linear model is fit using ordinary least squares to regress the centered response $y_{\\text{c,train}}$ onto the scores $Z_{\\text{train},k}$, yielding coefficients $\\hat{\\theta}_k \\in \\mathbb{R}^k$:\n    $$ \\hat{\\theta}_k = (Z_{\\text{train},k}^\\top Z_{\\text{train},k})^{-1} Z_{\\text{train},k}^\\top y_{\\text{c,train}} $$\n\n5.  **Prediction**: Predictions for the test set are made by first projecting the centered test data $X_{\\text{c,test}}$ onto the same $k$ principal axes to get test scores $Z_{\\text{test},k}$, then applying the fitted coefficients $\\hat{\\theta}_k$, and finally adding back the mean of the training response:\n    $$ Z_{\\text{test},k} = X_{\\text{c,test}} V_k $$\n    $$ \\hat{y}_{\\text{test}} = Z_{\\text{test},k} \\hat{\\theta}_k + \\bar{y}_{\\text{train}} $$\n\n### 3. Component Selection I: Cumulative Variance Explained\n\nThis method selects the number of components $k$ based on the proportion of variance in the predictor variables that is captured. It is applied to the full dataset $X$.\n\n1.  Center the full data matrix: $X_c = X - \\mathbf{1}\\bar{x}^\\top$.\n2.  Compute its SVD: $X_c = U S V^\\top$. The diagonal entries of $S$ are the singular values, $s_1 \\ge s_2 \\ge \\dots \\ge s_p \\ge 0$.\n3.  The variance explained by the $j$-th principal component is proportional to $s_j^2$. The cumulative explained variance ratio (CEVR) for the first $k$ components is:\n    $$ \\text{CEVR}(k) = \\frac{\\sum_{j=1}^k s_j^2}{\\sum_{j=1}^p s_j^2} $$\n4.  Given a threshold $\\tau$, the number of components $k_{\\text{var}}$ is chosen as the minimum $k$ that satisfies the threshold:\n    $$ k_{\\text{var}} = \\min \\{ k \\in \\{1, 2, \\dots, p\\} \\mid \\text{CEVR}(k) \\ge \\tau \\} $$\n\n### 4. Component Selection II: $K$-Fold Cross-Validation\n\nThis method selects $k$ by directly estimating the prediction error of the PCR model for each possible value of $k$.\n\n1.  The dataset of $n$ samples is partitioned into $K$ disjoint folds of approximately equal size. For this problem, $K=5$.\n2.  For each candidate number of components $k \\in \\{1, 2, \\dots, p\\}$:\n    a. An outer loop iterates through each fold $j=1, \\dots, K$.\n    b. In each iteration, the $j$-th fold is designated as the validation set $(X_{\\text{val}}, y_{\\text{val}})$, and the remaining $K-1$ folds constitute the training set $(X_{\\text{train}}, y_{\\text{train}})$.\n    c. A PCR model with $k$ components is trained on $(X_{\\text{train}}, y_{\\text{train}})$.\n    d. The trained model is used to make predictions $\\hat{y}_{\\text{val}}$ on the validation set $X_{\\text{val}}$.\n    e. The Mean Squared Error (MSE) is computed for that fold: $\\text{MSE}_j(k) = \\frac{1}{n_{\\text{val}}} \\sum_{i \\in \\text{fold }j} (y_{\\text{val},i} - \\hat{y}_{\\text{val},i})^2$.\n3.  After iterating through all folds, the cross-validation MSE for $k$ components is the average of the fold-specific MSEs:\n    $$ \\text{CV-MSE}(k) = \\frac{1}{K} \\sum_{j=1}^K \\text{MSE}_j(k) $$\n4.  The optimal number of components $k_{\\text{cv}}$ is the one that minimizes this average error. Ties are broken by choosing the smallest $k$.\n    $$ k_{\\text{cv}} = \\operatorname{argmin}_{k \\in \\{1, \\dots, p\\}} \\text{CV-MSE}(k) $$\n\n### 5. Implementation and Evaluation\n\nFor each scenario, we first generate the data $(X, y)$. We then apply both selection criteria. The variance-based method yields $k_{\\text{var}}$. The cross-validation procedure is run for all $k \\in \\{1, \\dots, p\\}$, producing a list of CV-MSE values. From this list, we identify $k_{\\text{cv}}$ as the minimizer. The final reported values are $k_{\\text{var}}$, $k_{\\text{cv}}$, the pre-computed CV-MSE for $k_{\\text{var}}$ (i.e., $\\text{CV-MSE}(k_{\\text{var}})$), and the minimum CV-MSE (i.e., $\\text{CV-MSE}(k_{\\text{cv}})$). This provides a direct comparison of the predictive performance of the two selected models. The random seed is used to ensure reproducibility of both the data generation and the fold assignments in cross-validation.", "answer": "[[3,1,0.0631,0.0583],[3,8,4.1030,0.0526],[12,3,2.3780,2.3023],[10,2,0.0210,0.0160]]", "id": "3160814"}, {"introduction": "After selecting an optimal model, a crucial next step is to quantify the uncertainty of its predictions. This advanced problem [@problem_id:3160797] delves into the statistical properties of the PCR estimator by tasking you with constructing confidence intervals for its predicted responses. By deriving these intervals and comparing their size to those from Ridge Regression—another regularization method with matched complexity—you will gain a deeper, more quantitative understanding of the bias-variance tradeoff and how PCR's specific form of shrinkage impacts estimator variance.", "problem": "Consider the fixed-design Gaussian linear model $y \\sim \\mathcal{N}(X\\beta,\\sigma^{2} I_{n})$ with known noise variance $\\sigma^{2}$ and $n=3$, $p=2$. Let the design matrix be\n$$\nX \\;=\\; \\begin{pmatrix}\n3 & 0 \\\\\n0 & 1 \\\\\n0 & 0\n\\end{pmatrix}.\n$$\nLet $X$ admit the thin Singular Value Decomposition (SVD) $X = U D V^{\\top}$ with $U \\in \\mathbb{R}^{3 \\times 2}$, $D=\\operatorname{diag}(d_{1},d_{2}) \\in \\mathbb{R}^{2 \\times 2}$, and $V \\in \\mathbb{R}^{2 \\times 2}$. For this $X$, you may take as given that $d_{1}=3$, $d_{2}=1$, $V = I_{2}$, and $U = \\big[e_{1},e_{2}\\big]$, where $e_{1}$ and $e_{2}$ are the first two standard basis vectors in $\\mathbb{R}^{3}$.\n\nPrincipal Component Regression (PCR) with $k$ components is defined as the least squares regression of $y$ on the first $k$ principal component score vectors, which yields fitted values at the training points $\\hat{y} = H_{k} y$, where $H_{k}$ is the orthogonal projector onto $\\operatorname{span}\\{u_{1},\\dots,u_{k}\\}$, with $u_{j}$ the $j$-th column of $U$. Ridge regression with regularization parameter $\\lambda > 0$ yields fitted values $\\hat{y} = S_{\\lambda} y$, where $S_{\\lambda} = X (X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top}$ and its effective degrees of freedom is $\\operatorname{df}(\\lambda) = \\operatorname{tr}(S_{\\lambda})$.\n\nUsing only the linearity of these estimators and the distributional properties of multivariate normal vectors (no other shortcuts), perform the following:\n\n- Derive the $95\\%$ confidence interval, under known $\\sigma^{2}$ and normal theory, for the predicted training response $\\hat{y}_{1}$ (the first component of $\\hat{y}$) under PCR with $k=1$. Identify its half-width in terms of $H_{1}$ and $\\sigma^{2}$.\n- Derive the corresponding $95\\%$ confidence interval for $\\hat{y}_{1}$ under ridge regression with parameter $\\lambda$, and identify its half-width in terms of $S_{\\lambda}$ and $\\sigma^{2}$.\n- Choose $\\lambda$ so that ridge has matched degrees of freedom with PCR, that is, $\\operatorname{df}(\\lambda)=1$. For this matched $\\lambda$, compute the exact ratio\n$$\nR \\;=\\; \\frac{\\text{half-width of the PCR$(k\\!=\\!1)$ interval for }\\hat{y}_{1}}{\\text{half-width of the ridge interval for }\\hat{y}_{1}}.\n$$\n\nExpress your final answer for $R$ as a single exact value. No rounding is required. There are no physical units in this problem.", "solution": "First, we validate the problem statement.\n**Step 1: Extract Givens**\n- Model: Fixed-design Gaussian linear model $y \\sim \\mathcal{N}(X\\beta,\\sigma^{2} I_{n})$.\n- Known variance $\\sigma^{2}$.\n- Dimensions: $n=3$, $p=2$.\n- Design matrix: $X = \\begin{pmatrix} 3 & 0 \\\\ 0 & 1 \\\\ 0 & 0 \\end{pmatrix}$.\n- Thin SVD of $X$: $X = U D V^{\\top}$, with $U \\in \\mathbb{R}^{3 \\times 2}$, $D=\\operatorname{diag}(d_{1},d_{2}) \\in \\mathbb{R}^{2 \\times 2}$, and $V \\in \\mathbb{R}^{2 \\times 2}$.\n- Given SVD components: $d_{1}=3$, $d_{2}=1$, $V = I_{2}$, and $U = \\big[e_{1},e_{2}\\big]$, where $e_{1}$ and $e_{2}$ are the first two standard basis vectors in $\\mathbb{R}^{3}$.\n- Principal Component Regression (PCR) with $k$ components definition: fitted values $\\hat{y} = H_{k} y$, where $H_{k}$ is the orthogonal projector onto $\\operatorname{span}\\{u_{1},\\dots,u_{k}\\}$.\n- Ridge regression definition: fitted values $\\hat{y} = S_{\\lambda} y$, where $S_{\\lambda} = X (X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top}$ with $\\lambda > 0$.\n- Effective degrees of freedom for ridge: $\\operatorname{df}(\\lambda) = \\operatorname{tr}(S_{\\lambda})$.\n- Tasks:\n  1. Derive the $95\\%$ confidence interval for $\\hat{y}_{1}$ under PCR with $k=1$ and find its half-width.\n  2. Derive the $95\\%$ confidence interval for $\\hat{y}_{1}$ under ridge regression and find its half-width.\n  3. Set $\\operatorname{df}(\\lambda)=1$, find the corresponding $\\lambda$, and compute the ratio $R$ of the PCR half-width to the ridge half-width.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is firmly rooted in the theory of linear models and statistical learning, specifically comparing two standard regularization/dimensionality reduction techniques (PCR and ridge regression). All concepts are well-established.\n- **Well-Posed:** The problem provides all necessary data and definitions to perform the required derivations and calculations. The objectives are clear and lead to a unique numerical answer.\n- **Objective:** The language is formal, precise, and free of any subjectivity.\n- **Completeness and Consistency:** The givens are self-consistent. The provided SVD components are correct for the given matrix $X$, as $X^\\top X = \\begin{pmatrix} 9 & 0 \\\\ 0 & 1 \\end{pmatrix}$, which has eigenvalues $d_1^2=9$ and $d_2^2=1$ and eigenvectors $v_1=e_1, v_2=e_2$ (so $V=I_2$). The left singular vectors are $u_1 = \\frac{1}{3}Xv_1 = e_1$ and $u_2=\\frac{1}{1}Xv_2=e_2$. All definitions are standard.\n\n**Step 3: Verdict and Action**\nThe problem is valid. We may proceed with the solution.\n\nA confidence interval for an estimator is typically constructed for its expected value. We will derive the $95\\%$ confidence interval for $E[\\hat{y}_1]$ for both PCR and ridge regression. A general $100(1-\\alpha)\\%$ confidence interval for the mean $E[\\hat{\\theta}]$ of a normally distributed estimator $\\hat{\\theta}$ with known variance is given by $\\hat{\\theta} \\pm z_{1-\\alpha/2} \\sqrt{\\operatorname{Var}(\\hat{\\theta})}$. For a $95\\%$ confidence level, $\\alpha=0.05$ and $z_{0.975}$ is the $0.975$ quantile of the standard normal distribution. The half-width of this interval is $z_{0.975} \\sqrt{\\operatorname{Var}(\\hat{\\theta})}$.\n\nThe response vector $y \\in \\mathbb{R}^3$ follows the distribution $y \\sim \\mathcal{N}(X\\beta, \\sigma^2 I_3)$. For any matrix $A \\in \\mathbb{R}^{m \\times 3}$, the transformed vector $Ay$ is also normally distributed: $Ay \\sim \\mathcal{N}(A(X\\beta), A(\\sigma^2 I_3)A^\\top) = \\mathcal{N}(AX\\beta, \\sigma^2 AA^\\top)$. The first component of a predicted vector $\\hat{y}$ is $\\hat{y}_1 = e_1^\\top \\hat{y}$, where $e_1 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$.\n\n**1. PCR with $k=1$**\nFor PCR with $k=1$, the fitted vector is $\\hat{y}_{PCR} = H_1 y$, where $H_1$ is the orthogonal projector onto the span of the first principal component vector, $u_1$.\nGiven $U = [e_1, e_2]$, we have $u_1 = e_1$. The projection matrix is:\n$$H_1 = u_1 u_1^\\top = e_1 e_1^\\top = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix}$$\nThe first component of the fitted vector is $\\hat{y}_{1,PCR} = e_1^\\top \\hat{y}_{PCR} = e_1^\\top H_1 y$.\nThe estimator $\\hat{y}_{1,PCR}$ is a linear transformation of $y$, so it is normally distributed. Its variance is:\n$\\operatorname{Var}(\\hat{y}_{1,PCR}) = \\operatorname{Var}(e_1^\\top H_1 y) = (e_1^\\top H_1) \\operatorname{Var}(y) (e_1^\\top H_1)^\\top = (e_1^\\top H_1) (\\sigma^2 I_3) (H_1^\\top e_1) = \\sigma^2 e_1^\\top H_1 H_1^\\top e_1$.\nSince $H_1$ is a projection matrix, it is symmetric ($H_1^\\top=H_1$) and idempotent ($H_1^2=H_1$).\n$\\operatorname{Var}(\\hat{y}_{1,PCR}) = \\sigma^2 e_1^\\top H_1^2 e_1 = \\sigma^2 e_1^\\top H_1 e_1 = \\sigma^2 (H_1)_{11}$.\nFrom the matrix $H_1$ above, its $(1,1)$ entry is $(H_1)_{11}=1$.\nSo, $\\operatorname{Var}(\\hat{y}_{1,PCR}) = \\sigma^2$.\nThe half-width of the $95\\%$ confidence interval for $E[\\hat{y}_{1,PCR}]$ is:\n$$ W_{PCR} = z_{0.975} \\sqrt{\\operatorname{Var}(\\hat{y}_{1,PCR})} = z_{0.975} \\sigma \\sqrt{(H_1)_{11}} = z_{0.975} \\sigma $$\n\n**2. Ridge Regression**\nFor ridge regression, the fitted vector is $\\hat{y}_{Ridge} = S_\\lambda y$, with $S_\\lambda = X(X^\\top X + \\lambda I_p)^{-1}X^\\top$.\nThe first component is $\\hat{y}_{1,Ridge} = e_1^\\top S_\\lambda y$. This is a linear transformation of $y$, so it is also normally distributed.\nIts variance is:\n$\\operatorname{Var}(\\hat{y}_{1,Ridge}) = \\operatorname{Var}(e_1^\\top S_\\lambda y) = (e_1^\\top S_\\lambda) (\\sigma^2 I_3) (S_\\lambda^\\top e_1) = \\sigma^2 e_1^\\top S_\\lambda S_\\lambda^\\top e_1$.\nThe matrix $S_\\lambda$ is symmetric, so $S_\\lambda^\\top = S_\\lambda$.\n$\\operatorname{Var}(\\hat{y}_{1,Ridge}) = \\sigma^2 e_1^\\top S_\\lambda^2 e_1 = \\sigma^2 (S_\\lambda^2)_{11}$.\nThe half-width of the $95\\%$ confidence interval for $E[\\hat{y}_{1,Ridge}]$ is:\n$$ W_{Ridge} = z_{0.975} \\sqrt{\\operatorname{Var}(\\hat{y}_{1,Ridge})} = z_{0.975} \\sigma \\sqrt{(S_\\lambda^2)_{11}} $$\n\n**3. Matching Degrees of Freedom and Computing the Ratio**\nThe degrees of freedom for PCR with $k=1$ is $\\operatorname{tr}(H_1) = \\operatorname{tr}(u_1 u_1^\\top) = \\operatorname{tr}(u_1^\\top u_1) = \\operatorname{tr}(1) = 1$.\nThe degrees of freedom for ridge regression is $\\operatorname{df}(\\lambda) = \\operatorname{tr}(S_\\lambda)$. Using the SVD of $X$:\n$$ S_\\lambda = U D (D^2+\\lambda I_p)^{-1} D U^\\top = U \\operatorname{diag}\\left(\\frac{d_j^2}{d_j^2+\\lambda}\\right) U^\\top $$\n$$ \\operatorname{df}(\\lambda) = \\operatorname{tr}(S_\\lambda) = \\operatorname{tr}\\left(U \\operatorname{diag}\\left(\\frac{d_j^2}{d_j^2+\\lambda}\\right) U^\\top\\right) = \\operatorname{tr}\\left(\\operatorname{diag}\\left(\\frac{d_j^2}{d_j^2+\\lambda}\\right) U^\\top U\\right) = \\sum_{j=1}^p \\frac{d_j^2}{d_j^2+\\lambda} $$\nGiven $d_1=3$, $d_2=1$, and $p=2$, we match the degrees of freedom to $1$:\n$$ \\frac{3^2}{3^2+\\lambda} + \\frac{1^2}{1^2+\\lambda} = 1 \\implies \\frac{9}{9+\\lambda} + \\frac{1}{1+\\lambda} = 1 $$\nMultiplying by $(9+\\lambda)(1+\\lambda)$:\n$$ 9(1+\\lambda) + (9+\\lambda) = (9+\\lambda)(1+\\lambda) $$\n$$ 9 + 9\\lambda + 9 + \\lambda = 9 + 10\\lambda + \\lambda^2 $$\n$$ 18 + 10\\lambda = 9 + 10\\lambda + \\lambda^2 $$\n$$ \\lambda^2 = 9 $$\nSince $\\lambda>0$, we select $\\lambda=3$.\n\nNow we compute the matrix $S_\\lambda$ for $\\lambda=3$:\n$$ S_3 = U \\operatorname{diag}\\left(\\frac{3^2}{3^2+3}, \\frac{1^2}{1^2+3}\\right) U^\\top = U \\operatorname{diag}\\left(\\frac{9}{12}, \\frac{1}{4}\\right) U^\\top = U \\begin{pmatrix} 3/4 & 0 \\\\ 0 & 1/4 \\end{pmatrix} U^\\top $$\nWith $U = [e_1, e_2] = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 0 & 0 \\end{pmatrix}$ and $U^\\top = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{pmatrix}$:\n$$ S_3 = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} 3/4 & 0 \\\\ 0 & 1/4 \\end{pmatrix} \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{pmatrix} = \\begin{pmatrix} 3/4 & 0 \\\\ 0 & 1/4 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{pmatrix} = \\begin{pmatrix} 3/4 & 0 & 0 \\\\ 0 & 1/4 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} $$\nNext, we compute $S_3^2$:\n$$ S_3^2 = \\begin{pmatrix} (3/4)^2 & 0 & 0 \\\\ 0 & (1/4)^2 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 9/16 & 0 & 0 \\\\ 0 & 1/16 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} $$\nThe $(1,1)$ entry is $(S_3^2)_{11} = \\frac{9}{16}$.\n\nFinally, we compute the ratio $R$:\n$$ R = \\frac{W_{PCR}}{W_{Ridge}} = \\frac{z_{0.975} \\sigma \\sqrt{(H_1)_{11}}}{z_{0.975} \\sigma \\sqrt{(S_3^2)_{11}}} = \\sqrt{\\frac{(H_1)_{11}}{(S_3^2)_{11}}} $$\nSubstituting the values we found: $(H_1)_{11}=1$ and $(S_3^2)_{11}=\\frac{9}{16}$.\n$$ R = \\sqrt{\\frac{1}{9/16}} = \\sqrt{\\frac{16}{9}} = \\frac{4}{3} $$", "answer": "$$\\boxed{\\frac{4}{3}}$$", "id": "3160797"}]}