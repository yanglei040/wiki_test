{"hands_on_practices": [{"introduction": "This exercise takes you to the heart of Ward's linkage, one of the most popular agglomerative methods. By deriving the increase in the total within-cluster sum of squares ($WSS$) from first principles, you will see exactly how this method quantifies the cost of a merge [@problem_id:3097665]. This fundamental practice builds a deep intuition for why Ward's method tends to produce compact, similarly-sized clusters.", "problem": "Consider agglomerative hierarchical clustering in Euclidean space on a dataset $\\mathcal{X} \\subset \\mathbb{R}^{p}$ using Ward linkage, which at each merge chooses the pair of clusters that yields the smallest increase in the total within-cluster sum of squares. Let $A$ and $B$ be two disjoint clusters with sizes $|A|=m$ and $|B|=n$, and centroids (means) $\\mu_{A}$ and $\\mu_{B}$ defined by $\\mu_{C}=\\frac{1}{|C|}\\sum_{x \\in C} x$ for any cluster $C$. The within-cluster sum of squares for a cluster $C$ is $W(C)=\\sum_{x \\in C}\\|x-\\mu_{C}\\|^{2}$, and the total within-cluster sum of squares is $W_{\\mathrm{tot}}=\\sum_{C} W(C)$ over all current clusters. Using only these definitions and standard linear algebra identities, first derive an exact expression for the centroid of the merged cluster $A \\cup B$ in terms of $m$, $n$, $\\mu_{A}$, and $\\mu_{B}$. Second, derive an exact expression for the increase $\\Delta W = W(A \\cup B) - W(A) - W(B)$ in the total within-cluster sum of squares due to merging $A$ and $B$, expressed only in terms of $m$, $n$, and the Euclidean distance $\\|\\mu_{A}-\\mu_{B}\\|$. Third, using your derived expression for $\\Delta W$, evaluate the numerical value of this increase for $m=7$, $n=3$, $\\mu_{A}=(1,2)$, and $\\mu_{B}=(5,8)$ in $\\mathbb{R}^{2}$, using the standard Euclidean norm. Finally, discuss qualitatively how the cost increase depends on the size ratio $r=m/n$ when the centroid distance $\\|\\mu_{A}-\\mu_{B}\\|$ is held fixed. Provide only the numerical value of the increase for the specified $m$, $n$, $\\mu_{A}$, and $\\mu_{B}$ as your final answer, rounded to $4$ significant figures.", "solution": "The problem statement is critically examined and found to be valid. It is scientifically grounded in the principles of statistical learning, specifically agglomerative hierarchical clustering. The definitions provided for cluster centroids, within-cluster sum of squares, and Ward linkage are standard and correct. The problem is well-posed, providing all necessary information for the requested derivations and calculations, and is free of ambiguity, contradictions, or factual errors.\n\nThe solution proceeds in four parts as requested by the problem statement.\n\nFirst, we derive the expression for the centroid of the merged cluster, $\\mu_{A \\cup B}$. Let the two disjoint clusters be $A$ and $B$, with sizes $|A|=m$ and $|B|=n$, and centroids $\\mu_A$ and $\\mu_B$, respectively. The merged cluster is $C = A \\cup B$, and its size is $|C| = |A| + |B| = m+n$.\n\nBy the definition of a centroid, $\\mu_{C} = \\frac{1}{|C|} \\sum_{x \\in C} x$. Applying this to the merged cluster $A \\cup B$:\n$$ \\mu_{A \\cup B} = \\frac{1}{|A \\cup B|} \\sum_{x \\in A \\cup B} x $$\nSince $A$ and $B$ are disjoint, the summation can be split:\n$$ \\mu_{A \\cup B} = \\frac{1}{m+n} \\left( \\sum_{x \\in A} x + \\sum_{x \\in B} x \\right) $$\nFrom the definition of the individual cluster centroids, we know that $\\sum_{x \\in A} x = |A|\\mu_A = m\\mu_A$ and $\\sum_{x \\in B} x = |B|\\mu_B = n\\mu_B$. Substituting these into the expression gives the centroid of the merged cluster:\n$$ \\mu_{A \\cup B} = \\frac{m\\mu_A + n\\mu_B}{m+n} $$\n\nSecond, we derive the expression for the increase in the total within-cluster sum of squares, $\\Delta W = W(A \\cup B) - W(A) - W(B)$. The within-cluster sum of squares for the merged cluster $A \\cup B$ is:\n$$ W(A \\cup B) = \\sum_{x \\in A \\cup B} \\|x - \\mu_{A \\cup B}\\|^2 $$\nAgain, we split the sum over the disjoint sets $A$ and $B$:\n$$ W(A \\cup B) = \\sum_{x \\in A} \\|x - \\mu_{A \\cup B}\\|^2 + \\sum_{x \\in B} \\|x - \\mu_{A \\cup B}\\|^2 $$\nLet's analyze the first term, $\\sum_{x \\in A} \\|x - \\mu_{A \\cup B}\\|^2$. We can introduce the centroid $\\mu_A$ by adding and subtracting it inside the norm:\n$$ \\sum_{x \\in A} \\|(x - \\mu_A) + (\\mu_A - \\mu_{A \\cup B})\\|^2 $$\nExpanding the squared norm $\\|u+v\\|^2 = \\|u\\|^2 + 2u \\cdot v + \\|v\\|^2$:\n$$ \\sum_{x \\in A} \\left( \\|x - \\mu_A\\|^2 + 2(x - \\mu_A) \\cdot (\\mu_A - \\mu_{A \\cup B}) + \\|\\mu_A - \\mu_{A \\cup B}\\|^2 \\right) $$\nWe can distribute the summation:\n$$ \\sum_{x \\in A} \\|x - \\mu_A\\|^2 + 2 \\left( \\sum_{x \\in A} (x - \\mu_A) \\right) \\cdot (\\mu_A - \\mu_{A \\cup B}) + \\sum_{x \\in A} \\|\\mu_A - \\mu_{A \\cup B}\\|^2 $$\nThe first term is, by definition, $W(A)$. The second term's summation part is $\\sum_{x \\in A} (x - \\mu_A) = \\sum_{x \\in A} x - \\sum_{x \\in A} \\mu_A = m\\mu_A - m\\mu_A = 0$. Thus, the entire second term vanishes. The third term is a sum of $m$ identical terms, so it equals $m\\|\\mu_A - \\mu_{A \\cup B}\\|^2$.\nTherefore, the sum over cluster $A$ simplifies to:\n$$ \\sum_{x \\in A} \\|x - \\mu_{A \\cup B}\\|^2 = W(A) + m\\|\\mu_A - \\mu_{A \\cup B}\\|^2 $$\nBy an identical argument for cluster $B$:\n$$ \\sum_{x \\in B} \\|x - \\mu_{A \\cup B}\\|^2 = W(B) + n\\|\\mu_B - \\mu_{A \\cup B}\\|^2 $$\nSubstituting these back into the expression for $W(A \\cup B)$:\n$$ W(A \\cup B) = W(A) + m\\|\\mu_A - \\mu_{A \\cup B}\\|^2 + W(B) + n\\|\\mu_B - \\mu_{A \\cup B}\\|^2 $$\nThe increase $\\Delta W$ is then:\n$$ \\Delta W = W(A \\cup B) - W(A) - W(B) = m\\|\\mu_A - \\mu_{A \\cup B}\\|^2 + n\\|\\mu_B - \\mu_{A \\cup B}\\|^2 $$\nNow we must express this in terms of $m$, $n$, and $\\|\\mu_A - \\mu_B\\|$. We use the expression for $\\mu_{A \\cup B}$:\n$$ \\mu_A - \\mu_{A \\cup B} = \\mu_A - \\frac{m\\mu_A + n\\mu_B}{m+n} = \\frac{(m+n)\\mu_A - (m\\mu_A + n\\mu_B)}{m+n} = \\frac{n(\\mu_A - \\mu_B)}{m+n} $$\n$$ \\mu_B - \\mu_{A \\cup B} = \\mu_B - \\frac{m\\mu_A + n\\mu_B}{m+n} = \\frac{(m+n)\\mu_B - (m\\mu_A + n\\mu_B)}{m+n} = \\frac{m(\\mu_B - \\mu_A)}{m+n} = -\\frac{m(\\mu_A - \\mu_B)}{m+n} $$\nSubstituting these into the expression for $\\Delta W$:\n$$ \\Delta W = m\\left\\|\\frac{n(\\mu_A - \\mu_B)}{m+n}\\right\\|^2 + n\\left\\|-\\frac{m(\\mu_A - \\mu_B)}{m+n}\\right\\|^2 $$\nUsing the property $\\|\\lambda v\\|^2 = \\lambda^2\\|v\\|^2$:\n$$ \\Delta W = m\\left(\\frac{n}{m+n}\\right)^2\\|\\mu_A - \\mu_B\\|^2 + n\\left(\\frac{m}{m+n}\\right)^2\\|\\mu_A - \\mu_B\\|^2 $$\n$$ \\Delta W = \\left( \\frac{mn^2}{(m+n)^2} + \\frac{nm^2}{(m+n)^2} \\right) \\|\\mu_A - \\mu_B\\|^2 $$\n$$ \\Delta W = \\frac{mn^2 + nm^2}{(m+n)^2} \\|\\mu_A - \\mu_B\\|^2 = \\frac{mn(n+m)}{(m+n)^2} \\|\\mu_A - \\mu_B\\|^2 $$\n$$ \\Delta W = \\frac{mn}{m+n} \\|\\mu_A - \\mu_B\\|^2 $$\nThis is the desired expression for the increase in the within-cluster sum of squares.\n\nThird, we evaluate the numerical value for $m=7$, $n=3$, $\\mu_A=(1,2)$, and $\\mu_B=(5,8)$.\nThe coefficient is:\n$$ \\frac{mn}{m+n} = \\frac{7 \\times 3}{7+3} = \\frac{21}{10} = 2.1 $$\nThe squared Euclidean distance between the centroids is:\n$$ \\|\\mu_A - \\mu_B\\|^2 = \\|(1-5, 2-8)\\|^2 = \\|(-4, -6)\\|^2 = (-4)^2 + (-6)^2 = 16 + 36 = 52 $$\nThe increase in the sum of squares is:\n$$ \\Delta W = (2.1) \\times (52) = 109.2 $$\nThe value $109.2$ has four significant figures as requested.\n\nFourth, we discuss qualitatively how the cost increase $\\Delta W$ depends on the size ratio $r=m/n$ when $\\|\\mu_A - \\mu_B\\|$ is held fixed. Let $D = \\|\\mu_A - \\mu_B\\|$, which is constant. The cost is $\\Delta W = \\frac{mn}{m+n} D^2$. The dependence is determined by the coefficient $C(m,n) = \\frac{mn}{m+n}$. Let's express this coefficient in terms of the ratio $r=m/n$ and the total number of points in one of the clusters, say $n$. By substituting $m=rn$, we get:\n$$ C(m,n) = \\frac{(rn)n}{rn+n} = \\frac{rn^2}{n(r+1)} = \\frac{rn}{r+1} $$\nThis shows the cost depends not just on the ratio $r$ but also on the absolute sizes of the clusters. A better way to analyze the effect of the ratio is to fix the total size $N=m+n$. Then $m = N-n$, and $C(m,n) = \\frac{(N-n)n}{N}$. This function of $n$ (for $1 \\le n < N$) is a downward-opening parabola maximized at $n=N/2$. This corresponds to $m=n$, or a size ratio $r=1$.\nWhen $r=1$ (clusters of equal size), the merge cost is highest for a fixed distance $D$ and total size $N$. As the size ratio becomes very large or very small (i.e., $r \\to \\infty$ or $r \\to 0$), the coefficient $\\frac{mn}{m+n}$ approaches $\\min(m,n)$. For a fixed total size $N$, this minimum value approaches $0$ as one cluster's size approaches $N$ and the other's approaches $0$.\nIn summary, Ward's method penalizes the merging of two balanced, same-size clusters more heavily than the merging of a small cluster into a large one. This characteristic promotes the formation of clusters with roughly equal sizes, a distinguishing feature of this linkage method.", "answer": "$$\\boxed{109.2}$$", "id": "3097665"}, {"introduction": "Theoretical understanding comes alive when you see algorithms in action. This hands-on coding practice demonstrates the dramatic difference between single and complete linkage on a dataset specifically designed to highlight the \"chaining effect\" [@problem_id:3097643]. By implementing and observing how single linkage follows a sparse chain of points while complete linkage identifies compact blobs, you will gain an unforgettable lesson in the behavioral consequences of your choice of linkage criterion.", "problem": "You will implement and compare agglomerative clustering with two linkages on a deterministic planar dataset that contains two dense blobs connected by a sparse chain. Your goal is to quantify the chaining effect by counting how many merges bridge from a dense blob to anything outside that blob (including the chain or the opposite blob) as a function of a distance threshold $h$.\n\nDataset definition:\n- Construct a set of points in $\\mathbb{R}^2$ consisting of three parts:\n  1. A left blob $L$: a grid of size $3 \\times 3$ centered near $(-4,0)$ with coordinates $x \\in \\{-4.1,-4.0,-3.9\\}$ and $y \\in \\{-0.1,0.0,0.1\\}$. This yields $9$ points.\n  2. A right blob $R$: a grid of size $3 \\times 3$ centered near $(4,0)$ with coordinates $x \\in \\{3.9,4.0,4.1\\}$ and $y \\in \\{-0.1,0.0,0.1\\}$. This yields $9$ points.\n  3. A chain $C$: collinear points along the $x$-axis at $y=0$ with $x \\in \\{-2.5,-2.0,-1.5,-1.0,-0.5,0.0,0.5,1.0,1.5,2.0,2.5\\}$. This yields $11$ points.\n- The total number of points is $29$.\n\nFundamental base for the method:\n- Use Euclidean distance $d(\\mathbf{x},\\mathbf{y})=\\|\\mathbf{x}-\\mathbf{y}\\|_{2}$ between points $\\mathbf{x},\\mathbf{y}\\in\\mathbb{R}^2$.\n- In agglomerative clustering, start from singleton clusters and repeatedly merge the pair of clusters $\\mathcal{A},\\mathcal{B}$ with minimum inter-cluster dissimilarity. For single linkage, the inter-cluster dissimilarity is $\\min\\{d(\\mathbf{x},\\mathbf{y}) : \\mathbf{x}\\in\\mathcal{A}, \\mathbf{y}\\in\\mathcal{B}\\}$. For complete linkage, it is $\\max\\{d(\\mathbf{x},\\mathbf{y}) : \\mathbf{x}\\in\\mathcal{A}, \\mathbf{y}\\in\\mathcal{B}\\}$.\n- The dendrogram merge height is the inter-cluster dissimilarity at the step the merge occurs.\n\nChaining-effect counting rule:\n- Define the left blob index set as $L$, the right blob index set as $R$, and the chain index set as $C$ (the complement of $L\\cup R$).\n- At each merge step, suppose two current clusters $\\mathcal{A}$ and $\\mathcal{B}$ are merged at height $h_{\\text{merge}}$. Define indicators $I_{L}(\\mathcal{A})=\\mathbf{1}[\\mathcal{A}\\cap L\\neq\\emptyset]$ and $I_{R}(\\mathcal{A})=\\mathbf{1}[\\mathcal{A}\\cap R\\neq\\emptyset]$ (and similarly for $\\mathcal{B}$).\n- A merge is counted as “across the chain” if $(I_{L}(\\mathcal{A}) \\oplus I_{L}(\\mathcal{B})) \\lor (I_{R}(\\mathcal{A}) \\oplus I_{R}(\\mathcal{B}))$ is true, where $\\oplus$ is exclusive-or and $\\lor$ is logical or. Intuitively, a merge is counted if and only if it attaches any portion of a blob to something that does not already contain that same blob.\n\nThresholded count:\n- For a given threshold $h$, define the chaining count $M_{\\text{linkage}}(h)$ as the total number of merges counted by the above rule among all dendrogram merges whose heights satisfy $h_{\\text{merge}} \\le h$.\n- You must compute $M_{\\text{single}}(h)$ and $M_{\\text{complete}}(h)$ for the specified thresholds.\n\nTest suite:\n- Use the following thresholds $h$ (in the same Euclidean units as the data): $[0.0,\\,0.1,\\,0.5,\\,1.401,\\,6.61,\\,8.21]$.\n  - These values include a boundary case $h=0.0$, within-blob spacing $h=0.1$, chain spacing $h=0.5$, a just-above the left/right-to-chain contact scale $h=1.401$, an intermediate scale $h=6.61$ that is above the complete-linkage left-to-chain merge height but below the left-chain to right complete height, and a large $h=8.21$ above the complete-linkage left-chain to right merge height.\n\nRequired output:\n- Your program must perform agglomerative clustering twice (single linkage and complete linkage), compute $M_{\\text{single}}(h)$ and $M_{\\text{complete}}(h)$ for each $h$ in the test suite, and output a single line containing a comma-separated list of pairs in the order of the thresholds, where each pair is of the form $[M_{\\text{single}}(h),M_{\\text{complete}}(h)]$.\n- The final output format must be exactly a single line:\n  - An example of the exact shape is $[[a_1,b_1],[a_2,b_2],\\dots,[a_6,b_6]]$ where each $a_i$ and $b_i$ are integers.\n\nConstraints and notes:\n- Implement the clustering strictly from the stated definitions using Euclidean distance. Do not assume any prebuilt hierarchical clustering routines.\n- Angles are not involved; there is no unit conversion.\n- All outputs must be integers.", "solution": "The user-provided problem is valid. It is a well-posed, scientifically grounded task within the domain of statistical learning, specifically concerning the properties of agglomerative clustering algorithms. All definitions, data, and constraints are clear, consistent, and sufficient for deriving a unique solution. I will proceed with a full solution.\n\nThe problem requires the implementation of agglomerative hierarchical clustering from first principles for two linkage methods, single and complete, on a specially constructed planar dataset. The objective is to quantify the \"chaining effect\" by counting the number of merges that bridge between distinct regions of the dataset as a function of a distance threshold $h$.\n\n**1. Dataset Construction**\n\nFirst, we construct the dataset in $\\mathbb{R}^2$ as specified. It comprises $N=29$ points divided into three groups:\n- A left blob $L$: A $3 \\times 3$ grid of $9$ points. The coordinates are generated from $x \\in \\{-4.1, -4.0, -3.9\\}$ and $y \\in \\{-0.1, 0.0, 0.1\\}$. We assign indices $i \\in \\{0, 1, \\dots, 8\\}$ to these points.\n- A right blob $R$: A $3 \\times 3$ grid of $9$ points. The coordinates are generated from $x \\in \\{3.9, 4.0, 4.1\\}$ and $y \\in \\{-0.1, 0.0, 0.1\\}$. We assign indices $i \\in \\{9, 10, \\dots, 17\\}$ to these points.\n- A chain $C$: A set of $11$ collinear points along the $x$-axis ($y=0$). The coordinates are $x \\in \\{-2.5, -2.0, \\dots, 2.5\\}$ with a step of $0.5$. We assign indices $i \\in \\{18, 19, \\dots, 28\\}$ to these points.\n\nThe distance metric between any two points $\\mathbf{p}_i = (x_i, y_i)$ and $\\mathbf{p}_j = (x_j, y_j)$ is the Euclidean distance, $d(\\mathbf{p}_i, \\mathbf{p}_j) = \\sqrt{(x_i - x_j)^2 + (y_i - y_j)^2}$. We pre-compute all pairwise distances and store them in a symmetric $N \\times N$ matrix $\\mathbf{D}$, where $\\mathbf{D}_{ij} = d(\\mathbf{p}_i, \\mathbf{p}_j)$.\n\n**2. Agglomerative Clustering Algorithm**\n\nWe implement the agglomerative clustering algorithm from its fundamental definition, without relying on pre-packaged library functions for the clustering logic itself. The general procedure is as follows:\n\n1.  **Initialization**: Begin with $N=29$ clusters, where each cluster is a singleton set containing one point, e.g., $\\mathcal{C}_i = \\{i\\}$ for $i=0, \\dots, 28$. We maintain a list of active clusters. A dissimilarity matrix between clusters is initialized with the pairwise point distances $\\mathbf{D}$.\n\n2.  **Iterative Merging**: Repeat for $N-1=28$ steps until only one cluster remains:\n    a.  **Find Closest Pair**: Identify the pair of distinct active clusters, $\\mathcal{A}$ and $\\mathcal{B}$, with the minimum inter-cluster dissimilarity. This minimum dissimilarity value is the merge height, $h_{\\text{merge}}$.\n    b.  **Apply Chaining Count Rule**: Before merging, we apply the specified rule to determine if this merge constitutes a \"chaining\" event.\n    c.  **Merge**: Form a new cluster $\\mathcal{A} \\cup \\mathcal{B}$. Deactivate the old clusters $\\mathcal{A}$ and $\\mathcal{B}$.\n    d.  **Update Dissimilarities**: Calculate the dissimilarity between the new cluster and all other active clusters.\n\nThis process is performed separately for single and complete linkage. The key difference lies in the update rule for dissimilarity (step 2d), which is governed by the Lance-Williams formulas. If cluster $\\mathcal{U}$ is formed by merging $\\mathcal{A}$ and $\\mathcal{B}$, its dissimilarity to any other cluster $\\mathcal{V}$ is:\n\n-   **Single Linkage**: $d_{\\text{single}}(\\mathcal{U}, \\mathcal{V}) = \\min(d(\\mathcal{A}, \\mathcal{V}), d(\\mathcal{B}, \\mathcal{V}))$. This corresponds to the minimum distance between any point in $\\mathcal{U}$ and any point in $\\mathcal{V}$.\n-   **Complete Linkage**: $d_{\\text{complete}}(\\mathcal{U}, \\mathcal{V}) = \\max(d(\\mathcal{A}, \\mathcal{V}), d(\\mathcal{B}, \\mathcal{V}))$. This corresponds to the maximum distance between any point in $\\mathcal{U}$ and any point in $\\mathcal{V}$.\n\n**3. Chaining-Effect Counting Rule**\n\nAt each merge step, for the two clusters $\\mathcal{A}$ and $\\mathcal{B}$ being merged, we evaluate a specific logical condition. We define indicator functions based on whether a cluster contains points from the original blobs $L$ and $R$:\n- $I_{L}(\\mathcal{A}) = \\mathbf{1}[\\mathcal{A} \\cap L \\neq \\emptyset]$, which is $1$ if cluster $\\mathcal{A}$ contains at least one point from blob $L$, and $0$ otherwise.\n- $I_{R}(\\mathcal{A}) = \\mathbf{1}[\\mathcal{A} \\cap R \\neq \\emptyset]$, similarly for blob $R$.\n\nA merge is counted if the condition $(I_{L}(\\mathcal{A}) \\oplus I_{L}(\\mathcal{B})) \\lor (I_{R}(\\mathcal{A}) \\oplus I_{R}(\\mathcal{B}))$ is true. Here, $\\oplus$ denotes the exclusive-OR (XOR) operation and $\\lor$ denotes the logical OR operation. This rule effectively flags any merge that connects a cluster containing points from a specific blob to a cluster that does not, thus capturing the act of a blob \"reaching out\" to the chain or the other blob.\n\nFor each of the $28$ merges, we record a tuple $(h_{\\text{merge}}, \\text{is\\_counted})$, where `is_counted` is a binary value ($1$ or $0$) from the rule above.\n\n**4. Thresholded Count Calculation**\n\nAfter completing the clustering process for a given linkage type and obtaining the list of $28$ merge records, we calculate the final counts for each specified threshold $h$. The thresholded count, $M_{\\text{linkage}}(h)$, is the cumulative sum of `is_counted` values for all merges where $h_{\\text{merge}} \\le h$.\n\nFor each threshold $h$ in the test suite $\\{0.0, 0.1, 0.5, 1.401, 6.61, 8.21\\}$, we compute $M_{\\text{single}}(h)$ and $M_{\\text{complete}}(h)$.\n\n- **Single Linkage Analysis**: This method is sensitive to the closest points between clusters. The small distances within blobs ($d \\ge 0.1$) and along the chain ($d=0.5$) lead to the formation of three large clusters: $\\mathcal{L}$ (all of $L$), $\\mathcal{R}$ (all of $R$), and $\\mathcal{C}$ (all of $C$) at a merge height just above $0.5$. The next merges connect these super-clusters. The minimum distance between $\\mathcal{L}$ and $\\mathcal{C}$ is $1.4$, and between $\\mathcal{R}$ and $\\mathcal{C}$ is also $1.4$. The first counted merge occurs at $h=1.4$ (e.g., $\\mathcal{L}$ with $\\mathcal{C}$). The subsequent merge of the resulting cluster $(\\mathcal{L} \\cup \\mathcal{C})$ with $\\mathcal{R}$ also occurs at $h=1.4$ and is also counted. This results in $2$ counted \"chaining\" merges.\n\n- **Complete Linkage Analysis**: This method is sensitive to the farthest points between clusters (the diameter). The blobs $\\mathcal{L}$ and $\\mathcal{R}$ consolidate quickly as their diameters are small ($\\approx 0.28$). The chain $\\mathcal{C}$ has a large diameter of $5.0$. A crucial observation is that the merge height to form a single cluster $\\mathcal{C}$ is $5.0$, which is less than the merge height required to join a blob to the chain. The complete-linkage dissimilarity between the fully-formed cluster $\\mathcal{L}$ and the fully-formed cluster $\\mathcal{C}$ is determined by their farthest points, $d((-4.1, \\cdot), (2.5,0)) \\approx 6.60$. The first counted merge therefore happens at this much larger height. The final merge, between $(\\mathcal{L} \\cup \\mathcal{C})$ and $\\mathcal{R}$, occurs at an even greater height of $\\approx 8.20$. This method robustly separates the blobs until high-distance thresholds are reached.\n\nThe implementation generates these results by explicitly carrying out the described algorithm.", "answer": "[[0,0],[0,0],[0,0],[2,0],[2,1],[2,2]]", "id": "3097643"}, {"introduction": "Real-world datasets are often messy and can contain outliers that may affect clustering algorithms. This exercise investigates the crucial concept of algorithmic robustness by testing how complete, average, and Ward's linkage respond to a single, distant outlier [@problem_id:3097618]. Quantifying the disruption to the cluster structure will provide practical insight into which methods are more resilient and which are more sensitive to noisy data.", "problem": "You are given a deterministic two-dimensional dataset and asked to compare how three standard agglomerative hierarchical clustering linkages respond to the addition of a single far-out outlier. The linkages are complete linkage, average linkage, and Ward’s linkage. Use the Euclidean metric throughout. Your task is to compute, for each linkage and each prescribed outlier position, how much the cut-level partition at $k=3$ clusters changes on the original points when the outlier is appended.\n\nFundamental base:\n- Agglomerative hierarchical clustering starts with each observation as its own cluster and iteratively merges clusters based on a linkage rule and a dissimilarity metric. Here the metric is Euclidean distance, denoted $d(\\mathbf{x}, \\mathbf{y}) = \\|\\mathbf{x} - \\mathbf{y}\\|_2$.\n- Complete linkage defines the distance between two clusters $A$ and $B$ as $\\max\\{d(\\mathbf{x}, \\mathbf{y}) : \\mathbf{x} \\in A, \\mathbf{y} \\in B\\}$.\n- Average linkage defines the distance between two clusters $A$ and $B$ as $\\frac{1}{|A|\\,|B|} \\sum_{\\mathbf{x} \\in A} \\sum_{\\mathbf{y} \\in B} d(\\mathbf{x}, \\mathbf{y})$.\n- Ward’s method chooses at each step the merge that yields the minimal increase in total within-cluster Sum of Squared Errors (SSE), where the SSE for a cluster $C$ with centroid $\\boldsymbol{\\mu}_C$ is $\\sum_{\\mathbf{x} \\in C} \\|\\mathbf{x} - \\boldsymbol{\\mu}_C\\|_2^2$.\n\nDataset:\n- Base set $X = \\{\\mathbf{x}_1, \\dots, \\mathbf{x}_{12}\\} \\subset \\mathbb{R}^2$ of $12$ points:\n  - Cluster near $(0,0)$: $\\;(-0.2,\\,0.1),\\;(0.0,\\,0.2),\\;(0.2,\\,-0.1),\\;(0.1,\\,0.0)$.\n  - Cluster near $(4,0)$: $\\;(3.8,\\,0.1),\\;(4.1,\\,-0.2),\\;(4.2,\\,0.2),\\;(3.9,\\,-0.1)$.\n  - Cluster near $(2,3)$: $\\;(2.0,\\,3.1),\\;(2.1,\\,2.9),\\;(1.9,\\,3.2),\\;(2.2,\\,3.0)$.\n\nPerturbation with a single outlier:\n- Let $n=12$ and $\\bar{\\mathbf{x}} = \\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{x}_i$ be the centroid of the base set.\n- For a given scalar $d \\ge 0$, define an outlier point $\\mathbf{z}(d) = \\bar{\\mathbf{x}} + (0,\\,d)$, i.e., placed vertically above the centroid at vertical offset $d$.\n- Construct the perturbed dataset $X(d) = X \\cup \\{\\mathbf{z}(d)\\}$.\n\nCut-level partition and sensitivity measure:\n- For each linkage $\\ell \\in \\{\\text{complete}, \\text{average}, \\text{Ward}\\}$ and each dataset (base and perturbed), form the agglomerative dendrogram using Euclidean distances and cut it to obtain exactly $k=3$ flat clusters.\n- Let $\\pi_\\ell$ denote the partition of $X$ obtained by cutting the dendrogram for the base set at $k=3$ under linkage $\\ell$.\n- Let $\\pi_\\ell'(d)$ denote the partition of $X(d)$ obtained by cutting at $k=3$, then restrict $\\pi_\\ell'(d)$ to the original $n$ indices to get a partition of $X$.\n- Define the sensitivity score $S_\\ell(d)$ as the number of unordered pairs $\\{i,j\\}$ with $1 \\le i &lt; j \\le n$ for which the co-membership of $\\mathbf{x}_i$ and $\\mathbf{x}_j$ differs between $\\pi_\\ell$ and the restriction of $\\pi_\\ell'(d)$. Equivalently,\n  $$S_\\ell(d) = \\sum_{1 \\le i < j \\le n} \\left| \\mathbb{I}\\{\\mathbf{x}_i \\sim_{\\pi_\\ell} \\mathbf{x}_j\\} - \\mathbb{I}\\{\\mathbf{x}_i \\sim_{\\pi_\\ell'(d)} \\mathbf{x}_j\\} \\right|,$$\n  where $\\mathbb{I}\\{\\cdot\\}$ is the indicator function and $\\sim_{\\pi}$ denotes “in the same cluster under partition $\\pi$.”\n\nTest suite:\n- Evaluate the four cases $d \\in \\{0,\\,8,\\,20,\\,60\\}$.\n\nRequired output:\n- For each $d$ in the order $d=0$, $d=8$, $d=20$, $d=60$, compute and report the triple $[S_{\\text{complete}}(d),\\, S_{\\text{average}}(d),\\, S_{\\text{Ward}}(d)]$ as integers.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a list of three integers corresponding to the three linkages in the fixed order $\\big[$complete, average, Ward$\\big]$. For example, an output with two cases would look like $[[a,b,c],[d,e,f]]$ with no spaces, where $a,\\dots,f$ are integers.\n\nAngle units: not applicable. Physical units: not applicable.\n\nImplementation constraints:\n- Use Euclidean distance and the specified linkages.\n- Cut the dendrogram to obtain exactly $k=3$ clusters in all cases.\n- Use only the Python Standard Library, NumPy, and SciPy as allowed by the execution environment.", "solution": "The problem statement has been validated and is deemed scientifically grounded, well-posed, and complete. We may proceed with a formal solution.\n\nThe objective is to quantify the sensitivity of three agglomerative hierarchical clustering linkages—complete, average, and Ward's—to the addition of a single outlier. The sensitivity is measured by the change in a $k=3$ cluster partition of a base dataset when an outlier is introduced.\n\n**1. Preliminaries: Dataset and Partitions**\n\nFirst, we define the base dataset $X = \\{\\mathbf{x}_1, \\dots, \\mathbf{x}_{12}\\} \\subset \\mathbb{R}^2$ consisting of $n=12$ points, organized into three visually distinct groups:\n- $C_1^{\\text{true}} = \\{(-0.2, 0.1), (0.0, 0.2), (0.2, -0.1), (0.1, 0.0)\\}$\n- $C_2^{\\text{true}} = \\{(3.8, 0.1), (4.1, -0.2), (4.2, 0.2), (3.9, -0.1)\\}$\n- $C_3^{\\text{true}} = \\{(2.0, 3.1), (2.1, 2.9), (1.9, 3.2), (2.2, 3.0)\\}$\n\nFor each linkage method $\\ell \\in \\{\\text{complete}, \\text{average}, \\text{Ward}\\}$, we first establish a baseline partition, $\\pi_\\ell$. This is achieved by performing agglomerative clustering on the base set $X$ using the Euclidean metric and cutting the resulting dendrogram to yield exactly $k=3$ clusters. Given the clear separation of the data, it is expected that for all three linkage methods, the resulting partition $\\pi_\\ell$ will correctly identify the three groups, i.e., $\\pi_\\ell = \\{C_1^{\\text{true}}, C_2^{\\text{true}}, C_3^{\\text{true}}\\}$.\n\n**2. Perturbation and Sensitivity Measure**\n\nNext, we introduce a perturbation. The centroid of the base set is calculated as $\\bar{\\mathbf{x}} = \\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{x}_i$. An outlier, $\\mathbf{z}(d)$, is defined as a point placed at a vertical distance $d$ from this centroid: $\\mathbf{z}(d) = \\bar{\\mathbf{x}} + (0, d)$. The perturbed dataset is then $X(d) = X \\cup \\{\\mathbf{z}(d)\\}$, containing $n+1 = 13$ points.\n\nFor each linkage $\\ell$ and each given value of $d$, we perform agglomerative clustering on the perturbed dataset $X(d)$ and again cut the dendrogram to form $k=3$ clusters. This results in a partition of $X(d)$, denoted $\\pi_\\ell'(d)$. We then consider the partition of the original points $X$ induced by $\\pi_\\ell'(d)$. This is obtained by observing the cluster assignments for only the first $n=12$ points.\n\nThe sensitivity of the linkage method is quantified by the score $S_\\ell(d)$, which counts the number of unordered pairs of points $\\{\\mathbf{x}_i, \\mathbf{x}_j\\}$ from the original set $X$ whose co-membership status changes. Mathematically, it is defined as:\n$$\nS_\\ell(d) = \\sum_{1 \\le i < j \\le n} \\left| \\mathbb{I}\\{\\mathbf{x}_i \\sim_{\\pi_\\ell} \\mathbf{x}_j\\} - \\mathbb{I}\\{\\mathbf{x}_i \\sim_{\\pi_\\ell'(d)} \\mathbf{x}_j\\} \\right|\n$$\nwhere $\\mathbb{I}\\{\\cdot\\}$ is the indicator function and $\\mathbf{x}_i \\sim_{\\pi} \\mathbf{x}_j$ signifies that points $\\mathbf{x}_i$ and $\\mathbf{x}_j$ belong to the same cluster in partition $\\pi$.\n\nA score of $S_\\ell(d)=0$ indicates that the partition of the original $12$ points was entirely unaffected by the outlier. A non-zero score indicates a structural change in the clustering. For instance, if the outlier's presence causes two of the original $4$-point clusters to merge, the number of pairs that change from \"not co-clustered\" to \"co-clustered\" is $4 \\times 4 = 16$. The sensitivity score would thus be $S_\\ell(d)=16$.\n\n**3. Algorithmic Procedure**\n\nThe computation for each required value of $d \\in \\{0, 8, 20, 60\\}$ follows these steps:\n1.  **Pre-computation**: For each linkage $\\ell \\in \\{\\text{complete}, \\text{average}, \\text{Ward}\\}$, compute the baseline partition $\\pi_\\ell$ of the $12$ points in $X$ and store the corresponding cluster labels. This is done using `scipy.spatial.distance.pdist` to find the pairwise Euclidean distances, `scipy.cluster.hierarchy.linkage` to build the hierarchy, and `scipy.cluster.hierarchy.fcluster` with `criterion='maxclust'` and a threshold of $3$ to obtain the labels.\n2.  **Iteration over $d$**: For each value of $d$:\n    a. Construct the perturbed dataset $X(d)$.\n    b. For each linkage $\\ell$:\n        i.  Perform agglomerative clustering on $X(d)$ to obtain the $k=3$ partition $\\pi_\\ell'(d)$.\n        ii. Extract the cluster labels for the first $12$ points from $\\pi_\\ell'(d)$.\n        iii. Compare the co-membership for all $\\binom{12}{2} = 66$ pairs of points between the baseline partition and the perturbed partition. Sum the number of disagreements to find the score $S_\\ell(d)$.\n3.  **Result Aggregation**: Collect the scores for each $d$ into a triplet $[S_{\\text{complete}}(d), S_{\\text{average}}(d), S_{\\text{Ward}}(d)]$.\n\nThis procedure is systematically applied to the test suite, and the resulting lists of integers are formatted as per the problem specification. This analysis will reveal how the choice of linkage criterion determines the robustness of the clustering solution in the presence of an outlier. Ward's method and average linkage are generally known to be more sensitive to outliers than complete linkage, which we expect to be reflected in the computed scores.", "answer": "[[0,0,0],[0,16,16],[0,16,16],[0,16,16]]", "id": "3097618"}]}