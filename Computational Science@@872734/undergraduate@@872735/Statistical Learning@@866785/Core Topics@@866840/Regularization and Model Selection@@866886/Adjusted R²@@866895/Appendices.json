{"hands_on_practices": [{"introduction": "Before interpreting the adjusted R-squared, it is essential to be comfortable with its calculation. This first exercise [@problem_id:1031765] provides a direct application of the formula, solidifying your understanding of how the sample size ($n$), the number of predictors ($p$), the residual sum of squares (RSS), and the total sum of squares (TSS) come together. Mastering this fundamental calculation is the first step toward applying the metric effectively in model evaluation.", "problem": "In a multiple linear regression analysis with $n=25$ observations and $p=4$ predictor variables, the residual sum of squares (RSS) is given as 15.8 and the total sum of squares (TSS) is 120.5. Compute the adjusted coefficient of determination, denoted by $\\bar{R}^2$.  \n\nThe adjusted $R^2$ accounts for the number of predictors and is defined as:  \n$$  \n\\bar{R}^2 = 1 - \\frac{\\mathrm{RSS}/(n-p-1)}{\\mathrm{TSS}/(n-1)}  \n$$  \nProvide the exact value of $\\bar{R}^2$.", "solution": "1. Start with the definition  \n$$\n\\bar R^2 \\;=\\; 1 \\;-\\; \\frac{\\mathrm{RSS}/(n-p-1)}{\\mathrm{TSS}/(n-1)}.\n$$  \n2. Substitute $n=25$, $p=4$, $\\mathrm{RSS}=15.8$, $\\mathrm{TSS}=120.5$. Then  \n$$\nn-p-1 = 25-4-1 = 20,\\qquad n-1 = 24.\n$$  \n3. Compute each mean square:  \n$$\n\\frac{\\mathrm{RSS}}{n-p-1}\n= \\frac{15.8}{20}\n= \\frac{79/5}{20}\n= \\frac{79}{100},\n\\quad\n\\frac{\\mathrm{TSS}}{n-1}\n= \\frac{120.5}{24}\n= \\frac{241/2}{24}\n= \\frac{241}{48}.\n$$  \n4. Form their ratio:  \n$$\n\\frac{\\mathrm{RSS}/(n-p-1)}{\\mathrm{TSS}/(n-1)}\n= \\frac{79/100}{241/48}\n= \\frac{79\\cdot48}{100\\cdot241}\n= \\frac{3792}{24100}\n= \\frac{948}{6025}.\n$$  \n5. Thus  \n$$\n\\bar R^2\n=1 - \\frac{948}{6025}\n= \\frac{6025 - 948}{6025}\n= \\frac{5077}{6025}.\n$$", "answer": "$$\\boxed{5077/6025}$$", "id": "1031765"}, {"introduction": "Now that we understand *how* to calculate adjusted R-squared, let's explore *why* it is such a crucial tool for model selection. This simulation exercise [@problem_id:3152035] powerfully demonstrates the primary weakness of the standard $R^2$: its tendency to increase with any added predictor, even one that is pure noise. By running this simulation, you will see firsthand how adjusted $R^2$ correctly penalizes this overfitting and how its judgment aligns with the gold standard of out-of-sample performance measured by cross-validation.", "problem": "Consider the multiple linear regression model with an intercept, where the response vector $y \\in \\mathbb{R}^n$ is generated by $y = X_{\\text{signal}}\\beta_{\\text{signal}} + \\varepsilon$, and the noise vector $\\varepsilon \\in \\mathbb{R}^n$ is independent and identically distributed Gaussian with $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$. The predictor matrix is decomposed as $X = [X_{\\text{signal}}, X_{\\text{noise}}]$, where $X_{\\text{signal}} \\in \\mathbb{R}^{n \\times p_{\\text{signal}}}$ contains the true signal predictors and $X_{\\text{noise}} \\in \\mathbb{R}^{n \\times q_{\\text{noise}}}$ contains additional noise predictors that are independent of both $X_{\\text{signal}}$ and $y$.\n\nStarting from the core definitions of the multiple linear regression model and Ordinary Least Squares (OLS), implement the following procedure to compare the coefficient of determination (R-squared) and adjusted R-squared under increasing number of predictors $p$, and validate the comparison via cross-validation error:\n\n- For each test case in the test suite below, generate data according to the specified parameters by drawing each column of $X_{\\text{signal}}$ and $X_{\\text{noise}}$ independently from a standard normal distribution, and drawing $\\varepsilon$ independently from a Gaussian distribution with variance $\\sigma^2$. Construct $y$ using the specified $\\beta_{\\text{signal}}$ and include an intercept term in all fitted models.\n- Fit two OLS models: a \"signal-only\" model using $X_{\\text{signal}}$ and an \"all-predictors\" model using both $X_{\\text{signal}}$ and $X_{\\text{noise}}$. Both models must include an intercept.\n- For each fitted model, compute the training coefficient of determination $R^2$ and the adjusted $R^2$ using their standard definitions arising from the decomposition of total variance and degrees-of-freedom adjustment for the intercept model.\n- Estimate the out-of-sample performance using $K$-fold Cross-Validation (CV), defined as partitioning the indices $\\{1,\\dots,n\\}$ into $K$ approximately equal folds, fitting on $K-1$ folds, predicting on the held-out fold, and averaging the mean squared error (MSE) across folds. Report the average CV MSE for each model.\n\nYour program must evaluate, for each test case, the following three boolean statements comparing the \"all-predictors\" model against the \"signal-only\" model:\n- $R^2$ of the all-predictors model is greater than or equal to the $R^2$ of the signal-only model.\n- Adjusted $R^2$ of the all-predictors model is strictly less than the adjusted $R^2$ of the signal-only model.\n- $K$-fold CV MSE of the all-predictors model is strictly greater than the $K$-fold CV MSE of the signal-only model.\n\nTest suite parameters (use the exact values and seeds provided):\n- Test case $1$: $n=400$, $p_{\\text{signal}}=5$, $q_{\\text{noise}}=40$, $\\beta_{\\text{signal}}=[2.0,-1.0,1.5,0.5,-0.8]$, $\\sigma=1.0$, $K=5$, random seed $42$.\n- Test case $2$: $n=120$, $p_{\\text{signal}}=3$, $q_{\\text{noise}}=80$, $\\beta_{\\text{signal}}=[1.8,-0.7,0.9]$, $\\sigma=1.2$, $K=5$, random seed $7$.\n- Test case $3$: $n=200$, $p_{\\text{signal}}=10$, $q_{\\text{noise}}=20$, $\\beta_{\\text{signal}}=[1.3,-0.6,0.7,0.5,-1.1,0.9,-0.4,0.3,-0.2,0.8]$, $\\sigma=2.0$, $K=5$, random seed $99$.\n\nAngle units and physical units are not applicable in this purely statistical problem. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to a test case and is itself a list of three booleans in the order described above. For example, the output format must be exactly of the form $[[b_{11},b_{12},b_{13}],[b_{21},b_{22},b_{23}],[b_{31},b_{32},b_{33}]]$ with $b_{ij} \\in \\{\\text{True},\\text{False}\\}$.", "solution": "The user wants to solve a statistical problem.\nHere's a plan:\n1.  **Validate the Problem Statement**: The problem is a standard statistical simulation study to demonstrate overfitting and the properties of different model selection metrics in multiple linear regression. It is scientifically sound, well-posed, objective, and complete. All parameters and procedures are clearly defined. The verdict is **valid**.\n2.  **Explain the Principles**: The solution will be structured around the core principles of Ordinary Least Squares (OLS), the coefficient of determination ($R^2$), the adjusted $R^2$, and K-fold cross-validation.\n3.  **Step-by-Step Logic**: Detail the algorithm for data generation, model fitting, metric calculation, and comparison as specified in the problem.\n4.  **Implement in Code**: A Python script using `numpy` will be developed to execute the procedure for the given test cases.\n\n### Principles and Methodology\n\nThis problem investigates the consequences of including irrelevant predictors in a multiple linear regression model. We compare a \"true\" model containing only signal predictors with an \"overfitted\" model that also includes noise predictors. The comparison is performed using three key metrics: the coefficient of determination ($R^2$), the adjusted $R^2$, and the cross-validated mean squared error (CV MSE).\n\n#### 1. The Ordinary Least Squares (OLS) Model\n\nThe multiple linear regression model aims to describe a response variable $y \\in \\mathbb{R}^n$ as a linear combination of predictor variables, contained in a design matrix $X \\in \\mathbb{R}^{n \\times p}$. For a model with an intercept, the equation is:\n$$ y = \\beta_0 + X\\beta + \\varepsilon $$\nwhere $\\beta_0$ is the intercept, $\\beta \\in \\mathbb{R}^p$ is the vector of predictor coefficients, and $\\varepsilon \\in \\mathbb{R}^n$ is the error vector. This can be written more compactly by augmenting the design matrix with a column of ones, $X_{\\text{aug}} = [\\mathbf{1}_n, X]$, and the coefficient vector as $\\beta_{\\text{aug}} = [\\beta_0, \\beta^T]^T$. The model becomes $y = X_{\\text{aug}}\\beta_{\\text{aug}} + \\varepsilon$.\n\nThe OLS method finds the coefficient estimates $\\hat{\\beta}_{\\text{aug}}$ that minimize the Residual Sum of Squares (RSS), defined as:\n$$ \\text{RSS} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = (y - X_{\\text{aug}}\\hat{\\beta}_{\\text{aug}})^T (y - X_{\\text{aug}}\\hat{\\beta}_{\\text{aug}}) $$\nThe solution to this minimization problem is given by the normal equations:\n$$ \\hat{\\beta}_{\\text{aug}} = (X_{\\text{aug}}^T X_{\\text{aug}})^{-1} X_{\\text{aug}}^T y $$\nThis procedure will be applied to fit two models:\n1.  **Signal-only model**: Using the design matrix $X_{\\text{signal}} \\in \\mathbb{R}^{n \\times p_{\\text{signal}}}$.\n2.  **All-predictors model**: Using the design matrix $X = [X_{\\text{signal}}, X_{\\text{noise}}] \\in \\mathbb{R}^{n \\times (p_{\\text{signal}} + q_{\\text{noise}})}$.\n\n#### 2. Coefficient of Determination ($R^2$)\n\n$R^2$ measures the proportion of the variance in the response variable $y$ that is predictable from the predictor variables. It is defined based on the decomposition of the Total Sum of Squares (TSS):\n$$ \\text{TSS} = \\sum_{i=1}^{n} (y_i - \\bar{y})^2 $$\nwhere $\\bar{y}$ is the mean of $y$. The decomposition is $\\text{TSS} = \\text{ESS} + \\text{RSS}$, where ESS is the Explained Sum of Squares. The $R^2$ is:\n$$ R^2 = 1 - \\frac{\\text{RSS}}{\\text{TSS}} $$\nA key property of $R^2$ is that it is non-decreasing when new predictors are added to the model. This is because adding a predictor can only decrease (or keep constant) the RSS, the minimized sum of squared errors. Therefore, we expect the $R^2$ of the \"all-predictors\" model to be greater than or equal to the $R^2$ of the \"signal-only\" model. This directly corresponds to the first boolean check: $R^2_{\\text{all}} \\ge R^2_{\\text{signal}}$.\n\n#### 3. Adjusted $R^2$\n\nThe non-decreasing nature of $R^2$ makes it a poor metric for comparing models with different numbers of predictors. A model can achieve a higher $R^2$ simply by including more variables, even if they are pure noise. The adjusted $R^2$ corrects for this by penalizing the score for each added predictor. It is defined as:\n$$ R^2_{\\text{adj}} = 1 - \\frac{\\text{RSS} / (n - p - 1)}{\\text{TSS} / (n - 1)} $$\nHere, $p$ is the number of predictors (excluding the intercept). The term $\\text{RSS} / (n - p - 1)$ is an unbiased estimate of the error variance $\\sigma^2$. When a useless predictor is added, $p$ increases by $1$, which increases the penalty. If the reduction in RSS is not sufficient to outweigh this penalty, the adjusted $R^2$ will decrease. Since the \"all-predictors\" model adds $q_{\\text{noise}}$ purely random variables, their inclusion is not justified, and we expect the adjusted $R^2$ to be lower than that of the simpler \"signal-only\" model. This corresponds to the second boolean check: $R^2_{\\text{adj, all}}  R^2_{\\text{adj, signal}}$.\n\n#### 4. K-Fold Cross-Validation (CV)\n\nWhile adjusted $R^2$ provides a better in-sample comparison, a more robust method for evaluating a model's predictive power is to estimate its performance on unseen data. K-fold CV is a standard technique for this. The dataset is partitioned into $K$ equal-sized folds. For each fold $k \\in \\{1, \\dots, K\\}$, the model is trained on the other $K-1$ folds and then used to predict the responses for the held-out fold $k$. The Mean Squared Error (MSE) is calculated for each fold:\n$$ \\text{MSE}_k = \\frac{1}{n_k} \\sum_{i \\in \\text{fold } k} (y_i - \\hat{y}_i)^2 $$\nwhere $n_k$ is the number of observations in fold $k$. The overall CV score is the average of these MSEs:\n$$ \\text{MSE}_{\\text{CV}} = \\frac{1}{K} \\sum_{k=1}^{K} \\text{MSE}_k $$\nBy including noise predictors, the \"all-predictors\" model tends to overfit the training data. It learns spurious correlations present in the specific training-set sample. When this overfitted model is applied to the unseen test fold, its performance is poor, resulting in a higher MSE. Therefore, we expect the CV MSE of the \"all-predictors\" model to be greater than that of the \"signal-only\" model, which corresponds to the third boolean check: $\\text{MSE}_{\\text{CV, all}}  \\text{MSE}_{\\text{CV, signal}}$.\n\n### Implementation Strategy\n\nThe following algorithm will be implemented for each test case:\n1.  **Initialization**: Set the random seed for reproducibility.\n2.  **Data Generation**: Generate $X_{\\text{signal}}$, $X_{\\text{noise}}$, and $\\varepsilon$ from their respective normal distributions using the specified parameters ($n$, $p_{\\text{signal}}$, $q_{\\text{noise}}$, $\\sigma$). Construct the response vector $y = X_{\\text{signal}}\\beta_{\\text{signal}} + \\varepsilon$.\n3.  **Fold Creation**: Partition the data indices $\\{0, \\dots, n-1\\}$ into $K$ folds. This partition will be used for both models to ensure a fair comparison.\n4.  **Model Analysis**:\n    *   For the **\"signal-only\" model** (using $X_1 = X_{\\text{signal}}$):\n        *   Fit the OLS model on the full dataset to get $\\hat{\\beta}_1$.\n        *   Calculate $R^2_1$ and $R^2_{\\text{adj}, 1}$.\n        *   Perform $K$-fold CV to compute $\\text{MSE}_{\\text{CV},1}$.\n    *   For the **\"all-predictors\" model** (using $X_2 = [X_{\\text{signal}}, X_{\\text{noise}}]$):\n        *   Fit the OLS model on the full dataset to get $\\hat{\\beta}_2$.\n        *   Calculate $R^2_2$ and $R^2_{\\text{adj}, 2}$.\n        *   Perform $K$-fold CV to compute $\\text{MSE}_{\\text{CV},2}$.\n5.  **Evaluate Boolean Conditions**:\n    *   Check if $R^2_2 \\ge R^2_1$.\n    *   Check if $R^2_{\\text{adj}, 2}  R^2_{\\text{adj}, 1}$.\n    *   Check if $\\text{MSE}_{\\text{CV}, 2}  \\text{MSE}_{\\text{CV}, 1}$.\n6.  **Store and Report**: Store the three boolean results and format them as required for the final output.", "answer": "```python\nimport numpy as np\n\ndef ols_fit(X, y):\n    \"\"\"\n    Fits an Ordinary Least Squares model with an intercept.\n    \n    Args:\n        X (np.ndarray): Predictor matrix (n_samples, n_features).\n        y (np.ndarray): Response vector (n_samples,).\n        \n    Returns:\n        tuple: (beta_hat, X_aug) where beta_hat are the fitted coefficients\n               (including intercept) and X_aug is the augmented design matrix.\n    \"\"\"\n    n_samples = X.shape[0]\n    X_aug = np.hstack([np.ones((n_samples, 1)), X])\n    \n    try:\n        # More stable than inv(X.T @ X) @ X.T @ y\n        beta_hat = np.linalg.solve(X_aug.T @ X_aug, X_aug.T @ y)\n    except np.linalg.LinAlgError:\n        # Fallback to pseudo-inverse if matrix is singular\n        beta_hat = np.linalg.pinv(X_aug.T @ X_aug) @ X_aug.T @ y\n        \n    return beta_hat, X_aug\n\ndef calculate_metrics(X_aug, y, beta_hat):\n    \"\"\"\n    Calculates R-squared and Adjusted R-squared.\n    \n    Args:\n        X_aug (np.ndarray): Augmented design matrix (with intercept).\n        y (np.ndarray): Response vector.\n        beta_hat (np.ndarray): Fitted coefficients.\n        \n    Returns:\n        tuple: (r_squared, adj_r_squared).\n    \"\"\"\n    n_samples, n_params = X_aug.shape\n    p_predictors = n_params - 1\n    \n    y_hat = X_aug @ beta_hat\n    rss = np.sum((y - y_hat)**2)\n    tss = np.sum((y - np.mean(y))**2)\n    \n    if tss == 0:\n        # Edge case: y is constant. R-squared is not well-defined.\n        # Fits will be perfect if model is just an intercept, otherwise RSS  0.\n        return (1.0 if rss == 0 else 0.0), (1.0 if rss == 0 else 0.0)\n\n    r_squared = 1 - rss / tss\n    \n    # Degrees of freedom for error and total\n    df_err = n_samples - p_predictors - 1\n    df_tot = n_samples - 1\n    \n    if df_err = 0:\n        # If p = n-1, adjusted R^2 can be negative or undefined.\n        # This problem's test cases avoid this.\n        adj_r_squared = r_squared\n    else:\n        adj_r_squared = 1 - (rss / df_err) / (tss / df_tot)\n        \n    return r_squared, adj_r_squared\n\ndef k_fold_cv_mse(X, y, K, folds):\n    \"\"\"\n    Calculates K-fold cross-validation Mean Squared Error.\n    \n    Args:\n        X (np.ndarray): Predictor matrix.\n        y (np.ndarray): Response vector.\n        K (int): Number of folds.\n        folds (list of np.ndarray): List of arrays, each containing indices for a fold.\n\n    Returns:\n        float: The average MSE across all K folds.\n    \"\"\"\n    mse_scores = []\n    n_samples = len(y)\n    \n    for k in range(K):\n        test_indices = folds[k]\n        all_indices = np.arange(n_samples)\n        train_indices = np.setdiff1d(all_indices, test_indices)\n        \n        X_train, y_train = X[train_indices], y[train_indices]\n        X_test, y_test = X[test_indices], y[test_indices]\n        \n        # Fit model on training data\n        beta_hat_train, _ = ols_fit(X_train, y_train)\n        \n        # Predict on test data\n        X_test_aug = np.hstack([np.ones((X_test.shape[0], 1)), X_test])\n        y_pred = X_test_aug @ beta_hat_train\n        \n        # Calculate MSE for the fold\n        mse = np.mean((y_test - y_pred)**2)\n        mse_scores.append(mse)\n        \n    return np.mean(mse_scores)\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases.\n    \"\"\"\n    test_cases = [\n        {'n': 400, 'p_signal': 5, 'q_noise': 40, 'beta_signal': [2.0, -1.0, 1.5, 0.5, -0.8], 'sigma': 1.0, 'K': 5, 'seed': 42},\n        {'n': 120, 'p_signal': 3, 'q_noise': 80, 'beta_signal': [1.8, -0.7, 0.9], 'sigma': 1.2, 'K': 5, 'seed': 7},\n        {'n': 200, 'p_signal': 10, 'q_noise': 20, 'beta_signal': [1.3, -0.6, 0.7, 0.5, -1.1, 0.9, -0.4, 0.3, -0.2, 0.8], 'sigma': 2.0, 'K': 5, 'seed': 99},\n    ]\n\n    all_results = []\n    for case in test_cases:\n        n = case['n']\n        p_signal = case['p_signal']\n        q_noise = case['q_noise']\n        beta_signal = np.array(case['beta_signal'])\n        sigma = case['sigma']\n        K = case['K']\n        seed = case['seed']\n        \n        np.random.seed(seed)\n        \n        # Generate data\n        X_signal = np.random.randn(n, p_signal)\n        X_noise = np.random.randn(n, q_noise)\n        epsilon = np.random.randn(n) * sigma\n        y = X_signal @ beta_signal + epsilon\n        \n        # Define predictor sets\n        X1 = X_signal  # Signal-only\n        X2 = np.hstack([X_signal, X_noise]) # All predictors\n        \n        # --- Model 1: Signal-only ---\n        beta_hat1, X1_aug = ols_fit(X1, y)\n        r2_1, adj_r2_1 = calculate_metrics(X1_aug, y, beta_hat1)\n\n        # --- Model 2: All predictors ---\n        beta_hat2, X2_aug = ols_fit(X2, y)\n        r2_2, adj_r2_2 = calculate_metrics(X2_aug, y, beta_hat2)\n        \n        # --- K-Fold CV ---\n        # Create folds once for fair comparison\n        indices = np.arange(n)\n        np.random.shuffle(indices)\n        folds = np.array_split(indices, K)\n        \n        cv_mse_1 = k_fold_cv_mse(X1, y, K, folds)\n        cv_mse_2 = k_fold_cv_mse(X2, y, K, folds)\n\n        # --- Evaluate boolean statements ---\n        bool1 = r2_2 = r2_1\n        bool2 = adj_r2_2  adj_r2_1\n        bool3 = cv_mse_2  cv_mse_1\n        \n        all_results.append([bool1, bool2, bool3])\n\n    # Format the final output string\n    inner_parts = [f\"[{','.join(map(lambda b: str(b), res))}]\" for res in all_results]\n    final_output = f\"[{','.join(inner_parts)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3152035"}, {"introduction": "While adjusted R-squared is known for penalizing useless predictors, this final practice [@problem_id:3096404] reveals a more subtle and surprising strength: its ability to identify and reward predictors that are valuable only when considered in combination. You will analyze a dataset exhibiting a classic \"suppressor effect,\" where a variable that appears uncorrelated with the outcome on its own dramatically improves the model. This exercise highlights that a predictor's value is not just in its direct relationship with the response, but also in its ability to \"clean up\" the irrelevant noise in other predictors, a nuance that adjusted $R^2$ is designed to capture.", "problem": "A data analyst suspects a suppression effect in a multiple linear regression: two predictors jointly improve the adjusted coefficient of determination substantially, while neither predictor alone appears useful. Consider the following dataset of six observations, with predictors $x_1$ and $x_2$ and response $y$:\n- $x_1 = \\{-5,\\,-3,\\,-1,\\,1,\\,3,\\,5\\}$\n- $x_2 = \\{-4,\\,-2,\\,-2,\\,0,\\,4,\\,6\\}$\n- $y = \\{-1,\\,-1,\\,1,\\,1,\\,-1,\\,-1\\}$\n\nYou will fit ordinary least squares (OLS) regressions with an intercept for three models: $y$ on $x_1$ alone, $y$ on $x_2$ alone, and $y$ on both $x_1$ and $x_2$. Use the standard definitions of sums of squares and degrees of freedom from linear regression with an intercept to compute the adjusted coefficient of determination for each model. Then, explain why two variables that are individually weak can jointly increase the adjusted coefficient of determination via suppression effects and partial correlations, referring to the structure of this dataset.\n\nFinally, report the adjusted coefficient of determination of the joint model with both predictors. Round your answer to four significant figures and express it as a pure number with no units.", "solution": "The user wants to analyze a dataset to understand the concept of a suppression effect in multiple linear regression. This involves fitting three ordinary least squares (OLS) models and comparing their adjusted coefficients of determination ($R^2_{adj}$).\n\nFirst, we establish the necessary definitions for the analysis. Let $n$ be the number of observations and $p$ be the number of predictors in a model. Let $y_i$ be the observed response values, $\\bar{y}$ be the mean of the response values, and $\\hat{y}_i$ be the predicted response values from the OLS model.\n\nThe Total Sum of Squares (SST) is the total variation in the response variable:\n$$SST = \\sum_{i=1}^{n} (y_i - \\bar{y})^2$$\n\nThe Sum of Squared Errors (SSE), also known as the residual sum of squares, is the variation not explained by the model:\n$$SSE = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n\nThe standard coefficient of determination ($R^2$) is the proportion of a variance in the dependent variable that is predictable from the independent variable(s):\n$$R^2 = 1 - \\frac{SSE}{SST}$$\n\nThe adjusted coefficient of determination ($R^2_{adj}$) modifies $R^2$ to account for the number of predictors in the model. It penalizes the score for adding predictors that do not improve the model.\n$$R^2_{adj} = 1 - \\frac{SSE / (n - p - 1)}{SST / (n - 1)} = 1 - (1 - R^2) \\frac{n-1}{n-p-1}$$\nNote that for all models, an intercept term is included, so the number of parameters in the model is $p+1$.\n\nThe given data are:\n$n=6$\n$x_1 = \\{-5,\\,-3,\\,-1,\\,1,\\,3,\\,5\\}$\n$x_2 = \\{-4,\\,-2,\\,-2,\\,0,\\,4,\\,6\\}$\n$y = \\{-1,\\,-1,\\,1,\\,1,\\,-1,\\,-1\\}$\n\nFirst, we calculate the mean of the response variable, $\\bar{y}$, and the Total Sum of Squares, $SST$.\n$$ \\bar{y} = \\frac{1}{6} \\sum_{i=1}^{6} y_i = \\frac{-1 - 1 + 1 + 1 - 1 - 1}{6} = \\frac{-2}{6} = -\\frac{1}{3} $$\n$$ SST = \\sum_{i=1}^{6} (y_i - \\bar{y})^2 = 4 \\times \\left(-1 - \\left(-\\frac{1}{3}\\right)\\right)^2 + 2 \\times \\left(1 - \\left(-\\frac{1}{3}\\right)\\right)^2 $$\n$$ SST = 4 \\times \\left(-\\frac{2}{3}\\right)^2 + 2 \\times \\left(\\frac{4}{3}\\right)^2 = 4 \\times \\frac{4}{9} + 2 \\times \\frac{16}{9} = \\frac{16}{9} + \\frac{32}{9} = \\frac{48}{9} = \\frac{16}{3} $$\n\n**Model 1: Simple linear regression of $y$ on $x_1$**\nThis model has $p=1$ predictor. The model is $y = \\beta_0 + \\beta_1 x_1 + \\epsilon$.\nThe mean of $x_1$ is $\\bar{x}_1 = \\frac{1}{6}(-5 - 3 - 1 + 1 + 3 + 5) = 0$.\nThe OLS estimate for the slope $\\hat{\\beta}_1$ is given by $\\frac{\\sum(x_{1i}-\\bar{x}_1)(y_i-\\bar{y})}{\\sum(x_{1i}-\\bar{x}_1)^2}$.\nNumerator: $\\sum x_{1i} y_i$ since $\\bar{x}_1=0$.\n$\\sum x_{1i} y_i = (-5)(-1) + (-3)(-1) + (-1)(1) + (1)(1) + (3)(-1) + (5)(-1) = 5 + 3 - 1 + 1 - 3 - 5 = 0$.\nSince the numerator is $0$, the slope $\\hat{\\beta}_1 = 0$. The correlation between $x_1$ and $y$ is $0$.\nThe intercept is $\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1\\bar{x}_1 = \\bar{y} = -1/3$.\nThe predicted values are $\\hat{y}_i = -1/3$ for all $i$.\nThus, $SSE_1 = \\sum(y_i - \\hat{y}_i)^2 = \\sum(y_i - \\bar{y})^2 = SST = 16/3$.\nThe $R^2$ for this model is $R^2_1 = 1 - \\frac{SSE_1}{SST} = 1 - 1 = 0$.\nThe adjusted $R^2$ is:\n$$ R^2_{adj,1} = 1 - (1 - R^2_1) \\frac{n-1}{n-p-1} = 1 - (1 - 0) \\frac{6-1}{6-1-1} = 1 - \\frac{5}{4} = -0.25 $$\nA negative $R^2_{adj}$ indicates that the model is worse than a simple horizontal line at the mean.\n\n**Model 2: Simple linear regression of $y$ on $x_2$**\nThis model also has $p=1$ predictor. The model is $y = \\beta_0 + \\beta_1 x_2 + \\epsilon$.\nThe mean of $x_2$ is $\\bar{x}_2 = \\frac{1}{6}(-4-2-2+0+4+6) = \\frac{2}{6} = 1/3$.\nCalculating $R^2_2$: $R^2_2 = (\\text{corr}(x_2, y))^2$.\n$\\sum(x_{2i}-\\bar{x}_2)(y_i-\\bar{y}) = \\sum(x_{2i}y_i) - n\\bar{x}_2\\bar{y} = (-6) - 6(1/3)(-1/3) = -6 + 2/3 = -16/3$.\n$\\sum(x_{2i}-\\bar{x}_2)^2 = \\sum x_{2i}^2 - n\\bar{x}_2^2 = (16+4+4+0+16+36) - 6(1/3)^2 = 76 - 6/9 = 76 - 2/3 = 226/3$.\n$\\sum(y_i-\\bar{y})^2 = SST = 16/3$.\n$r_{x_2,y} = \\frac{-16/3}{\\sqrt{(226/3)(16/3)}} = \\frac{-16/3}{(4/3)\\sqrt{226}} = \\frac{-4}{\\sqrt{226}}$.\n$R^2_2 = r_{x_2,y}^2 = \\frac{16}{226} = \\frac{8}{113} \\approx 0.0708$.\nThis is a very low $R^2$, indicating a weak linear relationship.\nThe adjusted $R^2$ is:\n$$ R^2_{adj,2} = 1 - (1 - R^2_2) \\frac{n-1}{n-p-1} = 1 - \\left(1 - \\frac{8}{113}\\right) \\frac{5}{4} = 1 - \\frac{105}{113} \\frac{5}{4} = 1 - \\frac{525}{452} = -\\frac{73}{452} \\approx -0.1615 $$\nAgain, this model provides no meaningful predictive power.\n\n**Model 3: Multiple linear regression of $y$ on $x_1$ and $x_2$**\nThis model has $p=2$ predictors. The model is $y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon$.\nWe solve for the coefficients using matrix algebra, $\\mathbf{\\hat{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$.\nThe design matrix $\\mathbf{X}$ and response vector $\\mathbf{y}$ are:\n$$ \\mathbf{X} = \\begin{pmatrix} 1  -5  -4 \\\\ 1  -3  -2 \\\\ 1  -1  -2 \\\\ 1  1  0 \\\\ 1  3  4 \\\\ 1  5  6 \\end{pmatrix}, \\quad \\mathbf{y} = \\begin{pmatrix} -1 \\\\ -1 \\\\ 1 \\\\ 1 \\\\ -1 \\\\ -1 \\end{pmatrix} $$\nWe compute $\\mathbf{X}^T\\mathbf{X}$ and $\\mathbf{X}^T\\mathbf{y}$:\n$\\sum 1 = 6$, $\\sum x_1 = 0$, $\\sum x_2 = 2$, $\\sum x_1^2 = 70$, $\\sum x_2^2 = 76$, $\\sum x_1x_2 = 70$.\n$\\sum y = -2$, $\\sum x_1y = 0$, $\\sum x_2y = -6$.\n$$ \\mathbf{X}^T\\mathbf{X} = \\begin{pmatrix} 6  0  2 \\\\ 0  70  70 \\\\ 2  70  76 \\end{pmatrix}, \\quad \\mathbf{X}^T\\mathbf{y} = \\begin{pmatrix} -2 \\\\ 0 \\\\ -6 \\end{pmatrix} $$\nThe vector of estimated coefficients $\\mathbf{\\hat{\\beta}} = [\\hat{\\beta}_0, \\hat{\\beta}_1, \\hat{\\beta}_2]^T$ can be found by solving the normal equations $(\\mathbf{X}^T\\mathbf{X})\\mathbf{\\hat{\\beta}} = \\mathbf{X}^T\\mathbf{y}$.\nNotice from the data that $y_i = x_{1i} - x_{2i}$ for all $i$:\n$i=1: -1 = -5 - (-4)$\n$i=2: -1 = -3 - (-2)$\n$i=3: 1 = -1 - (-2)$\n$i=4: 1 = 1 - 0$\n$i=5: -1 = 3 - 4$\n$i=6: -1 = 5 - 6$\nThe relationship is exact. This implies that a perfect fit is achievable with the model $\\hat{y} = \\hat{\\beta}_1x_1 + \\hat{\\beta}_2x_2$, where $\\hat{\\beta}_0=0$, $\\hat{\\beta}_1=1$, and $\\hat{\\beta}_2=-1$. Let's verify this is the OLS solution:\n$$ (\\mathbf{X}^T\\mathbf{X})\\begin{pmatrix} 0 \\\\ 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 6  0  2 \\\\ 0  70  70 \\\\ 2  70  76 \\end{pmatrix}\\begin{pmatrix} 0 \\\\ 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 0(6)+1(0)-1(2) \\\\ 0(0)+1(70)-1(70) \\\\ 0(2)+1(70)-1(76) \\end{pmatrix} = \\begin{pmatrix} -2 \\\\ 0 \\\\ -6 \\end{pmatrix} = \\mathbf{X}^T\\mathbf{y} $$\nThe solution is confirmed: $\\hat{\\beta}_0 = 0$, $\\hat{\\beta}_1 = 1$, $\\hat{\\beta}_2 = -1$.\nThe fitted model is $\\hat{y} = x_1 - x_2$. Since this relationship is exact, the predicted values $\\hat{y}_i$ are identical to the observed values $y_i$.\nTherefore, the Sum of Squared Errors is $SSE_3 = \\sum (y_i - \\hat{y}_i)^2 = 0$.\nThe $R^2$ for this model is $R^2_3 = 1 - \\frac{SSE_3}{SST} = 1 - \\frac{0}{16/3} = 1$.\nThe adjusted $R^2$ is:\n$$ R^2_{adj,3} = 1 - (1 - R^2_3) \\frac{n-1}{n-p-1} = 1 - (1 - 1) \\frac{6-1}{6-2-1} = 1 - 0 \\times \\frac{5}{3} = 1 $$\n\n**Explanation of the Suppression Effect**\nThe results show a dramatic increase in explanatory power when both predictors are used:\n- $R^2_{adj,1} = -0.25$\n- $R^2_{adj,2} \\approx -0.16$\n- $R^2_{adj,3} = 1.00$\n\nThis phenomenon is a classic example of a suppression effect. A suppressor variable is a predictor that has little to no correlation with the response variable but is correlated with another predictor. Its inclusion in the model \"suppresses\" the irrelevant variance in the other predictor, thereby enhancing the latter's predictive power.\n\nLet's examine the correlations:\n1.  Correlation of predictors with the response: We found $r_{x_1,y} = 0$ and $r_{x_2,y} \\approx -0.266$. Individually, $x_1$ has no linear relationship with $y$, and $x_2$ has a very weak one.\n2.  Correlation between predictors:\n    $\\sum(x_{1i}-\\bar{x}_1)(x_{2i}-\\bar{x}_2) = \\sum x_{1i}(x_{2i}-1/3) = \\sum x_1x_2 - (1/3)\\sum x_1 = 70 - 0 = 70$.\n    $\\sum(x_{1i}-\\bar{x}_1)^2 = 70$.\n    $\\sum(x_{2i}-\\bar{x}_2)^2 = 226/3$.\n    $r_{x_1,x_2} = \\frac{70}{\\sqrt{70 \\times (226/3)}} = \\sqrt{\\frac{70 \\times 3}{226}} = \\sqrt{\\frac{210}{226}} \\approx 0.964$.\n    The predictors $x_1$ and $x_2$ are very highly positively correlated.\n\nHere, $x_1$ acts as a suppressor variable. It is uncorrelated with $y$ but highly correlated with $x_2$. The variable $x_2$ is a mix of two components: one that is useful for predicting $y$ and another (noise) that is not. Because $x_1$ is highly correlated with $x_2$ but not with $y$, $x_1$ must be a proxy for the \"noise\" component within $x_2$.\n\nWhen both variables are included in the regression, the model can use $x_1$ to partial out, or \"suppress,\" the noise from $x_2$. The regression coefficient for $x_1$ (which is $\\hat{\\beta}_1=1$) adjusts the contribution of $x_2$ (with coefficient $\\hat{\\beta}_2=-1$) to remove the shared irrelevant variance. The resulting linear combination $x_1-x_2$ isolates the pure signal that perfectly predicts $y$.\n\nThe partial correlations confirm this. The partial correlation between $y$ and $x_1$ after controlling for $x_2$ is $r_{y,x_1|x_2} = +1$. The partial correlation between $y$ and $x_2$ after controlling for $x_1$ is $r_{y,x_2|x_1} = -1$. Once one predictor is accounted for, the other perfectly explains the remaining variability in $y$. This is the mathematical signature of the suppression effect observed in this deliberately constructed dataset.\n\nThe final answer requested is the adjusted coefficient of determination of the joint model. As calculated, this value is exactly $1$. Rounded to four significant figures, this is $1.000$.", "answer": "$$\n\\boxed{1.000}\n$$", "id": "3096404"}]}