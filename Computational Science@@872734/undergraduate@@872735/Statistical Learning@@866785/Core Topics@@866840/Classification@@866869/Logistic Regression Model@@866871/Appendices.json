{"hands_on_practices": [{"introduction": "Understanding a logistic regression model begins with interpreting its coefficients. Unlike in linear regression, these coefficients act on the log-odds scale, a concept this first exercise is designed to clarify. By calculating the change in log-odds for a given change in a predictor variable, you will practice a foundational skill essential for translating model outputs into meaningful insights [@problem_id:1931449].", "problem": "A research group is studying the factors that influence the success of students in a notoriously difficult professional certification exam. They collect data from a sample of candidates, recording the number of hours they spent studying ($x$) and whether they passed ($Y=1$) or failed ($Y=0$) the exam.\n\nThey fit a simple logistic regression model to predict the probability of passing, $p = P(Y=1|x)$. The model is given by the equation for the log-odds of passing:\n$$ \\ln\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 x $$\nAfter fitting the model to the data, the estimated coefficients are $\\hat{\\beta}_0 = -5.2$ and $\\hat{\\beta}_1 = 0.115$.\n\nA student who has been studying for some amount of time decides to study for an additional 7 hours. Based on the fitted model, what is the resulting change in the log-odds of this student passing the exam? Express your answer as a number rounded to three significant figures.", "solution": "We are given a simple logistic regression model for the log-odds:\n$$\\ln\\left(\\frac{p}{1-p}\\right) = \\beta_{0} + \\beta_{1} x.$$\nIf the study time increases from $x$ to $x + \\Delta x$, the change in the log-odds is\n$$\\Delta \\left[\\ln\\left(\\frac{p}{1-p}\\right)\\right] = \\left[\\beta_{0} + \\beta_{1}(x+\\Delta x)\\right] - \\left[\\beta_{0} + \\beta_{1}x\\right] = \\beta_{1}\\Delta x.$$\nUsing the estimated slope $\\hat{\\beta}_{1} = 0.115$ and $\\Delta x = 7$, the change is\n$$\\Delta \\left[\\ln\\left(\\frac{p}{1-p}\\right)\\right] = \\hat{\\beta}_{1} \\cdot 7 = 0.115 \\cdot 7 = 0.805.$$\nRounded to three significant figures, this is $0.805$.", "answer": "$$\\boxed{0.805}$$", "id": "1931449"}, {"introduction": "Moving beyond individual coefficients, it is crucial to understand the model's overall structure. This practice explores the fundamental symmetry of logistic regression through a thought experiment: what happens to the model if we swap the definitions of 'success' and 'failure'? By determining how this relabeling of the outcome variable $Y$ affects the estimated coefficients $\\boldsymbol{\\beta}$, you will gain a deeper intuition for the log-odds ratio and the core mechanics of the model [@problem_id:1931466].", "problem": "A data scientist is working on a binary classification problem using logistic regression. The dependent variable, $Y$, is coded as $1$ for a \"success\" event and $0$ for a \"failure\" event. The model uses a set of $p$ predictor variables, $X_1, X_2, \\dots, X_p$. The model includes an intercept term. The vector of predictors for an observation is denoted as $\\mathbf{X} = (1, X_1, \\dots, X_p)$, and the corresponding vector of estimated coefficients is $\\beta = (\\beta_0, \\beta_1, \\dots, \\beta_p)^T$. The logistic regression model for the probability of success is given by:\n\n$$P(Y=1|\\mathbf{X}) = \\frac{\\exp(\\mathbf{X}^T\\beta)}{1 + \\exp(\\mathbf{X}^T\\beta)}$$\n\nSuppose that after fitting the model and obtaining the coefficients $\\beta$, the data scientist realizes that the dependent variable was coded incorrectly. The \"success\" and \"failure\" events were swapped. A new variable, $Y'$, is created to correct this, such that $Y' = 1 - Y$. A new logistic regression model is then fitted using $Y'$ as the dependent variable, resulting in a new vector of estimated coefficients, $\\beta' = (\\beta'_0, \\beta'_1, \\dots, \\beta'_p)^T$.\n\nWhat is the relationship between the new coefficient vector $\\beta'$ and the original coefficient vector $\\beta$?\n\nA. $\\beta' = -\\beta$\n\nB. $\\beta'_0 = 1 - \\beta_0$ and $\\beta'_j = -\\beta_j$ for $j \\in \\{1, \\dots, p\\}$\n\nC. $\\beta' = 1 / \\beta$ (element-wise)\n\nD. $\\beta' = \\beta$\n\nE. The relationship cannot be determined without the dataset.", "solution": "We start from the given logistic regression model for the original coding:\n$$\nP(Y=1 \\mid \\mathbf{X}) = \\frac{\\exp(\\mathbf{X}^{T}\\beta)}{1 + \\exp(\\mathbf{X}^{T}\\beta)} \\equiv \\sigma(\\mathbf{X}^{T}\\beta),\n$$\nwhere $\\sigma(z) = \\frac{\\exp(z)}{1 + \\exp(z)}$ is the logistic function. Then\n$$\nP(Y=0 \\mid \\mathbf{X}) = 1 - \\sigma(\\mathbf{X}^{T}\\beta).\n$$\n\nAfter recoding the response as $Y' = 1 - Y$, the success probability under the original model becomes\n$$\nP(Y'=1 \\mid \\mathbf{X}) = P(Y=0 \\mid \\mathbf{X}) = 1 - \\sigma(\\mathbf{X}^{T}\\beta).\n$$\nUsing the identity\n$$\n1 - \\sigma(z) = \\sigma(-z),\n$$\nwe obtain\n$$\nP(Y'=1 \\mid \\mathbf{X}) = \\sigma(-\\mathbf{X}^{T}\\beta) = \\frac{\\exp(-\\mathbf{X}^{T}\\beta)}{1 + \\exp(-\\mathbf{X}^{T}\\beta)}.\n$$\nThis shows that the model for $Y'$ has linear predictor $-\\mathbf{X}^{T}\\beta$, i.e., the coefficient vector is the negation of the original:\n$$\n\\beta' = -\\beta,\n$$\nwhich in particular implies $\\beta'_{0} = -\\beta_{0}$ and $\\beta'_{j} = -\\beta_{j}$ for $j \\in \\{1,\\dots,p\\}$.\n\nEquivalently, in log-odds form, the original model satisfies\n$$\n\\ln\\!\\left(\\frac{P(Y=1 \\mid \\mathbf{X})}{1 - P(Y=1 \\mid \\mathbf{X})}\\right) = \\mathbf{X}^{T}\\beta.\n$$\nFor $Y'$, the log-odds are\n$$\n\\ln\\!\\left(\\frac{P(Y'=1 \\mid \\mathbf{X})}{1 - P(Y'=1 \\mid \\mathbf{X})}\\right)\n= \\ln\\!\\left(\\frac{1 - P(Y=1 \\mid \\mathbf{X})}{P(Y=1 \\mid \\mathbf{X})}\\right)\n= - \\ln\\!\\left(\\frac{P(Y=1 \\mid \\mathbf{X})}{1 - P(Y=1 \\mid \\mathbf{X})}\\right)\n= -\\mathbf{X}^{T}\\beta,\n$$\nagain implying $\\beta' = -\\beta$.\n\nOne can also verify this via the likelihood. Let $y_{i} \\in \\{0,1\\}$ and $\\eta_{i} = \\mathbf{X}_{i}^{T}\\beta$. The likelihood for $Y$ is\n$$\nL(\\beta) = \\prod_{i} \\sigma(\\eta_{i})^{y_{i}} \\left[1 - \\sigma(\\eta_{i})\\right]^{1 - y_{i}}.\n$$\nFor $Y' = 1 - y_{i}$ with the candidate $\\beta' = -\\beta$, we have\n$$\nL'(\\beta') = \\prod_{i} \\sigma(-\\eta_{i})^{1 - y_{i}} \\left[1 - \\sigma(-\\eta_{i})\\right]^{y_{i}}\n= \\prod_{i} \\left[1 - \\sigma(\\eta_{i})\\right]^{1 - y_{i}} \\sigma(\\eta_{i})^{y_{i}} = L(\\beta),\n$$\nso any maximizer $\\beta$ for the original problem maps to a maximizer $\\beta' = -\\beta$ for the relabeled problem. Therefore the correct relationship is $\\beta' = -\\beta$.\n\nHence, the correct option is A.", "answer": "$$\\boxed{A}$$", "id": "1931466"}, {"introduction": "To truly master logistic regression, one must bridge the gap between theory and practice. This comprehensive exercise challenges you to build a logistic regression solver from the ground up, starting from the maximum likelihood principle and implementing the Newton-Raphson algorithm. You will then use your implementation to compute and compare two different pseudo-$R^2$ measures, Tjur’s coefficient of discrimination and McFadden’s pseudo-$R^2$, gaining invaluable insight into how these models are trained and evaluated [@problem_id:3142117].", "problem": "You are asked to implement, from first principles, a binary logistic regression model and to compare two measures of explained variance: Tjur’s coefficient of discrimination and McFadden’s pseudo-coefficient of determination. Begin from the following fundamental base in statistical learning:\n- The binary outcomes are modeled as conditionally independent Bernoulli random variables with success probabilities $p_i \\in (0,1)$.\n- The logistic regression model specifies the link between covariates and success probability via the logistic function, so that $p_i = \\sigma(\\eta_i)$ with $\\sigma(t) = \\frac{1}{1 + e^{-t}}$ and linear predictor $\\eta_i = \\mathbf{x}_i^{\\top}\\boldsymbol{\\beta}$, where $\\boldsymbol{\\beta}$ is an unknown parameter vector including an intercept.\n- The Maximum Likelihood Estimation (MLE) principle selects $\\boldsymbol{\\beta}$ that maximizes the log-likelihood implied by the Bernoulli model and the logistic link.\n\nTasks:\n1. Starting from the Bernoulli likelihood and the logistic link definition (no other pre-derived formulas), derive the log-likelihood in terms of $\\boldsymbol{\\beta}$ and show how to obtain a Newton–Raphson iteration to compute the MLE $\\widehat{\\boldsymbol{\\beta}}$. Then, implement a Newton–Raphson solver that:\n   - Adds an intercept column of ones to the design matrix by default.\n   - Iterates until the Euclidean norm of the parameter increment is less than a tolerance $10^{-12}$ or until $100$ iterations are reached.\n2. After finding $\\widehat{\\boldsymbol{\\beta}}$, compute the fitted probabilities $\\widehat{p}_i = \\sigma(\\mathbf{x}_i^{\\top}\\widehat{\\boldsymbol{\\beta}})$ for all observations.\n3. From first principles, define Tjur’s coefficient of discrimination as the difference between the mean of $\\widehat{p}_i$ among all cases with $y_i = 1$ and the mean of $\\widehat{p}_i$ among all cases with $y_i = 0$. Also, from first principles, define McFadden’s pseudo-coefficient of determination using the maximized log-likelihoods of the full model and of the null model (intercept only), and compute it using natural logarithms.\n4. Use the following test suite, each case providing a design matrix consisting of a single predictor (the program must automatically add the intercept) and a binary response vector. For each case, compute both Tjur’s and McFadden’s measures.\n   - Case A (no apparent signal): predictor values $x$ and labels $y$\n     - $x = [0,0,0,0,0,0,1,1,1,1,1,1]$\n     - $y = [0,1,0,1,0,1,0,1,0,1,0,1]$\n   - Case B (moderate signal): predictor values $x$ and labels $y$\n     $$x = [\\underbrace{0,\\dots,0}_{10\\ \\text{times}},\\underbrace{1,\\dots,1}_{10\\ \\text{times}}]$$\n     - $y = [1,0,0,1,0,0,0,0,0,0,1,1,1,1,1,1,1,1,0,0]$\n   - Case C (stronger but not perfect signal): predictor values $x$ and labels $y$\n     $$x = [\\underbrace{0,\\dots,0}_{12\\ \\text{times}},\\underbrace{1,\\dots,1}_{8\\ \\text{times}}]$$\n     - $y = [1,0,0,0,0,0,0,0,0,1,0,1,1,1,1,1,1,1,1,0]$\n5. Final output format: Your program should produce a single line containing a comma-separated list enclosed in square brackets. The list must contain, in order, Tjur’s and McFadden’s measures for Case A, then Case B, then Case C, all as floating-point values rounded to six decimal places, i.e.,\n   - Output order: $[\\text{Tjur}_A,\\text{McFadden}_A,\\text{Tjur}_B,\\text{McFadden}_B,\\text{Tjur}_C,\\text{McFadden}_C]$.\n   - Example format: `[0.123456,0.234567,0.345678,0.456789,0.567890,0.678901]`.\n\nThere are no physical quantities or angles in this problem, so no units are required. All numerical answers should be printed as decimal floats rounded to six digits after the decimal point in the specified order.", "solution": "The problem statement is evaluated to be valid. It is scientifically grounded in the principles of statistical learning, well-posed, objective, and internally consistent. It provides all necessary information to derive and implement a binary logistic regression model, compute the specified measures of explained variance, and apply them to the provided test cases. The task is a standard, non-trivial exercise in computational statistics. We may proceed with the solution.\n\nThe solution is structured as follows: First, we derive the mathematical foundations for the logistic regression model, including the log-likelihood function and the Newton-Raphson optimization algorithm. Second, we define the two measures of explained variance, Tjur’s coefficient of discrimination and McFadden’s pseudo-coefficient of determination. Finally, we describe the implementation which applies these principles to the test cases.\n\n**1. Derivation of the Logistic Regression Model and its MLE**\n\nA binary logistic regression model is used to model a binary outcome variable $y_i \\in \\{0, 1\\}$ for $i=1, \\dots, N$ observations, based on a vector of predictors $\\mathbf{x}_i \\in \\mathbb{R}^k$.\n\n**Log-Likelihood Function**\nEach outcome $y_i$ is modeled as a realization of an independent Bernoulli random variable $Y_i \\sim \\text{Bernoulli}(p_i)$, where $p_i = P(Y_i=1 | \\mathbf{x}_i)$. The probability mass function is $P(Y_i=y_i) = p_i^{y_i}(1-p_i)^{1-y_i}$. The logistic regression model connects the probability $p_i$ to the predictors $\\mathbf{x}_i$ via the logit link function. The log-odds, or logit, of the probability is modeled as a linear function of the predictors:\n$$ \\text{logit}(p_i) = \\log\\left(\\frac{p_i}{1-p_i}\\right) = \\mathbf{x}_i^{\\top}\\boldsymbol{\\beta} $$\nwhere $\\boldsymbol{\\beta} \\in \\mathbb{R}^k$ is the vector of model parameters. The linear part $\\eta_i = \\mathbf{x}_i^{\\top}\\boldsymbol{\\beta}$ is called the linear predictor.\nBy inverting the logit function, we obtain the probability $p_i$ as the sigmoid function $\\sigma(\\cdot)$ applied to the linear predictor:\n$$ p_i = \\sigma(\\eta_i) = \\sigma(\\mathbf{x}_i^{\\top}\\boldsymbol{\\beta}) = \\frac{1}{1 + e^{-\\mathbf{x}_i^{\\top}\\boldsymbol{\\beta}}} $$\nGiven the independence of the observations, the likelihood of the entire dataset $(\\mathbf{y}, \\mathbf{X})$, where $\\mathbf{y} = (y_1, \\dots, y_N)^{\\top}$ and $\\mathbf{X}$ is the design matrix with rows $\\mathbf{x}_i^{\\top}$, is the product of the individual probabilities:\n$$ L(\\boldsymbol{\\beta}) = \\prod_{i=1}^N p_i^{y_i} (1-p_i)^{1-y_i} $$\nIt is computationally more convenient to work with the log-likelihood, $\\ell(\\boldsymbol{\\beta}) = \\log L(\\boldsymbol{\\beta})$:\n$$ \\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^N \\left[ y_i \\log p_i + (1-y_i) \\log(1-p_i) \\right] $$\nWe can express this directly in terms of $\\boldsymbol{\\beta}$ by using the relationships $\\log\\left(\\frac{p_i}{1-p_i}\\right) = \\mathbf{x}_i^{\\top}\\boldsymbol{\\beta}$ and $\\log(1-p_i) = -\\log(1+e^{\\mathbf{x}_i^{\\top}\\boldsymbol{\\beta}})$.\n$$ \\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^N \\left[ y_i(\\mathbf{x}_i^{\\top}\\boldsymbol{\\beta}) - \\log(1+e^{\\mathbf{x}_i^{\\top}\\boldsymbol{\\beta}}) \\right] $$\nThis is the log-likelihood function to be maximized to find the Maximum Likelihood Estimate (MLE), $\\widehat{\\boldsymbol{\\beta}}$.\n\n**Newton–Raphson Method for Maximization**\nThe MLE $\\widehat{\\boldsymbol{\\beta}}$ is found by solving $\\nabla \\ell(\\boldsymbol{\\beta}) = \\mathbf{0}$. Since this equation is nonlinear, we use an iterative numerical method, Newton-Raphson. The update rule for maximizing $\\ell(\\boldsymbol{\\beta})$ is:\n$$ \\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} - \\left[ \\mathbf{H}(\\boldsymbol{\\beta}^{(t)}) \\right]^{-1} \\nabla \\ell(\\boldsymbol{\\beta}^{(t)}) $$\nwhere $\\nabla \\ell(\\boldsymbol{\\beta})$ is the gradient vector of the log-likelihood and $\\mathbf{H}(\\boldsymbol{\\beta})$ is its Hessian matrix.\n\n**Gradient of the Log-Likelihood**\nThe gradient $\\nabla \\ell(\\boldsymbol{\\beta})$ is a vector of partial derivatives $\\frac{\\partial \\ell}{\\partial \\beta_j}$ for $j=1, \\dots, k$.\n$$ \\frac{\\partial \\ell}{\\partial \\beta_j} = \\frac{\\partial}{\\partial \\beta_j} \\sum_{i=1}^N \\left[ y_i(\\mathbf{x}_i^{\\top}\\boldsymbol{\\beta}) - \\log(1+e^{\\mathbf{x}_i^{\\top}\\boldsymbol{\\beta}}) \\right] = \\sum_{i=1}^N \\left[ y_i x_{ij} - \\frac{\\partial}{\\partial \\beta_j} \\log(1+e^{\\mathbf{x}_i^{\\top}\\boldsymbol{\\beta}}) \\right] $$\nUsing the chain rule, $\\frac{\\partial}{\\partial \\beta_j} \\log(1+e^{\\mathbf{x}_i^{\\top}\\boldsymbol{\\beta}}) = \\frac{e^{\\mathbf{x}_i^{\\top}\\boldsymbol{\\beta}}}{1+e^{\\mathbf{x}_i^{\\top}\\boldsymbol{\\beta}}} \\cdot x_{ij} = p_i x_{ij}$.\n$$ \\frac{\\partial \\ell}{\\partial \\beta_j} = \\sum_{i=1}^N (y_i - p_i) x_{ij} $$\nIn matrix form, with $\\mathbf{p}$ being the vector of probabilities $p_i$, the gradient is:\n$$ \\nabla \\ell(\\boldsymbol{\\beta}) = \\mathbf{X}^{\\top}(\\mathbf{y} - \\mathbf{p}) $$\n\n**Hessian of the Log-Likelihood**\nThe Hessian matrix $\\mathbf{H}$ has elements $H_{jl} = \\frac{\\partial^2 \\ell}{\\partial \\beta_l \\partial \\beta_j}$.\n$$ H_{jl} = \\frac{\\partial}{\\partial \\beta_l} \\sum_{i=1}^N (y_i - p_i) x_{ij} = \\sum_{i=1}^N -x_{ij} \\frac{\\partial p_i}{\\partial \\beta_l} $$\nWe need $\\frac{\\partial p_i}{\\partial \\beta_l} = \\frac{d\\sigma(\\eta_i)}{d\\eta_i} \\frac{\\partial \\eta_i}{\\partial \\beta_l}$. The derivative of the sigmoid function is $\\sigma'(\\eta) = \\sigma(\\eta)(1-\\sigma(\\eta)) = p_i(1-p_i)$.\nThus, $\\frac{\\partial p_i}{\\partial \\beta_l} = p_i(1-p_i)x_{il}$. Substituting this back:\n$$ H_{jl} = - \\sum_{i=1}^N x_{ij} x_{il} p_i(1-p_i) $$\nIn matrix form, let $\\mathbf{W}$ be a diagonal matrix with diagonal entries $W_{ii} = p_i(1-p_i)$. The Hessian is:\n$$ \\mathbf{H}(\\boldsymbol{\\beta}) = -\\mathbf{X}^{\\top}\\mathbf{W}\\mathbf{X} $$\nThe Newton-Raphson update becomes:\n$$ \\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} - (-\\mathbf{X}^{\\top}\\mathbf{W}^{(t)}\\mathbf{X})^{-1} (\\mathbf{X}^{\\top}(\\mathbf{y} - \\mathbf{p}^{(t)})) = \\boldsymbol{\\beta}^{(t)} + (\\mathbf{X}^{\\top}\\mathbf{W}^{(t)}\\mathbf{X})^{-1} \\mathbf{X}^{\\top}(\\mathbf{y} - \\mathbf{p}^{(t)}) $$\nThis is the update rule for the Iteratively Reweighted Least Squares (IRLS) algorithm.\n\n**2. Measures of Explained Variance**\n\n**Tjur’s Coefficient of Discrimination ($D$)**\nThis coefficient measures the separation in the distributions of fitted probabilities for the two outcome classes. From first principles, it is the difference between the average fitted probability for observations with $y_i=1$ and the average fitted probability for observations with $y_i=0$.\n$$ D = \\mathbb{E}[\\widehat{p} | Y=1] - \\mathbb{E}[\\widehat{p} | Y=0] $$\nThe sample estimate $\\widehat{D}$ is calculated from the fitted probabilities $\\widehat{p}_i = \\sigma(\\mathbf{x}_i^{\\top}\\widehat{\\boldsymbol{\\beta}})$:\n$$ \\widehat{D} = \\frac{\\sum_{i=1}^N y_i \\widehat{p}_i}{\\sum_{i=1}^N y_i} - \\frac{\\sum_{i=1}^N (1-y_i) \\widehat{p}_i}{\\sum_{i=1}^N (1-y_i)} $$\nA value of $D$ near $1$ indicates perfect separation, while a value near $0$ indicates no separation.\n\n**McFadden’s Pseudo-R-squared ($R^2_{\\text{McF}}$)**\nThis measure is analogous to the coefficient of determination ($R^2$) in linear regression. It compares the log-likelihood of the fitted model, $\\ell_M = \\ell(\\widehat{\\boldsymbol{\\beta}})$, to the log-likelihood of a null model with only an intercept, $\\ell_0 = \\ell(\\widehat{\\boldsymbol{\\beta}}_0)$. The null model represents a baseline where predictors have no effect, and the probability of a success is constant, estimated by the sample mean $\\bar{y}$.\nThe log-likelihood, being a measure of model fit, is always non-positive. A perfect fit would yield a log-likelihood of $0$. McFadden's $R^2$ is defined as:\n$$ R^2_{\\text{McF}} = 1 - \\frac{\\ell_M}{\\ell_0} $$\nThis value is interpreted as the proportional improvement in model fit provided by the predictors over the null model. It ranges from $0$ (no improvement) to a theoretical maximum of less than $1$.\n\n**3. Implementation Strategy**\n\nThe implementation will consist of a Python script.\n1.  A function, `newton_raphson_solver`, will be created to find the MLE $\\widehat{\\boldsymbol{\\beta}}$ for a given design matrix $\\mathbf{X}$ and response vector $\\mathbf{y}$. It will iteratively apply the Newton-Raphson update rule, starting with $\\boldsymbol{\\beta}=\\mathbf{0}$, until the Euclidean norm of the parameter increment is below $10^{-12}$ or $100$ iterations are reached.\n2.  For each test case, we will construct a \"full\" design matrix by taking the given predictor vector $x$ and prepending a column of ones for the intercept.\n3.  We will also construct a \"null\" design matrix, which is simply a column of ones, to fit the intercept-only model.\n4.  Both the full and null models will be fit using the `newton_raphson_solver` to obtain $\\widehat{\\boldsymbol{\\beta}}_{\\text{full}}$ and $\\widehat{\\boldsymbol{\\beta}}_{\\text{null}}$.\n5.  Fitted probabilities $\\widehat{\\mathbf{p}}$ for the full model will be computed. Tjur's $D$ will be calculated from these probabilities.\n6.  The log-likelihoods for both the maximized full model ($\\ell_M$) and the maximized null model ($\\ell_0$) will be computed.\n7.  McFadden's $R^2$ will be calculated using the formula $1 - \\ell_M / \\ell_0$.\n8.  This process will be repeated for all three test cases, and the results will be collected and printed in the specified format. The `scipy.special.expit` function will be used for a numerically stable sigmoid implementation.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import expit\n\ndef solve():\n    \"\"\"\n    Main function to run the logistic regression analysis for all test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (\n            np.array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]),\n            np.array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1])\n        ),\n        (\n            np.array([0]*10 + [1]*10),\n            np.array([1,0,0,1,0,0,0,0,0,0, 1,1,1,1,1,1,1,1,0,0])\n        ),\n        (\n            np.array([0]*12 + [1]*8),\n            np.array([1,0,0,0,0,0,0,0,0,1,0,1, 1,1,1,1,1,1,1,0])\n        )\n    ]\n\n    results = []\n    for x, y in test_cases:\n        tjur_d, mcfadden_r2 = compute_metrics(x, y)\n        results.append(round(tjur_d, 6))\n        results.append(round(mcfadden_r2, 6))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef newton_raphson_solver(X, y, tol=1e-12, max_iter=100):\n    \"\"\"\n    Finds the MLE for logistic regression parameters using Newton-Raphson.\n\n    Args:\n        X (np.ndarray): Design matrix of shape (n_samples, n_features).\n        y (np.ndarray): Response vector of shape (n_samples,).\n        tol (float): Convergence tolerance for the parameter increment norm.\n        max_iter (int): Maximum number of iterations.\n\n    Returns:\n        np.ndarray: The estimated parameter vector beta.\n    \"\"\"\n    # Initialize beta to zeros\n    beta = np.zeros(X.shape[1])\n    \n    for _ in range(max_iter):\n        # Linear predictor\n        eta = X @ beta\n        \n        # Probabilities using a numerically stable sigmoid (expit)\n        p = expit(eta)\n        \n        # Diagonal weight matrix W with weights w_i = p_i * (1 - p_i)\n        # Add a small epsilon for numerical stability in case p is 0 or 1\n        w = p * (1 - p) + 1e-15\n        W = np.diag(w)\n        \n        # Gradient of the log-likelihood\n        gradient = X.T @ (y - p)\n        \n        # Hessian of the log-likelihood\n        hessian = -X.T @ W @ X\n        \n        # Newton-Raphson step\n        try:\n            step = -np.linalg.inv(hessian) @ gradient\n        except np.linalg.LinAlgError:\n            # In case of singularity, break and return current best estimate.\n            # This can happen with perfect separation, but not in test cases.\n            break\n\n        # Update beta\n        beta = beta + step\n        \n        # Check for convergence\n        if np.linalg.norm(step) < tol:\n            break\n            \n    return beta\n\ndef calculate_log_likelihood(p, y):\n    \"\"\"\n    Calculates the log-likelihood of a Bernoulli model.\n\n    Args:\n        p (np.ndarray): Vector of success probabilities.\n        y (np.ndarray): Vector of binary outcomes.\n\n    Returns:\n        float: The total log-likelihood.\n    \"\"\"\n    # Add a small epsilon to prevent log(0)\n    eps = 1e-15\n    p_clipped = np.clip(p, eps, 1 - eps)\n    return np.sum(y * np.log(p_clipped) + (1 - y) * np.log(1 - p_clipped))\n\ndef compute_metrics(x, y):\n    \"\"\"\n    Computes Tjur's D and McFadden's R^2 for a given dataset.\n\n    Args:\n        x (np.ndarray): Single-predictor vector.\n        y (np.ndarray): Response vector.\n\n    Returns:\n        tuple: A tuple containing (tjur_d, mcfadden_r2).\n    \"\"\"\n    # 1. Prepare design matrices for full and null models\n    X_full = np.c_[np.ones(x.shape[0]), x]\n    X_null = np.ones((y.shape[0], 1))\n    \n    # 2. Fit both models to get parameter estimates\n    beta_full = newton_raphson_solver(X_full, y)\n    beta_null = newton_raphson_solver(X_null, y)\n    \n    # 3. Compute Tjur's coefficient of discrimination\n    p_hat_full = expit(X_full @ beta_full)\n    \n    p_if_y1 = p_hat_full[y == 1]\n    p_if_y0 = p_hat_full[y == 0]\n    \n    mean_p1 = np.mean(p_if_y1) if len(p_if_y1) > 0 else 0\n    mean_p0 = np.mean(p_if_y0) if len(p_if_y0) > 0 else 0\n    tjur_d = mean_p1 - mean_p0\n\n    # 4. Compute McFadden's pseudo-R-squared\n    # Log-likelihood of the full model\n    ll_full = calculate_log_likelihood(p_hat_full, y)\n    \n    # Log-likelihood of the null model\n    p_hat_null = expit(X_null @ beta_null)\n    ll_null = calculate_log_likelihood(p_hat_null, y)\n    \n    # Handle the case where ll_null is zero to avoid division by zero\n    if ll_null == 0:\n        mcfadden_r2 = 1.0 if ll_full == 0 else 0.0\n    else:\n        mcfadden_r2 = 1 - (ll_full / ll_null)\n        \n    return tjur_d, mcfadden_r2\n\nif __name__ == '__main__':\n    solve()\n\n```", "id": "3142117"}]}