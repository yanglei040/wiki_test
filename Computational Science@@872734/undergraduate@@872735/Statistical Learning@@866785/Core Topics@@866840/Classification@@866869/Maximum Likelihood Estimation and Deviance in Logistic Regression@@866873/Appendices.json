{"hands_on_practices": [{"introduction": "While the total deviance gives us a single number to assess overall model fit, its true diagnostic power is unlocked when we decompose it into individual contributions from each data point. This practice guides you through the derivation of these pointwise deviance terms directly from the log-likelihood, showing how each observation's 'surprise' to the model is quantified. By identifying points with the largest deviance, you will learn a fundamental technique for diagnosing model deficiencies and identifying influential observations. [@problem_id:3147524]", "problem": "Consider a binary-response logistic regression model fit by Maximum Likelihood Estimation (MLE). Let the observed responses be $\\{y_{i}\\}_{i=1}^{n}$ with $y_{i}\\in\\{0,1\\}$ and let the fitted probabilities be $\\{\\hat{p}_{i}\\}_{i=1}^{n}$, where each $y_{i}$ is modeled as conditionally independent $\\operatorname{Bernoulli}(\\hat{p}_{i})$. Begin from the definition of the Bernoulli log-likelihood and the definition of the deviance as twice the difference between the saturated-model log-likelihood and the fitted-model log-likelihood. Using only these definitions, derive an expression that shows the deviance is a sum of observation-specific terms. Then, using your derived expression, compute the total deviance for the following eight observations:\n$$\n\\begin{aligned}\ny_{1}=1,\\ \\hat{p}_{1}=0.84;\\quad y_{2}=0,\\ \\hat{p}_{2}=0.15;\\quad y_{3}=1,\\ \\hat{p}_{3}=0.62;\\\\\ny_{4}=0,\\ \\hat{p}_{4}=0.80;\\quad y_{5}=1,\\ \\hat{p}_{5}=0.30;\\quad y_{6}=0,\\ \\hat{p}_{6}=0.25;\\\\\ny_{7}=1,\\ \\hat{p}_{7}=0.95;\\quad y_{8}=0,\\ \\hat{p}_{8}=0.55.\n\\end{aligned}\n$$\nFinally, design a diagnostic rule that flags observations with the largest pointwise deviance contributions, ensuring that at most $k=2$ observations are flagged, and apply it to the data above to identify which indices would be flagged. Round your final numerical value for the total deviance to four significant figures. No physical units are involved, and the final numerical answer must be a single real number.", "solution": "We begin by deriving the general expression for the deviance in a binary logistic regression model. The problem states that the deviance $D$ is defined as twice the difference between the log-likelihood of the saturated model, $\\ell_{sat}$, and the log-likelihood of the fitted model, $\\ell_{fit}$:\n$$\nD = 2(\\ell_{sat} - \\ell_{fit})\n$$\nThe observations $\\{y_i\\}_{i=1}^n$ are modeled as independent Bernoulli trials, where $y_i \\in \\{0, 1\\}$. The probability mass function for a single observation $Y_i$ with parameter $p_i$ is $P(Y_i=y_i) = p_i^{y_i}(1-p_i)^{1-y_i}$. The log-likelihood for a single observation is therefore $\\ln(P(Y_i=y_i)) = y_i \\ln(p_i) + (1-y_i) \\ln(1-p_i)$.\n\nFirst, we determine the log-likelihood of the fitted model, $\\ell_{fit}$. This is obtained by summing the individual log-likelihoods, using the fitted probabilities $\\{\\hat{p}_i\\}_{i=1}^n$:\n$$\n\\ell_{fit} = \\sum_{i=1}^{n} \\left[ y_i \\ln(\\hat{p}_i) + (1 - y_i) \\ln(1 - \\hat{p}_i) \\right]\n$$\nNext, we determine the log-likelihood of the saturated model, $\\ell_{sat}$. A saturated model is one that fits the data perfectly. For a Bernoulli observation $y_i$, a perfect fit is achieved if the model's probability equals the outcome. That is, the probability parameter for the saturated model, $p_i^{sat}$, is equal to $y_i$.\n$$\np_i^{sat} = y_i\n$$\nThe log-likelihood of the saturated model is thus:\n$$\n\\ell_{sat} = \\sum_{i=1}^{n} \\left[ y_i \\ln(p_i^{sat}) + (1 - y_i) \\ln(1 - p_i^{sat}) \\right] = \\sum_{i=1}^{n} \\left[ y_i \\ln(y_i) + (1 - y_i) \\ln(1 - y_i) \\right]\n$$\nWe must evaluate the terms in this sum.\n- If $y_i = 1$, the term is $1 \\ln(1) + (1-1) \\ln(1-1) = \\ln(1) + 0 \\ln(0) = 0$.\n- If $y_i = 0$, the term is $0 \\ln(0) + (1-0) \\ln(1-0) = 0 \\ln(0) + \\ln(1) = 0$.\nIn both cases, we use the fact that $\\lim_{x\\to 0^+} x \\ln(x) = 0$. Thus, each term in the summation for $\\ell_{sat}$ is zero, which means the total log-likelihood for the saturated model is zero:\n$$\n\\ell_{sat} = 0\n$$\nSubstituting the expressions for $\\ell_{sat}$ and $\\ell_{fit}$ into the definition of deviance gives:\n$$\nD = 2 \\left( 0 - \\sum_{i=1}^{n} \\left[ y_i \\ln(\\hat{p}_i) + (1-y_i) \\ln(1-\\hat{p}_i) \\right] \\right)\n$$\n$$\nD = -2 \\sum_{i=1}^{n} \\left[ y_i \\ln(\\hat{p}_i) + (1 - y_i) \\ln(1 - \\hat{p}_i) \\right]\n$$\nThis expression shows the deviance as a sum of observation-specific terms. The contribution of the $i$-th observation to the total deviance, which we can call the pointwise deviance $d_i$, is:\n$$\nd_i = -2 \\left[ y_i \\ln(\\hat{p}_i) + (1 - y_i) \\ln(1 - \\hat{p}_i) \\right]\n$$\nThis expression simplifies based on the value of $y_i$:\n- If $y_i=1$, then $d_i = -2 \\ln(\\hat{p}_i)$.\n- If $y_i=0$, then $d_i = -2 \\ln(1-\\hat{p}_i)$.\n\nNow, we compute the total deviance for the given $n=8$ observations by calculating each $d_i$ and summing them.\n$d_1$: For $y_1=1, \\hat{p}_1=0.84$, $d_1 = -2 \\ln(0.84) \\approx 0.34870$\n$d_2$: For $y_2=0, \\hat{p}_2=0.15$, $d_2 = -2 \\ln(1-0.15) = -2 \\ln(0.85) \\approx 0.32504$\n$d_3$: For $y_3=1, \\hat{p}_3=0.62$, $d_3 = -2 \\ln(0.62) \\approx 0.95607$\n$d_4$: For $y_4=0, \\hat{p}_4=0.80$, $d_4 = -2 \\ln(1-0.80) = -2 \\ln(0.20) \\approx 3.21888$\n$d_5$: For $y_5=1, \\hat{p}_5=0.30$, $d_5 = -2 \\ln(0.30) \\approx 2.40795$\n$d_6$: For $y_6=0, \\hat{p}_6=0.25$, $d_6 = -2 \\ln(1-0.25) = -2 \\ln(0.75) \\approx 0.57536$\n$d_7$: For $y_7=1, \\hat{p}_7=0.95$, $d_7 = -2 \\ln(0.95) \\approx 0.10258$\n$d_8$: For $y_8=0, \\hat{p}_8=0.55$, $d_8 = -2 \\ln(1-0.55) = -2 \\ln(0.45) \\approx 1.59702$\n\nThe total deviance is the sum $D = \\sum_{i=1}^{8} d_i$:\n$$\nD \\approx 0.34870 + 0.32504 + 0.95607 + 3.21888 + 2.40795 + 0.57536 + 0.10258 + 1.59702\n$$\n$$\nD \\approx 9.53160\n$$\nRounding to four significant figures, the total deviance is $9.532$.\n\nFinally, we design and apply the diagnostic rule. The rule is to flag observations with the largest pointwise deviance contributions, with at most $k=2$ observations flagged. This requires ranking the individual deviance contributions, $d_i$, in descending order and selecting the top two.\nThe calculated pointwise deviances are:\n$d_4 \\approx 3.21888$\n$d_5 \\approx 2.40795$\n$d_8 \\approx 1.59702$\n$d_3 \\approx 0.95607$\n$d_6 \\approx 0.57536$\n$d_1 \\approx 0.34870$\n$d_2 \\approx 0.32504$\n$d_7 \\approx 0.10258$\n\nThe two largest values are $d_4$ and $d_5$. These correspond to observations with indices $4$ and $5$. Therefore, the diagnostic rule would flag observations $y_4$ and $y_5$. These large deviance contributions arise because the model assigned a high probability of success ($\\hat{p}_4=0.80$) to an observed failure ($y_4=0$), and a low probability of success ($\\hat{p}_5=0.30$) to an observed success ($y_5=1$).\n\nThe final numerical answer required is the total deviance rounded to four significant figures.", "answer": "$$\n\\boxed{9.532}\n$$", "id": "3147524"}, {"introduction": "One of the most powerful applications of deviance is in comparing different models to see which one better explains the data. In this exercise, you will confront a classic statistical pitfall known as Simpson's paradox, where a trend that appears in separate groups of data disappears or reverses when these groups are combined. By comparing the deviance of a simple pooled model to that of a more complex subgroup-specific model, you will see firsthand how deviance can be used to formally justify the need for a more nuanced model and uncover hidden confounding effects. [@problem_id:3147557]", "problem": "You are given a binary-outcome classification setting modeled by logistic regression. The outcome variable is $y \\in \\{0,1\\}$, the predictor is a binary treatment indicator $x \\in \\{0,1\\}$, and there is a binary subgroup indicator $g \\in \\{A,B\\}$. The statistical learning objective is to compare a pooled logistic regression model that ignores subgroup membership with a pair of subgroup-specific logistic regression models fit independently within each subgroup. The comparison is performed using the deviance under Maximum Likelihood Estimation (MLE), starting from the definition of the Bernoulli likelihood and the logistic link. Your task is to derive, implement, and compute the following, from first principles, without relying on pre-derived shortcut formulas:\n\n- Implement MLE for logistic regression by directly optimizing the Bernoulli log-likelihood under the logistic link, using a principled iterative method that follows from differentiating the log-likelihood with respect to the parameters.\n- Define and compute the deviance of a fitted logistic regression model from the foundational definition where the saturated model assigns $y_i$ exactly to each observation, and the fitted model uses the estimated probabilities from MLE.\n- For a pooled model, fit a single logistic regression to the entire dataset using only an intercept and the single predictor $x$ (i.e., subgroup $g$ is ignored).\n- For the subgroup-specific model, fit two separate logistic regressions, one within subgroup $A$ and one within subgroup $B$, each using an intercept and the single predictor $x$. The overall subgroup-specific log-likelihood is the sum of the two subgroup log-likelihoods, and the corresponding deviance is computed from this combined log-likelihood.\n- Compute the deviance drop, defined as the pooled-model deviance minus the subgroup-specific deviance. A positive deviance drop indicates that the subgroup-specific modeling yields a lower deviance (better fit) than the pooled modeling.\n\nUse the following test suite of datasets, expressed as counts to be expanded into individual observations. Each dataset specifies, for each subgroup $g \\in \\{A,B\\}$ and treatment level $x \\in \\{0,1\\}$, the total number of observations and the number of successes. You must construct the individual binary outcomes $y$ consistent with these counts, and the corresponding $x$ and $g$ labels. The datasets are designed to probe the presence or absence of Simpson’s paradox and to exercise both typical and edge-case behavior.\n\n- Test case $1$ (strong Simpson’s paradox: treatment helpful within subgroups but harmful when pooled due to confounding by subgroup):\n  - Subgroup $A$: \n    - $x=0$: total $80$, successes $56$.\n    - $x=1$: total $20$, successes $16$.\n  - Subgroup $B$:\n    - $x=0$: total $20$, successes $2$.\n    - $x=1$: total $80$, successes $16$.\n- Test case $2$ (balanced assignment, treatment helpful both within subgroups and in aggregate):\n  - Subgroup $A$:\n    - $x=0$: total $50$, successes $35$.\n    - $x=1$: total $50$, successes $40$.\n  - Subgroup $B$:\n    - $x=0$: total $50$, successes $5$.\n    - $x=1$: total $50$, successes $10$.\n- Test case $3$ (edge case: no treatment effect and no subgroup differences; pooled and subgroup-specific fits should be similar):\n  - Subgroup $A$:\n    - $x=0$: total $40$, successes $20$.\n    - $x=1$: total $40$, successes $20$.\n  - Subgroup $B$:\n    - $x=0$: total $40$, successes $20$.\n    - $x=1$: total $40$, successes $20$.\n\nYour program must:\n- For each test case, build the individual-level dataset consistent with the specified counts.\n- Fit the pooled logistic regression model with parameters $(\\beta_0, \\beta_1)$, where $\\beta_0$ is the intercept and $\\beta_1$ multiplies $x$.\n- Fit two subgroup-specific logistic regression models:\n  - For subgroup $A$, fit $(\\beta_{0A}, \\beta_{1A})$ using only the data where $g=A$.\n  - For subgroup $B$, fit $(\\beta_{0B}, \\beta_{1B})$ using only the data where $g=B$.\n- Compute the pooled deviance and the subgroup-specific deviance, each defined from the Bernoulli likelihood and the saturated model, and report the deviance drop (pooled deviance minus subgroup-specific deviance) for each case.\n\nFinal output format:\n- Your program should produce a single line of output containing the deviance drops for the three test cases as a comma-separated list enclosed in square brackets, for example $[d_1,d_2,d_3]$, where each $d_i$ is a floating-point number rounded to $6$ decimal places. No other text should be printed.\n\nAll quantities in this problem are pure numbers without physical units, and no angles are involved. Express all numerical answers as decimals (not percentages). Ensure that your implementation is grounded in the foundational definitions of the Bernoulli likelihood and logistic link, and that the iterative optimization procedure is derived from these foundations rather than using pre-packaged shortcuts.", "solution": "### Solution Derivation\n\nThe solution requires implementing logistic regression from first principles. This involves maximizing the log-likelihood function to find the MLE of the model parameters and then using these parameters to compute the model deviance.\n\n**1. Logistic Regression Model and Likelihood**\n\nFor an individual observation $i$, with outcome $y_i \\in \\{0, 1\\}$ and a vector of predictors $\\mathbf{x}_i$ (which includes a $1$ for the intercept), the logistic regression model assumes the outcome follows a Bernoulli distribution, $y_i \\sim \\text{Bernoulli}(p_i)$. The probability of success $p_i$ is related to a linear combination of the predictors, $\\eta_i = \\mathbf{x}_i^T \\boldsymbol{\\beta}$, via the logit link function's inverse (the logistic or sigmoid function):\n$$ p_i = P(y_i=1 | \\mathbf{x}_i, \\boldsymbol{\\beta}) = \\frac{e^{\\eta_i}}{1 + e^{\\eta_i}} = \\frac{1}{1 + e^{-\\eta_i}} $$\nThe likelihood for a single observation is given by the Bernoulli probability mass function:\n$$ L_i(\\boldsymbol{\\beta}) = p_i^{y_i} (1 - p_i)^{1 - y_i} $$\nFor a dataset of $N$ independent observations, the total likelihood is the product of individual likelihoods, $L(\\boldsymbol{\\beta}) = \\prod_{i=1}^N L_i(\\boldsymbol{\\beta})$. It is computationally more convenient to work with the log-likelihood, $\\ell(\\boldsymbol{\\beta})$:\n$$ \\ell(\\boldsymbol{\\beta}) = \\log(L(\\boldsymbol{\\beta})) = \\sum_{i=1}^N \\log(L_i(\\boldsymbol{\\beta})) = \\sum_{i=1}^N [y_i \\log(p_i) + (1-y_i) \\log(1-p_i)] $$\nSubstituting the expression for $p_i$ in terms of $\\eta_i$:\n$$ \\log(p_i) = \\eta_i - \\log(1 + e^{\\eta_i}) $$\n$$ \\log(1-p_i) = -\\log(1 + e^{\\eta_i}) $$\nThe log-likelihood becomes:\n$$ \\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^N [y_i \\eta_i - \\log(1 + e^{\\eta_i})] = \\sum_{i=1}^N [y_i (\\mathbf{x}_i^T \\boldsymbol{\\beta}) - \\log(1 + e^{\\mathbf{x}_i^T \\boldsymbol{\\beta}})] $$\n\n**2. Maximum Likelihood Estimation via Newton-Raphson**\n\nTo find the MLE parameters $\\hat{\\boldsymbol{\\beta}}$, we must maximize $\\ell(\\boldsymbol{\\beta})$. This is achieved by finding the roots of its gradient, $\\nabla \\ell(\\boldsymbol{\\beta}) = \\mathbf{0}$. We use the Newton-Raphson method, an iterative algorithm that requires the gradient (score vector) and the Hessian matrix (matrix of second derivatives) of the log-likelihood.\n\nThe gradient with respect to a parameter $\\beta_j$ is:\n$$ \\frac{\\partial \\ell}{\\partial \\beta_j} = \\sum_{i=1}^N \\left[ y_i x_{ij} - \\frac{e^{\\eta_i}}{1 + e^{\\eta_i}} x_{ij} \\right] = \\sum_{i=1}^N (y_i - p_i) x_{ij} $$\nIn matrix form, with $\\mathbf{X}$ as the design matrix, $\\mathbf{y}$ as the outcome vector, and $\\mathbf{p}$ as the vector of probabilities, the gradient is:\n$$ \\nabla \\ell(\\boldsymbol{\\beta}) = \\mathbf{X}^T (\\mathbf{y} - \\mathbf{p}) $$\nThe Hessian matrix $H$ has elements $H_{jk} = \\frac{\\partial^2 \\ell}{\\partial \\beta_j \\partial \\beta_k}$:\n$$ \\frac{\\partial^2 \\ell}{\\partial \\beta_j \\partial \\beta_k} = \\frac{\\partial}{\\partial \\beta_k} \\sum_{i=1}^N (y_i - p_i) x_{ij} = \\sum_{i=1}^N - \\frac{\\partial p_i}{\\partial \\beta_k} x_{ij} $$\nUsing the chain rule, $\\frac{\\partial p_i}{\\partial \\beta_k} = \\frac{\\partial p_i}{\\partial \\eta_i} \\frac{\\partial \\eta_i}{\\partial \\beta_k}$. Since $\\frac{\\partial p_i}{\\partial \\eta_i} = p_i(1-p_i)$ and $\\frac{\\partial \\eta_i}{\\partial \\beta_k} = x_{ik}$, we get:\n$$ H_{jk} = \\sum_{i=1}^N -p_i(1-p_i) x_{ij} x_{ik} $$\nIn matrix form, this is $H(\\boldsymbol{\\beta}) = -\\mathbf{X}^T \\mathbf{W} \\mathbf{X}$, where $\\mathbf{W}$ is a diagonal matrix with diagonal elements $W_{ii} = p_i(1-p_i)$.\n\nThe Newton-Raphson update rule for iteration $t+1$ is:\n$$ \\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} - [H(\\boldsymbol{\\beta}^{(t)})]^{-1} \\nabla \\ell(\\boldsymbol{\\beta}^{(t)}) $$\n$$ \\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} + (\\mathbf{X}^T \\mathbf{W}^{(t)} \\mathbf{X})^{-1} \\mathbf{X}^T (\\mathbf{y} - \\mathbf{p}^{(t)}) $$\nThis algorithm is also known as Iteratively Reweighted Least Squares (IRLS). We initialize $\\boldsymbol{\\beta}$ (e.g., to zeros) and iterate until convergence.\n\n**3. Deviance Calculation**\n\nThe deviance of a model is $D = 2(\\ell_{sat} - \\ell_{fit})$.\n- $\\ell_{fit}$ is the log-likelihood of the fitted model, $\\ell(\\hat{\\boldsymbol{\\beta}})$, calculated using the MLE parameters.\n- $\\ell_{sat}$ is the log-likelihood of the saturated model. A saturated model has one parameter per observation, allowing it to fit the data perfectly. For a Bernoulli outcome $y_i$, the saturated model predicts probability $\\hat{p}_{i,sat} = y_i$. The log-likelihood for observation $i$ is $y_i \\log(\\hat{p}_{i,sat}) + (1-y_i) \\log(1 - \\hat{p}_{i,sat})$. If $y_i=1$, this term is $1 \\log(1) + 0 \\log(0) = 0$. If $y_i=0$, the term is $0 \\log(0) + 1 \\log(1) = 0$. (The convention $0 \\log(0) = 0$ is used). Thus, the log-likelihood contribution from each observation is $0$, and the total log-likelihood for the saturated model is $\\ell_{sat} = 0$.\n\nTherefore, the deviance simplifies to:\n$$ D = -2 \\ell_{fit} = -2 \\sum_{i=1}^N [y_i \\log(\\hat{p}_i) + (1-y_i) \\log(1-\\hat{p}_i)] $$\nwhere $\\hat{p}_i$ are the probabilities predicted by the fitted model.\n\n**4. Model Comparison**\n\nWe fit the two specified models:\n- **Pooled Model**: A single logistic regression on the entire dataset. The design matrix $\\mathbf{X}$ has two columns: an intercept (all 1s) and the treatment indicator $x$. We compute $\\hat{\\boldsymbol{\\beta}}_{pooled}$ and the corresponding deviance, $D_{pooled}$.\n- **Subgroup-Specific Model**: We perform two independent fits.\n    1. For subgroup $A$, using only data where $g=A$, we fit a model to get $\\hat{\\boldsymbol{\\beta}}_A$ and its deviance $D_A$.\n    2. For subgroup $B$, using only data where $g=B$, we fit a model to get $\\hat{\\boldsymbol{\\beta}}_B$ and its deviance $D_B$.\nThe total log-likelihood for the subgroup-specific approach is $\\ell_{subgroup} = \\ell_A + \\ell_B$. The total deviance is $D_{subgroup} = D_A + D_B$.\n\nThe deviance drop is $D_{pooled} - D_{subgroup}$. This value is the likelihood-ratio test statistic for comparing the two nested models. A positive value indicates that the more complex subgroup-specific model provides a better fit to the data than the simpler pooled model.", "answer": "```python\nimport numpy as np\n\n# A small epsilon for numerical stability in log calculations and matrix inversion\nEPSILON = 1e-15\n\ndef sigmoid(eta):\n    \"\"\"Computes the sigmoid function.\"\"\"\n    # Clip eta to avoid overflow in exp\n    eta = np.clip(eta, -500, 500)\n    return 1.0 / (1.0 + np.exp(-eta))\n\ndef construct_dataset(counts):\n    \"\"\"\n    Expands dataset from counts into individual observations.\n    \n    Args:\n        counts (list): A list of tuples, where each tuple is\n                       (subgroup_char, x_val, total, successes).\n\n    Returns:\n        tuple: A tuple of numpy arrays (y, x, g).\n    \"\"\"\n    y_list, x_list, g_list = [], [], []\n    g_map = {'A': 0, 'B': 1}\n    for g_char, x_val, total, successes in counts:\n        failures = total - successes\n        # Add successes\n        y_list.extend([1] * successes)\n        x_list.extend([x_val] * successes)\n        g_list.extend([g_map[g_char]] * successes)\n        # Add failures\n        y_list.extend([0] * failures)\n        x_list.extend([x_val] * failures)\n        g_list.extend([g_map[g_char]] * failures)\n    return np.array(y_list), np.array(x_list), np.array(g_list)\n\ndef fit_logistic_regression(X, y, tol=1e-8, max_iter=30):\n    \"\"\"\n    Fits a logistic regression model using Newton-Raphson (IRLS).\n\n    Args:\n        X (np.ndarray): Design matrix (N_samples, N_features), with intercept.\n        y (np.ndarray): Binary outcome vector (N_samples,).\n        tol (float): Convergence tolerance for the norm of the step vector.\n        max_iter (int): Maximum number of iterations.\n\n    Returns:\n        np.ndarray: The fitted coefficient vector beta.\n    \"\"\"\n    # Initialize beta coefficients to zero\n    beta = np.zeros(X.shape[1])\n    \n    for _ in range(max_iter):\n        eta = X @ beta\n        p = sigmoid(eta)\n        \n        # Diagonal of the weight matrix W\n        W_diag = p * (1 - p)\n        # Ensure weights are not exactly zero to avoid singular matrix\n        W_diag = np.maximum(W_diag, EPSILON)\n        \n        # Gradient of the log-likelihood\n        grad = X.T @ (y - p)\n        \n        # Hessian of the log-likelihood is -X.T @ W @ X\n        # We compute J = -H, the Fisher Information Matrix\n        # (W_diag[:, np.newaxis] * X) performs row-wise multiplication (efficiently)\n        J = X.T @ (W_diag[:, np.newaxis] * X)\n        \n        try:\n            # Solve J * step = grad for the step vector\n            # This is more numerically stable than computing the inverse of J\n            step = np.linalg.solve(J, grad)\n        except np.linalg.LinAlgError:\n            # This may happen with (quasi-)complete separation, but the data\n            # given in the problem does not have this issue.\n            break\n\n        beta = beta + step\n        \n        if np.linalg.norm(step)  tol:\n            break\n            \n    return beta\n\ndef calculate_deviance(y, X, beta):\n    \"\"\"\n    Calculates the deviance of a fitted logistic regression model.\n\n    Args:\n        y (np.ndarray): True outcome vector.\n        X (np.ndarray): Design matrix.\n        beta (np.ndarray): Fitted coefficient vector.\n\n    Returns:\n        float: The deviance of the model.\n    \"\"\"\n    if y.size == 0:\n        return 0.0\n\n    eta = X @ beta\n    p_hat = sigmoid(eta)\n    \n    # Clip probabilities to avoid log(0)\n    p_hat = np.clip(p_hat, EPSILON, 1 - EPSILON)\n    \n    # Log-likelihood of the fitted model\n    log_likelihood = np.sum(y * np.log(p_hat) + (1 - y) * np.log(1 - p_hat))\n    \n    # Deviance = -2 * log_likelihood (since log_likelihood of saturated model is 0\n    # for individual Bernoulli trials)\n    return -2.0 * log_likelihood\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis for all test cases and print the result.\n    \"\"\"\n    test_cases = [\n        # Test case 1 (strong Simpson's paradox)\n        [('A', 0, 80, 56), ('A', 1, 20, 16), ('B', 0, 20, 2), ('B', 1, 80, 16)],\n        # Test case 2 (balanced assignment, consistent effect)\n        [('A', 0, 50, 35), ('A', 1, 50, 40), ('B', 0, 50, 5), ('B', 1, 50, 10)],\n        # Test case 3 (edge case: no effect)\n        [('A', 0, 40, 20), ('A', 1, 40, 20), ('B', 0, 40, 20), ('B', 1, 40, 20)],\n    ]\n    \n    results = []\n    \n    for case_counts in test_cases:\n        y, x, g = construct_dataset(case_counts)\n        \n        # Design matrix with intercept\n        X = np.c_[np.ones(x.shape[0]), x]\n        \n        # 1. Pooled Model\n        beta_pooled = fit_logistic_regression(X, y)\n        deviance_pooled = calculate_deviance(y, X, beta_pooled)\n        \n        # 2. Subgroup-specific Model\n        \n        # Subgroup A\n        mask_A = (g == 0)\n        y_A, X_A = y[mask_A], X[mask_A]\n        beta_A = fit_logistic_regression(X_A, y_A)\n        deviance_A = calculate_deviance(y_A, X_A, beta_A)\n\n        # Subgroup B\n        mask_B = (g == 1)\n        y_B, X_B = y[mask_B], X[mask_B]\n        beta_B = fit_logistic_regression(X_B, y_B)\n        deviance_B = calculate_deviance(y_B, X_B, beta_B)\n\n        deviance_subgroup = deviance_A + deviance_B\n        \n        # 3. Deviance Drop\n        deviance_drop = deviance_pooled - deviance_subgroup\n        results.append(round(deviance_drop, 6))\n\n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3147557"}, {"introduction": "A good classification model can mean different things: is it one that correctly ranks positive cases above negative ones, or one that produces probabilities that reflect the true underlying risk? This practice delves into this crucial distinction by contrasting a model's discrimination, measured by the ROC-AUC, with its probabilistic calibration, assessed by deviance. You will construct a scenario where a model has excellent discriminative power (high AUC) but is poorly calibrated (high deviance), learning why understanding both aspects is essential for responsibly deploying statistical models. [@problem_id:3147549]", "problem": "You are given the task of contrasting threshold-free discrimination versus probabilistic calibration using a binary classification setting with logistic regression. Work from the following fundamental base: a binary response $y_i \\in \\{0,1\\}$ is modeled with a logistic regression $p_i = \\Pr(y_i = 1 \\mid s_i) = \\sigma(\\beta_0 + \\beta_1 s_i)$, where $\\sigma(t) = \\dfrac{1}{1 + e^{-t}}$ is the logistic function. The joint log-likelihood for independent observations is $\\ell(\\beta_0,\\beta_1) = \\sum_{i=1}^n \\left[y_i \\log p_i + (1 - y_i) \\log (1 - p_i)\\right]$. The deviance is defined as $D = -2 \\, \\ell(\\beta_0,\\beta_1)$ evaluated at a specified set of predicted probabilities. The Receiver Operating Characteristic - Area Under the Curve (ROC-AUC) can be expressed in terms of ranks as $A = \\dfrac{\\sum_{i:y_i=1} R_i - \\frac{n_1(n_1+1)}{2}}{n_1 n_0}$, where $R_i$ is the rank (averaging ties) of $s_i$, $n_1$ is the number of positive responses and $n_0$ is the number of negative responses. Note that ROC-AUC is invariant to any strictly increasing transformation of the scores $s_i$, whereas deviance depends on the calibrated probabilities $p_i$.\n\nYou will use the following deterministic datasets. Each dataset is specified by two lists: one for scores with label $y=0$ and one for scores with label $y=1$. Concatenate them to form the full dataset $(s_i, y_i)$ with $y_i=0$ for the first list and $y_i=1$ for the second list, preserving the given order within each list. No physical units are involved.\n\nDataset $\\mathcal{D}_1$:\n- Scores with $y=0$: $[-2.5,\\,-2.1,\\,-1.8,\\,-1.6,\\,-1.4,\\,-1.2,\\,-1.0,\\,-0.9,\\,-0.7,\\,-0.5,\\,-0.4,\\,-0.2,\\,0.0,\\,0.2,\\,0.5,\\,0.7,\\,0.9,\\,1.1,\\,1.3,\\,1.5]$.\n- Scores with $y=1$: $[-0.3,\\,-0.1,\\,0.1,\\,0.3,\\,0.6,\\,0.8,\\,1.0,\\,1.2,\\,1.4,\\,1.6,\\,1.7,\\,1.9,\\,2.1,\\,2.2,\\,2.4,\\,2.6,\\,2.8,\\,3.0,\\,3.1,\\,3.3]$.\n\nDataset $\\mathcal{D}_2$:\n- Scores with $y=0$: $[-3.5,\\,-3.0,\\,-2.8,\\,-2.6,\\,-2.4,\\,-2.2,\\,-2.0,\\,-1.8,\\,-1.6,\\,-1.4,\\,-1.2,\\,-1.0,\\,-0.8,\\,-0.6,\\,-0.4,\\,2.0]$.\n- Scores with $y=1$: $[-0.2,\\,0.0,\\,0.2,\\,0.4,\\,0.6,\\,0.8,\\,1.0,\\,1.2,\\,1.4,\\,1.6,\\,1.8,\\,2.2,\\,2.4,\\,2.6,\\,3.0,\\,3.5]$.\n\nFor each test case, you must compute the following three quantities:\n- The ROC-AUC $A$ computed on the raw scores $s_i$.\n- The deviance $D_{\\text{mis}}$ of a deliberately miscalibrated model that predicts $p_i^{(\\gamma)} = \\sigma(\\gamma \\, s_i)$ with a fixed slope parameter $\\gamma$ and no intercept (i.e., $\\beta_0 = 0$ and $\\beta_1 = \\gamma$ are imposed, not estimated).\n- The deviance $D_{\\text{MLE}}$ at the maximum likelihood estimate $(\\hat{\\beta}_0,\\hat{\\beta}_1)$ for the logistic regression model $p_i = \\sigma(\\beta_0 + \\beta_1 s_i)$, where $(\\hat{\\beta}_0,\\hat{\\beta}_1)$ maximize $\\ell(\\beta_0,\\beta_1)$ over $\\mathbb{R}^2$.\n\nImplement the following, starting only from the above definitions and facts:\n- Compute $A$ using ranks of $s_i$ with average handling of ties.\n- Compute deviances using the stable identity $-\\,\\ell(\\beta_0,\\beta_1) = \\sum_{i=1}^n \\left[\\log(1 + e^{\\eta_i}) - y_i \\eta_i\\right]$ where $\\eta_i = \\beta_0 + \\beta_1 s_i$, and $D = 2 \\sum_{i=1}^n \\left[\\log(1 + e^{\\eta_i}) - y_i \\eta_i\\right]$.\n- Obtain $(\\hat{\\beta}_0,\\hat{\\beta}_1)$ by maximizing the log-likelihood (equivalently, minimizing the negative log-likelihood) with respect to $(\\beta_0,\\beta_1)$; any correct numerical method may be used.\n\nDesign a test suite that probes discrimination versus calibration by pairing datasets with several $\\gamma$ values:\n- Case $1$: $(\\mathcal{D}_1, \\gamma = 1.0)$.\n- Case $2$: $(\\mathcal{D}_1, \\gamma = 5.0)$.\n- Case $3$: $(\\mathcal{D}_1, \\gamma = 0.5)$.\n- Case $4$: $(\\mathcal{D}_2, \\gamma = 5.0)$.\n- Case $5$: $(\\mathcal{D}_2, \\gamma = 1.0)$.\n\nYour program must produce a single line of output containing the results aggregated for all test cases as a comma-separated list enclosed in square brackets, in the following order:\n$[A_1, D_{\\text{mis},1}, D_{\\text{MLE},1}, A_2, D_{\\text{mis},2}, D_{\\text{MLE},2}, \\dots, A_5, D_{\\text{mis},5}, D_{\\text{MLE},5}]$.\nExpress all numbers as decimal floating-point values rounded to exactly $6$ decimal places. No other text should be printed.", "solution": "The problem requires the computation of three metrics for several test cases to contrast discrimination (measured by ROC-AUC) and probabilistic calibration (measured by deviance) in the context of logistic regression. The three metrics are the Receiver Operating Characteristic - Area Under the Curve ($A$), the deviance of a deliberately miscalibrated model ($D_{\\text{mis}}$), and the deviance at the maximum likelihood estimate ($D_{\\text{MLE}}$).\n\nThe analysis proceeds in three main steps for each dataset: first, the calculation of the ROC-AUC; second, the calculation of deviances for specified models; and third, the determination of the maximum likelihood estimates and their associated deviance.\n\nThe binary response $y_i \\in \\{0, 1\\}$ is modeled using a logistic function $\\sigma(t) = \\frac{1}{1 + e^{-t}}$ applied to a linear predictor $\\eta_i$. The probability of a positive class is $p_i = \\Pr(y_i=1 \\mid s_i) = \\sigma(\\eta_i)$.\n\n**1. ROC-AUC Calculation**\n\nThe ROC-AUC, a measure of a model's ability to discriminate between positive and negative classes, is calculated using the rank-based formula provided:\n$$A = \\frac{\\sum_{i:y_i=1} R_i - \\frac{n_1(n_1+1)}{2}}{n_1 n_0}$$\nHere, $n_1$ is the count of observations with $y_i=1$, $n_0$ is the count of observations with $y_i=0$, and $R_i$ is the rank of the score $s_i$ among all scores. When scores are tied, their ranks are averaged. Since the AUC calculation depends only on the rank ordering of the scores $s_i$, it is invariant under any strictly increasing monotonic transformation. Therefore, for a given dataset, the value of $A$ will be the same for all test cases associated with it.\n\n**2. Deviance Calculation**\n\nThe deviance is a measure of goodness of fit, related to the log-likelihood $\\ell$. The joint log-likelihood for $n$ independent observations is:\n$$\\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^n \\left[y_i \\log p_i + (1 - y_i) \\log (1 - p_i)\\right]$$\nwhere $\\boldsymbol{\\beta}$ represents the model parameters. Let $\\eta_i$ be the linear predictor for observation $i$. The probabilities are $p_i = \\sigma(\\eta_i) = \\frac{1}{1+e^{-\\eta_i}}$. The corresponding log-probabilities can be written in a numerically stable form.\nNoting that $\\log p_i = \\log\\left(\\frac{e^{\\eta_i}}{1+e^{\\eta_i}}\\right) = \\eta_i - \\log(1+e^{\\eta_i})$ and $\\log(1-p_i) = \\log\\left(\\frac{1}{1+e^{\\eta_i}}\\right) = -\\log(1+e^{\\eta_i})$, the per-observation log-likelihood term becomes:\n$$y_i \\log p_i + (1-y_i) \\log(1-p_i) = y_i(\\eta_i - \\log(1+e^{\\eta_i})) + (1-y_i)(-\\log(1+e^{\\eta_i})) = y_i\\eta_i - \\log(1+e^{\\eta_i})$$\nThe total log-likelihood is $\\ell = \\sum_{i=1}^n [y_i\\eta_i - \\log(1+e^{\\eta_i})]$.\nThe deviance is defined as $D = -2\\ell$. The negative log-likelihood (NLL), which is minimized in model fitting, is:\n$$NLL = -\\ell = \\sum_{i=1}^n [\\log(1+e^{\\eta_i}) - y_i\\eta_i]$$\nThus, the deviance is $D = 2 \\times NLL = 2 \\sum_{i=1}^n [\\log(1+e^{\\eta_i}) - y_i\\eta_i]$. This confirms the stable identity provided in the problem. The term $\\log(1+e^{\\eta_i})$ requires a numerically stable implementation to prevent overflow when $\\eta_i$ is large.\n\n**2a. Miscalibrated Deviance ($D_{\\text{mis}}$)**\n\nFor the miscalibrated model, the parameters are fixed as $(\\beta_0, \\beta_1) = (0, \\gamma)$, so the linear predictor is $\\eta_i = \\gamma s_i$. The deviance $D_{\\text{mis}}$ is calculated by substituting this $\\eta_i$ directly into the deviance formula for the specified value of $\\gamma$ in each test case.\n\n**2b. Deviance at the MLE ($D_{\\text{MLE}}$)**\n\nTo calculate $D_{\\text{MLE}}$, we must first find the maximum likelihood estimates $(\\hat{\\beta}_0, \\hat{\\beta}_1)$ for the full logistic regression model $p_i = \\sigma(\\beta_0 + \\beta_1 s_i)$. This is achieved by minimizing the negative log-likelihood function with respect to the parameters $(\\beta_0, \\beta_1)$:\n$$(\\hat{\\beta}_0, \\hat{\\beta}_1) = \\underset{(\\beta_0, \\beta_1) \\in \\mathbb{R}^2}{\\arg\\min} \\sum_{i=1}^n \\left[\\log(1+e^{\\beta_0+\\beta_1 s_i}) - y_i(\\beta_0+\\beta_1 s_i)\\right]$$\nSince the NLL for logistic regression is a convex function, a unique minimum exists (provided the data is not perfectly separable, which is true for the given datasets). This minimization problem is solved using a numerical optimization algorithm, such as the Broyden–Fletcher–Goldfarb–Shanno (BFGS) method. The minimum value of the NLL, denoted $NLL_{min}$, is the result of this optimization. The deviance at the MLE is then $D_{\\text{MLE}} = 2 \\times NLL_{min}$. Like the ROC-AUC, $D_{\\text{MLE}}$ is a property of the dataset and the model form, so it is constant for all test cases using the same dataset.\n\n**3. Computational Strategy**\n\nFor each of the two datasets, $\\mathcal{D}_1$ and $\\mathcal{D}_2$:\n1.  The full data arrays for scores $s$ and labels $y$ are constructed.\n2.  The ROC-AUC, $A$, is computed once using the rank-based formula.\n3.  The maximum likelihood estimates $(\\hat{\\beta}_0, \\hat{\\beta}_1)$ are found by numerically minimizing the NLL function. The resulting minimum NLL is used to compute $D_{\\text{MLE}}$.\n4.  For each test case $(\\mathcal{D}, \\gamma)$, the miscalibrated deviance $D_{\\text{mis}}$ is computed using the fixed linear predictor $\\eta_i = \\gamma s_i$.\n5.  The three computed values, $A$, $D_{\\text{mis}}$, and $D_{\\text{MLE}}$, are collected for each test case in the specified order and formatted.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import rankdata\nfrom scipy.optimize import minimize\nfrom scipy.special import logsumexp\n\ndef solve():\n    \"\"\"\n    Computes ROC-AUC and deviance for logistic regression models\n    to contrast discrimination versus calibration.\n    \"\"\"\n\n    # Define datasets\n    datasets = {\n        \"D1\": {\n            \"s0\": [-2.5, -2.1, -1.8, -1.6, -1.4, -1.2, -1.0, -0.9, -0.7, -0.5, -0.4, -0.2, 0.0, 0.2, 0.5, 0.7, 0.9, 1.1, 1.3, 1.5],\n            \"s1\": [-0.3, -0.1, 0.1, 0.3, 0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.7, 1.9, 2.1, 2.2, 2.4, 2.6, 2.8, 3.0, 3.1, 3.3]\n        },\n        \"D2\": {\n            \"s0\": [-3.5, -3.0, -2.8, -2.6, -2.4, -2.2, -2.0, -1.8, -1.6, -1.4, -1.2, -1.0, -0.8, -0.6, -0.4, 2.0],\n            \"s1\": [-0.2, 0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.2, 2.4, 2.6, 3.0, 3.5]\n        }\n    }\n\n    # Define test cases\n    test_cases = [\n        (\"D1\", 1.0),\n        (\"D1\", 5.0),\n        (\"D1\", 0.5),\n        (\"D2\", 5.0),\n        (\"D2\", 1.0),\n    ]\n\n    def calculate_auc(s, y):\n        \"\"\"Computes ROC-AUC using the rank-based formula.\"\"\"\n        n1 = np.sum(y == 1)\n        n0 = len(y) - n1\n        ranks = rankdata(s, method='average')\n        sum_ranks_positives = np.sum(ranks[y == 1])\n        auc = (sum_ranks_positives - n1 * (n1 + 1) / 2) / (n1 * n0)\n        return auc\n\n    def neg_log_likelihood(beta, s, y):\n        \"\"\"Computes the negative log-likelihood for logistic regression.\"\"\"\n        eta = beta[0] + beta[1] * s\n        # Stable computation of log(1 + exp(eta))\n        log_1_plus_exp_eta = logsumexp(np.vstack([np.zeros_like(eta), eta]), axis=0)\n        nll = np.sum(log_1_plus_exp_eta - y * eta)\n        return nll\n\n    def calculate_deviance(eta, y):\n        \"\"\"Computes deviance from linear predictors.\"\"\"\n        log_1_plus_exp_eta = logsumexp(np.vstack([np.zeros_like(eta), eta]), axis=0)\n        nll = np.sum(log_1_plus_exp_eta - y * eta)\n        return 2 * nll\n\n    # Cache for dataset-specific calculations (A and D_MLE)\n    dataset_metrics = {}\n\n    for d_name, d_data in datasets.items():\n        s0 = np.array(d_data[\"s0\"])\n        s1 = np.array(d_data[\"s1\"])\n        \n        s = np.concatenate((s0, s1))\n        y = np.concatenate((np.zeros_like(s0), np.ones_like(s1)))\n\n        # Calculate ROC-AUC (A)\n        auc = calculate_auc(s, y)\n        \n        # Calculate Deviance at MLE (D_MLE)\n        initial_beta = np.array([0.0, 1.0])\n        opt_result = minimize(\n            neg_log_likelihood, \n            initial_beta, \n            args=(s, y), \n            method='BFGS'\n        )\n        d_mle = 2 * opt_result.fun\n        \n        dataset_metrics[d_name] = {\"A\": auc, \"D_MLE\": d_mle, \"s\": s, \"y\": y}\n\n    results = []\n    for d_name, gamma in test_cases:\n        metrics = dataset_metrics[d_name]\n        s = metrics[\"s\"]\n        y = metrics[\"y\"]\n        \n        # Retrieve pre-computed A and D_MLE\n        A = metrics[\"A\"]\n        D_MLE = metrics[\"D_MLE\"]\n\n        # Calculate Miscalibrated Deviance (D_mis)\n        eta_mis = gamma * s\n        D_mis = calculate_deviance(eta_mis, y)\n        \n        results.extend([A, D_mis, D_MLE])\n\n    # Format the final output string\n    formatted_results = [f\"{val:.6f}\" for val in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3147549"}]}