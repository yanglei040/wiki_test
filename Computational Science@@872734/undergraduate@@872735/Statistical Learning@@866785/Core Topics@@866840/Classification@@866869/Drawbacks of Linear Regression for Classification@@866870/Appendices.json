{"hands_on_practices": [{"introduction": "Linear regression's goal to minimize squared error makes it notoriously sensitive to outliers, or \"high-leverage points.\" This practice [@problem_id:3117088] guides you through a coding exercise to witness this flaw firsthand, a phenomenon known as \"masking.\" You will see how even when OLS achieves perfect classification on the training data, the presence of extreme data points can distort the model's predictions, rendering them useless as measures of confidence.", "problem": "You are asked to write a complete, runnable program that constructs three synthetic binary classification datasets and fits a linear regression model using Ordinary Least Squares (OLS). The objective is to demonstrate, through exact computation, that under linear separability of classes, OLS can achieve zero training misclassification while producing real-valued predictions that are not suitable as probabilities, thereby failing to reflect uncertainty and making abstention and risk control difficult.\n\nStart from the following fundamental base in statistical learning:\n- Binary labels are encoded as $y_i \\in \\{0,1\\}$.\n- Ordinary Least Squares (OLS) seeks parameters $(\\mathbf{w}, b)$ for a linear predictor $f(\\mathbf{x}) = \\mathbf{w}^{\\top}\\mathbf{x} + b$ that minimize the empirical squared loss $\\sum_{i=1}^{n} (y_i - f(\\mathbf{x}_i))^2$.\n- A classification rule induced from regression outputs thresholds at $0.5$: predict class $\\hat{y}_i = 1$ if $f(\\mathbf{x}_i) \\geq 0.5$ and $\\hat{y}_i = 0$ otherwise.\n- Linear separability means there exists $(\\mathbf{w}^\\ast, b^\\ast)$ such that $f^\\ast(\\mathbf{x}_i) \\geq 0.5$ for all points with $y_i = 1$ and $f^\\ast(\\mathbf{x}_i)  0.5$ for all points with $y_i = 0$.\n\nYour program must implement the OLS fit using the normal equations or a numerically stable equivalent. Use a design matrix $X \\in \\mathbb{R}^{n \\times d}$ with an added intercept column of ones, so the linear model is $f(\\mathbf{x}) = \\mathbf{w}^{\\top}\\mathbf{x} + b$. Compute the OLS solution via the least-squares solution to $X_{\\text{aug}}\\boldsymbol{\\theta} \\approx \\mathbf{y}$, where $X_{\\text{aug}} = [X \\,\\, \\mathbf{1}] \\in \\mathbb{R}^{n \\times (d+1)}$ and $\\boldsymbol{\\theta} \\in \\mathbb{R}^{d+1}$ concatenates $\\mathbf{w}$ and $b$.\n\nFor each dataset, after fitting OLS, compute the following three quantities:\n1. The Boolean indicator of zero training misclassification, defined as whether $\\frac{1}{n}\\sum_{i=1}^{n} \\mathbb{I}[\\hat{y}_i \\neq y_i] = 0$, where $\\hat{y}_i = \\mathbb{I}[f(\\mathbf{x}_i) \\geq 0.5]$.\n2. The fraction of regression predictions outside the unit interval, defined as $\\frac{1}{n}\\sum_{i=1}^{n} \\mathbb{I}[f(\\mathbf{x}_i)  0 \\,\\lor\\, f(\\mathbf{x}_i)  1]$.\n3. The abstention coverage at a margin level $\\epsilon$, defined as the fraction of points whose regression outputs are within $\\epsilon$ of an extreme, $\\frac{1}{n}\\sum_{i=1}^{n} \\mathbb{I}\\big(\\min\\{|f(\\mathbf{x}_i) - 0|, |f(\\mathbf{x}_i) - 1|\\} \\leq \\epsilon\\big)$. This metric quantifies how many predictions would be retained if one only accepts decisions deemed “high confidence” as if the regression output were a probability; it exposes the mismatch because OLS outputs are not probabilities.\n\nYour program must construct exactly the following test suite of datasets (all features are one-dimensional, $d=1$, and the intercept is to be included):\n\n- Test Case A (balanced, moderate separation):\n  - Negative class ($y=0$): $x \\in \\{-3,-2,-1\\}$, each value repeated $10$ times.\n  - Positive class ($y=1$): $x \\in \\{1,2,3\\}$, each value repeated $10$ times.\n  - Margin parameter $\\epsilon = 0.1$.\n\n- Test Case B (balanced with large-magnitude outliers that inflate variance and compress most predictions near $0.5$):\n  - Negative class ($y=0$): $x = -1$ repeated $20$ times and $x = -100$ repeated $5$ times.\n  - Positive class ($y=1$): $x = 1$ repeated $20$ times and $x = 100$ repeated $5$ times.\n  - Margin parameter $\\epsilon = 0.05$.\n\n- Test Case C (same dataset as Test Case B but stricter risk control through a tighter margin):\n  - Negative class ($y=0$): $x = -1$ repeated $20$ times and $x = -100$ repeated $5$ times.\n  - Positive class ($y=1$): $x = 1$ repeated $20$ times and $x = 100$ repeated $5$ times.\n  - Margin parameter $\\epsilon = 0.01$.\n\nFor each test case, fit the OLS model, produce the three metrics described above, and aggregate all results into a single list in the order:\n$[\\text{A\\_zero\\_error}, \\text{A\\_frac\\_outside}, \\text{A\\_coverage}, \\text{B\\_zero\\_error}, \\text{B\\_frac\\_outside}, \\text{B\\_coverage}, \\text{C\\_zero\\_error}, \\text{C\\_frac\\_outside}, \\text{C\\_coverage}]$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,\\dots,r_9]$). There are no physical units or angle units in this problem. All outputs must be of fundamental types: Booleans and floats. The computation must be deterministic and self-contained, requiring no user input or external files.", "solution": "The objective of this problem is to programmatically demonstrate a fundamental deficiency of using Ordinary Least Squares (OLS) linear regression for binary classification tasks. Specifically, we will show that even when the data is linearly separable and OLS achieves zero training misclassification error, its real-valued predictions are not suitable as probabilistic scores. This is because OLS is sensitive to the location of data points in the feature space, a property known as leverage. High-leverage points can skew the regression fit, producing predictions that fall far outside the $[0,1]$ interval and compressing the predictions for other points towards the decision boundary, thus failing to provide a meaningful measure of classification confidence.\n\nWe begin by formalizing the problem. For a dataset of $n$ points $(\\mathbf{x}_i, y_i)$, where $\\mathbf{x}_i \\in \\mathbb{R}^d$ are feature vectors and $y_i \\in \\{0, 1\\}$ are binary labels, OLS finds a linear function $f(\\mathbf{x}) = \\mathbf{w}^{\\top}\\mathbf{x} + b$ that minimizes the sum of squared errors between the predictions and the true labels:\n$$\n\\mathcal{L}(\\mathbf{w}, b) = \\sum_{i=1}^{n} (y_i - f(\\mathbf{x}_i))^2 = \\sum_{i=1}^{n} (y_i - (\\mathbf{w}^{\\top}\\mathbf{x}_i + b))^2\n$$\nTo handle the intercept term $b$ conveniently, we augment the feature vector $\\mathbf{x}_i$ by prepending a $1$ (or appending, as the problem specifies). Let the feature matrix be $X \\in \\mathbb{R}^{n \\times d}$. We construct an augmented design matrix $X_{\\text{aug}} = [X \\,\\, \\mathbf{1}] \\in \\mathbb{R}^{n \\times (d+1)}$, where $\\mathbf{1}$ is a column vector of ones. Let $\\boldsymbol{\\theta} = [\\mathbf{w}^{\\top} \\,\\, b]^{\\top} \\in \\mathbb{R}^{d+1}$ be the vector of parameters. The OLS objective is to find $\\boldsymbol{\\theta}$ that minimizes the squared $L_2$-norm of the residual vector, $\\|\\mathbf{y} - X_{\\text{aug}}\\boldsymbol{\\theta}\\|_2^2$. The solution can be found by solving the normal equations, $(X_{\\text{aug}}^{\\top}X_{\\text{aug}})\\boldsymbol{\\theta} = X_{\\text{aug}}^{\\top}\\mathbf{y}$, or more reliably using numerical methods like Singular Value Decomposition (SVD), which is implemented in standard scientific libraries.\n\nOnce the optimal $\\boldsymbol{\\theta}$ is found, we obtain regression predictions $f(\\mathbf{x}_i)$ for each data point. To convert these to class labels, we apply a threshold at $0.5$:\n$$\n\\hat{y}_i = \\mathbb{I}[f(\\mathbf{x}_i) \\geq 0.5]\n$$\nwhere $\\mathbb{I}[\\cdot]$ is the indicator function.\n\nWe will evaluate the OLS classifier on three synthetic, one-dimensional ($d=1$) datasets using the following three metrics:\n$1$. **Zero Training Misclassification**: A boolean value indicating if the misclassification rate, $\\frac{1}{n}\\sum_{i=1}^{n} \\mathbb{I}[\\hat{y}_i \\neq y_i]$, is exactly $0$. This verifies that the OLS model has found a perfectly separating hyperplane for the training data.\n$2$. **Fraction of Predictions Outside Unit Interval**: The fraction of data points for which the regression output is not in the range $[0, 1]$, computed as $\\frac{1}{n}\\sum_{i=1}^{n} \\mathbb{I}[f(\\mathbf{x}_i)  0 \\lor f(\\mathbf{x}_i)  1]$. This metric directly quantifies the failure of OLS to produce probability-like outputs.\n$3$. **Abstention Coverage**: The fraction of data points whose predictions are deemed \"high-confidence\" by being close to the ideal probability values of $0$ or $1$. It is calculated as $\\frac{1}{n}\\sum_{i=1}^{n} \\mathbb{I}\\big(\\min\\{|f(\\mathbf{x}_i) - 0|, |f(\\mathbf{x}_i) - 1|\\} \\leq \\epsilon\\big)$ for a given margin $\\epsilon$. A low value for this metric reveals that most predictions are far from the extremes, indicating low confidence and making risk-controlled decisions (i.e., abstaining on low-confidence predictions) difficult.\n\nThe algorithmic procedure for each test case is as follows:\n$1$. Construct the feature vector $\\mathbf{x}$ and label vector $\\mathbf{y}$ from the test case specification.\n$2$. Create the design matrix $X_{\\text{aug}}$ by taking the column vector $\\mathbf{x}$ and appending a column of ones.\n$3$. Compute the OLS parameter vector $\\boldsymbol{\\theta} = [w, b]^{\\top}$ by solving the linear system $X_{\\text{aug}}\\boldsymbol{\\theta} \\approx \\mathbf{y}$ using a least-squares solver.\n$4$. Calculate the regression predictions for all data points: $\\mathbf{f} = X_{\\text{aug}}\\boldsymbol{\\theta}$.\n$5$. Using the vector of predictions $\\mathbf{f}$, the true labels $\\mathbf{y}$, and the specified margin $\\epsilon$, compute the three required metrics.\n\nThe test cases are designed to illustrate the effect of high-leverage points. Test Case A provides a baseline with well-behaved, moderately separated data. Test Cases B and C introduce outliers in the feature space ($x=-100$ and $x=100$). Because OLS minimizes squared error, these outlying points exert a strong influence (high leverage) on the regression line. To minimize the large errors associated with these points, OLS will fit a line with a much smaller slope than would be chosen for the inlier data alone. This \"flattening\" of the regression line causes two effects: ($i$) the predictions for the outliers are driven to extreme values (often outside $[0,1]$) to satisfy the loss function, and ($ii$) the predictions for the more numerous inlier points are compressed towards the decision boundary ($0.5$), resulting in low-confidence scores for the majority of the data. This behavior contrasts sharply with models like logistic regression, which are designed for classification and are more robust to such outliers. The program will compute these effects precisely.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef process_case(x_data, y_data, epsilon):\n    \"\"\"\n    Fits an OLS model and computes the three specified metrics for a given dataset.\n\n    Args:\n        x_data (np.ndarray): 1D feature data.\n        y_data (np.ndarray): Binary labels (0 or 1).\n        epsilon (float): Margin for the abstention coverage metric.\n    \n    Returns:\n        tuple: A tuple containing the three metrics:\n               (zero_error, frac_outside, coverage).\n    \"\"\"\n    # Reshape x_data to be a column vector for the design matrix\n    X_mat = x_data.reshape(-1, 1)\n    \n    # Construct the augmented design matrix with an intercept column of ones.\n    # X_aug = [X, 1], so the model is f(x) = w*x + b\n    X_aug = np.hstack([X_mat, np.ones((x_data.shape[0], 1))])\n    \n    # Solve for the OLS parameters theta = [w, b]^T using np.linalg.lstsq.\n    # This provides a numerically stable solution to the normal equations.\n    theta, _, _, _ = np.linalg.lstsq(X_aug, y_data, rcond=None)\n    \n    # Calculate the continuous regression predictions f(x_i)\n    y_pred_reg = X_aug @ theta\n    \n    # --- Metric 1: Zero training misclassification ---\n    # Classify by thresholding regression outputs at 0.5\n    y_pred_class = (y_pred_reg = 0.5).astype(int)\n    # The rate is the mean of the indicator I[y_pred != y_true]\n    misclassification_rate = np.mean(y_pred_class != y_data)\n    zero_error = (misclassification_rate == 0.0)\n    \n    # --- Metric 2: Fraction of predictions outside the unit interval [0, 1] ---\n    frac_outside = np.mean((y_pred_reg  0) | (y_pred_reg  1))\n    \n    # --- Metric 3: Abstention coverage at margin epsilon ---\n    # This is the fraction of points where the prediction is within epsilon of 0 or 1.\n    # The metric is min(|f(x)-0|, |f(x)-1|) = epsilon\n    dist_to_extremes = np.minimum(np.abs(y_pred_reg - 0.0), np.abs(y_pred_reg - 1.0))\n    coverage = np.mean(dist_to_extremes = epsilon)\n    \n    return zero_error, frac_outside, coverage\n\n\ndef solve():\n    \"\"\"\n    Constructs datasets, runs OLS, computes metrics, and prints the final result.\n    \"\"\"\n    # The final list to be populated with 9 results.\n    all_results = []\n    \n    # --- Test Case A (balanced, moderate separation) ---\n    x_neg_A = np.repeat([-3.0, -2.0, -1.0], 10)\n    x_pos_A = np.repeat([1.0, 2.0, 3.0], 10)\n    x_A = np.concatenate([x_neg_A, x_pos_A])\n    y_A = np.concatenate([np.zeros(len(x_neg_A)), np.ones(len(x_pos_A))])\n    epsilon_A = 0.1\n    results_A = process_case(x_A, y_A, epsilon_A)\n    all_results.extend(results_A)\n\n    # --- Test Case B (balanced with large-magnitude outliers) ---\n    x_neg_B = np.concatenate([np.full(20, -1.0), np.full(5, -100.0)])\n    x_pos_B = np.concatenate([np.full(20, 1.0), np.full(5, 100.0)])\n    x_B = np.concatenate([x_neg_B, x_pos_B])\n    y_B = np.concatenate([np.zeros(len(x_neg_B)), np.ones(len(x_pos_B))])\n    epsilon_B = 0.05\n    results_B = process_case(x_B, y_B, epsilon_B)\n    all_results.extend(results_B)\n\n    # --- Test Case C (same as B, stricter margin) ---\n    # Data is identical to Test Case B\n    x_C, y_C = x_B, y_B\n    epsilon_C = 0.01\n    results_C = process_case(x_C, y_C, epsilon_C)\n    all_results.extend(results_C)\n    \n    # Final print statement in the exact required format.\n    # `map(str, ...)` converts boolean `True`/`False` and floats to strings.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```", "id": "3117088"}, {"introduction": "The way we represent class labels, such as using $y \\in \\{0, 1\\}$ versus $y' \\in \\{-1, 1\\}$, might seem like a trivial choice, but for linear regression, it has significant implications. This hands-on problem [@problem_id:3117107] explores how these different encodings alter the model's parameters and the interpretation of its decision boundary. You will investigate when and why the classification rule remains the same, and what happens when key modeling assumptions, like including an intercept, are changed.", "problem": "You are asked to examine how ordinary least squares linear regression behaves when used for binary classification under different label encodings and how this impacts the decision boundary and interpretability. Work entirely in mathematical terms using the following foundational base:\n\n- Ordinary Least Squares (OLS) minimizes the Mean Squared Error (MSE) objective given by $$\\min_{\\beta} \\sum_{i=1}^{n} (y_i - z_i^\\top \\beta)^2,$$ where each training input has augmented feature vector $$z_i = [1, x_i^\\top]^\\top \\in \\mathbb{R}^{d+1}$$ if an intercept is included, otherwise $$z_i = x_i \\in \\mathbb{R}^{d}$$ when no intercept is used.\n- The OLS solution satisfies the normal equations $$Z^\\top Z \\, \\beta = Z^\\top y,$$ where $$Z \\in \\mathbb{R}^{n \\times p}$$ is the design matrix formed by stacking $$z_i^\\top$$ row-wise, $$p = d+1$$ if an intercept is used and $$p = d$$ otherwise; $$y \\in \\mathbb{R}^{n}$$ is the label vector.\n\nYour program must implement OLS fits under two common label encodings for binary classification:\n- Encoding A: $$y \\in \\{0, 1\\}.$$\n- Encoding B: $$y' \\in \\{-1, 1\\}.$$\n\nUnder Encoding A, classification is typically performed by thresholding the fitted values $$\\hat{y} = z^\\top \\beta$$ at $$0.5$$; that is, predict class $$1$$ if $$\\hat{y} \\ge 0.5$$ and class $$0$$ otherwise. Under Encoding B, classification is typically performed by thresholding $$\\hat{y}' = z^\\top \\beta'$$ at $$0$$; that is, predict class $$1$$ if $$\\hat{y}' \\ge 0$$ and class $$-1$$ otherwise. The exact treatment of these thresholds and the influence of the encoding choice are the focus of this exercise.\n\nData specification:\n- Training set of $$n = 10$$ points in $$\\mathbb{R}^2$$:\n  - Class $$0$$ points (Encoding A labels $$0$$, Encoding B labels $$-1$$): $$x = [-5, 3], [-4, 2], [-3, 1], [-2, 0.5], [-1, -0.5].$$\n  - Class $$1$$ points (Encoding A labels $$1$$, Encoding B labels $$1$$): $$x = [2, 1], [3, 2], [4, 2.5], [5, 3], [6, 4].$$\n- For models \"with intercept\", use augmented features $$z = [1, x_1, x_2]^\\top.$$ For models \"without intercept\", use features $$z = [x_1, x_2]^\\top.$$\n\nEvaluation grid:\n- Define a grid of test points $$x = (x_1, x_2)$$ with $$x_1 \\in \\{-6, -5, \\dots, 6\\}$$ and $$x_2 \\in \\{-3, -2, \\dots, 6\\},$$ forming $$13 \\times 10$$ grid points. Use these for evaluating decision regions and interpretability properties.\n\nYou must implement the following four test cases that probe different facets of the drawbacks of linear regression for classification. Each test case must produce a single fundamental type: a boolean, an integer, or a float.\n\n- Test Case $$1$$ (Decision boundary invariance across encodings with correct thresholds, intercept included):\n  - Fit OLS with intercept under Encoding A to obtain $$\\hat{y}(x)$$ and classify grid points using threshold $$0.5$$ to labels in $$\\{0,1\\}$$.\n  - Fit OLS with intercept under Encoding B to obtain $$\\hat{y}'(x)$$ and classify grid points using threshold $$0$$ to labels in $$\\{-1,1\\}$$. Map these to $$\\{0,1\\}$$ via $$-1 \\mapsto 0, 1 \\mapsto 1$$.\n  - Return a boolean indicating whether the two classification labelings on the grid are identical for all grid points.\n\n- Test Case $$2$$ (Out-of-range fitted values under Encoding A, intercept included):\n  - Using the OLS fit with intercept under Encoding A, compute the proportion of grid points whose fitted values $$\\hat{y}(x)$$ lie outside $$[0, 1]$$. Return this proportion as a float in $$[0,1]$$. This quantifies a drawback: linear regression fitted values are not probabilities and can be outside the interpretable range.\n\n- Test Case $$3$$ (Failure of affine consistency without intercept):\n  - Fit OLS without intercept under both encodings to obtain fitted values $$\\hat{y}(x_i)$$ and $$\\hat{y}'(x_i)$$ on the training inputs.\n  - Let $$a$$ and $$c$$ be the unique scalars such that $$a \\cdot 0 + c = -1$$ and $$a \\cdot 1 + c = 1$$ (the unique affine map from $$[0,1]$$ to $$[-1,1]$$). Compute the maximum absolute deviation over the training points between $$\\hat{y}'(x_i)$$ and $$a \\, \\hat{y}(x_i) + c$$. Return this maximum deviation as a float. A nonzero deviation demonstrates that the encoding change alters the fitted decision function when the intercept is omitted.\n\n- Test Case $$4$$ (Encoding-dependent thresholding under Encoding B, intercept included):\n  - Using the OLS fit with intercept under Encoding B, classify the grid points twice: once with threshold $$0$$ and once with threshold $$0.5$$ for $$\\hat{y}'(x)$$.\n  - Return an integer equal to the number of grid points whose classifications differ between these two thresholds. This quantifies how a change in encoding and threshold moves the decision boundary and affects interpretability.\n\nFinal output format:\n- Your program should produce a single line of output containing the results of Test Cases $$1$$ through $$4$$ as a comma-separated list enclosed in square brackets, in the order $$[\\text{TC1}, \\text{TC2}, \\text{TC3}, \\text{TC4}]$$. For example, $$[ \\text{boolean}, \\text{float}, \\text{float}, \\text{integer} ]$$.\n- No external input is permitted; all data are as specified above.", "solution": "The user has provided a problem to analyze the behavior of ordinary least squares (OLS) linear regression when utilized for binary classification. The analysis focuses on the effects of different label encodings and the inclusion or exclusion of an intercept term. The problem is well-defined, scientifically sound, and all necessary data and evaluation criteria are provided. It is a valid problem in statistical learning.\n\nThe core of OLS is the minimization of the Mean Squared Error (MSE), defined as the sum of squared differences between observed labels $$y_i$$ and predicted values $$\\hat{y}_i = z_i^\\top \\beta$$. The objective function is $$J(\\beta) = \\sum_{i=1}^{n} (y_i - z_i^\\top \\beta)^2$$. The optimal parameter vector $$\\beta$$ that minimizes this objective is found by solving the normal equations $$Z^\\top Z \\beta = Z^\\top y$$, where $$Z$$ is the design matrix and $$y$$ is the vector of labels. The solution is $$\\beta = (Z^\\top Z)^{-1} Z^\\top y$$, provided that the matrix $$Z^\\top Z$$ is invertible, which is true for the given non-collinear data.\n\nWe will proceed to solve the four specified test cases.\n\n### Test Case 1: Decision boundary invariance across encodings\nThis test case examines whether the decision boundary is invariant to an affine transformation of the labels when the model includes an intercept.\nLet Encoding A use labels $$y_A \\in \\{0, 1\\}$$ and Encoding B use labels $$y_B \\in \\{-1, 1\\}$$. The two encodings are related by the affine transformation $$y_B = 2y_A - 1$$. Let $$\\beta_A$$ and $$\\beta_B$$ be the OLS coefficient vectors corresponding to $$y_A$$ and $$y_B$$ respectively, for a model with an intercept. The design matrix $$Z$$ is identical for both fits and includes a column of ones, i.e., $$z_i = [1, x_i^\\top]^\\top$$.\n\nThe relationship between the coefficient vectors is:\n$$\\beta_B = (Z^\\top Z)^{-1} Z^\\top y_B = (Z^\\top Z)^{-1} Z^\\top (2y_A - \\mathbf{1})$$\n$$= 2(Z^\\top Z)^{-1} Z^\\top y_A - (Z^\\top Z)^{-1} Z^\\top \\mathbf{1} = 2\\beta_A - (Z^\\top Z)^{-1} Z^\\top \\mathbf{1}$$\nwhere $$\\mathbf{1}$$ is a column vector of ones. Since the model includes an intercept, the first column of $$Z$$ is $$\\mathbf{1}$$. Let $$e_1 = [1, 0, \\dots, 0]^\\top$$. Then $$Z e_1 = \\mathbf{1}$$, which implies $$Z^\\top \\mathbf{1} = Z^\\top Z e_1$$. Substituting this into the expression for $$\\beta_B$$ yields:\n$$\\beta_B = 2\\beta_A - (Z^\\top Z)^{-1} (Z^\\top Z e_1) = 2\\beta_A - e_1$$\nThis means that if $$\\beta_A = [\\beta_{A,0}, \\beta_{A,1}, \\dots]^\\top$$, then $$\\beta_B = [2\\beta_{A,0} - 1, 2\\beta_{A,1}, \\dots]^\\top$$.\n\nFor any input point $$z = [1, x^\\top]^\\top$$, the fitted values are related as:\n$$\\hat{y}_B(z) = z^\\top \\beta_B = z^\\top(2\\beta_A - e_1) = 2z^\\top\\beta_A - z^\\top e_1 = 2\\hat{y}_A(z) - 1$$\nThus, the predicted values are also related by the same affine transformation.\n\nThe classification rules are:\n- Encoding A: Predict class 1 if $$\\hat{y}_A(z) \\ge 0.5$$.\n- Encoding B: Predict class 1 if $$\\hat{y}_B(z) \\ge 0$$.\n\nSubstituting the relationship between fitted values into the rule for Encoding B gives:\n$$2\\hat{y}_A(z) - 1 \\ge 0 \\implies 2\\hat{y}_A(z) \\ge 1 \\implies \\hat{y}_A(z) \\ge 0.5$$\nThis is identical to the classification rule for Encoding A. Therefore, the classifications on all grid points will be identical. The result is a boolean `True`.\n\n### Test Case 2: Out-of-range fitted values\nThis test case highlights a key drawback of OLS for classification: the fitted values $$\\hat{y}(z) = z^\\top\\beta$$ are not constrained to the range of the labels (e.g., $$[0, 1]$$). The prediction is a linear function of the input features and is thus unbounded. For points far from the decision hyperplane defined by $$z^\\top\\beta_A = 0.5$$, the value of $$z^\\top\\beta_A$$ can be significantly larger than $$1$$ or smaller than $$0$$. This behavior contrasts with models like logistic regression, which use a sigmoid function to map the linear predictor into the $$[0, 1]$$ interval, allowing for a probabilistic interpretation. We will compute the fitted values for the model with Encoding A on all grid points and calculate the proportion that falls outside the $$[0, 1]$$ range.\n\n### Test Case 3: Failure of affine consistency without intercept\nThis test investigates what happens to the affine equivariance property when the intercept term is omitted. The model is now $$ \\hat{y}(x) = x^\\top \\beta $$. The design matrix $$Z$$ now consists only of the feature columns $$x_i^\\top$$.\nAs derived in Test Case 1, $$\\beta_B = 2\\beta_A - (Z^\\top Z)^{-1} Z^\\top \\mathbf{1}$$. When no intercept is included, the column space of $$Z$$ does not generally contain the vector $$\\mathbf{1}$$. Consequently, the term $$(Z^\\top Z)^{-1} Z^\\top \\mathbf{1}$$ does not simplify to $$e_1$$.\nThe relationship between the fitted values on the training set becomes:\n$$\\hat{y}_B = Z \\beta_B = 2Z\\beta_A - Z(Z^\\top Z)^{-1} Z^\\top \\mathbf{1} = 2\\hat{y}_A - H\\mathbf{1}$$\nwhere $$H = Z(Z^\\top Z)^{-1} Z^\\top$$ is the hat matrix.\nThe affine relationship $$\\hat{y}_B = 2\\hat{y}_A - \\mathbf{1}$$ holds only if $$H\\mathbf{1} = \\mathbf{1}$$, which requires $$\\mathbf{1}$$ to be in the column space of $$Z$$. Without an intercept, this is not guaranteed and does not hold for the given data. This test computes the maximum absolute deviation $$|\\hat{y}'(x_i) - (2 \\hat{y}(x_i) - 1)|$$ over the training points, which is equivalent to computing the maximum element-wise magnitude of the vector $$| (2\\hat{y}_A - H\\mathbf{1}) - (2\\hat{y}_A - \\mathbf{1}) | = | \\mathbf{1} - H\\mathbf{1} |$$. A non-zero result demonstrates the failure of affine consistency.\n\n### Test Case 4: Encoding-dependent thresholding\nThe choice of classification threshold is intrinsically linked to the label encoding. For a $$\\{0, 1\\}$$ encoding, the natural threshold is $$0.5$$, the midpoint of the labels. For a $$\\{-1, 1\\}$$ encoding, the natural threshold is $$0$$. This test case quantifies the effect of using an \"incorrect\" threshold. It uses the model fitted under Encoding B (labels $$\\{-1, 1\\}$$) and classifies grid points using two different thresholds: the correct one ($$0$$) and one that is natural for a different encoding ($$0.5$$). The number of grid points where the classification changes reveals the extent of the decision boundary shift. A point's classification will change if its fitted value $$\\hat{y}_B(z)$$ falls between the two thresholds, i.e., $$0 \\le \\hat{y}_B(z)  0.5$$. The resulting integer count demonstrates the non-trivial impact of this interpretability error.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the four test cases related to the use of ordinary least squares\n    for binary classification.\n    \"\"\"\n    \n    # --- Data Specification ---\n    X_train = np.array([\n        [-5, 3], [-4, 2], [-3, 1], [-2, 0.5], [-1, -0.5], # Class 0\n        [2, 1], [3, 2], [4, 2.5], [5, 3], [6, 4]         # Class 1\n    ])\n    \n    # Encoding A: y in {0, 1}\n    y_A = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])\n    \n    # Encoding B: y' in {-1, 1}\n    y_B = np.array([-1, -1, -1, -1, -1, 1, 1, 1, 1, 1])\n\n    # Evaluation grid\n    x1_grid = np.arange(-6, 7)  # 13 points from -6 to 6\n    x2_grid = np.arange(-3, 7)  # 10 points from -3 to 6\n    xx1, xx2 = np.meshgrid(x1_grid, x2_grid, indexing='ij')\n    grid_points = np.vstack([xx1.ravel(), xx2.ravel()]).T\n\n    # --- Helper Functions ---\n    def fit_ols(X, y, intercept=True):\n        \"\"\"Fits an OLS model and returns the parameters beta.\"\"\"\n        if intercept:\n            Z = np.hstack([np.ones((X.shape[0], 1)), X])\n        else:\n            Z = X\n        \n        # Solve the normal equations: Z.T Z beta = Z.T y\n        try:\n            beta = np.linalg.solve(Z.T @ Z, Z.T @ y)\n        except np.linalg.LinAlgError:\n            # Fallback for singular matrices, though not expected here\n            # as the problem is well-posed.\n            Z_pinv = np.linalg.pinv(Z)\n            beta = Z_pinv @ y\n            \n        return beta\n\n    def predict_ols(X_new, beta, intercept=True):\n        \"\"\"Computes predictions y_hat = Z beta.\"\"\"\n        if intercept:\n            Z_new = np.hstack([np.ones((X_new.shape[0], 1)), X_new])\n        else:\n            Z_new = X_new\n        return Z_new @ beta\n\n    # --- Test Case 1: Decision boundary invariance ---\n    beta_A_int = fit_ols(X_train, y_A, intercept=True)\n    beta_B_int = fit_ols(X_train, y_B, intercept=True)\n    \n    y_hat_A_grid = predict_ols(grid_points, beta_A_int, intercept=True)\n    y_hat_B_grid = predict_ols(grid_points, beta_B_int, intercept=True)\n    \n    # Classification with Encoding A model and threshold 0.5\n    classif_A = (y_hat_A_grid = 0.5)\n    \n    # Classification with Encoding B model, threshold 0, then map to {0, 1}\n    pred_B = np.where(y_hat_B_grid = 0, 1, -1)\n    classif_B = (pred_B == 1)\n    \n    tc1_result = bool(np.array_equal(classif_A, classif_B))\n\n    # --- Test Case 2: Out-of-range fitted values ---\n    out_of_range = (y_hat_A_grid  0) | (y_hat_A_grid  1)\n    tc2_result = float(np.sum(out_of_range) / len(grid_points))\n\n    # --- Test Case 3: Failure of affine consistency ---\n    beta_A_noint = fit_ols(X_train, y_A, intercept=False)\n    beta_B_noint = fit_ols(X_train, y_B, intercept=False)\n    \n    y_hat_A_train = predict_ols(X_train, beta_A_noint, intercept=False)\n    y_hat_B_train = predict_ols(X_train, beta_B_noint, intercept=False)\n    \n    # Affine map from [0,1] to [-1,1] is f(t) = 2*t - 1\n    y_hat_A_transformed = 2 * y_hat_A_train - 1\n    \n    deviations = np.abs(y_hat_B_train - y_hat_A_transformed)\n    tc3_result = float(np.max(deviations))\n    \n    # --- Test Case 4: Encoding-dependent thresholding ---\n    # Use y_hat_B_grid from Test Case 1\n    classif_B_thresh0 = (y_hat_B_grid = 0)\n    classif_B_thresh05 = (y_hat_B_grid = 0.5)\n    \n    differences = np.sum(classif_B_thresh0 != classif_B_thresh05)\n    tc4_result = int(differences)\n\n    # --- Final Output ---\n    results = [tc1_result, tc2_result, tc3_result, tc4_result]\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3117107"}, {"introduction": "This final practice [@problem_id:3117167] isolates a deep conceptual flaw in using linear regression for classification: the decision rule itself. Through a focused thought experiment, you will see that even if an OLS model's raw scores were perfectly, monotonically related to the scores from a more appropriate model like logistic regression, their classification outputs can still disagree. This reveals that the problem isn't just that OLS scores are uncalibrated, but that the entire framework of applying a fixed threshold to a linear predictor is mismatched for the task of classification.", "problem": "You are given the task of demonstrating a drawback of using ordinary least squares (OLS) regression for binary classification by showing threshold dependence: even when two models produce predictions that are identical up to a monotone rescaling, their induced classifications can differ if they use different fixed thresholds. Use the following fundamental base. Ordinary least squares (OLS) regression predicts a continuous score $s$ from features $x$ by minimizing squared error on a coded response $\\tilde{y} \\in \\{-1,+1\\}$, yielding fitted scores $s_i = x_i^\\top w$ that are typically converted to class labels by the sign rule, that is, $\\hat{y}^{\\mathrm{OLS}}_i = \\mathrm{sign}(s_i)$, or equivalently the threshold rule $\\hat{y}^{\\mathrm{OLS}}_i = +1$ if $s_i \\ge 0$ and $\\hat{y}^{\\mathrm{OLS}}_i = -1$ if $s_i  0$. Logistic regression models the conditional probability $p_i = \\mathbb{P}(Y=1 \\mid x_i)$ via the logistic function $p_i = \\sigma(z_i)$ with $\\sigma(t) = 1/(1+e^{-t})$ and linear predictor $z_i = \\beta_0 + x_i^\\top \\beta$. The standard decision rule is $\\hat{y}^{\\mathrm{LR}}_i = 1$ if $p_i \\ge 0.5$ and $\\hat{y}^{\\mathrm{LR}}_i = 0$ otherwise, that is, equivalently $\\hat{y}^{\\mathrm{LR}}_i = 1$ if $z_i \\ge 0$ and $\\hat{y}^{\\mathrm{LR}}_i = 0$ otherwise. In order to compare to the $\\{-1,+1\\}$ coding used for OLS, map the logistic decision to $\\{-1,+1\\}$ by defining $\\tilde{y}^{\\mathrm{LR}}_i = +1$ if $z_i \\ge 0$ and $\\tilde{y}^{\\mathrm{LR}}_i = -1$ otherwise. Consider the case where the logistic model’s linear predictors $\\{z_i\\}_{i=1}^n$ are related to the OLS fitted scores $\\{s_i\\}_{i=1}^n$ by an affine transformation $z_i = a s_i + b$ with constants $a \\in \\mathbb{R} \\setminus \\{0\\}$ and $b \\in \\mathbb{R}$. Because the logistic link $\\sigma(\\cdot)$ is strictly monotone, the probabilities $\\{p_i\\}$ are a monotone rescaling of $\\{s_i\\}$ via $p_i = \\sigma(a s_i + b)$.\n\nYour program must implement the two decision rules above based solely on the vectors $\\{s_i\\}$ and parameters $(a,b)$, without performing any model fitting. In particular:\n- OLS decision: for each $s_i$, predict $+1$ if $s_i \\ge 0$ and $-1$ otherwise.\n- Logistic decision: for each $s_i$, compute $z_i = a s_i + b$ and predict $+1$ if $z_i \\ge 0$ and $-1$ otherwise.\n\nFor each test case listed below, compute a single integer $D$ equal to the number of indices $i$ where the OLS and logistic decisions disagree, that is, the count of $i$ such that $\\mathrm{sign}(s_i) \\ne \\mathrm{sign}(a s_i + b)$, using the convention that $\\mathrm{sign}(0) = +1$ for both decision rules (that is, ties at the threshold are assigned to the positive class). Note that all quantities are dimensionless; there are no physical units. The test suite is:\n\n1. Case A (happy path with different thresholds): $s = [-1.0,-0.25,0.0,0.25,1.0]$, $a = 1.0$, $b = 0.5$.\n2. Case B (boundary alignment, no disagreement): $s = [-2.0,-1.0,0.0,1.0,2.0]$, $a = 3.0$, $b = 0.0$.\n3. Case C (threshold shifted to $s^\\star = -b/a  0$): $s = [-0.2,0.0,0.2,0.25,0.5,1.0]$, $a = 2.0$, $b = -0.5$.\n4. Case D (monotone decreasing rescaling, orientation flipped): $s = [-1.0,-0.5,0.0,0.5,1.0]$, $a = -1.5$, $b = 0.0$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[d_A,d_B,d_C,d_D]$), where $d_A$, $d_B$, $d_C$, and $d_D$ are the integer disagreement counts $D$ for Cases A, B, C, and D respectively.", "solution": "The problem requires a quantitative comparison between two binary classification decision rules. The first rule is derived from a set of scores $\\{s_i\\}_{i=1}^n$ using a fixed threshold at $0$. The second rule is derived from a set of scores $\\{z_i\\}_{i=1}^n$ which are an affine transformation of the first, $z_i = a s_i + b$, again using a fixed threshold at $0$. The goal is to count the number of instances where the two rules produce different class labels. This exercise demonstrates how, even with scores that are related by a strictly monotone function, the choice of a fixed, non-data-adaptive threshold (as in an OLS-based classifier) versus a model-inherent threshold (as in logistic regression) can lead to different outcomes.\n\nFirst, let us formalize the two decision rules as specified. For each score $s_i$ in a given vector $\\mathbf{s}$, the OLS-based prediction, $\\hat{y}^{\\mathrm{OLS}}_i$, is determined by the sign of $s_i$. With the specified convention for handling ties at the threshold, the rule is:\n$$\n\\hat{y}^{\\mathrm{OLS}}_i = \\begin{cases} +1  \\text{if } s_i \\ge 0 \\\\ -1  \\text{if } s_i  0 \\end{cases}\n$$\nThis is equivalent to applying a sign function, $\\mathrm{sign}(\\cdot)$, with the convention that $\\mathrm{sign}(0) = +1$.\n\nThe logistic regression-based prediction, $\\tilde{y}^{\\mathrm{LR}}_i$, first computes a transformed score $z_i = a s_i + b$, and then applies the same sign-based rule:\n$$\n\\tilde{y}^{\\mathrm{LR}}_i = \\begin{cases} +1  \\text{if } z_i \\ge 0 \\\\ -1  \\text{if } z_i  0 \\end{cases}\n$$\nSubstituting the expression for $z_i$, the condition for a positive class prediction becomes $a s_i + b \\ge 0$.\n\nA disagreement between the two rules occurs for an index $i$ if and only if $\\hat{y}^{\\mathrm{OLS}}_i \\ne \\tilde{y}^{\\mathrm{LR}}_i$. This is equivalent to the condition $\\mathrm{sign}(s_i) \\ne \\mathrm{sign}(a s_i + b)$, using the specified sign convention. We must count the number of indices $i$ for which this condition holds for each test case.\n\nLet's analyze the condition for disagreement based on the parameters $a$ and $b$.\n\nCase I: $a  0$.\nThe transformation $z_i = a s_i + b$ is strictly monotone increasing.\nThe OLS rule partitions the real line at the threshold $s_i = 0$.\nThe logistic rule's condition $a s_i + b \\ge 0$ is equivalent to $a s_i \\ge -b$, which simplifies to $s_i \\ge -b/a$. So, the logistic rule partitions the real line at the threshold $s_i = -b/a$.\nA disagreement occurs if a score $s_i$ falls between these two thresholds, $0$ and $-b/a$.\n- If $-b/a  0$, a disagreement occurs for any $s_i$ such that $0 \\le s_i  -b/a$.\n- If $-b/a  0$, a disagreement occurs for any $s_i$ such that $-b/a \\le s_i  0$.\nIf $-b/a = 0$ (i.e., $b=0$), the thresholds are identical, and there are no disagreements.\n\nCase II: $a  0$.\nThe transformation $z_i = a s_i + b$ is strictly monotone decreasing.\nThe logistic rule's condition $a s_i + b \\ge 0$ is equivalent to $a s_i \\ge -b$, which simplifies to $s_i \\le -b/a$ because dividing by a negative number reverses the inequality.\nThe OLS rule still predicts $+1$ for $s_i \\ge 0$ and $-1$ for $s_i  0$.\nThe logistic rule now predicts $+1$ for $s_i \\le -b/a$ and $-1$ for $s_i  -b/a$.\nIf $b=0$, the threshold is $s_i=0$ for both rules, but the orientation is reversed for the logistic rule.\n- OLS: $\\hat{y}^{\\mathrm{OLS}}_i = +1$ if $s_i \\ge 0$, $-1$ if $s_i  0$.\n- LR: $\\tilde{y}^{\\mathrm{LR}}_i = +1$ if $s_i \\le 0$, $-1$ if $s_i  0$.\nA disagreement occurs for all $s_i \\ne 0$. For $s_i  0$, $\\hat{y}^{\\mathrm{OLS}}_i = +1$ while $\\tilde{y}^{\\mathrm{LR}}_i = -1$. For $s_i  0$, $\\hat{y}^{\\mathrm{OLS}}_i = -1$ while $\\tilde{y}^{\\mathrm{LR}}_i = +1$. For $s_i=0$, both rules yield $+1$, so there is no disagreement.\n\nWe now apply this analysis to each test case. The number of disagreements is denoted by $D$.\n\nCase A: $s = [-1.0, -0.25, 0.0, 0.25, 1.0]$, $a = 1.0$, $b = 0.5$.\nHere, $a = 1.0  0$. The OLS threshold is $s=0$. The logistic threshold is $s = -b/a = -0.5/1.0 = -0.5$.\nDisagreement occurs for $s_i$ in the interval $[-0.5, 0)$.\nThe only score in $s$ that falls into this interval is $s_2 = -0.25$.\nFor $s_2 = -0.25$: $\\hat{y}^{\\mathrm{OLS}}_2 = \\mathrm{sign}(-0.25) = -1$. The transformed score is $z_2 = 1.0(-0.25) + 0.5 = 0.25$, so $\\tilde{y}^{\\mathrm{LR}}_2 = \\mathrm{sign}(0.25) = +1$. They disagree.\nFor all other scores, the predictions match.\nThe number of disagreements is $D_A = 1$.\n\nCase B: $s = [-2.0, -1.0, 0.0, 1.0, 2.0]$, $a = 3.0$, $b = 0.0$.\nHere, $a = 3.0  0$ and $b=0$. The OLS threshold is $s=0$. The logistic threshold is $s = -b/a = 0.0$.\nThe thresholds are identical. For any $s_i$, $\\mathrm{sign}(s_i) = \\mathrm{sign}(3.0 \\cdot s_i)$.\nTherefore, there can be no disagreements.\nThe number of disagreements is $D_B = 0$.\n\nCase C: $s = [-0.2, 0.0, 0.2, 0.25, 0.5, 1.0]$, $a = 2.0$, $b = -0.5$.\nHere, $a = 2.0  0$. The OLS threshold is $s=0$. The logistic threshold is $s = -b/a = -(-0.5)/2.0 = 0.25$.\nDisagreement occurs for $s_i$ in the interval $[0, 0.25)$.\nThe scores in $s$ that fall into this interval are $s_2=0.0$ and $s_3=0.2$.\n- For $s_2=0.0$: $\\hat{y}^{\\mathrm{OLS}}_2 = \\mathrm{sign}(0.0)=+1$. The transformed score is $z_2 = 2.0(0.0) - 0.5 = -0.5$, so $\\tilde{y}^{\\mathrm{LR}}_2 = \\mathrm{sign}(-0.5)=-1$. Disagree.\n- For $s_3=0.2$: $\\hat{y}^{\\mathrm{OLS}}_3 = \\mathrm{sign}(0.2)=+1$. The transformed score is $z_3 = 2.0(0.2) - 0.5 = -0.1$, so $\\tilde{y}^{\\mathrm{LR}}_3 = \\mathrm{sign}(-0.1)=-1$. Disagree.\nThe number of disagreements is $D_C = 2$.\n\nCase D: $s = [-1.0, -0.5, 0.0, 0.5, 1.0]$, $a = -1.5$, $b = 0.0$.\nHere, $a = -1.5  0$ and $b=0.0$. As derived in the general analysis for this case, a disagreement occurs for every $s_i \\ne 0$.\nThe scores in $s$ are $\\{ -1.0, -0.5, 0.0, 0.5, 1.0 \\}$. There are $4$ non-zero scores.\n- For $s_1 = -1.0$ (negative): $\\hat{y}^{\\mathrm{OLS}}_1 = -1$, $\\tilde{y}^{\\mathrm{LR}}_1 = \\mathrm{sign}((-1.5)(-1.0)) = \\mathrm{sign}(1.5) = +1$. Disagree.\n- For $s_2 = -0.5$ (negative): $\\hat{y}^{\\mathrm{OLS}}_2 = -1$, $\\tilde{y}^{\\mathrm{LR}}_2 = \\mathrm{sign}((-1.5)(-0.5)) = \\mathrm{sign}(0.75) = +1$. Disagree.\n- For $s_3 = 0.0$: $\\hat{y}^{\\mathrm{OLS}}_3 = +1$, $\\tilde{y}^{\\mathrm{LR}}_3 = \\mathrm{sign}(0.0) = +1$. Match.\n- For $s_4 = 0.5$ (positive): $\\hat{y}^{\\mathrm{OLS}}_4 = +1$, $\\tilde{y}^{\\mathrm{LR}}_4 = \\mathrm{sign}((-1.5)(0.5)) = \\mathrm{sign}(-0.75) = -1$. Disagree.\n- For $s_5 = 1.0$ (positive): $\\hat{y}^{\\mathrm{OLS}}_5 = +1$, $\\tilde{y}^{\\mathrm{LR}}_5 = \\mathrm{sign}((-1.5)(1.0)) = \\mathrm{sign}(-1.5) = -1$. Disagree.\nThe number of disagreements is $D_D = 4$.\n\nThe final results are the list of disagreement counts: $[D_A, D_B, D_C, D_D] = [1, 0, 2, 4]$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Calculates the number of disagreements between two classification rules\n    for several test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (s_vector, a, b)\n    test_cases = [\n        # Case A\n        (np.array([-1.0, -0.25, 0.0, 0.25, 1.0]), 1.0, 0.5),\n        # Case B\n        (np.array([-2.0, -1.0, 0.0, 1.0, 2.0]), 3.0, 0.0),\n        # Case C\n        (np.array([-0.2, 0.0, 0.2, 0.25, 0.5, 1.0]), 2.0, -0.5),\n        # Case D\n        (np.array([-1.0, -0.5, 0.0, 0.5, 1.0]), -1.5, 0.0),\n    ]\n\n    results = []\n    # Process each test case by unpacking the parameters\n    for s, a, b in test_cases:\n        # The decision rule is to predict +1 for a score = 0, and -1 otherwise.\n        # This can be implemented efficiently for the entire vector using np.where.\n        \n        # 1. OLS decision based on scores s\n        # y_ols is a vector of +1s and -1s representing the class predictions.\n        y_ols = np.where(s = 0, 1, -1)\n\n        # 2. Logistic decision based on transformed scores z = a*s + b\n        # First, compute the vector of z scores.\n        z = a * s + b\n        # Then, apply the same decision rule to the z scores.\n        y_lr = np.where(z = 0, 1, -1)\n\n        # 3. Count the number of disagreements\n        # The boolean array (y_ols != y_lr) contains True where predictions differ.\n        # Summing a boolean array in numpy counts the number of True values.\n        disagreement_count = np.sum(y_ols != y_lr)\n        \n        results.append(disagreement_count)\n\n    # Final print statement in the exact required format \"[d_A,d_B,d_C,d_D]\".\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3117167"}]}