{"hands_on_practices": [{"introduction": "This first practice provides a foundational, hands-on demonstration of bagging's core strengths. By implementing bagging with polynomial regression learners, you will directly observe how this ensemble method reduces variance and improves prediction accuracy, especially for complex, high-variance models. This exercise [@problem_id:3101761] will also allow you to empirically validate that the Out-of-Bag (OOB) error provides a reliable, built-in estimate of the model's generalization performance, closely tracking the test error without needing a separate validation set.", "problem": "You are given a supervised regression setting in which the hypothesis complexity is controlled by the degree of polynomial features used by the base learners. The ensemble method of bootstrap aggregating (bagging) with Out-of-Bag (OOB) error estimation is to be implemented and analyzed. The task is to produce a program that constructs a synthetic dataset from a fixed data generating process, trains polynomial regression models inside a bagging scheme, computes the Out-of-Bag mean squared error, and compares it with the test mean squared error of both a single unbagged model and the bagged ensemble, as the polynomial degree increases and the number of bootstrap fits varies.\n\nFundamental base and setup:\n- Let the data generating process be defined by the deterministic function $f(x) = \\sin(2\\pi x) + 0.5x$ with additive noise. For any $x \\in [-1,1]$, the response is $y = f(x) + \\varepsilon$, where $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)$ with $\\sigma = 0.3$.\n- Construct a training set $\\mathcal{D}_{\\text{train}} = \\{(x_i, y_i)\\}_{i=1}^{n_{\\text{train}}}$ with $n_{\\text{train}} = 80$, where $x_i \\sim \\text{Uniform}([-1,1])$, and a test set $\\mathcal{D}_{\\text{test}} = \\{(x_j^{\\text{test}}, y_j^{\\text{test}})\\}_{j=1}^{n_{\\text{test}}}$ with $n_{\\text{test}} = 400$, where $x_j^{\\text{test}} \\sim \\text{Uniform}([-1,1])$. Both are independently sampled. Use a fixed random seed $s_{\\text{data}} = 24680$ to generate the training inputs and noise, and an independent fixed random seed $s_{\\text{test}} = 24681$ to generate the test inputs and noise. All $x$ values are real numbers with no physical units.\n- For a specified polynomial degree $d \\in \\{0,1,2,\\dots\\}$, define the feature map $\\phi_d(x) = [1, x, x^2, \\dots, x^d]^\\top$. A base learner is a polynomial regression model that minimizes the sum of squared residuals over the training data it receives. This is Ordinary Least Squares (OLS).\n\nBagging and Out-of-Bag estimation:\n- For a given number of bootstrap fits $M \\in \\mathbb{N}$ and degree $d$, construct $M$ bootstrap samples of size $n_{\\text{train}}$ by sampling indices from $\\{1,\\dots,n_{\\text{train}}\\}$ with replacement. For each bootstrap sample $b \\in \\{1,\\dots,M\\}$, fit an OLS polynomial regression of degree $d$ using only that sample to get a predictor $g_b(x)$.\n- Define the bagged predictor as the arithmetic average $G_M(x) = \\frac{1}{M}\\sum_{b=1}^M g_b(x)$.\n- For Out-of-Bag (OOB) error estimation, for each training index $i \\in \\{1,\\dots,n_{\\text{train}}\\}$, collect predictions $g_b(x_i)$ only from those models $b$ whose bootstrap sample did not include $i$. Denote by $\\mathcal{B}_i = \\{b \\in \\{1,\\dots,M\\} : i \\text{ is not in bootstrap sample } b\\}$ the set of Out-of-Bag models for $i$. If $\\mathcal{B}_i$ is nonempty, define the OOB prediction as $\\hat{y}_i^{\\text{OOB}} = \\frac{1}{|\\mathcal{B}_i|}\\sum_{b \\in \\mathcal{B}_i} g_b(x_i)$. The OOB mean squared error is computed over all indices with at least one OOB prediction:\n$$\n\\mathrm{MSE}_{\\text{OOB}} = \\frac{1}{|\\mathcal{I}_{\\text{OOB}}|}\\sum_{i \\in \\mathcal{I}_{\\text{OOB}}} \\left(y_i - \\hat{y}_i^{\\text{OOB}}\\right)^2,\n$$\nwhere $\\mathcal{I}_{\\text{OOB}} = \\{i : |\\mathcal{B}_i| \\ge 1\\}$.\n\nEvaluation metrics to compute for each test case $(d, M)$:\n- The OOB mean squared error of the bagged ensemble, $\\mathrm{MSE}_{\\text{OOB}}$.\n- The test mean squared error of a single unbagged degree-$d$ model fitted once on the full training set, denoted $\\mathrm{MSE}_{\\text{single,test}}$.\n- The test mean squared error of the bagged ensemble $G_M$, denoted $\\mathrm{MSE}_{\\text{bag,test}}$.\n\nRequirements for randomness and reproducibility:\n- Use the fixed seeds $s_{\\text{data}} = 24680$ and $s_{\\text{test}} = 24681$ for generating training and test sets as described.\n- For each test case $(d,M)$, re-seed the bootstrap sampling to $s_{\\text{boot}} = 100000 + 1000\\cdot d + M$ before creating the $M$ bootstrap samples and fitting the $M$ base learners. This ensures each case is reproducible and independent of other cases.\n\nProgram outputs and numerical formatting:\n- For each test case $(d,M)$, compute the triple $[\\mathrm{MSE}_{\\text{OOB}}, \\mathrm{MSE}_{\\text{single,test}}, \\mathrm{MSE}_{\\text{bag,test}}]$.\n- Round each mean squared error to $6$ decimal places in the output.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the bracketed comma-separated triple for a test case, in the specified order (e.g., \"[[a,b,c],[d,e,f],...]\").\n\nTest suite:\nUse the following test cases, listed in the exact order they must be evaluated and reported in the output:\n1. $(d, M) = (0, 25)$\n2. $(d, M) = (1, 25)$\n3. $(d, M) = (5, 25)$\n4. $(d, M) = (9, 25)$\n5. $(d, M) = (9, 200)$\n6. $(d, M) = (12, 1)$\n\nScientific realism and interpretation:\n- These cases are chosen to exercise both low-degree underfitting ($d=0,1$), moderate degree ($d=5$), and high-degree overfitting ($d=9,12$), as well as variance mitigation by increasing $M$ (comparing $M=25$ and $M=200$ at $d=9$). The Out-of-Bag error provides an internal estimate of the generalization error of the bagged model without using the test set.\n\nFinal output format:\n- The program must print exactly one line: a single list whose elements are the $6$ listed triples in the exact order given. Each numerical value must be rounded to $6$ decimal places and printed in decimal form.", "solution": "The problem requires the implementation and evaluation of bootstrap aggregating (bagging) for polynomial regression. We will analyze its performance by comparing the Out-of-Bag (OOB) error estimate with the test error of both a single model and the bagged ensemble across different model complexities (polynomial degrees $d$) and numbers of bootstrap fits ($M$).\n\n### Mathematical Formulation and Methodology\n\n**1. Data Generating Process**\nThe synthetic data is generated from the model:\n$$\ny = f(x) + \\varepsilon\n$$\nwhere $f(x) = \\sin(2\\pi x) + 0.5x$ is the true underlying function, and $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$ represents Gaussian noise with standard deviation $\\sigma = 0.3$. The input variable $x$ is sampled uniformly from the interval $[-1, 1]$. We generate a training set $\\mathcal{D}_{\\text{train}}$ of size $n_{\\text{train}} = 80$ and a test set $\\mathcal{D}_{\\text{test}}$ of size $n_{\\text{test}} = 400$. The generation process is made reproducible by using specified random seeds.\n\n**2. Base Learner: Polynomial Regression**\nThe base learner is a polynomial regression model of degree $d$. For a given input $x$, we construct a feature vector $\\boldsymbol{\\phi}_d(x) = [1, x, x^2, \\dots, x^d]^\\top$. The model's prediction is a linear combination of these features:\n$$\n\\hat{y}(x) = \\boldsymbol{\\phi}_d(x)^\\top \\boldsymbol{\\beta}\n$$\nThe coefficient vector $\\boldsymbol{\\beta} \\in \\mathbb{R}^{d+1}$ is determined by Ordinary Least Squares (OLS), which minimizes the sum of squared residuals on a given training set $\\{(x_i, y_i)\\}_{i=1}^n$:\n$$\n\\hat{\\boldsymbol{\\beta}} = \\arg\\min_{\\boldsymbol{\\beta}} \\sum_{i=1}^{n} \\left(y_i - \\boldsymbol{\\phi}_d(x_i)^\\top \\boldsymbol{\\beta}\\right)^2\n$$\nThe solution to this optimization problem is found via the normal equations, $\\hat{\\boldsymbol{\\beta}} = (\\mathbf{\\Phi}^\\top \\mathbf{\\Phi})^{-1} \\mathbf{\\Phi}^\\top \\mathbf{y}$, where $\\mathbf{\\Phi}$ is the design matrix whose rows are $\\boldsymbol{\\phi}_d(x_i)^\\top$. For numerical stability, this linear system is solved using methods like QR decomposition, as implemented in `numpy.linalg.lstsq`.\n\n**3. Bagging and Error Estimation**\nThe core of the problem lies in the bagging algorithm and its associated error metrics.\n\n- **Single Model Error ($\\mathrm{MSE}_{\\text{single,test}}$)**: A single polynomial regression model of degree $d$, denoted $g_{\\text{single}}(x)$, is trained on the entire training set $\\mathcal{D}_{\\text{train}}$. Its generalization performance is measured by the mean squared error on the test set:\n$$\n\\mathrm{MSE}_{\\text{single,test}} = \\frac{1}{n_{\\text{test}}} \\sum_{j=1}^{n_{\\text{test}}} \\left(y_j^{\\text{test}} - g_{\\text{single}}(x_j^{\\text{test}})\\right)^2\n$$\n\n- **Bagging and Bagged Model Error ($\\mathrm{MSE}_{\\text{bag,test}}$)**: Bagging aims to reduce the variance of unstable learners, such as high-degree polynomials. The procedure is as follows:\n    1. For $b = 1, \\dots, M$, create a bootstrap sample $\\mathcal{D}_b$ by drawing $n_{\\text{train}}$ points from $\\mathcal{D}_{\\text{train}}$ with replacement.\n    2. Train a base learner $g_b(x)$ on each $\\mathcal{D}_b$.\n    3. The final bagged predictor, $G_M(x)$, is the average of these individual predictors: $G_M(x) = \\frac{1}{M}\\sum_{b=1}^M g_b(x)$.\nThe generalization performance of the bagged model is evaluated on the test set:\n$$\n\\mathrm{MSE}_{\\text{bag,test}} = \\frac{1}{n_{\\text{test}}} \\sum_{j=1}^{n_{\\text{test}}} \\left(y_j^{\\text{test}} - G_M(x_j^{\\text{test}})\\right)^2\n$$\n\n- **Out-of-Bag (OOB) Error ($\\mathrm{MSE}_{\\text{OOB}}$)**: A key advantage of bagging is the ability to estimate generalization error without a separate validation set. For each training point $(x_i, y_i)$, its OOB prediction is the average of predictions from all models $g_b$ whose bootstrap sample $\\mathcal{D}_b$ did not include $(x_i, y_i)$. Let $\\mathcal{B}_i$ be the set of indices of such models. The OOB prediction is:\n$$\n\\hat{y}_i^{\\text{OOB}} = \\frac{1}{|\\mathcal{B}_i|}\\sum_{b \\in \\mathcal{B}_i} g_b(x_i)\n$$\nThis is defined only if $\\mathcal{B}_i$ is non-empty. The OOB mean squared error is then calculated over all training points that have at least one OOB predictor:\n$$\n\\mathrm{MSE}_{\\text{OOB}} = \\frac{1}{|\\mathcal{I}_{\\text{OOB}}|}\\sum_{i \\in \\mathcal{I}_{\\text{OOB}}} \\left(y_i - \\hat{y}_i^{\\text{OOB}}\\right)^2\n$$\nwhere $\\mathcal{I}_{\\text{OOB}} = \\{i \\mid |\\mathcal{B}_i| \\ge 1\\}$. $\\mathrm{MSE}_{\\text{OOB}}$ serves as an estimate of $\\mathrm{MSE}_{\\text{bag,test}}$.\n\n### Algorithmic Implementation\n\nThe implementation proceeds by first generating the fixed training and test datasets. Then, for each test case $(d, M)$ provided:\n1. The random seed for bootstrap sampling is set to $s_{\\text{boot}} = 100000 + 1000\\cdot d + M$ for reproducibility.\n2. A single polynomial model of degree $d$ is trained on the full dataset to compute $\\mathrm{MSE}_{\\text{single,test}}$.\n3. The bagging loop is executed $M$ times. In each iteration, a bootstrap sample is created, a model is trained, its predictions on the test set are accumulated, and its predictions for its OOB training points are also accumulated.\n4. After the loop, the accumulated OOB predictions and counts are used to compute $\\mathrm{MSE}_{\\text{OOB}}$. The accumulated test set predictions are averaged to form the final bagged prediction and compute $\\mathrm{MSE}_{\\text{bag,test}}$.\n5. The resulting triple of MSE values is rounded and stored. The final output is formatted as a list of these triples.\n\nThis procedure is repeated for all specified test cases, which are chosen to illustrate the effects of model complexity and ensemble size on bias, variance, and the effectiveness of bagging. For example, comparing the results for $(d=9, M=25)$ and $(d=9, M=200)$ will demonstrate how increasing $M$ can improve the stability and accuracy of the bagged ensemble, while comparing $\\mathrm{MSE}_{\\text{single,test}}$ and $\\mathrm{MSE}_{\\text{bag,test}}$ for $d=9$ will highlight bagging's variance reduction capability for high-variance models.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and evaluates bagging with OOB error estimation for polynomial regression.\n    \"\"\"\n    # Define constants from the problem statement\n    N_TRAIN = 80\n    N_TEST = 400\n    SIGMA = 0.3\n    S_DATA = 24680\n    S_TEST = 24681\n\n    # Define the true data generating function\n    def f(x):\n        return np.sin(2 * np.pi * x) + 0.5 * x\n\n    # Generate fixed training data for reproducibility\n    rng_data = np.random.default_rng(S_DATA)\n    x_train = rng_data.uniform(-1, 1, N_TRAIN)\n    y_train = f(x_train) + rng_data.normal(0, SIGMA, N_TRAIN)\n    \n    # Generate fixed test data for reproducibility\n    rng_test = np.random.default_rng(S_TEST)\n    x_test = rng_test.uniform(-1, 1, N_TEST)\n    y_test = f(x_test) + rng_test.normal(0, SIGMA, N_TEST)\n\n    # Helper function to create a polynomial feature matrix (design matrix)\n    def create_polynomial_features(x, degree):\n        # np.vander is a convenient way to generate powers of x.\n        # increasing=True makes the columns [x^0, x^1, ..., x^degree]\n        return np.vander(x, degree + 1, increasing=True)\n\n    # Test suite from the problem statement\n    test_cases = [\n        (0, 25),\n        (1, 25),\n        (5, 25),\n        (9, 25),\n        (9, 200),\n        (12, 1)\n    ]\n\n    all_results_str = []\n\n    # --- Pre-computation for efficiency ---\n    # Cache test design matrices and single model errors, as degrees can repeat.\n    X_test_dict = {}\n    for d, _ in test_cases:\n        if d not in X_test_dict:\n            X_test_dict[d] = create_polynomial_features(x_test, d)\n\n    single_model_mse_dict = {}\n    for d, _ in test_cases:\n        if d not in single_model_mse_dict:\n            X_train_full = create_polynomial_features(x_train, d)\n            # Use np.linalg.lstsq for a numerically stable OLS solution\n            beta_single, _, _, _ = np.linalg.lstsq(X_train_full, y_train, rcond=None)\n            y_pred_single = X_test_dict[d] @ beta_single\n            mse_single = np.mean((y_test - y_pred_single)**2)\n            single_model_mse_dict[d] = mse_single\n\n    # --- Main loop over test cases ---\n    for d, M in test_cases:\n        # Re-seed the bootstrap sampling for each case for reproducibility\n        s_boot = 100000 + 1000 * d + M\n        rng_boot = np.random.default_rng(s_boot)\n\n        # Get pre-computed values\n        X_test = X_test_dict[d]\n        mse_single_test = single_model_mse_dict[d]\n\n        # Prepare for bagging procedure\n        X_train = create_polynomial_features(x_train, d)\n        \n        oob_predictions_sum = np.zeros(N_TRAIN)\n        oob_counts = np.zeros(N_TRAIN, dtype=int)\n        bagged_test_predictions_sum = np.zeros(N_TEST)\n        train_indices = np.arange(N_TRAIN)\n\n        # Bagging loop: fit M models on bootstrap samples\n        for _ in range(M):\n            # Create a bootstrap sample of indices\n            bootstrap_indices = rng_boot.choice(train_indices, size=N_TRAIN, replace=True)\n            X_boot = X_train[bootstrap_indices]\n            y_boot = y_train[bootstrap_indices]\n\n            # Fit OLS model on the bootstrap sample\n            beta_b, _, _, _ = np.linalg.lstsq(X_boot, y_boot, rcond=None)\n\n            # Accumulate predictions on the test set for the bagged model\n            bagged_test_predictions_sum += X_test @ beta_b\n            \n            # Identify Out-of-Bag (OOB) indices for the current model\n            oob_indices = np.setdiff1d(train_indices, bootstrap_indices, assume_unique=False)\n            \n            # Accumulate predictions for OOB points\n            if oob_indices.size > 0:\n                X_oob = X_train[oob_indices]\n                y_pred_oob = X_oob @ beta_b\n                oob_predictions_sum[oob_indices] += y_pred_oob\n                oob_counts[oob_indices] += 1\n        \n        # --- Calculate and Store MSE Metrics ---\n        \n        # 1. MSE_OOB\n        # Identify indices with at least one OOB prediction (I_OOB)\n        I_oob = np.where(oob_counts > 0)[0]\n        if I_oob.size > 0:\n            y_oob_true = y_train[I_oob]\n            # Average the OOB predictions\n            y_oob_pred = oob_predictions_sum[I_oob] / oob_counts[I_oob]\n            mse_oob = np.mean((y_oob_true - y_oob_pred)**2)\n        else:\n            # This case is highly unlikely for M > 0 and should not occur here.\n            mse_oob = np.nan\n\n        # 2. MSE_single,test (already computed)\n\n        # 3. MSE_bag,test\n        # Average the predictions from all M models on the test set\n        y_pred_bagged = bagged_test_predictions_sum / M\n        mse_bag_test = np.mean((y_test - y_pred_bagged)**2)\n        \n        # Format the results for the current test case\n        result_triple = [mse_oob, mse_single_test, mse_bag_test]\n        # Round each value to 6 decimal places and format as a string\n        formatted_triple = [f\"{round(val, 6):.6f}\" for val in result_triple]\n        all_results_str.append(f\"[{','.join(formatted_triple)}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(all_results_str)}]\")\n\nsolve()\n```", "id": "3101761"}, {"introduction": "Having seen bagging's effect on polynomial regression, we now test its versatility with a different class of base learner: the $k$-Nearest Neighbors ($k$-NN) classifier. The complexity of a $k$-NN model is controlled by the neighborhood size $k$, which presents a different kind of bias-variance trade-off. This practice [@problem_id:3101765] will deepen your understanding of how bagging primarily tackles the variance component of error, showing its effectiveness for unstable learners (small $k$) and its diminishing returns for overly stable, biased learners (large $k$).", "problem": "You are asked to implement a complete experiment to study Bootstrap Aggregating (Bagging) with a $k$-Nearest Neighbors ($k$-NN) base learner and to estimate Out-of-Bag (OOB) accuracy as the neighborhood size $k$ varies. The design must start from fundamental definitions and construct all required components explicitly.\n\nFundamental definitions:\n- Bootstrap Aggregating (Bagging) constructs an ensemble by drawing $B$ bootstrap samples from a training dataset of size $N$. Each bootstrap sample is drawn by sampling $N$ indices independently and uniformly from $\\{0,1,\\dots,N-1\\}$ with replacement, forming an in-bag set. The Out-of-Bag (OOB) set for a bootstrap sample is the complement of the in-bag set within the $N$ training indices. Out-of-Bag (OOB) refers to the subset of observations that are not included in a specific bootstrap sample. OOB accuracy estimates predictive performance by aggregating predictions made only by models for which each observation was OOB.\n- The $k$-Nearest Neighbors ($k$-NN) classifier for a query point computes distances to all training points, selects the $k$ smallest distances, and predicts the class that is the majority among those $k$ neighbors. When there is a tie for the majority class among the $k$ neighbors, you must break ties deterministically by predicting class $0$.\n- Ensemble prediction aggregation for OOB estimates must be done by majority vote over all OOB predictions available for each observation across the $B$ bootstrap models. In the event of a tie in the OOB majority vote for an observation, you must break ties deterministically by predicting class $0$. Observations with no OOB predictions from any of the $B$ models must be excluded from the OOB accuracy calculation.\n\nYour task:\n- Generate a synthetic binary classification dataset in $\\mathbb{R}^2$ with $N=200$ samples. Exactly $N/2=100$ samples belong to class $0$ and $N/2=100$ samples belong to class $1$. Draw class $0$ features from a Gaussian distribution with mean $\\mu_0=(-0.8,0.0)$ and covariance matrix $\\Sigma=0.6\\cdot I_2$, and class $1$ features from a Gaussian distribution with mean $\\mu_1=(0.8,0.0)$ and the same covariance $\\Sigma=0.6\\cdot I_2$, where $I_2$ denotes the $2\\times 2$ identity matrix. Use a fixed pseudorandom generator seed $s_{\\text{data}}=42$ to ensure determinism.\n- Construct a bagging ensemble of size $B=60$ bootstrap models. For each bootstrap model, train a $k$-NN classifier on the in-bag data (including duplicates as separate training points), and produce predictions only for the OOB observations. The OOB accuracy is the fraction of correctly predicted labels among the observations that have at least one OOB prediction.\n- Implement Euclidean distance in $\\mathbb{R}^2$ and use majority vote with the deterministic tie-breaking rule described above at both the base learner level and the OOB aggregation level.\n\nTest suite:\n- Evaluate the OOB accuracy for the ensemble across the following neighborhood sizes $k$: $k\\in\\{1,2,5,15,60,200\\}$. These cases include a typical small neighborhood ($k=1$), an even neighborhood that can cause ties ($k=2$), intermediate neighborhoods ($k=5$ and $k=15$), a larger neighborhood ($k=60$), and the maximal neighborhood size equal to the bootstrap sample size ($k=200$). Use a fixed pseudorandom generator seed $s_{\\text{bag}}=123$ for all bootstrap sampling in the ensemble to ensure determinism.\n- For each $k$ in the test suite, compute the OOB accuracy as a decimal in $[0,1]$.\n\nScientific realism and derivation basis:\n- Base your design and reasoning on the definitions above and on the well-tested bias-variance decomposition principle for predictive models: the expected prediction error can be decomposed into contributions from irreducible noise, squared bias, and variance. Bagging aims to reduce variance by averaging across different bootstrap realizations of the training set.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each OOB accuracy rounded to six decimal places, ordered in the same sequence of $k$ values as specified above. For example: \"[$a_1,$$a_2,$$a_3,$$a_4,$$a_5,$$a_6$]\" where each $a_i$ is the computed OOB accuracy for the corresponding $k$.", "solution": "The problem statement has been meticulously reviewed and is determined to be valid. It is scientifically grounded, well-posed, objective, and contains all necessary information and constraints to derive a unique, verifiable solution. The task is a standard, well-defined exercise in computational statistics, requiring the implementation of the Bootstrap Aggregating (Bagging) meta-algorithm with a $k$-Nearest Neighbors ($k$-NN) base learner and the evaluation of its performance using Out-of-Bag (OOB) error estimation.\n\nThe solution will be developed by first principles, as requested. The core principle is the bias-variance trade-off. Bagging is an ensemble method designed to reduce the variance component of a model's prediction error, and it is most effective when applied to high-variance, low-bias base learners (often called \"unstable\" learners). The $k$-NN algorithm's stability is directly controlled by the neighborhood size $k$. A small $k$ (e.g., $k=1$) results in a complex decision boundary, leading to low bias but high variance. Conversely, a large $k$ increases bias by oversmoothing the decision boundary but reduces variance. This experiment investigates the interplay between the variance reduction from bagging and the intrinsic bias-variance characteristic of the $k$-NN learner as $k$ is varied.\n\nThe algorithm proceeds in the following structured steps:\n\n**1. Synthetic Data Generation**\nA dataset of $N=200$ samples in $\\mathbb{R}^2$ is generated. The dataset is balanced, with $100$ samples for class $0$ and $100$ samples for class $1$.\n- Class $0$ features are drawn from a bivariate Gaussian distribution $\\mathcal{N}(\\mu_0, \\Sigma)$, where the mean is $\\mu_0 = [-0.8, 0.0]^T$ and the covariance matrix is $\\Sigma = 0.6 \\cdot I_2$.\n- Class $1$ features are drawn from $\\mathcal{N}(\\mu_1, \\Sigma)$, where the mean is $\\mu_1 = [0.8, 0.0]^T$ and the covariance is the same $\\Sigma = 0.6 \\cdot I_2$. $I_2$ is the $2 \\times 2$ identity matrix.\nThis process is made deterministic by seeding the pseudorandom number generator with $s_{\\text{data}}=42$. The feature data is stored in a matrix $X \\in \\mathbb{R}^{200 \\times 2}$ and the corresponding labels in a vector $y \\in \\{0, 1\\}^{200}$.\n\n**2. Bagging Ensemble Construction and OOB Prediction**\nThe core of the experiment involves constructing a bagging ensemble of $B=60$ models for each specified value of $k$. The process, repeated for each $k \\in \\{1, 2, 5, 15, 60, 200\\}$, is as follows. A separate pseudorandom generator seeded with $s_{\\text{bag}}=123$ is used for all bootstrap sampling to ensure reproducibility across runs for different $k$.\nFor each of the $B=60$ iterations:\n- **Bootstrap Sampling**: A bootstrap sample is created by drawing $N=200$ indices from the set $\\{0, 1, \\dots, 199\\}$ uniformly and with replacement. These indices constitute the \"in-bag\" set for the current model. The original data points corresponding to these indices form the training set for this model. Note that this training set has size $N=200$ and typically contains duplicate instances.\n- **Out-of-Bag (OOB) Set Identification**: The OOB set for the current model consists of all data points from the original dataset that were not selected for the in-bag set.\n- **k-NN Model Training and OOB Prediction**: A $k$-NN classifier is conceptually trained on the in-bag data. For each data point in the OOB set, a prediction is made. The prediction process for a single OOB query point $\\mathbf{x}_{\\text{query}}$ is:\n    1.  Compute the Euclidean distance $d(\\mathbf{x}_{\\text{query}}, \\mathbf{x}_j)$ from the query point to every point $\\mathbf{x}_j$ in the in-bag training set.\n    2.  Identify the $k$ points in the training set with the smallest distances to $\\mathbf{x}_{\\text{query}}$. These are the $k$ nearest neighbors.\n    3.  Count the occurrences of class $0$ and class $1$ among the labels of these $k$ neighbors. Let these counts be $c_0$ and $c_1$.\n    4.  The predicted class is determined by majority vote. According to the specified tie-breaking rule, if $c_1 > c_0$, the prediction is $1$. Otherwise (if $c_0 > c_1$ or $c_0 = c_1$), the prediction is $0$.\n- **Storing OOB Predictions**: For each of the $N=200$ original data points, we maintain a running tally of its OOB predictions. Specifically, we use a $200 \\times 2$ matrix to store the vote counts for class $0$ and class $1$, and a vector of length $200$ to count how many times each point was in an OOB set.\n\n**3. OOB Accuracy Calculation**\nAfter all $B=60$ models have been processed for a given $k$, the OOB accuracy is calculated.\n- **Prediction Aggregation**: For each data point $i \\in \\{0, 1, \\dots, 199\\}$, we first check if it has received at least one OOB prediction. If not, it is excluded from the accuracy calculation. The probability of a point being in-bag for all $B=60$ models is $(1-(1-1/N)^N)^B \\approx (1-e^{-1})^{60}$, which is vanishingly small, so we expect all points to be included. For each included point, we aggregate its collected OOB predictions. Let the total votes for class $0$ be $V_0$ and for class $1$ be $V_1$.\n- **Final OOB Prediction**: The final aggregated prediction for point $i$, denoted $\\hat{y}_i^{\\text{OOB}}$, is determined by a final majority vote. In accordance with the problem's tie-breaking rule, if $V_1 > V_0$, then $\\hat{y}_i^{\\text{OOB}} = 1$. Otherwise (if $V_0 > V_1$ or $V_0 = V_1$), $\\hat{y}_i^{\\text{OOB}} = 0$.\n- **Accuracy Computation**: The OOB accuracy is the fraction of correctly classified points among those that had at least one OOB prediction. Let $S_{\\text{OOB}}$ be the set of indices of points with one or more OOB predictions. The OOB accuracy is:\n$$\n\\text{Accuracy}_{\\text{OOB}} = \\frac{1}{|S_{\\text{OOB}}|} \\sum_{i \\in S_{\\text{OOB}}} \\mathbb{I}(\\hat{y}_i^{\\text{OOB}} = y_i)\n$$\nwhere $\\mathbb{I}(\\cdot)$ is the indicator function, which is $1$ if its argument is true and $0$ otherwise.\n\nThis entire procedure is repeated for each value of $k$ in the test suite, and the resulting OOB accuracies are collected.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Implements the complete experiment to study Bagging with k-NN, evaluating\n    Out-of-Bag (OOB) accuracy for varying neighborhood sizes k.\n    \"\"\"\n    \n    # --- Problem Parameters ---\n    N = 200  # Total number of samples\n    B = 60   # Number of bootstrap models in the ensemble\n    s_data = 42  # Seed for data generation\n    s_bag = 123  # Seed for bootstrap sampling\n    mu_0 = np.array([-0.8, 0.0])\n    mu_1 = np.array([0.8, 0.0])\n    cov = np.array([[0.6, 0.0], [0.0, 0.6]])\n    k_values = [1, 2, 5, 15, 60, 200]\n\n    # --- Step 1: Generate Synthetic Dataset ---\n    rng_data = np.random.default_rng(s_data)\n    \n    # Generate N/2 samples for each class\n    n_per_class = N // 2\n    X_0 = rng_data.multivariate_normal(mu_0, cov, size=n_per_class)\n    X_1 = rng_data.multivariate_normal(mu_1, cov, size=n_per_class)\n    \n    # Combine into a single dataset\n    X = np.vstack((X_0, X_1))\n    y = np.array([0] * n_per_class + [1] * n_per_class)\n\n    oob_accuracies = []\n\n    # --- Main Loop: Iterate over k values ---\n    for k in k_values:\n        \n        # --- Step 2: Bagging and OOB Prediction Collection ---\n        # oob_preds_counts[i, j] stores the number of times observation i\n        # was predicted as class j in an OOB context.\n        oob_preds_counts = np.zeros((N, 2), dtype=int)\n        # oob_sample_counts[i] stores the number of times observation i was OOB.\n        oob_sample_counts = np.zeros(N, dtype=int)\n\n        rng_bag = np.random.default_rng(s_bag)\n        \n        for _ in range(B):\n            # C.1: Bootstrap Sampling and OOB Identification\n            in_bag_indices = rng_bag.choice(N, size=N, replace=True)\n            oob_indices = np.setdiff1d(np.arange(N), np.unique(in_bag_indices), assume_unique=True)\n            \n            # C.2: Prepare training data for the current bootstrap model\n            X_train_b = X[in_bag_indices]\n            y_train_b = y[in_bag_indices]\n            \n            # C.3: Predict for OOB points\n            for i in oob_indices:\n                query_point = X[i:i+1] # Shape (1, 2)\n                \n                # k-NN prediction logic\n                distances = np.linalg.norm(X_train_b - query_point, axis=1)\n                \n                # Get labels of k nearest neighbors\n                neighbor_indices = np.argsort(distances)[:k]\n                neighbor_labels = y_train_b[neighbor_indices]\n                \n                # Majority vote with deterministic tie-breaking for k-NN\n                votes_for_1 = np.sum(neighbor_labels)\n                votes_for_0 = k - votes_for_1\n                \n                # Predict 1 if it has a strict majority, else 0\n                prediction = 1 if votes_for_1 > votes_for_0 else 0\n                \n                # Record OOB prediction vote\n                oob_preds_counts[i, prediction] += 1\n                oob_sample_counts[i] += 1\n\n        # --- Step 3: OOB Accuracy Calculation ---\n        n_oob_samples_total = 0\n        n_oob_correct = 0\n        \n        for i in range(N):\n            # Consider only samples that were OOB at least once\n            if oob_sample_counts[i] > 0:\n                n_oob_samples_total += 1\n                \n                votes_0 = oob_preds_counts[i, 0]\n                votes_1 = oob_preds_counts[i, 1]\n                \n                # OOB aggregation majority vote with deterministic tie-breaking\n                final_oob_pred = 1 if votes_1 > votes_0 else 0\n                \n                if final_oob_pred == y[i]:\n                    n_oob_correct += 1\n\n        if n_oob_samples_total > 0:\n            oob_accuracy = n_oob_correct / n_oob_samples_total\n        else:\n            # This case is highly unlikely but included for robustness\n            oob_accuracy = 0.0\n        \n        oob_accuracies.append(oob_accuracy)\n        \n    # --- Final Output ---\n    # Format results to six decimal places as requested\n    formatted_results = [f\"{acc:.6f}\" for acc in oob_accuracies]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3101765"}, {"introduction": "Moving beyond simple error estimation, this final practice showcases a sophisticated diagnostic application of the Out-of-Bag mechanism. We can leverage OOB predictions not just in aggregate, but on a per-sample basis to scrutinize the quality of our training data. This exercise [@problem_id:3101746] guides you through implementing a system to flag potentially mislabeled data points by identifying instances where the consensus of OOB predictions strongly disagrees with the provided label, turning your bagged model into a powerful tool for data cleaning.", "problem": "You are asked to implement a procedure that uses Out-of-Bag (OOB) predictions from a bootstrap aggregation (bagging) ensemble to identify potentially mislabeled points in a binary classification dataset. The problem focuses on first constructing a mathematically well-defined synthetic dataset with injected label noise, then performing bagging with OOB estimation using a $k$-Nearest Neighbors (kNN) base learner, and finally flagging points whose OOB predictions consistently disagree with their given labels. All necessary definitions, parameters, and output requirements are specified below.\n\nFundamental base and definitions:\n- Bagging constructs an ensemble by repeatedly sampling with replacement a bootstrap training multiset from the original dataset and fitting a base learner per bootstrap sample. For each data point, the Out-of-Bag (OOB) set for a given bag is the set of indices not included in that bagâ€™s bootstrap multiset.\n- Out-of-Bag (OOB) estimation uses predictions only for those data points that were not used to train the corresponding base learner. This produces, for each data point, a collection of predictions across the bags in which the point was OOB.\n- $k$-Nearest Neighbors (kNN) classifies a query point by majority vote among its $k$ closest training points under the Euclidean metric, with ties broken toward the lower class label.\n\nDataset construction:\n- Generate a balanced binary classification dataset of $n$ points in $d$ dimensions. Class $0$ data are drawn independently from a multivariate normal distribution with mean vector $\\mu_0$ and covariance matrix equal to the $d \\times d$ identity matrix $I_d$, and class $1$ data are drawn independently from a multivariate normal distribution with mean vector $\\mu_1$ and covariance matrix $I_d$. Exactly $\\lfloor n/2 \\rfloor$ points belong to class $0$ and the remaining points belong to class $1$.\n- Let the labels be $y_i \\in \\{0,1\\}$ for $i \\in \\{1,\\dots,n\\}$. Inject label noise at rate $r$ by selecting a uniformly random subset $\\mathcal{M}$ of size $\\lfloor r n \\rfloor$ (without replacement) and flipping each selected label as $y_i \\leftarrow 1 - y_i$. The set $\\mathcal{M}$ is the ground-truth set of mislabeled points induced by injected noise.\n\nBagging with OOB predictions:\n- For $b \\in \\{1,\\dots,B\\}$:\n  - Draw a bootstrap training multiset $\\mathcal{T}_b$ of size $n$ by sampling indices from $\\{1,\\dots,n\\}$ with replacement.\n  - Define the OOB set $\\mathcal{O}_b = \\{ i \\in \\{1,\\dots,n\\} : i \\notin \\mathcal{T}_b \\}$.\n  - Train a $k$-Nearest Neighbors (kNN) classifier on $\\{(X_j, y_j): j \\in \\mathcal{T}_b\\}$ and, for each $i \\in \\mathcal{O}_b$, compute the predicted label $\\hat{y}_i^{(b)}$ using Euclidean distance, $k$ neighbors, and the tie-break rule in favor of the lower label.\n- For each index $i \\in \\{1,\\dots,n\\}$:\n  - Let $m_i$ denote the number of bags for which $i \\in \\mathcal{O}_b$ (i.e., the count of OOB predictions available for $i$).\n  - Define the aggregated OOB prediction by majority vote as $$\\hat{y}_i^{\\text{OOB}} = \\text{majority} \\big( \\{ \\hat{y}_i^{(b)} : i \\in \\mathcal{O}_b \\} \\big).$$\n  - Define the OOB disagreement fraction as $$\\delta_i = \\frac{1}{m_i} \\sum_{b : i \\in \\mathcal{O}_b} \\mathbf{1}\\{ \\hat{y}_i^{(b)} \\neq y_i \\}.$$\n\nFlagging rule and metrics:\n- Given a threshold $\\tau \\in [0,1]$ and a minimum OOB count $m_{\\min} \\in \\mathbb{N}$, flag index $i$ if $m_i \\ge m_{\\min}$ and $\\delta_i \\ge \\tau$. Let $\\mathcal{F}$ be the set of flagged indices.\n- Define the OOB error estimate for the bagged ensemble as $$E_{\\text{OOB}} = \\frac{1}{|\\{ i : m_i > 0 \\}|} \\sum_{i : m_i > 0} \\mathbf{1}\\{ \\hat{y}_i^{\\text{OOB}} \\neq y_i \\}.$$\n- Compute precision, recall, and $F_1$-score for the detection of injected noise using set $\\mathcal{M}$ of true mislabeled indices and set $\\mathcal{F}$ of flagged indices:\n  - $$P = \\begin{cases} \\frac{|\\mathcal{F} \\cap \\mathcal{M}|}{|\\mathcal{F}|},  |\\mathcal{F}| > 0, \\\\ 0,  \\text{otherwise,} \\end{cases} \\quad R = \\begin{cases} \\frac{|\\mathcal{F} \\cap \\mathcal{M}|}{|\\mathcal{M}|},  |\\mathcal{M}| > 0, \\\\ 0,  \\text{otherwise,} \\end{cases}$$\n  - $$F_1 = \\begin{cases} \\frac{2 P R}{P + R},  P + R > 0, \\\\ 0,  \\text{otherwise.} \\end{cases}$$\n- All metrics must be expressed as decimal fractions, with counts as integers.\n\nTest suite:\nYour program must evaluate the following four parameter sets. For each case, use an independent pseudorandom number generator seeded with the given seed. There are no physical units in this problem.\n1. Case $1$: $n = 120$, $d = 2$, $\\mu_0 = [-1.0, -1.0]$, $\\mu_1 = [1.0, 1.0]$, $r = 0.1$, $B = 64$, $k = 5$, $\\tau = 0.6$, $m_{\\min} = 5$, seed $= 12345$.\n2. Case $2$: $n = 120$, $d = 2$, $\\mu_0 = [-1.0, -1.0]$, $\\mu_1 = [1.0, 1.0]$, $r = 0.0$, $B = 64$, $k = 5$, $\\tau = 0.6$, $m_{\\min} = 5$, seed $= 12346$.\n3. Case $3$: $n = 120$, $d = 2$, $\\mu_0 = [-0.8, 0.8]$, $\\mu_1 = [0.8, -0.8]$, $r = 0.4$, $B = 64$, $k = 7$, $\\tau = 0.7$, $m_{\\min} = 5$, seed $= 12347$.\n4. Case $4$: $n = 30$, $d = 2$, $\\mu_0 = [-1.2, -1.2]$, $\\mu_1 = [1.2, 1.2]$, $r = 0.2$, $B = 32$, $k = 3$, $\\tau = 0.6$, $m_{\\min} = 1$, seed $= 12348$.\n\nRequired final output format:\n- For each test case, produce a result list $[E_{\\text{OOB}}, P, R, F_1, |\\mathcal{F}|]$.\n- Your program should produce a single line of output containing the results for all four test cases as a comma-separated list of these per-case lists, with no spaces, enclosed in square brackets. For example, the output must look like $$[[e_1,p_1,r_1,f_1,c_1],[e_2,p_2,r_2,f_2,c_2],[e_3,p_3,r_3,f_3,c_3],[e_4,p_4,r_4,f_4,c_4]]$$ where each $e_i$, $p_i$, $r_i$, $f_i$ is a decimal and each $c_i$ is an integer.", "solution": "The problem is assessed as valid, as it is scientifically grounded in statistical learning theory, well-posed with specific parameters and deterministic procedures, and objectively formulated. All definitions and algorithmic steps are provided, allowing for a unique and verifiable solution.\n\nThe task is to implement a procedure for identifying mislabeled data points in a synthetic binary classification dataset. This is achieved using an ensemble of $k$-Nearest Neighbors (kNN) classifiers within a bootstrap aggregation (bagging) framework. The core idea is to leverage Out-of-Bag (OOB) predictions, which are predictions for a data point made by classifiers that were not trained on that point. A high degree of disagreement between a point's given label and its OOB predictions suggests it may be mislabeled.\n\nThe procedure can be broken down into four main stages:\n\n**1. Synthetic Dataset Generation and Noise Injection**\nFirst, a controlled dataset is constructed to serve as a testbed. The dataset consists of $n$ points in a $d$-dimensional feature space.\nThe data is generated from two classes, labeled $0$ and $1$.\n- Class $0$ contains $n_0 = \\lfloor n/2 \\rfloor$ points, drawn from a multivariate normal distribution $\\mathcal{N}(\\mu_0, I_d)$, where $\\mu_0$ is the mean vector for class $0$ and $I_d$ is the $d \\times d$ identity matrix.\n- Class $1$ contains $n_1 = n - \\lfloor n/2 \\rfloor$ points, drawn from $\\mathcal{N}(\\mu_1, I_d)$.\nThe feature matrix is denoted by $X \\in \\mathbb{R}^{n \\times d}$, and the initial, correct labels are denoted by $y_{\\text{true}}$.\n\nTo simulate labeling errors, label noise is introduced. A random subset of indices $\\mathcal{M}$ of size $|\\mathcal{M}| = \\lfloor rn \\rfloor$ is selected uniformly without replacement from $\\{1, \\dots, n\\}$, where $r$ is the noise rate. For each index $i \\in \\mathcal{M}$, the true label is flipped: $y_i \\leftarrow 1 - y_i$. This creates the final, potentially noisy label vector $y$, which is used for training and evaluation. The set $\\mathcal{M}$ serves as the ground truth for mislabeled points.\n\n**2. Bagging with kNN and OOB Prediction**\nThe core of the method is a bagging ensemble of $B$ kNN classifiers. For each of the $B$ iterations (indexed by $b=1, \\dots, B$):\n- A bootstrap multiset $\\mathcal{T}_b$ is created by sampling $n$ indices from $\\{1, \\dots, n\\}$ with replacement. This set forms the training data for the $b$-th classifier.\n- The Out-of-Bag (OOB) set $\\mathcal{O}_b$ is defined as the set of indices not present in the bootstrap sample $\\mathcal{T}_b$. That is, $\\mathcal{O}_b = \\{i \\in \\{1, \\dots, n\\} \\mid i \\notin \\mathcal{T}_b\\}$.\n- A $k$-Nearest Neighbors classifier is trained on the data corresponding to $\\mathcal{T}_b$, i.e., $\\{(X_j, y_j) : j \\in \\mathcal{T}_b\\}$.\n- This classifier is then used to predict the labels for all points in the OOB set, $\\{X_i : i \\in \\mathcal{O}_b\\}$. Let $\\hat{y}_i^{(b)}$ be the prediction for point $i$. The kNN prediction is determined by a majority vote among the $k$ nearest neighbors in the training set, with distance measured by the Euclidean metric. Ties are broken in favor of the lower class label ($0$).\n\nAfter $B$ iterations, for each data point $i$, we will have a collection of OOB predictions, $\\{\\hat{y}_i^{(b)} : b \\text{ such that } i \\in \\mathcal{O}_b\\}$.\n\n**3. OOB Aggregation and Disagreement Calculation**\nFor each data point $i \\in \\{1, \\dots, n\\}$, we analyze its OOB predictions.\n- Let $m_i$ be the number of times point $i$ was in the OOB set.\n- The aggregated OOB prediction, $\\hat{y}_i^{\\text{OOB}}$, is calculated by a majority vote over its $m_i$ OOB predictions. Ties are again broken in favor of class $0$.\n- The OOB disagreement fraction, $\\delta_i$, is the fraction of OOB predictions for point $i$ that do not match its given label $y_i$:\n$$\n\\delta_i = \\frac{1}{m_i} \\sum_{b : i \\in \\mathcal{O}_b} \\mathbf{1}\\{ \\hat{y}_i^{(b)} \\neq y_i \\}\n$$\nwhere $\\mathbf{1}\\{\\cdot\\}$ is the indicator function. This value of $\\delta_i$ quantifies the evidence that point $i$ might be mislabeled. A high $\\delta_i$ indicates that ensemble members not trained on point $i$ consistently predict a label different from $y_i$.\n\n**4. Mislabeled Point Flagging and Performance Evaluation**\nA point $i$ is flagged as potentially mislabeled if it satisfies two conditions:\n1. It has a sufficient number of OOB predictions: $m_i \\ge m_{\\min}$.\n2. Its OOB disagreement fraction exceeds a certain threshold: $\\delta_i \\ge \\tau$.\nThe set of all such flagged indices is denoted by $\\mathcal{F}$.\n\nThe performance of this detection method is evaluated using standard classification metrics, comparing the flagged set $\\mathcal{F}$ against the ground-truth mislabeled set $\\mathcal{M}$:\n- **Precision ($P$)**: The fraction of flagged points that are truly mislabeled.\n$$P = \\frac{|\\mathcal{F} \\cap \\mathcal{M}|}{|\\mathcal{F}|} \\quad (\\text{if } |\\mathcal{F}| > 0 \\text{, else } 0)$$\n- **Recall ($R$)**: The fraction of truly mislabeled points that are successfully flagged.\n$$R = \\frac{|\\mathcal{F} \\cap \\mathcal{M}|}{|\\mathcal{M}|} \\quad (\\text{if } |\\mathcal{M}| > 0 \\text{, else } 0)$$\n- **$F_1$-score**: The harmonic mean of precision and recall.\n$$F_1 = \\frac{2 P R}{P + R} \\quad (\\text{if } P+R > 0 \\text{, else } 0)$$\n\nAdditionally, the overall OOB classification error of the bagged ensemble is calculated as:\n$$\nE_{\\text{OOB}} = \\frac{\\sum_{i : m_i > 0} \\mathbf{1}\\{ \\hat{y}_i^{\\text{OOB}} \\neq y_i \\}}{|\\{ i : m_i > 0 \\}|}\n$$\nThis represents the error rate of the ensemble as estimated on the OOB samples.\n\nThis entire procedure is applied to each parameter set provided in the test suite to generate the required output metrics.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.spatial.distance import cdist\n\ndef knn_predict(X_train, y_train, X_test, k):\n    \"\"\"\n    Predicts labels for X_test using k-NN trained on X_train, y_train.\n    \n    Ties in majority vote are broken in favor of the lower class label (0).\n    \"\"\"\n    if X_train.shape[0] == 0 or X_test.shape[0] == 0:\n        return np.array([])\n        \n    distances = cdist(X_test, X_train, 'euclidean')\n    \n    # Get indices of the k nearest neighbors for each test point\n    k_nearest_indices = np.argsort(distances, axis=1)[:, :k]\n    \n    # Get the labels of these neighbors\n    k_nearest_labels = y_train[k_nearest_indices]\n    \n    # Predict by majority vote. Sum of labels gives vote count for class 1.\n    # Prediction is 1 if votes_for_1 > k/2.\n    # If votes_for_1 = k/2 (including ties), prediction is 0.\n    votes_for_1 = np.sum(k_nearest_labels, axis=1)\n    predictions = (votes_for_1 > k / 2).astype(int)\n    \n    return predictions\n\ndef solve_case(n, d, mu_0, mu_1, r, B, k, tau, m_min, seed):\n    \"\"\"\n    Executes the entire procedure for a single test case.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    # 1. Dataset Generation\n    n0 = n // 2\n    n1 = n - n0\n    \n    X0 = rng.multivariate_normal(mu_0, np.identity(d), size=n0)\n    X1 = rng.multivariate_normal(mu_1, np.identity(d), size=n1)\n    \n    X = np.vstack((X0, X1))\n    y_true = np.array([0] * n0 + [1] * n1)\n    \n    # 2. Label Noise Injection\n    num_mislabeled = int(np.floor(r * n))\n    mislabeled_indices = np.array([], dtype=int)\n    y = np.copy(y_true)\n    \n    if num_mislabeled > 0:\n        indices_to_flip = rng.choice(n, size=num_mislabeled, replace=False)\n        y[indices_to_flip] = 1 - y[indices_to_flip]\n        mislabeled_indices = indices_to_flip\n        \n    # 3. Bagging and OOB Prediction\n    oob_preds_per_point = [[] for _ in range(n)]\n    \n    for _ in range(B):\n        # Create bootstrap sample and identify OOB set\n        train_indices = rng.choice(n, n, replace=True)\n        oob_indices = np.setdiff1d(np.arange(n), np.unique(train_indices))\n        \n        if len(oob_indices) == 0:\n            continue\n            \n        X_train_b, y_train_b = X[train_indices], y[train_indices]\n        X_oob = X[oob_indices]\n        \n        # Predict labels for OOB points\n        y_oob_pred = knn_predict(X_train_b, y_train_b, X_oob, k)\n        \n        # Store predictions\n        for i, idx in enumerate(oob_indices):\n            oob_preds_per_point[idx].append(y_oob_pred[i])\n            \n    # 4. Aggregation and Analysis\n    y_hat_oob_agg = np.full(n, -1, dtype=int)\n    deltas = np.full(n, -1.0, dtype=float)\n    m_counts = np.array([len(p) for p in oob_preds_per_point])\n    \n    oob_analyzed_indices = np.where(m_counts > 0)[0]\n    \n    for i in oob_analyzed_indices:\n        preds = np.array(oob_preds_per_point[i])\n        m_i = m_counts[i]\n        \n        # Aggregated OOB prediction (majority vote, tie-break to 0)\n        votes_for_1 = np.sum(preds)\n        y_hat_oob_agg[i] = 1 if votes_for_1 > m_i / 2 else 0\n        \n        # OOB disagreement fraction\n        deltas[i] = np.sum(preds != y[i]) / m_i\n\n    # 5. Flagging and Metric Calculation\n    # E_OOB calculation\n    y_target_oob = y[oob_analyzed_indices]\n    y_pred_oob_agg = y_hat_oob_agg[oob_analyzed_indices]\n    e_oob = np.sum(y_target_oob != y_pred_oob_agg) / len(oob_analyzed_indices) if len(oob_analyzed_indices) > 0 else 0.0\n    \n    # Flagging\n    flagged_indices = np.where((m_counts >= m_min)  (deltas >= tau))[0]\n    \n    # Precision, Recall, F1\n    m_set = set(mislabeled_indices)\n    f_set = set(flagged_indices)\n    \n    intersection_size = len(f_set.intersection(m_set))\n    f_size = len(f_set)\n    m_size = len(m_set)\n    \n    p = intersection_size / f_size if f_size > 0 else 0.0\n    r_val = intersection_size / m_size if m_size > 0 else 0.0\n    f1 = (2 * p * r_val) / (p + r_val) if (p + r_val) > 0 else 0.0\n    \n    return [e_oob, p, r_val, f1, f_size]\n\ndef solve():\n    test_cases = [\n        {'n': 120, 'd': 2, 'mu_0': [-1.0, -1.0], 'mu_1': [1.0, 1.0], 'r': 0.1, 'B': 64, 'k': 5, 'tau': 0.6, 'm_min': 5, 'seed': 12345},\n        {'n': 120, 'd': 2, 'mu_0': [-1.0, -1.0], 'mu_1': [1.0, 1.0], 'r': 0.0, 'B': 64, 'k': 5, 'tau': 0.6, 'm_min': 5, 'seed': 12346},\n        {'n': 120, 'd': 2, 'mu_0': [-0.8, 0.8], 'mu_1': [0.8, -0.8], 'r': 0.4, 'B': 64, 'k': 7, 'tau': 0.7, 'm_min': 5, 'seed': 12347},\n        {'n': 30, 'd': 2, 'mu_0': [-1.2, -1.2], 'mu_1': [1.2, 1.2], 'r': 0.2, 'B': 32, 'k': 3, 'tau': 0.6, 'm_min': 1, 'seed': 12348},\n    ]\n\n    all_results = []\n    for case in test_cases:\n        result = solve_case(\n            n=case['n'],\n            d=case['d'],\n            mu_0=case['mu_0'],\n            mu_1=case['mu_1'],\n            r=case['r'],\n            B=case['B'],\n            k=case['k'],\n            tau=case['tau'],\n            m_min=case['m_min'],\n            seed=case['seed']\n        )\n        all_results.append(result)\n\n    # Final print statement in the exact required format (no spaces)\n    print(str(all_results).replace(\" \", \"\"))\n\nsolve()\n```", "id": "3101746"}]}