{"hands_on_practices": [{"introduction": "The fundamental building block of any decision tree, and by extension a Random Forest, is the split. At each node, the algorithm must decide which feature and which threshold provide the 'purest' possible child nodes. This exercise [@problem_id:3166111] delves into the two most common criteria for measuring purity: Gini impurity and Shannon entropy. By computationally comparing their behavior, especially in imbalanced datasets, you will gain a deeper intuition for why one might be preferred over the other in different practical scenarios.", "problem": "You are given a binary classification setting with classes $\\{0,1\\}$. Let the class prior be $P(Y=1)=\\pi$ and $P(Y=0)=1-\\pi$, where $\\pi \\in [0,1]$. Consider a single binary candidate split based on a binary feature $X \\in \\{0,1\\}$, with class-conditional probabilities $P(X=1 \\mid Y=1)=a$ and $P(X=1 \\mid Y=0)=b$, where $a \\in [0,1]$ and $b \\in [0,1]$. The split partitions the parent node into two child nodes: the left child for $X=1$ and the right child for $X=0$.\n\nAs the fundamental base, use the following definitions:\n- The Gini impurity of a node with positive-class probability $p$ is $G(p)=2p(1-p)$.\n- The Shannon entropy (in bits) of a node with positive-class probability $p$ is $H(p)=-p\\log_2 p -(1-p)\\log_2(1-p)$, with the convention that $0\\log_2 0$ is taken to be $0$.\n- For any impurity function $I(\\cdot)$, the impurity reduction (also called information gain for entropy) when splitting a parent node with positive-class probability $p$ into two children is $I(p)-w_L I(p_L)-w_R I(p_R)$, where $w_L$ and $w_R$ are the proportions of samples going to the left and right child respectively, and $p_L$ and $p_R$ are the positive-class probabilities within the left and right child respectively.\n\nFrom these definitions, derive the expressions for $w_L$, $w_R$, $p_L$, and $p_R$ in terms of $\\pi$, $a$, and $b$, and then compute the impurity reduction for both the Gini impurity and the Shannon entropy. For a fixed pair $(a,b)$ and a given $\\pi$, denote the impurity reductions by $R_G(\\pi;a,b)$ and $R_H(\\pi;a,b)$ for Gini and entropy, respectively.\n\nYour task is to write a complete, runnable program that:\n- Uses the endpoints $\\pi=0.01$ and $\\pi=0.50$ to assess sensitivity of the split quality to rare classes, by computing the sensitivity ratios\n  $$S_G(a,b)=\\frac{R_G(0.01;a,b)}{R_G(0.50;a,b)} \\quad \\text{and} \\quad S_H(a,b)=\\frac{R_H(0.01;a,b)}{R_H(0.50;a,b)}.$$\n- If the denominator $R_G(0.50;a,b)$ is $0$, define $S_G(a,b)=0.0$. If the denominator $R_H(0.50;a,b)$ is $0$, define $S_H(a,b)=0.0$. This convention ensures well-defined outputs in degenerate cases.\n\nImplement the above using only the definitions provided, without invoking any other pre-derived results.\n\nTest suite:\n- Use the following five $(a,b)$ pairs as distinct test cases to probe different signal regimes and edge behavior:\n  - Case one (strong split): $(a,b)=(0.90,0.10)$.\n  - Case two (weak split): $(a,b)=(0.60,0.40)$.\n  - Case three (no signal boundary): $(a,b)=(0.50,0.50)$.\n  - Case four (high true positive and moderate false positive): $(a,b)=(0.99,0.49)$.\n  - Case five (rare trigger among negatives): $(a,b)=(0.30,0.01)$.\n\nRequired final output format:\n- Your program must output a single line containing a flat list of $10$ floating-point numbers in the order\n  $$[S_G(a_1,b_1), S_H(a_1,b_1), S_G(a_2,b_2), S_H(a_2,b_2), \\dots, S_G(a_5,b_5), S_H(a_5,b_5)],$$\n  where $(a_1,b_1),\\dots,(a_5,b_5)$ follow the test suite order above.\n- Each number must be printed in decimal form. There are no physical units or angles involved.\n- The list must be comma-separated and enclosed in square brackets, with no additional whitespace or text.\n\nYour program must be self-contained, require no input, and execute deterministically.", "solution": "**Derivation of intermediate quantities:**\n\nThe problem is defined within a standard Bayesian probability framework. Let $\\pi = P(Y=1)$ be the prior probability of the positive class. The feature $X$ is binary, and its class-conditional probabilities are given as $P(X=1 \\mid Y=1) = a$ and $P(X=1 \\mid Y=0) = b$. The split partitions data based on the value of $X$, with $X=1$ going to the left child and $X=0$ to the right child.\n\n1.  **Parent Node Positive-Class Probability, $p$**:\n    The parent node contains all samples before the split. Therefore, the probability of the positive class in the parent node is simply the overall prior probability of the positive class.\n    $$p = P(Y=1) = \\pi$$\n\n2.  **Child Node Proportions, $w_L$ and $w_R$**:\n    These are the marginal probabilities of observing $X=1$ (left child) and $X=0$ (right child). We find them using the law of total probability.\n    -   $w_L$ is the proportion of samples in the left child ($X=1$):\n        $$w_L = P(X=1) = P(X=1 \\mid Y=1)P(Y=1) + P(X=1 \\mid Y=0)P(Y=0)$$\n        Substituting the given values:\n        $$w_L = a\\pi + b(1-\\pi)$$\n    -   $w_R$ is the proportion of samples in the right child ($X=0$):\n        $$w_R = P(X=0) = 1 - P(X=1) = 1 - w_L$$\n        Alternatively, using the class-conditional probabilities for $X=0$, which are $P(X=0 \\mid Y=1) = 1-a$ and $P(X=0 \\mid Y=0) = 1-b$:\n        $$w_R = P(X=0 \\mid Y=1)P(Y=1) + P(X=0 \\mid Y=0)P(Y=0) = (1-a)\\pi + (1-b)(1-\\pi)$$\n        It is easy to verify that $w_L + w_R = (a\\pi + b(1-\\pi)) + ((1-a)\\pi + (1-b)(1-\\pi)) = \\pi(a+1-a) + (1-\\pi)(b+1-b) = \\pi + (1-\\pi) = 1$.\n\n3.  **Child Node Positive-Class Probabilities, $p_L$ and $p_R$**:\n    These are posterior probabilities, calculated using Bayes' theorem.\n    -   $p_L$ is the probability of the positive class in the left child, conditioned on $X=1$.\n        $$p_L = P(Y=1 \\mid X=1) = \\frac{P(X=1 \\mid Y=1)P(Y=1)}{P(X=1)}$$\n        Substituting the previously derived expressions:\n        $$p_L = \\frac{a\\pi}{w_L} = \\frac{a\\pi}{a\\pi + b(1-\\pi)}$$\n        This is defined only if $w_L > 0$. If $w_L=0$, the left child is empty, and its impurity contribution to the reduction is zero.\n    -   $p_R$ is the probability of the positive class in the right child, conditioned on $X=0$.\n        $$p_R = P(Y=1 \\mid X=0) = \\frac{P(X=0 \\mid Y=1)P(Y=1)}{P(X=0)}$$\n        Substituting the previously derived expressions:\n        $$p_R = \\frac{(1-a)\\pi}{w_R} = \\frac{(1-a)\\pi}{(1-a)\\pi + (1-b)(1-\\pi)}$$\n        This is defined only if $w_R > 0$. If $w_R=0$, the right child is empty.\n\n**Calculation of Impurity Reduction:**\n\nWith these quantities, we can compute the impurity reduction for any impurity function $I(\\cdot)$ using the formula provided:\n$$R_I(\\pi;a,b) = I(p) - [w_L I(p_L) + w_R I(p_R)]$$\nwhere $p=\\pi$.\n\n-   **For Gini Impurity**:\n    The Gini impurity is $G(p) = 2p(1-p)$. The reduction is:\n    $$R_G(\\pi;a,b) = G(\\pi) - [w_L G(p_L) + w_R G(p_R)]$$\n    $$R_G(\\pi;a,b) = 2\\pi(1-\\pi) - \\left[ w_L \\cdot 2p_L(1-p_L) + w_R \\cdot 2p_R(1-p_R) \\right]$$\n\n-   **For Shannon Entropy**:\n    The Shannon entropy is $H(p) = -p\\log_2 p - (1-p)\\log_2(1-p)$. The reduction, also known as information gain, is:\n    $$R_H(\\pi;a,b) = H(\\pi) - [w_L H(p_L) + w_R H(p_R)]$$\n    $$R_H(\\pi;a,b) = \\left( -\\pi\\log_2 \\pi - (1-\\pi)\\log_2(1-\\pi) \\right) - \\left[ w_L H(p_L) + w_R H(p_R) \\right]$$\n\nThese expressions are implemented directly in the provided program. The program computes these reduction values for $\\pi=0.01$ and $\\pi=0.50$ for each $(a, b)$ pair in the test suite and then calculates the required sensitivity ratios $S_G$ and $S_H$. The special case where the denominator is zero (which occurs when $a=b$, rendering the split uninformative) is handled by setting the ratio to $0.0$, as specified.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Calculates sensitivity ratios for Gini and Entropy impurity reductions based on a binary split.\n    \"\"\"\n\n    def shannon_entropy(p):\n        \"\"\"\n        Calculates Shannon entropy H(p) = -p*log2(p) - (1-p)*log2(1-p).\n        Handles p=0 and p=1 cases where H(p)=0.\n        \"\"\"\n        if p <= 0 or p >= 1:\n            return 0.0\n        return -p * np.log2(p) - (1 - p) * np.log2(1 - p)\n\n    def gini_impurity(p):\n        \"\"\"\n        Calculates Gini impurity G(p) = 2*p*(1-p).\n        \"\"\"\n        return 2 * p * (1 - p)\n\n    def calculate_reductions(pi, a, b):\n        \"\"\"\n        Calculates the impurity reduction for Gini and Shannon entropy.\n\n        Args:\n            pi (float): The prior probability of class 1, P(Y=1).\n            a (float): The class-conditional probability P(X=1 | Y=1).\n            b (float): The class-conditional probability P(X=1 | Y=0).\n\n        Returns:\n            tuple: A tuple containing (Gini reduction, Shannon entropy reduction).\n        \"\"\"\n        # If a=b, feature X is independent of class Y, so impurity reduction is 0.\n        if np.isclose(a, b):\n            return 0.0, 0.0\n        \n        # Parent node positive class probability is pi.\n        p_parent = pi\n\n        # Calculate proportions of samples in left (X=1) and right (X=0) children.\n        w_L = a * pi + b * (1 - pi)\n        w_R = 1.0 - w_L\n\n        # Calculate parent node impurity.\n        parent_gini = gini_impurity(p_parent)\n        parent_entropy = shannon_entropy(p_parent)\n\n        # Calculate weighted average of child node impurities.\n        gini_children = 0.0\n        entropy_children = 0.0\n\n        # Contribution from left child (X=1).\n        if w_L > 0:\n            p_L = (a * pi) / w_L\n            gini_children += w_L * gini_impurity(p_L)\n            entropy_children += w_L * shannon_entropy(p_L)\n\n        # Contribution from right child (X=0).\n        if w_R > 0:\n            p_R = ((1 - a) * pi) / w_R\n            gini_children += w_R * gini_impurity(p_R)\n            entropy_children += w_R * shannon_entropy(p_R)\n        \n        # Impurity reduction is parent impurity minus weighted child impurity.\n        reduction_gini = parent_gini - gini_children\n        reduction_entropy = parent_entropy - entropy_children\n        \n        return reduction_gini, reduction_entropy\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (0.90, 0.10), # Case one (strong split)\n        (0.60, 0.40), # Case two (weak split)\n        (0.50, 0.50), # Case three (no signal boundary)\n        (0.99, 0.49), # Case four (high true positive and moderate false positive)\n        (0.30, 0.01), # Case five (rare trigger among negatives)\n    ]\n\n    pi_rare = 0.01\n    pi_balanced = 0.50\n\n    results = []\n    for a, b in test_cases:\n        # Calculate reductions for the rare class (imbalanced) scenario.\n        R_G_rare, R_H_rare = calculate_reductions(pi_rare, a, b)\n        \n        # Calculate reductions for the balanced class scenario.\n        R_G_balanced, R_H_balanced = calculate_reductions(pi_balanced, a, b)\n\n        # Calculate sensitivity ratios S_G and S_H.\n        # Handle the case where the denominator is zero.\n        S_G = 0.0 if np.isclose(R_G_balanced, 0) else R_G_rare / R_G_balanced\n        S_H = 0.0 if np.isclose(R_H_balanced, 0) else R_H_rare / R_H_balanced\n        \n        results.extend([S_G, S_H])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3166111"}, {"introduction": "Having explored the mechanics of a single split, the next logical step is to assemble the entire Random Forest. This ambitious but highly rewarding exercise [@problem_id:3166180] guides you through building the algorithm from first principles, without relying on machine learning libraries. Implementing the key mechanisms of bootstrapping, random feature selection at each node, and the calculation of feature importance will provide an unparalleled, under-the-hood understanding of how a Random Forest operates and learns from data.", "problem": "You are asked to implement, from first principles, a binary-classification Random Forest based on Classification and Regression Trees (CART), using the Gini impurity as the split criterion, and to compute normalized feature importance scores defined as the mean decrease in impurity. Your implementation must handle edge cases where some features are constant (zero variance). You will then run the implementation on three specified test cases and report a compact numerical summary.\n\nDefinitions and requirements:\n- A decision tree is grown recursively. At each internal node with data indices $\\mathcal{I}$, you must:\n  1) Randomly select a subset of features of size $m$ (without replacement) from the $d$ available features, independently at each node.\n  2) For each selected feature $j$, consider all candidate thresholds formed as midpoints between sorted unique values of that feature among the samples in $\\mathcal{I}$. If a feature has only a single unique value, it cannot be split at this node.\n  3) For a candidate threshold $\\tau$, define the left child as the set of indices with $x_{ij} \\le \\tau$ and the right child as the set of indices with $x_{ij} &gt; \\tau$. Compute the weighted Gini impurity of this split and choose the feature and threshold that maximizes the impurity decrease. If the best impurity decrease is non-positive, make the node a leaf.\n- For any node with index set $\\mathcal{I}$ and class labels $y_i \\in \\{0,1\\}$, the Gini impurity is $G(\\mathcal{I}) = 1 - \\sum_{k \\in \\{0,1\\}} p_k^2$, where $p_k$ is the class proportion in $\\mathcal{I}$.\n- For a split of $\\mathcal{I}$ into left $\\mathcal{L}$ and right $\\mathcal{R}$ using feature $j$ and threshold $\\tau$, the impurity decrease contributed by that split is\n$$\\Delta(\\mathcal{I}, j, \\tau) = G(\\mathcal{I}) - \\frac{|\\mathcal{L}|}{|\\mathcal{I}|} G(\\mathcal{L}) - \\frac{|\\mathcal{R}|}{|\\mathcal{I}|} G(\\mathcal{R}).$$\n- A Random Forest with $T$ trees is constructed by training each tree on a bootstrap sample of size $N$ (sampling with replacement) from the original training set, and applying the node-wise random feature subspace selection of size $m$.\n- The feature importance for feature $j$ over one tree is the sum, over all internal nodes where the best split uses feature $j$, of $\\Delta(\\mathcal{I}, j, \\tau)$ weighted by the number of samples $|\\mathcal{I}|$ at that node. Aggregate this sum over all trees. Let $S$ denote the total across all features of these aggregated, sample-weighted decreases. If $S &gt; 0$, define the normalized importance of feature $j$ as $I_j = \\frac{\\text{aggregated decrease for } j}{S}$ so that $\\sum_{j=1}^{d} I_j = 1$. If $S = 0$, define $I_j = 0$ for all $j$.\n- There is no restriction on tree depth other than the implicit stopping condition when no valid split yields positive impurity decrease.\n\nImplementation constraints:\n- You must implement the above procedure exactly as stated, using pure numerical operations. No external machine learning libraries are allowed.\n- All randomness must be made reproducible using the specified seeds.\n\nDatasets and test suite:\nYou will run your implementation on three cases. In each case, construct the feature matrix $X \\in \\mathbb{R}^{N \\times d}$ and labels $y \\in \\{0,1\\}^N$ as specified, then train a Random Forest with the specified parameters, and compute the normalized feature importances $\\{I_j\\}_{j=1}^d$.\n\nCase $1$ (constant and noise features with a single informative feature):\n- Data generation seed: $42$.\n- $N = 200$, $d = 3$.\n- Generate $x_{i1} \\sim \\mathcal{N}(0,1)$ independently for $i = 1,\\dots,200$.\n- Set $x_{i2} = 0$ for all $i$ (a constant feature).\n- Generate $x_{i3} \\sim \\mathcal{N}(0,1)$ independently for all $i$.\n- Define labels $y_i = 1$ if $x_{i1} &gt; 0$ and $y_i = 0$ otherwise.\n- Random Forest parameters: number of trees $T = 50$, node-wise feature subset size $m = 2$, forest randomness seed $2024$.\n- Compute normalized importances $(I_1, I_2, I_3)$.\n\nCase $2$ (perfectly duplicated informative feature and one noise feature):\n- Data generation seed: $123$.\n- $N = 200$, $d = 3$.\n- Generate $x_{i1} \\sim \\mathcal{N}(0,1)$ independently for $i = 1,\\dots,200$.\n- Set $x_{i2} = x_{i1}$ for all $i$ (a perfect duplicate).\n- Generate $x_{i3} \\sim \\mathcal{N}(0,1)$ independently.\n- Define labels $y_i = 1$ if $x_{i1} &gt; 0$ and $y_i = 0$ otherwise.\n- Random Forest parameters: number of trees $T = 50$, node-wise feature subset size $m = 1$, forest randomness seed $2025$.\n- Compute normalized importances $(I_1, I_2, I_3)$, and also compute the duplicate-share statistic $S_{\\text{dup}} = \\frac{I_1 + I_2}{I_1 + I_2 + I_3}$.\n\nCase $3$ (all features constant):\n- Data generation seed: $7$.\n- $N = 100$, $d = 2$.\n- Set $x_{i1} = 0$ and $x_{i2} = 3$ for all $i$ (both features constant).\n- Generate labels $y_i$ as independent Bernoulli with $P(y_i = 1) = 0.5$ using the given seed.\n- Random Forest parameters: number of trees $T = 10$, node-wise feature subset size $m = 2$, forest randomness seed $99$.\n- Compute normalized importances $(I_1, I_2)$ and their sum $Q = I_1 + I_2$.\n\nNumerical reporting:\n- For Case $1$, report the three floats $I_1$, $I_2$, $I_3$.\n- For Case $2$, report the four floats $S_{\\text{dup}}$, $I_1$, $I_2$, $I_3$.\n- For Case $3$, report the three floats $Q$, $I_1$, $I_2$.\n- Round each reported float to six decimal places.\n\nFinal output format:\nYour program should produce a single line of output containing all the reported results as a comma-separated list enclosed in square brackets, in the following order:\n$[I_1^{(1)}, I_2^{(1)}, I_3^{(1)}, S_{\\text{dup}}^{(2)}, I_1^{(2)}, I_2^{(2)}, I_3^{(2)}, Q^{(3)}, I_1^{(3)}, I_2^{(3)}]$,\nwhere the superscripts indicate the case index. There are no physical units. Angles are not used. All proportions must be reported as decimals, not as percentages.", "solution": "This problem requires implementing a Random Forest classifier from first principles, adhering to a precise set of definitions for Classification and Regression Trees (CART), Gini impurity, node splitting, and a specific method for calculating feature importance. The implementation will be validated against three distinct test cases, and a set of numerical results must be reported in a specific format.\n\n### Algorithmic Implementation\n\nThe solution is structured as a set of functions that collectively implement the specified Random Forest algorithm.\n\n1.  **`gini_impurity(y)`**: A helper function to compute the Gini impurity for a given array of labels `y`.\n2.  **`find_best_split(X, y, idxs, feature_subset)`**: This function iterates through a given subset of features. For each feature, it evaluates all valid thresholds (midpoints of unique values) to find the split that maximizes the Gini gain. It returns a dictionary containing the details of the best split found (feature index, threshold, gain, and child indices) or `None` if no valid split with positive gain is found.\n3.  **`grow_tree(X, y, idxs, m, d, rng, tree_importances)`**: A recursive function that builds a single decision tree. At each step (node), it checks for stopping conditions (e.g., pure node). If not a leaf, it randomly selects `m` features, calls `find_best_split` to determine the optimal split, records the contribution to feature importance (`|\\mathcal{I}| \\cdot \\Delta`), and then recursively calls itself for the left and right children.\n4.  **`run_rf_case(X, y, T, m, forest_seed)`**: This function orchestrates the training of the entire Random Forest for a single test case. It initializes a random number generator with the forest seed. It then iterates `T` times, each time creating a bootstrap sample of the data and growing a tree using `grow_tree`. It aggregates the feature importances from all trees and returns the final normalized importance scores.\n5.  **`solve()`**: The main function that prepares the data for each of the three test cases as specified, calls `run_rf_case` with the appropriate parameters, computes the required summary statistics (`S_{\\text{dup}}`, `Q`), and prints the final combined results in the specified format.\n\nRandomness is managed carefully using `numpy.random.default_rng` to ensure reproducibility for data generation and all stochastic aspects of the algorithm (bootstrapping and feature selection). Special attention is given to the edge cases, such as handling constant features (which cannot be split) and normalizing importances only if the total impurity decrease is positive.", "answer": "```python\nimport numpy as np\n\ndef gini_impurity(y):\n    \"\"\"Computes the Gini impurity of a set of labels.\"\"\"\n    n_samples = len(y)\n    if n_samples == 0:\n        return 0.0\n    _, counts = np.unique(y, return_counts=True)\n    proportions = counts / n_samples\n    return 1.0 - np.sum(proportions**2)\n\ndef find_best_split(X, y, idxs, feature_subset):\n    \"\"\"Finds the best split for a node by iterating through features and thresholds.\"\"\"\n    X_node, y_node = X[idxs], y[idxs]\n    n_node = len(y_node)\n    \n    if n_node <= 1:\n        return None\n\n    g_parent = gini_impurity(y_node)\n    best_gain = -1.0\n    best_split_info = None\n\n    for j in feature_subset:\n        feature_values = X_node[:, j]\n        unique_vals = np.unique(feature_values)\n\n        if len(unique_vals) <= 1:\n            continue\n\n        thresholds = (unique_vals[:-1] + unique_vals[1:]) / 2.0\n\n        for tau in thresholds:\n            left_mask = feature_values <= tau\n            right_mask = ~left_mask\n\n            y_left, y_right = y_node[left_mask], y_node[right_mask]\n\n            if len(y_left) == 0 or len(y_right) == 0:\n                continue\n\n            g_left = gini_impurity(y_left)\n            g_right = gini_impurity(y_right)\n            n_left, n_right = len(y_left), len(y_right)\n            \n            w_left = n_left / n_node\n            w_right = n_right / n_node\n            \n            gain = g_parent - (w_left * g_left + w_right * g_right)\n\n            if gain > best_gain:\n                best_gain = gain\n                best_split_info = {\n                    'feature': j, \n                    'threshold': tau, \n                    'gain': gain,\n                    'left_idxs': idxs[left_mask],\n                    'right_idxs': idxs[right_mask]\n                }\n                \n    return best_split_info\n\ndef grow_tree(X, y, idxs, m, d, rng, tree_importances):\n    \"\"\"Recursively grows a single decision tree and accumulates feature importances.\"\"\"\n    if len(np.unique(y[idxs])) == 1:\n        return\n\n    m_node = min(m, d)\n    feature_subset = rng.choice(d, size=m_node, replace=False)\n    \n    best_split = find_best_split(X, y, idxs, feature_subset)\n    \n    if best_split is None or best_split['gain'] <= 0:\n        return\n\n    j = best_split['feature']\n    gain = best_split['gain']\n    n_samples = len(idxs)\n    tree_importances[j] += n_samples * gain\n    \n    left_idxs = best_split['left_idxs']\n    right_idxs = best_split['right_idxs']\n    \n    grow_tree(X, y, left_idxs, m, d, rng, tree_importances)\n    grow_tree(X, y, right_idxs, m, d, rng, tree_importances)\n\ndef run_rf_case(X, y, T, m, forest_seed):\n    \"\"\"Runs the Random Forest algorithm for a given case and computes feature importances.\"\"\"\n    N, d = X.shape\n    forest_rng = np.random.default_rng(forest_seed)\n    total_importances = np.zeros(d)\n\n    for _ in range(T):\n        bootstrap_idxs = forest_rng.choice(N, size=N, replace=True)\n        X_sample, y_sample = X[bootstrap_idxs], y[bootstrap_idxs]\n        \n        tree_importances = np.zeros(d)\n        \n        # Grow a tree on the bootstrap sample\n        initial_idxs = np.arange(len(y_sample))\n        grow_tree(X_sample, y_sample, initial_idxs, m, d, forest_rng, tree_importances)\n        \n        total_importances += tree_importances\n\n    S = np.sum(total_importances)\n    if S > 0:\n        normalized_importances = total_importances / S\n    else:\n        normalized_importances = np.zeros(d)\n        \n    return normalized_importances\n\ndef solve():\n    \"\"\"Generates data for three cases, runs the RF, and reports the results.\"\"\"\n    all_results = []\n\n    # Case 1\n    data_seed, N, d = 42, 200, 3\n    T, m, forest_seed = 50, 2, 2024\n    rng_case1 = np.random.default_rng(data_seed)\n    X1 = np.zeros((N, d))\n    X1[:, 0] = rng_case1.normal(size=N)\n    X1[:, 1] = 0.0\n    X1[:, 2] = rng_case1.normal(size=N)\n    y1 = (X1[:, 0] > 0).astype(int)\n    importances1 = run_rf_case(X1, y1, T, m, forest_seed)\n    all_results.extend(importances1)\n\n    # Case 2\n    data_seed, N, d = 123, 200, 3\n    T, m, forest_seed = 50, 1, 2025\n    rng_case2 = np.random.default_rng(data_seed)\n    X2 = np.zeros((N, d))\n    X2[:, 0] = rng_case2.normal(size=N)\n    X2[:, 1] = X2[:, 0]\n    X2[:, 2] = rng_case2.normal(size=N)\n    y2 = (X2[:, 0] > 0).astype(int)\n    importances2 = run_rf_case(X2, y2, T, m, forest_seed)\n    I1_2, I2_2, I3_2 = importances2\n    denom = I1_2 + I2_2 + I3_2\n    S_dup = (I1_2 + I2_2) / denom if denom > 0 else 0.0\n    all_results.extend([S_dup, I1_2, I2_2, I3_2])\n\n    # Case 3\n    data_seed, N, d = 7, 100, 2\n    T, m, forest_seed = 10, 2, 99\n    rng_case3 = np.random.default_rng(data_seed)\n    X3 = np.zeros((N, d))\n    X3[:, 0] = 0.0\n    X3[:, 1] = 3.0\n    y3 = rng_case3.integers(0, 2, size=N)\n    importances3 = run_rf_case(X3, y3, T, m, forest_seed)\n    I1_3, I2_3 = importances3\n    Q = I1_3 + I2_3\n    all_results.extend([Q, I1_3, I2_3])\n\n    # Format and print the final output\n    formatted_results = [f\"{r:.6f}\" for r in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3166180"}, {"introduction": "A powerful model is only as good as the data it is trained on, and one of the most insidious problems in applied machine learning is 'target leakage.' This occurs when your training data includes information that would not be available at the time of prediction. This final practice problem [@problem_id:2386893] simulates a financial credit default scenario to demonstrate how easily target leakage can occur and how a Random Forest's feature importance metric can be both a victim of this leakage and a diagnostic tool to detect it.", "problem": "You are modeling binary loan default outcomes in a credit portfolio. For each observation $i \\in \\{1,\\dots,n\\}$, let the binary target be $y_i \\in \\{0,1\\}$, where $y_i = 1$ indicates default. You will generate synthetic covariates that are economically interpretable and then add a subtle post-outcome covariate that leaks information about $y_i$. You must then quantify how an ensemble of decision stumps (a one-split decision forest) ranks the covariates by importance and report, for each test case, the zero-based index of the most important covariate. Indices must be reported as integers.\n\nData-generating process:\n- Let the number of base covariates be $p_b = 5$. For each observation $i$, draw a base feature vector $\\mathbf{x}_i \\in \\mathbb{R}^{p_b}$ from a mean-zero multivariate normal distribution with covariance matrix $\\Sigma(\\rho) \\in \\mathbb{R}^{p_b \\times p_b}$ defined by\n$$\n\\Sigma(\\rho) = (1-\\rho) I_{p_b} + \\rho \\mathbf{1}\\mathbf{1}^\\top,\n$$\nwhere $I_{p_b}$ is the identity matrix of size $p_b$ and $\\mathbf{1}$ is the $p_b$-dimensional vector of ones. The scalar $\\rho \\in (-\\frac{1}{p_b-1},1)$ controls the common correlation among base features.\n- Draw an idiosyncratic macro factor $m_i \\sim \\mathcal{N}(0,1)$ independently of $\\mathbf{x}_i$.\n- Define a latent score $s_i$ via a logistic index:\n$$\ns_i = \\beta_0 + \\sum_{j=1}^{p_b} \\beta_j x_{i,j} + \\gamma m_i,\n$$\nwith coefficients fixed at $\\beta_0 = -0.5$, $\\beta_1 = 0.8$, $\\beta_2 = -1.0$, $\\beta_3 = 0.6$, $\\beta_4 = 0.0$, $\\beta_5 = 0.5$, and $\\gamma = 0.7$.\n- Define the default probability $p_i$ by the logistic function:\n$$\np_i = \\frac{1}{1 + e^{-s_i}}.\n$$\n- Draw the binary outcome $y_i \\sim \\text{Bernoulli}(p_i)$ independently across $i$.\n- Define a post-outcome covariate $z_i$ that leaks target information as\n$$\nz_i = \\lambda \\, y_i + \\delta_i,\n$$\nwith $\\delta_i \\sim \\mathcal{N}(0,\\sigma^2)$ independent of everything else. The parameter $\\lambda \\in \\mathbb{R}$ controls the magnitude of leakage, and $\\sigma \\ge 0$ controls the amount of noise obscuring the leakage.\n- For modeling, you will form the feature vector $\\tilde{\\mathbf{x}}_i$ as follows. If the test case flag $\\text{include\\_leak} = 1$, then set\n$$\n\\tilde{\\mathbf{x}}_i = \\big[x_{i,1}, x_{i,2}, x_{i,3}, x_{i,4}, x_{i,5}, z_i\\big] \\in \\mathbb{R}^{p},\n$$\nwith $p = p_b + 1$ and the leaking covariate at zero-based index $p_b$. If $\\text{include\\_leak} = 0$, then set\n$$\n\\tilde{\\mathbf{x}}_i = \\big[x_{i,1}, x_{i,2}, x_{i,3}, x_{i,4}, x_{i,5}\\big] \\in \\mathbb{R}^{p},\n$$\nwith $p = p_b$.\n\nModel and importance:\n- Consider an ensemble of $T$ decision stumps (one-split decision trees) with $T = 200$. For each tree $t \\in \\{1,\\dots,T\\}$:\n  - Draw a bootstrap sample of size $n$ by sampling indices from $\\{1,\\dots,n\\}$ with replacement.\n  - Let $m_{\\text{try}} = \\lceil \\sqrt{p} \\rceil$ and select uniformly at random $m_{\\text{try}}$ distinct features from $\\{0,\\dots,p-1\\}$ to be considered for splitting.\n  - For each selected feature $j$, consider splits of the form $x_{j} \\le \\tau$ for thresholds $\\tau$ at midpoints between consecutive sorted observed values of that feature on the bootstrap sample, excluding thresholds that would yield an empty child node. Let $G(S)$ denote the Gini impurity of a set $S$ of binary labels:\n  $$\n  G(S) = 1 - \\sum_{c \\in \\{0,1\\}} \\left(\\frac{n_c}{|S|}\\right)^2,\n  $$\n  where $n_c$ is the count of class $c$ in $S$. For a parent node with label multiset $S$ and a split into left child $S_L$ and right child $S_R$, define the impurity decrease as\n  $$\n  \\Delta G = G(S) - \\left(\\frac{|S_L|}{|S|}G(S_L) + \\frac{|S_R|}{|S|}G(S_R)\\right).\n  $$\n  - Choose the feature $j^\\star$ and threshold $\\tau^\\star$ that maximize $\\Delta G$ among the considered features and thresholds. Grow a decision stump by splitting at $(j^\\star,\\tau^\\star)$.\n  - Attribute the realized impurity decrease $\\Delta G^\\star$ of the chosen split to feature $j^\\star$.\n- Define the importance of feature $j$ as the sum of attributed impurity decreases over the $T$ trees:\n$$\nI_j = \\sum_{t=1}^{T} \\Delta G_t \\cdot \\mathbf{1}\\{j_t^\\star = j\\}.\n$$\n- For each test case, compute the index $j_{\\max} \\in \\{0,\\dots,p-1\\}$ that maximizes $I_j$. In case of ties, take the smallest index achieving the maximum.\n\nTest suite:\nFor reproducibility, use an independent random seed $s$ per test case for both data generation and ensemble construction. Use the following four test cases, each specified by the tuple $(n,\\sigma,\\lambda,\\rho,\\text{include\\_leak}, s)$:\n- Case A: $(3000, 0.1, 0.9, 0.2, 1, 11)$.\n- Case B: $(3000, 0.3, 0.6, 0.2, 1, 12)$.\n- Case C: $(1200, 0.1, 0.9, 0.2, 0, 13)$.\n- Case D: $(3000, 0.6, 0.4, 0.2, 1, 14)$.\n\nRequired program behavior and output:\n- For each test case, generate data according to the process above, train the ensemble described above, compute feature importances $\\{I_j\\}_{j=0}^{p-1}$, and return the zero-based index $j_{\\max}$ of the most important feature.\n- Your program must produce a single line of output containing the four indices as a comma-separated list enclosed in square brackets, in the same order as the test cases, for example [$i_1$,$i_2$,$i_3$,$i_4$]. No additional text should be printed.", "solution": "The problem presented is a valid, well-posed exercise in computational statistics and machine learning, specifically concerning the evaluation of feature importance in tree-based ensemble models. The data-generating process is rigorously defined and scientifically grounded in standard models of financial econometrics. The task is to quantify feature importance using Gini impurity decrease in an ensemble of decision stumps and to identify the most impactful feature, particularly in the presence of a \"leaking\" covariate. The problem is objective, self-contained, and algorithmically specified, permitting a unique, reproducible solution.\n\nThe solution methodology proceeds in two main stages for each test case: data generation and model training with importance calculation. All mathematical entities, including variables, parameters, and numerical values, are represented in LaTeX as required.\n\n**1. Data Generation Process**\n\nFor each of the specified test cases, a synthetic dataset of size $n$ is generated according to the following stochastic process. A random seed $s$ is used to ensure reproducibility.\n\n- **Base Covariates**: A set of $p_b = 5$ base covariates, denoted by the vector $\\mathbf{x}_i \\in \\mathbb{R}^{p_b}$ for each observation $i \\in \\{1, \\dots, n\\}$, is drawn from a multivariate normal distribution $\\mathcal{N}(\\mathbf{0}, \\Sigma(\\rho))$. The covariance matrix $\\Sigma(\\rho)$ is defined as\n$$\n\\Sigma(\\rho) = (1-\\rho) I_{p_b} + \\rho \\mathbf{1}\\mathbf{1}^\\top\n$$\nwhere $I_{p_b}$ is the $p_b \\times p_b$ identity matrix and $\\mathbf{1}$ is a $p_b$-dimensional vector of ones. The parameter $\\rho$ controls the equicorrelation between these base features.\n\n- **Latent Score and Default Probability**: An idiosyncratic factor $m_i \\sim \\mathcal{N}(0,1)$ is drawn independently. A latent score $s_i$ is constructed as a linear combination of the base covariates and the macro factor:\n$$\ns_i = \\beta_0 + \\sum_{j=1}^{p_b} \\beta_j x_{i,j} + \\gamma m_i\n$$\nThe coefficients are fixed at $\\beta_0 = -0.5$, $\\boldsymbol{\\beta}_{1..p_b} = [0.8, -1.0, 0.6, 0.0, 0.5]$, and $\\gamma = 0.7$. Note that the coefficient for $x_{i,4}$ is $\\beta_4 = 0.0$, making this feature uninformative by construction with respect to the latent score. The latent score is then transformed into a probability of default, $p_i$, using the standard logistic function:\n$$\np_i = \\frac{1}{1 + e^{-s_i}}\n$$\n\n- **Binary Outcome**: The binary target variable, $y_i \\in \\{0, 1\\}$, indicating non-default ($0$) or default ($1$), is drawn from a Bernoulli distribution with the generated probability, $y_i \\sim \\text{Bernoulli}(p_i)$.\n\n- **Leaking Covariate**: A post-outcome covariate $z_i$ is generated to simulate information leakage from the target variable. Its definition is:\n$$\nz_i = \\lambda \\, y_i + \\delta_i\n$$\nwhere $\\delta_i$ is a noise term drawn from a normal distribution $\\mathcal{N}(0, \\sigma^2)$. The parameter $\\lambda$ controls the strength of the leakage, and $\\sigma$ controls the noise level. A high ratio of $|\\lambda|$ to $\\sigma$ implies a strong, easily detectable link between $z_i$ and $y_i$.\n\n- **Final Feature Matrix**: The complete feature matrix $\\tilde{\\mathbf{X}}$ is assembled. If the `include_leak` flag is $1$, the feature set is $\\tilde{\\mathbf{x}}_i = [x_{i,1}, \\dots, x_{i,5}, z_i] \\in \\mathbb{R}^{6}$. The leaking feature $z_i$ is located at the last position (zero-based index $5$). If the flag is $0$, only the base covariates are used, $\\tilde{\\mathbf{x}}_i = [x_{i,1}, \\dots, x_{i,5}] \\in \\mathbb{R}^{5}$. The number of features is denoted by $p$.\n\n**2. Feature Importance Quantification**\n\nThe importance of each feature is determined by training an ensemble of $T = 200$ decision stumps. A decision stump is a decision tree with only one split.\n\n- **Ensemble Construction**: For each of the $T$ stumps in the ensemble:\n    1. A bootstrap sample of size $n$ is created by sampling with replacement from the full dataset $(\\tilde{\\mathbf{X}}, \\mathbf{y})$.\n    2. A random subset of $m_{\\text{try}} = \\lceil \\sqrt{p} \\rceil$ distinct features is selected.\n    3. For each selected feature, the optimal split is found. A split is defined by a feature $j$ and a threshold $\\tau$. The quality of the split is measured by the Gini impurity decrease, $\\Delta G$. The Gini impurity of a set of labels $S$ is given by:\n    $$\n    G(S) = 1 - \\sum_{c \\in \\{0,1\\}} \\left(\\frac{n_c}{|S|}\\right)^2\n    $$\n    where $n_c$ is the count of class $c$ in the set $S$. The impurity decrease is the difference between the parent node's impurity and the weighted average of the two child nodes' impurities.\n    4. The feature $j^\\star$ and threshold $\\tau^\\star$ that yield the maximum impurity decrease $\\Delta G^\\star$ are chosen for the stump's split. Potential thresholds are the midpoints of consecutive unique sorted values of the feature in the bootstrap sample.\n\n- **Importance Aggregation**: The importance of a feature $j$, denoted $I_j$, is calculated as the sum of the impurity decreases it is responsible for across all trees in the ensemble:\n$$\nI_j = \\sum_{t=1}^{T} \\Delta G_t \\cdot \\mathbf{1}\\{j_t^\\star = j\\}\n$$\nwhere $\\mathbf{1}\\{\\cdot\\}$ is the indicator function.\n\n- **Result**: Finally, for each test case, the zero-based index of the feature with the highest importance score, $j_{\\max} = \\arg\\max_j I_j$, is identified. Ties are resolved by selecting the smallest index. This index is the output for the case. The procedure is implemented in Python, adhering to the specified libraries and random seeds to ensure verifiable results.", "answer": "```python\nimport numpy as np\nfrom math import ceil, sqrt\n\ndef _gini_impurity(y):\n    \"\"\"Calculates the Gini impurity of a set of binary labels.\"\"\"\n    n_samples = len(y)\n    if n_samples == 0:\n        return 0.0\n    n1_count = np.sum(y)\n    p1 = n1_count / n_samples\n    p0 = 1.0 - p1\n    return 1.0 - (p0**2 + p1**2)\n\ndef _find_best_split(X_boot, y_boot, feature_indices):\n    \"\"\"Finds the best split for a single decision stump.\"\"\"\n    n_samples = len(y_boot)\n    if n_samples <= 1:\n        return -1, -1.0\n\n    best_gain = -1.0\n    best_feature = -1\n\n    gini_parent = _gini_impurity(y_boot)\n    n1_total = np.sum(y_boot)\n    \n    for j in feature_indices:\n        feature_values = X_boot[:, j]\n        \n        unique_vals = np.unique(feature_values)\n        if len(unique_vals) < 2:\n            continue\n            \n        # Efficiently find the best split for feature j\n        sorted_indices = np.argsort(feature_values)\n        y_sorted = y_boot[sorted_indices]\n        x_sorted = feature_values[sorted_indices]\n        \n        y_cumsum = np.cumsum(y_sorted)\n\n        split_points = np.where(x_sorted[:-1] != x_sorted[1:])[0]\n\n        for i in split_points:\n            n_left = i + 1\n            n_right = n_samples - n_left\n\n            n1_left = y_cumsum[i]\n            n1_right = n1_total - n1_left\n\n            gini_left = 1.0 - ((n1_left / n_left)**2 + ((n_left - n1_left) / n_left)**2)\n            gini_right = 1.0 - ((n1_right / n_right)**2 + ((n_right - n1_right) / n_right)**2)\n            \n            gain = gini_parent - (n_left / n_samples * gini_left + n_right / n_samples * gini_right)\n\n            if gain > best_gain:\n                best_gain = gain\n                best_feature = j\n                \n    return best_feature, best_gain\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # (n, sigma, lambda, rho, include_leak, seed)\n        (3000, 0.1, 0.9, 0.2, 1, 11),\n        (3000, 0.3, 0.6, 0.2, 1, 12),\n        (1200, 0.1, 0.9, 0.2, 0, 13),\n        (3000, 0.6, 0.4, 0.2, 1, 14),\n    ]\n\n    results = []\n\n    for n, sigma, lam, rho, include_leak, seed in test_cases:\n        # Set seed for reproducibility for the entire test case\n        np.random.seed(seed)\n\n        # 1. Data Generation\n        p_b = 5\n        betas = np.array([0.8, -1.0, 0.6, 0.0, 0.5])\n        beta_0 = -0.5\n        gamma = 0.7\n        \n        # Covariance matrix\n        cov_matrix = (1 - rho) * np.identity(p_b) + rho * np.ones((p_b, p_b))\n        \n        # Base features\n        X_base = np.random.multivariate_normal(np.zeros(p_b), cov_matrix, n)\n        \n        # Macro factor\n        m = np.random.randn(n)\n        \n        # Latent score\n        s = beta_0 + X_base @ betas + gamma * m\n        \n        # Default probability\n        p_default = 1.0 / (1.0 + np.exp(-s))\n        \n        # Binary outcome\n        y = np.random.binomial(1, p_default, n)\n        \n        # Assemble final feature matrix\n        if include_leak == 1:\n            delta = np.random.normal(0, sigma, n)\n            z = lam * y + delta\n            X = np.c_[X_base, z]\n        else:\n            X = X_base\n        \n        n_samples, p = X.shape\n\n        # 2. Ensemble Training and Importance Calculation\n        T = 200\n        m_try = ceil(sqrt(p))\n        importances = np.zeros(p)\n        \n        all_indices = np.arange(n_samples)\n\n        for _ in range(T):\n            # Bootstrap sample\n            boot_indices = np.random.choice(all_indices, size=n_samples, replace=True)\n            X_boot, y_boot = X[boot_indices], y[boot_indices]\n            \n            # Feature subsampling\n            feature_indices = np.random.choice(p, size=m_try, replace=False)\n            \n            # Find best split for this stump\n            best_feature, best_gain = _find_best_split(X_boot, y_boot, feature_indices)\n            \n            if best_feature != -1:\n                importances[best_feature] += best_gain\n\n        # 3. Find the most important feature index\n        j_max = np.argmax(importances)\n        results.append(j_max)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2386893"}]}