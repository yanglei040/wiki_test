{"hands_on_practices": [{"introduction": "Understanding a model's predictive power is key, but brute-force cross-validation can be computationally expensive. This practice explores the elegant mathematical structure of GAMs, where the fitted values are a linear transformation of the observed data via a \"smoothing matrix.\" You will learn to use the properties of this matrix to perform leave-one-out cross-validation with remarkable efficiency and to estimate the model's \"optimism,\" providing deep insight into its generalization performance. [@problem_id:3123636]", "problem": "You are given a regression task under the Generalized Additive Model (GAM) framework with identity link, where the response vector is denoted by $y \\in \\mathbb{R}^n$, the covariate vector is denoted by $x \\in \\mathbb{R}^n$, and the fitted function is represented by a linear combination of fixed smooth basis functions. Concretely, consider a univariate additive model $f(x) = \\sum_{k=1}^{p} \\beta_k \\,\\phi_k(x)$ with a fixed design matrix $X \\in \\mathbb{R}^{n \\times p}$ defined by $X_{ik} = \\phi_k(x_i)$, and a positive semidefinite penalty matrix $K \\in \\mathbb{R}^{p \\times p}$ that enforces smoothness via penalized least squares. The penalized least squares estimator for the coefficients $\\beta \\in \\mathbb{R}^p$ is the unique minimizer of\n$$\n\\| y - X \\beta \\|_2^2 + \\lambda \\, \\beta^{\\top} K \\beta,\n$$\nwhere $\\lambda \\ge 0$ is the smoothing parameter. This estimator yields fitted values $\\hat{y} \\in \\mathbb{R}^n$ that can be written as a linear smoothing of the data $y$ by a matrix $S \\in \\mathbb{R}^{n \\times n$}, namely $\\hat{y} = S y$.\n\nYour task is to implement an efficient Leave-One-Out Cross-Validation (LOOCV) computation for the identity link GAM using only properties of the smoothing matrix $S$, without explicitly refitting the model $n$ times. Additionally, estimate the optimism in $\\hat{y}$ under squared error loss by exploiting the linear smoother structure and a consistent estimator of the noise variance.\n\nFoundational base to use:\n- The Generalized Additive Model (GAM) definition with identity link under penalized least squares.\n- Linear algebra properties of the penalized least squares estimator, including symmetric positive definiteness for appropriate choices of $\\lambda$ and $K$.\n- The definition of Leave-One-Out Cross-Validation (LOOCV) for squared error loss, and the concept of optimism as the expected difference between in-sample error and out-of-sample prediction error for linear smoothers.\n- The covariance identity for linear smoothers under additive independent noise with zero mean.\n\nData generation and basis specification:\n- Let $x_i$ be $n$ equally spaced points on the interval $[0,1]$, namely $x_i = \\frac{i-1}{n-1}$ for $i \\in \\{1,\\dots,n\\}$.\n- Generate $y_i$ according to $y_i = \\sin(2\\pi x_i) + \\varepsilon_i$, where angles are measured in radians, and $\\varepsilon_i$ are independent and identically distributed samples from a normal distribution with mean $0$ and standard deviation $\\sigma$.\n- Use a fixed random seed for reproducibility.\n- Construct the design matrix $X$ with $p = 2 + m$ columns using the following basis functions:\n  - $\\phi_1(x) = 1$ (intercept),\n  - $\\phi_2(x) = x$ (linear term),\n  - For $j \\in \\{1,\\dots,m\\}$ with equally spaced interior knots $c_j$ in $(0,1)$, define truncated cubic basis functions $\\phi_{2+j}(x) = \\max(x - c_j, 0)^3$.\n- Let the penalty matrix $K$ be diagonal with zeros on the first two entries (unpenalized intercept and linear term) and ones on all remaining diagonal entries (penalized truncated cubic coefficients).\n- For each test case, obtain the fitted values $\\hat{y}$ by solving the penalized normal equations, implicitly define the smoothing matrix $S$ corresponding to this penalized estimator, and then:\n  1. Compute the LOOCV predictions $\\hat{y}^{(-i)}$ for all $i \\in \\{1,\\dots,n\\}$ using only properties of $S$, without refitting $n$ times.\n  2. Compute the in-sample mean squared error $M_{\\text{in}} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$.\n  3. Compute the LOOCV mean squared error $M_{\\text{LOO}} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i^{(-i)})^2$.\n  4. Estimate the optimism in $\\hat{y}$ under squared error loss in two ways:\n     - As the difference $M_{\\text{LOO}} - M_{\\text{in}}$.\n     - By an analytic estimate derived from the linear smoother structure and a consistent estimator of the noise variance, using only properties of $S$ and the residuals.\n- Your implementation should avoid explicit construction of the full $S$ when possible, and must leverage efficient linear algebra to compute any needed diagonals or traces of $S$ from $X$, $K$, and $\\lambda$.\n\nTest suite:\nFor each tuple $(n, \\sigma, \\text{seed}, m, \\lambda)$ below, generate data and compute the four quantities described above. Angles in $\\sin(2\\pi x)$ are in radians. The test suite comprises four cases:\n- Case $1$: $(n, \\sigma, \\text{seed}, m, \\lambda) = (50, 0.2, 42, 8, 0.1)$.\n- Case $2$: $(n, \\sigma, \\text{seed}, m, \\lambda) = (50, 0.2, 42, 8, 0.0001)$.\n- Case $3$: $(n, \\sigma, \\text{seed}, m, \\lambda) = (50, 0.2, 42, 8, 10.0)$.\n- Case $4$: $(n, \\sigma, \\text{seed}, m, \\lambda) = (12, 0.15, 0, 4, 0.2)$.\n\nFinal output:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each entry corresponds to one test case and is itself a list of four floating-point numbers in the order $[M_{\\text{in}}, M_{\\text{LOO}}, M_{\\text{LOO}} - M_{\\text{in}}, \\text{Optimism}_{\\text{analytic}}]$. For example,\n$[[a_1,b_1,c_1,d_1],[a_2,b_2,c_2,d_2],[a_3,b_3,c_3,d_3],[a_4,b_4,c_4,d_4]]$,\nwhere $a_k$, $b_k$, $c_k$, and $d_k$ are the four quantities for case $k$. All numerical values in the output must be real numbers (floating-point) without any units.", "solution": "The user's request is to implement a procedure for evaluating a Generalized Additive Model (GAM) with an identity link, specifically focusing on the efficient computation of Leave-One-Out Cross-Validation (LOOCV) error and optimism estimates without refitting the model multiple times.\n\n### **Problem Validation**\n\nThe problem statement has been evaluated and is determined to be **valid**. It is scientifically grounded in the established statistical theory of linear smoothers and penalized least squares regression. The problem is well-posed, providing all necessary definitions, data generation procedures, and parameters for a unique and meaningful solution to be computed. The language is objective and the requirements are formalizable.\n\n### **Theoretical Framework**\n\nThe core of the problem lies in the properties of the linear smoother defined by the penalized least squares estimator.\n\n**1. Penalized Least Squares and the Smoothing Matrix ($S$)**\n\nGiven the objective function to minimize with respect to the coefficient vector $\\beta \\in \\mathbb{R}^p$:\n$$\nL(\\beta) = \\| y - X \\beta \\|_2^2 + \\lambda \\, \\beta^{\\top} K \\beta\n$$\nThe normal equations are found by setting the gradient $\\nabla_{\\beta} L(\\beta)$ to zero:\n$$\n(X^{\\top} X + \\lambda K) \\beta = X^{\\top} y\n$$\nThe estimator for the coefficients is:\n$$\n\\hat{\\beta} = (X^{\\top} X + \\lambda K)^{-1} X^{\\top} y\n$$\nThe fitted values $\\hat{y} \\in \\mathbb{R}^n$ are a linear transformation of the response vector $y$:\n$$\n\\hat{y} = X \\hat{\\beta} = X (X^{\\top} X + \\lambda K)^{-1} X^{\\top} y\n$$\nThis defines the smoothing matrix (or hat matrix) $S \\in \\mathbb{R}^{n \\times n}$:\n$$\nS = X (X^{\\top} X + \\lambda K)^{-1} X^{\\top}\n$$\n$S$ is a symmetric matrix since the penalty matrix $K$ is symmetric.\n\n**2. Efficient Leave-One-Out Cross-Validation (LOOCV)**\n\nLOOCV requires calculating the prediction for each observation $y_i$ from a model fitted on all data except the $i$-th point. For any linear smoother $\\hat{y} = Sy$, a well-known result allows for the direct computation of the LOOCV residuals from a single fit to the full data:\n$$\ny_i - \\hat{y}_i^{(-i)} = \\frac{y_i - \\hat{y}_i}{1 - S_{ii}}\n$$\nwhere $\\hat{y}_i^{(-i)}$ is the prediction for $y_i$ from the model fit without observation $i$, and $S_{ii}$ is the $i$-th diagonal element of the smoothing matrix $S$.\n\nThe LOOCV mean squared error, $M_{\\text{LOO}}$, can then be computed efficiently as:\n$$\nM_{\\text{LOO}} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i^{(-i)})^2 = \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\frac{y_i - \\hat{y}_i}{1 - S_{ii}} \\right)^2\n$$\nThis formula avoids refitting the model $n$ times. The main computational task is to find the diagonal elements $S_{ii}$. Explicitly forming the $n \\times n$ matrix $S$ is inefficient. Instead, we compute $S_{ii}$ as:\n$$\nS_{ii} = (X (X^{\\top} X + \\lambda K)^{-1} X^{\\top})_{ii} = X_{i, \\cdot} (X^{\\top} X + \\lambda K)^{-1} X_{i, \\cdot}^{\\top}\n$$\nwhere $X_{i, \\cdot}$ is the $i$-th row of $X$. This can be computed for all $i$ by first calculating the $p \\times p$ inverse matrix $M_{inv} = (X^{\\top} X + \\lambda K)^{-1}$ and then performing matrix-vector multiplications. An efficient implementation is `np.sum((X @ M_inv) * X, axis=1)`.\n\n**3. Optimism Estimation**\n\nOptimism quantifies how much the in-sample error underestimates the true prediction error. We are asked to estimate it in two ways.\n\n*   **Method 1: Using LOOCV ($M_{\\text{LOO}} - M_{\\text{in}}$)**\n    LOOCV provides a nearly unbiased estimate of the true prediction error. Therefore, the difference between the LOOCV error and the in-sample error serves as a direct estimate of the optimism for the given dataset.\n    $$\n    \\widehat{\\text{Optimism}}_{\\text{CV}} = M_{\\text{LOO}} - M_{\\text{in}}\n    $$\n    where $M_{\\text{in}} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$.\n\n*   **Method 2: Analytic Estimate**\n    For a linear smoother under an additive error model ($y_i = f(x_i) + \\varepsilon_i$ with $\\text{Var}(\\varepsilon_i) = \\sigma^2$), the expected optimism is given by:\n    $$\n    \\mathbb{E}[\\text{Optimism}] = \\frac{2}{n} \\text{tr}(S) \\sigma^2\n    $$\n    We can form an estimate by plugging in an estimator for $\\sigma^2$:\n    $$\n    \\widehat{\\text{Optimism}}_{\\text{analytic}} = \\frac{2}{n} \\text{tr}(S) \\hat{\\sigma}^2\n    $$\n    The term $\\text{tr}(S)$ is the *effective degrees of freedom* of the model. It can be computed efficiently without forming $S$ using the cyclic property of the trace:\n    $$\n    \\text{tr}(S) = \\text{tr}(X (X^{\\top} X + \\lambda K)^{-1} X^{\\top}) = \\text{tr}(X^{\\top} X (X^{\\top} X + \\lambda K)^{-1})\n    $$\n    A further simplification yields an even more efficient formula:\n    $$\n    \\text{tr}(S) = \\text{tr}((X^{\\top}X + \\lambda K - \\lambda K)(X^{\\top}X + \\lambda K)^{-1}) = \\text{tr}(I - \\lambda K (X^{\\top}X + \\lambda K)^{-1}) = p - \\lambda\\,\\text{tr}(K(X^{\\top}X + \\lambda K)^{-1})\n    $$\n    A consistent estimator for the noise variance $\\sigma^2$ is based on the residual sum of squares, adjusted for the effective degrees of freedom used by the fit:\n    $$\n    \\hat{\\sigma}^2 = \\frac{\\|y - \\hat{y}\\|^2}{n - \\text{tr}(S)}\n    $$\n\n### **Implementation Plan**\n\nFor each test case `(n, sigma, seed, m, lambda)`:\n1.  Generate the data $(x_i, y_i)$ as specified. $x_i$ are uniform on $[0, 1]$ and $y_i = \\sin(2\\pi x_i) + \\varepsilon_i$ with $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$.\n2.  Construct the $n \\times (2+m)$ design matrix $X$ using the intercept, linear, and truncated cubic spline basis functions with $m$ interior knots.\n3.  Construct the $(2+m) \\times (2+m)$ diagonal penalty matrix $K$ with zeros for the first two and ones for the remaining diagonal entries.\n4.  Solve for the coefficients $\\hat{\\beta}$ by computing $M_{inv} = (X^{\\top} X + \\lambda K)^{-1}$ and then $\\hat{\\beta} = M_{inv} X^{\\top} y$.\n5.  Calculate the fitted values $\\hat{y} = X\\hat{\\beta}$ and residuals $r = y - \\hat{y}$.\n6.  Compute the in-sample mean squared error $M_{\\text{in}} = \\frac{1}{n} \\sum r_i^2$.\n7.  Compute the diagonal of the smoother, $S_{ii}$, using $X$ and $M_{inv}$.\n8.  Compute the LOOCV mean squared error $M_{\\text{LOO}}$ using the formula involving $r_i$ and $S_{ii}$.\n9.  Calculate the first optimism estimate as $M_{\\text{LOO}} - M_{\\text{in}}$.\n10. Calculate the effective degrees of freedom $\\text{tr}(S)$ using the efficient trace formula.\n11. Estimate the noise variance $\\hat{\\sigma}^2$ using the residuals and $\\text{tr}(S)$.\n12. Calculate the second, analytic optimism estimate using $\\text{tr}(S)$ and $\\hat{\\sigma}^2$.\n13. Collate the four computed quantities: $[M_{\\text{in}}, M_{\\text{LOO}}, M_{\\text{LOO}} - M_{\\text{in}}, \\text{Optimism}_{\\text{analytic}}]$.\nThe final output will be a list of these lists for all test cases, formatted as a single string.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the GAM LOOCV and optimism estimation problem for a suite of test cases.\n    \"\"\"\n    test_cases = [\n        # (n, sigma, seed, m, lambda)\n        (50, 0.2, 42, 8, 0.1),\n        (50, 0.2, 42, 8, 0.0001),\n        (50, 0.2, 42, 8, 10.0),\n        (12, 0.15, 0, 4, 0.2),\n    ]\n\n    def process_case(n, sigma, seed, m, lambda_val):\n        \"\"\"\n        Processes a single GAM regression case, computing in-sample error, LOOCV error,\n        and two estimates of optimism.\n\n        Args:\n            n (int): Number of data points.\n            sigma (float): Standard deviation of the noise.\n            seed (int): Random seed for reproducibility.\n            m (int): Number of interior knots for the spline basis.\n            lambda_val (float): Smoothing parameter.\n\n        Returns:\n            list: A list containing four quantities: [M_in, M_LOO, Optimism_CV, Optimism_analytic].\n        \"\"\"\n        # Step 1: Data Generation\n        rng = np.random.default_rng(seed)\n        x = np.linspace(0.0, 1.0, n, dtype=np.float64)\n        y_true = np.sin(2 * np.pi * x)\n        epsilon = rng.normal(loc=0.0, scale=sigma, size=n)\n        y = y_true + epsilon\n\n        # Step 2: Basis and Penalty Construction\n        p = 2 + m\n        \n        # Knots are m equally spaced points in the interior of (0, 1)\n        knots = np.linspace(0.0, 1.0, m + 2, dtype=np.float64)[1:-1]\n\n        # Design matrix X\n        X = np.zeros((n, p), dtype=np.float64)\n        X[:, 0] = 1.0\n        X[:, 1] = x\n        for j in range(m):\n            X[:, 2 + j] = np.maximum(x - knots[j], 0)**3\n\n        # Penalty matrix K\n        K = np.zeros((p, p), dtype=np.float64)\n        np.fill_diagonal(K[2:, 2:], 1.0)\n        \n        # Step 3: Model Fitting\n        X_T_X = X.T @ X\n        M = X_T_X + lambda_val * K\n        M_inv = np.linalg.inv(M)\n\n        beta_hat = M_inv @ X.T @ y\n        y_hat = X @ beta_hat\n\n        # Step 4: Calculate Required Quantities\n        residuals = y - y_hat\n        \n        # In-sample Mean Squared Error (M_in)\n        M_in = np.mean(residuals**2)\n        \n        # LOOCV Mean Squared Error (M_LOO)\n        # Compute diagonal of smoother matrix S = X * (X^T*X + lambda*K)^-1 * X^T\n        diag_S = np.sum((X @ M_inv) * X, axis=1)\n\n        # LOOCV residuals and MSE\n        loo_residuals = residuals / (1.0 - diag_S)\n        M_LOO = np.mean(loo_residuals**2)\n\n        # Optimism Estimate 1: From LOOCV\n        optimism_loo = M_LOO - M_in\n\n        # Optimism Estimate 2: Analytic formula\n        # Efficiently compute trace(S) using the identity: tr(S) = p - lambda*tr(K*M_inv)\n        trace_S = p - lambda_val * np.sum(np.diag(M_inv)[2:])\n\n        rss = np.sum(residuals**2)\n        df_residuals = n - trace_S\n        \n        if df_residuals <= 1e-9:\n             optimism_analytic = np.nan \n        else:\n            sigma_hat_sq = rss / df_residuals\n            optimism_analytic = (2.0 / n) * trace_S * sigma_hat_sq\n\n        return [M_in, M_LOO, optimism_loo, optimism_analytic]\n        \n    results = []\n    for case in test_cases:\n        result = process_case(*case)\n        results.append(result)\n\n    # The format '[[a,b,c,d],[e,f,g,h]]' is produced by str(list_of_lists) with spaces removed.\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n```", "id": "3123636"}, {"introduction": "The flexibility of Generalized Additive Models is a great strength, but it can lead to unreliable behavior when making predictions far outside the range of the training data. This hands-on exercise demonstrates how the polynomial components of splines can cause extreme and overconfident extrapolations in a logistic GAM. Through implementing and comparing different penalization strategies, you will learn how to regularize your model to ensure more stable and scientifically plausible predictions in these uncharted territories. [@problem_id:3123638]", "problem": "You are asked to study extrapolation behavior in a logistic Generalized Additive Model (GAM) for a single predictor. The model is defined by the Bernoulli likelihood with canonical logit link: for labeled data $\\{(x_i, y_i)\\}_{i=1}^n$, with $y_i \\in \\{0,1\\}$, the conditional mean $\\mu_i = \\mathbb{E}[y_i \\mid x_i]$ is modeled as $g(\\mu_i) = \\eta_i$, where $g(\\cdot)$ is the logit function $g(p) = \\log\\left(\\frac{p}{1-p}\\right)$ and $\\eta_i = \\beta_0 + f(x_i)$ is the additive predictor that includes an unknown smooth function $f(\\cdot)$. The function $f(\\cdot)$ is to be represented using a cubic regression spline basis. The objective is to quantify how the fitted classification probabilities $\\hat{p}(x)$ behave for predictor values $x$ far outside the training domain and to control this behavior using priors or penalties to avoid extreme probabilities near $0$ or $1$.\n\nStart from the following fundamental base:\n- The Bernoulli log-likelihood for a single observation is $\\ell(\\mu; y) = y \\log(\\mu) + (1-y) \\log(1-\\mu)$ with $\\mu = \\mathbb{E}[y]$.\n- The canonical link for logistic regression is the logit, $g(\\mu) = \\log\\left(\\frac{\\mu}{1-\\mu}\\right)$, so the linear predictor $\\eta$ satisfies $\\mu = \\frac{1}{1 + e^{-\\eta}}$.\n- Iteratively Reweighted Least Squares (IRLS) for generalized linear models uses weighted least squares updates derived from the second-order Taylor approximation of the log-likelihood in the linear predictor.\n\nRepresent the smooth function $f(x)$ using the cubic regression spline truncated power basis with interior knots. Specifically, let the basis functions be\n- $b_0(x) = 1$,\n- $b_1(x) = x$,\n- $b_2(x) = x^2$,\n- $b_3(x) = x^3$,\n- and, for interior knots $c_j$, $b_{3+j}(x) = \\max(x - c_j, 0)^3$.\nThen $f(x) = \\sum_{j} \\beta_j b_j(x)$, where the $\\beta_j$ are coefficients to be estimated by maximizing the penalized log-likelihood.\n\nTo avoid extreme $\\hat{p}(x)$ for $x$ far from the training data, impose a Gaussian prior on the coefficients, equivalent to a ridge penalty in the IRLS normal equations. The penalty is a diagonal matrix with nonnegative entries for the non-intercept coefficients. Stronger penalties on higher-degree polynomial terms $x^2$, $x^3$, and the truncated cubic terms enforce more conservative tails and reduce the tendency of the log-odds to grow unbounded in magnitude outside the training region.\n\nYour program must:\n- Generate synthetic training data of size $n = 200$ with predictor values $x_i$ drawn uniformly from the interval $[-2,2]$, using a fixed random seed $s = 42$ to ensure reproducibility. The true data-generating model is $y_i \\sim \\text{Bernoulli}(p_i)$ with\n$$\np_i = \\frac{1}{1 + \\exp\\left(-\\left(\\alpha + f^*(x_i)\\right)\\right)}, \\quad \\alpha = 0.2,\\quad f^*(x) = 2 \\sin(x).\n$$\n- Construct the cubic regression spline basis with interior knots at $c_1 = -1.5$, $c_2 = -0.5$, $c_3 = 0.5$, and $c_4 = 1.5$.\n- Fit a penalized logistic GAM using IRLS. In each IRLS step, solve the weighted normal equations with a diagonal ridge penalty whose entries encode a Gaussian prior on $\\beta$ with zero mean and chosen variances. The intercept $\\beta_0$ must not be penalized.\n- For a given penalty configuration, compute the fitted probabilities $\\hat{p}(x)$ at the four extrapolation points $x = -10$, $x = -6$, $x = 6$, and $x = 10$ and return them as floats.\n\nTest Suite:\nRun the following four test cases, each corresponding to a different diagonal penalty configuration (list entries correspond to the coefficients in order $[b_0,b_1,b_2,b_3,b_{4},b_{5},b_{6},b_{7}]$):\n- Case A (no penalty beyond numerical stabilization): $[0, 0, 0, 0, 0, 0, 0, 0]$.\n- Case B (uniform mild penalty on all non-intercept terms): $[0, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]$.\n- Case C (strong penalty on higher-degree terms with lighter penalty on the linear term): $[0, 0.1, 100, 100, 10, 10, 10, 10]$.\n- Case D (very strong penalty on all non-intercept terms): $[0, 1000, 1000, 1000, 1000, 1000, 1000, 1000]$.\n\nFor each case, fit the model and compute the list of four floats $[\\hat{p}(-10), \\hat{p}(-6), \\hat{p}(6), \\hat{p}(10)]$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results for all four cases as a comma-separated list of lists of floats, with no spaces, enclosed in square brackets. For example: $[[p_{A,-10},p_{A,-6},p_{A,6},p_{A,10}],[p_{B,-10},p_{B,-6},p_{B,6},p_{B,10}],[p_{C,-10},p_{C,-6},p_{C,6},p_{C,10}],[p_{D,-10},p_{D,-6},p_{D,6},p_{D,10}]]$. The units are pure numbers in $[0,1]$ interpreted as probabilities, expressed as decimals.", "solution": "The problem requires the implementation and analysis of a penalized logistic Generalized Additive Model (GAM) for binary classification. The primary objective is to study the model's extrapolation behavior by fitting it to synthetic data and evaluating the predicted probabilities at points far outside the training data range under different regularization schemes.\n\n### Model Specification\nThe model for a binary response $y_i \\in \\{0, 1\\}$ given a single predictor $x_i$ is based on the Bernoulli distribution. The conditional mean $\\mu_i = \\mathbb{E}[y_i | x_i]$, which is the probability $p(y_i=1|x_i)$, is related to a linear predictor $\\eta_i$ via the logit link function:\n$$\ng(\\mu_i) = \\log\\left(\\frac{\\mu_i}{1-\\mu_i}\\right) = \\eta_i\n$$\nInverting the link function gives the probability as a function of the linear predictor:\n$$\n\\mu_i = \\frac{1}{1 + e^{-\\eta_i}}\n$$\nThe linear predictor $\\eta_i$ is modeled as an additive combination of an intercept $\\beta_0$ and a smooth function $f(x_i)$:\n$$\n\\eta_i = \\beta_0 + f(x_i)\n$$\n\n### Spline Basis Representation\nThe smooth function $f(x)$ is represented using a cubic regression spline with a truncated power basis. This allows us to express the non-linear function $f(x)$ as a linear combination of basis functions. The full linear predictor $\\eta(x)$ can be written as a linear model $\\eta(x) = \\mathbf{B}(x)^T \\boldsymbol{\\beta}$, where $\\mathbf{B}(x)$ is a vector of basis functions evaluated at $x$ and $\\boldsymbol{\\beta}$ is the vector of coefficients to be estimated.\n\nFor this problem, the basis vector for a predictor value $x$ is defined as:\n$$\n\\mathbf{B}(x) = [1, x, x^2, x^3, (x - c_1)_+^3, (x - c_2)_+^3, (x - c_3)_+^3, (x - c_4)_+^3]^T\n$$\nwhere $(u)_+ = \\max(u, 0)$ is the positive part function. The basis includes a constant term for the intercept $\\beta_0$, polynomial terms up to degree $3$ ($x$, $x^2$, $x^3$), and one truncated cubic term for each of the $4$ interior knots $c_j$. The specified knots are $c_1=-1.5$, $c_2=-0.5$, $c_3=0.5$, and $c_4=1.5$. This results in an $8$-dimensional coefficient vector $\\boldsymbol{\\beta} = [\\beta_0, \\beta_1, \\dots, \\beta_7]^T$. For a set of $n$ observations $\\{x_i\\}_{i=1}^n$, we form the $n \\times 8$ design matrix $X$, where the $i$-th row is $\\mathbf{B}(x_i)^T$. The vector of linear predictors for all observations is then $\\boldsymbol{\\eta} = X\\boldsymbol{\\beta}$.\n\n### Penalized Estimation via Iteratively Reweighted Least Squares (IRLS)\nThe coefficients $\\boldsymbol{\\beta}$ are estimated by maximizing a penalized log-likelihood. The Bernoulli log-likelihood for $n$ observations is:\n$$\n\\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^n \\left( y_i \\eta_i - \\log(1 + e^{\\eta_i}) \\right)\n$$\nTo control model complexity and prevent overfitting, particularly the erratic behavior of high-degree polynomials outside the training range, a penalty is added. This penalty is equivalent to imposing a zero-mean Gaussian prior on the coefficients. The intercept $\\beta_0$ is not penalized. The penalized log-likelihood is:\n$$\n\\ell_p(\\boldsymbol{\\beta}) = \\ell(\\boldsymbol{\\beta}) - \\frac{1}{2}\\sum_{j=1}^{7} \\lambda_j \\beta_j^2 = \\ell(\\boldsymbol{\\beta}) - \\frac{1}{2}\\boldsymbol{\\beta}^T S \\boldsymbol{\\beta}\n$$\nwhere $S$ is a diagonal penalty matrix with diagonal entries $[\\lambda_0, \\lambda_1, \\dots, \\lambda_7]$, and we set $\\lambda_0 = 0$.\n\nThe maximization of $\\ell_p(\\boldsymbol{\\beta})$ is performed using the Iteratively Reweighted Least Squares (IRLS) algorithm. IRLS is an application of Newton's method. At each iteration $(t)$, it solves a penalized weighted least squares problem to update the coefficient vector. The update rule is:\n$$\n\\boldsymbol{\\beta}^{(t+1)} = (X^T W^{(t)} X + S)^{-1} X^T W^{(t)} \\mathbf{z}^{(t)}\n$$\nHere:\n- $\\boldsymbol{\\beta}^{(t)}$ is the coefficient vector from the previous iteration.\n- $W^{(t)}$ is a diagonal matrix of weights with entries $W_{ii}^{(t)} = \\mu_i^{(t)}(1-\\mu_i^{(t)})$, where $\\mu_i^{(t)}$ are the fitted probabilities calculated using $\\boldsymbol{\\beta}^{(t)}$.\n- $\\mathbf{z}^{(t)}$ is the working response vector with elements $z_i^{(t)} = \\eta_i^{(t)} + \\frac{y_i - \\mu_i^{(t)}}{\\mu_i^{(t)}(1-\\mu_i^{(t)})}$.\n\nThe algorithm is initialized with $\\boldsymbol{\\beta}^{(0)} = \\mathbf{0}$ and run for a fixed number of iterations.\n\n### Implementation and Analysis\nThe procedure begins by generating synthetic training data of size $n=200$. Predictor values $x_i$ are drawn from a uniform distribution on $[-2, 2]$. The binary outcomes $y_i$ are sampled from a Bernoulli distribution with true probabilities $p_i = (1 + \\exp(-(\\alpha + f^*(x_i))))^{-1}$, where $\\alpha=0.2$ and $f^*(x) = 2\\sin(x)$. A fixed random seed $s=42$ ensures reproducibility.\n\nFor each of the four specified penalty configurations (Cases A-D), the IRLS algorithm is executed to find the estimated coefficient vector $\\hat{\\boldsymbol{\\beta}}$. The configurations represent:\n- Case A ($[0, 0, 0, 0, 0, 0, 0, 0]$): No regularization.\n- Case B ($[0, 0.5, \\dots, 0.5]$): A mild, uniform penalty on all non-intercept terms.\n- Case C ($[0, 0.1, 100, 100, 10, \\dots, 10]$): A strong penalty on quadratic, cubic, and spline terms, with a weak penalty on the linear term.\n- Case D ($[0, 1000, \\dots, 1000]$): A very strong penalty on all non-intercept terms, forcing the model towards a constant log-odds (intercept-only).\n\nAfter fitting, the models are used to predict probabilities $\\hat{p}(x)$ at the extrapolation points $x \\in \\{-10, -6, 6, 10\\}$. This is done by first constructing the design matrix $X_{extrap}$ for these points, then computing the log-odds $\\boldsymbol{\\eta}_{extrap} = X_{extrap} \\hat{\\boldsymbol{\\beta}}$, and finally applying the inverse logit function $\\hat{\\mathbf{p}}_{extrap} = (1 + \\exp(-\\boldsymbol{\\eta}_{extrap}))^{-1}$. Comparing the results from the four cases demonstrates how increasing the penalty on higher-order terms tames the extrapolation behavior, pulling the predicted probabilities away from the extremes of $0$ and $1$.", "answer": "```python\nimport numpy as np\n\ndef build_design_matrix(x, knots):\n    \"\"\"\n    Constructs the design matrix for a cubic regression spline with a truncated power basis.\n\n    Args:\n        x (np.ndarray): 1D array of predictor values.\n        knots (np.ndarray): 1D array of interior knot locations.\n\n    Returns:\n        np.ndarray: The design matrix X.\n    \"\"\"\n    n_obs = len(x)\n    n_knots = len(knots)\n    # Total columns: 1 (intercept) + 3 (polynomial terms) + n_knots\n    X = np.zeros((n_obs, 4 + n_knots))\n    \n    X[:, 0] = 1.0\n    X[:, 1] = x\n    X[:, 2] = x**2\n    X[:, 3] = x**3\n    \n    for i, knot in enumerate(knots):\n        X[:, 4 + i] = np.maximum(x - knot, 0)**3\n        \n    return X\n\n\ndef fit_penalized_gam(X, y, penalty_diag, n_iter=25):\n    \"\"\"\n    Fits a penalized logistic GAM using Iteratively Reweighted Least Squares (IRLS).\n\n    Args:\n        X (np.ndarray): The design matrix.\n        y (np.ndarray): The binary response vector.\n        penalty_diag (np.ndarray): The diagonal entries of the penalty matrix.\n        n_iter (int): The number of IRLS iterations.\n\n    Returns:\n        np.ndarray: The estimated coefficient vector beta.\n    \"\"\"\n    n_features = X.shape[1]\n    beta = np.zeros(n_features)\n    S = np.diag(penalty_diag)\n    \n    # Epsilon to prevent mu=0 or mu=1, which cause infinite weights/responses\n    epsilon = 1e-8\n\n    for _ in range(n_iter):\n        eta = X @ beta\n        \n        # Clip eta to avoid numerical overflow in exp()\n        eta = np.clip(eta, -25, 25)\n\n        mu = 1.0 / (1.0 + np.exp(-eta))\n        \n        # Clip mu to prevent weights from becoming exactly zero\n        mu = np.clip(mu, epsilon, 1.0 - epsilon)\n        \n        weights = mu * (1.0 - mu)\n        z = eta + (y - mu) / weights\n        \n        # Efficiently compute the left-hand side and right-hand side of the normal equations\n        # This avoids forming the full n x n weight matrix W\n        X_w = X * weights[:, np.newaxis]\n        lhs = X.T @ X_w + S\n        rhs = X.T @ (weights * z)\n        \n        try:\n            beta = np.linalg.solve(lhs, rhs)\n        except np.linalg.LinAlgError:\n            # Add a small ridge for numerical stability if lhs is singular.\n            # This is relevant for the unpenalized case (Case A).\n            beta = np.linalg.solve(lhs + np.eye(n_features) * 1e-8, rhs)\n\n    return beta\n\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation, fit models for each test case,\n    and generate the final output in the required format.\n    \"\"\"\n    # Define the four test cases for the penalty vector\n    test_cases = [\n        [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n        [0.0, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5],\n        [0.0, 0.1, 100.0, 100.0, 10.0, 10.0, 10.0, 10.0],\n        [0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0]\n    ]\n\n    # --- Problem constants and data generation ---\n    n_samples = 200\n    seed = 42\n    alpha = 0.2\n    knots = np.array([-1.5, -0.5, 0.5, 1.5])\n    extrapolation_points = np.array([-10.0, -6.0, 6.0, 10.0])\n\n    # Generate synthetic training data for reproducibility\n    rng = np.random.default_rng(seed)\n    x_train = rng.uniform(-2.0, 2.0, n_samples)\n    true_log_odds = alpha + 2.0 * np.sin(x_train)\n    p_true = 1.0 / (1.0 + np.exp(-true_log_odds))\n    y_train = rng.binomial(1, p_true, n_samples)\n    \n    # --- Construct design matrices ---\n    X_train = build_design_matrix(x_train, knots)\n    X_extrap = build_design_matrix(extrapolation_points, knots)\n\n    all_results = []\n    \n    # --- Loop through test cases, fit model, and predict ---\n    for penalty_values in test_cases:\n        penalty_diag = np.array(penalty_values)\n        \n        # Fit the penalized GAM model using IRLS\n        beta_hat = fit_penalized_gam(X_train, y_train, penalty_diag)\n        \n        # Predict probabilities at extrapolation points\n        eta_extrap = X_extrap @ beta_hat\n        p_extrap = 1.0 / (1.0 + np.exp(-eta_extrap))\n        \n        all_results.append(list(p_extrap))\n        \n    # --- Format final output string ---\n    # The required format is [[...],[...],...] with no spaces and decimal float representation.\n    final_output = str(all_results).replace(\" \", \"\")\n    print(final_output)\n\nsolve()\n```", "id": "3123638"}, {"introduction": "A core assumption in any GAM is the choice of the link function, which connects the model's additive predictor to the expected outcome. A mismatched link function can lead to systematic biases and flawed inferences. In this simulation-based practice, you will play the role of a statistical detective: after generating data with a specific link function, you will fit a model using an incorrect one and learn to identify the characteristic patterns of misspecification that appear in the residuals. [@problem_id:3123718]", "problem": "Consider generalized additive models for regression and classification, where a Generalized Additive Model (GAM) represents the conditional mean of a response as an additive function of covariates transformed by a link function. For binary outcomes, a GAM within the framework of a Generalized Linear Model (GLM) specifies a link function $g$ that connects the conditional mean $E[Y \\mid \\mathbf{x}] = \\mu(\\mathbf{x})$ to an additive predictor $\\eta(\\mathbf{x}) = \\beta_0 + \\sum_{j=1}^p f_j(x_j)$, where each $f_j$ is a smooth function represented via a fixed basis expansion and a penalty to control smoothness. The fundamental base for estimation is maximum likelihood for the Bernoulli distribution combined with Fisher scoring (Iterative Reweighted Least Squares), starting from the log-likelihood $L(\\mu \\mid y) = \\sum_{i=1}^n \\left\\{ y_i \\log \\mu_i + (1 - y_i) \\log (1 - \\mu_i) \\right\\}$ and the GLM structure $g(\\mu_i) = \\eta_i$ with variance function $V(\\mu_i) = \\mu_i (1 - \\mu_i)$.\n\nYour task is to diagnose link misspecification by simulating data from a complementary log-log link and fitting a logit GAM; then evaluate systematic residual patterns that reveal misspecification.\n\nImplement the following steps:\n\n1. Data generation:\n   - Let $n$ denote the sample size and define two covariates $x_1$ and $x_2$ drawn independently from a uniform distribution on $[0,1]$.\n   - Define smooth functions $f_1(x_1) = 1.2 \\sin(2 \\pi x_1) + 0.3(x_1 - 0.5)$ and $f_2(x_2) = -0.8 \\cos(2 \\pi x_2) + 0.5(x_2 - 0.5)^2$.\n   - Let a scale parameter $s$ control signal strength, and set the additive predictor $\\eta_{\\text{true}} = s \\cdot \\left( f_1(x_1) + f_2(x_2) \\right)$.\n   - Use the complementary log-log inverse link to obtain probabilities $p_i = 1 - \\exp\\left( -\\exp\\left( \\eta_{\\text{true}, i} \\right) \\right)$, then sample $y_i \\sim \\text{Bernoulli}(p_i)$ independently.\n\n2. Model fitting:\n   - Represent each $f_j$ by a truncated cubic spline basis with knots at fixed positions $0.2$, $0.4$, $0.6$, and $0.8$ on $[0,1]$, using the basis functions $\\{x, x^2, x^3, (x - \\kappa_m)_+^3\\}_{m=1}^4$, where $(u)_+ = \\max\\{u, 0\\}$.\n   - Build a design matrix with an intercept and the basis expansions for $x_1$ and $x_2$ (no interaction terms).\n   - Fit a penalized GLM using Fisher scoring (Iterative Reweighted Least Squares), where at each iteration you solve a penalized weighted-least-squares system derived from the Bernoulli log-likelihood and the chosen link function $g$.\n     - For a general link $g$, define $\\eta_i = g(\\mu_i)$, the derivative $d\\mu_i/d\\eta_i$, the variance $V(\\mu_i) = \\mu_i (1 - \\mu_i)$, the weights $w_i = \\left( d\\mu_i/d\\eta_i \\right)^2 / V(\\mu_i)$, and the working response $z_i = \\eta_i + (y_i - \\mu_i) / (d\\mu_i/d\\eta_i)$.\n     - Solve the penalized normal equations\n     $$\n     \\left( X^\\top W X + \\lambda P \\right) \\boldsymbol{\\beta}^{\\text{new}} = X^\\top W \\mathbf{z},\n     $$\n     where $W$ is diagonal with entries $w_i$, $\\lambda > 0$ is a ridge penalty parameter, and $P$ is the identity matrix with zero in the intercept position.\n   - Fit models with two link functions:\n     - The logit inverse link $\\mu_i = \\sigma(\\eta_i) = 1 / \\left( 1 + \\exp(-\\eta_i) \\right)$.\n     - The complementary log-log inverse link $\\mu_i = 1 - \\exp\\left( -\\exp(\\eta_i) \\right)$.\n\n3. Residual diagnostics:\n   - Compute Bernoulli deviance residuals \n   $$\n   r_i = \\operatorname{sign}(y_i - \\hat{\\mu}_i) \\sqrt{2 \\left( y_i \\log\\left( \\frac{y_i}{\\hat{\\mu}_i} \\right) + (1 - y_i) \\log\\left( \\frac{1 - y_i}{1 - \\hat{\\mu}_i} \\right) \\right)},\n   $$\n   using the fitted mean $\\hat{\\mu}_i$ from the model and appropriate numerical safeguards for boundary values.\n   - Quantify systematic patterns using:\n     - The absolute Pearson correlation between residuals and the true additive predictor, $|\\operatorname{corr}(r, \\eta_{\\text{true}})|$.\n     - The coefficient of determination $R^2$ from regressing $r$ on the spline basis for $x_1$ (including an intercept).\n     - The coefficient of determination $R^2$ from regressing $r$ on the spline basis for $x_2$ (including an intercept).\n   - Report these three floating-point diagnostics for each test case.\n\nUse the following test suite of parameter settings to exercise the procedure:\n- Case $1$: $n = 800$, $s = 1.0$, random seed $42$, fit with a logit link to misspecified data generated from the complementary log-log link.\n- Case $2$: $n = 800$, $s = 1.0$, random seed $42$, fit with a complementary log-log link to correctly specified data generated from the complementary log-log link.\n- Case $3$: $n = 800$, $s = 2.5$ (stronger signal; probabilities near $0$ or $1$ more frequently), random seed $123$, fit with a logit link to misspecified data generated from the complementary log-log link.\n- Case $4$: $n = 800$, $s = 0.2$ (weaker signal; probabilities clustered near $0.5$), random seed $7$, fit with a logit link to misspecified data generated from the complementary log-log link.\n\nYour program should produce a single line of output containing the results as a comma-separated list of lists enclosed in square brackets, where each inner list is the triplet $[|\\operatorname{corr}(r, \\eta_{\\text{true}})|, R^2_{x_1}, R^2_{x_2}]$ for each case, in the order of the four cases specified (for example, in the format $[[a_1,b_1,c_1],[a_2,b_2,c_2],[a_3,b_3,c_3],[a_4,b_4,c_4]]$). No physical units are involved in this problem; express all outputs as floating-point numbers without percentage signs.", "solution": "The user wants to implement a statistical simulation to diagnose link function misspecification in a Generalized Additive Model (GAM) for binary classification.\n\n### Step 1: Problem Validation\n\n**Givens Extraction:**\n\n*   **Model Framework**: Generalized Additive Model (GAM) for binary outcomes using a link function $g$ to connect the conditional mean $\\mu(\\mathbf{x})$ to an additive predictor $\\eta(\\mathbf{x}) = \\beta_0 + \\sum_{j=1}^p f_j(x_j)$.\n*   **Estimation**: Maximum likelihood for Bernoulli distribution via Fisher scoring (Iteratively Reweighted Least Squares, IRLS).\n*   **Log-Likelihood**: $L(\\mu \\mid y) = \\sum_{i=1}^n \\left\\{ y_i \\log \\mu_i + (1 - y_i) \\log (1 - \\mu_i) \\right\\}$.\n*   **Variance Function**: $V(\\mu_i) = \\mu_i (1 - \\mu_i)$.\n*   **Data Generation**:\n    *   Sample size $n$.\n    *   Covariates $x_1, x_2 \\sim U[0,1]$ independently.\n    *   Smooth functions: $f_1(x_1) = 1.2 \\sin(2 \\pi x_1) + 0.3(x_1 - 0.5)$ and $f_2(x_2) = -0.8 \\cos(2 \\pi x_2) + 0.5(x_2 - 0.5)^2$.\n    *   True additive predictor: $\\eta_{\\text{true}} = s \\cdot \\left( f_1(x_1) + f_2(x_2) \\right)$, with scale parameter $s$.\n    *   True link function (for data generation): complementary log-log, $p_i = 1 - \\exp\\left( -\\exp\\left( \\eta_{\\text{true}, i} \\right) \\right)$.\n    *   Response: $y_i \\sim \\text{Bernoulli}(p_i)$.\n*   **Model Fitting**:\n    *   Basis for $f_j$: Truncated cubic spline basis with knots at $\\{0.2, 0.4, 0.6, 0.8\\}$ on $[0,1]$ using basis functions $\\{x, x^2, x^3, (x - \\kappa_m)_+^3\\}_{m=1}^4$.\n    *   Design Matrix $X$: Intercept, basis for $x_1$, basis for $x_2$.\n    *   IRLS Algorithm: Solve penalized normal equations $\\left( X^\\top W X + \\lambda P \\right) \\boldsymbol{\\beta}^{\\text{new}} = X^\\top W \\mathbf{z}$ where $W_{ii} = \\left( d\\mu_i/d\\eta_i \\right)^2 / V(\\mu_i)$ and $z_i = \\eta_i + (y_i - \\mu_i) / (d\\mu_i/d\\eta_i)$.\n    *   Penalty: Ridge penalty with parameter $\\lambda > 0$ and $P$ as the identity matrix with a zero for the intercept term.\n    *   Links for Fitting: logit ($\\mu_i = 1 / \\left( 1 + \\exp(-\\eta_i) \\right)$) and complementary log-log.\n*   **Residual Diagnostics**:\n    *   Deviance residuals: $r_i = \\operatorname{sign}(y_i - \\hat{\\mu}_i) \\sqrt{2 \\left( y_i \\log\\left( \\frac{y_i}{\\hat{\\mu}_i} \\right) + (1 - y_i) \\log\\left( \\frac{1 - y_i}{1 - \\hat{\\mu}_i} \\right) \\right)}$.\n    *   Metrics: $|\\operatorname{corr}(r, \\eta_{\\text{true}})|$, $R^2$ of regressing $r$ on the spline basis for $x_1$, and $R^2$ of regressing $r$ on the spline basis for $x_2$.\n*   **Test Cases**:\n    1.  $n = 800, s = 1.0$, seed $42$, fit with logit.\n    2.  $n = 800, s = 1.0$, seed $42$, fit with cloglog.\n    3.  $n = 800, s = 2.5$, seed $123$, fit with logit.\n    4.  $n = 800, s = 0.2$, seed $7$, fit with logit.\n*   **Output Format**: A single line string `[[a1,b1,c1],[a2,b2,c2],[a3,b3,c3],[a4,b4,c4]]`.\n\n**Validation Assessment:**\n\n*   **Scientific Grounding**: The problem is well-grounded in the theory of generalized linear and additive models, which are fundamental topics in modern statistics and machine learning. All concepts (IRLS, link functions, basis expansions, deviance residuals) are standard.\n*   **Well-Posedness**: The problem is specific and provides all necessary components to construct a solution. A minor ambiguity exists in the value of the penalty parameter $\\lambda$. The problem states it is \"a ridge penalty parameter $\\lambda > 0$\" but does not assign a value. This is an acceptable omission in a simulation study context, as the implementer is expected to choose a reasonable, fixed value to ensure a fair comparison across test cases. I will select a small, standard value, $\\lambda=0.01$, to regularize the fit slightly and ensure numerical stability.\n*   **Objectivity**: The problem is stated in precise, mathematical language, free of subjectivity or opinion.\n*   **Conclusion**: The problem is valid. It is a well-defined computational task in statistical modeling that is scientifically sound and formalizable.\n\n### Solution\n\nThe solution proceeds by implementing the three main stages described in the problem: data generation, model fitting, and residual diagnostics.\n\n**1. Data Generation**\nFirst, we define the smooth functions $f_1(x_1)$ and $f_2(x_2)$. For each test case, we generate $n$ samples of covariates $x_1$ and $x_2$ from a uniform distribution $U[0,1]$. We then compute the true additive predictor $\\eta_{\\text{true}, i} = s \\cdot (f_1(x_{1i}) + f_2(x_{2i}))$. The probabilities $p_i$ are generated using the inverse complementary log-log (cloglog) link function: $p_i = 1 - \\exp(-\\exp(\\eta_{\\text{true},i}))$. Finally, the binary responses $y_i$ are sampled from a Bernoulli distribution with these probabilities, $y_i \\sim \\text{Bernoulli}(p_i)$.\n\n**2. Model Fitting**\nThe core of the task is to fit a GAM to the generated data. This involves two key components: constructing the design matrix and implementing the penalized IRLS algorithm.\n\n**2.1. Design Matrix Construction**\nEach smooth function $f_j$ is approximated by a linear combination of basis functions. The problem specifies a truncated cubic spline basis. For a covariate $x$, the basis functions are $\\{x, x^2, x^3, (x - \\kappa_1)_+^3, \\dots, (x - \\kappa_4)_+^3\\}$, where $(u)_+ = \\max(u,0)$ and the knots $\\kappa_m$ are given as $\\{0.2, 0.4, 0.6, 0.8\\}$. This results in $3+4=7$ basis functions per covariate. The full design matrix $X$ is constructed by concatenating a column of ones (for the intercept $\\beta_0$), the $7$ basis columns for $x_1$, and the $7$ basis columns for $x_2$. This gives $X$ a total of $1+7+7=15$ columns.\n\n**2.2. Penalized Iterative Reweighted Least Squares (IRLS)**\nThe IRLS algorithm is an iterative procedure to find the maximum likelihood estimate of the coefficients $\\boldsymbol{\\beta}$. Starting with an initial guess for $\\boldsymbol{\\beta}$, each iteration involves the following steps:\n1.  **Compute Predictor and Mean**: Calculate the linear predictor $\\eta_i = \\mathbf{x}_i^\\top \\boldsymbol{\\beta}$ and the conditional mean $\\mu_i = g^{-1}(\\eta_i)$, where $g^{-1}$ is the inverse link function (logit or cloglog).\n2.  **Compute Working Response and Weights**: The updates are found by solving a weighted least squares problem. The weights $w_i$ and working response $z_i$ depend on the chosen link function:\n    *   $d\\mu_i/d\\eta_i$: The derivative of the inverse link function.\n    *   $V(\\mu_i) = \\mu_i(1-\\mu_i)$: The variance of a Bernoulli variable.\n    *   $w_i = (d\\mu_i/d\\eta_i)^2 / V(\\mu_i)$.\n    *   $z_i = \\eta_i + (y_i - \\mu_i) / (d\\mu_i/d\\eta_i)$.\n3.  **Solve Penalized System**: A new coefficient vector $\\boldsymbol{\\beta}^{\\text{new}}$ is found by solving the penalized normal equations:\n    $$ \\left( X^\\top W X + \\lambda P \\right) \\boldsymbol{\\beta}^{\\text{new}} = X^\\top W \\mathbf{z} $$\n    Here, $W$ is a diagonal matrix of the weights $w_i$, $\\lambda$ is the penalty parameter (chosen as $\\lambda=0.01$), and $P$ is a $15 \\times 15$ identity matrix with $P_{00}=0$ to avoid penalizing the intercept.\nThe process is repeated until the coefficient vector $\\boldsymbol{\\beta}$ converges.\n\nFor the two link functions specified:\n*   **Logit**: $\\mu = \\sigma(\\eta) = (1+e^{-\\eta})^{-1}$. Here, $d\\mu/d\\eta = \\mu(1-\\mu)$, which simplifies the weight to $w = \\mu(1-\\mu)$.\n*   **Cloglog**: $\\mu = 1 - e^{-e^{\\eta}}$. Here, $d\\mu/d\\eta = (1-\\mu)e^{\\eta}$.\n\n**3. Residual Diagnostics**\nAfter fitting the model and obtaining the estimated means $\\hat{\\mu}_i$, we compute Bernoulli deviance residuals to assess model fit. The formula is:\n$$ r_i = \\operatorname{sign}(y_i - \\hat{\\mu}_i) \\sqrt{-2 \\left[ y_i \\log(\\hat{\\mu}_i) + (1 - y_i) \\log(1 - \\hat{\\mu}_i) \\right]} $$\nAppropriate numerical clipping is used for $\\hat{\\mu}_i$ to avoid taking logarithms of zero.\n\nWe then compute three diagnostic metrics to quantify systematic patterns in these residuals:\n1.  **$|\\operatorname{corr}(r, \\eta_{\\text{true}})|$**: The absolute Pearson correlation between the deviance residuals and the true additive predictor. A high value suggests that the model's errors are systematically related to the underlying signal, a sign of misspecification.\n2.  **$R^2_{x_1}$**: The coefficient of determination from an ordinary least squares regression of the residuals $r$ onto the spline basis for $x_1$ (including an intercept). A high $R^2$ indicates that the residuals have a structure that can be explained by $x_1$, which should not happen in a well-specified model.\n3.  **$R^2_{x_2}$**: Similarly, the $R^2$ from regressing $r$ onto the spline basis for $x_2$.\n\nThese diagnostics are computed for each of the four test cases. A correctly specified model (Case 2) is expected to have low values for all three metrics, indicating that the residuals are close to random noise. Misspecified models (Cases 1, 3, 4) are expected to show larger values, revealing the systematic error introduced by using the wrong link function. The magnitude of this error will depend on the signal strength $s$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a simulation to diagnose link function misspecification in a \n    Generalized Additive Model (GAM) and reports diagnostic metrics.\n    \"\"\"\n    \n    # Define problem constants and hyperparameters\n    KNOTS = np.array([0.2, 0.4, 0.6, 0.8])\n    LAMBDA_PEN = 0.01  # A fixed ridge penalty parameter\n    MAX_ITER = 25      # Maximum iterations for IRLS\n    TOL = 1e-7         # Convergence tolerance for IRLS\n\n    # --- Step 1: Data Generation ---\n\n    def f1(x1):\n        \"\"\"Smooth function for covariate x1.\"\"\"\n        return 1.2 * np.sin(2 * np.pi * x1) + 0.3 * (x1 - 0.5)\n\n    def f2(x2):\n        \"\"\"Smooth function for covariate x2.\"\"\"\n        return -0.8 * np.cos(2 * np.pi * x2) + 0.5 * (x2 - 0.5)**2\n\n    def generate_data(n, s, seed):\n        \"\"\"\n        Generates data from the true model with a complementary log-log link.\n        \"\"\"\n        rng = np.random.default_rng(seed)\n        x1 = rng.uniform(0, 1, n)\n        x2 = rng.uniform(0, 1, n)\n        \n        eta_true = s * (f1(x1) + f2(x2))\n        \n        # True link is complementary log-log\n        p = 1 - np.exp(-np.exp(eta_true))\n        # Clip for numerical stability\n        p = np.clip(p, 1e-15, 1 - 1e-15)\n        \n        y = rng.binomial(1, p)\n        \n        return x1, x2, y, eta_true\n\n    # --- Step 2: Model Fitting ---\n\n    def create_basis_matrix(x, knots):\n        \"\"\"Creates a truncated cubic spline basis matrix for a single covariate.\"\"\"\n        basis_cols = [x, x**2, x**3]\n        for k in knots:\n            basis_cols.append(np.maximum(x - k, 0)**3)\n        return np.stack(basis_cols, axis=1)\n\n    def create_design_matrix(x1, x2, knots):\n        \"\"\"Creates the full design matrix X with an intercept and basis expansions.\"\"\"\n        n = len(x1)\n        intercept = np.ones((n, 1))\n        basis_x1 = create_basis_matrix(x1, knots)\n        basis_x2 = create_basis_matrix(x2, knots)\n        return np.hstack([intercept, basis_x1, basis_x2])\n\n    def fit_gam(X, y, link, lambda_pen):\n        \"\"\"\n        Fits a penalized GAM using Iteratively Reweighted Least Squares (IRLS).\n        \"\"\"\n        n, p = X.shape\n        beta = np.zeros(p)\n\n        # Initial guess for eta based on empirical logits\n        mu_init = np.clip((y + 0.5) / 2, 1e-15, 1 - 1e-15)\n        if link == 'logit':\n            eta = np.log(mu_init / (1 - mu_init))\n        else: # cloglog\n            eta = np.log(-np.log(1 - mu_init))\n\n        penalty_matrix = lambda_pen * np.diag([0] + [1] * (p - 1))\n\n        for _ in range(MAX_ITER):\n            # Clip eta to avoid overflow in exp(), especially for cloglog\n            eta = np.clip(eta, -30, 30)\n\n            if link == 'logit':\n                mu = 1 / (1 + np.exp(-eta))\n                d_mu_d_eta = mu * (1 - mu)\n            else: # cloglog\n                exp_eta = np.exp(eta)\n                mu = 1 - np.exp(-exp_eta)\n                d_mu_d_eta = (1 - mu) * exp_eta\n            \n            # Clip values for numerical stability\n            mu = np.clip(mu, 1e-15, 1 - 1e-15)\n            V = np.maximum(mu * (1 - mu), 1e-15)\n            d_mu_d_eta = np.maximum(d_mu_d_eta, 1e-15)\n\n            weights = (d_mu_d_eta**2) / V\n            working_response = eta + (y - mu) / d_mu_d_eta\n            \n            # Solve the penalized weighted least squares system\n            XtW = X.T * weights\n            lhs = XtW @ X + penalty_matrix\n            rhs = XtW @ working_response\n\n            try:\n                beta_new = np.linalg.solve(lhs, rhs)\n            except np.linalg.LinAlgError:\n                # Add jitter for stability if the matrix is singular\n                lhs += np.eye(p) * 1e-8 \n                beta_new = np.linalg.solve(lhs, rhs)\n\n            # Check for convergence based on the change in coefficients\n            if np.linalg.norm(beta_new - beta) / (np.linalg.norm(beta) + TOL) < TOL:\n                beta = beta_new\n                break\n            \n            beta = beta_new\n            eta = X @ beta\n            \n        # Compute final fitted means\n        eta = np.clip(X @ beta, -30, 30)\n        if link == 'logit':\n            mu_hat = 1 / (1 + np.exp(-eta))\n        else: # cloglog\n            mu_hat = 1 - np.exp(-np.exp(eta))\n            \n        return np.clip(mu_hat, 1e-15, 1 - 1e-15)\n\n    # --- Step 3: Residual Diagnostics ---\n\n    def calculate_diagnostics(y, mu_hat, eta_true, x1, x2, knots):\n        \"\"\"\n        Computes deviance residuals and the three diagnostic metrics.\n        \"\"\"\n        # Bernoulli deviance residuals\n        # The formula given in the problem is actually for -2 * log-likelihood contribution per point.\n        # Let's use the standard definition.\n        with np.errstate(divide='ignore'):\n            term1 = np.where(y == 1, y * np.log(y / mu_hat), 0)\n            term2 = np.where(y == 0, (1 - y) * np.log((1 - y) / (1 - mu_hat)), 0)\n        \n        dev_res_unsigned = np.sqrt(2 * (term1 + term2))\n        residuals = np.sign(y - mu_hat) * dev_res_unsigned\n\n        # Metric 1: Absolute correlation with true linear predictor\n        corr_eta_true = np.abs(np.corrcoef(residuals, eta_true)[0, 1])\n\n        # Metrics 2 & 3: R^2 from regressing residuals on basis splines\n        def get_r_squared(x, knots_local, r):\n            n_obs = len(x)\n            X_reg = np.hstack([np.ones((n_obs, 1)), create_basis_matrix(x, knots_local)])\n            \n            coeffs = np.linalg.lstsq(X_reg, r, rcond=None)[0]\n            r_hat = X_reg @ coeffs\n            ss_res = np.sum((r - r_hat)**2)\n            ss_tot = np.sum((r - np.mean(r))**2)\n            \n            if ss_tot < 1e-12:\n                return 1.0 if ss_res < 1e-12 else 0.0\n            return 1 - ss_res / ss_tot\n\n        r_squared_x1 = get_r_squared(x1, knots, residuals)\n        r_squared_x2 = get_r_squared(x2, knots, residuals)\n\n        return [corr_eta_true, r_squared_x1, r_squared_x2]\n\n    # --- Main Execution Loop ---\n    \n    test_cases = [\n        # n, s, seed, link_to_fit\n        (800, 1.0, 42, 'logit'),   # Case 1: Misspecified (standard)\n        (800, 1.0, 42, 'cloglog'), # Case 2: Correctly specified\n        (800, 2.5, 123, 'logit'),  # Case 3: Misspecified, strong signal\n        (800, 0.2, 7, 'logit'),    # Case 4: Misspecified, weak signal\n    ]\n    \n    results = []\n    for n, s, seed, link_to_fit in test_cases:\n        x1, x2, y, eta_true = generate_data(n=n, s=s, seed=seed)\n        X = create_design_matrix(x1, x2, KNOTS)\n        mu_hat = fit_gam(X, y, link=link_to_fit, lambda_pen=LAMBDA_PEN)\n        diagnostics = calculate_diagnostics(y, mu_hat, eta_true, x1, x2, KNOTS)\n        results.append(diagnostics)\n\n    # Format the final output string exactly as required\n    case_strs = [f\"[{','.join(map(str, res))}]\" for res in results]\n    print(f\"[{','.join(case_strs)}]\")\n\nsolve()\n```", "id": "3123718"}]}