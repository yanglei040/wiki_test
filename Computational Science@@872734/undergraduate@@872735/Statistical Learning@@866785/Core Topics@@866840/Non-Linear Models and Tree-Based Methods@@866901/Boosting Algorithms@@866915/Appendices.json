{"hands_on_practices": [{"introduction": "Boosting algorithms work by sequentially correcting the errors of a \"weak\" learning model. The precise definition of \"error\" is dictated by the choice of a loss function. This exercise [@problem_id:3105987] moves beyond the classic exponential loss of AdaBoost to explore the widely used logistic loss, which connects boosting directly to probability estimation. By deriving the gradient and the population-optimal score, you will gain a first-principles understanding of how LogitBoost works and how its calibration differs from AdaBoost.", "problem": "Consider binary classification with labels $y \\in \\{-1,+1\\}$ and an additive score function $F(x)$ that is built stagewise by boosting, $F_{m}(x) = F_{m-1}(x) + \\nu h_{m}(x)$, where $\\nu \\in (0,1]$ is a fixed learning rate and $h_{m}$ is a base learner chosen at stage $m$. The training objective is Empirical Risk Minimization (ERM) with the logistic loss $\\ell_{\\log}(y,F) = \\log(1+\\exp(-yF))$. At each stage, Functional Gradient Descent (FGD) is applied: $h_{m}$ is chosen to approximate the negative gradient of the empirical risk with respect to $F$ evaluated at $F_{m-1}$.\n\nTasks:\n1. Starting from the definition of $\\ell_{\\log}(y,F)$ and the ERM objective, derive the per-example negative gradient $\\{-\\partial \\ell_{\\log}(y_{i},F)/\\partial F\\}_{F=F_{m-1}(x_{i})}$ that determines the stagewise update direction for FGD. Express your result explicitly as a function of $y_{i}$ and $F_{m-1}(x_{i})$.\n2. Using the population risk $\\mathbb{E}[\\ell_{\\log}(Y,F(X)) \\mid X=x]$ with $Y \\in \\{-1,+1\\}$ and $\\pi(x) \\equiv \\mathbb{P}(Y=+1 \\mid X=x)$, derive the closed-form expression for the population-optimal score $F^{\\star}(x)$ that minimizes $\\mathbb{E}[\\ell_{\\log}(Y,F(X)) \\mid X=x]$ at each $x$. Then, contrast this calibration with the analogous minimizer under the exponential loss $\\ell_{\\exp}(y,F)=\\exp(-yF)$, explaining the difference in scaling of the log-odds.\n3. Provide your final answer as the single closed-form analytic expression for $F^{\\star}(x)$ under logistic loss. No rounding is required and no physical units are involved.\n\nYour derivations must start from first principles (definitions of ERM and FGD, and the given loss functions) and proceed logically. Do not use or quote any pre-derived boosting update formulas.", "solution": "The problem statement is evaluated as valid. It is scientifically grounded in the principles of statistical learning theory, specifically boosting and functional gradient descent. The problem is well-posed, objective, and internally consistent, providing all necessary definitions and data to perform the required derivations.\n\n### Task 1: Derivation of the Per-Example Negative Gradient\n\nThe objective of Empirical Risk Minimization (ERM) is to find a function $F$ that minimizes the total loss over the training data. The empirical risk $R_{\\text{emp}}(F)$ for a dataset of $N$ examples $\\{(x_i, y_i)\\}_{i=1}^N$ is the average loss:\n$$\nR_{\\text{emp}}(F) = \\frac{1}{N} \\sum_{i=1}^N \\ell_{\\log}(y_i, F(x_i))\n$$\nIn Functional Gradient Descent (FGD), at each stage $m$, we seek a function $h_m$ that points in the direction of the negative gradient of the risk functional. For a finite dataset, this simplifies to finding a base learner $h_m(x)$ that best approximates the negative gradient components evaluated at each data point $x_i$, using the current score function $F_{m-1}(x)$. The per-example negative gradient for the $i$-th data point $(x_i, y_i)$ is the target that $h_m(x_i)$ should approximate.\n\nWe are given the logistic loss function $\\ell_{\\log}(y,F) = \\log(1+\\exp(-yF))$. We must compute its partial derivative with respect to the score $F$. Let $u = -yF$. Using the chain rule for differentiation, $\\frac{\\partial \\ell_{\\log}}{\\partial F} = \\frac{d \\ell_{\\log}}{du} \\frac{\\partial u}{\\partial F}$.\n\nThe derivatives are:\n$$\n\\frac{d}{du} \\log(1+\\exp(u)) = \\frac{1}{1+\\exp(u)} \\cdot \\exp(u)\n$$\n$$\n\\frac{\\partial u}{\\partial F} = \\frac{\\partial}{\\partial F}(-yF) = -y\n$$\nCombining these, we get the partial derivative of the loss with respect to $F$:\n$$\n\\frac{\\partial \\ell_{\\log}(y,F)}{\\partial F} = \\frac{\\exp(-yF)}{1+\\exp(-yF)} \\cdot (-y) = \\frac{-y \\exp(-yF)}{1+\\exp(-yF)}\n$$\nTo simplify this expression, we can multiply the numerator and denominator by $\\exp(yF)$:\n$$\n\\frac{\\partial \\ell_{\\log}(y,F)}{\\partial F} = \\frac{-y \\exp(-yF) \\cdot \\exp(yF)}{(1+\\exp(-yF)) \\cdot \\exp(yF)} = \\frac{-y}{\\exp(yF) + 1}\n$$\nThe negative gradient, which is the direction for the functional update, is therefore:\n$$\n-\\frac{\\partial \\ell_{\\log}(y,F)}{\\partial F} = \\frac{y}{1 + \\exp(yF)}\n$$\nAt stage $m$ of the boosting algorithm, this gradient is evaluated at the current score function, $F = F_{m-1}(x_i)$, for each example $(x_i, y_i)$. Thus, the per-example negative gradient that defines the stagewise update direction is:\n$$\nr_{im} = \\left[-\\frac{\\partial \\ell_{\\log}(y_i, F)}{\\partial F}\\right]_{F=F_{m-1}(x_i)} = \\frac{y_i}{1 + \\exp(y_i F_{m-1}(x_i))}\n$$\nThese values $r_{im}$ are often called the \"residuals\" or \"pseudo-residuals\" that the new base learner $h_m$ is trained to fit.\n\n### Task 2: Derivation of the Population-Optimal Score and Comparison\n\nThe population-optimal score $F^\\star(x)$ is the function that minimizes the expected loss at each point $x$ in the feature space. This is the minimizer of the conditional population risk $J(F) = \\mathbb{E}[\\ell(Y, F(X)) \\mid X=x]$. The expectation is taken over the random variable $Y$, conditioned on $X=x$.\n\nGiven that $Y \\in \\{-1, +1\\}$ and $\\pi(x) \\equiv \\mathbb{P}(Y=+1 \\mid X=x)$, we have $\\mathbb{P}(Y=-1 \\mid X=x) = 1-\\pi(x)$. The conditional risk for the logistic loss is:\n$$\nJ_{\\log}(F(x)) = \\mathbb{P}(Y=+1 \\mid X=x) \\cdot \\ell_{\\log}(+1, F(x)) + \\mathbb{P}(Y=-1 \\mid X=x) \\cdot \\ell_{\\log}(-1, F(x))\n$$\n$$\nJ_{\\log}(F(x)) = \\pi(x) \\log(1+\\exp(-F(x))) + (1-\\pi(x)) \\log(1+\\exp(F(x)))\n$$\nTo find the minimizer $F^\\star(x)$, we differentiate $J_{\\log}(F(x))$ with respect to $F(x)$ and set the result to zero. For simplicity, we can denote $F(x)$ as $F$.\n$$\n\\frac{dJ_{\\log}(F)}{dF} = \\pi(x) \\left(\\frac{-\\exp(-F)}{1+\\exp(-F)}\\right) + (1-\\pi(x)) \\left(\\frac{\\exp(F)}{1+\\exp(F)}\\right)\n$$\nSetting the derivative to zero:\n$$\n\\pi(x) \\left(\\frac{-\\exp(-F)}{1+\\exp(-F)}\\right) + (1-\\pi(x)) \\left(\\frac{\\exp(F)}{1+\\exp(F)}\\right) = 0\n$$\n$$\n(1-\\pi(x)) \\frac{\\exp(F)}{1+\\exp(F)} = \\pi(x) \\frac{\\exp(-F)}{1+\\exp(-F)}\n$$\nTo simplify the right-hand side, we multiply its numerator and denominator by $\\exp(F)$:\n$$\n\\frac{\\exp(-F)}{1+\\exp(-F)} = \\frac{\\exp(-F)\\exp(F)}{(1+\\exp(-F))\\exp(F)} = \\frac{1}{\\exp(F)+1}\n$$\nSubstituting this back into the equation yields:\n$$\n(1-\\pi(x)) \\frac{\\exp(F)}{1+\\exp(F)} = \\pi(x) \\frac{1}{1+\\exp(F)}\n$$\nSince $1+\\exp(F)$ is always positive, we can multiply both sides by it:\n$$\n(1-\\pi(x)) \\exp(F) = \\pi(x)\n$$\nSolving for $\\exp(F)$:\n$$\n\\exp(F) = \\frac{\\pi(x)}{1-\\pi(x)}\n$$\nFinally, taking the natural logarithm of both sides gives the population-optimal score for the logistic loss:\n$$\nF^\\star(x) = \\ln\\left(\\frac{\\pi(x)}{1-\\pi(x)}\\right)\n$$\nThis expression is the log-odds (or logit) of the conditional probability $\\pi(x)$.\n\nNow, we perform the same analysis for the exponential loss, $\\ell_{\\exp}(y,F) = \\exp(-yF)$. The conditional risk is:\n$$\nJ_{\\exp}(F(x)) = \\pi(x) \\exp(-F(x)) + (1-\\pi(x)) \\exp(F(x))\n$$\nDifferentiating with respect to $F(x)$ (again denoted as $F$) and setting to zero:\n$$\n\\frac{dJ_{\\exp}(F)}{dF} = -\\pi(x) \\exp(-F) + (1-\\pi(x)) \\exp(F) = 0\n$$\n$$\n(1-\\pi(x)) \\exp(F) = \\pi(x) \\exp(-F)\n$$\nMultiplying both sides by $\\exp(F)$ gives:\n$$\n(1-\\pi(x)) \\exp(2F) = \\pi(x)\n$$\n$$\n\\exp(2F) = \\frac{\\pi(x)}{1-\\pi(x)}\n$$\nTaking the natural logarithm and solving for $F$ gives the population-optimal score for the exponential loss:\n$$\nF^\\star_{\\exp}(x) = \\frac{1}{2} \\ln\\left(\\frac{\\pi(x)}{1-\\pi(x)}\\right)\n$$\nComparing the two optimal scores, we find:\n$$\nF^\\star_{\\log}(x) = \\ln\\left(\\frac{\\pi(x)}{1-\\pi(x)}\\right) \\quad \\text{and} \\quad F^\\star_{\\exp}(x) = \\frac{1}{2} \\ln\\left(\\frac{\\pi(x)}{1-\\pi(x)}\\right)\n$$\nThe comparison reveals that both boosting with logistic loss (LogitBoost) and with exponential loss (AdaBoost) produce a score function $F(x)$ that is proportional to the log-odds of the class probability $\\pi(x)$. However, the scaling differs by a factor of $2$. The score function under exponential loss converges to half the log-odds, whereas the score function under logistic loss converges directly to the log-odds. This implies that to recover the probability estimate $\\pi(x)$ from the learned score $F(x)$, one must use the transformation $\\pi(x) = 1/(1+\\exp(-F(x)))$ for logistic loss, but $\\pi(x) = 1/(1+\\exp(-2F(x)))$ for exponential loss.\n\n### Task 3: Final Answer Expression\n\nThe final answer is the closed-form analytic expression for the population-optimal score $F^{\\star}(x)$ under logistic loss derived in Task 2.", "answer": "$$\n\\boxed{\\ln\\left(\\frac{\\pi(x)}{1-\\pi(x)}\\right)}\n$$", "id": "3105987"}, {"introduction": "A powerful ensemble can easily overfit the training data. In gradient boosting, the learning rate or \"shrinkage\" parameter $\\nu$ is a crucial tool for regularization, controlling how much each new weak learner contributes to the final model. In this practical exercise [@problem_id:3125539], you will implement a gradient boosting regressor and investigate how different values of $\\nu$ affect the model's training loss and the norm of its predictions, providing a concrete demonstration of shrinkage as implicit regularization.", "problem": "Consider supervised regression with training data $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n$, where $\\mathbf{x}_i \\in \\mathbb{R}^d$ and $y_i \\in \\mathbb{R}$. The goal is to minimize the empirical risk $R(f)$ over additive functions $f$ in a stagewise fashion, starting from the fundamental base that empirical risk minimization with the Mean Squared Error (MSE) loss is governed by the objective\n$$\nR(f) = \\frac{1}{n}\\sum_{i=1}^n \\left(y_i - f(\\mathbf{x}_i)\\right)^2.\n$$\nIn stagewise additive modeling using gradient descent in function space, one chooses at each stage $m$ a weak learner $h_m$ that approximates the negative gradient of the loss evaluated at the current model $f_{m-1}$, and updates via\n$$\nf_m = f_{m-1} + \\nu \\, h_m,\n$$\nwhere $\\nu \\in (0, 1]$ is a stagewise shrinkage parameter acting as implicit regularization.\n\nYou must implement gradient boosting with squared error loss and axis-aligned regression stumps as weak learners. A regression stump $h(\\mathbf{x})$ is defined by selecting a feature index $j \\in \\{1, \\dots, d\\}$ and a threshold $t \\in \\mathbb{R}$, and predicting a constant $a$ when $x_j  t$ and a constant $b$ otherwise. For squared error fitting to a target vector $\\mathbf{r} \\in \\mathbb{R}^n$, the optimal constants for any fixed $(j,t)$ are the means of the targets on the two sides of the threshold.\n\nYou are given a fixed training set of $n = 14$ points in $d = 2$ dimensions:\n$$\n\\mathbf{X} = \\big[(-2.0,-1.0),\\, (-2.0,1.0),\\, (-1.0,-1.0),\\, (-1.0,1.0),\\, (0.0,-1.0),\\, (0.0,1.0),\\, (1.0,-1.0),\\, (1.0,1.0),\\, (2.0,-1.0),\\, (2.0,1.0),\\, (-3.0,0.0),\\, (3.0,0.0),\\, (0.5,-0.5),\\, (-0.5,0.5)\\big],\n$$\nand the deterministic rule for targets\n$$\ny = 2 \\cdot \\mathbb{1}\\{x_1  0\\} \\;-\\; \\mathbb{1}\\{x_2  0\\} \\;+\\; 0.3\\,x_1 \\;+\\; 0.1\\,x_2,\n$$\nwhich must be used to compute $\\mathbf{y} \\in \\mathbb{R}^{14}$ from $\\mathbf{X}$ exactly (with no randomness). Initialize $f_0(\\mathbf{x}) \\equiv 0$, and for each stage $m = 1, 2, \\dots, M$ with fixed $M$, compute pseudo-residuals\n$$\nr_i^{(m)} = y_i - f_{m-1}(\\mathbf{x}_i),\n$$\nfit a single regression stump $h_m$ to $\\{(\\mathbf{x}_i, r_i^{(m)})\\}_{i=1}^n$ by minimizing the squared error on the training points, and update $f_m = f_{m-1} + \\nu h_m$.\n\nInvestigate stagewise shrinkage $\\nu$ as implicit regularization: fix the number of boosting stages to $M = 12$ and evaluate, for each specified $\\nu$, the Euclidean prediction norm\n$$\n\\|f_M\\|_2 \\;=\\; \\left(\\sum_{i=1}^n f_M(\\mathbf{x}_i)^2\\right)^{1/2}\n$$\nand the training loss\n$$\nR(f_M) \\;=\\; \\frac{1}{n}\\sum_{i=1}^n \\big(y_i - f_M(\\mathbf{x}_i)\\big)^2.\n$$\n\nYour implementation must construct regression stumps by considering all features $j \\in \\{1,\\dots,d\\}$ and all thresholds $t$ formed as midpoints between consecutive distinct values of the chosen feature across the training samples. For any candidate $(j,t)$, the optimal left constant $a$ is the mean of the targets for indices with $x_{ij}  t$, and the optimal right constant $b$ is the mean of the targets for indices with $x_{ij} \\ge t$. Among all candidates across all features, choose the stump that minimizes the sum of squared residual errors on the training data. If a feature has no valid split (all values equal), skip it; if all features have no valid split, use the constant predictor equal to the global mean of the targets as the stump.\n\nTest Suite:\n- Use precisely the five shrinkage values $\\nu \\in \\{0.01,\\, 0.1,\\, 0.3,\\, 0.7,\\, 1.0\\}$.\n- Fix $M = 12$ stages for all tests.\n- For each $\\nu$, compute the pair $\\left(\\|f_M\\|_2,\\, R(f_M)\\right)$ rounded to $6$ decimal places.\n\nAnswer specification:\n- For each $\\nu$ in the given order, output the pair $\\left[\\|f_M\\|_2, R(f_M)\\right]$ as a list of two floats.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each pair enclosed in its own square brackets and no spaces. For example, the output should look like\n$$\n\\big[[\\|f_M\\|_2^{(1)}, R(f_M)^{(1)}],[\\|f_M\\|_2^{(2)}, R(f_M)^{(2)}],\\dots\\big],\n$$\nconcretely rendered as a single line string like \"[[n1,l1],[n2,l2],[n3,l3],[n4,l4],[n5,l5]]\" where each $n_k$ and $l_k$ are decimal floats rounded to $6$ decimal places.", "solution": "The user requests the implementation of a gradient boosting algorithm for a regression problem, utilizing regression stumps as weak learners. The objective is to evaluate the effect of the shrinkage parameter $\\nu$ on the final model's prediction norm and training loss. The process begins with a formal validation of the problem statement, which is found to be scientifically sound, well-posed, and complete.\n\n### Principles of Gradient Boosting with Squared Error Loss\n\nGradient boosting is an ensemble learning method that builds a strong predictive model in a stagewise fashion from a collection of weak learners. The core idea is to treat the optimization problem as a form of gradient descent in a function space.\n\nThe goal is to find a function $f(\\mathbf{x})$ that minimizes the empirical risk, defined by a loss function $L(y, f(\\mathbf{x}))$. For this problem, the specified loss is the Mean Squared Error (MSE), and the empirical risk is given by:\n$$\nR(f) = \\frac{1}{n} \\sum_{i=1}^{n} L(y_i, f(\\mathbf{x}_i)) = \\frac{1}{n}\\sum_{i=1}^{n} \\left(y_i - f(\\mathbf{x}_i)\\right)^2\n$$\nwhere $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n$ is the training set with $n=14$ samples and $\\mathbf{x}_i \\in \\mathbb{R}^2$.\n\nThe algorithm proceeds as follows:\n\n1.  **Initialization**: The model is initialized with a constant value. The problem specifies an initial model $f_0(\\mathbf{x}) \\equiv 0$. The vector of predictions on the training data is thus $\\mathbf{f}_0 = \\mathbf{0} \\in \\mathbb{R}^n$.\n\n2.  **Stagewise Additive Modeling**: For each stage $m = 1, 2, \\dots, M$, the model is updated iteratively. In the context of functional gradient descent, we seek to add a function $h_m$ that moves the current model $f_{m-1}$ in the direction of the negative gradient of the loss function. The negative gradient components with respect to the function values $f(\\mathbf{x}_i)$ are:\n    $$\n    r_i^{(m)} = -\\left[\\frac{\\partial L(y_i, F)}{\\partial F}\\right]_{F=f_{m-1}(\\mathbf{x}_i)} = -\\left[\\frac{\\partial}{\\partial F} \\frac{1}{2}(y_i - F)^2\\right]_{F=f_{m-1}(\\mathbf{x}_i)} = y_i - f_{m-1}(\\mathbf{x}_i)\n    $$\n    These quantities $r_i^{(m)}$ are called the pseudo-residuals for stage $m$.\n\n3.  **Weak Learner Fitting**: A weak learner, in this case, a regression stump $h_m(\\mathbf{x})$, is fitted to predict the pseudo-residuals. This is done by solving the least-squares problem:\n    $$\n    h_m = \\arg\\min_{h} \\sum_{i=1}^n \\left(r_i^{(m)} - h(\\mathbf{x}_i)\\right)^2\n    $$\n\n4.  **Model Update**: The full model is updated by adding the new weak learner, scaled by a shrinkage parameter $\\nu$:\n    $$\n    f_m(\\mathbf{x}) = f_{m-1}(\\mathbf{x}) + \\nu h_m(\\mathbf{x})\n    $$\n    The shrinkage parameter $\\nu \\in (0, 1]$ reduces the influence of each individual weak learner, which acts as a form of regularization to prevent overfitting.\n\nThis process is repeated for a fixed number of stages, $M=12$.\n\n### Fitting the Regression Stump\n\nA regression stump is a decision tree with only one split. It is defined by a feature index $j \\in \\{1, \\dots, d\\}$, a threshold $t \\in \\mathbb{R}$, and two constant output values, $a$ and $b$. Its functional form is:\n$$\nh(\\mathbf{x}; j, t, a, b) = a \\cdot \\mathbb{1}\\{x_j  t\\} + b \\cdot \\mathbb{1}\\{x_j \\ge t\\}\n$$\nwhere $\\mathbb{1}\\{\\cdot\\}$ is the indicator function.\n\nTo find the optimal stump $h_m$ for a given set of pseudo-residuals $\\{r_i^{(m)}\\}_{i=1}^n$, the following procedure is executed:\n1.  For each feature $j \\in \\{1, 2\\}$:\n    a.  Identify the set of unique values $\\{v_1, v_2, \\dots, v_k\\}$ for feature $j$ present in the training data $\\mathbf{X}$.\n    b.  Construct a set of candidate thresholds $\\{t_s\\}_{s=1}^{k-1}$ by taking the midpoint between each pair of consecutive unique values: $t_s = (v_s + v_{s+1})/2$.\n    c.  For each candidate $(j, t_s)$:\n        i.   Partition the sample indices into two sets: $I_{left} = \\{i \\mid x_{ij}  t_s\\}$ and $I_{right} = \\{i \\mid x_{ij} \\ge t_s\\}$.\n        ii.  For this split, the optimal constants $a$ and $b$ that minimize the sum of squared errors are the means of the residuals within each partition:\n             $$\n             a = \\frac{1}{|I_{left}|} \\sum_{i \\in I_{left}} r_i^{(m)}, \\quad b = \\frac{1}{|I_{right}|} \\sum_{i \\in I_{right}} r_i^{(m)}\n             $$\n        iii. Calculate the sum of squared errors (SSE) for this stump:\n             $$\n             SSE(j, t_s) = \\sum_{i \\in I_{left}} (r_i^{(m)} - a)^2 + \\sum_{i \\in I_{right}} (r_i^{(m)} - b)^2\n             $$\n2.  The best stump $(j^*, t^*, a^*, b^*)$ is the one that yields the minimum SSE over all possible features $j$ and thresholds $t_s$. Should no valid split be possible for any feature (e.g., if all samples have identical feature vectors), the stump defaults to a constant predictor equal to the global mean of the residuals.\n\n### Execution Plan\n\nThe implementation adheres to the following steps for each of the specified shrinkage values $\\nu \\in \\{0.01, 0.1, 0.3, 0.7, 1.0\\}$:\n1.  **Data Preparation**: The target values $y_i$ are computed from the given feature matrix $\\mathbf{X}$ using the deterministic rule:\n    $$\n    y_i = 2 \\cdot \\mathbb{1}\\{x_{i1}  0\\} - \\mathbb{1}\\{x_{i2}  0\\} + 0.3x_{i1} + 0.1x_{i2}\n    $$\n2.  **Boosting Loop**:\n    - Initialize the vector of predictions $\\mathbf{f}_0$ to all zeros.\n    - For $m = 1, \\dots, 12$:\n        a. Compute pseudo-residuals $\\mathbf{r}^{(m)} = \\mathbf{y} - \\mathbf{f}_{m-1}$.\n        b. Fit an optimal regression stump $h_m$ to the pairs $(\\mathbf{X}, \\mathbf{r}^{(m)})$.\n        c. Obtain the predictions $\\mathbf{h}_m$ from the fitted stump on the training data $\\mathbf{X}$.\n        d. Update the model predictions: $\\mathbf{f}_m = \\mathbf{f}_{m-1} + \\nu \\mathbf{h}_m$.\n3.  **Evaluation**: After $M=12$ stages, the final prediction vector is $\\mathbf{f}_M$. The following two quantities are computed:\n    - **Euclidean Prediction Norm**: $\\|f_M\\|_2 = \\sqrt{\\sum_{i=1}^n f_M(\\mathbf{x}_i)^2} = \\|\\mathbf{f}_M\\|_2$.\n    - **Training Loss (MSE)**: $R(f_M) = \\frac{1}{n} \\sum_{i=1}^n (y_i - f_M(\\mathbf{x}_i))^2$.\n4.  **Output Formatting**: The resulting pair $(\\|f_M\\|_2, R(f_M))$ is rounded to $6$ decimal places and stored. The final output is a list containing these pairs for each value of $\\nu$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements Gradient Boosting with Regression Stumps for a specified regression task.\n    \"\"\"\n    \n    # --- Problem Givens ---\n    X_data = np.array([\n        [-2.0, -1.0], [-2.0, 1.0], [-1.0, -1.0], [-1.0, 1.0],\n        [0.0, -1.0], [0.0, 1.0], [1.0, -1.0], [1.0, 1.0],\n        [2.0, -1.0], [2.0, 1.0], [-3.0, 0.0], [3.0, 0.0],\n        [0.5, -0.5], [-0.5, 0.5]\n    ])\n    n_samples, n_features = X_data.shape\n    M = 12\n    nu_values = [0.01, 0.1, 0.3, 0.7, 1.0]\n\n    def compute_y(X):\n        \"\"\"Computes the target values based on the given deterministic rule.\"\"\"\n        x1 = X[:, 0]\n        x2 = X[:, 1]\n        y = (2.0 * (x1 > 0) - 1.0 * (x2  0) + 0.3 * x1 + 0.1 * x2)\n        return y\n\n    class RegressionStump:\n        \"\"\"A regression stump weak learner.\"\"\"\n        def __init__(self):\n            self.feature_index = None\n            self.threshold = None\n            self.left_value = None\n            self.right_value = None\n\n        def fit(self, X, y):\n            \"\"\"\n            Finds the best split (feature and threshold) for the stump.\n            The best split is the one that minimizes the sum of squared errors.\n            \"\"\"\n            n_samples, n_features = X.shape\n            best_sse = float('inf')\n            \n            # Default prediction is the global mean if no split is found\n            global_mean = np.mean(y)\n\n            for j in range(n_features):\n                feature_values = X[:, j]\n                unique_values = np.unique(feature_values)\n\n                if len(unique_values) = 1:\n                    continue  # Cannot split on this feature\n\n                thresholds = (unique_values[:-1] + unique_values[1:]) / 2.0\n\n                for t in thresholds:\n                    left_indices = np.where(feature_values  t)[0]\n                    right_indices = np.where(feature_values >= t)[0]\n                    \n                    # This check is technically redundant with the len(unique_values)>1 check\n                    if len(left_indices) == 0 or len(right_indices) == 0:\n                        continue\n\n                    y_left, y_right = y[left_indices], y[right_indices]\n                    \n                    mean_left = np.mean(y_left)\n                    mean_right = np.mean(y_right)\n                    \n                    sse = np.sum((y_left - mean_left)**2) + np.sum((y_right - mean_right)**2)\n\n                    if sse  best_sse:\n                        best_sse = sse\n                        self.feature_index = j\n                        self.threshold = t\n                        self.left_value = mean_left\n                        self.right_value = mean_right\n            \n            # If no split improved upon the initial 'inf' sse, it means no valid split was found.\n            # In this case, use the global mean.\n            if self.feature_index is None:\n                self.left_value = global_mean\n                self.right_value = global_mean\n\n        def predict(self, X):\n            \"\"\"Makes predictions using the fitted stump.\"\"\"\n            if self.feature_index is None:\n                # No split was found, predict the global mean for all samples.\n                return np.full(X.shape[0], self.left_value)\n            \n            feature_values = X[:, self.feature_index]\n            return np.where(feature_values  self.threshold, self.left_value, self.right_value)\n\n    # --- Main Execution Logic ---\n    y_true = compute_y(X_data)\n    results = []\n\n    for nu in nu_values:\n        f_m_preds = np.zeros(n_samples)  # f_0(x) = 0\n\n        for _ in range(M):\n            residuals = y_true - f_m_preds\n            \n            stump = RegressionStump()\n            stump.fit(X_data, residuals)\n            \n            h_m_preds = stump.predict(X_data)\n            \n            f_m_preds += nu * h_m_preds\n\n        # After M stages, f_m_preds holds the final predictions f_M\n        norm_fM = np.linalg.norm(f_m_preds)\n        loss_fM = np.mean((y_true - f_m_preds)**2)\n\n        results.append([round(norm_fM, 6), round(loss_fM, 6)])\n\n    # Format the final output string as specified\n    formatted_pairs = [f\"[{r[0]},{r[1]}]\" for r in results]\n    print(f\"[{','.join(formatted_pairs)}]\")\n\nsolve()\n```", "id": "3125539"}, {"introduction": "While the squared-error loss is mathematically convenient, its sensitivity to outliers can be a major drawback when dealing with noisy, real-world data. Robust loss functions offer a powerful alternative. This hands-on comparison [@problem_id:3125607] challenges you to implement and evaluate gradient boosting models under both standard squared-error loss and the robust Huber loss, using data contaminated with heavy-tailed noise to highlight the practical benefits of robust regression in boosting.", "problem": "You are asked to implement a small, self-contained experiment that compares the behavior of gradient boosting for regression under two different loss functions when the observation noise has heavy tails. You will use the framework of empirical risk minimization and stagewise additive modeling. The setting is as follows.\n\nAssume a scalar input $x \\in \\mathbb{R}$ and a true regression function $f^*(x) = 2x$. Observations are generated according to $y = f^*(x) + \\epsilon$, where $\\epsilon$ has a Student’s $t$ distribution with degrees of freedom $\\nu$, scaled to have unit variance whenever $\\nu  2$. Concretely, if $t \\sim t_\\nu$ is a standard Student’s $t$ variate, define $\\epsilon = t \\cdot \\sqrt{\\frac{\\nu - 2}{\\nu}}$ so that $\\mathrm{Var}(\\epsilon) = 1$ whenever $\\nu  2$.\n\nYou will build two gradient boosting regressors that differ only in the choice of loss function:\n- The least-squares loss $L_2$: $\\ell(y, F) = \\frac{1}{2}(y - F)^2$.\n- The Huber loss with threshold $\\delta  0$: \n  $$\\ell(y,F) = \\begin{cases}\n  \\frac{1}{2}(y - F)^2,  \\text{if } |y - F| \\le \\delta,\\\\\n  \\delta |y - F| - \\frac{1}{2}\\delta^2,  \\text{otherwise.}\n  \\end{cases}$$\n\nBoth models must be implemented as stagewise additive models $F_m(x) = F_{m-1}(x) + \\eta \\, h_m(x)$ for $m = 1,2,\\dots,M$, where $\\eta \\in (0,1]$ is a given learning rate, and $h_m$ is a decision stump weak learner that maps $x$ to two regions $(-\\infty, s]$ and $(s, \\infty)$ with region-specific constant predictions determined by least squares on the current pseudo-residuals. The initial prediction $F_0$ must minimize the empirical risk over constants for the chosen loss:\n- For $L_2$, this is the empirical mean of $y$.\n- For the Huber loss with threshold $\\delta$, the optimal constant $c$ satisfies $\\sum_{i=1}^n \\psi(y_i - c) = 0$, where $\\psi(r) = r$ if $|r| \\le \\delta$ and $\\psi(r) = \\delta \\,\\mathrm{sign}(r)$ otherwise. You must compute this $c$ by solving the one-dimensional convex root-finding problem for the equation $\\sum_{i=1}^n \\psi(y_i - c) = 0$ with a bracketing method that guarantees a solution.\n\nAt each boosting iteration $m$, you must:\n- Compute the per-sample gradient with respect to the prediction $g_i^{(m)} = \\frac{\\partial \\ell(y_i, F_{m-1}(x_i))}{\\partial F}$ evaluated at the current model $F_{m-1}$. For $L_2$, this equals $F_{m-1}(x_i) - y_i$. For the Huber loss, it equals $-\\psi(y_i - F_{m-1}(x_i))$ using the same $\\psi$ as above.\n- Define the pseudo-residuals as the negative gradient $r_i^{(m)} = - g_i^{(m)}$.\n- Fit the decision stump $h_m$ to the targets $\\{r_i^{(m)}\\}_{i=1}^n$ by minimizing empirical squared error over all single-threshold splits. For a fixed split threshold $s$, the optimal region constants are the empirical means of the targets on each side of the split. You must select the split that yields the smallest total sum of squared errors over the two regions.\n\nYou must evaluate the following two statistics for each loss:\n- The final median absolute error after $M$ iterations: $\\mathrm{MedAE} = \\mathrm{median}_i\\left(|y_i - F_M(x_i)|\\right)$.\n- The average, across boosting iterations, of the median gradient magnitudes, defined as\n  $$\\overline{G} = \\frac{1}{M} \\sum_{m=1}^M \\mathrm{median}_i\\left(|g_i^{(m)}|\\right),$$\n  where $g_i^{(m)}$ is defined above and the median is computed over the sample index $i \\in \\{1,\\dots,n\\}$ for each $m$.\n\nImplement the above and run it on the following test suite. For each test case, generate $n$ inputs $x_i$ independently and identically distributed from the uniform distribution on $[-1, 1]$, and generate outputs as described with the specified degrees of freedom $\\nu$. Use the specified random seed for reproducibility. Use the same dataset to train both losses, and report metrics on that dataset. Use the same weak learner class and learning rate across both losses.\n\nTest suite parameter sets, each being $(\\nu, \\delta, M, \\eta, n, \\text{seed})$:\n- Case $1$: $(\\nu=\\;3.0,\\; \\delta=\\;1.0,\\; M=\\;30,\\; \\eta=\\;0.1,\\; n=\\;256,\\; \\text{seed}=\\;12345)$.\n- Case $2$: $(\\nu=\\;10.0,\\; \\delta=\\;1.0,\\; M=\\;30,\\; \\eta=\\;0.1,\\; n=\\;256,\\; \\text{seed}=\\;12346)$.\n- Case $3$: $(\\nu=\\;2.5,\\; \\delta=\\;0.5,\\; M=\\;30,\\; \\eta=\\;0.1,\\; n=\\;256,\\; \\text{seed}=\\;12347)$.\n\nYour program must produce a single line of output containing a list of lists, one sublist per test case, with each sublist equal to \n$[\\mathrm{MedAE}_{L_2}, \\mathrm{MedAE}_{\\text{Huber}}, \\overline{G}_{L_2}, \\overline{G}_{\\text{Huber}}]$ rounded to six decimal places. The exact output format must be a single line containing a string of the form \n$[[a_{11},a_{12},a_{13},a_{14}],[a_{21},a_{22},a_{23},a_{24}],[a_{31},a_{32},a_{33},a_{34}]]$ \nwith no spaces anywhere.\n\nFundamental base you must use:\n- Empirical risk minimization for a function $F$: minimize $\\frac{1}{n} \\sum_{i=1}^n \\ell(y_i, F(x_i))$.\n- Stagewise additive modeling: $F_m = F_{m-1} + \\eta h_m$ with $h_m$ chosen to reduce empirical risk.\n- Negative gradient fitting: at iteration $m$, fit $h_m$ to the targets $r_i^{(m)} = -\\left.\\frac{\\partial \\ell(y_i, F(x_i))}{\\partial F}\\right|_{F=F_{m-1}}$.\n- Least squares optimal constant on a region: the empirical mean of the targets on that region minimizes the sum of squared errors.\n\nNo other formulas are to be assumed. You must derive all additional steps from these principles within your implementation. There are no physical units in this problem. Angles are not used. Percentages are not used.\n\nYour program should produce a single line of output containing the results as a comma-separated list of lists enclosed in square brackets, for example $[[r_{11},r_{12},r_{13},r_{14}],[r_{21},r_{22},r_{23},r_{24}],[r_{31},r_{32},r_{33},r_{34}]]$.", "solution": "The problem requires the implementation and comparison of two gradient boosting regression models, one using the standard least-squares ($L_2$) loss and the other using the more robust Huber loss. The comparison is performed in a setting where the observation noise exhibits heavy tails, simulated using a Student's $t$-distribution. The implementation is based on the principles of empirical risk minimization and stagewise additive modeling.\n\nThe general structure of the model is a stagewise additive function of the form:\n$$F_m(x) = F_{m-1}(x) + \\eta \\, h_m(x)$$\nwhere $m=1, 2, \\dots, M$ is the boosting iteration, $\\eta \\in (0, 1]$ is the learning rate, and $h_m(x)$ is a simple function called a weak learner. The overall model $F_M(x)$ is an approximation to the true function $f^*(x)$. The process starts with an initial model $F_0(x)$, which is a constant value chosen to minimize the empirical risk for the chosen loss function.\n\nAt each iteration $m$, the gradient boosting algorithm seeks to improve the model by adding a weak learner that points in the direction of steepest descent on the loss surface. This direction is given by the negative gradient of the loss function with respect to the current model's predictions. The per-sample negative gradients are called pseudo-residuals:\n$$r_i^{(m)} = -\\left[\\frac{\\partial \\ell(y_i, F(x_i))}{\\partial F}\\right]_{F=F_{m-1}}$$\nThe weak learner $h_m(x)$ is then fitted to these pseudo-residuals, typically by minimizing squared error.\n\nThe weak learner specified is a decision stump, which partitions the input space $\\mathbb{R}$ at a split point $s$ into two regions, $(-\\infty, s]$ and $(s, \\infty)$. The stump produces a constant prediction in each region. To fit the stump $h_m$ to the pseudo-residuals $\\{r_i^{(m)}\\}_{i=1}^n$, we seek the split point $s$ and the two constant predictions, $c_1$ and $c_2$, that minimize the sum of squared errors:\n$$\\sum_{i: x_i \\le s} (r_i^{(m)} - c_1)^2 + \\sum_{i: x_i  s} (r_i^{(m)} - c_2)^2$$\nFor any given split $s$, the optimal constants are the means of the pseudo-residuals in each respective region. The best split point $s$ is found by iterating through all possible splits and selecting the one that results in the minimum total sum of squared errors. An efficient algorithm can find the optimal split in $O(n \\log n)$ time by first sorting the data by the feature $x$ and then using running sums to calculate the sum of squared errors for each potential split.\n\nThe specifics for each loss function are as follows:\n\n**1. Least-Squares ($L_2$) Loss**\nThe loss function is $\\ell(y, F) = \\frac{1}{2}(y - F)^2$.\n- **Initial Prediction $F_0$**: To minimize the empirical risk $\\frac{1}{n} \\sum_{i=1}^n \\frac{1}{2}(y_i - c)^2$, we set the derivative with respect to $c$ to zero: $\\sum_i -(y_i - c) = 0$. This yields the optimal constant $c = \\frac{1}{n}\\sum_{i=1}^n y_i$, which is the sample mean of the targets. So, $F_0(x) = \\bar{y}$.\n- **Pseudo-residuals**: The gradient with respect to the prediction $F$ at $F_{m-1}(x_i)$ is $\\frac{\\partial \\ell}{\\partial F} = F - y$, evaluated at $F_{m-1}(x_i)$. The pseudo-residuals are therefore:\n$$r_i^{(m)} = -(F_{m-1}(x_i) - y_i) = y_i - F_{m-1}(x_i)$$\nThese are simply the ordinary residuals of the model at iteration $m-1$.\n\n**2. Huber Loss**\nThe Huber loss function with threshold $\\delta  0$ is defined as:\n$$\\ell(y,F) = \\begin{cases}\n  \\frac{1}{2}(y - F)^2,  \\text{if } |y - F| \\le \\delta \\\\\n  \\delta |y - F| - \\frac{1}{2}\\delta^2,  \\text{otherwise.}\n  \\end{cases}$$\nThis loss function behaves quadratically for small errors and linearly for large errors, making it less sensitive to outliers than the $L_2$ loss.\n- **Initial Prediction $F_0$**: The optimal constant $c$ minimizes the empirical risk $\\frac{1}{n} \\sum_{i=1}^n \\ell(y_i, c)$. The optimality condition is that the derivative of the total loss with respect to $c$ is zero. This leads to the equation $\\sum_{i=1}^n \\psi(y_i - c) = 0$, where $\\psi$ is the derivative of the Huber loss with respect to its argument $r=y-F$:\n$$\\psi(r) = \\frac{\\partial \\ell}{\\partial r} = \\begin{cases}\n  r,  \\text{if } |r| \\le \\delta \\\\\n  \\delta \\cdot \\mathrm{sign}(r),  \\text{otherwise.}\n  \\end{cases}$$\nThe function $g(c) = \\sum_{i=1}^n \\psi(y_i - c)$ is monotonic and continuous, so its root can be reliably found using a bisection method on a suitable interval, such as $[\\min(y), \\max(y)]$.\n- **Pseudo-residuals**: The gradient with respect to the prediction $F$ at $F_{m-1}(x_i)$ is $\\frac{\\partial \\ell}{\\partial F} = -\\psi(y - F)$. The pseudo-residuals are therefore:\n$$r_i^{(m)} = -(-\\psi(y_i - F_{m-1}(x_i))) = \\psi(y_i - F_{m-1}(x_i))$$\nThis means the residuals $y_i - F_{m-1}(x_i)$ are passed through the clipping function $\\psi$ to generate the targets for the weak learner. This taming of large residuals is the core of the Huber method's robustness.\n\nFor each test case specified, the following steps are executed:\n1.  A dataset of $n$ points $(x_i, y_i)$ is generated. The inputs $x_i$ are drawn from a Uniform$(-1, 1)$ distribution. The outputs are generated as $y_i = 2x_i + \\epsilon_i$, where the noise $\\epsilon_i$ is sampled from a Student's $t$-distribution with $\\nu$ degrees of freedom, scaled by $\\sqrt{(\\nu-2)/\\nu}$ to ensure unit variance for $\\nu  2$.\n2.  Two gradient boosting models are trained on this dataset for $M$ iterations with a learning rate of $\\eta$: one with the $L_2$ loss and one with the Huber loss.\n3.  For each model, two metrics are computed:\n    - The final Median Absolute Error: $\\mathrm{MedAE} = \\mathrm{median}_i(|y_i - F_M(x_i)|)$.\n    - The average median gradient magnitude: $\\overline{G} = \\frac{1}{M} \\sum_{m=1}^M \\mathrm{median}_i(|g_i^{(m)}|)$, where $g_i^{(m)}$ is the per-sample gradient at iteration $m$.\n\nThe implementation encapsulates these principles into a main function that iterates through the test cases, functions to run the boosting algorithm for a given loss, and helper utilities for fitting the decision stump and finding the initial constant for the Huber loss.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import t as t_dist\nfrom scipy.optimize import bisect\n\nclass DecisionStump:\n    \"\"\"A decision stump for 1D regression.\"\"\"\n    def __init__(self, split_val, left_pred, right_pred):\n        self.split_val = split_val\n        self.left_pred = left_pred\n        self.right_pred = right_pred\n\n    def predict(self, x):\n        \"\"\"Predicts the output for a given input array x.\"\"\"\n        if self.split_val is None:\n            # If no split, predict the same value for all inputs.\n            return np.full_like(x, self.left_pred, dtype=float)\n        \n        predictions = np.full_like(x, self.right_pred, dtype=float)\n        predictions[x = self.split_val] = self.left_pred\n        return predictions\n\ndef fit_stump(x, r):\n    \"\"\"\n    Fits a decision stump to predict residuals r from feature x.\n    The stump is chosen to minimize the sum of squared errors.\n    \"\"\"\n    n = len(x)\n    if n == 0:\n        return DecisionStump(None, 0.0, 0.0)\n\n    sort_idx = np.argsort(x)\n    x_sorted, r_sorted = x[sort_idx], r[sort_idx]\n\n    best_sse = np.inf\n    best_split_val = None\n    best_left_pred = None\n    best_right_pred = None\n\n    r_sum_total = np.sum(r_sorted)\n    r2_sum_total = np.sum(r_sorted**2)\n\n    left_sum_r = 0.0\n    left_sum_r2 = 0.0\n\n    for i in range(n - 1):\n        left_sum_r += r_sorted[i]\n        left_sum_r2 += r_sorted[i]**2\n        n_left = i + 1\n        \n        # Avoid splitting between identical x values\n        if x_sorted[i] == x_sorted[i+1]:\n            continue\n\n        n_right = n - n_left\n        right_sum_r = r_sum_total - left_sum_r\n        right_sum_r2 = r2_sum_total - left_sum_r2\n\n        sse_left = left_sum_r2 - (left_sum_r**2) / n_left\n        sse_right = right_sum_r2 - (right_sum_r**2) / n_right\n        total_sse = sse_left + sse_right\n\n        if total_sse  best_sse:\n            best_sse = total_sse\n            best_split_val = (x_sorted[i] + x_sorted[i+1]) / 2.0\n            best_left_pred = left_sum_r / n_left\n            best_right_pred = right_sum_r / n_right\n    \n    if best_split_val is None:\n        # All x values are the same, no split is possible.\n        pred = r.mean()\n        return DecisionStump(None, pred, pred)\n    \n    return DecisionStump(best_split_val, best_left_pred, best_right_pred)\n\ndef find_huber_initial_constant(y, delta):\n    \"\"\"Finds the optimal initial constant for Huber loss via root-finding.\"\"\"\n    def huber_obj_grad(c):\n        residuals = y - c\n        return np.sum(np.clip(residuals, -delta, delta))\n\n    # The optimal constant is guaranteed to be within the range of y\n    y_min, y_max = np.min(y), np.max(y)\n    if y_min == y_max:\n        return y_min\n        \n    return bisect(huber_obj_grad, y_min, y_max)\n\ndef run_gbm(x, y, M, eta, loss_type, delta):\n    \"\"\"Runs a gradient boosting machine for regression.\"\"\"\n    n = len(y)\n    \n    # Initialize model F_0\n    if loss_type == 'l2':\n        F = np.full(n, np.mean(y))\n    elif loss_type == 'huber':\n        f0 = find_huber_initial_constant(y, delta)\n        F = np.full(n, f0)\n    else:\n        raise ValueError(\"Unknown loss type\")\n        \n    all_median_gradients = []\n\n    for _ in range(M):\n        # Compute pseudo-residuals (negative gradients)\n        if loss_type == 'l2':\n            gradients = F - y\n            residuals = -gradients\n        elif loss_type == 'huber':\n            residuals_unclipped = y - F\n            residuals = np.clip(residuals_unclipped, -delta, delta)\n            gradients = -residuals\n        \n        all_median_gradients.append(np.median(np.abs(gradients)))\n\n        # Fit a weak learner (decision stump) to the pseudo-residuals\n        stump = fit_stump(x, residuals)\n        \n        # Update the model\n        F += eta * stump.predict(x)\n        \n    # Calculate final metrics\n    med_ae = np.median(np.abs(y - F))\n    avg_med_grad = np.mean(all_median_gradients)\n    \n    return med_ae, avg_med_grad\n\n\ndef solve():\n    \"\"\"Main function to run the specified test suite and print results.\"\"\"\n    test_cases = [\n        # (nu,      delta, M,  eta,   n,   seed)\n        (3.0,     1.0,   30, 0.1, 256, 12345),\n        (10.0,    1.0,   30, 0.1, 256, 12346),\n        (2.5,     0.5,   30, 0.1, 256, 12347)\n    ]\n    \n    all_case_results = []\n    \n    for nu, delta, M, eta, n, seed in test_cases:\n        rng = np.random.default_rng(seed)\n        \n        # Generate data\n        x = rng.uniform(low=-1.0, high=1.0, size=n)\n        \n        # Generate noise from scaled Student's t-distribution\n        t_samples = t_dist.rvs(df=nu, size=n, random_state=rng)\n        # Scale to unit variance (valid since nu > 2 for all cases)\n        noise = t_samples * np.sqrt((nu - 2.0) / nu)\n        \n        # Generate observations\n        y = 2.0 * x + noise\n\n        # Run L2 model\n        l2_medae, l2_avg_g = run_gbm(x, y, M, eta, 'l2', delta=None)\n        \n        # Run Huber model\n        huber_medae, huber_avg_g = run_gbm(x, y, M, eta, 'huber', delta=delta)\n        \n        all_case_results.append([l2_medae, huber_medae, l2_avg_g, huber_avg_g])\n\n    # Format the output string\n    output_parts = []\n    for case_res in all_case_results:\n        formatted_res = ','.join([f\"{val:.6f}\" for val in case_res])\n        output_parts.append(f\"[{formatted_res}]\")\n    \n    final_output = f\"[{','.join(output_parts)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3125607"}]}