{"hands_on_practices": [{"introduction": "The core idea behind cost-complexity pruning is to balance a model's performance on training data with its complexity. This exercise provides a foundational scenario to illustrate why this trade-off is crucial [@problem_id:3189470]. By comparing two hypothetical subtrees with identical training error but different sizes, you will see how the regularization parameter $\\alpha$ formally implements the principle of parsimony, also known as Occam's razor.", "problem": "Consider a binary classification task and a large tree built by Classification and Regression Trees (CART). Let a fixed internal node $t$ of this large tree be the root of a subtree, and suppose two alternative prunings of the subtree at $t$ are candidates for selection: a deeper subtree $T_{A}$ and a shallower subtree $T_{B}$. For each subtree $T$, define the empirical training error under the $0$–$1$ loss as\n$$\nR(T) \\equiv \\sum_{l \\in \\text{Leaves}(T)} \\sum_{i \\in l} \\mathbf{1}\\!\\left(y_{i} \\neq \\hat{y}_{l}\\right),\n$$\nwhere $y_{i}$ is the class label of training point $i$, and $\\hat{y}_{l}$ is the leaf prediction (the majority class in leaf $l$). Let $|\\!T\\!|$ denote the number of terminal nodes (leaves) of subtree $T$.\n\nDesign the following scenario. The leaf-level misclassification counts for the deeper subtree $T_{A}$ are given by the multiset $\\{2,1,0,3,1,0,2,3\\}$ so that $|\\!T_{A}\\!| = 8$ and $R(T_{A}) = 12$. The leaf-level misclassification counts for the shallower subtree $T_{B}$ are given by the multiset $\\{4,0,2,3,3\\}$ so that $|\\!T_{B}\\!| = 5$ and $R(T_{B}) = 12$. Starting from the principle of empirical risk minimization and the structural risk minimization idea that model complexity should be penalized, consider the cost-complexity criterion\n$$\nR_{\\alpha}(T) \\equiv R(T) + \\alpha \\, |\\!T\\!|,\n$$\nwhere $\\alpha \\geq 0$ is a regularization parameter.\n\nUsing only these foundational definitions, explain how cost-complexity pruning resolves the tie in $R(T)$ between $T_{A}$ and $T_{B}$ by varying $\\alpha$, and derive the exact threshold value $\\alpha^{\\star}$ at which one subtree strictly dominates the other under $R_{\\alpha}(\\cdot)$ (that is, the smallest $\\alpha$ such that the preferred subtree has strictly smaller $R_{\\alpha}$ than the alternative). Provide your final answer for $\\alpha^{\\star}$ as a single real number. No rounding is required.", "solution": "The problem statement is first validated to ensure it is scientifically grounded, well-posed, objective, and complete.\n\n### Step 1: Extract Givens\n- A binary classification task using a CART-built tree.\n- Two alternative subtree prunings, $T_{A}$ and $T_{B}$, rooted at the same internal node $t$.\n- The empirical training error is defined as $R(T) \\equiv \\sum_{l \\in \\text{Leaves}(T)} \\sum_{i \\in l} \\mathbf{1}\\!\\left(y_{i} \\neq \\hat{y}_{l}\\right)$.\n- The complexity of a tree $T$ is its number of leaves, denoted by $|\\!T\\!|$.\n- For the deeper subtree $T_{A}$:\n  - Number of leaves: $|\\!T_{A}\\!| = 8$.\n  - Total empirical error: $R(T_{A}) = 12$.\n- For the shallower subtree $T_{B}$:\n  - Number of leaves: $|\\!T_{B}\\!| = 5$.\n  - Total empirical error: $R(T_{B}) = 12$.\n- The cost-complexity criterion is defined as $R_{\\alpha}(T) \\equiv R(T) + \\alpha \\, |\\!T\\!|$, with the regularization parameter $\\alpha \\geq 0$.\n- The objective is to explain how cost-complexity pruning resolves the tie in $R(T)$ and to find the threshold value $\\alpha^{\\star}$, defined as the smallest $\\alpha$ for which one subtree is strictly dominant.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is based on the standard and well-established theory of Classification and Regression Trees (CART) and cost-complexity pruning, a core concept in statistical learning. The definitions and setup are correct.\n- **Well-Posed**: The problem is clearly stated with all necessary quantitative information. It asks for a specific value, $\\alpha^{\\star}$, which can be uniquely determined from the given data and definitions.\n- **Objective**: The problem is expressed in precise mathematical language, free from subjectivity or ambiguity.\n- The problem is internally consistent, as the stated total errors $R(T_A)$ and $R(T_B)$ match the sums of the provided leaf-level misclassification counts. There are no contradictions or missing pieces of information.\n\n### Step 3: Verdict and Action\nThe problem is valid. A reasoned solution can be constructed.\n\n### Solution Derivation\n\nThe problem requires us to compare two candidate subtrees, $T_{A}$ and $T_{B}$, which have the same empirical training error but different complexities. This scenario highlights the core principle of structural risk minimization, often associated with Occam's razor: when two models perform equally well on training data, the simpler one should be preferred. Cost-complexity pruning provides a formal mechanism to implement this principle.\n\nThe cost-complexity criterion is given by $R_{\\alpha}(T) = R(T) + \\alpha |\\!T\\!|$. Here, $R(T)$ is the empirical risk (a measure of training data fit), and $\\alpha |\\!T\\!|$ is a penalty term that punishes model complexity, with $|\\!T\\!|$ being the measure of complexity and $\\alpha \\geq 0$ being the parameter that controls the trade-off.\n\nWe are given the following values for the two subtrees:\nFor subtree $T_{A}$:\n- Empirical error: $R(T_{A}) = 12$\n- Complexity (number of leaves): $|\\!T_{A}\\!| = 8$\n\nFor subtree $T_{B}$:\n- Empirical error: $R(T_{B}) = 12$\n- Complexity (number of leaves): $|\\!T_{B}\\!| = 5$\n\nLet's write down the cost-complexity for each subtree as a function of $\\alpha$:\n$$\nR_{\\alpha}(T_{A}) = R(T_{A}) + \\alpha |\\!T_{A}\\!| = 12 + 8\\alpha\n$$\n$$\nR_{\\alpha}(T_{B}) = R(T_{B}) + \\alpha |\\!T_{B}\\!| = 12 + 5\\alpha\n$$\n\nThe goal of pruning is to select the subtree with the minimum cost-complexity $R_{\\alpha}(T)$ for a given $\\alpha$.\n\nFirst, we analyze the case when $\\alpha = 0$. This corresponds to pure empirical risk minimization, where there is no penalty for complexity.\n$$\nR_{0}(T_{A}) = 12 + 8(0) = 12\n$$\n$$\nR_{0}(T_{B}) = 12 + 5(0) = 12\n$$\nIn this case, $R_{0}(T_{A}) = R_{0}(T_{B})$. Based on empirical risk alone, there is no reason to prefer one subtree over the other; they are tied.\n\nNext, we consider the case where $\\alpha > 0$. This introduces a penalty for complexity. The principle of structural risk minimization suggests that the simpler tree, $T_{B}$, should be preferred. Let's verify this by comparing $R_{\\alpha}(T_{A})$ and $R_{\\alpha}(T_{B})$. We seek the condition under which one is strictly smaller than the other.\nLet's determine for which values of $\\alpha$ the subtree $T_{B}$ is preferred, i.e., $R_{\\alpha}(T_{B})  R_{\\alpha}(T_{A})$.\n$$\n12 + 5\\alpha  12 + 8\\alpha\n$$\nSubtracting $12$ from both sides gives:\n$$\n5\\alpha  8\\alpha\n$$\nSubtracting $5\\alpha$ from both sides:\n$$\n0  3\\alpha\n$$\nSince $\\alpha$ is a non-negative parameter, this inequality holds for all $\\alpha > 0$.\n\nThis result explains how cost-complexity pruning resolves the tie. As soon as any positive penalty is applied to complexity (i.e., for any $\\alpha > 0$), the tie is broken. Because $T_{B}$ is simpler ($|\\!T_{B}\\!|  |\\!T_{A}\\!|$), its penalty term $\\alpha |\\!T_{B}\\!|$ is smaller than that of $T_{A}$, making its total cost-complexity $R_{\\alpha}(T_{B})$ strictly smaller. Thus, for any $\\alpha > 0$, $T_{B}$ is the strictly dominant subtree.\n\nThe problem asks for the \"threshold value $\\alpha^{\\star}$ at which one subtree strictly dominates the other.\" This threshold is the value of $\\alpha$ that marks the boundary between the regime of indifference and the regime of strict preference. We can find this boundary point by setting the cost-complexities equal to each other and solving for $\\alpha$:\n$$\nR_{\\alpha}(T_{A}) = R_{\\alpha}(T_{B})\n$$\n$$\n12 + 8\\alpha = 12 + 5\\alpha\n$$\n$$\n3\\alpha = 0\n$$\n$$\n\\alpha = 0\n$$\nThis value, $\\alpha = 0$, is the point of transition. At $\\alpha = 0$, the costs are equal. For any value $\\alpha > 0$, a strict preference for $T_{B}$ emerges. The problem defines $\\alpha^{\\star}$ as the smallest $\\alpha$ such that a strict preference exists. The set of $\\alpha$ values for which a strict preference exists is $(0, \\infty)$. The smallest value for which this condition holds is any $\\epsilon > 0$, however small. The question is asking for the threshold, which is the infimum of this set. The infimum of the set $(0, \\infty)$ is $0$.\n\nTherefore, the threshold value $\\alpha^{\\star}$ at which a strict preference appears is $0$.", "answer": "$$\n\\boxed{0}\n$$", "id": "3189470"}, {"introduction": "Rather than arbitrarily pruning branches, cost-complexity pruning generates a finite, nested sequence of optimal subtrees. This exercise delves into the mechanics of the \"weakest-link\" algorithm, the standard procedure for creating this sequence [@problem_id:3189468]. You will calculate the exact sequence of penalty breakpoints, $\\{\\alpha_{j}\\}$, where the optimal subtree structure changes, providing a concrete understanding of the pruning path and its properties.", "problem": "A regression tree is grown to a known maximal tree $T_{\\max}$ on a training set, and pruning is to be carried out using the cost-complexity criterion. Let $R(T)$ denote the empirical residual sum of squares of a tree $T$ on the training set, and let $|T|$ denote the number of terminal nodes (leaves) of $T$. For any nonnegative penalty parameter $\\alpha \\ge 0$, the cost-complexity of $T$ is defined as $R_{\\alpha}(T) = R(T) + \\alpha |T|$. Weakest-link pruning constructs a nested sequence of subtrees by repeatedly collapsing the internal node(s) whose removal causes the smallest increase in $R(T)$ per leaf removed.\n\nYou are given the following structure for $T_{\\max}$:\n- The root node $r$ has two internal children $L$ and $R$.\n- Each of $L$ and $R$ splits once into two terminal nodes; there are no deeper splits. Thus, $|T_{\\max}| = 4$ and the internal nodes are $\\{r,L,R\\}$.\n- The empirical residual sum of squares if a node is a leaf is given by $R(r)=400$, $R(L)=120$, and $R(R)=200$.\n- The empirical residual sum of squares of each two-leaf subtree is $R(T_{L})=80$ and $R(T_{R})=150$. Consequently, $R(T_{\\max}) = R(T_{L}) + R(T_{R}) = 230$.\n\nTask A. Starting from $T_{\\max}$, carry out weakest-link pruning and compute the nondecreasing sequence of penalty breakpoints $\\{\\alpha_{j}\\}$ at which the pruned subtree changes, from the first prune down to the last prune to a single leaf. Report the breakpoints in order as $\\alpha_{0}, \\alpha_{1}, \\alpha_{2}$, each exactly.\n\nTask B. Consider a parametric family of trees with the same topology and with quantities\n$$\\Delta_{L} := R(L) - R(T_{L}), \\quad \\Delta_{R} := R(R) - R(T_{R}), \\quad \\Delta_{r} := R(r) - \\big(R(L) + R(R)\\big),$$\nassumed to satisfy the inequalities $\\Delta_{r} + \\Delta_{L} \\ge 2 \\Delta_{R} \\ge 2 \\Delta_{L} > 0$. Under these constraints, derive symbolically the sequence of pruning breakpoints in terms of $\\Delta_{L}, \\Delta_{R}, \\Delta_{r}$, and then give a single closed-form expression for the largest gap between consecutive breakpoints in that sequence. Explain concisely the geometric reason, in the $(\\alpha, R_{\\alpha}(T))$ plane, why these constraints produce unusually large gaps.\n\nAnswer formatting: Provide your final answer as a single row matrix with four entries, in the order\n$$(\\alpha_{0}, \\alpha_{1}, \\alpha_{2}, \\text{largest-gap-expression}).$$\nNo rounding is required.", "solution": "The user-provided problem has been analyzed and validated as a well-posed and scientifically grounded question in the field of statistical learning.\n\nThe problem requires the application of cost-complexity pruning (also known as weakest-link pruning) to a given regression tree. The core of this method is to find a sequence of subtrees of the maximal tree $T_{\\max}$ that are optimal for a continuously increasing penalty parameter $\\alpha$. The cost-complexity of a tree $T$ is defined as $R_{\\alpha}(T) = R(T) + \\alpha |T|$, where $R(T)$ is the residual sum of squares and $|T|$ is the number of terminal nodes.\n\nThe pruning process generates a nested sequence of subtrees $T_0 \\supset T_1 \\supset \\dots \\supset T_k$, where $T_0 = T_{\\max}$. At each stage, the algorithm prunes the internal node $t$ that corresponds to the \"weakest link\", defined as the node that minimizes the function $g(t, T) = \\frac{R(t) - R(T_t)}{|T_t| - 1}$. Here, $T_t$ is the subtree rooted at $t$, and $R(t)$ denotes the residual sum of squares if node $t$ were a terminal node. The value $\\alpha_{k+1} = \\min_{t} g(t, T_k)$ becomes the next breakpoint in the sequence, which is the value of the penalty parameter at which the optimal subtree transitions from $T_k$ to $T_{k+1}$.\n\nLet's define the trees involved in the pruning sequence:\n- $T_0 = T_{\\max}$: The maximal tree with $|T_0| = 4$ leaves. Its RSS is given as $R(T_0) = R(T_{\\max}) = 230$.\n- $T_1$: The tree obtained by pruning the node $L$. It has $|T_1| = 3$ leaves ($L$ and the two children of $R$). Its RSS is $R(T_1) = R(L) + R(T_R) = 120 + 150 = 270$.\n- $T_2$: The tree obtained by pruning both $L$ and $R$. It has $|T_2| = 2$ leaves ($L$ and $R$). Its RSS is $R(T_2) = R(L) + R(R) = 120 + 200 = 320$.\n- $T_3$: The root tree, consisting of a single leaf $\\{r\\}$. It has $|T_3| = 1$ leaf. Its RSS is $R(T_3) = R(r) = 400$.\n\nAnother possible 3-leaf tree exists by pruning node $R$ first, let's call it $T'_1$. Its RSS would be $R(T'_1) = R(T_L) + R(R) = 80 + 200 = 280$. Since $R(T_1)=270  R(T'_1)=280$, for any $\\alpha>0$, $R_\\alpha(T_1)  R_\\alpha(T'_1)$. Thus, $T'_1$ will never be an optimal subtree and is not on the lower convex hull of the $(|T|,R(T))$ points. The sequence $T_0, T_1, T_2, T_3$ is the correct sequence of optimal subtrees as $\\alpha$ increases.\n\n### Task A: Numerical Calculation of Breakpoints\n\nWe start with the tree $T_0 = T_{\\max}$ and determine the first pruning step. The internal nodes are $L$ and $R$.\nThe weakest-link parameters are:\n- For node $L$: $g(L, T_0) = \\frac{R(L) - R(T_L)}{|T_L| - 1} = \\frac{120 - 80}{2 - 1} = \\frac{40}{1} = 40$.\n- For node $R$: $g(R, T_0) = \\frac{R(R) - R(T_R)}{|T_R| - 1} = \\frac{200 - 150}{2 - 1} = \\frac{50}{1} = 50$.\n\nThe minimum value is $40$, so the first breakpoint is $\\alpha_0 = 40$. At this point, node $L$ is pruned, and the optimal tree becomes $T_1$.\n\nNext, we start with tree $T_1$. Its internal nodes are $R$ and the root $r$.\n- For node $R$: The branch to be pruned is $T_R$. $g(R, T_1) = \\frac{R(R) - R(T_R)}{|T_R| - 1} = \\frac{50}{1} = 50$.\n- For the root node $r$: Pruning at $r$ means collapsing the entire tree $T_1$ into the single node $\\{r\\}$. The increase in RSS is $R(r) - R(T_1) = 400 - 270 = 130$. The number of leaves is reduced by $|T_1| - 1 = 3 - 1 = 2$.\n$g(r, T_1) = \\frac{R(r) - R(T_1)}{|T_1| - 1} = \\frac{130}{2} = 65$.\n\nThe minimum value is $50$, so the second breakpoint is $\\alpha_1 = 50$. At this point, node $R$ is pruned, and the optimal tree becomes $T_2$.\n\nFinally, we start with tree $T_2$. Its only internal node is the root $r$.\n- For the root node $r$: Pruning means collapsing $T_2$ into $\\{r\\}$. The increase in RSS is $R(r) - R(T_2) = 400 - 320 = 80$. The number of leaves is reduced by $|T_2| - 1 = 2 - 1 = 1$.\n$g(r, T_2) = \\frac{R(r) - R(T_2)}{|T_2| - 1} = \\frac{80}{1} = 80$.\n\nThis must be the next breakpoint, so $\\alpha_2 = 80$. For $\\alpha \\ge 80$, the optimal tree is the root tree $T_3$.\n\nThe sequence of breakpoints is $\\{\\alpha_0, \\alpha_1, \\alpha_2\\} = \\{40, 50, 80\\}$.\n\n### Task B: Symbolic Derivation and Analysis\n\nWe are given the definitions:\n$\\Delta_{L} = R(L) - R(T_{L})$\n$\\Delta_{R} = R(R) - R(T_{R})$\n$\\Delta_{r} = R(r) - (R(L) + R(R))$\nAnd the constraints $\\Delta_{r} + \\Delta_{L} \\ge 2 \\Delta_{R} \\ge 2 \\Delta_{L} > 0$.\n\nThe pruning breakpoints can be found by finding the values of $\\alpha$ at which the optimal tree changes. This occurs when $R_\\alpha(T_k) = R_\\alpha(T_{k+1})$.\n1.  **Breakpoint $\\alpha_0$ (transition from $T_0$ to $T_1$)**:\n    $R_{\\alpha}(T_0) = R_{\\alpha}(T_1) \\implies R(T_0) + \\alpha |T_0| = R(T_1) + \\alpha |T_1|$\n    $R(T_{\\max}) + 4\\alpha = (R(L) + R(T_R)) + 3\\alpha$\n    $\\alpha = R(L) + R(T_R) - R(T_{\\max}) = R(L) + R(T_R) - (R(T_L) + R(T_R)) = R(L) - R(T_L) = \\Delta_L$.\n    So, $\\alpha_0 = \\Delta_L$. This is consistent with the weakest link calculation, as from $2\\Delta_R \\ge 2\\Delta_L$, we get $\\Delta_R \\ge \\Delta_L$, making $L$ the weakest link.\n\n2.  **Breakpoint $\\alpha_1$ (transition from $T_1$ to $T_2$)**:\n    $R_{\\alpha}(T_1) = R_{\\alpha}(T_2) \\implies R(T_1) + 3\\alpha = R(T_2) + 2\\alpha$\n    $\\alpha = R(T_2) - R(T_1) = (R(L) + R(R)) - (R(L) + R(T_R)) = R(R) - R(T_R) = \\Delta_R$.\n    So, $\\alpha_1 = \\Delta_R$.\n\n3.  **Breakpoint $\\alpha_2$ (transition from $T_2$ to $T_3$)**:\n    $R_{\\alpha}(T_2) = R_{\\alpha}(T_3) \\implies R(T_2) + 2\\alpha = R(T_3) + \\alpha$\n    $\\alpha = R(T_3) - R(T_2) = R(r) - (R(L) + R(R)) = \\Delta_r$.\n    So, $\\alpha_2 = \\Delta_r$.\n\nThe sequence of breakpoints is $\\{\\Delta_L, \\Delta_R, \\Delta_r\\}$. The given inequality $2\\Delta_R \\ge 2\\Delta_L$ implies $\\Delta_R \\ge \\Delta_L$. The inequality $\\Delta_r + \\Delta_L \\ge 2\\Delta_R$ can be rewritten as $\\Delta_r \\ge 2\\Delta_R - \\Delta_L$. Since $\\Delta_R \\ge \\Delta_L$, we have $-\\Delta_L \\ge -\\Delta_R$, which implies $\\Delta_r \\ge 2\\Delta_R - \\Delta_L \\ge 2\\Delta_R - \\Delta_R = \\Delta_R$. Thus, we have the ordering $\\Delta_r \\ge \\Delta_R \\ge \\Delta_L$, which confirms that $\\{\\alpha_0, \\alpha_1, \\alpha_2\\}$ is a non-decreasing sequence.\n\nThe gaps between consecutive breakpoints are:\n- Gap 1: $\\alpha_1 - \\alpha_0 = \\Delta_R - \\Delta_L$.\n- Gap 2: $\\alpha_2 - \\alpha_1 = \\Delta_r - \\Delta_R$.\n\nTo find the largest gap, we compare these two expressions. The constraint $\\Delta_r + \\Delta_L \\ge 2\\Delta_R$ can be rearranged as $\\Delta_r - \\Delta_R \\ge \\Delta_R - \\Delta_L$.\nThis inequality demonstrates that the second gap is greater than or equal to the first gap. Therefore, the largest gap between consecutive breakpoints is $\\Delta_r - \\Delta_R$.\n\n**Geometric Reason for Large Gaps:**\nIn the $(\\alpha, R_{\\alpha}(T))$ plane, the cost-complexity of each candidate subtree $T_k$ is a line $R_{\\alpha}(T_k) = R(T_k) + \\alpha |T_k|$ with slope $|T_k|$. The optimal cost $R_{\\alpha}^* = \\min_k R_{\\alpha}(T_k)$ is the lower envelope of these lines, which is a piecewise-linear, concave function of $\\alpha$. The breakpoints $\\{\\alpha_j\\}$ are the $\\alpha$-coordinates of the vertices of this lower envelope.\nThe breakpoints were found to be $\\alpha_0 = \\Delta_L, \\alpha_1 = \\Delta_R, \\alpha_2 = \\Delta_r$. The quantities $\\Delta_L, \\Delta_R, \\Delta_r$ represent the increase in RSS for each of the three successive pruning steps. The constraint $\\Delta_r - \\Delta_R \\ge \\Delta_R - \\Delta_L$ implies that the sequence of RSS increases $(\\Delta_L, \\Delta_R, \\Delta_r)$ is convex. This \"acceleration\" in the RSS increments as pruning progresses causes the breakpoints—which are equal to these increments—to be spaced farther apart as $\\alpha$ increases. An \"unusually large gap\" arises when $\\Delta_r+\\Delta_L$ is significantly larger than $2\\Delta_R$, making the sequence of RSS increments sharply convex and thus stretching the interval $(\\alpha_1, \\alpha_2)$ over which the tree $T_2$ is optimal.\n\nFinal Answer Assembly:\n$\\alpha_0 = 40$\n$\\alpha_1 = 50$\n$\\alpha_2 = 80$\nLargest gap expression: $\\Delta_r - \\Delta_R$\n\nThese four values will be placed in a row matrix as requested.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n40  50  80  \\Delta_{r} - \\Delta_{R}\n\\end{pmatrix}\n}\n$$", "id": "3189468"}, {"introduction": "With a sequence of candidate trees, the final challenge is selecting the one that will generalize best to new data, a task typically handled by cross-validation. This comprehensive coding practice requires you to implement the full pipeline, from growing a tree to selecting the best pruned version using the common \"1-Standard Error\" rule [@problem_id:3189411]. Moreover, you will develop and apply a sophisticated diagnostic to detect potential \"over-pruning\" by analyzing the curvature of the validation error curve, a key skill in robust model selection.", "problem": "You are asked to design and implement a fully reproducible experiment to detect potential over-pruning in binary decision trees using cost-complexity pruning. The experiment must be programmatic and self-contained, producing a single boolean result for each of several pre-specified cases. Your program must implement the following principles and steps from first principles.\n\nFundamental base:\n- Use the definition of empirical risk minimization and structural risk control: a subtree is preferred when it reduces empirical misclassification error while keeping the number of leaves small. Consider a nonnegative penalty weight denoted by $\\alpha \\ge 0$ that trades off between empirical misclassification and structural complexity measured by the number of leaves.\n- Construct the cost-complexity pruning sequence by weakest-link pruning: at each step, identify the internal node whose removal produces the smallest increase in per-leaf average training misclassification and prune it; this yields an increasing sequence of critical penalty weights at which the optimal subtree changes. This sequence is finite and totally ordered by increasing $\\alpha$.\n- To quantify stability of the validation error near the selected penalty weight, fit a local quadratic model of the validation error as a function of the logarithm of the penalty weight and measure its local curvature (second derivative). Sharp positive curvature near the selected penalty suggests that overly simple subtrees incur rapidly increasing validation error.\n\nYour tasks to implement:\n- Build a univariate ($1$-dimensional) binary classification decision tree using greedy recursive partitioning on a single real-valued feature:\n  - Splitting criterion: at each non-leaf node, consider thresholds between sorted unique feature values and choose the split that minimizes the weighted Gini impurity. For class proportion $p$ in a node, use Gini impurity $2p(1-p)$.\n  - Stopping rules: stop splitting and make a node a leaf if any of these hold: the node is pure, either child would have fewer than a minimum of $3$ samples (minimum leaf size), or there are fewer than $2$ unique feature values available at the node.\n  - Leaf prediction: predict the majority class; in case of ties, predict class $0$.\n- Implement cost-complexity pruning by weakest-link pruning:\n  - For each internal node $t$, define the training misclassification at $t$ as the number of samples in $t$ minus the number of samples of the majority class in $t$, and define the training misclassification of its subtree as the sum of misclassifications over its leaves. Let $|T_t|$ be the number of leaves in the subtree rooted at $t$. The strength of that link is the per-leaf average decrease in training misclassification achieved by the subtree under $t$. Prune the weakest link(s) first and proceed iteratively to obtain the increasing sequence of critical penalty values at which the optimal subtree changes.\n  - For any given $\\alpha \\ge 0$, the optimal subtree is obtained by pruning all links in nondecreasing order of their strengths until the next link would exceed $\\alpha$.\n- Use $K$-fold cross-validation with $K=5$ to estimate mean validation error and its standard error for each subtree along a common penalty grid (formed by the union of all folds’ critical penalties, augmented with $0$ and a sufficiently large value to ensure the root-only tree).\n- Apply the $1$-Standard Error (SE) rule: let $\\bar{e}(\\alpha)$ be the mean validation error over folds at penalty $\\alpha$, and let $s(\\alpha)$ be the standard error (the standard deviation over folds divided by $\\sqrt{5}$). Let $\\alpha_{\\min}$ be any minimizer of $\\bar{e}(\\alpha)$. Select the largest penalty $\\alpha_{\\text{1SE}}$ such that $\\bar{e}(\\alpha_{\\text{1SE}}) \\le \\bar{e}(\\alpha_{\\min}) + s(\\alpha_{\\min})$.\n- Quantify curvature near selection: let $x=\\log(\\alpha+\\delta)$ with a small positive $\\delta$ defined below. Fit a quadratic to the three nearest grid points around $\\alpha_{\\text{1SE}}$ for $\\bar{e}$ as a function of $x$, and estimate the curvature as the second derivative $d^2 \\bar{e}/dx^2$ from that quadratic fit. Also compute a robust baseline by fitting such local quadratics across all interior grid points and taking the median of the absolute curvatures. Use $\\delta = 0.1 \\times \\min\\{\\alpha: \\alpha0\\}$ if there exists a positive $\\alpha$ on the grid, else $\\delta = 10^{-6}$.\n- Define the boolean flag “over-pruning detected” to be true if all of the following hold simultaneously:\n  - The selected penalty is strictly larger than the minimizer, i.e., $\\alpha_{\\text{1SE}} > \\alpha_{\\min}$.\n  - The excess mean validation error at selection exceeds $0.05$ expressed as a decimal, i.e., $\\bar{e}(\\alpha_{\\text{1SE}}) - \\bar{e}(\\alpha_{\\min}) > 0.05$.\n  - The local curvature at selection exceeds the larger of $0.3$ and three times the robust baseline curvature, i.e., $\\frac{d^2 \\bar{e}}{dx^2}\\big|_{\\alpha_{\\text{1SE}}} > \\max\\{0.3, 3\\,\\text{median}_{i} |d^2 \\bar{e}/dx^2|_i\\}$, where the median is taken over all interior grid points.\n\nData generation and test suite:\n- All cases are univariate ($1$-dimensional) binary classification with balanced labels where indicated. Use the provided seeds for both data generation and fold assignment to ensure reproducibility.\n- Case A (well-separated Gaussians): total $N=120$ samples, balanced. Class $0$: $X \\sim \\mathcal{N}(-1.0, 0.3^2)$. Class $1$: $X \\sim \\mathcal{N}(1.0, 0.3^2)$. Use seed $13$.\n- Case B (alternating intervals with light noise): total $N=180$. Draw $X$ uniformly on $[-3, 3]$. Set $Y=1$ if $X \\in [-3,-2] \\cup [-1,0] \\cup [1,2]$, else $Y=0$. Flip each label independently with probability $0.05$. Use seed $7$.\n- Case C (small-sample threshold): total $N=40$. Draw $X \\sim \\mathcal{N}(0,1)$. Set $Y=\\mathbf{1}\\{X>0.2\\}$. Use seed $29$.\n\nImplementation details:\n- Use stratified $5$-fold cross-validation preserving class balance as closely as possible in each fold. When class counts are not divisible by $5$, distribute the remainders to the lower-indexed folds.\n- Minimum leaf size is $3$. Use strictly binary splits on thresholds between sorted unique values.\n- For any evaluation with folds, always compute the union of the folds’ critical penalties into a single sorted grid, include $\\alpha=0$, and include a sufficiently large value to ensure that the root-only tree is represented on the grid.\n\nProgram output requirements:\n- Your program must compute the “over-pruning detected” boolean for each case in the test suite in the order A, B, C and output a single line containing a list of these three booleans in Python list syntax, for example: [True,False,True].\n- No physical units are involved. All angles, if any appear in helper computations, are irrelevant here.\n- All numeric outputs must be booleans. The final output must aggregate the results for all provided test cases into a single line with the exact format described above.\n\nYour final deliverable must be a complete, runnable program that performs all steps above and prints exactly one line in the required format.", "solution": "The user's request is to design and implement a fully specified, reproducible experiment to detect potential over-pruning in univariate binary decision trees. This requires implementing several core algorithms from statistical learning from first principles: decision tree induction, cost-complexity pruning, and cross-validation, culminating in a specific heuristic test for over-pruning. The problem is well-posed, scientifically grounded, and programmatically formalizable.\n\nThe methodology proceeds in four main stages:\n1.  **Decision Tree Growth**: A univariate binary decision tree is constructed using greedy recursive partitioning.\n2.  **Cost-Complexity Pruning**: A sequence of optimally pruned subtrees is generated using the weakest-link pruning algorithm.\n3.  **Cross-Validation**: The generalization error of the subtrees is estimated using $K$-fold cross-validation.\n4.  **Over-pruning Detection**: A specific set of criteria, including a local curvature measure of the validation error curve, is used to flag potential over-pruning.\n\nThe entire process is encapsulated within a Python program that runs the experiment on three predefined test cases and outputs a boolean flag for each.\n\n**1. Decision Tree Growth by Recursive Partitioning**\n\nA binary decision tree is built by repeatedly splitting the data into two partitions. At each node, the algorithm seeks the best split of the form $X  t$, where $X$ is the single feature and $t$ is a threshold.\n\n-   **Splitting Criterion**: The optimal split is the one that maximally reduces the impurity of the child nodes compared to the parent node. The impurity metric used is the Gini impurity, defined for a node with class proportions $p$ and $1-p$ as $G = 2p(1-p)$. The chosen split minimizes the weighted average of the Gini impurities of the two resulting children nodes. Thresholds $t$ are evaluated at the midpoints between unique sorted feature values.\n\n-   **Stopping Conditions**: The recursive splitting process terminates at a node, making it a leaf, if any of the following conditions are met:\n    1.  The node is pure (contains samples of only one class).\n    2.  Any potential split would result in a child node with fewer than a minimum of $3$ samples.\n    3.  There are fewer than two unique feature values left in the node's data partition, making a split impossible.\n\n-   **Leaf Prediction**: A leaf node predicts the majority class among the samples it contains. In the event of a tie in class counts, the prediction defaults to class $0$.\n\n**2. Cost-Complexity Pruning via Weakest-Link Pruning**\n\nA large, complex tree can overfit the training data. Cost-complexity pruning aims to find a smaller subtree that generalizes better by balancing empirical error and model complexity. The cost-complexity of a subtree $T$ is defined as:\n$$C_\\alpha(T) = \\text{Err}(T) + \\alpha |T|$$\nwhere $\\text{Err}(T)$ is the total misclassification count on the training data for the leaves of $T$, $|T|$ is the number of leaves in $T$, and $\\alpha \\ge 0$ is a complexity penalty parameter.\n\nThe pruning process follows the weakest-link algorithm:\n1.  For each internal (non-leaf) node $t$ in the fully grown tree $T_0$, we calculate a critical parameter value $\\alpha_t$. This value represents the point at which the cost of the full subtree rooted at $t$, $T_t$, equals the cost of collapsing $t$ into a single leaf. This value is given by:\n    $$\\alpha_t = \\frac{\\text{err}(t) - \\text{Err}(T_t)}{|T_t| - 1}$$\n    where $\\text{err}(t)$ is the misclassification count if node $t$ were a leaf, $\\text{Err}(T_t)$ is the summed misclassification count of the leaves in the subtree $T_t$, and $|T_t|$ is the number of leaves in $T_t$.\n2.  The node with the smallest positive $\\alpha_t$ is considered the \"weakest link.\"\n3.  By iteratively finding the weakest link and pruning its corresponding branch, we generate a finite sequence of nested subtrees, from the full tree $T_0$ to the root-only tree. This sequence corresponds to an increasing sequence of critical $\\alpha$ values. For any given $\\alpha$, the optimal subtree is the one obtained by pruning all nodes $t$ for which $\\alpha_t \\le \\alpha$.\n\n**3. $K$-fold Cross-Validation for Error Estimation**\n\nTo select the best complexity parameter $\\alpha$, we use $K$-fold cross-validation with $K=5$.\n1.  The training data is partitioned into $K=5$ stratified folds, preserving class proportions.\n2.  For each fold $k \\in \\{1, \\dots, 5\\}$, a full decision tree is grown on the other $K-1$ folds. From this tree, a sequence of critical $\\alpha$ values is extracted.\n3.  A global grid of $\\alpha$ values is created by taking the union of all critical $\\alpha$ values from all $K$ folds, augmented with $\\alpha=0$ and a sufficiently large $\\alpha$ to guarantee pruning to the root.\n4.  For each fold $k$ and each $\\alpha$ on the global grid, the tree trained on the other $K-1$ folds is pruned to the optimal subtree for that $\\alpha$. The validation error of this subtree is then measured on the held-out fold $k$.\n5.  After iterating through all folds, for each $\\alpha$ on the grid, we compute the mean validation error, $\\bar{e}(\\alpha)$, and its standard error, $s(\\alpha)$.\n\n**4. 1-SE Rule and Over-pruning Detection Heuristic**\n\n-   **1-Standard Error (1-SE) Rule**: This rule selects a more parsimonious model that is statistically comparable in performance to the best model. First, we find the parameter $\\alpha_{\\min}$ that minimizes the mean cross-validation error $\\bar{e}(\\alpha)$. Then, we select the largest $\\alpha$ in our grid, denoted $\\alpha_{\\text{1SE}}$, that satisfies:\n    $$\\bar{e}(\\alpha_{\\text{1SE}}) \\le \\bar{e}(\\alpha_{\\min}) + s(\\alpha_{\\min})$$\n\n-   **Over-pruning Detection**: The core of the experiment is to apply a set of criteria to flag potential over-pruning by the 1-SE rule. The boolean flag \"over-pruning detected\" is set to true if and only if all of the following three conditions are met:\n    1.  The selected penalty is strictly larger than the penalty that gives the minimum error: $\\alpha_{\\text{1SE}} > \\alpha_{\\min}$.\n    2.  The increase in mean validation error from the best model is substantial: $\\bar{e}(\\alpha_{\\text{1SE}}) - \\bar{e}(\\alpha_{\\min}) > 0.05$.\n    3.  The local curvature of the error curve at the selected point is high. To assess this, we analyze the error $\\bar{e}$ as a function of $x = \\log(\\alpha + \\delta)$, where $\\delta$ is a small positive constant. A local quadratic is fitted to the three grid points around $\\alpha_{\\text{1SE}}$ to estimate the second derivative (curvature) $\\frac{d^2\\bar{e}}{dx^2}\\big|_{\\alpha_{\\text{1SE}}}$. This curvature must exceed a threshold defined relative to a robust baseline:\n        $$\\frac{d^2 \\bar{e}}{dx^2}\\bigg|_{\\alpha_{\\text{1SE}}} > \\max\\left\\{0.3, 3 \\times \\text{median}_{i} \\left|\\frac{d^2 \\bar{e}}{dx^2}\\right|_i\\right\\}$$\n        The baseline is three times the median of the absolute curvatures computed across all interior points of the $\\alpha$ grid.\n\nThis combination of rules formalizes the intuition that over-pruning occurs when the 1-SE rule chooses a substantially simpler model that is significantly less accurate and lies on a steeply rising portion of the error curve.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\nclass Node:\n    \"\"\"A node in the decision tree.\"\"\"\n    def __init__(self, is_leaf=False, prediction=None, split_threshold=None):\n        self.is_leaf = is_leaf\n        self.prediction = prediction\n        self.split_threshold = split_threshold\n        self.left = None\n        self.right = None\n        # Properties for pruning calculation\n        self.n_samples = 0\n        self.misclassification_error = 0\n        self.n_leaves = 0\n        self.alpha = float('inf')\n\nclass DecisionTreeClassifier:\n    \"\"\"A univariate binary decision tree classifier.\"\"\"\n    def __init__(self, min_leaf_size=3):\n        self.min_leaf_size = min_leaf_size\n        self.root = None\n        self.node_info = {} # Stores class counts for each node\n\n    def _gini_impurity(self, y):\n        \"\"\"Calculates Gini impurity using the 2p(1-p) formula.\"\"\"\n        if y.size == 0:\n            return 0.0\n        p = np.sum(y == 1) / y.size\n        return 2 * p * (1 - p)\n\n    def _find_best_split(self, X, y):\n        \"\"\"Finds the best split for a node.\"\"\"\n        best_split = {'gini': self._gini_impurity(y), 'threshold': None}\n        n_samples = y.size\n        \n        unique_X = np.unique(X)\n        if unique_X.size  2:\n            return best_split\n\n        thresholds = (unique_X[:-1] + unique_X[1:]) / 2.0\n\n        for threshold in thresholds:\n            left_indices = np.where(X  threshold)[0]\n            right_indices = np.where(X >= threshold)[0]\n\n            if len(left_indices)  self.min_leaf_size or len(right_indices)  self.min_leaf_size:\n                continue\n\n            y_left, y_right = y[left_indices], y[right_indices]\n            gini = (len(y_left) / n_samples) * self._gini_impurity(y_left) + \\\n                   (len(y_right) / n_samples) * self._gini_impurity(y_right)\n\n            if gini  best_split['gini']:\n                best_split = {'gini': gini, 'threshold': threshold}\n        \n        return best_split\n\n    def _grow_tree(self, X, y):\n        \"\"\"Recursively grows the decision tree.\"\"\"\n        class_counts = np.bincount(y, minlength=2)\n        pred = 0 if class_counts[0] >= class_counts[1] else 1\n\n        # Stopping conditions\n        is_pure = self._gini_impurity(y) == 0.0\n        no_further_split = len(np.unique(X))  2\n        \n        if is_pure or no_further_split:\n            node = Node(is_leaf=True, prediction=pred)\n            self._finalize_node_properties(node, y, class_counts)\n            return node\n\n        best_split = self._find_best_split(X, y)\n\n        if best_split['threshold'] is None:\n            # No valid split found\n            node = Node(is_leaf=True, prediction=pred)\n            self._finalize_node_properties(node, y, class_counts)\n            return node\n\n        # Create internal node and recurse\n        threshold = best_split['threshold']\n        node = Node(is_leaf=False, split_threshold=threshold)\n        self.node_info[id(node)] = {'class_counts': class_counts}\n        \n        left_indices = np.where(X  threshold)[0]\n        right_indices = np.where(X >= threshold)[0]\n        \n        node.left = self._grow_tree(X[left_indices], y[left_indices])\n        node.right = self._grow_tree(X[right_indices], y[right_indices])\n\n        # Calculate pruning properties\n        node.n_samples = y.size\n        misclass_if_leaf = y.size - np.max(class_counts)\n        subtree_misclass = node.left.misclassification_error + node.right.misclassification_error\n        \n        node.n_leaves = node.left.n_leaves + node.right.n_leaves\n        node.misclassification_error = subtree_misclass\n\n        if node.n_leaves > 1:\n            node.alpha = (misclass_if_leaf - subtree_misclass) / (node.n_leaves - 1)\n        else:\n             node.alpha = float('inf')\n\n        return node\n\n    def _finalize_node_properties(self, node, y, class_counts):\n        \"\"\"Helper to set properties for a new leaf node.\"\"\"\n        node.n_samples = y.size\n        node.misclassification_error = y.size - np.max(class_counts)\n        node.n_leaves = 1\n        self.node_info[id(node)] = {'class_counts': class_counts}\n\n    def fit(self, X, y):\n        \"\"\"Fits the decision tree to the data.\"\"\"\n        self.node_info.clear()\n        self.root = self._grow_tree(X, y.astype(int))\n    \n    def _collect_internal_nodes(self, node, nodes):\n        if not node.is_leaf:\n            nodes.append(node)\n            self._collect_internal_nodes(node.left, nodes)\n            self._collect_internal_nodes(node.right, nodes)\n\n    def get_pruning_sequence(self):\n        \"\"\"Returns the sorted unique critical alpha values for the tree.\"\"\"\n        internal_nodes = []\n        if self.root:\n            self._collect_internal_nodes(self.root, internal_nodes)\n        \n        alphas = [node.alpha for node in internal_nodes if node.alpha != float('inf') and node.alpha > 0]\n        return np.unique(alphas)\n\n    def get_pruned_tree(self, alpha):\n        \"\"\"Returns a new tree pruned at a given alpha.\"\"\"\n        if not self.root:\n            return None\n        \n        def clone_and_prune(node, alpha_thresh):\n            if node.is_leaf:\n                return node\n            \n            # Prune if this node's alpha is smaller or equal to the threshold\n            if node.alpha = alpha_thresh:\n                class_counts = self.node_info[id(node)]['class_counts']\n                pred = 0 if class_counts[0] >= class_counts[1] else 1\n                return Node(is_leaf=True, prediction=pred)\n            \n            new_node = Node(is_leaf=False, split_threshold=node.split_threshold)\n            new_node.left = clone_and_prune(node.left, alpha_thresh)\n            new_node.right = clone_and_prune(node.right, alpha_thresh)\n            return new_node\n\n        pruned_root = clone_and_prune(self.root, alpha)\n        \n        pruned_tree = DecisionTreeClassifier(self.min_leaf_size)\n        pruned_tree.root = pruned_root\n        return pruned_tree\n\n    def _predict_single(self, x, node):\n        if node.is_leaf:\n            return node.prediction\n        if x  node.split_threshold:\n            return self._predict_single(x, node.left)\n        return self._predict_single(x, node.right)\n\n    def predict(self, X):\n        if not self.root:\n            raise RuntimeError(\"Tree is not fitted yet.\")\n        return np.array([self._predict_single(x, self.root) for x in X])\n\ndef generate_case_A(N, seed):\n    rng = np.random.default_rng(seed)\n    n_class = N // 2\n    X0 = rng.normal(loc=-1.0, scale=0.3, size=n_class)\n    X1 = rng.normal(loc=1.0, scale=0.3, size=n_class)\n    X = np.concatenate([X0, X1])\n    y = np.concatenate([np.zeros(n_class, dtype=int), np.ones(n_class, dtype=int)])\n    return X, y\n\ndef generate_case_B(N, seed):\n    rng = np.random.default_rng(seed)\n    X = rng.uniform(-3, 3, size=N)\n    y = np.zeros(N, dtype=int)\n    y[np.where((X >= -3)  (X = -2))] = 1\n    y[np.where((X >= -1)  (X = 0))] = 1\n    y[np.where((X >= 1)  (X = 2))] = 1\n    flip_mask = rng.random(size=N)  0.05\n    y[flip_mask] = 1 - y[flip_mask]\n    return X, y\n\ndef generate_case_C(N, seed):\n    rng = np.random.default_rng(seed)\n    X = rng.normal(0, 1, size=N)\n    y = (X > 0.2).astype(int)\n    return X, y\n\ndef create_stratified_folds(y, K, seed):\n    rng = np.random.default_rng(seed)\n    indices = np.arange(len(y))\n    folds = [[] for _ in range(K)]\n    \n    for class_label in np.unique(y):\n        class_indices = indices[y == class_label]\n        rng.shuffle(class_indices)\n        \n        n_class = len(class_indices)\n        base_size, remainder = divmod(n_class, K)\n        \n        ptr = 0\n        for i in range(K):\n            fold_size = base_size + (1 if i  remainder else 0)\n            folds[i].extend(class_indices[ptr : ptr + fold_size])\n            ptr += fold_size\n            \n    return [np.array(f) for f in folds]\n\ndef calculate_curvature(x_points, y_points):\n    x1, x2, x3 = x_points\n    y1, y2, y3 = y_points\n    \n    # Avoid division by zero if points are not unique\n    if (x1 == x2) or (x1 == x3) or (x2 == x3):\n      return 0.0\n\n    term1 = y1 / ((x1 - x2) * (x1 - x3))\n    term2 = y2 / ((x2 - x1) * (x2 - x3))\n    term3 = y3 / ((x3 - x1) * (x3 - x2))    \n    return 2 * (term1 + term2 + term3)\n\ndef run_case(data_generator, N, seed, K=5):\n    X, y = data_generator(N, seed)\n    folds = create_stratified_folds(y, K, seed)\n    all_indices = np.arange(N)\n    \n    all_alphas = {0.0}\n    fold_trees = []\n\n    for k in range(K):\n        val_indices = folds[k]\n        train_indices = np.setdiff1d(all_indices, val_indices, assume_unique=True)\n        X_train, y_train = X[train_indices], y[train_indices]\n\n        tree = DecisionTreeClassifier()\n        tree.fit(X_train, y_train)\n        fold_trees.append(tree)\n\n        alphas_k = tree.get_pruning_sequence()\n        all_alphas.update(alphas_k)\n    \n    alpha_grid = np.array(sorted(list(all_alphas)))\n    if alpha_grid.size == 0 or alpha_grid[-1] == 0:\n        alpha_grid = np.append(alpha_grid, 1.0)\n    else:\n        alpha_grid = np.append(alpha_grid, alpha_grid[-1] + 1.0)\n\n    validation_errors = np.zeros((len(alpha_grid), K))\n    for k in range(K):\n        tree = fold_trees[k]\n        val_indices = folds[k]\n        X_val, y_val = X[val_indices], y[val_indices]\n\n        for i, alpha in enumerate(alpha_grid):\n            pruned_tree = tree.get_pruned_tree(alpha)\n            if pruned_tree.root:\n                y_pred = pruned_tree.predict(X_val)\n                error = np.mean(y_pred != y_val)\n            else:\n                error = np.mean(tree.root.prediction != y_val) if tree.root else 1.0\n            validation_errors[i, k] = error\n\n    mean_errors = np.mean(validation_errors, axis=1)\n    std_errors = np.std(validation_errors, axis=1, ddof=1) / np.sqrt(K)\n\n    idx_min = np.argmin(mean_errors)\n    alpha_min = alpha_grid[idx_min]\n    target_error = mean_errors[idx_min] + std_errors[idx_min]\n    \n    eligible_indices = np.where(mean_errors = target_error + 1e-9)[0] # Add tolerance for float comparison\n    idx_1se = eligible_indices[-1] if eligible_indices.size > 0 else idx_min\n    alpha_1se = alpha_grid[idx_1se]\n\n    cond1 = alpha_1se > alpha_min\n    cond2 = (mean_errors[idx_1se] - mean_errors[idx_min]) > 0.05\n    \n    if not (cond1 and cond2):\n        return False\n\n    min_pos_alpha = min((a for a in alpha_grid if a > 0), default=None)\n    delta = 0.1 * min_pos_alpha if min_pos_alpha is not None else 1e-6\n    x_grid = np.log(alpha_grid + delta)\n    \n    curvatures = []\n    for i in range(1, len(x_grid) - 1):\n        curv = calculate_curvature(x_grid[i-1:i+2], mean_errors[i-1:i+2])\n        curvatures.append(curv)\n    \n    if not curvatures:\n        return False\n    \n    baseline_curvature = np.median(np.abs(np.array(curvatures)))\n\n    if 1 = idx_1se  len(x_grid) - 1:\n        curvature_1se = calculate_curvature(x_grid[idx_1se-1:idx_1se+2], mean_errors[idx_1se-1:idx_1se+2])\n    elif idx_1se == len(x_grid) - 1 and len(x_grid) >= 3:\n        curvature_1se = calculate_curvature(x_grid[-3:], mean_errors[-3:])\n    else:\n        curvature_1se = 0.0\n    \n    cond3 = curvature_1se > max(0.3, 3 * baseline_curvature)\n    \n    return cond1 and cond2 and cond3\n\ndef solve():\n    \"\"\"Main function to run the experiment for all cases.\"\"\"\n    test_cases = [\n        {'name': 'A', 'generator': generate_case_A, 'N': 120, 'seed': 13},\n        {'name': 'B', 'generator': generate_case_B, 'N': 180, 'seed': 7},\n        {'name': 'C', 'generator': generate_case_C, 'N': 40, 'seed': 29},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_case(case['generator'], case['N'], case['seed'])\n        results.append(result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3189411"}]}