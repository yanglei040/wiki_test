{"hands_on_practices": [{"introduction": "We begin by investigating a fundamental question: under what conditions do the Ordinary Least Squares (OLS) and Generalized Least Squares (GLS) estimators produce the exact same result? This practice provides a hands-on way to test the theoretical conditions for their equivalence, such as when the error covariance is spherical or when the column space of the design matrix is an invariant subspace of the covariance matrix. By computing the estimators for several carefully constructed scenarios [@problem_id:3112106], you will gain a deeper intuition for the relationship between OLS and GLS.", "problem": "You are given a linear model with possibly correlated error terms: $\\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$, where $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$ has full column rank, $\\mathbf{y} \\in \\mathbb{R}^{n}$, and the error vector $\\boldsymbol{\\varepsilon}$ has mean $\\mathbf{0}$ and covariance matrix $\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{n \\times n}$ that is symmetric positive definite. The Ordinary Least Squares (OLS) estimator is defined as the minimizer of the unweighted squared loss, and the Generalized Least Squares (GLS) estimator is defined as the minimizer of the squared loss weighted by the inverse covariance. Your task is to implement a program that, for a fixed dataset and a suite of covariance specifications, computes the Euclidean norm of the difference between the OLS and GLS estimators to assess when they coincide exactly and how sensitive GLS is to small deviations from exact-equality conditions.\n\nFundamental base for derivation and implementation:\n- The OLS estimator is the unique minimizer of the function $\\boldsymbol{\\beta} \\mapsto \\|\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta}\\|_{2}^{2}$.\n- The GLS estimator is the unique minimizer of the function $\\boldsymbol{\\beta} \\mapsto (\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta})^{\\top} \\boldsymbol{\\Sigma}^{-1} (\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta})$.\n\nDataset (fixed across all test cases):\n- Sample size and number of features: $n = 6$, $p = 2$.\n- Design matrix:\n$$\n\\mathbf{X} = \\begin{bmatrix}\n1  0 \\\\\n1  1 \\\\\n1  2 \\\\\n1  3 \\\\\n1  4 \\\\\n1  5\n\\end{bmatrix}.\n$$\n- Response vector:\n$$\n\\mathbf{y} = \\begin{bmatrix}\n1 \\\\\n2 \\\\\n2 \\\\\n4 \\\\\n3 \\\\\n5\n\\end{bmatrix}.\n$$\n\nTest suite of covariance matrices $\\boldsymbol{\\Sigma}$:\n1. Exact-equality, spherical errors: $\\boldsymbol{\\Sigma}_{1} = \\sigma^{2} \\mathbf{I}$ with $\\sigma^{2} = 2.0$ and $\\mathbf{I}$ the $n \\times n$ identity matrix.\n2. Exact-equality, general condition: $\\boldsymbol{\\Sigma}_{2} = \\sigma^{2} \\mathbf{I} + \\mathbf{X} \\mathbf{\\Lambda} \\mathbf{X}^{\\top}$ with $\\sigma^{2} = 1.5$ and\n$$\n\\mathbf{\\Lambda} = \\begin{bmatrix}\n0.7  0.2 \\\\\n0.2  0.5\n\\end{bmatrix}.\n$$\n3. Near-equality, small perturbation orthogonal to $\\operatorname{col}(\\mathbf{X})$: $\\boldsymbol{\\Sigma}_{3} = \\boldsymbol{\\Sigma}_{2} + \\epsilon \\, \\mathbf{v} \\mathbf{v}^{\\top}$ with $\\epsilon = 10^{-6}$ and\n$$\n\\mathbf{v} = \\begin{bmatrix}\n1 \\\\\n-2 \\\\\n1 \\\\\n0 \\\\\n0 \\\\\n0\n\\end{bmatrix},\n$$\nwhere $\\mathbf{v}$ is orthogonal to each column of $\\mathbf{X}$.\n4. Weak autoregressive correlation: $\\boldsymbol{\\Sigma}_{4}$ is the autoregressive of order $1$ (AR(1)) covariance with parameter $\\rho = 0.1$ and variance $\\sigma^{2} = 1.0$, i.e., entries $(\\boldsymbol{\\Sigma}_{4})_{ij} = \\sigma^{2} \\rho^{|i - j|}$ for all indices $i,j$.\n5. Strong autoregressive correlation: $\\boldsymbol{\\Sigma}_{5}$ is AR(1) with parameter $\\rho = 0.8$ and variance $\\sigma^{2} = 1.0$, i.e., entries $(\\boldsymbol{\\Sigma}_{5})_{ij} = \\sigma^{2} \\rho^{|i - j|}$ for all indices $i,j$.\n\nYour program must:\n- Compute the OLS estimator $\\hat{\\boldsymbol{\\beta}}_{\\mathrm{OLS}}$ as the unique minimizer of the unweighted squared loss.\n- For each $\\boldsymbol{\\Sigma}$ in the test suite, compute the GLS estimator $\\hat{\\boldsymbol{\\beta}}_{\\mathrm{GLS}}(\\boldsymbol{\\Sigma})$ as the unique minimizer of the $\\boldsymbol{\\Sigma}^{-1}$-weighted squared loss.\n- For each test case, compute the Euclidean norm of the difference $\\|\\hat{\\boldsymbol{\\beta}}_{\\mathrm{OLS}} - \\hat{\\boldsymbol{\\beta}}_{\\mathrm{GLS}}(\\boldsymbol{\\Sigma})\\|_{2}$.\n- Produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each value rounded to $8$ decimal places, in the order $\\boldsymbol{\\Sigma}_{1}, \\boldsymbol{\\Sigma}_{2}, \\boldsymbol{\\Sigma}_{3}, \\boldsymbol{\\Sigma}_{4}, \\boldsymbol{\\Sigma}_{5}$.\n\nDesign for coverage:\n- The case $\\boldsymbol{\\Sigma}_{1}$ tests the baseline condition where OLS and GLS coincide exactly.\n- The case $\\boldsymbol{\\Sigma}_{2}$ tests a non-trivial exact-equality condition.\n- The case $\\boldsymbol{\\Sigma}_{3}$ tests sensitivity to a tiny perturbation violating exact-equality conditions in a direction orthogonal to the columns of $\\mathbf{X}$.\n- The cases $\\boldsymbol{\\Sigma}_{4}$ and $\\boldsymbol{\\Sigma}_{5}$ test increasing deviations from equality under structured correlation (weak and strong AR(1)).\n\nFinal output format:\n- Your program should produce a single line of output containing the five results as a comma-separated list enclosed in square brackets (e.g., $[r_{1},r_{2},r_{3},r_{4},r_{5}]$), where each $r_{k}$ is a float rounded to $8$ decimal places. No other text must be printed.", "solution": "The user wants to compare the Ordinary Least Squares (OLS) and Generalized Least Squares (GLS) estimators for a linear model by computing the Euclidean norm of their difference under various specifications of the error covariance matrix.\n\n### Step 1: Theoretical Formulation\n\nThe linear model is given by $\\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$, where $\\mathbf{y} \\in \\mathbb{R}^{n}$ is the response vector, $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$ is the design matrix of full column rank, $\\boldsymbol{\\beta} \\in \\mathbb{R}^{p}$ is the vector of coefficients, and $\\boldsymbol{\\varepsilon} \\in \\mathbb{R}^{n}$ is the error vector with mean $\\mathbb{E}[\\boldsymbol{\\varepsilon}] = \\mathbf{0}$ and a symmetric positive definite covariance matrix $\\operatorname{Cov}(\\boldsymbol{\\varepsilon}) = \\boldsymbol{\\Sigma}$.\n\n**Ordinary Least Squares (OLS) Estimator**\n\nThe OLS estimator, $\\hat{\\boldsymbol{\\beta}}_{\\mathrm{OLS}}$, is found by minimizing the residual sum of squares (RSS):\n$$\n\\text{RSS}(\\boldsymbol{\\beta}) = (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^{\\top}(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}) = \\| \\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta} \\|_{2}^{2}\n$$\nTo find the minimum, we differentiate with respect to $\\boldsymbol{\\beta}$ and set the gradient to zero:\n$$\n\\frac{\\partial \\text{RSS}}{\\partial \\boldsymbol{\\beta}} = -2\\mathbf{X}^{\\top}(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}) = \\mathbf{0}\n$$\n$$\n\\mathbf{X}^{\\top}\\mathbf{y} - \\mathbf{X}^{\\top}\\mathbf{X}\\boldsymbol{\\beta} = \\mathbf{0} \\implies \\mathbf{X}^{\\top}\\mathbf{X}\\boldsymbol{\\beta} = \\mathbf{X}^{\\top}\\mathbf{y}\n$$\nSince $\\mathbf{X}$ is assumed to have full column rank, the matrix $\\mathbf{X}^{\\top}\\mathbf{X}$ is invertible. The unique OLS estimator is therefore:\n$$\n\\hat{\\boldsymbol{\\beta}}_{\\mathrm{OLS}} = (\\mathbf{X}^{\\top}\\mathbf{X})^{-1}\\mathbf{X}^{\\top}\\mathbf{y}\n$$\n\n**Generalized Least Squares (GLS) Estimator**\n\nThe GLS estimator, $\\hat{\\boldsymbol{\\beta}}_{\\mathrm{GLS}}$, accounts for the error covariance structure by minimizing a weighted residual sum of squares:\n$$\n\\text{RSS}_{\\boldsymbol{\\Sigma}}(\\boldsymbol{\\beta}) = (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^{\\top}\\boldsymbol{\\Sigma}^{-1}(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\n$$\nThis can be viewed as performing OLS on a transformed model. Since $\\boldsymbol{\\Sigma}$ is symmetric positive definite, its inverse $\\boldsymbol{\\Sigma}^{-1}$ is as well. We can find a matrix $\\mathbf{L}$ such that $\\boldsymbol{\\Sigma}^{-1} = \\mathbf{L}^{\\top}\\mathbf{L}$ (e.g., via Cholesky decomposition). The objective function becomes:\n$$\n\\text{RSS}_{\\boldsymbol{\\Sigma}}(\\boldsymbol{\\beta}) = (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^{\\top}\\mathbf{L}^{\\top}\\mathbf{L}(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}) = \\|\\mathbf{L}(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\\|_{2}^{2} = \\|\\mathbf{y}' - \\mathbf{X}'\\boldsymbol{\\beta}\\|_{2}^{2}\n$$\nwhere $\\mathbf{y}' = \\mathbf{L}\\mathbf{y}$ and $\\mathbf{X}' = \\mathbf{L}\\mathbf{X}$. The OLS solution for this transformed problem is:\n$$\n\\hat{\\boldsymbol{\\beta}}_{\\mathrm{GLS}} = ((\\mathbf{X}')^{\\top}\\mathbf{X}')^{-1}(\\mathbf{X}')^{\\top}\\mathbf{y}' = (\\mathbf{X}^{\\top}\\mathbf{L}^{\\top}\\mathbf{L}\\mathbf{X})^{-1}\\mathbf{X}^{\\top}\\mathbf{L}^{\\top}\\mathbf{L}\\mathbf{y}\n$$\nSubstituting back $\\boldsymbol{\\Sigma}^{-1} = \\mathbf{L}^{\\top}\\mathbf{L}$, we get the GLS estimator:\n$$\n\\hat{\\boldsymbol{\\beta}}_{\\mathrm{GLS}}(\\boldsymbol{\\Sigma}) = (\\mathbf{X}^{\\top}\\boldsymbol{\\Sigma}^{-1}\\mathbf{X})^{-1}\\mathbf{X}^{\\top}\\boldsymbol{\\Sigma}^{-1}\\mathbf{y}\n$$\n\n### Step 2: Condition for Equivalence of OLS and GLS\n\nThe OLS and GLS estimators are identical, i.e., $\\hat{\\boldsymbol{\\beta}}_{\\mathrm{OLS}} = \\hat{\\boldsymbol{\\beta}}_{\\mathrm{GLS}}$, if and only if the column space of the design matrix, $\\operatorname{col}(\\mathbf{X})$, is an invariant subspace of the covariance matrix $\\boldsymbol{\\Sigma}$. This means that for any vector $\\mathbf{z} \\in \\operatorname{col}(\\mathbf{X})$, we must have $\\boldsymbol{\\Sigma} \\mathbf{z} \\in \\operatorname{col}(\\mathbf{X})$. A practical way to state this condition is that there must exist a $p \\times p$ matrix $\\mathbf{M}$ such that $\\boldsymbol{\\Sigma} \\mathbf{X} = \\mathbf{X} \\mathbf{M}$.\n\nLet's examine the first three test cases with respect to this condition.\n1.  $\\boldsymbol{\\Sigma}_1 = \\sigma^2 \\mathbf{I}$: Here, $\\boldsymbol{\\Sigma}_1 \\mathbf{X} = (\\sigma^2 \\mathbf{I}) \\mathbf{X} = \\mathbf{X}(\\sigma^2 \\mathbf{I}_p)$. The condition holds with $\\mathbf{M} = \\sigma^2 \\mathbf{I}_p$, so $\\hat{\\boldsymbol{\\beta}}_{\\mathrm{OLS}} = \\hat{\\boldsymbol{\\beta}}_{\\mathrm{GLS}}(\\boldsymbol{\\Sigma}_1)$.\n2.  $\\boldsymbol{\\Sigma}_2 = \\sigma^2 \\mathbf{I} + \\mathbf{X} \\mathbf{\\Lambda} \\mathbf{X}^\\top$: Here, $\\boldsymbol{\\Sigma}_2 \\mathbf{X} = (\\sigma^2 \\mathbf{I} + \\mathbf{X} \\mathbf{\\Lambda} \\mathbf{X}^\\top)\\mathbf{X} = \\sigma^2 \\mathbf{X} + \\mathbf{X}\\mathbf{\\Lambda}(\\mathbf{X}^\\top \\mathbf{X}) = \\mathbf{X}(\\sigma^2 \\mathbf{I}_p + \\mathbf{\\Lambda}(\\mathbf{X}^\\top \\mathbf{X}))$. The condition holds with $\\mathbf{M} = \\sigma^2 \\mathbf{I}_p + \\mathbf{\\Lambda}(\\mathbf{X}^\\top \\mathbf{X})$, so $\\hat{\\boldsymbol{\\beta}}_{\\mathrm{OLS}} = \\hat{\\boldsymbol{\\beta}}_{\\mathrm{GLS}}(\\boldsymbol{\\Sigma}_2)$.\n3.  $\\boldsymbol{\\Sigma}_3 = \\boldsymbol{\\Sigma}_2 + \\epsilon \\mathbf{v} \\mathbf{v}^\\top$: The vector $\\mathbf{v}$ is given to be orthogonal to each column of $\\mathbf{X}$, which implies $\\mathbf{X}^\\top \\mathbf{v} = \\mathbf{0}$, and thus $\\mathbf{v}^\\top \\mathbf{X} = (\\mathbf{X}^\\top \\mathbf{v})^\\top = \\mathbf{0}$.\n    Then, $\\boldsymbol{\\Sigma}_3 \\mathbf{X} = (\\boldsymbol{\\Sigma}_2 + \\epsilon \\mathbf{v} \\mathbf{v}^\\top)\\mathbf{X} = \\boldsymbol{\\Sigma}_2 \\mathbf{X} + \\epsilon \\mathbf{v}(\\mathbf{v}^\\top \\mathbf{X}) = \\boldsymbol{\\Sigma}_2 \\mathbf{X} + \\mathbf{0} = \\boldsymbol{\\Sigma}_2 \\mathbf{X}$.\n    Since the invariance condition holds for $\\boldsymbol{\\Sigma}_2$, it must also hold for $\\boldsymbol{\\Sigma}_3$. Thus, we expect $\\hat{\\boldsymbol{\\beta}}_{\\mathrm{OLS}} = \\hat{\\boldsymbol{\\beta}}_{\\mathrm{GLS}}(\\boldsymbol{\\Sigma}_3)$.\n\nFor cases $4$ and $5$, the autoregressive covariance structure does not generally satisfy this invariance property, so we expect a non-zero difference between the estimators.\n\n### Step 3: Computational Procedure\n\nThe solution is implemented by following these steps:\n1.  Define the given dataset, the design matrix $\\mathbf{X}$ and response vector $\\mathbf{y}$, as numerical arrays. The sample size is $n=6$ and the number of features is $p=2$.\n2.  Compute the OLS estimator $\\hat{\\boldsymbol{\\beta}}_{\\mathrm{OLS}}$ once using its closed-form formula. For numerical stability, the linear system $\\mathbf{X}^{\\top}\\mathbf{X} \\boldsymbol{\\beta} = \\mathbf{X}^{\\top}\\mathbf{y}$ is solved directly rather than by computing $(\\mathbf{X}^{\\top}\\mathbf{X})^{-1}$ explicitly.\n3.  Iterate through the five specified test cases for the covariance matrix $\\boldsymbol{\\Sigma}$:\n    a. For each case, construct the corresponding $6 \\times 6$ matrix $\\boldsymbol{\\Sigma}_k$.\n    b. For cases $\\boldsymbol{\\Sigma}_4$ and $\\boldsymbol{\\Sigma}_5$, which have an AR($1$) structure, the covariance matrix is a symmetric Toeplitz matrix where $(\\boldsymbol{\\Sigma})_{ij} = \\sigma^2 \\rho^{|i-j|}$.\n    c. Compute the GLS estimator $\\hat{\\boldsymbol{\\beta}}_{\\mathrm{GLS}}(\\boldsymbol{\\Sigma}_k)$ using its formula. Again, for stability, the linear system $(\\mathbf{X}^{\\top}\\boldsymbol{\\Sigma}_k^{-1}\\mathbf{X})\\boldsymbol{\\beta} = \\mathbf{X}^{\\top}\\boldsymbol{\\Sigma}_k^{-1}\\mathbf{y}$ is solved directly.\n4.  For each case, calculate the Euclidean norm of the difference vector, $\\|\\hat{\\boldsymbol{\\beta}}_{\\mathrm{OLS}} - \\hat{\\boldsymbol{\\beta}}_{\\mathrm{GLS}}(\\boldsymbol{\\Sigma}_k)\\|_{2}$.\n5.  Collect the five norms and format them into a single string as specified.\n\nThe theoretical analysis predicts the first three norms will be zero (or close to it, subject to floating-point precision limitations), while the last two will be non-zero, with the norm for $\\boldsymbol{\\Sigma}_5$ ($\\rho=0.8$) expected to be larger than that for $\\boldsymbol{\\Sigma}_4$ ($\\rho=0.1$).", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import toeplitz\n\ndef solve():\n    \"\"\"\n    Computes the Euclidean norm of the difference between OLS and GLS estimators\n    for a given dataset and five different error covariance structures.\n    \"\"\"\n    # Fixed dataset\n    n, p = 6, 2\n    X = np.array([\n        [1.0, 0.0],\n        [1.0, 1.0],\n        [1.0, 2.0],\n        [1.0, 3.0],\n        [1.0, 4.0],\n        [1.0, 5.0]\n    ])\n    y = np.array([1.0, 2.0, 2.0, 4.0, 3.0, 5.0])\n\n    # Compute OLS estimator once\n    # Solve (X^T X) beta = X^T y\n    beta_ols = np.linalg.solve(X.T @ X, X.T @ y)\n\n    # --- Define the five covariance matrices ---\n\n    # Case 1: Spherical errors\n    sigma2_1 = 2.0\n    Sigma1 = sigma2_1 * np.identity(n)\n\n    # Case 2: General exact-equality condition\n    sigma2_2 = 1.5\n    Lambda = np.array([[0.7, 0.2], [0.2, 0.5]])\n    Sigma2 = sigma2_2 * np.identity(n) + X @ Lambda @ X.T\n\n    # Case 3: Near-equality, perturbation orthogonal to col(X)\n    epsilon = 1e-6\n    v = np.array([1.0, -2.0, 1.0, 0.0, 0.0, 0.0])\n    Sigma3 = Sigma2 + epsilon * np.outer(v, v)\n\n    # Case 4: Weak autoregressive correlation (AR(1))\n    rho4 = 0.1\n    sigma2_4 = 1.0\n    c4 = sigma2_4 * (rho4 ** np.arange(n))\n    Sigma4 = toeplitz(c4)\n\n    # Case 5: Strong autoregressive correlation (AR(1))\n    rho5 = 0.8\n    sigma2_5 = 1.0\n    c5 = sigma2_5 * (rho5 ** np.arange(n))\n    Sigma5 = toeplitz(c5)\n\n    # List of covariance matrices for iteration\n    sigmas = [Sigma1, Sigma2, Sigma3, Sigma4, Sigma5]\n    \n    results = []\n\n    # Iterate through each covariance matrix to compute GLS and the difference norm\n    for Sigma in sigmas:\n        # Compute GLS estimator\n        # Solve (X^T Sigma^-1 X) beta = X^T Sigma^-1 y\n        Sigma_inv = np.linalg.inv(Sigma)\n        XT_S_inv = X.T @ Sigma_inv\n        \n        A_gls = XT_S_inv @ X\n        b_gls = XT_S_inv @ y\n        \n        beta_gls = np.linalg.solve(A_gls, b_gls)\n        \n        # Compute the Euclidean norm of the difference\n        diff_norm = np.linalg.norm(beta_ols - beta_gls)\n        results.append(diff_norm)\n\n    # Format the final output string as per requirements\n    output_str = f\"[{','.join([f'{r:.8f}' for r in results])}]\"\n    print(output_str)\n\nsolve()\n\n```", "id": "3112106"}, {"introduction": "Knowing that GLS is generally more efficient than OLS when errors are correlated, we now seek to quantify this advantage. This practice challenges you to compare the two estimators not by their formulas alone, but by their statistical performance. You will explore how the gains from using GLS can differ for coefficient estimation (inference) versus predicting new outcomes (prediction), a crucial distinction in applied modeling [@problem_id:3112062].", "problem": "Consider the linear model with correlated errors. Let $n$ be the number of observations and $p$ be the number of predictors. The model is $\\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$, where $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$ is a fixed design matrix with full column rank, $\\boldsymbol{\\beta} \\in \\mathbb{R}^{p}$ is the true coefficient vector, and the error vector $\\boldsymbol{\\varepsilon} \\in \\mathbb{R}^{n}$ satisfies $E[\\boldsymbol{\\varepsilon}]=\\mathbf{0}$ and $\\mathrm{Cov}(\\boldsymbol{\\varepsilon})=\\sigma^2 \\boldsymbol{\\Sigma}$, with $\\sigma^2 \\in \\mathbb{R}_{+}$ and $\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{n \\times n}$ positive definite. Assume $\\sigma^2=1$ for all computations in this problem.\n\nDefine Ordinary Least Squares (OLS) and Generalized Least Squares (GLS) estimators as follows:\n- Ordinary Least Squares (OLS) estimator $\\hat{\\boldsymbol{\\beta}}_{\\mathrm{OLS}}$ is the solution to minimizing the sum of squared residuals under the Euclidean norm.\n- Generalized Least Squares (GLS) estimator $\\hat{\\boldsymbol{\\beta}}_{\\mathrm{GLS}}$ is the solution to minimizing the sum of squared residuals under the norm induced by $\\boldsymbol{\\Sigma}^{-1}$.\n\nStarting from the fundamental definitions of linear estimators, the unbiasedness of least squares under $E[\\boldsymbol{\\varepsilon}]=\\mathbf{0}$, and the covariance propagation for linear transformations, derive expressions for the coefficient covariance matrices $\\mathrm{Var}(\\hat{\\boldsymbol{\\beta}}_{\\mathrm{OLS}})$ and $\\mathrm{Var}(\\hat{\\boldsymbol{\\beta}}_{\\mathrm{GLS}})$, and the predictive covariance matrices for mean-response predictions at a given test design $\\mathbf{X}_{\\mathrm{test}}$, namely $\\mathrm{Var}(\\mathbf{X}_{\\mathrm{test}}\\hat{\\boldsymbol{\\beta}}_{\\mathrm{OLS}})$ and $\\mathrm{Var}(\\mathbf{X}_{\\mathrm{test}}\\hat{\\boldsymbol{\\beta}}_{\\mathrm{GLS}})$.\n\nFor each estimator and test design, define two mean squared error (MSE) metrics:\n- Coefficient MSE: $M_{\\boldsymbol{\\beta}} = \\frac{1}{p} \\mathrm{trace}(\\mathrm{Var}(\\hat{\\boldsymbol{\\beta}}))$.\n- Prediction MSE (for the mean response): $M_{\\mathrm{pred}} = \\frac{1}{n_{\\mathrm{test}}} \\mathrm{trace}(\\mathrm{Var}(\\mathbf{X}_{\\mathrm{test}} \\hat{\\boldsymbol{\\beta}}))$, where $n_{\\mathrm{test}}$ is the number of rows of $\\mathbf{X}_{\\mathrm{test}}$.\n\nYour task is to implement a program that, for specified test cases, computes the ratios\n- $R_{\\mathrm{inf}} = \\frac{M_{\\boldsymbol{\\beta},\\mathrm{OLS}}}{M_{\\boldsymbol{\\beta},\\mathrm{GLS}}}$,\n- $R_{\\mathrm{pred}} = \\frac{M_{\\mathrm{pred},\\mathrm{OLS}}}{M_{\\mathrm{pred},\\mathrm{GLS}}}$,\n\nand reports them to compare regimes where Generalized Least Squares (GLS) mainly improves inference (coefficients) versus prediction (mean response). Larger $R_{\\mathrm{inf}}$ indicates greater improvement in coefficient inference by GLS relative to OLS, while larger $R_{\\mathrm{pred}}$ indicates greater improvement in mean-response prediction by GLS relative to OLS.\n\nFor scientific realism, use an autoregressive correlation of order one for $\\boldsymbol{\\Sigma}$, with parameter $\\rho \\in [0,1)$, i.e., $\\boldsymbol{\\Sigma}_{ij} = \\rho^{|i-j|}$, and deterministic design matrices constructed from simple functions of a normalized time index.\n\nImplement the following test suite. For each case, construct $\\mathbf{t} = \\left(0, \\frac{1}{n-1}, \\ldots, \\frac{n-1}{n-1}\\right)$ and the specified $\\mathbf{X}$ and $\\mathbf{X}_{\\mathrm{test}}$. All computations use $\\sigma^2 = 1$.\n\n- Case 1 (boundary independence, two predictors, identical test design):\n    - $n=40$, $p=2$, $\\rho=0.0$,\n    - $\\mathbf{X} = [\\mathbf{1},\\ \\mathbf{t}]$,\n    - $\\mathbf{X}_{\\mathrm{test}} = \\mathbf{X}$.\n- Case 2 (strong correlation, two predictors, identical test design):\n    - $n=40$, $p=2$, $\\rho=0.8$,\n    - $\\mathbf{X} = [\\mathbf{1},\\ \\mathbf{t}]$,\n    - $\\mathbf{X}_{\\mathrm{test}} = \\mathbf{X}$.\n- Case 3 (moderate correlation, three predictors, test design drops high-frequency feature):\n    - $n=60$, $p=3$, $\\rho=0.6$,\n    - $\\mathbf{X} = [\\mathbf{1},\\ \\mathbf{t},\\ \\sin(10\\pi \\mathbf{t})]$,\n    - $\\mathbf{X}_{\\mathrm{test}} = [\\mathbf{1},\\ \\mathbf{t},\\ \\mathbf{0}]$ (same number of columns, third column zero).\n- Case 4 (very strong correlation, three predictors, test design emphasizes third feature):\n    - $n=60$, $p=3$, $\\rho=0.95$,\n    - $\\mathbf{X} = [\\mathbf{1},\\ \\mathbf{t},\\ \\cos(14\\pi \\mathbf{t})]$,\n    - $\\mathbf{X}_{\\mathrm{test}} = [\\mathbf{1},\\ \\mathbf{t},\\ 3\\cos(14\\pi \\mathbf{t})]$ (third column scaled by $3$).\n\nYour program must:\n- Compute $R_{\\mathrm{inf}}$ and $R_{\\mathrm{pred}}$ for each case using the derived covariance expressions.\n- Produce a single line of output containing the results as a comma-separated list of pairs, where each pair is formatted as $[R_{\\mathrm{inf}},R_{\\mathrm{pred}}]$ with each value rounded to six decimal places, and the overall output is enclosed in square brackets, e.g., $[[r_{11},r_{12}],[r_{21},r_{22}],\\ldots]$.\n\nNo physical units, angle units, or percentages are involved in this problem. The final output must be purely numeric in the specified format. The design ensures coverage including independence, strong correlation, feature alignment/misalignment between training and test designs, and emphasized directions to reveal regimes where Generalized Least Squares (GLS) mainly improves inference versus prediction.", "solution": "The problem requires the derivation of covariance matrices for Ordinary Least Squares (OLS) and Generalized Least Squares (GLS) estimators and their corresponding mean-response predictions, followed by a computational comparison using specified metrics. The linear model is given by $\\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$, where $\\mathbf{y} \\in \\mathbb{R}^{n}$, $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$, $\\boldsymbol{\\beta} \\in \\mathbb{R}^{p}$, and $\\boldsymbol{\\varepsilon} \\in \\mathbb{R}^{n}$. The error term $\\boldsymbol{\\varepsilon}$ has mean $E[\\boldsymbol{\\varepsilon}]=\\mathbf{0}$ and covariance $\\mathrm{Cov}(\\boldsymbol{\\varepsilon})=\\sigma^2 \\boldsymbol{\\Sigma}$. For this problem, we assume $\\sigma^2=1$.\n\nFirst, we derive the necessary theoretical expressions for the estimators and their covariances.\n\n**1. Ordinary Least Squares (OLS) Estimator**\n\nThe OLS estimator, $\\hat{\\boldsymbol{\\beta}}_{\\mathrm{OLS}}$, minimizes the sum of squared residuals, $S(\\boldsymbol{\\beta}) = (\\boldsymbol{\\varepsilon})^T\\boldsymbol{\\varepsilon} = (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})$. To find the minimum, we set the gradient with respect to $\\boldsymbol{\\beta}$ to zero:\n$$\n\\frac{\\partial S}{\\partial \\boldsymbol{\\beta}} = -2\\mathbf{X}^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}) = \\mathbf{0}\n$$\nSolving for $\\boldsymbol{\\beta}$, given that $\\mathbf{X}$ has full column rank and thus $\\mathbf{X}^T \\mathbf{X}$ is invertible, yields the OLS estimator:\n$$\n\\hat{\\boldsymbol{\\beta}}_{\\mathrm{OLS}} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}\n$$\nThe estimator is a linear function of $\\mathbf{y}$. Its expected value is:\n$$\nE[\\hat{\\boldsymbol{\\beta}}_{\\mathrm{OLS}}] = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T E[\\mathbf{y}] = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T E[\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}] = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T (\\mathbf{X}\\boldsymbol{\\beta}) = \\boldsymbol{\\beta}\n$$\nThus, $\\hat{\\boldsymbol{\\beta}}_{\\mathrm{OLS}}$ is an unbiased estimator of $\\boldsymbol{\\beta}$. The covariance matrix of $\\hat{\\boldsymbol{\\beta}}_{\\mathrm{OLS}}$ is found using the formula for covariance of a linear transformation, $\\mathrm{Var}(\\mathbf{A}\\mathbf{y}) = \\mathbf{A} \\mathrm{Var}(\\mathbf{y}) \\mathbf{A}^T$.\n$$\n\\mathrm{Var}(\\hat{\\boldsymbol{\\beta}}_{\\mathrm{OLS}}) = \\mathrm{Var}((\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}) = \\left((\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T\\right) \\mathrm{Var}(\\mathbf{y}) \\left((\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T\\right)^T\n$$\nSince $\\mathrm{Var}(\\mathbf{y}) = \\mathrm{Var}(\\boldsymbol{\\varepsilon}) = \\boldsymbol{\\Sigma}$ (with $\\sigma^2=1$), we get:\n$$\n\\mathrm{Var}(\\hat{\\boldsymbol{\\beta}}_{\\mathrm{OLS}}) = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\Sigma} \\mathbf{X} (\\mathbf{X}^T \\mathbf{X})^{-1}\n$$\n\n**2. Generalized Least Squares (GLS) Estimator**\n\nThe GLS estimator, $\\hat{\\boldsymbol{\\beta}}_{\\mathrm{GLS}}$, minimizes the weighted sum of squared residuals, $S_{\\boldsymbol{\\Sigma}}(\\boldsymbol{\\beta}) = (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})$, where $\\boldsymbol{\\Sigma}$ is positive definite. Setting the gradient to zero:\n$$\n\\frac{\\partial S_{\\boldsymbol{\\Sigma}}}{\\partial \\boldsymbol{\\beta}} = -2\\mathbf{X}^T \\boldsymbol{\\Sigma}^{-1}(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}) = \\mathbf{0}\n$$\nSolving for $\\boldsymbol{\\beta}$, given that $\\mathbf{X}^T \\boldsymbol{\\Sigma}^{-1} \\mathbf{X}$ is invertible, yields the GLS estimator:\n$$\n\\hat{\\boldsymbol{\\beta}}_{\\mathrm{GLS}} = (\\mathbf{X}^T \\boldsymbol{\\Sigma}^{-1} \\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\Sigma}^{-1} \\mathbf{y}\n$$\nThe GLS estimator is also unbiased:\n$$\nE[\\hat{\\boldsymbol{\\beta}}_{\\mathrm{GLS}}] = (\\mathbf{X}^T \\boldsymbol{\\Sigma}^{-1} \\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\Sigma}^{-1} E[\\mathbf{y}] = (\\mathbf{X}^T \\boldsymbol{\\Sigma}^{-1} \\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{X}\\boldsymbol{\\beta}) = \\boldsymbol{\\beta}\n$$\nThe covariance matrix of $\\hat{\\boldsymbol{\\beta}}_{\\mathrm{GLS}}$ is:\n$$\n\\mathrm{Var}(\\hat{\\boldsymbol{\\beta}}_{\\mathrm{GLS}}) = \\left((\\mathbf{X}^T \\boldsymbol{\\Sigma}^{-1} \\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\Sigma}^{-1}\\right) \\mathrm{Var}(\\mathbf{y}) \\left((\\mathbf{X}^T \\boldsymbol{\\Sigma}^{-1} \\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\Sigma}^{-1}\\right)^T\n$$\nSubstituting $\\mathrm{Var}(\\mathbf{y}) = \\boldsymbol{\\Sigma}$:\n$$\n\\mathrm{Var}(\\hat{\\boldsymbol{\\beta}}_{\\mathrm{GLS}}) = (\\mathbf{X}^T \\boldsymbol{\\Sigma}^{-1} \\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\Sigma} (\\boldsymbol{\\Sigma}^{-1} \\mathbf{X} (\\mathbf{X}^T \\boldsymbol{\\Sigma}^{-1} \\mathbf{X})^{-1}) = (\\mathbf{X}^T \\boldsymbol{\\Sigma}^{-1} \\mathbf{X})^{-1} (\\mathbf{X}^T \\boldsymbol{\\Sigma}^{-1} \\mathbf{X}) (\\mathbf{X}^T \\boldsymbol{\\Sigma}^{-1} \\mathbf{X})^{-1}\n$$\nThis simplifies to the well-known result:\n$$\n\\mathrm{Var}(\\hat{\\boldsymbol{\\beta}}_{\\mathrm{GLS}}) = (\\mathbf{X}^T \\boldsymbol{\\Sigma}^{-1} \\mathbf{X})^{-1}\n$$\n\n**3. Predictive Covariance Matrices**\n\nThe predicted mean response at a new design matrix $\\mathbf{X}_{\\mathrm{test}}$ is $\\hat{\\mathbf{y}}_{\\mathrm{test}} = \\mathbf{X}_{\\mathrm{test}} \\hat{\\boldsymbol{\\beta}}$. We need the covariance matrix of this vector, $\\mathrm{Var}(\\hat{\\mathbf{y}}_{\\mathrm{test}})$. Using the linear transformation covariance rule again:\n\nFor OLS, the predictive covariance is:\n$$\n\\mathrm{Var}(\\mathbf{X}_{\\mathrm{test}}\\hat{\\boldsymbol{\\beta}}_{\\mathrm{OLS}}) = \\mathbf{X}_{\\mathrm{test}} \\mathrm{Var}(\\hat{\\boldsymbol{\\beta}}_{\\mathrm{OLS}}) \\mathbf{X}_{\\mathrm{test}}^T = \\mathbf{X}_{\\mathrm{test}} \\left( (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\boldsymbol{\\Sigma} \\mathbf{X} (\\mathbf{X}^T \\mathbf{X})^{-1} \\right) \\mathbf{X}_{\\mathrm{test}}^T\n$$\n\nFor GLS, the predictive covariance is:\n$$\n\\mathrm{Var}(\\mathbf{X}_{\\mathrm{test}}\\hat{\\boldsymbol{\\beta}}_{\\mathrm{GLS}}) = \\mathbf{X}_{\\mathrm{test}} \\mathrm{Var}(\\hat{\\boldsymbol{\\beta}}_{\\mathrm{GLS}}) \\mathbf{X}_{\\mathrm{test}}^T = \\mathbf{X}_{\\mathrm{test}} \\left( (\\mathbf{X}^T \\boldsymbol{\\Sigma}^{-1} \\mathbf{X})^{-1} \\right) \\mathbf{X}_{\\mathrm{test}}^T\n$$\n\n**4. Metrics for Comparison**\n\nWith the covariance matrices derived, we can define the Mean Squared Error (MSE) metrics and the comparison ratios.\n\nThe Coefficient MSE, $M_{\\boldsymbol{\\beta}}$, is the average variance of the coefficient estimates:\n$$\nM_{\\boldsymbol{\\beta}} = \\frac{1}{p} \\mathrm{trace}(\\mathrm{Var}(\\hat{\\boldsymbol{\\beta}}))\n$$\nThe Prediction MSE, $M_{\\mathrm{pred}}$, is the average variance of the predicted mean responses, where $n_{\\mathrm{test}}$ is the number of rows in $\\mathbf{X}_{\\mathrm{test}}$:\n$$\nM_{\\mathrm{pred}} = \\frac{1}{n_{\\mathrm{test}}} \\mathrm{trace}(\\mathrm{Var}(\\mathbf{X}_{\\mathrm{test}} \\hat{\\boldsymbol{\\beta}}))\n$$\n\nThe ratios to be computed are:\n- Inference Ratio: $R_{\\mathrm{inf}} = \\frac{M_{\\boldsymbol{\\beta},\\mathrm{OLS}}}{M_{\\boldsymbol{\\beta},\\mathrm{GLS}}} = \\frac{\\mathrm{trace}(\\mathrm{Var}(\\hat{\\boldsymbol{\\beta}}_{\\mathrm{OLS}}))}{\\mathrm{trace}(\\mathrm{Var}(\\hat{\\boldsymbol{\\beta}}_{\\mathrm{GLS}}))}$\n- Prediction Ratio: $R_{\\mathrm{pred}} = \\frac{M_{\\mathrm{pred},\\mathrm{OLS}}}{M_{\\mathrm{pred},\\mathrm{GLS}}} = \\frac{\\mathrm{trace}(\\mathrm{Var}(\\mathbf{X}_{\\mathrm{test}}\\hat{\\boldsymbol{\\beta}}_{\\mathrm{OLS}}))}{\\mathrm{trace}(\\mathrm{Var}(\\mathbf{X}_{\\mathrm{test}}\\hat{\\boldsymbol{\\beta}}_{\\mathrm{GLS}}))}$\n\n**Computational Procedure**\n\nFor each test case, the following steps are performed:\n1.  Define the parameters $n$, $p$, and $\\rho$.\n2.  Construct the normalized time vector $\\mathbf{t} = (0, \\frac{1}{n-1}, \\ldots, 1)$.\n3.  Construct the design matrices $\\mathbf{X}$ and $\\mathbf{X}_{\\mathrm{test}}$ as specified. Let $n_{\\mathrm{test}}$ be the number of rows in $\\mathbf{X}_{\\mathrm{test}}$.\n4.  Construct the $n \\times n$ AR(1) error covariance matrix $\\boldsymbol{\\Sigma}$ where $\\boldsymbol{\\Sigma}_{ij} = \\rho^{|i-j|}$.\n5.  Compute the inverse of $\\boldsymbol{\\Sigma}$, which is $\\boldsymbol{\\Sigma}^{-1}$.\n6.  Calculate the four key covariance matrices using the derived formulas: $\\mathrm{Var}(\\hat{\\boldsymbol{\\beta}}_{\\mathrm{OLS}})$, $\\mathrm{Var}(\\hat{\\boldsymbol{\\beta}}_{\\mathrm{GLS}})$, $\\mathrm{Var}(\\mathbf{X}_{\\mathrm{test}}\\hat{\\boldsymbol{\\beta}}_{\\mathrm{OLS}})$, and $\\mathrm{Var}(\\mathbf{X}_{\\mathrm{test}}\\hat{\\boldsymbol{\\beta}}_{\\mathrm{GLS}})$.\n7.  Compute the traces of these matrices.\n8.  Calculate the ratios $R_{\\mathrm{inf}}$ and $R_{\\mathrm{pred}}$.\n\nThis procedure is implemented for each of the four specified test cases. For Case $1$ with $\\rho=0$, $\\boldsymbol{\\Sigma}$ becomes the identity matrix $\\mathbf{I}$. In this scenario, $\\mathrm{Var}(\\hat{\\boldsymbol{\\beta}}_{\\mathrm{OLS}}) = (\\mathbf{X}^T\\mathbf{X})^{-1}$ and $\\mathrm{Var}(\\hat{\\boldsymbol{\\beta}}_{\\mathrm{GLS}}) = (\\mathbf{X}^T \\mathbf{I}^{-1} \\mathbf{X})^{-1} = (\\mathbf{X}^T\\mathbf{X})^{-1}$. The estimators and their variances are identical, so $R_{\\mathrm{inf}} = 1$ and $R_{\\mathrm{pred}} = 1$, providing a baseline and a sanity check. For $\\rho  0$, the Gauss-Markov theorem implies GLS is more efficient than any other linear unbiased estimator, including OLS, so we expect $R_{\\mathrm{inf}} \\ge 1$ and $R_{\\mathrm{pred}} \\ge 1$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import toeplitz\n\ndef solve():\n    \"\"\"\n    Computes OLS/GLS performance ratios for specified linear model test cases.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"n\": 40, \"p\": 2, \"rho\": 0.0,\n            \"X_func\": lambda t: np.stack([np.ones_like(t), t], axis=1),\n            \"X_test_func\": lambda t: np.stack([np.ones_like(t), t], axis=1),\n            \"n_test_is_n\": True\n        },\n        {\n            \"n\": 40, \"p\": 2, \"rho\": 0.8,\n            \"X_func\": lambda t: np.stack([np.ones_like(t), t], axis=1),\n            \"X_test_func\": lambda t: np.stack([np.ones_like(t), t], axis=1),\n            \"n_test_is_n\": True\n        },\n        {\n            \"n\": 60, \"p\": 3, \"rho\": 0.6,\n            \"X_func\": lambda t: np.stack([np.ones_like(t), t, np.sin(10 * np.pi * t)], axis=1),\n            \"X_test_func\": lambda t: np.stack([np.ones_like(t), t, np.zeros_like(t)], axis=1),\n            \"n_test_is_n\": True\n        },\n        {\n            \"n\": 60, \"p\": 3, \"rho\": 0.95,\n            \"X_func\": lambda t: np.stack([np.ones_like(t), t, np.cos(14 * np.pi * t)], axis=1),\n            \"X_test_func\": lambda t: np.stack([np.ones_like(t), t, 3 * np.cos(14 * np.pi * t)], axis=1),\n            \"n_test_is_n\": True\n        }\n    ]\n\n    results = []\n\n    for case in test_cases:\n        n = case[\"n\"]\n        p = case[\"p\"]\n        rho = case[\"rho\"]\n        \n        # Construct time vector and design matrices\n        t = np.linspace(0, 1, n)\n        X = case[\"X_func\"](t)\n\n        if case[\"n_test_is_n\"]:\n            X_test = case[\"X_test_func\"](t)\n            n_test = n\n        # This branch can be extended for cases where n_test != n\n        # else:\n        #     t_test = ...\n        #     X_test = case[\"X_test_func\"](t_test)\n        #     n_test = len(t_test)\n\n        # Construct error covariance matrix Sigma\n        # Sigma_ij = rho^|i-j|\n        first_col = rho**np.arange(n)\n        Sigma = toeplitz(first_col)\n\n        # Common matrix products for OLS and GLS\n        XTX = X.T @ X\n        XTX_inv = np.linalg.inv(XTX)\n\n        # OLS calculations\n        cov_beta_ols_term = X.T @ Sigma @ X\n        cov_beta_ols = XTX_inv @ cov_beta_ols_term @ XTX_inv\n        \n        cov_pred_ols = X_test @ cov_beta_ols @ X_test.T\n        \n        m_beta_ols = np.trace(cov_beta_ols) / p\n        m_pred_ols = np.trace(cov_pred_ols) / n_test\n        \n        # GLS calculations\n        # In the special case rho=0, Sigma=I, so OLS is identical to GLS.\n        if rho == 0.0:\n            r_inf = 1.0\n            r_pred = 1.0\n        else:\n            Sigma_inv = np.linalg.inv(Sigma)\n            XT_Sigma_inv_X = X.T @ Sigma_inv @ X\n            \n            cov_beta_gls = np.linalg.inv(XT_Sigma_inv_X)\n            \n            cov_pred_gls = X_test @ cov_beta_gls @ X_test.T\n            \n            m_beta_gls = np.trace(cov_beta_gls) / p\n            m_pred_gls = np.trace(cov_pred_gls) / n_test\n\n            # Ratios\n            r_inf = m_beta_ols / m_beta_gls\n            r_pred = m_pred_ols / m_pred_gls\n            \n        results.append(f\"[{r_inf:.6f},{r_pred:.6f}]\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3112062"}, {"introduction": "In many real-world applications, especially with time-series or spatial data, the number of observations $n$ can be large, making standard GLS computationally infeasible due to the inversion of a dense $n \\times n$ covariance matrix $\\boldsymbol{\\Sigma}$. This final practice moves from theoretical formulas to a practical, scalable algorithm [@problem_id:3112081]. You will learn to implement GLS efficiently by working with the inverse covariance (precision) matrix $\\mathbf{Q} = \\boldsymbol{\\Sigma}^{-1}$ and exploiting its sparse structure, a common feature in models of structured dependence.", "problem": "You are given a linear model with correlated errors defined on a path network of nodes. The outcome vector is denoted by $\\mathbf{y} \\in \\mathbb{R}^n$, the design matrix is denoted by $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$, and the error vector is denoted by $\\boldsymbol{\\varepsilon} \\in \\mathbb{R}^n$. The model is $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$, with $E[\\boldsymbol{\\varepsilon}] = \\mathbf{0}$ and $\\operatorname{Cov}(\\boldsymbol{\\varepsilon}) = \\boldsymbol{\\Sigma}$. Errors are correlated due to shared links on a path graph; this correlation is encoded via a sparse precision (inverse covariance) matrix $\\mathbf{Q} = \\boldsymbol{\\Sigma}^{-1}$.\n\nStart from the following fundamental base:\n- The linear model definition $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$ with zero-mean errors.\n- The assumption that $\\boldsymbol{\\varepsilon}$ follows a multivariate normal distribution with covariance $\\boldsymbol{\\Sigma}$, so that the log-likelihood (up to an additive constant) is proportional to the negative of the quadratic form $(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^\\top \\boldsymbol{\\Sigma}^{-1}(\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\beta})$.\n- Sparse linear algebra principles for banded matrices and Cholesky factorization for symmetric positive definite matrices.\n\nNetwork-induced error correlation model. Consider a path graph of $n$ nodes, where the precision matrix $\\mathbf{Q}$ is defined as\n$$\n\\mathbf{Q} = \\alpha \\mathbf{I}_n + w \\mathbf{L}_{\\text{path}},\n$$\nwhere $\\alpha > 0$, $w \\ge 0$, $\\mathbf{I}_n$ is the $n \\times n$ identity matrix, and $\\mathbf{L}_{\\text{path}}$ is the graph Laplacian of a path: $\\mathbf{L}_{\\text{path}}$ has diagonal entries equal to $1$ at the two endpoints and $2$ at interior nodes, and sub- and super-diagonal entries equal to $-1$ for adjacent nodes, with all other entries equal to $0$. This makes $\\mathbf{Q}$ symmetric positive definite and tridiagonal. Your task is to use this structure to implement Generalized Least Squares (GLS) efficiently by exploiting sparsity via a sparse Cholesky routine tailored to tridiagonal matrices.\n\nRequirements:\n- Derive, from the stated base only (do not use any shortcut estimator expressions provided to you), the computational steps to obtain the estimator for $\\boldsymbol{\\beta}$ that minimizes the appropriate quadratic form induced by the precision matrix $\\mathbf{Q}$.\n- Implement a banded (tridiagonal) sparse Cholesky factorization for $\\mathbf{Q}$. That is, compute a lower bidiagonal $\\mathbf{L}$ such that $\\mathbf{Q} = \\mathbf{L}\\mathbf{L}^\\top$, using only the $O(n)$ recurrences valid for symmetric positive definite tridiagonal matrices.\n- Use the factorization to transform the problem into an ordinary least squares problem on appropriately whitened variables, and solve for $\\boldsymbol{\\beta}$.\n- You must not rely on dense Cholesky routines. Your implementation must explicitly exploit the tridiagonal sparsity of $\\mathbf{Q}$.\n\nTest suite. For each test case below, construct $\\mathbf{Q}$ via the given $\\alpha$ and $w$, perform the sparse Cholesky-based GLS, and return the estimated coefficient vector $\\widehat{\\boldsymbol{\\beta}}$, rounded to $6$ decimal places.\n\n- Test Case A (happy path):\n  - $n = 5$, $\\alpha = 1.0$, $w = 0.4$.\n  - $\\mathbf{X} \\in \\mathbb{R}^{5 \\times 2}$ with rows $\\big([1,0],[1,1],[1,2],[1,3],[1,4]\\big)$.\n  - $\\mathbf{y} = [1.0, 2.2, 2.0, 3.6, 5.1]^\\top$.\n- Test Case B (boundary, uncorrelated errors):\n  - $n = 5$, $\\alpha = 1.0$, $w = 0.0$.\n  - Same $\\mathbf{X}$ and $\\mathbf{y}$ as in Test Case A.\n- Test Case C (stronger correlation and higher dimension):\n  - $n = 6$, $\\alpha = 0.5$, $w = 1.0$.\n  - $\\mathbf{X} \\in \\mathbb{R}^{6 \\times 3}$ with rows $\\big([1,0,0],[1,1,1],[1,2,0],[1,3,1],[1,4,0],[1,5,1]\\big)$.\n  - $\\mathbf{y} = [0.5, 1.6, 2.1, 3.2, 3.7, 4.8]^\\top$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is a list of the estimated coefficients for one test case in the order A, B, C. Each coefficient must be rounded to $6$ decimal places. For example, a valid output shape is similar to $[[b_{A,1},b_{A,2}], [b_{B,1},b_{B,2}], [b_{C,1},b_{C,2},b_{C,3}]]$ with the actual numeric values substituted.\n- There are no physical units or angles in this problem.\n- All answers must be numbers (floating-point), and the single line must be exactly the aggregate list as described, with no extra text.", "solution": "The problem requires the calculation of the Generalized Least Squares (GLS) estimator for a linear model where the errors exhibit correlation defined by a specific tridiagonal precision matrix $\\mathbf{Q}$. The solution must be derived from fundamental principles and must exploit the sparse structure of $\\mathbf{Q}$ through a custom sparse Cholesky factorization.\n\nThe linear model is given by $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$, where $\\mathbf{y} \\in \\mathbb{R}^n$ is the outcome vector, $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$ is the design matrix, $\\boldsymbol{\\beta} \\in \\mathbb{R}^p$ is the coefficient vector to be estimated, and $\\boldsymbol{\\varepsilon} \\in \\mathbb{R}^n$ is the error vector. The errors are assumed to follow a multivariate normal distribution, $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\boldsymbol{\\Sigma})$, with mean $\\mathbf{0}$ and covariance matrix $\\boldsymbol{\\Sigma}$. The precision matrix is defined as $\\mathbf{Q} = \\boldsymbol{\\Sigma}^{-1}$.\n\nThe probability density function of $\\boldsymbol{\\varepsilon}$ is proportional to $\\exp\\left(-\\frac{1}{2}\\boldsymbol{\\varepsilon}^\\top \\mathbf{Q} \\boldsymbol{\\varepsilon}\\right)$. Substituting $\\boldsymbol{\\varepsilon} = \\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}$, the log-likelihood of the data, up to an additive constant, is proportional to $-\\frac{1}{2}(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^\\top \\mathbf{Q} (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})$. The maximum likelihood estimator $\\widehat{\\boldsymbol{\\beta}}$ is the vector that minimizes the quadratic form, which is the GLS objective function:\n$$\nS(\\boldsymbol{\\beta}) = (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^\\top \\mathbf{Q} (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\n$$\nThe problem specifies that the precision matrix $\\mathbf{Q}$ is symmetric and positive definite, which allows for a unique Cholesky decomposition $\\mathbf{Q} = \\mathbf{L}\\mathbf{L}^\\top$, where $\\mathbf{L}$ is a real, lower triangular matrix. Substituting this into the objective function:\n$$\nS(\\boldsymbol{\\beta}) = (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^\\top (\\mathbf{L}\\mathbf{L}^\\top) (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}) = \\left(\\mathbf{L}^\\top(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\\right)^\\top \\left(\\mathbf{L}^\\top(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\\right)\n$$\nThis can be rewritten as a standard sum-of-squares. Let us define the \"whitened\" variables by pre-multiplying by $\\mathbf{L}^\\top$:\n$$\n\\mathbf{y}' = \\mathbf{L}^\\top\\mathbf{y}\n$$\n$$\n\\mathbf{X}' = \\mathbf{L}^\\top\\mathbf{X}\n$$\nThe objective function then becomes the Ordinary Least Squares (OLS) objective for the transformed variables:\n$$\nS(\\boldsymbol{\\beta}) = (\\mathbf{y}' - \\mathbf{X}'\\boldsymbol{\\beta})^\\top (\\mathbf{y}' - \\mathbf{X}'\\boldsymbol{\\beta}) = ||\\mathbf{y}' - \\mathbf{X}'\\boldsymbol{\\beta}||_2^2\n$$\nThe solution to this OLS problem, $\\widehat{\\boldsymbol{\\beta}}$, is found by solving the normal equations:\n$$\n(\\mathbf{X}')^\\top \\mathbf{X}' \\widehat{\\boldsymbol{\\beta}} = (\\mathbf{X}')^\\top \\mathbf{y}'\n$$\nThe computational strategy is therefore to:\n$1$. Construct the precision matrix $\\mathbf{Q}$.\n$2$. Compute its Cholesky factor $\\mathbf{L}$.\n$3$. Use $\\mathbf{L}$ to whiten the data $\\mathbf{y}$ and $\\mathbf{X}$.\n$4$. Solve the resulting OLS problem.\n\nThe key to an efficient implementation lies in exploiting the specific structure of $\\mathbf{Q}$. It is given as $\\mathbf{Q} = \\alpha \\mathbf{I}_n + w \\mathbf{L}_{\\text{path}}$, where $\\alpha  0$, $w \\ge 0$, and $\\mathbf{L}_{\\text{path}}$ is the Laplacian of a path graph. This makes $\\mathbf{Q}$ a symmetric tridiagonal matrix.\nLet the diagonal of $\\mathbf{Q}$ be $d_0, d_1, \\dots, d_{n-1}$ and its sub-diagonal be $e_0, e_1, \\dots, e_{n-2}$. From the definition of $\\mathbf{L}_{\\text{path}}$:\n- Diagonal: $d_i = \\alpha + 2w$ for $i \\in \\{1, \\dots, n-2\\}$, and $d_0 = d_{n-1} = \\alpha + w$.\n- Sub-diagonal: $e_i = -w$ for $i \\in \\{0, \\dots, n-2\\}$.\n\nThe Cholesky factor $\\mathbf{L}$ of a symmetric tridiagonal matrix is a lower bidiagonal matrix. Let its diagonal be $l_0, \\dots, l_{n-1}$ and its sub-diagonal be $m_0, \\dots, m_{n-2}$. Equating the elements of $\\mathbf{Q}$ with those of $\\mathbf{L}\\mathbf{L}^\\top$:\n$$\n\\mathbf{Q}_{i,i} = d_i = l_i^2 + m_{i-1}^2 \\quad (\\text{with } m_{-1}=0)\n$$\n$$\n\\mathbf{Q}_{i,i-1} = e_{i-1} = l_{i-1}m_{i-1}\n$$\nThis leads to the following $O(n)$ recurrence relations for the elements of $\\mathbf{L}$:\n$1$. $l_0 = \\sqrt{d_0}$\n$2$. For $i = 1, \\dots, n-1$:\n   a. $m_{i-1} = e_{i-1} / l_{i-1}$\n   b. $l_i = \\sqrt{d_i - m_{i-1}^2}$\nThis constitutes a sparse Cholesky factorization.\n\nNext, the whitening step $\\mathbf{v}' = \\mathbf{L}^\\top \\mathbf{v}$ must be performed efficiently. The matrix $\\mathbf{L}^\\top$ is upper bidiagonal, with diagonal $l_i$ and super-diagonal $(\\mathbf{L}^\\top)_{i, i+1} = m_i$. The multiplication can be carried out in $O(n)$ time without forming dense matrices:\n$$\nv'_i = l_i v_i + m_i v_{i+1} \\quad \\text{for } i = 0, \\dots, n-2\n$$\n$$\nv'_{n-1} = l_{n-1} v_{n-1}\n$$\nThis sparse multiplication is applied to compute $\\mathbf{y}' = \\mathbf{L}^\\top \\mathbf{y}$ and each column of $\\mathbf{X}' = \\mathbf{L}^\\top \\mathbf{X}$. The total cost for whitening is $O(np)$.\n\nFinally, the $p \\times p$ system of normal equations $(\\mathbf{X}')^\\top \\mathbf{X}' \\widehat{\\boldsymbol{\\beta}} = (\\mathbf{X}')^\\top \\mathbf{y}'$ is constructed and solved for $\\widehat{\\boldsymbol{\\beta}}$, typically using a standard linear system solver. This entire procedure avoids forming dense $n \\times n$ matrices and is computationally efficient.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    \"\"\"\n\n    def perform_gls(n, alpha, w, X, y):\n        \"\"\"\n        Performs Generalized Least Squares using sparse Cholesky factorization.\n\n        Args:\n            n (int): Number of observations.\n            alpha (float): Parameter for the precision matrix.\n            w (float): Parameter for the precision matrix.\n            X (np.ndarray): Design matrix of shape (n, p).\n            y (np.ndarray): Outcome vector of shape (n,).\n\n        Returns:\n            np.ndarray: The estimated coefficient vector beta_hat.\n        \"\"\"\n        # Step 1: Construct the diagonal and sub-diagonal of the tridiagonal precision matrix Q.\n        # Q = alpha * I + w * L_path\n        q_diag = np.full(n, alpha + 2 * w)\n        if n > 0:\n            q_diag[0] = alpha + w\n            q_diag[-1] = alpha + w\n        q_subdiag = np.full(n - 1, -w)\n\n        # Step 2: Perform sparse Cholesky factorization Q = LL^T.\n        # L is a lower bidiagonal matrix with diagonal l_diag and sub-diagonal m_subdiag.\n        l_diag = np.zeros(n)\n        m_subdiag = np.zeros(n - 1)\n\n        if n > 0:\n            l_diag[0] = np.sqrt(q_diag[0])\n            for i in range(1, n):\n                m_subdiag[i - 1] = q_subdiag[i - 1] / l_diag[i - 1]\n                # The argument to sqrt is guaranteed to be positive because Q is positive definite.\n                l_diag[i] = np.sqrt(q_diag[i] - m_subdiag[i - 1]**2)\n\n        # Step 3: Whiten the variables using L^T.\n        # L^T is an upper bidiagonal matrix.\n        # y_prime = L^T * y and X_prime = L^T * X.\n        y_prime = np.zeros(n)\n        X_prime = np.zeros_like(X, dtype=float)\n        p = X.shape[1]\n\n        # Whiten y\n        if n > 0:\n            for i in range(n - 1):\n                y_prime[i] = l_diag[i] * y[i] + m_subdiag[i] * y[i + 1]\n            y_prime[n - 1] = l_diag[n - 1] * y[n - 1]\n\n        # Whiten X column by column\n        if n > 0:\n            for j in range(p):\n                x_col = X[:, j]\n                x_prime_col = np.zeros(n)\n                for i in range(n - 1):\n                    x_prime_col[i] = l_diag[i] * x_col[i] + m_subdiag[i] * x_col[i + 1]\n                x_prime_col[n - 1] = l_diag[n - 1] * x_col[n - 1]\n                X_prime[:, j] = x_prime_col\n        \n        # Step 4: Solve the OLS problem for the whitened variables.\n        # (X_prime^T * X_prime) * beta_hat = X_prime^T * y_prime\n        A_ols = X_prime.T @ X_prime\n        b_ols = X_prime.T @ y_prime\n        beta_hat = np.linalg.solve(A_ols, b_ols)\n\n        return beta_hat\n\n    test_cases = [\n        # Test Case A\n        {\n            \"n\": 5, \"alpha\": 1.0, \"w\": 0.4,\n            \"X\": np.array([[1, 0], [1, 1], [1, 2], [1, 3], [1, 4]]),\n            \"y\": np.array([1.0, 2.2, 2.0, 3.6, 5.1]),\n        },\n        # Test Case B\n        {\n            \"n\": 5, \"alpha\": 1.0, \"w\": 0.0,\n            \"X\": np.array([[1, 0], [1, 1], [1, 2], [1, 3], [1, 4]]),\n            \"y\": np.array([1.0, 2.2, 2.0, 3.6, 5.1]),\n        },\n        # Test Case C\n        {\n            \"n\": 6, \"alpha\": 0.5, \"w\": 1.0,\n            \"X\": np.array([[1, 0, 0], [1, 1, 1], [1, 2, 0], [1, 3, 1], [1, 4, 0], [1, 5, 1]]),\n            \"y\": np.array([0.5, 1.6, 2.1, 3.2, 3.7, 4.8]),\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        beta_hat = perform_gls(case[\"n\"], case[\"alpha\"], case[\"w\"], case[\"X\"], case[\"y\"])\n        # Round to 6 decimal places and convert to a list for formatting.\n        results.append(np.round(beta_hat, 6).tolist())\n\n    # Format the output as a list of lists.\n    # The f-string calls str() on each element, which correctly formats the inner lists.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3112081"}]}