{"hands_on_practices": [{"introduction": "Weighted Least Squares (WLS) offers a powerful solution to heteroscedasticity by assigning different weights to observations. However, blindly applying theoretically optimal weights can lead to practical issues. This exercise provides a hands-on look at a crucial pitfall: the numerical instability that arises when weights become excessively large, a common occurrence when error variance is tied to predictor values near zero. By investigating the WLS estimator's variance and the condition number of the normal matrix, you will gain a deeper mechanical understanding of how weights influence model stability and how a simple stabilization technique can lead to more reliable estimates. [@problem_id:3128028]", "problem": "Consider the linear regression model with heteroscedastic errors, where a response variable is modeled as $y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$, for $i = 1, \\dots, n$. Assume the errors satisfy $E[\\varepsilon_i] = 0$ and $\\operatorname{Var}(\\varepsilon_i) = \\sigma^2 |x_i|$, where $\\sigma^2 > 0$ is an unknown constant of proportionality. Weighted Least Squares (WLS) uses positive weights $w_i$ to minimize $\\sum_{i=1}^n w_i (y_i - \\beta_0 - \\beta_1 x_i)^2$. When $\\operatorname{Var}(\\varepsilon_i)$ is proportional to $|x_i|$, a natural choice is to take $w_i$ proportional to $1/|x_i|$, but this can excessively amplify observations with $x_i$ near $0$ and distort intercept estimation. To mitigate this, consider stabilized weights $w_i = 1 / (|x_i| + \\delta)$, where $\\delta \\ge 0$ is a stabilization parameter.\n\nStarting from the linear model definition, and the principle that the best linear unbiased estimator under heteroscedasticity arises from appropriately weighting by the inverse of the error variance, derive the variance of the intercept estimator under a general diagonal weight matrix and a diagonal error variance structure implied by $\\operatorname{Var}(\\varepsilon_i)=\\sigma^2|x_i|$. Explain, from first principles and without shortcut formulas provided in the problem statement, why the choice $w_i = 1/|x_i|$ can cause instability for intercept estimation when some $|x_i|$ are very small, and how the stabilized choice $w_i = 1/(|x_i| + \\delta)$ bounds the influence of near-zero $x_i$.\n\nThen, implement a program that, for each test case specified below, computes the following two quantities:\n- The ratio $r$ of the WLS intercept estimator variance to the Ordinary Least Squares (OLS) intercept estimator variance, where OLS sets $w_i = 1$ for all $i$ but still respects $\\operatorname{Var}(\\varepsilon_i)=\\sigma^2|x_i|$ when assessing the estimator variance.\n- The spectral condition number $\\kappa$ of the weighted normal matrix $X^\\top W X$, where $X$ is the design matrix with first column equal to ones and second column equal to the $x_i$, and $W = \\operatorname{diag}(w_1, \\dots, w_n)$.\n\nYour program must:\n- Use $X = \\begin{bmatrix} 1 & x_1 \\\\ \\vdots & \\vdots \\\\ 1 & x_n \\end{bmatrix}$.\n- Use $\\Sigma = \\operatorname{diag}(\\sigma^2 |x_1|, \\dots, \\sigma^2 |x_n|)$ to represent the error variance structure.\n- Compute the intercept variance under WLS using only the model assumptions provided above.\n- Compute the intercept variance under OLS using only the model assumptions provided above.\n- Set $\\sigma^2 = 1$ for all test cases.\n- Round each reported float to six decimal places.\n- Produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a comma-separated pair $[r,\\kappa]$ with no spaces.\n\nTest Suite (each test case specifies the vector of $x_i$ values and the stabilization parameter $\\delta$):\n1. Happy path (no zeros): $x = (-2.0, -1.0, -0.5, -0.2, 0.1, 0.3, 0.6, 1.0, 2.0)$, $\\delta = 0.0$.\n2. Boundary sensitivity (near-zero values, no exact zeros): $x = (-2.0, -1.0, -0.5, -10^{-8}, 10^{-8}, 0.1, 0.3, 0.6, 1.0, 2.0)$, $\\delta = 0.0$.\n3. Stabilized with an exact zero: $x = (-2.0, -1.0, -0.5, -0.2, 0.0, 0.05, 0.1, 0.3, 0.6, 1.0, 2.0)$, $\\delta = 0.1$.\n4. Strong stabilization: $x = (-2.0, -1.0, -0.5, -0.2, 0.0, 0.05, 0.1, 0.3, 0.6, 1.0, 2.0)$, $\\delta = 0.5$.\n\nAnswer specification:\n- For each test case, output the pair $[r,\\kappa]$ as described above.\n- Your program should produce a single line of output containing all test case results as a comma-separated list enclosed in square brackets. There must be no spaces anywhere in the output string.", "solution": "The problem statement is a valid exercise in statistical learning, concerning the properties of Weighted Least Squares (WLS) estimators under heteroscedasticity. It requires both a theoretical derivation and explanation from first principles and a numerical implementation to illustrate the concepts. All components of the problem are scientifically grounded, well-defined, and internally consistent.\n\nThe linear model is given in matrix form as $y = X\\beta + \\varepsilon$, where $y$ is an $n \\times 1$ vector of observations, $X$ is the $n \\times 2$ design matrix, $\\beta = [\\beta_0, \\beta_1]^\\top$ is the vector of coefficients, and $\\varepsilon$ is the $n \\times 1$ vector of errors. The assumptions on the errors are $E[\\varepsilon] = 0$ and $\\operatorname{Var}(\\varepsilon) = E[\\varepsilon \\varepsilon^\\top] = \\Sigma$, where $\\Sigma$ is a diagonal matrix with entries $[\\Sigma]_{ii} = \\sigma^2|x_i|$.\n\nThe Weighted Least Squares (WLS) estimator, $\\hat{\\beta}_{WLS}$, is found by minimizing the weighted sum of squared residuals, $S(\\beta) = (y - X\\beta)^\\top W (y - X\\beta)$, where $W$ is a diagonal matrix of positive weights $w_i$. The solution to this minimization problem is given by the weighted normal equations:\n$$ (X^\\top W X) \\hat{\\beta}_{WLS} = X^\\top W y $$\nThis yields the estimator:\n$$ \\hat{\\beta}_{WLS} = (X^\\top W X)^{-1} X^\\top W y $$\n\nThe variance of this estimator is derived as follows. Since $\\hat{\\beta}_{WLS}$ is a linear function of $y$, its variance is:\n$$ \\operatorname{Var}(\\hat{\\beta}_{WLS}) = \\operatorname{Var}((X^\\top W X)^{-1} X^\\top W y) $$\nUsing the fact that $\\beta$ is a constant vector and $\\operatorname{Var}(y) = \\operatorname{Var}(X\\beta + \\varepsilon) = \\operatorname{Var}(\\varepsilon) = \\Sigma$, we have:\n$$ \\operatorname{Var}(\\hat{\\beta}_{WLS}) = (X^\\top W X)^{-1} (X^\\top W) \\operatorname{Var}(y) (X^\\top W)^\\top ((X^\\top W X)^{-1})^\\top $$\n$$ \\operatorname{Var}(\\hat{\\beta}_{WLS}) = (X^\\top W X)^{-1} (X^\\top W \\Sigma W^\\top X) (X^\\top W X)^{-1} $$\nSince $W$ and $X^\\top W X$ are symmetric, this simplifies to the \"sandwich\" formula:\n$$ \\operatorname{Var}(\\hat{\\beta}_{WLS}) = (X^\\top W X)^{-1} (X^\\top W \\Sigma W X) (X^\\top W X)^{-1} $$\nThe variance of the intercept estimator, $\\operatorname{Var}(\\hat{\\beta}_{0, WLS})$, is the first diagonal element, i.e., the $(1,1)$-entry, of this $2 \\times 2$ matrix. This general formula is valid for any choice of positive weights $W$ and is not restricted to the \"optimal\" case where $W \\propto \\Sigma^{-1}$.\n\nNow we analyze the choice of weights.\nThe optimal weights for WLS, which produce the Best Linear Unbiased Estimator (BLUE), are proportional to the inverse of the error variances, i.e., $w_i \\propto 1/\\operatorname{Var}(\\varepsilon_i) = 1/(\\sigma^2|x_i|)$. Let us choose $w_i = 1/|x_i|$ (implicitly setting $\\sigma^2=1$ in the weight definition, which is permissible as the estimator is invariant to a constant scaling of weights). In this special case, the weight matrix is $W = (1/\\sigma^2)\\Sigma^{-1}$ (if we were to include the $\\sigma^2$ term). Let's see how this affects the sandwich formula's middle term:\n$$ X^\\top W \\Sigma W X = X^\\top \\left(\\frac{1}{\\sigma^2}\\Sigma^{-1}\\right) \\Sigma \\left(\\frac{1}{\\sigma^2}\\Sigma^{-1}\\right) X = \\frac{1}{\\sigma^2} X^\\top \\Sigma^{-1} X $$\nAnd the outer terms:\n$$ X^\\top W X = X^\\top \\left(\\frac{1}{\\sigma^2}\\Sigma^{-1}\\right) X = \\frac{1}{\\sigma^2} X^\\top \\Sigma^{-1} X $$\nSubstituting these into the sandwich formula yields:\n$$ \\operatorname{Var}(\\hat{\\beta}_{BLUE}) = \\left(\\frac{1}{\\sigma^2} X^\\top \\Sigma^{-1} X\\right)^{-1} \\left(\\frac{1}{\\sigma^2} X^\\top \\Sigma^{-1} X\\right) \\left(\\frac{1}{\\sigma^2} X^\\top \\Sigma^{-1} X\\right)^{-1} = \\sigma^2 (X^\\top \\Sigma^{-1} X)^{-1} = (X^\\top W X)^{-1} \\sigma^2 $$\n(using $W = \\Sigma^{-1}/\\sigma^2$ would give $\\sigma^4(X^\\top \\Sigma^{-1} X)^{-1}$ etc. If we set $W = \\Sigma^{-1}$, $\\operatorname{Var} = (X^\\top\\Sigma^{-1}X)^{-1}$). With the choice $w_i=1/|x_i|$ and $\\sigma^2=1$, we get $W = \\Sigma^{-1}$ and $\\operatorname{Var}(\\hat{\\beta})=(X^\\top W X)^{-1}$.\n\nInstability arises when an observation $x_k$ is very close to $0$ and we use the theoretically optimal weights $w_i = 1/|x_i|$ (i.e., $\\delta=0$).\n1.  **Numerical Instability**: The weight $w_k = 1/|x_k|$ becomes extremely large. The normal matrix $X^\\top W X = \\begin{pmatrix} \\sum w_i & \\sum w_i x_i \\\\ \\sum w_i x_i & \\sum w_i x_i^2 \\end{pmatrix}$ becomes ill-conditioned. The term $\\sum w_i$ is dominated by $w_k$, making the top-left entry enormous, while the other entries are not scaled to the same degree. For example, $\\sum w_i x_i^2 = \\sum |x_i|$ is not dominated by the $k$-th term. This disparity in magnitudes makes the matrix nearly singular, which is quantified by a very large spectral condition number, $\\kappa$. Inverting such a matrix is numerically unstable and highly sensitive to small perturbations in the data.\n2.  **Estimation Distortion**: The WLS objective function $\\sum w_i (y_i - \\beta_0 - \\beta_1 x_i)^2$ is dominated by the term for $i=k$. To minimize this sum, the regression line is forced to pass extremely close to the point $(x_k, y_k)$, i.e., $y_k \\approx \\hat{\\beta}_0 + \\hat{\\beta}_1 x_k$. As $x_k \\to 0$, this implies $\\hat{\\beta}_0 \\approx y_k$. The estimate of the intercept becomes almost entirely determined by a single observation, $y_k$, whose own value is subject to random error. This extreme amplification of the influence of one data point distorts the intercept estimate, making it unreliable.\n\nThe stabilized weights $w_i = 1 / (|x_i| + \\delta)$ with $\\delta > 0$ mitigate this problem.\nAs $x_i \\to 0$, the stabilized weight $w_i \\to 1/\\delta$. The weight is thus bounded from above by $1/\\delta$. This prevents any single weight from becoming arbitrarily large. Consequently, no single observation can dominate the least squares fit. All entries in the matrix $X^\\top W X$ remain bounded and of comparable magnitude, leading to a much smaller (better) condition number and a more stable, robust numerical solution. The resulting intercept estimate, $\\hat{\\beta}_0$, will be a balanced compromise across all data points, as intended by the least squares principle. The trade-off is that these non-optimal weights lead to an estimator that is no longer the BLUE, potentially increasing its variance compared to the theoretical (but unstable) optimum.\n\nFor the implementation, we compute the following quantities:\n- The variance of the WLS intercept estimator, $\\operatorname{Var}(\\hat{\\beta}_{0, WLS})$, is the $(1,1)$-entry of $(X^\\top W X)^{-1} (X^\\top W \\Sigma W X) (X^\\top W X)^{-1}$, where $W=\\operatorname{diag}(1/(|x_i|+\\delta))$ and $\\Sigma=\\operatorname{diag}(|x_i|)$ (with $\\sigma^2=1$).\n- The variance of the OLS intercept estimator, $\\operatorname{Var}(\\hat{\\beta}_{0, OLS})$, is computed using the same general sandwich formula but with weights $w_i=1$ for all $i$. So, we set $W_{OLS}=I$ (the identity matrix). The variance is the $(1,1)$-entry of $(X^\\top X)^{-1} (X^\\top \\Sigma X) (X^\\top X)^{-1}$. This correctly accounts for the true heteroscedastic error structure even though the OLS estimator itself does not use this information.\n- The ratio is $r = \\operatorname{Var}(\\hat{\\beta}_{0, WLS}) / \\operatorname{Var}(\\hat{\\beta}_{0, OLS})$.\n- The condition number is $\\kappa = \\kappa(X^\\top W X)$.\n\nThe program will execute these calculations for each specified test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the ratio of WLS to OLS intercept variance and the condition number\n    of the weighted normal matrix for several test cases of heteroscedastic linear regression.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\"x\": np.array([-2.0, -1.0, -0.5, -0.2, 0.1, 0.3, 0.6, 1.0, 2.0]), \"delta\": 0.0},\n        {\"x\": np.array([-2.0, -1.0, -0.5, -1e-8, 1e-8, 0.1, 0.3, 0.6, 1.0, 2.0]), \"delta\": 0.0},\n        {\"x\": np.array([-2.0, -1.0, -0.5, -0.2, 0.0, 0.05, 0.1, 0.3, 0.6, 1.0, 2.0]), \"delta\": 0.1},\n        {\"x\": np.array([-2.0, -1.0, -0.5, -0.2, 0.0, 0.05, 0.1, 0.3, 0.6, 1.0, 2.0]), \"delta\": 0.5},\n    ]\n\n    results = []\n    \n    # Set the unknown constant of proportionality to 1 as specified.\n    sigma_squared = 1.0\n\n    for case in test_cases:\n        x_vec = case[\"x\"]\n        delta = case[\"delta\"]\n        n = len(x_vec)\n\n        # Construct the design matrix X\n        X = np.vstack((np.ones(n), x_vec)).T\n\n        # Construct the true error covariance matrix Sigma\n        # Var(epsilon_i) = sigma^2 * |x_i|\n        Sigma = np.diag(sigma_squared * np.abs(x_vec))\n\n        # --- WLS Calculation ---\n        # Construct the WLS weight matrix W\n        # w_i = 1 / (|x_i| + delta)\n        w_vals = 1.0 / (np.abs(x_vec) + delta)\n        W = np.diag(w_vals)\n\n        # Calculate the weighted normal matrix for WLS\n        X_T_W_X = X.T @ W @ X\n        \n        # Calculate the \"sandwich\" middle part for WLS\n        X_T_W_Sigma_W_X = X.T @ W @ Sigma @ W @ X\n\n        # Calculate the variance-covariance matrix for the WLS estimator\n        # Var(beta_hat_WLS) = (X'WX)^-1 (X'W Sigma WX) (X'WX)^-1\n        X_T_W_X_inv = np.linalg.inv(X_T_W_X)\n        var_beta_wls = X_T_W_X_inv @ X_T_W_Sigma_W_X @ X_T_W_X_inv\n\n        # Extract variance of the intercept estimator (beta_0)\n        var_b0_wls = var_beta_wls[0, 0]\n\n        # Calculate the spectral condition number of the weighted normal matrix\n        kappa = np.linalg.cond(X_T_W_X)\n\n        # --- OLS Calculation ---\n        # For OLS, the weights w_i are all 1.\n        # The estimator is (X'X)^-1 X'y, and its variance must be calculated\n        # using the true heteroscedastic covariance matrix Sigma.\n        X_T_X = X.T @ X\n        \n        # OLS \"sandwich\" middle part\n        X_T_Sigma_X = X.T @ Sigma @ X\n\n        # Calculate the variance-covariance matrix for the OLS estimator\n        # Var(beta_hat_OLS) = (X'X)^-1 (X' Sigma X) (X'X)^-1\n        X_T_X_inv = np.linalg.inv(X_T_X)\n        var_beta_ols = X_T_X_inv @ X_T_Sigma_X @ X_T_X_inv\n        \n        # Extract variance of the intercept estimator (beta_0)\n        var_b0_ols = var_beta_ols[0, 0]\n\n        # --- Ratio Calculation ---\n        # Ratio of WLS intercept variance to OLS intercept variance\n        if var_b0_ols == 0:\n            # Handle potential division by zero, though unlikely in these cases\n            r = np.inf if var_b0_wls > 0 else 0.0\n        else:\n            r = var_b0_wls / var_b0_ols\n        \n        results.append((r, kappa))\n\n    # Format the final output string as specified, with 6 decimal places and no spaces\n    output_str = \",\".join([f\"[{r:.6f},{k:.6f}]\" for r, k in results])\n    print(f\"[{output_str}]\")\n\nsolve()\n\n```", "id": "3128028"}, {"introduction": "In most real-world scenarios, the exact structure of heteroscedasticity is unknown and must be estimated from the data. This practice introduces Feasible Weighted Least Squares (FWLS), a powerful multi-stage procedure to tackle this challenge. You will first run an Ordinary Least Squares (OLS) regression to obtain residuals, and then use these residuals in an \"auxiliary regression\" to model the unknown variance function, thereby estimating the appropriate weights for a final WLS fit. This exercise demonstrates the flexibility of modeling the variance, even when it depends on interactions between predictors, a common situation in complex datasets. [@problem_id:3127977]", "problem": "You are given a linear regression setup in which the response is modeled as $y_i = x_i^\\top \\beta + \\varepsilon_i$ with mean-zero errors that may be heteroscedastic due to an interaction between the first two non-intercept covariates. Assume that the conditional variance follows the interaction form $\\operatorname{Var}(\\varepsilon_i \\mid x_i) = \\sigma^2 \\left(1 + \\theta \\, z_i\\right)$ where $z_i = x_{i1} x_{i2}$ is the product of the first two non-intercept regressors. Your task is to derive and implement a feasible weighted least squares procedure based on a variance regression on interaction terms.\n\nFundamental base:\n- Ordinary least squares minimizes $\\sum_{i=1}^n (y_i - x_i^\\top \\beta)^2$.\n- Weighted least squares minimizes $\\sum_{i=1}^n w_i (y_i - x_i^\\top \\beta)^2$ for known positive weights $w_i$ and yields the estimate that solves $(X^\\top W X)\\hat\\beta = X^\\top W y$, where $W=\\operatorname{diag}(w_1,\\dots,w_n)$.\n- If $\\operatorname{Var}(\\varepsilon_i \\mid x_i)$ is proportional to a known positive function $v_i$, weights proportional to $1/v_i$ minimize the variance of the estimator under standard regularity conditions.\n\nYour derivation and implementation must adhere to the following:\n- Begin from the definitions above and the assumed conditional variance structure. Do not assume any particular estimator formula without showing why it follows from the base and the model. In particular, derive why weights proportional to the inverse of the fitted conditional variance are appropriate.\n- Propose a feasible method by first fitting ordinary least squares to obtain residuals $u_i$, then fitting a variance-regression model of the form $u_i^2 \\approx \\alpha + \\gamma z_i$ using ordinary least squares on the regressors consisting of an intercept and $z_i$, and finally using the fitted conditional variance proxy to construct weights.\n- Ensure scientific realism in the implementation: guard against nonpositive fitted variances by using a small positive floor $\\varepsilon_{\\min}$, i.e., use $\\hat v_i = \\max(\\varepsilon_{\\min}, \\hat\\alpha + \\hat\\gamma z_i)$ and set $w_i = 1/\\hat v_i$. Use $\\varepsilon_{\\min} = 10^{-8}$.\n- Let the interaction regressor $z_i$ be defined as the product of the first two non-intercept columns of the design matrix $X$ (that is, if $X$ has an intercept in the first column, then $x_{i1}$ is the second column and $x_{i2}$ is the third column).\n\nImplementation requirements:\n1) For each dataset below, construct the response $y$ using the provided recipe and then compute:\n   - The ordinary least squares estimate $\\hat\\beta_{\\mathrm{OLS}}$ and residuals $u$.\n   - The variance regression ordinary least squares fit of $u_i^2$ on an intercept and $z_i$, yielding $\\hat\\alpha$ and $\\hat\\gamma$.\n   - The parameter $\\hat\\theta = \\hat\\gamma/\\hat\\alpha$ if $\\hat\\alpha > 0$; otherwise set $\\hat\\theta = 0$.\n   - The feasible weighted least squares estimate $\\hat\\beta_{\\mathrm{FWLS}}$ using weights $w_i = 1/\\hat v_i$ with $\\hat v_i = \\max(\\varepsilon_{\\min}, \\hat\\alpha + \\hat\\gamma z_i)$.\n2) For each dataset, report exactly two quantities:\n   - The scalar $\\hat\\theta$.\n   - The intercept coefficient (the first component) of $\\hat\\beta_{\\mathrm{FWLS}}$.\n\nYour program should produce a single line of output containing all results concatenated into a single flat list in the order of the datasets as\n$[\\hat\\theta_1, \\hat\\beta_{\\mathrm{FWLS},0}^{(1)}, \\hat\\theta_2, \\hat\\beta_{\\mathrm{FWLS},0}^{(2)}, \\hat\\theta_3, \\hat\\beta_{\\mathrm{FWLS},0}^{(3)}]$,\nwith each real number rounded to $6$ decimal places.\n\nTest suite datasets:\n- In all datasets, $X$ includes an intercept in the first column, followed by two regressors $x_{i1}$ and $x_{i2}$, so $X \\in \\mathbb{R}^{n \\times 3}$. Define $z_i = x_{i1} x_{i2}$. The response is generated as\n$$\ny = X \\beta_{\\mathrm{true}} + \\sigma \\cdot \\sqrt{1 + \\theta_{\\mathrm{true}} z} \\odot e,\n$$\nwhere $\\odot$ denotes the elementwise product, the square root is applied elementwise, and $e$ is a provided vector.\n\n- Dataset $1$ (homoscedastic boundary):\n  - $n = 6$,\n  - $X$ rows $(1, x_{i1}, x_{i2})$ are\n    $[(1, -2, 1), (1, -1, 0), (1, 0, -1), (1, 1, 2), (1, 2, -2), (1, 3, 1)]$,\n    that is, $x_{1} = [-2, -1, 0, 1, 2, 3]$ and $x_{2} = [1, 0, -1, 2, -2, 1]$,\n  - $\\beta_{\\mathrm{true}} = [1, 2, -1]$,\n  - $\\sigma = 1$,\n  - $\\theta_{\\mathrm{true}} = 0$,\n  - $e = [0.1, -0.2, 0.05, -0.1, 0.0, 0.15]$.\n\n- Dataset $2$ (positive heteroscedasticity in interaction):\n  - $n = 8$,\n  - $X$ rows $(1, x_{i1}, x_{i2})$ are\n    $[(1, -1, -2), (1, -0.5, 1), (1, 0.5, 1.5), (1, 1, -1), (1, 1.5, 2), (1, -1.5, -1), (1, 2, 2.5), (1, -2, -2)]$,\n    that is, $x_{1} = [-1, -0.5, 0.5, 1, 1.5, -1.5, 2, -2]$ and $x_{2} = [-2, 1, 1.5, -1, 2, -1, 2.5, -2]$,\n  - $\\beta_{\\mathrm{true}} = [0.5, -1.0, 2.0]$,\n  - $\\sigma = 0.7$,\n  - $\\theta_{\\mathrm{true}} = 0.1$,\n  - $e = [0.5, -0.8, 0.3, -1.2, 1.5, -0.4, 0.9, -0.7]$.\n\n- Dataset $3$ (negative interaction effect in variance, ensuring positivity):\n  - $n = 7$,\n  - $X$ rows $(1, x_{i1}, x_{i2})$ are\n    $[(1, -2, 3), (1, -1, -2), (1, 0, 1), (1, 1, -1), (1, 2, 2), (1, -3, -1), (1, 3, 1)]$,\n    that is, $x_{1} = [-2, -1, 0, 1, 2, -3, 3]$ and $x_{2} = [3, -2, 1, -1, 2, -1, 1]$,\n  - $\\beta_{\\mathrm{true}} = [2.0, 0.5, 1.5]$,\n  - $\\sigma = 0.9$,\n  - $\\theta_{\\mathrm{true}} = -0.15$,\n  - $e = [1.0, -0.5, 0.2, -0.8, 1.1, -1.3, 0.6]$.\n\nAngle units are not applicable. No physical units are present. All numeric answers must be real numbers.", "solution": "The user-provided problem has been validated and is sound. It outlines a standard statistical task: implementing a Feasible Weighted Least Squares (FWLS) procedure to address heteroscedasticity of a specific form in a linear regression model.\n\nThe solution will first derive the theoretical underpinnings of the Weighted Least Squares (WLS) estimator and the rationale for the optimal weights. It will then detail the specified three-step feasible estimation procedure for the given variance structure.\n\n### 1. Derivation of the Weighted Least Squares (WLS) Estimator\n\nWe begin with the general linear regression model:\n$$y = X\\beta + \\varepsilon$$\nwhere $y$ is the $n \\times 1$ vector of observations, $X$ is the $n \\times p$ design matrix of rank $p$, $\\beta$ is the $p \\times 1$ vector of unknown parameters, and $\\varepsilon$ is the $n \\times 1$ vector of unobservable errors. We assume $E[\\varepsilon | X] = 0$.\n\nIf the errors are homoscedastic and uncorrelated, i.e., $\\operatorname{Var}(\\varepsilon | X) = \\sigma^2 I_n$, the Best Linear Unbiased Estimator (BLUE) for $\\beta$ is the Ordinary Least Squares (OLS) estimator, which minimizes the sum of squared residuals, $S(\\beta) = \\sum_{i=1}^n (y_i - x_i^\\top \\beta)^2 = (y-X\\beta)^\\top(y-X\\beta)$. The OLS estimator is given by:\n$$\\hat\\beta_{\\mathrm{OLS}} = (X^\\top X)^{-1} X^\\top y$$\n\nWhen the errors are heteroscedastic, meaning their variances are not all equal, the OLS estimator remains unbiased but is no longer the BLUE. Let the error covariance matrix be $\\operatorname{Var}(\\varepsilon | X) = \\Omega$, where $\\Omega$ is a diagonal matrix $\\Omega = \\operatorname{diag}(\\sigma_1^2, \\sigma_2^2, \\dots, \\sigma_n^2)$ with $\\sigma_i^2 = \\operatorname{Var}(\\varepsilon_i | x_i)$.\n\nThe Weighted Least Squares (WLS) estimator is designed to handle this situation by minimizing a weighted sum of squared residuals:\n$$S_W(\\beta) = \\sum_{i=1}^n w_i (y_i - x_i^\\top \\beta)^2$$\nwhere $w_i > 0$ are the weights. In matrix form, this is $S_W(\\beta) = (y-X\\beta)^\\top W (y-X\\beta)$, where $W=\\operatorname{diag}(w_1, \\dots, w_n)$. The first-order condition for minimization, $\\frac{\\partial S_W(\\beta)}{\\partial \\beta} = 0$, yields the normal equations:\n$$-2 X^\\top W (y - X\\beta) = 0$$\n$$\\implies (X^\\top W X) \\hat\\beta_{\\mathrm{WLS}} = X^\\top W y$$\nAssuming $X^\\top W X$ is invertible, the WLS estimator is:\n$$\\hat\\beta_{\\mathrm{WLS}} = (X^\\top W X)^{-1} X^\\top W y$$\n\nThe optimal weights $w_i$ are chosen to make the WLS estimator the BLUE. This is achieved by selecting weights that are inversely proportional to the error variances, $w_i \\propto 1/\\sigma_i^2$. To demonstrate this, let us set $w_i = 1/\\sigma_i^2$. We can then define a transformation matrix $P = \\operatorname{diag}(1/\\sigma_1, \\dots, 1/\\sigma_n)$ such that $W = P^\\top P = P^2$. Let's transform the original model:\n$$Py = PX\\beta + P\\varepsilon$$\nLet $y^*=Py$, $X^*=PX$, and $\\varepsilon^* = P\\varepsilon$. The transformed model is $y^* = X^*\\beta + \\varepsilon^*$. The variance of the transformed error $\\varepsilon^*$ is:\n$$\\operatorname{Var}(\\varepsilon^* | X) = E[P\\varepsilon\\varepsilon^\\top P^\\top | X] = P E[\\varepsilon\\varepsilon^\\top | X] P = P \\Omega P = \\operatorname{diag}(\\sigma_1^2, \\dots, \\sigma_n^2) \\operatorname{diag}(1/\\sigma_1^2, \\dots, 1/\\sigma_n^2) = I_n$$\nWe have scaled the variances such that $\\operatorname{Var}(\\varepsilon^*_i | x_i) = 1$. The transformed model satisfies the homoscedasticity assumption. By the Gauss-Markov theorem, applying OLS to the transformed model yields the BLUE for $\\beta$.\n$$\\hat\\beta = ((X^*)^\\top X^*)^{-1} (X^*)^\\top y^* = ((PX)^\\top (PX))^{-1} (PX)^\\top (Py) = (X^\\top P^\\top P X)^{-1} X^\\top P^\\top P y = (X^\\top W X)^{-1} X^\\top W y$$\nThis is precisely the WLS estimator with $w_i = 1/\\sigma_i^2$. Any constant of proportionality $c$ in $w_i = c/\\sigma_i^2$ would cancel out, so proportionality is sufficient.\n\n### 2. Feasible WLS for the Given Variance Model\n\nThe true variances $\\sigma_i^2$ are typically unknown. Feasible WLS (FWLS) is a procedure that uses estimated variances to construct the weights. The problem specifies the variance structure:\n$$\\operatorname{Var}(\\varepsilon_i | x_i) = \\sigma^2(1 + \\theta z_i)$$\nwhere $z_i = x_{i1} x_{i2}$. Since both $\\sigma^2$ and $\\theta$ are unknown, we must estimate them. The proposed FWLS procedure consists of three steps.\n\n**Step 1: OLS Regression and Residuals**\nFirst, we ignore the heteroscedasticity and compute the OLS estimator $\\hat\\beta_{\\mathrm{OLS}} = (X^\\top X)^{-1} X^\\top y$. From this, we obtain the OLS residuals:\n$$u = y - X\\hat\\beta_{\\mathrm{OLS}}$$\nThe squared residuals $u_i^2$ serve as proxies for the unobserved squared errors $\\varepsilon_i^2$, as $E[u_i^2]$ is a consistent estimator for $E[\\varepsilon_i^2] = \\sigma_i^2$ under standard conditions.\n\n**Step 2: Auxiliary Regression for Variance Estimation**\nThe model for the conditional variance implies a relationship for the expected squared error:\n$$E[\\varepsilon_i^2 | x_i] = \\operatorname{Var}(\\varepsilon_i | x_i) = \\sigma^2 + (\\sigma^2 \\theta) z_i$$\nThis suggests a linear regression model for the squared errors. We define $\\alpha = \\sigma^2$ and $\\gamma = \\sigma^2 \\theta$. The model becomes $E[\\varepsilon_i^2 | x_i] = \\alpha + \\gamma z_i$. We can estimate $\\alpha$ and $\\gamma$ by regressing the squared OLS residuals $u_i^2$ on a constant and $z_i$. Let $Z$ be the design matrix for this auxiliary regression, $Z = [\\mathbf{1} \\mid z]$, where $\\mathbf{1}$ is a column of ones. The parameter vector is $\\delta=[\\alpha, \\gamma]^\\top$. We estimate $\\delta$ via OLS:\n$$\\hat\\delta = [\\hat\\alpha, \\hat\\gamma]^\\top = (Z^\\top Z)^{-1} Z^\\top u^2$$\nwhere $u^2$ is the vector of squared residuals $(u_1^2, \\dots, u_n^2)^\\top$.\n\n**Step 3: FWLS Estimation with Estimated Weights**\nWith estimates $\\hat\\alpha$ and $\\hat\\gamma$, we can form an estimate of each error variance:\n$$\\hat\\sigma_i^2 = \\hat\\alpha + \\hat\\gamma z_i$$\nSince variance must be positive, and estimates $\\hat\\sigma_i^2$ are not guaranteed to be so, we impose a floor $\\varepsilon_{\\min} = 10^{-8}$. The estimated variances for weighting are:\n$$\\hat v_i = \\max(\\varepsilon_{\\min}, \\hat\\alpha + \\hat\\gamma z_i)$$\nThe weights for the FWLS regression are the reciprocals of these estimated variances:\n$$w_i = \\frac{1}{\\hat v_i}$$\nWith the weight matrix $W = \\operatorname{diag}(w_1, \\dots, w_n)$, the FWLS estimator is computed as:\n$$\\hat\\beta_{\\mathrm{FWLS}} = (X^\\top W X)^{-1} X^\\top W y$$\n\n### 3. Summary of Quantities to Be Computed\n\nFor each dataset, the following quantities are calculated based on the procedure above:\n1.  **Estimate of $\\theta$**: From the definitions $\\alpha = \\sigma^2$ and $\\gamma = \\sigma^2 \\theta$, it follows that $\\theta = \\gamma / \\alpha$. We estimate this parameter as $\\hat\\theta = \\hat\\gamma / \\hat\\alpha$. To avoid division by zero or a negative number, the estimate is defined as $\\hat\\theta = \\hat\\gamma / \\hat\\alpha$ if $\\hat\\alpha > 0$, and $\\hat\\theta = 0$ otherwise.\n2.  **FWLS Intercept Coefficient**: This is the first element of the estimated parameter vector $\\hat\\beta_{\\mathrm{FWLS}}$, denoted as $\\hat\\beta_{\\mathrm{FWLS},0}$.\n\nThe implementation will follow these derived steps precisely for each provided dataset.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the feasible weighted least squares problem for three given datasets.\n    \"\"\"\n    \n    # Define the small positive floor for variance estimates\n    E_MIN = 1e-8\n\n    test_cases = [\n        {\n            \"X_rows\": [\n                [1, -2, 1], [1, -1, 0], [1, 0, -1], [1, 1, 2], [1, 2, -2], [1, 3, 1]\n            ],\n            \"beta_true\": [1, 2, -1],\n            \"sigma\": 1.0,\n            \"theta_true\": 0.0,\n            \"e\": [0.1, -0.2, 0.05, -0.1, 0.0, 0.15],\n        },\n        {\n            \"X_rows\": [\n                [1, -1, -2], [1, -0.5, 1], [1, 0.5, 1.5], [1, 1, -1],\n                [1, 1.5, 2], [1, -1.5, -1], [1, 2, 2.5], [1, -2, -2]\n            ],\n            \"beta_true\": [0.5, -1.0, 2.0],\n            \"sigma\": 0.7,\n            \"theta_true\": 0.1,\n            \"e\": [0.5, -0.8, 0.3, -1.2, 1.5, -0.4, 0.9, -0.7],\n        },\n        {\n            \"X_rows\": [\n                [1, -2, 3], [1, -1, -2], [1, 0, 1], [1, 1, -1],\n                [1, 2, 2], [1, -3, -1], [1, 3, 1]\n            ],\n            \"beta_true\": [2.0, 0.5, 1.5],\n            \"sigma\": 0.9,\n            \"theta_true\": -0.15,\n            \"e\": [1.0, -0.5, 0.2, -0.8, 1.1, -1.3, 0.6],\n        },\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        # 1. Setup data for the current test case\n        X = np.array(case[\"X_rows\"], dtype=float)\n        beta_true = np.array(case[\"beta_true\"], dtype=float)\n        sigma = case[\"sigma\"]\n        theta_true = case[\"theta_true\"]\n        e = np.array(case[\"e\"], dtype=float)\n        n, p = X.shape\n\n        # Define the interaction term z_i = x_{i1} * x_{i2}\n        z = X[:, 1] * X[:, 2]\n\n        # Generate the response variable y\n        variance_term = np.sqrt(1 + theta_true * z)\n        y = X @ beta_true + sigma * variance_term * e\n\n        # 2. Step 1: OLS regression to obtain residuals\n        # Solves normal equations (X'X)b = X'y\n        beta_ols = np.linalg.solve(X.T @ X, X.T @ y)\n        u = y - X @ beta_ols\n\n        # 3. Step 2: Auxiliary variance regression\n        u_sq = u ** 2\n        # Design matrix for variance regression: Z = [1, z]\n        Z = np.c_[np.ones(n), z]\n        # OLS on u^2 = alpha + gamma*z\n        # Solves (Z'Z)d = Z'u^2 where d = [alpha, gamma]'\n        delta_hat = np.linalg.solve(Z.T @ Z, Z.T @ u_sq)\n        alpha_hat, gamma_hat = delta_hat[0], delta_hat[1]\n        \n        # Calculate theta_hat = gamma_hat / alpha_hat\n        theta_hat = gamma_hat / alpha_hat if alpha_hat > 0 else 0.0\n\n        # 4. Step 3: Feasible Weighted Least Squares (FWLS)\n        # Estimate variances: v_hat = alpha_hat + gamma_hat * z\n        v_hat = alpha_hat + gamma_hat * z\n        # Ensure positivity of estimated variances\n        v_hat_floored = np.maximum(E_MIN, v_hat)\n        \n        # Construct weights w_i = 1 / v_hat_i\n        w = 1.0 / v_hat_floored\n\n        # Perform WLS using the estimated weights\n        # Solves (X'WX)b = X'Wy\n        # This is implemented efficiently without forming the diagonal matrix W.\n        # w[:, np.newaxis] * X scales each row of X by the corresponding weight.\n        X_w = w[:, np.newaxis] * X\n        XtWX = X.T @ X_w\n        XtWy = X.T @ (w * y)\n        beta_fwls = np.linalg.solve(XtWX, XtWy)\n        \n        # Extract the intercept term\n        beta0_fwls = beta_fwls[0]\n\n        # Store results for this case\n        all_results.append(theta_hat)\n        all_results.append(beta0_fwls)\n\n    # Format final output as a single string\n    formatted_results = [f\"{r:.6f}\" for r in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\n\nsolve()\n```", "id": "3127977"}, {"introduction": "Weighted Least Squares is a direct approach to handling heteroscedasticity, but it is not the only one. Another common strategy is to apply a variance-stabilizing transformation to the response variable, such as the logarithm, which often makes the error variance more constant. This practice sets up a direct comparison between WLS and the log-transformation approach for a dataset with multiplicative noise. By implementing both methods and evaluating their residual diagnostics and predictive accuracy, you will develop a more nuanced understanding of the trade-offs involved and learn to critically choose the right tool for the job. [@problem_id:3128007]", "problem": "You are given a comparison task between two regression strategies in the presence of heteroscedasticity induced by multiplicative noise. The foundational setting is a univariate linear predictor with a strictly positive response generated by a multiplicative noise model. The base facts to use are the definitions of linear regression and its sum-of-squares objective, the notion of heteroscedasticity as non-constant error variance across observations, and the principle that Weighted Least Squares (WLS) minimizes a weighted sum of squared residuals to stabilize variance when it depends on the magnitude of the response. You must implement both strategies, evaluate residual diagnostics, and assess predictive accuracy.\n\nData-generating process for each test case:\n- For each observation $i \\in \\{1, \\dots, n\\}$, draw predictor $x_i$ independently and identically distributed from the uniform distribution on the interval $[0, L]$.\n- Draw noise $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$ independently across $i$.\n- Generate the response as $y_i = \\exp(\\beta_0 + \\beta_1 x_i + \\varepsilon_i)$, which induces multiplicative noise on the original scale and additive noise on the log scale.\n\nModeling strategies to implement:\n1. Log-transform model (Ordinary Least Squares (OLS) on the log scale):\n   - Fit the linear model $\\log y_i = \\gamma_0 + \\gamma_1 x_i + e_i$ on the training split by minimizing the unweighted sum of squares on the log scale.\n   - For predictions on the original response scale, use Duan's smearing estimate: for training residuals $r_j = \\log y_j - (\\hat{\\gamma}_0 + \\hat{\\gamma}_1 x_j)$, define the smearing factor $S = \\frac{1}{n_{\\text{train}}}\\sum_{j=1}^{n_{\\text{train}}} \\exp(r_j)$. Predict on the original scale via $\\hat{y}_i^{\\text{log}} = \\exp(\\hat{\\gamma}_0 + \\hat{\\gamma}_1 x_i) \\cdot S$.\n\n2. Weighted Least Squares (WLS) on the original scale:\n   - Fit the linear model $y_i = \\beta_0 + \\beta_1 x_i + \\eta_i$ on the training split by minimizing a weighted sum of squares with weights $w_i = 1 / y_i^2$.\n\nResidual diagnostics:\n- For each fitted model, compute the training residuals and the fitted values on the modelâ€™s working scale.\n- Define the heteroscedasticity index $H$ as the absolute Pearson correlation between the absolute residuals and the fitted values on the same scale, i.e., $H = \\left|\\operatorname{corr}\\left(|r_i|, \\hat{m}_i\\right)\\right|$, where $r_i$ are residuals and $\\hat{m}_i$ are fitted values. A smaller $H$ indicates better residual diagnostics.\n\nPredictive accuracy:\n- Use the Root Mean Squared Error (RMSE) on the original response scale computed on the test split, i.e., $\\operatorname{RMSE} = \\sqrt{\\frac{1}{n_{\\text{test}}}\\sum_{i=1}^{n_{\\text{test}}} (y_i - \\hat{y}_i)^2}$.\n- For the log-transform model, use the smearing back-transformation defined above; for the WLS model, use its fitted predictions directly on the original scale.\n\nTrain/test split:\n- Randomly split the data into training and test sets with a training proportion $p_{\\text{train}}$ for each test case. Use the provided random seed to ensure reproducibility.\n\nComparison rule and final outputs:\n- For residual diagnostics, return an integer $a$ per test case where $a=0$ if the log-transform model has a strictly smaller $H$ than WLS, $a=1$ if WLS has a strictly smaller $H$ than the log-transform model, and $a=2$ if they are tied within a tolerance $\\tau$.\n- For predictive accuracy, return an integer $b$ per test case where $b=0$ if the log-transform model has a strictly smaller $\\operatorname{RMSE}$ than WLS, $b=1$ if WLS has a strictly smaller $\\operatorname{RMSE}$ than the log-transform model, and $b=2$ if they are tied within the same tolerance $\\tau$.\n- Use the tolerance $\\tau = 10^{-8}$.\n\nTest suite parameters:\n- Case $1$: $n=200$, $\\beta_0=1.0$, $\\beta_1=0.8$, $\\sigma=0.5$, $L=3.0$, $p_{\\text{train}}=0.7$, seed $=42$.\n- Case $2$: $n=200$, $\\beta_0=1.0$, $\\beta_1=0.8$, $\\sigma=0.05$, $L=3.0$, $p_{\\text{train}}=0.7$, seed $=202$.\n- Case $3$: $n=60$, $\\beta_0=0.5$, $\\beta_1=1.2$, $\\sigma=0.6$, $L=2.0$, $p_{\\text{train}}=0.7$, seed $=7$.\n- Case $4$: $n=400$, $\\beta_0=0.2$, $\\beta_1=0.9$, $\\sigma=0.9$, $L=4.0$, $p_{\\text{train}}=0.7$, seed $=99$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each test case contributing a two-integer list $[a,b]$ in the same order as the test suite. For example, a valid output format is $[[a_1,b_1],[a_2,b_2],[a_3,b_3],[a_4,b_4]]$. No physical units or angles are involved in this problem, and there are no percentage outputs; all computed quantities are dimensionless.", "solution": "The problem is valid. It is scientifically grounded in established principles of statistical regression modeling, specifically addressing heteroscedasticity. The problem is well-posed, providing a complete and consistent set of instructions, data generation procedures, and evaluation metrics, ensuring a unique and verifiable solution for each test case. The language is objective and formal.\n\nThe solution methodology involves comparing two regression strategies for a dataset exhibiting heteroscedasticity arising from a multiplicative error structure. For each test case, we first generate the data, then split it into training and testing sets. We then apply two distinct modeling approaches: Ordinary Least Squares (OLS) on log-transformed data and Weighted Least Squares (WLS) on the original data. Finally, we evaluate and compare the models based on a residual diagnostic (heteroscedasticity index) and predictive accuracy (Root Mean Squared Error).\n\n**1. Data Generation and Splitting**\n\nFor each of the $n$ observations, the predictor variable $x_i$ is drawn from a uniform distribution:\n$$x_i \\sim \\mathcal{U}(0, L)$$\nThe response variable $y_i$ is generated from a log-normal model, where the noise is additive on the logarithmic scale:\n$$y_i = \\exp(\\beta_0 + \\beta_1 x_i + \\varepsilon_i), \\quad \\text{where } \\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$$\nThis process implies that on the original scale, the error is multiplicative and the variance of $y_i$ is not constant across different values of $x_i$, a condition known as heteroscedasticity. Specifically, $\\operatorname{Var}(y_i|x_i)$ is proportional to $(\\mathbb{E}[y_i|x_i])^2$.\n\nThe generated dataset of size $n$ is then partitioned into a training set and a test set, according to the proportion $p_{\\text{train}}$. A fixed random seed ensures the reproducibility of this split. Let $n_{\\text{train}} = \\lfloor n \\cdot p_{\\text{train}} \\rfloor$ and $n_{\\text{test}} = n - n_{\\text{train}}$.\n\n**2. Strategy 1: Log-Transform Ordinary Least Squares (OLS)**\n\nThis approach first linearizes the relationship and stabilizes the variance by applying a logarithmic transformation to the response variable. The model fitted to the training data is:\n$$\\log y_i = \\gamma_0 + \\gamma_1 x_i + e_i$$\nThis is a standard linear model, whose parameters $\\boldsymbol{\\gamma} = [\\gamma_0, \\gamma_1]^T$ are estimated using OLS by minimizing the sum of squared errors, $\\sum e_i^2$. The solution is given by the normal equations:\n$$\\hat{\\boldsymbol{\\gamma}} = (\\mathbf{X}_{\\text{train}}^T \\mathbf{X}_{\\text{train}})^{-1} \\mathbf{X}_{\\text{train}}^T \\mathbf{z}_{\\text{train}}$$\nwhere $\\mathbf{z}_{\\text{train}}$ is the vector of log-transformed training responses, $(\\log y_1, \\dots, \\log y_{n_{\\text{train}}})^T$, and $\\mathbf{X}_{\\text{train}}$ is the $n_{\\text{train}} \\times 2$ design matrix with a column of ones and a column of $x_i$ values.\n\nFor residual diagnostics, we use the training data. The fitted values are $\\hat{\\mathbf{z}}_{\\text{train}} = \\mathbf{X}_{\\text{train}} \\hat{\\boldsymbol{\\gamma}}$, and the residuals are $\\mathbf{r}_{\\text{log}} = \\mathbf{z}_{\\text{train}} - \\hat{\\mathbf{z}}_{\\text{train}}$. The heteroscedasticity index is calculated as:\n$$H_{\\text{log}} = \\left|\\operatorname{corr}\\left(|\\mathbf{r}_{\\text{log}}|, \\hat{\\mathbf{z}}_{\\text{train}}\\right)\\right|$$\n\nFor prediction, we must back-transform the predictions from the log scale to the original scale. A naive back-transformation $\\exp(\\hat{\\gamma}_0 + \\hat{\\gamma}_1 x_i)$ would be biased. We use Duan's smearing estimate to correct for this. The smearing factor $S$ is computed from the training residuals:\n$$S = \\frac{1}{n_{\\text{train}}}\\sum_{j=1}^{n_{\\text{train}}} \\exp(r_{\\text{log}, j})$$\nPredictions for the test set on the original scale are then:\n$$\\hat{y}_i^{\\text{log}} = \\exp(\\hat{\\gamma}_0 + \\hat{\\gamma}_1 x_{\\text{test}, i}) \\cdot S$$\nThe predictive accuracy is measured by the Root Mean Squared Error (RMSE) on the test set:\n$$\\operatorname{RMSE}_{\\text{log}} = \\sqrt{\\frac{1}{n_{\\text{test}}}\\sum_{i=1}^{n_{\\text{test}}} (y_{\\text{test}, i} - \\hat{y}_i^{\\text{log}})^2}$$\n\n**3. Strategy 2: Weighted Least Squares (WLS)**\n\nThis strategy models the linear relationship directly on the original scale, $y_i = \\beta_0 + \\beta_1 x_i + \\eta_i$, but accounts for heteroscedasticity by assigning weights to each observation that are inversely proportional to its variance. Given that $\\operatorname{Var}(y_i|x_i)$ is approximately proportional to $y_i^2$, we use the weights $w_i = 1/y_i^2$. WLS minimizes the weighted sum of squared residuals, $\\sum w_i \\eta_i^2$. The parameter estimate $\\hat{\\boldsymbol{\\beta}} = [\\hat{\\beta}_0, \\hat{\\beta}_1]^T$ is found by solving:\n$$\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}_{\\text{train}}^T \\mathbf{W} \\mathbf{X}_{\\text{train}})^{-1} \\mathbf{X}_{\\text{train}}^T \\mathbf{W} \\mathbf{y}_{\\text{train}}$$\nwhere $\\mathbf{W}$ is a diagonal matrix with the weights $w_i = 1/y_{\\text{train}, i}^2$ on its diagonal.\n\nThe residual diagnostics are performed on the original scale. The fitted values are $\\hat{\\mathbf{y}}_{\\text{wls, train}} = \\mathbf{X}_{\\text{train}} \\hat{\\boldsymbol{\\beta}}$, and the residuals are $\\mathbf{r}_{\\text{wls}} = \\mathbf{y}_{\\text{train}} - \\hat{\\mathbf{y}}_{\\text{wls, train}}$. The heteroscedasticity index is:\n$$H_{\\text{wls}} = \\left|\\operatorname{corr}\\left(|\\mathbf{r}_{\\text{wls}}|, \\hat{\\mathbf{y}}_{\\text{wls, train}}\\right)\\right|$$\n\nPredictions for the test set are made directly using the fitted model:\n$$\\hat{y}_i^{\\text{wls}} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_{\\text{test}, i}$$\nThe corresponding RMSE on the test set is:\n$$\\operatorname{RMSE}_{\\text{wls}} = \\sqrt{\\frac{1}{n_{\\text{test}}}\\sum_{i=1}^{n_{\\text{test}}} (y_{\\text{test}, i} - \\hat{y}_i^{\\text{wls}})^2}$$\n\n**4. Comparison**\n\nFor each test case, the two models are compared on two criteria using a tolerance $\\tau = 10^{-8}$.\n- **Residual Diagnostics (output $a$)**:\n  - If $H_{\\text{log}} < H_{\\text{wls}} - \\tau$, $a=0$.\n  - If $H_{\\text{wls}} < H_{\\text{log}} - \\tau$, $a=1$.\n  - Otherwise, $a=2$.\n- **Predictive Accuracy (output $b$)**:\n  - If $\\operatorname{RMSE}_{\\text{log}} < \\operatorname{RMSE}_{\\text{wls}} - \\tau$, $b=0$.\n  - If $\\operatorname{RMSE}_{\\text{wls}} < \\operatorname{RMSE}_{\\text{log}} - \\tau$, $b=1$.\n  - Otherwise, $b=2$.\n\nThe final output is a list of $[a, b]$ pairs for all specified test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the comparison for all test cases.\n    \"\"\"\n    \n    test_cases = [\n        # Case 1: n=200, beta0=1.0, beta1=0.8, sigma=0.5, L=3.0, p_train=0.7, seed=42\n        (200, 1.0, 0.8, 0.5, 3.0, 0.7, 42),\n        # Case 2: n=200, beta0=1.0, beta1=0.8, sigma=0.05, L=3.0, p_train=0.7, seed=202\n        (200, 1.0, 0.8, 0.05, 3.0, 0.7, 202),\n        # Case 3: n=60, beta0=0.5, beta1=1.2, sigma=0.6, L=2.0, p_train=0.7, seed=7\n        (60, 0.5, 1.2, 0.6, 2.0, 0.7, 7),\n        # Case 4: n=400, beta0=0.2, beta1=0.9, sigma=0.9, L=4.0, p_train=0.7, seed=99\n        (400, 0.2, 0.9, 0.9, 4.0, 0.7, 99),\n    ]\n\n    results = []\n    for case in test_cases:\n        result = _solve_case(*case)\n        results.append(result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef _solve_case(n, beta_0, beta_1, sigma, L, p_train, seed, tau=1e-8):\n    \"\"\"\n    Solves a single test case for the heteroscedasticity problem.\n    \"\"\"\n    # 1. Generate Data\n    np.random.seed(seed)\n    x = np.random.uniform(0, L, n)\n    # Note: np.random.normal's scale parameter is standard deviation (sigma), not variance (sigma^2)\n    eps = np.random.normal(0, sigma, n)\n    y = np.exp(beta_0 + beta_1 * x + eps)\n\n    # 2. Split Data\n    n_train = int(np.floor(n * p_train))\n    indices = np.arange(n)\n    np.random.shuffle(indices)\n    train_indices = indices[:n_train]\n    test_indices = indices[n_train:]\n\n    x_train, y_train = x[train_indices], y[train_indices]\n    x_test, y_test = x[test_indices], y[test_indices]\n    \n    # --- Strategy 1: Log-transform model (OLS on log scale) ---\n    log_y_train = np.log(y_train)\n    X_train = np.vstack([np.ones(n_train), x_train]).T\n    \n    # Fit using OLS normal equations\n    gamma_hat = np.linalg.inv(X_train.T @ X_train) @ X_train.T @ log_y_train\n    \n    # Residual diagnostics\n    log_y_fit_train = X_train @ gamma_hat\n    res_log = log_y_train - log_y_fit_train\n    \n    # Correlation for H_log. Handle constant array case for robustness.\n    if np.std(np.abs(res_log))  1e-12 or np.std(log_y_fit_train)  1e-12:\n        corr_log = 0.0\n    else:\n        corr_log = np.corrcoef(np.abs(res_log), log_y_fit_train)[0, 1]\n    H_log = np.abs(corr_log)\n    \n    # Prediction and RMSE with Duan's smearing\n    smearing_factor = np.mean(np.exp(res_log))\n    n_test = len(x_test)\n    X_test = np.vstack([np.ones(n_test), x_test]).T\n    log_y_pred_test = X_test @ gamma_hat\n    y_pred_log = np.exp(log_y_pred_test) * smearing_factor\n    RMSE_log = np.sqrt(np.mean((y_test - y_pred_log)**2))\n\n    # --- Strategy 2: Weighted Least Squares (WLS) on original scale ---\n    weights = 1 / y_train**2\n    \n    # Fit using WLS normal equations\n    X_train_T_W = X_train.T * weights  # Broadcasting for efficiency\n    beta_hat = np.linalg.inv(X_train_T_W @ X_train) @ (X_train_T_W @ y_train)\n\n    # Residual diagnostics\n    y_fit_wls_train = X_train @ beta_hat\n    res_wls = y_train - y_fit_wls_train\n\n    # Correlation for H_wls. Handle constant array case for robustness.\n    if np.std(np.abs(res_wls))  1e-12 or np.std(y_fit_wls_train)  1e-12:\n        corr_wls = 0.0\n    else:\n        corr_wls = np.corrcoef(np.abs(res_wls), y_fit_wls_train)[0, 1]\n    H_wls = np.abs(corr_wls)\n\n    # Prediction and RMSE\n    y_pred_wls = X_test @ beta_hat\n    RMSE_wls = np.sqrt(np.mean((y_test - y_pred_wls)**2))\n\n    # --- Comparison ---\n    # a: Residual diagnostics comparison\n    if H_log  H_wls - tau:\n        a = 0\n    elif H_wls  H_log - tau:\n        a = 1\n    else:\n        a = 2\n        \n    # b: Predictive accuracy comparison\n    if RMSE_log  RMSE_wls - tau:\n        b = 0\n    elif RMSE_wls  RMSE_log - tau:\n        b = 1\n    else:\n        b = 2\n\n    return [a, b]\n\nsolve()\n```", "id": "3128007"}]}