{"hands_on_practices": [{"introduction": "In regression diagnostics, not all \"unusual\" points are created equal. An observation with a large residual is not necessarily influential, and a point with high leverage might fit the model perfectly. This practice clarifies the specific meaning of \"influence\" by challenging you to compare Cook's distance, $D_i$, with other metrics like the leave-one-out prediction error, or PRESS residual [@problem_id:3111497]. By working through this exercise, you will develop a precise understanding of what different diagnostics reveal and why $D_i$, which measures a point's overall impact on the estimated coefficients, provides a unique and powerful perspective.", "problem": "Consider a univariate ordinary least squares (OLS) linear regression with an intercept, where the design matrix is $X \\in \\mathbb{R}^{n \\times p}$ with $p = 2$ columns corresponding to the intercept and a single predictor. Let the predictor values be $x_i \\in \\mathbb{R}$ for $i = 0, 1, \\dots, n-1$, and responses $y_i \\in \\mathbb{R}$. The model is $y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$, and the OLS estimator $\\hat{\\beta}$ is defined by the normal equations. Define the hat matrix $H$ as the orthogonal projector onto the column space of $X$, residuals $r = y - X\\hat{\\beta}$, and the mean squared error $\\mathrm{MSE}$ as the residual sum of squares divided by the degrees of freedom $n - p$. Cook's distance $D_i$ measures the overall impact of deleting observation $i$ on the estimated regression, while the Predicted Residual Error Sum of Squares (PRESS) residual for observation $i$, denoted $\\mathrm{PRESS}_i$, is the leave-one-out residual for observation $i$.\n\nYour task is to:\n- Start from the OLS normal equations and the definition of the hat matrix $H$, and derive expressions that let you compute, for each observation $i$, the Cook's distance $D_i$ and the magnitude of the leave-one-out residual $\\lvert \\mathrm{PRESS}_i \\rvert$ without refitting the regression $n$ times.\n- Define the predictive harm at a specific target point $x^\\ast$ for observation deletion $i$ as the absolute change in the predicted value at $x^\\ast$ between the full-data fit and the fit with observation $i$ deleted, that is $h_i(x^\\ast) = \\lvert \\hat{y}(x^\\ast) - \\hat{y}^{(-i)}(x^\\ast) \\rvert$.\n- Using these definitions, construct a data example and demonstrate that the ranking of observations by Cook's distance $D_i$ differs from the ranking suggested by the magnitudes $\\lvert \\mathrm{PRESS}_i \\rvert$ when evaluated for predictive harm at a specific $x^\\ast$. In other words, show that the observation flagged as most harmful by $D_i$ is not the same as the one whose deletion most changes the prediction at $x^\\ast$, and that this can also differ from the observation with the largest $\\lvert \\mathrm{PRESS}_i \\rvert$.\n\nUse the following scientifically consistent, self-contained dataset (constructed so that the OLS fit equals a simple line and residuals are orthogonal to the column space of $X$):\n- Number of observations $n = 5$.\n- Predictor values $x = [-2, -1, 0, 1, 2]$.\n- Choose a target line $y = 1 + x$ and residuals $r = [1, -2.5, 3, -2.5, 1]$ that satisfy $X^\\top r = 0$; define $y_i = 1 + x_i + r_i$ for $i = 0, 1, 2, 3, 4$.\n\nTest suite (three target points):\n- Case $1$: $x^\\ast = 2.5$.\n- Case $2$: $x^\\ast = 0.0$.\n- Case $3$: $x^\\ast = 10.0$.\n\nImplementation requirements:\n- Work from first principles: use the OLS normal equations, the definition of the hat matrix, and linear algebra identities to obtain computational formulas; do not refit the model $n$ times.\n- For each test case, compute:\n    1. The index (using zero-based indexing) of the observation with the largest Cook's distance $D_i$.\n    2. The index (zero-based) of the observation with the largest $\\lvert \\mathrm{PRESS}_i \\rvert$.\n    3. The index (zero-based) of the observation with the largest predictive harm $h_i(x^\\ast)$.\n    4. A boolean indicating whether the largest Cook's distance corresponds to the largest predictive harm at $x^\\ast$.\n    5. A boolean indicating whether the largest $\\lvert \\mathrm{PRESS}_i \\rvert$ corresponds to the largest predictive harm at $x^\\ast$.\n- In the presence of ties, choose the smallest index.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for the three test cases as a comma-separated list enclosed in square brackets. Each test case’s result must be a list of the form $[\\text{argmaxD}, \\text{argmaxPRESS}, \\text{argmaxHarm}, \\text{CookEqualsHarm}, \\text{PRESSEqualsHarm}]$, aggregated as $[[\\dots],[\\dots],[\\dots]]$ on a single line. All indices are zero-based integers and the booleans must be either $\\text{True}$ or $\\text{False}$.", "solution": "The problem requires the derivation of computational formulas for several influence diagnostics in ordinary least squares (OLS) regression and an application to a specific dataset to demonstrate that different diagnostics can rank the influence of observations differently.\n\n### **1. Theoretical Derivations**\n\nLet the OLS model be $y = X\\beta + \\varepsilon$, where $y \\in \\mathbb{R}^n$, $X \\in \\mathbb{R}^{n \\times p}$, $\\beta \\in \\mathbb{R}^p$, and $\\varepsilon$ is a vector of errors. The OLS estimate of $\\beta$ is given by the normal equations $X^\\top X \\hat{\\beta} = X^\\top y$, which yields $\\hat{\\beta} = (X^\\top X)^{-1} X^\\top y$. The predicted values are $\\hat{y} = X\\hat{\\beta} = Hy$, where $H = X(X^\\top X)^{-1}X^\\top$ is the hat matrix. The diagonal elements of $H$, denoted $h_{ii}$, are the leverages of each observation. The residuals are $r = y - \\hat{y} = (I-H)y$.\n\nLet $\\hat{\\beta}^{(-i)}$ denote the OLS estimate of $\\beta$ when observation $i$ (with corresponding data row $x_i^\\top$ and response $y_i$) is removed from the dataset. Our goal is to compute diagnostics related to this leave-one-out (LOO) model without explicitly re-fitting the regression $n$ times.\n\nThe core result is an expression for the change in the coefficient vector, $\\hat{\\beta} - \\hat{\\beta}^{(-i)}$. This can be efficiently derived using the Sherman-Morrison formula for the inverse of a rank-$1$ updated matrix. The matrix $(X^{(-i)})^\\top X^{(-i)}$ can be written as a rank-$1$ update of $X^\\top X$:\n$$ (X^{(-i)})^\\top X^{(-i)} = \\sum_{j \\neq i} x_j x_j^\\top = X^\\top X - x_i x_i^\\top $$\nApplying the Sherman-Morrison formula, $(A - uv^\\top)^{-1} = A^{-1} + \\frac{A^{-1}uv^\\top A^{-1}}{1 - v^\\top A^{-1}u}$, with $A=X^\\top X$ and $u=v=x_i$, gives:\n$$ ((X^{(-i)})^\\top X^{(-i)})^{-1} = (X^\\top X)^{-1} + \\frac{(X^\\top X)^{-1} x_i x_i^\\top (X^\\top X)^{-1}}{1 - x_i^\\top (X^\\top X)^{-1} x_i} = (X^\\top X)^{-1} + \\frac{(X^\\top X)^{-1} x_i x_i^\\top (X^\\top X)^{-1}}{1 - h_{ii}} $$\nThe LOO estimate is $\\hat{\\beta}^{(-i)} = ((X^{(-i)})^\\top X^{(-i)})^{-1} (X^{(-i)})^\\top y^{(-i)}$. Using the identity above and that $(X^{(-i)})^\\top y^{(-i)} = X^\\top y - x_i y_i$, after algebraic simplification, we arrive at the well-known result:\n$$ \\hat{\\beta} - \\hat{\\beta}^{(-i)} = \\frac{(X^\\top X)^{-1} x_i (y_i - x_i^\\top \\hat{\\beta})}{1 - h_{ii}} = \\frac{(X^\\top X)^{-1} x_i r_i}{1 - h_{ii}} $$\nwhere $r_i = y_i - \\hat{y}_i$ is the $i$-th ordinary residual. This expression is central to computing all required LOO diagnostics efficiently.\n\n#### **PRESS Residual ($\\mathrm{PRESS}_i$)**\nThe Predicted Residual Error Sum of Squares (PRESS) residual is the error in predicting $y_i$ using a model built without observation $i$.\n$$ \\mathrm{PRESS}_i = y_i - \\hat{y}_i^{(-i)} = y_i - x_i^\\top \\hat{\\beta}^{(-i)} $$\nSubstituting $\\hat{\\beta}^{(-i)} = \\hat{\\beta} - (\\hat{\\beta} - \\hat{\\beta}^{(-i)})$:\n$$ \\mathrm{PRESS}_i = y_i - x_i^\\top \\left( \\hat{\\beta} - \\frac{(X^\\top X)^{-1} x_i r_i}{1 - h_{ii}} \\right) = (y_i - x_i^\\top \\hat{\\beta}) + \\frac{x_i^\\top (X^\\top X)^{-1} x_i r_i}{1 - h_{ii}} $$\n$$ \\mathrm{PRESS}_i = r_i + \\frac{h_{ii} r_i}{1 - h_{ii}} = r_i \\left( 1 + \\frac{h_{ii}}{1 - h_{ii}} \\right) = \\frac{r_i}{1 - h_{ii}} $$\nThis provides a simple formula for the PRESS residual using only the ordinary residual $r_i$ and the leverage $h_{ii}$. We are interested in its magnitude, $\\lvert \\mathrm{PRESS}_i \\rvert$.\n\n#### **Cook's Distance ($D_i$)**\nCook's distance measures the effect of deleting observation $i$ on the vector of fitted values. It is defined as:\n$$ D_i = \\frac{(\\hat{y} - \\hat{y}^{(-i)})^\\top (\\hat{y} - \\hat{y}^{(-i)})}{p \\cdot \\mathrm{MSE}} $$\nwhere $\\hat{y}^{(-i)} = X \\hat{\\beta}^{(-i)}$ and $\\mathrm{MSE} = \\frac{1}{n-p} \\sum_{j=1}^n r_j^2$. The numerator is the squared Euclidean distance between the fitted value vectors.\n$$ \\hat{y} - \\hat{y}^{(-i)} = X(\\hat{\\beta} - \\hat{\\beta}^{(-i)}) = X \\frac{(X^\\top X)^{-1} x_i r_i}{1 - h_{ii}} $$\nThe numerator becomes:\n$$ \\left( X \\frac{(X^\\top X)^{-1} x_i r_i}{1 - h_{ii}} \\right)^\\top \\left( X \\frac{(X^\\top X)^{-1} x_i r_i}{1 - h_{ii}} \\right) = \\frac{r_i^2}{(1 - h_{ii})^2} x_i^\\top (X^\\top X)^{-1} X^\\top X (X^\\top X)^{-1} x_i = \\frac{r_i^2 h_{ii}}{(1 - h_{ii})^2} $$\nSubstituting this back into the definition of $D_i$:\n$$ D_i = \\frac{1}{p \\cdot \\mathrm{MSE}} \\frac{r_i^2 h_{ii}}{(1 - h_{ii})^2} $$\nThis formula allows calculation of $D_i$ from the residual $r_i$, leverage $h_{ii}$, number of predictors $p$, and the mean squared error $\\mathrm{MSE}$.\n\n#### **Predictive Harm ($h_i(x^\\ast)$)**\nThe predictive harm at a target point $x^\\ast$ is the absolute change in the prediction at that point when observation $i$ is deleted. Let $x_{new}^\\top = [1, x^\\ast]$.\n$$ h_i(x^\\ast) = \\lvert \\hat{y}(x^\\ast) - \\hat{y}^{(-i)}(x^\\ast) \\rvert = \\lvert x_{new}^\\top \\hat{\\beta} - x_{new}^\\top \\hat{\\beta}^{(-i)} \\rvert = \\lvert x_{new}^\\top (\\hat{\\beta} - \\hat{\\beta}^{(-i)}) \\rvert $$\nUsing our core result for $\\hat{\\beta} - \\hat{\\beta}^{(-i)}$:\n$$ h_i(x^\\ast) = \\left\\lvert x_{new}^\\top \\frac{(X^\\top X)^{-1} x_i r_i}{1 - h_{ii}} \\right\\rvert = \\frac{\\lvert r_i \\rvert}{1 - h_{ii}} \\lvert x_{new}^\\top (X^\\top X)^{-1} x_i \\rvert = \\lvert \\mathrm{PRESS}_i \\rvert \\cdot \\lvert x_{new}^\\top (X^\\top X)^{-1} x_i \\rvert $$\nThis shows that predictive harm is the magnitude of the PRESS residual, re-weighted by a factor that depends on the target point $x^\\ast$ and the deleted point $x_i$.\n\n### **2. Application to the Dataset**\n\nWe are given $n=5$, $p=2$, predictors $x = [-2, -1, 0, 1, 2]^\\top$, and residuals $r = [1, -2.5, 3, -2.5, 1]^\\top$. The design matrix $X$ is:\n$$ X = \\begin{pmatrix} 1  -2 \\\\ 1  -1 \\\\ 1  0 \\\\ 1  1 \\\\ 1  2 \\end{pmatrix} $$\nThe problem is constructed such that the OLS fit for $y_i = (1+x_i) + r_i$ is exactly $\\hat{y}_i = 1+x_i$. The predictor values are centered, $\\sum_{i=0}^{4} x_i = 0$. This simplifies $X^\\top X$:\n$$ X^\\top X = \\begin{pmatrix} n  \\sum x_i \\\\ \\sum x_i  \\sum x_i^2 \\end{pmatrix} = \\begin{pmatrix} 5  0 \\\\ 0  10 \\end{pmatrix} \\implies (X^\\top X)^{-1} = \\begin{pmatrix} 1/5  0 \\\\ 0  1/10 \\end{pmatrix} $$\nThe leverage for observation $i$ is $h_{ii} = x_i^\\top (X^\\top X)^{-1} x_i = \\begin{pmatrix} 1  x_i \\end{pmatrix} \\begin{pmatrix} 1/5  0 \\\\ 0  1/10 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ x_i \\end{pmatrix} = \\frac{1}{5} + \\frac{x_i^2}{10}$.\nThe leverages for $i=0, \\dots, 4$ are $[0.6, 0.3, 0.2, 0.3, 0.6]$.\n\nThe Residual Sum of Squares is $\\mathrm{RSS} = \\sum r_i^2 = 1^2 + (-2.5)^2 + 3^2 + (-2.5)^2 + 1^2 = 23.5$.\nThe Mean Squared Error is $\\mathrm{MSE} = \\frac{\\mathrm{RSS}}{n-p} = \\frac{23.5}{5-2} = \\frac{23.5}{3}$.\n\nNow we can compute the diagnostics.\n- The factor for Cook's distance is $D_i \\propto r_i^2 \\frac{h_{ii}}{(1-h_{ii})^2}$. The values, proportional to $D_i$, are $[3.75, 3.8265, 2.8125, 3.8265, 3.75]$. The maximum occurs at indices $1$ and $3$. The smallest index is selected: $\\text{argmax}(D_i) = 1$.\n- The PRESS residual magnitudes are $|\\mathrm{PRESS}_i| = \\frac{|r_i|}{1-h_{ii}}$. The values are $[2.5, 3.5714, 3.75, 3.5714, 2.5]$. The maximum occurs at index $2$: $\\text{argmax}(|\\mathrm{PRESS}_i|) = 2$.\n\nThe predictive harm factor is $\\lvert x_{new}^\\top (X^\\top X)^{-1} x_i \\rvert = \\lvert [1, x^\\ast] [1/5, x_i/10]^\\top \\rvert = \\lvert \\frac{1}{5} + \\frac{x^\\ast x_i}{10} \\rvert$.\n\n- **Case 1: $x^\\ast = 2.5$**\n$h_i(2.5) = |\\mathrm{PRESS}_i| \\cdot \\lvert 0.2 + 0.25 x_i \\rvert$. The values are $[0.75, 0.1786, 0.75, 1.6071, 1.75]$.\n$\\text{argmax}(h_i(2.5)) = 4$.\nResults: $[\\text{argmaxD}=1, \\text{argmaxPRESS}=2, \\text{argmaxHarm}=4, \\text{False}, \\text{False}]$.\n\n- **Case 2: $x^\\ast = 0.0$**\n$h_i(0.0) = |\\mathrm{PRESS}_i| \\cdot \\lvert 0.2 + 0 \\rvert = 0.2 |\\mathrm{PRESS}_i|$. The ranking is the same as for $|\\mathrm{PRESS}_i|$.\n$\\text{argmax}(h_i(0.0)) = 2$.\nResults: $[\\text{argmaxD}=1, \\text{argmaxPRESS}=2, \\text{argmaxHarm}=2, \\text{False}, \\text{True}]$.\n\n- **Case 3: $x^\\ast = 10.0$**\n$h_i(10.0) = |\\mathrm{PRESS}_i| \\cdot \\lvert 0.2 + x_i \\rvert$. The values are $[4.5, 2.8571, 0.75, 4.2857, 5.5]$.\n$\\text{argmax}(h_i(10.0)) = 4$.\nResults: $[\\text{argmaxD}=1, \\text{argmaxPRESS}=2, \\text{argmaxHarm}=4, \\text{False}, \\text{False}]$.\n\nThis analysis demonstrates that the concept of \"most influential point\" is context-dependent. Cook's distance averages influence over the entire data space and identifies observation $1$ (and $3$) as most influential due to a combination of a moderately large residual and moderate leverage. The PRESS score, focusing only on self-prediction, identifies observation $2$ due to its maximal residual magnitude. Predictive harm, however, depends on the target point $x^\\ast$. For predictions far from the data's center (large $|x^\\ast|$), high-leverage points (like $0$ and $4$) become dominant, as their removal most affects the regression slope.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the regression influence diagnostics problem.\n\n    The solution proceeds in three main steps:\n    1.  Setup: Define the dataset (x, r), the number of observations (n), and\n        the number of parameters (p). The response y and coefficients are\n        implicitly defined by the problem statement.\n    2.  Pre-computation: Calculate quantities that are constant across all test\n        cases. This includes the (X^T X)^-1 matrix, leverages (h_ii),\n        mean squared error (MSE), Cook's distances (D_i), and PRESS residuals.\n        The indices of the observations with the largest D_i and |PRESS_i|\n        are determined here.\n    3.  Per-case computation: Loop through each target point x_star provided\n        in the test suite. For each case, calculate the predictive harm h_i(x_star)\n        for all observations, find the index of the observation with the\n        largest harm, and compare this index with the pre-computed indices for\n        D_i and |PRESS_i|.\n    \"\"\"\n    # 1. Setup based on the problem statement\n    n = 5  # Number of observations\n    p = 2  # Number of parameters (intercept + 1 predictor)\n    x = np.array([-2.0, -1.0, 0.0, 1.0, 2.0])\n    r = np.array([1.0, -2.5, 3.0, -2.5, 1.0]) # Ordinary residuals\n\n    # 2. Pre-computation of constant quantities\n    \n    # Design matrix X would be np.c_[np.ones(n), x]\n    # Since sum(x) = 0, X^T*X is a diagonal matrix.\n    # X_T_X = [[n, sum(x)], [sum(x), sum(x^2)]] = [[5, 0], [0, 10]]\n    X_T_X_inv = np.array([[1/n, 0], [0, 1/np.sum(x**2)]])\n\n    # Calculate leverages (h_ii). h_ii = x_i^T * (X^T*X)^-1 * x_i\n    # where x_i is the i-th row of X, i.e., [1, x_i].\n    h_ii = np.zeros(n)\n    for i in range(n):\n        x_i_row = np.array([1, x[i]])\n        h_ii[i] = x_i_row @ X_T_X_inv @ x_i_row\n\n    # Calculate Mean Squared Error (MSE)\n    rss = np.sum(r**2)\n    mse = rss / (n - p)\n\n    # Calculate Cook's Distances (D_i)\n    # D_i = r_i^2 * h_ii / (p * MSE * (1 - h_ii)^2)\n    D_i = (r**2 / (p * mse)) * (h_ii / (1 - h_ii)**2)\n    argmax_D = np.argmax(D_i)\n\n    # Calculate magnitudes of PRESS residuals\n    # |PRESS_i| = |r_i / (1 - h_ii)|\n    press_i_mag = np.abs(r / (1 - h_ii))\n    argmax_press = np.argmax(press_i_mag)\n\n    # 3. Per-case computation for each target point x_star\n    test_cases = [2.5, 0.0, 10.0]\n    results = []\n\n    for x_star in test_cases:\n        # Calculate predictive harm h_i(x_star)\n        # h_i(x_star) = |PRESS_i| * |x_new^T * (X^T*X)^-1 * x_i|\n        # where x_new = [1, x_star] and x_i = [1, x[i]]\n        \n        harm_factor = np.zeros(n)\n        x_new_row = np.array([1, x_star])\n        for i in range(n):\n            x_i_row = np.array([1, x[i]])\n            harm_factor[i] = np.abs(x_new_row @ X_T_X_inv @ x_i_row)\n        \n        h_i = press_i_mag * harm_factor\n        argmax_harm = np.argmax(h_i)\n        \n        # Check if the argmax indices match\n        cook_equals_harm = (argmax_D == argmax_harm)\n        press_equals_harm = (argmax_press == argmax_harm)\n        \n        # Append the results for the current case\n        case_result = [\n            int(argmax_D),\n            int(argmax_press),\n            int(argmax_harm),\n            bool(cook_equals_harm),\n            bool(press_equals_harm)\n        ]\n        results.append(case_result)\n\n    # Final print statement in the exact required format.\n    # The string representation of a list of lists is the desired format.\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n```", "id": "3111497"}, {"introduction": "Having established what Cook's distance measures, we now explore how it behaves under a common and challenging modeling condition: multicollinearity. This exercise demonstrates a critical and often counterintuitive phenomenon where adding a nearly redundant predictor can dramatically redistribute influence among data points without significantly changing the model's overall predictions [@problem_id:3111583]. This practice is vital for learning to interpret diagnostics cautiously, as it shows how model instability can make influence measures themselves unstable, complicating the task of identifying genuinely problematic data.", "problem": "You are given the task of implementing a numerical experiment that illustrates how, in linear regression, introducing a nearly redundant predictor due to multicollinearity can substantially redistribute the per-observation Cook's distance while leaving fitted predictions almost unchanged. Your implementation must be fully deterministic and must compute, for each provided test case, a single boolean that is true if and only if the following two conditions simultaneously hold: the maximum absolute change in fitted values is small, and the normalized Cook's distance vectors differ by a non-trivial amount.\n\nStart from the following fundamental base in linear regression. Consider a response vector $y \\in \\mathbb{R}^{n}$ and a full-column-rank design matrix $X \\in \\mathbb{R}^{n \\times p}$. The ordinary least squares (OLS) estimator $\\hat{\\beta} \\in \\mathbb{R}^{p}$ minimizes the residual sum of squares and satisfies the normal equations. The fitted values are $\\hat{y} = X \\hat{\\beta}$ and the residuals are $e = y - \\hat{y}$. The hat matrix is $H = X (X^{\\top} X)^{-1} X^{\\top}$, with diagonal entries $h_{ii}$. The mean squared error is $\\operatorname{MSE} = \\|e\\|_{2}^{2} / (n - p)$. Cook's distance for observation $i$ is a measure of the influence of $i$ on all fitted coefficients and is defined in terms of the change $\\hat{\\beta} - \\hat{\\beta}_{(i)}$, equivalently expressible via $e$ and $h_{ii}$.\n\nYour program must implement from these bases the computation of fitted values and Cook's distances for two models: a base model with predictors $\\{1, x_{1}, x_{2}\\}$ and an extended model with predictors $\\{1, x_{1}, x_{2}, x_{3}\\}$. In the multicollinearity scenario, $x_{3}$ is constructed to be nearly redundant with $x_{1}$. In the non-redundant scenario, $x_{3}$ is independent and carries additional signal in $y$.\n\nFor each test case, do the following:\n- Generate data of size $n$ with a fixed random seed for reproducibility. Draw $x_{1}, x_{2}$ independently from a standard normal distribution. Construct $x_{3}$ according to the case description below. Construct the response $y$ using given coefficients and Gaussian noise. Always include an intercept column of ones in each design matrix.\n- Fit the base and extended models using ordinary least squares to obtain $\\hat{y}_{\\mathrm{base}}$ and $\\hat{y}_{\\mathrm{ext}}$, residuals $e_{\\mathrm{base}}$ and $e_{\\mathrm{ext}}$, and hat diagonals $h_{ii}^{\\mathrm{base}}$ and $h_{ii}^{\\mathrm{ext}}$.\n- Compute Cook's distances for each observation in each model, then normalize each model's Cook's distances by dividing by its own sum to obtain two probability vectors over observations. If the sum is zero, use the all-zeros vector for that model's normalized Cook's distances.\n- Define the maximum absolute change in fitted values as $\\Delta_{\\max} = \\max_{i} |\\hat{y}_{\\mathrm{ext}, i} - \\hat{y}_{\\mathrm{base}, i}|$.\n- Define the redistribution magnitude as the $\\ell_{1}$ distance between the normalized Cook's distance vectors, that is, $L_{1} = \\sum_{i=1}^{n} |d^{\\mathrm{ext}}_{i} - d^{\\mathrm{base}}_{i}|$.\n- Let the small-change tolerance be $\\varepsilon_{\\mathrm{pred}} = 0.05$ (unitless, as responses are abstract). Let the redistribution threshold be $\\delta_{D} = 0.10$ (unitless). The boolean result for a test case is true exactly when $\\Delta_{\\max} \\le \\varepsilon_{\\mathrm{pred}}$ and $L_{1} \\ge \\delta_{D}$.\n\nImplement three test cases that cover a happy path, a stronger multi-collinearity boundary, and a contrasting non-redundant scenario:\n- Test case $1$ (happy path, nearly redundant predictor):\n  - Seed $= 42$, $n = 120$, coefficients $(\\beta_{0}, \\beta_{1}, \\beta_{2}) = (0.5, 2.0, -1.5)$, noise standard deviation $\\sigma = 0.5$.\n  - Construct $x_{3} = x_{1} + \\tau \\,\\eta$ with $\\eta \\sim \\mathcal{N}(0, 1)$ independent of $x_{1}$ and $\\tau = 0.02$.\n  - Construct $y = \\beta_{0} + \\beta_{1} x_{1} + \\beta_{2} x_{2} + \\varepsilon$ with $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2})$ and no direct dependence on $x_{3}$.\n- Test case $2$ (boundary, stronger near-redundancy and smaller sample):\n  - Seed $= 7$, $n = 40$, coefficients $(\\beta_{0}, \\beta_{1}, \\beta_{2}) = (1.0, 1.8, 0.7)$, noise standard deviation $\\sigma = 0.3$.\n  - Construct $x_{3} = x_{1} + \\tau \\,\\eta$ with $\\tau = 0.005$.\n  - Construct $y = \\beta_{0} + \\beta_{1} x_{1} + \\beta_{2} x_{2} + \\varepsilon$.\n- Test case $3$ (contrast, non-redundant predictor with additional signal):\n  - Seed $= 99$, $n = 120$, coefficients $(\\beta_{0}, \\beta_{1}, \\beta_{2}, \\beta_{3}) = (0.5, 2.0, -1.5, 1.5)$, noise standard deviation $\\sigma = 0.5$.\n  - Construct $x_{3}$ as an independent standard normal, independent of $x_{1}$ and $x_{2}$.\n  - Construct $y = \\beta_{0} + \\beta_{1} x_{1} + \\beta_{2} x_{2} + \\beta_{3} x_{3} + \\varepsilon$.\n\nYour program must compute the boolean result for each test case as defined above and produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with no spaces, for example, $[r_{1},r_{2},r_{3}]$, where each $r_{k}$ is either true or false.\n\nThe final output of your program must be exactly one line in the format described. No user input is allowed, and no physical units are involved in this problem; all reported quantities are unitless. Angle units and percentages are not applicable here. Your code must be self-contained and reproducible using the provided seeds. The expected outputs are booleans, and the implementation must adhere strictly to the steps described above.", "solution": "The problem requires implementing a numerical experiment to demonstrate how adding a nearly redundant predictor (due to multicollinearity) can significantly alter the distribution of influence, measured by Cook's distance, while having a minimal effect on the model's fitted values. The solution involves defining a procedure to generate data, fit two OLS models (a base model and an extended model with the collinear predictor), and then compute the required metrics to test the specified conditions.\n\n### 1. OLS Fitting and Diagnostic Calculation\nThe core of the solution is a function that can perform Ordinary Least Squares (OLS) regression and compute the necessary statistics. For a given design matrix $X \\in \\mathbb{R}^{n \\times p}$ and response vector $y \\in \\mathbb{R}^{n}$, the OLS estimator $\\hat{\\beta}$ minimizes $\\|y - X\\beta\\|_2^2$. For numerical stability, especially in the presence of multicollinearity, the solution can be found using QR decomposition, where $X = QR$. The estimator $\\hat{\\beta}$ is then found by solving the stable triangular system $R\\hat{\\beta} = Q^{\\top}y$.\n\nFrom this single OLS fit, all necessary quantities are derived:\n- **Fitted values**: $\\hat{y} = X\\hat{\\beta}$.\n- **Residuals**: $e = y - \\hat{y}$.\n- **Leverage values ($h_{ii}$)**: These are the diagonal elements of the hat matrix $H = QQ^{\\top}$. The leverage for observation $i$ is $h_{ii} = \\sum_{j=1}^{p} Q_{ij}^2$.\n- **Mean Squared Error (MSE)**: $\\operatorname{MSE} = \\frac{\\|e\\|_2^2}{n-p} = \\frac{\\sum_{i=1}^n e_i^2}{n-p}$.\n- **Cook's Distance ($D_i$)**: The influence of each observation is calculated using the standard formula:\n$$ D_i = \\frac{e_i^2}{p \\cdot \\operatorname{MSE}} \\cdot \\frac{h_{ii}}{(1 - h_{ii})^2} $$\nHere, $p$ is the number of columns in the design matrix $X$ (including the intercept).\n\n### 2. Data Generation and Model Comparison\nFor each test case, data is generated according to the specified parameters, including a random seed for reproducibility. The predictors $x_1$ and $x_2$ are independent, while $x_3$ is constructed to be nearly collinear with $x_1$ (i.e., $x_3 = x_1 + \\tau\\eta$) in the multicollinearity scenarios, or as an independent predictor in the control case. The response $y$ is generated as a linear combination of the predictors plus Gaussian noise.\n\nTwo models are then fit:\n- **Base Model**: $y$ is regressed on an intercept, $x_1$, and $x_2$. This yields fitted values $\\hat{y}_{\\mathrm{base}}$ and Cook's distances $D^{\\mathrm{base}}$.\n- **Extended Model**: $y$ is regressed on an intercept, $x_1$, $x_2$, and $x_3$. This yields $\\hat{y}_{\\mathrm{ext}}$ and $D^{\\mathrm{ext}}$.\n\n### 3. Metric Computation and Evaluation\nTo compare the two models, two key metrics are computed:\n- **Maximum Absolute Change in Fitted Values**: $\\Delta_{\\max} = \\max_{i} |\\hat{y}_{\\mathrm{ext}, i} - \\hat{y}_{\\mathrm{base}, i}|$. This measures the practical impact of adding $x_3$ on the model's predictions.\n- **Redistribution of Influence**: First, the Cook's distances for each model are normalized to sum to 1, creating two probability vectors $d^{\\mathrm{base}}$ and $d^{\\mathrm{ext}}$. The redistribution is then quantified by the $\\ell_1$ distance between these vectors: $L_{1} = \\sum_{i=1}^{n} |d^{\\mathrm{ext}}_{i} - d^{\\mathrm{base}}_{i}|$. A value of $L_1$ close to 0 indicates a stable influence distribution, while a larger value indicates significant redistribution.\n\nFinally, the boolean condition $(\\Delta_{\\max} \\le 0.05) \\land (L_{1} \\ge 0.10)$ is evaluated. This condition is expected to be true when multicollinearity is introduced (Cases 1 and 2), as the nearly redundant predictor $x_3$ barely changes the fitted values but destabilizes the influence measures. In the control case (Case 3), where $x_3$ is an independent and useful predictor, it should significantly change the fitted values, causing $\\Delta_{\\max}$ to be large and the condition to be false.", "answer": "```python\nimport numpy as np\n\ndef fit_ols_and_get_stats(X, y):\n    \"\"\"\n    Fits an OLS model using QR decomposition for numerical stability and computes\n    fitted values and Cook's distances.\n\n    Args:\n        X (np.ndarray): Design matrix of shape (n, p).\n        y (np.ndarray): Response vector of shape (n,).\n\n    Returns:\n        tuple: A tuple containing:\n            - np.ndarray: Fitted values (y_hat).\n            - np.ndarray: Cook's distances for each observation.\n    \"\"\"\n    n, p = X.shape\n\n    # For robustness against collinearity, use QR decomposition to solve the least squares problem.\n    try:\n        Q, R = np.linalg.qr(X)\n        # Solve R @ beta_hat = Q.T @ y\n        qTy = Q.T @ y\n        beta_hat = np.linalg.solve(R, qTy)\n    except np.linalg.LinAlgError:\n        # Fallback for singular matrix, although problem setup should avoid this.\n        beta_hat = np.linalg.pinv(X) @ y\n        # Recalculate Q for leverage if pinv was used.\n        Q, R = np.linalg.qr(X)\n\n    y_hat = X @ beta_hat\n    residuals = y - y_hat\n    \n    # Leverages (diagonal of hat matrix H = Q @ Q.T) are sums of squares of Q's rows.\n    H_diag = np.sum(Q**2, axis=1)\n\n    # Mean Squared Error\n    if n  p:\n        rss = np.sum(residuals**2)\n        mse = rss / (n - p)\n    else:\n        mse = 0.0\n\n    # Cook's Distances\n    cooks_d = np.zeros(n)\n    if mse  1e-15:  # Avoid division by zero if it's a perfect fit\n        # Formula: D_i = (e_i^2 / (p * MSE)) * (h_ii / (1 - h_ii)^2)\n        term1 = (residuals**2) / (p * mse)\n        denom = (1 - H_diag)**2\n        # Use np.divide to handle cases where denom is zero (i.e., h_ii=1)\n        # to prevent inf/nan values, which would disrupt normalization.\n        term2 = np.divide(H_diag, denom, out=np.zeros_like(H_diag), where=denom  1e-15)\n        cooks_d = term1 * term2\n            \n    return y_hat, cooks_d\n\ndef process_case(seed, n, y_coeffs, sigma, tau, case_type):\n    \"\"\"\n    Runs one full test case: generates data, fits models, computes metrics, and returns the boolean result.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # 1. Generate data\n    x1 = rng.standard_normal(n)\n    x2 = rng.standard_normal(n)\n    \n    if case_type == 'collinear':\n        eta = rng.standard_normal(n)\n        x3 = x1 + tau * eta\n        y = y_coeffs[0] + y_coeffs[1] * x1 + y_coeffs[2] * x2 + rng.normal(0, sigma, n)\n    elif case_type == 'non-redundant':\n        x3 = rng.standard_normal(n)\n        y = y_coeffs[0] + y_coeffs[1] * x1 + y_coeffs[2] * x2 + y_coeffs[3] * x3 + rng.normal(0, sigma, n)\n    \n    intercept = np.ones(n)\n    X_base = np.column_stack([intercept, x1, x2])\n    X_ext = np.column_stack([intercept, x1, x2, x3])\n    \n    # 2. Fit base and extended models\n    y_hat_base, cooks_d_base = fit_ols_and_get_stats(X_base, y)\n    y_hat_ext, cooks_d_ext = fit_ols_and_get_stats(X_ext, y)\n    \n    # 3. Calculate metrics\n    # Max absolute change in fitted values\n    delta_max = np.max(np.abs(y_hat_ext - y_hat_base))\n    \n    # L1 distance between normalized Cook's distances\n    sum_cooks_base = np.sum(cooks_d_base)\n    sum_cooks_ext = np.sum(cooks_d_ext)\n    \n    norm_cooks_base = cooks_d_base / sum_cooks_base if sum_cooks_base  0 else np.zeros_like(cooks_d_base)\n    norm_cooks_ext = cooks_d_ext / sum_cooks_ext if sum_cooks_ext  0 else np.zeros_like(cooks_d_ext)\n    \n    l1_dist = np.sum(np.abs(norm_cooks_ext - norm_cooks_base))\n    \n    # 4. Apply decision rule\n    epsilon_pred = 0.05\n    delta_D = 0.10\n    \n    condition_fit = delta_max = epsilon_pred\n    condition_dist = l1_dist = delta_D\n    \n    return condition_fit and condition_dist\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, run them, and print results.\n    \"\"\"\n    test_cases = [\n        # Case 1: Happy path, nearly redundant predictor\n        {'seed': 42, 'n': 120, 'y_coeffs': (0.5, 2.0, -1.5), 'sigma': 0.5, 'tau': 0.02, 'case_type': 'collinear'},\n        # Case 2: Boundary, stronger near-redundancy and smaller sample\n        {'seed': 7, 'n': 40, 'y_coeffs': (1.0, 1.8, 0.7), 'sigma': 0.3, 'tau': 0.005, 'case_type': 'collinear'},\n        # Case 3: Contrast, non-redundant predictor with additional signal\n        {'seed': 99, 'n': 120, 'y_coeffs': (0.5, 2.0, -1.5, 1.5), 'sigma': 0.5, 'tau': None, 'case_type': 'non-redundant'}\n    ]\n    \n    results = []\n    for case_params in test_cases:\n        result = process_case(**case_params)\n        results.append(str(result).lower())\n        \n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3111583"}, {"introduction": "The final step in our practical journey addresses the crucial question: what should we do after identifying influential points? A tempting but potentially hazardous approach is to simply remove them from the dataset. This advanced exercise guides you through a principled evaluation of this strategy, using a cross-validation framework to measure the trade-off between reducing overfitting and losing valuable information that might be critical for prediction on new data [@problem_id:3111535]. Completing this practice will equip you with a robust methodology for moving beyond simple identification to making data-driven decisions about model refinement.", "problem": "You are asked to design and implement a complete, runnable program that evaluates a cross-validation procedure for ordinary least squares regression which removes influential observations before fitting, where influence is quantified by Cook’s distance. Your program must adhere to the following mathematical setup and produce results for a fixed test suite.\n\nThe fundamental base is ordinary least squares (OLS) linear regression. Given a design matrix $X \\in \\mathbb{R}^{n \\times p}$ and response vector $y \\in \\mathbb{R}^{n}$, an OLS estimator $\\hat{\\beta} \\in \\mathbb{R}^{p}$ minimizes the sum of squared residuals. The hat matrix is $H = X (X^{\\top} X)^{-1} X^{\\top}$, fitted values are $\\hat{y} = H y$, residuals are $e = y - \\hat{y}$, and the diagonal entries $h_{ii}$ of $H$ are called leverages. Cook's distance for observation $i$ is an influence measure that compares the fitted model using all data to the fitted model when observation $i$ is left out. You must derive the computable expression for Cook's distance starting from the OLS normal equations and the definition of the hat matrix, and then use it in your algorithm. Do not assume any pre-given formula for Cook's distance in your design.\n\nYour task is to:\n- Construct a $k$-fold cross-validation loop on a training set. For each fold, within the training portion of the fold only, compute Cook's distance $D_i$ for each training observation. Remove those points with $D_i$ strictly greater than a user-specified threshold $\\tau$ before fitting the model for that fold. Fit on the remaining training observations of the fold, compute the mean squared error (MSE) on the remaining training observations for that fold, and compute the MSE on the held-out validation portion for that fold. Average the validation-minus-training MSE gap across folds to obtain a cross-validation overfitting gap.\n- Separately, fit two models on the full training set (not per-fold): one model using all training observations and one model after removing points whose Cook's distance exceeds the same threshold $\\tau$ as computed on the full training set. Evaluate both models on an external, disjoint test set, and take the difference in test MSEs (cleaned model minus full model) as a measure of information loss on the external test set.\n\nData generation protocol:\n- For each test case, generate the training features $X \\in \\mathbb{R}^{n \\times p}$ with entries independently drawn from a standard normal distribution $\\mathcal{N}(0,1)$.\n- Generate a coefficient vector $\\beta \\in \\mathbb{R}^{p}$ deterministically by setting the $j$-th component to $\\beta_j = (-1)^{j+1} \\cdot \\frac{j}{p}$ for $j \\in \\{1,\\dots,p\\}$.\n- Generate a clean noise vector $\\varepsilon \\in \\mathbb{R}^{n}$ with independent entries $\\varepsilon_i \\sim \\mathcal{N}(0,1)$.\n- To induce influential observations, choose a subset $S$ of the training indices of size $\\lfloor f \\cdot n \\rfloor$ (where $f \\in [0,1]$ is the specified outlier fraction) and modify those rows by multiplying their feature vectors by a leverage scale $a  0$ and adding heavy-tailed noise by replacing $\\varepsilon_i$ with $\\varepsilon_i' \\sim \\mathcal{N}(0,b^2)$ for $i \\in S$, where $b  0$ is the specified outlier noise scale. The response is $y = X \\beta + \\varepsilon$ for non-outlier rows and $y = X \\beta + \\varepsilon'$ for outlier rows after scaling $X$ by $a$.\n- Generate an external test set $(X_{\\text{test}}, y_{\\text{test}})$ of size $n_{\\text{test}}$ in the same way as the clean (non-outlier) data with $\\varepsilon_{\\text{test}} \\sim \\mathcal{N}(0,1)$ and without any outlier modifications. There are no physical units.\n\nAlgorithmic requirements:\n- In each fold, compute influence from first principles using OLS quantities from the training portion, then apply the threshold $\\tau$ to drop high-influence points, refit, and compute fold training MSE and fold validation MSE. If removal yields fewer than $p+1$ training observations in a fold, do not remove any observations in that fold.\n- Compute the cross-validation overfitting gap as the average across folds of $(\\text{validation MSE} - \\text{training MSE})$.\n- Compute the information loss on the external test set as the difference $(\\text{test MSE of cleaned full-training model} - \\text{test MSE of full-training model without removal})$.\n- Also report the fraction of training observations removed on the full training set.\n\nTest suite:\n- Use the following four test cases. Each case specifies a random seed $s$, training size $n$, test size $n_{\\text{test}}$, number of features $p$, number of folds $k$, Cook's threshold $\\tau$, outlier fraction $f$, leverage scale $a$, and outlier noise scale $b$.\n    1. $(s,n,n_{\\text{test}},p,k,\\tau,f,a,b) = (42, 120, 200, 5, 5, +\\infty, 0.1, 6, 8)$.\n    2. $(s,n,n_{\\text{test}},p,k,\\tau,f,a,b) = (42, 120, 200, 5, 5, \\frac{4}{n}, 0.1, 6, 8)$.\n    3. $(s,n,n_{\\text{test}},p,k,\\tau,f,a,b) = (42, 120, 200, 5, 5, \\frac{1}{n}, 0.1, 6, 8)$.\n    4. $(s,n,n_{\\text{test}},p,k,\\tau,f,a,b) = (123, 80, 200, 6, 4, \\frac{1}{n}, 0, 6, 8)$.\n- For each test case, your program must output a list of three floats:\n    - The cross-validation overfitting reduction, defined as the baseline gap with no removal minus the gap with removal at threshold $\\tau$ for that case.\n    - The information loss on the external test set, defined as the test mean squared error of the cleaned full-training model minus the test mean squared error of the full-training model without removal.\n    - The fraction of training points removed when computing Cook's distance on the full training set.\n- The baseline gap is always computed with $\\tau = +\\infty$ (no removal); the comparison gap uses the case-specific $\\tau$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of lists, one list per test case, each inner list ordered as specified above. Round every float to $6$ decimal places. For example, an output with two cases should look like $[[0.123456,0.000001,0.100000],[0.000000,0.000000,0.000000]]$.\n\nAngle units are not applicable. No physical units are involved. All numeric quantities in the output must be dimensionless real numbers rounded to $6$ decimal places.", "solution": "The problem requires the design and implementation of an algorithm to assess the impact of removing influential data points, identified by Cook's distance, on the performance of an ordinary least squares (OLS) linear regression model. The assessment involves a cross-validation procedure to measure the effect on model overfitting and an evaluation on an external test set to measure information loss. The solution proceeds in two stages: first, a principled derivation of the computable formula for Cook's distance, and second, a detailed description of the algorithmic procedure.\n\n### 1. Derivation of Cook's Distance\n\nCook's distance, $D_i$, for an observation $i$ measures the change in the fitted regression coefficients when that observation is removed from the dataset. It is formally defined in terms of the change in the fitted response values.\n\nLet the linear model be $y = X\\beta + \\varepsilon$, where $X \\in \\mathbb{R}^{n \\times p}$ is the design matrix of $n$ observations and $p$ features, $y \\in \\mathbb{R}^{n}$ is the response vector, and $\\beta \\in \\mathbb{R}^{p}$ is the coefficient vector. The OLS estimator, $\\hat{\\beta}$, which minimizes the sum of squared residuals, is given by the normal equations:\n$$ \\hat{\\beta} = (X^\\top X)^{-1} X^\\top y $$\nThe vector of fitted values is $\\hat{y} = X\\hat{\\beta} = X(X^\\top X)^{-1} X^\\top y = Hy$, where $H = X(X^\\top X)^{-1} X^\\top$ is the hat matrix. The diagonal elements of $H$, denoted $h_{ii}$, are the leverages of each observation.\n\nLet $\\hat{\\beta}_{(i)}$ be the OLS estimator computed with the $i$-th observation $(x_i^\\top, y_i)$ removed. Let $X_{(i)}$ and $y_{(i)}$ denote the data matrices with the $i$-th row excluded. Then $\\hat{\\beta}_{(i)} = (X_{(i)}^\\top X_{(i)})^{-1} X_{(i)}^\\top y_{(i)}$. Cook's distance is defined as the scaled squared Euclidean distance between the vectors of fitted values obtained with all data and with observation $i$ removed:\n$$ D_i = \\frac{\\sum_{j=1}^n (\\hat{y}_j - \\hat{y}_{j(i)})^2}{p \\cdot s^2} = \\frac{\\| \\hat{y} - \\hat{y}_{(i)} \\|_2^2}{p \\cdot s^2} $$\nwhere $\\hat{y}_{(i)} = X \\hat{\\beta}_{(i)}$ is the vector of predictions for the original data points using the leave-one-out model, and $s^2 = \\frac{1}{n-p} e^\\top e$ is the mean squared error (unbiased variance estimate) of the full model, with $e = y - \\hat{y}$ being the vector of residuals.\n\nTo find a computable expression for $D_i$, we need to relate $\\hat{\\beta}$ to $\\hat{\\beta}_{(i)}$. A key identity, derived using the Sherman-Morrison-Woodbury formula, connects the two:\n$$ \\hat{\\beta} - \\hat{\\beta}_{(i)} = \\frac{(X^\\top X)^{-1} x_i (y_i - x_i^\\top \\hat{\\beta})}{1 - h_{ii}} = \\frac{(X^\\top X)^{-1} x_i e_i}{1 - h_{ii}} $$\nwhere $e_i$ is the residual for observation $i$ from the full model, and $h_{ii} = x_i^\\top (X^\\top X)^{-1} x_i$ is its leverage.\n\nThe numerator of $D_i$ is $\\| X(\\hat{\\beta} - \\hat{\\beta}_{(i)}) \\|_2^2 = (\\hat{\\beta} - \\hat{\\beta}_{(i)})^\\top X^\\top X (\\hat{\\beta} - \\hat{\\beta}_{(i)})$. Substituting the expression for $\\hat{\\beta} - \\hat{\\beta}_{(i)}$:\n$$ (\\hat{\\beta} - \\hat{\\beta}_{(i)})^\\top X^\\top X (\\hat{\\beta} - \\hat{\\beta}_{(i)}) = \\left( \\frac{(X^\\top X)^{-1} x_i e_i}{1 - h_{ii}} \\right)^\\top X^\\top X \\left( \\frac{(X^\\top X)^{-1} x_i e_i}{1 - h_{ii}} \\right) $$\n$$ = \\frac{e_i^2}{(1 - h_{ii})^2} x_i^\\top (X^\\top X)^{-1} (X^\\top X) (X^\\top X)^{-1} x_i $$\n$$ = \\frac{e_i^2}{(1 - h_{ii})^2} x_i^\\top (X^\\top X)^{-1} x_i = \\frac{e_i^2 h_{ii}}{(1 - h_{ii})^2} $$\nSubstituting this result back into the definition of $D_i$, we obtain the final computable expression:\n$$ D_i = \\frac{e_i^2}{p \\cdot s^2} \\frac{h_{ii}}{(1 - h_{ii})^2} $$\nThis formula allows for the efficient calculation of all $D_i$ from the quantities of a single OLS fit on the full dataset: the residuals $e_i$, leverages $h_{ii}$, the number of features $p$, and the overall mean squared error $s^2$. The leverages $h_{ii}$ can be computed efficiently as the diagonal elements of $QQ^\\top$, where $Q$ is the orthogonal matrix from the QR decomposition of $X$.\n\n### 2. Algorithmic Design\n\nThe implementation follows the problem specification, encompassing data generation, a cross-validation procedure, and a final evaluation on a test set.\n\n**Data Generation:**\nFor each test case, data is generated according to the specified protocol.\n1.  A random seed $s$ ensures reproducibility.\n2.  The training feature matrix $X \\in \\mathbb{R}^{n \\times p}$ is drawn from $\\mathcal{N}(0,1)$.\n3.  The true coefficient vector $\\beta \\in \\mathbb{R}^{p}$ is created deterministically with $\\beta_j = (-1)^{j+1} \\cdot \\frac{j}{p}$.\n4.  A noise vector $\\varepsilon \\in \\mathbb{R}^{n}$ is drawn from $\\mathcal{N}(0,1)$.\n5.  A subset $S$ of $\\lfloor f \\cdot n \\rfloor$ indices is selected to be influential points. For each $i \\in S$, the feature vector $x_i^\\top$ is scaled by $a$ and the noise term $\\varepsilon_i$ is replaced by a draw from a heavy-tailed distribution $\\mathcal{N}(0,b^2)$.\n6.  The final response vector is $y = X\\beta + \\text{noise}$.\n7.  A test set $(X_{\\text{test}}, y_{\\text{test}})$ of size $n_{\\text{test}}$ is generated in the same manner as the \"clean\" (non-influential) training data.\n\n**Cross-Validation Overfitting Gap:**\nThis metric quantifies the degree of overfitting.\n1.  The training data indices are randomly shuffled and split into $k$ equal-sized folds.\n2.  For each fold, the data for that fold is held out as the validation set, and the remaining $k-1$ folds constitute the training set for that iteration.\n3.  Within each iteration, Cook's distances $D_i$ are computed for all observations in the current training set.\n4.  Observations with $D_i  \\tau$ are identified for removal. A crucial condition is checked: if removing these points would leave the training set with fewer than $p+1$ observations, no points are removed for that fold to ensure the OLS problem remains well-defined.\n5.  An OLS model is fitted on the (potentially cleaned) training data to obtain $\\hat{\\beta}_{\\text{fold}}$.\n6.  The mean squared error (MSE) is calculated on this same (cleaned) training set, yielding $\\text{MSE}_{\\text{train,fold}}$.\n7.  The model is then used to predict responses for the held-out validation set, yielding $\\text{MSE}_{\\text{val,fold}}$.\n8.  The overfitting gap for the fold is $\\text{MSE}_{\\text{val,fold}} - \\text{MSE}_{\\text{train,fold}}$.\n9.  This process is repeated for all $k$ folds, and the average of the $k$ gaps is the final cross-validation overfitting gap.\n\n**Output Metrics:**\nFor each test case, three specific metrics are computed:\n1.  **Cross-validation overfitting reduction:** This is the primary metric for evaluating the cross-validation procedure. It's defined as $(\\text{Gap}_{\\tau=\\infty} - \\text{Gap}_{\\tau=\\text{case}})$, where $\\text{Gap}_{\\tau=\\infty}$ is the overfitting gap calculated without removing any points (baseline), and $\\text{Gap}_{\\tau=\\text{case}}$ is the gap calculated using the specific threshold $\\tau$ for that test case. A positive value indicates that removing influential points reduces overfitting.\n2.  **Information loss on the external test set:** This metric assesses the impact of cleaning on generalization performance on unseen data.\n    -   A \"full model\" is trained using all $n$ training observations.\n    -   A \"cleaned model\" is trained after removing observations from the full training set for which $D_i  \\tau$ (subject to the same $p+1$ constraint).\n    -   Both models are evaluated on the external test set. The information loss is the difference in their test MSEs: $\\text{MSE}_{\\text{test,cleaned}} - \\text{MSE}_{\\text{test,full}}$. A positive value suggests that the cleaning process removed information valuable for prediction on clean external data.\n3.  **Fraction of training points removed:** This is simply the number of points removed during the training of the \"cleaned model\" on the full training set, divided by the total number of training points, $n$.\n\nThe implementation will systematically execute these steps for each test case in the provided suite, ensuring all calculations adhere to these principles and rounding the final numerical results to six decimal places as required.", "answer": "```python\nimport numpy as np\n\ndef get_cooks_distances(X, y):\n    \"\"\"\n    Computes Cook's distance for each observation in a linear regression model.\n    \"\"\"\n    n, p = X.shape\n    if n = p:\n        return np.zeros(n)\n\n    try:\n        beta_hat = np.linalg.lstsq(X, y, rcond=None)[0]\n    except np.linalg.LinAlgError:\n        return np.full(n, np.inf)\n\n    residuals = y - X @ beta_hat\n    mse = (residuals @ residuals) / (n - p)\n\n    if mse  1e-12:\n        return np.full(n, np.inf)\n\n    # Efficiently compute leverages h_ii from QR decomposition\n    Q, _ = np.linalg.qr(X)\n    leverages = np.sum(Q**2, axis=1)\n\n    # Clip for numerical stability if h_ii is close to 1\n    leverages = np.clip(leverages, 0, 1 - 1e-8)\n\n    cooks_d = (residuals**2 / (p * mse)) * (leverages / (1 - leverages)**2)\n    return cooks_d\n\ndef calculate_cv_gap(X_full, y_full, k, p_features, tau, rng):\n    \"\"\"\n    Performs k-fold cross-validation and computes the average overfitting gap.\n    \"\"\"\n    n = X_full.shape[0]\n    indices = np.arange(n)\n    rng.shuffle(indices)\n    fold_indices = np.array_split(indices, k)\n    \n    gaps = []\n\n    for i in range(k):\n        val_idx = fold_indices[i]\n        train_idx_list = [fold_indices[j] for j in range(k) if j != i]\n        if not train_idx_list:\n            continue\n        train_idx = np.concatenate(train_idx_list)\n\n        X_train_fold, y_train_fold = X_full[train_idx], y_full[train_idx]\n        X_val_fold, y_val_fold = X_full[val_idx], y_full[val_idx]\n        \n        n_train_fold = X_train_fold.shape[0]\n        \n        X_train_final, y_train_final = X_train_fold, y_train_fold\n        \n        if tau != float('inf'):\n            cooks_d = get_cooks_distances(X_train_fold, y_train_fold)\n            to_remove = np.where(cooks_d  tau)[0]\n\n            if n_train_fold - len(to_remove) = p_features + 1:\n                keep_idx = np.setdiff1d(np.arange(n_train_fold), to_remove)\n                X_train_final = X_train_fold[keep_idx]\n                y_train_final = y_train_fold[keep_idx]\n        \n        if X_train_final.shape[0] = p_features:\n            # Not enough points to fit model, count as max error\n            gaps.append(np.var(y_val_fold))\n            continue\n\n        try:\n            beta_hat = np.linalg.lstsq(X_train_final, y_train_final, rcond=None)[0]\n        except np.linalg.LinAlgError:\n            gaps.append(np.var(y_val_fold))\n            continue\n\n        train_mse = np.mean((y_train_final - X_train_final @ beta_hat)**2)\n        val_mse = np.mean((y_val_fold - X_val_fold @ beta_hat)**2)\n        gaps.append(val_mse - train_mse)\n\n    return np.mean(gaps) if gaps else 0.0\n\ndef analyze_full_set(X_train, y_train, X_test, y_test, p_features, tau):\n    \"\"\"\n    Computes information loss and removal fraction on the full training set.\n    \"\"\"\n    n_train = X_train.shape[0]\n\n    # Full model\n    beta_full = np.linalg.lstsq(X_train, y_train, rcond=None)[0]\n    mse_full_test = np.mean((y_test - X_test @ beta_full)**2)\n\n    # Cleaned model\n    cooks_d = get_cooks_distances(X_train, y_train)\n    to_remove = np.where(cooks_d  tau)[0]\n    \n    keep_idx = np.arange(n_train)\n    if n_train - len(to_remove) = p_features + 1:\n        keep_idx = np.setdiff1d(np.arange(n_train), to_remove)\n    \n    fraction_removed = (n_train - len(keep_idx)) / n_train\n    \n    X_train_clean, y_train_clean = X_train[keep_idx], y_train[keep_idx]\n    \n    beta_clean = np.linalg.lstsq(X_train_clean, y_train_clean, rcond=None)[0]\n    mse_clean_test = np.mean((y_test - X_test @ beta_clean)**2)\n    \n    info_loss = mse_clean_test - mse_full_test\n    \n    return info_loss, fraction_removed\n    \ndef solve():\n    \"\"\"\n    Main function to run test cases and print results.\n    \"\"\"\n    test_cases = [\n        (42, 120, 200, 5, 5, float('inf'), 0.1, 6, 8),\n        (42, 120, 200, 5, 5, 4/120, 0.1, 6, 8),\n        (42, 120, 200, 5, 5, 1/120, 0.1, 6, 8),\n        (123, 80, 200, 6, 4, 1/80, 0, 6, 8)\n    ]\n    \n    all_results = []\n    baseline_gap_cache = {}\n\n    for s, n, n_test, p, k, tau, f, a, b in test_cases:\n        rng = np.random.default_rng(s)\n\n        # Generate data\n        X_train_full = rng.normal(0, 1, size=(n, p))\n        beta = np.array([((-1)**(j+1)) * (j/p) for j in range(1, p + 1)])\n        \n        noise = rng.normal(0, 1, size=n)\n        if f  0:\n            num_outliers = int(np.floor(f * n))\n            outlier_indices = rng.choice(n, size=num_outliers, replace=False)\n            X_train_full[outlier_indices, :] *= a\n            noise[outlier_indices] = rng.normal(0, b, size=num_outliers)\n            \n        y_train_full = X_train_full @ beta + noise\n        \n        X_test = rng.normal(0, 1, size=(n_test, p))\n        y_test = X_test @ beta + rng.normal(0, 1, size=n_test)\n\n        # Use caching for baseline gap on the same dataset\n        data_key = (s, n, p, f, a, b)\n        if data_key not in baseline_gap_cache:\n            # We need a new RNG for CV shuffling to be consistent across runs\n            cv_rng = np.random.default_rng(s) \n            baseline_gap_cache[data_key] = calculate_cv_gap(\n                X_train_full, y_train_full, k, p, float('inf'), cv_rng)\n\n        avg_baseline_gap = baseline_gap_cache[data_key]\n        \n        cv_rng = np.random.default_rng(s) # Reset RNG for fair comparison\n        avg_cleaned_gap = calculate_cv_gap(\n            X_train_full, y_train_full, k, p, tau, cv_rng)\n            \n        overfitting_reduction = avg_baseline_gap - avg_cleaned_gap\n\n        info_loss, removal_fraction = analyze_full_set(\n            X_train_full, y_train_full, X_test, y_test, p, tau)\n\n        all_results.append([overfitting_reduction, info_loss, removal_fraction])\n\n    # Format the final output string\n    formatted_results = []\n    for res_list in all_results:\n        formatted_list = [f\"{v:.6f}\" for v in res_list]\n        formatted_results.append(f\"[{','.join(formatted_list)}]\")\n    print(f\"[{','.join(formatted_results)}]\")\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3111535"}]}