{"hands_on_practices": [{"introduction": "The foundation of multiple linear regression lies in the principle of least squares, a method for finding the \"best-fit\" line through a set of data points. This first practice demystifies the core mechanics of this process by guiding you through the direct calculation of regression coefficients using matrix algebra. By working through this foundational example [@problem_id:1938980], you will build a concrete understanding of how a model's parameters are estimated from data, moving from abstract theory to tangible calculation.", "problem": "A food scientist is developing a new type of baked snack and wants to understand how baking conditions affect its crispiness. The crispiness is measured on a quantitative scale. The scientist conducts a small experiment with four batches, varying two factors: baking temperature and humidity. The factors are represented by coded variables, where $-1$ represents a low setting and $+1$ represents a high setting.\n\nThe proposed statistical model is a multiple linear regression model of the form:\n$$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon$$\nwhere:\n- $y$ is the crispiness score.\n- $x_1$ is the coded variable for baking temperature.\n- $x_2$ is the coded variable for humidity.\n- $\\beta_0$, $\\beta_1$, and $\\beta_2$ are the unknown model coefficients.\n- $\\epsilon$ is the random error term.\n\nThe results from the four experimental batches are as follows:\n- Batch 1: Temperature code ($x_1$) = -1, Humidity code ($x_2$) = -1, Crispiness ($y$) = 2.\n- Batch 2: Temperature code ($x_1$) = -1, Humidity code ($x_2$) = 1, Crispiness ($y$) = 4.\n- Batch 3: Temperature code ($x_1$) = 1, Humidity code ($x_2$) = -1, Crispiness ($y$) = 6.\n- Batch 4: Temperature code ($x_1$) = 1, Humidity code ($x_2$) = 1, Crispiness ($y$) = 8.\n\nUsing the method of least squares, determine the estimated values for the coefficients. Present your answer as a single row matrix containing the three estimated coefficients in the order $(\\hat{\\beta}_0, \\hat{\\beta}_1, \\hat{\\beta}_2)$.", "solution": "The goal is to find the least squares estimates for the coefficients $\\beta_0$, $\\beta_1$, and $\\beta_2$ in the multiple linear regression model. The model can be written in matrix form as $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}$, where $\\mathbf{y}$ is the vector of responses, $\\mathbf{X}$ is the design matrix, $\\boldsymbol{\\beta}$ is the vector of coefficients, and $\\boldsymbol{\\epsilon}$ is the vector of errors.\n\nFirst, we construct the response vector $\\mathbf{y}$ and the design matrix $\\mathbf{X}$ from the given experimental data. The design matrix includes a column of ones for the intercept term $\\beta_0$.\n\nThe response vector $\\mathbf{y}$ contains the crispiness scores:\n$$\n\\mathbf{y} = \\begin{pmatrix} 2 \\\\ 4 \\\\ 6 \\\\ 8 \\end{pmatrix}\n$$\n\nThe design matrix $\\mathbf{X}$ is constructed with a leading column of ones for the intercept, followed by the columns for the coded variables $x_1$ and $x_2$:\n$$\n\\mathbf{X} = \\begin{pmatrix} 1 & x_{11} & x_{21} \\\\ 1 & x_{12} & x_{22} \\\\ 1 & x_{13} & x_{23} \\\\ 1 & x_{14} & x_{24} \\end{pmatrix} = \\begin{pmatrix} 1 & -1 & -1 \\\\ 1 & -1 & 1 \\\\ 1 & 1 & -1 \\\\ 1 & 1 & 1 \\end{pmatrix}\n$$\n\nThe vector of coefficients to be estimated is $\\boldsymbol{\\beta} = (\\beta_0, \\beta_1, \\beta_2)^T$. The least squares estimate, denoted by $\\hat{\\boldsymbol{\\beta}}$, is given by the solution to the normal equations $\\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\mathbf{X}^T\\mathbf{y}$. Assuming $\\mathbf{X}^T\\mathbf{X}$ is invertible, the solution is:\n$$\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\n$$\n\nWe will compute this in several steps. First, we find the transpose of $\\mathbf{X}$:\n$$\n\\mathbf{X}^T = \\begin{pmatrix} 1 & 1 & 1 & 1 \\\\ -1 & -1 & 1 & 1 \\\\ -1 & 1 & -1 & 1 \\end{pmatrix}\n$$\n\nNext, we compute the product $\\mathbf{X}^T\\mathbf{X}$:\n$$\n\\mathbf{X}^T\\mathbf{X} = \\begin{pmatrix} 1 & 1 & 1 & 1 \\\\ -1 & -1 & 1 & 1 \\\\ -1 & 1 & -1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & -1 & -1 \\\\ 1 & -1 & 1 \\\\ 1 & 1 & -1 \\\\ 1 & 1 & 1 \\end{pmatrix}\n$$\nThe elements of the resulting matrix are:\n- $(1,1): 1 \\cdot 1 + 1 \\cdot 1 + 1 \\cdot 1 + 1 \\cdot 1 = 4$\n- $(1,2): 1 \\cdot (-1) + 1 \\cdot (-1) + 1 \\cdot 1 + 1 \\cdot 1 = 0$\n- $(1,3): 1 \\cdot (-1) + 1 \\cdot 1 + 1 \\cdot (-1) + 1 \\cdot 1 = 0$\n- $(2,1): (-1) \\cdot 1 + (-1) \\cdot 1 + 1 \\cdot 1 + 1 \\cdot 1 = 0$\n- $(2,2): (-1) \\cdot (-1) + (-1) \\cdot (-1) + 1 \\cdot 1 + 1 \\cdot 1 = 4$\n- $(2,3): (-1) \\cdot (-1) + (-1) \\cdot 1 + 1 \\cdot (-1) + 1 \\cdot 1 = 0$\n- $(3,1): (-1) \\cdot 1 + 1 \\cdot 1 + (-1) \\cdot 1 + 1 \\cdot 1 = 0$\n- $(3,2): (-1) \\cdot (-1) + 1 \\cdot (-1) + (-1) \\cdot 1 + 1 \\cdot 1 = 0$\n- $(3,3): (-1) \\cdot (-1) + 1 \\cdot 1 + (-1) \\cdot (-1) + 1 \\cdot 1 = 4$\n\nSo, the matrix is diagonal:\n$$\n\\mathbf{X}^T\\mathbf{X} = \\begin{pmatrix} 4 & 0 & 0 \\\\ 0 & 4 & 0 \\\\ 0 & 0 & 4 \\end{pmatrix} = 4\\mathbf{I}_3\n$$\nwhere $\\mathbf{I}_3$ is the $3 \\times 3$ identity matrix.\n\nThe inverse of this diagonal matrix is straightforward to compute:\n$$\n(\\mathbf{X}^T\\mathbf{X})^{-1} = \\frac{1}{4}\\mathbf{I}_3 = \\begin{pmatrix} \\frac{1}{4} & 0 & 0 \\\\ 0 & \\frac{1}{4} & 0 \\\\ 0 & 0 & \\frac{1}{4} \\end{pmatrix}\n$$\n\nNow, we compute the product $\\mathbf{X}^T\\mathbf{y}$:\n$$\n\\mathbf{X}^T\\mathbf{y} = \\begin{pmatrix} 1 & 1 & 1 & 1 \\\\ -1 & -1 & 1 & 1 \\\\ -1 & 1 & -1 & 1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 4 \\\\ 6 \\\\ 8 \\end{pmatrix} = \\begin{pmatrix} 1(2)+1(4)+1(6)+1(8) \\\\ -1(2)-1(4)+1(6)+1(8) \\\\ -1(2)+1(4)-1(6)+1(8) \\end{pmatrix} = \\begin{pmatrix} 20 \\\\ 8 \\\\ 4 \\end{pmatrix}\n$$\n\nFinally, we multiply $(\\mathbf{X}^T\\mathbf{X})^{-1}$ by $\\mathbf{X}^T\\mathbf{y}$ to find $\\hat{\\boldsymbol{\\beta}}$:\n$$\n\\hat{\\boldsymbol{\\beta}} = \\begin{pmatrix} \\hat{\\beta}_0 \\\\ \\hat{\\beta}_1 \\\\ \\hat{\\beta}_2 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{4} & 0 & 0 \\\\ 0 & \\frac{1}{4} & 0 \\\\ 0 & 0 & \\frac{1}{4} \\end{pmatrix} \\begin{pmatrix} 20 \\\\ 8 \\\\ 4 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{4}(20) \\\\ \\frac{1}{4}(8) \\\\ \\frac{1}{4}(4) \\end{pmatrix} = \\begin{pmatrix} 5 \\\\ 2 \\\\ 1 \\end{pmatrix}\n$$\n\nThe least squares estimates are $\\hat{\\beta}_0 = 5$, $\\hat{\\beta}_1 = 2$, and $\\hat{\\beta}_2 = 1$. The problem asks for the answer as a row matrix $(\\hat{\\beta}_0, \\hat{\\beta}_1, \\hat{\\beta}_2)$.", "answer": "$$\n\\boxed{\\begin{pmatrix} 5 & 2 & 1 \\end{pmatrix}}\n$$", "id": "1938980"}, {"introduction": "Once a regression model is built, its primary value often lies in its predictive power. However, a single point prediction is incomplete without an accompanying measure of its uncertainty. This exercise [@problem_id:1938959] transitions from model fitting to practical application by tasking you with the construction of a prediction interval. This is a crucial skill that quantifies the confidence in a forecast, making the model's output far more useful for real-world decision-making in fields like business analytics and finance.", "problem": "A retail analytics firm is developing a model to predict the weekly sales of new pop-up stores. They have collected data from `n=30` existing stores and fitted a multiple linear regression model of the form:\n$$\n\\text{Sales} = \\beta_0 + \\beta_1 \\cdot \\text{Size} + \\beta_2 \\cdot \\text{PopDensity} + \\epsilon\n$$\nwhere `Sales` is measured in thousands of dollars, `Size` is the floor area in square meters, and `PopDensity` is the population density in the surrounding 1-km radius, measured in thousands of people per square kilometer.\n\nThe team's analysis produced the following results based on the sample data:\n- Estimated regression equation: $\\hat{y} = 51.5 + 0.24 x_1 + 14.8 x_2$, where $x_1$ represents Size and $x_2$ represents PopDensity.\n- Residual standard error: $s_e = 12.5$ (in thousands of dollars).\n- The number of predictors in the model is $k=2$.\n\nThe company plans to open a new store with a `Size` of 520 square meters in an area with a `PopDensity` of 4.5 thousands of people per square kilometer. For this new observation, represented by the vector $x_0$, the statistical software reports the value of the leverage-related term as $x_0^T (X^T X)^{-1} x_0 = 0.082$. Here, $X$ is the design matrix for the original 30 observations.\n\nUsing the provided t-distribution critical value $t_{0.05, 27} = 1.703$, construct a 90% prediction interval for the weekly sales of this new store. Report the lower and upper bounds of the interval. Both values should be expressed in thousands of dollars and rounded to three significant figures.", "solution": "We are asked for a 90 percent prediction interval for a new observation in a multiple linear regression with $n=30$, $k=2$ predictors, residual standard error $s_{e}=12.5$, and leverage term $h_{0}=x_{0}^{T}(X^{T}X)^{-1}x_{0}=0.082$. The degrees of freedom are $n-k-1=27$, and the given critical value is $t_{0.05,27}=1.703$.\n\nThe point prediction at $x_{1}=520$, $x_{2}=4.5$ is obtained from the fitted equation $\\hat{y}=51.5+0.24x_{1}+14.8x_{2}$:\n$$\n\\hat{y}_{0}=51.5+0.24\\cdot 520+14.8\\cdot 4.5=51.5+124.8+66.6=242.9.\n$$\n\nThe standard error for prediction is $s_{e}\\sqrt{1+h_{0}}=12.5\\sqrt{1+0.082}=12.5\\sqrt{1.082}$. The $90$ percent prediction interval is\n$$\n\\hat{y}_{0}\\pm t_{0.05,27}\\,s_{e}\\sqrt{1+h_{0}}\n=242.9\\pm 1.703\\cdot 12.5\\cdot \\sqrt{1.082}.\n$$\nCompute the margin:\n$$\n\\sqrt{1.082}\\approx 1.040192,\\quad 12.5\\cdot \\sqrt{1.082}\\approx 13.0024,\\quad\n1.703\\cdot 13.0024\\approx 22.143.\n$$\nThus the interval bounds are\n$$\n\\text{Lower}=242.9-22.143\\approx 220.757,\\qquad\n\\text{Upper}=242.9+22.143\\approx 265.043.\n$$\nRounding each bound to three significant figures (in thousands of dollars) gives $221$ and $265$.", "answer": "$$\\boxed{\\begin{pmatrix}221 & 265\\end{pmatrix}}$$", "id": "1938959"}, {"introduction": "The Ordinary Least Squares (OLS) estimator is powerful, but its optimality hinges on key assumptions, including the assumption of constant error variance (homoskedasticity). This advanced practice [@problem_id:3152038] uses a computer simulation to explore the consequences of violating this assumption. You will generate data exhibiting heteroskedasticity, compare the performance of OLS to the more robust Weighted Least Squares (WLS) estimator, and learn to diagnose the issue, providing a deep, practical insight into a common challenge in applied statistical modeling.", "problem": "Consider the multiple linear regression model with a single observation index $i \\in \\{1,\\dots,n\\}$ defined by $y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\varepsilon_i$, where the covariates $x_{i1}$ and $x_{i2}$ are nonrandom design values known to the analyst, and the disturbance $\\varepsilon_i$ satisfies $\\mathbb{E}[\\varepsilon_i \\mid x_{i1}, x_{i2}] = 0$ and $\\operatorname{Var}(\\varepsilon_i \\mid x_{i1}, x_{i2}) = \\sigma^2 x_{i1}^2$. This model exhibits heteroskedasticity that scales with the square of the first covariate. You must investigate residual patterns and compare two estimation procedures: Ordinary Least Squares (OLS) and Weighted Least Squares (WLS), where WLS uses weights $w_i = 1/x_{i1}^2$.\n\nStarting only from the principles that (i) OLS chooses coefficients that minimize the unweighted sum of squared residuals, and (ii) WLS chooses coefficients that minimize the weighted sum of squared residuals with prespecified nonnegative weights, implement a program that for each test case below performs all of the following steps:\n\n1. Data generation. Construct $n$ observations according to $y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\varepsilon_i$, where $\\varepsilon_i = \\sigma x_{i1} z_i$ and $z_i$ are independent and identically distributed standard normal variables. Ensure $x_{i1} > 0$ for all $i$, and construct $x_{i2}$ as an independent covariate. Use the specified random seeds to make all draws deterministic.\n2. Estimation. Compute the OLS coefficient vector as the minimizer of $\\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1 x_{i1} - \\beta_2 x_{i2})^2$, and compute the WLS coefficient vector as the minimizer of $\\sum_{i=1}^n w_i (y_i - \\beta_0 - \\beta_1 x_{i1} - \\beta_2 x_{i2})^2$ with $w_i = 1/x_{i1}^2$.\n3. Residual pattern diagnostic. Compute the OLS residuals $e_i = y_i - \\hat{\\beta}_0^{\\text{OLS}} - \\hat{\\beta}_1^{\\text{OLS}} x_{i1} - \\hat{\\beta}_2^{\\text{OLS}} x_{i2}$. Evaluate the empirical Pearson correlation between the sequences $(e_i^2)_{i=1}^n$ and $(x_{i1}^2)_{i=1}^n$ to quantify whether larger $x_{i1}$ is associated with larger squared residuals.\n4. Efficiency comparison. Let $\\boldsymbol{\\beta}^\\star = (\\beta_0,\\beta_1,\\beta_2)$ denote the true coefficient vector. Compute the coefficient squared error for each estimator, defined by $\\mathrm{CSE}_{\\text{OLS}} = \\sum_{j=0}^2 (\\hat{\\beta}_j^{\\text{OLS}} - \\beta_j)^2$ and $\\mathrm{CSE}_{\\text{WLS}} = \\sum_{j=0}^2 (\\hat{\\beta}_j^{\\text{WLS}} - \\beta_j)^2$. Report the ratio $R = \\mathrm{CSE}_{\\text{WLS}} / \\mathrm{CSE}_{\\text{OLS}}$.\n\nYour program must process the following three test cases, each specified by $(n, \\boldsymbol{\\beta}^\\star, \\sigma, \\text{design parameters}, \\text{seed})$, where the design parameters define how $x_{i1}$ and $x_{i2}$ are generated:\n\n- Test Case A (general happy path): $n = 200$, $\\boldsymbol{\\beta}^\\star = (1.5, 2.0, -1.0)$, $\\sigma = 1.0$, $x_{i1} \\sim \\text{Uniform}[0.2, 3.0]$, $x_{i2} \\sim \\text{Normal}(0, 1.5^2)$, seed $= 42$.\n- Test Case B (mild heteroskedasticity boundary): $n = 200$, $\\boldsymbol{\\beta}^\\star = (0.5, 1.0, 0.5)$, $\\sigma = 0.2$, $x_{i1} \\sim \\text{Uniform}[0.5, 1.5]$, $x_{i2} \\sim \\text{Normal}(0, 1.0^2)$, seed $= 123$.\n- Test Case C (near-constant $x_{i1}$ edge case): $n = 200$, $\\boldsymbol{\\beta}^\\star = (2.0, -0.5, 1.0)$, $\\sigma = 0.8$, $x_{i1} = 1.0 + \\delta_i$ with $\\delta_i \\sim \\text{Normal}(0, 0.01^2)$ truncated to keep $x_{i1} \\geq 0.1$, $x_{i2} \\sim \\text{Normal}(0, 2.0^2)$, seed $= 987$.\n\nFor each test case, compute:\n- The Pearson correlation between $(e_i^2)$ and $(x_{i1}^2)$, reported as a float rounded to six decimal places.\n- The ratio $R = \\mathrm{CSE}_{\\text{WLS}} / \\mathrm{CSE}_{\\text{OLS}}$, reported as a float rounded to six decimal places.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets in the order $[\\text{corr}_A, R_A, \\text{corr}_B, R_B, \\text{corr}_C, R_C]$, where each entry is a float rounded to six decimal places. No units are involved in this task.", "solution": "The problem statement has been validated and is deemed sound. It is scientifically grounded in the statistical theory of linear regression, well-posed with all necessary parameters and conditions provided, and articulated using objective, formal language. There are no contradictions, ambiguities, or unrealistic assumptions. The problem is a standard, non-trivial simulation exercise in econometrics and statistics, designed to illustrate the consequences of heteroskedasticity and the superior efficiency of Weighted Least Squares (WLS) over Ordinary Least Squares (OLS) when the error variance structure is known.\n\nThe problem investigates the multiple linear regression model, which can be expressed in matrix form for a sample of $n$ observations as:\n$$\n\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\n$$\nwhere $\\mathbf{y}$ is an $n \\times 1$ vector of observations on the dependent variable, $\\mathbf{X}$ is an $n \\times p$ design matrix of covariates (here, $p=3$), $\\boldsymbol{\\beta}$ is a $p \\times 1$ vector of coefficients, and $\\boldsymbol{\\varepsilon}$ is an $n \\times 1$ vector of unobserved disturbances. For this problem, the $i$-th row of $\\mathbf{X}$, denoted $\\mathbf{x}_i^T$, is $[1, x_{i1}, x_{i2}]$, and the coefficient vector is $\\boldsymbol{\\beta} = (\\beta_0, \\beta_1, \\beta_2)^T$.\n\nThe disturbances are assumed to have a conditional mean of zero, $\\mathbb{E}[\\boldsymbol{\\varepsilon} \\mid \\mathbf{X}] = \\mathbf{0}$. The problem specifies a particular form of heteroskedasticity for the variance:\n$$\n\\operatorname{Var}(\\varepsilon_i \\mid x_{i1}, x_{i2}) = \\sigma^2 x_{i1}^2\n$$\nThis means the covariance matrix of the error vector, $\\boldsymbol{\\Omega} = \\operatorname{Cov}(\\boldsymbol{\\varepsilon} \\mid \\mathbf{X})$, is not the familiar $\\sigma^2 \\mathbf{I}$ of the classical model, but a diagonal matrix with non-constant diagonal elements:\n$$\n\\boldsymbol{\\Omega} = \\sigma^2 \\operatorname{diag}(x_{11}^2, x_{21}^2, \\dots, x_{n1}^2)\n$$\nThe condition $x_{i1} > 0$ ensures the variances are all positive.\n\n**Ordinary Least Squares (OLS) Estimation**\nThe OLS estimator, $\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}}$, is found by minimizing the sum of squared residuals (SSR):\n$$\nS(\\boldsymbol{\\beta}) = \\sum_{i=1}^n (y_i - \\mathbf{x}_i^T \\boldsymbol{\\beta})^2 = (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\n$$\nSetting the gradient with respect to $\\boldsymbol{\\beta}$ to zero, $\\frac{\\partial S}{\\partial \\boldsymbol{\\beta}} = -2\\mathbf{X}^T(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}) = \\mathbf{0}$, yields the normal equations $\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta} = \\mathbf{X}^T\\mathbf{y}$. The OLS estimator is thus:\n$$\n\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}\n$$\nUnder the given heteroskedasticity, the OLS estimator remains unbiased, but it is no longer the Best Linear Unbiased Estimator (BLUE); its variance is not minimized among all linear unbiased estimators.\n\n**Weighted Least Squares (WLS) Estimation**\nWLS is an extension of OLS that addresses heteroskedasticity by assigning a weight to each observation, with weights being inversely proportional to the error variance. The WLS estimator, $\\hat{\\boldsymbol{\\beta}}_{\\text{WLS}}$, minimizes the weighted sum of squared residuals:\n$$\nS_W(\\boldsymbol{\\beta}) = \\sum_{i=1}^n w_i (y_i - \\mathbf{x}_i^T \\boldsymbol{\\beta})^2 = (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T \\mathbf{W} (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\n$$\nwhere $\\mathbf{W}$ is a diagonal matrix of weights. The optimal weights are $w_i \\propto 1/\\operatorname{Var}(\\varepsilon_i)$. For this problem, we choose $w_i = 1/x_{i1}^2$, which corresponds to $\\mathbf{W} = \\operatorname{diag}(1/x_{11}^2, \\dots, 1/x_{n1}^2)$.\nDifferentiating $S_W(\\boldsymbol{\\beta})$ with respect to $\\boldsymbol{\\beta}$ and setting the result to zero yields the WLS normal equations $\\mathbf{X}^T\\mathbf{W}\\mathbf{X}\\boldsymbol{\\beta} = \\mathbf{X}^T\\mathbf{W}\\mathbf{y}$. The WLS estimator is:\n$$\n\\hat{\\boldsymbol{\\beta}}_{\\text{WLS}} = (\\mathbf{X}^T \\mathbf{W} \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{W} \\mathbf{y}\n$$\nWLS can be viewed as applying OLS to a transformed model. Let's define a transformed observation $y_i^* = y_i \\sqrt{w_i} = y_i / x_{i1}$ and transformed covariates $\\mathbf{x}_i^* = \\mathbf{x}_i \\sqrt{w_i} = \\mathbf{x}_i / x_{i1}$. The transformed model is $y_i^* = \\mathbf{x}_i^{*T} \\boldsymbol{\\beta} + \\varepsilon_i^*$, where the transformed error is $\\varepsilon_i^* = \\varepsilon_i / x_{i1}$. The variance of the transformed error is constant:\n$$\n\\operatorname{Var}(\\varepsilon_i^*) = \\operatorname{Var}\\left(\\frac{\\varepsilon_i}{x_{i1}}\\right) = \\frac{1}{x_{i1}^2} \\operatorname{Var}(\\varepsilon_i) = \\frac{1}{x_{i1}^2} (\\sigma^2 x_{i1}^2) = \\sigma^2\n$$\nSince the transformed model is homoskedastic, the Gauss-Markov theorem applies, and the OLS estimator for the transformed model (which is precisely the WLS estimator) is BLUE. Therefore, WLS is more efficient than OLS.\n\n**Computational Procedure**\nFor each test case, the following steps are implemented:\n1.  **Data Generation**: Using the specified seed, $n$ values for $x_{i1}$ and $x_{i2}$ are drawn from their respective distributions. The design matrix $\\mathbf{X}$ is formed. The errors are generated as $\\varepsilon_i = \\sigma x_{i1} z_i$, where $z_i \\sim \\mathcal{N}(0,1)$ are i.i.d. draws. The response vector is computed as $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta}^\\star + \\boldsymbol{\\varepsilon}$.\n2.  **Estimation**: The vectors $\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}}$ and $\\hat{\\boldsymbol{\\beta}}_{\\text{WLS}}$ are computed using their matrix formulas, implemented via `np.linalg.solve` for numerical stability.\n3.  **Residual Pattern Diagnostic**: The OLS residuals are calculated as $\\mathbf{e} = \\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}}$. The Pearson correlation coefficient between the sequence of squared residuals, $(e_i^2)_{i=1}^n$, and the sequence of squared covariates, $(x_{i1}^2)_{i=1}^n$, is computed. A positive correlation is expected, confirming that larger residual variance is associated with larger values of $x_{i1}$.\n4.  **Efficiency Comparison**: The Coefficient Squared Error (CSE) is computed for both estimators: $\\mathrm{CSE}_{\\text{OLS}} = \\|\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} - \\boldsymbol{\\beta}^\\star\\|_2^2$ and $\\mathrm{CSE}_{\\text{WLS}} = \\|\\hat{\\boldsymbol{\\beta}}_{\\text{WLS}} - \\boldsymbol{\\beta}^\\star\\|_2^2$. The ratio $R = \\mathrm{CSE}_{\\text{WLS}} / \\mathrm{CSE}_{\\text{OLS}}$ is calculated. Since WLS is expected to be more efficient, a ratio $R < 1$ is anticipated. The value of $R$ quantifies the realized efficiency gain of WLS over OLS for this specific data realization. A smaller $R$ indicates a larger gain. The magnitude of this gain depends on the severity of the heteroskedasticity, which is determined by the combination of $\\sigma$ and the variability of $x_{i1}$.\n\nThe procedure is applied to each of the three test cases, and the resulting correlation and ratio values are compiled.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the regression problem for all test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"name\": \"A\",\n            \"n\": 200,\n            \"beta_star\": np.array([1.5, 2.0, -1.0]),\n            \"sigma\": 1.0,\n            \"x1_params\": {\"type\": \"uniform\", \"low\": 0.2, \"high\": 3.0},\n            \"x2_params\": {\"type\": \"normal\", \"mean\": 0, \"std\": 1.5},\n            \"seed\": 42\n        },\n        {\n            \"name\": \"B\",\n            \"n\": 200,\n            \"beta_star\": np.array([0.5, 1.0, 0.5]),\n            \"sigma\": 0.2,\n            \"x1_params\": {\"type\": \"uniform\", \"low\": 0.5, \"high\": 1.5},\n            \"x2_params\": {\"type\": \"normal\", \"mean\": 0, \"std\": 1.0},\n            \"seed\": 123\n        },\n        {\n            \"name\": \"C\",\n            \"n\": 200,\n            \"beta_star\": np.array([2.0, -0.5, 1.0]),\n            \"sigma\": 0.8,\n            \"x1_params\": {\"type\": \"truncated_normal\", \"base\": 1.0, \"mean\": 0, \"std\": 0.01, \"min_val\": 0.1},\n            \"x2_params\": {\"type\": \"normal\", \"mean\": 0, \"std\": 2.0},\n            \"seed\": 987\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        n = case[\"n\"]\n        beta_star = case[\"beta_star\"]\n        sigma = case[\"sigma\"]\n        seed = case[\"seed\"]\n        rng = np.random.default_rng(seed)\n\n        # 1. Data generation\n        # Generate x1\n        x1_params = case[\"x1_params\"]\n        if x1_params[\"type\"] == \"uniform\":\n            x1 = rng.uniform(low=x1_params[\"low\"], high=x1_params[\"high\"], size=n)\n        elif x1_params[\"type\"] == \"truncated_normal\":\n            delta = rng.normal(loc=x1_params[\"mean\"], scale=x1_params[\"std\"], size=n)\n            x1 = x1_params[\"base\"] + delta\n            # The problem states truncation is to keep x1 >= 0.1.\n            # With mean 1.0 and std 0.01, the probability of x1 < 0.1 is negligible.\n            # Using np.maximum is a robust way to enforce this constraint.\n            x1 = np.maximum(x1, x1_params[\"min_val\"])\n\n        # Generate x2\n        x2_params = case[\"x2_params\"]\n        x2 = rng.normal(loc=x2_params[\"mean\"], scale=x2_params[\"std\"], size=n)\n\n        # Construct Design Matrix X\n        X = np.column_stack((np.ones(n), x1, x2))\n\n        # Generate errors and response variable y\n        z = rng.normal(size=n)\n        epsilon = sigma * x1 * z\n        y = X @ beta_star + epsilon\n\n        # 2. Estimation\n        # OLS estimation\n        # beta_hat_ols = (X'X)^-1 * X'y\n        try:\n            beta_hat_ols = np.linalg.solve(X.T @ X, X.T @ y)\n        except np.linalg.LinAlgError:\n            # Fallback to pseudoinverse if matrix is singular, though unlikely with this design\n            beta_hat_ols = np.linalg.pinv(X.T @ X) @ X.T @ y\n\n        # WLS estimation\n        # beta_hat_wls = (X'WX)^-1 * X'Wy\n        # W is diagonal with w_i = 1/x1_i^2\n        w = 1.0 / (x1**2)\n        # Efficiently compute X'WX and X'Wy without forming the full W matrix\n        X_T_W_X = X.T @ (w[:, np.newaxis] * X)\n        X_T_W_y = X.T @ (w * y)\n        try:\n            beta_hat_wls = np.linalg.solve(X_T_W_X, X_T_W_y)\n        except np.linalg.LinAlgError:\n            beta_hat_wls = np.linalg.pinv(X_T_W_X) @ X_T_W_y\n\n        # 3. Residual pattern diagnostic\n        # Compute OLS residuals\n        e_ols = y - X @ beta_hat_ols\n        # Pearson correlation between e_ols^2 and x1^2\n        corr = np.corrcoef(e_ols**2, x1**2)[0, 1]\n\n        # 4. Efficiency comparison\n        # Coefficient Squared Error (CSE)\n        cse_ols = np.sum((beta_hat_ols - beta_star)**2)\n        cse_wls = np.sum((beta_hat_wls - beta_star)**2)\n        \n        # Ratio R\n        # Handle division by zero, although cse_ols should not be zero in this context.\n        if cse_ols == 0:\n            ratio_R = np.inf if cse_wls > 0 else 1.0\n        else:\n            ratio_R = cse_wls / cse_ols\n\n        results.extend([corr, ratio_R])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{val:.6f}' for val in results)}]\")\n\nsolve()\n```", "id": "3152038"}]}