{"hands_on_practices": [{"introduction": "The width of a prediction interval is not arbitrary; it is determined by specific, quantifiable factors. This first practice invites you to dissect the prediction interval formula and isolate its core components: the sample size $n$, which influences the critical $t$-value and the residual standard error $\\hat{\\sigma}$, and the irreducible error. By programmatically exploring how interval width responds to changes in sample size under controlled conditions, you will build a foundational, quantitative intuition for the drivers of prediction uncertainty [@problem_id:3160052].", "problem": "You are asked to connect the construction of a two-sided prediction interval in a linear model to how the Student’s $t$-quantile and the residual standard error jointly control its width. Work from first principles, starting from the following base.\n\nAssume the linear model $y = X\\beta + \\varepsilon$ with $n$ observations and $p$ parameters, where $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$. Use the ordinary least squares (OLS) estimator and the Gaussian error model to derive the sampling distribution of the standardized prediction error at a new design point $x_{0} \\in \\mathbb{R}^{p}$. Then derive the exact two-sided $(1 - \\alpha)$ prediction interval for a new response $y_{0}$ at $x_{0}$ and its width. Your derivation should make clear how the Student’s $t$-quantile with $n-p$ degrees of freedom and the residual standard error $\\hat{\\sigma}$ appear in the width, and how the design dependence enters via the leverage at $x_{0}$.\n\nAfter your derivation, write a program that implements the derived formulas for a controlled design that isolates and quantifies the roles of the $t$-quantile and $\\hat{\\sigma}$. Use the following deterministic setup, common across all test cases:\n\n- Model: simple linear regression with intercept and one centered predictor, so $p = 2$.\n- Design matrix $X \\in \\mathbb{R}^{n \\times 2}$ is given by the first column of ones and the second column $x \\in \\mathbb{R}^{n}$ constructed deterministically as follows: take $n$ equally spaced points on $[-1, 1]$, center them to have empirical mean $0$, then scale them so that $\\sum_{i=1}^{n} x_{i}^{2} = n$. This ensures $X^{\\top} X$ is diagonal up to numerical symmetry with diagonal approximately $(n, n)$.\n- New design point for prediction: $x_{0} = (1, 0)^{\\top}$, i.e., prediction at the centered predictor level.\n- Residual sum of squares (RSS) is fixed at the same value across comparisons, $\\mathrm{RSS} = 100$ (unitless). The residual standard error is $\\hat{\\sigma} = \\sqrt{\\mathrm{RSS}/(n-p)}$.\n- Use a two-sided nominal level specified by $\\alpha$.\n\nFor each test case below, let $n$ denote the baseline sample size and $2n$ the doubled sample size, both with the same $\\alpha$ and the same $\\mathrm{RSS} = 100$. For each test case, your program must compute:\n\n1. The ratio $r_{\\mathrm{full}}$ of the prediction-interval width at $2n$ to that at $n$ under the design above, using the exact leverage at $x_{0}$ implied by $X$ for each $n$.\n2. The absolute deviation $|r_{\\mathrm{full}} - 0.5|$ to quantify how far the width ratio is from “half.”\n3. The ratio $r_{t,\\sigma}$ obtained by removing the design leverage from the width, i.e., the ratio of the factors that depend only on the $t$-quantile and $\\hat{\\sigma}$ when going from $n$ to $2n$.\n\nAll quantities are unitless; no physical units are involved. Angles are not used.\n\nTest suite (each item is a pair $(n,\\alpha)$; use $p = 2$ and $\\mathrm{RSS} = 100$ throughout):\n- Case A (happy path, small degrees of freedom): $(n, \\alpha) = (3, 0.05)$.\n- Case B (boundary leaning, still small sample): $(n, \\alpha) = (6, 0.05)$.\n- Case C (moderate sample): $(n, \\alpha) = (20, 0.05)$.\n- Case D (effect of a tighter level on $t$): $(n, \\alpha) = (50, 0.01)$.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list of lists in the order of the test suite, where each inner list is $[r_{\\mathrm{full}}, |r_{\\mathrm{full}} - 0.5|, r_{t,\\sigma}]$. For example, a valid output looks like\n[[a11,a12,a13],[a21,a22,a23],[a31,a32,a33],[a41,a42,a43]]\nwith floating-point numbers in place of each $a_{ij}$.", "solution": "The user-provided problem is assessed as valid. It is scientifically grounded in the theory of linear statistical models, well-posed with a clear objective and sufficient information, and uses objective, formal language. The task is to derive the width of a prediction interval and then implement a program to study its behavior under specific conditions.\n\nThe solution proceeds in two parts. First, a theoretical derivation of the prediction interval and its width. Second, the implementation of a program to compute specific quantities based on this derivation.\n\n**Part 1: Derivation of the Prediction Interval and its Width**\n\nWe begin with the specified linear model:\n$$ y = X\\beta + \\varepsilon $$\nwhere $y \\in \\mathbb{R}^{n}$ is the vector of observed responses, $X \\in \\mathbb{R}^{n \\times p}$ is the known design matrix of rank $p$, $\\beta \\in \\mathbb{R}^{p}$ is the vector of unknown parameters, and $\\varepsilon \\in \\mathbb{R}^{n}$ is the vector of unobserved random errors. We assume the errors are independent and identically distributed according to a Gaussian distribution, $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$, where $\\sigma^2$ is the unknown error variance and $I_n$ is the $n \\times n$ identity matrix.\n\nThe ordinary least squares (OLS) estimator for $\\beta$ is given by:\n$$ \\hat{\\beta} = (X^{\\top}X)^{-1}X^{\\top}y $$\nGiven the distributional assumption on $\\varepsilon$, the OLS estimator $\\hat{\\beta}$ is also a random variable with a Gaussian distribution. Its expected value is $E[\\hat{\\beta}] = (X^{\\top}X)^{-1}X^{\\top}E[y] = (X^{\\top}X)^{-1}X^{\\top}(X\\beta) = \\beta$, making it an unbiased estimator. Its covariance matrix is $\\mathrm{Var}(\\hat{\\beta}) = \\mathrm{Var}((X^{\\top}X)^{-1}X^{\\top}y) = (X^{\\top}X)^{-1}X^{\\top}(\\sigma^2 I_n)X(X^{\\top}X)^{-1} = \\sigma^2(X^{\\top}X)^{-1}$. Thus, the sampling distribution of the estimator is:\n$$ \\hat{\\beta} \\sim \\mathcal{N}(\\beta, \\sigma^2(X^{\\top}X)^{-1}) $$\n\nWe are interested in predicting a new response, $y_0$, at a new design point, $x_0 \\in \\mathbb{R}^{p}$. The model for this new observation is:\n$$ y_0 = x_0^{\\top}\\beta + \\varepsilon_0 $$\nwhere $\\varepsilon_0 \\sim \\mathcal{N}(0, \\sigma^2)$ is assumed to be independent of the training errors $\\varepsilon$.\n\nThe point prediction for $y_0$ is obtained by substituting the estimated parameters $\\hat{\\beta}$ into the model equation for the new point:\n$$ \\hat{y}_0 = x_0^{\\top}\\hat{\\beta} $$\nThe prediction error is the difference between the true future observation $y_0$ and our prediction $\\hat{y}_0$:\n$$ y_0 - \\hat{y}_0 = (x_0^{\\top}\\beta + \\varepsilon_0) - x_0^{\\top}\\hat{\\beta} = \\varepsilon_0 - x_0^{\\top}(\\hat{\\beta} - \\beta) $$\nThe prediction error is a linear combination of Gaussian random variables ($\\varepsilon_0$ and the elements of $\\hat{\\beta}$), so it is also normally distributed. Its expected value is $E[y_0 - \\hat{y}_0] = E[\\varepsilon_0] - x_0^{\\top}E[\\hat{\\beta} - \\beta] = 0 - x_0^{\\top}(0) = 0$.\n\nThe variance of the prediction error is calculated as follows, using the independence of $\\varepsilon_0$ and $\\hat{\\beta}$ (which is a function of the training data):\n$$ \\mathrm{Var}(y_0 - \\hat{y}_0) = \\mathrm{Var}(\\varepsilon_0) + \\mathrm{Var}(x_0^{\\top}(\\hat{\\beta} - \\beta)) $$\n$$ \\mathrm{Var}(y_0 - \\hat{y}_0) = \\sigma^2 + x_0^{\\top}\\mathrm{Var}(\\hat{\\beta})x_0 = \\sigma^2 + x_0^{\\top}(\\sigma^2(X^{\\top}X)^{-1})x_0 $$\nFactoring out $\\sigma^2$, we get:\n$$ \\mathrm{Var}(y_0 - \\hat{y}_0) = \\sigma^2 \\left(1 + x_0^{\\top}(X^{\\top}X)^{-1}x_0\\right) $$\nThus, the distribution of the prediction error is:\n$$ y_0 - \\hat{y}_0 \\sim \\mathcal{N}\\left(0, \\sigma^2 \\left(1 + x_0^{\\top}(X^{\\top}X)^{-1}x_0\\right)\\right) $$\nThe term $h_0 = x_0^{\\top}(X^{\\top}X)^{-1}x_0$ is known as the leverage of the new point $x_0$.\n\nIf $\\sigma^2$ were known, we could form a standard normal pivot:\n$$ Z = \\frac{y_0 - \\hat{y}_0}{\\sigma \\sqrt{1 + x_0^{\\top}(X^{\\top}X)^{-1}x_0}} \\sim \\mathcal{N}(0, 1) $$\nHowever, $\\sigma^2$ is typically unknown and must be estimated from the data. The unbiased estimator for $\\sigma^2$ is the mean squared error (MSE), based on the residual sum of squares (RSS):\n$$ \\hat{\\sigma}^2 = \\frac{\\mathrm{RSS}}{n-p} = \\frac{1}{n-p}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 $$\nThe quantity $\\hat{\\sigma} = \\sqrt{\\hat{\\sigma}^2}$ is the residual standard error. A fundamental result of linear model theory under Gaussian errors states that:\n$$ \\frac{(n-p)\\hat{\\sigma}^2}{\\sigma^2} = \\frac{\\mathrm{RSS}}{\\sigma^2} \\sim \\chi^2_{n-p} $$\nwhere $\\chi^2_{n-p}$ is the chi-squared distribution with $n-p$ degrees of freedom. Furthermore, $\\hat{\\beta}$ and $\\hat{\\sigma}^2$ are independent.\n\nTo form a pivotal quantity that does not depend on $\\sigma$, we construct a Student's $t$-statistic. This statistic is the ratio of a standard normal variable to the square root of an independent chi-squared variable divided by its degrees of freedom.\n$$ T = \\frac{\\frac{y_0 - \\hat{y}_0}{\\sigma \\sqrt{1 + h_0}}}{\\sqrt{\\frac{(n-p)\\hat{\\sigma}^2/\\sigma^2}{n-p}}} = \\frac{\\frac{y_0 - \\hat{y}_0}{\\sigma \\sqrt{1 + h_0}}}{\\frac{\\hat{\\sigma}}{\\sigma}} = \\frac{y_0 - \\hat{y}_0}{\\hat{\\sigma} \\sqrt{1 + h_0}} $$\nThis quantity $T$ follows a Student's $t$-distribution with $n-p$ degrees of freedom:\n$$ \\frac{y_0 - \\hat{y}_0}{\\hat{\\sigma} \\sqrt{1 + x_0^{\\top}(X^{\\top}X)^{-1}x_0}} \\sim t_{n-p} $$\nThis is the sampling distribution of the standardized prediction error.\n\nTo construct a two-sided $(1 - \\alpha)$ prediction interval for $y_0$, we use the quantiles of the $t_{n-p}$ distribution. Let $t_{\\alpha/2, n-p}$ be the value such that $P(T > t_{\\alpha/2, n-p}) = \\alpha/2$. Then:\n$$ P(-t_{\\alpha/2, n-p} \\le \\frac{y_0 - \\hat{y}_0}{\\hat{\\sigma} \\sqrt{1 + x_0^{\\top}(X^{\\top}X)^{-1}x_0}} \\le t_{\\alpha/2, n-p}) = 1 - \\alpha $$\nIsolating $y_0$ in the center of the inequality yields the prediction interval:\n$$ \\hat{y}_0 \\pm t_{\\alpha/2, n-p} \\hat{\\sigma} \\sqrt{1 + x_0^{\\top}(X^{\\top}X)^{-1}x_0} $$\nThe width, $W$, of this interval is the difference between the upper and lower bounds:\n$$ W = 2 \\cdot t_{\\alpha/2, n-p} \\cdot \\hat{\\sigma} \\cdot \\sqrt{1 + x_0^{\\top}(X^{\\top}X)^{-1}x_0} $$\nThis expression clearly decomposes the width into three key components as requested:\n1.  The Student's $t$-quantile, $t_{\\alpha/2, n-p}$, which depends on the confidence level $\\alpha$ and the degrees of freedom $n-p$.\n2.  The residual standard error, $\\hat{\\sigma} = \\sqrt{\\mathrm{RSS}/(n-p)}$, which estimates the intrinsic variability $\\sigma$ of the data.\n3.  The design-dependent factor, $\\sqrt{1+h_0} = \\sqrt{1 + x_0^{\\top}(X^{\\top}X)^{-1}x_0}$, which accounts for the uncertainty in estimating $\\beta$ and how it propagates to the prediction at $x_0$. It depends on the training design $X$ and the location of the new point $x_0$.\n\n**Part 2: Implementation for the Specified Design**\n\nThe problem asks for an implementation under a controlled setup. For simple linear regression ($p=2$), the design matrix $X$ has a first column of ones and a second column of centered predictors $x_i$ such that $\\sum_{i=1}^n x_i = 0$ and $\\sum_{i=1}^n x_i^2 = n$. For this specific design, the matrix $X^{\\top}X$ becomes diagonal:\n$$ X^{\\top}X = \\begin{pmatrix} \\sum 1 & \\sum x_i \\\\ \\sum x_i & \\sum x_i^2 \\end{pmatrix} = \\begin{pmatrix} n & 0 \\\\ 0 & n \\end{pmatrix} = nI_2 $$\nIts inverse is $(X^{\\top}X)^{-1} = \\frac{1}{n}I_2$.\nThe new design point is $x_0 = (1, 0)^{\\top}$. The leverage at this point is:\n$$ h_0 = x_0^{\\top}(X^{\\top}X)^{-1}x_0 = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1/n & 0 \\\\ 0 & 1/n \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\frac{1}{n} $$\nTherefore, the width formula simplifies for this specific case to:\n$$ W(n, \\alpha) = 2 \\cdot t_{\\alpha/2, n-2} \\cdot \\sqrt{\\frac{\\mathrm{RSS}}{n-2}} \\cdot \\sqrt{1 + \\frac{1}{n}} $$\nThe program will compute this width (or its proportional components) for sample sizes $n$ and $2n$ to find the required ratios. The ratio of widths $W_{2n} / W_n$ is:\n$$ r_{\\mathrm{full}} = \\frac{2 \\cdot t_{\\alpha/2, 2n-2} \\cdot \\sqrt{\\frac{\\mathrm{RSS}}{2n-2}} \\cdot \\sqrt{1 + \\frac{1}{2n}}}{2 \\cdot t_{\\alpha/2, n-2} \\cdot \\sqrt{\\frac{\\mathrm{RSS}}{n-2}} \\cdot \\sqrt{1 + \\frac{1}{n}}} = \\left(\\frac{t_{\\alpha/2, 2n-2}}{t_{\\alpha/2, n-2}}\\right) \\left(\\sqrt{\\frac{n-2}{2n-2}}\\right) \\left(\\sqrt{\\frac{1+1/(2n)}{1+1/n}}\\right) $$\nThe ratio $r_{t,\\sigma}$ is the product of the first two terms in the expression above, excluding the leverage-dependent term.\n$$ r_{t,\\sigma} = \\left(\\frac{t_{\\alpha/2, 2n-2}}{t_{\\alpha/2, n-2}}\\right) \\left(\\sqrt{\\frac{n-2}{2n-2}}\\right) $$\nThe following code implements these calculations for the given test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t\n\ndef solve():\n    \"\"\"\n    Calculates prediction interval width ratios for specified test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (baseline sample size n, significance level alpha)\n    test_cases = [\n        (3, 0.05),   # Case A\n        (6, 0.05),   # Case B\n        (20, 0.05),  # Case C\n        (50, 0.01)   # Case D\n    ]\n\n    # Fixed parameters from the problem statement\n    p = 2      # Number of parameters (intercept + 1 predictor)\n    RSS = 100.0  # Residual Sum of Squares\n\n    results = []\n\n    def calculate_width_components(n_sample, alpha_level):\n        \"\"\"\n        Calculates the components of the prediction interval width.\n        \n        Args:\n            n_sample (int): The sample size n.\n            alpha_level (float): The significance level alpha.\n            \n        Returns:\n            A tuple containing the t-quantile, residual standard error, and leverage factor.\n        \"\"\"\n        if n_sample <= p:\n            # Degrees of freedom must be positive.\n            return np.nan, np.nan, np.nan\n        \n        # Degrees of freedom for the t-distribution\n        df = n_sample - p\n        \n        # 1. Student's t-quantile\n        t_quantile = t.ppf(1 - alpha_level / 2, df)\n        \n        # 2. Residual standard error\n        sigma_hat = np.sqrt(RSS / df)\n        \n        # 3. Design dependence (leverage factor)\n        # For the given design, h_0 = 1/n.\n        h0 = 1 / n_sample\n        leverage_factor = np.sqrt(1 + h0)\n        \n        return t_quantile, sigma_hat, leverage_factor\n\n    for n_base, alpha in test_cases:\n        # Calculate components for the baseline sample size n\n        t_n, sigma_n, lev_n = calculate_width_components(n_base, alpha)\n        \n        # Calculate components for the doubled sample size 2n\n        n_doubled = 2 * n_base\n        t_2n, sigma_2n, lev_2n = calculate_width_components(n_doubled, alpha)\n        \n        # --- Calculate r_full ---\n        # The width is proportional to t_quantile * sigma_hat * leverage_factor.\n        # The constant factor of 2 cancels in the ratio.\n        width_prop_n = t_n * sigma_n * lev_n\n        width_prop_2n = t_2n * sigma_2n * lev_2n\n        \n        r_full = width_prop_2n / width_prop_n\n        \n        # --- Calculate |r_full - 0.5| ---\n        abs_dev_from_half = abs(r_full - 0.5)\n        \n        # --- Calculate r_t,sigma ---\n        # Ratio of factors depending only on t-quantile and sigma_hat\n        factor_ts_n = t_n * sigma_n\n        factor_ts_2n = t_2n * sigma_2n\n        \n        r_t_sigma = factor_ts_2n / factor_ts_n\n        \n        # Append the triple of results for the current test case.\n        results.append([r_full, abs_dev_from_half, r_t_sigma])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3160052"}, {"introduction": "Not all predictions are created equal; their certainty depends heavily on *where* in the predictor space we make them. This exercise focuses on the concept of leverage, $h_0 = x_0^{\\top}(X^{\\top}X)^{-1}x_0$, which quantifies the distance of a new point $x_0$ from the center of the training data. You will learn to identify the \"riskiest\" locations for prediction—those with the highest leverage—which result in the widest and least certain prediction intervals, a crucial skill for assessing model reliability [@problem_id:3160029].", "problem": "Consider the fixed-design linear regression model with a nonrandom design matrix $X \\in \\mathbb{R}^{n \\times p}$ and responses $y \\in \\mathbb{R}^{n}$. Assume the data are generated by the model $y = X\\beta + \\varepsilon$, where $\\beta \\in \\mathbb{R}^{p}$ is unknown and the noise vector $\\varepsilon \\in \\mathbb{R}^{n}$ has independent and identically distributed components with $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$ for some $\\sigma^2 > 0$. A new covariate vector for prediction is denoted by $x_0 \\in \\mathbb{R}^{p}$, and the corresponding new response is $y_0 = x_0^\\top \\beta + \\varepsilon_0$ with $\\varepsilon_0 \\sim \\mathcal{N}(0,\\sigma^2)$ independent of $\\varepsilon$.\n\nStarting from these assumptions and the definition of the Ordinary Least Squares (OLS) estimator, derive the two-sided prediction interval at significance level $\\alpha \\in (0,1)$ for $y_0$ at a given $x_0$, expressing its half-width as a function of the residual standard error, the degrees of freedom, and a scalar quantity that depends on $x_0$ and $X$. Show how the dependence on $x_0$ enters and identify the $x_0$ that maximizes the half-width under a given constraint set. Your derivation must start from the OLS normal equations and the distributional properties implied by the model and must not assume any shortcut formulas.\n\nYou will then implement a program that, for the test suite below, computes the following for each case:\n- The value identifying where the half-width of the two-sided prediction interval is maximized over the specified candidate set of $x_0$.\n- The corresponding maximizing value of the scalar quantity that depends on $x_0$ and $X$.\n- The corresponding maximum half-width of the prediction interval.\n\nFor all cases, use the Residual Standard Error (RSE), defined as $s = \\sqrt{\\mathrm{RSS}/(n - p)}$, where $\\mathrm{RSS}$ is the residual sum of squares computed from OLS. Use the Student’s $t$-distribution with $n - p$ degrees of freedom and significance level $\\alpha$, and express the confidence level as $1 - \\alpha$ (with $\\alpha$ provided in decimal form). The program must treat ties in the maximizing value according to the specified rule for each case.\n\nTest suite:\n- Case $1$ (simple linear regression with intercept, in-sample candidates): $X$ is an $n \\times p$ matrix with $n = 5$ and $p = 2$, whose first column is all ones (intercept) and second column is $[0,1,2,3,4]^\\top$. The response vector is $y = [2.3, 2.9, 3.1, 3.8, 4.4]^\\top$. The candidate set for $x_0$ consists of the in-sample design rows. Use $\\alpha = 0.10$. Tie-breaking rule: if multiple rows yield the same maximal value, return the smallest scalar $x_0$ (i.e., the smallest value in the second column of $X$).\n- Case $2$ (simple linear regression with intercept, interval-constrained candidates): Use the same $X$ and $y$ as in Case $1$. The candidate set for $x_0$ is constrained to the closed interval $[x_{\\min}, x_{\\max}] = [0,4]$ for the scalar predictor, with the design vector $[1, x_0]^\\top$. Use $\\alpha = 0.10$. Rule: choose the endpoint in $[0,4]$ that maximizes the half-width; if both endpoints yield the same value up to a numerical tolerance of $10^{-12}$, return the smaller endpoint.\n- Case $3$ (multiple linear regression with intercept, in-sample candidates): $X$ is an $n \\times p$ matrix with $n = 6$ and $p = 3$, first column all ones (intercept), second column $x_1 = [0,1,2,-1,-2,3]^\\top$, third column $x_2 = [0,2,-1,1,-2,4]^\\top$. The response vector is $y = [1.6, 1.65, 3.4, 0.6, 0.55, 2.6]^\\top$. The candidate set for $x_0$ consists of the in-sample design rows. Use $\\alpha = 0.05$. Tie-breaking rule: if multiple rows yield the same maximal value, return the smallest zero-based row index among the candidates.\n\nProgram requirements:\n- Compute the OLS estimator $\\hat{\\beta}$ using $X$ and $y$.\n- Compute the Residual Standard Error $s$ and degrees of freedom $n - p$.\n- For each candidate $x_0$, compute the scalar quantity that governs the dependence of the prediction interval half-width on $x_0$ and $X$, and then compute the half-width at significance level $\\alpha$.\n- Apply the case-specific tie-breaking rule to identify the maximizing $x_0$.\n- For Case $1$, output the scalar $x_0$ (the value in the second column of $X$) that maximizes the half-width, the corresponding scalar quantity, and the corresponding maximum half-width, as a list of three real numbers.\n- For Case $2$, output the endpoint scalar $x_0 \\in \\{0,4\\}$ chosen by the rule, the corresponding scalar quantity, and the corresponding maximum half-width, as a list of three real numbers.\n- For Case $3$, output the zero-based row index of the in-sample design row that maximizes the half-width, the corresponding scalar quantity, and the corresponding maximum half-width, as a list where the first element is an integer and the remaining elements are real numbers.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets; each case’s result must itself be a list as specified. For example, the output must look like $[[r_{1,1},r_{1,2},r_{1,3}],[r_{2,1},r_{2,2},r_{2,3}],[r_{3,1},r_{3,2},r_{3,3}]]$, with no spaces.", "solution": "The problem requires the derivation of a two-sided prediction interval for a new observation in a fixed-design linear regression model and the subsequent application of this result to specific numerical cases. The derivation must originate from the fundamental principles of Ordinary Least Squares (OLS) and the distributional assumptions of the model.\n\nLet the linear regression model be specified as $y = X\\beta + \\varepsilon$, where $y \\in \\mathbb{R}^{n}$ is the vector of responses, $X \\in \\mathbb{R}^{n \\times p}$ is the nonrandom design matrix of full column rank, $\\beta \\in \\mathbb{R}^{p}$ is the vector of unknown coefficients, and $\\varepsilon \\in \\mathbb{R}^{n}$ is the vector of unobservable errors. The errors are assumed to be independent and identically distributed (i.i.d.) as $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$, which implies the vector $\\varepsilon$ follows a multivariate normal distribution $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$, where $I_n$ is the $n \\times n$ identity matrix.\n\nA new observation is generated from the same process, with a new covariate vector $x_0 \\in \\mathbb{R}^{p}$ and a corresponding response $y_0 = x_0^\\top \\beta + \\varepsilon_0$, where $\\varepsilon_0 \\sim \\mathcal{N}(0,\\sigma^2)$ is independent of the original error vector $\\varepsilon$.\n\n**Step 1: Ordinary Least Squares (OLS) Estimator**\nThe OLS estimator $\\hat{\\beta}$ is found by minimizing the Residual Sum of Squares (RSS), defined as the squared Euclidean norm of the residual vector $e = y - X\\beta$:\n$$ \\mathrm{RSS}(\\beta) = e^\\top e = (y - X\\beta)^\\top(y - X\\beta) = y^\\top y - 2\\beta^\\top X^\\top y + \\beta^\\top X^\\top X \\beta $$\nTo find the minimum, we differentiate $\\mathrm{RSS}(\\beta)$ with respect to $\\beta$ and set the gradient to zero:\n$$ \\frac{\\partial \\mathrm{RSS}}{\\partial \\beta} = -2X^\\top y + 2X^\\top X \\beta = 0 $$\nThis yields the normal equations:\n$$ (X^\\top X)\\hat{\\beta} = X^\\top y $$\nAssuming $X$ has full column rank, the matrix $X^\\top X$ is invertible. The OLS estimator for $\\beta$ is thus unique and given by:\n$$ \\hat{\\beta} = (X^\\top X)^{-1}X^\\top y $$\n\n**Step 2: Distribution of the OLS Estimator**\nSince $\\hat{\\beta}$ is a linear transformation of the normally distributed vector $y$, $\\hat{\\beta}$ is also normally distributed.\nThe expected value of $\\hat{\\beta}$ is:\n$$ E[\\hat{\\beta}] = E[(X^\\top X)^{-1}X^\\top y] = (X^\\top X)^{-1}X^\\top E[y] = (X^\\top X)^{-1}X^\\top(X\\beta) = \\beta $$\nThis shows that $\\hat{\\beta}$ is an unbiased estimator of $\\beta$.\nThe covariance matrix of $\\hat{\\beta}$ is:\n$$ \\mathrm{Cov}(\\hat{\\beta}) = \\mathrm{Cov}((X^\\top X)^{-1}X^\\top y) = (X^\\top X)^{-1}X^\\top \\mathrm{Cov}(y) (X^\\top X)^{-1} = (X^\\top X)^{-1}X^\\top (\\sigma^2 I_n) X (X^\\top X)^{-1} = \\sigma^2 (X^\\top X)^{-1} $$\nTherefore, the distribution of the OLS estimator is $\\hat{\\beta} \\sim \\mathcal{N}(\\beta, \\sigma^2(X^\\top X)^{-1})$.\n\n**Step 3: Prediction Error**\nOur goal is to predict the new response $y_0$. The natural predictor is $\\hat{y}_0 = x_0^\\top \\hat{\\beta}$. The prediction error is the difference between the actual future response $y_0$ and our prediction $\\hat{y}_0$:\n$$ e_0 = y_0 - \\hat{y}_0 = (x_0^\\top \\beta + \\varepsilon_0) - x_0^\\top \\hat{\\beta} = x_0^\\top(\\beta - \\hat{\\beta}) + \\varepsilon_0 $$\nThis error is a sum of normally distributed random variables, so it is also normally distributed.\nThe expected value of the prediction error is:\n$$ E[e_0] = E[x_0^\\top(\\beta - \\hat{\\beta})] + E[\\varepsilon_0] = x_0^\\top(E[\\beta] - E[\\hat{\\beta}]) + 0 = x_0^\\top(\\beta - \\beta) = 0 $$\nThe variance of the prediction error is computed by noting that $\\hat{\\beta}$ (which depends on $\\varepsilon$) and $\\varepsilon_0$ are independent:\n$$ \\mathrm{Var}(e_0) = \\mathrm{Var}(x_0^\\top(\\beta - \\hat{\\beta}) + \\varepsilon_0) = \\mathrm{Var}(x_0^\\top(\\hat{\\beta} - \\beta)) + \\mathrm{Var}(\\varepsilon_0) $$\nThe first term is the variance of the predictor $\\hat{y}_0$:\n$$ \\mathrm{Var}(x_0^\\top \\hat{\\beta}) = x_0^\\top \\mathrm{Cov}(\\hat{\\beta}) x_0 = x_0^\\top (\\sigma^2 (X^\\top X)^{-1}) x_0 = \\sigma^2 x_0^\\top(X^\\top X)^{-1}x_0 $$\nThe second term is the variance of the new error: $\\mathrm{Var}(\\varepsilon_0) = \\sigma^2$.\nCombining them, the total variance of the prediction error is:\n$$ \\mathrm{Var}(y_0 - \\hat{y}_0) = \\sigma^2 x_0^\\top(X^\\top X)^{-1}x_0 + \\sigma^2 = \\sigma^2 \\left(1 + x_0^\\top(X^\\top X)^{-1}x_0\\right) $$\nThus, the prediction error follows the distribution $y_0 - \\hat{y}_0 \\sim \\mathcal{N}\\left(0, \\sigma^2 \\left(1 + x_0^\\top(X^\\top X)^{-1}x_0\\right)\\right)$.\n\n**Step 4: Constructing the Pivotal Quantity**\nThe variance of the prediction error depends on the unknown parameter $\\sigma^2$. We must estimate it. The standard unbiased estimator for $\\sigma^2$ is $s^2 = \\frac{\\mathrm{RSS}}{n-p}$, where $\\mathrm{RSS} = (y - X\\hat{\\beta})^\\top(y - X\\hat{\\beta})$ is the residual sum of squares and $n-p$ are the degrees of freedom. The square root of this quantity, $s$, is the Residual Standard Error (RSE).\nA key result in linear model theory (from Cochran's theorem) states that:\n$$ \\frac{\\mathrm{RSS}}{\\sigma^2} = \\frac{(n-p)s^2}{\\sigma^2} \\sim \\chi^2_{n-p} $$\nwhere $\\chi^2_{n-p}$ is the chi-squared distribution with $n-p$ degrees of freedom. Furthermore, $\\hat{\\beta}$ and RSS are independent.\n\nWe can now construct a pivotal quantity (a function of observables and parameters whose distribution is independent of the parameters). We standardize the prediction error using the true standard deviation to get a standard normal variable, and divide it by the square root of the independent scaled chi-squared variable:\n$$ T = \\frac{(y_0 - \\hat{y}_0) / \\left(\\sigma \\sqrt{1 + x_0^\\top(X^\\top X)^{-1}x_0}\\right)}{\\sqrt{ \\frac{(n-p)s^2}{\\sigma^2} / (n-p) }} = \\frac{(y_0 - \\hat{y}_0) / \\left(\\sigma \\sqrt{1 + x_0^\\top(X^\\top X)^{-1}x_0}\\right)}{s/\\sigma} $$\nThe unknown $\\sigma$ terms cancel, yielding:\n$$ T = \\frac{y_0 - \\hat{y}_0}{s \\sqrt{1 + x_0^\\top(X^\\top X)^{-1}x_0}} $$\nThis quantity follows Student's $t$-distribution with $n-p$ degrees of freedom, i.e., $T \\sim t_{n-p}$.\n\n**Step 5: Derivation of the Prediction Interval**\nA $100(1-\\alpha)\\%$ prediction interval for $y_0$ is constructed from the pivotal quantity $T$. Let $t_{n-p, \\alpha/2}$ be the critical value from the $t_{n-p}$ distribution such that $P(T > t_{n-p, \\alpha/2}) = \\alpha/2$. By symmetry, $P(|T| \\le t_{n-p, \\alpha/2}) = 1-\\alpha$.\n$$ P\\left(-t_{n-p, \\alpha/2} \\le \\frac{y_0 - \\hat{y}_0}{s \\sqrt{1 + x_0^\\top(X^\\top X)^{-1}x_0}} \\le t_{n-p, \\alpha/2}\\right) = 1-\\alpha $$\nIsolating $y_0$ in the center of the inequality gives:\n$$ \\hat{y}_0 - t_{n-p, \\alpha/2} \\cdot s \\sqrt{1 + x_0^\\top(X^\\top X)^{-1}x_0} \\le y_0 \\le \\hat{y}_0 + t_{n-p, \\alpha/2} \\cdot s \\sqrt{1 + x_0^\\top(X^\\top X)^{-1}x_0} $$\nThe two-sided prediction interval for $y_0$ is:\n$$ \\hat{y}_0 \\pm t_{n-p, \\alpha/2} \\cdot s \\sqrt{1 + x_0^\\top(X^\\top X)^{-1}x_0} $$\n\n**Step 6: Analysis of the Prediction Interval Half-Width**\nThe half-width ($HW$) of this interval is:\n$$ HW(x_0) = t_{n-p, \\alpha/2} \\cdot s \\cdot \\sqrt{1 + x_0^\\top(X^\\top X)^{-1}x_0} $$\nFor a given dataset and regression model, the values of $s$, $n$, $p$, and thus $t_{n-p, \\alpha/2}$ are fixed. The half-width's dependence on the new covariate vector $x_0$ is entirely contained in the term $x_0^\\top(X^\\top X)^{-1}x_0$. Let us define this scalar quantity as $h_0(x_0) = x_0^\\top(X^\\top X)^{-1}x_0$.\n\nTo maximize the half-width of the prediction interval, we must maximize $h_0(x_0)$ over the specified candidate set for $x_0$. The quantity $h_0(x_0)$ is a measure of leverage. It can be interpreted as a squared statistical distance (specifically, a Mahalanobis distance) of the point $x_0$ from the center of the data points in $X$, scaled by the covariance structure of $X$. Points $x_0$ that are \"far\" from the center of the existing data have higher leverage and result in wider, less certain prediction intervals.\n\n**Algorithmic Plan:**\nFor each test case:\n1.  Construct the design matrix $X$ and response vector $y$.\n2.  Compute the number of observations $n$ and parameters $p$.\n3.  Calculate the OLS estimate $\\hat{\\beta} = (X^\\top X)^{-1}X^\\top y$.\n4.  Calculate the residuals $e = y - X\\hat{\\beta}$ and the RSS, $\\mathrm{RSS} = e^\\top e$.\n5.  Calculate the degrees of freedom $df = n - p$ and the RSE, $s = \\sqrt{\\mathrm{RSS}/df}$.\n6.  Determine the critical t-value, $t_{df, \\alpha/2}$, for the given significance level $\\alpha$.\n7.  Pre-compute the matrix $(X^\\top X)^{-1}$.\n8.  Iterate through the specified candidate set of vectors $x_0$:\n    a. For each candidate $x_0$, calculate the scalar quantity $h_0 = x_0^\\top(X^\\top X)^{-1}x_0$.\n9.  Identify the candidate $x_0^*$ that maximizes $h_0$, applying the specified tie-breaking rule.\n10. For this maximizing candidate $x_0^*$, calculate the maximum half-width $HW_{max} = t_{df, \\alpha/2} \\cdot s \\cdot \\sqrt{1 + h_0(x_0^*)}$.\n11. Assemble and report the identifier for $x_0^*$, the maximizing value $h_0(x_0^*)$, and the maximum half-width $HW_{max}$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t\n\ndef solve():\n    \"\"\"\n    Solves the prediction interval maximization problem for three test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"case_id\": 1,\n            \"X_def\": {\n                \"cols\": [np.ones(5), np.array([0, 1, 2, 3, 4])]\n            },\n            \"y\": np.array([2.3, 2.9, 3.1, 3.8, 4.4]),\n            \"alpha\": 0.10,\n            \"candidates_type\": \"in-sample\",\n            \"tie_break_rule\": \"smallest_scalar_x0\"\n        },\n        {\n            \"case_id\": 2,\n            \"X_def\": {\n                \"cols\": [np.ones(5), np.array([0, 1, 2, 3, 4])]\n            },\n            \"y\": np.array([2.3, 2.9, 3.1, 3.8, 4.4]),\n            \"alpha\": 0.10,\n            \"candidates_type\": \"interval\",\n            \"candidate_interval\": [0, 4],\n            \"tie_break_rule\": \"smaller_endpoint\"\n        },\n        {\n            \"case_id\": 3,\n            \"X_def\": {\n                \"cols\": [np.ones(6), np.array([0, 1, 2, -1, -2, 3]), np.array([0, 2, -1, 1, -2, 4])]\n            },\n            \"y\": np.array([1.6, 1.65, 3.4, 0.6, 0.55, 2.6]),\n            \"alpha\": 0.05,\n            \"candidates_type\": \"in-sample\",\n            \"tie_break_rule\": \"smallest_row_index\"\n        }\n    ]\n\n    results = []\n\n    for case in test_cases:\n        # Step 1: Unpack case data and construct X matrix\n        X = np.stack(case[\"X_def\"][\"cols\"], axis=1)\n        y = case[\"y\"]\n        alpha = case[\"alpha\"]\n        n, p = X.shape\n\n        # Step 2-5: Perform OLS regression and compute key statistics\n        # (X^T X)^-1\n        try:\n            X_t_X_inv = np.linalg.inv(X.T @ X)\n        except np.linalg.LinAlgError:\n            # Handle cases where X is not full rank, though not expected here.\n            results.append([\"error\", \"singular_matrix\", \"n/a\"])\n            continue\n        \n        # OLS estimator beta_hat\n        beta_hat = X_t_X_inv @ X.T @ y\n        \n        # Residuals and RSS\n        residuals = y - (X @ beta_hat)\n        rss = residuals.T @ residuals\n        \n        # Degrees of freedom and Residual Standard Error (RSE)\n        df = n - p\n        rse = np.sqrt(rss / df)\n        \n        # Step 6: Get critical t-value\n        t_crit = t.ppf(1 - alpha / 2, df)\n\n        # Step 7-11: Find the maximizing candidate x0 and compute results\n        if case[\"candidates_type\"] == \"in-sample\":\n            # For in-sample candidates, the scalar quantity h_0 is the diagonal of the hat matrix\n            hat_matrix_diag = np.einsum('ij,ji->i', X, (X_t_X_inv @ X.T))\n\n            max_h0 = -1.0\n            best_identifier = -1\n            \n            # Find all candidates that achieve the maximum h0\n            max_h0_val = np.max(hat_matrix_diag)\n            \n            candidate_indices = np.where(np.isclose(hat_matrix_diag, max_h0_val))[0]\n            \n            # Apply tie-breaking rule\n            if case[\"tie_break_rule\"] == \"smallest_scalar_x0\":\n                # Assumes the scalar is in the second column (index 1)\n                candidate_scalars = X[candidate_indices, 1]\n                best_identifier = np.min(candidate_scalars)\n            elif case[\"tie_break_rule\"] == \"smallest_row_index\":\n                best_identifier = np.min(candidate_indices)\n                \n            max_h0 = max_h0_val\n\n\n        elif case[\"candidates_type\"] == \"interval\":\n            # For a convex quadratic, max on an interval is at an endpoint.\n            xmin, xmax = case[\"candidate_interval\"]\n            x0_min = np.array([1, xmin])\n            x0_max = np.array([1, xmax])\n            \n            h0_min = x0_min.T @ X_t_X_inv @ x0_min\n            h0_max_val = x0_max.T @ X_t_X_inv @ x0_max\n\n            # Apply tie-breaking rule\n            if abs(h0_max_val - h0_min) < 1e-12: # Tolerance for float comparison\n                best_identifier = float(xmin)\n                max_h0 = h0_min\n            elif h0_max_val > h0_min:\n                best_identifier = float(xmax)\n                max_h0 = h0_max_val\n            else:\n                best_identifier = float(xmin)\n                max_h0 = h0_min\n        \n        # Calculate maximum half-width\n        max_half_width = t_crit * rse * np.sqrt(1 + max_h0)\n\n        if case[\"case_id\"] == 3:\n            results.append([int(best_identifier), max_h0, max_half_width])\n        else:\n            results.append([float(best_identifier), max_h0, max_half_width])\n\n    # Format the final output string without spaces\n    outer_list_str = []\n    for res_list in results:\n        inner_list_str = f\"[{','.join(map(str, res_list))}]\"\n        outer_list_str.append(inner_list_str)\n    final_output = f\"[{','.join(outer_list_str)}]\"\n    \n    print(final_output)\n\nsolve()\n```", "id": "3160029"}, {"introduction": "A model is only as good as the data it's built on, and this is especially true for our estimates of uncertainty. This final practice demonstrates how a single influential observation can dramatically alter a model's estimate of the error variance, $\\hat{\\sigma}^2$, and consequently, the width of all its prediction intervals. By using a standard diagnostic tool, Cook's distance, to identify and remove influential points, you will gain a practical understanding of model robustness and the critical importance of diagnostic checking before deploying a model for prediction [@problem_id:3160002].", "problem": "You are given three self-contained simple linear regression scenarios. In each scenario, the data arise from the model $y = \\beta_0 + \\beta_1 x + \\varepsilon$, where the error term $\\varepsilon$ is assumed to be independent and identically distributed with mean $0$ and constant variance $\\sigma^2$. For each scenario, you must fit the ordinary least squares model with an intercept, compute the residual standard error $\\hat{\\sigma}$, identify influential observations using Cook’s distance, and study how removing each influential observation (one at a time) affects the widths of two-sided prediction intervals for a future observation at three typical design points. Your program must be a complete, runnable program that uses only the specified runtime environment. All computations must be carried out in purely mathematical terms, with no physical units. Angles are not involved. Express any confidence level as a decimal, not using a percentage sign.\n\nBase principles to use without quoting shortcut formulas in the problem statement: ordinary least squares minimizes the residual sum of squares; the residual standard error is computed from the residual sum of squares and appropriate degrees of freedom; leverage values arise from the hat matrix defined by the design matrix; Cook’s distance measures the influence of each observation on the fitted regression; two-sided prediction intervals for a future observation under the assumption of normally distributed errors are constructed using the Student’s $t$ distribution and account for both estimation uncertainty and irreducible error.\n\nImplement the following steps for each scenario:\n- Fit the ordinary least squares model with an intercept to obtain $\\hat{\\beta}_0$, $\\hat{\\beta}_1$, and residuals.\n- Compute the residual standard error $\\hat{\\sigma}$ from the fitted residuals and the degrees of freedom $n - p$, where $n$ is the sample size and $p$ is the number of fitted parameters including the intercept.\n- Compute the hat matrix diagonal entries (leverages) and Cook’s distance for each observation. Identify as influential those observations whose Cook’s distance exceeds the specified threshold $\\tau$ for that scenario.\n- Define three typical design points as the minimum of $x$, the mean of $x$, and the maximum of $x$. For each fitted model, compute the two-sided prediction interval width at each of these three design points using a confidence level $1 - \\alpha$ (with $\\alpha$ given below). Then aggregate these three widths by taking their average to produce a single width summary for that model.\n- For each scenario, report a list of floats consisting of the baseline average width (using all data) followed by the average widths after removing each influential point one at a time (remove exactly one influential observation at a time, refit, and recompute). If there are no influential points, the list contains only the baseline average width.\n\nUse the following test suite. In each case, the arrays are to be interpreted elementwise, and all numbers are exact constants.\n\nScenario A:\n- Inputs:\n  - $x = [0,1,2,3,4,5,6,7,8,9,10,11]$.\n  - Define $y$ by $y_i = 2 + 1.3 x_i + r_i$ with residuals $r = [0.2,-0.1,0.0,0.1,-0.2,0.05,-0.15,0.1,-0.05,0.1,16.0,-0.1]$.\n  - Confidence parameter $\\alpha = 0.05$.\n  - Cook’s distance threshold rule $\\tau = 4/n$, where $n$ is the number of observations in this scenario.\n- Output for Scenario A: a list of floats $[w_0, w_1, \\dots]$ where $w_0$ is the baseline average width over the three typical design points, and $w_j$ for $j \\ge 1$ is the average width after removing the $j$-th influential observation (with influential observation indices ordered increasingly by their original index).\n\nScenario B:\n- Inputs:\n  - $x = [0,1,2,3,4,5,6,7,8,9]$.\n  - Define $y$ by $y_i = 1 + 2 x_i + r_i$ with residuals $r = [0.02,-0.03,0.01,0.00,-0.02,0.01,-0.01,0.02,0.00,-0.02]$.\n  - Confidence parameter $\\alpha = 0.05$.\n  - Cook’s distance threshold $\\tau = 10$.\n- Output for Scenario B: a list of floats as above. If no influential points are detected, return only the baseline average width.\n\nScenario C:\n- Inputs:\n  - $x = [-5,-3,-1,0,1,2,3,4,5,20,-10,6,7,8,9]$.\n  - Define $y$ by $y_i = 0.5 + 0.8 x_i + r_i$ with residuals $r = [0.1,-0.1,0.05,0.0,0.02,-0.03,0.04,-0.02,0.01,-11.5,12.5,-0.05,0.03,-0.04,0.02]$.\n  - Confidence parameter $\\alpha = 0.05$.\n  - Cook’s distance threshold rule $\\tau = 4/n$, where $n$ is the number of observations in this scenario.\n- Output for Scenario C: a list of floats as above.\n\nFinal output format:\n- Your program should produce a single line of output containing the three scenario results as a comma-separated list of lists enclosed in square brackets, for example, $[[a_1,a_2],[b_1],[c_1,c_2,c_3]]$, with each float rounded to $6$ decimal places using standard rounding to nearest. The three lists must appear in the order Scenario A, Scenario B, Scenario C. No other text should be printed.", "solution": "The problem requires an analysis of simple linear regression models for three distinct scenarios. The core tasks involve fitting the model, identifying influential data points, and assessing their impact on the width of prediction intervals. The methodology is grounded in the principles of ordinary least squares (OLS) regression and standard statistical diagnostics.\n\n**1. Simple Linear Regression Model**\nThe underlying statistical model is a simple linear regression:\n$$y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$$\nwhere $y_i$ is the response variable for the $i$-th observation, $x_i$ is the predictor variable, $\\beta_0$ is the intercept, $\\beta_1$ is the slope, and $\\varepsilon_i$ are independent and identically distributed random errors with mean $E[\\varepsilon_i] = 0$ and constant variance $\\text{Var}(\\varepsilon_i) = \\sigma^2$. The problem provides the data pairs $(x_i, y_i)$ for each scenario.\n\n**2. Ordinary Least Squares (OLS) Estimation**\nThe OLS method estimates the parameters $\\beta_0$ and $\\beta_1$ by minimizing the Residual Sum of Squares (RSS). The estimated parameters, denoted $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$, are given by:\n$$ \\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} $$\n$$ \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x} $$\nwhere $n$ is the number of observations, $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i$ is the sample mean of the predictor variable, and $\\bar{y} = \\frac{1}{n}\\sum_{i=1}^n y_i$ is the sample mean of the response variable.\n\nThe fitted values are $\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i$, and the residuals are $e_i = y_i - \\hat{y}_i$.\n\n**3. Residual Standard Error (RSE)**\nThe RSE, denoted $\\hat{\\sigma}$, is an estimate of the standard deviation of the error term, $\\sigma$. It is calculated from the RSS:\n$$ \\text{RSS} = \\sum_{i=1}^n e_i^2 = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 $$\nThe RSE is then computed as:\n$$ \\hat{\\sigma} = \\sqrt{\\frac{\\text{RSS}}{n-p}} $$\nwhere the degrees of freedom are $n-p$. For simple linear regression with an intercept, the number of estimated parameters is $p=2$ (for $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$), so the degrees of freedom are $n-2$.\n\n**4. Influence Diagnostics**\nTo identify influential observations, we use Cook's distance. This metric combines information about a point's leverage and its residual size.\n\nFirst, the leverage $h_{ii}$ for each observation $i$ is calculated. Leverage measures how far an observation's predictor value $x_i$ is from the mean of the predictor values. A high-leverage point has the potential to strongly influence the regression fit. The leverage is the $i$-th diagonal element of the hat matrix $H = X(X^T X)^{-1} X^T$, where $X$ is the design matrix. For simple linear regression, this simplifies to:\n$$ h_{ii} = \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum_{j=1}^n (x_j - \\bar{x})^2} $$\n\nSecond, Cook's distance $D_i$ is computed for each observation:\n$$ D_i = \\frac{e_i^2}{p \\cdot \\hat{\\sigma}^2} \\left[ \\frac{h_{ii}}{(1-h_{ii})^2} \\right] $$\nAn observation is identified as influential if its Cook's distance exceeds a given threshold, $D_i > \\tau$. In this problem, the threshold $\\tau$ is either a fixed value or a function of the sample size $n$, such as $\\tau = 4/n$.\n\n**5. Prediction Intervals (PI)**\nA prediction interval provides a range within which a future observation $y_0$ is expected to fall for a given predictor value $x_0$, with a certain confidence level $1-\\alpha$. Assuming the errors $\\varepsilon_i$ are normally distributed, the $100(1-\\alpha)\\%$ PI is constructed as:\n$$ \\hat{y}_0 \\pm t_{n-p, \\alpha/2} \\cdot \\hat{\\sigma} \\sqrt{1 + \\frac{1}{n} + \\frac{(x_0 - \\bar{x})^2}{\\sum_{j=1}^n (x_j - \\bar{x})^2}} $$\nwhere $\\hat{y}_0 = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_0$ is the predicted value at $x_0$, and $t_{n-p, \\alpha/2}$ is the critical value from the Student's $t$ distribution with $n-p$ degrees of freedom that leaves an area of $\\alpha/2$ in the upper tail. The standard error of prediction incorporates both the uncertainty in estimating the regression line and the inherent variability of a new observation ($\\sigma^2$).\n\nThe width of this interval is:\n$$ W(x_0) = 2 \\cdot t_{n-p, \\alpha/2} \\cdot \\hat{\\sigma} \\sqrt{1 + \\frac{1}{n} + \\frac{(x_0 - \\bar{x})^2}{\\sum_{j=1}^n (x_j - \\bar{x})^2}} $$\n\n**6. Algorithmic Implementation**\nFor each scenario, the following procedure is executed:\n1.  **Data Generation**: Construct the vectors $x$ and $y$ as specified in the problem statement.\n2.  **Baseline Analysis**:\n    a. Fit a simple linear regression model to the full dataset $(x, y)$ to obtain $\\hat{\\beta}_0$, $\\hat{\\beta}_1$, $\\hat{\\sigma}$, and other model statistics.\n    b. Compute the PI widths $W(x_0)$ at three specific design points: $x_{0, \\text{min}} = \\min(x)$, $x_{0, \\text{mean}} = \\bar{x}$, and $x_{0, \\text{max}} = \\max(x)$.\n    c. Calculate the baseline average width, $w_0$, by averaging the three widths. This is the first value in the result list for the scenario.\n3.  **Influence Analysis**:\n    a. Using the full model, calculate the leverages $h_{ii}$ and Cook's distances $D_i$ for all observations.\n    b. Identify the set of influential points by collecting the indices $i$ for which $D_i > \\tau$. These indices are sorted in increasing order.\n4.  **Sensitivity Analysis**:\n    a. If no influential points are found, the process for the scenario terminates, and the result is simply $[w_0]$.\n    b. If influential points are found, iterate through their sorted indices. For each influential point, remove it from the dataset, creating a new, smaller dataset.\n    c. For each new dataset, refit the regression model and re-calculate the average PI width ($w_j$) using the same procedure as in step 2 (using the new dataset's properties: $n_{new}$, $\\bar{x}_{new}$, etc.).\n    d. Append each of these new average widths ($w_1, w_2, \\dots$) to the result list.\n5.  **Final Output**: Collate the lists of widths from all scenarios and format them into the specified final output string.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import t\n\ndef solve():\n    \"\"\"\n    Main function to solve the three regression scenarios and print the results.\n    \"\"\"\n\n    def fit_and_get_stats(x, y):\n        \"\"\"\n        Fits a simple linear regression model and computes relevant statistics.\n        Returns a dictionary of statistics or None if fitting is not possible.\n        \"\"\"\n        n = len(x)\n        if n <= 2:  # Cannot fit or compute RSE with <= 2 points\n            return None\n        \n        x_mean = np.mean(x)\n        y_mean = np.mean(y)\n        \n        S_xx = np.sum((x - x_mean)**2)\n        if S_xx == 0:  # Avoid division by zero if all x are the same\n            return None\n            \n        S_xy = np.sum((x - x_mean) * (y - y_mean))\n        \n        beta1_hat = S_xy / S_xx\n        beta0_hat = y_mean - beta1_hat * x_mean\n        \n        y_hat = beta0_hat + beta1_hat * x\n        residuals = y - y_hat\n        rss = np.sum(residuals**2)\n        p = 2  # Number of parameters (intercept and slope)\n        dof = n - p\n        rse = np.sqrt(rss / dof)\n        \n        leverages = 1/n + (x - x_mean)**2 / S_xx\n        cooks_d = (residuals**2 / (p * rse**2)) * (leverages / (1 - leverages)**2)\n        \n        return {\n            'n': n,\n            'dof': dof,\n            'rse': rse,\n            'cooks_d': cooks_d,\n            'x_mean': x_mean,\n            'S_xx': S_xx\n        }\n\n    def get_avg_pi_width(stats, x_coords, alpha_val):\n        \"\"\"\n        Calculates the average prediction interval width for a given fitted model.\n        \"\"\"\n        if stats is None:\n            return np.nan\n        \n        n = stats['n']\n        dof = stats['dof']\n        rse = stats['rse']\n        x_mean = stats['x_mean']\n        S_xx = stats['S_xx']\n        \n        if S_xx == 0:\n            return np.nan\n\n        t_crit = t.ppf(1 - alpha_val / 2, dof)\n        \n        x0_points = [np.min(x_coords), np.mean(x_coords), np.max(x_coords)]\n        \n        widths = []\n        for x0 in x0_points:\n            se_pred_factor = np.sqrt(1 + 1/n + (x0 - x_mean)**2 / S_xx)\n            width = 2 * t_crit * rse * se_pred_factor\n            widths.append(width)\n            \n        return np.mean(widths)\n\n    def process_scenario(x_data, y_data, alpha, tau_val):\n        \"\"\"\n        Runs the full analysis for a single scenario.\n        \"\"\"\n        results_for_scenario = []\n        \n        # 1. Baseline analysis on the full dataset\n        baseline_stats = fit_and_get_stats(x_data, y_data)\n        baseline_avg_width = get_avg_pi_width(baseline_stats, x_data, alpha)\n        results_for_scenario.append(baseline_avg_width)\n        \n        # 2. Identify influential points\n        cooks_distances = baseline_stats['cooks_d']\n        influential_indices = np.where(cooks_distances > tau_val)[0]\n        # np.where returns sorted indices, so no extra sorting is needed.\n        \n        # 3. Re-run analysis after removing each influential point one-by-one\n        for idx in influential_indices:\n            x_new = np.delete(x_data, idx)\n            y_new = np.delete(y_data, idx)\n            \n            new_stats = fit_and_get_stats(x_new, y_new)\n            new_avg_width = get_avg_pi_width(new_stats, x_new, alpha)\n            results_for_scenario.append(new_avg_width)\n            \n        return results_for_scenario\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"x_raw\": [0,1,2,3,4,5,6,7,8,9,10,11],\n            \"y_def\": lambda x, r: 2 + 1.3 * x + r,\n            \"r_raw\": [0.2,-0.1,0.0,0.1,-0.2,0.05,-0.15,0.1,-0.05,0.1,16.0,-0.1],\n            \"alpha\": 0.05,\n            \"tau_rule\": lambda n: 4/n\n        },\n        {\n            \"x_raw\": [0,1,2,3,4,5,6,7,8,9],\n            \"y_def\": lambda x, r: 1 + 2 * x + r,\n            \"r_raw\": [0.02,-0.03,0.01,0.00,-0.02,0.01,-0.01,0.02,0.00,-0.02],\n            \"alpha\": 0.05,\n            \"tau_rule\": lambda n: 10\n        },\n        {\n            \"x_raw\": [-5,-3,-1,0,1,2,3,4,5,20,-10,6,7,8,9],\n            \"y_def\": lambda x, r: 0.5 + 0.8 * x + r,\n            \"r_raw\": [0.1,-0.1,0.05,0.0,0.02,-0.03,0.04,-0.02,0.01,-11.5,12.5,-0.05,0.03,-0.04,0.02],\n            \"alpha\": 0.05,\n            \"tau_rule\": lambda n: 4/n\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        x = np.array(case['x_raw'], dtype=float)\n        r = np.array(case['r_raw'], dtype=float)\n        y = case['y_def'](x, r)\n        alpha = case['alpha']\n        n = len(x)\n        tau = case['tau_rule'](n)\n        \n        scenario_results = process_scenario(x, y, alpha, tau)\n        all_results.append(scenario_results)\n\n    # Format the final output string as specified\n    output_parts = []\n    for res_list in all_results:\n        formatted_list = [f\"{x:.6f}\" for x in res_list]\n        output_parts.append(f\"[{','.join(formatted_list)}]\")\n    \n    final_output_string = f\"[{','.join(output_parts)}]\"\n    print(final_output_string)\n\nsolve()\n```", "id": "3160002"}]}