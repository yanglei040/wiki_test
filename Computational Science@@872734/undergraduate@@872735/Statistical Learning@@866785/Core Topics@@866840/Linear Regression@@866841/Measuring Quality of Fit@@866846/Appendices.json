{"hands_on_practices": [{"introduction": "In real-world classification tasks, class imbalance is the rule, not the exception. A model that simply predicts the majority class can achieve high accuracy while being useless in practice. This exercise challenges you to move beyond simplistic metrics by calculating and interpreting macro- and micro-averaged $F_1$-scores, revealing how different averaging schemes can tell vastly different stories about a model's performance on minority versus majority classes [@problem_id:3147853]. You will also explore how log-loss penalizes overconfident errors, providing a crucial perspective on the quality of a model's probability estimates.", "problem": "A single-label, multi-class classifier is trained on a dataset with $N=100$ observations across three classes $A$, $B$, and $C$. The true class counts are $|A|=80$, $|B|=15$, and $|C|=5$. The classifier produces both a predicted class label and a probability distribution over classes for each observation.\n\nIts aggregate behavior on the test set is as follows:\n- For true class $A$: $70$ are predicted as $A$ and $10$ are predicted as $B$.\n- For true class $B$: $8$ are predicted as $B$ and $7$ are predicted as $A$.\n- For true class $C$: $1$ is predicted as $C$ and $4$ are predicted as $A$.\n\nFor each group of observations, the assigned probability to the true class (the probability mass on the correct label, regardless of the predicted label) is:\n- True $A$, predicted $A$: each has probability $0.90$ on $A$.\n- True $A$, predicted $B$: each has probability $0.05$ on $A$.\n- True $B$, predicted $B$: each has probability $0.85$ on $B$.\n- True $B$, predicted $A$: each has probability $0.10$ on $B$.\n- True $C$, predicted $C$: the one instance has probability $0.80$ on $C$.\n- True $C$, predicted $A$: each has probability $0.02$ on $C$.\n\nUsing only fundamental definitions of per-class precision, per-class recall, the $F_1$-score, macro averaging, micro averaging, and the average negative log-likelihood (log-loss, using the natural logarithm), identify which option correctly characterizes the macro $F_1$, micro $F_1$, and log-loss and what they reveal about fit quality under the given class imbalance.\n\nA. Macro $F_1 \\approx 0.563$ is substantially lower than micro $F_1 \\approx 0.790$, and the log-loss is approximately $0.706$, indicating that overconfident errors on the minority class inflate log-loss despite high micro performance.\n\nB. Macro $F_1 \\approx 0.790$ equals micro $F_1$, and the log-loss is approximately $0.300$, showing that the classifier is uniformly strong across classes.\n\nC. Macro $F_1 \\approx 0.563$ but micro $F_1 \\approx 0.563$ as well, and the log-loss is approximately $0.200$, so both $F_1$ variants and log-loss tell the same story.\n\nD. Macro $F_1 \\approx 0.790$ is higher than micro $F_1 \\approx 0.563$, and the log-loss is approximately $0.706$, meaning macro averaging reduces sensitivity to imbalanced errors.", "solution": "The problem statement has been evaluated and found to be valid. It is scientifically grounded in the principles of statistical learning, is well-posed with sufficient and consistent data, and is expressed in objective language. We may therefore proceed with the derivation of a solution.\n\nThe primary step is to construct the confusion matrix, $M$, where an entry $M_{ij}$ represents the number of observations with true class $i$ that are predicted as class $j$. The problem provides the following data:\nTotal observations $N=100$.\nTrue class counts: $|A|=80$, $|B|=15$, $|C|=5$.\n\nFrom the description of the classifier's behavior:\n- For true class $A$: $70$ predicted $A$, $10$ predicted $B$.\n- For true class $B$: $8$ predicted $B$, $7$ predicted $A$.\n- For true class $C$: $1$ predicted $C$, $4$ predicted $A$.\n\nThis leads to the following confusion matrix:\n$$\nM = \n\\begin{pmatrix}\n  \\text{Pred } A  \\text{Pred } B  \\text{Pred } C \\\\\n\\text{True } A  70  10  0 \\\\\n\\text{True } B  7  8  0 \\\\\n\\text{True } C  4  0  1 \n\\end{pmatrix}\n$$\nThe row sums correctly match the true class counts ($70+10=80$, $7+8=15$, $4+1=5$). The column sums yield the total predictions for each class:\n- Predicted $A$: $70+7+4 = 81$\n- Predicted $B$: $10+8+0 = 18$\n- Predicted $C$: $0+0+1 = 1$\nThe total number of observations is $81+18+1=100$, confirming consistency.\n\nFrom the confusion matrix, we calculate the True Positives ($TP_k$), False Positives ($FP_k$), and False Negatives ($FN_k$) for each class $k \\in \\{A, B, C\\}$.\n\nFor Class $A$:\n- $TP_A = 70$\n- $FP_A = 7+4 = 11$\n- $FN_A = 10+0 = 10$\n\nFor Class $B$:\n- $TP_B = 8$\n- $FP_B = 10+0 = 10$\n- $FN_B = 7+0 = 7$\n\nFor Class $C$:\n- $TP_C = 1$\n- $FP_C = 0+0 = 0$\n- $FN_C = 4+0 = 4$\n\nNext, we calculate the per-class precision ($P_k$) and recall ($R_k$):\n$P_k = \\frac{TP_k}{TP_k + FP_k}$ and $R_k = \\frac{TP_k}{TP_k + FN_k}$.\n\n- Class $A$:\n  $P_A = \\frac{70}{70+11} = \\frac{70}{81}$\n  $R_A = \\frac{70}{70+10} = \\frac{70}{80} = \\frac{7}{8} = 0.875$\n- Class $B$:\n  $P_B = \\frac{8}{8+10} = \\frac{8}{18} = \\frac{4}{9}$\n  $R_B = \\frac{8}{8+7} = \\frac{8}{15}$\n- Class $C$:\n  $P_C = \\frac{1}{1+0} = 1$\n  $R_C = \\frac{1}{1+4} = \\frac{1}{5} = 0.2$\n\nThe per-class $F_1$-score is the harmonic mean of precision and recall: $F_{1,k} = 2 \\frac{P_k R_k}{P_k + R_k}$.\n- $F_{1,A} = 2 \\frac{(70/81)(7/8)}{(70/81)+(7/8)} = 2 \\frac{490/648}{560/648 + 567/648} = 2 \\frac{490}{1127} = \\frac{980}{1127} \\approx 0.8696$\n- $F_{1,B} = 2 \\frac{(4/9)(8/15)}{(4/9)+(8/15)} = 2 \\frac{32/135}{60/135 + 72/135} = 2 \\frac{32}{132} = \\frac{64}{132} = \\frac{16}{33} \\approx 0.4848$\n- $F_{1,C} = 2 \\frac{1 \\cdot (1/5)}{1 + (1/5)} = 2 \\frac{1/5}{6/5} = \\frac{2}{6} = \\frac{1}{3} \\approx 0.3333$\n\nWe can now calculate the macro and micro averaged $F_1$-scores.\n\nThe **Macro $F_1$-score** is the unweighted average of the per-class $F_1$-scores.\n$$\n\\text{Macro } F_1 = \\frac{F_{1,A} + F_{1,B} + F_{1,C}}{3} = \\frac{1}{3} \\left(\\frac{980}{1127} + \\frac{16}{33} + \\frac{1}{3}\\right) \\approx \\frac{0.8696 + 0.4848 + 0.3333}{3} = \\frac{1.6877}{3} \\approx 0.5626\n$$\n\nThe **Micro $F_1$-score** requires micro-averaged precision and recall, which are computed from the sum of all $TPs$, $FPs$, and $FNs$.\n- $\\sum TP_k = 70+8+1 = 79$\n- $\\sum FP_k = 11+10+0 = 21$\n- $\\sum FN_k = 10+7+4 = 21$\n$P_{micro} = \\frac{\\sum TP_k}{\\sum TP_k + \\sum FP_k} = \\frac{79}{79+21} = \\frac{79}{100} = 0.79$\n$R_{micro} = \\frac{\\sum TP_k}{\\sum TP_k + \\sum FN_k} = \\frac{79}{79+21} = \\frac{79}{100} = 0.79$\nNote that micro-precision, micro-recall, and overall accuracy are identical in multi-class classification, all being $\\frac{\\text{correctly classified}}{\\text{total}}$.\n$$\n\\text{Micro } F_1 = 2 \\frac{P_{micro} R_{micro}}{P_{micro} + R_{micro}} = \\frac{2 \\cdot 0.79 \\cdot 0.79}{0.79+0.79} = 0.79\n$$\n\nFinally, we calculate the average negative log-likelihood (log-loss), $L$, using the natural logarithm. The formula is $L = -\\frac{1}{N} \\sum_{i=1}^N \\ln(p_{i, \\text{true}})$, where $p_{i, \\text{true}}$ is the probability the model assigned to the true class of observation $i$. We sum the log probabilities over the specified groups:\n- True $A$, pred $A$: $70$ instances, $p=0.90$. Contribution: $70 \\times \\ln(0.90)$.\n- True $A$, pred $B$: $10$ instances, $p=0.05$. Contribution: $10 \\times \\ln(0.05)$.\n- True $B$, pred $B$: $8$ instances, $p=0.85$. Contribution: $8 \\times \\ln(0.85)$.\n- True $B$, pred $A$: $7$ instances, $p=0.10$. Contribution: $7 \\times \\ln(0.10)$.\n- True $C$, pred $C$: $1$ instance, $p=0.80$. Contribution: $1 \\times \\ln(0.80)$.\n- True $C$, pred $A$: $4$ instances, $p=0.02$. Contribution: $4 \\times \\ln(0.02)$.\n\n$$\n\\begin{align*} L = -\\frac{1}{100} [70\\ln(0.90) + 10\\ln(0.05) + 8\\ln(0.85) + 7\\ln(0.10) + 1\\ln(0.80) + 4\\ln(0.02)] \\\\\n\\approx -\\frac{1}{100} [70(-0.1054) + 10(-2.9957) + 8(-0.1625) + 7(-2.3026) + 1(-0.2231) + 4(-3.9120)] \\\\\n\\approx -\\frac{1}{100} [-7.378 - 29.957 - 1.300 - 16.118 - 0.223 - 15.648] \\\\\n\\approx -\\frac{1}{100} [-70.624] \\approx 0.706\n\\end{align*}\n$$\nSummary of calculated metrics:\n- Macro $F_1 \\approx 0.563$\n- Micro $F_1 = 0.790$\n- Log-loss $\\approx 0.706$\n\nNow we evaluate each option.\n\nA. Macro $F_1 \\approx 0.563$ is substantially lower than micro $F_1 \\approx 0.790$, and the log-loss is approximately $0.706$, indicating that overconfident errors on the minority class inflate log-loss despite high micro performance.\nThe calculated values for Macro $F_1$, Micro $F_1$, and Log-loss are all correct. The interpretation is also sound. Macro $F_1$ is low because it gives equal weight to the very poor $F_1$-scores of minority classes $B$ ($\\approx 0.485$) and $C$ ($\\approx 0.333$). Micro $F_1$ is high because it is dominated by the performance on the majority class $A$ ($80\\%$ of the data), where the model performs well. The log-loss is significantly inflated by errors where the model assigns a very low probability to the true class (e.g., the $4$ errors on class $C$ where $p=0.02$ contribute $-\\frac{4}{100}\\ln(0.02) \\approx 0.156$ to the total loss of $0.706$, a disproportionately large amount from just $4\\%$ of the data). These are indeed overconfident errors on a minority class. This statement is a correct and insightful summary of the model's performance. **Correct.**\n\nB. Macro $F_1 \\approx 0.790$ equals micro $F_1$, and the log-loss is approximately $0.300$, showing that the classifier is uniformly strong across classes.\nThe values presented for Macro $F_1$ and log-loss are incorrect. Furthermore, the claim that Macro $F_1$ equals Micro $F_1$ is false, as is the claim that the classifier is uniformly strong. The per-class $F_1$ scores ($0.870, 0.485, 0.333$) show highly non-uniform performance. **Incorrect.**\n\nC. Macro $F_1 \\approx 0.563$ but micro $F_1 \\approx 0.563$ as well, and the log-loss is approximately $0.200$, so both $F_1$ variants and log-loss tell the same story.\nThe value for Micro $F_1$ is incorrect; it is $0.790$, not $0.563$. The log-loss value is also incorrect. The premise that both $F_1$ variants tell the same story is false; their divergence is the key insight here. **Incorrect.**\n\nD. Macro $F_1 \\approx 0.790$ is higher than micro $F_1 \\approx 0.563$, and the log-loss is approximately $0.706$, meaning macro averaging reduces sensitivity to imbalanced errors.\nThe values for Macro $F_1$ and Micro $F_1$ are swapped. Macro $F_1$ is $\\approx 0.563$ and Micro $F_1$ is $0.790$. The interpretation is also reversed; macro averaging *increases* sensitivity to performance on minority classes, it does not reduce it. **Incorrect.**", "answer": "$$\\boxed{A}$$", "id": "3147853"}, {"introduction": "When data has a hierarchical structure, such as students nested within schools, standard quality-of-fit measures like a single $R^2$ value are inadequate. This practice introduces a more nuanced approach for linear mixed-effects models by distinguishing between the variance explained by fixed predictors (marginal $R^2$) and the variance explained by the entire model, including random effects (conditional $R^2$). By calculating these two values from the model's variance components, you will learn to precisely quantify the explanatory power attributable to your main predictors versus the group-level heterogeneity in your data [@problem_id:3147863].", "problem": "A research team fits a linear mixed-effects model (LMM) to student test scores measured across schools, where each student belongs to exactly one school. The model is\n$$\ny_{ij} = \\beta_0 + \\beta_1 x_{ij} + b_j + \\varepsilon_{ij},\n$$\nwith school-specific random intercepts $b_j \\sim \\mathcal{N}(0,\\sigma_b^2)$, independent residuals $\\varepsilon_{ij} \\sim \\mathcal{N}(0,\\sigma_\\varepsilon^2)$, and $b_j$ independent of $\\varepsilon_{ij}$. Here $i$ indexes students within school $j$, and $x_{ij}$ is a centered predictor (mean zero across all observations). After fitting, the team reports:\n- the sample variance across all observations of the fixed-effects linear predictor $\\hat{\\eta}_{ij}^{(F)} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_{ij}$ is $\\widehat{\\sigma}_F^2 = 6.0$,\n- the estimated random-intercept variance is $\\widehat{\\sigma}_b^2 = 4.0$,\n- the estimated residual variance is $\\widehat{\\sigma}_\\varepsilon^2 = 10.0$.\n\nUsing a variance-partitioning definition of the coefficient of determination ($R^2$) that distinguishes the contribution of fixed effects from that of random effects, select the option that correctly reports the marginal $R^2$ and the conditional $R^2$, and provides a valid interpretation of what each implies about quality of fit for the fixed effects versus the whole model.\n\nA. Marginal $R^2 = 0.30$, Conditional $R^2 = 0.50$. Interpretation: The fixed effects alone explain $30\\%$ of the total variance implied by the fitted model; when both fixed and random effects are considered, the model explains $50\\%$ of the total variance. Thus, the fixed part has modest explanatory power, and the random intercepts capture additional between-school heterogeneity that improves overall fit.\n\nB. Marginal $R^2 = 0.375$, Conditional $R^2 = 0.50$. Interpretation: The fixed effects explain $37.5\\%$ of the variability after accounting only for residual noise, and adding random intercepts raises the explained variability to $50\\%$ of total, so fixed effects are the dominant source of fit.\n\nC. Marginal $R^2 = 0.60$, Conditional $R^2 = 0.80$. Interpretation: The fixed effects by themselves explain $60\\%$ of the observed outcomes, and including random intercepts explains $80\\%$, indicating an excellent fit dominated by fixed effects.\n\nD. Marginal $R^2 = 0.30$, Conditional $R^2 = 0.70$. Interpretation: The fixed effects explain $30\\%$ and the random effects alone explain $40\\%$, so the random effects contribute more to fit quality than the fixed effects, and together they explain $70\\%$ of the total variance.", "solution": "The problem statement is first validated to ensure it is scientifically sound, well-posed, and objective.\n\n### Step 1: Extract Givens\n-   **Model:** A linear mixed-effects model (LMM) is specified as $y_{ij} = \\beta_0 + \\beta_1 x_{ij} + b_j + \\varepsilon_{ij}$.\n-   **Indices:** $i$ for students, $j$ for schools.\n-   **Predictor:** $x_{ij}$ is a centered predictor, meaning its sample mean is zero.\n-   **Random Effects:** The school-specific random intercepts are $b_j \\sim \\mathcal{N}(0, \\sigma_b^2)$.\n-   **Residuals:** The residuals are $\\varepsilon_{ij} \\sim \\mathcal{N}(0, \\sigma_\\varepsilon^2)$.\n-   **Independence:** $b_j$ and $\\varepsilon_{ij}$ are independent.\n-   **Estimated Variance Components:**\n    1.  The sample variance of the fixed-effects linear predictor, $\\widehat{\\sigma}_F^2 = \\text{Var}(\\hat{\\eta}_{ij}^{(F)}) = \\text{Var}(\\hat{\\beta}_0 + \\hat{\\beta}_1 x_{ij})$, is $\\widehat{\\sigma}_F^2 = 6.0$.\n    2.  The estimated random-intercept variance is $\\widehat{\\sigma}_b^2 = 4.0$.\n    3.  The estimated residual variance is $\\widehat{\\sigma}_\\varepsilon^2 = 10.0$.\n-   **Objective:** Calculate the marginal $R^2$ and conditional $R^2$ and interpret their meanings for the quality of fit.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is based on the established statistical framework of linear mixed-effects models. The use of marginal and conditional $R^2$ for LMMs, as defined by partitioning variance components, is a standard and well-accepted methodology in statistical learning and ecology (e.g., Nakagawa  Schielzeth, 2013). The provided model structure is a canonical random-intercept model. The given numerical values for the variance components are positive, as required, and there are no internal contradictions. The problem is self-contained, objective, and well-posed, admitting a unique solution based on the provided data and standard definitions.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be derived.\n\n### Derivation of Solution\nThe quality of fit for a linear mixed-effects model can be assessed using the coefficient of determination, $R^2$, extended to marginal and conditional versions. These metrics partition the total variance of the outcome variable into components attributable to the model's fixed effects, random effects, and residual error.\n\nThe total variance of the response $y_{ij}$ as implied by the model is the sum of the variances of its constituent parts, due to their independence:\n$$\n\\text{Var}(y_{ij}) = \\text{Var}(\\beta_0 + \\beta_1 x_{ij} + b_j + \\varepsilon_{ij}) = \\underbrace{\\text{Var}(\\beta_0 + \\beta_1 x_{ij})}_{\\sigma_F^2} + \\underbrace{\\text{Var}(b_j)}_{\\sigma_b^2} + \\underbrace{\\text{Var}(\\varepsilon_{ij})}_{\\sigma_\\varepsilon^2}\n$$\nUsing the provided estimates, the total variance in the response variable is:\n$$\n\\widehat{\\sigma}_{\\text{Total}}^2 = \\widehat{\\sigma}_F^2 + \\widehat{\\sigma}_b^2 + \\widehat{\\sigma}_\\varepsilon^2\n$$\nSubstituting the given values:\n$$\n\\widehat{\\sigma}_{\\text{Total}}^2 = 6.0 + 4.0 + 10.0 = 20.0\n$$\n\n**Marginal Coefficient of Determination ($R_m^2$)**\nThe marginal $R^2$ quantifies the proportion of the total variance explained by the fixed effects alone. It is calculated as:\n$$\nR_m^2 = \\frac{\\text{Variance explained by fixed effects}}{\\text{Total variance}} = \\frac{\\widehat{\\sigma}_F^2}{\\widehat{\\sigma}_F^2 + \\widehat{\\sigma}_b^2 + \\widehat{\\sigma}_\\varepsilon^2}\n$$\nUsing the given values:\n$$\nR_m^2 = \\frac{6.0}{20.0} = 0.30\n$$\nThis means that the fixed predictor $x_{ij}$ and the overall intercept collectively account for $30\\%$ of the variance in student test scores.\n\n**Conditional Coefficient of Determination ($R_c^2$)**\nThe conditional $R^2$ quantifies the proportion of the total variance explained by both the fixed effects and the random effects combined. It is calculated as:\n$$\nR_c^2 = \\frac{\\text{Variance explained by fixed and random effects}}{\\text{Total variance}} = \\frac{\\widehat{\\sigma}_F^2 + \\widehat{\\sigma}_b^2}{\\widehat{\\sigma}_F^2 + \\widehat{\\sigma}_b^2 + \\widehat{\\sigma}_\\varepsilon^2}\n$$\nUsing the given values:\n$$\nR_c^2 = \\frac{6.0 + 4.0}{20.0} = \\frac{10.0}{20.0} = 0.50\n$$\nThis means that the full model, accounting for the predictor $x_{ij}$ and the school-level differences (random intercepts $b_j$), explains $50\\%$ of the variance in test scores. The difference $R_c^2 - R_m^2 = 0.50 - 0.30 = 0.20$ is the proportion of variance attributable specifically to the random effects (i.e., the clustering by school), which is consistent with $\\widehat{\\sigma}_b^2 / \\widehat{\\sigma}_{\\text{Total}}^2 = 4.0/20.0 = 0.20$.\n\n### Evaluation of Options\n\n**A. Marginal $R^2 = 0.30$, Conditional $R^2 = 0.50$. Interpretation: The fixed effects alone explain $30\\%$ of the total variance implied by the fitted model; when both fixed and random effects are considered, the model explains $50\\%$ of the total variance. Thus, the fixed part has modest explanatory power, and the random intercepts capture additional between-school heterogeneity that improves overall fit.**\n- The calculated values $R_m^2 = 0.30$ and $R_c^2 = 0.50$ are correct.\n- The interpretation is precise and logically sound. It correctly states what each metric represents. The qualitative summary (\"modest explanatory power,\" \"capture additional between-school heterogeneity\") is a reasonable and accurate assessment of the results.\n- **Verdict: Correct**\n\n**B. Marginal $R^2 = 0.375$, Conditional $R^2 = 0.50$. Interpretation: The fixed effects explain $37.5\\%$ of the variability after accounting only for residual noise, and adding random intercepts raises the explained variability to $50\\%$ of total, so fixed effects are the dominant source of fit.**\n- The value for marginal $R^2$ is incorrect. The value $0.375$ appears to be derived from an erroneous formula, likely $\\widehat{\\sigma}_F^2 / (\\widehat{\\sigma}_F^2 + \\widehat{\\sigma}_\\varepsilon^2) = 6.0 / (6.0 + 10.0) = 6/16 = 0.375$, which incorrectly omits the random effect variance from the denominator representing total variance.\n- **Verdict: Incorrect**\n\n**C. Marginal $R^2 = 0.60$, Conditional $R^2 = 0.80$. Interpretation: The fixed effects by themselves explain $60\\%$ of the observed outcomes, and including random intercepts explains $80\\%$, indicating an excellent fit dominated by fixed effects.**\n- Both the marginal and conditional $R^2$ values are incorrect. The value $0.60$ appears to result from calculating the proportion of *explained* variance due to fixed effects, $\\widehat{\\sigma}_F^2 / (\\widehat{\\sigma}_F^2 + \\widehat{\\sigma}_b^2) = 6.0 / (6.0 + 4.0) = 0.60$, which is not the definition of marginal $R^2$. The value $0.80$ for conditional $R^2$ does not correspond to any standard calculation.\n- **Verdict: Incorrect**\n\n**D. Marginal $R^2 = 0.30$, Conditional $R^2 = 0.70$. Interpretation: The fixed effects explain $30\\%$ and the random effects alone explain $40\\%$, so the random effects contribute more to fit quality than the fixed effects, and together they explain $70\\%$ of the total variance.**\n- The marginal $R^2$ value is correct, but the conditional $R^2$ is incorrect. Our calculation yielded $R_c^2 = 0.50$.\n- The interpretation contains multiple errors. The random effects explain $\\widehat{\\sigma}_b^2 / \\widehat{\\sigma}_{\\text{Total}}^2 = 4.0/20.0 = 20\\%$, not $40\\%$. The fixed effects contribute more to the explained variance ($\\widehat{\\sigma}_F^2 = 6.0$) than the random effects ($\\widehat{\\sigma}_b^2 = 4.0$). The combined explained variance is $50\\%$, not $70\\%$.\n- **Verdict: Incorrect**", "answer": "$$\\boxed{A}$$", "id": "3147863"}, {"introduction": "A model's quality of fit is not just about its performance on typical data, but also its robustness to small, plausible variations in the inputs. This exercise introduces a method from the field of adversarial machine learning to quantify model sensitivity. You will use the Fast Gradient Sign Method (FGSM) to find the \"worst-case\" small perturbations for input features and measure how much they degrade the model's performance, providing a direct measure of its local sensitivity [@problem_id:3147847]. This practice offers a powerful way to understand a model's stability and potential vulnerabilities.", "problem": "Consider a fixed predictive model with parameters held constant and a dataset of feature vectors and labels. The objective is to quantify the sensitivity of the fitted model to small, worst-case perturbations applied directly to the input features, under a norm constraint. The sensitivity is measured by the change in loss between the unperturbed and adversarially perturbed inputs, using either Root Mean Squared Error (RMSE) for regression or the logistic log-loss for binary classification. Root Mean Squared Error (RMSE) is defined as $\\mathrm{RMSE} = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}\\left(\\hat{y}_{i} - y_{i}\\right)^{2}}$, where $\\hat{y}_{i}$ is the model prediction and $y_{i}$ is the true response. For a linear regression model with prediction $\\hat{y}_{i} = x_{i}^{\\top} w$, where $x_{i} \\in \\mathbb{R}^{d}$ is the feature vector of sample $i$ and $w \\in \\mathbb{R}^{d}$ is a fixed parameter vector, RMSE is the quality-of-fit metric. For binary classification, the logistic log-loss (also called negative log-likelihood or cross-entropy) for predictions $p_{i} = \\sigma(x_{i}^{\\top} w)$ with $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ is $\\mathcal{L}_{\\mathrm{log}} = -\\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_{i} \\log p_{i} + (1 - y_{i}) \\log (1 - p_{i}) \\right]$, where $y_{i} \\in \\{0,1\\}$ are the labels and $p_{i}$ are the predicted probabilities.\n\nYou will construct adversarial perturbations using the Fast Gradient Sign Method (FGSM), defined as the element-wise sign of the gradient of the chosen loss with respect to each sample's feature vector, scaled by a nonnegative scalar bound $\\epsilon$ and applied under the $\\ell_{\\infty}$ norm constraint $\\|\\Delta x_{i}\\|_{\\infty} \\le \\epsilon$. The model parameters $w$ are fixed (no re-fitting). Specifically, for regression with RMSE, let $e_{i} = \\hat{y}_{i} - y_{i}$ and $\\mathrm{MSE} = \\frac{1}{n}\\sum_{i=1}^{n} e_{i}^{2}$. The gradient of RMSE with respect to the feature vector $x_{i}$ is $\\nabla_{x_{i}} \\mathrm{RMSE} = \\frac{e_{i}}{n \\sqrt{\\mathrm{MSE}}} w$. Since the multiplicative scalar $\\frac{e_{i}}{n \\sqrt{\\mathrm{MSE}}}$ is positive or negative depending on $e_{i}$, the Fast Gradient Sign Method direction for RMSE coincides with the element-wise sign of $e_{i} w$. Thus the adversarial perturbation for regression is $\\Delta x_{i} = \\epsilon \\cdot \\mathrm{sign}\\!\\left(e_{i} w\\right)$. For binary classification with logistic log-loss, the gradient with respect to $x_{i}$ is $\\nabla_{x_{i}} \\mathcal{L}_{\\mathrm{log}} = \\frac{1}{n} (p_{i} - y_{i}) w$, so the adversarial perturbation is $\\Delta x_{i} = \\epsilon \\cdot \\mathrm{sign}\\!\\left((p_{i} - y_{i}) w\\right)$. The adversarially perturbed features are $(x_i + \\Delta x_i)$ for each sample, and the sensitivity is the nonnegative change in the chosen loss: adversarial loss minus baseline loss.\n\nImplement a program that, for each provided test case, performs the following steps:\n1. Compute the baseline loss (RMSE for regression, logistic log-loss for classification) using $X \\in \\mathbb{R}^{n \\times d}$, $y$, and fixed $w$.\n2. Construct $\\Delta X$ by applying FGSM to each sample as described above with the given $\\epsilon$.\n3. Compute the adversarial loss on $(X + \\Delta X)$.\n4. Output the scalar difference between adversarial loss and baseline loss.\n\nTreat all computations as unitless scalars; no physical units are involved. The binary classification probabilities should be numerically stabilized by clipping $p_{i}$ into the interval $[10^{-15}, 1 - 10^{-15}]$ before applying the logarithm.\n\nTest Suite:\n- Case 1 (Regression, general case): $X = \\begin{bmatrix}1.0  2.0\\\\ 0.5  -1.0\\\\ 3.0  0.0\\end{bmatrix}$, $y = \\begin{bmatrix}4.0\\\\ -1.0\\\\ 5.0\\end{bmatrix}$, $w = \\begin{bmatrix}1.2\\\\ 0.8\\end{bmatrix}$, $\\epsilon = 0.1$.\n- Case 2 (Regression, boundary $\\epsilon = 0$): $X = \\begin{bmatrix}2.0  -0.5\\\\ -1.0  1.0\\end{bmatrix}$, $y = \\begin{bmatrix}1.0\\\\ 0.0\\end{bmatrix}$, $w = \\begin{bmatrix}0.5\\\\ -0.5\\end{bmatrix}$, $\\epsilon = 0.0$.\n- Case 3 (Classification, general case): $X = \\begin{bmatrix}0.2  -0.1\\\\ 1.5  0.3\\\\ -0.3  0.8\\end{bmatrix}$, $y = \\begin{bmatrix}0\\\\ 1\\\\ 1\\end{bmatrix}$, $w = \\begin{bmatrix}0.7\\\\ -0.5\\end{bmatrix}$, $\\epsilon = 0.2$.\n- Case 4 (Classification, zero-weights edge): $X = \\begin{bmatrix}1.0  2.0\\\\ -0.5  0.5\\end{bmatrix}$, $y = \\begin{bmatrix}0\\\\ 1\\end{bmatrix}$, $w = \\begin{bmatrix}0.0\\\\ 0.0\\end{bmatrix}$, $\\epsilon = 0.5$.\n- Case 5 (Classification, large $\\epsilon$ stress): $X = \\begin{bmatrix}2.0  -1.0\\\\ -1.0  2.5\\\\ 0.3  -0.7\\\\ 1.0  1.0\\end{bmatrix}$, $y = \\begin{bmatrix}1\\\\ 0\\\\ 0\\\\ 1\\end{bmatrix}$, $w = \\begin{bmatrix}1.0\\\\ 1.5\\end{bmatrix}$, $\\epsilon = 2.0$.\n\nYour program should produce a single line of output containing the results for all five test cases as a comma-separated list enclosed in square brackets, in the order the cases are listed above, for example, \"[result1,result2,result3,result4,result5]\". Each result must be a real number (float) equal to the adversarial loss minus the baseline loss for the corresponding case. No other text should be printed.", "solution": "The problem requires the calculation of a model's sensitivity to adversarial perturbations on its input features. This sensitivity is quantified as the change in a quality-of-fit metric (loss function) when the inputs are modified using the Fast Gradient Sign Method (FGSM). The model parameters $w$ are held constant. The process involves two types of models: linear regression with Root Mean Squared Error (RMSE) loss, and logistic regression for binary classification with logistic log-loss.\n\nThe core of the method is to compute a perturbation $\\Delta X$ for the input data matrix $X \\in \\mathbb{R}^{n \\times d}$, where $n$ is the number of samples and $d$ is the number of features. The perturbation for each sample $x_i$ is derived from the gradient of the loss function $\\mathcal{L}$ with respect to $x_i$, constrained by the $\\ell_{\\infty}$ norm:\n$$\n\\Delta x_{i} = \\epsilon \\cdot \\mathrm{sign}(\\nabla_{x_{i}} \\mathcal{L})\n$$\nwhere $\\epsilon \\geq 0$ is the perturbation magnitude. The final output for each test case is the difference $\\mathcal{L}_{\\mathrm{adv}} - \\mathcal{L}_{\\mathrm{base}}$, where $\\mathcal{L}_{\\mathrm{base}}$ is the loss on the original data $X$ and $\\mathcal{L}_{\\mathrm{adv}}$ is the loss on the perturbed data $X + \\Delta X$.\n\nFirst, we analyze the regression case. The model's prediction for a sample $x_i$ is $\\hat{y}_i = x_i^\\top w$. The loss function is the Root Mean Squared Error (RMSE):\n$$\n\\mathrm{RMSE} = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}\\left(\\hat{y}_{i} - y_{i}\\right)^{2}} = \\sqrt{\\mathrm{MSE}}\n$$\nwhere $y_i$ are the true continuous-valued responses. The gradient of RMSE with respect to the feature vector $x_i$ is given as:\n$$\n\\nabla_{x_{i}} \\mathrm{RMSE} = \\frac{e_{i}}{n \\sqrt{\\mathrm{MSE}}} w\n$$\nwhere $e_i = \\hat{y}_i - y_i$ is the prediction error for sample $i$. Since the scalar factor $\\frac{1}{n \\sqrt{\\mathrm{MSE}}}$ is positive (for $\\mathrm{MSE}  0$), the sign of the gradient is determined by the sign of $e_i w$. Consequently, the perturbation for the $i$-th sample's feature vector $x_i$ is:\n$$\n\\Delta x_i = \\epsilon \\cdot \\mathrm{sign}(e_i w)\n$$\nTo implement this for all samples simultaneously, we compute the predictions $\\hat{y} = Xw$ and the error vector $e = \\hat{y} - y$. The matrix of perturbations $\\Delta X \\in \\mathbb{R}^{n \\times d}$ can be computed by broadcasting the error vector $e$ (an $n \\times 1$ column vector) with the parameter vector $w^\\top$ (a $1 \\times d$ row vector), taking the element-wise sign, and scaling by $\\epsilon$:\n$$\n\\Delta X = \\epsilon \\cdot \\mathrm{sign}(e w^\\top)\n$$\nThe baseline loss $\\mathrm{RMSE}_{\\mathrm{base}}$ is computed on $X$. The adversarial features are $X_{\\mathrm{adv}} = X + \\Delta X$. The adversarial loss $\\mathrm{RMSE}_{\\mathrm{adv}}$ is computed using $X_{\\mathrm{adv}}$. The result is $\\mathrm{RMSE}_{\\mathrm{adv}} - \\mathrm{RMSE}_{\\mathrm{base}}$.\n\nNext, we analyze the binary classification case. The model predicts the probability of class $1$ using the sigmoid function, $p_i = \\sigma(x_i^\\top w)$, where $\\sigma(z) = (1 + e^{-z})^{-1}$. The loss function is the logistic log-loss (or cross-entropy):\n$$\n\\mathcal{L}_{\\mathrm{log}} = -\\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_{i} \\log p_{i} + (1 - y_{i}) \\log (1 - p_{i}) \\right]\n$$\nwhere $y_i \\in \\{0, 1\\}$ are the true binary labels. The gradient of the log-loss with respect to $x_i$ is given as:\n$$\n\\nabla_{x_{i}} \\mathcal{L}_{\\mathrm{log}} = \\frac{1}{n} (p_{i} - y_{i}) w\n$$\nThe factor $1/n$ is a positive scalar, so the sign of the gradient is determined by the sign of $(p_i - y_i)w$. The perturbation for sample $i$ is:\n$$\n\\Delta x_i = \\epsilon \\cdot \\mathrm{sign}((p_i - y_i)w)\n$$\nSimilar to the regression case, we can compute the perturbation matrix $\\Delta X$ for all samples. Let $p$ be the vector of probabilities and $y$ be the vector of true labels.\n$$\n\\Delta X = \\epsilon \\cdot \\mathrm{sign}((p - y)w^\\top)\n$$\nThe baseline loss $\\mathcal{L}_{\\mathrm{base}}$ is calculated using the original data $X$. To ensure numerical stability, the predicted probabilities $p_i$ are clipped to the range $[10^{-15}, 1 - 10^{-15}]$ before being passed to the logarithm function. The adversarial features are $X_{\\mathrm{adv}} = X + \\Delta X$, and the adversarial loss $\\mathcal{L}_{\\mathrm{adv}}$ is computed on these features (with clipping applied to the new probabilities). The result is $\\mathcal{L}_{\\mathrm{adv}} - \\mathcal{L}_{\\mathrm{base}}$.\n\nIn special cases where $\\epsilon = 0$ or the gradient is a zero vector (e.g., if $w=0$), the perturbation $\\Delta X$ becomes a zero matrix. In such scenarios, $X_{\\mathrm{adv}} = X$, the adversarial loss equals the baseline loss, and the sensitivity is $0$. This is handled naturally by the element-wise sign function, as $\\mathrm{sign}(0) = 0$.\n\nThe overall algorithm iterates through each test case, identifies the problem type (regression or classification), and applies the corresponding logic to compute the baseline loss, the FGSM perturbation, the adversarial loss, and finally the difference between the two losses.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    \"\"\"\n\n    def compute_regression_sensitivity(X, y, w, epsilon):\n        \"\"\"\n        Computes the sensitivity for a linear regression model.\n        \"\"\"\n        n, d = X.shape\n        # Ensure y and w are column vectors for matrix operations\n        y = y.reshape(-1, 1)\n        w = w.reshape(-1, 1)\n\n        # 1. Compute baseline loss (RMSE)\n        y_hat = X @ w\n        errors = y_hat - y\n        mse_base = np.mean(np.square(errors))\n        # Handle case where MSE is zero\n        rmse_base = np.sqrt(mse_base) if mse_base  0 else 0.0\n\n        if epsilon == 0.0:\n            return 0.0\n\n        # 2. Construct adversarial perturbation delta_X\n        # Shape of errors: (n, 1), w.T: (1, d) - result: (n, d)\n        sign_matrix = np.sign(errors @ w.T)\n        delta_X = epsilon * sign_matrix\n\n        # 3. Compute adversarial loss\n        X_adv = X + delta_X\n        y_hat_adv = X_adv @ w\n        errors_adv = y_hat_adv - y\n        mse_adv = np.mean(np.square(errors_adv))\n        rmse_adv = np.sqrt(mse_adv) if mse_adv  0 else 0.0\n        \n        # 4. Return the difference\n        return rmse_adv - rmse_base\n\n    def compute_classification_sensitivity(X, y, w, epsilon):\n        \"\"\"\n        Computes the sensitivity for a logistic classification model.\n        \"\"\"\n        n, d = X.shape\n        # Ensure y and w are column vectors for matrix operations\n        y = y.reshape(-1, 1)\n        w = w.reshape(-1, 1)\n        \n        clip_val = 1e-15\n\n        def sigmoid(z):\n            return 1 / (1 + np.exp(-z))\n\n        def log_loss(p, y_true):\n            p_clipped = np.clip(p, clip_val, 1 - clip_val)\n            return -np.mean(y_true * np.log(p_clipped) + (1 - y_true) * np.log(1 - p_clipped))\n\n        # 1. Compute baseline loss (Log-Loss)\n        z_base = X @ w\n        p_base = sigmoid(z_base)\n        loss_base = log_loss(p_base, y)\n\n        if epsilon == 0.0 or np.all(w == 0):\n             return 0.0\n\n        # 2. Construct adversarial perturbation delta_X\n        # Use unclipped probabilities for gradient calculation\n        pred_errors = p_base - y\n        # Shape of pred_errors: (n, 1), w.T: (1, d) - result: (n, d)\n        sign_matrix = np.sign(pred_errors @ w.T)\n        delta_X = epsilon * sign_matrix\n        \n        # 3. Compute adversarial loss\n        X_adv = X + delta_X\n        z_adv = X_adv @ w\n        p_adv = sigmoid(z_adv)\n        loss_adv = log_loss(p_adv, y)\n\n        # 4. Return the difference\n        return loss_adv - loss_base\n\n    test_cases = [\n        # Case 1 (Regression, general case)\n        ('regression', \n         np.array([[1.0, 2.0], [0.5, -1.0], [3.0, 0.0]]), \n         np.array([4.0, -1.0, 5.0]), \n         np.array([1.2, 0.8]), 0.1),\n        # Case 2 (Regression, boundary epsilon = 0)\n        ('regression', \n         np.array([[2.0, -0.5], [-1.0, 1.0]]), \n         np.array([1.0, 0.0]), \n         np.array([0.5, -0.5]), 0.0),\n        # Case 3 (Classification, general case)\n        ('classification', \n         np.array([[0.2, -0.1], [1.5, 0.3], [-0.3, 0.8]]), \n         np.array([0, 1, 1]), \n         np.array([0.7, -0.5]), 0.2),\n        # Case 4 (Classification, zero-weights edge)\n        ('classification', \n         np.array([[1.0, 2.0], [-0.5, 0.5]]), \n         np.array([0, 1]), \n         np.array([0.0, 0.0]), 0.5),\n        # Case 5 (Classification, large epsilon stress)\n        ('classification', \n         np.array([[2.0, -1.0], [-1.0, 2.5], [0.3, -0.7], [1.0, 1.0]]), \n         np.array([1, 0, 0, 1]), \n         np.array([1.0, 1.5]), 2.0),\n    ]\n\n    results = []\n    for case in test_cases:\n        case_type, X, y, w, epsilon = case\n        if case_type == 'regression':\n            result = compute_regression_sensitivity(X, y, w, epsilon)\n        else: # 'classification'\n            result = compute_classification_sensitivity(X, y, w, epsilon)\n        results.append(result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3147847"}]}