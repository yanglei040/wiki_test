{"hands_on_practices": [{"introduction": "To truly grasp how Support Vector Regression works, it is essential to move beyond abstract formulas and see the mechanism in action. This first practice invites you to do just that by solving an SVR problem from first principles [@problem_id:3178709]. By manually fitting a model to a very small dataset, you will gain a concrete understanding of the primal optimization problem, the geometry of the $\\epsilon$-insensitive tube, and what it means for a data point to become a support vector.", "problem": "Consider a one-dimensional training set with two data points: $(x_1, y_1) = (0, 0)$ and $(x_2, y_2) = (1, 2)$. Fit a linear Support Vector Regression (SVR) model, where the predictor is $f(x) = w x + b$, using the $\\epsilon$-insensitive loss with $\\epsilon = 0.5$ and regularization parameter $C = 1$. Start from the fundamental primal definition of SVR: minimize the sum of the weight regularizer and the $\\epsilon$-insensitive slack penalties subject to the $\\epsilon$-tube constraints, without resorting to pre-derived shortcut formulas. Explicitly set up the optimization as a Quadratic Programming (QP) problem, and solve it exactly by reasoning from first principles. Determine the exact optimal values of $w$ and $b$ and identify which training points are support vectors, explaining why in terms of Karush–Kuhn–Tucker (KKT) complementarity. Report your final fitted parameters $(w,b)$ as a row matrix. No rounding is required.", "solution": "The problem asks us to fit a linear Support Vector Regression (SVR) model $f(x) = w x + b$ to a training set of two points, $(x_1, y_1) = (0, 0)$ and $(x_2, y_2) = (1, 2)$. The SVR parameters are given as the insensitivity parameter $\\epsilon = 0.5$ and the regularization parameter $C=1$. We are required to solve this from the fundamental primal definition of SVR.\n\nThe SVR primal optimization problem is formulated to minimize a combination of the model's complexity (regularization term) and the training error (loss function). The complexity is measured by $\\frac{1}{2} \\|w\\|^2$, which for a one-dimensional input is $\\frac{1}{2} w^2$. The error is measured by the $\\epsilon$-insensitive loss, which only penalizes residuals that are greater than $\\epsilon$. This is achieved by introducing non-negative slack variables, $\\xi_i$ and $\\xi_i^*$, for each data point $i$. The objective function to minimize is:\n$$\n\\min_{w, b, \\xi, \\xi^*} \\frac{1}{2} w^2 + C \\sum_{i=1}^{n} (\\xi_i + \\xi_i^*)\n$$\nThis minimization is subject to a set of constraints that define the $\\epsilon$-tube around the data points:\n$$\n\\begin{cases}\ny_i - (w x_i + b) \\le \\epsilon + \\xi_i,  \\text{for } i=1, \\dots, n \\\\\n(w x_i + b) - y_i \\le \\epsilon + \\xi_i^*,  \\text{for } i=1, \\dots, n \\\\\n\\xi_i \\ge 0, \\xi_i^* \\ge 0,  \\text{for } i=1, \\dots, n\n\\end{cases}\n$$\nHere, $n=2$ is the number of data points. Let's substitute the given values:\n- Data point $1$: $(x_1, y_1) = (0, 0)$\n- Data point $2$: $(x_2, y_2) = (1, 2)$\n- Parameters: $\\epsilon = 0.5$, $C = 1$\n\nThe objective function becomes:\n$$\n\\min_{w, b, \\xi_1, \\xi_1^*, \\xi_2, \\xi_2^*} \\frac{1}{2} w^2 + 1 \\cdot (\\xi_1 + \\xi_1^* + \\xi_2 + \\xi_2^*)\n$$\nThe constraints for point $1$ $((x_1, y_1) = (0, 0))$ are:\n$$\n0 - (w \\cdot 0 + b) \\le 0.5 + \\xi_1 \\quad \\implies \\quad -b \\le 0.5 + \\xi_1\n$$\n$$\n(w \\cdot 0 + b) - 0 \\le 0.5 + \\xi_1^* \\quad \\implies \\quad b \\le 0.5 + \\xi_1^*\n$$\nThe constraints for point $2$ $((x_2, y_2) = (1, 2))$ are:\n$$\n2 - (w \\cdot 1 + b) \\le 0.5 + \\xi_2 \\quad \\implies \\quad 1.5 \\le w + b + \\xi_2\n$$\n$$\n(w \\cdot 1 + b) - 2 \\le 0.5 + \\xi_2^* \\quad \\implies \\quad w + b \\le 2.5 + \\xi_2^*\n$$\nAnd the non-negativity constraints on the slack variables:\n$$\n\\xi_1, \\xi_1^*, \\xi_2, \\xi_2^* \\ge 0\n$$\nSince the slack variables are penalized in the objective function, we want to make them as small as possible. This means we should first investigate the case where all slack variables are zero, i.e., $\\xi_1 = \\xi_1^* = \\xi_2 = \\xi_2^* = 0$. This corresponds to finding a function $f(x)$ that lies entirely within the $\\epsilon$-tube for all data points. The constraints simplify to:\n$$\n-b \\le 0.5 \\quad \\implies \\quad b \\ge -0.5\n$$\n$$\nb \\le 0.5\n$$\n$$\n1.5 \\le w + b\n$$\n$$\nw + b \\le 2.5\n$$\nThese inequalities define a feasible region for $(w, b)$. Within this region, the objective function is simply $\\frac{1}{2} w^2$. To find the solution, we must find the point $(w, b)$ in this feasible region that minimizes $\\frac{1}{2} w^2$, which is equivalent to minimizing $|w|$.\n\nThe feasible region is defined by $-0.5 \\le b \\le 0.5$ and $1.5 \\le w+b \\le 2.5$. To minimize $|w|$, we inspect the constraint $w+b \\ge 1.5$, which can be written as $w \\ge 1.5 - b$. To make $w$ as small as possible (while assuming $w \\ge 0$, which is reasonable as the line goes from $(0,0)$ to $(1,2)$), we must make $b$ as large as possible. The maximum value for $b$ in the feasible region is $b=0.5$. Substituting this into the inequality for $w$ gives $w \\ge 1.5 - 0.5 = 1$. The minimum possible value for $w$ is thus $1$.\n\nLet's check if the point $(w, b) = (1, 0.5)$ lies within the feasible region.\n- $b=0.5$: The condition $-0.5 \\le 0.5 \\le 0.5$ is met.\n- $w+b = 1+0.5=1.5$: The condition $1.5 \\le 1.5 \\le 2.5$ is met.\nSo, the point $(w, b) = (1, 0.5)$ is in the feasible region. At this point, the objective function value is $\\frac{1}{2} w^2 = \\frac{1}{2} (1)^2 = 0.5$.\n\nNow, we must consider if a solution with non-zero slack variables could yield a lower objective value. Any non-zero slack adds a positive penalty to the objective. A better solution would only be possible if allowing some penalty permitted a reduction in the $\\frac{1}{2}w^2$ term that is greater than the penalty incurred.\nLet's consider a small deviation from our current solution, e.g., reducing $w$ to $w' = 1-\\delta$ for some small $\\delta  0$. To minimize slack penalties, we might keep $b=0.5$. Then $w'+b' = 1.5-\\delta$. The constraint $w+b \\ge 1.5$ is violated. To satisfy it, we require $\\xi_2 \\ge \\delta$. The minimal penalty is $\\xi_2=\\delta$. The new objective value would be $\\frac{1}{2}(1-\\delta)^2 + \\delta = \\frac{1}{2}(1-2\\delta+\\delta^2) + \\delta = 0.5 - \\delta + 0.5\\delta^2 + \\delta = 0.5 + 0.5\\delta^2$. This value is greater than $0.5$. This reasoning suggests that any deviation that introduces slack penalties will increase the objective value. Thus, the optimal solution is indeed $(w, b) = (1, 0.5)$, with all slack variables being zero.\n\nTo formally confirm this solution and identify the support vectors, we use the Karush–Kuhn–Tucker (KKT) conditions. A data point $i$ is a support vector if its corresponding Lagrange multiplier $\\alpha_i$ or $\\alpha_i^*$ is non-zero. The KKT complementarity conditions require that for an active constraint, the multiplier can be non-zero, and for a non-zero multiplier, the constraint must be active.\n\nLet's examine our solution $(w,b)=(1,0.5)$ and $f(x)=x+0.5$:\n- For point $1$ $(x_1, y_1) = (0, 0)$:\n  - The prediction is $f(0) = 1(0) + 0.5 = 0.5$.\n  - The residual is $y_1 - f(0) = 0 - 0.5 = -0.5$.\n  - The absolute residual $|-0.5| = 0.5$, which is exactly equal to $\\epsilon$.\n  - The specific constraint that is active is $(w x_1 + b) - y_1 = \\epsilon$. Since this constraint is active, its associated Lagrange multiplier $\\alpha_1^*$ can be non-zero. Therefore, point $1$ is a support vector.\n\n- For point $2$ $(x_2, y_2) = (1, 2)$:\n  - The prediction is $f(1) = 1(1) + 0.5 = 1.5$.\n  - The residual is $y_2 - f(1) = 2 - 1.5 = 0.5$.\n  - The absolute residual $|0.5| = 0.5$, which is also exactly equal to $\\epsilon$.\n  - The specific constraint that is active is $y_2 - (w x_2 + b) = \\epsilon$. Since this constraint is active, its associated Lagrange multiplier $\\alpha_2$ can be non-zero. Therefore, point $2$ is also a support vector.\n\nBoth data points lie exactly on the boundary of the $\\epsilon$-tube. The KKT conditions for stationarity are $\\sum(\\alpha_i - \\alpha_i^*) = 0$ and $w = \\sum(\\alpha_i - \\alpha_i^*)x_i$. For our solution, $\\alpha_1=0$ and $\\alpha_2^*=0$ because only one side of the tube boundary is touched for each point. The stationarity conditions become $\\alpha_2 - \\alpha_1^* = 0$ and $w = \\alpha_2 x_2 = \\alpha_2$. Since $w=1$, we get $\\alpha_2=1$. This implies $\\alpha_1^*=1$. Since $C=1$, both multipliers are at their upper bound, which is consistent with the points being on the boundary (or outside, which is not the case here as all $\\xi_i, \\xi_i^*$ are zero). All KKT conditions are satisfied, confirming that $(w,b)=(1,0.5)$ is the exact optimal solution.\n\nThe optimal parameters are $w=1$ and $b=0.5$. Both training points $(0,0)$ and $(1,2)$ are support vectors because they lie on the boundary of the $\\epsilon$-tube, meaning their residuals have a magnitude exactly equal to $\\epsilon$. This is confirmed by their non-zero Lagrange multipliers ($\\alpha_1^*=1$ and $\\alpha_2=1$) in the dual problem formulation, which is a necessary condition for being a support vector from KKT theory.", "answer": "$$\n\\boxed{\\begin{pmatrix} 1  0.5 \\end{pmatrix}}\n$$", "id": "3178709"}, {"introduction": "Understanding the theory is one thing; applying it effectively is another. A crucial part of using SVR in practice is tuning its hyperparameters, and the choice of $\\epsilon$ can dramatically alter a model's behavior. This exercise [@problem_id:3178721] provides a hands-on demonstration of this principle, tasking you with building a simple SVR solver and using it to see how a poorly chosen $\\epsilon$ can lead to poor extrapolation, and how a better choice can correct it.", "problem": "You are asked to demonstrate, by explicit construction and computation, how Support Vector Regression (SVR) with a linear model can extrapolate poorly when the epsilon-insensitive tube parameter is mis-specified, and how retuning the epsilon-insensitive parameter improves both the learned slope and extrapolation error. Work in the setting of linear Support Vector Regression (SVR), where the predictor has the form $f(x) = w x + b$, and the empirical risk is based on the epsilon-insensitive loss. Use a one-dimensional dataset and solve the primal formulation by a principled convex optimization procedure.\n\nFundamental base to use:\n- The empirical risk minimization (ERM) principle: choose parameters $(w, b)$ to minimize a regularized empirical loss over a training set $\\{(x_i, y_i)\\}_{i=1}^n$.\n- The epsilon-insensitive loss used in Support Vector Regression (SVR): for residual $r_i = y_i - f(x_i)$, the loss is $L_{\\epsilon}(r_i) = \\max(0, |r_i| - \\epsilon)$.\n- The convex regularized objective in the primal form: minimize the sum of the regularization term and the empirical epsilon-insensitive loss.\n\nYour task:\n1. Use the fixed training dataset of $n = 13$ points specified below. The input locations are:\n   $x = [-1.5, -1.3, -1.0, -0.7, -0.4, -0.2, 0.0, 0.2, 0.5, 0.8, 1.0, 1.3, 1.5]$.\n   The corresponding outputs are generated by a true linear function plus small deterministic noise:\n   $y_i = 2 x_i + 1 + \\nu_i$ for the same $x_i$, with fixed noise values\n   $\\nu = [0.02, -0.03, 0.01, 0.05, -0.04, 0.03, 0.0, -0.02, 0.05, -0.01, 0.02, -0.04, 0.03]$,\n   so that $y$ is fully determined.\n\n2. Define the primal SVR objective to be minimized as\n   $$J(w, b) = \\frac{1}{2}\\lambda w^2 + C \\sum_{i=1}^{n} \\max\\big(0, |y_i - (w x_i + b)| - \\epsilon\\big),$$\n   where $\\lambda  0$ is the regularization weight on the slope $w$, $C  0$ is the empirical loss weight, and $\\epsilon \\ge 0$ is the epsilon-insensitive tube radius. You must treat $b$ as unregularized in the objective (that is, there is no penalty on $b$).\n\n3. Implement a correct algorithm to minimize $J(w, b)$ over $(w, b)$. You must use a principled convex optimization approach that starts from the above fundamental base. A valid approach is to use a subgradient method with a diminishing step size to handle the non-differentiability of the epsilon-insensitive loss. Your implementation must:\n   - Initialize $w$ and $b$ to $0$.\n   - Iterate updates based on a subgradient of $J(w, b)$ with respect to both $w$ and $b$.\n   - Use a diminishing step size schedule of the form $\\eta_t = \\eta_0 / \\sqrt{t + 1}$ for $t = 0, 1, \\dots, T-1$ with a fixed $\\eta_0  0$ and a sufficiently large number of iterations $T$ to ensure convergence for the given dataset.\n   - Use fixed hyperparameters $\\lambda = 1.0$, $C = 10.0$, $\\eta_0 = 0.01$, and $T = 10000$.\n\n4. Construct a counterexample to extrapolation by evaluating the trained linear SVR outside the training domain at the single test point $x_{\\text{out}} = 3.0$, and comparing it to the true underlying linear function $f^{\\star}(x) = 2x + 1$. Define the extrapolation error as the absolute difference:\n   $$E(\\epsilon) = \\big| \\, (w(\\epsilon) \\cdot x_{\\text{out}} + b(\\epsilon)) - (2 \\cdot x_{\\text{out}} + 1) \\, \\big|.$$\n\n5. Show how retuning $\\epsilon$ alters the learned slope $w(\\epsilon)$ and reduces extrapolation error $E(\\epsilon)$, by computing $(w(\\epsilon), E(\\epsilon))$ for the following test suite of epsilon values:\n   - A mis-specified large epsilon: $\\epsilon = 3.5$ (intended to produce poor extrapolation by collapsing the model toward $w \\approx 0$).\n   - A well-chosen small epsilon: $\\epsilon = 0.1$ (intended to recover a slope close to the true $w^{\\star} = 2$ and produce small extrapolation error).\n   - A boundary case: $\\epsilon = 0.0$ (absolute deviation loss, testing robustness).\n\n6. Your program must produce a single line of output containing the six results, ordered by the epsilon values as listed above. The line must be a comma-separated list enclosed in square brackets, in the following exact order, each value rounded to $6$ decimal places:\n   $$[w(3.5), E(3.5), w(0.1), E(0.1), w(0.0), E(0.0)].$$\n\nNotes:\n- Work entirely in purely mathematical terms; no physical units are involved.\n- Support Vector Regression (SVR) must be defined and used explicitly as above.\n- You must not use any shortcut formulas beyond the fundamental base and the stated objective; instead derive and implement the optimization updates from first principles.\n- Ensure scientific realism and internal consistency by adhering to the given dataset and optimization setup.", "solution": "The problem requires the demonstration of how Support Vector Regression (SVR) can extrapolate poorly with a mis-specified epsilon-insensitive tube parameter, $\\epsilon$, and how performance improves with a better-chosen $\\epsilon$. This will be shown by solving the primal SVR objective for a given dataset using a subgradient descent method.\n\nThe linear SVR model is given by $f(x) = w x + b$, where $w$ and $b$ are scalar parameters since the input $x$ is one-dimensional.\n\nThe objective is to find the parameters $(w, b)$ that minimize the regularized empirical risk function $J(w, b)$:\n$$\nJ(w, b) = \\frac{1}{2}\\lambda w^2 + C \\sum_{i=1}^{n} \\max\\big(0, |y_i - (w x_i + b)| - \\epsilon\\big)\n$$\nHere, $\\lambda  0$ is the regularization parameter, $C  0$ is the penalty parameter for the loss, and $\\epsilon \\ge 0$ defines the radius of the epsilon-insensitive tube. The term $\\frac{1}{2}\\lambda w^2$ is a Tikhonov regularization term that penalizes large slope values to prevent overfitting, and the summation term is the total empirical loss over the training set $\\{(x_i, y_i)\\}_{i=1}^n$.\n\nThe objective function $J(w,b)$ is convex because it is a sum of two convex functions: the regularization term $\\frac{1}{2}\\lambda w^2$ (which is strictly convex in $w$) and the empirical loss term. The loss function $L_{\\epsilon}(r) = \\max(0, |r| - \\epsilon)$ is a convex function of the residual $r$, and since the residual $r_i = y_i - (w x_i + b)$ is an affine function of $(w, b)$, the composition $L_{\\epsilon}(r_i(w, b))$ is also convex in $(w, b)$.\n\nDue to the presence of the absolute value and the $\\max$ operator, the objective function is non-differentiable at points where $|y_i - (w x_i + b)| = \\epsilon$ or $y_i - (w x_i + b) = 0$. We will therefore use the subgradient method, an extension of gradient descent for non-differentiable convex functions.\n\nThe update rule for subgradient descent at iteration $t$ is:\n$$\nw_{t+1} = w_t - \\eta_t g_{w,t}\n$$\n$$\nb_{t+1} = b_t - \\eta_t g_{b,t}\n$$\nwhere $\\eta_t$ is the step size and $(g_{w,t}, g_{b,t})$ is a subgradient of $J$ evaluated at $(w_t, b_t)$.\n\nA subgradient of a sum of functions is the sum of their subgradients. Thus, we can find the subgradient of $J(w, b)$ by summing the gradient of the regularization term and a subgradient of the loss term.\n\n1.  **Gradient of the Regularization Term:**\n    The regularization term $J_{reg}(w) = \\frac{1}{2}\\lambda w^2$ is differentiable with respect to $w$.\n    $$\n    \\frac{\\partial J_{reg}}{\\partial w} = \\lambda w\n    $$\n    This term does not depend on $b$, so its derivative with respect to $b$ is $0$.\n\n2.  **Subgradient of the Loss Term:**\n    The loss term is $J_{loss}(w, b) = C \\sum_{i=1}^{n} L_{\\epsilon}(r_i)$, where $r_i = y_i - (w x_i + b)$.\n    Let's find a subgradient of $L_{\\epsilon}(r_i) = \\max(0, |r_i| - \\epsilon)$ with respect to $w$ and $b$. We use the chain rule for subdifferentials. A subgradient of $L_{\\epsilon}(r_i)$ with respect to $w$ can be computed as $(\\partial_{r_i} L_{\\epsilon}) \\cdot (\\frac{\\partial r_i}{\\partial w})$.\n\n    First, let's find a subgradient of $L_{\\epsilon}$ with respect to the residual $r_i$. Let\n    $\\partial_{r_i} L_{\\epsilon}$ denote an element of the subdifferential.\n    -   If $|r_i|  \\epsilon$, then $|r_i| - \\epsilon  0$, so $L_{\\epsilon}(r_i) = 0$. The derivative is $0$.\n    -   If $r_i  \\epsilon$, then $|r_i| = r_i$, so $L_{\\epsilon}(r_i) = r_i - \\epsilon$. The derivative is $1$.\n    -   If $r_i  -\\epsilon$, then $|r_i| = -r_i$, so $L_{\\epsilon}(r_i) = -r_i - \\epsilon$. The derivative is $-1$.\n    -   At the non-differentiable points $|r_i| = \\epsilon$, we can select any value from the subdifferential interval. For algorithmic purposes, we can choose $0$.\n\n    A valid choice for a subgradient of $L_{\\epsilon}(r_i)$ with respect to $r_i$, denoted $g_{r_i}$, is:\n    $$\n    g_{r_i} \\in \\partial_{r_i} L_{\\epsilon}(r_i) \\quad \\text{where} \\quad g_{r_i} = \\begin{cases}\n    1  \\text{if } r_i  \\epsilon \\\\\n    -1  \\text{if } r_i  -\\epsilon \\\\\n    0  \\text{if } |r_i| \\le \\epsilon\n    \\end{cases}\n    $$\n    The partial derivatives of the residual $r_i$ with respect to $w$ and $b$ are:\n    $$\n    \\frac{\\partial r_i}{\\partial w} = -x_i \\quad \\text{and} \\quad \\frac{\\partial r_i}{\\partial b} = -1\n    $$\n    Using the chain rule, a subgradient of the total loss term with respect to $w$ and $b$ is:\n    $$\n    g_{J_{loss}, w} = C \\sum_{i=1}^n (g_{r_i} \\cdot \\frac{\\partial r_i}{\\partial w}) = C \\sum_{i=1}^n g_{r_i} (-x_i) = -C \\sum_{i=1}^n g_{r_i} x_i\n    $$\n    $$\n    g_{J_{loss}, b} = C \\sum_{i=1}^n (g_{r_i} \\cdot \\frac{\\partial r_i}{\\partial b}) = C \\sum_{i=1}^n g_{r_i} (-1) = -C \\sum_{i=1}^n g_{r_i}\n    $$\n\n3.  **Full Subgradient of J(w, b):**\n    Combining the parts, a subgradient $(g_w, g_b)$ of the full objective $J(w, b)$ is:\n    $$\n    g_w = \\lambda w - C \\sum_{i=1}^n g_{r_i} x_i\n    $$\n    $$\n    g_b = - C \\sum_{i=1}^n g_{r_i}\n    $$\n    where $g_{r_i}$ is determined based on the residual $r_i = y_i - (w x_i + b)$ and $\\epsilon$ as defined above.\n\nThe algorithm proceeds by initializing $w_0=0$, $b_0=0$ and iteratively applying the update rules for $T$ steps, using the diminishing step size schedule $\\eta_t = \\eta_0 / \\sqrt{t + 1}$. We perform this procedure for each specified value of $\\epsilon \\in \\{3.5, 0.1, 0.0\\}$. For each resulting model $(w(\\epsilon), b(\\epsilon))$, we compute the extrapolation error at $x_{\\text{out}}=3.0$ as $E(\\epsilon) = |(w(\\epsilon) x_{\\text{out}} + b(\\epsilon)) - (2 x_{\\text{out}} + 1)|$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the SVR problem using subgradient descent for different epsilon values\n    and computes the resulting learned slope and extrapolation error.\n    \"\"\"\n    # Step 1: Define the fixed training dataset and problem parameters.\n    x_train = np.array([-1.5, -1.3, -1.0, -0.7, -0.4, -0.2, 0.0, 0.2, 0.5, 0.8, 1.0, 1.3, 1.5])\n    nu = np.array([0.02, -0.03, 0.01, 0.05, -0.04, 0.03, 0.0, -0.02, 0.05, -0.01, 0.02, -0.04, 0.03])\n    # The output values are generated by a true linear function plus noise.\n    y_train = 2 * x_train + 1 + nu\n\n    # Step 2: Define the SVR and optimization hyperparameters.\n    lambda_reg = 1.0\n    C = 10.0\n    eta0 = 0.01\n    T = 10000\n\n    # Step 3: Define the test case for extrapolation.\n    x_out = 3.0\n    # The true value at the extrapolation point.\n    y_true_out = 2 * x_out + 1\n\n    # Epsilon values to be tested, as specified in the problem.\n    epsilon_values = [3.5, 0.1, 0.0]\n\n    results = []\n\n    # Step 4: Iterate through each epsilon, train the SVR model, and evaluate it.\n    for epsilon in epsilon_values:\n        # Train the SVR model using subgradient descent.\n        w, b = train_svr(x_train, y_train, lambda_reg, C, epsilon, eta0, T)\n\n        # Calculate the model's prediction at the extrapolation point.\n        y_pred_out = w * x_out + b\n\n        # Calculate the extrapolation error.\n        extrapolation_error = abs(y_pred_out - y_true_out)\n\n        # Store the results with the required precision.\n        results.append(f\"{w:.6f}\")\n        results.append(f\"{extrapolation_error:.6f}\")\n\n    # Step 5: Print the final output in the specified format.\n    print(f\"[{','.join(results)}]\")\n\n\ndef train_svr(x, y, lambda_reg, C, epsilon, eta0, T):\n    \"\"\"\n    Implements subgradient descent to find the optimal w and b for SVR.\n\n    Args:\n        x (np.ndarray): Input feature vector.\n        y (np.ndarray): Target value vector.\n        lambda_reg (float): Regularization parameter.\n        C (float): Loss penalty parameter.\n        epsilon (float): Epsilon-insensitive tube radius.\n        eta0 (float): Initial learning rate for the step size schedule.\n        T (int): Number of iterations.\n\n    Returns:\n        tuple[float, float]: The learned parameters (w, b).\n    \"\"\"\n    # Initialize parameters.\n    w = 0.0\n    b = 0.0\n    n = len(x)\n\n    # Perform T iterations of subgradient descent.\n    for t in range(T):\n        # Calculate the diminishing step size for the current iteration.\n        eta_t = eta0 / np.sqrt(t + 1)\n\n        # Calculate the residuals for all data points.\n        residuals = y - (w * x + b)\n\n        # Determine the coefficients for the loss subgradient based on the residuals.\n        # This implements the subgradient g_ri from the theoretical derivation.\n        g_loss_coeffs = np.zeros(n)\n        g_loss_coeffs[residuals > epsilon] = 1.0\n        g_loss_coeffs[residuals  -epsilon] = -1.0\n\n        # Calculate the full subgradient of the objective function J(w, b).\n        # g_w = (gradient of regularization) + (subgradient of loss wrt w)\n        g_w = lambda_reg * w - C * np.sum(g_loss_coeffs * x)\n        # g_b = (subgradient of loss wrt b)\n        g_b = - C * np.sum(g_loss_coeffs)\n\n        # Update the parameters using the subgradient descent rule.\n        w = w - eta_t * g_w\n        b = b - eta_t * g_b\n\n    return w, b\n\nsolve()\n```", "id": "3178721"}, {"introduction": "The power of SVR is often unlocked through its dual formulation, which not only provides an efficient path to a solution but also offers deeper insights into the model's structure. This advanced practice explores how to determine the model's bias term, $b$, using the Karush-Kuhn-Tucker (KKT) conditions derived from the dual problem [@problem_id:3178771]. You will investigate how the distribution of support vectors—particularly when they cluster on one side of the $\\epsilon$-tube—can reveal systematic biases in your data and influence the final model.", "problem": "Consider the support vector regression problem with the linear function class, where the prediction is $f(\\mathbf{x}) = \\mathbf{w}^\\top \\mathbf{x} + b$. The learning problem uses the $\\varepsilon$-insensitive loss and $\\ell_2$ regularization on $\\mathbf{w}$. The training data consist of pairs $(\\mathbf{x}_i, y_i)$ for $i = 1, \\dots, n$, with $\\mathbf{x}_i \\in \\mathbb{R}^d$ and $y_i \\in \\mathbb{R}$. The regularization parameter is $C > 0$, and the tube width is $\\varepsilon > 0$. The fundamental base is the empirical risk minimization using the $\\varepsilon$-insensitive loss with convex constraints, and Lagrangian duality for convex optimization with linear constraints and Karush–Kuhn–Tucker (KKT) conditions. The objective is to examine how the bias term $b$ is computed from the KKT conditions and how it is affected when all support vectors lie on the same side of the $\\varepsilon$-tube.\n\nStarting from the convex optimization formulation with the $\\varepsilon$-insensitive loss, derive the dual optimization problem for the linear kernel case and describe how to compute the bias term $b$ from the KKT conditions. Implement a solver that:\n- solves the dual optimization for linear support vector regression using a numerical optimizer for small-scale instances,\n- computes $\\mathbf{w}$ and then computes $b$ using only non-bound support vectors (those with $0  \\alpha_i  C$ or $0  \\alpha_i^*  C$) according to the KKT relationships; when only one side of the tube has non-bound support vectors, use the corresponding side to compute $b$ and explain the potential bias,\n- if there are no non-bound support vectors at all (a boundary case), fall back to a consistent estimator for $b$ based on the data and the learned $\\mathbf{w}$.\n\nFinally, illustrate the impact of residual skewness on the computed $b$ by reporting, for each test case, the learned $b$ and the mean residual $\\frac{1}{n}\\sum_{i=1}^n (y_i - f(\\mathbf{x}_i))$.\n\nYour program must implement the above logic for the following three deterministic test cases with $d = 1$ and $n = 10$:\n\n- Test case $1$ (balanced residuals; support vectors on both sides):\n  - $x_i$ values: $[-1.0, -0.8, -0.6, -0.4, -0.2, 0.0, 0.2, 0.4, 0.6, 0.8]$.\n  - Ground-truth linear model used to synthesize $y_i$: $y_i = w_{\\text{true}} x_i + b_{\\text{true}} + \\text{noise}_i$ with $w_{\\text{true}} = 1.5$, $b_{\\text{true}} = 0.2$, and $\\text{noise}_i$ values $[0.12, -0.12, 0.08, -0.08, 0.15, -0.15, 0.05, -0.05, 0.11, -0.11]$.\n  - Use $C = 1.0$ and $\\varepsilon = 0.1$.\n\n- Test case $2$ (one-sided residuals; all support vectors on the upper side):\n  - $x_i$ values: identical to test case $1$.\n  - Ground-truth linear model: $y_i = w_{\\text{true}} x_i + b_{\\text{true}} + \\text{noise}_i$ with $w_{\\text{true}} = 1.0$, $b_{\\text{true}} = 0.0$, and strictly nonnegative $\\text{noise}_i$ values $[0.2, 0.15, 0.12, 0.18, 0.25, 0.05, 0.14, 0.11, 0.16, 0.22]$.\n  - Use $C = 1.0$ and $\\varepsilon = 0.1$.\n\n- Test case $3$ (boundary case with very small $C$; likely no non-bound support vectors):\n  - $x_i$ values: identical to test case $1$.\n  - Ground-truth linear model: $y_i = w_{\\text{true}} x_i + b_{\\text{true}} + \\text{noise}_i$ with $w_{\\text{true}} = 1.2$, $b_{\\text{true}} = -0.1$, and $\\text{noise}_i$ values $[0.09, -0.09, 0.08, -0.08, 0.07, -0.07, 0.06, -0.06, 0.05, -0.05]$.\n  - Use $C = 0.05$ and $\\varepsilon = 0.1$.\n\nFor each test case, compute and return:\n- the learned bias term $b$ as a floating-point number,\n- the mean residual $\\frac{1}{n}\\sum_{i=1}^n (y_i - f(\\mathbf{x}_i))$ as a floating-point number.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[b_1,m_1,b_2,m_2,b_3,m_3]$), where $b_k$ is the bias term for test case $k$ and $m_k$ is the corresponding mean residual. No physical units are involved, and all reported values must be real numbers.", "solution": "The problem of Support Vector Regression (SVR) seeks to find a function $f(\\mathbf{x})$ that approximates a set of training data points $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n$ while being as \"flat\" as possible. For the linear case, the function is $f(\\mathbf{x}) = \\mathbf{w}^\\top \\mathbf{x} + b$, where $\\mathbf{x}_i \\in \\mathbb{R}^d$, $y_i \\in \\mathbb{R}$, $\\mathbf{w} \\in \\mathbb{R}^d$, and $b \\in \\mathbb{R}$.\n\nThe problem is formulated as a convex optimization problem using the $\\varepsilon$-insensitive loss function, which does not penalize errors within a certain distance $\\varepsilon > 0$ of the true value. Deviations beyond this \"tube\" are penalized linearly, controlled by a regularization parameter $C > 0$. The flatness of the function is measured by the $\\ell_2$-norm of the weight vector, $\\|\\mathbf{w}\\|^2$.\n\n**Primal Formulation**\n\nTo handle errors, we introduce non-negative slack variables $\\xi_i$ and $\\xi_i^*$ for each data point $i$. The primal problem is to minimize the regularized risk:\n$$\n\\min_{\\mathbf{w}, b, \\boldsymbol{\\xi}, \\boldsymbol{\\xi}^*} \\quad \\frac{1}{2} \\|\\mathbf{w}\\|^2 + C \\sum_{i=1}^n (\\xi_i + \\xi_i^*)\n$$\nsubject to the constraints:\n$$\n\\begin{align*}\ny_i - (\\mathbf{w}^\\top \\mathbf{x}_i + b) \\le \\varepsilon + \\xi_i, \\quad i=1, \\dots, n \\\\\n(\\mathbf{w}^\\top \\mathbf{x}_i + b) - y_i \\le \\varepsilon + \\xi_i^*, \\quad i=1, \\dots, n \\\\\n\\xi_i, \\xi_i^* \\ge 0, \\quad i=1, \\dots, n\n\\end{align*}\n$$\nThe first two constraints ensure that the prediction $f(\\mathbf{x}_i) = \\mathbf{w}^\\top \\mathbf{x}_i + b$ is within the $\\varepsilon$-tube around $y_i$, up to the slack variables.\n\n**Lagrangian and Dual Formulation**\n\nTo solve this constrained optimization problem, we form the Lagrangian by introducing non-negative Lagrange multipliers $\\alpha_i, \\alpha_i^*$ for the main constraints and $\\mu_i, \\mu_i^*$ for the slack variable non-negativity constraints:\n$$\nL = \\frac{1}{2} \\|\\mathbf{w}\\|^2 + C \\sum_{i=1}^n (\\xi_i + \\xi_i^*) - \\sum_{i=1}^n \\alpha_i (\\varepsilon + \\xi_i - y_i + \\mathbf{w}^\\top \\mathbf{x}_i + b) - \\sum_{i=1}^n \\alpha_i^* (\\varepsilon + \\xi_i^* + y_i - \\mathbf{w}^\\top \\mathbf{x}_i - b) - \\sum_{i=1}^n \\mu_i \\xi_i - \\sum_{i=1}^n \\mu_i^* \\xi_i^*\n$$\nSetting the partial derivatives of $L$ with respect to the primal variables ($\\mathbf{w}, b, \\xi_i, \\xi_i^*$) to zero yields the Karush-Kuhn-Tucker (KKT) conditions for optimality:\n$$\n\\frac{\\partial L}{\\partial \\mathbf{w}} = \\mathbf{w} - \\sum_{i=1}^n (\\alpha_i - \\alpha_i^*) \\mathbf{x}_i = 0 \\implies \\mathbf{w} = \\sum_{i=1}^n (\\alpha_i - \\alpha_i^*) \\mathbf{x}_i\n$$\n$$\n\\frac{\\partial L}{\\partial b} = - \\sum_{i=1}^n (\\alpha_i - \\alpha_i^*) = 0 \\implies \\sum_{i=1}^n \\alpha_i = \\sum_{i=1}^n \\alpha_i^*\n$$\n$$\n\\frac{\\partial L}{\\partial \\xi_i} = C - \\alpha_i - \\mu_i = 0\n$$\n$$\n\\frac{\\partial L}{\\partial \\xi_i^*} = C - \\alpha_i^* - \\mu_i^* = 0\n$$\nSubstituting these back into the Lagrangian leads to the Wolfe dual optimization problem. The goal is to maximize the dual objective, which is equivalent to minimizing its negative:\n$$\n\\min_{\\boldsymbol{\\alpha}, \\boldsymbol{\\alpha}^*} \\quad \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n (\\alpha_i - \\alpha_i^*)(\\alpha_j - \\alpha_j^*) (\\mathbf{x}_i^\\top \\mathbf{x}_j) - \\sum_{i=1}^n y_i(\\alpha_i - \\alpha_i^*) + \\varepsilon \\sum_{i=1}^n (\\alpha_i + \\alpha_i^*)\n$$\nsubject to the constraints derived from the KKT conditions:\n$$\n\\begin{align*}\n\\sum_{i=1}^n (\\alpha_i - \\alpha_i^*) = 0 \\\\\n0 \\le \\alpha_i \\le C, \\quad i=1, \\dots, n \\\\\n0 \\le \\alpha_i^* \\le C, \\quad i=1, \\dots, n\n\\end{align*}\nThis is a Quadratic Programming (QP) problem in the $2n$ dual variables $\\alpha_i$ and $\\alpha_i^*$, which can be solved using a numerical optimizer.\n\n**Computation of the Bias Term $b$**\n\nOnce the optimal dual variables $\\alpha_i$ and $\\alpha_i^*$ are found, the weight vector $\\mathbf{w}$ is computed as $\\mathbf{w} = \\sum_{i=1}^n (\\alpha_i - \\alpha_i^*) \\mathbf{x}_i$. The bias term $b$ is determined using the KKT complementary slackness conditions:\n$$\n\\begin{align*}\n\\alpha_i (\\varepsilon + \\xi_i - y_i + \\mathbf{w}^\\top \\mathbf{x}_i + b) = 0 \\\\\n\\alpha_i^* (\\varepsilon + \\xi_i^* + y_i - \\mathbf{w}^\\top \\mathbf{x}_i - b) = 0 \\\\\n(C - \\alpha_i) \\xi_i = 0 \\\\\n(C - \\alpha_i^*) \\xi_i^* = 0\n\\end{align*}\nA data point $i$ is a **support vector** if $\\alpha_i > 0$ or $\\alpha_i^* > 0$. We are particularly interested in **non-bound support vectors**, for which the dual variables are strictly between $0$ and $C$.\n\n1.  If $0  \\alpha_i  C$, then $\\xi_i = 0$ and the first KKT condition implies $\\varepsilon - y_i + \\mathbf{w}^\\top \\mathbf{x}_i + b = 0$. This gives $b = y_i - \\mathbf{w}^\\top \\mathbf{x}_i - \\varepsilon$. These points lie exactly on the upper boundary of the $\\varepsilon$-tube.\n2.  If $0  \\alpha_i^*  C$, then $\\xi_i^* = 0$ and the second KKT condition implies $\\varepsilon + y_i - \\mathbf{w}^\\top \\mathbf{x}_i - b = 0$. This gives $b = y_i - \\mathbf{w}^\\top \\mathbf{x}_i + \\varepsilon$. These points lie exactly on the lower boundary of the $\\varepsilon$-tube.\n\nThe standard procedure for computing $b$ is to average the values obtained from all non-bound support vectors for numerical stability.\n$$\nb = \\text{mean}\\left( \\{y_i - \\mathbf{w}^\\top \\mathbf{x}_i - \\varepsilon \\mid 0  \\alpha_i  C\\} \\cup \\{y_i - \\mathbf{w}^\\top \\mathbf{x}_i + \\varepsilon \\mid 0  \\alpha_i^*  C\\} \\right)\n$$\n\nIf all non-bound support vectors lie on one side of the tube (e.g., only points with $0  \\alpha_i  C$ exist), this indicates a systematic skew in the residuals. For instance, if all such points are on the upper boundary, it means the model tends to under-predict ($y_i > f(\\mathbf{x}_i)$). Computing $b$ based solely on these points may capture this systematic offset, resulting in a non-zero mean residual $\\frac{1}{n}\\sum_{i=1}^n (y_i - f(\\mathbf{x}_i))$, which quantifies the model's overall bias.\n\nIn the boundary case where no non-bound support vectors exist (all $\\alpha_i, \\alpha_i^*$ are at $0$ or $C$), the above method fails. A consistent estimator for $b$ must satisfy the KKT conditions for all data points. A valid $b$ must lie in the interval $[b_{\\text{low}}, b_{\\text{high}}]$ where:\n$$\nb_{\\text{low}} = \\max_{i: \\alpha_i  C} (y_i - \\mathbf{w}^\\top \\mathbf{x}_i - \\varepsilon) \\quad \\text{and} \\quad b_{\\text{high}} = \\min_{i: \\alpha_i^*  C} (y_i - \\mathbf{w}^\\top \\mathbf{x}_i + \\varepsilon)\n$$\nA robust choice is the midpoint of this feasible interval: $b = \\frac{1}{2}(b_{\\text{low}} + b_{\\text{high}})$.\n\nThe implementation will solve the dual QP problem for each test case, compute $\\mathbf{w}$, and then calculate $b$ using the appropriate method based on the presence of non-bound support vectors. Finally, it will report the learned $b$ and the resulting mean residual.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve_svr(X, y, C, epsilon):\n    \"\"\"\n    Solves a linear Support Vector Regression problem.\n\n    Args:\n        X (np.ndarray): Input data of shape (n_samples, n_features).\n        y (np.ndarray): Target values of shape (n_samples,).\n        C (float): Regularization parameter.\n        epsilon (float): Epsilon-tube width.\n\n    Returns:\n        tuple: A tuple containing the bias `b` (float) and the mean residual (float).\n    \"\"\"\n    n_samples, n_features = X.shape\n\n    # Construct the Gram matrix for the linear kernel.\n    K = X @ X.T\n    \n    # Formulate the dual QP problem for scipy.optimize.minimize.\n    # We want to minimize: 0.5 * beta.T @ Q @ beta + p.T @ beta\n    # where beta is the concatenation of alpha and alpha_star vectors.\n    \n    # Quadratic term matrix Q\n    Q = np.block([\n        [K, -K],\n        [-K, K]\n    ])\n    \n    # Linear term vector p\n    p = np.concatenate([-y + epsilon, y + epsilon])\n    \n    # Objective function and its Jacobian (gradient)\n    def objective(beta):\n        return 0.5 * beta.T @ Q @ beta + p.T @ beta\n        \n    def jacobian(beta):\n        return Q @ beta + p\n        \n    # Define constraints for the optimizer.\n    # Equality constraint: sum(alpha_i - alpha_i_star) = 0\n    constraints = ({\n        'type': 'eq',\n        'fun': lambda beta: np.sum(beta[:n_samples]) - np.sum(beta[n_samples:]),\n        'jac': lambda beta: np.concatenate([np.ones(n_samples), -np.ones(n_samples)])\n    })\n    \n    # Box constraints (bounds) for dual variables: 0 = alpha_i, alpha_i_star = C\n    bounds = [(0, C)] * (2 * n_samples)\n    \n    # Initial guess for the dual variables\n    beta_0 = np.zeros(2 * n_samples)\n    \n    # Solve the QP problem using SLSQP\n    result = minimize(objective, beta_0, jac=jacobian, bounds=bounds, constraints=constraints, method='SLSQP')\n    beta = result.x\n    \n    alpha = beta[:n_samples]\n    alpha_star = beta[n_samples:]\n    \n    # A small tolerance to handle numerical precision issues when checking boundaries.\n    tol = 1e-6\n\n    # Compute the weight vector w. For d=1, w is a scalar.\n    w = np.sum((alpha - alpha_star).reshape(-1, 1) * X, axis=0)\n    \n    # Compute the bias term b using KKT conditions.\n    \n    # Find indices of non-bound support vectors\n    non_bound_sv_indices_upper = np.where((alpha > tol)  (alpha  C - tol))[0]\n    non_bound_sv_indices_lower = np.where((alpha_star > tol)  (alpha_star  C - tol))[0]\n\n    b_values = []\n    \n    # Check for non-bound SVs on the upper boundary\n    if len(non_bound_sv_indices_upper) > 0:\n        b_upper_candidates = y[non_bound_sv_indices_upper] - X[non_bound_sv_indices_upper] @ w - epsilon\n        b_values.extend(b_upper_candidates.tolist())\n        \n    # Check for non-bound SVs on the lower boundary\n    if len(non_bound_sv_indices_lower) > 0:\n        b_lower_candidates = y[non_bound_sv_indices_lower] - X[non_bound_sv_indices_lower] @ w + epsilon\n        b_values.extend(b_lower_candidates.tolist())\n\n    if len(b_values) > 0:\n        # Standard method: average over all non-bound SVs\n        b = np.mean(b_values)\n    else:\n        # Fallback method: no non-bound SVs exist.\n        # Compute b from the interval defined by all points satisfying KKT conditions.\n        s1_indices = np.where(alpha  C - tol)[0]\n        s2_indices = np.where(alpha_star  C - tol)[0]\n        \n        b_low = np.max(y[s1_indices] - X[s1_indices] @ w - epsilon)\n        b_high = np.min(y[s2_indices] - X[s2_indices] @ w + epsilon)\n        \n        b = (b_low + b_high) / 2\n        \n    # Calculate final predictions and the mean residual.\n    f_x = X @ w + b\n    mean_residual = np.mean(y - f_x)\n    \n    return b, mean_residual\n\ndef solve():\n    # Define the test cases from the problem statement.\n    \n    # Test case 1 (balanced residuals)\n    x1 = np.array([-1.0, -0.8, -0.6, -0.4, -0.2, 0.0, 0.2, 0.4, 0.6, 0.8])\n    w_true1, b_true1 = 1.5, 0.2\n    noise1 = np.array([0.12, -0.12, 0.08, -0.08, 0.15, -0.15, 0.05, -0.05, 0.11, -0.11])\n    y1 = w_true1 * x1 + b_true1 + noise1\n    params1 = (x1.reshape(-1, 1), y1, 1.0, 0.1)\n\n    # Test case 2 (one-sided residuals)\n    x2 = x1\n    w_true2, b_true2 = 1.0, 0.0\n    noise2 = np.array([0.2, 0.15, 0.12, 0.18, 0.25, 0.05, 0.14, 0.11, 0.16, 0.22])\n    y2 = w_true2 * x2 + b_true2 + noise2\n    params2 = (x2.reshape(-1, 1), y2, 1.0, 0.1)\n\n    # Test case 3 (boundary case, small C)\n    x3 = x1\n    w_true3, b_true3 = 1.2, -0.1\n    noise3 = np.array([0.09, -0.09, 0.08, -0.08, 0.07, -0.07, 0.06, -0.06, 0.05, -0.05])\n    y3 = w_true3 * x3 + b_true3 + noise3\n    params3 = (x3.reshape(-1, 1), y3, 0.05, 0.1)\n\n    test_cases = [params1, params2, params3]\n\n    results = []\n    for case in test_cases:\n        X, y, C, epsilon = case\n        b, mean_res = solve_svr(X, y, C, epsilon)\n        results.append(b)\n        results.append(mean_res)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3178771"}]}