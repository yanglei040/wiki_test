{"hands_on_practices": [{"introduction": "To build a strong foundation, we first need to understand *why* polynomial kernels are necessary. This exercise presents a classic scenario, a parity-like problem, that cannot be solved by simple linear or quadratic models. By working through this thought experiment, you will see firsthand how a cubic kernel captures higher-order feature interactions that are invisible to lower-degree kernels, thus illustrating their unique expressive power.", "problem": "You are asked to construct a binary classification dataset in a way that highlights when a polynomial kernel of degree $3$ can capture interactions that kernels of degree $1$ and $2$ cannot. Consider inputs $x \\in \\{-1,+1\\}^3$ and labels defined by a parity-like rule on triple interactions. You train a kernel method with inhomogeneous polynomial kernels of degrees $1$, $2$, and $3$ on noiseless data. The training objective is empirical risk minimization with zero training error required. Which choice correctly specifies a dataset and the minimal sample size needed so that a degree $3$ kernel can exactly fit the data while degree $1$ and degree $2$ kernels cannot, for any algorithm that returns a solution in the span of the chosen kernel features?\n\nA. Use the dataset that contains all $8$ points in $\\{-1,+1\\}^3$, with labels $y = x_1 x_2 x_3$. The minimal sample size to reliably learn the cubic structure is $8$.\n\nB. Use a dataset of $6$ distinct points in $\\{-1,+1\\}^3$ chosen so that exactly half have $x_1 x_2 x_3 = +1$ and half have $x_1 x_2 x_3 = -1$. The minimal sample size is $6$.\n\nC. Use the dataset that contains all $8$ points in $\\{-1,+1\\}^3$, with labels $y = x_1 x_2 x_3$. The minimal sample size to reliably learn the cubic structure is $7$.\n\nD. Use a dataset of $4$ points in $\\{-1,+1\\}^3$ (for example, those with Hamming weight equal to $1$ or $2$), with labels $y = x_1 x_2 x_3$. The minimal sample size is $4$.", "solution": "The user wants me to meticulously solve a machine learning problem concerning polynomial kernels.\n\n### Problem Validation\n\nFirst, I will validate the problem statement according to the specified protocol.\n\n#### Step 1: Extract Givens\n\n-   Input space: $x \\in \\{-1,+1\\}^3$.\n-   Labels: Defined by a parity-like rule on triple interactions, specified in the options as $y = x_1 x_2 x_3$.\n-   Models: Kernel methods with inhomogeneous polynomial kernels of degrees $d=1$, $d=2$, and $d=3$. The general form is $K_d(x, z) = (\\langle x, z \\rangle + c)^d$. We assume the standard choice of $c=1$, so $K_d(x, z) = (\\langle x, z \\rangle + 1)^d$.\n-   Training: Empirical risk minimization with a requirement of zero training error.\n-   Algorithm constraint: The returned solution must be in the span of the chosen kernel features, which is standard for kernel methods as per the Representer Theorem.\n-   Question: What is the minimal sample size $N$ for which a dataset exists such that a degree $3$ kernel can achieve zero training error, but degree $1$ and degree $2$ kernels cannot?\n\n#### Step 2: Validate Using Extracted Givens\n\n-   **Scientifically Grounded**: The problem is a canonical exercise in statistical learning theory, specifically concerning the expressive power of kernel methods. The concepts of polynomial kernels, feature spaces, and the non-separability of certain functions (like parity) are fundamental to the field. The setup is scientifically sound.\n-   **Well-Posed**: The problem asks for a minimal integer value ($N$) under specific, formalizable conditions. The existence and uniqueness of such a minimum are well-defined.\n-   **Objective**: The language is precise and uses standard, unambiguous terminology from machine learning. There are no subjective elements.\n\nThe problem does not violate any of the invalidity criteria. It is a well-posed, scientifically grounded problem in statistical learning.\n\n#### Step 3: Verdict and Action\n\nThe problem is **valid**. I will proceed with the derivation and solution.\n\n### Derivation\n\nThe core of the problem lies in understanding the function spaces associated with polynomial kernels on the domain $x \\in \\{-1,+1\\}^3$. A function $f(x)$ can be learned with zero error on a training set $\\{(x_i, y_i)\\}_{i=1}^N$ by a given kernel method if there exists a function $f$ in the associated Reproducing Kernel Hilbert Space (RKHS) such that $f(x_i) = y_i$ for all $i=1, \\dots, N$.\n\nThe feature map $\\phi_d(x)$ for an inhomogeneous polynomial kernel of degree $d$ includes features corresponding to all monomials of degree up to $d$. For inputs $x \\in \\{-1,+1\\}^3$, any component $x_j$ satisfies $x_j^2 = 1$. This simplifies the set of unique monomials.\n\nThe set of all possible functions on the domain $\\{-1,+1\\}^3$ forms an $8$-dimensional vector space, since the domain has $2^3=8$ points. A basis for this space is given by the set of all unique, non-trivial monomials:\n$$\n\\mathcal{B} = \\{1, x_1, x_2, x_3, x_1x_2, x_1x_3, x_2x_3, x_1x_2x_3\\}\n$$\nThese basis functions are mutually orthogonal under the expectation taken over a uniform distribution on $x \\in \\{-1,+1\\}^3$.\n\nThe target function is $g(x) = x_1 x_2 x_3$, which is the highest-degree monomial in this basis.\n\nLet's analyze the function spaces for different kernel degrees:\n\n1.  **Degree $d=1$ kernel**: The feature space $\\mathcal{F}_1$ is spanned by monomials of degree up to $1$.\n    $$ \\mathcal{F}_1 = \\text{span}\\{1, x_1, x_2, x_3\\} $$\n    This is a $4$-dimensional space. Clearly, $g(x) = x_1x_2x_3$ is not in $\\mathcal{F}_1$.\n\n2.  **Degree $d=2$ kernel**: The feature space $\\mathcal{F}_2$ is spanned by monomials of degree up to $2$.\n    $$ \\mathcal{F}_2 = \\text{span}\\{1, x_1, x_2, x_3, x_1x_2, x_1x_3, x_2x_3\\} $$\n    This is a $7$-dimensional space. The target function $g(x) = x_1x_2x_3$ is orthogonal to all basis vectors of $\\mathcal{F}_2$ and is therefore not in $\\mathcal{F}_2$. Thus, a degree $2$ kernel cannot represent the function $g(x)$ over its entire domain.\n\n3.  **Degree $d=3$ kernel**: The feature space $\\mathcal{F}_3$ is spanned by monomials of degree up to $3$.\n    $$ \\mathcal{F}_3 = \\text{span}\\{1, x_1, x_2, x_3, x_1x_2, x_1x_3, x_2x_3, x_1x_2x_3\\} $$\n    This space is the full $8$-dimensional space of functions on $\\{-1,+1\\}^3$. The target function $g(x) = x_1x_2x_3$ is a basis vector of this space. Therefore, a degree $3$ kernel can represent $g(x)$ exactly.\n\nNow, we must find the minimal sample size $N$ such that there exists a dataset $\\{(x_i, y_i)\\}_{i=1}^N$ with $y_i=g(x_i)$ that can be fit by a degree $3$ kernel but not by a degree $2$ kernel.\n\nA function from $\\mathcal{F}_2$ can fit the data if there exist coefficients $c_j$ such that for a function $f(x) = \\sum_{j=1}^7 c_j h_j(x)$ (where $h_j$ are the basis functions of $\\mathcal{F}_2$), we have $f(x_i) = y_i = g(x_i)$ for all $i=1, \\dots, N$. This is a system of $N$ linear equations in $7$ unknown coefficients.\n\nLet's analyze this for a sample of size $N$. This amounts to checking if the vector of labels $(g(x_1), \\dots, g(x_N))$ is in the span of the vectors of basis function values $\\{(h_j(x_1), \\dots, h_j(x_N))\\}_{j=1}^7$.\n\nConsider any dataset of size $N \\le 7$. Let the points be $x_1, \\dots, x_N$. The $8$ basis functions of $\\mathcal{B}$, when evaluated on these $N$ points, become $8$ vectors in $\\mathbb{R}^N$. Since $N \\le 7$, these $8$ vectors must be linearly dependent. This means there exists a non-zero set of coefficients $\\{a_j\\}_{j=1}^8$ such that:\n$$ \\sum_{j=1}^8 a_j h_j(x_i) = 0, \\quad \\text{for } i=1, \\dots, N $$\nwhere $h_1, \\dots, h_7$ are the basis for $\\mathcal{F}_2$ and $h_8(x) = g(x) = x_1x_2x_3$.\n\nA crucial property of these basis functions (which form a Walsh-Hadamard system) is that any $7$ of them are linearly independent over any set of $7$ distinct points. However, the full set of $8$ functions becomes linearly dependent. The coefficients of this dependency are given by the values of the basis functions at the point that was left out of the full set of $8$. For any set of $7$ points, the linear dependency relation that holds among the basis functions will have a non-zero coefficient $a_8$ for the $h_8=x_1x_2x_3$ term. This is because the coefficient $a_8$ is proportional to $h_8(x_k)$ where $x_k$ is the point left out, and $h_8(x_k)=x_{k1}x_{k2}x_{k3}$ is always $\\pm 1$.\n\nSince $a_8 \\ne 0$, we can rearrange the dependency:\n$$ h_8(x_i) = -\\frac{1}{a_8} \\sum_{j=1}^7 a_j h_j(x_i) $$\nThis shows that on any set of $N \\le 7$ points, the function $g(x)=x_1x_2x_3$ can be expressed as a linear combination of the basis functions for $\\mathcal{F}_2$. Consequently, for any dataset of size $N \\le 7$, a degree $2$ kernel (and by extension a degree $1$ kernel) *can* achieve zero training error.\n\nTherefore, to find a dataset that *cannot* be fit by a degree $2$ kernel, we need a sample size $N > 7$.\n\nNow consider $N=8$. Let the dataset consist of all $8$ points in $\\{-1,+1\\}^3$. The labels are $y_i = g(x_i) = x_{i1}x_{i2}x_{i3}$.\n-   A degree $3$ kernel can fit this data because $g(x)$ is in its feature space $\\mathcal{F}_3$. The learner can simply select $f(x)=g(x)$.\n-   A degree $2$ kernel cannot fit this data. A fit would require finding a function $f \\in \\mathcal{F}_2$ such that $f(x_i) = g(x_i)$ for all $8$ points $x_i$. This would imply $f(x) = g(x)$ everywhere on the domain, which is impossible as $g(x) \\notin \\mathcal{F}_2$. More formally, the $8$ vectors of basis function values $\\{(h_j(x_1), \\dots, h_j(x_8))\\}_{j=1}^8$ are linearly independent. Therefore, the vector for $h_8=g$ cannot be written as a linear combination of the vectors for $h_1, \\dots, h_7$.\n\nThus, the minimal sample size required is $N=8$, and a dataset that demonstrates this is the full set of all $8$ points.\n\n### Option-by-Option Analysis\n\n**A. Use the dataset that contains all $8$ points in $\\{-1,+1\\}^3$, with labels $y = x_1 x_2 x_3$. The minimal sample size to reliably learn the cubic structure is $8$.**\nThis option correctly identifies both the dataset and the minimal sample size. As derived above, a sample size of $N=8$ is necessary and sufficient. With all $8$ points, the linear independence of the full set of monomials ensures that the cubic term $x_1x_2x_3$ cannot be represented by lower-degree terms.\n**Verdict: Correct.**\n\n**B. Use a dataset of $6$ distinct points in $\\{-1,+1\\}^3$ chosen so that exactly half have $x_1 x_2 x_3 = +1$ and half have $x_1 x_2 x_3 = -1$. The minimal sample size is $6$.**\nThis option claims the minimal sample size is $6$. My derivation shows that for any dataset of size $N \\le 7$, a degree $2$ kernel can perfectly fit the data. Therefore, a size of $6$ is insufficient to guarantee that the degree $2$ kernel fails.\n**Verdict: Incorrect.**\n\n**C. Use the dataset that contains all $8$ points in $\\{-1,+1\\}^3$, with labels $y = x_1 x_2 x_3$. The minimal sample size to reliably learn the cubic structure is $7$.**\nThis option claims the minimal sample size is $7$. This is incorrect. As shown, for any sample of size $7$, the basis functions become linearly dependent in a way that allows a degree $2$ kernel to fit the data perfectly. The minimal size must be greater than $7$.\n**Verdict: Incorrect.**\n\n**D. Use a dataset of $4$ points in $\\{-1,+1\\}^3$ (for example, those with Hamming weight equal to $1$ or $2$), with labels $y = x_1 x_2 x_3$. The minimal sample size is $4$.**\nThis option claims the minimal sample size is $4$. This is incorrect for the same reason as options B and C. A sample of size $4$ is insufficient; a degree $2$ kernel can always fit a dataset of this size.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{A}$$", "id": "3158522"}, {"introduction": "Having established what polynomial kernels can do, we now turn to the practical challenge of using them robustly. The scaling of your input features can dramatically affect a model's performance, but this exercise reveals a principled way to achieve stability. You will derive how to adjust the kernel's offset parameter $c$ in response to feature scaling, ensuring that the underlying geometry of the problem remains invariant and making your model's training more predictable and reliable.", "problem": "Consider binary classification with a soft-margin Support Vector Machine (SVM) in a Reproducing Kernel Hilbert Space (RKHS) induced by the polynomial kernel $$k(\\mathbf{x},\\mathbf{y})=\\left(\\mathbf{x}^{\\top}\\mathbf{y}+c\\right)^{d},$$ where $\\mathbf{x},\\mathbf{y}\\in\\mathbb{R}^{p}$, $c\\ge 0$ is the kernel offset, and $d\\in\\mathbb{N}$ is the degree. The primal SVM objective uses hinge loss and RKHS norm regularization: for training examples $\\{(\\mathbf{x}_{i},y_{i})\\}_{i=1}^{n}$ with labels $y_{i}\\in\\{-1,+1\\}$, it minimizes $$\\frac{\\lambda}{2}\\|w\\|_{\\mathcal{H}}^{2}+\\frac{1}{n}\\sum_{i=1}^{n}\\max\\{0,1-y_{i}f(\\mathbf{x}_{i})\\},$$ where $f(\\mathbf{x})=\\langle w,\\varphi(\\mathbf{x})\\rangle_{\\mathcal{H}}+b$ and $\\varphi$ is the feature map associated with $k$.\n\nSuppose the inputs are rescaled by a positive scalar $\\alpha>0$, that is, $\\mathbf{x}\\mapsto\\alpha\\mathbf{x}$ for all inputs. Let the original kernel parameter be $c_{0}$ at $\\alpha=1$, and the regularization be $\\lambda_{0}$. Define the rescaled kernel as $$k_{\\alpha}(\\mathbf{x},\\mathbf{y})=\\left(\\alpha^{2}\\mathbf{x}^{\\top}\\mathbf{y}+c(\\alpha)\\right)^{d},$$ where $c(\\alpha)$ is the (to-be-chosen) offset used after rescaling the inputs.\n\nYou are asked to proceed from first principles as follows:\n\n- Using only the kernel definition and basic properties of RKHS feature maps (that is, $k(\\mathbf{x},\\mathbf{y})=\\langle\\varphi(\\mathbf{x}),\\varphi(\\mathbf{y})\\rangle_{\\mathcal{H}}$), impose the requirement that the rescaled kernel $k_{\\alpha}$ differ from the original kernel $k$ only by a global, input-independent multiplicative factor $s(\\alpha)^{2}>0$, that is, $$k_{\\alpha}(\\mathbf{x},\\mathbf{y})=s(\\alpha)^{2}\\,k(\\mathbf{x},\\mathbf{y})\\quad\\text{for all }\\mathbf{x},\\mathbf{y}.$$ Derive the resulting expressions for $s(\\alpha)$ and the relationship between $c(\\alpha)$ and $c_{0}$.\n\n- Under the same requirement, determine how the SVM regularization parameter must transform, that is, find $\\lambda(\\alpha)$ such that if $(w^{\\star},b^{\\star})$ solves the SVM at $(\\alpha=1,c_{0},\\lambda_{0})$, then $(w^{\\star}/s(\\alpha),b^{\\star})$ solves the SVM with rescaled data and parameters $(\\alpha,c(\\alpha),\\lambda(\\alpha))$ and achieves the same objective value and functional margins on the original inputs.\n\n- Briefly interpret the scaling of the geometric margin in the feature space when $(w,b)$ is held fixed versus when it is adjusted as above.\n\nFor the purposes of this problem, define “stabilize training under feature rescaling” to mean meeting the global multiplicative equivalence requirement $k_{\\alpha}=s(\\alpha)^{2}k$ so that the optimization problem can be made invariant by an appropriate rescaling of the regularization parameter. Provide your final answer as a single closed-form expression for $c(\\alpha)$ in terms of $\\alpha$ and $c_{0}$. No numerical evaluation is required, and no rounding is needed. Express your final answer without units.", "solution": "The problem asks us to find a transformation for the kernel offset $c$ and the SVM regularization parameter $\\lambda$ that makes the learning problem invariant to a scaling of the input features.\n\n### 1. Derivation of $c(\\alpha)$ and $s(\\alpha)$\n\nThe core requirement is that the rescaled kernel $k_{\\alpha}$ is equivalent to the original kernel $k$ up to a global scaling factor $s(\\alpha)^2$:\n$$k_{\\alpha}(\\mathbf{x},\\mathbf{y}) = s(\\alpha)^{2}\\,k(\\mathbf{x},\\mathbf{y})$$\nSubstituting the definitions for the kernels:\n$$ \\left(\\alpha^{2}\\mathbf{x}^{\\top}\\mathbf{y}+c(\\alpha)\\right)^{d} = s(\\alpha)^{2} \\left(\\mathbf{x}^{\\top}\\mathbf{y}+c_{0}\\right)^{d} $$\nTaking the $d$-th root of both sides, we get a linear relationship in the variable $z = \\mathbf{x}^{\\top}\\mathbf{y}$:\n$$ \\alpha^{2}z+c(\\alpha) = \\left(s(\\alpha)^2\\right)^{1/d} (z + c_{0}) $$\nFor this identity to hold for all $z$, we must match the coefficients of the powers of $z$:\n1.  **Matching coefficients of $z$**: $\\alpha^2 = (s(\\alpha)^2)^{1/d}$.\n2.  **Matching constant terms**: $c(\\alpha) = (s(\\alpha)^2)^{1/d} c_0$.\n\nFrom the first equation, we can solve for the scaling factor $s(\\alpha)$:\n$$ s(\\alpha)^2 = (\\alpha^2)^d = \\alpha^{2d} \\implies s(\\alpha) = \\alpha^d $$\nSubstituting $(s(\\alpha)^2)^{1/d} = \\alpha^2$ into the second equation gives the transformation for the offset $c$:\n$$ c(\\alpha) = \\alpha^2 c_0 $$\nThis is the required expression.\n\n### 2. Derivation of $\\lambda(\\alpha)$\n\nThe solution to an SVM is found by maximizing the dual objective function. For the original problem with parameters $(\\lambda_0, c_0)$, the dual objective is:\n$$ Q_0(\\boldsymbol{\\beta}) = \\sum_i \\beta_i - \\frac{1}{2\\lambda_0} \\sum_{i,j} \\beta_i \\beta_j y_i y_j k(\\mathbf{x}_i, \\mathbf{x}_j) $$\nFor the rescaled problem with parameters $(\\lambda(\\alpha), c(\\alpha))$, the dual is:\n$$ Q_{\\alpha}(\\boldsymbol{\\beta}) = \\sum_i \\beta_i - \\frac{1}{2\\lambda(\\alpha)} \\sum_{i,j} \\beta_i \\beta_j y_i y_j k_{\\alpha}(\\mathbf{x}_i, \\mathbf{x}_j) $$\nTo ensure the solution (the optimal dual variables $\\boldsymbol{\\beta}^\\star$) remains the same, the two objective functions must be equivalent, meaning they lead to the same maximizer. Substituting $k_{\\alpha} = s(\\alpha)^2 k = \\alpha^{2d} k$, we get:\n$$ Q_{\\alpha}(\\boldsymbol{\\beta}) = \\sum_i \\beta_i - \\frac{\\alpha^{2d}}{2\\lambda(\\alpha)} \\sum_{i,j} \\beta_i \\beta_j y_i y_j k(\\mathbf{x}_i, \\mathbf{x}_j) $$\nFor $Q_{\\alpha}$ and $Q_0$ to be equivalent, the coefficients of the quadratic term must match:\n$$ \\frac{1}{\\lambda_0} = \\frac{\\alpha^{2d}}{\\lambda(\\alpha)} \\implies \\lambda(\\alpha) = \\alpha^{2d} \\lambda_0 $$\n\nWith these transformations for $c$ and $\\lambda$, the dual problem is unchanged, leading to the same optimal $\\boldsymbol{\\beta}^\\star$ and thus the same functional form for the decision boundary.\n\n### 3. Geometric Margin Interpretation\n\nThe geometric margin is inversely proportional to the RKHS norm of the weight vector, $1/\\|w\\|_{\\mathcal{H}}$. The RKHS norm itself is dependent on the kernel. Our kernel equivalence $k_{\\alpha} = s(\\alpha)^2 k$ implies that the norm of a function $f$ in the rescaled RKHS relates to its norm in the original RKHS by $\\|f\\|_{\\mathcal{H}_{\\alpha}} = \\|f\\|_{\\mathcal{H}_0} / s(\\alpha)$.\nSince the optimal decision function $f^\\star$ is the same for both problems, its norm in the new space is $\\|f^\\star\\|_{\\mathcal{H}_{\\alpha}} = \\|f^\\star\\|_{\\mathcal{H}_0} / s(\\alpha)$.\nThe geometric margin for the new problem is therefore:\n$$ \\gamma_{\\alpha}^\\star = \\frac{1}{\\|f^\\star\\|_{\\mathcal{H}_{\\alpha}}} = \\frac{s(\\alpha)}{\\|f^\\star\\|_{\\mathcal{H}_0}} = s(\\alpha)\\gamma_0^\\star = \\alpha^d \\gamma_0^\\star $$\nThus, stabilizing the training problem results in the geometric margin scaling by a factor of $\\alpha^d$.", "answer": "$$\n\\boxed{\\alpha^{2}c_{0}}\n$$", "id": "3158558"}, {"introduction": "This final practice moves from theory to implementation, challenging you to build and test your own kernel-based classifier. You will implement Kernel Ridge Regression and explore the concept of \"partial\" polynomial kernels, which include only a select subset of interaction degrees. This coding task not only solidifies your understanding of the mechanics but also introduces the advanced idea of kernel engineering, where you can tailor a kernel's structure to better suit a specific problem.", "problem": "You are given a binary classification task and asked to compare the performance of tailored partial polynomial kernels that include only selected homogeneous degrees against the full inhomogeneous polynomial kernel obtained by the complete binomial expansion. Your program must implement classification via Kernel Ridge Regression (KRR) and report, for each specified test case, the difference in test misclassification rates between a partial kernel and the corresponding full binomial kernel of the same total degree and offset.\n\nStart from the following core definitions and principles:\n- For vectors $x \\in \\mathbb{R}^p$ and $y \\in \\mathbb{R}^p$, the homogeneous degree-$m$ polynomial kernel is the symmetric tensor inner product given by $k_m(x,y) = (x^\\top y)^m$ for integer $m \\ge 0$.\n- Given an integer total degree $d \\ge 0$ and an offset $c \\ge 0$, the full inhomogeneous polynomial kernel of total degree $d$ is built by incorporating all homogeneous degrees from $0$ to $d$ with nonnegative weights induced by the binomial structure. A partial polynomial kernel selects a subset of the homogeneous degrees. Specifically, for a subset $S \\subseteq \\{0,1,\\dots,d\\}$, define\n$$\nK_S(x,y) \\;=\\; \\sum_{m \\in S} \\binom{d}{m}\\, c^{\\,d-m}\\, (x^\\top y)^m.\n$$\nThe full kernel is obtained by taking $S = \\{0,1,\\dots,d\\}$ and will be denoted by $K_{\\text{full}}(x,y)$.\n\nLearning rule:\n- Perform Kernel Ridge Regression (KRR) for classification with regularization parameter $\\lambda > 0$. Let the training data be $\\{(x_i, y_i)\\}_{i=1}^n$ with labels $y_i \\in \\{-1,+1\\}$. Let $K \\in \\mathbb{R}^{n \\times n}$ be the training Gram matrix with entries $K_{ij} = K(x_i,x_j)$ for a chosen kernel $K$. The coefficient vector $\\alpha \\in \\mathbb{R}^n$ is defined as the unique solution of the linear system\n$$\n\\big(K + n \\lambda I_n\\big)\\, \\alpha \\;=\\; y,\n$$\nwhere $I_n$ is the $n \\times n$ identity matrix and $y = (y_1,\\dots,y_n)^\\top$. For a test point $z$, the prediction is\n$$\nf(z) \\;=\\; \\sum_{i=1}^n \\alpha_i\\, K(x_i, z),\n$$\nand the predicted label is $\\mathrm{sign}(f(z))$, with $\\mathrm{sign}(0)$ defined to be $+1$. The misclassification rate on a test set is the fraction of test points whose predicted labels disagree with their true labels, represented as a real number in $[0,1]$.\n\nTraining and test data:\n- Training inputs $X_{\\text{train}}$ in $\\mathbb{R}^{8 \\times 2}$ (each row is one input):\n  - $(\\,1.0,\\,1.0\\,)$\n  - $(\\,1.0,\\,-1.0\\,)$\n  - $(\\,-1.0,\\,1.0\\,)$\n  - $(\\,-1.0,\\,-1.0\\,)$\n  - $(\\,2.0,\\,0.5\\,)$\n  - $(\\,-2.0,\\,0.5\\,)$\n  - $(\\,0.5,\\,-2.0\\,)$\n  - $(\\,-0.5,\\,-2.0\\,)$\n- Training labels $y_{\\text{train}} \\in \\{-1,+1\\}^8$:\n  - $+1$, $-1$, $-1$, $+1$, $+1$, $-1$, $-1$, $+1$.\n- Test inputs $X_{\\text{test}}$ in $\\mathbb{R}^{8 \\times 2}$:\n  - $(\\,0.5,\\,0.5\\,)$\n  - $(\\,0.5,\\,-0.5\\,)$\n  - $(\\,-0.5,\\,0.5\\,)$\n  - $(\\,-0.5,\\,-0.5\\,)$\n  - $(\\,3.0,\\,0.1\\,)$\n  - $(\\,-3.0,\\,0.1\\,)$\n  - $(\\,0.1,\\,-3.0\\,)$\n  - $(\\,-0.1,\\,-3.0\\,)$\n- Test labels $y_{\\text{test}} \\in \\{-1,+1\\}^8$:\n  - $+1$, $-1$, $-1$, $+1$, $+1$, $-1$, $-1$, $+1$.\n\nTask:\n- For each test case below, compute two misclassification rates on the test set: one using the partial kernel $K_S$ and one using the full kernel $K_{\\text{full}}$ with the same $d$ and $c$. Report the difference\n$$\n\\Delta \\;=\\; \\text{error}(K_S) \\;-\\; \\text{error}(K_{\\text{full}}).\n$$\nA negative $\\Delta$ indicates the partial kernel achieved a lower error (an improvement), while a positive $\\Delta$ indicates the full kernel performed better.\n\nTest suite (each case is $(d, c, S, \\lambda)$ with $S$ listed explicitly):\n1. $(\\,3,\\, 1.0,\\, \\{2,3\\},\\, 10^{-2}\\,)$\n2. $(\\,3,\\, 1.0,\\, \\{0,1,2,3\\},\\, 10^{-2}\\,)$\n3. $(\\,3,\\, 1.0,\\, \\{2\\},\\, 10^{-2}\\,)$\n4. $(\\,4,\\, 0.5,\\, \\{4\\},\\, 10^{-2}\\,)$\n5. $(\\,2,\\, 1.0,\\, \\{2\\},\\, 10^{-3}\\,)$\n6. $(\\,3,\\, 1.0,\\, \\{0\\},\\, 10^{-2}\\,)$\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of decimal numbers, rounded to six decimal places, enclosed in square brackets. For example, an output with three results should look like $[\\text{r}_1,\\text{r}_2,\\text{r}_3]$ where each $\\text{r}_i$ is a real number rounded to six decimals.\n\nScientific realism and constraints:\n- All computations are over the real numbers, and no physical units are involved.\n- Angles are not used.\n- Percentages must be represented as decimals in $[0,1]$.\n- Ensure numerical stability by using $\\lambda > 0$ as specified for each test, and by solving linear systems directly without iterative approximations.\n\nYour program must be a complete, runnable program that performs these computations and prints the single required output line. No user input is needed.", "solution": "The user has provided a problem in the domain of statistical learning, specifically concerning Kernel Ridge Regression (KRR) with polynomial kernels. The task is to compare the classification performance of partial polynomial kernels against full inhomogeneous polynomial kernels.\n\nThe problem statement has been validated and found to be self-contained, scientifically grounded in the principles of statistical learning theory, and mathematically well-posed. The definitions for the polynomial kernel, the KRR learning rule, and the evaluation metric (misclassification rate) are standard and unambiguous. The KRR objective function involves solving a linear system $(K + n \\lambda I_n) \\alpha = y$. Since the Gram matrix $K$ is positive semi-definite and the regularization parameter $\\lambda$ is strictly positive, the matrix $(K + n \\lambda I_n)$ is symmetric and positive definite, which guarantees the existence of a unique, stable solution for the coefficient vector $\\alpha$. All data and parameters are explicitly provided. Therefore, the problem is valid, and a solution will be constructed.\n\nThe solution proceeds systematically through the following steps for each test case:\n1.  **Define Kernels**: For each case specified by $(d, c, S, \\lambda)$, we define two kernels: the partial kernel $K_S(x,y)$ using the given set of degrees $S$, and the full inhomogeneous kernel $K_{\\text{full}}(x,y)$ using the complete set of degrees $S_{\\text{full}} = \\{0, 1, \\dots, d\\}$. The kernel function is given by:\n    $$\n    K(x,y) = \\sum_{m \\in \\text{degrees}} \\binom{d}{m} c^{d-m} (x^\\top y)^m\n    $$\n2.  **Compute Gram Matrices**: For each kernel, we must compute the training Gram matrix and the test Gram matrix.\n    *   The training Gram matrix, $G \\in \\mathbb{R}^{n \\times n}$, is computed on the training data $X_{\\text{train}}$. Its entries are $G_{ij} = K(x_i, x_j)$, where $x_i, x_j$ are training samples. For efficiency, we first compute a matrix of all dot products, $D_{\\text{train}} = X_{\\text{train}} X_{\\text{train}}^\\top$, and then apply the kernel-defining sum to this matrix element-wise.\n    *   The test Gram matrix, $G_{\\text{test}} \\in \\mathbb{R}^{n_{\\text{test}} \\times n}$, is used for prediction. Its entries are $(G_{\\text{test}})_{ij} = K(z_i, x_j)$, where $z_i$ is a test sample and $x_j$ is a training sample. This is computed efficiently from the dot product matrix $D_{\\text{test}} = X_{\\text{test}} X_{\\text{train}}^\\top$.\n\n3.  **Solve for KRR Coefficients**: The core of KRR is solving for the coefficient vector $\\alpha \\in \\mathbb{R}^n$. For a given training Gram matrix $G$, regularization parameter $\\lambda$, training labels $y_{\\text{train}}$, and number of training samples $n$, we solve the linear system:\n    $$\n    (G + n \\lambda I_n) \\alpha = y_{\\text{train}}\n    $$\n    where $I_n$ is the $n \\times n$ identity matrix. This system is solved for $\\alpha$. This step is performed separately for the partial kernel (yielding $\\alpha_S$) and the full kernel (yielding $\\alpha_{\\text{full}}$).\n\n4.  **Predict and Evaluate**: Predictions for the test set $X_{\\text{test}}$ are generated using the computed coefficients and the test Gram matrix. The vector of predicted scores is $f_{\\text{test}} = G_{\\text{test}} \\alpha$. The predicted labels $\\hat{y}$ are obtained by taking the sign of these scores:\n    $$\n    \\hat{y}_i = \\mathrm{sign}(f_{\\text{test}, i})\n    $$\n    The problem specifies that $\\mathrm{sign}(0)$ should be mapped to $+1$. The misclassification rate is the fraction of test samples for which the predicted label does not match the true label $y_{\\text{test}}$:\n    $$\n    \\text{error} = \\frac{1}{n_{\\text{test}}} \\sum_{i=1}^{n_{\\text{test}}} \\mathbb{I}(\\hat{y}_i \\neq y_{\\text{test}, i})\n    $$\n    where $\\mathbb{I}(\\cdot)$ is the indicator function. This rate is calculated for both the partial kernel, $\\text{error}(K_S)$, and the full kernel, $\\text{error}(K_{\\text{full}})$.\n\n5.  **Compute Final Difference**: For each test case, the final result is the difference in misclassification rates:\n    $$\n    \\Delta = \\text{error}(K_S) - \\text{error}(K_{\\text{full}})\n    $$\nThis value quantifies the change in performance due to restricting the set of homogeneous degrees in the polynomial kernel. A negative value indicates an improvement from using the partial kernel. Note that for test case $2$, the set $S$ is $\\{0,1,2,3\\}$, which is the full set for $d=3$. Therefore, $K_S = K_{\\text{full}}$, and the resulting difference $\\Delta$ must be exactly $0$. This serves as a useful sanity check for the implementation.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import comb\n\ndef solve():\n    \"\"\"\n    Solves the kernel ridge regression problem for the specified test cases.\n    \"\"\"\n    # Define training and test data as specified in the problem statement.\n    X_train = np.array([\n        [1.0, 1.0], [1.0, -1.0], [-1.0, 1.0], [-1.0, -1.0],\n        [2.0, 0.5], [-2.0, 0.5], [0.5, -2.0], [-0.5, -2.0]\n    ])\n    y_train = np.array([1, -1, -1, 1, 1, -1, -1, 1])\n\n    X_test = np.array([\n        [0.5, 0.5], [0.5, -0.5], [-0.5, 0.5], [-0.5, -0.5],\n        [3.0, 0.1], [-3.0, 0.1], [0.1, -3.0], [-0.1, -3.0]\n    ])\n    y_test = np.array([1, -1, -1, 1, 1, -1, -1, 1])\n    \n    n_train = X_train.shape[0]\n\n    # Pre-compute dot product matrices for efficiency.\n    dots_train = X_train @ X_train.T\n    dots_test = X_test @ X_train.T\n\n    # Define the test suite. S is represented as a set for efficient lookup.\n    test_cases = [\n        # (d, c, S, lambda)\n        (3, 1.0, {2, 3}, 1e-2),\n        (3, 1.0, {0, 1, 2, 3}, 1e-2),\n        (3, 1.0, {2}, 1e-2),\n        (4, 0.5, {4}, 1e-2),\n        (2, 1.0, {2}, 1e-3),\n        (3, 1.0, {0}, 1e-2)\n    ]\n\n    def gram_matrix_from_dots(dot_products_matrix, d, c, S):\n        \"\"\"\n        Computes the Gram matrix from a pre-computed matrix of dot products.\n        \"\"\"\n        gram = np.zeros_like(dot_products_matrix, dtype=np.float64)\n        for m in S:\n            if m  0 or m > d:\n                continue\n            # Binomial coefficient can be zero if c=0 and d-m>0.\n            # dot_product can be zero.\n            # Use np.power to handle element-wise exponentiation safely.\n            term_coeff = comb(d, m) * np.power(c, d - m, dtype=np.float64)\n            gram += term_coeff * np.power(dot_products_matrix, m, dtype=np.float64)\n        return gram\n\n    def calculate_misclassification_rate(d, c, S, lam):\n        \"\"\"\n        Performs KRR and returns the misclassification rate on the test set.\n        \"\"\"\n        # 1. Compute training Gram matrix\n        K_train = gram_matrix_from_dots(dots_train, d, c, S)\n        \n        # 2. Solve for alpha coefficients\n        A = K_train + n_train * lam * np.identity(n_train)\n        alpha = np.linalg.solve(A, y_train)\n        \n        # 3. Compute test Gram matrix for predictions\n        K_test = gram_matrix_from_dots(dots_test, d, c, S)\n        \n        # 4. Make predictions\n        f_test = K_test @ alpha\n        y_pred = np.sign(f_test)\n        y_pred[y_pred == 0] = 1.0 # As per rule sign(0) = +1\n        \n        # 5. Calculate misclassification rate\n        error_rate = np.mean(y_pred != y_test)\n        return error_rate\n\n    results = []\n    # Memoization for full kernel errors to avoid redundant calculations\n    full_kernel_errors = {}\n\n    for d, c, S, lam in test_cases:\n        # Calculate error for the partial kernel K_S\n        error_S = calculate_misclassification_rate(d, c, S, lam)\n        \n        # Check if the full kernel error for this (d, c, lam) is already computed\n        full_params_key = (d, c, lam)\n        if full_params_key not in full_kernel_errors:\n            S_full = set(range(d + 1))\n            error_full = calculate_misclassification_rate(d, c, S_full, lam)\n            full_kernel_errors[full_params_key] = error_full\n        else:\n            error_full = full_kernel_errors[full_params_key]\n        \n        # Compute the difference and append to results\n        delta = error_S - error_full\n        results.append(delta)\n\n    # Format the final output as specified\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```", "id": "3158464"}]}