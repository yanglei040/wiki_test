{"hands_on_practices": [{"introduction": "While simple basis sets like $\\{1, x, x^2, \\dots\\}$ are intuitive, they often lack the desirable numerical properties that make model fitting stable and efficient. Orthogonal bases, where each function is \"perpendicular\" to every other one under a chosen inner product, provide a much more robust foundation. This exercise provides fundamental practice with the Gram-Schmidt process, the classical algorithm for transforming any linearly independent set of functions into an orthogonal one, giving you a core tool for building better basis expansions. [@problem_id:2161554]", "problem": "In numerical analysis and approximation theory, it is often useful to construct a set of orthogonal basis functions from a simpler, non-orthogonal set. Consider the space of real-valued functions that are continuous on the interval $[0, 1]$. The inner product of two functions $f(x)$ and $g(x)$ in this space is defined as:\n$$ \\langle f, g \\rangle = \\int_{0}^{1} f(x)g(x) \\, dx $$\nTwo functions are considered orthogonal if their inner product is zero.\n\nStarting with the initial, non-orthogonal basis set of functions $\\{v_1(x), v_2(x)\\}$, where $v_1(x) = 1$ and $v_2(x) = x$, construct a new set of orthogonal basis functions $\\{u_1(x), u_2(x)\\}$ by applying the following procedure:\n1.  Set the first orthogonal function to be the same as the first initial function: $u_1(x) = v_1(x)$.\n2.  Construct the second orthogonal function by subtracting the projection of $v_2(x)$ onto $u_1(x)$:\n    $$ u_2(x) = v_2(x) - \\frac{\\langle v_2, u_1 \\rangle}{\\langle u_1, u_1 \\rangle} u_1(x) $$\n\nFind the resulting orthogonal functions $u_1(x)$ and $u_2(x)$. Present your answer as a pair of functions.", "solution": "We work in the space of continuous real-valued functions on $[0,1]$ with inner product $\\langle f,g\\rangle=\\int_{0}^{1}f(x)g(x)\\,dx$. The initial set is $v_{1}(x)=1$ and $v_{2}(x)=x$.\n\nStep 1: Set $u_{1}(x)=v_{1}(x)=1$ by the given procedure.\n\nStep 2: Compute the projection coefficient of $v_{2}$ onto $u_{1}$:\n$$\n\\langle v_{2},u_{1}\\rangle=\\int_{0}^{1}x\\cdot 1\\,dx=\\int_{0}^{1}x\\,dx=\\frac{1}{2},\n\\qquad\n\\langle u_{1},u_{1}\\rangle=\\int_{0}^{1}1\\cdot 1\\,dx=\\int_{0}^{1}1\\,dx=1.\n$$\nThus,\n$$\nu_{2}(x)=v_{2}(x)-\\frac{\\langle v_{2},u_{1}\\rangle}{\\langle u_{1},u_{1}\\rangle}u_{1}(x)\n=x-\\frac{\\frac{1}{2}}{1}\\cdot 1=x-\\frac{1}{2}.\n$$\n\nVerification of orthogonality:\n$$\n\\langle u_{1},u_{2}\\rangle=\\int_{0}^{1}1\\cdot\\left(x-\\frac{1}{2}\\right)\\,dx=\\int_{0}^{1}x\\,dx-\\frac{1}{2}\\int_{0}^{1}1\\,dx=\\frac{1}{2}-\\frac{1}{2}=0,\n$$\nso $u_{1}$ and $u_{2}$ are orthogonal. Therefore, the orthogonal pair is $u_{1}(x)=1$ and $u_{2}(x)=x-\\frac{1}{2}$.", "answer": "$$\\boxed{\\begin{pmatrix}1 & x-\\frac{1}{2}\\end{pmatrix}}$$", "id": "2161554"}, {"introduction": "Now that we have a tool to create orthogonal bases, let's explore why they are so valuable in practical statistical modeling. The choice of basis functions is not just a theoretical concern; it has profound consequences for the numerical stability and interpretability of your results. This practice problem puts the naive monomial (or Vandermonde) basis head-to-head with a properly standardized orthogonal polynomial basis, allowing you to computationally verify the dramatic improvements in stability and the elegant invariance properties that orthogonal systems offer. [@problem_id:3102236]", "problem": "Consider supervised regression with linear basis expansions in the context of statistical learning. Let a dataset consist of inputs $x_i$ and targets $t_i$ for $i=1,\\dots,n$, and consider a linear model $m(x) = \\sum_{j=0}^{d} w_j \\,\\phi_j(x)$ where the basis functions $\\phi_j$ are fixed. The least-squares estimator minimizes the sum of squared residuals and can be computed from a design matrix $X$ with entries $X_{ij} = \\phi_j(x_i)$ using well-tested numerical linear algebra. The two-norm condition number of a matrix, denoted by $\\kappa_2(X)$, quantifies numerical stability of solving linear systems with $X$.\n\nThis problem investigates the effect of input rescaling under the affine transformation $x \\mapsto y = a x + b$ on two families of basis functions: the monomial (Vandermonde) basis and an orthogonal polynomial basis. The monomial basis is defined by $\\phi_j(x) = x^j$ for $j=0,\\dots,d$, and its design matrix $V$ is the Vandermonde matrix. The orthogonal polynomial basis considered here is the Legendre family $\\{P_j(z)\\}_{j=0}^{d}$ evaluated on standardized inputs $z$. Standardization is performed by mapping the observed input range to the interval $[-1,1]$ via\n$$\nz = \\frac{2(x - m_x)}{r_x}, \\quad m_x = \\frac{x_{\\min} + x_{\\max}}{2}, \\quad r_x = x_{\\max} - x_{\\min},\n$$\nand analogously for the rescaled inputs $y$,\n$$\nz' = \\frac{2(y - m_y)}{r_y}, \\quad m_y = \\frac{y_{\\min} + y_{\\max}}{2}, \\quad r_y = y_{\\max} - y_{\\min}.\n$$\nThe Legendre polynomials satisfy the parity property $P_j(-z) = (-1)^j P_j(z)$.\n\nStarting from the core definitions above, you will derive algorithmic tests of the following claims:\n- Invariance claim for the orthogonal polynomial basis: when the design matrix is built from standardized inputs, the coefficients computed by least squares are invariant under $x \\mapsto y = a x + b$ up to a predictable parity adjustment when $a < 0$.\n- Sensitivity claim for the Vandermonde basis: monomial-basis coefficients and the condition number of the design matrix are sensitive to the scaling factor $a$ and shift $b$, often resulting in large changes.\n\nYou must implement a program that, for each test case specified below, performs the following steps in pure mathematical terms:\n1. Generate evenly spaced inputs $x_i$ over a specified interval $[x_{\\min}, x_{\\max}]$.\n2. Compute targets $t_i = f(x_i)$ where $f(x) = \\sin(x)$, with the angle unit in radians.\n3. Fit a degree-$d$ model using the orthogonal polynomial basis $\\{P_j(z)\\}_{j=0}^{d}$ on standardized inputs $z$ built from $x$, obtaining coefficients $w^{\\mathrm{leg}}$. Then apply the rescaling $y_i = a x_i + b$, rebuild standardized inputs $z'$ from $y$, and refit to obtain coefficients $w'^{\\mathrm{leg}}$.\n4. Test the invariance claim by checking whether $w'^{\\mathrm{leg}}$ equals $w^{\\mathrm{leg}}$ up to the parity adjustment $(\\operatorname{sign}(a))^j$ on the $j$-th coefficient, that is, whether $\\max_j \\left| w'^{\\mathrm{leg}}_j - \\left(\\operatorname{sign}(a)\\right)^j w^{\\mathrm{leg}}_j \\right|$ is below a tolerance $\\epsilon$.\n5. Fit a degree-$d$ model using the monomial basis on the original inputs $x$, obtaining coefficients $w^{\\mathrm{van}}$, and on the rescaled inputs $y$, obtaining $w'^{\\mathrm{van}}$. Compute the relative coefficient change\n$$\n\\Delta = \\frac{\\left\\| w'^{\\mathrm{van}} - w^{\\mathrm{van}} \\right\\|_2}{\\left\\| w^{\\mathrm{van}} \\right\\|_2 + 10^{-12}},\n$$\nand the condition number ratio\n$$\n\\rho = \\frac{\\kappa_2(V_y)}{\\kappa_2(V_x)},\n$$\nwhere $V_x$ and $V_y$ are the Vandermonde design matrices on $x$ and $y$, respectively.\n6. Return for each test case a list $[B, \\Delta, \\rho]$ where $B$ is the boolean result of the invariance test in step $4$.\n\nUse least squares with the standard two-norm objective and compute condition numbers using the two-norm. Use the tolerance $\\epsilon = 10^{-9}$ for the invariance test. Angles for the sine function must be in radians. No physical units other than radians appear in this problem.\n\nTest suite:\n- Case $1$: $n = 50$, degree $d = 10$, $x_{\\min} = -3$, $x_{\\max} = 3$, $a = 2$, $b = 1$.\n- Case $2$: $n = 6$, degree $d = 5$, $x_{\\min} = -1$, $x_{\\max} = 1$, $a = -1$, $b = 0$.\n- Case $3$: $n = 50$, degree $d = 10$, $x_{\\min} = -1$, $x_{\\max} = 2$, $a = 100$, $b = -5$.\n- Case $4$: $n = 50$, degree $d = 10$, $x_{\\min} = -3$, $x_{\\max} = 3$, $a = 0.01$, $b = 100$.\n- Case $5$: $n = 50$, degree $d = 10$, $x_{\\min} = -2$, $x_{\\max} = 4$, $a = 1$, $b = 10$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with one element per test case. Each element must itself be a list of the form $[B,\\Delta,\\rho]$. For example, a valid output format is $[[\\text{True},0.1,2.0],[\\text{False},3.5,100.0]]$.", "solution": "The problem is valid as it is scientifically grounded in the principles of numerical linear algebra and statistical regression, well-posed with all necessary information provided, and objective in its formulation. It presents a standard, verifiable comparison between two common choices of basis functions.\n\nThe core of the problem lies in analyzing the numerical stability and coefficient invariance of linear models under an affine transformation of the input variable, $x \\mapsto y = a x + b$. We investigate two types of basis expansions: one using orthogonal polynomials (Legendre polynomials) on a standardized domain, and another using a naive monomial basis.\n\n**1. Orthogonal Polynomial Basis: Legendre Polynomials**\n\nThe key to the stability of the Legendre polynomial basis is the standardization of the input variable. For any variable $v$ defined over an interval $[v_{\\min}, v_{\\max}]$, the standardization map is given by:\n$$\nz(v) = \\frac{2(v - m_v)}{r_v}, \\quad \\text{where } m_v = \\frac{v_{\\min} + v_{\\max}}{2} \\text{ and } r_v = v_{\\max} - v_{\\min}.\n$$\nThis transformation maps the interval $[v_{\\min}, v_{\\max}]$ to $[-1, 1]$, which is the canonical domain of orthogonality for Legendre polynomials. The model is then constructed using basis functions $\\phi_j(x) = P_j(z(x))$.\n\nLet's analyze the effect of the affine transformation $y = a x + b$ on the standardized variable. The original inputs $x_i$ lie in $[x_{\\min}, x_{\\max}]$. The rescaled inputs $y_i$ will lie in a new interval $[y_{\\min}, y_{\\max}]$.\n\nCase 1: $a > 0$. The transformation is order-preserving.\n$y_{\\min} = a x_{\\min} + b$ and $y_{\\max} = a x_{\\max} + b$.\nThe new midpoint is $m_y = \\frac{(a x_{\\min} + b) + (a x_{\\max} + b)}{2} = a \\frac{x_{\\min} + x_{\\max}}{2} + b = a m_x + b$.\nThe new range is $r_y = (a x_{\\max} + b) - (a x_{\\min} + b) = a (x_{\\max} - x_{\\min}) = a r_x$.\nThe new standardized variable $z'$ is:\n$$\nz'(y) = \\frac{2(y - m_y)}{r_y} = \\frac{2((ax+b) - (am_x+b))}{a r_x} = \\frac{2a(x - m_x)}{a r_x} = z(x).\n$$\nSince $z'(y_i) = z(x_i)$, the basis functions evaluated at the new points are identical to the old ones: $\\phi_j(y_i) = P_j(z'(y_i)) = P_j(z(x_i)) = \\phi_j(x_i)$. The design matrices for both fits are therefore identical. Since the target vector $\\mathbf{t}$ remains unchanged, the least-squares solution for the coefficients must also be identical: $w'^{\\mathrm{leg}} = w^{\\mathrm{leg}}$.\n\nCase 2: $a < 0$. The transformation is order-reversing.\n$y_{\\min} = a x_{\\max} + b$ and $y_{\\max} = a x_{\\min} + b$.\nThe midpoint $m_y = a m_x + b$ is derived similarly.\nThe new range is $r_y = (a x_{\\min} + b) - (a x_{\\max} + b) = a (x_{\\min} - x_{\\max}) = -a (x_{\\max} - x_{\\min}) = |a| r_x$.\nThe new standardized variable $z'$ is:\n$$\nz'(y) = \\frac{2(y - m_y)}{r_y} = \\frac{2((ax+b) - (am_x+b))}{|a| r_x} = \\frac{a}{|a|} \\frac{2(x - m_x)}{r_x} = -z(x), \\text{ since } a < 0 \\implies a/|a| = -1.\n$$\nThe basis functions for the new fit are $\\phi_j(y_i) = P_j(z'(y_i)) = P_j(-z(x_i))$. Using the parity property of Legendre polynomials, $P_j(-z) = (-1)^j P_j(z)$, we get $\\phi_j(y_i) = (-1)^j P_j(z(x_i))$.\nLet $X^{\\mathrm{leg}}$ be the design matrix for the fit on $x$, with entries $(X^{\\mathrm{leg}})_{ij} = P_j(z(x_i))$. Let $X'^{\\mathrm{leg}}$ be the design matrix for the fit on $y$, with entries $(X'^{\\mathrm{leg}})_{ij} = P_j(z'(y_i))$. The relationship is $(X'^{\\mathrm{leg}})_{ij} = (-1)^j (X^{\\mathrm{leg}})_{ij}$. This can be written in matrix form as $X'^{\\mathrm{leg}} = X^{\\mathrm{leg}} S$, where $S$ is a diagonal matrix with $S_{jj} = (-1)^j$.\n\nThe two least-squares problems are to find $w^{\\mathrm{leg}}$ and $w'^{\\mathrm{leg}}$ that minimize $\\| X^{\\mathrm{leg}} w^{\\mathrm{leg}} - \\mathbf{t} \\|_2^2$ and $\\| X'^{\\mathrm{leg}} w'^{\\mathrm{leg}} - \\mathbf{t} \\|_2^2$. Substituting the relation between the matrices into the second problem gives $\\| (X^{\\mathrm{leg}} S) w'^{\\mathrm{leg}} - \\mathbf{t} \\|_2^2$. Let $w^* = S w'^{\\mathrm{leg}}$. The problem becomes finding $w^*$ that minimizes $\\| X^{\\mathrm{leg}} w^* - \\mathbf{t} \\|_2^2$. By uniqueness of the least-squares solution, $w^* = w^{\\mathrm{leg}}$. Thus, $S w'^{\\mathrm{leg}} = w^{\\mathrm{leg}}$. Since $S$ is its own inverse ($S^2=I$), we can solve for $w'^{\\mathrm{leg}}$: $w'^{\\mathrm{leg}} = S^{-1} w^{\\mathrm{leg}} = S w^{\\mathrm{leg}}$, which means $w'^{\\mathrm{leg}}_j = (-1)^j w^{\\mathrm{leg}}_j$.\n\nCombining both cases using $\\operatorname{sign}(a)$, we find that $w'^{\\mathrm{leg}}_j = (\\operatorname{sign}(a))^j w^{\\mathrm{leg}}_j$. The invariance test checks this relationship to a numerical tolerance $\\epsilon=10^{-9}$. The boolean $B$ is the result of this test.\n\n**2. Monomial Basis: Vandermonde Matrix**\n\nThe monomial basis is defined by $\\phi_j(x) = x^j$. The design matrix $V_x$ with entries $(V_x)_{ij} = x_i^j$ is a Vandermonde matrix. Unlike the orthogonal basis, this basis has no self-correcting standardization mechanism.\n\nWhen the input is transformed to $y = ax+b$, the new basis functions are $\\phi_j(y) = (ax+b)^j$. Using the binomial expansion:\n$$\n\\phi_j(y) = (ax+b)^j = \\sum_{k=0}^{j} \\binom{j}{k} (ax)^k b^{j-k} = \\sum_{k=0}^{j} \\left[ \\binom{j}{k} a^k b^{j-k} \\right] x^k = \\sum_{k=0}^{j} C_{jk} \\phi_k(x)\n$$\nEach new basis function is a linear combination of the old basis functions up to the same degree. This creates a complicated relationship between the coefficient vectors $w^{\\mathrm{van}}$ and $w'^{\\mathrm{van}}$, precluding any simple invariance. The relative change $\\Delta = \\frac{\\| w'^{\\mathrm{van}} - w^{\\mathrm{van}} \\|_2}{\\| w^{\\mathrm{van}} \\|_2 + 10^{-12}}$ is expected to be large.\n\nVandermonde matrices are notoriously ill-conditioned, especially when the interval of points is far from the origin or has a very large/small scale. The columns of the matrix, being powers of $x_i$, can become nearly collinear. For example, if all $|x_i| \\gg 1$, the vectors $[x_i^d]$ and $[x_i^{d-1}]$ will point in very similar directions, leading to a high condition number $\\kappa_2(V_x)$. The transformation $y=ax+b$ can drastically alter the input domain's scale and location, often exacerbating this problem. The ratio of condition numbers, $\\rho = \\frac{\\kappa_2(V_y)}{\\kappa_2(V_x)}$, quantifies this instability. Large values of $|a|$ or $|b|$ are expected to yield large values of $\\rho$.\n\n**3. Algorithmic Implementation**\n\nThe described analysis is implemented as follows:\n- For each test case, we generate $n$ inputs $x_i$ and targets $t_i = \\sin(x_i)$.\n- For the Legendre basis, we construct the design matrices $X^{\\mathrm{leg}}$ and $X'^{\\mathrm{leg}}$ by first standardizing the $x_i$ and rescaled $y_i$ inputs, respectively, and then evaluating the Legendre polynomials $P_j$ for $j=0, \\dots, d$. The coefficients $w^{\\mathrm{leg}}$ and $w'^{\\mathrm{leg}}$ are found using `numpy.linalg.lstsq`. The invariance claim is tested, yielding a boolean $B$.\n- For the monomial basis, we construct the Vandermonde matrices $V_x$ and $V_y$ using `numpy.vander`. Coefficients $w^{\\mathrm{van}}$ and $w'^{\\mathrm{van}}$ are again found via least squares. The relative coefficient change $\\Delta$ and the condition number ratio $\\rho$ (using `numpy.linalg.cond`) are computed.\n- The results $[B, \\Delta, \\rho]$ are collected for all test cases and formatted as specified.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import legendre\n\ndef run_case(n, d, x_min, x_max, a, b):\n    \"\"\"\n    Performs the calculations for a single test case.\n    \"\"\"\n    # Step 1: Generate evenly spaced inputs and compute targets\n    x = np.linspace(x_min, x_max, n, dtype=np.float64)\n    t = np.sin(x)\n\n    # --- Part 1: Orthogonal Polynomial Basis (Legendre) ---\n\n    # Step 3 (part 1): Fit the model using the original inputs x\n    m_x = (x_min + x_max) / 2.0\n    r_x = x_max - x_min\n    # Handle the case where the interval has zero width\n    z = 2.0 * (x - m_x) / r_x if r_x != 0 else np.zeros_like(x)\n    \n    X_leg_x = np.zeros((n, d + 1), dtype=np.float64)\n    for j in range(d + 1):\n        p_j = legendre(j)\n        X_leg_x[:, j] = p_j(z)\n        \n    w_leg, _, _, _ = np.linalg.lstsq(X_leg_x, t, rcond=None)\n\n    # Step 3 (part 2): Apply rescaling y = ax + b and refit the model\n    y = a * x + b\n    y_min, y_max = np.min(y), np.max(y)\n    \n    m_y = (y_min + y_max) / 2.0\n    r_y = y_max - y_min\n    z_prime = 2.0 * (y - m_y) / r_y if r_y != 0 else np.zeros_like(y)\n\n    X_leg_y = np.zeros((n, d + 1), dtype=np.float64)\n    for j in range(d + 1):\n        p_j = legendre(j)\n        X_leg_y[:, j] = p_j(z_prime)\n\n    w_prime_leg, _, _, _ = np.linalg.lstsq(X_leg_y, t, rcond=None)\n\n    # Step 4: Test the invariance claim for the Legendre basis coefficients\n    epsilon = 1e-9\n    s_a = np.sign(a)\n    # The parity factor is (sign(a))^j for the j-th coefficient\n    parity_factor = np.array([s_a**j for j in range(d + 1)])\n    \n    expected_w_prime_leg = parity_factor * w_leg\n    max_abs_diff = np.max(np.abs(w_prime_leg - expected_w_prime_leg))\n    B = bool(max_abs_diff < epsilon)\n\n    # --- Part 2: Monomial Basis (Vandermonde) ---\n\n    # Step 5 (part 1): Fit models using the monomial basis\n    V_x = np.vander(x, d + 1, increasing=True)\n    w_van, _, _, _ = np.linalg.lstsq(V_x, t, rcond=None)\n\n    V_y = np.vander(y, d + 1, increasing=True)\n    w_prime_van, _, _, _ = np.linalg.lstsq(V_y, t, rcond=None)\n    \n    # Step 5 (part 2): Compute the relative coefficient change Delta\n    delta_num = np.linalg.norm(w_prime_van - w_van)\n    delta_den = np.linalg.norm(w_van) + 1e-12\n    Delta = delta_num / delta_den\n\n    # Step 5 (part 3): Compute the condition number ratio rho\n    kappa_x = np.linalg.cond(V_x, 2)\n    kappa_y = np.linalg.cond(V_y, 2)\n    \n    rho = kappa_y / kappa_x if kappa_x != 0 else np.inf\n\n    # Step 6: Return the list of results for this case\n    return [B, Delta, rho]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # n, d, xmin, xmax, a, b\n        (50, 10, -3.0, 3.0, 2.0, 1.0),\n        (6, 5, -1.0, 1.0, -1.0, 0.0),\n        (50, 10, -1.0, 2.0, 100.0, -5.0),\n        (50, 10, -3.0, 3.0, 0.01, 100.0),\n        (50, 10, -2.0, 4.0, 1.0, 10.0),\n    ]\n\n    results = []\n    for case_params in test_cases:\n        result = run_case(*case_params)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # The format [B, Delta, rho] is achieved by Python's default str(list).\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3102236"}, {"introduction": "We often think of basis expansion as finding the \"best\" approximation of a true, underlying function within a chosen function space. This theoretical \"best\" fit is its orthogonal projection with respect to the continuous $L^2$ inner product. In practice, however, we don't have the true function; we have a finite set of data points, and our \"best\" fit is the one that minimizes squared error on that data—an empirical projection. This hands-on problem delves into the crucial distinction between these two perspectives, demonstrating how the distribution of your training data determines how well the practical, data-driven model aligns with the theoretical ideal. [@problem_id:3102308]", "problem": "You are given a function approximation task in a finite-dimensional subspace using basis functions under two different inner products. The setting is as follows. Let the input domain be the closed interval $[0,1]$. Consider the candidate basis functions $\\{v_k(x)\\}_{k=0}^{m-1}$ with $v_k(x) = x^k$ and $m = 4$, so the model subspace is the span of $\\{1, x, x^2, x^3\\}$. Define two inner products on the space of square-integrable functions on $[0,1]$:\n- The continuous $L^2$ inner product: for functions $g$ and $h$, $\\langle g, h \\rangle_{L^2} = \\int_{0}^{1} g(x) h(x) \\, dx$.\n- The empirical inner product associated with a finite training set $\\{x_i\\}_{i=1}^n$: $\\langle g, h \\rangle_{\\text{emp}} = \\frac{1}{n} \\sum_{i=1}^{n} g(x_i) h(x_i)$.\n\nThe target function is $f(x) = \\sin(6 \\pi x) + x$, where the angle for $\\sin$ is measured in radians.\n\nYour tasks are:\n1. Using only the core definitions of inner-product spaces and the Gram–Schmidt orthonormalization procedure, construct an orthonormal basis of the model subspace with respect to each inner product:\n   - An $L^2$-orthonormal basis $\\{\\psi_k\\}_{k=0}^{m-1}$ for $\\langle \\cdot, \\cdot \\rangle_{L^2}$.\n   - An empirical-orthonormal basis $\\{\\phi_k\\}_{k=0}^{m-1}$ for $\\langle \\cdot, \\cdot \\rangle_{\\text{emp}}$ given a specific training set $\\{x_i\\}_{i=1}^n$.\n   Each orthonormal basis must be obtained from the candidate set $\\{v_k\\}_{k=0}^{m-1}$ using the Gram–Schmidt procedure with the corresponding inner product.\n\n2. Form two projections of $f$ onto the model subspace:\n   - The $L^2$ projection $P_{L^2} f = \\sum_{k=0}^{m-1} b_k \\, \\psi_k$ with coefficients $b_k = \\langle f, \\psi_k \\rangle_{L^2}$.\n   - The empirical projection $P_{\\text{emp}} f = \\sum_{k=0}^{m-1} a_k \\, \\phi_k$ with coefficients $a_k = \\langle f, \\phi_k \\rangle_{\\text{emp}}$.\n   Note that $P_{L^2} f$ depends only on the continuous inner product, while $P_{\\text{emp}} f$ depends on the training set through $\\langle \\cdot, \\cdot \\rangle_{\\text{emp}}$.\n\n3. For each specified training set in the test suite below, evaluate both approximations on a test grid of $201$ evenly spaced points in $[0,1]$, namely $T = \\{t_j\\}_{j=0}^{200}$ with $t_j = \\frac{j}{200}$. Compute the maximum absolute pointwise difference between the two predictions over $T$:\n   $$\\Delta = \\max_{t \\in T} \\left| P_{\\text{emp}} f(t) - P_{L^2} f(t) \\right|.$$\n   The angle unit used for the sine function must be radians. No physical units are involved.\n\nTest suite (each item defines a training set $\\{x_i\\}_{i=1}^n$):\n- Case $1$ (uniform midpoints): $n = 101$ and $x_i = \\frac{i + 0.5}{101}$ for integers $i = 0, 1, \\dots, 100$.\n- Case $2$ (quadratic cluster near $0$): $n = 101$ and $x_i = \\left(\\frac{i + 0.5}{101}\\right)^2$ for integers $i = 0, 1, \\dots, 100$.\n- Case $3$ (small, fixed set): $n = 4$ and $x = [0.05, 0.2, 0.5, 0.95]$.\n\nYour program must:\n- Implement Gram–Schmidt orthonormalization with respect to each inner product using only the definitions above.\n- Construct $P_{L^2} f$ and $P_{\\text{emp}} f$ as described.\n- For each case, compute $\\Delta$ on the specified test grid.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for the three cases as a comma-separated list enclosed in square brackets, for example, \"[r1,r2,r3]\".\n- Each $r_k$ must be a floating-point number rounded to exactly $6$ decimal places.", "solution": "The problem requires a comparative analysis of two different approximations of a target function, $f(x)$, within a finite-dimensional subspace of polynomials. The core of the problem lies in understanding how the choice of an inner product, which defines the geometry of the function space, influences the \"best\" approximation. We are given a model subspace $V = \\text{span}\\{1, x, x^2, x^3\\}$, which is the space of polynomials of degree at most $3$. The approximations are orthogonal projections of $f(x)$ onto $V$ with respect to two distinct inner products: the continuous $L^2$ inner product and a discrete empirical inner product derived from a finite set of data points.\n\nThe fundamental tool for constructing these projections is the Gram-Schmidt orthonormalization procedure. Given a vector space with an inner product $\\langle \\cdot, \\cdot \\rangle$ and a set of linearly independent vectors $\\{v_k\\}_{k=0}^{m-1}$, this procedure generates an orthonormal basis $\\{u_k\\}_{k=0}^{m-1}$ for the span of $\\{v_k\\}$. The process is iterative:\nLet $w_0 = v_0$ and $u_0 = \\frac{w_0}{\\|w_0\\|}$.\nFor $k = 1, 2, \\dots, m-1$, we compute\n$$w_k = v_k - \\sum_{j=0}^{k-1} \\langle v_k, u_j \\rangle u_j$$\n$$u_k = \\frac{w_k}{\\|w_k\\|}$$\nwhere $\\|g\\| = \\sqrt{\\langle g, g \\rangle}$ is the norm induced by the inner product. The vector $\\sum_{j=0}^{k-1} \\langle v_k, u_j \\rangle u_j$ is the orthogonal projection of $v_k$ onto the subspace spanned by $\\{u_0, \\dots, u_{k-1}\\}$, so $w_k$ is the component of $v_k$ orthogonal to this subspace.\n\nOnce an orthonormal basis $\\{u_k\\}_{k=0}^{m-1}$ for the subspace $V$ is found, the orthogonal projection of any function $f$ onto $V$ is given by:\n$$P_V f = \\sum_{k=0}^{m-1} \\langle f, u_k \\rangle u_k$$\nThis projection is the unique element in $V$ that minimizes the distance $\\|f - p\\|$ for $p \\in V$.\n\nWe will now apply this framework to the two specified inner products. The candidate basis is $\\{v_k(x) = x^k\\}_{k=0}^{3}$.\n\n**1. The $L^2$ Projection ($P_{L^2} f$)**\n\nThe continuous $L^2$ inner product on the interval $[0,1]$ is defined as:\n$$\\langle g, h \\rangle_{L^2} = \\int_{0}^{1} g(x) h(x) \\, dx$$\nWe construct an $L^2$-orthonormal basis $\\{\\psi_k(x)\\}_{k=0}^{3}$ from $\\{v_k(x) = x^k\\}_{k=0}^{3}$.\n\nFor $k=0$:\n$w_0(x) = v_0(x) = 1$.\n$\\|w_0\\|^2_{L^2} = \\int_0^1 1^2 \\,dx = 1$.\n$\\psi_0(x) = \\frac{w_0(x)}{\\|w_0\\|_{L^2}} = 1$.\n\nFor $k=1$:\nThe projection of $v_1$ onto the span of $\\{\\psi_0\\}$ is $\\langle v_1, \\psi_0 \\rangle_{L^2} \\psi_0(x)$.\n$\\langle v_1, \\psi_0 \\rangle_{L^2} = \\int_0^1 x \\cdot 1 \\,dx = \\frac{1}{2}$.\n$w_1(x) = v_1(x) - \\frac{1}{2} \\psi_0(x) = x - \\frac{1}{2}$.\n$\\|w_1\\|^2_{L^2} = \\int_0^1 (x - \\frac{1}{2})^2 \\,dx = \\frac{1}{12}$.\n$\\psi_1(x) = \\frac{w_1(x)}{\\|w_1\\|_{L^2}} = \\sqrt{12}(x - \\frac{1}{2})$.\n\nThis process is continued for $k=2$ and $k=3$ to obtain $\\psi_2(x)$ and $\\psi_3(x)$. The resulting polynomials are scaled versions of the shifted Legendre polynomials on $[0,1]$.\n\nThe target function is $f(x) = \\sin(6 \\pi x) + x$. The $L^2$ projection of $f$ onto the model subspace is:\n$$P_{L^2} f(x) = \\sum_{k=0}^{3} b_k \\psi_k(x), \\quad \\text{where } b_k = \\langle f, \\psi_k \\rangle_{L^2} = \\int_0^1 f(x) \\psi_k(x) \\,dx$$\nThese coefficients $b_k$ are constants, as they depend only on the fixed function $f$ and the $L^2$ inner product, not on any training data. The integrals are computed numerically.\n\n**2. The Empirical Projection ($P_{\\text{emp}} f$)**\n\nThe empirical inner product is defined with respect to a training set $\\{x_i\\}_{i=1}^n$:\n$$\\langle g, h \\rangle_{\\text{emp}} = \\frac{1}{n} \\sum_{i=1}^{n} g(x_i) h(x_i)$$\nUnlike the $L^2$ case, this inner product, and therefore the resulting orthonormal basis and projection, depends on the specific choice of the points $\\{x_i\\}$. For each test case, we are given a different set of points.\n\nFor a specific training set, we construct an empirical-orthonormal basis $\\{\\phi_k(x)\\}_{k=0}^{3}$ from $\\{v_k(x) = x^k\\}_{k=0}^{3}$ using the same Gram-Schmidt procedure, but with $\\langle \\cdot, \\cdot \\rangle_{\\text{emp}}$ instead of $\\langle \\cdot, \\cdot \\rangle_{L^2}$.\n\nThe projection of $f$ is then:\n$$P_{\\text{emp}} f(x) = \\sum_{k=0}^{3} a_k \\phi_k(x), \\quad \\text{where } a_k = \\langle f, \\phi_k \\rangle_{\\text{emp}} = \\frac{1}{n} \\sum_{i=1}^{n} f(x_i) \\phi_k(x_i)$$\nThe polynomial $P_{\\text{emp}} f(x)$ is the ordinary least squares solution for fitting a polynomial of degree $3$ to the data points $(x_i, f(x_i))$. In Case $3$, where the number of points $n=4$ equals the number of basis functions $m=4$, this projection becomes the unique polynomial of degree at most $3$ that interpolates $f(x)$ at the four given points.\n\n**3. Comparison of Projections**\n\nThe problem asks for the maximum absolute pointwise difference between the two projections over a fine test grid $T = \\{t_j = \\frac{j}{200}\\}_{j=0}^{200}$:\n$$\\Delta = \\max_{t \\in T} \\left| P_{\\text{emp}} f(t) - P_{L^2} f(t) \\right|$$\nThis metric quantifies how much the data-driven approximation ($P_{\\text{emp}} f$) deviates from the \"true\" function-space approximation ($P_{L^2} f$). The deviation is expected to be small when the empirical distribution of points $\\{x_i\\}$ closely resembles the uniform distribution on $[0,1]$ (as in Case $1$) and larger when it is skewed (Case $2$) or sparse (Case $3$). The following code implements this entire procedure for each training set specified.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.integrate import quad\n\ndef solve():\n    \"\"\"\n    Computes the maximum difference between L2 and empirical projections\n    for three different training sets.\n    \"\"\"\n\n    # Define the target function and basis functions\n    m = 4\n    target_func = lambda x: np.sin(6 * np.pi * x) + x\n    basis_v = [np.poly1d([1] + [0] * k) for k in range(m - 1, -1, -1)]\n\n    # Define the test grid\n    test_grid = np.linspace(0, 1, 201)\n\n    # =========================================================================\n    # Part 1: L2 Projection (independent of test cases)\n    # =========================================================================\n\n    def l2_inner_product(p1, p2):\n        integrand = p1 * p2\n        return quad(integrand, 0, 1)[0]\n\n    def gram_schmidt(basis_functions, inner_prod_func):\n        \"\"\"\n        Applies classical Gram-Schmidt to a list of polynomial functions.\n        \"\"\"\n        orthonormal_basis = []\n        for v in basis_functions:\n            w = v\n            for u in orthonormal_basis:\n                proj_coeff = inner_prod_func(v, u)\n                w = w - proj_coeff * u\n            \n            norm_w_sq = inner_prod_func(w, w)\n            # Add a small epsilon for numerical stability, though problem setup\n            # guarantees linear independence.\n            if norm_w_sq < 1e-20:\n                # This should not be reached with the given problem sets.\n                # Handle gracefully by returning a zero polynomial if needed.\n                u_new = np.poly1d([0])\n            else:\n                u_new = w / np.sqrt(norm_w_sq)\n            \n            orthonormal_basis.append(u_new)\n        return orthonormal_basis\n\n    # Construct the L2-orthonormal basis {psi_k}\n    psi_basis = gram_schmidt(basis_v, l2_inner_product)\n\n    # Compute the coefficients b_k for the L2 projection\n    b_coeffs = []\n    for psi_k in psi_basis:\n        integrand = lambda x: target_func(x) * psi_k(x)\n        b_k = quad(integrand, 0, 1)[0]\n        b_coeffs.append(b_k)\n\n    # Construct the L2 projection polynomial P_L2 f\n    p_l2_f = np.poly1d([0])\n    for b_k, psi_k in zip(b_coeffs, psi_basis):\n        p_l2_f += b_k * psi_k\n\n    # Evaluate the L2 projection on the test grid\n    p_l2_f_values = p_l2_f(test_grid)\n\n    # =========================================================================\n    # Part 2: Empirical Projections (for each test case)\n    # =========================================================================\n\n    # Define test cases\n    test_cases = [\n        # Case 1: n = 101, uniform midpoints\n        (lambda: (101, (np.arange(101) + 0.5) / 101)),\n        # Case 2: n = 101, quadratic cluster near 0\n        (lambda: (101, ((np.arange(101) + 0.5) / 101)**2)),\n        # Case 3: n = 4, small, fixed set\n        (lambda: (4, np.array([0.05, 0.2, 0.5, 0.95])))\n    ]\n\n    results = []\n    for case_generator in test_cases:\n        n, x_nodes = case_generator()\n\n        # Define the empirical inner product for the current training set\n        def empirical_inner_product(p1, p2):\n            return np.mean(p1(x_nodes) * p2(x_nodes))\n\n        # Construct the empirical-orthonormal basis {phi_k}\n        phi_basis = gram_schmidt(basis_v, empirical_inner_product)\n\n        # Compute the coefficients a_k for the empirical projection\n        f_at_nodes = target_func(x_nodes)\n        a_coeffs = []\n        for phi_k in phi_basis:\n            a_k = np.mean(f_at_nodes * phi_k(x_nodes))\n            a_coeffs.append(a_k)\n\n        # Construct the empirical projection polynomial P_emp f\n        p_emp_f = np.poly1d([0])\n        for a_k, phi_k in zip(a_coeffs, phi_basis):\n            p_emp_f += a_k * phi_k\n\n        # Evaluate the empirical projection on the test grid\n        p_emp_f_values = p_emp_f(test_grid)\n\n        # Compute the maximum absolute pointwise difference Delta\n        delta = np.max(np.abs(p_emp_f_values - p_l2_f_values))\n        results.append(delta)\n\n    # Final print statement in the exact required format\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3102308"}]}