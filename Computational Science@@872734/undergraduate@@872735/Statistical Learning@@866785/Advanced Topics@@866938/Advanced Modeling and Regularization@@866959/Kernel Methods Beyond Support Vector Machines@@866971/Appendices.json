{"hands_on_practices": [{"introduction": "While Support Vector Machines are the most famous application of the kernel trick, its power extends to many other linear models. This exercise demonstrates how to \"kernelize\" logistic regression, a fundamental algorithm for classification, allowing it to learn non-linear decision boundaries. You will implement the optimization entirely in the dual space using Newton's method, providing deep insight into the mechanics of kernel methods and the practical benefits of warm-starting from a related model like Kernel Ridge Regression [@problem_id:3136166].", "problem": "Implement kernel logistic regression using Newton’s method entirely in the dual, employing the kernel trick. Then compare its convergence behavior when initialized at zero versus when warm-started by a kernel ridge regression solution. Your program must be a single, complete, runnable script that produces the final results in the specified format, with no user input.\n\nFundamental base and setting:\n- You are given a binary classification dataset $\\{(x_i,t_i)\\}_{i=1}^n$ with $x_i \\in \\mathbb{R}^d$ and targets $t_i \\in \\{-1, +1\\}$. Consider a positive definite kernel $k(\\cdot,\\cdot)$ with Gram matrix $K \\in \\mathbb{R}^{n \\times n}$, where $K_{ij} = k(x_i,x_j)$. Let $f(\\cdot)$ lie in the reproducing kernel Hilbert space induced by $k$, and, by the representer theorem, restrict to $f(\\cdot)=\\sum_{i=1}^n \\alpha_i k(x_i,\\cdot)$ so that the vector of values on the training set is $f = K \\alpha$, where $\\alpha \\in \\mathbb{R}^n$.\n- Use the logistic negative log-likelihood with regularization parameter $\\lambda > 0$:\n  $$J(\\alpha) = \\sum_{i=1}^n \\log\\!\\big(1 + \\exp(-t_i f_i)\\big) + \\frac{\\lambda}{2}\\,\\alpha^\\top K \\alpha,$$\n  where $f = K \\alpha$. You must derive the gradient and the Hessian of $J(\\alpha)$ with respect to $\\alpha$ using only the chain rule, the derivative of the logistic function, and the properties of the kernel matrix. Do not assume any result beyond well-tested formulas for the logistic loss and linear algebra identities.\n- Use the radial basis function (Gaussian) kernel with length-scale $\\ell > 0$:\n  $$k(x,x') = \\exp\\!\\left(-\\frac{\\lVert x-x' \\rVert_2^2}{2 \\ell^2}\\right).$$\n\nAlgorithmic requirements:\n- Implement Newton’s method in the dual for minimizing $J(\\alpha)$:\n  - At each iteration, form the gradient and Hessian with respect to $\\alpha$ and compute the Newton step by solving a linear system. You must implement a backtracking line search to ensure descent of $J(\\alpha)$ when the full step does not decrease the objective. Use a backtracking factor of $0.5$.\n  - Terminate when the Euclidean norm of the gradient is at most $10^{-6}$ or when the number of iterations reaches $50$, whichever occurs first. Count a Newton iteration when you compute a search direction, regardless of how many backtracking reductions you perform.\n  - For numerical stability, you may add a tiny ridge term $\\epsilon I$ with $\\epsilon = 10^{-10}$ to the Hessian system you solve.\n- Implement kernel ridge regression (KRR) with the same kernel and the same regularization strength $\\lambda$ for labels $t \\in \\{-1,+1\\}$. Use the standard KRR system\n  $$(K + \\lambda I)\\,\\alpha_{\\mathrm{krr}} = t,$$\n  solved exactly by linear algebra. Use $\\alpha_{\\mathrm{krr}}$ as a warm start for Newton’s method.\n\nTest suite:\nFor all cases below, generate data in $\\mathbb{R}^2$ as two Gaussian blobs with equal class sizes. Use independent and identically distributed Gaussian noise with isotropic covariance $\\sigma^2 I_2$. Use the specified pseudo-random seeds for reproducibility. Construct the kernel using the given length-scale $\\ell$.\n\n- Case A (happy path):\n  - Seed: $0$.\n  - Total samples: $n = 60$ (balanced between the two classes).\n  - Means: positive class mean $\\mu_+ = (1, 1)$, negative class mean $\\mu_- = (-1, -1)$.\n  - Standard deviation: $\\sigma = 0.3$.\n  - Kernel length-scale: $\\ell = 1.0$.\n  - Regularization: $\\lambda = 10^{-2}$.\n- Case B (near-constant-kernel edge): \n  - Seed: $1$.\n  - Total samples: $n = 60$ (balanced).\n  - Means: positive class mean $\\mu_+ = (0.7, -0.7)$, negative class mean $\\mu_- = (-0.7, 0.7)$.\n  - Standard deviation: $\\sigma = 0.4$.\n  - Kernel length-scale: $\\ell = 5.0$.\n  - Regularization: $\\lambda = 10^{-2}$.\n- Case C (overlap and stronger nonlinearity):\n  - Seed: $2$.\n  - Total samples: $n = 80$ (balanced).\n  - Means: positive class mean $\\mu_+ = (0.5, 0.5)$, negative class mean $\\mu_- = (-0.5, -0.5)$.\n  - Standard deviation: $\\sigma = 0.8$.\n  - Kernel length-scale: $\\ell = 0.5$.\n  - Regularization: $\\lambda = 10^{-2}$.\n\nWhat to compute and compare:\n- For each case, run Newton’s method twice:\n  1. Initialize with $\\alpha^{(0)} = 0$ and obtain the number of iterations $I_0$ required to meet the termination criterion.\n  2. Initialize with $\\alpha^{(0)} = \\alpha_{\\mathrm{krr}}$ and obtain the number of iterations $I_{\\mathrm{krr}}$ required.\n- For each case, define the indicator $B$ that equals $1$ if $I_{\\mathrm{krr}} < I_0$ and $0$ otherwise.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for the three cases as a comma-separated list of lists in the exact format:\n  $$[\\,[I_0^{(A)}, I_{\\mathrm{krr}}^{(A)}, B^{(A)}],\\,[I_0^{(B)}, I_{\\mathrm{krr}}^{(B)}, B^{(B)}],\\,[I_0^{(C)}, I_{\\mathrm{krr}}^{(C)}, B^{(C)}]\\,].$$\n- No extra text or whitespace beyond this single line should be printed. There are no physical units involved, and all computations are real-valued floating-point arithmetic.", "solution": "The user provided a valid problem. The solution is presented below.\n\n### I. Problem Formulation\n\nThe objective is to minimize the regularized negative log-likelihood for kernel logistic regression (KLR). The dataset consists of pairs $\\{(x_i, t_i)\\}_{i=1}^n$, where $x_i \\in \\mathbb{R}^d$ are input vectors and $t_i \\in \\{-1, +1\\}$ are binary class labels. Using a positive definite kernel $k(\\cdot, \\cdot)$, the decision function is represented in the dual form as $f(\\cdot) = \\sum_{j=1}^n \\alpha_j k(x_j, \\cdot)$. The vector of function evaluations on the training data is $f = K\\alpha$, where $K$ is the $n \\times n$ Gram matrix with entries $K_{ij} = k(x_i, x_j)$ and $\\alpha \\in \\mathbb{R}^n$ is the vector of dual coefficients.\n\nThe objective function to minimize with respect to $\\alpha$ is:\n$$\nJ(\\alpha) = \\sum_{i=1}^n \\log\\left(1 + \\exp(-t_i f_i)\\right) + \\frac{\\lambda}{2} \\alpha^\\top K \\alpha\n$$\nwhere $f_i = (K\\alpha)_i = \\sum_{j=1}^n K_{ij} \\alpha_j$ and $\\lambda > 0$ is the regularization parameter. This function is convex, ensuring a unique minimum exists. We will use Newton's method for optimization.\n\n### II. Derivation of Gradient and Hessian\n\nTo apply Newton's method, we must compute the gradient $\\nabla J(\\alpha)$ and Hessian $\\nabla^2 J(\\alpha)$ of the objective function.\n\n#### Gradient Derivation\n\nThe gradient is a vector in $\\mathbb{R}^n$ with components $\\frac{\\partial J}{\\partial \\alpha_k}$. We apply the chain rule.\n$$\n\\frac{\\partial J}{\\partial \\alpha_k} = \\frac{\\partial}{\\partial \\alpha_k} \\left( \\sum_{i=1}^n \\log(1 + e^{-t_i f_i}) \\right) + \\frac{\\partial}{\\partial \\alpha_k} \\left( \\frac{\\lambda}{2} \\alpha^\\top K \\alpha \\right)\n$$\nThe derivative of the loss term with respect to $f_i$ is:\n$$\n\\frac{\\partial}{\\partial f_i} \\log(1 + e^{-t_i f_i}) = \\frac{1}{1 + e^{-t_i f_i}} \\cdot (-t_i e^{-t_i f_i}) = -t_i \\frac{e^{-t_i f_i}}{1 + e^{-t_i f_i}} = -t_i \\frac{1}{1 + e^{t_i f_i}} = -t_i \\sigma(-t_i f_i)\n$$\nwhere $\\sigma(z) = (1+e^{-z})^{-1}$ is the sigmoid function.\nThe derivative of $f_i$ with respect to $\\alpha_k$ is:\n$$\n\\frac{\\partial f_i}{\\partial \\alpha_k} = \\frac{\\partial}{\\partial \\alpha_k} \\sum_{j=1}^n K_{ij} \\alpha_j = K_{ik}\n$$\nCombining these using the chain rule for the loss part:\n$$\n\\frac{\\partial}{\\partial \\alpha_k} \\left( \\sum_{i=1}^n \\log(1 + e^{-t_i f_i}) \\right) = \\sum_{i=1}^n \\frac{\\partial \\log(1+e^{-t_i f_i})}{\\partial f_i} \\frac{\\partial f_i}{\\partial \\alpha_k} = \\sum_{i=1}^n \\left( -t_i \\sigma(-t_i f_i) \\right) K_{ik}\n$$\nThe gradient of the regularization term is standard for quadratic forms (noting $K$ is symmetric):\n$$\n\\frac{\\partial}{\\partial \\alpha_k} \\left( \\frac{\\lambda}{2} \\alpha^\\top K \\alpha \\right) = \\lambda (K\\alpha)_k = \\lambda \\sum_{i=1}^n K_{ki} \\alpha_i\n$$\nCombining both terms, the $k$-th component of the gradient is:\n$$\n\\nabla_k J(\\alpha) = \\sum_{i=1}^n K_{ki} (-t_i \\sigma(-t_i f_i)) + \\lambda (K\\alpha)_k\n$$\nIn matrix-vector notation, let $g_f$ be a vector with elements $(g_f)_i = -t_i \\sigma(-t_i f_i)$. The gradient is:\n$$\n\\nabla J(\\alpha) = K g_f + \\lambda K \\alpha = K(g_f + \\lambda \\alpha)\n$$\n\n#### Hessian Derivation\n\nThe Hessian is an $n \\times n$ matrix with entries $H_{k\\ell} = \\frac{\\partial^2 J}{\\partial \\alpha_k \\partial \\alpha_\\ell}$. We differentiate the gradient components with respect to $\\alpha_\\ell$:\n$$\n\\frac{\\partial^2 J}{\\partial \\alpha_k \\partial \\alpha_\\ell} = \\frac{\\partial}{\\partial \\alpha_\\ell} \\left( \\sum_{i=1}^n K_{ki} (-t_i \\sigma(-t_i f_i)) \\right) + \\frac{\\partial}{\\partial \\alpha_\\ell} \\left( \\lambda (K\\alpha)_k \\right)\n$$\nFor the loss term, we again use the chain rule, differentiating with respect to $f_j$:\n$$\n\\frac{\\partial}{\\partial \\alpha_\\ell} (-t_i \\sigma(-t_i f_i)) = \\sum_{j=1}^n \\frac{\\partial (-t_i \\sigma(-t_i f_i))}{\\partial f_j} \\frac{\\partial f_j}{\\partial \\alpha_\\ell}\n$$\nThe term only depends on $f_i$, so the sum over $j$ collapses to $j=i$. The derivative is:\n$$\n\\frac{d}{df_i} (-t_i \\sigma(-t_i f_i)) = -t_i \\cdot \\frac{d}{df_i} \\sigma(-t_i f_i) = -t_i \\cdot \\left[ \\sigma'(-t_i f_i) \\cdot (-t_i) \\right] = t_i^2 \\sigma'(-t_i f_i) = \\sigma'(-t_i f_i)\n$$\nUsing the property $\\sigma'(z) = \\sigma(z)(1-\\sigma(z)) = \\sigma(z)\\sigma(-z)$:\n$$\n\\frac{d}{df_i} (-t_i \\sigma(-t_i f_i)) = \\sigma(-t_i f_i)(1 - \\sigma(-t_i f_i)) = \\sigma(-t_i f_i)\\sigma(t_i f_i)\n$$\nLet $W$ be a diagonal matrix with diagonal entries $W_{ii} = \\sigma(t_i f_i)\\sigma(-t_i f_i)$.\nThe derivative of the loss part of the gradient component $\\nabla_k J(\\alpha)$ is:\n$$\n\\frac{\\partial}{\\partial \\alpha_\\ell} \\sum_{i=1}^n K_{ki}(-t_i\\sigma(-t_i f_i)) = \\sum_{i=1}^n K_{ki} W_{ii} K_{i\\ell} = (K W K)_{k\\ell}\n$$\nThe derivative of the regularization part of the gradient is:\n$$\n\\frac{\\partial}{\\partial \\alpha_\\ell} (\\lambda (K\\alpha)_k) = \\frac{\\partial}{\\partial \\alpha_\\ell} \\left( \\lambda \\sum_{j=1}^n K_{kj} \\alpha_j \\right) = \\lambda K_{k\\ell}\n$$\nCombining terms, the Hessian matrix is:\n$$\n\\nabla^2 J(\\alpha) = K W K + \\lambda K\n$$\n\n### III. Algorithmic Design\n\n#### Newton's Method\n\nAt each iteration $m$, we find a search direction $\\Delta\\alpha^{(m)}$ by solving the linear system:\n$$\n\\left(\\nabla^2 J(\\alpha^{(m)}) + \\epsilon I\\right) \\Delta\\alpha^{(m)} = -\\nabla J(\\alpha^{(m)})\n$$\nwhere $\\epsilon = 10^{-10}$ is a small regularization term to ensure numerical stability. The coefficient vector is then updated:\n$$\n\\alpha^{(m+1)} = \\alpha^{(m)} + \\eta \\Delta\\alpha^{(m)}\n$$\nThe step size $\\eta$ is determined by a backtracking line search.\n\n#### Backtracking Line Search\n\nTo guarantee a decrease in the objective function $J(\\alpha)$, we start with a full step $\\eta=1$. If $J(\\alpha + \\eta\\Delta\\alpha) \\ge J(\\alpha)$, we repeatedly halve the step size, $\\eta \\leftarrow 0.5\\eta$, until the condition $J(\\alpha + \\eta\\Delta\\alpha) < J(\\alpha)$ is met.\n\n#### Kernel Ridge Regression Warm Start\n\nKernel Ridge Regression (KRR) solves a simpler \"least-squares\" version of the problem, finding coefficients $\\alpha_{\\mathrm{krr}}$ that minimize $\\|\\sum_i \\alpha_i k(x_i, \\cdot) - t\\|_{\\text{L2}}^2 + \\lambda \\|\\alpha\\|^2_I$. In the dual representation using the kernel trick, this leads to the closed-form solution:\n$$\n(K + \\lambda I) \\alpha_{\\mathrm{krr}} = t\n$$\nwhere $t$ is the vector of labels $\\{ -1, +1 \\}$. The solution $\\alpha_{\\mathrm{krr}}$ provides a good initial guess for the KLR optimization, as KRR can be seen as an approximation to KLR. We compare the convergence of Newton's method starting from $\\alpha^{(0)} = 0$ versus starting from $\\alpha^{(0)} = \\alpha_{\\mathrm{krr}}$. The number of iterations required for convergence is recorded in each case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.spatial.distance import cdist\nfrom scipy.special import expit\n\ndef solve():\n    \"\"\"\n    Implements and compares Kernel Logistic Regression solvers.\n    \"\"\"\n\n    # --- Helper Function for Data Generation ---\n    def generate_data(seed, n, mu_pos, mu_neg, sigma):\n        \"\"\"Generates a two-class Gaussian blob dataset.\"\"\"\n        rng = np.random.default_rng(seed)\n        n_pos = n // 2\n        n_neg = n - n_pos\n        \n        # Draw samples from two multivariate normal distributions\n        X_pos = rng.multivariate_normal(mu_pos, sigma**2 * np.eye(2), n_pos)\n        X_neg = rng.multivariate_normal(mu_neg, sigma**2 * np.eye(2), n_neg)\n        \n        # Combine data and create labels\n        X = np.vstack((X_pos, X_neg))\n        t = np.hstack((np.ones(n_pos), -np.ones(n_neg)))\n        \n        return X, t\n\n    # --- Helper Function for RBF Kernel ---\n    def rbf_kernel(X1, X2, length_scale):\n        \"\"\"Computes the RBF (Gaussian) kernel matrix.\"\"\"\n        # Calculate squared Euclidean distances between all pairs of points\n        sq_dists = cdist(X1, X2, 'sqeuclidean')\n        # Apply the kernel function\n        return np.exp(-sq_dists / (2 * length_scale**2))\n\n    # --- Helper Function for Kernel Ridge Regression ---\n    def solve_krr(K, t, lambda_reg):\n        \"\"\"Solves for the dual coefficients of Kernel Ridge Regression.\"\"\"\n        n = K.shape[0]\n        # Solve the linear system (K + lambda*I) * alpha = t\n        A = K + lambda_reg * np.eye(n)\n        alpha_krr = np.linalg.solve(A, t)\n        return alpha_krr\n\n    # --- Kernel Logistic Regression Solver Class ---\n    class KernelLogisticRegression:\n        \"\"\"\n        Implements Kernel Logistic Regression using Newton's method with backtracking.\n        \"\"\"\n        def __init__(self, K, t, lambda_reg, tol=1e-6, max_iter=50, hessian_eps=1e-10):\n            self.K = K\n            self.t = t\n            self.lambda_reg = lambda_reg\n            self.tol = tol\n            self.max_iter = max_iter\n            self.hessian_eps = hessian_eps\n            self.n = K.shape[0]\n\n        def _objective(self, alpha):\n            \"\"\"Computes the KLR objective function J(alpha).\"\"\"\n            f = self.K @ alpha\n            tf = self.t * f\n            # Use np.logaddexp for numerical stability: log(1+exp(-x))\n            log_likelihood = np.sum(np.logaddexp(0, -tf))\n            regularization = (self.lambda_reg / 2.0) * (alpha @ self.K @ alpha)\n            return log_likelihood + regularization\n\n        def _gradient(self, alpha):\n            \"\"\"Computes the gradient of J(alpha).\"\"\"\n            f = self.K @ alpha\n            tf = self.t * f\n            # Gradient of loss w.r.t. f is -t * sigma(-t*f)\n            g_f = -self.t * expit(-tf)\n            grad = self.K @ (g_f + self.lambda_reg * alpha)\n            return grad\n\n        def _hessian(self, alpha):\n            \"\"\"Computes the Hessian of J(alpha).\"\"\"\n            f = self.K @ alpha\n            tf = self.t * f\n            # W_ii = sigma(t_i*f_i) * (1 - sigma(t_i*f_i)) = sigma(t_i*f_i) * sigma(-t_i*f_i)\n            # expit(x) is the sigmoid function sigma(x)\n            w_diag = expit(tf) * expit(-tf)\n            W = np.diag(w_diag)\n            hess = self.K @ W @ self.K + self.lambda_reg * self.K\n            return hess\n\n        def solve(self, alpha0):\n            \"\"\"Performs optimization starting from alpha0.\"\"\"\n            alpha = np.copy(alpha0)\n            \n            for i in range(self.max_iter):\n                grad = self._gradient(alpha)\n                \n                # Termination condition: gradient norm\n                if np.linalg.norm(grad) = self.tol:\n                    return i\n                \n                # Compute search direction\n                hess = self._hessian(alpha)\n                H_reg = hess + self.hessian_eps * np.eye(self.n)\n                \n                try:\n                    delta_alpha = np.linalg.solve(H_reg, -grad)\n                except np.linalg.LinAlgError:\n                    # Fallback if regularized Hessian is still numerically singular\n                    return i + 1\n\n                # Backtracking line search with factor 0.5\n                eta = 1.0\n                J_current = self._objective(alpha)\n                while True:\n                    alpha_new = alpha + eta * delta_alpha\n                    J_new = self._objective(alpha_new)\n                    \n                    if J_new  J_current:\n                        break # Found a step size that improves objective\n                    \n                    eta *= 0.5\n                    \n                    # Failsafe to prevent infinitely small steps\n                    if eta  1e-12:\n                        break\n                \n                if eta  1e-12:\n                    # Could not find a descent direction, terminate\n                    return i + 1\n\n                alpha = alpha + eta * delta_alpha\n\n            return self.max_iter\n\n    # --- Test Suite Definition ---\n    test_cases = [\n        # Case A: Happy path, well-separated data\n        {'seed': 0, 'n': 60, 'mu_pos': (1, 1), 'mu_neg': (-1, -1), 'sigma': 0.3, 'l': 1.0, 'lambda_reg': 1e-2},\n        # Case B: Near-constant kernel edge case\n        {'seed': 1, 'n': 60, 'mu_pos': (0.7, -0.7), 'mu_neg': (-0.7, 0.7), 'sigma': 0.4, 'l': 5.0, 'lambda_reg': 1e-2},\n        # Case C: High overlap, non-linear case\n        {'seed': 2, 'n': 80, 'mu_pos': (0.5, 0.5), 'mu_neg': (-0.5, -0.5), 'sigma': 0.8, 'l': 0.5, 'lambda_reg': 1e-2},\n    ]\n\n    all_results = []\n    \n    # --- Main Loop to Process Test Cases ---\n    for case in test_cases:\n        # Generate data and kernel matrix\n        X, t = generate_data(case['seed'], case['n'], case['mu_pos'], case['mu_neg'], case['sigma'])\n        K = rbf_kernel(X, X, case['l'])\n    \n        # Initialize the KLR solver\n        klr_solver = KernelLogisticRegression(K, t, case['lambda_reg'])\n        \n        # Run 1: Initialize with alpha = 0\n        alpha0_zero = np.zeros(case['n'])\n        I0 = klr_solver.solve(alpha0_zero)\n        \n        # Run 2: Initialize with KRR warm start\n        alpha_krr = solve_krr(K, t, case['lambda_reg'])\n        Ikrr = klr_solver.solve(alpha_krr)\n        \n        # Determine if warm-start was better\n        B = 1 if Ikrr  I0 else 0\n        \n        all_results.append(f\"[{I0},{Ikrr},{B}]\")\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```", "id": "3136166"}, {"introduction": "The true power of kernel methods lies not just in using standard kernels, but in designing custom ones that encode specific prior knowledge or invariances. This practice guides you through the process of constructing a kernel that is insensitive to global sign flips, a common ambiguity in signal processing and other domains. By applying this bespoke kernel to a classification problem on waveform data [@problem_id:3136231], you will see how encoding symmetries can dramatically improve a model's robustness and accuracy.", "problem": "Consider the task of designing a kernel method beyond Support Vector Machines (SVMs) that encodes invariance of the input representation under global sign flips. Let the input domain be vectors in $\\mathbb{R}^d$, and consider the action of the two-element group $\\{+1,-1\\}$ on $\\mathbb{R}^d$ via $x \\mapsto s x$ for $s \\in \\{+1,-1\\}$. The objective is to construct a positive definite kernel on $\\mathbb{R}^d$ that yields the same value when any input is replaced by its sign-flipped version, and then to use this kernel in a method other than Support Vector Machines to solve a supervised learning problem involving waveforms that have phase uncertainty.\n\nStart from the following fundamental bases:\n1. A function $k: \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}$ is a positive definite kernel if and only if for any finite set $\\{x_i\\}_{i=1}^n \\subset \\mathcal{X}$ and any real coefficients $\\{c_i\\}_{i=1}^n$, it holds that $\\sum_{i=1}^n \\sum_{j=1}^n c_i c_j k(x_i,x_j) \\geq 0$.\n2. A positive definite kernel $k$ defines a Reproducing Kernel Hilbert Space (RKHS) $\\mathcal{H}$ such that, for any $f \\in \\mathcal{H}$ and $x \\in \\mathcal{X}$, the reproducing property holds: $f(x) = \\langle f, k(\\cdot, x) \\rangle_{\\mathcal{H}}$.\n3. Kernel Ridge Regression (KRR) solves regularized empirical risk minimization in the RKHS via minimizing $\\sum_{i=1}^n (f(x_i) - y_i)^2 + \\lambda \\lVert f \\rVert_{\\mathcal{H}}^2$ for a regularization parameter $\\lambda  0$.\n\nYour tasks are as follows:\nA. Using the above bases, derive conditions under which a kernel obtained by composing a known positive definite kernel $\\kappa$ on some space $\\mathcal{Z}$ with a transformation $\\phi: \\mathcal{X} \\to \\mathcal{Z}$ yields a positive definite kernel $k(x,y) = \\kappa(\\phi(x), \\phi(y))$ on $\\mathcal{X}$. Use this to justify a design that encodes invariance under global sign flips.\nB. Implement two kernels on $\\mathbb{R}^d$: a baseline Gaussian radial basis function (RBF) kernel and a transformed kernel that is invariant under $x \\mapsto -x$. The baseline kernel must be $k_{\\text{rbf}}(x,y) = \\exp\\left(-\\frac{\\lVert x-y \\rVert_2^2}{2\\sigma^2}\\right)$ with a chosen bandwidth $\\sigma  0$. The invariant kernel must be constructed through a principled transformation that ensures invariance under the group action described above.\nC. Implement Kernel Ridge Regression (KRR) for binary classification with labels in $\\{-1,+1\\}$ using each kernel. Given training data $\\{(x_i, y_i)\\}_{i=1}^n$, the KRR predictor must be of the form $f(\\cdot) = \\sum_{i=1}^n \\alpha_i k(x_i, \\cdot)$ where the coefficients satisfy $(K + \\lambda I)\\alpha = y$ with $K_{ij} = k(x_i, x_j)$ and $\\lambda  0$.\nD. Generate synthetic waveform data for two classes using samples of $x(t) = \\sin(2\\pi f t + \\varphi)$ with unknown phase $\\varphi$ uniformly sampled from $[0, 2\\pi)$, and measured at $L$ equally spaced times in $[0,1)$. For each waveform, apply a random global sign flip with probability $p \\in [0,1]$ to model sensor orientation ambiguity, and add independent Gaussian noise with standard deviation $\\sigma_{\\text{noise}}  0$. Normalize each waveform to unit $\\ell_2$-norm. Use two distinct integer frequencies to define the classes and assign labels $+1$ and $-1$ respectively.\nE. Evaluate classification accuracy using the baseline kernel and the invariant kernel under different regimes of phase uncertainty, sign flips, and noise.\n\nImplement a single program that uses the following test suite of parameter values. For each case, use the specified values exactly and fix the random number generator seed to ensure reproducibility. No physical units are involved in this problem and all angles are in radians.\n\nTest Case $1$ (general case with moderate uncertainty):\n- Training samples per class: $60$.\n- Test samples per class: $200$.\n- Number of time samples per waveform: $128$.\n- Frequencies: $f_{+} = 3$ for label $+1$, $f_{-} = 7$ for label $-1$.\n- Phase distribution: uniform on $[0, 2\\pi)$.\n- Sign flip probability: $p = 0.5$.\n- Noise standard deviation: $\\sigma_{\\text{noise}} = 0.05$.\n- Kernel bandwidth: $\\sigma = 1.0$.\n- Regularization: $\\lambda = 10^{-3}$.\n- Random seed: $12345$.\n\nTest Case $2$ (boundary case with maximal sign uncertainty and no noise):\n- Training samples per class: $20$.\n- Test samples per class: $200$.\n- Number of time samples per waveform: $64$.\n- Frequencies: $f_{+} = 2$ for label $+1$, $f_{-} = 5$ for label $-1$.\n- Phase distribution: uniform on $[0, 2\\pi)$.\n- Sign flip probability: $p = 1.0$.\n- Noise standard deviation: $\\sigma_{\\text{noise}} = 0.0$.\n- Kernel bandwidth: $\\sigma = 1.0$.\n- Regularization: $\\lambda = 10^{-3}$.\n- Random seed: $54321$.\n\nTest Case $3$ (edge case property check for invariance):\n- Construct a single waveform with $L = 256$, frequency $f = 4$, phase $\\varphi = 0$, no noise, and no sign flip.\n- Compute the invariant kernel value between the waveform and its sign-flipped version, and compare it to the invariant kernel value between the waveform and itself.\n- Kernel bandwidth: $\\sigma = 1.0$.\n\nYour program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets in the following order:\n- For Test Case $1$: baseline classification accuracy (float), invariant-kernel classification accuracy (float).\n- For Test Case $2$: baseline classification accuracy (float), invariant-kernel classification accuracy (float).\n- For Test Case $3$: a boolean indicating whether the invariant kernel yields the same value for a waveform and its sign-flipped version as for the waveform and itself (using a numerical tolerance of $10^{-9}$).\n\nFor example, the final output format must be $[result_1,result_2,result_3,result_4,result_5]$, where $result_1$ through $result_4$ are floats and $result_5$ is a boolean.", "solution": "The problem statement is valid. It is scientifically grounded in the theory of statistical learning, well-posed with all necessary parameters and definitions, and its objectives are stated with mathematical precision. The tasks are logically structured, starting from theoretical derivation, proceeding to implementation, and concluding with empirical evaluation, forming a complete and verifiable exercise in kernel methods.\n\n### A. Derivation and Design of an Invariant Kernel\n\nThe first task is to establish a method for constructing new positive definite (PD) kernels and then apply it to design a kernel that is invariant to global sign flips.\n\nA function $k: \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}$ is a positive definite kernel if for any finite set of points $\\{x_i\\}_{i=1}^n \\subset \\mathcal{X}$ and any real coefficients $\\{c_i\\}_{i=1}^n$, the following condition holds:\n$$\n\\sum_{i=1}^n \\sum_{j=1}^n c_i c_j k(x_i, x_j) \\geq 0\n$$\nLet $\\kappa: \\mathcal{Z} \\times \\mathcal{Z} \\to \\mathbb{R}$ be a known PD kernel on a space $\\mathcal{Z}$, and let $\\phi: \\mathcal{X} \\to \\mathcal{Z}$ be an arbitrary function, or feature map. We can define a new kernel $k$ on $\\mathcal{X}$ by composition: $k(x, y) = \\kappa(\\phi(x), \\phi(y))$. To prove that $k$ is also a PD kernel on $\\mathcal{X}$, we take any finite set $\\{x_i\\}_{i=1}^n \\subset \\mathcal{X}$ and coefficients $\\{c_i\\}_{i=1}^n \\subset \\mathbb{R}$ and examine the quadratic form:\n$$\n\\sum_{i=1}^n \\sum_{j=1}^n c_i c_j k(x_i, x_j) = \\sum_{i=1}^n \\sum_{j=1}^n c_i c_j \\kappa(\\phi(x_i), \\phi(x_j))\n$$\nLet us define a set of points $\\{z_i\\}_{i=1}^n \\subset \\mathcal{Z}$ where $z_i = \\phi(x_i)$. The expression becomes:\n$$\n\\sum_{i=1}^n \\sum_{j=1}^n c_i c_j \\kappa(z_i, z_j)\n$$\nSince $\\kappa$ is a PD kernel on $\\mathcal{Z}$, this sum is guaranteed to be non-negative by definition. Thus, $k(x, y) = \\kappa(\\phi(x), \\phi(y))$ is a valid PD kernel on $\\mathcal{X}$ for any feature map $\\phi$.\n\nTo encode invariance under the group action $x \\mapsto s x$ for $s \\in G = \\{+1, -1\\}$, we seek a kernel $k(x, y)$ such that $k(sx, y) = k(x, y)$ and $k(x, sy) = k(x, y)$ for all $s \\in G$. Using the composition form, $k(x, y) = \\kappa(\\phi(x), \\phi(y))$, a straightforward way to achieve this is to design a feature map $\\phi$ which is itself invariant, meaning $\\phi(x) = \\phi(-x)$. For instance, if we choose $\\phi(x) = xx^T$, which maps a vector in $\\mathbb{R}^d$ to a matrix in $\\mathbb{R}^{d \\times d}$, we have $\\phi(-x) = (-x)(-x)^T = xx^T = \\phi(x)$. If we then use the linear kernel on the space of matrices, $\\kappa(A, B) = \\langle A, B \\rangle_F = \\text{tr}(A^TB)$, we obtain the invariant kernel $k(x, y) = \\text{tr}((xx^T)^T(yy^T)) = (x^Ty)^2$, which is the homogeneous polynomial kernel of degree $2$.\n\nA more general and powerful technique for constructing invariant kernels is by averaging a given base kernel over the group actions. Let $k_{\\text{base}}$ be any PD kernel. An invariant kernel $k_{\\text{inv}}$ can be constructed by symmetrization:\n$$\nk_{\\text{inv}}(x, y) = \\frac{1}{|G|^2} \\sum_{s_1 \\in G} \\sum_{s_2 \\in G} k_{\\text{base}}(s_1 x, s_2 y)\n$$\nFor our group $G = \\{+1, -1\\}$, this expands to:\n$$\nk_{\\text{inv}}(x, y) = \\frac{1}{4} [k_{\\text{base}}(x, y) + k_{\\text{base}}(x, -y) + k_{\\text{base}}(-x, y) + k_{\\text{base}}(-x, -y)]\n$$\nThis new kernel is a sum of PD kernels scaled by a positive constant ($1/4$), so it is also a PD kernel. The problem specifies using the Gaussian RBF kernel as a base: $k_{\\text{rbf}}(x, y) = \\exp\\left(-\\frac{\\lVert x-y \\rVert_2^2}{2\\sigma^2}\\right)$. This kernel has the property $k_{\\text{rbf}}(-u, -v) = \\exp\\left(-\\frac{\\lVert -u-(-v) \\rVert_2^2}{2\\sigma^2}\\right) = \\exp\\left(-\\frac{\\lVert -(u-v) \\rVert_2^2}{2\\sigma^2}\\right) = k_{\\text{rbf}}(u, v)$. Also, $k_{\\text{rbf}}(-x, y) = \\exp\\left(-\\frac{\\lVert -x-y \\rVert_2^2}{2\\sigma^2}\\right) = \\exp\\left(-\\frac{\\lVert x+y \\rVert_2^2}{2\\sigma^2}\\right) = k_{\\text{rbf}}(x, -y)$. Substituting these symmetries into the sum gives:\n$$\nk_{\\text{inv}}(x, y) = \\frac{1}{4} [k_{\\text{rbf}}(x, y) + k_{\\text{rbf}}(x, -y) + k_{\\text{rbf}}(x, -y) + k_{\\text{rbf}}(x, y)] = \\frac{1}{2} [k_{\\text{rbf}}(x, y) + k_{\\text{rbf}}(x, -y)]\n$$\nThis will be our implemented invariant kernel. It correctly averages over the possible sign of the second argument, and its symmetry ensures invariance with respect to the first argument as well.\n\n### B. Kernel Implementation\n\nWe implement two kernels on $\\mathbb{R}^d$:\n1.  **Baseline RBF Kernel**: $k_{\\text{rbf}}(x, y) = \\exp\\left(-\\frac{\\lVert x-y \\rVert_2^2}{2\\sigma^2}\\right)$. This is a standard choice, sensitive to the relative positions of $x$ and $y$.\n2.  **Invariant RBF Kernel**: $k_{\\text{inv}}(x, y) = \\frac{1}{2} [k_{\\text{rbf}}(x, y) + k_{\\text{rbf}}(x, -y)]$. This kernel effectively computes the similarity between the pair of points $\\{x, -x\\}$ and the pair $\\{y, -y\\}$, making it insensitive to global sign flips in its inputs.\n\n### C. Kernel Ridge Regression (KRR) Implementation\n\nKRR seeks a function $f$ from the RKHS $\\mathcal{H}$ associated with kernel $k$ that minimizes the regularized least-squares error: $\\min_{f \\in \\mathcal{H}} \\sum_{i=1}^n (y_i - f(x_i))^2 + \\lambda \\lVert f \\rVert_{\\mathcal{H}}^2$. The Representer Theorem guarantees that the solution has the form $f(\\cdot) = \\sum_{i=1}^n \\alpha_i k(x_i, \\cdot)$. Substituting this into the objective function and solving for the coefficients $\\alpha = (\\alpha_1, \\dots, \\alpha_n)^T$ leads to the linear system:\n$$\n(K + \\lambda I)\\alpha = y\n$$\nwhere $K$ is the $n \\times n$ Gram matrix with entries $K_{ij} = k(x_i, x_j)$, $y = (y_1, \\dots, y_n)^T$ is the vector of labels, and $I$ is the identity matrix. The training process involves computing $K$ and solving this system for $\\alpha$. For prediction on a new data point $x_{\\text{test}}$, we compute the function value $f(x_{\\text{test}}) = \\sum_{i=1}^n \\alpha_i k(x_i, x_{\\text{test}})$. For binary classification with labels $\\{-1, +1\\}$, the predicted class is $\\text{sign}(f(x_{\\text{test}}))$.\n\n### D. Synthetic Data Generation\n\nWe generate synthetic waveforms to simulate a scenario with phase and sign ambiguity. A waveform is a vector in $\\mathbb{R}^L$ obtained by sampling $x(t) = \\sin(2\\pi f t + \\varphi)$ at $L$ equally spaced time points $t \\in [0, 1)$. Two classes are defined by distinct frequencies, $f_+$ and $f_-$. The generation process for each sample is:\n1.  Select a frequency ($f_+$ or $f_-$) based on the class label.\n2.  Sample a phase $\\varphi$ from a uniform distribution $U[0, 2\\pi)$.\n3.  Generate the noiseless waveform $x_{\\text{clean}}(t) = \\sin(2\\pi f t + \\varphi)$.\n4.  Add independent and identically distributed Gaussian noise: $x_{\\text{noisy}} = x_{\\text{clean}} + \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0, \\sigma_{\\text{noise}}^2 I)$.\n5.  Apply a random sign flip: with probability $p$, replace $x_{\\text{noisy}}$ with $-x_{\\text{noisy}}$.\n6.  Normalize the final vector to have a unit $\\ell_2$-norm: $x = x_{\\text{final}}/\\lVert x_{\\text{final}} \\rVert_2$. This step is crucial as it ensures all inputs lie on the unit hypersphere, making distance-based kernels like RBF depend only on the angle between vectors.\n\n### E. Evaluation Procedure\n\nThe performance of KRR with the baseline and invariant kernels is evaluated on two test cases with different levels of uncertainty. For each case, we generate training and testing datasets. We train two KRR models on the training set, one for each kernel, by computing their respective $\\alpha$ vectors. We then predict labels for the test set and compute the classification accuracy, defined as the fraction of correctly predicted labels. For the third test case, we verify the invariance property of $k_{\\text{inv}}$ by directly comparing $k_{\\text{inv}}(x, x)$ and $k_{\\text{inv}}(x, -x)$ for a specific waveform $x$, confirming they are equal within a small numerical tolerance. This validates the theoretical construction. The expectation is that the invariant kernel will outperform the baseline, especially when the sign flip probability $p$ is high, as it is designed to be immune to this specific data perturbation.", "answer": "```python\nimport numpy as np\n\ndef k_rbf(x, y, sigma):\n    \"\"\"Computes the Gaussian RBF kernel between two vectors.\"\"\"\n    norm_sq = np.sum((x - y)**2)\n    return np.exp(-norm_sq / (2 * sigma**2))\n\ndef k_inv(x, y, sigma):\n    \"\"\"Computes the sign-invariant kernel based on the RBF kernel.\"\"\"\n    val1 = k_rbf(x, y, sigma)\n    val2 = k_rbf(x, -y, sigma)\n    return 0.5 * (val1 + val2)\n\ndef compute_gram_matrix(X, kernel_func, sigma):\n    \"\"\"Computes the Gram matrix for a dataset X and a given kernel.\"\"\"\n    n_samples = X.shape[0]\n    K = np.zeros((n_samples, n_samples))\n    for i in range(n_samples):\n        for j in range(i, n_samples):\n            val = kernel_func(X[i], X[j], sigma)\n            K[i, j] = val\n            K[j, i] = val\n    return K\n\ndef train_krr(K, y, lambda_reg):\n    \"\"\"Trains Kernel Ridge Regression by solving for alpha.\"\"\"\n    n_samples = K.shape[0]\n    I = np.eye(n_samples)\n    alpha = np.linalg.solve(K + lambda_reg * I, y)\n    return alpha\n\ndef predict_krr(X_test, X_train, alpha, kernel_func, sigma):\n    \"\"\"Makes predictions using a trained KRR model.\"\"\"\n    n_test = X_test.shape[0]\n    n_train = X_train.shape[0]\n    \n    K_cross = np.zeros((n_test, n_train))\n    for i in range(n_test):\n        for j in range(n_train):\n            K_cross[i, j] = kernel_func(X_test[i], X_train[j], sigma)\n            \n    f_values = K_cross @ alpha\n    y_pred = np.sign(f_values)\n    # Handle the case where f_value is 0; classify as +1 by convention.\n    y_pred[y_pred == 0] = 1\n    return y_pred\n\ndef generate_data(n_samples_class, L, f_pos, f_neg, p_flip, sigma_noise, rng):\n    \"\"\"Generates synthetic waveform data for two classes.\"\"\"\n    t = np.linspace(0, 1, L, endpoint=False)\n    X = []\n    y = []\n\n    for label, freq in [(1, f_pos), (-1, f_neg)]:\n        for _ in range(n_samples_class):\n            phase = rng.uniform(0, 2 * np.pi)\n            waveform = np.sin(2 * np.pi * freq * t + phase)\n            \n            # Add noise\n            noise = rng.normal(0, sigma_noise, L)\n            waveform += noise\n            \n            # Apply sign flip\n            if rng.random()  p_flip:\n                waveform *= -1\n            \n            # Normalize to unit l2-norm\n            norm = np.linalg.norm(waveform)\n            if norm > 1e-9:\n                waveform /= norm\n            \n            X.append(waveform)\n            y.append(label)\n            \n    return np.array(X), np.array(y)\n\ndef run_classification_test(params):\n    \"\"\"Runs a full classification test for a given set of parameters.\"\"\"\n    rng = np.random.default_rng(params['seed'])\n    \n    # Generate training data\n    X_train, y_train = generate_data(\n        params['n_train_class'], params['L'], params['f_pos'], params['f_neg'],\n        params['p_flip'], params['sigma_noise'], rng)\n    \n    # Generate testing data with a separate RNG sequence\n    X_test, y_test = generate_data(\n        params['n_test_class'], params['L'], params['f_pos'], params['f_neg'],\n        params['p_flip'], params['sigma_noise'], rng)\n\n    accuracies = []\n    for kernel_func in [k_rbf, k_inv]:\n        # Train\n        K_train = compute_gram_matrix(X_train, kernel_func, params['sigma'])\n        alpha = train_krr(K_train, y_train, params['lambda_reg'])\n        \n        # Predict and evaluate\n        y_pred = predict_krr(X_test, X_train, alpha, kernel_func, params['sigma'])\n        accuracy = np.mean(y_pred == y_test)\n        accuracies.append(accuracy)\n        \n    return accuracies\n\n\ndef solve():\n    \"\"\"Main function to run all test cases and print results.\"\"\"\n    \n    results = []\n\n    # Test Case 1\n    params1 = {\n        'n_train_class': 60, 'n_test_class': 200, 'L': 128,\n        'f_pos': 3, 'f_neg': 7, 'p_flip': 0.5, 'sigma_noise': 0.05,\n        'sigma': 1.0, 'lambda_reg': 1e-3, 'seed': 12345\n    }\n    acc_base_1, acc_inv_1 = run_classification_test(params1)\n    results.extend([acc_base_1, acc_inv_1])\n\n    # Test Case 2\n    params2 = {\n        'n_train_class': 20, 'n_test_class': 200, 'L': 64,\n        'f_pos': 2, 'f_neg': 5, 'p_flip': 1.0, 'sigma_noise': 0.0,\n        'sigma': 1.0, 'lambda_reg': 1e-3, 'seed': 54321\n    }\n    acc_base_2, acc_inv_2 = run_classification_test(params2)\n    results.extend([acc_base_2, acc_inv_2])\n\n    # Test Case 3\n    L3, f3, sigma3 = 256, 4, 1.0\n    t3 = np.linspace(0, 1, L3, endpoint=False)\n    waveform = np.sin(2 * np.pi * f3 * t3)\n    norm = np.linalg.norm(waveform)\n    if norm > 1e-9:\n        waveform /= norm\n\n    v1 = k_inv(waveform, -waveform, sigma3)\n    v2 = k_inv(waveform, waveform, sigma3)\n    \n    is_invariant = np.isclose(v1, v2, atol=1e-9)\n    results.append(is_invariant)\n\n    # Format and print the final output\n    # Convert boolean to lowercase 'true'/'false' as often expected, although\n    # default str() would be 'True'/'False'. Using default for compliance.\n    output_str = [f\"{r:.10f}\" if isinstance(r, float) else str(r) for r in results]\n    print(f\"[{','.join(output_str)}]\")\n\nsolve()\n\n```", "id": "3136231"}, {"introduction": "Kernel methods offer powerful tools that venture far beyond supervised classification and regression. This exercise introduces the Maximum Mean Discrepancy (MMD), a principled way to measure the distance between two probability distributions using kernel mean embeddings in a Reproducing Kernel Hilbert Space (RKHS). You will implement a gradient-based procedure to tune a simple generative model, using the MMD as a loss function to match the model's output distribution to a target dataset [@problem_id:3136216]. This provides a hands-on introduction to the role of kernels in modern generative modeling and two-sample testing.", "problem": "You are asked to derive and implement a principled gradient-based tuning procedure for a parametric generator using the Maximum Mean Discrepancy (MMD) objective in a Reproducing Kernel Hilbert Space (RKHS), focusing on kernel methods beyond support vector machines. Your generator is a simple location-scale simulator in one dimension, and the target is a sample from a univariate normal distribution. You must compute the gradient of the empirical squared MMD with respect to the generator parameters using only core definitions and fundamental calculus rules, and then use this gradient to tune the parameters by gradient descent.\n\nStart from the following fundamental base:\n- The squared Maximum Mean Discrepancy (MMD) between distributions $\\mathbb{P}$ and $\\mathbb{Q}$ with kernel $k$ is the squared RKHS norm of the difference between their kernel mean embeddings: \n$$\\operatorname{MMD}^2(\\mathbb{P},\\mathbb{Q}) \\equiv \\lVert \\mu_{\\mathbb{P}} - \\mu_{\\mathbb{Q}} \\rVert_{\\mathcal{H}}^2,$$\nwhere $\\mu_{\\mathbb{P}} \\equiv \\mathbb{E}_{X \\sim \\mathbb{P}}[k(\\cdot, X)]$ and $\\mu_{\\mathbb{Q}} \\equiv \\mathbb{E}_{Y \\sim \\mathbb{Q}}[k(\\cdot, Y)]$ in a Reproducing Kernel Hilbert Space (RKHS).\n- Using the polarization identity and bilinearity of the RKHS inner product, a widely used and well-tested expression is \n$$\\operatorname{MMD}^2(\\mathbb{P},\\mathbb{Q}) = \\mathbb{E}_{X,X' \\sim \\mathbb{P}}[k(X,X')] + \\mathbb{E}_{Y,Y' \\sim \\mathbb{Q}}[k(Y,Y')] - 2 \\mathbb{E}_{X \\sim \\mathbb{P}, Y \\sim \\mathbb{Q}}[k(X,Y)].$$\n- The empirical (biased) estimator of $\\operatorname{MMD}^2$ given samples $\\{x_i\\}_{i=1}^n$ and $\\{y_j\\}_{j=1}^m$ is \n$$\\widehat{\\operatorname{MMD}}^2 = \\frac{1}{n^2} \\sum_{i=1}^{n}\\sum_{i'=1}^{n} k(x_i,x_{i'}) + \\frac{1}{m^2} \\sum_{j=1}^{m}\\sum_{j'=1}^{m} k(y_j,y_{j'}) - \\frac{2}{nm} \\sum_{i=1}^{n}\\sum_{j=1}^{m} k(x_i,y_j).$$\n- The Gaussian (radial basis function) kernel with bandwidth $\\sigma$ is $k(u,v) = \\exp\\!\\left(-\\frac{\\lVert u - v\\rVert^2}{2\\sigma^2}\\right)$.\n- Use standard multivariable calculus (chain rule and product rule) to differentiate composite functions.\n\nGenerator model and task:\n- Consider a one-dimensional generator $g_{\\theta}(z) = \\theta_1 + \\theta_2 z$ with parameter vector $\\theta = (\\theta_1, \\theta_2)$, and base noise $z \\sim \\mathcal{N}(0,1)$ that is fixed across optimization steps for reproducibility.\n- Given a fixed target dataset $\\{x_i\\}_{i=1}^n$ and generator samples $\\{y_j\\}_{j=1}^m$ produced by $y_j = g_{\\theta}(z_j)$ from fixed noise $\\{z_j\\}_{j=1}^m$, derive $\\nabla_{\\theta} \\widehat{\\operatorname{MMD}}^2$ with the Gaussian kernel, starting from the definitions above without shortcut formulas.\n- Implement a program that:\n  - Computes the empirical squared MMD $\\widehat{\\operatorname{MMD}}^2$ for the Gaussian kernel.\n  - Computes the gradient $\\nabla_{\\theta} \\widehat{\\operatorname{MMD}}^2$ using your derivation.\n  - Runs gradient descent with a fixed learning rate for a specified number of steps to update $\\theta$.\n  - Returns the final tuned parameters and the final empirical squared MMD for each test case.\n\nTest suite:\n- Use the following test cases. For each case, independently generate the target data $\\{x_i\\}$ and the fixed generator noise $\\{z_j\\}$ using the provided random seed. Target data are drawn from a univariate normal distribution $\\mathcal{N}(\\mu_{\\mathrm{tgt}}, s_{\\mathrm{tgt}}^2)$, where $\\mu_{\\mathrm{tgt}}$ is the target mean and $s_{\\mathrm{tgt}}$ is the target standard deviation.\n  1. Case A (happy path): $\\mu_{\\mathrm{tgt}} = 1.0$, $s_{\\mathrm{tgt}} = 0.5$, $n = 200$, $m = 200$, initial $\\theta^{(0)} = (0.3, 0.8)$, kernel bandwidth $\\sigma = 1.0$, learning rate $\\alpha = 0.1$, steps $T = 200$, seed $= 123$.\n  2. Case B (identity target): $\\mu_{\\mathrm{tgt}} = 0.0$, $s_{\\mathrm{tgt}} = 1.0$, $n = 200$, $m = 200$, initial $\\theta^{(0)} = (0.2, 1.5)$, kernel bandwidth $\\sigma = 1.0$, learning rate $\\alpha = 0.1$, steps $T = 200$, seed $= 456$.\n  3. Case C (shifted and rescaled): $\\mu_{\\mathrm{tgt}} = -1.0$, $s_{\\mathrm{tgt}} = 0.7$, $n = 200$, $m = 200$, initial $\\theta^{(0)} = (0.0, 0.5)$, kernel bandwidth $\\sigma = 1.0$, learning rate $\\alpha = 0.1$, steps $T = 250$, seed $= 789$.\n  4. Case D (small-sample robustness): $\\mu_{\\mathrm{tgt}} = 0.5$, $s_{\\mathrm{tgt}} = 0.9$, $n = 20$, $m = 20$, initial $\\theta^{(0)} = (1.0, 0.2)$, kernel bandwidth $\\sigma = 1.0$, learning rate $\\alpha = 0.1$, steps $T = 150$, seed $= 31415$.\n\nFinal output format:\n- Your program should produce a single line of output containing a comma-separated list enclosed in square brackets, with no spaces. For each test case in the order A, B, C, D, output the final parameter $\\theta_1$, then $\\theta_2$, then the final empirical squared MMD value, all rounded to exactly $6$ decimal places. The final output should thus be a flat list of length $12$ of the form\n$[\\theta_{1}^{A},\\theta_{2}^{A},\\widehat{\\operatorname{MMD}}^{2}_{A},\\theta_{1}^{B},\\theta_{2}^{B},\\widehat{\\operatorname{MMD}}^{2}_{B},\\theta_{1}^{C},\\theta_{2}^{C},\\widehat{\\operatorname{MMD}}^{2}_{C},\\theta_{1}^{D},\\theta_{2}^{D},\\widehat{\\operatorname{MMD}}^{2}_{D}]$,\nwith numeric entries rounded to $6$ decimal places and no additional text.", "solution": "We begin from the definition of the Maximum Mean Discrepancy (MMD) in a Reproducing Kernel Hilbert Space (RKHS). For a positive definite kernel $k$ with RKHS $\\mathcal{H}$ and kernel mean embeddings $\\mu_{\\mathbb{P}} \\equiv \\mathbb{E}[k(\\cdot, X)]$ and $\\mu_{\\mathbb{Q}} \\equiv \\mathbb{E}[k(\\cdot, Y)]$, the squared MMD is \n$$\\operatorname{MMD}^2(\\mathbb{P},\\mathbb{Q}) \\equiv \\lVert \\mu_{\\mathbb{P}} - \\mu_{\\mathbb{Q}} \\rVert_{\\mathcal{H}}^2.$$\nUsing bilinearity of the RKHS inner product and the reproducing property yields the widely used and well-tested formula \n$$\\operatorname{MMD}^2(\\mathbb{P},\\mathbb{Q}) = \\mathbb{E}_{X,X'}[k(X,X')] + \\mathbb{E}_{Y,Y'}[k(Y,Y')] - 2 \\mathbb{E}_{X,Y}[k(X,Y)],$$\nwhere expectations are with respect to independent copies $X, X' \\sim \\mathbb{P}$ and $Y, Y' \\sim \\mathbb{Q}$. For empirical samples $\\{x_i\\}_{i=1}^{n}$ and $\\{y_j\\}_{j=1}^{m}$, the biased estimator is \n$$\\widehat{\\operatorname{MMD}}^2 = \\frac{1}{n^2} \\sum_{i=1}^{n}\\sum_{i'=1}^{n} k(x_i,x_{i'}) + \\frac{1}{m^2} \\sum_{j=1}^{m}\\sum_{j'=1}^{m} k(y_j,y_{j'}) - \\frac{2}{nm} \\sum_{i=1}^{n}\\sum_{j=1}^{m} k(x_i,y_j).$$\n\nWe consider a one-dimensional generator $g_{\\theta}(z) = \\theta_1 + \\theta_2 z$ with base noise $z \\sim \\mathcal{N}(0,1)$. Let $y_j(\\theta) = \\theta_1 + \\theta_2 z_j$ for fixed noise samples $\\{z_j\\}_{j=1}^{m}$. We use the Gaussian kernel \n$$k(u,v) = \\exp\\!\\left(-\\frac{(u-v)^2}{2\\sigma^2}\\right),$$ \nwith bandwidth $\\sigma  0$. The empirical squared MMD is a differentiable function of $\\theta$ via the dependence on $\\{y_j(\\theta)\\}_{j=1}^{m}$.\n\nWe now derive $\\nabla_{\\theta} \\widehat{\\operatorname{MMD}}^2$ from first principles. Let $L(\\theta) \\equiv \\widehat{\\operatorname{MMD}}^2(\\{x_i\\}, \\{y_j(\\theta)\\})$. By the chain rule, \n$$\\nabla_{\\theta} L(\\theta) = \\sum_{j=1}^{m} \\frac{\\partial L}{\\partial y_j} \\frac{\\partial y_j}{\\partial \\theta},$$ \nwhere $\\frac{\\partial y_j}{\\partial \\theta} = \\begin{bmatrix} \\frac{\\partial y_j}{\\partial \\theta_1} \\\\ \\frac{\\partial y_j}{\\partial \\theta_2} \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ z_j \\end{bmatrix}$.\n\nThus it suffices to compute $\\frac{\\partial L}{\\partial y_j}$. Using the empirical expression,\n$$L(\\theta) = \\underbrace{\\frac{1}{n^2} \\sum_{i=1}^{n}\\sum_{i'=1}^{n} k(x_i,x_{i'})}_{\\text{constant in } \\theta} + \\frac{1}{m^2} \\sum_{a=1}^{m}\\sum_{b=1}^{m} k(y_a,y_b) - \\frac{2}{nm} \\sum_{i=1}^{n}\\sum_{j=1}^{m} k(x_i,y_j).$$\nThe first term does not depend on $\\theta$. For the second term, the derivative with respect to $y_j$ collects contributions when $y_j$ appears as the first or the second argument:\n$$\\frac{\\partial}{\\partial y_j} \\left( \\frac{1}{m^2} \\sum_{a=1}^{m}\\sum_{b=1}^{m} k(y_a,y_b) \\right) = \\frac{1}{m^2} \\left( \\sum_{b=1}^{m} \\frac{\\partial}{\\partial y_j} k(y_j, y_b) + \\sum_{a=1}^{m} \\frac{\\partial}{\\partial y_j} k(y_a, y_j) \\right).$$\nFor the Gaussian kernel, the gradient with respect to the second argument equals \n$$\\frac{\\partial}{\\partial v} k(u,v) = k(u,v)\\cdot \\frac{u - v}{\\sigma^2}.$$\nHence, \n$$\\frac{\\partial}{\\partial y_j} k(y_j, y_b) = k(y_j, y_b) \\cdot \\frac{y_b - y_j}{\\sigma^2}, \\quad \\frac{\\partial}{\\partial y_j} k(y_a, y_j) = k(y_a, y_j) \\cdot \\frac{y_a - y_j}{\\sigma^2}.$$\nBy symmetry $k(y_j,y_b) = k(y_b,y_j)$, the two sums are of the same form, and together:\n$$\\frac{\\partial}{\\partial y_j} \\left( \\frac{1}{m^2} \\sum_{a,b} k(y_a,y_b) \\right) = \\frac{2}{m^2 \\sigma^2} \\sum_{b=1}^{m} k(y_j, y_b)\\, (y_b - y_j).$$\nFor the cross term,\n$$\\frac{\\partial}{\\partial y_j} \\left( - \\frac{2}{nm} \\sum_{i=1}^{n}\\sum_{j=1}^{m} k(x_i,y_j) \\right) = - \\frac{2}{nm} \\sum_{i=1}^{n} \\frac{\\partial}{\\partial y_j} k(x_i, y_j) = - \\frac{2}{nm \\sigma^2} \\sum_{i=1}^{n} k(x_i, y_j)\\, (x_i - y_j).$$\nCombining these gives the scalar derivative with respect to each generator sample:\n$$\\frac{\\partial L}{\\partial y_j} = \\frac{2}{m^2 \\sigma^2} \\sum_{b=1}^{m} k(y_j, y_b)\\, (y_b - y_j) \\;-\\; \\frac{2}{nm \\sigma^2} \\sum_{i=1}^{n} k(x_i, y_j)\\, (x_i - y_j).$$\n\nUsing the chain rule with $y_j(\\theta) = \\theta_1 + \\theta_2 z_j$ gives\n$$\\frac{\\partial L}{\\partial \\theta_1} = \\sum_{j=1}^{m} \\frac{\\partial L}{\\partial y_j} \\cdot 1, \\quad \\frac{\\partial L}{\\partial \\theta_2} = \\sum_{j=1}^{m} \\frac{\\partial L}{\\partial y_j} \\cdot z_j.$$\n\nAlgorithmic implementation:\n- Compute the kernel matrices $K_{YY}$ with entries $k(y_j, y_b)$ and $K_{XY}$ with entries $k(x_i, y_j)$ using pairwise differences and the Gaussian kernel.\n- Vectorize the sums. Let $y \\in \\mathbb{R}^{m}$ and $x \\in \\mathbb{R}^{n}$. Define the matrices of pairwise differences $\\Delta_{YY}$ with entries $(y_b - y_j)$ and $\\Delta_{XY}$ with entries $(x_i - y_j)$. Then\n$$\\left[ \\frac{\\partial L}{\\partial y_j} \\right]_{j=1}^{m} = \\frac{2}{m^2 \\sigma^2} \\left( K_{YY} \\odot \\Delta_{YY} \\right) \\mathbf{1}_m \\;-\\; \\frac{2}{nm \\sigma^2} \\left( K_{XY} \\odot \\Delta_{XY} \\right)^{\\top} \\mathbf{1}_n,$$\nwhere $\\odot$ denotes elementwise multiplication and $\\mathbf{1}_k$ is the vector of ones of length $k$; the sums are taken over the appropriate axes by matrix-vector operations.\n- Accumulate $\\frac{\\partial L}{\\partial \\theta_1}$ and $\\frac{\\partial L}{\\partial \\theta_2}$ by summation with weights $1$ and $z_j$ respectively.\n- Perform gradient descent updates $\\theta \\leftarrow \\theta - \\alpha \\nabla_{\\theta} L$ for the specified number of steps with learning rate $\\alpha$.\n\nWhy this is correct:\n- The starting point is the RKHS definition of MMD and its well-tested expectation expansion; the empirical estimator is a consistent plug-in estimate.\n- Differentiating the empirical objective uses only the chain rule and the derivative of the Gaussian kernel, which is smooth everywhere.\n- The generator is a differentiable map from parameters to samples, so the chain rule applies directly; the Jacobian $\\frac{\\partial y_j}{\\partial \\theta}$ is linear in the fixed noise $z_j$.\n- Using fixed $\\{x_i\\}$ and $\\{z_j\\}$ across iterations ensures a deterministic and differentiable objective suitable for gradient descent.\n\nNumerical specifications and output:\n- Implement the gradient descent exactly as specified for the four cases:\n  1. Case A: $\\mu_{\\mathrm{tgt}} = 1.0$, $s_{\\mathrm{tgt}} = 0.5$, $n = 200$, $m = 200$, $\\theta^{(0)} = (0.3, 0.8)$, $\\sigma = 1.0$, $\\alpha = 0.1$, $T = 200$, seed $= 123$.\n  2. Case B: $\\mu_{\\mathrm{tgt}} = 0.0$, $s_{\\mathrm{tgt}} = 1.0$, $n = 200$, $m = 200$, $\\theta^{(0)} = (0.2, 1.5)$, $\\sigma = 1.0$, $\\alpha = 0.1$, $T = 200$, seed $= 456$.\n  3. Case C: $\\mu_{\\mathrm{tgt}} = -1.0$, $s_{\\mathrm{tgt}} = 0.7$, $n = 200$, $m = 200$, $\\theta^{(0)} = (0.0, 0.5)$, $\\sigma = 1.0$, $\\alpha = 0.1$, $T = 250$, seed $= 789$.\n  4. Case D: $\\mu_{\\mathrm{tgt}} = 0.5$, $s_{\\mathrm{tgt}} = 0.9$, $n = 20$, $m = 20$, $\\theta^{(0)} = (1.0, 0.2)$, $\\sigma = 1.0$, $\\alpha = 0.1$, $T = 150$, seed $= 31415$.\n- After optimization, report for each case the final $\\theta_1$, $\\theta_2$, and the final empirical squared MMD value, rounded to $6$ decimal places, concatenated into a single flat list with no spaces and enclosed in square brackets, in the order A, B, C, D.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef rbf_kernel_matrix(a: np.ndarray, b: np.ndarray, sigma: float) -> np.ndarray:\n    # Compute pairwise squared distances and Gaussian kernel.\n    # a shape: (n,), b shape: (m,)\n    diff = a[:, None] - b[None, :]\n    K = np.exp(-0.5 * (diff ** 2) / (sigma ** 2))\n    return K\n\ndef mmd2_biased(x: np.ndarray, y: np.ndarray, sigma: float) -> float:\n    K_xx = rbf_kernel_matrix(x, x, sigma)\n    K_yy = rbf_kernel_matrix(y, y, sigma)\n    K_xy = rbf_kernel_matrix(x, y, sigma)\n    term_xx = K_xx.mean()\n    term_yy = K_yy.mean()\n    term_xy = K_xy.mean()\n    return float(term_xx + term_yy - 2.0 * term_xy)\n\ndef grad_y_mmd2_biased(x: np.ndarray, y: np.ndarray, sigma: float) -> np.ndarray:\n    # Computes gradient of biased empirical MMD^2 w.r.t. each y_j (1D).\n    n = x.shape[0]\n    m = y.shape[0]\n    K_yy = rbf_kernel_matrix(y, y, sigma)           # shape (m, m)\n    K_xy = rbf_kernel_matrix(x, y, sigma)           # shape (n, m)\n    # Differences\n    diff_yy = y[None, :] - y[:, None]               # shape (m, m): (y_b - y_j)\n    diff_xy = x[:, None] - y[None, :]               # shape (n, m): (x_i - y_j)\n    # Weighted sums\n    part1 = (K_yy * diff_yy).sum(axis=1)            # shape (m,)\n    part2 = (K_xy * diff_xy).sum(axis=0)            # shape (m,)\n    factor1 = 2.0 / (m * m * sigma * sigma)\n    factor2 = 2.0 / (n * m * sigma * sigma)\n    grad_y = factor1 * part1 - factor2 * part2      # shape (m,)\n    return grad_y\n\ndef optimize_theta(x: np.ndarray, z: np.ndarray, theta0: tuple, sigma_k: float, lr: float, steps: int) -> tuple:\n    a, b = float(theta0[0]), float(theta0[1])\n    for _ in range(steps):\n        y = a + b * z\n        gy = grad_y_mmd2_biased(x, y, sigma_k)\n        ga = gy.sum()\n        gb = (gy * z).sum()\n        a = a - lr * ga\n        b = b - lr * gb\n    # Final stats\n    y = a + b * z\n    mmd2 = mmd2_biased(x, y, sigma_k)\n    return a, b, mmd2\n\ndef run_case(mu_tgt: float, s_tgt: float, n: int, m: int, theta0: tuple, sigma_k: float, lr: float, steps: int, seed: int):\n    rng = np.random.default_rng(seed)\n    x = rng.normal(loc=mu_tgt, scale=s_tgt, size=n).astype(float)\n    z = rng.normal(loc=0.0, scale=1.0, size=m).astype(float)\n    a, b, mmd2 = optimize_theta(x, z, theta0, sigma_k, lr, steps)\n    return a, b, mmd2\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A\n        {\"mu\": 1.0, \"s\": 0.5, \"n\": 200, \"m\": 200, \"theta0\": (0.3, 0.8), \"sigma_k\": 1.0, \"lr\": 0.1, \"steps\": 200, \"seed\": 123},\n        # Case B\n        {\"mu\": 0.0, \"s\": 1.0, \"n\": 200, \"m\": 200, \"theta0\": (0.2, 1.5), \"sigma_k\": 1.0, \"lr\": 0.1, \"steps\": 200, \"seed\": 456},\n        # Case C\n        {\"mu\": -1.0, \"s\": 0.7, \"n\": 200, \"m\": 200, \"theta0\": (0.0, 0.5), \"sigma_k\": 1.0, \"lr\": 0.1, \"steps\": 250, \"seed\": 789},\n        # Case D\n        {\"mu\": 0.5, \"s\": 0.9, \"n\": 20, \"m\": 20, \"theta0\": (1.0, 0.2), \"sigma_k\": 1.0, \"lr\": 0.1, \"steps\": 150, \"seed\": 31415},\n    ]\n\n    results = []\n    for case in test_cases:\n        a, b, mmd2 = run_case(\n            mu_tgt=case[\"mu\"],\n            s_tgt=case[\"s\"],\n            n=case[\"n\"],\n            m=case[\"m\"],\n            theta0=case[\"theta0\"],\n            sigma_k=case[\"sigma_k\"],\n            lr=case[\"lr\"],\n            steps=case[\"steps\"],\n            seed=case[\"seed\"]\n        )\n        results.extend([a, b, mmd2])\n\n    # Format results to 6 decimal places, no spaces inside the list.\n    formatted = \"[\" + \",\".join(f\"{v:.6f}\" for v in results) + \"]\"\n    print(formatted)\n\nsolve()\n```", "id": "3136216"}]}