{"hands_on_practices": [{"introduction": "The relationship between model complexity, dataset size, and performance is governed by the bias-variance trade-off. This exercise provides a quantitative look at this fundamental principle by modeling learning curves as a function of bias, variance, and sample size $n$. By calculating the precise crossover point where a complex, low-bias model becomes more effective than a simpler, high-bias one, you will gain a concrete understanding of why the \"best\" model often depends on how much data you have. [@problem_id:3138225]", "problem": "Consider a supervised regression task with inputs $x \\in \\mathcal{X}$ and outputs $y \\in \\mathbb{R}$ generated by $y = f(x) + \\varepsilon$, where $\\varepsilon$ is zero-mean noise with variance $\\sigma^2$. The model performance is measured by expected validation mean squared error (MSE) as a function of the training sample size $n$, denoted by $L_{val}(n)$. For a fixed model class trained on $n$ samples, a widely accepted decomposition is\n$$\n\\mathbb{E}[(y - \\hat{f}(x))^2] \\;=\\; \\sigma^2 \\;+\\; \\text{Bias}[\\hat{f}(x)]^2 \\;+\\; \\text{Var}[\\hat{f}(x)],\n$$\nwhere the expectation is over draws of the training set and the validation example. Empirically and theoretically for many estimators, the variance term scales approximately as $v/n$ for some model-dependent constant $v$, while the squared bias is approximately constant in $n$ for a fixed model class.\n\nYou are comparing two models via their learning curves $L_{val}(n)$:\n- A low-capacity model with squared bias $b_L^2 = 0.49$ and variance coefficient $v_L = 3$.\n- A high-capacity model with squared bias $b_H^2 = 0.01$ and variance coefficient $v_H = 33$.\nAssume irreducible noise variance $\\sigma^2 = 1$ and the above scaling holds for both models.\n\nDefine the sample size $n^\\star$ as the smallest integer $n$ such that the high-capacity model has strictly lower expected validation loss than the low-capacity model for all $n \\ge n^\\star$. Which choice identifies $n^\\star$ and correctly explains the crossing of the learning curves in terms of the bias-variance trade-off?\n\nA. $n^\\star = 45$, because the crossing occurs when the difference in variance terms equals the sum of raw biases, and beyond that the high-capacity model’s variance becomes negligible.\n\nB. $n^\\star = 63$, because the crossing occurs when the reduction in squared bias offsets the larger variance term, i.e., when $\\frac{v_H - v_L}{n} = b_L^2 - b_H^2$, and for $n \\ge 63$ the high-capacity model’s lower squared bias dominates despite its higher variance at small $n$.\n\nC. $n^\\star = 30$, because the difference in variance coefficients alone determines the threshold, independent of bias.\n\nD. There is no crossing, because the irreducible noise $\\sigma^2$ prevents the high-capacity model from ever outperforming the low-capacity model.\n\nE. $n^\\star = 60$, because setting $\\frac{v_H - v_L}{n} = b_L^2 + b_H^2$ gives the threshold where the high-capacity model starts to win.", "solution": "The user wants me to solve the problem by first doing a validation of the problem statement, and then provide a detailed solution.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\nThe problem statement provides the following information:\n- A supervised regression task with inputs $x \\in \\mathcal{X}$ and outputs $y \\in \\mathbb{R}$.\n- The data generating process is $y = f(x) + \\varepsilon$.\n- $\\varepsilon$ is zero-mean noise with variance $\\sigma^2$.\n- The performance measure is the expected validation mean squared error, $L_{val}(n)$.\n- The bias-variance decomposition is given as $\\mathbb{E}[(y - \\hat{f}(x))^2] = \\sigma^2 + \\text{Bias}[\\hat{f}(x)]^2 + \\text{Var}[\\hat{f}(x)]$.\n- The variance term scales as $\\text{Var}[\\hat{f}(x)] \\approx v/n$ for a constant $v$.\n- The squared bias term, $\\text{Bias}[\\hat{f}(x)]^2$, is approximately constant in $n$ for a fixed model class.\n- The irreducible noise variance is $\\sigma^2 = 1$.\n- Low-capacity model parameters: squared bias $b_L^2 = 0.49$ and variance coefficient $v_L = 3$.\n- High-capacity model parameters: squared bias $b_H^2 = 0.01$ and variance coefficient $v_H = 33$.\n- $n^\\star$ is defined as the smallest integer $n$ such that the high-capacity model has strictly lower expected validation loss than the low-capacity model for all $n \\ge n^\\star$.\n\n**Step 2: Validate Using Extracted Givens**\n\nThe problem statement is evaluated against the following criteria:\n- **Scientifically Grounded:** The problem is based on the fundamental bias-variance trade-off in statistical learning theory, a cornerstone of machine learning diagnostics. The decomposition of mean squared error into bias, variance, and irreducible error is standard. The scaling assumptions for bias (constant for a fixed, biased model) and variance ($\\propto 1/n$) are widely used and theoretically justified for many common estimators. All aspects are consistent with established principles.\n- **Well-Posed:** The problem provides all necessary numerical values and functional forms to construct the loss functions for both models. The objective is to find a specific integer $n^\\star$ based on a clear inequality, which is a well-defined mathematical task leading to a unique solution.\n- **Objective:** The problem is stated in precise, quantitative terms. The parameters are given as fixed values, and the definitions are formal. There is no subjective or ambiguous language.\n- **Completeness and Consistency:** The problem is self-contained. All variables required to calculate the learning curves ($b_L^2, v_L, b_H^2, v_H, \\sigma^2$) are provided. There are no contradictions in the setup.\n- **Realism:** The numerical values are chosen to clearly illustrate the trade-off. While specific to this problem, they are not scientifically implausible. The scenario where a higher-capacity model has lower bias but higher variance is a classic one.\n\n**Step 3: Verdict and Action**\n\nThe problem statement is **valid**. It is scientifically sound, well-posed, objective, and complete. I will now proceed with the solution.\n\n### Solution Derivation\n\nThe expected validation loss, $L_{val}(n)$, for a model trained on a sample of size $n$ is given by the sum of irreducible error, squared bias, and variance.\n\nFor the low-capacity model (L), the expected loss is:\n$$\nL_{val, L}(n) = \\sigma^2 + b_L^2 + \\frac{v_L}{n}\n$$\nSubstituting the given values:\n$$\nL_{val, L}(n) = 1 + 0.49 + \\frac{3}{n} = 1.49 + \\frac{3}{n}\n$$\n\nFor the high-capacity model (H), the expected loss is:\n$$\nL_{val, H}(n) = \\sigma^2 + b_H^2 + \\frac{v_H}{n}\n$$\nSubstituting the given values:\n$$\nL_{val, H}(n) = 1 + 0.01 + \\frac{33}{n} = 1.01 + \\frac{33}{n}\n$$\n\nThe problem defines $n^\\star$ as the smallest integer $n$ such that the high-capacity model has strictly lower expected validation loss for all training set sizes greater than or equal to $n^\\star$. This corresponds to finding the smallest integer $n$ that satisfies $L_{val, H}(n) < L_{val, L}(n)$ for itself and all subsequent integers.\n\nLet's set up the inequality:\n$$\nL_{val, H}(n) < L_{val, L}(n)\n$$\n$$\n1.01 + \\frac{33}{n} < 1.49 + \\frac{3}{n}\n$$\nThe irreducible error term $\\sigma^2=1$ is present on both sides and thus does not affect the comparison. We can subtract it, which is equivalent to comparing the reducible error terms (squared bias + variance):\n$$\n0.01 + \\frac{33}{n} < 0.49 + \\frac{3}{n}\n$$\nTo solve for $n$, we isolate the terms involving $n$. Subtract $\\frac{3}{n}$ from both sides and $0.01$ from both sides:\n$$\n\\frac{33}{n} - \\frac{3}{n} < 0.49 - 0.01\n$$\n$$\n\\frac{30}{n} < 0.48\n$$\nSince $n$ is a sample size, $n > 0$. We can multiply both sides by $n$ without changing the inequality's direction:\n$$\n30 < 0.48 \\cdot n\n$$\nNow, solve for $n$:\n$$\nn > \\frac{30}{0.48}\n$$\nTo evaluate the fraction, we can write $0.48$ as $48/100$:\n$$\nn > \\frac{30}{48/100} = \\frac{30 \\times 100}{48} = \\frac{3000}{48}\n$$\nSimplifying the fraction:\n$$\n\\frac{3000}{48} = \\frac{1500}{24} = \\frac{750}{12} = \\frac{375}{6} = \\frac{125}{2} = 62.5\n$$\nSo, the inequality holds for $n > 62.5$.\nThe problem asks for $n^\\star$, the smallest *integer* $n$ for which the high-capacity model is strictly better for all $n \\ge n^\\star$. Since the condition $n > 62.5$ must be met, the smallest integer value for $n$ is $63$. For any $n \\ge 63$, the inequality will continue to hold, as the term $\\frac{30}{n}$ will continue to decrease, making the left side of $\\frac{30}{n} < 0.48$ even smaller.\nTherefore, $n^\\star = 63$.\n\nThe crossing of the learning curves occurs at $n = 62.5$. This is the point where the total reducible errors are equal: $b_H^2 + \\frac{v_H}{n} = b_L^2 + \\frac{v_L}{n}$. Rearranging this gives $b_L^2 - b_H^2 = \\frac{v_H - v_L}{n}$. This equation signifies that the crossing happens when the advantage of the high-capacity model in squared bias ($b_L^2 - b_H^2$) is exactly balanced by its disadvantage in variance ($\\frac{v_H - v_L}{n}$). For $n > 62.5$, the variance disadvantage shrinks, and the bias advantage dominates, making the high-capacity model superior.\n\n### Option-by-Option Analysis\n\n**A. $n^\\star = 45$, because the crossing occurs when the difference in variance terms equals the sum of raw biases, and beyond that the high-capacity model’s variance becomes negligible.**\n- The value $n^\\star = 45$ is incorrect. Our calculation yielded $n^\\star = 63$.\n- The reasoning is flawed. The crossing is determined by squared biases, not raw biases, and by the *difference* in squared biases, not the sum. The variance of the high-capacity model does not become negligible; rather, its *excess* variance relative to the low-capacity model becomes smaller than the bias improvement.\n- **Verdict:** Incorrect.\n\n**B. $n^\\star = 63$, because the crossing occurs when the reduction in squared bias offsets the larger variance term, i.e., when $\\frac{v_H - v_L}{n} = b_L^2 - b_H^2$, and for $n \\ge 63$ the high-capacity model’s lower squared bias dominates despite its higher variance at small $n$.**\n- The value $n^\\star = 63$ is correct.\n- The reasoning is sound. The crossing point is correctly identified as the point where the \"reduction in squared bias\" ($b_L^2 - b_H^2$) is equal to the \"larger variance term\" (more accurately, the difference in variance terms, $\\frac{v_H - v_L}{n}$). The provided equation $\\frac{v_H - v_L}{n} = b_L^2 - b_H^2$ correctly describes this balancing point, which we calculated to be at $n = 62.5$. The conclusion that for $n \\ge 63$ the high-capacity model's lower squared bias dominates is the correct interpretation of the result.\n- **Verdict:** Correct.\n\n**C. $n^\\star = 30$, because the difference in variance coefficients alone determines the threshold, independent of bias.**\n- The value $n^\\star = 30$ is incorrect.\n- The reasoning is fundamentally incorrect. The problem is a classic illustration of the bias-*variance* trade-off. The threshold for which model is better depends explicitly on both bias and variance terms. The claim that it is \"independent of bias\" is false. The number $30$ is simply the value of $v_H - v_L$.\n- **Verdict:** Incorrect.\n\n**D. There is no crossing, because the irreducible noise $\\sigma^2$ prevents the high-capacity model from ever outperforming the low-capacity model.**\n- The statement \"There is no crossing\" is incorrect. We calculated a crossing point at $n = 62.5$.\n- The reasoning is incorrect. The irreducible noise $\\sigma^2$ is an additive term common to both models' loss functions. When comparing the two models, this term cancels out and has no impact on which model is better or where their learning curves cross.\n- **Verdict:** Incorrect.\n\n**E. $n^\\star = 60$, because setting $\\frac{v_H - v_L}{n} = b_L^2 + b_H^2$ gives the threshold where the high-capacity model starts to win.**\n- The value $n^\\star = 60$ is incorrect.\n- The reasoning is based on an incorrect formula. It uses the *sum* of squared biases ($b_L^2 + b_H^2 = 0.49 + 0.01 = 0.50$) instead of the *difference* ($b_L^2 - b_H^2 = 0.48$). The correct comparison involves the gain in bias versus the loss in variance. Using the incorrect formula gives $n = \\frac{30}{0.50} = 60$, which explains the origin of the incorrect value.\n- **Verdict:** Incorrect.", "answer": "$$\\boxed{B}$$", "id": "3138225"}, {"introduction": "While manually inspecting a single learning curve is useful, real-world model tuning often involves comparing many models at once. This practice moves you from passive interpretation to active implementation by asking you to build a simple but powerful diagnostic algorithm. You will write a program that analyzes loss curves across different model capacities—controlled here by network pruning—to automatically detect regimes of underfitting and overfitting based on predefined rules. [@problem_id:3135754]", "problem": "You are given a discrete family of models indexed by pruning sparsity $s \\in [0,1]$, where $s$ is the decimal fraction of parameters pruned. The dense model has $s=0$, and higher $s$ means lower model capacity. Let $L_{\\text{train}}(s)$ denote the empirical risk (training loss) at sparsity $s$, and $L_{\\text{val}}(s)$ denote the held-out risk (validation loss) at sparsity $s$. Assume a fixed dataset and training setup across $s$. The fundamental base is as follows:\n- Empirical Risk Minimization (ERM): minimizing $L_{\\text{train}}(s)$ over parameters fits the training distribution.\n- The expected risk on unseen data is estimated by $L_{\\text{val}}(s)$.\n- The generalization gap is $G(s) = L_{\\text{val}}(s) - L_{\\text{train}}(s)$.\n- Increasing $s$ reduces capacity; very high $s$ can cause underfitting (high bias), while very low $s$ can cause overfitting (high variance).\n\nDesign and implement a curve-based diagnosis that, from the sequences $\\{s_i\\}_{i=1}^n$, $\\{L_{\\text{train}}(s_i)\\}_{i=1}^n$, and $\\{L_{\\text{val}}(s_i)\\}_{i=1}^n$, decides:\n- Whether there is an underfitting regime near the high-sparsity end (large $s$).\n- Whether there is an overfitting regime near the low-sparsity end (small $s$).\n- Which index $i^\\star$ (zero-based) achieves the minimum validation loss.\n\nYour diagnosis must be based only on the following principles derived from the base:\n- Underfitting regime (high $s$): both $L_{\\text{train}}$ and $L_{\\text{val}}$ should be large relative to their ranges and should increase as $s$ increases further (losses get worse as capacity decreases).\n- Overfitting regime (low $s$): $L_{\\text{train}}$ should be small (the model fits training very well), and the generalization gap $G(s)$ should be appreciably large (validation is worse than training), indicating high variance.\n\nAlgorithmic requirements:\n1. Normalize losses to compare scales. For any sequence $X(s_i)$, define the normalized sequence $\\tilde{X}(s_i) = \\dfrac{X(s_i) - \\min_i X(s_i)}{\\max_i X(s_i) - \\min_i X(s_i) + \\varepsilon}$ for a small $\\varepsilon > 0$ to avoid division by zero. Use this for $L_{\\text{train}}$ and $L_{\\text{val}}$.\n2. Use raw generalization gap $G(s) = L_{\\text{val}}(s) - L_{\\text{train}}(s)$ without normalization to detect overfitting, as the absolute gap magnitude is meaningful across the test cases provided.\n3. Compute discrete slopes on the normalized curves with forward differences: for any consecutive pair $(s_i, s_{i+1})$, the slope is $\\dfrac{\\tilde{X}(s_{i+1}) - \\tilde{X}(s_i)}{s_{i+1} - s_i}$.\n4. Partition the index set into thirds by position. Let the “low-sparsity region” be the first $\\lceil n/3 \\rceil$ indices, and the “high-sparsity region” be the last $\\lceil n/3 \\rceil$ indices.\n5. Declare “underfitting detected” if, in the high-sparsity region:\n   - The average of $\\tilde{L}_{\\text{train}}$ exceeds the high-loss threshold $\\tau_{\\text{high}}$, and the average of $\\tilde{L}_{\\text{val}}$ exceeds $\\tau_{\\text{high}}$, and\n   - The average discrete slope of $\\tilde{L}_{\\text{train}}$ across that region exceeds $\\tau_{\\text{slope}}$, and the average discrete slope of $\\tilde{L}_{\\text{val}}$ across that region exceeds $\\tau_{\\text{slope}}$.\n6. Declare “overfitting detected” if, in the low-sparsity region:\n   - The average of $\\tilde{L}_{\\text{train}}$ is below the low-train threshold $\\tau_{\\text{train-low}}$, and\n   - The average of the raw gap $G(s)$ in that region exceeds the absolute gap threshold $\\tau_{\\text{gap-abs}}$.\n7. Let $i^\\star$ be the zero-based index of $\\min_i L_{\\text{val}}(s_i)$.\n\nUse the following fixed thresholds for all test cases:\n- $\\tau_{\\text{high}} = 0.5$,\n- $\\tau_{\\text{slope}} = 0.05$,\n- $\\tau_{\\text{train-low}} = 0.25$,\n- $\\tau_{\\text{gap-abs}} = 0.05$,\n- $\\varepsilon = 10^{-12}$.\n\nTest suite:\nProvide your program with the following three test cases. Each case is a tuple of arrays $(\\mathbf{s}, \\mathbf{L}_{\\text{train}}, \\mathbf{L}_{\\text{val}})$.\n\n- Case A (balanced trade-off: both underfitting at high $s$ and overfitting at low $s$ are present):\n  - $\\mathbf{s} = [0.0,\\,0.1,\\,0.2,\\,0.4,\\,0.6,\\,0.75,\\,0.9]$\n  - $\\mathbf{L}_{\\text{train}} = [0.09,\\,0.11,\\,0.14,\\,0.18,\\,0.22,\\,0.30,\\,0.42]$\n  - $\\mathbf{L}_{\\text{val}} = [0.26,\\,0.23,\\,0.20,\\,0.19,\\,0.22,\\,0.31,\\,0.49]$\n- Case B (no overfitting; validation steadily worsens with $s$, best model is dense):\n  - $\\mathbf{s} = [0.0,\\,0.2,\\,0.5,\\,0.8,\\,0.95]$\n  - $\\mathbf{L}_{\\text{train}} = [0.12,\\,0.15,\\,0.22,\\,0.40,\\,0.60]$\n  - $\\mathbf{L}_{\\text{val}} = [0.14,\\,0.17,\\,0.25,\\,0.45,\\,0.70]$\n- Case C (overfitting dominant at low $s$; high sparsity generalizes best without clear underfitting):\n  - $\\mathbf{s} = [0.0,\\,0.1,\\,0.3,\\,0.6,\\,0.85,\\,0.95]$\n  - $\\mathbf{L}_{\\text{train}} = [0.08,\\,0.09,\\,0.12,\\,0.18,\\,0.27,\\,0.38]$\n  - $\\mathbf{L}_{\\text{val}} = [0.35,\\,0.28,\\,0.22,\\,0.18,\\,0.16,\\,0.17]$\n\nComputation and output specification:\n- For each test case, compute the zero-based index $i^\\star$ of the minimum validation loss, an integer indicator $u \\in \\{0,1\\}$ for “underfitting detected” (high $s$), and an integer indicator $v \\in \\{0,1\\}$ for “overfitting detected” (low $s$).\n- Aggregate the results across the three test cases into a single flat list in the order $[i^\\star_{\\text{A}}, u_{\\text{A}}, v_{\\text{A}}, i^\\star_{\\text{B}}, u_{\\text{B}}, v_{\\text{B}}, i^\\star_{\\text{C}}, u_{\\text{C}}, v_{\\text{C}}]$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[r_1,r_2,r_3,r_4,r_5,r_6,r_7,r_8,r_9]$).", "solution": "The problem posed is to develop and implement a deterministic algorithm for diagnosing underfitting and overfitting in a family of machine learning models indexed by a pruning sparsity parameter, $s$. The diagnosis is based on discrete sequences of training loss, $L_{\\text{train}}(s_i)$, and validation loss, $L_{\\text{val}}(s_i)$, at corresponding sparsity levels $s_i$.\n\nThe problem is scientifically well-grounded, algorithmically specific, and logically consistent. It rests on the foundational principles of statistical learning theory, namely the bias-variance trade-off, where model capacity (controlled here by $s$) influences both empirical risk and generalization ability. The provided definitions, conditions, and constants constitute a complete and unambiguous set of instructions for constructing a solution. The problem is therefore deemed valid.\n\nThe solution proceeds by precisely implementing the specified algorithmic steps.\n\n**1. Data Representation and Normalization**\n\nThe inputs for each test case are three sequences: $\\{s_i\\}_{i=1}^n$, $\\{L_{\\text{train}}(s_i)\\}_{i=1}^n$, and $\\{L_{\\text{val}}(s_i)\\}_{i=1}^n$.\nTo compare losses on a common scale, the training and validation loss sequences are normalized. For any given sequence of measurements $X = \\{X(s_i)\\}$, its normalized counterpart $\\tilde{X} = \\{\\tilde{X}(s_i)\\}$ is calculated as:\n$$\n\\tilde{X}(s_i) = \\frac{X(s_i) - \\min_j X(s_j)}{\\max_j X(s_j) - \\min_j X(s_j) + \\varepsilon}\n$$\nwhere $\\varepsilon = 10^{-12}$ is a small constant to prevent division by zero in the case of a constant sequence. This transformation scales the values to approximately the range $[0, 1]$. We will compute $\\tilde{L}_{\\text{train}}$ and $\\tilde{L}_{\\text{val}}$ accordingly.\n\n**2. Region Partitioning**\n\nThe sequence of $n$ data points, ordered by sparsity $s$, is partitioned into three regions based on index. The analysis focuses on the two ends of the sparsity spectrum:\n- **Low-Sparsity Region**: The first $k = \\lceil n/3 \\rceil$ indices, corresponding to models with higher capacity.\n- **High-Sparsity Region**: The last $k = \\lceil n/3 \\rceil$ indices, corresponding to models with lower capacity.\n\n**3. Algorithmic Criteria for Diagnosis**\n\nThe diagnosis is performed by evaluating a set of conditions against fixed thresholds in the defined regions.\n\n**3.1. Overfitting Detection ($v$)**\nOverfitting is characterized by a model that fits the training data exceptionally well (low $L_{\\text{train}}$) but fails to generalize, resulting in a large gap between validation and training performance ($G(s) = L_{\\text{val}}(s) - L_{\\text{train}}(s)$). The algorithm formalizes this in the low-sparsity region. The indicator $v$ is set to $1$ if both of the following conditions are met, and $0$ otherwise:\n1. The average normalized training loss is below a low-loss threshold: $\\text{mean}(\\tilde{L}_{\\text{train}}) < \\tau_{\\text{train-low}}$, where $\\tau_{\\text{train-low}} = 0.25$.\n2. The average raw generalization gap is above an absolute gap threshold: $\\text{mean}(G(s)) > \\tau_{\\text{gap-abs}}$, where $\\tau_{\\text{gap-abs}} = 0.05$.\n\n**3.2. Underfitting Detection ($u$)**\nUnderfitting is characterized by a model with insufficient capacity to capture the underlying patterns in the data, resulting in high loss on both training and validation sets. As capacity is further reduced (increasing $s$), the performance is expected to degrade further. The algorithm formalizes this in the high-sparsity region. The indicator $u$ is set to $1$ if all four of the following conditions are met, and $0$ otherwise:\n1. The average normalized training loss exceeds a high-loss threshold: $\\text{mean}(\\tilde{L}_{\\text{train}}) > \\tau_{\\text{high}}$, where $\\tau_{\\text{high}} = 0.5$.\n2. The average normalized validation loss exceeds the same high-loss threshold: $\\text{mean}(\\tilde{L}_{\\text{val}}) > \\tau_{\\text{high}}$.\n3. The average discrete slope of the normalized training loss is positive and exceeds a threshold: $\\text{mean}(\\text{slope}(\\tilde{L}_{\\text{train}})) > \\tau_{\\text{slope}}$, where $\\tau_{\\text{slope}} = 0.05$.\n4. The average discrete slope of the normalized validation loss also exceeds the threshold: $\\text{mean}(\\text{slope}(\\tilde{L}_{\\text{val}})) > \\tau_{\\text{slope}}$.\n\nThe discrete slope for a sequence $\\tilde{X}$ between consecutive points indexed by $i$ and $i+1$ is computed via forward difference:\n$$\n\\text{slope}_i = \\frac{\\tilde{X}(s_{i+1}) - \\tilde{X}(s_i)}{s_{i+1} - s_i}\n$$\nThe average slope is calculated over all such consecutive pairs within the high-sparsity region. The strict inequality `>` in the conditions is critical; a value equal to the threshold does not satisfy the criterion.\n\n**4. Optimal Model Identification ($i^\\star$)**\nThe optimal model in the given discrete family is defined as the one that achieves the minimum validation loss, as this is our best estimate of generalization performance. The index $i^\\star$ is therefore determined by:\n$$\ni^\\star = \\underset{i}{\\mathrm{argmin}} \\{L_{\\text{val}}(s_i)\\}\n$$\nThis is a standard practice in model selection via a validation set.\n\n**Summary of Computations for Test Cases**\n\nApplying this algorithm to the provided test cases yields the following results:\n\n- **Case A**: $[s, L_{\\text{train}}, L_{\\text{val}}]$ with $n=7$. The optimal model is at index $i^\\star=3$ where $L_{\\text{val}}$ is minimal ($0.19$). The low-sparsity region (indices $0,1,2$) meets both criteria for overfitting ($v=1$). The high-sparsity region (indices $4,5,6$) satisfies three of the four criteria for underfitting. However, the average of $\\tilde{L}_{\\text{val}}$ in this region is exactly $0.5$, which does not strictly exceed $\\tau_{\\text{high}}=0.5$. Therefore, the underfitting condition is not met, and $u=0$.\n- **Case B**: $[s, L_{\\text{train}}, L_{\\text{val}}]$ with $n=5$. The optimal model is the densest one at index $i^\\star=0$. The low-sparsity region (indices $0,1$) fails the overfitting check on the generalization gap, thus $v=0$. The high-sparsity region (indices $3,4$) satisfies all four underfitting criteria, leading to $u=1$.\n- **Case C**: $[s, L_{\\text{train}}, L_{\\text{val}}]$ with $n=6$. The optimal model is a heavily pruned one at index $i^\\star=4$. The low-sparsity region (indices $0,1$) satisfies both conditions for overfitting, thus $v=1$. The high-sparsity region (indices $4,5$) fails the underfitting check because the average normalized validation loss is far below $\\tau_{\\text{high}}$, resulting in $u=0$.\n\nThe final results are an aggregation of $(i^\\star, u, v)$ for each case.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the model diagnosis problem by implementing the specified\n    curve-based analysis for underfitting and overfitting.\n    \"\"\"\n\n    # Define the fixed thresholds and constants from the problem statement.\n    TAU_HIGH = 0.5\n    TAU_SLOPE = 0.05\n    TAU_TRAIN_LOW = 0.25\n    TAU_GAP_ABS = 0.05\n    EPSILON = 1e-12\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: Balanced trade-off\n        (\n            np.array([0.0, 0.1, 0.2, 0.4, 0.6, 0.75, 0.9]),\n            np.array([0.09, 0.11, 0.14, 0.18, 0.22, 0.30, 0.42]),\n            np.array([0.26, 0.23, 0.20, 0.19, 0.22, 0.31, 0.49])\n        ),\n        # Case B: No overfitting\n        (\n            np.array([0.0, 0.2, 0.5, 0.8, 0.95]),\n            np.array([0.12, 0.15, 0.22, 0.40, 0.60]),\n            np.array([0.14, 0.17, 0.25, 0.45, 0.70])\n        ),\n        # Case C: Overfitting dominant\n        (\n            np.array([0.0, 0.1, 0.3, 0.6, 0.85, 0.95]),\n            np.array([0.08, 0.09, 0.12, 0.18, 0.27, 0.38]),\n            np.array([0.35, 0.28, 0.22, 0.18, 0.16, 0.17])\n        )\n    ]\n\n    def normalize(x):\n        \"\"\"\n        Normalizes a sequence according to the problem specification.\n        \"\"\"\n        min_x = np.min(x)\n        max_x = np.max(x)\n        denominator = max_x - min_x + EPSILON\n        return (x - min_x) / denominator\n\n    def analyze_case(s, l_train, l_val):\n        \"\"\"\n        Performs the full diagnosis for a single test case.\n        Returns a tuple (i_star, u, v).\n        \"\"\"\n        n = len(s)\n\n        # 1. Optimal model index\n        i_star = np.argmin(l_val)\n\n        # 2. Normalize losses\n        l_train_norm = normalize(l_train)\n        l_val_norm = normalize(l_val)\n\n        # 3. Partition regions\n        k = int(np.ceil(n / 3))\n        # Low-sparsity region indices: 0 to k-1\n        low_sparsity_indices = slice(0, k)\n        # High-sparsity region indices: n-k to n-1\n        high_sparsity_indices = slice(n - k, n)\n\n        # 4. Overfitting detection (low-sparsity region)\n        avg_l_train_norm_low = np.mean(l_train_norm[low_sparsity_indices])\n        \n        g = l_val - l_train\n        avg_g_low = np.mean(g[low_sparsity_indices])\n\n        v = 0\n        if avg_l_train_norm_low  TAU_TRAIN_LOW and avg_g_low > TAU_GAP_ABS:\n            v = 1\n\n        # 5. Underfitting detection (high-sparsity region)\n        avg_l_train_norm_high = np.mean(l_train_norm[high_sparsity_indices])\n        avg_l_val_norm_high = np.mean(l_val_norm[high_sparsity_indices])\n\n        # Calculate discrete slopes in the high-sparsity region\n        slopes_train = []\n        slopes_val = []\n        \n        # The indices for which we can calculate a forward slope within the region\n        start_idx = n - k\n        end_idx = n - 1\n        \n        if end_idx > start_idx: # Need at least 2 points to calculate a slope\n            for i in range(start_idx, end_idx):\n                delta_s = s[i+1] - s[i]\n                if abs(delta_s) > EPSILON:\n                    slope_train = (l_train_norm[i+1] - l_train_norm[i]) / delta_s\n                    slope_val = (l_val_norm[i+1] - l_val_norm[i]) / delta_s\n                    slopes_train.append(slope_train)\n                    slopes_val.append(slope_val)\n\n        avg_slope_train = np.mean(slopes_train) if slopes_train else -np.inf\n        avg_slope_val = np.mean(slopes_val) if slopes_val else -np.inf\n        \n        u = 0\n        if (avg_l_train_norm_high > TAU_HIGH and\n            avg_l_val_norm_high > TAU_HIGH and\n            avg_slope_train > TAU_SLOPE and\n            avg_slope_val > TAU_SLOPE):\n            u = 1\n\n        return i_star, u, v\n\n    # Process all test cases and aggregate results\n    final_results = []\n    for case in test_cases:\n        s, l_train, l_val = case\n        i_star, u, v = analyze_case(s, l_train, l_val)\n        final_results.extend([i_star, u, v])\n\n    # Print the final aggregated list in the specified format\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n```", "id": "3135754"}, {"introduction": "Learning curve analysis is not just for diagnosis; it's a powerful tool for making strategic decisions about model development. In practical scenarios, especially with class imbalance, deciding how to invest in further data collection is a critical challenge. This exercise simulates this decision-making process, guiding you to use the local slope of learning curves to determine whether collecting more \"anomaly\" or \"normal\" data points offers the fastest path to improving your model's performance. [@problem_id:3138165]", "problem": "You are given a binary classification setting with severe class imbalance, where the two classes are \"normal\" and \"anomaly.\" The learner is trained on samples with counts $n_a$ anomalies and $n_n$ normals. The validation performance is quantified by the validation loss $L_{\\mathrm{val}}(n_a,n_n)$, defined as the expected loss on a held-out validation distribution. Under Independent and Identically Distributed (i.i.d.) sampling and Empirical Risk Minimization (ERM), the expectation of $L_{\\mathrm{val}}$ is non-increasing as more informative training data are added, but finite-sample fluctuations and class imbalance make the local behavior dependent on the type of sample added.\n\nYour task is to implement a program that, for a set of cases, decides whether collecting more anomalies or more normals will most effectively decrease $L_{\\mathrm{val}}$ at the current regime by analyzing local learning curves. For each case, you are provided two localized learning curve segments:\n\n- A sequence where only anomalies are added: positions $\\{x^{(a)}_i\\}$ representing added anomalies (relative to a baseline), and the corresponding validation losses $\\{L^{(a)}_i\\}$ measured on the validation set at those positions.\n- A sequence where only normals are added: positions $\\{x^{(n)}_j\\}$ representing added normals (relative to a baseline), and the corresponding validation losses $\\{L^{(n)}_j\\}$ measured on the validation set at those positions.\n\nYou must estimate the local marginal effect per sample of adding anomalies versus normals using a local linear model. Concretely, for a sequence $\\{(x_k, L_k)\\}$, fit a line $L \\approx \\alpha + \\beta x$ using Ordinary Least Squares (OLS) on the last $K$ points, where $K$ is a small integer. The slope $\\beta$ estimates the local discrete derivative $\\frac{\\partial L_{\\mathrm{val}}}{\\partial n}$ at the current regime per one added sample of the corresponding class. Use $K=3$ or all available points if the sequence is shorter.\n\nDefine the decision rule using a small tolerance $\\epsilon$ to avoid reacting to negligible differences:\n\n- Compute $\\hat{g}_a$ as the OLS slope $\\beta$ for the anomalies-only sequence and $\\hat{g}_n$ as the OLS slope $\\beta$ for the normals-only sequence. Here $\\hat{g}_a$ estimates $\\frac{\\partial L_{\\mathrm{val}}}{\\partial n_a}$ and $\\hat{g}_n$ estimates $\\frac{\\partial L_{\\mathrm{val}}}{\\partial n_n}$.\n- If both $\\hat{g}_a \\ge 0$ and $\\hat{g}_n \\ge 0$, output $-1$ (no expected improvement from adding either class locally).\n- Otherwise, compare the slopes with tolerance $\\epsilon$:\n  - If $\\hat{g}_a  \\hat{g}_n - \\epsilon$, output $1$ (collect anomalies).\n  - Else if $\\hat{g}_n  \\hat{g}_a - \\epsilon$, output $0$ (collect normals).\n  - Else, output $-1$ (indifferent within tolerance).\n\nUse $\\epsilon = 10^{-5}$ and $K=3$ for all cases.\n\nYour program must solve the following test suite, where each case is specified by the tuples of positions and losses for anomalies and normals. All numbers below are decimals (no percentages) and represent counts or loss values without physical units.\n\n- Case $1$:\n  - Anomalies-only positions: $[0,10,20,30]$\n  - Anomalies-only losses: $[0.210,0.200,0.193,0.188]$\n  - Normals-only positions: $[0,100,200,300]$\n  - Normals-only losses: $[0.210,0.207,0.205,0.204]$\n- Case $2$:\n  - Anomalies-only positions: $[0,10,20,30]$\n  - Anomalies-only losses: $[0.160,0.1595,0.1590,0.1587]$\n  - Normals-only positions: $[0,100,200,300]$\n  - Normals-only losses: $[0.160,0.152,0.147,0.145]$\n- Case $3$:\n  - Anomalies-only positions: $[0,20,40,60]$\n  - Anomalies-only losses: $[0.300,0.298,0.296,0.294]$\n  - Normals-only positions: $[0,100,200,300]$\n  - Normals-only losses: $[0.300,0.290,0.280,0.270]$\n- Case $4$:\n  - Anomalies-only positions: $[0,10,20,30,40]$\n  - Anomalies-only losses: $[0.250,0.245,0.242,0.243,0.241]$\n  - Normals-only positions: $[0,10,20,30,40]$\n  - Normals-only losses: $[0.250,0.248,0.2478,0.2477,0.2476]$\n- Case $5$:\n  - Anomalies-only positions: $[0,5,10,15]$\n  - Anomalies-only losses: $[0.180,0.181,0.182,0.1825]$\n  - Normals-only positions: $[0,5,10,15]$\n  - Normals-only losses: $[0.180,0.1805,0.181,0.1815]$\n\nFinal output format: Your program should produce a single line containing the decisions for the cases in order, encoded as integers in a comma-separated list enclosed in square brackets. Use the encoding $1$ for \"collect anomalies,\" $0$ for \"collect normals,\" and $-1$ for \"indifferent or no improvement\" (e.g., $[1,0,-1,1,-1]$).", "solution": "The problem requires a decision-making algorithm to guide data collection in a binary classification task with class imbalance. The goal is to determine whether adding more \"anomaly\" samples or \"normal\" samples is more likely to decrease the validation loss, $L_{\\mathrm{val}}$. This decision is based on a local analysis of learning curves. We are given two sequences of validation loss measurements for each case: one where only anomalies are added, $\\{ (x^{(a)}_i, L^{(a)}_i) \\}$, and one where only normals are added, $\\{ (x^{(n)}_j, L^{(n)}_j) \\}$. The variables $x_i$ represent the number of added samples of a given class.\n\nThe core of the method is to estimate the local rate of change of the validation loss with respect to the number of added samples for each class. This rate, or local discrete derivative, is denoted as $\\frac{\\partial L_{\\mathrm{val}}}{\\partial n_a}$ for anomalies and $\\frac{\\partial L_{\\mathrm{val}}}{\\partial n_n}$ for normals. A negative value indicates that adding more samples of that class is reducing the validation loss, which is the desired outcome.\n\nWe estimate these derivatives by fitting a local linear model, $L \\approx \\alpha + \\beta x$, to the learning curve data. The slope $\\beta$ of this line serves as our estimate for the derivative. The problem specifies using Ordinary Least Squares (OLS) on the last $K=3$ data points of each sequence to find this slope. If a sequence has fewer than $K$ points, all available points are used. For a set of $M$ points $\\{(x_k, L_k)\\}_{k=1}^M$, the OLS estimate for the slope $\\beta$ is given by the formula:\n$$\n\\beta = \\frac{\\sum_{k=1}^{M} (x_k - \\bar{x})(L_k - \\bar{L})}{\\sum_{k=1}^{M} (x_k - \\bar{x})^2}\n$$\nwhere $\\bar{x} = \\frac{1}{M}\\sum_{k=1}^{M} x_k$ and $\\bar{L} = \\frac{1}{M}\\sum_{k=1}^{M} L_k$ are the sample means.\n\nLet $\\hat{g}_a$ be the estimated slope for the anomalies-only sequence and $\\hat{g}_n$ be the estimated slope for the normals-only sequence. The decision rule is then applied using a tolerance $\\epsilon = 10^{-5}$ to avoid making decisions based on statistically insignificant differences. The rules are as follows:\n\n$1$. If both $\\hat{g}_a \\ge 0$ and $\\hat{g}_n \\ge 0$, it indicates that adding samples of either class is not expected to improve (and may even worsen) the validation loss locally. In this situation, the output is $-1$.\n\n$2$. Otherwise, we compare the slopes:\n- If $\\hat{g}_a  \\hat{g}_n - \\epsilon$, the rate of decrease for anomalies is significantly greater than for normals. The decision is to collect anomalies, with an output of $1$.\n- Else if $\\hat{g}_n  \\hat{g}_a - \\epsilon$, the rate of decrease for normals is significantly greater. The decision is to collect normals, with an output of $0$.\n- Else, the difference in the rates of change is within the tolerance $\\epsilon$, so we are indifferent. The output is $-1$.\n\nWe will now apply this procedure to each of the $5$ test cases provided, using $K=3$ and $\\epsilon=10^{-5}$.\n\n**Case 1:**\n- Anomalies: Last $3$ points are $\\{(10, 0.200), (20, 0.193), (30, 0.188)\\}$.\n  The calculated OLS slope is $\\hat{g}_a = -0.0006$.\n- Normals: Last $3$ points are $\\{(100, 0.207), (200, 0.205), (300, 0.204)\\}$.\n  The calculated OLS slope is $\\hat{g}_n = -0.000015$.\n- Decision: Both slopes are negative. We check if $\\hat{g}_a  \\hat{g}_n - \\epsilon$:\n  $-0.0006  -0.000015 - 10^{-5} \\implies -0.0006  -0.000025$. This inequality is true.\n  The decision is to collect anomalies. Result: $1$.\n\n**Case 2:**\n- Anomalies: Last $3$ points are $\\{(10, 0.1595), (20, 0.1590), (30, 0.1587)\\}$.\n  The calculated OLS slope is $\\hat{g}_a \\approx -0.00004$.\n- Normals: Last $3$ points are $\\{(100, 0.152), (200, 0.147), (300, 0.145)\\}$.\n  The calculated OLS slope is $\\hat{g}_n = -0.000035$.\n- Decision: Both slopes are negative.\n  - Check $\\hat{g}_a  \\hat{g}_n - \\epsilon$: $-0.00004  -0.000035 - 10^{-5} \\implies -0.00004  -0.000045$. This is false.\n  - Check $\\hat{g}_n  \\hat{g}_a - \\epsilon$: $-0.000035  -0.00004 - 10^{-5} \\implies -0.000035  -0.00005$. This is false.\n  The difference is not significant enough to prefer one over the other. The decision is indifferent. Result: $-1$.\n\n**Case 3:**\n- Anomalies: Last $3$ points are $\\{(20, 0.298), (40, 0.296), (60, 0.294)\\}$. Since these points are perfectly linear, the OLS slope is exact.\n  $\\hat{g}_a = \\frac{0.294-0.298}{60-20} = \\frac{-0.004}{40} = -0.0001$.\n- Normals: Last $3$ points are $\\{(100, 0.290), (200, 0.280), (300, 0.270)\\}$. These are also perfectly linear.\n  $\\hat{g}_n = \\frac{0.270-0.290}{300-100} = \\frac{-0.02}{200} = -0.0001$.\n- Decision: The slopes are identical, $\\hat{g}_a = \\hat{g}_n = -0.0001$. Neither comparison condition involving $\\epsilon$ can be met. The decision is indifferent. Result: $-1$.\n\n**Case 4:**\n- Anomalies: There are $5$ points; we use the last $3$: $\\{(20, 0.242), (30, 0.243), (40, 0.241)\\}$.\n  The calculated OLS slope is $\\hat{g}_a = -0.00005$.\n- Normals: There are $5$ points; we use the last $3$: $\\{(20, 0.2478), (30, 0.2477), (40, 0.2476)\\}$.\n  The calculated OLS slope is $\\hat{g}_n = -0.00001$.\n- Decision: Both slopes are negative. We check if $\\hat{g}_a  \\hat{g}_n - \\epsilon$:\n  $-0.00005  -0.00001 - 10^{-5} \\implies -0.00005  -0.00002$. This inequality is true.\n  The decision is to collect anomalies. Result: $1$.\n\n**Case 5:**\n- Anomalies: Last $3$ points are $\\{(5, 0.181), (10, 0.182), (15, 0.1825)\\}$.\n  The calculated OLS slope is $\\hat{g}_a \\approx 0.00015$.\n- Normals: Last $3$ points are $\\{(5, 0.1805), (10, 0.181), (15, 0.1815)\\}$.\n  The calculated OLS slope is $\\hat{g}_n = 0.0001$.\n- Decision: Both slopes are positive, $\\hat{g}_a  0$ and $\\hat{g}_n  0$. According to the first rule, this indicates no expected improvement from adding either class of samples locally. The decision is no improvement. Result: $-1$.\n\nThe final sequence of decisions for the five cases is $[1, -1, -1, 1, -1]$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_slope(positions, losses, K):\n    \"\"\"\n    Calculates the OLS slope for the last K points of a sequence.\n\n    Args:\n        positions (list of float): The x-values (number of samples).\n        losses (list of float): The y-values (validation loss).\n        K (int): The number of recent points to use for the linear fit.\n\n    Returns:\n        float: The slope of the linear regression line.\n    \"\"\"\n    if len(positions)  2:\n        # Not enough points to fit a line.\n        return 0.0\n\n    # Determine the number of points to use for fitting.\n    num_points_to_fit = min(len(positions), K)\n\n    # Slice the last `num_points_to_fit` points.\n    x_fit = np.array(positions[-num_points_to_fit:])\n    y_fit = np.array(losses[-num_points_to_fit:])\n\n    # Calculate the means.\n    x_mean = np.mean(x_fit)\n    y_mean = np.mean(y_fit)\n\n    # Calculate the numerator and denominator for the slope formula.\n    # beta = sum((x_i - x_mean) * (y_i - y_mean)) / sum((x_i - x_mean)^2)\n    numerator = np.sum((x_fit - x_mean) * (y_fit - y_mean))\n    denominator = np.sum((x_fit - x_mean)**2)\n\n    if np.isclose(denominator, 0):\n        # This case occurs if all x_fit values are identical, which means\n        # no new samples were added in the window. The slope is taken as 0.\n        return 0.0\n\n    slope = numerator / denominator\n    return slope\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test suite of learning curve data.\n    \"\"\"\n    # Define constants from the problem statement.\n    K = 3\n    epsilon = 1e-5\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        { # Case 1\n            \"anomalies_pos\": [0, 10, 20, 30],\n            \"anomalies_loss\": [0.210, 0.200, 0.193, 0.188],\n            \"normals_pos\": [0, 100, 200, 300],\n            \"normals_loss\": [0.210, 0.207, 0.205, 0.204],\n        },\n        { # Case 2\n            \"anomalies_pos\": [0, 10, 20, 30],\n            \"anomalies_loss\": [0.160, 0.1595, 0.1590, 0.1587],\n            \"normals_pos\": [0, 100, 200, 300],\n            \"normals_loss\": [0.160, 0.152, 0.147, 0.145],\n        },\n        { # Case 3\n            \"anomalies_pos\": [0, 20, 40, 60],\n            \"anomalies_loss\": [0.300, 0.298, 0.296, 0.294],\n            \"normals_pos\": [0, 100, 200, 300],\n            \"normals_loss\": [0.300, 0.290, 0.280, 0.270],\n        },\n        { # Case 4\n            \"anomalies_pos\": [0, 10, 20, 30, 40],\n            \"anomalies_loss\": [0.250, 0.245, 0.242, 0.243, 0.241],\n            \"normals_pos\": [0, 10, 20, 30, 40],\n            \"normals_loss\": [0.250, 0.248, 0.2478, 0.2477, 0.2476],\n        },\n        { # Case 5\n            \"anomalies_pos\": [0, 5, 10, 15],\n            \"anomalies_loss\": [0.180, 0.181, 0.182, 0.1825],\n            \"normals_pos\": [0, 5, 10, 15],\n            \"normals_loss\": [0.180, 0.1805, 0.181, 0.1815],\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        # Calculate local slopes for anomalies and normals\n        g_a = calculate_slope(case[\"anomalies_pos\"], case[\"anomalies_loss\"], K)\n        g_n = calculate_slope(case[\"normals_pos\"], case[\"normals_loss\"], K)\n\n        # Apply the decision rule\n        decision = -1  # Default to indifferent\n        if g_a >= 0 and g_n >= 0:\n            decision = -1  # No expected improvement from either\n        elif g_a  g_n - epsilon:\n            decision = 1  # Collect anomalies\n        elif g_n  g_a - epsilon:\n            decision = 0  # Collect normals\n        else:\n            decision = -1  # Indifferent within tolerance\n        \n        results.append(decision)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3138165"}]}