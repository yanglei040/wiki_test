{"hands_on_practices": [{"introduction": "To begin our hands-on exploration, we will start with a fundamental question in statistics: how many samples do we need to accurately estimate a mean? This exercise directly compares two cornerstone concentration inequalities, Hoeffding's and Bernstein's, to quantify the value of information. By working through this problem [@problem_id:3189962], you will see how incorporating knowledge of a distribution's variance leads to significantly more efficient estimates, a principle with wide-ranging practical implications.", "problem": "A data scientist is estimating the mean of a bounded loss in a binary classification task. Let $\\{X_{i}\\}_{i=1}^{n}$ be independent and identically distributed random variables with $X_{i} \\in [0,1]$ and $\\mathbb{E}[X_{i}] = \\mu$. The goal is to choose a sample size $n$ so that the empirical mean $\\hat{\\mu}_{n} = \\frac{1}{n}\\sum_{i=1}^{n} X_{i}$ satisfies $|\\hat{\\mu}_{n} - \\mu| \\leq \\varepsilon$ with probability at least $1 - \\delta$, where $\\varepsilon \\in (0,1)$ and $\\delta \\in (0,1)$ are given design parameters.\n\nTwo approaches are considered:\n\n1) A range-only approach that uses only the information that $X_{i} \\in [0,1]$.\n\n2) A variance-aware approach that additionally uses the knowledge that $\\operatorname{Var}(X_{i}) = \\sigma^{2}$ for a known $\\sigma^{2} \\in [0, \\tfrac{1}{4}]$.\n\nDefine $n_{\\text{H}}(\\varepsilon,\\delta)$ to be a sufficient sample size obtained under the range-only approach, and $n_{\\text{B}}(\\varepsilon,\\delta,\\sigma^{2})$ to be a sufficient sample size obtained under the variance-aware approach. Let the sample size savings factor be\n$$\nS(\\varepsilon,\\delta,\\sigma^{2}) \\equiv \\frac{n_{\\text{H}}(\\varepsilon,\\delta)}{n_{\\text{B}}(\\varepsilon,\\delta,\\sigma^{2})}.\n$$\n\nUsing foundational concentration results for bounded independent random variables and the knowledge of variance when available, derive an exact simplified expression for $S(\\varepsilon,\\delta,\\sigma^{2})$ as a function of $\\varepsilon$ and $\\sigma^{2}$ only. Your final answer must be a single closed-form expression; no inequalities or implicit definitions are allowed. Do not provide a numerical approximation.", "solution": "The problem requires the derivation of the sample size savings factor $S(\\varepsilon,\\delta,\\sigma^{2}) = \\frac{n_{\\text{H}}(\\varepsilon,\\delta)}{n_{\\text{B}}(\\varepsilon,\\delta,\\sigma^{2})}$, where $n_{\\text{H}}$ and $n_{\\text{B}}$ are sufficient sample sizes derived from concentration inequalities. The goal is to find an expression for $S$ that depends only on $\\varepsilon$ and $\\sigma^{2}$. This will be achieved by applying the appropriate foundational concentration inequalities for each case and then computing the ratio of the derived sample sizes.\n\nFirst, we determine the sufficient sample size $n_{\\text{H}}(\\varepsilon,\\delta)$ for the range-only approach. We are given that $\\{X_{i}\\}_{i=1}^{n}$ are independent and identically distributed (i.i.d.) random variables with $X_{i} \\in [0,1]$. Let $\\mathbb{E}[X_i] = \\mu$ and $\\hat{\\mu}_{n} = \\frac{1}{n}\\sum_{i=1}^{n} X_{i}$. The appropriate tool for this scenario is Hoeffding's inequality, which bounds the deviation of a sum of bounded independent random variables from its expected value. For the sample mean, Hoeffding's inequality is stated as:\n$$P(|\\hat{\\mu}_{n} - \\mu| \\ge \\varepsilon) \\le 2 \\exp(-2n\\varepsilon^2)$$\nThis inequality holds because the range of each $X_i$ is $1-0=1$. We require the probability of the deviation being larger than $\\varepsilon$ to be at most $\\delta$:\n$$2 \\exp(-2n\\varepsilon^2) \\le \\delta$$\nTo find a sufficient sample size $n$, we solve this inequality for $n$:\n$$\\exp(-2n\\varepsilon^2) \\le \\frac{\\delta}{2}$$\n$$-2n\\varepsilon^2 \\le \\ln\\left(\\frac{\\delta}{2}\\right)$$\n$$2n\\varepsilon^2 \\ge -\\ln\\left(\\frac{\\delta}{2}\\right) = \\ln\\left(\\frac{2}{\\delta}\\right)$$\n$$n \\ge \\frac{1}{2\\varepsilon^2}\\ln\\left(\\frac{2}{\\delta}\\right)$$\nTherefore, a sufficient sample size for the range-only approach is:\n$$n_{\\text{H}}(\\varepsilon,\\delta) = \\frac{1}{2\\varepsilon^2}\\ln\\left(\\frac{2}{\\delta}\\right)$$\n\nNext, we determine the sufficient sample size $n_{\\text{B}}(\\varepsilon,\\delta,\\sigma^{2})$ for the variance-aware approach. In addition to the previous conditions, we know that $\\operatorname{Var}(X_{i}) = \\sigma^{2}$. The foundational result that incorporates variance information for bounded variables is Bernstein's inequality. We apply it to the zero-mean variables $Y_i = X_i - \\mu$. We have $\\mathbb{E}[Y_i] = 0$ and $\\operatorname{Var}(Y_i) = \\operatorname{Var}(X_i) = \\sigma^2$. Since $X_i \\in [0,1]$ and $\\mu = \\mathbb{E}[X_i] \\in [0,1]$, the variable $Y_i$ is bounded as $|Y_i| = |X_i - \\mu| \\le \\max(\\mu, 1-\\mu) \\le 1$. We take the bound $M=1$. A common form of the two-sided Bernstein's inequality is:\n$$P\\left(\\left|\\frac{1}{n}\\sum_{i=1}^{n} Y_i\\right| \\ge \\varepsilon\\right) \\le 2 \\exp\\left(-\\frac{n\\varepsilon^2}{2(\\sigma^2 + M\\varepsilon/3)}\\right)$$\nSubstituting $|\\hat{\\mu}_{n} - \\mu|$ for $\\left|\\frac{1}{n}\\sum_{i=1}^{n} Y_i\\right|$ and $M=1$:\n$$P(|\\hat{\\mu}_{n} - \\mu| \\ge \\varepsilon) \\le 2 \\exp\\left(-\\frac{n\\varepsilon^2}{2(\\sigma^2 + \\varepsilon/3)}\\right)$$\nWe require this probability to be at most $\\delta$:\n$$2 \\exp\\left(-\\frac{n\\varepsilon^2}{2(\\sigma^2 + \\varepsilon/3)}\\right) \\le \\delta$$\nSolving for a sufficient sample size $n$:\n$$\\exp\\left(-\\frac{n\\varepsilon^2}{2(\\sigma^2 + \\varepsilon/3)}\\right) \\le \\frac{\\delta}{2}$$\n$$-\\frac{n\\varepsilon^2}{2(\\sigma^2 + \\varepsilon/3)} \\le \\ln\\left(\\frac{\\delta}{2}\\right)$$\n$$\\frac{n\\varepsilon^2}{2(\\sigma^2 + \\varepsilon/3)} \\ge -\\ln\\left(\\frac{\\delta}{2}\\right) = \\ln\\left(\\frac{2}{\\delta}\\right)$$\n$$n \\ge \\frac{2(\\sigma^2 + \\varepsilon/3)}{\\varepsilon^2}\\ln\\left(\\frac{2}{\\delta}\\right)$$\nTherefore, a sufficient sample size for the variance-aware approach is:\n$$n_{\\text{B}}(\\varepsilon,\\delta,\\sigma^{2}) = \\frac{2(\\sigma^2 + \\varepsilon/3)}{\\varepsilon^2}\\ln\\left(\\frac{2}{\\delta}\\right)$$\n\nFinally, we compute the sample size savings factor $S(\\varepsilon,\\delta,\\sigma^{2})$ by taking the ratio of $n_{\\text{H}}$ and $n_{\\text{B}}$:\n$$S(\\varepsilon,\\delta,\\sigma^{2}) = \\frac{n_{\\text{H}}(\\varepsilon,\\delta)}{n_{\\text{B}}(\\varepsilon,\\delta,\\sigma^{2})} = \\frac{\\frac{1}{2\\varepsilon^2}\\ln\\left(\\frac{2}{\\delta}\\right)}{\\frac{2(\\sigma^2 + \\varepsilon/3)}{\\varepsilon^2}\\ln\\left(\\frac{2}{\\delta}\\right)}$$\nThe terms $\\ln\\left(\\frac{2}{\\delta}\\right)$ and $\\varepsilon^2$ cancel, yielding an expression that depends only on $\\sigma^2$ and $\\varepsilon$, as required:\n$$S(\\sigma^{2},\\varepsilon) = \\frac{1/2}{2(\\sigma^2 + \\varepsilon/3)} = \\frac{1}{4(\\sigma^2 + \\varepsilon/3)}$$\nTo provide a simplified closed-form expression without fractions in the denominator, we can rewrite this as:\n$$S(\\sigma^{2},\\varepsilon) = \\frac{1}{4\\sigma^2 + \\frac{4\\varepsilon}{3}} = \\frac{1}{\\frac{12\\sigma^2 + 4\\varepsilon}{3}} = \\frac{3}{12\\sigma^2 + 4\\varepsilon} = \\frac{3}{4(3\\sigma^2 + \\varepsilon)}$$\nThis is the final simplified expression for the sample size savings factor.", "answer": "$$\n\\boxed{\\frac{3}{4(3\\sigma^2 + \\varepsilon)}}\n$$", "id": "3189962"}, {"introduction": "Having seen how to bound the deviation of a single empirical average, we now turn to a core challenge in machine learning: ensuring our model's performance on the training data is a reliable indicator of its performance on new, unseen data. This practice [@problem_id:3189954] explores the powerful concept of uniform convergence, which provides such a guarantee over an entire class of functions. By analyzing a simple class of threshold functions, we will calculate its Vapnik-Chervonenkis (VC) dimension and apply the Dvoretzky–Kiefer–Wolfowitz (DKW) inequality to understand how the \"complexity\" of a model class affects generalization.", "problem": "Consider the hypothesis class of monotone threshold functions on the real line, defined by $\\mathcal{H} = \\{ h_{t} : \\mathbb{R} \\to \\{0,1\\} \\mid h_{t}(x) = \\mathbf{1}\\{x \\le t\\},\\ t \\in \\mathbb{R} \\}$. Let $X_{1},\\dots,X_{n}$ be independent and identically distributed real-valued samples drawn from an arbitrary distribution $P$ on $\\mathbb{R}$ with cumulative distribution function $F$. For $h \\in \\mathcal{H}$, define the true risk $P(h) = \\mathbb{E}[h(X)]$ and the empirical risk $P_{n}(h) = \\frac{1}{n} \\sum_{i=1}^{n} h(X_{i})$.\n\nTasks:\n1. From first principles, compute the Vapnik–Chervonenkis (VC) dimension of $\\mathcal{H}$ by directly applying the definition of shattering.\n2. Let $\\epsilon \\in (0,1)$ and $\\delta \\in (0,1)$. Using only the exact-constant form of the Dvoretzky–Kiefer–Wolfowitz inequality as a foundational fact, derive the smallest integer sample size $n^{\\star}(\\epsilon,\\delta)$ such that, for every distribution $P$ on $\\mathbb{R}$, with probability at least $1-\\delta$ (over the draw of the sample), the uniform deviation bound\n$$\n\\sup_{h \\in \\mathcal{H}} \\big| P(h) - P_{n}(h) \\big| \\le \\epsilon\n$$\nholds simultaneously.\n3. By reducing the uniform deviation over $\\mathcal{H}$ to a deviation of a binomial proportion at a fixed threshold for a distribution with a continuous and strictly increasing cumulative distribution function at its median, argue that in any distribution-free sub-Gaussian tail bound of the form\n$$\n\\sup_{P} \\Pr\\!\\left( \\sup_{h \\in \\mathcal{H}} \\big| P(h) - P_{n}(h) \\big|  \\epsilon \\right) \\le K \\exp\\!\\big( - c\\, n \\epsilon^{2} \\big),\n$$\nthat holds for all $n$ sufficiently large and all $\\epsilon$ in a fixed interval $(0,\\epsilon_{0}]$, the largest possible exponent constant $c$ is a fixed numerical value. Determine this optimal $c$.\n\nReport your final answer as a single row matrix in the order: the VC dimension of $\\mathcal{H}$, the exact closed-form expression for $n^{\\star}(\\epsilon,\\delta)$, and the optimal exponent constant $c$. No rounding is required, and the answer must be an exact closed-form expression.", "solution": "The solution is presented in three parts, corresponding to the three tasks in the problem statement.\n\n**Task 1: VC dimension of $\\mathcal{H}$**\nThe Vapnik–Chervonenkis (VC) dimension of a hypothesis class $\\mathcal{H}$, denoted $\\mathrm{VCdim}(\\mathcal{H})$, is the cardinality of the largest set of points that $\\mathcal{H}$ can shatter. A set of points $\\{x_1, \\dots, x_m\\}$ is said to be shattered by $\\mathcal{H}$ if, for every possible labeling $(y_1, \\dots, y_m) \\in \\{0, 1\\}^m$, there exists a hypothesis $h \\in \\mathcal{H}$ such that $h(x_i) = y_i$ for all $i \\in \\{1, \\dots, m\\}$. The hypotheses in our class are $h_t(x) = \\mathbf{1}\\{x \\le t\\}$.\n\nFirst, we show that $\\mathrm{VCdim}(\\mathcal{H}) \\ge 1$. Consider any single point $\\{x_1\\}$. There are $2^1 = 2$ possible labelings: $\\{0\\}$ and $\\{1\\}$.\n*   To obtain the labeling $\\{1\\}$, we need $h_t(x_1) = 1$, which means $x_1 \\le t$. We can choose $t = x_1$.\n*   To obtain the labeling $\\{0\\}$, we need $h_t(x_1) = 0$, which means $x_1 > t$. We can choose $t = x_1 - 1$.\nSince a single point can be shattered, $\\mathrm{VCdim}(\\mathcal{H}) \\ge 1$.\n\nNext, we show that $\\mathrm{VCdim}(\\mathcal{H})  2$. Consider any set of two distinct points, $\\{x_1, x_2\\}$. Without loss of generality, let $x_1  x_2$. There are $2^2 = 4$ possible labelings: $(0,0), (0,1), (1,0), (1,1)$. Let's see if we can realize all of them.\n*   For $(0,0)$: We need $h_t(x_1)=0$ and $h_t(x_2)=0$. This means $x_1 > t$ and $x_2 > t$. We can choose any $t  x_1$, for example $t=x_1 - 1$.\n*   For $(1,1)$: We need $h_t(x_1)=1$ and $h_t(x_2)=1$. This means $x_1 \\le t$ and $x_2 \\le t$. We can choose any $t \\ge x_2$, for example $t=x_2$.\n*   For $(1,0)$: We need $h_t(x_1)=1$ and $h_t(x_2)=0$. This means $x_1 \\le t$ and $x_2 > t$. We can choose any $t$ such that $x_1 \\le t  x_2$, for example $t=x_1$.\n*   For $(0,1)$: We need $h_t(x_1)=0$ and $h_t(x_2)=1$. This means $x_1 > t$ and $x_2 \\le t$. This would imply $t  x_1$ and $t \\ge x_2$. Since we assumed $x_1  x_2$, it is impossible to find such a $t$.\n\nBecause the labeling $(0,1)$ cannot be generated for any pair of points $\\{x_1, x_2\\}$ with $x_1  x_2$, no set of size $2$ can be shattered by $\\mathcal{H}$. Therefore, the VC dimension is $1$.\n\n**Task 2: Sample size $n^{\\star}(\\epsilon,\\delta)$**\nThe uniform deviation that we want to bound is $\\sup_{h \\in \\mathcal{H}} |P(h) - P_n(h)|$. For the given hypothesis class $\\mathcal{H}$, the true risk and empirical risk are:\n$P(h_t) = \\mathbb{E}[\\mathbf{1}\\{X \\le t\\}] = \\Pr(X \\le t) = F(t)$, where $F$ is the CDF of the data-generating distribution $P$.\n$P_n(h_t) = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}\\{X_i \\le t\\} = \\hat{F}_n(t)$, where $\\hat{F}_n$ is the empirical CDF.\nThus, the uniform deviation is equivalent to the Kolmogorov-Smirnov distance between the true and empirical CDFs:\n$$ \\sup_{h \\in \\mathcal{H}} \\big| P(h) - P_{n}(h) \\big| = \\sup_{t \\in \\mathbb{R}} \\big| F(t) - \\hat{F}_n(t) \\big| $$\nThe problem asks to use the exact-constant form of the Dvoretzky–Kiefer–Wolfowitz (DKW) inequality, which was established by Massart (1990). This inequality states that for any $n \\ge 1$ and any $\\epsilon > 0$,\n$$ \\Pr\\left( \\sup_{t \\in \\mathbb{R}} |F(t) - \\hat{F}_n(t)| > \\epsilon \\right) \\le 2 \\exp(-2n\\epsilon^2). $$\nWe want to find the smallest integer $n^{\\star}(\\epsilon, \\delta)$ such that the probability of the complementary event is at least $1-\\delta$:\n$$ \\Pr\\left( \\sup_{t \\in \\mathbb{R}} |F(t) - \\hat{F}_n(t)| \\le \\epsilon \\right) \\ge 1 - \\delta. $$\nThis is equivalent to ensuring that the probability of the tail event is at most $\\delta$:\n$$ \\Pr\\left( \\sup_{t \\in \\mathbb{R}} |F(t) - \\hat{F}_n(t)| > \\epsilon \\right) \\le \\delta. $$\nBy applying the DKW inequality, we can satisfy this condition by setting its upper bound to be less than or equal to $\\delta$:\n$ 2 \\exp(-2n\\epsilon^2) \\le \\delta $.\nWe now solve this inequality for $n$:\n$$ \\exp(-2n\\epsilon^2) \\le \\frac{\\delta}{2} $$\nTaking the natural logarithm of both sides:\n$$ -2n\\epsilon^2 \\le \\ln\\left(\\frac{\\delta}{2}\\right) $$\nMultiplying by $-1$ and reversing the inequality sign:\n$$ 2n\\epsilon^2 \\ge -\\ln\\left(\\frac{\\delta}{2}\\right) = \\ln\\left(\\left(\\frac{\\delta}{2}\\right)^{-1}\\right) = \\ln\\left(\\frac{2}{\\delta}\\right) $$\nFinally, isolating $n$:\n$$ n \\ge \\frac{1}{2\\epsilon^2} \\ln\\left(\\frac{2}{\\delta}\\right) $$\nSince $n$ must be an integer, the smallest integer $n$ that satisfies this condition is the ceiling of the right-hand side.\n$$ n^{\\star}(\\epsilon,\\delta) = \\left\\lceil \\frac{1}{2\\epsilon^2} \\ln\\left(\\frac{2}{\\delta}\\right) \\right\\rceil. $$\n\n**Task 3: Optimal exponent constant $c$**\nWe are asked for the largest possible constant $c$ such that for some constant $K$, the following bound holds for all distributions $P$, all sufficiently large $n$, and all $\\epsilon \\in (0, \\epsilon_0]$:\n$$ \\sup_{P} \\Pr\\!\\left( \\sup_{h \\in \\mathcal{H}} |P(h) - P_n(h)| > \\epsilon \\right) \\le K \\exp(-cn\\epsilon^2). $$\nThe left-hand side is $\\sup_{P} \\Pr\\!\\left( \\sup_{t \\in \\mathbb{R}} |F(t) - \\hat{F}_n(t)| > \\epsilon \\right)$. The DKW inequality with constant $c=2$ shows that such a bound exists. To prove that $c=2$ is the optimal (largest possible) value, we must establish a matching lower bound on the rate of decay.\n\nWe follow the hint to reduce the problem to the deviation of a binomial proportion. The supremum over all distributions must be greater than or equal to the probability for any single distribution. Furthermore, the supremum over all thresholds $t$ must be greater than or equal to the deviation at any single threshold $t_0$.\n$$ \\sup_{P} \\Pr\\!\\left( \\sup_{t \\in \\mathbb{R}} |F(t) - \\hat{F}_n(t)| > \\epsilon \\right) \\ge \\Pr_{P_0}\\!\\left( |F_{0}(t_0) - \\hat{F}_{n,0}(t_0)| > \\epsilon \\right) $$\nfor any specific distribution $P_0$ and threshold $t_0$.\n\nLet's choose $P_0$ to be the uniform distribution on $[0,1]$, for which the CDF is $F_0(t) = t$ for $t \\in [0,1]$. Let's choose the threshold $t_0 = 1/2$. At this point, $F_0(1/2) = 1/2$. The empirical value is $\\hat{F}_{n,0}(1/2) = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}\\{X_i \\le 1/2\\}$. The sum $S_n = n \\hat{F}_{n,0}(1/2)$ follows a binomial distribution, $S_n \\sim \\mathrm{Binomial}(n, 1/2)$.\n\nThe deviation at $t_0=1/2$ is $|\\hat{F}_{n,0}(1/2) - F_0(1/2)| = |\\frac{S_n}{n} - \\frac{1}{2}|$. The large deviation behavior of this quantity is well-known. A tight lower bound on this tail probability for large $n$ can be established, showing that it decays as $\\exp(-2n\\epsilon^2(1+o(1)))$. More formally, the local central limit theorem or Sanov's theorem implies that for small $\\epsilon$, the decay rate is dominated by the Kullback-Leibler divergence $D_{KL}(1/2+\\epsilon || 1/2)$, which has a Taylor expansion of $2\\epsilon^2 + O(\\epsilon^4)$.\nThis implies that for large $n$, there exists a constant $C'>0$ such that\n$$ \\sup_{P} \\Pr\\!\\left( \\sup_{t} |F(t) - \\hat{F}_n(t)| > \\epsilon \\right) \\ge \\Pr_{U(0,1)}\\!\\left( \\left|\\hat{F}_n(1/2) - 1/2\\right| > \\epsilon \\right) \\approx C' \\exp(-2n\\epsilon^2). $$\nIf we have an upper bound of the form $K \\exp(-cn\\epsilon^2)$, it must accommodate this lower bound. For the inequality $C' \\exp(-2n\\epsilon^2) \\le K \\exp(-cn\\epsilon^2)$ to hold for all large $n$, we must have $-c \\ge -2$, which implies $c \\le 2$.\n\nSince the DKW inequality provides an upper bound with $c=2$, and our analysis of a specific case shows that no $c>2$ is possible, the largest possible value for the constant $c$ is $2$.", "answer": "$$\n\\boxed{\\begin{pmatrix} 1  \\left\\lceil \\frac{1}{2\\epsilon^2} \\ln\\left(\\frac{2}{\\delta}\\right) \\right\\rceil  2 \\end{pmatrix}}\n$$", "id": "3189954"}, {"introduction": "Theoretical guarantees are most instructive when we also understand their breaking points. This final practice [@problem_id:3138523] investigates a scenario where uniform convergence dramatically fails, which can occur when using unbounded loss functions, a common situation in regression. By dissecting this failure, we will see how practical remedies, such as using robust losses like the Huber loss, can restore convergence, thereby revealing a fundamental trade-off between a model's bias and its variance.", "problem": "Consider a supervised learning setting with input-output pairs $(x,y)$ drawn independently and identically distributed from a fixed but unknown distribution $\\mathcal{D}$ over $\\mathcal{X} \\times \\mathbb{R}$. Let the hypothesis class $\\mathcal{H}$ consist of real-valued predictors $h:\\mathcal{X} \\to \\mathbb{R}$. Define the population risk of $h$ under a loss $\\ell$ by $R(h) = \\mathbb{E}_{(x,y)\\sim \\mathcal{D}}[\\ell((x,y),h)]$ and the empirical risk on a sample $S = \\{(x_i,y_i)\\}_{i=1}^n$ by $\\hat{R}_n(h) = \\frac{1}{n}\\sum_{i=1}^n \\ell((x_i,y_i),h)$. Uniform convergence for $\\mathcal{H}$ (under $\\ell$) means that, with high probability over the draw of $S$, the deviation $\\sup_{h\\in\\mathcal{H}}|R(h) - \\hat{R}_n(h)|$ is small.\n\nBuild the following concrete scenario. Let $\\mathcal{X} = \\{x_\\star, x_0\\}$ with $\\mathbb{P}(X = x_\\star) = p \\in (0,1)$ and $\\mathbb{P}(X = x_0) = 1 - p$. Let $Y \\equiv 0$ almost surely. Consider the two-parameter family of hypotheses indexed by $M \\ge 0$,\n$$\n\\mathcal{H} = \\left\\{ h_0, h_M :\\ h_0(x) \\equiv 0,\\quad h_M(x_\\star) = M,\\ h_M(x_0) = 0 \\right\\},\n$$\nand the non-Lipschitz squared loss $\\ell_2((x,y),h) = (h(x) - y)^2$. For tail handling, also consider the truncated squared loss at level $T0$, defined by\n$$\n\\ell_T((x,y),h) = \\min\\left\\{(h(x)-y)^2,\\ T\\right\\},\n$$\nand the Huber loss with parameter $\\delta0$,\n$$\n\\ell_\\delta((x,y),h) = \\rho_\\delta\\big(h(x)-y\\big),\\quad \\rho_\\delta(u) = \n\\begin{cases}\n\\frac{1}{2}u^2,  |u| \\le \\delta,\\\\\n\\delta\\left(|u| - \\frac{1}{2}\\delta\\right),  |u|  \\delta.\n\\end{cases}\n$$\nAssume an Empirical Risk Minimization (ERM) procedure that selects $h \\in \\mathcal{H}$ to minimize empirical risk under a chosen loss.\n\nWhich of the following statements are correct in this scenario and its extensions? Select all that apply.\n\nA. Under the squared loss $\\ell_2$, on the event that a sample of size $n$ contains no occurrence of $x_\\star$, it holds that $\\hat{R}_n(h_M) = 0$ while $R(h_M) = pM^2$. Therefore, for any fixed $n$ and $p$, for every $\\varepsilon0$ there exists $M$ such that $\\sup_{h\\in\\mathcal{H}}|R(h)-\\hat{R}_n(h)|  \\varepsilon$ with probability at least $(1-p)^n$, and uniform convergence over $\\mathcal{H}$ fails.\n\nB. Under the truncated loss $\\ell_T$, every per-sample loss lies in $[0,T]$. Consequently, for the finite class $\\{h_0, h_M\\}$, one can make $\\sup_{h\\in\\mathcal{H}}|R_T(h) - \\hat{R}_{n,T}(h)|$ smaller than any prescribed $\\varepsilon0$ with probability at least $1-\\alpha$ by choosing $n$ large enough, independently of $M$; this recovers uniform convergence.\n\nC. Because the Huber loss $\\ell_\\delta$ is globally Lipschitz in its residual argument, McDiarmid’s bounded differences inequality applies without any additional assumptions and yields uniform convergence bounds that do not depend on the distribution of $Y$; no moment assumptions on $Y$ are needed.\n\nD. Clipping predictions by replacing $h$ with $\\tilde{h}(x) = \\max\\{-B,\\min\\{h(x),B\\}\\}$ for some $B0$ transforms the squared loss into a bounded loss for any $Y$ and preserves the population risk minimizer; thus, clipping introduces no bias.\n\nE. Choosing smaller $T$ in $\\ell_T$ or smaller $\\delta$ in $\\ell_\\delta$ reduces the influence of large residuals and tightens the concentration of empirical around population risk (for suitable tail conditions), but increases bias relative to the true squared-loss risk; this represents a tunable bias-variance trade-off via truncation or Lipschitzization.", "solution": "This problem requires evaluating five statements about uniform convergence in a specific scenario designed to highlight its potential failure and remedies. Let's analyze each statement.\n\n**Analysis of Statement A**\nThe statement considers the squared loss $\\ell_2$ and the hypothesis class $\\mathcal{H} = \\{h_M \\mid M \\ge 0\\}$.\n- The population risk of $h_M$ is $R(h_M) = \\mathbb{E}[(h_M(X))^2] = p \\cdot (h_M(x_\\star))^2 + (1-p) \\cdot (h_M(x_0))^2 = pM^2$.\n- On the event that a sample of size $n$ contains no occurrences of $x_\\star$, the empirical risk is $\\hat{R}_n(h_M) = \\frac{1}{n} \\sum_{i=1}^n (h_M(x_i))^2 = \\frac{1}{n} \\sum_{i=1}^n (0)^2 = 0$.\n- On this event, the deviation for a specific $h_M$ is $|R(h_M) - \\hat{R}_n(h_M)| = pM^2$.\n- The uniform deviation over the class is $\\sup_{h \\in \\mathcal{H}}|R(h) - \\hat{R}_n(h)| = \\sup_{M \\ge 0} pM^2 = \\infty$.\n- The probability of this event (no $x_\\star$ in $n$ i.i.d. draws) is $(1-p)^n$.\nSince there is a non-zero probability (for finite $n$) that the uniform deviation is infinite, it cannot be bounded by any finite $\\varepsilon$ with high probability. Thus, uniform convergence fails. The statement's logic is sound.\n**Verdict: A is correct.**\n\n**Analysis of Statement B**\nThe statement considers the truncated loss $\\ell_T$.\n- By definition, $\\ell_T((x,y),h) = \\min\\{(h(x)-y)^2, T\\}$, which is always in $[0, T]$. The per-sample loss is bounded.\n- The uniform deviation is $\\sup_{h \\in \\mathcal{H}}|R_T(h) - \\hat{R}_{n,T}(h)|$. Because the loss function is bounded in $[0,T]$ for all $h \\in \\mathcal{H}$, classical uniform convergence results for bounded function classes apply. For any class with finite complexity (e.g., finite VC dimension), the uniform deviation converges to zero as $n \\to \\infty$. In this specific case, the class of loss functions induced by $\\mathcal{H}$ is $\\{c \\cdot \\mathbb{I}(x=x_\\star) \\mid c \\in [0, T]\\}$. This class has a VC-subgraph dimension of 1. Because the class has finite complexity and the losses are bounded, uniform convergence is guaranteed. The key insight is that the bound $T$ is independent of $M$, which was the source of the problem in A.\n**Verdict: B is correct.**\n\n**Analysis of Statement C**\nThe statement concerns the Huber loss $\\ell_\\delta$.\n- The Huber loss function $\\rho_\\delta(u)$ is indeed globally Lipschitz in its argument $u$. However, the loss itself, $\\ell_\\delta((x,y),h) = \\rho_\\delta(h(x)-y)$, is an unbounded function of its inputs, specifically $h(x)$ and $y$.\n- McDiarmid's inequality requires the function of the sample to have bounded differences. The function here is $f(S) = \\sup_{h \\in \\mathcal{H}} |R(h) - \\hat{R}_n(h)|$. Changing one sample point $(x_i, y_i)$ changes $\\hat{R}_n(h)$ by at most $\\frac{1}{n} \\sup_{h \\in \\mathcal{H}} \\ell_\\delta((x_i,y_i),h)$. Since our hypothesis class $\\mathcal{H}$ is unbounded (M can be arbitrarily large) and the Huber loss is unbounded, this change is not bounded.\n- Therefore, McDiarmid's inequality does not apply \"without any additional assumptions\". We would need to assume, for example, that the hypothesis class $\\mathcal{H}$ is uniformly bounded or that the output $Y$ has bounded moments, neither of which is given.\n**Verdict: C is incorrect.**\n\n**Analysis of Statement D**\nThe statement proposes clipping the predictions $\\tilde{h}(x) = \\text{clip}(h(x), -B, B)$.\n- Claim 1: \"transforms the squared loss into a bounded loss for any $Y$\". The new loss is $(\\tilde{h}(x)-y)^2$. Since $\\tilde{h}(x) \\in [-B, B]$ but $y$ can be an unbounded real number, the difference $(\\tilde{h}(x)-y)$ is unbounded, and so is its square. This claim is false.\n- Claim 2: \"preserves the population risk minimizer; thus, clipping introduces no bias\". This is also false. Clipping the hypothesis function changes the optimization problem. The minimizer of $\\mathbb{E}[(\\tilde{h}(X)-Y)^2]$ is not, in general, the clipped version of the minimizer of $\\mathbb{E}[(h(X)-Y)^2]$. For example, if the optimal predictor $h^*(x)$ is always greater than $B$, clipping it to $B$ will almost certainly result in a suboptimal predictor for the clipped problem. This introduces bias.\n**Verdict: D is incorrect.**\n\n**Analysis of Statement E**\nThis statement describes the bias-variance trade-off when using $\\ell_T$ or $\\ell_\\delta$ as surrogates for the squared loss $\\ell_2$.\n- \"reduces the influence of large residuals and tightens the concentration\": Smaller $T$ or $\\delta$ makes the loss function \"less sensitive\" to large errors. For $\\ell_T$, the loss is capped at $T$. For $\\ell_\\delta$, the penalty transitions from quadratic to linear earlier, and with a smaller slope $\\delta$. This reduces the range or the Lipschitz constant of the loss class, leading to tighter concentration bounds (lower \"variance\"). This part is correct.\n- \"increases bias relative to the true squared-loss risk\": As $T \\to \\infty$ and $\\delta \\to \\infty$, both $\\ell_T$ and $\\ell_\\delta$ approach the squared loss $\\ell_2$. Using smaller, finite $T$ or $\\delta$ means we are optimizing a different objective. The minimizer of this surrogate objective will generally differ from the true squared-loss minimizer, introducing approximation bias. The smaller $T$ or $\\delta$, the larger this deviation can be. This part is correct.\n- \"represents a tunable bias-variance trade-off\": This is the correct interpretation. The parameters $T$ and $\\delta$ control this trade-off between the bias from using a surrogate loss and the variance (instability) of the empirical minimizer.\n**Verdict: E is correct.**\n\nFinal selection: A, B, and E are the correct statements.", "answer": "$$\\boxed{ABE}$$", "id": "3138523"}]}