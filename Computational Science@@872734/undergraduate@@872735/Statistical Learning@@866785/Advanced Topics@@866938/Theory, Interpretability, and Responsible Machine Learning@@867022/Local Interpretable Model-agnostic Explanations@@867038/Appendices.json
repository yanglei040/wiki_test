{"hands_on_practices": [{"introduction": "Standard LIME explanations are typically generated by fitting a local linear model using weighted least squares, which minimizes an $L_2$ loss. However, this approach can be sensitive to outliers in the sampled neighborhood, potentially skewing the local explanation. This exercise [@problem_id:3140869] challenges you to implement and compare this standard approach with a more robust surrogate model trained using the Huber loss, allowing you to directly investigate how to build more reliable local explanations in the presence of noisy or outlier-contaminated data.", "problem": "You are to implement and compare two local linear surrogate explanations in the sense of Local Interpretable Model-Agnostic Explanations (LIME), one trained with the squared error loss (denoted $L_2$ loss) and the other trained with the Huber loss, in order to examine robustness to outlier perturbations sampled far from a target point $x_0$.\n\nThe core of the task is to fit a locally weighted linear surrogate model to approximate a smooth black-box regression function $f:\\mathbb{R}^p \\to \\mathbb{R}$ near a fixed point $x_0 \\in \\mathbb{R}^p$. The local surrogate uses the original features as interpretable features, and the locality is enforced by a kernel that downweights points as they move away from $x_0$. You must implement both the $L_2$-based surrogate (weighted least squares) and the Huber-loss-based surrogate (weighted robust regression). You will then quantitatively compare the estimated local coefficients against the true gradient of $f$ at $x_0$.\n\nFundamental base:\n- Local surrogates in Local Interpretable Model-Agnostic Explanations (LIME) are defined by minimizing a locality-weighted empirical risk over a linear model. For a dataset $\\{(x_i,y_i)\\}_{i=1}^n$ with locality weights $\\{w_i\\}_{i=1}^n$, the $L_2$ surrogate solves\n$$\n\\min_{\\beta_0,\\beta \\in \\mathbb{R}^p} \\sum_{i=1}^n w_i \\left(y_i - \\beta_0 - \\beta^\\top x_i\\right)^2,\n$$\nand the Huber-loss surrogate solves\n$$\n\\min_{\\beta_0,\\beta \\in \\mathbb{R}^p} \\sum_{i=1}^n w_i \\,\\rho_\\delta\\!\\left(y_i - \\beta_0 - \\beta^\\top x_i\\right),\n$$\nwhere the Huber loss $\\rho_\\delta$ with threshold $\\delta>0$ is defined by\n$$\n\\rho_\\delta(r)=\\begin{cases}\n\\frac{1}{2} r^2, & \\text{if } |r|\\le \\delta,\\\\\n\\delta |r| - \\frac{1}{2}\\delta^2, & \\text{if } |r|>\\delta.\n\\end{cases}\n$$\n- A Gaussian kernel is a standard choice to encode locality. For bandwidth $\\sigma>0$, define\n$$\nw_i = \\exp\\!\\left(-\\frac{\\|x_i-x_0\\|_2^2}{2\\sigma^2}\\right).\n$$\n\nYou will work with a fixed, smooth black-box model $f$ in dimension $p=5$, defined by\n$$\nf(x) \\;=\\; \\tanh\\!\\left(a^\\top x\\right) \\;+\\; \\frac{1}{2}\\left(b^\\top x\\right)^2 \\;+\\; c^\\top x \\;+\\; 0.2\\,\\sin\\!\\left(d^\\top x\\right),\n$$\nwhere $a,b,c,d \\in \\mathbb{R}^5$ are fixed vectors and $x\\in\\mathbb{R}^5$. The exact analytical gradient $\\nabla f(x)$ must be computed and used to evaluate the quality of the surrogate’s slope coefficients at $x_0$.\n\nNeighborhood data must be generated by perturbing $x_0$ as follows. Given a contamination fraction $\\gamma \\in [0,1]$, a far-outlier distance scale $R>0$, local noise standard deviation $s_{\\text{local}}>0$, and far-outlier noise standard deviation $s_{\\text{far}}>0$, construct $n$ perturbations with $n_{\\text{out}}=\\lfloor \\gamma n \\rfloor$ “far” points and $n_{\\text{in}}=n-n_{\\text{out}}$ “near” points:\n- Near points: $x_i = x_0 + \\epsilon_i$ with $\\epsilon_i \\sim \\mathcal{N}(0, s_{\\text{local}}^2 I_p)$, for $i=1,\\dots,n_{\\text{in}}$.\n- Far points: sample unit directions $u_j \\in \\mathbb{S}^{p-1}$ uniformly by normalizing standard Gaussian vectors, then set $x_j = x_0 + R\\,u_j + \\eta_j$ with $\\eta_j \\sim \\mathcal{N}(0, s_{\\text{far}}^2 I_p)$, for $j=1,\\dots,n_{\\text{out}}$.\n\nFor each synthetic dataset, compute $y_i=f(x_i)$ and locality weights $w_i$ using the Gaussian kernel with bandwidth $\\sigma>0$. Fit two local surrogates at $x_0$:\n- $L_2$ surrogate by minimizing the weighted squared error.\n- Huber-loss surrogate by minimizing the weighted Huber objective with a fixed threshold $\\delta>0$.\n\nEvaluation: Let $\\widehat{\\beta}^{(2)}$ denote the slope vector from the $L_2$ surrogate and $\\widehat{\\beta}^{(H)}$ denote the slope vector from the Huber surrogate. Let $g_0=\\nabla f(x_0)$ be the true gradient at $x_0$. Compute the slope error for each surrogate as the Euclidean norm\n$$\nE_2 = \\|\\widehat{\\beta}^{(2)}-g_0\\|_2, \\qquad E_H=\\|\\widehat{\\beta}^{(H)}-g_0\\|_2.\n$$\nReport the improvement of the Huber surrogate over the $L_2$ surrogate as the single float\n$$\n\\Delta \\;=\\; E_2 - E_H.\n$$\nPositive $\\Delta$ indicates that the Huber surrogate is more accurate with respect to the true local slope at $x_0$.\n\nImplementation requirements:\n- Use $p=5$, $n=400$, $s_{\\text{local}}=0.4$, $s_{\\text{far}}=0.1$, $\\delta=1.0$. Use a fixed random seed so results are deterministic.\n- Draw $x_0$, $a$, $b$, $c$, and $d$ from independent standard normal distributions once and keep them fixed across all test cases.\n- Use the Gaussian kernel for weights with given $\\sigma$.\n- Solve the $L_2$ problem by weighted least squares. Solve the Huber problem by a principled method such as Iteratively Reweighted Least Squares (IRLS), correctly integrating the locality weights $\\{w_i\\}$ into the objective. Convergence tolerance and iteration cap can be chosen reasonably, but the solution must be numerically stable.\n\nTest suite:\nRun your program for the following five parameter sets $(\\gamma, R, \\sigma)$:\n1. $(0.0, 5.0, 1.0)$\n2. $(0.2, 5.0, 1.0)$\n3. $(0.6, 10.0, 1.0)$\n4. $(0.2, 5.0, 0.2)$\n5. $(0.2, 5.0, 5.0)$\n\nFor each case, construct the neighborhood dataset as specified, fit both surrogates, compute $E_2$, $E_H$, and return $\\Delta=E_2-E_H$ as a float. To ensure comparability, within a run the underlying randomness apart from the test case parameters must be fixed by the single global seed.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list of the five $\\Delta$ values, rounded to six decimal places, enclosed in square brackets, for example, \"[0.123456,0.000001,-0.010203,0.500000,0.250000]\".", "solution": "The objective is to implement and compare two local linear surrogate models used for interpretable machine learning explanations, specifically within the framework of Local Interpretable Model-Agnostic Explanations (LIME). One surrogate is based on the standard squared error ($L_2$) loss, while the other employs the robust Huber loss. The comparison focuses on the accuracy of the estimated local linear coefficients (slope) relative to the true gradient of a black-box function, particularly in the presence of outliers in the local neighborhood data.\n\n### 1. Problem Formulation\n\nWe are given a smooth, nonlinear black-box function $f:\\mathbb{R}^p \\to \\mathbb{R}$, where the dimension is fixed at $p=5$. The function is defined as:\n$$\nf(x) = \\tanh(a^\\top x) + \\frac{1}{2}\\left(b^\\top x\\right)^2 + c^\\top x + 0.2\\,\\sin(d^\\top x)\n$$\nwhere $a, b, c, d \\in \\mathbb{R}^p$ are fixed parameter vectors. The local behavior of this function at a point of interest $x_0 \\in \\mathbb{R}^p$ is to be approximated by a linear model. The ground truth for the local linear approximation's slope is the gradient of $f$ evaluated at $x_0$, which we denote as $g_0 = \\nabla f(x_0)$. Using the chain rule, a component-wise differentiation yields the analytical gradient:\n$$\n\\nabla f(x) = \\left(1 - \\tanh^2(a^\\top x)\\right)a + \\left(b^\\top x\\right)b + c + 0.2\\cos(d^\\top x)d\n$$\n\nA synthetic dataset $\\{(x_i, y_i)\\}_{i=1}^n$ is generated around $x_0$ to train the local surrogates, where $y_i=f(x_i)$. The dataset of size $n=400$ is contaminated with a fraction $\\gamma$ of outliers. There are $n_{\\text{in}} = n - \\lfloor \\gamma n \\rfloor$ \"near\" points and $n_{\\text{out}} = \\lfloor \\gamma n \\rfloor$ \"far\" points.\n- Near points (inliers): $x_i = x_0 + \\epsilon_i$, with noise $\\epsilon_i \\sim \\mathcal{N}(0, s_{\\text{local}}^2 I_p)$.\n- Far points (outliers): $x_j = x_0 + R\\,u_j + \\eta_j$, where $u_j$ are uniformly sampled from the unit sphere $\\mathbb{S}^{p-1}$, $R$ is a large distance scale, and $\\eta_j \\sim \\mathcal{N}(0, s_{\\text{far}}^2 I_p)$ is a small perturbation.\n\nTo enforce locality, each data point $(x_i, y_i)$ is assigned a weight $w_i$ based on its proximity to $x_0$, determined by a Gaussian kernel with bandwidth $\\sigma$:\n$$\nw_i = \\exp\\!\\left(-\\frac{\\|x_i-x_0\\|_2^2}{2\\sigma^2}\\right)\n$$\n\n### 2. Local Surrogate Models\n\nThe surrogate model is a linear function $g(x) = \\beta_0 + \\beta^\\top x$, where $\\beta_0 \\in \\mathbb{R}$ is the intercept and $\\beta \\in \\mathbb{R}^p$ is the slope vector. The parameters are found by minimizing a weighted loss function. Let $\\tilde{x}_i = [1, x_i^\\top]^\\top$ be the augmented feature vector and $\\tilde{\\beta} = [\\beta_0, \\beta^\\top]^\\top$ be the full coefficient vector.\n\n#### 2.1. $L_2$ Loss Surrogate (Weighted Least Squares)\nThe standard approach minimizes the sum of weighted squared errors:\n$$\n\\min_{\\tilde{\\beta} \\in \\mathbb{R}^{p+1}} \\sum_{i=1}^n w_i \\left(y_i - \\tilde{x}_i^\\top \\tilde{\\beta}\\right)^2\n$$\nThis is a standard Weighted Least Squares (WLS) problem. In matrix form, let $\\tilde{X}$ be the $n \\times (p+1)$ design matrix whose rows are $\\tilde{x}_i^\\top$, $\\mathbf{y}$ be the vector of responses, and $W$ be the $n \\times n$ diagonal matrix of locality weights $w_i$. The objective is to minimize $(\\mathbf{y} - \\tilde{X}\\tilde{\\beta})^\\top W (\\mathbf{y} - \\tilde{X}\\tilde{\\beta})$. The closed-form solution for the estimated coefficients, denoted $\\widehat{\\tilde{\\beta}}^{(2)}$, is:\n$$\n\\widehat{\\tilde{\\beta}}^{(2)} = (\\tilde{X}^\\top W \\tilde{X})^{-1} \\tilde{X}^\\top W \\mathbf{y}\n$$\nThe slope vector for explanation is $\\widehat{\\beta}^{(2)}$, which consists of the last $p$ elements of $\\widehat{\\tilde{\\beta}}^{(2)}$.\n\n#### 2.2. Huber Loss Surrogate (Robust Regression)\nTo improve robustness against outliers, we replace the squared error with the Huber loss $\\rho_\\delta(\\cdot)$ with a threshold $\\delta > 0$:\n$$\n\\rho_\\delta(r) = \\begin{cases}\n\\frac{1}{2} r^2, & \\text{if } |r|\\le \\delta \\\\\n\\delta |r| - \\frac{1}{2}\\delta^2, & \\text{if } |r| > \\delta\n\\end{cases}\n$$\nThe Huber surrogate's coefficients, $\\widehat{\\tilde{\\beta}}^{(H)}$, are found by solving the following convex optimization problem:\n$$\n\\min_{\\tilde{\\beta} \\in \\mathbb{R}^{p+1}} \\sum_{i=1}^n w_i \\rho_\\delta\\left(y_i - \\tilde{x}_i^\\top \\tilde{\\beta}\\right)\n$$\nThis problem does not have a closed-form solution and is solved numerically using Iteratively Reweighted Least Squares (IRLS). The first-order optimality conditions (estimating equations) are $\\sum_{i=1}^n w_i \\psi_\\delta(r_i) \\tilde{x}_i = \\mathbf{0}$, where $r_i = y_i - \\tilde{x}_i^\\top \\tilde{\\beta}$ and $\\psi_\\delta(r) = \\rho'_\\delta(r)$ is the derivative of the Huber loss. These equations can be rewritten as $\\sum_{i=1}^n w_i \\omega_i(r_i) r_i \\tilde{x}_i = \\mathbf{0}$, where $\\omega_i(r_i) = \\psi_\\delta(r_i)/r_i$ are residual-dependent weights.\nThe IRLS algorithm proceeds as follows:\n1.  Initialize coefficients $\\tilde{\\beta}^{(0)}$, for instance, using the $L_2$ solution $\\widehat{\\tilde{\\beta}}^{(2)}$.\n2.  For iteration $k=0, 1, 2, \\dots$ until convergence:\n    a.  Compute residuals: $r_i^{(k)} = y_i - \\tilde{x}_i^\\top \\tilde{\\beta}^{(k)}$.\n    b.  Compute IRLS weights: $\\omega_i^{(k)} = 1$ if $|r_i^{(k)}| \\le \\delta$, and $\\omega_i^{(k)} = \\delta / |r_i^{(k)}|$ if $|r_i^{(k)}| > \\delta$. These weights down-weight points with large residuals.\n    c.  Form a diagonal matrix of total weights $W_{\\text{total}}^{(k)}$ with diagonal entries $w_i \\cdot \\omega_i^{(k)}$, combining locality and residual weights.\n    d.  Update the coefficients by solving the WLS problem with these total weights:\n        $$\n        \\tilde{\\beta}^{(k+1)} = (\\tilde{X}^\\top W_{\\text{total}}^{(k)} \\tilde{X})^{-1} \\tilde{X}^\\top W_{\\text{total}}^{(k)} \\mathbf{y}\n        $$\n3.  The algorithm terminates when the change in coefficients, $\\|\\tilde{\\beta}^{(k+1)} - \\tilde{\\beta}^{(k)}\\|_2$, falls below a small tolerance. The final slope vector is $\\widehat{\\beta}^{(H)}$, comprising the last $p$ elements of the converged $\\tilde{\\beta}$.\n\n### 3. Evaluation\nThe quality of each surrogate is measured by the Euclidean distance between its estimated slope vector and the true gradient $g_0 = \\nabla f(x_0)$:\n$$\nE_2 = \\|\\widehat{\\beta}^{(2)}-g_0\\|_2, \\qquad E_H=\\|\\widehat{\\beta}^{(H)}-g_0\\|_2\n$$\nThe final metric reported is the improvement of the Huber surrogate over the $L_2$ surrogate, defined as the difference in their errors:\n$$\n\\Delta = E_2 - E_H\n$$\nA positive value of $\\Delta$ indicates that the Huber loss surrogate provides a more accurate estimate of the local behavior of $f$ at $x_0$, demonstrating its robustness to the outlier contamination.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares L2-loss and Huber-loss local surrogate models.\n    \"\"\"\n    # --- Problem Constants and Fixed Parameters ---\n    P = 5\n    N = 400\n    S_LOCAL = 0.4\n    S_FAR = 0.1\n    DELTA = 1.0\n    RANDOM_SEED = 42\n\n    # Initialize a random number generator for reproducibility\n    rng = np.random.default_rng(RANDOM_SEED)\n\n    # Generate and fix the model parameters and the point of interest\n    x0 = rng.standard_normal(size=P)\n    a = rng.standard_normal(size=P)\n    b = rng.standard_normal(size=P)\n    c = rng.standard_normal(size=P)\n    d = rng.standard_normal(size=P)\n\n    test_cases = [\n        (0.0, 5.0, 1.0),\n        (0.2, 5.0, 1.0),\n        (0.6, 10.0, 1.0),\n        (0.2, 5.0, 0.2),\n        (0.2, 5.0, 5.0),\n    ]\n\n    # --- Helper Functions ---\n\n    def f_model(x_vec, a_p, b_p, c_p, d_p):\n        \"\"\"The black-box function f(x).\"\"\"\n        return (\n            np.tanh(a_p @ x_vec)\n            + 0.5 * (b_p @ x_vec) ** 2\n            + c_p @ x_vec\n            + 0.2 * np.sin(d_p @ x_vec)\n        )\n\n    def grad_f_model(x_vec, a_p, b_p, c_p, d_p):\n        \"\"\"The analytical gradient of f(x).\"\"\"\n        grad = (\n            (1 - np.tanh(a_p @ x_vec) ** 2) * a_p\n            + (b_p @ x_vec) * b_p\n            + c_p\n            + 0.2 * np.cos(d_p @ x_vec) * d_p\n        )\n        return grad\n\n    def solve_wls(X_tilde, y, weights):\n        \"\"\"Solves a weighted least squares problem.\"\"\"\n        W = np.diag(weights)\n        # Using np.linalg.solve for stability: (X.T @ W @ X) @ beta = X.T @ W @ y\n        lhs = X_tilde.T @ W @ X_tilde\n        rhs = X_tilde.T @ W @ y\n        try:\n            beta_tilde = np.linalg.solve(lhs, rhs)\n        except np.linalg.LinAlgError:\n            # Fallback to pseudoinverse if singular\n            beta_tilde = np.linalg.pinv(lhs) @ rhs\n        return beta_tilde\n\n    def solve_huber_irls(X_tilde, y, locality_weights, delta, tol=1e-7, max_iter=100):\n        \"\"\"Solves a Huber regression problem using IRLS.\"\"\"\n        # Initialize with the standard WLS solution\n        beta_tilde = solve_wls(X_tilde, y, locality_weights)\n\n        for _ in range(max_iter):\n            residuals = y - X_tilde @ beta_tilde\n            abs_residuals = np.abs(residuals)\n            \n            # IRLS weights: handles r=0 case correctly via np.where\n            irls_weights = np.where(abs_residuals <= delta, 1.0, delta / abs_residuals)\n            \n            total_weights = locality_weights * irls_weights\n            \n            beta_tilde_new = solve_wls(X_tilde, y, total_weights)\n            \n            # Check for convergence\n            if np.linalg.norm(beta_tilde_new - beta_tilde) < tol:\n                beta_tilde = beta_tilde_new\n                break\n            \n            beta_tilde = beta_tilde_new\n            \n        return beta_tilde\n\n    results = []\n\n    # --- Main Loop over Test Cases ---\n    for gamma, R, sigma in test_cases:\n        # 1. Generate neighborhood data\n        n_out = int(np.floor(gamma * N))\n        n_in = N - n_out\n\n        # Near points\n        epsilons = rng.normal(scale=S_LOCAL, size=(n_in, P))\n        X_in = x0 + epsilons\n\n        # Far points\n        if n_out > 0:\n            Z = rng.normal(size=(n_out, P))\n            directions = Z / np.linalg.norm(Z, axis=1, keepdims=True)\n            etas = rng.normal(scale=S_FAR, size=(n_out, P))\n            X_out = x0 + R * directions + etas\n            X = np.vstack((X_in, X_out))\n        else:\n            X = X_in\n\n        # Compute responses y = f(x)\n        y = np.array([f_model(x_i, a, b, c, d) for x_i in X])\n\n        # 2. Compute locality weights\n        distances_sq = np.sum((X - x0) ** 2, axis=1)\n        locality_weights = np.exp(-distances_sq / (2 * sigma**2))\n\n        # 3. Prepare matrices for regression\n        X_tilde = np.c_[np.ones(N), X]\n\n        # 4. Solve for L2 surrogate\n        beta_tilde_2 = solve_wls(X_tilde, y, locality_weights)\n        beta_hat_2 = beta_tilde_2[1:]\n\n        # 5. Solve for Huber surrogate\n        beta_tilde_H = solve_huber_irls(X_tilde, y, locality_weights, DELTA)\n        beta_hat_H = beta_tilde_H[1:]\n\n        # 6. Evaluate against the true gradient\n        g0 = grad_f_model(x0, a, b, c, d)\n        \n        e2 = np.linalg.norm(beta_hat_2 - g0)\n        eH = np.linalg.norm(beta_hat_H - g0)\n        \n        delta_error = e2 - eH\n        results.append(delta_error)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{v:.6f}' for v in results)}]\")\n\nsolve()\n```", "id": "3140869"}, {"introduction": "The power of LIME comes from approximating a complex model with a simple, locally-faithful linear one. This approximation, however, relies on the assumption that the black-box function is reasonably smooth in the local neighborhood. This practice problem [@problem_id:3140899] asks you to explore what happens when this core assumption is violated by applying LIME to a function with a sharp discontinuity, providing critical insight into the boundaries of LIME's effectiveness and how to interpret its output in challenging scenarios.", "problem": "Consider a black-box real-valued function $f:\\mathbb{R}^d \\to \\mathbb{R}$ that exhibits a sharp threshold on the first coordinate. Define the function as\n$$\nf(\\mathbf{x}) \\;=\\; J \\cdot \\mathbf{1}\\{x_1 \\ge \\tau\\},\n$$\nwhere $\\mathbf{x} = (x_1,\\dots,x_d)$, $J \\in \\mathbb{R}$ is a fixed jump magnitude, $\\tau \\in \\mathbb{R}$ is the threshold, and $\\mathbf{1}\\{\\cdot\\}$ is the indicator function.\n\nA local surrogate is constructed using the principle of Local Interpretable Model-Agnostic Explanations (LIME). The surrogate is a locally weighted linear regression fitted at a target point $\\mathbf{x}_0 \\in \\mathbb{R}^d$. The weight of a sample $\\mathbf{x}$ is given by an exponential kernel in the Euclidean norm\n$$\nw(\\mathbf{x}) \\;=\\; \\exp\\!\\left(-\\frac{\\|\\mathbf{x} - \\mathbf{x}_0\\|_2^2}{2\\sigma^2}\\right),\n$$\nwith kernel width $\\sigma > 0$. The local linear surrogate has the form\n$$\n\\hat{f}(\\mathbf{x}) \\;=\\; \\beta_0 + \\sum_{j=1}^d \\beta_j x_j,\n$$\nwhere the coefficients are obtained by minimizing the weighted least squares objective\n$$\n\\min_{\\beta_0,\\beta_1,\\dots,\\beta_d} \\sum_{i=1}^N w(\\mathbf{x}^{(i)}) \\left( f(\\mathbf{x}^{(i)}) - \\beta_0 - \\sum_{j=1}^d \\beta_j x^{(i)}_j \\right)^2,\n$$\ngiven a set of $N$ perturbation samples $\\{\\mathbf{x}^{(i)}\\}_{i=1}^N$ drawn independently from a Gaussian centered at $\\mathbf{x}_0$ with isotropic standard deviation $s$ on each coordinate:\n$$\n\\mathbf{x}^{(i)} \\sim \\mathcal{N}\\!\\left(\\mathbf{x}_0, s^2 \\mathbf{I}_d\\right).\n$$\n\nDefine the following measurable notion of whether the local surrogate captures the threshold effect near $\\mathbf{x}_0$:\n- The true local contrast across the kernel scale on the first coordinate is\n$$\nC_{\\text{true}} \\;=\\; \\left|\\, f\\big(\\mathbf{x}_0 + \\sigma \\mathbf{e}_1\\big) - f\\big(\\mathbf{x}_0 - \\sigma \\mathbf{e}_1\\big) \\,\\right|,\n$$\nwhere $\\mathbf{e}_1$ is the first canonical basis vector in $\\mathbb{R}^d$. For the given $f$, this simplifies to $C_{\\text{true}} = J$ if $(x_{0,1} - \\sigma) < \\tau \\le (x_{0,1} + \\sigma)$, and $C_{\\text{true}} = 0$ otherwise.\n- The surrogate-predicted local contrast at the same scale is\n$$\nC_{\\text{pred}} \\;=\\; \\left|\\, \\hat{f}\\big(\\mathbf{x}_0 + \\sigma \\mathbf{e}_1\\big) - \\hat{f}\\big(\\mathbf{x}_0 - \\sigma \\mathbf{e}_1\\big) \\,\\right| \\;=\\; \\left|\\, 2\\sigma \\,\\beta_1 \\,\\right|.\n$$\n\nDeclare that the local surrogate captures the threshold if the absolute error satisfies\n$$\n\\left| C_{\\text{pred}} - C_{\\text{true}} \\right| \\;\\le\\; \\varepsilon \\,\\max\\{1, |J|\\},\n$$\nfor a fixed tolerance $\\varepsilon > 0$.\n\nTask. Write a complete program that:\n1. Implements the function $f$ as specified.\n2. For each test case below, generates $N$ samples $\\mathbf{x}^{(i)}$ from $\\mathcal{N}(\\mathbf{x}_0, s^2 \\mathbf{I}_d)$ with a fixed pseudo-random seed equal to $12345$ for reproducibility.\n3. Computes weights $w(\\mathbf{x}^{(i)})$ using the kernel width $\\sigma$ of the test case.\n4. Fits the locally weighted linear regression to obtain $(\\beta_0,\\dots,\\beta_d)$ using weighted least squares with an intercept term. Any numerically stable method is acceptable, but it must implement the stated weighted objective exactly.\n5. Computes $C_{\\text{true}}$ and $C_{\\text{pred}}$ as defined, and returns a boolean indicating whether the capture criterion holds with $\\varepsilon = 0.35$.\n6. Aggregates the boolean results for all test cases into a single line of output containing the results as a comma-separated list enclosed in square brackets, in the exact format: for example, \"[True,False,True]\".\n\nTest suite. Use the following four test cases, each defined by the tuple $(d,\\tau,J,\\mathbf{x}_0,s,\\sigma,N)$:\n- Case A (near-threshold, small kernel): $(d=\\;2,\\;\\tau=\\;0,\\;J=\\;4,\\;\\mathbf{x}_0=\\;(0.02,\\;0.0),\\;s=\\;0.2,\\;\\sigma=\\;0.08,\\;N=\\;12000)$.\n- Case B (far from threshold): $(d=\\;2,\\;\\tau=\\;0,\\;J=\\;4,\\;\\mathbf{x}_0=\\;(0.5,\\;0.0),\\;s=\\;0.2,\\;\\sigma=\\;0.08,\\;N=\\;8000)$.\n- Case C (exactly at threshold, very wide kernel): $(d=\\;2,\\;\\tau=\\;0,\\;J=\\;4,\\;\\mathbf{x}_0=\\;(0.0,\\;0.0),\\;s=\\;0.05,\\;\\sigma=\\;0.5,\\;N=\\;12000)$.\n- Case D (exactly at threshold, very narrow kernel, higher dimension): $(d=\\;3,\\;\\tau=\\;0,\\;J=\\;4,\\;\\mathbf{x}_0=\\;(0.0,\\;0.1,\\;-0.1),\\;s=\\;0.2,\\;\\sigma=\\;0.02,\\;N=\\;20000)$.\n\nAngle units do not apply. No physical units appear in this problem. Your program should produce a single line of output containing the boolean results as a comma-separated list enclosed in square brackets, in the exact order of the test suite cases above, for example: \"[True,False,True,True]\". No additional text should be printed.", "solution": "### Step 1: Extract Givens\n\nThe problem provides the following definitions and data:\n\n- **Black-box function**: $f:\\mathbb{R}^d \\to \\mathbb{R}$ defined as $f(\\mathbf{x}) = J \\cdot \\mathbf{1}\\{x_1 \\ge \\tau\\}$, where $\\mathbf{x} = (x_1,\\dots,x_d)$, $J \\in \\mathbb{R}$ is a jump magnitude, $\\tau \\in \\mathbb{R}$ is a threshold, and $\\mathbf{1}\\{\\cdot\\}$ is the indicator function.\n- **LIME weighting kernel**: $w(\\mathbf{x}) = \\exp\\!\\left(-\\frac{\\|\\mathbf{x} - \\mathbf{x}_0\\|_2^2}{2\\sigma^2}\\right)$, for a target point $\\mathbf{x}_0$ and kernel width $\\sigma > 0$.\n- **Local surrogate model**: A linear model $\\hat{f}(\\mathbf{x}) = \\beta_0 + \\sum_{j=1}^d \\beta_j x_j$.\n- **Objective function**: The coefficients $(\\beta_0, \\beta_1, \\dots, \\beta_d)$ are determined by minimizing the weighted least squares error:\n$$ \\min_{\\beta_0,\\beta_1,\\dots,\\beta_d} \\sum_{i=1}^N w(\\mathbf{x}^{(i)}) \\left( f(\\mathbf{x}^{(i)}) - \\beta_0 - \\sum_{j=1}^d \\beta_j x^{(i)}_j \\right)^2 $$\n- **Perturbation samples**: A set of $N$ samples $\\{\\mathbf{x}^{(i)}\\}_{i=1}^N$ drawn from $\\mathbf{x}^{(i)} \\sim \\mathcal{N}(\\mathbf{x}_0, s^2 \\mathbf{I}_d)$.\n- **True local contrast**: $C_{\\text{true}} = \\left|\\, f\\big(\\mathbf{x}_0 + \\sigma \\mathbf{e}_1\\big) - f\\big(\\mathbf{x}_0 - \\sigma \\mathbf{e}_1\\big) \\,\\right|$.\n- **Predicted local contrast**: $C_{\\text{pred}} = \\left|\\, \\hat{f}\\big(\\mathbf{x}_0 + \\sigma \\mathbf{e}_1\\big) - \\hat{f}\\big(\\mathbf{x}_0 - \\sigma \\mathbf{e}_1\\big) \\,\\right| = \\left|\\, 2\\sigma \\,\\beta_1 \\,\\right|$.\n- **Capture criterion**: $\\left| C_{\\text{pred}} - C_{\\text{true}} \\right| \\le \\varepsilon \\,\\max\\{1, |J|\\}$, with a fixed tolerance $\\varepsilon = 0.35$.\n- **Reproducibility**: A fixed pseudo-random seed of $12345$ must be used for sample generation.\n- **Test Cases**:\n    - Case A: $(d=2, \\tau=0, J=4, \\mathbf{x}_0=(0.02, 0.0), s=0.2, \\sigma=0.08, N=12000)$\n    - Case B: $(d=2, \\tau=0, J=4, \\mathbf{x}_0=(0.5, 0.0), s=0.2, \\sigma=0.08, N=8000)$\n    - Case C: $(d=2, \\tau=0, J=4, \\mathbf{x}_0=(0.0, 0.0), s=0.05, \\sigma=0.5, N=12000)$\n    - Case D: $(d=3, \\tau=0, J=4, \\mathbf{x}_0=(0.0, 0.1, -0.1), s=0.2, \\sigma=0.02, N=20000)$\n\n### Step 2: Validate Using Extracted Givens\n\n- **Scientifically Grounded**: The problem is well-grounded in the field of statistical learning, specifically concerning local interpretable model-agnostic explanations (LIME). The methods described—locally weighted linear regression, Gaussian kernel weighting, and Gaussian perturbation—are standard techniques. The function $f(\\mathbf{x})$ is a simple, well-defined mathematical function (a step function).\n- **Well-Posed**: The problem is well-posed. The objective is to compute a boolean value based on a series of deterministic calculations, given a fixed set of parameters and a fixed random seed. The weighted least squares problem has a unique, stable solution under the specified conditions (samples drawn from a continuous distribution).\n- **Objective**: The problem is stated in precise, objective mathematical language. All definitions, parameters, and criteria are quantitative and unambiguous.\n- **Completeness and Consistency**: The problem is self-contained. All necessary data and definitions for each test case are provided. There are no internal contradictions.\n\n### Step 3: Verdict and Action\n\nThe problem is valid. It is a well-defined computational task based on established principles in machine learning. A complete solution will be provided.\n\n### Solution\n\nThe task is to determine, for several test cases, whether a LIME-style local linear surrogate can capture the sharp threshold behavior of a given function $f(\\mathbf{x})$. This is assessed by comparing the true change in $f(\\mathbf{x})$ across a local interval with the change predicted by the surrogate model. The following algorithm is implemented for each test case.\n\nFirst, for a given test case with parameters $(d, \\tau, J, \\mathbf{x}_0, s, \\sigma, N)$, we generate $N$ perturbation samples. For reproducibility, the pseudo-random number generator is seeded with the value $12345$. Each sample $\\mathbf{x}^{(i)}$ is drawn from the multivariate normal distribution $\\mathcal{N}(\\mathbf{x}_0, s^2 \\mathbf{I}_d)$, where $\\mathbf{I}_d$ is the $d \\times d$ identity matrix.\n\nNext, we evaluate the black-box function $f(\\mathbf{x}^{(i)}) = J \\cdot \\mathbf{1}\\{x_1^{(i)} \\ge \\tau\\}$ for each sample $\\mathbf{x}^{(i)}$. This yields a vector of responses $\\mathbf{y} \\in \\mathbb{R}^N$, where each element $y_i = f(\\mathbf{x}^{(i)})$.\n\nThe core of the LIME method is to fit a local surrogate model by solving a weighted least squares problem. The weight for each sample $\\mathbf{x}^{(i)}$ is calculated using the exponential kernel $w(\\mathbf{x}^{(i)}) = \\exp\\!\\left(-\\frac{\\|\\mathbf{x}^{(i)} - \\mathbf{x}_0\\|_2^2}{2\\sigma^2}\\right)$. These weights, $w_i = w(\\mathbf{x}^{(i)})$, form the diagonal entries of a weight matrix $\\mathbf{W}$.\n\nThe linear surrogate model is $\\hat{f}(\\mathbf{x}) = \\beta_0 + \\sum_{j=1}^d \\beta_j x_j$. To find the coefficient vector $\\boldsymbol{\\beta} = (\\beta_0, \\beta_1, \\dots, \\beta_d)^T$, we solve the normal equations for weighted least squares:\n$$ \\boldsymbol{\\beta} = (\\mathbf{X}_{\\text{aug}}^T \\mathbf{W} \\mathbf{X}_{\\text{aug}})^{-1} \\mathbf{X}_{\\text{aug}}^T \\mathbf{W} \\mathbf{y} $$\nHere, $\\mathbf{y}$ is the $N \\times 1$ vector of function values $f(\\mathbf{x}^{(i)})$. $\\mathbf{X}_{\\text{aug}}$ is the $N \\times (d+1)$ augmented design matrix, constructed by prepending a column of ones to the $N \\times d$ matrix of samples: $\\mathbf{X}_{\\text{aug}} = [\\mathbf{1}_N, \\mathbf{X}]$. The equation is solved numerically for $\\boldsymbol{\\beta}$ using a stable linear algebra solver, which is more robust than computing the matrix inverse directly.\n\nWith the coefficient vector $\\boldsymbol{\\beta}$ determined, we can calculate the predicted local contrast. The coefficient of interest is $\\beta_1$, which corresponds to the feature $x_1$. The predicted contrast is given by $C_{\\text{pred}} = |2\\sigma \\beta_1|$.\n\nWe then compute the true local contrast, $C_{\\text{true}}$, which is defined as the actual change in the function $f(\\mathbf{x})$ across the interval $[x_{0,1} - \\sigma, x_{0,1} + \\sigma]$ on the first coordinate:\n$$ C_{\\text{true}} = \\left| f(\\mathbf{x}_0 + \\sigma\\mathbf{e}_1) - f(\\mathbf{x}_0 - \\sigma\\mathbf{e}_1) \\right| $$\nwhere $\\mathbf{e}_1$ is the first standard basis vector. This is calculated by evaluating $f$ at the two points $\\mathbf{x}_{\\text{plus}} = \\mathbf{x}_0 + \\sigma\\mathbf{e}_1$ and $\\mathbf{x}_{\\text{minus}} = \\mathbf{x}_0 - \\sigma\\mathbf{e}_1$.\n\nFinally, we apply the capture criterion. The local surrogate is deemed to have captured the threshold effect if the absolute error between the predicted and true contrasts is within a specified tolerance:\n$$ |C_{\\text{pred}} - C_{\\text{true}}| \\le \\varepsilon \\max\\{1, |J|\\} $$\nWith the given value $\\varepsilon = 0.35$, this inequality is evaluated. The resulting boolean value (True or False) is recorded for the test case. This entire process is repeated for all four test cases, and the sequence of boolean results is formatted into the final output.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the LIME surrogate model validation problem for a series of test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (d, tau, J, x_0, s, sigma, N)\n    test_cases = [\n        # Case A: near-threshold, small kernel\n        (2, 0.0, 4.0, (0.02, 0.0), 0.2, 0.08, 12000),\n        # Case B: far from threshold\n        (2, 0.0, 4.0, (0.5, 0.0), 0.2, 0.08, 8000),\n        # Case C: exactly at threshold, very wide kernel\n        (2, 0.0, 4.0, (0.0, 0.0), 0.05, 0.5, 12000),\n        # Case D: exactly at threshold, very narrow kernel, higher dimension\n        (3, 0.0, 4.0, (0.0, 0.1, -0.1), 0.2, 0.02, 20000),\n    ]\n\n    # Fixed tolerance for the capture criterion\n    epsilon = 0.35\n    \n    # Store boolean results for each case\n    results = []\n\n    for case in test_cases:\n        d, tau, J, x_0_tuple, s, sigma, N = case\n        x_0 = np.array(x_0_tuple)\n\n        # Set the pseudo-random seed for reproducibility for each case\n        np.random.seed(12345)\n        \n        # 1. Generate N samples from N(x_0, s^2 * I_d)\n        samples = x_0 + s * np.random.standard_normal(size=(N, d))\n        \n        # 2. Evaluate the black-box function f(x) for all samples\n        # f(x) = J * 1{x_1 >= tau}\n        f_values = J * (samples[:, 0] >= tau).astype(float)\n        \n        # 3. Compute weights w(x) for all samples\n        # w(x) = exp(-||x - x_0||^2 / (2 * sigma^2))\n        sq_dists = np.sum((samples - x_0)**2, axis=1)\n        weights = np.exp(-sq_dists / (2 * sigma**2))\n        \n        # 4. Fit the locally weighted linear regression\n        # Create the augmented design matrix X_aug with an intercept column\n        X_aug = np.hstack([np.ones((N, 1)), samples])\n        Y = f_values\n        \n        # Construct the matrices for the normal equation: (X^T W X) beta = X^T W Y\n        # To do this efficiently, we use broadcasting with the weights vector\n        # instead of creating a large diagonal matrix W.\n        # Let A = X^T W X and b = X^T W Y\n        \n        # A = X_aug.T @ (weights[:, np.newaxis] * X_aug)\n        A = X_aug.T @ (weights.reshape(-1, 1) * X_aug) \n        # b = X_aug.T @ (weights * Y)\n        b = X_aug.T @ (weights * Y)\n        \n        # Solve the linear system A * beta = b for beta\n        try:\n            beta = np.linalg.solve(A, b)\n        except np.linalg.LinAlgError:\n            # Use pseudo-inverse as a fallback for singular or ill-conditioned matrices\n            beta = np.linalg.pinv(A) @ b\n        \n        # The coefficient for the first coordinate, x_1, is beta[1]\n        beta_1 = beta[1]\n        \n        # 5. Compute C_true and C_pred\n        \n        # C_true = | f(x_0 + sigma*e_1) - f(x_0 - sigma*e_1) |\n        f_plus = J if (x_0[0] + sigma) >= tau else 0.0\n        f_minus = J if (x_0[0] - sigma) >= tau else 0.0\n        C_true = np.abs(f_plus - f_minus)\n        \n        # C_pred = | 2 * sigma * beta_1 |\n        C_pred = np.abs(2 * sigma * beta_1)\n        \n        # 6. Apply the capture criterion\n        abs_error = np.abs(C_pred - C_true)\n        tolerance_threshold = epsilon * np.max([1.0, np.abs(J)])\n        \n        is_captured = abs_error <= tolerance_threshold\n        results.append(is_captured)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3140899"}, {"introduction": "While local linear models are excellent for explaining main effects, they cannot, by themselves, reveal how features interact. Understanding these non-additive relationships is often key to unlocking a deeper understanding of a model's behavior. This advanced exercise [@problem_id:3140901] guides you through implementing a hierarchical LIME procedure, where you first model the main effects and then use the remaining unexplained variance to systematically hunt for pairwise feature interactions, demonstrating a powerful extension to the core LIME framework.", "problem": "You are asked to design and implement a hierarchical Local Interpretable Model-Agnostic Explanations (LIME) procedure to detect pairwise feature interactions in black-box predictors at a specific query point. Local Interpretable Model-Agnostic Explanations (LIME) approximates a black-box predictor locally around a query point by sampling perturbed inputs near the point and fitting a simple surrogate model using similarity-based weights. The hierarchical extension proceeds by first fitting a linear surrogate model with only main effects, then augmenting the model with a selected set of pairwise interaction terms based on evidence in the residuals of the first-stage fit. Your implementation must be guided by foundational definitions, not shortcut formulas.\n\nFundamental base:\n- The black-box predictor is a function $f: \\mathbb{R}^d \\to \\mathbb{R}$ that maps a feature vector $\\mathbf{x} \\in \\mathbb{R}^d$ to a real-valued prediction.\n- Locality is modeled by sampling perturbations $\\mathbf{z}_i$ near a query point $\\mathbf{x}_0$ and weighting each sample by a kernel $w_i = K(\\lVert \\mathbf{z}_i - \\mathbf{x}_0 \\rVert)$, where $K$ is a nonnegative, decaying function of distance.\n- A weighted least squares surrogate model minimizes the weighted sum of squared residuals. In the first stage, consider an intercept and main effects only, leading to a linear surrogate that explains $f(\\mathbf{z}_i)$ in terms of the individual features. In the second stage, consider augmenting with selected pairwise interaction terms to capture non-additive structure that remains in residuals.\n- Weighted goodness of fit is quantified by weighted sums of squares and their ratio, forming a weighted coefficient of determination.\n\nTasks:\n1. Implement local sampling around $\\mathbf{x}_0$ by drawing $N$ perturbations $\\mathbf{z}_i \\sim \\mathcal{N}(\\mathbf{x}_0, \\operatorname{diag}(\\mathbf{s}^2))$, where $\\mathbf{s}$ is a vector of per-feature standard deviations. Ensure reproducibility by using a fixed random seed inside your program. The features are dimensionless and bounded within a plausible range; do not require any physical units.\n2. Define a radial kernel for locality as $w_i = \\exp\\!\\left(-\\frac{\\lVert \\mathbf{z}_i - \\mathbf{x}_0 \\rVert_2^2}{2\\sigma^2}\\right)$ for a given bandwidth $\\sigma > 0$.\n3. Stage one (main effects):\n   - Construct a design matrix using an intercept and centered main effects $u_{i,j} = z_{i,j} - x_{0,j}$ for $j = 1,\\dots,d$. Fit a weighted ridge least squares surrogate that minimizes the weighted sum of squared differences between $f(\\mathbf{z}_i)$ and the linear combination of the columns, with regularization on non-intercept coefficients only. Let the regularization strength be $\\lambda > 0$.\n   - Compute the weighted residuals $r_i = f(\\mathbf{z}_i) - \\hat{y}_i$ and the weighted coefficient of determination $R^2_{\\text{lin}}$ derived from the fundamental definition of explained variance relative to a weighted mean.\n4. Stage two (interaction augmentation):\n   - For each pair of distinct features $(p,q)$ with $1 \\le p < q \\le d$, construct the interaction regressor $v_{i,p,q} = u_{i,p} \\cdot u_{i,q}$ (the product of the centered features).\n   - Quantify the evidence for each interaction regressor by computing its weighted correlation with the residuals $r_i$ based on weighted means, weighted covariances, and weighted variances. Select up to $k$ interaction pairs with largest absolute correlation whose absolute correlation exceeds a threshold $\\tau$. If none exceed the threshold, select none.\n   - Augment the design matrix with the selected pairwise interactions and refit the weighted ridge surrogate to obtain a new weighted coefficient of determination $R^2_{\\text{aug}}$.\n5. Interaction detection:\n   - Let $S_{\\text{true}}$ be the set of ground-truth interacting feature pairs for the test case. Let $S_{\\text{sel}}$ be the set of selected interaction pairs by the algorithm. Define two boolean metrics:\n     - Exact-match correctness: $C_{\\text{exact}}$ is true if and only if $S_{\\text{sel}} = S_{\\text{true}}$.\n     - Consistent detection: $C_{\\text{cons}}$ is true if and only if either $S_{\\text{true}}$ is nonempty and $S_{\\text{sel}}$ intersects $S_{\\text{true}}$, or $S_{\\text{true}}$ is empty and $S_{\\text{sel}}$ is empty.\n   - Also compute the improvement in weighted fit $\\Delta R^2 = R^2_{\\text{aug}} - R^2_{\\text{lin}}$ as a float. No percentages are allowed; report $\\Delta R^2$ as a decimal number.\n6. Output specification:\n   - For each test case, your program must output a list $[C_{\\text{exact}}, C_{\\text{cons}}, \\Delta R^2]$.\n   - The final program output must be a single line containing a list of these per-test-case results in order, printed as a comma-separated list enclosed in square brackets. For example, an output with two test cases would look like $[[\\text{True},\\text{True},0.123456],[\\text{False},\\text{True},0.000000]]$. You must format each $\\Delta R^2$ to exactly six digits after the decimal point.\n\nTest suite:\nImplement the procedure for the following four test cases; each test uses $d=3$ features, and the ground-truth interactions are defined as sets of index pairs with zero-based indexing. For each case, define $f$, $\\mathbf{x}_0$, $N$, $\\sigma$, $\\mathbf{s}$, $k$, $\\tau$, and $\\lambda$, and report the specified outputs.\n\n- Case $1$ (happy path with a single interaction):\n  - $f(\\mathbf{x}) = x_1 x_2 + 0.2 x_3 + 0.1 \\sin(\\pi x_1)$.\n  - $\\mathbf{x}_0 = [0.3, -0.5, 0.1]$, $N = 1200$, $\\sigma = 0.7$, $\\mathbf{s} = [0.5, 0.5, 0.5]$, $k = 3$, $\\tau = 0.1$, $\\lambda = 10^{-3}$.\n  - $S_{\\text{true}} = \\{(0,1)\\}$.\n- Case $2$ (no interaction, purely additive):\n  - $f(\\mathbf{x}) = 0.7 x_1 - 0.4 x_2 + 0.5 x_3$.\n  - $\\mathbf{x}_0 = [-0.2, 0.4, -0.1]$, $N = 800$, $\\sigma = 0.6$, $\\mathbf{s} = [0.5, 0.5, 0.5]$, $k = 3$, $\\tau = 0.3$, $\\lambda = 10^{-3}$.\n  - $S_{\\text{true}} = \\varnothing$.\n- Case $3$ (strong main effect plus interaction):\n  - $f(\\mathbf{x}) = 4.0 x_1 + x_1 x_2 + 0.3 \\sin(2 \\pi x_3)$.\n  - $\\mathbf{x}_0 = [0.1, 0.8, 0.0]$, $N = 1000$, $\\sigma = 0.7$, $\\mathbf{s} = [0.5, 0.5, 0.5]$, $k = 3$, $\\tau = 0.1$, $\\lambda = 10^{-3}$.\n  - $S_{\\text{true}} = \\{(0,1)\\}$.\n- Case $4$ (correlated structure with quadratic terms):\n  - $f(\\mathbf{x}) = (x_1 + x_2)^2 - 0.3 x_3$.\n  - $\\mathbf{x}_0 = [0.6, 0.6, -0.3]$, $N = 1200$, $\\sigma = 0.7$, $\\mathbf{s} = [0.5, 0.5, 0.5]$, $k = 3$, $\\tau = 0.15$, $\\lambda = 10^{-3}$.\n  - $S_{\\text{true}} = \\{(0,1)\\}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a list of the form $[C_{\\text{exact}}, C_{\\text{cons}}, \\Delta R^2]$ with $\\Delta R^2$ printed to six decimal places, like $[[\\text{True},\\text{True},0.123456],[\\text{False},\\text{True},0.000000],[\\text{True},\\text{True},0.234567],[\\text{True},\\text{True},0.345678]]$.", "solution": "The problem requires the design and implementation of a hierarchical, two-stage procedure for detecting pairwise feature interactions in a black-box model $f(\\mathbf{x})$ at a specific query point $\\mathbf{x}_0$. The methodology is an extension of Local Interpretable Model-Agnostic Explanations (LIME), where a complex model is approximated locally by a simpler, interpretable surrogate model. The procedure is grounded in the principles of weighted least squares and statistical correlation.\n\nFirst, the local neighborhood of the query point $\\mathbf{x}_0 \\in \\mathbb{R}^d$ is defined by sampling $N$ perturbation points $\\mathbf{z}_i$. These points are drawn from a multivariate normal distribution, $\\mathbf{z}_i \\sim \\mathcal{N}(\\mathbf{x}_0, \\operatorname{diag}(\\mathbf{s}^2))$, where $\\mathbf{s}$ is a vector of per-feature standard deviations. The locality of each sample $\\mathbf{z}_i$ is quantified by a weight $w_i$, computed using a radial kernel function:\n$$w_i = \\exp\\!\\left(-\\frac{\\lVert \\mathbf{z}_i - \\mathbf{x}_0 \\rVert_2^2}{2\\sigma^2}\\right)$$\nHere, $\\sigma$ is the kernel bandwidth, which controls the size of the local neighborhood. A larger $\\sigma$ results in a more global approximation.\n\n**Stage 1: Main Effects Model**\n\nThe first stage aims to capture the linear, additive effects of the features. A surrogate model linear in the centered features $u_{i,j} = z_{i,j} - x_{0,j}$ is fit to the black-box predictions $y_i = f(\\mathbf{z}_i)$. The model takes the form:\n$$\\hat{y}_i = \\beta_0 + \\sum_{j=1}^{d} \\beta_j u_{i,j}$$\nThe coefficients $\\boldsymbol{\\beta} = [\\beta_0, \\beta_1, \\dots, \\beta_d]^T$ are estimated using weighted ridge regression, which minimizes the objective function:\n$$\\mathcal{L}(\\boldsymbol{\\beta}) = \\sum_{i=1}^{N} w_i (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^{d} \\beta_j^2$$\nThe regularization term, with strength $\\lambda > 0$, is applied only to the main effect coefficients ($\\beta_1, \\dots, \\beta_d$), not the intercept $\\beta_0$. This prevents overfitting and stabilizes the solution. In matrix form, let $\\mathbf{X}_{\\text{lin}}$ be the design matrix with columns for the intercept and the $d$ centered features, $\\mathbf{y}$ be the vector of black-box predictions, and $\\mathbf{W}$ be a diagonal matrix of weights $w_i$. The coefficient vector $\\boldsymbol{\\beta}_{\\text{lin}}$ is the solution to the normal equations:\n$$(\\mathbf{X}_{\\text{lin}}^T \\mathbf{W} \\mathbf{X}_{\\text{lin}} + \\mathbf{\\Lambda}) \\boldsymbol{\\beta}_{\\text{lin}} = \\mathbf{X}_{\\text{lin}}^T \\mathbf{W} \\mathbf{y}$$\nwhere $\\mathbf{\\Lambda}$ is a diagonal matrix with $\\lambda$ for the entries corresponding to main effects and $0$ for the intercept.\n\nAfter fitting, we compute the weighted coefficient of determination, $R^2_{\\text{lin}}$, to quantify the goodness of fit. This is defined as:\n$$R^2_{\\text{lin}} = 1 - \\frac{\\text{SS}_{\\text{res}}}{\\text{SS}_{\\text{tot}}} = 1 - \\frac{\\sum_{i=1}^N w_i(y_i - \\hat{y}_{i, \\text{lin}})^2}{\\sum_{i=1}^N w_i(y_i - \\bar{y}_w)^2}$$\nwhere $\\hat{y}_{i, \\text{lin}}$ are the predictions from the stage one model and $\\bar{y}_w = \\frac{\\sum w_i y_i}{\\sum w_i}$ is the weighted mean of the true predictions. The residuals from this stage, $r_i = y_i - \\hat{y}_{i, \\text{lin}}$, represent the portion of the black-box function's behavior not captured by the linear, additive model.\n\n**Stage 2: Interaction Augmentation**\n\nThe second stage probes for pairwise interaction effects by examining the residuals $r_i$. The hypothesis is that if an interaction between features $p$ and $q$ exists, the residuals will be correlated with the interaction term $v_{i,p,q} = u_{i,p} \\cdot u_{i,q}$. The evidence for each potential interaction $(p,q)$, where $1 \\le p < q \\le d$, is quantified by the weighted Pearson correlation coefficient between the residual vector $\\mathbf{r}$ and the interaction regressor vector $\\mathbf{v}_{p,q}$:\n$$\\rho_w(\\mathbf{r}, \\mathbf{v}_{p,q}) = \\frac{\\text{Cov}_w(\\mathbf{r}, \\mathbf{v}_{p,q})}{\\sqrt{\\sigma^2_w(\\mathbf{r})\\sigma^2_w(\\mathbf{v}_{p,q})}}$$\nHere, $\\text{Cov}_w$ and $\\sigma^2_w$ denote the weighted covariance and variance, respectively, calculated using the weights $w_i$.\n\nA set of candidate interactions is formed by selecting up to $k$ pairs with the largest absolute weighted correlation, provided this value exceeds a threshold $\\tau$. If any interactions are selected, they are added as new columns to the design matrix, creating an augmented matrix $\\mathbf{X}_{\\text{aug}}$. The weighted ridge regression is then refit using this augmented matrix, yielding a new set of coefficients and a new weighted coefficient of determination, $R^2_{\\text{aug}}$. If no interactions are selected, $R^2_{\\text{aug}}$ is simply equal to $R^2_{\\text{lin}}$. The improvement in fit due to the inclusion of interaction terms is measured by $\\Delta R^2 = R^2_{\\text{aug}} - R^2_{\\text{lin}}$.\n\nFinally, the set of selected interaction pairs, $S_{\\text{sel}}$, is compared to the ground-truth set, $S_{\\text{true}}$, to evaluate the procedure's accuracy using two Boolean metrics: exact-match correctness ($C_{\\text{exact}}$) and consistent detection ($C_{\\text{cons}}$).\n\nThe entire procedure is implemented deterministically by using a fixed random seed for the initial sampling step, ensuring reproducibility of the results across all test cases.", "answer": "```python\nimport numpy as np\nfrom itertools import combinations\nimport_scipy = False\ntry:\n    from scipy.special import expit\nexcept ImportError:\n    # This is just to satisfy the version requirement in problem description,\n    # but the library is not used in the solution.\n    import_scipy = False\n\n\ndef solve():\n    \"\"\"\n    Main function to run the hierarchical LIME procedure on all test cases\n    and print the formatted results.\n    \"\"\"\n\n    def solve_case(f, x0, d, N, sigma, s, k, tau, lambda_reg, S_true):\n        \"\"\"\n        Solves a single test case of the hierarchical LIME procedure.\n        \"\"\"\n        # Set a fixed seed for reproducibility.\n        rng = np.random.default_rng(seed=0)\n\n        # 1. Local Sampling: Generate N perturbations around x0.\n        x0_np = np.array(x0, dtype=float)\n        s_np = np.array(s, dtype=float)\n        cov_matrix = np.diag(s_np**2)\n        z_samples = rng.multivariate_normal(x0_np, cov_matrix, size=N)\n        y_true = np.array([f(z) for z in z_samples])\n\n        # 2. Locality Kernel: Compute weights for each sample.\n        distances_sq = np.sum((z_samples - x0_np)**2, axis=1)\n        weights = np.exp(-distances_sq / (2 * sigma**2))\n        sum_weights = np.sum(weights)\n        W = np.diag(weights)\n\n        # 3. Stage 1 (Main Effects Model)\n        # Construct design matrix with centered features.\n        u_centered = z_samples - x0_np\n        X_lin = np.hstack([np.ones((N, 1)), u_centered])\n        \n        # Fit weighted ridge regression.\n        reg_matrix_lin = lambda_reg * np.diag([0] + [1] * d)\n        \n        A_lin = X_lin.T @ W @ X_lin + reg_matrix_lin\n        b_lin = X_lin.T @ W @ y_true\n        beta_lin = np.linalg.solve(A_lin, b_lin)\n        \n        y_pred_lin = X_lin @ beta_lin\n        residuals = y_true - y_pred_lin\n        \n        # Compute weighted R-squared.\n        y_mean_w = np.sum(weights * y_true) / sum_weights\n        ss_tot = np.sum(weights * (y_true - y_mean_w)**2)\n        ss_res_lin = np.sum(weights * residuals**2)\n        r_squared_lin = 1 - ss_res_lin / ss_tot if ss_tot > 1e-12 else 0.0\n\n        # 4. Stage 2 (Interaction Augmentation)\n        feature_pairs = list(combinations(range(d), 2))\n        interaction_correlations = []\n        \n        res_mean_w = np.sum(weights * residuals) / sum_weights\n        res_var_w = np.sum(weights * (residuals - res_mean_w)**2) / sum_weights\n        \n        if res_var_w > 1e-12: # Only check for correlations if residuals have variance.\n            for p, q in feature_pairs:\n                v_interaction = u_centered[:, p] * u_centered[:, q]\n                \n                v_mean_w = np.sum(weights * v_interaction) / sum_weights\n                v_var_w = np.sum(weights * (v_interaction - v_mean_w)**2) / sum_weights\n                \n                if v_var_w > 1e-12:\n                    cov_w = np.sum(weights * (residuals - res_mean_w) * (v_interaction - v_mean_w)) / sum_weights\n                    corr = cov_w / np.sqrt(res_var_w * v_var_w)\n                    interaction_correlations.append({'pair': (p, q), 'corr': abs(corr)})\n\n        # Select interactions based on correlation threshold and k.\n        interaction_correlations.sort(key=lambda x: x['corr'], reverse=True)\n        selected_interactions = [ic['pair'] for ic in interaction_correlations if ic['corr'] > tau]\n        S_sel = set(selected_interactions[:k])\n\n        # If interactions are selected, augment the model and refit.\n        if not S_sel:\n            r_squared_aug = r_squared_lin\n        else:\n            sorted_sel = sorted(list(S_sel))\n            interaction_cols = [(u_centered[:, p] * u_centered[:, q])[:, np.newaxis] for p, q in sorted_sel]\n            X_aug = np.hstack([X_lin] + interaction_cols)\n            \n            num_interactions = len(S_sel)\n            reg_matrix_aug = lambda_reg * np.diag([0] + [1] * d + [1] * num_interactions)\n            \n            A_aug = X_aug.T @ W @ X_aug + reg_matrix_aug\n            b_aug = X_aug.T @ W @ y_true\n            beta_aug = np.linalg.solve(A_aug, b_aug)\n            \n            y_pred_aug = X_aug @ beta_aug\n            ss_res_aug = np.sum(weights * (y_true - y_pred_aug)**2)\n            r_squared_aug = 1 - ss_res_aug / ss_tot if ss_tot > 1e-12 else 0.0\n\n        # 5. Calculate final metrics.\n        delta_r2 = r_squared_aug - r_squared_lin\n        \n        C_exact = (S_sel == S_true)\n        \n        if not S_true:  # S_true is empty\n            C_cons = (not S_sel)\n        else:  # S_true is not empty\n            C_cons = bool(S_sel.intersection(S_true))\n\n        return C_exact, C_cons, delta_r2\n\n    test_cases = [\n        {\n            \"f\": lambda x: x[0] * x[1] + 0.2 * x[2] + 0.1 * np.sin(np.pi * x[0]),\n            \"x0\": [0.3, -0.5, 0.1], \"d\": 3, \"N\": 1200, \"sigma\": 0.7, \"s\": [0.5, 0.5, 0.5],\n            \"k\": 3, \"tau\": 0.1, \"lambda_reg\": 1e-3, \"S_true\": {(0, 1)}\n        },\n        {\n            \"f\": lambda x: 0.7 * x[0] - 0.4 * x[1] + 0.5 * x[2],\n            \"x0\": [-0.2, 0.4, -0.1], \"d\": 3, \"N\": 800, \"sigma\": 0.6, \"s\": [0.5, 0.5, 0.5],\n            \"k\": 3, \"tau\": 0.3, \"lambda_reg\": 1e-3, \"S_true\": set()\n        },\n        {\n            \"f\": lambda x: 4.0 * x[0] + x[0] * x[1] + 0.3 * np.sin(2 * np.pi * x[2]),\n            \"x0\": [0.1, 0.8, 0.0], \"d\": 3, \"N\": 1000, \"sigma\": 0.7, \"s\": [0.5, 0.5, 0.5],\n            \"k\": 3, \"tau\": 0.1, \"lambda_reg\": 1e-3, \"S_true\": {(0, 1)}\n        },\n        {\n            \"f\": lambda x: (x[0] + x[1])**2 - 0.3 * x[2],\n            \"x0\": [0.6, 0.6, -0.3], \"d\": 3, \"N\": 1200, \"sigma\": 0.7, \"s\": [0.5, 0.5, 0.5],\n            \"k\": 3, \"tau\": 0.15, \"lambda_reg\": 1e-3, \"S_true\": {(0, 1)}\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = solve_case(**case)\n        results.append(result)\n\n    # 6. Format the output as specified.\n    inner_results_str = []\n    for res in results:\n        c_exact, c_cons, delta_r2 = res\n        inner_str = f\"[{str(c_exact)},{str(c_cons)},{delta_r2:.6f}]\"\n        inner_results_str.append(inner_str)\n    \n    final_output = f\"[{','.join(inner_results_str)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3140901"}]}