{"hands_on_practices": [{"introduction": "To begin our practical exploration of the VC dimension, we start with a classifier that is simple to visualize but offers a profound insight. Consider a hypothesis class of radial thresholds, which classifies points based on whether their distance from the origin is less than some radius $r$. This exercise [@problem_id:3192514] challenges the intuition that models in high-dimensional spaces are inherently complex, forcing us to discover how a limited number of parameters—in this case, just one—constrains the model's expressive power, regardless of the ambient dimension $d$.", "problem": "Let $\\mathcal{H}_{d}$ be the hypothesis class of binary classifiers on $\\mathbb{R}^{d}$ defined by radial thresholds centered at the origin:\n$$\n\\mathcal{H}_{d} \\;=\\; \\left\\{\\, h_{r}(x) \\;=\\; \\mathbb{I}\\left\\{ \\|x\\|_{2} \\leq r \\right\\} \\;:\\; r \\in [0,\\infty) \\,\\right\\}.\n$$\nUsing only core definitions from statistical learning theory, in particular the definition of the Vapnik–Chervonenkis (VC) dimension, determine the VC dimension $d_{\\mathrm{VC}}(\\mathcal{H}_{d})$ and explain whether it depends on the ambient dimension $d$. Your final answer must be a single integer.", "solution": "The problem is valid as it is scientifically grounded in statistical learning theory, well-posed, objective, and contains all necessary information for a solution.\n\nThe problem asks for the Vapnik–Chervonenkis (VC) dimension of the hypothesis class $\\mathcal{H}_{d}$ of binary classifiers on $\\mathbb{R}^{d}$ defined as:\n$$\n\\mathcal{H}_{d} \\;=\\; \\left\\{\\, h_{r}(x) \\;=\\; \\mathbb{I}\\left\\{ \\|x\\|_{2} \\leq r \\right\\} \\;:\\; r \\in [0,\\infty) \\,\\right\\}\n$$\nwhere $\\mathbb{I}\\{\\cdot\\}$ is the indicator function. A hypothesis $h_r \\in \\mathcal{H}_d$ classifies a point $x$ as $1$ if its Euclidean norm $\\|x\\|_2$ is less than or equal to the radius $r$, and $0$ otherwise. The set of hypotheses is parameterized by the non-negative radius $r$.\n\nThe VC dimension of a hypothesis class $\\mathcal{H}$, denoted $d_{\\mathrm{VC}}(\\mathcal{H})$, is the size of the largest set of points that can be shattered by $\\mathcal{H}$. A set of points $S = \\{x_1, x_2, \\ldots, x_k\\}$ is said to be shattered by $\\mathcal{H}$ if for every possible labeling of the points in $S$ with $\\{0, 1\\}$, there exists a hypothesis $h \\in \\mathcal{H}$ that realizes that labeling. There are $2^k$ such distinct labelings.\n\nTo determine $d_{\\mathrm{VC}}(\\mathcal{H}_{d})$, we must find the maximum integer $k$ for which a set of $k$ points can be shattered. This involves two steps:\n1.  Prove that $d_{\\mathrm{VC}}(\\mathcal{H}_{d}) \\geq k$ by constructing a set of $k$ points that can be shattered.\n2.  Prove that $d_{\\mathrm{VC}}(\\mathcal{H}_{d})  k+1$ by showing that no set of $k+1$ points can be shattered.\n\nLet us begin by testing small values of $k$.\n\n**Step 1: Show that $d_{\\mathrm{VC}}(\\mathcal{H}_{d}) \\geq 1$.**\n\nTo show that the VC dimension is at least $1$, we must find a set of size $1$ that can be shattered. Let $S = \\{x_1\\}$ be a set containing a single point in $\\mathbb{R}^d$ such that $x_1 \\neq 0$. Let its norm be $\\|x_1\\|_2 = \\delta$, where $\\delta  0$.\n\nThere are $2^1=2$ possible labelings for $x_1$: $\\{0\\}$ and $\\{1\\}$.\n- **To obtain the labeling $\\{1\\}$:** We need a hypothesis $h_r \\in \\mathcal{H}_d$ such that $h_r(x_1) = 1$. This is equivalent to finding an $r$ such that $\\mathbb{I}\\{\\|x_1\\|_2 \\le r\\} = 1$, which means $\\|x_1\\|_2 \\le r$. We can choose $r = \\delta$. This is a valid choice since $\\delta \\ge 0$, so $h_{\\delta} \\in \\mathcal{H}_d$.\n- **To obtain the labeling $\\{0\\}$:** We need a hypothesis $h_r \\in \\mathcal{H}_d$ such that $h_r(x_1) = 0$. This is equivalent to finding an $r$ such that $\\mathbb{I}\\{\\|x_1\\|_2 \\le r\\} = 0$, which means $\\|x_1\\|_2  r$. We can choose $r = \\frac{\\delta}{2}$. Since $\\delta0$, we have $r \\ge 0$, so $h_{\\delta/2} \\in \\mathcal{H}_d$.\n\nSince both possible labelings can be generated, the set $S = \\{x_1\\}$ is shattered by $\\mathcal{H}_{d}$. Therefore, we conclude that $d_{\\mathrm{VC}}(\\mathcal{H}_{d}) \\geq 1$.\n\n**Step 2: Show that $d_{\\mathrm{VC}}(\\mathcal{H}_{d})  2$.**\n\nTo show that the VC dimension is less than $2$, we must prove that no set of $2$ points can be shattered by $\\mathcal{H}_{d}$. Let $S = \\{x_1, x_2\\}$ be an arbitrary set of two distinct points in $\\mathbb{R}^d$. Let their respective L2-norms be $d_1 = \\|x_1\\|_2$ and $d_2 = \\|x_2\\|_2$. We must show that at least one of the $2^2 = 4$ possible labelings for the pair $(x_1, x_2)$ cannot be produced by any hypothesis in $\\mathcal{H}_d$. The four labelings are $(0,0), (0,1), (1,0), (1,1)$.\n\nWe consider two cases based on the norms of the points.\n\n**Case A: $d_1 = d_2$.**\nThe points $x_1$ and $x_2$ lie on the same hypersphere of radius $d_1$ centered at the origin. For any hypothesis $h_r \\in \\mathcal{H}_d$, the classification of $x_1$ is $h_r(x_1) = \\mathbb{I}\\{d_1 \\le r\\}$ and the classification of $x_2$ is $h_r(x_2) = \\mathbb{I}\\{d_2 \\le r\\}$. Since $d_1=d_2$, the conditions are identical, so $h_r(x_1) = h_r(x_2)$ for any choice of $r$.\n- If we choose $r  d_1$, both points are labeled $0$, yielding the labeling $(0,0)$.\n- If we choose $r \\ge d_1$, both points are labeled $1$, yielding the labeling $(1,1)$.\nIt is impossible to generate the labelings $(0,1)$ and $(1,0)$. As not all $4$ labelings can be produced, the set $S$ is not shattered.\n\n**Case B: $d_1 \\neq d_2$.**\nWithout loss of generality, let us assume $d_1  d_2$. (The case $d_2  d_1$ is symmetric). Now let us try to generate the labeling $(0,1)$, i.e., $h_r(x_1)=0$ and $h_r(x_2)=1$.\n- The condition $h_r(x_1)=0$ requires $\\|x_1\\|_2  r$, which means $d_1  r$.\n- The condition $h_r(x_2)=1$ requires $\\|x_2\\|_2 \\le r$, which means $d_2 \\le r$.\nCombining these two inequalities gives $d_1  r \\ge d_2$. This implies $d_1  d_2$, which contradicts our assumption that $d_1  d_2$. Therefore, the labeling $(0,1)$ cannot be generated by any hypothesis in $\\mathcal{H}_d$. As not all $4$ labelings can be produced, the set $S$ is not shattered.\n\nSince we have shown that for any arbitrary set of two points, the set cannot be shattered, we conclude that $d_{\\mathrm{VC}}(\\mathcal{H}_{d})  2$.\n\n**Conclusion:**\n\nFrom Step 1, we have $d_{\\mathrm{VC}}(\\mathcal{H}_{d}) \\geq 1$. From Step 2, we have $d_{\\mathrm{VC}}(\\mathcal{H}_{d})  2$. Since the VC dimension must be an integer, the only possible value is $1$.\n\n$$d_{\\mathrm{VC}}(\\mathcal{H}_{d}) = 1$$\n\nThe question also asks whether the VC dimension depends on the ambient dimension $d$. Our derivation relied only on the magnitudes of the L2-norms of the points, which are non-negative real numbers. The logic of comparing these distances to a threshold $r$ is independent of the dimension $d$ in which the points reside (as long as $d \\ge 1$ so that we can find a non-zero point). The impossibility of shattering two points is a consequence of the one-dimensional ordering of their norms, irrespective of their geometric configuration in $\\mathbb{R}^d$. Therefore, the VC dimension $d_{\\mathrm{VC}}(\\mathcal{H}_{d})$ does not depend on the ambient dimension $d$.", "answer": "$$\\boxed{1}$$", "id": "3192514"}, {"introduction": "Having seen how a single parameter leads to a constant VC dimension, we now investigate a fundamental class of models where complexity scales with the environment. This practice focuses on halfspaces in $\\mathbb{R}^d$ that are constrained to pass through a fixed point, which are equivalent to homogeneous linear classifiers. This exercise [@problem_id:3192452] is crucial for deriving the canonical result that the VC dimension of this class is precisely the dimensionality of the space, $d$, establishing a direct link between the geometry of the feature space and the capacity of linear models.", "problem": "Consider the instance space $\\mathcal{X} = \\mathbb{R}^{d} \\setminus \\{p\\}$, where $d \\in \\mathbb{N}$ and $p \\in \\mathbb{R}^{d}$ is a fixed point. Define the hypothesis class $\\mathcal{H}_{p}$ consisting of all halfspaces whose boundary hyperplane passes through $p$ and that classify points by the sign of the linear functional relative to $p$:\n$$\n\\mathcal{H}_{p} = \\left\\{ h_{w} : \\mathcal{X} \\to \\{-1,+1\\} \\,\\middle|\\, h_{w}(x) = \\operatorname{sign}\\!\\big(w^{\\top}(x - p)\\big),\\, w \\in \\mathbb{R}^{d} \\right\\}.\n$$\nHere $\\operatorname{sign}(t)$ denotes $+1$ if $t \\ge 0$ and $-1$ otherwise. Using only the core definitions of shattering and Vapnik–Chervonenkis (VC) dimension, determine the VC dimension $d_{\\mathrm{VC}}(\\mathcal{H}_{p})$ as a function of $d$. Your answer must be a single closed-form analytic expression in $d$. No rounding is required.", "solution": "We begin from the fundamental definition of Vapnik–Chervonenkis (VC) dimension. For a hypothesis class $\\mathcal{H}$ on instance space $\\mathcal{X}$, a finite set $S \\subset \\mathcal{X}$ is said to be shattered by $\\mathcal{H}$ if, for every labeling $\\ell : S \\to \\{-1,+1\\}$, there exists $h \\in \\mathcal{H}$ such that $h(x) = \\ell(x)$ for all $x \\in S$. The VC dimension $d_{\\mathrm{VC}}(\\mathcal{H})$ is the maximum cardinality of a set $S$ shattered by $\\mathcal{H}$.\n\nWe must compute $d_{\\mathrm{VC}}(\\mathcal{H}_{p})$ for\n$$\n\\mathcal{H}_{p} = \\left\\{ h_{w}(x) = \\operatorname{sign}\\!\\big(w^{\\top}(x - p)\\big) : w \\in \\mathbb{R}^{d} \\right\\}.\n$$\nObserve that for each $w \\in \\mathbb{R}^{d}$, the decision boundary $\\{x \\in \\mathbb{R}^{d} : w^{\\top}(x - p) = 0\\}$ is a hyperplane passing through $p$. By the translation $z = x - p$, the class $\\mathcal{H}_{p}$ is equivalent to the class of homogeneous linear separators in $\\mathbb{R}^{d}$ acting on $z$ via $h_{w}(z) = \\operatorname{sign}(w^{\\top} z)$.\n\nWe show a matching lower and upper bound that establishes $d_{\\mathrm{VC}}(\\mathcal{H}_{p}) = d$.\n\nLower bound ($d_{\\mathrm{VC}}(\\mathcal{H}_{p}) \\ge d$). Choose the set of $d$ points\n$$\nS = \\{x_{1}, x_{2}, \\dots, x_{d}\\} \\subset \\mathcal{X}\n$$\nwith\n$$\nx_{i} = p + e_{i}, \\quad i = 1,2,\\dots,d,\n$$\nwhere $e_{i}$ are the standard basis vectors of $\\mathbb{R}^{d}$. Consider any labeling $\\ell : S \\to \\{-1,+1\\}$, and let $y \\in \\{-1,+1\\}^{d}$ be the vector with components $y_{i} = \\ell(x_{i})$. Define\n$$\nw = y \\in \\mathbb{R}^{d}.\n$$\nThen for each $i$,\n$$\nw^{\\top}(x_{i} - p) = y^{\\top} e_{i} = y_{i},\n$$\nso $h_{w}(x_{i}) = \\operatorname{sign}(y_{i}) = y_{i} = \\ell(x_{i})$. Because this holds for every labeling $\\ell$ of $S$, the set $S$ is shattered by $\\mathcal{H}_{p}$. Therefore, $d_{\\mathrm{VC}}(\\mathcal{H}_{p}) \\ge d$.\n\nUpper bound ($d_{\\mathrm{VC}}(\\mathcal{H}_{p}) \\le d$). Suppose, for contradiction, that there exists a set $T = \\{x_{1}, x_{2}, \\dots, x_{d+1}\\} \\subset \\mathcal{X}$ of $d+1$ points that is shattered by $\\mathcal{H}_{p}$. Consider the vectors $v_{i} = x_{i} - p \\in \\mathbb{R}^{d}$. Since we have $d+1$ vectors in a $d$-dimensional space, the set $\\{v_{1}, \\dots, v_{d+1}\\}$ is linearly dependent. Thus there exist coefficients $a_{1}, \\dots, a_{d+1}$, not all zero, such that\n$$\n\\sum_{i=1}^{d+1} a_{i} v_{i} = 0.\n$$\nBecause the coefficients are not all zero and the sum is the zero vector, there must be at least one index with $a_{i}  0$ and at least one index with $a_{j}  0$. Define a labeling $\\ell : T \\to \\{-1,+1\\}$ by\n$$\n\\ell(x_{i}) = \\operatorname{sign}(a_{i}).\n$$\nIf $T$ were shattered, there would exist $w \\in \\mathbb{R}^{d}$ such that\n$$\n\\operatorname{sign}\\!\\big(w^{\\top}(x_{i} - p)\\big) = \\operatorname{sign}(a_{i}) \\quad \\text{for all } i.\n$$\nUnder this labeling, for every $i$ with $a_{i} \\ne 0$ we have $a_{i} w^{\\top}(x_{i} - p) \\ge 0$, and because there are both positive and negative $a_{i}$, and none of the corresponding inner products are zero under the realized signs, it follows that\n$$\n\\sum_{i=1}^{d+1} a_{i} w^{\\top}(x_{i} - p)  0.\n$$\nHowever, by linearity,\n$$\n\\sum_{i=1}^{d+1} a_{i} w^{\\top}(x_{i} - p) = w^{\\top} \\sum_{i=1}^{d+1} a_{i} (x_{i} - p) = w^{\\top} \\cdot 0 = 0,\n$$\na contradiction. Therefore, no set of size $d+1$ can be shattered by $\\mathcal{H}_{p}$, proving $d_{\\mathrm{VC}}(\\mathcal{H}_{p}) \\le d$.\n\nCombining the lower and upper bounds yields\n$$\nd_{\\mathrm{VC}}(\\mathcal{H}_{p}) = d.\n$$\nThis is a closed-form expression in $d$, as required.", "answer": "$$\\boxed{d}$$", "id": "3192452"}, {"introduction": "Our final practice delves into the practical consequences of model design choices on learning capacity. We will compare the unrestricted class of perceptrons, which define general halfspaces, with a more constrained, 'biologically plausible' version where all weights are non-negative. This exercise [@problem_id:3192435] powerfully illustrates how imposing constraints on a hypothesis class can serve as a form of regularization, reducing its VC dimension and, consequently, its risk of overfitting. By comparing the capacities of these two related classes, you will gain a deeper appreciation for the trade-off between model expressiveness and generalization.", "problem": "Consider the hypothesis class of perceptrons on $\\mathbb{R}^{d}$ defined by linear threshold functions. A perceptron assigns a label according to $h_{\\mathbf{w},b}(\\mathbf{x}) = \\mathrm{sign}(\\mathbf{w}^{\\top}\\mathbf{x} + b)$, where $\\mathbf{w} \\in \\mathbb{R}^{d}$ is the weight vector and $b \\in \\mathbb{R}$ is the bias. In biologically plausible models with only excitatory synapses, weights are constrained to be nonnegative so that $\\mathbf{w} \\in \\mathbb{R}_{+}^{d} := \\{\\mathbf{w} \\in \\mathbb{R}^{d} : w_{i} \\geq 0 \\text{ for all } i\\}$. Let $\\mathcal{H}_{+} := \\{h_{\\mathbf{w},b} : \\mathbf{w} \\in \\mathbb{R}_{+}^{d},\\ b \\in \\mathbb{R}\\}$ denote this excitatory-only class, and let $\\mathcal{H} := \\{h_{\\mathbf{w},b} : \\mathbf{w} \\in \\mathbb{R}^{d},\\ b \\in \\mathbb{R}\\}$ denote the unrestricted class of halfspaces.\n\nStarting from the core definitions of Vapnik–Chervonenkis (VC) dimension and linear separability, compute the VC dimension $d_{\\mathrm{VC}}(\\mathcal{H}_{+})$ of excitatory-only perceptrons on $\\mathbb{R}^{d}$ and compare it to the VC dimension $d_{\\mathrm{VC}}(\\mathcal{H})$ of unrestricted halfspaces on $\\mathbb{R}^{d}$. Justify your answer from first principles, using only standard facts about affine independence and monotonicity induced by nonnegative weights. Express your final answer as a two-entry row vector giving $d_{\\mathrm{VC}}(\\mathcal{H}_{+})$ and $d_{\\mathrm{VC}}(\\mathcal{H})$ in terms of $d$. No rounding is required and no physical units are involved.", "solution": "The problem asks for the Vapnik-Chervonenkis (VC) dimension of two hypothesis classes of perceptrons on $\\mathbb{R}^d$. The first is the unrestricted class $\\mathcal{H}$, and the second is the excitatory-only class $\\mathcal{H}_+$, where the weight vector $\\mathbf{w}$ is constrained to have non-negative components.\n\n**Step 1: Preliminaries and Definition of VC Dimension**\n\nThe Vapnik-Chervonenkis (VC) dimension of a hypothesis class $\\mathcal{C}$ of binary classifiers is the size of the largest finite set of points that can be shattered by $\\mathcal{C}$. A set of points $S = \\{\\mathbf{x}_1, \\dots, \\mathbf{x}_N\\}$ is said to be shattered by $\\mathcal{C}$ if for every possible labeling of the points in $S$, represented by a vector $\\boldsymbol{\\ell} = (\\ell_1, \\dots, \\ell_N) \\in \\{-1, +1\\}^N$, there exists a classifier $h \\in \\mathcal{C}$ such that $h(\\mathbf{x}_i) = \\ell_i$ for all $i = 1, \\dots, N$. This means $\\mathcal{C}$ can realize all $2^N$ possible dichotomies of the set $S$.\n\nThe classifiers are of the form $h_{\\mathbf{w},b}(\\mathbf{x}) = \\mathrm{sign}(\\mathbf{w}^{\\top}\\mathbf{x} + b)$. A labeling $\\{\\ell_i\\}$ is realized if there exist parameters $(\\mathbf{w}, b)$ specified by the hypothesis class such that $\\ell_i (\\mathbf{w}^{\\top}\\mathbf{x}_i + b)  0$ for all $i$. For the sign function, we define $\\mathrm{sign}(z)=+1$ if $z \\geq 0$ and $\\mathrm{sign}(z)=-1$ if $z  0$. The strict inequality is used to avoid issues with points lying on the boundary.\n\n**Step 2: VC Dimension of Unrestricted Halfspaces, $d_{\\mathrm{VC}}(\\mathcal{H})$**\n\nThe hypothesis class $\\mathcal{H} = \\{h_{\\mathbf{w},b} : \\mathbf{w} \\in \\mathbb{R}^{d},\\ b \\in \\mathbb{R}\\}$ corresponds to the set of all separating hyperplanes in $\\mathbb{R}^d$. It is a standard result in statistical learning theory that the VC dimension of this class is $d+1$. We demonstrate this by proving a lower and an upper bound.\n\n*   **Lower Bound: $d_{\\mathrm{VC}}(\\mathcal{H}) \\geq d+1$**\n\n    To show that the VC dimension is at least $d+1$, we must find a set of $d+1$ points that can be shattered by $\\mathcal{H}$. Consider the set of $d+1$ points in $\\mathbb{R}^d$ consisting of the origin and the standard basis vectors: $S = \\{\\mathbf{0}, \\mathbf{e}_1, \\dots, \\mathbf{e}_d\\}$.\n\n    For any arbitrary labeling $(\\ell_0, \\ell_1, \\dots, \\ell_d) \\in \\{-1, +1\\}^{d+1}$, we need to find $\\mathbf{w} \\in \\mathbb{R}^d$ and $b \\in \\mathbb{R}$ such that:\n    1.  $\\mathrm{sign}(\\mathbf{w}^{\\top}\\mathbf{0} + b) = \\mathrm{sign}(b) = \\ell_0$\n    2.  $\\mathrm{sign}(\\mathbf{w}^{\\top}\\mathbf{e}_i + b) = \\mathrm{sign}(w_i + b) = \\ell_i$ for $i=1, \\dots, d$.\n\n    We can construct $(\\mathbf{w}, b)$ as follows. Set $b = \\ell_0$.\n    Then, for $i=1, \\dots, d$, we need $\\mathrm{sign}(w_i + \\ell_0) = \\ell_i$.\n    - If $\\ell_i = \\ell_0$, we need $w_i + \\ell_0$ to have the same sign as $\\ell_0$. We can choose $w_i = 0$. Then $w_i + \\ell_0 = \\ell_0$, which has sign $\\ell_0$.\n    - If $\\ell_i = -\\ell_0$, we need $w_i + \\ell_0$ to have the opposite sign of $\\ell_0$. We can choose $w_i = -2\\ell_0$. Then $w_i + \\ell_0 = -2\\ell_0 + \\ell_0 = -\\ell_0$, which has sign $-\\ell_0 = \\ell_i$.\n\n    Since we can find a $(\\mathbf{w}, b)$ for any of the $2^{d+1}$ possible labelings, the set $S$ is shattered. This proves that $d_{\\mathrm{VC}}(\\mathcal{H}) \\geq d+1$.\n\n*   **Upper Bound: $d_{\\mathrm{VC}}(\\mathcal{H}) \\leq d+1$**\n\n    To show that the VC dimension is at most $d+1$, we must show that no set of $d+2$ points can be shattered. This is a direct consequence of Radon's Theorem.\n    Let $S = \\{\\mathbf{x}_1, \\dots, \\mathbf{x}_{d+2}\\}$ be any set of $d+2$ points in $\\mathbb{R}^d$. By Radon's Theorem, $S$ can be partitioned into two disjoint subsets, $S_1$ and $S_2$, such that their convex hulls intersect: $\\mathrm{conv}(S_1) \\cap \\mathrm{conv}(S_2) \\neq \\emptyset$.\n\n    Consider the labeling that assigns $+1$ to all points in $S_1$ and $-1$ to all points in $S_2$. For this labeling to be realized by a hyperplane $h_{\\mathbf{w},b}$, we would need $\\mathbf{w}^\\top\\mathbf{x} + b  0$ for all $\\mathbf{x} \\in S_1$ and $\\mathbf{w}^\\top\\mathbf{x} + b  0$ for all $\\mathbf{x} \\in S_2$. By linearity, this implies the hyperplane separates the convex hulls of $S_1$ and $S_2$. However, since their convex hulls intersect, they cannot be separated by a hyperplane. Therefore, this specific labeling cannot be realized.\n\n    Since for any set of $d+2$ points there exists at least one labeling that cannot be realized, no set of $d+2$ points can be shattered. This proves $d_{\\mathrm{VC}}(\\mathcal{H}) \\leq d+1$.\n\nCombining the bounds, we conclude that $d_{\\mathrm{VC}}(\\mathcal{H}) = d+1$.\n\n**Step 3: VC Dimension of Excitatory-Only Perceptrons, $d_{\\mathrm{VC}}(\\mathcal{H}_{+})$**\n\nThe hypothesis class $\\mathcal{H}_{+} = \\{h_{\\mathbf{w},b} : \\mathbf{w} \\in \\mathbb{R}_{+}^{d}, b \\in \\mathbb{R}\\}$ consists of hyperplanes whose normal vectors $\\mathbf{w}$ have non-negative components ($w_i \\geq 0$ for all $i$). This constraint introduces a monotonicity property. If $\\mathbf{x} \\succeq \\mathbf{y}$ (i.e., $x_i \\geq y_i$ for all $i$), then for any $\\mathbf{w} \\in \\mathbb{R}_+^d$, we have $\\mathbf{w}^\\top\\mathbf{x} \\geq \\mathbf{w}^\\top\\mathbf{y}$. This implies $\\mathbf{w}^\\top\\mathbf{x}+b \\geq \\mathbf{w}^\\top\\mathbf{y}+b$. Thus, if $h(\\mathbf{y})=+1$, it is impossible to have $h(\\mathbf{x})=-1$.\n\n*   **Lower Bound: $d_{\\mathrm{VC}}(\\mathcal{H}_{+}) \\geq d$**\n\n    We must find a set of $d$ points that can be shattered by $\\mathcal{H}_{+}$. Consider the set of standard basis vectors $S = \\{\\mathbf{e}_1, \\dots, \\mathbf{e}_d\\}$. This set is an antichain, meaning no element dominates another, so the monotonicity property does not trivially prevent shattering.\n\n    For any arbitrary labeling $(\\ell_1, \\dots, \\ell_d) \\in \\{-1, +1\\}^d$, we need to find $\\mathbf{w} \\in \\mathbb{R}_+^d$ and $b \\in \\mathbb{R}$ such that $\\mathrm{sign}(w_i + b) = \\ell_i$ for all $i$.\n    - Set $b = -1/2$.\n    - For each $i \\in \\{1, \\dots, d\\}$:\n        - If $\\ell_i = +1$, we need $w_i - 1/2 \\geq 0$, so $w_i \\geq 1/2$. We can choose $w_i = 1$. This satisfies $w_i \\geq 0$.\n        - If $\\ell_i = -1$, we need $w_i - 1/2  0$, so $w_i  1/2$. We can choose $w_i = 0$. This satisfies $w_i \\geq 0$.\n    This construction provides a valid $(\\mathbf{w}, b)$ for any labeling. Thus, the set $S$ can be shattered, which proves $d_{\\mathrm{VC}}(\\mathcal{H}_{+}) \\geq d$.\n\n*   **Upper Bound: $d_{\\mathrm{VC}}(\\mathcal{H}_{+}) \\leq d$**\n\n    We show that no set of $d+1$ points can be shattered. Let $S = \\{\\mathbf{x}_1, \\dots, \\mathbf{x}_{d+1}\\}$ be an arbitrary set of $d+1$ points in $\\mathbb{R}^d$.\n\n    First, we can translate the entire coordinate system without changing the shatterability of the set by $\\mathcal{H}_{+}$. Let $\\mathbf{p} \\in \\mathbb{R}^d$ be a vector where each component $p_j$ is the minimum of the $j$-th coordinates of all points in $S$: $p_j = \\min_{i=1, \\dots, d+1} x_{ij}$. Let's define a new set of points $\\mathbf{y}_i = \\mathbf{x}_i - \\mathbf{p}$. A classifier $h_{\\mathbf{w},b}(\\mathbf{x})$ on the original points is equivalent to a classifier $h_{\\mathbf{w},b'}(\\mathbf{y})$ on the new points, where $b' = b + \\mathbf{w}^\\top\\mathbf{p}$. Since $\\mathbf{w}$ and $\\mathbf{p}$ are fixed for a given classifier, $b'$ is just another bias term. The constraint $\\mathbf{w} \\in \\mathbb{R}_+^d$ is unaffected. The new points $\\{\\mathbf{y}_i\\}$ have the property that $y_{ij} \\geq 0$ for all $i,j$.\n\n    The set $\\{\\mathbf{y}_1, \\dots, \\mathbf{y}_{d+1}\\}$ consists of $d+1$ vectors in $\\mathbb{R}^d$. Therefore, they must be linearly dependent. There exist scalars $\\alpha_1, \\dots, \\alpha_{d+1}$, not all zero, such that:\n    $$ \\sum_{i=1}^{d+1} \\alpha_i \\mathbf{y}_i = \\mathbf{0} $$\n    Let $P = \\{i : \\alpha_i  0\\}$ and $N = \\{i : \\alpha_i  0\\}$. At least one of these sets is non-empty. If either is empty, the corresponding labeling (all $+1$ or all $-1$) is trivially realizable by setting $\\mathbf{w}=\\mathbf{0}$ and $b=1$ or $b=-1$. However, we can always find a linear dependence such that both $P$ and $N$ are non-empty, unless all points lie on a single ray from the origin, a degenerate case we can ignore.\n\n    Consider the labeling given by $\\ell_i = \\mathrm{sgn}(\\alpha_i)$ for $i \\in P \\cup N$. Suppose, for the sake of contradiction, that this labeling can be realized by some $h_{\\mathbf{w},b} \\in \\mathcal{H}_{+}$, with $\\mathbf{w} \\in \\mathbb{R}_+^d$. Then we have:\n    - $\\mathbf{w}^\\top\\mathbf{y}_i + b  0$ for $i \\in P$\n    - $\\mathbf{w}^\\top\\mathbf{y}_i + b  0$ for $i \\in N$\n\n    For any $i \\in N$, we have $\\mathbf{w}^\\top\\mathbf{y}_i + b  0$. Since $\\mathbf{w} \\in \\mathbb{R}_+^d$ and all components of $\\mathbf{y}_i$ are non-negative, $\\mathbf{w}^\\top\\mathbf{y}_i \\geq 0$. This forces the bias term to be negative: $b  0$.\n\n    Now, multiply each inequality by the corresponding $\\alpha_i$ and sum over all $i$:\n    $$ \\sum_{i=1}^{d+1} \\alpha_i (\\mathbf{w}^\\top\\mathbf{y}_i + b)  0 $$\n    The reason for the strict inequality is that for each $i \\in P \\cup N$, $\\alpha_i$ has the same sign as $(\\mathbf{w}^\\top\\mathbf{y}_i + b)$, so their product is positive. The sum of positive terms is positive. Distributing the sum:\n    $$ \\sum_{i=1}^{d+1} \\alpha_i (\\mathbf{w}^\\top\\mathbf{y}_i) + \\sum_{i=1}^{d+1} \\alpha_i b  0 $$\n    $$ \\mathbf{w}^\\top \\left(\\sum_{i=1}^{d+1} \\alpha_i \\mathbf{y}_i\\right) + b \\left(\\sum_{i=1}^{d+1} \\alpha_i\\right)  0 $$\n    By the linear dependence, the first term is $\\mathbf{w}^\\top\\mathbf{0} = 0$. We are left with:\n    $$ b \\left(\\sum_{i=1}^{d+1} \\alpha_i\\right)  0 $$\n    It is a standard result from convex geometry that for a set of points like $\\{\\mathbf{y}_i\\}$, constructed by this translation, a linear dependence can be chosen such that $\\sum \\alpha_i \\geq 0$. Assuming this, we have two conditions:\n    1.  $b  0$ (derived from considering points labeled $-1$)\n    2.  $b (\\sum \\alpha_i)  0$ where $\\sum \\alpha_i \\geq 0$.\n\n    If $\\sum \\alpha_i  0$, these two conditions imply $b0$ and $b0$, a contradiction. If $\\sum \\alpha_i = 0$, the second condition becomes $0  0$, also a contradiction.\n    Therefore, the labeling based on the signs of the $\\alpha_i$ coefficients cannot be realized. This shows that no set of $d+1$ points can be shattered by $\\mathcal{H}_+$. This proves $d_{\\mathrm{VC}}(\\mathcal{H}_{+}) \\leq d$.\n\nCombining the bounds, we conclude that $d_{\\mathrm{VC}}(\\mathcal{H}_{+}) = d$.\n\n**Step 4: Comparison and Final Answer**\n\nWe have found that the VC dimension of unrestricted halfspaces in $\\mathbb{R}^d$ is $d_{\\mathrm{VC}}(\\mathcal{H}) = d+1$. The VC dimension of excitatory-only perceptrons (non-negative weights) in $\\mathbb{R}^d$ is $d_{\\mathrm{VC}}(\\mathcal{H}_{+}) = d$.\n\nThe constraint of non-negative weights reduces the complexity of the hypothesis class, decreasing its VC dimension by exactly 1.\n\nThe final answer is a two-entry row vector giving $[d_{\\mathrm{VC}}(\\mathcal{H}_{+}), d_{\\mathrm{VC}}(\\mathcal{H})]$.\nThis is $[d, d+1]$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\nd  d+1\n\\end{pmatrix}\n}\n$$", "id": "3192435"}]}