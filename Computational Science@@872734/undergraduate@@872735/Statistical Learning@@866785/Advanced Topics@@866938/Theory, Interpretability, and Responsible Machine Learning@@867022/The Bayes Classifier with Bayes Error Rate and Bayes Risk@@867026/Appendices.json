{"hands_on_practices": [{"introduction": "To build a solid foundation, our first practice starts from first principles. We will calculate the Bayes risk for a classification task involving discrete count data, a common scenario in fields like natural language processing where we count word occurrences. By modeling the counts with two distinct distributions—the Poisson and the Negative Binomial—this exercise [@problem_id:3180205] will guide you through the process of deriving the exact Bayes risk as the expected value of the minimum error probability, reinforcing the core concepts for discrete feature spaces.", "problem": "Consider a document classification setting where the feature is a single nonnegative integer count $X \\in \\{0,1,2,\\ldots\\}$ representing the number of occurrences of a particular word in a message. The class label $Y \\in \\{1,2\\}$ denotes two categories (for example, two types of messages), with equal class priors $\\mathbb{P}(Y=1) = \\mathbb{P}(Y=2) = \\frac{1}{2}$.\n\nConditioned on the class label, the word count is modeled by two distinct class-conditional distributions:\n- If $Y=1$ then $X \\mid Y=1 \\sim \\operatorname{Poisson}(\\lambda)$ with parameter $\\lambda > 0$, that is, $\\mathbb{P}(X=x \\mid Y=1) = \\frac{\\exp(-\\lambda)\\lambda^{x}}{x!}$ for $x=0,1,2,\\ldots$.\n- If $Y=2$ then $X \\mid Y=2 \\sim \\operatorname{Negative\\ Binomial}(r,q)$ with parameters $r \\in \\mathbb{N}$ and $q \\in (0,1)$, representing the number of failures $x$ before $r$ successes with success probability $q$. Its probability mass function is $\\mathbb{P}(X=x \\mid Y=2) = \\binom{r+x-1}{x} q^{r}(1-q)^{x}$ for $x=0,1,2,\\ldots$.\n\nThe decision-maker uses the Bayes classifier under the zero-one loss (0-1 loss), which classifies to the label that minimizes the probability of error at each observation. Starting from first principles, derive the Bayes risk (the minimum expected misclassification probability under the given generative model and zero-one loss) as an exact analytic expression in terms of $\\lambda$, $r$, and $q$. Your final answer must be a single closed-form expression; it may involve an infinite series. Do not approximate or round; provide the exact result.", "solution": "The problem has been validated and is a well-posed, scientifically grounded question in statistical learning theory.\n\nThe objective is to derive the Bayes risk for a binary classification problem. The Bayes risk, denoted $R^*$, is the minimum achievable expected loss under a given generative model. For the zero-one loss function, $\\mathcal{L}(y, \\hat{y}) = \\mathbb{I}(y \\neq \\hat{y})$, the Bayes risk corresponds to the minimum probability of misclassification.\n\nThe feature is a discrete random variable $X \\in \\{0, 1, 2, \\ldots\\}$ and the class label is $Y \\in \\{1, 2\\}$.\nThe class priors are given as equal: $\\pi_1 = \\mathbb{P}(Y=1) = \\frac{1}{2}$ and $\\pi_2 = \\mathbb{P}(Y=2) = \\frac{1}{2}$.\n\nThe class-conditional probability mass functions (PMFs) are:\nFor class $Y=1$, $X$ follows a Poisson distribution with parameter $\\lambda > 0$:\n$$p_1(x) = \\mathbb{P}(X=x \\mid Y=1) = \\frac{\\exp(-\\lambda)\\lambda^{x}}{x!}, \\quad x \\in \\{0, 1, 2, \\ldots\\}$$\nFor class $Y=2$, $X$ follows a Negative Binomial distribution with parameters $r \\in \\mathbb{N}$ and $q \\in (0,1)$:\n$$p_2(x) = \\mathbb{P}(X=x \\mid Y=2) = \\binom{r+x-1}{x} q^{r}(1-q)^{x}, \\quad x \\in \\{0, 1, 2, \\ldots\\}$$\n\nThe Bayes classifier, $\\hat{y}^*(x)$, is the decision rule that minimizes the posterior expected loss for each observation $x$. For the zero-one loss, this is equivalent to choosing the class with the maximum posterior probability:\n$$\\hat{y}^*(x) = \\arg\\max_{k \\in \\{1,2\\}} \\mathbb{P}(Y=k \\mid X=x)$$\nThe posterior probability is given by Bayes' theorem:\n$$\\mathbb{P}(Y=k \\mid X=x) = \\frac{\\mathbb{P}(X=x \\mid Y=k) \\mathbb{P}(Y=k)}{\\mathbb{P}(X=x)} = \\frac{p_k(x) \\pi_k}{\\sum_{j=1}^2 p_j(x) \\pi_j}$$\nSince the denominator is positive and common to all classes, and the priors $\\pi_1$ and $\\pi_2$ are equal, the decision rule simplifies to comparing the class-conditional likelihoods:\n$$\\hat{y}^*(x) = \\arg\\max_{k \\in \\{1,2\\}} p_k(x)$$\nTherefore, the Bayes classifier predicts class $1$ if $p_1(x) > p_2(x)$, and class $2$ if $p_2(x) > p_1(x)$. If $p_1(x) = p_2(x)$, the choice of class is arbitrary and does not affect the minimum error rate.\n\nThe probability of error for a given observation $x$, when using the Bayes classifier, is the probability of the less likely class:\n$$\\mathbb{P}(\\text{error} \\mid X=x) = 1 - \\max_{k \\in \\{1,2\\}} \\mathbb{P}(Y=k \\mid X=x) = \\min_{k \\in \\{1,2\\}} \\mathbb{P}(Y=k \\mid X=x)$$\nThe Bayes risk $R^*$ is the expected value of this pointwise error probability, averaged over all possible values of $X$:\n$$R^* = \\mathbb{E}_X[\\mathbb{P}(\\text{error} \\mid X)] = \\sum_{x=0}^{\\infty} \\mathbb{P}(X=x) \\mathbb{P}(\\text{error} \\mid X=x)$$\nSubstituting the expression for the pointwise error:\n$$R^* = \\sum_{x=0}^{\\infty} \\mathbb{P}(X=x) \\min_{k \\in \\{1,2\\}} \\mathbb{P}(Y=k \\mid X=x)$$\nNow, we substitute the posterior probabilities:\n$$R^* = \\sum_{x=0}^{\\infty} \\mathbb{P}(X=x) \\min\\left( \\frac{p_1(x) \\pi_1}{\\mathbb{P}(X=x)}, \\frac{p_2(x) \\pi_2}{\\mathbb{P}(X=x)} \\right)$$\nThe term $\\mathbb{P}(X=x)$ can be factored out of the minimum operator:\n$$R^* = \\sum_{x=0}^{\\infty} \\mathbb{P}(X=x) \\frac{1}{\\mathbb{P}(X=x)} \\min(p_1(x) \\pi_1, p_2(x) \\pi_2)$$\nThis simplifies to a sum over the minimum of the joint probabilities $\\mathbb{P}(X=x, Y=k) = p_k(x)\\pi_k$:\n$$R^* = \\sum_{x=0}^{\\infty} \\min(p_1(x) \\pi_1, p_2(x) \\pi_2)$$\nGiven the priors $\\pi_1 = \\pi_2 = \\frac{1}{2}$, we can factor out the constant $\\frac{1}{2}$:\n$$R^* = \\sum_{x=0}^{\\infty} \\min\\left(p_1(x) \\frac{1}{2}, p_2(x) \\frac{1}{2}\\right) = \\frac{1}{2} \\sum_{x=0}^{\\infty} \\min(p_1(x), p_2(x))$$\nFinally, we substitute the specific PMFs for $p_1(x)$ and $p_2(x)$ into this expression. This provides the exact analytic expression for the Bayes risk as an infinite series:\n$$R^* = \\frac{1}{2} \\sum_{x=0}^{\\infty} \\min\\left( \\frac{\\exp(-\\lambda)\\lambda^{x}}{x!}, \\binom{r+x-1}{x} q^{r}(1-q)^{x} \\right)$$\nThis expression is the final answer, as it is an exact analytic form in terms of the given parameters $\\lambda$, $r$, and $q$, and the problem allows for the answer to be an infinite series.", "answer": "$$\n\\boxed{\\frac{1}{2} \\sum_{x=0}^{\\infty} \\min\\left( \\frac{\\exp(-\\lambda)\\lambda^{x}}{x!}, \\binom{r+x-1}{x} q^{r}(1-q)^{x} \\right)}\n$$", "id": "3180205"}, {"introduction": "Moving from discrete counts to continuous measurements, this practice explores features that lie within a fixed range, such as percentages or scores. We will use the flexible Beta distribution to model our class-conditional densities on the interval $[0, 1]$. The main goal of this exercise [@problem_id:3180189] is not only to compute the exact Bayes error rate through numerical integration but also to contrast it with important theoretical upper bounds. This comparison is a vital skill, as it helps us understand the limits of classifier performance even before building a model.", "problem": "You are to implement a program that computes the Bayes error rate for binary classification on the unit interval using Beta distributions as class-conditional densities, and compares it to two bounds. Work from first principles of Bayesian decision theory with $0$-$1$ loss. All quantities are to be expressed in pure mathematical terms without any physical units.\n\nDefinitions to use:\n- Two classes $C_1$ and $C_2$ have prior probabilities $\\pi_1$ and $\\pi_2$ with $\\pi_1 + \\pi_2 = 1$, and class-conditional densities $f_1(x)$ and $f_2(x)$ supported on $[0,1]$.\n- The Bayes classifier chooses the class with the larger posterior probability at each $x \\in [0,1]$.\n- Under $0$-$1$ loss, the Bayes risk equals the Bayes error rate $L^\\star$, which is the minimum possible misclassification probability over all classifiers. It equals the integral\n$$\nL^\\star \\;=\\; \\int_0^1 \\min\\!\\big(\\pi_1 f_1(x), \\pi_2 f_2(x)\\big)\\,dx.\n$$\n- The Bhattacharyya coefficient is\n$$\n\\rho \\;=\\; \\int_0^1 \\sqrt{f_1(x) f_2(x)}\\,dx,\n$$\nand the Bhattacharyya bound gives\n$$\nL^\\star \\;\\le\\; \\sqrt{\\pi_1 \\pi_2}\\;\\rho.\n$$\n- A trivial upper bound is $L^\\star \\le \\min(\\pi_1,\\pi_2)$, corresponding to the constant classifier that always predicts the majority class.\n\nIn this problem, the class-conditional densities are Beta distributions on $[0,1]$ with respective shape parameters $(\\alpha_1,\\beta_1)$ and $(\\alpha_2,\\beta_2)$:\n$$\nf_k(x) \\;=\\; \\frac{x^{\\alpha_k - 1} (1-x)^{\\beta_k - 1}}{B(\\alpha_k,\\beta_k)},\\quad x\\in[0,1],\\quad \\alpha_k>0,\\;\\beta_k>0,\\quad k\\in\\{1,2\\},\n$$\nwhere $B(\\cdot,\\cdot)$ is the Beta function. You must compute $L^\\star$ by numerical integration over $[0,1]$. You must also compute the Bhattacharyya coefficient $\\rho$ by numerical integration and then report the Bhattacharyya upper bound $\\sqrt{\\pi_1\\pi_2}\\,\\rho$. Finally, compute the trivial upper bound $\\min(\\pi_1,\\pi_2)$.\n\nImplementation requirements:\n- Implement numerical integration on $[0,1]$ for the integrands $\\min(\\pi_1 f_1(x), \\pi_2 f_2(x))$ and $\\sqrt{f_1(x) f_2(x)}$ with sufficient accuracy to produce stable results to at least $6$ decimal places.\n- For each test case, output a list of three floating-point numbers (rounded to $6$ decimal places): $[L^\\star, \\sqrt{\\pi_1\\pi_2}\\,\\rho, \\min(\\pi_1,\\pi_2)]$.\n\nTest suite:\nCompute the outputs for the following four parameter sets, each given as $(\\pi_1,\\alpha_1,\\beta_1,\\pi_2,\\alpha_2,\\beta_2)$.\n- Case $1$ (balanced priors, mirror shapes): $(0.5, 2, 5, 0.5, 5, 2)$.\n- Case $2$ (unequal priors, moderate overlap): $(0.7, 2, 2, 0.3, 5, 5)$.\n- Case $3$ (identical distributions, unequal priors; boundary case): $(0.6, 3, 4, 0.4, 3, 4)$. In this case $L^\\star$ should equal $\\min(0.6,0.4)$ when computed exactly.\n- Case $4$ (balanced priors, highly concentrated near endpoints): $(0.5, 0.5, 5, 0.5, 5, 0.5)$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of lists, each inner list corresponding to one test case in the order above. For example, the exact formatting should be:\n- $[[L^\\star_1, U_{B,1}, U_{T,1}], [L^\\star_2, U_{B,2}, U_{T,2}], [L^\\star_3, U_{B,3}, U_{T,3}], [L^\\star_4, U_{B,4}, U_{T,4}]]$\nwhere $U_{B,i} = \\sqrt{\\pi_{1,i}\\pi_{2,i}}\\,\\rho_i$ and $U_{T,i} = \\min(\\pi_{1,i},\\pi_{2,i})$, with each floating-point value rounded to $6$ decimal places and no additional text printed.", "solution": "The user-provided problem is assessed as valid, as it is scientifically grounded in Bayesian decision theory, well-posed, and contains a complete and consistent set of definitions and parameters. The task is to compute the Bayes error rate and two of its upper bounds for a binary classification problem on the unit interval, where class-conditional densities are modeled by Beta distributions. The solution will be obtained through numerical integration.\n\nThe problem involves two classes, $C_1$ and $C_2$, with prior probabilities $\\pi_1$ and $\\pi_2$ respectively, where $\\pi_1 + \\pi_2 = 1$. The class-conditional probability density functions, $f_1(x)$ and $f_2(x)$, are defined on the interval $x \\in [0,1]$.\n\nAccording to Bayesian decision theory, for a given observation $x$, the posterior probability for class $C_k$ is given by Bayes' theorem:\n$$\nP(C_k|x) = \\frac{\\pi_k f_k(x)}{f(x)} = \\frac{\\pi_k f_k(x)}{\\pi_1 f_1(x) + \\pi_2 f_2(x)}\n$$\nThe Bayes classifier minimizes the probability of misclassification by assigning $x$ to the class with the highest posterior probability. This is equivalent to choosing the class $C_k$ for which the quantity $\\pi_k f_k(x)$ is maximized.\n\nThe minimum probability of error, known as the Bayes error rate $L^\\star$, is achieved by the Bayes classifier. For a binary classification problem under a $0$-$1$ loss function, the Bayes risk is equal to the Bayes error rate. The error at a point $x$ is the posterior probability of the minority class, $\\min(P(C_1|x), P(C_2|x))$. Integrating this error over all possible values of $x$ gives the total Bayes error rate:\n$$\nL^\\star = \\int_0^1 \\min(P(C_1|x), P(C_2|x)) f(x) \\,dx = \\int_0^1 \\min(\\pi_1 f_1(x), \\pi_2 f_2(x)) \\,dx\n$$\nThis is the first quantity to be computed.\n\nThe problem requires comparison with two upper bounds. The first is the Bhattacharyya bound. It is based on the Bhattacharyya coefficient, $\\rho$, which measures the overlap between the two probability distributions:\n$$\n\\rho = \\int_0^1 \\sqrt{f_1(x) f_2(x)} \\,dx\n$$\nThe Bhattacharyya bound on the Bayes error is given by:\n$$\nL^\\star \\le \\sqrt{\\pi_1 \\pi_2} \\, \\rho\n$$\nThis is the second quantity to be computed.\n\nThe second, simpler upper bound is the trivial bound, which is the error rate of a classifier that ignores the data $x$ and always predicts the majority class. The error rate of such a classifier is the prior probability of the minority class:\n$$\nL^\\star \\le \\min(\\pi_1, \\pi_2)\n$$\nThis is the third quantity to be computed.\n\nThe class-conditional densities are specified as Beta distributions with shape parameters $(\\alpha_k, \\beta_k)$:\n$$\nf_k(x) = \\frac{x^{\\alpha_k - 1} (1-x)^{\\beta_k - 1}}{B(\\alpha_k, \\beta_k)}, \\quad x \\in [0,1], \\quad \\alpha_k > 0, \\beta_k > 0\n$$\nwhere $B(\\alpha_k, \\beta_k)$ is the Beta function, which serves as the normalization constant ensuring that $\\int_0^1 f_k(x)\\,dx = 1$.\n\nThe computation of $L^\\star$ and $\\rho$ requires evaluating one-dimensional integrals whose integrands are composed of Beta probability density functions. Since these integrals do not generally have closed-form analytical solutions, numerical integration is necessary. For this purpose, a standard numerical quadrature method, such as the one provided by `scipy.integrate.quad`, is suitable. This function can handle the integrable singularities that may arise at the endpoints of the interval $[0,1]$ when the Beta distribution parameters $\\alpha_k$ or $\\beta_k$ are less than $1$.\n\nFor each test case, specified by the parameter set $(\\pi_1, \\alpha_1, \\beta_1, \\pi_2, \\alpha_2, \\beta_2)$, the procedure is as follows:\n1.  Define the two Beta PDF functions, $f_1(x)$ and $f_2(x)$, using the given parameters.\n2.  Numerically integrate the function $g(x) = \\min(\\pi_1 f_1(x), \\pi_2 f_2(x))$ over the interval $[0,1]$ to obtain the Bayes error rate $L^\\star$.\n3.  Numerically integrate the function $h(x) = \\sqrt{f_1(x) f_2(x)}$ over $[0,1]$ to obtain the Bhattacharyya coefficient $\\rho$.\n4.  Calculate the Bhattacharyya bound as $\\sqrt{\\pi_1 \\pi_2} \\, \\rho$.\n5.  Calculate the trivial bound as $\\min(\\pi_1, \\pi_2)$.\n6.  Collect the three values $[L^\\star, \\sqrt{\\pi_1 \\pi_2} \\, \\rho, \\min(\\pi_1, \\pi_2)]$ and format them as required.\n\nA special case is Case 3, where $(\\alpha_1, \\beta_1) = (\\alpha_2, \\beta_2)$, meaning $f_1(x) = f_2(x)$. The integrand for $L^\\star$ becomes $\\min(\\pi_1, \\pi_2) f_1(x)$. Since $\\int_0^1 f_1(x) dx = 1$, the Bayes error rate simplifies to $L^\\star = \\min(\\pi_1, \\pi_2)$, which must match the trivial bound. This provides a valuable check for the correctness of the implementation.\n\nAnother interesting case is Case 1, where the distributions are mirror images, $f_2(x) = f_1(1-x)$, and the priors are balanced, $\\pi_1=\\pi_2=0.5$. The decision boundary is at $x=0.5$, and by symmetry, the Bayes error is $L^\\star = \\int_{0.5}^1 \\pi_1 f_1(x) dx$.\n\nThe implementation will proceed by defining the integrands and using a numerical integration routine to compute the definite integrals for each set of parameters provided in the test suite.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.integrate import quad\nfrom scipy.stats import beta\n\ndef solve():\n    \"\"\"\n    Computes the Bayes error rate, Bhattacharyya bound, and trivial bound\n    for a binary classification problem with Beta class-conditional densities.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (pi1, alpha1, beta1, pi2, alpha2, beta2)\n    test_cases = [\n        (0.5, 2, 5, 0.5, 5, 2),        # Case 1: balanced priors, mirror shapes\n        (0.7, 2, 2, 0.3, 5, 5),        # Case 2: unequal priors, moderate overlap\n        (0.6, 3, 4, 0.4, 3, 4),        # Case 3: identical distributions, unequal priors\n        (0.5, 0.5, 5, 0.5, 5, 0.5),    # Case 4: balanced priors, high concentration\n    ]\n\n    all_results = []\n    for case in test_cases:\n        pi1, a1, b1, pi2, a2, b2 = case\n\n        # Define the class-conditional density functions (PDFs) for the two Beta distributions.\n        # The scipy.stats.beta class provides an object representing the distribution.\n        # Its pdf method evaluates the probability density function.\n        f1 = beta(a=a1, b=b1).pdf\n        f2 = beta(a=a2, b=b2).pdf\n\n        # --- 1. Compute the Bayes error rate (L_star) ---\n        # The integrand is min(pi1*f1(x), pi2*f2(x)).\n        integrand_L_star = lambda x: np.minimum(pi1 * f1(x), pi2 * f2(x))\n        \n        # Perform numerical integration over the interval [0, 1].\n        # quad returns the integral result and an error estimate; we only need the result.\n        L_star, _ = quad(integrand_L_star, 0, 1, epsabs=1e-12, epsrel=1e-12)\n\n        # --- 2. Compute the Bhattacharyya bound ---\n        # First, compute the Bhattacharyya coefficient (rho).\n        # The integrand is sqrt(f1(x) * f2(x)).\n        integrand_rho = lambda x: np.sqrt(f1(x) * f2(x))\n        rho, _ = quad(integrand_rho, 0, 1, epsabs=1e-12, epsrel=1e-12)\n        \n        # The Bhattacharyya bound is sqrt(pi1 * pi2) * rho.\n        bhattacharyya_bound = np.sqrt(pi1 * pi2) * rho\n\n        # --- 3. Compute the trivial upper bound ---\n        # The trivial bound is the error of always guessing the majority class,\n        # which is the prior of the minority class.\n        trivial_bound = min(pi1, pi2)\n        \n        # Store the three computed values for this case.\n        results_for_case = [L_star, bhattacharyya_bound, trivial_bound]\n        all_results.append(results_for_case)\n\n    # --- Format the final output string as specified ---\n    # The output must be a single-line string representation of a list of lists,\n    # with each float rounded to 6 decimal places.\n    result_strings = []\n    for inner_list in all_results:\n        # Format each float to 6 decimal places.\n        formatted_numbers = [f\"{x:.6f}\" for x in inner_list]\n        # Create the string for the inner list, e.g., \"[0.123456,0.789012,0.345678]\"\n        result_strings.append(f\"[{','.join(formatted_numbers)}]\")\n    \n    # Join the inner list strings into the final output string.\n    final_output = f\"[{','.join(result_strings)}]\"\n    \n    # Final print statement in the exact required format.\n    print(final_output)\n\nsolve()\n```", "id": "3180189"}, {"introduction": "Our final practice delves into a more complex and realistic scenario, demonstrating the sophistication of the Bayes classifier. Here, we encounter a situation where one class is described by a simple unimodal distribution, while the other is a bimodal mixture of Gaussians. This setup [@problem_id:3180228] reveals a crucial insight: optimal decision boundaries are not always simple or connected, and can form non-convex regions. Furthermore, this exercise requires you to implement a numerical solution that distinguishes between the standard Bayes error rate (from $0$-$1$ loss) and the more general Bayes risk under asymmetric costs, a critical consideration in many real-world applications.", "problem": "Consider a binary classification problem in statistical learning with feature $X \\in \\mathbb{R}$ and label $Y \\in \\{0,1\\}$. The class prior probabilities are $P(Y=0)=\\pi_0$ and $P(Y=1)=\\pi_1$, where $\\pi_0,\\pi_1 \\in (0,1)$ and $\\pi_0+\\pi_1=1$. The class-conditional distributions are specified as follows:\n- For class $Y=1$, the conditional distribution $X \\mid Y=1$ is a bimodal mixture of two normal components: the Probability Density Function (PDF) is\n$$\nf_1(x) \\equiv w \\,\\phi(x;\\mu_{1a},\\sigma_{1a}) + (1-w)\\,\\phi(x;\\mu_{1b},\\sigma_{1b}),\n$$\nwhere $w \\in (0,1)$ is the mixture weight and $\\phi(x;\\mu,\\sigma)$ denotes the normal PDF with mean $\\mu$ and standard deviation $\\sigma$, namely\n$$\n\\phi(x;\\mu,\\sigma) \\equiv \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\!\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right).\n$$\n- For class $Y=0$, the conditional distribution $X \\mid Y=0$ is unimodal normal with PDF\n$$\nf_0(x) \\equiv \\phi(x;\\mu_0,\\sigma_0).\n$$\n\nA classification decision $\\hat{Y}(x) \\in \\{0,1\\}$ incurs a loss according to the following $2\\times 2$ loss specification: predicting the correct class has zero loss, predicting $1$ when the true class is $0$ has loss $\\lambda_{10} > 0$ (false positive), and predicting $0$ when the true class is $1$ has loss $\\lambda_{01} > 0$ (false negative). The Bayes classifier is defined as the decision rule that minimizes the conditional expected loss at each $x$, and the Bayes risk is the expected loss of the Bayes classifier under the joint distribution of $(X,Y)$.\n\nTask:\n1. Starting from core definitions (Bayes’ theorem for posterior probabilities and conditional risk minimization), reason about the Bayes decision rule for this setting. Explain why, when $f_1$ is bimodal and $f_0$ is unimodal, the Bayes decision region for predicting class $1$ can be a non-convex set in $\\mathbb{R}$ (a union of disjoint intervals), and identify, in terms of primitive quantities, the inequality that characterizes the decision region for class $1$ under both symmetric $0$-$1$ loss and asymmetric loss.\n2. Implement a numerical method to:\n   - Determine the number of disjoint intervals in the Bayes decision region for class $1$ under symmetric $0$-$1$ loss.\n   - Determine whether that decision region is non-convex (return a boolean that is `True` if and only if the number of intervals exceeds $1$).\n   - Compute the Bayes error rate under symmetric $0$-$1$ loss. Express this error rate as a real number in decimal form (no percentage sign).\n   - Compute the Bayes risk under the specified asymmetric loss $(\\lambda_{10},\\lambda_{01})$. Express this risk as a real number in decimal form.\n\nUse purely mathematical and algorithmic reasoning. Do not appeal to formulas that are not derivable from the stated definitions. Your program must carry out the computations numerically and must not require any external input.\n\nTest Suite:\nFor each test case, parameters are given as a tuple\n$$\n(\\pi_0,\\pi_1,w,\\mu_{1a},\\sigma_{1a},\\mu_{1b},\\sigma_{1b},\\mu_0,\\sigma_0,\\lambda_{10},\\lambda_{01}),\n$$\nwith all quantities real-valued and $\\pi_0+\\pi_1=1$, $w \\in (0,1)$, and all standard deviations strictly positive. The test suite is:\n\n- Case A (general happy path with clearly bimodal $f_1$ and a broader unimodal $f_0$, symmetric loss):\n$$\n(0.5,\\,0.5,\\,0.5,\\,-2.0,\\,0.6,\\,2.0,\\,0.6,\\,0.0,\\,1.2,\\,1.0,\\,1.0).\n$$\n- Case B (boundary tendency with dominant prior for $Y=0$, symmetric loss):\n$$\n(0.8,\\,0.2,\\,0.5,\\,-2.0,\\,0.6,\\,2.0,\\,0.6,\\,0.0,\\,1.2,\\,1.0,\\,1.0).\n$$\n- Case C (alternative shapes with narrower $f_0$ and shifted mixture components, symmetric loss):\n$$\n(0.5,\\,0.5,\\,0.6,\\,-2.5,\\,0.5,\\,1.8,\\,0.5,\\,0.0,\\,0.7,\\,1.0,\\,1.0).\n$$\n- Case D (asymmetric loss emphasizing false positive penalty, same shapes as Case A):\n$$\n(0.5,\\,0.5,\\,0.5,\\,-2.0,\\,0.6,\\,2.0,\\,0.6,\\,0.0,\\,1.2,\\,1.0,\\,3.0).\n$$\n\nAlgorithmic Output Specification:\n- For each test case, produce a result list of the form $[N,B,E,R]$, where:\n  - $N$ is an integer giving the number of disjoint intervals in the Bayes decision region for class $1$ under symmetric $0$-$1$ loss.\n  - $B$ is a boolean that is `True` if and only if the decision region is non-convex under symmetric $0$-$1$ loss (that is, $N>1$).\n  - $E$ is a float giving the Bayes error rate under symmetric $0$-$1$ loss.\n  - $R$ is a float giving the Bayes risk under the specified $(\\lambda_{10},\\lambda_{01})$ for that test case.\n- Your program should produce a single line of output containing the results for the four cases as a comma-separated list enclosed in square brackets, for example:\n$$\n[\\,[N_1,B_1,E_1,R_1],\\,[N_2,B_2,E_2,R_2],\\,[N_3,B_3,E_3,R_3],\\,[N_4,B_4,E_4,R_4]\\,].\n$$\nNo physical units or angles are involved. All probabilities and risks must be expressed as decimals (not percentages).", "solution": "The problem asks for a theoretical and numerical analysis of a Bayes classifier for a binary classification problem with a one-dimensional feature $X$.\n\n### **Part 1: Theoretical Derivation and Analysis**\n\n**1. Bayes Decision Rule from Conditional Risk Minimization**\n\nLet $Y \\in \\{0, 1\\}$ be the true class label and $\\hat{Y}(x) \\in \\{0, 1\\}$ be the predicted class for an observation $X=x$. The loss function is given by a matrix $L$, where $L(i, j)$ is the loss of predicting class $i$ when the true class is $j$. From the problem statement, we have:\n- $L(0, 0) = 0$ (Correctly predicting class $0$)\n- $L(1, 1) = 0$ (Correctly predicting class $1$)\n- $L(1, 0) = \\lambda_{10} > 0$ (False Positive, Type I error)\n- $L(0, 1) = \\lambda_{01} > 0$ (False Negative, Type II error)\n\nThe Bayes classifier seeks to minimize the conditional expected loss (or conditional risk) for each $x$. The conditional risk of predicting class $k$ given $X=x$ is:\n$$\nR(k | x) = \\mathbb{E}[L(k, Y) | X=x] = \\sum_{j=0}^{1} L(k, j) P(Y=j | X=x)\n$$\nLet's compute this for our two possible decisions, $\\hat{Y}(x)=0$ and $\\hat{Y}(x)=1$.\n\nThe conditional risk of predicting class $0$ is:\n$$\nR(\\hat{Y}=0 | x) = L(0, 0)P(Y=0 | x) + L(0, 1)P(Y=1 | x) = 0 \\cdot P(Y=0 | x) + \\lambda_{01} P(Y=1 | x) = \\lambda_{01} P(Y=1 | x)\n$$\nThe conditional risk of predicting class $1$ is:\n$$\nR(\\hat{Y}=1 | x) = L(1, 0)P(Y=0 | x) + L(1, 1)P(Y=1 | x) = \\lambda_{10} P(Y=0 | x) + 0 \\cdot P(Y=1 | x) = \\lambda_{10} P(Y=0 | x)\n$$\nThe Bayes decision rule is to choose the class that minimizes this conditional risk. Therefore, we predict class $1$ if and only if:\n$$\nR(\\hat{Y}=1 | x) < R(\\hat{Y}=0 | x)\n$$\n$$\n\\lambda_{10} P(Y=0 | x) < \\lambda_{01} P(Y=1 | x)\n$$\nThis inequality defines the decision criterion. To make it operational, we express the posterior probabilities $P(Y=j|x)$ using Bayes' theorem. The posterior is proportional to the likelihood times the prior:\n$$\nP(Y=j | x) = \\frac{f(x|Y=j) P(Y=j)}{f(x)} = \\frac{f_j(x) \\pi_j}{f(x)}\n$$\nwhere $f_j(x)$ is the class-conditional PDF for class $j$, $\\pi_j$ is the prior probability of class $j$, and $f(x) = \\sum_{j=0}^{1} f_j(x) \\pi_j$ is the marginal PDF of $X$.\n\nSubstituting these into the decision rule inequality:\n$$\n\\lambda_{10} \\frac{f_0(x) \\pi_0}{f(x)} < \\lambda_{01} \\frac{f_1(x) \\pi_1}{f(x)}\n$$\nSince $f(x) > 0$ for all $x$, we can multiply both sides by $f(x)$ without changing the inequality. This yields the final form of the decision rule for predicting class $1$:\n$$\n\\lambda_{10} f_0(x) \\pi_0 < \\lambda_{01} f_1(x) \\pi_1\n$$\nThis can be expressed as a likelihood ratio test. We predict class $1$ if:\n$$\n\\frac{f_1(x)}{f_0(x)} > \\frac{\\lambda_{10} \\pi_0}{\\lambda_{01} \\pi_1}\n$$\nThe decision region for class $1$, denoted $\\mathcal{R}_1$, is the set $\\{x \\in \\mathbb{R} \\mid \\lambda_{01} \\pi_1 f_1(x) > \\lambda_{10} \\pi_0 f_0(x)\\}$.\n\nFor the special case of symmetric $0-1$ loss, we have $\\lambda_{10} = \\lambda_{01} = 1$. The rule simplifies to:\n$$\nf_1(x) \\pi_1 > f_0(x) \\pi_0\n$$\nThis is the maximum a posteriori (MAP) rule, which states that we should choose the class with the higher posterior probability.\n\n**2. Non-Convexity of the Decision Region**\n\nThe decision boundaries are the points $x$ where the two sides of the inequality are equal:\n$$\n\\lambda_{01} \\pi_1 f_1(x) = \\lambda_{10} \\pi_0 f_0(x)\n$$\nLet's define a function $g(x) = \\lambda_{01} \\pi_1 f_1(x) - \\lambda_{10} \\pi_0 f_0(x)$. The decision region for class $1$ is where $g(x)>0$. The boundaries are the roots of $g(x)=0$.\n\nIn this problem, $f_0(x)$ is a unimodal normal PDF, $\\phi(x;\\mu_0,\\sigma_0)$. The function $f_1(x)$ is a bimodal mixture of two normal PDFs, $w \\,\\phi(x;\\mu_{1a},\\sigma_{1a}) + (1-w)\\,\\phi(x;\\mu_{1b},\\sigma_{1b})$. Therefore, $g(x)$ is a weighted sum and difference of three Gaussian functions.\n\nA single Gaussian function has a single peak. A mixture of Gaussians can be multimodal. The function $\\lambda_{01} \\pi_1 f_1(x)$ can have two distinct peaks (if the components are sufficiently separated), while the function $\\lambda_{10} \\pi_0 f_0(x)$ has only one peak. A curve with two humps can intersect a curve with one hump multiple times. For example, if the two peaks of the bimodal distribution are high enough relative to the unimodal distribution that lies between them, the bimodal curve can rise above the unimodal one, dip below it in the middle, and then rise above it again. This scenario would create four intersection points (roots of $g(x)=0$).\n\nLet the roots be $r_1 < r_2 < r_3 < r_4$. The sign of $g(x)$ might alternate across these roots. For instance, if $g(x)$ is negative for large $|x|$, the decision region for class $1$ (where $g(x)>0$) could be the union of two disjoint intervals, such as $\\mathcal{R}_1 = (r_1, r_2) \\cup (r_3, r_4)$. A union of disjoint intervals is a non-convex set in $\\mathbb{R}$. Thus, the bimodality of $f_1(x)$ is the key feature that allows for non-convex decision regions.\n\n**3. Bayes Error Rate and Bayes Risk**\n\nThe **Bayes risk** $R$ is the minimum possible expected loss for the classifier, achieved by the Bayes decision rule $\\hat{Y}_{Bayes}(x)$.\n$$\nR = \\mathbb{E}[L(\\hat{Y}_{Bayes}(X), Y)] = \\int_{-\\infty}^{\\infty} \\mathbb{E}[L(\\hat{Y}_{Bayes}(x), Y) | X=x] f(x) dx\n$$\nThe inner expectation is the conditional risk of the optimal decision at $x$, which is $\\min\\{R(\\hat{Y}=0 | x), R(\\hat{Y}=1 | x)\\}$.\n$$\nR = \\int_{-\\infty}^{\\infty} \\min\\{\\lambda_{01} P(Y=1|x), \\lambda_{10} P(Y=0|x)\\} f(x) dx\n$$\nUsing the relation $P(Y=j|x)f(x) = \\pi_j f_j(x)$, this simplifies to:\n$$\nR = \\int_{-\\infty}^{\\infty} \\min\\{\\lambda_{01} \\pi_1 f_1(x), \\lambda_{10} \\pi_0 f_0(x)\\} dx\n$$\nThe **Bayes error rate** $E$ is a special case of the Bayes risk for symmetric $0-1$ loss ($\\lambda_{10} = \\lambda_{01} = 1$).\n$$\nE = \\int_{-\\infty}^{\\infty} \\min\\{\\pi_1 f_1(x), \\pi_0 f_0(x)\\} dx\n$$\nThese integrals can be computed numerically. The procedure involves finding the roots of the corresponding $g(x)$ function to determine the decision regions. Let $\\mathcal{R}_0$ and $\\mathcal{R}_1$ be the regions where we predict classes $0$ and $1$ respectively. The integrals can be rewritten as:\n- For error rate $E$: $E = \\int_{\\mathcal{R}_1} \\pi_0 f_0(x) dx + \\int_{\\mathcal{R}_0} \\pi_1 f_1(x) dx$.\n- For risk $R$: $R = \\int_{\\mathcal{R}'_1} \\lambda_{10} \\pi_0 f_0(x) dx + \\int_{\\mathcal{R}'_0} \\lambda_{01} \\pi_1 f_1(x) dx$, where $\\mathcal{R}'_0, \\mathcal{R}'_1$ are the regions for the asymmetric loss.\n\n### **Part 2: Numerical Implementation Strategy**\n\nThe numerical solution involves the following steps for each test case:\n1.  **Define Functions**: Implement Python functions for the PDFs $f_0(x)$ and $f_1(x)$, and the decision functions $g_{sym}(x) = \\pi_1 f_1(x) - \\pi_0 f_0(x)$ and $g_{asym}(x) = \\lambda_{01} \\pi_1 f_1(x) - \\lambda_{10} \\pi_0 f_0(x)$.\n2.  **Find Roots**: To find the number of disjoint intervals for symmetric loss, determine all real roots of $g_{sym}(x)=0$. A robust method is to scan a wide range of $x$ values for sign changes, bracketing the roots. Then, use a numerical solver like `scipy.optimize.brentq` within each bracket to find the roots with high precision.\n3.  **Determine Intervals and N, B**: The sorted roots $\\{r_1, r_2, \\dots, r_k\\}$ partition $\\mathbb{R}$ into $k+1$ intervals. By testing the sign of $g_{sym}(x)$ at a sample point within each interval, we can identify the set of disjoint intervals that form the decision region $\\mathcal{R}_1$. The number of these intervals gives $N$, and $B$ is `True` if $N>1$.\n4.  **Compute Bayes Error Rate E**: With the roots for the symmetric case, partition the real line $(-\\infty, \\infty)$ into intervals. For each interval, determine whether it belongs to $\\mathcal{R}_0$ or $\\mathcal{R}_1$. Sum the results of numerically integrating the appropriate error term ($\\pi_0 f_0(x)$ over $\\mathcal{R}_1$ and $\\pi_1 f_1(x)$ over $\\mathcal{R}_0$) using `scipy.integrate.quad`.\n5.  **Compute Bayes Risk R**: Repeat the process for the asymmetric case. Find the roots of $g_{asym}(x)=0$. Use these new roots to define the asymmetric decision regions $\\mathcal{R}'_0$ and $\\mathcal{R}'_1$. Sum the results of numerically integrating the appropriate risk term ($\\lambda_{10} \\pi_0 f_0(x)$ over $\\mathcal{R}'_1$ and $\\lambda_{01} \\pi_1 f_1(x)$ over $\\mathcal{R}'_0$) to find the total Bayes risk $R$.\nThis procedure is implemented for the provided test suite.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\nfrom scipy.integrate import quad\nfrom scipy.optimize import brentq\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    \"\"\"\n    test_cases = [\n        # Case A\n        (0.5, 0.5, 0.5, -2.0, 0.6, 2.0, 0.6, 0.0, 1.2, 1.0, 1.0),\n        # Case B\n        (0.8, 0.2, 0.5, -2.0, 0.6, 2.0, 0.6, 0.0, 1.2, 1.0, 1.0),\n        # Case C\n        (0.5, 0.5, 0.6, -2.5, 0.5, 1.8, 0.5, 0.0, 0.7, 1.0, 1.0),\n        # Case D\n        (0.5, 0.5, 0.5, -2.0, 0.6, 2.0, 0.6, 0.0, 1.2, 1.0, 3.0),\n    ]\n\n    results = []\n    for case in test_cases:\n        results.append(process_case(case))\n\n    # Format the final output string as a compact list of lists\n    # e.g., [[N1,B1,E1,R1],[N2,B2,E2,R2],...]\n    # The problem specifies boolean output as 'true' or 'false', so replace Python's default\n    output_string = str(results).replace(\" \", \"\").replace(\"True\", \"true\").replace(\"False\", \"false\")\n    print(output_string)\n\ndef process_case(params):\n    \"\"\"\n    Computes [N, B, E, R] for a single test case.\n    \"\"\"\n    pi0, pi1, w, mu1a, sigma1a, mu1b, sigma1b, mu0, sigma0, lam10, lam01 = params\n\n    # 1. Define PDFs\n    def f1(x):\n        return w * norm.pdf(x, loc=mu1a, scale=sigma1a) + (1-w) * norm.pdf(x, loc=mu1b, scale=sigma1b)\n\n    def f0(x):\n        return norm.pdf(x, loc=mu0, scale=sigma0)\n\n    # 2. Analyze symmetric loss case for N, B, E\n    def g_sym(x):\n        return pi1 * f1(x) - pi0 * f0(x)\n\n    def find_roots(func):\n        # Determine a sufficiently wide range to find all relevant roots\n        all_mus = [mu1a, mu1b, mu0]\n        all_sigmas = [sigma1a, sigma1b, sigma0]\n        search_min = min(all_mus) - 15 * max(all_sigmas)\n        search_max = max(all_mus) + 15 * max(all_sigmas)\n        \n        roots = []\n        grid = np.linspace(search_min, search_max, 20001)\n        y_vals = func(grid)\n        \n        for i in range(len(grid) - 1):\n            if np.sign(y_vals[i]) != np.sign(y_vals[i+1]):\n                try:\n                    root = brentq(func, grid[i], grid[i+1])\n                    roots.append(root)\n                except (ValueError, RuntimeError):\n                    # In case of issues, we skip, as sign change is the main trigger.\n                    pass\n        return sorted(list(set(roots)))\n\n    roots_sym = find_roots(g_sym)\n    \n    # 3. Determine N and B\n    R1_sym_intervals = []\n    all_sym_points = [-np.inf] + roots_sym + [np.inf]\n    for i in range(len(all_sym_points) - 1):\n        a, b = all_sym_points[i], all_sym_points[i+1]\n        test_point = (a + b) / 2.0\n        if np.isinf(a): test_point = b - 1.0\n        if np.isinf(b): test_point = a + 1.0\n        if g_sym(test_point) > 0:\n            R1_sym_intervals.append((a, b))\n    \n    N = len(R1_sym_intervals)\n    B = N > 1\n\n    # 4. Compute Bayes Error Rate E (symmetric loss)\n    # E = integral over R of min(pi0*f0, pi1*f1)\n    # This is equivalent to integrating pi1*f1 over R0 and pi0*f0 over R1\n    error = 0.0\n    all_sym_points = [-np.inf] + roots_sym + [np.inf]\n    for i in range(len(all_sym_points) - 1):\n        a, b = all_sym_points[i], all_sym_points[i+1]\n        test_point = (a + b) / 2.0\n        if np.isinf(a): test_point = b - 1.0\n        if np.isinf(b): test_point = a + 1.0\n\n        if g_sym(test_point) > 0: # Predict 1, error is if Y=0\n            error += quad(lambda x: pi0 * f0(x), a, b)[0]\n        else: # Predict 0, error is if Y=1\n            error += quad(lambda x: pi1 * f1(x), a, b)[0]\n    E = error\n    \n    # 5. Compute Bayes Risk R (asymmetric loss)\n    def g_asym(x):\n        return lam01 * pi1 * f1(x) - lam10 * pi0 * f0(x)\n\n    roots_asym = find_roots(g_asym)\n    \n    risk = 0.0\n    all_asym_points = [-np.inf] + roots_asym + [np.inf]\n    for i in range(len(all_asym_points) - 1):\n        a, b = all_asym_points[i], all_asym_points[i+1]\n        test_point = (a + b) / 2.0\n        if np.isinf(a): test_point = b - 1.0\n        if np.isinf(b): test_point = a + 1.0\n\n        if g_asym(test_point) > 0: # Predict 1, risk is lambda_10 * P(Y=0)\n            risk += quad(lambda x: lam10 * pi0 * f0(x), a, b)[0]\n        else: # Predict 0, risk is lambda_01 * P(Y=1)\n            risk += quad(lambda x: lam01 * pi1 * f1(x), a, b)[0]\n    R = risk\n\n    return [N, B, E, R]\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3180228"}]}