{"hands_on_practices": [{"introduction": "This exercise will have you delve into the mathematics of a kernel-based One-Class SVM. By calculating the gradient of the decision function, you will gain a geometric understanding of the decision boundary and determine the most efficient way to alter a data point to no longer be considered an anomaly. This practice is a foundational step towards model interpretability and understanding the local behavior of complex models [@problem_id:3099119].", "problem": "A one-class Support Vector Machine (SVM) with a Radial Basis Function (RBF) kernel is trained for anomaly detection in two dimensions. The decision function is defined by the kernel representation\n$$\nf(x) \\;=\\; \\sum_{i=1}^{n} \\alpha_{i} \\, k(x, x_{i}) \\;-\\; \\rho,\n$$\nwhere the Radial Basis Function (RBF) kernel is\n$$\nk(x, z) \\;=\\; \\exp\\!\\big(-\\gamma \\, \\|x - z\\|^{2}\\big),\n$$\nand candidate points are flagged as anomalies when $f(x)  0$. Consider a model with $n=3$ support vectors and weights given by\n$$\n\\alpha_{1} = 0.5, \\quad \\alpha_{2} = 0.3, \\quad \\alpha_{3} = 0.2, \\quad \\gamma = 1.2, \\quad \\rho = 0.25,\n$$\nlocated at\n$$\nx_{1} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\quad x_{2} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\quad x_{3} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}.\n$$\nA test point\n$$\nx = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n$$\nis flagged as an anomaly. Starting from first principles for kernel methods and multivariate calculus, do the following:\n- Compute the gradient $\\nabla_{x} f(x)$ of the decision function at the point $x$.\n- Using a first-order approximation of $f$ near $x$, derive and compute the minimal Euclidean-norm perturbation vector $\\delta^{\\star}$ that moves $x$ onto the local decision boundary $f(x + \\delta) = 0$.\n\nExpress your final answer as the perturbation vector $\\delta^{\\star}$ in the form of a row matrix. Round each component of the vector to four significant figures. No physical units are involved.", "solution": "This problem requires us to find the smallest perturbation $\\delta^\\star$ that moves a point $x$ onto the decision boundary $f(x+\\delta^\\star)=0$. We use a first-order Taylor approximation and principles of constrained optimization.\n\n**1. Define the Optimization Problem**\nWe want to find the perturbation vector $\\delta$ that minimizes its own Euclidean norm $\\|\\delta\\|$ while satisfying the condition $f(x+\\delta) = 0$. Using a first-order approximation for $f$ around $x$, the condition becomes:\n$$f(x + \\delta) \\approx f(x) + \\nabla_x f(x)^\\top \\delta = 0$$\nThis gives us the linear constraint:\n$$\\nabla_x f(x)^\\top \\delta = -f(x)$$\nThe problem is to minimize $\\|\\delta\\|^2$ subject to this constraint. The minimal-norm vector $\\delta^\\star$ that satisfies this equation is parallel to the gradient $\\nabla_x f(x)$. Thus, we can write $\\delta^\\star = c \\cdot \\nabla_x f(x)$ for some scalar $c$. Substituting this into the constraint gives:\n$$\\nabla_x f(x)^\\top (c \\cdot \\nabla_x f(x)) = -f(x) \\implies c \\|\\nabla_x f(x)\\|^2 = -f(x)$$\nSolving for $c$ yields $c = -\\frac{f(x)}{\\|\\nabla_x f(x)\\|^2}$. The optimal perturbation is therefore:\n$$\\delta^\\star = - \\frac{f(x)}{\\|\\nabla_x f(x)\\|^2} \\nabla_x f(x)$$\nTo compute this, we need to calculate $f(x)$ and its gradient $\\nabla_x f(x)$ at the given point $x$.\n\n**2. Compute the Gradient of the Decision Function**\nThe gradient of the RBF kernel $k(x,z)$ with respect to $x$ is:\n$$\\nabla_x k(x, z) = \\nabla_x \\exp(-\\gamma \\|x - z\\|^2) = -2\\gamma(x-z)k(x,z)$$\nThe gradient of the decision function $f(x)$ is then:\n$$\\nabla_x f(x) = \\sum_{i=1}^{n} \\alpha_i \\nabla_x k(x, x_i) = -2\\gamma \\sum_{i=1}^{n} \\alpha_i k(x, x_i) (x - x_i)$$\n\n**3. Numerical Calculation**\nGiven data: $\\gamma=1.2$, $\\rho=0.25$, $x = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$. Support vectors $x_1=\\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$, $x_2=\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$, $x_3=\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$ with weights $\\alpha_1=0.5, \\alpha_2=0.3, \\alpha_3=0.2$.\n\nFirst, compute the kernel values $k(x, x_i)$:\n-   $i=1$: $\\|x-x_1\\|^2 = \\|(1,1)\\|^2 = 2$. $k_1 = \\exp(-1.2 \\cdot 2) = \\exp(-2.4) \\approx 0.090718$.\n-   $i=2$: $\\|x-x_2\\|^2 = \\|(0,1)\\|^2 = 1$. $k_2 = \\exp(-1.2 \\cdot 1) = \\exp(-1.2) \\approx 0.301194$.\n-   $i=3$: $\\|x-x_3\\|^2 = \\|(1,0)\\|^2 = 1$. $k_3 = \\exp(-1.2 \\cdot 1) = \\exp(-1.2) \\approx 0.301194$.\n\nNow, calculate $f(x)$:\n$$f(x) = (\\alpha_1 k_1 + \\alpha_2 k_2 + \\alpha_3 k_3) - \\rho$$\n$$f(x) = (0.5 \\cdot 0.090718 + 0.3 \\cdot 0.301194 + 0.2 \\cdot 0.301194) - 0.25$$\n$$f(x) = 0.045359 + 0.090358 + 0.060239 - 0.25 = 0.195956 - 0.25 = -0.054044$$\n\nNext, calculate the gradient $\\nabla_x f(x)$:\n$$\\nabla_x f(x) = -2.4 \\left( \\alpha_1 k_1 (x-x_1) + \\alpha_2 k_2 (x-x_2) + \\alpha_3 k_3 (x-x_3) \\right)$$\n$$\\nabla_x f(x) = -2.4 \\left( 0.045359 \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} + 0.090358 \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} + 0.060239 \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\right)$$\n$$\\nabla_x f(x) = -2.4 \\begin{pmatrix} 0.045359 + 0.060239 \\\\ 0.045359 + 0.090358 \\end{pmatrix} = -2.4 \\begin{pmatrix} 0.105598 \\\\ 0.135717 \\end{pmatrix} = \\begin{pmatrix} -0.253435 \\\\ -0.325721 \\end{pmatrix}$$\n\nFinally, compute the optimal perturbation $\\delta^\\star$:\n$$\\|\\nabla_x f(x)\\|^2 = (-0.253435)^2 + (-0.325721)^2 \\approx 0.064229 + 0.106094 = 0.170323$$\n$$\\delta^\\star = - \\frac{-0.054044}{0.170323} \\begin{pmatrix} -0.253435 \\\\ -0.325721 \\end{pmatrix} \\approx 0.31730 \\begin{pmatrix} -0.253435 \\\\ -0.325721 \\end{pmatrix} \\approx \\begin{pmatrix} -0.080421 \\\\ -0.103389 \\end{pmatrix}$$\nRounding each component to four significant figures, we get:\n$$\\delta^\\star \\approx \\begin{pmatrix} -0.08042 \\\\ -0.1034 \\end{pmatrix}$$", "answer": "$$\\boxed{\\begin{pmatrix} -0.08042  -0.1034 \\end{pmatrix}}$$", "id": "3099119"}, {"introduction": "Anomaly detection models typically output continuous scores, but real-world applications demand discrete classifications. This practice bridges that gap by simulating a common evaluation scenario, where you will implement a prevalence-based thresholding strategy to convert scores from OCSVM and Isolation Forest into predictions. You will then assess their performance using the precision metric, highlighting the practical steps involved in comparing and deploying different anomaly detection algorithms [@problem_id:3099063].", "problem": "You are given anomaly scores from two unsupervised anomaly detection methods for multiple test sets: One-Class Support Vector Machine (OCSVM) and Isolation Forest (IF). For each test set, you must set a prevalence-based threshold by selecting the top $k$ most anomalous points, where $k = \\lfloor \\pi n \\rfloor$, with $\\pi \\in [0,1]$ the target anomaly prevalence and $n$ the number of samples in the test set. Larger scores indicate a higher degree of anomaly. Break any ties at the threshold deterministically by preferring the smaller sample index. Compute the precision for each method under this thresholding rule, where precision is defined as $TP/(TP+FP)$ if $TP+FP  0$, and defined as $0$ when $TP+FP = 0$.\n\nFundamental definitions and rules:\n- Given a vector of scores $\\mathbf{s} \\in \\mathbb{R}^n$, sort indices by descending score using a lexicographic rule that first orders by $-s_i$ and breaks ties by ascending index $i$. Select the first $k = \\lfloor \\pi n \\rfloor$ indices as predicted anomalies. This implements a prevalence-based threshold via order statistics.\n- True positives are $TP = \\sum_{i=1}^n \\mathbb{I}[\\hat{y}_i = 1 \\wedge y_i = 1]$, false positives are $FP = \\sum_{i=1}^n \\mathbb{I}[\\hat{y}_i = 1 \\wedge y_i = 0]$, and precision is $TP/(TP+FP)$ if $TP+FP  0$, and $0$ otherwise.\n- When $\\pi = 0$, $k = 0$ and precision is defined to be $0$. When $\\pi = 1$, $k = n$ and precision equals the fraction of anomalies in the dataset.\n\nYour program must compute the precision for both methods on each of the following test cases and output the results in a single line as a comma-separated list enclosed in square brackets in the order specified below. All reported values must be floats rounded to four decimal places.\n\nTest suite:\n1. Test case $1$ (happy path, no ties):\n   - $n = 12$\n   - OCSVM scores $\\mathbf{s}^{(1)}_{\\mathrm{OCSVM}} = [0.10, 0.95, 0.40, 0.30, 0.60, 0.90, 0.20, 0.85, 0.50, 0.55, 0.15, 0.45]$\n   - IF scores $\\mathbf{s}^{(1)}_{\\mathrm{IF}} = [0.12, 0.88, 0.80, 0.65, 0.50, 0.90, 0.85, 0.40, 0.30, 0.45, 0.20, 0.60]$\n   - Ground-truth labels $\\mathbf{y}^{(1)} = [0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0]$\n   - Target prevalence $\\pi^{(1)} = 0.25$\n2. Test case $2$ (tie at cutoff, tie-break by index):\n   - $n = 10$\n   - OCSVM scores $\\mathbf{s}^{(2)}_{\\mathrm{OCSVM}} = [0.10, 0.50, 0.95, 0.70, 0.40, 0.85, 0.20, 0.60, 0.70, 0.30]$\n   - IF scores $\\mathbf{s}^{(2)}_{\\mathrm{IF}} = [0.30, 0.60, 0.92, 0.70, 0.45, 0.80, 0.20, 0.58, 0.89, 0.40]$\n   - Ground-truth labels $\\mathbf{y}^{(2)} = [0, 0, 1, 0, 0, 0, 0, 0, 1, 0]$\n   - Target prevalence $\\pi^{(2)} = 0.30$\n3. Test case $3$ (boundary case $\\pi = 0$):\n   - Reuse $\\mathbf{s}^{(1)}_{\\mathrm{OCSVM}}$, $\\mathbf{s}^{(1)}_{\\mathrm{IF}}$, and $\\mathbf{y}^{(1)}$ from test case $1$\n   - Target prevalence $\\pi^{(3)} = 0.00$\n4. Test case $4$ (boundary case $\\pi = 1$):\n   - Reuse $\\mathbf{s}^{(1)}_{\\mathrm{OCSVM}}$, $\\mathbf{s}^{(1)}_{\\mathrm{IF}}$, and $\\mathbf{y}^{(1)}$ from test case $1$\n   - Target prevalence $\\pi^{(4)} = 1.00$\n\nFinal output specification:\n- For each test case $j \\in \\{1,2,3,4\\}$, compute precision for OCSVM, denoted $P^{(j)}_{\\mathrm{OCSVM}}$, and for IF, denoted $P^{(j)}_{\\mathrm{IF}}$.\n- Your program should produce a single line of output containing a single list of $8$ floats, rounded to four decimal places, in the following order:\n  $[\\,P^{(1)}_{\\mathrm{OCSVM}}, P^{(1)}_{\\mathrm{IF}}, P^{(2)}_{\\mathrm{OCSVM}}, P^{(2)}_{\\mathrm{IF}}, P^{(3)}_{\\mathrm{OCSVM}}, P^{(3)}_{\\mathrm{IF}}, P^{(4)}_{\\mathrm{OCSVM}}, P^{(4)}_{\\mathrm{IF}}\\,]$.", "solution": "The problem is valid. It presents a well-defined computational task based on standard principles of performance evaluation for machine learning models. The problem statement is self-contained, with all necessary data, definitions, and environmental constraints provided. It is scientifically grounded in the field of statistical learning, logically consistent, and free of ambiguity or factual errors.\n\nThe task is to compute the precision of two anomaly detection models, One-Class Support Vector Machine (OCSVM) and Isolation Forest (IF), across four test scenarios. The evaluation is based on a prevalence-based thresholding strategy.\n\nThe core of the problem lies in correctly implementing the specified thresholding rule and the precision metric, including all edge cases. The procedure can be broken down into the following steps:\n\n**1. Determining the Number of Anomalies ($k$)**\nFor a given test set with $n$ samples and a target anomaly prevalence $\\pi \\in [0, 1]$, the number of data points to be flagged as anomalies, denoted by $k$, is determined by the floor function:\n$$k = \\lfloor \\pi n \\rfloor$$\nThis value $k$ represents the size of the set of predicted anomalies.\n\n**2. Identifying Anomalies via Score-Based Ranking**\nThe prediction of which samples are anomalous is based on their scores, $\\mathbf{s} \\in \\mathbb{R}^n$. A higher score indicates a greater likelihood of being an anomaly. To select the top $k$ anomalies, we must establish a deterministic ranking. The problem specifies a lexicographical sorting rule:\n- The primary sorting key is the anomaly score $s_i$, in descending order (i.e., we sort by -$s_i$ in ascending order).\n- The secondary sorting key is the sample index $i$, in ascending order. This rule is used to break any ties in the scores.\n\nTherefore, for two samples with indices $i$ and $j$, sample $i$ is considered more anomalous than sample $j$ if ($s_i  s_j$) or ($s_i = s_j$ and $i  j$). The set of predicted anomalies, $\\hat{Y}_1$, consists of the $k$ samples with the highest rank according to this rule.\n\n**3. Computing Precision**\nPrecision is a metric that measures the fraction of correctly identified anomalies among all samples predicted as anomalies. It is defined in terms of true positives ($TP$) and false positives ($FP$).\n- A True Positive ($TP$) is a sample that is correctly identified as an anomaly (predicted label $\\hat{y}_i = 1$ and true label $y_i = 1$).\n- A False Positive ($FP$) is a sample that is incorrectly identified as an anomaly (predicted label $\\hat{y}_i = 1$ and true label $y_i = 0$).\n\nThe total number of predicted anomalies is $TP + FP$, which is equal to $k$. The precision $P$ is then calculated as:\n$$P = \\begin{cases} \\frac{TP}{TP + FP}  \\text{if } TP + FP  0 \\\\ 0  \\text{if } TP + FP = 0 \\end{cases}$$\nSince $TP+FP = k$, this simplifies to $P = TP / k$ for $k  0$.\n\n**4. Handling Boundary Cases for Prevalence ($\\pi$)**\nThe problem defines specific behavior for the boundary values of $\\pi$:\n- If $\\pi = 0$, then $k = \\lfloor 0 \\cdot n \\rfloor = 0$. No samples are predicted as anomalies. In this case, $TP=0$ and $FP=0$, so $TP+FP=0$. The precision is explicitly defined to be $0$.\n- If $\\pi = 1$, then $k = \\lfloor 1 \\cdot n \\rfloor = n$. All samples are predicted as anomalies. The precision is defined as the overall fraction of true anomalies in the dataset. This is equivalent to $P = (\\sum_{i=1}^n y_i) / n$. This is consistent with the general formula, as $k=n$ and $TP$ becomes the total count of true anomalies.\n\n**Step-by-Step Calculation for a Single Test Case:**\n\nGiven a vector of scores $\\mathbf{s}$, a vector of true labels $\\mathbf{y}$, and a prevalence $\\pi$:\n1.  Calculate $n = \\text{length}(\\mathbf{s})$.\n2.  Calculate $k = \\lfloor \\pi n \\rfloor$.\n3.  If $k=0$, the precision is $0$.\n4.  If $k0$, create a list of indices $I = [0, 1, \\dots, n-1]$.\n5.  Sort the indices $I$ based on the key $(-s_i, i)$ to get the sorted list $I'$.\n6.  Select the top $k$ indices from $I'$ to form the set of predicted anomaly indices, $I'_{\\text{anom}} = \\{I'_1, I'_2, \\dots, I'_k\\}$.\n7.  Calculate the number of true positives: $TP = \\sum_{i \\in I'_{\\text{anom}}} y_i$.\n8.  Calculate precision: $P = TP / k$.\n9.  This procedure is applied to the scores from both OCSVM and IF for each of the four test cases. The resulting eight precision values are collected, rounded, and formatted.\n\n**Execution on Test Cases:**\n\n- **Test Case 1**: $n = 12$, $\\pi = 0.25$, $k = \\lfloor 0.25 \\times 12 \\rfloor = 3$.\n  - OCSVM: The highest scores are at indices $1$ ($0.95$), $5$ ($0.90$), and $7$ ($0.85$). Predicted anomalies are at indices $\\{1, 5, 7\\}$. True labels are $y_1=1$, $y_5=0$, $y_7=1$. So, $TP=2$, $FP=1$. Precision $P = 2/3 \\approx 0.6667$.\n  - IF: The highest scores are at indices $5$ ($0.90$), $1$ ($0.88$), and $6$ ($0.85$). Predicted anoms: $\\{5, 1, 6\\}$. True labels are $y_5=0$, $y_1=1$, $y_6=0$. So, $TP=1$, $FP=2$. Precision $P = 1/3 \\approx 0.3333$.\n\n- **Test Case 2**: $n = 10$, $\\pi = 0.30$, $k = \\lfloor 0.30 \\times 10 \\rfloor = 3$.\n  - OCSVM: Highest scores are at index $2$ ($0.95$), $5$ ($0.85$). There is a tie for the third spot between index $3$ ($0.70$) and index $8$ ($0.70$). The tie-breaking rule (smaller index wins) selects index $3$. Predicted anoms: $\\{2, 5, 3\\}$. True labels are $y_2=1$, $y_5=0$, $y_3=0$. So, $TP=1$, $FP=2$. Precision $P = 1/3 \\approx 0.3333$.\n  - IF: The highest scores are at indices $2$ ($0.92$), $8$ ($0.89$), and $5$ ($0.80$). Predicted anoms: $\\{2, 8, 5\\}$. True labels are $y_2=1$, $y_8=1$, $y_5=0$. So, $TP=2$, $FP=1$. Precision $P = 2/3 \\approx 0.6667$.\n\n- **Test Case 3**: $\\pi = 0.0$.\n  - Per the problem definition, precision for both OCSVM and IF is $0.0000$.\n\n- **Test Case 4**: $\\pi = 1.0$.\n  - Per the problem definition, precision is the fraction of true anomalies in the dataset. The data is from test case $1$, where there are $4$ anomalies out of $12$ samples.\n  - For both OCSVM and IF, precision is $4/12 = 1/3 \\approx 0.3333$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes precision for OCSVM and IF methods on four test cases\n    and prints the results in the specified format.\n    \"\"\"\n\n    def compute_precision(scores, labels, pi):\n        \"\"\"\n        Calculates precision based on a prevalence-based threshold.\n\n        Args:\n            scores (np.ndarray): Anomaly scores for each sample.\n            labels (np.ndarray): Ground-truth labels (1 for anomaly, 0 for normal).\n            pi (float): Target anomaly prevalence.\n\n        Returns:\n            float: The calculated precision.\n        \"\"\"\n        n = len(scores)\n\n        # Handle the boundary case pi=0 as per problem definition.\n        if pi == 0.0:\n            return 0.0\n\n        num_anomalies = int(np.floor(pi * n))\n\n        # If k=0, no anomalies are predicted, so TP=0, FP=0, and precision is 0.\n        if num_anomalies == 0:\n            return 0.0\n        \n        # Handle the boundary case pi=1 as per problem definition.\n        if pi == 1.0:\n            return np.sum(labels) / n\n\n        # For 0  pi  1:\n        # Create a list of indices from 0 to n-1.\n        indices = np.arange(n)\n\n        # Sort indices based on the specified lexicographical rule:\n        # Primary key: score (descending).\n        # Secondary key: index (ascending).\n        sorted_indices = sorted(indices, key=lambda i: (-scores[i], i))\n\n        # Select the top k indices as predicted anomalies.\n        top_k_indices = sorted_indices[:num_anomalies]\n\n        # Calculate True Positives (TP).\n        tp = 0\n        for i in top_k_indices:\n            if labels[i] == 1:\n                tp += 1\n        \n        # The number of predicted positives is TP + FP = num_anomalies.\n        # Precision is TP / (TP + FP).\n        precision = tp / num_anomalies\n        \n        return precision\n\n    # Test suite data\n    # Test case 1 data\n    s_ocsvm1 = np.array([0.10, 0.95, 0.40, 0.30, 0.60, 0.90, 0.20, 0.85, 0.50, 0.55, 0.15, 0.45])\n    s_if1 = np.array([0.12, 0.88, 0.80, 0.65, 0.50, 0.90, 0.85, 0.40, 0.30, 0.45, 0.20, 0.60])\n    y1 = np.array([0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0])\n    pi1 = 0.25\n\n    # Test case 2 data\n    s_ocsvm2 = np.array([0.10, 0.50, 0.95, 0.70, 0.40, 0.85, 0.20, 0.60, 0.70, 0.30])\n    s_if2 = np.array([0.30, 0.60, 0.92, 0.70, 0.45, 0.80, 0.20, 0.58, 0.89, 0.40])\n    y2 = np.array([0, 0, 1, 0, 0, 0, 0, 0, 1, 0])\n    pi2 = 0.30\n\n    # Test cases parameters\n    test_cases = [\n        # (s_ocsvm, s_if, y, pi)\n        (s_ocsvm1, s_if1, y1, pi1),  # Test case 1\n        (s_ocsvm2, s_if2, y2, pi2),  # Test case 2\n        (s_ocsvm1, s_if1, y1, 0.0),  # Test case 3\n        (s_ocsvm1, s_if1, y1, 1.0),  # Test case 4\n    ]\n\n    results = []\n    for s_ocsvm, s_if, y, pi in test_cases:\n        p_ocsvm = compute_precision(s_ocsvm, y, pi)\n        p_if = compute_precision(s_if, y, pi)\n        results.extend([p_ocsvm, p_if])\n\n    # Format results to four decimal places and print in the required format.\n    formatted_results = [f\"{res:.4f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3099063"}, {"introduction": "This advanced exercise moves beyond simple thresholding to explore score calibration, a powerful technique for optimizing decisions under a fixed error tolerance. You will model the score distributions of normal and anomalous data and apply principles from statistical decision theory to derive a threshold based on the likelihood ratio. This practice demonstrates how to improve a model's detection power through sophisticated post-processing, a method that is theoretically optimal for a fixed false positive rate [@problem_id:3099077].", "problem": "You will implement and evaluate a semi-parametric calibration method for anomaly detection decision scores, using Gaussian mixture models to estimate class-conditional score densities and likelihood-ratio thresholding under a fixed false positive rate. The context is anomaly detection with One-Class Support Vector Machine (OCSVM) and Isolation Forest (IF), both of which produce scalar decision scores per sample. The OCSVM decision function is typically denoted by $f(x)$; we adopt the anomaly score $s=-f(x)$ so that larger $s$ indicates higher anomaly likelihood. For IF, we treat its anomaly score $s$ as a scalar on the real line with larger values indicating higher anomaly likelihood. You are given synthetic, scientifically plausible score distributions representing typical outcomes from OCSVM and IF. Your program must estimate class-conditional densities for normal scores and injected anomaly scores using Gaussian mixtures and then apply the likelihood-ratio test principle to set thresholds. Finally, you will compare this calibrated decision rule to naive thresholding on raw scores at the same false positive rate.\n\nFoundational base to use:\n- Likelihood-ratio testing under the Neyman–Pearson paradigm: the most powerful test at a fixed false positive rate $\\alpha$ classifies a score $s$ as anomaly if the likelihood ratio $\\Lambda(s)=\\frac{p(s \\mid y=\\text{anomaly})}{p(s \\mid y=\\text{normal})}$ exceeds a threshold chosen to meet the specified false positive rate $\\alpha$, where $p(\\cdot \\mid y)$ denotes a class-conditional density.\n- Semi-parametric modeling with Gaussian mixtures: each class-conditional density is represented as a finite mixture of univariate Gaussians whose parameters are estimated from data via maximum likelihood.\n\nYour task:\n- For each test case, generate two sets of one-dimensional scores using a fixed random seed: one set from the normal score distribution and one set from the injected anomaly score distribution. Each distribution is defined as a Gaussian mixture with specified component weights, means, and standard deviations. Larger scores indicate greater anomaly likelihood.\n- Fit a Gaussian mixture to the normal scores and a (potentially different) Gaussian mixture to the anomaly scores by maximum likelihood using the Expectation–Maximization procedure. Each fit must be univariate and use the specified number of mixture components.\n- Compute the class-conditional density estimates $\\hat{p}(s \\mid y=\\text{normal})$ and $\\hat{p}(s \\mid y=\\text{anomaly})$. Construct the likelihood ratio $\\hat{\\Lambda}(s)=\\frac{\\hat{p}(s \\mid y=\\text{anomaly})}{\\hat{p}(s \\mid y=\\text{normal})}$.\n- Choose the calibrated threshold $\\tau$ to achieve false positive rate $\\alpha$ by setting $\\tau$ to the $1-\\alpha$ quantile of $\\hat{\\Lambda}(s)$ over the normal scores. Classify a score $s$ as anomaly if $\\hat{\\Lambda}(s)\\tau$. Compute the resulting true positive rate (the fraction of anomaly scores classified as anomalies).\n- As a baseline, choose the raw score threshold $t$ to achieve the same false positive rate $\\alpha$ by setting $t$ to the $1-\\alpha$ quantile of the raw scores over the normal scores. Classify a score $s$ as anomaly if $st$. Compute the resulting true positive rate.\n- For each test case, output the improvement defined as the difference between the calibrated true positive rate and the raw-score true positive rate. The false positive rate $\\alpha$ values are specified as decimal fractions. No physical units are involved.\n\nTest suite and parameters:\n- Case $1$ (happy path, IF-like separation):\n  - Normal mixture: weights $[0.6,0.4]$, means $[0.22,0.38]$, standard deviations $[0.05,0.04]$, sample size $300$.\n  - Anomaly mixture: weights $[1.0]$, means $[0.72]$, standard deviations $[0.06]$, sample size $30$.\n  - Fit with $K_{\\text{normal}}=2$ and $K_{\\text{anomaly}}=1$. False positive rate $\\alpha=0.1$. Seed $1$.\n- Case $2$ (bimodal normal, central anomalies, OCSVM-like score inversion accounted for by orientation):\n  - Normal mixture: weights $[0.5,0.5]$, means $[0.15,0.85]$, standard deviations $[0.04,0.06]$, sample size $300$.\n  - Anomaly mixture: weights $[1.0]$, means $[0.50]$, standard deviations $[0.07]$, sample size $30$.\n  - Fit with $K_{\\text{normal}}=2$ and $K_{\\text{anomaly}}=1$. False positive rate $\\alpha=0.1$. Seed $2$.\n- Case $3$ (class imbalance and heteroscedastic anomalies):\n  - Normal mixture: weights $[1.0]$, means $[0.30]$, standard deviations $[0.05]$, sample size $1000$.\n  - Anomaly mixture: weights $[1.0]$, means $[0.60]$, standard deviations $[0.20]$, sample size $10$.\n  - Fit with $K_{\\text{normal}}=1$ and $K_{\\text{anomaly}}=2$. False positive rate $\\alpha=0.05$. Seed $3$.\n- Case $4$ (near-identical distributions, boundary case):\n  - Normal mixture: weights $[1.0]$, means $[0.50]$, standard deviations $[0.10]$, sample size $500$.\n  - Anomaly mixture: weights $[1.0]$, means $[0.55]$, standard deviations $[0.10]$, sample size $50$.\n  - Fit with $K_{\\text{normal}}=1$ and $K_{\\text{anomaly}}=1$. False positive rate $\\alpha=0.1$. Seed $4$.\n\nProgram requirements:\n- Implement univariate Gaussian mixture fitting via Expectation–Maximization from first principles, without external machine learning libraries.\n- Use reproducible sampling with the specified seeds.\n- For each case, compute the improvement as a float. Aggregate all improvements in order into a single output list.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3,r_4]$), where each $r_i$ is the improvement for case $i$ expressed as a decimal number.", "solution": "The problem statement is assessed to be valid. It is scientifically grounded in statistical learning theory, well-posed with all necessary parameters provided, and objective in its formulation. The task is to implement and evaluate a semi-parametric calibration method for anomaly detection scores based on the Neyman-Pearson testing paradigm.\n\nThe core principle is to transform the raw anomaly score $s$ into a more discriminative space defined by the likelihood ratio $\\Lambda(s) = \\frac{p(s|y=\\text{anomaly})}{p(s|y=\\text{normal})}$, where $y$ denotes the class label. The Neyman-Pearson lemma states that a test which rejects the null hypothesis ($y=\\text{normal}$) when $\\Lambda(s)$ exceeds a certain threshold is the most powerful test for a given significance level (false positive rate).\n\nOur approach is semi-parametric because we do not assume a single, simple form for the class-conditional densities $p(s|y)$. Instead, we model each density as a univariate Gaussian Mixture Model (GMM), a flexible and powerful choice for representing complex, multimodal distributions. The density of a score $s$ under a GMM with $K$ components is given by:\n$$p(s) = \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(s | \\mu_k, \\sigma_k^2)$$\nwhere $\\pi_k$ are the mixture weights ($\\sum \\pi_k = 1$), and $\\mu_k$ and $\\sigma_k^2$ are the mean and variance of the $k$-th Gaussian component $\\mathcal{N}$.\n\nThe parameters of these GMMs ($\\pi_k, \\mu_k, \\sigma_k$) are not known a priori and must be estimated from the provided score data for both the normal and anomaly classes. We employ the Expectation-Maximization (EM) algorithm, an iterative procedure for finding maximum likelihood estimates of parameters in statistical models with latent variables. In the context of GMMs, the latent variables are the component identities for each data point.\n\nThe EM algorithm proceeds in two steps:\n1.  **Expectation (E-step)**: Given the current parameter estimates, we compute the posterior probability, or \"responsibility,\" that each data point $s_i$ belongs to each component $k$. This is given by Bayes' theorem:\n    $$\\gamma_{ik} = \\frac{\\pi_k \\mathcal{N}(s_i | \\mu_k, \\sigma_k^2)}{\\sum_{j=1}^{K} \\pi_j \\mathcal{N}(s_i | \\mu_j, \\sigma_j^2)}$$\n2.  **Maximization (M-step)**: We update the model parameters using the computed responsibilities to maximize the expected log-likelihood. The updates are:\n    -   Effective number of points in component $k$: $N_k = \\sum_{i=1}^{N} \\gamma_{ik}$\n    -   New weight: $\\pi_k^{\\text{new}} = \\frac{N_k}{N}$\n    -   New mean: $\\mu_k^{\\text{new}} = \\frac{1}{N_k} \\sum_{i=1}^{N} \\gamma_{ik} s_i$\n    -   New variance: $(\\sigma_k^2)^{\\text{new}} = \\frac{1}{N_k} \\sum_{i=1}^{N} \\gamma_{ik} (s_i - \\mu_k^{\\text{new}})^2$\n\nThese steps are repeated until the log-likelihood of the data, $LL = \\sum_{i=1}^{N} \\log(\\sum_{k=1}^{K} \\pi_k \\mathcal{N}(s_i | \\mu_k, \\sigma_k^2))$, converges. For the special case of $K=1$, the EM algorithm is unnecessary, as the maximum likelihood estimates are simply the sample mean and sample variance.\n\nThe full procedure for each test case is as follows:\n1.  **Data Generation**: Using the specified random seed, generate `normal_scores` and `anomaly_scores` from their respective GMM definitions.\n2.  **Density Estimation**: Fit a GMM to `normal_scores` with $K_{\\text{normal}}$ components and another GMM to `anomaly_scores` with $K_{\\text{anomaly}}$ components using the EM algorithm. This yields the estimated densities $\\hat{p}(s|y=\\text{normal})$ and $\\hat{p}(s|y=\\text{anomaly})$.\n3.  **Baseline Evaluation (Raw Scores)**:\n    -   Determine a threshold $t$ as the $(1-\\alpha)$ quantile of the `normal_scores`. This ensures that the false positive rate on the training normal data is $\\alpha$.\n    -   Calculate the true positive rate, $TPR_{raw}$, as the fraction of `anomaly_scores` that are greater than $t$.\n4.  **Calibrated Evaluation (Likelihood Ratio)**:\n    -   Compute the estimated likelihood ratio $\\hat{\\Lambda}(s) = \\frac{\\hat{p}(s|y=\\text{anomaly})}{\\hat{p}(s|y=\\text{normal})}$ for each score in the `normal_scores` set. A small epsilon is added to the denominator to prevent division by zero.\n    -   Determine a threshold $\\tau$ as the $(1-\\alpha)$ quantile of these computed likelihood ratio values.\n    -   Compute $\\hat{\\Lambda}(s)$ for each score in the `anomaly_scores` set.\n    -   Calculate the calibrated true positive rate, $TPR_{calibrated}$, as the fraction of these anomaly likelihood ratios that are greater than $\\tau$.\n5.  **Performance Comparison**: The final output for the case is the improvement, defined as the difference $TPR_{calibrated} - TPR_{raw}$. This quantifies the gain in detection power at a fixed false positive rate achieved by the calibration procedure.\n\nThis process is repeated for all specified test cases to evaluate the method's effectiveness under different score distribution scenarios.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef generate_gmm_samples(weights, means, stds, n_samples):\n    \"\"\"Generates samples from a Gaussian Mixture Model.\"\"\"\n    component_choices = np.random.choice(len(weights), size=n_samples, p=weights)\n    samples = np.random.normal(loc=np.array(means)[component_choices], scale=np.array(stds)[component_choices])\n    return samples\n\ndef gmm_pdf(x, weights, means, stds):\n    \"\"\"Calculates the PDF of a GMM at points x.\"\"\"\n    pdf_val = np.zeros_like(x, dtype=float)\n    for w, m, s in zip(weights, means, stds):\n        pdf_val += w * norm.pdf(x, loc=m, scale=s)\n    return pdf_val\n\ndef fit_gmm_em(data, K, n_iter=150, tol=1e-6):\n    \"\"\"Fits a univariate GMM using the Expectation-Maximization algorithm.\"\"\"\n    n_samples = len(data)\n    \n    if K == 1:\n        mean = np.mean(data)\n        std = np.std(data)\n        if std  1e-6: std = 1e-6 # handle cases with identical data points\n        return np.array([1.0]), np.array([mean]), np.array([std])\n\n    # Initialization\n    weights = np.ones(K) / K\n    # Deterministic initialization based on data range\n    means = np.linspace(np.min(data), np.max(data), K)\n    stds = np.full(K, np.std(data))\n    \n    prev_log_likelihood = -np.inf\n    \n    for _ in range(n_iter):\n        # E-step: Calculate responsibilities\n        weighted_pdfs = np.zeros((n_samples, K))\n        for k in range(K):\n            # Prevent std dev from becoming zero\n            s_k = stds[k] if stds[k]  1e-6 else 1e-6\n            weighted_pdfs[:, k] = weights[k] * norm.pdf(data, means[k], s_k)\n        \n        total_likelihood_per_point = np.sum(weighted_pdfs, axis=1)\n        \n        # Add a small constant to prevent log(0) and division by zero\n        safe_total_likelihood = total_likelihood_per_point + 1e-9\n        responsibilities = weighted_pdfs / safe_total_likelihood[:, np.newaxis]\n        \n        # Check for convergence\n        log_likelihood = np.sum(np.log(safe_total_likelihood))\n        if abs(log_likelihood - prev_log_likelihood)  tol:\n            break\n        prev_log_likelihood = log_likelihood\n\n        # M-step: Update parameters\n        Nk = np.sum(responsibilities, axis=0)\n        safe_Nk = Nk + 1e-9 # Prevent division by zero\n        \n        weights = Nk / n_samples\n        means = np.sum(responsibilities * data[:, np.newaxis], axis=0) / safe_Nk\n        variances = np.sum(responsibilities * (data[:, np.newaxis] - means)**2, axis=0) / safe_Nk\n        stds = np.sqrt(variances)\n        \n        # Prevent component collapse\n        stds = np.maximum(stds, 1e-6)\n        \n    return weights, means, stds\n\ndef solve():\n    \"\"\"Main function to run the test suite and print results.\"\"\"\n    test_cases = [\n        # Case 1 (happy path, IF-like separation)\n        {'normal_w': [0.6, 0.4], 'normal_m': [0.22, 0.38], 'normal_s': [0.05, 0.04], 'n_normal': 300,\n         'anomaly_w': [1.0], 'anomaly_m': [0.72], 'anomaly_s': [0.06], 'n_anomaly': 30,\n         'k_normal': 2, 'k_anomaly': 1, 'alpha': 0.1, 'seed': 1},\n        # Case 2 (bimodal normal, central anomalies)\n        {'normal_w': [0.5, 0.5], 'normal_m': [0.15, 0.85], 'normal_s': [0.04, 0.06], 'n_normal': 300,\n         'anomaly_w': [1.0], 'anomaly_m': [0.50], 'anomaly_s': [0.07], 'n_anomaly': 30,\n         'k_normal': 2, 'k_anomaly': 1, 'alpha': 0.1, 'seed': 2},\n        # Case 3 (class imbalance and heteroscedastic anomalies)\n        {'normal_w': [1.0], 'normal_m': [0.30], 'normal_s': [0.05], 'n_normal': 1000,\n         'anomaly_w': [1.0], 'anomaly_m': [0.60], 'anomaly_s': [0.20], 'n_anomaly': 10,\n         'k_normal': 1, 'k_anomaly': 2, 'alpha': 0.05, 'seed': 3},\n        # Case 4 (near-identical distributions)\n        {'normal_w': [1.0], 'normal_m': [0.50], 'normal_s': [0.10], 'n_normal': 500,\n         'anomaly_w': [1.0], 'anomaly_m': [0.55], 'anomaly_s': [0.10], 'n_anomaly': 50,\n         'k_normal': 1, 'k_anomaly': 1, 'alpha': 0.1, 'seed': 4},\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        np.random.seed(case['seed'])\n        \n        # 1. Generate data\n        normal_scores = generate_gmm_samples(case['normal_w'], case['normal_m'], case['normal_s'], case['n_normal'])\n        anomaly_scores = generate_gmm_samples(case['anomaly_w'], case['anomaly_m'], case['anomaly_s'], case['n_anomaly'])\n        \n        # 2. Fit GMMs\n        w_n, m_n, s_n = fit_gmm_em(normal_scores, case['k_normal'])\n        w_a, m_a, s_a = fit_gmm_em(anomaly_scores, case['k_anomaly'])\n        \n        # 3. Baseline TPR (Raw Score Thresholding)\n        raw_score_threshold = np.quantile(normal_scores, 1 - case['alpha'])\n        raw_tpr = np.mean(anomaly_scores  raw_score_threshold)\n        \n        # 4. Calibrated TPR (Likelihood-Ratio Thresholding)\n        # Denominator for likelihood ratio, with a small epsilon for stability\n        epsilon = 1e-9\n        \n        # Calculate LR on normal scores to find the threshold\n        p_normal_on_normal = gmm_pdf(normal_scores, w_n, m_n, s_n)\n        p_anomaly_on_normal = gmm_pdf(normal_scores, w_a, m_a, s_a)\n        lr_on_normal = p_anomaly_on_normal / (p_normal_on_normal + epsilon)\n        calibrated_threshold = np.quantile(lr_on_normal, 1 - case['alpha'])\n        \n        # Calculate LR on anomaly scores to get TPR\n        p_normal_on_anomaly = gmm_pdf(anomaly_scores, w_n, m_n, s_n)\n        p_anomaly_on_anomaly = gmm_pdf(anomaly_scores, w_a, m_a, s_a)\n        lr_on_anomaly = p_anomaly_on_anomaly / (p_normal_on_anomaly + epsilon)\n        calibrated_tpr = np.mean(lr_on_anomaly  calibrated_threshold)\n        \n        # 5. Compute improvement and store result\n        improvement = calibrated_tpr - raw_tpr\n        results.append(improvement)\n\n    print(f\"[{','.join(f'{r:.12f}' for r in results)}]\")\n\nsolve()\n```", "id": "3099077"}]}