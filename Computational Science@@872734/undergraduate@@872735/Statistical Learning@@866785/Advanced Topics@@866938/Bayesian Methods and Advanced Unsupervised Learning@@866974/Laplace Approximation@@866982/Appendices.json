{"hands_on_practices": [{"introduction": "The power of the Laplace approximation stems from its accuracy when the posterior distribution is nearly Gaussian. This first exercise provides a theoretical lens to understand why this is the case. We will analyze a model with a Gaussian likelihood and a prior that is only slightly perturbed from a Gaussian, allowing us to see analytically how the approximation holds up and how small deviations from normality can be quantified [@problem_id:3137251]. This practice will sharpen your skills with Taylor expansions and build a solid intuition for the conditions under which the Laplace approximation truly shines.", "problem": "Consider a one-parameter Bayesian model in statistical learning where data points $y_{1}, y_{2}, \\dots, y_{n}$ are conditionally independent and identically distributed given a parameter $\\theta$, with a Gaussian likelihood $p(y_{i} \\mid \\theta) = \\mathcal{N}(y_{i}; \\theta, \\sigma^{2})$ where $\\sigma^{2} > 0$ is known. Let the prior on $\\theta$ be mildly non-Gaussian, defined via its logarithm as\n$$\n\\ln p(\\theta) = -\\frac{\\theta^{2}}{2 \\tau^{2}} - \\epsilon \\, \\frac{\\theta^{4}}{4 \\tau^{4}} + C,\n$$\nwhere $\\tau^{2} > 0$ is known, $0 < \\epsilon \\ll 1$ is a small dimensionless parameter that perturbs the Gaussian prior toward lighter tails, and $C$ is a normalizing constant. The posterior is $p(\\theta \\mid y_{1:n}) \\propto p(y_{1:n} \\mid \\theta) \\, p(\\theta)$.\n\nStarting from Bayes’ rule and the definition of the Maximum a Posteriori (MAP) estimator, use the Laplace approximation to characterize the posterior near its mode by Taylor expanding the log-posterior around the baseline Gaussian case $\\epsilon = 0$. Derive the first-order (in $\\epsilon$) correction to the MAP estimator $\\theta^{*}$ relative to the Gaussian-prior baseline, by solving the stationarity condition for the log-posterior and expanding in powers of $\\epsilon$.\n\nAdditionally, use the Taylor expansion to identify the scaling in $\\epsilon$ of the first neglected terms in the Laplace approximation (involving third and fourth derivatives of the log-posterior) evaluated at the baseline mode, and explain why they are negligible when $\\epsilon$ is small and the combined precision is not vanishingly small.\n\nYour final answer should be a single closed-form analytic expression for the MAP estimator $\\theta^{*}$ accurate to first order in $\\epsilon$, expressed in terms of $n$, $\\sigma^{2}$, $\\tau^{2}$, $\\bar{y} = \\frac{1}{n} \\sum_{i=1}^{n} y_{i}$, and $\\epsilon$. No numerical rounding is required.", "solution": "We begin with the fundamental definitions. The likelihood under the Gaussian sampling model is\n$$\np(y_{1:n} \\mid \\theta) = \\prod_{i=1}^{n} \\mathcal{N}(y_{i}; \\theta, \\sigma^{2}),\n$$\nwhose log-likelihood is\n$$\n\\ln p(y_{1:n} \\mid \\theta) = -\\frac{1}{2 \\sigma^{2}} \\sum_{i=1}^{n} (y_{i} - \\theta)^{2} + \\text{const} = -\\frac{n}{2 \\sigma^{2}} (\\theta - \\bar{y})^{2} + \\text{const},\n$$\nwhere $\\bar{y} = \\frac{1}{n} \\sum_{i=1}^{n} y_{i}$, and we drop additive constants that do not depend on $\\theta$. The prior is specified via\n$$\n\\ln p(\\theta) = -\\frac{\\theta^{2}}{2 \\tau^{2}} - \\epsilon \\, \\frac{\\theta^{4}}{4 \\tau^{4}} + C,\n$$\nand we again drop the additive constant $C$ in derivatives with respect to $\\theta$.\n\nLet $A = \\frac{n}{\\sigma^{2}}$ denote the likelihood precision and $B = \\frac{1}{\\tau^{2}}$ denote the prior precision. The log-posterior up to an additive constant is\n$$\nf(\\theta) \\equiv \\ln p(y_{1:n} \\mid \\theta) + \\ln p(\\theta)\n= -\\frac{A}{2} (\\theta - \\bar{y})^{2} - \\frac{B}{2} \\theta^{2} - \\epsilon \\, \\frac{B^{2}}{4} \\theta^{4}.\n$$\nThe Maximum a Posteriori (MAP) estimator $\\theta^{*}$ is defined as the maximizer of $f(\\theta)$, equivalently the stationary point satisfying $f'(\\theta^{*}) = 0$ with negative second derivative ensuring a maximum.\n\nCompute the first derivative:\n$$\nf'(\\theta) = -A(\\theta - \\bar{y}) - B \\theta - \\epsilon \\, B^{2} \\theta^{3}.\n$$\nThe stationarity condition $f'(\\theta^{*}) = 0$ is\n$$\nA \\bar{y} - (A + B) \\theta^{*} - \\epsilon \\, B^{2} (\\theta^{*})^{3} = 0.\n$$\nWe solve this perturbatively for small $\\epsilon$ by expanding\n$$\n\\theta^{*} = \\mu_{0} + \\epsilon \\, \\mu_{1} + \\mathcal{O}(\\epsilon^{2}),\n$$\nand substituting into the stationarity equation. Using $(\\theta^{*})^{3} = \\mu_{0}^{3} + 3 \\epsilon \\, \\mu_{0}^{2} \\mu_{1} + \\mathcal{O}(\\epsilon^{2})$, the equation becomes\n$$\nA \\bar{y} - (A + B) (\\mu_{0} + \\epsilon \\mu_{1}) - \\epsilon \\, B^{2} \\left( \\mu_{0}^{3} + \\mathcal{O}(\\epsilon) \\right) = 0.\n$$\nCollecting terms by powers of $\\epsilon$, the zeroth-order equation is\n$$\nA \\bar{y} - (A + B) \\mu_{0} = 0 \\quad \\Rightarrow \\quad \\mu_{0} = \\frac{A}{A + B} \\, \\bar{y}.\n$$\nThe first-order equation is\n$$\n-(A + B) \\mu_{1} - B^{2} \\mu_{0}^{3} = 0 \\quad \\Rightarrow \\quad \\mu_{1} = -\\frac{B^{2}}{A + B} \\, \\mu_{0}^{3}.\n$$\nSubstitute $\\mu_{0} = \\frac{A}{A + B} \\bar{y}$ to obtain\n$$\n\\mu_{1} = -\\frac{B^{2}}{A + B} \\left( \\frac{A}{A + B} \\right)^{3} \\bar{y}^{3}\n= -\\frac{A^{3} B^{2}}{(A + B)^{4}} \\, \\bar{y}^{3}.\n$$\nTherefore, the MAP estimator to first order in $\\epsilon$ is\n$$\n\\theta^{*} = \\frac{A}{A + B} \\, \\bar{y} \\; - \\; \\epsilon \\, \\frac{A^{3} B^{2}}{(A + B)^{4}} \\, \\bar{y}^{3} \\; + \\; \\mathcal{O}(\\epsilon^{2}).\n$$\nReturning to the original parameters $A = \\frac{n}{\\sigma^{2}}$ and $B = \\frac{1}{\\tau^{2}}$, we have\n$$\n\\theta^{*} = \\frac{\\frac{n}{\\sigma^{2}}}{\\frac{n}{\\sigma^{2}} + \\frac{1}{\\tau^{2}}} \\, \\bar{y}\n\\; - \\; \\epsilon \\, \\frac{\\left( \\frac{n}{\\sigma^{2}} \\right)^{3} \\left( \\frac{1}{\\tau^{2}} \\right)^{2}}{\\left( \\frac{n}{\\sigma^{2}} + \\frac{1}{\\tau^{2}} \\right)^{4}} \\, \\bar{y}^{3}\n\\; + \\; \\mathcal{O}(\\epsilon^{2}).\n$$\n\nTo assess the negligible correction terms in the Laplace approximation, we examine higher derivatives of $f(\\theta)$ near the baseline mode $\\mu_{0}$. The second, third, and fourth derivatives are\n$$\nf''(\\theta) = - (A + B) - 3 \\epsilon \\, B^{2} \\theta^{2}, \\quad\nf^{(3)}(\\theta) = - 6 \\epsilon \\, B^{2} \\theta, \\quad\nf^{(4)}(\\theta) = - 6 \\epsilon \\, B^{2}.\n$$\nAt $\\theta = \\mu_{0}$, the Hessian magnitude is\n$$\nH_{0} \\equiv - f''(\\mu_{0}) = (A + B) + 3 \\epsilon \\, B^{2} \\mu_{0}^{2},\n$$\nwhose leading term is $A + B$ and the relative correction is $\\frac{3 \\epsilon \\, B^{2} \\mu_{0}^{2}}{A + B} = \\mathcal{O}(\\epsilon)$. Dimensionless measures of non-Gaussianity that govern Laplace error terms include\n$$\nR_{3} \\equiv \\frac{| f^{(3)}(\\mu_{0}) |}{| f''(\\mu_{0}) |^{3/2}} \\approx \\frac{6 \\epsilon \\, B^{2} | \\mu_{0} |}{(A + B)^{3/2}},\n\\quad\nR_{4} \\equiv \\frac{| f^{(4)}(\\mu_{0}) |}{| f''(\\mu_{0}) |^{2}} \\approx \\frac{6 \\epsilon \\, B^{2}}{(A + B)^{2}}.\n$$\nBoth $R_{3}$ and $R_{4}$ are of order $\\epsilon$ when $A + B$ is not vanishingly small, demonstrating that higher-order corrections in the Laplace approximation are negligible for small $\\epsilon$. Consequently, the first-order MAP correction derived above reliably captures the effect of the mildly non-Gaussian prior, and Laplace’s Gaussian approximation around the mode remains remarkably accurate in this setting.\n\nThe requested final expression is the MAP estimator $\\theta^{*}$ to first order in $\\epsilon$.", "answer": "$$\\boxed{\\frac{\\frac{n}{\\sigma^{2}}}{\\frac{n}{\\sigma^{2}} + \\frac{1}{\\tau^{2}}} \\,\\bar{y} \\; - \\; \\epsilon \\, \\frac{\\left( \\frac{n}{\\sigma^{2}} \\right)^{3} \\left( \\frac{1}{\\tau^{2}} \\right)^{2}}{\\left( \\frac{n}{\\sigma^{2}} + \\frac{1}{\\tau^{2}} \\right)^{4}} \\, \\bar{y}^{3}}$$", "id": "3137251"}, {"introduction": "Real-world models often involve multiple parameters that are not independent; their uncertainty is correlated. A major advantage of the Laplace approximation is its ability to capture this covariance structure through its Hessian-derived covariance matrix. In this practice, you will work with a two-parameter Bayesian linear regression model and deliberately induce correlation between the parameters [@problem_id:3137211]. By comparing the full Laplace approximation to a simpler, factorized approximation, you'll gain a concrete understanding of why capturing the posterior's elliptical shape is crucial for accurate uncertainty quantification.", "problem": "You must write a complete, runnable program that constructs a two-parameter Bayesian posterior with strong correlation, computes the Laplace approximation’s covariance and correlation, and compares it to an independent Gaussian variational approximation by evaluating the Kullback–Leibler divergence (KL). Work entirely in a purely mathematical setting using the following generative model as the fundamental base.\n\nConsider a Bayesian linear model with two parameters. For given data matrix $X \\in \\mathbb{R}^{n \\times 2}$ and response vector $y \\in \\mathbb{R}^{n}$, assume the likelihood is Gaussian with known variance and the prior is isotropic Gaussian:\n- Likelihood: $y \\mid \\theta \\sim \\mathcal{N}(X \\theta, \\sigma^{2} I)$ where $\\theta \\in \\mathbb{R}^{2}$ and $I$ is the identity matrix.\n- Prior: $\\theta \\sim \\mathcal{N}(0, \\tau^{2} I)$.\n\nYou must derive, from Bayes’ rule and standard identities of the multivariate normal distribution, the form of the posterior $p(\\theta \\mid y)$, the form of the Laplace approximation about its mode, and the best independent Gaussian variational approximation $q(\\theta)$ that minimizes the Kullback–Leibler divergence (KL) from $q(\\theta)$ to the Laplace approximation. You must not assume any shortcut formulas beyond these foundational facts and identities.\n\nData generation for each test case should be performed as follows:\n- Draw $x_{1} \\in \\mathbb{R}^{n}$ with independent and identically distributed standard normal entries.\n- Draw $z \\in \\mathbb{R}^{n}$ with independent and identically distributed standard normal entries, independent of $x_{1}$.\n- Construct $x_{2} = \\rho x_{1} + \\sqrt{1 - \\rho^{2}} \\, z$ to control collinearity via $\\rho \\in (-1, 1)$.\n- Form $X = [x_{1}, x_{2}] \\in \\mathbb{R}^{n \\times 2}$.\n- Fix a true parameter vector $\\theta_{\\text{true}} \\in \\mathbb{R}^{2}$, generate noise $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I)$, and set $y = X \\theta_{\\text{true}} + \\varepsilon$.\n\nYour program must:\n- Compute the Laplace approximation at the maximum a posteriori (MAP) point. In this model the Laplace covariance at the mode equals the inverse of the posterior precision (negative Hessian) at the mode.\n- Compute the posterior correlation under the Laplace covariance, namely the correlation between the two coordinates of $\\theta$ implied by the Laplace covariance.\n- Compute the best independent Gaussian variational approximation $q(\\theta) = \\mathcal{N}(\\mu, \\operatorname{diag}(s^{2}))$ that minimizes the Kullback–Leibler divergence (KL) from $q(\\theta)$ to the Laplace approximation, and then evaluate this KL.\n- Report, for each test case, two floats: the Laplace posterior correlation and the KL value of the best independent Gaussian variational approximation relative to the Laplace approximation.\n\nAngles do not appear in this problem. No physical units are involved; report pure numbers. All final numerical outputs should be rounded to $6$ decimal places.\n\nTest Suite:\nUse the following test cases, where $n$ is the number of samples, $\\rho$ controls feature collinearity, $\\sigma$ is the noise standard deviation, $\\tau$ is the prior standard deviation, and the final element is the random seed to ensure reproducibility. Use the fixed true parameter $\\theta_{\\text{true}} = [1.5, -1.0]^{\\top}$ in all cases.\n\n- Case $1$: $(n, \\rho, \\sigma, \\tau, \\text{seed}) = (200, 0.95, 0.5, 3.0, 0)$\n- Case $2$: $(n, \\rho, \\sigma, \\tau, \\text{seed}) = (200, 0.50, 0.5, 3.0, 1)$\n- Case $3$: $(n, \\rho, \\sigma, \\tau, \\text{seed}) = (200, 0.995, 0.5, 3.0, 2)$\n\nRequired final output format:\n- Your program should produce a single line of output containing a single list with $2 \\times 3 = 6$ floats: for each test case in the order given above, append the Laplace posterior correlation, then the KL value. The final output must be a single line in the exact format\n- Example shape (not actual values): $[c_{1},k_{1},c_{2},k_{2},c_{3},k_{3}]$\n- Each value must be rounded to $6$ decimal places.", "solution": "The problem is valid as it is scientifically grounded, well-posed, objective, and self-contained. We will proceed with a full derivation and solution.\n\n### 1. Posterior Distribution Derivation\n\nThe analysis begins with Bayes' rule to find the posterior distribution of the parameters $\\theta$. The posterior is proportional to the product of the likelihood and the prior:\n$$\np(\\theta | y, X) \\propto p(y | \\theta, X) p(\\theta)\n$$\nThe likelihood and prior are given as Gaussian distributions:\n-   Likelihood: $p(y | \\theta, X) = \\mathcal{N}(y | X\\theta, \\sigma^2 I) \\propto \\exp\\left(-\\frac{1}{2\\sigma^2}(y - X\\theta)^T(y - X\\theta)\\right)$\n-   Prior: $p(\\theta) = \\mathcal{N}(\\theta | 0, \\tau^2 I) \\propto \\exp\\left(-\\frac{1}{2\\tau^2}\\theta^T\\theta\\right)$\n\nThe logarithm of the posterior distribution is therefore:\n$$\n\\log p(\\theta | y, X) = -\\frac{1}{2\\sigma^2}(y - X\\theta)^T(y - X\\theta) - \\frac{1}{2\\tau^2}\\theta^T\\theta + C\n$$\nwhere $C$ is a normalization constant independent of $\\theta$. To identify the form of the posterior, we expand the terms and group by powers of $\\theta$:\n$$\n\\log p(\\theta | y, X) = -\\frac{1}{2\\sigma^2}(y^Ty - 2y^TX\\theta + \\theta^T X^T X \\theta) - \\frac{1}{2\\tau^2}\\theta^T\\theta + C\n$$\n$$\n\\log p(\\theta | y, X) = -\\frac{1}{2}\\left( \\theta^T \\left(\\frac{1}{\\sigma^2}X^TX + \\frac{1}{\\tau^2}I\\right) \\theta - 2\\theta^T \\left(\\frac{1}{\\sigma^2}X^Ty\\right) \\right) + C'\n$$\nThis expression is a quadratic form in $\\theta$, which indicates that the posterior distribution is also a multivariate Gaussian, $p(\\theta | y, X) = \\mathcal{N}(\\theta | \\mu_p, \\Sigma_p)$. The log-density of a general multivariate Gaussian $\\mathcal{N}(\\mu, \\Sigma)$ is $-\\frac{1}{2}(\\theta-\\mu)^T\\Sigma^{-1}(\\theta-\\mu) + \\text{const} = -\\frac{1}{2}(\\theta^T\\Sigma^{-1}\\theta - 2\\theta^T\\Sigma^{-1}\\mu + \\text{const})$.\n\nBy comparing the terms in the log-posterior with the general form, we can identify the posterior precision matrix $\\Lambda_p = \\Sigma_p^{-1}$ and the posterior mean $\\mu_p$:\n$$\n\\Lambda_p = \\Sigma_p^{-1} = \\frac{1}{\\sigma^2}X^TX + \\frac{1}{\\tau^2}I\n$$\n$$\n\\Lambda_p \\mu_p = \\frac{1}{\\sigma^2}X^Ty \\implies \\mu_p = \\Lambda_p^{-1}\\left(\\frac{1}{\\sigma^2}X^Ty\\right) = \\left(\\frac{1}{\\sigma^2}X^TX + \\frac{1}{\\tau^2}I\\right)^{-1}\\left(\\frac{1}{\\sigma^2}X^Ty\\right)\n$$\nThus, the posterior is $p(\\theta | y, X) = \\mathcal{N}(\\theta | \\mu_p, \\Lambda_p^{-1})$.\n\n### 2. Laplace Approximation and Posterior Correlation\n\nThe Laplace approximation provides a Gaussian approximation to a posterior distribution, centered at the mode of the posterior (the Maximum a Posteriori or MAP estimate, $\\theta_{\\text{MAP}}$). The covariance of this Gaussian is given by the negative inverse of the Hessian of the log-posterior evaluated at the mode.\n\nFirst, we find the mode by setting the gradient of the log-posterior with respect to $\\theta$ to zero:\n$$\n\\nabla_\\theta \\log p(\\theta | y, X) = \\frac{1}{\\sigma^2}(X^Ty - X^TX\\theta) - \\frac{1}{\\tau^2}\\theta = 0\n$$\nSolving for $\\theta$ gives $\\theta_{\\text{MAP}} = \\mu_p$. As the posterior is exactly Gaussian, its mode is equal to its mean.\n\nNext, we calculate the Hessian (the matrix of second derivatives):\n$$\n\\nabla_\\theta^2 \\log p(\\theta | y, X) = -\\frac{1}{\\sigma^2}X^TX - \\frac{1}{\\tau^2}I = -\\Lambda_p\n$$\nThe Hessian is constant and does not depend on $\\theta$. The covariance of the Laplace approximation, $\\Sigma_L$, is:\n$$\n\\Sigma_L = \\left(-\\nabla_\\theta^2 \\log p(\\theta | y, X)\\Big|_{\\theta_{\\text{MAP}}}\\right)^{-1} = (\\Lambda_p)^{-1} = \\Sigma_p\n$$\nFor this specific model, the Laplace approximation is not an approximation; it is the exact posterior distribution, $q_L(\\theta) = p(\\theta | y, X)$. The required posterior correlation is computed from this exact covariance matrix $\\Sigma_L = \\Sigma_p$. For a $2 \\times 2$ covariance matrix $\\Sigma_L = \\begin{pmatrix} \\Sigma_{11} & \\Sigma_{12} \\\\ \\Sigma_{21} & \\Sigma_{22} \\end{pmatrix}$, the correlation is:\n$$\n\\rho_{12} = \\frac{\\Sigma_{12}}{\\sqrt{\\Sigma_{11}\\Sigma_{22}}}\n$$\n\n### 3. Variational Approximation and KL Divergence\n\nWe seek the best independent Gaussian variational approximation $q(\\theta) = q_1(\\theta_1)q_2(\\theta_2) = \\mathcal{N}(\\theta | \\mu_q, \\Sigma_q)$, where $\\Sigma_q$ is a diagonal matrix. The objective is to minimize the Kullback-Leibler (KL) divergence from $q(\\theta)$ to the true posterior $p(\\theta|y,X)$, which is equivalent to the Laplace approximation $q_L(\\theta)$ in this case. The objective is to minimize $\\text{KL}(q || q_L)$.\n\nThe general formula for the KL divergence between two multivariate Gaussian distributions $q=\\mathcal{N}(\\mu_q, \\Sigma_q)$ and $p=\\mathcal{N}(\\mu_p, \\Sigma_p)$ is:\n$$\n\\text{KL}(q || p) = \\frac{1}{2} \\left[ \\log\\frac{|\\Sigma_p|}{|\\Sigma_q|} - d + \\text{tr}(\\Sigma_p^{-1}\\Sigma_q) + (\\mu_p - \\mu_q)^T\\Sigma_p^{-1}(\\mu_p - \\mu_q) \\right]\n$$\nwhere $d$ is the dimension ($d=2$ here). The term involving the means is minimized at zero when $\\mu_q = \\mu_p$.\n\nFor the covariance, the standard result from mean-field variational inference for a Gaussian target $p(\\theta) = \\mathcal{N}(\\mu_p, \\Sigma_p)$ is that the optimal factorized distribution $q(\\theta) = \\mathcal{N}(\\mu_q, \\Sigma_q)$ with $\\Sigma_q$ being diagonal has $\\mu_q = \\mu_p$ and a precision matrix $\\Lambda_q = \\Sigma_q^{-1}$ equal to the diagonal of the target's precision matrix, $\\Lambda_q = \\text{diag}(\\Lambda_p)$. That is, $\\Sigma_q = (\\text{diag}(\\Lambda_p))^{-1}$.\n\nWith $\\mu_q = \\mu_p$, the KL divergence simplifies to:\n$$\n\\text{KL}(q || p) = \\frac{1}{2} \\left[ \\log\\frac{|\\Sigma_p|}{|\\Sigma_q|} - 2 + \\text{tr}(\\Sigma_p^{-1}\\Sigma_q) \\right]\n$$\nLet's evaluate the terms. Let $\\Lambda_p = \\Sigma_p^{-1}$. The variational precision is $\\Lambda_q = \\text{diag}(\\Lambda_{p,11}, \\Lambda_{p,22})$.\nThe trace term becomes:\n$$\n\\text{tr}(\\Sigma_p^{-1}\\Sigma_q) = \\text{tr}(\\Lambda_p \\Lambda_q^{-1}) = \\text{tr}\\left( \\begin{pmatrix} \\Lambda_{p,11} & \\Lambda_{p,12} \\\\ \\Lambda_{p,21} & \\Lambda_{p,22} \\end{pmatrix} \\begin{pmatrix} 1/\\Lambda_{p,11} & 0 \\\\ 0 & 1/\\Lambda_{p,22} \\end{pmatrix} \\right) = \\text{tr}\\begin{pmatrix} 1 & \\dots \\\\ \\dots & 1 \\end{pmatrix} = 2\n$$\nThe log-determinant ratio is:\n$$\n\\frac{|\\Sigma_p|}{|\\Sigma_q|} = \\frac{|\\Lambda_q|}{|\\Lambda_p|} = \\frac{\\Lambda_{p,11}\\Lambda_{p,22}}{\\Lambda_{p,11}\\Lambda_{p,22} - \\Lambda_{p,12}^2} = \\frac{1}{1 - \\frac{\\Lambda_{p,12}^2}{\\Lambda_{p,11}\\Lambda_{p,22}}}\n$$\nThe correlation computed from the covariance matrix $\\Sigma_p$ is $\\rho_{12}$. The correlation computed from the precision matrix $\\Lambda_p$ is $\\rho_{\\Lambda,12} = \\frac{\\Lambda_{p,12}}{\\sqrt{\\Lambda_{p,11}\\Lambda_{p,22}}}$. For a $2 \\times 2$ matrix, it holds that $\\rho_{12} = -\\rho_{\\Lambda,12}$. Therefore, $\\rho_{12}^2 = \\rho_{\\Lambda,12}^2$.\n$$\n\\frac{|\\Sigma_p|}{|\\Sigma_q|} = \\frac{1}{1 - \\rho_{12}^2}\n$$\nSubstituting these into the KL formula yields a remarkably simple result:\n$$\n\\text{KL}(q || p) = \\frac{1}{2} \\left[ \\log\\left(\\frac{1}{1 - \\rho_{12}^2}\\right) - 2 + 2 \\right] = -\\frac{1}{2}\\log(1 - \\rho_{12}^2)\n$$\nThe KL divergence, which measures the inadequacy of the factorized approximation, depends only on the squared posterior correlation of the parameters.\n\n### 4. Algorithm\n\nFor each test case:\n1.  Set the random seed. Generate the data matrix $X \\in \\mathbb{R}^{n \\times 2}$ with correlated columns according to the specified procedure with parameters $n$ and $\\rho$.\n2.  Compute the posterior precision matrix $\\Lambda_p = \\frac{1}{\\sigma^2}X^T X + \\frac{1}{\\tau^2}I$.\n3.  Invert $\\Lambda_p$ to find the posterior covariance matrix $\\Sigma_p = \\Sigma_L = \\Lambda_p^{-1}$.\n4.  Extract the elements $\\Sigma_{11}$, $\\Sigma_{22}$, and $\\Sigma_{12}$ from $\\Sigma_L$.\n5.  Calculate the posterior correlation $\\rho_{12} = \\Sigma_{12} / \\sqrt{\\Sigma_{11}\\Sigma_{22}}$.\n6.  Calculate the KL divergence of the best independent Gaussian variational approximation as $\\text{KL} = -0.5 \\log(1 - \\rho_{12}^2)$.\n7.  Round both results to $6$ decimal places and store them.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test cases.\n    For a Bayesian linear model with Gaussian likelihood and prior, the posterior\n    is also Gaussian. This function computes its properties.\n    \"\"\"\n    \n    # Test cases: (n, rho, sigma, tau, seed)\n    test_cases = [\n        (200, 0.95, 0.5, 3.0, 0),\n        (200, 0.50, 0.5, 3.0, 1),\n        (200, 0.995, 0.5, 3.0, 2),\n    ]\n\n    # Fixed true parameter vector for data generation\n    theta_true = np.array([1.5, -1.0])\n\n    results = []\n    for case in test_cases:\n        n, rho, sigma, tau, seed = case\n\n        # 1. Data Generation\n        # Set a random number generator with a seed for reproducibility.\n        rng = np.random.default_rng(seed)\n\n        # Generate x1 and z from standard normal distributions.\n        x1 = rng.standard_normal(n)\n        z = rng.standard_normal(n)\n        \n        # Construct x2 to have a specified correlation rho with x1.\n        x2 = rho * x1 + np.sqrt(1 - rho**2) * z\n        \n        # Form the data matrix X.\n        X = np.stack([x1, x2], axis=1) # Shape: (n, 2)\n        \n        # Generate response vector y. Note: y is not needed for the posterior\n        # covariance, correlation, or the KL divergence, as these depend only\n        # on X, sigma, and tau in this model.\n        # eps = rng.normal(0, sigma, n)\n        # y = X @ theta_true + eps\n\n        # 2. Compute Posterior Precision and Covariance (Laplace Approximation)\n        # The posterior precision matrix is Lambda_p = (1/sigma^2) * X'X + (1/tau^2) * I\n        XtX = X.T @ X\n        lambda_p = (1 / sigma**2) * XtX + (1 / tau**2) * np.eye(2)\n        \n        # The posterior covariance matrix is the inverse of the precision matrix.\n        # This is also the covariance of the Laplace approximation.\n        sigma_l = np.linalg.inv(lambda_p)\n\n        # 3. Compute Posterior Correlation\n        # Extract elements of the covariance matrix.\n        sigma_11 = sigma_l[0, 0]\n        sigma_22 = sigma_l[1, 1]\n        sigma_12 = sigma_l[0, 1]\n        \n        # Calculate the correlation coefficient.\n        correlation = sigma_12 / np.sqrt(sigma_11 * sigma_22)\n\n        # 4. Compute KL Divergence\n        # The KL divergence for the best independent Gaussian variational approximation\n        # has a simple closed form related to the posterior correlation.\n        kl_divergence = -0.5 * np.log(1 - correlation**2)\n\n        # Append rounded results to the list.\n        results.append(round(correlation, 6))\n        results.append(round(kl_divergence, 6))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3137211"}, {"introduction": "While powerful, the Laplace approximation has important limitations that every practitioner must recognize. This final exercise explores one of the most common pitfalls: its performance on skewed posterior distributions. Using a conjugate Poisson-Gamma model, where the true posterior is a skewed Gamma distribution, you will see a clear mismatch between the posterior's mode, where the symmetric Laplace approximation is centered, and its mean [@problem_id:3137250]. This discrepancy has significant consequences, leading to inaccurate estimates of posterior expectations and expected loss, providing a critical lesson on when to be cautious with this approximation.", "problem": "Consider the following Bayesian one-parameter model, where independent observations are drawn from a Poisson distribution and the prior is Gamma. Let $\\{y_i\\}_{i=1}^n$ be independent draws from $\\text{Poisson}(\\lambda)$ with unknown rate $\\lambda > 0$, and use a Gamma prior $\\lambda \\sim \\text{Gamma}(a,b)$ with shape $a > 0$ and rate $b > 0$. Use Bayes' theorem together with the definitions of the Poisson likelihood and Gamma prior as the fundamental base. The Maximum A Posteriori (MAP) estimator is the posterior mode. The Laplace approximation approximates the posterior density near the MAP by a Gaussian distribution centered at the mode, with variance equal to the inverse of the negative second derivative (the negative Hessian for scalar) of the log-posterior at the mode.\n\nYour tasks:\n- Derive the posterior density up to a normalizing constant and express it in a recognized parametric form.\n- Derive the posterior mean and the posterior mode by differentiating the log-posterior.\n- Using the Laplace approximation centered at the posterior mode, derive the scalar Gaussian variance from the second derivative of the log-posterior at the mode.\n- For the convex risk given by the squared loss $L(\\lambda; t) = (\\lambda - t)^2$, compute the expected loss under the exact posterior and under the Laplace approximation. Use only the definitions of expectation for these distributions and quantities you derived.\n- Implement a program that, for each test case in the test suite below, computes the absolute difference between the exact expected loss and the Laplace-approximated expected loss.\n\nTest suite:\n- Case $1$: Prior $(a,b) = (2.5, 1.0)$, data $y = (0,1,2)$, loss target $t$ equal to the posterior mode of this case.\n- Case $2$: Prior $(a,b) = (0.3, 1.0)$, data $y = (1)$, loss target $t$ equal to the sample mean of the data for this case.\n- Case $3$: Prior $(a,b) = (2.0, 1.0)$, data $y = (10,12,9,11,8)$, loss target $t = 10.0$.\n- Case $4$: Prior $(a,b) = (3.0, 0.5)$, data $y = (0,0,0,1)$, loss target $t = 0.8$.\n\nProgram requirements:\n- Compute, for each case, the posterior shape $\\alpha$ and rate $\\beta$, the posterior mean and mode, the Laplace variance at the mode, and the expected squared loss under the exact posterior and under the Laplace approximation.\n- For each case, output the absolute difference between the exact expected loss and the Laplace-approximated expected loss as a floating-point number.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each number rounded to six decimal places (e.g., $[0.123456,0.000001,2.718282,1.000000]$).\n\nAll mathematical symbols, variables, functions, operators, and numbers in this problem statement are written using LaTeX. There are no physical units, angles, or percentages involved in this problem; all numerical outputs must be real numbers without units.", "solution": "The problem requires a thorough analysis of a Bayesian one-parameter model involving a Poisson likelihood and a Gamma prior. This framework is a classic example of conjugate families in Bayesian statistics. We will proceed by first validating the problem statement, which is found to be sound, and then systematically deriving the required quantities.\n\nThe model is specified as follows:\n- The data $\\{y_i\\}_{i=1}^n$ are independent and identically distributed draws from a Poisson distribution with parameter $\\lambda$: $y_i \\mid \\lambda \\sim \\text{Poisson}(\\lambda)$.\n- The prior distribution for the unknown parameter $\\lambda$ is a Gamma distribution with shape parameter $a$ and rate parameter $b$: $\\lambda \\sim \\text{Gamma}(a, b)$.\n\nThe probability mass function (PMF) for a Poisson distribution is $P(y \\mid \\lambda) = \\frac{e^{-\\lambda}\\lambda^y}{y!}$ for $y \\in \\{0, 1, 2, \\dots\\}$. The probability density function (PDF) for a Gamma distribution is $p(\\lambda \\mid a, b) = \\frac{b^a}{\\Gamma(a)}\\lambda^{a-1}e^{-b\\lambda}$ for $\\lambda > 0$.\n\nFirst, we derive the posterior density of $\\lambda$. By Bayes' theorem, the posterior density is proportional to the product of the likelihood and the prior density:\n$$ p(\\lambda \\mid y_1, \\dots, y_n) \\propto P(y_1, \\dots, y_n \\mid \\lambda) \\, p(\\lambda) $$\nGiven that the observations are independent, the likelihood is the product of the individual PMFs:\n$$ P(y_1, \\dots, y_n \\mid \\lambda) = \\prod_{i=1}^n \\frac{e^{-\\lambda}\\lambda^{y_i}}{y_i!} = \\frac{e^{-n\\lambda} \\lambda^{\\sum_{i=1}^n y_i}}{\\prod_{i=1}^n y_i!} $$\nAs a function of $\\lambda$, the likelihood is proportional to $e^{-n\\lambda} \\lambda^{\\sum y_i}$. The prior density is proportional to $\\lambda^{a-1}e^{-b\\lambda}$.\nCombining these, the posterior density is:\n$$ p(\\lambda \\mid \\mathbf{y}) \\propto \\left(e^{-n\\lambda} \\lambda^{\\sum_{i=1}^n y_i}\\right) \\left(\\lambda^{a-1}e^{-b\\lambda}\\right) $$\n$$ p(\\lambda \\mid \\mathbf{y}) \\propto \\lambda^{(\\sum y_i + a) - 1} e^{-(n+b)\\lambda} $$\nThis expression is the kernel of a Gamma distribution. Thus, the posterior distribution is also a Gamma distribution, which demonstrates the conjugacy of the Gamma prior for the Poisson likelihood. The posterior distribution is $\\lambda \\mid \\mathbf{y} \\sim \\text{Gamma}(\\alpha, \\beta)$, with updated parameters:\n- Posterior shape: $\\alpha = \\sum_{i=1}^n y_i + a$\n- Posterior rate: $\\beta = n + b$\n\nSecond, we derive the posterior mean and posterior mode. For a Gamma distribution $\\text{Gamma}(\\alpha, \\beta)$, the mean and mode are well-known quantities.\n- The posterior mean is $E[\\lambda \\mid \\mathbf{y}] = \\frac{\\alpha}{\\beta} = \\frac{\\sum y_i + a}{n + b}$.\n- The posterior mode (Maximum A Posteriori estimate, $\\lambda_{\\text{MAP}}$) is found by maximizing the posterior density. For a Gamma distribution with shape $\\alpha > 1$, the mode is $\\lambda_{\\text{MAP}} = \\frac{\\alpha - 1}{\\beta} = \\frac{\\sum y_i + a - 1}{n + b}$. All test cases provided in the problem satisfy the condition $\\alpha > 1$.\n\nThird, we derive the variance for the Laplace approximation. The Laplace approximation approximates the posterior density with a Gaussian (Normal) distribution centered at the posterior mode, $\\lambda_{\\text{MAP}}$. The variance, which we denote $\\sigma_L^2$, is the inverse of the negative of the second derivative of the log-posterior density evaluated at the mode.\nThe log-posterior, up to an additive constant, is:\n$$ \\log p(\\lambda \\mid \\mathbf{y}) = (\\alpha-1)\\log\\lambda - \\beta\\lambda + C $$\nThe first derivative with respect to $\\lambda$ is:\n$$ \\frac{d}{d\\lambda} \\log p(\\lambda \\mid \\mathbf{y}) = \\frac{\\alpha-1}{\\lambda} - \\beta $$\nThe second derivative is:\n$$ \\frac{d^2}{d\\lambda^2} \\log p(\\lambda \\mid \\mathbf{y}) = -\\frac{\\alpha-1}{\\lambda^2} $$\nEvaluating the negative of the second derivative (the observed Fisher information) at the mode $\\lambda_{\\text{MAP}} = \\frac{\\alpha - 1}{\\beta}$:\n$$ J(\\lambda_{\\text{MAP}}) = -\\left(-\\frac{\\alpha-1}{(\\frac{\\alpha-1}{\\beta})^2}\\right) = \\frac{\\alpha-1}{(\\alpha-1)^2 / \\beta^2} = \\frac{\\beta^2}{\\alpha-1} $$\nThe variance of the Laplace approximation is the inverse of this quantity:\n$$ \\sigma_L^2 = [J(\\lambda_{\\text{MAP}})]^{-1} = \\frac{\\alpha-1}{\\beta^2} $$\nThus, the Laplace approximation to the posterior is $\\lambda_{\\text{approx}} \\sim N(\\mu_L, \\sigma_L^2)$, where the mean is $\\mu_L = \\lambda_{\\text{MAP}} = \\frac{\\alpha-1}{\\beta}$ and the variance is $\\sigma_L^2 = \\frac{\\alpha-1}{\\beta^2}$.\n\nFourth, we compute the expected squared loss $L(\\lambda; t) = (\\lambda - t)^2$. The expected loss under a probability distribution for $\\lambda$ is given by $E[(\\lambda-t)^2]$. This can be expanded using the definition of variance, $\\text{Var}(\\lambda) = E[\\lambda^2] - (E[\\lambda])^2$:\n$$ E[(\\lambda-t)^2] = E[\\lambda^2 - 2t\\lambda + t^2] = E[\\lambda^2] - 2tE[\\lambda] + t^2 $$\n$$ E[(\\lambda-t)^2] = (\\text{Var}(\\lambda) + (E[\\lambda])^2) - 2tE[\\lambda] + t^2 = \\text{Var}(\\lambda) + (E[\\lambda] - t)^2 $$\nThis formula relates the expected loss to the variance and the squared bias of the distribution's mean relative to the target $t$.\n\nWe apply this formula to both the exact posterior and the Laplace approximation.\n- For the exact posterior, $\\lambda \\mid \\mathbf{y} \\sim \\text{Gamma}(\\alpha, \\beta)$:\n  - Mean: $E_{\\text{post}}[\\lambda] = \\frac{\\alpha}{\\beta}$\n  - Variance: $\\text{Var}_{\\text{post}}(\\lambda) = \\frac{\\alpha}{\\beta^2}$\n  - Expected loss: $$E_{\\text{exact}} = \\text{Var}_{\\text{post}}(\\lambda) + (E_{\\text{post}}[\\lambda] - t)^2 = \\frac{\\alpha}{\\beta^2} + \\left(\\frac{\\alpha}{\\beta} - t\\right)^2$$\n- For the Laplace approximation, $\\lambda_{\\text{approx}} \\sim N(\\mu_L, \\sigma_L^2)$:\n  - Mean: $E_{\\text{Laplace}}[\\lambda] = \\mu_L = \\frac{\\alpha-1}{\\beta}$\n  - Variance: $\\text{Var}_{\\text{Laplace}}(\\lambda) = \\sigma_L^2 = \\frac{\\alpha-1}{\\beta^2}$\n  - Expected loss: $$E_{\\text{Laplace}} = \\text{Var}_{\\text{Laplace}}(\\lambda) + (E_{\\text{Laplace}}[\\lambda] - t)^2 = \\frac{\\alpha-1}{\\beta^2} + \\left(\\frac{\\alpha-1}{\\beta} - t\\right)^2$$\n\nThe program will implement these final formulas to compute the absolute difference $|E_{\\text{exact}} - E_{\\text{Laplace}}|$ for each test case.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test suite by calculating the absolute difference\n    between the expected squared loss under the exact posterior and under the Laplace approximation.\n    \"\"\"\n    \n    # Test suite definition: (prior_a, prior_b, data_y, t_config)\n    # t_config is a tuple (type, value) where type is 'mode', 'mean', or 'value'.\n    test_cases = [\n        (2.5, 1.0, [0, 1, 2], ('mode', None)),\n        (0.3, 1.0, [1], ('mean', None)),\n        (2.0, 1.0, [10, 12, 9, 11, 8], ('value', 10.0)),\n        (3.0, 0.5, [0, 0, 0, 1], ('value', 0.8)),\n    ]\n\n    results = []\n\n    for case in test_cases:\n        a_prior, b_prior, y, t_config = case\n        \n        # Convert y to a numpy array for easier calculations\n        y = np.array(y)\n        \n        # Calculate sufficient statistics from data\n        n = len(y)\n        sum_y = np.sum(y)\n        \n        # Calculate posterior parameters\n        # Posterior is Gamma(alpha, beta)\n        alpha_post = sum_y + a_prior\n        beta_post = float(n + b_prior)\n\n        # Ensure posterior mode is well-defined (alpha > 1)\n        if alpha_post <= 1:\n            # This case is not expected based on problem validation\n            # but good practice to handle.\n            results.append(np.nan)\n            continue\n\n        # Determine the loss target t based on the configuration\n        t_type, t_val = t_config\n        t = 0.0\n        if t_type == 'value':\n            t = t_val\n        elif t_type == 'mode':\n            # Posterior mode (MAP)\n            t = (alpha_post - 1) / beta_post\n        elif t_type == 'mean':\n            # Sample mean of the data\n            t = np.mean(y)\n\n        # === Calculations for the exact posterior: Gamma(alpha_post, beta_post) ===\n        \n        # Mean of the exact posterior\n        mean_exact = alpha_post / beta_post\n        # Variance of the exact posterior\n        var_exact = alpha_post / (beta_post**2)\n        # Expected squared loss for the exact posterior\n        expected_loss_exact = var_exact + (mean_exact - t)**2\n\n        # === Calculations for the Laplace approximation: Normal(mu_L, sigma_L^2) ===\n        \n        # Mean of the Laplace approximation is the posterior mode\n        mean_laplace = (alpha_post - 1) / beta_post\n        # Variance of the Laplace approximation\n        var_laplace = (alpha_post - 1) / (beta_post**2)\n        # Expected squared loss for the Laplace approximation\n        expected_loss_laplace = var_laplace + (mean_laplace - t)**2\n\n        # Calculate the absolute difference between the two expected losses\n        abs_diff = abs(expected_loss_exact - expected_loss_laplace)\n        results.append(abs_diff)\n    \n    # Format the output as a comma-separated list of strings with 6 decimal places\n    output_str = \",\".join([f\"{res:.6f}\" for res in results])\n    print(f\"[{output_str}]\")\n\nsolve()\n```", "id": "3137250"}]}