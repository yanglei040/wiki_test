{"hands_on_practices": [{"introduction": "The best way to understand the Expectation-Maximization algorithm is to perform the calculations by hand. This first exercise guides you through a single, complete iteration for a Gaussian Mixture Model. By manually computing the 'responsibilities' in the E-step and then using them to update the model parameters in the M-step, you will build a concrete understanding of the algorithm's core mechanics [@problem_id:1960172].", "problem": "A researcher is analyzing a dataset of measurements, which is believed to be sampled from a mixture of two distinct populations, A and B. They decide to model the data using a two-component Gaussian Mixture Model (GMM). The probability density function (PDF) for a single data point $x$ is given by:\n$$p(x | \\theta) = \\pi_A \\mathcal{N}(x | \\mu_A, \\sigma_A^2) + \\pi_B \\mathcal{N}(x | \\mu_B, \\sigma_B^2)$$\nwhere $\\pi_A$ and $\\pi_B$ are the mixing proportions (with $\\pi_A + \\pi_B = 1$), and $\\mathcal{N}(x | \\mu, \\sigma^2)$ is the Gaussian PDF:\n$$\\mathcal{N}(x | \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)$$\n\nThe dataset consists of the following five measurements: $\\{4.0, 4.5, 5.0, 8.0, 9.0\\}$.\nFor simplicity, the variances of the two components are assumed to be known and fixed at $\\sigma_A^2 = \\sigma_B^2 = 1.0$.\n\nThe researcher initializes the model parameters for the Expectation-Maximization (EM) algorithm as follows:\n- Initial means: $\\mu_A^{(0)} = 4.2$ and $\\mu_B^{(0)} = 8.8$.\n- Initial mixing proportions: $\\pi_A^{(0)} = 0.5$ and $\\pi_B^{(0)} = 0.5$.\n\nYour task is to perform a single, full iteration of the EM algorithm, which consists of one Expectation step (E-step) followed by one Maximization step (M-step), to compute the updated parameters.\n\nCalculate the updated value for the mean of the first component, $\\mu_A^{(1)}$. Report your answer as a real number, rounded to four significant figures.", "solution": "We apply one Expectation-Maximization (EM) iteration for the two-component Gaussian Mixture Model with fixed variances $\\sigma_{A}^{2}=\\sigma_{B}^{2}=1$ and initial parameters $\\mu_{A}^{(0)}=4.2$, $\\mu_{B}^{(0)}=8.8$, $\\pi_{A}^{(0)}=\\pi_{B}^{(0)}=0.5$.\n\nE-step (responsibilities):\nFor each data point $x_{i}$, the responsibility of component $A$ is\n$$\nr_{iA} \\equiv \\gamma_{iA} = \\frac{\\pi_{A}^{(0)} \\mathcal{N}(x_{i} \\mid \\mu_{A}^{(0)},1)}{\\pi_{A}^{(0)} \\mathcal{N}(x_{i} \\mid \\mu_{A}^{(0)},1) + \\pi_{B}^{(0)} \\mathcal{N}(x_{i} \\mid \\mu_{B}^{(0)},1)}.\n$$\nWith $\\pi_{A}^{(0)}=\\pi_{B}^{(0)}$ and equal variances, the common factors cancel:\n$$\nr_{iA}=\\frac{\\exp\\!\\left(-\\frac{(x_{i}-\\mu_{A}^{(0)})^{2}}{2}\\right)}{\\exp\\!\\left(-\\frac{(x_{i}-\\mu_{A}^{(0)})^{2}}{2}\\right)+\\exp\\!\\left(-\\frac{(x_{i}-\\mu_{B}^{(0)})^{2}}{2}\\right)}=\\frac{1}{1+\\exp\\!\\left(\\frac{(x_{i}-\\mu_{A}^{(0)})^{2}-(x_{i}-\\mu_{B}^{(0)})^{2}}{2}\\right)}.\n$$\nFor the dataset $\\{4.0,4.5,5.0,8.0,9.0\\}$ and $(\\mu_{A}^{(0)},\\mu_{B}^{(0)})=(4.2,8.8)$:\n- For $x_{1}=4.0$: $(x_{1}-\\mu_{A}^{(0)})^{2}=0.04$, $(x_{1}-\\mu_{B}^{(0)})^{2}=23.04$, so\n$$\nr_{1A}=\\frac{1}{1+\\exp(-11.5)}\\approx 0.9999898625.\n$$\n- For $x_{2}=4.5$: $(x_{2}-\\mu_{A}^{(0)})^{2}=0.09$, $(x_{2}-\\mu_{B}^{(0)})^{2}=18.49$, so\n$$\nr_{2A}=\\frac{1}{1+\\exp(-9.2)}\\approx 0.9998989606.\n$$\n- For $x_{3}=5.0$: $(x_{3}-\\mu_{A}^{(0)})^{2}=0.64$, $(x_{3}-\\mu_{B}^{(0)})^{2}=14.44$, so\n$$\nr_{3A}=\\frac{1}{1+\\exp(-6.9)}\\approx 0.9989932309.\n$$\n- For $x_{4}=8.0$: $(x_{4}-\\mu_{A}^{(0)})^{2}=14.44$, $(x_{4}-\\mu_{B}^{(0)})^{2}=0.64$, so\n$$\nr_{4A}=\\frac{1}{1+\\exp(6.9)}=1-r_{3A}\\approx 0.0010067691.\n$$\n- For $x_{5}=9.0$: $(x_{5}-\\mu_{A}^{(0)})^{2}=23.04$, $(x_{5}-\\mu_{B}^{(0)})^{2}=0.04$, so\n$$\nr_{5A}=\\frac{1}{1+\\exp(11.5)}=1-r_{1A}\\approx 0.0000101375.\n$$\nNote $r_{1A}+r_{5A}=1$ and $r_{3A}+r_{4A}=1$ by symmetry.\n\nM-step (update mean of component A):\nWith fixed variance, the updated mean is the responsibility-weighted average:\n$$\n\\mu_{A}^{(1)}=\\frac{\\sum_{i=1}^{5} r_{iA} x_{i}}{\\sum_{i=1}^{5} r_{iA}}.\n$$\nCompute the denominator using symmetry:\n$$\n\\sum_{i=1}^{5} r_{iA}=(r_{1A}+r_{5A})+(r_{3A}+r_{4A})+r_{2A}=2+r_{2A}\\approx 2.9998989606.\n$$\nCompute the numerator; group symmetric pairs:\n$$\n\\sum_{i=1}^{5} r_{iA} x_{i}=(r_{1A}\\cdot 4.0 + r_{5A}\\cdot 9.0) + (r_{3A}\\cdot 5.0 + r_{4A}\\cdot 8.0) + r_{2A}\\cdot 4.5.\n$$\nUse $r_{5A}=1-r_{1A}$ and $r_{4A}=1-r_{3A}$:\n$$\nr_{1A}\\cdot 4.0 + r_{5A}\\cdot 9.0 = 9 - 5 r_{1A},\\quad\nr_{3A}\\cdot 5.0 + r_{4A}\\cdot 8.0 = 8 - 3 r_{3A}.\n$$\nThus\n$$\n\\sum_{i=1}^{5} r_{iA} x_{i} = 17 - 5 r_{1A} - 3 r_{3A} + 4.5 r_{2A} \\approx 13.5026163178.\n$$\nTherefore,\n$$\n\\mu_{A}^{(1)}=\\frac{13.5026163178}{2.9998989606}\\approx 4.501023693.\n$$\nRounded to four significant figures, the updated mean is $4.501$.", "answer": "$$\\boxed{4.501}$$", "id": "1960172"}, {"introduction": "While the EM algorithm is powerful, its success often depends on a good initialization of the parameters. This practice explores a classic failure mode where symmetric initialization prevents the algorithm from separating the underlying clusters. By working through this scenario [@problem_id:1960187], you will gain crucial insight into why breaking symmetry is essential and how the algorithm can get stuck at suboptimal solutions.", "problem": "A statistician is tasked with fitting a two-component Gaussian Mixture Model (GMM) to a small one-dimensional dataset using the Expectation-Maximization (EM) algorithm. The dataset consists of four data points: $X = \\{-3, -1, 1, 5\\}$.\n\nThe GMM has two components, denoted as A and B. For the initial step (iteration $t=0$), the parameters are initialized symmetrically as follows:\n-   Mixing coefficients: $\\pi_A^{(0)} = 0.5$ and $\\pi_B^{(0)} = 0.5$.\n-   Means: $\\mu_A^{(0)} = 0$ and $\\mu_B^{(0)} = 0$.\n-   Variances: $\\sigma_A^{2(0)} = 1$ and $\\sigma_B^{2(0)} = 1$.\n\nYour task is to perform one complete iteration of the EM algorithm. Calculate the updated values for the means, $\\mu_A^{(1)}$ and $\\mu_B^{(1)}$, after the first Expectation-step and Maximization-step. Present the final answer as a pair of exact values for $(\\mu_A^{(1)}, \\mu_B^{(1)})$.", "solution": "We model the data with a two-component one-dimensional Gaussian mixture with parameters at iteration $t=0$: $\\pi_{A}^{(0)}=\\pi_{B}^{(0)}=\\frac{1}{2}$, $\\mu_{A}^{(0)}=\\mu_{B}^{(0)}=0$, and $\\sigma_{A}^{2(0)}=\\sigma_{B}^{2(0)}=1$. The Gaussian density is\n$$\nf(x \\mid \\mu,\\sigma^{2})=\\frac{1}{\\sqrt{2\\pi \\sigma^{2}}}\\exp\\!\\left(-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}\\right).\n$$\n\nExpectation-step: The responsibility of component $A$ for data point $x_{i}$ is\n$$\n\\gamma_{iA}^{(0)}=\\frac{\\pi_{A}^{(0)} f(x_{i}\\mid \\mu_{A}^{(0)},\\sigma_{A}^{2(0)})}{\\pi_{A}^{(0)} f(x_{i}\\mid \\mu_{A}^{(0)},\\sigma_{A}^{2(0)})+\\pi_{B}^{(0)} f(x_{i}\\mid \\mu_{B}^{(0)},\\sigma_{B}^{2(0)})}.\n$$\nBecause $\\pi_{A}^{(0)}=\\pi_{B}^{(0)}$, $\\mu_{A}^{(0)}=\\mu_{B}^{(0)}$, and $\\sigma_{A}^{2(0)}=\\sigma_{B}^{2(0)}$, the two terms in the denominator are equal for every $x_{i}$, hence\n$$\n\\gamma_{iA}^{(0)}=\\frac{1}{2},\\qquad \\gamma_{iB}^{(0)}=\\frac{1}{2}\\quad \\text{for all } i\\in\\{1,2,3,4\\}.\n$$\n\nMaximization-step: The mean updates are\n$$\n\\mu_{A}^{(1)}=\\frac{\\sum_{i=1}^{4}\\gamma_{iA}^{(0)} x_{i}}{\\sum_{i=1}^{4}\\gamma_{iA}^{(0)}},\\qquad\n\\mu_{B}^{(1)}=\\frac{\\sum_{i=1}^{4}\\gamma_{iB}^{(0)} x_{i}}{\\sum_{i=1}^{4}\\gamma_{iB}^{(0)}}.\n$$\nWith $\\gamma_{iA}^{(0)}=\\gamma_{iB}^{(0)}=\\frac{1}{2}$,\n$$\n\\mu_{A}^{(1)}=\\frac{\\frac{1}{2}\\sum_{i=1}^{4} x_{i}}{\\frac{1}{2}\\cdot 4}=\\frac{\\sum_{i=1}^{4} x_{i}}{4},\\qquad\n\\mu_{B}^{(1)}=\\frac{\\sum_{i=1}^{4} x_{i}}{4}.\n$$\nFor the dataset $X=\\{-3,-1,1,5\\}$,\n$$\n\\sum_{i=1}^{4} x_{i}=(-3)+(-1)+1+5=2,\\quad \\text{so}\\quad \\mu_{A}^{(1)}=\\mu_{B}^{(1)}=\\frac{2}{4}=\\frac{1}{2}.\n$$\nTherefore, after one complete EM iteration, the updated means are both $\\frac{1}{2}$.", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{1}{2}  \\frac{1}{2} \\end{pmatrix}}$$", "id": "1960187"}, {"introduction": "The Expectation-Maximization framework is a general tool, not limited to Gaussian mixtures. This exercise challenges you to adapt the EM algorithm for a mixture of exponential distributions, a model often used for waiting times or lifetimes. You will first derive the M-step update rule for the exponential rate parameters and then apply your findings to a numerical example [@problem_id:3119716], demonstrating the algorithm's versatility and deepening your understanding of its theoretical foundation.", "problem": "Consider a finite mixture of exponential distributions with $K$ components. The observed data are $x_{1},\\dots,x_{N}$, each $x_{n} \\in \\mathbb{R}_{+}$, assumed independent and identically distributed (i.i.d.) from the mixture model with parameters $\\{\\pi_{k}\\}_{k=1}^{K}$ and $\\{\\lambda_{k}\\}_{k=1}^{K}$, where $\\pi_{k} \\ge 0$, $\\sum_{k=1}^{K} \\pi_{k} = 1$, and $\\lambda_{k}  0$. The exponential probability density function is $p(x \\mid \\lambda) = \\lambda \\exp(-\\lambda x)$ for $x \\ge 0$. Introduce latent indicator variables $z_{n,k} \\in \\{0,1\\}$ satisfying $\\sum_{k=1}^{K} z_{n,k} = 1$. Starting from Bayesâ€™ rule and the complete-data likelihood $p(\\{x_{n}\\},\\{z_{n,k}\\}\\mid \\{\\pi_{k}\\},\\{\\lambda_{k}\\})$, derive the expectation-maximization (EM) responsibilities $\\gamma_{n,k} = \\mathbb{E}[z_{n,k} \\mid x_{n}]$ and the maximization-step (M-step) update for the exponential rates $\\lambda_{k}$ by maximizing the expected complete-data log-likelihood with respect to $\\lambda_{k}$.\n\nThen, apply your derivations to the following specific instance. Let $K=2$, $N=4$, data $x_{1} = 0.2$, $x_{2} = 0.6$, $x_{3} = 1.0$, $x_{4} = 2.0$. Suppose the current parameters at iteration $t$ are $\\pi_{1}^{(t)} = 0.7$, $\\pi_{2}^{(t)} = 0.3$, $\\lambda_{1}^{(t)} = 2.0$, $\\lambda_{2}^{(t)} = 0.5$. Compute the responsibilities $\\gamma_{n,1}$ for $n=1,2,3,4$, and then compute the updated rate $\\lambda_{1}^{(t+1)}$ using your M-step expression. Round your final numerical answer to four significant figures. Report only the value of $\\lambda_{1}^{(t+1)}$.", "solution": "The problem asks for the derivation of the expectation-maximization (EM) algorithm update equations for a mixture of exponential distributions and the application of these equations to a specific numerical example.\n\nFirst, we derive the general equations. The model is a mixture of $K$ exponential distributions. The observed data are $\\mathbf{x} = \\{x_1, \\dots, x_N\\}$, with each $x_n \\in \\mathbb{R}_{+}$. The parameters are $\\theta = \\{\\{\\pi_k\\}_{k=1}^K, \\{\\lambda_k\\}_{k=1}^K\\}$. The probability density function (PDF) for a single exponential distribution with rate $\\lambda_k$ is $p(x | \\lambda_k) = \\lambda_k \\exp(-\\lambda_k x)$. The latent variables $z_{n,k} \\in \\{0,1\\}$ indicate which component generated data point $x_n$, with $\\sum_{k=1}^K z_{n,k} = 1$. The prior probability of $z_{n,k}=1$ is the mixing coefficient $\\pi_k$.\n\nThe complete-data likelihood, given the observed data $\\mathbf{x}$ and the latent variables $\\mathbf{z} = \\{z_{n,k}\\}$, is:\n$$p(\\mathbf{x}, \\mathbf{z} \\mid \\theta) = \\prod_{n=1}^N p(x_n, \\mathbf{z}_n \\mid \\theta) = \\prod_{n=1}^N \\prod_{k=1}^K [p(x_n \\mid z_{n,k}=1, \\theta) p(z_{n,k}=1 \\mid \\theta)]^{z_{n,k}}$$\n$$p(\\mathbf{x}, \\mathbf{z} \\mid \\theta) = \\prod_{n=1}^N \\prod_{k=1}^K [\\pi_k p(x_n \\mid \\lambda_k)]^{z_{n,k}}$$\nSubstituting the exponential PDF, we get:\n$$p(\\mathbf{x}, \\mathbf{z} \\mid \\theta) = \\prod_{n=1}^N \\prod_{k=1}^K [\\pi_k \\lambda_k \\exp(-\\lambda_k x_n)]^{z_{n,k}}$$\nThe complete-data log-likelihood is:\n$$\\ln p(\\mathbf{x}, \\mathbf{z} \\mid \\theta) = \\sum_{n=1}^N \\sum_{k=1}^K z_{n,k} (\\ln \\pi_k + \\ln \\lambda_k - \\lambda_k x_n)$$\n\nThe EM algorithm proceeds in two steps: Expectation (E-step) and Maximization (M-step).\n\n**E-step: Derivation of Responsibilities**\nIn the E-step, we compute the posterior probability of the latent variables given the data and the current parameter estimates $\\theta^{(t)} = \\{\\pi_k^{(t)}, \\lambda_k^{(t)}\\}$. This posterior probability is the responsibility, $\\gamma_{n,k}$.\n$$\\gamma_{n,k} = p(z_{n,k}=1 \\mid x_n, \\theta^{(t)}) = \\mathbb{E}[z_{n,k} \\mid x_n, \\theta^{(t)}]$$\nUsing Bayes' rule:\n$$\\gamma_{n,k} = \\frac{p(x_n \\mid z_{n,k}=1, \\theta^{(t)}) p(z_{n,k}=1 \\mid \\theta^{(t)})}{\\sum_{j=1}^K p(x_n \\mid z_{n,j}=1, \\theta^{(t)}) p(z_{n,j}=1 \\mid \\theta^{(t)})} = \\frac{\\pi_k^{(t)} p(x_n \\mid \\lambda_k^{(t)})}{\\sum_{j=1}^K \\pi_j^{(t)} p(x_n \\mid \\lambda_j^{(t)})}$$\nSubstituting the exponential PDF, we obtain the expression for the responsibilities:\n$$\\gamma_{n,k} = \\frac{\\pi_k^{(t)} \\lambda_k^{(t)} \\exp(-\\lambda_k^{(t)} x_n)}{\\sum_{j=1}^K \\pi_j^{(t)} \\lambda_j^{(t)} \\exp(-\\lambda_j^{(t)} x_n)}$$\n\n**M-step: Derivation of the Update for $\\lambda_k$**\nIn the M-step, we maximize the expected complete-data log-likelihood, denoted $Q(\\theta \\mid \\theta^{(t)})$, with respect to the parameters $\\theta$.\n$$Q(\\theta \\mid \\theta^{(t)}) = \\mathbb{E}_{\\mathbf{z}\\mid\\mathbf{x}, \\theta^{(t)}}[\\ln p(\\mathbf{x}, \\mathbf{z} \\mid \\theta)] = \\sum_{n=1}^N \\sum_{k=1}^K \\mathbb{E}[z_{n,k}] (\\ln \\pi_k + \\ln \\lambda_k - \\lambda_k x_n)$$\nSince $\\mathbb{E}[z_{n,k}] = \\gamma_{n,k}$, we have:\n$$Q(\\theta \\mid \\theta^{(t)}) = \\sum_{n=1}^N \\sum_{k=1}^K \\gamma_{n,k} (\\ln \\pi_k + \\ln \\lambda_k - \\lambda_k x_n)$$\nTo find the update for $\\lambda_k$, we maximize $Q$ with respect to $\\lambda_k$. We differentiate $Q$ with respect to $\\lambda_k$ and set the derivative to zero. We only need to consider terms involving $\\lambda_k$:\n$$\\frac{\\partial Q}{\\partial \\lambda_k} = \\frac{\\partial}{\\partial \\lambda_k} \\sum_{n=1}^N \\gamma_{n,k} (\\ln \\lambda_k - \\lambda_k x_n) = \\sum_{n=1}^N \\gamma_{n,k} \\left( \\frac{1}{\\lambda_k} - x_n \\right) = 0$$\nSolving for $\\lambda_k$:\n$$\\frac{1}{\\lambda_k} \\sum_{n=1}^N \\gamma_{n,k} = \\sum_{n=1}^N \\gamma_{n,k} x_n$$\nThis gives the M-step update equation for the rate parameter $\\lambda_k$:\n$$\\lambda_k^{(t+1)} = \\frac{\\sum_{n=1}^N \\gamma_{n,k}}{\\sum_{n=1}^N \\gamma_{n,k} x_n}$$\n\nNow we apply these derivations to the specific instance.\nThe given data are $K=2$, $N=4$, $x_1=0.2$, $x_2=0.6$, $x_3=1.0$, $x_4=2.0$.\nThe parameters at iteration $t$ are $\\pi_1^{(t)} = 0.7$, $\\pi_2^{(t)} = 0.3$, $\\lambda_1^{(t)} = 2.0$, $\\lambda_2^{(t)} = 0.5$.\n\nFirst, we compute the responsibilities $\\gamma_{n,1}$ for $n=1,2,3,4$ using the derived E-step formula.\nFor a generic data point $x_n$, let the numerator terms be $A_n = \\pi_1^{(t)} \\lambda_1^{(t)} \\exp(-\\lambda_1^{(t)} x_n)$ and $B_n = \\pi_2^{(t)} \\lambda_2^{(t)} \\exp(-\\lambda_2^{(t)} x_n)$. Then $\\gamma_{n,1} = A_n / (A_n + B_n)$.\n\nFor $x_1 = 0.2$:\n$A_1 = 0.7 \\times 2.0 \\times \\exp(-2.0 \\times 0.2) = 1.4 \\exp(-0.4) \\approx 0.93845$\n$B_1 = 0.3 \\times 0.5 \\times \\exp(-0.5 \\times 0.2) = 0.15 \\exp(-0.1) \\approx 0.13573$\n$\\gamma_{1,1} = \\frac{0.93845}{0.93845 + 0.13573} \\approx 0.87365$\n\nFor $x_2 = 0.6$:\n$A_2 = 1.4 \\exp(-2.0 \\times 0.6) = 1.4 \\exp(-1.2) \\approx 0.42167$\n$B_2 = 0.15 \\exp(-0.5 \\times 0.6) = 0.15 \\exp(-0.3) \\approx 0.11112$\n$\\gamma_{2,1} = \\frac{0.42167}{0.42167 + 0.11112} \\approx 0.79143$\n\nFor $x_3 = 1.0$:\n$A_3 = 1.4 \\exp(-2.0 \\times 1.0) = 1.4 \\exp(-2.0) \\approx 0.18947$\n$B_3 = 0.15 \\exp(-0.5 \\times 1.0) = 0.15 \\exp(-0.5) \\approx 0.09098$\n$\\gamma_{3,1} = \\frac{0.18947}{0.18947 + 0.09098} \\approx 0.67563$\n\nFor $x_4 = 2.0$:\n$A_4 = 1.4 \\exp(-2.0 \\times 2.0) = 1.4 \\exp(-4.0) \\approx 0.02564$\n$B_4 = 0.15 \\exp(-0.5 \\times 2.0) = 0.15 \\exp(-1.0) \\approx 0.05518$\n$\\gamma_{4,1} = \\frac{0.02564}{0.02564 + 0.05518} \\approx 0.31726$\n\nNext, we compute the updated parameter $\\lambda_1^{(t+1)}$ using the M-step formula.\nNumerator: Sum of responsibilities for component $1$.\n$$\\sum_{n=1}^4 \\gamma_{n,1} \\approx 0.87365 + 0.79143 + 0.67563 + 0.31726 \\approx 2.65797$$\nDenominator: Weighted sum of data points for component $1$.\n$$\\sum_{n=1}^4 \\gamma_{n,1} x_n \\approx (0.87365 \\times 0.2) + (0.79143 \\times 0.6) + (0.67563 \\times 1.0) + (0.31726 \\times 2.0)$$\n$$\\sum_{n=1}^4 \\gamma_{n,1} x_n \\approx 0.17473 + 0.47486 + 0.67563 + 0.63452 \\approx 1.95974$$\nFinally, we compute the updated rate $\\lambda_1^{(t+1)}$:\n$$\\lambda_1^{(t+1)} = \\frac{\\sum_{n=1}^4 \\gamma_{n,1}}{\\sum_{n=1}^4 \\gamma_{n,1} x_n} \\approx \\frac{2.65797}{1.95974} \\approx 1.356294$$\nRounding the final answer to four significant figures gives $1.356$.", "answer": "$$\\boxed{1.356}$$", "id": "3119716"}]}