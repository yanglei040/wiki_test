{"hands_on_practices": [{"introduction": "The best way to understand an algorithm is to build it. This first exercise guides you through implementing Locally Linear Embedding (LLE) from its fundamental principles, focusing on neighbor finding, weight reconstruction, and final embedding. You will then use your implementation to investigate a critical aspect of LLE's behavior: its sensitivity to how input features are scaled, a crucial consideration for any practical application. [@problem_id:3141684]", "problem": "Implement a complete program that from first principles constructs Locally Linear Embedding (LLE) for given point clouds and uses it to quantitatively compare embeddings under different input scalings. Your program must not rely on any external machine learning library. It must use only linear algebra, Euclidean geometry, and eigen-decomposition. Work in purely mathematical terms as follows.\n\nDefinitions to use as the fundamental base:\n- Given a point cloud with $N$ samples in $D$ dimensions arranged as a matrix $X \\in \\mathbb{R}^{N \\times D}$, the $k$ nearest neighbors of a sample are those with smallest Euclidean distances (ties may be broken by index order). For any vectors $x,y \\in \\mathbb{R}^{D}$, the Euclidean distance is $\\lVert x - y \\rVert_{2}$.\n- For each sample $x_i \\in \\mathbb{R}^{D}$ with neighbor indices $\\mathcal{N}(i)$, define the local least-squares reconstruction problem to find reconstruction weights $w_{ij}$ that minimize a squared error subject to a unit-sum constraint. Formulate this using the local data matrix built from neighbor differences.\n- Assemble a global cost from the local reconstructions and recover a low-dimensional embedding $Y \\in \\mathbb{R}^{N \\times d}$ by solving an eigen-decomposition problem derived from the weights. Use the nontrivial eigenvectors associated with the smallest eigenvalues (excluding the one associated with the trivial constant mode).\n- A standard tool for comparing two embeddings that are only defined up to rigid transformations and uniform rescaling is orthogonal Procrustes analysis. To compare $Y_1, Y_2 \\in \\mathbb{R}^{N \\times d}$, center each to zero mean, normalize each to unit Frobenius norm, compute the optimal orthogonal alignment by singular value decomposition (SVD), and report the sum of squared residuals after alignment as the disparity. This yields a nonnegative real number; smaller is closer.\n\nInvariance concepts to explore:\n- Uniform global scaling: replacing $X$ by $s X$ for a scalar $s \\in \\mathbb{R}_{+}$ multiplies all pairwise distances by the same factor. Investigate how this affects the neighbor sets, local reconstructions, and the final embedding.\n- Anisotropic feature scaling: replacing $X$ by $X A$ for a diagonal matrix $A \\in \\mathbb{R}^{D \\times D}$ with unequal positive entries rescales each feature differently. Investigate how this affects neighbor sets, local reconstructions, and the final embedding.\n- Per-feature standardization: transforming $X$ to $\\tilde{X}$ by standardizing each feature to zero mean and unit variance using the empirical mean and variance across the $N$ samples, computed featurewise.\n\nAlgorithmic requirements for your implementation:\n- Implement $k$-nearest neighbors using exact Euclidean distances on the full dataset.\n- For each point $x_i$, pose and solve the constrained least-squares reconstruction problem of $x_i$ from its $k$ neighbors with a unit-sum constraint on the weights. To ensure numerical stability when the local Gram matrix is ill-conditioned, add a Tikhonov regularizer proportional to its trace times a small scalar $\\epsilon$.\n- Assemble the global matrix from the weights and compute the embedding of dimension $d$ as the eigenvectors associated with the $d$ smallest nontrivial eigenvalues of the symmetric positive semidefinite matrix arising from the reconstruction operator.\n- Implement Procrustes alignment as described above using linear algebra and singular value decomposition.\n\nUse the following fixed data and parameters as a test suite. All angles are dimensionless; there are no physical units. All numerical values must be treated as exact scalars.\n\nShared parameters across tests:\n- Number of neighbors $k = 8$.\n- Target embedding dimension $d = 2$.\n- Regularization parameter $\\epsilon = 10^{-3}$ used by scaling the identity matrix by $\\epsilon$ times the trace of the local Gram matrix.\n- Procrustes disparity thresholds are given below for each boolean test.\n\nDatasets:\n1) Curved two-dimensional manifold in three dimensions.\n- Construct a regular grid with $m = 11$ so that $u$ and $v$ each take the $m$ equally spaced values in $[0,1]$, namely $\\{0, \\frac{1}{m-1}, \\ldots, 1\\}$. Stack all $N = m^2$ pairs $(u,v)$ in lexicographic order into a matrix $U \\in \\mathbb{R}^{N \\times 2}$.\n- Map each $(u,v)$ to a point in $\\mathbb{R}^{3}$ via $x = \\begin{bmatrix} u & v & u^2 + v^2 \\end{bmatrix}$, forming $X_{\\text{base}} \\in \\mathbb{R}^{N \\times 3}$.\n- Define three transformed versions:\n  a) Global scaling: $X_{\\text{glob}} = s X_{\\text{base}}$ with $s = 7$.\n  b) Strong anisotropic feature scaling: $X_{\\text{aniso}} = X_{\\text{base}} A$ with $A = \\mathrm{diag}(100, 0.01, 50)$.\n  c) Per-feature standardizations: $\\mathrm{Std}(X)$ denotes featurewise standardization to zero mean and unit variance. Compute $X_{\\text{std-base}} = \\mathrm{Std}(X_{\\text{base}})$ and $X_{\\text{std-aniso}} = \\mathrm{Std}(X_{\\text{aniso}})$.\n\n2) Approximately balanced linear manifold in three dimensions.\n- Using the same $U$ as above, define $X_{\\text{bal}} \\in \\mathbb{R}^{N \\times 3}$ by $x = \\begin{bmatrix} u & v & \\frac{u+v}{2} \\end{bmatrix}$ and its standardized version $X_{\\text{std-bal}} = \\mathrm{Std}(X_{\\text{bal}})$.\n\nFor each dataset $X$, compute the LLE embedding $Y \\in \\mathbb{R}^{N \\times d}$.\n\nQuantitative comparisons to compute:\n- Let $\\Delta(\\cdot,\\cdot)$ denote the Procrustes disparity between two embeddings of the same dataset size.\n- Compute the following five boolean test outcomes:\n  1) $b_1$: Invariance to uniform global scaling on the curved manifold. Require $b_1 = [\\Delta(Y_{\\text{base}}, Y_{\\text{glob}}) < 10^{-6}]$.\n  2) $b_2$: Sensitivity to anisotropic feature scaling on the curved manifold. Require $b_2 = [\\Delta(Y_{\\text{base}}, Y_{\\text{aniso}}) > 2 \\times 10^{-2}]$.\n  3) $b_3$: Standardization cancels anisotropy up to small residuals on the curved manifold. Require $b_3 = [\\Delta(Y_{\\text{std-base}}, Y_{\\text{std-aniso}}) < 2 \\times 10^{-2}]$.\n  4) $b_4$: For the curved manifold, standardizing the raw input improves closeness to the raw embedding compared to the heavily anisotropically scaled input. Require $b_4 = [\\Delta(Y_{\\text{base}}, Y_{\\text{std-base}}) < \\Delta(Y_{\\text{base}}, Y_{\\text{aniso}})]$.\n  5) $b_5$: On the approximately balanced linear manifold, standardization does not substantially change the embedding. Require $b_5 = [\\Delta(Y_{\\text{bal}}, Y_{\\text{std-bal}}) < 5 \\times 10^{-3}]$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of the five boolean outcomes, in order $[b_1, b_2, b_3, b_4, b_5]$, enclosed in square brackets. For example, a valid output line would look like $[\\mathrm{True},\\mathrm{False},\\mathrm{True},\\mathrm{True},\\mathrm{True}]$ but with the actual boolean values computed by your program.\n\nNotes:\n- All computations must be deterministic. Do not use randomness.\n- Angles do not appear. There are no physical units in this problem.", "solution": "The user requests an implementation of the Locally Linear Embedding (LLE) algorithm from first principles to investigate its sensitivity to different feature scaling transformations. The implementation must adhere to a specific set of algorithmic and parametric definitions.\n\nThe process is divided into three main components: the LLE algorithm itself, a method for comparing embedding geometries (Procrustes analysis), and a series of quantitative tests on generated datasets.\n\n**1. Locally Linear Embedding (LLE) Algorithm**\n\nThe LLE algorithm constructs a low-dimensional embedding of a high-dimensional dataset by preserving local linear reconstruction properties. The algorithm proceeds in three steps for a given dataset $X \\in \\mathbb{R}^{N \\times D}$.\n\n**Step 1: Neighbor Identification**\nFor each data point $x_i$, we identify its $k$ nearest neighbors. The neighborhood $\\mathcal{N}(i)$ consists of the $k$ points $x_j$ ($j \\neq i$) that have the smallest Euclidean distance $\\lVert x_i - x_j \\rVert_2$. To ensure deterministic results, any ties in distance are resolved by favoring the point with the smaller index $j$.\n\n**Step 2: Local Reconstruction Weight Computation**\nThe core assumption of LLE is that each point $x_i$ can be well-approximated by a linear combination of its neighbors. We seek a set of reconstruction weights $W_{ij}$ that minimize the total reconstruction error, subject to two constraints: $W_{ij} = 0$ if $x_j$ is not a neighbor of $x_i$, and the weights for each point must sum to unity, $\\sum_{j \\in \\mathcal{N}(i)} W_{ij} = 1$. This leads to the constrained least-squares problem for each point $x_i$:\n$$ \\min_{W} \\left\\| x_i - \\sum_{j \\in \\mathcal{N}(i)} W_{ij} x_j \\right\\|^2 \\quad \\text{subject to} \\quad \\sum_{j \\in \\mathcal{N}(i)} W_{ij} = 1 $$\nThis problem is invariant to rotation, translation, and uniform scaling of the input data. By substituting the constraint, the objective can be rewritten as finding weights $w \\in \\mathbb{R}^k$ for point $x_i$ that minimize:\n$$ \\left\\| \\sum_{j=1}^k w_j (x_i - x_{\\eta_j}) \\right\\|^2 = w^T G_i w $$\nwhere $\\{x_{\\eta_j}\\}_{j=1}^k$ are the neighbors of $x_i$, and $G_i$ is the local Gram matrix. The elements of $G_i$ are $(G_i)_{jk} = (x_i-x_{\\eta_j})^T(x_i-x_{\\eta_k})$. The solution for the optimal weights $w$ is found by solving the linear system $G_i z = \\mathbf{1}$ (where $\\mathbf{1}$ is a vector of ones) and then normalizing $w = z / \\sum z_l$.\n\nTo handle ill-conditioned Gram matrices, a Tikhonov regularization term is added. The problem specifies adding a term proportional to the trace of the Gram matrix, which makes the regularization itself scale-invariant. The modified Gram matrix $G'_i$ is:\n$$ G'_i = G_i + \\epsilon \\cdot \\mathrm{tr}(G_i) \\cdot I_k $$\nwhere $I_k$ is the $k \\times k$ identity matrix and $\\epsilon = 10^{-3}$ is a small scalar. We then solve $G'_i z = \\mathbf{1}$ for $z$ to find the weights.\n\n**Step 3: Global Embedding Computation**\nAfter computing the $N \\times N$ weight matrix $W$ (where $W_{ij}=0$ for non-neighbors), we seek a low-dimensional embedding $Y \\in \\mathbb{R}^{N \\times d}$ that preserves these local reconstruction weights. This is achieved by minimizing the embedding cost function:\n$$ \\Phi(Y) = \\sum_{i=1}^N \\left\\| y_i - \\sum_{j=1}^N W_{ij} y_j \\right\\|^2 $$\nThis quadratic form can be expressed as $\\Phi(Y) = \\mathrm{Tr}(Y^T M Y)$, where $M = (I-W)^T(I-W)$. The matrix $M$ is symmetric and positive semidefinite. To minimize $\\Phi(Y)$ while avoiding a trivial collapsed solution ($Y=0$), we impose centering and unit covariance constraints on $Y$. The solution is given by the eigenvectors of $M$ corresponding to its smallest eigenvalues. The very smallest eigenvalue of $M$ is always zero (or near-zero numerically), with a corresponding eigenvector of all ones. This represents a trivial constant embedding and is discarded. The desired $d$-dimensional embedding $Y$ is constructed from the eigenvectors associated with the next $d$ smallest eigenvalues (i.e., eigenvalues $2$ through $d+1$).\n\n**2. Orthogonal Procrustes Analysis for Embedding Comparison**\n\nTo quantitatively compare two embeddings, $Y_1, Y_2 \\in \\mathbb{R}^{N \\times d}$, which are defined only up to rigid transformations (rotation, reflection) and uniform scaling, we use orthogonal Procrustes analysis. The disparity $\\Delta(Y_1, Y_2)$ is the minimum sum of squared differences after optimal alignment. The procedure is:\n1.  **Center**: Both $Y_1$ and $Y_2$ are translated so their centroids are at the origin.\n2.  **Normalize**: Both centered matrices are scaled to have a unit Frobenius norm, $\\|Y\\|_F = \\sqrt{\\sum_{i,j} Y_{ij}^2} = 1$.\n3.  **Align**: The optimal orthogonal matrix $R$ that minimizes $\\|Y_1 R - Y_2\\|_F^2$ is found using the singular value decomposition (SVD) of the matrix $C = Y_1^T Y_2$. If $C = U \\Sigma V^T$, the optimal rotation is $R=UV^T$.\n4.  **Compute Disparity**: The minimum squared residual, or Procrustes disparity, can be calculated efficiently as $\\Delta = \\|Y_1\\|_F^2 + \\|Y_2\\|_F^2 - 2 \\sum_i \\sigma_i$, where $\\sigma_i$ are the singular values of $C$. After normalization, this simplifies to $\\Delta = 2(1 - \\sum_i \\sigma_i)$. A smaller disparity indicates greater similarity between the embeddings.\n\n**3. Datasets and Quantitative Tests**\n\nThe analysis is performed on several transformations of two base datasets, using parameters $k=8$, $d=2$, and $\\epsilon=10^{-3}$. The tests are designed to probe the known properties of LLE:\n- $b_1$: Tests invariance to uniform global scaling. LLE is theoretically invariant, so this should be true.\n- $b_2$: Tests sensitivity to anisotropic feature scaling. This is a known weakness of LLE, so the disparity should be large.\n- $b_3$: Tests whether per-feature standardization can counteract the effect of anisotropy. Standardizing the features of $X_{\\text{base}}$ and $X_{\\text{aniso}}$ results in identical datasets, thus their embeddings should be identical and their disparity near-zero.\n- $b_4$: Compares the effect of standardization versus strong anisotropic scaling. Standardization is expected to be a much less disruptive transformation.\n- $b_5$: Examines the effect of standardization on a nearly balanced linear manifold. The change is expected to be minor, resulting in a small disparity.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import eigh, svd\n\ndef solve():\n    \"\"\"\n    Implements Locally Linear Embedding (LLE) from first principles and uses it to\n    compare embeddings under different input scalings.\n    \"\"\"\n\n    # -----------------------------------------------------\n    # Algorithm Implementation based on Problem Description\n    # -----------------------------------------------------\n\n    def standardize(X):\n        \"\"\"Standardizes each feature of X to have zero mean and unit variance.\"\"\"\n        mean = np.mean(X, axis=0)\n        std = np.std(X, axis=0)\n        # Avoid division by zero for constant features\n        std[np.isclose(std, 0)] = 1.0\n        return (X - mean) / std\n\n    def find_neighbors(X, k):\n        \"\"\"Finds the k-nearest neighbors for each point in X.\"\"\"\n        N = X.shape[0]\n        # Calculate squared Euclidean distances to avoid sqrt\n        sum_X_sq = np.sum(X**2, axis=1)\n        dist_sq_matrix = np.add.outer(sum_X_sq, sum_X_sq) - 2 * (X @ X.T)\n        dist_sq_matrix[dist_sq_matrix  0] = 0.0 # for numerical stability\n        \n        # Argsort on distances to find nearest neighbors\n        # kind='stable' ensures ties are broken by index order as required\n        neighbor_indices = np.argsort(dist_sq_matrix, axis=1, kind='stable')\n        \n        # Return indices of k-nearest neighbors.\n        # Index 0 is the point itself, so we take from 1 to k+1.\n        return neighbor_indices[:, 1:k+1]\n\n    def compute_weights(X, neighbor_indices, k, epsilon):\n        \"\"\"Computes the LLE reconstruction weights for each point.\"\"\"\n        N = X.shape[0]\n        W = np.zeros((N, N))\n        \n        for i in range(N):\n            neighbors = neighbor_indices[i]\n            \n            # Local data matrix of neighbor differences\n            C_i = X[i] - X[neighbors]  # Uses broadcasting\n            \n            # Local Gram matrix\n            G_i = C_i @ C_i.T\n            \n            # Tikhonov regularization as specified\n            trace_G = np.trace(G_i)\n            reg_val = epsilon * trace_G\n            G_i += reg_val * np.identity(k)\n            \n            # Solve for weights by solving G_i * z = 1\n            ones = np.ones(k)\n            try:\n                z = np.linalg.solve(G_i, ones)\n            except np.linalg.LinAlgError:\n                # Fallback to pseudo-inverse if singular despite regularization\n                z = np.linalg.pinv(G_i) @ ones\n\n            # Normalize weights to sum to 1\n            w_i = z / np.sum(z)\n            \n            # Populate sparse weight matrix W\n            W[i, neighbors] = w_i\n            \n        return W\n\n    def compute_embedding(W, N, d):\n        \"\"\"Computes the low-dimensional embedding from the weight matrix.\"\"\"\n        # Construct the cost matrix M = (I-W)^T(I-W)\n        I = np.identity(N)\n        M = (I - W).T @ (I - W)\n        \n        # Find eigenvectors for the d smallest non-trivial eigenvalues.\n        # eigh returns eigenvalues in ascending order.\n        # We need eigenvectors for indices 1 up to and including d.\n        # This corresponds to the 2nd to (d+1)-th smallest eigenvalues.\n        _, eigvecs = eigh(M, subset_by_index=[1, d])\n        \n        return eigvecs\n\n    def procrustes_disparity(Y1, Y2):\n        \"\"\"Computes the orthogonal Procrustes disparity between two embeddings.\"\"\"\n        # 1. Center the embeddings to have zero mean.\n        Y1_c = Y1 - np.mean(Y1, axis=0)\n        Y2_c = Y2 - np.mean(Y2, axis=0)\n\n        # 2. Normalize to unit Frobenius norm.\n        norm1 = np.linalg.norm(Y1_c, 'fro')\n        norm2 = np.linalg.norm(Y2_c, 'fro')\n        Y1_norm = Y1_c / norm1 if norm1 > 1e-10 else Y1_c\n        Y2_norm = Y2_c / norm2 if norm2 > 1e-10 else Y2_c\n\n        # 3. Compute optimal alignment via SVD of the cross-covariance matrix.\n        M = Y1_norm.T @ Y2_norm\n        _, s, _ = svd(M)\n        \n        # 4. Compute disparity using the singular values.\n        # Disparity = ||Y1_norm||^2 + ||Y2_norm||^2 - 2 * tr(Sigma)\n        #           = 1 + 1 - 2 * sum(s) = 2 * (1 - sum(s))\n        disparity = 2.0 - 2.0 * np.sum(s)\n        return disparity\n\n    def run_lle(X, k, d, epsilon):\n        \"\"\"A wrapper function to run the full LLE pipeline.\"\"\"\n        neighbor_indices = find_neighbors(X, k)\n        W = compute_weights(X, neighbor_indices, k, epsilon)\n        N = X.shape[0]\n        Y = compute_embedding(W, N, d)\n        return Y\n\n    # ------------------\n    # Main Logic\n    # ------------------\n    \n    # Shared parameters from the problem description\n    k = 8\n    d = 2\n    epsilon = 1e-3\n    m = 11\n\n    # 1. Generate Datasets\n    u_vals = np.linspace(0.0, 1.0, m)\n    v_vals = np.linspace(0.0, 1.0, m)\n    uu, vv = np.meshgrid(u_vals, v_vals, indexing='ij')\n    U = np.stack([uu.ravel(), vv.ravel()], axis=1)\n\n    # Curved manifold datasets\n    X_base = np.column_stack((U[:, 0], U[:, 1], U[:, 0]**2 + U[:, 1]**2))\n    X_glob = 7.0 * X_base\n    A = np.diag([100.0, 0.01, 50.0])\n    X_aniso = X_base @ A\n    X_std_base = standardize(X_base)\n    X_std_aniso = standardize(X_aniso)\n\n    # Balanced linear manifold datasets\n    X_bal = np.column_stack((U[:, 0], U[:, 1], (U[:, 0] + U[:, 1]) / 2.0))\n    X_std_bal = standardize(X_bal)\n\n    datasets = {\n        \"base\": X_base, \"glob\": X_glob, \"aniso\": X_aniso,\n        \"std-base\": X_std_base, \"std-aniso\": X_std_aniso,\n        \"bal\": X_bal, \"std-bal\": X_std_bal,\n    }\n\n    # 2. Compute LLE Embeddings for all datasets\n    embeddings = {name: run_lle(X, k, d, epsilon) for name, X in datasets.items()}\n\n    # 3. Perform Quantitative Comparisons\n    Y_base, Y_glob, Y_aniso = embeddings[\"base\"], embeddings[\"glob\"], embeddings[\"aniso\"]\n    Y_std_base, Y_std_aniso = embeddings[\"std-base\"], embeddings[\"std-aniso\"]\n    Y_bal, Y_std_bal = embeddings[\"bal\"], embeddings[\"std-bal\"]\n\n    # Test 1: Invariance to uniform global scaling\n    d1 = procrustes_disparity(Y_base, Y_glob)\n    b1 = bool(d1  1e-6)\n\n    # Test 2: Sensitivity to anisotropic feature scaling\n    d2 = procrustes_disparity(Y_base, Y_aniso)\n    b2 = bool(d2 > 2e-2)\n\n    # Test 3: Standardization cancels anisotropy\n    d3 = procrustes_disparity(Y_std_base, Y_std_aniso)\n    b3 = bool(d3  2e-2)\n\n    # Test 4: Standardization vs. Anisotropic Scaling\n    d4_std = procrustes_disparity(Y_base, Y_std_base)\n    b4 = bool(d4_std  d2)\n    \n    # Test 5: Standardization on a balanced manifold\n    d5 = procrustes_disparity(Y_bal, Y_std_bal)\n    b5 = bool(d5  5e-3)\n\n    results = [b1, b2, b3, b4, b5]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3141684"}, {"introduction": "Theoretical algorithms often assume ideal data, but real-world datasets can contain imperfections like duplicate entries. This practice explores a critical failure mode of LLE that arises when a point's local neighborhood collapses due to identical data points, leading to a singular local covariance matrix. You will diagnose this degeneracy and investigate practical remedies like data jittering and diagonal loading to make the algorithm more robust. [@problem_id:3141726]", "problem": "You are given a data matrix $X \\in \\mathbb{R}^{N \\times D}$ with rows $x_{i} \\in \\mathbb{R}^{D}$. For each index $i$, consider the $K$ nearest neighbors of $x_{i}$ under the Euclidean norm, with ties broken by ascending row index. Define the neighbor-difference matrix $Z_{i} \\in \\mathbb{R}^{K \\times D}$ by stacking the row vectors $x_{j} - x_{i}$ for the selected neighbors $j$. The locally linear embedding construction forms the local covariance $C_{i} \\in \\mathbb{R}^{K \\times K}$ as $C_{i} = Z_{i} Z_{i}^{\\top}$. You will analyze how exact duplicate rows in $X$ degenerate $C_{i}$ and evaluate two remedies based on jittering.\n\nFundamental base: Use the following core definitions and facts.\n- The Euclidean norm of $v \\in \\mathbb{R}^{D}$ is $\\lVert v \\rVert_{2} = \\sqrt{\\sum_{d=1}^{D} v_{d}^{2}}$.\n- For any matrix $A$, the rank satisfies $\\mathrm{rank}(A A^{\\top}) = \\mathrm{rank}(A)$, and $A A^{\\top}$ is symmetric positive semidefinite.\n- For a symmetric positive semidefinite matrix $M$, the eigenvalues are real and nonnegative. The two-norm condition number is $\\kappa_{2}(M) = \\lambda_{\\max}(M) / \\lambda_{\\min}(M)$ when $\\lambda_{\\min}(M)  0$.\n- If two data points are exact duplicates, then their Euclidean distance is $0$, and subtracting them yields the zero vector.\n\nYou must implement the following, starting from these principles and definitions, without using any shortcut formulas presented as hints in this problem statement.\n\nComputational specification.\n- Nearest neighbors: For a given $i$, compute all distances $\\lVert x_{j} - x_{i} \\rVert_{2}$ for $j \\neq i$, sort increasingly by distance and then by $j$, and take the first $K$ indices.\n- Local covariance: Form $Z_{i}$ by stacking $x_{j} - x_{i}$ for the chosen neighbors, then set $C_{i} = Z_{i} Z_{i}^{\\top}$.\n- Singular detection: Let $\\tau = 10^{-12}$. Decide that $C_{i}$ is singular if its smallest eigenvalue is less than or equal to $\\tau$, and nonsingular otherwise.\n\nJittering strategies to evaluate.\n- Isotropic row jitter in $Z_{i}$: Add independent Gaussian noise with mean $0$ and standard deviation $\\varepsilon$ to each entry of $Z_{i}$ before forming $C_{i}$. Use $\\varepsilon = 10^{-3}$ and a fixed random seed of $0$ for reproducibility.\n- Diagonal loading (ridge) of $C_{i}$: Given a target condition number $\\kappa_{\\mathrm{target}}  1$, find the smallest nonnegative scalar $r_{\\min}$ such that the two-norm condition number of $C_{i} + r I$ is at most $\\kappa_{\\mathrm{target}}$. Express $r_{\\min}$ rounded to $6$ decimal places.\n\nTest suite.\nUse the following fixed datasets and parameters. All points are in $\\mathbb{R}^{4}$.\n- Case $1$ (happy path, no duplicates): $X_{A}$ has $N = 6$ rows,\n  $x_{0} = [0, 0, 0, 0]$,\n  $x_{1} = [1, 0, 0, 0]$,\n  $x_{2} = [0, 1, 0, 0]$,\n  $x_{3} = [0, 0, 1, 0]$,\n  $x_{4} = [0, 0, 0, 1]$,\n  $x_{5} = [1, 1, 1, 1]$.\n  Evaluate at $i = 5$ with $K = 4$. Output a boolean indicating whether $C_{5}$ is nonsingular under the singularity rule with threshold $\\tau$.\n- Case $2$ (single exact duplicate neighbor): $X_{B}$ extends $X_{A}$ by appending a duplicate of $x_{5}$, so $N = 7$ and\n  $x_{6} = [1, 1, 1, 1]$.\n  Evaluate at $i = 5$ with $K = 4$. Output a boolean indicating whether $C_{5}$ is singular under the same rule.\n- Case $3$ (all neighbors identical to the center): $X_{C}$ has $N = 5$ rows, all equal to $[0, 0, 0, 0]$. Evaluate at $i = 0$ with $K = 4$. Apply isotropic row jitter in $Z_{0}$ using $\\varepsilon = 10^{-3}$ and seed $0$, form $C_{0}$ from the jittered $Z_{0}$, and output a boolean indicating whether the resulting $C_{0}$ is nonsingular under the same rule.\n- Case $4$ (ridge needed for a target condition number): Reuse $X_{B}$ at $i = 5$ with $K = 4$. Let $\\kappa_{\\mathrm{target}} = 10^{6}$. Compute the smallest nonnegative $r_{\\min}$ such that the two-norm condition number of $C_{5} + r I$ is at most $\\kappa_{\\mathrm{target}}$. Output this $r_{\\min}$ as a float rounded to $6$ decimal places.\n\nFinal output format.\nYour program should produce a single line of output containing the results for Cases $1$ through $4$ in order, as a comma-separated list enclosed in square brackets (for example, $[b_{1},b_{2},b_{3},r]$ where $b_{1}$, $b_{2}$, and $b_{3}$ are booleans and $r$ is a float rounded to $6$ decimal places). No other text should be printed.", "solution": "The problem requires an analysis of the local covariance matrix $C_i$ used in the Locally Linear Embedding (LLE) algorithm. We will investigate its properties, particularly its potential for singularity due to duplicate data points, and evaluate two methods for numerical stabilization: jittering and diagonal loading (ridge regularization).\n\nThe core procedure is as follows. Given a data matrix $X \\in \\mathbb{R}^{N \\times D}$ and a point of interest $x_i$, we first identify its $K$ nearest neighbors. The neighbors are determined by sorting all other points $x_j$ ($j \\neq i$) based on the Euclidean distance $\\lVert x_j - x_i \\rVert_2$ in increasing order. Any ties in distance are resolved by sorting by the row index $j$ in ascending order.\n\nFrom these $K$ neighbors, whose indices we denote $j_1, \\dots, j_K$, we construct the neighbor-difference matrix $Z_i \\in \\mathbb{R}^{K \\times D}$. The $k$-th row of $Z_i$ is the vector $x_{j_k} - x_i$.\n\nThe local covariance matrix $C_i \\in \\mathbb{R}^{K \\times K}$ is then defined as the Gram matrix $C_i = Z_i Z_i^\\top$. Each element $(a, b)$ of $C_i$ is the dot product of the $a$-th and $b$-th difference vectors: $(C_i)_{ab} = (x_{j_a} - x_i) \\cdot (x_{j_b} - x_i)$.\n\nBy its construction, $C_i$ is symmetric and positive semidefinite. Its rank is determined by the rank of $Z_i$, which is the dimension of the vector space spanned by the neighbor-difference vectors $\\{x_{j_k} - x_i\\}_{k=1}^K$. The rank of $Z_i$ is at most $\\min(K, D)$. If the rank is less than $K$, the $K \\times K$ matrix $C_i$ is singular, implying it has at least one zero eigenvalue. For numerical purposes, we will classify $C_i$ as singular if its smallest eigenvalue, $\\lambda_{\\min}(C_i)$, satisfies $\\lambda_{\\min}(C_i) \\le \\tau$, where the threshold is given as $\\tau = 10^{-12}$.\n\n### Case 1: No Duplicates (Nonsingular Case)\nWe are given the dataset $X_A \\in \\mathbb{R}^{6 \\times 4}$ and asked to evaluate $C_5$ for the point $x_5 = [1, 1, 1, 1]$ with $K=4$.\n\nFirst, we compute the squared Euclidean distances from $x_5$ to all other points $x_j$ for $j \\in \\{0, 1, 2, 3, 4\\}$:\n- $\\lVert x_0 - x_5 \\rVert_2^2 = \\lVert [-1, -1, -1, -1] \\rVert_2^2 = 4$\n- $\\lVert x_1 - x_5 \\rVert_2^2 = \\lVert [0, -1, -1, -1] \\rVert_2^2 = 3$\n- $\\lVert x_2 - x_5 \\rVert_2^2 = \\lVert [-1, 0, -1, -1] \\rVert_2^2 = 3$\n- $\\lVert x_3 - x_5 \\rVert_2^2 = \\lVert [-1, -1, 0, -1] \\rVert_2^2 = 3$\n- $\\lVert x_4 - x_5 \\rVert_2^2 = \\lVert [-1, -1, -1, 0] \\rVert_2^2 = 3$\n\nThe four smallest distances are identical ($\\sqrt{3}$), corresponding to points $x_1, x_2, x_3, x_4$. Since indices are already in ascending order, these are the $K=4$ nearest neighbors. The neighbor-difference matrix $Z_5$ is formed by rows $x_j - x_5$:\n$$Z_5 = \\begin{pmatrix} 0  -1  -1  -1 \\\\ -1  0  -1  -1 \\\\ -1  -1  0  -1 \\\\ -1  -1  -1  0 \\end{pmatrix}$$\nThe local covariance matrix $C_5 = Z_5 Z_5^\\top$ is:\n$$C_5 = \\begin{pmatrix} 3  2  2  2 \\\\ 2  3  2  2 \\\\ 2  2  3  2 \\\\ 2  2  2  3 \\end{pmatrix}$$\nThis matrix has the form $2J + I$, where $J$ is the all-ones matrix and $I$ is the identity matrix. The eigenvalues of a $k \\times k$ matrix of the form $aJ+bI$ are $a \\cdot k + b$ (with multiplicity $1$) and $b$ (with multiplicity $k-1$). For $C_5$, we have $k=4$, $a=2$, and $b=1$. The eigenvalues are $2 \\cdot 4 + 1 = 9$ and $1$ (with multiplicity $3$).\nThe smallest eigenvalue is $\\lambda_{\\min}(C_5) = 1$. Since $1  \\tau=10^{-12}$, the matrix is nonsingular. The result for this case is `True`.\n\n### Case 2: Single Exact Duplicate Neighbor (Singular Case)\nThe dataset $X_B$ is $X_A$ augmented with $x_6 = x_5 = [1, 1, 1, 1]$. We re-evaluate for $x_5$ with $K=4$.\nThe distance from $x_5$ to its duplicate $x_6$ is $\\lVert x_6 - x_5 \\rVert_2 = 0$. This is the minimum possible distance, so $x_6$ is the closest neighbor. The remaining three neighbors are $x_1, x_2, x_3$, each at distance $\\sqrt{3}$.\nThe neighbor-difference matrix $Z_5$ is:\n$$Z_5 = \\begin{pmatrix} x_6-x_5 \\\\ x_1-x_5 \\\\ x_2-x_5 \\\\ x_3-x_5 \\end{pmatrix} = \\begin{pmatrix} 0  0  0  0 \\\\ 0  -1  -1  -1 \\\\ -1  0  -1  -1 \\\\ -1  -1  0  -1 \\end{pmatrix}$$\nSince the first row of $Z_5$ is the zero vector, the rows are linearly dependent. Thus, $\\mathrm{rank}(Z_5)  4$. The Gram matrix $C_5 = Z_5 Z_5^\\top$ must therefore be singular, with $\\mathrm{rank}(C_5) = \\mathrm{rank}(Z_5)  4$. This implies that its smallest eigenvalue is exactly $\\lambda_{\\min}(C_5)=0$. Since $0 \\le \\tau=10^{-12}$, the matrix is singular. The result for this case is `True`.\n\n### Case 3: Jittering for Regularization\nThe dataset $X_C$ consists of $N=5$ identical points $[0, 0, 0, 0]$. We evaluate for $x_0$ with $K=4$.\nThe neighbors are $x_1, x_2, x_3, x_4$. The distance to each is $0$. The difference vectors $x_j - x_0$ are all zero vectors. The initial neighbor-difference matrix $Z_0$ is a $4 \\times 4$ zero matrix:\n$$Z_0 = \\mathbf{0}_{4 \\times 4}$$\nThis matrix is maximally singular with rank $0$. To remedy this, we apply isotropic jittering. A noise matrix, with entries drawn independently from a Gaussian distribution $\\mathcal{N}(0, \\varepsilon^2)$ with $\\varepsilon = 10^{-3}$, is added to $Z_0$ to form a new matrix $Z'_0$. Since the entries of $Z'_0$ are drawn from a continuous probability distribution, the matrix will be invertible with probability $1$. Its rank will be $4$.\nThe new covariance matrix $C_0 = Z'_0 (Z'_0)^\\top$ will also have rank $4$ and thus be nonsingular. Its smallest eigenvalue $\\lambda_{\\min}(C_0)$ will be strictly positive. Given the non-zero variance of the noise, it is practically certain that $\\lambda_{\\min}(C_0)$ will be greater than the small tolerance $\\tau = 10^{-12}$. Thus, the jittered matrix is nonsingular. The result for this case is `True`.\n\n### Case 4: Diagonal Loading for Regularization\nWe reuse the singular matrix $C_5$ from Case 2. Its eigenvalues are $\\{0, 1, 1, 7\\}$. We need to find the smallest nonnegative scalar $r_{\\min}$ such that the regularized matrix $C'_5 = C_5 + rI$ has a condition number $\\kappa_2(C'_5) \\le \\kappa_{\\mathrm{target}}$, where $\\kappa_{\\mathrm{target}} = 10^6$.\nThe eigenvalues of $C'_5$ are obtained by shifting the eigenvalues of $C_5$ by $r$. For $r \\ge 0$, they are $\\{r, 1+r, 1+r, 7+r\\}$. The smallest and largest eigenvalues are $\\lambda'_{\\min} = r$ and $\\lambda'_{\\max} = 7+r$.\nThe condition number (for $r  0$) is $\\kappa_2(C'_5) = \\frac{\\lambda'_{\\max}}{\\lambda'_{\\min}} = \\frac{7+r}{r}$.\nWe solve the inequality:\n$$\\frac{7+r}{r} \\le \\kappa_{\\mathrm{target}}$$\nAs $r$ must be positive (otherwise $\\kappa_2$ is undefined or infinite), we can multiply by $r$:\n$$7 + r \\le r \\cdot \\kappa_{\\mathrm{target}} \\implies 7 \\le r (\\kappa_{\\mathrm{target}} - 1)$$\nThis gives the condition for $r$:\n$$r \\ge \\frac{7}{\\kappa_{\\mathrm{target}} - 1}$$\nThe smallest nonnegative value $r$ can take is $r_{\\min} = \\frac{7}{\\kappa_{\\mathrm{target}} - 1}$. Substituting $\\kappa_{\\mathrm{target}} = 10^6$:\n$$r_{\\min} = \\frac{7}{10^6 - 1} = \\frac{7}{999999} \\approx 7.000007000007 \\times 10^{-6}$$\nRounding this value to $6$ decimal places yields $0.000007$.", "answer": "```python\nimport numpy as np\n\ndef find_neighbors_and_form_CZ(X, i, K):\n    \"\"\"\n    Finds K nearest neighbors for X[i] and computes C_i and Z_i.\n    Ties are broken by ascending row index.\n    \"\"\"\n    x_i = X[i, :]\n    num_points = X.shape[0]\n    \n    distances = []\n    for j in range(num_points):\n        if i == j:\n            continue\n        dist = np.linalg.norm(X[j, :] - x_i)\n        distances.append((dist, j))\n    \n    # Sort by distance, then by index for tie-breaking\n    distances.sort()\n    \n    # Get the K nearest neighbor indices\n    neighbor_indices = [j for dist, j in distances[:K]]\n    \n    # Form the Z_i matrix. The shape should be K x D\n    Z_i = np.zeros((K, X.shape[1]))\n    for k, j in enumerate(neighbor_indices):\n        Z_i[k, :] = X[j, :] - x_i\n    \n    # Form the C_i matrix\n    C_i = Z_i @ Z_i.T\n    \n    return C_i, Z_i\n\ndef solve():\n    \"\"\"\n    Solves the four test cases specified in the problem.\n    \"\"\"\n    # Define datasets\n    X_A = np.array([\n        [0., 0., 0., 0.],\n        [1., 0., 0., 0.],\n        [0., 1., 0., 0.],\n        [0., 0., 1., 0.],\n        [0., 0., 0., 1.],\n        [1., 1., 1., 1.]\n    ], dtype=float)\n\n    X_B = np.array([\n        [0., 0., 0., 0.],\n        [1., 0., 0., 0.],\n        [0., 1., 0., 0.],\n        [0., 0., 1., 0.],\n        [0., 0., 0., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.]\n    ], dtype=float)\n\n    X_C = np.zeros((5, 4), dtype=float)\n    \n    tau = 1e-12\n    results = []\n\n    # Case 1: Happy path, no duplicates. Check for nonsingular.\n    C5_A, _ = find_neighbors_and_form_CZ(X_A, i=5, K=4)\n    eigenvalues_A = np.linalg.eigh(C5_A)[0]\n    lambda_min_A = eigenvalues_A[0]\n    results.append(lambda_min_A > tau)\n\n    # Case 2: Single exact duplicate neighbor. Check for singular.\n    C5_B, _ = find_neighbors_and_form_CZ(X_B, i=5, K=4)\n    eigenvalues_B = np.linalg.eigh(C5_B)[0]\n    lambda_min_B = eigenvalues_B[0]\n    results.append(lambda_min_B = tau)\n\n    # Case 3: All neighbors identical. Jitter and check for nonsingular.\n    epsilon = 1e-3\n    seed = 0\n    _, Z0_C = find_neighbors_and_form_CZ(X_C, i=0, K=4)\n    \n    np.random.seed(seed)\n    noise = np.random.normal(scale=epsilon, size=Z0_C.shape)\n    Z0_jittered = Z0_C + noise\n    \n    C0_jittered = Z0_jittered @ Z0_jittered.T\n    eigenvalues_C = np.linalg.eigh(C0_jittered)[0]\n    lambda_min_C = eigenvalues_C[0]\n    results.append(lambda_min_C > tau)\n    \n    # Case 4: Ridge needed for a target condition number.\n    kappa_target = 10**6\n    # Re-use eigenvalues from Case 2.\n    lambda_min_B_case4 = eigenvalues_B[0]\n    lambda_max_B_case4 = eigenvalues_B[-1]\n    \n    # We need to find the smallest r >= 0 such that:\n    # (lambda_max + r) / (lambda_min + r) = kappa_target\n    # This leads to: r >= (lambda_max - kappa_target * lambda_min) / (kappa_target - 1)\n    if kappa_target > 1:\n        r_min_candidate = (lambda_max_B_case4 - kappa_target * lambda_min_B_case4) / (kappa_target - 1)\n        r_min = max(0.0, r_min_candidate)\n    else:\n        # This case is not relevant for the problem, but for completeness:\n        # If target condition number = 1, it's generally impossible unless a matrix is scalar multiple of identity.\n        r_min = np.inf \n\n    results.append(r_min)\n    \n    # Format final output string as per requirements\n    formatted_results = []\n    for item in results:\n        if isinstance(item, bool):\n            formatted_results.append(str(item))\n        elif isinstance(item, float):\n            # Format the float to 6 decimal places for the output string\n            formatted_results.append(f\"{item:.6f}\")\n        else:\n            formatted_results.append(str(item))\n            \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3141726"}, {"introduction": "Standard LLE is just one formulation; we can modify its objective function to achieve different properties. This exercise introduces a regularized variant of LLE that balances the classic goal of reconstruction accuracy with a penalty on the magnitude of the reconstruction weights. By adjusting a penalty parameter $\\lambda$, you will explore this fundamental trade-off between model fidelity and regularization, a recurring theme across machine learning. [@problem_id:3141689]", "problem": "You are asked to investigate a variant of Locally Linear Embedding (LLE) that balances local reconstruction accuracy and the spread of reconstruction weights. Consider a dataset of points $\\{ \\mathbf{x}_i \\}_{i=1}^n$ in $\\mathbb{R}^D$. For each point $\\mathbf{x}_i$, select $k$ nearest neighbors under the Euclidean distance. Let the neighbor index set be $\\mathcal{N}(i)$ and define the local offset matrix $\\mathbf{Z}_i \\in \\mathbb{R}^{D \\times k}$ whose columns are $\\mathbf{x}_j - \\mathbf{x}_i$ for $j \\in \\mathcal{N}(i)$. The local reconstruction of $\\mathbf{x}_i$ from its neighbors uses weights $\\mathbf{w}_i \\in \\mathbb{R}^k$ that satisfy the affine constraint $\\sum_{j \\in \\mathcal{N}(i)} w_{ij} = 1$. The reconstruction error vector equals $\\mathbf{Z}_i \\mathbf{w}_i$, and its squared norm $\\| \\mathbf{Z}_i \\mathbf{w}_i \\|_2^2$ quantifies the local reconstruction accuracy. The spread of the weights is captured by $\\sum_{j \\in \\mathcal{N}(i)} w_{ij}^2$, which is minimized by distributing weight uniformly among the selected neighbors under the affine constraint. In this variant, you must compute weights that balance reconstruction accuracy with weight spread by penalizing $\\sum_{j \\in \\mathcal{N}(i)} w_{ij}^2$ while enforcing the affine constraint $\\sum_{j \\in \\mathcal{N}(i)} w_{ij} = 1$. For a given penalty parameter $\\lambda \\ge 0$, determine weights $\\mathbf{w}_i$ for each $i$ that minimize the sum of the local reconstruction squared norm and the penalty term, subject to the affine constraint.\n\nYour program must, for each test case specified below, perform the following steps:\n- For every point $\\mathbf{x}_i$ in the dataset, select exactly $k$ nearest neighbors under Euclidean distance (excluding $\\mathbf{x}_i$ itself).\n- For each point $\\mathbf{x}_i$, construct the local offset matrix $\\mathbf{Z}_i$ whose $k$ columns are $\\mathbf{x}_j - \\mathbf{x}_i$ for $j \\in \\mathcal{N}(i)$.\n- For the given $\\lambda$, compute weights $\\mathbf{w}_i$ that minimize the sum of the squared norm of the reconstruction error $\\| \\mathbf{Z}_i \\mathbf{w}_i \\|_2^2$ and the penalty $\\lambda \\sum_{j \\in \\mathcal{N}(i)} w_{ij}^2$, while satisfying the constraint $\\sum_{j \\in \\mathcal{N}(i)} w_{ij} = 1$.\n- Compute the per-point reconstruction error $e_i = \\| \\mathbf{Z}_i \\mathbf{w}_i \\|_2^2$ and the per-point weight spread $s_i = \\sum_{j \\in \\mathcal{N}(i)} w_{ij}^2$.\n- Aggregate over all points to obtain the mean reconstruction error $\\bar{e} = \\frac{1}{n} \\sum_{i=1}^n e_i$ and the mean weight spread $\\bar{s} = \\frac{1}{n} \\sum_{i=1}^n s_i$.\n\nYou must ensure scientific realism and numerical stability. If any linear system or matrix inverse required by your method is singular or ill-conditioned due to geometry (for example when $k  D$), you must use a numerically robust approach that yields a valid solution under the affine constraint. Angles must be interpreted in radians in all internal computations.\n\nTest Suite:\nDefine the following deterministic datasets and parameter sets. No randomness is permitted.\n\n- Dataset $\\mathcal{A}$ in $\\mathbb{R}^2$: $n = 8$ points on the unit circle with angles $\\theta_i = \\frac{2\\pi i}{8}$ for $i \\in \\{0,1,2,3,4,5,6,7\\}$, so $\\mathbf{x}_i = (\\cos \\theta_i, \\sin \\theta_i)$.\n- Dataset $\\mathcal{B}$ in $\\mathbb{R}^3$: $n = 10$ points along a line with small deterministic perturbations. Let $t_i = \\frac{i}{9}$ for $i \\in \\{0,1,2,3,4,5,6,7,8,9\\}$. Define $\\epsilon = 0.01$ and $\\mathbf{x}_i = (t_i, 2 t_i, -t_i) + \\epsilon \\cdot (\\sin(10 t_i), \\cos(7 t_i), \\sin(5 t_i))$.\n- Dataset $\\mathcal{C}$ in $\\mathbb{R}^2$: $n = 6$ points forming a non-degenerate rectangle with midpoints: $\\mathbf{x}_1 = (0,0)$, $\\mathbf{x}_2 = (2,0)$, $\\mathbf{x}_3 = (2,1)$, $\\mathbf{x}_4 = (0,1)$, $\\mathbf{x}_5 = (1,0.5)$, $\\mathbf{x}_6 = (1,1.5)$.\n\nThe test cases are:\n- Case $1$: Dataset $\\mathcal{A}$, $k = 3$, $\\lambda = 0$.\n- Case $2$: Dataset $\\mathcal{A}$, $k = 3$, $\\lambda = 0.1$.\n- Case $3$: Dataset $\\mathcal{A}$, $k = 3$, $\\lambda = 10$.\n- Case $4$: Dataset $\\mathcal{B}$, $k = 2$, $\\lambda = 0$.\n- Case $5$: Dataset $\\mathcal{B}$, $k = 2$, $\\lambda = 1$.\n- Case $6$: Dataset $\\mathcal{C}$, $k = 3$, $\\lambda = 50$.\n\nYour program must compute $(\\bar{e}, \\bar{s})$ for each case and produce a single line of output containing the results as a comma-separated list of lists with no spaces, where each inner list is ordered as $[\\bar{e},\\bar{s}]$ and both values are rounded to $6$ decimal places. For example, the output format must be like $[[a_1,b_1],[a_2,b_2],[a_3,b_3],[a_4,b_4],[a_5,b_5],[a_6,b_6]]$ with $a_i$ and $b_i$ replaced by the computed decimal values.", "solution": "The user-provided problem is a valid and well-posed exercise in statistical learning, specifically concerning a regularized variant of the weight computation step in Locally Linear Embedding (LLE). The problem is scientifically grounded, requires the solution of a constrained quadratic optimization problem, and provides deterministic, verifiable test cases. We will proceed with a full derivation and solution.\n\nThe core of the problem is to find, for each data point $\\mathbf{x}_i \\in \\mathbb{R}^D$, a set of weights $\\mathbf{w}_i \\in \\mathbb{R}^k$ that reconstructs $\\mathbf{x}_i$ from its $k$ nearest neighbors. The weights are determined by minimizing an objective function that balances two competing goals: minimizing the local reconstruction error and minimizing the spread of the weights. This is subject to an affine constraint ensuring the weights are invariant to translation.\n\nLet the set of indices for the $k$ nearest neighbors of $\\mathbf{x}_i$ be $\\mathcal{N}(i)$. The local offset matrix $\\mathbf{Z}_i \\in \\mathbb{R}^{D \\times k}$ has columns given by $\\mathbf{z}_{ij} = \\mathbf{x}_j - \\mathbf{x}_i$ for each $j \\in \\mathcal{N}(i)$. The reconstruction of the zero vector from these offsets is given by $\\sum_{j \\in \\mathcal{N}(i)} w_{ij}(\\mathbf{x}_j - \\mathbf{x}_i) = \\mathbf{Z}_i \\mathbf{w}_i$.\n\nThe optimization problem for each point $\\mathbf{x}_i$ is to find the weight vector $\\mathbf{w}_i \\in \\mathbb{R}^k$ that solves:\n$$\n\\min_{\\mathbf{w}_i} \\left( \\| \\mathbf{Z}_i \\mathbf{w}_i \\|_2^2 + \\lambda \\sum_{j \\in \\mathcal{N}(i)} w_{ij}^2 \\right) \\quad \\text{subject to} \\quad \\sum_{j \\in \\mathcal{N}(i)} w_{ij} = 1\n$$\nwhere $\\lambda \\ge 0$ is a regularization parameter.\n\nWe can express the objective function in matrix notation. The first term, the squared reconstruction error, is $\\| \\mathbf{Z}_i \\mathbf{w}_i \\|_2^2 = (\\mathbf{Z}_i \\mathbf{w}_i)^T(\\mathbf{Z}_i \\mathbf{w}_i) = \\mathbf{w}_i^T \\mathbf{Z}_i^T \\mathbf{Z}_i \\mathbf{w}_i$. Let $\\mathbf{C}_i = \\mathbf{Z}_i^T \\mathbf{Z}_i \\in \\mathbb{R}^{k \\times k}$ be the local Gram matrix of the neighbor offsets. This matrix is symmetric and positive semi-definite. Its $(p,q)$-th entry is the dot product $(\\mathbf{x}_p - \\mathbf{x}_i)^T (\\mathbf{x}_q - \\mathbf{x}_i)$ for $p, q \\in \\mathcal{N}(i)$.\n\nThe second term, the penalty on weight spread, is $\\lambda \\sum_{j \\in \\mathcal{N}(i)} w_{ij}^2 = \\lambda \\| \\mathbf{w}_i \\|_2^2 = \\lambda \\mathbf{w}_i^T \\mathbf{I}_k \\mathbf{w}_i$, where $\\mathbf{I}_k$ is the $k \\times k$ identity matrix.\n\nThe affine constraint can be written as $\\mathbf{1}^T \\mathbf{w}_i = 1$, where $\\mathbf{1}$ is a column vector of $k$ ones.\n\nCombining these terms, the problem is a constrained quadratic program:\n$$\n\\min_{\\mathbf{w}_i} \\mathcal{L}(\\mathbf{w}_i) = \\mathbf{w}_i^T (\\mathbf{C}_i + \\lambda \\mathbf{I}_k) \\mathbf{w}_i \\quad \\text{subject to} \\quad \\mathbf{1}^T \\mathbf{w}_i = 1\n$$\nLet $\\mathbf{G}_i = \\mathbf{C}_i + \\lambda \\mathbf{I}_k$. The matrix $\\mathbf{G}_i$ is symmetric. For $\\lambda  0$, it is strictly positive definite, guaranteeing a unique solution. For $\\lambda = 0$, $\\mathbf{G}_i = \\mathbf{C}_i$ is positive semi-definite and may be singular, particularly if the number of neighbors $k$ is greater than the data dimension $D$.\n\nWe solve this using the method of Lagrange multipliers. The Lagrangian is:\n$$\n\\mathcal{J}(\\mathbf{w}_i, \\mu) = \\mathbf{w}_i^T \\mathbf{G}_i \\mathbf{w}_i - \\mu (\\mathbf{1}^T \\mathbf{w}_i - 1)\n$$\nwhere $\\mu$ is the Lagrange multiplier. To find the minimum, we set the gradient with respect to $\\mathbf{w}_i$ to zero:\n$$\n\\nabla_{\\mathbf{w}_i} \\mathcal{J} = 2 \\mathbf{G}_i \\mathbf{w}_i - \\mu \\mathbf{1} = \\mathbf{0}\n$$\nThis gives the stationarity condition $2 \\mathbf{G}_i \\mathbf{w}_i = \\mu \\mathbf{1}$.\n\nIf $\\mathbf{G}_i$ is invertible (which is guaranteed for $\\lambda  0$), we can write $\\mathbf{w}_i = \\frac{\\mu}{2} \\mathbf{G}_i^{-1} \\mathbf{1}$. To find the multiplier $\\mu$, we substitute this back into the constraint $\\mathbf{1}^T \\mathbf{w}_i = 1$:\n$$\n\\mathbf{1}^T \\left( \\frac{\\mu}{2} \\mathbf{G}_i^{-1} \\mathbf{1} \\right) = 1 \\implies \\frac{\\mu}{2} \\left( \\mathbf{1}^T \\mathbf{G}_i^{-1} \\mathbf{1} \\right) = 1\n$$\nSolving for the term $\\frac{\\mu}{2}$ yields $\\frac{\\mu}{2} = \\frac{1}{\\mathbf{1}^T \\mathbf{G}_i^{-1} \\mathbf{1}}$.\nSubstituting this back into the expression for $\\mathbf{w}_i$, we obtain the optimal weights:\n$$\n\\mathbf{w}_i = \\frac{\\mathbf{G}_i^{-1} \\mathbf{1}}{\\mathbf{1}^T \\mathbf{G}_i^{-1} \\mathbf{1}}\n$$\nFrom a numerical standpoint, explicitly computing the matrix inverse $\\mathbf{G}_i^{-1}$ is inefficient and can be unstable. A better approach is to first solve the linear system $\\mathbf{G}_i \\mathbf{v}_i = \\mathbf{1}$ for the vector $\\mathbf{v}_i = \\mathbf{G}_i^{-1} \\mathbf{1}$. Then, the weights are found by normalizing $\\mathbf{v}_i$:\n$$\n\\mathbf{w}_i = \\frac{\\mathbf{v}_i}{\\mathbf{1}^T \\mathbf{v}_i} = \\frac{\\mathbf{v}_i}{\\sum_{j=1}^k v_{ij}}\n$$\nThis procedure must be robust to cases where $\\mathbf{G}_i$ is singular (i.e., when $\\lambda = 0$ and $\\mathbf{C}_i$ is rank-deficient). In such scenarios, the system $\\mathbf{G}_i\\mathbf{v}_i = \\mathbf{1}$ may have no solution or infinitely many solutions. The problem statement requires a robust approach. The standard procedure is to find the minimum-norm least-squares solution for $\\mathbf{v}_i$. This corresponds to using the Moore-Penrose pseudoinverse, $\\mathbf{v}_i = \\mathbf{G}_i^{\\dagger} \\mathbf{1}$. Modern linear algebra solvers, such as `numpy.linalg.lstsq`, are designed to handle this case effectively.\n\nOnce the optimal weights $\\mathbf{w}_i$ are found for a point $\\mathbf{x}_i$, we compute the required metrics:\n- Per-point reconstruction error: $e_i = \\| \\mathbf{Z}_i \\mathbf{w}_i \\|_2^2 = \\mathbf{w}_i^T \\mathbf{C}_i \\mathbf{w}_i$.\n- Per-point weight spread: $s_i = \\sum_{j \\in \\mathcal{N}(i)} w_{ij}^2 = \\| \\mathbf{w}_i \\|_2^2$.\n\nThese values are then averaged over all $n$ points in the dataset to obtain the final outputs, $\\bar{e} = \\frac{1}{n} \\sum_{i=1}^n e_i$ and $\\bar{s} = \\frac{1}{n} \\sum_{i=1}^n s_i$.\n\nThe overall algorithm is as follows:\n1. For each of the $n$ points $\\mathbf{x}_i$, $i=1, \\dots, n$:\n    a. Calculate the Euclidean distance to all other points $\\mathbf{x}_j$ ($j \\ne i$).\n    b. Identify the indices $\\mathcal{N}(i)$ of the $k$ nearest neighbors.\n    c. Form the $D \\times k$ offset matrix $\\mathbf{Z}_i$ with columns $\\mathbf{x}_j - \\mathbf{x}_i$ for $j \\in \\mathcal{N}(i)$.\n    d. Compute the $k \\times k$ Gram matrix $\\mathbf{C}_i = \\mathbf{Z}_i^T \\mathbf{Z}_i$.\n    e. Form the regularized matrix $\\mathbf{G}_i = \\mathbf{C}_i + \\lambda \\mathbf{I}_k$.\n    f. Solve for $\\mathbf{v}_i$ in $\\mathbf{G}_i \\mathbf{v}_i = \\mathbf{1}$ using a robust least-squares solver.\n    g. Normalize to find the weights: $\\mathbf{w}_i = \\mathbf{v}_i / \\sum_j v_{ij}$.\n    h. Compute $e_i = \\mathbf{w}_i^T \\mathbf{C}_i \\mathbf{w}_i$ and $s_i = \\mathbf{w}_i^T \\mathbf{w_i}$.\n2. Compute the means $\\bar{e}$ and $\\bar{s}$ from the collected $e_i$ and $s_i$ values.\n3. Repeat for all test cases and format the output as specified.", "answer": "```python\nimport numpy as np\n\ndef get_dataset_A():\n    \"\"\"Generates n=8 points on a unit circle.\"\"\"\n    n = 8\n    thetas = 2 * np.pi * np.arange(n) / n\n    return np.array([np.cos(thetas), np.sin(thetas)]).T\n\ndef get_dataset_B():\n    \"\"\"Generates n=10 points on a perturbed line in R^3.\"\"\"\n    n = 10\n    epsilon = 0.01\n    t = np.arange(n) / (n - 1.0)\n    base_line = np.array([t, 2*t, -t]).T\n    perturbations = epsilon * np.array([np.sin(10*t), np.cos(7*t), np.sin(5*t)]).T\n    return base_line + perturbations\n\ndef get_dataset_C():\n    \"\"\"Generates n=6 points forming a non-degenerate rectangle with midpoints.\"\"\"\n    return np.array([\n        [0.0, 0.0],\n        [2.0, 0.0],\n        [2.0, 1.0],\n        [0.0, 1.0],\n        [1.0, 0.5],\n        [1.0, 1.5]\n    ])\n\ndef compute_lle_variant(X, k, lambda_val):\n    \"\"\"\n    Computes the mean reconstruction error and weight spread for the LLE variant.\n    \"\"\"\n    n, D = X.shape\n    all_e_i = []\n    all_s_i = []\n\n    for i in range(n):\n        # Find k nearest neighbors\n        x_i = X[i]\n        \n        # Calculate squared Euclidean distances to all other points\n        # Using squared distances for finding neighbors is equivalent and faster\n        dists_sq = np.sum((X - x_i)**2, axis=1)\n        \n        # Exclude the point itself\n        dists_sq[i] = np.inf\n        \n        # Get indices of the k nearest neighbors\n        neighbor_indices = np.argsort(dists_sq)[:k]\n        \n        # Form the local offset matrix Z_i (D x k)\n        neighbors = X[neighbor_indices]\n        Z_i = (neighbors - x_i).T\n        \n        # Compute the local Gram matrix C_i (k x k)\n        C_i = Z_i.T @ Z_i\n        \n        # Form the regularized Gram matrix G_i = C_i + lambda * I\n        G_i = C_i + lambda_val * np.eye(k)\n        \n        # Solve the linear system G_i * v_i = 1 for v_i\n        # Using lstsq is robust against singularity, which occurs when lambda=0 and k>D\n        ones_k = np.ones(k)\n        v_i = np.linalg.lstsq(G_i, ones_k, rcond=None)[0]\n        \n        # Compute the weights w_i by normalizing v_i\n        # The sum can be zero in pathological cases, handle division by zero\n        sum_v_i = np.sum(v_i)\n        if np.isclose(sum_v_i, 0):\n             # This indicates a failure to find a valid reconstruction under the \n             # sum-to-one constraint. This shouldn't happen for the given problems.\n             # A uniform weight distribution might be a fallback.\n             w_i = np.full(k, 1.0/k)\n        else:\n             w_i = v_i / sum_v_i\n\n        # Compute per-point reconstruction error e_i\n        e_i = w_i.T @ C_i @ w_i\n        all_e_i.append(e_i)\n        \n        # Compute per-point weight spread s_i\n        s_i = w_i.T @ w_i\n        all_s_i.append(s_i)\n\n    # Compute mean error and spread\n    mean_e = np.mean(all_e_i)\n    mean_s = np.mean(all_s_i)\n    \n    return [round(mean_e, 6), round(mean_s, 6)]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    datasets = {\n        'A': get_dataset_A(),\n        'B': get_dataset_B(),\n        'C': get_dataset_C()\n    }\n    \n    test_cases = [\n        ('A', 3, 0.0),\n        ('A', 3, 0.1),\n        ('A', 3, 10.0),\n        ('B', 2, 0.0),\n        ('B', 2, 1.0),\n        ('C', 3, 50.0)\n    ]\n    \n    results = []\n    for dataset_key, k, lambda_val in test_cases:\n        X = datasets[dataset_key]\n        result = compute_lle_variant(X, k, lambda_val)\n        results.append(result)\n\n    # Format output string\n    # e.g., [[0.1,0.2],[0.3,0.4]]\n    # Using f-strings to represent floats without trailing zeros if they are integers\n    # but the rounding to 6 decimal places should make this consistent\n    output_str = '[' + ','.join([f\"[{e:.6f},{s:.6f}]\" for e, s in results]) + ']'\n    print(output_str)\n\nsolve()\n```", "id": "3141689"}]}