{"hands_on_practices": [{"introduction": "A primary motivation for non-linear dimensionality reduction (NLDR) is the failure of linear methods like PCA to capture the true structure of curved data. This practice provides a hands-on demonstration using a classic \"cusp\" manifold, where PCA incorrectly folds the data. By implementing an Isomap-like algorithm and comparing it to PCA, you will learn to \"unroll\" the manifold and use quantitative metrics like neighborhood preservation and local Lipschitz estimates to evaluate how well an embedding preserves the intrinsic geometry of the original data [@problem_id:3144257].", "problem": "Consider the planar curve defined parametrically by $p(t) = (t^2, t^3)$ for $t \\in [-1,1]$. This set is a one-dimensional subset of $\\mathbb{R}^2$ with a cusp at $t = 0$. You will analyze non-linear dimensionality reduction procedures that attempt to map this subset into a one-dimensional target space while preserving local neighborhood structure and continuity. The goal is to construct the data, define an approximation to the intrinsic (geodesic) metric, produce embeddings using two algorithms, and quantify distortion using local Lipschitz estimates and a neighborhood preservation score.\n\nFundamental base:\n- Let $M \\subset \\mathbb{R}^2$ be the set $\\{p(t) : t \\in [-1,1]\\}$ sampled at $N$ points. The ambient Euclidean distance between two points $x, x' \\in \\mathbb{R}^2$ is $\\|x - x'\\|_2$.\n- A $k$-Nearest Neighbor (kNN) graph is constructed on the sample using ambient Euclidean distances, with undirected edges of weight equal to the Euclidean distance between neighbors. Let $k_{\\text{graph}}$ denote the graph connectivity parameter (number of neighbors).\n- The intrinsic geodesic distance $d_M(i,j)$ between samples $i$ and $j$ on $M$ is approximated by the shortest-path distance in this kNN graph, with edge weights equal to ambient Euclidean distances.\n- Classical Multidimensional Scaling (MDS) embeds points into a target dimension $d$ from a given symmetric distance matrix $D$ by double-centering the squared distances and extracting the top $d$ eigenpairs of the Gram matrix. Specifically, let $H = I - \\frac{1}{N}\\mathbf{1}\\mathbf{1}^\\top$ and $B = -\\frac{1}{2}H(D^{\\circ 2})H$, where $D^{\\circ 2}$ is the elementwise square. If $\\lambda_1 \\ge \\lambda_2 \\ge \\cdots$ are the eigenvalues of $B$ with corresponding orthonormal eigenvectors $v_1, v_2, \\dots$, then an embedding into dimension $d$ is given by $Y = [\\sqrt{\\lambda_1} v_1, \\dots, \\sqrt{\\lambda_d} v_d]$ using only strictly positive eigenvalues.\n- Principal Component Analysis (PCA) embeds into $d=1$ by projecting centered data $X \\in \\mathbb{R}^{N \\times 2}$ onto the leading eigenvector of the sample covariance matrix.\n\nDistortion quantification:\n- Define a mapping $f: M \\to \\mathbb{R}$ that yields the one-dimensional embedding coordinate for each sampled point. For a fixed local neighborhood size $k_{\\text{local}}$, define for each index $i$ the set of intrinsic neighbors $N_{k_{\\text{local}}}^M(i)$ as the $k_{\\text{local}}$ indices with smallest $d_M(i,j)$ for $j \\ne i$. Define the local Lipschitz estimate at $i$ by\n$$\nL_i = \\max_{j \\in N_{k_{\\text{local}}}^M(i)} \\frac{|f(i) - f(j)|}{d_M(i,j)}.\n$$\nAggregate with the mean $L_{\\text{avg}} = \\frac{1}{N}\\sum_{i=1}^N L_i$ and the upper-tail descriptor $L_{95}$ defined as the $95$th percentile of $\\{L_i\\}_{i=1}^N$. Large values indicate local stretching and potential tears where nearby points in $M$ are mapped far apart.\n- Let $N_{k_{\\text{local}}}^Y(i)$ be the $k_{\\text{local}}$ indices with smallest embedding-space distances $|f(i) - f(j)|$ for $j \\ne i$. Define the neighborhood preservation score\n$$\nQ = \\frac{1}{N} \\sum_{i=1}^N \\frac{|N_{k_{\\text{local}}}^M(i) \\cap N_{k_{\\text{local}}}^Y(i)|}{k_{\\text{local}}}.\n$$\nValues of $Q$ in $[0,1]$ closer to $1$ indicate better local neighborhood preservation (continuity).\n\nAlgorithms to test:\n- Isometric Mapping (Isomap-like): compute $d_M$ via shortest paths on the kNN graph, then apply Classical Multidimensional Scaling (MDS) to $d_M$ to obtain an embedding into $d=1$. Denote the resulting coordinate function by $f_{\\text{iso}}$.\n- Principal Component Analysis (PCA): compute the leading principal component of the centered ambient coordinates $X$ and project onto it to obtain a one-dimensional embedding. Denote the resulting coordinate function by $f_{\\text{pca}}$.\n\nTasks to implement in a complete, runnable program:\n1. Sample $N$ points from the cusp curve by taking $t$ evenly spaced in $[-1,1]$ and mapping $p(t) = (t^2, t^3)$.\n2. Construct the undirected weighted kNN graph with parameter $k_{\\text{graph}}$ using ambient Euclidean distances among the sampled points.\n3. Compute the approximate intrinsic geodesic distance matrix $d_M$ using shortest paths on the kNN graph.\n4. Produce a one-dimensional embedding by:\n   - For Isomap-like: apply Classical Multidimensional Scaling (MDS) to $d_M$ to obtain $f_{\\text{iso}}$.\n   - For PCA: project onto the leading principal component to obtain $f_{\\text{pca}}$.\n5. For each embedding, compute $Q$, $L_{\\text{avg}}$, and $L_{95}$ using $k_{\\text{local}}$ intrinsic neighbors from $d_M$.\n\nTest suite:\n- Use $N=200$ and $k_{\\text{local}}=6$ for all cases. Evaluate the following four parameter cases that probe a happy path, a boundary, and an edge case:\n  1. Method Isomap-like with $k_{\\text{graph}}=8$ (happy path).\n  2. Method Isomap-like with $k_{\\text{graph}}=4$ (boundary: sparser graph, still connected).\n  3. Method Isomap-like with $k_{\\text{graph}}=30$ (edge case: potential short-circuiting near the cusp).\n  4. Method PCA with $k_{\\text{graph}}=8$ (linear baseline; $k_{\\text{graph}}$ used only for defining $d_M$ in the metrics).\nFor each case, the program must compute and return the triple $[Q, L_{\\text{avg}}, L_{95}]$ as floats.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of the four triples, each triple itself being a list of three floats, enclosed in square brackets. The printed floats must have six digits after the decimal point. For example, the output should look like\n$[[q_1,\\ell_1^{\\text{avg}},\\ell_1^{95}],[q_2,\\ell_2^{\\text{avg}},\\ell_2^{95}],[q_3,\\ell_3^{\\text{avg}},\\ell_3^{95}],[q_4,\\ell_4^{\\text{avg}},\\ell_4^{95}]]$\nwith all numbers printed as decimal floats (no units).", "solution": "The problem requires an analysis of two dimensionality reduction algorithms, Principal Component Analysis (PCA) and an Isometric Mapping (Isomap) variant, on a synthetic dataset. The dataset is a sampling of the planar curve $p(t) = (t^2, t^3)$ for $t \\in [-1,1]$, which forms a one-dimensional manifold with a cusp. The quality of the one-dimensional embeddings produced by these algorithms is to be quantified using a neighborhood preservation score and local Lipschitz estimates.\n\nThe solution proceeds systematically through the tasks outlined in the problem statement.\n\n**1. Data Generation and k-Nearest Neighbor (kNN) Graph Construction**\n\nFirst, we generate the data. $N=200$ points are sampled from the manifold $M = \\{p(t) = (t^2, t^3) \\mid t \\in [-1,1]\\}$ by creating a vector of $N$ equally spaced values of $t$ in the interval $[-1, 1]$ and then computing the corresponding $(x,y)$ coordinates. Let this data matrix be $X \\in \\mathbb{R}^{N \\times 2}$.\n\nNext, for a given connectivity parameter $k_{\\text{graph}}$, we construct a weighted, undirected kNN graph. The vertices of the graph are the $N$ sampled points. An edge is established between two points based on their proximity in the ambient space $\\mathbb{R}^2$. The weight of an edge $(i, j)$ is the Euclidean distance $\\|x_i - x_j\\|_2$. An undirected graph is constructed by connecting points $i$ and $j$ if either is within the other's $k_{\\text{graph}}$ nearest neighbors. The all-pairs Euclidean distance matrix is first computed. Then, for each point $i$, its $k_{\\text{graph}}$ nearest neighbors are identified. Edges are added to an adjacency matrix for each such neighborhood relationship, ensuring the matrix is symmetric to represent an undirected graph. Non-existent edges are assigned an infinite weight.\n\n**2. Intrinsic Geodesic Distance Approximation**\n\nThe intrinsic geodesic distance $d_M(i,j)$ between two points $x_i, x_j \\in M$ is the length of the shortest path between them along the manifold. This is approximated by computing the all-pairs shortest-path distances on the constructed kNN graph. An algorithm such as Floyd-Warshall or running Dijkstra's from each node is used for this purpose. This computation yields the geodesic distance matrix $d_M \\in \\mathbb{R}^{N \\times N}$, which serves as the input for the Isomap-like embedding and as the ground truth for calculating distortion metrics.\n\n**3. Embedding Algorithms**\n\nTwo methods are used to embed the $N$ points into a one-dimensional space, $\\mathbb{R}^1$. The resulting embedding is a vector $f \\in \\mathbb{R}^N$, where $f(i)$ is the coordinate of the $i$-th point.\n\n- **Isomap-like (Classical MDS on $d_M$)**: This method, central to the Isomap algorithm, aims to find an embedding that preserves the geodesic distances. It applies Classical Multidimensional Scaling (MDS) to the geodesic distance matrix $d_M$. The procedure is as follows:\n    1.  From the distance matrix $D = d_M$, compute the element-wise squared distance matrix $D^{\\circ 2}$.\n    2.  Apply a double-centering transformation to $D^{\\circ 2}$ to obtain the Gram matrix $B = -\\frac{1}{2} H (D^{\\circ 2}) H$, where $H = I - \\frac{1}{N}\\mathbf{1}\\mathbf{1}^\\top$ is the centering matrix.\n    3.  Perform an eigendecomposition of the symmetric matrix $B$, yielding eigenvalues $\\lambda_1 \\ge \\lambda_2 \\ge \\dots$ and corresponding orthonormal eigenvectors $v_1, v_2, \\dots$.\n    4.  The one-dimensional embedding is constructed using the largest strictly positive eigenvalue $\\lambda_1$ and its eigenvector $v_1$: $f_{\\text{iso}} = \\sqrt{\\lambda_1} v_1$.\n\n- **Principal Component Analysis (PCA)**: PCA is a linear dimensionality reduction technique that projects the data onto the direction of maximum variance.\n    1.  The original data $X \\in \\mathbb{R}^{N \\times 2}$ is centered by subtracting the mean of each feature: $X_{\\text{centered}} = X - \\bar{X}$.\n    2.  The sample covariance matrix $C = \\frac{1}{N-1}X_{\\text{centered}}^\\top X_{\\text{centered}}$ is computed.\n    3.  An eigendecomposition of the $2 \\times 2$ matrix $C$ is performed. The eigenvector $u_1$ corresponding to the largest eigenvalue is the first principal component.\n    4.  The one-dimensional embedding is obtained by projecting the centered data onto this principal component: $f_{\\text{pca}} = X_{\\text{centered}} u_1$.\n\n**4. Distortion Quantification**\n\nFor each embedding $f$, we quantify its quality using the pre-computed intrinsic distance matrix $d_M$ and a local neighborhood size $k_{\\text{local}}=6$.\n\n- **Neighborhood Preservation Score ($Q$)**: This metric evaluates how well local neighborhoods are preserved. For each point $i$, we identify its $k_{\\text{local}}$ nearest neighbors in the intrinsic space, $N_{k_{\\text{local}}}^M(i)$, and in the embedding space, $N_{k_{\\text{local}}}^Y(i)$. The score is the average fractional overlap between these two neighborhood sets over all points:\n$$Q = \\frac{1}{N} \\sum_{i=1}^N \\frac{|N_{k_{\\text{local}}}^M(i) \\cap N_{k_{\\text{local}}}^Y(i)|}{k_{\\text{local}}}$$\nA value of $Q$ near $1$ implies excellent preservation of local structure.\n\n- **Local Lipschitz Estimates ($L_i$, $L_{\\text{avg}}$, $L_{95}$)**: These metrics measure local stretching of the manifold. For each point $i$, the local Lipschitz estimate is the maximum ratio of the change in embedding coordinates to the change in intrinsic distance, over its intrinsic neighborhood:\n$$L_i = \\max_{j \\in N_{k_{\\text{local}}}^M(i)} \\frac{|f(i) - f(j)|}{d_M(i,j)}$$\nFor a perfect isometric embedding, all $L_i=1$. Large values of $L_i$ indicate that points close on the manifold are mapped far apart in the embedding, suggesting a \"tear\". We compute the mean ($L_{\\text{avg}}$) and the $95$th percentile ($L_{95}$) of the set $\\{L_i\\}_{i=1}^N$ to summarize the distribution of these estimates.\n\nThe program implements these steps for the four specified test cases and computes the triple $[Q, L_{\\text{avg}}, L_{95}]$ for each.\n- **PCA vs. Isomap**: PCA, being linear, is expected to fail at \"unrolling\" the cusp curve, instead folding its two branches on top of each other. This will lead to poor neighborhood preservation ($Q \\ll 1$) near the origin. Isomap is designed for this task and should perform better.\n- **Isomap and $k_{\\text{graph}}$**: The choice of $k_{\\text{graph}}$ is critical for Isomap. A small $k_{\\text{graph}}$ (e.g., $4$) risks creating a disconnected graph. A large $k_{\\text{graph}}$ (e.g., $30$) can cause \"short-circuit\" errors, where the kNN graph incorrectly connects points across the cusp due to their small Euclidean distance, violating the manifold's intrinsic topology. This would degrade the geodesic distance approximation and the final embedding. An intermediate value (e.g., $8$) is expected to yield the best result.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.sparse.csgraph import shortest_path\nfrom scipy.spatial.distance import pdist, squareform\n\ndef solve():\n    \"\"\"\n    Solves the manifold learning problem specified in the prompt.\n    \"\"\"\n\n    def run_isomap(d_M, N):\n        \"\"\"\n        Performs Isomap-like embedding using Classical MDS.\n        \"\"\"\n        # Element-wise square of the distance matrix\n        D2 = d_M**2\n        \n        # Double-centering\n        H = np.eye(N) - (1/N) * np.ones((N, N))\n        B = -0.5 * H @ D2 @ H\n        \n        # Eigendecomposition of the Gram matrix\n        # np.linalg.eigh is used for symmetric matrices and returns sorted eigenvalues\n        eigvals, eigvecs = np.linalg.eigh(B)\n        \n        # Select the top eigenpair (largest eigenvalue)\n        # eigh sorts in ascending order, so we take the last one.\n        lambda_1 = eigvals[-1]\n        v_1 = eigvecs[:, -1]\n        \n        # Handle potential small negative eigenvalues due to numerical precision\n        if lambda_1 < 0:\n            lambda_1 = 0\n            \n        # Return 1D embedding\n        return np.sqrt(lambda_1) * v_1\n\n    def run_pca(X, N):\n        \"\"\"\n        Performs PCA for 1D embedding.\n        \"\"\"\n        # Center the data\n        X_centered = X - np.mean(X, axis=0)\n        \n        # Compute covariance matrix\n        cov_mat = (X_centered.T @ X_centered) / (N - 1)\n        \n        # Eigendecomposition of the covariance matrix\n        eigvals, eigvecs = np.linalg.eigh(cov_mat)\n        \n        # Select the principal eigenvector (corresponding to the largest eigenvalue)\n        # eigh sorts in ascending order, so we take the last one.\n        u_1 = eigvecs[:, -1]\n        \n        # Project centered data onto the principal component\n        return X_centered @ u_1\n    \n    def compute_metrics(d_M, f_embedding, N, k_local):\n        \"\"\"\n        Computes the Q, L_avg, and L_95 metrics.\n        \"\"\"\n        # Find intrinsic and embedding space neighborhoods\n        intrinsic_neighbors = np.argsort(d_M, axis=1)[:, 1:k_local + 1]\n        \n        embedding_dists = np.abs(f_embedding[:, np.newaxis] - f_embedding)\n        embedding_neighbors = np.argsort(embedding_dists, axis=1)[:, 1:k_local + 1]\n\n        # Compute Neighborhood Preservation Score (Q)\n        q_sum = 0\n        for i in range(N):\n            intersection_size = len(np.intersect1d(intrinsic_neighbors[i], embedding_neighbors[i]))\n            q_sum += intersection_size / k_local\n        Q = q_sum / N\n\n        # Compute Local Lipschitz Estimates (L)\n        L_vals = np.zeros(N)\n        for i in range(N):\n            neighbors_i = intrinsic_neighbors[i]\n            d_M_local = d_M[i, neighbors_i]\n            d_f_local = np.abs(f_embedding[i] - f_embedding[neighbors_i])\n            \n            # np.divide handles division by zero (or inf) correctly for this case.\n            # If d_M_local is 0, it would be an error, but it won't be for j != i.\n            # If d_M_local is inf (disconnected graph), ratio becomes 0, which is fine.\n            ratios = np.divide(d_f_local, d_M_local, out=np.zeros_like(d_f_local), where=d_M_local!=0)\n            L_vals[i] = np.max(ratios)\n\n        L_avg = np.mean(L_vals)\n        L_95 = np.percentile(L_vals, 95)\n        \n        return [Q, L_avg, L_95]\n\n    # Global parameters\n    N = 200\n    k_local = 6\n    \n    # Define the four test cases\n    test_cases = [\n        {\"method\": \"isomap\", \"k_graph\": 8},\n        {\"method\": \"isomap\", \"k_graph\": 4},\n        {\"method\": \"isomap\", \"k_graph\": 30},\n        {\"method\": \"pca\", \"k_graph\": 8}\n    ]\n\n    # Generate the dataset from the cusp curve\n    t_vals = np.linspace(-1, 1, N)\n    X = np.vstack((t_vals**2, t_vals**3)).T\n\n    # Pre-compute Euclidean distances once\n    euclidean_dists = squareform(pdist(X, 'euclidean'))\n    \n    final_results = []\n    for case in test_cases:\n        method = case[\"method\"]\n        k_graph = case[\"k_graph\"]\n        \n        # 1. Construct kNN graph and compute geodesic distance matrix d_M\n        # Adjacency matrix weighted by distance, infinite for non-edges\n        adj_mat = np.full((N, N), np.inf)\n        np.fill_diagonal(adj_mat, 0)\n        \n        # Build a symmetric kNN graph (mutual kNN)\n        for i in range(N):\n            # Find k_graph nearest neighbors (excluding self)\n            neighbors = np.argsort(euclidean_dists[i, :])[1:k_graph + 1]\n            for j in neighbors:\n                adj_mat[i, j] = euclidean_dists[i, j]\n                adj_mat[j, i] = euclidean_dists[j, i] # ensure symmetry\n        \n        d_M = shortest_path(csgraph=adj_mat, directed=False)\n        \n        # Check for graph connectivity, problem statement implies it stays connected\n        if np.any(np.isinf(d_M)):\n            raise RuntimeError(f\"Graph is not connected for k_graph={k_graph}\")\n\n        # 2. Produce the 1D embedding\n        if method == \"isomap\":\n            f_embedding = run_isomap(d_M, N)\n        elif method == \"pca\":\n            f_embedding = run_pca(X, N)\n        \n        # 3. Compute and store metrics for the current case\n        metrics = compute_metrics(d_M, f_embedding, N, k_local)\n        final_results.append(metrics)\n    \n    # Format the final output string\n    result_str_parts = []\n    for res in final_results:\n        q, l_avg, l_95 = res\n        part = f\"[{q:.6f},{l_avg:.6f},{l_95:.6f}]\"\n        result_str_parts.append(part)\n        \n    final_output = f\"[{','.join(result_str_parts)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3144257"}, {"introduction": "Building on graph-based embeddings, we now turn to methods based on diffusion processes, such as Diffusion Maps. A crucial but often overlooked step in these algorithms is the normalization of the graph affinity matrix. This choice has significant consequences for the geometry of the embedding space. This exercise challenges you to compare the embeddings derived from a row-stochastic normalization versus a symmetric normalization, giving you practical experience in how these decisions impact the resulting diffusion distances and the interpretation of the low-dimensional representation [@problem_id:3144175].", "problem": "You are given a set of finite point clouds embedded in Euclidean space, and you will construct a weighted graph on each point cloud. From this graph, you will form two normalizations to define two related Markov chains: a row-stochastic random walk normalization and a symmetric normalization. Your task is to implement both normalizations, compute their leading embeddings, and quantify how the choice of normalization affects the Euclidean distances in embedding space relative to the diffusion distance. All computations should be performed in pure mathematical terms without physical units, and angles must be specified in radians.\n\nFoundational base: Start from the following fundamental definitions and facts.\n\n- Let $\\{x_i\\}_{i=1}^n \\subset \\mathbb{R}^d$ be a dataset. Define a symmetric, nonnegative weight matrix $W \\in \\mathbb{R}^{n \\times n}$ by\n$$\nW_{ij} = \\exp\\left(-\\frac{\\|x_i - x_j\\|^2}{2\\sigma^2}\\right) \\quad \\text{for } i \\neq j, \\quad W_{ii} = 0,\n$$\nwhere $\\sigma > 0$ is a specified bandwidth and $\\|\\cdot\\|$ is the Euclidean norm.\n\n- Define the diagonal degree matrix $D \\in \\mathbb{R}^{n \\times n}$ with entries $D_{ii} = \\sum_{j=1}^n W_{ij}$.\n\n- Define the row-stochastic random walk matrix $P \\in \\mathbb{R}^{n \\times n}$ by\n$$\nP = D^{-1} W,\n$$\nwhich satisfies $\\sum_{j=1}^n P_{ij} = 1$ for each $i$. The largest eigenvalue of $P$ is $1$, and for connected, undirected graphs derived from symmetric $W$, $P$ is similar to a symmetric matrix and has real eigenvalues.\n\n- Define the symmetric normalization $S \\in \\mathbb{R}^{n \\times n}$ by\n$$\nS = D^{-1/2} W D^{-1/2}.\n$$\nThe matrices $P$ and $S$ are related by similarity:\n$$\nS = D^{1/2} P D^{-1/2}, \\quad \\text{and} \\quad P = D^{-1/2} S D^{1/2}.\n$$\nTherefore, $P$ and $S$ have the same set of eigenvalues.\n\n- Let $\\{\\lambda_k\\}_{k=0}^{n-1}$ denote the eigenvalues of $S$ in descending order, with $\\lambda_0 = 1$, and let $\\{u_k\\}_{k=0}^{n-1}$ be corresponding orthonormal eigenvectors of $S$ (orthonormal in the standard Euclidean inner product). The right eigenvectors $\\{\\psi_k\\}_{k=0}^{n-1}$ of $P$ are related to the eigenvectors of $S$ via\n$$\n\\psi_k = D^{-1/2} u_k,\n$$\nand satisfy $P \\psi_k = \\lambda_k \\psi_k$.\n\n- For integer diffusion time $t \\geq 1$, consider the Diffusion Maps (DM) embedding derived from the row-stochastic normalization (using the right eigenvectors of $P$):\n$$\n\\Phi_t(i) = \\left(\\lambda_1^t \\psi_1(i), \\lambda_2^t \\psi_2(i), \\ldots, \\lambda_m^t \\psi_m(i)\\right) \\in \\mathbb{R}^m,\n$$\nfor a chosen embedding dimension $m \\in \\mathbb{N}$ excluding the trivial eigenpair $\\lambda_0 = 1$. The Euclidean distance in this embedding equals the diffusion distance:\n$$\nD_t(i,j)^2 = \\sum_{k=1}^{m} \\lambda_k^{2t} \\left(\\psi_k(i) - \\psi_k(j)\\right)^2,\n$$\nwhen $m$ is large enough to capture the spectrum, and is a monotone approximation when truncated.\n\n- Consider also the embedding from the symmetric normalization:\n$$\n\\Psi_t(i) = \\left(\\lambda_1^t u_1(i), \\lambda_2^t u_2(i), \\ldots, \\lambda_m^t u_m(i)\\right) \\in \\mathbb{R}^m.\n$$\nIts Euclidean distances are\n$$\n\\tilde{D}_t(i,j)^2 = \\sum_{k=1}^{m} \\lambda_k^{2t} \\left(u_k(i) - u_k(j)\\right)^2.\n$$\nIn general, $\\tilde{D}_t(i,j)$ is not equal to $D_t(i,j)$ unless the graph has constant degree, in which case $u_k(i) = \\sqrt{D_{ii}} \\, \\psi_k(i)$ implies $\\tilde{D}_t(i,j) = \\sqrt{D_{ii}} \\, D_t(i,j)$ for all $i$ and $j$, yielding a constant scaling factor.\n\nYour program must:\n\n1. For each test case, construct $W$, compute $D$, and form both $P$ and $S$.\n2. Compute the leading $m$ nontrivial eigenpairs $(\\lambda_k, u_k)$ of $S$ (excluding the trivial eigenvalue $\\lambda_0 = 1$), and obtain $\\psi_k = D^{-1/2} u_k$.\n3. Build the embeddings $\\Phi_t$ (row-stochastic) and $\\Psi_t$ (symmetric) of dimension $m$ for diffusion time $t$.\n4. Compute all pairwise Euclidean distances between embedded points for both embeddings:\n   - For the row-stochastic embedding, compute $D_t(i,j)$.\n   - For the symmetric embedding, compute $\\tilde{D}_t(i,j)$.\n5. Quantify the discrepancy between the two sets of distances using:\n   - The relative root-mean-square difference:\n     $$\n     E_{\\mathrm{rel}} = \\frac{\\sqrt{\\frac{1}{M}\\sum_{(i,j), i<j} \\left(\\tilde{D}_t(i,j) - D_t(i,j)\\right)^2}}{\\sqrt{\\frac{1}{M}\\sum_{(i,j), i<j} D_t(i,j)^2}},\n     $$\n     where the sum is over all unordered pairs $(i,j)$ with $i<j$, and $M = \\frac{n(n-1)}{2}$ is the number of pairs.\n   - The standard deviation of the pairwise ratios:\n     $$\n     \\mathrm{StdRatio} = \\mathrm{Std}\\left(\\left\\{\\frac{\\tilde{D}_t(i,j)}{D_t(i,j)} \\,:\\, i<j\\right\\}\\right),\n     $$\n     interpreting $\\frac{\\tilde{D}_t(i,j)}{D_t(i,j)}$ only for $D_t(i,j) > 0$.\n\n6. Output, for each test case, the two floats $E_{\\mathrm{rel}}$ and $\\mathrm{StdRatio}$, rounded to six decimal places.\n\nTest Suite:\n\n- Case $1$ (regular degrees on a circle):\n  - $n = 30$ points on the unit circle in $\\mathbb{R}^2$ at angles $\\theta_i = \\frac{2\\pi i}{n}$ for $i = 0, 1, \\ldots, n-1$, where angles are in radians; points are $x_i = (\\cos(\\theta_i), \\sin(\\theta_i))$.\n  - Bandwidth $\\sigma = 0.3$.\n  - Embedding dimension $m = 3$.\n  - Diffusion time $t = 1$.\n  Expected behavior: $\\mathrm{StdRatio}$ is near zero because degrees are equal, and $E_{\\mathrm{rel}}$ is near zero up to a constant scaling absorbed by the ratio behavior.\n\n- Case $2$ (moderately heterogeneous degrees on a line):\n  - $n = 30$ points in $\\mathbb{R}^1$, consisting of $18$ points uniformly spaced on $[0, 0.4]$ and $12$ points uniformly spaced on $[0.6, 1.0]$.\n  - Bandwidth $\\sigma = 0.15$.\n  - Embedding dimension $m = 3$.\n  - Diffusion time $t = 1$.\n  Expected behavior: Both $E_{\\mathrm{rel}}$ and $\\mathrm{StdRatio}$ are non-negligible due to degree heterogeneity.\n\n- Case $3$ (strongly heterogeneous degrees; near-disconnected clusters):\n  - $n = 30$ points in $\\mathbb{R}^1$, consisting of $25$ points uniformly spaced on $[0, 0.3]$ and $5$ points uniformly spaced on $[0.7, 1.0]$.\n  - Bandwidth $\\sigma = 0.1$.\n  - Embedding dimension $m = 3$.\n  - Diffusion time $t = 1$.\n  Expected behavior: Larger $E_{\\mathrm{rel}}$ and $\\mathrm{StdRatio}$ due to very uneven degrees and weak inter-cluster connectivity.\n\nFinal Output Format:\n\nYour program should produce a single line of output containing the six rounded results as a comma-separated list enclosed in square brackets, in the order\n$$\n[\\text{Case 1 } E_{\\mathrm{rel}}, \\text{Case 1 } \\mathrm{StdRatio}, \\text{Case 2 } E_{\\mathrm{rel}}, \\text{Case 2 } \\mathrm{StdRatio}, \\text{Case 3 } E_{\\mathrm{rel}}, \\text{Case 3 } \\mathrm{StdRatio}],\n$$\nfor example, $\\left[0.000123,0.000456,0.012345,0.067890,0.123456,0.234567\\right]$.", "solution": "The problem requires a comparison between two different normalization schemes for a graph affinity matrix in the context of manifold learning. Specifically, we will analyze the difference in Euclidean distances between points embedded using a row-stochastic normalization (standard for Diffusion Maps) and a symmetric normalization.\n\nThe solution proceeds in four main stages for each test case:\n1.  Construction of the graph and relevant matrices.\n2.  Spectral decomposition to find eigenvalues and eigenvectors.\n3.  Construction of the low-dimensional embeddings.\n4.  Computation of pairwise distances and the specified discrepancy metrics.\n\n**1. Graph Construction**\n\nGiven a set of points $\\{x_i\\}_{i=1}^n \\subset \\mathbb{R}^d$, we first construct a weighted graph where the points are vertices. The affinity between two distinct points $x_i$ and $x_j$ is quantified by a weight $W_{ij}$, computed using a Gaussian kernel with a specified bandwidth $\\sigma > 0$. The weight matrix $W \\in \\mathbb{R}^{n \\times n}$ has entries:\n$$ W_{ij} = \\exp\\left(-\\frac{\\|x_i - x_j\\|^2}{2\\sigma^2}\\right) \\quad \\text{for } i \\neq j, \\quad W_{ii} = 0 $$\nwhere $\\|\\cdot\\|$ denotes the Euclidean norm. This choice of kernel ensures that closer points have a stronger connection (larger weight). The matrix $W$ is symmetric and non-negative.\n\nFrom $W$, we define the diagonal degree matrix $D \\in \\mathbb{R}^{n \\times n}$, where each diagonal entry $D_{ii}$ represents the total affinity of point $x_i$:\n$$ D_{ii} = \\sum_{j=1}^n W_{ij} $$\nSince $W_{ij} > 0$ for all $i \\neq j$ (due to the nature of the exponential function), every point is connected to every other point, ensuring $D_{ii} > 0$ for all $i$. Thus, $D$ is invertible.\n\nTwo normalizations are then defined:\n- The **row-stochastic random walk matrix** $P \\in \\mathbb{R}^{n \\times n}$:\n$$ P = D^{-1} W $$\nEach row of $P$ sums to $1$, so $P_{ij}$ can be interpreted as the probability of transitioning from node $i$ to node $j$ in one step of a random walk on the graph.\n- The **symmetric normalization** $S \\in \\mathbb{R}^{n \\times n}$:\n$$ S = D^{-1/2} W D^{-1/2} $$\nwhere $D^{-1/2}$ is the diagonal matrix with entries $(D^{-1/2})_{ii} = 1/\\sqrt{D_{ii}}$. The matrix $S$ is symmetric, which is advantageous for numerical eigendecomposition.\n\n**2. Spectral Analysis**\n\nThe core of the method lies in the spectral properties of these matrices. $P$ and $S$ are related by a similarity transformation, $S = D^{1/2} P D^{-1/2}$, and thus share the same set of eigenvalues, which we denote as $\\{\\lambda_k\\}_{k=0}^{n-1}$ in descending order ($1 = \\lambda_0 \\geq \\lambda_1 \\geq \\dots \\geq \\lambda_{n-1}$).\n\nWe compute the eigenpairs of the symmetric matrix $S$. Let $\\{u_k\\}_{k=0}^{n-1}$ be the set of orthonormal eigenvectors of $S$ corresponding to the eigenvalues $\\{\\lambda_k\\}_{k=0}^{n-1}$.\n$$ S u_k = \\lambda_k u_k $$\nThe right eigenvectors of $P$, denoted $\\{\\psi_k\\}_{k=0}^{n-1}$, are then obtained from the eigenvectors of $S$ via the transformation:\n$$ \\psi_k = D^{-1/2} u_k $$\nThese vectors satisfy the eigenvalue equation $P \\psi_k = \\lambda_k \\psi_k$. The eigenvector $\\psi_0$ corresponding to $\\lambda_0=1$ is a constant vector and is considered trivial, so it is excluded from the embedding.\n\n**3. Embedding Construction**\n\nFor a chosen embedding dimension $m \\in \\mathbb{N}$ and diffusion time $t \\geq 1$, we construct two different embeddings into $\\mathbb{R}^m$.\n\n- The first embedding, $\\Phi_t: \\{x_i\\}_{i=1}^n \\to \\mathbb{R}^m$, is the standard Diffusion Map embedding derived from the row-stochastic matrix $P$. The $i$-th point $x_i$ is mapped to:\n$$ \\Phi_t(i) = \\left(\\lambda_1^t \\psi_1(i), \\lambda_2^t \\psi_2(i), \\ldots, \\lambda_m^t \\psi_m(i)\\right) $$\n- The second embedding, $\\Psi_t: \\{x_i\\}_{i=1}^n \\to \\mathbb{R}^m$, is constructed similarly but uses the eigenvectors of the symmetric matrix $S$:\n$$ \\Psi_t(i) = \\left(\\lambda_1^t u_1(i), \\lambda_2^t u_2(i), \\ldots, \\lambda_m^t u_m(i)\\right) $$\n\n**4. Distance Comparison and Metric Calculation**\n\nThe central task is to quantify the difference between the geometric structures of these two embeddings. We do this by comparing the pairwise Euclidean distances.\nFor any two points $x_i, x_j$, the squared Euclidean distance in the first embedding is the squared diffusion distance $D_t(i,j)^2$:\n$$ D_t(i,j)^2 = \\|\\Phi_t(i) - \\Phi_t(j)\\|^2 = \\sum_{k=1}^{m} \\lambda_k^{2t} \\left(\\psi_k(i) - \\psi_k(j)\\right)^2 $$\nThe squared Euclidean distance in the second embedding is:\n$$ \\tilde{D}_t(i,j)^2 = \\|\\Psi_t(i) - \\Psi_t(j)\\|^2 = \\sum_{k=1}^{m} \\lambda_k^{2t} \\left(u_k(i) - u_k(j)\\right)^2 $$\n\nWe compute two metrics to quantify the discrepancy between the sets of distances $\\{D_t(i,j)\\}$ and $\\{\\tilde{D}_t(i,j)\\}$ over all pairs $(i,j)$ with $i<j$.\n- The **relative root-mean-square difference** $E_{\\mathrm{rel}}$ measures the overall magnitude of the difference relative to the magnitude of the diffusion distances:\n$$ E_{\\mathrm{rel}} = \\frac{\\sqrt{\\frac{1}{M}\\sum_{i<j} \\left(\\tilde{D}_t(i,j) - D_t(i,j)\\right)^2}}{\\sqrt{\\frac{1}{M}\\sum_{i<j} D_t(i,j)^2}} = \\frac{\\left(\\sum_{i<j} (\\tilde{D}_t(i,j) - D_t(i,j))^2\\right)^{1/2}}{\\left(\\sum_{i<j} D_t(i,j)^2\\right)^{1/2}} $$\nwhere $M = n(n-1)/2$.\n- The **standard deviation of the pairwise ratios** $\\mathrm{StdRatio}$ measures the non-uniformity of the relationship between the two distance measures:\n$$ \\mathrm{StdRatio} = \\mathrm{Std}\\left(\\left\\{\\frac{\\tilde{D}_t(i,j)}{D_t(i,j)} \\mid i<j, D_t(i,j) > 0\\right\\}\\right) $$\nIf the degrees are uniform, $D$ is a multiple of the identity matrix, which implies $\\psi_k$ is proportional to $u_k$. This results in a constant ratio $\\tilde{D}_t(i,j)/D_t(i,j)$ for all pairs, and thus $\\mathrm{StdRatio}$ will be zero. Heterogeneity in data density leads to non-uniform degrees and a non-zero $\\mathrm{StdRatio}$.\n\nThe implementation calculates these quantities for each specified test case.", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import eigh\nfrom scipy.spatial.distance import pdist, cdist\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis for all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        {'id': 1, 'n': 30, 'sigma': 0.3, 'm': 3, 't': 1},\n        {'id': 2, 'n': 30, 'sigma': 0.15, 'm': 3, 't': 1},\n        {'id': 3, 'n': 30, 'sigma': 0.1, 'm': 3, 't': 1},\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        # 1. Generate data for the test case\n        n, sigma, m, t = case['n'], case['sigma'], case['m'], case['t']\n        \n        if case['id'] == 1:\n            # Case 1: Points on a unit circle\n            angles = 2 * np.pi * np.arange(n) / n\n            X = np.column_stack([np.cos(angles), np.sin(angles)])\n        elif case['id'] == 2:\n            # Case 2: Moderately heterogeneous degrees on a line\n            n1, n2 = 18, 12\n            X1 = np.linspace(0.0, 0.4, n1)\n            X2 = np.linspace(0.6, 1.0, n2)\n            X = np.concatenate((X1, X2)).reshape(-1, 1)\n        elif case['id'] == 3:\n            # Case 3: Strongly heterogeneous degrees on a line\n            n1, n2 = 25, 5\n            X1 = np.linspace(0.0, 0.3, n1)\n            X2 = np.linspace(0.7, 1.0, n2)\n            X = np.concatenate((X1, X2)).reshape(-1, 1)\n        \n        # 2. Construct W, D, and S\n        # Compute pairwise squared Euclidean distances to build the affinity matrix W\n        dist_sq = cdist(X, X, 'sqeuclidean')\n        W = np.exp(-dist_sq / (2 * sigma**2))\n        np.fill_diagonal(W, 0)\n        \n        # Compute degree matrix D and its inverse square root\n        D_diag = W.sum(axis=1)\n        D_diag[D_diag == 0] = 1e-12 # Avoid division by zero for isolated points\n        D_inv_sqrt_diag = 1.0 / np.sqrt(D_diag)\n        D_inv_sqrt = np.diag(D_inv_sqrt_diag)\n        \n        # Compute the symmetrically normalized matrix S\n        S = D_inv_sqrt @ W @ D_inv_sqrt\n        \n        # 3. Compute eigenpairs of S\n        evals, evecs = eigh(S)\n        \n        # Sort eigenvalues and eigenvectors in descending order\n        idx = np.argsort(evals)[::-1]\n        evals = evals[idx]\n        evecs = evecs[:, idx]\n        \n        # 4. Extract non-trivial eigenpairs and compute eigenvectors of P\n        # We exclude the first eigenpair (lambda_0=1, u_0)\n        lambdas_m = evals[1:m+1]\n        u_m = evecs[:, 1:m+1]\n        \n        # Compute right eigenvectors of P, psi_k = D^(-1/2) u_k\n        psi_m = D_inv_sqrt @ u_m\n        \n        # 5. Build the two embeddings\n        lambdas_m_t = lambdas_m**t\n        \n        # Embedding from row-stochastic normalization (P)\n        phi_embedding = psi_m * lambdas_m_t\n        # Embedding from symmetric normalization (S)\n        psi_embedding_sym = u_m * lambdas_m_t\n        \n        # 6. Compute pairwise Euclidean distances in both embedding spaces\n        dist_phi = pdist(phi_embedding, 'euclidean')\n        dist_psi_sym = pdist(psi_embedding_sym, 'euclidean')\n        \n        # 7. Quantify the discrepancy between distance sets\n        \n        # Relative root-mean-square difference (E_rel)\n        norm_dist_phi = np.linalg.norm(dist_phi)\n        if norm_dist_phi < 1e-12:\n            norm_diff = np.linalg.norm(dist_psi_sym - dist_phi)\n            E_rel = 0.0 if norm_diff < 1e-12 else np.inf\n        else:\n            E_rel = np.linalg.norm(dist_psi_sym - dist_phi) / norm_dist_phi\n            \n        # Standard deviation of pairwise ratios (StdRatio)\n        valid_indices = np.where(dist_phi > 1e-12)[0]\n        if len(valid_indices) > 1:\n            ratios = dist_psi_sym[valid_indices] / dist_phi[valid_indices]\n            StdRatio = np.std(ratios)\n        else:\n            StdRatio = 0.0 # Not enough pairs with non-zero distance for std dev\n            \n        results.extend([E_rel, StdRatio])\n\n    # Print the final results in the specified format\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```", "id": "3144175"}, {"introduction": "To deepen our understanding of graph-based NLDR, we must scrutinize the very notion of \"distance\" on a graph. Isomap relies on the shortest-path distance, but is this always the best choice? This practice contrasts the shortest-path metric with the more subtle and robust commute time distance, which is intrinsically linked to random walks and diffusion processes. By deriving the spectral representation of commute time and comparing it to the shortest-path distance on a simple graph, you will uncover the theoretical advantages of diffusion-based metrics in capturing the global connectivity of a manifold [@problem_id:3144263].", "problem": "Consider a connected, undirected, weighted graph with $n$ vertices and symmetric weight matrix $W \\in \\mathbb{R}^{n \\times n}$ with nonnegative entries, degree matrix $D = \\mathrm{diag}(d_{1},\\dots,d_{n})$ where $d_{i} = \\sum_{j=1}^{n} W_{ij}$, and total volume $\\mathrm{vol}(G) = \\sum_{i=1}^{n} d_{i}$. Let the random walk transition matrix be $P = D^{-1} W$, and the random walk Laplacian be $L_{\\mathrm{rw}} = I - P$. Assume the Markov chain defined by $P$ is ergodic and reversible with respect to the stationary distribution $\\pi$ where $\\pi_{i} = d_{i}/\\mathrm{vol}(G)$. Let $\\{ \\lambda_{k} \\}_{k=1}^{n}$ and $\\{ r_{k} \\}_{k=1}^{n}$ be the eigenvalues and right eigenvectors of $P$, with $1 = \\lambda_{1} > |\\lambda_{2}| \\ge \\cdots \\ge |\\lambda_{n}|$ and $\\{ r_{k} \\}_{k=1}^{n}$ chosen to be orthonormal in $\\ell^{2}(\\pi)$, i.e., $\\sum_{i=1}^{n} \\pi_{i} r_{k}(i) r_{\\ell}(i) = \\delta_{k\\ell}$ and $r_{1} \\equiv \\mathbf{1}$.\n\nDefine the diffusion map at time $t \\in \\mathbb{N}$ by the coordinate map $\\Psi_{t}: i \\mapsto (\\lambda_{2}^{t} r_{2}(i), \\dots, \\lambda_{n}^{t} r_{n}(i)) \\in \\mathbb{R}^{n-1}$, and the diffusion distance by $D_{t}^{2}(i,j) = \\sum_{k=2}^{n} \\lambda_{k}^{2t} ( r_{k}(i) - r_{k}(j) )^{2}$. Define the commute time between vertices $i$ and $j$ by $C_{ij} = H_{ij} + H_{ji}$, where $H_{ij}$ is the expected hitting time of $j$ starting from $i$ for the random walk with transition $P$.\n\nTask:\n- Starting from the definitions above and well-tested spectral facts for reversible Markov chains and graph Laplacians, derive an expression that represents $C_{ij}$ as a squared Euclidean distance in a diffusion-type embedding built from the eigenpairs $\\{(\\lambda_{k}, r_{k})\\}_{k=2}^{n}$ of $P$ and weights depending only on $\\lambda_{k}$. Your derivation must begin from the definitions of $L_{\\mathrm{rw}}$ and $P$, use only standard facts about their spectral decompositions and pseudoinverses, and arrive at a representation of $C_{ij}$ as a sum over $k \\in \\{2,\\dots,n\\}$ of terms of the form $(r_{k}(i) - r_{k}(j))^{2}$ multiplied by functions of $\\lambda_{k}$ and $\\mathrm{vol}(G)$.\n- Then, specialize to the weighted triangle with vertex set $\\{1,2,3\\}$ and nonzero weights $W_{12} = W_{21} = w > 0$, $W_{23} = W_{32} = w > 0$, and $W_{13} = W_{31} = a \\ge 0$. Using any valid method consistent with the definitions (for example, electrical network equivalence for reversible random walks on weighted graphs), compute the commute time $C_{13}$ between vertices $1$ and $3$ as a closed-form analytic function of $w$ and $a$ only.\n- For comparison, define the weighted shortest path length between vertices $1$ and $3$ by assigning each edge $(u,v)$ the length $\\ell_{uv} = 1/W_{uv}$ and taking the minimum path length with respect to these edge lengths. Express this shortest path length in terms of $w$ and $a$, and explain qualitatively how it compares to the commute time as $a$ increases from $0$ to $+\\infty$ for fixed $w > 0$. No numerical value is required for this qualitative comparison.\n\nYour final answer must be the single analytic expression you obtain for $C_{13}(w,a)$. Do not include any units. Do not provide inequalities or equations as the final answer. If you simplify, do so exactly; no rounding is required.", "solution": "The problem is assessed to be valid as it is scientifically grounded, well-posed, objective, and internally consistent. It presents a standard set of tasks in spectral graph theory and its application to random walks. We will proceed with the solution in three parts as requested.\n\n### Part 1: Derivation of the Commute Time Spectral Representation\n\nThe commute time $C_{ij}$ between two vertices $i$ and $j$ of a connected, weighted graph is related to the effective resistance $R_{\\mathrm{eff}}(i,j)$ in an electrical network where edge conductances are given by the graph weights $W_{uv}$. The relationship is given by the well-known formula:\n$$C_{ij} = \\mathrm{vol}(G) R_{\\mathrm{eff}}(i,j)$$\nwhere $\\mathrm{vol}(G) = \\sum_{k=1}^{n} d_k$ is the total volume of the graph.\n\nThe effective resistance can be expressed in terms of the pseudoinverse of the unnormalized graph Laplacian $L = D - W$. Let $\\mathbf{e}_i \\in \\mathbb{R}^n$ be the standard basis vector with a $1$ in the $i$-th position and $0$s elsewhere. The effective resistance is:\n$$R_{\\mathrm{eff}}(i,j) = (\\mathbf{e}_i - \\mathbf{e}_j)^T L^{\\dagger} (\\mathbf{e}_i - \\mathbf{e}_j)$$\nOur goal is to express this in terms of the eigenpairs of the random walk transition matrix $P = D^{-1}W$.\n\nLet us consider the symmetrically normalized Laplacian, $\\mathcal{L}$, defined as:\n$$\\mathcal{L} = D^{-1/2} L D^{-1/2} = D^{-1/2} (D-W) D^{-1/2} = I - D^{-1/2} W D^{-1/2}$$\nThe random walk Laplacian is $L_{\\mathrm{rw}} = I - P$. The eigenvalues of $P$ are $\\{\\lambda_k\\}_{k=1}^n$ with $1 = \\lambda_1 > |\\lambda_2| \\ge \\dots \\ge |\\lambda_n|$. The eigenvalues of $L_{\\mathrm{rw}}$ are $\\{1-\\lambda_k\\}_{k=1}^n$.\nThe matrix $\\mathcal{L}$ is similar to $L_{\\mathrm{rw}}$ via the transformation $\\mathcal{L} = D^{1/2} L_{\\mathrm{rw}} D^{-1/2}$. Therefore, $\\mathcal{L}$ has the same eigenvalues as $L_{\\mathrm{rw}}$, which are $\\mu_k = 1-\\lambda_k$ for $k=1, \\dots, n$. The corresponding eigenvectors of $\\mathcal{L}$ are $v_k = D^{1/2} r_k$, where $r_k$ are the right eigenvectors of $P$.\n\nThe eigenvectors $\\{r_k\\}_{k=1}^n$ are given to be orthonormal in the space $\\ell^2(\\pi)$, meaning $\\sum_{i=1}^n \\pi_i r_k(i) r_l(i) = \\delta_{kl}$. Since $\\pi_i = d_i / \\mathrm{vol}(G)$, this is equivalent to $\\frac{1}{\\mathrm{vol}(G)} \\sum_{i=1}^n d_i r_k(i) r_l(i) = \\delta_{kl}$. This can be written in vector form as $\\frac{1}{\\mathrm{vol}(G)} r_k^T D r_l = \\delta_{kl}$.\n\nLet's check the orthogonality of the eigenvectors $v_k$ of $\\mathcal{L}$:\n$$v_k^T v_l = (D^{1/2} r_k)^T (D^{1/2} r_l) = r_k^T D r_l = \\mathrm{vol}(G) \\delta_{kl}$$\nThis shows that the eigenvectors $\\{v_k\\}$ are orthogonal but not orthonormal. An orthonormal set of eigenvectors for $\\mathcal{L}$ is $\\{u_k = v_k / \\sqrt{\\mathrm{vol}(G)}\\}_{k=1}^n$.\n\nThe spectral decomposition of the pseudoinverse of $\\mathcal{L}$ is given by summing over the non-zero eigenvalues:\n$$\\mathcal{L}^{\\dagger} = \\sum_{k=2}^{n} \\frac{1}{\\mu_k} u_k u_k^T = \\sum_{k=2}^{n} \\frac{1}{1-\\lambda_k} \\frac{v_k v_k^T}{\\mathrm{vol}(G)}$$\nWe can relate the pseudoinverse of $L$ to that of $\\mathcal{L}$. From $\\mathcal{L} = D^{-1/2} L D^{-1/2}$, we have $L = D^{1/2} \\mathcal{L} D^{1/2}$. The pseudoinverse relationship is $L^{\\dagger} = D^{-1/2} \\mathcal{L}^{\\dagger} D^{-1/2}$. Substituting the expression for $\\mathcal{L}^{\\dagger}$:\n$$L^{\\dagger} = D^{-1/2} \\left( \\sum_{k=2}^{n} \\frac{1}{1-\\lambda_k} \\frac{v_k v_k^T}{\\mathrm{vol}(G)} \\right) D^{-1/2}$$\nNow, we substitute this into the formula for $R_{\\mathrm{eff}}(i,j)$:\n$$R_{\\mathrm{eff}}(i,j) = (\\mathbf{e}_i - \\mathbf{e}_j)^T D^{-1/2} \\left( \\sum_{k=2}^{n} \\frac{1}{1-\\lambda_k} \\frac{v_k v_k^T}{\\mathrm{vol}(G)} \\right) D^{-1/2} (\\mathbf{e}_i - \\mathbf{e}_j)$$\nLet's analyze the term $(D^{-1/2}(\\mathbf{e}_i - \\mathbf{e}_j))^T v_k$. The vector $D^{-1/2}(\\mathbf{e}_i - \\mathbf{e}_j)$ is a column vector whose $m$-th entry is $\\frac{1}{\\sqrt{d_m}}\\delta_{mi} - \\frac{1}{\\sqrt{d_m}}\\delta_{mj}$. The inner product with $v_k$ is:\n$$(D^{-1/2}(\\mathbf{e}_i - \\mathbf{e}_j))^T v_k = \\sum_{m=1}^n \\left( \\frac{\\delta_{mi}}{\\sqrt{d_m}} - \\frac{\\delta_{mj}}{\\sqrt{d_m}} \\right) v_k(m) = \\frac{v_k(i)}{\\sqrt{d_i}} - \\frac{v_k(j)}{\\sqrt{d_j}}$$\nUsing $v_k(m) = \\sqrt{d_m} r_k(m)$, this simplifies to:\n$$\\frac{\\sqrt{d_i} r_k(i)}{\\sqrt{d_i}} - \\frac{\\sqrt{d_j} r_k(j)}{\\sqrt{d_j}} = r_k(i) - r_k(j)$$\nThe expression for $R_{\\mathrm{eff}}(i,j)$ becomes a sum of squared terms:\n$$R_{\\mathrm{eff}}(i,j) = \\frac{1}{\\mathrm{vol}(G)} \\sum_{k=2}^{n} \\frac{1}{1-\\lambda_k} (r_k(i) - r_k(j))^2$$\nFinally, we find the commute time $C_{ij}$:\n$$C_{ij} = \\mathrm{vol}(G) R_{\\mathrm{eff}}(i,j) = \\mathrm{vol}(G) \\left( \\frac{1}{\\mathrm{vol}(G)} \\sum_{k=2}^{n} \\frac{1}{1-\\lambda_k} (r_k(i) - r_k(j))^2 \\right)$$\n$$C_{ij} = \\sum_{k=2}^{n} \\frac{1}{1-\\lambda_k} (r_k(i) - r_k(j))^2$$\nThis formula expresses $C_{ij}$ as a weighted sum of squared differences of eigenvector components, where the weights $1/(1-\\lambda_k)$ depend only on the eigenvalues of $P$.\n\n### Part 2: Calculation of $C_{13}$ for the Weighted Triangle\n\nWe use the electrical network analogy where the conductance of an edge $(u,v)$ is $W_{uv}$, and its resistance is $1/W_{uv}$.\nThe graph has vertices $\\{1,2,3\\}$ with weights $W_{12}=w$, $W_{23}=w$, and $W_{13}=a$.\nThe degrees of the vertices are:\n$d_1 = W_{12} + W_{13} = w+a$\n$d_2 = W_{21} + W_{23} = w+w = 2w$\n$d_3 = W_{31} + W_{32} = a+w = w+a$\nThe total volume of the graph is:\n$\\mathrm{vol}(G) = d_1 + d_2 + d_3 = (w+a) + (2w) + (w+a) = 4w+2a$.\n\nTo find the effective resistance $R_{\\mathrm{eff}}(1,3)$, we consider the electrical circuit equivalent. There are two parallel paths for current to flow from vertex $1$ to vertex $3$:\n1. The direct edge $(1,3)$ with resistance $R_{\\text{direct}} = 1/W_{13} = 1/a$ (for $a>0$).\n2. The path through vertex $2$, consisting of edges $(1,2)$ and $(2,3)$ in series. The total resistance of this path is $R_{\\text{series}} = 1/W_{12} + 1/W_{23} = 1/w + 1/w = 2/w$.\n\nThe total effective resistance $R_{\\mathrm{eff}}(1,3)$ is the parallel combination of these two paths:\n$$\\frac{1}{R_{\\mathrm{eff}}(1,3)} = \\frac{1}{R_{\\text{direct}}} + \\frac{1}{R_{\\text{series}}} = a + \\frac{1}{2/w} = a + \\frac{w}{2} = \\frac{2a+w}{2}$$\nThis formula also holds for $a=0$, where the direct path is an open circuit.\nThus, the effective resistance is:\n$$R_{\\mathrm{eff}}(1,3) = \\frac{2}{2a+w}$$\nNow, we can compute the commute time $C_{13}$:\n$$C_{13} = \\mathrm{vol}(G) \\cdot R_{\\mathrm{eff}}(1,3) = (4w+2a) \\cdot \\frac{2}{2a+w} = 2(2w+a) \\frac{2}{2a+w} = \\frac{4(2w+a)}{w+2a}$$\n\n### Part 3: Comparison with Weighted Shortest Path Length\n\nThe weighted shortest path length $S_{ij}$ is defined by assigning each edge $(u,v)$ a length $\\ell_{uv} = 1/W_{uv}$ and finding the path with the minimum total length. For the path between vertices $1$ and $3$, there are two possibilities:\n1. The direct path $1 \\to 3$ with length $\\ell_{13} = 1/a$ (for $a>0$).\n2. The path through vertex $2$, $1 \\to 2 \\to 3$, with length $\\ell_{12} + \\ell_{23} = 1/w + 1/w = 2/w$.\n\nThe shortest path length is the minimum of these two:\n$$S_{13} = \\min\\left(\\frac{1}{a}, \\frac{2}{w}\\right)$$\n\nNow, we compare the behavior of $C_{13}$ and $S_{13}$ as the weight of the direct edge, $a$, increases from $0$ to $+\\infty$ for a fixed $w>0$.\n\n- **Shortest Path Length $S_{13}$**:\n  As $a \\to \\infty$, the length of the direct path $1/a \\to 0$. Since $2/w$ is a positive constant, for sufficiently large $a$ (specifically, for $a > w/2$), we have $1/a < 2/w$. Thus, $S_{13} = 1/a$. In the limit:\n  $$\\lim_{a \\to \\infty} S_{13} = \\lim_{a \\to \\infty} \\frac{1}{a} = 0$$\n  The shortest path length vanishes, reflecting that an infinitely strong connection makes the distance negligible.\n\n- **Commute Time $C_{13}$**:\n  We examine the limit of $C_{13}(w,a) = \\frac{4(2w+a)}{w+2a}$ as $a \\to \\infty$:\n  $$\\lim_{a \\to \\infty} C_{13} = \\lim_{a \\to \\infty} \\frac{4(2w+a)}{w+2a} = \\lim_{a \\to \\infty} \\frac{4a(2w/a+1)}{a(w/a+2)} = \\frac{4(0+1)}{0+2} = 2$$\n  The commute time approaches a non-zero constant, $2$.\n\n- **Qualitative Explanation**:\nThe shortest path metric is a local measure; it identifies the single most efficient route and ignores all others. As the direct connection $(1,3)$ becomes a \"superhighway\" (large $a$), its length $1/a$ tends to zero, and this is all that the shortest path metric considers.\n\nThe commute time, however, is a global measure related to the behavior of a random walk on the graph. A random walker at vertex $1$ chooses its next step probabilistically. The probability of moving to vertex $3$ is $P_{13} = W_{13}/d_1 = a/(w+a)$, and the probability of moving to vertex $2$ is $P_{12} = W_{12}/d_1 = w/(w+a)$. As $a \\to \\infty$, $P_{13} \\to 1$ and $P_{12} \\to 0$. This implies that a walk starting at $1$ will almost certainly reach $3$ in one step, so the expected hitting time $H_{13} \\to 1$. Symmetrically, $H_{31} \\to 1$. Therefore, $C_{13} = H_{13} + H_{31} \\to 1+1=2$. The random walk can still take the \"detour\" through vertex $2$, although with decreasing probability. This possibility of taking longer paths, inherent in the random process, prevents the average round-trip time from vanishing, unlike the shortest path distance. The commute time encapsulates the overall connectivity of the graph, not just the single best path.", "answer": "$$\\boxed{\\frac{4(2w+a)}{w+2a}}$$", "id": "3144263"}]}