{"hands_on_practices": [{"introduction": "Our first practice problem provides a hands-on implementation of the classic Coordinate Ascent Variational Inference (CAVI) algorithm for a Gaussian Mixture Model (GMM). This exercise serves as a foundational blueprint, guiding you through the derivation and implementation of the iterative updates for both the latent responsibilities and the component parameters. By working through this canonical example [@problem_id:3191998], you will build concrete intuition for the mean-field approach and grapple with practical modeling issues like the label switching problem.", "problem": "Consider a one-dimensional, two-component Gaussian mixture model with latent component indicators and unknown component means. Let there be $K=2$ components and $N$ scalar observations $\\{x_n\\}_{n=1}^N$. The generative model is defined by the following well-tested construction:\n- Latent indicators $z_n \\in \\{1,2\\}$ are drawn independently from a categorical distribution with mixing weights $\\pi = (\\pi_1,\\pi_2)$, where $\\pi_k \\in (0,1)$ and $\\pi_1 + \\pi_2 = 1$.\n- Conditional on $z_n = k$ and the component mean $\\mu_k$, each observation $x_n$ is drawn from a Gaussian distribution with known variance $\\sigma^2$, that is $x_n \\mid (z_n=k,\\mu_k) \\sim \\mathcal{N}(\\mu_k,\\sigma^2)$.\n- The unknown component means have independent Gaussian priors $\\mu_k \\sim \\mathcal{N}(m_{0k},\\tau_{0k}^{-1})$, where $\\tau_{0k} > 0$ is the prior precision and $m_{0k}$ is the prior mean.\n\nUsing the Evidence Lower Bound (ELBO) definition from variational inference and the mean-field factorization $q(z,\\mu) = \\left(\\prod_{n=1}^N q(z_n)\\right)\\left(\\prod_{k=1}^2 q(\\mu_k)\\right)$, derive from first principles the coordinate ascent variational updates for:\n1. The responsibilities $r_{nk} = q(z_n = k)$ for $n \\in \\{1,\\dots,N\\}$ and $k \\in \\{1,2\\}$.\n2. The variational factors $q(\\mu_k)$, including closed-form expressions for their means and precisions.\n\nBase your derivation on the foundational ELBO identity $\\mathcal{L}(q) = \\mathbb{E}_q[\\log p(x,z,\\mu)] - \\mathbb{E}_q[\\log q(z,\\mu)]$ and the optimal mean-field factor update rule $q^*(\\theta_i) \\propto \\exp\\left(\\mathbb{E}_{q(\\theta_{\\setminus i})}[\\log p(x,\\theta)]\\right)$ for each factor. Do not use shortcut formulas beyond those definitions.\n\nAfter deriving the updates, implement a deterministic coordinate ascent algorithm that:\n- Initializes responsibilities uniformly, $r_{nk} = 1/2$ for all $n,k$.\n- Alternates between updating the parameters of $q(\\mu_k)$ and updating the responsibilities $r_{nk}$.\n- Stops when the sum of absolute differences in responsibilities across all $n,k$ between successive iterations is at most a tolerance $T$ or upon reaching a maximum number of iterations.\n\nUse the following test suite to assess label switching implications and a boundary case. For each test, run the algorithm twice if needed and compare the outcomes according to the description. Use tolerance $T = 10^{-8}$ for comparisons of responsibilities.\n\n- Test $1$ (happy path, label invariance under full permutation):\n  - Observations: $x = [\\, -2.2,\\, -1.9,\\, -1.7,\\, 1.5,\\, 1.8,\\, 2.2 \\,]$.\n  - Known variance: $\\sigma^2 = 0.25$.\n  - Mixing weights: $\\pi = [\\, 0.5,\\, 0.5 \\,]$.\n  - Prior means: $m_0 = [\\, -2.0,\\, 2.0 \\,]$.\n  - Prior precisions: $\\tau_0 = [\\, 1.0,\\, 1.0 \\,]$.\n  Run the algorithm to obtain responsibilities $R^{(A)}$. Then run it again with the labels fully permuted, that is mixing weights and prior means swapped: $\\pi' = [\\, 0.5,\\, 0.5 \\,]$, $m_0' = [\\, 2.0,\\, -2.0 \\,]$, $\\tau_0' = [\\, 1.0,\\, 1.0 \\,]$, producing $R^{(B)}$. Check whether $R^{(A)}$ equals $R^{(B)}$ up to a column permutation, within tolerance $T$. The result should be a boolean.\n\n- Test $2$ (edge case, partial permutation that breaks invariance):\n  - Observations: $x = [\\, -2.2,\\, -1.9,\\, -1.7,\\, 1.5,\\, 1.8,\\, 2.2 \\,]$.\n  - Known variance: $\\sigma^2 = 0.25$.\n  - Mixing weights: $\\pi = [\\, 0.7,\\, 0.3 \\,]$.\n  - Prior means: $m_0 = [\\, -2.0,\\, 2.0 \\,]$.\n  - Prior precisions: $\\tau_0 = [\\, 1.0,\\, 1.0 \\,]$.\n  Run the algorithm to obtain responsibilities $R^{(C)}$. Then run it again with prior means swapped but mixing weights kept in the original order (a partial permutation): $m_0' = [\\, 2.0,\\, -2.0 \\,]$, $\\pi' = [\\, 0.7,\\, 0.3 \\,]$, producing $R^{(D)}$. Determine whether $R^{(C)}$ differs from $R^{(D)}$ beyond the tolerance $T$ even after a column permutation. The result should be a boolean indicating non-invariance.\n\n- Test $3$ (boundary condition, indistinguishable components):\n  - Observations: $x = [\\, -1.0,\\, 1.0 \\,]$.\n  - Known variance: $\\sigma^2 = 1.0$.\n  - Mixing weights: $\\pi = [\\, 0.5,\\, 0.5 \\,]$.\n  - Prior means: $m_0 = [\\, 0.0,\\, 0.0 \\,]$.\n  - Prior precisions: $\\tau_0 = [\\, 100.0,\\, 100.0 \\,]$.\n  Run the algorithm once and compute the maximum absolute deviation of the responsibilities from $0.5$ across all data points and components. The result should be a float.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[\\,\\text{result1},\\text{result2},\\text{result3}\\,]$), where $\\text{result1}$ and $\\text{result2}$ are booleans for Tests $1$ and $2$, and $\\text{result3}$ is a float for Test $3$.", "solution": "The problem is to derive and implement a coordinate ascent variational inference (CAVI) algorithm for a two-component Gaussian mixture model (GMM). The derivation will be based on the mean-field approximation and the general form of the optimal variational factor.\n\n### 1. Model Specification\n\nLet the observed data be a set of $N$ scalars $\\{x_n\\}_{n=1}^N$. The latent variables are the component means $\\mu = \\{\\mu_1, \\mu_2\\}$ and the component assignments $z = \\{z_n\\}_{n=1}^N$, where each $z_n \\in \\{1, 2\\}$. For convenience, we represent $z_n$ using a one-hot encoding, where $z_{nk}=1$ if the $n$-th observation is from component $k$, and $z_{nk}=0$ otherwise.\n\nThe generative process is as follows:\n- The prior for each component mean is an independent Gaussian:\n$$p(\\mu_k) = \\mathcal{N}(\\mu_k | m_{0k}, \\tau_{0k}^{-1})$$\nwhere $m_{0k}$ is the prior mean and $\\tau_{0k} > 0$ is the prior precision.\n- The latent component indicators $z_n$ are drawn from a categorical distribution with known mixing weights $\\pi = (\\pi_1, \\pi_2)$:\n$$p(z_n | \\pi) = \\prod_{k=1}^2 \\pi_k^{z_{nk}}$$\n- Each observation $x_n$ is drawn from a Gaussian distribution conditioned on its assigned component $k$ and the corresponding mean $\\mu_k$, with a known variance $\\sigma^2$:\n$$p(x_n | z_n, \\mu) = \\prod_{k=1}^2 \\mathcal{N}(x_n | \\mu_k, \\sigma^2)^{z_{nk}}$$\n\nThe full joint probability distribution over all variables (observed and latent) is given by:\n$$p(x, z, \\mu) = p(\\mu) p(z | \\pi) p(x | z, \\mu) = \\left(\\prod_{k=1}^2 p(\\mu_k)\\right) \\left(\\prod_{n=1}^N p(z_n | \\pi)\\right) \\left(\\prod_{n=1}^N p(x_n | z_n, \\mu)\\right)$$\nThe logarithm of the joint distribution is:\n$$\\log p(x, z, \\mu) = \\sum_{k=1}^2 \\log p(\\mu_k) + \\sum_{n=1}^N \\sum_{k=1}^2 z_{nk} \\log \\pi_k + \\sum_{n=1}^N \\sum_{k=1}^2 z_{nk} \\log \\mathcal{N}(x_n | \\mu_k, \\sigma^2)$$\n\n### 2. Variational Inference Setup\n\nWe use a mean-field variational family to approximate the true posterior $p(z, \\mu | x)$. The variational distribution $q(z, \\mu)$ factorizes as:\n$$q(z, \\mu) = q(z)q(\\mu) = \\left(\\prod_{n=1}^N q(z_n)\\right) \\left(\\prod_{k=1}^2 q(\\mu_k)\\right)$$\nwhere each $q(z_n)$ is a categorical distribution over $K=2$ components, characterized by probabilities $r_{nk} = q(z_n=k)$, and each $q(\\mu_k)$ is a distribution over the mean of component $k$.\n\nThe coordinate ascent algorithm iteratively optimizes each factor $q(\\theta_i)$ while holding the others fixed. The optimal form for a factor $q^*(\\theta_i)$ is given by:\n$$\\log q^*(\\theta_i) = \\mathbb{E}_{q(\\theta_{\\setminus i})}[\\log p(x, \\theta)] + \\mathrm{constant}$$\nwhere $\\theta_i$ is one of the latent variables ($z_n$ or $\\mu_k$) and $\\theta_{\\setminus i}$ denotes all other latent variables.\n\n### 3. Derivation of Update for $q(\\mu_k)$\n\nTo find the optimal form for $q(\\mu_k)$, we apply the general update rule. The logarithm of the optimal distribution $q^*(\\mu_k)$ is proportional to the expectation of the log-joint probability with respect to all other factors, $q(z)$ and $q(\\mu_{j \\neq k})$:\n$$\\log q^*(\\mu_k) = \\mathbb{E}_{q(z)}[\\log p(x, z, \\mu)] + \\mathrm{constant}$$\nWe only need to consider terms in $\\log p(x, z, \\mu)$ that depend on $\\mu_k$:\n$$\\log q^*(\\mu_k) = \\mathbb{E}_{q(z)}\\left[\\sum_{n=1}^N z_{nk} \\log \\mathcal{N}(x_n | \\mu_k, \\sigma^2)\\right] + \\log \\mathcal{N}(\\mu_k | m_{0k}, \\tau_{0k}^{-1}) + \\mathrm{const}$$\nThe expectation $\\mathbb{E}_{q(z)}[z_{nk}]$ is the responsibility $r_{nk}$. Let $\\tau = 1/\\sigma^2$ be the known data precision.\n$$\\log q^*(\\mu_k) = \\sum_{n=1}^N r_{nk} \\left(-\\frac{1}{2} \\log(2\\pi\\sigma^2) - \\frac{\\tau}{2}(x_n - \\mu_k)^2\\right) + \\left(-\\frac{1}{2} \\log(2\\pi\\tau_{0k}^{-1}) - \\frac{\\tau_{0k}}{2}(\\mu_k - m_{0k})^2\\right) + \\mathrm{const}$$\nTo identify the form of $q^*(\\mu_k)$, we collect terms involving $\\mu_k$:\n$$\\log q^*(\\mu_k) = -\\frac{\\tau}{2} \\sum_{n=1}^N r_{nk}(x_n^2 - 2x_n\\mu_k + \\mu_k^2) - \\frac{\\tau_{0k}}{2}(\\mu_k^2 - 2\\mu_k m_{0k} + m_{0k}^2) + \\mathrm{const}$$\nGrouping terms by powers of $\\mu_k$:\n- Term with $\\mu_k^2$: $-\\frac{1}{2}\\mu_k^2 \\left(\\tau_{0k} + \\tau \\sum_{n=1}^N r_{nk}\\right)$\n- Term with $\\mu_k$: $\\mu_k \\left(\\tau_{0k}m_{0k} + \\tau \\sum_{n=1}^N r_{nk}x_n\\right)$\nThis is a quadratic form in $\\mu_k$, which is characteristic of the logarithm of a Gaussian density. A Gaussian $\\mathcal{N}(\\mu | m, \\lambda^{-1})$ has a log-density of the form $-\\frac{\\lambda}{2}\\mu^2 + \\lambda m \\mu + \\mathrm{const}$.\nBy matching coefficients, we find that $q^*(\\mu_k)$ is a Gaussian distribution $\\mathcal{N}(\\mu_k | m_k, \\tau_k^{-1})$ with precision $\\tau_k$ and mean $m_k$:\n$$\\tau_k = \\tau_{0k} + \\tau \\sum_{n=1}^N r_{nk}$$\n$$m_k = \\frac{\\tau_{0k}m_{0k} + \\tau \\sum_{n=1}^N r_{nk}x_n}{\\tau_k}$$\nThese are the update equations for the parameters of the variational distribution $q(\\mu_k)$.\n\n### 4. Derivation of Update for $q(z_n)$\n\nSimilarly, we find the optimal form for $q(z_n)$ by taking the expectation of the log-joint with respect to all other factors, $\\{q(z_j)\\}_{j \\neq n}$ and $\\{q(\\mu_k)\\}_{k=1}^2$:\n$$\\log q^*(z_n) = \\mathbb{E}_{q(\\mu)}[\\log p(x, z, \\mu)] + \\mathrm{constant}$$\nWe collect terms that depend on $z_n$:\n$$\\log q^*(z_n) = \\sum_{k=1}^2 z_{nk} \\log \\pi_k + \\sum_{k=1}^2 z_{nk} \\mathbb{E}_{q(\\mu_k)}[\\log \\mathcal{N}(x_n | \\mu_k, \\sigma^2)] + \\mathrm{const}$$\nThis implies that $q^*(z_n)$ is a categorical distribution. The log-probability for component $k$ is:\n$$\\log q^*(z_n=k) \\equiv \\log r_{nk} \\propto \\log \\pi_k + \\mathbb{E}_{q(\\mu_k)}[\\log \\mathcal{N}(x_n | \\mu_k, \\sigma^2)]$$\nLet's expand the expectation term:\n$$\\mathbb{E}_{q(\\mu_k)}[\\log \\mathcal{N}(x_n | \\mu_k, \\sigma^2)] = \\mathbb{E}_{q(\\mu_k)}\\left[-\\frac{1}{2}\\log(2\\pi\\sigma^2) - \\frac{\\tau}{2}(x_n - \\mu_k)^2\\right]$$\n$$= -\\frac{1}{2}\\log(2\\pi\\sigma^2) - \\frac{\\tau}{2} \\mathbb{E}_{q(\\mu_k)}[x_n^2 - 2x_n\\mu_k + \\mu_k^2]$$\n$$= -\\frac{1}{2}\\log(2\\pi\\sigma^2) - \\frac{\\tau}{2} (x_n^2 - 2x_n\\mathbb{E}[\\mu_k] + \\mathbb{E}[\\mu_k^2])$$\nFrom our derived form of $q(\\mu_k) = \\mathcal{N}(\\mu_k | m_k, \\tau_k^{-1})$, we have:\n$$\\mathbb{E}[\\mu_k] = m_k$$\n$$\\mathbb{E}[\\mu_k^2] = \\mathrm{Var}[\\mu_k] + (\\mathbb{E}[\\mu_k])^2 = \\tau_k^{-1} + m_k^2$$\nSubstituting these into the expression for $\\log q^*(z_n=k)$ and dropping terms constant with respect to $k$ (like $-\\frac{1}{2}\\log(2\\pi\\sigma^2)$ and $-\\frac{\\tau}{2}x_n^2$):\n$$\\log \\tilde{\\rho}_{nk} \\propto \\log \\pi_k + \\tau x_n \\mathbb{E}[\\mu_k] - \\frac{\\tau}{2} \\mathbb{E}[\\mu_k^2]$$\n$$\\log \\tilde{\\rho}_{nk} = \\log \\pi_k + \\tau x_n m_k - \\frac{\\tau}{2}(m_k^2 + \\tau_k^{-1})$$\nThe responsibilities $r_{nk} = q(z_n=k)$ are obtained by normalizing the exponentiated values:\n$$r_{nk} = \\frac{\\exp(\\log \\tilde{\\rho}_{nk})}{\\sum_{j=1}^2 \\exp(\\log \\tilde{\\rho}_{nj})}$$\n\n### 5. Algorithm Summary\n\nThe coordinate ascent variational inference (CAVI) algorithm proceeds as follows:\n1.  **Initialize**: Initialize responsibilities $r_{nk}$ (e.g., uniformly as $r_{nk} = 1/2$).\n2.  **Iterate** until convergence:\n    a. **Update for $q(\\mu)$ (M-like step)**: For each component $k=1,2$, update the parameters $m_k$ and $\\tau_k$ of the variational distribution $q(\\mu_k)$ using the current responsibilities $r_{nk}$:\n       $$\\tau_k \\leftarrow \\tau_{0k} + \\tau \\sum_{n=1}^N r_{nk}$$\n       $$m_k \\leftarrow \\frac{\\tau_{0k}m_{0k} + \\tau \\sum_{n=1}^N r_{nk}x_n}{\\tau_k}$$\n    b. **Update for $q(z)$ (E-like step)**: For each data point $n=1,\\dots,N$, update the responsibilities $r_{nk}$ using the updated parameters of $q(\\mu)$:\n       $$\\log \\tilde{\\rho}_{nk} \\leftarrow \\log \\pi_k + \\tau x_n m_k - \\frac{\\tau}{2}(m_k^2 + \\tau_k^{-1})$$\n       $$r_{nk} \\leftarrow \\frac{\\exp(\\log \\tilde{\\rho}_{nk})}{\\sum_{j=1}^2 \\exp(\\log \\tilde{\\rho}_{nj})}$$\n    c. **Check for convergence**: Stop if the change in responsibilities is below a tolerance $T$ or a maximum number of iterations is reached.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the variational inference problem for the provided test cases.\n    \"\"\"\n\n    def run_cavi(x, sigma_sq, pi, m0, tau0, tol, max_iter=100):\n        \"\"\"\n        Runs the Coordinate Ascent Variational Inference algorithm for a GMM.\n\n        Args:\n            x (np.ndarray): 1D array of observations.\n            sigma_sq (float): Known variance of the Gaussian components.\n            pi (np.ndarray): 1D array of mixing weights.\n            m0 (np.ndarray): 1D array of prior means for component means.\n            tau0 (np.ndarray): 1D array of prior precisions for component means.\n            tol (float): Convergence tolerance for responsibilities.\n            max_iter (int): Maximum number of iterations.\n\n        Returns:\n            np.ndarray: A (N, K) array of final responsibilities.\n        \"\"\"\n        N = x.shape[0]\n        K = pi.shape[0]\n        tau = 1.0 / sigma_sq\n\n        # Initialize responsibilities uniformly\n        r_nk = np.full((N, K), 1.0 / K)\n\n        for i in range(max_iter):\n            r_nk_old = r_nk.copy()\n\n            # M-step: Update q(mu_k)\n            # Sum of responsibilities for each component\n            N_k = np.sum(r_nk, axis=0) # Shape (K,)\n            # Update variational precision for mu_k\n            tau_k = tau0 + tau * N_k # Shape (K,)\n            # Update variational mean for mu_k\n            # r_nk.T is (K,N), x is (N,). (r_nk.T @ x) is sum(r_nk * x) for each k\n            sum_r_x = r_nk.T @ x # Shape (K,)\n            m_k = (tau0 * m0 + tau * sum_r_x) / tau_k # Shape (K,)\n\n            # E-step: Update q(z_n), i.e., the responsibilities r_nk\n            E_mu_k_sq = 1.0 / tau_k + m_k**2 # Shape (K,)\n            \n            # Use broadcasting for efficiency. x[:, np.newaxis] is (N,1)\n            # m_k and E_mu_k_sq are (K,) which broadcasts to (N,K)\n            log_rho_nk = np.log(pi) + tau * x[:, np.newaxis] * m_k - (tau / 2) * E_mu_k_sq\n            \n            # Log-sum-exp trick for numerical stability\n            log_rho_nk_max = np.max(log_rho_nk, axis=1, keepdims=True)\n            log_rho_nk_stable = log_rho_nk - log_rho_nk_max\n            rho_nk = np.exp(log_rho_nk_stable)\n            \n            # Normalize to get responsibilities\n            r_nk = rho_nk / np.sum(rho_nk, axis=1, keepdims=True)\n\n            # Check for convergence\n            diff = np.sum(np.abs(r_nk - r_nk_old))\n            if diff < tol:\n                break\n        \n        return r_nk\n\n    T = 10**-8\n    results = []\n\n    # Test 1: Happy path, label invariance under full permutation\n    x1 = np.array([-2.2, -1.9, -1.7, 1.5, 1.8, 2.2])\n    sigma_sq1 = 0.25\n    pi_A = np.array([0.5, 0.5])\n    m0_A = np.array([-2.0, 2.0])\n    tau0_1 = np.array([1.0, 1.0])\n    R_A = run_cavi(x1, sigma_sq1, pi_A, m0_A, tau0_1, T)\n\n    pi_B = np.array([0.5, 0.5]) # Same pi\n    m0_B = np.array([2.0, -2.0]) # Swapped means\n    tau0_B = np.array([1.0, 1.0])\n    R_B = run_cavi(x1, sigma_sq1, pi_B, m0_B, tau0_B, T)\n\n    # Check for equality up to column permutation (label switching)\n    is_invariant = np.allclose(R_A, R_B, atol=T) or np.allclose(R_A, R_B[:, ::-1], atol=T)\n    results.append(is_invariant)\n\n    # Test 2: Edge case, partial permutation that breaks invariance\n    x2 = np.array([-2.2, -1.9, -1.7, 1.5, 1.8, 2.2])\n    sigma_sq2 = 0.25\n    pi_C = np.array([0.7, 0.3])\n    m0_C = np.array([-2.0, 2.0])\n    tau0_2 = np.array([1.0, 1.0])\n    R_C = run_cavi(x2, sigma_sq2, pi_C, m0_C, tau0_2, T)\n    \n    pi_D = np.array([0.7, 0.3]) # Unswapped pi\n    m0_D = np.array([2.0, -2.0]) # Swapped means\n    tau0_D = np.array([1.0, 1.0])\n    R_D = run_cavi(x2, sigma_sq2, pi_D, m0_D, tau0_D, T)\n\n    # Check if results are different even after accounting for label switching\n    is_non_invariant = not (np.allclose(R_C, R_D, atol=T) or np.allclose(R_C, R_D[:, ::-1], atol=T))\n    results.append(is_non_invariant)\n\n    # Test 3: Boundary condition, indistinguishable components\n    x3 = np.array([-1.0, 1.0])\n    sigma_sq3 = 1.0\n    pi_3 = np.array([0.5, 0.5])\n    m0_3 = np.array([0.0, 0.0])\n    tau0_3 = np.array([100.0, 100.0])\n    R_3 = run_cavi(x3, sigma_sq3, pi_3, m0_3, tau0_3, T)\n    max_dev = np.max(np.abs(R_3 - 0.5))\n    results.append(max_dev)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3191998"}, {"introduction": "While mixture models often have conjugate priors that lead to clean, analytical updates, many real-world models are not so straightforward. This next exercise tackles the common challenge of non-conjugacy in the context of Bayesian logistic regression, where the sigmoid likelihood does not have a conjugate Gaussian prior. This problem [@problem_id:691486] introduces a powerful technique: using a local variational bound to approximate the intractable term, thereby making the coordinate ascent optimization feasible again.", "problem": "Consider a Bayesian logistic regression model for a binary classification task. We are given a single data point $(x, t)$, where $x=(x_1, x_2)^T \\in \\mathbb{R}^2$ is a feature vector and $t \\in \\{-1, 1\\}$ is the corresponding class label. The likelihood of the label is given by $p(t|w,x) = \\sigma(t w^T x)$, where $w=(w_1, w_2)^T$ is the weight vector and $\\sigma(z) = (1+e^{-z})^{-1}$ is the logistic sigmoid function.\n\nA zero-mean isotropic Gaussian prior with precision $\\alpha > 0$ is placed on the weights:\n$$p(w) = \\mathcal{N}(w | 0, \\alpha^{-1}I)$$\nwhere $I$ is the $2 \\times 2$ identity matrix.\n\nThe true posterior distribution $p(w|x,t)$ is intractable. We approximate it using variational inference with a factorized Gaussian posterior approximation (mean-field):\n$$q(w) = q_1(w_1)q_2(w_2) = \\mathcal{N}(w_1|\\mu_1, v_1)\\mathcal{N}(w_2|\\mu_2, v_2)$$\n\nTo handle the non-conjugacy of the likelihood and prior, the log-likelihood term in the Evidence Lower Bound (ELBO) is approximated. The term $\\log \\sigma(z)$ is lower-bounded by a quadratic function involving a variational parameter $\\xi \\in \\mathbb{R}$:\n$$ \\log \\sigma(z) \\ge \\log \\sigma(\\xi) + \\frac{1}{2}(z - \\xi) - \\lambda(\\xi)(z^2 - \\xi^2) $$\nwhere the function $\\lambda(\\xi)$ is defined as:\n$$ \\lambda(\\xi) = \\frac{\\tanh(\\xi/2)}{4\\xi} $$\n\nThe variational parameters are updated iteratively. Consider a single update step for the distribution $q_1(w_1)$. Assuming the parameters for the other factor ($\\mu_2$) and the local bound ($\\xi$) are fixed, find the expression for the optimal mean $\\mu_1$ that maximizes the ELBO. Your answer should be expressed in terms of $x_1, x_2, t, \\alpha, \\xi$, and $\\mu_2$.", "solution": "1. Bound the log‐likelihood term:\n$$\\log\\sigma(tw^T x)\\ge\\log\\sigma(\\xi)+\\frac12\\,(tw^T x - \\xi)-\\lambda(\\xi)\\bigl[(tw^T x)^2-\\xi^2\\bigr].$$\nSince $t \\in \\{-1,1\\}$, we have $(tw^T x)^2 = (w^T x)^2$. The terms depending on $w_1$ inside the lower bound are: $\\frac{1}{2}tw_1x_1$ and $-\\lambda(\\xi)(w_1x_1+w_2x_2)^2$.\n\n2. Log‐prior:\n$$\\log p(w)=-\\frac12\\alpha(w_1^2+w_2^2)+\\text{const}.$$\n\n3. Form the variational exponent for $w_1$ by taking the expectation w.r.t. $q_2(w_2)$:\n$$\n\\mathbb{E}_{q_2}[\\log p(w)+\\text{bound on }\\log p(t|w,x)]\n=\\;-\\tfrac12\\,\\alpha\\,w_1^2\n+\\tfrac12\\,t\\,x_1\\,w_1\n-\\lambda(\\xi)\\,\\mathbb{E}_{q_2}[(w_1x_1+w_2x_2)^2]\n+\\text{const}.\n$$\n\n4. Compute $\\mathbb{E}_{q_2}[(w_1x_1+w_2x_2)^2]$:\n$$\n\\mathbb{E}_{q_2}[w_1^2x_1^2 + 2w_1x_1w_2x_2 + w_2^2x_2^2]\n= w_1^2x_1^2+2w_1x_1\\mathbb{E}_{q_2}[w_2]x_2+\\mathbb{E}_{q_2}[w_2^2]x_2^2\n= w_1^2x_1^2+2w_1x_1\\mu_2x_2+\\text{terms not involving } w_1.\n$$\n\n5. Collect quadratic and linear terms in $w_1$:\n- Quadratic: From the prior, $-\\frac12\\alpha w_1^2$. From the bound, $-\\lambda(\\xi)w_1^2x_1^2$. Total: $-\\frac12(\\alpha+2\\lambda(\\xi)x_1^2)w_1^2$.\n- Linear: From the bound's linear term, $\\frac12t x_1 w_1$. From the bound's quadratic term, $-2\\lambda(\\xi)x_1x_2\\mu_2 w_1$. Total: $(\\frac12t x_1-2\\lambda(\\xi)x_1x_2\\mu_2)w_1$.\n\n6. Match coefficients to a Gaussian log-density of the form $-\\frac{1}{2}(\\text{precision})w_1^2 + (\\text{natural mean})w_1$:\n$$\n\\text{precision}=\\alpha+2\\lambda(\\xi)x_1^2,\n$$\n$$\n\\text{natural mean}=\\tfrac12t x_1-2\\lambda(\\xi)x_1x_2\\mu_2\n$$\nThe mean $\\mu_1$ is the natural mean divided by the precision:\n$$\n\\mu_1=\\frac{\\tfrac12t x_1-2\\lambda(\\xi)x_1x_2\\mu_2}{\\alpha+2\\lambda(\\xi)x_1^2}.\n$$", "answer": "$$\\boxed{\\frac{\\tfrac12\\,t\\,x_1 \\;-\\;2\\,\\lambda(\\xi)\\,x_1\\,x_2\\,\\mu_2}{\\alpha+2\\,\\lambda(\\xi)\\,x_1^2}}$$", "id": "691486"}, {"introduction": "The batch-based CAVI algorithm is powerful but does not scale to the massive datasets common in modern machine learning. To make that leap, we turn to Stochastic Variational Inference (SVI), which uses noisy gradients from small minibatches of data. This coding-based exercise [@problem_id:3192036] dives into the heart of SVI by asking you to derive and compare two key methods for estimating these gradients: the score-function estimator and the reparameterization trick, empirically demonstrating why the latter offers a lower-variance path to scalable inference.", "problem": "Consider Bayesian inference for a positive scalar parameter $\\theta \\in (0,\\infty)$ with a Poisson likelihood and a Gamma prior. Let the observed data be counts $\\{x_i\\}_{i=1}^n$ with $x_i \\in \\{0,1,2,\\dots\\}$, modeled by $x_i \\mid \\theta \\sim \\text{Poisson}(\\theta)$, and take a Gamma prior $\\theta \\sim \\text{Gamma}(a,b)$ with shape $a>0$ and rate $b>0$. We will approximate the posterior with a Log-Normal variational family $q(\\theta \\mid \\mu,\\sigma)$ defined by $\\log \\theta \\sim \\mathcal{N}(\\mu,\\sigma^2)$ where $\\mu \\in \\mathbb{R}$ and $\\sigma > 0$.\n\nStarting from first principles, use the Evidence Lower Bound (ELBO) definition for variational inference and standard probability definitions to derive reparameterization gradients for the variational parameters $\\mu$ and $\\sigma$. Use the chain rule and a valid sampling-based parameterization for $\\theta$ expressed in terms of a standard Normal random variable. Separately, derive the score-function gradients using the log-derivative trick. You should express the ELBO in a form whose gradient decomposes into the contribution from the expectation of the model terms and the entropy of the variational distribution. Do not assume any conjugacy or closed-form posterior beyond these basic model definitions.\n\nImplement both gradient estimators (reparameterization and score-function) and, for each, compute a Monte Carlo estimate of the gradient of the ELBO with respect to $\\mu$ and $\\sigma$ from a small number of samples. To assess bias reduction, compare each small-sample gradient estimate against a high-accuracy reference gradient computed with a large number of samples using the reparameterization estimator. Quantify the discrepancy as the absolute difference between the small-sample estimate and the reference, averaged over multiple independent replicates. Report whether the reparameterization estimator yields a smaller average absolute discrepancy than the score-function estimator for both $\\mu$ and $\\sigma$.\n\nYou must handle the following test suite. Each test case provides variational parameters $(\\mu,\\sigma)$, prior parameters $(a,b)$, and a data set $\\{x_i\\}_{i=1}^n$:\n\n- Test case $1$: $\\mu = 0.0$, $\\sigma = 0.5$, $a = 2.0$, $b = 1.0$, data $\\{1,0,2,1,3,2,1,0,1,2\\}$ (here $n=10$).\n- Test case $2$: $\\mu = 1.0$, $\\sigma = 0.1$, $a = 3.0$, $b = 2.0$, data $\\{5,4,6,3,7,5,6,4\\}$ (here $n=8$).\n- Test case $3$: $\\mu = -0.5$, $\\sigma = 1.5$, $a = 1.5$, $b = 0.5$, data $\\{0,1,0,2,1\\}$ (here $n=5$).\n- Test case $4$: $\\mu = 0.2$, $\\sigma = 0.2$, $a = 5.0$, $b = 4.0$, data $\\{2,0,1,0,3,1\\}$ (here $n=6$).\n\nFor each test case:\n- Compute a high-accuracy reference gradient using the reparameterization estimator with $S_{\\text{ref}} = 200000$ samples.\n- Compute small-sample gradient estimates using $S_{\\text{small}} = 16$ samples, and average the absolute discrepancy over $R = 100$ independent replicates for both estimators.\n- For each test case, output a boolean indicating whether the reparameterization estimator’s average absolute discrepancy is strictly smaller than the score-function estimator’s average absolute discrepancy simultaneously for both $\\mu$ and $\\sigma$.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result_1,result_2,result_3,result_4]$), where each $result_k$ is a boolean for test case $k$. No physical units or angles are involved in this problem; all quantities are dimensionless. Your implementation must be self-contained and deterministic by fixing all pseudorandom seeds internally.", "solution": "The problem compares the performance of two different gradient estimators for the Evidence Lower Bound (ELBO) in the context of variational inference. The model consists of a Poisson likelihood for observed counts $\\{x_i\\}$ with a rate parameter $\\theta$, a Gamma prior on $\\theta$, and a Log-Normal variational approximation for the posterior of $\\theta$.\n\n**Mathematical Derivations**\n\nLet $\\phi = (\\mu, \\sigma)$ be the variational parameters. The ELBO, $\\mathcal{L}(\\phi)$, is given by:\n$$ \\mathcal{L}(\\phi) = \\mathbb{E}_{q(\\theta;\\phi)}[\\log p(\\mathbf{x}, \\theta) - \\log q(\\theta;\\phi)] $$\nThe joint log-probability $\\log p(\\mathbf{x}, \\theta)$ is the sum of the log-likelihood and the log-prior.\n$$ \\log p(\\mathbf{x} \\mid \\theta) = \\sum_{i=1}^n \\log\\left(\\frac{\\theta^{x_i}e^{-\\theta}}{x_i!}\\right) = \\left(\\sum_{i=1}^n x_i\\right)\\log\\theta - n\\theta - \\sum_{i=1}^n \\log(x_i!) $$\n$$ \\log p(\\theta) = \\log\\left(\\frac{b^a}{\\Gamma(a)}\\theta^{a-1}e^{-b\\theta}\\right) = (a-1)\\log\\theta - b\\theta + a\\log b - \\log\\Gamma(a) $$\nLet $S_x = \\sum_{i=1}^n x_i$. Combining these gives:\n$$ \\log p(\\mathbf{x}, \\theta) = (S_x + a - 1)\\log\\theta - (n+b)\\theta + \\text{const} $$\nThe variational distribution $q(\\theta;\\phi)$ is Log-Normal, so its log-PDF is:\n$$ \\log q(\\theta;\\phi) = -\\log\\theta - \\log\\sigma - \\frac{1}{2}\\log(2\\pi) - \\frac{(\\log\\theta - \\mu)^2}{2\\sigma^2} $$\nThe problem suggests expressing the ELBO gradient as the sum of two components: the gradient of the expected log-joint and the gradient of the entropy of $q$.\n$$ \\mathcal{L}(\\phi) = \\mathbb{E}_{q(\\theta;\\phi)}[\\log p(\\mathbf{x}, \\theta)] + H(q) $$\nwhere $H(q) = -\\mathbb{E}_{q(\\theta;\\phi)}[\\log q(\\theta;\\phi)]$ is the entropy. For a Log-Normal distribution, $H(q) = \\mu + \\log\\sigma + \\frac{1}{2}\\log(2\\pi e)$. The gradients of the entropy term are analytical:\n$$ \\nabla_\\mu H(q) = 1, \\quad \\nabla_\\sigma H(q) = \\frac{1}{\\sigma} $$\n\n**1. Reparameterization Gradient Estimator**\nThe reparameterization trick allows us to write $\\theta$ as a deterministic function of $\\phi$ and an auxiliary random variable $\\epsilon$ with a fixed distribution. Since $\\log\\theta \\sim \\mathcal{N}(\\mu, \\sigma^2)$, we can set $\\log\\theta = \\mu + \\sigma\\epsilon$ with $\\epsilon \\sim \\mathcal{N}(0,1)$. Thus, $\\theta(\\phi, \\epsilon) = \\exp(\\mu + \\sigma\\epsilon)$.\nThe gradient of the expected log-joint can be computed by moving the gradient inside the expectation:\n$$ \\nabla_\\phi \\mathbb{E}_{q(\\theta;\\phi)}[\\log p(\\mathbf{x}, \\theta)] = \\nabla_\\phi \\mathbb{E}_{\\epsilon\\sim\\mathcal{N}(0,1)}[\\log p(\\mathbf{x}, \\theta(\\phi,\\epsilon))] = \\mathbb{E}_{\\epsilon\\sim\\mathcal{N}(0,1)}[\\nabla_\\phi \\log p(\\mathbf{x}, \\theta(\\phi,\\epsilon))] $$\nUsing the chain rule, $\\nabla_\\phi \\log p = \\frac{d \\log p}{d\\theta} \\nabla_\\phi \\theta$.\n$$ \\frac{d\\log p}{d\\theta} = \\frac{S_x+a-1}{\\theta} - (n+b) $$\nThe gradients of $\\theta$ with respect to $\\phi$ are:\n$$ \\frac{\\partial\\theta}{\\partial\\mu} = \\exp(\\mu+\\sigma\\epsilon) \\cdot 1 = \\theta, \\quad \\frac{\\partial\\theta}{\\partial\\sigma} = \\exp(\\mu+\\sigma\\epsilon) \\cdot \\epsilon = \\theta\\epsilon $$\nSo, the gradients of the log-joint term are:\n$$ \\frac{\\partial \\log p}{\\partial\\mu} = \\left(\\frac{S_x+a-1}{\\theta} - (n+b)\\right)\\theta = S_x+a-1 - (n+b)\\theta $$\n$$ \\frac{\\partial \\log p}{\\partial\\sigma} = \\left(\\frac{S_x+a-1}{\\theta} - (n+b)\\right)\\theta\\epsilon = (S_x+a-1 - (n+b)\\theta)\\epsilon $$\nCombining with the gradients of the entropy, the full ELBO gradients are:\n$$ \\nabla_\\mu \\mathcal{L}(\\phi) = \\mathbb{E}_{\\epsilon}[S_x+a-1 - (n+b)\\theta] + 1 $$\n$$ \\nabla_\\sigma \\mathcal{L}(\\phi) = \\mathbb{E}_{\\epsilon}[(S_x+a-1 - (n+b)\\theta)\\epsilon] + \\frac{1}{\\sigma} $$\nA Monte Carlo estimator with $S$ samples $\\{\\epsilon^{(s)}\\}_{s=1}^S$ where $\\theta^{(s)} = \\exp(\\mu+\\sigma\\epsilon^{(s)})$ is:\n$$ \\hat{g}_\\mu^{\\text{reparam}} = \\frac{1}{S}\\sum_{s=1}^S [S_x+a-1 - (n+b)\\theta^{(s)}] + 1 $$\n$$ \\hat{g}_\\sigma^{\\text{reparam}} = \\frac{1}{S}\\sum_{s=1}^S [(S_x+a-1 - (n+b)\\theta^{(s)})\\epsilon^{(s)}] + \\frac{1}{\\sigma} $$\n\n**2. Score-Function Gradient Estimator (REINFORCE)**\nThis estimator is based on the log-derivative trick, $\\nabla_\\phi q(\\theta;\\phi) = q(\\theta;\\phi) \\nabla_\\phi \\log q(\\theta;\\phi)$. It applies to the full ELBO expression $\\mathcal{L}(\\phi) = \\mathbb{E}_{q}[\\log p(\\mathbf{x}, \\theta) - \\log q(\\theta;\\phi)]$.\n$$ \\nabla_\\phi \\mathcal{L}(\\phi) = \\mathbb{E}_{q(\\theta;\\phi)}[(\\log p(\\mathbf{x}, \\theta) - \\log q(\\theta;\\phi)) \\nabla_\\phi \\log q(\\theta;\\phi)] $$\nWe need the score function, $\\nabla_\\phi \\log q(\\theta;\\phi)$.\n$$ \\nabla_\\mu \\log q = \\frac{\\partial}{\\partial\\mu} \\left(-\\frac{(\\log\\theta - \\mu)^2}{2\\sigma^2}\\right) = \\frac{\\log\\theta - \\mu}{\\sigma^2} $$\n$$ \\nabla_\\sigma \\log q = \\frac{\\partial}{\\partial\\sigma} \\left(-\\log\\sigma - \\frac{(\\log\\theta - \\mu)^2}{2\\sigma^2}\\right) = -\\frac{1}{\\sigma} + \\frac{(\\log\\theta - \\mu)^2}{\\sigma^3} = \\frac{(\\log\\theta - \\mu)^2 - \\sigma^2}{\\sigma^3} $$\nThe Monte Carlo estimator with $S$ samples $\\{\\theta^{(s)}\\}_{s=1}^S \\sim q(\\theta;\\phi)$ is:\n$$ \\hat{g}_\\mu^{\\text{score}} = \\frac{1}{S}\\sum_{s=1}^S \\left[ (\\log p(\\mathbf{x}, \\theta^{(s)}) - \\log q(\\theta^{(s)};\\phi)) \\left(\\frac{\\log\\theta^{(s)} - \\mu}{\\sigma^2}\\right) \\right] $$\n$$ \\hat{g}_\\sigma^{\\text{score}} = \\frac{1}{S}\\sum_{s=1}^S \\left[ (\\log p(\\mathbf{x}, \\theta^{(s)}) - \\log q(\\theta^{(s)};\\phi)) \\left(\\frac{(\\log\\theta^{(s)} - \\mu)^2 - \\sigma^2}{\\sigma^3}\\right) \\right] $$\nTo implement this, we sample $\\theta^{(s)}$ via the same reparameterization $\\theta^{(s)} = \\exp(\\mu+\\sigma\\epsilon^{(s)})$ with $\\epsilon^{(s)} \\sim \\mathcal{N}(0,1)$, and compute the full terms $\\log p(\\mathbf{x}, \\theta^{(s)})$ and $\\log q(\\theta^{(s)};\\phi)$ for each sample.\n\nThe implementation will follow these derived formulas to compute the gradients and compare their discrepancies against a high-accuracy reference. The boolean result for each test case indicates whether the reparameterization estimator is superior in terms of average absolute discrepancy for both $\\mu$ and $\\sigma$.", "answer": "```python\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef solve():\n    \"\"\"\n    Derives, implements, and compares reparameterization and score-function\n    gradient estimators for a Bayesian model with a Poisson likelihood.\n    \"\"\"\n    \n    # Fix the global random seed for deterministic and reproducible execution.\n    np.random.seed(42)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'mu': 0.0, 'sigma': 0.5, 'a': 2.0, 'b': 1.0, 'data': [1, 0, 2, 1, 3, 2, 1, 0, 1, 2]},\n        {'mu': 1.0, 'sigma': 0.1, 'a': 3.0, 'b': 2.0, 'data': [5, 4, 6, 3, 7, 5, 6, 4]},\n        {'mu': -0.5, 'sigma': 1.5, 'a': 1.5, 'b': 0.5, 'data': [0, 1, 0, 2, 1]},\n        {'mu': 0.2, 'sigma': 0.2, 'a': 5.0, 'b': 4.0, 'data': [2, 0, 1, 0, 3, 1]},\n    ]\n\n    results = []\n    for case in test_cases:\n        mu, sigma, a, b, data = case['mu'], case['sigma'], case['a'], case['b'], case['data']\n        \n        # Pre-compute data-dependent constants\n        n = len(data)\n        Sx = np.sum(data)\n\n        # Monte Carlo settings\n        S_ref = 200000\n        S_small = 16\n        R = 100\n\n        # --- High-Accuracy Reference Gradient Calculation ---\n        # Generate a large number of samples from the base distribution.\n        eps_ref = np.random.standard_normal(S_ref)\n        # Transform samples to the Log-Normal variational distribution for theta.\n        theta_ref = np.exp(mu + sigma * eps_ref)\n        \n        # Compute the reference gradient using the reparameterization estimator,\n        # known for its low variance and unbiasedness.\n        grad_mu_ref = np.mean(Sx + a - 1 - (n + b) * theta_ref) + 1.0\n        grad_sigma_ref = np.mean((Sx + a - 1 - (n + b) * theta_ref) * eps_ref) + 1.0 / sigma\n\n        # --- Discrepancy Evaluation over Replicates ---\n        reparam_disc_mu = np.zeros(R)\n        reparam_disc_sigma = np.zeros(R)\n        score_disc_mu = np.zeros(R)\n        score_disc_sigma = np.zeros(R)\n\n        # Pre-calculate constants for the score function estimator's f(theta) term\n        sum_log_fact_x = np.sum(gammaln(np.array(data) + 1.0))\n        log_prior_const = a * np.log(b) - gammaln(a)\n        log_joint_const = log_prior_const - sum_log_fact_x\n\n        for r in range(R):\n            # Generate a small sample set, used by both estimators for a fair comparison\n            eps_small = np.random.standard_normal(S_small)\n            log_theta_small = mu + sigma * eps_small\n            theta_small = np.exp(log_theta_small)\n\n            # --- 1. Reparameterization Estimator (Small Sample) ---\n            grad_mu_reparam = np.mean(Sx + a - 1 - (n + b) * theta_small) + 1.0\n            grad_sigma_reparam = np.mean((Sx + a - 1 - (n + b) * theta_small) * eps_small) + 1.0 / sigma\n            \n            # Store absolute discrepancy from the reference\n            reparam_disc_mu[r] = np.abs(grad_mu_reparam - grad_mu_ref)\n            reparam_disc_sigma[r] = np.abs(grad_sigma_reparam - grad_sigma_ref)\n\n            # --- 2. Score-Function Estimator (Small Sample) ---\n            # Calculate f(theta) = log p(x, theta) - log q(theta)\n            # log p(x, theta)\n            log_joint = (Sx + a - 1) * log_theta_small - (n + b) * theta_small + log_joint_const\n            # log q(theta)\n            log_q = -log_theta_small - np.log(sigma) - 0.5 * np.log(2 * np.pi) - (log_theta_small - mu)**2 / (2 * sigma**2)\n            f_theta = log_joint - log_q\n            \n            # Calculate grad log q(theta)\n            grad_log_q_mu = (log_theta_small - mu) / (sigma**2)\n            grad_log_q_sigma = ((log_theta_small - mu)**2 - sigma**2) / (sigma**3)\n            \n            # Calculate score function gradients\n            grad_mu_score = np.mean(f_theta * grad_log_q_mu)\n            grad_sigma_score = np.mean(f_theta * grad_log_q_sigma)\n            \n            # Store absolute discrepancy from the reference\n            score_disc_mu[r] = np.abs(grad_mu_score - grad_mu_ref)\n            score_disc_sigma[r] = np.abs(grad_sigma_score - grad_sigma_ref)\n    \n        # Compute average discrepancies over all replicates\n        avg_reparam_disc_mu = np.mean(reparam_disc_mu)\n        avg_reparam_disc_sigma = np.mean(reparam_disc_sigma)\n        avg_score_disc_mu = np.mean(score_disc_mu)\n        avg_score_disc_sigma = np.mean(score_disc_sigma)\n    \n        # --- Final Comparison ---\n        # Check if reparameterization estimator is strictly better for both parameters\n        is_reparam_better = (avg_reparam_disc_mu < avg_score_disc_mu) and \\\n                            (avg_reparam_disc_sigma < avg_score_disc_sigma)\n        \n        results.append(is_reparam_better)\n\n    # Print the final results in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3192036"}]}