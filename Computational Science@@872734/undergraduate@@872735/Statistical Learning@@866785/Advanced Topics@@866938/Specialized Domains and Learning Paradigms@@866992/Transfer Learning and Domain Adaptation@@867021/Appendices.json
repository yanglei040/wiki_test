{"hands_on_practices": [{"introduction": "This practice introduces a core concept in domain adaptation theory: measuring the \"distance\" between the source and target domains. You will implement a method to estimate this distance, known as the Proxy A-distance, by training a classifier to distinguish between domains. This exercise demonstrates how this measure of domain shift can be practically used to guide the learning algorithm, specifically by adjusting the regularization strength to prevent overfitting when the domains are far apart. [@problem_id:3189023]", "problem": "You are given the task of designing an adaptive regularization scheme for a linear classifier in a transfer learning setting. The goal is to select the strength of Euclidean squared norm regularization (L2) based on the estimated divergence between a source domain and a target domain. Your program must implement the following principle: when the domains are very similar, use weak regularization; when the domains are very different, use strong regularization.\n\nStart from the following core definitions and facts:\n- Empirical Risk Minimization (ERM) with L2 regularization uses a penalty proportional to the squared Euclidean norm of the parameter vector. For a parameter vector $w$ and empirical risk $R(w)$, the regularized objective is $R(w) + \\lambda \\lVert w \\rVert_2^2$, where $\\lambda \\ge 0$ is the regularization strength.\n- In domain adaptation, the generalization performance on the target domain can be bounded in terms of the source risk, a divergence between domains, and an irreducible joint error term. A representative bound states that for any hypothesis $h$ in a hypothesis class $\\mathcal{H}$, the target risk is controlled by the source risk plus a divergence term such as the symmetric difference hypothesis divergence $d_{\\mathcal{H}\\Delta\\mathcal{H}}$, plus a constant that depends on the best joint hypothesis. This motivates increasing inductive bias (via larger $\\lambda$) when the domain gap is large.\n- A practical, data-driven surrogate for $d_{\\mathcal{H}\\Delta\\mathcal{H}}$ is the so-called Proxy A-distance (PAD), computed from the error of a classifier trained to distinguish source versus target examples using hypotheses from $\\mathcal{H}$. Let $\\hat{\\varepsilon}$ be the empirical error rate of such a domain discriminator on held-out data. Then a standard estimator of the divergence is $\\hat{d} = 2 \\left(1 - 2 \\min(\\hat{\\varepsilon}, 1 - \\hat{\\varepsilon})\\right)$, which lies in $[0, 2]$.\n\nYour task is to derive, justify, and implement an adaptive mapping from the estimated divergence $\\hat{d}$ to a regularization strength $\\hat{\\lambda}$, subject to the following constraints:\n- $\\hat{\\lambda}$ must be a monotonically non-decreasing function of $\\hat{d}$.\n- When $\\hat{d} = 0$, set $\\hat{\\lambda} = \\lambda_{\\mathrm{base}}$ with $\\lambda_{\\mathrm{base}} = 0.1$.\n- When $\\hat{d} = 2$, set $\\hat{\\lambda} = \\lambda_{\\mathrm{max}}$ with $\\lambda_{\\mathrm{max}} = 10.0$.\n- Among all functions meeting the above constraints, choose the simplest nontrivial choice that is consistent with using the divergence as a linear signal of target risk amplification.\n\nProcedure to implement for each test case:\n1. Data generation. You will synthesize feature-only data for the source and target domains in $\\mathbb{R}^2$. For a given random seed $s$, sample size $n_s$ for the source domain, sample size $n_t$ for the target domain, source mean vector $\\mu_s \\in \\mathbb{R}^2$, and target mean vector $\\mu_t \\in \\mathbb{R}^2$, draw samples independently as follows:\n   - Source features $X_s \\sim \\mathcal{N}(\\mu_s, I_2)$, a Gaussian distribution with mean $\\mu_s$ and identity covariance $I_2$.\n   - Target features $X_t \\sim \\mathcal{N}(\\mu_t, I_2)$.\n   Use the given seed $s$ to initialize a pseudorandom number generator so that your results are deterministic.\n2. Domain discriminator and divergence estimation.\n   - Assign domain labels $0$ to all source samples and $1$ to all target samples.\n   - Randomly shuffle each domain separately using the same pseudorandom number generator as above, then split each domain in half (first $\\lfloor n_s/2 \\rfloor$ for training and the rest for testing in the source; similarly for the target).\n   - Train a linear logistic regression classifier to predict the domain label from the two-dimensional features using the training halves. The logistic regression shall minimize the average logistic loss without any explicit regularization term. Use an intercept (bias) term. The logistic loss for a single example with features $x \\in \\mathbb{R}^2$, label $y \\in \\{0,1\\}$, and parameter vector $w \\in \\mathbb{R}^3$ (including bias) is $\\ell(w; x, y) = \\log\\left(1 + e^{z}\\right) - y z$, where $z = w^\\top \\tilde{x}$ and $\\tilde{x}$ is $x$ augmented with a constant $1$ for the bias.\n   - Compute the empirical domain classification error $\\hat{\\varepsilon}$ on the held-out (testing) halves using the threshold $0.5$ on the logistic sigmoid. To ensure the divergence estimate is nonnegative, define $\\hat{\\varepsilon}_{\\min} = \\min(\\hat{\\varepsilon}, 1 - \\hat{\\varepsilon})$.\n   - Compute the estimated divergence $\\hat{d} = 2 \\left(1 - 2 \\hat{\\varepsilon}_{\\min}\\right)$, and ensure $\\hat{d} \\in [0, 2]$ by construction.\n3. Adaptive regularization mapping.\n   - Derive the simplest monotone function of $\\hat{d}$ that satisfies $\\hat{\\lambda} = \\lambda_{\\mathrm{base}}$ when $\\hat{d} = 0$ and $\\hat{\\lambda} = \\lambda_{\\mathrm{max}}$ when $\\hat{d} = 2$, and apply it to compute $\\hat{\\lambda}$.\n   - Output the value $\\hat{\\lambda}$ for each test case as a floating-point number rounded to $6$ decimal places.\n\nTest suite:\nImplement the pipeline above for the following parameter sets, in the given order. In all cases, the feature dimension is $2$ and the covariance is the identity matrix $I_2$.\n- Case $1$: seed $= 7$, $n_s = 200$, $n_t = 200$, $\\mu_s = (0.0, 0.0)$, $\\mu_t = (0.0, 0.0)$.\n- Case $2$: seed $= 13$, $n_s = 200$, $n_t = 200$, $\\mu_s = (0.0, 0.0)$, $\\mu_t = (1.0, 0.5)$.\n- Case $3$: seed $= 21$, $n_s = 200$, $n_t = 200$, $\\mu_s = (0.0, 0.0)$, $\\mu_t = (3.0, 3.0)$.\n- Case $4$: seed $= 5$, $n_s = 40$, $n_t = 40$, $\\mu_s = (0.0, 0.0)$, $\\mu_t = (0.8, -0.8)$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4]\"). Each result is the rounded value of $\\hat{\\lambda}$ for the corresponding case, in the order specified above. No units are required because $\\lambda$ is dimensionless. Angles are not involved. Percentages must not be used; all values are decimals.", "solution": "The problem requires the design and implementation of an adaptive regularization scheme for a linear classifier in a transfer learning context. The regularization strength, denoted by $\\hat{\\lambda}$, is to be determined by the estimated divergence, $\\hat{d}$, between a source and a target domain. The solution proceeds in two main stages: first, the derivation of the functional mapping from $\\hat{d}$ to $\\hat{\\lambda}$, and second, the detailed algorithmic procedure for estimating $\\hat{d}$ from data and subsequently computing $\\hat{\\lambda}$.\n\n### Part 1: Derivation of the Adaptive Regularization Function\n\nThe problem specifies three constraints for the function $\\hat{\\lambda}(\\hat{d})$ that maps the estimated Proxy A-distance $\\hat{d} \\in [0, 2]$ to a regularization strength $\\hat{\\lambda}$:\n1.  $\\hat{\\lambda}(\\hat{d})$ must be a monotonically non-decreasing function of $\\hat{d}$.\n2.  $\\hat{\\lambda}(0) = \\lambda_{\\mathrm{base}} = 0.1$.\n3.  $\\hat{\\lambda}(2) = \\lambda_{\\mathrm{max}} = 10.0$.\n\nFurthermore, we are instructed to select the \"simplest nontrivial choice that is consistent with using the divergence as a linear signal of target risk amplification.\" A linear function is the simplest non-constant function that can satisfy these constraints and aligns with the interpretation of $\\hat{d}$ as a linear signal.\n\nLet us define the linear function as:\n$$\n\\hat{\\lambda}(\\hat{d}) = m \\hat{d} + c\n$$\nwhere $m$ is the slope and $c$ is the intercept. For the function to be monotonically non-decreasing, we must have $m \\ge 0$.\n\nWe use the two boundary conditions to determine the parameters $m$ and $c$:\n-   Applying the first condition, $\\hat{\\lambda}(0) = 0.1$:\n    $$\n    m(0) + c = 0.1 \\implies c = 0.1\n    $$\n-   Applying the second condition, $\\hat{\\lambda}(2) = 10.0$, and substituting $c=0.1$:\n    $$\n    m(2) + 0.1 = 10.0 \\\\\n    2m = 9.9 \\\\\n    m = 4.95\n    $$\nSince $m = 4.95 > 0$, the monotonicity constraint is satisfied.\nThus, the derived adaptive regularization function is:\n$$\n\\hat{\\lambda}(\\hat{d}) = 4.95 \\hat{d} + 0.1\n$$\n\n### Part 2: Algorithmic Implementation\n\nThe core of the task is to implement a pipeline that computes $\\hat{\\lambda}$ for several test cases. This involves data generation, training a domain discriminator to estimate $\\hat{d}$, and finally applying the function derived above.\n\n1.  **Data Generation**: For each test case defined by a random seed $s$, source sample size $n_s$, target sample size $n_t$, source mean $\\mu_s \\in \\mathbb{R}^2$, and target mean $\\mu_t \\in \\mathbb{R}^2$, we generate data. A pseudorandom number generator is initialized with the seed $s$ for reproducibility.\n    -   Source domain data $X_s$ consists of $n_s$ samples drawn from a multivariate normal distribution $\\mathcal{N}(\\mu_s, I_2)$, where $I_2$ is the $2 \\times 2$ identity matrix.\n    -   Target domain data $X_t$ consists of $n_t$ samples drawn from $\\mathcal{N}(\\mu_t, I_2)$.\n\n2.  **Domain Discriminator and Divergence Estimation**:\n    -   **Labeling**: We create a new dataset for domain classification. Source samples from $X_s$ are assigned the label $y=0$, and target samples from $X_t$ are assigned the label $y=1$.\n    -   **Data Splitting**: Both the source and target datasets are shuffled independently. Each is then split into a training set and a testing set. The training set consists of the first $\\lfloor n/2 \\rfloor$ samples, and the test set consists of the remaining samples. The training data from both domains are combined to form the discriminator's training set, and similarly for the test data.\n    -   **Model Training**: A linear logistic regression classifier is trained on the combined training data to distinguish between the source ($y=0$) and target ($y=1$) domains. The model's parameter vector $w \\in \\mathbb{R}^3$ includes weights for the two features and a bias term. This is achieved by augmenting each feature vector $x \\in \\mathbb{R}^2$ with a constant $1$, forming $\\tilde{x} \\in \\mathbb{R}^3$. The objective is to find the parameter vector $w^*$ that minimizes the average logistic loss, $J(w)$, over the training set:\n        $$\n        w^* = \\arg\\min_w J(w) = \\arg\\min_w \\frac{1}{N_{\\text{train}}} \\sum_{i=1}^{N_{\\text{train}}} \\left[ \\log\\left(1 + e^{w^\\top \\tilde{x}_i}\\right) - y_i (w^\\top \\tilde{x}_i) \\right]\n        $$\n        This is a convex optimization problem, which can be solved using numerical methods like gradient-based optimizers (e.g., BFGS). The gradient of the average loss, required by such optimizers, is:\n        $$\n        \\nabla_w J(w) = \\frac{1}{N_{\\text{train}}} \\sum_{i=1}^{N_{\\text{train}}} (\\sigma(w^\\top \\tilde{x}_i) - y_i) \\tilde{x}_i\n        $$\n        where $\\sigma(z) = (1 + e^{-z})^{-1}$ is the logistic sigmoid function.\n    -   **Divergence Calculation**: The trained classifier, with parameters $w^*$, is used to predict domain labels on the held-out test set. The predicted probability for a sample with augmented features $\\tilde{x}$ is $p = \\sigma((w^*)^\\top \\tilde{x})$. The predicted label is $1$ if $p \\ge 0.5$ and $0$ otherwise.\n        The empirical error rate, $\\hat{\\varepsilon}$, is the fraction of misclassified samples in the test set. To account for label-swapping ambiguity, we compute $\\hat{\\varepsilon}_{\\min} = \\min(\\hat{\\varepsilon}, 1 - \\hat{\\varepsilon})$.\n        Finally, the Proxy A-distance is estimated as:\n        $$\n        \\hat{d} = 2(1 - 2\\hat{\\varepsilon}_{\\min})\n        $$\n\n3.  **Adaptive Regularization Calculation**:\n    -   The estimated divergence $\\hat{d}$ is plugged into the derived linear mapping to compute the final regularization strength:\n        $$\n        \\hat{\\lambda} = 4.95 \\hat{d} + 0.1\n        $$\n    -   The resulting value of $\\hat{\\lambda}$ is rounded to $6$ decimal places as required. This procedure is repeated for each test case specified in the problem statement.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    It orchestrates the calculation of the adaptive regularization parameter lambda_hat\n    for each specified scenario.\n    \"\"\"\n    \n    test_cases = [\n        # (seed, n_s, n_t, mu_s, mu_t)\n        (7, 200, 200, (0.0, 0.0), (0.0, 0.0)),\n        (13, 200, 200, (0.0, 0.0), (1.0, 0.5)),\n        (21, 200, 200, (0.0, 0.0), (3.0, 3.0)),\n        (5, 40, 40, (0.0, 0.0), (0.8, -0.8)),\n    ]\n\n    results = []\n    for case in test_cases:\n        seed, n_s, n_t, mu_s, mu_t = case\n        lambda_hat = compute_lambda_for_case(seed, n_s, n_t, np.array(mu_s), np.array(mu_t))\n        results.append(round(lambda_hat, 6))\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef compute_lambda_for_case(seed, n_s, n_t, mu_s, mu_t):\n    \"\"\"\n    Implements the full pipeline for a single test case:\n    1. Generates source and target data.\n    2. Trains a domain discriminator (logistic regression).\n    3. Computes the Proxy A-distance.\n    4. Maps the distance to a regularization parameter lambda_hat.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    # 1. Data Generation\n    cov = np.identity(2)\n    source_data = rng.multivariate_normal(mu_s, cov, size=n_s)\n    target_data = rng.multivariate_normal(mu_t, cov, size=n_t)\n\n    # 2. Domain discriminator and divergence estimation\n    \n    # Shuffle each domain's data separately\n    source_data_shuffled = source_data[rng.permutation(n_s)]\n    target_data_shuffled = target_data[rng.permutation(n_t)]\n    \n    # Split each domain into training and testing halves\n    s_split_idx = n_s // 2\n    source_train, source_test = source_data_shuffled[:s_split_idx], source_data_shuffled[s_split_idx:]\n    \n    t_split_idx = n_t // 2\n    target_train, target_test = target_data_shuffled[:t_split_idx], target_data_shuffled[t_split_idx:]\n\n    # Combine to form domain discriminator datasets\n    X_train = np.vstack((source_train, target_train))\n    y_train = np.hstack((np.zeros(len(source_train)), np.ones(len(target_train))))\n    \n    X_test = np.vstack((source_test, target_test))\n    y_test = np.hstack((np.zeros(len(source_test)), np.ones(len(target_test))))\n    \n    # Augment features with a bias term (column of ones)\n    X_train_aug = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n    X_test_aug = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n\n    # Define objective function (average logistic loss) and its gradient\n    def logistic_loss(w, X, y):\n        z = X @ w\n        # Numerically stable computation of log(1 + exp(z))\n        log_1_p_ez = np.zeros_like(z)\n        pos_mask = z > 0\n        log_1_p_ez[pos_mask] = z[pos_mask] + np.log1p(np.exp(-z[pos_mask]))\n        log_1_p_ez[~pos_mask] = np.log1p(np.exp(z[~pos_mask]))\n        \n        loss = np.sum(log_1_p_ez - y * z)\n        return loss / len(y)\n\n    def sigmoid(z):\n        # Numerically stable sigmoid function\n        res = np.zeros_like(z, dtype=float)\n        p_mask = z >= 0\n        n_mask = ~p_mask\n        res[p_mask] = 1.0 / (1.0 + np.exp(-z[p_mask]))\n        exp_z_n = np.exp(z[n_mask])\n        res[n_mask] = exp_z_n / (1.0 + exp_z_n)\n        return res\n\n    def logistic_gradient(w, X, y):\n        # Gradient of the average logistic loss\n        predictions = sigmoid(X @ w)\n        error = predictions - y\n        grad = (X.T @ error) / len(y)\n        return grad\n    \n    # Train the logistic regression model\n    initial_w = np.zeros(X_train_aug.shape[1])\n    opt_result = minimize(\n        logistic_loss, \n        initial_w, \n        args=(X_train_aug, y_train), \n        jac=logistic_gradient, \n        method='BFGS'\n    )\n    w_opt = opt_result.x\n\n    # Compute classification error on the test set\n    test_probs = sigmoid(X_test_aug @ w_opt)\n    test_preds = (test_probs >= 0.5).astype(int)\n    error_rate = np.mean(test_preds != y_test)\n    \n    # Compute Proxy A-distance\n    eps_min = min(error_rate, 1 - error_rate)\n    d_hat = 2 * (1 - 2 * eps_min)\n\n    # 3. Adaptive regularization mapping\n    lambda_hat = 4.95 * d_hat + 0.1\n    \n    return lambda_hat\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3189023"}, {"introduction": "Domain shifts can occur in many forms, and one of the simplest to address is \"prior shift,\" where the class frequencies change between domains but the class-conditional features do not. This exercise challenges you to derive and implement a principled correction for this specific type of shift. By adjusting the model's outputs (logits) based on the ratio of source and target class priors, you will see how a simple analytical fix can significantly improve model performance on the target domain. [@problem_id:3189015]", "problem": "Consider multiclass domain adaptation under prior probability shift between a source domain and a target domain. The fundamental base for this problem consists of the following definitions and facts. For a random input $x$ and a discrete label $y \\in \\{0,1,\\ldots,K-1\\}$, let $p_S(y)$ and $p_T(y)$ denote the source and target class-prior distributions, respectively. Let $p_S(y \\mid x)$ and $p_T(y \\mid x)$ denote the source and target posteriors. Assume the prior shift model in which the class-conditional densities are invariant across domains, i.e., $p_S(x \\mid y) = p_T(x \\mid y)$ for all $y$. By Bayes' rule and the prior shift assumption, the target posterior satisfies $p_T(y \\mid x) \\propto p_S(y \\mid x) \\cdot \\frac{p_T(y)}{p_S(y)}$. In multinomial logistic models, one represents a classifier by a score vector (logits) $h(x) \\in \\mathbb{R}^K$ whose softmax yields source probabilities via $p_S(y=k \\mid x) = \\frac{\\exp(h_k(x))}{\\sum_{j=0}^{K-1} \\exp(h_j(x))}$. The standard supervised training objective for multiclass classification is the cross-entropy loss, defined for a labeled example $(x,y)$ as $\\ell(h(x), y) = -\\log\\left( \\frac{\\exp(h_y(x))}{\\sum_{j=0}^{K-1} \\exp(h_j(x))} \\right)$, which is the negative log-likelihood under the softmax model. Maximum a posteriori (MAP) decision rules choose the class that maximizes the posterior probability.\n\nTask. Starting from the prior shift model and the definition of the softmax and cross-entropy loss above, derive the calibration-aware adaptation principle that modifies the logits by a class-dependent offset derived from the ratio of target to source class priors so that the cross-entropy reflects the target domain posterior. Then, implement this principle in a program that, for each test case below, computes:\n- The mean cross-entropy loss without adaptation (base loss) on the given labeled target samples.\n- The mean cross-entropy loss with calibration-aware adaptation (adapted loss) on the same samples.\n- The classification accuracy without adaptation (base accuracy), expressed as a decimal in $[0,1]$.\n- The classification accuracy with adaptation (adapted accuracy), expressed as a decimal in $[0,1]$.\n\nUse the following definitions for computation:\n- For any logits vector $z \\in \\mathbb{R}^K$, the softmax is $\\operatorname{softmax}(z)_k = \\frac{\\exp(z_k)}{\\sum_{j=0}^{K-1}\\exp(z_j)}$.\n- The cross-entropy for a sample with logits $z$ and label $y$ is $\\ell(z, y) = -\\log(\\operatorname{softmax}(z)_y)$.\n- The classification rule is $\\hat{y} = \\arg\\max_{k \\in \\{0,\\ldots,K-1\\}} z_k$.\n- The calibration-aware adaptation modifies logits by adding a constant offset vector in $\\mathbb{R}^K$ that depends only on the class index $k$ and the given domain priors, and then uses the same softmax and cross-entropy definitions.\n\nNumerical requirements:\n- All mean losses must be rounded to six decimal places.\n- All accuracies must be rounded to four decimal places and expressed as decimals (not percentages).\n- No physical units or angles are involved.\n\nTest suite. Each test case provides the number of classes $K$, source priors $\\pi_S \\in \\mathbb{R}^K$, target priors $\\pi_T \\in \\mathbb{R}^K$, a set of target-domain logits $\\{h^{(i)}\\}_{i=1}^n$, and corresponding target labels $\\{y^{(i)}\\}_{i=1}^n$.\n\n- Test case $1$ (no prior shift):\n  - $K = 3$.\n  - $\\pi_S = [\\,0.4,\\,0.35,\\,0.25\\,]$.\n  - $\\pi_T = [\\,0.4,\\,0.35,\\,0.25\\,]$.\n  - Logits rows:\n    - $[\\,2.0,\\,0.5,\\,-1.0\\,]$, $y=0$.\n    - $[\\,0.2,\\,1.5,\\,0.0\\,]$, $y=1$.\n    - $[\\,\\,-0.5,\\,-0.2,\\,1.0\\,]$, $y=2$.\n    - $[\\,1.0,\\,1.2,\\,0.8\\,]$, $y=1$.\n    - $[\\,0.9,\\,-1.0,\\,0.3\\,]$, $y=0$.\n\n- Test case $2$ (target prior heavily favors class $0$):\n  - $K = 3$.\n  - $\\pi_S = [\\,0.3333333333,\\,0.3333333333,\\,0.3333333333\\,]$.\n  - $\\pi_T = [\\,0.7,\\,0.2,\\,0.1\\,]$.\n  - Logits rows:\n    - $[\\,\\,-0.2,\\,0.4,\\,0.0\\,]$, $y=0$.\n    - $[\\,0.1,\\,0.0,\\,0.2\\,]$, $y=0$.\n    - $[\\,1.0,\\,-0.5,\\,-0.3\\,]$, $y=0$.\n    - $[\\,0.0,\\,1.1,\\,-0.1\\,]$, $y=1$.\n    - $[\\,\\,-0.8,\\,0.3,\\,0.4\\,]$, $y=2$.\n    - $[\\,0.6,\\,-0.2,\\,0.0\\,]$, $y=0$.\n\n- Test case $3$ (opposite prior shift, target favors classes $1$ and $2$):\n  - $K = 3$.\n  - $\\pi_S = [\\,0.6,\\,0.3,\\,0.1\\,]$.\n  - $\\pi_T = [\\,0.2,\\,0.5,\\,0.3\\,]$.\n  - Logits rows:\n    - $[\\,1.5,\\,0.2,\\,-0.2\\,]$, $y=1$.\n    - $[\\,0.8,\\,0.7,\\,0.6\\,]$, $y=2$.\n    - $[\\,\\,-0.3,\\,0.9,\\,0.0\\,]$, $y=1$.\n    - $[\\,2.0,\\,-0.5,\\,-0.2\\,]$, $y=0$.\n    - $[\\,\\,-0.5,\\,0.1,\\,1.2\\,]$, $y=2$.\n    - $[\\,0.1,\\,0.2,\\,0.0\\,]$, $y=1$.\n    - $[\\,0.5,\\,-0.3,\\,0.4\\,]$, $y=2$.\n\nProgram output specification. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case, append four numbers to this list in the order $[L_{\\text{base}}, L_{\\text{adapt}}, A_{\\text{base}}, A_{\\text{adapt}}]$, where $L_{\\text{base}}$ is the mean base loss rounded to six decimal places, $L_{\\text{adapt}}$ is the mean adapted loss rounded to six decimal places, $A_{\\text{base}}$ is the base accuracy rounded to four decimal places, and $A_{\\text{adapt}}$ is the adapted accuracy rounded to four decimal places. Thus, for $3$ test cases, the output list must contain $12$ numbers. The program must be self-contained and must not read any input or access external files.", "solution": "The problem requires the derivation and implementation of a calibration-aware adaptation method for domain adaptation under a prior probability shift. The core task is to adjust a source-domain classifier's logits, $h(x)$, to properly reflect the target-domain posterior probabilities.\n\nFirst, we formalize the problem based on the provided givens. We are given a source domain $S$ and a target domain $T$. For an input $x$ and a class label $y \\in \\{0, 1, \\ldots, K-1\\}$, the source and target class-prior probabilities are denoted by $p_S(y)$ and $p_T(y)$, respectively, which we will write as $\\pi_S(k)$ and $\\pi_T(k)$ for $y=k$. The central assumption is that of prior shift, where the class-conditional densities are invariant across domains:\n$$ p_S(x \\mid y) = p_T(x \\mid y) \\quad \\forall y $$\nThe source-domain classifier provides logits $h(x) \\in \\mathbb{R}^K$, from which the source posterior probabilities $p_S(y \\mid x)$ are obtained via the softmax function:\n$$ p_S(y=k \\mid x) = \\frac{\\exp(h_k(x))}{\\sum_{j=0}^{K-1} \\exp(h_j(x))} $$\nThe goal is to find a transformation for the logits $h(x)$ to new logits $h'_{\\text{adapt}}(x)$ such that the softmax of these new logits corresponds to the target posterior $p_T(y \\mid x)$.\n\nWe begin the derivation from the relationship between target and source posteriors under the prior shift assumption. By Bayes' rule, $p_T(y \\mid x) = \\frac{p_T(x \\mid y) p_T(y)}{p_T(x)}$. The marginal $p_T(x)$ is a normalization constant with respect to $y$, so we can write this as a proportionality:\n$$ p_T(y \\mid x) \\propto p_T(x \\mid y) p_T(y) $$\nUsing the class-conditional invariance $p_T(x \\mid y) = p_S(x \\mid y)$, we have:\n$$ p_T(y \\mid x) \\propto p_S(x \\mid y) p_T(y) $$\nFrom Bayes' rule in the source domain, we can express $p_S(x \\mid y)$ as $p_S(x \\mid y) = \\frac{p_S(y \\mid x) p_S(x)}{p_S(y)}$. Substituting this into the proportionality gives:\n$$ p_T(y \\mid x) \\propto \\frac{p_S(y \\mid x) p_S(x)}{p_S(y)} p_T(y) $$\nThe term $p_S(x)$ is constant for a given $x$ and can be absorbed into the proportionality constant. Thus, we arrive at the key relationship provided:\n$$ p_T(y=k \\mid x) \\propto p_S(y=k \\mid x) \\cdot \\frac{\\pi_T(k)}{\\pi_S(k)} $$\nNow, we substitute the softmax definition of the source posterior $p_S(y=k \\mid x)$:\n$$ p_T(y=k \\mid x) \\propto \\left( \\frac{\\exp(h_k(x))}{\\sum_{j=0}^{K-1} \\exp(h_j(x))} \\right) \\cdot \\frac{\\pi_T(k)}{\\pi_S(k)} $$\nThe denominator $\\sum_{j=0}^{K-1} \\exp(h_j(x))$ is also constant with respect to the class index $k$ and can be absorbed into the proportionality constant. This leaves:\n$$ p_T(y=k \\mid x) \\propto \\exp(h_k(x)) \\cdot \\frac{\\pi_T(k)}{\\pi_S(k)} $$\nUsing the property that $a \\cdot b = \\exp(\\log(a)) \\cdot \\exp(\\log(b)) = \\exp(\\log(a) + \\log(b))$, we can rewrite the expression as:\n$$ p_T(y=k \\mid x) \\propto \\exp\\left(h_k(x) + \\log\\left(\\frac{\\pi_T(k)}{\\pi_S(k)}\\right)\\right) $$\nThis result shows that the target posterior probability for class $k$ is proportional to the exponential of a modified logit. This defines the **calibration-aware adaptation principle**: the adapted logits $h'_{\\text{adapt}}(x)$ are obtained by adding a class-dependent offset to the original source logits. Let this offset vector be $c \\in \\mathbb{R}^K$, with components $c_k$:\n$$ c_k = \\log\\left(\\frac{\\pi_T(k)}{\\pi_S(k)}\\right) $$\nThe adapted logits for a given input $x$ are then:\n$$ h'_{k, \\text{adapt}}(x) = h_k(x) + c_k $$\nThe softmax of these adapted logits will correctly yield the target posterior probabilities:\n$$ p_T(y=k \\mid x) = \\operatorname{softmax}(h'_{\\text{adapt}}(x))_k = \\frac{\\exp(h'_{k, \\text{adapt}}(x))}{\\sum_{j=0}^{K-1} \\exp(h'_{j, \\text{adapt}}(x))} $$\nWith this derived principle, we can now define the required metrics. For a given sample $(x, y)$ with logits $h(x)$, we compute:\n1.  **Base (unadapted) metrics**: These are calculated using the original logits $h(x)$.\n    *   Base cross-entropy loss: $\\ell_{\\text{base}}(h(x), y) = -\\log(\\operatorname{softmax}(h(x))_y)$.\n    *   Base prediction: $\\hat{y}_{\\text{base}} = \\arg\\max_{k} h_k(x)$.\n    *   The mean base loss $L_{\\text{base}}$ and base accuracy $A_{\\text{base}}$ are the averages of these quantities over the dataset.\n\n2.  **Adapted metrics**: These are calculated using the adapted logits $h'_{\\text{adapt}}(x) = h(x) + c$.\n    *   Adapted cross-entropy loss: $\\ell_{\\text{adapt}}(h'_{\\text{adapt}}(x), y) = -\\log(\\operatorname{softmax}(h'_{\\text{adapt}}(x))_y)$. This is the negative log-likelihood under the target-domain model.\n    *   Adapted prediction (MAP rule for target posterior): $\\hat{y}_{\\text{adapt}} = \\arg\\max_{k} h'_{k, \\text{adapt}}(x)$.\n    *   The mean adapted loss $L_{\\text{adapt}}$ and adapted accuracy $A_{\\text{adapt}}$ are the averages of these quantities over the dataset.\n\nThe implementation will process each test case by first computing the constant offset vector $c$ from the given source and target priors, $\\pi_S$ and $\\pi_T$. Then, for each sample, it will compute the four metrics: base loss, adapted loss, base accuracy, and adapted accuracy. Finally, it will average these over all samples in the test case and round them to the specified precision.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the domain adaptation problem for the given test suite.\n    Derives and applies calibration-aware adaptation, then computes and\n    formats the required metrics.\n    \"\"\"\n    \n    test_cases = [\n        {\n            \"K\": 3,\n            \"pi_S\": [0.4, 0.35, 0.25],\n            \"pi_T\": [0.4, 0.35, 0.25],\n            \"logits\": [\n                [2.0, 0.5, -1.0], [0.2, 1.5, 0.0], [-0.5, -0.2, 1.0],\n                [1.0, 1.2, 0.8], [0.9, -1.0, 0.3]\n            ],\n            \"labels\": [0, 1, 2, 1, 0]\n        },\n        {\n            \"K\": 3,\n            \"pi_S\": [0.3333333333, 0.3333333333, 0.3333333333],\n            \"pi_T\": [0.7, 0.2, 0.1],\n            \"logits\": [\n                [-0.2, 0.4, 0.0], [0.1, 0.0, 0.2], [1.0, -0.5, -0.3],\n                [0.0, 1.1, -0.1], [-0.8, 0.3, 0.4], [0.6, -0.2, 0.0]\n            ],\n            \"labels\": [0, 0, 0, 1, 2, 0]\n        },\n        {\n            \"K\": 3,\n            \"pi_S\": [0.6, 0.3, 0.1],\n            \"pi_T\": [0.2, 0.5, 0.3],\n            \"logits\": [\n                [1.5, 0.2, -0.2], [0.8, 0.7, 0.6], [-0.3, 0.9, 0.0],\n                [2.0, -0.5, -0.2], [-0.5, 0.1, 1.2], [0.1, 0.2, 0.0],\n                [0.5, -0.3, 0.4]\n            ],\n            \"labels\": [1, 2, 1, 0, 2, 1, 2]\n        }\n    ]\n\n    def softmax(z):\n        \"\"\"Computes the numerically stable softmax of a vector.\"\"\"\n        z_shifted = z - np.max(z)\n        exps = np.exp(z_shifted)\n        return exps / np.sum(exps)\n\n    def compute_metrics_for_case(pi_S_list, pi_T_list, logits_list, labels_list):\n        \"\"\"\n        Computes base and adapted metrics for a single test case.\n        \"\"\"\n        pi_S = np.array(pi_S_list, dtype=np.float64)\n        pi_T = np.array(pi_T_list, dtype=np.float64)\n        logits = np.array(logits_list, dtype=np.float64)\n        labels = np.array(labels_list)\n        n_samples = len(labels)\n\n        # The prior shift assumption requires pi_S(k) > 0 for the ratio to be defined.\n        # Problem data adheres to this.\n        offset = np.log(pi_T / pi_S)\n\n        base_losses = []\n        base_correct_count = 0\n        adapted_losses = []\n        adapted_correct_count = 0\n\n        for i in range(n_samples):\n            h = logits[i]\n            y = labels[i]\n\n            # Base (unadapted) metrics\n            base_probs = softmax(h)\n            base_loss = -np.log(base_probs[y])\n            base_losses.append(base_loss)\n            \n            base_pred = np.argmax(h)\n            if base_pred == y:\n                base_correct_count += 1\n\n            # Adapted metrics\n            h_adapt = h + offset\n            adapted_probs = softmax(h_adapt)\n            adapted_loss = -np.log(adapted_probs[y])\n            adapted_losses.append(adapted_loss)\n            \n            adapted_pred = np.argmax(h_adapt)\n            if adapted_pred == y:\n                adapted_correct_count += 1\n\n        mean_base_loss = np.mean(base_losses)\n        base_accuracy = base_correct_count / n_samples\n        mean_adapted_loss = np.mean(adapted_losses)\n        adapted_accuracy = adapted_correct_count / n_samples\n        \n        # Rounding to specified precision\n        L_base = round(mean_base_loss, 6)\n        L_adapt = round(mean_adapted_loss, 6)\n        A_base = round(base_accuracy, 4)\n        A_adapt = round(adapted_accuracy, 4)\n\n        return [L_base, L_adapt, A_base, A_adapt]\n\n    all_results = []\n    for case in test_cases:\n        case_results = compute_metrics_for_case(\n            case[\"pi_S\"],\n            case[\"pi_T\"],\n            case[\"logits\"],\n            case[\"labels\"]\n        )\n        all_results.extend(case_results)\n\n    # Format output according to specification.\n    # Losses to 6 decimal places, accuracies to 4 decimal places.\n    formatted_results = [\n        f\"{res:.6f}\" if i % 4  2 else f\"{res:.4f}\"\n        for i, res in enumerate(all_results)\n    ]\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3189015"}, {"introduction": "When adapting models to new domains, is improving accuracy or ranking ability the only goal? This practice explores the crucial, and often overlooked, trade-off between a model's ability to discriminate between classes (measured by AUC) and the reliability of its probability estimates (measured by Brier score). By simulating a scenario where an adaptation improves discrimination but harms calibration, you will learn the importance of using a comprehensive suite of metrics to evaluate adapted models for real-world deployment. [@problem_id:3188990]", "problem": "Consider a binary classification problem with a source domain and multiple target domains. The goal is to study calibration versus discrimination trade-offs under domain shift in a principled way and to implement a program that demonstrates when a method that improves discrimination on a target domain can worsen calibration.\n\nStart from the following foundational definitions.\n\n1. Let the input be a real-valued feature denoted by $x \\in \\mathbb{R}$ and the label be $y \\in \\{0,1\\}$. A probabilistic classifier outputs a score $s(x) \\in \\mathbb{R}$ and a predicted probability $\\hat{p}(x) \\in [0,1]$, often via a logistic link $\\sigma(t) = 1/(1+e^{-t})$ where $\\hat{p}(x) = \\sigma(s(x))$.\n\n2. The Area Under the Receiver Operating Characteristic Curve (AUC) on a distribution is the probability that a randomly chosen positive instance is scored higher than a randomly chosen negative instance. Formally, for scores $s(X)$ and labels $Y$, \n$$\n\\mathrm{AUC} = \\mathbb{P}\\big(s(X^+) > s(X^-)\\big) + \\tfrac{1}{2}\\mathbb{P}\\big(s(X^+) = s(X^-)\\big).\n$$\nDiscrimination quality is assessed by $\\mathrm{AUC}$; it depends only on the induced ranking by $s(x)$.\n\n3. The Brier score is the mean squared error of predicted probabilities, \n$$\n\\mathrm{Brier} = \\mathbb{E}\\big[(\\hat{p}(X) - Y)^2\\big],\n$$\nwhich is a proper scoring rule emphasizing calibration.\n\n4. The source domain $\\mathcal{S}$ is generated by a homoscedastic Gaussian class-conditional model with shared variance. Specifically, let $Y \\sim \\mathrm{Bernoulli}(\\pi_{\\mathcal{S}})$ with $\\pi_{\\mathcal{S}} = 0.5$, and conditionals\n$$\nX \\mid Y=1 \\sim \\mathcal{N}(\\mu_{1,\\mathcal{S}}, \\sigma_{\\mathcal{S}}^2), \\quad X \\mid Y=0 \\sim \\mathcal{N}(\\mu_{0,\\mathcal{S}}, \\sigma_{\\mathcal{S}}^2),\n$$\nwith $\\mu_{1,\\mathcal{S}} = 1.5$, $\\mu_{0,\\mathcal{S}} = -1.5$, and $\\sigma_{\\mathcal{S}} = 1.0$.\n\n5. The Bayes posterior log-odds under this source model is linear in $x$. From basic Gaussian likelihoods and Bayes' rule, the log posterior odds has the form\n$$\n\\log\\frac{\\mathbb{P}(Y=1\\mid X=x)}{\\mathbb{P}(Y=0\\mid X=x)} = \\log\\frac{\\pi_{\\mathcal{S}}}{1-\\pi_{\\mathcal{S}}} + \\frac{\\mu_{1,\\mathcal{S}} - \\mu_{0,\\mathcal{S}}}{\\sigma_{\\mathcal{S}}^2}\\,x - \\frac{\\mu_{1,\\mathcal{S}}^2 - \\mu_{0,\\mathcal{S}}^2}{2\\sigma_{\\mathcal{S}}^2}.\n$$\nTherefore, a calibrated source model is\n$$\ns_{\\mathrm{base}}(x) = w_{\\mathcal{S}} x + b_{\\mathcal{S}}, \\quad \\hat{p}_{\\mathrm{base}}(x) = \\sigma\\big(s_{\\mathrm{base}}(x)\\big),\n$$\nwith $w_{\\mathcal{S}} = (\\mu_{1,\\mathcal{S}} - \\mu_{0,\\mathcal{S}})/\\sigma_{\\mathcal{S}}^2$ and $b_{\\mathcal{S}} = \\log\\frac{\\pi_{\\mathcal{S}}}{1-\\pi_{\\mathcal{S}}} - \\frac{\\mu_{1,\\mathcal{S}}^2 - \\mu_{0,\\mathcal{S}}^2}{2\\sigma_{\\mathcal{S}}^2}$.\n\n6. A rank-focused domain-adapted scoring function intended for variance-shifted targets is defined by the quadratic statistic,\n$$\ns_{\\mathrm{rank}}(x) = \\gamma x^2 + \\delta, \\quad \\hat{p}_{\\mathrm{rank}}(x) = \\sigma\\big(s_{\\mathrm{rank}}(x)\\big),\n$$\nwith fixed constants $\\gamma = 1.0$ and $\\delta = 2.0$. This choice intentionally prioritizes discrimination based on magnitude $\\lvert x \\rvert$ and does not aim for probability calibration. Note that for targets where class conditionals have equal means but different variances, the log-likelihood ratio is quadratic in $x$, so ranking by $x^2$ aligns with the most powerful test in that setting.\n\nYou will evaluate both methods on three target domains to highlight calibration versus discrimination trade-offs under different types of shift. Each target domain $\\mathcal{T}$ has its own label prior $\\pi_{\\mathcal{T}}$ and class-conditional Gaussian parameters. For each target case, generate a dataset of size $n_{\\mathcal{T}}$ i.i.d. from the specified target distribution, evaluate both $\\mathrm{AUC}$ and $\\mathrm{Brier}$ for the base model and the rank-focused model, and then return a boolean indicating if the rank-focused model strictly improves $\\mathrm{AUC}$ on the target while strictly worsening the Brier score on the target.\n\nTest suite of target domains:\n\n- Case A (variance shift, equal means): $n_{\\mathcal{T}} = 80000$, $\\pi_{\\mathcal{T}} = 0.2$, $X \\mid Y=1 \\sim \\mathcal{N}(0.0, 2.0^2)$, $X \\mid Y=0 \\sim \\mathcal{N}(0.0, 0.5^2)$. This target has equal means and different variances, favoring a quadratic ranking on $\\lvert x \\rvert$.\n\n- Case B (no shift): $n_{\\mathcal{T}} = 80000$, $\\pi_{\\mathcal{T}} = 0.5$, $X \\mid Y=1 \\sim \\mathcal{N}(1.5, 1.0^2)$, $X \\mid Y=0 \\sim \\mathcal{N}(-1.5, 1.0^2)$. This target equals the source.\n\n- Case C (label shift only): $n_{\\mathcal{T}} = 80000$, $\\pi_{\\mathcal{T}} = 0.2$, $X \\mid Y=1 \\sim \\mathcal{N}(1.5, 1.0^2)$, $X \\mid Y=0 \\sim \\mathcal{N}(-1.5, 1.0^2)$. This target keeps conditionals but changes the class prior.\n\nImplementation requirements:\n\n- Use the source parameters to compute $w_{\\mathcal{S}}$ and $b_{\\mathcal{S}}$ analytically. Then, for each target case, simulate data, compute the $\\mathrm{AUC}$ and $\\mathrm{Brier}$ for $\\hat{p}_{\\mathrm{base}}$ and for $\\hat{p}_{\\mathrm{rank}}$, and determine whether\n$$\n\\big(\\mathrm{AUC}_{\\mathrm{rank}} > \\mathrm{AUC}_{\\mathrm{base}}\\big) \\;\\;\\text{and}\\;\\; \\big(\\mathrm{Brier}_{\\mathrm{rank}} > \\mathrm{Brier}_{\\mathrm{base}}\\big)\n$$\nholds. Use a fixed random seed so that results are deterministic.\n\n- Your program must output a single line containing a list of three booleans corresponding to Cases A, B, and C in that order. The $i$-th boolean must be true if and only if the rank-focused model has strictly higher $\\mathrm{AUC}$ and strictly higher (worse) Brier score than the base model on the $i$-th target case. The output format is a single line with a comma-separated list enclosed in square brackets, for example, \"[True,False,True]\".\n\nAdditional conceptual task (addressed in your written solution, not in the code): Based on first principles, explain why a method can increase $\\mathrm{AUC}$ while worsening Brier score under shift, and propose an evaluation metric suite for domain adaptation that jointly assesses discrimination and calibration, explicitly justifying each metric from well-tested definitions. No physical units are involved in this problem.", "solution": "The problem has been validated and is determined to be a valid, well-posed, and scientifically grounded exercise in statistical learning. It provides a clear and complete setup for analyzing the trade-offs between discrimination and calibration of probabilistic classifiers under domain shift. All definitions, parameters, and evaluation criteria are formally and correctly stated.\n\nThe solution proceeds in three stages. First, we determine the analytical forms of the provided models. Second, we analyze the expected behavior of each model on the three target domains. Third, we address the conceptual questions regarding the divergence of discrimination and calibration metrics.\n\n**1. Analytical Model Specification**\n\nThe problem defines a base model derived from the source domain $\\mathcal{S}$ and a rank-focused model.\n\nThe source domain parameters are $\\pi_{\\mathcal{S}} = 0.5$, $\\mu_{1,\\mathcal{S}} = 1.5$, $\\mu_{0,\\mathcal{S}} = -1.5$, and $\\sigma_{\\mathcal{S}} = 1.0$.\n\nThe base model's score function is $s_{\\mathrm{base}}(x) = w_{\\mathcal{S}} x + b_{\\mathcal{S}}$, where the parameters are derived from the Bayes posterior log-odds for the source distribution.\nThe weight $w_{\\mathcal{S}}$ is calculated as:\n$$\nw_{\\mathcal{S}} = \\frac{\\mu_{1,\\mathcal{S}} - \\mu_{0,\\mathcal{S}}}{\\sigma_{\\mathcal{S}}^2} = \\frac{1.5 - (-1.5)}{1.0^2} = \\frac{3.0}{1.0} = 3.0\n$$\nThe bias $b_{\\mathcal{S}}$ is calculated as:\n$$\nb_{\\mathcal{S}} = \\log\\frac{\\pi_{\\mathcal{S}}}{1-\\pi_{\\mathcal{S}}} - \\frac{\\mu_{1,\\mathcal{S}}^2 - \\mu_{0,\\mathcal{S}}^2}{2\\sigma_{\\mathcal{S}}^2}\n$$\nGiven $\\pi_{\\mathcal{S}} = 0.5$, the first term $\\log\\frac{0.5}{1-0.5} = \\log(1) = 0$.\nGiven $\\mu_{1,\\mathcal{S}} = 1.5$ and $\\mu_{0,\\mathcal{S}} = -1.5$, the numerator of the second term is $\\mu_{1,\\mathcal{S}}^2 - \\mu_{0,\\mathcal{S}}^2 = (1.5)^2 - (-1.5)^2 = 2.25 - 2.25 = 0$.\nTherefore, $b_{\\mathcal{S}} = 0 - 0 = 0$.\n\nThe base model is thus:\n$$\ns_{\\mathrm{base}}(x) = 3.0x, \\quad \\hat{p}_{\\mathrm{base}}(x) = \\sigma(3.0x)\n$$\nThis model is, by construction, the Bayes-optimal classifier for the source domain $\\mathcal{S}$, meaning it is perfectly calibrated and maximally discriminative on that specific data distribution.\n\nThe rank-focused model is given with fixed parameters $\\gamma = 1.0$ and $\\delta = 2.0$:\n$$\ns_{\\mathrm{rank}}(x) = 1.0x^2 + 2.0, \\quad \\hat{p}_{\\mathrm{rank}}(x) = \\sigma(x^2 + 2.0)\n$$\nThis model is designed to perform well under a specific type of domain shift where the classes have equal means but different variances, making the log-likelihood ratio a quadratic function of $x$.\n\n**2. Analysis of Performance on Target Domains**\n\nWe evaluate the condition $(\\mathrm{AUC}_{\\mathrm{rank}} > \\mathrm{AUC}_{\\mathrm{base}}) \\land (\\mathrm{Brier}_{\\mathrm{rank}} > \\mathrm{Brier}_{\\mathrm{base}})$ for each target case.\n\n**Case A (Variance Shift):**\nThe target distribution has parameters $\\pi_{\\mathcal{T}} = 0.2$, $X \\mid Y=1 \\sim \\mathcal{N}(0.0, 2.0^2)$, and $X \\mid Y=0 \\sim \\mathcal{N}(0.0, 0.5^2)$.\nHere, the class-conditional means are equal, $\\mu_{1,\\mathcal{T}} = \\mu_{0,\\mathcal{T}} = 0.0$, but the variances differ, $\\sigma_{1,\\mathcal{T}}^2 = 4.0$ and $\\sigma_{0,\\mathcal{T}}^2 = 0.25$. The Bayes-optimal score function (proportional to the log-likelihood ratio) for this target is a quadratic function of $x$. Specifically, large values of $|x|$ provide evidence for $Y=1$ since $\\sigma_{1,\\mathcal{T}} > \\sigma_{0,\\mathcal{T}}$.\n-   **Discrimination (AUC):** The rank-focused model's score $s_{\\mathrm{rank}}(x) = x^2 + 2.0$ increases with $|x|$, correctly capturing the ranking principle for this target. The base model's score $s_{\\mathrm{base}}(x) = 3.0x$ will perform poorly, as positive and negative values of $x$ of the same magnitude will be scored very differently, even though they provide the same evidence for $Y=1$. Thus, we expect $\\mathrm{AUC}_{\\mathrm{rank}} > \\mathrm{AUC}_{\\mathrm{base}}$.\n-   **Calibration (Brier Score):** The rank-focused model uses arbitrary coefficients $\\gamma=1.0, \\delta=2.0$ that are not derived from the target distribution's parameters ($\\pi_{\\mathcal{T}}, \\mu_i, \\sigma_i$). Therefore, the probabilities $\\hat{p}_{\\mathrm{rank}}(x)$ will not be calibrated to the true posterior $\\mathbb{P}(Y=1|X=x)$, resulting in a high Brier score. The base model is also misspecified and will be uncalibrated. However, the rank model, while superior in ranking, is likely to be severely miscalibrated, leading to a higher (worse) Brier score. We expect $\\mathrm{Brier}_{\\mathrm{rank}} > \\mathrm{Brier}_{\\mathrm{base}}$.\n-   **Conclusion:** The condition is expected to be **True**.\n\n**Case B (No Shift):**\nThe target distribution is identical to the source.\n-   **Discrimination (AUC):** The base model $s_{\\mathrm{base}}(x)$ is the Bayes-optimal classifier for this distribution and will achieve the maximum possible AUC. The rank-focused model $s_{\\mathrm{rank}}(x)$ is misspecified, as the optimal discriminator is linear, not quadratic. We expect $\\mathrm{AUC}_{\\mathrm{rank}}  \\mathrm{AUC}_{\\mathrm{base}}$.\n-   **Calibration (Brier Score):** The base model is perfectly calibrated by construction and will achieve the lowest possible Brier score. The rank-focused model is misspecified and will be poorly calibrated, resulting in a much higher Brier score.\n-   **Conclusion:** The first part of the condition, $\\mathrm{AUC}_{\\mathrm{rank}} > \\mathrm{AUC}_{\\mathrm{base}}$, is false. The condition is expected to be **False**.\n\n**Case C (Label Shift Only):**\nThe class-conditional distributions are identical to the source, but the class prior changes to $\\pi_{\\mathcal{T}} = 0.2$.\n-   **Discrimination (AUC):** AUC is a measure of rank-ordering and is insensitive to class balance (label shift). The optimal ranking of instances is determined by the likelihood ratio $p(x|Y=1)/p(x|Y=0)$, which has not changed from the source. The linear score $s_{\\mathrm{base}}(x)$ thus remains the optimal ranker. The quadratic $s_{\\mathrm{rank}}(x)$ is still misspecified. Therefore, we expect $\\mathrm{AUC}_{\\mathrm{rank}}  \\mathrm{AUC}_{\\mathrm{base}}$.\n-   **Calibration (Brier Score):** The base model's probabilities $\\hat{p}_{\\mathrm{base}}(x) = \\sigma(3.0x)$ implicitly assume the source prior $\\pi_{\\mathcal{S}}=0.5$ via the intercept $b_{\\mathcal{S}}=0$. The correctly calibrated model for the target would be $\\hat{p}(x) = \\sigma(3.0x + b_{\\mathcal{T}})$, where $b_{\\mathcal{T}} = \\log(\\pi_{\\mathcal{T}}/(1-\\pi_{\\mathcal{T}})) = \\log(0.2/0.8) \\approx -1.386$. Because its intercept is wrong, the base model is now miscalibrated. The rank model is also miscalibrated. However, the base model is far closer to the true form.\n-   **Conclusion:** The first part of the condition, $\\mathrm{AUC}_{\\mathrm{rank}} > \\mathrm{AUC}_{\\mathrm{base}}$, is false. The condition is expected to be **False**.\n\n**3. Conceptual Task: Discrimination vs. Calibration and Metric Suite**\n\n**Why AUC can increase while Brier score worsens**\n\nThis phenomenon is a direct consequence of the different properties these two metrics evaluate.\n-   **AUC (Discrimination):** The Area Under the ROC Curve is a rank-based metric. It is invariant to any strictly order-preserving transformation of the classifier's scores. That is, if a model outputs scores $s(x)$, its AUC is identical to that of a model outputting scores $f(s(x))$ for any strictly increasing function $f$. AUC solely measures whether the classifier consistently assigns higher scores to positive instances than to negative instances.\n-   **Brier Score (Calibration):** The Brier score, $\\mathbb{E}[(\\hat{p}(X) - Y)^2]$, is a proper scoring rule that measures the mean squared error of the predicted probabilities. It is highly sensitive to the actual numerical values of $\\hat{p}(x)$. A model is well-calibrated if, for all instances where it predicts a probability of, say, $0.8$, approximately $80\\%$ of those instances are actually positive. Applying an arbitrary monotonic transformation $f$ to the scores $s(x)$ will generally destroy calibration, as the new \"probabilities\" $\\sigma(f(s(x)))$ will no longer correspond to the true posterior probabilities $\\mathbb{P}(Y=1|X=x)$.\n\nUnder domain shift, a new feature transformation (like the quadratic score in Case A) might better separate the classes in the target domain, leading to an improved ranking and thus a higher AUC. However, if the resulting scores are not a proper representation of the log-posterior-odds on the target distribution, applying the sigmoid function will yield miscalibrated probabilities. This leads to a higher (worse) Brier score, creating the observed trade-off: improved discrimination at the cost of worsened calibration.\n\n**Proposed Evaluation Metric Suite for Domain Adaptation**\n\nTo comprehensively evaluate a classifier's performance after domain adaptation, a suite of metrics that separately and jointly assess discrimination and calibration is required.\n\n1.  **Discrimination Metric: Area Under the ROC Curve (AUC)**\n    -   **Justification:** As the standard measure of a model's ranking ability, AUC is essential for understanding if the model can distinguish between classes. Its invariance to score transformations and class prevalence makes it a robust indicator of pure discriminative power, which is a primary goal of adaptation.\n\n2.  **Calibration Metric: Expected Calibration Error (ECE)**\n    -   **Justification:** ECE directly measures miscalibration by partitioning predictions into bins based on their confidence (predicted probability) and computing the weighted average of the difference between the average confidence and the observed accuracy in each bin. Formally, $\\mathrm{ECE} = \\sum_{m=1}^{M} \\frac{|B_m|}{n} |\\mathrm{acc}(B_m) - \\mathrm{conf}(B_m)|$. Unlike Brier score, ECE isolates calibration error, making it highly interpretable for diagnosing calibration-specific failures. A good adaptation method should not significantly increase ECE.\n\n3.  **Overall Quality Metric: Brier Score**\n    -   **Justification:** The Brier score is a proper scoring rule, meaning a model is incentivized to report its true belief to minimize the score. It elegantly combines aspects of both discrimination and calibration. The Brier score can be decomposed into reliability (calibration), resolution (a facet of discrimination), and uncertainty components. A low Brier score indicates that a model is both discriminative (high resolution) and well-calibrated (low reliability error). It serves as an excellent single-figure summary of the overall quality of a probabilistic prediction.\n\nThis suite of $\\{\\mathrm{AUC}, \\mathrm{ECE}, \\mathrm{Brier\\ Score}\\}$ provides a holistic view. An ideal adaptation would improve AUC while keeping ECE and Brier score low. Observing an increase in AUC alongside an increase in ECE and Brier score clearly signals the trade-off this problem aims to highlight.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import expit as sigmoid\n\ndef solve():\n    \"\"\"\n    Solves the problem by simulating three target domains and evaluating two models\n    on each, checking for a discrimination-calibration trade-off.\n    \"\"\"\n\n    # Set a fixed random seed for reproducibility\n    np.random.seed(0)\n\n    # --- Analytical Parameters ---\n    # Source parameters\n    mu1_s, mu0_s, sigma_s, pi_s = 1.5, -1.5, 1.0, 0.5\n    \n    # Base model parameters (derived analytically)\n    w_s = (mu1_s - mu0_s) / sigma_s**2\n    b_s = np.log(pi_s / (1.0 - pi_s)) - (mu1_s**2 - mu0_s**2) / (2 * sigma_s**2)\n\n    # Rank model parameters (given)\n    gamma, delta = 1.0, 2.0\n\n    # Test suite of target domain parameters\n    test_cases = [\n        # Case A: Variance shift, equal means\n        {'name': 'A', 'n_t': 80000, 'pi_t': 0.2, 'mu1_t': 0.0, 'sigma1_t': 2.0, 'mu0_t': 0.0, 'sigma0_t': 0.5},\n        # Case B: No shift\n        {'name': 'B', 'n_t': 80000, 'pi_t': 0.5, 'mu1_t': 1.5, 'sigma1_t': 1.0, 'mu0_t': -1.5, 'sigma0_t': 1.0},\n        # Case C: Label shift only\n        {'name': 'C', 'n_t': 80000, 'pi_t': 0.2, 'mu1_t': 1.5, 'sigma1_t': 1.0, 'mu0_t': -1.5, 'sigma0_t': 1.0},\n    ]\n\n    results = []\n\n    for case in test_cases:\n        # --- Data Generation ---\n        n_t, pi_t = case['n_t'], case['pi_t']\n        n1_t = int(n_t * pi_t)\n        n0_t = n_t - n1_t\n\n        x1_t = np.random.normal(loc=case['mu1_t'], scale=case['sigma1_t'], size=n1_t)\n        x0_t = np.random.normal(loc=case['mu0_t'], scale=case['sigma0_t'], size=n0_t)\n\n        x_target = np.concatenate((x1_t, x0_t))\n        y_target = np.concatenate((np.ones(n1_t), np.zeros(n0_t)))\n\n        # --- Model Evaluation ---\n        # Base model\n        s_base = w_s * x_target + b_s\n        p_base = sigmoid(s_base)\n        \n        # Rank-focused model\n        s_rank = gamma * x_target**2 + delta\n        p_rank = sigmoid(s_rank)\n\n        # --- Metric Calculation ---\n        # Brier score\n        brier_base = np.mean((p_base - y_target)**2)\n        brier_rank = np.mean((p_rank - y_target)**2)\n        \n        # AUC calculation (using the efficient rank-based method)\n        def calculate_auc(y_true, y_score):\n            y_true = np.asarray(y_true, dtype=bool)\n            \n            n1 = np.count_nonzero(y_true)\n            n0 = len(y_true) - n1\n            \n            if n1 == 0 or n0 == 0:\n                return 0.5 # Or handle as undefined, 0.5 is a common convention\n            \n            # Use argsort to get ranks efficiently\n            order = np.argsort(y_score)\n            y_true_sorted = y_true[order]\n            \n            # Sum of ranks for the positive class\n            # np.where returns a tuple, we need the first element\n            pos_ranks_sum = np.sum(np.where(y_true_sorted)[0] + 1)\n            \n            # Wilcoxon-Mann-Whitney U statistic relationship\n            auc = (pos_ranks_sum - n1 * (n1 + 1) / 2) / (n1 * n0)\n            return auc\n\n        auc_base = calculate_auc(y_target, s_base)\n        auc_rank = calculate_auc(y_target, s_rank)\n\n        # --- Condition Check ---\n        # Did the rank model strictly improve AUC and strictly worsen Brier?\n        trade_off_occurred = (auc_rank > auc_base) and (brier_rank > brier_base)\n        results.append(trade_off_occurred)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3188990"}]}