{"hands_on_practices": [{"introduction": "The general formula for the instrumental variable (IV) estimator, $\\beta = \\frac{\\operatorname{Cov}(Z, Y)}{\\operatorname{Cov}(Z, D)}$, can feel abstract. This practice demystifies the concept by focusing on a simple yet powerful case where the instrument is binary. You will derive the celebrated Wald estimator, showing how the covariance ratio simplifies to an intuitive ratio of differences in means, and then apply it to a practical scenario involving educational research [@problem_id:3131775]. This exercise bridges the gap between abstract theory and concrete calculation, revealing the core logic of IV in its most accessible form.", "problem": "An education researcher seeks to estimate the causal effect of training intensity on test performance. Let $Y$ denote a student’s post-program test score and let $D$ denote the number of training hours completed, which is continuous. Because students may self-select their training intensity, $D$ may be endogenous. The researcher exploits an encouragement assignment $Z \\in \\{0,1\\}$ (discrete) that offers additional logistical support to attend training, where $Z=1$ indicates assignment to encouragement and $Z=0$ indicates no encouragement.\n\nAssume the following foundational conditions:\n- A linear structural relation holds: $Y = \\alpha + \\beta D + u$, where $u$ is an unobserved error.\n- Relevance: $\\operatorname{Cov}(Z,D) \\neq 0$.\n- Exclusion and exogeneity: $Z$ affects $Y$ only through $D$, and $E[u \\mid Z] = E[u]$.\n- Random assignment of $Z$: $Z$ is independent of $(D,u)$ except through the effect on $D$ stated above.\n\nStarting from these conditions and the basic definitions of covariance and conditional expectation, derive an expression for the causal parameter $\\beta$ that can be identified by the discrete instrument $Z$ when $D$ is continuous. Then show how the identified expression specializes, for binary $Z$, into a ratio involving differences in conditional expectations across the two groups $Z=1$ and $Z=0$ (that is, a two-sample grouping and ratio-of-differences representation).\n\nFinally, the researcher collects summary statistics from a large sample:\n- Average training hours for $Z=1$: $\\overline{D}_{1} = 5.77$; for $Z=0$: $\\overline{D}_{0} = 4.22$.\n- Average test scores for $Z=1$: $\\overline{Y}_{1} = 12.84$; for $Z=0$: $\\overline{Y}_{0} = 10.31$.\n\nUsing the two-sample grouping and ratio-of-differences representation you derived, compute the instrumental variable estimate of $\\beta$ and report your final numeric answer. Round your answer to four significant figures.", "solution": "The problem presents a valid and well-posed scenario for instrumental variable (IV) estimation. We are tasked with deriving the IV estimator for a causal parameter and then computing its value from summary statistics.\n\nFirst, we derive the general expression for the causal parameter $\\beta$. The structural model is given by:\n$$ Y = \\alpha + \\beta D + u $$\nwhere $Y$ is the test score, $D$ is the training intensity, $\\beta$ is the causal effect of $D$ on $Y$, and $u$ is an unobserved error term. The variable $D$ is potentially endogenous, meaning $\\operatorname{Cov}(D, u) \\neq 0$.\n\nWe are given an instrumental variable $Z$, the encouragement assignment, which satisfies two key conditions:\n1.  **Relevance**: $\\operatorname{Cov}(Z, D) \\neq 0$. The instrument has a causal effect on the endogenous variable $D$.\n2.  **Exogeneity and Exclusion**: $E[u \\mid Z] = E[u]$. The instrument $Z$ is uncorrelated with the structural error $u$ and affects the outcome $Y$ only through its effect on $D$.\n\nThe condition $E[u \\mid Z] = E[u]$ implies that the covariance between $Z$ and $u$ is zero. We can prove this using the law of total expectation:\n$$ \\operatorname{Cov}(Z, u) = E[Zu] - E[Z]E[u] $$\nWe have $E[Zu] = E[E[Zu \\mid Z]] = E[Z \\cdot E[u \\mid Z]]$. Since $E[u \\mid Z] = E[u]$ (a constant with respect to $Z$), this becomes $E[Z \\cdot E[u]] = E[u]E[Z]$.\nSubstituting this into the covariance formula yields:\n$$ \\operatorname{Cov}(Z, u) = E[u]E[Z] - E[Z]E[u] = 0 $$\n\nTo identify $\\beta$, we leverage this property by taking the covariance of the structural equation with the instrument $Z$:\n$$ \\operatorname{Cov}(Z, Y) = \\operatorname{Cov}(Z, \\alpha + \\beta D + u) $$\nUsing the properties of covariance, we can expand the right-hand side:\n$$ \\operatorname{Cov}(Z, Y) = \\operatorname{Cov}(Z, \\alpha) + \\operatorname{Cov}(Z, \\beta D) + \\operatorname{Cov}(Z, u) $$\nWe evaluate each term:\n-   $\\operatorname{Cov}(Z, \\alpha) = 0$ because $\\alpha$ is a constant.\n-   $\\operatorname{Cov(Z, \\beta D)} = \\beta \\operatorname{Cov}(Z, D)$ by bilinearity.\n-   $\\operatorname{Cov}(Z, u) = 0$ from the exogeneity assumption.\n\nThis simplifies the equation to:\n$$ \\operatorname{Cov}(Z, Y) = \\beta \\operatorname{Cov}(Z, D) $$\nGiven the relevance condition ($\\operatorname{Cov}(Z, D) \\neq 0$), we can solve for $\\beta$:\n$$ \\beta = \\frac{\\operatorname{Cov}(Z, Y)}{\\operatorname{Cov}(Z, D)} $$\nThis is the general formula for the IV estimator.\n\nNext, we specialize this formula for the case where the instrument $Z$ is a binary variable, $Z \\in \\{0, 1\\}$. For any random variable $X$, its covariance with a binary $Z$ can be expressed in terms of conditional expectations. Let $p = P(Z=1)$. Then $P(Z=0) = 1-p$.\n$$ \\operatorname{Cov}(Z, X) = E[ZX] - E[Z]E[X] $$\nThe terms are:\n$E[ZX] = 1 \\cdot E[X \\mid Z=1] P(Z=1) + 0 \\cdot E[X \\mid Z=0] P(Z=0) = p \\cdot E[X \\mid Z=1]$.\n$E[Z] = p$.\n$E[X] = E[X \\mid Z=1]P(Z=1) + E[X \\mid Z=0]P(Z=0) = p E[X \\mid Z=1] + (1-p)E[X \\mid Z=0]$.\nSubstituting these gives:\n$$ \\operatorname{Cov}(Z, X) = p E[X \\mid Z=1] - p [p E[X \\mid Z=1] + (1-p)E[X \\mid Z=0]] $$\n$$ \\operatorname{Cov}(Z, X) = p(1-p)E[X \\mid Z=1] - p(1-p)E[X \\mid Z=0] $$\n$$ \\operatorname{Cov}(Z, X) = p(1-p) (E[X \\mid Z=1] - E[X \\mid Z=0]) $$\nApplying this result to both the numerator and the denominator of the IV formula for $\\beta$:\n$$ \\beta = \\frac{p(1-p)(E[Y \\mid Z=1] - E[Y \\mid Z=0])}{p(1-p)(E[D \\mid Z=1] - E[D \\mid Z=0])} $$\nAssuming $p$ is not $0$ or $1$, the term $p(1-p)$ cancels, yielding the ratio-of-differences representation, also known as the Wald estimator:\n$$ \\beta = \\frac{E[Y \\mid Z=1] - E[Y \\mid Z=0]}{E[D \\mid Z=1] - E[D \\mid Z=0]} $$\n\nFinally, we compute the numerical estimate of $\\beta$ using the provided summary statistics. The sample means are used as estimates for the population conditional expectations. The sample IV estimate, $\\hat{\\beta}_{IV}$, is given by:\n$$ \\hat{\\beta}_{IV} = \\frac{\\overline{Y}_{1} - \\overline{Y}_{0}}{\\overline{D}_{1} - \\overline{D}_{0}} $$\nThe provided data are:\n-   Average training hours: $\\overline{D}_{1} = 5.77$ for $Z=1$, and $\\overline{D}_{0} = 4.22$ for $Z=0$.\n-   Average test scores: $\\overline{Y}_{1} = 12.84$ for $Z=1$, and $\\overline{Y}_{0} = 10.31$ for $Z=0$.\n\nSubstituting these values into the formula:\n$$ \\hat{\\beta}_{IV} = \\frac{12.84 - 10.31}{5.77 - 4.22} $$\nWe calculate the differences in the numerator and denominator:\n$$ \\hat{\\beta}_{IV} = \\frac{2.53}{1.55} $$\nPerforming the division gives the numerical estimate:\n$$ \\hat{\\beta}_{IV} \\approx 1.632258... $$\nRounding the result to four significant figures, as requested, we obtain $1.632$.", "answer": "$$\\boxed{1.632}$$", "id": "3131775"}, {"introduction": "The power of instrumental variables lies in their ability to isolate the \"clean\" variation in an endogenous regressor. This exercise takes you under the hood of the IV mechanism by using a data simulation. You will create a hypothetical dataset where a variable $x$ is correlated with the error term $u$, and then demonstrate numerically how projecting $x$ onto a valid instrument $Z$ purges this correlation, producing a new variable $\\hat{x}$ that is uncorrelated with $u$ [@problem_id:2402330]. This simulation illustrates the foundational principle that makes the popular Two-Stage Least Squares (2SLS) method effective.", "problem": "You are given the structural model $y = \\beta_0 + \\beta_1 x + u$ with endogenous regressor $x$ such that $\\mathrm{Cov}(x, u) \\ne 0$. Let $Z$ be an external variable intended to serve as an instrument. The goal is to numerically demonstrate, via sample correlations, that the orthogonal projection of $x$ onto the span of a constant and $Z$ removes the sample correlation with the structural error $u$ when $Z$ is exogenous and relevant.\n\nData-generating process. For each observation $i \\in \\{1,\\dots,n\\}$:\n- Draw $Z_i \\sim \\mathcal{N}(0,1)$ independently and identically distributed (i.i.d.).\n- Draw $(u_i, v_i)^\\top \\sim \\mathcal{N}\\!\\left(\\begin{bmatrix}0 \\\\ 0\\end{bmatrix}, \\begin{bmatrix}1 & \\rho \\\\ \\rho & 1\\end{bmatrix}\\right)$ i.i.d., independently of $\\{Z_i\\}_{i=1}^n$.\n- Construct $x_i = \\pi Z_i + v_i$.\n- Construct $y_i = \\beta_0 + \\beta_1 x_i + u_i$.\n\nDefine the $n \\times 2$ matrix $X = \\begin{bmatrix}\\mathbf{1} & Z\\end{bmatrix}$, where $\\mathbf{1}$ is the $n \\times 1$ vector of ones and $Z$ is the $n \\times 1$ vector stacking $\\{Z_i\\}_{i=1}^n$. Let $\\hat{x}$ denote the orthogonal projection of $x$ onto the column space of $X$, i.e., $\\hat{x} = P x$ where $P = X(X^\\top X)^{-1}X^\\top$.\n\nFor any two vectors $a$ and $b$ of length $n$, define the sample correlation\n$$\n\\widehat{\\mathrm{Corr}}(a,b) = \\frac{\\sum_{i=1}^n (a_i - \\bar{a})(b_i - \\bar{b})}{\\sqrt{\\sum_{i=1}^n (a_i - \\bar{a})^2}\\,\\sqrt{\\sum_{i=1}^n (b_i - \\bar{b})^2}},\n$$\nwhere $\\bar{a} = \\frac{1}{n}\\sum_{i=1}^n a_i$ and $\\bar{b} = \\frac{1}{n}\\sum_{i=1}^n b_i$.\n\nTasks. For each parameter tuple $(n,\\beta_0,\\beta_1,\\pi,\\rho,s)$ in the test suite below:\n- Using the above data-generating process, simulate a dataset of size $n$ with parameters $(\\beta_0,\\beta_1,\\pi,\\rho)$, using $s$ as the pseudorandom seed for all draws in that case.\n- Compute two floats: $\\widehat{\\mathrm{Corr}}(x,u)$ and $\\widehat{\\mathrm{Corr}}(\\hat{x},u)$.\n- Round each float to $6$ decimal places.\n\nTest suite. Use the following four parameter sets:\n- Case A: $(n,\\beta_0,\\beta_1,\\pi,\\rho,s) = (2000, 1.0, 1.5, 1.0, 0.7, 12345)$.\n- Case B: $(n,\\beta_0,\\beta_1,\\pi,\\rho,s) = (2000, 0.0, 2.0, 0.2, 0.7, 54321)$.\n- Case C: $(n,\\beta_0,\\beta_1,\\pi,\\rho,s) = (2000, 0.5, 1.0, 0.8, 0.0, 202311)$.\n- Case D: $(n,\\beta_0,\\beta_1,\\pi,\\rho,s) = (50, -0.5, 1.2, 0.9, 0.8, 777)$.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list of four lists, one per test case, in the order A, B, C, D. Each inner list must be $[\\widehat{\\mathrm{Corr}}(x,u), \\widehat{\\mathrm{Corr}}(\\hat{x},u)]$ with each entry rounded to $6$ decimals. The overall output must be a single line formatted exactly as\n$[[a_1,b_1],[a_2,b_2],[a_3,b_3],[a_4,b_4]]$\nwith no spaces, where $a_j$ and $b_j$ are the required rounded floats for case $j \\in \\{1,2,3,4\\}$.", "solution": "The problem statement is valid. It presents a well-defined computational task in econometrics that is scientifically sound, internally consistent, and objective. The goal is to numerically demonstrate a foundational principle of the instrumental variable (IV) method, specifically the role of the first-stage projection in purging an endogenous regressor of its correlation with the structural error term.\n\nThe structural model is given by $y_i = \\beta_0 + \\beta_1 x_i + u_i$ for each observation $i \\in \\{1, \\dots, n\\}$. The regressor $x$ is endogenous, meaning it is correlated with the error term $u$, formally $\\mathrm{Cov}(x, u) \\ne 0$. This correlation violates the core assumption of Ordinary Least Squares (OLS) and renders the OLS estimator of $\\beta_1$ inconsistent. The source of this endogeneity is a common unobserved factor, modeled through the correlation $\\rho$ between the error components $u_i$ and $v_i$.\n\nThe data-generating process is specified as follows:\n$1$. The instrumental variable $Z_i$ is drawn from a standard normal distribution, $Z_i \\sim \\mathcal{N}(0,1)$.\n$2$. The error components $(u_i, v_i)^\\top$ are drawn from a bivariate normal distribution with mean $\\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$ and covariance matrix $\\begin{bmatrix}1 & \\rho \\\\ \\rho & 1\\end{bmatrix}$. Crucially, the instruments $\\{Z_i\\}$ are independent of the error components $\\{(u_i,v_i)\\}$. This makes $Z$ an exogenous variable, as $\\mathrm{Cov}(Z_i, u_i) = 0$.\n$3$. The endogenous regressor is constructed via the first-stage equation $x_i = \\pi Z_i + v_i$. The parameter $\\pi$ governs the relevance of the instrument; if $\\pi \\ne 0$, then $Z$ is a relevant instrument for $x$. The endogeneity arises because $\\mathrm{Cov}(x_i, u_i) = \\mathrm{Cov}(\\pi Z_i + v_i, u_i) = \\pi\\mathrm{Cov}(Z_i, u_i) + \\mathrm{Cov}(v_i, u_i) = 0 + \\rho = \\rho$. Thus, for $\\rho \\ne 0$, we have endogeneity. The theoretical population correlation is $\\mathrm{Corr}(x_i, u_i) = \\frac{\\rho}{\\sqrt{\\mathrm{Var}(x_i)\\mathrm{Var}(u_i)}} = \\frac{\\rho}{\\sqrt{(\\pi^2+1)(1)}} = \\frac{\\rho}{\\sqrt{\\pi^2+1}}$. Our first computational task is to calculate the sample correlation $\\widehat{\\mathrm{Corr}}(x, u)$ to confirm the presence of this endogeneity in the simulated data.\n\nThe core of the IV method is to isolate the variation in $x$ that is attributable only to the exogenous instrument $Z$, thereby creating a new variable that is uncorrelated with the structural error $u$. This is achieved by orthogonally projecting the vector $x$ onto the linear subspace spanned by the instruments. In this problem, the instruments are a constant and the variable $Z$. We construct the $n \\times 2$ matrix of instruments $X = [\\mathbf{1} \\quad Z]$, where $\\mathbf{1}$ is an $n \\times 1$ vector of ones.\n\nThe projection of $x$ onto the column space of $X$ is given by $\\hat{x} = P x$, where $P = X(X^\\top X)^{-1}X^\\top$ is the projection matrix. Computationally, $\\hat{x}$ are the fitted values from an OLS regression of $x$ on $X$. That is, we solve for the coefficients $\\hat{\\gamma}$ in the linear model $x = X\\gamma + \\text{residual}$, which yields $\\hat{\\gamma} = (X^\\top X)^{-1}X^\\top x$. Then, the projected vector is $\\hat{x} = X\\hat{\\gamma}$. A numerically stable approach, which we will employ, is to solve for $\\hat{\\gamma}$ using a least-squares algorithm (e.g., based on QR decomposition) rather than by explicitly computing the matrix inverse.\n\nThe second computational task is to calculate $\\widehat{\\mathrm{Corr}}(\\hat{x}, u)$. By construction, $\\hat{x}$ is a linear combination of the columns of $X$, specifically $\\hat{x} = \\hat{\\gamma}_0 \\mathbf{1} + \\hat{\\gamma}_1 Z$. Since $Z$ is exogenous ($\\mathrm{Cov}(Z, u) = 0$) and the constant is trivially exogenous, any linear combination of them must also be uncorrelated with $u$ in the population. Therefore, we expect the sample correlation $\\widehat{\\mathrm{Corr}}(\\hat{x}, u)$ to be close to $0$, reflecting the successful \"purging\" of endogeneity from the regressor. The simulation will demonstrate this property across various parameterizations.\n\nThe algorithm proceeds as follows for each test case $(n, \\beta_0, \\beta_1, \\pi, \\rho, s)$:\n$1$. Initialize a pseudorandom number generator with the given seed $s$.\n$2$. Generate $n$ samples for $Z$, $u$, and $v$ according to the specified distributions.\n$3$. Construct the vector $x$ using $x_i = \\pi Z_i + v_i$.\n$4$. Compute and store $\\widehat{\\mathrm{Corr}}(x, u)$ using the standard sample correlation formula.\n$5$. Construct the matrix $X = [\\mathbf{1} \\quad Z]$.\n$6$. Solve the least-squares problem for $\\hat{\\gamma}$ in $x \\approx X\\gamma$.\n$7$. Compute the projected vector $\\hat{x} = X\\hat{\\gamma}$.\n$8$. Compute and store $\\widehat{\\mathrm{Corr}}(\\hat{x}, u)$.\n$9$. Round the two correlation values to $6$ decimal places as required.\nThe results from all test cases are then aggregated into the specified final output format.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the instrumental variables simulation problem for a given set of test cases.\n    \"\"\"\n    test_cases = [\n        # (n, beta_0, beta_1, pi, rho, s)\n        (2000, 1.0, 1.5, 1.0, 0.7, 12345),\n        (2000, 0.0, 2.0, 0.2, 0.7, 54321),\n        (2000, 0.5, 1.0, 0.8, 0.0, 202311),\n        (50, -0.5, 1.2, 0.9, 0.8, 777),\n    ]\n\n    all_results = []\n\n    for n, beta_0, beta_1, pi, rho, s in test_cases:\n        # 1. Set up the pseudorandom number generator for reproducibility.\n        rng = np.random.default_rng(s)\n\n        # 2. Generate data according to the Data-Generating Process (DGP).\n        # Draw the exogenous instrument Z.\n        Z = rng.normal(loc=0.0, scale=1.0, size=n)\n\n        # Draw the correlated error components (u, v).\n        mean_uv = [0, 0]\n        cov_uv = [[1, rho], [rho, 1]]\n        uv_draws = rng.multivariate_normal(mean_uv, cov_uv, size=n)\n        u = uv_draws[:, 0]\n        v = uv_draws[:, 1]\n\n        # Construct the endogenous regressor x.\n        x = pi * Z + v\n        \n        # Note: y is not needed for the requested calculations.\n        # y = beta_0 + beta_1 * x + u\n\n        # 3. Compute the sample correlation between original x and the error u.\n        # This demonstrates the endogeneity.\n        corr_xu = np.corrcoef(x, u)[0, 1]\n\n        # 4. Compute the orthogonal projection of x onto the space of instruments.\n        # The instruments are a constant and Z.\n        X_mat = np.c_[np.ones(n), Z]\n        \n        # Find the coefficients of the projection (first-stage regression).\n        # np.linalg.lstsq is a numerically stable way to solve this.\n        gamma_hat = np.linalg.lstsq(X_mat, x, rcond=None)[0]\n        \n        # Compute the projected vector x_hat.\n        x_hat = X_mat @ gamma_hat\n\n        # 5. Compute the sample correlation between the projected x_hat and the error u.\n        # This should be close to zero, demonstrating the \"purging\" of endogeneity.\n        corr_xhat_u = np.corrcoef(x_hat, u)[0, 1] if np.std(x_hat) > 1e-9 else 0.0 # handle case of zero variance if pi=0\n\n        # 6. Store the rounded results for this case.\n        result_pair = [round(corr_xu, 6), round(corr_xhat_u, 6)]\n        all_results.append(result_pair)\n\n    # 7. Print the final results in the specified format.\n    # The format is a string representation of a list of lists with no spaces.\n    print(str(all_results).replace(\" \", \"\"))\n\nsolve()\n```", "id": "2402330"}, {"introduction": "A valid instrument must be both exogenous and relevant. While exogeneity is an assumption that is difficult to test, relevance can and must be measured, as its weakness can have severe consequences for our analysis. This practice uses Monte Carlo simulation to explore the problem of \"weak instruments,\" demonstrating how a low correlation between the instrument and the endogenous regressor leads to low statistical power, making it difficult to detect true causal effects [@problem_id:2402339]. Understanding this pitfall and learning to diagnose it with tools like the F-statistic is essential for any applied researcher using IV methods.", "problem": "Construct a program that uses Monte Carlo simulation to quantify how weak instruments affect the empirical power of a test of the null hypothesis $H_0: \\beta=0$ in a linear instrumental variables setting. Consider the following data-generating process for a single endogenous regressor and a single instrument, with no intercepts:\n- For each repetition and for each observation index $i \\in \\{1,\\dots,n\\}$, draw $z_i \\sim \\mathcal{N}(0,1)$.\n- Draw independent standard normal shocks $e_{1i} \\sim \\mathcal{N}(0,1)$ and $e_{2i} \\sim \\mathcal{N}(0,1)$, and construct the pair $(v_i,u_i)$ with correlation $\\rho$ by setting $v_i = e_{1i}$ and $u_i = \\rho\\,e_{1i} + \\sqrt{1-\\rho^2}\\,e_{2i}$.\n- Set $x_i = \\pi\\,z_i + v_i$ and $y_i = \\beta\\,x_i + u_i$.\n\nFor each simulated dataset of size $n$, compute:\n1. The just-identified instrumental variables estimator for $\\beta$ defined by the sample moment condition $\\frac{1}{n}\\sum_{i=1}^n z_i(y_i-\\beta x_i)=0$, namely\n$$\n\\hat{\\beta} = \\frac{\\sum_{i=1}^n z_i y_i}{\\sum_{i=1}^n z_i x_i}.\n$$\n2. The associated conventional large-sample standard error based on the scalar moment condition,\n$$\n\\widehat{V}(\\hat{\\beta}) \\;=\\; \\frac{\\widehat{S}}{n\\,\\widehat{Q}^2},\\quad \\widehat{S}=\\frac{1}{n}\\sum_{i=1}^n (z_i\\hat{u}_i)^2,\\quad \\widehat{Q}=\\frac{1}{n}\\sum_{i=1}^n z_i x_i,\\quad \\hat{u}_i = y_i - \\hat{\\beta} x_i,\n$$\nand the corresponding test statistic for $H_0: \\beta=0$,\n$$\nT \\;=\\; \\frac{\\hat{\\beta}-0}{\\sqrt{\\widehat{V}(\\hat{\\beta})}}.\n$$\nUse a two-sided rejection rule based on the standard normal quantile $c_\\alpha$, rejecting $H_0$ if $|T|>c_\\alpha$, where $c_\\alpha$ is such that $\\Pr(|Z|>c_\\alpha)=\\alpha$ for $Z\\sim \\mathcal{N}(0,1)$.\n\n3. The first-stage $F$-statistic for the relevance of the instrument in the regression of $x_i$ on $z_i$ (no intercept), computed from the ordinary least squares slope $\\hat{a} = \\frac{\\sum_{i=1}^n z_i x_i}{\\sum_{i=1}^n z_i^2}$, with residuals $r_i = x_i - \\hat{a} z_i$, residual variance $\\hat{\\sigma}^2 = \\frac{1}{n-1}\\sum_{i=1}^n r_i^2$, standard error $\\operatorname{se}(\\hat{a})=\\sqrt{\\hat{\\sigma}^2 / \\sum_{i=1}^n z_i^2}$, the corresponding $t$-statistic $t_1=\\hat{a}/\\operatorname{se}(\\hat{a})$, and $F = t_1^2$.\n\nFor each parameter set below, repeat the above steps for $R$ independent repetitions and report:\n- The empirical power defined as the fraction of repetitions in which the null is rejected at significance level $\\alpha$ when the true $\\beta$ is not equal to $0$.\n- The average first-stage $F$-statistic across the $R$ repetitions.\n\nUse the following test suite of parameter values, each specified as a tuple $(n,\\pi,\\rho,\\beta,\\alpha,R,\\text{seed})$:\n- Case A (strong instrument, larger sample): $(500,\\,0.5,\\,0.6,\\,1.0,\\,0.05,\\,1000,\\,123)$.\n- Case B (weak instrument, larger sample): $(500,\\,0.05,\\,0.6,\\,1.0,\\,0.05,\\,1000,\\,456)$.\n- Case C (moderate instrument, smaller sample): $(100,\\,0.35,\\,0.6,\\,1.0,\\,0.05,\\,1000,\\,789)$.\n\nYour program must:\n- Implement the simulation exactly as described.\n- For each case, compute the empirical power and the mean first-stage $F$-statistic across the $R$ repetitions.\n- Round each reported value to three decimal places.\n- Produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case’s result must itself be a two-element list in the order $[\\text{power}, \\text{average F}]$. For example, a valid output with three cases would look like $[[0.842,12.531],[0.121,1.472],[0.563,6.214]]$.\n\nThere are no physical units involved. All numeric answers must be pure numbers. Angles are not involved. Percentages must be expressed as decimals, not with a percentage sign.", "solution": "The problem statement is validated as scientifically sound, well-posed, and objective. It presents a standard exercise in computational econometrics to analyze the consequences of weak instruments on the statistical power of hypothesis tests. The solution is constructed via a Monte Carlo simulation as specified.\n\nThe core of the simulation is a data-generating process (DGP) for a linear model with one endogenous regressor $x_i$ and one instrumental variable $z_i$. The model is defined by two equations:\nThe first-stage equation, $x_i = \\pi z_i + v_i$, links the endogenous regressor $x_i$ to the instrument $z_i$. The parameter $\\pi$ governs the strength of the instrument. A value of $\\pi$ close to zero signifies a \"weak instrument,\" meaning $z_i$ has little explanatory power for $x_i$.\nThe structural equation, $y_i = \\beta x_i + u_i$, defines the outcome variable $y_i$. Endogeneity arises because the error terms $v_i$ and $u_i$ are correlated, with $\\text{Corr}(v_i, u_i) = \\rho$. This correlation is explicitly constructed from independent standard normal shocks $e_{1i} \\sim \\mathcal{N}(0,1)$ and $e_{2i} \\sim \\mathcal{N}(0,1)$ by setting $v_i = e_{1i}$ and $u_i = \\rho e_{1i} + \\sqrt{1-\\rho^2} e_{2i}$. This correlation violates the ordinary least squares assumption that the regressor is uncorrelated with the error term, necessitating the use of an instrumental variable estimator.\n\nFor each of the $R$ repetitions in a simulation run, a dataset of size $n$ is generated according to this DGP. Then, the following quantities are computed:\n\n1.  **Instrumental Variables Estimator**: The just-identified IV estimator for $\\beta$ is computed from the sample analogue of the moment condition $E[z_i(y_i - \\beta x_i)]=0$. This yields the expression $\\hat{\\beta} = \\left(\\sum_{i=1}^n z_i y_i\\right) / \\left(\\sum_{i=1}^n z_i x_i\\right)$. This estimator is consistent for $\\beta$ provided the instrument is valid, i.e., relevant ($\\pi \\neq 0$) and exogenous ($E[z_i u_i] = 0$, which holds by construction).\n\n2.  **Hypothesis Test**: A $t$-test for the null hypothesis $H_0: \\beta=0$ is performed. The test statistic is $T = (\\hat{\\beta} - 0) / \\sqrt{\\widehat{V}(\\hat{\\beta})}$, where $\\widehat{V}(\\hat{\\beta})$ is the estimated variance of $\\hat{\\beta}$. The variance formula provided, $\\widehat{V}(\\hat{\\beta}) = \\widehat{S} / (n \\widehat{Q}^2)$ with $\\widehat{S}=\\frac{1}{n}\\sum_{i=1}^n (z_i\\hat{u}_i)^2$ and $\\widehat{Q}=\\frac{1}{n}\\sum_{i=1}^n z_i x_i$, is a heteroskedasticity-robust form. The null hypothesis is rejected at significance level $\\alpha$ if the absolute value of the test statistic, $|T|$, exceeds the critical value $c_\\alpha$ from the standard normal distribution, where $c_\\alpha$ is defined by $\\Pr(|Z| > c_\\alpha) = \\alpha$ for $Z \\sim \\mathcal{N}(0,1)$. The empirical power of the test is the fraction of the $R$ repetitions in which $H_0$ is correctly rejected, given that the true value in the simulation is $\\beta \\neq 0$.\n\n3.  **First-Stage F-Statistic**: As a diagnostic for instrument strength, the $F$-statistic from the first-stage regression of $x_i$ on $z_i$ is calculated. Because there is only one instrument, this is equivalent to the square of the $t$-statistic for the coefficient on $z_i$, $\\hat{a}$. This $t$-statistic is $t_1 = \\hat{a} / \\operatorname{se}(\\hat{a})$. A low average $F$-statistic (commonly, a value below $10$ is used as a rule of thumb) is a widely used indicator of weak instruments. Such instruments are known to cause a number of problems, including biased IV estimators in finite samples and distorted test sizes.\n\nThe program implements this simulation for three distinct parameter sets. These cases are designed to highlight different scenarios: a strong instrument (Case A: $\\pi=0.5$), a weak instrument (Case B: $\\pi=0.05$), and a moderately strong instrument with a smaller sample size (Case C: $n=100$, $\\pi=0.35$). The final output reports the calculated empirical power and the average first-stage $F$-statistic for each case, providing a quantitative illustration of how instrument strength and sample size affect the reliability of statistical inference in IV models.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef run_simulation(n, pi, rho, beta, alpha, R, seed):\n    \"\"\"\n    Runs a Monte Carlo simulation for the instrumental variables model.\n\n    Args:\n        n (int): Sample size.\n        pi (float): Instrument strength parameter.\n        rho (float): Correlation between structural and first-stage errors.\n        beta (float): True coefficient of the endogenous regressor.\n        alpha (float): Significance level for the hypothesis test.\n        R (int): Number of repetitions.\n        seed (int): Seed for the random number generator.\n\n    Returns:\n        tuple: A tuple containing the empirical power and the average F-statistic.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    rejection_count = 0\n    total_f_stat = 0.0\n\n    # Calculate the two-sided critical value from the standard normal distribution.\n    c_alpha = norm.ppf(1 - alpha / 2)\n\n    for _ in range(R):\n        # Step 1: Generate data according to the DGP\n        z = rng.normal(loc=0, scale=1, size=n)\n        e1 = rng.normal(loc=0, scale=1, size=n)\n        e2 = rng.normal(loc=0, scale=1, size=n)\n\n        v = e1\n        u = rho * e1 + np.sqrt(1 - rho**2) * e2\n        x = pi * z + v\n        y = beta * x + u\n\n        # Step 2: Compute the IV estimator for beta\n        sum_zy = np.dot(z, y)\n        sum_zx = np.dot(z, x)\n        \n        # Avoid division by zero, though highly unlikely with continuous variables\n        if sum_zx == 0:\n            continue\n            \n        beta_hat = sum_zy / sum_zx\n\n        # Step 3: Compute the t-statistic for H_0: beta = 0\n        u_hat = y - beta_hat * x\n        Q_hat = sum_zx / n\n        S_hat = np.mean((z * u_hat)**2)\n        \n        # Denominator of variance estimator\n        var_denom = n * Q_hat**2\n        if var_denom == 0:\n            continue\n\n        V_hat_beta_hat = S_hat / var_denom\n        \n        # Ensure variance is non-negative before taking square root\n        if V_hat_beta_hat  0:\n            continue\n        \n        se_beta_hat = np.sqrt(V_hat_beta_hat)\n        \n        if se_beta_hat == 0:\n            continue\n            \n        t_stat = beta_hat / se_beta_hat\n\n        # Step 4: Perform the hypothesis test\n        if np.abs(t_stat) > c_alpha:\n            rejection_count += 1\n\n        # Step 5: Compute the first-stage F-statistic\n        sum_zz = np.dot(z, z)\n        \n        if sum_zz == 0:\n            continue\n\n        a_hat = sum_zx / sum_zz\n        r = x - a_hat * z\n        sigma2_hat = np.sum(r**2) / (n - 1)\n        se_a_hat = np.sqrt(sigma2_hat / sum_zz)\n        \n        if se_a_hat == 0:\n            continue\n\n        t1_stat = a_hat / se_a_hat\n        f_stat = t1_stat**2\n        total_f_stat += f_stat\n\n    empirical_power = rejection_count / R\n    average_f_stat = total_f_stat / R\n\n    return empirical_power, average_f_stat\n\ndef solve():\n    \"\"\"\n    Main function to run simulations for all test cases and print results.\n    \"\"\"\n    test_cases = [\n        # (n, pi, rho, beta, alpha, R, seed)\n        (500, 0.5, 0.6, 1.0, 0.05, 1000, 123),  # Case A\n        (500, 0.05, 0.6, 1.0, 0.05, 1000, 456), # Case B\n        (100, 0.35, 0.6, 1.0, 0.05, 1000, 789), # Case C\n    ]\n\n    results = []\n    for case in test_cases:\n        power, avg_f = run_simulation(*case)\n        # Format each result pair to 3 decimal places\n        results.append([round(power, 3), round(avg_f, 3)])\n\n    # Format the final output string as a list of lists with no spaces\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n```", "id": "2402339"}]}