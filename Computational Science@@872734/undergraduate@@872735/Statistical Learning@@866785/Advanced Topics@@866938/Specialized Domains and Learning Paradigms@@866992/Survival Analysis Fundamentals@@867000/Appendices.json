{"hands_on_practices": [{"introduction": "A defining feature of survival analysis is censoring, which occurs when we have partial information about an individual's event time. While censoring allows us to include all subjects in the analysis, it comes at the cost of statistical precision. This exercise will help you build intuition for how the amount of censoring directly impacts the reliability of your survival estimates by comparing the variance of the Kaplan-Meier estimator under two different censoring scenarios [@problem_id:3179136].", "problem": "A cohort study follows $n=100$ independent subjects from baseline. Consider estimating the survival function at $t=9$ months using the nonparametric Kaplanâ€“Meier (KM) estimator. In both study designs below, there are exactly three distinct event times prior to $t=9$ months at $t_1=3$, $t_2=6$, and $t_3=9$ months with event counts $d_1=5$, $d_2=4$, and $d_3=3$, respectively. No ties between events and censorings occur exactly at the same recorded time. The designs differ only in how many subjects are censored between event times, as follows:\n\n- Design L (lower censoring): between $0$ and $3$ months, $c_{(0,3)}=0$ are censored; between $3$ and $6$ months, $c_{(3,6)}=2$ are censored; between $6$ and $9$ months, $c_{(6,9)}=3$ are censored. There are no other losses before $t=9$ months.\n- Design H (higher censoring): between $0$ and $3$ months, $c_{(0,3)}=10$ are censored; between $3$ and $6$ months, $c_{(3,6)}=10$ are censored; between $6$ and $9$ months, $c_{(6,9)}=10$ are censored. There are no other losses before $t=9$ months.\n\nUsing only fundamental definitions of the KM estimator, risk sets, and well-tested large-sample variance behavior for the KM estimator at a fixed time, decide which statement best characterizes how the variance at $t=9$ months compares between Design H and Design L. Choose the single best answer.\n\nA. The variance at $t=9$ months is smaller in Design H because fewer individuals contribute information, leading to reduced variability.\n\nB. The variance at $t=9$ months is larger in Design H because increased censoring before $t=9$ months reduces the risk set sizes at the event times, inflating the variability of the product-limit estimator.\n\nC. The variance at $t=9$ months is the same in both designs because the KM estimator and its variability depend only on the event counts and event times, not on censoring.\n\nD. The variance at $t=9$ months is undefined in Design H because heavy censoring invalidates nonparametric survival estimation before $t=9$ months.", "solution": "The user wants me to validate a problem statement concerning the comparison of variances for the Kaplan-Meier estimator under two different censoring scenarios.\n\n### Step 1: Extract Givens\n\n-   Total subjects at baseline: $n=100$, independent.\n-   Task: Estimate the survival function $S(t)$ at $t=9$ months using the Kaplan-Meier (KM) estimator.\n-   Distinct event times: $t_1=3$ months, $t_2=6$ months, $t_3=9$ months.\n-   Event counts: $d_1=5$ at $t_1$, $d_2=4$ at $t_2$, $d_3=3$ at $t_3$.\n-   No ties between events and censorings at the same recorded time.\n-   Design L (lower censoring):\n    -   Number of subjects censored in $(0, 3)$ months: $c_{(0,3)}=0$.\n    -   Number of subjects censored in $(3, 6)$ months: $c_{(3,6)}=2$.\n    -   Number of subjects censored in $(6, 9)$ months: $c_{(6,9)}=3$.\n-   Design H (higher censoring):\n    -   Number of subjects censored in $(0, 3)$ months: $c_{(0,3)}=10$.\n    -   Number of subjects censored in $(3, 6)$ months: $c_{(3,6)}=10$.\n    -   Number of subjects censored in $(6, 9)$ months: $c_{(6,9)}=10$.\n-   The question asks to compare the variance of the KM estimator at $t=9$ months, $\\text{Var}(\\hat{S}(9))$, between Design H and Design L.\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem statement must be evaluated for scientific grounding, being well-posed, and objectivity.\n\n1.  **Scientific or Factual Unsoundness**: The problem is grounded in the established statistical theory of survival analysis. The Kaplan-Meier estimator, risk sets, censoring, and variance estimation via Greenwood's formula are standard topics in this field. The setup describes a plausible cohort study. No scientific or factual unsoundness is detected.\n2.  **Non-Formalizable or Irrelevant**: The problem is entirely formalizable within the framework of survival analysis and is directly relevant to the topic of *survival analysis fundamentals*.\n3.  **Incomplete or Contradictory Setup**: The problem provides all necessary data to compute the quantities of interest. Let's verify the consistency of the numbers.\n    -   For Design L, the number of individuals at risk, $n_i$, at each event time $t_i$:\n        -   $n_{1,L} = 100 - c_{(0,3)} = 100 - 0 = 100$.\n        -   $n_{2,L} = (n_{1,L} - d_1) - c_{(3,6)} = (100 - 5) - 2 = 93$.\n        -   $n_{3,L} = (n_{2,L} - d_2) - c_{(6,9)} = (93 - 4) - 3 = 86$.\n    -   For Design H, the number of individuals at risk, $n_i$, at each event time $t_i$:\n        -   $n_{1,H} = 100 - c_{(0,3)} = 100 - 10 = 90$.\n        -   $n_{2,H} = (n_{1,H} - d_1) - c_{(3,6)} = (90 - 5) - 10 = 75$.\n        -   $n_{3,H} = (n_{2,H} - d_2) - c_{(6,9)} = (75 - 4) - 10 = 61$.\n    In both designs, the number at risk ($n_i$) is always greater than the number of events ($d_i$) at each time point, so the calculations are well-defined. The setup is complete and internally consistent.\n4.  **Unrealistic or Infeasible**: The numbers are plausible for a medical or engineering reliability study. There are no physical or scientific impossibilities.\n5.  **Ill-Posed or Poorly Structured**: The question is unambiguous and asks for a comparison of variances, which is a well-defined statistical quantity. A unique, stable, and meaningful solution can be derived.\n6.  **Pseudo-Profound, Trivial, or Tautological**: The problem tests a core conceptual point about the effect of censoring on the precision of a survival estimate. It is not trivial, as it requires understanding the structure of the variance formula for the KM estimator.\n7.  **Outside Scientific Verifiability**: The claims are mathematically verifiable.\n\n### Step 3: Verdict and Action\n\nThe problem statement is **valid**. Proceeding to the solution.\n\n### Derivation\n\nThe Kaplan-Meier (KM) estimator of the survival function $S(t)$ is given by the product-limit formula:\n$$ \\hat{S}(t) = \\prod_{i: t_i \\le t} \\left( 1 - \\frac{d_i}{n_i} \\right) $$\nwhere $t_i$ are the distinct event times, $d_i$ is the number of events at $t_i$, and $n_i$ is the number of individuals in the risk set (i.e., under observation and not having had the event) just prior to $t_i$.\n\nThe question concerns the variance of the KM estimator at $t=9$ months. The most commonly used formula for the large-sample approximate variance of $\\hat{S}(t)$ is Greenwood's formula:\n$$ \\widehat{\\text{Var}}(\\hat{S}(t)) \\approx [\\hat{S}(t)]^2 \\sum_{i: t_i \\le t} \\frac{d_i}{n_i(n_i - d_i)} $$\nWe need to compare $\\widehat{\\text{Var}}(\\hat{S}_L(9))$ from Design L with $\\widehat{\\text{Var}}(\\hat{S}_H(9))$ from Design H. The summation will be over the three event times $t_1=3$, $t_2=6$, and $t_3=9$.\n\nLet's compute the risk set sizes, $n_i$, for both designs.\n\n**For Design L (lower censoring):**\n-   Risk set at $t_1=3$: $n_{1,L} = 100$.\n-   Number of survivors after $t_1$: $100 - 5 = 95$.\n-   Number censored in $(3,6)$: $c_{(3,6)} = 2$.\n-   Risk set at $t_2=6$: $n_{2,L} = 95 - 2 = 93$.\n-   Number of survivors after $t_2$: $93 - 4 = 89$.\n-   Number censored in $(6,9)$: $c_{(6,9)} = 3$.\n-   Risk set at $t_3=9$: $n_{3,L} = 89 - 3 = 86$.\n\n**For Design H (higher censoring):**\n-   Risk set at $t_1=3$: $n_{1,H} = 100 - 10 = 90$.\n-   Number of survivors after $t_1$: $90 - 5 = 85$.\n-   Number censored in $(3,6)$: $c_{(3,6)} = 10$.\n-   Risk set at $t_2=6$: $n_{2,H} = 85 - 10 = 75$.\n-   Number of survivors after $t_2$: $75 - 4 = 71$.\n-   Number censored in $(6,9)$: $c_{(6,9)} = 10$.\n-   Risk set at $t_3=9$: $n_{3,H} = 71 - 10 = 61$.\n\nBy direct comparison, for each event time $i \\in \\{1, 2, 3\\}$, the risk set size in Design H is smaller than in Design L:\n-   $n_{1,H} = 90  n_{1,L} = 100$\n-   $n_{2,H} = 75  n_{2,L} = 93$\n-   $n_{3,H} = 61  n_{3,L} = 86$\n\nNow, let's analyze the effect on the variance. The variance formula consists of two parts: the squared survival estimate $[\\hat{S}(9)]^2$ and the summation term $\\sum_{i=1}^3 \\frac{d_i}{n_i(n_i-d_i)}$.\n\n1.  **Analysis of the summation term:** The terms in the sum are of the form $\\frac{d_i}{n_i(n_i-d_i)}$. Since $d_i$ is constant for both designs at each time $i$, and the function $f(x) = \\frac{k}{x(x-k)}$ (for fixed $k0$) is a decreasing function for $xk$, a smaller value of $n_i$ results in a larger value for the term.\n    Since $n_{i,H}  n_{i,L}$ for all $i=1, 2, 3$, it follows that:\n    $$ \\frac{d_i}{n_{i,H}(n_{i,H} - d_i)}  \\frac{d_i}{n_{i,L}(n_{i,L} - d_i)} \\quad \\text{for each } i. $$\n    Therefore, the entire sum is larger for Design H:\n    $$ \\sum_{i=1}^3 \\frac{d_i}{n_{i,H}(n_{i,H} - d_i)}  \\sum_{i=1}^3 \\frac{d_i}{n_{i,L}(n_{i,L} - d_i)} $$\n\n2.  **Analysis of the survival estimate:** The survival estimate $\\hat{S}(9)$ is a product of terms $(1 - d_i/n_i)$. Since $n_{i,H}  n_{i,L}$, the hazard estimate $d_i/n_{i,H}  d_i/n_{i,L}$, which means the survival factor $(1 - d_i/n_{i,H})  (1 - d_i/n_{i,L})$ for each $i$. Consequently, the product is smaller for Design H: $\\hat{S}_H(9)  \\hat{S}_L(9)$. This means the pre-factor $[\\hat{S}_H(9)]^2$ is smaller than $[\\hat{S}_L(9)]^2$.\n\nWhile these two effects (the sum and the pre-factor) go in opposite directions, the change in the summation term is generally dominant. The core principle being tested is that censoring reduces the amount of information available for estimation, which intuitively must lead to higher variance (lower precision). The smaller risk sets directly reflect this loss of information. Let's confirm with a calculation.\n\nFor Design L: $\\hat{S}_L(9) = (1 - \\frac{5}{100})(1 - \\frac{4}{93})(1 - \\frac{3}{86}) \\approx 0.8770$.\nSum term for L: $\\frac{5}{100(95)} + \\frac{4}{93(89)} + \\frac{3}{86(83)} \\approx 0.000526 + 0.000483 + 0.000420 = 0.001429$.\n$\\widehat{\\text{Var}}(\\hat{S}_L(9)) \\approx (0.8770)^2 \\times 0.001429 \\approx 0.7691 \\times 0.001429 \\approx 0.001099$.\n\nFor Design H: $\\hat{S}_H(9) = (1 - \\frac{5}{90})(1 - \\frac{4}{75})(1 - \\frac{3}{61}) \\approx 0.8494$.\nSum term for H: $\\frac{5}{90(85)} + \\frac{4}{75(71)} + \\frac{3}{61(58)} \\approx 0.000654 + 0.000751 + 0.000848 = 0.002253$.\n$\\widehat{\\text{Var}}(\\hat{S}_H(9)) \\approx (0.8494)^2 \\times 0.002253 \\approx 0.7215 \\times 0.002253 \\approx 0.001626$.\n\nThe calculation confirms that $\\widehat{\\text{Var}}(\\hat{S}_H(9))  \\widehat{\\text{Var}}(\\hat{S}_L(9))$. The variance is larger in Design H. This is because the reduction in the size of the risk sets ($n_i$) due to heavier censoring increases the individual variance contributions $\\frac{d_i}{n_i(n_i-d_i)}$ more significantly than it reduces the survival estimate pre-factor $[\\hat{S}(9)]^2$.\n\n### Option-by-Option Analysis\n\nA. The variance at $t=9$ months is smaller in Design H because fewer individuals contribute information, leading to reduced variability.\nThis statement is incorrect on two grounds. First, our calculation shows the variance is larger, not smaller, in Design H. Second, the reasoning is flawed; having fewer individuals contribute information (i.e., smaller effective sample size) leads to *increased*, not reduced, variability.\n**Verdict: Incorrect.**\n\nB. The variance at $t=9$ months is larger in Design H because increased censoring before $t=9$ months reduces the risk set sizes at the event times, inflating the variability of the product-limit estimator.\nThis statement correctly claims that the variance is larger in Design H. The reasoning provided is precisely the mechanism driving this result: increased censoring leads to smaller risk sets ($n_i$), which in turn increases the terms in Greenwood's formula, thereby \"inflating the variability\". This aligns perfectly with our derivation.\n**Verdict: Correct.**\n\nC. The variance at $t=9$ months is the same in both designs because the KM estimator and its variability depend only on the event counts and event times, not on censoring.\nThis statement is incorrect. Our calculation shows the variances are different. The reasoning is fundamentally wrong. Both the KM estimate $\\hat{S}(t)$ and its variance explicitly depend on the risk set sizes $n_i$, which are directly affected by censoring patterns.\n**Verdict: Incorrect.**\n\nD. The variance at $t=9$ months is undefined in Design H because heavy censoring invalidates nonparametric survival estimation before $t=9$ months.\nThis statement is incorrect. The KM estimator and its variance are well-defined as long as the risk set does not become zero. In Design H, the risk set at $t=9$ months is $n_{3,H} = 61$, which is well above zero. The censoring in Design H is not heavy enough to cause the estimator to be undefined.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{B}$$", "id": "3179136"}, {"introduction": "Comparing survival between two groups is a central task in many fields. While the hazard ratio is a common metric, it can be misleading if the proportional hazards assumption is violated. This exercise introduces a robust alternative: the Restricted Mean Survival Time (RMST), which summarizes the average event-free time up to a specified horizon $\\tau$. You will gain practical experience by calculating the RMST directly from estimated survival curves, a fundamental skill for interpreting modern clinical trials and reliability studies [@problem_id:3179108].", "problem": "You are given two treatment arms, A and B, each with an estimated survival curve represented as a right-continuous, nonincreasing, piecewise-constant step function on a finite grid of times. The aim is to compute the restricted mean survival time (RMST) up to a finite time horizon and then compare the treatment arms in a way that remains valid under non-proportional hazards (NPH). The comparison should be made via the difference in restricted mean survival time at the horizon, which summarizes the overall average survival up to that time without relying on the proportional hazards assumption.\n\nFundamental base:\n- The survival function is defined as $S(t)=\\mathbb{P}(Tt)$ for a nonnegative time-to-event random variable $T$.\n- The restricted mean survival time up to a finite horizon $\\tau$ is defined as the area under the survival curve from $t=0$ to $\\tau$.\n- For a right-continuous step function $S(t)$ with a finite grid $0=g_0  g_1  \\dots  g_m$, the function is constant on each interval $[g_k,g_{k+1})$ and takes the value given at $g_k$. If $\\tau$ lies strictly inside an interval, the last interval is truncated at $\\tau$.\n\nProgram input specification:\n- The program should internally store a list of test cases. Each test case consists of:\n  - A strictly increasing time grid $[g_0,g_1,\\dots,g_m]$ with $g_0=0$ and $g_m \\ge \\tau$.\n  - Two lists of survival values, one for arm A and one for arm B, each of length $m+1$. The value at index $k$ is the right-continuous survival level on $[g_k,g_{k+1})$.\n  - A finite horizon $\\tau$ with $0 \\le \\tau \\le g_m$.\n- All survival values are in $[0,1]$ and are nonincreasing in the grid index.\n\nTask:\n- For each test case, compute the restricted mean survival time for arm A and for arm B by integrating the provided step functions from $t=0$ up to $\\tau$ using the piecewise-constant interpretation described above.\n- Compute the difference $\\Delta = \\mathrm{RMST}_B - \\mathrm{RMST}_A$.\n- Report a preference indicator $I$ defined as:\n  - $I=1$ if $\\Delta > 0$,\n  - $I=-1$ if $\\Delta  0$,\n  - $I=0$ if $\\Delta = 0$,\n  using a numerical tolerance of $10^{-12}$ to decide equality.\n\nRequired numerical output format:\n- For each test case, output a list of four values: $[\\mathrm{RMST}_A,\\mathrm{RMST}_B,\\Delta,I]$.\n- The first three values must be floating-point numbers rounded to six decimal places. The last value must be an integer.\n- The program must produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, where each element is the bracketed four-value list for a test case. For example: $[[a_1,b_1,c_1,d_1],[a_2,b_2,c_2,d_2],\\dots]$.\n\nTest suite to implement inside the program:\n- Case 1 (non-proportional hazards with crossing survival and interior truncation):\n  - Grid: $[0.0,2.0,5.0,8.0,12.0]$\n  - Arm A: $[1.0,0.92,0.80,0.74,0.70]$\n  - Arm B: $[1.0,0.96,0.78,0.77,0.72]$\n  - Horizon: $\\tau=10.0$\n- Case 2 (horizon equals last grid point):\n  - Grid: $[0.0,3.0,6.0,9.0,12.0]$\n  - Arm A: $[1.0,0.90,0.82,0.70,0.60]$\n  - Arm B: $[1.0,0.95,0.75,0.72,0.65]$\n  - Horizon: $\\tau=12.0$\n- Case 3 (small horizon inside the first interval, tie expected):\n  - Grid: $[0.0,4.0,9.0]$\n  - Arm A: $[1.0,0.85,0.70]$\n  - Arm B: $[1.0,0.88,0.68]$\n  - Horizon: $\\tau=2.0$\n- Case 4 (multiple crossings and horizon on a grid point):\n  - Grid: $[0.0,1.0,2.5,4.0,7.5,10.0]$\n  - Arm A: $[1.0,0.97,0.90,0.86,0.80,0.78]$\n  - Arm B: $[1.0,0.99,0.91,0.85,0.79,0.76]$\n  - Horizon: $\\tau=7.5$\n\nYour program should compute the four requested values for each case and print them in the exact required format in a single line as described above. No user input is involved; all data are embedded in the program.", "solution": "The posed problem is valid. It is scientifically grounded in the principles of survival analysis, a branch of statistics, and is well-posed with a clear, objective, and complete specification. All data and definitions required for a unique solution are provided.\n\nThe primary objective is to compute the Restricted Mean Survival Time (RMST) for two treatment arms, A and B, up to a specified time horizon $\\tau$. The RMST is a robust measure for comparing survival distributions, particularly when the proportional hazards assumption is not met. The survival function for each arm, $S(t)$, is provided as a right-continuous, piecewise-constant step function.\n\nThe RMST up to a time $\\tau  0$ is defined as the total area under the survival curve from time $t=0$ to $t=\\tau$:\n$$\n\\mathrm{RMST}(\\tau) = \\int_{0}^{\\tau} S(t) dt\n$$\nThe survival functions, $S_A(t)$ and $S_B(t)$, are defined over a finite, strictly increasing time grid $G = \\{g_0, g_1, \\dots, g_m\\}$, where $g_0=0$. For each arm, a corresponding list of survival probabilities $s = \\{s_0, s_1, \\dots, s_m\\}$ is given. The problem specifies that the survival function $S(t)$ is constant on each interval $[g_k, g_{k+1})$, taking the value $s_k$. That is:\n$$\nS(t) = s_k \\quad \\text{for } t \\in [g_k, g_{k+1})\n$$\nThe integration to compute $\\mathrm{RMST}(\\tau)$ can be performed by summing the areas of the rectangles formed by the step function over the interval $[0, \\tau]$. Let the grid points be $g_0, g_1, \\dots, g_m$. We iterate through the intervals defined by these grid points.\n\nFor each interval $[g_k, g_{k+1})$ where $k \\in \\{0, 1, \\dots, m-1\\}$, we calculate its contribution to the integral.\nLet us consider an interval starting at $g_k$.\nIf the horizon $\\tau$ is less than or equal to $g_k$, then this interval and all subsequent intervals contribute zero area to the integral.\nIf the horizon $\\tau$ is greater than $g_{k+1}$, the full area of the rectangle for this interval is included. The area is the height $s_k$ multiplied by the width of the interval $(g_{k+1} - g_k)$.\nIf the horizon $\\tau$ falls within the interval, i.e., $g_k  \\tau \\le g_{k+1}$, the contribution is from a truncated rectangle. The area is the height $s_k$ multiplied by the width $(\\tau - g_k)$. This is the final contribution to the integral.\n\nThis process can be formulated as an algorithm. We initialize the $\\mathrm{RMST}$ to $0$. We then iterate from $k=0$ to $m-1$:\n1. Define the interval start $t_{\\text{start}} = g_k$ and end $t_{\\text{end}} = g_{k+1}$.\n2. If $\\tau \\le t_{\\text{start}}$, the integration is complete, so we stop.\n3. Determine the effective end of the integration interval. This is the minimum of the interval end $t_{\\text{end}}$ and the horizon $\\tau$. Let this be $t_{\\text{effective\\_end}} = \\min(\\tau, t_{\\text{end}})$.\n4. The width of the rectangle for this segment is $w_k = t_{\\text{effective\\_end}} - t_{\\text{start}}$.\n5. The survival probability for this segment is $s_k$.\n6. The area to add is $A_k = s_k \\times w_k$.\n7. Add this area to the total: $\\mathrm{RMST} = \\mathrm{RMST} + A_k$.\n\nAfter computing $\\mathrm{RMST}_A$ and $\\mathrm{RMST}_B$ using this method, we find their difference:\n$$\n\\Delta = \\mathrm{RMST}_B - \\mathrm{RMST}_A\n$$\nThe sign of $\\Delta$ indicates which treatment arm has a longer average survival time up to the horizon $\\tau$. A positive $\\Delta$ favors arm B, while a negative $\\Delta$ favors arm A.\n\nFinally, we define a preference indicator $I$ based on $\\Delta$, using a numerical tolerance of $\\epsilon=10^{-12}$ to handle potential floating-point inaccuracies when comparing to zero.\n$$\nI = \\begin{cases}\n1  \\text{if } \\Delta  \\epsilon \\\\\n-1  \\text{if } \\Delta  -\\epsilon \\\\\n0  \\text{otherwise}\n\\end{cases}\n$$\nThe final output for each test case is a list containing the computed values: $[\\mathrm{RMST}_A, \\mathrm{RMST}_B, \\Delta, I]$, with the first three values rounded to six decimal places.\n\nFor example, let's calculate the result for Case 1:\n- Grid: $g = [0.0, 2.0, 5.0, 8.0, 12.0]$\n- Arm A survival: $s_A = [1.0, 0.92, 0.80, 0.74, 0.70]$\n- Arm B survival: $s_B = [1.0, 0.96, 0.78, 0.77, 0.72]$\n- Horizon: $\\tau = 10.0$\n\nFor Arm A:\n- Interval $[0.0, 2.0)$: Area = $s_{A,0} \\times (2.0 - 0.0) = 1.0 \\times 2.0 = 2.0$\n- Interval $[2.0, 5.0)$: Area = $s_{A,1} \\times (5.0 - 2.0) = 0.92 \\times 3.0 = 2.76$\n- Interval $[5.0, 8.0)$: Area = $s_{A,2} \\times (8.0 - 5.0) = 0.80 \\times 3.0 = 2.40$\n- Interval $[8.0, 12.0)$, truncated at $\\tau=10.0$: Area = $s_{A,3} \\times (10.0 - 8.0) = 0.74 \\times 2.0 = 1.48$\n- $\\mathrm{RMST}_A = 2.0 + 2.76 + 2.40 + 1.48 = 8.64$\n\nFor Arm B:\n- Interval $[0.0, 2.0)$: Area = $s_{B,0} \\times (2.0 - 0.0) = 1.0 \\times 2.0 = 2.0$\n- Interval $[2.0, 5.0)$: Area = $s_{B,1} \\times (5.0 - 2.0) = 0.96 \\times 3.0 = 2.88$\n- Interval $[5.0, 8.0)$: Area = $s_{B,2} \\times (8.0 - 5.0) = 0.78 \\times 3.0 = 2.34$\n- Interval $[8.0, 12.0)$, truncated at $\\tau=10.0$: Area = $s_{B,3} \\times (10.0 - 8.0) = 0.77 \\times 2.0 = 1.54$\n- $\\mathrm{RMST}_B = 2.0 + 2.88 + 2.34 + 1.54 = 8.76$\n\nComparison:\n- $\\Delta = \\mathrm{RMST}_B - \\mathrm{RMST}_A = 8.76 - 8.64 = 0.12$\n- Since $0.12  10^{-12}$, the indicator $I=1$.\n- The result for Case 1 is $[8.640000, 8.760000, 0.120000, 1]$.\nThis procedure is applied to all provided test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Restricted Mean Survival Time (RMST) problem for a set of predefined test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: non-proportional hazards with crossing survival and interior truncation\n        {\n            \"grid\": np.array([0.0, 2.0, 5.0, 8.0, 12.0]),\n            \"surv_a\": np.array([1.0, 0.92, 0.80, 0.74, 0.70]),\n            \"surv_b\": np.array([1.0, 0.96, 0.78, 0.77, 0.72]),\n            \"tau\": 10.0,\n        },\n        # Case 2: horizon equals last grid point\n        {\n            \"grid\": np.array([0.0, 3.0, 6.0, 9.0, 12.0]),\n            \"surv_a\": np.array([1.0, 0.90, 0.82, 0.70, 0.60]),\n            \"surv_b\": np.array([1.0, 0.95, 0.75, 0.72, 0.65]),\n            \"tau\": 12.0,\n        },\n        # Case 3: small horizon inside the first interval, tie expected\n        {\n            \"grid\": np.array([0.0, 4.0, 9.0]),\n            \"surv_a\": np.array([1.0, 0.85, 0.70]),\n            \"surv_b\": np.array([1.0, 0.88, 0.68]),\n            \"tau\": 2.0,\n        },\n        # Case 4: multiple crossings and horizon on a grid point\n        {\n            \"grid\": np.array([0.0, 1.0, 2.5, 4.0, 7.5, 10.0]),\n            \"surv_a\": np.array([1.0, 0.97, 0.90, 0.86, 0.80, 0.78]),\n            \"surv_b\": np.array([1.0, 0.99, 0.91, 0.85, 0.79, 0.76]),\n            \"tau\": 7.5,\n        },\n    ]\n\n    def calculate_rmst(grid, survival_values, tau):\n        \"\"\"\n        Calculates the RMST for a right-continuous piecewise-constant survival function.\n        \n        Args:\n            grid (np.ndarray): The time grid points.\n            survival_values (np.ndarray): The survival probabilities on each interval.\n            tau (float): The time horizon for restriction.\n            \n        Returns:\n            float: The calculated RMST.\n        \"\"\"\n        rmst = 0.0\n        # Iterate through the intervals defined by the grid\n        for i in range(len(grid) - 1):\n            t_start = grid[i]\n            t_end = grid[i+1]\n            \n            # If the integration horizon is before or at the start of this interval, we are done.\n            if tau = t_start:\n                break\n            \n            # The value of the survival function in this interval [t_start, t_end)\n            s_val = survival_values[i]\n            \n            # Calculate the width of the integration for this interval.\n            # It's the distance from t_start to either tau or t_end, whichever is smaller.\n            integration_end = min(tau, t_end)\n            width = integration_end - t_start\n            \n            # Add the area of this rectangular segment to the total RMST.\n            rmst += s_val * width\n            \n        return rmst\n\n    formatted_results = []\n    tolerance = 1e-12\n\n    for case in test_cases:\n        grid, surv_a, surv_b, tau = case[\"grid\"], case[\"surv_a\"], case[\"surv_b\"], case[\"tau\"]\n\n        rmst_a = calculate_rmst(grid, surv_a, tau)\n        rmst_b = calculate_rmst(grid, surv_b, tau)\n        \n        delta = rmst_b - rmst_a\n        \n        if delta  tolerance:\n            indicator = 1\n        elif delta  -tolerance:\n            indicator = -1\n        else:\n            indicator = 0\n            \n        # Format the result list for this case as a string\n        case_result_str = (\n            f\"[{rmst_a:.6f},{rmst_b:.6f},{delta:.6f},{indicator}]\"\n        )\n        formatted_results.append(case_result_str)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3179108"}, {"introduction": "Choosing the right statistical model is critical for making accurate predictions. In survival analysis, the Cox Proportional Hazards (PH) and Accelerated Failure Time (AFT) models represent two different assumptions about how covariates affect survival. This advanced practice places you in a realistic scenario where you must determine which model performs better when the underlying data violates the PH assumption. Through simulation, you will see firsthand the consequences of model misspecification and learn how to use metrics like RMST to evaluate and compare model performance [@problem_id:3179084].", "problem": "Consider a binary covariate $x \\in \\{0,1\\}$ affecting a time-to-event random variable $T$. Let the survival function be defined by $S(t \\mid x) = \\mathbb{P}(T  t \\mid x)$ for $t \\ge 0$, the probability density function by $f(t \\mid x)$, the hazard function by $h(t \\mid x) = \\frac{f(t \\mid x)}{S(t \\mid x)}$, and the cumulative hazard by $H(t \\mid x) = \\int_0^t h(u \\mid x) \\, du$. The Restricted Mean Survival Time (RMST) up to a finite horizon $\\tau  0$ is defined by\n$$\n\\mathrm{RMST}_x(\\tau) = \\int_0^\\tau S(t \\mid x)\\, dt.\n$$\n\nTwo widely used model classes are:\n- The Cox Proportional Hazards (PH) model, which posits $h(t \\mid x) = h_0(t)\\,\\exp(\\beta_{\\mathrm{Cox}} x)$ for an unspecified baseline hazard $h_0(t)$ and a regression coefficient $\\beta_{\\mathrm{Cox}}$.\n- The Accelerated Failure Time (AFT) model, which posits $\\log T = \\mu + \\beta_{\\mathrm{AFT}} x + \\sigma Z$, where $Z \\sim \\mathcal{N}(0,1)$, $\\mu \\in \\mathbb{R}$, $\\beta_{\\mathrm{AFT}} \\in \\mathbb{R}$, and $\\sigma  0$; this implies a log-normal survival distribution and typically violates the proportional hazards assumption when $\\beta_{\\mathrm{AFT}} \\ne 0$.\n\nStarting from the core definitions above, you must:\n1. Simulate right-censored survival data under a log-normal AFT data-generating process with parameters $(\\mu, \\beta, \\sigma)$ and independent exponential censoring with rate $\\lambda_c$. For each individual $i$, draw $x_i \\in \\{0,1\\}$, $Z_i \\sim \\mathcal{N}(0,1)$, set $T_i = \\exp(\\mu + \\beta x_i + \\sigma Z_i)$, draw $C_i \\sim \\mathrm{Exp}(\\lambda_c)$, and observe $Y_i = \\min(T_i, C_i)$ with event indicator $\\delta_i = \\mathbb{I}\\{T_i \\le C_i\\}$.\n2. Fit the Cox PH model by maximizing the partial likelihood for a single binary covariate $x$, and estimate the baseline cumulative hazard using the Breslow estimator. Use these to construct predicted survival functions $\\widehat{S}_{\\mathrm{Cox}}(t \\mid x)$.\n3. Fit the log-normal AFT model by maximum likelihood under right-censoring to estimate $(\\widehat{\\mu}, \\widehat{\\beta}, \\widehat{\\sigma})$, and construct predicted survival functions $\\widehat{S}_{\\mathrm{AFT}}(t \\mid x)$ using the fitted parameters.\n4. Compute the true survival function $S_{\\mathrm{true}}(t \\mid x)$ under the log-normal data-generating process and compute the corresponding true restricted mean survival times $\\mathrm{RMST}^{\\mathrm{true}}_x(\\tau) = \\int_0^\\tau S_{\\mathrm{true}}(t \\mid x) \\, dt$. Numerically approximate all integrals using a sufficiently fine grid on $[0,\\tau]$.\n5. For each model $M \\in \\{\\mathrm{Cox}, \\mathrm{AFT}\\}$ and group $x \\in \\{0,1\\}$, compute the predicted restricted mean survival time $\\mathrm{RMST}^{M}_x(\\tau) = \\int_0^\\tau \\widehat{S}_M(t \\mid x)\\, dt$, and its absolute calibration error with respect to truth, $E^{M}_x(\\tau) = \\left|\\mathrm{RMST}^{M}_x(\\tau) - \\mathrm{RMST}^{\\mathrm{true}}_x(\\tau)\\right|$. Aggregate calibration across the two groups by the mean absolute error,\n$$\n\\overline{E}^{M}(\\tau) = \\frac{E^{M}_0(\\tau) + E^{M}_1(\\tau)}{2}.\n$$\n6. Decide which model yields better RMST$(\\tau)$ calibration by comparing $\\overline{E}^{\\mathrm{AFT}}(\\tau)$ to $\\overline{E}^{\\mathrm{Cox}}(\\tau)$: the model with the smaller value is deemed better calibrated.\n\nYour program must implement the full pipeline described above and evaluate a test suite comprising the following parameter sets. Each test case is a tuple $(n, p, \\mu, \\beta, \\sigma, \\lambda_c, \\tau, \\text{seed})$, where $n$ is the sample size, $p$ is the probability of $x=1$, $\\mu$ is the log-scale intercept, $\\beta$ is the AFT effect, $\\sigma$ is the log-scale standard deviation, $\\lambda_c$ is the exponential censoring rate, $\\tau$ is the RMST horizon, and $\\text{seed}$ is the random seed:\n- Case $1$: $(n, p, \\mu, \\beta, \\sigma, \\lambda_c, \\tau, \\text{seed}) = (1000, 0.5, 2.0, 0.7, 0.6, 0.05, 10.0, 1)$.\n- Case $2$: $(n, p, \\mu, \\beta, \\sigma, \\lambda_c, \\tau, \\text{seed}) = (1000, 0.5, 2.0, 1.0, 0.9, 0.2, 8.0, 2)$.\n- Case $3$: $(n, p, \\mu, \\beta, \\sigma, \\lambda_c, \\tau, \\text{seed}) = (500, 0.2, 1.5, 0.8, 0.5, 0.1, 6.0, 3)$.\n- Case $4$: $(n, p, \\mu, \\beta, \\sigma, \\lambda_c, \\tau, \\text{seed}) = (400, 0.5, 1.0, 1.2, 1.0, 0.3, 5.0, 4)$.\n\nNumerical units: treat time in arbitrary units; no physical unit conversion is required. Angles are not involved. Percentages must be expressed as decimals.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case (in the order above), the element must be a boolean value indicating whether the Accelerated Failure Time model yields better RMST$(\\tau)$ calibration than the Cox Proportional Hazards model, i.e., whether $\\overline{E}^{\\mathrm{AFT}}(\\tau)  \\overline{E}^{\\mathrm{Cox}}(\\tau)$. The final output must look like\n$[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4]$,\nwhere each $\\text{result}_k \\in \\{\\text{True},\\text{False}\\}$.", "solution": "The user wants to solve a statistical simulation problem.\n\nHere's the plan:\n1.  **Validate the problem statement**:\n    -   Extract all the givens: definitions, model specifications, simulation steps, calculation formulas, and test cases.\n    -   Check for any scientific, logical, or structural flaws.\n\n2.  **If valid, design the solution**:\n    -   The problem is a multi-step simulation and modeling exercise. I need to break it down into a clear sequence of operations.\n    -   **Step 1: Data Generation**. This is clearly specified. For each individual $i=1, ..., n$:\n        -   Draw $x_i \\sim \\text{Bernoulli}(p)$.\n        -   Draw $Z_i \\sim \\mathcal{N}(0,1)$.\n        -   Calculate the true event time $T_i = \\exp(\\mu + \\beta x_i + \\sigma Z_i)$.\n        -   Draw the censoring time $C_i \\sim \\text{Exp}(\\lambda_c)$.\n        -   The observed time is $Y_i = \\min(T_i, C_i)$.\n        -   The event indicator is $\\delta_i = \\mathbb{I}\\{T_i \\le C_i\\}$.\n    -   **Step 2: Cox PH Model Fitting**.\n        -   The model is $h(t \\mid x) = h_0(t) \\exp(\\beta_{\\text{Cox}} x)$.\n        -   The partial log-likelihood for a single binary covariate $x$ needs to be maximized to find $\\widehat{\\beta}_{\\text{Cox}}$. The partial log-likelihood is:\n            $$ \\ell(\\beta) = \\sum_{i=1}^n \\delta_i \\left( \\beta x_i - \\log \\sum_{j \\in R(Y_i)} \\exp(\\beta x_j) \\right) $$\n            where $R(Y_i) = \\{j: Y_j \\ge Y_i\\}$ is the risk set at time $Y_i$.\n        -   I will need a numerical optimizer (like Newton-Raphson or any from `scipy.optimize`) to find $\\widehat{\\beta}_{\\text{Cox}}$. Newton-Raphson is standard for Cox models.\n            -   Score function (first derivative): $U(\\beta) = \\frac{\\partial \\ell}{\\partial \\beta} = \\sum_{i=1}^n \\delta_i \\left( x_i - \\frac{\\sum_{j \\in R(Y_i)} x_j \\exp(\\beta x_j)}{\\sum_{j \\in R(Y_i)} \\exp(\\beta x_j)} \\right)$.\n            -   Information matrix (negative second derivative): $I(\\beta) = -\\frac{\\partial^2 \\ell}{\\partial \\beta^2} = \\sum_{i=1}^n \\delta_i \\left( \\frac{\\sum_{j \\in R(Y_i)} x_j^2 \\exp(\\beta x_j)}{\\sum_{j \\in R(Y_i)} \\exp(\\beta x_j)} - \\left( \\frac{\\sum_{j \\in R(Y_i)} x_j \\exp(\\beta x_j)}{\\sum_{j \\in R(Y_i)} \\exp(\\beta x_j)} \\right)^2 \\right)$.\n        -   After finding $\\widehat{\\beta}_{\\text{Cox}}$, I need to estimate the cumulative baseline hazard $H_0(t)$ using the Breslow estimator.\n            $$ \\widehat{H}_0(t) = \\sum_{Y_i \\le t, \\delta_i=1} \\frac{1}{\\sum_{j \\in R(Y_i)} \\exp(\\widehat{\\beta}_{\\text{Cox}} x_j)} $$\n            This is a step function that jumps at observed event times.\n        -   The predicted survival function for the Cox model is $\\widehat{S}_{\\text{Cox}}(t \\mid x) = \\exp(-\\widehat{H}(t \\mid x)) = \\exp(-\\widehat{H}_0(t) \\exp(\\widehat{\\beta}_{\\text{Cox}} x))$.\n    -   **Step 3: Log-normal AFT Model Fitting**.\n        -   The model is $\\log T = \\mu + \\beta x + \\sigma Z$, with $Z \\sim \\mathcal{N}(0,1)$. This is equivalent to $T \\sim \\text{Log-Normal}(\\mu + \\beta x, \\sigma^2)$.\n        -   Let $V_i = \\log Y_i$. The model can be written as a linear regression model for censored data: $V_i = \\mu + \\beta x_i + \\sigma Z_i^*$, where $Z_i^*$ are the errors.\n        -   The likelihood function for right-censored data from a parametric distribution needs to be maximized. For individual $i$, the contribution to the likelihood is:\n            $$ L_i(\\mu, \\beta, \\sigma) = [f_{T_i}(Y_i \\mid x_i)]^{\\delta_i} [S_{T_i}(Y_i \\mid x_i)]^{1-\\delta_i} $$\n            where $f_{T_i}$ and $S_{T_i}$ are the PDF and survival function of the log-normal distribution.\n        -   Let's work with the log-likelihood and the standardized variable. Let $z_i = (\\log Y_i - (\\mu + \\beta x_i)) / \\sigma$.\n        -   The PDF of $\\log T_i$ is $f_{\\log T_i}(v) = \\frac{1}{\\sigma} \\phi\\left(\\frac{v - (\\mu+\\beta x_i)}{\\sigma}\\right)$, where $\\phi$ is the standard normal PDF.\n        -   The survival function of $\\log T_i$ is $S_{\\log T_i}(v) = 1 - \\Phi\\left(\\frac{v - (\\mu+\\beta x_i)}{\\sigma}\\right) = \\Phi\\left(-\\frac{v - (\\mu+\\beta x_i)}{\\sigma}\\right)$, where $\\Phi$ is the standard normal CDF.\n        -   Using the change of variables formula, the PDF of $T_i = \\exp(\\log T_i)$ is $f_{T_i}(t) = f_{\\log T_i}(\\log t) \\frac{1}{t}$.\n        -   So, $f_{T_i}(t) = \\frac{1}{t\\sigma} \\phi\\left(\\frac{\\log t - (\\mu+\\beta x_i)}{\\sigma}\\right)$.\n        -   The survival function of $T_i$ is $S_{T_i}(t) = S_{\\log T_i}(\\log t)$.\n        -   So, $S_{T_i}(t) = \\Phi\\left(-\\frac{\\log t - (\\mu+\\beta x_i)}{\\sigma}\\right)$.\n        -   The log-likelihood to be maximized is:\n            $$ \\ell(\\mu, \\beta, \\sigma) = \\sum_{i=1}^n \\left\\{ \\delta_i \\log f_{T_i}(Y_i \\mid x_i) + (1-\\delta_i) \\log S_{T_i}(Y_i \\mid x_i) \\right\\} $$\n            $$ \\ell(\\mu, \\beta, \\sigma) = \\sum_{i=1}^n \\left\\{ \\delta_i \\left[ -\\log Y_i - \\log \\sigma + \\log \\phi(z_i) \\right] + (1-\\delta_i) \\log \\Phi(-z_i) \\right\\} $$\n            where $z_i = (\\log Y_i - \\mu - \\beta x_i)/\\sigma$.\n        -   I need to maximize this function with respect to $(\\mu, \\beta, \\sigma)$ using a numerical optimizer from `scipy.optimize`. Let's use `minimize` on the negative log-likelihood. The parameters are $(\\mu, \\beta, \\log \\sigma)$ to ensure $\\sigma  0$.\n        -   After finding $(\\widehat{\\mu}, \\widehat{\\beta}, \\widehat{\\sigma})$, the predicted survival function for the AFT model is:\n            $$ \\widehat{S}_{\\text{AFT}}(t \\mid x) = \\Phi\\left(-\\frac{\\log t - (\\widehat{\\mu}+\\widehat{\\beta} x)}{\\widehat{\\sigma}}\\right) $$\n    -   **Step 4: True Survival Function and RMST**.\n        -   The data is generated from a log-normal AFT model with parameters $(\\mu, \\beta, \\sigma)$. So the true survival function is known.\n        -   $S_{\\text{true}}(t \\mid x) = \\Phi\\left(-\\frac{\\log t - (\\mu+\\beta x)}{\\sigma}\\right)$.\n        -   The true RMST is $\\mathrm{RMST}^{\\text{true}}_x(\\tau) = \\int_0^\\tau S_{\\text{true}}(t \\mid x) \\, dt$.\n    -   **Step 5: Predicted RMST and Error Calculation**.\n        -   For each model $M \\in \\{\\text{Cox}, \\text{AFT}\\}$ and group $x \\in \\{0,1\\}$, compute the predicted RMST:\n            $$ \\mathrm{RMST}^{M}_x(\\tau) = \\int_0^\\tau \\widehat{S}_M(t \\mid x)\\, dt $$\n        -   These integrals will be computed numerically. The problem specifies using a \"sufficiently fine grid on $[0,\\tau]$\". I can use Simpson's rule or the trapezoidal rule. Let's use a grid of points, say 1000 points, from $0$ to $\\tau$.\n        -   Then, calculate the absolute calibration error:\n            $$ E^{M}_x(\\tau) = \\left|\\mathrm{RMST}^{M}_x(\\tau) - \\mathrm{RMST}^{\\text{true}}_x(\\tau)\\right| $$\n        -   Finally, the mean absolute error:\n            $$ \\overline{E}^{M}(\\tau) = \\frac{E^{M}_0(\\tau) + E^{M}_1(\\tau)}{2} $$\n    -   **Step 6: Comparison**.\n        -   Compare $\\overline{E}^{\\text{AFT}}(\\tau)$ and $\\overline{E}^{\\text{Cox}}(\\tau)$.\n        -   The result for the test case is `True` if $\\overline{E}^{\\text{AFT}}(\\tau)  \\overline{E}^{\\text{Cox}}(\\tau)$, and `False` otherwise.\n    -   **Implementation Details**:\n        -   Numerical integration: I can use `np.trapz` or `scipy.integrate.simps` or `scipy.integrate.quad`. Given the requested self-contained nature and the grid suggestion, a simple trapezoidal rule implementation over a numpy array will be robust and sufficient.\n        -   Optimization: `scipy.optimize.minimize` will be the workhorse for both the Cox and AFT model fitting. I'll need to define the negative log-likelihood functions. For the Cox model, a Newton-Raphson implementation might be more standard and efficient. Let me check the complexity.\n        -   Cox model fitting: Implementing the score and information matrix for Newton-Raphson is straightforward but involves iterating over sorted event times.\n            -   Sort data by observed times $Y_i$.\n            -   Iterate through event times $t_k$. For each $t_k$, the risk set $R(t_k)$ is easy to identify.\n            -   The sums $\\sum_{j \\in R(t_k)} \\exp(\\beta x_j)$, $\\sum_{j \\in R(t_k)} x_j \\exp(\\beta x_j)$, etc., can be computed efficiently by reverse cumulative sums. Let's see. Sort the data by $Y_i$ in ascending order. Let the sorted indices be $p_1, ..., p_n$. The risk set at time $Y_{p_k}$ is $\\{p_k, p_{k+1}, ..., p_n\\}$. So sums over risk sets are reverse cumulative sums. This is efficient.\n            `w_j = exp(beta * x_j)`. Compute `S0 = sum(w_j for j in R(t))`, `S1 = sum(x_j * w_j for j in R(t))`, `S2 = sum(x_j^2 * w_j for j in R(t))`.\n            The score is a sum over events: `Sum_{i: delta_i=1} (x_i - S1(Y_i) / S0(Y_i))`.\n            The information is a sum over events: `Sum_{i: delta_i=1} (S2(Y_i)/S0(Y_i) - (S1(Y_i)/S0(Y_i))^2)`.\n            This looks solid.\n        -   AFT model fitting: The log-likelihood involves `scipy.stats.norm.logpdf` and `scipy.stats.norm.logsf`. `scipy` is allowed. `scipy.stats.norm` is part of it. `logsf(x) = log(1-cdf(x))`. This is exactly what I need.\n        -   Breslow estimator: After getting $\\widehat{\\beta}_{\\text{Cox}}$, I need to compute $\\widehat{H}_0(t)$. This is also best done on the sorted data.\n            -   Let the unique event times be $t_{(1)}  t_{(2)}  ...  t_{(D)}$.\n            -   Let $d_k$ be the number of events at $t_{(k)}$.\n            -   Let $R_k$ be the risk set just before $t_{(k)}$.\n            -   The jump in $\\widehat{H}_0(t)$ at $t_{(k)}$ is $\\frac{d_k}{\\sum_{j \\in R_k} \\exp(\\widehat{\\beta}_{\\text{Cox}} x_j)}$.\n            -   Again, this can be efficiently calculated using reverse cumulative sums on the sorted data.\n        -   General structure for the `solve` function:\n            -   Loop through each test case.\n            -   Inside the loop:\n                -   Set random seed.\n                -   Generate data (Step 1).\n                -   Fit Cox model (Step 2).\n                -   Fit AFT model (Step 3).\n                -   Calculate true RMST (Step 4).\n                -   Calculate predicted RMSTs and errors (Step 5).\n                -   Compare errors and store the boolean result (Step 6).\n            -   Print the list of results.\nThe problem is scientifically and mathematically sound, describing a standard simulation study in survival analysis. All terms are well-defined, and the procedure is specified algorithmically. The parameters provided in the test cases are reasonable for such a study. Therefore, the problem is deemed valid.\n\n### Principle-Based Design of the Solution\n\nThe solution proceeds by implementing the six-step pipeline described in the problem statement for each test case.\n\n1.  **Data Generation**: A function will simulate right-censored survival data from a log-normal Accelerated Failure Time (AFT) model. For each of $n$ individuals, it generates a binary covariate $x_i$, a true event time $T_i = \\exp(\\mu + \\beta x_i + \\sigma Z_i)$ where $Z_i \\sim \\mathcal{N}(0,1)$, and a censoring time $C_i$ from an exponential distribution with rate $\\lambda_c$. The observed data for individual $i$ are the follow-up time $Y_i = \\min(T_i, C_i)$ and the event indicator $\\delta_i = \\mathbb{I}(T_i \\leq C_i)$. The parameters $(n, p, \\mu, \\beta, \\sigma, \\lambda_c)$ and a random seed are provided for each test case to ensure reproducibility.\n\n2.  **Cox Model Fitting**:\n    -   **Parameter Estimation**: The Cox Proportional Hazards (PH) model coefficient, $\\beta_{\\mathrm{Cox}}$, is estimated by maximizing the partial log-likelihood. This is a non-linear optimization problem, solved here using the BFGS algorithm provided by `scipy.optimize.minimize`. The objective function (the negative partial log-likelihood) is constructed based on the formula $\\ell(\\beta) = \\sum_{i: \\delta_i=1} [\\beta x_i - \\log(\\sum_{j \\in R(Y_i)} \\exp(\\beta x_j))]$, where $R(Y_i)$ is the set of individuals at risk at time $Y_i$. To compute the risk-set sums efficiently, the data is first sorted by observation time $Y_i$, and reverse cumulative sums are used.\n    -   **Baseline Hazard Estimation**: The cumulative baseline hazard, $H_0(t)$, is estimated using the Breslow estimator. This is a step function that increases at each observed event time $t_k$. The size of the jump at $t_k$ is the number of events at $t_k$ divided by the sum of risk scores, $\\sum_{j \\in R(t_k)} \\exp(\\widehat{\\beta}_{\\mathrm{Cox}}x_j)$.\n    -   **Survival Prediction**: The estimated survival function is then $\\widehat{S}_{\\mathrm{Cox}}(t|x) = \\exp(-\\widehat{H}_0(t)\\exp(\\widehat{\\beta}_{\\mathrm{Cox}}x))$.\n\n3.  **AFT Model Fitting**:\n    -   **Parameter Estimation**: The log-normal AFT model parameters $(\\mu, \\beta, \\sigma)$ are estimated by maximizing the full log-likelihood for right-censored data. The log-likelihood function is $\\ell(\\mu, \\beta, \\sigma) = \\sum_i [\\delta_i \\log f(Y_i|x_i) + (1-\\delta_i) \\log S(Y_i|x_i)]$, where $f$ and $S$ are the PDF and survival function of the log-normal distribution. The parameters are found by minimizing the negative log-likelihood, again using the BFGS algorithm. To ensure the constraint $\\sigma  0$, the optimization is performed over $\\log(\\sigma)$.\n    -   **Survival Prediction**: With the estimated parameters $(\\widehat{\\mu}, \\widehat{\\beta}, \\widehat{\\sigma})$, the predicted survival function is $\\widehat{S}_{\\mathrm{AFT}}(t|x) = \\Phi\\left(-\\frac{\\log t - (\\widehat{\\mu} + \\widehat{\\beta}x)}{\\widehat{\\sigma}}\\right)$, where $\\Phi$ is the standard normal CDF.\n\n4.  **True Quantities**: Since the data generating process is known, the true survival function $S_{\\mathrm{true}}(t|x)$ is simply the log-normal survival function with the true parameters $(\\mu, \\beta, \\sigma)$. The true Restricted Mean Survival Time (RMST) is the integral of this function from $0$ to a specified horizon $\\tau$.\n\n5.  **RMST Calculation and Error Assessment**: All RMST values (true, Cox-predicted, AFT-predicted) are calculated by numerically integrating the corresponding survival functions from $t=0$ to $t=\\tau$. The trapezoidal rule (`numpy.trapz`) is applied over a fine grid of points for this purpose. The mean absolute calibration error, $\\overline{E}^M(\\tau)$, is then computed for each model $M \\in \\{\\mathrm{Cox}, \\mathrm{AFT}\\}$ by averaging the absolute differences between predicted and true RMSTs over the two covariate groups ($x=0$ and $x=1$).\n\n6.  **Model Comparison**: Finally, the AFT model is judged to have better calibration if its mean absolute error $\\overline{E}^{\\mathrm{AFT}}(\\tau)$ is less than that of the Cox model, $\\overline{E}^{\\mathrm{Cox}}(\\tau)$. This comparison yields a boolean result for each test case. As the data are generated from an AFT model, it is expected that the AFT model will generally perform better, providing a conceptual check on the implementation's correctness.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation study and print the results.\n    \"\"\"\n\n    def _fit_cox_model(Y, delta, x):\n        \"\"\"\n        Fits a Cox PH model for a single binary covariate using partial likelihood.\n        Implements Breslow's method for handling event times.\n        Returns the coefficient and a function for the Breslow baseline cumulative hazard.\n        \"\"\"\n        sort_idx = np.argsort(Y)\n        Y_sorted = Y[sort_idx]\n        delta_sorted = delta[sort_idx]\n        x_sorted = x[sort_idx]\n\n        def neg_log_partial_likelihood(beta, Y_s, delta_s, x_s):\n            beta = beta[0]\n            risk_scores = np.exp(beta * x_s)\n            risk_set_sums = np.cumsum(risk_scores[::-1])[::-1]\n            \n            # Use a small epsilon for numerical stability\n            log_risk_set_sums = np.log(risk_set_sums + 1e-30)\n            \n            event_indices = (delta_s == 1)\n            linear_term_at_events = beta * x_s[event_indices]\n            log_risk_set_sums_at_events = log_risk_set_sums[event_indices]\n            \n            log_likelihood = np.sum(linear_term_at_events - log_risk_set_sums_at_events)\n            return -log_likelihood\n\n        res = minimize(\n            neg_log_partial_likelihood,\n            x0=np.array([0.0]),\n            args=(Y_sorted, delta_sorted, x_sorted),\n            method='BFGS'\n        )\n        beta_cox = res.x[0]\n\n        risk_scores_est = np.exp(beta_cox * x_sorted)\n        risk_set_sums_est = np.cumsum(risk_scores_est[::-1])[::-1]\n        \n        event_mask = (delta_sorted == 1)\n        unique_event_times, event_time_indices = np.unique(Y_sorted[event_mask], return_inverse=True)\n        num_events_at_time = np.bincount(event_time_indices)\n        \n        y_event_indices = np.searchsorted(Y_sorted, unique_event_times)\n        risk_set_sum_at_event_time = risk_set_sums_est[y_event_indices]\n        \n        hazard_jumps = num_events_at_time / (risk_set_sum_at_event_time + 1e-30)\n        cum_hazard_jumps = np.cumsum(hazard_jumps)\n\n        def breslow_estimator(t_grid):\n            H0_t = np.zeros_like(t_grid, dtype=float)\n            indices = np.searchsorted(unique_event_times, t_grid, side='right')\n            valid_indices = (indices  0)\n            H0_t[valid_indices] = cum_hazard_jumps[indices[valid_indices] - 1]\n            return H0_t\n            \n        return beta_cox, breslow_estimator\n\n    def _fit_aft_model(Y, delta, x):\n        \"\"\"\n        Fits a log-normal AFT model using maximum likelihood for right-censored data.\n        \"\"\"\n        log_Y = np.log(Y)\n        initial_params = np.array([1.0, 0.0, 0.0])\n        is_event = (delta == 1)\n        if np.sum(is_event)  2:\n            x_event = x[is_event]\n            if np.any(x_event != x_event[0]):\n                log_Y_event = log_Y[is_event]\n                X = np.vstack([np.ones(len(x_event)), x_event]).T\n                try:\n                    params, _, _, _ = np.linalg.lstsq(X, log_Y_event, rcond=None)\n                    mu_init, beta_init = params\n                    residuals = log_Y_event - X @ params\n                    sigma_init = np.std(residuals)\n                    if sigma_init  1e-4:\n                        initial_params = np.array([mu_init, beta_init, np.log(sigma_init)])\n                except np.linalg.LinAlgError:\n                    pass\n\n        def neg_log_likelihood(params, log_Y_l, delta_l, x_l):\n            mu, beta, log_sigma = params\n            sigma = np.exp(log_sigma)\n            if sigma  1e-6: return np.inf\n\n            z = (log_Y_l - mu - beta * x_l) / sigma\n            \n            events_mask = (delta_l == 1)\n            loglik = np.sum(norm.logpdf(z[events_mask]) - log_sigma - log_Y_l[events_mask])\n            loglik += np.sum(norm.logsf(z[~events_mask]))\n            \n            return -loglik if np.isfinite(loglik) else np.inf\n\n        res = minimize(\n            neg_log_likelihood,\n            initial_params,\n            args=(log_Y, delta, x),\n            method='BFGS'\n        )\n        mu_aft, beta_aft, log_sigma_aft = res.x\n        sigma_aft = np.exp(log_sigma_aft)\n        return mu_aft, beta_aft, sigma_aft\n\n    def _run_simulation_case(n, p, mu, beta, sigma, lambda_c, tau, seed):\n        \"\"\"\n        Runs the full pipeline for one test case.\n        \"\"\"\n        rng = np.random.default_rng(seed)\n        x = rng.binomial(1, p, size=n)\n        Z = rng.standard_normal(size=n)\n        T = np.exp(mu + beta * x + sigma * Z)\n        if lambda_c  0:\n            C = rng.exponential(scale=1 / lambda_c, size=n)\n        else:\n            C = np.full(n, np.inf)\n        Y = np.minimum(T, C)\n        delta = (T = C).astype(int)\n\n        t_grid = np.linspace(0, tau, 2001)\n\n        beta_cox, H0_func = _fit_cox_model(Y, delta, x)\n        H0_on_grid = H0_func(t_grid)\n        S_cox_0 = np.exp(-H0_on_grid)\n        S_cox_1 = np.exp(-H0_on_grid * np.exp(beta_cox))\n        rmst_cox_0 = np.trapz(S_cox_0, t_grid)\n        rmst_cox_1 = np.trapz(S_cox_1, t_grid)\n\n        mu_aft, beta_aft, sigma_aft = _fit_aft_model(Y, delta, x)\n        with np.errstate(divide='ignore'):\n            log_t_grid = np.log(t_grid)\n        \n        S_aft_0 = norm.sf((log_t_grid - mu_aft) / sigma_aft)\n        S_aft_1 = norm.sf((log_t_grid - (mu_aft + beta_aft)) / sigma_aft)\n        S_aft_0[0] = 1.0\n        S_aft_1[0] = 1.0\n        rmst_aft_0 = np.trapz(S_aft_0, t_grid)\n        rmst_aft_1 = np.trapz(S_aft_1, t_grid)\n\n        S_true_0 = norm.sf((log_t_grid - mu) / sigma)\n        S_true_1 = norm.sf((log_t_grid - (mu + beta)) / sigma)\n        S_true_0[0] = 1.0\n        S_true_1[0] = 1.0\n        rmst_true_0 = np.trapz(S_true_0, t_grid)\n        rmst_true_1 = np.trapz(S_true_1, t_grid)\n\n        E_cox_0 = np.abs(rmst_cox_0 - rmst_true_0)\n        E_cox_1 = np.abs(rmst_cox_1 - rmst_true_1)\n        E_bar_cox = (E_cox_0 + E_cox_1) / 2.0\n\n        E_aft_0 = np.abs(rmst_aft_0 - rmst_true_0)\n        E_aft_1 = np.abs(rmst_aft_1 - rmst_true_1)\n        E_bar_aft = (E_aft_0 + E_aft_1) / 2.0\n\n        return E_bar_aft  E_bar_cox\n\n    test_cases = [\n        (1000, 0.5, 2.0, 0.7, 0.6, 0.05, 10.0, 1),\n        (1000, 0.5, 2.0, 1.0, 0.9, 0.2, 8.0, 2),\n        (500, 0.2, 1.5, 0.8, 0.5, 0.1, 6.0, 3),\n        (400, 0.5, 1.0, 1.2, 1.0, 0.3, 5.0, 4),\n    ]\n\n    results = []\n    for case in test_cases:\n        result = _run_simulation_case(*case)\n        results.append(result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3179084"}]}