{"hands_on_practices": [{"introduction": "The Kaplan-Meier estimator is a cornerstone of survival analysis, providing a non-parametric way to estimate the survival function $S(t)$ from right-censored data. This exercise takes you beyond simple calculation by exploring the theoretical basis for the estimator's uncertainty. By deriving the influence function and using it to arrive at Greenwood's formula, you will gain a deeper understanding of how each data point contributes to the final estimate and its confidence interval [@problem_id:3107041].", "problem": "A biomedical study observes survival times under right-censoring. For subject $i$, let $T_i$ be the failure time and $C_i$ be the censoring time, with independent censoring. The observed data are $(X_i, \\Delta_i)$ where $X_i = \\min(T_i, C_i)$ and $\\Delta_i = \\mathbf{1}\\{T_i \\leq C_i\\}$. Define the survival function $S(t) = \\mathbb{P}(T > t)$, the cumulative hazard $\\Lambda(t) = \\int_0^t \\lambda(u)\\,\\mathrm{d}u$, and the Kaplan–Meier (KM) estimator $\\hat{S}(t)$ constructed from the empirical jumps at distinct observed event times. The study follows $n = 10$ individuals with observed pairs\n$$\n\\{(t_i, \\delta_i)\\}_{i=1}^{10} = \\{(1.5,1),\\ (2.0,0),\\ (3.0,1),\\ (4.0,1),\\ (5.5,0),\\ (6.0,1),\\ (7.0,0),\\ (7.5,1),\\ (8.0,1),\\ (9.0,0)\\},\n$$\nwhere $t_i$ is measured in months and $\\delta_i \\in \\{0,1\\}$ indicates censoring ($0$) or event ($1$). Assume non-informative right-censoring and a continuous underlying failure-time distribution.\n\nStarting from foundational definitions of right-censoring, the at-risk process $Y(t) = \\sum_{i=1}^{n} \\mathbf{1}\\{X_i \\geq t\\}$, and the counting process $N(t) = \\sum_{i=1}^{n} \\mathbf{1}\\{X_i \\leq t,\\ \\Delta_i=1\\}$, derive the influence function for $\\hat{S}(t)$ at a fixed time $t$ by:\n- establishing the influence function for the Nelson–Aalen estimator of $\\Lambda(t)$ under independent right-censoring,\n- applying a functional delta method to map the cumulative hazard to the survival function,\n- carefully linearizing the product-limit form of $\\hat{S}(t)$ to obtain a right-censoring-adjusted influence function representation that leads to the classical large-sample variance of $\\hat{S}(t)$ known as Greenwood’s formula.\n\nThen, using the provided dataset, compute the Kaplan–Meier estimate $\\hat{S}(t)$ at $t = 8.0$ months and the Greenwood large-sample variance of $\\hat{S}(8.0)$. Express the final numerical value of the estimated asymptotic variance as a real number, and round your answer to four significant figures. No physical units are required.", "solution": "The user wants a detailed derivation and calculation for a survival analysis problem.\n\n### Step 1: Problem Validation\n\n**1.1. Extract Givens**\n\n- **Model:** Right-censored survival data. $T_i$ is failure time, $C_i$ is censoring time. Censoring is independent and non-informative.\n- **Observed Data:** For subject $i$, $(X_i, \\Delta_i)$ where $X_i = \\min(T_i, C_i)$ and $\\Delta_i = \\mathbf{1}\\{T_i \\leq C_i\\}$.\n- **Definitions:**\n    - Survival function: $S(t) = \\mathbb{P}(T > t)$.\n    - Cumulative hazard function: $\\Lambda(t) = \\int_0^t \\lambda(u)\\,\\mathrm{d}u$.\n    - At-risk process: $Y(t) = \\sum_{i=1}^{n} \\mathbf{1}\\{X_i \\geq t\\}$.\n    - Counting process: $N(t) = \\sum_{i=1}^{n} \\mathbf{1}\\{X_i \\leq t, \\Delta_i=1\\}$.\n    - Kaplan-Meier (KM) estimator: $\\hat{S}(t)$.\n- **Dataset:** $n = 10$ subjects, with data $\\{(t_i, \\delta_i)\\}_{i=1}^{10} = \\{(1.5,1), (2.0,0), (3.0,1), (4.0,1), (5.5,0), (6.0,1), (7.0,0), (7.5,1), (8.0,1), (9.0,0)\\}$. Time is in months.\n- **Task:**\n    1. Derive the influence function for $\\hat{S}(t)$ using a specified path involving the Nelson-Aalen estimator, functional delta method, and linearization of the product-limit formula, leading to Greenwood's formula for the variance.\n    2. Compute $\\hat{S}(8.0)$ and its large-sample variance using Greenwood's formula for the provided dataset.\n\n**1.2. Validate Using Extracted Givens**\n\n- **Verdict:** The problem is valid.\n- **Reasoning:**\n    - **Scientifically Grounded:** The problem is a standard exercise in survival analysis, a core topic in biostatistics and statistical learning. All concepts introduced—right-censoring, Kaplan-Meier estimation, Nelson-Aalen estimation, influence functions, counting processes, and Greenwood's formula—are fundamental and well-established in the field.\n    - **Well-Posed:** The problem provides all necessary data and theoretical constructs to perform both the derivation and the calculation. The request is specific and leads to a unique, meaningful solution.\n    - **Objective:** The problem is stated in precise, formal mathematical and statistical language, free from ambiguity or subjectivity.\n    - The problem does not violate any of the invalidity criteria. It is a complete, consistent, and formalizable problem within the specified scientific domain.\n\n### Step 2: Derivation and Solution\n\nThe solution proceeds in two parts: first, the theoretical derivation of the influence function and variance, and second, the numerical computation using the provided dataset.\n\n**Part 1: Theoretical Derivation**\n\nThe primary goal is to find an asymptotic representation for $\\hat{S}(t) - S(t)$ which reveals the influence of each observation. This leads to the influence function and a formula for the asymptotic variance.\n\n**1.1. Influence Function for the Nelson-Aalen Estimator**\n\nThe Nelson-Aalen (NA) estimator for the cumulative hazard function $\\Lambda(t) = \\int_0^t \\lambda(u)\\,\\mathrm{d}u$ is given by a Stieltjes integral:\n$$\n\\hat{\\Lambda}_{NA}(t) = \\int_0^t \\frac{\\mathrm{d}N(u)}{Y(u)} = \\sum_{j: t_j \\leq t, \\delta_j=1} \\frac{1}{Y(t_j)}\n$$\nwhere $t_j$ are the distinct event times.\n\nTo understand the properties of $\\hat{\\Lambda}_{NA}(t)$, we use counting process theory. For each subject $i$, we define the counting process $N_i(t) = \\mathbf{1}\\{X_i \\leq t, \\Delta_i=1\\}$ and the at-risk process $Y_i(t) = \\mathbf{1}\\{X_i \\geq t\\}$. The process $M_i(t) = N_i(t) - \\int_0^t Y_i(u)\\,\\mathrm{d}\\Lambda(u)$ is a zero-mean martingale. The aggregate processes are $N(t) = \\sum_i N_i(t)$ and $Y(t) = \\sum_i Y_i(t)$, and the aggregate martingale is $M(t) = \\sum_i M_i(t)$.\n\nWe can write the difference $\\hat{\\Lambda}_{NA}(t) - \\Lambda(t)$ as:\n$$\n\\hat{\\Lambda}_{NA}(t) - \\Lambda(t) = \\int_0^t \\frac{\\mathrm{d}N(u)}{Y(u)} - \\int_0^t \\mathrm{d}\\Lambda(u) = \\int_0^t \\frac{\\mathrm{d}N(u) - Y(u)\\mathrm{d}\\Lambda(u)}{Y(u)}\n$$\nSubstituting $\\mathrm{d}N(u) = \\mathrm{d}M(u) + Y(u)\\mathrm{d}\\Lambda(u)$, we obtain:\n$$\n\\hat{\\Lambda}_{NA}(t) - \\Lambda(t) = \\int_0^t \\frac{\\mathrm{d}M(u)}{Y(u)} = \\sum_{i=1}^n \\int_0^t \\frac{\\mathrm{d}M_i(u)}{Y(u)}\n$$\nFor large $n$, the law of large numbers implies $Y(u)/n \\to \\pi(u) = \\mathbb{P}(X_i \\geq u) = S(u-)P(C_i \\geq u-)$. We can thus approximate $Y(u)$ by $n\\pi(u)$.\n$$\n\\hat{\\Lambda}_{NA}(t) - \\Lambda(t) \\approx \\sum_{i=1}^n \\int_0^t \\frac{\\mathrm{d}M_i(u)}{n\\pi(u)} = \\frac{1}{n} \\sum_{i=1}^n \\int_0^t \\frac{\\mathrm{d}M_i(u)}{\\pi(u)}\n$$\nThis representation shows that $\\hat{\\Lambda}_{NA}(t) - \\Lambda(t)$ is asymptotically a sum of i.i.d. terms. The influence function (IF) for the $i$-th subject on $\\hat{\\Lambda}_{NA}(t)$ is the term inside the sum:\n$$\nIF_i(\\hat{\\Lambda}_{NA}, t) = \\int_0^t \\frac{\\mathrm{d}M_i(u)}{\\pi(u)} = \\int_0^t \\frac{\\mathrm{d}N_i(u) - Y_i(u)\\mathrm{d}\\Lambda(u)}{\\pi(u)}\n= \\frac{\\Delta_i \\mathbf{1}\\{X_i \\leq t\\}}{\\pi(X_i)} - \\int_0^t \\frac{Y_i(u)\\mathrm{d}\\Lambda(u)}{\\pi(u)}\n$$\n\n**1.2. From Cumulative Hazard to Survival Function**\n\nThe Kaplan-Meier estimator is defined by the product-limit formula:\n$$\n\\hat{S}(t) = \\prod_{u \\leq t} \\left(1 - \\frac{\\mathrm{d}N(u)}{Y(u)}\\right)\n$$\nThe true survival function can be expressed as a product-integral of the cumulative hazard: $S(t) = \\prod_{u \\leq t}(1 - \\mathrm{d}\\Lambda(u))$. To find the influence function for $\\hat{S}(t)$, we linearize the estimator. A common method is to first linearize the logarithm:\n$$\n\\ln \\hat{S}(t) = \\sum_{u \\leq t} \\ln\\left(1 - \\frac{\\mathrm{d}N(u)}{Y(u)}\\right)\n$$\nUsing a Taylor expansion $\\ln(1-x) \\approx -x$ for small $x$, and noting that $\\mathrm{d}N(u)/Y(u)$ is small, we have:\n$$\n\\ln \\hat{S}(t) \\approx \\sum_{u \\leq t} -\\frac{\\mathrm{d}N(u)}{Y(u)} = -\\hat{\\Lambda}_{NA}(t)\n$$\nSimilarly, $\\ln S(t) = \\sum_{u \\leq t} \\ln(1 - \\mathrm{d}\\Lambda(u)) \\approx -\\int_0^t \\mathrm{d}\\Lambda(u) = -\\Lambda(t)$.\nThis gives the approximation $\\ln \\hat{S}(t) - \\ln S(t) \\approx -(\\hat{\\Lambda}_{NA}(t) - \\Lambda(t))$.\nExponentiating and linearizing $\\exp(x) \\approx 1+x$ yields:\n$$\n\\hat{S}(t) = S(t) \\exp(\\ln \\hat{S}(t) - \\ln S(t)) \\approx S(t) (1 + \\ln \\hat{S}(t) - \\ln S(t)) \\approx S(t) (1 - (\\hat{\\Lambda}_{NA}(t) - \\Lambda(t)))\n$$\nRearranging gives the key relationship:\n$$\n\\hat{S}(t) - S(t) \\approx -S(t) (\\hat{\\Lambda}_{NA}(t) - \\Lambda(t))\n$$\nThis is an application of the functional delta method, where the mapping is $\\phi(\\Lambda) = \\exp(-\\Lambda(t))$ for the continuous case, whose derivative is $-\\exp(-\\Lambda(t))=-S(t)$. A more rigorous argument based on the Duhamel equation confirms this result.\n\nSubstituting the influence function representation for $\\hat{\\Lambda}_{NA}(t)$, we obtain the influence function for $\\hat{S}(t)$:\n$$\nIF_i(\\hat{S}, t) = -S(t) IF_i(\\hat{\\Lambda}_{NA}, t) = -S(t) \\int_0^t \\frac{\\mathrm{d}M_i(u)}{\\pi(u)}\n$$\n\n**1.3. Asymptotic Variance and Greenwood's Formula**\n\nThe asymptotic variance of $\\sqrt{n}(\\hat{S}(t) - S(t))$ is given by the variance of the influence function, $\\mathbb{E}[IF_i(\\hat{S},t)^2]$.\n$$\n\\text{Var}(\\sqrt{n}(\\hat{S}(t)-S(t))) \\approx \\mathbb{E}[IF_i(\\hat{S},t)^2] = S(t)^2 \\mathbb{E}\\left[ \\left(\\int_0^t \\frac{\\mathrm{d}M_i(u)}{\\pi(u)}\\right)^2 \\right]\n$$\nBy properties of stochastic integrals with respect to martingales, the variance is:\n$$\n\\mathbb{E}\\left[\\int_0^t \\frac{\\mathrm{d}\\langle M_i \\rangle_u}{\\pi(u)^2}\\right] = \\int_0^t \\frac{\\mathbb{E}[\\mathrm{d}\\langle M_i \\rangle_u]}{\\pi(u)^2}\n$$\nThe predictable variation is $\\mathrm{d}\\langle M_i \\rangle_u = Y_i(u)(1-\\mathrm{d}\\Lambda(u))\\mathrm{d}\\Lambda(u)$, and its expectation is $\\pi(u)(1-\\mathrm{d}\\Lambda(u))\\mathrm{d}\\Lambda(u)$.\nSo, the asymptotic variance of $\\sqrt{n}(\\hat{S}(t)-S(t))$ is:\n$$\nS(t)^2 \\int_0^t \\frac{(1-\\mathrm{d}\\Lambda(u))\\mathrm{d}\\Lambda(u)}{\\pi(u)}\n$$\nTo estimate this variance from data, we replace the theoretical quantities with their empirical counterparts. The integral becomes a sum over observed event times $t_j$. We use $d_j/Y_j$ to estimate $\\mathrm{d}\\Lambda(t_j)$ and $Y_j/n$ to estimate $\\pi(t_j)$. The estimator for the variance of $\\hat{S}(t)$ (not scaled by $\\sqrt{n}$) is:\n$$\n\\widehat{\\text{Var}}(\\hat{S}(t)) = \\frac{1}{n} \\hat{S}(t)^2 \\left( \\sum_{j: t_j \\leq t} \\frac{(1-d_j/Y_j)(d_j/Y_j)}{Y_j/n} \\right) = \\hat{S}(t)^2 \\sum_{j: t_j \\leq t} \\frac{d_j(Y_j-d_j)}{Y_j^3}\n$$\nThis formula is asymptotically equivalent to Greenwood's formula. Greenwood's formula, derived from a finite-sample (hypergeometric) argument, is generally preferred for its better performance in smaller samples. It is given by:\n$$\n\\widehat{\\text{Var}}_{GW}(\\hat{S}(t)) = \\hat{S}(t)^2 \\sum_{j: t_j \\leq t} \\frac{d_j}{Y_j(Y_j - d_j)}\n$$\nFor large $Y_j$, $Y_j-d_j \\approx Y_j$, so $\\frac{d_j}{Y_j(Y_j-d_j)} \\approx \\frac{d_j}{Y_j^2}$, which differs from the martingale-based estimate by a factor of $(Y_j-d_j)/Y_j$. Both approaches lead to asymptotically equivalent results, justifying the use of Greenwood's formula as the \"classical large-sample variance\".\n\n**Part 2: Numerical Computation**\n\nWe compute $\\hat{S}(8.0)$ and its variance for the given data. The ordered observed times are $1.5_E, 2.0_C, 3.0_E, 4.0_E, 5.5_C, 6.0_E, 7.0_C, 7.5_E, 8.0_E, 9.0_C$, where $E$ denotes an event and $C$ denotes censoring. The distinct event times are $1.5, 3.0, 4.0, 6.0, 7.5, 8.0$. Since all event times are unique, the number of events $d_j$ at each event time $t_j$ is $1$.\n\nWe construct the Kaplan-Meier table:\n- $t_j$: distinct event time.\n- $Y_j = Y(t_j)$: number of subjects at risk just before time $t_j$.\n- $d_j$: number of events at time $t_j$.\n\n| Event Time $t_j$ | At Risk $Y_j$ | Events $d_j$ | Survival Factor $(1-d_j/Y_j)$ | KM Estimate $\\hat{S}(t_j)$ | Greenwood Term $\\frac{d_j}{Y_j(Y_j-d_j)}$ |\n|---|---|---|---|---|---|\n| $1.5$ | $10$ | $1$ | $1 - \\frac{1}{10} = 0.9$ | $1.0 \\times 0.9 = 0.90$ | $\\frac{1}{10(9)} = \\frac{1}{90}$ |\n| $3.0$ | $8$ | $1$ | $1 - \\frac{1}{8} = 0.875$ | $0.90 \\times 0.875 = 0.7875$ | $\\frac{1}{8(7)} = \\frac{1}{56}$ |\n| $4.0$ | $7$ | $1$ | $1 - \\frac{1}{7} \\approx 0.8571$ | $0.7875 \\times \\frac{6}{7} \\approx 0.675$ | $\\frac{1}{7(6)} = \\frac{1}{42}$ |\n| $6.0$ | $5$ | $1$ | $1 - \\frac{1}{5} = 0.8$ | $0.675 \\times 0.8 = 0.54$ | $\\frac{1}{5(4)} = \\frac{1}{20}$ |\n| $7.5$ | $3$ | $1$ | $1 - \\frac{1}{3} \\approx 0.6667$ | $0.54 \\times \\frac{2}{3} = 0.36$ | $\\frac{1}{3(2)} = \\frac{1}{6}$ |\n| $8.0$ | $2$ | $1$ | $1 - \\frac{1}{2} = 0.5$ | $0.36 \\times 0.5 = 0.18$ | $\\frac{1}{2(1)} = \\frac{1}{2}$ |\n\n**2.1. Calculation of $\\hat{S}(8.0)$**\n\nThe Kaplan-Meier estimate at $t=8.0$ is the value calculated after the event at $t=8.0$.\n$$\n\\hat{S}(8.0) = \\left(1-\\frac{1}{10}\\right)\\left(1-\\frac{1}{8}\\right)\\left(1-\\frac{1}{7}\\right)\\left(1-\\frac{1}{5}\\right)\\left(1-\\frac{1}{3}\\right)\\left(1-\\frac{1}{2}\\right)\n$$\n$$\n\\hat{S}(8.0) = \\frac{9}{10} \\times \\frac{7}{8} \\times \\frac{6}{7} \\times \\frac{4}{5} \\times \\frac{2}{3} \\times \\frac{1}{2}\n$$\n$$\n\\hat{S}(8.0) = \\frac{9 \\times 7 \\times 6 \\times 4 \\times 2 \\times 1}{10 \\times 8 \\times 7 \\times 5 \\times 3 \\times 2} = \\frac{9 \\times 6 \\times 4}{10 \\times 8 \\times 5 \\times 3} = \\frac{216}{1200} = \\frac{9}{50} = 0.18\n$$\n\n**2.2. Calculation of the Variance of $\\hat{S}(8.0)$**\n\nWe use Greenwood's formula: $\\widehat{\\text{Var}}_{GW}(\\hat{S}(t)) = \\hat{S}(t)^2 \\sum_{j: t_j \\leq t} \\frac{d_j}{Y_j(Y_j - d_j)}$. For $t=8.0$, the summation includes all terms calculated in the table.\n$$\n\\sum_{j: t_j \\leq 8.0} \\frac{d_j}{Y_j(Y_j-d_j)} = \\frac{1}{90} + \\frac{1}{56} + \\frac{1}{42} + \\frac{1}{20} + \\frac{1}{6} + \\frac{1}{2}\n$$\nTo sum the fractions, we find a common denominator. The least common multiple of $90, 56, 42, 20, 6, 2$ is $2520$.\n$$\n\\text{Sum} = \\frac{28}{2520} + \\frac{45}{2520} + \\frac{60}{2520} + \\frac{126}{2520} + \\frac{420}{2520} + \\frac{1260}{2520} = \\frac{28+45+60+126+420+1260}{2520} = \\frac{1939}{2520}\n$$\nNow, we can compute the variance:\n$$\n\\widehat{\\text{Var}}_{GW}(\\hat{S}(8.0)) = (\\hat{S}(8.0))^2 \\times \\frac{1939}{2520} = (0.18)^2 \\times \\frac{1939}{2520}\n$$\n$$\n\\widehat{\\text{Var}}_{GW}(\\hat{S}(8.0)) = 0.0324 \\times \\frac{1939}{2520} \\approx 0.0324 \\times 0.769444... \\approx 0.02492999...\n$$\nRounding the final value to four significant figures gives $0.02493$.", "answer": "$$\\boxed{0.02493}$$", "id": "3107041"}, {"introduction": "Modeling the impact of covariates on survival is a primary goal in many fields, and the Cox proportional hazards model is a powerful tool for this task. However, a model is only as good as its assumptions, and the proportional hazards assumption is critical. This practice guides you through the essential skill of model diagnostics, exploring how different types of residuals can help you assess the model's validity and check for time-varying effects [@problem_id:3107112].", "problem": "A clinical study follows $n=200$ patients with covariate vector $X_i$ and event time $T_i$. Because follow-up ends at a fixed administrative time $C_i$, many events are not observed; the observed time is $U_i = \\min(T_i, C_i)$ and the indicator $\\delta_i = \\mathbf{1}\\{T_i \\le C_i\\}$ encodes right-censoring. In this study, approximately $80\\%$ of patients are right-censored, i.e., $\\delta_i=0$ for about $80\\%$ of $i$.\n\nAssume the Cox proportional hazards (PH) model with hazard $\\lambda(t \\mid X_i) = \\lambda_0(t)\\exp(X_i^\\top \\beta)$ is fit, and you wish to use residual-based diagnostics to assess two aspects: (i) the suitability of residuals for detecting covariate functional form under heavy right-censoring (contrasting martingale versus deviance residuals), and (ii) whether a particular coefficient $\\beta_k$ might vary with time, i.e., $\\beta_k(t)$, using Schoenfeld residuals.\n\nBased on the foundational definitions of right-censoring, the hazard function, and the Cox PH model, choose the option that correctly contrasts the behavior of martingale versus deviance residuals under heavy right-censoring and proposes a scientifically valid diagnostic to detect time-varying effects $\\beta_k(t)$ with Schoenfeld residuals.\n\nA. Under heavy right-censoring, martingale residuals are highly skewed with a long negative tail and are bounded above by $1$, so deviance residuals are preferred because they approximately symmetrize the martingale residuals and stabilize their scale. To diagnose a time-varying effect for covariate $X_k$, compute scaled Schoenfeld residuals at event times and regress them on a function of time, such as $\\log(t)$, testing for a nonzero slope; a significant trend indicates that $\\beta_k(t)$ varies with time.\n\nB. Under heavy right-censoring, deviance residuals are more skewed than martingale residuals because many individuals have $0$ observed events; therefore, martingale residuals are preferred. To diagnose time-varying effects, average Schoenfeld residuals across all individuals at each follow-up time and test whether their mean is zero; deviations from zero imply no time-varying effects.\n\nC. Under heavy right-censoring, both martingale and deviance residuals are symmetric with mean zero, so the choice between them does not matter. To diagnose time-varying effects, regress Schoenfeld residuals for $X_k$ on the covariate $X_k$ itself and test whether the slope differs from zero; a nonzero slope indicates time variation in $\\beta_k(t)$.\n\nD. Under heavy right-censoring, martingale residuals tend to cluster tightly around zero, but deviance residuals inflate to very large positive values; therefore, deviance residuals should be avoided. To diagnose time-varying effects, compute Schoenfeld residuals at both event times and censored times and test for independence from $\\log(t)$.", "solution": "The problem statement poses a valid question within the domain of survival analysis and statistical learning, specifically concerning diagnostic procedures for the Cox proportional hazards model. The setup is standard, the terminology is precise, and the context of heavy censoring is a well-defined and common practical challenge. All elements are scientifically grounded and formalizable. We may therefore proceed to a full solution.\n\nThe problem requires an evaluation of two distinct aspects of residual diagnostics for the Cox model:\n1.  The properties and relative suitability of martingale versus deviance residuals under heavy right-censoring for assessing covariate functional form.\n2.  The correct procedure for using Schoenfeld residuals to test for time-varying coefficients, i.e., violations of the proportional hazards (PH) assumption.\n\n**Part 1: Martingale versus Deviance Residuals**\n\nThe martingale residual for subject $i$ in a Cox model is defined as:\n$$M_i = \\delta_i - \\hat{\\Lambda}(U_i \\mid X_i) = \\delta_i - \\hat{\\Lambda}_0(U_i) \\exp(X_i^\\top \\hat{\\beta})$$\nwhere $\\delta_i$ is the event indicator ($1$ if an event occurred, $0$ if censored), $U_i$ is the observed follow-up time, and $\\hat{\\Lambda}(U_i \\mid X_i)$ is the estimated cumulative hazard for subject $i$ at time $U_i$.\n\nThe range of the martingale residual is $(-\\infty, 1]$.\n-   If subject $i$ is censored at time $U_i$, then $\\delta_i = 0$, and the residual is $M_i = -\\hat{\\Lambda}(U_i \\mid X_i)$. This value is always negative.\n-   If subject $i$ experiences an event at time $U_i$, then $\\delta_i = 1$, and the residual is $M_i = 1 - \\hat{\\Lambda}(U_i \\mid X_i)$. This value is typically positive or close to zero, as the cumulative hazard up to most event times is often less than $1$.\n\nThe problem states that approximately $80\\%$ of subjects are right-censored. This implies that about $80\\%$ of the martingale residuals will be negative (corresponding to $\\delta_i = 0$), while only about $20\\%$ correspond to events ($\\delta_i = 1$). This imbalance leads to a highly skewed distribution of martingale residuals, with a mass of negative values and a long negative tail. Such skewed residuals are difficult to interpret visually, for instance, when plotting them against a covariate to check for a non-linear functional form.\n\nDeviance residuals are a transformation of martingale residuals designed to address this skewness. The deviance residual $D_i$ is given by:\n$$D_i = \\text{sign}(M_i) \\sqrt{-2[M_i + \\delta_i \\log(\\delta_i - M_i)]}$$\nwhere the $\\log$ term is handled appropriately for $\\delta_i=0$. This transformation \"symmetrizes\" the distribution of the residuals, making them appear more like the residuals from a standard linear regression model, ideally following a standard normal distribution if the model is correctly specified. By creating a more symmetric, less skewed distribution, deviance residuals are generally considered superior to martingale residuals for graphical diagnostics like detecting non-linearities or identifying outliers, particularly under heavy censoring where the skewness of martingale residuals is most pronounced.\n\n**Part 2: Schoenfeld Residuals for Time-Varying Effects**\n\nThe proportional hazards assumption underlying the Cox model states that the hazard ratio between any two individuals is constant over time. For a specific covariate $X_k$, this means its coefficient $\\beta_k$ is constant. A time-varying coefficient, $\\beta_k(t)$, violates this assumption.\n\nSchoenfeld residuals are specifically designed to diagnose such violations. For each event that occurs at time $t_j$, a Schoenfeld residual is calculated for each covariate $k$. The residual is the difference between the covariate value for the subject $j$ who had the event, $X_{jk}$, and the expected value of that covariate across all subjects in the risk set $R(t_j)$ at that time:\n$$r_{jk} = X_{jk} - E[X_k \\mid R(t_j)] = X_{jk} - \\frac{\\sum_{i \\in R(t_j)} X_{ik} \\exp(X_i^\\top \\hat{\\beta})}{\\sum_{i \\in R(t_j)} \\exp(X_i^\\top \\hat{\\beta})}$$\nCrucially, these residuals are defined **only at event times**.\n\nUnder the PH assumption (i.e., $\\beta_k$ is constant), the expected value of the Schoenfeld residuals for covariate $k$ is zero at all times $t$. Therefore, a plot of the Schoenfeld residuals $r_{jk}$ against time $t_j$ should show a random scatter around zero. If $\\beta_k(t)$ is not constant, a systematic trend will appear in this plot.\n\nA formal test for a time-varying effect involves regressing the (scaled) Schoenfeld residuals on a function of time, $g(t)$. Common choices for $g(t)$ include $t$ itself, or $\\log(t)$ which can be more appropriate for capturing certain patterns of deviation. The model for this test is:\n$$r^*_{jk} = \\gamma_0 + \\gamma_1 g(t_j) + \\epsilon_j$$\nwhere $r^*_{jk}$ are the scaled Schoenfeld residuals. A statistically significant non-zero slope ($\\gamma_1 \\neq 0$) provides evidence against the PH assumption, suggesting that the effect of covariate $X_k$ varies with time.\n\n**Evaluation of Options**\n\n**A. Under heavy right-censoring, martingale residuals are highly skewed with a long negative tail and are bounded above by $1$, so deviance residuals are preferred because they approximately symmetrize the martingale residuals and stabilize their scale. To diagnose a time-varying effect for covariate $X_k$, compute scaled Schoenfeld residuals at event times and regress them on a function of time, such as $\\log(t)$, testing for a nonzero slope; a significant trend indicates that $\\beta_k(t)$ varies with time.**\nThis option correctly describes the properties of martingale and deviance residuals under heavy censoring and the rationale for preferring deviance residuals in this context. It also accurately outlines the standard, scientifically valid procedure for testing the proportional hazards assumption using Schoenfeld residuals.\n**Verdict: Correct.**\n\n**B. Under heavy right-censoring, deviance residuals are more skewed than martingale residuals because many individuals have $0$ observed events; therefore, martingale residuals are preferred. To diagnose time-varying effects, average Schoenfeld residuals across all individuals at each follow-up time and test whether their mean is zero; deviations from zero imply no time-varying effects.**\nThis option is incorrect on both counts. First, deviance residuals are *less* skewed than martingale residuals by design. Second, the procedure for Schoenfeld residuals is misstated: they are only computed at event times, the test is for a trend over time (not a static mean), and the final clause (\"deviations from zero imply no time-varying effects\") is the reverse of the correct interpretation.\n**Verdict: Incorrect.**\n\n**C. Under heavy right-censoring, both martingale and deviance residuals are symmetric with mean zero, so the choice between them does not matter. To diagnose time-varying effects, regress Schoenfeld residuals for $X_k$ on the covariate $X_k$ itself and test whether the slope differs from zero; a nonzero slope indicates time variation in $\\beta_k(t)$.**\nThis option is incorrect. Martingale residuals are not symmetric under heavy censoring. Further, it proposes regressing Schoenfeld residuals on the covariate $X_k$, which is a procedure for checking functional form (linearity), not for testing the proportional hazards assumption (time-invariance). A test for time-variance requires relating the residuals to time.\n**Verdict: Incorrect.**\n\n**D. Under heavy right-censoring, martingale residuals tend to cluster tightly around zero, but deviance residuals inflate to very large positive values; therefore, deviance residuals should be avoided. To diagnose time-varying effects, compute Schoenfeld residuals at both event times and censored times and test for independence from $\\log(t)$.**\nThis option presents a false description of the residuals' behavior. Under heavy censoring, most martingale residuals are large and negative, not clustered at zero. Deviance residuals do not \"inflate to very large positive values\". The description of the Schoenfeld residual test is also fundamentally flawed, as these residuals are not defined at censored times.\n**Verdict: Incorrect.**\n\nBased on this analysis, only option A provides a correct and coherent account of the statistical concepts and procedures.", "answer": "$$\\boxed{A}$$", "id": "3107112"}, {"introduction": "Not all data is right-censored; in many studies, an event is only known to have occurred within a specific interval, a situation known as interval censoring. This exercise introduces this challenge and a powerful framework for handling it: combining a parametric Accelerated Failure Time (AFT) model with the Expectation-Maximization (EM) algorithm. You will derive the key component of the EM algorithm, the $Q$-function, providing a blueprint for estimating model parameters when event times are not observed exactly [@problem_id:3107138].", "problem": "Consider independent event times $T_1,\\dots,T_n$ with covariates $\\mathbf{X}_i \\in \\mathbb{R}^p$, modeled by an accelerated failure time (AFT) specification $\\ln T_i = \\mathbf{X}_i^{\\top}\\beta + \\sigma \\epsilon_i$, where $\\epsilon_i$ has the standard Type I extreme value (Gumbel, minimum) distribution, so that $T_i \\mid \\mathbf{X}_i$ follows a Weibull distribution with shape $k = 1/\\sigma$ and scale $\\lambda_i = \\exp(\\mathbf{X}_i^{\\top}\\beta)$. The Weibull density and distribution functions for $T_i$ given $\\mathbf{X}_i$ are\n$$\nf_i(t \\mid \\beta,k) = \\frac{k}{\\lambda_i}\\left(\\frac{t}{\\lambda_i}\\right)^{k-1}\\exp\\!\\left(-\\left(\\frac{t}{\\lambda_i}\\right)^k\\right), \\quad\nF_i(t \\mid \\beta,k) = 1 - \\exp\\!\\left(-\\left(\\frac{t}{\\lambda_i}\\right)^k\\right),\n$$\nwith $\\lambda_i = \\exp(\\mathbf{X}_i^{\\top}\\beta)$ and $k>0$. Each $T_i$ is not observed exactly; instead, you observe an interval $(L_i, R_i]$ such that $T_i \\in (L_i, R_i]$, where left-censoring corresponds to $L_i = 0$, right-censoring corresponds to $R_i = \\infty$, and general interval censoring has $0  L_i  R_i  \\infty$.\n\nYou aim to estimate $(\\beta,k)$ via the Expectation–Maximization (EM) algorithm by treating the unobserved exact event times $T_i$ as latent complete data. Let the complete-data log-likelihood be the sum of $\\ln f_i(T_i \\mid \\beta,k)$ over $i=1,\\dots,n$. At EM iteration $m$, with current parameter values $(\\beta^{(m)},k^{(m)})$, the E-step computes the conditional expectation of the complete-data log-likelihood given the observed intervals and current parameters, denoted\n$$\nQ(\\beta,k \\mid \\beta^{(m)},k^{(m)}) \\equiv \\mathbb{E}\\!\\left[\\sum_{i=1}^{n} \\ln f_i(T_i \\mid \\beta,k)\\,\\bigg|\\, T_i \\in (L_i, R_i],\\, \\beta^{(m)},k^{(m)}\\right].\n$$\nStarting only from the definitions above and the laws of conditional expectations for truncated distributions, derive a closed-form expression for $Q(\\beta,k \\mid \\beta^{(m)},k^{(m)})$ up to an additive constant that does not depend on $(\\beta,k)$, expressed in terms of the covariates $\\mathbf{X}_i$ and the following conditional expectations computed under the current parameters $(\\beta^{(m)},k^{(m)})$:\n- $M_{1i}^{(m)} \\equiv \\mathbb{E}^{(m)}\\!\\left[\\ln T_i \\,\\big|\\, T_i \\in (L_i,R_i]\\right]$,\n- $M_{2i}^{(m)}(k) \\equiv \\mathbb{E}^{(m)}\\!\\left[T_i^{k} \\,\\big|\\, T_i \\in (L_i,R_i]\\right]$,\nwhere the superscript $(m)$ emphasizes that the conditional distribution used for these expectations is the truncated Weibull induced by $(\\beta^{(m)},k^{(m)})$.\n\nProvide your final answer as a single analytic expression for $Q(\\beta,k \\mid \\beta^{(m)},k^{(m)})$ up to an additive constant independent of $(\\beta,k)$. Do not simplify $M_{1i}^{(m)}$ or $M_{2i}^{(m)}(k)$ beyond their definitions. Your final answer must be a single closed-form expression and contain no units. No rounding is required.", "solution": "The user wants me to derive the Q-function for an interval-censored Weibull accelerated failure time (AFT) model for use in an Expectation-Maximization (EM) algorithm.\n\n### Step 1: Extract Givens\n\n-   **AFT Model**: $\\ln T_i = \\mathbf{X}_i^{\\top}\\beta + \\sigma \\epsilon_i$, where $T_i$ are independent event times and $\\mathbf{X}_i \\in \\mathbb{R}^p$ are covariates.\n-   **Error Distribution**: $\\epsilon_i$ follows a standard Type I extreme value (Gumbel, minimum) distribution.\n-   **Event Time Distribution**: $T_i \\mid \\mathbf{X}_i$ follows a Weibull distribution.\n-   **Weibull Parameters**: Shape parameter $k = 1/\\sigma$ and scale parameter $\\lambda_i = \\exp(\\mathbf{X}_i^{\\top}\\beta)$.\n-   **Weibull Probability Density Function (PDF)**: $f_i(t \\mid \\beta,k) = \\frac{k}{\\lambda_i}\\left(\\frac{t}{\\lambda_i}\\right)^{k-1}\\exp\\!\\left(-\\left(\\frac{t}{\\lambda_i}\\right)^k\\right)$.\n-   **Weibull Cumulative Distribution Function (CDF)**: $F_i(t \\mid \\beta,k) = 1 - \\exp\\!\\left(-\\left(\\frac{t}{\\lambda_i}\\right)^k\\right)$.\n-   **Observed Data**: Intervals $(L_i, R_i]$ for each observation $i=1, \\dots, n$, such that the true event time $T_i \\in (L_i, R_i]$.\n-   **Complete Data**: The unobserved exact event times $T_i$.\n-   **Complete-Data Log-Likelihood**: $\\sum_{i=1}^{n} \\ln f_i(T_i \\mid \\beta,k)$. Note the instruction to use $\\ln$ for the natural logarithm has been followed, replacing the `log` from the problem statement.\n-   **Q-function Definition**: $Q(\\beta,k \\mid \\beta^{(m)},k^{(m)}) \\equiv \\mathbb{E}\\!\\left[\\sum_{i=1}^{n} \\ln f_i(T_i \\mid \\beta,k)\\,\\bigg|\\, T_i \\in (L_i, R_i],\\, \\beta^{(m)},k^{(m)}\\right]$.\n-   **Required Conditional Expectations**:\n    -   $M_{1i}^{(m)} \\equiv \\mathbb{E}^{(m)}\\!\\left[\\ln T_i \\,\\big|\\, T_i \\in (L_i,R_i]\\right]$.\n    -   $M_{2i}^{(m)}(k) \\equiv \\mathbb{E}^{(m)}\\!\\left[T_i^{k} \\,\\big|\\, T_i \\in (L_i,R_i]\\right]$.\n    The superscript $(m)$ on $\\mathbb{E}^{(m)}$ indicates the expectation is taken with respect to the distribution defined by parameters $(\\beta^{(m)}, k^{(m)})$.\n\n### Step 2: Validate Using Extracted Givens\n\n-   **Scientifically Grounded**: The problem is based on standard, well-established principles of survival analysis, statistical modeling (Weibull AFT model), and computational statistics (EM algorithm). The connection between the Gumbel error distribution and the Weibull event time distribution in an AFT model is correct. All elements are scientifically sound.\n-   **Well-Posed**: The problem is well-defined. It asks for the derivation of a specific mathematical object (the Q-function) under a clear set of definitions and constraints. A unique solution exists.\n-   **Objective**: The language is formal and mathematical, with no subjective or ambiguous statements.\n-   **Completeness and Consistency**: The problem provides all necessary definitions for the derivation, including the model, distributions, and the quantities in terms of which the final answer should be expressed. The setup is internally consistent.\n\n### Step 3: Verdict and Action\n\nThe problem is **valid**. I will proceed with the derivation.\n\n### Derivation of the Q-function\n\nThe objective is to compute the $Q$-function, which is the conditional expectation of the complete-data log-likelihood given the observed data (the intervals) and the current parameter estimates $(\\beta^{(m)}, k^{(m)})$.\n\nThe $Q$-function is defined as:\n$$\nQ(\\beta,k \\mid \\beta^{(m)},k^{(m)}) = \\mathbb{E}^{(m)}\\!\\left[\\sum_{i=1}^{n} \\ln f_i(T_i \\mid \\beta,k)\\,\\bigg|\\, \\{T_j \\in (L_j, R_j]\\}_{j=1}^n \\right]\n$$\nBy linearity of expectation and the independence of the observations $T_i$, we can write this as a sum of individual expectations:\n$$\nQ(\\beta,k \\mid \\beta^{(m)},k^{(m)}) = \\sum_{i=1}^{n} \\mathbb{E}^{(m)}\\!\\left[\\ln f_i(T_i \\mid \\beta,k)\\,\\bigg|\\, T_i \\in (L_i, R_i]\\right]\n$$\nLet's first expand the log-likelihood term for a single observation, $\\ln f_i(T_i \\mid \\beta,k)$. Using the provided PDF:\n$$\nf_i(T_i \\mid \\beta,k) = \\frac{k}{\\lambda_i}\\left(\\frac{T_i}{\\lambda_i}\\right)^{k-1}\\exp\\!\\left(-\\left(\\frac{T_i}{\\lambda_i}\\right)^k\\right)\n$$\nTaking the natural logarithm, we get:\n$$\n\\ln f_i(T_i \\mid \\beta,k) = \\ln\\left(\\frac{k}{\\lambda_i}\\right) + (k-1)\\ln\\left(\\frac{T_i}{\\lambda_i}\\right) - \\left(\\frac{T_i}{\\lambda_i}\\right)^k\n$$\n$$\n= \\ln(k) - \\ln(\\lambda_i) + (k-1)(\\ln(T_i) - \\ln(\\lambda_i)) - \\frac{T_i^k}{\\lambda_i^k}\n$$\n$$\n= \\ln(k) - \\ln(\\lambda_i) + (k-1)\\ln(T_i) - (k-1)\\ln(\\lambda_i) - \\frac{T_i^k}{\\lambda_i^k}\n$$\nCombining the terms with $\\ln(\\lambda_i)$:\n$$\n\\ln f_i(T_i \\mid \\beta,k) = \\ln(k) + (k-1)\\ln(T_i) - k\\ln(\\lambda_i) - \\frac{T_i^k}{\\lambda_i^k}\n$$\nWe are given that $\\lambda_i = \\exp(\\mathbf{X}_i^{\\top}\\beta)$. Therefore, $\\ln(\\lambda_i) = \\mathbf{X}_i^{\\top}\\beta$. Substituting this into the expression:\n$$\n\\ln f_i(T_i \\mid \\beta,k) = \\ln(k) + (k-1)\\ln(T_i) - k\\mathbf{X}_i^{\\top}\\beta - T_i^k \\exp(-k\\mathbf{X}_i^{\\top}\\beta)\n$$\nNow, we take the conditional expectation of this expression with respect to the distribution of $T_i$ conditional on $T_i \\in (L_i, R_i]$, using the parameters $(\\beta^{(m)}, k^{(m)})$. Let us denote this expectation by $\\mathbb{E}^{(m)}[\\cdot \\mid T_i \\in (L_i,R_i]]$. The expectation is taken over the random variable $T_i$. The parameters $(\\beta, k)$ are the arguments of the function we are maximizing in the M-step, so they are treated as fixed values with respect to the expectation operator.\n$$\n\\mathbb{E}^{(m)}\\!\\left[\\ln f_i(T_i \\mid \\beta,k)\\,\\bigg|\\, T_i \\in (L_i, R_i]\\right] = \\mathbb{E}^{(m)}\\!\\left[\\ln(k) + (k-1)\\ln(T_i) - k\\mathbf{X}_i^{\\top}\\beta - T_i^k \\exp(-k\\mathbf{X}_i^{\\top}\\beta)\\,\\bigg|\\, T_i \\in (L_i, R_i]\\right]\n$$\nApplying linearity of expectation:\n$$\n= \\mathbb{E}^{(m)}\\left[\\ln(k)\\right] + (k-1)\\mathbb{E}^{(m)}\\left[\\ln(T_i)\\,\\big|\\, T_i \\in (L_i,R_i]\\right] - k\\mathbf{X}_i^{\\top}\\beta - \\exp(-k\\mathbf{X}_i^{\\top}\\beta) \\mathbb{E}^{(m)}\\left[T_i^k\\,\\big|\\, T_i \\in (L_i,R_i]\\right]\n$$\nThe terms $\\ln(k)$, $k-1$, $k\\mathbf{X}_i^{\\top}\\beta$, and $\\exp(-k\\mathbf{X}_i^{\\top}\\beta)$ do not involve the random variable $T_i$, so they are treated as constants within the expectation. Now, we substitute the defined quantities $M_{1i}^{(m)}$ and $M_{2i}^{(m)}(k)$:\n$$\n\\mathbb{E}^{(m)}\\!\\left[\\ln f_i(T_i \\mid \\beta,k)\\,\\bigg|\\, T_i \\in (L_i, R_i]\\right] = \\ln(k) + (k-1)M_{1i}^{(m)} - k\\mathbf{X}_i^{\\top}\\beta - M_{2i}^{(m)}(k) \\exp(-k\\mathbf{X}_i^{\\top}\\beta)\n$$\nThe full $Q$-function is the sum of these terms over $i=1, \\dots, n$:\n$$\nQ(\\beta,k \\mid \\beta^{(m)},k^{(m)}) = \\sum_{i=1}^{n} \\left[ \\ln(k) + (k-1)M_{1i}^{(m)} - k\\mathbf{X}_i^{\\top}\\beta - M_{2i}^{(m)}(k) \\exp(-k\\mathbf{X}_i^{\\top}\\beta) \\right]\n$$\nWe can regroup the terms in the summation:\n$$\nQ(\\beta,k \\mid \\beta^{(m)},k^{(m)}) = n\\ln(k) + (k-1)\\sum_{i=1}^{n} M_{1i}^{(m)} - k\\sum_{i=1}^{n}\\mathbf{X}_i^{\\top}\\beta - \\sum_{i=1}^{n} M_{2i}^{(m)}(k) \\exp(-k\\mathbf{X}_i^{\\top}\\beta)\n$$\nThe problem asks for the expression up to an additive constant that does not depend on the optimization parameters $(\\beta, k)$. We can expand the term $(k-1)\\sum_{i=1}^{n} M_{1i}^{(m)}$:\n$$\nQ(\\beta,k \\mid \\beta^{(m)},k^{(m)}) = n\\ln(k) + k\\sum_{i=1}^{n} M_{1i}^{(m)} - \\sum_{i=1}^{n} M_{1i}^{(m)} - k\\sum_{i=1}^{n}\\mathbf{X}_i^{\\top}\\beta - \\sum_{i=1}^{n} M_{2i}^{(m)}(k) \\exp(-k\\mathbf{X}_i^{\\top}\\beta)\n$$\nThe term $-\\sum_{i=1}^{n} M_{1i}^{(m)}$ does not depend on $(\\beta, k)$, since $M_{1i}^{(m)}$ is a pre-computed value from the E-step using parameters $(\\beta^{(m)}, k^{(m)})$. Therefore, this term is an additive constant with respect to the maximization in the M-step and can be dropped.\n\nThe final expression for the $Q$-function, up to an additive constant independent of $(\\beta,k)$, is:\n$$\nn\\ln(k) + k\\sum_{i=1}^{n} M_{1i}^{(m)} - k\\sum_{i=1}^{n}\\mathbf{X}_i^{\\top}\\beta - \\sum_{i=1}^{n} M_{2i}^{(m)}(k) \\exp(-k\\mathbf{X}_i^{\\top}\\beta)\n$$\nThis expression can be rearranged for clarity:\n$$\nn\\ln(k) + k \\sum_{i=1}^{n} \\left(M_{1i}^{(m)} - \\mathbf{X}_i^{\\top}\\beta\\right) - \\sum_{i=1}^{n} M_{2i}^{(m)}(k) \\exp(-k\\mathbf{X}_i^{\\top}\\beta)\n$$\nThis is the desired closed-form expression.", "answer": "$$\\boxed{n\\ln(k) + k \\sum_{i=1}^{n} \\left(M_{1i}^{(m)} - \\mathbf{X}_i^{\\top}\\beta\\right) - \\sum_{i=1}^{n} M_{2i}^{(m)}(k) \\exp(-k\\mathbf{X}_i^{\\top}\\beta)}$$", "id": "3107138"}]}