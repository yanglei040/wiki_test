{"hands_on_practices": [{"introduction": "The cornerstone of solving for optimal policies when the environment's dynamics are fully known is dynamic programming. This exercise provides a direct application of the Bellman optimality equation through backward induction in a finite-horizon setting. By working through the calculations, you will gain a concrete understanding of how optimal actions can change depending on the time remaining, illustrating the important concept of non-stationary policies. [@problem_id:3190850]", "problem": "Consider the following finite-horizon Markov Decision Process (MDP) with horizon $H = 3$. The state space is $\\mathcal{S} = \\{s, j, z\\}$. The action space depends on the state: at state $s$ the agent can choose between actions $a$ and $b$; at states $j$ and $z$ there is a single available action. The one-step reward function $r(\\cdot,\\cdot)$ and transition dynamics are given as follows:\n- If the current state is $s$ and action $a$ is chosen, then the immediate reward is $r(s,a) = 2$, and the next state is $z$ with probability $1$.\n- If the current state is $s$ and action $b$ is chosen, then the immediate reward is $r(s,b) = 0$, and the next state is $j$ with probability $\\tfrac{1}{2}$ and $s$ with probability $\\tfrac{1}{2}$.\n- If the current state is $j$, the immediate reward is $r(j,\\cdot) = 5$, and the next state is $z$ with probability $1$.\n- If the current state is $z$, the immediate reward is $r(z,\\cdot) = 0$, and the next state is $z$ with probability $1$.\n\nAssume episodes have exactly $H = 3$ decision stages, and rewards are accumulated at each stage $t \\in \\{1,2,3\\}$ from the current state-action pair at that stage. Let a time-dependent policy be denoted by $\\pi = \\{\\pi_{1}, \\pi_{2}, \\pi_{3}\\}$, where $\\pi_{t}$ maps the current state to an action at time $t$. Let the finite-horizon value function of a policy be defined by\n$$\nV_{t}^{\\pi}(x) \\equiv \\mathbb{E}^{\\pi}\\!\\left[\\sum_{k=t}^{H} r\\!\\left(S_{k}, A_{k}\\right)\\,\\middle|\\, S_{t} = x \\right],\n$$\nfor any state $x \\in \\mathcal{S}$ and time $t \\in \\{1,2,3\\}$. Here $\\mathbb{E}^{\\pi}[\\cdot]$ denotes the expectation under the trajectory distribution induced by policy $\\pi$, the Markov property, and the given transition probabilities.\n\nUsing only these definitions, the Markov property, and the law of total expectation, do the following:\n1) Determine, for each time $t \\in \\{1,2,3\\}$, which action maximizes the expected return at state $s$ under an optimal policy, and thereby justify whether the optimal policy is stationary or non-stationary over the horizon.\n2) Compute the optimal finite-horizon values $V_{t}^{\\ast}(s)$ for $t \\in \\{1,2,3\\}$ and compare them across $t$, explaining why they differ.\n\nProvide the exact value of $V_{1}^{\\ast}(s)$ as a reduced fraction as your final answer. Do not round; provide an exact value.", "solution": "The problem provides a complete and consistent description of a finite-horizon Markov Decision Process (MDP). It is scientifically grounded in the theory of stochastic optimal control, well-posed, and objective. All necessary parameters—state space $\\mathcal{S}$, action space $\\mathcal{A}$, reward function $r$, transition probabilities $P$, and horizon $H$—are clearly defined. The problem is valid and can be solved using the standard method of dynamic programming (backward induction).\n\nThe value function for a policy $\\pi = \\{\\pi_1, \\pi_2, \\dots, \\pi_H\\}$ starting from state $x$ at time $t$ is defined as:\n$$V_{t}^{\\pi}(x) = \\mathbb{E}^{\\pi}\\left[\\sum_{k=t}^{H} r(S_k, A_k) \\, \\middle| \\, S_t = x \\right]$$\nWe can derive the recursive relationship for this value function. By splitting the sum and applying the law of total expectation:\n$$V_{t}^{\\pi}(x) = \\mathbb{E}^{\\pi}\\left[r(S_t, A_t) + \\sum_{k=t+1}^{H} r(S_k, A_k) \\, \\middle| \\, S_t = x \\right]$$\n$$V_{t}^{\\pi}(x) = \\mathbb{E}^{\\pi}[r(S_t, A_t) | S_t = x] + \\mathbb{E}^{\\pi}\\left[\\sum_{k=t+1}^{H} r(S_k, A_k) \\, \\middle| \\, S_t = x \\right]$$\nGiven the policy $A_t = \\pi_t(S_t)$, the first term is $r(x, \\pi_t(x))$. For the second term, we condition on the next state $S_{t+1}$:\n$$\\mathbb{E}^{\\pi}\\left[\\sum_{k=t+1}^{H} r(S_k, A_k) \\, \\middle| \\, S_t = x \\right] = \\sum_{x' \\in \\mathcal{S}} P(x'|x, \\pi_t(x)) \\, \\mathbb{E}^{\\pi}\\left[\\sum_{k=t+1}^{H} r(S_k, A_k) \\, \\middle| \\, S_{t+1} = x' \\right]$$\nThe inner expectation is, by definition, $V_{t+1}^{\\pi}(x')$. Thus, we obtain the Bellman equation for a given policy $\\pi$:\n$$V_{t}^{\\pi}(x) = r(x, \\pi_t(x)) + \\sum_{x' \\in \\mathcal{S}} P(x'|x, \\pi_t(x)) V_{t+1}^{\\pi}(x')$$\nThe optimal value function, $V_t^*(x) = \\max_{\\pi} V_t^\\pi(x)$, is found by choosing the action that maximizes the right-hand side at each step. This gives the Bellman optimality equation for a finite horizon:\n$$V_t^*(x) = \\max_{u \\in \\mathcal{A}(x)} \\left\\{ r(x, u) + \\sum_{x' \\in \\mathcal{S}} P(x'|x, u) V_{t+1}^*(x') \\right\\}$$\nThe process starts at the final time step $H$ and works backward. For $t=H=3$, the sum in the value function definition has only one term:\n$$V_3^{\\pi}(x) = \\mathbb{E}^{\\pi}[r(S_3, A_3) | S_3=x] = r(x, \\pi_3(x))$$\nTherefore, the optimal value function at time $t=3$ is simply the maximum immediate reward:\n$$V_3^*(x) = \\max_{u \\in \\mathcal{A}(x)} r(x, u)$$\nThis can be seen as applying the Bellman equation with a terminal condition $V_{H+1}^*(x) = V_4^*(x) = 0$ for all states $x \\in \\mathcal{S}$.\n\nWe now solve the problem by backward induction. The horizon is $H=3$.\n\n**Time Step $t=3$:**\nWe compute $V_3^*(x)$ for each state $x \\in \\{s, j, z\\}$.\n-   For state $s$: $V_3^*(s) = \\max_{u \\in \\{a,b\\}} r(s,u) = \\max\\{r(s,a), r(s,b)\\} = \\max\\{2, 0\\} = 2$. The optimal action is $\\pi_3^*(s) = a$.\n-   For state $j$: There is a single action. $V_3^*(j) = r(j, \\cdot) = 5$.\n-   For state $z$: There is a single action. $V_3^*(z) = r(z, \\cdot) = 0$.\n\n**Time Step $t=2$:**\nWe compute $V_2^*(x)$ using the values of $V_3^*(x')$.\n$$V_2^*(x) = \\max_{u \\in \\mathcal{A}(x)} \\left\\{ r(x, u) + \\sum_{x' \\in \\mathcal{S}} P(x'|x, u) V_3^*(x') \\right\\}$$\n-   For state $s$: We compare the value of taking action $a$ versus action $b$.\n    -   Action $a$: The expected return is $r(s, a) + P(z|s,a)V_3^*(z) = 2 + 1 \\cdot 0 = 2$.\n    -   Action $b$: The expected return is $r(s, b) + P(j|s,b)V_3^*(j) + P(s|s,b)V_3^*(s) = 0 + \\frac{1}{2} \\cdot V_3^*(j) + \\frac{1}{2} \\cdot V_3^*(s) = \\frac{1}{2}(5) + \\frac{1}{2}(2) = \\frac{5}{2} + 1 = \\frac{7}{2}$.\n    -   $V_2^*(s) = \\max\\{2, \\frac{7}{2}\\} = \\frac{7}{2}$. The optimal action is $\\pi_2^*(s) = b$.\n-   For state $j$: $V_2^*(j) = r(j, \\cdot) + P(z|j, \\cdot)V_3^*(z) = 5 + 1 \\cdot 0 = 5$.\n-   For state $z$: $V_2^*(z) = r(z, \\cdot) + P(z|z, \\cdot)V_3^*(z) = 0 + 1 \\cdot 0 = 0$.\n\n**Time Step $t=1$:**\nWe compute $V_1^*(x)$ using the values of $V_2^*(x')$.\n$$V_1^*(x) = \\max_{u \\in \\mathcal{A}(x)} \\left\\{ r(x, u) + \\sum_{x' \\in \\mathcal{S}} P(x'|x, u) V_2^*(x') \\right\\}$$\n-   For state $s$: We compare the value of taking action $a$ versus action $b$.\n    -   Action $a$: The expected return is $r(s, a) + P(z|s,a)V_2^*(z) = 2 + 1 \\cdot 0 = 2$.\n    -   Action $b$: The expected return is $r(s, b) + P(j|s,b)V_2^*(j) + P(s|s,b)V_2^*(s) = 0 + \\frac{1}{2} \\cdot V_2^*(j) + \\frac{1}{2} \\cdot V_2^*(s) = \\frac{1}{2}(5) + \\frac{1}{2}(\\frac{7}{2}) = \\frac{5}{2} + \\frac{7}{4} = \\frac{10}{4} + \\frac{7}{4} = \\frac{17}{4}$.\n    -   $V_1^*(s) = \\max\\{2, \\frac{17}{4}\\} = \\frac{17}{4}$. The optimal action is $\\pi_1^*(s) = b$.\n-   For state $j$: $V_1^*(j) = r(j, \\cdot) + P(z|j, \\cdot)V_2^*(z) = 5 + 1 \\cdot 0 = 5$.\n-   For state $z$: $V_1^*(z) = r(z, \\cdot) + P(z|z, \\cdot)V_2^*(z) = 0 + 1 \\cdot 0 = 0$.\n\nWe can now address the specific questions.\n\n1) For each time $t \\in \\{1,2,3\\}$, the optimal action at state $s$ is:\n-   At $t=3$, $\\pi_3^*(s) = a$, because $r(s,a)=2 > r(s,b)=0$.\n-   At $t=2$, $\\pi_2^*(s) = b$, because the expected future return from action $b$ (which is $\\frac{7}{2}$) is greater than the return from action $a$ (which is $2$).\n-   At $t=1$, $\\pi_1^*(s) = b$, because the expected future return from action $b$ (which is $\\frac{17}{4}$) is greater than the return from action $a$ (which is $2$).\nA policy is stationary if the optimal action for any given state does not change with time. Here, the optimal action for state $s$ is not constant: $\\pi_1^*(s) = b$, $\\pi_2^*(s) = b$, but $\\pi_3^*(s) = a$. Therefore, the optimal policy $\\pi^*$ is **non-stationary**. This is a characteristic feature of finite-horizon MDPs; the choice of action depends on the time remaining. At $t=3$, with no future, the agent greedily takes the immediate reward. At $t=1$ and $t=2$, the agent forgoes an immediate reward of $2$ by choosing action $b$ in favor of a chance to transition to states with higher future value ($j$ and $s$ itself), leading to a greater overall expected return.\n\n2) The optimal finite-horizon values $V_t^*(s)$ for $t \\in \\{1,2,3\\}$ are:\n-   $V_1^*(s) = \\frac{17}{4} = 4.25$\n-   $V_2^*(s) = \\frac{7}{2} = 3.5$\n-   $V_3^*(s) = 2$\nComparing them, we see that $V_1^*(s) > V_2^*(s) > V_3^*(s)$.\nThe value $V_t^*(s)$ represents the maximum possible expected total reward accumulated from time $t$ to the end of the horizon at $t=3$.\n$V_3^*(s)$ is the expected reward over one stage ($k=3$).\n$V_2^*(s)$ is the expected reward over two stages ($k=2,3$).\n$V_1^*(s)$ is the expected reward over three stages ($k=1,2,3$).\nSince all one-step rewards in this MDP are non-negative, the cumulative reward is non-decreasing as the number of stages increases. At each step before the last, there is an opportunity to accumulate more reward. For example, $V_2^*(s)$ is greater than $V_3^*(s)$ because starting at $t=2$ gives an additional time step to gather rewards compared to starting at $t=3$. The strict inequality holds because at $t=2$, the optimal action at $s$ leads to states where positive rewards can be obtained at $t=3$. Similarly, $V_1^*(s)$ is greater than $V_2^*(s)$ because there is one more stage ($t=1$) to collect rewards.\n\nThe final answer required is the exact value of $V_1^*(s)$.\n$$V_1^*(s) = \\frac{17}{4}$$", "answer": "$$\\boxed{\\frac{17}{4}}$$", "id": "3190850"}, {"introduction": "In most real-world scenarios, the model of the environment is unknown, and an agent must learn from direct interaction. This practice shifts our focus to Q-learning, a fundamental model-free algorithm, and the associated challenge of ensuring adequate exploration. You will analyze how \"optimistic initialization\"—a simple yet powerful heuristic—drives the agent to try new actions, and calculate precisely how this initial optimism influences the learning process. [@problem_id:3190816]", "problem": "Consider a deterministic Markov Decision Process (MDP) with finite state set and bounded rewards. Let the agent use tabular action-value learning with the one-step update\n$$\nQ_{t+1}(s_{t},a_{t}) \\leftarrow (1-\\alpha)Q_{t}(s_{t},a_{t}) + \\alpha\\left(r_{t} + \\gamma \\max_{a'} Q_{t}(s_{t+1},a')\\right),\n$$\nwhere $Q_{0}(s,a)=Q_{0}0$ for all state-action pairs, the learning rate is $\\alpha \\in (0,1]$, and the discount factor is $\\gamma \\in (0,1)$. The agent selects actions greedily with respect to $Q_{t}$ at each decision time $t$, and ties are broken by a fixed, state-dependent but arbitrary ordering. Rewards are bounded by $0 \\le r_{t} \\le R_{\\max}$.\n\nPart A (conceptual). Using only the definitions above, and without assuming any particular MDP structure beyond determinism and bounded rewards, justify why an optimistic initialization $Q_{0}  \\frac{R_{\\max}}{1-\\gamma}$ encourages exploration under a purely greedy policy. In particular, argue that in any state, once an action has been tried at least once, its $Q$-value drops below the $Q$-values of any as-yet-untried actions in that state, ensuring the agent will eventually try all actions available in that state.\n\nPart B (calculation). Now specialize to a deterministic gridworld with a single nonterminal start cell $s$ and a terminal goal cell $g$. There are exactly two actions available in $s$:\n- Action $a_{R}$ deterministically moves to $g$ and yields reward $r=1$, after which the episode ends and the agent is reset to $s$ with no further reward.\n- Action $a_{L}$ deterministically leaves the agent in $s$ and yields reward $r=0$.\n\nLet the agent use the same one-step update with learning rate $\\alpha=1$, discount factor $\\gamma \\in (0,1)$, greedy action selection with ties broken in favor of $a_{L}$, and initialization $Q_{0}(s,a_{L})=Q_{0}(s,a_{R})=Q_{0}0$, while $Q_{0}(g,\\cdot)=0$. Define the convergence time $T$ to be the smallest integer such that, after exactly $T$ action selections and their associated updates, the greedy action at the next visit to $s$ is $a_{R}$ and remains $a_{R}$ for all subsequent visits to $s$.\n\nDerive a closed-form expression for $T$ as a function of $Q_{0}$ and $\\gamma$. Your final answer must be a single analytic expression, not an inequality or equation. No numerical rounding is required.", "solution": "The user has provided a two-part problem concerning Q-learning in a deterministic Markov Decision Process. The problem is well-posed, scientifically grounded in the principles of reinforcement learning, and contains all necessary information for a complete solution.\n\n### Part A: Justification of Exploration via Optimistic Initialization\n\nThe goal is to justify why an optimistic initialization $Q_{0}  \\frac{R_{\\max}}{1-\\gamma}$ encourages exploration under a purely greedy policy. Exploration, in this context, means that the agent will eventually try all available actions in any given state.\n\nFirst, we must understand the significance of the quantity $\\frac{R_{\\max}}{1-\\gamma}$. For any state $s$ and any policy $\\pi$, the state-value function $V^{\\pi}(s)$ is the expected discounted sum of future rewards starting from state $s$ and following policy $\\pi$. Since rewards are bounded such that $r_t \\le R_{\\max}$, the maximum possible value of any state is bounded by the sum of discounted maximum rewards:\n$$V^*(s) = \\max_{\\pi} V^{\\pi}(s) \\le \\sum_{t=0}^{\\infty} \\gamma^t R_{\\max} = \\frac{R_{\\max}}{1-\\gamma}$$\nThe optimal action-value function $Q^*(s,a)$ is similarly bounded. Therefore, the condition $Q_{0}(s,a) = Q_{0}  \\frac{R_{\\max}}{1-\\gamma}$ for all $(s,a)$ pairs means that all initial Q-values are strictly greater than any possible optimal Q-value. This is the definition of \"optimistic initialization\".\n\nNow, we argue that any action, once taken, will have its Q-value updated to a value strictly less than $Q_0$. This will make any untried actions in that state appear more appealing to a greedy agent, thereby encouraging the agent to select them.\n\nLet's prove by induction that for all $t \\ge 1$, any Q-value $Q_t(s,a)$ that has been updated at least once is strictly less than $Q_0$, and any Q-value that has not been updated remains at $Q_0$.\nLet $(s_t, a_t)$ be the state-action pair selected at decision time $t$. The update rule is:\n$$Q_{t+1}(s_{t},a_{t}) = (1-\\alpha)Q_{t}(s_{t},a_{t}) + \\alpha\\left(r_{t} + \\gamma \\max_{a'} Q_{t}(s_{t+1},a')\\right)$$\nAll other Q-values remain unchanged: $Q_{t+1}(s,a) = Q_t(s,a)$ for $(s,a) \\neq (s_t,a_t)$.\n\n**Base Case:** At $t=0$, no values have been updated. All values are $Q_0$. Suppose at $t=0$, the agent takes action $a_0$ in state $s_0$. The Q-value $Q_0(s_0, a_0) = Q_0$.\nThe update rule gives:\n$$Q_{1}(s_{0},a_{0}) = (1-\\alpha)Q_{0} + \\alpha\\left(r_{0} + \\gamma \\max_{a'} Q_{0}(s_{1},a')\\right)$$\nSince $Q_0(s_1, a') = Q_0$ for all $a'$, the expression simplifies to:\n$$Q_{1}(s_{0},a_{0}) = (1-\\alpha)Q_{0} + \\alpha(r_{0} + \\gamma Q_{0})$$\nTo show that $Q_1(s_0, a_0)  Q_0$, we need to show that the target value, $r_0 + \\gamma Q_0$, is less than $Q_0$. From the given condition $Q_0  \\frac{R_{\\max}}{1-\\gamma}$, it follows that $Q_0(1-\\gamma)  R_{\\max}$. Since $r_0 \\le R_{\\max}$, we have $r_0  Q_0(1-\\gamma)$, which rearranges to $r_0 + \\gamma Q_0  Q_0$.\nThe new value $Q_1(s_0, a_0)$ is a convex combination of $Q_0$ and a value strictly less than $Q_0$. Since $\\alpha \\in (0,1]$, the new value must be strictly less than $Q_0$. All other Q-values remain at $Q_0$.\n\n**Inductive Step:** Assume that at time $t$, all Q-values satisfy $Q_t(s,a) \\le Q_0$. Let's compute the update for $(s_t, a_t)$. The target value is $T_t = r_t + \\gamma \\max_{a'} Q_t(s_{t+1}, a')$.\nBy the inductive hypothesis, $\\max_{a'} Q_t(s_{t+1}, a') \\le Q_0$. Thus, the target is bounded:\n$$T_t \\le r_t + \\gamma Q_0$$\nAs established before, $r_t + \\gamma Q_0  Q_0$. So, $T_t  Q_0$.\nThe new value is $Q_{t+1}(s_t, a_t) = (1-\\alpha)Q_t(s_t, a_t) + \\alpha T_t$. This is a convex combination of $Q_t(s_t, a_t) \\le Q_0$ and $T_t  Q_0$. Therefore, $Q_{t+1}(s_t, a_t)  Q_0$.\nFor any other pair $(s,a) \\neq (s_t, a_t)$, $Q_{t+1}(s,a) = Q_t(s,a) \\le Q_0$.\nThus, by induction, any Q-value, once updated, becomes strictly less than $Q_0$.\n\n**Conclusion:** Consider any state $s$. Let $A_{tried}$ be the set of actions that have been tried in state $s$, and $A_{untried}$ be the set of actions that have not. For any $a_{tried} \\in A_{tried}$, its value $Q_t(s, a_{tried})$ has been updated at least once, so $Q_t(s, a_{tried})  Q_0$. For any $a_{untried} \\in A_{untried}$, its value has never been updated, so $Q_t(s, a_{untried}) = Q_0$.\nA greedy agent at state $s$ selects the action $a = \\arg\\max_{a'} Q_t(s, a')$. The maximum value is $Q_0$, which corresponds to any of the actions in $A_{untried}$. Therefore, the agent is guaranteed to select an action from $A_{untried}$ (with the specific choice determined by the tie-breaking rule). This process continues until $A_{untried}$ is empty, ensuring that every action in state $s$ is eventually tried. This demonstrates that optimistic initialization with a greedy policy encourages systematic exploration.\n\n### Part B: Calculation of Convergence Time\n\nThe problem specifies a deterministic MDP with two states, $s$ and $g$, and two actions from $s$, $a_L$ and $a_R$. We are given $\\alpha=1$. The one-step update simplifies to:\n$$Q_{t+1}(s_t, a_t) \\leftarrow r_t + \\gamma \\max_{a'} Q_t(s_{t+1}, a')$$\n\nLet's trace the evolution of the Q-values, $Q(s, a_L)$ and $Q(s, a_R)$.\nInitial values: $Q_0(s, a_L) = Q_0$ and $Q_0(s, a_R) = Q_0$. Terminal state values are $Q_t(g, \\cdot) = 0$ for all $t$.\n\n**Step 0 (t=0):**\n- State: $s_0=s$. Q-values are tied: $Q_0(s, a_L) = Q_0(s, a_R)$.\n- Action: Tie-breaking favors $a_L$, so $a_0 = a_L$.\n- Outcome: Reward $r_0=0$, next state $s_1=s$.\n- Update: $Q_1(s, a_L) \\leftarrow r_0 + \\gamma \\max_{a'} Q_0(s, a') = 0 + \\gamma \\max(Q_0, Q_0) = \\gamma Q_0$.\n- Other Q-value is unchanged: $Q_1(s, a_R) = Q_0(s, a_R) = Q_0$.\n- After 1 action, the values are $Q_1(s, a_L) = \\gamma Q_0$ and $Q_1(s, a_R) = Q_0$.\n\n**Step 1 (t=1):**\n- State: $s_1=s$. Since $\\gamma \\in (0,1)$ and $Q_00$, we have $Q_1(s, a_R)  Q_1(s, a_L)$.\n- Action: The greedy choice is $a_1 = a_R$.\n- Outcome: Reward $r_1=1$, next state $s_2=g$ (terminal). Episode ends.\n- Update: $Q_2(s, a_R) \\leftarrow r_1 + \\gamma \\max_{a'} Q_1(g, a') = 1 + \\gamma \\cdot 0 = 1$.\n- Other Q-value is unchanged: $Q_2(s, a_L) = Q_1(s, a_L) = \\gamma Q_0$.\n- After 2 actions, the agent is reset to state $s$, and the values are $Q_2(s, a_L) = \\gamma Q_0$ and $Q_2(s, a_R) = 1$.\n\n**Subsequent Steps (t $\\ge$ 2):**\nThe agent is now at state $s$ and compares $Q(s, a_L) = \\gamma Q_0$ with $Q(s, a_R) = 1$. The greedy choice depends on which is larger.\nLet $m$ be the number of additional times $a_L$ is chosen consecutively. This happens as long as $Q(s,a_L) \\ge Q(s,a_R)$.\n\n- If $\\gamma Q_0  1$, at $t=2$ the agent chooses $a_R$. The update is $Q_3(s, a_R) \\leftarrow 1$, so the value does not change. The choice of $a_R$ becomes stable. Convergence is reached after $T=2$ steps. In this case, $m=0$.\n\n- If $\\gamma Q_0 \\ge 1$, at $t=2$ the agent chooses $a_L$.\n  - Update: $Q_3(s, a_L) \\leftarrow 0 + \\gamma \\max\\{Q_2(s, a_L), Q_2(s, a_R)\\} = \\gamma \\max\\{\\gamma Q_0, 1\\} = \\gamma (\\gamma Q_0) = \\gamma^2 Q_0$.\n  - $Q_3(s, a_R)=1$.\n- At $t=3$, the agent compares $\\gamma^2 Q_0$ and $1$. It will choose $a_L$ again if $\\gamma^2 Q_0 \\ge 1$.\n\nThis pattern continues. Let $m$ be the number of times $a_L$ is chosen after the initial $(a_L, a_R)$ sequence. $a_L$ is chosen for the $(k+1)$-th time (where $k=0, 1, ..., m-1$) if the condition $Q(s,a_L) \\ge 1$ holds. After the initial episode (2 steps) and $k$ additional $a_L$ choices, the value is $Q(s, a_L) = \\gamma^{k+1} Q_0$. The condition to choose $a_L$ again is therefore $\\gamma^{k+1} Q_0 \\ge 1$.\n\n$m$ is the count of non-negative integers $k$ for which $\\gamma^{k+1} Q_0 \\ge 1$.\n$$\\gamma^{k+1} \\ge \\frac{1}{Q_0}$$\nTaking logarithms with base $1/\\gamma  1$:\n$$k+1 \\le \\log_{1/\\gamma}\\left(\\frac{1}{Q_0^{-1}}\\right) = \\log_{1/\\gamma}(Q_0)$$\n$$k \\le \\log_{1/\\gamma}(Q_0) - 1$$\nWe need to count the number of non-negative integers $k$ satisfying this.\nIf $\\log_{1/\\gamma}(Q_0)  1$ (i.e., $Q_0  1/\\gamma$ or $\\gamma Q_0  1$), then $\\log_{1/\\gamma}(Q_0) - 1  0$, so there are no non-negative integers $k$ satisfying the inequality. In this case, $m=0$.\nIf $\\log_{1/\\gamma}(Q_0) \\ge 1$, the non-negative integers are $k=0, 1, \\dots, \\lfloor \\log_{1/\\gamma}(Q_0) - 1 \\rfloor$. The count is $m = \\lfloor \\log_{1/\\gamma}(Q_0) - 1 \\rfloor + 1 = \\lfloor \\log_{1/\\gamma}(Q_0) \\rfloor$.\nCombining these cases, $m = \\max(0, \\lfloor \\log_{1/\\gamma}(Q_0) \\rfloor)$.\n\nThe total number of actions $T$ until convergence is the sum of actions taken.\n- $1$ action for the initial $a_L$.\n- $1$ action for the first $a_R$.\n- $m$ actions for the subsequent $a_L$ choices.\nTotal actions: $T = 1 + 1 + m = 2 + m$.\nAfter these $T$ actions, at decision time $t=T$, the value $Q_T(s, a_L)$ has become $\\gamma^{m+1} Q_0$. By the definition of $m$, we have $\\gamma^{m+1}Q_0  1$. So, at $t=T$, the agent compares $Q_T(s, a_L)  1$ and $Q_T(s, a_R) = 1$ and chooses $a_R$. This choice remains stable for all future visits. This matches the problem's definition of $T$.\n\nThe final expression for $T$ is:\n$$T = 2 + \\max\\left(0, \\left\\lfloor \\log_{1/\\gamma}(Q_0) \\right\\rfloor\\right)$$\nThis can also be written using the natural logarithm:\n$$T = 2 + \\max\\left(0, \\left\\lfloor \\frac{\\ln(Q_0)}{\\ln(1/\\gamma)} \\right\\rfloor\\right)$$", "answer": "$$ \\boxed{2 + \\max\\left(0, \\left\\lfloor \\frac{\\ln(Q_0)}{\\ln(1/\\gamma)} \\right\\rfloor\\right)} $$", "id": "3190816"}, {"introduction": "The true power of modern reinforcement learning comes from combining its algorithms with flexible function approximators, but this pairing introduces new challenges rooted in statistical learning. This thought experiment demonstrates a critical pitfall known as policy regression, where a seemingly \"improved\" policy based on an overfitted value function actually performs worse. By analyzing this scenario, you will see why principles like cross-validation are not just helpful but essential for developing robust RL agents from finite data. [@problem_id:3190845]", "problem": "Consider a finite-horizon, one-step Markov Decision Process (MDP) with a single nonterminal state $s$ and two actions $a$ and $b$. When either action is taken at $s$, the episode terminates immediately and yields a random reward. The reward distributions are as follows:\n- If action $a$ is taken, the reward $R_a$ is $+1$ with probability $0.5$ and $-1$ with probability $0.5$.\n- If action $b$ is taken, the reward $R_b$ is deterministically $+0.4$.\n\nSuppose a behavior policy $\\mu$ used to collect a static dataset selects actions uniformly at random at $s$, so that in the dataset there are $n_a=5$ samples from action $a$ and $n_b=5$ samples from action $b$. In one particular realization of this dataset, all five observed rewards for action $a$ happened to be $+1$, and all five observed rewards for action $b$ are $+0.4$. A flexible action-value function model $\\hat q(s,\\cdot)$ is fit by empirical risk minimization on these data and exactly interpolates the empirical means, yielding $\\hat q(s,a)=1$ and $\\hat q(s,b)=0.4$ on $s$.\n\nDefine a policy $\\pi$ by $\\pi(s)=b$ deterministically (for every episode, $b$ is chosen at $s$), and define a greedy policy $\\pi'$ with respect to $\\hat q$ by $\\pi'(s)=\\arg\\max_{u\\in\\{a,b\\}} \\hat q(s,u)$. Let the discount factor be $\\gamma=1$ and the horizon be the single time step. The value function $v^\\pi(s)$ is defined by the standard fixed-policy evaluation expression $v^\\pi(s)=\\mathbb{E}_\\pi\\left[\\sum_{t=0}^{T-1} \\gamma^t R_t \\mid S_0=s\\right]$, which reduces here to the expected immediate reward under the action distribution induced by the policy.\n\nFrom first principles—namely, the definitions of the value function $v^\\pi(s)$, the action-value function $q^\\pi(s,a)$ as the expected return from taking action $a$ in state $s$ and thereafter following policy $\\pi$, and the greedy improvement operator—determine which statements below are correct in this setting. Your reasoning should account for statistical learning effects (overfitting to finite data) and should start from the given definitions, not from any specialized policy improvement guarantees.\n\nA. In this MDP, $v^\\pi(s)$ under the current policy $\\pi$ exceeds $v^{\\pi'}(s)$ under the greedy policy $\\pi'$ induced by $\\hat q$, so greedy improvement with an overfitted $\\hat q$ strictly reduces the true value at $s$.\n\nB. A practical mitigation is to partition the data into a training set and a validation set, compute a sequence of greedy policies $\\{\\pi_t\\}$ as the $\\hat q$ model is trained over epochs $t$, estimate $v^{\\pi_t}(s)$ off-policy on the validation set, and stop at the smallest $t$ for which the validation estimate of $v^{\\pi_t}(s)$ fails to improve (cross-validated early stopping), thereby reducing the chance of policy regression due to overfitting.\n\nC. Because the empirical training error of $\\hat q$ decreases monotonically with training iterations, the true value $v^{\\pi_t}(s)$ of the induced greedy policy sequence necessarily increases monotonically, so policy regression cannot occur.\n\nD. If the behavior policy $\\mu$ explores both actions, importance sampling on the validation set always yields low-variance, unbiased estimates of $v^{\\pi_t}(s)$, making early stopping unnecessary.\n\nSelect all that apply.", "solution": "The problem statement is first validated to ensure it is scientifically sound, well-posed, and objective.\n\n### Step 1: Extract Givens\n-   **MDP Structure**: A finite-horizon, one-step Markov Decision Process (MDP).\n-   **State Space**: A single nonterminal state, denoted as $s$.\n-   **Action Space**: Two actions, $a$ and $b$.\n-   **Transition/Termination**: Any action taken at state $s$ leads to immediate episode termination.\n-   **Reward Distributions (True)**:\n    -   Action $a$: Reward $R_a$ is $+1$ with probability $0.5$ and $-1$ with probability $0.5$.\n    -   Action $b$: Reward $R_b$ is deterministically $+0.4$.\n-   **Behavior Policy**: $\\mu$ selects actions uniformly at random: $\\mu(a|s) = \\mu(b|s) = 0.5$.\n-   **Dataset**: A static dataset collected under $\\mu$ with $n_a=5$ samples for action $a$ and $n_b=5$ samples for action $b$.\n-   **Observed Data Realization**:\n    -   For all $5$ samples of action $a$, the observed reward was $+1$.\n    -   For all $5$ samples of action $b$, the observed reward was $+0.4$.\n-   **Action-Value Model**: A flexible model $\\hat q(s, \\cdot)$ is fit via empirical risk minimization.\n-   **Fitted Model Values**: The model exactly interpolates the empirical means, resulting in $\\hat q(s,a)=1$ and $\\hat q(s,b)=0.4$.\n-   **Policy Definitions**:\n    -   $\\pi$ is a deterministic policy: $\\pi(s)=b$.\n    -   $\\pi'$ is a greedy policy with respect to $\\hat q$: $\\pi'(s)=\\arg\\max_{u\\in\\{a,b\\}} \\hat q(s,u)$.\n-   **Parameters**: Discount factor $\\gamma=1$, horizon $T=1$.\n-   **Value Function Definition**: $v^\\pi(s)=\\mathbb{E}_\\pi\\left[\\sum_{t=0}^{T-1} \\gamma^t R_t \\mid S_0=s\\right]$. Since $T=1$ and $\\gamma=1$, this simplifies to $v^\\pi(s) = \\mathbb{E}[R_1 | S_0=s, A_0 \\sim \\pi(\\cdot|s)]$.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientifically Grounded**: The problem is a well-constructed example within the field of reinforcement learning and statistical learning. It uses standard concepts like MDPs, value functions, policies, empirical risk minimization, and overfitting. The scenario where a finite sample deviates from its true expectation is a fundamental concept in statistics; a realization of $5$ consecutive heads from a fair coin has a non-zero probability ($ (0.5)^5 = 0.03125 $), and the scenario presented is analogous. The problem is scientifically sound.\n-   **Well-Posed**: All necessary information is provided. The MDP, policies, and learning outcomes are defined precisely, allowing for unambiguous calculation and evaluation.\n-   **Objective**: The problem is stated in precise, objective, and formal language standard to the field. It contains no subjective claims.\n-   **Verdict**: The problem is valid. It presents a clear and instructive scenario illustrating the dangers of policy improvement based on overfitted value estimates in an offline setting.\n\n### Step 3: Derivation and Option Analysis\nWe proceed to solve the problem based on the validated statement.\n\n**Derivation from First Principles**\n\n1.  **True Action-Value Functions**: The true action-value function $q^*(s,u)$ is the true expected reward for taking action $u$ in state $s$.\n    -   For action $a$: $q^*(s,a) = \\mathbb{E}[R_a] = (0.5 \\times 1) + (0.5 \\times -1) = 0.5 - 0.5 = 0$.\n    -   For action $b$: $q^*(s,b) = \\mathbb{E}[R_b] = 1 \\times 0.4 = 0.4$.\n\n2.  **True Optimal Policy**: The optimal policy $\\pi^*$ selects the action with the highest true expected return. Since $q^*(s,b) = 0.4  q^*(s,a) = 0$, the optimal policy is to always select action $b$.\n\n3.  **Value of Policy $\\pi$**: The policy $\\pi$ is defined as $\\pi(s)=b$. Its value $v^\\pi(s)$ is the expected return when following it.\n    -   $v^\\pi(s) = \\mathbb{E}_{\\pi}[R_1 | S_0=s] = q^*(s, \\pi(s)) = q^*(s,b) = 0.4$.\n\n4.  **Fitted Action-Value Function $\\hat q$**: The problem states $\\hat q$ is fit by empirical risk minimization and interpolates the empirical means.\n    -   Empirical mean for action $a$: Based on $5$ observed rewards of $+1$, the mean is $\\frac{1}{5}\\sum_{i=1}^{5} 1 = 1$. Thus, $\\hat q(s,a) = 1$.\n    -   Empirical mean for action $b$: Based on $5$ observed rewards of $+0.4$, the mean is $\\frac{1}{5}\\sum_{i=1}^{5} 0.4 = 0.4$. Thus, $\\hat q(s,b) = 0.4$.\n    -   Note that $\\hat q(s,a)=1$ is a severe overestimation of the true value $q^*(s,a)=0$, caused by an unrepresentative, albeit possible, random sample. This is overfitting.\n\n5.  **Greedy Policy $\\pi'$**: The policy $\\pi'$ is greedy with respect to $\\hat q$.\n    -   $\\pi'(s) = \\arg\\max_{u \\in \\{a,b\\}} \\hat q(s,u) = \\arg\\max(1, 0.4) = a$.\n\n6.  **Value of Policy $\\pi'$**: The value $v^{\\pi'}(s)$ is the true expected return when following $\\pi'$.\n    -   $v^{\\pi'}(s) = \\mathbb{E}_{\\pi'}[R_1 | S_0=s] = q^*(s, \\pi'(s)) = q^*(s,a) = 0$.\n\n**Option-by-Option Analysis**\n\n**A. In this MDP, $v^\\pi(s)$ under the current policy $\\pi$ exceeds $v^{\\pi'}(s)$ under the greedy policy $\\pi'$ induced by $\\hat q$, so greedy improvement with an overfitted $\\hat q$ strictly reduces the true value at $s$.**\n\n-   From our derivation, we have $v^\\pi(s) = 0.4$ and $v^{\\pi'}(s) = 0$.\n-   The first part of the statement, \"$v^\\pi(s)$ exceeds $v^{\\pi'}(s)$,\" is true since $0.4  0$.\n-   The policy \"improvement\" step changes the policy from $\\pi$ (choosing $b$) to $\\pi'$ (choosing $a$). This change is based on the overfitted estimates $\\hat q(s,a) = 1  \\hat q(s,b) = 0.4$.\n-   The result of this change is that the true value of the policy at state $s$ decreases from $v^\\pi(s)=0.4$ to $v^{\\pi'}(s)=0$.\n-   This phenomenon, where a policy change suggested by an approximate value function leads to a decrease in the true policy value, is known as policy regression. It occurred here because the value function $\\hat q$ was overfitted to a finite, misleading dataset.\n-   The statement accurately describes the situation.\n-   **Verdict: Correct.**\n\n**B. A practical mitigation is to partition the data into a training set and a validation set, compute a sequence of greedy policies $\\{\\pi_t\\}$ as the $\\hat q$ model is trained over epochs $t$, estimate $v^{\\pi_t}(s)$ off-policy on the validation set, and stop at the smallest $t$ for which the validation estimate of $v^{\\pi_t}(s)$ fails to improve (cross-validated early stopping), thereby reducing the chance of policy regression due to overfitting.**\n\n-   This option describes a standard technique from machine learning applied to offline reinforcement learning.\n-   Overfitting occurs when a model learns the specifics of the training data too well, losing its ability to generalize to new, unseen data. In RL, this can manifest as an inaccurate value function that leads to a poor policy.\n-   The proposed mitigation is to use a validation set, which is not used for training the model $\\hat q_t$. At each training epoch $t$, the induced greedy policy $\\pi_t$ is evaluated on this hold-out data. This evaluation must be off-policy (e.g., using importance sampling) because the validation data was collected with the behavior policy $\\mu$, not $\\pi_t$.\n-   By tracking the estimated value of $\\pi_t$ on the validation set, one can observe when the model starts to overfit. Overfitting would cause the validation performance to plateau or degrade, even as the training performance continues to improve.\n-   Stopping at the point of best validation performance (early stopping) is a well-established heuristic to find a model that generalizes well and, in this context, to find a policy that performs well in the true environment. This directly mitigates the policy regression shown in A.\n-   **Verdict: Correct.**\n\n**C. Because the empirical training error of $\\hat q$ decreases monotonically with training iterations, the true value $v^{\\pi_t}(s)$ of the induced greedy policy sequence necessarily increases monotonically, so policy regression cannot occur.**\n\n-   This statement posits a necessary positive correlation between the model's training error and the true value of the induced policy.\n-   The problem setup itself is a decisive counterexample. The model $\\hat q$ is fitted to \"exactly interpolate the empirical means,\" which implies it has achieved the minimum possible empirical error on the training data (e.g., zero mean squared error). Let this be the final state of training, $\\hat q_T$.\n-   Consider a hypothetical starting point for training, $\\hat q_0$, where $\\hat q_0(s,a) = \\hat q_0(s,b) = 0$. The greedy policy $\\pi_0$ would be to select $b$ (assuming ties are broken in favor of $b$), with a true value $v^{\\pi_0}(s) = 0.4$.\n-   As training progresses, $\\hat q_t(s,a)$ will increase from $0$ to $1$, and $\\hat q_t(s,b)$ will increase from $0$ to $0.4$. The empirical training error of $\\hat q_t$ will decrease monotonically.\n-   As soon as $\\hat q_t(s,a)$ becomes greater than $\\hat q_t(s,b)$ (which will happen when $\\hat q_t(s,a)  0.4$), the greedy policy $\\pi_t$ will flip from choosing $b$ to choosing $a$. At this exact moment, the true value of the policy, $v^{\\pi_t}(s)$, drops from $0.4$ to $0$.\n-   Thus, a monotonically decreasing training error for $\\hat q$ can correspond to a sharp decrease in the true value of the policy. The statement is false.\n-   **Verdict: Incorrect.**\n\n**D. If the behavior policy $\\mu$ explores both actions, importance sampling on the validation set always yields low-variance, unbiased estimates of $v^{\\pi_t}(s)$, making early stopping unnecessary.**\n\n-   This option makes two strong, incorrect claims.\n-   First, it claims that importance sampling (IS) \"always yields low-variance\" estimates as long as the behavior policy is exploratory (i.e., $\\mu(u|s)0$ for all actions $u$ taken by the target policy). While IS provides unbiased estimates under these conditions, its variance is a significant practical problem. The variance of the IS estimator depends on the variance of the importance weights, $\\rho = \\frac{\\pi(A|S)}{\\mu(A|S)}$. If the target policy $\\pi$ is substantially different from the behavior policy $\\mu$, these ratios can be large, leading to extremely high variance in the estimate. The claim of \"always low-variance\" is false.\n-   Second, it claims that a good estimator makes \"early stopping unnecessary.\" This misinterprets the role of validation and early stopping. Early stopping is a model selection algorithm. It *requires* an estimator of generalization performance to work. Having a good estimator (unbiased, low-variance) is what makes early stopping effective; it does not eliminate the need for it. Without a selection criterion like early stopping, one would simply train the model to completion on the training data, which would lead directly to the overfitted model $\\hat q$ and the suboptimal policy $\\pi'$ described in the problem. The estimator is a tool for model selection, not a replacement for it.\n-   **Verdict: Incorrect.**", "answer": "$$\\boxed{AB}$$", "id": "3190845"}]}