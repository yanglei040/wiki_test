{"hands_on_practices": [{"introduction": "Before implementing any algorithm, it's crucial to understand the concepts on which it is built. This exercise challenges you to analytically compare two popular uncertainty sampling methods: least-confidence and margin sampling. By reasoning through their behavior in both binary ($K=2$) and multi-class ($K > 2$) settings, you will develop a sharp intuition for why their underlying heuristics are not always equivalent and when they might diverge [@problem_id:3095044].", "problem": "A probabilistic multi-class classifier produces, for each input $x$, class-posterior probabilities $p(y \\mid x)$ over a finite label set $\\mathcal{Y}$, satisfying $\\sum_{y \\in \\mathcal{Y}} p(y \\mid x) = 1$ and $p(y \\mid x) \\in [0,1]$. In Active Learning (AL), uncertainty sampling selects $x$ according to an uncertainty score. Two common uncertainty scores are the least-confidence score and the margin score, defined using the order statistics of the posterior. Let $p_{(1)}(x) = \\max_{y \\in \\mathcal{Y}} p(y \\mid x)$ denote the largest posterior and $p_{(2)}(x)$ denote the second-largest posterior. Least-confidence (LC) sampling chooses $x$ that maximizes $u_{\\mathrm{LC}}(x) = 1 - p_{(1)}(x)$, and margin sampling (MS) chooses $x$ that minimizes $u_{\\mathrm{margin}}(x) = p_{(1)}(x) - p_{(2)}(x)$.\n\nStarting only from these definitions and the facts that the posteriors are nonnegative and sum to $1$, reason about when the two selection rules yield identical choices. In particular, determine whether they are equivalent in binary classification ($K=2$ classes) and whether they can diverge in multi-class settings ($K>2$). Then, for a $K=3$ setting with $\\mathcal{Y} = \\{1,2,3\\}$, consider two unlabeled points with posterior vectors\n- $x_A$: $(p(1 \\mid x_A), p(2 \\mid x_A), p(3 \\mid x_A)) = (\\,0.60,\\,0.39,\\,0.01\\,)$,\n- $x_B$: $(p(1 \\mid x_B), p(2 \\mid x_B), p(3 \\mid x_B)) = (\\,0.51,\\,0.26,\\,0.23\\,)$.\n\nUse the uncertainty definitions to compute $u_{\\mathrm{LC}}(x)$ and $u_{\\mathrm{margin}}(x)$ for $x_A$ and $x_B$, and determine which point each strategy would select.\n\nWhich option is correct?\n\nA. In binary classification ($K=2$), least-confidence and margin sampling always select the same point; in the $K=3$ example, least-confidence selects $x_B$ and margin sampling selects $x_A$.\n\nB. In binary classification ($K=2$), least-confidence and margin sampling can disagree; in the $K=3$ example, both strategies select $x_B$.\n\nC. In binary classification ($K=2$), the two strategies agree only when $p_{(1)}(x) = \\tfrac{1}{2}$; in the $K=3$ example, least-confidence selects $x_A$ and margin sampling selects $x_B$.\n\nD. For any number of classes ($K \\ge 2$), least-confidence and margin sampling always agree; in the $K=3$ example, both strategies select $x_A$.", "solution": "The problem statement is first validated for soundness and completeness.\n\n### Step 1: Extract Givens\n- A probabilistic multi-class classifier produces class-posteriors $p(y \\mid x)$ for an input $x$ over a label set $\\mathcal{Y}$ of size $K = |\\mathcal{Y}|$.\n- The posteriors satisfy $\\sum_{y \\in \\mathcal{Y}} p(y \\mid x) = 1$ and $p(y \\mid x) \\in [0,1]$.\n- Order statistics of the posterior are defined: $p_{(1)}(x) = \\max_{y \\in \\mathcal{Y}} p(y \\mid x)$ (the largest) and $p_{(2)}(x)$ (the second-largest).\n- Least-confidence (LC) sampling selects an unlabeled point $x$ that maximizes the score $u_{\\mathrm{LC}}(x) = 1 - p_{(1)}(x)$.\n- Margin sampling (MS) selects an unlabeled point $x$ that minimizes the score $u_{\\mathrm{margin}}(x) = p_{(1)}(x) - p_{(2)}(x)$.\n- The problem requires analysis of the equivalence of these two rules for binary ($K=2$) and multi-class ($K2$) classification.\n- For a $K=3$ scenario with $\\mathcal{Y} = \\{1,2,3\\}$, two specific unlabeled points are given:\n  - $x_A$ with posterior vector $(p(1 \\mid x_A), p(2 \\mid x_A), p(3 \\mid x_A)) = (0.60, 0.39, 0.01)$.\n  - $x_B$ with posterior vector $(p(1 \\mid x_B), p(2 \\mid x_B), p(3 \\mid x_B)) = (0.51, 0.26, 0.23)$.\n- The task is to determine which point each strategy would select from the pair $\\{x_A, x_B\\}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the validation criteria.\n- **Scientifically Grounded:** The problem is based on standard, well-established concepts in machine learning, specifically in the subfield of active learning. The definitions for least-confidence and margin sampling are canonical.\n- **Well-Posed:** The problem is clearly defined. The objectives are to compare two selection rules based on their formal definitions and to apply these rules to a concrete numerical example. The existence of a unique and meaningful solution is guaranteed.\n- **Objective:** The problem is stated in precise, mathematical language, free from subjectivity or ambiguity.\n- **Completeness and Consistency:** All necessary definitions and data are provided. The posterior probabilities for the given examples correctly sum to $1$. There are no contradictions.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. It is a well-posed, scientifically sound query within the field of statistical learning. The solution will proceed.\n\n### Solution Derivation\n\nThe solution is derived in three parts as requested: analysis for binary classification, analysis for the general multi-class case, and application to the specific $K=3$ example.\n\n**Part 1: Analysis for Binary Classification ($K=2$)**\n\nLet the set of classes be $\\mathcal{Y} = \\{y_1, y_2\\}$. For any input $x$, the posterior probabilities are $p(y_1 \\mid x)$ and $p(y_2 \\mid x)$.\nFrom the sum-to-one property, we have $p(y_1 \\mid x) + p(y_2 \\mid x) = 1$.\nWithout loss of generality, let's assume $p(y_1 \\mid x) \\ge p(y_2 \\mid x)$. This implies $p(y_1 \\mid x) \\ge 0.5$.\nBy definition, the first- and second-largest posteriors are:\n$p_{(1)}(x) = p(y_1 \\mid x)$\n$p_{(2)}(x) = p(y_2 \\mid x) = 1 - p(y_1 \\mid x)$\n\nNow, we analyze the selection rules.\n1.  **Least-Confidence (LC) Sampling:** LC selects the point $x$ that maximizes $u_{\\mathrm{LC}}(x) = 1 - p_{(1)}(x)$. Maximizing $1 - p_{(1)}(x)$ is equivalent to minimizing $p_{(1)}(x)$. So, LC selects $\\arg\\min_x p_{(1)}(x)$. This strategy seeks the point where the classifier is least certain about its most-probable prediction.\n\n2.  **Margin Sampling (MS):** MS selects the point $x$ that minimizes $u_{\\mathrm{margin}}(x) = p_{(1)}(x) - p_{(2)}(x)$. Substituting the expression for $p_{(2)}(x)$ in the binary case:\n    $$u_{\\mathrm{margin}}(x) = p_{(1)}(x) - (1 - p_{(1)}(x)) = 2p_{(1)}(x) - 1$$\n    So, MS selects $\\arg\\min_x (2p_{(1)}(x) - 1)$.\n\nTo compare the strategies, we compare the objective functions they optimize. LC minimizes $p_{(1)}(x)$, and MS minimizes $2p_{(1)}(x) - 1$.\nThe function $f(z) = 2z - 1$ is a strictly increasing linear transformation of $z$. Therefore, for any set of points, the point that minimizes $p_{(1)}(x)$ is precisely the same point that minimizes $2p_{(1)}(x) - 1$.\nFor instance, if we are choosing between $x_i$ and $x_j$, and $p_{(1)}(x_i)  p_{(1)}(x_j)$, then it is also true that $2p_{(1)}(x_i) - 1  2p_{(1)}(x_j) - 1$. Both strategies will prefer $x_i$.\nThus, the ranking of all candidate points is identical under both criteria, and they will always select the same point. The first part of the problem statement is established: in binary classification, LC and MS are equivalent selection strategies.\n\n**Part 2: Divergence in Multi-class Settings ($K2$)**\n\nFor $K > 2$, $p_{(2)}(x)$ is no longer a simple function of $p_{(1)}(x)$. The sum-to-one constraint is $\\sum_{i=1}^K p_{(i)}(x) = 1$, where $p_{(i)}(x)$ are the ordered posteriors. This means $p_{(2)}(x)$ can vary independently of $p_{(1)}(x)$ (within certain constraints).\n- LC sampling still selects $\\arg\\min_x p_{(1)}(x)$. This strategy is myopic, as it only considers the confidence in the top choice, ignoring the distribution of probability among the other classes.\n- MS sampling selects $\\arg\\min_x (p_{(1)}(x) - p_{(2)}(x))$. This strategy explicitly considers the ambiguity between the two most likely classes.\n\nA disagreement can occur if one point has a lower $p_{(1)}$ (favored by LC) while another has a smaller margin $p_{(1)}-p_{(2)}$ (favored by MS). The provided $K=3$ example will serve to demonstrate this divergence.\n\n**Part 3: Analysis of the $K=3$ Example**\n\nWe compute the uncertainty scores for the two given points, $x_A$ and $x_B$.\n\nFor point $x_A$:\n- Posteriors: $(0.60, 0.39, 0.01)$.\n- Ordered posteriors: $p_{(1)}(x_A) = 0.60$, $p_{(2)}(x_A) = 0.39$, $p_{(3)}(x_A) = 0.01$.\n- LC score: $u_{\\mathrm{LC}}(x_A) = 1 - p_{(1)}(x_A) = 1 - 0.60 = 0.40$.\n- Margin score: $u_{\\mathrm{margin}}(x_A) = p_{(1)}(x_A) - p_{(2)}(x_A) = 0.60 - 0.39 = 0.21$.\n\nFor point $x_B$:\n- Posteriors: $(0.51, 0.26, 0.23)$.\n- Ordered posteriors: $p_{(1)}(x_B) = 0.51$, $p_{(2)}(x_B) = 0.26$, $p_{(3)}(x_B) = 0.23$.\n- LC score: $u_{\\mathrm{LC}}(x_B) = 1 - p_{(1)}(x_B) = 1 - 0.51 = 0.49$.\n- Margin score: $u_{\\mathrm{margin}}(x_B) = p_{(1)}(x_B) - p_{(2)}(x_B) = 0.51 - 0.26 = 0.25$.\n\nNow, we determine the selection for each strategy:\n- **LC Selection:** The goal is to **maximize** $u_{\\mathrm{LC}}(x)$. Comparing the scores, $u_{\\mathrm{LC}}(x_B) = 0.49  u_{\\mathrm{LC}}(x_A) = 0.40$. Therefore, least-confidence sampling selects point **$x_B$**.\n- **MS Selection:** The goal is to **minimize** $u_{\\mathrm{margin}}(x)$. Comparing the scores, $u_{\\mathrm{margin}}(x_A) = 0.21  u_{\\mathrm{margin}}(x_B) = 0.25$. Therefore, margin sampling selects point **$x_A$**.\n\nThis example confirms that for $K2$, the two strategies can indeed select different points. LC prefers $x_B$ because its most likely class ($0.51$) is less certain than $x_A$'s most likely class ($0.60$). MS prefers $x_A$ because the two most likely classes ($0.60$ and $0.39$) are closer together than for $x_B$ ($0.51$ and $0.26$), indicating greater ambiguity between the top two contenders.\n\n### Option-by-Option Analysis\n\nBased on the above derivation, each option is evaluated.\n\n**A. In binary classification ($K=2$), least-confidence and margin sampling always select the same point; in the $K=3$ example, least-confidence selects $x_B$ and margin sampling selects $x_A$.**\n- The first part, regarding equivalence in binary classification, was proven to be correct.\n- The second part, regarding the selection for the $K=3$ example, matches our calculations exactly.\n- **Verdict: Correct.**\n\n**B. In binary classification ($K=2$), least-confidence and margin sampling can disagree; in the $K=3$ example, both strategies select $x_B$.**\n- The first part is incorrect. Our proof shows they are equivalent for $K=2$.\n- The second part is incorrect. Our calculation shows that margin sampling selects $x_A$, not $x_B$.\n- **Verdict: Incorrect.**\n\n**C. In binary classification ($K=2$), the two strategies agree only when $p_{(1)}(x) = \\tfrac{1}{2}$; in the $K=3$ example, least-confidence selects $x_A$ and margin sampling selects $x_B$.**\n- The first part is incorrect. Our proof shows they always agree for $K=2$, not just in the special case of maximum uncertainty where $p_{(1)}(x) = 1/2$.\n- The second part is incorrect. The selections are reversed from our findings.\n- **Verdict: Incorrect.**\n\n**D. For any number of classes ($K \\ge 2$), least-confidence and margin sampling always agree; in the $K=3$ example, both strategies select $x_A$.**\n- The first part is incorrect. The $K=3$ example explicitly demonstrates that they can disagree.\n- The second part is incorrect. Our calculation shows that least-confidence sampling selects $x_B$, not $x_A$.\n- **Verdict: Incorrect.**", "answer": "$$\\boxed{A}$$", "id": "3095044"}, {"introduction": "Moving from theory to practice is a key step in mastering any machine learning technique. This hands-on coding problem asks you to implement three foundational uncertainty sampling strategies: least-confidence, margin sampling, and entropy sampling. By applying your code to a curated set of data points, you will see firsthand how these different definitions of 'uncertainty' lead to distinct choices, cementing your understanding of their practical implications [@problem_id:3095122].", "problem": "You are given a pool-based active learning scenario for multiclass classification, where a probabilistic classifier with fixed parameters $\\hat{\\theta}$ provides, for each unlabeled input $x$, a categorical predictive distribution $p_{\\hat{\\theta}}(y \\mid x)$ over a finite label set $\\mathcal{Y}$. Your task is to implement three uncertainty sampling strategies—least-confidence, margin, and entropy sampling—and to ensure that they produce distinct selections on an appropriately constructed pool. The solution must be a complete, runnable program that evaluates a specified test suite and prints the requested outputs in the exact format.\n\nFundamental base and definitions:\n- For each unlabeled input $x$, the classifier outputs a vector of class probabilities $p_{\\hat{\\theta}}(y \\mid x)$ for $y \\in \\mathcal{Y}$ that satisfies $p_{\\hat{\\theta}}(y \\mid x) \\ge 0$ for all $y$ and $\\sum_{y \\in \\mathcal{Y}} p_{\\hat{\\theta}}(y \\mid x) = 1$.\n- Let $p_{(1)}(x) \\ge p_{(2)}(x) \\ge \\cdots$ denote the sorted probabilities of $p_{\\hat{\\theta}}(y \\mid x)$ in descending order.\n- Least-confidence uncertainty for $x$ is measured by $\\max_{y \\in \\mathcal{Y}} p_{\\hat{\\theta}}(y \\mid x) = p_{(1)}(x)$. Least-confidence sampling selects the $x$ with the smallest $p_{(1)}(x)$ over the pool.\n- Margin uncertainty for $x$ is measured by the difference between the top two probabilities, $p_{(1)}(x) - p_{(2)}(x)$. Margin sampling selects the $x$ with the smallest $p_{(1)}(x) - p_{(2)}(x)$ over the pool.\n- Entropy uncertainty for $x$ is given by the Shannon entropy $H\\!\\left[Y \\mid x, \\hat{\\theta}\\right] = - \\sum_{y \\in \\mathcal{Y}} p_{\\hat{\\theta}}(y \\mid x) \\log p_{\\hat{\\theta}}(y \\mid x)$, where $\\log$ denotes the natural logarithm. Entropy sampling selects the $x$ with the largest $H\\!\\left[Y \\mid x, \\hat{\\theta}\\right]$ over the pool.\n- Ties must be broken deterministically by selecting the smallest index under $0$-based indexing.\n\nProgramming requirements:\n- For each test case, you will be given a finite pool $\\mathcal{U} = \\{x_i\\}_{i=0}^{n-1}$, represented by the matrix of predicted class-probability vectors $\\left( p_{\\hat{\\theta}}(y \\mid x_i) \\right)$. Each row corresponds to one $x_i$, and each row sums to $1$ up to floating-point precision.\n- For each test case, compute:\n  $1.$ The index (under $0$-based indexing) selected by least-confidence sampling and the corresponding value $p_{(1)}(x)$ at that index.\n  $2.$ The index selected by margin sampling and the corresponding value $\\left(p_{(1)}(x) - p_{(2)}(x)\\right)$ at that index.\n  $3.$ The index selected by entropy sampling and the corresponding value $H\\!\\left[Y \\mid x, \\hat{\\theta}\\right]$ at that index.\n  $4.$ The number of distinct indices among the three selections (that is, the cardinality of the set containing the three selected indices).\n- Use the convention $0 \\cdot \\log 0 = 0$ when computing entropy. Use the natural logarithm for $\\log$.\n- All floating-point outputs must be rounded to exactly $6$ decimal places.\n- Ties for any selection criterion must be resolved by choosing the smallest index under $0$-based indexing.\n\nTest suite:\n**Test case 1** (happy path: three criteria select different inputs):\n  - Pool with four unlabeled inputs (rows) and three classes (columns):\n    - $x_0$: $\\left[0.39,\\; 0.38,\\; 0.23\\right]$\n    - $x_1$: $\\left[0.41,\\; 0.295,\\; 0.295\\right]$\n    - $x_2$: $\\left[0.500,\\; 0.495,\\; 0.005\\right]$\n    - $x_3$: $\\left[0.70,\\; 0.20,\\; 0.10\\right]$\n**Test case 2** (boundary: exact tie across all criteria within the pool; tie-breaking by smallest index):\n  - Pool with three unlabeled inputs and three classes:\n    - $x_0$: $\\left[\\frac{1}{3},\\; \\frac{1}{3},\\; \\frac{1}{3}\\right]$\n    - $x_1$: $\\left[\\frac{1}{3},\\; \\frac{1}{3},\\; \\frac{1}{3}\\right]$\n    - $x_2$: $\\left[\\frac{1}{3},\\; \\frac{1}{3},\\; \\frac{1}{3}\\right]$\n**Test case 3** (edge case: zero probabilities present; near-deterministic predictions):\n  - Pool with four unlabeled inputs and three classes:\n    - $x_0$: $\\left[0.99,\\; 0.01,\\; 0.00\\right]$\n    - $x_1$: $\\left[0.60,\\; 0.40,\\; 0.00\\right]$\n    - $x_2$: $\\left[0.49,\\; 0.49,\\; 0.02\\right]$\n    - $x_3$: $\\left[0.55,\\; 0.45,\\; 0.00\\right]$\n\nFinal output specification:\n- For each test case, produce the following seven values in order as a flat sequence:\n  - $\\text{LC\\_idx}$, $p_{(1)}$ at $\\text{LC\\_idx}$,\n  - $\\text{M\\_idx}$, $\\left(p_{(1)} - p_{(2)}\\right)$ at $\\text{M\\_idx}$,\n  - $\\text{ENT\\_idx}$, $H$ at $\\text{ENT\\_idx}$,\n  - $\\text{distinct\\_count}$.\n- Aggregate the results for all test cases into a single list in the order of the test cases. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with all floating-point numbers rounded to exactly $6$ decimal places. For example, the output must have the form $\\left[\\text{v}_0,\\text{v}_1,\\ldots,\\text{v}_{m-1}\\right]$ where each $\\text{v}_j$ is an integer or a floating-point number with exactly $6$ decimal places.", "solution": "The problem statement has been critically examined and is determined to be **valid**. It is scientifically grounded in the principles of statistical learning, specifically active learning. The definitions for least-confidence, margin, and entropy sampling are standard and correct. The problem is well-posed, providing all necessary data, constraints, and deterministic tie-breaking rules, which ensures a unique and computable solution exists for each test case. The inputs are self-consistent, and the objectives are formalizable and objective.\n\nThe task is to implement and compare three distinct uncertainty sampling strategies for a multiclass classifier in a pool-based active learning setting. Given a pool of unlabeled data points $\\mathcal{U}$, each represented by a vector of predictive probabilities $p_{\\hat{\\theta}}(y \\mid x)$ over the set of classes $\\mathcal{Y}$, we must select the single most informative point to query for its true label. The \"informativeness\" or \"uncertainty\" is quantified by three different metrics.\n\nLet $P$ be a matrix of dimensions $n \\times k$, where $n$ is the number of data points in the pool and $k = |\\mathcal{Y}|$ is the number of classes. Each row $P_i$ corresponds to the probability vector $p_{\\hat{\\theta}}(y \\mid x_i)$. The objective is to find the index $i^*$ that corresponds to the most uncertain data point according to each of the following criteria.\n\n**1. Least-Confidence Sampling**\n\nThis strategy is based on the classifier's confidence in its most likely prediction. Uncertainty is considered maximal when the probability of the most probable class is minimal. The most confident prediction for an input $x$ is given by $p_{(1)}(x) = \\max_{y \\in \\mathcal{Y}} p_{\\hat{\\theta}}(y \\mid x)$. The least-confidence sampling strategy selects the data point $x^*$ from the pool $\\mathcal{U}$ that minimizes this value.\n\nThe selection index $i^*_{LC}$ is therefore:\n$$\ni^*_{LC} = \\arg\\min_{i \\in \\{0, \\dots, n-1\\}} p_{(1)}(x_i)\n$$\nIn case of a tie, the smallest index $i$ is chosen. The associated uncertainty value is $p_{(1)}(x_{i^*_{LC}})$.\n\n**2. Margin Sampling**\n\nThis strategy refines least-confidence sampling by considering the ambiguity between the two most likely classes. A small margin between the first and second most probable classes, $p_{(1)}(x)$ and $p_{(2)}(x)$, indicates that the classifier is struggling to discriminate between them, thus signaling high uncertainty. The margin is defined as $p_{(1)}(x) - p_{(2)}(x)$. Margin sampling selects the data point $x^*$ that has the smallest margin.\n\nThe selection index $i^*_{M}$ is:\n$$\ni^*_{M} = \\arg\\min_{i \\in \\{0, \\dots, n-1\\}} \\left( p_{(1)}(x_i) - p_{(2)}(x_i) \\right)\n$$\nAgain, ties are broken by selecting the smallest index. The associated uncertainty value is the margin itself, $p_{(1)}(x_{i^*_{M}}) - p_{(2)}(x_{i^*_{M}})$.\n\n**3. Entropy Sampling**\n\nThis is the most comprehensive of the three metrics as it takes into account the entire probability distribution. It uses Shannon entropy as a measure of uncertainty. A probability distribution that is sharply peaked (one high probability, others low) has low entropy, indicating low uncertainty. Conversely, a distribution that is close to uniform has high entropy, indicating high uncertainty. The entropy for a given input $x$ is:\n$$\nH[Y \\mid x, \\hat{\\theta}] = - \\sum_{y \\in \\mathcal{Y}} p_{\\hat{\\theta}}(y \\mid x) \\log p_{\\hat{\\theta}}(y \\mid x)\n$$\nwhere $\\log$ is the natural logarithm, and the convention $0 \\cdot \\log 0 = 0$ is used. Entropy sampling selects the data point $x^*$ with the maximum entropy.\n\nThe selection index $i^*_{ENT}$ is:\n$$\ni^*_{ENT} = \\arg\\max_{i \\in \\{0, \\dots, n-1\\}} H[Y \\mid x_i, \\hat{\\theta}]\n$$\nTies are broken by selecting the smallest index. The associated uncertainty value is the entropy $H[Y \\mid x_{i^*_{ENT}}, \\hat{\\theta}]$.\n\n**Computational Procedure**\n\nFor each test case represented by an $n \\times k$ probability matrix $P$:\n1.  **Least-Confidence:** For each row, find the maximum probability. Then, find the index of the row with the minimum of these maximums.\n2.  **Margin:** For each row, sort the probabilities in descending order to find $p_{(1)}$ and $p_{(2)}$. Compute their difference. Then, find the index of the row with the minimum difference.\n3.  **Entropy:** For each row, compute the Shannon entropy, handling the $p=0$ case correctly by ensuring terms of the form $0 \\cdot \\log 0$ contribute $0$ to the sum. Then, find the index of the row with the maximum entropy.\n4.  **Distinct Indices:** Collect the three resulting indices ($i^*_{LC}$, $i^*_{M}$, $i^*_{ENT}$) and count the number of unique values in this collection.\n5.  **Formatting:** All floating-point results are rounded to $6$ decimal places and collated with the integer indices and counts into a single flat list as per the problem specification.\n\nThe implementation will utilize the `numpy` library, whose `argmin` and `argmax` functions inherently satisfy the specified tie-breaking rule by returning the first encountered index of the extremal value.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements least-confidence, margin, and entropy uncertainty sampling\n    for a series of test cases and formats the output as specified.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1\n        [\n            [0.39, 0.38, 0.23],\n            [0.41, 0.295, 0.295],\n            [0.500, 0.495, 0.005],\n            [0.70, 0.20, 0.10],\n        ],\n        # Test case 2\n        [\n            [1/3, 1/3, 1/3],\n            [1/3, 1/3, 1/3],\n            [1/3, 1/3, 1/3],\n        ],\n        # Test case 3\n        [\n            [0.99, 0.01, 0.00],\n            [0.60, 0.40, 0.00],\n            [0.49, 0.49, 0.02],\n            [0.55, 0.45, 0.00],\n        ]\n    ]\n\n    results = []\n    for case in test_cases:\n        p_matrix = np.array(case, dtype=np.float64)\n\n        # 1. Least-Confidence Sampling\n        # The goal is to find the point with the smallest most-confident prediction.\n        p1_values = np.max(p_matrix, axis=1)\n        lc_idx = np.argmin(p1_values)\n        lc_val = p1_values[lc_idx]\n\n        # 2. Margin Sampling\n        # The goal is to find the point with the smallest difference between the\n        # top two predictions.\n        # Sort probabilities in descending order for each row.\n        sorted_p = -np.sort(-p_matrix, axis=1)\n        p1 = sorted_p[:, 0]\n        p2 = sorted_p[:, 1]\n        margin_values = p1 - p2\n        m_idx = np.argmin(margin_values)\n        m_val = margin_values[m_idx]\n\n        # 3. Entropy Sampling\n        # The goal is to find the point with the highest Shannon entropy.\n        # We must handle the 0 * log(0) = 0 case.\n        # Create a temporary array where p=0 is replaced by p=1.\n        # This makes log(p)=0, so p*log(p) becomes 0, satisfying the convention.\n        p_for_log = p_matrix.copy()\n        p_for_log[p_for_log == 0] = 1.0\n        entropy_values = -np.sum(p_matrix * np.log(p_for_log), axis=1)\n        ent_idx = np.argmax(entropy_values)\n        ent_val = entropy_values[ent_idx]\n\n        # 4. Count of distinct indices\n        distinct_count = len({lc_idx, m_idx, ent_idx})\n\n        # Append results for the current test case.\n        results.extend([\n            lc_idx, lc_val,\n            m_idx, m_val,\n            ent_idx, ent_val,\n            distinct_count\n        ])\n\n    def format_val(v):\n        \"\"\"Formats an integer or float according to problem specs.\"\"\"\n        if isinstance(v, (int, np.integer)):\n            return str(v)\n        else:  # float or np.floating\n            return f\"{v:.6f}\"\n\n    # Format the final list of results for printing.\n    formatted_results = [format_val(r) for r in results]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3095122"}, {"introduction": "Active learning strategies rarely exist in a vacuum; they are part of a larger modeling pipeline. This advanced practice explores the interplay between entropy-based uncertainty sampling and label smoothing, a common regularization technique used to improve model calibration and generalization. By quantifying the change in data point rankings, you will gain insight into the stability of uncertainty measures and the importance of considering the entire system when designing an active learning loop [@problem_id:3095020].", "problem": "You are given a pool-based active learning scenario for a $K$-class classification task. For each unlabeled input $x$ in the pool, a trained model provides a predictive distribution $p(y \\mid x) \\in \\Delta^{K-1}$ over the $K$ classes. In entropy-based uncertainty sampling, inputs are ranked by their conditional Shannon entropy $H[Y \\mid x]$, defined by the fundamental definition of entropy in information theory:\n$$\nH[Y \\mid x] \\equiv - \\sum_{k=1}^{K} p(y=k \\mid x) \\, \\log p(y=k \\mid x),\n$$\nwhere the logarithm is the natural logarithm, so entropy is measured in nats. The highest-entropy inputs are selected as most uncertain.\n\nConsider applying label smoothing to the predictive distributions prior to ranking. Define the smoothed predictive distribution as the convex combination\n$$\np_{\\alpha}(y \\mid x) \\equiv (1-\\alpha)\\, p(y \\mid x) + \\alpha \\, u,\n$$\nwhere $u$ is the uniform distribution over $K$ classes with $u_k = 1/K$, and $\\alpha \\in [0,1]$ is the smoothing parameter. After smoothing, the ranking is recomputed by the entropies $H_{\\alpha}[Y \\mid x] \\equiv - \\sum_{k=1}^{K} p_{\\alpha}(y=k \\mid x)\\,\\log p_{\\alpha}(y=k \\mid x)$.\n\nYour task is to quantify the change in the entropy-based ranking of pool items induced by smoothing. Use the following formal procedure:\n1. For a given pool of $N$ inputs $\\{x_i\\}_{i=1}^{N}$ with predictive distributions $\\{p_i\\}_{i=1}^{N}$, compute the original entropies $\\{H_i\\}_{i=1}^{N}$ with $\\alpha = 0$. Form the original ranking $R_0$ by sorting indices $i \\in \\{1,\\dots,N\\}$ by descending $H_i$, breaking ties by ascending index $i$.\n2. For a given $\\alpha \\in [0,1]$, compute smoothed distributions $p_{i,\\alpha} \\equiv (1-\\alpha) p_i + \\alpha u$, then compute entropies $H_{i,\\alpha}$. Form the smoothed ranking $R_{\\alpha}$ by sorting indices $i$ by descending $H_{i,\\alpha}$, breaking ties by ascending $i$.\n3. Define the inversion count between two rankings $R_a$ and $R_b$ as the number of unordered index pairs $\\{i,j\\}$ with $i \\neq j$ for which the order of $i$ and $j$ differs between $R_a$ and $R_b$. Equivalently, it is the number of pairs $(i,j)$ with $i \\neq j$ such that $i$ precedes $j$ in $R_a$ but $j$ precedes $i$ in $R_b$. This count is an integer between $0$ and $N(N-1)/2$.\n\nImplement a program that, for each test case below, computes the inversion count between $R_0$ and $R_{\\alpha}$ for each specified $\\alpha$. Use the natural logarithm for entropy. All vectors below are valid probability distributions with strictly positive components, and all computations are dimensionless (no physical units are involved).\n\nTest Suite:\n**Case A:**\n  - $K = 3$, $N = 5$.\n  - Pool distributions (each row $p_i$ corresponds to one input $x_i$ in index order $i=1,2,3,4,5$):\n    - $p_1 = [0.8, 0.1, 0.1]$\n    - $p_2 = [0.34, 0.33, 0.33]$\n    - $p_3 = [0.6, 0.3, 0.1]$\n    - $p_4 = [0.5, 0.49, 0.01]$\n    - $p_5 = [0.9, 0.05, 0.05]$\n  - Evaluate at $\\alpha \\in \\{0.0, 0.2, 0.5, 1.0\\}$.\n**Case B:**\n  - $K = 4$, $N = 4$.\n  - Pool distributions in index order $i=1,2,3,4$:\n    - $p_1 = [0.7, 0.1, 0.1, 0.1]$\n    - $p_2 = [0.4, 0.3, 0.2, 0.1]$\n    - $p_3 = [0.4, 0.3, 0.2, 0.1]$\n    - $p_4 = [0.25, 0.25, 0.25, 0.25]$\n  - Evaluate at $\\alpha \\in \\{0.3\\}$.\n**Case C:**\n  - $K = 2$, $N = 6$.\n  - Pool distributions in index order $i=1,2,3,4,5,6$:\n    - $p_1 = [0.95, 0.05]$\n    - $p_2 = [0.8, 0.2]$\n    - $p_3 = [0.6, 0.4]$\n    - $p_4 = [0.51, 0.49]$\n    - $p_5 = [0.99, 0.01]$\n    - $p_6 = [0.5, 0.5]$\n  - Evaluate at $\\alpha \\in \\{0.1, 0.9\\}$.\n\nFinal Output Format:\n- Your program should produce a single line of output containing the inversion counts for all requested $(\\text{case}, \\alpha)$ pairs as a comma-separated list enclosed in square brackets, in the following order: Case A for $\\alpha = 0.0$, then Case A for $\\alpha = 0.2$, then Case A for $\\alpha = 0.5$, then Case A for $\\alpha = 1.0$, then Case B for $\\alpha = 0.3$, then Case C for $\\alpha = 0.1$, then Case C for $\\alpha = 0.9$. For example, the output should look like\n$$\n[\\text{A}_{0.0}, \\text{A}_{0.2}, \\text{A}_{0.5}, \\text{A}_{1.0}, \\text{B}_{0.3}, \\text{C}_{0.1}, \\text{C}_{0.9}],\n$$\nwhere each symbol stands for the corresponding integer inversion count as defined above. The program must not read any input and must print only this single line.", "solution": "The problem requires us to quantify the change in an uncertainty-based ranking of items in an active learning pool when the model's predictive distributions are altered by label smoothing. The measure of change is the inversion count between the original ranking and the ranking after smoothing.\n\nThe problem is grounded in the fields of statistical learning and information theory. The core concepts are:\n$1$. **Conditional Shannon Entropy**: For a classification problem with $K$ classes, given an input $x$, a model provides a predictive probability distribution $p(y \\mid x)$ over the classes. The uncertainty associated with this prediction is quantified by the conditional Shannon entropy, defined as:\n$$\nH[Y \\mid x] = - \\sum_{k=1}^{K} p(y=k \\mid x) \\log p(y=k \\mid x)\n$$\nwhere the logarithm is the natural logarithm (base $e$), and the entropy is measured in nats. Higher entropy corresponds to higher uncertainty. In entropy-based uncertainty sampling, unlabeled inputs with the highest entropy are prioritized for labeling.\n\n$2$. **Label Smoothing**: This is a regularization technique that modifies a model's predictive distribution. The smoothed distribution $p_{\\alpha}(y \\mid x)$ is a convex combination of the original distribution $p(y \\mid x)$ and the uniform distribution $u$ (where $u_k = 1/K$ for all $k \\in \\{1, \\dots, K\\}$):\n$$\np_{\\alpha}(y \\mid x) \\equiv (1-\\alpha) p(y \\mid x) + \\alpha u\n$$\nThe parameter $\\alpha \\in [0, 1]$ controls the amount of smoothing. As $\\alpha$ increases from $0$ to $1$, the distribution is pulled towards the uniform distribution, which has the maximum possible entropy, $\\log K$. The entropy of the smoothed distribution is denoted $H_{\\alpha}[Y \\mid x]$.\n\n$3$. **Ranking and Inversion Count**: The task requires comparing two rankings of the $N$ items in the pool. The original ranking, $R_0$, is obtained by sorting the indices $\\{1, \\dots, N\\}$ in descending order of their original entropies $\\{H_i\\}_{i=1}^{N}$. The smoothed ranking, $R_{\\alpha}$, is obtained by sorting the same indices in descending order of their smoothed entropies $\\{H_{i,\\alpha}\\}_{i=1}^{N}$. A crucial detail is the tie-breaking rule: if two items have identical entropy, the one with the smaller original index $i$ is ranked higher.\n\nThe dissimilarity between two rankings, $R_a$ and $R_b$, is measured by the inversion count. This is the total number of pairs of distinct indices $\\{i, j\\}$ whose relative order is different in the two rankings. Formally, it is the cardinality of the set of pairs $(i, j)$ such that $i$ precedes $j$ in $R_a$ but $j$ precedes $i$ in $R_b$.\n\nThe solution is implemented via a step-by-step computational procedure for each test case provided.\n\n**Step 1: Entropy Calculation**\nA function is defined to compute the Shannon entropy of a given probability vector $p = [p_1, \\dots, p_K]$. Following the definition, it computes $-\\sum_{k=1}^{K} p_k \\log p_k$. Since the problem statement guarantees that all probability components are strictly positive, we do not need to handle the case of $p_k = 0$.\n\n**Step 2: Ranking Generation**\nFor a given set of $N$ entropies, a ranking is generated. This involves creating a list of tuples, where each tuple contains the negative entropy and the original index $i$ for an item. For example, for item $i$, the tuple is $(-H_i, i)$. Sorting this list in standard ascending lexicographical order effectively sorts by descending entropy values. The ascending index $i$ serves as the tie-breaker, as specified. The final ranking is the list of indices extracted from the sorted tuples.\n\n**Step 3: Inversion Count Calculation**\nA function is implemented to calculate the inversion count between a base ranking $R_0$ and a new ranking $R_{\\alpha}$. Given the two rankings as ordered lists of indices, the function iterates through all pairs of indices $(u, v)$ that appear in $R_0$. For each pair where $u$ precedes $v$ in $R_0$, it checks their relative order in $R_{\\alpha}$. If $v$ precedes $u$ in $R_{\\alpha}$, an inversion is counted. A small optimization involves pre-computing the positions of each index in $R_{\\alpha}$ to make lookups efficient.\n\n**Step 4: Main Procedure**\nFor each test case, defined by the number of classes $K$, the number of pool items $N$, the set of predictive distributions $\\{p_i\\}_{i=1}^{N}$, and a set of smoothing parameters $\\{\\alpha\\}$, the following process is executed:\n$1$. The original entropies $\\{H_i\\}_{i=1}^{N}$ are computed from the given distributions $\\{p_i\\}_{i=1}^{N}$.\n$2$. The original ranking $R_0$ is generated from these entropies.\n$3$. For each specified value of $\\alpha$:\n    a. If $\\alpha=0$, the smoothed ranking is identical to the original, so the inversion count is $0$ by definition.\n    b. The uniform distribution $u$ for $K$ classes is created.\n    c. The set of smoothed distributions $\\{p_{i,\\alpha}\\}_{i=1}^{N}$ is calculated using the formula $p_{i,\\alpha} = (1-\\alpha)p_i + \\alpha u$.\n    d. The smoothed entropies $\\{H_{i,\\alpha}\\}_{i=1}^{N}$ are computed from the smoothed distributions.\n    e. The new ranking $R_\\alpha$ is generated from the smoothed entropies.\n    f. The inversion count between $R_0$ and $R_\\alpha$ is computed and stored.\n\nThis procedure is applied to all test cases, and the resulting inversion counts are collected in the specified order for the final output. The implementation uses the `numpy` library for efficient vectorized computations.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the inversion count between original and smoothed entropy rankings\n    for a series of test cases in an active learning scenario.\n    \"\"\"\n\n    def calculate_entropy(p: np.ndarray) - float:\n        \"\"\"\n        Calculates the Shannon entropy of a probability distribution.\n        Logs are natural logarithms (base e).\n        It is assumed that all probabilities are  0.\n        \"\"\"\n        return -np.sum(p * np.log(p))\n\n    def get_ranking(entropies: np.ndarray, n_items: int) - list[int]:\n        \"\"\"\n        Generates a ranking of item indices.\n        The ranking is based on descending entropy, with ties broken by ascending index.\n        \"\"\"\n        # Create pairs of (-entropy, index) for sorting.\n        # Sorting this ascendingly achieves descending entropy sort, with index as tie-breaker.\n        indexed_entropies = [(-entropies[i], i) for i in range(n_items)]\n        indexed_entropies.sort()\n        ranking = [index for _, index in indexed_entropies]\n        return ranking\n\n    def calculate_inversion_count(r_a: list[int], r_b: list[int]) - int:\n        \"\"\"\n        Calculates the number of inversions between two rankings, r_a and r_b.\n        An inversion is a pair of items (u, v) such that u precedes v in r_a\n        but v precedes u in r_b.\n        \"\"\"\n        n_items = len(r_a)\n        if n_items == 0:\n            return 0\n        \n        # Create a map of item index to its position in the second ranking for O(1) lookups.\n        pos_b = {val: idx for idx, val in enumerate(r_b)}\n        \n        count = 0\n        for i in range(n_items):\n            for j in range(i + 1, n_items):\n                # u precedes v in ranking r_a\n                u, v = r_a[i], r_a[j]\n                \n                # Check if their order is inverted in r_b\n                if pos_b[u]  pos_b[v]:\n                    count += 1\n        return count\n\n    def process_case(k: int, n: int, p_pool: list[list[float]], alphas: list[float]) - list[int]:\n        \"\"\"\n        Processes a single test case to find inversion counts for each alpha.\n        \"\"\"\n        p_pool_np = np.array(p_pool, dtype=float)\n        \n        # Calculate original entropies and ranking (for alpha = 0)\n        h_0 = np.array([calculate_entropy(p) for p in p_pool_np])\n        r_0 = get_ranking(h_0, n)\n        \n        results = []\n        for alpha in alphas:\n            if alpha == 0.0:\n                # The inversion count between a ranking and itself is 0.\n                results.append(0)\n                continue\n\n            # Calculate smoothed distributions\n            u = np.full(k, 1.0 / k)\n            p_alpha = (1.0 - alpha) * p_pool_np + alpha * u\n            \n            # Calculate smoothed entropies and new ranking\n            h_alpha = np.array([calculate_entropy(p_a) for p_a in p_alpha])\n            r_alpha = get_ranking(h_alpha, n)\n            \n            # Compute and store inversion count\n            count = calculate_inversion_count(r_0, r_alpha)\n            results.append(count)\n            \n        return results\n\n    # --- Test Suite Definition ---\n\n    # Case A\n    case_A_K, case_A_N = 3, 5\n    case_A_P = [\n        [0.8, 0.1, 0.1], [0.34, 0.33, 0.33], [0.6, 0.3, 0.1],\n        [0.5, 0.49, 0.01], [0.9, 0.05, 0.05]\n    ]\n    case_A_alphas = [0.0, 0.2, 0.5, 1.0]\n\n    # Case B\n    case_B_K, case_B_N = 4, 4\n    case_B_P = [\n        [0.7, 0.1, 0.1, 0.1], [0.4, 0.3, 0.2, 0.1],\n        [0.4, 0.3, 0.2, 0.1], [0.25, 0.25, 0.25, 0.25]\n    ]\n    case_B_alphas = [0.3]\n\n    # Case C\n    case_C_K, case_C_N = 2, 6\n    case_C_P = [\n        [0.95, 0.05], [0.8, 0.2], [0.6, 0.4],\n        [0.51, 0.49], [0.99, 0.01], [0.5, 0.5]\n    ]\n    case_C_alphas = [0.1, 0.9]\n\n    # --- Execution and Output ---\n\n    all_results = []\n    \n    results_A = process_case(case_A_K, case_A_N, case_A_P, case_A_alphas)\n    all_results.extend(results_A)\n    \n    results_B = process_case(case_B_K, case_B_N, case_B_P, case_B_alphas)\n    all_results.extend(results_B)\n    \n    results_C = process_case(case_C_K, case_C_N, case_C_P, case_C_alphas)\n    all_results.extend(results_C)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```", "id": "3095020"}]}