{"hands_on_practices": [{"introduction": "Before we can estimate a causal effect, we must first determine if it is identifiable from the data we possess. The fundamental challenge is confounding, where a variable influences both treatment assignment and the outcome, creating a spurious association. This exercise [@problem_id:3106696] builds your intuition for this core problem by asking you to evaluate several hypothetical scenarios, determining when a simple regression adjustment strategy is sufficient to estimate the Average Treatment Effect (ATE) and, more importantly, when it fails due to unobserved confounders.", "problem": "Consider a binary treatment variable $T \\in \\{0,1\\}$, an observed covariate $X$, a possibly unobserved variable $U$, and an outcome $Y$. Assume the Stable Unit Treatment Value Assumption (SUTVA) and consistency, so that the observed outcome satisfies $Y = Y(T)$ for each unit, and define the average treatment effect (ATE) as $E\\big[Y(1) - Y(0)\\big]$. You will investigate whether regression adjustment using the conditional mean $E[Y \\mid T, X]$ suffices to identify the ATE, starting from the definitions of conditional independence and the law of total expectation. In all scenarios below, assume the error term $\\varepsilon$ is independent of $(T,X,U)$ with $E[\\varepsilon] = 0$ and finite variance.\n\nFor each scenario, determine whether using the regression function $E[Y \\mid T, X]$ is sufficient to identify the ATE, that is, whether the conditions under which $E[Y \\mid T, X]$ can be used to recover $E\\big[Y(1) - Y(0)\\big]$ hold. Select all scenarios where this is true.\n\nOption A:\n- $X \\sim \\text{Bernoulli}(0.5)$, $T \\sim \\text{Bernoulli}(0.5)$, and $T$ is independent of $X$.\n- The outcome is generated by $Y = 2T + X + \\varepsilon$.\n- There is no $U$ in this scenario.\n\nOption B:\n- $U \\sim \\text{Bernoulli}(0.5)$, $X \\sim \\text{Bernoulli}(0.5)$, and $X$ is independent of $U$.\n- The treatment is deterministically $T = U$.\n- The outcome is generated by $Y = 2T + U + \\varepsilon$.\n\nOption C:\n- $U \\sim \\text{Bernoulli}(0.5)$.\n- The observed covariate $X$ is a noisy proxy for $U$: $P(X = U) = 0.8$ and $P(X \\neq U) = 0.2$.\n- The treatment is deterministically $T = U$.\n- The outcome is generated by $Y = 2T + U + \\varepsilon$.\n\nOption D:\n- $U \\sim \\text{Bernoulli}(0.5)$, and $X = U$ (that is, $X$ equals $U$ without error).\n- The treatment is randomized conditional on $U$ so that $P(T = 1 \\mid U = 1) = 0.8$ and $P(T = 1 \\mid U = 0) = 0.2$.\n- The outcome is generated by $Y = 2T + U + \\varepsilon$.\n\nIn each option, argue from the base definitions (consistency/SUTVA, conditional independence, and the law of total expectation) whether $E[Y \\mid T, X]$ identifies $E\\big[Y(1) - Y(0)\\big]$, and when it fails due to $X$ not closing all backdoor paths. Choose all correct options.\n\nA. Option A\n\nB. Option B\n\nC. Option C\n\nD. Option D", "solution": "The problem asks to determine in which of the four scenarios the regression adjustment formula, based on the conditional expectation $E[Y \\mid T, X]$, is sufficient to identify the Average Treatment Effect (ATE), defined as $E\\big[Y(1) - Y(0)\\big]$.\n\nFirst, let's establish the theoretical foundation. The problem states we can assume SUTVA and consistency, which means $Y = Y(T)$. The ATE is $E[Y(1) - Y(0)]$. By the law of total expectation, we can write the ATE as:\n$$ E\\big[Y(1) - Y(0)\\big] = E_X\\Big[E\\big[Y(1) \\mid X\\big] - E\\big[Y(0) \\mid X\\big]\\Big] $$\nThe regression adjustment estimand is defined as:\n$$ \\tau_{adj} = E_X\\Big[E[Y \\mid T=1, X] - E[Y \\mid T=0, X]\\Big] $$\nBy consistency, $E[Y \\mid T=1, X] = E[Y(1) \\mid T=1, X]$ and $E[Y \\mid T=0, X] = E[Y(0) \\mid T=0, X]$.\nFor $\\tau_{adj}$ to be equal to the ATE, we require the conditional ignorability assumption (also known as unconfoundedness or selection on observables) to hold:\n$$ \\big(Y(1), Y(0)\\big) \\perp T \\mid X $$\nThis means that, conditional on the covariates $X$, the treatment assignment $T$ is independent of the potential outcomes. In the language of causal graphs, this is equivalent to stating that the set of covariates $X$ blocks all backdoor paths from the treatment $T$ to the outcome $Y$. A backdoor path is a non-causal path between $T$ and $Y$ that has an arrow into $T$.\n\nA common source of a backdoor path is a variable $U$ that is a common cause of both $T$ and $Y$ (a confounder). The path $T \\leftarrow U \\rightarrow Y$ must be blocked. If $U$ is part of the observed covariates $X$, conditioning on $X$ can block this path. If $U$ is unobserved, identification may fail.\n\nThe structural equation for the outcome in all scenarios is of the form $Y = 2T + \\text{terms not involving } T + \\varepsilon$.\nThe potential outcomes are derived by setting $T$ to $1$ or $0$:\n- $Y(1) = 2(1) + \\dots + \\varepsilon$\n- $Y(0) = 2(0) + \\dots + \\varepsilon$\nThe causal effect for any unit is $Y(1) - Y(0) = 2$.\nTherefore, the true Average Treatment Effect (ATE) in all four scenarios is:\n$$ E\\big[Y(1) - Y(0)\\big] = E[2] = 2 $$\nThe problem thus reduces to checking in which scenarios the regression adjustment estimand $\\tau_{adj}$ is equal to $2$.\n\nWe are given that for all scenarios, $E[\\varepsilon]=0$ and $\\varepsilon$ is independent of $(T, X, U)$.\n\n### Option A\n\n- **Setup**: $X \\sim \\text{Bernoulli}(0.5)$, $T \\sim \\text{Bernoulli}(0.5)$, $T \\perp X$. $Y = 2T + X + \\varepsilon$. There is no unobserved confounder $U$.\n- **Analysis**:\nThe potential outcomes are $Y(1) = 2 + X + \\varepsilon$ and $Y(0) = X + \\varepsilon$.\nThe conditional ignorability assumption is $(Y(1), Y(0)) \\perp T \\mid X$. The potential outcomes are functions of $X$ and $\\varepsilon$. We are given that $T$ is independent of $X$. We are also given that $\\varepsilon$ is independent of $(T, X, U)$, which implies $\\varepsilon \\perp T$. Since $T$ is independent of both $X$ and $\\varepsilon$, it is independent of any function of $(X, \\varepsilon)$. Thus, $T \\perp (Y(1), Y(0))$. Unconditional independence implies conditional independence given $X$. So, the condition holds. There are no backdoor paths from $T$ to $Y$. This is a randomized controlled trial.\nLet's compute the estimand:\n$$ E[Y \\mid T, X] = E[2T + X + \\varepsilon \\mid T, X] = 2T + X + E[\\varepsilon \\mid T, X] $$\nSince $\\varepsilon$ is independent of $(T,X)$, $E[\\varepsilon \\mid T, X] = E[\\varepsilon]=0$. So, $E[Y \\mid T, X] = 2T + X$.\nThe regression adjustment estimand is:\n$$ \\tau_{adj} = E_X\\Big[E[Y \\mid T=1, X] - E[Y \\mid T=0, X]\\Big] = E_X\\Big[ (2(1) + X) - (2(0) + X) \\Big] = E_X[2] = 2 $$\n- **Verdict**: Since $\\tau_{adj} = 2$, which is the true ATE, regression adjustment is sufficient to identify the ATE. **Correct**.\n\n### Option B\n\n- **Setup**: $U \\sim \\text{Bernoulli}(0.5)$, $X \\sim \\text{Bernoulli}(0.5)$, $X \\perp U$. Treatment is $T = U$. Outcome is $Y = 2T + U + \\varepsilon$.\n- **Analysis**:\nHere, $U$ is a common cause of $T$ (since $T=U$) and $Y$ (since $U$ is in the structural equation for $Y$). This creates a backdoor path $T \\leftarrow U \\rightarrow Y$. Since $U$ is unobserved, this path is potentially open. The observed covariate $X$ is independent of $U$, so conditioning on $X$ cannot block the path.\nThe potential outcomes are $Y(t) = 2t + U + \\varepsilon$. We need to check if $(Y(1), Y(0)) \\perp T \\mid X$. This is equivalent to checking if $U \\perp T \\mid X$. But $T=U$, so we are checking if $U \\perp U \\mid X$. This is clearly false. Conditional ignorability fails.\nLet's compute the estimand. First, substitute $T=U$ into the outcome equation for the observed data: $Y = 2T + T + \\varepsilon = 3T + \\varepsilon$.\n$$ E[Y \\mid T, X] = E[3T + \\varepsilon \\mid T, X] = 3T + E[\\varepsilon \\mid T, X] = 3T $$\nThe regression adjustment estimand is:\n$$ \\tau_{adj} = E_X\\Big[E[Y \\mid T=1, X] - E[Y \\mid T=0, X]\\Big] = E_X\\big[3(1) - 3(0)\\big] = E_X[3] = 3 $$\n- **Verdict**: The estimand $\\tau_{adj} = 3$ is not equal to the true ATE of $2$. The difference is due to confounding by the unobserved variable $U$. **Incorrect**.\n\n### Option C\n\n- **Setup**: $U \\sim \\text{Bernoulli}(0.5)$, $X$ is a noisy proxy for $U$ with $P(X=U)=0.8$. Treatment is $T = U$. Outcome is $Y = 2T + U + \\varepsilon$.\n- **Analysis**:\nSimilar to Option B, $U$ is a confounder, creating the backdoor path $T \\leftarrow U \\rightarrow Y$. Here, we observe a proxy $X$ for $U$. The causal structure is $U \\rightarrow X$ and $T \\leftarrow U \\rightarrow Y$. Conditioning on $X$, a child of the confounder, does not block the backdoor path. The condition $U \\perp T \\mid X$ fails because knowing $X$ provides information about $U$, but does not render $U$ independent of $T=U$.\nLet's compute the estimand. As in Option B, the observed outcome is $Y = 3T+\\varepsilon$.\n$$ E[Y \\mid T, X] = E[3T + \\varepsilon \\mid T, X] = 3T + E[\\varepsilon \\mid T, X] = 3T $$\nThe calculation is identical to Option B because once $T$ is given, $U$ is also known ($U=T$), so any additional information from $X$ about $U$ becomes redundant for the purpose of taking the conditional expectation of $U$.\nThe regression adjustment estimand is:\n$$ \\tau_{adj} = E_X\\Big[E[Y \\mid T=1, X] - E[Y \\mid T=0, X]\\Big] = E_X\\big[3 - 0\\big] = 3 $$\n- **Verdict**: The estimand $\\tau_{adj} = 3$ is not equal to the true ATE of $2$. This is a classic case of residual confounding due to an imperfect proxy. **Incorrect**.\n\n### Option D\n\n- **Setup**: $U \\sim \\text{Bernoulli}(0.5)$, $X = U$. Treatment $T$ is randomized conditional on $U$: $P(T=1|U=1)=0.8, P(T=1|U=0)=0.2$. Outcome is $Y = 2T + U + \\varepsilon$.\n- **Analysis**:\nHere again, $U$ is a common cause of $T$ and $Y$, so the backdoor path $T \\leftarrow U \\rightarrow Y$ exists. However, in this scenario, the confounder $U$ is perfectly observed via $X=U$. By conditioning on $X$, we are conditioning on the confounder itself, which blocks the backdoor path.\nLet's formally check the conditional ignorability assumption: $(Y(1), Y(0)) \\perp T \\mid X$. The potential outcomes are $Y(t) = 2t + U + \\varepsilon$. Since they are functions of $U$ and $\\varepsilon$, we must check if $(U, \\varepsilon) \\perp T \\mid X$. Given $X=U$, this becomes $(U, \\varepsilon) \\perp T \\mid U$. A variable is always dependent on itself, but when we condition on it, its value is fixed. So we only need to check if $\\varepsilon \\perp T \\mid U$. We are given that $\\varepsilon$ is independent of the joint distribution of $(T,X,U)$, which implies $\\varepsilon \\perp T \\mid U$. Thus, conditional ignorability holds.\nLet's compute the estimand:\n$$ E[Y \\mid T, X] = E[2T + U + \\varepsilon \\mid T, X] $$\nSince $X=U$, this is equivalent to:\n$$ E[Y \\mid T, X] = E[2T + X + \\varepsilon \\mid T, X] = 2T + X + E[\\varepsilon \\mid T,X] = 2T + X $$\nThe regression adjustment estimand is:\n$$ \\tau_{adj} = E_X\\Big[E[Y \\mid T=1, X] - E[Y \\mid T=0, X]\\Big] = E_X\\Big[ (2(1) + X) - (2(0) + X) \\Big] = E_X[2] = 2 $$\n- **Verdict**: Since $\\tau_{adj} = 2$, which is the true ATE, regression adjustment is sufficient to identify the ATE. This scenario represents successful control of confounding by including the confounder in the regression model. **Correct**.\n\nThe scenarios where using the regression function $E[Y \\mid T, X]$ is sufficient to identify the ATE are A and D.", "answer": "$$\\boxed{AD}$$", "id": "3106696"}, {"introduction": "Once we establish that an effect is identifiable, we can proceed to estimation. Inverse Probability Weighting (IPW) is a powerful technique that re-weights the observed data to mimic a randomized experiment. However, this power comes with a practical challenge: individuals with very low probability of receiving the treatment they got can lead to extremely large weights and high variance in the final estimate. This practice [@problem_id:3106651] is a hands-on coding exercise where you will implement the IPW estimator and confront this issue directly, exploring propensity score trimming as a method to navigate the crucial bias-variance trade-off.", "problem": "You are given a binary treatment variable $A \\in \\{0,1\\}$, a real-valued covariate $X \\in \\mathbb{R}$, and an outcome $Y \\in \\mathbb{R}$. Assume the Stable Unit Treatment Value Assumption (SUTVA), unconfoundedness $Y(a) \\perp A \\mid X$ for $a \\in \\{0,1\\}$, and positivity $0 < p(X) < 1$ almost surely, where $p(X)$ is the propensity score $p(X) = \\mathbb{P}(A=1 \\mid X)$. The target is the Average Treatment Effect (ATE) $\\tau = \\mathbb{E}[Y(1) - Y(0)]$. Consider the Inverse Probability Weighting (IPW) estimator of $\\tau$, which in its basic Horvitz–Thompson form uses weights $1/p(X)$ for treated and $1/(1-p(X))$ for control.\n\nTo assess robustness to extreme weights and study the bias–variance trade-off, define a trimming rule based on clipping the propensity score at a threshold $t \\in [0, 1/2)$. Let $p_t(X) = \\min(\\max(p(X), t), 1 - t)$. The clipped IPW estimator replaces $p(X)$ by $p_t(X)$ in the weights. This reduces variance but can introduce bias. Your task is to design and implement a robustness check that:\n- computes the untrimmed IPW estimator and its variance estimate,\n- computes the clipped IPW estimator across a grid of thresholds and quantifies the empirical bias–variance trade-off,\n- proposes and selects an adaptive trimming threshold based on the propensity score that balances a proxy of mean squared error (MSE),\n- and compares this adaptive choice to an oracle that minimizes true MSE (available in this simulation setting).\n\nBase your derivations on the core definitions and the law of iterated expectations. Do not assume any formula that directly gives the bias induced by clipping; instead, derive a workable plug-in approximation using conditional outcome models $\\mathbb{E}[Y \\mid A=a, X]$ estimated from data.\n\nImplementation details to follow exactly:\n1. Data-generating process. For each test case, simulate $n$ observations as follows. Draw $X \\sim \\mathcal{N}(0,1)$. Define the logit $\\ell(X) = \\beta_0 + \\beta_1 X$ and the propensity score $p(X) = \\sigma(\\ell(X)) = 1/(1+\\exp(-\\ell(X)))$. Draw $A \\sim \\mathrm{Bernoulli}(p(X))$. Let $Y = \\tau A + \\gamma X + \\varepsilon$, with $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)$ independent of $(X,A)$. The true ATE is the constant $\\tau$ in all test cases.\n2. Untrimmed IPW estimator. Using the known $p(X)$, compute the untrimmed Horvitz–Thompson IPW estimate\n   $$\\widehat{\\tau}_0 = \\frac{1}{n}\\sum_{i=1}^n \\left(\\frac{A_i Y_i}{p(X_i)} - \\frac{(1-A_i)Y_i}{1-p(X_i)}\\right).$$\n   Estimate its variance by the sample variance of the summands divided by $n$:\n   $$\\widehat{\\mathbb{V}}_0 = \\frac{1}{n}\\widehat{\\mathrm{Var}}\\left(\\frac{A Y}{p(X)} - \\frac{(1-A)Y}{1-p(X)}\\right).$$\n3. Clipped IPW estimator. For a grid $\\mathcal{T} = \\{0.00, 0.01, 0.02, \\ldots, 0.20\\}$, define $p_t(X)$ for each $t \\in \\mathcal{T}$ and compute\n   $$\\widehat{\\tau}_t = \\frac{1}{n}\\sum_{i=1}^n \\left(\\frac{A_i Y_i}{p_t(X_i)} - \\frac{(1-A_i)Y_i}{1-p_t(X_i)}\\right), \\quad \\widehat{\\mathbb{V}}_t = \\frac{1}{n}\\widehat{\\mathrm{Var}}\\left(\\frac{A Y}{p_t(X)} - \\frac{(1-A)Y}{1-p_t(X)}\\right).$$\n4. Bias–variance analysis. Derive an approximation to the clipping-induced bias using the law of iterated expectations and conditional outcome regression functions $m_a(x) = \\mathbb{E}[Y \\mid A=a, X=x]$. Use ordinary least squares to fit a single linear model with interaction,\n   $$Y = \\alpha_0 + \\alpha_A A + \\alpha_X X + \\alpha_{AX} (A X) + \\text{noise},$$\n   and then set $\\widehat{m}_0(x) = \\widehat{\\alpha}_0 + \\widehat{\\alpha}_X x$ and $\\widehat{m}_1(x) = \\widehat{\\alpha}_0 + \\widehat{\\alpha}_A + (\\widehat{\\alpha}_X + \\widehat{\\alpha}_{AX}) x$.\n   Show that the expected bias from clipping admits the approximation\n   $$\\mathrm{Bias}(t) \\approx \\mathbb{E}\\left[\\widehat{m}_1(X)\\left(\\frac{p(X)}{p_t(X)} - 1\\right) - \\widehat{m}_0(X)\\left(\\frac{1-p(X)}{1-p_t(X)} - 1\\right)\\right],$$\n   and implement its sample analogue by averaging over the observed sample. Define the proxy mean squared error\n   $$\\widehat{\\mathrm{MSE}}_{\\mathrm{proxy}}(t) = \\widehat{\\mathbb{V}}_t + \\left(\\widehat{\\mathrm{Bias}}(t)\\right)^2.$$\n5. Oracle and adaptive trimming. For each $t \\in \\mathcal{T}$, define the oracle mean squared error using the true $\\tau$:\n   $$\\widehat{\\mathrm{MSE}}_{\\mathrm{oracle}}(t) = \\widehat{\\mathbb{V}}_t + \\left(\\widehat{\\tau}_t - \\tau\\right)^2.$$\n   Define $t_{\\mathrm{oracle}}$ as any minimizer of $\\widehat{\\mathrm{MSE}}_{\\mathrm{oracle}}(t)$ over $\\mathcal{T}$, choosing the smallest $t$ in case of ties. Define $t_{\\mathrm{adapt}}$ as any minimizer of $\\widehat{\\mathrm{MSE}}_{\\mathrm{proxy}}(t)$ over $\\mathcal{T}$, choosing the smallest $t$ in case of ties. Report the adaptive trimmed estimate $\\widehat{\\tau}_{t_{\\mathrm{adapt}}}$ and its true mean squared error $\\widehat{\\mathbb{V}}_{t_{\\mathrm{adapt}}} + (\\widehat{\\tau}_{t_{\\mathrm{adapt}}} - \\tau)^2$.\n6. Test suite. Implement the above for the following four simulation scenarios, each with the stated random seed, which must be used to initialize a pseudo-random number generator so that results are reproducible:\n   - Case $1$: $n = 4000$, $\\beta_0 = 0.0$, $\\beta_1 = 1.2$, $\\tau = 1.0$, $\\gamma = 1.0$, $\\sigma = 1.0$, $\\text{seed} = 1$.\n   - Case $2$: $n = 4000$, $\\beta_0 = -2.5$, $\\beta_1 = 3.5$, $\\tau = 1.0$, $\\gamma = 1.0$, $\\sigma = 1.0$, $\\text{seed} = 2$.\n   - Case $3$: $n = 300$, $\\beta_0 = -3.0$, $\\beta_1 = 4.0$, $\\tau = 1.0$, $\\gamma = 1.0$, $\\sigma = 1.5$, $\\text{seed} = 3$.\n   - Case $4$: $n = 4000$, $\\beta_0 = 0.0$, $\\beta_1 = 0.0$ (constant propensity score $p(X)=0.5$), $\\tau = 1.0$, $\\gamma = 1.0$, $\\sigma = 1.0$, $\\text{seed} = 4$.\n7. Program output. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each case, output a list of six floating-point numbers in the following order:\n   - $\\widehat{\\tau}_0$,\n   - $t_{\\mathrm{oracle}}$,\n   - $t_{\\mathrm{adapt}}$,\n   - $\\widehat{\\tau}_{t_{\\mathrm{adapt}}}$,\n   - $\\min_{t \\in \\mathcal{T}} \\widehat{\\mathrm{MSE}}_{\\mathrm{oracle}}(t)$,\n   - $\\widehat{\\mathbb{V}}_{t_{\\mathrm{adapt}}} + (\\widehat{\\tau}_{t_{\\mathrm{adapt}}} - \\tau)^2$.\n   All floating-point numbers should be rounded to $6$ decimal places. The final output should thus be a single line representing a list of four inner lists (one per case), for example, $[[\\cdots],[\\cdots],[\\cdots],[\\cdots]]$.\nThere are no physical units in this problem, and no angles or percentages need to be reported. All outputs are real numbers without unit annotations.", "solution": "The user has provided a problem in the domain of statistical learning and causal inference, specifically concerning the evaluation of an Inverse Probability Weighting (IPW) estimator for the Average Treatment Effect (ATE). The task involves a simulation study to compare an untrimmed estimator with a clipped (trimmed) version, analyzing the induced bias–variance trade-off.\n\n### **Problem Validation**\n\n**Step 1: Extract Givens**\n\n*   **Variables**: Binary treatment $A \\in \\{0,1\\}$; real-valued covariate $X \\in \\mathbb{R}$; real-valued outcome $Y \\in \\mathbb{R}$.\n*   **Assumptions**: Stable Unit Treatment Value Assumption (SUTVA); unconfoundedness $Y(a) \\perp A \\mid X$ for $a \\in \\{0,1\\}$; positivity $0 < p(X) < 1$ where $p(X) = \\mathbb{P}(A=1 \\mid X)$.\n*   **Target Parameter**: Average Treatment Effect (ATE), $\\tau = \\mathbb{E}[Y(1) - Y(0)]$.\n*   **Estimators**:\n    *   Untrimmed IPW: $\\widehat{\\tau}_0 = \\frac{1}{n}\\sum_{i=1}^n \\left(\\frac{A_i Y_i}{p(X_i)} - \\frac{(1-A_i)Y_i}{1-p(X_i)}\\right)$.\n    *   Untrimmed Variance Estimator: $\\widehat{\\mathbb{V}}_0 = \\frac{1}{n}\\widehat{\\mathrm{Var}}\\left(\\frac{A Y}{p(X)} - \\frac{(1-A)Y}{1-p(X)}\\right)$.\n    *   Clipped Propensity Score: $p_t(X) = \\min(\\max(p(X), t), 1 - t)$ for a threshold $t \\in [0, 1/2)$.\n    *   Clipped IPW: $\\widehat{\\tau}_t = \\frac{1}{n}\\sum_{i=1}^n \\left(\\frac{A_i Y_i}{p_t(X_i)} - \\frac{(1-A_i)Y_i}{1-p_t(X_i)}\\right)$.\n    *   Clipped Variance Estimator: $\\widehat{\\mathbb{V}}_t = \\frac{1}{n}\\widehat{\\mathrm{Var}}\\left(\\frac{A Y}{p_t(X)} - \\frac{(1-A)Y}{1-p_t(X)}\\right)$.\n*   **Trimming Grid**: $\\mathcal{T} = \\{0.00, 0.01, 0.02, \\ldots, 0.20\\}$.\n*   **Data-Generating Process (DGP)**:\n    *   $X \\sim \\mathcal{N}(0,1)$.\n    *   $\\ell(X) = \\beta_0 + \\beta_1 X$, $p(X) = \\sigma(\\ell(X)) = 1/(1+\\exp(-\\ell(X)))$.\n    *   $A \\sim \\mathrm{Bernoulli}(p(X))$.\n    *   $Y = \\tau A + \\gamma X + \\varepsilon$, with $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)$. The true ATE is the given parameter $\\tau$.\n*   **Bias-Variance Analysis**:\n    *   Conditional outcome model to fit: $Y = \\alpha_0 + \\alpha_A A + \\alpha_X X + \\alpha_{AX} (A X) + \\text{noise}$.\n    *   Estimated models: $\\widehat{m}_0(x) = \\alpha_0 + \\alpha_X x$ and $\\widehat{m}_1(x) = \\alpha_0 + \\alpha_A + (\\alpha_X + \\alpha_{AX}) x$.\n    *   Bias approximation formula to derive and use: $\\mathrm{Bias}(t) \\approx \\mathbb{E}\\left[\\widehat{m}_1(X)\\left(\\frac{p(X)}{p_t(X)} - 1\\right) - \\widehat{m}_0(X)\\left(\\frac{1-p(X)}{1-p_t(X)} - 1\\right)\\right]$.\n    *   Proxy Mean Squared Error (MSE): $\\widehat{\\mathrm{MSE}}_{\\mathrm{proxy}}(t) = \\widehat{\\mathbb{V}}_t + \\left(\\widehat{\\mathrm{Bias}}(t)\\right)^2$.\n*   **Oracle and Adaptive Selection**:\n    *   Oracle MSE: $\\widehat{\\mathrm{MSE}}_{\\mathrm{oracle}}(t) = \\widehat{\\mathbb{V}}_t + \\left(\\widehat{\\tau}_t - \\tau\\right)^2$.\n    *   Oracle threshold: $t_{\\mathrm{oracle}} = \\arg\\min_{t \\in \\mathcal{T}} \\widehat{\\mathrm{MSE}}_{\\mathrm{oracle}}(t)$ (smallest $t$ in case of ties).\n    *   Adaptive threshold: $t_{\\mathrm{adapt}} = \\arg\\min_{t \\in \\mathcal{T}} \\widehat{\\mathrm{MSE}}_{\\mathrm{proxy}}(t)$ (smallest $t$ in case of ties).\n*   **Test Cases (Scenarios)**:\n    *   Case 1: $n = 4000$, $\\beta_0 = 0.0$, $\\beta_1 = 1.2$, $\\tau = 1.0$, $\\gamma = 1.0$, $\\sigma = 1.0$, $\\text{seed} = 1$.\n    *   Case 2: $n = 4000$, $\\beta_0 = -2.5$, $\\beta_1 = 3.5$, $\\tau = 1.0$, $\\gamma = 1.0$, $\\sigma = 1.0$, $\\text{seed} = 2$.\n    *   Case 3: $n = 300$, $\\beta_0 = -3.0$, $\\beta_1 = 4.0$, $\\tau = 1.0$, $\\gamma = 1.0$, $\\sigma = 1.5$, $\\text{seed} = 3$.\n    *   Case 4: $n = 4000$, $\\beta_0 = 0.0$, $\\beta_1 = 0.0$, $\\tau = 1.0$, $\\gamma = 1.0$, $\\sigma = 1.0$, $\\text{seed} = 4$.\n*   **Output Format**: For each case, a list of six numbers: $\\widehat{\\tau}_0$, $t_{\\mathrm{oracle}}$, $t_{\\mathrm{adapt}}$, $\\widehat{\\tau}_{t_{\\mathrm{adapt}}}$, $\\min_{t \\in \\mathcal{T}} \\widehat{\\mathrm{MSE}}_{\\mathrm{oracle}}(t)$, and $\\widehat{\\mathbb{V}}_{t_{\\mathrm{adapt}}} + (\\widehat{\\tau}_{t_{\\mathrm{adapt}}} - \\tau)^2$. All rounded to 6 decimal places, in a list of lists format.\n\n**Step 2: Validate Using Extracted Givens**\n\nThe problem is a well-defined simulation study in statistics, a standard methodology for evaluating estimator properties.\n1.  **Scientifically Grounded**: The problem is rooted in the core principles of causal inference. The IPW estimator, the concepts of unconfoundedness and positivity, and the bias-variance trade-off are central topics in this field. The DGP is standard and the requested derivations are based on fundamental statistical theory (law of iterated expectations, OLS).\n2.  **Well-Posed**: All necessary parameters, sample sizes, distributions, and procedural steps are explicitly provided. The target quantities are unambiguously defined. The existence and uniqueness of a solution (given the random seed) are guaranteed.\n3.  **Objective**: The problem is stated in precise, formal mathematical and statistical language, free of any subjectivity or ambiguity.\n4.  **No Other Flaws**: The problem is self-contained, consistent, and scientifically plausible as a simulation exercise. It is not trivial and requires non-negligible derivation and implementation.\n\n**Step 3: Verdict and Action**\n\nThe problem is **valid**. A complete solution will be provided.\n\n### **Solution Derivations and Methodology**\n\nThe objective is to estimate the Average Treatment Effect (ATE) $\\tau = \\mathbb{E}[Y(1) - Y(0)]$ and analyze the properties of the Inverse Probability Weighting (IPW) estimator under propensity score trimming.\n\n**1. IPW Estimator Principle**\n\nUnder the assumption of unconfoundedness ($Y(a) \\perp A \\mid X$), we have $\\mathbb{E}[Y \\mid A=a, X=x] = \\mathbb{E}[Y(a) \\mid A=a, X=x] = \\mathbb{E}[Y(a) \\mid X=x]$. Let $m_a(x) = \\mathbb{E}[Y(a) \\mid X=x]$. The ATE is then $\\tau = \\mathbb{E}[m_1(X) - m_0(X)]$. The IPW estimator leverages the propensity score $p(X) = \\mathbb{P}(A=1 \\mid X)$ to re-weight the observed data to estimate the means of the potential outcomes. For the untrimmed estimator, its expectation is:\n$$ \\mathbb{E}[\\widehat{\\tau}_0] = \\mathbb{E}\\left[\\frac{A Y}{p(X)} - \\frac{(1-A)Y}{1-p(X)}\\right] $$\nBy the law of iterated expectations and unconfoundedness:\n$$ \\mathbb{E}\\left[\\frac{A Y}{p(X)}\\right] = \\mathbb{E}\\left[\\mathbb{E}\\left[\\frac{A Y}{p(X)} \\mid X\\right]\\right] = \\mathbb{E}\\left[\\frac{p(X)\\mathbb{E}[Y \\mid A=1, X]}{p(X)}\\right] = \\mathbb{E}[m_1(X)] = \\mathbb{E}[Y(1)] $$\nSimilarly, $\\mathbb{E}\\left[\\frac{(1-A)Y}{1-p(X)}\\right] = \\mathbb{E}[Y(0)]$.\nThus, $\\mathbb{E}[\\widehat{\\tau}_0] = \\mathbb{E}[Y(1)] - \\mathbb{E}[Y(0)] = \\tau$, confirming that the untrimmed IPW estimator is unbiased. Its variance is high when propensity scores are close to $0$ or $1$.\n\n**2. Clipped IPW and Induced Bias**\n\nClipping the propensity score at a threshold $t$ replaces $p(X)$ with $p_t(X) = \\min(\\max(p(X), t), 1-t)$. This bounds the weights,\n$\\frac{1}{p_t(X)}$ and $\\frac{1}{1-p_t(X)}$, reducing estimator variance. However, it introduces bias.\nThe expectation of the clipped estimator $\\widehat{\\tau}_t$ is:\n$$ \\mathbb{E}[\\widehat{\\tau}_t] = \\mathbb{E}\\left[\\frac{A Y}{p_t(X)} - \\frac{(1-A)Y}{1-p_t(X)}\\right] $$\n$$ \\mathbb{E}[\\widehat{\\tau}_t] = \\mathbb{E}\\left[\\mathbb{E}\\left[\\frac{A Y}{p_t(X)} \\mid X\\right] - \\mathbb{E}\\left[\\frac{(1-A)Y}{1-p_t(X)} \\mid X\\right]\\right] $$\n$$ \\mathbb{E}[\\widehat{\\tau}_t] = \\mathbb{E}\\left[\\frac{p(X)}{p_t(X)}m_1(X) - \\frac{1-p(X)}{1-p_t(X)}m_0(X)\\right] $$\nThe bias is the difference between this expectation and the true ATE, $\\tau = \\mathbb{E}[m_1(X) - m_0(X)]$.\n$$ \\mathrm{Bias}(t) = \\mathbb{E}[\\widehat{\\tau}_t] - \\tau = \\mathbb{E}\\left[\\frac{p(X)}{p_t(X)}m_1(X) - \\frac{1-p(X)}{1-p_t(X)}m_0(X) - (m_1(X) - m_0(X))\\right] $$\nRearranging the terms inside the expectation gives the desired expression for the bias:\n$$ \\mathrm{Bias}(t) = \\mathbb{E}\\left[m_1(X)\\left(\\frac{p(X)}{p_t(X)} - 1\\right) - m_0(X)\\left(\\frac{1-p(X)}{1-p_t(X)} - 1\\right)\\right] $$\nThis expression clarifies the source of bias: it arises where $p(X) \\neq p_t(X)$ (i.e., where clipping occurs) and depends on the conditional mean outcomes in those regions.\n\n**3. Bias Estimation and Adaptive Threshold Selection**\n\nTo use this bias formula in practice, the true conditional outcome functions $m_a(X)$ are unknown and must be estimated from data. The problem specifies fitting a single linear model with an interaction term via Ordinary Least Squares (OLS):\n$$ Y = \\alpha_0 + \\alpha_A A + \\alpha_X X + \\alpha_{AX} (A \\cdot X) + \\text{noise} $$\nFrom the estimated coefficients $(\\widehat{\\alpha}_0, \\widehat{\\alpha}_A, \\widehat{\\alpha}_X, \\widehat{\\alpha}_{AX})$, we obtain estimates for the conditional outcome functions:\n$$ \\widehat{m}_0(x) = \\widehat{\\alpha}_0 + \\widehat{\\alpha}_X x $$\n$$ \\widehat{m}_1(x) = (\\widehat{\\alpha}_0 + \\widehat{\\alpha}_A) + (\\widehat{\\alpha}_X + \\widehat{\\alpha}_{AX}) x $$\nThe bias for a given trimming level $t$ is then approximated by its sample analogue, averaging over the observed data points $\\{X_i\\}_{i=1}^n$:\n$$ \\widehat{\\mathrm{Bias}}(t) = \\frac{1}{n}\\sum_{i=1}^n \\left[\\widehat{m}_1(X_i)\\left(\\frac{p(X_i)}{p_t(X_i)} - 1\\right) - \\widehat{m}_0(X_i)\\left(\\frac{1-p(X_i)}{1-p_t(X_i)} - 1\\right)\\right] $$\nThis estimated bias is combined with the estimated variance $\\widehat{\\mathbb{V}}_t$ to form a proxy for the Mean Squared Error (MSE):\n$$ \\widehat{\\mathrm{MSE}}_{\\mathrm{proxy}}(t) = \\widehat{\\mathbb{V}}_t + (\\widehat{\\mathrm{Bias}}(t))^2 $$\nThe adaptive threshold $t_{\\mathrm{adapt}}$ is chosen to minimize this proxy MSE over the grid $\\mathcal{T}$. This procedure attempts to find a data-driven balance between the reduction in variance and the introduction of bias from clipping. We compare this to an oracle choice, $t_{\\mathrm{oracle}}$, which minimizes the true sample MSE, $\\widehat{\\mathrm{MSE}}_{\\mathrm{oracle}}(t) = \\widehat{\\mathbb{V}}_t + (\\widehat{\\tau}_t - \\tau)^2$, an unachievable benchmark in practice where $\\tau$ is unknown. The final step evaluates the actual performance of the adaptive choice by computing its true sample MSE.\n\n**4. Implementation Plan**\nThe implementation will proceed as follows for each test case:\n1.  Set the random seed for reproducibility.\n2.  Generate the dataset $(X_i, A_i, Y_i)_{i=1}^n$ according to the specified DGP. The true propensity scores $p(X_i)$ are known.\n3.  Compute the untrimmed estimate $\\widehat{\\tau}_0$ and its variance $\\widehat{\\mathbb{V}}_0$.\n4.  Fit the OLS model for $Y$ to obtain coefficients and construct the estimated outcome functions $\\widehat{m}_0(X)$ and $\\widehat{m}_1(X)$.\n5.  Iterate through the trimming grid $\\mathcal{T}$:\n    a. For each $t$, compute the clipped scores $p_t(X_i)$.\n    b. Compute the clipped estimate $\\widehat{\\tau}_t$ and its variance $\\widehat{\\mathbb{V}}_t$.\n    c. Compute the estimated bias $\\widehat{\\mathrm{Bias}}(t)$.\n    d. Compute $\\widehat{\\mathrm{MSE}}_{\\mathrm{proxy}}(t)$ and $\\widehat{\\mathrm{MSE}}_{\\mathrm{oracle}}(t)$.\n6.  Determine $t_{\\mathrm{adapt}}$ and $t_{\\mathrm{oracle}}$ by finding the arguments that minimize the respective MSEs over the grid, respecting the tie-breaking rule.\n7.  Extract the required quantities: $\\widehat{\\tau}_0$, $t_{\\mathrm{oracle}}$, $t_{\\mathrm{adapt}}$, the adaptively-trimmed estimate $\\widehat{\\tau}_{t_{\\mathrm{adapt}}}$, the minimum oracle MSE, and the actual MSE of the adaptive estimator.\n8.  Format and collect the results for final output.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation study for all test cases and print results.\n    \"\"\"\n    test_cases = [\n        # (n, beta0, beta1, tau, gamma, sigma_eps, seed)\n        (4000, 0.0, 1.2, 1.0, 1.0, 1.0, 1),\n        (4000, -2.5, 3.5, 1.0, 1.0, 1.0, 2),\n        (300, -3.0, 4.0, 1.0, 1.0, 1.5, 3),\n        (4000, 0.0, 0.0, 1.0, 1.0, 1.0, 4),\n    ]\n\n    all_case_results = []\n    \n    for case in test_cases:\n        results = run_simulation_case(*case)\n        # Format results to 6 decimal places\n        formatted_results = [f\"{x:.6f}\" for x in results]\n        all_case_results.append(f\"[{','.join(formatted_results)}]\")\n\n    # Print in the specified final format [[...], [...], ...]\n    print(f\"[{','.join(all_case_results)}]\")\n\ndef run_simulation_case(n, beta0, beta1, tau, gamma, sigma_eps, seed):\n    \"\"\"\n    Executes the simulation and analysis for a single test case.\n    \"\"\"\n    # 1. Set seed and generate data\n    rng = np.random.default_rng(seed)\n    X = rng.normal(0, 1, n)\n    \n    logit_p = beta0 + beta1 * X\n    p_X = 1 / (1 + np.exp(-logit_p))\n    \n    A = rng.binomial(1, p_X, n)\n    \n    epsilon = rng.normal(0, sigma_eps, n)\n    Y = tau * A + gamma * X + epsilon\n\n    # 2. Untrimmed IPW estimator\n    ipw_term_0 = (A * Y / p_X) - ((1 - A) * Y / (1 - p_X))\n    tau_hat_0 = np.mean(ipw_term_0)\n    # var_hat_0 = np.var(ipw_term_0, ddof=1) / n # Not required for output\n\n    # 3. Clipped IPW estimators for grid of t\n    trim_grid = np.linspace(0.00, 0.20, 21)\n    \n    tau_hat_t_list = []\n    var_hat_t_list = []\n    \n    for t in trim_grid:\n        p_t_X = np.clip(p_X, t, 1 - t)\n        ipw_term_t = (A * Y / p_t_X) - ((1 - A) * Y / (1 - p_t_X))\n        tau_hat_t = np.mean(ipw_term_t)\n        var_hat_t = np.var(ipw_term_t, ddof=1) / n\n        tau_hat_t_list.append(tau_hat_t)\n        var_hat_t_list.append(var_hat_t)\n\n    # 4. Bias-variance analysis\n    # Fit OLS: Y ~ 1 + A + X + A*X\n    design_matrix = np.vstack([np.ones(n), A, X, A * X]).T\n    coeffs = np.linalg.lstsq(design_matrix, Y, rcond=None)[0]\n    alpha0_hat, alphaA_hat, alphaX_hat, alphaAX_hat = coeffs\n    \n    m0_hat = alpha0_hat + alphaX_hat * X\n    m1_hat = (alpha0_hat + alphaA_hat) + (alphaX_hat + alphaAX_hat) * X\n\n    bias_hat_t_list = []\n    for t in trim_grid:\n        p_t_X = np.clip(p_X, t, 1 - t)\n        bias_term1 = m1_hat * (p_X / p_t_X - 1)\n        bias_term2 = m0_hat * ((1 - p_X) / (1 - p_t_X) - 1)\n        bias_hat_t = np.mean(bias_term1 - bias_term2)\n        bias_hat_t_list.append(bias_hat_t)\n        \n    mse_proxy_t_list = np.array(var_hat_t_list) + np.array(bias_hat_t_list)**2\n    \n    # 5. Oracle and adaptive trimming\n    mse_oracle_t_list = np.array(var_hat_t_list) + (np.array(tau_hat_t_list) - tau)**2\n\n    # argmin returns the index of the first minimum, fulfilling the tie-breaking rule\n    idx_oracle = np.argmin(mse_oracle_t_list)\n    t_oracle = trim_grid[idx_oracle]\n    \n    idx_adapt = np.argmin(mse_proxy_t_list)\n    t_adapt = trim_grid[idx_adapt]\n\n    # 6. Collect final quantities\n    tau_hat_t_adapt = tau_hat_t_list[idx_adapt]\n    min_mse_oracle = mse_oracle_t_list[idx_oracle]\n    \n    # Actual MSE of the adaptively chosen estimator\n    var_hat_t_adapt = var_hat_t_list[idx_adapt]\n    mse_of_adapt_choice = var_hat_t_adapt + (tau_hat_t_adapt - tau)**2\n\n    return [\n        tau_hat_0,\n        t_oracle,\n        t_adapt,\n        tau_hat_t_adapt,\n        min_mse_oracle,\n        mse_of_adapt_choice\n    ]\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3106651"}, {"introduction": "We have seen that regression-based methods and IPW offer different approaches to estimating causal effects, each with its own vulnerabilities. It is natural to ask if we can combine them to create an estimator that is more reliable than either one alone. The Augmented Inverse Probability Weighted (AIPW) estimator achieves this by leveraging a property known as \"double robustness,\" which ensures the estimator is consistent if either the outcome model or the propensity score model is correctly specified. In this exercise [@problem_id:3106777], you will implement the AIPW estimator and empirically verify its double robustness, giving you a tangible understanding of why this method is a cornerstone of modern causal analysis.", "problem": "Consider a binary treatment setting with potential outcomes $Y(1)$ and $Y(0)$, an observed covariate $X \\in \\mathbb{R}$, a treatment indicator $T \\in \\{0,1\\}$, and an observed outcome $Y \\in \\mathbb{R}$. Assume the following foundational principles: consistency $Y = T Y(1) + (1 - T) Y(0)$, conditional ignorability $(Y(1), Y(0)) \\perp T \\mid X$, and positivity $0 < \\mathbb{P}(T = 1 \\mid X = x) < 1$ for all realizations $x$. The target parameter is the average treatment effect (ATE), defined as $\\Delta = \\mathbb{E}[Y(1) - Y(0)]$.\n\nYou will study the Augmented Inverse Probability Weighted (AIPW) estimator under two statistical learning models:\n- An outcome regression model $m_t(x) \\approx \\mathbb{E}[Y \\mid T = t, X = x]$ for $t \\in \\{0,1\\}$.\n- A propensity score model $e(x) \\approx \\mathbb{P}(T = 1 \\mid X = x)$.\n\nDevelop an example where the outcome regression is correctly specified while the propensity score is misspecified, and empirically demonstrate the double robustness property of the Augmented Inverse Probability Weighted (AIPW) estimator, which states that the estimator is consistent if either the outcome regression model is correctly specified or the propensity score model is correctly specified (not necessarily both).\n\nData generating process for simulation:\n- Covariate: $X \\sim \\mathcal{N}(0,1)$.\n- Treatment assignment: $T \\mid X \\sim \\text{Bernoulli}(e^\\star(X))$, where $e^\\star(x) = \\frac{1}{1 + \\exp(-(\\alpha x + c))}$ is the logistic function with parameters $\\alpha$ and $c$.\n- Outcome: $Y = \\beta_0 + \\beta_1 X + \\tau T + \\varepsilon$, with $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)$ independent of $X$ and $T$ given $X$ and $T$. The true average treatment effect is constant and equal to $\\tau$.\n\nModel specifications to be used in estimation:\n- Correct outcome regression: regress $Y$ on an intercept, $T$, and $X$; then set $m_1(x)$ and $m_0(x)$ to the fitted conditional means under $T = 1$ and $T = 0$, respectively, using this linear model.\n- Misspecified outcome regression: regress $Y$ on an intercept and $T$ only (omit $X$).\n- Correct propensity score: use the true function $e^\\star(x) = \\frac{1}{1 + \\exp(-(\\alpha x + c))}$ with the known parameters $\\alpha$ and $c$ from the data generating process.\n- Misspecified propensity score: use the constant function $e(x) = 0.5$.\n\nFrom the foundational principles and the definitions above, derive the AIPW estimator for $\\Delta$ and implement it. For a sample $\\{(X_i,T_i,Y_i)\\}_{i=1}^n$, with estimated $m_1(X_i)$, $m_0(X_i)$, and $e(X_i)$, compute the AIPW estimate of $\\Delta$ and report its bias relative to the true $\\tau$.\n\nTest suite:\nFor each test, simulate data according to the data generating process with the specified parameters, fit the models as specified, compute the AIPW estimate $\\widehat{\\Delta}_{\\text{AIPW}}$, and report the bias $\\widehat{\\Delta}_{\\text{AIPW}} - \\tau$ as a float rounded to $6$ decimal places.\n\n- Test $1$ (double robustness with correct outcome and misspecified propensity, happy path): $n = 50000$, $\\beta_0 = 2.0$, $\\beta_1 = 1.0$, $\\tau = 3.0$, $\\sigma = 1.0$, $\\alpha = 1.2$, $c = -0.2$, outcome regression correct, propensity score misspecified, random seed $123$.\n- Test $2$ (double robustness with correct propensity and misspecified outcome): $n = 50000$, $\\beta_0 = 2.0$, $\\beta_1 = 1.0$, $\\tau = 3.0$, $\\sigma = 1.0$, $\\alpha = 1.2$, $c = -0.2$, outcome regression misspecified, propensity score correct, random seed $456$.\n- Test $3$ (both models misspecified): $n = 50000$, $\\beta_0 = 2.0$, $\\beta_1 = 1.0$, $\\tau = 3.0$, $\\sigma = 1.0$, $\\alpha = 1.2$, $c = -0.2$, outcome regression misspecified, propensity score misspecified, random seed $789$.\n- Test $4$ (both models correct): $n = 50000$, $\\beta_0 = 2.0$, $\\beta_1 = 1.0$, $\\tau = 3.0$, $\\sigma = 1.0$, $\\alpha = 1.2$, $c = -0.2$, outcome regression correct, propensity score correct, random seed $321$.\n- Test $5$ (edge case with near-positivity boundary but valid, double robustness with correct outcome and misspecified propensity): $n = 50000$, $\\beta_0 = 2.0$, $\\beta_1 = 1.0$, $\\tau = 3.0$, $\\sigma = 1.0$, $\\alpha = 4.0$, $c = 0.0$, outcome regression correct, propensity score misspecified, random seed $654$.\n\nFinal output format:\nYour program should produce a single line of output containing the five biases as a comma-separated list enclosed in square brackets (e.g., $[b_1,b_2,b_3,b_4,b_5]$), where each $b_j$ is the bias for Test $j$ rounded to $6$ decimal places. No other text should be printed.", "solution": "The user wants me to solve a problem in the domain of causal inference.\nThe user wants me to solve a problem in the domain of causal inference. I will adopt the persona of a meticulous and exacting STEM professor. I will validate the problem first.\nI will adopt the persona of a meticulous and exacting STEM professor. I will validate the problem first.\n### Problem Validation\n\n#### Step 1: Extract Givens\n\n-   **Foundational Principles**:\n    1.  Consistency: $Y = T Y(1) + (1 - T) Y(0)$\n    2.  Conditional Ignorability (Unconfoundedness): $(Y(1), Y(0)) \\perp T \\mid X$\n    3.  Positivity (Overlap): $0 < \\mathbb{P}(T = 1 \\mid X = x) < 1$ for all realizations $x$.\n-   **Target Parameter**: Average Treatment Effect (ATE), $\\Delta = \\mathbb{E}[Y(1) - Y(0)]$.\n-   **Nuisance Models**:\n    1.  Outcome regression model: $m_t(x) \\approx \\mathbb{E}[Y \\mid T = t, X = x]$ for $t \\in \\{0,1\\}$.\n    2.  Propensity score model: $e(x) \\approx \\mathbb{P}(T = 1 \\mid X = x)$.\n-   **Data Generating Process (DGP)**:\n    -   Covariate: $X \\sim \\mathcal{N}(0,1)$.\n    -   Treatment assignment: $T \\mid X \\sim \\text{Bernoulli}(e^\\star(X))$, where the true propensity score is $e^\\star(x) = \\frac{1}{1 + \\exp(-(\\alpha x + c))}$.\n    -   Outcome: $Y = \\beta_0 + \\beta_1 X + \\tau T + \\varepsilon$, with $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)$. The true ATE is $\\tau$.\n-   **Model Specifications for Estimation**:\n    -   Correct outcome regression: Regress $Y$ on an intercept, $T$, and $X$.\n    -   Misspecified outcome regression: Regress $Y$ on an intercept and $T$ only.\n    -   Correct propensity score: Use the true function $e^\\star(x)$ with known parameters $\\alpha$ and $c$.\n    -   Misspecified propensity score: Use the constant function $e(x) = 0.5$.\n-   **Estimator**: Augmented Inverse Probability Weighted (AIPW) estimator for $\\Delta$.\n-   **Task**: For a sample $\\{(X_i,T_i,Y_i)\\}_{i=1}^n$, compute the AIPW estimate $\\widehat{\\Delta}_{\\text{AIPW}}$ and report its bias, $\\widehat{\\Delta}_{\\text{AIPW}} - \\tau$.\n-   **Test Suite**: Five test cases are provided with specific parameters: $n$, $\\beta_0$, $\\beta_1$, $\\tau$, $\\sigma$, $\\alpha$, $c$, model specification choices, and a random seed for reproducibility.\n\n#### Step 2: Validate Using Extracted Givens\n\nThe problem is subjected to rigorous validation against the specified criteria.\n\n-   **Scientifically Grounded**: The problem is firmly rooted in the potential outcomes framework (Rubin Causal Model), a standard and widely accepted paradigm for causal inference. The assumptions of consistency, ignorability, and positivity are foundational to this field. The AIPW estimator is a canonical, well-studied method. The data generating process is statistically sound and appropriate for a simulation study. The problem is free of pseudoscience or scientifically unsound claims.\n-   **Well-Posed**: The problem is clearly defined. The data generating process, the nuisance models to be estimated, the target estimand ($\\Delta$), and the specific estimator ($\\widehat{\\Delta}_{\\text{AIPW}}$) are all specified unambiguously. The provision of random seeds ensures that the simulation results are deterministic and reproducible. A unique, stable, and meaningful numerical solution (the list of biases) exists and is requested.\n-   **Objective**: The problem is stated in precise, quantitative, and unbiased language. All terms are standard within statistical learning and causal inference. There are no subjective or opinion-based statements.\n\nThe problem does not exhibit any of the defined invalidating flaws. It is not scientifically unsound, non-formalizable, incomplete, unrealistic, ill-posed, or trivial. The simulation study is a standard and informative method for investigating the properties of statistical estimators.\n\n#### Step 3: Verdict and Action\n\nThe problem is **valid**. A full solution will be provided.\n\n### Solution\n\nThe objective is to derive the Augmented Inverse Probability Weighted (AIPW) estimator and use it in a simulation study to demonstrate its double robustness property. The ATE, $\\Delta$, is the target parameter, defined as $\\Delta = \\mathbb{E}[Y(1) - Y(0)]$. By linearity of expectation, this is $\\mathbb{E}[Y(1)] - \\mathbb{E}[Y(0)]$. We will derive the estimator for each term separately.\n\n#### Derivation of the AIPW Estimator\n\nLet's focus on estimating $\\mu_1 = \\mathbb{E}[Y(1)]$. An analogous derivation holds for $\\mu_0 = \\mathbb{E}[Y(0)]$. The AIPW estimator is constructed to be consistent for $\\mu_1$ if either an outcome regression model $m_1(x) = \\mathbb{E}[Y \\mid T=1, X=x]$ is correctly specified, or a propensity score model $e(x) = \\mathbb{P}(T=1 \\mid X=x)$ is correctly specified.\n\nConsider the following variable, which is a function of the data $(X, T, Y)$ and the nuisance models $(m_1, e)$:\n$$\n\\psi_1(X, T, Y; m_1, e) = \\frac{T(Y - m_1(X))}{e(X)} + m_1(X)\n$$\nWe show that the expectation of $\\psi_1$ is $\\mu_1$ under either of the two correctness conditions.\n\n**Case 1: The propensity score model $e(x)$ is correct.**\nLet $e(x) = e^\\star(x) = \\mathbb{P}(T=1 \\mid X=x)$. We take the expectation of $\\psi_1$:\n$$ \\mathbb{E}[\\psi_1] = \\mathbb{E}\\left[ \\frac{T(Y - m_1(X))}{e^\\star(X)} + m_1(X) \\right] = \\mathbb{E}\\left[ \\frac{T(Y - m_1(X))}{e^\\star(X)} \\right] + \\mathbb{E}[m_1(X)] $$\nUsing the law of total expectation, $\\mathbb{E}[A] = \\mathbb{E}[\\mathbb{E}[A \\mid X]]$:\n$$ \\mathbb{E}\\left[ \\frac{T(Y - m_1(X))}{e^\\star(X)} \\right] = \\mathbb{E}\\left[ \\mathbb{E}\\left[ \\frac{T(Y - m_1(X))}{e^\\star(X)} \\mid X \\right] \\right] $$\nThe inner expectation is:\n$$ \\mathbb{E}\\left[ \\frac{T(Y - m_1(X))}{e^\\star(X)} \\mid X=x \\right] = \\frac{1}{e^\\star(x)} \\mathbb{E}[T(Y - m_1(x)) \\mid X=x] $$\nBy the law of total expectation again, conditioning on $T$:\n$$ \\mathbb{E}[T(Y - m_1(x)) \\mid X=x] = \\mathbb{E}[Y-m_1(x) \\mid X=x, T=1]\\mathbb{P}(T=1 \\mid X=x) + 0 $$\n$$ = \\mathbb{E}[Y-m_1(x) \\mid X=x, T=1] e^\\star(x) $$\nCombining these, the inner expectation becomes $\\mathbb{E}[Y-m_1(x) \\mid X=x, T=1]$. By consistency, $Y=Y(1)$ when $T=1$, so this is $\\mathbb{E}[Y(1)-m_1(x) \\mid X=x, T=1]$. By ignorability, this is $\\mathbb{E}[Y(1)-m_1(x) \\mid X=x]$.\nTherefore,\n$$ \\mathbb{E}[\\psi_1] = \\mathbb{E}[\\mathbb{E}[Y(1)-m_1(X) \\mid X]] + \\mathbb{E}[m_1(X)] = \\mathbb{E}[Y(1)-m_1(X)] + \\mathbb{E}[m_1(X)] = \\mathbb{E}[Y(1)] = \\mu_1 $$\nThe estimator is consistent for $\\mu_1$ if the propensity score is correct, regardless of the specification of $m_1(x)$.\n\n**Case 2: The outcome regression model $m_1(x)$ is correct.**\nLet $m_1(x) = m_1^\\star(x) = \\mathbb{E}[Y \\mid T=1, X=x]$. We again start with $\\mathbb{E}[\\psi_1] = \\mathbb{E}\\left[ \\frac{T(Y - m_1^\\star(X))}{e(X)} \\right] + \\mathbb{E}[m_1^\\star(X)]$.\nBy ignorability and consistency, $\\mathbb{E}[m_1^\\star(X)] = \\mathbb{E}[\\mathbb{E}[Y \\mid T=1, X]] = \\mathbb{E}[\\mathbb{E}[Y(1) \\mid X]] = \\mathbb{E}[Y(1)] = \\mu_1$.\nWe need to show that the first term is zero. Using the law of total expectation:\n$$ \\mathbb{E}\\left[ \\mathbb{E}\\left[ \\frac{T(Y - m_1^\\star(X))}{e(X)} \\mid X \\right] \\right] $$\nThe inner expectation is $\\frac{1}{e(x)} \\mathbb{E}[T(Y - m_1^\\star(x)) \\mid X=x]$. This evaluates to:\n$$ \\frac{1}{e(x)} \\mathbb{E}[Y-m_1^\\star(x) \\mid X=x, T=1] \\mathbb{P}(T=1 \\mid X=x) $$\nSince $m_1^\\star(x)$ is the true conditional mean, $\\mathbb{E}[Y \\mid X=x, T=1] - m_1^\\star(x) = 0$.\nThus, the first term is zero, and $\\mathbb{E}[\\psi_1] = \\mathbb{E}[m_1^\\star(X)] = \\mu_1$. The estimator is consistent for $\\mu_1$ if the outcome model is correct, regardless of the specification of $e(x)$.\n\n**The Full Estimator**\nAnalogously, the component for $\\mu_0 = \\mathbb{E}[Y(0)]$ is:\n$$ \\psi_0(X, T, Y; m_0, e) = \\frac{(1-T)(Y - m_0(X))}{1-e(X)} + m_0(X) $$\nThe AIPW estimator for the ATE, $\\Delta = \\mu_1 - \\mu_0$, based on a sample of size $n$ and estimated nuisance models $\\hat{m}_0, \\hat{m}_1, \\hat{e}$, is the sample mean of the difference of these components:\n$$ \\widehat{\\Delta}_{\\text{AIPW}} = \\frac{1}{n} \\sum_{i=1}^n (\\psi_{1,i} - \\psi_{0,i}) $$\n$$ \\widehat{\\Delta}_{\\text{AIPW}} = \\frac{1}{n} \\sum_{i=1}^n \\left[ \\left(\\frac{T_i(Y_i - \\hat{m}_1(X_i))}{\\hat{e}(X_i)} + \\hat{m}_1(X_i)\\right) - \\left(\\frac{(1-T_i)(Y_i - \\hat{m}_0(X_i))}{1-\\hat{e}(X_i)} + \\hat{m}_0(X_i)\\right) \\right] $$\n\n#### Simulation and Implementation\n\nWe implement a simulation to verify these properties. The problem provides a data generating process where the true outcome model is linear in $X$ and $T$, and the true propensity score follows a logistic model.\n$Y = \\beta_0 + \\beta_1 X + \\tau T + \\varepsilon$. From this, the true conditional outcome models are:\n$m_1^\\star(x) = \\mathbb{E}[Y \\mid X=x, T=1] = \\beta_0 + \\beta_1 x + \\tau$\n$m_0^\\star(x) = \\mathbb{E}[Y \\mid X=x, T=0] = \\beta_0 + \\beta_1 x$\n\n-   **Correct Outcome Model**: We fit a linear regression of $Y$ on an intercept, $T$, and $X$. This model form matches the true DGP, so it is correctly specified. The coefficients will be estimated from data. From the fitted model with coefficients $(\\hat{\\beta}_0, \\hat{\\beta}_T, \\hat{\\beta}_X)$, the estimated functions are $\\hat{m}_1(x) = \\hat{\\beta}_0 + \\hat{\\beta}_T + \\hat{\\beta}_X x$ and $\\hat{m}_0(x) = \\hat{\\beta}_0 + \\hat{\\beta}_X x$.\n-   **Misspecified Outcome Model**: We fit a linear regression of $Y$ on an intercept and $T$, omitting the confounder $X$. This is a misspecification because the true outcome depends on $X$ ($\\beta_1 \\neq 0$) and $X$ is related to $T$. The estimated functions $\\hat{m}_1(x)$ and $\\hat{m}_0(x)$ will be constant with respect to $x$.\n-   **Correct Propensity Score Model**: We use the true logistic functional form with the true parameters $\\alpha$ and $c$, so $\\hat{e}(x) = e^\\star(x)$. In a real-world scenario, these parameters would be estimated (e.g., via logistic regression), but here we use the true values to isolate the effect of model specification.\n-   **Misspecified Propensity Score Model**: We use a constant model, $\\hat{e}(x) = 0.5$, which is incorrect as the true propensity score depends on $X$ ($\\alpha \\neq 0$).\n\nThe five test cases explore the four combinations of model correctness, plus an edge case.\n-   Tests 1, 2, and 4 have at least one correctly specified model. Due to the double robustness property, the AIPW estimator should be approximately unbiased for the true ATE, $\\tau$.\n-   Test 3 has both models misspecified. The conditions for consistency are not met, and we expect a significant bias.\n-   Test 5 uses a larger $\\alpha$, pushing true propensity scores closer to $0$ and $1$, which can challenge estimators. However, with a correct outcome model, double robustness should still hold, and the bias should remain small.\n\nThe following Python code implements this simulation study. For each test case, it simulates data, estimates the nuisance functions as specified, computes the AIPW estimate for $\\Delta$, and calculates the bias relative to the known true ATE, $\\tau$.", "answer": "```python\nimport numpy as np\n\ndef simulate_data(n, beta0, beta1, tau, sigma, alpha, c, seed):\n    \"\"\"\n    Simulates data according to the specified data generating process.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # Generate covariate X from a standard normal distribution\n    X = rng.normal(0, 1, n)\n    \n    # Calculate true propensity scores e*(X)\n    logit_e_star = alpha * X + c\n    e_star = 1 / (1 + np.exp(-logit_e_star))\n    \n    # Generate treatment assignment T from a Bernoulli distribution\n    T = rng.binomial(1, e_star, n)\n    \n    # Generate outcome Y\n    epsilon = rng.normal(0, sigma, n)\n    Y = beta0 + beta1 * X + tau * T + epsilon\n    \n    return X, T, Y\n\ndef fit_outcome_regression(X, T, Y, is_correct):\n    \"\"\"\n    Fits the outcome regression model (correctly or misspecified) and returns\n    predicted outcomes for T=0 and T=1.\n    \"\"\"\n    n = len(Y)\n    if is_correct:\n        # Correct specification: Y ~ intercept + T + X\n        A = np.vstack([np.ones(n), T, X]).T\n        # Solve Normal Equations (A'A)beta = A'Y for OLS coefficients\n        try:\n            coeffs = np.linalg.solve(A.T @ A, A.T @ Y)\n        except np.linalg.LinAlgError:\n            coeffs = np.linalg.pinv(A.T @ A) @ A.T @ Y\n            \n        b0_hat, bT_hat, bX_hat = coeffs\n        \n        # Predicted outcomes under T=1 and T=0\n        m1_hat = b0_hat + bT_hat * 1 + bX_hat * X\n        m0_hat = b0_hat + bT_hat * 0 + bX_hat * X\n    else:\n        # Misspecified: Y ~ intercept + T (omits confounder X)\n        A = np.vstack([np.ones(n), T]).T\n        try:\n            coeffs = np.linalg.solve(A.T @ A, A.T @ Y)\n        except np.linalg.LinAlgError:\n            coeffs = np.linalg.pinv(A.T @ A) @ A.T @ Y\n\n        b0_hat, bT_hat = coeffs\n        \n        # Predicted outcomes are constant with respect to X\n        m1_hat = np.full(n, b0_hat + bT_hat * 1)\n        m0_hat = np.full(n, b0_hat + bT_hat * 0)\n        \n    return m0_hat, m1_hat\n\ndef get_propensity_scores(X, is_correct, alpha, c):\n    \"\"\"\n    Returns the propensity scores (correctly or misspecified).\n    \"\"\"\n    if is_correct:\n        # Use the true propensity score function with true parameters\n        logit_e = alpha * X + c\n        e_hat = 1 / (1 + np.exp(-logit_e))\n    else:\n        # Misspecified as a constant\n        e_hat = np.full(len(X), 0.5)\n        \n    return e_hat\n\ndef calculate_aipw_estimate(T, Y, m0_hat, m1_hat, e_hat):\n    \"\"\"\n    Calculates the AIPW estimate of the Average Treatment Effect (ATE).\n    \"\"\"\n    # Component for E[Y(1)]\n    psi1 = (T * (Y - m1_hat)) / e_hat + m1_hat\n    \n    # Component for E[Y(0)]\n    psi0 = ((1 - T) * (Y - m0_hat)) / (1 - e_hat) + m0_hat\n    \n    # AIPW estimate is the sample average of the difference\n    aipw_est = np.mean(psi1 - psi0)\n    \n    return aipw_est\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print results.\n    \"\"\"\n    test_cases = [\n        # (n, beta0, beta1, tau, sigma, alpha, c, outcome_correct, propensity_correct, seed)\n        (50000, 2.0, 1.0, 3.0, 1.0, 1.2, -0.2, True, False, 123), # Test 1\n        (50000, 2.0, 1.0, 3.0, 1.0, 1.2, -0.2, False, True, 456), # Test 2\n        (50000, 2.0, 1.0, 3.0, 1.0, 1.2, -0.2, False, False, 789), # Test 3\n        (50000, 2.0, 1.0, 3.0, 1.0, 1.2, -0.2, True, True, 321), # Test 4\n        (50000, 2.0, 1.0, 3.0, 1.0, 4.0, 0.0, True, False, 654), # Test 5\n    ]\n    \n    results = []\n    \n    for case in test_cases:\n        n, beta0, beta1, tau, sigma, alpha, c, outcome_correct, propensity_correct, seed = case\n        \n        # 1. Simulate data from the DGP\n        X, T, Y = simulate_data(n, beta0, beta1, tau, sigma, alpha, c, seed)\n        \n        # 2. Estimate nuisance functions based on test case specification\n        m0_hat, m1_hat = fit_outcome_regression(X, T, Y, outcome_correct)\n        e_hat = get_propensity_scores(X, propensity_correct, alpha, c)\n        \n        # 3. Calculate the AIPW estimate of the ATE\n        aipw_estimate = calculate_aipw_estimate(T, Y, m0_hat, m1_hat, e_hat)\n        \n        # 4. Calculate and store the bias\n        bias = aipw_estimate - tau\n        results.append(f\"{bias:.6f}\")\n        \n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3106777"}]}