{"hands_on_practices": [{"introduction": "Understanding text representation goes beyond simply counting words. It involves creating a geometric space where documents can be compared. This first practice provides a hands-on calculation to see how Term Frequency-Inverse Document Frequency (TF-IDF) weighting fundamentally alters this geometry compared to a raw Bag-of-Words (BoW) approach. By deriving the conditions under which a simple classifier changes its decision, you will build a concrete intuition for why weighting is a critical step in creating meaningful text features. [@problem_id:3179893]", "problem": "Consider binary text classification with $k$-Nearest Neighbors (k-NN), using cosine similarity on vector representations of documents. Two standard representations are Bag-of-Words (BoW) and Term Frequency–Inverse Document Frequency (TF-IDF). In BoW, each document $d$ is represented by raw term counts $tf_{d,j} \\in \\mathbb{N}$ over a fixed vocabulary. In TF-IDF, each term $j$ is weighted by $tf_{d,j} \\cdot \\mathrm{idf}_{j}$, where the inverse document frequency (IDF) is defined as $\\mathrm{idf}_{j} = \\ln\\!\\left(\\frac{N}{df_{j}}\\right)$, with $N$ the corpus size and $df_{j}$ the document frequency of term $j$. Cosine similarity between two vectors is the dot product of their $\\ell^{2}$-normalized representations. All vectors in this problem are $\\ell^{2}$-normalized after weighting. We use $k=1$ (one-nearest neighbor) for classification.\n\nA synthetic corpus has size $N = 1000$ and vocabulary terms $\\{t_{1}, t_{2}, t_{3}\\}$. Two labeled training documents are given by their BoW counts:\n- Class $\\mathcal{A}$: $D_{\\mathcal{A}}$ has counts $(tf_{D_{\\mathcal{A}},1}, tf_{D_{\\mathcal{A}},2}, tf_{D_{\\mathcal{A}},3}) = (10, 0, 0)$.\n- Class $\\mathcal{B}$: $D_{\\mathcal{B}}$ has counts $(tf_{D_{\\mathcal{B}},1}, tf_{D_{\\mathcal{B}},2}, tf_{D_{\\mathcal{B}},3}) = (0, 2, 3)$.\n\nA query document $q$ has counts $(tf_{q,1}, tf_{q,2}, tf_{q,3}) = (3, 0, 1)$. The document frequencies are $df_{1} = 900$, $df_{2} = 50$, and $df_{3}$ is unknown. Let $w_{j} = \\mathrm{idf}_{j} = \\ln\\!\\left(\\frac{N}{df_{j}}\\right)$ denote the IDF for term $t_{j}$.\n\nStarting from the core definitions above (BoW counts, TF-IDF weighting, cosine similarity on $\\ell^{2}$-normalized vectors, and $1$-NN decision by largest cosine similarity), do the following:\n\n1. Define the decision boundary between classes $\\mathcal{A}$ and $\\mathcal{B}$ as the locus of queries whose cosine similarities to $D_{\\mathcal{A}}$ and $D_{\\mathcal{B}}$ are equal. Using the given synthetic data, derive an analytic condition in terms of $w_{1}$, $w_{2}$, and $w_{3}$ under which the BoW-based $1$-NN and TF-IDF-based $1$-NN produce different class decisions for the fixed query $q$. Your derivation must begin from the base definitions provided and proceed by first principles.\n\n2. Compute the unique critical document frequency $df_{3}^{\\star}$ at which the TF-IDF decision boundary passes through the query $q$ (that is, the TF-IDF cosine similarities of $q$ to $D_{\\mathcal{A}}$ and $D_{\\mathcal{B}}$ are equal), while BoW $1$-NN assigns $q$ to class $\\mathcal{A}$. Use the given $N$, $df_{1}$, and $df_{2}$ to determine $w_{1}$ and $w_{2}$, and solve for $df_{3}^{\\star}$. Round your final numeric answer to four significant figures. Express your answer as a real-valued document count. For conceptual visualization, interpret how the TF-IDF weighting changes the decision boundary relative to BoW in the $(t_{1}, t_{3})$ subspace, but report only the requested numeric $df_{3}^{\\star}$.", "solution": "The problem requires us to analyze the classification of a query document $q$ using a $1$-Nearest Neighbor ($1$-NN) classifier with cosine similarity, under two different text representation schemes: Bag-of-Words (BoW) and Term Frequency-Inverse Document Frequency (TF-IDF). We will first derive a general condition under which the two schemes produce different results for the given query, and then solve for a specific parameter value where the TF-IDF decision boundary crosses the query.\n\nFirst, let us define the unnormalized vectors for the training documents $D_{\\mathcal{A}}$ and $D_{\\mathcal{B}}$, and the query document $q$, based on their given term frequency counts $(tf_1, tf_2, tf_3)$.\n\nFor the BoW representation, the vectors are the raw term counts:\n$v_{\\mathcal{A}} = (10, 0, 0)$\n$v_{\\mathcal{B}} = (0, 2, 3)$\n$v_{q} = (3, 0, 1)$\n\nFor the TF-IDF representation, the components are weighted by $w_j = \\mathrm{idf}_j = \\ln(N/df_j)$. The resulting vectors are:\n$v'_{\\mathcal{A}} = (10 \\cdot w_1, 0 \\cdot w_2, 0 \\cdot w_3) = (10w_1, 0, 0)$\n$v'_{\\mathcal{B}} = (0 \\cdot w_1, 2 \\cdot w_2, 3 \\cdot w_3) = (0, 2w_2, 3w_3)$\n$v'_{q} = (3 \\cdot w_1, 0 \\cdot w_2, 1 \\cdot w_3) = (3w_1, 0, w_3)$\n\nCosine similarity between two vectors $u$ and $v$ is given by $S(u, v) = \\frac{u \\cdot v}{\\|u\\|_2 \\|v\\|_2}$. The $1$-NN decision rule assigns the query to the class of the training document with the highest cosine similarity.\n\nLet's first analyze the BoW case. We compute the $\\ell^2$-norms of the BoW vectors:\n$\\|v_{\\mathcal{A}}\\|_2 = \\sqrt{10^2 + 0^2 + 0^2} = 10$\n$\\|v_{\\mathcal{B}}\\|_2 = \\sqrt{0^2 + 2^2 + 3^2} = \\sqrt{4+9} = \\sqrt{13}$\n$\\|v_{q}\\|_2 = \\sqrt{3^2 + 0^2 + 1^2} = \\sqrt{9+1} = \\sqrt{10}$\n\nThe cosine similarities for the BoW case are:\n$S_{\\text{BoW}}(q, D_{\\mathcal{A}}) = \\frac{v_q \\cdot v_{\\mathcal{A}}}{\\|v_q\\|_2 \\|v_{\\mathcal{A}}\\|_2} = \\frac{(3)(10) + (0)(0) + (1)(0)}{\\sqrt{10} \\cdot 10} = \\frac{30}{10\\sqrt{10}} = \\frac{3}{\\sqrt{10}}$\n$S_{\\text{BoW}}(q, D_{\\mathcal{B}}) = \\frac{v_q \\cdot v_{\\mathcal{B}}}{\\|v_q\\|_2 \\|v_{\\mathcal{B}}\\|_2} = \\frac{(3)(0) + (0)(2) + (1)(3)}{\\sqrt{10} \\sqrt{13}} = \\frac{3}{\\sqrt{130}}$\n\nTo compare these values, we note that $10  130$, so $\\sqrt{10}  \\sqrt{130}$, which implies $\\frac{1}{\\sqrt{10}} > \\frac{1}{\\sqrt{130}}$. Therefore, $S_{\\text{BoW}}(q, D_{\\mathcal{A}}) > S_{\\text{BoW}}(q, D_{\\mathcal{B}})$. The BoW-based $1$-NN classifier assigns query $q$ to Class $\\mathcal{A}$.\n\nNow, let's analyze the TF-IDF case. The norms of the TF-IDF vectors are:\n$\\|v'_{\\mathcal{A}}\\|_2 = \\sqrt{(10w_1)^2} = 10w_1$ (since $w_1 = \\ln(1000/900)>0$)\n$\\|v'_{\\mathcal{B}}\\|_2 = \\sqrt{(2w_2)^2 + (3w_3)^2} = \\sqrt{4w_2^2 + 9w_3^2}$\n$\\|v'_{q}\\|_2 = \\sqrt{(3w_1)^2 + (w_3)^2} = \\sqrt{9w_1^2 + w_3^2}$\n\nThe cosine similarities for the TF-IDF case are:\n$S_{\\text{TF-IDF}}(q, D_{\\mathcal{A}}) = \\frac{v'_q \\cdot v'_{\\mathcal{A}}}{\\|v'_q\\|_2 \\|v'_{\\mathcal{A}}\\|_2} = \\frac{(3w_1)(10w_1)}{\\sqrt{9w_1^2 + w_3^2} \\cdot 10w_1} = \\frac{3w_1}{\\sqrt{9w_1^2 + w_3^2}}$\n$S_{\\text{TF-IDF}}(q, D_{\\mathcal{B}}) = \\frac{v'_q \\cdot v'_{\\mathcal{B}}}{\\|v'_q\\|_2 \\|v'_{\\mathcal{B}}\\|_2} = \\frac{(3w_1)(0) + (0)(2w_2) + (w_3)(3w_3)}{\\sqrt{9w_1^2 + w_3^2} \\sqrt{4w_2^2 + 9w_3^2}} = \\frac{3w_3^2}{\\sqrt{9w_1^2 + w_3^2} \\sqrt{4w_2^2 + 9w_3^2}}$\n\n**Part 1: Condition for Different Decisions**\nThe BoW and TF-IDF based classifiers produce different decisions if the TF-IDF classifier assigns $q$ to Class $\\mathcal{B}$, which happens when $S_{\\text{TF-IDF}}(q, D_{\\mathcal{B}}) > S_{\\text{TF-IDF}}(q, D_{\\mathcal{A}})$.\n$$ \\frac{3w_3^2}{\\sqrt{9w_1^2 + w_3^2} \\sqrt{4w_2^2 + 9w_3^2}} > \\frac{3w_1}{\\sqrt{9w_1^2 + w_3^2}} $$\nSince the term $\\sqrt{9w_1^2 + w_3^2}$ is positive, we can multiply both sides by it and also divide by $3$:\n$$ \\frac{w_3^2}{\\sqrt{4w_2^2 + 9w_3^2}} > w_1 $$\nAssuming $w_1, w_2, w_3$ are positive (which they are for $1 \\le df_j  N$), both sides are positive, so we can square them without changing the inequality's direction:\n$$ \\frac{w_3^4}{4w_2^2 + 9w_3^2} > w_1^2 $$\n$$ w_3^4 > w_1^2 (4w_2^2 + 9w_3^2) $$\n$$ w_3^4 > 4w_1^2 w_2^2 + 9w_1^2 w_3^2 $$\nRearranging the terms gives the final analytic condition:\n$$ w_3^4 - 9w_1^2 w_3^2 - 4w_1^2 w_2^2 > 0 $$\n\n**Part 2: Calculation of Critical Document Frequency $df_3^{\\star}$**\nThe TF-IDF decision boundary passes through the query $q$ when the similarities are equal: $S_{\\text{TF-IDF}}(q, D_{\\mathcal{A}}) = S_{\\text{TF-IDF}}(q, D_{\\mathcal{B}})$. This corresponds to the equality case of the condition derived above:\n$$ w_3^4 - 9w_1^2 w_3^2 - 4w_1^2 w_2^2 = 0 $$\nThis is a quadratic equation for the variable $w_3^2$. Let $x = w_3^2$. The equation is $x^2 - (9w_1^2)x - (4w_1^2 w_2^2) = 0$.\n\nWe first compute the values of $w_1$ and $w_2$ using the provided data: $N=1000$, $df_1=900$, $df_2=50$.\n$w_1 = \\ln\\left(\\frac{1000}{900}\\right) = \\ln\\left(\\frac{10}{9}\\right)$\n$w_2 = \\ln\\left(\\frac{1000}{50}\\right) = \\ln(20)$\n\nWe solve for $x=w_3^2$ using the quadratic formula $x = \\frac{-b \\pm \\sqrt{b^2-4ac}}{2a}$ with $a=1$, $b=-9w_1^2$, and $c=-4w_1^2 w_2^2$.\n$$ x = w_3^2 = \\frac{9w_1^2 \\pm \\sqrt{(-9w_1^2)^2 - 4(1)(-4w_1^2 w_2^2)}}{2} $$\n$$ w_3^2 = \\frac{9w_1^2 \\pm \\sqrt{81w_1^4 + 16w_1^2 w_2^2}}{2} $$\nSince $w_3^2$ must be non-negative, we must take the positive root. The term under the square root is $\\sqrt{81w_1^4 + 16w_1^2 w_2^2} > \\sqrt{81w_1^4} = 9w_1^2$, so the numerator is negative if we choose the minus sign. Thus, we select the plus sign:\n$$ w_3^2 = \\frac{9w_1^2 + \\sqrt{81w_1^4 + 16w_1^2 w_2^2}}{2} $$\nNow we substitute the numerical values for the weights:\n$w_1 = \\ln(10/9) \\approx 0.1053605$\n$w_2 = \\ln(20) \\approx 2.9957323$\n$w_1^2 \\approx 0.0111008$\n$w_2^2 \\approx 8.974415$\n\nThe quadratic equation for $x=w_3^2$ is approximately:\n$x^2 - 9(0.0111008)x - 4(0.0111008)(8.974415) = 0$\n$x^2 - 0.0999072x - 0.398556 = 0$\n\nSolving for $x$:\n$$ x = \\frac{0.0999072 + \\sqrt{(-0.0999072)^2 - 4(1)(-0.398556)}}{2} $$\n$$ x = \\frac{0.0999072 + \\sqrt{0.0099814 + 1.594224}}{2} $$\n$$ x = \\frac{0.0999072 + \\sqrt{1.6042054}}{2} $$\n$$ x = \\frac{0.0999072 + 1.2665723}{2} = \\frac{1.3664795}{2} = 0.68323975 $$\nSo, $w_3^2 \\approx 0.68323975$. Taking the square root to find $w_3$:\n$w_3 = \\sqrt{0.68323975} \\approx 0.8265831$\n\nThe critical document frequency $df_3^{\\star}$ is found by inverting the IDF formula, $w_3 = \\ln(N/df_3^{\\star})$:\n$$ df_3^{\\star} = \\frac{N}{\\exp(w_3)} = \\frac{1000}{\\exp(0.8265831)} $$\n$$ df_3^{\\star} = \\frac{1000}{2.285460} \\approx 437.5492 $$\nRounding to four significant figures, we get $df_3^{\\star} = 437.5$. As established at the beginning, the BoW classifier assigns $q$ to class $\\mathcal{A}$, so this result satisfies all conditions of the problem.", "answer": "$$ \\boxed{437.5} $$", "id": "3179893"}, {"introduction": "While TF-IDF offers a powerful heuristic for weighting terms, probabilistic models provide a generative framework for understanding how documents are created. This exercise guides you through the derivation of the Expectation-Maximization (EM) algorithm for a foundational topic model, the Mixture of Unigrams. By building the learning algorithm from first principles and applying it to a concrete example, you will gain a deep appreciation for the inner workings of probabilistic text modeling and its conceptual limitations compared to more advanced models like Latent Dirichlet Allocation (LDA). [@problem_id:3179899]", "problem": "Consider the bag-of-words representation of a corpus with $D$ documents, vocabulary size $V$, and $K$ latent topics. Each document $d \\in \\{1,\\dots,D\\}$ is represented by a count vector $\\mathbf{n}_{d} = (n_{d,1},\\dots,n_{d,V})$ with total length $N_{d} = \\sum_{v=1}^{V} n_{d,v}$. In the mixture of unigrams model (a mixture of multinomials), the generative process for each document $d$ is: draw a single latent topic $z_{d} \\in \\{1,\\dots,K\\}$ according to mixing proportions $\\boldsymbol{\\pi} = (\\pi_{1},\\dots,\\pi_{K})$ on the probability simplex ($\\sum_{k=1}^{K} \\pi_{k} = 1$, $\\pi_{k} \\geq 0$), and then draw each of the $N_{d}$ word tokens independently from a topic-specific multinomial distribution $\\boldsymbol{\\phi}_{k} = (\\phi_{k,1},\\dots,\\phi_{k,V})$ on the vocabulary simplex ($\\sum_{v=1}^{V} \\phi_{k,v} = 1$, $\\phi_{k,v} \\geq 0$). The observed-data likelihood for a document $d$ under topic $k$ is $p(\\mathbf{n}_{d} \\mid z_{d}=k) = \\frac{N_{d}!}{\\prod_{v=1}^{V} n_{d,v}!} \\prod_{v=1}^{V} \\phi_{k,v}^{n_{d,v}}$.\n\nExpectation Maximization (EM) seeks to maximize the observed log-likelihood $\\ln p(\\{\\mathbf{n}_{d}\\}_{d=1}^{D} \\mid \\Theta)$, where $\\Theta = \\{\\boldsymbol{\\pi}, \\{\\boldsymbol{\\phi}_{k}\\}_{k=1}^{K}\\}$, by iterating an expectation step and a maximization step. The expectation step computes the posterior distribution of latent variables given current parameters, and the maximization step maximizes the expected complete-data log-likelihood subject to the probability simplex constraints. Begin from these core definitions and the general EM framework; do not invoke any pre-derived update formulas.\n\nTasks:\n- Derive, from first principles, the EM updates for the mixture of unigrams model: the expression for the responsibilities $\\gamma_{d,k} = p(z_{d}=k \\mid \\mathbf{n}_{d}, \\Theta)$ in the expectation step, and the parameter updates for $\\pi_{k}$ and $\\phi_{k,v}$ in the maximization step using Lagrange multipliers to enforce the simplex constraints.\n- Compare conceptual and modeling limitations of the mixture of unigrams to Latent Dirichlet Allocation (LDA), which assigns topics at the token level and places Dirichlet priors over document-topic proportions and topic-word distributions. Explain why a single-topic-per-document assumption restricts expressivity, and connect this to the bag-of-words setup. Discuss how Term Frequency–Inverse Document Frequency (TF-IDF) weighting differs from probabilistic generative modeling and why naively substituting TF-IDF weights for counts can break the multinomial likelihood interpretation.\n- Analyze convergence: argue why EM for this model guarantees a non-decreasing observed log-likelihood across iterations, and explain the possibility of convergence to local optima due to non-convexity. Provide a concise, principled explanation based on the EM lower bound and Jensen’s inequality.\n- Finally, consider the following concrete corpus and initialization. Let $V = 3$, $K = 2$, and $D = 2$. The documents are:\n  - Document $d=1$: $\\mathbf{n}_{1} = (n_{1,1}, n_{1,2}, n_{1,3}) = (2, 1, 0)$ with $N_{1} = 3$,\n  - Document $d=2$: $\\mathbf{n}_{2} = (n_{2,1}, n_{2,2}, n_{2,3}) = (0, 1, 2)$ with $N_{2} = 3$.\n  Initialize the parameters as $\\boldsymbol{\\pi} = (\\pi_{1}, \\pi_{2}) = (0.5, 0.5)$, $\\boldsymbol{\\phi}_{1} = (\\phi_{1,1}, \\phi_{1,2}, \\phi_{1,3}) = (0.6, 0.3, 0.1)$, and $\\boldsymbol{\\phi}_{2} = (\\phi_{2,1}, \\phi_{2,2}, \\phi_{2,3}) = (0.1, 0.3, 0.6)$. Perform one full EM iteration (compute the responsibilities and then update the parameters), and then compute the updated value of $\\phi_{1,1}$. Round your final numeric answer to four significant figures.", "solution": "The problem is valid as it is scientifically grounded in statistical learning theory, well-posed with all necessary data and definitions, and objective in its formulation. It requests derivations, conceptual analysis, and a numerical calculation based on standard models and algorithms.\n\n### Part 1: Derivation of Expectation-Maximization (EM) Updates\n\nThe mixture of unigrams model assumes that each document $d$ is generated from a single latent topic $z_d$, drawn from a distribution with parameters $\\boldsymbol{\\pi}$, and its words are then drawn from the corresponding topic-word distribution $\\boldsymbol{\\phi}_{z_d}$. We seek to find the parameters $\\Theta = \\{\\boldsymbol{\\pi}, \\{\\boldsymbol{\\phi}_{k}\\}_{k=1}^{K}\\}$ that maximize the observed-data log-likelihood for a corpus of $D$ documents, $\\mathcal{L}(\\Theta) = \\ln p(\\{\\mathbf{n}_{d}\\}_{d=1}^{D} \\mid \\Theta) = \\sum_{d=1}^{D} \\ln p(\\mathbf{n}_{d} \\mid \\Theta)$. The Expectation-Maximization (EM) algorithm is an iterative method for this purpose.\n\nThe marginal likelihood for a single document $\\mathbf{n}_d$ is given by summing over the latent topic assignments:\n$$p(\\mathbf{n}_{d} \\mid \\Theta) = \\sum_{k=1}^{K} p(\\mathbf{n}_{d}, z_{d}=k \\mid \\Theta) = \\sum_{k=1}^{K} p(\\mathbf{n}_{d} \\mid z_d=k, \\Theta) p(z_d=k \\mid \\Theta)$$\n$$p(\\mathbf{n}_{d} \\mid \\Theta) = \\sum_{k=1}^{K} \\pi_k \\left( \\frac{N_{d}!}{\\prod_{v=1}^{V} n_{d,v}!} \\prod_{v=1}^{V} \\phi_{k,v}^{n_{d,v}} \\right)$$\n\n**E-Step: Compute Responsibilities**\n\nThe expectation step computes the posterior probability of the latent topic assignment for each document, given the observed word counts $\\mathbf{n}_d$ and the current parameter estimates $\\Theta^{(t)}$. These posterior probabilities are called responsibilities, denoted by $\\gamma_{d,k}$.\n$$\\gamma_{d,k} = p(z_{d}=k \\mid \\mathbf{n}_{d}, \\Theta^{(t)})$$\nUsing Bayes' theorem:\n$$\\gamma_{d,k} = \\frac{p(\\mathbf{n}_{d} \\mid z_{d}=k, \\Theta^{(t)}) p(z_{d}=k \\mid \\Theta^{(t)})}{p(\\mathbf{n}_{d} \\mid \\Theta^{(t)})} = \\frac{p(\\mathbf{n}_{d} \\mid z_{d}=k, \\Theta^{(t)}) p(z_{d}=k \\mid \\Theta^{(t)})}{\\sum_{j=1}^{K} p(\\mathbf{n}_{d} \\mid z_{d}=j, \\Theta^{(t)}) p(z_{d}=j \\mid \\Theta^{(t)})}$$\nSubstituting the model definitions, $p(z_{d}=k \\mid \\Theta^{(t)}) = \\pi_k^{(t)}$ and $p(\\mathbf{n}_{d} \\mid z_{d}=k, \\Theta^{(t)}) = C_d \\prod_{v=1}^{V} (\\phi_{k,v}^{(t)})^{n_{d,v}}$, where $C_d = \\frac{N_{d}!}{\\prod_{v=1}^{V} n_{d,v}!}$ is the multinomial coefficient. Since $C_d$ does not depend on the topic index $k$ or the parameters, it cancels out from the numerator and denominator.\n$$\\gamma_{d,k} = \\frac{\\pi_k^{(t)} \\prod_{v=1}^{V} (\\phi_{k,v}^{(t)})^{n_{d,v}}}{\\sum_{j=1}^{K} \\pi_j^{(t)} \\prod_{v=1}^{V} (\\phi_{j,v}^{(t)})^{n_{d,v}}}$$\nThis is the update for the responsibilities in the E-step.\n\n**M-Step: Maximize Expected Complete-Data Log-Likelihood**\n\nThe maximization step finds the new parameter estimates $\\Theta^{(t+1)}$ that maximize the expected complete-data log-likelihood, $Q(\\Theta \\mid \\Theta^{(t)})$. The complete-data log-likelihood is:\n$$\\ln p(\\{\\mathbf{n}_d, z_d\\}_{d=1}^D \\mid \\Theta) = \\sum_{d=1}^{D} \\ln p(\\mathbf{n}_d, z_d \\mid \\Theta) = \\sum_{d=1}^{D} \\sum_{k=1}^{K} \\mathbb{I}(z_d=k) \\ln \\left( \\pi_k p(\\mathbf{n}_d \\mid z_d=k, \\Theta) \\right)$$\n$$= \\sum_{d=1}^{D} \\sum_{k=1}^{K} \\mathbb{I}(z_d=k) \\left( \\ln \\pi_k + \\ln C_d + \\sum_{v=1}^{V} n_{d,v} \\ln \\phi_{k,v} \\right)$$\nwhere $\\mathbb{I}(\\cdot)$ is the indicator function. The expectation $Q(\\Theta \\mid \\Theta^{(t)})$ is taken with respect to the posterior $p(\\{z_d\\} \\mid \\{\\mathbf{n}_d\\}, \\Theta^{(t)})$. The expectation of the indicator function is the responsibility: $E[\\mathbb{I}(z_d=k)] = p(z_d=k \\mid \\mathbf{n}_d, \\Theta^{(t)}) = \\gamma_{d,k}$.\n$$Q(\\Theta \\mid \\Theta^{(t)}) = \\sum_{d=1}^{D} \\sum_{k=1}^{K} \\gamma_{d,k} \\left( \\ln \\pi_k + \\sum_{v=1}^{V} n_{d,v} \\ln \\phi_{k,v} \\right) + \\text{const.}$$\nThe term $\\sum_{d=1}^D \\sum_{k=1}^K \\gamma_{d,k} \\ln C_d$ is constant with respect to $\\Theta$ and can be ignored during maximization. We must maximize $Q$ subject to the constraints $\\sum_{k=1}^{K} \\pi_k = 1$ and $\\sum_{v=1}^{V} \\phi_{k,v} = 1$ for all $k \\in \\{1,\\dots,K\\}$.\n\nUpdate for $\\pi_k$: We form the Lagrangian for the terms involving $\\boldsymbol{\\pi}$:\n$$\\mathcal{L}_{\\pi} = \\sum_{d=1}^{D} \\sum_{k=1}^{K} \\gamma_{d,k} \\ln \\pi_k + \\lambda_{\\pi} \\left( \\sum_{k=1}^{K} \\pi_k - 1 \\right)$$\nTaking the derivative with respect to $\\pi_k$ and setting it to zero:\n$$\\frac{\\partial \\mathcal{L}_{\\pi}}{\\partial \\pi_k} = \\frac{1}{\\pi_k} \\sum_{d=1}^{D} \\gamma_{d,k} + \\lambda_{\\pi} = 0 \\implies \\pi_k = -\\frac{\\sum_{d=1}^{D} \\gamma_{d,k}}{\\lambda_{\\pi}}$$\nSumming over $k$: $\\sum_{k=1}^{K} \\pi_k = 1 = -\\frac{1}{\\lambda_{\\pi}} \\sum_{k=1}^{K} \\sum_{d=1}^{D} \\gamma_{d,k}$. Since $\\sum_{k=1}^{K} \\gamma_{d,k} = 1$, the double summation equals $D$. Thus, $1 = -D/\\lambda_{\\pi}$, so $\\lambda_{\\pi} = -D$. Substituting back gives the update rule:\n$$\\pi_k^{(t+1)} = \\frac{\\sum_{d=1}^{D} \\gamma_{d,k}}{D}$$\n\nUpdate for $\\phi_{k,v}$: The terms involving $\\boldsymbol{\\phi}_k$ are independent for each topic $k$. For a fixed $k$, we form the Lagrangian:\n$$\\mathcal{L}_{\\phi_k} = \\sum_{d=1}^{D} \\gamma_{d,k} \\sum_{v=1}^{V} n_{d,v} \\ln \\phi_{k,v} + \\lambda_{\\phi_k} \\left( \\sum_{v=1}^{V} \\phi_{k,v} - 1 \\right)$$\nTaking the derivative with respect to $\\phi_{k,v}$ and setting it to zero:\n$$\\frac{\\partial \\mathcal{L}_{\\phi_k}}{\\partial \\phi_{k,v}} = \\frac{1}{\\phi_{k,v}} \\sum_{d=1}^{D} \\gamma_{d,k} n_{d,v} + \\lambda_{\\phi_k} = 0 \\implies \\phi_{k,v} = -\\frac{\\sum_{d=1}^{D} \\gamma_{d,k} n_{d,v}}{\\lambda_{\\phi_k}}$$\nSumming over $v$: $\\sum_{v=1}^{V} \\phi_{k,v} = 1 = -\\frac{1}{\\lambda_{\\phi_k}} \\sum_{v=1}^{V} \\sum_{d=1}^{D} \\gamma_{d,k} n_{d,v}$. Let the double summation be $S_k = \\sum_{d=1}^{D} \\gamma_{d,k} \\sum_{v=1}^{V} n_{d,v} = \\sum_{d=1}^{D} \\gamma_{d,k} N_d$. This is the effective total number of words assigned to topic $k$. Then $1 = -S_k/\\lambda_{\\phi_k}$, so $\\lambda_{\\phi_k} = -S_k$. Substituting back gives the update rule:\n$$\\phi_{k,v}^{(t+1)} = \\frac{\\sum_{d=1}^{D} \\gamma_{d,k} n_{d,v}}{\\sum_{d=1}^{D} \\gamma_{d,k} N_d} = \\frac{\\sum_{d=1}^{D} \\gamma_{d,k} n_{d,v}}{\\sum_{j=1}^{V} \\sum_{d=1}^{D} \\gamma_{d,k} n_{d,j}}$$\n\n### Part 2: Conceptual and Modeling Limitations\n\nThe mixture of unigrams (MoU) model, while a foundational topic model, has significant limitations compared to more advanced models like Latent Dirichlet Allocation (LDA), and its probabilistic nature distinguishes it from heuristic methods like Term Frequency–Inverse Document Frequency (TF-IDF).\n\n- **Mixture of Unigrams vs. Latent Dirichlet Allocation (LDA)**: The primary distinction lies in the modeling of document-level topic composition. MoU assumes that each document is generated from exactly one topic. This \"single-topic-per-document\" assumption severely restricts its expressivity. A document (e.g., a scientific paper, a news article) often discusses multiple themes. LDA addresses this by assuming each document is a mixture of topics. For each word token within a document, a topic is chosen from that document's specific topic mixture, and then the word is drawn from the selected topic's word distribution. This allows a document to be represented by a proportion vector over topics (e.g., $40\\%$ physics, $30\\%$ computer science, $30\\%$ mathematics), which is far more realistic for most real-world corpora. This token-level topic assignment is a more fine-grained and powerful representation than the document-level assignment in MoU. The bag-of-words setup, which discards word order, is common to both models, but LDA leverages it more effectively by modeling topical mixtures.\n\n- **TF-IDF vs. Probabilistic Models**: TF-IDF is a numerical weighting scheme, not a probabilistic generative model. Its value for a term $t$ in a document $d$, $\\text{TF-IDF}(t,d)$, is a heuristic score designed to reflect the term's importance to that document within a corpus. In contrast, probabilistic models like MoU and LDA provide a generative story for how the document was created. The parameters of these models (e.g., $\\boldsymbol{\\phi}_k$) are probabilities with a clear interpretation. Naively substituting TF-IDF weights, which are non-integer real values, for the integer word counts $n_{d,v}$ in the multinomial likelihood function $p(\\mathbf{n}_{d} \\mid ...)=C_d\\prod_{v} \\phi_{k,v}^{n_{d,v}}$ fundamentally breaks the model. The multinomial distribution is defined for discrete counts. Using real-valued exponents $\\phi_{k,v}^{\\text{TF-IDF}_{d,v}}$ would change the function, but it would no longer represent the likelihood of observed counts under a multinomial model. The statistical foundation, including the generative process and the principled EM-based parameter estimation, would be invalidated.\n\n### Part 3: Convergence Analysis\n\nThe EM algorithm is guaranteed to produce a sequence of parameter estimates $\\Theta^{(t)}$ for which the observed-data log-likelihood $\\mathcal{L}(\\Theta^{(t)}) = \\ln p(X \\mid \\Theta^{(t)})$ is non-decreasing. This can be proven by introducing the evidence lower bound (ELBO) on the log-likelihood. For any auxiliary distribution $q(Z)$ over the latent variables $Z$, Jensen's inequality for the concave logarithm function allows us to write:\n$$\\mathcal{L}(\\Theta) = \\ln \\sum_Z p(X, Z \\mid \\Theta) = \\ln \\sum_Z q(Z)\\frac{p(X, Z \\mid \\Theta)}{q(Z)} \\geq \\sum_Z q(Z) \\ln \\frac{p(X, Z \\mid \\Theta)}{q(Z)} = \\mathcal{F}(q, \\Theta)$$\nThe EM algorithm can be viewed as a coordinate ascent algorithm on this lower bound $\\mathcal{F}(q, \\Theta)$.\n1.  **E-Step**: With parameters $\\Theta^{(t)}$ fixed, we maximize $\\mathcal{F}(q, \\Theta^{(t)})$ with respect to $q(Z)$. This maximum is achieved when $q(Z)$ is set to the posterior distribution of the latent variables, $p(Z \\mid X, \\Theta^{(t)})$. At this point, the lower bound is tight, i.e., $\\mathcal{L}(\\Theta^{(t)}) = \\mathcal{F}(p(Z \\mid X, \\Theta^{(t)}), \\Theta^{(t)})$.\n2.  **M-Step**: With $q(Z) = p(Z \\mid X, \\Theta^{(t)})$ fixed, we find new parameters $\\Theta^{(t+1)}$ by maximizing $\\mathcal{F}(q, \\Theta)$ with respect to $\\Theta$. By definition of maximization, $\\mathcal{F}(q, \\Theta^{(t+1)}) \\geq \\mathcal{F}(q, \\Theta^{(t)})$.\n\nCombining these steps, we have the sequence:\n$$\\mathcal{L}(\\Theta^{(t+1)}) \\geq \\mathcal{F}(p(Z \\mid X, \\Theta^{(t)}), \\Theta^{(t+1)}) \\geq \\mathcal{F}(p(Z \\mid X, \\Theta^{(t)}), \\Theta^{(t)}) = \\mathcal{L}(\\Theta^{(t)})$$\nThus, $\\mathcal{L}(\\Theta^{(t+1)}) \\geq \\mathcal{L}(\\Theta^{(t)})$.\n\nHowever, the log-likelihood surface for mixture models is generally non-convex. EM is a deterministic hill-climbing procedure on this surface. While it is guaranteed to find a stationary point (a local maximum or saddle point), it is not guaranteed to find the global maximum. The final parameters it converges to are sensitive to the initial parameter settings $\\Theta^{(0)}$.\n\n### Part 4: Numerical Calculation\n\nGiven: $V=3$, $K=2$, $D=2$.\n- Document $d=1$: $\\mathbf{n}_{1} = (2, 1, 0)$, $N_1 = 3$.\n- Document $d=2$: $\\mathbf{n}_{2} = (0, 1, 2)$, $N_2 = 3$.\nInitial parameters $\\Theta^{(0)}$:\n- $\\boldsymbol{\\pi}^{(0)} = (0.5, 0.5)$.\n- $\\boldsymbol{\\phi}_{1}^{(0)} = (0.6, 0.3, 0.1)$.\n- $\\boldsymbol{\\phi}_{2}^{(0)} = (0.1, 0.3, 0.6)$.\n\n**E-Step: Compute responsibilities $\\gamma_{d,k}$**\n\nFirst, we compute the unnormalized probabilities $\\alpha_{d,k} = \\pi_k^{(0)} \\prod_{v=1}^{V} (\\phi_{k,v}^{(0)})^{n_{d,v}}$.\nFor document $d=1$:\n- $\\alpha_{1,1} = 0.5 \\times (0.6^2 \\times 0.3^1 \\times 0.1^0) = 0.5 \\times (0.36 \\times 0.3) = 0.5 \\times 0.108 = 0.054$.\n- $\\alpha_{1,2} = 0.5 \\times (0.1^2 \\times 0.3^1 \\times 0.6^0) = 0.5 \\times (0.01 \\times 0.3) = 0.5 \\times 0.003 = 0.0015$.\nThe normalization constant is $\\sum_j \\alpha_{1,j} = 0.054 + 0.0015 = 0.0555$.\n- $\\gamma_{1,1} = \\frac{0.054}{0.0555} = \\frac{540}{555} = \\frac{36}{37}$.\n- $\\gamma_{1,2} = \\frac{0.0015}{0.0555} = \\frac{15}{555} = \\frac{1}{37}$.\n\nFor document $d=2$:\n- $\\alpha_{2,1} = 0.5 \\times (0.6^0 \\times 0.3^1 \\times 0.1^2) = 0.5 \\times (0.3 \\times 0.01) = 0.5 \\times 0.003 = 0.0015$.\n- $\\alpha_{2,2} = 0.5 \\times (0.1^0 \\times 0.3^1 \\times 0.6^2) = 0.5 \\times (0.3 \\times 0.36) = 0.5 \\times 0.108 = 0.054$.\nThe normalization constant is $\\sum_j \\alpha_{2,j} = 0.0015 + 0.054 = 0.0555$.\n- $\\gamma_{2,1} = \\frac{0.0015}{0.0555} = \\frac{15}{555} = \\frac{1}{37}$.\n- $\\gamma_{2,2} = \\frac{0.054}{0.0555} = \\frac{540}{555} = \\frac{36}{37}$.\n\n**M-Step: Update parameters**\n\nWe need to compute the updated value of $\\phi_{1,1}$, denoted $\\phi_{1,1}^{(1)}$.\nUsing the derived formula:\n$$\\phi_{1,1}^{(1)} = \\frac{\\sum_{d=1}^{2} \\gamma_{d,1} n_{d,1}}{\\sum_{d=1}^{2} \\gamma_{d,1} N_d}$$\nNumerator calculation:\n$$\\sum_{d=1}^{2} \\gamma_{d,1} n_{d,1} = \\gamma_{1,1} n_{1,1} + \\gamma_{2,1} n_{2,1} = \\left(\\frac{36}{37}\\right) \\times 2 + \\left(\\frac{1}{37}\\right) \\times 0 = \\frac{72}{37}$$\nDenominator calculation:\n$$\\sum_{d=1}^{2} \\gamma_{d,1} N_d = \\gamma_{1,1} N_1 + \\gamma_{2,1} N_2 = \\left(\\frac{36}{37}\\right) \\times 3 + \\left(\\frac{1}{37}\\right) \\times 3 = \\frac{108}{37} + \\frac{3}{37} = \\frac{111}{37} = 3$$\nThus, the updated parameter is:\n$$\\phi_{1,1}^{(1)} = \\frac{72/37}{3} = \\frac{72}{37 \\times 3} = \\frac{24}{37}$$\nAs a decimal rounded to four significant figures:\n$$\\phi_{1,1}^{(1)} \\approx 0.6486486... \\approx 0.6486$$", "answer": "$$\n\\boxed{0.6486}\n$$", "id": "3179899"}, {"introduction": "Different text representation methods have different statistical properties, especially when applied to noisy, short texts like social media posts. This final practice moves into a comparative analysis of the uncertainty inherent in TF-IDF versus Latent Dirichlet Allocation (LDA) representations. By deriving and calculating the variance for both models under various conditions, you will explore how the Bayesian framework of LDA helps manage statistical noise, a crucial concept for building robust natural language processing systems. [@problem_id:3179876]", "problem": "You are tasked with modeling short social media posts of length $L$ using bag-of-words and comparing two variance measures: one arising from term frequency–inverse document frequency (TF-IDF) and one arising from latent Dirichlet allocation (LDA). Both comparisons must be grounded in first principles.\n\nFundamental base for derivation:\n- Bag-of-words represents a document as counts over a fixed vocabulary, where each word occurrence is an independent draw from a categorical distribution with probabilities $\\{p_w\\}_{w=1}^V$ and total length $L$. The resulting count vector follows a multinomial distribution over the vocabulary.\n- Term frequency–inverse document frequency (TF-IDF) uses term frequency (normalized counts) together with a corpus-level inverse document frequency based on document frequency. Use the natural logarithm for the inverse document frequency.\n- Latent Dirichlet allocation (LDA) models document-level topic proportions with a Dirichlet prior over topics. Given observed topic assignment counts for a document, the posterior distribution of topic proportions is again Dirichlet, and the posterior variance can be derived from Dirichlet properties.\n\nYour program must, for each test case, compute two quantities:\n1. The mean coordinate-wise variance of the TF-IDF vector across the vocabulary induced purely by word-sampling randomness from the multinomial model for the given $L$ and $\\{p_w\\}$. Use the standard TF-IDF definition where inverse document frequency depends on the corpus size $N$ and document frequency $df(w)$, and term frequency is the normalized count for a word in the document.\n2. The mean component-wise posterior variance of the LDA topic proportion vector across topics for the same document, given a Dirichlet prior parameter vector $\\boldsymbol{\\alpha}$ and observed per-document topic assignment counts $\\{n_k\\}_{k=1}^K$. Treat the topic assignment counts as observed.\n\nYour implementation must derive the analytic expressions needed for variance from the stated base definitions and properties of the multinomial and Dirichlet distributions, without introducing any nonstandard assumptions.\n\nTest Suite:\nUse the following four test cases. For all cases, the inverse document frequency must use the natural logarithm. Each case specifies vocabulary size $V$, corpus size $N$, document frequency array $df(w)$ for $w=1,\\dots,V$, vocabulary probabilities $\\{p_w\\}$, document length $L$, number of topics $K$, Dirichlet prior $\\boldsymbol{\\alpha}$, and observed topic assignment counts $\\{n_k\\}$.\n\n- Case $1$ (short document, moderately informative prior):\n  - $V = 5$\n  - $N = 1000$\n  - $df = [800, 600, 400, 100, 10]$\n  - $p = [0.4, 0.3, 0.2, 0.08, 0.02]$\n  - $L = 5$\n  - $K = 3$\n  - $\\boldsymbol{\\alpha} = [0.5, 0.5, 0.5]$\n  - $\\boldsymbol{n} = [3, 2, 0]$\n\n- Case $2$ (extreme short document, weak prior):\n  - $V = 5$\n  - $N = 1000$\n  - $df = [800, 600, 400, 100, 10]$\n  - $p = [0.4, 0.3, 0.2, 0.08, 0.02]$\n  - $L = 1$\n  - $K = 3$\n  - $\\boldsymbol{\\alpha} = [0.1, 0.1, 0.1]$\n  - $\\boldsymbol{n} = [1, 0, 0]$\n\n- Case $3$ (longer document, strong prior):\n  - $V = 5$\n  - $N = 1000$\n  - $df = [800, 600, 400, 100, 10]$\n  - $p = [0.4, 0.3, 0.2, 0.08, 0.02]$\n  - $L = 50$\n  - $K = 3$\n  - $\\boldsymbol{\\alpha} = [5.0, 5.0, 5.0]$\n  - $\\boldsymbol{n} = [20, 15, 15]$\n\n- Case $4$ (uniform vocabulary probabilities, extreme inverse document frequency variation, moderate prior):\n  - $V = 5$\n  - $N = 1000$\n  - $df = [900, 900, 900, 50, 5]$\n  - $p = [0.2, 0.2, 0.2, 0.2, 0.2]$\n  - $L = 5$\n  - $K = 3$\n  - $\\boldsymbol{\\alpha} = [1.0, 1.0, 1.0]$\n  - $\\boldsymbol{n} = [1, 4, 0]$\n\nOutput specification:\nFor each test case, return a list of two floating-point numbers $[m_{\\text{TFIDF}}, m_{\\text{LDA}}]$, where $m_{\\text{TFIDF}}$ is the mean TF-IDF coordinate-wise variance across vocabulary words and $m_{\\text{LDA}}$ is the mean posterior variance across topics. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each test case result itself formatted as a list. For example, the final line should look like $[[x_1,y_1],[x_2,y_2],[x_3,y_3],[x_4,y_4]]$, where each $x_i$ and $y_i$ are floats.", "solution": "The problem is valid as it is scientifically grounded in standard statistical learning models (Bag-of-Words, TF-IDF, LDA), well-posed with all necessary parameters provided, and objectively formulated. The derivations for the required quantities rely on fundamental properties of the Multinomial and Dirichlet distributions.\n\nThe objective is to compute two quantities for each test case: $m_{\\text{TFIDF}}$, the mean coordinate-wise variance of the Term Frequency–Inverse Document Frequency (TF-IDF) vector, and $m_{\\text{LDA}}$, the mean component-wise posterior variance of the Latent Dirichlet Allocation (LDA) topic proportion vector.\n\nFirst, we derive the analytical expression for $m_{\\text{TFIDF}}$. A document is modeled as a sequence of $L$ words drawn independently from a vocabulary of size $V$ with probabilities $\\{p_w\\}_{w=1}^V$. The count of word $w$, denoted by the random variable $C_w$, follows a Binomial distribution, $C_w \\sim \\text{Binomial}(L, p_w)$. The vector of all word counts $(C_1, \\dots, C_V)$ follows a Multinomial distribution, but for the variance of a single word's TF-IDF score, its marginal distribution is sufficient.\n\nThe TF-IDF score for word $w$, $T_w$, is the product of its Term Frequency (TF), $\\text{TF}_w$, and its Inverse Document Frequency (IDF), $\\text{IDF}_w$. According to the problem statement:\n$$ \\text{TF}_w = \\frac{C_w}{L} $$\n$$ \\text{IDF}_w = \\ln\\left(\\frac{N}{df(w)}\\right) $$\nwhere $N$ is the total number of documents in the corpus and $df(w)$ is the number of documents containing word $w$. The term $\\text{IDF}_w$ is a constant for a given word $w$. The randomness in $T_w$ comes entirely from the word count $C_w$.\nThe TF-IDF score is thus the random variable:\n$$ T_w = \\frac{C_w}{L} \\ln\\left(\\frac{N}{df(w)}\\right) = \\frac{C_w}{L} \\text{IDF}_w $$\nThe variance of $T_w$ is found using the property $\\text{Var}(aX) = a^2 \\text{Var}(X)$ for a constant $a$ and random variable $X$:\n$$ \\text{Var}(T_w) = \\text{Var}\\left(\\frac{C_w}{L} \\text{IDF}_w\\right) = \\left(\\frac{\\text{IDF}_w}{L}\\right)^2 \\text{Var}(C_w) $$\nThe variance of a Binomially distributed random variable $C_w \\sim \\text{Binomial}(L, p_w)$ is $\\text{Var}(C_w) = L p_w (1-p_w)$. Substituting this into the equation for $\\text{Var}(T_w)$:\n$$ \\text{Var}(T_w) = \\frac{(\\text{IDF}_w)^2}{L^2} [L p_w (1-p_w)] = \\frac{p_w(1-p_w)}{L} \\left[\\ln\\left(\\frac{N}{df(w)}\\right)\\right]^2 $$\nThe required quantity $m_{\\text{TFIDF}}$ is the mean of these variances across all $V$ words in the vocabulary:\n$$ m_{\\text{TFIDF}} = \\frac{1}{V} \\sum_{w=1}^{V} \\text{Var}(T_w) = \\frac{1}{V} \\sum_{w=1}^{V} \\frac{p_w(1-p_w)}{L} \\left[\\ln\\left(\\frac{N}{df(w)}\\right)\\right]^2 $$\n\nSecond, we derive the analytical expression for $m_{\\text{LDA}}$. In LDA, the topic proportions for a document, $\\boldsymbol{\\theta} = (\\theta_1, \\dots, \\theta_K)$, are given a Dirichlet prior distribution, $\\boldsymbol{\\theta} \\sim \\text{Dirichlet}(\\boldsymbol{\\alpha})$, where $\\boldsymbol{\\alpha} = (\\alpha_1, \\dots, \\alpha_K)$ is the vector of prior parameters.\nWe are given the observed counts of topic assignments for the words in the document, $\\boldsymbol{n} = (n_1, \\dots, n_K)$, where $\\sum_{k=1}^K n_k = L$. Due to the conjugacy between the Dirichlet and Multinomial distributions, the posterior distribution of the topic proportions given the counts is also a Dirichlet distribution:\n$$ p(\\boldsymbol{\\theta} | \\boldsymbol{n}, \\boldsymbol{\\alpha}) \\sim \\text{Dirichlet}(\\boldsymbol{\\alpha} + \\boldsymbol{n}) $$\nLet the posterior parameter vector be $\\boldsymbol{\\alpha}' = \\boldsymbol{\\alpha} + \\boldsymbol{n}$, so $\\alpha'_k = \\alpha_k + n_k$ for topic $k=1, \\dots, K$.\nFor a general random vector $\\boldsymbol{X} \\sim \\text{Dirichlet}(\\boldsymbol{\\beta})$, the variance of its $i$-th component is given by:\n$$ \\text{Var}(X_i) = \\frac{\\beta_i (\\beta_0 - \\beta_i)}{\\beta_0^2 (\\beta_0 + 1)} $$\nwhere $\\beta_0 = \\sum_{j=1}^K \\beta_j$.\nTo find the posterior variance of $\\theta_k$, we apply this formula with the posterior parameters $\\boldsymbol{\\beta} = \\boldsymbol{\\alpha}'$. Let $\\alpha_0 = \\sum_{k=1}^K \\alpha_k$. The sum of the posterior parameters is $\\alpha'_0 = \\sum_{k=1}^K (\\alpha_k + n_k) = \\alpha_0 + L$.\nThe posterior variance of the $k$-th topic proportion $\\theta_k$ is:\n$$ \\text{Var}(\\theta_k | \\boldsymbol{n}, \\boldsymbol{\\alpha}) = \\frac{\\alpha'_k (\\alpha'_0 - \\alpha'_k)}{(\\alpha'_0)^2 (\\alpha'_0 + 1)} = \\frac{(\\alpha_k + n_k) ((\\alpha_0 + L) - (\\alpha_k + n_k))}{(\\alpha_0 + L)^2 (\\alpha_0 + L + 1)} $$\nThe required quantity $m_{\\text{LDA}}$ is the mean of these posterior variances across all $K$ topics:\n$$ m_{\\text{LDA}} = \\frac{1}{K} \\sum_{k=1}^{K} \\text{Var}(\\theta_k | \\boldsymbol{n}, \\boldsymbol{\\alpha}) = \\frac{1}{K} \\sum_{k=1}^{K} \\frac{(\\alpha_k + n_k) (\\alpha_0 + L - \\alpha_k - n_k)}{(\\alpha_0 + L)^2 (\\alpha_0 + L + 1)} $$\nThese derived expressions for $m_{\\text{TFIDF}}$ and $m_{\\text{LDA}}$ will be implemented to solve for the given test cases.", "answer": "[[0.40798157771746654,0.02674681190587216],[2.0399078885873327,0.07119047619047619],[0.040798157771746654,0.000624021164021164],[0.7027415865239924,0.024691358024691357]]", "id": "3179876"}]}