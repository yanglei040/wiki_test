{"hands_on_practices": [{"introduction": "Before building complex architectures, it is essential to master the geometric properties of a single convolutional layer. A crucial and practical skill is the ability to control the spatial dimensions of the output feature map. This exercise challenges you to derive the precise mathematical relationship between kernel size $k$, dilation $d$, and the padding $P$ required to ensure the output dimensions match the input, a common configuration known as \"same\" convolution. By working through this derivation from first principles, you will gain a robust understanding of how to craft layers that preserve spatial information across a network [@problem_id:3126176].", "problem": "Consider a single two-dimensional convolutional layer in a Convolutional Neural Network (CNN) that operates on an input feature map of height $H$ and width $W$. The layer uses a square kernel of size $k \\times k$, dilation $d$ in both spatial dimensions, stride $s$ in both spatial dimensions, and zero-padding applied on the spatial boundaries. Assume standard cross-correlation as implemented in common libraries (that is, no kernel flipping), and integer-valued padding amounts on each side of each spatial dimension.\n\nStarting from the definition of discrete convolution on a grid and the definition of dilation and stride, derive the condition under which the output height equals the input height, independently of $H$, and the analogous condition for width. Show that this condition forces a specific relation among the total padding per spatial dimension, the effective receptive field, and the stride. Then, assuming the only admissible choice that preserves spatial size for all $H$ is $s=1$, derive a closed-form expression for the minimal total padding per spatial dimension, $P_{\\text{total}}(d,k)$, that guarantees the output height and width both equal $H$ and $W$, respectively. In your derivation, explain why, when $k$ is odd, symmetric per-side padding $p = \\frac{d\\,(k-1)}{2}$ achieves exact “same” convolution, and analyze the off-by-one asymmetry that arises when $k$ is even, distinguishing cases by the parity of $d$.\n\nReport only the single closed-form expression for $P_{\\text{total}}(d,k)$ as your final answer. Do not include any units. No numerical rounding is required.", "solution": "The problem asks for the derivation of the minimal total padding, $P_{\\text{total}}(d, k)$, required to ensure the output spatial dimensions of a 2D convolutional layer are identical to the input spatial dimensions, a configuration often referred to as \"same\" convolution. We begin by formalizing the relationship between input and output dimensions.\n\nLet the input feature map have a spatial dimension of size $N_{in}$ (representing either height $H$ or width $W$). The convolutional layer has a kernel of size $k$, a dilation factor $d$, a stride $s$, and a total amount of zero-padding $P_{\\text{total}}$ applied along that dimension. The padding is composed of integer-valued amounts on each side, $p_{left}$ and $p_{right}$, such that $P_{\\text{total}} = p_{left} + p_{right}$.\n\nThe effective size of the kernel, or its receptive field span, is not simply $k$ but is expanded by the dilation. The dilated kernel covers a span of $k_{eff}$ input elements, given by:\n$$k_{eff} = d \\cdot (k-1) + 1$$\nThis formula arises because there are $k-1$ gaps between the $k$ kernel weights, and each gap is of size $d-1$. The total span is the number of weights plus the total size of the gaps: $k + (k-1)(d-1) = k + kd - k - d + 1 = kd - d + 1 = d(k-1) + 1$.\n\nAfter applying total padding $P_{\\text{total}}$, the effective size of the input dimension becomes $N_{in} + P_{\\text{total}}$. The number of possible positions for the top-left element of the dilated kernel on this padded input is $(N_{in} + P_{\\text{total}}) - k_{eff} + 1$. With a stride of $s$, the number of output elements, $N_{out}$, is given by the standard formula:\n$$N_{out} = \\left\\lfloor \\frac{(N_{in} + P_{\\text{total}}) - k_{eff}}{s} \\right\\rfloor + 1$$\nSubstituting the expression for $k_{eff}$, we get:\n$$N_{out} = \\left\\lfloor \\frac{N_{in} + P_{\\text{total}} - (d(k-1) + 1)}{s} \\right\\rfloor + 1$$\n\nThe first task is to find the condition under which $N_{out} = N_{in}$ holds for any input size $N_{in} \\ge 1$.\nSetting $N_{out} = N_{in}$:\n$$N_{in} = \\left\\lfloor \\frac{N_{in} + P_{\\text{total}} - d(k-1) - 1}{s} \\right\\rfloor + 1$$\n$$N_{in} - 1 = \\left\\lfloor \\frac{N_{in} + P_{\\text{total}} - d(k-1) - 1}{s} \\right\\rfloor$$\nBy the definition of the floor function, $\\lfloor x \\rfloor = y$ is equivalent to $y \\le x < y+1$. Applying this, we obtain the double inequality:\n$$N_{in} - 1 \\le \\frac{N_{in} + P_{\\text{total}} - d(k-1) - 1}{s} < N_{in}$$\nMultiplying by $s$ (where $s \\ge 1$ is a positive integer):\n$$s(N_{in} - 1) \\le N_{in} + P_{\\text{total}} - d(k-1) - 1 < s N_{in}$$\nThis inequality must hold for all values of $N_{in} \\ge 1$. Let's rearrange the terms to isolate $N_{in}$:\n$$(s-1)N_{in} - s \\le P_{\\text{total}} - d(k-1) - 1 < (s-1)N_{in}$$\nFor this inequality to be satisfied for all $N_{in}$, the terms dependent on $N_{in}$ must not allow the constant middle term to escape the bounds. If $s > 1$, then $s-1 > 0$, and the lower bound $(s-1)N_{in} - s$ and upper bound $(s-1)N_{in}$ both grow with $N_{in}$. The interval between them, $[(s-1)N_{in} - s, (s-1)N_{in})$, has a fixed width of $s$ but its position shifts. A constant value, $P_{\\text{total}} - d(k-1) - 1$, cannot remain within this shifting interval for all $N_{in}$. The only way to satisfy the condition for all $N_{in}$ is if the coefficient of $N_{in}$ is zero. This forces $s-1=0$, which means $s=1$.\n\nThis derivation confirms the problem's premise that $s=1$ is necessary to preserve spatial size independently of the input dimensions. With $s=1$, the inequality simplifies to:\n$$-1 \\le P_{\\text{total}} - d(k-1) - 1 < 0$$\nSince $P_{\\text{total}}$, $d$, and $k$ are integers, the expression $P_{\\text{total}} - d(k-1) - 1$ must be an integer. The only integer $z$ that satisfies $-1 \\le z < 0$ is $z=-1$. Therefore, we must have:\n$$P_{\\text{total}} - d(k-1) - 1 = -1$$\n$$P_{\\text{total}} = d(k-1)$$\nThis is the unique value for the total padding that guarantees $N_{out} = N_{in}$ when $s=1$. It is therefore the minimal, and only, such value. We can express this as $P_{\\text{total}}(d,k) = d(k-1)$.\n\nThe problem also asks to show this forces a relation between total padding, effective receptive field, and stride. We have $s=1$ and $P_{\\text{total}} = d(k-1)$. The effective receptive field is $k_{eff} = d(k-1)+1$. The relation is thus $P_{\\text{total}} = k_{eff} - 1$.\n\nFinally, we analyze the implementation of this total padding. $P_{\\text{total}}$ must be split into padding on two sides, $p_{left}$ and $p_{right}$, which must be integers.\nIf $k$ is an odd integer, then $k-1$ is an even integer. Consequently, the total required padding $P_{\\text{total}} = d(k-1)$ is always an even number, regardless of whether $d$ is odd or even. An even amount of total padding can be perfectly balanced, allowing for symmetric padding on each side:\n$$p = p_{left} = p_{right} = \\frac{P_{\\text{total}}}{2} = \\frac{d(k-1)}{2}$$\nThis maintains the spatial alignment of the output feature map with the input.\n\nIf $k$ is an even integer, then $k-1$ is an odd integer. The parity of the total padding $P_{\\text{total}} = d(k-1)$ now depends on the parity of the dilation $d$.\n- If $d$ is even, $P_{\\text{total}}$ is even (even $\\times$ odd = even). Symmetric padding $p = \\frac{d(k-1)}{2}$ is once again possible.\n- If $d$ is odd, $P_{\\text{total}}$ is odd (odd $\\times$ odd = odd). Since $P_{\\text{total}}$ is odd, it cannot be divided into two equal integer halves. This forces asymmetric padding, where $p_{left} \\neq p_{right}$. For example, one might choose $p_{left} = \\lfloor \\frac{P_{\\text{total}}}{2} \\rfloor = \\frac{d(k-1)-1}{2}$ and $p_{right} = \\lceil \\frac{P_{\\text{total}}}{2} \\rceil = \\frac{d(k-1)+1}{2}$. This unavoidable asymmetry results in a half-pixel spatial shift of the output feature map relative to the input, which is the \"off-by-one\" asymmetry mentioned.\n\nDespite these implementation details regarding symmetric or asymmetric application, the total amount of padding required per dimension is uniquely determined. The closed-form expression for the minimal total padding that guarantees the output size equals the input size is $P_{\\text{total}}(d,k) = d(k-1)$.", "answer": "$$\n\\boxed{d(k-1)}\n$$", "id": "3126176"}, {"introduction": "The power of Convolutional Neural Networks for processing grid-like data, such as images, is rooted in a fundamental property called translation equivariance. In simple terms, this means that if you shift a feature in the input, the network's representation of that feature also shifts by the same amount. This practice provides a direct, hands-on opportunity to verify this abstract principle by implementing the relation $f(T_{\\delta} x) = T_{\\delta} f(x)$ numerically. By testing various layer compositions, you will discover which operations preserve this property and, crucially, how choices like zero-padding can break it, providing a concrete intuition for this cornerstone of CNNs [@problem_id:3126241].", "problem": "You are tasked with designing and verifying translation-equivariant spatial prediction networks using fundamental properties of discrete convolution, translation operators on finite grids, and pointwise nonlinearities. Translation equivariance means that for a function $f$ acting on two-dimensional arrays and a translation operator $T_{\\delta}$, the relation $f(T_{\\delta} x) = T_{\\delta} f(x)$ holds for all valid inputs $x$ and translations $\\delta$. The grids are finite with periodic boundary conditions unless explicitly stated otherwise.\n\nFoundational base and definitions to be used:\n- A two-dimensional discrete signal is a function $x: \\{0,\\dots,H-1\\} \\times \\{0,\\dots,W-1\\} \\to \\mathbb{R}$, represented as an array of shape $H \\times W$.\n- The circular translation operator $T_{\\delta}$ with $\\delta = (\\delta_y,\\delta_x)$ acts as $(T_{\\delta} x)[i,j] = x[(i-\\delta_y) \\bmod H, (j-\\delta_x) \\bmod W]$.\n- A two-dimensional discrete cross-correlation with kernel $k$ of odd size $k_h \\times k_w$ centered at $(\\lfloor k_h/2 \\rfloor,\\lfloor k_w/2 \\rfloor)$ is given, under circular boundary conditions, by\n  $$ (x \\star_{\\mathrm{circ}} k)[i,j] = \\sum_{m=0}^{k_h-1}\\sum_{n=0}^{k_w-1} x\\big((i+m-c_h) \\bmod H, (j+n-c_w) \\bmod W\\big)\\, k[m,n], $$\n  where $c_h = \\lfloor k_h/2 \\rfloor$ and $c_w = \\lfloor k_w/2 \\rfloor$.\n- A pointwise nonlinearity $\\sigma$ acts independently per spatial site, for example $\\sigma(u) = \\max(u,0)$ (rectified linear unit).\n- A constant bias $b \\in \\mathbb{R}$ added uniformly over space yields $y[i,j] = u[i,j] + b$.\n\nTasks:\n- Design a small family of networks composed from circular cross-correlation layers, constant biases, and pointwise nonlinearities, each producing an $H \\times W$ spatial map. For one specified counterexample, use zero-padding (non-circular) cross-correlation to illustrate a failure case.\n- For each network, verify analytically (by reasoning from the definitions) for which boundary condition the relation $f(T_{\\delta} x) = T_{\\delta} f(x)$ holds for all $x$ and $\\delta$, and for which it fails.\n- Implement a program that numerically tests the equivariance relation $f(T_{\\delta} x) = T_{\\delta} f(x)$ using the maximum absolute difference criterion with tolerance $\\epsilon$, and reports, for each test case, a boolean indicating whether the property holds within tolerance.\n\nNumerical details and constraints:\n- Use arrays of shape $H \\times W$ with $H = 16$ and $W = 16$.\n- All kernels must have odd spatial dimensions.\n- For circular layers, use circular cross-correlation as defined above. For the one non-circular case, use zero-padding cross-correlation that treats values outside the domain as zero and returns an array of the same shape.\n- Use the rectified linear unit $\\sigma(u) = \\max(u,0)$ wherever a nonlinearity is requested.\n- Use tolerance $\\epsilon = 10^{-9}$ to decide equality via the condition $\\max_{i,j} |A[i,j] - B[i,j]| \\le \\epsilon$.\n- Randomness must be reproducible. For each test case, initialize a pseudorandom number generator with the provided seed $s$ and draw all entries of inputs and kernels independently from a standard normal distribution.\n\nTest suite:\nEach test case specifies $(H,W,\\delta,\\text{architecture},\\text{kernel sizes},\\text{seed})$. Implement the following six cases and evaluate the property $f(T_{\\delta} x) = T_{\\delta} f(x)$.\n\n- Case A (happy path, single layer):\n  - $H = 16$, $W = 16$, $\\delta = (3,-5)$, architecture: one circular cross-correlation layer with kernel size $3 \\times 3$, add a constant bias, followed by a rectified linear unit, seed $s = 0$.\n- Case B (composition of layers):\n  - $H = 16$, $W = 16$, $\\delta = (7,2)$, architecture: two circular cross-correlation layers, both with kernel size $3 \\times 3$, each with its own constant bias, with a rectified linear unit applied after the first layer and after the second layer, seed $s = 1$.\n- Case C (failure under zero padding):\n  - $H = 16$, $W = 16$, $\\delta = (5,4)$, architecture: one zero-padding cross-correlation layer (non-circular) with kernel size $3 \\times 3$, add a constant bias, followed by a rectified linear unit, seed $s = 2$.\n- Case D (bias invariance, large translation):\n  - $H = 16$, $W = 16$, $\\delta = (15,-16)$, architecture: one circular cross-correlation layer with kernel size $3 \\times 3$, add a nonzero constant bias, followed by a rectified linear unit, seed $s = 3$.\n- Case E (pointwise linear head):\n  - $H = 16$, $W = 16$, $\\delta = (1,1)$, architecture: one circular cross-correlation layer with kernel size $3 \\times 3$, followed by a rectified linear unit, followed by a pointwise linear layer implemented as a $1 \\times 1$ circular cross-correlation with its own bias, seed $s = 4$.\n- Case F (dilated circular layer):\n  - $H = 16$, $W = 16$, $\\delta = (2,-3)$, architecture: one circular cross-correlation layer whose kernel is constructed by inserting zeros into a base $3 \\times 3$ kernel with dilation factor $d = 2$, yielding an effective kernel size $5 \\times 5$, add a constant bias, followed by a rectified linear unit, seed $s = 5$.\n\nProgram input and output:\n- There is no external input. Your program must internally construct the six test cases as specified, generate the random inputs and kernels using the provided seeds, evaluate the equivariance relation for each case with tolerance $\\epsilon = 10^{-9}$, and aggregate the six boolean results into a single line of output formatted as a comma-separated list enclosed in square brackets, for example $[\\text{True},\\text{False},\\dots]$ with exactly six entries and no extra whitespace.", "solution": "The problem requires an analysis of translation equivariance for several small neural network architectures built from standard components. A function $f$ is translation-equivariant if, for any valid input signal $x$ and any translation vector $\\delta$, the relation $f(T_{\\delta} x) = T_{\\delta} f(x)$ holds. Here, $T_{\\delta}$ is the circular translation operator defined on a finite two-dimensional grid of size $H \\times W$. The grid is defined by indices $\\{0, \\dots, H-1\\} \\times \\{0, \\dots, W-1\\}$.\n\nThe translation operator $T_{\\delta}$ with $\\delta = (\\delta_y, \\delta_x)$ is defined as $(T_{\\delta} x)[i,j] = x[(i-\\delta_y) \\bmod H, (j-\\delta_x) \\bmod W]$. This operation shifts the content of the signal $x$ by the vector $\\delta$, with periodic boundary conditions.\n\nA network $f$ is constructed as a composition of primitive operations, $f = f_L \\circ f_{L-1} \\circ \\dots \\circ f_1$. The property of translation equivariance is preserved under composition. If two functions $g$ and $h$ are individually translation-equivariant, their composition $g \\circ h$ is also translation-equivariant. This can be shown as follows:\n$$ (g \\circ h)(T_{\\delta} x) = g(h(T_{\\delta} x)) $$\nBy the equivariance of $h$, $h(T_{\\delta} x) = T_{\\delta} h(x)$. Substituting this gives:\n$$ g(T_{\\delta} h(x)) $$\nBy the equivariance of $g$, applied to the input $y = h(x)$:\n$$ g(T_{\\delta} y) = T_{\\delta} g(y) \\implies g(T_{\\delta} h(x)) = T_{\\delta} g(h(x)) = T_{\\delta} (g \\circ h)(x) $$\nThus, we have $(g \\circ h)(T_{\\delta} x) = T_{\\delta} (g \\circ h)(x)$. To determine if a network $f$ is translation-equivariant, we must analyze each of its constituent layers.\n\nThe building blocks are:\n1.  Circular cross-correlation\n2.  Addition of a constant bias\n3.  Pointwise nonlinearity\n4.  Zero-padding cross-correlation (for one counterexample)\n\nLet's analyze each component for translation equivariance.\n\n1.  **Circular Cross-Correlation ($C_k$)**:\nLet the operation be $y = x \\star_{\\mathrm{circ}} k$. The output is given by:\n$$ y[i,j] = (x \\star_{\\mathrm{circ}} k)[i,j] = \\sum_{m=0}^{k_h-1}\\sum_{n=0}^{k_w-1} x\\big((i+m-c_h) \\bmod H, (j+n-c_w) \\bmod W\\big)\\, k[m,n] $$\nwhere $c_h = \\lfloor k_h/2 \\rfloor$ and $c_w = \\lfloor k_w/2 \\rfloor$.\nTo test for equivariance, we compare $T_{\\delta}(x \\star_{\\mathrm{circ}} k)$ with $(T_{\\delta} x) \\star_{\\mathrm{circ}} k$.\n\nFirst, let's compute the translated output, $(T_{\\delta} y)[i,j]$:\n$$ (T_{\\delta} y)[i,j] = y[(i-\\delta_y) \\bmod H, (j-\\delta_x) \\bmod W] $$\nSubstituting the definition of $y$:\n$$ (T_{\\delta} y)[i,j] = \\sum_{m, n} x\\Big(\\big(((i-\\delta_y) \\bmod H) + m-c_h\\big) \\bmod H, \\big(((j-\\delta_x) \\bmod W) + n-c_w\\big) \\bmod W\\Big) \\, k[m,n] $$\nDue to the properties of modular arithmetic, this simplifies to:\n$$ (T_{\\delta} y)[i,j] = \\sum_{m, n} x\\big((i-\\delta_y+m-c_h) \\bmod H, (j-\\delta_x+n-c_w) \\bmod W\\big) \\, k[m,n] $$\n\nNext, let's compute the output for a translated input, $y' = (T_{\\delta} x) \\star_{\\mathrm{circ}} k$:\n$$ y'[i,j] = \\sum_{m,n} (T_{\\delta} x)\\big((i+m-c_h) \\bmod H, (j+n-c_w) \\bmod W\\big) \\, k[m,n] $$\nUsing the definition of $T_{\\delta}$, $(T_{\\delta} x)[a,b] = x[(a-\\delta_y)\\bmod H, (b-\\delta_x)\\bmod W]$, we get:\n$$ y'[i,j] = \\sum_{m,n} x\\Big(\\big((i+m-c_h)\\bmod H - \\delta_y\\big) \\bmod H, \\big((j+n-c_w)\\bmod W - \\delta_x\\big) \\bmod W\\Big) \\, k[m,n] $$\nThis again simplifies to:\n$$ y'[i,j] = \\sum_{m,n} x\\big((i+m-c_h-\\delta_y) \\bmod H, (j+n-c_w-\\delta_x) \\bmod W\\big) \\, k[m,n] $$\nThe expressions for $(T_{\\delta} y)[i,j]$ and $y'[i,j]$ are identical. Therefore, circular cross-correlation is a translation-equivariant operation. This holds regardless of the kernel values or size, including $1 \\times 1$ kernels and dilated kernels, as long as the boundary condition is circular.\n\n2.  **Constant Bias Addition ($B_b$)**:\nLet the operation be $y[i,j] = u[i,j] + b$.\nThe translated output is $(T_{\\delta} y)[i,j] = y[(i-\\delta_y)\\bmod H, (j-\\delta_x)\\bmod W] = u[(i-\\delta_y)\\bmod H, (j-\\delta_x)\\bmod W] + b$.\nThe output for a translated input is $(T_{\\delta} u + b)[i,j] = (T_{\\delta} u)[i,j] + b = u[(i-\\delta_y)\\bmod H, (j-\\delta_x)\\bmod W] + b$.\nThe two expressions are identical. Thus, adding a constant bias is a translation-equivariant operation.\n\n3.  **Pointwise Nonlinearity ($\\Sigma$)**:\nLet the operation be $y[i,j] = \\sigma(u[i,j])$, where $\\sigma$ is a function applied to each element independently.\nThe translated output is $(T_{\\delta} y)[i,j] = y[(i-\\delta_y)\\bmod H, (j-\\delta_x)\\bmod W] = \\sigma\\big(u[(i-\\delta_y)\\bmod H, (j-\\delta_x)\\bmod W]\\big)$.\nThe output for a translated input is $\\sigma(T_{\\delta} u)[i,j] = \\sigma\\big((T_{\\delta} u)[i,j]\\big) = \\sigma\\big(u[(i-\\delta_y)\\bmod H, (j-\\delta_x)\\bmod W]\\big)$.\nThe expressions are identical. Any pointwise operation, including the rectified linear unit (ReLU), is translation-equivariant.\n\n4.  **Zero-Padding Cross-Correlation ($C'_k$)**:\nThis operation is defined similarly to circular cross-correlation, but inputs from outside the grid $\\{0, \\dots, H-1\\} \\times \\{0, \\dots, W-1\\}$ are treated as zero. This breaks the periodic symmetry of the grid. Consider an input signal $x$ that is zero everywhere except for a single point near a boundary, for instance $x[0,0]=1$. The output $y = C'_k(x)$ will have a pattern determined by the kernel $k$. Now consider a translated input $x' = T_\\delta x$. If $\\delta$ is such that the non-zero element is moved away from the boundary into the interior of the grid, the cross-correlation will be computed using neighboring signal values (which are zero in this example), not padded zeros. The resulting output pattern $y' = C'_k(x')$ will be identical in shape to $y$. However, if we compute the translation of the original output, $T_\\delta y$, the result will be a simple shift of the original output pattern, including any artifacts caused by the boundary. These two results, $y'$ and $T_\\delta y$, will not match in general because the padding interacts differently with the signal depending on its position relative to the boundary. Thus, cross-correlation with zero-padding is not translation-equivariant.\n\n**Analysis of Test Cases:**\n\n-   **Case A**: `circ_corr` $\\to$ `bias` $\\to$ `relu`. This is a composition of three translation-equivariant operations under circular boundary conditions. The resulting network is therefore **equivariant**.\n-   **Case B**: `(circ_corr -> bias -> relu)` $\\to$ `(circ_corr -> bias -> relu)`. This is a composition of two blocks, each of which is equivariant as established in Case A. The composition of equivariant functions is equivariant. The network is **equivariant**.\n-   **Case C**: `zero_pad_corr` $\\to$ `bias` $\\to$ `relu`. This network starts with a zero-padding cross-correlation layer, which is not translation-equivariant. The presence of a single non-equivariant layer makes the entire network **not equivariant**.\n-   **Case D**: `circ_corr` $\\to$ `bias` $\\to$ `relu`. This is the same architecture as Case A. The specific values of the translation vector $\\delta = (15,-16) \\equiv (-1,0) \\pmod{16}$ and the non-zero bias do not alter the fundamental property of equivariance. The network is **equivariant**.\n-   **Case E**: `circ_corr(3x3)` $\\to$ `relu` $\\to$ `circ_corr(1x1)` $\\to$ `bias`. A $1 \\times 1$ cross-correlation is a pointwise scaling, which is a translation-equivariant operation. All other components are also equivariant. The network is a composition of equivariant blocks and is therefore **equivariant**.\n-   **Case F**: `dilated_circ_corr` $\\to$ `bias` $\\to$ `relu`. A dilated circular cross-correlation is a specific instance of circular cross-correlation where the kernel has a sparse structure of inserted zeros. The proof of equivariance for circular cross-correlation is independent of the kernel's specific values, only depending on the circular summation. Thus, this layer is also equivariant. The network is **equivariant**.\n\nThe expected boolean results for the test suite are: [True, True, False, True, True, True].", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.signal import correlate2d\n\ndef solve():\n    \"\"\"\n    Solves the problem by numerically testing the translation equivariance\n    of different network architectures specified in the test cases.\n    \"\"\"\n\n    # --- Helper Functions for Network Layers ---\n\n    def circular_translation(x, delta):\n        \"\"\"Applies circular translation T_delta to a 2D array x.\"\"\"\n        # delta = (delta_y, delta_x)\n        # np.roll shifts array elements. A shift of (dy, dx) moves the\n        # element at (r, c) to (r+dy, c+dx).\n        # We want (T_delta x)[i,j] = x[i-delta_y, j-delta_x].\n        # This corresponds to shifting the content by delta.\n        return np.roll(x, shift=delta, axis=(0, 1))\n\n    def circular_cross_correlation(x, k):\n        \"\"\"Performs circular cross-correlation.\"\"\"\n        return correlate2d(x, k, mode='same', boundary='wrap')\n\n    def zero_padding_cross_correlation(x, k):\n        \"\"\"Performs zero-padding cross-correlation.\"\"\"\n        return correlate2d(x, k, mode='same', boundary='fill', fillvalue=0)\n\n    def relu(x):\n        \"\"\"Applies the Rectified Linear Unit pointwise.\"\"\"\n        return np.maximum(x, 0)\n    \n    def create_dilated_kernel(base_kernel, dilation_factor):\n        \"\"\"Creates a dilated kernel from a base kernel.\"\"\"\n        kh_base, kw_base = base_kernel.shape\n        d = dilation_factor\n        \n        # Effective kernel size calculation\n        kh_dil = kh_base + (kh_base - 1) * (d - 1)\n        kw_dil = kw_base + (kw_base - 1) * (d - 1)\n        \n        dilated_kernel = np.zeros((kh_dil, kw_dil))\n        \n        for i in range(kh_base):\n            for j in range(kw_base):\n                dilated_kernel[i * d, j * d] = base_kernel[i, j]\n                \n        return dilated_kernel\n\n    # --- Test Case Definitions ---\n    \n    test_cases = [\n        # Case A: (H, W, delta, architecture, kernel_sizes, seed)\n        (16, 16, (3, -5), 'A', [(3, 3)], 0),\n        # Case B\n        (16, 16, (7, 2), 'B', [(3, 3), (3, 3)], 1),\n        # Case C\n        (16, 16, (5, 4), 'C', [(3, 3)], 2),\n        # Case D\n        (16, 16, (15, -16), 'D', [(3, 3)], 3),\n        # Case E\n        (16, 16, (1, 1), 'E', [(3, 3), (1, 1)], 4),\n        # Case F\n        (16, 16, (2, -3), 'F', [(3, 3)], 5),\n    ]\n\n    results = []\n    epsilon = 1e-9\n\n    for case in test_cases:\n        H, W, delta, arch, kernel_sizes, seed = case\n        \n        # Initialize RNG for reproducibility for each case\n        np.random.seed(seed)\n        \n        # Generate random input\n        x = np.random.standard_normal((H, W))\n\n        # Define the network function f(x) for the current architecture\n        if arch == 'A':\n            k = np.random.standard_normal(kernel_sizes[0])\n            b = np.random.standard_normal()\n            f = lambda z: relu(circular_cross_correlation(z, k) + b)\n        \n        elif arch == 'B':\n            k1 = np.random.standard_normal(kernel_sizes[0])\n            b1 = np.random.standard_normal()\n            k2 = np.random.standard_normal(kernel_sizes[1])\n            b2 = np.random.standard_normal()\n            f = lambda z: relu(circular_cross_correlation(relu(circular_cross_correlation(z, k1) + b1), k2) + b2)\n            \n        elif arch == 'C':\n            k = np.random.standard_normal(kernel_sizes[0])\n            b = np.random.standard_normal()\n            f = lambda z: relu(zero_padding_cross_correlation(z, k) + b)\n            \n        elif arch == 'D':\n            k = np.random.standard_normal(kernel_sizes[0])\n            b = np.random.standard_normal()\n            f = lambda z: relu(circular_cross_correlation(z, k) + b)\n\n        elif arch == 'E':\n            k1 = np.random.standard_normal(kernel_sizes[0])\n            k2 = np.random.standard_normal(kernel_sizes[1])\n            b2 = np.random.standard_normal()\n            f = lambda z: circular_cross_correlation(relu(circular_cross_correlation(z, k1)), k2) + b2\n\n        elif arch == 'F':\n            k_base = np.random.standard_normal(kernel_sizes[0])\n            k_dilated = create_dilated_kernel(k_base, dilation_factor=2)\n            b = np.random.standard_normal()\n            f = lambda z: relu(circular_cross_correlation(z, k_dilated) + b)\n            \n        # --- Equivariance Test ---\n        # LHS: f(T_delta(x))\n        x_translated = circular_translation(x, delta)\n        lhs = f(x_translated)\n\n        # RHS: T_delta(f(x))\n        fx = f(x)\n        rhs = circular_translation(fx, delta)\n        \n        # Compare LHS and RHS\n        max_abs_diff = np.max(np.abs(lhs - rhs))\n        is_equivariant = max_abs_diff = epsilon\n        results.append(is_equivariant)\n\n    # Print results in the required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3126241"}, {"introduction": "While theoretical properties like translation equivariance provide a powerful framework, their practical implementation in finite systems introduces important edge cases. The boundaries of an image are a prime example where ideal behavior can break down, leading to potentially unexpected outcomes. This exercise demonstrates how a seemingly minor implementation detail—the choice of padding—can be exploited to alter a network's final prediction. You will construct a scenario where moving a feature to the image border and changing the padding method is sufficient to flip a classifier's decision, offering a memorable lesson on the practical impact of boundary conditions [@problem_id:3126196].", "problem": "You are given a directive to construct adversarial inputs that exploit boundary conditions in Convolutional Neural Networks (CNNs). Begin from the foundational definitions of two-dimensional discrete convolution, translation, and receptive fields, then reason about how boundary conditions affect translation equivariance and effective receptive fields near image edges.\n\nFundamental base:\n- Define the two-dimensional discrete convolution of a finite image signal $x \\in \\mathbb{R}^{H \\times W}$ with a finite kernel $k \\in \\mathbb{R}^{m \\times n}$ on an infinite lattice as\n$$\n(y \\ast k)[i,j] \\triangleq \\sum_{u=0}^{m-1}\\sum_{v=0}^{n-1} k[u,v]\\; y[i+u-\\lfloor m/2 \\rfloor, j+v-\\lfloor n/2 \\rfloor],\n$$\nwhere $y$ is an extension of $x$ by a boundary condition. The extension $y$ is defined by a padding scheme, for example, zero padding or reflection padding. The unpadded case computes only where indices are valid. The receptive field of an output position is the subset of the input that influences that output via the convolution sum. The translation operator $T_{\\Delta i, \\Delta j}$ acts on $x$ by $(T_{\\Delta i, \\Delta j}x)[i,j] = x[i-\\Delta i,j-\\Delta j]$. In the ideal infinite-lattice case without boundary truncation, convolution is translation equivariant, meaning $(T_{\\Delta i, \\Delta j}(x) \\ast k) = T_{\\Delta i, \\Delta j}(x \\ast k)$.\n\nTask:\n- Implement a single-layer Convolutional Neural Network (CNN) with one $3 \\times 3$ convolutional kernel $K$ whose entries are all $1$, identity nonlinearity, and global average pooling that produces a scalar score $s$. Explicitly, for an output feature map $z$, define the pooled score\n$$\ns \\triangleq \\frac{1}{|z|}\\sum_{i,j} z[i,j],\n$$\nwhere $|z|$ is the number of elements in the feature map. Use a binary classifier that outputs the label $1$ if $s \\ge \\tau$ and $0$ otherwise, with the threshold fixed to $\\tau = 1.13$.\n\n- Construct input images $x \\in \\mathbb{R}^{8 \\times 8}$ with background intensity $0$ and a single bright square feature of intensity $1$ and size $3 \\times 3$ placed at specified coordinates $(i_0,j_0)$, meaning\n$$\nx[i,j] = \\begin{cases}\n1  \\text{if } i_0 \\le i \\le i_0+2 \\text{ and } j_0 \\le j \\le j_0+2,\\\\\n0  \\text{otherwise.}\n\\end{cases}\n$$\n\n- Evaluate the network under three boundary conditions:\n    1. Same convolution with zero padding (\"same\\_zero\"), where the image is padded by a $1$-pixel border of zeros before convolution and the output size is $8 \\times 8$.\n    2. Same convolution with reflection padding (\"same\\_reflect\"), where the image is padded by a $1$-pixel reflection before convolution and the output size is $8 \\times 8$.\n    3. Valid convolution (\"valid\"), where no padding is applied and only positions where the $3 \\times 3$ kernel fully fits inside the image are convolved, resulting in an output size of $6 \\times 6$.\n\n- For each test case, compute two scores $s_A$ and $s_B$ using two specified boundary conditions $A$ and $B$, classify with the threshold $\\tau$, and return:\n    - A boolean indicating whether the predicted class under $A$ differs from the predicted class under $B$.\n    - The margin difference $s_B - s_A$ as a float.\n\nTest suite:\n- Use the following parameter values, each test case encoded as $(H, k, i_0, j_0, A, B)$:\n    1. $(8, 3, 2, 2, \\text{same\\_zero}, \\text{same\\_reflect})$ — feature near the center (happy path).\n    2. $(8, 3, 2, 5, \\text{same\\_zero}, \\text{same\\_reflect})$ — feature touching the right edge.\n    3. $(8, 3, 0, 0, \\text{valid}, \\text{same\\_reflect})$ — feature at the top-left corner, comparing valid versus reflective padding.\n    4. $(8, 3, 5, 2, \\text{valid}, \\text{same\\_zero})$ — feature touching the bottom edge, comparing valid versus zero padding.\n\nOutput specification:\n- Your program should produce a single line of output containing the results as a comma-separated list of lists, each inner list in the form $[\\text{flip}, \\text{margin}]$. For example, the output must look like\n$$\n[[\\text{bool}_1,\\text{float}_1],[\\text{bool}_2,\\text{float}_2],[\\text{bool}_3,\\text{float}_3],[\\text{bool}_4,\\text{float}_4]].\n$$\nNo physical units or angles are involved. All numeric outputs must be plain booleans and floats. The program must be self-contained and require no input.", "solution": "The problem statement is assessed as valid. It is scientifically grounded in the principles of convolutional neural networks, well-posed with all necessary parameters defined, and objective in its formulation. It presents a clear computational task to explore the effects of different boundary conditions on a simple CNN's output, a standard topic in the study of deep learning.\n\nThe solution will be constructed following a step-by-step implementation of the specified single-layer CNN. The network comprises three stages: convolution with a specified padding scheme, global average pooling, and threshold-based classification.\n\nFirst, let's establish the core computational components.\n\n**1. Input Image Generation**\nFor each test case, an input image $x \\in \\mathbb{R}^{8 \\times 8}$ is generated. This image consists of a constant background of intensity $0$ and a $3 \\times 3$ square feature of intensity $1$. The feature's top-left corner is located at coordinates $(i_0, j_0)$.\nThe image $x$ is defined as:\n$$\nx[i,j] = \\begin{cases}\n1  \\text{if } i_0 \\le i  i_0+3 \\text{ and } j_0 \\le j  j_0+3 \\\\\n0  \\text{otherwise}\n\\end{cases}\n$$\nfor $i, j \\in \\{0, 1, \\dots, 7\\}$.\n\n**2. Convolutional Layer with Padding**\nThe convolutional layer uses a $3 \\times 3$ kernel $K$ where all entries are $1$. The operation performed is a two-dimensional discrete cross-correlation, which is equivalent to convolution here because the kernel is symmetric. The problem's convolution definition is:\n$$\nz[i,j] = (y \\ast k)[i,j] = \\sum_{u=0}^{2}\\sum_{v=0}^{2} K[u,v]\\; y[i+u-1, j+v-1]\n$$\nwhere $y$ is the padded input image. This operation can be implemented using standard library functions like `scipy.signal.correlate2d`, which computes $z[i,j] = \\sum_{u,v} K[u,v] y[i+u, j+v]$. The difference is a simple shift in the output coordinates, which does not affect the set of values in the output feature map $z$. Since the subsequent step is global average pooling, this shift has no impact on the final score.\n\nThe problem specifies three padding schemes (`A`, `B`):\n\n- **`same_zero`**: The $8 \\times 8$ input image $x$ is padded with a $1$-pixel border of zeros to produce a $10 \\times 10$ image $y$. A 'valid' correlation is then performed on $y$, resulting in an output feature map $z$ of size $(10-3+1) \\times (10-3+1) = 8 \\times 8$.\n- **`same_reflect`**: The input image $x$ is padded with a $1$-pixel border using reflection of the edge values. This also produces a $10 \\times 10$ image $y$. A 'valid' correlation on $y$ yields an $8 \\times 8$ feature map $z$.\n- **`valid`**: No padding is applied. The correlation is performed directly on the $8 \\times 8$ input image $x$. The resulting feature map $z$ has a size of $(8-3+1) \\times (8-3+1) = 6 \\times 6$.\n\n**3. Global Average Pooling**\nThe scalar score $s$ is computed by taking the arithmetic mean of all elements in the output feature map $z$:\n$$\ns = \\frac{1}{|z|}\\sum_{i,j} z[i,j]\n$$\nHere, $|z|$ denotes the total number of elements in $z$. Critically, $|z|$ is $8 \\times 8 = 64$ for `same_zero` and `same_reflect` padding, but $6 \\times 6 = 36$ for `valid` padding. This difference in the denominator is a key part of the calculation.\n\n**4. Classification and Comparison**\nFor each test case, we compute two scores, $s_A$ and $s_B$, using the two specified padding schemes $A$ and $B$. These scores are then used to determine the predicted classes:\n$$\n\\text{class}_A = \\begin{cases} 1  \\text{if } s_A \\ge \\tau \\\\ 0  \\text{if } s_A  \\tau \\end{cases} \\quad \\text{and} \\quad \\text{class}_B = \\begin{cases} 1  \\text{if } s_B \\ge \\tau \\\\ 0  \\text{if } s_B  \\tau \\end{cases}\n$$\nwith the given threshold $\\tau = 1.13$.\n\nThe final outputs for each test case are:\n- A boolean value indicating if the classification outcome is different: $\\text{flip} = (\\text{class}_A \\neq \\text{class}_B)$.\n- The floating-point difference in scores: $\\text{margin} = s_B - s_A$.\n\nThe procedure is systematically applied to each of the four test cases provided in the problem statement. For instance, let's analyze test case 3: $(i_0=0, j_0=0, A=\\text{valid}, B=\\text{same\\_reflect})$.\n\n- **Input `x`**: A $3 \\times 3$ block of ones at the top-left corner of an $8 \\times 8$ zero-matrix.\n- **Path A (`valid`)**:\n    - No padding. The correlation is performed on $x$.\n    - The output $z_A$ is $6 \\times 6$. A direct calculation shows that the sum of its elements is $\\sum z_A[i,j] = 36$.\n    - The score is $s_A = \\frac{1}{|z_A|} \\sum z_A = \\frac{36}{36} = 1.0$.\n    - Since $1.0  1.13$, $\\text{class}_A = 0$.\n- **Path B (`same_reflect`)**:\n    - The input `x` is padded with a 1-pixel border using reflection. The $3 \\times 3$ feature at the corner `(0,0)` interacts with this padding.\n    - Correlation on the padded image produces an $8 \\times 8$ feature map $z_B$.\n    - Direct computation, as performed by the code, shows the sum of elements in $z_B$ is $\\sum z_B[i,j] = 81$.\n    - The score is $s_B = \\frac{1}{|z_B|} \\sum z_B = \\frac{81}{64} = 1.265625$.\n    - Since $1.265625 \\ge 1.13$, $\\text{class}_B = 1$.\n- **Result for Case 3**:\n    - $\\text{flip} = (\\text{class}_A \\neq \\text{class}_B) = (0 \\neq 1) = \\text{True}$.\n    - $\\text{margin} = s_B - s_A = 1.265625 - 1.0 = 0.265625$.\nThe pair to be returned is $[\\text{True}, 0.265625]$.\n\nThis same logic is implemented for all test cases in the provided Python code.", "answer": "```python\nimport numpy as np\nfrom scipy.signal import correlate2d\n\ndef cnn_forward_pass(x, padding_mode, kernel):\n    \"\"\"\n    Performs a single forward pass of the simple CNN for a given padding mode.\n\n    Args:\n        x (np.ndarray): The input image.\n        padding_mode (str): The padding mode ('same_zero', 'same_reflect', 'valid').\n        kernel (np.ndarray): The convolutional kernel.\n\n    Returns:\n        float: The scalar score 's'.\n    \"\"\"\n    if padding_mode == 'same_zero':\n        # Pad with a 1-pixel border of zeros. Output size will be 8x8.\n        padded_x = np.pad(x, pad_width=1, mode='constant', constant_values=0)\n        # Correlate to get 8x8 output.\n        # (10x10 input corr 3x3) - (10-3+1)x(10-3+1) = 8x8 output.\n        z = correlate2d(padded_x, kernel, mode='valid')\n    elif padding_mode == 'same_reflect':\n        # Pad with a 1-pixel reflection. Output size will be 8x8.\n        padded_x = np.pad(x, pad_width=1, mode='reflect')\n        # Correlate to get 8x8 output.\n        z = correlate2d(padded_x, kernel, mode='valid')\n    elif padding_mode == 'valid':\n        # No padding. Output size will be 6x6.\n        # (8x8 input corr 3x3) - (8-3+1)x(8-3+1) = 6x6 output.\n        z = correlate2d(x, kernel, mode='valid')\n    else:\n        raise ValueError(f\"Unknown padding mode: {padding_mode}\")\n\n    # Global Average Pooling\n    score = np.mean(z)\n    return score\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and generate the final output.\n    \"\"\"\n    # Problem constants\n    H, W = 8, 8\n    kernel_size = 3\n    feature_size = 3\n    tau = 1.13\n    kernel = np.ones((kernel_size, kernel_size))\n\n    # Test suite: (i0, j0, mode_A, mode_B)\n    test_cases = [\n        (2, 2, 'same_zero', 'same_reflect'),\n        (2, 5, 'same_zero', 'same_reflect'),\n        (0, 0, 'valid', 'same_reflect'),\n        (5, 2, 'valid', 'same_zero')\n    ]\n\n    results = []\n    for i0, j0, mode_A, mode_B in test_cases:\n        # 1. Construct input image x\n        x = np.zeros((H, W), dtype=np.float64)\n        x[i0 : i0 + feature_size, j0 : j0 + feature_size] = 1.0\n\n        # 2. Evaluate for condition A\n        s_A = cnn_forward_pass(x, mode_A, kernel)\n        class_A = 1 if s_A >= tau else 0\n\n        # 3. Evaluate for condition B\n        s_B = cnn_forward_pass(x, mode_B, kernel)\n        class_B = 1 if s_B >= tau else 0\n\n        # 4. Compare results\n        classification_flipped = (class_A != class_B)\n        margin_difference = s_B - s_A\n\n        results.append([classification_flipped, margin_difference])\n\n    # Format the final output string exactly as specified.\n    # [ [bool_1,float_1], [bool_2,float_2], ... ]\n    # We build the string manually to avoid spaces introduced by str(list).\n    inner_strings = []\n    for flip, margin in results:\n        # Python's str(bool) is 'True'/'False'. The problem states \"plain booleans\".\n        # This is the most direct interpretation.\n        inner_strings.append(f\"[{str(flip).lower()},{margin}]\")\n    \n    final_output = f\"[{','.join(inner_strings)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3126196"}]}