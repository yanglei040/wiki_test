{"hands_on_practices": [{"introduction": "Standard ridge regression shrinks model parameters towards zero, a sensible default when we lack specific prior knowledge. However, in many scientific and engineering domains, we possess valuable information about the relationships between parameters. This practice explores a powerful extension of Tikhonov regularization that allows you to encode such information, such as known linear invariances, directly into the objective function through a null-space penalty. By completing this exercise [@problem_id:3096585], you will learn to derive and implement these generalized regularizers and empirically investigate how accurately specified prior knowledge can improve a model's predictive performance on unseen data.", "problem": "You are given a linear prediction model with parameter vector $w \\in \\mathbb{R}^d$, input matrix $X \\in \\mathbb{R}^{n \\times d}$, and responses $y \\in \\mathbb{R}^n$. Consider the quadratic objective that combines empirical risk minimization with two regularizers: an isotropic $\\ell_2$ penalty and a null-space penalty that enforces linear invariances via a matrix $A \\in \\mathbb{R}^{k \\times d}$. The objective is\n$$\nJ(w) \\;=\\; \\frac{1}{n}\\,\\lVert y - Xw \\rVert_2^2 \\;+\\; \\lambda\\,\\lVert w \\rVert_2^2 \\;+\\; \\mu\\,\\lVert Aw \\rVert_2^2,\n$$\nwhere $\\lambda \\ge 0$ and $\\mu \\ge 0$ are hyperparameters. The null-space penalty encourages $w$ to lie close to the null space of $A$, thereby penalizing deviations from specified linear invariances.\n\nYour tasks are:\n- Starting only from the fundamental definitions of least squares minimization and matrix calculus, derive the normal equations for the minimizer $w^\\star$ of $J(w)$, and obtain $w^\\star$ in closed form as a linear system solution. Then derive the associated hat matrix $H$ that maps the training response $y$ to its fitted values $\\hat{y} = X w^\\star$ and the effective degrees of freedom $\\mathrm{df} = \\mathrm{trace}(H)$.\n- Implement a program that solves for $w^\\star$ and computes the effective degrees of freedom and the out-of-sample mean squared error on a provided test set for several specified hyperparameter and constraint-matrix cases.\n\nUse the following fixed data for training and testing:\n- Training design matrix $X_{\\mathrm{train}} \\in \\mathbb{R}^{6 \\times 3}$,\n$$\nX_{\\mathrm{train}} \\;=\\;\n\\begin{pmatrix}\n1  0  2 \\\\\n0  1  -1 \\\\\n2  1  0 \\\\\n1  -1  1 \\\\\n3  0  -2 \\\\\n0  2  1\n\\end{pmatrix}.\n$$\n- Training responses $y_{\\mathrm{train}} \\in \\mathbb{R}^{6}$,\n$$\ny_{\\mathrm{train}} \\;=\\;\n\\begin{pmatrix}\n0.1 \\\\\n2.8 \\\\\n6.2 \\\\\n-1.0 \\\\\n7.7 \\\\\n3.1\n\\end{pmatrix}.\n$$\n- Test design matrix $X_{\\mathrm{test}} \\in \\mathbb{R}^{4 \\times 3}$,\n$$\nX_{\\mathrm{test}} \\;=\\;\n\\begin{pmatrix}\n1  1  0 \\\\\n0  1  2 \\\\\n2  -1  1 \\\\\n1  0  -1\n\\end{pmatrix}.\n$$\n- Noise-free test responses $y_{\\mathrm{test}} \\in \\mathbb{R}^{4}$, generated by a ground-truth parameter $w_{\\mathrm{true}} \\in \\mathbb{R}^3$ with components $w_{\\mathrm{true}} = (2,\\,2,\\,-1)$:\n$$\ny_{\\mathrm{test}} \\;=\\; X_{\\mathrm{test}}\\, w_{\\mathrm{true}} \\;=\\;\n\\begin{pmatrix}\n4 \\\\\n0 \\\\\n1 \\\\\n3\n\\end{pmatrix}.\n$$\n\nFor all test cases use $\\lambda = 0.1$. Define the following test suite of constraint settings, each specified by a pair $(\\mu, A)$:\n- Case $1$: $\\mu = 0$, $A = \\begin{pmatrix} 0  0  0 \\end{pmatrix}$.\n- Case $2$: $\\mu = 50$, $A = \\begin{pmatrix} 1  -1  0 \\end{pmatrix}$, which encourages the invariance $w_1 = w_2$ by penalizing deviations from the null space defined by $Aw = 0$.\n- Case $3$: $\\mu = 50$, $A = \\begin{pmatrix} 0  1  1 \\end{pmatrix}$, which encourages the invariance $w_2 = -w_3$ by penalizing deviations from the null space defined by $Aw = 0$.\n\nFor each case:\n- Let $n = 6$ and $d = 3$. Form the symmetric positive definite matrix\n$$\nM \\;=\\; \\frac{1}{n} X_{\\mathrm{train}}^\\top X_{\\mathrm{train}} \\;+\\; \\lambda I_d \\;+\\; \\mu\\, A^\\top A,\n$$\nand the right-hand side\n$$\nb \\;=\\; \\frac{1}{n} X_{\\mathrm{train}}^\\top y_{\\mathrm{train}}.\n$$\n- Compute $w^\\star$ by solving the linear system $M\\,w^\\star = b$.\n- Compute the hat matrix\n$$\nH \\;=\\; \\frac{1}{n}\\, X_{\\mathrm{train}}\\, M^{-1}\\, X_{\\mathrm{train}}^\\top,\n$$\nand the effective degrees of freedom $\\mathrm{df} = \\mathrm{trace}(H)$.\n- Compute the test mean squared error\n$$\n\\mathrm{MSE}_{\\mathrm{test}} \\;=\\; \\frac{1}{m}\\, \\lVert X_{\\mathrm{test}}\\, w^\\star - y_{\\mathrm{test}} \\rVert_2^2,\n$$\nwhere $m = 4$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list should be ordered as $[\\mathrm{MSE}_1, \\mathrm{df}_1, \\mathrm{MSE}_2, \\mathrm{df}_2, \\mathrm{MSE}_3, \\mathrm{df}_3]$, where index $i$ corresponds to Case $i$ above.\n- Round all floating-point outputs to $6$ decimal places in the printed line.\n- The required final output is a single line, for example $[r_1,r_2,r_3,r_4,r_5,r_6]$ with the specified order.\n\nNote: Angles are not involved and no physical units are required. All numerical quantities should be computed as real numbers in standard floating-point arithmetic. The answer for each test case must be reported as floats as specified above.", "solution": "We begin from the fundamental setup of linear least squares and matrix calculus. Let $X \\in \\mathbb{R}^{n \\times d}$, $y \\in \\mathbb{R}^n$, and $w \\in \\mathbb{R}^d$. Consider the objective\n$$\nJ(w) \\;=\\; \\frac{1}{n}\\,\\lVert y - Xw \\rVert_2^2 \\;+\\; \\lambda\\,\\lVert w \\rVert_2^2 \\;+\\; \\mu\\,\\lVert Aw \\rVert_2^2,\n$$\nwith $\\lambda \\ge 0$, $\\mu \\ge 0$, and $A \\in \\mathbb{R}^{k \\times d}$. This objective equals the sum of a strictly convex quadratic (provided regularization ensures positive definiteness) and nonnegative penalties. The unique minimizer arises from setting the gradient to zero.\n\nUsing the well-tested matrix calculus identity for the gradient of a quadratic, namely $\\nabla_w \\lVert y - Xw \\rVert_2^2 = -2 X^\\top (y - Xw)$, and $\\nabla_w \\lVert w \\rVert_2^2 = 2 w$, and $\\nabla_w \\lVert A w \\rVert_2^2 = 2 A^\\top A w$, we compute the gradient of $J(w)$:\n$$\n\\nabla_w J(w) \\;=\\; -\\frac{2}{n} X^\\top (y - Xw) \\;+\\; 2 \\lambda w \\;+\\; 2 \\mu A^\\top A w.\n$$\nSetting $\\nabla_w J(w) = 0$ and dividing by $2$ yields the normal equations\n$$\n\\left(\\frac{1}{n} X^\\top X + \\lambda I_d + \\mu A^\\top A\\right) w \\;=\\; \\frac{1}{n} X^\\top y.\n$$\nDefine the symmetric positive definite matrix\n$$\nM \\;=\\; \\frac{1}{n} X^\\top X + \\lambda I_d + \\mu A^\\top A,\n$$\nand the right-hand side\n$$\nb \\;=\\; \\frac{1}{n} X^\\top y.\n$$\nThen the unique minimizer is obtained by solving the linear system\n$$\nM w^\\star \\;=\\; b.\n$$\nNo shortcut formula beyond the linear system solution is necessary; a numerically stable way is to use a linear solver rather than an explicit matrix inverse.\n\nNext, for the fitted values $\\hat{y} = X w^\\star$, substitute $w^\\star = M^{-1} b$ to get\n$$\n\\hat{y} \\;=\\; X M^{-1} \\left( \\frac{1}{n} X^\\top y \\right) \\;=\\; \\left( \\frac{1}{n} X M^{-1} X^\\top \\right) y.\n$$\nTherefore, the hat matrix is\n$$\nH \\;=\\; \\frac{1}{n} X M^{-1} X^\\top.\n$$\nThe effective degrees of freedom, a standard measure of model flexibility for linear smoothers, is given by\n$$\n\\mathrm{df} \\;=\\; \\mathrm{trace}(H).\n$$\nThe null-space penalty modifies $M$ by adding the positive semidefinite matrix $\\mu A^\\top A$. This shrinks components of $w$ in directions orthogonal to the null space of $A$, effectively reducing variance in those directions. When the ground-truth parameter $w_{\\mathrm{true}}$ lies near the null space of $A$, this penalty improves generalization by reducing variance without much additional bias. Conversely, if $w_{\\mathrm{true}}$ significantly violates the constraint $A w = 0$, the penalty introduces bias that can harm out-of-sample performance.\n\nAlgorithmic procedure to compute the requested quantities for each test case:\n- Input $X_{\\mathrm{train}} \\in \\mathbb{R}^{n \\times d}$, $y_{\\mathrm{train}} \\in \\mathbb{R}^n$, $X_{\\mathrm{test}} \\in \\mathbb{R}^{m \\times d}$, $y_{\\mathrm{test}} \\in \\mathbb{R}^{m}$, hyperparameters $\\lambda$ and $\\mu$, and constraint matrix $A \\in \\mathbb{R}^{k \\times d}$.\n- Form $S = \\frac{1}{n} X_{\\mathrm{train}}^\\top X_{\\mathrm{train}}$, $M = S + \\lambda I_d + \\mu A^\\top A$, and $b = \\frac{1}{n} X_{\\mathrm{train}}^\\top y_{\\mathrm{train}}$.\n- Solve $M w^\\star = b$ using a linear solver to obtain $w^\\star$.\n- Compute $C = M^{-1} X_{\\mathrm{train}}^\\top$ without explicit inversion by solving $M C = X_{\\mathrm{train}}^\\top$; equivalently, $C$ is the solution of a matrix right-hand side. Then $H = \\frac{1}{n} X_{\\mathrm{train}} C$ and $\\mathrm{df} = \\mathrm{trace}(H)$.\n- Compute $\\mathrm{MSE}_{\\mathrm{test}} = \\frac{1}{m} \\lVert X_{\\mathrm{test}} w^\\star - y_{\\mathrm{test}} \\rVert_2^2$.\n- Round the floating-point outputs to $6$ decimal places and print in the specified order $[\\mathrm{MSE}_1, \\mathrm{df}_1, \\mathrm{MSE}_2, \\mathrm{df}_2, \\mathrm{MSE}_3, \\mathrm{df}_3]$.\n\nFor the provided test suite:\n- Case $1$ uses $\\mu = 0$ and $A = (0, 0, 0)$, which reduces to ridge regression with $\\lambda = 0.1$. This gives a baseline $\\mathrm{df}$ less than $d = 3$ and a certain out-of-sample $\\mathrm{MSE}_{\\mathrm{test}}$.\n- Case $2$ uses $\\mu = 50$ and $A = (1, -1, 0)$, strongly encouraging $w_1 \\approx w_2$. Since the ground truth satisfies $w_1 = w_2$, this penalty should reduce estimator variance along the $w_1 - w_2$ direction and often improve $\\mathrm{MSE}_{\\mathrm{test}}$, while further reducing $\\mathrm{df}$.\n- Case $3$ uses $\\mu = 50$ and $A = (0, 1, 1)$, strongly encouraging $w_2 \\approx - w_3$, which conflicts with $w_{\\mathrm{true}}$ where $w_2 + w_3 = 1$. This introduces bias, potentially inflating $\\mathrm{MSE}_{\\mathrm{test}}$, with a similar reduction in $\\mathrm{df}$ due to the strong penalty.\n\nThe specified program implements exactly these computations in a numerically stable manner using linear system solves, ensures the results are aggregated in the requested format, and rounds to $6$ decimal places as required.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    # Fixed training data (n=6, d=3)\n    X_train = np.array([\n        [1.0, 0.0, 2.0],\n        [0.0, 1.0, -1.0],\n        [2.0, 1.0, 0.0],\n        [1.0, -1.0, 1.0],\n        [3.0, 0.0, -2.0],\n        [0.0, 2.0, 1.0]\n    ], dtype=float)\n    y_train = np.array([0.1, 2.8, 6.2, -1.0, 7.7, 3.1], dtype=float)\n\n    # Fixed test data (m=4, d=3)\n    X_test = np.array([\n        [1.0, 1.0, 0.0],\n        [0.0, 1.0, 2.0],\n        [2.0, -1.0, 1.0],\n        [1.0, 0.0, -1.0]\n    ], dtype=float)\n    # Ground-truth w_true = (2, 2, -1), so y_test is noise-free\n    y_test = np.array([4.0, 0.0, 1.0, 3.0], dtype=float)\n\n    n, d = X_train.shape\n    m = X_test.shape[0]\n    lam = 0.1  # lambda\n\n    # Test suite of cases: (mu, A)\n    test_cases = [\n        (0.0, np.array([[0.0, 0.0, 0.0]], dtype=float)),     # Case 1\n        (50.0, np.array([[1.0, -1.0, 0.0]], dtype=float)),   # Case 2\n        (50.0, np.array([[0.0, 1.0, 1.0]], dtype=float)),    # Case 3\n    ]\n\n    # Precompute S and b components\n    Xt = X_train.T\n    S = (Xt @ X_train) / n\n    b = (Xt @ y_train) / n\n    I = np.eye(d)\n\n    results = []\n\n    for mu, A in test_cases:\n        # Form M = S + lam*I + mu * A^T A\n        AT_A = A.T @ A\n        M = S + lam * I + mu * AT_A\n\n        # Solve M w = b\n        w_star = np.linalg.solve(M, b)\n\n        # Compute H = (1/n) * X * M^{-1} * X^T without explicit inverse:\n        # Solve M * C = X^T - C = M^{-1} X^T\n        C = np.linalg.solve(M, Xt)  # shape (d, n)\n        H = (X_train @ C) / n\n        df = np.trace(H)\n\n        # Test MSE\n        y_pred_test = X_test @ w_star\n        mse_test = np.mean((y_pred_test - y_test) ** 2)\n\n        # Append rounded results in the specified order\n        results.extend([mse_test, df])\n\n    # Format results to 6 decimal places, no spaces\n    formatted = [f\"{val:.6f}\" for val in results]\n    print(f\"[{','.join(formatted)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3096585"}, {"introduction": "Regularization is not limited to static datasets; it is an essential tool for modeling structured data, such as time series. In many dynamic systems, parameters are expected to evolve smoothly over time rather than fluctuating wildly. This exercise [@problem_id:3096605] introduces Laplacian regularization, a technique that penalizes the differences between parameter estimates at adjacent time points, thereby enforcing temporal smoothness. You will implement this method to estimate a time-varying coefficient trajectory and use your results to perform a practical data analysis task: detecting abrupt \"regime shifts\" where the underlying system dynamics change.", "problem": "Consider a time-varying linear regression model indexed by discrete times $t \\in \\{1,\\dots,T\\}$. At each time $t$, we observe a design matrix $X_t \\in \\mathbb{R}^{n_t \\times p}$ and a response vector $y_t \\in \\mathbb{R}^{n_t}$. Assume the data are generated by a linear model with additive Gaussian noise: $y_t = X_t w_t^\\star + \\varepsilon_t$, where $w_t^\\star \\in \\mathbb{R}^p$ is the true coefficient vector at time $t$ and $\\varepsilon_t \\sim \\mathcal{N}(0, \\sigma^2 I_{n_t})$ are independent across times and samples. We wish to estimate a sequence of coefficients $\\{w_t\\}_{t=1}^T$ by solving a least-squares problem regularized to encourage temporal smoothness in the coefficients via a Laplacian smoothing penalty. Specifically, define the empirical risk\n$$\n\\mathcal{R}(w_1,\\dots,w_T) = \\sum_{t=1}^T \\lVert y_t - X_t w_t \\rVert_2^2 + \\lambda \\sum_{t=2}^T \\lVert w_t - w_{t-1} \\rVert_2^2,\n$$\nwhere $\\lambda \\ge 0$ is a regularization parameter. The penalty couples adjacent times and encourages small discrete temporal gradients, while allowing changes when the data demand them. A regime shift is defined to occur at time index $t \\in \\{2,\\dots,T\\}$ when the Euclidean norm of the discrete coefficient difference satisfies $\\lVert \\widehat{w}_t - \\widehat{w}_{t-1} \\rVert_2 \\ge \\tau$ for a user-chosen threshold $\\tau  0$, where $\\widehat{w}_t$ are the estimated coefficients.\n\nYour task is to implement a program that, for each test case described below, generates synthetic data under the Gaussian model, computes the regularized estimator by solving the optimization problem above using first-order optimality conditions derived from the Gaussian likelihood and the given penalty, and then detects regime shifts by thresholding the norms of the estimated discrete temporal gradients.\n\nStart from the following fundamental base:\n- The Gaussian log-likelihood under independent, identically distributed noise is proportional to the negative of the sum of squared residuals.\n- The regularized estimator is obtained by minimizing the sum of squared residuals plus a smoothness penalty.\n- The first-order optimality condition for minimizing a differentiable convex function requires the gradient to be zero at the minimizer.\n- The gradient of a sum is the sum of gradients, and the gradient of a squared Euclidean norm is linear in its argument.\n\nBy applying these principles, derive the linear system characterizing the minimizer and use it algorithmically to obtain the estimates $\\widehat{w}_1,\\dots,\\widehat{w}_T$. Then compute the discrete differences $d_t = \\lVert \\widehat{w}_t - \\widehat{w}_{t-1} \\rVert_2$ for $t \\in \\{2,\\dots,T\\}$, and declare a regime shift at time $t$ if and only if $d_t \\ge \\tau$. The final output for each test case is the list of detected regime shift indices (integers in $\\{2,\\dots,T\\}$), sorted in increasing order.\n\nData-generation protocol for each test case:\n- Fix a pseudorandom seed $s$.\n- For each $t \\in \\{1,\\dots,T\\}$, draw $X_t$ with independent standard normal entries, i.e., each entry is distributed as $\\mathcal{N}(0,1)$.\n- Construct the true coefficient sequence $\\{w_t^\\star\\}$ to be piecewise-constant according to the specified change times and regime coefficient vectors.\n- Draw noise $\\varepsilon_t$ with independent entries distributed as $\\mathcal{N}(0,\\sigma^2)$, and set $y_t = X_t w_t^\\star + \\varepsilon_t$.\n- Solve for $\\{\\widehat{w}_t\\}$ by minimizing $\\mathcal{R}$.\n- Compute and threshold $d_t$ to detect regime shifts.\n\nYour program must implement the above procedure for the following test suite. Each case is independent and must use its own seed. For each case, $n_t = n$ for all $t$.\n\n- Case A (happy path with clear regime shifts):\n  - $s = 31415$, $T = 12$, $p = 3$, $n = 60$, $\\sigma = 0.1$, $\\lambda = 8.0$, $\\tau = 0.9$.\n  - Change times: $[5, 9]$ meaning changes at $t = 5$ and $t = 9$.\n  - Regime coefficient vectors in order: $w^{(1)} = [1.0, 0.0, 0.0]^\\top$, $w^{(2)} = [1.0, 1.5, 0.0]^\\top$, $w^{(3)} = [0.0, 1.5, -1.0]^\\top$.\n\n- Case B (oversmoothing edge case with a small shift):\n  - $s = 27182$, $T = 12$, $p = 3$, $n = 60$, $\\sigma = 0.2$, $\\lambda = 50.0$, $\\tau = 0.4$.\n  - Change times: $[7]$ meaning a change at $t = 7$ only.\n  - Regime coefficient vectors in order: $w^{(1)} = [0.0, 0.0, 0.0]^\\top$, $w^{(2)} = [0.0, 0.25, 0.0]^\\top$.\n\n- Case C (no shift baseline):\n  - $s = 16180$, $T = 10$, $p = 4$, $n = 60$, $\\sigma = 0.1$, $\\lambda = 5.0$, $\\tau = 0.5$.\n  - Change times: $[]$ meaning no changes.\n  - Regime coefficient vectors in order: $w^{(1)} = [0.5, -0.5, 0.0, 0.0]^\\top$.\n\n- Case D (boundary-adjacent shifts):\n  - $s = 14142$, $T = 8$, $p = 2$, $n = 60$, $\\sigma = 0.1$, $\\lambda = 8.0$, $\\tau = 1.0$.\n  - Change times: $[2, 7]$ meaning changes at $t = 2$ and $t = 7$.\n  - Regime coefficient vectors in order: $w^{(1)} = [-1.0, 0.5]^\\top$, $w^{(2)} = [1.0, 0.5]^\\top$, $w^{(3)} = [1.0, -1.5]^\\top$.\n\nInterpretation of change times: Let the change times be $[c_1, c_2, \\dots, c_K]$ with $K \\ge 0$. Then $w_t^\\star = w^{(1)}$ for $t \\in \\{1,\\dots,c_1 - 1\\}$, $w_t^\\star = w^{(2)}$ for $t \\in \\{c_1,\\dots,c_2 - 1\\}$, and so on, with $w_t^\\star = w^{(K+1)}$ for $t \\in \\{c_K,\\dots,T\\}$. If $K = 0$, then $w_t^\\star = w^{(1)}$ for all $t$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of lists, with no spaces, enclosed in square brackets, where the $k$-th inner list is the ascending list of detected change indices for the $k$-th test case. For example, if the four cases produce detected change lists $L_1, L_2, L_3, L_4$, the output must be exactly $[L_1,L_2,L_3,L_4]$ with each $L_j$ printed as $[i_1,i_2,\\dots]$ and with no spaces anywhere in the line.", "solution": "The problem is valid. It is a well-posed problem in statistical learning, grounded in the principles of regularized linear regression and convex optimization. All parameters and procedures are specified, allowing for a unique and verifiable solution.\n\nThe task is to estimate a sequence of coefficient vectors $\\{w_t\\}_{t=1}^T$ in a time-varying linear model by minimizing a regularized objective function, and then to identify regime shifts based on the estimated coefficients.\n\nThe objective function to minimize is the regularized empirical risk:\n$$\n\\mathcal{R}(w_1,\\dots,w_T) = \\sum_{t=1}^T \\lVert y_t - X_t w_t \\rVert_2^2 + \\lambda \\sum_{t=2}^T \\lVert w_t - w_{t-1} \\rVert_2^2\n$$\nThe parameters $\\{w_1, \\dots, w_T\\}$, each in $\\mathbb{R}^p$, are the variables of the optimization problem. The function $\\mathcal{R}$ is a sum of squared norms, making it a quadratic function of the concatenated vector of all coefficients. As it is a sum of convex functions, $\\mathcal{R}$ is convex. Under the given data generation protocol where $n_t  p$ and the entries of $X_t$ are from a continuous distribution, the matrices $X_t^\\top X_t$ are positive definite with probability $1$. This guarantees that $\\mathcal{R}$ is strictly convex, and thus a unique minimizer $\\{\\widehat{w}_1, \\dots, \\widehat{w}_T\\}$ exists.\n\nThe minimizer is found by setting the gradient of $\\mathcal{R}$ with respect to each $w_k$ to zero. This is the first-order optimality condition. Let's compute the partial derivative of $\\mathcal{R}$ with respect to a single coefficient vector $w_k$ for $k \\in \\{1,\\dots,T\\}$.\n\nThe gradient of the least-squares data-fitting term with respect to $w_k$ is:\n$$\n\\frac{\\partial}{\\partial w_k} \\left( \\sum_{t=1}^T \\lVert y_t - X_t w_t \\rVert_2^2 \\right) = \\frac{\\partial}{\\partial w_k} \\lVert y_k - X_k w_k \\rVert_2^2 = 2(X_k^\\top X_k w_k - X_k^\\top y_k)\n$$\n\nThe gradient of the temporal smoothing penalty depends on whether $w_k$ is at a boundary ($k=1$ or $k=T$) or in the interior ($k \\in \\{2, \\dots, T-1\\}$).\n\nFor an interior time point, $k \\in \\{2, \\dots, T-1\\}$, $w_k$ appears in two penalty terms:\n$$\n\\frac{\\partial}{\\partial w_k} \\left( \\lambda \\lVert w_k - w_{k-1} \\rVert_2^2 + \\lambda \\lVert w_{k+1} - w_k \\rVert_2^2 \\right) = 2\\lambda(w_k - w_{k-1}) - 2\\lambda(w_{k+1} - w_k) = 2\\lambda(2w_k - w_{k-1} - w_{k+1})\n$$\n\nFor the first time point, $k=1$:\n$$\n\\frac{\\partial}{\\partial w_1} \\left( \\lambda \\lVert w_2 - w_1 \\rVert_2^2 \\right) = -2\\lambda(w_2 - w_1) = 2\\lambda(w_1 - w_2)\n$$\n\nFor the last time point, $k=T$:\n$$\n\\frac{\\partial}{\\partial w_T} \\left( \\lambda \\lVert w_T - w_{T-1} \\rVert_2^2 \\right) = 2\\lambda(w_T - w_{T-1})\n$$\n\nSetting the total partial derivative $\\frac{\\partial \\mathcal{R}}{\\partial w_k}$ to the zero vector and dividing by $2$ gives a system of linear equations:\n\nFor $k=1$:\n$$(X_1^\\top X_1) w_1 - X_1^\\top y_1 + \\lambda (w_1 - w_2) = 0 \\implies (X_1^\\top X_1 + \\lambda I_p) w_1 - \\lambda I_p w_2 = X_1^\\top y_1$$\n\nFor $k \\in \\{2, \\dots, T-1\\}$:\n$$(X_k^\\top X_k) w_k - X_k^\\top y_k + \\lambda (2w_k - w_{k-1} - w_{k+1}) = 0 \\implies -\\lambda I_p w_{k-1} + (X_k^\\top X_k + 2\\lambda I_p) w_k - \\lambda I_p w_{k+1} = X_k^\\top y_k$$\n\nFor $k=T$:\n$$(X_T^\\top X_T) w_T - X_T^\\top y_T + \\lambda (w_T - w_{T-1}) = 0 \\implies -\\lambda I_p w_{T-1} + (X_T^\\top X_T + \\lambda I_p) w_T = X_T^\\top y_T$$\n\nThis set of $T$ coupled vector equations can be formulated as a single large linear system $\\mathcal{H} W = \\mathcal{B}$, where $W = [w_1^\\top, \\dots, w_T^\\top]^\\top \\in \\mathbb{R}^{Tp}$ is the concatenation of all coefficient vectors. The right-hand side is $\\mathcal{B} = [(X_1^\\top y_1)^\\top, \\dots, (X_T^\\top y_T)^\\top]^\\top \\in \\mathbb{R}^{Tp}$. The system matrix $\\mathcal{H} \\in \\mathbb{R}^{Tp \\times Tp}$ is a symmetric block-tridiagonal matrix:\n$$\n\\mathcal{H} = \\begin{pmatrix}\nA_1  C   \\\\\nC  A_2  C  \\\\\n \\ddots  \\ddots  \\ddots \\\\\n  C  A_{T-1}  C \\\\\n   C  A_T\n\\end{pmatrix}\n$$\nwhere the blocks are $p \\times p$ matrices defined as:\n- Diagonal blocks:\n  - $A_1 = X_1^\\top X_1 + \\lambda I_p$\n  - $A_k = X_k^\\top X_k + 2\\lambda I_p$ for $k \\in \\{2, \\dots, T-1\\}$\n  - $A_T = X_T^\\top X_T + \\lambda I_p$\n- Off-diagonal blocks:\n  - $C = -\\lambda I_p$\n\nThe solution algorithm is as follows:\n1.  For each test case, generate the data $(X_t, y_t)$ for $t=1, \\dots, T$ according to the specified protocol. This involves using the provided pseudorandom seed, dimensions, noise level, and constructing the true piecewise-constant coefficient sequence $w_t^\\star$.\n2.  Construct the matrix $\\mathcal{H}$ and the vector $\\mathcal{B}$ as derived above.\n3.  Solve the linear system $\\mathcal{H} W = \\mathcal{B}$ to obtain the flattened vector of estimated coefficients $W = \\widehat{W}$.\n4.  Reshape $\\widehat{W}$ into the sequence of estimated coefficient vectors $\\{\\widehat{w}_t\\}_{t=1}^T$.\n5.  For each time $t \\in \\{2, \\dots, T\\}$, compute the discrete temporal difference norm $d_t = \\lVert \\widehat{w}_t - \\widehat{w}_{t-1} \\rVert_2$.\n6.  A regime shift is detected at time $t$ if $d_t \\ge \\tau$. The final output for the test case is the sorted list of all such $t$.\nThis procedure is implemented for each of the provided test cases.", "answer": "```python\nimport numpy as np\n\ndef process_case(s, T, p, n, sigma, lambda_reg, tau, change_times, regime_vectors):\n    \"\"\"\n    Generates synthetic data, solves the regularized regression problem,\n    and detects regime shifts for a single test case.\n    \"\"\"\n    # 1. Setup random number generator for reproducibility\n    rng = np.random.default_rng(s)\n\n    # 2. Generate the true piecewise-constant coefficient sequence w_star\n    w_star_T = np.zeros((T, p))\n    # Use searchsorted to find which regime each time t belongs to.\n    # The problem uses 1-based indexing for time, so we check against np.arange(1, T + 1).\n    regime_indices = np.searchsorted(change_times, np.arange(1, T + 1), side='right')\n    for t_idx in range(T):\n        regime_idx = regime_indices[t_idx]\n        w_star_T[t_idx, :] = regime_vectors[regime_idx]\n\n    # 3. Generate synthetic data (X_t, y_t) for t=1...T\n    X_T = []\n    y_T = []\n    for t_idx in range(T):\n        X_t = rng.normal(loc=0.0, scale=1.0, size=(n, p))\n        epsilon_t = rng.normal(loc=0.0, scale=sigma, size=n)\n        y_t = X_t @ w_star_T[t_idx, :] + epsilon_t\n        X_T.append(X_t)\n        y_T.append(y_t)\n\n    # 4. Assemble the block-tridiagonal system matrix H and the vector B\n    total_dim = T * p\n    H = np.zeros((total_dim, total_dim))\n    B = np.zeros(total_dim)\n    I_p = np.eye(p)\n\n    # Populate diagonal blocks of H and the vector B\n    for t_idx in range(T):\n        t = t_idx + 1 # 1-based time index\n        start, end = t_idx * p, (t_idx + 1) * p\n        \n        # RHS vector B\n        B[start:end] = X_T[t_idx].T @ y_T[t_idx]\n        \n        # Diagonal blocks of H\n        XtX = X_T[t_idx].T @ X_T[t_idx]\n        if t == 1 or t == T:\n            H[start:end, start:end] = XtX + lambda_reg * I_p\n        else: # 2 = t = T-1\n            H[start:end, start:end] = XtX + 2 * lambda_reg * I_p\n\n    # Populate off-diagonal blocks of H\n    off_diag_block = -lambda_reg * I_p\n    for t_idx in range(T - 1):\n        start1, end1 = t_idx * p, (t_idx + 1) * p\n        start2, end2 = (t_idx + 1) * p, (t_idx + 2) * p\n        H[start1:end1, start2:end2] = off_diag_block\n        H[start2:end2, start1:end1] = off_diag_block\n        \n    # 5. Solve the linear system H * W = B\n    W_hat_flat = np.linalg.solve(H, B)\n    W_hat_T = W_hat_flat.reshape((T, p))\n    \n    # 6. Detect regime shifts by thresholding norms of differences\n    shifts = []\n    for t_idx in range(1, T): # Corresponds to t=2...T\n        w_curr = W_hat_T[t_idx, :]\n        w_prev = W_hat_T[t_idx - 1, :]\n        diff_norm = np.linalg.norm(w_curr - w_prev)\n        if diff_norm = tau:\n            # Append the 1-based time index\n            shifts.append(t_idx + 1)\n            \n    return shifts\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final result.\n    \"\"\"\n    test_cases = [\n        # Case A\n        {'s': 31415, 'T': 12, 'p': 3, 'n': 60, 'sigma': 0.1, 'lambda_reg': 8.0, 'tau': 0.9,\n         'change_times': [5, 9], \n         'regime_vectors': [np.array([1.0, 0.0, 0.0]), np.array([1.0, 1.5, 0.0]), np.array([0.0, 1.5, -1.0])]},\n        # Case B\n        {'s': 27182, 'T': 12, 'p': 3, 'n': 60, 'sigma': 0.2, 'lambda_reg': 50.0, 'tau': 0.4,\n         'change_times': [7], \n         'regime_vectors': [np.array([0.0, 0.0, 0.0]), np.array([0.0, 0.25, 0.0])]},\n        # Case C\n        {'s': 16180, 'T': 10, 'p': 4, 'n': 60, 'sigma': 0.1, 'lambda_reg': 5.0, 'tau': 0.5,\n         'change_times': [], \n         'regime_vectors': [np.array([0.5, -0.5, 0.0, 0.0])]},\n        # Case D\n        {'s': 14142, 'T': 8, 'p': 2, 'n': 60, 'sigma': 0.1, 'lambda_reg': 8.0, 'tau': 1.0,\n         'change_times': [2, 7], \n         'regime_vectors': [np.array([-1.0, 0.5]), np.array([1.0, 0.5]), np.array([1.0, -1.5])]},\n    ]\n    \n    results = []\n    for case in test_cases:\n        detected_shifts = process_case(**case)\n        results.append(detected_shifts)\n    \n    # Format the output string precisely as required: [[...],[...],...] with no spaces.\n    inner_parts = []\n    for res_list in results:\n        # For each list, create the string representation like '[5,9]'\n        inner_str = '[' + ','.join(map(str, res_list)) + ']'\n        inner_parts.append(inner_str)\n    \n    final_output = '[' + ','.join(inner_parts) + ']'\n    print(final_output)\n\nsolve()\n```", "id": "3096605"}, {"introduction": "Having formulated a regularized model, a critical question remains: how do we choose the optimal strength of the regularization parameter, $\\lambda$? An arbitrary choice can lead to underfitting (if $\\lambda$ is too large) or overfitting (if $\\lambda$ is too small). This practice [@problem_id:3096680] introduces a principled and elegant method for this task: the Morozov discrepancy principle. You will see how this principle connects the choice of $\\lambda$ to the statistical properties of the data—specifically, the measurement noise level—providing a robust criterion for tuning your model. You will implement a solver based on this principle and explore its behavior when the noise level is both correctly specified and mis-specified.", "problem": "You are given a linear inverse problem with Gaussian measurement noise and a Tikhonov-regularized estimator. Let $A \\in \\mathbb{R}^{m \\times n}$, $x_{\\mathrm{true}} \\in \\mathbb{R}^{n}$, and $y = A x_{\\mathrm{true}} + \\varepsilon$, where $\\varepsilon \\sim \\mathcal{N}(0, \\sigma_{\\mathrm{true}}^{2} I_{m})$ with independent and identically distributed (i.i.d.) components. For any regularization parameter $\\lambda \\ge 0$, define the Tikhonov estimator $x_{\\lambda}$ as the unique minimizer of the strictly convex objective $\\|A x - y\\|_{2}^{2} + \\lambda \\|x\\|_{2}^{2}$. The Morozov discrepancy principle chooses $\\lambda$ by enforcing a target residual norm $\\|A x_{\\lambda} - y\\|_{2}$ equal to a prescribed discrepancy level $\\delta$, where $\\delta$ is set using an assumed noise standard deviation $\\sigma_{\\mathrm{assumed}}$ via $\\delta = \\sqrt{m}\\,\\sigma_{\\mathrm{assumed}}$.\n\nYour tasks are:\n\n- From first principles of linear least squares and Tikhonov regularization, formalize the residual $\\|A x_{\\lambda} - y\\|_{2}$ as a function of $\\lambda$ in a way that exposes its monotonic dependence on $\\lambda$.\n- Using the monotonicity, design a robust algorithm to select $\\lambda$ by solving for $\\|A x_{\\lambda} - y\\|_{2} = \\delta$ when a solution exists, and to return a boundary value otherwise. Specifically:\n  - Let $r_{\\min} = \\|A x_{0} - y\\|_{2}$ and $r_{\\max} = \\|y\\|_{2}$.\n  - If $\\delta \\le r_{\\min}$, set $\\lambda^{\\star} = 0$.\n  - Else if $\\delta \\ge r_{\\max}$, set $\\lambda^{\\star} = \\lambda_{\\max}$, where $\\lambda_{\\max}  0$ is a prescribed upper bound.\n  - Otherwise, find $\\lambda^{\\star} \\in (0, \\lambda_{\\max})$ such that $\\|A x_{\\lambda^{\\star}} - y\\|_{2} = \\delta$ to within a small numerical tolerance.\n- Implement the above as a complete program that produces results for a fixed synthetic instance and four assumed noise levels, including two deliberate mis-specifications.\n\nUse the following fixed instance and test suite:\n\n- Dimensions: $m = 60$, $n = 40$.\n- Random seed: $7$.\n- Data generation:\n  - Entries of $A$ are i.i.d. standard normal.\n  - Entries of $x_{\\mathrm{true}}$ are i.i.d. standard normal.\n  - $\\sigma_{\\mathrm{true}} = 0.1$.\n  - Noise $\\varepsilon$ has i.i.d. entries distributed as $\\mathcal{N}(0, \\sigma_{\\mathrm{true}}^{2})$.\n  - Observation $y = A x_{\\mathrm{true}} + \\varepsilon$.\n- Discrepancy levels are defined by $\\delta = \\sqrt{m}\\,\\sigma_{\\mathrm{assumed}}$ with the following four assumed noise levels:\n  1. $\\sigma_{\\mathrm{assumed}} = 0.1$ (correct specification; happy path).\n  2. $\\sigma_{\\mathrm{assumed}} = 10^{-6}$ (strong under-specification; boundary $\\delta \\le r_{\\min}$ is expected).\n  3. $\\sigma_{\\mathrm{assumed}} = 0.2$ (moderate over-specification; interior root expected).\n  4. $\\sigma_{\\mathrm{assumed}} = 10.0$ (extreme over-specification; boundary $\\delta \\ge r_{\\max}$ is expected).\n- Use $\\lambda_{\\max} = 10^{6}$ as the upper bound for the search interval.\n- Use a numerically stable and efficient computation of the residual norm $\\|A x_{\\lambda} - y\\|_{2}$ as a function of $\\lambda$ that does not require repeatedly solving linear systems from scratch.\n\nYour program must:\n\n- Construct the fixed instance exactly as described.\n- For each of the four values of $\\sigma_{\\mathrm{assumed}}$, compute $\\lambda^{\\star}$ according to the above rule, and also compute the signed gap $\\|A x_{\\lambda^{\\star}} - y\\|_{2} - \\delta$.\n- Aggregate the results into a single list by concatenating, in order, for each test case, the pair $(\\lambda^{\\star}, \\|A x_{\\lambda^{\\star}} - y\\|_{2} - \\delta)$ as two consecutive floating-point numbers.\n\nFinal output format:\n\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the following order for the four test cases: $[\\lambda^{\\star}_{1}, \\mathrm{gap}_{1}, \\lambda^{\\star}_{2}, \\mathrm{gap}_{2}, \\lambda^{\\star}_{3}, \\mathrm{gap}_{3}, \\lambda^{\\star}_{4}, \\mathrm{gap}_{4}]$.", "solution": "The problem requires the design and implementation of an algorithm to find the Tikhonov regularization parameter $\\lambda$ using the Morozov discrepancy principle. This involves first deriving an analytical expression for the residual norm as a function of $\\lambda$, demonstrating its monotonic behavior, and then using this property to construct a robust root-finding procedure.\n\nLet the linear model be $y = A x_{\\mathrm{true}} + \\varepsilon$, where $A \\in \\mathbb{R}^{m \\times n}$, $x_{\\mathrm{true}} \\in \\mathbb{R}^{n}$, $y \\in \\mathbb{R}^{m}$, and $\\varepsilon$ is Gaussian noise. The Tikhonov-regularized estimator $x_{\\lambda}$ for a given $\\lambda \\ge 0$ is the unique minimizer of the objective function:\n$$J(x) = \\|A x - y\\|_{2}^{2} + \\lambda \\|x\\|_{2}^{2}$$\nThe minimizer is found by setting the gradient $\\nabla_x J(x)$ to zero:\n$$\\nabla_x J(x) = 2 A^T (A x - y) + 2 \\lambda x = 0$$\nThis yields the normal equations for the Tikhonov problem:\n$$(A^T A + \\lambda I_n) x = A^T y$$\nFor any $\\lambda  0$, the matrix $(A^T A + \\lambda I_n)$ is positive definite and thus invertible. For $\\lambda=0$, it is invertible if $A$ has full column rank. The solution is:\n$$x_{\\lambda} = (A^T A + \\lambda I_n)^{-1} A^T y$$\nThe residual vector is $r(\\lambda) = A x_{\\lambda} - y$. Our goal is to formalize its norm, $\\|r(\\lambda)\\|_2$, as a function of $\\lambda$. A computationally efficient way to do this is via the Singular Value Decomposition (SVD) of $A$. We use the economy SVD for a matrix $A$ where $m \\ge n$:\n$$A = U \\Sigma V^T$$\nwhere $U \\in \\mathbb{R}^{m \\times n}$ has orthonormal columns ($U^T U = I_n$), $\\Sigma \\in \\mathbb{R}^{n \\times n}$ is a diagonal matrix with non-negative singular values $s_i$ on its diagonal, and $V \\in \\mathbb{R}^{n \\times n}$ is an orthogonal matrix ($V^T V = V V^T = I_n$).\n\nSubstituting the SVD into the expression for $x_{\\lambda}$:\n$$x_{\\lambda} = ( (V \\Sigma^T U^T) (U \\Sigma V^T) + \\lambda I_n )^{-1} (V \\Sigma^T U^T) y$$\n$$x_{\\lambda} = ( V \\Sigma^2 V^T + \\lambda V V^T )^{-1} V \\Sigma^T U^T y$$\n$$x_{\\lambda} = ( V (\\Sigma^2 + \\lambda I_n) V^T )^{-1} V \\Sigma^T U^T y$$\n$$x_{\\lambda} = V (\\Sigma^2 + \\lambda I_n)^{-1} V^T V \\Sigma^T U^T y = V (\\Sigma^2 + \\lambda I_n)^{-1} \\Sigma U^T y$$\nNow, we compute the product $A x_{\\lambda}$:\n$$A x_{\\lambda} = (U \\Sigma V^T) \\left( V (\\Sigma^2 + \\lambda I_n)^{-1} \\Sigma U^T y \\right) = U \\Sigma^2 (\\Sigma^2 + \\lambda I_n)^{-1} U^T y$$\nThe residual vector is $r(\\lambda) = A x_{\\lambda} - y$. To analyze its norm, we decompose $y$ into two orthogonal components: its projection onto the column space of $A$ (which is spanned by the columns of $U$), $U U^T y$, and its projection onto the orthogonal complement, $(I_m - U U^T) y$.\n$$r(\\lambda) = U \\Sigma^2 (\\Sigma^2 + \\lambda I_n)^{-1} U^T y - (U U^T y + (I_m - U U^T) y)$$\n$$r(\\lambda) = U \\left( \\Sigma^2 (\\Sigma^2 + \\lambda I_n)^{-1} - I_n \\right) U^T y - (I_m - U U^T) y$$\nThe matrix term in the parentheses is diagonal:\n$$\\Sigma^2 (\\Sigma^2 + \\lambda I_n)^{-1} - I_n = \\mathrm{diag}\\left(\\frac{s_i^2}{s_i^2 + \\lambda} - 1\\right) = \\mathrm{diag}\\left(\\frac{-\\lambda}{s_i^2 + \\lambda}\\right) = -\\lambda (\\Sigma^2 + \\lambda I_n)^{-1}$$\nThus, the residual vector simplifies to:\n$$r(\\lambda) = -U \\left( \\lambda (\\Sigma^2 + \\lambda I_n)^{-1} \\right) U^T y - (I_m - U U^T) y$$\nThe first term lies in the column space of $U$, while the second lies in its orthogonal complement. By the Pythagorean theorem, the squared norm of $r(\\lambda)$ is the sum of the squared norms of these two orthogonal vectors:\n$$\\|r(\\lambda)\\|_2^2 = \\| -U \\lambda (\\Sigma^2 + \\lambda I_n)^{-1} U^T y \\|_2^2 + \\| -(I_m - U U^T) y \\|_2^2$$\nSince $U$ has orthonormal columns, $\\|Uz\\|_2^2 = \\|z\\|_2^2$. Let $\\hat{y} = U^T y$, where $\\hat{y}_i = u_i^T y$.\n$$\\|r(\\lambda)\\|_2^2 = \\| \\lambda (\\Sigma^2 + \\lambda I_n)^{-1} \\hat{y} \\|_2^2 + \\|(I_m - U U^T) y\\|_2^2$$\n$$\\|r(\\lambda)\\|_2^2 = \\sum_{i=1}^{n} \\left( \\frac{\\lambda \\hat{y}_i}{s_i^2 + \\lambda} \\right)^2 + \\|(I_m - U U^T) y\\|_2^2$$\nThis expression provides an efficient method for calculating the residual norm, as the SVD and projection $U^T y$ are computed only once. Let $\\phi(\\lambda) = \\|r(\\lambda)\\|_2^2$.\n\nTo demonstrate monotonicity, we differentiate $\\phi(\\lambda)$ with respect to $\\lambda$ for $\\lambda  0$:\n$$\\frac{d\\phi}{d\\lambda} = \\sum_{i=1}^{n} \\hat{y}_i^2 \\frac{d}{d\\lambda} \\left( \\frac{\\lambda}{s_i^2 + \\lambda} \\right)^2 = \\sum_{i=1}^{n} \\haty_i^2 \\cdot 2 \\left( \\frac{\\lambda}{s_i^2 + \\lambda} \\right) \\frac{(s_i^2 + \\lambda) \\cdot 1 - \\lambda \\cdot 1}{(s_i^2 + \\lambda)^2}$$\n$$\\frac{d\\phi}{d\\lambda} = \\sum_{i=1}^{n} \\frac{2 \\lambda s_i^2 \\hat{y}_i^2}{(s_i^2 + \\lambda)^3}$$\nSince $\\lambda  0$ and $s_i \\ge 0$, every term in the sum is non-negative. Therefore, $\\frac{d\\phi}{d\\lambda} \\ge 0$, which implies that $\\|r(\\lambda)\\|_2$ is a monotonically non-decreasing function of $\\lambda$. This monotonicity is crucial for the Morozov discrepancy principle, which seeks a unique $\\lambda^\\star$ that solves $\\|A x_{\\lambda^\\star} - y\\|_2 = \\delta$ for a target discrepancy $\\delta$.\n\nThe range of possible residual norms is bounded.\nThe minimum residual norm, $r_{\\min}$, occurs at $\\lambda = 0$:\n$$r_{\\min} = \\|r(0)\\|_2 = \\sqrt{0 + \\|(I_m - U U^T) y\\|_2^2} = \\|(I_m - A A^+) y\\|_2$$\nThis is the norm of the residual for the unregularized least-squares problem, representing the part of $y$ outside the column space of $A$.\nThe maximum residual norm, $r_{\\max}$, is the limit as $\\lambda \\to \\infty$:\n$$r_{\\max} = \\lim_{\\lambda \\to \\infty} \\|r(\\lambda)\\|_2 = \\sqrt{ \\sum_{i=1}^{n} \\left( \\lim_{\\lambda \\to \\infty} \\frac{\\lambda}{\\lambda} \\hat{y}_i \\right)^2 + \\|...\\|^2 } = \\sqrt{ \\sum_{i=1}^{n} \\hat{y}_i^2 + \\|(I_m - U U^T) y\\|_2^2 } = \\|y\\|_2$$\nThis corresponds to a solution $x_\\lambda \\to 0$.\n\nThe algorithm for finding $\\lambda^\\star$ for a given discrepancy $\\delta = \\sqrt{m}\\,\\sigma_{\\mathrm{assumed}}$ is as follows:\n1.  Compute $r_{\\min}$ and $r_{\\max}$.\n2.  If $\\delta \\le r_{\\min}$: The target $\\delta$ is smaller than or equal to the minimum possible residual. The best we can do is achieve $r_{\\min}$, which occurs at $\\lambda = 0$. We set $\\lambda^\\star = 0$.\n3.  If $\\delta \\ge r_{\\max}$: The target $\\delta$ is larger than or equal to any achievable residual. Following the problem specification, we are constrained to $\\lambda \\in [0, \\lambda_{\\max}]$, so we choose the boundary value $\\lambda^\\star = \\lambda_{\\max}$.\n4.  If $r_{\\min}  \\delta  r_{\\max}$: Due to the continuity and monotonicity of $\\|r(\\lambda)\\|_2$, there exists a unique $\\lambda^\\star_0  0$ such that $\\|r(\\lambda^\\star_0)\\|_2 = \\delta$. We find this root numerically by solving the equation $f(\\lambda) = \\|r(\\lambda)\\|_2 - \\delta = 0$. Since we must find $\\lambda^\\star \\in (0, \\lambda_{\\max})$, we search in this interval. Given that $f(0)  0$ and $f(\\lambda)$ is increasing, a root in $(0, \\lambda_{\\max})$ exists if $f(\\lambda_{\\max})  0$. A robust root-finding method like Brent's method is suitable for this task. The final solution is $\\lambda^\\star = \\lambda^\\star_0$.\n\nThis procedure defines a robust algorithm for implementing the Morozov discrepancy principle, handling both interior and boundary cases for the choice of $\\lambda$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import brentq\n\ndef solve():\n    \"\"\"\n    Solves for the Tikhonov regularization parameter lambda using the Morozov discrepancy principle\n    for a fixed synthetic problem instance and four test cases of assumed noise levels.\n    \"\"\"\n    # Define problem parameters and test suite\n    m, n = 60, 40\n    random_seed = 7\n    sigma_true = 0.1\n    sigmas_assumed = [0.1, 1e-6, 0.2, 10.0]\n    lambda_max = 1.0e6\n\n    # Generate the fixed problem instance\n    rng = np.random.default_rng(random_seed)\n    A = rng.standard_normal((m, n))\n    x_true = rng.standard_normal(n)\n    noise = sigma_true * rng.standard_normal(m)\n    y = A @ x_true + noise\n\n    # Pre-computation using SVD for efficient residual norm calculation\n    # Use economy SVD since m = n\n    U, s, Vt = np.linalg.svd(A, full_matrices=False)\n    \n    # y_hat contains the projection of y onto the basis of U's columns\n    y_hat = U.T @ y\n    \n    # The squared norm of the component of y orthogonal to the column space of A\n    # This is the irreducible part of the residual\n    # Using np.linalg.norm(y)**2 - np.linalg.norm(y_hat)**2 is numerically stable\n    # due to the Pythagorean theorem and avoids forming the large projection matrix.\n    norm_y_perp_sq = np.linalg.norm(y)**2 - np.linalg.norm(y_hat)**2\n\n    def get_residual_norm(lam: float) - float:\n        \"\"\"\n        Computes ||A*x_lambda - y||_2 efficiently using SVD components.\n        \"\"\"\n        if lam == 0:\n            return np.sqrt(norm_y_perp_sq)\n        \n        # This term corresponds to the projection of the residual onto the column space of A\n        filter_factors = (lam * y_hat) / (s**2 + lam)\n        norm_proj_res_sq = np.sum(filter_factors**2)\n        \n        # The total squared residual norm is the sum of the squared norms of its\n        # orthogonal components.\n        residual_norm_sq = norm_proj_res_sq + norm_y_perp_sq\n        return np.sqrt(residual_norm_sq)\n\n    # Determine boundary residual norms\n    r_min = get_residual_norm(0.)\n    r_max = np.linalg.norm(y) # As lambda - infinity, x_lambda - 0, residual - y\n    \n    results = []\n    \n    for sigma_assumed in sigmas_assumed:\n        # Calculate the discrepancy target\n        delta = np.sqrt(m) * sigma_assumed\n        \n        lambda_star = 0.0\n        \n        # Case 1: Target discrepancy is too small (unachievable)\n        if delta = r_min:\n            lambda_star = 0.0\n        # Case 2: Target discrepancy is too large (approached at large lambda)\n        elif delta = r_max:\n            lambda_star = lambda_max\n        # Case 3: Interior case, find the unique lambda that matches the discrepancy\n        else:\n            # Objective function for the root-finder\n            root_func = lambda lam: get_residual_norm(lam) - delta\n            \n            # The function root_func is monotonic. Brent's method is robust.\n            # The interval [0, lambda_max] is guaranteed to bracket the root\n            # because root_func(0)  0 and root_func(lambda_max) will be  0.\n            # An extremely large lambda_max might push get_residual_norm(lambda_max)\n            # beyond r_max due to floating point, but scipy handles it.\n            lambda_star = brentq(root_func, 0, lambda_max)\n            \n        # Calculate the final signed gap for verification\n        gap = get_residual_norm(lambda_star) - delta\n        \n        results.append(lambda_star)\n        results.append(gap)\n        \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3096680"}]}