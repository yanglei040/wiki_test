{"hands_on_practices": [{"introduction": "Why is random search often more effective than grid search, even with the same number of trials? This practice explores the concept of \"effective dimension,\" where performance is only sensitive to a few hyperparameters. You will quantify how random search's ability to explore the hyperparameter space isotropically provides a more robust coverage of the optimal region compared to the rigid, axis-aligned structure of a grid search [@problem_id:3129463].", "problem": "Consider a two-dimensional hyperparameter space $\\mathcal{H} = [-1,1]^2$. Suppose the \"near-optimal\" configurations of two hyperparameters are described by an elliptical set centered at the origin,\n$$\n\\mathcal{E}(\\theta,a,b) = \\left\\{ x \\in \\mathcal{H} \\,:\\, x^\\top Q(\\theta,a,b)\\, x \\le 1 \\right\\},\n$$\nwhere the quadratic form matrix $Q(\\theta,a,b)$ is defined from the rotation matrix\n$$\nR(\\theta) = \\begin{pmatrix} \\cos\\theta & -\\sin\\theta \\\\[4pt] \\sin\\theta & \\cos\\theta \\end{pmatrix},\n$$\nand the diagonal scaling matrix\n$$\nD(a,b) = \\operatorname{diag}\\!\\left(\\frac{1}{a^2}, \\frac{1}{b^2}\\right),\n$$\nso that\n$$\nQ(\\theta,a,b) = R(\\theta)\\, D(a,b)\\, R(\\theta)^\\top.\n$$\nHere, $\\theta$ (in radians) encodes the correlation structure of the hyperparameters by rotating the principal axes of the ellipse, and $a>0$, $b>0$ are the semi-axis lengths. Assume $\\mathcal{E}(\\theta,a,b) \\subset \\mathcal{H}$ for the parameters considered.\n\nDefine two hyperparameter search strategies:\n- Grid search: sample $S$ points deterministically on an axis-aligned $N \\times N$ grid with $N = \\sqrt{S}$ (assume $S$ is a perfect square), using equispaced coordinates $\\{-1 + \\frac{2(i-1)}{N-1} \\,:\\, i=1,\\dots,N\\}$ along each axis.\n- Random search: sample $S$ points independently and uniformly from $\\mathcal{H}$, i.e., $S$ Independent and Identically Distributed (IID) draws from the uniform distribution on $\\mathcal{H}$, but evaluate its performance using analytical expectation rather than randomness.\n\nUse the \"coverage fraction\" (fraction of sampled points that fall inside $\\mathcal{E}(\\theta,a,b)$) as the performance metric. Let $\\hat{\\alpha}_{\\text{grid}}$ be the empirical coverage fraction of grid search, computed exactly by testing grid points for membership in $\\mathcal{E}(\\theta,a,b)$ via the inequality $x^\\top Q x \\le 1$. Let $\\alpha_{\\text{rand}}$ be the expected coverage fraction of random search, computed analytically from first principles under uniform sampling over $\\mathcal{H}$.\n\nFrom well-tested facts, the area of an ellipse with semi-axes $a$ and $b$ is $\\pi a b$, and the area of the square $\\mathcal{H}$ is $4$. Under uniform sampling, the expected coverage fraction is the ratio of areas, i.e., $\\alpha_{\\text{rand}} = \\frac{\\pi a b}{4}$, which is invariant to $\\theta$.\n\nYour task is to implement a program that, for each test case in the set below, computes:\n- $\\hat{\\alpha}_{\\text{grid}}$ exactly from the grid,\n- $\\alpha_{\\text{rand}}$ analytically as $\\frac{\\pi a b}{4}$,\n- the difference $\\Delta = \\hat{\\alpha}_{\\text{grid}} - \\alpha_{\\text{rand}}$.\n\nAngles are in radians. No physical units apply. The final outputs for each test case must be floats.\n\nTest suite (each tuple lists $(\\theta, a, b, S)$):\n- Case $1$: $(0, 0.6, 0.2, 100)$\n- Case $2$: $\\left(\\frac{\\pi}{4}, 0.6, 0.2, 100\\right)$\n- Case $3$: $\\left(\\frac{\\pi}{3}, 0.8, 0.05, 64\\right)$\n- Case $4$: $(0.3, 0.3, 0.3, 25)$\n- Case $5$: $\\left(\\frac{\\pi}{4}, 0.45, 0.25, 400\\right)$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered by the cases above. For example, the output format must be of the form $[\\Delta_1,\\Delta_2,\\Delta_3,\\Delta_4,\\Delta_5]$.", "solution": "The problem requires a comparative analysis of two hyperparameter search strategies, grid search and random search, on a two-dimensional hyperparameter space $\\mathcal{H} = [-1,1]^2$. The performance metric is the \"coverage fraction,\" which measures the proportion of sampled points that lie within a specified elliptical region of \"near-optimal\" configurations, $\\mathcal{E}(\\theta,a,b)$.\n\nOur objective is to compute the difference $\\Delta = \\hat{\\alpha}_{\\text{grid}} - \\alpha_{\\text{rand}}$ for a given set of test cases. Here, $\\hat{\\alpha}_{\\text{grid}}$ is the empirical coverage fraction for a deterministic grid search, and $\\alpha_{\\text{rand}}$ is the expected coverage fraction for a uniform random search.\n\nFirst, let us formalize the components of the problem.\n\nThe elliptical region $\\mathcal{E}$ is defined by the set of points $x \\in \\mathcal{H}$ satisfying the inequality $x^\\top Q x \\le 1$. The matrix $Q$ is a positive-definite symmetric matrix encoding the shape, size, and orientation of the ellipse. It is constructed as $Q(\\theta,a,b) = R(\\theta)\\, D(a,b)\\, R(\\theta)^\\top$, where:\n-   $a>0$ and $b>0$ are the lengths of the semi-axes of the ellipse.\n-   $\\theta$ is the rotation angle of the ellipse's principal axes with respect to the coordinate axes, given in radians.\n-   $R(\\theta)$ is the $2 \\times 2$ rotation matrix:\n    $$\n    R(\\theta) = \\begin{pmatrix} \\cos\\theta & -\\sin\\theta \\\\ \\sin\\theta & \\cos\\theta \\end{pmatrix}\n    $$\n-   $D(a,b)$ is a diagonal matrix containing the inverse squared semi-axis lengths:\n    $$\n    D(a,b) = \\operatorname{diag}\\!\\left(\\frac{1}{a^2}, \\frac{1}{b^2}\\right) = \\begin{pmatrix} \\frac{1}{a^2} & 0 \\\\ 0 & \\frac{1}{b^2} \\end{pmatrix}\n    $$\nThe quadratic form for a point $x = (x_1, x_2)^\\top$ can be written as $x^\\top Q x = Q_{11}x_1^2 + (Q_{12}+Q_{21})x_1x_2 + Q_{22}x_2^2$. Since $Q$ is symmetric ($Q=Q^\\top$), this simplifies to $Q_{11}x_1^2 + 2Q_{12}x_1x_2 + Q_{22}x_2^2$.\n\nThe calculation proceeds in two parts for each test case $(\\theta, a, b, S)$.\n\n**1. Expected Coverage Fraction for Random Search ($\\alpha_{\\text{rand}}$)**\n\nRandom search involves drawing $S$ points independently from a uniform distribution over the hyperparameter space $\\mathcal{H}$. The probability of a single point falling within the subregion $\\mathcal{E}$ is the ratio of the area of $\\mathcal{E}$ to the area of $\\mathcal{H}$. By the law of large numbers, the empirical fraction of points converges to this probability. The problem specifies using this analytical expectation for performance.\n\nThe area of the hyperparameter space $\\mathcal{H} = [-1,1]^2$ is $\\text{Area}(\\mathcal{H}) = (1 - (-1)) \\times (1 - (-1)) = 2 \\times 2 = 4$.\nThe area of an ellipse with semi-axes $a$ and $b$ is given by $\\text{Area}(\\mathcal{E}) = \\pi a b$. The problem assumes $\\mathcal{E} \\subset \\mathcal{H}$, so we do not need to consider the more complex case of intersection.\n\nTherefore, the expected coverage fraction for random search is:\n$$\n\\alpha_{\\text{rand}} = \\frac{\\text{Area}(\\mathcal{E})}{\\text{Area}(\\mathcal{H})} = \\frac{\\pi a b}{4}\n$$\nThis quantity depends only on the semi-axis lengths $a$ and $b$, and is invariant to the rotation angle $\\theta$.\n\n**2. Empirical Coverage Fraction for Grid Search ($\\hat{\\alpha}_{\\text{grid}}$)**\n\nGrid search samples points deterministically from a predefined grid. We are given $S$ total sample points, arranged on an $N \\times N$ grid, where $N = \\sqrt{S}$. The grid points are formed by the Cartesian product of one-dimensional coordinate sets. For each axis, the $N$ coordinates are equispaced within the interval $[-1, 1]$:\n$$\nc_i = -1 + \\frac{2(i-1)}{N-1} \\quad \\text{for } i = 1, 2, \\dots, N\n$$\nThe grid consists of $S$ points $(c_i, c_j)$ for all pairs $(i, j)$ where $i,j \\in \\{1, \\dots, N\\}$.\n\nTo find the empirical coverage fraction $\\hat{\\alpha}_{\\text{grid}}$, we must explicitly test each of the $S$ grid points for membership in $\\mathcal{E}$. Let $N_{\\text{inside}}$ be the number of grid points $x$ satisfying the condition $x^\\top Q x \\le 1$. The fraction is then:\n$$\n\\hat{\\alpha}_{\\text{grid}} = \\frac{N_{\\text{inside}}}{S}\n$$\nThe computational algorithm for each test case $(\\theta, a, b, S)$ is as follows:\n1.  Calculate $\\alpha_{\\text{rand}} = (\\pi a b) / 4$.\n2.  Determine the grid dimension $N = \\sqrt{S}$.\n3.  Construct the matrix $Q$ from the given $\\theta$, $a$, and $b$.\n4.  Generate the set of $S$ grid points $(x_1, x_2)$ where $x_1, x_2 \\in \\{-1 + \\frac{2(i-1)}{N-1} \\mid i=1,\\dots,N\\}$.\n5.  Initialize a counter $N_{\\text{inside}} = 0$.\n6.  For each grid point $x = (x_1, x_2)^\\top$:\n    a.  Compute the quadratic form value $v = x^\\top Q x = Q_{11}x_1^2 + 2Q_{12}x_1x_2 + Q_{22}x_2^2$.\n    b.  If $v \\le 1$, increment $N_{\\text{inside}}$.\n7.  Calculate $\\hat{\\alpha}_{\\text{grid}} = N_{\\text{inside}} / S$.\n8.  Compute the final difference $\\Delta = \\hat{\\alpha}_{\\text{grid}} - \\alpha_{\\text{rand}}$.\n\nThis procedure will be implemented for each test case specified in the problem statement to generate the final list of results.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the hyperparameter search comparison problem for a suite of test cases.\n    \"\"\"\n    \n    # Test suite: each tuple is (theta, a, b, S)\n    test_cases = [\n        (0, 0.6, 0.2, 100),\n        (np.pi / 4, 0.6, 0.2, 100),\n        (np.pi / 3, 0.8, 0.05, 64),\n        (0.3, 0.3, 0.3, 25),\n        (np.pi / 4, 0.45, 0.25, 400),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        theta, a, b, S = case\n        \n        # 1. Calculate the expected coverage fraction for random search (alpha_rand)\n        # This is the ratio of the area of the ellipse to the area of the search space.\n        # Area of ellipse = pi * a * b\n        # Area of search space [-1,1]^2 = 4\n        alpha_rand = (np.pi * a * b) / 4.0\n        \n        # 2. Calculate the empirical coverage fraction for grid search (alpha_grid)\n        \n        # Grid dimension N\n        N = int(np.sqrt(S))\n        \n        # Construct the quadratic form matrix Q = R * D * R.T\n        c, s = np.cos(theta), np.sin(theta)\n        R = np.array([[c, -s], \n                      [s, c]])\n        \n        D = np.diag([1.0 / a**2, 1.0 / b**2])\n        \n        Q = R @ D @ R.T\n        \n        # Generate grid points\n        # Using np.linspace is robust for N=1 case (though not in test suite)\n        if N > 1:\n            coords = np.linspace(-1.0, 1.0, N)\n        else:\n            # A 1x1 grid is at the center (0,0)\n            coords = np.array([0.0])\n\n        grid_x, grid_y = np.meshgrid(coords, coords)\n        \n        # Evaluate the quadratic form x.T * Q * x for all grid points\n        # x.T*Q*x = Q_11*x1^2 + 2*Q_12*x1*x2 + Q_22*x2^2\n        # Since Q is symmetric, Q_12 = Q_21\n        qf_values = (Q[0, 0] * grid_x**2 + \n                     (Q[0, 1] + Q[1, 0]) * grid_x * grid_y +\n                     Q[1, 1] * grid_y**2)\n                     \n        # Count points inside or on the ellipse (where x.T*Q*x = 1)\n        points_inside = np.sum(qf_values = 1.0)\n        \n        alpha_grid = points_inside / S\n        \n        # 3. Compute the difference Delta\n        delta = alpha_grid - alpha_rand\n        results.append(delta)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3129463"}, {"introduction": "Once we choose random search, the next critical decision is the sampling distribution. For parameters like learning rates or regularization strength, what matters is often the order of magnitude. This exercise provides a rigorous, first-principles derivation to show why sampling on a logarithmic scale can drastically reduce the expected \"regret\"—the performance gap between your best-found point and the true optimum—compared to a simple uniform sampling strategy [@problem_id:3129504].", "problem": "Consider the task of tuning a regularization parameter $\\lambda$ for a model whose validation performance depends primarily on the logarithm of the parameter. Assume the following simplified performance model in natural logarithm: the performance score for a given $\\lambda$ is $P(\\lambda)=1-c\\cdot\\left|\\log(\\lambda)-\\log(\\lambda^*)\\right|$, where $c0$ is a small constant and $\\lambda^*$ is the unknown optimal parameter within the search interval. A random search draws $N$ independent samples $\\lambda_1,\\dots,\\lambda_N$ from a specified sampling distribution over a search interval $\\lambda\\in[\\lambda_{\\min},\\lambda_{\\max}]$. The random search selects the sample with the highest performance. Define the regret as $R=P(\\lambda^*)-\\max_{i=1,\\dots,N} P(\\lambda_i)$.\n\nStarting from the definitions of expected value and independent sampling, and without using any shortcut formulas, derive how to compute the expected regret $\\mathbb{E}[R]$ for two sampling distributions:\n\n1. Uniform-in-parameter sampling: each $\\lambda_i$ is drawn independently from a uniform distribution over $[\\lambda_{\\min},\\lambda_{\\max}]$.\n2. Uniform-in-log sampling: each $\\lambda_i$ is drawn independently so that $X_i=\\log(\\lambda_i)$ is uniform over $[a,b]$, where $a=\\log(\\lambda_{\\min})$ and $b=\\log(\\lambda_{\\max})$.\n\nUse the following fundamental base:\n- The definition of expected value $\\mathbb{E}[Y]=\\int y\\,\\mathrm{d}\\mathbb{P}(y)$ for a random variable $Y$.\n- The independence of samples implies that the probability of no sample falling in a measurable set $A$ is the product of the complements of the single-sample probabilities.\n- The identity for nonnegative random variables $Y$: $\\mathbb{E}[Y]=\\int_{0}^{\\infty}\\mathbb{P}(Yt)\\,\\mathrm{d}t$.\n\nLet $X^*=\\log(\\lambda^*)$, $a=\\log(\\lambda_{\\min})$, and $b=\\log(\\lambda_{\\max})$, and define the minimum absolute log-distance $D_N=\\min_{i=1,\\dots,N}\\left|X_i-X^*\\right|$. For this performance model, the regret can be written as $R=c\\cdot D_N$, so $\\mathbb{E}[R]=c\\cdot\\mathbb{E}[D_N]$. Show that for each sampling scheme, $\\mathbb{E}[D_N]$ can be expressed as a one-dimensional integral over $t\\in[0,t_{\\max}]$, where $t_{\\max}=\\max\\{X^*-a,\\,b-X^*\\}$, involving the probability that none of the $N$ samples fall into the interval $(X^*-t,X^*+t)$ intersected with $[a,b]$. Precisely specify this probability for each scheme:\n- For uniform-in-log sampling, write the single-sample probability of landing in $(X^*-t,X^*+t)\\cap[a,b]$ as the ratio of lengths in log-space.\n- For uniform-in-parameter sampling, write the single-sample probability of landing in the corresponding interval as the ratio of lengths in parameter space, using the exponential mapping from log-space.\n\nYour program must compute $\\mathbb{E}[R]$ numerically for both sampling schemes using the integral representation and produce the following outputs for a test suite of four cases. For each case, output three floating-point numbers in order: the expected regret under uniform-in-parameter sampling, the expected regret under uniform-in-log sampling, and their difference (uniform minus log). Aggregate all results into a single line as a comma-separated list enclosed in square brackets.\n\nUse natural logarithms in all computations. The test suite is:\n- Case $1$: $\\lambda_{\\min}=10^{-4}$, $\\lambda_{\\max}=10^{2}$, $\\lambda^*=10^{-1}$, $c=0.05$, $N=20$.\n- Case $2$: $\\lambda_{\\min}=10^{-4}$, $\\lambda_{\\max}=10^{2}$, $\\lambda^*=10^{-4}$, $c=0.05$, $N=20$.\n- Case $3$: $\\lambda_{\\min}=10^{-6}$, $\\lambda_{\\max}=10^{0}$, $\\lambda^*=10^{-3}$, $c=0.10$, $N=1$.\n- Case $4$: $\\lambda_{\\min}=10^{-3}$, $\\lambda_{\\max}=10^{-2}$, $\\lambda^*=5\\cdot 10^{-3}$, $c=0.20$, $N=10$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order: $[\\mathbb{E}[R]_{\\text{uniform}},\\mathbb{E}[R]_{\\text{log}},\\Delta,\\dots]$ for the four cases, where $\\Delta=\\mathbb{E}[R]_{\\text{uniform}}-\\mathbb{E}[R]_{\\text{log}}$ for each case.", "solution": "The problem statement is first subjected to a rigorous validation process before a solution is attempted.\n\n### Problem Validation\n\n**Step 1: Extracted Givens**\n\nThe verbatim data, variables, and conditions provided in the problem statement are as follows:\n- Performance model: $P(\\lambda)=1-c\\cdot\\left|\\log(\\lambda)-\\log(\\lambda^*)\\right|$, with $c0$.\n- Optimal parameter: $\\lambda^*$.\n- Search interval: $\\lambda\\in[\\lambda_{\\min},\\lambda_{\\max}]$.\n- Search method: Random search with $N$ independent samples $\\lambda_1, \\dots, \\lambda_N$.\n- Regret definition: $R=P(\\lambda^*)-\\max_{i=1,\\dots,N} P(\\lambda_i)$.\n- Sampling distributions:\n  1.  Uniform-in-parameter: $\\lambda_i \\sim U[\\lambda_{\\min},\\lambda_{\\max}]$.\n  2.  Uniform-in-log: $X_i=\\log(\\lambda_i) \\sim U[a,b]$, where $a=\\log(\\lambda_{\\min})$ and $b=\\log(\\lambda_{\\max})$.\n- Fundamental principles to be used:\n  - Definition of expected value: $\\mathbb{E}[Y]=\\int y\\,\\mathrm{d}\\mathbb{P}(y)$.\n  - Independence of samples.\n  - Identity for nonnegative random variables: $\\mathbb{E}[Y]=\\int_{0}^{\\infty}\\mathbb{P}(Yt)\\,\\mathrm{d}t$.\n- Notation: $X^*=\\log(\\lambda^*)$, $D_N=\\min_{i=1,\\dots,N}\\left|X_i-X^*\\right|$.\n- Relation: $R=c\\cdot D_N$, and $\\mathbb{E}[R]=c\\cdot\\mathbb{E}[D_N]$.\n- Task: Express $\\mathbb{E}[D_N]$ as a one-dimensional integral over $t\\in[0,t_{\\max}]$, where $t_{\\max}=\\max\\{X^*-a,\\,b-X^*\\}$.\n- Test cases:\n  - Case $1$: $\\lambda_{\\min}=10^{-4}$, $\\lambda_{\\max}=10^{2}$, $\\lambda^*=10^{-1}$, $c=0.05$, $N=20$.\n  - Case $2$: $\\lambda_{\\min}=10^{-4}$, $\\lambda_{\\max}=10^{2}$, $\\lambda^*=10^{-4}$, $c=0.05$, $N=20$.\n  - Case $3$: $\\lambda_{\\min}=10^{-6}$, $\\lambda_{\\max}=10^{0}$, $\\lambda^*=10^{-3}$, $c=0.10$, $N=1$.\n  - Case $4$: $\\lambda_{\\min}=10^{-3}$, $\\lambda_{\\max}=10^{-2}$, $\\lambda^*=5\\cdot 10^{-3}$, $c=0.20$, $N=10$.\n- Computation rule: Use natural logarithms in all computations.\n\n**Step 2: Validate Using Extracted Givens**\n\nThe problem is evaluated against the established validation criteria:\n- **Scientifically Grounded:** The problem is a well-established exercise in statistical learning theory, concerning the analysis of hyperparameter optimization strategies. The mathematical framework is standard probability theory. The performance model is a valid simplification used in theoretical analyses. This criterion is met.\n- **Well-Posed:** The problem is clearly specified. It provides all necessary data and definitions to derive the requested formulas and to perform the numerical computations. A unique, stable, and meaningful solution exists. This criterion is met.\n- **Objective:** The problem is stated in precise, mathematical language, free of any subjective or ambiguous terminology. This criterion is met.\n- **Completeness and Consistency:** The setup is self-contained and internally consistent. The relationship $R=c\\cdot D_N$ is a direct consequence of the definitions of $P(\\lambda)$ and $R$. All parameters for the test cases are provided. This criterion is met.\n- **Realism and Feasibility:** The problem is a theoretical model but is directly relevant to real-world challenges in machine learning. The parameter values are scientifically plausible. This criterion is met.\n- **Non-Triviality:** The derivation requires a rigorous application of probability theory, calculus, and transformation of random variables. It is a substantive problem that is not reducible to a trivial statement. This criterion is met.\n\n**Step 3: Verdict and Action**\n\nThe problem is determined to be **valid**. It is scientifically sound, well-posed, and complete. A full solution will be provided.\n\n### Derivation of the Expected Regret\n\nThe objective is to derive the expected regret, $\\mathbb{E}[R]$, for two different sampling schemes. As given, the regret $R$ is related to the minimum absolute log-distance $D_N$ by $R=c \\cdot D_N$. Due to the linearity of expectation, we have $\\mathbb{E}[R] = c \\cdot \\mathbb{E}[D_N]$.\n\nThe derivation of $\\mathbb{E}[D_N]$ will proceed from the fundamental identity for a non-negative random variable $Y$:\n$$ \\mathbb{E}[Y] = \\int_0^\\infty \\mathbb{P}(Y  t) \\, \\mathrm{d}t $$\nIn our case, the random variable is $D_N = \\min_{i=1, \\dots, N} |X_i - X^*|$, which is non-negative. Thus,\n$$ \\mathbb{E}[D_N] = \\int_0^\\infty \\mathbb{P}(D_N  t) \\, \\mathrm{d}t $$\nThe event $\\{D_N  t\\}$ is equivalent to the event that for all samples $i \\in \\{1, \\dots, N\\}$, the condition $|X_i - X^*|  t$ holds. The samples $X_i$ are drawn independently and are identically distributed (i.i.d.). Therefore, the probability of the joint event is the product of the individual probabilities:\n$$ \\mathbb{P}(D_N  t) = \\mathbb{P}\\left(\\bigcap_{i=1}^N \\{|X_i - X^*|  t\\}\\right) = \\prod_{i=1}^N \\mathbb{P}(|X_i - X^*|  t) = [\\mathbb{P}(|X_1 - X^*|  t)]^N $$\nLet $p(t)$ denote the probability that a single sample $X_1$ falls within a log-distance $t$ of the optimum $X^*$, i.e., $p(t) = \\mathbb{P}(|X_1 - X^*| \\le t)$. The complementary probability is $\\mathbb{P}(|X_1 - X^*|  t) = 1 - p(t)$. Substituting this into the expression for $\\mathbb{P}(D_N  t)$ gives:\n$$ \\mathbb{P}(D_N  t) = [1 - p(t)]^N $$\nThe search space for the log-parameter $X_1$ is the interval $[a, b] = [\\log(\\lambda_{\\min}), \\log(\\lambda_{\\max})]$. Since $X_1 \\in [a, b]$ and $X^* \\in [a, b]$, the maximum possible value for $|X_1 - X^*|$ is $t_{\\max} = \\max(X^* - a, b - X^*)$. For any $t  t_{\\max}$, the interval $[a, b]$ is fully contained within $[X^* - t, X^* + t]$. This implies that for $t  t_{\\max}$, $p(t) = \\mathbb{P}(X_1 \\in [a, b]) = 1$, and consequently $\\mathbb{P}(D_N  t) = [1 - 1]^N = 0$. The integral for the expected value can thus be restricted to the interval $[0, t_{\\max}]$:\n$$ \\mathbb{E}[D_N] = \\int_0^{t_{\\max}} [1 - p(t)]^N \\, \\mathrm{d}t $$\nWe now derive the specific form of $p(t)$ for each sampling scheme.\n\n**1. Uniform-in-Log Sampling**\n\nIn this scheme, $X_i = \\log(\\lambda_i)$ is drawn from a uniform distribution over $[a, b]$. The probability density function is $f_X(x) = 1/(b-a)$ for $x \\in [a, b]$.\nThe probability $p_{\\text{log}}(t) = \\mathbb{P}(|X_1 - X^*| \\le t)$ is the probability that $X_1$ falls in the interval $[X^* - t, X^* + t]$. Since $X_1$ is confined to $[a, b]$, we are interested in the probability of $X_1$ falling into the intersection of these two intervals: $I(t) = [X^* - t, X^* + t] \\cap [a, b]$.\nFor a uniform distribution, this probability is the ratio of the length of the intersection to the length of the total space:\n$$ p_{\\text{log}}(t) = \\frac{\\text{length}(I(t))}{b-a} = \\frac{\\text{length}([\\max(a, X^* - t), \\min(b, X^* + t)])}{b-a} = \\frac{\\min(b, X^* + t) - \\max(a, X^* - t)}{b-a} $$\nThe expected regret for uniform-in-log sampling is therefore:\n$$ \\mathbb{E}[R]_{\\text{log}} = c \\int_0^{t_{\\max}} \\left[1 - \\frac{\\min(b, X^* + t) - \\max(a, X^* - t)}{b-a}\\right]^N \\, \\mathrm{d}t $$\n\n**2. Uniform-in-Parameter Sampling**\n\nIn this scheme, $\\lambda_i$ is drawn from a uniform distribution over $[\\lambda_{\\min}, \\lambda_{\\max}]$. The probability density function is $f_\\lambda(\\ell) = 1/(\\lambda_{\\max} - \\lambda_{\\min})$ for $\\ell \\in [\\lambda_{\\min}, \\lambda_{\\max}]$.\nWe need to find $p_{\\text{uniform}}(t) = \\mathbb{P}(|X_1 - X^*| \\le t)$. Since $X_1 = \\log(\\lambda_1)$ and $X^* = \\log(\\lambda^*)$, this is equivalent to $\\mathbb{P}(\\log(\\lambda^*) - t \\le \\log(\\lambda_1) \\le \\log(\\lambda^*) + t)$, which simplifies to $\\mathbb{P}(\\lambda^* e^{-t} \\le \\lambda_1 \\le \\lambda^* e^t)$.\nThis probability is computed with respect to the uniform distribution of $\\lambda_1$ on $[\\lambda_{\\min}, \\lambda_{\\max}]$. The event corresponds to $\\lambda_1$ falling in the interval $J(t) = [\\lambda^* e^{-t}, \\lambda^* e^t] \\cap [\\lambda_{\\min}, \\lambda_{\\max}]$.\nThe probability is the ratio of the lengths of the intervals in the parameter space:\n$$ p_{\\text{uniform}}(t) = \\frac{\\text{length}(J(t))}{\\lambda_{\\max} - \\lambda_{\\min}} = \\frac{\\text{length}([\\max(\\lambda_{\\min}, \\lambda^* e^{-t}), \\min(\\lambda_{\\max}, \\lambda^* e^t)])}{\\lambda_{\\max} - \\lambda_{\\min}} $$\n$$ p_{\\text{uniform}}(t) = \\frac{\\min(\\lambda_{\\max}, \\lambda^* e^t) - \\max(\\lambda_{\\min}, \\lambda^* e^{-t})}{\\lambda_{\\max} - \\lambda_{\\min}} $$\nThe expected regret for uniform-in-parameter sampling is:\n$$ \\mathbb{E}[R]_{\\text{uniform}} = c \\int_0^{t_{\\max}} \\left[1 - \\frac{\\min(\\lambda_{\\max}, \\lambda^* e^t) - \\max(\\lambda_{\\min}, \\lambda^* e^{-t})}{\\lambda_{\\max} - \\lambda_{\\min}}\\right]^N \\, \\mathrm{d}t $$\nThese integral expressions allow for the numerical computation of the expected regret for both sampling strategies.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.integrate import quad\n\ndef solve():\n    \"\"\"\n    Computes the expected regret for hyperparameter tuning under two sampling schemes\n    for a suite of test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (lambda_min, lambda_max, lambda_star, c, N)\n        (10**-4, 10**2, 10**-1, 0.05, 20),\n        (10**-4, 10**2, 10**-4, 0.05, 20),\n        (10**-6, 10**0, 10**-3, 0.10, 1),\n        (10**-3, 10**-2, 5 * 10**-3, 0.20, 10),\n    ]\n\n    results = []\n    \n    for l_min, l_max, l_star, c, N in test_cases:\n        # Calculate log-space parameters. All logs are natural logarithms.\n        a = np.log(l_min)\n        b = np.log(l_max)\n        X_star = np.log(l_star)\n\n        # The upper limit of integration for t, the log-distance.\n        t_max = max(X_star - a, b - X_star)\n\n        # --- Integrand for Uniform-in-Log Sampling ---\n        def integrand_log(t, a, b, X_star, N):\n            \"\"\"\n            Computes the value of the integrand for the expected log-distance\n            under uniform-in-log sampling. The integrand is P(D_N > t).\n            \"\"\"\n            # Length of the intersection: [X*-t, X*+t] cap [a,b]\n            len_intersection = min(b, X_star + t) - max(a, X_star - t)\n            \n            # Probability p(t) of one sample falling within log-distance t\n            p_t = len_intersection / (b - a)\n            \n            # Probability that all N samples fall outside log-distance t\n            return (1.0 - p_t)**N\n\n        # --- Integrand for Uniform-in-Parameter Sampling ---\n        def integrand_uniform(t, l_min, l_max, l_star, N):\n            \"\"\"\n            Computes the value of the integrand for the expected log-distance\n            under uniform-in-parameter sampling.\n            \"\"\"\n            # Length of the intersection in the parameter space:\n            # [l_star*exp(-t), l_star*exp(t)] cap [l_min, l_max]\n            len_intersection = min(l_max, l_star * np.exp(t)) - max(l_min, l_star * np.exp(-t))\n            \n            # Probability p(t) that one sample's log falls within distance t\n            if l_max - l_min == 0:\n                p_t = 1.0 # Should not happen with valid inputs\n            else:\n                p_t = len_intersection / (l_max - l_min)\n\n            return (1.0 - p_t)**N\n\n        # --- Numerical Integration ---\n\n        # Expected log-distance for uniform-in-log sampling\n        E_DN_log, _ = quad(\n            integrand_log, 0, t_max, args=(a, b, X_star, N)\n        )\n        # Expected regret\n        E_R_log = c * E_DN_log\n\n        # Expected log-distance for uniform-in-parameter sampling\n        E_DN_uniform, _ = quad(\n            integrand_uniform, 0, t_max, args=(l_min, l_max, l_star, N)\n        )\n        # Expected regret\n        E_R_uniform = c * E_DN_uniform\n        \n        # Difference in expected regrets\n        diff = E_R_uniform - E_R_log\n\n        results.extend([E_R_uniform, E_R_log, diff])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{x:.16g}' for x in results)}]\")\n\nsolve()\n```", "id": "3129504"}, {"introduction": "Theoretical efficiency translates into practical risk management. This exercise models a common and critical scenario in machine learning: a performance landscape with a sharp \"cliff,\" where crossing a certain threshold (e.g., for a learning rate) causes model instability. You will compare how grid search and log-uniform random search differ in their exposure to these \"dangerous\" regions, highlighting how a well-chosen search strategy can improve not only performance but also reliability [@problem_id:3129497].", "problem": "You will implement a program to quantify and compare the exposure risk of two hyperparameter tuning strategies—grid search and random search—over a learning rate parameter with a sharp instability cliff. Consider a learning rate $\\eta \\in [\\eta_{\\min}, \\eta_{\\max}]$ and an instability threshold $\\eta_c$ such that models trained with $\\eta  \\eta_c$ are deemed dangerous (unstable), while models trained with $\\eta \\le \\eta_c$ are stable. To make the notion of a sharp cliff explicit, define a synthetic validation loss $L(\\eta)$ by\n$$\nL(\\eta) = \n\\begin{cases}\nL_0 + (\\eta - \\eta_*)^2,  \\text{if } \\eta \\le \\eta_c, \\\\\nD,  \\text{if } \\eta  \\eta_c,\n\\end{cases}\n$$\nwhere $L_0  0$ is a baseline loss, $\\eta_*$ is a target learning rate inside the stable region, and $D \\gg L_0$ is a large penalty representing instability.\n\nGrid search uses $G$ equally spaced points including endpoints,\n$$\n\\eta_i = \\eta_{\\min} + i \\cdot \\frac{\\eta_{\\max} - \\eta_{\\min}}{G - 1}, \\quad i \\in \\{0,1,\\ldots,G-1\\}.\n$$\nRandom search draws $\\eta$ from a log-uniform distribution on $[\\eta_{\\min}, \\eta_{\\max}]$ with probability density function\n$$\nf(\\eta) = \\frac{1}{\\eta \\, \\ln\\!\\left(\\frac{\\eta_{\\max}}{\\eta_{\\min}}\\right)} \\quad \\text{for } \\eta \\in [\\eta_{\\min}, \\eta_{\\max}].\n$$\n\nStarting from the definitions of probability, counting, and the specified loss function, you must:\n- Compute the fraction of grid points that fall in the dangerous region, i.e., $\\eta_i  \\eta_c$.\n- Compute the expected fraction of random samples (equivalently, the probability mass) in the dangerous region under log-uniform sampling.\n- Compute the difference between these fractions (grid minus random).\n- Compute the mean grid loss $\\frac{1}{G} \\sum_{i=0}^{G-1} L(\\eta_i)$.\n- Compute the expected loss for a single random draw $\\mathbb{E}[L(\\eta)]$ under the log-uniform distribution.\n- Compute the expected number of dangerous exposures in $R$ independent random draws, i.e., $R$ times the dangerous fraction.\n\nYour program must implement these computations for the following test suite (each case specifies $(\\eta_{\\min}, \\eta_{\\max}, \\eta_c, G, R, L_0, \\eta_*, D)$), which is chosen to cover typical usage, boundary conditions, and edge cases:\n- Case $1$: $(10^{-5}, 1, 10^{-1}, 21, 100, 1, 10^{-2}, 10^{6})$.\n- Case $2$: $(10^{-4}, 10^{-1}, 2, 11, 50, 1, 10^{-2}, 10^{6})$.\n- Case $3$: $(10^{-3}, 1, 10^{-4}, 25, 200, 1, 5 \\times 10^{-2}, 10^{6})$.\n- Case $4$: $(10^{-3}, 2.01 \\times 10^{-1}, 1.01 \\times 10^{-1}, 21, 120, 1, 5.1 \\times 10^{-2}, 10^{6})$.\n\nFor each case, your program must produce six numbers in the following order:\n$[$grid-danger-fraction, random-danger-fraction, fraction-difference, grid-mean-loss, random-expected-loss, expected-random-danger-count$]$.\n\nAll fractions must be expressed as decimals (not using a percentage sign), and all outputs must be rounded to six decimal places.\n\nFinal output format requirement:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must aggregate the six-number results from all test cases in order, i.e., \n$$\n[\\text{c1\\_m1}, \\text{c1\\_m2}, \\ldots, \\text{c1\\_m6}, \\text{c2\\_m1}, \\ldots, \\text{c4\\_m6}],\n$$\nwhere $\\text{c}k\\_\\text{m}j$ denotes the $j$-th metric from case $k$.", "solution": "The problem is well-posed, scientifically grounded in the context of statistical learning, and contains all necessary information for a unique solution. We proceed by deriving the analytical or computational formulas for the six required metrics.\n\nLet the parameters for a given test case be $(\\eta_{\\min}, \\eta_{\\max}, \\eta_c, G, R, L_0, \\eta_*, D)$.\n\nThe validation loss function $L(\\eta)$ is defined as:\n$$\nL(\\eta) = \n\\begin{cases}\nL_0 + (\\eta - \\eta_*)^2,  \\text{if } \\eta \\le \\eta_c, \\\\\nD,  \\text{if } \\eta  \\eta_c,\n\\end{cases}\n$$\nThe dangerous region for the learning rate $\\eta$ is the interval $(\\eta_c, \\eta_{\\max}]$. The stable region is $[\\eta_{\\min}, \\eta_c]$.\n\n**1. Grid Danger Fraction**\n\nGrid search evaluates the model at $G$ discrete points, $\\eta_i$, given by:\n$$\n\\eta_i = \\eta_{\\min} + i \\cdot \\frac{\\eta_{\\max} - \\eta_{\\min}}{G - 1}, \\quad i \\in \\{0, 1, \\ldots, G-1\\}.\n$$\nA grid point $\\eta_i$ is dangerous if $\\eta_i  \\eta_c$. Let $N_{\\text{dangerous}}$ be the number of such points.\n$$\nN_{\\text{dangerous}} = \\sum_{i=0}^{G-1} \\mathbb{I}(\\eta_i  \\eta_c)\n$$\nwhere $\\mathbb{I}(\\cdot)$ is the indicator function, which is $1$ if the condition is true and $0$ otherwise.\nThe fraction of dangerous grid points, $F_{\\text{grid}}$, is then:\n$$\nF_{\\text{grid}} = \\frac{N_{\\text{dangerous}}}{G}.\n$$\nThis quantity is computed by generating all $G$ grid points and counting those that exceed $\\eta_c$.\n\n**2. Random Danger Fraction**\n\nRandom search draws $\\eta$ from a log-uniform distribution over $[\\eta_{\\min}, \\eta_{\\max}]$. The probability density function (PDF) is:\n$$\nf(\\eta) = \\frac{1}{\\eta \\ln(\\eta_{\\max}/\\eta_{\\min})}, \\quad \\text{for } \\eta \\in [\\eta_{\\min}, \\eta_{\\max}].\n$$\nThe fraction of dangerous random samples is the probability $P(\\eta  \\eta_c)$. This is found by integrating the PDF over the dangerous portion of the search space, which is $(\\eta_c, \\eta_{\\max}] \\cap [\\eta_{\\min}, \\eta_{\\max}]$. The integration range is therefore $[\\max(\\eta_{\\min}, \\eta_c), \\eta_{\\max}]$.\nLet $C = \\ln(\\eta_{\\max}/\\eta_{\\min})$. The probability, $F_{\\text{random}}$, is:\n$$\nF_{\\text{random}} = P(\\eta  \\eta_c) = \\int_{\\max(\\eta_{\\min}, \\eta_c)}^{\\eta_{\\max}} f(\\eta) \\, d\\eta.\n$$\nIf $\\max(\\eta_{\\min}, \\eta_c) \\ge \\eta_{\\max}$, the integral is over an empty set, and the probability is $0$. Otherwise:\n$$\nF_{\\text{random}} = \\int_{\\max(\\eta_{\\min}, \\eta_c)}^{\\eta_{\\max}} \\frac{1}{\\eta C} \\, d\\eta = \\frac{1}{C} [\\ln(\\eta)]_{\\max(\\eta_{\\min}, \\eta_c)}^{\\eta_{\\max}} = \\frac{\\ln(\\eta_{\\max}) - \\ln(\\max(\\eta_{\\min}, \\eta_c))}{\\ln(\\eta_{\\max}) - \\ln(\\eta_{\\min})}.\n$$\n\n**3. Fraction Difference**\n\nThis is the straightforward difference between the two fractions computed above:\n$$\n\\Delta_F = F_{\\text{grid}} - F_{\\text{random}}.\n$$\n\n**4. Grid Mean Loss**\n\nThe mean loss for the grid search strategy is the arithmetic mean of the loss function $L(\\eta_i)$ evaluated at each of the $G$ grid points:\n$$\n\\bar{L}_{\\text{grid}} = \\frac{1}{G} \\sum_{i=0}^{G-1} L(\\eta_i).\n$$\nThe computation involves iterating through the generated grid points $\\eta_i$, evaluating $L(\\eta_i)$ according to its piecewise definition, summing the results, and dividing by $G$.\n\n**5. Random Expected Loss**\n\nThe expected loss for the random search strategy, $\\mathbb{E}[L(\\eta)]$, is found by integrating the product of the loss function $L(\\eta)$ and the PDF $f(\\eta)$ over the search space $[\\eta_{\\min}, \\eta_{\\max}]$:\n$$\n\\mathbb{E}[L(\\eta)] = \\int_{\\eta_{\\min}}^{\\eta_{\\max}} L(\\eta) f(\\eta) \\, d\\eta.\n$$\nWe split the integral into stable and dangerous regions based on the piecewise definition of $L(\\eta)$. The split point is $\\eta_c$. Let the stable region of integration be $S = [\\eta_{\\min}, \\min(\\eta_{\\max}, \\eta_c)]$ and the dangerous region be $T=[\\max(\\eta_{\\min}, \\eta_c), \\eta_{\\max}]$.\n$$\n\\mathbb{E}[L(\\eta)] = \\int_{S} (L_0 + (\\eta - \\eta_*)^2) f(\\eta) \\, d\\eta + \\int_{T} D f(\\eta) \\, d\\eta.\n$$\nThe second integral (dangerous part) simplifies to $D \\cdot F_{\\text{random}}$.\nThe first integral (stable part) is:\n$$\n\\mathbb{E}[L(\\eta)]_{\\text{stable}} = \\frac{1}{C} \\int_{S} \\frac{L_0 + \\eta^2 - 2\\eta\\eta_* + \\eta_*^2}{\\eta} \\, d\\eta = \\frac{1}{C} \\int_{S} \\left(\\frac{L_0 + \\eta_*^2}{\\eta} + \\eta - 2\\eta_*\\right) \\, d\\eta.\n$$\nThe antiderivative is:\n$$\n\\mathcal{L}(\\eta) = \\frac{1}{C} \\left[ (L_0 + \\eta_*^2)\\ln(\\eta) + \\frac{\\eta^2}{2} - 2\\eta_*\\eta \\right].\n$$\nIf $\\eta_{\\min}  \\min(\\eta_{\\max}, \\eta_c)$, the stable contribution is $\\mathcal{L}(\\min(\\eta_{\\max}, \\eta_c)) - \\mathcal{L}(\\eta_{\\min})$. Otherwise, it is $0$.\nThe total expected loss is the sum of the stable and dangerous contributions.\n\n**6. Expected Random Danger Count**\n\nGiven $R$ independent random draws, the number of dangerous exposures follows a binomial distribution. The expected number of dangerous exposures is simply $R$ multiplied by the probability of a single draw being dangerous:\n$$\nE_{\\text{count}} = R \\cdot F_{\\text{random}}.\n$$\nThis concludes the derivation of all required metrics. We will now implement these calculations for the provided test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and prints the results for the hyperparameter tuning problem.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (eta_min, eta_max, eta_c, G, R, L0, eta_star, D)\n        (1e-5, 1.0, 1e-1, 21, 100, 1.0, 1e-2, 1e6),\n        (1e-4, 1e-1, 2.0, 11, 50, 1.0, 1e-2, 1e6),\n        (1e-3, 1.0, 1e-4, 25, 200, 1.0, 5e-2, 1e6),\n        (1e-3, 2.01e-1, 1.01e-1, 21, 120, 1.0, 5.1e-2, 1e6),\n    ]\n\n    all_results = []\n    for case in test_cases:\n        eta_min, eta_max, eta_c, G, R, L0, eta_star, D = case\n\n        # Metric 1: Grid Danger Fraction\n        grid_points = np.linspace(eta_min, eta_max, G)\n        dangerous_count_grid = np.sum(grid_points > eta_c)\n        grid_danger_fraction = dangerous_count_grid / G\n\n        # Metric 2: Random Danger Fraction\n        # Using np.log which is natural logarithm\n        log_eta_max = np.log(eta_max)\n        log_eta_min = np.log(eta_min)\n        log_range_const = log_eta_max - log_eta_min\n\n        random_danger_fraction = 0.0\n        # The lower bound for integrating over the dangerous region\n        eta_d_lower = max(eta_min, eta_c)\n        if eta_d_lower  eta_max and log_range_const > 0:\n            random_danger_fraction = (log_eta_max - np.log(eta_d_lower)) / log_range_const\n\n        # Metric 3: Fraction Difference\n        fraction_difference = grid_danger_fraction - random_danger_fraction\n\n        # Metric 4: Grid Mean Loss\n        grid_losses = []\n        for eta in grid_points:\n            if eta > eta_c:\n                loss = D\n            else:\n                loss = L0 + (eta - eta_star)**2\n            grid_losses.append(loss)\n        grid_mean_loss = np.mean(grid_losses)\n\n        # Metric 5: Random Expected Loss\n        exp_loss_dangerous = D * random_danger_fraction\n\n        # Stable part of the expectation\n        exp_loss_stable = 0.0\n        # The upper bound for integrating over the stable region\n        eta_s_upper = min(eta_max, eta_c)\n        \n        if eta_s_upper > eta_min and log_range_const > 0:\n            # Antiderivative of the stable part integrand, evaluated at a point eta\n            def F(eta):\n                log_eta = np.log(eta)\n                term1 = (L0 + eta_star**2) * log_eta\n                term2 = eta**2 / 2.0\n                term3 = -2.0 * eta_star * eta\n                return (term1 + term2 + term3) / log_range_const\n\n            exp_loss_stable = F(eta_s_upper) - F(eta_min)\n\n        random_expected_loss = exp_loss_stable + exp_loss_dangerous\n        \n        # Metric 6: Expected Random Danger Count\n        expected_random_danger_count = R * random_danger_fraction\n        \n        case_results = [\n            grid_danger_fraction,\n            random_danger_fraction,\n            fraction_difference,\n            grid_mean_loss,\n            random_expected_loss,\n            expected_random_danger_count\n        ]\n        \n        all_results.extend(case_results)\n\n    # Format the final output string, rounding each value to 6 decimal places.\n    formatted_results = [f\"{val:.6f}\" for val in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3129497"}]}