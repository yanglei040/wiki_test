{"hands_on_practices": [{"introduction": "To build a solid foundation, we first consider the simplest possible scenario: a classifier with no features, relying only on a bias term $b$. This exercise [@problem_id:3108628] reveals the intrinsic behaviors of cross-entropy and hinge loss in response to class imbalance. By deriving the optimal bias for each loss function from first principles, you will discover how cross-entropy naturally aligns the model's baseline output with the class prior probabilities, a crucial property for calibrated models.", "problem": "Consider a binary classification task with independent and identically distributed samples and class labels. Suppose that the feature vectors carry no information, so the modelâ€™s linear score reduces to a constant bias term $b$, that is, $g(x) = b$ for all inputs $x$. The dataset consists of $n_{+} = 20$ samples from the positive class and $n_{-} = 80$ samples from the negative class. Let $\\pi_{+}$ and $\\pi_{-}$ denote the class-prior probabilities, estimated by the empirical frequencies $\\pi_{+} = \\frac{n_{+}}{n_{+}+n_{-}}$ and $\\pi_{-} = \\frac{n_{-}}{n_{+}+n_{-}}$.\n\nStarting from the fundamental definitions of loss functions and probabilistic decision rules:\n\n- For cross-entropy (also called logistic) loss for binary labels $y \\in \\{0,1\\}$, the predicted probability of the positive class is $p = \\sigma(b)$, where $\\sigma(z) = \\frac{1}{1+\\exp(-z)}$ is the logistic function. The empirical cross-entropy loss is the negative log-likelihood of independent Bernoulli trials, $L_{\\mathrm{CE}}(b) = -\\sum_{i=1}^{n} \\left( y_{i} \\ln(\\sigma(b)) + (1-y_{i}) \\ln(1-\\sigma(b)) \\right)$.\n- For hinge loss with labels $y \\in \\{-1,+1\\}$, the empirical hinge loss for the linear score $g(x)=b$ is $L_{\\mathrm{hinge}}(b) = \\sum_{i=1}^{n} \\max\\!\\left(0,\\,1 - y_{i} b\\right)$.\n- Under $0$-$1$ loss and equal misclassification costs, the Bayes decision rule chooses the class that maximizes the posterior probability. In the absence of informative features (so that the class-conditional likelihoods are equal for all $x$), this decision reduces to a threshold on a calibrated score representing the log-odds of the positive class. A threshold value $\\tau$ that equates the posterior probabilities satisfies $\\sigma(\\tau) = \\pi_{+}$, so that $\\tau$ matches the log-odds of the class priors.\n\nUsing only these foundations, perform the following:\n1. Derive the value $b_{\\mathrm{CE}}^{\\star}$ that minimizes $L_{\\mathrm{CE}}(b)$.\n2. Derive the value $b_{\\mathrm{hinge}}^{\\star}$ that minimizes $L_{\\mathrm{hinge}}(b)$.\n3. Derive the Bayes-optimal threshold $\\tau^{\\star}$ on the score $g(x)=b$ under $0$-$1$ loss with equal misclassification costs, expressed in terms of $\\pi_{+}$ and $\\pi_{-}$.\n\nProvide the final results for $\\left(b_{\\mathrm{CE}}^{\\star},\\,b_{\\mathrm{hinge}}^{\\star},\\,\\tau^{\\star}\\right)$ as exact analytic expressions. No rounding is required. Your final answer must be a single expression.", "solution": "The problem requires the derivation of optimal bias parameters for three different loss function frameworks in a binary classification setting where the model score is a constant bias term, $g(x) = b$. We are given $n_{+} = 20$ positive samples and $n_{-} = 80$ negative samples. The corresponding empirical class-prior probabilities are $\\pi_{+} = \\frac{n_{+}}{n_{+}+n_{-}} = \\frac{20}{100} = \\frac{1}{5}$ and $\\pi_{-} = \\frac{n_{-}}{n_{+}+n_{-}} = \\frac{80}{100} = \\frac{4}{5}$.\n\n### 1. Minimization of Cross-Entropy Loss\n\nThe empirical cross-entropy loss is given for labels $y \\in \\{0, 1\\}$, where the positive class is labeled $y=1$ and the negative class is labeled $y=0$. The total loss is a sum over all $n = n_{+} + n_{-}$ samples:\n$$L_{\\mathrm{CE}}(b) = -\\sum_{i=1}^{n} \\left( y_{i} \\ln(\\sigma(b)) + (1-y_{i}) \\ln(1-\\sigma(b)) \\right)$$\nwhere $\\sigma(b) = \\frac{1}{1+\\exp(-b)}$ is the logistic function.\n\nWe can separate the sum into contributions from the $n_{+}$ positive samples (where $y_i=1$) and the $n_{-}$ negative samples (where $y_i=0$):\n$$L_{\\mathrm{CE}}(b) = - n_{+} \\left( 1 \\cdot \\ln(\\sigma(b)) + (1-1) \\ln(1-\\sigma(b)) \\right) - n_{-} \\left( 0 \\cdot \\ln(\\sigma(b)) + (1-0) \\ln(1-\\sigma(b)) \\right)$$\n$$L_{\\mathrm{CE}}(b) = - n_{+} \\ln(\\sigma(b)) - n_{-} \\ln(1-\\sigma(b))$$\nTo find the value of $b$ that minimizes this loss, $b_{\\mathrm{CE}}^{\\star}$, we compute the derivative of $L_{\\mathrm{CE}}(b)$ with respect to $b$ and set it to zero. We use the following derivative properties: $\\frac{d}{db}\\sigma(b) = \\sigma(b)(1-\\sigma(b))$, $\\frac{d}{db}\\ln(\\sigma(b)) = 1-\\sigma(b)$, and $\\frac{d}{db}\\ln(1-\\sigma(b)) = -\\sigma(b)$.\n\nThe derivative of the loss function is:\n$$\\frac{dL_{\\mathrm{CE}}}{db} = - n_{+} (1-\\sigma(b)) - n_{-} (-\\sigma(b)) = -n_{+} + n_{+}\\sigma(b) + n_{-}\\sigma(b)$$\n$$\\frac{dL_{\\mathrm{CE}}}{db} = -n_{+} + (n_{+}+n_{-})\\sigma(b)$$\nSetting the derivative to zero to find the critical point $b_{\\mathrm{CE}}^{\\star}$:\n$$-n_{+} + (n_{+}+n_{-})\\sigma(b_{\\mathrm{CE}}^{\\star}) = 0$$\n$$\\sigma(b_{\\mathrm{CE}}^{\\star}) = \\frac{n_{+}}{n_{+}+n_{-}} = \\pi_{+}$$\nThis result shows that the optimal predicted probability matches the empirical frequency of the positive class. To find $b_{\\mathrm{CE}}^{\\star}$, we solve for $b$ in the equation $\\sigma(b) = \\pi_{+}$:\n$$\\frac{1}{1+\\exp(-b_{\\mathrm{CE}}^{\\star})} = \\pi_{+}$$\n$$1 = \\pi_{+}(1+\\exp(-b_{\\mathrm{CE}}^{\\star}))$$\n$$\\frac{1}{\\pi_{+}} - 1 = \\exp(-b_{\\mathrm{CE}}^{\\star})$$\n$$\\frac{1-\\pi_{+}}{\\pi_{+}} = \\exp(-b_{\\mathrm{CE}}^{\\star})$$\nSince $1-\\pi_{+} = \\pi_{-}$, we have:\n$$\\frac{\\pi_{-}}{\\pi_{+}} = \\exp(-b_{\\mathrm{CE}}^{\\star})$$\nTaking the natural logarithm of both sides:\n$$-b_{\\mathrm{CE}}^{\\star} = \\ln\\left(\\frac{\\pi_{-}}{\\pi_{+}}\\right)$$\n$$b_{\\mathrm{CE}}^{\\star} = -\\ln\\left(\\frac{\\pi_{-}}{\\pi_{+}}\\right) = \\ln\\left(\\left(\\frac{\\pi_{-}}{\\pi_{+}}\\right)^{-1}\\right) = \\ln\\left(\\frac{\\pi_{+}}{\\pi_{-}}\\right)$$\nThe second derivative, $\\frac{d^2L_{\\mathrm{CE}}}{db^2} = (n_{+}+n_{-})\\sigma(b)(1-\\sigma(b))$, is positive for all $b$, confirming that the loss function is convex and $b_{\\mathrm{CE}}^{\\star}$ is a global minimum.\n\nSubstituting the given values, $\\pi_{+} = \\frac{1}{5}$ and $\\pi_{-} = \\frac{4}{5}$:\n$$b_{\\mathrm{CE}}^{\\star} = \\ln\\left(\\frac{1/5}{4/5}\\right) = \\ln\\left(\\frac{1}{4}\\right) = -\\ln(4)$$\n\n### 2. Minimization of Hinge Loss\n\nThe empirical hinge loss is given for labels $y \\in \\{-1, +1\\}$, where the positive class is $y=+1$ and the negative class is $y=-1$. The total loss is:\n$$L_{\\mathrm{hinge}}(b) = \\sum_{i=1}^{n} \\max(0, 1 - y_{i} b)$$\nWe separate the sum over the $n_{+}$ positive samples ($y_i=+1$) and $n_{-}$ negative samples ($y_i=-1$):\n$$L_{\\mathrm{hinge}}(b) = n_{+} \\max(0, 1 - b) + n_{-} \\max(0, 1 + b)$$\nThis function is convex and piecewise linear, with non-differentiable points at $b=-1$ and $b=1$. We analyze the subgradient of $L_{\\mathrm{hinge}}(b)$ to find the minimum. The minimum $b_{\\mathrm{hinge}}^{\\star}$ must satisfy the condition $0 \\in \\partial L_{\\mathrm{hinge}}(b_{\\mathrm{hinge}}^{\\star})$.\n\nThe subgradient is the sum of the subgradients of the individual terms:\n- For $b \\in (-1, 1)$, $\\partial L_{\\mathrm{hinge}}(b) = \\{ -n_{+} + n_{-} \\}$.\n- For $b = -1$, $\\partial L_{\\mathrm{hinge}}(b) = \\{-n_{+}\\} + n_{-}[0, 1] = [-n_{+}, -n_{+} + n_{-}]$.\n- For $b = 1$, $\\partial L_{\\mathrm{hinge}}(b) = n_{+}[-1, 0] + \\{n_{-}\\} = [-n_{+} + n_{-}, n_{-}]$.\n\nWe are given $n_{+} = 20$ and $n_{-} = 80$. Thus, $n_{-}  n_{+}$.\n- For $b \\in (-1, 1)$, the derivative is $n_{-} - n_{+} = 80 - 20 = 60 \\neq 0$. The minimum is not in this interval.\n- For $b = -1$, the subgradient interval is $[-n_{+}, -n_{+} + n_{-}] = [-20, 60]$. Since $0 \\in [-20, 60]$, $b = -1$ is a minimizer.\n- For $b = 1$, the subgradient interval is $[-n_{+} + n_{-}, n_{-}] = [60, 80]$. Since $0 \\notin [60, 80]$, $b=1$ is not a minimizer.\nBecause the function is convex, the point $b_{\\mathrm{hinge}}^{\\star} = -1$ where the subgradient contains zero is the global minimum.\n\n### 3. Bayes-Optimal Threshold under 0-1 Loss\n\nThe problem states that under 0-1 loss with equal misclassification costs and non-informative features, the decision can be based on a threshold $\\tau$ on a calibrated score. The problem defines the Bayes-optimal threshold $\\tau^{\\star}$ as the value that satisfies the condition:\n$$\\sigma(\\tau^{\\star}) = \\pi_{+}$$\nThis condition means that the probability associated with the threshold itself (via the logistic function) is equal to the prior probability of the positive class. The problem notes that this implies $\\tau^{\\star}$ is the log-odds of the class priors. We can derive this directly by solving for $\\tau^{\\star}$:\n$$\\frac{1}{1 + \\exp(-\\tau^{\\star})} = \\pi_{+}$$\nThis is the same equation we solved for $b_{\\mathrm{CE}}^{\\star}$. The solution is therefore identical:\n$$\\tau^{\\star} = \\ln\\left(\\frac{\\pi_{+}}{\\pi_{-}}\\right)$$\nThe logic is that for a score calibrated to be the log-odds of the posterior, $g(x)=\\ln\\left(\\frac{P(C_+|x)}{P(C_-|x)}\\right)$, the optimal decision boundary for 0-1 loss is at $g(x)=0$. In our featureless case, the score is constant and equals the log-odds of the priors, $g(x) = \\ln(\\pi_+/\\pi_-)$. A threshold of $\\tau = \\ln(\\pi_+/\\pi_-)$ on a score of $b=g(x)$ corresponds to asking if $b  \\tau$, which would not be a useful decision rule since both are constant. The problem's framing defines $\\tau^*$ as the value of the log-odds of the priors itself.\n\nUsing the given values:\n$$\\tau^{\\star} = \\ln\\left(\\frac{1/5}{4/5}\\right) = \\ln\\left(\\frac{1}{4}\\right) = -\\ln(4)$$\n\n### Summary of Results\nThe derived values are:\n1. $b_{\\mathrm{CE}}^{\\star} = -\\ln(4)$\n2. $b_{\\mathrm{hinge}}^{\\star} = -1$\n3. $\\tau^{\\star} = -\\ln(4)$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n-\\ln(4)  -1  -\\ln(4)\n\\end{pmatrix}\n}\n$$", "id": "3108628"}, {"introduction": "On a cleanly separable dataset, when should a classifier stop learning? This practice explores the fundamental difference between the optimization goals of hinge loss and cross-entropy. You will demonstrate through a hands-on coding exercise [@problem_id:3108625] that hinge loss is content once a functional margin of $1$ is met, whereas cross-entropy relentlessly pushes to maximize the margin. By evaluating the resulting classifiers on a test set, you will directly observe how this margin-maximizing behavior often leads to superior generalization and lower test error.", "problem": "Consider binary classification with labels $y \\in \\{-1,+1\\}$ and a linear score function $f_{\\boldsymbol{w},b}(\\boldsymbol{x}) = \\boldsymbol{w}^{\\top}\\boldsymbol{x} + b$. The empirical $0$-$1$ classification error on a dataset $\\{(\\boldsymbol{x}_i,y_i)\\}_{i=1}^n$ is defined as the fraction of indices $i$ such that $y_i f_{\\boldsymbol{w},b}(\\boldsymbol{x}_i) \\le 0$. The hinge loss is defined as $\\ell_{\\text{hinge}}(y, f) = \\max\\{0, 1 - y f\\}$, and the cross-entropy (logistic) loss for binary labels is defined as $\\ell_{\\text{ce}}(y, f) = \\log\\big(1 + \\exp(-y f)\\big)$. The (geometric) margin of a linear classifier on a dataset is defined as\n$$\n\\gamma(\\boldsymbol{w},b;\\{\\boldsymbol{x}_i,y_i\\}) = \\min_{i} \\frac{y_i(\\boldsymbol{w}^{\\top}\\boldsymbol{x}_i + b)}{\\|\\boldsymbol{w}\\|_2}.\n$$\nStarting only from these core definitions, construct and analyze an explicit example that illustrates the following phenomenon:\n- When training with hinge loss alone (no regularization), attaining $y_i f_{\\boldsymbol{w},b}(\\boldsymbol{x}_i) \\ge 1$ for all training points yields zero hinge loss, which can occur at a relatively small geometric margin.\n- When training with cross-entropy, the loss continues to incentivize increasing $y_i f_{\\boldsymbol{w},b}(\\boldsymbol{x}_i)$ beyond $1$, effectively pushing toward larger geometric margins.\n\nYour program must instantiate a reproducible, linearly separable training dataset and a broader test distribution, then quantify how the empirical test $0$-$1$ risk varies as a function of the achieved geometric margin for a family of linear classifiers that keep a fixed direction and translate the decision boundary to meet prescribed margin targets. Proceed as follows.\n\n1. Generate a training dataset of $n=16$ points in $\\mathbb{R}^2$, with exactly $8$ positive examples drawn independently from a Normal distribution with mean $\\boldsymbol{\\mu}_+ = (3,3)$ and covariance matrix $\\sigma_{\\text{train}}^2 \\boldsymbol{I}$, and $8$ negative examples drawn independently from a Normal distribution with mean $\\boldsymbol{\\mu}_- = (-3,-3)$ and the same covariance. Use $\\sigma_{\\text{train}} = 0.3$ and a fixed pseudorandom seed to ensure reproducibility.\n\n2. Generate a test dataset of $N=2000$ points in $\\mathbb{R}^2$, with exactly $1000$ positive examples drawn independently from a Normal distribution with mean $\\boldsymbol{\\mu}_+ = (3,3)$ and covariance matrix $\\sigma_{\\text{test}}^2 \\boldsymbol{I}$, and $1000$ negative examples drawn independently from a Normal distribution with mean $\\boldsymbol{\\mu}_- = (-3,-3)$ and the same covariance. Use $\\sigma_{\\text{test}} = 1.2$ and the same fixed pseudorandom seed.\n\n3. Fix the direction vector $\\boldsymbol{v} = \\frac{1}{\\sqrt{2}}(1,1)$ and set $\\|\\boldsymbol{v}\\|_2 = 1$. For the training set, compute the projections $p_i = \\boldsymbol{v}^{\\top}\\boldsymbol{x}_i$ for all points. Let $p_{\\min}^+ = \\min\\{\\boldsymbol{v}^{\\top}\\boldsymbol{x}_i : y_i = +1\\}$ and $p_{\\max}^- = \\max\\{\\boldsymbol{v}^{\\top}\\boldsymbol{x}_i : y_i = -1\\}$, and define the projection gap $g = p_{\\min}^+ - p_{\\max}^-$. This gap is strictly positive for a linearly separable sample along $\\boldsymbol{v}$. The maximum feasible geometric margin along the direction $\\boldsymbol{v}$ while classifying the training set correctly is then $m_{\\max} = \\frac{g}{2}$.\n\n4. For any target geometric margin $m$ with $0  m \\le m_{\\max}$, construct the linear classifier with weight $\\boldsymbol{w} = \\boldsymbol{v}$ and bias\n$$\nb(m) = m - p_{\\min}^+,\n$$\nso that the closest positive training point achieves margin exactly $m$. Verify that for all training points, $y_i(\\boldsymbol{w}^{\\top}\\boldsymbol{x}_i + b(m)) \\ge m$, hence the empirical hinge loss is zero whenever $m \\ge 1$. This explicitly shows an instance where hinge loss can achieve zero training error at a relatively small geometric margin (e.g., $m$ just above $1$), while cross-entropy would keep encouraging increases in $m$.\n\n5. Compute the empirical test $0$-$1$ risk as the fraction of test points $(\\boldsymbol{x},y)$ for which $y \\cdot (\\boldsymbol{v}^{\\top}\\boldsymbol{x} + b(m)) \\le 0$, for each target margin $m$ in the following test suite:\n- $m = 1.0$ (boundary condition for zero hinge loss with unit-norm $\\boldsymbol{w}$),\n- $m = 1.5$ (small margin case),\n- $m = 2.5$ (moderate margin case),\n- $m = 3.5$ (larger margin case),\n- $m = m_{\\max} - 0.1$ (near-maximum feasible margin case).\n\nOnly margins $m$ not exceeding $m_{\\max}$ are feasible by construction; for this dataset, $m_{\\max}$ will be sufficiently large to make all listed margins feasible.\n\n6. Your program should produce a single line of output containing the empirical test $0$-$1$ risks for the margins in the specified order as a comma-separated list enclosed in square brackets (for example, $[r_{1.0},r_{1.5},r_{2.5},r_{3.5},r_{m_{\\max}-0.1}]$), where each $r_m$ is a decimal representing the fraction of misclassified test examples.\n\nThe objective is to demonstrate, from first principles, how zero hinge loss can be achieved at a small geometric margin and how increasing the margin (as cross-entropy training would encourage) affects the empirical test $0$-$1$ risk for this explicit family of classifiers, all while keeping the direction fixed and translating the decision boundary via the bias to realize a prescribed margin.", "solution": "The problem statement is valid. It presents a well-defined, scientifically sound, and computationally verifiable exercise in statistical learning theory. The task is to construct and analyze a specific numerical example to illustrate the differing behaviors of hinge loss and cross-entropy loss concerning the geometric margin in binary classification. The setup is complete, consistent, and poses a clear, non-trivial question that can be answered through the prescribed step-by-step procedure.\n\nThe core principle to be demonstrated is that minimizing the hinge loss can terminate once a functional margin of $1$ is achieved for all data points, potentially resulting in a suboptimal geometric margin and poorer generalization. In contrast, minimizing the cross-entropy loss continuously incentivizes an increase in the margin, which often leads to better generalization. We will now proceed with the formal construction and analysis.\n\nFirst, we establish the environment for our experiment. We work with a binary classification problem where labels are $y \\in \\{-1, +1\\}$ and our classifier uses a linear score function $f_{\\boldsymbol{w},b}(\\boldsymbol{x}) = \\boldsymbol{w}^{\\top}\\boldsymbol{x} + b$.\n\n**1. Data Generation**\n\nAs specified, we generate a training set and a test set. A fixed pseudorandom seed of $42$ will be used to ensure the reproducibility of the results.\n\nThe training dataset, $\\{(\\boldsymbol{x}_i,y_i)\\}_{i=1}^n$, consists of $n=16$ points in $\\mathbb{R}^2$.\n- $8$ positive examples ($y_i = +1$) are drawn from a bivariate Normal distribution $\\mathcal{N}(\\boldsymbol{\\mu}_+, \\sigma_{\\text{train}}^2 \\boldsymbol{I})$, with mean $\\boldsymbol{\\mu}_+ = (3,3)$ and covariance $\\sigma_{\\text{train}}^2 \\boldsymbol{I}$ where $\\sigma_{\\text{train}} = 0.3$.\n- $8$ negative examples ($y_i = -1$) are drawn from $\\mathcal{N}(\\boldsymbol{\\mu}_-, \\sigma_{\\text{train}}^2 \\boldsymbol{I})$, with mean $\\boldsymbol{\\mu}_- = (-3,-3)$ and the same covariance.\nThe small variance $\\sigma_{\\text{train}}^2$ ensures that the training data will be linearly separable with high probability.\n\nThe test dataset, consisting of $N=2000$ points, is generated from a broader distribution to assess generalization.\n- $1000$ positive examples are drawn from $\\mathcal{N}(\\boldsymbol{\\mu}_+, \\sigma_{\\text{test}}^2 \\boldsymbol{I})$, where $\\sigma_{\\text{test}} = 1.2$.\n- $1000$ negative examples are drawn from $\\mathcal{N}(\\boldsymbol{\\mu}_-, \\sigma_{\\text{test}}^2 \\boldsymbol{I})$, with the same $\\sigma_{\\text{test}} = 1.2$.\nThe larger variance $\\sigma_{\\text{test}}^2$ creates significant overlap between the class distributions, making the classification task on the test set non-trivial.\n\n**2. Construction of the Classifier Family**\n\nTo isolate the effect of the margin, we fix the orientation of the separating hyperplane and only vary its position. We choose a direction vector $\\boldsymbol{v} = \\frac{1}{\\sqrt{2}}(1,1)$, which is the optimal direction connecting the centers of the two class distributions. We fix the classifier's weight vector to be $\\boldsymbol{w} = \\boldsymbol{v}$. Since $\\|\\boldsymbol{v}\\|_2 = 1$, the geometric margin for a point $(\\boldsymbol{x}_i, y_i)$ simplifies from its general definition, $\\gamma_i = \\frac{y_i(\\boldsymbol{w}^{\\top}\\boldsymbol{x}_i + b)}{\\|\\boldsymbol{w}\\|_2}$, to become $\\gamma_i = y_i(\\boldsymbol{v}^{\\top}\\boldsymbol{x}_i + b)$. This is also known as the functional margin.\n\nThe problem effectively becomes one-dimensional by projecting the data points $\\boldsymbol{x}_i$ onto the direction $\\boldsymbol{v}$. Let $p_i = \\boldsymbol{v}^{\\top}\\boldsymbol{x}_i$. For the training set to be linearly separable by a hyperplane with normal vector $\\boldsymbol{v}$, the projected positive and negative points must not overlap. We define:\n- $p_{\\min}^+ = \\min_{i: y_i = +1} p_i$, the smallest projection for a positive training point.\n- $p_{\\max}^- = \\max_{i: y_i = -1} p_i$, the largest projection for a negative training point.\n\nThe separability condition is $p_{\\min}^+  p_{\\max}^-$. The space between these points is the projection gap, $g = p_{\\min}^+ - p_{\\max}^-$.\nThe maximum possible geometric margin for a classifier with direction $\\boldsymbol{v}$ on this training set is achieved by placing the decision boundary at the midpoint of this gap. The distance from this midpoint to either $p_{\\min}^+$ or $p_{\\max}^-$ is $\\frac{g}{2}$. Thus, the maximum feasible geometric margin is $m_{\\max} = \\frac{g}{2}$.\n\nWe now construct a family of classifiers parameterized by a target margin $m$, where $0  m \\le m_{\\max}$. For each $m$, the classifier is defined by $(\\boldsymbol{w}, b(m))$ where $\\boldsymbol{w}=\\boldsymbol{v}$ and the bias $b(m)$ is chosen to ensure the minimum geometric margin on the training set is exactly $m$. We set the bias such that the positive training point closest to the boundary (the one defining $p_{\\min}^+$) has a margin of exactly $m$.\nThe score for this point is $\\boldsymbol{v}^{\\top}\\boldsymbol{x} + b(m) = p_{\\min}^+ + b(m)$. Since its label is $y=+1$, we require its margin to be $1 \\cdot (p_{\\min}^+ + b(m)) = m$. Solving for the bias gives:\n$$\nb(m) = m - p_{\\min}^+\n$$\nLet's verify this choice. For any positive training point $(\\boldsymbol{x}_i, y_i=+1)$, the margin is $y_i(\\boldsymbol{v}^{\\top}\\boldsymbol{x}_i + b(m)) = 1 \\cdot (p_i + m - p_{\\min}^+) \\ge p_{\\min}^+ + m - p_{\\min}^+ = m$.\nFor any negative training point $(\\boldsymbol{x_j}, y_j=-1)$, the margin is $y_j(\\boldsymbol{v}^{\\top}\\boldsymbol{x}_j + b(m)) = -1 \\cdot (p_j + m - p_{\\min}^+) = p_{\\min}^+ - p_j - m$. Since $p_j \\le p_{\\max}^-$, this margin is at least $p_{\\min}^+ - p_{\\max}^- - m = g - m$. For the margin to be at least $m$, we need $g - m \\ge m$, which implies $g \\ge 2m$, or $m \\le \\frac{g}{2} = m_{\\max}$.\nThis confirms that for any $m \\le m_{\\max}$, our classifier construction guarantees a geometric margin of at least $m$ for all training points.\n\n**3. Loss Functions and Margin**\n\nThe hinge loss is defined as $\\ell_{\\text{hinge}}(y, f) = \\max\\{0, 1 - yf\\}$. For our classifier, $yf = y(\\boldsymbol{v}^{\\top}\\boldsymbol{x} + b(m))$, which is the geometric margin. The minimum margin on the training set is $m$. Therefore, for every training point, the margin is $\\ge m$. If we choose $m \\ge 1$, the hinge loss becomes $\\max\\{0, 1 - (\\text{margin})\\} \\le \\max\\{0, 1-1\\} = 0$. This means that any classifier in our family with $m \\ge 1$ achieves zero empirical hinge loss on the training set. A training algorithm based solely on minimizing hinge loss would have no incentive to increase the margin beyond $m=1$.\n\nThe cross-entropy loss is $\\ell_{\\text{ce}}(y, f) = \\log(1 + \\exp(-yf))$. The term $-yf$ is the negative of the geometric margin. As the margin $m$ increases, $-yf$ becomes more negative, $\\exp(-yf)$ approaches $0$, and $\\log(1 + \\exp(-yf))$ also approaches $0$. Since $\\log(1+z)$ is strictly increasing for $z0$, the loss is a strictly decreasing function of the margin. Even if the margin is already greater than $1$, the loss can be further reduced by increasing it. Thus, training with cross-entropy loss encourages pushing the margin as large as possible, toward $m_{\\max}$.\n\n**4. Empirical Evaluation on the Test Set**\n\nWe will now quantify the practical consequence of this difference by computing the $0$-$1$ classification risk on the test set. The $0$-$1$ risk is the fraction of test points $(\\boldsymbol{x}, y)$ that are misclassified. A misclassification occurs when the sign of the score does not match the label, i.e., $y(\\boldsymbol{v}^{\\top}\\boldsymbol{x} + b(m)) \\le 0$. We compute this risk for the following set of target margins: $m \\in \\{1.0, 1.5, 2.5, 3.5, m_{\\max}-0.1\\}$. We expect that increasing the margin from the hinge-loss-sufficient value of $m=1.0$ will improve generalization and decrease the test error, illustrating the benefit of the behavior encouraged by cross-entropy loss. The provided code implements this computation.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Constructs an example to illustrate the relationship between geometric margin,\n    loss functions (hinge vs. cross-entropy), and generalization error.\n    \"\"\"\n    # 0. Define problem parameters and set seed for reproducibility.\n    seed = 42\n    rng = np.random.default_rng(seed)\n\n    mu_pos = np.array([3, 3])\n    mu_neg = np.array([-3, -3])\n\n    n_train_per_class = 8\n    n_train = 2 * n_train_per_class\n    sigma_train = 0.3\n\n    n_test_per_class = 1000\n    n_test = 2 * n_test_per_class\n    sigma_test = 1.2\n\n    # 1. Generate training dataset\n    train_cov = sigma_train**2 * np.identity(2)\n    train_x_pos = rng.multivariate_normal(mu_pos, train_cov, size=n_train_per_class)\n    train_x_neg = rng.multivariate_normal(mu_neg, train_cov, size=n_train_per_class)\n    \n    train_X = np.vstack((train_x_pos, train_x_neg))\n    train_y = np.hstack((np.ones(n_train_per_class), -np.ones(n_train_per_class)))\n\n    # 2. Generate test dataset\n    test_cov = sigma_test**2 * np.identity(2)\n    test_x_pos = rng.multivariate_normal(mu_pos, test_cov, size=n_test_per_class)\n    test_x_neg = rng.multivariate_normal(mu_neg, test_cov, size=n_test_per_class)\n\n    test_X = np.vstack((test_x_pos, test_x_neg))\n    test_y = np.hstack((np.ones(n_test_per_class), -np.ones(n_test_per_class)))\n\n    # 3. Fix direction and compute maximum feasible margin on the training set\n    v = np.array([1, 1]) / np.sqrt(2) # Unit-norm direction vector\n\n    # Project training points onto the direction v\n    projections = train_X @ v\n    p_pos = projections[train_y == 1]\n    p_neg = projections[train_y == -1]\n\n    p_min_pos = np.min(p_pos)\n    p_max_neg = np.max(p_neg)\n\n    # Calculate the projection gap and maximum margin\n    projection_gap = p_min_pos - p_max_neg\n    m_max = projection_gap / 2.0\n    \n    # 4. Define the set of target margins to test\n    margins_to_test = [\n        1.0,\n        1.5,\n        2.5,\n        3.5,\n        m_max - 0.1\n    ]\n\n    results = []\n\n    # 5. Compute test risk for each target margin\n    for m in margins_to_test:\n        if m > m_max:\n            # This case should not happen given the problem parameters,\n            # but it is a good sanity check.\n            raise ValueError(f\"Target margin m={m} exceeds maximum feasible margin m_max={m_max}\")\n\n        # Construct the classifier (w=v, b(m))\n        # Bias b(m) is set to achieve margin exactly m for the closest positive point\n        b_m = m - p_min_pos\n        w = v\n\n        # Calculate scores on the test set: f(x) = w^T x + b\n        scores = test_X @ w + b_m\n\n        # A point is misclassified if y * f(x) = 0\n        y_times_f = test_y * scores\n        misclassified_count = np.sum(y_times_f = 0)\n\n        # Compute empirical 0-1 risk\n        risk = misclassified_count / n_test\n        results.append(risk)\n\n    # 6. Print results in the specified format\n    print(f\"[{','.join(f'{r:.4f}' for r in results)}]\")\n\nsolve()\n```", "id": "3108625"}, {"introduction": "Real-world datasets are often noisy and contain outliers that can mislead a learning algorithm. This exercise investigates the critical concept of robustness by analyzing how different loss functions react to an extreme, mislabeled data point. You will implement and compare the performance of standard cross-entropy and hinge losses against their bounded, more robust counterparts [@problem_id:3108631], gaining practical insight into why unbounded losses can be fragile and how robust alternatives provide a safeguard against such data corruption.", "problem": "Consider binary classification in two dimensions with labels in $\\{-1,+1\\}$. A linear classifier is $f(x)=w^\\top x + b$ with parameter vector $w \\in \\mathbb{R}^2$ and bias $b \\in \\mathbb{R}$. For a sample $(x_i,y_i)$ define the signed margin $u_i = y_i f(x_i)$. The empirical risk with $\\ell_2$ regularization on $w$ is\n$$\nR(w,b) = \\frac{1}{n}\\sum_{i=1}^n L(u_i) + \\lambda \\lVert w \\rVert_2^2,\n$$\nwhere $L$ is a pointwise loss function and $\\lambda  0$ is the regularization strength. You will analyze how different losses respond to an extreme mislabeled outlier.\n\nUse the following fundamental base definitions of loss functions:\n- Cross-entropy (logistic) loss: $L_{\\text{log}}(u) = \\log\\left(1 + e^{-u}\\right)$.\n- Hinge loss: $L_{\\text{hinge}}(u) = \\max\\left(0, 1 - u\\right)$.\n- Ramp loss (a clipped hinge-type robust loss): $L_{\\text{ramp}}(u) = \\max\\left(0, \\min\\left(1 - u, 1\\right)\\right)$.\n- Truncated cross-entropy (capped logistic) loss with cap $\\tau$: $L_{\\text{tlog}}(u) = \\min\\left(\\log\\left(1 + e^{-u}\\right), \\tau\\right)$.\n\nTrain by full-batch gradient descent on $R(w,b)$ for a fixed number of iterations and step size, using the following exact gradient rules derived from the chain rule:\n- For cross-entropy, $\\frac{\\partial L_{\\text{log}}}{\\partial u} = -\\frac{1}{1+e^{u}}$.\n- For hinge, $\\frac{\\partial L_{\\text{hinge}}}{\\partial u} = -\\mathbb{I}\\{u  1\\}$ where $\\mathbb{I}\\{\\cdot\\}$ is the indicator function.\n- For ramp, $\\frac{\\partial L_{\\text{ramp}}}{\\partial u} = \\begin{cases} 0  \\text{if } u  0, \\\\ -1  \\text{if } 0 \\le u  1, \\\\ 0  \\text{if } u \\ge 1. \\end{cases}$\n- For truncated cross-entropy, $\\frac{\\partial L_{\\text{tlog}}}{\\partial u} = \\begin{cases} -\\frac{1}{1+e^{u}}  \\text{if } \\log(1+e^{-u})  \\tau, \\\\ 0  \\text{otherwise.} \\end{cases}$\n\nNote that $\\frac{\\partial L}{\\partial f} = \\frac{\\partial L}{\\partial u} \\cdot y$ and the gradient contributions per sample are $\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial f} \\cdot x$ and $\\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial f}$, with the regularization term contributing $2\\lambda w$ to $\\frac{\\partial R}{\\partial w}$ and $0$ to $\\frac{\\partial R}{\\partial b}$.\n\nDataset construction:\n- Fix a random seed for reproducibility.\n- Draw $n_+=20$ inliers for class $+1$ from a Gaussian with mean $(2,0)$ and covariance matrix $\\operatorname{diag}(0.25,0.25)$.\n- Draw $n_-=20$ inliers for class $-1$ from a Gaussian with mean $(-2,0)$ and the same covariance.\n- Append exactly one mislabeled extreme outlier with label $y=-1$ at position $(d,d)$, where $d0$ is a parameter. This yields $n=41$ samples in total for each test case.\n\nTraining protocol:\n- Initialize $w=(0,0)$ and $b=0$.\n- Use learning rate $\\eta = 0.001$, iterations $T = 1000$, and regularization $\\lambda = 0.001$.\n- Train four separate models corresponding to $L_{\\text{log}}$, $L_{\\text{hinge}}$, $L_{\\text{ramp}}$, and $L_{\\text{tlog}}$, with truncated cap $\\tau=2.0$.\n\nRobustness analysis metric:\n- After training each model, compute the outlier loss fraction\n$$\nF_L(d) = \\frac{L(u_{\\text{outlier}})}{\\sum_{i=1}^{n} L(u_i)},\n$$\nfor the corresponding loss $L$. A smaller $F_L(d)$ indicates the outlier contributes a smaller fraction of the total training loss and hence the loss is more robust to that outlier.\n\nTest suite:\n- Consider four test cases with outlier distances $d \\in \\{3, 10, 30, 100\\}$.\n- For each $d$, compute the following boolean results:\n    1. $B_1(d)$: whether hinge appears more robust than cross-entropy, defined as $F_{\\text{hinge}}(d)  F_{\\text{log}}(d)$.\n    2. $B_2(d)$: whether truncated cross-entropy improves robustness compared to hinge, defined as $F_{\\text{tlog}}(d)  F_{\\text{hinge}}(d)$.\n    3. $B_3(d)$: whether ramp loss improves robustness compared to hinge, defined as $F_{\\text{ramp}}(d)  F_{\\text{hinge}}(d)$.\n\nFinal output format:\n- Your program should produce a single line of output containing the boolean triples for the four test distances as a comma-separated list enclosed in square brackets. That is, print a list of lists in the form $[[B_1(d_1),B_2(d_1),B_3(d_1)], [B_1(d_2),B_2(d_2),B_3(d_2)], [B_1(d_3),B_2(d_3),B_3(d_3)], [B_1(d_4),B_2(d_4),B_3(d_4)]]$ with $d_1=3$, $d_2=10$, $d_3=30$, and $d_4=100$.", "solution": "The user wants to validate the provided problem statement.\n\n### Step 1: Extract Givens\n-   **Classifier**: A linear classifier in $\\mathbb{R}^2$ is defined as $f(x) = w^\\top x + b$, where $w \\in \\mathbb{R}^2$ is the weight vector and $b \\in \\mathbb{R}$ is the bias.\n-   **Labels**: The binary class labels are $y \\in \\{-1, +1\\}$.\n-   **Signed Margin**: For a sample $(x_i, y_i)$, the signed margin is $u_i = y_i f(x_i)$.\n-   **Objective Function**: The empirical risk with $\\ell_2$ regularization is $R(w,b) = \\frac{1}{n}\\sum_{i=1}^n L(u_i) + \\lambda \\lVert w \\rVert_2^2$, where $L$ is a loss function and $\\lambda  0$.\n-   **Loss Functions**:\n    -   Cross-entropy (logistic): $L_{\\text{log}}(u) = \\log\\left(1 + e^{-u}\\right)$.\n    -   Hinge: $L_{\\text{hinge}}(u) = \\max\\left(0, 1 - u\\right)$.\n    -   Ramp: $L_{\\text{ramp}}(u) = \\max\\left(0, \\min\\left(1 - u, 1\\right)\\right)$.\n    -   Truncated cross-entropy: $L_{\\text{tlog}}(u) = \\min\\left(\\log\\left(1 + e^{-u}\\right), \\tau\\right)$.\n-   **Gradients of Losses (w.r.t. $u$)**:\n    -   $\\frac{\\partial L_{\\text{log}}}{\\partial u} = -\\frac{1}{1+e^{u}}$.\n    -   $\\frac{\\partial L_{\\text{hinge}}}{\\partial u} = -\\mathbb{I}\\{u  1\\}$.\n    -   $\\frac{\\partial L_{\\text{ramp}}}{\\partial u}$ is defined as $0$ for $u  0$ or $u \\ge 1$, and $-1$ for $0 \\le u  1$.\n    -   $\\frac{\\partial L_{\\text{tlog}}}{\\partial u}$ is defined as $-\\frac{1}{1+e^{u}}$ if $\\log(1+e^{-u})  \\tau$, and $0$ otherwise.\n-   **Dataset Generation**:\n    -   A fixed random seed for reproducibility.\n    -   $n_+=20$ inliers for class $+1$ from a Gaussian with mean $(2,0)$ and covariance $\\operatorname{diag}(0.25,0.25)$.\n    -   $n_-=20$ inliers for class $-1$ from a Gaussian with mean $(-2,0)$ and the same covariance.\n    -   One mislabeled outlier with label $y=-1$ at position $(d,d)$.\n    -   Total samples $n=41$.\n-   **Training Protocol**:\n    -   Optimizer: Full-batch gradient descent.\n    -   Initialization: $w=(0,0)$, $b=0$.\n    -   Learning rate: $\\eta = 0.001$.\n    -   Iterations: $T = 1000$.\n    -   Regularization strength: $\\lambda = 0.001$.\n    -   Truncation cap: $\\tau=2.0$.\n-   **Evaluation Metric**:\n    -   Outlier loss fraction: $F_L(d) = L(u_{\\text{outlier}}) / \\sum_{i=1}^{n} L(u_i)$.\n-   **Test Suite**:\n    -   Outlier distances: $d \\in \\{3, 10, 30, 100\\}$.\n    -   Boolean comparisons:\n        1.  $B_1(d): F_{\\text{hinge}}(d)  F_{\\text{log}}(d)$.\n        2.  $B_2(d): F_{\\text{tlog}}(d)  F_{\\text{hinge}}(d)$.\n        3.  $B_3(d): F_{\\text{ramp}}(d)  F_{\\text{hinge}}(d)$.\n-   **Output Format**: A string representation of a list of lists of booleans, `[[B1(3),...], [B1(10),...], ...]`.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientific or Factual Unsoundness**: The problem is scientifically sound. It describes a standard numerical experiment in statistical learning to compare the robustness properties of well-established loss functions. The mathematical formulations are correct.\n2.  **Non-Formalizable or Irrelevant**: The problem is formal, quantitative, and directly relevant to the topic of classification losses and robust optimization in machine learning.\n3.  **Incomplete or Contradictory Setup**: The problem is fully specified. All necessary parameters for dataset generation, model training, and evaluation are provided. There are no contradictions.\n4.  **Unrealistic or Infeasible**: The setup is a synthetic experiment, which is a common and valid methodology in machine learning research. The parameters and computations are entirely feasible.\n5.  **Ill-Posed or Poorly Structured**: The problem is well-posed. The use of gradient descent on a specified objective function with fixed initial conditions, hyperparameters, and dataset will produce a deterministic outcome (given the fixed random seed). The evaluation metrics are clear and lead to a unique boolean result for each comparison.\n6.  **Pseudo-Profound, Trivial, or Tautological**: The problem is not trivial. It requires a full implementation of a machine learning training and evaluation pipeline and provides insight into the important, non-obvious practical behavior of different loss functions.\n7.  **Outside Scientific Verifiability**: The results are computationally verifiable. Anyone implementing the specified procedure with the given seed will obtain the same results.\n\n### Step 3: Verdict and Action\nThe problem is valid and well-posed.\n\nThis problem investigates the robustness of different loss functions to a mislabeled outlier. The core principle at play is how a loss function's value and gradient behave for large negative margins $u$, which correspond to confidently incorrect predictions.\n\n1.  **Principle of Robustness**: A loss function is considered robust to outliers if it limits their influence on the final model.\n    -   **Unbounded Losses**: The cross-entropy ($L_{\\text{log}}$) and hinge ($L_{\\text{hinge}}$) losses are unbounded. As the margin $u_i$ becomes very negative (i.e., the point is severely misclassified), $L(u_i)$ grows linearly. This means a single extreme outlier can contribute an enormous amount to the total loss, and its gradient will persistently pull the decision boundary towards it, potentially at the expense of correctly classifying the inliers.\n    -   **Bounded Losses**: The ramp ($L_{\\text{ramp}}$) and truncated cross-entropy ($L_{\\text{tlog}}$) losses are bounded. For ramp loss, the loss is capped at $1$ for $u0$. For truncated cross-entropy, the loss is capped at $\\tau=2.0$. Critically, for severely misclassified points (where $u$ is very negative), the gradients of these losses become zero. The optimizer effectively \"gives up\" on these points and they cease to influence the model's parameters. This behavior is the hallmark of a robust loss function.\n\n2.  **Methodology**: The solution proceeds by first implementing the components of the experiment.\n    -   **Data Generation**: For each distance $d$, a dataset is created with $40$ inliers drawn from two separate Gaussian distributions and one extreme outlier at $(d,d)$ with a negative label. A fixed random number generator state ensures reproducibility.\n    -   **Loss and Gradient Functions**: The four loss functions and their respective gradients (or subgradients) with respect to the margin $u$ are implemented according to their definitions.\n    -   **Training**: A `train` function implements full-batch gradient descent. For $1000$ iterations, it computes the gradient of the regularized risk $R(w,b)$ and updates the parameters $w$ and $b$. This process is repeated for each of the four loss functions.\n    -   **Evaluation**: After training, for each model, the outlier loss fraction $F_L(d)$ is computed. This metric quantifies the outlier's contribution to the total loss. A smaller fraction indicates greater robustness.\n\n3.  **Analysis**: The boolean results $B_1, B_2, B_3$ are computed by comparing the $F_L(d)$ values.\n    -   We expect $B_2$ and $B_3$ to be `True`, especially for large $d$. The bounded nature of $L_{\\text{ramp}}$ and $L_{\\text{tlog}}$ will cap the outlier's loss, while the unbounded hinge loss $L_{\\text{hinge}}$ on the same point will be very large. This should lead to $F_{\\text{tlog}} \\ll F_{\\text{hinge}}$ and $F_{\\text{ramp}} \\ll F_{\\text{hinge}}$.\n    -   The comparison between hinge and cross-entropy ($B_1$) is less obvious a priori. Both are unbounded and grow linearly for large negative margins ($L_{\\text{hinge}}(u) = 1-u$ vs. $L_{\\text{log}}(u) \\approx -u$). Their relative performance depends on the complex dynamics of the training process. The simulation will reveal the empirical result.\n\nThe final output is a list of these boolean triples for each value of $d$, formatted as a string.", "answer": "```python\nimport numpy as np\nfrom functools import partial\n\ndef log_loss(u):\n    \"\"\"Computes the logistic loss.\"\"\"\n    return np.log(1 + np.exp(-u))\n\ndef log_loss_grad(u):\n    \"\"\"Computes the gradient of the logistic loss.\"\"\"\n    return -1 / (1 + np.exp(u))\n\ndef hinge_loss(u):\n    \"\"\"Computes the hinge loss.\"\"\"\n    return np.maximum(0, 1 - u)\n\ndef hinge_loss_grad(u):\n    \"\"\"Computes the subgradient of the hinge loss.\"\"\"\n    return -1.0 * (u  1)\n\ndef ramp_loss(u):\n    \"\"\"Computes the ramp loss.\"\"\"\n    return np.maximum(0, np.minimum(1 - u, 1))\n    \ndef ramp_loss_grad(u):\n    \"\"\"Computes the subgradient of the ramp loss.\"\"\"\n    grad = np.zeros_like(u)\n    mask = (u >= 0)  (u  1)\n    grad[mask] = -1.0\n    return grad\n\ndef tlog_loss(u, tau):\n    \"\"\"Computes the truncated logistic loss.\"\"\"\n    return np.minimum(log_loss(u), tau)\n\ndef tlog_loss_grad(u, tau):\n    \"\"\"Computes the subgradient of the truncated logistic loss.\"\"\"\n    grad = log_loss_grad(u)\n    # The gradient is zero where the loss is capped.\n    # The condition for capping is log(1 + exp(-u)) >= tau.\n    grad[log_loss(u) >= tau] = 0.0\n    return grad\n\ndef generate_data(d, rng):\n    \"\"\"Generates the dataset with inliers and one outlier.\"\"\"\n    n_plus = 20\n    n_minus = 20\n    mean_plus = np.array([2, 0])\n    mean_minus = np.array([-2, 0])\n    cov = np.array([[0.25, 0], [0, 0.25]])\n    \n    # Inliers\n    X_plus = rng.multivariate_normal(mean_plus, cov, n_plus)\n    y_plus = np.ones(n_plus)\n    X_minus = rng.multivariate_normal(mean_minus, cov, n_minus)\n    y_minus = -np.ones(n_minus)\n    \n    # Outlier\n    X_outlier = np.array([[d, d]])\n    y_outlier = np.array([-1])\n    \n    X = np.vstack((X_plus, X_minus, X_outlier))\n    y = np.hstack((y_plus, y_minus, y_outlier))\n    \n    return X, y\n\ndef train(X, y, loss_grad_fn, T, eta, lambda_reg):\n    \"\"\"Trains a linear classifier using gradient descent.\"\"\"\n    n_samples, n_features = X.shape\n    w = np.zeros(n_features)\n    b = 0.0\n    \n    for _ in range(T):\n        f = X @ w + b\n        u = y * f\n        \n        grad_L_u = loss_grad_fn(u)\n        \n        grad_w_loss = (1 / n_samples) * X.T @ (y * grad_L_u)\n        grad_b_loss = (1 / n_samples) * np.sum(y * grad_L_u)\n        \n        grad_w = grad_w_loss + 2 * lambda_reg * w\n        grad_b = grad_b_loss\n        \n        w -= eta * grad_w\n        b -= eta * grad_b\n        \n    return w, b\n\ndef solve():\n    \"\"\"Main function to run the experiment and produce the final output.\"\"\"\n    test_ds = [3, 10, 30, 100]\n    eta = 0.001\n    T = 1000\n    lambda_reg = 0.001\n    tau = 2.0\n    seed = 42\n    \n    rng = np.random.default_rng(seed)\n\n    loss_functions = {\n        'log': log_loss,\n        'hinge': hinge_loss,\n        'ramp': ramp_loss,\n        'tlog': partial(tlog_loss, tau=tau)\n    }\n    loss_grad_functions = {\n        'log': log_loss_grad,\n        'hinge': hinge_loss_grad,\n        'ramp': ramp_loss_grad,\n        'tlog': partial(tlog_loss_grad, tau=tau)\n    }\n\n    all_results = []\n\n    for d in test_ds:\n        X, y = generate_data(d, rng)\n        \n        trained_models = {}\n        for name, grad_fn in loss_grad_functions.items():\n            w, b = train(X, y, grad_fn, T, eta, lambda_reg)\n            trained_models[name] = (w, b)\n            \n        loss_fractions = {}\n        for name, model in trained_models.items():\n            w, b = model\n            loss_fn = loss_functions[name]\n            \n            f = X @ w + b\n            u = y * f\n            \n            all_losses = loss_fn(u)\n            outlier_loss = all_losses[-1]\n            total_loss = np.sum(all_losses)\n            \n            # Avoid division by zero, though unlikely here.\n            loss_fractions[name] = outlier_loss / (total_loss + 1e-12)\n\n        F_log = loss_fractions['log']\n        F_hinge = loss_fractions['hinge']\n        F_tlog = loss_fractions['tlog']\n        F_ramp = loss_fractions['ramp']\n\n        B1 = F_hinge  F_log\n        B2 = F_tlog  F_hinge\n        B3 = F_ramp  F_hinge\n        \n        all_results.append([B1, B2, B3])\n\n    # Format the output string as a list of lists of booleans\n    sublist_strs = []\n    for res_list in all_results:\n        # Convert each boolean in the sublist to its string representation\n        # and join with commas. Enclose in square brackets.\n        sublist_str = f\"[{','.join(map(str, res_list))}]\"\n        sublist_strs.append(sublist_str)\n    \n    # Join the sublist strings with commas and enclose in outer brackets.\n    final_output = f\"[{','.join(sublist_strs)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3108631"}]}