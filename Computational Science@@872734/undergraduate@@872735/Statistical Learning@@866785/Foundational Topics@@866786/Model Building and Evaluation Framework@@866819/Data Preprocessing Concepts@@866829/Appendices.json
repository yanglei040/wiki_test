{"hands_on_practices": [{"introduction": "Linear models are powerful but rely on key assumptions, such as a linear relationship between variables and constant error variance (homoscedasticity). Real-world data often violates these assumptions, for instance, when noise is multiplicative rather than additive. A logarithmic transformation can be a powerful tool to stabilize variance and linearize relationships by converting a model like $Y = \\beta X \\varepsilon$ into an additive one, $\\log Y = \\log\\beta + \\log X + \\log\\varepsilon$, making the data much more amenable to linear regression. This hands-on coding exercise [@problem_id:3112629] will guide you through simulating data with multiplicative noise, allowing you to empirically verify how a log transform can improve model fit, as measured by the coefficient of determination ($R^2$), and lead to more accurate parameter estimates.", "problem": "You are tasked with constructing datasets under a multiplicative noise model and evaluating how a logarithmic transformation affects linear modeling performance. The setting is statistical learning with data preprocessing concepts. The fundamental base you may use includes the property of logarithms that for positive real numbers $a$ and $b$, $\\log(ab)=\\log a+\\log b$, and standard definitions from probability, including expectations under independence and well-tested formulas for the lognormal distribution.\n\nConsider an independent pair of positive random variables $(X,\\varepsilon)$ and a positive constant $\\beta$, with the generative relationship $Y=\\beta X \\varepsilon$. Assume $\\varepsilon$ is lognormally distributed with $\\log \\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)$, so that the mean $\\mathbb{E}[\\varepsilon]$ equals $\\exp(\\sigma^2/2)$. The goal is to compare ordinary least squares fits before and after a logarithmic transformation and assess whether the transformation linearizes the effect of multiplicative noise in a way that improves fit quality.\n\nYour program must, for each test case, perform the following steps:\n\n- Generate a dataset of size $n$, using the specified distribution for $X$ and the specified parameters for $\\beta$ and $\\sigma$.\n- Construct $Y=\\beta X \\varepsilon$ with $\\varepsilon$ drawn independently from a lognormal distribution with parameters $\\mu=0$ and $\\sigma$ as given for the test case.\n- Fit an ordinary least squares (OLS) linear model with intercept to the untransformed variables $Y$ versus $X$, i.e., approximate $Y \\approx a+bX$, and compute the estimated slope $b_{\\text{pre}}$ and the coefficient of determination $R^2_{\\text{pre}}$.\n- Fit an ordinary least squares linear model with intercept to the log-transformed variables $\\log Y$ versus $\\log X$, i.e., approximate $\\log Y \\approx a_{\\log}+b_{\\log}\\log X$, and compute the estimated slope $b_{\\text{post}}$ and the coefficient of determination $R^2_{\\text{post}}$.\n- Compute the absolute slope error in the original scale relative to the population conditional mean linear slope, $\\left|b_{\\text{pre}} - \\beta \\exp(\\sigma^2/2)\\right|$, and the absolute slope error in the log-transformed scale relative to the expected linear relation, $\\left|b_{\\text{post}} - 1\\right|$.\n- Report whether the $R^2$ improves after the logarithmic transformation as a boolean $R^2_{\\text{post}} > R^2_{\\text{pre}}$, along with the two slope errors.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case’s result must be a list of the form $[R2\\_improves,err\\_pre,err\\_post]$, where $R2\\_improves$ is a boolean, and $err\\_pre$ and $err\\_post$ are floats rounded to six decimal places. The final printed output should look like $[[\\text{bool},\\text{float},\\text{float}],\\ldots]$ with one entry per test case, in the order they are specified below.\n\nUse the following test suite. For reproducibility, use independent pseudorandom number generator seeds as specified for each case.\n\n- Test case $1$: $\\beta=2.0$, $n=200$, $\\sigma=0.3$, $X \\sim \\text{Uniform}[1,10]$, seed $42$.\n- Test case $2$: $\\beta=1.5$, $n=200$, $\\sigma=0.0$, $X \\sim \\text{Uniform}[1,10]$, seed $123$.\n- Test case $3$: $\\beta=1.0$, $n=1000$, $\\sigma=1.0$, $X \\sim \\text{Exponential}(\\text{scale}=3)$, seed $7$.\n- Test case $4$: $\\beta=0.5$, $n=50$, $\\sigma=0.8$, $X \\sim \\text{Lognormal}(\\mu=-0.2,\\sigma=0.5)$, seed $2025$.\n\nAngle units are not applicable, and there are no physical units involved. Percentages must not be used; all quantities should be represented as decimals.\n\nYour program must adhere to the specified output format and must not read input or write files.", "solution": "The problem statement has been validated and is deemed sound. It is scientifically grounded in statistical learning principles, well-posed with a clear objective and sufficient data, and free from ambiguity or contradiction.\n\nThe core of this problem lies in understanding and mitigating heteroscedasticity—non-constant variance of errors—in a linear regression context. We are given a multiplicative noise model which is a common source of such issues.\n\n**Theoretical Framework**\n\nThe generative model for the data is given by the relationship $Y = \\beta X \\varepsilon$, where $Y$ and $X$ are the observable variables, $\\beta$ is a positive constant scaling factor, and $\\varepsilon$ is a multiplicative noise term. The variables $X$ and $\\varepsilon$ are independent and positive. The noise term $\\varepsilon$ is specified to follow a lognormal distribution such that its natural logarithm, $\\log \\varepsilon$, is normally distributed with mean $0$ and variance $\\sigma^2$, denoted as $\\log \\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$.\n\n**Analysis of the Untransformed Model: $Y$ versus $X$**\n\nFirst, we consider a direct linear regression on the untransformed variables. The theoretical relationship between $Y$ and $X$ is described by the conditional expectation $\\mathbb{E}[Y | X=x]$. Due to the independence of $X$ and $\\varepsilon$:\n$$\n\\mathbb{E}[Y | X=x] = \\mathbb{E}[\\beta x \\varepsilon | X=x] = \\beta x \\mathbb{E}[\\varepsilon]\n$$\nFor a lognormal random variable $\\varepsilon$ where $\\log \\varepsilon \\sim \\mathcal{N}(\\mu, \\sigma^2)$, its expectation is $\\mathbb{E}[\\varepsilon] = \\exp(\\mu + \\sigma^2/2)$. In our case, $\\mu=0$, so $\\mathbb{E}[\\varepsilon] = \\exp(\\sigma^2/2)$.\nTherefore, the conditional expectation is:\n$$\n\\mathbb{E}[Y | X=x] = (\\beta e^{\\sigma^2/2}) x\n$$\nThis relationship is linear in $x$, with a slope of $\\beta e^{\\sigma^2/2}$. An ordinary least squares (OLS) fit of $Y$ on $X$ attempts to estimate this underlying linear trend. The target slope for comparison is thus $\\beta e^{\\sigma^2/2}$.\n\nHowever, a critical assumption for the OLS estimator to be the Best Linear Unbiased Estimator (BLUE) is homoscedasticity, meaning the variance of the error term is constant. Let's examine the conditional variance:\n$$\n\\text{Var}(Y | X=x) = \\text{Var}(\\beta x \\varepsilon) = (\\beta x)^2 \\text{Var}(\\varepsilon)\n$$\nSince $\\text{Var}(\\varepsilon)$ is a positive constant (for $\\sigma > 0$), the variance of $Y$ is proportional to $x^2$. This is a classic case of heteroscedasticity: the variance of the observations increases as the magnitude of the predictor $X$ increases. This violation can lead to an inefficient OLS estimator and unreliable hypothesis tests.\n\n**Analysis of the Log-Transformed Model: $\\log Y$ versus $\\log X$**\n\nTo address the issues of multiplicative noise and heteroscedasticity, a logarithmic transformation is proposed. Applying the natural logarithm to the generative model yields:\n$$\n\\log Y = \\log(\\beta X \\varepsilon) = \\log \\beta + \\log X + \\log \\varepsilon\n$$\nLet us define a new set of variables: $Y' = \\log Y$, $X' = \\log X$, and an additive error term $\\epsilon' = \\log \\varepsilon$. The model becomes:\n$$\nY' = (\\log \\beta) + 1 \\cdot X' + \\epsilon'\n$$\nBy definition, the error term $\\epsilon'$ follows a normal distribution, $\\epsilon' \\sim \\mathcal{N}(0, \\sigma^2)$. In this transformed space, the relationship between $Y'$ and $X'$ is perfectly linear with a true slope of $1$ and an intercept of $\\log \\beta$. The error term $\\epsilon'$ is now additive, and its variance, $\\sigma^2$, is constant for all values of $X'$. This new model satisfies the key assumptions of OLS (linearity, normality of errors, and homoscedasticity). Consequently, we expect the OLS fit on the log-transformed data to be more robust and provide a better characterization of the underlying relationship. The target slope for comparison in this transformed scale is $1$.\n\n**Computational Methodology**\n\nThe program implements a simulation to empirically verify these theoretical insights. For each specified test case:\n$1$. A pseudorandom number generator is initialized with a specific seed to ensure reproducibility.\n$2$. A dataset of size $n$ is generated. The predictor variable $X$ is drawn from its specified distribution. The multiplicative noise term $\\varepsilon$ is drawn from a lognormal distribution with parameters $\\mu=0$ and the given $\\sigma$. The response variable $Y$ is then constructed as $Y = \\beta X \\varepsilon$.\n$3$. **Pre-transformation fit**: An OLS model of the form $Y \\approx a_{\\text{pre}} + b_{\\text{pre}}X$ is fitted to the data $(X, Y)$. The slope estimate $b_{\\text{pre}}$ and the coefficient of determination $R^2_{\\text{pre}}$ are calculated. The absolute slope error is computed as $|b_{\\text{pre}} - \\beta e^{\\sigma^2/2}|$.\n$4$. **Post-transformation fit**: The variables are transformed to $\\log X$ and $\\log Y$. An OLS model of the form $\\log Y \\approx a_{\\text{post}} + b_{\\text{post}}\\log X$ is fitted to the data $(\\log X, \\log Y)$. The slope estimate $b_{\\text{post}}$ and the coefficient of determination $R^2_{\\text{post}}$ are calculated. The absolute slope error is computed as $|b_{\\text{post}} - 1|$.\n$5$. The coefficient of determination, $R^2$, is computed as the square of the Pearson correlation coefficient between the predictor and response variables, which is equivalent to $1 - \\frac{\\sum_i (y_i - \\hat{y}_i)^2}{\\sum_i (y_i - \\bar{y})^2}$ for simple linear regression.\n$6$. The final metrics are assembled into a list: [$R^2_{\\text{post}} > R^2_{\\text{pre}}$, $|b_{\\text{pre}} - \\beta e^{\\sigma^2/2}|$, $|b_{\\text{post}} - 1|$]. The boolean indicates whether the log transformation improved the model fit in terms of explained variance. The two error terms quantify the accuracy of the slope estimation in each domain.", "answer": "```python\nimport numpy as np\n\ndef perform_analysis(beta, n, sigma, x_dist_params, seed):\n    \"\"\"\n    Generates a dataset under a multiplicative noise model and compares\n    OLS fits before and after a logarithmic transformation.\n\n    Args:\n        beta (float): The scaling factor in the model Y = beta * X * epsilon.\n        n (int): The number of data points to generate.\n        sigma (float): The scale parameter (std dev) of the log-normal noise.\n                       log(epsilon) ~ N(0, sigma^2).\n        x_dist_params (tuple): A tuple describing the distribution of X.\n                               e.g., ('uniform', low, high)\n        seed (int): The seed for the random number generator.\n\n    Returns:\n        list: A list containing [R2_improves, err_pre, err_post].\n    \"\"\"\n    # 1. Initialize RNG and generate data\n    rng = np.random.default_rng(seed)\n\n    dist_type = x_dist_params[0]\n    if dist_type == 'uniform':\n        # Generate X from Uniform(low, high)\n        X = rng.uniform(low=x_dist_params[1], high=x_dist_params[2], size=n)\n    elif dist_type == 'exponential':\n        # Generate X from Exponential(scale)\n        X = rng.exponential(scale=x_dist_params[1], size=n)\n    elif dist_type == 'lognormal':\n        # Generate X from Lognormal(mu, sigma)\n        X = rng.lognormal(mean=x_dist_params[1], sigma=x_dist_params[2], size=n)\n    else:\n        # This case should not be reached with the given problem\n        raise ValueError(\"Unknown distribution for X\")\n\n    # Generate multiplicative noise epsilon from Lognormal(0, sigma)\n    # This means log(epsilon) is N(0, sigma^2)\n    epsilon = rng.lognormal(mean=0.0, sigma=sigma, size=n)\n\n    # Construct Y\n    Y = beta * X * epsilon\n    \n    # 2. Pre-transformation OLS fit (Y vs X)\n    # Using np.polyfit to get slope (b_pre) and intercept\n    b_pre, _ = np.polyfit(X, Y, 1)\n\n    # R-squared is the square of correlation for simple linear regression\n    # Handle cases where variance is zero to avoid NaN\n    if np.var(X) > 1e-12 and np.var(Y) > 1e-12:\n        R2_pre = np.corrcoef(X, Y)[0, 1]**2\n    else:\n        R2_pre = 0.0\n\n    # 3. Log-transform data\n    log_X = np.log(X)\n    log_Y = np.log(Y)\n\n    # 4. Post-transformation OLS fit (log Y vs log X)\n    b_post, _ = np.polyfit(log_X, log_Y, 1)\n\n    if np.var(log_X) > 1e-12 and np.var(log_Y) > 1e-12:\n        R2_post = np.corrcoef(log_X, log_Y)[0, 1]**2\n    else:\n        R2_post = 0.0\n\n    # 5. Compute comparison metrics\n    R2_improves = R2_post > R2_pre\n\n    # Absolute slope error for the pre-transformation model\n    target_slope_pre = beta * np.exp(sigma**2 / 2.0)\n    err_pre = np.abs(b_pre - target_slope_pre)\n\n    # Absolute slope error for the post-transformation model\n    target_slope_post = 1.0\n    err_post = np.abs(b_post - target_slope_post)\n\n    return [R2_improves, round(err_pre, 6), round(err_post, 6)]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final output.\n    \"\"\"\n    test_cases = [\n        {'beta': 2.0, 'n': 200, 'sigma': 0.3, 'x_dist_params': ('uniform', 1.0, 10.0), 'seed': 42},\n        {'beta': 1.5, 'n': 200, 'sigma': 0.0, 'x_dist_params': ('uniform', 1.0, 10.0), 'seed': 123},\n        {'beta': 1.0, 'n': 1000, 'sigma': 1.0, 'x_dist_params': ('exponential', 3.0), 'seed': 7},\n        {'beta': 0.5, 'n': 50, 'sigma': 0.8, 'x_dist_params': ('lognormal', -0.2, 0.5), 'seed': 2025}\n    ]\n\n    results = []\n    for case in test_cases:\n        result = perform_analysis(**case)\n        results.append(result)\n\n    # Format the output string exactly as required\n    formatted_results = []\n    for res in results:\n        # res[0] is boolean, res[1] and res[2] are floats\n        formatted_results.append(f\"[{str(res[0])},{res[1]:.6f},{res[2]:.6f}]\")\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3112629"}, {"introduction": "Machine learning models require numerical input, yet many datasets contain categorical features like country names or product types. The method used to convert these categories into numbers—a process called encoding—can dramatically affect a model's performance and its ability to learn meaningful patterns. The choice between encoding schemes, such as simple ordinal encoding (e.g., 'Red':1, 'Green':2, 'Blue':3) and one-hot encoding, depends critically on both the nature of the categorical variable and the type of model being used. This exercise [@problem_id:3112621] challenges you to think critically about the implicit assumptions imposed by different encoding strategies, developing a deeper understanding of when to use each technique to avoid introducing spurious relationships into your model by analyzing their impact on both linear and decision tree models.", "problem": "A dataset has a single categorical predictor $Z$ taking values in a finite set $\\mathcal{C}=\\{c_1,\\dots,c_k\\}$ with $k \\ge 3$, and a scalar response $Y \\in \\mathbb{R}$. Two model classes are considered: (i) a linear model $f(\\mathbf{x})=\\mathbf{w}^\\top \\mathbf{x}+b$ estimated by least squares or logistic loss, and (ii) a standard axis-aligned decision tree that splits numeric features by thresholds of the form $x_j \\le t$. Two encoding schemes for $Z$ are contemplated: an ordinal (label) encoding $e_{\\mathrm{ord}}: \\mathcal{C}\\to \\{1,2,\\dots,k\\}$ assigning an integer code to each category, and a one-hot encoding $e_{\\mathrm{oh}}:\\mathcal{C}\\to \\{0,1\\}^k$ mapping each category to a standard basis vector. Assume categories are nominal unless explicitly stated otherwise, and the tree implementation does not have native multiway or categorical subset splits (only numeric threshold splits). Select all statements that are correct about the implications of these encodings for the two model classes.\n\nA. For a linear model with an intercept, using an ordinal encoding for a nominal $Z$ implicitly imposes a monotonic, equally spaced effect of moving from category $c_j$ to $c_{j+1}$, which can induce spurious linear trends unless the categories are truly ordered.\n\nB. For a decision tree with numeric threshold splits, using an ordinal encoding allows the tree to isolate any arbitrary subset of categories with at most a single split, so it does not restrict expressivity compared to one-hot encoding.\n\nC. For a linear model with an intercept, using one-hot encoding with all $k$ dummy columns included makes the design matrix rank-deficient due to perfect multicollinearity; dropping one category as a reference avoids this and yields coefficients interpretable as deviations from the reference.\n\nD. For axis-aligned trees without native categorical subset splits, one-hot encoding often allows shallower, simpler splits to isolate specific categories (for example, a single split on an indicator), whereas ordinal encoding can require multiple threshold splits to separate noncontiguous category groups; thus, for nominal $Z$, one-hot is typically preferable in such trees.\n\nE. For a linear model without an intercept, including all $k$ one-hot dummy columns is identifiable (no perfect multicollinearity) provided each category appears in the data at least once, allowing the model to fit a separate mean per category.\n\nSelect all that apply.", "solution": "The problem statement has been validated and is sound. It is a well-posed question within the domain of statistical learning, using standard terminology and posing a verifiable set of claims. We will now proceed with a detailed analysis of each statement.\n\nThe setup involves a categorical predictor $Z$ with $k \\ge 3$ nominal categories from the set $\\mathcal{C}=\\{c_1,\\dots,c_k\\}$, and a real-valued response $Y$. We compare the effects of two encoding schemes for $Z$—ordinal encoding ($e_{\\mathrm{ord}}$) and one-hot encoding ($e_{\\mathrm{oh}}$)—on two model classes: a linear model and a decision tree.\n\n**Analysis of Statement A**\n\nThis statement claims that for a linear model with an intercept, using an ordinal encoding for a nominal predictor $Z$ imposes a monotonic, equally spaced effect.\n\nLet the ordinal encoding be $e_{\\mathrm{ord}}(c_j) = j$ for $j \\in \\{1, 2, \\dots, k\\}$. The categorical predictor $Z$ is transformed into a single numerical feature, let's call it $x_Z$. The linear model with an intercept is given by:\n$$ E[Y|Z=c_j] = f(x_Z=j) = w \\cdot j + b $$\nwhere $w$ is the weight for the feature $x_Z$ and $b$ is the intercept.\n\nLet's examine the change in the predicted response when moving from an arbitrary category $c_j$ to the next category in the imposed sequence, $c_{j+1}$:\n$$ E[Y|Z=c_{j+1}] - E[Y|Z=c_j] = (w \\cdot (j+1) + b) - (w \\cdot j + b) = w $$\nThis difference is constant and equal to the weight $w$, regardless of the specific value of $j$. This demonstrates two properties:\n$1$. **Monotonicity**: If $w > 0$, the predicted response strictly increases with the integer code assigned to the category. If $w < 0$, it strictly decreases. If $w = 0$, it is constant. In any case, the relationship is monotonic with respect to the artificial ordering.\n$2$. **Equally spaced effect**: The change in response between any two adjacent categories in the arbitrary ordering is identical ($w$). This implies that the model is forced to assume that the effect of changing from $c_1$ to $c_2$ is identical to changing from $c_2$ to $c_3$, and so on.\n\nFor a nominal variable (e.g., 'Paris', 'Tokyo', 'London'), there is no inherent order, let alone an equally spaced one. Imposing one (e.g., Paris=$1$, Tokyo=$2$, London=$3$) forces the model to learn a spurious linear trend that is an artifact of the encoding, not a true property of the data. This is only appropriate if the categories are truly ordinal and the steps between them can be considered equal. Thus, the statement is correct.\n\n**Verdict: Correct**\n\n**Analysis of Statement B**\n\nThis statement claims that for a decision tree with numeric threshold splits, ordinal encoding can isolate any arbitrary subset of categories with at most a single split.\n\nA decision tree with numeric threshold splits partitions the feature space using rules of the form $x \\le t$. When using ordinal encoding, we have a single feature $x_Z$ taking integer values $\\{1, 2, \\dots, k\\}$. A split on this feature takes the form $x_Z \\le t$. For any threshold $t$, this split divides the set of categories $\\mathcal{C}$ into two contiguous subsets based on the assigned integer codes. For a threshold $t$ where $\\lfloor t \\rfloor = j$ and $j \\in \\{1, \\dots, k-1\\}$, the split partitions the data into two groups: one where $Z \\in \\{c_1, \\dots, c_j\\}$ and another where $Z \\in \\{c_{j+1}, \\dots, c_k\\}$.\n\nThe statement claims that *any arbitrary subset* can be isolated. Let's test this with a counterexample. Let $k = 4$ and the categories be $\\{c_1, c_2, c_3, c_4\\}$, encoded as $\\{1, 2, 3, 4\\}$. Consider the arbitrary subset $\\{c_1, c_3\\}$. To isolate this subset, we would need to separate data points where $Z \\in \\{c_1, c_3\\}$ from those where $Z \\in \\{c_2, c_4\\}$. No single split of the form $x_Z \\le t$ can achieve this. For instance, a split $x_Z \\le 2.5$ separates $\\{c_1, c_2\\}$ from $\\{c_3, c_4\\}$. It fails to group $c_1$ with $c_3$.\n\nTherefore, ordinal encoding severely restricts the expressivity of a decision tree for nominal variables, as it cannot create partitions based on non-contiguous groups of categories in a single split. The statement is false.\n\n**Verdict: Incorrect**\n\n**Analysis of Statement C**\n\nThis statement claims that for a linear model with an intercept, using all $k$ one-hot encoded columns leads to a rank-deficient design matrix, and dropping one column resolves this.\n\nLet the one-hot encoding of $Z$ produce $k$ binary features, $x_1, \\dots, x_k$, where $x_j=1$ if $Z=c_j$ and $x_j=0$ otherwise. For any observation, exactly one of these features is $1$, which implies the following relationship holds for every data point:\n$$ \\sum_{j=1}^k x_j = 1 $$\nA linear model with an intercept is specified as:\n$$ Y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_k x_k + \\epsilon $$\nThe design matrix $\\mathbf{X}$ has columns corresponding to the intercept (a column of all ones, denoted $\\mathbf{1}$) and the $k$ dummy variables, $\\mathbf{x}_1, \\dots, \\mathbf{x}_k$. The relationship $\\sum_{j=1}^k x_j = 1$ for all observations means that the sum of the dummy variable columns is equal to the intercept column:\n$$ \\sum_{j=1}^k \\mathbf{x}_j = \\mathbf{1} $$\nThis constitutes a perfect linear dependency among the columns of the design matrix $\\mathbf{X}$. The set of column vectors $\\{\\mathbf{1}, \\mathbf{x}_1, \\dots, \\mathbf{x}_k\\}$ is linearly dependent, meaning the matrix $\\mathbf{X}$ is not full rank (it is rank-deficient). This situation is known as perfect multicollinearity, and it makes the ordinary least squares estimator $\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{X}^\\top\\mathbf{Y}$ non-unique because the matrix $\\mathbf{X}^\\top\\mathbf{X}$ is singular (non-invertible).\n\nTo resolve this, one dummy variable column is dropped. Let's say we drop the column $\\mathbf{x}_k$ corresponding to category $c_k$. The model becomes:\n$$ Y = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_{k-1} x_{k-1} + \\epsilon $$\nIn this formulation, $c_k$ is the *reference category*.\n- For an observation in the reference category $c_k$, all $x_j$ ($j=1, \\dots, k-1$) are $0$, so the expected response is $E[Y|Z=c_k] = \\beta_0$. The intercept $\\beta_0$ is the mean response for the reference category.\n- For an observation in another category $c_j$ (where $j < k$), $x_j=1$ and all other $x_i=0$. The expected response is $E[Y|Z=c_j] = \\beta_0 + \\beta_j$.\nTherefore, the coefficient $\\beta_j$ represents the difference in the mean response between category $c_j$ and the reference category $c_k$: $\\beta_j = E[Y|Z=c_j] - E[Y|Z=c_k]$. The interpretation as \"deviations from the reference\" is correct.\n\n**Verdict: Correct**\n\n**Analysis of Statement D**\n\nThis statement compares one-hot and ordinal encoding for decision trees with numeric threshold splits, concluding one-hot is preferable for nominal variables.\n\nAs established in the analysis of statement B, ordinal encoding forces an artificial ordering and only allows splits that partition the categories into two contiguous blocks. To separate a non-contiguous group (e.g., $\\{c_1, c_4\\}$ from $\\{c_2, c_3\\}$) requires multiple, nested splits, which can lead to a deeper and more complex tree that may overfit. For example, to isolate category $c_j$ (where $1 < j < k$), it requires two splits: one of the form $x_Z > j-1$ and another of the form $x_Z \\le j$.\n\nWith one-hot encoding, we have $k$ independent binary features $x_1, \\dots, x_k$.\n- To isolate a single category $c_j$, the tree can use a single split on its corresponding indicator variable: $x_j > 0.5$ (or $x_j \\le 0.5$). This perfectly separates all observations of category $c_j$ from all others in one step. This is a very efficient and \"simple\" split.\n- The tree can learn to approximate any partition of the categories. For example, if categories $c_1$ and $c_3$ have a similar effect on the response $Y$, the tree can learn to group them. It is not constrained by any predefined adjacency. This flexibility is crucial when the predictor is nominal.\n\nBecause one-hot encoding does not impose any spurious structure on the categories and allows the tree to find optimal groupings based on the data, it generally leads to more expressive and often simpler (shallower) models for nominal variables compared to the restrictive nature of ordinal encoding. The statement's reasoning and conclusion are sound.\n\n**Verdict: Correct**\n\n**Analysis of Statement E**\n\nThis statement claims that for a linear model *without* an intercept, using all $k$ one-hot columns is identifiable.\n\nThe model without an intercept is:\n$$ Y = \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_k x_k + \\epsilon $$\nThe design matrix $\\mathbf{X}$ now consists only of the $k$ dummy variable columns, $\\{\\mathbf{x}_1, \\dots, \\mathbf{x}_k\\}$. The intercept column (vector of ones) is not part of the design matrix.\n\nTo check for identifiability (i.e., no perfect multicollinearity), we must check if the columns of $\\mathbf{X}$ are linearly independent. We test if there is a non-trivial solution to the equation:\n$$ a_1 \\mathbf{x}_1 + a_2 \\mathbf{x}_2 + \\dots + a_k \\mathbf{x}_k = \\mathbf{0} $$\nLet's consider an arbitrary row $i$ of this vector equation, corresponding to a single data point. Suppose this data point belongs to category $c_j$. By definition of one-hot encoding, for this row, the value of the $j$-th feature is $x_{ij} = 1$, and all other feature values are $x_{il} = 0$ for $l \\neq j$. The equation for this row thus becomes:\n$$ a_1 \\cdot 0 + \\dots + a_j \\cdot 1 + \\dots + a_k \\cdot 0 = 0 \\implies a_j = 0 $$\nThe problem states that each category appears in the data at least once. This guarantees that for each $j \\in \\{1,\\dots, k\\}$, there is at least one row for which $x_j=1$. We can therefore apply this logic for each category $c_1, \\dots, c_k$ and conclude that $a_1=0, a_2=0, \\dots, a_k=0$. Since the only solution is the trivial one, the columns $\\{\\mathbf{x}_1, \\dots, \\mathbf{x}_k\\}$ are linearly independent. The model is identifiable.\n\nIn this model, for an observation in category $c_j$, the predicted response is $E[Y|Z=c_j] = \\beta_j$, since $x_j=1$ and all other $x_l=0$. This means that each coefficient $\\beta_j$ directly models the mean response for category $c_j$. This is a valid and common modeling choice known as \"effects coding\" or fitting a \"cell means model\".\n\n**Verdict: Correct**", "answer": "$$\\boxed{ACDE}$$", "id": "3112621"}, {"introduction": "In the age of high-dimensional data, it's common practice to filter out features that seem uninformative to simplify models and reduce computational load. A popular heuristic is to remove features with very low variance, based on the assumption that a feature that rarely changes cannot be predictive. This intuition, however, can be dangerously misleading, as a feature's predictive power is determined by its conditional relationship with the target variable, not its marginal variance. This practice problem [@problem_id:3112623] uses a carefully constructed counterexample to demonstrate how a low-variance feature, representing a rare but significant event, can be highly predictive and crucial for building an accurate classifier, serving as a critical lesson against the blind application of filtering heuristics.", "problem": "A common preprocessing heuristic in statistical learning is to remove features with low variance under the belief that such features carry little predictive information. Consider the following binary classification setup with a feature selection rule that removes any feature whose sample variance is below a threshold $\\tau$. Let the label be $Y \\in \\{0,1\\}$ with prior $\\mathbb{P}(Y=1)=\\pi$ and $\\mathbb{P}(Y=0)=1-\\pi$, where $\\pi = 0.02$. There are two features, $X_1 \\in \\{0,1\\}$ and $X_2 \\in \\mathbb{R}$. The data-generating mechanism is as follows:\n- $X_1$ is a rare indicator tied to the label: $\\mathbb{P}(X_1=1 \\mid Y=1)=0.9$ and $\\mathbb{P}(X_1=1 \\mid Y=0)=0.001$.\n- $X_2$ is independent of $Y$ and follows a Gaussian (Normal) distribution $X_2 \\sim \\mathcal{N}(0,1)$.\nAssume the training set is sufficiently large so that sample quantities equal their population values. The classifier is chosen according to the standard Bayes decision principle: for any feature vector $x$, predict the class $y$ that maximizes the posterior probability $\\mathbb{P}(Y=y \\mid X=x)$. The variance of a real-valued random variable $Z$ is defined by $\\mathrm{Var}(Z)=\\mathbb{E}\\big[(Z-\\mathbb{E}Z)^2\\big]$, and for a Bernoulli random variable $B$ with success probability $p$, $\\mathrm{Var}(B)=p(1-p)$.\n\nFix the variance-threshold parameter at $\\tau=0.02$. Analyze the effect of removing $X_1$ under this rule and the resulting impact on classification. Select all statements that are correct.\n\nA. With $\\tau=0.02$, the variance-threshold filter removes $X_1$ because its variance is below the threshold, yet the Bayes misclassification probability using $X_1$ is strictly smaller than the misclassification probability achievable without $X_1$.\n\nB. The posterior probability $\\mathbb{P}(Y=1 \\mid X_1=1)$ is approximately $0.95$, showing that $X_1$ is strongly predictive despite its low marginal variance; hence removing $X_1$ is detrimental.\n\nC. Because $\\mathrm{Var}(X_1)$ is small, the Pearson correlation between $X_1$ and $Y$ must also be small, so $X_1$ cannot be useful for prediction.\n\nD. Removing $X_1$ cannot worsen expected misclassification risk because any classifier could have chosen to ignore $X_1$ even if it were present.\n\nE. This counterexample shows that variance is a property of the marginal distribution of a feature and does not capture conditional dependence on the label; a low-variance feature can have a large likelihood ratio $\\mathbb{P}(X_1=1 \\mid Y=1)\\big/\\mathbb{P}(X_1=1 \\mid Y=0)$ and substantial mutual information with $Y$.", "solution": "The problem statement is critically validated before proceeding to a solution.\n\n### Step 1: Extract Givens\n-   **Binary Classification:** Label $Y \\in \\{0,1\\}$.\n-   **Prior Probabilities:** $\\mathbb{P}(Y=1) = \\pi = 0.02$, and $\\mathbb{P}(Y=0) = 1-\\pi = 0.98$.\n-   **Features:** $X_1 \\in \\{0,1\\}$ and $X_2 \\in \\mathbb{R}$.\n-   **Data-Generating Mechanism for $X_1$:**\n    -   $\\mathbb{P}(X_1=1 \\mid Y=1) = 0.9$.\n    -   $\\mathbb{P}(X_1=1 \\mid Y=0) = 0.001$.\n-   **Data-Generating Mechanism for $X_2$:**\n    -   $X_2 \\sim \\mathcal{N}(0,1)$.\n    -   $X_2$ is independent of $Y$.\n-   **Assumption:** Sample quantities equal their population values.\n-   **Feature Selection Rule:** Remove any feature whose sample variance is below a threshold $\\tau=0.02$.\n-   **Classifier:** Bayes decision principle (maximize posterior probability).\n-   **Variance Definitions:**\n    -   $\\mathrm{Var}(Z)=\\mathbb{E}\\big[(Z-\\mathbb{E}Z)^2\\big]$ for a real-valued random variable $Z$.\n    -   $\\mathrm{Var}(B)=p(1-p)$ for a Bernoulli random variable $B$ with success probability $p$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded in probability theory and statistical learning. It is well-posed, with all necessary parameters and distributions defined to allow for unique calculation of the quantities of interest (variances, misclassification probabilities). The language is objective and precise. The premise is a standard textbook counterexample used to illustrate the potential pitfalls of a naive feature selection heuristic, making it a valid and conceptually important problem. The problem does not violate any of the invalidity criteria.\n\n### Step 3: Verdict and Action\nThe problem is valid. A full derivation and evaluation of options will be performed.\n\n### Derivations\n\nFirst, we analyze the feature $X_1$. To determine if it is removed by the variance-threshold filter, we must compute its marginal variance, $\\mathrm{Var}(X_1)$. Since $X_1$ is a Bernoulli random variable, we first need to find its marginal probability of success, $p_1 = \\mathbb{P}(X_1=1)$. Using the law of total probability:\n$$\n\\mathbb{P}(X_1=1) = \\mathbb{P}(X_1=1 \\mid Y=1)\\mathbb{P}(Y=1) + \\mathbb{P}(X_1=1 \\mid Y=0)\\mathbb{P}(Y=0)\n$$\nSubstituting the given values:\n$$\np_1 = (0.9)(0.02) + (0.001)(0.98) = 0.018 + 0.00098 = 0.01898\n$$\nThe variance of a Bernoulli variable with success probability $p_1$ is $p_1(1-p_1)$:\n$$\n\\mathrm{Var}(X_1) = 0.01898 \\times (1 - 0.01898) = 0.01898 \\times 0.98102 \\approx 0.01862\n$$\nThe threshold for removal is given as $\\tau=0.02$. Since $\\mathrm{Var}(X_1) \\approx 0.01862 < 0.02$, the variance-threshold filter will indeed remove feature $X_1$.\n\nNext, we evaluate the predictive power of $X_1$ by calculating the Bayes misclassification probability (or risk) when using $X_1$ versus not using it. Note that feature $X_2$ is independent of the label $Y$, so $\\mathbb{P}(Y \\mid X_1, X_2) = \\mathbb{P}(Y \\mid X_1)$. Therefore, $X_2$ provides no information for classification, and the risk using $\\{X_1, X_2\\}$ is the same as the risk using only $X_1$.\n\n**Case 1: Classifier uses feature $X_1$.**\nThe Bayes optimal classifier predicts the class $y$ that maximizes the posterior $\\mathbb{P}(Y=y \\mid X_1=x_1)$. The decision rule is to predict $\\hat{Y}=1$ if $\\mathbb{P}(Y=1 \\mid X_1=x_1) > \\mathbb{P}(Y=0 \\mid X_1=x_1)$, and $\\hat{Y}=0$ otherwise. This is equivalent to predicting $\\hat{Y}=1$ if $\\mathbb{P}(Y=1 \\mid X_1=x_1) > 0.5$.\n\nLet's find the posteriors. Using Bayes' theorem:\n$$\n\\mathbb{P}(Y=1 \\mid X_1=1) = \\frac{\\mathbb{P}(X_1=1 \\mid Y=1)\\mathbb{P}(Y=1)}{\\mathbb{P}(X_1=1)} = \\frac{0.9 \\times 0.02}{0.01898} = \\frac{0.018}{0.01898} \\approx 0.9484\n$$\nSince $0.9484 > 0.5$, if $X_1=1$, the classifier predicts $\\hat{Y}=1$.\nWe also need the posterior for $X_1=0$.\n$\\mathbb{P}(X_1=0) = 1 - \\mathbb{P}(X_1=1) = 1 - 0.01898 = 0.98102$.\n$\\mathbb{P}(X_1=0 \\mid Y=1) = 1 - \\mathbb{P}(X_1=1 \\mid Y=1) = 1-0.9 = 0.1$.\n$$\n\\mathbb{P}(Y=1 \\mid X_1=0) = \\frac{\\mathbb{P}(X_1=0 \\mid Y=1)\\mathbb{P}(Y=1)}{\\mathbb{P}(X_1=0)} = \\frac{0.1 \\times 0.02}{0.98102} = \\frac{0.002}{0.98102} \\approx 0.00204\n$$\nSince $0.00204 < 0.5$, if $X_1=0$, the classifier predicts $\\hat{Y}=0$.\n\nThe total Bayes misclassification probability, $R^*(X_1)$, is the sum of probabilities of the error events:\n$R^*(X_1) = \\mathbb{P}(\\hat{Y}=1, Y=0) + \\mathbb{P}(\\hat{Y}=0, Y=1)$\n$R^*(X_1) = \\mathbb{P}(X_1=1, Y=0) + \\mathbb{P}(X_1=0, Y=1)$\n$R^*(X_1) = \\mathbb{P}(X_1=1 \\mid Y=0)\\mathbb{P}(Y=0) + \\mathbb{P}(X_1=0 \\mid Y=1)\\mathbb{P}(Y=1)$\n$R^*(X_1) = (0.001)(0.98) + (0.1)(0.02) = 0.00098 + 0.002 = 0.00298$.\n\n**Case 2: Classifier does not use feature $X_1$ (because it was removed).**\nThe classifier has no useful features (since $X_2$ is independent of $Y$). The only information available is the prior distribution of $Y$. To minimize misclassification risk, the classifier must always predict the majority class.\nSince $\\mathbb{P}(Y=0) = 0.98 > \\mathbb{P}(Y=1) = 0.02$, the classifier always predicts $\\hat{Y}=0$.\nThe misclassification risk in this case is the probability of the minority class, which is $\\mathbb{P}(Y=1)$.\n$R^*(\\text{no features}) = \\pi = 0.02$.\n\nComparing the risks: $R^*(X_1) = 0.00298$ is strictly smaller than $R^*(\\text{no features}) = 0.02$. Thus, removing $X_1$ is detrimental to the classifier's performance.\n\n### Option-by-Option Analysis\n\n**A. With $\\tau=0.02$, the variance-threshold filter removes $X_1$ because its variance is below the threshold, yet the Bayes misclassification probability using $X_1$ is strictly smaller than the misclassification probability achievable without $X_1$.**\nOur calculation shows $\\mathrm{Var}(X_1) \\approx 0.01862$, which is less than $\\tau=0.02$. So, the filter removes $X_1$. Our risk analysis shows that the misclassification probability increases from $0.00298$ to $0.02$ upon removing $X_1$. Both parts of the statement are true.\n**Verdict: Correct.**\n\n**B. The posterior probability $\\mathbb{P}(Y=1 \\mid X_1=1)$ is approximately $0.95$, showing that $X_1$ is strongly predictive despite its low marginal variance; hence removing $X_1$ is detrimental.**\nOur calculation showed $\\mathbb{P}(Y=1 \\mid X_1=1) \\approx 0.9484$, which is approximately $0.95$. A posterior probability this close to $1$ signifies very strong predictive evidence. We have already established that the marginal variance is low and that removing the feature is detrimental (increases error rate). The reasoning presented in the statement is entirely sound.\n**Verdict: Correct.**\n\n**C. Because $\\mathrm{Var}(X_1)$ is small, the Pearson correlation between $X_1$ and $Y$ must also be small, so $X_1$ cannot be useful for prediction.**\nThis statement makes a claim about correlation. Let's compute the Pearson correlation coefficient $\\rho(X_1, Y) = \\frac{\\mathrm{Cov}(X_1, Y)}{\\sqrt{\\mathrm{Var}(X_1)\\mathrm{Var}(Y)}}$.\nWe need the covariance: $\\mathrm{Cov}(X_1, Y) = \\mathbb{E}[X_1 Y] - \\mathbb{E}[X_1]\\mathbb{E}[Y]$.\nBoth $X_1$ and $Y$ are Bernoulli variables. $\\mathbb{E}[Y] = \\pi = 0.02$ and $\\mathbb{E}[X_1] = p_1 = 0.01898$.\nThe product $X_1 Y = 1$ if and only if $X_1=1$ and $Y=1$. So, $\\mathbb{E}[X_1 Y] = \\mathbb{P}(X_1=1, Y=1) = \\mathbb{P}(X_1=1 \\mid Y=1)\\mathbb{P}(Y=1) = 0.9 \\times 0.02 = 0.018$.\n$\\mathrm{Cov}(X_1, Y) = 0.018 - (0.01898)(0.02) = 0.018 - 0.0003796 = 0.0176204$.\nThe variances are $\\mathrm{Var}(X_1) \\approx 0.01862$ and $\\mathrm{Var}(Y) = \\pi(1-\\pi) = 0.02(0.98) = 0.0196$.\n$$\n\\rho(X_1, Y) = \\frac{0.0176204}{\\sqrt{0.01862 \\times 0.0196}} \\approx \\frac{0.0176204}{\\sqrt{0.000364952}} \\approx \\frac{0.0176204}{0.01910} \\approx 0.9225\n$$\nA correlation of approximately $0.92$ is extremely high, not small. The premise of the statement is false, and so is its conclusion.\n**Verdict: Incorrect.**\n\n**D. Removing $X_1$ cannot worsen expected misclassification risk because any classifier could have chosen to ignore $X_1$ even if it were present.**\nThis is a fallacious argument. While an optimal classifier on the full feature set $\\{X_1, X_2\\}$ would achieve the minimal possible risk ($0.00298$), the feature selection procedure is a preceding, separate step. This step forces the removal of $X_1$ from the feature set available to the learning algorithm. The resulting classifier, built on the impoverished feature set (only the useless $X_2$), cannot \"choose\" to use $X_1$. Its optimal performance is limited by the information available to it, leading to a higher risk of $0.02$. The preprocessing step constrains the subsequent modeling step, and this constraint can provably worsen performance.\n**Verdict: Incorrect.**\n\n**E. This counterexample shows that variance is a property of the marginal distribution of a feature and does not capture conditional dependence on the label; a low-variance feature can have a large likelihood ratio $\\mathbb{P}(X_1=1 \\mid Y=1)\\big/\\mathbb{P}(X_1=1 \\mid Y=0)$ and substantial mutual information with $Y$.**\nThis statement accurately summarizes the conceptual lesson of the problem. Variance is calculated from the marginal distribution $\\mathbb{P}(X_1)$ and reflects how often the feature varies from its mean. Predictive power, however, stems from the conditional distributions $\\mathbb{P}(X_1 \\mid Y)$, which describe how the feature's distribution changes given the class label. Let's verify the likelihood ratio:\n$$\n\\frac{\\mathbb{P}(X_1=1 \\mid Y=1)}{\\mathbb{P}(X_1=1 \\mid Y=0)} = \\frac{0.9}{0.001} = 900\n$$\nThis is a very large likelihood ratio, indicating that observing $X_1=1$ is $900$ times more likely if the true class is $Y=1$ than if it is $Y=0$. This strong conditional dependence is what makes the feature so predictive, despite its low marginal variance (which is low because $X_1=1$ is a rare event overall). The high likelihood ratio and large change in risk imply a substantial mutual information $I(X_1; Y)$. The statement is a correct and insightful summary.\n**Verdict: Correct.**", "answer": "$$\\boxed{ABE}$$", "id": "3112623"}]}