{"hands_on_practices": [{"introduction": "To truly grasp the bootstrap method, it is invaluable to first perform the calculations by hand on a manageable scale. This exercise demystifies the process by asking you to compute the bootstrap standard error for a simple linear regression slope using a very small, hypothetical dataset. By manually calculating the statistic for each given resample and then finding the standard deviation of those results, you will gain a concrete, step-by-step understanding of how resampling generates an estimate of statistical uncertainty [@problem_id:851901].", "problem": "In statistical analysis, the non-parametric pairs bootstrap is a powerful resampling technique used to estimate the uncertainty of a statistic when its analytical distribution is unknown or relies on strong assumptions. This problem explores the application of the bootstrap method to estimate the standard error of a regression coefficient.\n\nConsider a simple linear regression model, $Y = \\beta_0 + \\beta_1 X + \\epsilon$, where the slope parameter $\\beta_1$ is estimated from a set of $n$ data pairs $\\{(x_i, y_i)\\}_{i=1}^n$. The ordinary least squares (OLS) estimator for the slope is given by:\n$$\n\\hat{\\beta}_1 = \\frac{S_{xy}}{S_{xx}} = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2}\n$$\nwhere $\\bar{x}$ and $\\bar{y}$ are the sample means of $X$ and $Y$, respectively.\n\nThe bootstrap procedure to estimate the standard error of $\\hat{\\beta}_1$ is as follows:\n1.  Draw $B$ bootstrap samples, $\\mathcal{D}^*_1, \\mathcal{D}^*_2, ..., \\mathcal{D}^*_B$, each of size $n$, by sampling with replacement from the original data set $\\mathcal{D}$.\n2.  For each bootstrap sample $\\mathcal{D}^*_b$, calculate the slope estimate, denoted as $\\hat{\\beta}_{1,b}^*$.\n3.  The bootstrap estimate of the standard error of $\\hat{\\beta}_1$ is the sample standard deviation of the $B$ bootstrap slope estimates:\n    $$\n    \\hat{\\text{se}}_{\\text{boot}}(\\hat{\\beta}_1) = \\sqrt{\\frac{1}{B-1} \\sum_{b=1}^{B} (\\hat{\\beta}_{1,b}^* - \\bar{\\beta}_1^*)^2}\n    $$\n    where $\\bar{\\beta}_1^* = \\frac{1}{B} \\sum_{b=1}^{B} \\hat{\\beta}_{1,b}^*$ is the mean of the bootstrap estimates.\n\n**Problem:**\nSuppose you have an original dataset of size $n=3$:\n$$\n\\mathcal{D} = \\{ (0,0), (1, \\alpha), (2, \\beta) \\}\n$$\nwhere $\\alpha$ and $\\beta$ are real-valued parameters, and it is given that $\\beta \\neq 2\\alpha$, which ensures the three points are not collinear.\n\nYou perform a bootstrap analysis with a very small number of bootstrap samples, $B=2$. The two resulting bootstrap samples are:\n1.  $\\mathcal{D}^*_1 = \\{ (0,0), (0,0), (1, \\alpha) \\}$\n2.  $\\mathcal{D}^*_2 = \\{ (0,0), (1, \\alpha), (2, \\beta) \\}$ (which is identical to the original sample $\\mathcal{D}$)\n\nDerive a closed-form expression for the bootstrap estimate of the standard error of the slope, $\\hat{\\text{se}}_{\\text{boot}}(\\hat{\\beta}_1)$, in terms of the parameters $\\alpha$ and $\\beta$.", "solution": "1. Relevant equations:\n   $\\displaystyle \\hat\\beta_1=\\frac{S_{xy}}{S_{xx}},\\quad S_{xy}=\\sum_{i=1}^n (x_i-\\bar x)(y_i-\\bar y),\\quad S_{xx}=\\sum_{i=1}^n (x_i-\\bar x)^2.$  \n   For $B=2$, the bootstrap standard error is  \n   $$\n   \\hat{\\mathrm{se}}_{\\mathrm{boot}}(\\hat\\beta_1)\n   =\\sqrt{\\frac{1}{B-1}\\sum_{b=1}^2\\bigl(\\hat\\beta_{1,b}^*-\\bar\\beta^*_1\\bigr)^2}\n   =\\sqrt{\\sum_{b=1}^2\\bigl(\\hat\\beta_{1,b}^*-\\bar\\beta^*_1\\bigr)^2}\\,,\n   $$\n   since $B-1=1$.\n\n2. Compute $\\hat\\beta_{1,1}^*$ for $\\mathcal D_1^*=\\{(0,0),(0,0),(1,\\alpha)\\}$.\n   $$\n   \\bar x_1^*=\\frac{0+0+1}{3}=\\frac13,\\quad \\bar y_1^*=\\frac{0+0+\\alpha}{3}=\\frac\\alpha3;\n   $$\n   $$\n   S_{xy}^{(1)}=2\\bigl(-\\tfrac13\\bigr)\\bigl(-\\tfrac\\alpha3\\bigr)+\\bigl(\\tfrac23\\bigr)\\bigl(\\tfrac{2\\alpha}3\\bigr)\n   =\\frac{2\\alpha}{9}+\\frac{4\\alpha}{9}=\\frac{2\\alpha}{3},\\quad\n   S_{xx}^{(1)}=2\\!\\bigl(\\tfrac{1}{3}\\bigr)^2+\\bigl(\\tfrac{2}{3}\\bigr)^2=\\frac{2}{3},\n   $$\n   $$\n   \\hat\\beta_{1,1}^*=\\frac{S_{xy}^{(1)}}{S_{xx}^{(1)}}=\\frac{\\tfrac{2\\alpha}{3}}{\\tfrac{2}{3}}=\\alpha.\n   $$\n\n3. Compute $\\hat\\beta_{1,2}^*$ for the original sample $\\{(0,0),(1,\\alpha),(2,\\beta)\\}$.\n   $$\n   \\bar x=1,\\quad \\bar y=\\frac{\\alpha+\\beta}{3},\\quad\n   S_{xy}=\\bar y+(\\beta-\\bar y)=\\beta,\\quad S_{xx}=1+0+1=2,\n   $$\n   $$\n   \\hat\\beta_{1,2}^*=\\frac{\\beta}{2}.\n   $$\n\n4. Mean of bootstrap slopes:\n   $$\n   \\bar\\beta^*_1=\\frac{\\alpha+\\tfrac{\\beta}{2}}{2}=\\frac{2\\alpha+\\beta}{4}.\n   $$\n   Deviations:\n   $$\n   \\hat\\beta_{1,1}^*-\\bar\\beta^*_1\n   =\\alpha-\\frac{2\\alpha+\\beta}{4}\n   =\\frac{2\\alpha-\\beta}{4},\\quad\n   \\hat\\beta_{1,2}^*-\\bar\\beta^*_1\n   =\\frac{\\beta}{2}-\\frac{2\\alpha+\\beta}{4}\n   =-\\frac{2\\alpha-\\beta}{4}.\n   $$\n   Sum of squared deviations:\n   $$\n   \\sum_{b=1}^2\\bigl(\\hat\\beta_{1,b}^*-\\bar\\beta^*_1\\bigr)^2\n   =2\\Bigl(\\frac{2\\alpha-\\beta}{4}\\Bigr)^2\n   =\\frac{(2\\alpha-\\beta)^2}{8}.\n   $$\n\n5. Bootstrap standard error:\n   $$\n   \\hat{\\mathrm{se}}_{\\mathrm{boot}}(\\hat\\beta_1)\n   =\\sqrt{\\frac{(2\\alpha-\\beta)^2}{8}}\n   =\\frac{|2\\alpha-\\beta|}{2\\sqrt2}.\n   $$", "answer": "$$\\boxed{\\frac{\\lvert\\beta-2\\alpha\\rvert}{2\\sqrt2}}$$", "id": "851901"}, {"introduction": "While the bootstrap is a powerful and broadly applicable tool, it is not infallible, and its properties are worth exploring. This next practice challenges you to move beyond the mechanics to investigate the theoretical performance of a bootstrap confidence interval. You will calculate the exact coverage probability for a percentile bootstrap interval in a specific, controlled scenario, revealing important insights into how the method's actual performance can compare to its nominal target [@problem_id:851841].", "problem": "The bootstrap is a powerful resampling method used in statistics to estimate the uncertainty of an estimator. This problem explores the theoretical properties of a non-parametric bootstrap confidence interval in a simplified setting.\n\nConsider a random sample of size $n=3$, denoted by $X_1, X_2, X_3$, drawn independently and identically from an exponential distribution with an unknown rate parameter $\\lambda$. The probability density function is given by $f(x; \\lambda) = \\lambda e^{-\\lambda x}$ for $x \\ge 0$. We are interested in estimating the median of this distribution, which we denote by $m$.\n\nA non-parametric percentile bootstrap confidence interval for the median is constructed as follows:\n1.  From the original observed sample $\\{x_1, x_2, x_3\\}$, a large number, $B$, of \"bootstrap samples\" are generated. Each bootstrap sample, $\\{x_1^*, x_2^*, x_3^*\\}$, is a new sample of size $n=3$ created by drawing with replacement from the original sample $\\{x_1, x_2, x_3\\}$.\n2.  For each of the $B$ bootstrap samples, the sample median, $\\hat{m}^*$, is calculated. The sample median of three numbers is the middle value when they are sorted.\n3.  The collection of these $B$ bootstrap medians, $\\{\\hat{m}^*_1, \\dots, \\hat{m}^*_B\\}$, forms an empirical distribution that approximates the sampling distribution of the sample median.\n4.  An approximate 95% confidence interval for the true median $m$ is given by the 2.5th and 97.5th percentiles of this empirical distribution.\n\nFor this problem, we consider the idealized case where the number of bootstrap resamples $B \\to \\infty$. In this limit, the percentiles are determined from the exact theoretical probability mass function of the bootstrap median $\\hat{m}^*$, conditional on the original sample. For a discrete distribution, the $p$-th percentile is defined as the minimum value $v$ such that the cumulative probability $P(\\hat{m}^* \\le v)$ is at least $p$.\n\nYour task is to derive the exact theoretical coverage probability of this 95% bootstrap confidence interval. The coverage probability is defined as the probability that the random interval, constructed from a sample $\\{X_1, X_2, X_3\\}$, successfully contains the true population median $m$.", "solution": "1. The bootstrap median $\\hat m^*$ from three draws with replacement from $\\{x_{(1)},x_{(2)},x_{(3)}\\}$ takes values $x_{(1)},x_{(2)},x_{(3)}$ with probabilities\n$$P(\\hat m^*=x_{(1)})=P(N_1\\ge2)=\\sum_{k=2}^3\\binom{3}{k}\\Bigl(\\tfrac13\\Bigr)^k\\Bigl(\\tfrac23\\Bigr)^{3-k}=\\frac7{27},$$\n$$P(\\hat m^*=x_{(3)})=P(N_3\\ge2)=\\frac7{27},\\quad\nP(\\hat m^*=x_{(2)})=1-\\frac{7+7}{27}=\\frac{13}{27}.$$\n2. The 2.5th percentile is the smallest $v$ with $P(\\hat m^*\\le v)\\ge0.025$. Since $P(\\hat m^*\\le x_{(1)})=7/270.025$, the lower bound is $x_{(1)}$.  The 97.5th percentile satisfies $P(\\hat m^*\\le x_{(2)})=20/270.975$ but $P(\\hat m^*\\le x_{(3)})=1\\ge0.975$, so the upper bound is $x_{(3)}$. Thus the interval is $[X_{(1)},X_{(3)}]$.\n3. The coverage is\n$$P\\bigl(X_{(1)}\\le m\\le X_{(3)}\\bigr)\n=1-P(X_{(3)}m)-P(X_{(1)}m).$$\nFor an exponential distribution with median $m$, $P(Xm)=\\tfrac12$ and $P(Xm)=\\tfrac12$. Hence\n$$P(X_{(3)}m)=\\Bigl(\\tfrac12\\Bigr)^3,\\quad P(X_{(1)}m)=\\Bigl(\\tfrac12\\Bigr)^3,$$\nso\n$$\\text{Coverage}=1-2\\Bigl(\\tfrac12\\Bigr)^3=1-\\tfrac14=\\tfrac34.$$", "answer": "$$\\boxed{\\frac{3}{4}}$$", "id": "851841"}, {"introduction": "Now let's apply our understanding to a practical data analysis challenge by translating the bootstrap algorithm into code. This problem introduces a sophisticated diagnostic technique called the jackknife-after-bootstrap, which you will implement to identify influential data points in a regression analysis. By systematically removing each data point and observing its effect on the bootstrap distribution, you will learn a powerful method for assessing the stability of your model and the impact of individual observations [@problem_id:3180777].", "problem": "You are given three independent test cases, each consisting of a fixed dataset of paired observations $\\{(x_i,y_i)\\}_{i=1}^n$. For each test case, you must use the bootstrap method combined with the jackknife-after-bootstrap procedure to quantify the influence of each observation on the bootstrap distribution of a chosen statistic and then identify high-influence points. The chosen statistic is the slope parameter of a simple linear regression fitted by Ordinary Least Squares (OLS), which minimizes the sum of squared residuals.\n\nFundamental base:\n- Ordinary Least Squares (OLS) estimates for simple linear regression minimize $S(b_0,b_1) = \\sum_{i=1}^n (y_i - b_0 - b_1 x_i)^2$. The first-order optimality conditions imply the estimator $\\hat{b}_1$ satisfies $\\hat{b}_1 = \\dfrac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2}$, where $\\bar{x} = \\dfrac{1}{n}\\sum_{i=1}^n x_i$ and $\\bar{y} = \\dfrac{1}{n}\\sum_{i=1}^n y_i$.\n- The bootstrap method approximates the sampling distribution of a statistic $\\theta$ by repeatedly drawing, with replacement, $n$ indices from $\\{1,\\dots,n\\}$ to form a bootstrap resample and computing the statistic on each resample. Let $B$ denote the number of bootstrap replicates. The empirical distribution of the $B$ bootstrap statistics approximates the true sampling distribution of $\\theta$.\n- The jackknife-after-bootstrap approach assesses the influence of each observation $i$ by recomputing the bootstrap distribution with that observation removed, yielding a new bootstrap mean for the statistic. Comparing this to the original bootstrap mean provides an influence measure.\n\nPrecise task:\n1. For each test case, compute the OLS slope statistic on each bootstrap resample of size $n$ using $B$ replicates to obtain a collection of slopes $\\{\\hat{b}_1^{*(b)}\\}_{b=1}^B$ from the full dataset. Let $\\bar{b}_{\\text{full}}$ be the mean of the valid bootstrap slopes and let $s_{\\text{full}}$ be their sample standard deviation, calculated using an N-1 denominator (Bessel's correction).\n2. For each observation index $i \\in \\{0,\\dots,n-1\\}$, remove that observation to form the reduced dataset of size $n-1$, generate $B$ bootstrap replicates on the reduced dataset (resampling $n-1$ points with replacement), compute the bootstrap slopes for this reduced dataset, and let $\\bar{b}_{(-i)}$ be the mean of the valid slopes from this reduced dataset.\n3. Define the influence measure for observation $i$ as\n$$\nI_i = \n\\begin{cases}\n\\dfrac{\\left|\\bar{b}_{(-i)} - \\bar{b}_{\\text{full}}\\right|}{s_{\\text{full}}},  \\text{if } s_{\\text{full}}  0 \\\\\n\\left|\\bar{b}_{(-i)} - \\bar{b}_{\\text{full}}\\right|,  \\text{if } s_{\\text{full}} = 0\n\\end{cases}\n$$\nand classify observation $i$ as high-influence if $I_i > \\tau$, where $\\tau$ is a given threshold.\n4. Implementation details you must respect:\n   - If a bootstrap resample yields zero variance in $x$ (that is, $\\sum_{j} (x_j - \\bar{x})^2 = 0$ for that resample), the OLS slope is undefined; you must discard that replicate and not include it in the computation of $\\bar{b}$ or $s$.\n   - If for a given dataset there are no valid bootstrap replicates at all (extremely unlikely for the provided test cases), define $I_i = 0$ for all $i$.\n   - Use $0$-based indexing for observations throughout.\n   - Do not use any physical units in your output; all results are unit-free real numbers or integers.\n\nYou must apply the above to the following test suite:\n\n- Test Case $1$:\n  - Data: $x = (0, 1, 2, 3, 10)$, $y = (0.5, 2.1, 3.9, 6.2, 50.0)$\n  - Replicates: $B = 1200$\n  - Threshold: $\\tau = 2.0$\n- Test Case $2$:\n  - Data: $x = (0, 1, 2, 3, 4, 5, 6, 7, 8, 9)$, $y = (0.1, 3.2, 6.1, 9.0, 12.0, 15.1, 18.2, 21.3, 24.1, 27.0)$\n  - Replicates: $B = 1200$\n  - Threshold: $\\tau = 2.0$\n- Test Case $3$:\n  - Data: $x = (0, 1, 1, 2, 2, 100)$, $y = (0.1, 0.7, 0.6, 1.0, 0.9, 55.0)$\n  - Replicates: $B = 1200$\n  - Threshold: $\\tau = 1.5$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element corresponds to one test case and must be a list of integers containing the $0$-based indices of the high-influence observations for that case, in ascending order. For example, the final output format must look like $[[i\\_1,i\\_2],[j\\_1,j\\_2,j\\_3],[]]$ where empty square brackets denote that no observation is high-influence for that case.", "solution": "The user's request is a well-defined computational statistics problem. The validation of the problem statement is as follows.\n\n**Problem Validation**\n\n1.  **Extract Givens**:\n    -   **Statistic of Interest**: The Ordinary Least Squares (OLS) slope parameter for simple linear regression, $\\hat{b}_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2}$.\n    -   **Core Method**: The bootstrap method, which involves generating $B$ resamples of size $n$ by drawing with replacement from the original dataset.\n    -   **Influence Assessment Procedure (Jackknife-after-Bootstrap)**:\n        1.  Compute the mean $\\bar{b}_{\\text{full}}$ and sample standard deviation $s_{\\text{full}}$ (using $N-1$ degrees of freedom for variance) from $B$ bootstrap slope replicates on the full dataset.\n        2.  For each observation $i \\in \\{0, \\dots, n-1\\}$, remove the $i$-th point to create a reduced dataset. Compute the mean $\\bar{b}_{(-i)}$ from $B$ new bootstrap replicates on this reduced dataset.\n    -   **Influence Measure ($I_i$)**: $I_i = \\frac{|\\bar{b}_{(-i)} - \\bar{b}_{\\text{full}}|}{s_{\\text{full}}}$ if $s_{\\text{full}}  0$, and $I_i = |\\bar{b}_{(-i)} - \\bar{b}_{\\text{full}}|$ if $s_{\\text{full}} = 0$.\n    -   **High-Influence Criterion**: An observation $i$ is classified as high-influence if $I_i  \\tau$.\n    -   **Constraints**:\n        -   Bootstrap replicates with zero variance in the explanatory variable ($x$) are discarded.\n        -   If a dataset (full or reduced) yields no valid bootstrap replicates, the corresponding influence $I_i$ is treated as $0$.\n        -   $0$-based indexing is required.\n    -   **Data Suite**: Three test cases are provided, each with a dataset $(x, y)$, a number of replicates $B$, and a threshold $\\tau$.\n\n2.  **Validate Using Extracted Givens**:\n    -   **Scientific Grounding**: The problem is firmly rooted in fundamental statistical theory. The OLS estimator is a cornerstone of regression analysis. The bootstrap is a canonical non-parametric technique for estimating sampling distributions. The described jackknife-after-bootstrap procedure is a valid, though computationally intensive, method for diagnosing influential data points. All definitions and formulas are correct. The problem is scientifically sound.\n    -   **Well-Posedness and Objectivity**: The problem is well-posed, providing a complete and unambiguous set of instructions, data, and edge-case handling rules. This ensures a unique, deterministic solution can be algorithmically derived. The use of a quantitative threshold $\\tau$ makes the classification objective.\n\n3.  **Verdict**: The problem is valid and can be solved as stated.\n\n**Principle-Based Solution Design**\n\nThe task requires implementing a multi-stage statistical analysis for each test case. The solution is designed by breaking down the procedure into modular, reusable functions, each grounded in a specific statistical principle.\n\n1.  **OLS Slope Calculation**:\n    The core statistic is the OLS slope parameter, $\\hat{b}_1$. For a given set of paired observations $\\{(x_j, y_j)\\}_{j=1}^m$, the estimator $\\hat{b}_1$ is calculated using the established formula:\n    $$\n    \\hat{b}_1 = \\frac{\\sum_{j=1}^m (x_j - \\bar{x})(y_j - \\bar{y})}{\\sum_{j=1}^m (x_j - \\bar{x})^2}\n    $$\n    where $\\bar{x} = \\frac{1}{m}\\sum_{j=1}^m x_j$ and $\\bar{y} = \\frac{1}{m}\\sum_{j=1}^m y_j$. A critical implementation detail is to handle cases where the denominator is zero, which occurs if all $x_j$ in a sample are identical. In such instances, the slope is undefined, and the replicate must be discarded as per the problem statement.\n\n2.  **Bootstrap Slope Distribution Generation**:\n    The bootstrap method provides a non-parametric estimate of a statistic's sampling distribution. This is achieved by simulating the sampling process by drawing data from the empirical distribution, which is the original sample itself. For a given dataset of size $m$, this involves:\n    -   Repeating $B$ times:\n        1.  Draw $m$ indices with replacement from $\\{0, 1, \\dots, m-1\\}$.\n        2.  Construct a bootstrap resample using the data at these indices.\n        3.  Compute the OLS slope $\\hat{b}_1^*$ on this resample.\n    -   Collect all valid slopes $\\{\\hat{b}_1^{*(b)}\\}_{b=1}^{B'}$ where $B' \\le B$.\n    This procedure will be applied to both the full dataset (size $n$) and each of the $n$ reduced datasets (size $n-1$).\n\n3.  **Main Procedure: Jackknife-after-Bootstrap Analysis**:\n    The main logic orchestrates the entire analysis for a single test case.\n    -   **Step A: Full Data Analysis**. First, we apply the bootstrap procedure to the complete dataset of size $n$ to generate a distribution of slopes. From this distribution, we calculate its mean, $\\bar{b}_{\\text{full}}$, and its sample standard deviation, $s_{\\text{full}}$. The standard deviation is computed with $N-1$ in the denominator (Bessel's correction), which corresponds to `ddof=1` in numerical libraries like NumPy. This establishes the baseline for our influence comparison.\n    -   **Step B: Leave-One-Out Analysis**. We then iterate through each observation $i$ from $0$ to $n-1$. In each iteration, we programmatically perform a jackknife deletion by removing observation $i$ from the dataset.\n    -   **Step C: Bootstrap on Reduced Data**. For each of these $n$ reduced datasets (of size $n-1$), we execute a full bootstrap procedure with $B$ replicates. We then calculate the mean of the resulting slope distribution, denoted $\\bar{b}_{(-i)}$. This value represents the typical slope estimate when observation $i$ is absent from the data generation process.\n    -   **Step D: Influence Calculation**. The influence of observation $i$, $I_i$, quantifies the change in the average bootstrap slope caused by its removal, standardized by the variability of the original bootstrap slopes. The formula is applied as specified:\n    $$\n    I_i = \n    \\begin{cases}\n    \\dfrac{\\left|\\bar{b}_{(-i)} - \\bar{b}_{\\text{full}}\\right|}{s_{\\text{full}}}  \\text{if } s_{\\text{full}}  0 \\\\\n    \\left|\\bar{b}_{(-i)} - \\bar{b}_{\\text{full}}\\right|  \\text{if } s_{\\text{full}} = 0\n    \\end{cases}\n    $$\n    -   **Step E: Identification**. Finally, we compare each influence measure $I_i$ against the provided threshold $\\tau$. If $I_i  \\tau$, the $0$-based index $i$ is recorded as a high-influence point. The collected indices for the test case are then sorted in ascending order.\n\nThis structured approach ensures all specifications of the problem are met precisely while maintaining a clear connection between the algorithm and the underlying statistical principles.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    It implements the jackknife-after-bootstrap procedure to identify high-influence points.\n    \"\"\"\n\n    def ols_slope(x, y):\n        \"\"\"\n        Calculates the OLS slope for a simple linear regression.\n\n        Args:\n            x (np.ndarray): The independent variable data.\n            y (np.ndarray): The dependent variable data.\n\n        Returns:\n            float: The OLS slope, or np.nan if the slope is undefined.\n        \"\"\"\n        n = len(x)\n        if n  2:\n            return np.nan\n\n        x_mean = np.mean(x)\n        \n        # Denominator of the slope formula\n        ss_xx = np.sum((x - x_mean)**2)\n\n        # As per problem, if variance in x is zero, the slope is undefined.\n        if ss_xx == 0:\n            return np.nan\n\n        y_mean = np.mean(y)\n        # Numerator of the slope formula\n        ss_xy = np.sum((x - x_mean) * (y - y_mean))\n        \n        return ss_xy / ss_xx\n\n    def get_bootstrap_slopes(x, y, B):\n        \"\"\"\n        Generates a distribution of OLS slopes using the bootstrap method.\n\n        Args:\n            x (np.ndarray): The independent variable data.\n            y (np.ndarray): The dependent variable data.\n            B (int): The number of bootstrap replicates.\n\n        Returns:\n            np.ndarray: An array of valid bootstrap slopes.\n        \"\"\"\n        n = len(x)\n        if n == 0:\n            return np.array([])\n        \n        slopes = []\n        # Pre-generate all random indices for performance\n        # Each row is a set of indices for one bootstrap replicate\n        all_indices = np.random.choice(n, size=(B, n), replace=True)\n\n        for resample_indices in all_indices:\n            x_resample = x[resample_indices]\n            y_resample = y[resample_indices]\n            \n            slope = ols_slope(x_resample, y_resample)\n            \n            # Discard replicates where the slope is undefined\n            if not np.isnan(slope):\n                slopes.append(slope)\n                \n        return np.array(slopes)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case 1\n        (np.array([0, 1, 2, 3, 10]), np.array([0.5, 2.1, 3.9, 6.2, 50.0]), 1200, 2.0),\n        # Test Case 2\n        (np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), np.array([0.1, 3.2, 6.1, 9.0, 12.0, 15.1, 18.2, 21.3, 24.1, 27.0]), 1200, 2.0),\n        # Test Case 3\n        (np.array([0, 1, 1, 2, 2, 100]), np.array([0.1, 0.7, 0.6, 1.0, 0.9, 55.0]), 1200, 1.5)\n    ]\n\n    all_results = []\n    for x_full, y_full, B, tau in test_cases:\n        n = len(x_full)\n        \n        # 1. Compute bootstrap statistics for the full dataset.\n        full_slopes = get_bootstrap_slopes(x_full, y_full, B)\n        \n        # Per problem: if no valid replicates, I_i = 0 for all i.\n        # This means no high-influence points.\n        if len(full_slopes) == 0:\n            all_results.append([])\n            continue\n\n        b_full_mean = np.mean(full_slopes)\n        \n        # Per problem: sample standard deviation (ddof=1).\n        # If less than 2 valid slopes, standard deviation is 0.\n        b_full_std = 0.0\n        if len(full_slopes) = 2:\n            b_full_std = np.std(full_slopes, ddof=1)\n            \n        high_influence_indices = []\n        # 2. Loop through each observation for jackknife-after-bootstrap.\n        for i in range(n):\n            # Create the reduced dataset by removing observation i.\n            x_reduced = np.delete(x_full, i)\n            y_reduced = np.delete(y_full, i)\n            \n            # Compute bootstrap mean for the reduced dataset.\n            reduced_slopes = get_bootstrap_slopes(x_reduced, y_reduced, B)\n            \n            # Per problem: if no valid replicates, I_i = 0.\n            if len(reduced_slopes) == 0:\n                influence_i = 0.0\n            else:\n                b_reduced_mean = np.mean(reduced_slopes)\n                \n                # 3. Define the influence measure for observation i.\n                diff = np.abs(b_reduced_mean - b_full_mean)\n                if b_full_std  0:\n                    influence_i = diff / b_full_std\n                else:\n                    influence_i = diff\n            \n            # 4. Classify observation i as high-influence.\n            if influence_i  tau:\n                high_influence_indices.append(i)\n        \n        # Results must be in ascending order.\n        all_results.append(sorted(high_influence_indices))\n\n    # Final print statement in the exact required format.\n    formatted_results = [f\"[{','.join(map(str, res))}]\" for res in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3180777"}]}