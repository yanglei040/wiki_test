{"hands_on_practices": [{"introduction": "The importance of stratification, particularly for imbalanced datasets, is a cornerstone of robust model evaluation. But what is the actual cost of ignoring it? This exercise provides a concrete, quantitative answer by guiding you through a scenario with severe class imbalance [@problem_id:3134712]. You will derive and compute the \"error inflation\"—the systematic increase in estimated error—that occurs when random, non-stratified folds are used, solidifying your understanding of why stratification is a critical procedure, not just an optional refinement.", "problem": "Consider a binary classification problem with severe class imbalance. Let the class label be $Y \\in \\{0,1\\}$ with population prior $\\mathbb{P}(Y=1)=\\pi$ and $\\mathbb{P}(Y=0)=1-\\pi$, where $\\pi \\ll 0.5$. Given a single real-valued feature $X$, suppose the class-conditional distributions are Gaussian with equal variance: $X \\mid Y=0 \\sim \\mathcal{N}(0,1)$ and $X \\mid Y=1 \\sim \\mathcal{N}(\\mu,1)$, with $\\mu0$ known. Consider $k$-fold cross-validation (CV) performed on a dataset of size $N$, with $k$ equally sized folds.\n\nUse the following fundamental base:\n- The Bayes classifier under known class-conditional densities and prior uses the likelihood ratio test, which reduces to a threshold rule on $X$ when the variances are equal.\n- The misclassification risk is defined as the population error $\\mathbb{P}(\\hat{Y}\\neq Y)$, computed under the true data distribution.\n- A non-stratified $k$-fold split is a random partition of the $N$ samples into $k$ folds without regard to class labels.\n- A stratified $k$-fold split ensures each fold reflects the population class proportion as closely as possible. In this problem, assume exact stratification: $N\\pi$ is an integer and each fold contains exactly $N\\pi/k$ samples from the minority class and $N(1-\\pi)/k$ from the majority class.\n\nDefine the classifier trained within a fold as the likelihood-ratio threshold rule that uses the empirical class prior $\\hat{\\pi}$ estimated from the training portion of that fold, while using the true, known class-conditional densities. Specifically, the decision rule is \"predict $Y=1$ if $X \\ge t(\\hat{\\pi})$ and $Y=0$ otherwise\", where $t(\\hat{\\pi})$ is the threshold implied by the likelihood ratio test with prior $\\hat{\\pi}$. The fold’s test error is measured on data drawn from the same population.\n\nYou must:\n1. Derive from first principles the decision threshold $t(\\hat{\\pi})$ for the Gaussian equal-variance case starting from the likelihood ratio test, and the corresponding misclassification risk $R(t)$ as a function of $t$, $\\pi$, and $\\mu$.\n2. Under non-stratified $k$-fold, model the training prior $\\hat{\\pi}$ as a random variable induced by the random fold assignment. Let $M=N\\pi$ be the total number of minority samples (an integer by assumption), and $n_{\\text{train}}=N(k-1)/k$ be the training size per fold. For a given fold, the number of minority samples in the training set, $m$, follows a Hypergeometric distribution with parameters $(N, M, n_{\\text{train}})$. Therefore, $\\hat{\\pi}=m/n_{\\text{train}}$. Using this, compute the expected non-stratified CV risk as the expectation of $R\\big(t(\\hat{\\pi})\\big)$ with respect to the Hypergeometric distribution of $m$.\n3. Under stratified $k$-fold with exact per-fold class proportions, the training prior equals the population prior $\\pi$ exactly, hence the stratified CV risk equals $R\\big(t(\\pi)\\big)$.\n4. Quantify the error inflation $\\Delta$ due to non-stratification as\n$$\n\\Delta = \\mathbb{E}\\left[R\\big(t(\\hat{\\pi})\\big)\\right] - R\\big(t(\\pi)\\big).\n$$\nCompute $\\Delta$ for each test case specified below.\n\nYou must implement a complete, runnable program that, for each parameter tuple $(N,k,\\pi,\\mu)$, computes the single-fold expected non-stratified CV risk via the exact Hypergeometric distribution, the stratified CV risk, and outputs the inflation $\\Delta$ as a float. Handle the degenerate cases $\\hat{\\pi}=0$ and $\\hat{\\pi}=1$ correctly by interpreting the threshold rule limits: if $\\hat{\\pi}=0$, the classifier predicts $Y=0$ always, yielding risk $R=\\pi$; if $\\hat{\\pi}=1$, the classifier predicts $Y=1$ always, yielding risk $R=1-\\pi$.\n\nTest Suite:\n- Case $1$ (happy path): $(N,k,\\pi,\\mu) = (500,5,0.05,2.0)$.\n- Case $2$ (small data, still severely imbalanced): $(N,k,\\pi,\\mu) = (100,10,0.10,1.5)$.\n- Case $3$ (large data, extreme imbalance): $(N,k,\\pi,\\mu) = (5000,10,0.01,1.0)$.\n- Case $4$ (moderate data, strong separation): $(N,k,\\pi,\\mu) = (1200,4,0.03,3.0)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result_1,result_2,result_3,result_4]$), where each $result_i$ is the computed $\\Delta$ for Case $i$, rounded to $6$ decimal places. No other text should be printed.", "solution": "The problem requires us to quantify the error inflation, denoted by $\\Delta$, that arises from using non-stratified $k$-fold cross-validation compared to stratified $k$-fold cross-validation for a binary classifier with a severe class imbalance. The analysis is based on a specific theoretical setup involving Gaussian class-conditional distributions. We will proceed by first deriving the necessary theoretical components, then applying them to the specified cross-validation schemes.\n\n### 1. Decision Threshold and Misclassification Risk\n\nThe classifier's decision is based on the likelihood ratio test, which compares the posterior probabilities of the two classes. For a given feature value $X$, the classifier predicts class $Y=1$ if $\\mathbb{P}(Y=1|X)  \\mathbb{P}(Y=0|X)$. Using Bayes' theorem, this is equivalent to $\\frac{p(X|Y=1)\\mathbb{P}(Y=1)}{p(X)}  \\frac{p(X|Y=0)\\mathbb{P}(Y=0)}{p(X)}$, which simplifies to the likelihood ratio test:\n$$\n\\frac{p(X|Y=1)}{p(X|Y=0)}  \\frac{\\mathbb{P}(Y=0)}{\\mathbb{P}(Y=1)}\n$$\nThe problem specifies that the classifier uses the true class-conditional densities but estimates the prior probability $\\mathbb{P}(Y=1)$ with its empirical estimate $\\hat{\\pi}$ from the training data. The decision rule thus becomes:\n$$\n\\frac{p(X|Y=1)}{p(X|Y=0)}  \\frac{1-\\hat{\\pi}}{\\hat{\\pi}}\n$$\nThe class-conditional densities are given as $X \\mid Y=0 \\sim \\mathcal{N}(0,1)$ and $X \\mid Y=1 \\sim \\mathcal{N}(\\mu,1)$. Their probability density functions (PDFs) are:\n$$\np(X|Y=0) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{X^2}{2}\\right)\n$$\n$$\np(X|Y=1) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{(X-\\mu)^2}{2}\\right)\n$$\nThe likelihood ratio is:\n$$\n\\frac{p(X|Y=1)}{p(X|Y=0)} = \\frac{\\exp\\left(-\\frac{(X-\\mu)^2}{2}\\right)}{\\exp\\left(-\\frac{X^2}{2}\\right)} = \\exp\\left(\\frac{X^2 - (X-\\mu)^2}{2}\\right) = \\exp\\left(\\frac{X^2 - (X^2 - 2\\mu X + \\mu^2)}{2}\\right) = \\exp\\left(\\mu X - \\frac{\\mu^2}{2}\\right)\n$$\nSubstituting this into the decision rule inequality:\n$$\n\\exp\\left(\\mu X - \\frac{\\mu^2}{2}\\right)  \\frac{1-\\hat{\\pi}}{\\hat{\\pi}}\n$$\nTaking the natural logarithm of both sides (a monotonic transformation that preserves the inequality):\n$$\n\\mu X - \\frac{\\mu^2}{2}  \\ln\\left(\\frac{1-\\hat{\\pi}}{\\hat{\\pi}}\\right)\n$$\nSince $\\mu  0$ is given, we can solve for $X$ to find the decision threshold $t(\\hat{\\pi})$:\n$$\nX  \\frac{1}{\\mu}\\left(\\frac{\\mu^2}{2} + \\ln\\left(\\frac{1-\\hat{\\pi}}{\\hat{\\pi}}\\right)\\right)\n$$\nThus, the decision threshold as a function of the empirical prior $\\hat{\\pi}$ is:\n$$\nt(\\hat{\\pi}) = \\frac{\\mu}{2} + \\frac{1}{\\mu}\\ln\\left(\\frac{1-\\hat{\\pi}}{\\hat{\\pi}}\\right)\n$$\nThe classifier's rule is to predict $Y=1$ if $X \\geq t(\\hat{\\pi})$ and $Y=0$ otherwise.\n\nNext, we derive the misclassification risk $R(t)$ for a given threshold $t$. The risk is the population error rate, evaluated using the true population prior $\\pi$. It is the sum of the probabilities of Type I and Type II errors, weighted by the true class priors:\n$$\nR(t) = \\mathbb{P}(\\text{predict } 1 \\mid Y=0)\\mathbb{P}(Y=0) + \\mathbb{P}(\\text{predict } 0 \\mid Y=1)\\mathbb{P}(Y=1)\n$$\n$$\nR(t) = \\mathbb{P}(X \\ge t \\mid Y=0)(1-\\pi) + \\mathbb{P}(X  t \\mid Y=1)\\pi\n$$\nLet $\\Phi(\\cdot)$ be the cumulative distribution function (CDF) of the standard normal distribution $\\mathcal{N}(0,1)$.\nFor the first term, since $X \\mid Y=0 \\sim \\mathcal{N}(0,1)$, we have $\\mathbb{P}(X \\ge t \\mid Y=0) = 1 - \\Phi(t)$.\nFor the second term, since $X \\mid Y=1 \\sim \\mathcal{N}(\\mu,1)$, the variable $X-\\mu$ follows $\\mathcal{N}(0,1)$. Thus, $\\mathbb{P}(X  t \\mid Y=1) = \\mathbb{P}(X-\\mu  t-\\mu \\mid Y=1) = \\Phi(t-\\mu)$.\nThe misclassification risk is therefore:\n$$\nR(t) = (1-\\pi)(1 - \\Phi(t)) + \\pi \\Phi(t-\\mu)\n$$\n\n### 2. Stratified vs. Non-Stratified Cross-Validation Risk\n\n**Stratified CV Risk:**\nIn a stratified $k$-fold split, each fold is constructed to have the same class proportion as the overall dataset. The training set for any fold, which comprises $k-1$ folds, will therefore also have this exact proportion. Thus, the empirical prior $\\hat{\\pi}$ estimated from the training set is always equal to the population prior $\\pi$.\n$$\n\\hat{\\pi}_{\\text{strat}} = \\pi\n$$\nThe decision threshold is constant for all folds: $t_{\\text{strat}} = t(\\pi) = \\frac{\\mu}{2} + \\frac{1}{\\mu}\\ln\\left(\\frac{1-\\pi}{\\pi}\\right)$. This is the optimal Bayes threshold. The resulting risk, which we denote as $R_{\\text{strat}}$, is the Bayes error rate:\n$$\nR_{\\text{strat}} = R(t(\\pi)) = (1-\\pi)(1 - \\Phi(t(\\pi))) + \\pi \\Phi(t(\\pi)-\\mu)\n$$\n\n**Non-Stratified CV Risk:**\nIn a non-stratified split, the data is partitioned randomly into $k$ folds. The number of minority class samples in the training data for a fold is a random variable. Let $N$ be the total sample size, $M=N\\pi$ be the total number of minority samples, and $n_{\\text{train}} = N(k-1)/k$ be the training set size. The number of minority samples in the training set, $m$, follows a Hypergeometric distribution with parameters for population size ($N$), number of successes in population ($M$), and number of draws ($n_{\\text{train}}$). We write this as $m \\sim \\text{Hypergeometric}(N, M, n_{\\text{train}})$.\nThe probability mass function (PMF) is $\\mathbb{P}(m) = \\frac{\\binom{M}{m}\\binom{N-M}{n_{\\text{train}}-m}}{\\binom{N}{n_{\\text{train}}}}$.\nThe empirical prior for a given fold is $\\hat{\\pi} = m/n_{\\text{train}}$, which is a random variable. The decision threshold $t(\\hat{\\pi})$ and the corresponding risk $R(t(\\hat{\\pi}))$ are also random variables. The expected non-stratified CV risk is the expectation of this risk over the distribution of $m$:\n$$\n\\mathbb{E}\\left[R\\big(t(\\hat{\\pi})\\big)\\right] = \\sum_{m} \\mathbb{P}(m) \\cdot R\\left(t\\left(\\frac{m}{n_{\\text{train}}}\\right)\\right)\n$$\nThe summation is over the support of the Hypergeometric distribution, $m \\in [\\max(0, n_{\\text{train}} - (N-M)), \\min(n_{\\text{train}}, M)]$.\n\nThe problem specifies handling for the edge cases where $\\hat{\\pi}=0$ or $\\hat{\\pi}=1$:\n-   If $\\hat{\\pi}=0$ (i.e., $m=0$), $\\ln((1-\\hat{\\pi})/\\hat{\\pi}) \\to \\infty$, so $t(0) \\to \\infty$. The classifier always predicts $Y=0$. The risk is $R(\\infty) = (1-\\pi)(1-\\Phi(\\infty)) + \\pi \\Phi(\\infty-\\mu) = (1-\\pi) \\cdot 0 + \\pi \\cdot 1 = \\pi$.\n-   If $\\hat{\\pi}=1$ (i.e., $m=n_{\\text{train}}$), $\\ln((1-\\hat{\\pi})/\\hat{\\pi}) \\to -\\infty$, so $t(1) \\to -\\infty$. The classifier always predicts $Y=1$. The risk is $R(-\\infty) = (1-\\pi)(1-\\Phi(-\\infty)) + \\pi \\Phi(-\\infty-\\mu) = (1-\\pi) \\cdot 1 + \\pi \\cdot 0 = 1-\\pi$.\n\n### 3. Error Inflation $\\Delta$\n\nThe error inflation $\\Delta$ is defined as the difference between the expected risk under non-stratified CV and the risk under stratified CV:\n$$\n\\Delta = \\mathbb{E}\\left[R\\big(t(\\hat{\\pi})\\big)\\right] - R_{\\text{strat}}\n$$\nThis quantity captures the average increase in misclassification error due to the variability of the empirical prior estimate in non-stratified folds. This variability pushes the decision threshold away from the optimal Bayes threshold, and due to the convexity of the risk function around its minimum (the Bayes error), the expected risk is higher than the risk at the expected prior. By Jensen's inequality, since the risk function is convex, $\\mathbb{E}[R(t(\\hat{\\pi}))] \\ge R(t(\\mathbb{E}[\\hat{\\pi}]))$. Since $\\mathbb{E}[\\hat{\\pi}] = \\pi$, we expect $\\Delta \\ge 0$. Our task is to compute this value for the given test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import hypergeom, norm\n\ndef solve():\n    \"\"\"\n    Computes the error inflation delta for a series of test cases.\n    \"\"\"\n    \n    # Test cases: tuples of (N, k, pi, mu)\n    test_cases = [\n        (500, 5, 0.05, 2.0),\n        (100, 10, 0.10, 1.5),\n        (5000, 10, 0.01, 1.0),\n        (1200, 4, 0.03, 3.0),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        N, k, pi, mu = case\n        \n        # Ensure N*pi is an integer as per problem statement\n        M = int(round(N * pi))\n        n_train = int(N * (k - 1) / k)\n        \n        # --- 1. Calculate Stratified CV Risk (R_strat) ---\n        # The empirical prior is the true prior\n        pi_strat = pi\n        \n        # Calculate the Bayes optimal threshold t(pi)\n        # logit(pi) = log(pi / (1-pi)), so -log((1-pi)/pi)\n        t_strat = mu / 2.0 - (1.0 / mu) * np.log(pi_strat / (1.0 - pi_strat))\n        \n        # Calculate the risk R(t(pi)), which is the Bayes error rate\n        risk_type1_strat = 1.0 - norm.cdf(t_strat)\n        risk_type2_strat = norm.cdf(t_strat - mu)\n        R_strat = (1.0 - pi) * risk_type1_strat + pi * risk_type2_strat\n        \n        # --- 2. Calculate Expected Non-Stratified CV Risk (E_non_strat) ---\n        \n        # The number of minority samples 'm' in the training set follows a\n        # Hypergeometric distribution.\n        # Scipy's hypergeom(M, n, N) takes:\n        # M: total number of objects (our N)\n        # n: total number of type I objects (our M)\n        # N: number of draws (our n_train)\n        dist = hypergeom(M=N, n=M, N=n_train)\n        \n        # Support of the hypergeometric distribution for m\n        m_min = max(0, n_train - (N - M))\n        m_max = min(n_train, M)\n        \n        E_non_strat = 0.0\n        \n        for m in range(m_min, m_max + 1):\n            prob_m = dist.pmf(m)\n            \n            # If there's no probability mass, skip to avoid unnecessary calculation\n            if prob_m == 0:\n                continue\n\n            pi_hat = m / n_train\n            \n            # Handle edge cases as per problem statement\n            if m == 0:  # pi_hat = 0\n                risk_m = pi\n            elif m == n_train:  # pi_hat = 1\n                risk_m = 1.0 - pi\n            else:\n                # Calculate threshold t(pi_hat) for this m\n                t_m = mu / 2.0 - (1.0 / mu) * np.log(pi_hat / (1.0 - pi_hat))\n                \n                # Calculate risk R(t(pi_hat))\n                risk_type1_m = 1.0 - norm.cdf(t_m)\n                risk_type2_m = norm.cdf(t_m - mu)\n                risk_m = (1.0 - pi) * risk_type1_m + pi * risk_type2_m\n            \n            E_non_strat += prob_m * risk_m\n            \n        # --- 3. Compute Error Inflation Delta ---\n        delta = E_non_strat - R_strat\n        results.append(round(delta, 6))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3134712"}, {"introduction": "Stratification is a powerful tool, but it encounters practical limits when dealing with extremely rare classes. If a class has too few samples relative to the number of folds $k$, a validation set might end up with zero instances of that class, making metrics like recall or precision undefined. This exercise delves into this crucial implementation challenge, asking you to derive the minimum per-class support needed for a stable evaluation [@problem_id:3177513]. You will also develop a Laplace smoothing scheme, a standard method for ensuring that performance metrics remain robust and well-defined even in these sparse data situations.", "problem": "A dataset for a multi-class classification task contains a particular class labeled $c$ with total count $n_c$. You plan to evaluate a classifier using stratified $k$-fold cross-validation, where each fold uses one stratified split as the validation set and the remaining $k-1$ splits as the training set. Stratification means each fold’s validation set receives either $\\lfloor n_c/k \\rfloor$ or $\\lceil n_c/k \\rceil$ instances of class $c$, and the corresponding training set receives the complementary count of class $c$.\n\nPer-class recall for class $c$ computed on a validation fold is defined from the confusion matrix entries as $\\mathrm{Recall}_c = \\frac{\\mathrm{TP}_c}{\\mathrm{TP}_c + \\mathrm{FN}_c}$, where $\\mathrm{TP}_c$ and $\\mathrm{FN}_c$ denote the number of true positives and false negatives for class $c$ in that fold’s validation set. Per-class precision is defined as $\\mathrm{Precision}_c = \\frac{\\mathrm{TP}_c}{\\mathrm{TP}_c + \\mathrm{FP}_c}$, where $\\mathrm{FP}_c$ denotes the number of false positives for class $c$ in the validation set. A metric is undefined in a fold if its denominator equals $0$.\n\nStarting only from the definitions above and the stratified $k$-fold allocation rule, derive a conservative minimal per-class support requirement on $n_c$ (as a function of $k$) that guarantees, in every fold’s validation set, strictly positive actual support for class $c$ (so that the recall denominator is strictly positive) while also ensuring the corresponding training set for that fold has at least one instance of class $c$. To eliminate rounding edge cases, impose the stronger requirement that every validation fold must contain at least $2$ instances of class $c$.\n\nThen, for the regime $n_c  2k$, propose an additive-Laplace smoothing scheme that makes per-class precision and recall well-defined in every fold regardless of zero counts. Specifically, apply a symmetric additive constant $\\alpha  0$ to each confusion-matrix cell of the one-versus-rest decomposition for class $c$ in the validation set, and derive closed-form expressions for the smoothed per-class precision and smoothed per-class recall in terms of $\\mathrm{TP}_c$, $\\mathrm{FP}_c$, $\\mathrm{FN}_c$, and $\\alpha$.\n\nYour final answer must list:\n- the minimal $n_c$ in terms of $k$ that suffices under the stated conservative requirement, and\n- the analytic expressions for the Laplace-smoothed per-class precision and recall.\n\nExpress all quantities symbolically. No numerical rounding is required.", "solution": "This problem consists of two parts. The first part is to determine the minimum total count $n_c$ for a class $c$ to satisfy certain conditions in a stratified $k$-fold cross-validation setup. The second part is to derive expressions for Laplace-smoothed per-class precision and recall.\n\nPart 1: Minimal Per-Class Support Requirement\n\nLet $n_c$ be the total number of instances of a class $c$ in the dataset, and let $k$ be the number of folds in the cross-validation, where $k$ is an integer greater than or equal to $2$.\n\nIn stratified $k$-fold cross-validation, the $n_c$ instances are partitioned into $k$ disjoint subsets (folds). For any given fold $i$ (where $i \\in \\{1, 2, \\dots, k\\}$), that fold's subset serves as the validation set, and the union of the other $k-1$ subsets serves as the training set.\n\nLet $n_{c, \\text{val}}^{(i)}$ be the number of instances of class $c$ in the validation set of fold $i$. According to the problem's stratification rule, this number is given by:\n$$n_{c, \\text{val}}^{(i)} \\in \\{\\lfloor n_c/k \\rfloor, \\lceil n_c/k \\rceil\\}$$\nLet $n_{c, \\text{train}}^{(i)}$ be the number of instances of class $c$ in the corresponding training set for fold $i$. Since the training and validation sets are complementary with respect to the total set of instances of class $c$:\n$$n_{c, \\text{train}}^{(i)} = n_c - n_{c, \\text{val}}^{(i)}$$\nThe problem imposes two requirements that must be met for *every* fold $i$:\n1. The validation set for each fold must contain at least $2$ instances of class $c$.\n2. The training set for each fold must contain at least $1$ instance of class $c$.\n\nLet us formalize these requirements as inequalities.\nRequirement 1: For all $i \\in \\{1, 2, \\dots, k\\}$,\n$$n_{c, \\text{val}}^{(i)} \\geq 2$$\nTo guarantee this for every fold, the condition must hold even for the fold that receives the minimum possible number of instances. The minimum number of instances allocated to any fold's validation set is $\\lfloor n_c/k \\rfloor$. Therefore, we must enforce:\n$$\\lfloor n_c/k \\rfloor \\geq 2$$\nBy the definition of the floor function, if $\\lfloor x \\rfloor \\geq m$ for an integer $m$, then $x \\geq m$. Applying this, we get:\n$$\\frac{n_c}{k} \\geq 2 \\implies n_c \\geq 2k$$\n\nRequirement 2: For all $i \\in \\{1, 2, \\dots, k\\}$,\n$$n_{c, \\text{train}}^{(i)} \\geq 1$$\nSubstituting the expression for the training set count, we have:\n$$n_c - n_{c, \\text{val}}^{(i)} \\geq 1$$\nTo guarantee this for every fold, the condition must hold even when the training set is at its smallest. The training set size $n_{c, \\text{train}}^{(i)}$ is minimized when the corresponding validation set size $n_{c, \\text{val}}^{(i)}$ is maximized. The maximum number of instances allocated to any fold's validation set is $\\lceil n_c/k \\rceil$. Therefore, we must enforce:\n$$n_c - \\lceil n_c/k \\rceil \\geq 1$$\n\nWe now have two conditions on $n_c$: $n_c \\geq 2k$ and $n_c - \\lceil n_c/k \\rceil \\geq 1$. The final minimal value of $n_c$ must satisfy both. Let's check if the first, more stringent-appearing condition ($n_c \\geq 2k$) is sufficient to satisfy the second one.\n\nAssume $n_c \\geq 2k$. We know that for any real number $x$, $\\lceil x \\rceil  x+1$. Let $x = n_c/k$.\n$$n_c - \\lceil n_c/k \\rceil  n_c - \\left(\\frac{n_c}{k} + 1\\right) = n_c \\left(1 - \\frac{1}{k}\\right) - 1 = \\frac{n_c(k-1)}{k} - 1$$\nSince we assumed $n_c \\geq 2k$, we can substitute this into the inequality:\n$$\\frac{n_c(k-1)}{k} - 1 \\geq \\frac{2k(k-1)}{k} - 1 = 2(k-1) - 1 = 2k - 2 - 1 = 2k - 3$$\nSo, we have established that $n_c - \\lceil n_c/k \\rceil  2k-3$. For the condition $n_c - \\lceil n_c/k \\rceil \\geq 1$ to hold, we need $2k-3 \\geq 0$ (as the left side is an integer). This is true for $k \\geq 1.5$. Since $k$ represents the number of folds, $k \\geq 2$ is a standard assumption, so this is satisfied.\nThus, the condition $n_c \\geq 2k$ is the stricter of the two and is sufficient to guarantee both requirements.\n\nThe question asks for the minimal per-class support $n_c$. Since $n_c$ must be an integer, the minimal value that satisfies $n_c \\geq 2k$ is $n_c = 2k$.\n\nPart 2: Laplace-Smoothed Metrics\n\nThe problem asks for expressions for smoothed per-class precision and recall. The smoothing is performed by adding a constant $\\alpha  0$ to each cell of the one-versus-rest confusion matrix for class $c$.\n\nThe one-versus-rest confusion matrix for class $c$ has four cells based on predictions made on a validation set:\n- $\\mathrm{TP}_c$: True Positives (correctly predicted as class $c$)\n- $\\mathrm{FN}_c$: False Negatives (incorrectly predicted as not class $c$)\n- $\\mathrm{FP}_c$: False Positives (incorrectly predicted as class $c$)\n- $\\mathrm{TN}_c$: True Negatives (correctly predicted as not class $c$)\n\nApplying the symmetric additive smoothing, the smoothed counts become:\n- $\\mathrm{TP}'_c = \\mathrm{TP}_c + \\alpha$\n- $\\mathrm{FN}'_c = \\mathrm{FN}_c + \\alpha$\n- $\\mathrm{FP}'_c = \\mathrm{FP}_c + \\alpha$\n- $\\mathrm{TN}'_c = \\mathrm{TN}_c + \\alpha$\n\nThe standard definition for per-class recall for class $c$ is:\n$$\\mathrm{Recall}_c = \\frac{\\mathrm{TP}_c}{\\mathrm{TP}_c + \\mathrm{FN}_c}$$\nThe denominator $\\mathrm{TP}_c + \\mathrm{FN}_c$ represents the total number of actual instances of class $c$ in the validation set.\n\nThe smoothed recall, $\\mathrm{Recall}'_c$, is derived by substituting the smoothed counts into this formula:\n$$\\mathrm{Recall}'_c = \\frac{\\mathrm{TP}'_c}{\\mathrm{TP}'_c + \\mathrm{FN}'_c} = \\frac{\\mathrm{TP}_c + \\alpha}{(\\mathrm{TP}_c + \\alpha) + (\\mathrm{FN}_c + \\alpha)} = \\frac{\\mathrm{TP}_c + \\alpha}{\\mathrm{TP}_c + \\mathrm{FN}_c + 2\\alpha}$$\n\nThe standard definition for per-class precision for class $c$ is:\n$$\\mathrm{Precision}_c = \\frac{\\mathrm{TP}_c}{\\mathrm{TP}_c + \\mathrm{FP}_c}$$\nThe denominator $\\mathrm{TP}_c + \\mathrm{FP}_c$ represents the total number of instances predicted as class $c$.\n\nThe smoothed precision, $\\mathrm{Precision}'_c$, is derived by substituting the smoothed counts into this formula:\n$$\\mathrm{Precision}'_c = \\frac{\\mathrm{TP}'_c}{\\mathrm{TP}'_c + \\mathrm{FP}'_c} = \\frac{\\mathrm{TP}_c + \\alpha}{(\\mathrm{TP}_c + \\alpha) + (\\mathrm{FP}_c + \\alpha)} = \\frac{\\mathrm{TP}_c + \\alpha}{\\mathrm{TP}_c + \\mathrm{FP}_c + 2\\alpha}$$\nSince $\\alpha  0$ and the counts ($\\mathrm{TP}_c$, $\\mathrm{FP}_c$, $\\mathrm{FN}_c$) are non-negative integers, the denominators of these smoothed metrics are guaranteed to be strictly positive, ensuring the metrics are always well-defined.\n\nThe final answer requires three components: the minimal $n_c$ in terms of $k$, the expression for smoothed precision, and the expression for smoothed recall.\n1. Minimal $n_c$: $2k$\n2. Smoothed Precision: $\\frac{\\mathrm{TP}_c + \\alpha}{\\mathrm{TP}_c + \\mathrm{FP}_c + 2\\alpha}$\n3. Smoothed Recall: $\\frac{\\mathrm{TP}_c + \\alpha}{\\mathrm{TP}_c + \\mathrm{FN}_c + 2\\alpha}$", "answer": "$$\\boxed{\\begin{pmatrix} 2k  \\frac{\\mathrm{TP}_c + \\alpha}{\\mathrm{TP}_c + \\mathrm{FP}_c + 2\\alpha}  \\frac{\\mathrm{TP}_c + \\alpha}{\\mathrm{TP}_c + \\mathrm{FN}_c + 2\\alpha} \\end{pmatrix}}$$", "id": "3177513"}, {"introduction": "After establishing why and how to use stratification, a key practical question remains: what is the best value for $k$? This decision involves a fundamental trade-off between statistical performance and computational cost; a larger $k$ typically reduces the variance of the error estimate but requires more training time. This exercise moves beyond simple heuristics by framing the choice of $k$ as a constrained optimization problem [@problem_id:3177490]. You will find the optimal $k$ that minimizes estimator variance subject to a fixed computational budget, developing a crucial skill for designing efficient and effective machine learning experiments.", "problem": "Consider a binary classification dataset with total sample size $n=1200$, consisting of a majority class with $n_{+}=900$ samples and a minority class with $n_{-}=300$ samples. You plan to evaluate a fixed classifier using stratified $k$-fold Cross-Validation (CV), where the folds preserve the overall class proportions. Let the stratified $k$-fold CV risk estimator be denoted by $\\hat{R}_{\\mathrm{CV}}(k)$, defined as the arithmetic mean of the $k$ held-out fold risks.\n\nAssume the following modeling and computational facts are established from prior measurement and standard scaling:\n- The variance of the risk estimator obeys the parametric model $V(k)\\approx \\frac{a}{k}+b$ with constants $a0$ and $b\\ge 0$ that summarize data- and model-dependent variability. A pilot study yields $a=0.6$ and $b=0.01$.\n- The training runtime for the classifier scales linearly with the number of training samples $m$, i.e., the per-fold training time is $t(m)=c\\,m$, with $c0$ constant. A pilot fit on the full training set of size $n=1200$ took $60$ seconds, implying $c=\\frac{60}{1200}$ seconds per sample.\n- The total runtime for stratified $k$-fold CV is dominated by model fits and is given by $T(k)=k\\,t((1-\\frac{1}{k})\\,n)=c\\,n\\,(k-1)$.\n- The runtime budget is $T_{\\max}=220$ seconds.\n\nIn stratified $k$-fold CV, to ensure each fold’s test split contains at least one sample from each class and that class proportions are preserved without empty strata, you must satisfy the stratification feasibility bound $k\\le \\min\\{n_{+},n_{-}\\}$.\n\nUsing only the definitions of $\\hat{R}_{\\mathrm{CV}}(k)$ as an average of $k$ held-out risks, the given variance model $V(k)$, and the linear-time training scaling, derive the integer $k^{*}$ that minimizes $V(k)$ subject to the constraints $T(k)\\le T_{\\max}$ and stratification feasibility. Report $k^{*}$ as a single integer. No rounding beyond what is implied by integrality is required. Additionally, verify that your chosen $k^{*}$ admits a stratified partition by stating the per-fold test split sizes for the majority and minority classes (do not include these verification values in the final answer).", "solution": "We begin from the definition of stratified $k$-fold Cross-Validation (CV): the CV risk estimator $\\hat{R}_{\\mathrm{CV}}(k)$ is an average of $k$ held-out fold risks, each computed on a test split of size $\\frac{n}{k}$ with class proportions preserved. Averaging $k$ fold-wise estimates reduces variability due to the law of large numbers for averages; consistent with this fundamental behavior, the given model $V(k)\\approx \\frac{a}{k}+b$ captures a reduction in variance as $k$ increases together with a nonzero floor $b$ due to irreducible sources such as model instability or data heterogeneity.\n\nWe next use the linear runtime scaling to characterize computational feasibility. The per-fold training time on $m$ samples is $t(m)=c\\,m$. The pilot measurement provides the calibration $t(n)=60$ seconds at $n=1200$, hence\n$$\nc=\\frac{t(n)}{n}=\\frac{60}{1200}=0.05.\n$$\nIn $k$-fold CV, each of the $k$ fits uses $(1-\\frac{1}{k})\\,n$ training samples. The total runtime is therefore\n$$\nT(k)=k\\,t\\!\\left(\\left(1-\\frac{1}{k}\\right)n\\right)=k\\,c\\,\\left(1-\\frac{1}{k}\\right)n=c\\,n\\,(k-1).\n$$\nSubstituting $c=0.05$ and $n=1200$, we obtain\n$$\nT(k)=0.05\\times 1200 \\times (k-1)=60\\,(k-1).\n$$\nThe runtime budget is $T_{\\max}=220$, so the feasibility condition $T(k)\\le T_{\\max}$ becomes\n$$\n60\\,(k-1)\\le 220 \\quad \\Longrightarrow \\quad k-1\\le \\frac{220}{60}=\\frac{11}{3} \\quad \\Longrightarrow \\quad k\\le 1+\\frac{11}{3}=\\frac{14}{3}.\n$$\nSince $k$ must be an integer, the maximum $k$ allowed by runtime is\n$$\nk\\le \\left\\lfloor \\frac{14}{3}\\right\\rfloor = 4.\n$$\n\nStratification feasibility further requires that each test split contains at least one sample from each class. A sufficient condition is $k\\le \\min\\{n_{+},n_{-}\\}$, which here reads\n$$\nk\\le \\min\\{900,300\\}=300.\n$$\nThis bound is far looser than the runtime bound $k\\le 4$, so the active constraint is computational.\n\nTo minimize $V(k)=\\frac{a}{k}+b$ subject to the constraints, we use monotonicity: for $a0$, $V(k)$ is strictly decreasing in $k$, and $b$ does not depend on $k$. Therefore, under constraints, the minimum is achieved at the largest feasible $k$. Combining the bounds, the largest feasible $k$ is\n$$\nk^{*}=4.\n$$\n\nFinally, we verify stratification with $k^{*}=4$. Each test split has size $\\frac{n}{k^{*}}=\\frac{1200}{4}=300$. Preserving proportions yields per-fold test counts\n$$\n\\text{majority}:\\ \\frac{n_{+}}{k^{*}}=\\frac{900}{4}=225, \\qquad \\text{minority}:\\ \\frac{n_{-}}{k^{*}}=\\frac{300}{4}=75,\n$$\nwhich are integers and strictly positive, so stratified folds are feasible. Since $V(k)$ is decreasing in $k$ and $k^{*}=4$ is the largest $k$ allowed by the runtime budget, $k^{*}=4$ is the minimizer of $V(k)$ under the stated constraints.", "answer": "$$\\boxed{4}$$", "id": "3177490"}]}