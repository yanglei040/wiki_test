{"hands_on_practices": [{"introduction": "To truly grasp a statistical method, it's often best to start by analyzing its behavior in the simplest possible scenario. This practice challenges you to derive the leave-one-out cross-validation (LOOCV) error for a model that predicts every point using the sample mean. By working through the mathematics, you will uncover a direct and elegant relationship between the LOOCV error and the sample variance of the data, providing a foundational insight into how LOOCV quantifies prediction error [@problem_id:1912461].", "problem": "In the field of statistical learning, cross-validation is a fundamental technique for assessing how the results of a statistical analysis will generalize to an independent dataset. A common variant is Leave-One-Out Cross-Validation (LOOCV).\n\nConsider a dataset consisting of $n$ observations, $y_1, y_2, \\ldots, y_n$. We wish to evaluate a very simple predictive model: for any given training set, the model's prediction for a new data point is simply the arithmetic mean of the observations in that training set.\n\nThe LOOCV procedure for this dataset involves $n$ iterations. In the $i$-th iteration (for $i=1, \\ldots, n$), the $i$-th observation, $y_i$, is held out as the validation set, and the remaining $n-1$ observations are used as the training set. The model is trained on these $n-1$ observations, and a prediction is made for the held-out observation $y_i$. The squared error between the prediction and the actual value $y_i$ is then calculated.\n\nYour task is to derive a general, closed-form expression for the LOOCV Mean Squared Error (MSE), which is the average of these squared errors over all $n$ iterations. Express your final answer in terms of the number of observations, $n$, and the sample variance of the full dataset, $s^2$. The sample variance is defined as $s^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (y_i - \\bar{y})^2$, where $\\bar{y}$ is the sample mean of all $n$ observations.", "solution": "We consider the predictive model that outputs the arithmetic mean of the training set. Let the full-sample mean be $\\bar{y} = \\frac{1}{n}\\sum_{j=1}^{n} y_{j}$. In the $i$-th LOOCV fold, the training set excludes $y_{i}$, so the leave-one-out mean is\n$$\n\\bar{y}_{-i} = \\frac{1}{n-1}\\sum_{\\substack{j=1 \\\\ j\\neq i}}^{n} y_{j} = \\frac{n\\bar{y} - y_{i}}{n-1}.\n$$\nThe prediction error for the held-out $y_{i}$ is\n$$\ny_{i} - \\bar{y}_{-i} = y_{i} - \\frac{n\\bar{y} - y_{i}}{n-1} = \\frac{n(y_{i} - \\bar{y})}{n-1}.\n$$\nThus, the squared error in the $i$-th fold is\n$$\n\\left(y_{i} - \\bar{y}_{-i}\\right)^{2} = \\frac{n^{2}}{(n-1)^{2}}(y_{i} - \\bar{y})^{2}.\n$$\nThe LOOCV Mean Squared Error (MSE) is the average of these squared errors over $i=1,\\ldots,n$:\n$$\n\\text{LOOCV MSE} = \\frac{1}{n}\\sum_{i=1}^{n} \\left(y_{i} - \\bar{y}_{-i}\\right)^{2} = \\frac{1}{n}\\cdot \\frac{n^{2}}{(n-1)^{2}} \\sum_{i=1}^{n} (y_{i} - \\bar{y})^{2}.\n$$\nUsing the definition of the sample variance $s^{2} = \\frac{1}{n-1}\\sum_{i=1}^{n} (y_{i} - \\bar{y})^{2}$, we substitute $\\sum_{i=1}^{n} (y_{i} - \\bar{y})^{2} = (n-1)s^{2}$ to obtain\n$$\n\\text{LOOCV MSE} = \\frac{1}{n}\\cdot \\frac{n^{2}}{(n-1)^{2}} \\cdot (n-1) s^{2} = \\frac{n}{n-1} s^{2}.\n$$\nTherefore, the LOOCV MSE for the mean-as-predictor model is $\\frac{n}{n-1}s^{2}$.", "answer": "$$\\boxed{\\frac{n}{n-1}s^{2}}$$", "id": "1912461"}, {"introduction": "After exploring the theory, the next step is to apply the LOOCV procedure manually to a concrete dataset. This exercise guides you through the process using a 1-Nearest Neighbor (1-NN) classifier on a small, hypothetical materials science dataset. Calculating the accuracy step-by-step demystifies the iterative nature of LOOCV and solidifies your understanding of how it operates in a classification context before you rely on automated library functions [@problem_id:90086].", "problem": "In the field of computational materials science, machine learning models are increasingly used to predict material properties and accelerate the discovery of novel materials. A common task is to classify materials based on a set of computed or experimental features.\n\nConsider a simplified problem of classifying 2D materials as either a \"Trivial Insulator\" (Class 0) or a \"Topological Insulator\" (Class 1). The classification is based on two features: $f_1$, the exfoliation energy per atom (in eV/atom), and $f_2$, the electronic band gap (in eV). We will use a 1-Nearest Neighbor (1-NN) classifier for this task.\n\nThe 1-NN algorithm classifies a new data point by assigning it the class of its single nearest neighbor in the training data. The \"nearness\" is determined by a distance metric, for which we will use the standard Euclidean distance. For two points $\\mathbf{p}=(p_1, p_2)$ and $\\mathbf{q}=(q_1, q_2)$ in the feature space, the Euclidean distance is $d(\\mathbf{p}, \\mathbf{q}) = \\sqrt{(p_1-q_1)^2 + (p_2-q_2)^2}$.\n\nTo evaluate the performance of this classifier on a small dataset without splitting it into a separate training and test set, we employ Leave-One-Out Cross-Validation (LOOCV). In LOOCV, for a dataset of $N$ points, we perform $N$ iterations. In each iteration $i$, the $i$-th data point is held out as the test sample, and the model is trained on the remaining $N-1$ data points. The prediction for the held-out point is then compared to its true class. The overall accuracy is the fraction of points that are correctly classified.\n\nYou are given the following dataset of five 2D materials:\n\n| Material ID | Feature $f_1$ | Feature $f_2$ | Class Label |\n| :--- | :---: | :---: | :---: |\n| A | 1.0 | 1.0 | 0 |\n| B | 2.0 | 2.0 | 0 |\n| C | 5.0 | 5.0 | 1 |\n| D | 6.0 | 4.0 | 1 |\n| E | 2.5 | 2.5 | 1 |\n\nIn case of a tie in distances to the nearest neighbors, the neighbor that appears earlier in the dataset table (i.e., A before B, B before C, etc.) is chosen.\n\nDerive the leave-one-out cross-validation (LOOCV) accuracy for the 1-NN classifier on this dataset.", "solution": "The goal is to compute the Leave-One-Out Cross-Validation (LOOCV) accuracy for a 1-Nearest Neighbor (1-NN) classifier on the given dataset of $N=5$ materials. The accuracy is defined as:\n$$\n\\text{Accuracy} = \\frac{\\text{Number of Correctly Classified Samples}}{\\text{Total Number of Samples}}\n$$\n\nWe will perform $N=5$ iterations, one for each material in the dataset. In each iteration, we hold out one material, treat it as the test point, and use the remaining four as the training set. We find the nearest neighbor to the test point from the training set and assign its class to the test point.\n\nTo find the nearest neighbor, we need to calculate the Euclidean distance. Note that comparing squared Euclidean distances $d^2 = (p_1-q_1)^2 + (p_2-q_2)^2$ is equivalent to comparing the distances themselves and avoids dealing with square roots.\n\n**Iteration 1: Hold out Material A**\n-   Test Point: A = (1.0, 1.0), True Class = 0\n-   Training Set: {B, C, D, E}\n-   Squared distances from A:\n    -   $d^2(A, B) = (1.0 - 2.0)^2 + (1.0 - 2.0)^2 = (-1.0)^2 + (-1.0)^2 = 1.0 + 1.0 = 2.0$\n    -   $d^2(A, C) = (1.0 - 5.0)^2 + (1.0 - 5.0)^2 = (-4.0)^2 + (-4.0)^2 = 16.0 + 16.0 = 32.0$\n    -   $d^2(A, D) = (1.0 - 6.0)^2 + (1.0 - 4.0)^2 = (-5.0)^2 + (-3.0)^2 = 25.0 + 9.0 = 34.0$\n    -   $d^2(A, E) = (1.0 - 2.5)^2 + (1.0 - 2.5)^2 = (-1.5)^2 + (-1.5)^2 = 2.25 + 2.25 = 4.5$\n-   The minimum squared distance is 2.0, which corresponds to Material B.\n-   The nearest neighbor is B. The class of B is 0.\n-   Predicted Class for A: 0.\n-   Result: Correct (True: 0, Predicted: 0).\n\n**Iteration 2: Hold out Material B**\n-   Test Point: B = (2.0, 2.0), True Class = 0\n-   Training Set: {A, C, D, E}\n-   Squared distances from B:\n    -   $d^2(B, A) = (2.0 - 1.0)^2 + (2.0 - 1.0)^2 = (1.0)^2 + (1.0)^2 = 1.0 + 1.0 = 2.0$\n    -   $d^2(B, C) = (2.0 - 5.0)^2 + (2.0 - 5.0)^2 = (-3.0)^2 + (-3.0)^2 = 9.0 + 9.0 = 18.0$\n    -   $d^2(B, D) = (2.0 - 6.0)^2 + (2.0 - 4.0)^2 = (-4.0)^2 + (-2.0)^2 = 16.0 + 4.0 = 20.0$\n    -   $d^2(B, E) = (2.0 - 2.5)^2 + (2.0 - 2.5)^2 = (-0.5)^2 + (-0.5)^2 = 0.25 + 0.25 = 0.5$\n-   The minimum squared distance is 0.5, which corresponds to Material E.\n-   The nearest neighbor is E. The class of E is 1.\n-   Predicted Class for B: 1.\n-   Result: Incorrect (True: 0, Predicted: 1).\n\n**Iteration 3: Hold out Material C**\n-   Test Point: C = (5.0, 5.0), True Class = 1\n-   Training Set: {A, B, D, E}\n-   Squared distances from C:\n    -   $d^2(C, A) = (5.0 - 1.0)^2 + (5.0 - 1.0)^2 = (4.0)^2 + (4.0)^2 = 16.0 + 16.0 = 32.0$\n    -   $d^2(C, B) = (5.0 - 2.0)^2 + (5.0 - 2.0)^2 = (3.0)^2 + (3.0)^2 = 9.0 + 9.0 = 18.0$\n    -   $d^2(C, D) = (5.0 - 6.0)^2 + (5.0 - 4.0)^2 = (-1.0)^2 + (1.0)^2 = 1.0 + 1.0 = 2.0$\n    -   $d^2(C, E) = (5.0 - 2.5)^2 + (5.0 - 2.5)^2 = (2.5)^2 + (2.5)^2 = 6.25 + 6.25 = 12.5$\n-   The minimum squared distance is 2.0, which corresponds to Material D.\n-   The nearest neighbor is D. The class of D is 1.\n-   Predicted Class for C: 1.\n-   Result: Correct (True: 1, Predicted: 1).\n\n**Iteration 4: Hold out Material D**\n-   Test Point: D = (6.0, 4.0), True Class = 1\n-   Training Set: {A, B, C, E}\n-   Squared distances from D:\n    -   $d^2(D, A) = (6.0 - 1.0)^2 + (4.0 - 1.0)^2 = (5.0)^2 + (3.0)^2 = 25.0 + 9.0 = 34.0$\n    -   $d^2(D, B) = (6.0 - 2.0)^2 + (4.0 - 2.0)^2 = (4.0)^2 + (2.0)^2 = 16.0 + 4.0 = 20.0$\n    -   $d^2(D, C) = (6.0 - 5.0)^2 + (4.0 - 5.0)^2 = (1.0)^2 + (-1.0)^2 = 1.0 + 1.0 = 2.0$\n    -   $d^2(D, E) = (6.0 - 2.5)^2 + (4.0 - 2.5)^2 = (3.5)^2 + (1.5)^2 = 12.25 + 2.25 = 14.5$\n-   The minimum squared distance is 2.0, which corresponds to Material C.\n-   The nearest neighbor is C. The class of C is 1.\n-   Predicted Class for D: 1.\n-   Result: Correct (True: 1, Predicted: 1).\n\n**Iteration 5: Hold out Material E**\n-   Test Point: E = (2.5, 2.5), True Class = 1\n-   Training Set: {A, B, C, D}\n-   Squared distances from E:\n    -   $d^2(E, A) = (2.5 - 1.0)^2 + (2.5 - 1.0)^2 = (1.5)^2 + (1.5)^2 = 2.25 + 2.25 = 4.5$\n    -   $d^2(E, B) = (2.5 - 2.0)^2 + (2.5 - 2.0)^2 = (0.5)^2 + (0.5)^2 = 0.25 + 0.25 = 0.5$\n    -   $d^2(E, C) = (2.5 - 5.0)^2 + (2.5 - 5.0)^2 = (-2.5)^2 + (-2.5)^2 = 6.25 + 6.25 = 12.5$\n    -   $d^2(E, D) = (2.5 - 6.0)^2 + (2.5 - 4.0)^2 = (-3.5)^2 + (-1.5)^2 = 12.25 + 2.25 = 14.5$\n-   The minimum squared distance is 0.5, which corresponds to Material B.\n-   The nearest neighbor is B. The class of B is 0.\n-   Predicted Class for E: 0.\n-   Result: Incorrect (True: 1, Predicted: 0).\n\n**Final Accuracy Calculation**\nWe summarize the results:\n-   A: Correctly classified\n-   B: Incorrectly classified\n-   C: Correctly classified\n-   D: Correctly classified\n-   E: Incorrectly classified\n\nThe total number of correctly classified samples is 3.\nThe total number of samples is 5.\n\nThe LOOCV accuracy is:\n$$\n\\text{Accuracy} = \\frac{3}{5}\n$$", "answer": "$$ \\boxed{\\frac{3}{5}} $$", "id": "90086"}, {"introduction": "Beyond estimating error, a primary use of cross-validation is to select the best model or hyperparameters. This hands-on coding problem demonstrates a critical advantage of LOOCV over the naive training error: its robustness in model selection, especially when the data contains outliers. By constructing a specific dataset and observing how LOOCV and training error prefer different values of $k$ in a $k$-NN classifier, you will gain a powerful, intuitive understanding of why cross-validation is essential for building models that generalize well [@problem_id:3139300].", "problem": "You will implement and analyze Leave-One-Out Cross-Validation (LOOCV) for the $k$-Nearest Neighbors (k-NN) classifier on constructed datasets designed to reveal the effect of a single extreme outlier on model selection. The aim is to demonstrate from first principles why LOOCV can select a larger $k$ to counteract an outlier, while the training resubstitution error favors $k=1$. Begin from the following foundational definitions.\n\nLet there be a dataset of $N$ labeled points $\\{(x_i, y_i)\\}_{i=1}^N$ with $x_i \\in \\mathbb{R}^2$ and $y_i \\in \\{0,1\\}$. The $k$-Nearest Neighbors classifier predicts the label for a point by:\n- computing Euclidean distances to all candidate neighbors (excluding the query point under Leave-One-Out Cross-Validation),\n- selecting the $k$ points with smallest distances (if equal distances occur, break ties deterministically by the smaller original index),\n- returning the class with the majority of votes among these $k$ neighbors (for odd $k$, this avoids count ties; if a rare tie persists, break it by choosing the class whose neighbors have the smaller sum of distances, and if still tied, choose the smaller class label).\n\nDefine the empirical $0\\text{-}1$ loss for a predictor $\\hat{f}$ on a dataset as $$\\frac{1}{N}\\sum_{i=1}^N \\mathbb{I}\\{\\hat{f}(x_i) \\neq y_i\\},$$ where $\\mathbb{I}\\{\\cdot\\}$ is the indicator function. Define the training resubstitution error as the empirical $0\\text{-}1$ loss measured by classifying each $x_i$ using the same dataset that contains $x_i$ and allowing $x_i$ to be selected among its own neighbors. Define Leave-One-Out Cross-Validation (LOOCV) error as the empirical $0\\text{-}1$ loss measured by classifying each $x_i$ using a model that excludes $x_i$ from the training set.\n\nYour program must:\n1. Implement $k$-Nearest Neighbors classification, Euclidean distance in $\\mathbb{R}^2$, training resubstitution error, and Leave-One-Out Cross-Validation error, strictly following the above rules.\n2. For each dataset in the test suite below and for candidate values $k \\in \\{1,3\\}$, compute:\n   - the $k$ that minimizes LOOCV error, using the tie-breaking rule \"choose the smallest $k$\" if multiple $k$ yield the same minimal error,\n   - the $k$ that minimizes training resubstitution error, using the same tie-breaking rule.\n3. Produce a single line of output containing the per-dataset results in order, formatted as a comma-separated list enclosed in square brackets, where each per-dataset result is itself a two-element list $[k_{\\text{LOOCV}}, k_{\\text{train}}]$. For example, the output should look like $$[[k_1^{\\text{LOOCV}},k_1^{\\text{train}}],[k_2^{\\text{LOOCV}},k_2^{\\text{train}}],[k_3^{\\text{LOOCV}},k_3^{\\text{train}}]].$$\n\nTest suite (all coordinates are in $\\mathbb{R}^2$ with no physical units):\n- Case $A$ (one extreme outlier centered in the cluster):\n  - Class $0$ points: $(1,0)$, $(-1,0)$, $(0,1)$, $(0,-1)$.\n  - Class $1$ outlier: $(0,0)$.\n  - Candidate $k$: $\\{1,3\\}$.\n- Case $B$ (no outlier; well-separated clusters):\n  - Class $0$ points: $(1,0)$, $(-1,0)$, $(0,1)$, $(0,-1)$.\n  - Class $1$ points: $(10,0)$, $(-10,0)$, $(0,10)$, $(0,-10)$.\n  - Candidate $k$: $\\{1,3\\}$.\n- Case $C$ (an extreme outlier far from all other points but sharing the label of one cluster):\n  - Class $0$ points: $(1,0)$, $(-1,0)$, $(0,1)$, $(0,-1)$, $(100,100)$.\n  - Class $1$ points: $(3,0)$, $(3,1)$, $(4,0)$, $(4,1)$.\n  - Candidate $k$: $\\{1,3\\}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with no spaces, in the exact format $$[[k_A^{\\text{LOOCV}},k_A^{\\text{train}}],[k_B^{\\text{LOOCV}},k_B^{\\text{train}}],[k_C^{\\text{LOOCV}},k_C^{\\text{train}}]].$$ The numbers in the output must be integers.", "solution": "The problem requires a comparative analysis of the training resubstitution error and the Leave-One-Out Cross-Validation (LOOCV) error for a $k$-Nearest Neighbors ($k$-NN) classifier. We will implement the classifier and error metrics according to the specified rules and apply them to three distinct datasets for candidate values of $k \\in \\{1, 3\\}$. The goal is to determine the optimal $k$ selected by each error metric, denoted $k_{\\text{train}}$ and $k_{\\text{LOOCV}}$ respectively.\n\nThe foundational definitions are as follows. The dataset is a collection of $N$ points $\\{(x_i, y_i)\\}_{i=1}^N$ with $x_i \\in \\mathbb{R}^2$ and $y_i \\in \\{0,1\\}$. A prediction $\\hat{f}(x)$ for a query point $x$ is made by majority vote of the $k$ nearest training points, where distance is Euclidean. Ties in distance are broken by the smaller original index of the points. The training resubstitution error, $E_{\\text{train}}$, is the $0\\text{-}1$ loss on the full training set, allowing a point to be its own a neighbor. The LOOCV error, $E_{\\text{LOOCV}}$, is the $0\\text{-}1$ loss computed by averaging the error of predicting each point $x_i$ using a model trained on the dataset excluding $(x_i, y_i)$. The optimal $k$ for each metric is the one that minimizes the corresponding error; ties are broken by selecting the smaller $k$.\n\nWe will analyze each case systematically.\n\n**Case A: One extreme outlier centered in the cluster.**\nThe dataset consists of $N=5$ points.\nClass $0$: $P_0=(1,0)$, $P_1=(-1,0)$, $P_2=(0,1)$, $P_3=(0,-1)$.\nClass $1$: $P_4=(0,0)$.\n\n**Training Resubstitution Error Analysis ($E_{\\text{train}}$):**\nFor $k=1$: When classifying a point $x_i$ from the training set, its nearest neighbor is always itself, with a distance of $0$. Therefore, the predicted label $\\hat{f}(x_i)$ is its own label $y_i$. This results in zero misclassifications.\n$E_{\\text{train}}(k=1) = \\frac{0}{5} = 0$.\nFor $k=3$: We find the $3$ nearest neighbors for each point, including the point itself.\n- For any Class $0$ point (e.g., $P_0=(1,0)$), its neighbors are itself ($P_0$, Class $0$, dist $0$), the outlier $P_4$ ($P_4$, Class $1$, dist $1$), and another Class $0$ point ($P_2$ or $P_3$, dist $\\sqrt{2}$). The labels are $\\{0, 1, 0\\}$, so the majority vote is Class $0$. The prediction is correct. This holds for $P_0, P_1, P_2, P_3$.\n- For the outlier $P_4=(0,0)$ (Class $1$), its neighbors are itself ($P_4$, Class $1$, dist $0$) and any two of the Class $0$ points, which are all equidistant at dist $1$. Using the index tie-breaking rule, we select $P_0$ and $P_1$. The neighbors' labels are $\\{1, 0, 0\\}$. The majority vote is Class $0$. The prediction is $0$, but the true label is $1$. This is one misclassification.\nTotal misclassifications for $k=3$ is $1$.\n$E_{\\text{train}}(k=3) = \\frac{1}{5} = 0.2$.\nComparing errors, $E_{\\text{train}}(k=1) = 0  E_{\\text{train}}(k=3) = 0.2$. Thus, $k_{\\text{train}} = 1$.\n\n**Leave-One-Out Cross-Validation Error Analysis ($E_{\\text{LOOCV}}$):**\nFor $k=1$: We classify each point $x_i$ using the other $N-1$ points.\n- For any Class $0$ point (e.g., $P_0=(1,0)$), its nearest neighbor in the remaining set $\\{P_1, P_2, P_3, P_4\\}$ is the outlier $P_4=(0,0)$ (Class $1$) at a distance of $1$. The prediction is Class $1$, while the true label is $0$. This is a misclassification. All $4$ Class $0$ points are misclassified this way.\n- For the outlier $P_4=(0,0)$ (Class $1$), its nearest neighbor in the set $\\{P_0, P_1, P_2, P_3\\}$ is any of them (all at distance $1$). By index tie-breaking, we select $P_0=(1,0)$ (Class $0$). The prediction is Class $0$, while the true label is $1$. This is a misclassification.\nTotal misclassifications for $k=1$ is $5$.\n$E_{\\text{LOOCV}}(k=1) = \\frac{5}{5} = 1.0$.\nFor $k=3$:\n- For any Class $0$ point (e.g., $P_0=(1,0)$), its $3$ nearest neighbors in the remaining set $\\{P_1, P_2, P_3, P_4\\}$ are $P_4$ (Class $1$, dist $1$), $P_2$ (Class $0$, dist $\\sqrt{2}$), and $P_3$ (Class $0$, dist $\\sqrt{2}$). The labels are $\\{1, 0, 0\\}$. The majority vote is Class $0$. The prediction is correct. This holds for all $4$ Class $0$ points.\n- For the outlier $P_4=(0,0)$ (Class $1$), its $3$ nearest neighbors in the set $\\{P_0, P_1, P_2, P_3\\}$ are any three of these points (all at distance $1$). By index tie-breaking, we select $P_0, P_1, P_2$, all of which are Class $0$. The labels are $\\{0, 0, 0\\}$. The prediction is Class $0$, while the true label is $1$. This is a misclassification.\nTotal misclassifications for $k=3$ is $1$.\n$E_{\\text{LOOCV}}(k=3) = \\frac{1}{5} = 0.2$.\nComparing errors, $E_{\\text{LOOCV}}(k=3) = 0.2  E_{\\text{LOOCV}}(k=1) = 1.0$. Thus, $k_{\\text{LOOCV}} = 3$.\nThe result for Case A is $[k_{\\text{LOOCV}}, k_{\\text{train}}] = [3, 1]$.\n\n**Case B: No outlier; well-separated clusters.**\nThe dataset consists of $N=8$ points.\nClass $0$: $\\{(1,0), (-1,0), (0,1), (0,-1)\\}$.\nClass $1$: $\\{(10,0), (-10,0), (0,10), (0,-10)\\}$.\nFollowing the same methodology:\n$E_{\\text{train}}(k=1) = 0$. For $k=3$, each point in the outer cluster (Class $1$) is misclassified because its two nearest neighbors (besides itself) are from the inner cluster (Class $0$), resulting in $4$ errors. $E_{\\text{train}}(k=3) = \\frac{4}{8} = 0.5$. So $k_{\\text{train}} = 1$.\n$E_{\\text{LOOCV}}(k=1)$: Each Class $1$ point's nearest neighbor is a Class $0$ point, leading to $4$ errors. Each Class $0$ point's nearest neighbor is another Class $0$ point, leading to $0$ errors. $E_{\\text{LOOCV}}(k=1) = \\frac{4}{8} = 0.5$.\n$E_{\\text{LOOCV}}(k=3)$: Each Class $1$ point's three nearest neighbors are all Class $0$ points, leading to $4$ errors. Each Class $0$ point's three nearest neighbors are other Class $0$ points, leading to $0$ errors. $E_{\\text{LOOCV}}(k=3) = \\frac{4}{8} = 0.5$.\nErrors are tied: $E_{\\text{LOOCV}}(k=1) = E_{\\text{LOOCV}}(k=3) = 0.5$. By the tie-breaking rule, we choose the smallest $k$. Thus, $k_{\\text{LOOCV}} = 1$.\nThe result for Case B is $[k_{\\text{LOOCV}}, k_{\\text{train}}] = [1, 1]$.\n\n**Case C: An extreme outlier far from all other points.**\nThe dataset consists of $N=9$ points.\nClass $0$: $\\{(1,0), (-1,0), (0,1), (0,-1), (100,100)\\}$.\nClass $1$: $\\{(3,0), (3,1), (4,0), (4,1)\\}$.\nFollowing the same methodology:\n$E_{\\text{train}}(k=1) = 0$. For $k=3$, only the outlier point $(100,100)$ is misclassified, as its two nearest neighbors (besides itself) are from the Class 1 cluster. $E_{\\text{train}}(k=3) = \\frac{1}{9}$. So $k_{\\text{train}} = 1$.\n$E_{\\text{LOOCV}}(k=1)$: When the outlier $(100,100)$ is held out, its nearest neighbor is a Class $1$ point, resulting in one error. All other points are correctly classified. $E_{\\text{LOOCV}}(k=1) = \\frac{1}{9}$.\n$E_{\\text{LOOCV}}(k=3)$: When the outlier $(100,100)$ is held out, its three nearest neighbors are all Class $1$ points, resulting in one error. All other points are correctly classified. $E_{\\text{L_OOCV}}(k=3) = \\frac{1}{9}$.\nErrors are tied: $E_{\\text{LOOCV}}(k=1) = E_{\\text{LOOCV}}(k=3) = \\frac{1}{9}$. By the tie-breaking rule, we choose the smallest $k$. Thus, $k_{\\text{LOOCV}} = 1$.\nThe result for Case C is $[k_{\\text{LOOCV}}, k_{\\text{train}}] = [1, 1]$.\n\nThe final results are collected from each case.\n- Case A: $[3, 1]$\n- Case B: $[1, 1]$\n- Case C: $[1, 1]$\nThese results demonstrate the key insight: training resubstitution error will always trivially select $k=1$ (as its error is always $0$), which is a pathological aspect of this error metric. LOOCV provides a more honest estimate of generalization error. In Case A, it correctly identifies that a larger $k=3$ is needed to be robust against the central outlier. In Cases B and C, the geometry of the data is such that $k=1$ is preferred or tied with $k=3$, and the tie-breaking rule selects $k=1$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and analyzes LOOCV vs. training error for k-NN on three datasets.\n    \"\"\"\n\n    test_cases = [\n        # Case A: one extreme outlier centered in the cluster\n        {\n            \"class_0_points\": np.array([[1, 0], [-1, 0], [0, 1], [0, -1]]),\n            \"class_1_points\": np.array([[0, 0]]),\n        },\n        # Case B: no outlier; well-separated clusters\n        {\n            \"class_0_points\": np.array([[1, 0], [-1, 0], [0, 1], [0, -1]]),\n            \"class_1_points\": np.array([[10, 0], [-10, 0], [0, 10], [0, -10]]),\n        },\n        # Case C: an extreme outlier far from all other points\n        {\n            \"class_0_points\": np.array([[1, 0], [-1, 0], [0, 1], [0, -1], [100, 100]]),\n            \"class_1_points\": np.array([[3, 0], [3, 1], [4, 0], [4, 1]]),\n        }\n    ]\n    \n    candidate_k_values = [1, 3]\n    final_results = []\n\n    for case_data in test_cases:\n        # Prepare dataset with original indices for tie-breaking\n        points = []\n        labels = []\n        \n        index = 0\n        for p in case_data[\"class_0_points\"]:\n            points.append(p)\n            labels.append(0)\n            index += 1\n        for p in case_data[\"class_1_points\"]:\n            points.append(p)\n            labels.append(1)\n            index += 1\n\n        points = np.array(points)\n        labels = np.array(labels)\n        full_dataset = list(zip(range(len(points)), points, labels))\n\n        # --- k-NN Implementation ---\n        def euclidean_distance(p1, p2):\n            return np.sqrt(np.sum((p1 - p2)**2))\n\n        def k_nn_predict(query_point, training_data, k):\n            if not training_data:\n                # Undefined, but for this problem, training set is never empty\n                return 0 \n            \n            distances = []\n            for idx, train_point, label in training_data:\n                dist = euclidean_distance(query_point, train_point)\n                distances.append((dist, idx, label))\n            \n            # Sort by distance, then by original index for tie-breaking\n            distances.sort(key=lambda x: (x[0], x[1]))\n            \n            neighbors = distances[:k]\n            \n            neighbor_labels = [label for _, _, label in neighbors]\n            \n            # Majority vote\n            count_0 = neighbor_labels.count(0)\n            count_1 = neighbor_labels.count(1)\n\n            if count_0 > count_1:\n                return 0\n            elif count_1 > count_0:\n                return 1\n            else: # Vote tie-break (unlikely for odd k here, but for completeness)\n                sum_dist_0 = sum(dist for dist, _, label in neighbors if label == 0)\n                sum_dist_1 = sum(dist for dist, _, label in neighbors if label == 1)\n                if sum_dist_0  sum_dist_1:\n                    return 0\n                elif sum_dist_1  sum_dist_0:\n                    return 1\n                else: # Tie in sum of distances, choose smaller class label\n                    return 0\n        \n        # --- Error Calculation ---\n        \n        # Training Resubstitution Error\n        train_errors = {}\n        for k in candidate_k_values:\n            misclassifications = 0\n            for i in range(len(points)):\n                query_point = points[i]\n                true_label = labels[i]\n                # In training error, the point itself is part of the training set\n                # For k-NN, if k>=1, the nearest neighbor to a point is itself.\n                # If k=1, error is always 0.\n                if k == 1:\n                    predicted_label = true_label\n                else:\n                    predicted_label = k_nn_predict(query_point, full_dataset, k)\n                    \n                if predicted_label != true_label:\n                    misclassifications += 1\n            train_errors[k] = misclassifications / len(points)\n        \n        # LOOCV Error\n        loocv_errors = {}\n        for k in candidate_k_values:\n            misclassifications = 0\n            for i in range(len(points)):\n                query_point = points[i]\n                true_label = labels[i]\n                \n                # Create LOOCV training set by excluding point i\n                loocv_train_set = full_dataset[:i] + full_dataset[i+1:]\n                \n                predicted_label = k_nn_predict(query_point, loocv_train_set, k)\n                \n                if predicted_label != true_label:\n                    misclassifications += 1\n            loocv_errors[k] = misclassifications / len(points)\n\n        # --- Model Selection (Find best k) ---\n        \n        # Tie-breaking rule: choose smallest k\n        min_loocv_error = float('inf')\n        best_k_loocv = -1\n        for k in sorted(loocv_errors.keys()):\n            if loocv_errors[k]  min_loocv_error:\n                min_loocv_error = loocv_errors[k]\n                best_k_loocv = k\n        \n        min_train_error = float('inf')\n        best_k_train = -1\n        # For k=1 training error is always 0, so it will always be selected\n        # if 1 is a candidate k.\n        for k in sorted(train_errors.keys()):\n            if train_errors[k]  min_train_error:\n                min_train_error = train_errors[k]\n                best_k_train = k\n                \n        final_results.append([best_k_loocv, best_k_train])\n\n    # Format the final output string\n    output_str = \"[\" + \",\".join([f\"[{k_l},{k_t}]\" for k_l, k_t in final_results]) + \"]\"\n    print(output_str)\n\nsolve()\n```", "id": "3139300"}]}