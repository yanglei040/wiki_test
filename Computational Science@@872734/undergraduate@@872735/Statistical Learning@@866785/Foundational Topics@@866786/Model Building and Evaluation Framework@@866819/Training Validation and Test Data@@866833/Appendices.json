{"hands_on_practices": [{"introduction": "The purpose of a validation set is to tune hyperparameters, but this tuning process itself can lead to a form of overfitting. This exercise provides a concrete, hands-on demonstration of this phenomenon by focusing on a simple but crucial hyperparameter: the classification threshold. By finding the optimal threshold on a validation set and then applying it to a test set, you will directly measure the performance gap, or \"optimism,\" that arises from tuning, reinforcing the necessity of a final, untouched test set for unbiased evaluation [@problem_id:3188635].", "problem": "Consider a binary classification model that outputs calibrated predicted probabilities on two disjoint datasets: a validation set $X_{\\text{val}}$ and a test set $X_{\\text{test}}$. Let the validation set consist of predicted probabilities $p^{(\\text{val})}_i \\in [0,1]$ with binary labels $y^{(\\text{val})}_i \\in \\{0,1\\}$ for $i = 1,\\dots,n_{\\text{val}}$, and similarly for the test set with $p^{(\\text{test})}_j \\in [0,1]$ and $y^{(\\text{test})}_j \\in \\{0,1\\}$ for $j = 1,\\dots,n_{\\text{test}}$. A threshold $t \\in [0,1]$ induces predicted labels $\\hat{y}_i(t) = 1$ if and only if $p_i \\ge t$ and $\\hat{y}_i(t) = 0$ otherwise.\n\nStarting from core definitions in statistical learning:\n- Precision $P$ is defined as $P = \\frac{TP}{TP + FP}$ with the convention $P = 0$ when $TP + FP = 0$, where $TP$ denotes True Positives, i.e., the count of instances with $(\\hat{y}=1, y=1)$, and $FP$ denotes False Positives, i.e., the count of instances with $(\\hat{y}=1, y=0)$.\n- Recall $R$ is defined as $R = \\frac{TP}{TP + FN}$ with the convention $R = 0$ when $TP + FN = 0$, where $FN$ denotes False Negatives, i.e., the count of instances with $(\\hat{y}=0, y=1)$.\n- The $F_\\beta$ score for $\\beta > 0$ is defined as the weighted harmonic mean of $P$ and $R$, namely $F_\\beta = \\frac{(1+\\beta^2) \\cdot P \\cdot R}{\\beta^2 \\cdot P + R}$, with the convention $F_\\beta = 0$ when the denominator is $0$.\n\nThe goal is to quantify threshold overfitting due to tuning on $X_{\\text{val}}$ under class distribution shift and varying $\\beta$. For a given $\\beta$, select the threshold $t^\\ast$ that maximizes $F_\\beta$ on $X_{\\text{val}}$ by exhaustive search over the finite candidate set $\\{0\\} \\cup S \\cup \\{1\\}$, where $S$ is the set of unique values of $p^{(\\text{val})}_i$. If multiple thresholds achieve the same maximum $F_\\beta$ on $X_{\\text{val}}$, break ties by selecting the largest threshold among the maximizers. Then, compute $F_\\beta$ on $X_{\\text{val}}$ and on $X_{\\text{test}}$ using $t^\\ast$, and report the overfitting gap defined as\n$$\n\\Delta = F_\\beta\\big(X_{\\text{val}}; t^\\ast\\big) - F_\\beta\\big(X_{\\text{test}}; t^\\ast\\big).\n$$\n\nYour program must implement the above procedure and produce the overfitting gaps for the following test suite of parameter values. All arrays are ordered lists; all numbers are real-valued scalars.\n\nTest case A (happy path, moderate $\\beta$, mild shift):\n- Validation predicted probabilities: $[0.92, 0.81, 0.76, 0.63, 0.58, 0.49, 0.45, 0.41, 0.35, 0.22, 0.17, 0.08]$.\n- Validation labels: $[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]$.\n- Test predicted probabilities: $[0.90, 0.84, 0.79, 0.70, 0.34, 0.30, 0.27, 0.20, 0.15, 0.12]$.\n- Test labels: $[1, 1, 0, 1, 0, 0, 0, 0, 0, 0]$.\n- $\\beta = 1$.\n\nTest case B (recall-emphasis, stronger shift):\n- Validation predicted probabilities: $[0.88, 0.83, 0.77, 0.74, 0.62, 0.52, 0.47, 0.40, 0.33, 0.25]$.\n- Validation labels: $[1, 1, 1, 0, 1, 0, 0, 0, 0, 0]$.\n- Test predicted probabilities: $[0.86, 0.80, 0.55, 0.51, 0.49, 0.45, 0.38, 0.31, 0.28, 0.18]$.\n- Test labels: $[0, 1, 1, 0, 0, 0, 0, 0, 0, 0]$.\n- $\\beta = 2$.\n\nTest case C (flat scores, tie handling):\n- Validation predicted probabilities: $[0.50, 0.50, 0.50, 0.50]$.\n- Validation labels: $[1, 0, 0, 1]$.\n- Test predicted probabilities: $[0.50, 0.50, 0.50]$.\n- Test labels: $[0, 0, 1]$.\n- $\\beta = 1$.\n\nTest case D (precision-emphasis, extreme shift causing no positives predicted on $X_{\\text{test}}$ at $t^\\ast$):\n- Validation predicted probabilities: $[0.95, 0.90, 0.85, 0.40, 0.35, 0.30]$.\n- Validation labels: $[1, 1, 1, 0, 0, 0]$.\n- Test predicted probabilities: $[0.60, 0.55, 0.50, 0.45, 0.20, 0.10]$.\n- Test labels: $[1, 0, 0, 0, 0, 0]$.\n- $\\beta = 0.5$.\n\nImplementation requirements:\n- Use the exhaustive search over the specified candidate thresholds on $X_{\\text{val}}$ to select $t^\\ast$.\n- When computing $F_\\beta$, use the equivalent count-based expression\n$$\nF_\\beta = \\frac{(1+\\beta^2)\\,TP}{(1+\\beta^2)\\,TP + \\beta^2\\,FN + FP},\n$$\nwith the convention that $F_\\beta = 0$ if the denominator equals $0$.\n- For each test case, compute $\\Delta$ as specified above.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each $\\Delta$ expressed as a decimal rounded to $6$ places, in the order A, B, C, D. For example, the format must be `[0.123456,0.234567,0.345678,0.456789]`.", "solution": "The problem requires us to quantify threshold overfitting for a binary classification model. This is achieved by calculating the performance gap, $\\Delta$, between a validation set $X_{\\text{val}}$ and a test set $X_{\\text{test}}$. The performance metric is the $F_\\beta$ score. The core of the problem lies in first selecting an optimal classification threshold $t^\\ast$ on the validation set and then measuring how well the performance at this threshold generalizes to the unseen test set. A large positive gap $\\Delta = F_\\beta(X_{\\text{val}}; t^\\ast) - F_\\beta(X_{\\text{test}}; t^\\ast)$ indicates that the threshold $t^\\ast$ is overfitted to the specific characteristics of the validation data.\n\nThe solution is implemented by following a deterministic, step-by-step procedure for each test case provided.\n\n**Step 1: Identification of Candidate Thresholds**\n\nThe classification outcome for a sample with predicted probability $p$ changes only when the threshold $t$ crosses the value of $p$. Therefore, to find the threshold that maximizes the $F_\\beta$ score, it is sufficient to evaluate a finite set of candidate thresholds. The problem specifies this set to be $\\{0\\} \\cup S \\cup \\{1\\}$, where $S$ is the set of unique predicted probabilities in the validation set, $p^{(\\text{val})}_i$. We construct this set of candidates, denoted $T_{\\text{candidates}}$, and sort it in ascending order. Sorting is crucial for correctly implementing the specified tie-breaking rule.\n\n**Step 2: Optimal Threshold Selection on the Validation Set**\n\nWe perform an exhaustive search for the optimal threshold $t^\\ast$ over the sorted candidate set $T_{\\text{candidates}}$. For each candidate threshold $t \\in T_{\\text{candidates}}$, we compute the $F_\\beta$ score on the validation data $X_{\\text{val}}$.\n\nThe predicted label for an instance $i$ is given by $\\hat{y}_i(t) = 1$ if its probability score $p_i \\ge t$, and $\\hat{y}_i(t) = 0$ otherwise. Based on these predictions and the true labels $y_i$, we count the number of True Positives ($TP$), False Positives ($FP$), and False Negatives ($FN$).\n\nThe $F_\\beta$ score is then calculated using the provided count-based formula, which is numerically stable and avoids division by zero in intermediate precision and recall calculations:\n$$\nF_\\beta = \\frac{(1+\\beta^2) \\cdot TP}{(1+\\beta^2) \\cdot TP + \\beta^2 \\cdot FN + FP}\n$$\nIf the denominator is zero (which occurs only if $TP=FP=FN=0$), the score is defined to be $F_\\beta = 0$.\n\nWe iterate through the sorted candidates $t \\in T_{\\text{candidates}}$ and track the maximum $F_\\beta$ score found so far, let's call it $F_{\\beta, \\text{max}}$, and the threshold that achieved it, $t^\\ast$. The update rule is as follows: if the current threshold $t$ yields an $F_\\beta$ score greater than or equal to $F_{\\beta, \\text{max}}$, we update $F_{\\beta, \\text{max}}$ to this new score and set $t^\\ast = t$. Because we are iterating through thresholds in increasing order, this rule ensures that if multiple thresholds yield the same maximum score, the largest of these thresholds is chosen as $t^\\ast$, satisfying the problem's tie-breaking requirement.\n\n**Step 3: Calculation of the Overfitting Gap**\n\nOnce the optimal threshold $t^\\ast$ is determined from the validation set, we can calculate the overfitting gap $\\Delta$.\n\nFirst, the $F_\\beta$ score on the validation set is simply the maximum score found in the previous step: $F_\\beta(X_{\\text{val}}; t^\\ast) = F_{\\beta, \\text{max}}$.\nSecond, we calculate the $F_\\beta$ score on the test set, $F_\\beta(X_{\\text{test}}; t^\\ast)$, by applying the same threshold $t^\\ast$ to the test probabilities $p^{(\\text{test})}_j$ to generate predictions and then using the same $F_\\beta$ formula with the test set's $TP$, $FP$, and $FN$ counts.\n\nThe overfitting gap is the difference between these two scores:\n$$\n\\Delta = F_\\beta(X_{\\text{val}}; t^\\ast) - F_\\beta(X_{\\text{test}}; t^\\ast)\n$$\n\nThis procedure is encapsulated in a program that processes each of the four specified test cases, calculates the corresponding $\\Delta$, and formats the results as a single comma-separated list. The implementation uses the `numpy` library for efficient vectorized computation of counts ($TP, FP, FN$) and array manipulations.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_overfitting_gap(p_val, y_val, p_test, y_test, beta):\n    \"\"\"\n    Computes the F-beta overfitting gap for a given set of parameters.\n\n    1. Finds the optimal threshold t* on the validation set.\n    2. Calculates F_beta(X_val; t*) and F_beta(X_test; t*).\n    3. Returns the difference Delta.\n    \"\"\"\n    # Ensure inputs are numpy arrays for vectorized operations\n    p_val, y_val = np.array(p_val), np.array(y_val)\n    p_test, y_test = np.array(p_test), np.array(y_test)\n\n    # Step 1: Identify candidate thresholds\n    # The set is {0} U S U {1}, where S is the set of unique validation probabilities.\n    candidate_thresholds = sorted(list(set(p_val) | {0., 1.}))\n\n    # Step 2: Find the optimal threshold t* on the validation set\n    max_f_beta = -1.0\n    t_star = -1.0\n\n    beta_sq = beta**2\n    one_plus_beta_sq = 1 + beta_sq\n    \n    total_pos_val = np.sum(y_val)\n\n    for t in candidate_thresholds:\n        y_hat_val = (p_val >= t).astype(int)\n\n        tp = np.sum((y_hat_val == 1)  (y_val == 1))\n        fp = np.sum((y_hat_val == 1)  (y_val == 0))\n        # fn = np.sum((y_hat_val == 0)  (y_val == 1))\n        fn = total_pos_val - tp\n\n        numerator = one_plus_beta_sq * tp\n        denominator = (one_plus_beta_sq * tp) + (beta_sq * fn) + fp\n\n        f_beta = numerator / denominator if denominator > 0 else 0.0\n\n        # Tie-breaking rule: select the largest threshold.\n        # Since we iterate through sorted thresholds, this condition correctly captures it.\n        if f_beta >= max_f_beta:\n            max_f_beta = f_beta\n            t_star = t\n            \n    f_beta_val = max_f_beta\n\n    # Step 3: Calculate F_beta on the test set using t* and find the gap\n    total_pos_test = np.sum(y_test)\n    y_hat_test = (p_test >= t_star).astype(int)\n\n    tp_test = np.sum((y_hat_test == 1)  (y_test == 1))\n    fp_test = np.sum((y_hat_test == 1)  (y_test == 0))\n    # fn_test = np.sum((y_hat_test == 0)  (y_test == 1))\n    fn_test = total_pos_test - tp_test\n\n    numerator_test = one_plus_beta_sq * tp_test\n    denominator_test = (one_plus_beta_sq * tp_test) + (beta_sq * fn_test) + fp_test\n\n    f_beta_test = numerator_test / denominator_test if denominator_test > 0 else 0.0\n\n    gap = f_beta_val - f_beta_test\n    return gap\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        # Test case A\n        {\n            \"p_val\": [0.92, 0.81, 0.76, 0.63, 0.58, 0.49, 0.45, 0.41, 0.35, 0.22, 0.17, 0.08],\n            \"y_val\": [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n            \"p_test\": [0.90, 0.84, 0.79, 0.70, 0.34, 0.30, 0.27, 0.20, 0.15, 0.12],\n            \"y_test\": [1, 1, 0, 1, 0, 0, 0, 0, 0, 0],\n            \"beta\": 1.0\n        },\n        # Test case B\n        {\n            \"p_val\": [0.88, 0.83, 0.77, 0.74, 0.62, 0.52, 0.47, 0.40, 0.33, 0.25],\n            \"y_val\": [1, 1, 1, 0, 1, 0, 0, 0, 0, 0],\n            \"p_test\": [0.86, 0.80, 0.55, 0.51, 0.49, 0.45, 0.38, 0.31, 0.28, 0.18],\n            \"y_test\": [0, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n            \"beta\": 2.0\n        },\n        # Test case C\n        {\n            \"p_val\": [0.50, 0.50, 0.50, 0.50],\n            \"y_val\": [1, 0, 0, 1],\n            \"p_test\": [0.50, 0.50, 0.50],\n            \"y_test\": [0, 0, 1],\n            \"beta\": 1.0\n        },\n        # Test case D\n        {\n            \"p_val\": [0.95, 0.90, 0.85, 0.40, 0.35, 0.30],\n            \"y_val\": [1, 1, 1, 0, 0, 0],\n            \"p_test\": [0.60, 0.55, 0.50, 0.45, 0.20, 0.10],\n            \"y_test\": [1, 0, 0, 0, 0, 0],\n            \"beta\": 0.5\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        gap = compute_overfitting_gap(\n            case[\"p_val\"], case[\"y_val\"],\n            case[\"p_test\"], case[\"y_test\"],\n            case[\"beta\"]\n        )\n        results.append(gap)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```", "id": "3188635"}, {"introduction": "A fundamental assumption in model validation is that the validation set is a representative sample of the data the model will encounter in the future. This practice explores what happens when this assumption is violated in a structured way, creating a \"gap\" in the feature space seen during validation. You will see how this can mislead hyperparameter selection and lead to a model that performs poorly on the comprehensive test set, a critical lesson in understanding and mitigating extrapolation risk [@problem_id:3188594].", "problem": "You are asked to examine the effect of systematic coverage gaps in the validation set on model selection and to quantify the resulting extrapolation risk on a comprehensive test set. Work entirely within the following purely mathematical and algorithmic setup.\n\nLet the feature domain be the closed interval $[0,1]$, and let the true regression function be $f:[0,1]\\to\\mathbb{R}$. Consider squared loss. The training distribution for $X$ is uniform on $[0,1]$, and the learned predictor in a polynomial hypothesis class is defined as the minimizer of the expected squared loss under this training distribution. The validation distribution is uniform on $S_{\\mathrm{val}}=[0,a]\\cup[b,1]$, for given $0\\le a\\le b\\le 1$, renormalized to unit mass. The test distribution is uniform on $[0,1]$. All trigonometric arguments are in radians.\n\nFundamental base and definitions to be used:\n- For any hypothesis class $\\mathcal{H}$ and loss $\\ell$, the risk of $h\\in\\mathcal{H}$ under a distribution $\\mathbb{P}$ over $X$ is $R_{\\mathbb{P}}(h)=\\mathbb{E}_{X\\sim \\mathbb{P}}[\\ell(h(X),f(X))]$. Under squared loss, $\\ell(y,\\hat y)=(y-\\hat y)^2$.\n- Under squared loss and a fixed distribution over $X$, the risk minimizer in a linear hypothesis class is the orthogonal projection (with respect to the $L^2$ inner product defined by that distribution) of $f$ onto the linear span of the class.\n- Let $\\{P_k\\}_{k\\ge 0}$ denote the Legendre polynomials on $[-1,1]$, which are orthogonal with respect to the uniform weight. Define the orthonormal basis on $[0,1]$ by $\\varphi_k(x)=\\sqrt{2k+1}\\,P_k(2x-1)$ so that $\\int_0^1 \\varphi_k(x)\\varphi_j(x)\\,dx=\\delta_{kj}$ for all $k,j\\in\\mathbb{N}_0$.\n\nModel and selection protocol:\n- For a nonnegative integer $d$, define the hypothesis class $\\mathcal{H}_d=\\mathrm{span}\\{\\varphi_0,\\dots,\\varphi_d\\}$. The training distribution is uniform on $[0,1]$, so the risk-minimizing predictor in $\\mathcal{H}_d$ is\n$$\ng_d^\\star(x)=\\sum_{k=0}^d a_k\\,\\varphi_k(x),\\quad\\text{where } a_k=\\int_0^1 f(x)\\,\\varphi_k(x)\\,dx.\n$$\n- Given a finite candidate degree set $\\mathcal{D}$, model selection chooses\n$$\nd^\\star\\in\\arg\\min_{d\\in\\mathcal{D}} R_{\\mathrm{val}}(g_d^\\star),\\quad R_{\\mathrm{val}}(g)=\\frac{1}{|S_{\\mathrm{val}}|}\\int_{S_{\\mathrm{val}}} \\bigl(f(x)-g(x)\\bigr)^2\\,dx,\n$$\nwith $|S_{\\mathrm{val}}|=(a-0)+(1-b)$ when $a\\le b$. Break ties by choosing the smallest $d$.\n\nQuantities to report per test case:\n- The selected degree $d^\\star$.\n- The test mean squared error $R_{\\mathrm{test}}(g_{d^\\star}^\\star)=\\int_0^1 \\bigl(f(x)-g_{d^\\star}^\\star(x)\\bigr)^2\\,dx$.\n- The extrapolation risk fraction\n$$\n\\mathrm{Frac}_{\\mathrm{mask}}(d^\\star)=\\frac{\\int_a^b \\bigl(f(x)-g_{d^\\star}^\\star(x)\\bigr)^2\\,dx}{\\int_0^1 \\bigl(f(x)-g_{d^\\star}^\\star(x)\\bigr)^2\\,dx},\n$$\ndefined to be $0$ if the denominator is $0$.\n\nNumerical requirement:\n- All required integrals must be computed numerically using Gauss–Legendre quadrature on $[L,R]\\subseteq[0,1]$ with at least $N_q=200$ nodes, obtained by an affine mapping from $[-1,1]$. This ensures stable and accurate approximation while remaining fully deterministic.\n- Use the orthonormal basis $\\{\\varphi_k\\}$ as defined above. Do not approximate training by finite samples; use the orthogonal projection coefficients $a_k=\\int_0^1 f(x)\\,\\varphi_k(x)\\,dx$.\n- Angles in trigonometric functions are in radians.\n\nCandidate degree set and test suite:\n- Candidate degree set $\\mathcal{D}=\\{0,1,3,5,7\\}$.\n- Four test cases $(f,a,b)$:\n    - Case $1$: $f(x)=\\sin(2\\pi x)$, $(a,b)=(0.6,0.8)$.\n    - Case $2$: $f(x)=x^2+0.05\\sin(8\\pi x)$, $(a,b)=(0.4,0.4)$.\n    - Case $3$: $f(x)=\\dfrac{1}{1+25(x-0.5)^2}$, $(a,b)=(0.45,0.55)$.\n    - Case $4$: $f(x)=\\sin(10\\pi x)$, $(a,b)=(0.0,0.3)$.\n\nTask:\n- Implement a program that, for each test case, computes $g_d^\\star$ for all $d\\in\\mathcal{D}$, selects $d^\\star$ by validation risk on $S_{\\mathrm{val}}$, then reports $[d^\\star, R_{\\mathrm{test}}(g_{d^\\star}^\\star), \\mathrm{Frac}_{\\mathrm{mask}}(d^\\star)]$.\n- For numerical reporting, round floating-point outputs to six decimal places.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the three-element list for a test case, and there are no spaces anywhere in the line. For example, the printed line should look like `[[d_1,r_1,q_1],[d_2,r_2,q_2],[d_3,r_3,q_3],[d_4,r_4,q_4]]` with each $r_i$ and $q_i$ rounded to six decimals.", "solution": "The user requests a solution to a problem involving model selection in a regression context, where the validation data has a systematic gap. The task is to quantify the effect of this gap on the choice of model and the resulting extrapolation error on a complete test set. The problem is well-defined, scientifically sound, and computationally tractable. I will proceed with a full solution.\n\n### Principle-Based Solution Design\n\nThe solution is constructed based on fundamental principles of statistical learning theory, approximation theory, and numerical analysis.\n\n**1. Function Approximation with Orthogonal Polynomials**\n\nThe core of the problem lies in approximating a true function $f(x)$ with a sequence of polynomials. The hypothesis classes $\\mathcal{H}_d$ are nested linear spaces spanned by a set of basis functions, $\\mathcal{H}_d = \\mathrm{span}\\{\\varphi_0, \\ldots, \\varphi_d\\}$. The problem specifies a particular basis, $\\{\\varphi_k\\}_{k \\ge 0}$, derived from the Legendre polynomials. These basis functions $\\varphi_k(x) = \\sqrt{2k+1} P_k(2x-1)$ are constructed to be orthonormal with respect to the standard $L^2$ inner product on $[0,1]$ with a uniform weighting function, i.e., $\\langle \\varphi_k, \\varphi_j \\rangle = \\int_0^1 \\varphi_k(x) \\varphi_j(x) dx = \\delta_{kj}$, where $\\delta_{kj}$ is the Kronecker delta.\n\nThe training distribution for the feature $X$ is uniform on $[0,1]$. This means the training risk is $R_{\\text{train}}(g) = \\mathbb{E}_{X \\sim U[0,1]}[(f(X)-g(X))^2] = \\int_0^1 (f(x)-g(x))^2 dx$. For a given hypothesis class $\\mathcal{H}_d$, the optimal predictor $g_d^\\star$ that minimizes this training risk is the orthogonal projection of the true function $f$ onto the subspace $\\mathcal{H}_d$. Due to the orthonormality of the basis $\\{\\varphi_k\\}$, this projection takes a simple form analogous to a Fourier series expansion:\n$$\ng_d^\\star(x) = \\sum_{k=0}^d \\langle f, \\varphi_k \\rangle \\varphi_k(x) = \\sum_{k=0}^d a_k \\varphi_k(x)\n$$\nwhere the coefficients are given by $a_k = \\int_0^1 f(x) \\varphi_k(x) dx$. This formulation avoids the need for empirical risk minimization on a finite training set and allows for a direct, analytical-style solution.\n\n**2. Risk Evaluation and Model Selection**\n\nWhile the models $g_d^\\star$ are optimal for their class under the training distribution, we must select the best class (i.e., the best degree $d$) from the candidate set $\\mathcal{D}=\\{0,1,3,5,7\\}$. The guiding principle is to choose the model that is expected to generalize best to new data. This is typically estimated using a validation set.\n\nThe problem introduces a validation distribution that is uniform on $S_{\\mathrm{val}} = [0,a] \\cup [b,1]$, but is zero on the \"masked\" interval $(a,b)$. The validation risk is thus:\n$$\nR_{\\mathrm{val}}(g_d^\\star) = \\frac{1}{|S_{\\mathrm{val}}|} \\int_{S_{\\mathrm{val}}} (f(x) - g_d^\\star(x))^2 dx = \\frac{1}{a+1-b} \\left( \\int_0^a (f(x) - g_d^\\star(x))^2 dx + \\int_b^1 (f(x) - g_d^\\star(x))^2 dx \\right)\n$$\nModel selection proceeds by calculating $R_{\\mathrm{val}}(g_d^\\star)$ for each $d \\in \\mathcal{D}$ and choosing the degree $d^\\star$ that yields the minimum validation risk. This protocol simulates a common practical scenario where available data for model tuning may not cover the entire operational domain.\n\n**3. Quantifying Performance and Extrapolation Risk**\n\nThe true performance of the selected model $g_{d^\\star}^\\star$ is measured by its test risk, which, like the training risk, is evaluated over the complete domain $[0,1]$:\n$$\nR_{\\mathrm{test}}(g_{d^\\star}^\\star) = \\int_0^1 (f(x) - g_{d^\\star}^\\star(x))^2 dx\n$$\nThanks to the properties of orthogonal projection and Parseval's theorem, this test risk can be computed accurately and efficiently without integrating the squared error directly. The squared error is the squared norm of the residual function, $f - g_{d^\\star}^\\star$, which is orthogonal to the subspace $\\mathcal{H}_{d^\\star}$. This leads to the identity:\n$$\nR_{\\mathrm{test}}(g_{d^\\star}^\\star) = \\int_0^1 f(x)^2 dx - \\sum_{k=0}^{d^\\star} a_k^2\n$$\nThis formula is numerically more stable than integrating the squared difference, as it avoids potential subtractive cancellation.\n\nThe core of the analysis is to measure the extrapolation risk, which is the error concentrated in the region $(a,b)$ unseen during validation. The extrapolation risk fraction, $\\mathrm{Frac}_{\\mathrm{mask}}(d^\\star)$, quantifies this:\n$$\n\\mathrm{Frac}_{\\mathrm{mask}}(d^\\star) = \\frac{\\int_a^b (f(x) - g_{d^\\star}^\\star(x))^2 dx}{R_{\\mathrm{test}}(g_{d^\\star}^\\star)}\n$$\nThis ratio indicates what proportion of the total test error comes from the model's failure to predict correctly in the un-validated region. A high value signifies that the validation risk was a poor proxy for the true test risk, a direct consequence of the distributional shift.\n\n**4. Numerical Implementation using Gauss-Legendre Quadrature**\n\nAll required integrals—for the coefficients $a_k$, for the validation and masked risks, and for $\\int f^2 dx$—do not necessarily have closed-form antiderivatives. The problem mandates a specific, deterministic numerical method: Gauss-Legendre quadrature with at least $N_q=200$ nodes. For an integral $\\int_L^R h(x) dx$, this method provides a highly accurate approximation:\n$$\n\\int_L^R h(x) dx \\approx \\frac{R-L}{2} \\sum_{i=1}^{N_q} w_i h\\left(\\frac{R-L}{2} x_i + \\frac{R+L}{2}\\right)\n$$\nwhere $\\{x_i\\}$ and $\\{w_i\\}$ are the nodes and weights of the quadrature rule on the canonical interval $[-1,1]$. This approach ensures that the calculations are repeatable and precise, suitable for a scientific computing problem.\n\nThe algorithm proceeds by first computing the coefficients $a_k$ up to the maximum degree in $\\mathcal{D}$. Then, for each candidate degree $d$, it constructs the predictor $g_d^\\star(x)$ and computes its validation risk. After selecting the best degree $d^\\star$, it calculates the final test risk and extrapolation fraction using the formulas above. This entire process is repeated for each test case.", "answer": "```python\nimport numpy as np\nfrom scipy.special import roots_legendre, eval_legendre\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    It implements the full pipeline from model definition to final metric calculation.\n    \"\"\"\n\n    # --- Problem Constants and Numerical Setup ---\n    DEGREES = [0, 1, 3, 5, 7]\n    N_Q = 200 # Number of quadrature points\n\n    # Pre-compute Gauss-Legendre nodes and weights for the interval [-1, 1]\n    X_GL, W_GL = roots_legendre(N_Q)\n\n    # --- Helper Functions ---\n\n    def gauss_legendre_integrate(func, a, b):\n        \"\"\"\n        Numerically integrates a function `func` over the interval [a, b]\n        using N_Q-point Gauss-Legendre quadrature.\n        \"\"\"\n        if a >= b:\n            return 0.0\n        # Affine transformation of nodes from [-1, 1] to [a, b]\n        t = 0.5 * (b - a) * X_GL + 0.5 * (b + a)\n        integral = 0.5 * (b - a) * np.sum(W_GL * func(t))\n        return integral\n\n    def phi(k, x):\n        \"\"\"\n        Evaluates the k-th orthonormal basis function phi_k at points x.\n        The basis is orthonormal on [0, 1].\n        \"\"\"\n        # Ensure x is a numpy array for vectorized operations\n        x = np.asarray(x)\n        # Map x from [0, 1] to [-1, 1] for Legendre polynomial evaluation\n        y = 2 * x - 1\n        return np.sqrt(2 * k + 1) * eval_legendre(k, y)\n\n    def process_case(f, a, b):\n        \"\"\"\n        Processes a single test case (f, a, b) and returns the required metrics.\n        \"\"\"\n        max_d = max(DEGREES)\n        \n        # 1. Pre-compute all necessary coefficients a_k up to max_d\n        a_coeffs = np.zeros(max_d + 1)\n        for k in range(max_d + 1):\n            integrand = lambda x: f(x) * phi(k, x)\n            a_coeffs[k] = gauss_legendre_integrate(integrand, 0, 1)\n\n        # 2. Iterate through candidate degrees to find the best one via validation\n        validation_risks = []\n        for d in DEGREES:\n            # Define the predictor g_d*(x) and squared error function\n            def g_d_star(x):\n                x = np.asarray(x)\n                res = np.zeros_like(x, dtype=float)\n                for k in range(d + 1):\n                    res += a_coeffs[k] * phi(k, x)\n                return res\n\n            def squared_error(x):\n                return (f(x) - g_d_star(x))**2\n\n            # Calculate validation risk R_val over S_val = [0, a] U [b, 1]\n            integral_val = gauss_legendre_integrate(squared_error, 0, a) + \\\n                           gauss_legendre_integrate(squared_error, b, 1)\n            \n            s_val_measure = a + (1 - b)\n            r_val = integral_val / s_val_measure if s_val_measure > 1e-12 else 0.0\n            validation_risks.append(r_val)\n\n        # 3. Select d_star: the degree that minimizes validation risk\n        # Ties are broken by choosing the smallest degree, which is handled naturally\n        # by finding the first index of the minimum risk.\n        min_risk = np.min(validation_risks)\n        d_star_index = np.where(np.isclose(validation_risks, min_risk))[0][0]\n        d_star = DEGREES[d_star_index]\n\n        # 4. Calculate final quantities for the selected model g_{d_star}^*\n        \n        # Test Risk R_test using the numerically stable formula\n        integral_f_squared = gauss_legendre_integrate(lambda x: f(x)**2, 0, 1)\n        sum_a_k_squared = np.sum(a_coeffs[:d_star + 1]**2)\n        r_test = max(0, integral_f_squared - sum_a_k_squared)\n\n        # Extrapolation Risk Fraction\n        frac_mask = 0.0\n        if r_test > 1e-12:\n            def g_d_star_final(x):\n                x = np.asarray(x)\n                res = np.zeros_like(x, dtype=float)\n                for k in range(d_star + 1):\n                    res += a_coeffs[k] * phi(k, x)\n                return res\n            \n            def squared_error_final(x):\n                return (f(x) - g_d_star_final(x))**2\n\n            integral_mask = gauss_legendre_integrate(squared_error_final, a, b)\n            frac_mask = integral_mask / r_test\n        \n        return d_star, r_test, frac_mask\n\n    # --- Test Suite Definition ---\n    test_cases = [\n        (lambda x: np.sin(2 * np.pi * x), 0.6, 0.8),\n        (lambda x: x**2 + 0.05 * np.sin(8 * np.pi * x), 0.4, 0.4),\n        (lambda x: 1.0 / (1.0 + 25 * (x - 0.5)**2), 0.45, 0.55),\n        (lambda x: np.sin(10 * np.pi * x), 0.0, 0.3)\n    ]\n\n    # --- Main Execution Loop ---\n    results = []\n    for f, a, b in test_cases:\n        d_star, r_test, frac_mask = process_case(f, a, b)\n        results.append((d_star, r_test, frac_mask))\n\n    # --- Format and Print Final Output ---\n    formatted_results = [f\"[{d},{r:.6f},{q:.6f}]\" for d, r, q in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3188594"}, {"introduction": "Standard random data splits rely on the assumption that data points are independent and identically distributed (I.I.D.). However, this assumption breaks down for temporal data, where observations have a natural order and dependency. This exercise challenges you to move beyond simple random splits by implementing and evaluating time-aware strategies, such as contiguous blocks and interleaved splitting, to see firsthand how the right splitting methodology is crucial for reliable model evaluation in time-series forecasting [@problem_id:3188604].", "problem": "You are given a synthetic, fully specified statistical learning task designed to evaluate how training, validation, and test splitting strategies interact with temporal covariates and static features when feature extraction uses sliding windows. Your goal is to write a program that implements a complete training-validation-test pipeline, selects a feature extraction window by validation, and reports the test-set mean squared error (expressed as a decimal), while respecting time-aware constraints. The program must be self-contained and must not read input.\n\nThe data generating process is defined as follows. There is a single static feature and a univariate temporal covariate. Let $t \\in \\{0,1,\\dots,T-1\\}$ index time. Define a temporal covariate sequence $c_t$ by\n$$\nc_t = A \\cdot \\sin\\left(\\frac{2\\pi}{P} \\, t + \\phi_t\\right),\n$$\nwhere the phase $\\phi_t$ may change once at a designated change-point $T_{\\text{shift}}$. Specifically,\n$$\n\\phi_t = \n\\begin{cases}\n0  \\text{if } t  T_{\\text{shift}},\\\\\n\\phi_{\\text{shift}}  \\text{if } t \\ge T_{\\text{shift}}.\n\\end{cases}\n$$\nAll trigonometric arguments are in radians. The static feature is a constant scalar $s$ for all times. The response $y_t$ is generated using a rolling mean of the temporal covariate with a true window size that may change at $T_{\\text{shift}}$:\n$$\nw^*(t) = \n\\begin{cases}\nw_1  \\text{if } t  T_{\\text{shift}},\\\\\nw_2  \\text{if } t \\ge T_{\\text{shift}},\n\\end{cases}\n$$\nand\n$$\ny_t = \\beta_0 + \\beta_1 \\cdot \\frac{1}{w^*(t)} \\sum_{j=1}^{w^*(t)} c_{t-j} + \\beta_2 \\cdot s,\n$$\nwhich is well-defined only when $t \\ge \\max\\{w_1,w_2\\}$. There is no stochastic noise.\n\nYou must build a linear model to predict $y_t$ from features constructed with a candidate rolling window $w$ applied to the observed covariate $c_t$:\n- For any candidate window $w$, the feature vector at time $t$ is\n$$\nx^{(w)}_t = \\left[\\,1,\\;\\frac{1}{w}\\sum_{j=1}^{w} c_{t-j},\\; s\\,\\right],\n$$\nvalid for $t \\ge w$. You must consider multiple candidate windows from a provided finite set $\\mathcal{W}$ and choose the best window by validation.\n\nTraining-validation-test process and constraints:\n- Let the set of “eligible” time indices be $\\mathcal{I} = \\{t_{\\min}, t_{\\min}+1, \\dots, T-1\\}$ where $t_{\\min} = \\max\\left(\\{w_1,w_2\\} \\cup \\mathcal{W}\\right)$ to ensure all rolling means are well-defined.\n- You must support two deterministic splitting strategies:\n\n  1. Contiguous time blocks with gaps: Given proportions $p_{\\text{train}} = 0.6$, $p_{\\text{val}} = 0.2$, and an integer gap $g \\ge 0$, split $\\mathcal{I}$ into disjoint contiguous segments for training, validation, and test, inserting a gap of size $g$ between train and validation, and another gap of size $g$ between validation and test. Let $n = |\\mathcal{I}|$. Define $n_{\\text{train}} = \\lfloor 0.6n \\rfloor$, $n_{\\text{val}} = \\lfloor 0.2n \\rfloor$, and let the remaining indices after accounting for gaps be assigned to the test set (this remaining count is $n - n_{\\text{train}} - n_{\\text{val}} - 2g$ and is assumed nonnegative in the provided test suite). The order of time must be preserved within each segment.\n  \n  2. Interleaved periodic split: Fix a period $m = 5$. Assign indices in $\\mathcal{I}$ to sets by their residue modulo $m$: training if $t \\bmod m \\in \\{0,1,2\\}$, validation if $t \\bmod m \\in \\{3\\}$, test if $t \\bmod m \\in \\{4\\}$.\n\n- For each candidate window $w \\in \\mathcal{W}$, fit an ordinary least squares linear regression on the training set to minimize empirical squared loss for predicting $y_t$ from $x^{(w)}_t$. Denote the training design matrix by $X_{\\text{train}}^{(w)}$ and the training response vector by $y_{\\text{train}}$. The least squares estimate $\\hat{\\theta}^{(w)}$ satisfies the normal equations $X_{\\text{train}}^{(w)\\top} X_{\\text{train}}^{(w)} \\hat{\\theta}^{(w)} = X_{\\text{train}}^{(w)\\top} y_{\\text{train}}$. Use any numerically stable method to obtain one solution (for instance, the Moore-Penrose pseudoinverse).\n- Evaluate each $w$ on the validation set by its mean squared error\n$$\n\\text{MSE}_{\\text{val}}(w) = \\frac{1}{|\\mathcal{V}|} \\sum_{t \\in \\mathcal{V}} \\left(y_t - x^{(w)}_t \\cdot \\hat{\\theta}^{(w)}\\right)^2,\n$$\nand select the window $\\hat{w}$ that minimizes $\\text{MSE}_{\\text{val}}(w)$. In case of ties in $\\text{MSE}_{\\text{val}}$, choose the smallest $w$ among the minimizers.\n- After selecting $\\hat{w}$, refit the model using the union of training and validation sets with the same feature definition to obtain $\\hat{\\theta}^{(\\hat{w})}_{\\text{refit}}$.\n- Report the test-set mean squared error\n$$\n\\text{MSE}_{\\text{test}} = \\frac{1}{|\\mathcal{T}|} \\sum_{t \\in \\mathcal{T}} \\left(y_t - x^{(\\hat{w})}_t \\cdot \\hat{\\theta}^{(\\hat{w})}_{\\text{refit}}\\right)^2.\n$$\n\nAngle unit requirement: all angles are in radians.\n\nYour program must implement the above procedure for the following test suite of parameter settings. Each test case specifies $(T, P, A, \\beta_0, \\beta_1, \\beta_2, s, w_1, w_2, T_{\\text{shift}}, \\phi_{\\text{shift}}, \\text{split}, \\text{extra}, \\mathcal{W})$:\n\n- Test case $1$ (happy path, no seasonal phase or window change):\n  - $T=120$, $P=24$, $A=2$, $\\beta_0=0.5$, $\\beta_1=1.2$, $\\beta_2=-0.7$, $s=1.3$.\n  - $w_1=8$, $w_2=8$, $T_{\\text{shift}}=120$, $\\phi_{\\text{shift}}=0$.\n  - Split: contiguous with gap $g=3$; use $p_{\\text{train}}=0.6$, $p_{\\text{val}}=0.2$ as defined above.\n  - Candidate windows $\\mathcal{W}=\\{4,8,12\\}$.\n\n- Test case $2$ (seasonal phase and window change only at test time):\n  - $T=120$, $P=24$, $A=2$, $\\beta_0=0.5$, $\\beta_1=1.2$, $\\beta_2=-0.7$, $s=1.3$.\n  - $w_1=4$, $w_2=12$, $T_{\\text{shift}}=80$, $\\phi_{\\text{shift}}=\\frac{\\pi}{2}$.\n  - Split: contiguous with gap $g=3$; use $p_{\\text{train}}=0.6$, $p_{\\text{val}}=0.2$ as defined above.\n  - Candidate windows $\\mathcal{W}=\\{4,8,12\\}$.\n\n- Test case $3$ (interleaved split under seasonal phase and window change):\n  - $T=120$, $P=24$, $A=2$, $\\beta_0=0.5$, $\\beta_1=1.2$, $\\beta_2=-0.7$, $s=1.3$.\n  - $w_1=6$, $w_2=10$, $T_{\\text{shift}}=60$, $\\phi_{\\text{shift}}=\\frac{\\pi}{3}$.\n  - Split: interleaved with period $m=5$, assigning training to residues $\\{0,1,2\\}$, validation to $\\{3\\}$, test to $\\{4\\}$.\n  - Candidate windows $\\mathcal{W}=\\{4,6,10,12\\}$.\n\n- Test case $4$ (boundary on window size and no change):\n  - $T=40$, $P=10$, $A=1.5$, $\\beta_0=0.5$, $\\beta_1=1.2$, $\\beta_2=-0.7$, $s=1.3$.\n  - $w_1=1$, $w_2=1$, $T_{\\text{shift}}=40$, $\\phi_{\\text{shift}}=0$.\n  - Split: contiguous with gap $g=0$; use $p_{\\text{train}}=0.6$, $p_{\\text{val}}=0.2$ as defined above.\n  - Candidate windows $\\mathcal{W}=\\{1,2\\}$.\n\nImplementation requirements:\n- For each test case, build $c_t$ and $y_t$ exactly as described.\n- Compute features using only past covariate values $c_{t-j}$; do not use future values relative to $t$.\n- Apply the specified split strategy to the eligible index set $\\mathcal{I} = \\{t_{\\min},\\dots,T-1\\}$ with $t_{\\min} = \\max(\\{w_1,w_2\\}\\cup \\mathcal{W})$.\n- Train per-candidate models on training data, select $\\hat{w}$ by minimizing validation mean squared error with tie-breaker favoring smaller $w$, refit on train plus validation for $\\hat{w}$, and compute the test mean squared error.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of test cases $1$ through $4$, with each mean squared error rounded to exactly $6$ decimal places. For example, an output might look like `[0.000123,0.045678,0.010000,0.000000]`.", "solution": "The problem presents a synthetic, deterministic statistical learning task designed to evaluate model selection strategies in a time-series context with potential structural breaks. The procedure involves data generation, feature engineering using sliding windows, time-aware data splitting, model training, hyperparameter (window size) selection via validation, and final performance evaluation on a test set. The solution adheres strictly to the specified process.\n\n**1. Data Generation**\n\nFirst, for each test case, we generate the temporal covariate sequence $c_t$ and the response sequence $y_t$ over the time horizon $t \\in \\{0, 1, \\dots, T-1\\}$.\n\nThe temporal covariate $c_t$ is defined as a sinusoidal function:\n$$\nc_t = A \\cdot \\sin\\left(\\frac{2\\pi}{P} \\, t + \\phi_t\\right)\n$$\nwhere the phase $\\phi_t$ is piecewise constant, changing from $0$ to a specified value $\\phi_{\\text{shift}}$ at a given time $t=T_{\\text{shift}}$:\n$$\n\\phi_t = \n\\begin{cases}\n0  \\text{if } t  T_{\\text{shift}},\\\\\n\\phi_{\\text{shift}}  \\text{if } t \\ge T_{\\text{shift}}.\n\\end{cases}\n$$\n\nThe response variable $y_t$ is a linear function of a constant static feature $s$ and a rolling mean of the past values of $c_t$. The window size $w^*(t)$ used for this rolling mean is also piecewise constant, changing from $w_1$ to $w_2$ at $T_{\\text{shift}}$:\n$$\nw^*(t) = \n\\begin{cases}\nw_1  \\text{if } t  T_{\\text{shift}},\\\\\nw_2  \\text{if } t \\ge T_{\\text{shift}}.\n\\end{cases}\n$$\nThe response $y_t$ is then given by:\n$$\ny_t = \\beta_0 + \\beta_1 \\cdot \\left(\\frac{1}{w^*(t)} \\sum_{j=1}^{w^*(t)} c_{t-j}\\right) + \\beta_2 \\cdot s\n$$\nThis value is defined only for time points $t$ where the lookback window does not extend before $t=0$. To ensure all candidate models and the true data generating process are well-defined, we operate on a set of \"eligible\" time indices $\\mathcal{I} = \\{t_{\\min}, t_{\\min}+1, \\dots, T-1\\}$, where $t_{\\min}$ is the maximum of all window sizes involved (both true and candidate): $t_{\\min} = \\max\\left(\\{w_1, w_2\\} \\cup \\mathcal{W}\\right)$.\n\n**2. Data Splitting**\n\nThe set of eligible indices $\\mathcal{I}$ is partitioned into training, validation, and test sets according to one of two specified strategies:\n\n- **Contiguous Time Blocks:** The ordered indices of $\\mathcal{I}$ are split into three contiguous blocks for training, validation, and test. The training set takes the first $\\lfloor 0.6n \\rfloor$ indices and the validation set takes the next $\\lfloor 0.2n \\rfloor$ indices, where $n = |\\mathcal{I}|$. A temporal gap of $g$ indices is enforced between the training and validation sets, and again between the validation and test sets to simulate a delay in receiving data. The test set comprises all remaining indices.\n- **Interleaved Periodic Split:** Indices from $\\mathcal{I}$ are assigned to sets based on their value modulo a period $m=5$. Indices with $t \\bmod 5 \\in \\{0, 1, 2\\}$ form the training set, those with $t \\bmod 5 = 3$ form the validation set, and those with $t \\bmod 5 = 4$ form the test set. This strategy ensures that all three sets are drawn from across the entire time period, which can be advantageous if the data properties change over time.\n\n**3. Feature Engineering and Model Selection**\n\nThe predictive model is a linear regression model. For each candidate window size $w$ from the set $\\mathcal{W}$, a feature vector $x^{(w)}_t$ is constructed at each time $t$:\n$$\nx^{(w)}_t = \\left[\\,1,\\;\\frac{1}{w}\\sum_{j=1}^{w} c_{t-j},\\; s\\,\\right]\n$$\nThe three components of this vector correspond to the intercept, a feature derived from the temporal covariate, and the static feature.\n\nThe core of the task is to select the optimal window size $\\hat{w}$. This is achieved through the following steps:\n1.  For each candidate window $w \\in \\mathcal{W}$, an ordinary least squares (OLS) linear model is fitted using the training data. The model coefficients $\\hat{\\theta}^{(w)}$ are found by solving the normal equations, for which we use the Moore-Penrose pseudoinverse to ensure numerical stability: $\\hat{\\theta}^{(w)} = (X_{\\text{train}}^{(w)\\top} X_{\\text{train}}^{(w)})^{-1} X_{\\text{train}}^{(w)\\top} y_{\\text{train}}$. In implementation, this is calculated as $\\hat{\\theta}^{(w)} = \\text{pinv}(X_{\\text{train}}^{(w)}) \\cdot y_{\\text{train}}$.\n2.  The performance of each trained model $(\\hat{\\theta}^{(w)}, w)$ is evaluated on the validation set by computing the mean squared error (MSE):\n    $$\n    \\text{MSE}_{\\text{val}}(w) = \\frac{1}{|\\mathcal{V}|} \\sum_{t \\in \\mathcal{V}} \\left(y_t - x^{(w)}_t \\cdot \\hat{\\theta}^{(w)}\\right)^2\n    $$\n3.  The window size $\\hat{w}$ that yields the minimum validation MSE is selected as the best hyperparameter. A tie-breaking rule specifies that if multiple windows result in the same minimal MSE, the smallest window size among them is chosen.\n\n**4. Final Evaluation**\n\nAfter identifying the optimal window size $\\hat{w}$, the model is retrained. This time, the training data is augmented with the validation data, forming a combined training-validation set. This step leverages more data to get a potentially more robust estimate of the model parameters. The new coefficients $\\hat{\\theta}^{(\\hat{w})}_{\\text{refit}}$ are obtained by fitting the model with feature set $x^{(\\hat{w})}_t$ on this combined dataset.\n\nFinally, the performance of this refitted model is assessed on the hitherto unused test set. The test MSE is calculated, representing the model's generalization error on unseen data:\n$$\n\\text{MSE}_{\\text{test}} = \\frac{1}{|\\mathcal{T}|} \\sum_{t \\in \\mathcal{T}} \\left(y_t - x^{(\\hat{w})}_t \\cdot \\hat{\\theta}^{(\\hat{w})}_{\\text{refit}}\\right)^2\n$$\nThis final value is the reported result for each test case. The entire procedure is deterministic, yielding a unique result for each set of input parameters.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        # (T, P, A, beta0, beta1, beta2, s, w1, w2, T_shift, phi_shift, split_type, extra, W_set)\n        (120, 24, 2, 0.5, 1.2, -0.7, 1.3, 8, 8, 120, 0.0, 'contiguous', 3, {4, 8, 12}),\n        (120, 24, 2, 0.5, 1.2, -0.7, 1.3, 4, 12, 80, np.pi/2, 'contiguous', 3, {4, 8, 12}),\n        (120, 24, 2, 0.5, 1.2, -0.7, 1.3, 6, 10, 60, np.pi/3, 'interleaved', 5, {4, 6, 10, 12}),\n        (40, 10, 1.5, 0.5, 1.2, -0.7, 1.3, 1, 1, 40, 0.0, 'contiguous', 0, {1, 2}),\n    ]\n\n    results = []\n    for params in test_cases:\n        mse = solve_case(*params)\n        results.append(f\"{mse:.6f}\")\n    \n    print(f\"[{','.join(results)}]\")\n\ndef solve_case(T, P, A, beta0, beta1, beta2, s, w1, w2, T_shift, phi_shift, split_type, extra, W_set):\n    \"\"\"\n    Solves a single instance of the statistical learning problem.\n    \"\"\"\n    # 1. Determine eligible time indices\n    t_min = max(list(W_set) + [w1, w2])\n\n    # 2. Generate data: c_t and y_t\n    t_range = np.arange(T)\n    phi_t = np.where(t_range  T_shift, 0, phi_shift)\n    c_t = A * np.sin(2 * np.pi / P * t_range + phi_t)\n\n    w_star_t = np.where(t_range  T_shift, w1, w2)\n    y_full = np.full(T, np.nan)\n    for t in range(t_min, T):\n        w_star = w_star_t[t]\n        # Rolling mean for the true response. Slicing c_t[t-w_star:t] corresponds to c_{t-j} for j=1..w_star\n        mean_c = np.mean(c_t[t - w_star : t])\n        y_full[t] = beta0 + beta1 * mean_c + beta2 * s\n    \n    # 3. Create data splits\n    eligible_indices = np.arange(t_min, T)\n    if split_type == 'contiguous':\n        n = len(eligible_indices)\n        g = extra\n        n_train = int(0.6 * n)\n        n_val = int(0.2 * n)\n        \n        train_indices = eligible_indices[:n_train]\n        val_indices = eligible_indices[n_train + g : n_train + g + n_val]\n        test_indices = eligible_indices[n_train + g + n_val + g :]\n    elif split_type == 'interleaved':\n        m = extra\n        train_indices = eligible_indices[np.isin(eligible_indices % m, [0, 1, 2])]\n        val_indices = eligible_indices[eligible_indices % m == 3]\n        test_indices = eligible_indices[eligible_indices % m == 4]\n    else:\n        raise ValueError(\"Unknown split type\")\n\n    # 4. Pre-compute rolling mean features for all candidate windows\n    rolling_means = {}\n    for w in W_set:\n        means_w = np.full(T, np.nan)\n        for t in range(w, T):\n            means_w[t] = np.mean(c_t[t-w:t])\n        rolling_means[w] = means_w\n\n    # 5. Model selection loop (hyperparameter tuning)\n    val_mses = {}\n    for w in sorted(list(W_set)): # Sort for deterministic tie-breaking\n        # Construct training data matrix and response vector\n        X_train = np.c_[np.ones(len(train_indices)), rolling_means[w][train_indices], np.full(len(train_indices), s)]\n        y_train = y_full[train_indices]\n        \n        # Fit model using pseudoinverse for numerical stability\n        theta_hat = np.linalg.pinv(X_train) @ y_train\n        \n        # Evaluate on validation set\n        if len(val_indices) > 0:\n            X_val = np.c_[np.ones(len(val_indices)), rolling_means[w][val_indices], np.full(len(val_indices), s)]\n            y_val = y_full[val_indices]\n            \n            y_pred_val = X_val @ theta_hat\n            val_mses[w] = np.mean((y_val - y_pred_val)**2)\n        else: # Handle cases with empty validation set\n            val_mses[w] = np.inf\n\n    # Select best window w_hat based on validation MSE, with tie-breaking\n    w_hat = min(val_mses.keys(), key=lambda w: (val_mses[w], w))\n\n    # 6. Refit on train+validation and evaluate on test set\n    refit_indices = np.sort(np.concatenate((train_indices, val_indices)))\n    \n    X_refit = np.c_[np.ones(len(refit_indices)), rolling_means[w_hat][refit_indices], np.full(len(refit_indices), s)]\n    y_refit = y_full[refit_indices]\n    \n    theta_refit = np.linalg.pinv(X_refit) @ y_refit\n    \n    if len(test_indices) > 0:\n        X_test = np.c_[np.ones(len(test_indices)), rolling_means[w_hat][test_indices], np.full(len(test_indices), s)]\n        y_test = y_full[test_indices]\n        \n        y_pred_test = X_test @ theta_refit\n        test_mse = np.mean((y_test - y_pred_test)**2)\n    else: # Handle cases with empty test set\n        test_mse = 0.0\n    \n    return test_mse\n\nsolve()\n```", "id": "3188604"}]}