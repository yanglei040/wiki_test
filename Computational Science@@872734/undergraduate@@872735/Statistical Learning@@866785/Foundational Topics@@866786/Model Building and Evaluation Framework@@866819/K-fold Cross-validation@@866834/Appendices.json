{"hands_on_practices": [{"introduction": "K-fold cross-validation is essential not only for estimating a model's final performance but also as a robust framework for tuning hyperparameters. In this exercise, you will tackle a common and critical tuning task: selecting the optimal decision threshold for a binary classifier to maximize the $F_\\beta$ score. By comparing a threshold derived from the entire dataset against one found by aggregating results from each fold, you will gain hands-on experience in using CV to find hyperparameters that generalize well [@problem_id:3139044].", "problem": "You are given binary classification outputs from a deep learning model in the form of predicted probabilities and true labels. Consider a decision rule parameterized by a threshold $t \\in [0,1]$ defined as $\\hat{y}_i(t) = 1$ if $p_i \\ge t$ and $\\hat{y}_i(t) = 0$ otherwise, where $\\mathbf{p} = (p_1,\\dots,p_N)$ are predicted probabilities and $\\mathbf{y} = (y_1,\\dots,y_N)$ are true labels with $y_i \\in \\{0,1\\}$. Define the confusion counts for threshold $t$ as $\\mathrm{TP}(t)$, $\\mathrm{FP}(t)$, and $\\mathrm{FN}(t)$ for true positives, false positives, and false negatives, respectively.\n\nThe weighted $F$-score, $F_\\beta$, is defined for $\\beta > 0$ using the well-tested formulas for precision and recall. A count-based form that is well-defined under edge cases is\n$$\nF_\\beta(t) = \\frac{(1+\\beta^2)\\,\\mathrm{TP}(t)}{(1+\\beta^2)\\,\\mathrm{TP}(t) + \\beta^2\\,\\mathrm{FN}(t) + \\mathrm{FP}(t)}.\n$$\nYou will implement threshold selection for $F_\\beta$ using $k$-fold cross-validation (CV), compare an aggregated threshold against a global threshold, and examine effects under class imbalance. Use the following foundational base: the definitions of precision, recall, and $F_\\beta$, the binary decision rule $\\hat{y}_i(t)$, and the standard $k$-fold CV procedure that partitions indices into $k$ contiguous folds of as equal size as possible, differing by at most one.\n\nAlgorithmic requirements:\n- For any dataset $(\\mathbf{p},\\mathbf{y})$, define the global threshold $t^*$ as the value of $t$ that maximizes $F_\\beta(t)$ on the full dataset. Justify and implement a discrete search over the candidate set of thresholds consisting of all unique predicted probabilities in $\\mathbf{p}$, augmented with $0$ and $1+\\epsilon$ for $\\epsilon = 10^{-6}$. This is valid because the decision rule changes only when $t$ crosses a predicted probability, making $F_\\beta(t)$ piecewise constant between these points.\n- Resolve ties by choosing the smallest threshold among those that attain the maximum $F_\\beta$.\n- For $k$-fold cross-validation, partition the index set $\\{1,\\dots,N\\}$ into $k$ contiguous folds whose sizes differ by at most one (the first $r = N \\bmod k$ folds have size $\\lfloor N/k \\rfloor + 1$, and the remaining have size $\\lfloor N/k \\rfloor$). For each fold $j \\in \\{1,\\dots,k\\}$, compute the fold-optimal threshold $t_j$ that maximizes $F_\\beta(t)$ using only that fold's validation data. Define the aggregated threshold as $\\bar{t} = \\frac{1}{k}\\sum_{j=1}^k t_j$.\n- Compare the global threshold $t^*$ and aggregated threshold $\\bar{t}$ by evaluating $F_\\beta(t^*)$ and $F_\\beta(\\bar{t})$ on the full dataset.\n\nTest suite:\n- Case A (balanced, $k=5$, $\\beta=1$):\n  - $\\mathbf{y} = [$ $1$, $0$, $1$, $0$, $1$, $0$, $1$, $0$, $1$, $0$, $1$, $0$, $1$, $0$, $1$, $0$, $1$, $0$, $1$, $0$ $]$.\n  - $\\mathbf{p} = [$ $0.92$, $0.18$, $0.88$, $0.32$, $0.83$, $0.41$, $0.77$, $0.47$, $0.71$, $0.52$, $0.66$, $0.56$, $0.61$, $0.60$, $0.59$, $0.62$, $0.58$, $0.63$, $0.54$, $0.68$ $]$.\n- Case B (imbalanced, $k=3$, $\\beta=2$):\n  - $\\mathbf{y}$ has $N = 30$ with positives at indices $3$, $7$, $12$, $18$, $24$, $29$ (using $0$-based indexing for description): $\\mathbf{y} = [$ $0$, $0$, $0$, $1$, $0$, $0$, $0$, $1$, $0$, $0$, $0$, $0$, $1$, $0$, $0$, $0$, $0$, $0$, $1$, $0$, $0$, $0$, $0$, $0$, $1$, $0$, $0$, $0$, $0$, $1$ $]$.\n  - $\\mathbf{p} = [$ $0.05$, $0.15$, $0.20$, $0.90$, $0.25$, $0.30$, $0.35$, $0.87$, $0.40$, $0.45$, $0.50$, $0.55$, $0.89$, $0.58$, $0.62$, $0.66$, $0.70$, $0.74$, $0.86$, $0.78$, $0.60$, $0.52$, $0.48$, $0.44$, $0.92$, $0.42$, $0.38$, $0.34$, $0.80$, $0.88$ $]$.\n- Case C (extreme imbalance, $k=5$, $\\beta=0.5$):\n  - $\\mathbf{y}$ has $N = 15$ with a single positive at index $11$: $\\mathbf{y} = [$ $0$, $0$, $0$, $0$, $0$, $0$, $0$, $0$, $0$, $0$, $0$, $1$, $0$, $0$, $0$ $]$.\n  - $\\mathbf{p} = [$ $0.10$, $0.20$, $0.30$, $0.40$, $0.45$, $0.50$, $0.55$, $0.60$, $0.65$, $0.85$, $0.88$, $0.95$, $0.70$, $0.72$, $0.75$ $]$.\n\nYour program must:\n- Implement the threshold search and $k$-fold CV aggregation exactly as specified, including the tie-breaking rule and the candidate set augmentation with $\\epsilon = 10^{-6}$.\n- For each test case, compute the global threshold $t^*$, aggregated threshold $\\bar{t}$, and the corresponding scores $F_\\beta(t^*)$ and $F_\\beta(\\bar{t})$ evaluated on the full dataset.\n\nFinal output format:\n- Produce a single line of output containing the results for the three test cases as a flat comma-separated list enclosed in square brackets. The list must be\n  - [$t^*_A$, $\\bar{t}_A$, $F_\\beta(t^*_A)$, $F_\\beta(\\bar{t}_A)$, $t^*_B$, $\\bar{t}_B$, $F_\\beta(t^*_B)$, $F_\\beta(\\bar{t}_B)$, $t^*_C$, $\\bar{t}_C$, $F_\\beta(t^*_C)$, $F_\\beta(\\bar{t}_C)$],\n  with each numeric value formatted to six decimal places.", "solution": "The user has provided a well-defined computational problem in the domain of machine learning model evaluation. The problem is scientifically grounded, formalizable, and self-contained. All necessary data, definitions, and algorithmic procedures are specified without ambiguity. Therefore, the problem is valid and a solution will be provided.\n\nThe core of the problem is to compare two strategies for selecting a classification threshold, $t$, for a binary classifier. The performance of the classifier under a given threshold is measured by the weighted $F_\\beta$-score.\n\nFirst, let us formalize the components. We are given a set of $N$ true labels $\\mathbf{y} = (y_1, \\dots, y_N)$ where $y_i \\in \\{0,1\\}$, and a corresponding set of predicted probabilities $\\mathbf{p} = (p_1, \\dots, p_N)$ where $p_i \\in [0,1]$. A decision threshold $t$ is used to convert these probabilities into binary predictions $\\hat{\\mathbf{y}}(t) = (\\hat{y}_1(t), \\dots, \\hat{y}_N(t))$ according to the rule:\n$$\n\\hat{y}_i(t) = \\begin{cases} 1 & \\text{if } p_i \\ge t \\\\ 0 & \\text{if } p_i < t \\end{cases}\n$$\nBased on these predictions, we can compute the number of true positives ($\\mathrm{TP}$), false positives ($\\mathrm{FP}$), and false negatives ($\\mathrm{FN}$) as functions of $t$:\n$$\n\\mathrm{TP}(t) = \\sum_{i=1}^N \\mathbf{1}(p_i \\ge t \\land y_i = 1)\n$$\n$$\n\\mathrm{FP}(t) = \\sum_{i=1}^N \\mathbf{1}(p_i \\ge t \\land y_i = 0)\n$$\n$$\n\\mathrm{FN}(t) = \\sum_{i=1}^N \\mathbf{1}(p_i < t \\land y_i = 1)\n$$\nwhere $\\mathbf{1}(\\cdot)$ is the indicator function. The problem provides a robust, count-based definition of the $F_\\beta$-score for a given $\\beta > 0$:\n$$\nF_\\beta(t) = \\frac{(1+\\beta^2)\\,\\mathrm{TP}(t)}{(1+\\beta^2)\\,\\mathrm{TP}(t) + \\beta^2\\,\\mathrm{FN}(t) + \\mathrm{FP}(t)}\n$$\nThis form is well-defined even when the total number of predicted positives or actual positives is zero, in which case the numerator becomes $0$. If the denominator is also $0$ (i.e., $\\mathrm{TP}=\\mathrm{FP}=\\mathrm{FN}=0$), the score is taken to be $0$.\n\nThe primary task is to implement an algorithm to find the optimal threshold $t$ that maximizes $F_\\beta(t)$. The problem mandates a specific search strategy. The search for an optimal $t$ does not require scanning the entire interval $[0,1]$. The values of $\\mathrm{TP}(t)$, $\\mathrm{FP}(t)$, and $\\mathrm{FN}(t)$ only change when the threshold $t$ crosses one of the probability values $p_i$. Therefore, the function $F_\\beta(t)$ is piecewise constant. A complete search can be performed over a discrete set of candidate thresholds. The specified candidate set is composed of all unique probability scores in $\\mathbf{p}$, augmented with two special values: $0$ and $1+\\epsilon$ where $\\epsilon=10^{-6}$. The value $t=0$ accounts for the case where all samples are classified as positive, and $t=1+\\epsilon$ ensures all samples are classified as negative (as some $p_i$ could be exactly $1$). By searching over this sorted set of candidates, we can find the maximum $F_\\beta$ value. The problem specifies a tie-breaking rule: if multiple thresholds yield the same maximum $F_\\beta$ score, the smallest of these thresholds must be chosen.\n\nWith this optimization procedure defined, we can find two distinct types of thresholds:\n\n1.  **Global Threshold ($t^*$):** This threshold is found by applying the optimization procedure described above to the entire dataset $(\\mathbf{y}, \\mathbf{p})$. It represents the single best threshold for the observed data in its entirety.\n\n2.  **Aggregated Threshold ($\\bar{t}$):** This threshold is derived using $k$-fold cross-validation. The dataset's indices $\\{0, 1, \\dots, N-1\\}$ are partitioned into $k$ contiguous, non-overlapping folds. The partitioning is deterministic: the first $r = N \\bmod k$ folds contain $\\lfloor N/k \\rfloor + 1$ samples, and the remaining $k-r$ folds contain $\\lfloor N/k \\rfloor$ samples. For each fold $j \\in \\{1, \\dots, k\\}$, the data within that fold serves as a validation set. The optimal threshold for that fold, $t_j$, is found by maximizing $F_\\beta(t)$ using only the data points $(\\mathbf{y}_j, \\mathbf{p}_j)$ corresponding to fold $j$. The aggregated threshold is then computed as the arithmetic mean of these fold-specific thresholds:\n    $$\n    \\bar{t} = \\frac{1}{k} \\sum_{j=1}^k t_j\n    $$\nThis cross-validation approach simulates the process of finding a threshold on unseen data, and averaging the results provides a more robust estimate that is less prone to overfitting to a specific data split.\n\nFinally, the problem requires a comparison of the performance of these two thresholds, $t^*$ and $\\bar{t}$. The comparison is performed by evaluating the $F_\\beta$-score for each threshold on the **entire dataset**. This allows for a direct assessment of which threshold selection strategy yields a better result on the full distribution of observed data.\n\nThe implementation will consist of three main components: a function to calculate $F_\\beta(t)$, a function to find the optimal threshold for a given dataset, and a main routine to orchestrate the computation for each test case, including the k-fold splitting logic and final evaluation.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to execute the problem's requirements for all test cases.\n    \"\"\"\n    \n    # Define test cases from the problem statement\n    case_a = {\n        \"y\": [1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0],\n        \"p\": [0.92, 0.18, 0.88, 0.32, 0.83, 0.41, 0.77, 0.47, 0.71, 0.52, 0.66, 0.56, 0.61, 0.60, 0.59, 0.62, 0.58, 0.63, 0.54, 0.68],\n        \"k\": 5,\n        \"beta\": 1.0,\n    }\n    \n    y_b = np.zeros(30, dtype=int)\n    pos_indices_b = [3, 7, 12, 18, 24, 29]\n    y_b[pos_indices_b] = 1\n    case_b = {\n        \"y\": y_b.tolist(),\n        \"p\": [0.05, 0.15, 0.20, 0.90, 0.25, 0.30, 0.35, 0.87, 0.40, 0.45, 0.50, 0.55, 0.89, 0.58, 0.62, 0.66, 0.70, 0.74, 0.86, 0.78, 0.60, 0.52, 0.48, 0.44, 0.92, 0.42, 0.38, 0.34, 0.80, 0.88],\n        \"k\": 3,\n        \"beta\": 2.0,\n    }\n    \n    y_c = np.zeros(15, dtype=int)\n    y_c[11] = 1\n    case_c = {\n        \"y\": y_c.tolist(),\n        \"p\": [0.10, 0.20, 0.30, 0.40, 0.45, 0.50, 0.55, 0.60, 0.65, 0.85, 0.88, 0.95, 0.70, 0.72, 0.75],\n        \"k\": 5,\n        \"beta\": 0.5,\n    }\n\n    test_cases = [case_a, case_b, case_c]\n    epsilon = 1e-6\n    results = []\n\n    def calculate_f_beta(y, p, t, beta):\n        \"\"\"Calculates the F-beta score for a given threshold.\"\"\"\n        if y.size == 0:\n            return 0.0\n        \n        y_pred = (p >= t).astype(int)\n        \n        tp = np.sum((y_pred == 1) & (y == 1))\n        fp = np.sum((y_pred == 1) & (y == 0))\n        fn = np.sum((y_pred == 0) & (y == 1))\n        \n        beta_sq = beta**2\n        numerator = (1 + beta_sq) * tp\n        denominator = (1 + beta_sq) * tp + beta_sq * fn + fp\n        \n        if denominator == 0:\n            return 0.0\n        else:\n            return numerator / denominator\n\n    def find_optimal_threshold(y, p, beta, epsilon):\n        \"\"\"Finds the optimal threshold that maximizes F-beta score.\"\"\"\n        if y.size == 0:\n            return 0.0\n        \n        unique_p = np.unique(p)\n        candidate_thresholds = np.concatenate(([0.0], unique_p, [1.0 + epsilon]))\n        # Sorting is implicit from np.unique and concatenation order, but let's be explicit\n        candidate_thresholds.sort()\n        \n        best_t = candidate_thresholds[0]\n        max_f_beta = -1.0\n        \n        for t in candidate_thresholds:\n            f_beta_score = calculate_f_beta(y, p, t, beta)\n            if f_beta_score > max_f_beta:\n                max_f_beta = f_beta_score\n                best_t = t\n                \n        return best_t\n\n    def process_case(y, p, k, beta, epsilon):\n        \"\"\"Processes a single test case to find thresholds and scores.\"\"\"\n        y = np.array(y)\n        p = np.array(p)\n        N = len(y)\n        \n        # 1. Calculate global threshold t*\n        t_star = find_optimal_threshold(y, p, beta, epsilon)\n        \n        # 2. Calculate k-fold aggregated threshold t_bar\n        fold_thresholds = []\n        \n        n_per_fold = N // k\n        n_larger_folds = N % k\n        \n        current_idx = 0\n        for i in range(k):\n            fold_size = n_per_fold + 1 if i < n_larger_folds else n_per_fold\n            fold_indices = np.arange(current_idx, current_idx + fold_size)\n            \n            y_fold = y[fold_indices]\n            p_fold = p[fold_indices]\n            \n            t_j = find_optimal_threshold(y_fold, p_fold, beta, epsilon)\n            fold_thresholds.append(t_j)\n            \n            current_idx += fold_size\n            \n        t_bar = np.mean(fold_thresholds)\n        \n        # 3. Evaluate F-beta scores on the full dataset\n        f_beta_star = calculate_f_beta(y, p, t_star, beta)\n        f_beta_bar = calculate_f_beta(y, p, t_bar, beta)\n        \n        return t_star, t_bar, f_beta_star, f_beta_bar\n\n    for case in test_cases:\n        t_star, t_bar, f_beta_star, f_beta_bar = process_case(\n            case[\"y\"], case[\"p\"], case[\"k\"], case[\"beta\"], epsilon\n        )\n        results.extend([t_star, t_bar, f_beta_star, f_beta_bar])\n\n    # Format and print the final results\n    formatted_results = [f\"{x:.6f}\" for x in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3139044"}, {"introduction": "Data augmentation is a powerful technique for improving model generalization, but its interaction with cross-validation requires great care. A common mistake is to perform augmentation on the entire dataset before splitting it into folds, which causes \"data leakage\" and leads to overly optimistic performance estimates. This practice will allow you to quantify the optimistic bias from such leakage by comparing a methodologically sound CV procedure with a flawed one, reinforcing the fundamental principle of keeping validation data completely separate from the training process [@problem_id:3134696].", "problem": "You are tasked with designing and implementing a computational experiment to analyze $k$-fold Cross-Validation (CV) in Statistical Learning under data augmentation with image rotations. The aim is to quantify how applying augmentation only to training folds compares to the scenario where augmentation leaks into validation folds. Your program must be a complete, runnable implementation that produces the requested outputs with no external input.\n\nFundamental base and definitions to be used:\n- Cross-Validation (CV): For a given dataset $D = \\{(x_i, y_i)\\}_{i=1}^n$ and a learning algorithm, the $k$-fold CV estimator of the generalization error is defined as the average validation loss over $k$ folds. Let the folds be index sets $V_1, \\dots, V_k$ that partition $\\{1,\\dots,n\\}$, and $T_j = \\{1,\\dots,n\\} \\setminus V_j$ denote the corresponding training indices. For a model $\\hat{f}_j$ trained on $T_j$, the CV estimator of accuracy is\n$$\n\\widehat{\\text{Acc}}_{\\text{CV}} = \\frac{1}{k} \\sum_{j=1}^k \\frac{1}{|V_j|} \\sum_{i \\in V_j} \\mathbf{1}\\{\\hat{f}_j(x_i) = y_i\\},\n$$\nwhere $\\mathbf{1}\\{\\cdot\\}$ is the indicator function.\n- Data augmentation: A label-preserving transformation $\\mathcal{A}_\\theta$ applied to an input $x$ parameterized by a rotation angle $\\theta$ measured in degrees. In this problem, $\\theta \\in \\{90, 180, 270\\}$, and $\\mathcal{A}_\\theta$ is image rotation by $\\theta$ degrees about the image center.\n- Independence principle: Proper CV requires that validation samples are independent of training samples conditional on the data-generating process. Leakage occurs when transformed versions of the same base sample appear in both training and validation folds, violating the independence assumption and biasing the estimator.\n\nDataset construction:\n- Consider $n$ base images $\\{x_i\\}_{i=1}^n$ of size $h \\times w$ with labels $y_i \\in \\{0,1\\}$, where class $0$ is a centered \"$+$\" pattern and class $1$ is a centered \"$\\times$\" pattern. Each base image is produced by perturbing the ideal template with small random offsets in line thickness and additive Gaussian noise with standard deviation $\\sigma$, then clipping pixel intensities to the interval $[0,1]$. Rotations by $\\theta \\in \\{90,180,270\\}$ preserve class labels.\n- The classifier is the nearest-centroid classifier under squared $\\ell_2$ distance. For a training set $(X_{\\text{train}}, Y_{\\text{train}})$, compute class centroids\n$$\n\\mu_c = \\frac{1}{|\\{i: Y_{\\text{train},i} = c\\}|} \\sum_{i:Y_{\\text{train},i}=c} X_{\\text{train},i}, \\quad c \\in \\{0,1\\},\n$$\nand predict $\\hat{y}(x) = \\arg\\min_{c \\in \\{0,1\\}} \\|x - \\mu_c\\|_2^2$.\n\nTwo CV estimators to compare:\n1. Proper augmentation CV: For each fold $j \\in \\{1,\\dots,k\\}$, augment only the training images by rotations in a given angle set $S$ (angles in degrees), i.e., use $\\{x_i\\} \\cup \\{\\mathcal{A}_\\theta(x_i): \\theta \\in S\\}$ for $i \\in T_j$ to train $\\hat{f}_j$, and evaluate on the unaugmented validation images $\\{x_i: i \\in V_j\\}$.\n2. Leakage CV: First, globally augment the entire dataset by rotations in $S$ to form $D' = \\bigcup_{i=1}^n \\left(\\{x_i\\} \\cup \\{\\mathcal{A}_\\theta(x_i): \\theta \\in S\\}\\right)$, then perform $k$-fold CV on $D'$, training and validating on these augmented samples. This procedure allows augmented variants of the same base image to appear across different folds, producing dependence between training and validation samples.\n\nYour program must:\n- Generate the synthetic dataset as specified.\n- Implement the nearest-centroid classifier.\n- Compute the average $k$-fold CV accuracy for both procedures and return the difference\n$$\n\\Delta = \\widehat{\\text{Acc}}_{\\text{CV}}^{\\text{leak}} - \\widehat{\\text{Acc}}_{\\text{CV}}^{\\text{proper}}.\n$$\n\nAngle units: All angles are measured in degrees.\n\nAnswer types: All outputs must be real numbers (floats).\n\nTest suite and coverage:\nImplement the following test cases, each defined as a tuple $(n, h, w, k, S, \\sigma, \\text{seed})$ and processed in order. For every test case, compute and output $\\Delta$.\n\n- Case $1$ (happy path): $(n, h, w, k, S, \\sigma, \\text{seed}) = (60, 16, 16, 5, \\{90, 180, 270\\}, 0.10, 42)$.\n- Case $2$ (boundary, no augmentation): $(60, 16, 16, 5, \\varnothing, 0.10, 43)$, where $\\varnothing$ denotes the empty set of angles.\n- Case $3$ (few folds): $(60, 16, 16, 2, \\{90, 180, 270\\}, 0.10, 44)$.\n- Case $4$ (high noise): $(60, 16, 16, 5, \\{90, 180, 270\\}, 0.50, 45)$.\n- Case $5$ (single-angle augmentation): $(60, 16, 16, 10, \\{90\\}, 0.10, 46)$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the same order as the test cases. For example, an output with three results should look like $[r_1,r_2,r_3]$, where each $r_i$ is a float. Angles are in degrees. No additional text should be printed.", "solution": "We start from the core definition of $k$-fold Cross-Validation (CV) and the independence requirement. The $k$-fold CV estimator averages validation performance across disjoint folds, and its reliability as an estimator of generalization performance depends on the conditional independence between training and validation sets given the data-generating process. When augmentation is applied only to training folds, transformations $\\mathcal{A}_\\theta$ enrich the training distribution while keeping validation samples independent draws from the original data distribution, preserving the CV estimator’s integrity. When augmentation leaks into validation folds—specifically by augmenting the entire dataset before constructing folds—augmented versions of the same base sample can be assigned both to training and validation sets, violating independence and inducing optimism bias.\n\nWe formalize the two estimators:\n- Proper augmentation CV estimator:\n$$\n\\widehat{\\text{Acc}}_{\\text{proper}} = \\frac{1}{k} \\sum_{j=1}^k \\frac{1}{|V_j|} \\sum_{i \\in V_j} \\mathbf{1}\\left\\{ \\arg\\min_{c \\in \\{0,1\\}} \\|x_i - \\mu_{c}^{(j)}\\|_2^2 = y_i \\right\\},\n$$\nwith\n$$\n\\mu_{c}^{(j)} = \\frac{1}{|\\{t \\in T_j: y_t = c\\}| + |\\{(t,\\theta): t \\in T_j, y_t = c, \\theta \\in S\\}|} \\left( \\sum_{t \\in T_j, y_t=c} x_t + \\sum_{t \\in T_j, y_t=c} \\sum_{\\theta \\in S} \\mathcal{A}_\\theta(x_t) \\right).\n$$\nValidation uses the unaugmented $x_i$ for $i \\in V_j$.\n\n- Leakage CV estimator:\nConstruct $D' = \\bigcup_{i=1}^n \\left(\\{x_i\\} \\cup \\{\\mathcal{A}_\\theta(x_i): \\theta \\in S\\}\\right)$, then perform $k$-fold CV directly on $D'$. Let $V'_1, \\dots, V'_k$ be the folds and $T'_j$ be their complements. The leakage estimator is\n$$\n\\widehat{\\text{Acc}}_{\\text{leak}} = \\frac{1}{k} \\sum_{j=1}^k \\frac{1}{|V'_j|} \\sum_{(i,\\phi) \\in V'_j} \\mathbf{1}\\left\\{ \\arg\\min_{c \\in \\{0,1\\}} \\| \\mathcal{A}_\\phi (x_i) - \\tilde{\\mu}_{c}^{(j)} \\|_2^2 = y_i \\right\\},\n$$\nwith\n$$\n\\tilde{\\mu}_{c}^{(j)} = \\frac{1}{|\\{(t,\\psi) \\in T'_j: y_t = c\\}|} \\sum_{(t,\\psi) \\in T'_j, y_t=c} \\mathcal{A}_\\psi(x_t).\n$$\n\nBias mechanism:\nIn the proper procedure, $x_i$ in validation is not used (directly or via its transformed copies) in training. In the leakage procedure, for many $i$, some transformed $\\mathcal{A}_\\psi(x_i)$ can be present in training while $\\mathcal{A}_\\phi(x_i)$ is used for validation, creating high correlation between training and validation samples. For a nearest-centroid classifier, this correlation reduces $\\| \\mathcal{A}_\\phi (x_i) - \\tilde{\\mu}_{y_i}^{(j)} \\|_2^2$ and increases the probability of correct classification, hence $\\widehat{\\text{Acc}}_{\\text{leak}} \\ge \\widehat{\\text{Acc}}_{\\text{proper}}$ on average. The quantity of interest is the difference\n$$\n\\Delta = \\widehat{\\text{Acc}}_{\\text{leak}} - \\widehat{\\text{Acc}}_{\\text{proper}}.\n$$\n\nAlgorithmic design:\n1. Synthetic data generation:\n   - Create $n$ base images of size $h \\times w$. For class $0$, draw a centered \"$+$\" by setting pixels in a vertical and horizontal bar crossing the center, with random thickness (e.g., $1$ or $2$ pixels) and small random center offsets. For class $1$, draw a centered \"$\\times$\" by setting pixels along both diagonals with similar random thickness and offsets.\n   - Add independent Gaussian noise with standard deviation $\\sigma$ to each pixel and clip values to $[0,1]$.\n   - Assign labels such that $n/2$ images belong to each class (balanced), then shuffle.\n\n2. Augmentation operator:\n   - For angles $\\theta \\in S \\subseteq \\{90,180,270\\}$ measured in degrees, implement $\\mathcal{A}_\\theta$ via $90$-degree increments (i.e., $\\text{np.rot90}$ with $k = \\theta/90$), preserving labels.\n\n3. Classifier:\n   - Nearest-centroid classifier under squared $\\ell_2$ distance. Flatten images to vectors in $\\mathbb{R}^{h \\cdot w}$ for computations. Compute class centroids from the training set (including augmented samples as required) and predict the class of validation samples by nearest centroid.\n\n4. $k$-fold splitting:\n   - For proper augmentation CV, split the $n$ base samples into $k$ folds. For fold $j$, augment only the training set by angles in $S$, keep validation unaugmented, train centroids, and compute accuracy on the validation fold. Average across folds to obtain $\\widehat{\\text{Acc}}_{\\text{proper}}$.\n   - For leakage CV, first augment the entire dataset (include originals and rotations in $S$), then perform $k$-fold CV on this augmented dataset, training and validating on augmented samples to obtain $\\widehat{\\text{Acc}}_{\\text{leak}}$.\n\n5. Difference:\n   - Compute $\\Delta$ for each test case.\n\nTest suite coverage rationale:\n- Case $1$ uses moderate $k$, multiple angles, and small noise, representing a typical scenario.\n- Case $2$ uses $S = \\varnothing$ (no augmentation), where we expect $\\Delta \\approx 0$ because both procedures coincide.\n- Case $3$ uses small $k$, increasing the chance that augmented duplicates cross folds, potentially amplifying leakage effects.\n- Case $4$ increases noise $\\sigma$, examining robustness; leakage can artificially boost performance under noisy conditions.\n- Case $5$ uses $k = 10$ and a single angle, testing sensitivity to the augmentation set size.\n\nFinal output:\nA single line $[\\Delta_1,\\Delta_2,\\Delta_3,\\Delta_4,\\Delta_5]$ with floats corresponding to the cases above, in order.\n\nThis design strictly adheres to the independence principle underlying CV and exposes, through computation, how violating it via augmentation leakage produces optimistic bias in the estimated accuracy.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef make_plus_image(h, w, thickness, off_y, off_x):\n    \"\"\"\n    Create a centered plus '+' pattern with given thickness and small offsets.\n    Intensities are 1.0 on the pattern, 0.0 elsewhere.\n    \"\"\"\n    img = np.zeros((h, w), dtype=np.float64)\n    cy = h // 2 + off_y\n    cx = w // 2 + off_x\n    # Horizontal bar\n    y_start = max(cy - thickness, 0)\n    y_end = min(cy + thickness + 1, h)\n    img[y_start:y_end, :] = 1.0\n    # Vertical bar\n    x_start = max(cx - thickness, 0)\n    x_end = min(cx + thickness + 1, w)\n    img[:, x_start:x_end] = 1.0\n    return img\n\ndef make_x_image(h, w, thickness, off_y, off_x):\n    \"\"\"\n    Create a centered 'x' pattern (two diagonals) with given thickness and small offsets.\n    Intensities are 1.0 on the pattern, 0.0 elsewhere.\n    \"\"\"\n    img = np.zeros((h, w), dtype=np.float64)\n    cy = h // 2 + off_y\n    cx = w // 2 + off_x\n    # Draw diagonals with thickness by marking pixels close to diagonal lines\n    yy, xx = np.meshgrid(np.arange(h), np.arange(w), indexing='ij')\n    # Centered diagonals adjusted by offsets\n    # Line 1: slope +1 through (cy, cx)\n    # Distance from line y - cy = x - cx => y - x = cy - cx\n    d1 = np.abs((yy - xx) - (cy - cx))\n    # Line 2: slope -1 through (cy, cx)\n    # Distance from line y - cy = -(x - cx) => y + x = cy + cx\n    d2 = np.abs((yy + xx) - (cy + cx))\n    mask = (d1 <= thickness) | (d2 <= thickness)\n    img[mask] = 1.0\n    return img\n\ndef add_noise_and_clip(img, noise_std, rng):\n    noisy = img + rng.normal(loc=0.0, scale=noise_std, size=img.shape)\n    return np.clip(noisy, 0.0, 1.0)\n\ndef generate_dataset(n, h, w, noise_std, seed):\n    \"\"\"\n    Generate n base images: half '+' class (label 0), half 'x' class (label 1),\n    with random thickness and small offsets, plus Gaussian noise.\n    Returns images (n, h, w) and labels (n,).\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    images = np.zeros((n, h, w), dtype=np.float64)\n    labels = np.zeros(n, dtype=np.int64)\n    # Balanced labels: 0 then 1 alternating\n    base_labels = np.array([0, 1] * (n // 2) + ([0] if n % 2 == 1 else []), dtype=np.int64)\n    # Shuffle labels for randomness\n    rng.shuffle(base_labels)\n    for i in range(n):\n        c = base_labels[i]\n        labels[i] = c\n        thickness = rng.integers(1, 3)  # 1 or 2\n        off_y = rng.integers(-1, 2)     # -1, 0, 1\n        off_x = rng.integers(-1, 2)     # -1, 0, 1\n        if c == 0:\n            img = make_plus_image(h, w, thickness, off_y, off_x)\n        else:\n            img = make_x_image(h, w, thickness, off_y, off_x)\n        img = add_noise_and_clip(img, noise_std, rng)\n        images[i] = img\n    return images, labels\n\ndef rotate_image(img, angle_deg):\n    \"\"\"\n    Rotate the image by angle degrees (must be in {90,180,270}) using np.rot90.\n    \"\"\"\n    if angle_deg % 90 != 0 or angle_deg == 0:\n        # Only support non-zero multiples of 90 as per augmentation set; skip 0 in augmentation\n        raise ValueError(\"Only 90, 180, 270 degrees supported for augmentation\")\n    k = (angle_deg // 90) % 4\n    return np.rot90(img, k)\n\ndef augment_images(images, labels, angles):\n    \"\"\"\n    For each image, return list of (original + rotated versions) according to angles.\n    angles: list of integers from {90, 180, 270}. Original is always included when leak=True;\n            for training-only augmentation, we'll explicitly include original.\n    \"\"\"\n    augmented_imgs = []\n    augmented_labels = []\n    for img, y in zip(images, labels):\n        # include original\n        augmented_imgs.append(img)\n        augmented_labels.append(y)\n        for ang in angles:\n            aug = rotate_image(img, ang)\n            augmented_imgs.append(aug)\n            augmented_labels.append(y)\n    return np.array(augmented_imgs), np.array(augmented_labels)\n\ndef kfold_indices(n, k, seed):\n    \"\"\"\n    Return list of k folds, each a numpy array of indices.\n    Shuffle indices using seed, then split into k contiguous folds as evenly as possible.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    indices = np.arange(n, dtype=np.int64)\n    rng.shuffle(indices)\n    folds = []\n    fold_sizes = np.full(k, n // k, dtype=int)\n    fold_sizes[:(n % k)] += 1\n    start = 0\n    for fs in fold_sizes:\n        end = start + fs\n        folds.append(indices[start:end])\n        start = end\n    return folds\n\ndef compute_centroids(X_train, y_train):\n    \"\"\"\n    Compute class centroids for labels 0 and 1.\n    X_train: (m, d), y_train: (m,)\n    Returns mu0, mu1 as (d,) arrays.\n    \"\"\"\n    # In case a class is missing, handle by using zeros to avoid crash\n    c0 = (y_train == 0)\n    c1 = (y_train == 1)\n    if not np.any(c0) or not np.any(c1):\n        # If missing a class, return centroids computed from available class and zeros for missing\n        mu0 = X_train[c0].mean(axis=0) if np.any(c0) else np.zeros(X_train.shape[1], dtype=np.float64)\n        mu1 = X_train[c1].mean(axis=0) if np.any(c1) else np.zeros(X_train.shape[1], dtype=np.float64)\n        return mu0, mu1\n    mu0 = X_train[c0].mean(axis=0)\n    mu1 = X_train[c1].mean(axis=0)\n    return mu0, mu1\n\ndef predict_nearest_centroid(X_val, mu0, mu1):\n    \"\"\"\n    Predict labels for validation set X_val given centroids mu0 and mu1.\n    \"\"\"\n    # Compute squared distances to centroids\n    d0 = np.sum((X_val - mu0) ** 2, axis=1)\n    d1 = np.sum((X_val - mu1) ** 2, axis=1)\n    return (d1 < d0).astype(np.int64)  # predict 1 if distance to mu1 is smaller, else 0\n\ndef cross_val_accuracy_proper(images, labels, k, angles, seed):\n    \"\"\"\n    Proper augmentation: augment only training folds; validate on original base images.\n    \"\"\"\n    n = images.shape[0]\n    folds = kfold_indices(n, k, seed)\n    h, w = images.shape[1], images.shape[2]\n    accs = []\n    for fold in folds:\n        val_idx = fold\n        train_idx = np.setdiff1d(np.arange(n, dtype=np.int64), val_idx, assume_unique=True)\n        # Training data: original + augment rotations for training indices\n        X_train_list = []\n        y_train_list = []\n        for idx in train_idx:\n            img = images[idx]\n            y = labels[idx]\n            # include original\n            X_train_list.append(img.reshape(h * w))\n            y_train_list.append(y)\n            # include augmentations\n            for ang in angles:\n                aug = rotate_image(img, ang)\n                X_train_list.append(aug.reshape(h * w))\n                y_train_list.append(y)\n        X_train = np.stack(X_train_list, axis=0)\n        y_train = np.array(y_train_list, dtype=np.int64)\n        mu0, mu1 = compute_centroids(X_train, y_train)\n        # Validation data: original base images only\n        X_val = images[val_idx].reshape(len(val_idx), h * w)\n        y_val = labels[val_idx]\n        y_pred = predict_nearest_centroid(X_val, mu0, mu1)\n        acc = np.mean(y_pred == y_val)\n        accs.append(acc)\n    return float(np.mean(accs))\n\ndef cross_val_accuracy_leak(images, labels, k, angles, seed):\n    \"\"\"\n    Leakage augmentation: augment entire dataset first (include originals and rotations),\n    then perform k-fold CV on the augmented dataset.\n    \"\"\"\n    # Augment entire dataset: originals + rotations in angles\n    aug_imgs, aug_labels = augment_images(images, labels, angles)\n    n_aug = aug_imgs.shape[0]\n    folds = kfold_indices(n_aug, k, seed)\n    h, w = images.shape[1], images.shape[2]\n    accs = []\n    for fold in folds:\n        val_idx = fold\n        train_idx = np.setdiff1d(np.arange(n_aug, dtype=np.int64), val_idx, assume_unique=True)\n        X_train = aug_imgs[train_idx].reshape(len(train_idx), h * w)\n        y_train = aug_labels[train_idx]\n        mu0, mu1 = compute_centroids(X_train, y_train)\n        X_val = aug_imgs[val_idx].reshape(len(val_idx), h * w)\n        y_val = aug_labels[val_idx]\n        y_pred = predict_nearest_centroid(X_val, mu0, mu1)\n        acc = np.mean(y_pred == y_val)\n        accs.append(acc)\n    return float(np.mean(accs))\n\ndef compute_delta(n, h, w, k, angles, noise_std, seed):\n    images, labels = generate_dataset(n, h, w, noise_std, seed)\n    acc_proper = cross_val_accuracy_proper(images, labels, k, angles, seed)\n    acc_leak = cross_val_accuracy_leak(images, labels, k, angles, seed)\n    return acc_leak - acc_proper\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (n, h, w, k, angles_list, noise_std, seed)\n    test_cases = [\n        (60, 16, 16, 5, [90, 180, 270], 0.10, 42),  # Case 1: happy path\n        (60, 16, 16, 5, [],              0.10, 43),  # Case 2: boundary, no augmentation\n        (60, 16, 16, 2, [90, 180, 270],  0.10, 44),  # Case 3: few folds\n        (60, 16, 16, 5, [90, 180, 270],  0.50, 45),  # Case 4: high noise\n        (60, 16, 16, 10, [90],           0.10, 46),  # Case 5: single-angle augmentation\n    ]\n\n    results = []\n    for case in test_cases:\n        n, h, w, k, angles, noise_std, seed = case\n        delta = compute_delta(n, h, w, k, angles, noise_std, seed)\n        # Format with a reasonable precision\n        results.append(f\"{delta:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3134696"}, {"introduction": "Beyond providing a final performance score, the epoch-by-epoch validation results from $k$-fold CV offer a rich source of information for dynamically controlling the training process itself. In this advanced practice, you will move beyond simple patience-based rules and implement a sophisticated, unified early stopping criterion. You will learn to aggregate signals across all folds to create a rule that is robust to noise and avoids premature stopping due to the behavior of any single fold, a key skill for efficiently training deep learning models [@problem_id:3139126].", "problem": "You are given the task of deriving and implementing a unified Early Stopping (ES) rule under $k$-fold Cross-Validation (CV) for a deep learning model. Begin from the following foundational base and definitions, and construct a principled stopping criterion that simultaneously reduces epoch-level noise, aggregates evidence across folds, and avoids stopping based on idiosyncratic overfitting in any single fold.\n\nFundamental base and core definitions:\n- In Supervised Learning, Empirical Risk Minimization (ERM) chooses parameters to minimize empirical risk. Given training data and a model with parameters updated by epochs, the validation loss provides an estimate of generalization error at each epoch.\n- In $k$-fold Cross-Validation (CV), the data are partitioned into $k$ folds; for each fold $k$ in $\\{1,\\dots,K\\}$, the model is trained on $K-1$ folds and validated on the held-out fold, yielding a per-epoch validation loss sequence $\\{\\ell_{k,t}\\}_{t=0}^{T-1}$.\n- The validation loss is typically noisy across epochs due to mini-batch stochasticity and optimizer dynamics. Noise reduction can be achieved via an Exponential Moving Average (EMA) applied independently to each fold, defined for a smoothing coefficient $\\alpha \\in (0,1)$ by $S_{k,0} = \\ell_{k,0}$ and $S_{k,t} = \\alpha S_{k,t-1} + (1-\\alpha)\\ell_{k,t}$ for $t \\ge 1$.\n- Aggregation across folds at epoch $t$ is captured by the cross-fold mean $M_t = \\frac{1}{K}\\sum_{k=1}^{K} S_{k,t}$ and the cross-fold standard deviation $\\sigma_t = \\sqrt{\\frac{1}{K}\\sum_{k=1}^{K} (S_{k,t}-M_t)^2}$.\n- Improvement at epoch $t$ is assessed relative to the best value so far, with a small tolerance $\\varepsilon > 0$ to ignore negligible changes. Define the running minima $m_t = \\min\\{M_0,\\dots,M_t\\}$ and $b_{k,t} = \\min\\{S_{k,0},\\dots,S_{k,t}\\}$ for each fold $k$.\n\nYour unified stopping rule must satisfy all of the following design requirements simultaneously:\n1. Cross-fold smoothing: Use the EMA $S_{k,t}$ for each fold $k$ with the given $\\alpha$; aggregate with $M_t$ and $\\sigma_t$.\n2. Mean-based patience: Define a non-improvement indicator for the cross-fold mean at epoch $t$ as $I_t^{(\\text{mean})} = 1$ if $M_t > m_{t-1} - \\varepsilon$ and $I_t^{(\\text{mean})} = 0$ otherwise (for $t \\ge 1$; at $t=0$ treat $I_0^{(\\text{mean})}=0$). Let $p \\in \\mathbb{N}$ be a patience parameter; require $I_{t-p+1}^{(\\text{mean})}=\\dots=I_t^{(\\text{mean})}=1$ for a stop at $t$.\n3. Cross-fold consensus to avoid fold-wise overfitting: At epoch $t$, compute the fraction of folds that fail to improve by more than $\\varepsilon$ relative to their own best so far: $q_t = \\frac{1}{K}\\sum_{k=1}^{K} \\mathbb{1}\\left(S_{k,t} > b_{k,t-1} - \\varepsilon\\right)$ (with $\\mathbb{1}$ the indicator function). Let $q \\in (0,1]$ be a required consensus threshold; require $q_t \\ge q$ at the stop epoch $t$.\n4. Variance growth control: To avoid stopping when a single fold overfits while others still improve, impose $\\sigma_t \\le r \\cdot \\min\\{\\sigma_0,\\dots,\\sigma_t\\}$ for some $r \\ge 1$ at the stop epoch $t$.\n5. Warmup: Let $w \\in \\mathbb{N}$; the stopping rule cannot trigger for $t < w$.\n\nDefine the stopping epoch index as the earliest $t \\in \\{0,\\dots,T-1\\}$ satisfying all conditions $2$–$5$; if no such epoch exists, return $T-1$. All indices are $0$-based integers.\n\nYour program must implement the above rule and evaluate it on the following test suite. Each test case provides:\n- A $K \\times T$ matrix of per-fold validation losses $\\ell_{k,t}$,\n- The parameters $\\alpha$, $\\varepsilon$, $p$, $q$, $r$, and $w$.\n\nTest suite (all numbers are provided as decimals; all epoch indices are integers starting at $0$):\n\nCase $1$ (happy path plateau):\n- $K = 5$, $T = 15$,\n- Losses per fold $k=1,\\dots,5$:\n  - Fold $1$: $[1.00, 0.92, 0.85, 0.80, 0.78, 0.77, 0.77, 0.775, 0.78, 0.781, 0.782, 0.783, 0.785, 0.79, 0.795]$\n  - Fold $2$: $[1.05, 0.95, 0.86, 0.81, 0.79, 0.78, 0.78, 0.782, 0.784, 0.785, 0.786, 0.787, 0.79, 0.795, 0.80]$\n  - Fold $3$: $[0.98, 0.90, 0.83, 0.79, 0.77, 0.765, 0.765, 0.767, 0.768, 0.769, 0.77, 0.771, 0.773, 0.775, 0.78]$\n  - Fold $4$: $[1.02, 0.93, 0.84, 0.80, 0.78, 0.77, 0.77, 0.771, 0.772, 0.773, 0.774, 0.776, 0.778, 0.782, 0.785]$\n  - Fold $5$: $[1.01, 0.94, 0.85, 0.81, 0.79, 0.78, 0.78, 0.781, 0.782, 0.784, 0.785, 0.786, 0.788, 0.79, 0.792]$\n- Parameters: $\\alpha = 0.6$, $\\varepsilon = 0.001$, $p = 3$, $q = 0.6$, $r = 1.5$, $w = 2$.\n\nCase $2$ (strictly improving, no stop until end):\n- $K = 4$, $T = 12$,\n- Losses:\n  - Fold $1$: $[1.2, 1.1, 1.0, 0.9, 0.8, 0.75, 0.70, 0.66, 0.63, 0.60, 0.58, 0.56]$\n  - Fold $2$: $[1.3, 1.18, 1.06, 0.95, 0.85, 0.78, 0.72, 0.67, 0.64, 0.61, 0.59, 0.57]$\n  - Fold $3$: $[1.15, 1.05, 0.96, 0.88, 0.81, 0.75, 0.71, 0.67, 0.64, 0.61, 0.59, 0.57]$\n  - Fold $4$: $[1.25, 1.13, 1.00, 0.90, 0.82, 0.76, 0.71, 0.67, 0.64, 0.61, 0.59, 0.56]$\n- Parameters: $\\alpha = 0.5$, $\\varepsilon = 0.0005$, $p = 2$, $q = 0.75$, $r = 2.0$, $w = 0$.\n\nCase $3$ (noise with strong overfitting in one fold; variance control prevents premature stop):\n- $K = 3$, $T = 18$,\n- Losses:\n  - Fold $1$: $[0.9, 0.85, 0.82, 0.79, 0.77, 0.76, 0.755, 0.753, 0.752, 0.753, 0.755, 0.758, 0.761, 0.765, 0.77, 0.772, 0.775, 0.778]$\n  - Fold $2$: $[0.92, 0.86, 0.83, 0.80, 0.78, 0.765, 0.76, 0.76, 0.761, 0.763, 0.766, 0.77, 0.775, 0.78, 0.785, 0.79, 0.795, 0.80]$\n  - Fold $3$: $[0.88, 0.84, 0.81, 0.78, 0.76, 0.75, 0.745, 0.743, 0.744, 0.748, 0.753, 0.759, 0.766, 0.775, 0.785, 0.795, 0.805, 0.815]$\n- Parameters: $\\alpha = 0.7$, $\\varepsilon = 0.0015$, $p = 3$, $q = \\frac{2}{3}$, $r = 1.3$, $w = 3$.\n\nCase $4$ (mean plateaus but most folds still improve; high consensus threshold delays stop):\n- $K = 5$, $T = 14$,\n- Losses:\n  - Fold $1$: $[1.0, 0.93, 0.87, 0.83, 0.80, 0.79, 0.785, 0.784, 0.784, 0.785, 0.786, 0.787, 0.788, 0.789]$\n  - Fold $2$: $[0.99, 0.92, 0.86, 0.82, 0.79, 0.78, 0.775, 0.773, 0.772, 0.772, 0.773, 0.774, 0.776, 0.778]$\n  - Fold $3$: $[1.02, 0.94, 0.88, 0.84, 0.81, 0.80, 0.796, 0.795, 0.795, 0.796, 0.797, 0.798, 0.799, 0.80]$\n  - Fold $4$: $[1.01, 0.93, 0.87, 0.83, 0.80, 0.79, 0.785, 0.783, 0.782, 0.782, 0.783, 0.784, 0.786, 0.788]$\n  - Fold $5$: $[1.03, 0.95, 0.89, 0.85, 0.82, 0.81, 0.805, 0.804, 0.804, 0.805, 0.806, 0.808, 0.81, 0.812]$\n- Parameters: $\\alpha = 0.6$, $\\varepsilon = 0.001$, $p = 2$, $q = 0.8$, $r = 1.4$, $w = 2$.\n\nCase $5$ (early plateau but warmup prevents premature stop):\n- $K = 4$, $T = 10$,\n- Losses:\n  - Fold $1$: $[0.8, 0.78, 0.77, 0.77, 0.77, 0.77, 0.77, 0.771, 0.772, 0.773]$\n  - Fold $2$: $[0.82, 0.79, 0.78, 0.78, 0.78, 0.78, 0.781, 0.782, 0.783, 0.784]$\n  - Fold $3$: $[0.81, 0.79, 0.78, 0.78, 0.78, 0.781, 0.782, 0.783, 0.784, 0.785]$\n  - Fold $4$: $[0.83, 0.80, 0.79, 0.79, 0.79, 0.79, 0.791, 0.792, 0.793, 0.794]$\n- Parameters: $\\alpha = 0.5$, $\\varepsilon = 0.0008$, $p = 2$, $q = 0.75$, $r = 1.2$, $w = 5$.\n\nFinal output specification:\n- Your program should produce a single line of output containing the stopping epochs for Cases $1$–$5$ as a comma-separated list enclosed in square brackets, for example: $[e_1,e_2,e_3,e_4,e_5]$.\n- Each $e_i$ must be an integer epoch index in $\\{0,\\dots,T-1\\}$ for the corresponding case.", "solution": "The problem requires the derivation and implementation of a unified early stopping rule for a deep learning model trained using $k$-fold cross-validation. The rule is based on a set of clearly defined mathematical and logical conditions. The solution is thus to construct an algorithm that precisely implements these definitions and conditions.\n\nThe overall algorithmic procedure can be divided into two main phases:\n1.  **Pre-computation of Time-Series Metrics:** For all epochs $t \\in \\{0, \\dots, T-1\\}$, we first calculate all necessary statistical quantities from the raw per-fold validation losses, $\\{\\ell_{k,t}\\}$.\n2.  **Iterative Search for Stopping Epoch:** We then iterate through the epochs, starting from the specified warmup period, to find the first epoch that satisfies all stopping conditions simultaneously.\n\nLet's detail the steps involved in each phase.\n\n**Phase 1: Pre-computation of Metrics**\n\nGiven the $K \\times T$ matrix of validation losses $\\ell_{k,t}$, we compute the following quantities for each epoch $t$ from $0$ to $T-1$.\n\n-   **Smoothed Per-Fold Loss ($S_{k,t}$):** To reduce epoch-level noise, an Exponential Moving Average (EMA) is applied to each fold's loss sequence. The smoothed loss $S_{k,t}$ for fold $k$ at epoch $t$ is defined by the recurrence relation:\n    $$\n    S_{k,t} = \\begin{cases} \\ell_{k,0} & \\text{if } t = 0 \\\\ \\alpha S_{k,t-1} + (1-\\alpha)\\ell_{k,t} & \\text{if } t \\ge 1 \\end{cases}\n    $$\n    where $\\alpha \\in (0,1)$ is the smoothing coefficient.\n\n-   **Cross-Fold Aggregates ($M_t, \\sigma_t$):** At each epoch $t$, the smoothed losses from all $K$ folds are aggregated into a mean $M_t$ and a population standard deviation $\\sigma_t$:\n    $$\n    M_t = \\frac{1}{K}\\sum_{k=1}^{K} S_{k,t}\n    $$\n    $$\n    \\sigma_t = \\sqrt{\\frac{1}{K}\\sum_{k=1}^{K} (S_{k,t}-M_t)^2}\n    $$\n\n-   **Running Minima ($m_t, b_{k,t}$):** To track progress, we maintain the best (minimum) values seen so far for both the aggregated mean and the individual fold losses.\n    -   The running minimum of the cross-fold mean is $m_t = \\min\\{M_0, M_1, \\dots, M_t\\}$.\n    -   The running minimum for each fold's smoothed loss is $b_{k,t} = \\min\\{S_{k,0}, S_{k,1}, \\dots, S_{k,t}\\}$.\n\n-   **Derived Indicators ($I_t^{(\\text{mean})}, q_t$):** Using the above quantities, we compute the two primary indicators for non-improvement.\n    -   **Mean Non-Improvement Indicator ($I_t^{(\\text{mean})}$):** This binary indicator flags if the cross-fold mean $M_t$ has failed to improve meaningfully upon its best value so far, $m_{t-1}$.\n        $$\n        I_t^{(\\text{mean})} = \\begin{cases} 0 & \\text{if } t = 0 \\\\ 1 & \\text{if } t \\ge 1 \\text{ and } M_t > m_{t-1} - \\varepsilon \\\\ 0 & \\text{if } t \\ge 1 \\text{ and } M_t \\le m_{t-1} - \\varepsilon \\end{cases}\n        $$\n        where $\\varepsilon > 0$ is a small tolerance.\n    -   **Fold Consensus Indicator ($q_t$):** This metric quantifies the fraction of folds that have failed to improve their own best smoothed loss, $b_{k,t-1}$. For $t \\ge 1$, it is defined as:\n        $$\n        q_t = \\frac{1}{K}\\sum_{k=1}^{K} \\mathbb{1}\\left(S_{k,t} > b_{k,t-1} - \\varepsilon\\right)\n        $$\n        where $\\mathbb{1}(\\cdot)$ is the indicator function. For $t=0$, $q_0$ is taken to be $0$ as there is no prior history to improve upon.\n\n**Phase 2: Identification of the Stopping Epoch**\n\nAfter pre-computing all necessary metrics for every epoch, we search for the stopping epoch, defined as the earliest integer epoch index $t$ that satisfies a set of five conditions. We iterate $t$ from $0$ to $T-1$ and check the following conditions:\n\n1.  **Warmup ($w$):** The rule cannot trigger before a specified warmup period. The condition is $t \\ge w$, where $w \\in \\mathbb{N}$.\n\n2.  **Mean-Based Patience ($p$):** The cross-fold mean must have failed to improve for $p$ consecutive epochs. This requires the mean non-improvement indicator to be active for the entire patience window:\n    $$\n    I_{t'}^{(\\text{mean})} = 1 \\quad \\forall t' \\in \\{t-p+1, \\dots, t\\}\n    $$\n    This condition implies that $t$ must also be at least $p-1$.\n\n3.  **Cross-Fold Consensus ($q$):** A minimum fraction of folds must agree that training is no longer productive. The condition is that the fraction of non-improving folds, $q_t$, must meet or exceed a threshold $q \\in (0,1]$:\n    $$\n    q_t \\ge q\n    $$\n\n4.  **Variance Growth Control ($r$):** To prevent stopping due to idiosyncratic overfitting in a single fold (which would increase variance), the cross-fold standard deviation $\\sigma_t$ must not be excessively large compared to its historical minimum. The condition is:\n    $$\n    \\sigma_t \\le r \\cdot \\min\\{\\sigma_0, \\sigma_1, \\dots, \\sigma_t\\}\n    $$\n    for a factor $r \\ge 1$.\n\nThe stopping epoch is the first value of $t$ that meets all four of these conditions (the warmup is the first gate). If the loop completes without finding any such $t$, the process has not converged according to the rule, and training should continue until the final epoch. In this case, the stopping epoch is designated as $T-1$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef find_stopping_epoch(losses, alpha, eps, p, q, r, w):\n    \"\"\"\n    Implements the unified Early Stopping rule under k-fold Cross-Validation.\n    \"\"\"\n    K, T = losses.shape\n\n    # Handle trivial case of T=0 or T=1\n    if T <= 1:\n        return T - 1\n\n    # Phase 1: Pre-computation of all time-series metrics\n    \n    # Smoothed per-fold loss (S_k,t)\n    S = np.zeros_like(losses, dtype=float)\n    S[:, 0] = losses[:, 0]\n    for t in range(1, T):\n        S[:, t] = alpha * S[:, t - 1] + (1 - alpha) * losses[:, t]\n\n    # Cross-fold mean (M_t) and standard deviation (sigma_t)\n    M = np.mean(S, axis=0)\n    sigma = np.std(S, axis=0) # Population std dev (ddof=0) is the default\n\n    # Running minimum of the mean (m_t) and per-fold smoothed loss (b_k,t)\n    m = np.minimum.accumulate(M)\n    b = np.minimum.accumulate(S, axis=1)\n\n    # Mean non-improvement indicator (I_t_mean)\n    I_mean = np.zeros(T)\n    # Loop starts from t=1, as I_0 is defined as 0\n    for t in range(1, T):\n        if M[t] > m[t - 1] - eps:\n            I_mean[t] = 1\n\n    # Fold consensus indicator (q_t)\n    q_t_arr = np.zeros(T)\n    # Loop starts from t=1, as q_0 is undefined/taken as 0\n    for t in range(1, T):\n        # S_k,t > b_k,t-1 - eps\n        not_improving_folds = S[:, t] > b[:, t - 1] - eps\n        q_t_arr[t] = np.mean(not_improving_folds)\n\n    # Phase 2: Identification of the stopping epoch\n\n    # Pre-calculate number of consecutive ones for patience check efficiency\n    # This correctly implements the check `I_{t-p+1}=...=I_t=1`\n    consecutive_ones = np.zeros(T, dtype=int)\n    for t in range(1, T):\n        if I_mean[t] == 1:\n            consecutive_ones[t] = consecutive_ones[t - 1] + 1\n    \n    # Pre-calculate running minimum of sigma for variance check\n    min_sigma_so_far = np.minimum.accumulate(sigma)\n\n    # Iterate through epochs to find the first one satisfying all conditions\n    for t in range(T):\n        # Condition 5: Warmup\n        if t < w:\n            continue\n\n        # Condition 2: Mean-based patience\n        # p is in N, so p >= 1. The check is for `p` consecutive 1s in I_mean.\n        if p > 0:\n            mean_patience_cond = (consecutive_ones[t] >= p)\n        else: # Should not happen based on problem, but robustly handle p=0\n            mean_patience_cond = True\n        \n        if not mean_patience_cond:\n            continue\n        \n        # Condition 3: Cross-fold consensus\n        consensus_cond = (q_t_arr[t] >= q)\n        if not consensus_cond:\n            continue\n\n        # Condition 4: Variance growth control\n        variance_cond = (sigma[t] <= r * min_sigma_so_far[t])\n        if not variance_cond:\n            continue\n        \n        # If all conditions are met, this is the stopping epoch\n        return t\n\n    # If the loop finishes, no stopping epoch was found, so return T-1\n    return T - 1\n\ndef solve():\n    \"\"\"\n    Defines test cases and runs the solver for each case.\n    \"\"\"\n    test_cases = [\n        # Case 1\n        (np.array([\n            [1.00, 0.92, 0.85, 0.80, 0.78, 0.77, 0.77, 0.775, 0.78, 0.781, 0.782, 0.783, 0.785, 0.79, 0.795],\n            [1.05, 0.95, 0.86, 0.81, 0.79, 0.78, 0.78, 0.782, 0.784, 0.785, 0.786, 0.787, 0.79, 0.795, 0.80],\n            [0.98, 0.90, 0.83, 0.79, 0.77, 0.765, 0.765, 0.767, 0.768, 0.769, 0.77, 0.771, 0.773, 0.775, 0.78],\n            [1.02, 0.93, 0.84, 0.80, 0.78, 0.77, 0.77, 0.771, 0.772, 0.773, 0.774, 0.776, 0.778, 0.782, 0.785],\n            [1.01, 0.94, 0.85, 0.81, 0.79, 0.78, 0.78, 0.781, 0.782, 0.784, 0.785, 0.786, 0.788, 0.79, 0.792]\n        ]), 0.6, 0.001, 3, 0.6, 1.5, 2),\n        # Case 2\n        (np.array([\n            [1.2, 1.1, 1.0, 0.9, 0.8, 0.75, 0.70, 0.66, 0.63, 0.60, 0.58, 0.56],\n            [1.3, 1.18, 1.06, 0.95, 0.85, 0.78, 0.72, 0.67, 0.64, 0.61, 0.59, 0.57],\n            [1.15, 1.05, 0.96, 0.88, 0.81, 0.75, 0.71, 0.67, 0.64, 0.61, 0.59, 0.57],\n            [1.25, 1.13, 1.00, 0.90, 0.82, 0.76, 0.71, 0.67, 0.64, 0.61, 0.59, 0.56]\n        ]), 0.5, 0.0005, 2, 0.75, 2.0, 0),\n        # Case 3\n        (np.array([\n            [0.9, 0.85, 0.82, 0.79, 0.77, 0.76, 0.755, 0.753, 0.752, 0.753, 0.755, 0.758, 0.761, 0.765, 0.77, 0.772, 0.775, 0.778],\n            [0.92, 0.86, 0.83, 0.80, 0.78, 0.765, 0.76, 0.76, 0.761, 0.763, 0.766, 0.77, 0.775, 0.78, 0.785, 0.79, 0.795, 0.80],\n            [0.88, 0.84, 0.81, 0.78, 0.76, 0.75, 0.745, 0.743, 0.744, 0.748, 0.753, 0.759, 0.766, 0.775, 0.785, 0.795, 0.805, 0.815]\n        ]), 0.7, 0.0015, 3, 2/3, 1.3, 3),\n        # Case 4\n        (np.array([\n            [1.0, 0.93, 0.87, 0.83, 0.80, 0.79, 0.785, 0.784, 0.784, 0.785, 0.786, 0.787, 0.788, 0.789],\n            [0.99, 0.92, 0.86, 0.82, 0.79, 0.78, 0.775, 0.773, 0.772, 0.772, 0.773, 0.774, 0.776, 0.778],\n            [1.02, 0.94, 0.88, 0.84, 0.81, 0.80, 0.796, 0.795, 0.795, 0.796, 0.797, 0.798, 0.799, 0.80],\n            [1.01, 0.93, 0.87, 0.83, 0.80, 0.79, 0.785, 0.783, 0.782, 0.782, 0.783, 0.784, 0.786, 0.788],\n            [1.03, 0.95, 0.89, 0.85, 0.82, 0.81, 0.805, 0.804, 0.804, 0.805, 0.806, 0.808, 0.81, 0.812]\n        ]), 0.6, 0.001, 2, 0.8, 1.4, 2),\n        # Case 5\n        (np.array([\n            [0.8, 0.78, 0.77, 0.77, 0.77, 0.77, 0.77, 0.771, 0.772, 0.773],\n            [0.82, 0.79, 0.78, 0.78, 0.78, 0.78, 0.781, 0.782, 0.783, 0.784],\n            [0.81, 0.79, 0.78, 0.78, 0.78, 0.781, 0.782, 0.783, 0.784, 0.785],\n            [0.83, 0.80, 0.79, 0.79, 0.79, 0.79, 0.791, 0.792, 0.793, 0.794]\n        ]), 0.5, 0.0008, 2, 0.75, 1.2, 5),\n    ]\n\n    results = []\n    for case in test_cases:\n        losses, alpha, eps, p, q, r, w = case\n        result = find_stopping_epoch(losses, alpha, eps, p, q, r, w)\n        results.append(result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3139126"}]}