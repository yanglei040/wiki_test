{"hands_on_practices": [{"introduction": "This practice will build your intuition by going back to first principles. We will explore how class imbalance naturally affects an optimal decision boundary in a simple linear classifier, demonstrating that accounting for priors is a core component of probabilistic modeling. By deriving the decision boundary for Linear Discriminant Analysis (LDA) from scratch, you will see precisely how unequal class frequencies shift the boundary to favor the majority class and quantify this effect [@problem_id:3127149].", "problem": "Consider a two-class classification task modeled by Linear Discriminant Analysis (LDA), where class-conditional feature distributions are one-dimensional Gaussian and share a common variance. Specifically, suppose that for class label $y \\in \\{0,1\\}$, the feature $x \\in \\mathbb{R}$ is distributed as\n$$\nx \\mid y=k \\sim \\mathcal{N}(\\mu_k,\\sigma^2), \\quad k \\in \\{0,1\\},\n$$\nwith parameters $\\mu_0 = 0.5$, $\\mu_1 = 2.3$, and $\\sigma^2 = 1.44$. The prior probabilities are imbalanced: $\\pi_0 = 0.7$ and $\\pi_1 = 0.3$.\n\nStarting only from (i) Bayes’ decision rule “assign to the class with larger posterior probability,” (ii) Bayes’ theorem relating posteriors to priors and likelihoods, and (iii) the Gaussian probability density function, derive the location $x^{\\star}_{\\text{unequal}}$ of the LDA decision boundary under these unequal priors by solving for the point where the posteriors are equal. Then, derive the location $x^{\\star}_{\\text{equal}}$ of the decision boundary in the equal-prior case with $\\pi_0=\\pi_1=\\tfrac{1}{2}$ using the same principles.\n\nCompute the shift in the decision boundary due to class imbalance,\n$$\n\\Delta x \\equiv x^{\\star}_{\\text{unequal}} - x^{\\star}_{\\text{equal}}.\n$$\nRound your numeric answer for $\\Delta x$ to four significant figures.", "solution": "We begin with Bayes’ decision rule: assign an observation $x$ to the class with the larger posterior probability. The decision boundary is the set of $x$ for which the posteriors are equal:\n$$\np(y=1 \\mid x) = p(y=0 \\mid x).\n$$\nUsing Bayes’ theorem and the fact that the class-conditional densities share the same support, this boundary satisfies\n$$\np(x \\mid y=1)\\,\\pi_1 = p(x \\mid y=0)\\,\\pi_0.\n$$\nThe Gaussian probability density function for class $k \\in \\{0,1\\}$ is\n$$\np(x \\mid y=k) = \\frac{1}{\\sqrt{2\\pi}\\,\\sigma}\\exp\\!\\left(-\\frac{(x-\\mu_k)^2}{2\\sigma^2}\\right).\n$$\nSubstituting these into the equality and canceling the common factor $\\frac{1}{\\sqrt{2\\pi}\\,\\sigma}$ yields\n$$\n\\exp\\!\\left(-\\frac{(x-\\mu_1)^2}{2\\sigma^2}\\right)\\pi_1\n=\n\\exp\\!\\left(-\\frac{(x-\\mu_0)^2}{2\\sigma^2}\\right)\\pi_0.\n$$\nTaking natural logarithms of both sides gives\n$$\n-\\frac{(x-\\mu_1)^2}{2\\sigma^2} + \\ln \\pi_1\n=\n-\\frac{(x-\\mu_0)^2}{2\\sigma^2} + \\ln \\pi_0.\n$$\nRearranging,\n$$\n\\frac{(x-\\mu_0)^2 - (x-\\mu_1)^2}{2\\sigma^2}\n=\n\\ln\\!\\left(\\frac{\\pi_0}{\\pi_1}\\right).\n$$\nExpanding the squares,\n$$\n(x^2 - 2x\\mu_0 + \\mu_0^2) - (x^2 - 2x\\mu_1 + \\mu_1^2)\n=\n2x(\\mu_1 - \\mu_0) + (\\mu_0^2 - \\mu_1^2).\n$$\nThus,\n$$\n\\frac{2x(\\mu_1 - \\mu_0) + (\\mu_0^2 - \\mu_1^2)}{2\\sigma^2}\n=\n\\ln\\!\\left(\\frac{\\pi_0}{\\pi_1}\\right),\n$$\nso\n$$\n2x(\\mu_1 - \\mu_0) + (\\mu_0^2 - \\mu_1^2) = 2\\sigma^2 \\ln\\!\\left(\\frac{\\pi_0}{\\pi_1}\\right).\n$$\nSolving for $x$,\n$$\nx\n=\n\\frac{2\\sigma^2 \\ln\\!\\left(\\frac{\\pi_0}{\\pi_1}\\right) - (\\mu_0^2 - \\mu_1^2)}{2(\\mu_1 - \\mu_0)}\n=\n\\frac{\\mu_0 + \\mu_1}{2} + \\frac{\\sigma^2}{\\mu_1 - \\mu_0}\\,\\ln\\!\\left(\\frac{\\pi_0}{\\pi_1}\\right).\n$$\nThis is the LDA decision boundary for unequal priors:\n$$\nx^{\\star}_{\\text{unequal}} = \\frac{\\mu_0 + \\mu_1}{2} + \\frac{\\sigma^2}{\\mu_1 - \\mu_0}\\,\\ln\\!\\left(\\frac{\\pi_0}{\\pi_1}\\right).\n$$\nFor equal priors $\\pi_0=\\pi_1=\\tfrac{1}{2}$, we have $\\ln(\\pi_0/\\pi_1)=\\ln 1 = 0$, so the decision boundary simplifies to the midpoint:\n$$\nx^{\\star}_{\\text{equal}} = \\frac{\\mu_0 + \\mu_1}{2}.\n$$\nTherefore, the shift induced by unequal priors is\n$$\n\\Delta x \\equiv x^{\\star}_{\\text{unequal}} - x^{\\star}_{\\text{equal}}\n=\n\\frac{\\sigma^2}{\\mu_1 - \\mu_0}\\,\\ln\\!\\left(\\frac{\\pi_0}{\\pi_1}\\right).\n$$\nNow substitute the given numerical values $\\mu_0 = 0.5$, $\\mu_1 = 2.3$, $\\sigma^2 = 1.44$, $\\pi_0 = 0.7$, and $\\pi_1 = 0.3$:\n$$\n\\mu_1 - \\mu_0 = 2.3 - 0.5 = 1.8,\n\\quad\n\\frac{\\sigma^2}{\\mu_1 - \\mu_0} = \\frac{1.44}{1.8} = 0.8,\n\\quad\n\\ln\\!\\left(\\frac{\\pi_0}{\\pi_1}\\right) = \\ln\\!\\left(\\frac{0.7}{0.3}\\right) = \\ln\\!\\left(\\frac{7}{3}\\right).\n$$\nCompute the logarithm:\n$$\n\\ln\\!\\left(\\frac{7}{3}\\right) \\approx 0.847297860.\n$$\nTherefore,\n$$\n\\Delta x \\approx 0.8 \\times 0.847297860 \\approx 0.677838288.\n$$\nRounding to four significant figures yields\n$$\n\\Delta x \\approx 0.6778.\n$$\nThis positive shift indicates that, because $\\pi_0 > \\pi_1$, the decision boundary moves to the right relative to the equal-prior midpoint, enlarging the region predicted as class $0$. This is the manifestation of Linear Discriminant Analysis (LDA)’s log prior term handling class imbalance by offsetting the boundary toward the minority class side.", "answer": "$$\\boxed{0.6778}$$", "id": "3127149"}, {"introduction": "With a foundational understanding of how imbalance impacts decision rules, we now compare two popular intervention strategies: weighting the data during training versus adjusting the decision threshold after training. This exercise uses a decision tree to illustrate a crucial difference: weighting can change the very structure of the learned model, while thresholding cannot. By working through a concrete example, you will gain insight into when one approach might be preferable to the other [@problem_id:3112943].", "problem": "Consider a binary classification task with classes $y \\in \\{0,1\\}$, where the training data are highly imbalanced. A decision tree is trained at the root with a single candidate split on a binary feature $X$ that partitions the training set into two regions $A$ and $B$. The training set has $190$ instances of class $0$ and $10$ instances of class $1$. Region $A$ contains $160$ instances of class $0$ and $2$ instances of class $1$, while region $B$ contains $30$ instances of class $0$ and $8$ instances of class $1$. The tree uses a standard stopping rule at each node: do not split unless the impurity decrease exceeds a minimum threshold $\\Delta I_{\\min} = 0.02$.\n\nTwo handling strategies for imbalance are considered:\n\n- Strategy $S_{\\text{train}}$: train the tree with class weights $w_0 = 1$ for class $0$ and $w_1 = 10$ for class $1$, applied in the splitting criterion and node weighting during training.\n- Strategy $S_{\\text{pred}}$: train the tree without class weights, but at prediction time use a probability threshold $t \\in (0,1)$ so that the classifier predicts class $1$ whenever the estimated posterior probability $\\hat{p}(y=1 \\mid x)$ exceeds $t$, else predicts class $0$.\n\nBased on first principles of risk minimization in classification, and the mechanics of decision tree splitting and prediction, which of the following statements are true?\n\nA. When the unweighted tree would not split at the root because the impurity decrease is less than $\\Delta I_{\\min}$, but the weighted tree would split because the minority class is emphasized, $S_{\\text{train}}$ can isolate a minority region and improve recall, while $S_{\\text{pred}}$ cannot create new partitions and therefore cannot recover that structure; in this case, $S_{\\text{train}}$ is preferable.\n\nB. If the tree’s leaf posterior estimates $\\hat{p}(y=1 \\mid x)$ are well calibrated, then $S_{\\text{pred}}$ and $S_{\\text{train}}$ are always exactly equivalent in their final classifications; adjusting $t$ after training can reproduce any effect of class weighting during training.\n\nC. If the deployment scenario changes misclassification costs or class priors relative to training, and the tree’s leaf posterior estimates $\\hat{p}(y=1 \\mid x)$ remain well calibrated for the deployment distribution, then adjusting $t$ at prediction time implements the Bayes decision rule without retraining; in this case $S_{\\text{pred}}$ is sufficient, and retraining with class weights is unnecessary.\n\nD. For decision trees using the Gini impurity, class weighting during training is mathematically equivalent to shifting the probability threshold $t$ at prediction time, regardless of calibration, stopping rules, or tree depth.\n\nE. Threshold adjustment at prediction time is preferable when the minority class is concentrated in a small, well-defined region of feature space that requires deeper splits to isolate, because $S_{\\text{pred}}$ can classify those cases correctly even if the tree never splits to isolate that region.", "solution": "The problem asks for an evaluation of statements comparing two strategies for handling class imbalance in a binary classification task: $S_{\\text{train}}$, which uses class weights during training, and $S_{\\text{pred}}$, which adjusts the prediction threshold after training an unweighted tree.\n\nFirst, the validity of the problem statement is confirmed. All given numerical data is self-consistent and the scenario is a standard problem in statistical learning.\n- Training set: $N=200$ instances. Class $0$: $N_0=190$. Class $1$: $N_1=10$.\n- Root node (let's call it $R$): $N_R=200$, with $N_{R,0}=190$ and $N_{R,1}=10$.\n- Candidate split partitions $R$ into:\n  - Region $A$: $N_A=162$, with $N_{A,0}=160$ and $N_{A,1}=2$.\n  - Region $B$: $N_B=38$, with $N_{B,0}=30$ and $N_{B,1}=8$.\n- Stopping rule threshold: $\\Delta I_{\\min} = 0.02$.\n- Strategy $S_{\\text{train}}$ weights: $w_0=1$ for class $0$, $w_1=10$ for class $1$.\n\nTo evaluate the specific scenario, we must calculate the impurity decrease for both the unweighted and weighted cases. We will use the Gini impurity, a standard metric for decision trees. The Gini impurity for a node $m$ with class proportions $p_{m,k}$ is $I_G(m) = 1 - \\sum_k p_{m,k}^2$. The impurity decrease for a split is $\\Delta I = I(\\text{parent}) - \\sum_{j \\in \\text{children}} \\frac{N_j}{N_{\\text{parent}}} I(\\text{child}_j)$.\n\n**Analysis of the Unweighted Case ($S_{\\text{pred}}$)**\n\nThe prediction threshold adjustment of $S_{\\text{pred}}$ is applied to the tree trained without weights. We must first determine if this tree will split at the root.\n\n1.  **Gini Impurity of the Root Node ($R$)**:\n    The proportions are $p_{R,0} = \\frac{190}{200} = 0.95$ and $p_{R,1} = \\frac{10}{200} = 0.05$.\n    $$I(R) = 1 - (0.95^2 + 0.05^2) = 1 - (0.9025 + 0.0025) = 0.095$$\n\n2.  **Gini Impurity of Child Nodes ($A$ and $B$)**:\n    - For node $A$, proportions are $p_{A,0} = \\frac{160}{162}$ and $p_{A,1} = \\frac{2}{162}$.\n      $$I(A) = 1 - \\left( \\left(\\frac{160}{162}\\right)^2 + \\left(\\frac{2}{162}\\right)^2 \\right) = 1 - \\frac{160^2 + 2^2}{162^2} = 1 - \\frac{25604}{26244} \\approx 0.0244$$\n    - For node $B$, proportions are $p_{B,0} = \\frac{30}{38}$ and $p_{B,1} = \\frac{8}{38}$.\n      $$I(B) = 1 - \\left( \\left(\\frac{30}{38}\\right)^2 + \\left(\\frac{8}{38}\\right)^2 \\right) = 1 - \\frac{30^2 + 8^2}{38^2} = 1 - \\frac{964}{1444} \\approx 0.3324$$\n\n3.  **Impurity Decrease ($\\Delta I$)**:\n    The impurity decrease is the weighted average impurity of the children subtracted from the parent's impurity.\n    $$\\Delta I = I(R) - \\left( \\frac{N_A}{N_R} I(A) + \\frac{N_B}{N_R} I(B) \\right)$$\n    $$\\Delta I \\approx 0.095 - \\left( \\frac{162}{200} \\times 0.0244 + \\frac{38}{200} \\times 0.3324 \\right)$$\n    $$\\Delta I \\approx 0.095 - (0.81 \\times 0.0244 + 0.19 \\times 0.3324) \\approx 0.095 - (0.01976 + 0.06316) \\approx 0.095 - 0.08292 = 0.01208$$\n\n4.  **Conclusion on Splitting**:\n    Since $\\Delta I \\approx 0.0121 < \\Delta I_{\\min} = 0.02$, the tree trained without weights will **not** split at the root. It will remain a single leaf node.\n\n**Analysis of the Weighted Case ($S_{\\text{train}}$)**\n\nClass weights modify the impurity calculation. The contribution of each class $k$ is multiplied by its weight $w_k$. In the Gini impurity, this is equivalent to using weighted proportions. The weighted proportion of class $k$ in node $m$ is $p'_{m,k} = \\frac{w_k N_{m,k}}{\\sum_j w_j N_{m,j}}$. Let $W_m = \\sum_j w_j N_{m,j}$ be the total weight in node $m$.\n\n1.  **Weighted Gini Impurity of the Root Node ($R$)**:\n    The total weight is $W_R = (w_0 \\times 190) + (w_1 \\times 10) = (1 \\times 190) + (10 \\times 10) = 290$.\n    The weighted proportions are $p'_{R,0} = \\frac{190}{290}$ and $p'_{R,1} = \\frac{100}{290}$.\n    $$I'(R) = 1 - \\left( \\left(\\frac{190}{290}\\right)^2 + \\left(\\frac{100}{290}\\right)^2 \\right) = 1 - \\frac{19^2 + 10^2}{29^2} = 1 - \\frac{361+100}{841} = 1 - \\frac{461}{841} \\approx 0.4518$$\n\n2.  **Weighted Gini Impurity of Child Nodes ($A$ and $B$)**:\n    - For node $A$, $W_A = (1 \\times 160) + (10 \\times 2) = 180$.\n      $$I'(A) = 1 - \\left( \\left(\\frac{160}{180}\\right)^2 + \\left(\\frac{20}{180}\\right)^2 \\right) = 1 - \\left( \\left(\\frac{8}{9}\\right)^2 + \\left(\\frac{1}{9}\\right)^2 \\right) = 1 - \\frac{64+1}{81} = \\frac{16}{81} \\approx 0.1975$$\n    - For node $B$, $W_B = (1 \\times 30) + (10 \\times 8) = 110$.\n      $$I'(B) = 1 - \\left( \\left(\\frac{30}{110}\\right)^2 + \\left(\\frac{80}{110}\\right)^2 \\right) = 1 - \\left( \\left(\\frac{3}{11}\\right)^2 + \\left(\\frac{8}{11}\\right)^2 \\right) = 1 - \\frac{9+64}{121} = \\frac{48}{121} \\approx 0.3967$$\n\n3.  **Weighted Impurity Decrease ($\\Delta I'$)**:\n    The child impurities are weighted by their share of the total weight.\n    $$\\Delta I' = I'(R) - \\left( \\frac{W_A}{W_R} I'(A) + \\frac{W_B}{W_R} I'(B) \\right)$$\n    $$\\Delta I' \\approx 0.4518 - \\left( \\frac{180}{290} \\times 0.1975 + \\frac{110}{290} \\times 0.3967 \\right)$$\n    $$\\Delta I' \\approx 0.4518 - (0.6207 \\times 0.1975 + 0.3793 \\times 0.3967) \\approx 0.4518 - (0.1226 + 0.1505) \\approx 0.4518 - 0.2731 = 0.1787$$\n\n4.  **Conclusion on Splitting**:\n    Since $\\Delta I' \\approx 0.1787 > \\Delta I_{\\min} = 0.02$, the tree trained with weights will **split** at the root.\n\nNow we evaluate each statement.\n\n**A. When the unweighted tree would not split at the root because the impurity decrease is less than $\\Delta I_{\\min}$, but the weighted tree would split because the minority class is emphasized, $S_{\\text{train}}$ can isolate a minority region and improve recall, while $S_{\\text{pred}}$ cannot create new partitions and therefore cannot recover that structure; in this case, $S_{\\text{train}}$ is preferable.**\nOur calculations have confirmed the premise of this statement: the unweighted tree does not split, but the weighted tree does. The $S_{\\text{pred}}$ strategy uses the unweighted tree, which is just a single node. It cannot distinguish between instances that would fall in region $A$ versus region $B$. All instances receive the same posterior estimate, $\\hat{p}(y=1|x) = 10/200 = 0.05$. Varying the threshold $t$ can only classify all instances as $0$ or all as $1$. It cannot achieve any separation. In contrast, $S_{\\text{train}}$ creates a partition. Leaf node $B$ is now relatively pure in the minority class (from a weighted perspective) and contains $8$ of the $10$ total class $1$ instances. This allows the model to identify a sub-population with a high likelihood of being class $1$, dramatically improving recall for the minority class. Therefore, the statement is a correct description of the dynamic between the two strategies in this exact scenario.\n**Verdict: Correct**\n\n**B. If the tree’s leaf posterior estimates $\\hat{p}(y=1 \\mid x)$ are well calibrated, then $S_{\\text{pred}}$ and $S_{\\text{train}}$ are always exactly equivalent in their final classifications; adjusting $t$ after training can reproduce any effect of class weighting during training.**\nThis statement is false. The core difference between the two strategies is that $S_{\\text{train}}$ (weighting) can alter the *structure* of the tree itself—the sequence of splits and the final partitions of the feature space. $S_{\\text{pred}}$ (thresholding) operates on a fixed tree structure and only changes the decision rule applied to the posteriors of the existing leaves. As demonstrated in our analysis of statement A, weighting can cause a split to occur where it otherwise wouldn't, leading to a fundamentally different set of leaf nodes and posteriors. Since the two strategies can produce different tree structures, they cannot be \"always exactly equivalent\".\n**Verdict: Incorrect**\n\n**C. If the deployment scenario changes misclassification costs or class priors relative to training, and the tree’s leaf posterior estimates $\\hat{p}(y=1 \\mid x)$ remain well calibrated for the deployment distribution, then adjusting $t$ at prediction time implements the Bayes decision rule without retraining; in this case $S_{\\text{pred}}$ is sufficient, and retraining with class weights is unnecessary.**\nThis statement is a correct application of Bayesian decision theory. The Bayes optimal decision rule is to predict the class that minimizes the expected loss. This decision can be expressed as comparing the posterior probability $p(y=1|x)$ to a threshold $t^*$ that is a function of the misclassification costs. The rule is: predict $1$ if $p(y=1|x) > t^*$. If class priors change from training to deployment, the posterior probabilities can also be updated (a process sometimes called prior correction), which is also equivalent to shifting the decision threshold on the original posteriors. Therefore, if a model produces well-calibrated posteriors, it is not necessary to retrain it to adapt to new costs or priors; one can simply calculate the new optimal threshold $t^*$ and apply it at prediction time. This is precisely the mechanism of the $S_{\\text{pred}}$ strategy. The key assumption is that the trained tree is \"good enough\" at capturing the conditional likelihoods $p(x|y)$, which is implied by the premise that the posteriors \"remain well calibrated\".\n**Verdict: Correct**\n\n**D. For decision trees using the Gini impurity, class weighting during training is mathematically equivalent to shifting the probability threshold $t$ at prediction time, regardless of calibration, stopping rules, or tree depth.**\nThis is an over-generalized and false statement. As shown in the analysis for A, class weighting affects the impurity calculation for each potential split, which in turn influences which feature is chosen for splitting and whether the stopping criteria are met. This fundamentally changes the tree's construction. Shifting the prediction threshold has no impact on tree construction. Because they affect different parts of the modeling process (training vs. prediction) and can lead to different tree structures, they are not mathematically equivalent. The phrase \"regardless of... stopping rules\" is directly contradicted by our calculations.\n**Verdict: Incorrect**\n\n**E. Threshold adjustment at prediction time is preferable when the minority class is concentrated in a small, well-defined region of feature space that requires deeper splits to isolate, because $S_{\\text{pred}}$ can classify those cases correctly even if the tree never splits to isolate that region.**\nThis statement contains a logical contradiction. If a tree \"never splits to isolate that region,\" then all instances in that small region will be grouped into a larger, more heterogeneous leaf node. The posterior probability for that leaf, $\\hat{p}(y=1|x)$, will be diluted by the majority class instances in the leaf. The $S_{\\text{pred}}$ strategy can only apply a threshold to this diluted posterior. It has no access to the fine-grained information within the leaf that the tree failed to partition. Therefore, it *cannot* classify those cases correctly. This scenario is, in fact, an argument *for* using a method like $S_{\\text{train}}$, which might force the tree to perform the necessary deeper splits to find that small, concentrated region. The statement's reasoning is flawed.\n**Verdict: Incorrect**\n\nThe true statements are A and C.", "answer": "$$\\boxed{AC}$$", "id": "3112943"}, {"introduction": "Moving from theory to application, this practice challenges you to implement a principled post-processing correction for a multi-class imbalanced problem. You will derive and code the logit adjustment technique, which directly incorporates class priors into the model's output scores based on Bayes' rule. This exercise will not only solidify your understanding but also highlight the important distinction between improving a model's ranking ability (accuracy) and its probabilistic calibration (confidence) [@problem_id:3127123].", "problem": "You are given a multi-class classification setting with class imbalance, where a model produces real-valued scores for each class that are commonly called logits. The softmax function transforms logits into a probability distribution over classes. You will implement a principled logit adjustment that incorporates class prior information and then quantify how this adjustment affects two distinct aspects: ranking (as measured by top-$1$ accuracy) and calibration (as measured by mean negative log-likelihood).\n\nStarting from the fundamental definition of conditional probability through Bayes’ rule, for any input feature vector $x$ and class $k$, the posterior satisfies\n$$\np(y=k \\mid x) \\propto p(x \\mid y=k)\\,p(y=k),\n$$\nwith proportionality constant given by the evidence $p(x)$. When a discriminative model’s per-class logits $z_k(x)$ are proportional to class-conditional log-likelihoods up to an additive class-dependent constant, the probabilistic prediction that correctly accounts for class priors $p(y=k)$ is obtained by transforming logits into posterior-proportional scores before normalization. This motivates a logit transformation that encodes the prior information, after which probabilities are obtained with a softmax normalization. Your task is to derive this transformation from Bayes’ rule and implement both the unadjusted and adjusted softmax predictions.\n\nYour program must:\n1. Implement the unadjusted softmax mapping from logits $z_{i,k}$ to probabilities $p_{i,k}$.\n2. Derive and implement a prior-informed logit transformation based on Bayes’ rule that yields adjusted probabilities $q_{i,k}$ after softmax normalization.\n3. For each test case, compute:\n   - Top-$1$ accuracy under unadjusted and adjusted probabilities, defined as the fraction (expressed as a decimal) of examples for which the predicted class $\\arg\\max_k p_{i,k}$ or $\\arg\\max_k q_{i,k}$ equals the true label.\n   - Mean negative log-likelihood (NLL) under unadjusted and adjusted probabilities, defined as the average of $-\\log p_{i,y_i}$ or $-\\log q_{i,y_i}$ over all examples $i$, where $y_i$ is the true class label of example $i$.\n4. For each test case, output two values:\n   - The accuracy difference $a = \\text{accuracy}_{\\text{adjusted}} - \\text{accuracy}_{\\text{unadjusted}}$.\n   - The NLL difference $\\ell = \\text{NLL}_{\\text{adjusted}} - \\text{NLL}_{\\text{unadjusted}}$.\n   Report $a$ and $\\ell$ as decimal numbers (no percentage symbol), rounded to six decimal places.\n\nDefinitions to use:\n- For each example $i$ with logits $\\{z_{i,1},\\dots,z_{i,K}\\}$, the unadjusted softmax probabilities are\n$$\np_{i,k} = \\frac{\\exp(z_{i,k})}{\\sum_{j=1}^{K} \\exp(z_{i,j})}.\n$$\n- The adjusted probabilities must be obtained by first transforming the logits using the class prior information in a manner implied by Bayes’ rule, and then applying a softmax normalization to produce a valid distribution over classes.\n\nTest suite specification. For each test case, you are given a logit matrix $Z \\in \\mathbb{R}^{n \\times K}$, integer labels $\\boldsymbol{y} \\in \\{0,\\dots,K-1\\}^n$, and a class-prior vector $\\boldsymbol{\\pi} \\in \\mathbb{R}^K$ with entries in $(0,1)$ summing to $1$. Use the following four test cases:\n\nTest case $1$ (general long-tail):\n$$\nZ^{(1)} = \\begin{bmatrix}\n2.5 & 1.0 & 0.0\\\\\n0.0 & 1.2 & 1.0\\\\\n1.2 & 0.2 & 0.1\\\\\n-1.0 & 0.0 & 3.0\\\\\n0.5 & 0.4 & 0.3\n\\end{bmatrix},\\quad\n\\boldsymbol{y}^{(1)} = \\begin{bmatrix}0\\\\1\\\\0\\\\2\\\\0\\end{bmatrix},\\quad\n\\boldsymbol{\\pi}^{(1)} = \\begin{bmatrix}0.7\\\\0.2\\\\0.1\\end{bmatrix}.\n$$\n\nTest case $2$ (uniform prior boundary; invariance check):\n$$\nZ^{(2)} = \\begin{bmatrix}\n2.5 & 1.0 & 0.0\\\\\n0.0 & 1.2 & 1.0\\\\\n1.2 & 0.2 & 0.1\\\\\n-1.0 & 0.0 & 3.0\\\\\n0.5 & 0.4 & 0.3\n\\end{bmatrix},\\quad\n\\boldsymbol{y}^{(2)} = \\begin{bmatrix}2\\\\2\\\\1\\\\2\\\\0\\end{bmatrix},\\quad\n\\boldsymbol{\\pi}^{(2)} = \\begin{bmatrix}\\tfrac{1}{3}\\\\\\tfrac{1}{3}\\\\\\tfrac{1}{3}\\end{bmatrix}.\n$$\n\nTest case $3$ (extreme imbalance; long-tail stress):\n$$\nZ^{(3)} = \\begin{bmatrix}\n4.0 & 3.9 & 5.5\\\\\n1.0 & 2.0 & 3.0\\\\\n3.0 & 0.0 & 0.5\\\\\n0.5 & 0.6 & 0.7\n\\end{bmatrix},\\quad\n\\boldsymbol{y}^{(3)} = \\begin{bmatrix}2\\\\2\\\\0\\\\1\\end{bmatrix},\\quad\n\\boldsymbol{\\pi}^{(3)} = \\begin{bmatrix}0.95\\\\0.04\\\\0.01\\end{bmatrix}.\n$$\n\nTest case $4$ (binary boundary; threshold sensitivity):\n$$\nZ^{(4)} = \\begin{bmatrix}\n2.0 & 1.8\\\\\n0.1 & 0.2\\\\\n-0.5 & -0.6\\\\\n1.0 & 2.0\\\\\n0.0 & 0.0\n\\end{bmatrix},\\quad\n\\boldsymbol{y}^{(4)} = \\begin{bmatrix}0\\\\1\\\\0\\\\1\\\\0\\end{bmatrix},\\quad\n\\boldsymbol{\\pi}^{(4)} = \\begin{bmatrix}0.9\\\\0.1\\end{bmatrix}.\n$$\n\nFinal output format:\nYour program should produce a single line of output containing all results concatenated in order as a comma-separated list enclosed in square brackets. For test cases $1$ through $4$, append in sequence the pair for each test case: first the accuracy difference $a$ and then the NLL difference $\\ell$, both rounded to six decimal places. For example, the output must have the form\n$$\n[\\;a_1,\\ell_1,a_2,\\ell_2,a_3,\\ell_3,a_4,\\ell_4\\;].\n$$", "solution": "We begin from Bayes’ rule, which states that for any input $x$ and class $k$,\n$$\np(y=k \\mid x) = \\frac{p(x \\mid y=k)\\,p(y=k)}{p(x)} \\propto p(x \\mid y=k)\\,p(y=k).\n$$\nIn many discriminative models used for multi-class classification, the model produces a logit vector $z(x) \\in \\mathbb{R}^K$. A canonical probabilistic interpretation of logits is that they parameterize a categorical distribution through the softmax function,\n$$\np_{k}(x) = \\frac{\\exp\\big(z_k(x)\\big)}{\\sum_{j=1}^{K} \\exp\\big(z_j(x)\\big)}.\n$$\nFor models trained without explicit prior correction, the logits $z_k(x)$ typically act like proportional surrogates for a class-conditional score (for example, as an affine proxy to $\\log p(x \\mid y=k)$) that omits the prior $p(y=k)$. To incorporate class priors, we must construct a transformed score $\\tilde{z}_k(x)$ whose exponentiation is proportional to the posterior. From Bayes’ rule,\n$$\np(y=k \\mid x) \\propto p(x \\mid y=k)\\,p(y=k).\n$$\nIf $z_k(x)$ is proportional to $\\log p(x \\mid y=k)$ up to an additive constant that does not depend on $x$, then adding $\\log p(y=k)$ inside the exponential brings the score into alignment with the posterior, that is,\n$$\n\\exp\\big(\\tilde{z}_k(x)\\big) \\propto \\exp\\big(z_k(x)\\big)\\,p(y=k),\n$$\nwhich implies the logit transformation\n$$\n\\tilde{z}_k(x) = z_k(x) + \\log p(y=k) + c,\n$$\nwhere $c$ is any constant not depending on $k$; the softmax normalization makes $c$ irrelevant. Therefore, the adjusted probabilities satisfying Bayes’ rule are obtained by applying softmax to $\\tilde{z}(x)$:\n$$\nq_{k}(x) = \\frac{\\exp\\big(z_k(x) + \\log \\pi_k\\big)}{\\sum_{j=1}^{K} \\exp\\big(z_j(x) + \\log \\pi_j\\big)},\n$$\nwhere $\\pi_k = p(y=k)$ denotes the class prior.\n\nWe now assess the effect of this logit adjustment on two properties.\n\nRanking: The top-$1$ decision rule predicts $\\arg\\max_k p_k(x)$ (unadjusted) or $\\arg\\max_k q_k(x)$ (adjusted). Adding the same constant to all logits leaves the argmax unchanged, but adding class-dependent constants $\\log \\pi_k$ can change the ordering among classes. A boundary case arises when the prior is uniform, namely $\\pi_k = \\tfrac{1}{K}$ for all $k$, which yields $\\log \\pi_k$ equal across $k$, so the argmax is identical to the unadjusted case. More generally, under imbalance, the adjusted argmax can shift toward frequent classes when the unadjusted scores do not already encode the priors; this can change top-$1$ accuracy depending on the data distribution and the adequacy of the original logits.\n\nCalibration: A common calibration metric is the mean negative log-likelihood (NLL),\n$$\n\\text{NLL} = -\\frac{1}{n} \\sum_{i=1}^{n} \\log r_{i,y_i},\n$$\nwhere $r_{i,k}$ denotes the predicted probability for class $k$ on example $i$, and $y_i$ is the true label. If the unadjusted logits approximately reflect class-conditional evidence without priors, then injecting $\\log \\pi_k$ aligns the model closer to the posterior distribution implied by Bayes’ rule, often improving calibration on imbalanced data. However, the effect on NLL depends on the quality of the logits and the correctness of the specified priors.\n\nAlgorithmic procedure for each test case:\n1. Compute unadjusted probabilities $p_{i,k}$ by applying a numerically stable softmax to each row of $Z$.\n2. Compute adjusted logits $\\tilde{z}_{i,k} = z_{i,k} + \\log \\pi_k$ and then adjusted probabilities $q_{i,k}$ via softmax.\n3. Compute top-$1$ accuracies:\n   $$\n   \\text{acc}_{\\text{unadj}} = \\frac{1}{n}\\sum_{i=1}^{n} \\mathbf{1}\\!\\left(\\arg\\max_k p_{i,k} = y_i\\right),\\quad\n   \\text{acc}_{\\text{adj}} = \\frac{1}{n}\\sum_{i=1}^{n} \\mathbf{1}\\!\\left(\\arg\\max_k q_{i,k} = y_i\\right).\n   $$\n4. Compute mean NLLs:\n   $$\n   \\text{NLL}_{\\text{unadj}} = -\\frac{1}{n}\\sum_{i=1}^{n} \\log p_{i,y_i},\\quad\n   \\text{NLL}_{\\text{adj}} = -\\frac{1}{n}\\sum_{i=1}^{n} \\log q_{i,y_i}.\n   $$\n5. Report the differences $a = \\text{acc}_{\\text{adj}} - \\text{acc}_{\\text{unadj}}$ and $\\ell = \\text{NLL}_{\\text{adj}} - \\text{NLL}_{\\text{unadj}}$ rounded to six decimal places.\n\nCoverage of the provided test suite:\n- Test case $1$ is a general imbalanced scenario where the priors favor one class; this tests typical long-tail behavior.\n- Test case $2$ uses uniform priors, a boundary under which ranking should be invariant and calibration unchanged up to numerical tolerance; this validates invariance to adding a class-independent constant.\n- Test case $3$ exhibits extreme imbalance; this stresses the adjustment and its effect on both ranking and calibration when a rare class receives large unadjusted logits.\n- Test case $4$ is binary and highlights how the prior induces an effective threshold shift in the log-odds, which can change the top-$1$ decision while also affecting NLL.\n\nThe program will compute the requested differences for all test cases and print them as a single list $[a_1,\\ell_1,a_2,\\ell_2,a_3,\\ell_3,a_4,\\ell_4]$, each value rounded to six decimal places and expressed as a decimal number without a percentage sign.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Execution environment: Python 3.12, numpy 1.23.5, scipy 1.11.4 (not used).\nimport numpy as np\n\ndef softmax_rows(logits: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Numerically stable softmax applied row-wise.\n    logits: shape (n, K)\n    returns: probabilities, shape (n, K)\n    \"\"\"\n    # Subtract row-wise max for numerical stability.\n    z = logits - np.max(logits, axis=1, keepdims=True)\n    np.exp(z, out=z)\n    z_sum = np.sum(z, axis=1, keepdims=True)\n    # Avoid division by zero in degenerate cases.\n    z_sum = np.where(z_sum == 0.0, 1.0, z_sum)\n    return z / z_sum\n\ndef top1_accuracy(probs: np.ndarray, y: np.ndarray) -> float:\n    preds = np.argmax(probs, axis=1)\n    return float(np.mean(preds == y))\n\ndef mean_nll(probs: np.ndarray, y: np.ndarray) -> float:\n    # Clip to avoid log(0).\n    eps = 1e-15\n    p_true = probs[np.arange(probs.shape[0]), y]\n    p_true = np.clip(p_true, eps, 1.0)\n    return float(-np.mean(np.log(p_true)))\n\ndef logit_adjusted_probs(logits: np.ndarray, pi: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Apply Bayes-inspired logit adjustment: add log(pi_k) to class k logits.\n    \"\"\"\n    # Ensure pi is a proper distribution and strictly positive.\n    pi = np.asarray(pi, dtype=float)\n    pi_sum = np.sum(pi)\n    if pi_sum <= 0.0:\n        raise ValueError(\"Class prior vector must have positive sum.\")\n    pi = pi / pi_sum\n    if np.any(pi <= 0.0):\n        raise ValueError(\"Class priors must be strictly positive.\")\n    adjusted = logits + np.log(pi)[None, :]\n    return softmax_rows(adjusted)\n\ndef evaluate_case(Z: np.ndarray, y: np.ndarray, pi: np.ndarray):\n    # Unadjusted probabilities\n    p = softmax_rows(Z)\n    # Adjusted probabilities\n    q = logit_adjusted_probs(Z, pi)\n    # Metrics\n    acc_unadj = top1_accuracy(p, y)\n    acc_adj = top1_accuracy(q, y)\n    nll_unadj = mean_nll(p, y)\n    nll_adj = mean_nll(q, y)\n    # Differences\n    a = acc_adj - acc_unadj\n    l = nll_adj - nll_unadj\n    return a, l\n\ndef solve():\n    # Define the test cases from the problem statement.\n\n    # Test case 1\n    Z1 = np.array([\n        [2.5, 1.0, 0.0],\n        [0.0, 1.2, 1.0],\n        [1.2, 0.2, 0.1],\n        [-1.0, 0.0, 3.0],\n        [0.5, 0.4, 0.3]\n    ], dtype=float)\n    y1 = np.array([0, 1, 0, 2, 0], dtype=int)\n    pi1 = np.array([0.7, 0.2, 0.1], dtype=float)\n\n    # Test case 2 (uniform prior)\n    Z2 = np.array([\n        [2.5, 1.0, 0.0],\n        [0.0, 1.2, 1.0],\n        [1.2, 0.2, 0.1],\n        [-1.0, 0.0, 3.0],\n        [0.5, 0.4, 0.3]\n    ], dtype=float)\n    y2 = np.array([2, 2, 1, 2, 0], dtype=int)\n    pi2 = np.array([1/3, 1/3, 1/3], dtype=float)\n\n    # Test case 3 (extreme imbalance)\n    Z3 = np.array([\n        [4.0, 3.9, 5.5],\n        [1.0, 2.0, 3.0],\n        [3.0, 0.0, 0.5],\n        [0.5, 0.6, 0.7]\n    ], dtype=float)\n    y3 = np.array([2, 2, 0, 1], dtype=int)\n    pi3 = np.array([0.95, 0.04, 0.01], dtype=float)\n\n    # Test case 4 (binary)\n    Z4 = np.array([\n        [2.0, 1.8],\n        [0.1, 0.2],\n        [-0.5, -0.6],\n        [1.0, 2.0],\n        [0.0, 0.0]\n    ], dtype=float)\n    y4 = np.array([0, 1, 0, 1, 0], dtype=int)\n    pi4 = np.array([0.9, 0.1], dtype=float)\n\n    test_cases = [\n        (Z1, y1, pi1),\n        (Z2, y2, pi2),\n        (Z3, y3, pi3),\n        (Z4, y4, pi4),\n    ]\n\n    results = []\n    for Z, y, pi in test_cases:\n        a, l = evaluate_case(Z, y, pi)\n        # Round to six decimals as required\n        results.append(round(a + 0.0, 6))\n        results.append(round(l + 0.0, 6))\n\n    # Final print statement in the exact required format.\n    # Ensure each number has up to six decimal places formatting.\n    formatted = []\n    for val in results:\n        # Format with exactly six decimal places\n        formatted.append(f\"{val:.6f}\")\n    print(f\"[{','.join(formatted)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3127123"}]}