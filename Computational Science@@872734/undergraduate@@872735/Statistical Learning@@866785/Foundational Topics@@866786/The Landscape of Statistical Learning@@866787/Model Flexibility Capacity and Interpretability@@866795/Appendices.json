{"hands_on_practices": [{"introduction": "Understanding model capacity begins with being able to quantify it. This exercise provides a hands-on method for calculating the \"size\" of a model's feature space when using polynomial kernels, a common technique for increasing model flexibility. By deriving the number of features from first principles and relating it to a theoretical generalization bound, you will gain a concrete understanding of how rapidly a model's capacity can expand and why this often leads to the classic problem of overfitting, where a model performs well on training data but poorly on unseen test data [@problem_id:3148660].", "problem": "A team is comparing polynomial-kernel models for a regression task with $p=20$ input variables and $n=500$ independent and identically distributed samples. The polynomial kernel corresponds to a linear model in a feature space consisting of all monomials in the original inputs up to total degree $d$. The team observes that both degree $d=2$ and degree $d=4$ models achieve essentially zero training error, while the test mean squared error is approximately $0.08$ for $d=2$ and $0.20$ for $d=4$.\n\nStarting from first principles in counting and statistical learning, do the following.\n\n1) Using only the definition of a monomial as a product $x_{1}^{\\alpha_{1}} x_{2}^{\\alpha_{2}} \\cdots x_{p}^{\\alpha_{p}}$ with nonnegative integer exponents $\\alpha_{j}$ whose total degree satisfies $\\sum_{j=1}^{p} \\alpha_{j} \\le d$, derive the closed-form expression for the number of distinct monomials in $p$ variables of total degree at most $d$. Your derivation must not assume any pre-existing counting formula and should proceed from a valid combinatorial argument.\n\n2) Evaluate your expression from part (1) at $p=20$ for $d=2$ and $d=4$.\n\n3) It is a well-tested fact in statistical learning theory that for linear predictors in an $M$-dimensional bounded feature space with norm constraints, data-independent generalization bounds based on complexity measures such as Rademacher complexity scale proportionally to $\\sqrt{M/n}$. Using this scaling as a proxy, compute the predicted ratio $r$ of the generalization gaps (degree $d=4$ divided by degree $d=2$) for this problem. Express $r$ as a unitless number and round your answer to three significant figures.\n\n4) Briefly explain, using your counts from part (2), how the change in feature dimensionality relates to the observed increase in test error and what this implies about model flexibility, capacity, and interpretability.\n\nOnly the numerical value of $r$ is to be submitted as the final answer.", "solution": "The problem is assessed to be valid as it is scientifically grounded in statistical learning theory, well-posed, objective, and contains all necessary information for a solution.\n\nThe problem asks for four tasks: 1) derive a formula for the number of monomials, 2) evaluate this formula for specific parameters, 3) compute a ratio of generalization gaps based on a given scaling law, and 4) provide a conceptual explanation of the results. The final answer is specified to be only the numerical value from part (3).\n\n**Part 1: Derivation of the Number of Monomials**\n\nWe are asked to find the number of distinct monomials in $p$ variables, $x_1, x_2, \\dots, x_p$, of total degree at most $d$. A monomial is of the form $x_{1}^{\\alpha_{1}} x_{2}^{\\alpha_{2}} \\cdots x_{p}^{\\alpha_{p}}$, where the exponents $\\alpha_j$ are non-negative integers. The condition is that the total degree, $\\sum_{j=1}^{p} \\alpha_{j}$, is less than or equal to $d$.\n$$ \\sum_{j=1}^{p} \\alpha_{j} \\le d, \\quad \\alpha_j \\in \\{0, 1, 2, \\dots\\} $$\nThis problem is equivalent to counting the number of non-negative integer solutions to the inequality above. To transform this inequality into a more manageable equality, we introduce a non-negative integer \"slack\" variable, $\\alpha_{p+1}$. We define $\\alpha_{p+1}$ as:\n$$ \\alpha_{p+1} = d - \\sum_{j=1}^{p} \\alpha_{j} $$\nSince $\\sum_{j=1}^{p} \\alpha_{j} \\le d$, it is guaranteed that $\\alpha_{p+1} \\ge 0$. The original inequality is now perfectly equivalent to finding the number of non-negative integer solutions to the equation:\n$$ \\alpha_1 + \\alpha_2 + \\cdots + \\alpha_p + \\alpha_{p+1} = d $$\nThis is a classic combinatorial problem that can be solved using a \"stars and bars\" argument, as required. Imagine we have $d$ identical items (stars, $\\star$) to be distributed among $p+1$ distinct bins (the variables $\\alpha_1, \\dots, \\alpha_{p+1}$). We can represent this arrangement by placing the $d$ stars in a row and using $p$ bars ($|$) to partition them into $p+1$ groups. The number of stars in the $j$-th group corresponds to the value of $\\alpha_j$.\n\nFor example, with $d=3$ stars and $p=2$ variables (meaning $p+1=3$ bins), the arrangement $\\star|\\star\\star|$ corresponds to $\\alpha_1=1$, $\\alpha_2=2$, and the slack $\\alpha_3=0$. This corresponds to the monomial $x_1^1 x_2^2$ of degree $3$. The arrangement $| \\star | \\star\\star$ corresponds to $\\alpha_1=0$, $\\alpha_2=1$, and $\\alpha_3=2$. This implies $\\alpha_1+\\alpha_2 = 1 \\le 3$, representing the monomial $x_2^1$ of degree $1$.\n\nTo count the number of such arrangements, we consider a sequence of $d+p$ total positions. We need to choose $p$ of these positions to place the bars (the remaining $d$ positions will be filled by stars). The number of ways to do this is given by the binomial coefficient \"($d+p$) choose $p$\".\n\nLet $M(p, d)$ be the number of monomials. The number of ways to choose $p$ positions for the bars from $d+p$ total positions is:\n$$ M(p, d) = \\binom{p+d}{p} $$\nAlternatively, we could choose $d$ positions for the stars, which gives the equivalent expression:\n$$ M(p, d) = \\binom{p+d}{d} = \\frac{(p+d)!}{d! p!} $$\nThis is the closed-form expression for the number of distinct monomials in $p$ variables of total degree at most $d$. This count includes the constant term (the monomial of degree $0$), which corresponds to the solution $\\alpha_1 = \\alpha_2 = \\dots = \\alpha_p = 0$.\n\n**Part 2: Evaluation of the Expression**\n\nWe are given $p=20$ variables and are asked to evaluate the number of features for models of degree $d=2$ and $d=4$.\n\nFor $d=2$, let the number of features be $M_2$.\n$$ M_2 = M(20, 2) = \\binom{20+2}{2} = \\binom{22}{2} $$\n$$ M_2 = \\frac{22!}{2!(22-2)!} = \\frac{22!}{2!20!} = \\frac{22 \\times 21}{2 \\times 1} = 11 \\times 21 = 231 $$\n\nFor $d=4$, let the number of features be $M_4$.\n$$ M_4 = M(20, 4) = \\binom{20+4}{4} = \\binom{24}{4} $$\n$$ M_4 = \\frac{24!}{4!(24-4)!} = \\frac{24!}{4!20!} = \\frac{24 \\times 23 \\times 22 \\times 21}{4 \\times 3 \\times 2 \\times 1} $$\nSince $4 \\times 3 \\times 2 \\times 1 = 24$, we can simplify:\n$$ M_4 = 23 \\times 22 \\times 21 = 506 \\times 21 = 10626 $$\nSo, the number of features is $M_2 = 231$ for the degree-2 model and $M_4 = 10626$ for the degree-4 model.\n\n**Part 3: Computation of the Ratio of Generalization Gaps**\n\nWe are told that for a linear predictor in an $M$-dimensional feature space, a typical data-independent generalization bound scales proportionally to $\\sqrt{M/n}$. The generalization gap, $G$, is the difference between the test error and the training error. We are asked to use this scaling as a proxy.\nLet $G_d$ be the generalization gap for a model of degree $d$.\n$$ G_d \\propto \\sqrt{\\frac{M_d}{n}} $$\nThis implies $G_d = C \\sqrt{M_d/n}$ for some constant of proportionality $C$ that depends on factors not relevant to this ratio, such as norm constraints on the weights and properties of the data distribution.\n\nThe predicted ratio, $r$, of the generalization gaps is:\n$$ r = \\frac{G_4}{G_2} = \\frac{C \\sqrt{M_4/n}}{C \\sqrt{M_2/n}} $$\nThe constant $C$ and the sample size $n$ cancel out:\n$$ r = \\sqrt{\\frac{M_4}{M_2}} $$\nUsing the values calculated in Part 2:\n$$ r = \\sqrt{\\frac{10626}{231}} $$\nFirst, we simplify the fraction:\n$$ \\frac{10626}{231} = 46 $$\nTherefore, the ratio $r$ is:\n$$ r = \\sqrt{46} $$\nTo express this as a number rounded to three significant figures:\n$$ r \\approx 6.7823... $$\nRounding to three significant figures gives $r \\approx 6.78$.\n\n**Part 4: Explanation of the Observed Increase in Test Error**\n\nThe number of features, $M$, is a direct measure of a model's capacity or flexibility. A model with a larger $M$ can represent a more complex class of functions.\nIn this problem, we have $p=20$ inputs and $n=500$ samples.\n- The degree $d=2$ model has $M_2=231$ features. Since $M_2  n$, this is a regime where a good fit is possible but not guaranteed to be perfect interpolation.\n- The degree $d=4$ model has $M_4=10626$ features. Here, $M_4 \\gg n$, placing the model in a heavily over-parameterized regime.\n\nThe observation that both models achieve \"essentially zero training error\" is explained by their high capacity. With $231$ features for $500$ data points, the $d=2$ model is already very flexible. The $d=4$ model, being in the over-parameterized regime ($M_4 > n$), has more parameters than data points, which generally allows it to interpolate the training data, thus driving the training error to zero.\n\nThe test error, however, is a measure of how well the model generalizes to new, unseen data. It is influenced by the bias-variance tradeoff.\n- **Flexibility and Capacity**: Moving from $d=2$ to $d=4$ increases the feature space dimensionality from $M_2=231$ to $M_4=10626$, a dramatic increase in model capacity.\n- **Overfitting and Variance**: While this increased capacity helps to reduce the training error (and potentially model bias), it comes at the cost of much higher variance. The $d=4$ model, with its vast number of features, is not only fitting the underlying signal in the training data but is also fitting the random noise specific to that sample. This is overfitting. The model becomes highly sensitive to the particularities of the training set.\n- **Test Error**: The increase in test MSE from $0.08$ for $d=2$ to $0.20$ for $d=4$ is a classic symptom of overfitting. The increased variance of the $d=4$ model dominates any potential reduction in bias, leading to poorer generalization performance. The $d=2$ model represents a better balance between bias and variance for this specific problem and dataset size.\n- **Interpretability**: Interpretability is inversely related to model complexity. A linear model on $M_2=231$ features is already very difficult to interpret. A model on $M_4=10626$ features, which involve complex products of up to four original variables, is for all practical purposes a \"black box.\" It is impossible for a human to analyze the individual effects of tens of thousands of coefficients to understand the model's decision process. The simpler $d=2$ model is comparatively more interpretable.\n\nIn summary, the transition from $d=2$ to $d=4$ massively increases model capacity, leading to severe overfitting, which manifests as an increase in test error due to high variance, and a stark decrease in model interpretability.", "answer": "$$\n\\boxed{6.78}\n$$", "id": "3148660"}, {"introduction": "After seeing how unconstrained flexibility can harm generalization, the next logical step is to find a principled way to choose the right level of complexity. This practice introduces Structural Risk Minimization (SRM), a cornerstone of statistical learning theory that formalizes the bias-variance tradeoff. You will implement the SRM selection rule, which involves choosing a model that minimizes an upper bound on its true error, balancing the observed training error with a penalty for the model's complexity (its VC dimension). This exercise moves from the problem of overfitting to a powerful theoretical solution for model selection [@problem_id:3148607].", "problem": "You are given a series of nested binary classification hypothesis classes $\\mathcal{H}_1 \\subset \\cdots \\subset \\mathcal{H}_k$ with zero-one loss, a sample size $n$, and measured empirical risks $\\hat{R}_i$ for the empirical risk minimizers within each class $\\mathcal{H}_i$. The objective is to implement Structural Risk Minimization (SRM) by selecting an index $i^\\star \\in \\{1,\\ldots,k\\}$ that minimizes a uniform generalization bound on the true risk. The selection rule should be based on a uniform convergence bound with Vapnik–Chervonenkis (VC) dimension regularization and a structural confidence allocation. Your program must implement the following selection principle:\n- For each class $\\mathcal{H}_i$ with VC dimension $d_i$, define the allocated confidence level $\\delta_i$ by $\\delta_i = \\delta / 2^i$, where $\\delta \\in (0,1)$ is a user-specified total confidence level.\n- For each $i \\in \\{1,\\ldots,k\\}$, compute the bound\n$$\nB_i \\;=\\; \\hat{R}_i \\;+\\; \\Phi(d_i,n,\\delta_i),\n$$\nwhere\n$$\n\\Phi(d,n,\\delta)\\;=\\;\\sqrt{\\frac{2}{n}\\left(d \\ln\\!\\left(\\frac{2 e n}{\\max\\{d,1\\}}\\right) + \\ln\\!\\left(\\frac{4}{\\delta}\\right)\\right)}.\n$$\nHere $e$ denotes Euler’s number and $\\ln(\\cdot)$ is the natural logarithm. The term $d \\ln\\!\\left(\\frac{2 e n}{\\max\\{d,1\\}}\\right)$ implements the standard growth-function bound from Sauer–Shelah for the Vapnik–Chervonenkis (VC) dimension, with the convention that it vanishes when $d=0$ because of the $\\max\\{d,1\\}$ in the denominator. The selection rule is\n$$\ni^\\star \\in \\arg\\min_{1 \\le i \\le k} \\; B_i.\n$$\nIf there are ties, break ties by choosing the smallest index $i$.\n\nYour task:\n- Implement the above SRM selection and return the chosen index $i^\\star$ for each test case below.\n- Use the exact formulas specified above.\n- Angles are not involved. There are no physical units.\n- The final output must be a single line containing the chosen indices for all test cases, as a comma-separated list enclosed in square brackets, with no spaces, in the order of the test suite. For example, a valid output format looks like $[2,1,3]$.\n\nTest suite:\nFor each test case, you are given the tuple $(n,\\delta,\\mathbf{d},\\hat{\\mathbf{R}})$ where $\\mathbf{d} = [d_1,\\ldots,d_k]$ and $\\hat{\\mathbf{R}} = [\\hat{R}_1,\\ldots,\\hat{R}_k]$.\n\n- Test case $1$: $(n=\\;200,\\;\\delta=\\;0.05,\\;\\mathbf{d}=\\;[1,3,5,10],\\;\\hat{\\mathbf{R}}=\\;[0.08,0.06,0.05,0.048])$.\n- Test case $2$: $(n=\\;20000,\\;\\delta=\\;0.05,\\;\\mathbf{d}=\\;[1,3,5,10],\\;\\hat{\\mathbf{R}}=\\;[0.08,0.06,0.05,0.03])$.\n- Test case $3$: $(n=\\;400,\\;\\delta=\\;0.1,\\;\\mathbf{d}=\\;[0,2,4],\\;\\hat{\\mathbf{R}}=\\;[0.15,0.10,0.09])$.\n- Test case $4$: $(n=\\;30,\\;\\delta=\\;0.05,\\;\\mathbf{d}=\\;[1,2,3],\\;\\hat{\\mathbf{R}}=\\;[0.20,0.15,0.12])$.\n- Test case $5$: $(n=\\;5000,\\;\\delta=\\;0.05,\\;\\mathbf{d}=\\;[1,10,50,200],\\;\\hat{\\mathbf{R}}=\\;[0.04,0.02,0.0,0.0])$.\n\nYour program should produce a single line of output containing the results as a comma-separated list of the selected indices in order, enclosed in square brackets, with no spaces, exactly in the form $[i_1,i_2,i_3,i_4,i_5]$.", "solution": "The user wants to implement the Structural Risk Minimization (SRM) principle to select a model from a nested set of hypothesis classes.\n\n### Step 1: Extract Givens\n\nThe problem provides the following definitions and data:\n- A series of $k$ nested hypothesis classes: $\\mathcal{H}_1 \\subset \\cdots \\subset \\mathcal{H}_k$.\n- A sample size $n$.\n- A total confidence level $\\delta \\in (0,1)$.\n- A vector of VC dimensions $\\mathbf{d} = [d_1, \\ldots, d_k]$ for each class $\\mathcal{H}_i$.\n- A vector of empirical risks $\\hat{\\mathbf{R}} = [\\hat{R}_1, \\ldots, \\hat{R}_k]$ for the empirical risk minimizers in each class $\\mathcal{H}_i$.\n\nThe selection rule is to choose the index $i^\\star$ that minimizes an upper bound on the true risk, $B_i$.\nThe bound for each class $\\mathcal{H}_i$ is given by:\n$$\nB_i = \\hat{R}_i + \\Phi(d_i, n, \\delta_i)\n$$\nwhere the allocated confidence level $\\delta_i$ for class $\\mathcal{H}_i$ is:\n$$\n\\delta_i = \\frac{\\delta}{2^i}\n$$\nand the penalty term $\\Phi(d, n, \\delta)$ is defined as:\n$$\n\\Phi(d,n,\\delta) = \\sqrt{\\frac{2}{n}\\left(d \\ln\\!\\left(\\frac{2 e n}{\\max\\{d,1\\}}\\right) + \\ln\\!\\left(\\frac{4}{\\delta}\\right)\\right)}\n$$\nThe value $e$ is Euler's number, and $\\ln(\\cdot)$ is the natural logarithm. The term dependent on the growth function, $d \\ln(\\dots)$, is taken to be $0$ when $d=0$.\n\nThe selection rule is:\n$$\ni^\\star \\in \\arg\\min_{1 \\le i \\le k} B_i\n$$\nTies are broken by selecting the smallest index $i$.\n\nThe provided test suite is:\n- Test case $1$: $(n=200, \\delta=0.05, \\mathbf{d}=[1,3,5,10], \\hat{\\mathbf{R}}=[0.08,0.06,0.05,0.048])$\n- Test case $2$: $(n=20000, \\delta=0.05, \\mathbf{d}=[1,3,5,10], \\hat{\\mathbf{R}}=[0.08,0.06,0.05,0.03])$\n- Test case $3$: $(n=400, \\delta=0.1, \\mathbf{d}=[0,2,4], \\hat{\\mathbf{R}}=[0.15,0.10,0.09])$\n- Test case $4$: $(n=30, \\delta=0.05, \\mathbf{d}=[1,2,3], \\hat{\\mathbf{R}}=[0.20,0.15,0.12])$\n- Test case $5$: $(n=5000, \\delta=0.05, \\mathbf{d}=[1,10,50,200], \\hat{\\mathbf{R}}=[0.04,0.02,0.0,0.0])$\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem is subjected to validation according to the specified criteria.\n- **Scientifically Grounded:** The problem is based on the well-established Structural Risk Minimization (SRM) principle from statistical learning theory. The provided formula for the generalization bound is a standard Vapnik–Chervonenkis (VC) bound, incorporating a penalty for model complexity (via VC dimension $d$) and a structural confidence allocation ($\\delta_i$) to handle the uniform convergence over multiple hypothesis classes. The structure of the formula is consistent with known theoretical results. The handling of the $d=0$ case is explicitly and correctly defined. The problem is scientifically sound.\n- **Well-Posed:** All necessary inputs ($n, \\delta, \\mathbf{d}, \\hat{\\mathbf{R}}$) are provided for each test case. The objective function $B_i$ is unambiguously defined. The minimization criterion, combined with the tie-breaking rule (select the smallest index), ensures a unique solution exists for each test case. The problem is well-posed.\n- **Objective:** The problem is formulated using precise mathematical expressions and objective data. There are no subjective or ambiguous statements.\n- **Completeness and Consistency:** The problem is self-contained. All variables and formulas required for the solution are explicitly given. The premise of nested hypothesis classes implies that true risk minimizers within these classes will have non-increasing empirical risks ($\\hat{R}_1 \\ge \\hat{R}_2 \\ge \\dots$), a property that holds for all test cases. The setup is complete and internally consistent.\n\nThe problem does not exhibit any of the invalidating flaws.\n\n### Step 3: Verdict and Action\n\nThe problem is valid. A reasoned solution will be provided.\n\n### Solution\n\nThe objective is to find the index $i^\\star$ that minimizes the risk bound $B_i$ for each given test case. The overall logic is to iterate through each hypothesis class $\\mathcal{H}_i$ for $i=1, \\ldots, k$, calculate the corresponding bound $B_i$, and then identify the index $i$ that yields the minimum value of $B_i$.\n\nFor a given test case $(n, \\delta, \\mathbf{d}, \\hat{\\mathbf{R}})$, let $k$ be the number of classes, which is the length of the vector $\\mathbf{d}$. We perform the following calculation for each class index $i$ from $1$ to $k$:\n\n1.  **Identify Parameters**: For each class $i$, we use the corresponding VC dimension $d_i$ and empirical risk $\\hat{R}_i$.\n2.  **Calculate Allocated Confidence**: The confidence level for class $i$, $\\delta_i$, is calculated as $\\delta_i = \\delta / 2^i$. This allocation ensures that the total probability of error across all classes is bounded by $\\sum_{i=1}^k \\delta_i  \\sum_{i=1}^\\infty \\delta/2^i = \\delta$.\n3.  **Calculate Penalty Term $\\Phi_i$**: The complexity penalty $\\Phi_i = \\Phi(d_i, n, \\delta_i)$ is computed using the given formula:\n    $$\n    \\Phi_i = \\sqrt{\\frac{2}{n}\\left(d_i \\ln\\!\\left(\\frac{2 e n}{\\max\\{d_i,1\\}}\\right) + \\ln\\!\\left(\\frac{4}{\\delta_i}\\right)\\right)}\n    $$\n    The term $d_i \\ln(\\dots)$ correctly evaluates to $0$ if $d_i=0$, as specified. The `max` function prevents division by zero.\n4.  **Calculate Risk Bound $B_i$**: The total risk bound is the sum of the empirical risk and the penalty term:\n    $$\n    B_i = \\hat{R}_i + \\Phi_i\n    $$\n5.  **Select Optimal Index $i^\\star$**: After calculating all $k$ bounds $[B_1, B_2, \\ldots, B_k]$, we find the index $i^\\star$ that corresponds to the minimum bound. The problem specifies that in case of a tie, the smallest index $i$ should be chosen. This is naturally handled by finding the first occurrence of the minimum value in the sequence of bounds.\n\nThis procedure is applied to each test case in the suite. The results, a list of the determined indices $i^\\star$, are then formatted into the required output string.\n\nFor example, let's analyze Test Case 1: $(n=200, \\delta=0.05, \\mathbf{d}=[1,3,5,10], \\hat{\\mathbf{R}}=[0.08,0.06,0.05,0.048])$.\n-   For $i=1$: $d_1=1$, $\\hat{R}_1=0.08$, $\\delta_1 = 0.05/2^1 = 0.025$.\n    $B_1 = 0.08 + \\sqrt{\\frac{2}{200}\\left(1 \\cdot \\ln(\\frac{2e \\cdot 200}{1}) + \\ln(\\frac{4}{0.025})\\right)} \\approx 0.08 + 0.34737 = 0.42737$.\n-   For $i=2$: $d_2=3$, $\\hat{R}_2=0.06$, $\\delta_2 = 0.05/2^2 = 0.0125$.\n    $B_2 = 0.06 + \\sqrt{\\frac{2}{200}\\left(3 \\cdot \\ln(\\frac{2e \\cdot 200}{3}) + \\ln(\\frac{4}{0.0125})\\right)} \\approx 0.06 + 0.48422 = 0.54422$.\n-   For $i=3$: $d_3=5$, $\\hat{R}_3=0.05$, $\\delta_3 = 0.05/2^3 = 0.00625$.\n    $B_3 = 0.05 + \\sqrt{\\frac{2}{200}\\left(5 \\cdot \\ln(\\frac{2e \\cdot 200}{5}) + \\ln(\\frac{4}{0.00625})\\right)} \\approx 0.05 + 0.57768 = 0.62768$.\n-   For $i=4$: $d_4=10$, $\\hat{R}_4=0.048$, $\\delta_4 = 0.05/2^4 = 0.003125$.\n    $B_4 = 0.048 + \\sqrt{\\frac{2}{200}\\left(10 \\cdot \\ln(\\frac{2e \\cdot 200}{10}) + \\ln(\\frac{4}{0.003125})\\right)} \\approx 0.048 + 0.73515 = 0.78315$.\n\nComparing the bounds $[0.42737, 0.54422, 0.62768, 0.78315]$, the minimum is $B_1$. Therefore, for this case, $i^\\star=1$. The same logic is applied to all other test cases.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the Structural Risk Minimization (SRM) selection principle for a series of nested\n    hypothesis classes, based on a VC dimension generalization bound.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, delta, d_list, R_hat_list)\n        (200, 0.05, [1, 3, 5, 10], [0.08, 0.06, 0.05, 0.048]),\n        (20000, 0.05, [1, 3, 5, 10], [0.08, 0.06, 0.05, 0.03]),\n        (400, 0.1, [0, 2, 4], [0.15, 0.10, 0.09]),\n        (30, 0.05, [1, 2, 3], [0.20, 0.15, 0.12]),\n        (5000, 0.05, [1, 10, 50, 200], [0.04, 0.02, 0.0, 0.0]),\n    ]\n\n    results = []\n    for case in test_cases:\n        n, delta, d_list, R_hat_list = case\n        k = len(d_list)\n        bounds = []\n\n        for i in range(k):\n            # The class index is 1-based (i+1) for the formulas.\n            class_index = i + 1\n            d_i = d_list[i]\n            R_hat_i = R_hat_list[i]\n            \n            # Calculate the allocated confidence level for the current class.\n            delta_i = delta / (2**class_index)\n\n            # Calculate the penalty term Phi(d, n, delta) using the provided formula.\n            # growth_term: d*ln(2*e*n / max(d,1))\n            # The formula is robust: if d_i is 0, the term becomes 0.\n            growth_term = d_i * np.log((2 * np.e * n) / max(d_i, 1.0))\n            \n            # confidence_term: ln(4/delta)\n            confidence_term = np.log(4 / delta_i)\n            \n            # Combine terms to get the full penalty.\n            penalty = np.sqrt((2 / n) * (growth_term + confidence_term))\n\n            # Calculate the risk bound B_i = R_hat_i + Phi_i.\n            bound_i = R_hat_i + penalty\n            bounds.append(bound_i)\n\n        # Find the 1-based index that minimizes the bound.\n        # np.argmin returns the index of the first minimum, which respects the tie-breaking rule.\n        i_star = np.argmin(bounds) + 1\n        results.append(i_star)\n\n    # Format the final output as a comma-separated list in brackets.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3148607"}, {"introduction": "Beyond selecting an optimal model, we often need to interpret its behavior to gain scientific insight. This practice explores the critical relationship between model flexibility, regularization, and interpretability. By implementing a logistic regression solver and varying the strength of an $L_2$ penalty, you will observe how regularization affects the stability of feature coefficients across different data scenarios. This exercise demonstrates that controlling flexibility is not just about improving test error; it is also essential for building robust models whose interpretations we can trust, especially in the presence of noise or correlated features [@problem_id:3148628].", "problem": "You are given the task of analyzing how the signs of logistic regression coefficients change as the regularization strength varies, and to connect this behavior to the model’s flexibility, capacity, and interpretability. Consider binary classification with logistic regression under a squared Euclidean norm (also called L2) penalty within Empirical Risk Minimization (ERM). The model parameters are a weight vector $w \\in \\mathbb{R}^d$ and an intercept $b \\in \\mathbb{R}$, with predictions $p_i = \\sigma(z_i)$ where $z_i = w^\\top x_i + b$ and $\\sigma(t) = \\frac{1}{1 + e^{-t}}$. For a dataset $\\{(x_i, y_i)\\}_{i=1}^n$ with $y_i \\in \\{0,1\\}$, the regularized objective for a given regularization parameter $\\lambda \\ge 0$ is\n$$\nJ_\\lambda(w,b) = \\frac{1}{n} \\sum_{i=1}^n \\left( \\log\\left(1 + e^{z_i}\\right) - y_i z_i \\right) + \\frac{\\lambda}{2} \\|w\\|_2^2,\n$$\nwhere $\\|w\\|_2$ is the Euclidean norm, and the intercept $b$ is not penalized.\n\nDefinitions to be used:\n- The sign of a coefficient with a tolerance $\\varepsilon > 0$ is defined by the function $S_\\varepsilon:\\mathbb{R}\\to\\{-1,0,1\\}$,\n$$\nS_\\varepsilon(u) =\n\\begin{cases}\n-1,  \\text{if } u  -\\varepsilon, \\\\\n0,  \\text{if } |u| \\le \\varepsilon, \\\\\n1,  \\text{if } u  \\varepsilon.\n\\end{cases}\n$$\n- Given a grid of regularization parameters $\\Lambda = \\{\\lambda_1, \\lambda_2, \\dots, \\lambda_m\\}$ ordered increasingly, for feature index $j \\in \\{1, \\dots, d\\}$, the sign-change count across the path is\n$$\nC_j = \\sum_{k=1}^{m-1} \\mathbf{1}\\left\\{ S_\\varepsilon\\big(w_j(\\lambda_k)\\big) \\ne S_\\varepsilon\\big(w_j(\\lambda_{k+1})\\big) \\right\\},\n$$\nwhere $\\mathbf{1}\\{\\cdot\\}$ denotes the indicator function, and $w_j(\\lambda)$ is the coefficient for feature $j$ fitted at regularization $\\lambda$.\n- The per-feature stability over the path is the fraction of adjacent pairs without a sign change:\n$$\n\\text{stab}_j = \\frac{1}{m-1} \\sum_{k=1}^{m-1} \\mathbf{1}\\left\\{ S_\\varepsilon\\big(w_j(\\lambda_k)\\big) = S_\\varepsilon\\big(w_j(\\lambda_{k+1})\\big) \\right\\}.\n$$\n- The overall stability index is the average stability across features:\n$$\n\\text{Stability} = \\frac{1}{d} \\sum_{j=1}^d \\text{stab}_j.\n$$\n\nStarting from the fundamental definitions of the logistic model, its loss, and the regularization penalty, derive and implement a solver for the regularized objective $J_\\lambda(w,b)$ that fits $(w(\\lambda), b(\\lambda))$ for each $\\lambda \\in \\Lambda$ using gradient-based optimization. Then, compute $C_j$ for each feature $j$ and the overall Stability for each test case described below.\n\nThe regularization grid is fixed as $\\Lambda = \\{0.001, 0.01, 0.1, 1.0, 10.0, 100.0\\}$, the tolerance is $\\varepsilon = 10^{-6}$, and the intercept $b$ must not be penalized.\n\nTest suite (three cases) to ensure coverage:\n- Case A (general, clear signal): Generate data with seed $0$. Let $n=120$, $d=3$. Draw $X \\in \\mathbb{R}^{n \\times d}$ with entries independently from a standard normal distribution. Define the ground-truth parameters $w^* = (3, -2, 0)$ and $b^* = 0$. For each $i$, compute $z_i^* = x_i^\\top w^* + b^*$ and set $y_i = 1$ if $z_i^* \\ge 0$ and $y_i = 0$ otherwise.\n- Case B (high collinearity, ambiguous attribution): Generate data with seed $1$. Let $n=100$, $d=3$. Draw $x^{(1)} \\in \\mathbb{R}^n$ from a standard normal distribution. Create $x^{(2)} = x^{(1)} + 0.01 \\cdot \\xi$ where $\\xi$ is standard normal with the same seed progression. Draw $x^{(3)}$ independently from a standard normal distribution. Form $X$ by stacking $(x^{(1)}, x^{(2)}, x^{(3)})$. Define $w^* = (1, -1, 0)$, $b^* = 0$. For each $i$, set $y_i = 1$ if $x_i^{(1)} - x_i^{(2)} \\ge 0$ and $y_i = 0$ otherwise.\n- Case C (low signal-to-noise, regularization-dominated): Generate data with seed $2$. Let $n=200$, $d=3$. Draw $X$ as standard normal. Define $w^* = (0.2, -0.2, 0)$ and $b^* = 0$. Draw independent noise $\\eta \\in \\mathbb{R}^n$ from a standard normal distribution with the same seed. Compute $z_i^* = x_i^\\top w^* + b^* + 0.1 \\eta_i$ and set $y_i = 1$ if $z_i^* \\ge 0$ and $y_i = 0$ otherwise.\n\nProgram requirements:\n- Fit $(w(\\lambda), b(\\lambda))$ for each $\\lambda \\in \\Lambda$ by minimizing $J_\\lambda(w,b)$ using a gradient-based optimizer with an exact gradient; ensure the intercept $b$ is not penalized.\n- Compute, for each test case, the list $[C_1, C_2, C_3]$ of sign-change counts across $\\Lambda$ for the $d=3$ features, and the overall Stability as defined above, expressed as a decimal rounded to three digits after the decimal point.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a two-element list of the form $[[C_1,C_2,C_3],\\text{Stability}]$. For example, the final printed line must look like $[[[c_{A1},c_{A2},c_{A3}],s_A],[[c_{B1},c_{B2},c_{B3}],s_B],[[c_{C1},c_{C2},c_{C3}],s_C]]$, with all $c$ values as integers and all $s$ values as decimals rounded to three digits.\n\nNo physical units or angles are involved, and no percentages should be used. All numerical outputs must be integers or decimals as specified. The datasets must be generated exactly as described to ensure reproducibility. The implementation must not require any external input and must run as-is.", "solution": "The user wants to analyze the stability of logistic regression coefficients under varying L2 regularization strength. This involves implementing a logistic regression solver, generating three distinct datasets, fitting the model for a range of regularization parameters ($\\lambda$), and then quantifying the stability of the coefficient signs across this regularization path.\n\n### 1. Mathematical Formulation\n\nThe problem is to minimize the regularized objective function for logistic regression:\n$$\nJ_\\lambda(w,b) = \\frac{1}{n} \\sum_{i=1}^n \\left( \\log\\left(1 + e^{w^\\top x_i + b}\\right) - y_i (w^\\top x_i + b) \\right) + \\frac{\\lambda}{2} \\|w\\|_2^2\n$$\nThis objective function is convex, ensuring that a gradient-based optimization method can find the unique global minimum for any $\\lambda > 0$. To use such an optimizer, we need the gradient of $J_\\lambda$ with respect to the parameters $w$ and $b$.\n\nLet $z_i = w^\\top x_i + b$. The sigmoid function is $\\sigma(z_i) = p_i = \\frac{1}{1 + e^{-z_i}}$.\nThe gradient of the loss term for a single data point with respect to $z_i$ is:\n$$\n\\frac{\\partial}{\\partial z_i} \\left( \\log(1 + e^{z_i}) - y_i z_i \\right) = \\frac{e^{z_i}}{1 + e^{z_i}} - y_i = \\sigma(z_i) - y_i = p_i - y_i\n$$\nUsing the chain rule, we can find the gradients with respect to each component of $w$ and $b$:\n$$\n\\frac{\\partial z_i}{\\partial w_j} = x_{ij} \\quad \\text{and} \\quad \\frac{\\partial z_i}{\\partial b} = 1\n$$\nThe gradient of the objective function $J_\\lambda$ with respect to a weight $w_j$ is:\n$$\n\\nabla_{w_j} J_\\lambda = \\frac{1}{n} \\sum_{i=1}^n \\frac{\\partial J_\\lambda}{\\partial z_i} \\frac{\\partial z_i}{\\partial w_j} + \\frac{\\partial}{\\partial w_j} \\left( \\frac{\\lambda}{2} \\|w\\|_2^2 \\right) = \\frac{1}{n} \\sum_{i=1}^n (p_i - y_i)x_{ij} + \\lambda w_j\n$$\nIn vector form, the gradient with respect to the entire weight vector $w$ is:\n$$\n\\nabla_w J_\\lambda = \\frac{1}{n} X^\\top (p - y) + \\lambda w\n$$\nwhere $p$ is the vector of predicted probabilities and $y$ is the vector of true labels.\n\nThe intercept $b$ is not penalized. Its gradient is:\n$$\n\\nabla_b J_\\lambda = \\frac{1}{n} \\sum_{i=1}^n \\frac{\\partial J_\\lambda}{\\partial z_i} \\frac{\\partial z_i}{\\partial b} = \\frac{1}{n} \\sum_{i=1}^n (p_i - y_i)\n$$\n\nThese exact gradients will be supplied to a quasi-Newton optimization algorithm (L-BFGS-B) to find the optimal $(w(\\lambda), b(\\lambda))$ for each $\\lambda$ in the specified grid.\n\n### 2. Implementation Strategy\n\nThe solution is implemented in Python, adhering to the specified environment. The overall process for each test case is as follows:\n\n1.  **Data Generation**: For each of the three test cases (A, B, C), a specific dataset $(X, y)$ is generated according to the problem's rules, using `numpy.random.default_rng` with the specified seed for reproducibility.\n\n2.  **Model Fitting**: A loop iterates through the fixed regularization grid $\\Lambda = \\{0.001, 0.01, 0.1, 1.0, 10.0, 100.0\\}$.\n    *   For each $\\lambda \\in \\Lambda$, the objective function $J_\\lambda(w,b)$ and its gradient are passed to `scipy.optimize.minimize` with the 'L-BFGS-B' method. The parameters to be optimized are a single vector concatenating $w$ and $b$.\n    *   The implementation uses \"warm starts\": the solution $(w, b)$ found for a given $\\lambda_k$ is used as the initial guess for the optimization at the next value, $\\lambda_{k+1}$. This improves efficiency as solutions for adjacent $\\lambda$ values are typically close.\n    *   Numerical stability is ensured by using the `log-sum-exp` trick for the loss term and `scipy.special.expit` for a stable sigmoid function.\n\n3.  **Metrics Calculation**: After fitting the model for all $\\lambda$ values, the resulting coefficient path (a sequence of $w$ vectors) is analyzed.\n    *   The sign of each coefficient $w_j(\\lambda)$ is determined using the provided sign function $S_\\varepsilon(u)$ with $\\varepsilon = 10^{-6}$.\n    *   The number of sign changes, $C_j$, for each feature $j$ is counted by comparing the signs of coefficients at adjacent $\\lambda$ values in the path.\n    *   The `Stability` index is calculated as the average per-feature stability, $\\text{stab}_j$, where $\\text{stab}_j$ is the fraction of adjacent $\\lambda$ pairs for which the sign of $w_j$ does not change. This is equivalent to $\\text{stab}_j = 1 - C_j / (m-1)$, where $m$ is the number of points in the $\\lambda$ grid.\n\n4.  **Output Formatting**: The results for each test case, consisting of the list of sign-change counts $[C_1, C_2, C_3]$ and the overall `Stability` (rounded to three decimal places), are formatted into a precise string representation without extra spaces, as specified. The final output is a single line containing a list of these results.\n\n### 3. Expected Behavior for Test Cases\n\n*   **Case A (Clear Signal)**: With strong underlying coefficients for features 1 and 2, their signs should be correctly identified and remain stable until high regularization shrinks them to zero. Feature 3 has no signal, so its coefficient should remain near zero. We expect low sign-change counts and high stability.\n*   **Case B (High Collinearity)**: Features 1 and 2 are nearly identical ($x^{(2)} \\approx x^{(1)}$), while the outcome depends on their difference ($x^{(1)} - x^{(2)}$). This creates ambiguity, as many combinations of $w_1$ and $w_2$ where $w_1 \\approx -w_2$ can produce a good fit. The L2 penalty will select a unique solution, but the coefficient path might be less smooth than in Case A, potentially leading to instability and more sign changes for $w_1$ and $w_2$.\n*   **Case C (Low Signal-to-Noise)**: The true coefficients are small and significant noise is added to the linear predictor. The model may struggle to distinguish the weak signal from the noise, especially at low regularization. As $\\lambda$ increases, the penalty will quickly dominate, shrinking the already small coefficients to zero. This scenario is expected to exhibit the most instability and the lowest stability score.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.special import expit\n\ndef solve():\n    \"\"\"\n    Solves the logistic regression regularization path problem for three test cases.\n    \"\"\"\n\n    # --- Constants from the problem statement ---\n    LAMBDA_GRID = np.array([0.001, 0.01, 0.1, 1.0, 10.0, 100.0])\n    EPSILON = 1e-6\n\n    # --- Nested Helper Functions ---\n    def _generate_data(case_spec):\n        \"\"\"Generates dataset (X, y) based on the case specification.\"\"\"\n        case_id = case_spec['id']\n        seed = case_spec['seed']\n        n = case_spec['n']\n        d = case_spec['d']\n        rng = np.random.default_rng(seed)\n\n        if case_id == 'A':\n            w_star = np.array(case_spec['w_star'])\n            b_star = case_spec['b_star']\n            X = rng.standard_normal(size=(n, d))\n            z_star = X @ w_star + b_star\n            y = (z_star >= 0).astype(int)\n        elif case_id == 'B':\n            # Collinear features\n            x1 = rng.standard_normal(size=n)\n            xi = rng.standard_normal(size=n)\n            x2 = x1 + 0.01 * xi\n            x3 = rng.standard_normal(size=n)\n            X = np.stack([x1, x2, x3], axis=1)\n            y = ((x1 - x2) >= 0).astype(int)\n        elif case_id == 'C':\n            w_star = np.array(case_spec['w_star'])\n            b_star = case_spec['b_star']\n            noise_std = case_spec['noise_std']\n            X = rng.standard_normal(size=(n, d))\n            eta = rng.standard_normal(size=n)\n            z_star = X @ w_star + b_star + noise_std * eta\n            y = (z_star >= 0).astype(int)\n        \n        return X, y\n\n    def _objective_and_grad(params, X, y, lambda_val):\n        \"\"\"Computes the objective function J and its gradient.\"\"\"\n        n, d = X.shape\n        w = params[:d]\n        b = params[d]\n\n        z = X @ w + b\n\n        # Numerically stable calculation of log-likelihood part\n        log_1_plus_exp_z = np.maximum(0, z) + np.log(1 + np.exp(-np.abs(z)))\n        loss = np.mean(log_1_plus_exp_z - y * z)\n\n        # L2 regularization term (only on w)\n        reg_term = (lambda_val / 2.0) * np.dot(w, w)\n        J = loss + reg_term\n\n        # Gradient calculation\n        p = expit(z)\n        p_minus_y = p - y\n        grad_w = (1.0 / n) * X.T @ p_minus_y + lambda_val * w\n        grad_b = np.mean(p_minus_y)\n        grad = np.append(grad_w, grad_b)\n        \n        return J, grad\n\n    def _s_epsilon_vec(u_vec, epsilon):\n        \"\"\"Vectorized sign function with tolerance.\"\"\"\n        return np.select([u_vec  -epsilon, u_vec > epsilon], [-1, 1], default=0)\n\n    def _calculate_metrics(w_path, m, d, epsilon):\n        \"\"\"Calculates sign-change counts and stability.\"\"\"\n        w_path_np = np.array(w_path)\n        sign_path = _s_epsilon_vec(w_path_np, epsilon)\n        \n        # Sign-change count C_j\n        sign_diffs = np.diff(sign_path, axis=0)\n        sign_changes = (sign_diffs != 0).astype(int)\n        c_counts = np.sum(sign_changes, axis=0)\n        \n        # Stability\n        num_transitions = m - 1\n        if num_transitions == 0:\n            return c_counts, 1.0\n        \n        stab_j = (num_transitions - c_counts) / num_transitions\n        overall_stability = np.mean(stab_j)\n        \n        return c_counts, overall_stability\n\n    # --- Main Logic ---\n    test_cases = [\n        {'id': 'A', 'seed': 0, 'n': 120, 'd': 3, 'w_star': [3, -2, 0], 'b_star': 0},\n        {'id': 'B', 'seed': 1, 'n': 100, 'd': 3},\n        {'id': 'C', 'seed': 2, 'n': 200, 'd': 3, 'w_star': [0.2, -0.2, 0], 'b_star': 0, 'noise_std': 0.1},\n    ]\n\n    all_results_str = []\n\n    for case_spec in test_cases:\n        X, y = _generate_data(case_spec)\n        d = X.shape[1]\n        \n        w_path = []\n        # Initial guess for the optimization (cold start for the first lambda)\n        params_0 = np.zeros(d + 1)\n        \n        for lambda_val in LAMBDA_GRID:\n            result = minimize(\n                fun=_objective_and_grad,\n                x0=params_0,\n                args=(X, y, lambda_val),\n                method='L-BFGS-B',\n                jac=True\n            )\n            # Use the current solution as a warm start for the next iteration\n            params_0 = result.x\n            w_path.append(result.x[:d])\n\n        c_counts, stability = _calculate_metrics(w_path, len(LAMBDA_GRID), d, EPSILON)\n        \n        c_counts_list = list(c_counts.astype(int))\n        \n        # Format the result string for this case\n        result_str = f\"[[{','.join(map(str, c_counts_list))}],{stability:.3f}]\"\n        all_results_str.append(result_str)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(all_results_str)}]\")\n\nsolve()\n```", "id": "3148628"}]}