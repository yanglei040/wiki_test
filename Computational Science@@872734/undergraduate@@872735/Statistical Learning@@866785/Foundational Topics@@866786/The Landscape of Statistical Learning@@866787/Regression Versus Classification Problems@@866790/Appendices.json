{"hands_on_practices": [{"introduction": "The theoretical best-possible performance for any learning task is limited by its \"Bayes risk,\" or irreducible error. This exercise challenges you to derive this limit from first principles for two distinct tasks—one classification, one regression—that share the exact same input feature. By constructing this elegant counterexample, you will gain a profound insight into how the nature of the target variable fundamentally distinguishes the inherent predictability of classification versus regression problems [@problem_id:3169383].", "problem": "Consider a supervised learning setting with a single real-valued feature $X$ drawn from a continuous distribution and two learning tasks defined on the same input $X$ but with different targets: a binary classification task and a real-valued regression task. Let the data-generating mechanism be:\n- $X \\sim \\mathrm{Uniform}(-1,1)$,\n- $\\epsilon \\sim \\mathcal{N}(0,\\sigma^{2})$ independent of $X$,\n- classification label $C \\in \\{0,1\\}$ defined by $C = \\mathbb{I}\\{X \\geq 0\\}$,\n- regression target $Y \\in \\mathbb{R}$ defined by $Y = X + \\epsilon$.\n\nYou are asked to start from the foundational definitions of expected risk for zero-one loss in classification and squared error loss in regression, together with the definitions of conditional probability and conditional expectation. Using only these bases, derive the Bayes-optimal risk for each task under the above generative model. In particular:\n\n1. For the classification task with zero-one loss, derive the minimal achievable expected misclassification rate across all measurable classifiers.\n2. For the regression task with squared error loss, derive the minimal achievable expected mean squared error (MSE) across all measurable predictors.\n\nExplain why this construction provides a counterexample in which perfect classification is possible (the classes are separable), yet the regression MSE is bounded below by a positive constant due to aleatoric noise. Express your final answer as a row matrix whose first entry is the minimal classification risk and whose second entry is the minimal regression risk, in terms of $\\sigma^{2}$. No rounding is required.", "solution": "The problem as stated is scientifically grounded, well-posed, objective, and self-contained. It is a standard problem in statistical learning theory used to illustrate the concepts of Bayes-optimal predictors and irreducible error (aleatoric uncertainty). All provided definitions and distributions are standard. There are no contradictions or ambiguities. The problem is valid and can be solved.\n\nWe will address the two tasks sequentially, starting from the foundational definitions of risk and the Bayes-optimal predictor for each case.\n\n**1. Minimal Classification Risk (Zero-One Loss)**\n\nThe task is to classify an observation with feature $X$ into one of two classes, $C \\in \\{0, 1\\}$. The loss function is the zero-one loss, defined as $L(C, \\hat{C}) = \\mathbb{I}\\{C \\neq \\hat{C}\\}$, where $\\hat{C}$ is the predicted class from a classifier $\\hat{c}(X)$. The risk of a classifier $\\hat{c}$ is the expected loss:\n$$\nR(\\hat{c}) = E[L(C, \\hat{c}(X))] = E[\\mathbb{I}\\{C \\neq \\hat{c}(X)\\}] = P(C \\neq \\hat{c}(X))\n$$\nThe goal is to find the classifier $\\hat{c}^*(X)$ that minimizes this risk. This classifier is known as the Bayes classifier. The risk of the Bayes classifier is the Bayes risk.\n\nBy the law of total expectation, the risk can be written as:\n$$\nR(\\hat{c}) = E_X[E_{C|X}[ \\mathbb{I}\\{C \\neq \\hat{c}(X)\\} | X=x ]]\n$$\nTo minimize the overall risk, we must minimize the inner conditional expectation for each value of $x$. The Bayes decision rule for a given $x$ is:\n$$\n\\hat{c}^*(x) = \\arg\\min_{k \\in \\{0,1\\}} E[ \\mathbb{I}\\{C \\neq k\\} | X=x ] = \\arg\\min_{k \\in \\{0,1\\}} P(C \\neq k | X=x)\n$$\nMinimizing the probability of being wrong is equivalent to maximizing the probability of being right. Thus, the Bayes classifier chooses the class with the highest posterior probability:\n$$\n\\hat{c}^*(x) = \\arg\\max_{k \\in \\{0,1\\}} P(C = k | X=x)\n$$\nLet us define the conditional probability function $\\eta(x) = P(C=1|X=x)$. Then $P(C=0|X=x) = 1 - \\eta(x)$. The rule becomes: choose class $1$ if $\\eta(x) > 1/2$, and class $0$ otherwise.\n\nIn this problem, the class label $C$ is a deterministic function of the feature $X$: $C = \\mathbb{I}\\{X \\geq 0\\}$. This means that given a value $x$ for the random variable $X$, the value of $C$ is known with certainty. There is no randomness in $C$ conditional on $X$.\nSpecifically:\n- If $x \\geq 0$, then $C = 1$ with probability $1$. Thus, $\\eta(x) = P(C=1|X=x) = 1$.\n- If $x  0$, then $C = 0$ with probability $1$. Thus, $\\eta(x) = P(C=1|X=x) = 0$.\n\nApplying the Bayes decision rule:\n- For $x \\geq 0$, $\\eta(x)=1 > 1/2$, so the Bayes classifier predicts $\\hat{c}^*(x) = 1$.\n- For $x  0$, $\\eta(x)=0  1/2$, so the Bayes classifier predicts $\\hat{c}^*(x) = 0$.\n\nThis can be summarized as $\\hat{c}^*(X) = \\mathbb{I}\\{X \\geq 0\\}$. We observe that the Bayes classifier is identical to the true data-generating function for the class label: $\\hat{c}^*(X) = C$.\n\nThe minimal achievable risk (the Bayes risk) is the risk of this optimal classifier:\n$$\nR^* = R(\\hat{c}^*) = E[\\mathbb{I}\\{C \\neq \\hat{c}^*(X)\\}]\n$$\nSince $\\hat{c}^*(X) = C$ for all possible outcomes of $X$, the event $\\{C \\neq \\hat{c}^*(X)\\}$ is an impossible event. Therefore, the indicator function is always $0$.\n$$\nR^* = E[0] = 0\n$$\nThe minimal achievable expected misclassification rate is $0$. This signifies that the classes are perfectly separable, and the optimal decision boundary can be learned without error.\n\n**2. Minimal Regression Risk (Squared Error Loss)**\n\nThe task is to predict a real-valued target $Y$ from the feature $X$. The loss function is the squared error loss, defined as $L(Y, \\hat{Y}) = (Y - \\hat{Y})^2$, where $\\hat{Y}$ is the predicted value from a regression function $f(X)$. The risk of a regression function $f$ is the expected loss, or Mean Squared Error (MSE):\n$$\nR(f) = E[L(Y, f(X))] = E[(Y - f(X))^2]\n$$\nThe goal is to find the function $f^*(X)$ that minimizes this risk. This is the Bayes predictor for regression.\n\nUsing the law of total expectation, we decompose the risk:\n$$\nR(f) = E_X[E_{Y|X}[(Y - f(X))^2|X=x]]\n$$\nTo minimize the total risk, we must select $f(x)$ to minimize the inner conditional expectation for each $x$. For a fixed $x$, $f(x)$ is a constant, let's call it $g$. We need to find $g$ that minimizes $E[(Y-g)^2|X=x]$. This is a classic result from probability theory: the value of $g$ that minimizes the expected squared difference to a random variable $Y$ is the expectation of $Y$. Therefore, the optimal predictor is the conditional expectation of the target given the feature:\n$$\nf^*(x) = E[Y|X=x]\n$$\nLet's compute this for the given data-generating process, $Y = X + \\epsilon$, where $X \\sim \\mathrm{Uniform}(-1,1)$ and $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$ are independent.\n$$\nf^*(x) = E[X + \\epsilon | X=x]\n$$\nUsing the linearity of conditional expectation:\n$$\nf^*(x) = E[X | X=x] + E[\\epsilon | X=x]\n$$\nFor the first term, $E[X | X=x] = x$. For the second term, since $\\epsilon$ and $X$ are independent, conditioning on $X$ does not provide any information about $\\epsilon$. Thus, $E[\\epsilon | X=x] = E[\\epsilon]$. We are given that $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$, so its mean is $E[\\epsilon] = 0$.\nSubstituting these back, we get the Bayes predictor:\n$$\nf^*(x) = x + 0 = x\n$$\nSo, the optimal regression function is $f^*(X) = X$.\n\nNow, we compute the minimal achievable risk (the Bayes risk) by evaluating the MSE for this optimal predictor:\n$$\nR^* = R(f^*) = E[(Y - f^*(X))^2] = E[(Y - X)^2]\n$$\nSubstitute the definition of $Y = X + \\epsilon$:\n$$\nR^* = E[((X + \\epsilon) - X)^2] = E[\\epsilon^2]\n$$\nThe value $E[\\epsilon^2]$ can be found from the definition of variance: $\\mathrm{Var}(\\epsilon) = E[\\epsilon^2] - (E[\\epsilon])^2$.\nWe are given $\\mathrm{Var}(\\epsilon) = \\sigma^2$ and $E[\\epsilon] = 0$.\n$$\n\\sigma^2 = E[\\epsilon^2] - 0^2 \\implies E[\\epsilon^2] = \\sigma^2\n$$\nThus, the minimal achievable MSE is:\n$$\nR^* = \\sigma^2\n$$\n\n**Explanation of the Counterexample**\n\nThis construction serves as a clear counterexample to any naive assumption that \"easy\" classification problems imply \"easy\" regression problems on the same features.\n- In the classification task, the target $C = \\mathbb{I}\\{X \\geq 0\\}$ is a deterministic, noise-free function of the input feature $X$. The relationship is exact. Therefore, an ideal learner can find the decision boundary $X = 0$ and achieve perfect classification, resulting in a Bayes risk of $0$.\n- In the regression task, the target $Y = X + \\epsilon$ has two components: a deterministic part ($X$) and a stochastic part ($\\epsilon$). The Bayes predictor $f^*(X)=X$ can perfectly learn the deterministic relationship. However, the noise term $\\epsilon$ is random and, crucially, independent of $X$. No function of $X$ can possibly predict the value of $\\epsilon$. This unpredictable component gives rise to an irreducible error, often called aleatoric uncertainty. The Bayes risk, $R^* = \\sigma^2$, is precisely the variance of this noise. Unless $\\sigma^2=0$, the minimal MSE is strictly positive, meaning perfect regression is impossible.\n\nThe minimal classification risk is $0$, and the minimal regression risk is $\\sigma^2$. The final answer is the row matrix containing these two values.", "answer": "$$\n\\boxed{\\begin{pmatrix} 0  \\sigma^{2} \\end{pmatrix}}\n$$", "id": "3169383"}, {"introduction": "In many classification problems, the underlying reality is probabilistic; our goal is to estimate the conditional probability $p(x) = \\mathbb{P}(Y=1 \\mid X=x)$. This raises a crucial question: how should we train our model? This practice contrasts two approaches: naively treating the binary $\\{0,1\\}$ labels as regression targets with squared error loss, versus using the more principled cross-entropy loss. By deriving the Bayes risk for both, you will discover why specialized loss functions are essential and how they relate to fundamental concepts like information entropy [@problem_id:3169373].", "problem": "Consider a binary response variable $Y \\in \\{0,1\\}$ and a feature vector $X$ taking values in a measurable space. Let the conditional distribution of $Y$ given $X=x$ be specified by the soft label $\\mathbb{P}(Y=1 \\mid X=x)=p(x)$, so that $\\mathbb{P}(Y=0 \\mid X=x)=1-p(x)$. Assume the induced random variable $p(X)$ is uniformly distributed on $[0,1]$. Two learning paradigms are considered, each producing a pointwise predictor $s(x)\\in(0,1)$: (i) regression with squared error loss $\\ell_{\\mathrm{SE}}(y,s)=(y-s)^{2}$, trained on the numeric encoding $y\\in\\{0,1\\}$; and (ii) probabilistic classification with binary cross-entropy loss $\\ell_{\\mathrm{CE}}(y,s)=-y\\ln(s)-(1-y)\\ln(1-s)$.\n\nUsing the foundational definition that the Bayes decision rule minimizes the conditional expected loss, and that the Bayes risk is the expectation, over $X$, of this minimized conditional loss, derive from first principles the Bayes risk $R_{\\mathrm{SE}}$ under squared error and $R_{\\mathrm{CE}}$ under cross-entropy, and then compute the exact value of the difference $R_{\\mathrm{CE}}-R_{\\mathrm{SE}}$ under the stated uniform distribution of $p(X)$ on $[0,1]$. Express your final answer exactly as a simplified fraction.", "solution": "The problem requires the derivation and calculation of the Bayes risks for two different loss functions, squared error and cross-entropy, under a specific distributional assumption, and then to compute the difference between these two risks.\n\nFirst, we establish the general framework for finding the Bayes decision rule and Bayes risk. Let $Y$ be the response variable, $X$ be the feature vector, and $s(x)$ be a predictor for the outcome of $Y$ given $X=x$. The conditional expected loss at $X=x$ for a given loss function $\\ell(y, s)$ is defined as:\n$$L(s(x) \\mid X=x) = \\mathbb{E}_{Y \\mid X=x}[\\ell(Y, s(x))]$$\nThe problem states that $Y$ is a binary random variable, $Y \\in \\{0,1\\}$, with conditional probability mass function given by $\\mathbb{P}(Y=1 \\mid X=x) = p(x)$ and $\\mathbb{P}(Y=0 \\mid X=x) = 1-p(x)$. Thus, the conditional expectation can be written as:\n$$L(s(x) \\mid x) = \\ell(1, s(x)) \\cdot p(x) + \\ell(0, s(x)) \\cdot (1-p(x))$$\nThe Bayes decision rule (or Bayes predictor) $s^*(x)$ is the function that minimizes this conditional expected loss for each $x$:\n$$s^*(x) = \\arg\\min_{s \\in (0,1)} L(s \\mid x)$$\nThe minimized value of this conditional loss is the conditional Bayes risk, $R(x) = L(s^*(x) \\mid x)$. The overall Bayes risk $R$ is the expectation of the conditional Bayes risk over the distribution of $X$:\n$$R = \\mathbb{E}_X[R(X)] = \\mathbb{E}_X[L(s^*(X) \\mid X)]$$\n\nWe will now apply this framework to the two specified loss functions.\n\n**Part 1: Squared Error Loss**\n\nThe squared error loss is given by $\\ell_{\\mathrm{SE}}(y,s) = (y-s)^2$. The conditional expected loss is:\n$$L_{\\mathrm{SE}}(s \\mid x) = (1-s)^2 \\cdot p(x) + (0-s)^2 \\cdot (1-p(x)) = (1-2s+s^2)p(x) + s^2(1-p(x))$$\n$$L_{\\mathrm{SE}}(s \\mid x) = p(x) - 2sp(x) + s^2p(x) + s^2 - s^2p(x) = s^2 - 2sp(x) + p(x)$$\nThis is a quadratic function of $s$, convex in $s$. To find the minimum, we take the derivative with respect to $s$ and set it to $0$:\n$$\\frac{\\partial L_{\\mathrm{SE}}(s \\mid x)}{\\partial s} = 2s - 2p(x) = 0$$\nThis yields the Bayes predictor for squared error loss:\n$$s_{\\mathrm{SE}}^*(x) = p(x)$$\nThe conditional Bayes risk for squared error, $R_{\\mathrm{SE}}(x)$, is obtained by substituting $s_{\\mathrm{SE}}^*(x)$ back into the conditional expected loss function:\n$$R_{\\mathrm{SE}}(x) = L_{\\mathrm{SE}}(p(x) \\mid x) = \\mathbb{E}_{Y \\mid X=x}[(Y - p(x))^2]$$\nThis is the definition of the variance of the Bernoulli random variable $Y \\mid X=x$, which has parameter $p(x)$. The variance of a Bernoulli($p$) random variable is $p(1-p)$. Therefore:\n$$R_{\\mathrm{SE}}(x) = p(x)(1-p(x))$$\n\n**Part 2: Cross-Entropy Loss**\n\nThe binary cross-entropy loss is given by $\\ell_{\\mathrm{CE}}(y,s) = -y\\ln(s) - (1-y)\\ln(1-s)$. The conditional expected loss is:\n$$L_{\\mathrm{CE}}(s \\mid x) = [-\\ln(s)] \\cdot p(x) + [-\\ln(1-s)] \\cdot (1-p(x))$$\n$$L_{\\mathrm{CE}}(s \\mid x) = -p(x)\\ln(s) - (1-p(x))\\ln(1-s)$$\nThis function is also convex in $s$. To find the minimum, we take the derivative with respect to $s$ and set it to $0$:\n$$\\frac{\\partial L_{\\mathrm{CE}}(s \\mid x)}{\\partial s} = -\\frac{p(x)}{s} - (1-p(x))\\left(\\frac{-1}{1-s}\\right) = -\\frac{p(x)}{s} + \\frac{1-p(x)}{1-s} = 0$$\n$$\\frac{1-p(x)}{1-s} = \\frac{p(x)}{s} \\implies s(1-p(x)) = p(x)(1-s) \\implies s - sp(x) = p(x) - sp(x)$$\nThis yields the Bayes predictor for cross-entropy loss:\n$$s_{\\mathrm{CE}}^*(x) = p(x)$$\nThe conditional Bayes risk for cross-entropy, $R_{\\mathrm{CE}}(x)$, is obtained by substituting $s_{\\mathrm{CE}}^*(x)$ back into the conditional expected loss function:\n$$R_{\\mathrm{CE}}(x) = L_{\\mathrm{CE}}(p(x) \\mid x) = -p(x)\\ln(p(x)) - (1-p(x))\\ln(1-p(x))$$\nThis is the Shannon entropy of a Bernoulli random variable with parameter $p(x)$.\n\n**Part 3: Calculation of Overall Bayes Risks**\n\nThe problem states that the induced random variable $P = p(X)$ follows a uniform distribution on the interval $[0,1]$. Its probability density function is $f_P(p)=1$ for $p \\in [0,1]$ and $0$ otherwise. The overall Bayes risks are the expectations of the conditional Bayes risks with respect to this distribution.\n\nFor squared error:\n$$R_{\\mathrm{SE}} = \\mathbb{E}_P[P(1-P)] = \\int_0^1 p(1-p) \\, dp = \\int_0^1 (p-p^2) \\, dp$$\n$$R_{\\mathrm{SE}} = \\left[ \\frac{p^2}{2} - \\frac{p^3}{3} \\right]_0^1 = \\left(\\frac{1^2}{2} - \\frac{1^3}{3}\\right) - (0) = \\frac{1}{2} - \\frac{1}{3} = \\frac{3-2}{6} = \\frac{1}{6}$$\n\nFor cross-entropy:\n$$R_{\\mathrm{CE}} = \\mathbb{E}_P[-P\\ln(P) - (1-P)\\ln(1-P)] = \\int_0^1 [-p\\ln(p) - (1-p)\\ln(1-p)] \\, dp$$\n$$R_{\\mathrm{CE}} = -\\int_0^1 p\\ln(p) \\, dp - \\int_0^1 (1-p)\\ln(1-p) \\, dp$$\nLet's evaluate the integral $\\int_0^1 p\\ln(p) \\, dp$ using integration by parts, with $u=\\ln(p)$ and $dv=p \\, dp$. Then $du = \\frac{1}{p}dp$ and $v = \\frac{p^2}{2}$.\n$$\\int p\\ln(p) \\, dp = \\frac{p^2}{2}\\ln(p) - \\int \\frac{p^2}{2} \\cdot \\frac{1}{p} \\, dp = \\frac{p^2}{2}\\ln(p) - \\int \\frac{p}{2} \\, dp = \\frac{p^2}{2}\\ln(p) - \\frac{p^2}{4}$$\nEvaluating the definite integral:\n$$\\int_0^1 p\\ln(p) \\, dp = \\left[ \\frac{p^2}{2}\\ln(p) - \\frac{p^2}{4} \\right]_0^1$$\nThe upper limit gives $\\frac{1^2}{2}\\ln(1) - \\frac{1^2}{4} = 0 - \\frac{1}{4} = -\\frac{1}{4}$. For the lower limit, we need $\\lim_{p \\to 0^+} \\left( \\frac{p^2}{2}\\ln(p) - \\frac{p^2}{4} \\right)$. The term $\\frac{p^2}{4}$ goes to $0$. The term $p^2\\ln(p)$ has an indeterminate form $0 \\cdot (-\\infty)$, which can be evaluated using L'Hôpital's rule:\n$$\\lim_{p \\to 0^+} p^2\\ln(p) = \\lim_{p \\to 0^+} \\frac{\\ln(p)}{p^{-2}} = \\lim_{p \\to 0^+} \\frac{1/p}{-2p^{-3}} = \\lim_{p \\to 0^+} \\frac{-p^2}{2} = 0$$\nSo the lower limit evaluates to $0$. The integral is $-\\frac{1}{4} - 0 = -\\frac{1}{4}$.\n\nThe second integral in the expression for $R_{\\mathrm{CE}}$ is $-\\int_0^1 (1-p)\\ln(1-p) \\, dp$. Let $q=1-p$, so $dq=-dp$. The limits of integration for $q$ are from $1$ to $0$.\n$$-\\int_1^0 q\\ln(q) (-dq) = \\int_1^0 q\\ln(q) \\, dq = -\\int_0^1 q\\ln(q) \\, dq$$\nThis is the negative of the same integral form we just computed. Thus, its value is $-(-\\frac{1}{4})=\\frac{1}{4}$.\nThe first integral in the expression for $R_{\\mathrm{CE}}$ is $-\\int_0^1 p\\ln(p) \\, dp = -(-\\frac{1}{4}) = \\frac{1}{4}$.\nTherefore, the total Bayes risk for cross-entropy is:\n$$R_{\\mathrm{CE}} = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2}$$\n\n**Part 4: Final Calculation**\n\nThe problem asks for the difference $R_{\\mathrm{CE}} - R_{\\mathrm{SE}}$:\n$$R_{\\mathrm{CE}} - R_{\\mathrm{SE}} = \\frac{1}{2} - \\frac{1}{6} = \\frac{3}{6} - \\frac{1}{6} = \\frac{2}{6} = \\frac{1}{3}$$\nThis result represents the average difference in irreducible error between the two loss functions, over a uniform distribution of possible true probabilities.", "answer": "$$\\boxed{\\frac{1}{3}}$$", "id": "3169373"}, {"introduction": "Textbook models often assume that our predictors are measured perfectly, but in the real world, data is frequently noisy. This practice delves into the critical topic of errors-in-variables, guiding you to derive the consequences of using a predictor that is corrupted by measurement error. You will demonstrate how this noise leads to the systematic underestimation of regression slopes—a phenomenon called attenuation—and consequently distorts the decision boundaries of classifiers built from these biased models [@problem_id:3169412].", "problem": "A single scalar predictor is measured with noise. Let the true predictor be $X$ with $\\mathbb{E}[X] = 0$ and $\\operatorname{Var}(X) = \\sigma_{X}^{2}$. The observed predictor is $W = X + U$, where $U$ is measurement error with $\\mathbb{E}[U] = 0$, $\\operatorname{Var}(U) = \\sigma_{U}^{2}$, independent of $X$. The response is $Y = \\beta_{0} + \\beta_{1} X + \\varepsilon$, where $\\mathbb{E}[\\varepsilon] = 0$, $\\operatorname{Var}(\\varepsilon) = \\sigma_{\\varepsilon}^{2}$, and $\\varepsilon$ is independent of $X$ and $U$. Consider the population ordinary least squares (OLS) regression of $Y$ on $W$, whose slope and intercept are denoted $\\tilde{\\beta}_{1}$ and $\\tilde{\\beta}_{0}$, respectively. A practitioner uses a regression-threshold classifier built from this naive fit: predict class $1$ if the linear score $\\tilde{\\beta}_{0} + \\tilde{\\beta}_{1} w$ is at least a given threshold $\\tau$, and class $0$ otherwise. \n\nStarting from the definitions of expectation, variance, and covariance, and the linearity of these operators under independence, derive the regression attenuation induced by measurement error and show how the threshold-based classification boundary in the observed predictor space is distorted relative to the boundary under the true predictor. Then, assuming $\\sigma_{X}^{2}$ and $\\sigma_{U}^{2}$ are known, provide closed-form bias corrections for both the regression slope and the classification decision boundary so that the corrected classifier uses $W$ but matches the boundary implied by the true predictor. \n\nExpress your final answers in terms of $\\tilde{\\beta}_{1}$, $\\sigma_{X}^{2}$, $\\sigma_{U}^{2}$, $\\beta_{0}$, and $\\tau$. Your final answer must be a single closed-form analytic expression containing both quantities arranged as a two-entry row matrix using the $\\mathrm{pmatrix}$ environment. No rounding is required.", "solution": "We begin from the fundamental definitions of expectation, variance, and covariance. The population ordinary least squares (OLS) slope for regressing $Y$ on $W$ is given by\n$$\n\\tilde{\\beta}_{1} \\;=\\; \\frac{\\operatorname{Cov}(Y, W)}{\\operatorname{Var}(W)}.\n$$\nThe OLS intercept is\n$$\n\\tilde{\\beta}_{0} \\;=\\; \\mathbb{E}[Y] \\;-\\; \\tilde{\\beta}_{1}\\,\\mathbb{E}[W].\n$$\nBy independence and linearity, we compute $\\operatorname{Cov}(Y,W)$. Using $Y = \\beta_{0} + \\beta_{1} X + \\varepsilon$ and $W = X + U$,\n$$\n\\operatorname{Cov}(Y, W) \\;=\\; \\operatorname{Cov}(\\beta_{0} + \\beta_{1} X + \\varepsilon,\\, X + U)\n\\;=\\; \\beta_{1}\\,\\operatorname{Cov}(X, X) \\;+\\; \\beta_{1}\\,\\operatorname{Cov}(X, U) \\;+\\; \\operatorname{Cov}(\\varepsilon, X) \\;+\\; \\operatorname{Cov}(\\varepsilon, U).\n$$\nBecause $X$, $U$, and $\\varepsilon$ are mutually independent and centered as specified, $\\operatorname{Cov}(X,U) = 0$, $\\operatorname{Cov}(\\varepsilon,X) = 0$, and $\\operatorname{Cov}(\\varepsilon,U) = 0$. Hence\n$$\n\\operatorname{Cov}(Y, W) \\;=\\; \\beta_{1}\\,\\operatorname{Var}(X) \\;=\\; \\beta_{1}\\,\\sigma_{X}^{2}.\n$$\nNext, \n$$\n\\operatorname{Var}(W) \\;=\\; \\operatorname{Var}(X + U) \\;=\\; \\operatorname{Var}(X) + \\operatorname{Var}(U) \\;=\\; \\sigma_{X}^{2} + \\sigma_{U}^{2},\n$$\nagain by independence. Therefore the population OLS slope for the naive regression of $Y$ on $W$ is\n$$\n\\tilde{\\beta}_{1} \\;=\\; \\frac{\\beta_{1}\\,\\sigma_{X}^{2}}{\\sigma_{X}^{2} + \\sigma_{U}^{2}},\n$$\nwhich demonstrates the classical attenuation: the measured slope is shrunk toward zero by the factor $\\lambda = \\sigma_{X}^{2}/(\\sigma_{X}^{2} + \\sigma_{U}^{2})$. For the intercept, note that $\\mathbb{E}[W] = \\mathbb{E}[X] + \\mathbb{E}[U] = 0$ and $\\mathbb{E}[Y] = \\beta_{0} + \\beta_{1}\\mathbb{E}[X] + \\mathbb{E}[\\varepsilon] = \\beta_{0}$, so\n$$\n\\tilde{\\beta}_{0} \\;=\\; \\beta_{0} \\;-\\; \\tilde{\\beta}_{1}\\cdot 0 \\;=\\; \\beta_{0}.\n$$\n\nWe now analyze the threshold-based classification boundary. Under the true predictor $X$, the regression-threshold decision rule that predicts class $1$ if the deterministic part of $Y$ exceeds $\\tau$ can be written as classify $1$ if $\\beta_{0} + \\beta_{1} x \\ge \\tau$. The boundary in $X$-space is obtained by solving\n$$\n\\beta_{0} + \\beta_{1} x^{\\ast} = \\tau \\quad\\Rightarrow\\quad x^{\\ast} = \\frac{\\tau - \\beta_{0}}{\\beta_{1}}.\n$$\nThe practitioner, using the naive regression on $W$, forms the decision rule: predict class $1$ if $\\tilde{\\beta}_{0} + \\tilde{\\beta}_{1} w \\ge \\tau$. Since $\\tilde{\\beta}_{0} = \\beta_{0}$, the boundary in $W$-space is\n$$\nw_{\\text{naive}} \\;=\\; \\frac{\\tau - \\beta_{0}}{\\tilde{\\beta}_{1}} \n\\;=\\; \\frac{\\tau - \\beta_{0}}{\\beta_{1}\\,\\sigma_{X}^{2}/(\\sigma_{X}^{2} + \\sigma_{U}^{2})}\n\\;=\\; \\frac{\\sigma_{X}^{2} + \\sigma_{U}^{2}}{\\sigma_{X}^{2}} \\cdot \\frac{\\tau - \\beta_{0}}{\\beta_{1}}\n\\;=\\; \\frac{1}{\\lambda}\\,x^{\\ast}.\n$$\nThis shows the distortion of the classification boundary: measurement error expands the threshold by the factor $(\\sigma_{X}^{2} + \\sigma_{U}^{2})/\\sigma_{X}^{2}$ in the observed predictor space.\n\nAssuming $\\sigma_{X}^{2}$ and $\\sigma_{U}^{2}$ are known, we can correct the attenuation bias in the slope by inverting the attenuation factor. Define the correction factor\n$$\nc \\;=\\; \\frac{\\sigma_{X}^{2} + \\sigma_{U}^{2}}{\\sigma_{X}^{2}} \\;=\\; \\frac{1}{\\lambda}.\n$$\nA bias-corrected slope is then\n$$\n\\hat{\\beta}_{\\text{corr}} \\;=\\; \\tilde{\\beta}_{1}\\,c \\;=\\; \\tilde{\\beta}_{1}\\,\\frac{\\sigma_{X}^{2} + \\sigma_{U}^{2}}{\\sigma_{X}^{2}}.\n$$\nUsing this corrected slope in the regression-threshold classifier yields the corrected boundary in $W$-space:\n$$\nw_{\\text{corr}} \\;=\\; \\frac{\\tau - \\beta_{0}}{\\hat{\\beta}_{\\text{corr}}}\n\\;=\\; \\frac{\\tau - \\beta_{0}}{\\tilde{\\beta}_{1}\\,(\\sigma_{X}^{2} + \\sigma_{U}^{2})/\\sigma_{X}^{2}}\n\\;=\\; \\frac{\\sigma_{X}^{2}}{\\tilde{\\beta}_{1}\\,(\\sigma_{X}^{2} + \\sigma_{U}^{2})}\\,(\\tau - \\beta_{0}).\n$$\nEquivalently, since $\\tilde{\\beta}_{1} = \\lambda \\beta_{1}$, $w_{\\text{corr}} = (\\tau - \\beta_{0})/\\beta_{1} = x^{\\ast}$, demonstrating that the corrected classifier recovers the true boundary when expressed in the observed predictor space.\n\nThe requested closed-form bias corrections, written purely in terms of $\\tilde{\\beta}_{1}$, $\\sigma_{X}^{2}$, $\\sigma_{U}^{2}$, $\\beta_{0}$, and $\\tau$, are the pair $(\\hat{\\beta}_{\\text{corr}},\\, w_{\\text{corr}})$ above.", "answer": "$$\\boxed{\\begin{pmatrix}\n\\tilde{\\beta}_{1}\\,\\dfrac{\\sigma_{X}^{2} + \\sigma_{U}^{2}}{\\sigma_{X}^{2}} \\;  \\; \\dfrac{\\sigma_{X}^{2}}{\\tilde{\\beta}_{1}\\,(\\sigma_{X}^{2} + \\sigma_{U}^{2})}\\,(\\tau - \\beta_{0})\n\\end{pmatrix}}$$", "id": "3169412"}]}