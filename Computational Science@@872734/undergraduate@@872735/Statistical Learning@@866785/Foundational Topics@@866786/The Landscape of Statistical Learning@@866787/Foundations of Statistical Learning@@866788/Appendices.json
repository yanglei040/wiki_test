{"hands_on_practices": [{"introduction": "Before we can build effective models, we must be precise about how we measure their performance. This is especially true in classification, where a single accuracy number can be misleading, particularly with imbalanced classes. This exercise guides you through the fundamental algebraic relationships between key metrics like recall, specificity, and precision, revealing how they are all interconnected through the underlying class prevalence and the classifier's behavior. [@problem_id:3094141]", "problem": "A binary classifier is evaluated on a dataset of $N$ independent samples drawn from a population in which the prevalence (fraction of truly positive instances) is $\\pi \\in [0,1]$. The classifier is tuned to achieve a target recall $R \\in [0,1]$ and a target specificity $S \\in [0,1]$, each interpreted exactly as their population counterparts on this dataset. Relying only on the fundamental definitions of prevalence, recall, specificity, precision, and the F-one score (F1-score), proceed as follows:\n\n- Using the definitions of recall and specificity together with prevalence, express the confusion matrix counts: true positives ($TP$), false positives ($FP$), true negatives ($TN$), and false negatives ($FN$) in terms of $N$, $\\pi$, $R$, and $S$.\n\n- From these counts and the definition of precision, deduce the precision $P$.\n\n- Using the definition of the F-one score (F1-score) as the harmonic mean of precision and recall, or equivalently in terms of the confusion matrix counts, derive a closed-form expression for the F-one score purely as a function of $\\pi$, $R$, and $S$.\n\n- State the feasibility constraints on $N$, $\\pi$, $R$, and $S$ required for these quantities to be well-defined and for the counts to be nonnegative and interpretable as dataset counts.\n\nYour final answer must be the single, simplified analytic expression for the F-one score as a function of $\\pi$, $R$, and $S$ only. No numerical evaluation is required, and no rounding is needed.", "solution": "First, we define the total number of actual positive and actual negative instances in the dataset of size $N$ with a prevalence $\\pi$.\nThe number of actual positives, $P_{actual}$, is given by $P_{actual} = N\\pi$.\nThe number of actual negatives, $N_{actual}$, is given by $N_{actual} = N(1-\\pi)$.\n\nNext, we use the definitions of Recall ($R$) and Specificity ($S$) to express the four counts of the confusion matrix: true positives ($TP$), false negatives ($FN$), true negatives ($TN$), and false positives ($FP$).\n\nRecall, or the True Positive Rate, is the fraction of actual positives that are correctly identified. Its definition is:\n$$R = \\frac{TP}{P_{actual}} = \\frac{TP}{TP + FN}$$\nFrom this, we can express $TP$ in terms of $N$, $\\pi$, and $R$:\n$$TP = R \\cdot P_{actual} = R N\\pi$$\nThe number of false negatives are the actual positives that were not identified as positive:\n$$FN = P_{actual} - TP = N\\pi - R N\\pi = N\\pi(1-R)$$\n\nSpecificity, or the True Negative Rate, is the fraction of actual negatives that are correctly identified. Its definition is:\n$$S = \\frac{TN}{N_{actual}} = \\frac{TN}{TN + FP}$$\nFrom this, we can express $TN$ in terms of $N$, $\\pi$, and $S$:\n$$TN = S \\cdot N_{actual} = S N(1-\\pi)$$\nThe number of false positives are the actual negatives that were not identified as negative:\n$$FP = N_{actual} - TN = N(1-\\pi) - S N(1-\\pi) = N(1-\\pi)(1-S)$$\n\nSummarizing the confusion matrix counts in terms of the given parameters:\n- $TP = N\\pi R$\n- $FN = N\\pi(1-R)$\n- $TN = N(1-\\pi)S$\n- $FP = N(1-\\pi)(1-S)$\n\nNext, we deduce the expression for Precision ($P$). Precision, or the Positive Predictive Value, is the fraction of predicted positives that are actually positive. Its definition is:\n$$P = \\frac{TP}{TP + FP}$$\nSubstituting the expressions for $TP$ and $FP$:\n$$P = \\frac{N\\pi R}{N\\pi R + N(1-\\pi)(1-S)}$$\nAssuming $N \\neq 0$, we can cancel $N$ from the numerator and denominator:\n$$P = \\frac{\\pi R}{\\pi R + (1-\\pi)(1-S)}$$\n\nNow, we derive the closed-form expression for the F-one score ($F_1$). The $F_1$-score is defined as the harmonic mean of Precision and Recall. A more direct definition in terms of the confusion matrix counts is:\n$$F_1 = \\frac{2TP}{2TP + FP + FN}$$\nThis form is computationally robust as it avoids potential division-by-zero issues if Precision is undefined (i.e., if $TP+FP=0$).\nSubstituting the expressions for $TP$, $FP$, and $FN$:\n$$F_1 = \\frac{2(N\\pi R)}{2(N\\pi R) + N(1-\\pi)(1-S) + N\\pi(1-R)}$$\nAgain, assuming $N \\neq 0$, we cancel $N$ from all terms:\n$$F_1 = \\frac{2\\pi R}{2\\pi R + (1-\\pi)(1-S) + \\pi(1-R)}$$\nTo simplify, we expand and collect terms in the denominator:\n$$ \\text{Denominator} = 2\\pi R + (1 - S - \\pi + \\pi S) + (\\pi - \\pi R) $$\n$$ \\text{Denominator} = (2\\pi R - \\pi R) + (-\\pi + \\pi) + 1 - S + \\pi S $$\n$$ \\text{Denominator} = \\pi R + 1 - S + \\pi S $$\nThis can be factored to group terms involving $\\pi$:\n$$ \\text{Denominator} = \\pi(R+S) + 1 - S $$\nThus, the final simplified expression for the F-one score is:\n$$F_1 = \\frac{2\\pi R}{\\pi(R+S) + 1 - S}$$\n\nFinally, we state the feasibility constraints.\nThe given parameters $\\pi, R, S$ are defined in the interval $[0,1]$.\nThe number of samples, $N$, must be a non-negative integer, typically $N \\in \\mathbb{Z}^+$.\nFor the problem to be interpretable on a real dataset, the counts $TP$, $FP$, $TN$, and $FN$ must be non-negative integers.\n- Non-negativity is guaranteed since $N \\ge 0$ and $\\pi, R, S \\in [0,1]$.\n- For the counts to be integers, the following conditions must hold:\n  1. The number of actual positives, $N\\pi$, must be an integer.\n  2. The number of actual negatives, $N(1-\\pi)$, must be an integer. (This is implied by condition 1 if $N$ is an integer).\n  3. The number of true positives, $TP = (N\\pi)R$, must be an integer.\n  4. The number of true negatives, $TN = N(1-\\pi)S$, must be an integer.\nThe derived expression for $F_1$ is well-defined as long as its denominator is not zero. The denominator $\\pi(R+S) + 1 - S$ is zero if and only if $\\pi=0$ and $S=1$. This corresponds to a scenario with no positive instances and a classifier that makes no positive predictions ($FP=0$). In this specific case, $TP$, $FP$, and $FN$ are all $0$, leading to an $F_1$ score of $0$ by convention. Our formula yields the indeterminate form $\\frac{0}{0}$ in this case, but correctly evaluates to $0$ in the limit as $\\pi \\to 0$ for any $S1$.", "answer": "$$\\boxed{\\frac{2\\pi R}{\\pi(R+S) + 1 - S}}$$", "id": "3094141"}, {"introduction": "The choice of model complexity is central to statistical learning and involves a critical trade-off. A simple model may fail to capture the underlying patterns in the data, resulting in high bias, while an overly complex model may learn the noise in the training set, leading to high variance and poor performance on new data. This practice provides a hands-on derivation of the classic bias-variance decomposition, offering a clear, quantitative view of how model complexity (represented by polynomial degree $k$), sample size $n$, and inherent data noise $\\sigma^2$ together shape a model's expected prediction error. [@problem_id:3121938]", "problem": "Consider a one-dimensional regression problem on the interval $[-1,1]$. Let $x$ be drawn from the uniform distribution on $[-1,1]$, and let the response be generated by $y = f^{\\star}(x) + \\varepsilon$, where $\\varepsilon$ is independent noise with $\\mathbb{E}[\\varepsilon \\mid x] = 0$ and $\\operatorname{Var}(\\varepsilon \\mid x) = \\sigma^{2}$. Assume the unknown regression function $f^{\\star}$ admits an $L^{2}$ expansion with respect to an orthonormal polynomial basis $\\{\\phi_{j}\\}_{j \\ge 0}$ on $[-1,1]$ under the uniform measure, namely $f^{\\star}(x) = \\sum_{j=0}^{\\infty} \\theta_{j} \\phi_{j}(x)$ with $\\sum_{j=0}^{\\infty} \\theta_{j}^{2}  \\infty$, where $\\int_{-1}^{1} \\phi_{j}(x)\\phi_{\\ell}(x)\\,\\frac{dx}{2} = \\mathbf{1}\\{j=\\ell\\}$.\n\nYou collect a training sample $\\{(x_{i},y_{i})\\}_{i=1}^{n}$ of size $n$, with the $x_{i}$ chosen deterministically so that the first $k+1$ basis functions are empirically orthonormal and orthogonal to all higher-degree basis functions with respect to the discrete uniform measure on the design points:\n\n$$\n\\frac{1}{n}\\sum_{i=1}^{n} \\phi_{j}(x_{i})\\,\\phi_{\\ell}(x_{i}) = \n\\begin{cases}\n1,  j=\\ell \\le k,\\\\\n0,  j \\ne \\ell,\\; j,\\ell \\le k,\\\\\n0,  \\ell \\le k  j.\n\\end{cases}\n$$\n\nYou fit the degree-$k$ polynomial least-squares predictor $\\widehat{f}_{k}(x) = \\sum_{j=0}^{k} \\widehat{\\beta}_{j}\\,\\phi_{j}(x)$ by ordinary least squares on the features $\\{\\phi_{0},\\dots,\\phi_{k}\\}$.\n\nUsing only first principles (namely, the definitions of conditional expectation and variance, the orthonormality relations above, and the normal equations for least squares), derive the bias-variance decomposition of the expected integrated squared prediction error for a fresh test point $(x,y)$ drawn independently from the same data-generating process:\n\n$$\n\\mathcal{R}_{k,n} \\equiv \\mathbb{E}\\big[(y - \\widehat{f}_{k}(x))^{2}\\big],\n$$\n\nwhere the expectation is over the training noise, the training inputs, the test input $x$, and the test noise. Express your final result in closed form as a function of $k$, $n$, $\\sigma^{2}$, and the coefficients $\\{\\theta_{j}\\}_{j \\ge 0}$. Your final answer must be a single closed-form analytic expression. No rounding is required.", "solution": "The quantity to be analyzed is the expected integrated squared prediction error, given by\n$$ \\mathcal{R}_{k,n} \\equiv \\mathbb{E}\\big[(y - \\widehat{f}_{k}(x))^{2}\\big] $$\nThe total expectation $\\mathbb{E}$ is taken over all sources of randomness: the test point's input $x$ and its associated noise $\\varepsilon$, and the noise in the training data, which we denote as $\\{\\varepsilon_{i}\\}_{i=1}^{n}$. The training inputs $\\{x_i\\}_{i=1}^n$ are deterministic. The fitted model $\\widehat{f}_k$ depends on the training data $\\{(x_i, y_i)\\}_{i=1}^n$ and thus on the training noise $\\{\\varepsilon_i\\}_{i=1}^n$.\n\nWe begin by applying the law of total expectation, conditioning on the training data $\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^n$ and the test input $x$. The response $y$ is given by $y = f^{\\star}(x)+\\varepsilon$.\n$$ \\mathcal{R}_{k,n} = \\mathbb{E}_{\\mathcal{D}, x} \\left[ \\mathbb{E}_{\\varepsilon | \\mathcal{D}, x} \\left[ (f^{\\star}(x) + \\varepsilon - \\widehat{f}_{k}(x))^2 \\mid \\mathcal{D}, x \\right] \\right] $$\nExpanding the square, we get:\n$$ (f^{\\star}(x) - \\widehat{f}_{k}(x))^2 + 2\\varepsilon(f^{\\star}(x) - \\widehat{f}_{k}(x)) + \\varepsilon^2 $$\nThe conditional expectation of the cross-term is zero, since the test noise $\\varepsilon$ is independent of $\\mathcal{D}$ and $x$, and $\\mathbb{E}[\\varepsilon | x] = 0$:\n$$ \\mathbb{E}_{\\varepsilon | \\mathcal{D}, x} \\left[ 2\\varepsilon(f^{\\star}(x) - \\widehat{f}_{k}(x)) \\mid \\mathcal{D}, x \\right] = 2(f^{\\star}(x) - \\widehat{f}_{k}(x))\\mathbb{E}_{\\varepsilon | x}[\\varepsilon] = 0 $$\nThe conditional expectation of the $\\varepsilon^2$ term is the conditional variance of the noise, since its mean is zero:\n$$ \\mathbb{E}_{\\varepsilon | \\mathcal{D}, x} [\\varepsilon^2 \\mid \\mathcal{D}, x] = \\mathbb{E}_{\\varepsilon|x}[\\varepsilon^2 | x] = \\operatorname{Var}(\\varepsilon|x) + (\\mathbb{E}[\\varepsilon|x])^2 = \\sigma^2 + 0^2 = \\sigma^2 $$\nSubstituting these back, the risk becomes:\n$$ \\mathcal{R}_{k,n} = \\mathbb{E}_{\\mathcal{D}, x} \\left[ (f^{\\star}(x) - \\widehat{f}_{k}(x))^2 + \\sigma^2 \\right] = \\mathbb{E}_{\\mathcal{D}, x} \\left[ (f^{\\star}(x) - \\widehat{f}_{k}(x))^2 \\right] + \\sigma^2 $$\nThe term $\\sigma^2$ is the irreducible error. The remaining term is the Mean Integrated Squared Error (MISE). We decompose it into bias and variance components by introducing the average predictor function $\\bar{f}_k(x) = \\mathbb{E}_{\\mathcal{D}}[\\widehat{f}_k(x)]$.\n$$ \\mathbb{E}_{\\mathcal{D}, x} \\left[ (f^{\\star}(x) - \\widehat{f}_{k}(x))^2 \\right] = \\mathbb{E}_{\\mathcal{D}, x} \\left[ (f^{\\star}(x) - \\bar{f}_k(x) + \\bar{f}_k(x) - \\widehat{f}_{k}(x))^2 \\right] $$\nExpanding the square and noting that the cross-term $\\mathbb{E}_{\\mathcal{D}}[(f^{\\star}(x) - \\bar{f}_k(x))(\\bar{f}_k(x) - \\widehat{f}_{k}(x))]$ vanishes because $\\mathbb{E}_{\\mathcal{D}}[\\bar{f}_k(x) - \\widehat{f}_{k}(x)] = 0$, we get the standard decomposition:\n$$ \\mathcal{R}_{k,n} = \\underbrace{\\mathbb{E}_{x}\\left[(f^{\\star}(x) - \\bar{f}_k(x))^2\\right]}_{\\text{Integrated Squared Bias}} + \\underbrace{\\mathbb{E}_{\\mathcal{D},x}\\left[(\\widehat{f}_{k}(x) - \\bar{f}_k(x))^2\\right]}_{\\text{Integrated Variance}} + \\sigma^2 $$\nWe now derive each term. This requires finding the ordinary least squares (OLS) coefficients $\\widehat{\\beta}_j$. The OLS estimator minimizes $\\sum_{i=1}^n (y_i - \\sum_{j=0}^k \\beta_j \\phi_j(x_i))^2$. The normal equations are:\n$$ \\sum_{j=0}^{k} \\widehat{\\beta}_j \\left(\\frac{1}{n}\\sum_{i=1}^{n} \\phi_j(x_i) \\phi_{\\ell}(x_i)\\right) = \\frac{1}{n}\\sum_{i=1}^{n} y_i \\phi_{\\ell}(x_i), \\quad \\text{for } \\ell=0, \\dots, k $$\nUsing the given empirical orthonormality condition on the design points $\\{x_i\\}$, the left side simplifies to $\\widehat{\\beta}_{\\ell}$. Thus, for $j=0, \\dots, k$:\n$$ \\widehat{\\beta}_{j} = \\frac{1}{n} \\sum_{i=1}^{n} y_i \\phi_j(x_i) $$\nNext, we find the average predictor $\\bar{f}_k(x) = \\sum_{j=0}^k \\mathbb{E}_{\\mathcal{D}}[\\widehat{\\beta}_j] \\phi_j(x)$. The expectation is over the training noise. Since $y_i = f^{\\star}(x_i) + \\varepsilon_i$ and $\\mathbb{E}[\\varepsilon_i|x_i]=0$, we have $\\mathbb{E}_{\\mathcal{D}}[y_i] = f^{\\star}(x_i)$.\n$$ \\mathbb{E}_{\\mathcal{D}}[\\widehat{\\beta}_j] = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{E}_{\\mathcal{D}}[y_i] \\phi_j(x_i) = \\frac{1}{n} \\sum_{i=1}^{n} f^{\\star}(x_i) \\phi_j(x_i) $$\nSubstituting the expansion $f^{\\star}(x_i) = \\sum_{\\ell=0}^{\\infty} \\theta_{\\ell} \\phi_{\\ell}(x_i)$:\n$$ \\mathbb{E}_{\\mathcal{D}}[\\widehat{\\beta}_j] = \\frac{1}{n} \\sum_{i=1}^{n} \\left(\\sum_{\\ell=0}^{\\infty} \\theta_{\\ell} \\phi_{\\ell}(x_i)\\right) \\phi_j(x_i) = \\sum_{\\ell=0}^{\\infty} \\theta_{\\ell} \\left(\\frac{1}{n}\\sum_{i=1}^{n} \\phi_{\\ell}(x_i) \\phi_j(x_i)\\right) $$\nUsing the provided empirical orthonormality conditions for $j \\le k$, the term in parentheses is $\\mathbf{1}\\{j=\\ell\\}$ if $\\ell \\le k$ and $0$ if $\\ell  k$. Thus, for $j \\le k$:\n$$ \\mathbb{E}_{\\mathcal{D}}[\\widehat{\\beta}_j] = \\theta_j $$\nThe average predictor is the projection of $f^{\\star}$ onto the chosen function space:\n$$ \\bar{f}_k(x) = \\sum_{j=0}^{k} \\theta_j \\phi_j(x) $$\nNow we can compute the bias term. The expectation $\\mathbb{E}_x$ corresponds to integration with respect to $\\frac{dx}{2}$ over $[-1,1]$.\n$$ \\text{Bias}^2 = \\mathbb{E}_{x}\\left[ \\left( f^{\\star}(x) - \\bar{f}_k(x) \\right)^2 \\right] = \\mathbb{E}_{x}\\left[ \\left( \\sum_{j=0}^{\\infty} \\theta_j \\phi_j(x) - \\sum_{j=0}^{k} \\theta_j \\phi_j(x) \\right)^2 \\right] $$\n$$ = \\mathbb{E}_{x}\\left[ \\left( \\sum_{j=k+1}^{\\infty} \\theta_j \\phi_j(x) \\right)^2 \\right] = \\sum_{j=k+1}^{\\infty} \\sum_{\\ell=k+1}^{\\infty} \\theta_j \\theta_{\\ell} \\mathbb{E}_{x}[\\phi_j(x)\\phi_{\\ell}(x)] $$\nUsing the population orthonormality, $\\mathbb{E}_{x}[\\phi_j(x)\\phi_{\\ell}(x)] = \\int_{-1}^1 \\phi_j(x)\\phi_{\\ell}(x)\\frac{dx}{2} = \\mathbf{1}\\{j=\\ell\\}$, the bias term is:\n$$ \\text{Integrated Squared Bias} = \\sum_{j=k+1}^{\\infty} \\theta_j^2 $$\nNext, we compute the variance term.\n$$ \\text{Variance} = \\mathbb{E}_{\\mathcal{D},x}\\left[ (\\widehat{f}_{k}(x) - \\bar{f}_k(x))^2 \\right] = \\mathbb{E}_{\\mathcal{D}} \\left[ \\mathbb{E}_{x} \\left[ \\left( \\sum_{j=0}^{k} (\\widehat{\\beta}_j - \\theta_j)\\phi_j(x) \\right)^2 \\right] \\right] $$\nThe inner expectation over $x$ simplifies due to population orthonormality:\n$$ \\mathbb{E}_{x} \\left[ \\sum_{j=0}^{k} \\sum_{\\ell=0}^{k} (\\widehat{\\beta}_j - \\theta_j)(\\widehat{\\beta}_{\\ell} - \\theta_{\\ell}) \\phi_j(x)\\phi_{\\ell}(x) \\right] = \\sum_{j=0}^{k} (\\widehat{\\beta}_j - \\theta_j)^2 $$\nThe integrated variance is then the expectation over the training data:\n$$ \\text{Integrated Variance} = \\mathbb{E}_{\\mathcal{D}}\\left[ \\sum_{j=0}^{k} (\\widehat{\\beta}_j - \\theta_j)^2 \\right] = \\sum_{j=0}^{k} \\mathbb{E}_{\\mathcal{D}}\\left[ (\\widehat{\\beta}_j - \\theta_j)^2 \\right] = \\sum_{j=0}^{k} \\operatorname{Var}_{\\mathcal{D}}(\\widehat{\\beta}_j) $$\nsince $\\mathbb{E}_{\\mathcal{D}}[\\widehat{\\beta}_j] = \\theta_j$. We compute the variance of the coefficients.\n$$ \\widehat{\\beta}_j - \\theta_j = \\left( \\frac{1}{n}\\sum_{i=1}^n y_i \\phi_j(x_i) \\right) - \\left( \\frac{1}{n}\\sum_{i=1}^n f^{\\star}(x_i) \\phi_j(x_i) \\right) = \\frac{1}{n}\\sum_{i=1}^n \\varepsilon_i \\phi_j(x_i) $$\nThe noises $\\{\\varepsilon_i\\}$ are independent with variance $\\sigma^2$, and the $\\phi_j(x_i)$ are deterministic constants.\n$$ \\operatorname{Var}_{\\mathcal{D}}(\\widehat{\\beta}_j) = \\operatorname{Var}\\left( \\frac{1}{n}\\sum_{i=1}^n \\varepsilon_i \\phi_j(x_i) \\right) = \\frac{1}{n^2} \\sum_{i=1}^n \\operatorname{Var}(\\varepsilon_i \\phi_j(x_i)) = \\frac{1}{n^2} \\sum_{i=1}^n (\\phi_j(x_i))^2 \\operatorname{Var}(\\varepsilon_i) $$\n$$ = \\frac{\\sigma^2}{n^2} \\sum_{i=1}^n (\\phi_j(x_i))^2 = \\frac{\\sigma^2}{n} \\left(\\frac{1}{n}\\sum_{i=1}^n \\phi_j(x_i)\\phi_j(x_i)\\right) $$\nUsing the empirical orthonormality for $j \\le k$, the term in parentheses is $1$.\n$$ \\operatorname{Var}_{\\mathcal{D}}(\\widehat{\\beta}_j) = \\frac{\\sigma^2}{n} $$\nThe integrated variance is the sum over $j=0, \\dots, k$:\n$$ \\text{Integrated Variance} = \\sum_{j=0}^{k} \\frac{\\sigma^2}{n} = \\frac{(k+1)\\sigma^2}{n} $$\nFinally, we assemble the three components to obtain the total expected error $\\mathcal{R}_{k,n}$.\n$$ \\mathcal{R}_{k,n} = \\text{Integrated Squared Bias} + \\text{Integrated Variance} + \\text{Irreducible Error} $$\n$$ \\mathcal{R}_{k,n} = \\sum_{j=k+1}^{\\infty} \\theta_j^2 + \\frac{(k+1)\\sigma^2}{n} + \\sigma^2 $$\nThis expression represents the complete bias-variance decomposition of the expected prediction error.", "answer": "$$\\boxed{\\sigma^2 + \\frac{\\sigma^2(k+1)}{n} + \\sum_{j=k+1}^{\\infty} \\theta_j^2}$$", "id": "3121938"}, {"introduction": "In modern high-dimensional statistics, we often manage the bias-variance trade-off using regularization, for example by constraining the $\\ell_1$ or $\\ell_2$ norm of a model's weight vector. This exercise introduces Rademacher complexity, a sophisticated tool for measuring a model class's capacity to fit random noise, which provides powerful generalization bounds. By deriving and comparing complexity bounds for $\\ell_1$ and $\\ell_2$ regularized models, you will gain a theoretical justification for a widely used heuristic: that $\\ell_1$ regularization (Lasso) is particularly effective for problems where the true underlying signal is sparse. [@problem_id:3121905]", "problem": "Consider a binary classification problem in $\\mathbb{R}^{d}$ with linear predictors $f_{\\mathbf{w}}(\\mathbf{x}) = \\mathbf{w}^{\\top} \\mathbf{x}$, learned by Empirical Risk Minimization (ERM) under norm constraints. Let the loss function $\\ell(y, f_{\\mathbf{w}}(\\mathbf{x}))$ be $1$-Lipschitz in its second argument and bounded in $[0,1]$. Suppose the input $\\mathbf{x}$ satisfies, almost surely, both $\\|\\mathbf{x}\\|_{\\infty} \\leq r$ and $\\|\\mathbf{x}\\|_{2} \\leq R$, where $r  0$, $R  0$, and $d \\geq 2$.\n\nDefine the two hypothesis classes:\n$$\n\\mathcal{F}_{1} = \\left\\{ \\mathbf{x} \\mapsto \\mathbf{w}^{\\top}\\mathbf{x} \\; : \\; \\|\\mathbf{w}\\|_{1} \\leq B_{1} \\right\\}\n\\quad\\text{and}\\quad\n\\mathcal{F}_{2} = \\left\\{ \\mathbf{x} \\mapsto \\mathbf{w}^{\\top}\\mathbf{x} \\; : \\; \\|\\mathbf{w}\\|_{2} \\leq B_{2} \\right\\},\n$$\nwith $B_{1}, B_{2}  0$. Assume the unknown target parameter $\\mathbf{w}^{\\star}$ is $s$-sparse, meaning it has at most $s$ nonzero coordinates, and satisfies $\\|\\mathbf{w}^{\\star}\\|_{2} \\leq B_{2}$. Under this sparsity, use the inequality $\\|\\mathbf{w}^{\\star}\\|_{1} \\leq \\sqrt{s}\\,\\|\\mathbf{w}^{\\star}\\|_{2}$ to set $B_{1} = \\sqrt{s}\\,B_{2}$ so that both $\\mathcal{F}_{1}$ and $\\mathcal{F}_{2}$ contain $\\mathbf{w}^{\\star}$.\n\nLet $S = \\{(\\mathbf{x}_{i}, y_{i})\\}_{i=1}^{n}$ be a sample of size $n \\geq 2$. Starting from the definition of empirical Rademacher complexity,\n$$\n\\mathfrak{R}_{n}(\\mathcal{F}) = \\mathbb{E}_{\\boldsymbol{\\sigma}, S} \\left[ \\sup_{f \\in \\mathcal{F}} \\frac{1}{n} \\sum_{i=1}^{n} \\sigma_{i} f(\\mathbf{x}_{i}) \\right],\n$$\nwhere $\\sigma_{1},\\dots,\\sigma_{n}$ are independent Rademacher random variables taking values in $\\{-1,+1\\}$, derive upper bounds on $\\mathfrak{R}_{n}(\\mathcal{F}_{1})$ and $\\mathfrak{R}_{n}(\\mathcal{F}_{2})$ in terms of $B_{1}$, $B_{2}$, $r$, $R$, $d$, and $n$, by appealing only to dual norm relationships and basic concentration for sums of Rademacher random variables. Then, using a standard generalization argument that smaller empirical Rademacher complexity yields a smaller ERM generalization gap for $1$-Lipschitz bounded losses, compute the largest sparsity level $s_{\\star}$ (as a closed-form symbolic expression depending on $d$, $r$, and $R$) such that the upper bound on $\\mathfrak{R}_{n}(\\mathcal{F}_{1})$ is less than or equal to the upper bound on $\\mathfrak{R}_{n}(\\mathcal{F}_{2})$. Provide your final answer as the expression $s_{\\star} = \\cdots$.", "solution": "The problem requires the derivation of upper bounds on the empirical Rademacher complexities for two hypothesis classes, $\\mathcal{F}_{1}$ and $\\mathcal{F}_{2}$, and then finding the maximum sparsity level $s_{\\star}$ for which the bound for $\\mathcal{F}_{1}$ is tighter than or equal to the bound for $\\mathcal{F}_{2}$.\n\nLet $S = \\{(\\mathbf{x}_{i}, y_{i})\\}_{i=1}^{n}$ be a fixed sample. The empirical Rademacher complexity is defined as:\n$$\n\\hat{\\mathfrak{R}}_{S}(\\mathcal{F}) = \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\sup_{f \\in \\mathcal{F}} \\frac{1}{n} \\sum_{i=1}^{n} \\sigma_{i} f(\\mathbf{x}_{i}) \\right]\n$$\nThe total Rademacher complexity is $\\mathfrak{R}_{n}(\\mathcal{F}) = \\mathbb{E}_{S}[\\hat{\\mathfrak{R}}_{S}(\\mathcal{F})]$. We will derive upper bounds on $\\hat{\\mathfrak{R}}_{S}$ that are independent of the specific sample $S$, which will then directly serve as bounds on $\\mathfrak{R}_{n}(\\mathcal{F})$.\n\n**Step 1: Derivation of the upper bound for $\\mathfrak{R}_{n}(\\mathcal{F}_{1})$**\n\nThe hypothesis class $\\mathcal{F}_{1}$ is defined as $\\mathcal{F}_{1} = \\{ \\mathbf{x} \\mapsto \\mathbf{w}^{\\top}\\mathbf{x} \\; : \\; \\|\\mathbf{w}\\|_{1} \\leq B_{1} \\}$.\nThe empirical Rademacher complexity for $\\mathcal{F}_{1}$ is:\n$$\n\\hat{\\mathfrak{R}}_{S}(\\mathcal{F}_{1}) = \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\sup_{\\|\\mathbf{w}\\|_{1} \\leq B_{1}} \\frac{1}{n} \\sum_{i=1}^{n} \\sigma_{i} \\mathbf{w}^{\\top}\\mathbf{x}_{i} \\right]\n$$\nBy linearity of the inner product, we can rearrange the summation:\n$$\n\\hat{\\mathfrak{R}}_{S}(\\mathcal{F}_{1}) = \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\sup_{\\|\\mathbf{w}\\|_{1} \\leq B_{1}} \\mathbf{w}^{\\top} \\left( \\frac{1}{n} \\sum_{i=1}^{n} \\sigma_{i} \\mathbf{x}_{i} \\right) \\right]\n$$\nThe expression inside the expectation is of the form $\\sup_{\\|\\mathbf{w}\\|_{p} \\leq B} \\mathbf{w}^{\\top}\\mathbf{v}$. By the definition of a dual norm, this supremum is equal to $B \\|\\mathbf{v}\\|_{q}$, where $\\|\\cdot\\|_{q}$ is the dual norm to $\\|\\cdot\\|_{p}$. The dual norm of the $\\ell_1$-norm is the $\\ell_{\\infty}$-norm. Thus, we have:\n$$\n\\sup_{\\|\\mathbf{w}\\|_{1} \\leq B_{1}} \\mathbf{w}^{\\top} \\left( \\frac{1}{n} \\sum_{i=1}^{n} \\sigma_{i} \\mathbf{x}_{i} \\right) = B_{1} \\left\\| \\frac{1}{n} \\sum_{i=1}^{n} \\sigma_{i} \\mathbf{x}_{i} \\right\\|_{\\infty}\n$$\nSubstituting this back into the expression for $\\hat{\\mathfrak{R}}_{S}(\\mathcal{F}_{1})$:\n$$\n\\hat{\\mathfrak{R}}_{S}(\\mathcal{F}_{1}) = \\frac{B_{1}}{n} \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\left\\| \\sum_{i=1}^{n} \\sigma_{i} \\mathbf{x}_{i} \\right\\|_{\\infty} \\right]\n$$\nTo bound the expectation, we use a standard maximal inequality for sums of independent random vectors. For a set of vectors $\\{\\mathbf{v}_1, \\dots, \\mathbf{v}_n\\}$ in $\\mathbb{R}^d$, a bound derived from Hoeffding's inequality and a union bound is:\n$$\n\\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\left\\| \\sum_{i=1}^{n} \\sigma_{i} \\mathbf{v}_{i} \\right\\|_{\\infty} \\right] \\leq \\sqrt{2 \\ln(2d)} \\max_{j \\in \\{1,\\dots,d\\}} \\sqrt{\\sum_{i=1}^{n} v_{ij}^{2}}\n$$\nIn our case, $\\mathbf{v}_i = \\mathbf{x}_i$. We are given that $\\|\\mathbf{x}_{i}\\|_{\\infty} \\leq r$ for all $i$, which implies that $|x_{ij}| \\leq r$ for all $i,j$. We can bound the sum of squares:\n$$\n\\sum_{i=1}^{n} x_{ij}^{2} \\leq \\sum_{i=1}^{n} r^{2} = n r^{2}\n$$\nThis holds for any coordinate $j$. Therefore, $\\max_{j} \\sqrt{\\sum_{i=1}^{n} x_{ij}^{2}} \\leq \\sqrt{n r^{2}} = r\\sqrt{n}$.\nPlugging this into the maximal inequality:\n$$\n\\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\left\\| \\sum_{i=1}^{n} \\sigma_{i} \\mathbf{x}_{i} \\right\\|_{\\infty} \\right] \\leq r \\sqrt{n} \\sqrt{2 \\ln(2d)}\n$$\nFinally, we substitute this back into the expression for $\\hat{\\mathfrak{R}}_{S}(\\mathcal{F}_{1})$:\n$$\n\\hat{\\mathfrak{R}}_{S}(\\mathcal{F}_{1}) \\leq \\frac{B_{1}}{n} \\left( r \\sqrt{n} \\sqrt{2 \\ln(2d)} \\right) = \\frac{B_{1} r \\sqrt{2 \\ln(2d)}}{\\sqrt{n}}\n$$\nSince this upper bound is independent of the sample $S$, it is also an upper bound for the full Rademacher complexity $\\mathfrak{R}_{n}(\\mathcal{F}_{1})$.\n\n**Step 2: Derivation of the upper bound for $\\mathfrak{R}_{n}(\\mathcal{F}_{2})$**\n\nThe hypothesis class $\\mathcal{F}_{2}$ is defined as $\\mathcal{F}_{2} = \\{ \\mathbf{x} \\mapsto \\mathbf{w}^{\\top}\\mathbf{x} \\; : \\; \\|\\mathbf{w}\\|_{2} \\leq B_{2} \\}$.\nThe empirical Rademacher complexity for $\\mathcal{F}_{2}$ is:\n$$\n\\hat{\\mathfrak{R}}_{S}(\\mathcal{F}_{2}) = \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\sup_{\\|\\mathbf{w}\\|_{2} \\leq B_{2}} \\frac{1}{n} \\sum_{i=1}^{n} \\sigma_{i} \\mathbf{w}^{\\top}\\mathbf{x}_{i} \\right] = \\frac{1}{n} \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\sup_{\\|\\mathbf{w}\\|_{2} \\leq B_{2}} \\mathbf{w}^{\\top} \\left( \\sum_{i=1}^{n} \\sigma_{i} \\mathbf{x}_{i} \\right) \\right]\n$$\nThe $\\ell_2$-norm is self-dual. Therefore:\n$$\n\\sup_{\\|\\mathbf{w}\\|_{2} \\leq B_{2}} \\mathbf{w}^{\\top} \\left( \\sum_{i=1}^{n} \\sigma_{i} \\mathbf{x}_{i} \\right) = B_{2} \\left\\| \\sum_{i=1}^{n} \\sigma_{i} \\mathbf{x}_{i} \\right\\|_{2}\n$$\nSubstituting this back, we get:\n$$\n\\hat{\\mathfrak{R}}_{S}(\\mathcal{F}_{2}) = \\frac{B_{2}}{n} \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\left\\| \\sum_{i=1}^{n} \\sigma_{i} \\mathbf{x}_{i} \\right\\|_{2} \\right]\n$$\nWe can bound the expectation using Jensen's inequality, since the square root function is concave: $\\mathbb{E}[\\sqrt{X}] \\leq \\sqrt{\\mathbb{E}[X]}$.\n$$\n\\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\left\\| \\sum_{i=1}^{n} \\sigma_{i} \\mathbf{x}_{i} \\right\\|_{2} \\right] = \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\sqrt{\\left\\| \\sum_{i=1}^{n} \\sigma_{i} \\mathbf{x}_{i} \\right\\|_{2}^{2}} \\right] \\leq \\sqrt{\\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\left\\| \\sum_{i=1}^{n} \\sigma_{i} \\mathbf{x}_{i} \\right\\|_{2}^{2} \\right]}\n$$\nLet's compute the argument of the square root:\n$$\n\\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\left\\| \\sum_{i=1}^{n} \\sigma_{i} \\mathbf{x}_{i} \\right\\|_{2}^{2} \\right] = \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\left(\\sum_{i=1}^{n} \\sigma_{i} \\mathbf{x}_{i}\\right)^{\\top} \\left(\\sum_{j=1}^{n} \\sigma_{j} \\mathbf{x}_{j}\\right) \\right] = \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\sum_{i=1}^{n}\\sum_{j=1}^{n} \\sigma_{i}\\sigma_{j} (\\mathbf{x}_{i}^{\\top}\\mathbf{x}_{j}) \\right]\n$$\nBy linearity of expectation, we move the expectation inside the sums. Since $\\sigma_i$ are independent Rademacher variables, $\\mathbb{E}[\\sigma_i]=0$ and $\\mathbb{E}[\\sigma_i^2]=1$. Thus, $\\mathbb{E}[\\sigma_i \\sigma_j] = \\delta_{ij}$ (the Kronecker delta).\n$$\n\\sum_{i=1}^{n}\\sum_{j=1}^{n} (\\mathbf{x}_{i}^{\\top}\\mathbf{x}_{j}) \\mathbb{E}_{\\boldsymbol{\\sigma}}[\\sigma_{i}\\sigma_{j}] = \\sum_{i=1}^{n} (\\mathbf{x}_{i}^{\\top}\\mathbf{x}_{i}) \\mathbb{E}[\\sigma_{i}^{2}] = \\sum_{i=1}^{n} \\|\\mathbf{x}_{i}\\|_{2}^{2}\n$$\nWe are given that $\\|\\mathbf{x}_{i}\\|_{2} \\leq R$ for all $i$. So, $\\sum_{i=1}^{n} \\|\\mathbf{x}_{i}\\|_{2}^{2} \\leq \\sum_{i=1}^{n} R^{2} = nR^{2}$.\nPutting this together:\n$$\n\\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\left\\| \\sum_{i=1}^{n} \\sigma_{i} \\mathbf{x}_{i} \\right\\|_{2} \\right] \\leq \\sqrt{\\sum_{i=1}^{n} \\|\\mathbf{x}_{i}\\|_{2}^{2}} \\leq \\sqrt{nR^{2}} = R\\sqrt{n}\n$$\nSubstituting this into the expression for $\\hat{\\mathfrak{R}}_{S}(\\mathcal{F}_{2})$:\n$$\n\\hat{\\mathfrak{R}}_{S}(\\mathcal{F}_{2}) \\leq \\frac{B_{2}}{n} (R\\sqrt{n}) = \\frac{B_{2}R}{\\sqrt{n}}\n$$\nThis upper bound is also independent of $S$, so it is an upper bound for $\\mathfrak{R}_{n}(\\mathcal{F}_{2})$.\n\n**Step 3: Comparison of the bounds**\n\nWe want to find the largest sparsity level $s_{\\star}$ such that the upper bound for $\\mathfrak{R}_{n}(\\mathcal{F}_{1})$ is less than or equal to the upper bound for $\\mathfrak{R}_{n}(\\mathcal{F}_{2})$:\n$$\n\\frac{B_{1} r \\sqrt{2 \\ln(2d)}}{\\sqrt{n}} \\leq \\frac{B_{2}R}{\\sqrt{n}}\n$$\nSince $n \\geq 2$, we can multiply by $\\sqrt{n}$:\n$$\nB_{1} r \\sqrt{2 \\ln(2d)} \\leq B_{2}R\n$$\nThe problem specifies that we should set $B_{1} = \\sqrt{s}B_{2}$ to ensure the $s$-sparse vector $\\mathbf{w}^{\\star}$ is in $\\mathcal{F}_{1}$. Substituting this into the inequality:\n$$\n(\\sqrt{s}B_{2}) r \\sqrt{2 \\ln(2d)} \\leq B_{2}R\n$$\nSince $B_2  0$, we can divide by $B_{2}$:\n$$\n\\sqrt{s} \\, r \\sqrt{2 \\ln(2d)} \\leq R\n$$\nNow, we solve for $s$. Since $r, R  0$ and $d \\ge 2$, all terms are positive.\n$$\n\\sqrt{s} \\leq \\frac{R}{r \\sqrt{2 \\ln(2d)}}\n$$\nSquaring both sides gives the condition on $s$:\n$$\ns \\leq \\frac{R^2}{r^2 (2 \\ln(2d))} = \\frac{R^2}{2r^2 \\ln(2d)}\n$$\nThe largest value of $s$ that satisfies this inequality is the right-hand side. This is the desired sparsity level $s_{\\star}$.", "answer": "$$\\boxed{\\frac{R^2}{2 r^2 \\ln(2d)}}$$", "id": "3121905"}]}