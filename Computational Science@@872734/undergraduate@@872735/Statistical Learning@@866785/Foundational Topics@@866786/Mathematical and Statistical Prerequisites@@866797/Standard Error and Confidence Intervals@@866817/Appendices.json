{"hands_on_practices": [{"introduction": "Before diving into complex formulas, it's crucial to build intuition about what standard error represents. This exercise explores how the uncertainty of a regression estimate is not an intrinsic property but is deeply tied to the *design* of your data. By examining the confidence interval for the intercept, you will see how extrapolating far from the data's center can dramatically inflate uncertainty, providing a clear, geometric lesson on the risks of prediction beyond the scope of your observations [@problem_id:3176623].", "problem": "A researcher fits a simple linear regression of the form $y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$ with $\\varepsilon_i \\stackrel{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0,\\sigma^2)$ (independent and identically distributed normal with mean $0$ and variance $\\sigma^2$). Under the classical linear model, the ordinary least squares (OLS) estimators are unbiased and their sampling variability is governed by the design matrix and $\\sigma^2$.\n\nTo study how the uncertainty of the intercept estimator behaves when the predictor range is tiny, consider three experimental designs, each with $n = 30$ and the same error variance $\\sigma^2$:\n\n- Design A: $x_i$ are uniformly spaced on $[9.8, 10.2]$.\n- Design B: $x_i$ are uniformly spaced on $[0.0, 0.4]$.\n- Design C: $x_i$ are uniformly spaced on $[-0.2, 0.2]$.\n\nAssume that the fitted models produce similar residual standard deviations across the designs (so differences in the intercept’s uncertainty are driven by the $x$-design). You compute a $95\\%$ confidence interval for the intercept $\\beta_0$ in each design.\n\nWhich of the following statements best characterizes when the intercept’s confidence interval is uninformatively wide in this setting and correctly explains how the standard error of the intercept estimator, $\\operatorname{SE}(\\hat{\\beta}_0)$, depends on $\\bar{x}$ and $\\sum_{i=1}^{n} (x_i - \\bar{x})^2$?\n\nA. With a tiny predictor range, the intercept’s confidence interval can be extremely wide if $|\\bar{x}|$ is large, because extrapolating from data far from $x=0$ inflates $\\operatorname{SE}(\\hat{\\beta}_0)$. Specifically, $\\operatorname{SE}(\\hat{\\beta}_0)$ increases with $|\\bar{x}|$ and decreases with $\\sum_{i=1}^{n} (x_i - \\bar{x})^2$. Thus Design A yields a much wider intercept interval than Designs B and C.\n\nB. The intercept’s standard error increases with $\\sum_{i=1}^{n} (x_i - \\bar{x})^2$ and is essentially independent of $\\bar{x}$. Therefore a tiny predictor range directly narrows the intercept’s interval unless errors are heteroskedastic.\n\nC. A tiny predictor range always narrows the intercept’s confidence interval because less variability in $x$ reduces overall uncertainty; larger $|\\bar{x}|$ stabilizes the intercept due to higher leverage at large $x$.\n\nD. The intercept’s standard error is approximately the residual standard deviation divided by $\\sqrt{n}$, regardless of $\\bar{x}$ or the spread of $x_i$. Hence all three designs should yield similar intercept intervals if the residual variability is similar.", "solution": "The user wants me to follow the role of a meticulous STEM professor to solve a statistics problem. I will first validate the problem statement, then derive the solution based on first principles, and finally evaluate each option.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n-   **Model:** Simple linear regression, $y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$.\n-   **Error Term:** $\\varepsilon_i \\stackrel{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0,\\sigma^2)$ (independent and identically distributed Normal with mean $0$ and variance $\\sigma^2$).\n-   **Estimators:** Ordinary Least Squares (OLS).\n-   **Sample Size:** $n = 30$ for all designs.\n-   **Error Variance:** $\\sigma^2$ is the same across designs.\n-   **Assumption:** The residual standard deviation is similar across designs.\n-   **Design A:** $x_i$ are uniformly spaced on $[9.8, 10.2]$.\n-   **Design B:** $x_i$ are uniformly spaced on $[0.0, 0.4]$.\n-   **Design C:** $x_i$ are uniformly spaced on $[-0.2, 0.2]$.\n-   **Task:** Characterize the confidence interval for the intercept $\\beta_0$ and explain the dependence of its standard error, $\\operatorname{SE}(\\hat{\\beta}_0)$, on the predictor's mean $\\bar{x}$ and spread $\\sum_{i=1}^{n} (x_i - \\bar{x})^2$.\n\n**Step 2: Validate Using Extracted Givens**\n1.  **Scientifically Grounded:** The problem is firmly located within the classical linear regression model framework, a cornerstone of statistics. The model, assumptions, and concepts (standard error, confidence interval) are standard and mathematically sound.\n2.  **Well-Posed:** The problem provides sufficient information to determine the functional form of the standard error of the intercept estimator and to qualitatively compare its magnitude across the three specified designs. A unique conceptual conclusion can be reached.\n3.  **Objective:** The language is clear, precise, and free of subjective statements. The designs are specified objectively.\n4.  **Completeness:** The problem is self-contained. All necessary components ($n$, model form, error structure, and specifics of the predictor designs) are provided to analyze the uncertainty of the intercept. The assumption of similar residual standard deviations simplifies the comparison, focusing it on the design matrix as intended.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. It is a well-formulated conceptual question in regression analysis. I will now proceed with the solution derivation.\n\n### Solution Derivation\n\nThe analysis is centered on the formula for the variance, and consequently the standard error, of the OLS intercept estimator, $\\hat{\\beta}_0$. In a simple linear regression model, the OLS estimators are given by:\n$$ \\hat{\\beta}_1 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2} $$\n$$ \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x} $$\nwhere $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i$ and $\\bar{y} = \\frac{1}{n}\\sum_{i=1}^{n} y_i$.\n\nTo find the standard error of $\\hat{\\beta}_0$, we must first derive its variance. We use the properties of variance and the fact that under the model assumptions, $\\operatorname{Cov}(\\bar{y}, \\hat{\\beta}_1) = 0$.\n$$ \\operatorname{Var}(\\hat{\\beta}_0) = \\operatorname{Var}(\\bar{y} - \\hat{\\beta}_1 \\bar{x}) $$\n$$ \\operatorname{Var}(\\hat{\\beta}_0) = \\operatorname{Var}(\\bar{y}) + \\bar{x}^2 \\operatorname{Var}(\\hat{\\beta}_1) - 2\\bar{x}\\operatorname{Cov}(\\bar{y}, \\hat{\\beta}_1) $$\nThe required components are:\n-   $\\operatorname{Var}(\\bar{y}) = \\operatorname{Var}\\left(\\frac{1}{n}\\sum_{i=1}^n y_i\\right) = \\frac{1}{n^2}\\sum_{i=1}^n \\operatorname{Var}(y_i) = \\frac{n\\sigma^2}{n^2} = \\frac{\\sigma^2}{n}$.\n-   $\\operatorname{Var}(\\hat{\\beta}_1) = \\frac{\\sigma^2}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}$.\n-   $\\operatorname{Cov}(\\bar{y}, \\hat{\\beta}_1) = 0$. This is a standard result; a centered predictor is uncorrelated with the sample mean of the response.\n\nSubstituting these into the variance expression for $\\hat{\\beta}_0$:\n$$ \\operatorname{Var}(\\hat{\\beta}_0) = \\frac{\\sigma^2}{n} + \\bar{x}^2 \\frac{\\sigma^2}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2} $$\n$$ \\operatorname{Var}(\\hat{\\beta}_0) = \\sigma^2 \\left( \\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2} \\right) $$\n\nThe standard error, $\\operatorname{SE}(\\hat{\\beta}_0)$, is the square root of the estimated variance, where $\\sigma^2$ is replaced by its estimator $\\hat{\\sigma}^2$ (the residual variance, often written as $s^2$ or $RSE^2$).\n$$ \\operatorname{SE}(\\hat{\\beta}_0) = \\hat{\\sigma} \\sqrt{\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}} $$\nThe width of a $95\\%$ confidence interval for $\\beta_0$ is proportional to $\\operatorname{SE}(\\hat{\\beta}_0)$: $2 \\times t_{n-2, 0.975} \\times \\operatorname{SE}(\\hat{\\beta}_0)$. Since $n$ and $\\hat{\\sigma}$ are assumed to be constant across designs, the width is determined by the term under the square root.\n\nThis formula reveals two key dependencies:\n1.  The standard error increases as $|\\bar{x}|$ increases, due to the $\\bar{x}^2$ term. This reflects the uncertainty introduced by extrapolating from the center of the data ($\\bar{x}$) to the y-axis intercept at $x=0$.\n2.  The standard error decreases as the spread of the predictor, $\\sum_{i=1}^{n} (x_i - \\bar{x})^2$, increases. A larger spread provides a more stable regression line.\n\nThe problem states that the predictor range is \"tiny\" for all three designs. Let us analyze the designs:\n-   All designs have $n=30$ and a predictor range of $0.4$. Since the points are uniformly spaced, the sum of squared deviations, $S_{xx} = \\sum_{i=1}^{n} (x_i - \\bar{x})^2$, will be identical and small for all three designs.\n-   The critical difference lies in the mean of the predictors, $\\bar{x}$:\n    -   **Design A:** $x_i \\in [9.8, 10.2] \\implies \\bar{x}_A = 10.0$.\n    -   **Design B:** $x_i \\in [0.0, 0.4] \\implies \\bar{x}_B = 0.2$.\n    -   **Design C:** $x_i \\in [-0.2, 0.2] \\implies \\bar{x}_C = 0.0$.\n\nNow we compare the term $\\frac{\\bar{x}^2}{\\sum (x_i - \\bar{x})^2}$ for each design:\n-   **Design A:** $\\frac{(10.0)^2}{S_{xx}} = \\frac{100}{S_{xx}}$. Since $S_{xx}$ is small, this term is very large.\n-   **Design B:** $\\frac{(0.2)^2}{S_{xx}} = \\frac{0.04}{S_{xx}}$. This term is much smaller than for Design A.\n-   **Design C:** $\\frac{(0.0)^2}{S_{xx}} = 0$. The uncertainty is minimized, and $\\operatorname{SE}(\\hat{\\beta}_0) = \\hat{\\sigma}/\\sqrt{n}$.\n\nConclusion: With a tiny predictor range (small $S_{xx}$), the uncertainty of the intercept is dominated by the extrapolation distance, $|\\bar{x}|$. Design A involves a massive extrapolation from data centered at $\\bar{x}=10.0$ back to $x=0$, resulting in an extremely large standard error and a wide, uninformative confidence interval for $\\beta_0$. Designs B and C, which have data much closer to $x=0$, will have substantially narrower confidence intervals. Specifically, Design A will yield a much wider interval than B and C.\n\n### Option-by-Option Analysis\n\n**A. With a tiny predictor range, the intercept’s confidence interval can be extremely wide if $|\\bar{x}|$ is large, because extrapolating from data far from $x=0$ inflates $\\operatorname{SE}(\\hat{\\beta}_0)$. Specifically, $\\operatorname{SE}(\\hat{\\beta}_0)$ increases with $|\\bar{x}|$ and decreases with $\\sum_{i=1}^{n} (x_i - \\bar{x})^2$. Thus Design A yields a much wider intercept interval than Designs B and C.**\nThis statement is entirely consistent with our derivation. It correctly identifies the role of extrapolation when $|\\bar{x}|$ is large, correctly states the mathematical dependencies of $\\operatorname{SE}(\\hat{\\beta}_0)$ on $|\\bar{x}|$ and $\\sum_{i=1}^{n} (x_i - \\bar{x})^2$, and correctly compares the three designs.\n**Verdict: Correct**\n\n**B. The intercept’s standard error increases with $\\sum_{i=1}^{n} (x_i - \\bar{x})^2$ and is essentially independent of $\\bar{x}$. Therefore a tiny predictor range directly narrows the intercept’s interval unless errors are heteroskedastic.**\nThis statement is incorrect on multiple grounds. First, $\\operatorname{SE}(\\hat{\\beta}_0)$ *decreases* with increasing $\\sum_{i=1}^{n} (x_i - \\bar{x})^2$, as this term appears in the denominator. Second, it is strongly dependent on $\\bar{x}$, except in the special case where $\\bar{x} \\approx 0$.\n**Verdict: Incorrect**\n\n**C. A tiny predictor range always narrows the intercept’s confidence interval because less variability in $x$ reduces overall uncertainty; larger $|\\bar{x}|$ stabilizes the intercept due to higher leverage at large $x$.**\nThis statement is fundamentally flawed. A tiny predictor range (small $\\sum(x_i - \\bar{x})^2$) *increases* the uncertainty of the slope, which in turn feeds into the uncertainty of the intercept. Furthermore, a larger $|\\bar{x}|$ profoundly *destabilizes* the intercept estimate due to the need for extrapolation, as seen by the $\\bar{x}^2$ term in the numerator of the variance formula.\n**Verdict: Incorrect**\n\n**D. The intercept’s standard error is approximately the residual standard deviation divided by $\\sqrt{n}$, regardless of $\\bar{x}$ or the spread of $x_i$. Hence all three designs should yield similar intercept intervals if the residual variability is similar.**\nThis statement describes a special case as if it were a general rule. The formula $\\operatorname{SE}(\\hat{\\beta}_0) \\approx \\hat{\\sigma}/\\sqrt{n}$ is only valid when the term $\\frac{\\bar{x}^2}{\\sum(x_i - \\bar{x})^2}$ is negligible, which requires $\\bar{x} \\approx 0$. This holds for Design C but is dramatically false for Design A. Therefore, the claim that all designs will have similar intervals is incorrect.\n**Verdict: Incorrect**", "answer": "$$\\boxed{A}$$", "id": "3176623"}, {"introduction": "In many scientific and business applications, our interest lies not in a single coefficient but in a comparison or combination of effects. This practice moves beyond the uncertainty of a single parameter to that of a linear combination of multiple regression coefficients. You will derive the standard error for the difference between two predictions, a task that requires an understanding of the full covariance structure of the coefficient estimators, demonstrating how the interplay between parameters governs the uncertainty of more complex functionals [@problem_id:3176582].", "problem": "Consider a linear regression model with an intercept and two predictors, written in matrix form as $y = X\\beta + \\varepsilon$, where $y \\in \\mathbb{R}^{n}$, $X \\in \\mathbb{R}^{n \\times p}$ with $p=3$, $\\beta \\in \\mathbb{R}^{3}$, and $\\varepsilon \\in \\mathbb{R}^{n}$. Assume the errors satisfy $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$. Let $\\hat{\\beta}$ be the Ordinary Least Squares (OLS) estimator. You are given $n=25$, the residual sum of squares $\\text{SSE} = 220$, and the matrix $(X^{\\top}X)^{-1}$ equals\n$$\n\\begin{pmatrix}\n0.50 & 0.02 & -0.01 \\\\\n0.02 & 0.20 & 0.00 \\\\\n-0.01 & 0.00 & 0.10\n\\end{pmatrix}.\n$$\nConsider the prediction-difference functional $f(\\beta) = x_{a}^{\\top}\\beta - x_{b}^{\\top}\\beta$ at two covariate row-vectors $x_{a}^{\\top} = \\begin{pmatrix} 1 & 2 & -1 \\end{pmatrix}$ and $x_{b}^{\\top} = \\begin{pmatrix} 1 & 0 & 3 \\end{pmatrix}$. Construct a two-sided $95\\%$ confidence interval for $f(\\beta)$ by starting from foundational principles of the distribution of OLS estimators under the stated assumptions, carefully deriving the standard error in terms of the covariance of $\\hat{\\beta}$ and $(X^{\\top}X)^{-1}$. For numerical work, use that the two-sided Student’s t distribution critical value at level $0.95$ with $n-p$ degrees of freedom is $t_{0.975,\\,22} = 2.074$. Explicitly identify the confidence interval center and half-width, and then report only the half-width. Round your reported half-width to four significant figures.", "solution": "The user wants to construct a $95\\%$ confidence interval for a linear combination of regression coefficients and report its half-width.\n\n### Step 1: Extract Givens\n- Linear model: $y = X\\beta + \\varepsilon$\n- $y \\in \\mathbb{R}^{n}$, $X \\in \\mathbb{R}^{n \\times p}$, $\\beta \\in \\mathbb{R}^{p}$, $\\varepsilon \\in \\mathbb{R}^{n}$\n- Number of observations: $n=25$\n- Number of parameters (including intercept): $p=3$\n- Error distribution assumption: $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$\n- Residual Sum of Squares: $\\text{SSE} = 220$\n- Matrix $(X^{\\top}X)^{-1}$:\n$$\n(X^{\\top}X)^{-1} = \n\\begin{pmatrix}\n0.50 & 0.02 & -0.01 \\\\\n0.02 & 0.20 & 0.00 \\\\\n-0.01 & 0.00 & 0.10\n\\end{pmatrix}\n$$\n- Functional of interest: $f(\\beta) = x_{a}^{\\top}\\beta - x_{b}^{\\top}\\beta$\n- Covariate vectors: $x_{a}^{\\top} = \\begin{pmatrix} 1 & 2 & -1 \\end{pmatrix}$ and $x_{b}^{\\top} = \\begin{pmatrix} 1 & 0 & 3 \\end{pmatrix}$\n- Confidence level: $95\\%$\n- Critical value: $t_{0.975,\\,22} = 2.074$ (Student's t-distribution with $n-p = 25-3=22$ degrees of freedom)\n\n### Step 2: Validate Using Extracted Givens\nThe problem is well-defined within the framework of linear regression theory. All necessary components are provided to construct the confidence interval. The assumptions (normality of errors) are standard for this type of problem. The matrix $(X^{\\top}X)^{-1}$ is symmetric, a necessary property for such a matrix. The problem is scientifically grounded, well-posed, and objective. It does not violate any of the invalidity criteria.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A full solution will be provided.\n\n### Derivation of the Confidence Interval and Half-Width\nThe Ordinary Least Squares (OLS) estimator for $\\beta$ is $\\hat{\\beta} = (X^{\\top}X)^{-1}X^{\\top}y$. Under the given assumption that $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$, the estimator $\\hat{\\beta}$ is also normally distributed. Specifically, its distribution is:\n$$\n\\hat{\\beta} \\sim \\mathcal{N}\\left(\\beta, \\sigma^{2}(X^{\\top}X)^{-1}\\right)\n$$\nThe functional of interest is $f(\\beta) = x_{a}^{\\top}\\beta - x_{b}^{\\top}\\beta$. We can express this as a linear combination of the components of $\\beta$:\n$$\nf(\\beta) = (x_{a} - x_{b})^{\\top}\\beta\n$$\nLet us define a vector $c = x_{a} - x_{b}$. Then $f(\\beta) = c^{\\top}\\beta$. The natural estimator for $f(\\beta)$ is $f(\\hat{\\beta}) = c^{\\top}\\hat{\\beta}$.\n\nSince $\\hat{\\beta}$ is normally distributed, any linear combination of its components, such as $c^{\\top}\\hat{\\beta}$, is also normally distributed.\nThe expected value of the estimator is:\n$$\nE[f(\\hat{\\beta})] = E[c^{\\top}\\hat{\\beta}] = c^{\\top}E[\\hat{\\beta}] = c^{\\top}\\beta = f(\\beta)\n$$\nThis shows that $f(\\hat{\\beta})$ is an unbiased estimator of $f(\\beta)$.\nThe variance of the estimator is:\n$$\n\\text{Var}(f(\\hat{\\beta})) = \\text{Var}(c^{\\top}\\hat{\\beta}) = c^{\\top}\\text{Cov}(\\hat{\\beta})c = c^{\\top}\\left(\\sigma^{2}(X^{\\top}X)^{-1}\\right)c = \\sigma^{2}c^{\\top}(X^{\\top}X)^{-1}c\n$$\nTherefore, the distribution of our estimator is:\n$$\nf(\\hat{\\beta}) \\sim \\mathcal{N}\\left(f(\\beta), \\sigma^{2}c^{\\top}(X^{\\top}X)^{-1}c\\right)\n$$\nThe variance $\\sigma^{2}$ is typically unknown and must be estimated from the data. The unbiased estimator for $\\sigma^{2}$ is the mean squared error (MSE), which is given by:\n$$\n\\hat{\\sigma}^{2} = \\frac{\\text{SSE}}{n-p}\n$$\nwhere $n-p$ are the degrees of freedom for the residuals. It is a standard result that the quantity $\\frac{(n-p)\\hat{\\sigma}^{2}}{\\sigma^{2}} = \\frac{\\text{SSE}}{\\sigma^{2}}$ follows a chi-squared distribution with $n-p$ degrees of freedom, i.e., $\\chi_{n-p}^{2}$.\n\nTo construct a confidence interval, we form a pivot quantity that follows a Student's t-distribution. The standardized random variable\n$$\nZ = \\frac{f(\\hat{\\beta}) - f(\\beta)}{\\sqrt{\\sigma^{2}c^{\\top}(X^{\\top}X)^{-1}c}} \\sim \\mathcal{N}(0, 1)\n$$\nis a standard normal variable. If we replace the unknown $\\sigma^{2}$ with its estimate $\\hat{\\sigma}^{2}$, we obtain the t-statistic:\n$$\nT = \\frac{f(\\hat{\\beta}) - f(\\beta)}{\\sqrt{\\hat{\\sigma}^{2}c^{\\top}(X^{\\top}X)^{-1}c}} = \\frac{f(\\hat{\\beta}) - f(\\beta)}{\\text{SE}(f(\\hat{\\beta}))}\n$$\nwhere $\\text{SE}(f(\\hat{\\beta})) = \\sqrt{\\hat{\\sigma}^{2}c^{\\top}(X^{\\top}X)^{-1}c}$ is the standard error of the estimator $f(\\hat{\\beta})$. This statistic $T$ follows a Student's t-distribution with $n-p$ degrees of freedom.\n$$\nT \\sim t_{n-p}\n$$\nA two-sided $(1-\\alpha)$ confidence interval for $f(\\beta)$ is constructed using the critical value $t_{\\alpha/2, n-p}$ from the t-distribution, such that $P(|T| > t_{\\alpha/2, n-p}) = \\alpha$. In our case, the confidence level is $95\\%$, so $\\alpha = 0.05$ and $\\alpha/2 = 0.025$. The relevant critical value is denoted by its upper quantile, $t_{1-\\alpha/2, n-p} = t_{0.975, n-p}$. The interval is given by:\n$$\nf(\\hat{\\beta}) \\pm t_{1-\\alpha/2, n-p} \\times \\text{SE}(f(\\hat{\\beta}))\n$$\nThe center of the confidence interval is $f(\\hat{\\beta})$ and the half-width is $W = t_{1-\\alpha/2, n-p} \\times \\text{SE}(f(\\hat{\\beta}))$. The problem asks for this half-width.\n\nNow, we perform the calculations with the given values.\nFirst, compute the vector $c$:\n$$\nc^{\\top} = x_{a}^{\\top} - x_{b}^{\\top} = \\begin{pmatrix} 1 & 2 & -1 \\end{pmatrix} - \\begin{pmatrix} 1 & 0 & 3 \\end{pmatrix} = \\begin{pmatrix} 0 & 2 & -4 \\end{pmatrix}\n$$\nSo, $c = \\begin{pmatrix} 0 \\\\ 2 \\\\ -4 \\end{pmatrix}$.\n\nNext, compute the estimate of the error variance, $\\hat{\\sigma}^{2}$:\n$$\n\\hat{\\sigma}^{2} = \\frac{\\text{SSE}}{n-p} = \\frac{220}{25-3} = \\frac{220}{22} = 10\n$$\nNext, calculate the quadratic form $c^{\\top}(X^{\\top}X)^{-1}c$:\n$$\nc^{\\top}(X^{\\top}X)^{-1} = \\begin{pmatrix} 0 & 2 & -4 \\end{pmatrix} \\begin{pmatrix}\n0.50 & 0.02 & -0.01 \\\\\n0.02 & 0.20 & 0.00 \\\\\n-0.01 & 0.00 & 0.10\n\\end{pmatrix}\n$$\n$$\n= \\begin{pmatrix} (0)(0.50)+(2)(0.02)+(-4)(-0.01) & (0)(0.02)+(2)(0.20)+(-4)(0.00) & (0)(-0.01)+(2)(0.00)+(-4)(0.10) \\end{pmatrix}\n$$\n$$\n= \\begin{pmatrix} 0+0.04+0.04 & 0+0.40+0 & 0+0-0.40 \\end{pmatrix} = \\begin{pmatrix} 0.08 & 0.40 & -0.40 \\end{pmatrix}\n$$\nNow, multiply by $c$:\n$$\nc^{\\top}(X^{\\top}X)^{-1}c = \\begin{pmatrix} 0.08 & 0.40 & -0.40 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 2 \\\\ -4 \\end{pmatrix}\n$$\n$$\n= (0.08)(0) + (0.40)(2) + (-0.40)(-4) = 0 + 0.80 + 1.60 = 2.40\n$$\nNow, we can compute the squared standard error of $f(\\hat{\\beta})$:\n$$\n\\text{SE}(f(\\hat{\\beta}))^{2} = \\hat{\\sigma}^{2} c^{\\top}(X^{\\top}X)^{-1}c = (10)(2.40) = 24\n$$\nThe standard error is:\n$$\n\\text{SE}(f(\\hat{\\beta})) = \\sqrt{24}\n$$\nThe degrees of freedom are $n-p=22$. The critical value for a $95\\%$ confidence interval is given as $t_{0.975, 22} = 2.074$.\n\nThe half-width of the confidence interval is:\n$$\nW = t_{0.975, 22} \\times \\text{SE}(f(\\hat{\\beta})) = 2.074 \\times \\sqrt{24}\n$$\nNumerically, $\\sqrt{24} \\approx 4.898979...$\n$$\nW = 2.074 \\times 4.898979... \\approx 10.158683...\n$$\nRounding this result to four significant figures gives $10.16$.\nThe confidence interval center is $f(\\hat{\\beta}) = c^{\\top}\\hat{\\beta}$, which we do not need to calculate. The half-width is the value requested.\nThe half-width is $10.16$.", "answer": "$$\n\\boxed{10.16}\n$$", "id": "3176582"}, {"introduction": "Analytic formulas for standard errors are powerful, but they rely on a set of idealized assumptions. This hands-on coding exercise bridges the gap between theory and modern practice by introducing computational resampling techniques—the jackknife and the bootstrap—as robust alternatives for estimating uncertainty. By applying these methods to datasets with challenging features like high-leverage points, you will learn to implement and compare different approaches to constructing confidence intervals and appreciate why computational statistics are an indispensable part of the modern data scientist's toolkit [@problem_id:3176572].", "problem": "Consider the classical linear regression setting where an observed response vector $y \\in \\mathbb{R}^n$ is modeled as $y = X \\beta + \\varepsilon$, with design matrix $X \\in \\mathbb{R}^{n \\times p}$, coefficient vector $\\beta \\in \\mathbb{R}^p$, and noise vector $\\varepsilon \\in \\mathbb{R}^n$. Assume $\\varepsilon$ has zero mean and finite variance, and ordinary least squares (OLS) estimates are used. In this problem, you will estimate the standard error (SE) of a single coefficient $\\hat{\\beta}_j$ using three approaches—an analytic method under the classical linear model assumptions, the jackknife via leave-one-out, and the nonparametric bootstrap—and then construct and compare $95\\%$ confidence intervals (CI) for leverage-sensitive designs.\n\nYou must implement a program that:\n- Derives the ordinary least squares (OLS) coefficient estimates, where ordinary least squares (OLS) means choosing $\\hat{\\beta}$ to minimize the sum of squared residuals.\n- Obtains the analytic standard error (SE) for a chosen coordinate $\\hat{\\beta}_j$ using the classical linear model assumptions and an estimated error variance from residuals. Construct a two-sided $95\\%$ confidence interval (CI) using the Student’s $t$ distribution with $n - p$ degrees of freedom.\n- Computes the jackknife SE for $\\hat{\\beta}_j$ by systematically leaving out each observation in turn and aggregating the leave-one-out estimates. Construct a two-sided $95\\%$ CI using the same $t$-based quantile as in the analytic CI.\n- Computes the bootstrap SE and a percentile $95\\%$ CI by resampling the rows of $(X, y)$ with replacement a specified number of times $B$, re-estimating $\\hat{\\beta}_j$ for each bootstrap sample, and taking the empirical quantiles at $2.5\\%$ and $97.5\\%$.\n\nYou must focus on leverage-sensitive designs (i.e., designs where some rows of $X$ exert unusually high influence on the fit). The experiment is fully specified and deterministic: your program must generate $y$ using the provided seeds and parameters and then report the requested metrics for each test case.\n\nFundamental base you must use:\n- The ordinary least squares (OLS) estimator under the classical linear model.\n- The notion of leverage in regression designs (rows of $X$ that can strongly influence the fit).\n- Resampling principles for jackknife and bootstrap.\n\nDo not use or quote shortcut formulas in the problem statement. Your implementation should logically follow from these principles.\n\nTest Suite:\n- Case A (balanced design, low leverage):\n  - Let $n = 40$, $p = 2$, and let $x$ be evenly spaced over $[-2, 2]$ with $40$ points. Define $X = [\\mathbf{1}, x]$ (an intercept and a single predictor).\n  - True coefficients $\\beta^\\star = [1.0, 2.0]$.\n  - Noise is independent and normally distributed with standard deviation $\\sigma = 0.5$.\n  - Generate $y = X \\beta^\\star + \\varepsilon$ with random seed $123$.\n  - Target index $j = 1$ (the slope on $x$).\n  - Bootstrap replicates $B = 1000$ with bootstrap seed $321$.\n\n- Case B (one extreme leverage point, same dimension):\n  - Let $n = 40$, $p = 2$, and let $x$ be evenly spaced over $[-1, 1]$ with $40$ points except set the first element to $10.0$. Define $X = [\\mathbf{1}, x]$.\n  - True coefficients $\\beta^\\star = [1.0, 2.0]$.\n  - Noise is independent and normally distributed with standard deviation $\\sigma = 0.5$.\n  - Generate $y = X \\beta^\\star + \\varepsilon$ with random seed $456$.\n  - Target index $j = 1$ (the slope on $x$).\n  - Bootstrap replicates $B = 1000$ with bootstrap seed $654$.\n\n- Case C (near-collinearity with a leverage point, three predictors including intercept):\n  - Let $n = 12$, $p = 3$. Let $x_1$ be evenly spaced over $[-1, 1]$ with $12$ points, and define $x_2 = 0.98 x_1 + 0.02 \\sin(k)$ for $k = 0, 1, \\dots, 11$, then set $x_1[11] = 5.0$ and $x_2[11] = 4.9$. Define $X = [\\mathbf{1}, x_1, x_2]$.\n  - True coefficients $\\beta^\\star = [0.5, 1.5, -0.5]$.\n  - Noise is independent and normally distributed with standard deviation $\\sigma = 0.8$.\n  - Generate $y = X \\beta^\\star + \\varepsilon$ with random seed $789$.\n  - Target index $j = 2$ (the coefficient on $x_2$).\n  - Bootstrap replicates $B = 2000$ with bootstrap seed $987$.\n\nFor each test case, compute and return, in order:\n1. Analytic SE of $\\hat{\\beta}_j$ (a float).\n2. Bootstrap SE of $\\hat{\\beta}_j$ (a float).\n3. Jackknife SE of $\\hat{\\beta}_j$ (a float).\n4. Analytic CI width (a float).\n5. Bootstrap percentile CI width (a float).\n6. Jackknife CI width (a float).\n7. Does the analytic CI contain the true coefficient $\\beta^\\star_j$? (a boolean).\n8. Does the bootstrap percentile CI contain $\\beta^\\star_j$? (a boolean).\n9. Does the jackknife CI contain $\\beta^\\star_j$? (a boolean).\n\nYour program should produce a single line of output containing the results as a comma-separated list of the three per-case lists enclosed in square brackets, in the exact order of the test suite. For example, the final printed line must look like:\n$[[r_{A,1}, r_{A,2}, \\dots, r_{A,9}], [r_{B,1}, \\dots, r_{B,9}], [r_{C,1}, \\dots, r_{C,9}]]$,\nwhere all $r_{*,*}$ are the computed floats or booleans for each case. No units are required; represent quantities as dimensionless floats and booleans.", "solution": "The problem is evaluated as valid, being scientifically grounded in statistical regression theory, well-posed with all necessary parameters and conditions specified, and objective in its formulation. We will proceed to derive the necessary formulae and construct the solution.\n\n### 1. Ordinary Least Squares (OLS) Estimation\n\nThe classical linear regression model is given by $y = X \\beta + \\varepsilon$, where $y \\in \\mathbb{R}^n$ is the response vector, $X \\in \\mathbb{R}^{n \\times p}$ is the design matrix, $\\beta \\in \\mathbb{R}^p$ is the vector of coefficients, and $\\varepsilon \\in \\mathbb{R}^n$ is the noise vector. The ordinary least squares (OLS) estimator, denoted $\\hat{\\beta}$, is the value of $\\beta$ that minimizes the residual sum of squares (RSS):\n$$\nS(\\beta) = \\|y - X\\beta\\|^2_2 = (y - X\\beta)^T (y - X\\beta)\n$$\nTo find the minimum, we differentiate $S(\\beta)$ with respect to $\\beta$ and set the gradient to zero:\n$$\n\\frac{\\partial S(\\beta)}{\\partial \\beta} = -2X^T(y - X\\beta) = 0\n$$\nThis leads to the normal equations:\n$$\n(X^T X) \\hat{\\beta} = X^T y\n$$\nAssuming the matrix $X^T X$ is invertible (i.e., $X$ has full column rank), the unique OLS solution is:\n$$\n\\hat{\\beta} = (X^T X)^{-1} X^T y\n$$\nThis estimator is linear in $y$. For numerical stability, it is preferable to solve the system of linear equations directly rather than computing the inverse of $X^T X$.\n\n### 2. Analytic Standard Error and Confidence Interval\n\nUnder the classical linear model (CLM) assumptions, the errors $\\varepsilon_i$ are uncorrelated and have constant variance $\\sigma^2$, i.e., $\\text{Cov}(\\varepsilon) = \\sigma^2 I_n$, where $I_n$ is the $n \\times n$ identity matrix. The covariance matrix of the OLS estimator $\\hat{\\beta}$ is:\n$$\n\\text{Cov}(\\hat{\\beta}) = \\text{Cov}((X^T X)^{-1} X^T y) = (X^T X)^{-1} X^T \\text{Cov}(y) X (X^T X)^{-1}\n$$\nSince $\\text{Cov}(y) = \\text{Cov}(X\\beta + \\varepsilon) = \\text{Cov}(\\varepsilon) = \\sigma^2 I_n$, this simplifies to:\n$$\n\\text{Cov}(\\hat{\\beta}) = \\sigma^2 (X^T X)^{-1}\n$$\nThe variance of a single coefficient estimate $\\hat{\\beta}_j$ (for $j \\in \\{0, 1, \\dots, p-1\\}$) is the $j$-th diagonal element of this matrix:\n$$\n\\text{Var}(\\hat{\\beta}_j) = \\sigma^2 \\left[(X^T X)^{-1}\\right]_{jj}\n$$\nIn practice, the true error variance $\\sigma^2$ is unknown and must be estimated from the data. An unbiased estimator for $\\sigma^2$ is:\n$$\n\\hat{\\sigma}^2 = \\frac{1}{n-p} \\sum_{i=1}^n e_i^2 = \\frac{\\text{RSS}}{n-p}\n$$\nwhere $e = y - X\\hat{\\beta}$ are the residuals. The denominator $n-p$ represents the degrees of freedom of the residuals.\nThe estimated standard error (SE) of $\\hat{\\beta}_j$ is the square root of the estimated variance:\n$$\n\\text{SE}_{\\text{analytic}}(\\hat{\\beta}_j) = \\sqrt{\\hat{\\sigma}^2 \\left[(X^T X)^{-1}\\right]_{jj}} = \\hat{\\sigma} \\sqrt{\\left[(X^T X)^{-1}\\right]_{jj}}\n$$\nAssuming the errors are also normally distributed, the quantity $\\frac{\\hat{\\beta}_j - \\beta_j}{\\text{SE}(\\hat{\\beta}_j)}$ follows a Student's $t$ distribution with $n-p$ degrees of freedom. A two-sided $(1-\\alpha) \\cdot 100\\%$ confidence interval (CI) for $\\beta_j$ is constructed as:\n$$\n\\text{CI}_{\\text{analytic}} = \\left[ \\hat{\\beta}_j - t_{1-\\alpha/2, n-p} \\cdot \\text{SE}_{\\text{analytic}}(\\hat{\\beta}_j), \\quad \\hat{\\beta}_j + t_{1-\\alpha/2, n-p} \\cdot \\text{SE}_{\\text{analytic}}(\\hat{\\beta}_j) \\right]\n$$\nwhere $t_{1-\\alpha/2, n-p}$ is the $(1-\\alpha/2)$-quantile of the $t_{n-p}$ distribution. For a $95\\%$ CI, $\\alpha = 0.05$. The width of this interval is $2 \\cdot t_{0.975, n-p} \\cdot \\text{SE}_{\\text{analytic}}(\\hat{\\beta}_j)$.\n\n### 3. Jackknife Standard Error and Confidence Interval\n\nThe jackknife is a resampling technique that estimates the variance of an estimator by systematically leaving out one observation at a time. Let $\\hat{\\beta}$ be the estimate from the full dataset of size $n$.\n1. For each $i = 1, \\dots, n$, form the $i$-th leave-one-out dataset by removing the $i$-th row from $X$ and $y$, denoted $(X_{(-i)}, y_{(-i)})$.\n2. Compute the OLS estimate for each of these $n$ datasets: $\\hat{\\beta}_{(-i)} = (X_{(-i)}^T X_{(-i)})^{-1} X_{(-i)}^T y_{(-i)}$.\n3. We focus on the target coefficient, yielding $n$ estimates $\\{\\hat{\\beta}_{j,(-1)}, \\dots, \\hat{\\beta}_{j,(-n)}\\}$.\n4. The jackknife estimate of the variance of $\\hat{\\beta}_j$ is given by:\n$$\n\\widehat{\\text{Var}}_{\\text{jack}}(\\hat{\\beta}_j) = \\frac{n-1}{n} \\sum_{i=1}^n (\\hat{\\beta}_{j,(-i)} - \\bar{\\beta}_j)^2\n$$\nwhere $\\bar{\\beta}_j = \\frac{1}{n} \\sum_{i=1}^n \\hat{\\beta}_{j,(-i)}$ is the mean of the leave-one-out estimates.\n5. The jackknife standard error is the square root of this variance:\n$$\n\\text{SE}_{\\text{jack}}(\\hat{\\beta}_j) = \\sqrt{\\widehat{\\text{Var}}_{\\text{jack}}(\\hat{\\beta}_j)}\n$$\nAs specified, the jackknife confidence interval is constructed symmetrically around the full-sample estimate $\\hat{\\beta}_j$, using the same $t$-quantile as the analytic method:\n$$\n\\text{CI}_{\\text{jack}} = \\left[ \\hat{\\beta}_j - t_{1-\\alpha/2, n-p} \\cdot \\text{SE}_{\\text{jack}}(\\hat{\\beta}_j), \\quad \\hat{\\beta}_j + t_{1-\\alpha/2, n-p} \\cdot \\text{SE}_{\\text{jack}}(\\hat{\\beta}_j) \\right]\n$$\nThe width is $2 \\cdot t_{0.975, n-p} \\cdot \\text{SE}_{\\text{jack}}(\\hat{\\beta}_j)$.\n\n### 4. Bootstrap Standard Error and Confidence Interval\n\nThe nonparametric bootstrap generates replicate datasets by resampling with replacement from the original data. This approach does not rely on the CLM assumptions.\n1. Specify a number of bootstrap replicates, $B$.\n2. For each replicate $b = 1, \\dots, B$:\n    a. Create a bootstrap sample $(X^*_b, y^*_b)$ of size $n$ by drawing $n$ rows with replacement from the original data $(X, y)$.\n    b. Compute the OLS estimate on this bootstrap sample: $\\hat{\\beta}^*_b = (X^{*T}_b X^*_b)^{-1} X^{*T}_b y^*_b$. Note that some bootstrap samples may result in a singular $X^{*T}_b X^*_b$ matrix, especially with high-leverage designs. In such cases, a generalized inverse (e.g., Moore-Penrose pseudoinverse) can be used to find the minimum-norm least-squares solution.\n3. This process yields a collection of $B$ bootstrap coefficient vectors, $\\{\\hat{\\beta}^*_1, \\dots, \\hat{\\beta}^*_B\\}$. Consider the distribution of the target coefficient, $\\{\\hat{\\beta}^*_{j,1}, \\dots, \\hat{\\beta}^*_{j,B}\\}$.\n4. The bootstrap standard error is the sample standard deviation of these bootstrap estimates:\n$$\n\\text{SE}_{\\text{boot}}(\\hat{\\beta}_j) = \\sqrt{\\frac{1}{B-1} \\sum_{b=1}^B (\\hat{\\beta}^*_{j,b} - \\bar{\\beta}^*_j)^2}, \\text{ where } \\bar{\\beta}^*_j = \\frac{1}{B} \\sum_{b=1}^B \\hat{\\beta}^*_{j,b}\n$$\n5. The percentile bootstrap confidence interval is constructed directly from the empirical distribution of the bootstrap estimates. The $(1-\\alpha) \\cdot 100\\%$ percentile CI is given by the $(\\alpha/2)$ and $(1-\\alpha/2)$ quantiles of the sorted bootstrap replicates. For a $95\\%$ CI:\n$$\n\\text{CI}_{\\text{percentile}} = [q_{0.025}, q_{0.975}]\n$$\nwhere $q_{p}$ is the $p$-th empirical quantile of the set $\\{\\hat{\\beta}^*_{j,1}, \\dots, \\hat{\\beta}^*_{j,B}\\}$. The width of this interval is $q_{0.975} - q_{0.025}$. This method has the advantage of producing asymmetric intervals if the bootstrap distribution is skewed, which often occurs in the presence of influential data points.\n\nThe program will now implement these three methods for each specified test case.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import t\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n\n    test_cases = [\n        # Case A: Balanced design, low leverage\n        {\n            \"n\": 40, \"p\": 2,\n            \"x_def\": lambda n: (np.c_[np.ones(n), np.linspace(-2, 2, n)], None),\n            \"beta_star\": np.array([1.0, 2.0]),\n            \"sigma\": 0.5,\n            \"y_seed\": 123,\n            \"target_j\": 1,\n            \"B\": 1000,\n            \"bootstrap_seed\": 321\n        },\n        # Case B: One extreme leverage point\n        {\n            \"n\": 40, \"p\": 2,\n            \"x_def\": lambda n: (\n                np.c_[np.ones(n), (lambda x: (x.__setitem__(0, 10.0), x)[1])(np.linspace(-1, 1, n))], None\n            ),\n            \"beta_star\": np.array([1.0, 2.0]),\n            \"sigma\": 0.5,\n            \"y_seed\": 456,\n            \"target_j\": 1,\n            \"B\": 1000,\n            \"bootstrap_seed\": 654\n        },\n        # Case C: Near-collinearity with a leverage point\n        {\n            \"n\": 12, \"p\": 3,\n            \"x_def\": lambda n: (\n                (lambda x1, x2: (\n                    (x1.__setitem__(n-1, 5.0), x2.__setitem__(n-1, 4.9)),\n                    np.c_[np.ones(n), x1, x2]\n                )[1])(np.linspace(-1, 1, n), 0.98 * np.linspace(-1, 1, n) + 0.02 * np.sin(np.arange(n))),\n                None\n            ),\n            \"beta_star\": np.array([0.5, 1.5, -0.5]),\n            \"sigma\": 0.8,\n            \"y_seed\": 789,\n            \"target_j\": 2,\n            \"B\": 2000,\n            \"bootstrap_seed\": 987\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        results = run_case(case)\n        all_results.append(results)\n\n    # Convert boolean to lower case string representation for printing\n    formatted_results = []\n    for case_res in all_results:\n        str_res = [f\"{v:.6f}\" if isinstance(v, float) else str(v).lower() for v in case_res]\n        formatted_results.append(f\"[{','.join(str_res)}]\")\n\n    print(f\"[{','.join(formatted_results)}]\")\n\ndef get_ols_estimate(X, y):\n    \"\"\"\n    Computes OLS estimates using np.linalg.lstsq for numerical stability.\n    \"\"\"\n    beta_hat, _, _, _ = np.linalg.lstsq(X, y, rcond=None)\n    return beta_hat\n\ndef run_case(params):\n    \"\"\"\n    Runs a single test case and computes all required metrics.\n    \"\"\"\n    n, p = params[\"n\"], params[\"p\"]\n    X, _ = params[\"x_def\"](n)\n    beta_star = params[\"beta_star\"]\n    sigma = params[\"sigma\"]\n    y_seed = params[\"y_seed\"]\n    target_j = params[\"target_j\"]\n    B = params[\"B\"]\n    bootstrap_seed = params[\"bootstrap_seed\"]\n    \n    # Generate response vector y\n    rng_y = np.random.default_rng(y_seed)\n    epsilon = rng_y.normal(0, sigma, n)\n    y = X @ beta_star + epsilon\n\n    # --- Full Sample OLS ---\n    beta_hat = get_ols_estimate(X, y)\n    beta_hat_j = beta_hat[target_j]\n\n    # --- 1. Analytic Method ---\n    residuals = y - X @ beta_hat\n    rss = residuals.T @ residuals\n    sigma_hat_sq = rss / (n - p)\n    sigma_hat = np.sqrt(sigma_hat_sq)\n    \n    # Using np.linalg.inv is acceptable here as X.T @ X is small.\n    # For larger p, solving a system would be better.\n    try:\n        C = np.linalg.inv(X.T @ X)\n        analytic_se = sigma_hat * np.sqrt(C[target_j, target_j])\n        \n        df = n - p\n        t_crit = t.ppf(0.975, df)\n        analytic_ci_half_width = t_crit * analytic_se\n        analytic_ci = [beta_hat_j - analytic_ci_half_width, beta_hat_j + analytic_ci_half_width]\n        analytic_ci_width = 2 * analytic_ci_half_width\n        analytic_contains_true = (analytic_ci[0] <= beta_star[target_j] <= analytic_ci[1])\n    except np.linalg.LinAlgError:\n        # Handle cases where X is rank deficient from the start\n        analytic_se = np.nan\n        analytic_ci_width = np.nan\n        analytic_contains_true = False\n\n    # --- 2. Bootstrap Method ---\n    rng_boot = np.random.default_rng(bootstrap_seed)\n    bootstrap_betas_j = []\n    \n    for _ in range(B):\n        indices = rng_boot.choice(n, size=n, replace=True)\n        X_boot, y_boot = X[indices], y[indices]\n        \n        # lstsq handles potential singularity in bootstrap samples\n        beta_boot = get_ols_estimate(X_boot, y_boot)\n        bootstrap_betas_j.append(beta_boot[target_j])\n\n    bootstrap_betas_j = np.array(bootstrap_betas_j)\n    \n    bootstrap_se = np.std(bootstrap_betas_j, ddof=1)\n    \n    # Percentile CI\n    bootstrap_ci = np.quantile(bootstrap_betas_j, [0.025, 0.975])\n    bootstrap_ci_width = bootstrap_ci[1] - bootstrap_ci[0]\n    bootstrap_contains_true = (bootstrap_ci[0] <= beta_star[target_j] <= bootstrap_ci[1])\n\n    # --- 3. Jackknife Method ---\n    jackknife_betas_j = []\n    for i in range(n):\n        X_jack = np.delete(X, i, axis=0)\n        y_jack = np.delete(y, i, axis=0)\n        \n        beta_jack = get_ols_estimate(X_jack, y_jack)\n        jackknife_betas_j.append(beta_jack[target_j])\n    \n    jackknife_betas_j = np.array(jackknife_betas_j)\n    beta_j_bar_jack = np.mean(jackknife_betas_j)\n    \n    jackknife_var = ((n - 1) / n) * np.sum((jackknife_betas_j - beta_j_bar_jack)**2)\n    jackknife_se = np.sqrt(jackknife_var)\n    \n    # CI using t-critical value from analytic method\n    jackknife_ci_half_width = t_crit * jackknife_se\n    jackknife_ci = [beta_hat_j - jackknife_ci_half_width, beta_hat_j + jackknife_ci_half_width]\n    jackknife_ci_width = 2 * jackknife_ci_half_width\n    jackknife_contains_true = (jackknife_ci[0] <= beta_star[target_j] <= jackknife_ci[1])\n    \n    return [\n        analytic_se, bootstrap_se, jackknife_se,\n        analytic_ci_width, bootstrap_ci_width, jackknife_ci_width,\n        analytic_contains_true, bootstrap_contains_true, jackknife_contains_true\n    ]\n\n\nif __name__ == '__main__':\n    solve()\n\n```", "id": "3176572"}]}