{"hands_on_practices": [{"introduction": "Gradient descent's performance is deeply tied to the geometry of the objective function. When a function's level sets are stretched and skewed—a property known as ill-conditioning or anisotropy—the standard algorithm can slow to a crawl, taking a characteristic zig-zag path toward the minimum. This exercise [@problem_id:3139501] provides a hands-on demonstration of this phenomenon and a powerful remedy. By rotating the coordinate system to align with the function's principal axes, we can transform an ill-conditioned problem into a simple one and achieve dramatic acceleration.", "problem": "You will implement, analyze, and compare two variants of the gradient descent method on a convex quadratic in two dimensions. The goal is to construct a mathematically controlled case where the basic method exhibits a zig-zag trajectory due to anisotropy (ill-conditioning), and then demonstrate how a rotation-based change of variables combined with axis-aligned per-coordinate step sizes aligns the update directions with the principal axes of the contours and accelerates convergence.\n\nStart from the following core definitions and well-tested facts in numerical optimization and linear algebra:\n- A twice continuously differentiable function with a symmetric positive definite (Positive Definite (PD)) Hessian matrix is strongly convex. In particular, the quadratic function $f(\\mathbf{x}) = \\tfrac{1}{2}\\,\\mathbf{x}^{\\top} \\mathbf{Q}\\,\\mathbf{x}$ with symmetric PD matrix $\\mathbf{Q} \\in \\mathbb{R}^{2\\times 2}$ has a unique minimizer at $\\mathbf{x}^{\\star} = \\mathbf{0}$ and gradient $\\nabla f(\\mathbf{x}) = \\mathbf{Q}\\,\\mathbf{x}.$\n- The gradient descent update with a fixed step size $\\alpha \\in (0, 2/L)$ for minimizing $f$ is $\\mathbf{x}_{k+1} = \\mathbf{x}_{k} - \\alpha\\,\\nabla f(\\mathbf{x}_{k}).$ For the quadratic above, this is $\\mathbf{x}_{k+1} = \\mathbf{x}_{k} - \\alpha\\,\\mathbf{Q}\\,\\mathbf{x}_{k}.$\n- For an orthogonal rotation matrix $\\mathbf{R}(\\theta) = \\begin{bmatrix}\\cos\\theta  -\\sin\\theta\\\\ \\sin\\theta  \\cos\\theta\\end{bmatrix}$ with angle $\\theta$ measured in radians, the transformation $\\mathbf{y} = \\mathbf{R}^{\\top}\\mathbf{x}$ changes coordinates without changing Euclidean norms, that is $\\|\\mathbf{y}\\|_{2} = \\|\\mathbf{x}\\|_{2}.$\n\nYou will consider a family of convex quadratics whose Hessians are constructed by rotating a diagonal matrix of eigenvalues. Let $\\lambda_{1}  \\lambda_{2}  0$ and set $$\\mathbf{\\Lambda} = \\mathrm{diag}(\\lambda_{1}, \\lambda_{2}), \\quad \\mathbf{Q} = \\mathbf{R}(\\theta)\\,\\mathbf{\\Lambda}\\,\\mathbf{R}(\\theta)^{\\top}.$$ Large ratios $\\lambda_{1}/\\lambda_{2}$ induce anisotropy (ill-conditioning), which causes the basic gradient descent method to take a zig-zag path in the original coordinate system when $\\theta \\neq 0.$\n\nYour tasks are:\n- Implement baseline gradient descent in the original coordinates with a fixed step size $\\alpha = 1/\\lambda_{\\max} = 1/\\lambda_{1}$, so that $\\mathbf{x}_{k+1} = \\mathbf{x}_{k} - \\alpha\\,\\mathbf{Q}\\,\\mathbf{x}_{k}.$ Use initial condition $\\mathbf{x}_{0} = (1,\\,1)^{\\top}$. Terminate when the Euclidean norm satisfies $\\|\\mathbf{x}_{k}\\|_{2} \\le \\varepsilon.$\n- Implement a rotated-coordinate, axis-aligned, per-coordinate step-size variant. Define the rotated variable $\\mathbf{y} = \\mathbf{R}(\\theta)^{\\top}\\mathbf{x}$ and update in the rotated coordinates using per-axis learning rates $\\alpha_{1} = c/\\lambda_{1}, \\ \\alpha_{2} = c/\\lambda_{2}$ with a shared scalar $c \\in (0,2)$, that is, $\\mathbf{y}_{k+1} = \\mathbf{y}_{k} - \\mathrm{diag}(\\alpha_{1}, \\alpha_{2})\\,\\nabla_{\\mathbf{y}} f(\\mathbf{y}_{k})$, where $\\nabla_{\\mathbf{y}} f(\\mathbf{y}) = \\mathbf{\\Lambda}\\,\\mathbf{y}.$ Use the same initial condition expressed in rotated coordinates $\\mathbf{y}_{0} = \\mathbf{R}(\\theta)^{\\top}\\mathbf{x}_{0}$, and terminate when $\\|\\mathbf{y}_{k}\\|_{2} \\le \\varepsilon.$ Note that this choice aligns updates with the principal axes and decouples the dynamics along each axis. You must use the same $\\varepsilon$ as in the baseline.\n- For each test case below, compute the number of iterations $N_{\\mathrm{plain}}$ required by the baseline and $N_{\\mathrm{rot}}$ required by the rotated-coordinate, per-axis variant, and report the speedup factor defined as the float $S = N_{\\mathrm{plain}}/N_{\\mathrm{rot}}.$\n\nAngle unit specification: all angles $\\theta$ are provided in radians and must be interpreted in radians.\n\nNumerical termination and safety: if the stopping criterion is not met within $10^{6}$ iterations for either method, stop and use the achieved count; however, all the provided test cases are well within this limit.\n\nTest suite (happy path, significant anisotropy; moderate anisotropy; near-isotropic edge case):\n- Test $1$: $(\\lambda_{1}, \\lambda_{2}, \\theta, \\varepsilon, c) = (200.0,\\,1.0,\\,\\pi/4,\\,10^{-8},\\,1.6).$\n- Test $2$: $(\\lambda_{1}, \\lambda_{2}, \\theta, \\varepsilon, c) = (10.0,\\,1.0,\\,0.4,\\,10^{-8},\\,1.6).$\n- Test $3$: $(\\lambda_{1}, \\lambda_{2}, \\theta, \\varepsilon, c) = (1.0,\\,1.0,\\,0.7,\\,10^{-8},\\,1.6).$\n\nYour program must:\n- Construct $\\mathbf{Q}$ from $(\\lambda_{1}, \\lambda_{2}, \\theta)$ as specified.\n- Run both methods from $\\mathbf{x}_{0} = (1,\\,1)^{\\top}$ using the corresponding step rules and stopping conditions.\n- For each test, compute $S = N_{\\mathrm{plain}}/N_{\\mathrm{rot}}$ and round to $3$ decimal places.\n\nFinal output format: Your program should produce a single line of output containing the three rounded results as a comma-separated list enclosed in square brackets, for example $[S_{1},S_{2},S_{3}]$, with each $S_{i}$ being a float rounded to $3$ decimals. No other text should be printed.", "solution": "The user-provided problem statement is valid. It is scientifically grounded in the principles of numerical optimization and linear algebra, well-posed with all necessary parameters and conditions defined, and objective in its formulation. The task is to analyze the performance of two gradient descent variants on a two-dimensional convex quadratic function, which is a classic and instructive problem in computational science.\n\nThe solution proceeds by first implementing the baseline gradient descent method and analyzing its behavior, then implementing the coordinate-rotated variant and analyzing its improved convergence properties in the face of anisotropy. Finally, for each test case, the number of iterations for both methods is computed to determine the speedup factor.\n\nThe objective function for this problem is the convex quadratic form $f(\\mathbf{x}) = \\frac{1}{2}\\,\\mathbf{x}^{\\top} \\mathbf{Q}\\,\\mathbf{x}$. The unique minimizer is at $\\mathbf{x}^{\\star} = \\mathbf{0}$, and the gradient is $\\nabla f(\\mathbf{x}) = \\mathbf{Q}\\,\\mathbf{x}$. The Hessian matrix $\\mathbf{Q}$ is constructed as $\\mathbf{Q} = \\mathbf{R}(\\theta)\\,\\mathbf{\\Lambda}\\,\\mathbf{R}(\\theta)^{\\top}$, where $\\mathbf{\\Lambda} = \\mathrm{diag}(\\lambda_{1}, \\lambda_{2})$ with eigenvalues $\\lambda_{1}  \\lambda_{2}  0$, and $\\mathbf{R}(\\theta)$ is the standard $2\\times 2$ rotation matrix for an angle $\\theta$. This construction creates a function whose level sets are ellipses with principal axes aligned with the columns of $\\mathbf{R}(\\theta)$. The ratio $\\kappa = \\lambda_{1}/\\lambda_{2}$, known as the condition number, measures the anisotropy of these ellipses. A large $\\kappa$ signifies an ill-conditioned problem.\n\n### Method 1: Baseline Gradient Descent\n\nThe standard gradient descent algorithm updates the current iterate $\\mathbf{x}_{k}$ by taking a step in the direction of the negative gradient:\n$$\n\\mathbf{x}_{k+1} = \\mathbf{x}_{k} - \\alpha\\,\\nabla f(\\mathbf{x}_{k})\n$$\nwhere $\\alpha  0$ is the step size or learning rate. For the given quadratic function, this becomes:\n$$\n\\mathbf{x}_{k+1} = \\mathbf{x}_{k} - \\alpha\\,\\mathbf{Q}\\,\\mathbf{x}_{k} = (\\mathbf{I} - \\alpha\\,\\mathbf{Q})\\,\\mathbf{x}_{k}\n$$\nThe problem specifies a fixed step size $\\alpha = 1/\\lambda_{\\max}(\\mathbf{Q}) = 1/\\lambda_{1}$. The convergence of this iterative process is determined by the spectral radius of the iteration matrix $\\mathbf{M} = \\mathbf{I} - \\alpha\\,\\mathbf{Q}$. The eigenvalues of $\\mathbf{M}$ are $1-\\alpha\\lambda_{i}$. With $\\alpha=1/\\lambda_1$, the eigenvalues are $1 - \\lambda_1/\\lambda_1 = 0$ and $1 - \\lambda_2/\\lambda_1$. The spectral radius is therefore $\\rho(\\mathbf{M}) = |1 - \\lambda_2/\\lambda_1| = 1 - 1/\\kappa$. When the condition number $\\kappa$ is large, $\\rho(\\mathbf{M})$ is very close to $1$, leading to slow convergence. The algorithm starts from $\\mathbf{x}_{0} = (1,\\,1)^{\\top}$ and terminates when the Euclidean norm $\\|\\mathbf{x}_{k}\\|_{2} \\le \\varepsilon$, yielding the iteration count $N_{\\mathrm{plain}}$.\n\n### Method 2: Rotated-Coordinate Gradient Descent\n\nThis method first performs a change of variables to align the coordinate system with the principal axes of the function's level sets. The new coordinate vector is defined as $\\mathbf{y} = \\mathbf{R}(\\theta)^{\\top}\\mathbf{x}$. The inverse transformation is $\\mathbf{x} = \\mathbf{R}(\\theta)\\mathbf{y}$. Substituting this into the objective function yields:\n$$\nf(\\mathbf{y}) = \\frac{1}{2}\\,\\left(\\mathbf{R}(\\theta)\\mathbf{y}\\right)^{\\top} \\left(\\mathbf{R}(\\theta)\\mathbf{\\Lambda}\\mathbf{R}(\\theta)^{\\top}\\right) \\left(\\mathbf{R}(\\theta)\\mathbf{y}\\right)\n$$\nSince $\\mathbf{R}(\\theta)$ is orthogonal, $\\mathbf{R}(\\theta)^{\\top}\\mathbf{R}(\\theta) = \\mathbf{I}$. The expression simplifies to:\n$$\nf(\\mathbf{y}) = \\frac{1}{2}\\,\\mathbf{y}^{\\top} \\mathbf{R}(\\theta)^{\\top}\\mathbf{R}(\\theta)\\mathbf{\\Lambda}\\mathbf{R}(\\theta)^{\\top}\\mathbf{R}(\\theta)\\mathbf{y} = \\frac{1}{2}\\,\\mathbf{y}^{\\top} \\mathbf{\\Lambda}\\,\\mathbf{y}\n$$\nIn this new coordinate system, the Hessian is the diagonal matrix $\\mathbf{\\Lambda}$, meaning the problem decouples into two independent one-dimensional optimization problems. The gradient in the $\\mathbf{y}$-coordinates is $\\nabla_{\\mathbf{y}} f(\\mathbf{y}) = \\mathbf{\\Lambda}\\mathbf{y}$.\n\nThe update rule uses per-coordinate step sizes, $\\alpha_{1} = c/\\lambda_{1}$ and $\\alpha_{2} = c/\\lambda_{2}$, where $c \\in (0, 2)$ is a shared scalar. The update is:\n$$\n\\mathbf{y}_{k+1} = \\mathbf{y}_{k} - \\mathrm{diag}(\\alpha_{1}, \\alpha_{2})\\,\\nabla_{\\mathbf{y}} f(\\mathbf{y}_{k}) = \\mathbf{y}_{k} - \\mathrm{diag}(c/\\lambda_{1}, c/\\lambda_{2})\\,(\\mathbf{\\Lambda}\\mathbf{y}_{k})\n$$\nSince $\\mathbf{\\Lambda} = \\mathrm{diag}(\\lambda_1, \\lambda_2)$, the product of the diagonal matrices simplifies:\n$$\n\\mathrm{diag}(c/\\lambda_{1}, c/\\lambda_{2})\\,\\mathrm{diag}(\\lambda_{1}, \\lambda_{2}) = \\mathrm{diag}(c, c) = c\\mathbf{I}\n$$\nThe update rule thus becomes remarkably simple:\n$$\n\\mathbf{y}_{k+1} = \\mathbf{y}_{k} - c\\mathbf{I}\\mathbf{y}_{k} = (1-c)\\mathbf{y}_{k}\n$$\nThis is a simple geometric contraction. The convergence rate is $|1-c|$, which is independent of the eigenvalues $\\lambda_1, \\lambda_2$ and thus independent of the original problem's condition number. The algorithm starts from the transformed initial condition $\\mathbf{y}_{0} = \\mathbf{R}(\\theta)^{\\top}\\mathbf{x}_{0}$ and terminates when $\\|\\mathbf{y}_{k}\\|_{2} \\le \\varepsilon$, giving the iteration count $N_{\\mathrm{rot}}$. Note that $\\|\\mathbf{y}_{k}\\|_{2} = \\|\\mathbf{R}(\\theta)^{\\top}\\mathbf{x}_{k}\\|_{2} = \\|\\mathbf{x}_{k}\\|_{2}$ because rotations preserve the Euclidean norm.\n\n### Speedup Calculation\n\nThe performance improvement of the rotated-coordinate method over the baseline is quantified by the speedup factor $S = N_{\\mathrm{plain}}/N_{\\mathrm{rot}}$. For each test case, we implement both algorithms, count their iterations, and compute this ratio.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares two gradient descent methods on a 2D convex quadratic,\n    calculates the speedup factor for three test cases.\n    \"\"\"\n    test_cases = [\n        # (lambda_1, lambda_2, theta, epsilon, c)\n        (200.0, 1.0, np.pi/4, 1e-8, 1.6),\n        (10.0, 1.0, 0.4, 1e-8, 1.6),\n        (1.0, 1.0, 0.7, 1e-8, 1.6),\n    ]\n\n    results = []\n    max_iterations = 1_000_000\n\n    for case in test_cases:\n        lambda_1, lambda_2, theta, epsilon, c = case\n\n        # Common initial condition\n        x0 = np.array([1.0, 1.0])\n\n        # --- Task 1: Baseline Gradient Descent ---\n        \n        # Construct the Hessian matrix Q\n        cos_th, sin_th = np.cos(theta), np.sin(theta)\n        R = np.array([[cos_th, -sin_th], [sin_th, cos_th]])\n        Lambda = np.diag([lambda_1, lambda_2])\n        Q = R @ Lambda @ R.T\n\n        # Set up and run the iteration\n        alpha = 1.0 / lambda_1\n        x_k = np.copy(x0)\n        N_plain = 0\n        while np.linalg.norm(x_k) > epsilon and N_plain  max_iterations:\n            gradient = Q @ x_k\n            x_k = x_k - alpha * gradient\n            N_plain += 1\n\n        # --- Task 2: Rotated-Coordinate Gradient Descent ---\n\n        # Initial condition in rotated coordinates\n        y0 = R.T @ x0\n        \n        # Rate of contraction is (1-c)\n        contraction_factor = 1.0 - c\n        \n        y_k = np.copy(y0)\n        N_rot = 0\n        while np.linalg.norm(y_k) > epsilon and N_rot  max_iterations:\n            # The update rule y_{k+1} = (1-c) * y_k is computationally simpler\n            # than re-calculating the gradient at each step, but we implement\n            # the specified update rule for formal correctness.\n            # grad_y = Lambda @ y_k\n            # step_sizes = np.diag([c / lambda_1, c / lambda_2])\n            # y_k = y_k - step_sizes @ grad_y\n            # The above is equivalent to:\n            y_k = contraction_factor * y_k\n            N_rot += 1\n\n        # --- Task 3: Compute Speedup Factor ---\n        if N_rot > 0:\n            speedup = N_plain / N_rot\n        else:\n            # Handle the case where N_rot could be zero, though not expected\n            # for the given test cases and epsilon > 0.\n            speedup = float('inf') if N_plain > 0 else 1.0\n\n        results.append(round(speedup, 3))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3139501"}, {"introduction": "Many optimization problems in science and engineering are not unconstrained; they require the solution to satisfy specific conditions, such as probabilities needing to be non-negative and sum to one. This exercise [@problem_id:3139517] tackles this challenge by asking you to minimize a function over the probability simplex. You will implement and compare two cornerstone methods for constrained optimization: projected gradient descent, which enforces constraints after each step, and reparameterization using the softmax function, which transforms the problem into an unconstrained one.", "problem": "You will implement and compare two variants of gradient descent for minimizing smooth objectives over a probability vector. A probability vector is a vector $\\mathbf{p} \\in \\mathbb{R}^n$ with $p_i \\ge 0$ for all $i$ and $\\sum_{i=1}^n p_i = 1$. The feasible set is the probability simplex $\\Delta^n := \\{\\mathbf{p} \\in \\mathbb{R}^n \\mid p_i \\ge 0, \\sum_i p_i = 1\\}$. You must use only first principles and core definitions of gradients and constrained optimization, without shortcut formulas provided here. Your program must be a single, standalone script that takes no input and prints exactly one line as specified below.\n\nFundamental base and target. Start from the definition that, for a differentiable objective $f(\\mathbf{p})$, a basic gradient descent step in an unconstrained space updates $\\mathbf{z}$ by $\\mathbf{z}^{(t+1)} = \\mathbf{z}^{(t)} - \\eta_t \\nabla f(\\mathbf{z}^{(t)})$, where $\\eta_t$ is a positive step size and $\\nabla f$ is the gradient. When constraints are present, one principled approach is to reparameterize to an unconstrained variable and use the chain rule, and another is to descend in the original variable and then project back to the feasible set with respect to the Euclidean norm. You must derive the necessary expressions from these bases.\n\nAlgorithms to implement and compare:\n- Method A (Projected Gradient on the Simplex): Work directly on $\\mathbf{p} \\in \\Delta^n$. At each iteration, take an unconstrained gradient descent step in $\\mathbf{p}$ and then compute the Euclidean projection back onto $\\Delta^n$. You must derive and implement the Euclidean projection onto $\\Delta^n$ by solving the corresponding constrained least-squares problem using first-order optimality conditions; do not use any pre-packaged projection routine.\n- Method B (Softmax Reparameterization): Introduce an unconstrained parameter $\\boldsymbol{\\theta} \\in \\mathbb{R}^n$ and define $\\mathbf{p} = \\mathrm{softmax}(\\boldsymbol{\\theta})$, where the softmax is given by $p_i = \\exp(\\theta_i) / \\sum_{j=1}^n \\exp(\\theta_j)$. Apply gradient descent in $\\boldsymbol{\\theta}$ for the composite objective $f(\\mathrm{softmax}(\\boldsymbol{\\theta}))$. You must derive the explicit expression for $\\nabla_{\\boldsymbol{\\theta}} f(\\mathrm{softmax}(\\boldsymbol{\\theta}))$ from the chain rule and the Jacobian of the softmax function; do not assume the expression is given.\n\nStep size schedule and initialization. For both methods use the same diminishing step size schedule $\\eta_t = \\eta_0 / \\sqrt{t+1}$ with $\\eta_0 = 0.3$, and a fixed iteration budget of $T = 4000$. Initialize $\\mathbf{p}^{(0)}$ for the projected method as the uniform vector, that is $p_i^{(0)} = 1/n$. Initialize $\\boldsymbol{\\theta}^{(0)}$ as the zero vector so that $\\mathrm{softmax}(\\boldsymbol{\\theta}^{(0)})$ is also uniform. Use $n = 4$ throughout.\n\nObjectives and gradients. You must implement the following objectives $f(\\mathbf{p})$ on $\\Delta^4$ and their gradients $\\nabla_{\\mathbf{p}} f(\\mathbf{p})$ from definitions. In all cases, $\\|\\cdot\\|_2$ denotes the Euclidean norm. All computations are dimensionless; no physical units are involved.\n- Case $1$ (Quadratic toward a target vector): $f(\\mathbf{p}) = \\tfrac{1}{2} \\|\\mathbf{p} - \\mathbf{q}\\|_2^2$, with $\\mathbf{q} = [0.7, 0.1, 0.1, 0.1]$.\n- Case $2$ (Linear objective): $f(\\mathbf{p}) = \\mathbf{c}^\\top \\mathbf{p}$, with $\\mathbf{c} = [0.5, -1.0, 0.2, 0.2]$.\n- Case $3$ (Quadratic data fit): $f(\\mathbf{p}) = \\tfrac{1}{2} \\|A \\mathbf{p} - \\mathbf{b}\\|_2^2$, with\n  $$\n  A = \\begin{bmatrix}\n  2  -1  0  0 \\\\\n  -1  2  -1  0 \\\\\n  0  -1  2  -1 \\\\\n  0  0  -1  2\n  \\end{bmatrix}, \\quad \\mathbf{b} = [1.0, 0.0, 0.0, 0.0].\n  $$\n- Case $4$ (Quadratic toward uniform): $f(\\mathbf{p}) = \\tfrac{1}{2} \\|\\mathbf{p} - \\mathbf{u}\\|_2^2$, with $\\mathbf{u} = [0.25, 0.25, 0.25, 0.25]$.\n\nFor each case, implement $\\nabla_{\\mathbf{p}} f(\\mathbf{p})$ from first principles. For the linear objective, recall that the gradient is constant. For the quadratic objectives, use the standard rule that for $f(\\mathbf{p}) = \\tfrac{1}{2}\\|\\mathbf{M}\\mathbf{p} - \\mathbf{r}\\|_2^2$, one has $\\nabla_{\\mathbf{p}} f(\\mathbf{p}) = \\mathbf{M}^\\top (\\mathbf{M}\\mathbf{p} - \\mathbf{r})$, which follows from the chain rule and linearity of differentiation.\n\nComparison metric. For each case, after running both methods for exactly $T$ iterations with the specified schedule and initializations, compute the $\\ell_1$ distance between the resulting probability vectors $\\mathbf{p}_{\\mathrm{softmax}}$ and $\\mathbf{p}_{\\mathrm{proj}}$, namely $\\|\\mathbf{p}_{\\mathrm{softmax}} - \\mathbf{p}_{\\mathrm{proj}}\\|_1 = \\sum_{i=1}^4 |p_{\\mathrm{softmax},i} - p_{\\mathrm{proj},i}|$.\n\nTest suite. Use the four cases above as the test suite. These cover: an interior-like target (Case $1$), a boundary optimum (Case $2$), a structured quadratic with coupling (Case $3$), and an already optimal starting point (Case $4$).\n\nRequired final output format. Your program should produce a single line of output containing a list of four floating-point numbers, each equal to the $\\ell_1$ distance for the corresponding case, rounded to six decimal places, as a comma-separated list enclosed in square brackets, for example, \"[0.000123,0.045678,0.001000,0.000000]\".", "solution": "We present principled derivations and algorithmic designs grounded in core definitions of gradients, the chain rule, and constrained optimization via projection.\n\nFeasible set and objectives. The feasible set is the probability simplex $\\Delta^n = \\{\\mathbf{p} \\in \\mathbb{R}^n \\mid p_i \\ge 0,\\ \\sum_{i=1}^n p_i = 1\\}$. We consider $n = 4$ and the four objectives specified. For linear and quadratic functions, the gradients with respect to $\\mathbf{p}$ follow from basic rules:\n- For $f(\\mathbf{p}) = \\mathbf{c}^\\top \\mathbf{p}$, the gradient is $\\nabla_{\\mathbf{p}} f(\\mathbf{p}) = \\mathbf{c}$, because differentiation is linear and $\\partial (\\mathbf{c}^\\top \\mathbf{p}) / \\partial \\mathbf{p} = \\mathbf{c}$.\n- For $f(\\mathbf{p}) = \\tfrac{1}{2}\\|\\mathbf{M} \\mathbf{p} - \\mathbf{r}\\|_2^2$, define $\\mathbf{y}(\\mathbf{p}) = \\mathbf{M}\\mathbf{p} - \\mathbf{r}$ and $g(\\mathbf{y}) = \\tfrac{1}{2} \\|\\mathbf{y}\\|_2^2$. Then by the chain rule,\n$$\n\\nabla_{\\mathbf{p}} f(\\mathbf{p}) = \\mathbf{M}^\\top \\nabla_{\\mathbf{y}} g(\\mathbf{y}(\\mathbf{p})) = \\mathbf{M}^\\top \\mathbf{y}(\\mathbf{p}) = \\mathbf{M}^\\top (\\mathbf{M}\\mathbf{p} - \\mathbf{r}).\n$$\nTaking $\\mathbf{M} = \\mathbf{I}$ and $\\mathbf{r} = \\mathbf{q}$ yields $\\nabla_{\\mathbf{p}} f(\\mathbf{p}) = \\mathbf{p} - \\mathbf{q}$. Taking $\\mathbf{M} = A$ and $\\mathbf{r} = \\mathbf{b}$ yields $\\nabla_{\\mathbf{p}} f(\\mathbf{p}) = A^\\top( A\\mathbf{p} - \\mathbf{b})$.\n\nMethod A (Projected Gradient on the Simplex). Start from the unconstrained gradient descent update\n$$\n\\tilde{\\mathbf{p}}^{(t+1)} = \\mathbf{p}^{(t)} - \\eta_t \\nabla_{\\mathbf{p}} f(\\mathbf{p}^{(t)}),\n$$\nthen enforce feasibility by computing the Euclidean projection back onto $\\Delta^n$:\n$$\n\\mathbf{p}^{(t+1)} = \\operatorname{Proj}_{\\Delta^n}(\\tilde{\\mathbf{p}}^{(t+1)}) := \\arg\\min_{\\mathbf{x} \\in \\Delta^n} \\|\\mathbf{x} - \\tilde{\\mathbf{p}}^{(t+1)}\\|_2^2.\n$$\nWe derive the projection using the method of Lagrange multipliers and optimality conditions. Consider the optimization problem\n$$\n\\min_{\\mathbf{x} \\in \\mathbb{R}^n} \\tfrac{1}{2}\\|\\mathbf{x} - \\mathbf{v}\\|_2^2 \\quad \\text{subject to} \\quad \\sum_{i=1}^n x_i = 1,\\ \\ x_i \\ge 0 \\ \\forall i,\n$$\nwhere $\\mathbf{v} \\in \\mathbb{R}^n$ is given. Form the Lagrangian\n$$\n\\mathcal{L}(\\mathbf{x}, \\tau, \\boldsymbol{\\mu}) = \\tfrac{1}{2}\\|\\mathbf{x} - \\mathbf{v}\\|_2^2 + \\tau \\left(\\sum_{i=1}^n x_i - 1\\right) - \\sum_{i=1}^n \\mu_i x_i,\n$$\nwith $\\tau \\in \\mathbb{R}$ for the equality constraint and $\\mu_i \\ge 0$ for the inequality constraints. The Karush-Kuhn-Tucker (KKT) conditions (stationarity, primal feasibility, dual feasibility, complementary slackness) are:\n- Stationarity:\n$$\n\\nabla_{\\mathbf{x}} \\mathcal{L} = \\mathbf{x} - \\mathbf{v} + \\tau \\mathbf{1} - \\boldsymbol{\\mu} = \\mathbf{0}.\n$$\n- Primal feasibility: $\\sum_i x_i = 1$, $x_i \\ge 0$ for all $i$.\n- Dual feasibility: $\\mu_i \\ge 0$ for all $i$.\n- Complementary slackness: $\\mu_i x_i = 0$ for all $i$.\n\nFrom stationarity, for each $i$,\n$$\nx_i = v_i - \\tau + \\mu_i.\n$$\nIf $x_i  0$, then complementary slackness implies $\\mu_i = 0$, hence $x_i = v_i - \\tau$. If $x_i = 0$, then $\\mu_i \\ge 0$ and $0 = v_i - \\tau + \\mu_i$ implies $v_i - \\tau \\le 0$. Therefore the solution has the thresholding form\n$$\nx_i^\\star = \\max\\{v_i - \\tau, 0\\}\n$$\nfor some scalar $\\tau$ chosen such that $\\sum_{i=1}^n x_i^\\star = 1$. The scalar $\\tau$ is uniquely determined because the map $\\tau \\mapsto \\sum_i \\max\\{v_i - \\tau, 0\\}$ is continuous and strictly decreasing. An efficient way to compute $\\tau$ is to sort the entries of $\\mathbf{v}$ in descending order, find the largest index $\\rho$ such that $u_\\rho - \\frac{1}{\\rho+1}\\left(\\sum_{j=0}^{\\rho} u_j - 1\\right)  0$ for the sorted vector $\\mathbf{u}$, then set\n$$\n\\tau = \\frac{1}{\\rho+1}\\left(\\sum_{j=0}^{\\rho} u_j - 1\\right),\n$$\nand output $\\mathbf{x}^\\star = \\max\\{\\mathbf{v} - \\tau, 0\\}$ applied elementwise. This implements the Euclidean projection onto $\\Delta^n$.\n\nMethod B (Softmax Reparameterization). Define the unconstrained parameter $\\boldsymbol{\\theta} \\in \\mathbb{R}^n$ and set\n$$\np_i(\\boldsymbol{\\theta}) = \\frac{\\exp(\\theta_i)}{\\sum_{j=1}^n \\exp(\\theta_j)}.\n$$\nWe perform gradient descent on the composite function $F(\\boldsymbol{\\theta}) = f(\\mathbf{p}(\\boldsymbol{\\theta}))$. By the chain rule,\n$$\n\\nabla_{\\boldsymbol{\\theta}} F(\\boldsymbol{\\theta}) = J(\\boldsymbol{\\theta})^\\top \\nabla_{\\mathbf{p}} f(\\mathbf{p}(\\boldsymbol{\\theta})),\n$$\nwhere $J(\\boldsymbol{\\theta}) \\in \\mathbb{R}^{n \\times n}$ is the Jacobian of the softmax. The derivative of the softmax is a standard result obtained by differentiating $p_i = \\exp(\\theta_i)/\\sum_k \\exp(\\theta_k)$. For each $i,k$,\n$$\n\\frac{\\partial p_i}{\\partial \\theta_k} = p_i(\\delta_{ik} - p_k),\n$$\nwhere $\\delta_{ik}$ is the Kronecker delta. Therefore, if we denote $\\mathbf{g} = \\nabla_{\\mathbf{p}} f(\\mathbf{p}(\\boldsymbol{\\theta}))$, the $k$-th component of $\\nabla_{\\boldsymbol{\\theta}} F(\\boldsymbol{\\theta})$ is\n$$\n\\left[\\nabla_{\\boldsymbol{\\theta}} F(\\boldsymbol{\\theta})\\right]_k\n= \\sum_{i=1}^n g_i \\frac{\\partial p_i}{\\partial \\theta_k}\n= \\sum_{i=1}^n g_i p_i(\\delta_{ik} - p_k)\n= p_k \\left(g_k - \\sum_{i=1}^n g_i p_i\\right).\n$$\nIn vector form,\n$$\n\\nabla_{\\boldsymbol{\\theta}} F(\\boldsymbol{\\theta}) = \\mathbf{p} \\odot \\left(\\mathbf{g} - (\\mathbf{g}^\\top \\mathbf{p}) \\mathbf{1}\\right),\n$$\nwhere $\\odot$ denotes elementwise multiplication. The gradient descent update becomes\n$$\n\\boldsymbol{\\theta}^{(t+1)} = \\boldsymbol{\\theta}^{(t)} - \\eta_t \\left[ \\mathbf{p}^{(t)} \\odot \\left(\\nabla_{\\mathbf{p}} f(\\mathbf{p}^{(t)}) - \\left(\\nabla_{\\mathbf{p}} f(\\mathbf{p}^{(t)})\\right)^\\top \\mathbf{p}^{(t)} \\cdot \\mathbf{1} \\right) \\right],\n$$\nwith $\\mathbf{p}^{(t)} = \\mathrm{softmax}(\\boldsymbol{\\theta}^{(t)})$. This update ensures that the corresponding $\\mathbf{p}^{(t)}$ stays in $\\Delta^n$ by construction.\n\nStep size and initialization. We use the diminishing schedule $\\eta_t = \\eta_0/\\sqrt{t+1}$ with $\\eta_0 = 0.3$, which satisfies the common heuristic of decreasing step size to promote stability. For both methods, the initialization is the uniform distribution: directly for Method A by setting $p_i^{(0)} = 1/n$, and for Method B by setting $\\boldsymbol{\\theta}^{(0)} = \\mathbf{0}$ so that softmax yields the uniform distribution.\n\nTest cases and expectations. We implement four test cases:\n- Case $1$: $f(\\mathbf{p}) = \\tfrac{1}{2}\\|\\mathbf{p} - \\mathbf{q}\\|_2^2$ with $\\mathbf{q} = [0.7, 0.1, 0.1, 0.1]$. The unconstrained minimizer is $\\mathbf{q}$, which lies in the simplex, so the constrained minimizer is also $\\mathbf{q}$. Both methods should converge close to $\\mathbf{q}$.\n- Case $2$: $f(\\mathbf{p}) = \\mathbf{c}^\\top \\mathbf{p}$ with $\\mathbf{c} = [0.5, -1.0, 0.2, 0.2]$. The constrained minimizer is the vertex corresponding to the smallest component of $\\mathbf{c}$, namely the second coordinate; both methods should approach a distribution concentrated on that coordinate.\n- Case $3$: $f(\\mathbf{p}) = \\tfrac{1}{2}\\|A\\mathbf{p} - \\mathbf{b}\\|_2^2$ with the tridiagonal $A$ and $\\mathbf{b}$ given. The optimum may be interior or on the boundary; both methods should approach a similar solution.\n- Case $4$: $f(\\mathbf{p}) = \\tfrac{1}{2}\\|\\mathbf{p} - \\mathbf{u}\\|_2^2$ with $\\mathbf{u}$ uniform. The uniform initialization is already optimal, so both methods should remain close to uniform.\n\nComparison metric. After $T = 4000$ iterations for each case, we compute the $\\ell_1$ distance $\\|\\mathbf{p}_{\\mathrm{softmax}} - \\mathbf{p}_{\\mathrm{proj}}\\|_1 = \\sum_{i=1}^4 |p_{\\mathrm{softmax},i} - p_{\\mathrm{proj},i}|$. This metric is nonnegative, equals zero if and only if the two probability vectors match, and is sensitive to differences across coordinates.\n\nProgram design. The program will:\n- Implement the projection onto $\\Delta^n$ using the KKT-derived thresholding method described above.\n- Implement Method A and Method B with the common step schedule and initializations.\n- Define the objective and gradient functions for the four cases.\n- Run both methods for $T = 4000$ iterations on each case and collect the $\\ell_1$ distances.\n- Print a single line containing the list of four distances rounded to six decimal places, in the specified format.\n\nThis approach integrates the fundamental rules of differentiation, the chain rule, and the Karush-Kuhn-Tucker conditions to derive implementable algorithms for constrained gradient descent and reparameterized gradient descent on the probability simplex.", "answer": "```python\nimport numpy as np\n\ndef softmax(theta: np.ndarray) - np.ndarray:\n    # Numerically stable softmax\n    z = theta - np.max(theta)\n    e = np.exp(z)\n    return e / np.sum(e)\n\ndef grad_softmax_chain(p: np.ndarray, grad_p: np.ndarray) - np.ndarray:\n    # Computes grad wrt theta: p * (grad_p - grad_p, p)\n    s = float(np.dot(grad_p, p))\n    return p * (grad_p - s)\n\ndef project_to_simplex(v: np.ndarray) - np.ndarray:\n    # Euclidean projection onto the probability simplex\n    # Implements the KKT-derived thresholding with sorting\n    n = v.size\n    u = np.sort(v)[::-1]\n    cssv = np.cumsum(u)\n    rho = -1\n    for j in range(n):\n        t = u[j] - (cssv[j] - 1.0) / (j + 1)\n        if t > 0:\n            rho = j\n    if rho == -1:\n        # In pathological case, project to uniform\n        return np.full(n, 1.0 / n)\n    theta = (cssv[rho] - 1.0) / (rho + 1)\n    w = v - theta\n    w[w  0] = 0.0\n    # Numerical safeguard to ensure sum to 1\n    s = w.sum()\n    if s = 0:\n        return np.full(n, 1.0 / n)\n    return w / s\n\ndef projected_gradient(f_grad, n=4, T=4000, eta0=0.3):\n    # Initialize at uniform distribution\n    p = np.full(n, 1.0 / n)\n    for t in range(T):\n        eta = eta0 / np.sqrt(t + 1.0)\n        g = f_grad(p)\n        p = p - eta * g\n        p = project_to_simplex(p)\n    return p\n\ndef softmax_reparam_gradient(f_grad, n=4, T=4000, eta0=0.3):\n    theta = np.zeros(n)\n    for t in range(T):\n        eta = eta0 / np.sqrt(t + 1.0)\n        p = softmax(theta)\n        g_p = f_grad(p)\n        g_theta = grad_softmax_chain(p, g_p)\n        theta = theta - eta * g_theta\n    p_final = softmax(theta)\n    return p_final\n\n# Define objectives and gradients for the four cases\ndef make_case1(q):\n    q = np.asarray(q, dtype=float)\n    def f_grad(p):\n        return p - q\n    return f_grad\n\ndef make_case2(c):\n    c = np.asarray(c, dtype=float)\n    def f_grad(p):\n        return c\n    return f_grad\n\ndef make_case3(A, b):\n    A = np.asarray(A, dtype=float)\n    b = np.asarray(b, dtype=float)\n    AT = A.T\n    def f_grad(p):\n        return AT @ (A @ p - b)\n    return f_grad\n\ndef make_case4(u):\n    u = np.asarray(u, dtype=float)\n    def f_grad(p):\n        return p - u\n    return f_grad\n\ndef run_case(f_grad):\n    n = 4\n    T = 4000\n    eta0 = 0.3\n    p_proj = projected_gradient(f_grad, n=n, T=T, eta0=eta0)\n    p_soft = softmax_reparam_gradient(f_grad, n=n, T=T, eta0=eta0)\n    l1 = float(np.sum(np.abs(p_proj - p_soft)))\n    return l1\n\ndef solve():\n    # Test suite definitions\n    case1_grad = make_case1([0.7, 0.1, 0.1, 0.1])\n    case2_grad = make_case2([0.5, -1.0, 0.2, 0.2])\n    A = np.array([[2.0, -1.0, 0.0, 0.0],\n                  [-1.0, 2.0, -1.0, 0.0],\n                  [0.0, -1.0, 2.0, -1.0],\n                  [0.0, 0.0, -1.0, 2.0]], dtype=float)\n    b = np.array([1.0, 0.0, 0.0, 0.0], dtype=float)\n    case3_grad = make_case3(A, b)\n    case4_grad = make_case4([0.25, 0.25, 0.25, 0.25])\n\n    test_cases = [case1_grad, case2_grad, case3_grad, case4_grad]\n\n    results = []\n    for grad_fun in test_cases:\n        l1 = run_case(grad_fun)\n        results.append(l1)\n\n    # Print with required formatting: six decimal places, single line\n    print(\"[\" + \",\".join(f\"{x:.6f}\" for x in results) + \"]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3139517"}, {"introduction": "Beyond the mathematical theory, practical optimization on modern hardware involves trade-offs between computational speed and numerical precision. This exercise [@problem_id:3139464] delves into mixed-precision training, a key technique for accelerating gradient descent by using fast, low-precision arithmetic for most calculations. You will build a solver that maintains a high-precision master copy of the parameters while using low-precision gradients, and explore the role of \"loss scaling\" to prevent numerical underflow and maintain accuracy.", "problem": "You are to design, implement, and evaluate a mixed-precision gradient descent method for smooth convex objectives, using a principled derivation from first principles. Begin from the definitions of differentiability and convexity, and the first-order Taylor model of a differentiable function. Using this base, derive a descent direction update rule that ensures decrease of the objective under an appropriate step size. Your implementation must realize two solvers: a full-precision baseline and a mixed-precision variant that performs arithmetic in half precision while maintaining a master copy of the parameters in single precision. You will test both solvers on a suite of convex quadratic objectives and quantify the accuracy loss introduced by mixed precision.\n\nDefinitions and constraints to use:\n- Let $f:\\mathbb{R}^d \\to \\mathbb{R}$ be differentiable and convex with Lipschitz-continuous gradient with constant $L \\gt 0$. The first-order Taylor model about $x \\in \\mathbb{R}^d$ is $f(x + \\Delta) \\approx f(x) + \\nabla f(x)^\\top \\Delta$. Steepest descent in the negative gradient direction provides a principled way to reduce $f$ for sufficiently small step size.\n- A convex quadratic objective has the form $f(x) = \\tfrac{1}{2} x^\\top A x - b^\\top x + c$ where $A \\in \\mathbb{R}^{d \\times d}$ is symmetric positive definite, $b \\in \\mathbb{R}^d$, and $c \\in \\mathbb{R}$.\n\nAlgorithmic requirements:\n- Baseline solver (full precision): Maintain parameters and all computations in single precision (floating-point $32$-bit). Start from $x_0 \\in \\mathbb{R}^d$, and iterate $x_{k+1} = x_k - \\alpha \\, g_k$ for $k = 0,1,\\dots,T-1$, where $g_k$ is the gradient of $f$ at $x_k$ and $\\alpha \\gt 0$ is a fixed step size.\n- Mixed-precision solver: Maintain a master copy of parameters $x_k$ in floating-point $32$-bit. At each iteration:\n  1. Cast the current parameters to floating-point $16$-bit to obtain $x_k^{(16)}$.\n  2. Compute the gradient using floating-point $16$-bit arithmetic with an explicit loss-scaling factor $s \\in \\mathbb{R}_{\\gt 0}$ to mitigate underflow. For a quadratic objective the true gradient is $\\nabla f(x) = A x - b$. Compute in $16$-bit the scaled gradient $g_k^{\\mathrm{scaled},(16)} = (s A^{(16)}) x_k^{(16)} - (s b^{(16)})$, where $A^{(16)}$ and $b^{(16)}$ are $A$ and $b$ cast to floating-point $16$-bit. Then convert back to floating-point $32$-bit and unscale: $g_k^{(32)} = \\mathrm{cast32}(g_k^{\\mathrm{scaled},(16)}) / s$.\n  3. Update the master parameters in floating-point $32$-bit: $x_{k+1} = x_k - \\alpha \\, g_k^{(32)}$.\n- For objective evaluation and error measurement after the iterations, always compute $f(x)$ in high precision using floating-point $64$-bit arithmetic to avoid contaminating the accuracy assessment.\n\nTest suite:\nImplement and run both solvers on the following four convex quadratic test cases. In each case, use the same fixed step size $\\alpha$ and iteration count $T$ for both solvers, start from the same initial point $x_0$, and compare the final objective values and parameters.\n\nFor each case $i \\in \\{1,2,3,4\\}$, report whether the mixed-precision result matches the baseline within specified absolute thresholds $\\tau_f$ for the final objective value difference and $\\tau_x$ for the root-mean-square (RMS) parameter difference. The pass/fail condition is:\n- Let $\\Delta f_i = \\left| f(x^{\\mathrm{mixed}}_T) - f(x^{\\mathrm{base}}_T) \\right|$ and $\\Delta x_i = \\sqrt{\\tfrac{1}{d} \\| x^{\\mathrm{mixed}}_T - x^{\\mathrm{base}}_T \\|_2^2}$. Case $i$ passes if and only if $\\Delta f_i \\le \\tau_f$ and $\\Delta x_i \\le \\tau_x$.\n\nThe four cases are:\n\n- Case $1$ (well-conditioned $2$-dimensional quadratic, happy path):\n  - $d = 2$,\n  - $A = \\begin{bmatrix} 1  0 \\\\ 0  2 \\end{bmatrix}$,\n  - $b = \\begin{bmatrix} 1 \\\\ -2 \\end{bmatrix}$,\n  - $c = 0$,\n  - $x_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$,\n  - $\\alpha = 0.45$,\n  - $T = 200$,\n  - loss-scaling factor $s = 1$,\n  - thresholds: $\\tau_f = 10^{-4}$, $\\tau_x = 10^{-4}$.\n\n- Case $2$ (ill-conditioned $2$-dimensional quadratic, near stability boundary):\n  - $d = 2$,\n  - $A = \\begin{bmatrix} 1  0 \\\\ 0  1000 \\end{bmatrix}$,\n  - $b = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$,\n  - $c = 0$,\n  - $x_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$,\n  - $\\alpha = 0.0009$,\n  - $T = 5000$,\n  - $s = 1$,\n  - thresholds: $\\tau_f = 10^{-3}$, $\\tau_x = 10^{-3}$.\n\n- Case $3$ (moderately conditioned $10$-dimensional diagonal quadratic):\n  - $d = 10$,\n  - $A = \\mathrm{diag}\\left( 1, 2, 3, 5, 8, 13, 21, 34, 55, 89 \\right)$,\n  - $b = \\begin{bmatrix} 1 \\\\ -1 \\\\ 0.5 \\\\ -0.5 \\\\ 0.25 \\\\ -0.25 \\\\ 0.125 \\\\ -0.125 \\\\ 0.0625 \\\\ -0.0625 \\end{bmatrix}$,\n  - $c = 0$,\n  - $x_0 = \\mathbf{0}_{10}$,\n  - $\\alpha = 0.009$,\n  - $T = 8000$,\n  - $s = 1$,\n  - thresholds: $\\tau_f = 5 \\times 10^{-3}$, $\\tau_x = 5 \\times 10^{-3}$.\n\n- Case $4$ (tiny-scale $5$-dimensional quadratic to test underflow and loss scaling):\n  - $d = 5$,\n  - $A = 10^{-6} \\, I_5$,\n  - $b = 10^{-6} \\, \\mathbf{1}_5$,\n  - $c = 0$,\n  - $x_0 = \\mathbf{0}_5$,\n  - $\\alpha = 5 \\times 10^{5}$,\n  - $T = 20$,\n  - $s = 4096$,\n  - thresholds: $\\tau_f = 5 \\times 10^{-7}$, $\\tau_x = 2 \\times 10^{-5}$.\n\nImplementation details:\n- For the quadratic objective, use the fundamental gradient identity $\\nabla f(x) = A x - b$ derived from the definition of differentiation applied to a quadratic form.\n- For final accuracy measurement only, compute $f(x)$ in floating-point $64$-bit as $f(x) = \\tfrac{1}{2} x^\\top A x - b^\\top x + c$.\n\nRequired final output format:\n- Your program should produce a single line of output containing a Python-style list of booleans for the four cases, in order from Case $1$ to Case $4$, where each boolean is $\\mathrm{True}$ if and only if the case passes the thresholds and $\\mathrm{False}$ otherwise. For example: \"[True,False,True,True]\".\n- No other text should be printed.\n\nAngle units are not involved. There are no physical units. All numeric results are dimensionless real numbers represented as standard floating-point values. The output should be computed deterministically with the specified data and must not depend on any external input.", "solution": "We begin from first principles for smooth convex optimization. Let $f:\\mathbb{R}^d \\to \\mathbb{R}$ be differentiable and convex with Lipschitz-continuous gradient with constant $L \\gt 0$, meaning $\\| \\nabla f(x) - \\nabla f(y) \\|_2 \\le L \\|x - y\\|_2$ for all $x,y \\in \\mathbb{R}^d$. The first-order Taylor expansion gives, for small $\\Delta \\in \\mathbb{R}^d$,\n$$\nf(x + \\Delta) \\approx f(x) + \\nabla f(x)^\\top \\Delta.\n$$\nTo decrease $f$, one selects a direction $\\Delta$ that makes $\\nabla f(x)^\\top \\Delta$ negative. The direction of steepest descent under the Euclidean norm is the negative gradient, which follows from the Cauchy–Schwarz inequality:\n$$\n\\min_{\\|\\Delta\\|_2 = \\eta} \\nabla f(x)^\\top \\Delta = - \\eta \\, \\|\\nabla f(x)\\|_2,\n$$\nattained at $\\Delta = - \\eta \\, \\tfrac{\\nabla f(x)}{\\|\\nabla f(x)\\|_2}$. A practical iterative scheme uses a step of size $\\alpha \\gt 0$ in the negative gradient direction,\n$$\nx_{k+1} = x_k - \\alpha \\, \\nabla f(x_k).\n$$\nFor $L$-Lipschitz gradients, the standard descent lemma implies that for $\\alpha \\in (0, 2/L)$ the method produces a non-increasing sequence of objective values, and for convex $f$ it converges to a minimizer under appropriate conditions.\n\nFor the convex quadratic $f(x) = \\tfrac{1}{2} x^\\top A x - b^\\top x + c$ with $A \\in \\mathbb{R}^{d \\times d}$ symmetric positive definite and $b \\in \\mathbb{R}^d$, the gradient is obtained by differentiating term by term. The gradient of the quadratic form is\n$$\n\\nabla \\left( \\tfrac{1}{2} x^\\top A x \\right) = \\tfrac{1}{2} \\left( A + A^\\top \\right) x = A x,\n$$\nbecause $A$ is symmetric, and the gradient of the linear term is $-b$. Therefore,\n$$\n\\nabla f(x) = A x - b.\n$$\nThis identity is the workhorse for our implementation.\n\nMixed precision design. Floating-point $16$-bit arithmetic (half precision) has a limited dynamic range and coarser discretization than floating-point $32$-bit (single precision). To preserve stability, we maintain a master copy of parameters $x_k$ in floating-point $32$-bit and perform gradient computations in floating-point $16$-bit to reduce computational cost. To mitigate underflow in the small-magnitude regime, we use loss scaling. Define a scale $s \\gt 0$ and consider the scaled objective $f_s(x) = s \\, f(x)$. Its gradient obeys $\\nabla f_s(x) = s \\, \\nabla f(x)$. If we compute $\\nabla f_s(x)$ in floating-point $16$-bit and then divide by $s$ in floating-point $32$-bit before applying the update, we recover the same update direction but with larger intermediate magnitudes that are less likely to underflow in $16$-bit.\n\nFor the quadratic case, computing $\\nabla f_s(x)$ stably in floating-point $16$-bit means performing the scaling before the operations prone to underflow. Specifically, one should compute\n$$\n\\nabla f_s(x) = s \\, (A x - b) = (s A) x - s b\n$$\nin floating-point $16$-bit arithmetic, where the scale $s$ is applied to $A$ and $b$ prior to the matrix-vector product and subtraction. This ensures that small elements of $A$ and $b$ are amplified before arithmetic, lessening the chance of being rounded to zero in $16$-bit. After obtaining the scaled gradient in floating-point $16$-bit, we convert it to floating-point $32$-bit and unscale:\n$$\ng^{(32)} = \\frac{\\mathrm{cast}_{32} \\left( \\nabla f_s(x)^{(16)} \\right)}{s}.\n$$\nThen the parameter update is applied in floating-point $32$-bit:\n$$\nx_{k+1} = x_k - \\alpha \\, g^{(32)}.\n$$\n\nAccuracy assessment. To quantify the mixed-precision accuracy loss, we compare against a full single-precision baseline computed using the same step size and iteration count from the same initial condition. We define two metrics:\n- Final objective gap $\\Delta f = \\left| f(x_T^{\\mathrm{mixed}}) - f(x_T^{\\mathrm{base}}) \\right|$.\n- Parameter RMS difference $\\Delta x = \\sqrt{ \\tfrac{1}{d} \\| x_T^{\\mathrm{mixed}} - x_T^{\\mathrm{base}} \\|_2^2 }$.\nTo avoid contaminating these assessments with arithmetic noise, $f(x)$ is evaluated in floating-point $64$-bit.\n\nTest suite design rationale:\n- Case $1$ is well-conditioned with $A = \\mathrm{diag}(1,2)$ and a step size $\\alpha = 0.45$ close to but below $1/L = 1/2$. It exercises a typical “happy path” where both solvers should agree closely; thresholds $\\tau_f = 10^{-4}$ and $\\tau_x = 10^{-4}$ are tight but achievable.\n- Case $2$ is ill-conditioned with $A = \\mathrm{diag}(1,1000)$ and a step size $\\alpha = 9 \\times 10^{-4}$ slightly below $1/L = 10^{-3}$, stressing sensitivity to roundoff and slow convergence in the stiff direction. Thresholds are relaxed to $\\tau_f = 10^{-3}$ and $\\tau_x = 10^{-3}$.\n- Case $3$ is a $10$-dimensional diagonal quadratic with a spread of eigenvalues up to $89$ and mixed signs and magnitudes in $b$, using $\\alpha = 0.009$ which respects $\\alpha \\lt 2/L \\approx 0.02247$. Thresholds $\\tau_f = 5 \\times 10^{-3}$ and $\\tau_x = 5 \\times 10^{-3}$ reflect the wider dimensionality and rounding accumulation.\n- Case $4$ uses a tiny-scale problem with $A = 10^{-6} I_5$, $b = 10^{-6} \\mathbf{1}$, and a large but stable step size $\\alpha = 5 \\times 10^5$ (well below $2/L = 2 \\times 10^6$) to deliberately drive gradients into the underflow regime for floating-point $16$-bit. Loss scaling with $s = 4096$ rescales intermediate quantities to the normal range of floating-point $16$-bit. Thresholds $\\tau_f = 5 \\times 10^{-7}$ and $\\tau_x = 2 \\times 10^{-5}$ balance the small absolute scale with the expected rounding.\n\nImplementation plan:\n- Implement a function to evaluate $f(x)$ in floating-point $64$-bit as $f(x) = \\tfrac{1}{2} x^\\top A x - b^\\top x + c$.\n- Implement baseline gradient descent in floating-point $32$-bit using $g = A x - b$ and the update $x \\leftarrow x - \\alpha g$ for $T$ iterations.\n- Implement mixed-precision gradient descent with a master parameter vector in floating-point $32$-bit, computing the scaled gradient in floating-point $16$-bit as $(s A) x - s b$, unscaling in floating-point $32$-bit, and updating the master parameters in floating-point $32$-bit.\n- For each case, compute $(\\Delta f, \\Delta x)$ and check whether both are below $(\\tau_f, \\tau_x)$, returning $\\mathrm{True}$ if they are and $\\mathrm{False}$ otherwise.\n- Output a single-line Python-style list of booleans for the four cases.\n\nThis approach adheres to the fundamental descent principle derived from the first-order model and leverages mixed precision carefully by applying loss scaling before half-precision arithmetic to preserve numerical fidelity in challenging regimes such as Case $4$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef objective_value(A64, b64, c64, x64):\n    # Compute f(x) = 0.5 * x^T A x - b^T x + c in float64\n    return 0.5 * float(x64.T @ (A64 @ x64)) - float(b64.T @ x64) + float(c64)\n\ndef gradient_descent_baseline(A, b, x0, alpha, iters):\n    # Baseline GD in float32 for compute and state\n    A32 = A.astype(np.float32)\n    b32 = b.astype(np.float32)\n    x = x0.astype(np.float32).copy()\n    for _ in range(iters):\n        g = A32 @ x - b32  # grad in float32\n        x = x - np.float32(alpha) * g\n    return x.astype(np.float32)\n\ndef gradient_descent_mixed(A, b, x0, alpha, iters, loss_scale):\n    # Mixed precision: FP32 master weights, FP16 compute with loss scaling\n    # Master\n    x_master = x0.astype(np.float32).copy()\n    # Constants in both dtypes\n    A16 = A.astype(np.float16)\n    b16 = b.astype(np.float16)\n    s16 = np.float16(loss_scale)\n    s32 = np.float32(loss_scale)\n    alpha32 = np.float32(alpha)\n    for _ in range(iters):\n        # Cast master params to FP16 for compute\n        x16 = x_master.astype(np.float16)\n        # Compute scaled gradient entirely in FP16: (s*A) x - s*b\n        # Do scaling before the matvec/subtraction to mitigate underflow\n        # This is equivalent to s * (A @ x - b)\n        g_scaled_16 = (s16 * A16) @ x16 - (s16 * b16)\n        # Unscale in FP32\n        g32 = g_scaled_16.astype(np.float32) / s32\n        # Update master weights in FP32\n        x_master = x_master - alpha32 * g32\n    return x_master.astype(np.float32)\n\ndef run_case(A, b, c, x0, alpha, iters, loss_scale, tau_f, tau_x):\n    # Solve with baseline and mixed precision\n    x_base = gradient_descent_baseline(A, b, x0, alpha, iters)\n    x_mix = gradient_descent_mixed(A, b, x0, alpha, iters, loss_scale)\n\n    # Evaluate objective and differences in float64 for assessment\n    A64 = A.astype(np.float64)\n    b64 = b.astype(np.float64)\n    c64 = np.float64(c)\n    x_base_64 = x_base.astype(np.float64)\n    x_mix_64 = x_mix.astype(np.float64)\n\n    f_base = objective_value(A64, b64, c64, x_base_64)\n    f_mix = objective_value(A64, b64, c64, x_mix_64)\n    df = abs(f_mix - f_base)\n\n    dx_rms = float(np.sqrt(np.mean((x_mix_64 - x_base_64) ** 2)))\n\n    ok = (df = tau_f) and (dx_rms = tau_x)\n    return ok, df, dx_rms\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        {\n            \"A\": np.array([[1.0, 0.0],\n                           [0.0, 2.0]], dtype=np.float64),\n            \"b\": np.array([1.0, -2.0], dtype=np.float64),\n            \"c\": 0.0,\n            \"x0\": np.zeros(2, dtype=np.float64),\n            \"alpha\": 0.45,\n            \"iters\": 200,\n            \"loss_scale\": 1.0,\n            \"tau_f\": 1e-4,\n            \"tau_x\": 1e-4,\n        },\n        # Case 2\n        {\n            \"A\": np.array([[1.0, 0.0],\n                           [0.0, 1000.0]], dtype=np.float64),\n            \"b\": np.array([1.0, 1.0], dtype=np.float64),\n            \"c\": 0.0,\n            \"x0\": np.zeros(2, dtype=np.float64),\n            \"alpha\": 0.0009,\n            \"iters\": 5000,\n            \"loss_scale\": 1.0,\n            \"tau_f\": 1e-3,\n            \"tau_x\": 1e-3,\n        },\n        # Case 3\n        {\n            \"A\": np.diag([1.0, 2.0, 3.0, 5.0, 8.0, 13.0, 21.0, 34.0, 55.0, 89.0]).astype(np.float64),\n            \"b\": np.array([1.0, -1.0, 0.5, -0.5, 0.25, -0.25, 0.125, -0.125, 0.0625, -0.0625], dtype=np.float64),\n            \"c\": 0.0,\n            \"x0\": np.zeros(10, dtype=np.float64),\n            \"alpha\": 0.009,\n            \"iters\": 8000,\n            \"loss_scale\": 1.0,\n            \"tau_f\": 5e-3,\n            \"tau_x\": 5e-3,\n        },\n        # Case 4\n        {\n            \"A\": (1e-6) * np.eye(5, dtype=np.float64),\n            \"b\": (1e-6) * np.ones(5, dtype=np.float64),\n            \"c\": 0.0,\n            \"x0\": np.zeros(5, dtype=np.float64),\n            \"alpha\": 5e5,\n            \"iters\": 20,\n            \"loss_scale\": 4096.0,\n            \"tau_f\": 5e-7,\n            \"tau_x\": 2e-5,\n        },\n    ]\n\n    results = []\n    # Optionally also compute and store df and dx_rms internally; only booleans are printed\n    for case in test_cases:\n        ok, df, dx = run_case(\n            case[\"A\"], case[\"b\"], case[\"c\"], case[\"x0\"],\n            case[\"alpha\"], case[\"iters\"], case[\"loss_scale\"],\n            case[\"tau_f\"], case[\"tau_x\"]\n        )\n        results.append(ok)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3139464"}]}