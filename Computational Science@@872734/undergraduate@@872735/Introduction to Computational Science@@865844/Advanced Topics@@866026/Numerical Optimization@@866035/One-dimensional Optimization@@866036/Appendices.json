{"hands_on_practices": [{"introduction": "The best way to truly understand an algorithm is to build it from the ground up. This first practice challenges you to implement the Golden-Section Search (GSS) algorithm starting from its fundamental principles. By deriving the point-placement rule based on self-similarity and interval contraction, you will gain a deep appreciation for its elegant design. The curated test suite, including a special case where an initial point lands exactly on the minimizer [@problem_id:3166878], is specifically designed to demonstrate a key property of GSS: its guaranteed, steady convergence rate, which is independent of the function's specific shape beyond unimodality.", "problem": "You are to design and implement an algorithm for one-dimensional minimization using Golden-section search (GSS), starting from first principles. The fundamental base to use is the definition of a unimodal function on a closed interval and the requirement that the sample points be chosen to preserve self-similarity of the search interval so that one function value can be reused at each iteration. A continuous unimodal function has exactly one local minimum on an interval, and shrinking the interval while preserving a fixed ratio between successive interval lengths enables systematic convergence to the minimum. Your task is to derive the point-placement rule that achieves this invariance, implement the resulting algorithm, and quantify how quickly convergence is detected when the initial interior points are favorably placed.\n\nImplement Golden-section search for minimizing a continuous unimodal function $f(x)$ on an interval $[a,b]$. The algorithm must:\n- Initialize two interior evaluation points in $[a,b]$, reusing one function evaluation after each interval shrink.\n- At each iteration, shrink the interval to the subinterval that contains the minimizer, determined by comparing the function values at the interior points.\n- Terminate when the interval length is less than or equal to a prescribed tolerance $\\varepsilon$.\n- Return three quantities: the number of iterations $n$ until termination, the final approximation $x^\\star$ to the minimizer, and the function value $f(x^\\star)$.\n\nCreate a special test case to align one of the initial interior points exactly with the minimizer at the start. Consider the function $f(x) = (x - \\tau)^2$ on the interval $[0,1]$, where $\\tau$ is the golden ratio conjugate $\\tau = (\\sqrt{5} - 1)/2 \\approx 0.618$. Because the initial interior points in Golden-section search are placed at $x_1 = b - \\tau (b-a)$ and $x_2 = a + \\tau (b-a)$, one of them equals $x^\\star = \\tau$ on $[0,1]$. Verify how quickly the algorithm detects convergence by measuring the iteration count $n$ for a stringent tolerance $\\varepsilon$.\n\nYour program must implement Golden-section search from these principles and apply it to the following test suite. For each test case, return a list of the form $[n, x^\\star, f(x^\\star)]$ and aggregate the results in a single list printed on one line, as specified below.\n\nTest suite:\n- Case $1$ (aligned interior point): $f_1(x) = (x - \\tau)^2$, interval $[0,1]$, tolerance $\\varepsilon = 10^{-8}$.\n- Case $2$ (symmetric but not aligned): $f_2(x) = (x - 0.5)^2$, interval $[0,1]$, tolerance $\\varepsilon = 10^{-8}$.\n- Case $3$ (minimum at the endpoint): $f_3(x) = x^2$, interval $[0,1]$, tolerance $\\varepsilon = 10^{-8}$.\n- Case $4$ (tiny initial bracket around the minimizer): $f_4(x) = (x - \\tau)^2$, interval $[\\tau - 10^{-12}, \\tau + 10^{-12}]$, tolerance $\\varepsilon = 10^{-9}$.\n- Case $5$ (smooth strictly convex, wider interval): $f_5(x) = \\mathrm{e}^{x} + \\mathrm{e}^{-x}$, interval $[-1,2]$, tolerance $\\varepsilon = 10^{-8}$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result_1,result_2,\\dots]$).\n- For each case, output the list $[n, x^\\star, f(x^\\star)]$ where $n$ is an integer, and both $x^\\star$ and $f(x^\\star)$ are floating-point numbers rounded to $10$ decimal places.\n- The order of results must match the order of the test suite above.\n- Angles are not involved; no angle unit is required. No physical units are involved.", "solution": "The problem requires the design and implementation of the Golden-Section Search (GSS) algorithm from first principles for minimizing a continuous unimodal function $f(x)$ on a closed interval $[a,b]$.\n\n### Principle and Derivation of the Golden-Section Search\n\nA function $f(x)$ is unimodal on an interval $[a,b]$ if there is a unique value $x^\\star \\in [a,b]$ where $f(x)$ has its minimum, and for any two points $x_1  x_2$ in the interval, if $x_2  x^\\star$ then $f(x_1)  f(x_2)$, and if $x_1  x^\\star$ then $f(x_1)  f(x_2)$. In simpler terms, the function is strictly decreasing to the left of the minimum and strictly increasing to the right.\n\nThe core of the Golden-Section Search is to iteratively shrink the interval $[a,b]$ while ensuring the minimum $x^\\star$ remains within the shrinking interval. To do this, we sample the function at two interior points, $x_1$ and $x_2$, such that $a  x_1  x_2  b$.\n\nBy comparing the function values $f(x_1)$ and $f(x_2)$, we can discard a portion of the interval:\n1.  If $f(x_1)  f(x_2)$, the minimum cannot be in the subinterval $(x_2, b]$, because if it were, then both $x_1$ and $x_2$ would be to the left of the minimum, which by the definition of unimodality would imply $f(x_1)  f(x_2)$, a contradiction. Therefore, the new search interval must be $[a, x_2]$.\n2.  If $f(x_1) \\geq f(x_2)$, the minimum cannot be in the subinterval $[a, x_1)$, because if it were, both $x_1$ and $x_2$ would be to the right of the minimum, implying $f(x_1)  f(x_2)$, a contradiction. Therefore, the new search interval must be $[x_1, b]$.\n\nThe key to efficiency is to choose the locations of $x_1$ and $x_2$ in a way that allows one of the points (and its corresponding function evaluation) to be reused in the next iteration. This is achieved by placing the points symmetrically and maintaining a constant ratio of interval reduction at each step.\n\nLet the length of the interval at step $k$ be $L_k = b_k - a_k$. We wish to reduce the length by a constant factor $\\tau$ at each step, i.e., $L_{k+1} = \\tau L_k$. Let's define the placement of the interior points relative to the interval length. We choose a parameter $\\tau \\in (1/2, 1)$ and place the points symmetrically from the ends of the interval:\n$x_{1,k} = b_k - \\tau(b_k - a_k)$\n$x_{2,k} = a_k + \\tau(b_k - a_k)$\n\nNote that because $\\tau  1/2$, we have $a_k  x_{1,k}  x_{2,k}  b_k$, as required.\n\nNow, consider the case where $f(x_{1,k})  f(x_{2,k})$. The new interval is $[a_{k+1}, b_{k+1}] = [a_k, x_{2,k}]$.\nThe length of this new interval is $L_{k+1} = x_{2,k} - a_k = \\tau(b_k - a_k) = \\tau L_k$. This is consistent with our goal of constant-ratio reduction.\nThe new interval $[a_{k+1}, b_{k+1}]$ contains one of the old interior points, $x_{1,k}$. For the next iteration, we need two new interior points, $x_{1,k+1}$ and $x_{2,k+1}$, defined by the same rule:\n$x_{1,k+1} = b_{k+1} - \\tau L_{k+1} = x_{2,k} - \\tau(\\tau L_k) = (a_k + \\tau L_k) - \\tau^2 L_k = a_k + (\\tau - \\tau^2)L_k$.\n$x_{2,k+1} = a_{k+1} + \\tau L_{k+1} = a_k + \\tau(\\tau L_k) = a_k + \\tau^2 L_k$.\n\nFor one function evaluation to be reused, the old point $x_{1,k}$ must coincide with one of the new points, $x_{1,k+1}$ or $x_{2,k+1}$.\nThe position of the old point is $x_{1,k} = b_k - \\tau L_k = (a_k+L_k) - \\tau L_k = a_k + (1-\\tau)L_k$.\nComparing this with the new points, we can enforce the condition $x_{1,k} = x_{2,k+1}$ (by inspection, these are the two points closer to the center of their respective intervals). This gives the equation:\n$a_k + (1-\\tau)L_k = a_k + \\tau^2 L_k$\n$1 - \\tau = \\tau^2$\n$\\tau^2 + \\tau - 1 = 0$\n\nSolving this quadratic equation for the positive root gives:\n$\\tau = \\frac{-1 + \\sqrt{1^2 - 4(1)(-1)}}{2(1)} = \\frac{\\sqrt{5} - 1}{2}$\n\nThis value, approximately $0.618034$, is the golden ratio conjugate, often denoted $\\phi^{-1}$ or simply $\\tau$. A symmetric argument for the case $f(x_{1,k}) \\geq f(x_{2,k})$ shows that the old point $x_{2,k}$ becomes the new point $x_{1,k+1}$.\n\nThus, by choosing $\\tau = (\\sqrt{5} - 1)/2$, we guarantee that at each step, the interval length is reduced by a factor of $\\tau$, and one function evaluation can be reused.\n\n### Algorithm Implementation\n\nThe algorithm proceeds as follows:\n1.  **Initialization**: Given $f(x)$, $[a,b]$, and tolerance $\\varepsilon$. Define $\\tau = (\\sqrt{5}-1)/2$. Calculate the initial interior points $x_1 = b - \\tau(b-a)$ and $x_2 = a + \\tau(b-a)$, and evaluate $f_1 = f(x_1)$ and $f_2 = f(x_2)$. Initialize an iteration counter $n=0$.\n2.  **Iteration**: While the interval length $(b-a)  \\varepsilon$:\n    a. Increment $n$.\n    b. If $f_1  f_2$:\n        i. The new interval is $[a, x_2]$. Set $b = x_2$.\n        ii. The old $x_1$ becomes the new $x_2$. Set $x_2 = x_1$ and $f_2 = f_1$.\n        iii. Calculate the new $x_1 = b - \\tau(b-a)$ and evaluate $f_1 = f(x_1)$.\n    c. Else ($f_1 \\geq f_2$):\n        i. The new interval is $[x_1, b]$. Set $a = x_1$.\n        ii. The old $x_2$ becomes the new $x_1$. Set $x_1 = x_2$ and $f_1 = f_2$.\n        iii. Calculate the new $x_2 = a + \\tau(b-a)$ and evaluate $f_2 = f(x_2)$.\n3.  **Termination**: When the loop terminates, the minimizer $x^\\star$ is within the final interval $[a,b]$. The best estimate for the minimizer is the midpoint of this interval, $x^\\star = (a+b)/2$.\n4.  **Return**: Return the number of iterations $n$, the final approximation $x^\\star$, and the function value $f(x^\\star)$.\n\n### Analysis of the Special Test Case (Case 1)\n\nFor the function $f_1(x) = (x - \\tau)^2$ on $[0,1]$, the true minimum is at $x^\\star = \\tau$.\nThe initial interval is $[a_0, b_0] = [0,1]$. The initial interior points are:\n$x_{1,0} = 1 - \\tau(1-0) = 1-\\tau$\n$x_{2,0} = 0 + \\tau(1-0) = \\tau$\nOne of the initial points, $x_{2,0}$, coincides exactly with the true minimizer. Consequently, $f(x_{2,0}) = (\\tau - \\tau)^2 = 0$. The other point gives $f(x_{1,0}) = ((1-\\tau) - \\tau)^2 = (1-2\\tau)^2  0$.\n\nSince $f(x_{1,0})  f(x_{2,0})$, the algorithm sets the new interval to $[a_1, b_1] = [x_{1,0}, b_0] = [1-\\tau, 1]$.\nThe algorithm proceeds by shrinking the interval around the point with the lower function value. At every subsequent iteration, one of the interior points will be exactly $\\tau$, and its function value will be $0$. The algorithm will always choose the subinterval containing $\\tau$.\n\nHowever, the question of \"how quickly convergence is detected\" is answered by examining the termination criterion: $(b-a) \\leq \\varepsilon$. The algorithm's rate of convergence is determined solely by the interval reduction factor $\\tau$. The length of the interval after $n$ iterations is $L_n = \\tau^n L_0$. The number of iterations required to meet the tolerance is found by solving $\\tau^n L_0 \\leq \\varepsilon$, which gives $n \\geq \\log(\\varepsilon/L_0) / \\log(\\tau)$. For $L_0=1$ and $\\varepsilon=10^{-8}$, this requires $n \\geq \\log(10^{-8})/\\log(\\tau) \\approx 38.28$, meaning $n=39$ iterations.\n\nThe favorable placement of an initial point at the exact minimum does not alter the number of iterations required for convergence. The algorithm has no mechanism to \"detect\" that it has found the minimum value; it only terminates based on interval width. This demonstrates a key property of Golden-section search: it has a guaranteed, but fixed, linear convergence rate, independent of the function's specific behavior beyond unimodality. In contrast, an algorithm using derivative information (like Newton's method) could converge much faster in such a scenario.\n\nThe exception to this is Case 4, where the initial interval $[\\tau - 10^{-12}, \\tau + 10^{-12}]$ has a length of $2 \\times 10^{-12}$, which is already smaller than the tolerance $\\varepsilon = 10^{-9}$. In this case, the termination condition is met before the first iteration, and the algorithm correctly returns $n=0$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for Golden-section search.\n    \"\"\"\n\n    def золотой_раздел(f, a, b, tol):\n        \"\"\"\n        Implements the Golden-section search algorithm.\n        \n        Args:\n            f: The unimodal function to minimize.\n            a: The lower bound of the interval.\n            b: The upper bound of the interval.\n            tol: The tolerance for the interval length.\n            \n        Returns:\n            A list containing [n, x_star, f_x_star]:\n            - n: Number of iterations.\n            - x_star: The approximation of the minimizer.\n            - f_x_star: The function value at the approximation.\n        \"\"\"\n        # The golden ratio conjugate\n        tau = (np.sqrt(5) - 1) / 2\n\n        # Initial interior points\n        x1 = b - tau * (b - a)\n        x2 = a + tau * (b - a)\n\n        # Initial function evaluations\n        f1 = f(x1)\n        f2 = f(x2)\n\n        n = 0\n        while (b - a)  tol:\n            n += 1\n            if f1  f2:\n                # The minimum is in [a, x2]\n                b = x2\n                x2 = x1\n                f2 = f1\n                x1 = b - tau * (b - a)\n                f1 = f(x1)\n            else:\n                # The minimum is in [x1, b]\n                a = x1\n                x1 = x2\n                f1 = f2\n                x2 = a + tau * (b - a)\n                f2 = f(x2)\n        \n        # The final approximation is the midpoint of the last interval.\n        x_star = (a + b) / 2\n        f_x_star = f(x_star)\n        \n        return [n, x_star, f_x_star]\n\n    # Define constants and test functions\n    tau_val = (np.sqrt(5) - 1) / 2\n\n    def f1(x):\n        return (x - tau_val)**2\n\n    def f2(x):\n        return (x - 0.5)**2\n\n    def f3(x):\n        return x**2\n\n    def f4(x):\n        return (x - tau_val)**2\n\n    def f5(x):\n        return np.exp(x) + np.exp(-x)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (f1, 0.0, 1.0, 1e-8),\n        (f2, 0.0, 1.0, 1e-8),\n        (f3, 0.0, 1.0, 1e-8),\n        (f4, tau_val - 1e-12, tau_val + 1e-12, 1e-9),\n        (f5, -1.0, 2.0, 1e-8),\n    ]\n\n    results = []\n    for case in test_cases:\n        func, a, b, tol = case\n        n, x_star, f_x_star = золотой_раздел(func, a, b, tol)\n        \n        # Format the result as a string with required precision\n        result_str = f\"[{n},{x_star:.10f},{f_x_star:.10f}]\"\n        results.append(result_str)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3166878"}, {"introduction": "While theoretical examples often involve smooth, strictly unimodal functions, real-world optimization problems are rarely so well-behaved. Objective functions can have flat regions, or plateaus, where multiple points yield the same minimal value. This practice [@problem_id:3166877] extends your foundational GSS implementation to handle these ambiguities. You will develop and implement a deterministic tie-breaking rule to ensure your algorithm behaves predictably and consistently, choosing the leftmost minimizer from a plateau. This exercise is crucial for developing robust optimizers that are reliable in practical applications.", "problem": "You are asked to implement a complete, runnable program that performs one-dimensional optimization using the golden-section search to minimize a given real-valued function on a closed interval. The fundamental base you must use is the definition of unimodality on a closed interval and the concept of bracketing a minimizer by repeatedly shrinking the interval. Specifically, your program must minimize functions that are unimodal or piecewise-unimodal on a closed interval and explicitly handle the case when there is a flat region of minimizers. You must reason from the fact that a unimodal function has a single valley in which the function decreases and then increases, and that evaluating a function at two interior points can determine which subinterval still contains a minimizer without differentiating the function. You must not use derivative information.\n\nThe core task is to minimize the function $f(x) = \\max(0, x)$ on the interval $[-1, 1]$ using golden-section search and to handle ties when multiple points achieve the same minimum value. The function $f(x) = \\max(0, x)$ has a flat region of minimizers on $[-1, 0]$, which creates an ambiguity for algorithms that assume a unique minimizer. You must address this ambiguity by proposing and implementing a deterministic selection rule that specifies which minimizer to return when there are infinitely many. Your deterministic rule must be: when multiple points satisfy $f(x) \\le f_{\\min} + \\varepsilon_f$, return the smallest $x$ among them (the leftmost minimizer). During the interval-shrinking steps of the golden-section search, when two interior function values are equal up to a function tolerance, your update rule must preserve consistency with this selection rule by shrinking the interval in a way that biases the search toward the leftmost minimizer. State this rule precisely in your implementation by comparing interior values $f(c)$ and $f(d)$ using an absolute function tolerance $\\varepsilon_f$.\n\nYour implementation requirements:\n- Implement golden-section search based solely on evaluating $f(x)$ and bracketing the minimizer by shrinking an interval $[a, b]$ using two interior points $c$ and $d$ chosen so that a single evaluation can be reused at the next iteration.\n- Use an absolute interval-length tolerance $\\varepsilon_x = 10^{-8}$, an absolute function-value tie tolerance $\\varepsilon_f = 10^{-12}$, and a maximum of $200$ iterations.\n- Tie-handling update rule: if $|f(c) - f(d)| \\le \\varepsilon_f$, then update the interval to $[a, d]$ so that the search is biased toward the left and remains consistent with returning the leftmost minimizer. If $f(c)  f(d) - \\varepsilon_f$, update to $[a, d]$; if $f(c)  f(d) + \\varepsilon_f$, update to $[c, b]$.\n- Final point selection rule after termination: among all points that your algorithm has evaluated (including the endpoints), identify $f_{\\min}$ as the smallest observed function value; then return the smallest $x$ among those with $f(x) \\le f_{\\min} + \\varepsilon_f$.\n\nYour program must apply this method to the following test suite of functions and intervals. For each case, return a single floating-point number representing the selected minimizer $x^\\star$ according to the rule above. Round each reported $x^\\star$ to $8$ decimal places.\n\nTest suite:\n1. $f_1(x) = \\max(0, x)$ on $[-1, 1]$ (flat-bottom case requiring tie-handling). The deterministic rule should return the leftmost minimizer.\n2. $f_2(x) = (x - 0.3)^2$ on $[-1, 1]$ (unique interior minimizer).\n3. $f_3(x) = -x$ on $[-1, 1]$ (monotone function with boundary minimizer at the right endpoint).\n4. $f_4(x) = \\max(0, |x - 0.5| - 0.25)$ on $[0, 1]$ (flat plateau of minimizers on $[0.25, 0.75]$, the deterministic rule should return the leftmost minimizer).\n\nAngle units are not applicable. No physical units are involved.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4]\"). Each result must be a float rounded to exactly $8$ decimal places, in the order of the test suite above.", "solution": "The user wants to implement the golden-section search algorithm for one-dimensional optimization, with specific rules for handling functions that have flat regions of minimizers. The solution must be a complete Python program that provides the minimizers for a given test suite of functions.\n\nThe problem is valid as it presents a well-defined computational task based on established numerical methods. All constants, tolerances, and rules for the algorithm's behavior are specified unambiguously, allowing for a unique and verifiable solution.\n\nThe core of the problem is the golden-section search, a derivative-free method for finding the extremum of a unimodal function by iteratively narrowing the interval in which the extremum is known to lie.\n\n### Algorithmic Principles of Golden-Section Search\n\nLet the function to be minimized be $f(x)$ on a closed interval $[a, b]$, where $f(x)$ is assumed to be unimodal. The algorithm maintains a bracketing interval $[a, b]$ that is guaranteed to contain the minimizer.\n\n1.  **Interior Point Selection**: Two interior points, $c$ and $d$, are chosen within the interval $[a, b]$ such that $a  c  d  b$. The placement of these points is determined by the golden ratio, $\\phi = \\frac{1+\\sqrt{5}}{2} \\approx 1.618034$. To maintain symmetry and enable the reuse of a function evaluation in the subsequent iteration, the points are chosen using the golden ratio conjugate, $\\rho = 2 - \\phi = \\frac{3-\\sqrt{5}}{2} \\approx 0.381966$. The points are:\n    $$ c = a + \\rho(b-a) $$\n    $$ d = b - \\rho(b-a) $$\n    This choice ensures that the ratio of the length of the larger subinterval to the smaller one is $\\phi$.\n\n2.  **Interval Reduction**: The function is evaluated at these two interior points, $f(c)$ and $f(d)$. Based on the comparison of these two values, the interval is reduced while ensuring the minimizer remains bracketed.\n    -   If $f(c)  f(d)$, the minimizer cannot be in the subinterval $(d, b]$. Thus, the new search interval becomes $[a, d]$.\n    -   If $f(c) \\ge f(d)$, the minimizer cannot be in the subinterval $[a, c)$. The new search interval becomes $[c, b]$.\n\n    A key feature of this method is its efficiency. When the interval is reduced, one of the previous interior points ($c$ or $d$) becomes an interior point of the new interval, and its function value is already known. For example, if the new interval is $[a, d]$, the old point $c$ becomes the new $d$ point for the next iteration. This means only one new function evaluation is required per iteration.\n\n### Problem-Specific Implementation Requirements\n\nThe problem specifies modifications to the standard algorithm to handle non-unique minimizers and to enforce a deterministic outcome.\n\n1.  **Tie-Handling Rule**: The standard comparison $f(c)  f(d)$ is replaced with a more robust rule involving an absolute function-value tolerance, $\\varepsilon_f = 10^{-12}$.\n    -   If $f(c)  f(d) + \\varepsilon_f$, the new interval is $[c, b]$.\n    -   Otherwise (i.e., if $f(c) \\le f(d) + \\varepsilon_f$), the new interval is $[a, d]$. This condition covers both the case where $f(c)$ is clearly less than $f(d)$ and the case where they are approximately equal ($|f(c) - f(d)| \\le \\varepsilon_f$). By choosing $[a, d]$ in the tie-case, the search is biased toward the left, which is consistent with the final selection rule.\n\n2.  **Termination Criteria**: The iterative process continues until the length of the search interval $b-a$ is smaller than a specified absolute tolerance $\\varepsilon_x = 10^{-8}$, or a maximum of $200$ iterations is reached.\n\n3.  **Final Minimizer Selection**: This problem mandates a specific post-processing step to select the final answer. Instead of simply returning the midpoint of the final interval, the algorithm must:\n    -   Record every point $x_i$ at which the function $f(x)$ is evaluated during the search, including the initial endpoints.\n    -   After the loop terminates, identify the global minimum value found among all evaluated points: $f_{\\min} = \\min_{i} f(x_i)$.\n    -   Construct a set of candidate minimizers, $S = \\{x_i \\mid f(x_i) \\le f_{\\min} + \\varepsilon_f\\}$. This set includes all points whose function value is within the tolerance $\\varepsilon_f$ of the observed minimum.\n    -   The final reported minimizer, $x^\\star$, must be the smallest value in the set $S$: $x^\\star = \\min(S)$. This enforces the rule of returning the \"leftmost\" minimizer in case of a flat valley or multiple discrete minimizers.\n\n### Application to Test Suite\n\nThe implemented algorithm will be applied to four test functions:\n1.  $f_1(x) = \\max(0, x)$ on $[-1, 1]$: This function has a flat minimum ($f_1(x)=0$) for $x \\in [-1, 0]$. The specified rules should lead to finding $x^\\star = -1.0$.\n2.  $f_2(x) = (x - 0.3)^2$ on $[-1, 1]$: A standard convex function with a unique minimum at $x=0.3$. The algorithm is expected to find a value very close to $0.3$.\n3.  $f_3(x) = -x$ on $[-1, 1]$: A monotone function whose minimum is at the boundary, $x=1$. The search should converge to this boundary.\n4.  $f_4(x) = \\max(0, |x - 0.5| - 0.25)$ on $[0, 1]$: This function has a flat minimum ($f_4(x)=0$) on the interval $[0.25, 0.75]$. The tie-handling and final selection rules should find the leftmost point of this plateau, $x^\\star \\approx 0.25$.\n\nThe implementation will store all evaluated points and their function values in a dictionary to facilitate the final selection step. The final results are rounded to $8$ decimal places as required.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef golden_section_search(f, a, b, eps_x=1e-8, eps_f=1e-12, max_iter=200):\n    \"\"\"\n    Performs one-dimensional optimization using the golden-section search.\n\n    This implementation includes specific rules for tie-handling and final\n    point selection as defined in the problem statement.\n\n    Args:\n        f (callable): The function to minimize.\n        a (float): The lower bound of the interval.\n        b (float): The upper bound of the interval.\n        eps_x (float): The absolute tolerance for the interval length.\n        eps_f (float): The absolute tolerance for function value comparisons.\n        max_iter (int): The maximum number of iterations.\n\n    Returns:\n        float: The selected minimizer x*, according to the specified rules.\n    \"\"\"\n    sqrt5 = np.sqrt(5.0)\n    # Golden ratio conjugate, used for placing the interior points.\n    # Also denoted as 2 - phi, where phi is the golden ratio.\n    r = (3.0 - sqrt5) / 2.0\n\n    # Store all evaluated points and their function values.\n    evaluated_points = {}\n\n    def f_memoized(x):\n        \"\"\"A wrapper for f that stores a history of evaluations.\"\"\"\n        # Use a canonical representation if possible to avoid floating point noise keys\n        # For this problem, direct keys are sufficient.\n        if x not in evaluated_points:\n            evaluated_points[x] = f(x)\n        return evaluated_points[x]\n\n    # Initial function evaluations at the boundaries\n    f_memoized(a)\n    f_memoized(b)\n\n    # Initial interior points and their function values\n    c = a + r * (b - a)\n    d = b - r * (b - a)\n    fc = f_memoized(c)\n    fd = f_memoized(d)\n\n    for _ in range(max_iter):\n        # Termination condition on interval length\n        if (b - a)  eps_x:\n            break\n\n        # The core update rule, modified for tie-handling\n        # if f(c)  f(d) + eps_f, update to [c, b]\n        # otherwise (for f(c)  f(d) or ties), update to [a, d] to bias left.\n        if fc  fd + eps_f:\n            a = c\n            c = d\n            fc = fd\n            d = b - r * (b - a)\n            fd = f_memoized(d)\n        else:\n            b = d\n            d = c\n            fd = fc\n            c = a + r * (b - a)\n            fc = f_memoized(c)\n\n    # Final point selection process\n    if not evaluated_points:\n        # Should not be reached with a valid interval [a,b]\n        return (a + b) / 2.0\n\n    # 1. Find the minimum function value observed during the search.\n    f_min = min(evaluated_points.values())\n\n    # 2. Identify all points x where f(x) is close to f_min.\n    candidates = []\n    for x, fx in evaluated_points.items():\n        if fx = f_min + eps_f:\n            candidates.append(x)\n\n    # 3. Return the smallest x (leftmost minimizer) from the candidates.\n    return min(candidates)\n\ndef solve():\n    \"\"\"\n    Defines the test suite and runs the golden-section search for each case.\n    Prints the results in the specified format.\n    \"\"\"\n\n    # Define the test functions as specified in the problem statement.\n    def f1(x):\n        \"\"\"Test Case 1: f(x) = max(0, x) on [-1, 1]\"\"\"\n        return np.maximum(0.0, x)\n\n    def f2(x):\n        \"\"\"Test Case 2: f(x) = (x - 0.3)^2 on [-1, 1]\"\"\"\n        return (x - 0.3)**2\n\n    def f3(x):\n        \"\"\"Test Case 3: f(x) = -x on [-1, 1]\"\"\"\n        return -x\n\n    def f4(x):\n        \"\"\"Test Case 4: f(x) = max(0, |x - 0.5| - 0.25) on [0, 1]\"\"\"\n        return np.maximum(0.0, np.abs(x - 0.5) - 0.25)\n\n    test_cases = [\n        (f1, -1.0, 1.0),\n        (f2, -1.0, 1.0),\n        (f3, -1.0, 1.0),\n        (f4, 0.0, 1.0)\n    ]\n    \n    # Tolerances and iteration limit from the problem statement\n    eps_x = 1e-8\n    eps_f = 1e-12\n    max_iter = 200\n\n    results = []\n    for func, a, b in test_cases:\n        minimizer = golden_section_search(func, a, b, eps_x, eps_f, max_iter)\n        # Format the result to exactly 8 decimal places\n        results.append(f\"{minimizer:.8f}\")\n    \n    # Print the final output in the required format\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3166877"}, {"introduction": "Many optimization problems in science and engineering are not continuous but discrete, involving integer-only variables like the number of components or specific configuration indices. This final practice [@problem_id:3166898] guides you through the process of adapting the continuous Golden-Section Search to an integer domain. This task is more than a simple rounding exercise; it requires careful thought on how to select discrete interior points to guarantee the search interval shrinks and converges correctly. By tackling this challenge, you will learn how to translate the core logic of a continuous algorithm to a discrete setting, a vital skill in computational science.", "problem": "You are asked to design, analyze, and implement a discrete variant of one-dimensional golden-section search suitable for integer-domain, unimodal objective functions. Consider the integer domain $[0,1000]$ and let $f(x)$ be unimodal on the integers, meaning there exists at least one integer $x^\\star$ such that $f$ is nonincreasing on $\\{0,1,\\dots,x^\\star\\}$ and nondecreasing on $\\{x^\\star,\\dots,1000\\}$. Your task is to adapt the golden-section search so that each iteration selects and evaluates two distinct interior integers derived from the continuous golden-section points, and updates a bracketing interval that is guaranteed to shrink toward a minimizer. The core requirement is to define a rounding strategy that maps the continuous interior points to integers without breaking the bracketing invariants or convergence.\n\nStarting from fundamental definitions, you must:\n- Use the principle of a bracketing search for a unimodal function on a closed interval, which maintains an interval $[a,b]$ known to contain a minimizer.\n- Use the idea of interior points based on a constant partition ratio (originating from the golden-section principle) to reduce the number of function evaluations across iterations.\n- Devise an integer-mapping rule that takes the real-valued interior points and produces two distinct integers strictly inside the current bracket $[a,b]$ (that is, $a  P  Q  b$) while ensuring that:\n  1) The interval length in integers strictly decreases by at least $1$ at each iteration.\n  2) The unimodal minimizer on the integers remains in the bracket after each update.\n  3) If $f(P) = f(Q)$ at any iteration, the update rule must ensure that, when multiple integer minimizers exist, the algorithm ultimately returns the smallest minimizer (that is, the smallest integer $x$ that achieves the minimal value).\n\nImplementation requirements:\n- The implementation must not evaluate $f(x)$ for any $x$ outside $[0,1000]$.\n- Terminate when the integer bracket has length at most $2$ and then choose the minimizer by direct inspection of all integer points remaining in the bracket.\n- The program must handle plateaus (a flat bottom), strict unimodality, and boundary minima, and must be robust with respect to the integer rounding of interior points.\n\nTest suite:\nImplement your algorithm and apply it to the following five unimodal test functions on the domain $\\{0,1,\\dots,1000\\}$:\n\n- Case $1$: $f_1(x) = (x - 321.7)^2$.\n- Case $2$: $f_2(x) = |x - 250| + 0.001\\,x$.\n- Case $3$: $f_3(x) = \\max\\{|x - 700| - 2,\\,0\\}$.\n- Case $4$: $f_4(x) = x^2$.\n- Case $5$: $f_5(x) = \\begin{cases} 2\\,(873 - x),  x \\le 873 \\\\ 3\\,(x - 873),  x \\ge 873 \\end{cases}$.\n\nFor each case, compute the integer $x^\\star \\in \\{0,1,\\dots,1000\\}$ that minimizes $f(x)$, with the convention that if multiple integers achieve the same minimal value, you must return the smallest such integer.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, in the form \"[r1,r2,r3,r4,r5]\"). Each entry must be the integer minimizer for the corresponding case, in the order of the cases listed above. No additional text should be printed.", "solution": "The problem requires the design and implementation of a discrete one-dimensional optimization algorithm, specifically an adaptation of the golden-section search for unimodal functions over an integer domain. The solution must be robust, convergent, and handle specific tie-breaking rules.\n\nThe fundamental principle is that of a bracketing search. For a unimodal function $f(x)$ on a closed integer interval $[a, b]$, we know a minimizer $x^\\star$ lies within this bracket. The goal is to iteratively shrink the bracket $[a, b]$ while ensuring $x^\\star$ remains within it. This is achieved by evaluating the function at two distinct interior points, $P$ and $Q$, such that $a  P  Q  b$.\n\nThe update rules for the bracket are derived from the definition of unimodality.\nLet $f$ be a function that is nonincreasing up to a minimizer $x^\\star$ and nondecreasing thereafter.\n1. If $f(P)  f(Q)$, the minimizer $x^\\star$ cannot be in the interval $(Q, b]$. The function value at $Q$ is higher than at $P$, and since the function is nondecreasing to the right of $x^\\star$, any minimizer must be to the left of $Q$. Thus, the new bracket becomes $[a, Q]$.\n2. If $f(P)  f(Q)$, the minimizer $x^\\star$ cannot be in the interval $[a, P)$. The function value at $P$ is higher than at $Q$. Since the function is nonincreasing to the left of $x^\\star$, any minimizer must be to the right of $P$. Thus, the new bracket becomes $[P, b]$.\n3. If $f(P) = f(Q)$, the minimizer(s) could lie in a plateau between $P$ and $Q$. The problem requires finding the smallest integer minimizer. To ensure we do not discard the leftmost minimizer, we must keep the left portion of the domain under consideration. Therefore, this case is handled like the first one: the new bracket becomes $[a, Q]$.\n\nCombining these, the update logic is: if $f(P) \\le f(Q)$, the new bracket is $[a, Q]$; otherwise, the new bracket is $[P, b]$.\n\nThe core of the task is to select the integer points $P$ and $Q$. The golden-section search for continuous domains places these points using the golden ratio conjugate, $\\psi = (\\sqrt{5}-1)/2 \\approx 0.618$. The points are located at $a + (1-\\psi)(b-a)$ and $a + \\psi(b-a)$. For an integer domain $[a, b]$ of length $L = b-a$, the continuous analogues are $p_{cont} = b - \\psi L$ and $q_{cont} = a + \\psi L$.\n\nA naive rounding of these continuous points to integers can fail. The distance between them is $q_{cont} - p_{cont} = L(2\\psi - 1) = L(\\sqrt{5}-2) \\approx 0.236 L$. If this distance is less than $1$, rounding can map both to the same integer, violating the $P  Q$ requirement. This occurs for small $L$, for example, if $L=4$, $p_{cont} \\approx a+1.528$ and $q_{cont} \\approx a+2.472$, which can both round to $a+2$.\n\nTo create a robust integer point selection strategy, we define the following procedure for any interval $[a,b]$ with length $b-a  2$:\n1. Calculate the ideal interior points $Q = \\text{round}(a + \\psi (b-a))$ and $P = \\text{round}(b - \\psi (b-a))$. We use standard rounding (round half to nearest even).\n2. Check for collision. If $P \\ge Q$, we enforce distinctness by setting $P = Q - 1$. This correction is necessary for certain small interval lengths.\n3. This rule guarantees $a  P  Q  b$. For an interval length $L = b-a \\ge 3$, the point $q_{cont} = a + \\psi L$ is greater than $a + 3\\psi \\approx a + 1.854$. After rounding, $Q \\ge a+2$. Therefore, the corrected point $P = Q - 1$ will be at least $a+1$, satisfying $P  a$. Symmetrically, $Q  b$ is also guaranteed. The interval length $b-a$ is guaranteed to shrink by at least $1$ in each iteration, ensuring convergence.\n\nThe search algorithm proceeds as follows:\n1. Initialize the bracket $[a, b]$ to the full domain, i.e., $[0, 1000]$.\n2. Loop while the number of integers in the bracket, $b-a+1$, is greater than $3$ (or, equivalently, while $b-a  2$).\n   a. Calculate the integer interior points $P$ and $Q$ using the robust rule described above.\n   b. Evaluate $f(P)$ and $f(Q)$.\n   c. If $f(P) \\le f(Q)$, update $b = Q$.\n   d. If $f(P)  f(Q)$, update $a = P$.\n3. Once the loop terminates ($b-a \\le 2$), the minimizer is contained in the small final bracket $[a, b]$. A final exhaustive search is performed over the integers $\\{a, a+1, \\dots, b\\}$ to find the $x$ that minimizes $f(x)$. If multiple integers yield the same minimum value, the smallest such integer is chosen, satisfying the problem's tie-breaking requirement.\n\nThis complete algorithm correctly adapts the golden-section search to an integer domain, ensuring convergence to the smallest integer minimizer while satisfying all specified constraints.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by implementing a discrete golden-section search for\n    the five specified test cases and printing the results.\n    \"\"\"\n\n    def golden_section_integer_search(f, a, b):\n        \"\"\"\n        Performs a golden-section search for a unimodal function on an\n        integer domain.\n\n        Args:\n            f: The unimodal objective function.\n            a: The lower bound of the integer interval.\n            b: The upper bound of the integer interval.\n\n        Returns:\n            The integer x that minimizes f(x) in [a, b]. If multiple\n            minimizers exist, the smallest is returned.\n        \"\"\"\n        psi = (np.sqrt(5) - 1) / 2  # Golden-ratio conjugate\n\n        # Main loop to shrink the bracketing interval\n        while (b - a)  2:\n            L = b - a\n            \n            # Calculate integer interior points based on the golden ratio\n            # np.round rounds to the nearest even number for .5 cases\n            q_val = a + psi * L\n            p_val = b - psi * L\n            Q = int(np.round(q_val))\n            P = int(np.round(p_val))\n\n            # Robustness check: ensure interior points are distinct.\n            # This can happen for small integer interval lengths where rounding\n            # causes the two points to collide.\n            if P = Q:\n                P = Q - 1\n            \n            # Evaluate the function at the interior points\n            fP = f(P)\n            fQ = f(Q)\n\n            # Update the bracket based on the function values.\n            # The rule f(P) = f(Q) ensures that for plateaus (flat minima),\n            # the algorithm favors the left side, eventually finding the\n            # smallest integer minimizer.\n            if fP = fQ:\n                b = Q\n            else:\n                a = P\n\n        # Termination: when b-a = 2, the bracket is small enough.\n        # Perform a final exhaustive search on the remaining candidates.\n        min_val = f(a)\n        min_x = a\n        for x in range(a + 1, b + 1):\n            val = f(x)\n            # In case of a tie in value, the smaller x is kept.\n            if val  min_val:\n                min_val = val\n                min_x = x\n        \n        return min_x\n\n    # Define the five test functions as per the problem statement.\n    f1 = lambda x: (x - 321.7)**2\n    f2 = lambda x: np.abs(x - 250) + 0.001 * x\n    f3 = lambda x: np.maximum(np.abs(x - 700) - 2, 0)\n    f4 = lambda x: x**2\n    \n    def f5(x):\n        if x = 873:\n            return 2 * (873 - x)\n        else:\n            return 3 * (x - 873)\n\n    test_cases = [f1, f2, f3, f4, f5]\n    initial_a, initial_b = 0, 1000\n\n    results = []\n    for f in test_cases:\n        result = golden_section_integer_search(f, initial_a, initial_b)\n        results.append(result)\n\n    # Print the final result in the specified format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3166898"}]}