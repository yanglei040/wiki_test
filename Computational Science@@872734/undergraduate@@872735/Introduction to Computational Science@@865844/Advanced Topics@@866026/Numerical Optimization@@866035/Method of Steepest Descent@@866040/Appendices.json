{"hands_on_practices": [{"introduction": "The method of steepest descent is a powerful analytical tool for approximating integrals dominated by a large parameter. This exercise provides a foundational look at its core principle by applying it to an integral whose exponent resembles a classic \"double-well potential\". By identifying the points where the function in the exponent is minimized, you will learn how to approximate the entire integral by summing Gaussian approximations around these critical points, offering a clear illustration of how the method works in practice. [@problem_id:920264]", "problem": "Consider the integral given by\n$$I(\\lambda) = \\int_{-\\infty}^\\infty \\exp\\left[-\\lambda(t^2-a^2)^2\\right] dt$$\nwhere $\\lambda$ is a large positive parameter and $a$ is a positive real constant. The evaluation of such integrals in the limit of large $\\lambda$ is a common task in various fields of physics and engineering, often solvable using the method of steepest descent (also known as the saddle-point method).\n\nYour task is to find the leading-order asymptotic expression for $I(\\lambda)$ as $\\lambda \\to \\infty$.", "solution": "The problem is to find the asymptotic behavior of the integral $I(\\lambda) = \\int_{-\\infty}^\\infty \\exp\\left[-\\lambda(t^2-a^2)^2\\right] dt$ for $\\lambda \\to \\infty$. This integral is in the standard form for the method of steepest descent, $\\int e^{-\\lambda \\phi(t)} dt$, where the phase function is $\\phi(t) = (t^2-a^2)^2$.\n\nThe method of steepest descent approximates the integral by contributions from the saddle points of the integrand, which correspond to the minima of the phase function $\\phi(t)$.\n\n1.  **Find the saddle points:**\n    The saddle points are the stationary points of $\\phi(t)$, found by setting its first derivative to zero.\n    $$\\phi'(t) = \\frac{d}{dt}(t^2-a^2)^2 = 2(t^2-a^2)(2t) = 4t(t^2-a^2)$$\n    Setting $\\phi'(t) = 0$ gives $4t(t-a)(t+a) = 0$. The saddle points are located at $t_1 = 0$, $t_2 = a$, and $t_3 = -a$.\n\n2.  **Evaluate the phase function at the saddle points:**\n    To determine which saddle points give the dominant contribution, we evaluate $\\phi(t)$ at each point. The integral is dominated by the points where $\\phi(t)$ is at its minimum value, since the exponential term $e^{-\\lambda \\phi(t)}$ will be largest there.\n    $$\\phi(t_1) = \\phi(0) = (0^2 - a^2)^2 = a^4$$\n    $$\\phi(t_2) = \\phi(a) = (a^2 - a^2)^2 = 0$$\n    $$\\phi(t_3) = \\phi(-a) = ((-a)^2 - a^2)^2 = 0$$\n    Since $a$ is a positive constant, $a^4  0$. The minimum value of $\\phi(t)$ is $0$, which occurs at the two saddle points $t = a$ and $t = -a$. Therefore, the leading-order asymptotic behavior of the integral is given by the sum of the contributions from the neighborhoods of these two points. The contribution from $t=0$ will be suppressed by a factor of $e^{-\\lambda a^4}$ and is thus negligible for large $\\lambda$.\n\n3.  **Analyze the behavior near the dominant saddle points:**\n    The contribution of a saddle point $t_k$ to the integral is given by the Gaussian integral approximation in its vicinity. The standard formula for the leading-order contribution from a simple saddle point $t_k$ is:\n    $$I_k(\\lambda) \\sim \\sqrt{\\frac{2\\pi}{\\lambda \\phi''(t_k)}} e^{-\\lambda \\phi(t_k)}$$\n    We need the second derivative of $\\phi(t)$:\n    $$\\phi''(t) = \\frac{d}{dt}(4t^3 - 4a^2t) = 12t^2 - 4a^2$$\n    Now, we evaluate $\\phi''(t)$ at the two dominant saddle points:\n    For $t=a$:\n    $$\\phi''(a) = 12a^2 - 4a^2 = 8a^2$$\n    For $t=-a$:\n    $$\\phi''(-a) = 12(-a)^2 - 4a^2 = 12a^2 - 4a^2 = 8a^2$$\n    Since $\\phi''(a)$ and $\\phi''(-a)$ are positive, these points are indeed minima along the real axis, and the path of integration (the real line) is a path of steepest descent through these saddles.\n\n4.  **Calculate the contributions and sum them:**\n    The contribution from the saddle point at $t=a$ is:\n    $$I_a(\\lambda) \\sim \\sqrt{\\frac{2\\pi}{\\lambda \\phi''(a)}} e^{-\\lambda \\phi(a)} = \\sqrt{\\frac{2\\pi}{\\lambda (8a^2)}} e^{-\\lambda(0)} = \\sqrt{\\frac{\\pi}{4\\lambda a^2}} = \\frac{\\sqrt{\\pi}}{2a\\sqrt{\\lambda}}$$\n    The contribution from the saddle point at $t=-a$ is:\n    $$I_{-a}(\\lambda) \\sim \\sqrt{\\frac{2\\pi}{\\lambda \\phi''(-a)}} e^{-\\lambda \\phi(-a)} = \\sqrt{\\frac{2\\pi}{\\lambda (8a^2)}} e^{-\\lambda(0)} = \\frac{\\sqrt{\\pi}}{2a\\sqrt{\\lambda}}$$\n    The total leading-order asymptotic approximation for the integral is the sum of these two contributions:\n    $$I(\\lambda) \\sim I_a(\\lambda) + I_{-a}(\\lambda) = \\frac{\\sqrt{\\pi}}{2a\\sqrt{\\lambda}} + \\frac{\\sqrt{\\pi}}{2a\\sqrt{\\lambda}} = \\frac{2\\sqrt{\\pi}}{2a\\sqrt{\\lambda}} = \\frac{\\sqrt{\\pi}}{a\\sqrt{\\lambda}}$$", "answer": "$$\\boxed{\\frac{\\sqrt{\\pi}}{a\\sqrt{\\lambda}}}$$", "id": "920264"}, {"introduction": "Transitioning from analytical theory to numerical practice, this exercise asks you to implement the method of steepest descent from first principles to find the minimum of a function. Often called gradient descent in this context, the algorithm's performance is compared against another cornerstone of optimization, Newton's method. This hands-on comparison will illuminate the fundamental trade-offs between first-order and second-order methods, particularly highlighting the robustness of steepest descent in situations where Newton's method can fail. [@problem_id:3159949]", "problem": "You are to implement and compare two iterative optimization methods on a smooth scalar field, strictly from first principles: the method of steepest descent and Newton’s method. The target function is the two-dimensional real-valued function defined by $f(\\mathbf{x}) = f(x,y) = x^{2} + y^{4}$. The aim is to demonstrate, by explicit computation, both the underlying principles and the numerical behavior of the methods, including a case where Newton’s method fails because the Hessian matrix is not positive definite in the sense of symmetric positive definite (SPD).\n\nYour tasks are:\n\n- Use only the base definitions from multivariable calculus: differentiability, directional derivative, gradient as the direction of steepest ascent in the Euclidean norm, and the second-order Taylor approximation to derive the search directions chosen by the method of steepest descent and by Newton’s method. Do not assume any prepackaged iteration formula.\n- Implement the method of steepest descent with a backtracking line search that enforces a standard sufficient decrease condition based on a first-order model. The method must be stable and produce a monotonically decreasing sequence of function values for this problem when started from any of the provided initial points.\n- Implement Newton’s method as the minimization of the quadratic model obtained from the second-order Taylor expansion; at each iteration, compute the step by solving a linear system. Declare Newton’s method as failed at an iterate if the Hessian is not invertible (for example, not SPD in a way that prevents solving the linear system), making the Newton step undefined.\n- Use the Euclidean norm in $\\mathbb{R}^{2}$. Use the following generic stopping criterion for both methods: stop successfully when $\\lVert \\nabla f(\\mathbf{x}_{k}) \\rVert_{2} \\le \\varepsilon$ with tolerance $\\varepsilon = 10^{-10}$. Use a maximum of $50$ iterations for Newton’s method and a maximum of $10000$ iterations for steepest descent. For the backtracking line search in steepest descent, use an initial step length $\\alpha_{0} = 1$, a sufficient decrease parameter $c_{1} = 10^{-4}$, and a contraction factor $\\tau = \\tfrac{1}{2}$. If the step length falls below $10^{-16}$ without satisfying the sufficient decrease condition, declare the steepest descent method as failed at that iteration.\n- The program must treat the Hessian singularity explicitly: if the linear system for Newton’s step cannot be solved because the Hessian is singular (or otherwise prevents a unique solution), report a failure for that test case in Newton’s method.\n\nTest suite:\nImplement your program to run the following initial points $(x_{0},y_{0})$:\n- Case A (general “happy path”): $(x_{0},y_{0}) = (1,1)$.\n- Case B (Hessian singular along one axis): $(x_{0},y_{0}) = (1,0)$.\n- Case C (stationary optimum already at start): $(x_{0},y_{0}) = (0,0)$.\n- Case D (negative coordinate, quartic asymmetry test): $(x_{0},y_{0}) = (5,-1)$.\n\nQuantifiable outputs:\nFor each test case $i$ in the order A, B, C, D, compute and report the following six values in order:\n- `n_status_i`: an integer where $1$ means Newton’s method converged within the iteration limit, and $0$ means it failed to converge or could not compute a step due to a non-invertible Hessian.\n- `n_iters_i`: the integer number of iterations actually performed by Newton’s method when it stopped (either by convergence or by failure).\n- `n_f_i`: the final function value $f$ at termination of Newton’s method as a floating-point number.\n- `sd_status_i`: an integer where $1$ means the method of steepest descent converged within the iteration limit, and $0$ means it failed to converge under the given line search rules.\n- `sd_iters_i`: the integer number of iterations actually performed by the method of steepest descent when it stopped.\n- `sd_f_i`: the final function value $f$ at termination of the method of steepest descent as a floating-point number.\n\nFinal output format:\nYour program should produce a single line of output containing the concatenated results for all four cases as a comma-separated list enclosed in square brackets. That is, the output must be\n[`n_status_A`,`n_iters_A`,`n_f_A`,`sd_status_A`,`sd_iters_A`,`sd_f_A`,`n_status_B`,`n_iters_B`,`n_f_B`,`sd_status_B`,`sd_iters_B`,`sd_f_B`,`n_status_C`,`n_iters_C`,`n_f_C`,`sd_status_C`,`sd_iters_C`,`sd_f_C`,`n_status_D`,`n_iters_D`,`n_f_D`,`sd_status_D`,`sd_iters_D`,`sd_f_D`].", "solution": "The problem requires the implementation and comparative analysis of two fundamental iterative optimization algorithms, the method of steepest descent and Newton's method, applied to the minimization of the multivariate function $f(\\mathbf{x}) = x^2 + y^4$. The derivation of the iterative steps must be performed from first principles of multivariable calculus, and the implementation must adhere to specified parameters and handle a specific failure case for Newton's method involving a singular Hessian matrix.\n\nFirst, we establish the necessary mathematical objects for the function $f(\\mathbf{x})$ where $\\mathbf{x} = [x, y]^T \\in \\mathbb{R}^2$. The function is $f(x, y) = x^2 + y^4$.\nThe gradient of $f$, which is a vector of its partial derivatives, is required for both methods. The gradient points in the direction of the steepest local ascent.\n$$\n\\nabla f(\\mathbf{x}) = \\begin{bmatrix} \\frac{\\partial f}{\\partial x} \\\\ \\frac{\\partial f}{\\partial y} \\end{bmatrix} = \\begin{bmatrix} 2x \\\\ 4y^3 \\end{bmatrix}\n$$\nA necessary condition for a point $\\mathbf{x}^*$ to be a minimizer is that the gradient vanishes, $\\nabla f(\\mathbf{x}^*) = \\mathbf{0}$. For the given function, this condition is $2x = 0$ and $4y^3 = 0$, which uniquely yields the point $(0, 0)$. Thus, $\\mathbf{x}^* = (0, 0)^T$ is the only stationary point, and since $f(\\mathbf{x}) > 0$ for all $\\mathbf{x} \\neq \\mathbf{0}$ and $f(\\mathbf{0}) = 0$, it is the global minimum.\n\nFor Newton's method, the Hessian matrix, which is the matrix of second-order partial derivatives, is also required.\n$$\nH_f(\\mathbf{x}) = \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x^2}  \\frac{\\partial^2 f}{\\partial x \\partial y} \\\\ \\frac{\\partial^2 f}{\\partial y \\partial x}  \\frac{\\partial^2 f}{\\partial y^2} \\end{bmatrix} = \\begin{bmatrix} 2  0 \\\\ 0  12y^2 \\end{bmatrix}\n$$\nThe Hessian matrix describes the local curvature of the function.\n\n**Method of Steepest Descent**\n\nThe method of steepest descent is an iterative algorithm that moves in the direction opposite to the gradient at each step. This direction, $-\\nabla f(\\mathbf{x})$, is the direction of steepest local descent.\nThe derivation begins with the first-order Taylor approximation of $f$ around a point $\\mathbf{x}_k$:\n$$\nf(\\mathbf{x}_k + \\mathbf{p}) \\approx f(\\mathbf{x}_k) + \\nabla f(\\mathbf{x}_k)^T \\mathbf{p}\n$$\nTo achieve the most significant decrease in $f$ for a step $\\mathbf{p}$ of a fixed small length, say $\\lVert \\mathbf{p} \\rVert_2 = \\delta$, we must minimize $\\nabla f(\\mathbf{x}_k)^T \\mathbf{p}$. By the Cauchy-Schwarz inequality, this dot product is minimized when $\\mathbf{p}$ is antiparallel to $\\nabla f(\\mathbf{x}_k)$. Thus, the search direction $\\mathbf{p}_k$ is chosen as:\n$$\n\\mathbf{p}_k = -\\nabla f(\\mathbf{x}_k)\n$$\nThe iterative update rule is therefore:\n$$\n\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{p}_k = \\mathbf{x}_k - \\alpha_k \\nabla f(\\mathbf{x}_k)\n$$\nwhere $\\alpha_k > 0$ is the step length. An appropriate value for $\\alpha_k$ is crucial for convergence. A backtracking line search is employed to find a suitable $\\alpha_k$. Starting with an initial guess, $\\alpha = \\alpha_0$, the step length is repeatedly reduced by a contraction factor $\\tau$ until the Armijo sufficient decrease condition is met:\n$$\nf(\\mathbf{x}_k + \\alpha \\mathbf{p}_k) \\le f(\\mathbf{x}_k) + c_1 \\alpha \\nabla f(\\mathbf{x}_k)^T \\mathbf{p}_k\n$$\nFor our choice of $\\mathbf{p}_k = - \\nabla f(\\mathbf{x}_k)$, this simplifies to:\n$$\nf(\\mathbf{x}_k - \\alpha \\nabla f(\\mathbf{x}_k)) \\le f(\\mathbf{x}_k) - c_1 \\alpha \\lVert \\nabla f(\\mathbf{x}_k) \\rVert_2^2\n$$\nThe parameters for the line search are specified as $\\alpha_0 = 1$, $c_1 = 10^{-4}$, and $\\tau = 1/2$. The method terminates when $\\lVert \\nabla f(\\mathbf{x}_k) \\rVert_2 \\le \\varepsilon = 10^{-10}$ or if the maximum number of iterations, $10000$, is reached. A failure is declared if the step length $\\alpha$ becomes smaller than $10^{-16}$ during the line search without satisfying the condition.\n\n**Newton's Method**\n\nNewton's method uses a more sophisticated model of the function by employing the second-order Taylor expansion around the current iterate $\\mathbf{x}_k$:\n$$\nf(\\mathbf{x}_k + \\mathbf{p}) \\approx m_k(\\mathbf{p}) = f(\\mathbf{x}_k) + \\nabla f(\\mathbf{x}_k)^T \\mathbf{p} + \\frac{1}{2}\\mathbf{p}^T H_f(\\mathbf{x}_k) \\mathbf{p}\n$$\nThe method determines the next step $\\mathbf{p}_k$ by finding the minimizer of this quadratic model $m_k(\\mathbf{p})$. A necessary condition for $\\mathbf{p}_k$ to minimize $m_k(\\mathbf{p})$ is that the gradient of the model with respect to $\\mathbf{p}$ is zero:\n$$\n\\nabla m_k(\\mathbf{p}) = \\nabla f(\\mathbf{x}_k) + H_f(\\mathbf{x}_k) \\mathbf{p} = \\mathbf{0}\n$$\nThis yields the Newton system, a system of linear equations for the step $\\mathbf{p}_k$:\n$$\nH_f(\\mathbf{x}_k) \\mathbf{p}_k = -\\nabla f(\\mathbf{x}_k)\n$$\nThe next iterate is then $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\mathbf{p}_k$. No line search is required in this pure form of the method (step length $\\alpha_k = 1$ is always used).\n\nA unique solution for $\\mathbf{p}_k$ exists if and only if the Hessian matrix $H_f(\\mathbf{x}_k)$ is invertible. If $H_f(\\mathbf{x}_k)$ is singular (not invertible), the Newton step is not well-defined, and the method fails. For the function $f(x,y) = x^2 + y^4$, the Hessian $H_f(\\mathbf{x}) = \\text{diag}(2, 12y^2)$ has determinant $\\det(H_f(\\mathbf{x})) = 24y^2$. The Hessian is singular if and only if $y=0$. Therefore, Newton's method will fail if an iterate $\\mathbf{x}_k$ has its y-component equal to zero, unless the gradient is already zero at that point.\n\nThe method terminates when $\\lVert \\nabla f(\\mathbf{x}_k) \\rVert_2 \\le \\varepsilon = 10^{-10}$ or if the maximum number of iterations, $50$, is reached. Failure is reported if the Newton system cannot be solved due to a singular Hessian.\n\n**Analysis of Test Cases**\n\n- **Case A: $\\mathbf{x}_0 = (1, 1)^T$.** Here, $y_0 \\ne 0$, so the initial Hessian $H_f(1, 1) = \\text{diag}(2, 12)$ is positive-definite. Both methods are expected to converge to the minimum at $(0, 0)^T$. Newton's method is expected to exhibit quadratic convergence and be significantly faster.\n\n- **Case B: $\\mathbf{x}_0 = (1, 0)^T$.** At this point, $y_0 = 0$. The gradient is $\\nabla f(1, 0) = [2, 0]^T \\neq \\mathbf{0}$, so we are not at the minimum. The Hessian is $H_f(1, 0) = \\text{diag}(2, 0)$, which is singular. The Newton system $H_f(\\mathbf{x}_0) \\mathbf{p}_0 = -\\nabla f(\\mathbf{x}_0)$ is inconsistent and does not have a unique solution. Therefore, Newton's method is expected to fail at the first iteration. Steepest descent will proceed with search direction $\\mathbf{p}_0 = -[2, 0]^T$, moving along the x-axis towards the origin, and is expected to converge.\n\n- **Case C: $\\mathbf{x}_0 = (0, 0)^T$.** This is the optimal solution. The initial gradient is $\\nabla f(0, 0) = [0, 0]^T$, and its norm is $0$, which satisfies the stopping criterion. Both methods should terminate immediately with $0$ iterations.\n\n- **Case D: $\\mathbf{x}_0 = (5, -1)^T$.** Similar to Case A, $y_0 \\ne 0$, so the initial Hessian is positive-definite. Both methods are expected to converge, with Newton's method being much faster. The negative y-coordinate has no qualitative effect since the function depends on $y^4$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the optimization algorithms on the specified test cases\n    and print the results in the required format.\n    \"\"\"\n\n    # --- Problem Definition ---\n    def f(x_vec):\n        \"\"\"Target function f(x, y) = x^2 + y^4.\"\"\"\n        x, y = x_vec\n        return x**2 + y**4\n\n    def grad_f(x_vec):\n        \"\"\"Gradient of the target function.\"\"\"\n        x, y = x_vec\n        return np.array([2 * x, 4 * y**3], dtype=float)\n\n    def hess_f(x_vec):\n        \"\"\"Hessian of the target function.\"\"\"\n        x, y = x_vec\n        return np.array([[2.0, 0.0], [0.0, 12.0 * y**2]], dtype=float)\n\n    # --- Newton's Method Implementation ---\n    def newtons_method(x0, tol=1e-10, max_iter=50):\n        \"\"\"\n        Implements Newton's method for optimization.\n\n        Returns:\n            (status, iterations, final_f_val)\n            status: 1 for convergence, 0 for failure.\n            iterations: Number of iterations performed.\n            final_f_val: Function value at the final point.\n        \"\"\"\n        xk = np.array(x0, dtype=float)\n        \n        for k in range(max_iter):\n            gk = grad_f(xk)\n            \n            # Check for convergence\n            if np.linalg.norm(gk) = tol:\n                return 1, k, f(xk)\n            \n            Hk = hess_f(xk)\n            \n            try:\n                # Solve the Newton system: Hk * p = -gk\n                pk = np.linalg.solve(Hk, -gk)\n            except np.linalg.LinAlgError:\n                # Failure due to singular Hessian\n                return 0, k, f(xk)\n                \n            # Update step\n            xk = xk + pk\n            \n        # Failure due to exceeding max iterations\n        gk = grad_f(xk)\n        if np.linalg.norm(gk) = tol:\n             return 1, max_iter, f(xk)\n        return 0, max_iter, f(xk)\n\n    # --- Steepest Descent Implementation ---\n    def steepest_descent(x0, tol=1e-10, max_iter=10000):\n        \"\"\"\n        Implements the method of steepest descent with backtracking line search.\n\n        Returns:\n            (status, iterations, final_f_val)\n            status: 1 for convergence, 0 for failure.\n            iterations: Number of iterations performed.\n            final_f_val: Function value at the final point.\n        \"\"\"\n        xk = np.array(x0, dtype=float)\n        \n        # Line search parameters\n        alpha0 = 1.0\n        c1 = 1e-4\n        tau = 0.5\n        alpha_min = 1e-16\n\n        for k in range(max_iter):\n            fk = f(xk)\n            gk = grad_f(xk)\n            \n            # Check for convergence\n            if np.linalg.norm(gk) = tol:\n                return 1, k, fk\n            \n            # Search direction\n            pk = -gk\n            \n            # Backtracking line search\n            alpha = alpha0\n            gk_pk_dot = np.dot(gk, pk)\n            \n            while f(xk + alpha * pk) > fk + c1 * alpha * gk_pk_dot:\n                alpha *= tau\n                if alpha  alpha_min:\n                    return 0, k, fk # Line search failure\n            \n            # Update step\n            xk = xk + alpha * pk\n\n        # Failure due to exceeding max iterations\n        fk = f(xk)\n        gk = grad_f(xk)\n        if np.linalg.norm(gk) = tol:\n             return 1, max_iter, fk\n        return 0, max_iter, fk\n\n    # --- Test Suite ---\n    test_cases = [\n        (1.0, 1.0),   # Case A\n        (1.0, 0.0),   # Case B\n        (0.0, 0.0),   # Case C\n        (5.0, -1.0),  # Case D\n    ]\n\n    results = []\n    for x0 in test_cases:\n        # Run Newton's method\n        n_status, n_iters, n_f_val = newtons_method(x0)\n        results.extend([n_status, n_iters, n_f_val])\n        \n        # Run Steepest Descent\n        sd_status, sd_iters, sd_f_val = steepest_descent(x0)\n        results.extend([sd_status, sd_iters, sd_f_val])\n\n    # --- Final Output Formatting ---\n    # Python's default float representation is sufficient here.\n    output_str = ','.join(map(str, results))\n    print(f\"[{output_str}]\")\n\nsolve()\n```", "id": "3159949"}, {"introduction": "This practice applies the steepest descent algorithm to a large-scale problem central to computational science: solving the system of linear equations that arises from discretizing a partial differential equation. You will frame this linear algebra problem as an optimization task—minimizing a quadratic functional—and use steepest descent to find the solution. By comparing its convergence rate to that of the more advanced Conjugate Gradient method, you will gain practical insight into the performance and limitations of steepest descent in high-dimensional settings. [@problem_id:3159947]", "problem": "Consider the two-dimensional (2D) Poisson problem on the unit square with homogeneous Dirichlet boundary conditions, written in continuous form as $-\\Delta u = f$ on $\\Omega = (0,1) \\times (0,1)$ with $u = 0$ on $\\partial \\Omega$. Discretize the Laplacian using the central finite difference method on a uniform interior grid of size $n \\times n$ with grid spacing $h = \\frac{1}{n+1}$, yielding a linear system $A x = b$ where $A$ is Symmetric Positive Definite (SPD). Interpret this linear system as the stationarity condition of a strictly convex quadratic functional in the unknown vector $x$. Starting from the definitions of the gradient of a quadratic functional and the Euclidean inner product, derive algorithmic update rules to implement both the method of steepest descent and the Conjugate Gradient (CG) method for SPD systems, and explain the role of the SPD property in ensuring convergence. Use the residual-based stopping criterion that terminates iterations when the normalized residual satisfies $\\frac{\\lVert r_k \\rVert_2}{\\lVert r_0 \\rVert_2} \\le \\varepsilon$, where $r_k = b - A x_k$ and $\\varepsilon$ is a prescribed tolerance.\n\nConstruct $A$ implicitly through the five-point finite difference stencil corresponding to the discretized operator $-\\Delta$, and construct $b$ by sampling the source term $f(x,y) = 1$ (a constant function) at the interior grid points. Use the zero vector as the initial guess. For each test case below, run both algorithms until the stopping criterion is met or a reasonable iteration cap is reached to guarantee termination. Record, for each test case, the iteration counts $k_{\\mathrm{SD}}$ and $k_{\\mathrm{CG}}$ required by steepest descent and Conjugate Gradient, respectively. The final output must aggregate all test cases into a single comma-separated list enclosed in square brackets, ordered as $[k_{\\mathrm{SD}}^{(1)},k_{\\mathrm{CG}}^{(1)},k_{\\mathrm{SD}}^{(2)},k_{\\mathrm{CG}}^{(2)},\\dots]$. All iteration counts must be integers.\n\nBase your derivations and implementations on the following fundamental definitions:\n- The Euclidean inner product $\\langle u, v \\rangle = u^{\\mathsf{T}} v$ and the Euclidean norm $\\lVert v \\rVert_2 = \\sqrt{\\langle v, v \\rangle}$.\n- For an SPD matrix $A$, the quadratic functional $F(x) = \\frac{1}{2} x^{\\mathsf{T}} A x - b^{\\mathsf{T}} x$ has a unique minimizer satisfying $A x = b$, and its gradient is $\\nabla F(x) = A x - b$.\n- The five-point stencil realization of $-\\Delta$ on the interior grid produces an SPD matrix $A$ for homogeneous Dirichlet boundary conditions.\n\nTest Suite:\n- Case $1$: $n = 8$, $\\varepsilon = 10^{-8}$.\n- Case $2$: $n = 16$, $\\varepsilon = 10^{-8}$.\n- Case $3$: $n = 16$, $\\varepsilon = 10^{-10}$.\n- Case $4$: $n = 4$, $\\varepsilon = 10^{-8}$.\n\nUnits are not applicable, and no angle units are involved. The final output format must be exactly one line: a single list containing, for each test case in the order listed, first the steepest descent iteration count then the Conjugate Gradient iteration count; for example, $[k_{\\mathrm{SD}}^{(1)},k_{\\mathrm{CG}}^{(1)},k_{\\mathrm{SD}}^{(2)},k_{\\mathrm{CG}}^{(2)},k_{\\mathrm{SD}}^{(3)},k_{\\mathrm{CG}}^{(3)},k_{\\mathrm{SD}}^{(4)},k_{\\mathrm{CG}}^{(4)}]$.", "solution": "The task is to solve the linear system $Ax=b$, which arises from the finite difference discretization of the Poisson equation, using the Steepest Descent (SD) and Conjugate Gradient (CG) methods. The core idea is to reframe this linear algebra problem as an optimization problem: finding the minimum of a strictly convex quadratic functional $F(x)$. The solution $x$ to $Ax=b$ is the unique minimizer of $F(x)$, where the gradient $\\nabla F(x)$ is zero.\n\nThe given quadratic functional is:\n$$\nF(x) = \\frac{1}{2} x^{\\mathsf{T}} A x - b^{\\mathsf{T}} x\n$$\nIts gradient is:\n$$\n\\nabla F(x) = A x - b\n$$\nThe problem states that the matrix $A$ arising from the discretization is Symmetric Positive Definite (SPD). For an SPD matrix $A$, the functional $F(x)$ is strictly convex, which guarantees the existence of a unique global minimum. The condition for this minimum is $\\nabla F(x) = 0$, which is equivalent to solving the linear system $Ax = b$.\n\nIterative methods for minimizing $F(x)$ follow the general update rule:\n$$\nx_{k+1} = x_k + \\alpha_k p_k\n$$\nwhere $x_k$ is the current approximation, $p_k$ is a search direction, and $\\alpha_k > 0$ is the step size. The optimal step size $\\alpha_k$ for a given direction $p_k$ is found by minimizing $F(x_k + \\alpha p_k)$ with respect to $\\alpha$. Let $g(\\alpha) = F(x_k + \\alpha p_k)$. The minimum is found by setting the derivative to zero:\n$$\n\\frac{d g}{d \\alpha} = \\frac{d}{d \\alpha} F(x_k + \\alpha p_k) = \\nabla F(x_k + \\alpha p_k)^{\\mathsf{T}} p_k = 0\n$$\nUsing the expression for the gradient, we have:\n$$\n(A(x_k + \\alpha p_k) - b)^{\\mathsf{T}} p_k = 0\n$$\n$$\n(A x_k - b + \\alpha A p_k)^{\\mathsf{T}} p_k = 0\n$$\nDefining the residual at step $k$ as $r_k = b - A x_k = -\\nabla F(x_k)$, this becomes:\n$$\n(-r_k + \\alpha A p_k)^{\\mathsf{T}} p_k = 0\n$$\n$$\n-r_k^{\\mathsf{T}} p_k + \\alpha p_k^{\\mathsf{T}} A p_k = 0\n$$\nSolving for $\\alpha$, we get the optimal step size:\n$$\n\\alpha_k = \\frac{r_k^{\\mathsf{T}} p_k}{p_k^{\\mathsf{T}} A p_k}\n$$\n\nThe methods of Steepest Descent and Conjugate Gradient differ in their choice of the search direction $p_k$.\n\n### Method of Steepest Descent (SD)\n\nThe method of steepest descent chooses the search direction to be aligned with the negative gradient, which is the direction of the steepest local descent of the functional $F(x)$.\n$$\np_k = -\\nabla F(x_k) = b - A x_k = r_k\n$$\nSubstituting $p_k = r_k$ into the formula for $\\alpha_k$:\n$$\n\\alpha_k = \\frac{r_k^{\\mathsf{T}} r_k}{r_k^{\\mathsf{T}} A r_k} = \\frac{\\langle r_k, r_k \\rangle}{\\langle r_k, A r_k \\rangle} = \\frac{\\lVert r_k \\rVert_2^2}{r_k^{\\mathsf{T}} A r_k}\n$$\nThe complete algorithm for Steepest Descent is:\n1.  Initialize $x_0$ (e.g., $x_0 = 0$), set $k=0$.\n2.  Compute the initial residual $r_0 = b - A x_0$.\n3.  Loop for $k = 0, 1, 2, \\dots$ until convergence:\n    a. Compute the step size: $\\alpha_k = \\frac{r_k^{\\mathsf{T}} r_k}{r_k^{\\mathsf{T}} A r_k}$.\n    b. Update the solution: $x_{k+1} = x_k + \\alpha_k r_k$.\n    c. Update the residual efficiently: $r_{k+1} = b - A x_{k+1} = b - A(x_k + \\alpha_k r_k) = (b - A x_k) - \\alpha_k A r_k = r_k - \\alpha_k A r_k$.\n\n### Conjugate Gradient (CG) Method\n\nThe CG method improves upon steepest descent by choosing search directions $p_k$ that are A-orthogonal (or conjugate), i.e., $\\langle p_i, p_j \\rangle_A = p_i^{\\mathsf{T}} A p_j = 0$ for $i \\neq j$. This prevents the algorithm from undoing progress made in previous directions, leading to much faster convergence.\n\nThe search directions are constructed iteratively. Starting with the steepest descent direction $p_0 = r_0$, subsequent directions $p_{k+1}$ are generated by taking the new residual $r_{k+1}$ and making it A-orthogonal to the previous search direction $p_k$:\n$$\np_{k+1} = r_{k+1} + \\beta_k p_k\n$$\nTo enforce A-orthogonality ($p_{k+1}^{\\mathsf{T}} A p_k = 0$), we project:\n$$\n(r_{k+1} + \\beta_k p_k)^{\\mathsf{T}} A p_k = 0 \\implies r_{k+1}^{\\mathsf{T}} A p_k + \\beta_k p_k^{\\mathsf{T}} A p_k = 0\n$$\nThis gives the formula for $\\beta_k$:\n$$\n\\beta_k = -\\frac{r_{k+1}^{\\mathsf{T}} A p_k}{p_k^{\\mathsf{T}} A p_k}\n$$\nWhile correct, this formula is computationally inefficient. Using orthogonality properties of the CG method (notably $\\langle r_i, r_j \\rangle = 0$ for $i \\ne j$), this can be shown to be equivalent to the much simpler Hestenes-Stiefel formula:\n$$\n\\beta_k = \\frac{r_{k+1}^{\\mathsf{T}} r_{k+1}}{r_k^{\\mathsf{T}} r_k} = \\frac{\\lVert r_{k+1} \\rVert_2^2}{\\lVert r_k \\rVert_2^2}\n$$\nThe step size $\\alpha_k$ uses the general formula with the CG search direction $p_k$. It can also be simplified by using the property that $\\langle r_k, p_k \\rangle = \\langle r_k, r_k \\rangle$:\n$$\n\\alpha_k = \\frac{r_k^{\\mathsf{T}} r_k}{p_k^{\\mathsf{T}} A p_k}\n$$\nThe complete algorithm for the Conjugate Gradient method is:\n1.  Initialize $x_0$ (e.g., $x_0 = 0$), set $k=0$.\n2.  Compute $r_0 = b - A x_0$.\n3.  Set the first search direction $p_0 = r_0$.\n4.  Loop for $k = 0, 1, 2, \\dots$ until convergence:\n    a. Compute step size: $\\alpha_k = \\frac{r_k^{\\mathsf{T}} r_k}{p_k^{\\mathsf{T}} A p_k}$.\n    b. Update solution: $x_{k+1} = x_k + \\alpha_k p_k$.\n    c. Update residual: $r_{k+1} = r_k - \\alpha_k A p_k$.\n    d. Compute $\\beta_k$: $\\beta_k = \\frac{r_{k+1}^{\\mathsf{T}} r_{k+1}}{r_k^{\\mathsf{T}} r_k}$.\n    e. Update search direction: $p_{k+1} = r_{k+1} + \\beta_k p_k$.\n\n### Role of the SPD Property\n\nThe Symmetric Positive Definite (SPD) property of the matrix $A$ is crucial for both methods.\n- **Existence of a Unique Solution:** For an SPD matrix $A$, the quadratic functional $F(x)$ is strictly convex. This guarantees that a unique minimizer exists, which corresponds to the solution of $Ax = b$.\n- **Steepest Descent Convergence:** Because $A$ is positive definite, the denominator in the step size, $r_k^{\\mathsf{T}} A r_k = \\langle r_k, r_k \\rangle_A$, is always positive for any non-zero residual $r_k$. This ensures $\\alpha_k > 0$, guaranteeing that each step is a descent direction for $F(x)$, which is a necessary condition for convergence.\n- **Conjugate Gradient Foundation:** The SPD property is the theoretical bedrock of the CG method. It ensures that $\\langle u, v \\rangle_A = u^{\\mathsf{T}} A v$ defines a valid inner product. The entire concept of A-orthogonality, which is central to CG's efficiency, relies on this. The denominators in the formulas for $\\alpha_k$ ($p_k^{\\mathsf{T}} A p_k$) are guaranteed to be positive, preventing division by zero. The SPD property underpins the theoretical guarantee that CG will find the exact solution to an $N \\times N$ system in at most $N$ iterations in exact arithmetic. Without it, the method's convergence properties are lost.", "answer": "```python\nimport numpy as np\n\ndef _matvec(u_vec, n, h):\n    \"\"\"\n    Computes the matrix-vector product A*u for the 2D Poisson problem.\n    A is constructed implicitly from the 5-point finite difference stencil.\n    \"\"\"\n    u_grid = u_vec.reshape((n, n))\n    \n    # Pad the grid with zeros to handle homogeneous Dirichlet boundary conditions\n    u_padded = np.zeros((n + 2, n + 2))\n    u_padded[1:-1, 1:-1] = u_grid\n    \n    # Apply the 5-point stencil for the negative Laplacian\n    # -(u_xx + u_yy) approx (4*u_ij - u_{i-1,j} - u_{i+1,j} - u_{i,j-1} - u_{i,j+1}) / h^2\n    Au_grid = (4 * u_padded[1:-1, 1:-1]\n               - u_padded[0:-2, 1:-1]  # Up\n               - u_padded[2:, 1:-1]    # Down\n               - u_padded[1:-1, 0:-2]  # Left\n               - u_padded[1:-1, 2:])   # Right\n    Au_grid /= (h * h)\n    \n    return Au_grid.flatten()\n\ndef steepest_descent(n, epsilon):\n    \"\"\"\n    Solves Ax=b using the method of steepest descent.\n    \"\"\"\n    h = 1.0 / (n + 1)\n    N = n * n\n    \n    x = np.zeros(N)\n    b = np.ones(N)  # Source term f(x,y)=1 sampled on the grid\n    \n    r = b - _matvec(x, n, h) # Since x_0 is 0, r_0 = b\n    r0_norm = np.linalg.norm(r)\n    \n    if r0_norm == 0:\n        return 0\n\n    max_iter = 10000 \n    k = 0\n    while k  max_iter:\n        if np.linalg.norm(r) / r0_norm = epsilon:\n            break\n        \n        Ar = _matvec(r, n, h)\n        r_dot_r = np.dot(r, r)\n        \n        alpha = r_dot_r / np.dot(r, Ar)\n        \n        x += alpha * r\n        r -= alpha * Ar\n        \n        k += 1\n        \n    return k\n\ndef conjugate_gradient(n, epsilon):\n    \"\"\"\n    Solves Ax=b using the Conjugate Gradient method.\n    \"\"\"\n    h = 1.0 / (n + 1)\n    N = n * n\n    \n    x = np.zeros(N)\n    b = np.ones(N)\n    \n    r = b - _matvec(x, n, h) # Since x_0 is 0, r_0 = b\n    p = r.copy()\n    r0_norm = np.linalg.norm(r)\n    \n    if r0_norm == 0:\n        return 0\n        \n    rs_old = np.dot(r, r)\n\n    max_iter = n*n + 1 # Theoretical max is N, add 1 for safety\n    k = 0\n    while k  max_iter:\n        if np.linalg.norm(r) / r0_norm = epsilon:\n            break\n\n        Ap = _matvec(p, n, h)\n        \n        alpha = rs_old / np.dot(p, Ap)\n        \n        x += alpha * p\n        r -= alpha * Ap\n        \n        rs_new = np.dot(r, r)\n        \n        beta = rs_new / rs_old\n        p = r + beta * p\n        \n        rs_old = rs_new\n        k += 1\n\n    return k\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and print the results.\n    \"\"\"\n    test_cases = [\n        (8, 1e-8),  # Case 1\n        (16, 1e-8), # Case 2\n        (16, 1e-10),# Case 3\n        (4, 1e-8),  # Case 4\n    ]\n\n    results = []\n    for n, epsilon in test_cases:\n        k_sd = steepest_descent(n, epsilon)\n        k_cg = conjugate_gradient(n, epsilon)\n        results.append(k_sd)\n        results.append(k_cg)\n\n    # Reorder results to match the problem statement order (case 1, 2, 3, 4)\n    # The loop processes them in this order already.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3159947"}]}