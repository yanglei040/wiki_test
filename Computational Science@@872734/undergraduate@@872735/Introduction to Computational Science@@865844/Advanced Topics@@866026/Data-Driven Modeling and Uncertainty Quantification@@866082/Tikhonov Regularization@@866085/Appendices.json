{"hands_on_practices": [{"introduction": "To truly grasp Tikhonov regularization, it's helpful to first work through the mechanics by hand. This exercise provides a small, ill-conditioned $2 \\times 2$ system, allowing you to manually compute the regularized solution $x_{\\lambda}$ using its defining formula. By finding a specific point on the solution path, you will develop a concrete intuition for how the regularization parameter $\\lambda$ stabilizes the solution and systematically shifts it away from the unstable least-squares estimate. [@problem_id:2223157]", "problem": "In many scientific and engineering applications, one encounters the need to solve a linear system of equations $Ax=b$ where the matrix $A$ is ill-conditioned. A common approach to stabilize the solution is Tikhonov regularization, which seeks to find a vector $x$ that minimizes the composite objective function $\\|Ax-b\\|^2_2 + \\lambda \\|x\\|^2_2$, where $\\| \\cdot \\|_2$ denotes the Euclidean norm and $\\lambda > 0$ is a regularization parameter. The unique solution to this minimization problem, denoted as $x_\\lambda$, is given by the formula $x_\\lambda = (A^T A + \\lambda I)^{-1} A^T b$, where $I$ is the identity matrix.\n\nConsider the ill-conditioned linear system with the matrix $A$ and vector $b$ defined as:\n$$\nA = \\begin{pmatrix} 1 & 1.01 \\\\ 1 & 1 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}\n$$\nThe solution vector $x_\\lambda = \\begin{pmatrix} x_1(\\lambda) \\\\ x_2(\\lambda) \\end{pmatrix}$ traces a path in the plane as the parameter $\\lambda$ varies. There exists a unique positive value of $\\lambda$, let's call it $\\lambda^*$, for which the solution path intersects the $x_2$-axis. This corresponds to the first component of the solution vector being zero, i.e., $x_1(\\lambda^*) = 0$.\n\nDetermine the value of the second component, $x_2(\\lambda^*)$, at this specific point of intersection. Round your final answer to four significant figures.", "solution": "The Tikhonov regularized solution to the system $Ax=b$ is given by $x_\\lambda = (A^T A + \\lambda I)^{-1} A^T b$. We are given the matrix $A$ and vector $b$:\n$$\nA = \\begin{pmatrix} 1 & 1.01 \\\\ 1 & 1 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}\n$$\n\nFirst, we compute the required matrix and vector products.\nThe transpose of $A$ is:\n$$\nA^T = \\begin{pmatrix} 1 & 1 \\\\ 1.01 & 1 \\end{pmatrix}\n$$\nNext, we compute $A^T A$:\n$$\nA^T A = \\begin{pmatrix} 1 & 1 \\\\ 1.01 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 1.01 \\\\ 1 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 1 + 1 \\cdot 1 & 1 \\cdot 1.01 + 1 \\cdot 1 \\\\ 1.01 \\cdot 1 + 1 \\cdot 1 & 1.01 \\cdot 1.01 + 1 \\cdot 1 \\end{pmatrix} = \\begin{pmatrix} 2 & 2.01 \\\\ 2.01 & 1.0201 + 1 \\end{pmatrix} = \\begin{pmatrix} 2 & 2.01 \\\\ 2.01 & 2.0201 \\end{pmatrix}\n$$\nNow, we compute $A^T b$:\n$$\nA^T b = \\begin{pmatrix} 1 & 1 \\\\ 1.01 & 1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 2 + 1 \\cdot 1 \\\\ 1.01 \\cdot 2 + 1 \\cdot 1 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 2.02 + 1 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 3.02 \\end{pmatrix}\n$$\nThe expression for $x_\\lambda$ involves the matrix $(A^T A + \\lambda I)$:\n$$\nA^T A + \\lambda I = \\begin{pmatrix} 2 & 2.01 \\\\ 2.01 & 2.0201 \\end{pmatrix} + \\lambda \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 2+\\lambda & 2.01 \\\\ 2.01 & 2.0201+\\lambda \\end{pmatrix}\n$$\nTo find the inverse of this matrix, we first compute its determinant:\n$$\n\\det(A^T A + \\lambda I) = (2+\\lambda)(2.0201+\\lambda) - (2.01)^2\n$$\n$$\n= (4.0402 + 2\\lambda + 2.0201\\lambda + \\lambda^2) - 4.0401\n$$\n$$\n= \\lambda^2 + 4.0201\\lambda + 0.0001\n$$\nThe inverse matrix is:\n$$\n(A^T A + \\lambda I)^{-1} = \\frac{1}{\\lambda^2 + 4.0201\\lambda + 0.0001} \\begin{pmatrix} 2.0201+\\lambda & -2.01 \\\\ -2.01 & 2+\\lambda \\end{pmatrix}\n$$\nNow we can write the full expression for $x_\\lambda$:\n$$\nx_\\lambda = \\begin{pmatrix} x_1(\\lambda) \\\\ x_2(\\lambda) \\end{pmatrix} = \\frac{1}{\\lambda^2 + 4.0201\\lambda + 0.0001} \\begin{pmatrix} 2.0201+\\lambda & -2.01 \\\\ -2.01 & 2+\\lambda \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 3.02 \\end{pmatrix}\n$$\nLet's compute the matrix-vector product in the numerator:\n$$\n\\begin{pmatrix} (2.0201+\\lambda) \\cdot 3 - 2.01 \\cdot 3.02 \\\\ -2.01 \\cdot 3 + (2+\\lambda) \\cdot 3.02 \\end{pmatrix} = \\begin{pmatrix} 6.0603 + 3\\lambda - 6.0702 \\\\ -6.03 + 6.04 + 3.02\\lambda \\end{pmatrix} = \\begin{pmatrix} 3\\lambda - 0.0099 \\\\ 3.02\\lambda + 0.01 \\end{pmatrix}\n$$\nSo, the solution vector is:\n$$\nx_\\lambda = \\begin{pmatrix} x_1(\\lambda) \\\\ x_2(\\lambda) \\end{pmatrix} = \\frac{1}{\\lambda^2 + 4.0201\\lambda + 0.0001} \\begin{pmatrix} 3\\lambda - 0.0099 \\\\ 3.02\\lambda + 0.01 \\end{pmatrix}\n$$\nThe problem asks for the value of $x_2(\\lambda^*)$ where $\\lambda^*$ is the positive value of $\\lambda$ that makes $x_1(\\lambda) = 0$.\nWe set the first component's numerator to zero to find $\\lambda^*$:\n$$\n3\\lambda^* - 0.0099 = 0\n$$\n$$\n3\\lambda^* = 0.0099 \\implies \\lambda^* = 0.0033\n$$\nThis value is positive, as required. Now we substitute $\\lambda^*=0.0033$ into the expression for $x_2(\\lambda)$:\n$$\nx_2(\\lambda^*) = \\frac{3.02(0.0033) + 0.01}{(0.0033)^2 + 4.0201(0.0033) + 0.0001}\n$$\nCalculate the numerator:\n$$\n3.02 \\times 0.0033 + 0.01 = 0.009966 + 0.01 = 0.019966\n$$\nCalculate the denominator:\n$$\n(0.0033)^2 = 0.00001089\n$$\n$$\n4.0201 \\times 0.0033 = 0.01326633\n$$\n$$\n\\text{Denominator} = 0.00001089 + 0.01326633 + 0.0001 = 0.01337722\n$$\nFinally, we compute the value of $x_2(\\lambda^*)$:\n$$\nx_2(\\lambda^*) = \\frac{0.019966}{0.01337722} \\approx 1.4925343\n$$\nRounding the result to four significant figures, we get $1.493$.", "answer": "$$\\boxed{1.493}$$", "id": "2223157"}, {"introduction": "In practical applications, we rarely choose a regularization parameter $\\lambda$ in isolation. Instead, we compute the solution for a range of $\\lambda$ values to trace the 'regularization path,' which reveals the trade-off between data fidelity and solution size. This practice guides you through implementing the standard, numerically robust algorithm for computing this path using the Singular Value Decomposition (SVD), a cornerstone of modern numerical linear algebra. You will apply this to various scenarios, from well-posed to rank-deficient systems, and verify the characteristic shrinkage behavior of the solution coefficients. [@problem_id:3200591]", "problem": "You are given linear systems with noisy observations where a vector of observations $b \\in \\mathbb{R}^m$ is approximately modeled by a matrix $A \\in \\mathbb{R}^{m \\times n}$ and an unknown coefficient vector $x^\\star \\in \\mathbb{R}^n$ via $b \\approx A x^\\star$. To stabilize estimation when $A$ is ill-conditioned or rank-deficient, consider the Tikhonov-regularized objective\n$$\nJ_\\lambda(x) = \\lVert A x - b \\rVert_2^2 + \\lambda^2 \\lVert x \\rVert_2^2,\n$$\nwhere $\\lambda \\ge 0$ is the regularization parameter and $\\lVert \\cdot \\rVert_2$ denotes the Euclidean norm. The statistical ridge regression path is obtained by varying $\\lambda$ and observing how the coefficients shrink.\n\nStarting from the fundamental definition above and well-tested facts about linear algebra (including the existence and properties of the Singular Value Decomposition (SVD) and the Moore–Penrose pseudoinverse), derive a numerically stable algorithm to compute, for a logarithmically spaced grid of regularization strengths $\\lambda$, the corresponding minimizers $x_\\lambda$ of $J_\\lambda(x)$, and track coefficient shrinkage along this path. Do not rely on any shortcut formulas presented in this problem statement.\n\nYour program must implement the algorithm and apply it to the following test suite. For reproducibility, all random draws must use the specified seeds and standard normal distributions as indicated.\n\n- Test case $1$ (well-conditioned, overdetermined):\n  - Dimensions: $m = 80$, $n = 20$.\n  - Random seed: $42$.\n  - Construction:\n    - Draw $A \\in \\mathbb{R}^{80 \\times 20}$ with independent standard normal entries.\n    - Draw $x^\\text{true} \\in \\mathbb{R}^{20}$ with independent standard normal entries.\n    - Draw $\\epsilon \\in \\mathbb{R}^{80}$ with independent standard normal entries, then scale by $\\sigma = 0.01$.\n    - Set $b = A x^\\text{true} + \\epsilon$.\n- Test case $2$ (underdetermined, wide design):\n  - Dimensions: $m = 50$, $n = 100$.\n  - Random seed: $123$.\n  - Construction:\n    - Draw $A \\in \\mathbb{R}^{50 \\times 100}$, $x^\\text{true} \\in \\mathbb{R}^{100}$, and $\\epsilon \\in \\mathbb{R}^{50}$ as independent standard normal, with $\\epsilon$ scaled by $\\sigma = 0.01$.\n    - Set $b = A x^\\text{true} + \\epsilon$.\n- Test case $3$ (rank-deficient design):\n  - Dimensions: $m = 40$, $n = 40$.\n  - Random seed: $7$.\n  - Construction:\n    - Draw $A \\in \\mathbb{R}^{40 \\times 40}$ with independent standard normal entries.\n    - Impose exact column duplication to force rank deficiency: set the column at index $10$ equal to the column at index $5$, and the column at index $15$ equal to the column at index $5$.\n    - Draw $x^\\text{true} \\in \\mathbb{R}^{40}$ and $\\epsilon \\in \\mathbb{R}^{40}$ as independent standard normal, with $\\epsilon$ scaled by $\\sigma = 0.01$.\n    - Set $b = A x^\\text{true} + \\epsilon$.\n- Test case $4$ (highly ill-conditioned design with correlated columns):\n  - Dimensions: $m = 60$, $n = 20$.\n  - Random seed: $314$.\n  - Construction:\n    - Draw $G \\in \\mathbb{R}^{60 \\times 20}$ with independent standard normal entries.\n    - Create correlated and ill-conditioned columns by scaling: for column index $j \\in \\{0,1,\\dots,19\\}$, set the $j$-th column of $A$ to be the $j$-th column of $G$ multiplied by $10^{-\\frac{j}{3}}$.\n    - Draw $x^\\text{true} \\in \\mathbb{R}^{20}$ and $\\epsilon \\in \\mathbb{R}^{60}$ as independent standard normal, with $\\epsilon$ scaled by $\\sigma = 0.01$.\n    - Set $b = A x^\\text{true} + \\epsilon$.\n\nFor each test case, use a logarithmically spaced grid of $60$ values\n$$\n\\lambda \\in \\{\\lambda_k \\mid \\lambda_k = 10^{\\ell_k},\\ \\ell_k \\text{ equispaced in } [-10,6]\\},\n$$\nthat is, from $10^{-10}$ up to $10^{6}$.\n\nFor each test case, compute the regularization path $\\{x_{\\lambda_k}\\}$ and evaluate the following three boolean properties:\n\n1. Monotone shrinkage of the coefficient Euclidean norm: the sequence $\\{\\lVert x_{\\lambda_k} \\rVert_2\\}_{k=1}^{60}$ is nonincreasing within a small numerical tolerance, meaning $\\lVert x_{\\lambda_{k}} \\rVert_2 \\le \\lVert x_{\\lambda_{k-1}} \\rVert_2 + \\tau$ for all $k$, where $\\tau = 10^{-12} \\cdot \\max(1, \\lVert x_{\\lambda_{k-1}} \\rVert_2)$.\n2. Near-agreement at small regularization with the minimum-norm least squares solution: let $x_{\\mathrm{lsq}}$ be the Moore–Penrose pseudoinverse solution minimizing $\\lVert A x - b \\rVert_2$ with minimum $\\lVert x \\rVert_2$. Check that the relative difference between $x_{\\lambda_{\\min}}$ and $x_{\\mathrm{lsq}}$ satisfies\n$$\n\\frac{\\lVert x_{\\lambda_{\\min}} - x_{\\mathrm{lsq}} \\rVert_2}{\\max(1,\\lVert x_{\\mathrm{lsq}} \\rVert_2)} < 10^{-6}.\n$$\n3. Near-zero coefficients at very large regularization: check that\n$$\n\\frac{\\lVert x_{\\lambda_{\\max}} \\rVert_2}{\\max(1,\\lVert x_{\\mathrm{lsq}} \\rVert_2)} < 10^{-6}.\n$$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with entries grouped per test case in the order described above and flattened across all test cases. Concretely, the output must be a list of $12$ boolean values\n$$\n[\\text{m1\\_t1},\\text{m2\\_t1},\\text{m3\\_t1},\\ \\text{m1\\_t2},\\text{m2\\_t2},\\text{m3\\_t2},\\ \\text{m1\\_t3},\\text{m2\\_t3},\\text{m3\\_t3},\\ \\text{m1\\_t4},\\text{m2\\_t4},\\text{m3\\_t4}],\n$$\nwhere, for test case $i$, $\\text{m1\\_t}i$ is the monotone-shrinkage check, $\\text{m2\\_t}i$ is the small-$\\lambda$ agreement check, and $\\text{m3\\_t}i$ is the large-$\\lambda$ near-zero check.", "solution": "### Derivation of the Numerically Stable Algorithm\n\nThe Tikhonov-regularized objective function is given by:\n$$\nJ_\\lambda(x) = \\lVert A x - b \\rVert_2^2 + \\lambda^2 \\lVert x \\rVert_2^2\n$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^m$, $x \\in \\mathbb{R}^n$, and $\\lambda \\ge 0$ is the regularization parameter. The function $J_\\lambda(x)$ is convex in $x$. For $\\lambda > 0$, it is strictly convex, guaranteeing a unique minimizer. The minimizer $x_\\lambda$ can be found by setting the gradient of $J_\\lambda(x)$ with respect to $x$ to zero.\n\nThe gradient is $\\nabla_x J_\\lambda(x) = 2 A^T (A x - b) + 2 \\lambda^2 x$. Setting it to zero gives the Tikhonov normal equations:\n$$\n(A^T A + \\lambda^2 I) x = A^T b\n$$\nWhile the solution is formally $x_\\lambda = (A^T A + \\lambda^2 I)^{-1} A^T b$, directly forming the matrix $A^T A$ is numerically unstable if $A$ is ill-conditioned, as the condition number squares. A numerically stable algorithm is derived using the Singular Value Decomposition (SVD) of $A$:\n$$\nA = U \\Sigma V^T\n$$\nHere, $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is a rectangular diagonal matrix with non-negative singular values $\\sigma_i$.\n\nUsing the orthogonal invariance of the Euclidean norm, the objective function can be rewritten by defining a change of variables $y = V^T x$ (so $x=Vy$) and $c = U^T b$:\n$$\n\\begin{aligned}\nJ_\\lambda(x) = \\lVert U \\Sigma V^T x - b \\rVert_2^2 + \\lambda^2 \\lVert V y \\rVert_2^2 \\\\\n= \\lVert \\Sigma y - c \\rVert_2^2 + \\lambda^2 \\lVert y \\rVert_2^2\n\\end{aligned}\n$$\nThis objective is separable in the components of $y$. For each component $y_i$, we minimize $(\\sigma_i y_i - c_i)^2 + \\lambda^2 y_i^2$. Taking the derivative with respect to $y_i$ and setting to zero yields:\n$$\n2(\\sigma_i y_i - c_i)\\sigma_i + 2\\lambda^2 y_i = 0 \\implies (\\sigma_i^2 + \\lambda^2) y_i = \\sigma_i c_i\n$$\nThe solution for the transformed coefficients is:\n$$\n(y_\\lambda)_i =\n\\begin{cases}\n\\frac{\\sigma_i c_i}{\\sigma_i^2 + \\lambda^2}  \\text{for } 1 \\le i \\le \\min(m,n) \\\\\n0  \\text{otherwise}\n\\end{cases}\n$$\nFinally, we transform the solution $y_\\lambda$ back to the original variable $x_\\lambda = V y_\\lambda$. This SVD-based approach is numerically stable as it avoids forming $A^T A$.\n\n### Algorithm Summary\n1.  Compute the full SVD of $A = U \\Sigma V^T$.\n2.  Compute the transformed vector $c = U^T b$.\n3.  For each desired $\\lambda$ in the grid:\n    a. Initialize an $n$-dimensional zero vector $y_\\lambda$.\n    b. For $i=1, \\dots, \\min(m,n)$, compute $(y_\\lambda)_i = \\frac{\\sigma_i c_i}{\\sigma_i^2 + \\lambda^2}$.\n    c. Compute the solution $x_\\lambda = V y_\\lambda$.\nThis algorithm is implemented in the provided Python code to compute the regularization path and verify the theoretical properties.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import svd\n\ndef solve():\n    \"\"\"\n    Solves the Tikhonov regularization problem for four test cases\n    and verifies theoretical properties of the regularization path.\n    \"\"\"\n    test_cases_params = [\n        # (m, n, seed)\n        (80, 20, 42),\n        (50, 100, 123),\n        (40, 40, 7),\n        (60, 20, 314),\n    ]\n\n    all_results = []\n\n    for i, params in enumerate(test_cases_params):\n        m, n, seed = params\n        rng = np.random.default_rng(seed)\n\n        if i == 0:  # Case 1: well-conditioned, overdetermined\n            A = rng.standard_normal((m, n))\n            x_true = rng.standard_normal(n)\n            epsilon = rng.standard_normal(m) * 0.01\n            b = A @ x_true + epsilon\n        elif i == 1:  # Case 2: underdetermined\n            A = rng.standard_normal((m, n))\n            x_true = rng.standard_normal(n)\n            epsilon = rng.standard_normal(m) * 0.01\n            b = A @ x_true + epsilon\n        elif i == 2:  # Case 3: rank-deficient\n            A = rng.standard_normal((m, n))\n            A[:, 10] = A[:, 5]\n            A[:, 15] = A[:, 5]\n            x_true = rng.standard_normal(n)\n            epsilon = rng.standard_normal(m) * 0.01\n            b = A @ x_true + epsilon\n        elif i == 3:  # Case 4: ill-conditioned\n            G = rng.standard_normal((m, n))\n            A = np.zeros_like(G)\n            for j in range(n):\n                A[:, j] = G[:, j] * (10**(-j / 3.0))\n            x_true = rng.standard_normal(n)\n            epsilon = rng.standard_normal(m) * 0.01\n            b = A @ x_true + epsilon\n\n        # Regularization path calculation\n        lambda_grid = np.logspace(-10, 6, 60)\n        \n        # SVD-based solution\n        U, s, Vt = svd(A, full_matrices=True)\n        k = len(s)\n        V = Vt.T\n        c = U.T @ b\n\n        x_path = []\n        for lam in lambda_grid:\n            y = np.zeros(n)\n            y[:k] = (s * c[:k]) / (s**2 + lam**2)\n            x_lam = V @ y\n            x_path.append(x_lam)\n\n        x_path_norms = [np.linalg.norm(x) for x in x_path]\n\n        # Verification checks\n        \n        # 1. Monotone shrinkage of the coefficient Euclidean norm\n        is_monotone = True\n        for k_idx in range(1, len(x_path_norms)):\n            norm_prev = x_path_norms[k_idx - 1]\n            norm_curr = x_path_norms[k_idx]\n            # Since lambda grid is increasing, norm should be non-increasing\n            tolerance = 1e-12 * max(1, norm_prev)\n            if norm_curr > norm_prev + tolerance:\n                is_monotone = False\n                break\n        \n        # 2. Near-agreement with least squares at small lambda\n        x_lsq = np.linalg.pinv(A) @ b\n        norm_x_lsq = np.linalg.norm(x_lsq)\n        x_lambda_min = x_path[0]\n        \n        rel_diff_small_lambda = np.linalg.norm(x_lambda_min - x_lsq) / max(1, norm_x_lsq)\n        agrees_at_small_lambda = rel_diff_small_lambda  1e-6\n        \n        # 3. Near-zero coefficients at very large lambda\n        x_lambda_max = x_path[-1]\n        norm_x_lambda_max = np.linalg.norm(x_lambda_max)\n        \n        ratio_large_lambda = norm_x_lambda_max / max(1, norm_x_lsq)\n        zero_at_large_lambda = ratio_large_lambda  1e-6\n\n        all_results.extend([is_monotone, agrees_at_small_lambda, zero_at_large_lambda])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```", "id": "3200591"}, {"introduction": "While the SVD-based approach is a powerful general-purpose tool, many real-world problems feature matrices with special structures that can be exploited for significant computational gains. This advanced practice explores such a scenario, where the system matrix has a low-rank structure. You will derive and implement a highly efficient solution using the Sherman-Morrison-Woodbury identity, bypassing the need to form and invert large, dense matrices and showcasing a key principle of high-performance scientific computing. [@problem_id:3283854]", "problem": "You are given a square system matrix of the form $A = I + u v^{\\top}$ with vectors $u, v \\in \\mathbb{R}^{n}$. When the inner product $u^{\\top} v$ is close to $-1$, the matrix $A$ becomes nearly singular, which can cause numerical instability when inverting or solving linear systems with $A$. To handle this, consider the Tikhonov regularized least-squares problem: find $x_{\\alpha} \\in \\mathbb{R}^{n}$ that minimizes $\\lVert A x - b \\rVert_{2}^{2} + \\alpha \\lVert x \\rVert_{2}^{2}$ for a regularization parameter $\\alpha \\ge 0$. This solution satisfies the normal equations $(A^{\\top} A + \\alpha I) x_{\\alpha} = A^{\\top} b$.\n\nStarting only from core definitions and well-tested facts, derive a computationally stable formula for $x_{\\alpha}$ that exploits the low-rank structure of $A^{\\top} A + \\alpha I$ and avoids explicitly inverting large matrices:\n\n- Use the normal equations $(A^{\\top} A + \\alpha I) x_{\\alpha} = A^{\\top} b$ as the defining relation for the Tikhonov regularized solution.\n- Use the matrix inversion lemma (also known as the Sherman–Morrison–Woodbury identity) as a foundational tool: for an invertible matrix $C \\in \\mathbb{R}^{n \\times n}$ and matrices $U, V \\in \\mathbb{R}^{n \\times k}$, $(C + U V^{\\top})^{-1} = C^{-1} - C^{-1} U (I_{k} + V^{\\top} C^{-1} U)^{-1} V^{\\top} C^{-1}$. Do not assume any other specialized formulas.\n- Express your final algorithm solely in terms of scalar inner products, small fixed-size matrix inversions, and matrix–vector products with $u$ and $v$. Avoid forming dense $n \\times n$ matrices whenever a lower-dimensional computation suffices.\n\nThen implement a program that, for each test case in the suite below, computes $x_{\\alpha}$ in two ways:\n- Method $1$ (direct): Solve $(A^{\\top} A + \\alpha I) x = A^{\\top} b$ using a dense linear solver.\n- Method $2$ (structure-exploiting): Apply your derived low-rank, matrix-inversion-lemma-based formula to compute $(A^{\\top} A + \\alpha I)^{-1} A^{\\top} b$ without forming large inverses.\n\nFor each test case, report the maximum absolute difference between the two computed vectors $x_{\\alpha}$, that is, $\\max_{i} |(x_{\\alpha}^{(1)})_{i} - (x_{\\alpha}^{(2)})_{i}|$, as a single floating-point number.\n\nNo physical units or angle units are involved. All outputs must be real numbers.\n\nTest suite (use exactly these values):\n- Dimension: $n = 5$.\n- Fixed vector $u = [1, -2, 3, -4, 5]^{\\top}$.\n- Base right-hand side $b_{0} = [2, -1, 0.5, 1.5, -3]^{\\top}$.\n- For each test case, construct $v$ from a specified $\\delta$ via $v = c u$ with $c = \\dfrac{-1 + \\delta}{\\lVert u \\rVert_{2}^{2}}$. This ensures $u^{\\top} v = -1 + \\delta$.\n- Define the five cases as tuples $(\\delta, \\alpha, b)$:\n  1. $(0.2, 0, b_{0})$,\n  2. $(10^{-8}, 10^{-8}, b_{0})$,\n  3. $(10^{-12}, 10^{-4}, b_{0})$,\n  4. $(0, 1, b_{0})$,\n  5. $(10^{-14}, 10^{-6}, 0 \\cdot b_{0})$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test suite above, for example, $[r_{1}, r_{2}, r_{3}, r_{4}, r_{5}]$, where each $r_{i}$ is the maximum absolute difference for test case $i$ computed as described.", "solution": "The problem requires deriving a computationally efficient formula for the Tikhonov-regularized solution $x_{\\alpha}$ using the Sherman-Morrison-Woodbury (SMW) identity.\n\nThe solution $x_{\\alpha}$ is defined by the normal equations:\n$$ (A^{\\top} A + \\alpha I) x_{\\alpha} = A^{\\top} b $$\nLet the matrix to be inverted be $M = A^{\\top} A + \\alpha I$. The goal is to compute $M^{-1}$ efficiently by exploiting the structure of $A = I + u v^{\\top}$.\n\nFirst, we expand the term $A^{\\top} A$:\n$$\n\\begin{aligned}\nA^{\\top} A = (I + v u^{\\top})(I + u v^{\\top}) \\\\\n= I \\cdot I + I u v^{\\top} + v u^{\\top} I + v (u^{\\top} u) v^{\\top} \\\\\n= I + u v^{\\top} + v u^{\\top} + (u^{\\top}u) v v^{\\top}\n\\end{aligned}\n$$\nThe matrix $M$ is then:\n$$ M = A^{\\top} A + \\alpha I = (1 + \\alpha)I + u v^{\\top} + v u^{\\top} + (u^{\\top}u) v v^{\\top} $$\nTo apply the SMW formula, $(C + UV^{\\top})^{-1} = C^{-1} - C^{-1}U(I_k + V^{\\top}C^{-1}U)^{-1}V^{\\top}C^{-1}$, we must express the update part as a low-rank product $UV^{\\top}$. The update is a sum of two outer products:\n$$ u v^{\\top} + v u^{\\top} + (u^{\\top}u) v v^{\\top} = u v^{\\top} + v(u^{\\top} + (u^{\\top}u)v^{\\top}) = u v^{\\top} + v(u + (u^{\\top}u)v)^{\\top} $$\nWe can write this in the form $UV^{\\top}$ with $k=2$ by setting:\n- $U = \\begin{bmatrix} u  v \\end{bmatrix}$ (an $n \\times 2$ matrix)\n- $V = \\begin{bmatrix} v  u + (u^{\\top}u)v \\end{bmatrix}$ (an $n \\times 2$ matrix)\n\nOur matrix $M$ can now be written as $M = C + UV^{\\top}$, where $C = (1+\\alpha)I$. Since $C$ is diagonal, its inverse is simple: $C^{-1} = \\frac{1}{1+\\alpha}I$.\n\nApplying the SMW identity:\n$$ M^{-1} = C^{-1} - C^{-1}U (I_2 + V^{\\top}C^{-1}U)^{-1} V^{\\top}C^{-1} $$\n$$ M^{-1} = \\frac{1}{1+\\alpha}I - \\left(\\frac{1}{1+\\alpha}U\\right) \\left(I_2 + V^{\\top}\\left(\\frac{1}{1+\\alpha}I\\right)U\\right)^{-1} \\left(\\frac{1}{1+\\alpha}V^{\\top}\\right) $$\n$$ M^{-1} = \\frac{1}{1+\\alpha}I - \\frac{1}{(1+\\alpha)^2} U \\left(I_2 + \\frac{1}{1+\\alpha}V^{\\top}U\\right)^{-1} V^{\\top} $$\nThe solution is $x_{\\alpha} = M^{-1} (A^{\\top} b)$. This algorithm avoids forming the dense $n \\times n$ matrix $M$. The main cost is in computing the inverse of the small $2 \\times 2$ matrix $S = I_2 + \\frac{1}{1+\\alpha}V^{\\top}U$ and performing several vector operations (inner products, scaling, and addition), leading to an $O(n)$ complexity instead of $O(n^3)$.\n\nThe overall algorithm is:\n1.  Compute scalar inner products needed for the $2 \\times 2$ matrix $K = V^{\\top}U$: $u^{\\top}u$, $u^{\\top}v$, $v^{\\top}v$.\n2.  Form the $2 \\times 2$ matrix $S = I_2 + \\frac{1}{1+\\alpha}K$ and compute its inverse $S^{-1}$.\n3.  Compute the vector $A^{\\top}b = (I + v u^{\\top})b = b + (u^{\\top}b)v$.\n4.  Compute the vector $z = V^{\\top}(A^{\\top}b)$. This involves two inner products.\n5.  Compute the vector $\\gamma = S^{-1}z$.\n6.  The final solution is given by:\n    $$ x_{\\alpha} = \\frac{1}{1+\\alpha} (A^{\\top}b) - \\frac{1}{(1+\\alpha)^2} U \\gamma $$\n    where $U\\gamma = \\gamma_1 u + \\gamma_2 v$.\n\nThis procedure is implemented as \"Method 2\" in the code.", "answer": "```python\nimport numpy as np\n\ndef solve_direct(A, b, alpha):\n    \"\"\"\n    Method 1: Solve (A^T A + alpha*I)x = A^T b using a dense linear solver.\n    \"\"\"\n    n = A.shape[0]\n    I = np.identity(n)\n    \n    A_T_A = A.T @ A\n    \n    # System matrix and right-hand side\n    system_matrix = A_T_A + alpha * I\n    rhs_vector = A.T @ b\n    \n    # Solve the linear system\n    try:\n        x1 = np.linalg.solve(system_matrix, rhs_vector)\n    except np.linalg.LinAlgError:\n        # Fallback to pseudo-inverse if solve fails (e.g., for alpha=0 and singular matrix)\n        x1 = np.linalg.pinv(system_matrix) @ rhs_vector\n        \n    return x1\n\ndef solve_smw(u, v, b, alpha):\n    \"\"\"\n    Method 2: Apply the derived low-rank, SMW-based formula.\n    \"\"\"\n    n = u.shape[0]\n    I = np.identity(n)\n    I2 = np.identity(2)\n\n    # 1. Compute scalar products and related values\n    s_uu = u.T @ u\n    s_uv = u.T @ v\n    s_vv = v.T @ v\n    u_b = u.T @ b\n\n    # 2. Compute vectors needed for the formula\n    A_T_b = b + v * u_b\n    w = u + s_uu * v\n\n    # 3. Form the 2x2 matrix K\n    # K = V_smw.T @ U_smw, where U_smw=[u,v], V_smw=[v,w]\n    K = np.array([\n        [s_uv, s_vv],\n        [w.T @ u, w.T @ v]\n    ])\n\n    # 4. Invert the 2x2 matrix M_2x2 = I2 + (1/(1+alpha)) * K\n    inv_1_plus_alpha = 1.0 / (1.0 + alpha)\n    M_2x2 = I2 + inv_1_plus_alpha * K\n    M_2x2_inv = np.linalg.inv(M_2x2)\n\n    # 5. Compute the 2x1 vector z = V_smw.T @ A_T_b\n    z = np.array([v.T @ A_T_b, w.T @ A_T_b])\n\n    # 6. Compute gamma = M_2x2_inv @ z\n    gamma = M_2x2_inv @ z\n\n    # 7. Assemble the final solution x_alpha\n    # U_smw @ gamma = gamma[0]*u + gamma[1]*v\n    U_smw_gamma = gamma[0] * u + gamma[1] * v\n    \n    term1 = inv_1_plus_alpha * A_T_b\n    term2 = (inv_1_plus_alpha**2) * U_smw_gamma\n    \n    x2 = term1 - term2\n    \n    return x2\n\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and compute differences.\n    \"\"\"\n    # Fixed parameters from the problem statement\n    n = 5\n    u = np.array([1.0, -2.0, 3.0, -4.0, 5.0])\n    b0 = np.array([2.0, -1.0, 0.5, 1.5, -3.0])\n    \n    test_cases = [\n        (0.2, 0.0, b0),\n        (1e-8, 1e-8, b0),\n        (1e-12, 1e-4, b0),\n        (0.0, 1.0, b0),\n        (1e-14, 1e-6, 0.0 * b0),\n    ]\n\n    results = []\n\n    s_uu = u.T @ u\n\n    for delta, alpha, b in test_cases:\n        # Construct v based on delta\n        c = (-1.0 + delta) / s_uu\n        v = c * u\n        \n        # Form matrix A\n        A = np.identity(n) + np.outer(u, v)\n\n        # Method 1: Direct solver\n        x1 = solve_direct(A, b, alpha)\n        \n        # Method 2: SMW-based formula\n        x2 = solve_smw(u, v, b, alpha)\n\n        # Calculate the maximum absolute difference\n        max_abs_diff = np.max(np.abs(x1 - x2))\n        results.append(max_abs_diff)\n    \n    print(f\"[{','.join(f'{r:.15e}' for r in results)}]\")\n\nsolve()\n```", "id": "3283854"}]}