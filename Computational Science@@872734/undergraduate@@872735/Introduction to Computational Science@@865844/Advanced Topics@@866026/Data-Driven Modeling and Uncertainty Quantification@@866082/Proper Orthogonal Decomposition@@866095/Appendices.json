{"hands_on_practices": [{"introduction": "At its heart, Proper Orthogonal Decomposition (POD) is a direct application of a powerful linear algebra tool: the Singular Value Decomposition (SVD). This first exercise solidifies that foundational connection by having you work with a data matrix constructed explicitly from its SVD components. By analyzing this matrix, you will see how POD precisely recovers the underlying rank and singular values, reinforcing the core algebraic principles of the method.", "problem": "Consider a snapshot matrix $X \\in \\mathbb{R}^{100 \\times 50}$ used in reduced-order modeling via Proper Orthogonal Decomposition (POD). Construct $X$ by selecting three orthonormal spatial vectors $u_1, u_2, u_3 \\in \\mathbb{R}^{100}$, three orthonormal temporal vectors $v_1, v_2, v_3 \\in \\mathbb{R}^{50}$, and three positive scalars $\\sigma_1, \\sigma_2, \\sigma_3$. Let the spatial vectors be the first three standard basis vectors, $u_1 = e_1$, $u_2 = e_2$, $u_3 = e_3$, where $e_i \\in \\mathbb{R}^{100}$ has a $1$ in the $i$-th position and $0$ elsewhere. Let the temporal vectors be the first three standard basis vectors, $v_1 = e_1$, $v_2 = e_2$, $v_3 = e_3$, where $e_i \\in \\mathbb{R}^{50}$ has a $1$ in the $i$-th position and $0$ elsewhere. Choose the singular values $\\sigma_1 = 6$, $\\sigma_2 = 3$, and $\\sigma_3 = 1$. Define the snapshot matrix as the rank-$3$ sum of outer products\n$$\nX \\;=\\; \\sigma_1 \\, u_1 v_1^{\\top} \\;+\\; \\sigma_2 \\, u_2 v_2^{\\top} \\;+\\; \\sigma_3 \\, u_3 v_3^{\\top}.\n$$\nUsing only the foundational definitions of singular value decomposition and matrix rank, determine the number of non-zero singular values recovered by Proper Orthogonal Decomposition (POD) applied to $X$. Express your final answer as a single integer.", "solution": "The problem requires determining the number of non-zero singular values of a given matrix $X$, which is constructed in a specific manner relevant to Proper Orthogonal Decomposition (POD).\n\nFirst, let us establish the foundational principles. For any real matrix $X \\in \\mathbb{R}^{m \\times n}$, the Singular Value Decomposition (SVD) provides a factorization of the form $X = U \\Sigma V^{\\top}$. Here, $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is a rectangular diagonal matrix. The diagonal entries of $\\Sigma$, denoted by $\\sigma_i$, are the singular values of $X$. They are non-negative and conventionally ordered such that $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_p \\ge 0$, where $p = \\min(m, n)$.\n\nThe SVD can also be expressed as a sum of rank-$1$ outer products:\n$$\nX = \\sum_{i=1}^{r} \\sigma_i u_i v_i^{\\top}\n$$\nwhere $r$ is the rank of the matrix $X$. The vectors $u_i \\in \\mathbb{R}^m$ are the columns of $U$ (the left singular vectors) and form an orthonormal set, i.e., $u_i^{\\top} u_j = \\delta_{ij}$. The vectors $v_i \\in \\mathbb{R}^n$ are the columns of $V$ (the right singular vectors) and also form an orthonormal set, i.e., $v_i^{\\top} v_j = \\delta_{ij}$. The scalars $\\sigma_i > 0$ for $i=1, \\dots, r$ are the non-zero singular values of $X$. The number of non-zero singular values is, by definition, the rank of the matrix.\n\nThe problem states that Proper Orthogonal Decomposition (POD) is applied to the snapshot matrix $X$. Within the framework of numerical linear algebra, POD (specifically, using the method of snapshots) is mathematically equivalent to the SVD of the snapshot matrix. The process recovers the left singular vectors $u_i$ (the POD modes), the right singular vectors $v_i$ (the temporal modes), and the singular values $\\sigma_i$. The number of non-zero singular values recovered is therefore the rank of $X$.\n\nThe snapshot matrix $X \\in \\mathbb{R}^{100 \\times 50}$ is defined as:\n$$\nX = \\sigma_1 u_1 v_1^{\\top} + \\sigma_2 u_2 v_2^{\\top} + \\sigma_3 u_3 v_3^{\\top}\n$$\nWe are given the following definitions:\n1.  The scalars are $\\sigma_1 = 6$, $\\sigma_2 = 3$, and $\\sigma_3 = 1$. These are all positive.\n2.  The spatial vectors are $u_1 = e_1$, $u_2 = e_2$, $u_3 = e_3$, where $e_i \\in \\mathbb{R}^{100}$ are the standard basis vectors. The set $\\{u_1, u_2, u_3\\}$ is an orthonormal set, as $e_i^{\\top}e_j = \\delta_{ij}$ for $i, j \\in \\{1, 2, 3\\}$.\n3.  The temporal vectors are $v_1 = e_1$, $v_2 = e_2$, $v_3 = e_3$, where $e_i \\in \\mathbb{R}^{50}$ are the standard basis vectors. The set $\\{v_1, v_2, v_3\\}$ is also an orthonormal set.\n\nThe given expression for $X$ perfectly matches the structure of the SVD outer product expansion. We have a sum of three terms. Each term is the product of a positive scalar $\\sigma_i$, a unit vector $u_i$, and the transpose of a unit vector $v_i$. The sets of vectors $\\{u_1, u_2, u_3\\}$ and $\\{v_1, v_2, v_3\\}$ are both orthonormal.\n\nThis means that the provided expression is, in fact, the singular value decomposition of $X$. The non-zero singular values of $X$ are precisely the scalars given: $\\sigma_1 = 6$, $\\sigma_2 = 3$, and $\\sigma_3 = 1$. All other singular values, $\\sigma_i$ for $i > 3$, are zero.\n\nThe problem asks for the *number* of non-zero singular values recovered by applying POD to $X$. Based on our analysis, there are exactly three positive singular values. The number of non-zero singular values is therefore $3$. This is also the rank of the matrix $X$.\n\nThe question is a direct test of the foundational definition of singular value decomposition and its relation to matrix rank. The construction of the matrix $X$ is a deliberate formulation of a rank-$3$ matrix via its SVD expansion. No further computation is necessary. The number of terms in the sum, given that the scalars are non-zero and the vector sets are orthonormal, directly provides the answer.\nThe number of non-zero singular values is $3$.", "answer": "$$\n\\boxed{3}\n$$", "id": "3265930"}, {"introduction": "Moving from the discrete world of matrices to the continuous world of functions, this next practice challenges you to derive POD modes analytically. Instead of a pre-made data matrix, you will start with a function representing a space-time field and use calculus to find the optimal basis functions. This exercise is crucial for understanding how POD is defined for continuous systems, a common scenario in physics, by constructing and analyzing the spatial correlation operator.", "problem": "Consider the spatial domain $x \\in [0,1]$ with the spatial inner product given by $\\langle f, g \\rangle = \\int_{0}^{1} f(x) g(x) \\, dx$, and the temporal ensemble average defined by the long-time mean $\\lim_{T \\to \\infty} \\frac{1}{T} \\int_{0}^{T} (\\cdot) \\, dt$. Let the space-time field be\n$$\nu(x,t) = \\sin(2\\pi x) \\cos(\\omega_{1} t) + \\sin(3\\pi x) \\cos(\\omega_{2} t),\n$$\nwhere $\\omega_{1} > 0$ and $\\omega_{2} > 0$ are distinct real constants. Using the foundational definition of Proper Orthogonal Decomposition (POD), where the spatial POD modes are the eigenfunctions of the spatial correlation operator\n$$\n\\mathcal{C}[\\phi](x) = \\int_{0}^{1} C(x,y) \\, \\phi(y) \\, dy,\n$$\nconstructed from the correlation kernel\n$$\nC(x,y) = \\lim_{T \\to \\infty} \\frac{1}{T} \\int_{0}^{T} u(x,t) \\, u(y,t) \\, dt,\n$$\nderive the analytical spatial POD modes associated with $u(x,t)$. Express the modes as $L^{2}([0,1])$-normalized functions. Your final answer must list the two normalized spatial POD modes in a single row matrix. No units are required, and no rounding is needed.", "solution": "The problem requires the derivation of the spatial Proper Orthogonal Decomposition (POD) modes for a given space-time field $u(x,t)$. By definition, the spatial POD modes, denoted by $\\phi(x)$, are the eigenfunctions of the spatial two-point correlation operator $\\mathcal{C}$, which is an integral operator with kernel $C(x,y)$. The eigenproblem is given by:\n$$\n\\mathcal{C}[\\phi](x) = \\int_{0}^{1} C(x,y) \\, \\phi(y) \\, dy = \\lambda \\phi(x)\n$$\nThe kernel $C(x,y)$ is defined as the time-average of the product of the field at two spatial locations, $x$ and $y$.\n$$\nC(x,y) = \\lim_{T \\to \\infty} \\frac{1}{T} \\int_{0}^{T} u(x,t) \\, u(y,t) \\, dt\n$$\nThe first step is to compute this kernel for the given field $u(x,t) = \\sin(2\\pi x) \\cos(\\omega_{1} t) + \\sin(3\\pi x) \\cos(\\omega_{2} t)$.\n\nLet's expand the product $u(x,t) \\, u(y,t)$:\n\\begin{align*}\nu(x,t) \\, u(y,t) = & \\left[ \\sin(2\\pi x) \\cos(\\omega_{1} t) + \\sin(3\\pi x) \\cos(\\omega_{2} t) \\right] \\left[ \\sin(2\\pi y) \\cos(\\omega_{1} t) + \\sin(3\\pi y) \\cos(\\omega_{2} t) \\right] \\\\\n= & \\sin(2\\pi x) \\sin(2\\pi y) \\cos^2(\\omega_{1} t) \\\\\n& + \\sin(3\\pi x) \\sin(3\\pi y) \\cos^2(\\omega_{2} t) \\\\\n& + \\sin(2\\pi x) \\sin(3\\pi y) \\cos(\\omega_{1} t) \\cos(\\omega_{2} t) \\\\\n& + \\sin(3\\pi x) \\sin(2\\pi y) \\cos(\\omega_{1} t) \\cos(\\omega_{2} t)\n\\end{align*}\nTo find $C(x,y)$, we must compute the long-time average of the temporal components. We need the following standard time-average results for harmonic functions:\n$1$. For any non-zero frequency $\\omega > 0$,\n$$\n\\lim_{T \\to \\infty} \\frac{1}{T} \\int_{0}^{T} \\cos^2(\\omega t) \\, dt = \\lim_{T \\to \\infty} \\frac{1}{T} \\int_{0}^{T} \\frac{1 + \\cos(2\\omega t)}{2} \\, dt = \\lim_{T \\to \\infty} \\frac{1}{T} \\left[ \\frac{t}{2} + \\frac{\\sin(2\\omega t)}{4\\omega} \\right]_{0}^{T} = \\frac{1}{2}\n$$\n$2$. For two distinct positive frequencies $\\omega_1 \\neq \\omega_2$,\n$$\n\\lim_{T \\to \\infty} \\frac{1}{T} \\int_{0}^{T} \\cos(\\omega_1 t) \\cos(\\omega_2 t) \\, dt = \\lim_{T \\to \\infty} \\frac{1}{2T} \\int_{0}^{T} \\left[ \\cos((\\omega_1 - \\omega_2)t) + \\cos((\\omega_1 + \\omega_2)t) \\right] dt = 0\n$$\nThe second result holds because the integral of a cosine function over a period is zero, and its indefinite integral is a sine function, which is bounded. Dividing by $T \\to \\infty$ makes the average tend to $0$.\n\nApplying these time averages to the expanded product, the cross-terms involving $\\cos(\\omega_{1} t) \\cos(\\omega_{2} t)$ average to zero. We are left with:\n\\begin{align*}\nC(x,y) &= \\sin(2\\pi x) \\sin(2\\pi y) \\left( \\lim_{T \\to \\infty} \\frac{1}{T} \\int_{0}^{T} \\cos^2(\\omega_{1} t) \\, dt \\right) + \\sin(3\\pi x) \\sin(3\\pi y) \\left( \\lim_{T \\to \\infty} \\frac{1}{T} \\int_{0}^{T} \\cos^2(\\omega_{2} t) \\, dt \\right) \\\\\n&= \\frac{1}{2} \\sin(2\\pi x) \\sin(2\\pi y) + \\frac{1}{2} \\sin(3\\pi x) \\sin(3\\pi y)\n\\end{align*}\nThis is a separable kernel of rank $2$. The eigenfunctions of an operator with such a kernel must lie in the span of the functions that constitute the kernel, i.e., $\\text{span}\\{\\sin(2\\pi x), \\sin(3\\pi x)\\}$.\n\nLet's check if the basis functions $\\psi_1(x) = \\sin(2\\pi x)$ and $\\psi_2(x) = \\sin(3\\pi x)$ are orthogonal under the given inner product $\\langle f, g \\rangle = \\int_{0}^{1} f(x) g(x) \\, dx$:\n$$\n\\langle \\psi_1, \\psi_2 \\rangle = \\int_{0}^{1} \\sin(2\\pi x) \\sin(3\\pi x) \\, dx = \\frac{1}{2} \\int_{0}^{1} \\left[ \\cos(\\pi x) - \\cos(5\\pi x) \\right] dx = \\frac{1}{2} \\left[ \\frac{\\sin(\\pi x)}{\\pi} - \\frac{\\sin(5\\pi x)}{5\\pi} \\right]_{0}^{1} = 0\n$$\nSince the spatial functions are orthogonal, they are indeed the eigenfunctions of the correlation operator $\\mathcal{C}$. We can verify this by substituting them into the eigenproblem.\n\nFor the first eigenfunction candidate $\\phi(x) = \\psi_1(x) = \\sin(2\\pi x)$:\n\\begin{align*}\n\\mathcal{C}[\\psi_1](x) &= \\int_{0}^{1} \\left[ \\frac{1}{2} \\sin(2\\pi x) \\sin(2\\pi y) + \\frac{1}{2} \\sin(3\\pi x) \\sin(3\\pi y) \\right] \\sin(2\\pi y) \\, dy \\\\\n&= \\frac{1}{2} \\sin(2\\pi x) \\int_{0}^{1} \\sin^2(2\\pi y) \\, dy + \\frac{1}{2} \\sin(3\\pi x) \\int_{0}^{1} \\sin(3\\pi y) \\sin(2\\pi y) \\, dy\n\\end{align*}\nThe second integral is zero due to orthogonality. The first integral is:\n$$\n\\int_{0}^{1} \\sin^2(2\\pi y) \\, dy = \\int_{0}^{1} \\frac{1 - \\cos(4\\pi y)}{2} \\, dy = \\left[ \\frac{y}{2} - \\frac{\\sin(4\\pi y)}{8\\pi} \\right]_{0}^{1} = \\frac{1}{2}\n$$\nThus, $\\mathcal{C}[\\psi_1](x) = \\frac{1}{2} \\sin(2\\pi x) \\cdot \\frac{1}{2} = \\frac{1}{4} \\sin(2\\pi x)$.\nSo, $\\phi_1(x) = \\sin(2\\pi x)$ is an eigenfunction with eigenvalue $\\lambda_1 = \\frac{1}{4}$.\n\nFor the second eigenfunction candidate $\\phi(x) = \\psi_2(x) = \\sin(3\\pi x)$:\n\\begin{align*}\n\\mathcal{C}[\\psi_2](x) &= \\int_{0}^{1} \\left[ \\frac{1}{2} \\sin(2\\pi x) \\sin(2\\pi y) + \\frac{1}{2} \\sin(3\\pi x) \\sin(3\\pi y) \\right] \\sin(3\\pi y) \\, dy \\\\\n&= \\frac{1}{2} \\sin(2\\pi x) \\int_{0}^{1} \\sin(2\\pi y) \\sin(3\\pi y) \\, dy + \\frac{1}{2} \\sin(3\\pi x) \\int_{0}^{1} \\sin^2(3\\pi y) \\, dy\n\\end{align*}\nThe first integral is zero. The second integral is:\n$$\n\\int_{0}^{1} \\sin^2(3\\pi y) \\, dy = \\int_{0}^{1} \\frac{1 - \\cos(6\\pi y)}{2} \\, dy = \\left[ \\frac{y}{2} - \\frac{\\sin(6\\pi y)}{12\\pi} \\right]_{0}^{1} = \\frac{1}{2}\n$$\nThus, $\\mathcal{C}[\\psi_2](x) = \\frac{1}{2} \\sin(3\\pi x) \\cdot \\frac{1}{2} = \\frac{1}{4} \\sin(3\\pi x)$.\nSo, $\\phi_2(x) = \\sin(3\\pi x)$ is an eigenfunction with eigenvalue $\\lambda_2 = \\frac{1}{4}$.\n\nThe final step is to normalize these eigenfunctions to have unit $L^2$-norm. The squared norm of an eigenfunction $\\phi$ is $\\|\\phi\\|^2 = \\langle \\phi, \\phi \\rangle = \\int_{0}^{1} \\phi(x)^2 \\, dx$.\nFor the first mode, $\\sin(2\\pi x)$:\n$$\n\\|\\sin(2\\pi x)\\|^2 = \\int_{0}^{1} \\sin^2(2\\pi x) \\, dx = \\frac{1}{2}\n$$\nThe normalization constant is $1 / \\sqrt{1/2} = \\sqrt{2}$. The first normalized POD mode is $\\hat{\\phi_1}(x) = \\sqrt{2} \\sin(2\\pi x)$.\n\nFor the second mode, $\\sin(3\\pi x)$:\n$$\n\\|\\sin(3\\pi x)\\|^2 = \\int_{0}^{1} \\sin^2(3\\pi x) \\, dx = \\frac{1}{2}\n$$\nThe normalization constant is also $\\sqrt{2}$. The second normalized POD mode is $\\hat{\\phi_2}(x) = \\sqrt{2} \\sin(3\\pi x)$.\n\nThe two non-trivial spatial POD modes are therefore $\\sqrt{2} \\sin(2\\pi x)$ and $\\sqrt{2} \\sin(3\\pi x)$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\sqrt{2}\\sin(2\\pi x) & \\sqrt{2}\\sin(3\\pi x)\n\\end{pmatrix}\n}\n$$", "id": "3265890"}, {"introduction": "Now, let's put theory into practice with a computational exercise that demonstrates both the power and the limitations of POD. You will implement POD to analyze a common physical phenomenon—a translating wave—and quantitatively measure how well a low-rank model can capture its dynamics. This coding practice will not only build your skills but also reveal a critical insight into POD's performance: its effectiveness at data compression is not universal and depends heavily on the nature of the system's evolution.", "problem": "Consider the family of snapshot functions of a translating Gaussian pulse defined by $u(x,t) = \\exp\\!\\left(-\\big(x - c t\\big)^{2}\\right)$ on the spatial interval $x \\in [-L,L]$ and discrete times $t \\in \\{t_{0}, t_{1}, \\dots, t_{m-1}\\}$. From first principles, Proper Orthogonal Decomposition (POD) is the procedure that, for a given rank $r$, selects an $r$-dimensional orthonormal basis in space that minimizes the total squared projection error of the snapshot set. Equivalently, it produces the best rank-$r$ approximation of the snapshot data (in the Euclidean least-squares sense across all grid points and times).\n\nYour task is to implement a program that:\n- Constructs the snapshot matrix $X \\in \\mathbb{R}^{N_x \\times m}$ whose $k$-th column is the sampled snapshot $u(x,t_k)$ at $N_x$ uniformly spaced grid points in $[-L,L]$, for a given speed $c$, number of snapshots $m$, and final time $T$ with $t_k$ equally spaced in $[0,T]$.\n- Computes, for ranks $r \\in \\{1,2,5,10\\}$, the best rank-$r$ approximation $X_r$ (as defined by POD) and the corresponding relative reconstruction error\n$$E_r = \\frac{\\lVert X - X_r \\rVert_F}{\\lVert X \\rVert_F},$$\nwhere $\\lVert \\cdot \\rVert_F$ denotes the Frobenius norm.\n- Reports the errors $E_r$ for each test case as floating-point numbers rounded to six decimal places.\n\nFundamental base to use:\n- Definitions of Euclidean inner product and Frobenius norm.\n- The defining optimization property of Proper Orthogonal Decomposition (POD): among all $r$-dimensional orthonormal bases, POD minimizes the total squared projection error of the snapshots. This yields the best rank-$r$ approximation of the snapshot matrix in the least-squares sense.\n\nTest suite:\nUse $L = 10$ and $N_x = 401$ for all cases. The four test cases are:\n1. Case A (stationary pulse): $c = 0$, $T = 5$, $m = 50$.\n2. Case B (slow translation): $c = 0.5$, $T = 10$, $m = 100$.\n3. Case C (fast translation): $c = 2.0$, $T = 4$, $m = 80$.\n4. Case D (few snapshots): $c = 0.5$, $T = 10$, $m = 5$.\n\nAnswer specification:\n- For each test case, output a list $[E_{1},E_{2},E_{5},E_{10}]$ of four floats rounded to six decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the bracketed list of four errors for one test case, in the order of Cases A, B, C, D. For example, an output of the correct format would look like\n$[[e_{A,1},e_{A,2},e_{A,5},e_{A,10}],[e_{B,1},e_{B,2},e_{B,5},e_{B,10}],[e_{C,1},e_{C,2},e_{C,5},e_{C,10}],[e_{D,1},e_{D,2},e_{D,5},e_{D,10}]]$,\nwith no spaces anywhere in the line.\n\nUnits:\n- There are no physical units required in this problem.\n\nAngle units:\n- Not applicable.\n\nPercentages:\n- Not applicable; express all quantities as decimals.\n\nYour implementation must be self-contained and require no user input, external files, or network access. It must run in a modern programming language and produce the exact final output format described above in a single line.", "solution": "### Principle-Based Solution\nThe objective is to compute the relative reconstruction error for a rank-$r$ approximation of a set of data snapshots. The foundational principle is that the optimal rank-$r$ approximation, in the least-squares sense defined by the Frobenius norm, is obtained via the Singular Value Decomposition (SVD). This result is formally stated by the Eckart-Young-Mirsky theorem.\n\n**1. Snapshot Matrix Construction**\nFirst, we discretize the problem domain. The spatial domain $x \\in [-L, L]$ is sampled at $N_x$ uniformly spaced points, forming the grid $\\{x_j\\}_{j=0}^{N_x-1}$. The time interval $t \\in [0, T]$ is sampled at $m$ discrete, equally spaced points $\\{t_k\\}_{k=0}^{m-1}$. The snapshot data at each time point $t_k$ is a vector in $\\mathbb{R}^{N_x}$ whose entries are given by the function $u(x_j, t_k)$. The collection of these snapshots forms the columns of the snapshot matrix $X \\in \\mathbb{R}^{N_x \\times m}$. An element $X_{jk}$ of this matrix is given by:\n$$X_{jk} = u(x_j, t_k) = \\exp\\!\\left(-\\big(x_j - c t_k\\big)^{2}\\right)$$\n\n**2. Singular Value Decomposition and Optimal Approximation**\nThe SVD of the snapshot matrix $X$ is given by:\n$$X = U \\Sigma V^T$$\nwhere $U \\in \\mathbb{R}^{N_x \\times N_x}$ is an orthogonal matrix whose columns $u_i$ are the left-singular vectors (POD modes), $V \\in \\mathbb{R}^{m \\times m}$ is an orthogonal matrix whose columns $v_i$ are the right-singular vectors, and $\\Sigma \\in \\mathbb{R}^{N_x \\times m}$ is a rectangular diagonal matrix containing the singular values $\\sigma_i$. The singular values are non-negative and ordered by convention: $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_k \\ge 0$, where $k = \\min(N_x, m)$.\n\nThe Eckart-Young-Mirsky theorem states that the best rank-$r$ approximation of $X$ that minimizes the Frobenius norm of the difference, $\\lVert X - X_r \\rVert_F$, is the truncated SVD:\n$$X_r = \\sum_{i=1}^{r} \\sigma_i u_i v_i^T$$\nThis approximation is constructed using the first $r$ singular values and their corresponding left and right singular vectors.\n\n**3. Error Calculation**\nThe relative reconstruction error $E_r$ is defined as the ratio of the Frobenius norm of the error matrix $(X - X_r)$ to the Frobenius norm of the original matrix $X$. The Frobenius norm is related to the singular values by the identity $\\lVert A \\rVert_F^2 = \\sum_{i=1}^{\\text{rank}(A)} \\sigma_i(A)^2$.\nApplying this property, the squared norm of the original matrix is the sum of the squares of all its singular values:\n$$\\lVert X \\rVert_F^2 = \\sum_{i=1}^{k} \\sigma_i^2$$\nThe error matrix is $X - X_r = \\sum_{i=r+1}^{k} \\sigma_i u_i v_i^T$. Due to the orthogonality of the singular vectors, the squared Frobenius norm of the error matrix is the sum of the squares of the discarded singular values:\n$$\\lVert X - X_r \\rVert_F^2 = \\sum_{i=r+1}^{k} \\sigma_i^2$$\nCombining these results, the relative reconstruction error is given by:\n$$E_r = \\frac{\\lVert X - X_r \\rVert_F}{\\lVert X \\rVert_F} = \\frac{\\sqrt{\\sum_{i=r+1}^{k} \\sigma_i^2}}{\\sqrt{\\sum_{i=1}^{k} \\sigma_i^2}}$$\nNote that if the requested rank $r$ is greater than or equal to the actual rank of the matrix, $k$, the sum in the numerator is empty and evaluates to $0$, correctly yielding an error $E_r = 0$.\n\n**4. Computational Procedure**\nThe algorithm proceeds as follows for each test case:\n1.  Define the parameters $c$, $T$, and $m$, along with the fixed constants $L=10$ and $N_x=401$.\n2.  Construct the spatial grid $x$ and temporal grid $t$.\n3.  Assemble the $N_x \\times m$ snapshot matrix $X$ using the given function $u(x,t)$.\n4.  Compute the singular values $\\sigma_i$ of $X$ using a standard numerical library function for SVD. It is most efficient to compute only the singular values, not the full $U$ and $V$ matrices.\n5.  Calculate the total energy, represented by the squared Frobenius norm, $S_{total} = \\sum_{i=1}^{k} \\sigma_i^2$.\n6.  For each required rank $r \\in \\{1, 2, 5, 10\\}$, calculate the error energy, $S_{error} = \\sum_{i=r+1}^{k} \\sigma_i^2$.\n7.  The relative error is then $E_r = \\sqrt{S_{error} / S_{total}}$.\n8.  The calculated errors for each test case are collected and formatted according to the output specification.\nThis procedure provides a direct and numerically stable method for determining the required reconstruction errors based on fundamental principles of linear algebra.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Proper Orthogonal Decomposition problem for a translating Gaussian pulse.\n    \"\"\"\n    # Global parameters for all test cases\n    L = 10.0\n    Nx = 401\n    ranks_to_compute = [1, 2, 5, 10]\n\n    # Test suite: (c, T, m)\n    # c: speed, T: final time, m: number of snapshots\n    test_cases = [\n        (0.0, 5.0, 50),   # Case A: stationary pulse\n        (0.5, 10.0, 100), # Case B: slow translation\n        (2.0, 4.0, 80),   # Case C: fast translation\n        (0.5, 10.0, 5),   # Case D: few snapshots\n    ]\n\n    all_results = []\n\n    for c, T, m in test_cases:\n        # 1. Create spatial and temporal grids\n        x = np.linspace(-L, L, Nx)\n        t = np.linspace(0.0, T, m)\n\n        # 2. Construct the snapshot matrix X using broadcasting\n        # x_col has shape (Nx, 1) and t_row has shape (1, m)\n        # Broadcasting expands them to (Nx, m) for element-wise operations\n        x_col = x[:, np.newaxis]\n        t_row = t[np.newaxis, :]\n        X = np.exp(-((x_col - c * t_row) ** 2))\n\n        # 3. Compute the singular values of X\n        # We only need the singular values, so compute_uv=False is most efficient.\n        s = np.linalg.svd(X, compute_uv=False)\n        num_singular_values = s.shape[0]\n\n        # 4. Calculate the total energy (squared Frobenius norm of X)\n        # This is the sum of the squares of all singular values.\n        norm_X_sq = np.sum(s**2)\n\n        case_errors = []\n        for r in ranks_to_compute:\n            # 5. Calculate the reconstruction error for rank r\n            \n            # If norm_X_sq is zero, all errors are zero.\n            if norm_X_sq == 0.0:\n                 error = 0.0\n            # If rank r is >= number of singular values, the approximation is perfect.\n            elif r >= num_singular_values:\n                error = 0.0\n            else:\n                # The error norm is based on the truncated singular values (from r to end).\n                # s[r:] corresponds to sigma_{r+1}, sigma_{r+2}, ...\n                norm_err_sq = np.sum(s[r:]**2)\n                error = np.sqrt(norm_err_sq / norm_X_sq)\n            \n            case_errors.append(error)\n\n        all_results.append(case_errors)\n\n    # 6. Format the output string exactly as specified.\n    # e.g., [[err1,err2,...],[err1,err2,...]] with no spaces.\n    formatted_sublists = []\n    for res_list in all_results:\n        # Format each number to 6 decimal places.\n        formatted_numbers = [f\"{err:.6f}\" for err in res_list]\n        # Join numbers with commas and enclose in brackets.\n        formatted_sublists.append(f\"[{','.join(formatted_numbers)}]\")\n    \n    # Join the sublists with commas and enclose in outer brackets.\n    final_output = f\"[{','.join(formatted_sublists)}]\"\n\n    print(final_output)\n\nsolve()\n```", "id": "3265968"}]}