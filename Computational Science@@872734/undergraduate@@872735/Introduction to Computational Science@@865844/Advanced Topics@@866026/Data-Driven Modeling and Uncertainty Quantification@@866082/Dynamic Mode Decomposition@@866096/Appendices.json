{"hands_on_practices": [{"introduction": "The first step in mastering any new computational method is to verify its performance in an ideal, controlled environment. This practice guides you through constructing a synthetic dataset from a known linear time-invariant system. By applying Dynamic Mode Decomposition (DMD) to this clean data, you will confirm the core theoretical promise of the method: that for noiseless data from a linear process, exact DMD can perfectly reconstruct the system's operator and its dynamics [@problem_id:2387371].", "problem": "You are given the task of assessing whether a data-driven linear model obtained via the exact Dynamic Mode Decomposition (DMD) achieves zero reconstruction error when applied to noiseless snapshots generated by a linear, time-invariant discrete-time system. Consider a state sequence $\\{x_k\\}_{k=0}^{m-1}$ in $\\mathbb{C}^n$ produced by the recursion $x_{k+1} = A x_k$, where $A \\in \\mathbb{C}^{n \\times n}$ is a constant matrix, and define the snapshot matrices $X_1 = [x_0, x_1, \\dots, x_{m-2}] \\in \\mathbb{C}^{n \\times (m-1)}$ and $X_2 = [x_1, x_2, \\dots, x_{m-1}] \\in \\mathbb{C}^{n \\times (m-1)}$. Let $\\widehat{A}$ denote the linear operator produced by the exact Dynamic Mode Decomposition from the pair $(X_1, X_2)$ using the full numerical rank of $X_1$. Define the normalized reconstruction error as\n$$\n\\varepsilon = \\frac{\\lVert X_2 - \\widehat{A} X_1 \\rVert_F}{\\lVert X_2 \\rVert_F},\n$$\nwhere $\\lVert \\cdot \\rVert_F$ denotes the Frobenius norm. A dataset satisfies the conditions for exact DMD if it is noise-free, generated by a linear time-invariant model as above, and the operator $\\widehat{A}$ is formed using the full numerical rank of $X_1$.\n\nConstruct three synthetic datasets by specifying $A$, $x_0$, and $m$ that meet these conditions. For each dataset, generate the snapshots $\\{x_k\\}$ by $x_{k+1} = A x_k$ and form $X_1$ and $X_2$ as defined above. For each dataset, compute $\\varepsilon$ and return a boolean indicating whether $\\varepsilon \\leq 10^{-12}$.\n\nYour program must implement this for the following test suite. In all cases where complex numbers appear, angles are in radians, and all computations are over $\\mathbb{C}$.\n\nTest Suite (each bullet fully specifies one dataset):\n\n- Dataset 1 (diagonalizable with real eigenvalues):\n  - Dimension $n = 3$, number of snapshots $m = 6$.\n  - Choose $W_1 = \\begin{bmatrix} 1  2  0 \\\\ 0  1  1 \\\\ 1  0  1 \\end{bmatrix}$ and $\\Lambda_1 = \\mathrm{diag}(0.8,\\, 1.2,\\, -0.5)$, and define $A_1 = W_1 \\Lambda_1 W_1^{-1}$.\n  - Initial state $x_0^{(1)} = \\begin{bmatrix} 1 \\\\ -1 \\\\ 2 \\end{bmatrix}$.\n\n- Dataset 2 (includes an oscillatory complex-conjugate pair):\n  - Dimension $n = 3$, number of snapshots $m = 7$.\n  - Choose $W_2 = I_3$ (the $3 \\times 3$ identity), $\\Lambda_2 = \\mathrm{diag}\\!\\left(\\mathrm{e}^{\\mathrm{i}\\pi/6},\\, \\mathrm{e}^{-\\mathrm{i}\\pi/6},\\, 0.9\\right)$, and define $A_2 = \\Lambda_2$.\n  - Initial state $x_0^{(2)} = \\begin{bmatrix} 2 \\\\ 1 \\\\ -1 \\end{bmatrix}$.\n\n- Dataset 3 (rank-deficient snapshots due to unexcited modes):\n  - Dimension $n = 4$, number of snapshots $m = 5$.\n  - Choose $W_3 = I_4$ (the $4 \\times 4$ identity), $\\Lambda_3 = \\mathrm{diag}(0.7,\\, 0.7,\\, 0.3,\\, 1.1)$, and define $A_3 = \\Lambda_3$.\n  - Initial state $x_0^{(3)} = \\begin{bmatrix} 3 \\\\ -2 \\\\ 0 \\\\ 0 \\end{bmatrix}$.\n\nFor each dataset, you must:\n1. Construct $A$, generate $x_k$ via $x_{k+1} = A x_k$ for $k = 0, 1, \\dots, m-2$, and form $X_1$ and $X_2$.\n2. Compute $\\widehat{A}$ by applying the exact Dynamic Mode Decomposition to $(X_1, X_2)$ using the full numerical rank of $X_1$.\n3. Compute $\\varepsilon$ as defined above and compare it to $10^{-12}$ to obtain a boolean result.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with one boolean per dataset in the order given above. For example, a valid output looks like \"[True,True,True]\".", "solution": "The goal is to verify that the exact Dynamic Mode Decomposition (DMD) produces zero reconstruction error for noiseless snapshots generated by a linear time-invariant system when the method uses the full numerical rank of the data. The data construction and the error metric are specified as follows. Given $A \\in \\mathbb{C}^{n \\times n}$, an initial state $x_0 \\in \\mathbb{C}^n$, and an integer $m \\ge 2$, define $x_{k+1} = A x_k$ for $k = 0, \\dots, m-2$, and set\n$$\nX_1 = [x_0, x_1, \\dots, x_{m-2}] \\in \\mathbb{C}^{n \\times (m-1)}, \\quad\nX_2 = [x_1, x_2, \\dots, x_{m-1}] \\in \\mathbb{C}^{n \\times (m-1)}.\n$$\nBy construction, $X_2 = A X_1$.\n\nThe exact DMD operator $\\widehat{A}$ is the data-driven linear map on the column space of $X_1$ that maps each $x_k$ in $X_1$ to $x_{k+1}$ in $X_2$. To construct $\\widehat{A}$ from data, one uses the singular value decomposition (SVD) of $X_1$. Let $X_1$ have the (thin) SVD\n$$\nX_1 = U_r S_r V_r^*,\n$$\nwhere $U_r \\in \\mathbb{C}^{n \\times r}$, $S_r \\in \\mathbb{R}^{r \\times r}$ is diagonal with strictly positive diagonal entries, $V_r \\in \\mathbb{C}^{(m-1) \\times r}$ has orthonormal columns, and $r = \\mathrm{rank}(X_1)$ is the full numerical rank determined from the singular values. The exact DMD operator is then defined by\n$$\n\\widehat{A} = X_2 V_r S_r^{-1} U_r^*.\n$$\nWe now show that this choice guarantees zero reconstruction error in exact arithmetic for noiseless sequential data.\n\nFirst, observe that\n$$\n\\widehat{A} X_1 = X_2 V_r S_r^{-1} U_r^* \\, (U_r S_r V_r^*) = X_2 V_r V_r^*.\n$$\nThe matrix $V_r V_r^*$ is the orthogonal projector in $\\mathbb{C}^{(m-1) \\times (m-1)}$ onto the row space of $X_1$. For sequential noiseless data, $X_2 = A X_1$, which implies that the row space of $X_2$ is contained in the row space of $X_1$ because left multiplication by $A$ forms linear combinations of the rows of $X_1$. Therefore, projecting $X_2$ onto the row space of $X_1$ leaves it unchanged:\n$$\nX_2 V_r V_r^* = X_2.\n$$\nHence,\n$$\n\\widehat{A} X_1 = X_2,\n$$\nwhich implies the reconstruction error defined by\n$$\n\\varepsilon = \\frac{\\lVert X_2 - \\widehat{A} X_1 \\rVert_F}{\\lVert X_2 \\rVert_F}\n$$\nis exactly zero in exact arithmetic.\n\nIn finite-precision arithmetic, numerical roundoff leads to a small nonzero value. Using a strict tolerance such as $10^{-12}$ captures this and validates the theoretical result.\n\nNext, we justify that each dataset in the test suite satisfies the conditions required:\n\n- Dataset 1: $A_1 = W_1 \\Lambda_1 W_1^{-1}$ with $W_1$ invertible and $\\Lambda_1$ diagonal with distinct real eigenvalues. This ensures $A_1$ is diagonalizable. The snapshots are noiseless and satisfy $X_2 = A_1 X_1$. The numerical rank of $X_1$ equals the dimension of the invariant subspace excited by $x_0^{(1)}$, which here will be full due to the choice of $x_0^{(1)}$ and distinct eigenvalues.\n\n- Dataset 2: $A_2 = \\Lambda_2$ with $\\Lambda_2 = \\mathrm{diag}\\!\\left(\\mathrm{e}^{\\mathrm{i}\\pi/6}, \\mathrm{e}^{-\\mathrm{i}\\pi/6}, 0.9\\right)$. This is diagonal over $\\mathbb{C}$ and thus diagonalizable, with an oscillatory complex-conjugate eigenpair and a real decaying eigenvalue. The snapshots are noiseless and generated by $x_{k+1} = A_2 x_k$, hence $X_2 = A_2 X_1$. The numerical rank is detected from the singular values of $X_1$.\n\n- Dataset 3: $A_3 = \\Lambda_3$ with $\\Lambda_3 = \\mathrm{diag}(0.7, 0.7, 0.3, 1.1)$ and $x_0^{(3)} = [3, -2, 0, 0]^T$. Only the first two eigenmodes are excited, so the snapshots lie in a two-dimensional invariant subspace, making $X_1$ rank-deficient (rank equal to $2$) even though $n = 4$. The exact DMD constructed with the full numerical rank $r = 2$ still satisfies $\\widehat{A} X_1 = X_2$ exactly, because the data remain noiseless and $X_2 = A_3 X_1$ with row space of $X_2$ contained in that of $X_1$.\n\nAlgorithmic plan for the program:\n1. For each dataset, construct $A$ as specified, generate $x_k$ iteratively, and form $X_1$ and $X_2$.\n2. Compute the thin SVD of $X_1$ to obtain $U, S, V^*$, determine the numerical rank $r$ using the threshold $S_i  \\tau$ with $\\tau = \\max(n, m-1) \\cdot \\epsilon \\cdot S_1$, where $\\epsilon$ is machine precision.\n3. Truncate to $U_r, S_r, V_r$ and compute $\\widehat{A} = X_2 V_r S_r^{-1} U_r^*$.\n4. Compute $\\varepsilon$ and compare to $10^{-12}$ to produce the boolean result for each dataset.\n5. Output the list of booleans in the required single-line format.\n\nBecause the datasets satisfy the exact DMD conditions, the reconstruction should be numerically zero within the specified tolerance for all three cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef numerical_rank(S, shape, eps=None):\n    \"\"\"\n    Determine the numerical rank based on singular values S and matrix shape.\n    Uses threshold tau = max(shape) * eps * S[0].\n    \"\"\"\n    if eps is None:\n        eps = np.finfo(float).eps\n    if S.size == 0:\n        return 0\n    tau = max(shape) * eps * S[0]\n    return int(np.sum(S  tau))\n\ndef exact_dmd_operator(X1, X2):\n    \"\"\"\n    Compute the exact DMD operator A_hat = X2 V_r S_r^{-1} U_r^* using the full numerical rank.\n    \"\"\"\n    # Compute SVD of X1\n    U, s, Vh = np.linalg.svd(X1, full_matrices=False)\n    # Determine numerical rank\n    r = numerical_rank(s, X1.shape)\n    if r == 0:\n        # Degenerate case: no information\n        return np.zeros((X2.shape[0], X1.shape[0]), dtype=X1.dtype)\n    U_r = U[:, :r]\n    S_r_inv = np.diag(1.0 / s[:r])\n    V_r = Vh.conj().T[:, :r]\n    # Exact DMD operator\n    A_hat = X2 @ V_r @ S_r_inv @ U_r.conj().T\n    return A_hat\n\ndef generate_snapshots(A, x0, m):\n    \"\"\"\n    Generate snapshots x_0, x_1, ..., x_{m-1} using x_{k+1} = A x_k.\n    Returns X1 = [x0 ... x_{m-2}] and X2 = [x1 ... x_{m-1}].\n    \"\"\"\n    n = A.shape[0]\n    X = np.zeros((n, m), dtype=complex)\n    X[:, 0] = x0\n    for k in range(m - 1):\n        X[:, k + 1] = A @ X[:, k]\n    X1 = X[:, :-1]\n    X2 = X[:, 1:]\n    return X1, X2\n\ndef build_dataset_1():\n    # Dataset 1: A = W * Lambda * W^{-1}, real diagonalizable\n    W = np.array([[1, 2, 0],\n                  [0, 1, 1],\n                  [1, 0, 1]], dtype=float)\n    Lambda = np.diag([0.8, 1.2, -0.5])\n    Winv = np.linalg.inv(W)\n    A = (W @ Lambda @ Winv).astype(complex)\n    x0 = np.array([1, -1, 2], dtype=complex)\n    m = 6\n    return A, x0, m\n\ndef build_dataset_2():\n    # Dataset 2: complex conjugate pair and a real eigenvalue\n    lam1 = np.exp(1j * np.pi / 6.0)\n    lam2 = np.exp(-1j * np.pi / 6.0)\n    lam3 = 0.9 + 0j\n    A = np.diag([lam1, lam2, lam3]).astype(complex)\n    x0 = np.array([2, 1, -1], dtype=complex)\n    m = 7\n    return A, x0, m\n\ndef build_dataset_3():\n    # Dataset 3: rank-deficient snapshots (only first two modes excited)\n    A = np.diag([0.7, 0.7, 0.3, 1.1]).astype(complex)\n    x0 = np.array([3, -2, 0, 0], dtype=complex)\n    m = 5\n    return A, x0, m\n\ndef reconstruction_boolean(X1, X2, tol=1e-12):\n    A_hat = exact_dmd_operator(X1, X2)\n    diff = X2 - (A_hat @ X1)\n    num = np.linalg.norm(diff, ord='fro')\n    den = np.linalg.norm(X2, ord='fro')\n    # Handle the degenerate case where X2 is zero matrix\n    rel_err = 0.0 if den == 0.0 else (num / den)\n    return rel_err = tol\n\ndef solve():\n    # Define the test cases from the problem statement.\n    datasets = [\n        build_dataset_1(),\n        build_dataset_2(),\n        build_dataset_3(),\n    ]\n\n    results = []\n    for A, x0, m in datasets:\n        X1, X2 = generate_snapshots(A, x0, m)\n        results.append(reconstruction_boolean(X1, X2, tol=1e-12))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2387371"}, {"introduction": "Real-world sensor data is often contaminated with artifacts that are not part of the underlying physical process. This exercise tackles a common issue: linear sensor drift, which can appear as a non-physical trend in your measurements. You will see firsthand how DMD can misinterpret this drift as a spurious mode with an eigenvalue near $1$, and learn how to apply a crucial pre-processing step—detrending—to remove the artifact and uncover the true system dynamics [@problem_id:3121344].", "problem": "You are given a discrete-time, linear, time-invariant system with state dimension $m = 3$ and state transition matrix $A \\in \\mathbb{R}^{3 \\times 3}$ defined by a similarity transform of a diagonal matrix of stable eigenvalues. The true system evolves according to the fundamental definition of linear time-stepping dynamics $x_{k+1} = A x_k$, and a sequence of state snapshots is collected at integer time indices $k \\in \\{0,1,\\dots,K\\}$ with $K = 200$. The true initial condition is $x_0 \\in \\mathbb{R}^3$. The measurement is corrupted by a constant per-sensor linear drift (a time-ramp) so that the measured snapshots obey the model $x_k^{\\text{meas}} = x_k + d\\,k$, where $d \\in \\mathbb{R}^3$ is a constant drift vector and $k$ is the time index.\n\nDynamic Mode Decomposition (DMD) is a data-driven method that approximates a best-fit linear operator advancing one snapshot to the next by minimizing the least-squares prediction error between consecutive snapshots, and then analyzes the eigenvalues of this operator. You will study how linear drift introduces a spurious mode with an eigenvalue near $1$ and how detrending the time series removes this artifact.\n\nUse the following fully specified, reproducible setup.\n\n- System definition:\n  - Let the similarity matrix be\n    $$P = \\begin{bmatrix} 1  1  0 \\\\ 0  1  1 \\\\ 1  0  1 \\end{bmatrix}, \\quad P^{-1} \\text{ is well-defined.}$$\n  - Let the diagonal of eigenvalues be\n    $$\\Lambda = \\operatorname{diag}(0.9,\\,0.8,\\,0.7).$$\n  - Define the state transition matrix by the fundamental relation\n    $$A = P \\Lambda P^{-1}.$$\n  - Let the true initial condition be\n    $$x_0 = \\begin{bmatrix} 1.2 \\\\ -0.5 \\\\ 0.3 \\end{bmatrix}.$$\n  - Generate true snapshots by iterating the fundamental time-stepping law $x_{k+1} = A x_k$ for $k = 0,1,\\dots,K-1$, thereby forming the snapshot matrix\n    $$X_{\\text{true}} = \\begin{bmatrix} x_0  x_1  \\cdots  x_K \\end{bmatrix} \\in \\mathbb{R}^{3 \\times (K+1)}.$$\n\n- Drift model:\n  - Let the drift direction be the fixed vector\n    $$v = \\begin{bmatrix} 1.0 \\\\ 0.5 \\\\ -0.25 \\end{bmatrix}.$$\n  - For a given nonnegative scalar $d_{\\text{mag}}$, define the drift vector as\n    $$d = d_{\\text{mag}}\\, v.$$\n  - Construct measured snapshots via the model\n    $$x_k^{\\text{meas}} = x_k + d\\,k,$$\n    and form\n    $$X_{\\text{meas}} = \\begin{bmatrix} x_0^{\\text{meas}}  x_1^{\\text{meas}}  \\cdots  x_K^{\\text{meas}} \\end{bmatrix}.$$\n\n- DMD operator and eigenvalues:\n  - Given any snapshot matrix $X \\in \\mathbb{R}^{m \\times (K+1)}$, define the paired data matrices\n    $$X_- = \\begin{bmatrix} x_0  x_1  \\cdots  x_{K-1} \\end{bmatrix}, \\quad X_+ = \\begin{bmatrix} x_1  x_2  \\cdots  x_K \\end{bmatrix}.$$\n  - Let $\\widehat{A}$ denote the best-fit linear operator that advances $X_-$ to $X_+$ in the least-squares sense. Compute the eigenvalues of $\\widehat{A}$ and denote them by the set $\\{\\lambda_j\\}$.\n  - Define the diagnostic metric for “closeness to unity” as\n    $$r(X) = \\min_j \\left| \\lambda_j - 1 \\right|,$$\n    where the modulus is the complex modulus in $\\mathbb{C}$.\n\n- Detrending:\n  - Define per-sensor least-squares linear detrending as follows. For each sensor (row) $i \\in \\{1,2,3\\}$, fit the line $a_i k + b_i$ that minimizes the sum of squared residuals over $k \\in \\{0,1,\\dots,K\\}$ for that sensor’s measured series $\\{x_{k,i}^{\\text{meas}}\\}$. Subtract the fitted trend from the measured series to produce the detrended snapshot matrix $X_{\\text{detrend}}$.\n\nYour tasks:\n\n1. Implement the DMD pipeline described above using only the definitions provided. Specifically:\n   - Construct $A$, generate $X_{\\text{true}}$, and then $X_{\\text{meas}}$ for each specified $d_{\\text{mag}}$.\n   - Compute $r(X_{\\text{meas}})$ and $r(X_{\\text{detrend}})$.\n2. Explain, from first principles, why linear drift $x_k^{\\text{meas}} = x_k + d\\,k$ induces an eigenvalue near $1$ in the DMD spectrum and why detrending removes this spurious mode.\n3. Use the results of item $1$ to evaluate the following pass/fail criteria for each test case.\n\nTest suite (use exactly these values):\n\n- Common parameters: $m = 3$, $K = 200$, $P$, $\\Lambda$, $A$, $x_0$, and $v$ as defined above.\n- Test cases specified by $d_{\\text{mag}}$:\n  - Case $1$: $d_{\\text{mag}} = 0.01$.\n  - Case $2$: $d_{\\text{mag}} = 0$.\n  - Case $3$: $d_{\\text{mag}} = 0.05$.\n\nFor each case $i \\in \\{1,2,3\\}$, let $r_{\\text{before}}^{(i)} = r(X_{\\text{meas}})$ and $r_{\\text{after}}^{(i)} = r(X_{\\text{detrend}})$. Define the boolean $b^{(i)}$ according to:\n\n- If $d_{\\text{mag}}  0$ (drift present): $b^{(i)}$ is true if and only if $r_{\\text{before}}^{(i)}  0.05$, $r_{\\text{after}}^{(i)} \\ge 0.08$, and $r_{\\text{after}}^{(i)} - r_{\\text{before}}^{(i)} \\ge 0.02$.\n- If $d_{\\text{mag}} = 0$ (no drift): $b^{(i)}$ is true if and only if $r_{\\text{before}}^{(i)} \\ge 0.08$ and $r_{\\text{after}}^{(i)} \\ge 0.08$.\n\nFinal output specification:\n\n- Your program must produce a single line of output containing the $9$ results in the following order:\n  $$\\big[r_{\\text{before}}^{(1)},\\, r_{\\text{after}}^{(1)},\\, b^{(1)},\\, r_{\\text{before}}^{(2)},\\, r_{\\text{after}}^{(2)},\\, b^{(2)},\\, r_{\\text{before}}^{(3)},\\, r_{\\text{after}}^{(3)},\\, b^{(3)}\\big].$$\n- All floating-point numbers must be rounded to $6$ decimal places.\n- The output must be a comma-separated list enclosed in square brackets with no spaces (for example, $\\big[0.012345,0.098765,True,\\dots\\big]$).\n\nNote: No physical units are involved. No angles are involved. If any of your internal computations lead to complex numbers, use the complex modulus for $\\left|\\cdot\\right|$ as specified above, and output the real nonnegative distances as floats rounded to $6$ decimals.", "solution": "The problem requires an analysis of Dynamic Mode Decomposition (DMD) applied to data from a linear time-invariant (LTI) system corrupted by linear drift. We must first explain from first principles why this drift introduces a spurious mode with an eigenvalue near $1$, and why detrending removes this artifact. Subsequently, we will implement the specified DMD pipeline to numerically verify these principles and evaluate a set of pass/fail criteria.\n\n### Theoretical Analysis\n\n#### 1. The Dynamic Mode Decomposition (DMD) Framework\n\nDMD is a data-driven method for finding a best-fit linear operator that approximates the evolution of a dynamical system. Given a sequence of state measurements, or \"snapshots,\" $\\{x_k\\}_{k=0}^K$, we form two data matrices:\n$$X_- = \\begin{bmatrix} x_0  x_1  \\cdots  x_{K-1} \\end{bmatrix}, \\quad X_+ = \\begin{bmatrix} x_1  x_2  \\cdots  x_K \\end{bmatrix}$$\nThe DMD algorithm seeks a matrix $\\widehat{A}$ that best propagates the snapshots forward in time in a least-squares sense, i.e., it minimizes $\\|X_+ - \\widehat{A} X_-\\|_F^2$, where $\\|\\cdot\\|_F$ is the Frobenius norm. The solution to this optimization problem is given by:\n$$\\widehat{A} = X_+ X_-^{\\dagger}$$\nwhere $X_-^{\\dagger}$ is the Moore-Penrose pseudoinverse of $X_-$. The eigenvalues and eigenvectors of $\\widehat{A}$ are then used to characterize the dynamics of the system.\n\n#### 2. The Spurious Mode Induced by Linear Drift\n\nThe problem specifies that the true system dynamics are governed by $x_{k+1} = A x_k$. The eigenvalues of the true system matrix $A$ are given as $\\Lambda = \\operatorname{diag}(0.9, 0.8, 0.7)$. Since all eigenvalues have a modulus less than $1$, the system is stable, and the state vector $x_k$ decays to zero as the time index $k \\to \\infty$.\n\nThe measured data, however, is corrupted by a linear drift:\n$$x_k^{\\text{meas}} = x_k + d\\,k$$\nwhere $d$ is a constant drift vector. Let us analyze the effect of this drift on the DMD algorithm. The data matrices for DMD are now formed from the measured snapshots:\n$$X_{\\text{meas},-} = \\begin{bmatrix} x_0^{\\text{meas}}  \\cdots  x_{K-1}^{\\text{meas}} \\end{bmatrix}, \\quad X_{\\text{meas},+} = \\begin{bmatrix} x_1^{\\text{meas}}  \\cdots  x_K^{\\text{meas}} \\end{bmatrix}$$\nThe relationship between consecutive measured snapshots is:\n$$x_{k+1}^{\\text{meas}} = x_{k+1} + d(k+1) = A x_k + dk + d$$\nDMD attempts to find a single operator $\\widehat{A}$ such that $x_{k+1}^{\\text{meas}} \\approx \\widehat{A} x_k^{\\text{meas}}$. Substituting the expressions for the measured states:\n$$A x_k + dk + d \\approx \\widehat{A} (x_k + dk)$$\nAs $k$ becomes large, the true state $x_k$ decays to zero. The drift term $dk$, however, grows linearly. Therefore, for large $k$, the dynamics are dominated by the drift, and the approximation becomes:\n$$dk + d \\approx \\widehat{A}(dk) \\implies (k+1)d \\approx k \\widehat{A}d$$\nAssuming $d \\neq 0$, this implies that $\\widehat{A}d \\approx \\frac{k+1}{k} d$. The DMD operator $\\widehat{A}$ is a single matrix computed over the entire time series. The least-squares fit will be heavily influenced by the large-magnitude snapshots at later times, where the ratio $\\frac{k+1}{k}$ is very close to $1$. Consequently, the algorithm will find an operator $\\widehat{A}$ that has an eigenvector closely aligned with the drift direction $d$, with a corresponding eigenvalue $\\lambda$ very close to $1$. This is the origin of the spurious mode, which does not reflect the true underlying dynamics but rather the measurement artifact. The diagnostic metric $r(X_{\\text{meas}}) = \\min_j |\\lambda_j - 1|$ will therefore be close to zero when drift is present.\n\n#### 3. Removal of the Spurious Mode by Detrending\n\nLinear detrending aims to identify and remove this artifact. For each sensor (row) $i$ of the data matrix $X_{\\text{meas}}$, we perform a linear regression to find a line $y_{k,i} = a_i k + b_i$ that best fits the time series $\\{x_{k,i}^{\\text{meas}}\\}_{k=0}^K$.\nThe data for sensor $i$ is $x_{k,i}^{\\text{meas}} = x_{k,i} + d_i k$. Since $x_{k,i}$ is a sum of decaying exponentials, it acts as a transient term superimposed on the linear trend $d_i k$. For a sufficiently long time series (large $K$), the least-squares fit will accurately estimate the slope, yielding $a_i \\approx d_i$. The intercept $b_i$ will approximate the average value of the transient part $x_{k,i}$.\n\nThe detrended data $x_{k,i}^{\\text{detrend}}$ is obtained by subtracting this fitted trend:\n$$x_{k,i}^{\\text{detrend}} = x_{k,i}^{\\text{meas}} - (a_i k + b_i) \\approx (x_{k,i} + d_i k) - (d_i k + b_i) = x_{k,i} - b_i$$\nIn vector form, the detrended snapshot matrix is approximately $X_{\\text{detrend}} \\approx X_{\\text{true}} - \\mathbf{b}\\mathbf{1}^T$, where $\\mathbf{b}$ is the vector of intercepts and $\\mathbf{1}^T$ is a row vector of ones. The dominant linear drift has been removed.\n\nWhen DMD is applied to the detrended data, the algorithm no longer needs to account for the spurious growing mode. The dynamics it will now try to model are those of $x_k^{\\text{detrend}}$, which closely follow the true system dynamics of $x_k$. The resulting DMD operator, $\\widehat{A}_{\\text{detrend}}$, will therefore have eigenvalues that are good approximations of the true system eigenvalues ($0.9, 0.8, 0.7$). The diagnostic metric $r(X_{\\text{detrend}})$ will now be approximately $\\min(|0.9-1|, |0.8-1|, |0.7-1|) = |0.9-1| = 0.1$. This value is significantly larger than the near-zero value obtained from the raw, drifted data, confirming the removal of the spurious mode.\n\n### Implementation and Verification\n\nThe computational part of the task involves translating these principles into code.\n1.  **System Setup**: The state transition matrix $A$ is constructed as $A = P \\Lambda P^{-1}$.\n2.  **Snapshot Generation**: A true trajectory $X_{\\text{true}}$ is generated by iterating $x_{k+1} = A x_k$. For each test case, the drift vector $d=d_{\\text{mag}}v$ is computed, and the measured snapshot matrix $X_{\\text{meas}}$ is formed by adding the time-ramped drift term $dk$ to each true snapshot $x_k$.\n3.  **DMD on Measured Data**: The matrices $X_{\\text{meas},-}$ and $X_{\\text{meas},+}$ are formed. The DMD operator $\\widehat{A}_{\\text{meas}} = X_{\\text{meas},+} X_{\\text{meas},-}^{\\dagger}$ is computed using the pseudoinverse, and its eigenvalues yield $r_{\\text{before}} = r(X_{\\text{meas}})$.\n4.  **Detrending**: For each row of $X_{\\text{meas}}$, a linear polynomial is fitted against the time indices $k \\in \\{0, \\dots, K\\}$ using least squares. The resulting linear trends are subtracted from each row to produce $X_{\\text{detrend}}$.\n5.  **DMD on Detrended Data**: The same DMD procedure is applied to $X_{\\text{detrend}}$ to compute $\\widehat{A}_{\\text{detrend}}$ and subsequently $r_{\\text{after}} = r(X_{\\text{detrend}})$.\n6.  **Criteria Evaluation**: The computed metrics $r_{\\text{before}}$ and $r_{\\text{after}}$ for each test case are used to evaluate the specified boolean conditions. The numerical results from this pipeline are expected to align with the theoretical analysis presented above.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the DMD problem with linear drift artifact analysis.\n    \"\"\"\n    \n    # Common parameters\n    m = 3\n    K = 200\n    P = np.array([[1., 1., 0.], [0., 1., 1.], [1., 0., 1.]])\n    Lambda_diag = np.array([0.9, 0.8, 0.7])\n    x0 = np.array([1.2, -0.5, 0.3])\n    v = np.array([1.0, 0.5, -0.25])\n\n    # Test cases specified by d_mag\n    test_cases = [\n        0.01,  # Case 1\n        0.0,   # Case 2\n        0.05,  # Case 3\n    ]\n\n    # --- Step 1: System Definition ---\n    Lambda = np.diag(Lambda_diag)\n    try:\n        P_inv = np.linalg.inv(P)\n    except np.linalg.LinAlgError:\n        # This should not happen given the problem statement, but is good practice.\n        return \"Error: Matrix P is singular.\"\n    A = P @ Lambda @ P_inv\n\n    # --- Step 2: Generate True Snapshots ---\n    X_true = np.zeros((m, K + 1))\n    X_true[:, 0] = x0\n    for k in range(K):\n        X_true[:, k + 1] = A @ X_true[:, k]\n\n    def compute_dmd_and_r(X_snapshots):\n        \"\"\"\n        Computes the DMD operator and the metric r(X).\n        \"\"\"\n        X_minus = X_snapshots[:, :-1]\n        X_plus = X_snapshots[:, 1:]\n        \n        # Compute DMD operator A_hat = X_+ * pinv(X_-)\n        A_hat = X_plus @ np.linalg.pinv(X_minus)\n        \n        # Compute eigenvalues of A_hat\n        eigvals = np.linalg.eigvals(A_hat)\n        \n        # Compute the diagnostic metric r(X)\n        r_val = np.min(np.abs(eigvals - 1.0))\n        return r_val\n\n    def detrend_data(X_snapshots):\n        \"\"\"\n        Performs per-sensor least-squares linear detrending.\n        \"\"\"\n        num_sensors, num_snapshots = X_snapshots.shape\n        X_detrended = np.zeros_like(X_snapshots)\n        time_indices = np.arange(num_snapshots)\n        \n        for i in range(num_sensors):\n            # Fit a line (polynomial of degree 1)\n            coeffs = np.polyfit(time_indices, X_snapshots[i, :], 1)\n            # Construct the trend line\n            trend = coeffs[0] * time_indices + coeffs[1]\n            # Subtract the trend\n            X_detrended[i, :] = X_snapshots[i, :] - trend\n            \n        return X_detrended\n\n    results = []\n    \n    # --- Step 3: Process Each Test Case ---\n    for d_mag in test_cases:\n        # Construct measured snapshots with drift\n        d = d_mag * v\n        time_ramp = np.arange(K + 1)\n        drift_matrix = np.outer(d, time_ramp)\n        X_meas = X_true + drift_matrix\n        \n        # Compute r_before (on measured data)\n        r_before = compute_dmd_and_r(X_meas)\n        \n        # Detrend the data\n        X_detrend = detrend_data(X_meas)\n        \n        # Compute r_after (on detrended data)\n        r_after = compute_dmd_and_r(X_detrend)\n        \n        # Evaluate boolean flag b\n        b = False\n        if d_mag  0:\n            if r_before  0.05 and r_after = 0.08 and (r_after - r_before) = 0.02:\n                b = True\n        else: # d_mag == 0\n            if r_before = 0.08 and r_after = 0.08:\n                b = True\n        \n        # Append results for this case\n        results.append(round(r_before, 6))\n        results.append(round(r_after, 6))\n        results.append(b)\n\n    # --- Final Output Formatting ---\n    # Convert all items to string for joining\n    results_str = [str(item) for item in results]\n    print(f\"[{','.join(results_str)}]\")\n\nsolve()\n```", "id": "3121344"}, {"introduction": "The numerical engine of DMD is the Singular Value Decomposition (SVD), but its power comes with a critical caveat regarding stability, especially in the presence of noise. This practice delves into the heart of a robust DMD implementation by exploring the role of singular value truncation. By systematically varying the truncation threshold and observing its effect on the stability of the computed eigenvalues, you will gain a practical intuition for how to regularize the DMD algorithm to ensure reliable and physically meaningful results [@problem_id:3121377].", "problem": "You are tasked with investigating the numerical stability of computing the inverse of singular values in the Dynamic Mode Decomposition (DMD) pipeline and quantifying how truncation of small singular values influences the sensitivity of computed eigenvalues. Work from first principles appropriate to an introduction to computational science course: begin from the least-squares mapping between consecutive state snapshots to define the fitting objective, use the singular value decomposition (SVD) to construct a pseudoinverse, and reason about the instability caused by inverting small singular values. Then implement a truncated pseudoinverse that zeroes out reciprocals below a cutoff threshold and quantify eigenvalue sensitivity as that cutoff varies.\n\nConstruct a synthetic discrete-time linear system of dimension $n=2$ with a true linear propagator $A_{\\mathrm{true}} \\in \\mathbb{R}^{2 \\times 2}$, where\n$$\nA_{\\mathrm{true}}=\\begin{bmatrix}0.96  0\\\\ 0  0.90\\end{bmatrix}.\n$$\nGenerate a sequence of $m=60$ state snapshots $\\{x_k\\}_{k=0}^{m-1}$ via $x_{k+1}=A_{\\mathrm{true}}x_k$ from the initial condition\n$$\nx_0=\\begin{bmatrix}1.0\\\\-0.5\\end{bmatrix}.\n$$\nForm the data matrices\n$$\nX=[x_0, x_1, \\ldots, x_{m-2}],\\quad Y=[x_1, x_2, \\ldots, x_{m-1}].\n$$\nInterpret the DMD linear map $\\hat{A}$ as the least-squares solution to minimizing the Frobenius norm of the residual between $Y$ and a linear map applied to $X$. Use the singular value decomposition to express the pseudoinverse of $X$ and to stabilize the inversion by truncating small singular values. Specifically, if $X=U\\Sigma V^\\top$ with singular values $\\{\\sigma_i\\}$, define a truncated pseudoinverse by inverting only those singular values satisfying $\\sigma_i \\ge \\tau$, and setting the inverse to $0$ otherwise. Construct the reduced DMD operator following the standard projection with this truncated pseudoinverse. Do not construct or use any formula shortcuts that bypass this reasoning.\n\nTo study sensitivity, add independent and identically distributed Gaussian measurement noise with standard deviation $\\epsilon=10^{-4}$ to both $X$ and $Y$ to obtain noisy data $\\tilde{X}$ and $\\tilde{Y}$. Use a fixed random seed $s=2024$ for reproducibility. For each truncation cutoff $\\tau$, compute the DMD eigenvalues from both the clean and the noisy data and measure a scalar sensitivity as the symmetric Hausdorff distance between the two finite sets of eigenvalues in the complex plane. If $S$ and $T$ are two finite sets of complex numbers, define the symmetric Hausdorff distance as\n$$\nH(S,T)=\\max\\Big\\{\\max_{z\\in S}\\min_{w\\in T}|z-w|,\\ \\max_{w\\in T}\\min_{z\\in S}|w-z|\\Big\\},\n$$\nwith the convention that if both $S$ and $T$ are empty, then $H(S,T)=0$.\n\nTo ensure that the cutoffs cover a range from no truncation to overly aggressive truncation, define cutoffs as multiples of the largest singular value of $X$. Let $\\sigma_{\\max}$ denote the largest singular value of $X$. For each multiplier $\\alpha$, set $\\tau=\\alpha\\,\\sigma_{\\max}$. Your test suite is the set of cutoff multipliers\n$$\n\\alpha \\in \\{0,\\ 10^{-8},\\ 10^{-2},\\ 2\\}.\n$$\nThis includes the happy path (near-zero truncation), a moderate truncation, and an edge case with $\\tau$ larger than $\\sigma_{\\max}$.\n\nProgram requirements:\n- Implement the DMD pipeline based on the least-squares formulation and singular value decomposition as described above.\n- Implement the truncated pseudoinverse by zeroing reciprocals of singular values below the cutoff $\\tau$.\n- Compute DMD eigenvalues for both clean and noisy datasets for each $\\tau$ and then compute the symmetric Hausdorff distance $H$ between the two eigenvalue sets.\n- For each $\\alpha$ in the test suite, report a single floating-point sensitivity value $H$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the same order as the provided $\\alpha$ values, with each floating-point number rounded to exactly $6$ decimal places (for example, $[0.123456,0.000001,0.314159,2.718282]$).\n- No other output should be produced.\n\nAngles, if any arise in your implementation, are not required; do not report angles. All quantities are unitless real numbers or complex numbers internally; the final reported values are unitless real numbers as defined by $H$.", "solution": "The problem requires an investigation into the numerical stability of the Dynamic Mode Decomposition (DMD) method, specifically focusing on the effect of truncating small singular values. The solution will be constructed from first principles, beginning with the formulation of DMD as a least-squares problem and culminating in a numerical experiment to quantify eigenvalue sensitivity to noise under varying truncation levels.\n\n### 1. Dynamic Mode Decomposition as a Least-Squares Problem\n\nDynamic Mode Decomposition seeks a linear operator $\\hat{A}$ that best approximates the evolution of a sequence of state snapshots. Given two data matrices, $X = [x_0, x_1, \\ldots, x_{m-2}]$ and $Y = [x_1, x_2, \\ldots, x_{m-1}]$, where each $x_k \\in \\mathbb{R}^n$ is a state vector, we assume a linear relationship $Y \\approx \\hat{A}X$. The optimal operator $\\hat{A}$ is found by minimizing the Frobenius norm of the residual, which is equivalent to solving the linear matrix equation in a least-squares sense:\n$$\n\\min_{\\hat{A} \\in \\mathbb{R}^{n \\times n}} \\|Y - \\hat{A}X\\|_F^2\n$$\nThe solution to this problem is given by $\\hat{A} = YX^+$, where $X^+$ is the Moore-Penrose pseudoinverse of $X$. The eigenvalues and eigenvectors of $\\hat{A}$ are the DMD eigenvalues and modes, respectively.\n\n### 2. SVD-based Truncated Pseudoinverse\n\nDirectly computing the pseudoinverse can be numerically unstable if the matrix $X$ is ill-conditioned (i.e., has a large condition number, the ratio of its largest to smallest singular value). Ill-conditioning often arises in practice from linear dependencies or near-dependencies in the snapshot data, or from measurement noise. The Singular Value Decomposition (SVD) provides a robust method to compute the pseudoinverse and a mechanism to regularize the problem.\n\nLet the SVD of $X \\in \\mathbb{R}^{n \\times (m-1)}$ be:\n$$\nX = U\\Sigma V^\\top\n$$\nwhere $U \\in \\mathbb{R}^{n \\times p}$, $\\Sigma \\in \\mathbb{R}^{p \\times p}$, and $V \\in \\mathbb{R}^{(m-1) \\times p}$ are the truncated SVD matrices, with $p = \\min(n, m-1)$. $U$ and $V$ have orthonormal columns ($U^\\top U = I$, $V^\\top V = I$), and $\\Sigma$ is a diagonal matrix containing the singular values $\\sigma_1 \\ge \\sigma_2 \\ge \\ldots \\ge \\sigma_p \\ge 0$.\n\nThe standard pseudoinverse is $X^+ = V\\Sigma^{-1}U^\\top$, where $\\Sigma^{-1}$ is a diagonal matrix with entries $1/\\sigma_i$. Numerical instability arises when inverting very small singular values $\\sigma_i$, as this magnifies their corresponding modes, which are often associated with noise.\n\nTo stabilize the computation, we introduce a truncated pseudoinverse, $X^+_\\tau$. A rank-$r$ approximation is formed by considering only singular values $\\sigma_i$ that meet or exceed a threshold $\\tau$:\n$$\nr = |\\{i \\mid \\sigma_i \\ge \\tau \\}|\n$$\nThe truncated SVD matrices are $U_r \\in \\mathbb{R}^{n \\times r}$, $\\Sigma_r \\in \\mathbb{R}^{r \\times r}$, and $V_r \\in \\mathbb{R}^{(m-1) \\times r}$. The truncated pseudoinverse is then defined as:\n$$\nX^+_\\tau = V_r \\Sigma_r^{-1} U_r^\\top\n$$\nThis effectively filters out modes associated with singular values below $\\tau$.\n\n### 3. The Reduced-Order DMD Operator\n\nThe full DMD operator $\\hat{A} = YX^+_\\tau$ is an $n \\times n$ matrix, which can be large. A more computationally efficient approach, which captures the essential dynamics on the dominant subspace, is to construct a reduced-order operator, $\\tilde{A}$. The dynamics are projected onto the subspace spanned by the columns of $U_r$, often called the Proper Orthogonal Decomposition (POD) modes. The reduced operator $\\tilde{A}$ is the representation of $\\hat{A}$ in the basis of $U_r$:\n$$\n\\tilde{A} = U_r^\\top \\hat{A} U_r = U_r^\\top (Y X^+_\\tau) U_r\n$$\nSubstituting the expression for $X^+_\\tau$:\n$$\n\\tilde{A} = U_r^\\top (Y V_r \\Sigma_r^{-1} U_r^\\top) U_r\n$$\nSince the columns of $U_r$ are orthonormal ($U_r^\\top U_r = I_r$), the expression simplifies to:\n$$\n\\tilde{A} = U_r^\\top Y V_r \\Sigma_r^{-1}\n$$\nThe eigenvalues of this smaller $r \\times r$ matrix $\\tilde{A}$ are the desired DMD eigenvalues. This formulation is both computationally efficient and numerically preferable.\n\n### 4. Sensitivity Analysis Procedure\n\nThe core of the problem is to quantify how the eigenvalues of $\\tilde{A}$ change when the data is perturbed by noise, as a function of the truncation threshold $\\tau$.\n\n1.  **Data Generation**: A clean dataset $(X, Y)$ is generated from the true linear system $x_{k+1} = A_{\\mathrm{true}}x_k$.\n2.  **Noise Injection**: A noisy dataset $(\\tilde{X}, \\tilde{Y})$ is created by adding independent, identically distributed Gaussian noise with standard deviation $\\epsilon = 10^{-4}$ to $X$ and $Y$. The same random seed $s=2024$ is used for reproducibility.\n3.  **Threshold Calculation**: The truncation threshold $\\tau$ is determined relative to the maximum singular value, $\\sigma_{\\max}$, of the *clean* data matrix $X$, using a multiplier $\\alpha$: $\\tau = \\alpha \\sigma_{\\max}$.\n4.  **Eigenvalue Computation**: For each $\\tau$, two sets of DMD eigenvalues are computed:\n    -   $S_{\\tau}$: Eigenvalues from the clean data $(X, Y)$, using the SVD of $X$ and threshold $\\tau$.\n    -   $T_{\\tau}$: Eigenvalues from the noisy data $(\\tilde{X}, \\tilde{Y})$, using the SVD of $\\tilde{X}$ and the same threshold $\\tau$.\n5.  **Sensitivity Quantification**: The sensitivity for each $\\tau$ is measured by the symmetric Hausdorff distance $H(S_{\\tau}, T_{\\tau})$ between the two sets of complex eigenvalues. For finite sets $S$ and $T$, this is:\n    $$\n    H(S,T)=\\max\\Big\\{\\max_{z\\in S}\\min_{w\\in T}|z-w|,\\ \\max_{w\\in T}\\min_{z\\in S}|w-z|\\Big\\}\n    $$\n\n### 5. Algorithmic Implementation\n\nThe algorithm proceeds as follows for each specified value of $\\alpha$:\n\n1.  Generate the clean data matrices $X$ and $Y$ of size $2 \\times 59$ using the given $A_{\\mathrm{true}}$, $x_0$, and $m=60$.\n2.  Generate the noisy data matrices $\\tilde{X}$ and $\\tilde{Y}$ by adding Gaussian noise with $\\epsilon=10^{-4}$ and seed $s=2024$.\n3.  Compute the SVD of the clean matrix $X$ to find its maximum singular value $\\sigma_{\\max}$.\n4.  For each $\\alpha \\in \\{0, 10^{-8}, 10^{-2}, 2\\}$:\n    a. Calculate the cutoff $\\tau = \\alpha \\sigma_{\\max}$.\n    b. Compute the clean eigenvalues $S_\\tau$ by applying the reduced-order DMD algorithm to $(X, Y)$ with threshold $\\tau$. This involves computing the SVD of $X$, truncating to rank $r$ based on $\\tau$, and finding the eigenvalues of $\\tilde{A}_{\\mathrm{clean}} = U_r^\\top Y V_r \\Sigma_r^{-1}$.\n    c. Compute the noisy eigenvalues $T_\\tau$ by applying the same algorithm to $(\\tilde{X}, \\tilde{Y})$ with the same threshold $\\tau$. This involves computing the SVD of $\\tilde{X}$ for its specific modes.\n    d. If the rank $r$ for a given dataset is $0$, the corresponding set of eigenvalues is empty.\n    e. Calculate the symmetric Hausdorff distance $H(S_\\tau, T_\\tau)$. Note that for $\\alpha=2$, $\\tau$ will exceed all singular values, leading to $r=0$ for both clean and noisy cases. The distance between two empty sets is defined as $0$.\n5.  Collect the computed distances and format them as a comma-separated list rounded to $6$ decimal places.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the DMD pipeline to study eigenvalue sensitivity to noise\n    as a function of singular value truncation.\n    \"\"\"\n    # Define the parameters from the problem statement.\n    A_true = np.array([[0.96, 0.0], [0.0, 0.90]])\n    x0 = np.array([[1.0], [-0.5]])\n    m = 60\n    epsilon = 1e-4\n    seed = 2024\n    alphas = [0.0, 1e-8, 1e-2, 2.0]\n\n    def symmetric_hausdorff(s_set, t_set):\n        \"\"\"\n        Computes the symmetric Hausdorff distance between two sets of complex numbers.\n        \"\"\"\n        s_set = np.asarray(s_set, dtype=np.complex128)\n        t_set = np.asarray(t_set, dtype=np.complex128)\n\n        if s_set.size == 0 and t_set.size == 0:\n            return 0.0\n        \n        # This case (one empty, one not) is not expected for the problem's parameters\n        # but is included for completeness. It represents infinite distance.\n        if s_set.size == 0 or t_set.size == 0:\n            return np.inf\n\n        # Create a matrix of pairwise absolute differences |s_i - t_j|\n        dist_matrix = np.abs(s_set[:, np.newaxis] - t_set[np.newaxis, :])\n\n        # unidirected Hausdorff distance h(S, T) = max_{s in S} min_{t in T} |s-t|\n        h1 = np.max(np.min(dist_matrix, axis=1))\n\n        # unidirected Hausdorff distance h(T, S) = max_{t in T} min_{s in S} |t-s|\n        h2 = np.max(np.min(dist_matrix, axis=0))\n\n        return np.max([h1, h2])\n\n    def compute_dmd_eigs(X_data, Y_data, tau):\n        \"\"\"\n        Computes DMD eigenvalues using a truncated SVD-based pseudoinverse.\n        \"\"\"\n        # Step 1: Compute the SVD of the input data matrix X_data.\n        U, s, Vh = np.linalg.svd(X_data, full_matrices=False)\n\n        # Step 2: Determine the rank for truncation based on the threshold tau.\n        rank = np.sum(s = tau)\n\n        # If rank is 0, there are no eigenvalues.\n        if rank == 0:\n            return np.array([], dtype=np.complex128)\n\n        # Step 3: Truncate the SVD components.\n        Ur = U[:, :rank]\n        sr = s[:rank]\n        # Vh is V-transpose, so Vhr is the first `rank` rows of Vh.\n        Vhr = Vh[:rank, :]\n        Vr = Vhr.T\n\n        # Step 4: Construct the reduced-order DMD operator A_tilde.\n        # A_tilde = U_r^T * Y * V_r * Sigma_r^{-1}\n        # In code: Ur.T @ Y_data @ Vr @ np.diag(1/sr)\n        A_tilde = (Ur.T @ Y_data @ Vr) @ np.diag(1.0 / sr)\n\n        # Step 5: Compute and return the eigenvalues of A_tilde.\n        eigenvalues = np.linalg.eigvals(A_tilde)\n        return eigenvalues\n\n    # Generate the clean data sequence {x_k} from the true linear system.\n    snapshots = [x0]\n    for _ in range(m - 1):\n        snapshots.append(A_true @ snapshots[-1])\n    \n    # Form the clean data matrices X and Y.\n    all_snapshots = np.hstack(snapshots)\n    X = all_snapshots[:, :-1]\n    Y = all_snapshots[:, 1:]\n\n    # Add i.i.d. Gaussian noise to create the noisy data matrices.\n    rng = np.random.default_rng(seed)\n    noise_X = rng.normal(loc=0.0, scale=epsilon, size=X.shape)\n    noise_Y = rng.normal(loc=0.0, scale=epsilon, size=Y.shape)\n    X_tilde = X + noise_X\n    Y_tilde = Y + noise_Y\n\n    # Compute the maximum singular value of the clean data matrix X for reference.\n    _, s_clean, _ = np.linalg.svd(X, full_matrices=False)\n    sigma_max = s_clean[0]\n\n    # Iterate through the specified alpha values to compute sensitivities.\n    sensitivities = []\n    for alpha in alphas:\n        # Define the truncation cutoff threshold tau.\n        tau = alpha * sigma_max\n\n        # Compute DMD eigenvalues for both the clean and noisy datasets.\n        eigs_clean = compute_dmd_eigs(X, Y, tau)\n        eigs_noisy = compute_dmd_eigs(X_tilde, Y_tilde, tau)\n        \n        # Compute the sensitivity as the symmetric Hausdorff distance.\n        h_dist = symmetric_hausdorff(eigs_clean, eigs_noisy)\n        sensitivities.append(h_dist)\n\n    # Format the results according to the specified output format.\n    formatted_results = [f\"{res:.6f}\" for res in sensitivities]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3121377"}]}