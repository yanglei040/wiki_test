{"hands_on_practices": [{"introduction": "Before diving into complex algorithms, it's essential to grasp the fundamental building block they all share: the Krylov subspace. This exercise provides a direct, hands-on opportunity to apply the definition of a Krylov subspace. By generating the sequence of vectors $\\{b, Ab, A^2b, \\dots\\}$, you will construct the basis for the subspace and determine its dimension, a foundational skill for understanding how these methods explore the solution space [@problem_id:2183348].", "problem": "Consider the real matrix $A$ and the real column vector $b$ given by:\n$$A = \\begin{pmatrix} 1  1  0 \\\\ 1  2  1 \\\\ 0  1  1 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 4 \\\\ 1 \\\\ 0 \\end{pmatrix}$$\nThe Krylov subspace of order $m$ generated by $A$ and $b$, denoted as $\\mathcal{K}_m(A, b)$, is the linear subspace spanned by the vectors $\\{b, Ab, A^2b, \\dots, A^{m-1}b\\}$. Determine the dimension of the Krylov subspace $\\mathcal{K}_3(A, b)$.", "solution": "The Krylov subspace $\\mathcal{K}_{3}(A,b)$ is the span of $\\{b, Ab, A^{2}b\\}$. To determine its dimension, we compute these vectors and check their linear independence.\nGiven\n$$\nA=\\begin{pmatrix}110\\\\121\\\\011\\end{pmatrix},\\quad b=\\begin{pmatrix}4\\\\1\\\\0\\end{pmatrix},\n$$\nwe compute\n$$\nAb=A\\begin{pmatrix}4\\\\1\\\\0\\end{pmatrix}=\\begin{pmatrix}1\\cdot 4+1\\cdot 1+0\\cdot 0\\\\1\\cdot 4+2\\cdot 1+1\\cdot 0\\\\0\\cdot 4+1\\cdot 1+1\\cdot 0\\end{pmatrix}=\\begin{pmatrix}5\\\\6\\\\1\\end{pmatrix}.\n$$\nNext,\n$$\nA^{2}b=A(Ab)=A\\begin{pmatrix}5\\\\6\\\\1\\end{pmatrix}=\\begin{pmatrix}1\\cdot 5+1\\cdot 6+0\\cdot 1\\\\1\\cdot 5+2\\cdot 6+1\\cdot 1\\\\0\\cdot 5+1\\cdot 6+1\\cdot 1\\end{pmatrix}=\\begin{pmatrix}11\\\\18\\\\7\\end{pmatrix}.\n$$\nForm the matrix with these vectors as columns:\n$$\nK=\\begin{pmatrix}4511\\\\1618\\\\017\\end{pmatrix}.\n$$\nThe vectors are linearly independent if and only if $\\det(K)\\neq 0$. Compute\n$$\n\\det(K)=4\\begin{vmatrix}618\\\\17\\end{vmatrix}-5\\begin{vmatrix}118\\\\07\\end{vmatrix}+11\\begin{vmatrix}16\\\\01\\end{vmatrix}\n=4(42-18)-5(7)+11(1)=96-35+11=72\\neq 0.\n$$\nTherefore, $\\{b, Ab, A^{2}b\\}$ is linearly independent, and the dimension of $\\mathcal{K}_{3}(A,b)$ is $3$.", "answer": "$$\\boxed{3}$$", "id": "2183348"}, {"introduction": "With an understanding of what a Krylov subspace is, we can now see how an algorithm like the Conjugate Gradient (CG) method puts it to work. Each iteration of CG finds an optimal approximate solution within the growing Krylov subspace. This practice problem focuses on the very first, critical step of the process: calculating the optimal step size $\\alpha_0$ that minimizes error along the initial search direction [@problem_id:2183332]. Mastering this calculation provides insight into the per-iteration optimization that makes CG so powerful for symmetric positive-definite systems.", "problem": "The Conjugate Gradient (CG) method is an iterative algorithm for numerically solving a linear system of equations $Ax = b$, where the matrix $A$ is Symmetric Positive-Definite (SPD). The method generates a sequence of approximations $x_k$ that converges to the true solution.\n\nConsider the $2 \\times 2$ linear system $Ax = b$ defined by:\n$$\nA = \\begin{pmatrix} 3  2 \\\\ 2  3 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\n$$\nThe first step of the CG algorithm is to compute an improved solution $x_1$ starting from an initial guess $x_0$. The update is given by the formula $x_1 = x_0 + \\alpha_0 p_0$, where $p_0$ is the initial search direction and $\\alpha_0$ is the optimal step size that minimizes the error along this direction.\n\nGiven the initial guess $x_0 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$, determine the value of the optimal step size $\\alpha_0$ for the first iteration of the Conjugate Gradient method.\n\nExpress your answer as a fraction in simplest form.", "solution": "For the Conjugate Gradient method with an SPD matrix, the initial search direction is $p_{0} = r_{0}$, where the residual is $r_{0} = b - A x_{0}$. The optimal step size along $p_{0}$ that minimizes the quadratic $f(x) = \\frac{1}{2} x^{T} A x - b^{T} x$ is obtained by minimizing $f(x_{0} + \\alpha p_{0})$, which yields\n$$\n\\alpha_{0} = \\frac{p_{0}^{T} r_{0}}{p_{0}^{T} A p_{0}}.\n$$\nWith $p_{0} = r_{0}$, this becomes\n$$\n\\alpha_{0} = \\frac{r_{0}^{T} r_{0}}{r_{0}^{T} A r_{0}}.\n$$\n\nCompute the residual using $x_{0} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$:\n$$\nA x_{0} = \\begin{pmatrix} 3  2 \\\\ 2  3 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 5 \\\\ 5 \\end{pmatrix}, \\quad\nr_{0} = b - A x_{0} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 5 \\\\ 5 \\end{pmatrix} = \\begin{pmatrix} -4 \\\\ -5 \\end{pmatrix}.\n$$\nThen\n$$\nr_{0}^{T} r_{0} = (-4)^{2} + (-5)^{2} = 16 + 25 = 41,\n$$\nand\n$$\nA r_{0} = \\begin{pmatrix} 3  2 \\\\ 2  3 \\end{pmatrix} \\begin{pmatrix} -4 \\\\ -5 \\end{pmatrix} = \\begin{pmatrix} -22 \\\\ -23 \\end{pmatrix}, \\quad\nr_{0}^{T} A r_{0} = \\begin{pmatrix} -4  -5 \\end{pmatrix} \\begin{pmatrix} -22 \\\\ -23 \\end{pmatrix} = 88 + 115 = 203.\n$$\nTherefore,\n$$\n\\alpha_{0} = \\frac{41}{203}.\n$$\nThe fraction $\\frac{41}{203}$ is already in simplest form.", "answer": "$$\\boxed{\\frac{41}{203}}$$", "id": "2183332"}, {"introduction": "The theoretical efficiency of Krylov methods translates into practical performance, but this performance is not always the same; it is deeply tied to the spectral properties (the eigenvalues) of the system matrix $A$. This advanced practice transitions from manual calculation to computational experimentation, a core activity in modern scientific computing. You are tasked with designing and implementing a numerical experiment to demonstrate firsthand how clustering the eigenvalues of a matrix can dramatically accelerate the convergence of the Conjugate Gradient method, offering a tangible confirmation of the method's theoretical foundations [@problem_id:3149643].", "problem": "You are asked to design and execute a numerical experiment to demonstrate, from first principles, how eigenvalue clustering of a symmetric positive definite (SPD) matrix accelerates the Conjugate Gradient (CG) method, a Krylov subspace method. The experiment must be implemented as a complete, runnable program. Your program must construct matrices of the form $A = Q \\,\\operatorname{diag}(\\{\\lambda_i\\}_{i=1}^n)\\, Q^\\top$, where $Q$ is an orthogonal matrix and the eigenvalues $\\{\\lambda_i\\}$ are chosen to produce different spectral clustering patterns. You must track the Euclidean two-norm of the residual sequence $\\{\\lVert r_k \\rVert_2\\}_{k=0,1,\\dots}$, where $r_k = b - A x_k$, generated by the Conjugate Gradient (CG) method, and use this to quantify convergence.\n\nBegin from the following core definitions and well-tested facts:\n- A real matrix $A \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite (SPD) if $A^\\top = A$ and $x^\\top A x  0$ for all nonzero $x \\in \\mathbb{R}^n$.\n- The Conjugate Gradient (CG) method is a Krylov subspace (KS) method that, for SPD $A$, produces iterates $x_k \\in x_0 + \\mathcal{K}_k(A,r_0)$ with residuals $r_k = b - A x_k$, where the $k$th Krylov subspace is $\\mathcal{K}_k(A,r_0) = \\operatorname{span}\\{r_0, A r_0, A^2 r_0, \\dots, A^{k-1} r_0\\}$.\n- There exists a degree-$k$ polynomial $p_k$ with $p_k(0) = 1$ such that $r_k = p_k(A)\\, r_0$. Hence, the convergence of CG depends on how well a polynomial of degree $k$ can be made small on the spectrum of $A$, that is, on the set $\\{\\lambda_i\\}$.\n\nYour program must do the following:\n1. Fix a dimension $n = 120$, a maximum iteration count $K_{\\max} = 50$, and a tolerance $\\text{tol} = 10^{-10}$. Use the initial guess $x_0 = 0$ and a fixed right-hand side $b \\in \\mathbb{R}^n$ with independent standard normal entries, generated using a fixed seed $s_b = 27182$. Construct a single orthogonal matrix $Q \\in \\mathbb{R}^{n \\times n}$ using a fixed seed $s_Q = 31415$ by generating a Gaussian random matrix, applying a $QR$ decomposition, and adjusting column signs so that $Q$ is orthogonal (this procedure approximates a draw from the Haar distribution on the orthogonal group).\n2. For each test case below, construct $A = Q \\,\\operatorname{diag}(\\{\\lambda_i\\})\\, Q^\\top$ with the specified eigenvalues. Run Conjugate Gradient (CG) for at most $K_{\\max}$ iterations, tracking the sequence of relative residual norms $\\rho_k = \\lVert r_k \\rVert_2 / \\lVert r_0 \\rVert_2$ for $k = 0,1,\\dots,K_{\\max}$. Define the convergence iteration count for a test case to be the smallest integer $k \\in \\{1,2,\\dots,K_{\\max}\\}$ such that $\\rho_k \\le \\text{tol}$. If no such $k$ exists up to $K_{\\max}$, declare the count as $K_{\\max} + 1$.\n3. Use the same $Q$ and the same $b$ for all test cases to isolate the effect of the eigenvalue distribution.\n\nTest suite (four matrices, designed to cover a happy path, a spread spectrum, a two-cluster case, and a boundary case):\n- Case $1$ (tight cluster with two outliers):\n  - Dimension $n = 120$.\n  - Eigenvalues: $116$ copies of $1.0$, $2$ copies of $5.0$, and $2$ copies of $20.0$.\n- Case $2$ (uniformly spread spectrum):\n  - Dimension $n = 120$.\n  - Eigenvalues: $120$ distinct values linearly spaced in the closed interval $\\left[0.1,\\,20.0\\right]$.\n- Case $3$ (two tight clusters):\n  - Dimension $n = 120$.\n  - Eigenvalues: $60$ copies of $0.5$ and $60$ copies of $5.0$.\n- Case $4$ (boundary case: perfectly clustered):\n  - Dimension $n = 120$.\n  - Eigenvalues: $120$ copies of $3.0$.\n\nImplementation requirements:\n- Implement CG using only the three-term recurrence based on residuals and search directions, with $x_0 = 0$, $r_0 = b$, $p_0 = r_0$, the step length $\\alpha_k = \\dfrac{r_k^\\top r_k}{p_k^\\top A p_k}$, update $x_{k+1} = x_k + \\alpha_k p_k$, $r_{k+1} = r_k - \\alpha_k A p_k$, and direction update $p_{k+1} = r_{k+1} + \\beta_{k+1} p_k$ with $\\beta_{k+1} = \\dfrac{r_{k+1}^\\top r_{k+1}}{r_k^\\top r_k}$.\n- Ensure $A$ is exactly symmetric by construction.\n\nFinal output specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must have four integers, one per test case, in the order listed above. For each test case, output the convergence iteration count defined above. For example, the output must look like\n  - Example format: $[\\text{case1\\_count},\\text{case2\\_count},\\text{case3\\_count},\\text{case4\\_count}]$.\n- No additional text may be printed.\n\nAngle units are not involved, and there are no physical units. All numeric answers must be reported as integers according to the rule specified above. No user input is required; the program must be self-contained and deterministic under the given seeds.", "solution": "We begin from the fundamental properties of the Conjugate Gradient (CG) method on symmetric positive definite (SPD) matrices and the structure of Krylov subspaces. Let $A \\in \\mathbb{R}^{n \\times n}$ be SPD, $b \\in \\mathbb{R}^n$, and an initial guess $x_0 \\in \\mathbb{R}^n$ be given. The residual is $r_k = b - A x_k$. The $k$th iterate of CG satisfies $x_k \\in x_0 + \\mathcal{K}_k(A,r_0)$ where $\\mathcal{K}_k(A,r_0) = \\operatorname{span}\\{r_0, A r_0, \\dots, A^{k-1} r_0\\}$. There exists a polynomial $p_k$ of degree $k$ with $p_k(0) = 1$ such that $r_k = p_k(A)\\, r_0$. This polynomial representation reveals that CG convergence is governed by how small a degree-$k$ polynomial constrained by $p_k(0)=1$ can be made on the spectrum $\\sigma(A) = \\{\\lambda_i\\}_{i=1}^n$.\n\nIf the spectrum is clustered, then there exist polynomials of relatively low degree that are simultaneously small over those clusters. In the extreme case where $A$ has exactly $m$ distinct eigenvalues, there exists a polynomial $q$ of degree $m$ that interpolates $q(\\lambda^{(j)}) = 0$ for each distinct eigenvalue $\\lambda^{(j)}$ and satisfies $q(0) = 1$. Then $q(A)$ is the zero operator on the invariant subspaces corresponding to these eigenvalues, and in exact arithmetic CG terminates in at most $m$ steps with $r_m = 0$. This principle is a foundational reason why eigenvalue clustering accelerates CG.\n\nBy contrast, if eigenvalues are spread out across a wide interval with many distinct values, the best degree-$k$ polynomial constrained by $p_k(0)=1$ cannot be made uniformly small over the entire spread unless $k$ is large. A classic consequence is the Chebyshev-based bound (in the $A$-norm of the error) involving the condition number $\\kappa(A)$, where the factor $\\left(\\frac{\\sqrt{\\kappa}-1}{\\sqrt{\\kappa}+1}\\right)^k$ controls the decay rate; broad spectral spread typically increases $\\kappa(A)$ and slows convergence.\n\nAlgorithmic design for the experiment:\n1. We construct matrices as $A = Q \\,\\operatorname{diag}(\\{\\lambda_i\\})\\, Q^\\top$ where $Q$ is orthogonal. We use one fixed $Q$ for all tests to isolate spectral effects. The orthogonal matrix $Q$ is generated by creating a Gaussian random matrix and applying a $QR$ decomposition with column-sign normalization so that $Q^\\top Q = I$. This method approximates a Haar-distributed orthogonal matrix.\n2. We generate $b$ as a standard normal random vector to avoid bias toward any particular eigensubspace, and fix the seed for reproducibility. We use $x_0 = 0$, so $r_0 = b$.\n3. We implement CG using the three-term recurrence:\n   - Initialize $x_0 = 0$, $r_0 = b$, $p_0 = r_0$, $\\rho_0 = \\lVert r_0 \\rVert_2 / \\lVert r_0 \\rVert_2 = 1$.\n   - For $k = 0,1,\\dots,K_{\\max}-1$:\n     - Compute $Ap_k = A p_k$, $\\alpha_k = \\dfrac{r_k^\\top r_k}{p_k^\\top A p_k}$.\n     - Update $x_{k+1} = x_k + \\alpha_k p_k$, $r_{k+1} = r_k - \\alpha_k Ap_k$.\n     - Compute $\\rho_{k+1} = \\lVert r_{k+1} \\rVert_2 / \\lVert r_0 \\rVert_2$.\n     - If $\\rho_{k+1} \\le \\text{tol}$, record the convergence iteration count as $k+1$ and stop for that case.\n     - Otherwise compute $\\beta_{k+1} = \\dfrac{r_{k+1}^\\top r_{k+1}}{r_k^\\top r_k}$ and set $p_{k+1} = r_{k+1} + \\beta_{k+1} p_k$.\n   - If no $\\rho_k$ meets the tolerance by $K_{\\max}$, define the count as $K_{\\max}+1$.\n4. Test suite design:\n   - Case $1$ uses $116$ copies of $\\lambda = 1.0$ and two outliers at $\\lambda = 5.0$ and $\\lambda = 20.0$ (two copies each). There are only $3$ distinct eigenvalues, so in exact arithmetic CG terminates in at most $3$ steps. We therefore expect a very small iteration count.\n   - Case $2$ uses $120$ distinct eigenvalues uniformly spread on $\\left[0.1, 20.0\\right]$. The spread is wide, and many distinct values imply that CG requires many iterations to reduce the residual, so we expect no convergence to $\\text{tol} = 10^{-10}$ within $K_{\\max} = 50$, yielding the count $51$.\n   - Case $3$ uses a two-cluster spectrum with $60$ copies at $\\lambda = 0.5$ and $60$ copies at $\\lambda = 5.0$. There are $2$ distinct eigenvalues, so in exact arithmetic CG terminates in at most $2$ steps. We expect a count of about $2$.\n   - Case $4$ is the boundary case with $120$ copies at $\\lambda = 3.0$. There is $1$ distinct eigenvalue; CG terminates in $1$ step in exact arithmetic. Numerically, with double precision, the relative residual should drop well below $\\text{tol} = 10^{-10}$ after one step.\n\nGiven the fixed seeds, $n = 120$, the tolerance $\\text{tol} = 10^{-10}$, and $K_{\\max} = 50$, the program deterministically computes the smallest $k$ such that $\\rho_k \\le \\text{tol}$ for each case. Based on the spectral reasoning above, the expected outputs are integers emphasizing the acceleration due to clustering. Specifically, we anticipate the result\n- Case $1$: $3$,\n- Case $2$: $51$,\n- Case $3$: $2$,\n- Case $4$: $1$.\n\nThe final program produces a single line with four comma-separated integers in brackets, in the order of the cases, that is, $[3,51,2,1]$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef random_orthogonal(n: int, seed: int) - np.ndarray:\n    rng = np.random.default_rng(seed)\n    G = rng.standard_normal((n, n))\n    # QR decomposition\n    Q, R = np.linalg.qr(G)\n    # Normalize signs to ensure deterministic orientation\n    d = np.sign(np.diag(R))\n    d[d == 0] = 1.0\n    Q = Q @ np.diag(d)\n    return Q\n\ndef build_spd_from_spectrum(eigs: np.ndarray, Q: np.ndarray) - np.ndarray:\n    D = np.diag(eigs)\n    A = Q @ D @ Q.T\n    # Symmetrize to counteract tiny numerical asymmetry\n    A = 0.5 * (A + A.T)\n    return A\n\ndef conjugate_gradient(A: np.ndarray, b: np.ndarray, kmax: int, tol: float):\n    n = A.shape[0]\n    x = np.zeros(n)\n    r = b.copy()\n    p = r.copy()\n    rTr = float(r @ r)\n    r0_norm = np.sqrt(rTr)\n    rel_residuals = [1.0]  # k=0\n    if r0_norm == 0.0:\n        return rel_residuals, 0  # Already converged\n    for k in range(kmax):\n        Ap = A @ p\n        denom = float(p @ Ap)\n        # For SPD, denom should be positive\n        if denom = 0:\n            # Breakdown (should not happen for SPD), stop\n            return rel_residuals, kmax + 1\n        alpha = rTr / denom\n        x = x + alpha * p\n        r = r - alpha * Ap\n        r_new_Tr = float(r @ r)\n        rel = np.sqrt(r_new_Tr) / r0_norm\n        rel_residuals.append(rel)\n        if rel = tol:\n            return rel_residuals, (k + 1)\n        beta = r_new_Tr / rTr\n        p = r + beta * p\n        rTr = r_new_Tr\n    return rel_residuals, (kmax + 1)\n\ndef solve():\n    # Common parameters\n    n = 120\n    K_max = 50\n    tol = 1e-10\n    seed_Q = 31415\n    seed_b = 27182\n\n    # Construct a single orthogonal Q\n    Q = random_orthogonal(n, seed_Q)\n\n    # Right-hand side b\n    rng_b = np.random.default_rng(seed_b)\n    b = rng_b.standard_normal(n)\n\n    # Test cases (eigenvalue spectra)\n    # Case 1: 116 x 1.0, 2 x 5.0, 2 x 20.0\n    eigs_case1 = np.array([1.0]*116 + [5.0]*2 + [20.0]*2, dtype=float)\n\n    # Case 2: 120 distinct values in [0.1, 20.0]\n    eigs_case2 = np.linspace(0.1, 20.0, n, dtype=float)\n\n    # Case 3: 60 x 0.5, 60 x 5.0\n    eigs_case3 = np.array([0.5]*60 + [5.0]*60, dtype=float)\n\n    # Case 4: 120 x 3.0\n    eigs_case4 = np.array([3.0]*n, dtype=float)\n\n    spectra = [eigs_case1, eigs_case2, eigs_case3, eigs_case4]\n\n    results = []\n    for eigs in spectra:\n        A = build_spd_from_spectrum(eigs, Q)\n        _, iters = conjugate_gradient(A, b, K_max, tol)\n        results.append(int(iters))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3149643"}]}