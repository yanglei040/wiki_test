{"hands_on_practices": [{"introduction": "A crucial skill in computational modeling is knowing when a simple model can effectively replace a more complex one. This practice explores the boundary between discrete stochastic processes and their continuous deterministic approximations, a common scenario in fields like physics and biology. By analyzing photon counts in a microscope, you will derive a quantitative criterion based on the coefficient of variation to determine when a simple intensity model is a valid substitute for a more detailed Poisson process, a fundamental concept for simplifying models while maintaining accuracy [@problem_id:3160694].", "problem": "A single pixel in a fluorescence microscope records photon arrivals over an exposure window $\\left[0, T\\right]$. The discrete random variable $N$ counting the number of detected photons is modeled as a Poisson random variable with parameter $\\lambda(T)$, where the Poisson parameter is the expected number of arrivals in the window. The instrument is illuminated by a light source with deterministic, continuous photon flux (intensity) $I(t)$ (photons per second arriving at the pixel), and the detector has quantum efficiency $\\eta$ (the probability that an arriving photon is detected). The instantaneous arrival rate is therefore $r(t) = \\eta I(t)$, and the expected count over an exposure is $\\lambda(T) = \\int_{0}^{T} r(t)\\, dt = \\eta \\int_{0}^{T} I(t)\\, dt$. The microscope vendor suggests using a continuous deterministic model for the signal when the coefficient of variation (CV), defined as $\\mathrm{CV} = \\sqrt{\\mathrm{Var}(N)}/\\mathbb{E}[N]$, is below a tolerance $\\varepsilon$.\n\nClassify the model for $N$ and the model for $I(t)$ according to the categories discrete versus continuous and deterministic versus stochastic. Then, starting only from the foundational properties of the Poisson process that $\\mathbb{E}[N] = \\lambda(T)$ and $\\mathrm{Var}(N) = \\lambda(T)$, and the given definition of the coefficient of variation, derive a criterion on the exposure time $T$ under which the continuous deterministic approximation is acceptable, and compute the minimal exposure time $T_{\\min}$ that meets this criterion for the following operating point:\n- The flux is constant over the exposure, $I(t) = I_{0}$ with $I_{0} = 8.0 \\times 10^{4}$ photons per second.\n- The detector efficiency is $\\eta = 0.60$.\n- The tolerance is $\\varepsilon = 0.020$.\nRound your final numerical answer for $T_{\\min}$ to three significant figures. Express the final time in seconds.", "solution": "The problem will be addressed in two parts as requested: first, the classification of the models for the photon count $N$ and the incident flux $I(t)$; second, the derivation of the criterion for the exposure time $T$ and the calculation of the minimal exposure time $T_{\\min}$.\n\nFirst, we validate the problem statement.\n**Step 1: Extract Givens**\n- Exposure window: $\\left[0, T\\right]$\n- Photon count: $N$, a discrete random variable\n- Model for $N$: Poisson random variable with parameter $\\lambda(T)$\n- Photon flux: $I(t)$, a deterministic, continuous function\n- Detector quantum efficiency: $\\eta$\n- Instantaneous arrival rate: $r(t) = \\eta I(t)$\n- Expected count (Poisson parameter): $\\lambda(T) = \\int_{0}^{T} r(t)\\, dt = \\eta \\int_{0}^{T} I(t)\\, dt$\n- Condition for continuous deterministic approximation: Coefficient of variation $\\mathrm{CV} < \\varepsilon$\n- Definition of coefficient of variation: $\\mathrm{CV} = \\sqrt{\\mathrm{Var}(N)}/\\mathbb{E}[N]$\n- Foundational properties of Poisson distribution for $N$: $\\mathbb{E}[N] = \\lambda(T)$ and $\\mathrm{Var}(N) = \\lambda(T)$\n- Operating point parameters:\n    - Constant flux: $I(t) = I_{0}$ with $I_{0} = 8.0 \\times 10^{4}$ photons per second\n    - Detector efficiency: $\\eta = 0.60$\n    - Tolerance: $\\varepsilon = 0.020$\n- Requirement: Round the final numerical answer for $T_{\\min}$ to three significant figures.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, describing a standard Poisson process model for photon detection, which is fundamental in optics and imaging. All terms are clearly defined and the relationships between them ($\\lambda(T)$, $r(t)$, $I(t)$) are physically correct. The problem is well-posed, providing all necessary information to derive the criterion and compute the final value. The language is objective and unambiguous. The provided numerical values are physically plausible. The problem is therefore deemed **valid**.\n\n**Part 1: Model Classification**\nThe problem statement provides the classifications directly.\n- The model for the number of detected photons, $N$, is described as a \"discrete random variable\". This means the state space of $N$ is discrete (the non-negative integers $\\{0, 1, 2, ...\\}$) and its evolution is stochastic (governed by a probability distribution, specifically Poisson). Therefore, the model for $N$ is **discrete and stochastic**.\n- The model for the photon flux, $I(t)$, is described as a \"deterministic, continuous photon flux\". This means its state space is continuous (the non-negative real numbers) and its evolution is deterministic (its value is known with certainty at any time $t$). Therefore, the model for $I(t)$ is **continuous and deterministic**.\n\n**Part 2: Derivation and Calculation of Minimal Exposure Time**\nThe condition for the continuous deterministic approximation to be acceptable is that the coefficient of variation, $\\mathrm{CV}$, is below a tolerance $\\varepsilon$.\n$$\n\\mathrm{CV} < \\varepsilon\n$$\nSubstituting the definition of $\\mathrm{CV}$:\n$$\n\\frac{\\sqrt{\\mathrm{Var}(N)}}{\\mathbb{E}[N]} < \\varepsilon\n$$\nThe problem states that $N$ follows a Poisson distribution with parameter $\\lambda(T)$. For a Poisson random variable, the expected value and the variance are both equal to the parameter:\n$$\n\\mathbb{E}[N] = \\lambda(T)\n$$\n$$\n\\mathrm{Var}(N) = \\lambda(T)\n$$\nSubstituting these properties into the inequality gives:\n$$\n\\frac{\\sqrt{\\lambda(T)}}{\\lambda(T)} < \\varepsilon\n$$\nSince $\\lambda(T)$ must be positive for a non-zero signal, we can simplify this to:\n$$\n\\frac{1}{\\sqrt{\\lambda(T)}} < \\varepsilon\n$$\nBecause both sides are positive, we can square the inequality without changing its direction after inverting:\n$$\n\\sqrt{\\lambda(T)} > \\frac{1}{\\varepsilon} \\implies \\lambda(T) > \\frac{1}{\\varepsilon^2}\n$$\nNow we use the definition of $\\lambda(T)$ for the specific case of a constant flux $I(t) = I_0$:\n$$\n\\lambda(T) = \\eta \\int_{0}^{T} I(t)\\, dt = \\eta \\int_{0}^{T} I_0\\, dt = \\eta I_0 T\n$$\nSubstituting this into our derived inequality provides the criterion on the exposure time $T$:\n$$\n\\eta I_0 T > \\frac{1}{\\varepsilon^2}\n$$\nTo find the minimal exposure time, $T_{\\min}$, we solve for the boundary condition of this inequality:\n$$\n\\eta I_0 T_{\\min} = \\frac{1}{\\varepsilon^2}\n$$\n$$\nT_{\\min} = \\frac{1}{\\eta I_0 \\varepsilon^2}\n$$\nNow we substitute the given numerical values:\n- $\\eta = 0.60$\n- $I_{0} = 8.0 \\times 10^{4} \\, \\text{s}^{-1}$\n- $\\varepsilon = 0.020 = 2.0 \\times 10^{-2}$\n\nFirst, we calculate the denominator:\n$$\n\\varepsilon^2 = (2.0 \\times 10^{-2})^2 = 4.0 \\times 10^{-4}\n$$\n$$\n\\eta I_0 = (0.60) \\times (8.0 \\times 10^{4} \\, \\text{s}^{-1}) = 4.8 \\times 10^{4} \\, \\text{s}^{-1}\n$$\n$$\n\\eta I_0 \\varepsilon^2 = (4.8 \\times 10^{4} \\, \\text{s}^{-1}) \\times (4.0 \\times 10^{-4}) = 19.2 \\, \\text{s}^{-1}\n$$\nFinally, we calculate $T_{\\min}$:\n$$\nT_{\\min} = \\frac{1}{19.2 \\, \\text{s}^{-1}} \\approx 0.0520833... \\, \\text{s}\n$$\nThe problem requires the answer to be rounded to three significant figures. The first three significant digits are $5$, $2$, and $0$. The fourth digit is $8$, so we round the third digit up.\n$$\nT_{\\min} \\approx 0.0521 \\, \\text{s}\n$$\nExpressed in scientific notation, this is $5.21 \\times 10^{-2} \\, \\text{s}$.", "answer": "$$\n\\boxed{5.21 \\times 10^{-2}}\n$$", "id": "3160694"}, {"introduction": "Once we have collected data, how do we classify the underlying process that generated it? This exercise provides a hands-on computational method to classify event streams as deterministic, Poisson-like, or having more complex stochastic behavior. You will implement a statistical test based on the Fano factor, a powerful metric that compares the variance of a process to its mean, and use a parametric bootstrap to build confidence in your classification, giving you a robust tool for data-driven model identification [@problem_id:3160668].", "problem": "You are given event count streams observed over equal-duration windows, construed as realizations of a discrete counting process $N(t)$. The goal is to design and implement a decision test grounded in the Fano factor to classify each stream into one of three model classes: deterministic rate (replace variability by a constant rate), stochastic Poisson (consistent with Poisson variability), or stochastic non-Poisson (variability that deviates from Poisson). The test must be based on the Fano factor defined by $F = \\mathrm{Var}(N)/\\mathbb{E}[N]$, alongside core statistical definitions and properties.\n\nFundamental base:\n- Expectation $\\mathbb{E}[N]$ and variance $\\mathrm{Var}(N)$ are defined via $\\mathbb{E}[N] = \\lim_{m \\to \\infty} \\frac{1}{m} \\sum_{i=1}^{m} N_i$ and $\\mathrm{Var}(N) = \\mathbb{E}\\left[(N - \\mathbb{E}[N])^2\\right]$, where $N_i$ is the count in window $i$ and $m$ is the number of windows.\n- For a Poisson process with rate $\\lambda$, the distribution of counts in any fixed window is Poisson with $\\mathbb{P}(N=k) = \\frac{e^{-\\lambda} \\lambda^k}{k!}$, mean $\\mathbb{E}[N] = \\lambda$, and variance $\\mathrm{Var}(N) = \\lambda$, which implies $F = 1$.\n- For a deterministic process with constant count per window, $\\mathrm{Var}(N) = 0$, which implies $F = 0$.\n- The coefficient of variation is defined as $\\mathrm{CV} = \\sqrt{\\mathrm{Var}(N)}/\\mathbb{E}[N]$, which quantifies relative fluctuations. Small $\\mathrm{CV}$ indicates that fluctuations are negligible compared to the mean, making deterministic replacement reasonable.\n\nDecision criteria to implement:\n1. Deterministic rate: If the empirical coefficient of variation $\\widehat{\\mathrm{CV}}$ is below a tolerance $\\varepsilon$ specified per test case, classify as deterministic and output the code $0$.\n2. Poisson consistency: If not deterministic, test whether the empirical Fano factor $\\widehat{F}$ can be explained by sampling variability under a Poisson model with mean $\\hat{\\lambda} = \\bar{N}$ (the sample mean). Use a parametric bootstrap under the Poisson model to approximate the sampling distribution of $\\widehat{F}$, construct a two-sided confidence interval at level $\\alpha$, and classify as Poisson (code $1$) if $\\widehat{F}$ falls inside this interval.\n3. Non-Poisson stochastic: If $\\widehat{F}$ falls outside the Poisson-based confidence interval and the stream is not deterministic, classify as non-Poisson stochastic (code $2$).\n\nYour program must:\n- Compute the sample mean $\\bar{N}$ and sample variance $s^2$ with the unbiased estimator $s^2 = \\frac{1}{m-1} \\sum_{i=1}^{m} (N_i - \\bar{N})^2$ whenever $m \\geq 2$, and set $s^2 = 0$ if $m = 1$.\n- Compute the empirical Fano factor $\\widehat{F} = s^2/\\bar{N}$ for $\\bar{N} > 0$ (if $\\bar{N} = 0$, classify as deterministic, code $0$).\n- Compute $\\widehat{\\mathrm{CV}} = \\sqrt{s^2}/\\bar{N}$ for $\\bar{N} > 0$ and compare to the test-case-specific tolerance $\\varepsilon$.\n- For the Poisson consistency test, perform a parametric bootstrap: generate $B$ replicates of size $m$ from $\\mathrm{Poisson}(\\hat{\\lambda})$, compute the bootstrap Fano factors using the same estimators, and obtain the empirical quantiles at probabilities $q_{\\text{low}} = \\alpha/2$ and $q_{\\text{high}} = 1 - \\alpha/2$. If $\\widehat{F}$ lies in $[q_{\\text{low}}, q_{\\text{high}}]$, classify as Poisson (code $1$), otherwise classify as non-Poisson (code $2$). Use a fixed internal random seed for reproducibility.\n\nTest suite:\nFor each case below, the program must apply the above decision rules with the provided parameters. If generation is specified, you must use the given seed to produce the counts; otherwise, use the explicit counts. All numbers are dimensionless counts; no physical units are required.\n\n- Case $1$ (explicit deterministic): counts $[10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10,10]$, $\\alpha = 0.05$, $B = 10000$, $\\varepsilon = 0.05$.\n- Case $2$ (generated Poisson-like): generate $m = 200$ counts from $\\mathrm{Poisson}(\\lambda)$ with $\\lambda = 12$, seed $1$, $\\alpha = 0.05$, $B = 10000$, $\\varepsilon = 0.05$.\n- Case $3$ (generated overdispersed): generate $m = 200$ counts from the Negative Binomial with parameters $n = 12$, $p = 0.6$ (mean $n(1-p)/p$), seed $2$, $\\alpha = 0.05$, $B = 10000$, $\\varepsilon = 0.05$.\n- Case $4$ (generated underdispersed): generate $m = 200$ counts from the Binomial with parameters $n = 20$, $p = 0.5$ (mean $np$), seed $3$, $\\alpha = 0.05$, $B = 10000$, $\\varepsilon = 0.05$.\n- Case $5$ (explicit small-sample near-deterministic): counts $[5,5,6]$, $\\alpha = 0.05$, $B = 10000$, $\\varepsilon = 0.12$.\n- Case $6$ (explicit zero stream): counts $[0,0,0,0,0,0]$, $\\alpha = 0.05$, $B = 10000$, $\\varepsilon = 0.05$.\n\nFinal output format:\nYour program should produce a single line of output containing the classification codes for the six cases as a comma-separated list enclosed in square brackets (e.g., $[c_1,c_2,c_3,c_4,c_5,c_6]$), where each $c_i \\in \\{0,1,2\\}$ is the integer code for case $i$ in the order given above.", "solution": "The task is to develop a computational method for classifying discrete event count streams, $N(t)$, into one of three categories: deterministic (code $0$), stochastic Poisson (code $1$), or stochastic non-Poisson (code $2$). The classification is based on the statistical properties of the observed counts, primarily using the Fano factor and the coefficient of variation. The methodology relies on fundamental statistical estimators and a parametric bootstrap procedure for hypothesis testing.\n\nFirst, we define the key statistical measures derived from a sample of $m$ counts, $\\{N_1, N_2, \\ldots, N_m\\}$. The sample mean, $\\bar{N}$, is the estimator for the process's expected count per window, $\\mathbb{E}[N]$, and is calculated as:\n$$ \\bar{N} = \\frac{1}{m} \\sum_{i=1}^{m} N_i $$\nThe sample variance, $s^2$, is the unbiased estimator for the process's variance, $\\mathrm{Var}(N)$. It is calculated as:\n$$ s^2 = \\frac{1}{m-1} \\sum_{i=1}^{m} (N_i - \\bar{N})^2 \\quad (\\text{for } m \\ge 2) $$\nIf $m=1$, the variance is undefined, and we are instructed to set $s^2=0$. From these, we compute two dimensionless quantities. The empirical coefficient of variation, $\\widehat{\\mathrm{CV}}$, quantifies the variability relative to the mean:\n$$ \\widehat{\\mathrm{CV}} = \\frac{\\sqrt{s^2}}{\\bar{N}} \\quad (\\text{for } \\bar{N} > 0) $$\nThe empirical Fano factor, $\\widehat{F}$, compares the variance to the mean:\n$$ \\widehat{F} = \\frac{s^2}{\\bar{N}} \\quad (\\text{for } \\bar{N} > 0) $$\n\nThe classification algorithm proceeds in a prioritized sequence of tests.\n\nThe first test identifies nearly deterministic processes. A truly deterministic process has zero variance, hence $\\mathrm{Var}(N)=0$, which implies both $F=0$ and $\\mathrm{CV}=0$. For an empirical sample, we test if the relative fluctuation, $\\widehat{\\mathrm{CV}}$, is smaller than a given tolerance, $\\varepsilon$. If $\\widehat{\\mathrm{CV}} < \\varepsilon$, the stream's variability is considered negligible, and it is classified as deterministic (code $0$). Additionally, a stream with a mean count of zero, $\\bar{N}=0$, also implies a variance of zero, and is thus classified as deterministic.\n\nIf the stream is not classified as deterministic, we proceed to distinguish between Poisson and non-Poisson stochastic behavior. This distinction hinges on the Fano factor. A key property of the Poisson process is that its variance equals its mean, meaning its theoretical Fano factor is $F=1$. For other processes, $F$ may differ from $1$. Processes with $F>1$ are termed \"overdispersed\" (more variable than Poisson, e.g., Negative Binomial), while those with $F<1$ are \"underdispersed\" (less variable, e.g., Binomial). The question is whether the deviation of the empirical Fano factor, $\\widehat{F}$, from $1$ is statistically significant or merely due to finite-sample effects.\n\nTo answer this, we employ a parametric bootstrap hypothesis test. The null hypothesis, $H_0$, is that the data-generating process is Poisson. Under $H_0$, the best estimate for the Poisson rate parameter, $\\lambda$, is the sample mean, $\\hat{\\lambda}=\\bar{N}$. We can then simulate the sampling distribution of $\\widehat{F}$ under this null hypothesis. The procedure is as follows:\n1. Generate a large number, $B$, of bootstrap datasets. Each dataset consists of $m$ counts drawn from a Poisson distribution with rate $\\hat{\\lambda}$, i.e., $\\mathrm{Poisson}(\\bar{N})$.\n2. For each bootstrap dataset, calculate its empirical Fano factor using the same estimators, $s^2/\\bar{N}$. This yields a collection of $B$ bootstrap Fano factors, $\\{ \\widehat{F}^{*}_1, \\widehat{F}^{*}_2, \\ldots, \\widehat{F}^{*}_B \\}$.\n3. This collection approximates the sampling distribution of $\\widehat{F}$ under the null hypothesis. We use it to construct a two-sided $(1-\\alpha)$ confidence interval by finding the empirical quantiles. The lower bound is the quantile at probability $\\alpha/2$, and the upper bound is the quantile at $1 - \\alpha/2$.\n\nThe final classification decision is made by comparing the original sample's Fano factor, $\\widehat{F}$, to this confidence interval. If $\\widehat{F}$ falls within the interval, we conclude that the observed value is consistent with the sampling variability of a Poisson process. We thus fail to reject $H_0$ and classify the stream as Poisson (code $1$). If $\\widehat{F}$ falls outside the interval, the observed variability is significantly different from what is expected from a Poisson process. We reject $H_0$ and classify the stream as non-Poisson stochastic (code $2$). This robust, simulation-based approach allows for a statistically principled classification without assuming an analytical form for the distribution of $\\widehat{F}$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the classification on the specified test suite.\n    \"\"\"\n    # This fixed random number generator is used for the bootstrap procedure\n    # to ensure reproducibility of the statistical test itself.\n    BOOTSTRAP_RNG = np.random.default_rng(seed=42)\n\n    def classify_stream(counts, alpha, B, epsilon):\n        \"\"\"\n        Classifies a count stream into deterministic (0), Poisson (1), or\n        non-Poisson (2) based on statistical tests.\n        \"\"\"\n        data = np.array(counts, dtype=np.float64)\n        m = len(data)\n\n        if m == 0:\n            # An empty stream has no data, but problem cases have m>=3.\n            # A logical choice for an empty stream could be deterministic.\n            return 0\n        \n        N_bar = np.mean(data)\n\n        # Per problem specification, a stream with a mean of 0 is deterministic.\n        if N_bar == 0:\n            return 0\n\n        # Calculate unbiased sample variance.\n        # Per problem spec, if m=1, s^2 = 0.\n        if m < 2:\n            s2 = 0.0\n        else:\n            s2 = np.var(data, ddof=1)\n        \n        # Guard against potential floating point issues making variance slightly negative.\n        if s2 < 0:\n            s2 = 0.0\n            \n        s = np.sqrt(s2)\n        CV_hat = s / N_bar\n\n        # Decision Rule 1: Test for deterministic behavior.\n        if CV_hat < epsilon:\n            return 0\n\n        # If not deterministic, perform Poisson consistency test.\n        F_hat = s2 / N_bar\n        lambda_hat = N_bar\n        \n        # Parametric Bootstrap to find the sampling distribution of the Fano factor\n        # under the null hypothesis that the process is Poisson.\n        bootstrap_F_values = np.zeros(B)\n        for i in range(B):\n            # Generate a bootstrap sample from Poisson(lambda_hat)\n            bootstrap_sample = BOOTSTRAP_RNG.poisson(lambda_hat, size=m)\n            \n            bs_mean = np.mean(bootstrap_sample)\n            \n            if bs_mean == 0:\n                # If a bootstrap sample consists of all zeros, mean and variance are 0.\n                # The Fano factor is ill-defined (0/0). We can treat it as 0,\n                # corresponding to a deterministic-like outcome which is far from F=1.\n                bs_F = 0.0\n            else:\n                if m < 2:\n                    bs_var = 0.0\n                else:\n                    bs_var = np.var(bootstrap_sample, ddof=1)\n                bs_F = bs_var / bs_mean\n            \n            bootstrap_F_values[i] = bs_F\n        \n        # Construct the two-sided confidence interval from bootstrap distribution\n        q_low = alpha / 2.0\n        q_high = 1.0 - alpha / 2.0\n        ci_lower = np.quantile(bootstrap_F_values, q_low)\n        ci_upper = np.quantile(bootstrap_F_values, q_high)\n        \n        # Decision Rules 2 & 3: Classify based on the confidence interval.\n        if ci_lower <= F_hat <= ci_upper:\n            return 1  # Poisson\n        else:\n            return 2  # Non-Poisson\n\n    test_cases = [\n        # Case 1: explicit deterministic\n        {'id': 1, 'type': 'explicit', 'args': {'counts': [10]*20, 'alpha': 0.05, 'B': 10000, 'epsilon': 0.05}},\n        # Case 2: generated Poisson-like\n        {'id': 2, 'type': 'generate', 'gen': 'poisson', 'params': {'lam': 12, 'size': 200, 'seed': 1}, 'args': {'alpha': 0.05, 'B': 10000, 'epsilon': 0.05}},\n        # Case 3: generated overdispersed (Negative Binomial)\n        {'id': 3, 'type': 'generate', 'gen': 'neg_binom', 'params': {'n': 12, 'p': 0.6, 'size': 200, 'seed': 2}, 'args': {'alpha': 0.05, 'B': 10000, 'epsilon': 0.05}},\n        # Case 4: generated underdispersed (Binomial)\n        {'id': 4, 'type': 'generate', 'gen': 'binom', 'params': {'n': 20, 'p': 0.5, 'size': 200, 'seed': 3}, 'args': {'alpha': 0.05, 'B': 10000, 'epsilon': 0.05}},\n        # Case 5: explicit small-sample near-deterministic\n        {'id': 5, 'type': 'explicit', 'args': {'counts': [5, 5, 6], 'alpha': 0.05, 'B': 10000, 'epsilon': 0.12}},\n        # Case 6: explicit zero stream\n        {'id': 6, 'type': 'explicit', 'args': {'counts': [0]*6, 'alpha': 0.05, 'B': 10000, 'epsilon': 0.05}},\n    ]\n\n    results = []\n    for case in test_cases:\n        if case['type'] == 'explicit':\n            counts = case['args']['counts']\n        elif case['type'] == 'generate':\n            gen_rng = np.random.default_rng(seed=case['params']['seed'])\n            if case['gen'] == 'poisson':\n                counts = gen_rng.poisson(lam=case['params']['lam'], size=case['params']['size'])\n            elif case['gen'] == 'neg_binom':\n                counts = gen_rng.negative_binomial(n=case['params']['n'], p=case['params']['p'], size=case['params']['size'])\n            elif case['gen'] == 'binom':\n                counts = gen_rng.binomial(n=case['params']['n'], p=case['params']['p'], size=case['params']['size'])\n        \n        classification = classify_stream(\n            counts, \n            case['args']['alpha'], \n            case['args']['B'], \n            case['args']['epsilon']\n        )\n        results.append(classification)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3160668"}, {"introduction": "Can a purely deterministic algorithm behave stochastically? This thought-provoking exercise delves into the heart of computational science, revealing how finite-precision arithmetic can introduce an element of randomness into what is, in theory, a perfectly predictable system. By analyzing the accumulation of rounding errors in a simple iterative model, you will derive how these tiny, systematic errors give rise to a diffusive, stochastic behavior, illustrating that the choice between a deterministic and stochastic model can depend on the very nature of its implementation on a computer [@problem_id:3160672].", "problem": "A discrete-time computational model updates a one-dimensional state according to the deterministic rule $x_{n+1} = x_n + \\alpha$, where $x_0 \\in \\mathbb{R}$ and $\\alpha \\in \\mathbb{R}$ is a fixed constant. In a computer with finite precision, each value is stored on a uniform grid with spacing $\\epsilon > 0$ by rounding to the nearest grid point. Let the rounding operator be $Q(z) = \\epsilon \\cdot \\mathrm{round}(z/\\epsilon)$, so that the implemented update is $x_{n+1} = Q(x_n + \\alpha)$. Assume $x_0$ is exactly representable on the grid and that $\\alpha/\\epsilon$ is irrational.\n\nStarting from the core definitions of deterministic and stochastic models, and the well-tested fact that adding an irrational increment on a circle produces an equidistributed sequence of fractional parts, model the rounding error at step $n$ by the random variable $r_n = Q(x_n + \\alpha) - (x_n + \\alpha)$, which takes values in the interval $[-\\epsilon/2, \\epsilon/2]$. Under the equidistribution assumption, treat $\\{r_n\\}$ as independent and identically distributed with a uniform distribution on $[-\\epsilon/2, \\epsilon/2]$.\n\nLet $N \\in \\mathbb{N}$ be a fixed number of steps. Derive, from first principles of variance and independence of random variables, a closed-form analytic expression for the variance of $x_N$ relative to its unrounded deterministic value $x_0 + N\\alpha$. Express your final answer as a single analytic expression in terms of $N$ and $\\epsilon$. No rounding is required.", "solution": "The problem requires the derivation of the variance of a computationally implemented state after $N$ steps, relative to its ideal deterministic value. The validation of the problem statement has confirmed its scientific soundness and well-posed nature.\n\nLet us define the ideal, unrounded deterministic state at step $n$ as $y_n$. According to the problem, this state evolves as $y_{n+1} = y_n + \\alpha$ with $y_0 = x_0$. Unrolling this recursion gives the explicit form for the ideal state at step $N$:\n$$y_N = x_0 + N\\alpha$$\nThe computationally implemented state, denoted by $x_n$, evolves according to the rule involving a rounding operator $Q(z)$:\n$$x_{n+1} = Q(x_n + \\alpha)$$\nThe problem defines the rounding error at step $n$ as the random variable $r_n$:\n$$r_n = Q(x_n + \\alpha) - (x_n + \\alpha)$$\nUsing this definition, we can rewrite the update rule for the implemented state as:\n$$x_{n+1} = (x_n + \\alpha) + r_n$$\nWe can now express the state $x_N$ after $N$ steps by unrolling this recursion, starting from the initial condition $x_0$, which is assumed to be exactly representable and thus free of initial rounding error.\nFor $n=0$:\n$$x_1 = (x_0 + \\alpha) + r_0$$\nFor $n=1$:\n$$x_2 = (x_1 + \\alpha) + r_1 = ((x_0 + \\alpha) + r_0 + \\alpha) + r_1 = (x_0 + 2\\alpha) + r_0 + r_1$$\nFor $n=2$:\n$$x_3 = (x_2 + \\alpha) + r_2 = ((x_0 + 2\\alpha) + r_0 + r_1 + \\alpha) + r_2 = (x_0 + 3\\alpha) + r_0 + r_1 + r_2$$\nBy induction, the general form for the state $x_N$ after $N$ steps is:\n$$x_N = (x_0 + N\\alpha) + \\sum_{n=0}^{N-1} r_n$$\nThe problem asks for the variance of $x_N$ relative to its unrounded deterministic value, $y_N = x_0 + N\\alpha$. Let us define this difference as the total error, $E_N$:\n$$E_N = x_N - y_N = x_N - (x_0 + N\\alpha)$$\nSubstituting our expression for $x_N$:\n$$E_N = \\left( (x_0 + N\\alpha) + \\sum_{n=0}^{N-1} r_n \\right) - (x_0 + N\\alpha) = \\sum_{n=0}^{N-1} r_n$$\nThe quantity to be derived is the variance of this total error, $\\mathrm{Var}(E_N)$.\n$$\\mathrm{Var}(E_N) = \\mathrm{Var}\\left(\\sum_{n=0}^{N-1} r_n\\right)$$\nThe problem statement provides a crucial modeling assumption: the rounding errors $\\{r_n\\}$ are to be treated as independent and identically distributed (i.i.d.) random variables. A fundamental property of variance states that for a sum of independent random variables, the variance of the sum is the sum of the variances. Therefore:\n$$\\mathrm{Var}(E_N) = \\sum_{n=0}^{N-1} \\mathrm{Var}(r_n)$$\nSince the variables are also identically distributed, they all have the same variance. Let us denote this common variance as $\\sigma_r^2 = \\mathrm{Var}(r_n)$ for any $n$. The sum simplifies to:\n$$\\mathrm{Var}(E_N) = \\sum_{n=0}^{N-1} \\sigma_r^2 = N \\sigma_r^2$$\nOur task now reduces to calculating the variance $\\sigma_r^2$ of a single rounding error $r_n$. According to the problem, $r_n$ is uniformly distributed on the interval $[-\\epsilon/2, \\epsilon/2]$. The probability density function (PDF) for a random variable $r$ uniformly distributed on an interval $[a, b]$ is $f(r) = \\frac{1}{b-a}$ for $r \\in [a, b]$ and $f(r)=0$ otherwise. For our case, $a = -\\epsilon/2$ and $b = \\epsilon/2$, so the PDF is:\n$$f(r) = \\frac{1}{\\epsilon/2 - (-\\epsilon/2)} = \\frac{1}{\\epsilon}, \\quad \\text{for } r \\in [-\\epsilon/2, \\epsilon/2]$$\nThe variance of $r$ is defined as $\\mathrm{Var}(r) = E[r^2] - (E[r])^2$. We calculate the expected value, $E[r]$, first:\n$$E[r] = \\int_{-\\infty}^{\\infty} r f(r) dr = \\int_{-\\epsilon/2}^{\\epsilon/2} r \\cdot \\frac{1}{\\epsilon} dr = \\frac{1}{\\epsilon} \\left[ \\frac{r^2}{2} \\right]_{-\\epsilon/2}^{\\epsilon/2}$$\n$$E[r] = \\frac{1}{2\\epsilon} \\left( \\left(\\frac{\\epsilon}{2}\\right)^2 - \\left(-\\frac{\\epsilon}{2}\\right)^2 \\right) = \\frac{1}{2\\epsilon} \\left( \\frac{\\epsilon^2}{4} - \\frac{\\epsilon^2}{4} \\right) = 0$$\nThe expected value of the rounding error is $0$. Now we calculate the expected value of the square of the rounding error, $E[r^2]$:\n$$E[r^2] = \\int_{-\\infty}^{\\infty} r^2 f(r) dr = \\int_{-\\epsilon/2}^{\\epsilon/2} r^2 \\cdot \\frac{1}{\\epsilon} dr = \\frac{1}{\\epsilon} \\left[ \\frac{r^3}{3} \\right]_{-\\epsilon/2}^{\\epsilon/2}$$\n$$E[r^2] = \\frac{1}{3\\epsilon} \\left( \\left(\\frac{\\epsilon}{2}\\right)^3 - \\left(-\\frac{\\epsilon}{2}\\right)^3 \\right) = \\frac{1}{3\\epsilon} \\left( \\frac{\\epsilon^3}{8} - \\left(-\\frac{\\epsilon^3}{8}\\right) \\right) = \\frac{1}{3\\epsilon} \\left( 2 \\cdot \\frac{\\epsilon^3}{8} \\right) = \\frac{1}{3\\epsilon} \\frac{\\epsilon^3}{4} = \\frac{\\epsilon^2}{12}$$\nNow we can compute the variance of a single rounding error:\n$$\\sigma_r^2 = \\mathrm{Var}(r) = E[r^2] - (E[r])^2 = \\frac{\\epsilon^2}{12} - 0^2 = \\frac{\\epsilon^2}{12}$$\nFinally, we substitute this result back into our expression for the variance of the total error after $N$ steps:\n$$\\mathrm{Var}(E_N) = N \\sigma_r^2 = N \\frac{\\epsilon^2}{12}$$\nThis is the closed-form analytic expression for the variance of the implemented state $x_N$ relative to its ideal deterministic value $x_0 + N\\alpha$.", "answer": "$$\n\\boxed{\\frac{N\\epsilon^2}{12}}\n$$", "id": "3160672"}]}