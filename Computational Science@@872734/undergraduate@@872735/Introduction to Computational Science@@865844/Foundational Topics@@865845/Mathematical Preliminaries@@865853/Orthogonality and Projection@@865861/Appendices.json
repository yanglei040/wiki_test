{"hands_on_practices": [{"introduction": "Orthonormal bases are the gold standard in linear algebra and computational science, simplifying complex problems by making coordinates easy to compute through projections. The Gram-Schmidt process is the fundamental algorithm for converting any set of linearly independent vectors into an orthonormal basis. This exercise provides essential hands-on practice with this constructive procedure in the familiar setting of $\\mathbb{R}^3$, a task central to fields ranging from computer graphics to quantum mechanics [@problem_id:2309884].", "problem": "In the study of a particular crystal structure, the behavior of electrons is analyzed within a 3-dimensional space represented by $\\mathbb{R}^3$, equipped with the standard Euclidean inner product (dot product). Three fundamental, linearly independent direction vectors are identified:\n$$v_1 = (1, 1, 0)$$\n$$v_2 = (1, 0, 1)$$\n$$v_3 = (0, 1, 1)$$\nThese vectors form a basis for $\\mathbb{R}^3$, but they are not mutually orthogonal, which complicates physical calculations. To simplify the analysis, one must construct a new orthonormal basis $\\{e_1, e_2, e_3\\}$ from the set $\\{v_1, v_2, v_3\\}$ by applying the Gram-Schmidt process in the specified order. The process requires that for each new vector $e_k$, the inner product $\\langle e_k, u_k \\rangle$ is positive, where $u_k$ is the intermediate orthogonal vector from which $e_k$ is derived.\n\nWhich of the following sets represents the resulting orthonormal basis $\\{e_1, e_2, e_3\\}$?\n\nA. $\\left\\{ \\left(\\frac{1}{\\sqrt{2}}, \\frac{1}{\\sqrt{2}}, 0\\right), \\left(\\frac{1}{\\sqrt{2}}, -\\frac{1}{\\sqrt{2}}, 0\\right), (0, 0, 1) \\right\\}$\n\nB. $\\left\\{ (1, 1, 0), \\left(\\frac{1}{2}, -\\frac{1}{2}, 1\\right), \\left(-\\frac{2}{3}, \\frac{2}{3}, \\frac{2}{3}\\right) \\right\\}$\n\nC. $\\left\\{ \\left(\\frac{1}{\\sqrt{2}}, \\frac{1}{\\sqrt{2}}, 0\\right), \\left(\\frac{1}{\\sqrt{6}}, -\\frac{1}{\\sqrt{6}}, \\frac{2}{\\sqrt{6}}\\right), \\left(-\\frac{1}{\\sqrt{3}}, \\frac{1}{\\sqrt{3}}, \\frac{1}{\\sqrt{3}}\\right) \\right\\}$\n\nD. $\\left\\{ (1, 1, 0), (1, 0, 1), (0, 1, 1) \\right\\}$\n\nE. $\\left\\{ \\left(\\frac{1}{\\sqrt{2}}, \\frac{1}{\\sqrt{2}}, 0\\right), \\left(\\frac{1}{\\sqrt{3}}, -\\frac{1}{\\sqrt{3}}, \\frac{1}{\\sqrt{3}}\\right), \\left(\\frac{1}{\\sqrt{6}}, \\frac{1}{\\sqrt{6}}, -\\frac{2}{\\sqrt{6}}\\right) \\right\\}$", "solution": "We apply Gram–Schmidt to $v_1=(1,1,0)$, $v_2=(1,0,1)$, $v_3=(0,1,1)$ in this order, using the standard dot product, and choose signs so that $\\langle e_k,u_k \\rangle > 0$.\n\nFirst, set $u_1=v_1$, so\n$$\n\\|u_1\\|=\\sqrt{1^2+1^2+0^2}=\\sqrt{2},\\quad e_1=\\frac{u_1}{\\|u_1\\|}=\\left(\\frac{1}{\\sqrt{2}},\\frac{1}{\\sqrt{2}},0\\right).\n$$\nThen\n$$\nu_2=v_2-\\operatorname{proj}_{e_1}(v_2)=v_2-(\\langle v_2,e_1\\rangle)e_1.\n$$\nCompute $\\langle v_2,e_1\\rangle=\\left\\langle (1,0,1),\\left(\\frac{1}{\\sqrt{2}},\\frac{1}{\\sqrt{2}},0\\right)\\right\\rangle=\\frac{1}{\\sqrt{2}}$, hence\n$$\nu_2=(1,0,1)-\\frac{1}{\\sqrt{2}}\\left(\\frac{1}{\\sqrt{2}},\\frac{1}{\\sqrt{2}},0\\right)=\\left(1,0,1\\right)-\\left(\\frac{1}{2},\\frac{1}{2},0\\right)=\\left(\\frac{1}{2},-\\frac{1}{2},1\\right).\n$$\nIts norm is\n$$\n\\|u_2\\|=\\sqrt{\\left(\\frac{1}{2}\\right)^2+\\left(-\\frac{1}{2}\\right)^2+1^2}=\\sqrt{\\frac{3}{2}},\n$$\nso\n$$\ne_2=\\frac{u_2}{\\|u_2\\|}=\\sqrt{\\frac{2}{3}}\\left(\\frac{1}{2},-\\frac{1}{2},1\\right)=\\left(\\frac{1}{\\sqrt{6}},-\\frac{1}{\\sqrt{6}},\\frac{2}{\\sqrt{6}}\\right),\n$$\nwhich satisfies $\\langle e_2,u_2\\rangle=\\|u_2\\|>0$.\n\nNext,\n$$\nu_3=v_3-\\operatorname{proj}_{e_1}(v_3)-\\operatorname{proj}_{e_2}(v_3)=v_3-(\\langle v_3,e_1\\rangle)e_1-(\\langle v_3,e_2\\rangle)e_2.\n$$\nCompute $\\langle v_3,e_1\\rangle=\\left\\langle (0,1,1),\\left(\\frac{1}{\\sqrt{2}},\\frac{1}{\\sqrt{2}},0\\right)\\right\\rangle=\\frac{1}{\\sqrt{2}}$, hence $(\\langle v_3,e_1\\rangle)e_1=\\left(\\frac{1}{2},\\frac{1}{2},0\\right)$. Also,\n$$\n\\langle v_3,e_2\\rangle=\\left\\langle (0,1,1),\\left(\\frac{1}{\\sqrt{6}},-\\frac{1}{\\sqrt{6}},\\frac{2}{\\sqrt{6}}\\right)\\right\\rangle=\\frac{1}{\\sqrt{6}},\n$$\nso $(\\langle v_3,e_2\\rangle)e_2=\\frac{1}{\\sqrt{6}}\\left(\\frac{1}{\\sqrt{6}},-\\frac{1}{\\sqrt{6}},\\frac{2}{\\sqrt{6}}\\right)=\\left(\\frac{1}{6},-\\frac{1}{6},\\frac{1}{3}\\right)$. Therefore\n$$\nu_3=(0,1,1)-\\left(\\frac{1}{2},\\frac{1}{2},0\\right)-\\left(\\frac{1}{6},-\\frac{1}{6},\\frac{1}{3}\\right)=\\left(-\\frac{2}{3},\\frac{2}{3},\\frac{2}{3}\\right).\n$$\nIts norm is\n$$\n\\|u_3\\|=\\sqrt{\\left(-\\frac{2}{3}\\right)^2+\\left(\\frac{2}{3}\\right)^2+\\left(\\frac{2}{3}\\right)^2}=\\sqrt{\\frac{4}{3}}=\\frac{2}{\\sqrt{3}},\n$$\nso\n$$\ne_3=\\frac{u_3}{\\|u_3\\|}=\\frac{\\sqrt{3}}{2}\\left(-\\frac{2}{3},\\frac{2}{3},\\frac{2}{3}\\right)=\\left(-\\frac{1}{\\sqrt{3}},\\frac{1}{\\sqrt{3}},\\frac{1}{\\sqrt{3}}\\right),\n$$\nwith $\\langle e_3,u_3\\rangle=\\|u_3\\|>0$. The resulting orthonormal basis is\n$$\n\\left\\{\\left(\\frac{1}{\\sqrt{2}},\\frac{1}{\\sqrt{2}},0\\right),\\left(\\frac{1}{\\sqrt{6}},-\\frac{1}{\\sqrt{6}},\\frac{2}{\\sqrt{6}}\\right),\\left(-\\frac{1}{\\sqrt{3}},\\frac{1}{\\sqrt{3}},\\frac{1}{\\sqrt{3}}\\right)\\right\\},\n$$\nwhich corresponds to option C.", "answer": "$$\\boxed{C}$$", "id": "2309884"}, {"introduction": "The power of linear algebra lies in its abstraction, allowing us to apply geometric intuition to spaces of functions, matrices, or signals. This practice extends the concept of projection from simple geometric vectors to the vector space of $2 \\times 2$ matrices, using the Frobenius inner product to define orthogonality. By decomposing a matrix into its symmetric and skew-symmetric parts—a key result in mechanics and data analysis—you will gain a deeper appreciation for how these fundamental tools operate in more abstract settings [@problem_id:2309898].", "problem": "Consider the vector space $V = M_{2 \\times 2}(\\mathbb{R})$ of all $2 \\times 2$ matrices with real entries. This space is equipped with the Frobenius inner product, defined as $\\langle A, B \\rangle = \\operatorname{tr}(A^T B)$ for any two matrices $A, B \\in V$, where $\\operatorname{tr}(M)$ denotes the trace of a matrix $M$. Let $S$ be the subspace of $V$ consisting of all symmetric matrices.\n\nGiven the matrix $A = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix}$, find its component that is orthogonal to the subspace $S$.\n\nPresent your answer, which will be a $2 \\times 2$ matrix $\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$, as a row matrix of its components in the format $\\begin{pmatrix} a & b & c & d \\end{pmatrix}$.", "solution": "Let $V = M_{2 \\times 2}(\\mathbb{R})$ be the vector space of $2 \\times 2$ real matrices, and let $S$ be the subspace of symmetric matrices. The given matrix is $A = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix}$. We are asked to find the component of $A$ that is orthogonal to the subspace $S$.\n\nAny vector $A$ in an inner product space can be uniquely decomposed into two components: one that lies in a subspace $S$ and one that is orthogonal to it. This is written as $A = A_S + A_{S^\\perp}$, where $A_S = \\operatorname{proj}_S(A)$ is the orthogonal projection of $A$ onto $S$, and $A_{S^\\perp}$ is the component of $A$ orthogonal to $S$. We need to find $A_{S^\\perp}$, which can be calculated as $A_{S^\\perp} = A - \\operatorname{proj}_S(A)$.\n\nOur first step is to find the orthogonal projection of $A$ onto $S$. To do this, we need an orthogonal basis for the subspace $S$. A general symmetric $2 \\times 2$ matrix has the form $\\begin{pmatrix} x & y \\\\ y & z \\end{pmatrix}$. We can write this as a linear combination of basis matrices:\n$$\n\\begin{pmatrix} x & y \\\\ y & z \\end{pmatrix} = x \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} + z \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix} + y \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix}\n$$\nSo, a basis for $S$ is $\\{B_1, B_2, B_3\\}$, where $B_1 = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix}$, $B_2 = \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix}$, and $B_3 = \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix}$.\n\nNext, we check if this basis is orthogonal with respect to the Frobenius inner product, $\\langle X, Y \\rangle = \\operatorname{tr}(X^T Y)$. Since the basis matrices are symmetric, $X^T = X$.\n$$\n\\langle B_1, B_2 \\rangle = \\operatorname{tr}(B_1^T B_2) = \\operatorname{tr}(B_1 B_2) = \\operatorname{tr}\\left( \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix} \\right) = \\operatorname{tr}\\left( \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix} \\right) = 0\n$$\n$$\n\\langle B_1, B_3 \\rangle = \\operatorname{tr}(B_1^T B_3) = \\operatorname{tr}(B_1 B_3) = \\operatorname{tr}\\left( \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix} \\right) = \\operatorname{tr}\\left( \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix} \\right) = 0\n$$\n$$\n\\langle B_2, B_3 \\rangle = \\operatorname{tr}(B_2^T B_3) = \\operatorname{tr}(B_2 B_3) = \\operatorname{tr}\\left( \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix} \\right) = \\operatorname{tr}\\left( \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix} \\right) = 0\n$$\nThe basis is indeed orthogonal. The formula for the orthogonal projection of $A$ onto $S$ is:\n$$\n\\operatorname{proj}_S(A) = \\frac{\\langle A, B_1 \\rangle}{\\langle B_1, B_1 \\rangle} B_1 + \\frac{\\langle A, B_2 \\rangle}{\\langle B_2, B_2 \\rangle} B_2 + \\frac{\\langle A, B_3 \\rangle}{\\langle B_3, B_3 \\rangle} B_3\n$$\nWe compute the required inner products. First, the denominators (squared norms of basis vectors):\n$$\n\\langle B_1, B_1 \\rangle = \\operatorname{tr}(B_1^T B_1) = \\operatorname{tr}(B_1^2) = \\operatorname{tr}\\left( \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} \\right) = 1\n$$\n$$\n\\langle B_2, B_2 \\rangle = \\operatorname{tr}(B_2^T B_2) = \\operatorname{tr}(B_2^2) = \\operatorname{tr}\\left( \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix} \\right) = 1\n$$\n$$\n\\langle B_3, B_3 \\rangle = \\operatorname{tr}(B_3^T B_3) = \\operatorname{tr}(B_3^2) = \\operatorname{tr}\\left( \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix}^2 \\right) = \\operatorname{tr}\\left( \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} \\right) = 2\n$$\nNext, the numerators. For $A = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix}$, its transpose is $A^T = \\begin{pmatrix} 1 & 3 \\\\ 2 & 4 \\end{pmatrix}$.\n$$\n\\langle A, B_1 \\rangle = \\operatorname{tr}(A^T B_1) = \\operatorname{tr}\\left( \\begin{pmatrix} 1 & 3 \\\\ 2 & 4 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} \\right) = \\operatorname{tr}\\left( \\begin{pmatrix} 1 & 0 \\\\ 2 & 0 \\end{pmatrix} \\right) = 1\n$$\n$$\n\\langle A, B_2 \\rangle = \\operatorname{tr}(A^T B_2) = \\operatorname{tr}\\left( \\begin{pmatrix} 1 & 3 \\\\ 2 & 4 \\end{pmatrix} \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix} \\right) = \\operatorname{tr}\\left( \\begin{pmatrix} 0 & 3 \\\\ 0 & 4 \\end{pmatrix} \\right) = 4\n$$\n$$\n\\langle A, B_3 \\rangle = \\operatorname{tr}(A^T B_3) = \\operatorname{tr}\\left( \\begin{pmatrix} 1 & 3 \\\\ 2 & 4 \\end{pmatrix} \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix} \\right) = \\operatorname{tr}\\left( \\begin{pmatrix} 3 & 1 \\\\ 4 & 2 \\end{pmatrix} \\right) = 3+2 = 5\n$$\nNow, we can assemble the projection:\n$$\n\\operatorname{proj}_S(A) = \\frac{1}{1} B_1 + \\frac{4}{1} B_2 + \\frac{5}{2} B_3 = 1 \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} + 4 \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix} + \\frac{5}{2} \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix}\n$$\n$$\n\\operatorname{proj}_S(A) = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} + \\begin{pmatrix} 0 & 0 \\\\ 0 & 4 \\end{pmatrix} + \\begin{pmatrix} 0 & 5/2 \\\\ 5/2 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 5/2 \\\\ 5/2 & 4 \\end{pmatrix}\n$$\nFinally, we find the orthogonal component $A_{S^\\perp}$ by subtracting the projection from the original matrix $A$:\n$$\nA_{S^\\perp} = A - \\operatorname{proj}_S(A) = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix} - \\begin{pmatrix} 1 & 5/2 \\\\ 5/2 & 4 \\end{pmatrix}\n$$\n$$\nA_{S^\\perp} = \\begin{pmatrix} 1-1 & 2 - 5/2 \\\\ 3 - 5/2 & 4-4 \\end{pmatrix} = \\begin{pmatrix} 0 & 4/2 - 5/2 \\\\ 6/2 - 5/2 & 0 \\end{pmatrix} = \\begin{pmatrix} 0 & -1/2 \\\\ 1/2 & 0 \\end{pmatrix}\n$$\nThis resulting matrix is skew-symmetric, which is expected as the space of skew-symmetric matrices is the orthogonal complement of the space of symmetric matrices.\n\nThe resulting matrix is $\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix} = \\begin{pmatrix} 0 & -1/2 \\\\ 1/2 & 0 \\end{pmatrix}$. Following the specified output format, we present this as a row matrix.", "answer": "$$\\boxed{\\begin{pmatrix} 0 & -\\frac{1}{2} & \\frac{1}{2} & 0 \\end{pmatrix}}$$", "id": "2309898"}, {"introduction": "In computational science, the geometry of a problem often governs the efficiency of its solution. This advanced practice demonstrates a profound connection between the angle between subspaces and the convergence rate of the method of alternating projections, a powerful iterative algorithm used in optimization and signal processing. By implementing this method and analyzing its performance, you will numerically verify that the asymptotic contraction rate is $\\cos^2\\theta$ and see firsthand how abstract geometric concepts have concrete consequences for algorithmic performance [@problem_id:3168150].", "problem": "You will implement and analyze alternating orthogonal projections between two affine subspaces to explore the relationship between geometric angle and convergence. Work in the real Euclidean space $\\mathbb{R}^3$ with the standard inner product $\\langle \\cdot,\\cdot \\rangle$ and induced norm $\\|\\cdot\\|_2$. An affine subspace is a translate of a linear subspace. The orthogonal projection of a point $\\mathbf{x} \\in \\mathbb{R}^3$ onto an affine line $\\mathcal{L} = \\mathbf{p} + \\operatorname{span}\\{\\mathbf{u}\\}$, where $\\mathbf{u}$ is a unit vector and $\\mathbf{p}$ is a point on the line, is the unique point in $\\mathcal{L}$ minimizing the Euclidean distance to $\\mathbf{x}$. Using only these definitions and facts, derive from first principles an estimation method for the geometric angle between two affine lines based on the asymptotic contraction of the alternating projection mapping.\n\nConstruct your instances as follows. Fix the base point $\\mathbf{p} = (0.6,-0.3,0.2)^\\top \\in \\mathbb{R}^3$ and the starting point $\\mathbf{x}_0 = (2.0,1.5,-0.7)^\\top \\in \\mathbb{R}^3$. For each given angle parameter $\\theta \\in (0,\\pi/2)$, define two affine lines that intersect at $\\mathbf{p}$ with directions\n- $\\mathbf{u} = (1,0,0)^\\top$,\n- $\\mathbf{v}(\\theta) = (\\cos \\theta,\\ \\sin \\theta,\\ 0)^\\top$,\nso that $\\mathcal{A}(\\theta) = \\mathbf{p} + \\operatorname{span}\\{\\mathbf{u}\\}$ and $\\mathcal{B}(\\theta) = \\mathbf{p} + \\operatorname{span}\\{\\mathbf{v}(\\theta)\\}$.\n\nLet $P_{\\mathcal{A}}$ and $P_{\\mathcal{B}}$ denote the orthogonal projections onto $\\mathcal{A}(\\theta)$ and $\\mathcal{B}(\\theta)$, respectively. Consider the alternating projection iteration $\\mathbf{x}_{k+1} = P_{\\mathcal{B}}(P_{\\mathcal{A}}(\\mathbf{x}_k))$ for $k \\in \\{0,1,2,\\dots\\}$, and define $d_k = \\|\\mathbf{x}_k - \\mathbf{p}\\|_2$. For each $\\theta$, generate a sufficiently long sequence $\\{d_k\\}_{k=0}^K$ with $K = 200$. Estimate the asymptotic contraction constant $r$ by computing the median of the ratios $d_{k+1}/d_k$ over a window of consecutive indices $k$ that excludes an initial transient and avoids numerical underflow; more precisely, use the set of indices $k \\in \\{2,3,\\dots,\\min(2+20,K-1)\\}$ such that both $d_k$ and $d_{k+1}$ exceed the threshold $10^{-300}$. If this set is empty, fall back to using the earliest available ratios that satisfy the threshold. Then invert the theoretical relationship between the contraction constant and the geometric angle to produce an estimate $\\widehat{\\theta}$ in radians.\n\nAngle unit requirement: all angles must be expressed in radians.\n\nYour program must implement the above construction and procedure and output the estimates $\\widehat{\\theta}$ for the following test suite of angle parameters:\n- $\\theta_1 = \\pi/6$,\n- $\\theta_2 = \\pi/3$,\n- $\\theta_3 = \\pi/36$,\n- $\\theta_4 = 17\\pi/36$.\n\nFinal output format: your program should produce a single line of output containing the four estimated angles as a comma-separated list enclosed in square brackets, in radians, each rounded to six decimal places (for example, $[0.523599,1.047198,0.087266,1.483530]$).", "solution": "The problem is valid as it is scientifically grounded in the theory of linear algebra and numerical analysis, specifically concerning projections onto convex sets. It is well-posed, objective, and internally consistent.\n\nThe objective is to derive and implement a method for estimating the angle $\\theta$ between two intersecting affine lines, $\\mathcal{A}$ and $\\mathcal{B}$, by analyzing the convergence rate of an iterative projection scheme. The lines are defined in $\\mathbb{R}^3$ as $\\mathcal{A} = \\mathbf{p} + \\operatorname{span}\\{\\mathbf{u}\\}$ and $\\mathcal{B} = \\mathbf{p} + \\operatorname{span}\\{\\mathbf{v}(\\theta)\\}$, where $\\mathbf{p}$ is their common intersection point and $\\mathbf{u}, \\mathbf{v}(\\theta)$ are unit direction vectors. The angle between the lines is the angle $\\theta$ between their direction vectors.\n\nFirst, we derive the formula for the orthogonal projection of a point $\\mathbf{x} \\in \\mathbb{R}^3$ onto a generic affine line $\\mathcal{L} = \\mathbf{p}_{\\mathcal{L}} + \\operatorname{span}\\{\\mathbf{w}\\}$, where $\\mathbf{w}$ is a unit vector. The projection $P_{\\mathcal{L}}(\\mathbf{x})$ is the point $\\mathbf{y} \\in \\mathcal{L}$ that minimizes the squared Euclidean distance $f(\\mathbf{y}) = \\|\\mathbf{x} - \\mathbf{y}\\|_2^2$. Any point $\\mathbf{y} \\in \\mathcal{L}$ can be parameterized by a scalar $t \\in \\mathbb{R}$ as $\\mathbf{y}(t) = \\mathbf{p}_{\\mathcal{L}} + t\\mathbf{w}$. The objective function becomes:\n$$f(t) = \\|\\mathbf{x} - (\\mathbf{p}_{\\mathcal{L}} + t\\mathbf{w})\\|_2^2 = \\langle (\\mathbf{x} - \\mathbf{p}_{\\mathcal{L}}) - t\\mathbf{w}, (\\mathbf{x} - \\mathbf{p}_{\\mathcal{L}}) - t\\mathbf{w} \\rangle$$\nExpanding the inner product gives:\n$$f(t) = \\|\\mathbf{x} - \\mathbf{p}_{\\mathcal{L}}\\|_2^2 - 2t \\langle \\mathbf{x} - \\mathbf{p}_{\\mathcal{L}}, \\mathbf{w} \\rangle + t^2 \\|\\mathbf{w}\\|_2^2$$\nSince $\\mathbf{w}$ is a unit vector, $\\|\\mathbf{w}\\|_2^2=1$. To find the minimum, we differentiate with respect to $t$ and set the result to zero:\n$$\\frac{df}{dt} = -2 \\langle \\mathbf{x} - \\mathbf{p}_{\\mathcal{L}}, \\mathbf{w} \\rangle + 2t = 0$$\nSolving for $t$ yields the optimal value $t^* = \\langle \\mathbf{x} - \\mathbf{p}_{\\mathcal{L}}, \\mathbf{w} \\rangle$. The orthogonal projection is thus:\n$$P_{\\mathcal{L}}(\\mathbf{x}) = \\mathbf{p}_{\\mathcal{L}} + t^*\\mathbf{w} = \\mathbf{p}_{\\mathcal{L}} + \\langle \\mathbf{x} - \\mathbf{p}_{\\mathcal{L}}, \\mathbf{w} \\rangle \\mathbf{w}$$\n\nNext, we analyze the iterative sequence $\\mathbf{x}_{k+1} = P_{\\mathcal{B}}(P_{\\mathcal{A}}(\\mathbf{x}_k))$. It is more convenient to analyze the dynamics of the vector relative to the intersection point $\\mathbf{p}$. Let $\\tilde{\\mathbf{x}}_k = \\mathbf{x}_k - \\mathbf{p}$. The projection operator $P_{\\mathcal{L}}$ onto $\\mathcal{L} = \\mathbf{p} + \\operatorname{span}\\{\\mathbf{w}\\}$ can be related to the linear projection operator $P_S$ onto the corresponding linear subspace $S=\\operatorname{span}\\{\\mathbf{w}\\}$. The formula for $P_S$ is $P_S(\\mathbf{z}) = \\langle \\mathbf{z}, \\mathbf{w} \\rangle \\mathbf{w}$.\n$$P_{\\mathcal{L}}(\\mathbf{x}) - \\mathbf{p} = (\\mathbf{p} + \\langle \\mathbf{x} - \\mathbf{p}, \\mathbf{w} \\rangle \\mathbf{w}) - \\mathbf{p} = \\langle \\mathbf{x} - \\mathbf{p}, \\mathbf{w} \\rangle \\mathbf{w} = P_S(\\mathbf{x} - \\mathbf{p})$$\nLet $S_A = \\operatorname{span}\\{\\mathbf{u}\\}$ and $S_B = \\operatorname{span}\\{\\mathbf{v}(\\theta)\\}$. The iteration for the centered vectors is:\n$$\\tilde{\\mathbf{x}}_{k+1} = \\mathbf{x}_{k+1} - \\mathbf{p} = P_{\\mathcal{B}}(P_{\\mathcal{A}}(\\mathbf{x}_k)) - \\mathbf{p} = P_{S_B}(P_{\\mathcal{A}}(\\mathbf{x}_k) - \\mathbf{p})$$\nLet $\\mathbf{y}_k = P_{\\mathcal{A}}(\\mathbf{x}_k)$. Then $P_{\\mathcal{A}}(\\mathbf{x}_k) - \\mathbf{p} = P_{S_A}(\\mathbf{x}_k - \\mathbf{p}) = P_{S_A}(\\tilde{\\mathbf{x}}_k)$. Substituting this back, we get:\n$$\\tilde{\\mathbf{x}}_{k+1} = P_{S_B}(P_{S_A}(\\tilde{\\mathbf{x}}_k))$$\nThis shows that the affine iteration is equivalent to a linear iteration on the vectors translated by $-\\mathbf{p}$.\n\nNow we derive the contraction rate. The vector $\\tilde{\\mathbf{x}}_{k+1}$ is the result of a projection onto $S_B$, so it must lie within $S_B$. This means $\\tilde{\\mathbf{x}}_{k+1}$ must be a scalar multiple of the direction vector $\\mathbf{v}(\\theta)$ for all $k \\ge 0$. Consequently, for all $k \\ge 1$, the vector $\\tilde{\\mathbf{x}}_k$ is parallel to $\\mathbf{v}(\\theta)$. Let $\\tilde{\\mathbf{x}}_k = c_k \\mathbf{v}(\\theta)$ for some scalar $c_k$ and for all $k \\ge 1$.\nThe iteration for $k \\ge 1$ becomes:\n$$\\tilde{\\mathbf{x}}_{k+1} = P_{S_B}(P_{S_A}(\\tilde{\\mathbf{x}}_k)) = P_{S_B}(P_{S_A}(c_k \\mathbf{v}(\\theta))) = c_k P_{S_B}(P_{S_A}(\\mathbf{v}(\\theta)))$$\nWe compute the composed linear projection $P_{S_B}(P_{S_A}(\\mathbf{v}(\\theta)))$:\n$$P_{S_A}(\\mathbf{v}(\\theta)) = \\langle \\mathbf{v}(\\theta), \\mathbf{u} \\rangle \\mathbf{u}$$\nThe inner product $\\langle \\mathbf{v}(\\theta), \\mathbf{u} \\rangle$ is the cosine of the angle between the direction vectors, which is $\\cos\\theta$.\n$$P_{S_A}(\\mathbf{v}(\\theta)) = (\\cos\\theta) \\mathbf{u}$$\nNext, we project this result onto $S_B$:\n$$P_{S_B}((\\cos\\theta) \\mathbf{u}) = (\\cos\\theta) P_{S_B}(\\mathbf{u}) = (\\cos\\theta) \\langle \\mathbf{u}, \\mathbf{v}(\\theta) \\rangle \\mathbf{v}(\\theta) = (\\cos\\theta) (\\cos\\theta) \\mathbf{v}(\\theta) = (\\cos^2\\theta) \\mathbf{v}(\\theta)$$\nSubstituting this back into the iteration for $\\tilde{\\mathbf{x}}_{k+1}$:\n$$\\tilde{\\mathbf{x}}_{k+1} = c_k (\\cos^2\\theta) \\mathbf{v}(\\theta) = (\\cos^2\\theta) (c_k \\mathbf{v}(\\theta)) = (\\cos^2\\theta) \\tilde{\\mathbf{x}}_k$$\nThis relation holds for all $k \\ge 1$. Taking the Euclidean norm of both sides:\n$$d_{k+1} = \\|\\tilde{\\mathbf{x}}_{k+1}\\|_2 = \\|(\\cos^2\\theta) \\tilde{\\mathbf{x}}_k\\|_2 = |\\cos^2\\theta| \\|\\tilde{\\mathbf{x}}_k\\|_2 = (\\cos^2\\theta) d_k$$\nThe absolute value is removed because $\\theta \\in (0, \\pi/2)$ implies $\\cos\\theta > 0$. The ratio of successive distances to the intersection point is constant for $k \\ge 1$:\n$$\\frac{d_{k+1}}{d_k} = \\cos^2\\theta$$\nThe ratio $d_1/d_0$ is a transient that depends on the initial point $\\mathbf{x}_0$, but for all subsequent steps, the contraction is governed by the constant factor $r = \\cos^2\\theta$.\n\nThe estimation procedure involves numerically generating the sequence $\\{\\mathbf{x}_k\\}$ and then calculating the sequence of distances $\\{d_k\\}$. The asymptotic contraction constant $r$ is estimated by taking the median, $\\widehat{r}$, of the ratios $d_{k+1}/d_k$ over a window of indices $k \\ge 2$ to ensure the transient initial step is excluded.\nGiven the estimate $\\widehat{r}$, the theoretical relationship $r = \\cos^2\\theta$ is inverted to find the angle estimate $\\widehat{\\theta}$:\n$$\\widehat{r} \\approx \\cos^2\\theta \\implies \\cos\\theta \\approx \\sqrt{\\widehat{r}}$$\nWe take the positive square root because $\\theta \\in (0, \\pi/2)$ ensures $\\cos\\theta > 0$. The final estimate is:\n$$\\widehat{\\theta} = \\arccos(\\sqrt{\\widehat{r}})$$\nThe algorithm implemented will perform these steps for each given value of $\\theta$ in the test suite.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and analyzes alternating orthogonal projections between two affine\n    subspaces to estimate the geometric angle between them based on the\n    asymptotic contraction rate.\n    \"\"\"\n\n    # Define the problem parameters\n    p = np.array([0.6, -0.3, 0.2])\n    x0 = np.array([2.0, 1.5, -0.7])\n    u = np.array([1.0, 0.0, 0.0])\n    K = 200\n    \n    # Define the test cases for the angle parameter theta\n    test_cases = [\n        np.pi / 6,\n        np.pi / 3,\n        np.pi / 36,\n        17 * np.pi / 36,\n    ]\n\n    estimated_angles = []\n\n    def project_on_affine_line(x, line_point, line_direction):\n        \"\"\"\n        Calculates the orthogonal projection of a point x onto an affine line.\n        The line is defined by a point (line_point) and a unit direction vector.\n        Formula: P(x) = p + <x-p, u> * u\n        \"\"\"\n        return line_point + np.dot(x - line_point, line_direction) * line_direction\n\n    for theta in test_cases:\n        # Define the second affine line's direction vector for the current theta\n        v = np.array([np.cos(theta), np.sin(theta), 0.0])\n\n        # Generate the sequence of points using alternating projections\n        x_k = [x0]\n        current_x = x0\n        for _ in range(K):\n            # Project onto the first line A\n            y = project_on_affine_line(current_x, p, u)\n            # Project the result onto the second line B\n            next_x = project_on_affine_line(y, p, v)\n            x_k.append(next_x)\n            current_x = next_x\n        \n        x_k_array = np.array(x_k)\n\n        # Calculate the sequence of distances to the intersection point p\n        d_k = np.linalg.norm(x_k_array - p, axis=1)\n\n        # Estimate the asymptotic contraction constant r\n        \n        # Define the window for ratio calculation as per problem description\n        start_k = 2\n        end_k = min(start_k + 20, K - 1)\n        \n        # Calculate ratios d_{k+1}/d_k within the specified window,\n        # adhering to the numerical threshold.\n        threshold = 1.0e-300\n        ratios = []\n        for k in range(start_k, end_k + 1):\n            if d_k[k] > threshold and d_k[k+1] > threshold:\n                ratios.append(d_k[k+1] / d_k[k])\n\n        # Fallback mechanism if the primary window yields no valid ratios\n        if not ratios:\n            # Search for the earliest available ratios in the entire sequence\n            for k in range(1, K):\n                if d_k[k] > threshold and d_k[k+1] > threshold:\n                    ratios.append(d_k[k+1] / d_k[k])\n\n        # The estimated contraction constant is the median of the collected ratios\n        if not ratios:\n            # This case should not be reached with the given parameters,\n            # but as a safeguard, we can assume contraction is zero.\n            r_hat = 0.0\n        else:\n            r_hat = np.median(ratios)\n\n        # Invert the theoretical relationship r = cos^2(theta) to estimate theta\n        # Clamp r_hat to [0, 1] to avoid domain errors in sqrt and arccos\n        r_hat_clamped = np.clip(r_hat, 0.0, 1.0)\n        theta_hat = np.arccos(np.sqrt(r_hat_clamped))\n        \n        estimated_angles.append(theta_hat)\n        \n    # Format the final output as a comma-separated list with 6 decimal places\n    print(f\"[{','.join(f'{angle:.6f}' for angle in estimated_angles)}]\")\n\nsolve()\n```", "id": "3168150"}]}