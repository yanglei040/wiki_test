{"hands_on_practices": [{"introduction": "We begin by applying the Taylor series to a classic geometric problem. This exercise demonstrates how we can approximate a formula involving a non-polynomial function, providing a powerful way to understand how a system's properties change when a parameter deviates from a simple, well-understood case. By expanding the area of an ellipse in terms of its eccentricity $e$, you will see how the Taylor series quantifies the deviation from the area of a perfect circle ($e=0$) [@problem_id:2442188].", "problem": "Consider the ellipse defined by the implicit equation $x^{2}/a^{2} + y^{2}/b^{2} = 1$ with semi-major axis $a0$, semi-minor axis $b0$, and eccentricity $e \\in [0,1)$ defined by $e = \\sqrt{1 - b^{2}/a^{2}}$. The area $A(e)$ of the ellipse can be written using the area-by-integration formula as\n$$\nA(e) = 4 \\int_{0}^{a} b \\sqrt{1 - \\frac{x^{2}}{a^{2}}} \\, dx,\n$$\nwhere $b = a \\sqrt{1 - e^{2}}$.\n\nLet $A(e)$ be regarded as a function of $e$. Determine the fourth-order Taylor polynomial $T_{4}(e)$ of $A(e)$ about $e=0$, expressed purely in terms of $a$, $\\pi$, and $e$. Give your final answer as a single closed-form expression. Do not round your answer, and do not include units.", "solution": "First, we must rigorously validate the problem statement.\n\nStep 1: Extract Givens\nThe problem provides the following information:\n- Ellipse implicit equation: $\\frac{x^{2}}{a^{2}} + \\frac{y^{2}}{b^{2}} = 1$.\n- Semi-major axis: $a > 0$.\n- Semi-minor axis: $b > 0$.\n- Eccentricity: $e \\in [0,1)$ defined by $e = \\sqrt{1 - \\frac{b^{2}}{a^{2}}}$.\n- Area as a function of eccentricity, $A(e)$: $A(e) = 4 \\int_{0}^{a} b \\sqrt{1 - \\frac{x^{2}}{a^{2}}} \\, dx$.\n- Relationship between $b$ and $e$: $b = a \\sqrt{1 - e^{2}}$.\n- Objective: Find the fourth-order Taylor polynomial $T_{4}(e)$ of $A(e)$ about $e=0$, expressed in terms of $a$, $\\pi$, and $e$.\n\nStep 2: Validate Using Extracted Givens\nThe problem is subjected to validation against established criteria.\n- **Scientifically Grounded**: The problem is based on standard, well-established principles of Euclidean geometry and differential and integral calculus. The definitions of an ellipse, its eccentricity, and its area are all correct and standard.\n- **Well-Posed**: The function $A(e)$ is well-defined and infinitely differentiable for $e \\in [0,1)$, ensuring a unique Taylor polynomial of any order exists about $e=0$. The objective is clearly stated.\n- **Objective**: The problem is stated in precise mathematical language, free from ambiguity or subjective interpretation.\n- **Consistency**: The given relation $b = a \\sqrt{1-e^2}$ is a direct consequence of the definition of eccentricity. The integral formulation for area is also correct. We can verify this: substituting $b = a\\sqrt{1-e^2}$ into the area formula gives $A(e) = 4a\\sqrt{1-e^2} \\int_0^a \\sqrt{1-x^2/a^2} dx$. The integral $\\int_0^a \\sqrt{1-x^2/a^2} dx = \\frac{1}{a} \\int_0^a \\sqrt{a^2-x^2} dx$ is $\\frac{1}{a}$ times the area of a quarter circle of radius $a$, which amounts to $\\frac{1}{a} (\\frac{\\pi a^2}{4}) = \\frac{\\pi a}{4}$. Thus, $A(e) = 4a\\sqrt{1-e^2} (\\frac{\\pi a}{4}) = \\pi a^2 \\sqrt{1-e^2}$. This is precisely $\\pi ab$, the correct area of an ellipse. The problem is internally consistent.\n- **Completeness**: All necessary information to solve the problem is provided.\n\nStep 3: Verdict and Action\nThe problem is found to be scientifically grounded, well-posed, objective, and self-consistent. There are no flaws. Therefore, the problem is **valid**. We proceed to the solution.\n\nThe area of the ellipse, as a function of eccentricity $e$, is given by $A(e) = \\pi a^{2} \\sqrt{1 - e^{2}}$. To find the fourth-order Taylor polynomial, $T_{4}(e)$, of $A(e)$ about $e=0$, we require the Maclaurin series expansion of $A(e)$ up to the term of order $e^4$.\n\nThe most direct method is to use the generalized binomial theorem for $(1+u)^{\\alpha}$, which states:\n$$\n(1+u)^{\\alpha} = \\sum_{k=0}^{\\infty} \\binom{\\alpha}{k} u^{k} = 1 + \\alpha u + \\frac{\\alpha(\\alpha-1)}{2!}u^{2} + \\frac{\\alpha(\\alpha-1)(\\alpha-2)}{3!}u^{3} + \\dots\n$$\nThis series converges for $|u|  1$.\n\nOur function is $A(e) = \\pi a^{2} (1 - e^{2})^{\\frac{1}{2}}$. We identify $\\alpha = \\frac{1}{2}$ and $u = -e^{2}$. Since the expansion is about $e=0$ and the problem specifies $e \\in [0,1)$, the condition $|u| = |-e^2| = e^2  1$ is satisfied.\n\nWe expand the term $(1 - e^{2})^{\\frac{1}{2}}$ up to the term containing $e^4$. This requires terms for $k=0, 1, 2$ in the binomial series.\nFor $k=0$:\n$$\n\\binom{\\frac{1}{2}}{0} (-e^{2})^{0} = 1 \\cdot 1 = 1\n$$\nFor $k=1$:\n$$\n\\binom{\\frac{1}{2}}{1} (-e^{2})^{1} = \\frac{1}{2} (-e^{2}) = -\\frac{1}{2} e^{2}\n$$\nFor $k=2$:\n$$\n\\binom{\\frac{1}{2}}{2} (-e^{2})^{2} = \\frac{(\\frac{1}{2})(\\frac{1}{2}-1)}{2!} (e^{4}) = \\frac{(\\frac{1}{2})(-\\frac{1}{2})}{2} e^{4} = -\\frac{1}{8} e^{4}\n$$\nThe next term in the series (for $k=3$) would involve $(-e^2)^3 = -e^6$, which is of a higher order than required for $T_{4}(e)$.\n\nCombining these terms, the expansion of the square root term is:\n$$\n(1 - e^{2})^{\\frac{1}{2}} = 1 - \\frac{1}{2} e^{2} - \\frac{1}{8} e^{4} + O(e^{6})\n$$\nThe fourth-order Taylor polynomial is obtained by truncating the series after the $e^4$ term. We multiply this polynomial by the constant factor $\\pi a^{2}$:\n$$\nT_{4}(e) = \\pi a^{2} \\left( 1 - \\frac{1}{2} e^{2} - \\frac{1}{8} e^{4} \\right)\n$$\nThis is the required fourth-order Taylor polynomial for the area of the ellipse as a function of its eccentricity, centered at $e=0$.", "answer": "$$\n\\boxed{\\pi a^{2} \\left(1 - \\frac{1}{2} e^{2} - \\frac{1}{8} e^{4}\\right)}\n$$", "id": "2442188"}, {"introduction": "A Taylor series provides a brilliant approximation, but only *locally*. This practice challenges you to think critically about the choice of the expansion center and its profound impact on accuracy. By comparing two different Taylor expansions for the sine function, you will gain a concrete understanding of why a \"local\" approximation is essential and how exploiting a function's symmetries can lead to far more effective computational strategies [@problem_id:3200380].", "problem": "In computational practice, one often replaces a smooth function by a low-degree Taylor polynomial near a point of interest to achieve a fast local approximation with a quantifiable remainder (error). Consider the function $f(x) = \\sin(x)$ and two centers: $a = \\pi$ and $a = 0$. Let $p^{(a)}_n(x)$ denote the Taylor polynomial of degree $n$ of $f$ about $x = a$. Define the interval $I = [\\pi - 0.1, \\,\\pi + 0.1]$.\n\nUsing only the definitions of Taylor polynomials and Taylor’s theorem with the Lagrange form of the remainder, the periodicity and symmetry of the sine function, and bounds on derivatives, decide which of the following statements are correct. Select all that apply.\n\nA. For all $x \\in I$, the first-degree Taylor approximation at $a=\\pi$ obeys the bound $\\lvert \\sin(x) - p^{(\\pi)}_1(x) \\rvert \\le \\dfrac{(0.1)^2}{2}$.\n\nB. For all $x \\in I$, the first-degree Taylor approximation at $a=0$ obeys the bound $\\lvert \\sin(x) - p^{(0)}_1(x) \\rvert \\le \\dfrac{(0.1)^2}{2}$.\n\nC. Writing $x = \\pi + h$ with $\\lvert h \\rvert \\le 0.1$, the identity $\\sin(x) = -\\sin(h)$ implies that the two linearizations agree after the shift in the sense that $p^{(\\pi)}_1(\\pi + h) = -\\,p^{(0)}_1(h)$.\n\nD. With the same change of variables $x=\\pi+h$, for any fixed integer $n \\ge 0$ and $\\lvert h \\rvert$ sufficiently small, the remainder of the degree-$n$ Taylor approximation of $\\sin(x)$ about $a=\\pi$ at $x=\\pi+h$ has the same magnitude as the remainder of the degree-$n$ Taylor approximation of $-\\sin(h)$ about $h=0$ at $h$, so any magnitude bound obtained for the Maclaurin remainder of $\\sin$ transfers directly to the expansion at $a=\\pi$.\n\nE. Over $I$, the maximum absolute error of the first-degree Taylor approximation at $a=\\pi$ is at most $0.005$, whereas the maximum absolute error of the first-degree Taylor approximation at $a=0$ exceeds $3.0$.\n\nF. Because $\\sin(\\pi) = 0$, the constant polynomial $p(x) \\equiv 0$ is the best linearization at $a=\\pi$ in the sense of Taylor polynomials.\n\nG. For $a \\in \\{0,\\pi\\}$, the true leading-order behavior of the error of the first-degree Taylor approximation is cubic in the displacement from $a$; that is, with $h=x-a$, the error behaves like $C\\,h^3$ for small $h$ (for some nonzero constant $C$), even though a generic remainder bound based on the second derivative yields only a quadratic upper bound.", "solution": "The problem statement has been validated and is scientifically sound, well-posed, objective, and contains all necessary information to evaluate the given statements.\n\nThe function under consideration is $f(x) = \\sin(x)$. Its derivatives are $f'(x) = \\cos(x)$, $f''(x) = -\\sin(x)$, $f'''(x) = -\\cos(x)$, and so on. A crucial property is that for any integer $k \\ge 0$, the $k$-th derivative $f^{(k)}(x)$ has its absolute value bounded by $1$, i.e., $\\lvert f^{(k)}(x) \\rvert \\le 1$ for all $x \\in \\mathbb{R}$.\n\nThe Taylor polynomial of degree $n$ of $f$ about a center $x=a$ is given by $p_n^{(a)}(x) = \\sum_{k=0}^n \\dfrac{f^{(k)}(a)}{k!}(x-a)^k$.\nThe remainder, or error, is $R_n^{(a)}(x) = f(x) - p_n^{(a)}(x)$. Taylor's theorem with the Lagrange form of the remainder states that $R_n^{(a)}(x) = \\dfrac{f^{(n+1)}(c)}{(n+1)!}(x-a)^{n+1}$ for some value $c$ between $a$ and $x$.\nThis gives a bound on the magnitude of the error: $\\lvert R_n^{(a)}(x) \\rvert \\le \\dfrac{M_{n+1}}{(n+1)!}\\lvert x-a \\rvert^{n+1}$, where $M_{n+1}$ is an upper bound for $\\lvert f^{(n+1)}(t) \\rvert$ for $t$ between $a$ and $x$. For $f(x) = \\sin(x)$, we can always use $M_{k}=1$ for any $k$.\n\nThe interval of interest is $I = [\\pi - 0.1, \\,\\pi + 0.1]$. For any $x \\in I$, the displacement from $a=\\pi$ is $\\lvert x-\\pi \\rvert \\le 0.1$.\n\n**A. For all $x \\in I$, the first-degree Taylor approximation at $a=\\pi$ obeys the bound $\\lvert \\sin(x) - p^{(\\pi)}_1(x) \\rvert \\le \\dfrac{(0.1)^2}{2}$.**\n\nThe first-degree Taylor approximation at $a=\\pi$ is $p^{(\\pi)}_1(x) = f(\\pi) + f'(\\pi)(x-\\pi)$.\nWe have $f(\\pi) = \\sin(\\pi) = 0$ and $f'(\\pi) = \\cos(\\pi) = -1$.\nSo, $p^{(\\pi)}_1(x) = 0 + (-1)(x-\\pi) = \\pi - x$.\nThe error is the remainder $R_1^{(\\pi)}(x) = \\sin(x) - p^{(\\pi)}_1(x)$.\nUsing the Lagrange form of the remainder with $n=1$ and $a=\\pi$:\n$$R_1^{(\\pi)}(x) = \\dfrac{f''(c)}{2!}(x-\\pi)^2 = \\dfrac{-\\sin(c)}{2}(x-\\pi)^2$$\nfor some $c$ between $\\pi$ and $x$.\nThe magnitude of the error is $\\lvert R_1^{(\\pi)}(x) \\rvert = \\left\\lvert \\dfrac{-\\sin(c)}{2}(x-\\pi)^2 \\right\\rvert = \\dfrac{\\lvert \\sin(c) \\rvert}{2}(x-\\pi)^2$.\nWe can use the general bound $\\lvert \\sin(c) \\rvert \\le 1$.\nFor $x \\in I$, we have $\\lvert x-\\pi \\rvert \\le 0.1$.\nSubstituting these bounds, we get:\n$$\\lvert \\sin(x) - p^{(\\pi)}_1(x) \\rvert \\le \\dfrac{1}{2}(\\lvert x-\\pi \\rvert)^2 \\le \\dfrac{1}{2}(0.1)^2$$\nThe statement is a direct application of Taylor's remainder theorem.\n\nVerdict: **Correct**.\n\n**B. For all $x \\in I$, the first-degree Taylor approximation at $a=0$ obeys the bound $\\lvert \\sin(x) - p^{(0)}_1(x) \\rvert \\le \\dfrac{(0.1)^2}{2}$.**\n\nThe first-degree Taylor approximation at $a=0$ (the Maclaurin polynomial) is $p^{(0)}_1(x) = f(0) + f'(0)(x-0)$.\nWe have $f(0)=\\sin(0)=0$ and $f'(0)=\\cos(0)=1$. So, $p^{(0)}_1(x) = x$.\nThe error is $\\lvert \\sin(x) - x \\rvert$. The interval for $x$ is $I = [\\pi-0.1, \\pi+0.1]$.\nThe expression $\\dfrac{(0.1)^2}{2}$ in the proposed bound suggests an application of the remainder theorem where the distance from the center is at most $0.1$. The center of expansion is $a=0$. For $x \\in I$, the distance from the center is $\\lvert x-a \\rvert = \\lvert x-0 \\rvert = x$, which is approximately $\\pi \\approx 3.14159$. The value is much larger than $0.1$.\nLet's evaluate the error at $x=\\pi \\in I$.\nError = $\\lvert \\sin(\\pi) - \\pi \\rvert = \\lvert 0 - \\pi \\rvert = \\pi \\approx 3.14159$.\nThe proposed bound is $\\dfrac{(0.1)^2}{2} = \\dfrac{0.01}{2} = 0.005$.\nClearly, $\\pi  0.005$. The statement is false. The approximation $p^{(0)}_1(x)=x$ is very poor for $x$ near $\\pi$.\n\nVerdict: **Incorrect**.\n\n**C. Writing $x = \\pi + h$ with $\\lvert h \\rvert \\le 0.1$, the identity $\\sin(x) = -\\sin(h)$ implies that the two linearizations agree after the shift in the sense that $p^{(\\pi)}_1(\\pi + h) = -\\,p^{(0)}_1(h)$.**\n\nLet's compute the two sides of the proposed equality.\nLeft-hand side: From analysis A, the linearization at $a=\\pi$ is $p^{(\\pi)}_1(x) = \\pi - x$.\nSubstituting $x = \\pi + h$, we get $p^{(\\pi)}_1(\\pi+h) = \\pi - (\\pi+h) = -h$.\nRight-hand side: First, we need the linearization of the function $f(y)=\\sin(y)$ at $y=0$. This is $p^{(0)}_1(y)=y$, as found in analysis B.\nThe expression $-p^{(0)}_1(h)$ is thus $-h$.\nSince both the left-hand side and the right-hand side are equal to $-h$, the equality $p^{(\\pi)}_1(\\pi + h) = -\\,p^{(0)}_1(h)$ holds true for any $h$.\n\nVerdict: **Correct**.\n\n**D. With the same change of variables $x=\\pi+h$, for any fixed integer $n \\ge 0$ and $\\lvert h \\rvert$ sufficiently small, the remainder of the degree-$n$ Taylor approximation of $\\sin(x)$ about $a=\\pi$ at $x=\\pi+h$ has the same magnitude as the remainder of the degree-$n$ Taylor approximation of $-\\sin(h)$ about $h=0$ at $h$, so any magnitude bound obtained for the Maclaurin remainder of $\\sin$ transfers directly to the expansion at $a=\\pi$.**\n\nLet $g(x) = \\sin(x)$ and $f(h) = -\\sin(h)$. We compare the Taylor expansions of $g(x)$ about $a=\\pi$ and $f(h)$ about $a=0$.\nThe Taylor coefficients for $g(x)$ at $x=\\pi$ are given by $g^{(k)}(\\pi)$.\nThe Taylor coefficients for $f(h)$ at $h=0$ are given by $f^{(k)}(0)$.\nLet's compare these derivatives. Using the chain rule and trigonometric identities:\n$g^{(k)}(x) = \\dfrac{d^k}{dx^k}\\sin(x) = \\sin(x + k\\pi/2)$. At $x=\\pi$, $g^{(k)}(\\pi) = \\sin(\\pi + k\\pi/2) = \\sin(\\pi)\\cos(k\\pi/2) + \\cos(\\pi)\\sin(k\\pi/2) = -\\sin(k\\pi/2)$.\n$f^{(k)}(h) = \\dfrac{d^k}{dh^k}(-\\sin(h)) = -\\sin(h + k\\pi/2)$. At $h=0$, $f^{(k)}(0) = -\\sin(k\\pi/2)$.\nSo, $g^{(k)}(\\pi) = f^{(k)}(0)$ for all integers $k \\ge 0$.\nThe Taylor polynomial for $g(x)$ at $a=\\pi$, evaluated at $x=\\pi+h$, is $p_{g,n}^{(\\pi)}(\\pi+h) = \\sum_{k=0}^n \\frac{g^{(k)}(\\pi)}{k!} h^k$.\nThe Taylor polynomial for $f(h)$ at $a=0$, evaluated at $h$, is $p_{f,n}^{(0)}(h) = \\sum_{k=0}^n \\frac{f^{(k)}(0)}{k!} h^k$.\nSince the coefficients are identical, the polynomials are identical: $p_{g,n}^{(\\pi)}(\\pi+h) = p_{f,n}^{(0)}(h)$.\nNow we examine the remainders.\nThe remainder for $g(x)$ is $R_{g,n}^{(\\pi)}(\\pi+h) = g(\\pi+h) - p_{g,n}^{(\\pi)}(\\pi+h) = \\sin(\\pi+h) - p_{g,n}^{(\\pi)}(\\pi+h)$. Using the identity $\\sin(\\pi+h)=-\\sin(h)$, this becomes $-\\sin(h) - p_{g,n}^{(\\pi)}(\\pi+h)$.\nThe remainder for $f(h)$ is $R_{f,n}^{(0)}(h) = f(h) - p_{f,n}^{(0)}(h) = -\\sin(h) - p_{f,n}^{(0)}(h)$.\nSince the polynomials are equal, the remainders are also equal: $R_{g,n}^{(\\pi)}(\\pi+h) = R_{f,n}^{(0)}(h)$.\nTherefore, their magnitudes are equal: $\\lvert R_{g,n}^{(\\pi)}(\\pi+h) \\rvert = \\lvert R_{f,n}^{(0)}(h) \\rvert$.\nThe remainder for $f(h)=-\\sin(h)$ is just the negative of the remainder for $\\sin(h)$, so their magnitudes are identical. This means a bound on the Maclaurin remainder for $\\sin(h)$ is also a bound for the remainder of $\\sin(x)$ expanded about $\\pi$.\n\nVerdict: **Correct**.\n\n**E. Over $I$, the maximum absolute error of the first-degree Taylor approximation at $a=\\pi$ is at most $0.005$, whereas the maximum absolute error of the first-degree Taylor approximation at $a=0$ exceeds $3.0$.**\n\nFirst part: max error for $a=\\pi$.\nFrom analysis A, for any $x \\in I$, the error obeys $\\lvert \\sin(x) - p^{(\\pi)}_1(x) \\rvert \\le \\dfrac{1}{2}(0.1)^2 = 0.005$. Thus, the maximum absolute error is at most $0.005$. This part is correct.\n\nSecond part: max error for $a=0$.\nThe error is $\\lvert \\sin(x) - p^{(0)}_1(x) \\rvert = \\lvert \\sin(x) - x \\rvert$ for $x \\in I = [\\pi-0.1, \\pi+0.1]$. Since $x0$ in this interval, we have $x  \\sin(x)$, so the error is $x - \\sin(x)$.\nLet $E(x) = x-\\sin(x)$. The derivative is $E'(x) = 1-\\cos(x)$. For $x \\in I$, $x$ is near $\\pi$, so $\\cos(x)$ is near $-1$. Thus $E'(x) = 1-\\cos(x)  0$. The error function $E(x)$ is strictly increasing on $I$.\nThe maximum error will occur at the right endpoint of the interval, $x = \\pi+0.1$.\nMax error = $(\\pi+0.1) - \\sin(\\pi+0.1) = \\pi + 0.1 - (-\\sin(0.1)) = \\pi + 0.1 + \\sin(0.1)$.\nUsing approximations $\\pi \\approx 3.14159$ and $\\sin(0.1) \\approx 0.09983$:\nMax error $\\approx 3.14159 + 0.1 + 0.09983 = 3.34142$.\nSince $3.34142  3.0$, the maximum absolute error indeed exceeds $3.0$. Both parts of the statement are true.\n\nVerdict: **Correct**.\n\n**F. Because $\\sin(\\pi) = 0$, the constant polynomial $p(x) \\equiv 0$ is the best linearization at $a=\\pi$ in the sense of Taylor polynomials.**\n\nThe \"best linearization\" in the context of Taylor series is the first-degree Taylor polynomial, $p_1^{(a)}(x)$.\nFrom analysis A, the linearization at $a=\\pi$ is $p_1^{(\\pi)}(x) = \\pi - x$.\nThis is a non-constant polynomial.\nThe polynomial $p(x) \\equiv 0$ is the zero-degree Taylor polynomial, $p_0^{(\\pi)}(x) = \\sin(\\pi) = 0$. This is the best *constant* approximation near $x=\\pi$, but \"linearization\" implies a first-degree polynomial, which accounts for the function's slope. The slope of $\\sin(x)$ at $x=\\pi$ is $\\cos(\\pi)=-1$, which is non-zero. Therefore, the best linear approximation is not a constant function.\n\nVerdict: **Incorrect**.\n\n**G. For $a \\in \\{0,\\pi\\}$, the true leading-order behavior of the error of the first-degree Taylor approximation is cubic in the displacement from $a$; that is, with $h=x-a$, the error behaves like $C\\,h^3$ for small $h$ (for some nonzero constant $C$), even though a generic remainder bound based on the second derivative yields only a quadratic upper bound.**\n\nThe error of the first-degree Taylor approximation is $R_1^{(a)}(x) = f(x) - p_1^{(a)}(x)$.\nThe Taylor series can be extended to higher orders: $f(x) = p_1^{(a)}(x) + \\dfrac{f''(a)}{2!}(x-a)^2 + \\dfrac{f'''(a)}{3!}(x-a)^3 + \\dots$\nSo, the error is $R_1^{(a)}(x) = \\dfrac{f''(a)}{2!}(x-a)^2 + \\dfrac{f'''(a)}{3!}(x-a)^3 + \\dots$\n\nLet's check $f''(a)$ for $a \\in \\{0, \\pi\\}$. The second derivative is $f''(x) = -\\sin(x)$.\nFor $a=0$, $f''(0) = -\\sin(0) = 0$.\nFor $a=\\pi$, $f''(\\pi) = -\\sin(\\pi) = 0$.\nIn both cases, the coefficient of the $(x-a)^2$ term is zero.\nThis means the error's leading term is the $(x-a)^3$ term.\n$R_1^{(a)}(x) = \\dfrac{f'''(a)}{3!}(x-a)^3 + \\dfrac{f^{(4)}(a)}{4!}(x-a)^4 + \\dots$\nLet's check the third derivative coefficient. $f'''(x) = -\\cos(x)$.\nFor $a=0$, $f'''(0) = -\\cos(0) = -1 \\ne 0$.\nFor $a=\\pi$, $f'''(\\pi) = -\\cos(\\pi) = -(-1) = 1 \\ne 0$.\nSo for small $h=x-a$, the error behaves as $R_1^{(a)}(a+h) \\approx \\dfrac{f'''(a)}{6}h^3$.\nFor $a=0$, the error is $\\approx -\\frac{1}{6}h^3$. For $a=\\pi$, the error is $\\approx \\frac{1}{6}h^3$.\nIn both cases, the leading-order behavior is cubic with a non-zero constant $C$.\nThe statement also correctly notes that a generic remainder bound $\\lvert R_1(x) \\rvert \\le \\frac{M_2}{2}(x-a)^2$ (where $M_2$ is a bound on $\\lvert f''(c)\\rvert$) suggests a quadratic upper bound, which is true but not as sharp as the actual cubic behavior in this special case where $f''(a)=0$.\n\nVerdict: **Correct**.", "answer": "$$\\boxed{ACDEG}$$", "id": "3200380"}, {"introduction": "Now, let's synthesize theory into a robust computational tool. This final exercise guides you through the process of building a numerically stable function to calculate $\\sin(x)$ for any input, a cornerstone of scientific software. You will implement an argument reduction strategy and use remainder estimates for rigorous error control, directly confronting and solving the practical challenges of finite-precision arithmetic, such as catastrophic cancellation [@problem_id:2442233].", "problem": "You are to write a complete, runnable program that evaluates the sine function using a Maclaurin series with rigorously controlled truncation error, and that analyzes the numerical stability of this approach for large arguments. Work entirely in radians.\n\nStart from the following fundamental base:\n- The definition of the Maclaurin series for analytic functions and the Lagrange form of the remainder. For a sufficiently differentiable function $f$, its Maclaurin polynomial of degree $m$ at $x=0$ has a remainder $R_{m}(x) = \\dfrac{f^{(m+1)}(\\xi)}{(m+1)!} x^{m+1}$ for some $\\xi$ between $0$ and $x$.\n- The periodicity and parity identities for trigonometric functions: $\\sin(x + 2\\pi k) = \\sin(x)$ for any integer $k$, and the angle-sum and quadrant symmetries that relate $\\sin(x)$ to $\\sin(r)$ or $\\cos(r)$ for a small residual $r$ after reduction by integer multiples of $\\dfrac{\\pi}{2}$.\n\nYour tasks:\n1) Derive, from the Maclaurin definition and the Lagrange remainder formula, a stopping criterion for truncating the Maclaurin series of $\\sin(x)$ and $\\cos(x)$ to achieve a user-prescribed absolute error tolerance $\\varepsilon  0$. In particular, justify bounds of the form\n$$\n\\left| R_{2N+1}^{\\sin}(x) \\right| \\le \\frac{|x|^{2N+3}}{(2N+3)!}, \n\\qquad\n\\left| R_{2N}^{\\cos}(x) \\right| \\le \\frac{|x|^{2N+2}}{(2N+2)!},\n$$\nby using that the absolute value of any derivative of $\\sin(x)$ or $\\cos(x)$ is at most $1$ on the real line. Then formulate a procedure that, given a small argument $z$, finds the smallest integer $N \\ge 0$ such that the corresponding remainder bound is less than or equal to $\\varepsilon$.\n\n2) Devise and implement an argument reduction strategy that, given any real $x$, uses integer multiples of $\\dfrac{\\pi}{2}$ to map $x$ to a residual $r$ in the interval $[-\\dfrac{\\pi}{4}, \\dfrac{\\pi}{4}]$ and a quadrant index $q \\in \\{0,1,2,3\\}$ such that\n$$\n\\sin(x) = \n\\begin{cases}\n\\phantom{-}\\sin(r),  q \\equiv 0 \\ (\\text{mod } 4),\\\\\n\\phantom{-}\\cos(r),  q \\equiv 1 \\ (\\text{mod } 4),\\\\\n-\\sin(r),  q \\equiv 2 \\ (\\text{mod } 4),\\\\\n-\\cos(r),  q \\equiv 3 \\ (\\text{mod } 4).\n\\end{cases}\n$$\nUse this mapping to select which Maclaurin series (sine or cosine) to evaluate at the small residual $r$. Use the stopping criterion from Task $1$ to determine the minimal number of series terms needed to guarantee that the truncation error does not exceed $\\varepsilon$.\n\n3) Explain, from first principles, why the direct Maclaurin evaluation at a large $|x|$ is numerically unstable: analyze how the magnitude of intermediate terms behaves before factorial growth dominates and how finite-precision arithmetic exacerbates cancellation. Then contrast this with the stability of the reduced-argument strategy, and also explain the limitation that for extremely large $|x|$ the floating-point reduction $x \\mapsto r$ can itself lose accuracy because of limited mantissa bits.\n\n4) Implement the following programmatic outputs for a test suite of inputs. For each test case parameter pair $(x, \\varepsilon)$:\n- Compute $s_{\\text{approx}}$ using your argument-reduced Maclaurin method.\n- Compute a boolean $b_{\\text{ok}}$ that is true if and only if $|s_{\\text{approx}} - \\sin(x)| \\le \\varepsilon$, where $\\sin(x)$ on the right-hand side is evaluated using a high-quality library function as a reference.\n- Report the minimal number of nonzero Maclaurin terms actually summed for the chosen series, denoted $T$ (for $\\sin$ this is $T = N+1$ corresponding to degrees $1,3,\\dots,2N+1$, and for $\\cos$ this is $T = N+1$ corresponding to degrees $0,2,\\dots,2N$).\n- Assess whether a naive, unreduced Maclaurin series for $\\sin(x)$ is practically unusable under the same tolerance by checking if, before the remainder bound falls below $\\varepsilon$, one must exceed a cap of $T_{\\max} = 1000$ nonzero terms, or if intermediate terms overflow to non-finite values. If so, record a boolean $b_{\\text{naive\\_impractical}}$ as true, otherwise false.\n\nAngle unit: radians. There are no physical units. All percentages, if any, must be expressed as decimals.\n\nTest suite:\n- Case $1$: $(x, \\varepsilon) = (100.0, 10^{-12})$.\n- Case $2$: $(x, \\varepsilon) = (10^{6} + 0.1, 10^{-12})$.\n- Case $3$: $(x, \\varepsilon) = (10^{16} + 0.1, 10^{-12})$.\n- Case $4$: $(x, \\varepsilon) = \\left(\\dfrac{\\pi}{2} + 10^{-8}, 10^{-15}\\right)$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For the $k$-th test case in the order above, output the triple $[b_{\\text{ok}}, T, b_{\\text{naive\\_impractical}}]$ and flatten these triples into a single list in test-case order. For example, the final line should look like\n$[b_{\\text{ok},1}, T_{1}, b_{\\text{naive\\_impractical},1}, b_{\\text{ok},2}, T_{2}, b_{\\text{naive\\_impractical},2}, b_{\\text{ok},3}, T_{3}, b_{\\text{naive\\_impractical},3}, b_{\\text{ok},4}, T_{4}, b_{\\text{naive\\_impractical},4}]$.", "solution": "The problem as stated is well-defined, internally consistent, and grounded in the fundamental principles of numerical analysis and calculus. It is neither ambiguous nor scientifically unsound. We shall therefore proceed with a rigorous derivation and implementation as requested.\n\nThe task is to implement a numerically stable method for evaluating the sine function, $\\sin(x)$, using its Maclaurin series expansion, and to analyze its performance against a naive, direct evaluation. The solution is structured into three principal parts: the derivation of a series truncation criterion, the development of an argument reduction strategy, and an analysis of numerical stability.\n\n**1. Derivation of the Truncation Criterion**\n\nThe Maclaurin series for an analytic function $f(x)$ expanded around $x=0$ is given by $f(x) = \\sum_{k=0}^{\\infty} \\frac{f^{(k)}(0)}{k!} x^k$. When this series is truncated after the term of degree $m$, the result is the Maclaurin polynomial $P_m(x)$, and the error is the remainder term $R_m(x)$. According to the Lagrange form of the remainder, $R_m(x) = \\frac{f^{(m+1)}(\\xi)}{(m+1)!} x^{m+1}$ for some $\\xi$ between $0$ and $x$.\n\nFor $f(x) = \\sin(x)$, the derivatives are cyclical: $\\sin'(x) = \\cos(x)$, $\\sin''(x) = -\\sin(x)$, and so on. Critically, for any integer $k \\ge 0$, $|f^{(k)}(x)| \\le 1$ for all real $x$. The Maclaurin series for $\\sin(x)$ contains only odd powers of $x$:\n$$\n\\sin(x) = \\sum_{k=0}^{N} \\frac{(-1)^k}{(2k+1)!} x^{2k+1} + R_{2N+1}(x)\n$$\nHere, we have truncated the series after the term corresponding to $k=N$, which has degree $2N+1$. The polynomial part, $P_{2N+1}(x)$, is identical to $P_{2N+2}(x)$ because the coefficient of $x^{2N+2}$ is zero. The remainder is therefore $R_{2N+2}(x)$, given by:\n$$\nR_{2N+2}(x) = \\frac{f^{(2N+3)}(\\xi)}{(2N+3)!} x^{2N+3}\n$$\nwhere $f^{(2N+3)}(x)$ is either $\\pm\\sin(x)$ or $\\pm\\cos(x)$. Using the bound $|f^{(2N+3)}(\\xi)| \\le 1$, we establish the error bound for the sine series truncated at degree $2N+1$:\n$$\n\\left| R_{2N+1}^{\\sin}(x) \\right| = \\left| R_{2N+2}^{\\sin}(x) \\right| \\le \\frac{|x|^{2N+3}}{(2N+3)!}\n$$\nThis confirms the inequality given in the problem statement.\n\nFor $f(x) = \\cos(x)$, the Maclaurin series contains only even powers of $x$:\n$$\n\\cos(x) = \\sum_{k=0}^{N} \\frac{(-1)^k}{(2k)!} x^{2k} + R_{2N}(x)\n$$\nThe polynomial $P_{2N}(x)$ is identical to $P_{2N+1}(x)$. The remainder is $R_{2N+1}(x)$:\n$$\nR_{2N+1}(x) = \\frac{f^{(2N+2)}(\\xi)}{(2N+2)!} x^{2N+2}\n$$\nAgain, using the bound $|f^{(2N+2)}(\\xi)| \\le 1$, we obtain the error bound for the cosine series truncated at degree $2N$:\n$$\n\\left| R_{2N}^{\\cos}(x) \\right| = \\left| R_{2N+1}^{\\cos}(x) \\right| \\le \\frac{|x|^{2N+2}}{(2N+2)!}\n$$\nThis also confirms the problem's formulation.\n\nTo satisfy a prescribed absolute error tolerance $\\varepsilon > 0$ for a small argument $z$, we must find the smallest non-negative integer $N$ such that the error bound does not exceed $\\varepsilon$.\nFor $\\sin(z)$, we must find the minimum $N \\ge 0$ such that $\\frac{|z|^{2N+3}}{(2N+3)!} \\le \\varepsilon$.\nFor $\\cos(z)$, we must find the minimum $N \\ge 0$ such that $\\frac{|z|^{2N+2}}{(2N+2)!} \\le \\varepsilon$.\nThis requires an iterative search, starting from $N=0$ and incrementing $N$ until the condition is met. The number of non-zero terms to be summed is then $T = N+1$.\n\n**2. Argument Reduction Strategy**\n\nDirect evaluation of the Maclaurin series is inefficient and numerically unstable for large $|x|$. A standard and robust technique is argument reduction, which leverages the periodic properties of trigonometric functions. Any real number $x$ can be expressed as $x = q \\cdot \\frac{\\pi}{2} + r$, where $q$ is an integer and $r$ is a small residual. We choose $q$ to be the integer nearest to the value of $x / (\\pi/2)$, which ensures that the residual $r$ lies in the interval $[-\\frac{\\pi}{4}, \\frac{\\pi}{4}]$.\nThe procedure is as follows:\n1. Compute $y = x / (\\pi/2)$.\n2. Find the nearest integer $q = \\text{round}(y)$.\n3. Calculate the residual $r = x - q \\cdot (\\pi/2)$. By construction, $|r| \\le \\frac{1}{2} \\cdot \\frac{\\pi}{2} = \\frac{\\pi}{4}$.\n\nThe value of $\\sin(x)$ is then related to either $\\sin(r)$ or $\\cos(r)$ based on the value of $q$ modulo $4$. Let $q_{\\text{mod} 4} = q \\pmod 4$. We use the angle-sum identities:\n- If $q_{\\text{mod} 4} = 0$: $\\sin(x) = \\sin(4k \\cdot \\frac{\\pi}{2} + r) = \\sin(2k\\pi + r) = \\sin(r)$.\n- If $q_{\\text{mod} 4} = 1$: $\\sin(x) = \\sin((4k+1) \\cdot \\frac{\\pi}{2} + r) = \\sin(2k\\pi + \\frac{\\pi}{2} + r) = \\cos(r)$.\n- If $q_{\\text{mod} 4} = 2$: $\\sin(x) = \\sin((4k+2) \\cdot \\frac{\\pi}{2} + r) = \\sin(2k\\pi + \\pi + r) = -\\sin(r)$.\n- If $q_{\\text{mod} 4} = 3$: $\\sin(x) = \\sin((4k+3) \\cdot \\frac{\\pi}{2} + r) = \\sin(2k\\pi + \\frac{3\\pi}{2} + r) = -\\cos(r)$.\n\nThis strategy reduces the problem of computing $\\sin(x)$ for any $x$ to computing either $\\sin(r)$ or $\\cos(r)$ for a small argument $|r| \\le \\pi/4$. For such small arguments, the Maclaurin series converges rapidly, and the truncation criterion derived in the previous section can be applied efficiently to determine the required number of terms.\n\n**3. Analysis of Numerical Stability**\n\nThe direct evaluation of the Maclaurin series for $\\sin(x)$ with large $|x|$ is numerically unstable due to two primary issues: intermediate term overflow and catastrophic cancellation.\n\n- **Intermediate Term Growth and Overflow**: The $k$-th term of the series for $\\sin(x)$ is $t_k = \\frac{(-1)^k x^{2k+1}}{(2k+1)!}$. The magnitude of these terms initially grows rapidly with $k$. The maximum term magnitude occurs when the ratio $|t_{k+1}/t_k| = \\frac{|x|^2}{(2k+2)(2k+3)} \\approx 1$, which implies $2k+2 \\approx |x|$. For a large value such as $x=100$, the terms grow to an astronomical size (e.g., for $k=49$, the term is on the order of $100^{99}/99!$, which can easily exceed the range of standard double-precision floating-point numbers, $\\approx 10^{308}$).\n\n- **Catastrophic Cancellation**: The final result, $\\sin(x)$, must lie in $[-1, 1]$. The series evaluation for large $x$ involves summing very large positive and negative terms to produce a small final result. Standard floating-point numbers have a fixed number of significant digits (the mantissa). When two large, nearly equal numbers are subtracted, the leading significant digits cancel, leaving a result with far fewer significant digits of precision. This loss of relative accuracy is known as catastrophic cancellation and renders the final result meaningless.\n\nThe reduced-argument strategy circumvents these issues entirely. The argument $r$ is small ($|r| \\le \\pi/4 \\approx 0.785$), so the Maclaurin series terms $\\frac{r^k}{k!}$ decrease monotonically in magnitude from the start. There is no growth of intermediate terms and thus no catastrophic cancellation. The method is numerically stable.\n\nHowever, the argument reduction itself has a limitation. The step $r = x - q \\cdot (\\pi/2)$ can suffer from catastrophic cancellation when $x$ is extremely large. The reason is that standard floating-point representations of $\\pi$ have finite precision. For a very large $x$ (e.g., $x=10^{16}+0.1$), the value of $x$ itself may be subject to rounding in standard `float64` arithmetic (the unit in the last place for $10^{16}$ is greater than $0.1$, so $10^{16}+0.1$ is stored as exactly $10^{16}$). Even if $x$ is representable, the product $q \\cdot (\\pi/2)$ will be a large number close to $x$. The finite precision of the `float64` representation of $\\pi$ introduces an absolute error into this product that scales with $q$. When $x$ is large, $q$ is large, and this error can become significant, potentially larger than $\\pi/2$ itself. The subtraction $x - q \\cdot (\\pi/2)$ then cancels most significant digits, yielding a value for $r$ with few, if any, correct digits. This demonstrates the fundamental limit of computations with fixed-precision arithmetic for extremely large arguments. This phenomenon is expected to cause the test case for $x = 10^{16} + 0.1$ to fail a precision check.\n\nThe provided program implements these principles to compute the sine function and analyze its numerical properties across the specified test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\ndef get_required_terms_sin(z_abs, ε):\n    \"\"\"\n    Calculates the minimum number of terms T for the sin Maclaurin series.\n    The error bound after T terms (degree 2T-1) is |z|^(2T+1)/(2T+1)!.\n    \"\"\"\n    if z_abs == 0.0:\n        return 1\n    \n    z2 = z_abs * z_abs\n    \n    # Bound for T=1 term (degree 1)\n    bound = (z_abs**3) / 6.0\n    T = 1\n    \n    denom_fac1 = 4\n    denom_fac2 = 5\n    \n    while bound > ε:\n        T += 1\n        bound *= z2 / (denom_fac1 * denom_fac2)\n        denom_fac1 += 2\n        denom_fac2 += 2\n        if T > 1000: # Safety break\n            return T\n            \n    return T\n\ndef get_required_terms_cos(z_abs, ε):\n    \"\"\"\n    Calculates the minimum number of terms T for the cos Maclaurin series.\n    The error bound after T terms (degree 2T-2) is |z|^(2T)/(2T)!.\n    \"\"\"\n    if z_abs == 0.0:\n        return 1\n    \n    z2 = z_abs * z_abs\n    \n    # Bound for T=1 term (degree 0)\n    bound = z2 / 2.0\n    T = 1\n    \n    denom_fac1 = 3\n    denom_fac2 = 4\n\n    while bound > ε:\n        T += 1\n        bound *= z2 / (denom_fac1 * denom_fac2)\n        denom_fac1 += 2\n        denom_fac2 += 2\n        if T > 1000: # Safety break\n            return T\n            \n    return T\n\ndef eval_sin_series(z, T):\n    \"\"\"Evaluates the sin Maclaurin series for T terms.\"\"\"\n    if z == 0.0:\n        return 0.0\n\n    z2 = z * z\n    term = z\n    total = term\n    for k in range(1, T):\n        # term_k = -term_{k-1} * z^2 / ((2k)(2k+1))\n        term *= -z2 / ((2 * k) * (2 * k + 1))\n        total += term\n    return total\n\ndef eval_cos_series(z, T):\n    \"\"\"Evaluates the cos Maclaurin series for T terms.\"\"\"\n    z2 = z * z\n    term = 1.0\n    total = term\n    for k in range(1, T):\n        # term_k = -term_{k-1} * z^2 / ((2k-1)(2k))\n        term *= -z2 / ((2 * k - 1) * (2 * k))\n        total += term\n    return total\n\ndef compute_sin_reduced(x, ε):\n    \"\"\"\n    Computes sin(x) using argument reduction and Maclaurin series.\n    Returns the computed value and the number of terms used.\n    \"\"\"\n    pi_over_2 = np.pi / 2.0\n    \n    # Argument reduction\n    q_float = x / pi_over_2\n    q = np.round(q_float)\n    r = x - q * pi_over_2\n    \n    q_int = int(q)\n    quadrant = q_int % 4\n    \n    r_abs = abs(r)\n\n    if quadrant == 0:  # sin(r)\n        T = get_required_terms_sin(r_abs, ε)\n        val = eval_sin_series(r, T)\n        return val, T\n    elif quadrant == 1:  # cos(r)\n        T = get_required_terms_cos(r_abs, ε)\n        val = eval_cos_series(r, T)\n        return val, T\n    elif quadrant == 2:  # -sin(r)\n        T = get_required_terms_sin(r_abs, ε)\n        val = eval_sin_series(r, T)\n        return -val, T\n    else:  # quadrant == 3, -cos(r)\n        T = get_required_terms_cos(r_abs, ε)\n        val = eval_cos_series(r, T)\n        return -val, T\n\ndef check_naive_impractical(x, ε, T_max):\n    \"\"\"\n    Checks if a naive Maclaurin series evaluation of sin(x) is impractical.\n    Impractical if > T_max terms are needed or if intermediate terms overflow.\n    \"\"\"\n    x_abs = abs(x)\n    if x_abs == 0.0:\n        return False\n        \n    x2 = x_abs * x_abs\n    \n    # Check terms and remainder bound iteratively for T = 1, 2, ...\n    \n    # T=1 term magnitude (|x|)\n    term_mag = x_abs\n    if np.isinf(term_mag):\n        return True # Overflow\n\n    # Remainder bound for T=1 term\n    remainder_bound = term_mag * x2 / 6.0\n    if remainder_bound = ε:\n        return False # Practical\n    \n    for T in range(2, T_max + 1):\n        # Magnitude of the T-th term\n        # term_mag(T) = term_mag(T-1) * x^2 / ((2T-2)*(2T-1))\n        term_mag *= x2 / ((2*T - 2) * (2*T - 1))\n        if np.isinf(term_mag):\n            return True # Overflow of intermediate term\n\n        # Remainder bound for T terms\n        # bound(T) = term_mag(T) * x^2 / ((2T)*(2T+1))\n        remainder_bound = term_mag * x2 / ((2*T) * (2*T + 1))\n        if np.isinf(remainder_bound):\n            # This can happen if term_mag is huge but finite\n            # and gets multiplied by a large x2\n            return True\n            \n        if remainder_bound = ε:\n            return False # Practical, convergence within T_max terms\n\n    return True # Not converged within T_max terms\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and produce the final output.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (100.0, 1e-12),\n        (10**6 + 0.1, 1e-12),\n        (10**16 + 0.1, 1e-12),\n        (np.pi/2 + 1e-8, 1e-15),\n    ]\n\n    results = []\n    for x, ε in test_cases:\n        # 1. Compute s_approx and T using the reduced method.\n        s_approx, T = compute_sin_reduced(x, ε)\n        \n        # 2. Compute b_ok by comparing with a high-quality reference.\n        # Use np.longdouble for reference calculation where precision matters\n        ref_val = np.sin(np.longdouble(x))\n        b_ok = np.abs(s_approx - ref_val) = ε\n        \n        # 3. Assess if naive method is impractical.\n        T_max = 1000\n        b_naive_impractical = check_naive_impractical(x, ε, T_max)\n        \n        results.extend([b_ok, T, b_naive_impractical])\n\n    # Final print statement in the exact required format.\n    # Python's str() for a boolean is 'True' or 'False'.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2442233"}]}