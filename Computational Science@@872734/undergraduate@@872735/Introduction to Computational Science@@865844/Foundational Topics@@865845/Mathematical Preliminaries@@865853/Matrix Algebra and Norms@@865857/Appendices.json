{"hands_on_practices": [{"introduction": "The familiar rules for computing the matrix $1$-norm and $\\infty$-norm—taking the maximum absolute column or row sum—are more than just convenient shortcuts. This first practice invites you to derive these formulas directly from the fundamental definition of an induced norm, strengthening your conceptual foundation. By then applying this understanding to a block matrix [@problem_id:3158838], you will explore how the norm of a large matrix can be bounded by the norms of its constituent blocks, a powerful technique in large-scale numerical analysis.", "problem": "Consider the block matrix $A \\in \\mathbb{R}^{4 \\times 4}$ partitioned into $2 \\times 2$ blocks,\n$$\nA \\;=\\; \\begin{pmatrix}\nB  C \\\\\nD  E\n\\end{pmatrix},\n\\quad\nB \\;=\\; \\begin{pmatrix} 1  -2 \\\\ 3  0 \\end{pmatrix},\\;\nC \\;=\\; \\begin{pmatrix} -1  4 \\\\ 2  -3 \\end{pmatrix},\\;\nD \\;=\\; \\begin{pmatrix} 0  5 \\\\ -4  1 \\end{pmatrix},\\;\nE \\;=\\; \\begin{pmatrix} 2  -1 \\\\ 0  3 \\end{pmatrix}.\n$$\nFor a vector $x \\in \\mathbb{R}^{n}$, the vector $1$-norm is defined by $\\|x\\|_{1} = \\sum_{i=1}^{n} |x_{i}|$ and the vector $\\infty$-norm is defined by $\\|x\\|_{\\infty} = \\max_{1 \\leq i \\leq n} |x_{i}|$. For a matrix $A \\in \\mathbb{R}^{n \\times n}$ and a given vector norm $\\|\\cdot\\|$, the induced operator norm (also called subordinate norm) is\n$$\n\\|A\\| \\;=\\; \\sup_{x \\neq 0} \\frac{\\|A x\\|}{\\|x\\|}.\n$$\nTasks:\n- Using only these core definitions and the triangle inequality, derive expressions that allow you to compute $\\|A\\|_{1}$ and $\\|A\\|_{\\infty}$ exactly, and then compute their numerical values for the matrix $A$ given above.\n- Let $x \\in \\mathbb{R}^{4}$ be partitioned conformally with the block structure of $A$ as $x = \\begin{pmatrix} x_{1} \\\\ x_{2} \\end{pmatrix}$ with $x_{1}, x_{2} \\in \\mathbb{R}^{2}$. Explain, starting from the same core definitions and basic inequalities, how the block structure of $A$ leads to induced norm bounds on $\\|A x\\|_{1}$ and $\\|A x\\|_{\\infty}$ in terms of the induced norms of the blocks $B, C, D, E$ and the norms of $x_{1}, x_{2}$. Illustrate your explanation by forming a $2 \\times 2$ scalar matrix whose entries are induced norms of the blocks and show how its induced norm bounds $\\|A x\\|$.\n\nProvide exact values for $\\|A\\|_{1}$ and $\\|A\\|_{\\infty}$. No rounding is required.", "solution": "The problem statement has been validated and is deemed sound. It is a well-posed problem in introductory computational science, specifically within the topic of matrix algebra and norms. All definitions are standard, and the required tasks are clear and mathematically formalizable.\n\nThe full matrix $A \\in \\mathbb{R}^{4 \\times 4}$ is constructed by assembling its constituent blocks:\n$$\nA \\;=\\; \\begin{pmatrix}\nB  C \\\\\nD  E\n\\end{pmatrix} \\;=\\; \\begin{pmatrix}\n1  -2  -1  4 \\\\\n3  0  2  -3 \\\\\n0  5  2  -1 \\\\\n-4  1  0  3\n\\end{pmatrix}.\n$$\n\n### Derivation and Computation of the Matrix $1$-norm, $\\|A\\|_{1}$\n\nThe induced matrix $1$-norm is defined as $\\|A\\|_{1} = \\sup_{x \\neq 0} \\frac{\\|A x\\|_{1}}{\\|x\\|_{1}}$. Let $y = Ax$. The $i$-th component of $y$ is $y_i = \\sum_{j=1}^{4} A_{ij} x_j$.\nThe vector $1$-norm of $y=Ax$ is given by $\\|Ax\\|_{1} = \\sum_{i=1}^{4} |y_i| = \\sum_{i=1}^{4} \\left| \\sum_{j=1}^{4} A_{ij} x_j \\right|$.\nApplying the triangle inequality for sums of scalars, $|\\sum_j a_j| \\le \\sum_j |a_j|$, we get:\n$$\n\\|Ax\\|_{1} \\le \\sum_{i=1}^{4} \\sum_{j=1}^{4} |A_{ij} x_j| = \\sum_{i=1}^{4} \\sum_{j=1}^{4} |A_{ij}| |x_j|.\n$$\nBy changing the order of summation, we can group terms by $|x_j|$:\n$$\n\\|Ax\\|_{1} \\le \\sum_{j=1}^{4} \\left( |x_j| \\sum_{i=1}^{4} |A_{ij}| \\right).\n$$\nLet $C_j = \\sum_{i=1}^{4} |A_{ij}|$ be the sum of the absolute values of the entries in the $j$-th column of $A$. Let $C_{\\max} = \\max_{1 \\le j \\le 4} C_j$ be the maximum absolute column sum. We can bound each $C_j$ by $C_{\\max}$:\n$$\n\\|Ax\\|_{1} \\le \\sum_{j=1}^{4} |x_j| C_j \\le \\sum_{j=1}^{4} |x_j| C_{\\max} = C_{\\max} \\sum_{j=1}^{4} |x_j| = C_{\\max} \\|x\\|_{1}.\n$$\nThis implies that for any non-zero $x$, $\\frac{\\|A x\\|_{1}}{\\|x\\|_{1}} \\le C_{\\max}$, and therefore $\\|A\\|_{1} \\le C_{\\max}$.\n\nTo show that this bound is achieved, we must find a vector $x$ for which the equality holds. Let $k$ be the index of the column with the maximum absolute sum, such that $C_k = C_{\\max}$. Consider the standard basis vector $x = e_k$, where $(e_k)_j = \\delta_{kj}$. For this vector, $\\|x\\|_{1} = \\sum_{j=1}^{4} |(e_k)_j| = 1$. The product $Ax = Ae_k$ is the $k$-th column of $A$.\nThe $1$-norm of this product is:\n$$\n\\|Ae_k\\|_{1} = \\sum_{i=1}^{4} |(Ae_k)_i| = \\sum_{i=1}^{4} |A_{ik}| = C_k = C_{\\max}.\n$$\nFor this specific choice of $x$, we have $\\frac{\\|A x\\|_{1}}{\\|x\\|_{1}} = \\frac{C_{\\max}}{1} = C_{\\max}$. Since we found a vector that achieves the upper bound, the supremum must be this value. Thus, the expression for the matrix $1$-norm is the maximum absolute column sum:\n$$\n\\|A\\|_{1} = \\max_{1 \\le j \\le 4} \\sum_{i=1}^{4} |A_{ij}|.\n$$\nFor the given matrix $A$, the absolute column sums are:\n\\begin{itemize}\n    \\item Column $1$: $|1| + |3| + |0| + |-4| = 1 + 3 + 0 + 4 = 8$.\n    \\item Column $2$: $|-2| + |0| + |5| + |1| = 2 + 0 + 5 + 1 = 8$.\n    \\item Column $3$: $|-1| + |2| + |2| + |0| = 1 + 2 + 2 + 0 = 5$.\n    \\item Column $4$: $|4| + |-3| + |-1| + |3| = 4 + 3 + 1 + 3 = 11$.\n\\end{itemize}\nThe maximum of these values is $11$. Therefore, $\\|A\\|_{1} = 11$.\n\n### Derivation and Computation of the Matrix $\\infty$-norm, $\\|A\\|_{\\infty}$\n\nThe induced matrix $\\infty$-norm is defined as $\\|A\\|_{\\infty} = \\sup_{x \\neq 0} \\frac{\\|A x\\|_{\\infty}}{\\|x\\|_{\\infty}}$. The vector $\\infty$-norm of $y=Ax$ is $\\|Ax\\|_{\\infty} = \\max_{1 \\le i \\le 4} |y_i| = \\max_{1 \\le i \\le 4} \\left| \\sum_{j=1}^{4} A_{ij} x_j \\right|$.\nFor any specific row $i$, we apply the triangle inequality:\n$$\n\\left| \\sum_{j=1}^{4} A_{ij} x_j \\right| \\le \\sum_{j=1}^{4} |A_{ij} x_j| = \\sum_{j=1}^{4} |A_{ij}| |x_j|.\n$$\nBy definition, $|x_j| \\le \\max_{1 \\le k \\le 4} |x_k| = \\|x\\|_{\\infty}$ for all $j$. Thus:\n$$\n\\left| \\sum_{j=1}^{4} A_{ij} x_j \\right| \\le \\sum_{j=1}^{4} |A_{ij}| \\|x\\|_{\\infty} = \\|x\\|_{\\infty} \\sum_{j=1}^{4} |A_{ij}|.\n$$\nLet $R_i = \\sum_{j=1}^{4} |A_{ij}|$ be the sum of the absolute values of the entries in the $i$-th row. Then $|(Ax)_i| \\le R_i \\|x\\|_{\\infty}$. Since this holds for all $i$, it must also hold for the maximum component:\n$$\n\\|Ax\\|_{\\infty} = \\max_{1 \\le i \\le 4} |(Ax)_i| \\le \\max_{1 \\le i \\le 4} (R_i \\|x\\|_{\\infty}) = \\left(\\max_{1 \\le i \\le 4} R_i\\right) \\|x\\|_{\\infty}.\n$$\nLet $R_{\\max} = \\max_{1 \\le i \\le 4} R_i$. Then $\\frac{\\|A x\\|_{\\infty}}{\\|x\\|_{\\infty}} \\le R_{\\max}$, which implies $\\|A\\|_{\\infty} \\le R_{\\max}$.\n\nTo show this bound is achieved, let $k$ be the index of the row with the maximum absolute sum, $R_k = R_{\\max}$. We construct a vector $x \\in \\mathbb{R}^4$ with components $x_j = \\text{sgn}(A_{kj})$, where $\\text{sgn}(z) = 1$ if $z>0$, $-1$ if $z0$, and we can define $\\text{sgn}(0)=1$ for simplicity. For this vector $x$, all components have absolute value $|x_j| \\le 1$, and at least one component has absolute value $1$ (unless the $k$-th row is all zeros, a trivial case). Therefore, $\\|x\\|_{\\infty} = 1$.\nNow, consider the $k$-th component of the product $Ax$:\n$$\n(Ax)_k = \\sum_{j=1}^{4} A_{kj} x_j = \\sum_{j=1}^{4} A_{kj} \\text{sgn}(A_{kj}) = \\sum_{j=1}^{4} |A_{kj}| = R_k = R_{\\max}.\n$$\nThe $\\infty$-norm of $Ax$ must be at least the absolute value of its $k$-th component:\n$$\n\\|Ax\\|_{\\infty} = \\max_{1 \\le i \\le 4} |(Ax)_i| \\ge |(Ax)_k| = R_{\\max}.\n$$\nCombined with the earlier inequality $\\|Ax\\|_{\\infty} \\le R_{\\max}\\|x\\|_{\\infty} = R_{\\max}$ (since $\\|x\\|_\\infty=1$), we must have $\\|Ax\\|_{\\infty} = R_{\\max}$.\nFor this specific $x$, $\\frac{\\|A x\\|_{\\infty}}{\\|x\\|_{\\infty}} = \\frac{R_{\\max}}{1} = R_{\\max}$. The supremum is therefore $R_{\\max}$. The expression for the matrix $\\infty$-norm is the maximum absolute row sum:\n$$\n\\|A\\|_{\\infty} = \\max_{1 \\le i \\le 4} \\sum_{j=1}^{4} |A_{ij}|.\n$$\nFor the given matrix $A$, the absolute row sums are:\n\\begin{itemize}\n    \\item Row $1$: $|1| + |-2| + |-1| + |4| = 1 + 2 + 1 + 4 = 8$.\n    \\item Row $2$: $|3| + |0| + |2| + |-3| = 3 + 0 + 2 + 3 = 8$.\n    \\item Row $3$: $|0| + |5| + |2| + |-1| = 0 + 5 + 2 + 1 = 8$.\n    \\item Row $4$: $|-4| + |1| + |0| + |3| = 4 + 1 + 0 + 3 = 8$.\n\\end{itemize}\nAll absolute row sums are equal to $8$. The maximum is $8$. Therefore, $\\|A\\|_{\\infty} = 8$.\n\n### Block Matrix Norm Bounds\n\nLet the vector $x \\in \\mathbb{R}^4$ be partitioned as $x = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$, where $x_1, x_2 \\in \\mathbb{R}^2$. The matrix-vector product $Ax$ can be written in block form:\n$$\nAx = \\begin{pmatrix} B  C \\\\ D  E \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} Bx_1 + Cx_2 \\\\ Dx_1 + Ex_2 \\end{pmatrix}.\n$$\nWe analyze the $1$-norm and $\\infty$-norm of $Ax$. First, we compute the norms of the individual blocks using the formulas derived above.\n\\begin{itemize}\n    \\item $\\|B\\|_{1} = \\max(|1|+|3|, |-2|+|0|) = 4$.\n    \\item $\\|C\\|_{1} = \\max(|-1|+|2|, |4|+|-3|) = 7$.\n    \\item $\\|D\\|_{1} = \\max(|0|+|-4|, |5|+|1|) = 6$.\n    \\item $\\|E\\|_{1} = \\max(|2|+|0|, |-1|+|3|) = 4$.\n\\end{itemize}\n\\begin{itemize}\n    \\item $\\|B\\|_{\\infty} = \\max(|1|+|-2|, |3|+|0|) = 3$.\n    \\item $\\|C\\|_{\\infty} = \\max(|-1|+|4|, |2|+|-3|) = 5$.\n    \\item $\\|D\\|_{\\infty} = \\max(|0|+|5|, |-4|+|1|) = 5$.\n    \\item $\\|E\\|_{\\infty} = \\max(|2|+|-1|, |0|+|3|) = 3$.\n\\end{itemize}\n\n**Bound for $\\|Ax\\|_{\\infty}$:**\nThe vector $\\infty$-norm of a block vector $\\begin{pmatrix} u \\\\ v \\end{pmatrix}$ is $\\max(\\|u\\|_{\\infty}, \\|v\\|_{\\infty})$.\n$$\n\\|Ax\\|_{\\infty} = \\left\\| \\begin{pmatrix} Bx_1 + Cx_2 \\\\ Dx_1 + Ex_2 \\end{pmatrix} \\right\\|_{\\infty} = \\max\\left( \\|Bx_1 + Cx_2\\|_{\\infty}, \\|Dx_1 + Ex_2\\|_{\\infty} \\right).\n$$\nUsing the triangle inequality and the definition of induced norms for each block:\n\\begin{align*}\n\\|Bx_1 + Cx_2\\|_{\\infty} \\le \\|Bx_1\\|_{\\infty} + \\|Cx_2\\|_{\\infty} \\le \\|B\\|_{\\infty}\\|x_1\\|_{\\infty} + \\|C\\|_{\\infty}\\|x_2\\|_{\\infty}. \\\\\n\\|Dx_1 + Ex_2\\|_{\\infty} \\le \\|Dx_1\\|_{\\infty} + \\|Ex_2\\|_{\\infty} \\le \\|D\\|_{\\infty}\\|x_1\\|_{\\infty} + \\|E\\|_{\\infty}\\|x_2\\|_{\\infty}.\n\\end{align*}\nCombining these, we get:\n$$\n\\|Ax\\|_{\\infty} \\le \\max\\left( \\|B\\|_{\\infty}\\|x_1\\|_{\\infty} + \\|C\\|_{\\infty}\\|x_2\\|_{\\infty}, \\|D\\|_{\\infty}\\|x_1\\|_{\\infty} + \\|E\\|_{\\infty}\\|x_2\\|_{\\infty} \\right).\n$$\nThis expression can be related to a $2 \\times 2$ matrix of norms. Let $\\mathcal{A}_{\\infty} = \\begin{pmatrix} \\|B\\|_{\\infty}  \\|C\\|_{\\infty} \\\\ \\|D\\|_{\\infty}  \\|E\\|_{\\infty} \\end{pmatrix}$ and $\\chi_{\\infty} = \\begin{pmatrix} \\|x_1\\|_{\\infty} \\\\ \\|x_2\\|_{\\infty} \\end{pmatrix}$. The right side of the inequality is precisely the $\\infty$-norm of the product $\\mathcal{A}_{\\infty}\\chi_{\\infty}$.\n$$\n\\|Ax\\|_{\\infty} \\le \\|\\mathcal{A}_{\\infty}\\chi_{\\infty}\\|_{\\infty}.\n$$\nUsing the induced norm property for $\\mathcal{A}_{\\infty}$, we have $\\|\\mathcal{A}_{\\infty}\\chi_{\\infty}\\|_{\\infty} \\le \\|\\mathcal{A}_{\\infty}\\|_{\\infty}\\|\\chi_{\\infty}\\|_{\\infty}$. Furthermore, $\\|\\chi_{\\infty}\\|_{\\infty} = \\max(\\|x_1\\|_{\\infty}, \\|x_2\\|_{\\infty}) = \\|x\\|_{\\infty}$. This gives the final bound on $\\|Ax\\|_{\\infty}$:\n$$\n\\|Ax\\|_{\\infty} \\le \\|\\mathcal{A}_{\\infty}\\|_{\\infty} \\|x\\|_{\\infty}.\n$$\nThis further implies a bound on the matrix norm itself: $\\|A\\|_{\\infty} \\le \\|\\mathcal{A}_{\\infty}\\|_{\\infty}$. For our specific matrix, $\\mathcal{A}_{\\infty} = \\begin{pmatrix} 3  5 \\\\ 5  3 \\end{pmatrix}$. Its $\\infty$-norm is $\\|\\mathcal{A}_{\\infty}\\|_{\\infty} = \\max(3+5, 5+3) = 8$. Thus, $\\|A\\|_{\\infty} \\le 8$, which is consistent with our exact calculation.\n\n**Bound for $\\|Ax\\|_{1}$:**\nThe vector $1$-norm of a block vector $\\begin{pmatrix} u \\\\ v \\end{pmatrix}$ is $\\|u\\|_{1} + \\|v\\|_{1}$.\n$$\n\\|Ax\\|_{1} = \\left\\| \\begin{pmatrix} Bx_1 + Cx_2 \\\\ Dx_1 + Ex_2 \\end{pmatrix} \\right\\|_{1} = \\|Bx_1 + Cx_2\\|_{1} + \\|Dx_1 + Ex_2\\|_{1}.\n$$\nUsing the triangle inequality and the definition of induced norms:\n$$\n\\|Ax\\|_{1} \\le (\\|Bx_1\\|_{1} + \\|Cx_2\\|_{1}) + (\\|Dx_1\\|_{1} + \\|Ex_2\\|_{1}) \\le (\\|B\\|_{1}\\|x_1\\|_{1} + \\|C\\|_{1}\\|x_2\\|_{1}) + (\\|D\\|_{1}\\|x_1\\|_{1} + \\|E\\|_{1}\\|x_2\\|_{1}).\n$$\nLet $\\mathcal{A}_{1} = \\begin{pmatrix} \\|B\\|_{1}  \\|C\\|_{1} \\\\ \\|D\\|_{1}  \\|E\\|_{1} \\end{pmatrix}$ and $\\chi_{1} = \\begin{pmatrix} \\|x_1\\|_{1} \\\\ \\|x_2\\|_{1} \\end{pmatrix}$. The right-hand side can be recognized as the $1$-norm of the product $\\mathcal{A}_{1}\\chi_{1}$, since all quantities $\\|B\\|_{1}, \\|x_1\\|_{1}$, etc., are non-negative.\n$$\n\\|Ax\\|_{1} \\le (\\|B\\|_{1}\\|x_1\\|_{1} + \\|C\\|_{1}\\|x_2\\|_{1}) + (\\|D\\|_{1}\\|x_1\\|_{1} + \\|E\\|_{1}\\|x_2\\|_{1}) = \\|\\mathcal{A}_{1}\\chi_{1}\\|_{1}.\n$$\nUsing the induced norm property for $\\mathcal{A}_{1}$, we have $\\|\\mathcal{A}_{1}\\chi_{1}\\|_{1} \\le \\|\\mathcal{A}_{1}\\|_{1}\\|\\chi_{1}\\|_{1}$. Also, $\\|\\chi_{1}\\|_{1} = \\|x_1\\|_{1} + \\|x_2\\|_{1} = \\|x\\|_{1}$. This gives the bound:\n$$\n\\|Ax\\|_{1} \\le \\|\\mathcal{A}_{1}\\|_{1} \\|x\\|_{1}.\n$$\nThis implies the matrix norm bound $\\|A\\|_{1} \\le \\|\\mathcal{A}_{1}\\|_{1}$. For our specific matrix, $\\mathcal{A}_{1} = \\begin{pmatrix} 4  7 \\\\ 6  4 \\end{pmatrix}$. Its $1$-norm is $\\|\\mathcal{A}_{1}\\|_{1} = \\max(4+6, 7+4) = \\max(10, 11) = 11$. Thus, $\\|A\\|_{1} \\le 11$, which is also consistent with our exact calculation.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n11  8\n\\end{pmatrix}\n}\n$$", "id": "3158838"}, {"introduction": "Different matrix norms provide different perspectives on a matrix's \"size\" or amplifying power. A key result connecting the most common induced norms is the inequality $\\|A\\|_2 \\le \\sqrt{\\|A\\|_1 \\|A\\|_\\infty}$. This practice delves into the meaning of this bound by asking you to investigate when it is tight and when it is loose [@problem_id:3158812]. By analyzing two matrices with starkly contrasting structures, you will gain a more profound intuition for what each norm measures and how these measures relate to one another.", "problem": "In introduction to computational science, matrix norms quantify how linear transformations amplify vectors measured in different ways. For a real matrix $A \\in \\mathbb{R}^{m \\times n}$ and a vector $x \\in \\mathbb{R}^{n}$, the induced matrix norm from a vector norm $\\|\\cdot\\|$ is defined by $\\|A\\| = \\sup_{x \\neq 0} \\frac{\\|Ax\\|}{\\|x\\|}$. Consider the following three induced norms: the $1$-norm ($\\|x\\|_{1} = \\sum_{i=1}^{n} |x_{i}|$), the infinity norm ($\\|x\\|_{\\infty} = \\max_{1 \\leq i \\leq n} |x_{i}|$), and the $2$-norm ($\\|x\\|_{2} = \\sqrt{\\sum_{i=1}^{n} x_{i}^{2}}$). The corresponding induced matrix norms are denoted by $\\|A\\|_{1}$, $\\|A\\|_{\\infty}$, and $\\|A\\|_{2}$. Define the tightness ratio\n$$\nR(A) \\;=\\; \\frac{\\|A\\|_{2}}{\\sqrt{\\|A\\|_{1}\\,\\|A\\|_{\\infty}}}.\n$$\nStarting from the definitions above and standard facts about eigenvalues of $A^{\\top}A$, compute the exact values of $R(A)$ for the following two matrices:\n\n1. Let $E \\in \\mathbb{R}^{m \\times n}$ have a single nonzero entry $E_{11} = s$ with $s>0$ and all other entries equal to $0$. Compute $R(E)$.\n\n2. Let $H_{n} \\in \\mathbb{R}^{n \\times n}$ be a matrix whose $n$ columns are pairwise orthogonal and every entry is either $+1$ or $-1$. Compute $R(H_{n})$ in closed form as a function of $n$, and then evaluate this ratio for $n=16$.\n\nReport your final answers for the ordered pair $\\left(R(E),\\,R(H_{16})\\right)$ as exact values. No rounding is required.", "solution": "The solution proceeds by calculating the required norms for each matrix and then computing the tightness ratio $R(A)$.\n\n**Part 1: Calculation for Matrix $E$**\n\nThe matrix $E \\in \\mathbb{R}^{m \\times n}$ has only one non-zero entry, $E_{11} = s > 0$. All other entries $E_{ij}$ are $0$.\n\n1.  **Calculation of $\\|E\\|_{1}$**: The induced $1$-norm of a matrix is the maximum absolute column sum.\n    $$\n    \\|E\\|_{1} = \\max_{1 \\le j \\le n} \\sum_{i=1}^{m} |E_{ij}|\n    $$\n    The sum of absolute values for the first column ($j=1$) is $\\sum_{i=1}^{m} |E_{i1}| = |E_{11}| + \\sum_{i=2}^{m} |E_{i1}| = |s| + 0 = s$, since $s>0$.\n    For any other column ($j > 1$), the sum is $\\sum_{i=1}^{m} |E_{ij}| = 0$, as all entries are $0$.\n    The maximum of these column sums is $s$. Thus, $\\|E\\|_{1} = s$.\n\n2.  **Calculation of $\\|E\\|_{\\infty}$**: The induced $\\infty$-norm of a matrix is the maximum absolute row sum.\n    $$\n    \\|E\\|_{\\infty} = \\max_{1 \\le i \\le m} \\sum_{j=1}^{n} |E_{ij}|\n    $$\n    The sum of absolute values for the first row ($i=1$) is $\\sum_{j=1}^{n} |E_{1j}| = |E_{11}| + \\sum_{j=2}^{n} |E_{1j}| = |s| + 0 = s$.\n    For any other row ($i > 1$), the sum is $\\sum_{j=1}^{n} |E_{ij}| = 0$.\n    The maximum of these row sums is $s$. Thus, $\\|E\\|_{\\infty} = s$.\n\n3.  **Calculation of $\\|E\\|_{2}$**: The induced $2$-norm is the largest singular value of the matrix, which is the square root of the largest eigenvalue of $E^{\\top}E$. The matrix $E^{\\top}$ is an $n \\times m$ matrix with $(E^{\\top})_{11} = s$ and all other entries equal to $0$. Let's compute the product $E^{\\top}E \\in \\mathbb{R}^{n \\times n}$.\n    The entry $(k,l)$ of $E^{\\top}E$ is given by $(E^{\\top}E)_{kl} = \\sum_{i=1}^{m} (E^{\\top})_{ki}E_{il} = \\sum_{i=1}^{m} E_{ik}E_{il}$.\n    The only non-zero entry in $E$ is $E_{11}=s$. So, $E_{ik}$ is non-zero only if $i=1$ and $k=1$. Similarly, $E_{il}$ is non-zero only if $i=1$ and $l=1$.\n    Therefore, the sum is non-zero only if $k=1$ and $l=1$:\n    $(E^{\\top}E)_{11} = \\sum_{i=1}^{m} E_{i1}E_{i1} = (E_{11})^2 = s^2$.\n    All other entries $(E^{\\top}E)_{kl}$ are $0$.\n    So, $E^{\\top}E$ is a matrix with $s^2$ in the $(1,1)$ position and zeros everywhere else. The eigenvalues of this matrix are its diagonal entries, which are $s^2$ and $n-1$ zeros.\n    The maximum eigenvalue is $\\lambda_{\\max}(E^{\\top}E) = s^2$.\n    The $2$-norm is $\\|E\\|_{2} = \\sqrt{\\lambda_{\\max}(E^{\\top}E)} = \\sqrt{s^2} = s$ (since $s>0$).\n\n4.  **Calculation of $R(E)$**:\n    $$\n    R(E) = \\frac{\\|E\\|_{2}}{\\sqrt{\\|E\\|_{1}\\,\\|E\\|_{\\infty}}} = \\frac{s}{\\sqrt{s \\cdot s}} = \\frac{s}{\\sqrt{s^2}} = \\frac{s}{s} = 1.\n    $$\n\n**Part 2: Calculation for Matrix $H_n$**\n\nThe matrix $H_{n} \\in \\mathbb{R}^{n \\times n}$ has pairwise orthogonal columns, and every entry $(H_n)_{ij}$ is either $+1$ or $-1$.\n\nLet $h_j$ be the $j$-th column of $H_n$. The given properties are:\n-   $h_i^{\\top}h_j = 0$ for $i \\neq j$.\n-   The entries of $H_n$ are $(H_n)_{ij} \\in \\{+1, -1\\}$.\n\nFrom the second property, the squared Euclidean norm of any column $h_j$ is:\n$h_j^{\\top}h_j = \\|h_j\\|^2_2 = \\sum_{i=1}^{n} ((H_n)_{ij})^2 = \\sum_{i=1}^{n} (\\pm 1)^2 = \\sum_{i=1}^{n} 1 = n$.\n\nCombining these properties, we can determine the matrix product $H_n^{\\top}H_n$. The $(i,j)$-th entry of this product is $h_i^{\\top}h_j$.\n$$\n(H_n^{\\top}H_n)_{ij} = h_i^{\\top}h_j = \\begin{cases} n,  \\text{if } i=j \\\\ 0,  \\text{if } i \\neq j \\end{cases}\n$$\nThis means $H_n^{\\top}H_n = n I_n$, where $I_n$ is the $n \\times n$ identity matrix.\n\n1.  **Calculation of $\\|H_n\\|_{1}$**: The maximum absolute column sum.\n    For any column $j$, the absolute column sum is $\\sum_{i=1}^{n} |(H_n)_{ij}|$. Since every entry is $\\pm 1$, its absolute value is $1$.\n    $$\n    \\sum_{i=1}^{n} |(H_n)_{ij}| = \\sum_{i=1}^{n} 1 = n.\n    $$\n    Since this sum is $n$ for every column, the maximum is $n$. Thus, $\\|H_n\\|_{1} = n$.\n\n2.  **Calculation of $\\|H_n\\|_{\\infty}$**: The maximum absolute row sum.\n    Similarly, for any row $i$, the absolute row sum is $\\sum_{j=1}^{n} |(H_n)_{ij}| = \\sum_{j=1}^{n} 1 = n$.\n    The maximum of these identical sums is $n$. Thus, $\\|H_n\\|_{\\infty} = n$.\n\n3.  **Calculation of $\\|H_n\\|_{2}$**: The square root of the largest eigenvalue of $H_n^{\\top}H_n$.\n    As established, $H_n^{\\top}H_n = n I_n$. This is a diagonal matrix with all diagonal entries equal to $n$.\n    The eigenvalues of $n I_n$ are all equal to $n$.\n    The maximum eigenvalue is $\\lambda_{\\max}(H_n^{\\top}H_n) = n$.\n    Therefore, the $2$-norm is $\\|H_n\\|_{2} = \\sqrt{n}$.\n\n4.  **Calculation of $R(H_n)$**:\n    $$\n    R(H_n) = \\frac{\\|H_n\\|_{2}}{\\sqrt{\\|H_n\\|_{1}\\,\\|H_n\\|_{\\infty}}} = \\frac{\\sqrt{n}}{\\sqrt{n \\cdot n}} = \\frac{\\sqrt{n}}{\\sqrt{n^2}} = \\frac{\\sqrt{n}}{n} = \\frac{1}{\\sqrt{n}}.\n    $$\n\nFinally, we evaluate $R(H_{n})$ for $n=16$:\n$$\nR(H_{16}) = \\frac{1}{\\sqrt{16}} = \\frac{1}{4}.\n$$\n\nThe required ordered pair is $(R(E), R(H_{16}))$.\nWe found $R(E)=1$ and $R(H_{16}) = \\frac{1}{4}$.\nThe pair is $(1, \\frac{1}{4})$.", "answer": "$$\n\\boxed{\\begin{pmatrix} 1  \\frac{1}{4} \\end{pmatrix}}\n$$", "id": "3158812"}, {"introduction": "The spectral norm, $\\|A\\|_2$, is central to numerical analysis, but its definition via eigenvalues of $A^\\top A$ makes it challenging to compute directly. This final practice bridges this gap between theory and computation by guiding you to implement the power iteration algorithm, a cornerstone method for finding the largest eigenvalue. By coding this algorithm and testing it against a varied suite of matrices [@problem_id:3158797], you will gain a practical understanding of how this crucial norm is estimated and validated in real-world applications.", "problem": "You are to write a complete, runnable program that estimates the spectral norm of a matrix using power iteration and validates the estimate against norm-based bounds. The task must be solved from first principles appropriate to matrix algebra in an introduction to computational science setting, starting from the definitions of induced norms and basic properties of eigenvalues and singular values, without using specialized numerical linear algebra routines to directly compute singular values or eigenvalues.\n\nBegin from the following fundamental definitions and well-tested facts:\n- For a real matrix $A \\in \\mathbb{R}^{m \\times n}$, the induced $2$-norm (spectral norm) is defined by $$\\|A\\|_2 = \\max_{\\mathbf{x} \\neq \\mathbf{0}} \\frac{\\|A \\mathbf{x}\\|_2}{\\|\\mathbf{x}\\|_2}.$$ This norm equals the largest singular value of $A$.\n- Form the symmetric positive semidefinite matrix $$B = A^\\top A \\in \\mathbb{R}^{n \\times n}.$$ The largest eigenvalue of $B$ equals the square of the largest singular value of $A$. Therefore, if $\\lambda_{\\max}(B)$ denotes the largest eigenvalue of $B$, then $$\\|A\\|_2 = \\sqrt{\\lambda_{\\max}(A^\\top A)}.$$\n- The $1$-norm and infinity-norm induced on matrices are defined, respectively, by $$\\|A\\|_1 = \\max_{1 \\leq j \\leq n} \\sum_{i=1}^m |a_{ij}| \\quad \\text{and} \\quad \\|A\\|_\\infty = \\max_{1 \\leq i \\leq m} \\sum_{j=1}^n |a_{ij}|.$$\n- The following well-tested inequalities provide bounds on the spectral norm in terms of the $1$-norm and infinity-norm: $$\\|A\\|_2 \\leq \\sqrt{\\|A\\|_1 \\, \\|A\\|_\\infty},$$ and $$\\|A\\|_2 \\geq \\max\\left(\\frac{\\|A\\|_1}{\\sqrt{m}}, \\frac{\\|A\\|_\\infty}{\\sqrt{n}}\\right).$$\n\nYour program must implement power iteration on $B = A^\\top A$ to approximate $\\lambda_{\\max}(B)$ and then estimate $\\|A\\|_2$ by taking the square root of the approximation. Use a deterministic initial vector, normalize at each step by the Euclidean norm, compute the Rayleigh quotient at each iteration, and terminate when the relative change in the Rayleigh quotient falls below a tolerance or when a maximum number of iterations is reached. Make sure to handle degenerate cases such as the zero matrix where $B$ is the zero matrix.\n\nTest Suite:\nUse the following matrices $A$ in $\\mathbb{R}^{m \\times n}$ to exercise different aspects of the implementation. These are provided explicitly to ensure scientific realism and coverage.\n\n1. Happy path (rectangular, mixed signs): $$A_1 = \\begin{pmatrix} 3  -1 \\\\ 0  2 \\\\ 1  1 \\end{pmatrix}$$ where $m = 3$ and $n = 2$.\n2. Boundary case (all zeros): $$A_2 = \\begin{pmatrix} 0  0  0  0 \\\\ 0  0  0  0 \\\\ 0  0  0  0 \\\\ 0  0  0  0 \\end{pmatrix}$$ where $m = 4$ and $n = 4$.\n3. Edge case (rank-deficient, linearly dependent columns): $$A_3 = \\begin{pmatrix} 1  2  3 \\\\ 2  4  6 \\end{pmatrix}$$ where $m = 2$ and $n = 3$.\n4. Ill-conditioned symmetric positive definite case (Hilbert matrix): $$A_4 = \\left[ a_{ij} \\right]_{i,j=1}^5, \\quad a_{ij} = \\frac{1}{i + j - 1}$$ where $m = 5$ and $n = 5$.\n\nComputation and Validation Requirements for Each Test Case:\n- Estimate $\\|A\\|_2$ via power iteration applied to $B = A^\\top A$.\n- Compute the lower bound $$L = \\max\\left(\\frac{\\|A\\|_1}{\\sqrt{m}}, \\frac{\\|A\\|_\\infty}{\\sqrt{n}}\\right).$$\n- Compute the upper bound $$U = \\sqrt{\\|A\\|_1 \\, \\|A\\|_\\infty}.$$\n- Check whether the estimated $\\|A\\|_2$ lies within the interval $[L, U]$.\n\nAnswer Specification:\n- For each test case, output a list containing four values: the estimated spectral norm (rounded to eight decimal places), the lower bound $L$ (rounded to eight decimal places), the upper bound $U$ (rounded to eight decimal places), and a boolean indicating whether the estimate lies within the bounds.\n- Your program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets and with each test case represented as its own bracketed list. For example, the format should be $$\\texttt{[[est1,low1,up1,flag1],[est2,low2,up2,flag2],\\dots]}$$ with no spaces, where each $\\texttt{est}$, $\\texttt{low}$, and $\\texttt{up}$ is a decimal number rounded to eight places, and each $\\texttt{flag}$ is either $\\texttt{True}$ or $\\texttt{False}$.", "solution": "The task is to estimate the spectral norm, $\\|A\\|_2$, of a given real matrix $A \\in \\mathbb{R}^{m \\times n}$. The solution will be developed from first principles, proceeding in three stages: first, establishing the theoretical connection between the spectral norm and the eigenvalues of a related matrix; second, detailing the power iteration algorithm to approximate the required eigenvalue; and third, computing norm-based bounds to validate the numerical estimate.\n\n**1. The Spectral Norm and Eigenvalues**\n\nThe spectral norm, or induced $2$-norm, of a matrix $A \\in \\mathbb{R}^{m \\times n}$ is defined as:\n$$\n\\|A\\|_2 = \\max_{\\mathbf{x} \\neq \\mathbf{0}} \\frac{\\|A \\mathbf{x}\\|_2}{\\|\\mathbf{x}\\|_2}\n$$\nwhere $\\mathbf{x} \\in \\mathbb{R}^n$ and $\\|\\cdot\\|_2$ denotes the Euclidean vector norm. This definition is equivalent to stating that $\\|A\\|_2$ is the largest singular value of $A$, denoted $\\sigma_{\\max}(A)$.\n\nTo find $\\sigma_{\\max}(A)$ without a direct singular value decomposition, we use its relationship with the eigenvalues of the matrix $B = A^\\top A$. Let us consider the squared spectral norm:\n$$\n\\|A\\|_2^2 = \\left( \\max_{\\|\\mathbf{x}\\|_2 = 1} \\|A \\mathbf{x}\\|_2 \\right)^2 = \\max_{\\|\\mathbf{x}\\|_2 = 1} \\|A \\mathbf{x}\\|_2^2\n$$\nThe squared Euclidean norm of a vector can be written as an inner product: $\\|A \\mathbf{x}\\|_2^2 = (A \\mathbf{x})^\\top (A \\mathbf{x}) = \\mathbf{x}^\\top A^\\top A \\mathbf{x}$.\nSubstituting this into the expression gives:\n$$\n\\|A\\|_2^2 = \\max_{\\|\\mathbf{x}\\|_2 = 1} \\mathbf{x}^\\top (A^\\top A) \\mathbf{x}\n$$\nThis is the definition of the largest eigenvalue of the matrix $B = A^\\top A$. The matrix $B$ is symmetric (since $(A^\\top A)^\\top = A^\\top (A^\\top)^\\top = A^\\top A$) and positive semidefinite (since $\\mathbf{x}^\\top B \\mathbf{x} = \\|A\\mathbf{x}\\|_2^2 \\geq 0$ for all $\\mathbf{x}$). Let $\\lambda_{\\max}(B)$ be the largest eigenvalue of $B$. Then:\n$$\n\\|A\\|_2^2 = \\lambda_{\\max}(A^\\top A)\n$$\nTherefore, the spectral norm can be computed as:\n$$\n\\|A\\|_2 = \\sqrt{\\lambda_{\\max}(A^\\top A)}\n$$\nOur primary computational task is thus to find the largest eigenvalue of the symmetric matrix $B = A^\\top A$.\n\n**2. The Power Iteration Method**\n\nThe power iteration method is an algorithm for approximating the dominant eigenvalue (the eigenvalue with the largest magnitude) and its corresponding eigenvector. For a real symmetric matrix like $B = A^\\top A$, all eigenvalues are real, and the dominant eigenvalue is $\\lambda_{\\max}(B)$.\n\nThe algorithm proceeds as follows:\n1.  **Initialization**: Choose a deterministic initial vector $\\mathbf{x}_0 \\in \\mathbb{R}^n$ with $\\|\\mathbf{x}_0\\|_2 = 1$. A common choice is a normalized vector of all ones. Let $\\lambda^{(0)} = 0$.\n2.  **Iteration**: For $k = 1, 2, 3, \\dots, \\text{max_iter}$:\n    a.  **Apply Matrix**: Compute $\\mathbf{y}^{(k)} = B \\mathbf{x}^{(k-1)}$.\n    b.  **Normalize**: Compute the Euclidean norm $\\|\\mathbf{y}^{(k)}\\|_2$. If the norm is zero, it implies $\\lambda_{\\max}(B)=0$; the iteration can stop. Otherwise, normalize the vector: $\\mathbf{x}^{(k)} = \\mathbf{y}^{(k)} / \\|\\mathbf{y}^{(k)}\\|_2$.\n    c.  **Estimate Eigenvalue**: The eigenvalue is estimated using the Rayleigh quotient. Since $\\mathbf{x}^{(k)}$ is a unit vector, this simplifies to:\n        $$\n        \\lambda^{(k)} = (\\mathbf{x}^{(k)})^\\top B \\mathbf{x}^{(k)}\n        $$\n    d.  **Check Convergence**: The iteration terminates if the relative change in the eigenvalue estimate is below a specified tolerance, $\\epsilon$:\n        $$\n        \\frac{|\\lambda^{(k)} - \\lambda^{(k-1)}|}{|\\lambda^{(k)}|}  \\epsilon\n        $$\n        The process also stops if a maximum number of iterations is reached.\nUpon convergence at iteration $k$, the estimate for the largest eigenvalue is $\\lambda_{\\max}(B) \\approx \\lambda^{(k)}$.\n\nA special case is when $A$ is the zero matrix. In this situation, $B = A^\\top A$ is also the zero matrix. For any initial vector $\\mathbf{x}_0$, $B\\mathbf{x}_0=\\mathbf{0}$, so $\\lambda_{\\max}(B)=0$ and $\\|A\\|_2=0$. This case must be handled explicitly to avoid division by zero during normalization.\n\n**3. Validation Using Norm-Based Bounds**\n\nThe numerical estimate for $\\|A\\|_2$ can be validated by checking if it lies within an interval defined by other, more easily computable matrix norms. The required norms are the $1$-norm and the $\\infty$-norm:\n-   **$1$-norm (Maximum Absolute Column Sum)**: $\\|A\\|_1 = \\max_{1 \\leq j \\leq n} \\sum_{i=1}^m |a_{ij}|$\n-   **$\\infty$-norm (Maximum Absolute Row Sum)**: $\\|A\\|_\\infty = \\max_{1 \\leq i \\leq m} \\sum_{j=1}^n |a_{ij}|$\n\nThese norms provide the following bounds for the spectral norm:\n-   **Lower Bound**: $L = \\max\\left(\\frac{\\|A\\|_1}{\\sqrt{m}}, \\frac{\\|A\\|_\\infty}{\\sqrt{n}}\\right)$\n-   **Upper Bound**: $U = \\sqrt{\\|A\\|_1 \\|A\\|_\\infty}$\n\nThe estimated spectral norm, which we will call $\\text{est}_{\\|A\\|_2}$, should satisfy the inequality $L \\leq \\text{est}_{\\|A\\|_2} \\leq U$. This provides a crucial sanity check on the result of the power iteration.\n\n**Summary of the Algorithm for each test case:**\n1.  Given a matrix $A \\in \\mathbb{R}^{m \\times n}$.\n2.  Compute the matrix $1$-norm $\\|A\\|_1$ and $\\infty$-norm $\\|A\\|_\\infty$.\n3.  Calculate the lower bound $L$ and upper bound $U$ for $\\|A\\|_2$.\n4.  Handle the special case: if $A$ is the zero matrix, set the estimated spectral norm $\\text{est}_{\\|A\\|_2} = 0$.\n5.  Otherwise, form the matrix $B = A^\\top A$.\n6.  Apply the power iteration method to $B$ to find an estimate for $\\lambda_{\\max}(B)$.\n7.  Calculate the estimated spectral norm: $\\text{est}_{\\|A\\|_2} = \\sqrt{\\lambda_{\\max}(B)}$.\n8.  Verify if the estimate lies within the bounds: $L \\leq \\text{est}_{\\|A\\|_2} \\leq U$.\n9.  Collect the estimated norm, the lower bound, the upper bound, and the boolean validation result. All numerical values are to be rounded to eight decimal places.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef estimate_spectral_norm(A, tol=1e-12, max_iter=2000):\n    \"\"\"\n    Estimates the spectral norm of a matrix A using power iteration on A.T @ A.\n\n    Args:\n        A (np.ndarray): The input matrix.\n        tol (float): The tolerance for convergence.\n        max_iter (int): The maximum number of iterations.\n\n    Returns:\n        float: The estimated spectral norm of A.\n    \"\"\"\n    m, n = A.shape\n    \n    # Handle the zero matrix case, where the norm is 0.\n    if not np.any(A):\n        return 0.0\n        \n    B = A.T @ A\n    \n    # Initialize a deterministic starting vector (vector of ones).\n    # Has to be of size n for the matrix B which is n x n.\n    x = np.ones(n)\n    \n    lambda_old = 0.0\n    \n    for _ in range(max_iter):\n        # Power iteration step\n        Bx = B @ x\n        norm_Bx = np.linalg.norm(Bx)\n        \n        # If Bx is zero, the largest eigenvalue is 0.\n        # This can happen if the initial vector is orthogonal to the dominant eigenvector space\n        # or if the matrix is a zero matrix (already handled).\n        if norm_Bx == 0:\n            lambda_new = 0.0\n            break\n            \n        x = Bx / norm_Bx\n        \n        # Rayleigh quotient to estimate the eigenvalue\n        lambda_new = x.T @ B @ x\n        \n        # Check for convergence using relative error\n        if lambda_new > 0 and abs(lambda_new - lambda_old)  tol * lambda_new:\n            break\n        \n        lambda_old = lambda_new\n    else: # This else belongs to the for loop and is executed if the loop finishes without break\n        # This could be a warning in a real application, but for this problem we just return the last value.\n        pass\n\n    return np.sqrt(lambda_new)\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test suite.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # 1. Happy path (rectangular, mixed signs)\n        np.array([[3., -1.], [0., 2.], [1., 1.]]),\n        \n        # 2. Boundary case (all zeros)\n        np.array([[0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.]]),\n\n        # 3. Edge case (rank-deficient)\n        np.array([[1., 2., 3.], [2., 4., 6.]]),\n        \n        # 4. Ill-conditioned symmetric positive definite case (Hilbert matrix)\n        # Using 1-based indexing for i,j from 1 to 5 as per problem.\n        # Python uses 0-based, so a[i,j] = 1/((i+1)+(j+1)-1) = 1/(i+j+1)\n        np.array([[1.0 / (i + j + 1) for j in range(5)] for i in range(5)])\n    ]\n\n    all_results_str = []\n    \n    for A in test_cases:\n        m, n = A.shape\n        \n        # Estimate the spectral norm using power iteration\n        est_norm_2 = estimate_spectral_norm(A)\n        \n        # Compute 1-norm and infinity-norm from first principles\n        norm_1 = np.max(np.sum(np.abs(A), axis=0))\n        norm_inf = np.max(np.sum(np.abs(A), axis=1))\n\n        # Compute the lower and upper bounds\n        # Handle division by zero if m or n is zero (not in test cases, but good practice).\n        lower_bound = 0\n        if m > 0 and n > 0:\n            term1 = norm_1 / np.sqrt(m)\n            term2 = norm_inf / np.sqrt(n)\n            lower_bound = np.max([term1, term2])\n        \n        upper_bound = np.sqrt(norm_1 * norm_inf)\n        \n        # Validate if the estimate is within the bounds\n        # Using a small tolerance for floating point comparisons\n        is_valid = (est_norm_2 >= lower_bound - 1e-9) and (est_norm_2 = upper_bound + 1e-9)\n\n        # Format results for the current case\n        case_result_str = (\n            f\"[{est_norm_2:.8f},\"\n            f\"{lower_bound:.8f},\"\n            f\"{upper_bound:.8f},\"\n            f\"{is_valid}]\"\n        )\n        all_results_str.append(case_result_str.replace(\"True\", \"True\").replace(\"False\", \"False\"))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(all_results_str)}]\")\n\nsolve()\n```", "id": "3158797"}]}