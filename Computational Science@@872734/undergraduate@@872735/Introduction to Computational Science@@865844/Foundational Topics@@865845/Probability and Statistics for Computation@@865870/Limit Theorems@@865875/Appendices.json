{"hands_on_practices": [{"introduction": "The Central Limit Theorem is not just a theoretical curiosity; it is a powerful tool for making quantitative decisions in engineering and system design. This first practice exercise places you in the role of a control systems designer facing a classic trade-off: precision versus speed. By applying the CLT, you will determine the optimal number of sensor measurements needed to achieve a desired accuracy without violating a critical time deadline, providing a concrete example of how limit theorems guide real-world hardware and software configuration [@problem_id:3153047].", "problem": "A real-time controller must estimate a constant signal by averaging $n$ independent and identically distributed sensor readings before issuing an actuation command. Each reading is modeled as $X_{i}=\\theta+\\eta_{i}$, where $\\theta$ is the constant signal over the averaging window and $\\eta_{i}$ are independent noise terms with mean $0$ and standard deviation $\\sigma$. The controller uses the sample mean $\\bar{X}_{n}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}$ as the estimate. Collecting each reading requires a fixed time $\\Delta t$, so the sensing-to-actuation latency equals $n\\Delta t$. The controller must respect a hard deadline $T_{\\max}$, so $n\\Delta t\\leq T_{\\max}$. The design requirement is that, at the moment of actuation, the absolute estimation error satisfies $|\\bar{X}_{n}-\\theta|\\leq \\varepsilon$ with probability at least $p_{0}$, justified by the Law of Large Numbers (LLN) and quantified using the Central Limit Theorem (CLT) approximation for large $n$.\n\nYou are given:\n- Noise standard deviation $\\sigma=0.8$.\n- Tolerance $\\varepsilon=0.10$.\n- Target confidence $p_{0}=0.95$.\n- Per-sample time $\\Delta t=2\\times 10^{-3}$ seconds.\n- Maximum allowable latency $T_{\\max}=0.50$ seconds.\n\nStarting from the definitions of the sample mean and variance, and invoking the Central Limit Theorem to approximate the distribution of $\\bar{X}_{n}$, derive the smallest integer $n$ that simultaneously satisfies the probability requirement and the latency constraint. Report the integer value of $n$ only.", "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and all necessary information is provided. The problem is a standard application of the Central Limit Theorem (CLT) to determine the required sample size for an estimation problem, subject to a latency constraint. All parameters are provided and are physically consistent. The problem is deemed valid.\n\nThe solution proceeds as follows. Each sensor reading $X_i$ is a random variable given by $X_{i}=\\theta+\\eta_{i}$, where $\\theta$ is a constant signal and $\\eta_{i}$ are independent and identically distributed (i.i.d.) noise terms with mean $E[\\eta_i] = 0$ and standard deviation $\\sigma$.\n\nFirst, we determine the statistical properties of a single reading $X_i$:\nThe expected value of $X_i$ is $E[X_i] = E[\\theta + \\eta_i] = \\theta + E[\\eta_i] = \\theta + 0 = \\theta$.\nThe variance of $X_i$ is $Var(X_i) = Var(\\theta + \\eta_i) = Var(\\eta_i) = \\sigma^2$.\n\nThe estimate for $\\theta$ is the sample mean of $n$ readings, $\\bar{X}_{n}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}$. We determine the statistical properties of this estimator.\nThe expected value of the sample mean is $E[\\bar{X}_n] = E\\left[\\frac{1}{n}\\sum_{i=1}^{n}X_{i}\\right] = \\frac{1}{n}\\sum_{i=1}^{n}E[X_i] = \\frac{1}{n}(n\\theta) = \\theta$. This confirms that the sample mean is an unbiased estimator of $\\theta$.\n\nThe variance of the sample mean, given that the $X_i$ are independent, is:\n$$Var(\\bar{X}_n) = Var\\left(\\frac{1}{n}\\sum_{i=1}^{n}X_{i}\\right) = \\frac{1}{n^2}\\sum_{i=1}^{n}Var(X_i) = \\frac{1}{n^2}(n\\sigma^2) = \\frac{\\sigma^2}{n}$$\nThe standard deviation of the sample mean is therefore $SD(\\bar{X}_n) = \\sqrt{\\frac{\\sigma^2}{n}} = \\frac{\\sigma}{\\sqrt{n}}$.\n\nAccording to the Central Limit Theorem, for a sufficiently large sample size $n$, the distribution of the sample mean $\\bar{X}_n$ is approximately normal, with mean $\\theta$ and variance $\\frac{\\sigma^2}{n}$. We can write this as $\\bar{X}_n \\sim \\mathcal{N}\\left(\\theta, \\frac{\\sigma^2}{n}\\right)$.\nTo analyze the probability requirement, we standardize the random variable $\\bar{X}_n$ to obtain a standard normal variable $Z$:\n$$Z = \\frac{\\bar{X}_n - E[\\bar{X}_n]}{SD(\\bar{X}_n)} = \\frac{\\bar{X}_n - \\theta}{\\sigma/\\sqrt{n}}$$\nThe variable $Z$ is approximately distributed as a standard normal distribution, $Z \\sim \\mathcal{N}(0,1)$.\n\nThe design requirement is that the absolute estimation error $|\\bar{X}_n - \\theta|$ is no more than $\\varepsilon$ with a probability of at least $p_0$. This is expressed as:\n$$P(|\\bar{X}_n - \\theta| \\leq \\varepsilon) \\geq p_0$$\nWe can rewrite the inequality inside the probability as $-\\varepsilon \\leq \\bar{X}_n - \\theta \\leq \\varepsilon$. Dividing all parts of the inequality by the standard deviation of $\\bar{X}_n$ gives:\n$$-\\frac{\\varepsilon}{\\sigma/\\sqrt{n}} \\leq \\frac{\\bar{X}_n - \\theta}{\\sigma/\\sqrt{n}} \\leq \\frac{\\varepsilon}{\\sigma/\\sqrt{n}}$$\nThis is equivalent to $-\\frac{\\varepsilon\\sqrt{n}}{\\sigma} \\leq Z \\leq \\frac{\\varepsilon\\sqrt{n}}{\\sigma}$. The probability requirement becomes:\n$$P\\left(-\\frac{\\varepsilon\\sqrt{n}}{\\sigma} \\leq Z \\leq \\frac{\\varepsilon\\sqrt{n}}{\\sigma}\\right) \\geq p_0$$\nLet $\\Phi(z)$ be the cumulative distribution function (CDF) of the standard normal distribution. The probability is given by $\\Phi\\left(\\frac{\\varepsilon\\sqrt{n}}{\\sigma}\\right) - \\Phi\\left(-\\frac{\\varepsilon\\sqrt{n}}{\\sigma}\\right)$. Due to the symmetry of the normal distribution, $\\Phi(-z) = 1 - \\Phi(z)$, so the probability is $2\\Phi\\left(\\frac{\\varepsilon\\sqrt{n}}{\\sigma}\\right) - 1$.\nThe inequality is $2\\Phi\\left(\\frac{\\varepsilon\\sqrt{n}}{\\sigma}\\right) - 1 \\geq p_0$, which can be rearranged to:\n$$\\Phi\\left(\\frac{\\varepsilon\\sqrt{n}}{\\sigma}\\right) \\geq \\frac{1+p_0}{2}$$\nWe are given $p_0 = 0.95$, so we need $\\Phi\\left(\\frac{\\varepsilon\\sqrt{n}}{\\sigma}\\right) \\geq \\frac{1+0.95}{2} = 0.975$.\nLet $z_c$ be the critical value from the standard normal distribution such that $\\Phi(z_c) = 0.975$. A standard lookup gives $z_c \\approx 1.96$.\nThus, we require $\\frac{\\varepsilon\\sqrt{n}}{\\sigma} \\geq z_c$. Solving for $n$:\n$$\\sqrt{n} \\geq \\frac{\\sigma z_c}{\\varepsilon} \\implies n \\geq \\left(\\frac{\\sigma z_c}{\\varepsilon}\\right)^2$$\nSubstituting the given values $\\sigma=0.8$, $\\varepsilon=0.10$, and $z_c \\approx 1.96$:\n$$n \\geq \\left(\\frac{0.8 \\times 1.96}{0.10}\\right)^2 = \\left(\\frac{1.568}{0.10}\\right)^2 = (15.68)^2 = 245.8624$$\nSince $n$ must be an integer, the smallest number of samples that satisfies the probability requirement is $n = 246$.\n\nNext, we evaluate the latency constraint. The total time for sensing is $n\\Delta t$, which must not exceed the deadline $T_{\\max}$:\n$$n\\Delta t \\leq T_{\\max}$$\nThis imposes an upper bound on $n$:\n$$n \\leq \\frac{T_{\\max}}{\\Delta t}$$\nSubstituting the given values $\\Delta t=2\\times 10^{-3}$ seconds and $T_{\\max}=0.50$ seconds:\n$$n \\leq \\frac{0.50}{2 \\times 10^{-3}} = \\frac{0.50}{0.002} = 250$$\nSo, $n$ must be an integer less than or equal to $250$.\n\nWe must find the smallest integer $n$ that satisfies both conditions simultaneously:\n1. $n \\geq 246$ (from the probability requirement)\n2. $n \\leq 250$ (from the latency constraint)\n\nThe range of possible integer values for $n$ is $[246, 250]$. The problem asks for the smallest integer $n$ in this range.\nTherefore, the smallest valid value for $n$ is $246$.\nLet's check this value:\nFor $n=246$, the latency is $246 \\times (2 \\times 10^{-3}) = 0.492$ seconds, which is less than $T_{\\max}=0.50$ seconds.\nFor $n=246$, the condition $n \\geq 245.8624$ is met, satisfying the probability requirement.\nFor $n=245$, the probability requirement would not be met.\nThus, the smallest integer $n$ is $246$.", "answer": "$$\\boxed{246}$$", "id": "3153047"}, {"introduction": "Powerful theorems like the CLT come with important prerequisites, such as the requirement of a finite variance. This hands-on coding practice challenges you to explore what happens when these conditions are not met. By simulating a Galton-Watson branching process, you will observe how the system's behavior changes dramatically as it approaches a critical point where the variance becomes infinite, and the classical $\\sqrt{n}$ scaling of the CLT gives way to different, non-Gaussian limit laws [@problem_id:3153023]. This exercise provides crucial insight into the boundaries of the theorem and the existence of other families of limit distributions.", "problem": "You will write a complete, runnable program that uses Monte Carlo simulation to probe the Law of Large Numbers (LLN) and the Central Limit Theorem (CLT) in the context of Galton–Watson branching processes near criticality. Consider a Galton–Watson process where each individual independently produces a number of offspring distributed as a Poisson random variable with mean $m$, and the process starts with exactly $1$ ancestor. Let $T$ denote the total population size (the sum of all generation sizes until extinction, including the initial ancestor). Work in purely mathematical terms: there are no physical units. All angles, if any, must be treated in radians, but none are required here.\n\nFundamental base and definitions:\n- A Galton–Watson process is defined by independent, identically distributed offspring counts for each individual, with mean $m$ and a probability generating function $f(s)$.\n- The Law of Large Numbers states that for independent, identically distributed random variables with finite mean, the empirical average converges to the mean.\n- The Central Limit Theorem states that for independent, identically distributed random variables with finite, nonzero variance, the standardized sum converges in distribution to a standard normal distribution.\n- A stable law (also called an $\\alpha$-stable distribution) is a limit law for sums of independent, identically distributed random variables when appropriate normalization may differ from the $\\sqrt{n}$ scaling of the Central Limit Theorem, often arising when moments are infinite.\n\nYour task is to:\n1. Implement a simulator to generate samples of $T$ for given $m$ by simulating the Galton–Watson process until extinction. Use the property that the sum of independent Poisson random variables is Poisson to simulate the next generation size from a Poisson distribution with parameter equal to the current generation size multiplied by $m$. Repeat until the population becomes $0$.\n2. Use this simulator to test LLN and CLT behavior for a subcritical case $m=0.9$, and to demonstrate non-Gaussian scaling near criticality $m=1.0$, where the total population size exhibits heavy-tailed behavior and invalidates classical $\\sqrt{n}$ scaling.\n3. For the subcritical Poisson case $m<1$, you may rely on the known facts that the total size $T$ has finite mean and variance; you must compute the theoretical mean and variance for use in standardization. For critical $m=1$, you should not attempt to use nonexistent finite moments in any standardization.\n\nTest suite and required outputs:\nYour program must execute the following four test cases and output a single line containing a list of four booleans in the order described below.\n\n- Test case $1$ (CLT standardization at $m=0.9$):\n  - Parameters: $m=0.9$, block size $n=400$, number of blocks $B=400$.\n  - Construct $B$ independent block sums, each block sum being the sum of $n$ independent copies of $T$.\n  - Using the correct finite mean $\\mu$ and variance $\\sigma^2$ of $T$ for the subcritical Poisson case, form standardized block sums $Z_j = \\dfrac{S_j - n\\mu}{\\sqrt{n\\sigma^2}}$, where $S_j$ is the $j$-th block sum.\n  - Compute the empirical mean $\\overline{Z}$ and empirical variance $s_Z^2$ across the $B$ standardized block sums.\n  - Output boolean $\\mathrm{result}_1$ that is true if both $|\\overline{Z}| \\le 0.15$ and $|s_Z^2 - 1| \\le 0.2$, and false otherwise.\n\n- Test case $2$ (CLT scaling consistency at $m=0.9$):\n  - Parameters: $m=0.9$, block sizes $n_1=200$ and $n_2=800$, with $B=300$ blocks for each $n$.\n  - For each $n \\in \\{n_1,n_2\\}$, compute the $B$ block sums and let $s_n$ be the empirical standard deviation of block sums divided by $\\sqrt{n}$.\n  - Output boolean $\\mathrm{result}_2$ that is true if the two estimates agree within $10\\%$, i.e., $\\left| s_{n_1} - s_{n_2} \\right| \\le 0.1 \\times s_{n_1}$, and false otherwise.\n\n- Test case $3$ (non-Gaussian scaling growth at $m=1.0$):\n  - Parameters: $m=1.0$, block sizes $n_1=200$ and $n_2=600$, with $B=200$ blocks for each $n$.\n  - For each $n \\in \\{n_1,n_2\\}$, compute $B$ block sums and define $g_n$ as the empirical standard deviation of block sums divided by $\\sqrt{n}$.\n  - Output boolean $\\mathrm{result}_3$ that is true if $g_{n_2} > 1.5 \\times g_{n_1}$, indicating growth inconsistent with classical $\\sqrt{n}$ scaling, and false otherwise.\n\n- Test case $4$ (extreme-concentration diagnostic at $m=1.0$):\n  - Parameters: $m=1.0$, number of independent outbreaks $K=15000$, top-fraction $q=0.01$.\n  - Generate $K$ independent samples $T_1,\\dots,T_K$. Let $S=\\sum_{i=1}^K T_i$, and let $S_{\\mathrm{top}}$ be the sum of the largest $\\lceil qK \\rceil$ values among $\\{T_i\\}$.\n  - Output boolean $\\mathrm{result}_4$ that is true if the concentration share $C = S_{\\mathrm{top}} / S \\ge 0.5$, and false otherwise.\n\nImplementation requirements:\n- Use a fixed random seed equal to $42$ to ensure reproducibility.\n- Your program should print exactly one line containing the results as a comma-separated list of four booleans enclosed in square brackets, in the order $[\\mathrm{result}_1,\\mathrm{result}_2,\\mathrm{result}_3,\\mathrm{result}_4]$.\n- No user input or external files are allowed.\n\nHints are forbidden. You must not include any formulas or intermediate results that directly solve the core derivations in this problem statement. The scientific setup above is realistic and standard for branching processes and limit theorems. Ensure your implementation uses the specified parameters as a built-in test suite and produces quantifiable boolean outputs in the required single-line format.", "solution": "The problem is valid. It is scientifically grounded in the theory of stochastic processes, specifically Galton-Watson branching processes, and its connection to fundamental limit theorems in probability theory. The problem is well-posed, with all necessary parameters and definitions provided for a unique computational solution. The language is objective and the tasks are verifiable through simulation.\n\nThe objective is to use Monte Carlo simulation to explore the Law of Large Numbers (LLN) and the Central Limit Theorem (CLT) for the total population size, $T$, of a Galton-Watson process. We will contrast the behavior of a subcritical process ($m=0.9$) where classical limit theorems apply, with a critical process ($m=1.0$) where they do not, leading to non-Gaussian scaling typical of stable laws.\n\nThe process starts with a single ancestor, $Z_0=1$. Each individual in generation $k$, of size $Z_k$, produces a number of offspring drawn independently from a Poisson distribution with mean $m$. The size of the next generation, $Z_{k+1}$, is the sum of offspring from all $Z_k$ individuals. Due to the additive property of the Poisson distribution, $Z_{k+1}$ follows a Poisson distribution with mean $m \\cdot Z_k$. The process continues until the population goes extinct, i.e., $Z_k=0$ for some $k$. The total population size is $T = \\sum_{k=0}^{\\infty} Z_k$.\n\nFor the subcritical case where the mean number of offspring $m<1$, the process is guaranteed to go extinct. The total population size $T$ is a random variable with finite mean $\\mu$ and finite variance $\\sigma^2$. For a Poisson offspring distribution, these are given by:\n$$ \\mu = E[T] = \\frac{1}{1-m} $$\n$$ \\sigma^2 = \\mathrm{Var}(T) = \\frac{m}{(1-m)^3} $$\nFor $m=0.9$, we have $\\mu = \\frac{1}{1-0.9} = 10$ and $\\sigma^2 = \\frac{0.9}{(1-0.9)^3} = 900$. Because these moments are finite, the sum of a large number $n$ of independent and identically distributed (i.i.d.) copies of $T$ will obey the classical LLN and CLT.\n\nFor the critical case where $m=1$, the process is also guaranteed to go extinct, but the expected time to extinction is infinite. The distribution of the total population size $T$ is heavy-tailed, specifically with a power-law tail such that its mean $E[T]$ is infinite. Consequently, its variance is also infinite. The classical LLN and CLT, which require a finite mean and variance, do not apply. Sums of such random variables are governed by different limit theorems, often involving $\\alpha$-stable distributions and scaling factors other than the classical $\\sqrt{n}$.\n\nA simulation function is implemented to generate a single sample of $T$. This function initializes the total population and current generation size to $1$. It then enters a loop, where in each step it calculates the size of the next generation by drawing from a Poisson distribution with parameter $\\lambda = m \\times (\\text{current generation size})$. The loop continues until the generation size becomes zero, at which point the accumulated total population is returned.\n\nThe four test cases are designed to probe these theoretical properties:\n\nTest case $1$ validates the CLT for the subcritical case ($m=0.9$). We construct $B=400$ block sums $S_j = \\sum_{i=1}^{n} T_i$ with a block size of $n=400$. We then standardize these sums to $Z_j = \\frac{S_j - n\\mu}{\\sqrt{n\\sigma^2}}$. According to the CLT, the distribution of $Z_j$ should approximate the standard normal distribution $\\mathcal{N}(0, 1)$. We test this by computing the empirical mean $\\overline{Z}$ and variance $s_Z^2$ of the sample $\\{Z_j\\}$ and checking if they are close to $0$ and $1$, respectively.\n\nTest case $2$ further examines the scaling properties in the subcritical case ($m=0.9$). The CLT implies that the standard deviation of a block sum $S_n$ is $\\mathrm{StdDev}(S_n) = \\sigma \\sqrt{n}$. Thus, the quantity $s_n$, defined as the empirical standard deviation of block sums divided by $\\sqrt{n}$, should provide a consistent estimate of $\\sigma$ regardless of the block size $n$. We verify this by comparing the estimates $s_{n_1}$ and $s_{n_2}$ for two different block sizes, $n_1=200$ and $n_2=800$.\n\nTest case $3$ demonstrates the failure of classical CLT scaling at criticality ($m=1.0$). Since $T$ has an infinite variance, the scaling of the standard deviation of the sum $S_n$ is not proportional to $\\sqrt{n}$. We compute the quantity $g_n = \\mathrm{StdDev}(S_n) / \\sqrt{n}$ for two different block sizes, $n_1=200$ and $n_2=600$. If classical scaling held, $g_{n_1}$ and $g_{n_2}$ would be comparable. The test expects $g_{n_2}$ to be significantly larger than $g_{n_1}$, indicating that the standardized deviation grows with $n$, a hallmark of attraction to a stable law with an index $\\alpha < 2$.\n\nTest case $4$ provides a direct diagnostic for the heavy-tailed nature of $T$ at criticality ($m=1.0$). A key feature of heavy-tailed distributions is that the sum of many i.i.d. samples is often dominated by a few extremely large values. We generate $K=15000$ samples of $T$ and calculate the proportion of the total sum that is contributed by the top $q=1\\%$ of the samples. The test checks if this proportion exceeds $50\\%$, a strong indicator of extreme value concentration.\n\nThe program is implemented in Python using the `numpy` library for numerical computations and random number generation. A fixed random seed is used to ensure the reproducibility of the Monte Carlo simulation results. Helper functions encapsulate the logic for simulating $T$ and for generating blocks of sums, improving code structure and readability.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Runs a series of Monte Carlo simulations to test limit theorems\n    for Galton-Watson branching processes.\n    \"\"\"\n    SEED = 42\n    rng = np.random.default_rng(SEED)\n\n    def simulate_T(m, rng_instance):\n        \"\"\"\n        Simulates one instance of a Galton-Watson process and returns the total size T.\n        Offspring distribution is Poisson(m). Process starts with 1 ancestor.\n        \"\"\"\n        total_pop = 1\n        current_gen_size = 1\n        while current_gen_size > 0:\n            # The sum of N Poisson(m) is Poisson(N*m)\n            lam = m * current_gen_size\n            next_gen_size = rng_instance.poisson(lam=lam)\n            total_pop += next_gen_size\n            current_gen_size = next_gen_size\n        return total_pop\n\n    def get_block_sums(m, n, B, rng_instance):\n        \"\"\"\n        Generates B block sums, where each block is the sum of n samples of T.\n        \"\"\"\n        block_sums_arr = np.zeros(B)\n        for j in range(B):\n            s_j = 0\n            for _ in range(n):\n                s_j += simulate_T(m, rng_instance)\n            block_sums_arr[j] = s_j\n        return block_sums_arr\n\n    results = []\n\n    # Test case 1: CLT standardization at m=0.9\n    m1 = 0.9\n    n1 = 400\n    B1 = 400\n    # Theoretical mean and variance for subcritical GW process with Poisson offspring\n    mu1 = 1 / (1 - m1)\n    sigma2_1 = m1 / (1 - m1)**3\n    \n    block_sums1 = get_block_sums(m1, n1, B1, rng)\n    Z = (block_sums1 - n1 * mu1) / np.sqrt(n1 * sigma2_1)\n    \n    Z_mean = np.mean(Z)\n    Z_var = np.var(Z, ddof=1) # Sample variance\n    \n    result1 = abs(Z_mean) <= 0.15 and abs(Z_var - 1) <= 0.2\n    results.append(result1)\n\n    # Test case 2: CLT scaling consistency at m=0.9\n    m2 = 0.9\n    n2_1, n2_2 = 200, 800\n    B2 = 300\n    \n    block_sums2_1 = get_block_sums(m2, n2_1, B2, rng)\n    std_dev_s1 = np.std(block_sums2_1, ddof=1)\n    s_n1 = std_dev_s1 / np.sqrt(n2_1)\n    \n    block_sums2_2 = get_block_sums(m2, n2_2, B2, rng)\n    std_dev_s2 = np.std(block_sums2_2, ddof=1)\n    s_n2 = std_dev_s2 / np.sqrt(n2_2)\n\n    result2 = abs(s_n1 - s_n2) <= 0.1 * s_n1\n    results.append(result2)\n\n    # Test case 3: non-Gaussian scaling growth at m=1.0\n    m3 = 1.0\n    n3_1, n3_2 = 200, 600\n    B3 = 200\n\n    block_sums3_1 = get_block_sums(m3, n3_1, B3, rng)\n    std_dev_g1 = np.std(block_sums3_1, ddof=1)\n    g_n1 = std_dev_g1 / np.sqrt(n3_1)\n\n    block_sums3_2 = get_block_sums(m3, n3_2, B3, rng)\n    std_dev_g2 = np.std(block_sums3_2, ddof=1)\n    g_n2 = std_dev_g2 / np.sqrt(n3_2)\n    \n    result3 = g_n2 > 1.5 * g_n1\n    results.append(result3)\n\n    # Test case 4: extreme-concentration diagnostic at m=1.0\n    m4 = 1.0\n    K4 = 15000\n    q4 = 0.01\n\n    T_samples = np.array([simulate_T(m4, rng) for _ in range(K4)])\n    \n    S_total = np.sum(T_samples)\n    num_top = math.ceil(q4 * K4)\n    \n    T_samples.sort()\n    S_top = np.sum(T_samples[-int(num_top):])\n    \n    C = S_top / S_total if S_total > 0 else 0\n    \n    result4 = C >= 0.5\n    results.append(result4)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, [r.item() for r in results]))}]\")\n\nsolve()\n```", "id": "3153023"}, {"introduction": "Moving beyond analysis to design, this final practice demonstrates how limit theorems can be used to create optimal resource allocation algorithms. Framed in the intuitive context of Monte Carlo ray tracing for computer graphics, you will develop a strategy to distribute a fixed computational budget across different pixels to minimize the maximum visual noise. This exercise in minimax optimization, grounded in the principles of the CLT, showcases how statistical theory can be leveraged to build intelligent systems that make the most of limited resources [@problem_id:3153099].", "problem": "Consider a computational imaging task where each pixel corresponds to a random luminance variable with independent and identically distributed samples. For pixel index $i \\in \\{1,\\dots,P\\}$, let the luminance samples be independent draws of a real-valued random variable $L_i$ with unknown mean $\\mu_i$ and finite variance $\\sigma_i^2$. The per-pixel Monte Carlo estimate of the pixel luminance is the sample mean $\\bar{L}_{i,n_i}$ computed from $n_i$ samples. Assume that an initial pilot pass provides $n_{0,i}$ samples per pixel and an unbiased variance estimate $s_i^2$ of $\\sigma_i^2$ from these pilot samples, and that we have a total sample budget $N_{\\text{total}}$ for the final rendering pass such that $\\sum_{i=1}^P n_i = N_{\\text{total}}$ and $n_i \\ge n_{0,i}$ for all $i$.\n\nUse the Law of Large Numbers (LLN) and the Central Limit Theorem (CLT) as the only probabilistic foundations to design an allocation rule for the integer sample counts $\\{n_i\\}_{i=1}^P$ that minimizes the maximum predicted two-sided confidence interval half-width across pixels at a given confidence level $1-\\alpha$, under the constraints $\\sum_{i=1}^P n_i = N_{\\text{total}}$ and $n_i \\ge n_{0,i}$. Specifically:\n\n- Under the Central Limit Theorem, for large $n_i$, the distribution of $\\bar{L}_{i,n_i}$ can be approximated by a normal distribution with mean $\\mu_i$ and variance $\\sigma_i^2/n_i$. Thus, the two-sided $(1-\\alpha)$ confidence interval half-width for $\\mu_i$ is approximately $z_{1-\\alpha/2}\\sqrt{\\sigma_i^2/n_i}$, where $z_{1-\\alpha/2}$ is the $(1-\\alpha/2)$ quantile of the standard normal distribution.\n- Use $s_i^2$ in place of $\\sigma_i^2$ as a plug-in estimate, and restrict yourself to algorithmic decisions justified by the LLN and CLT alone (for example, separability across pixels, convexity arguments, and monotonicity of marginal variance reduction with respect to $n_i$ are admissible consequences to use).\n\nYour program must:\n1. Derive from first principles how to choose integer $\\{n_i\\}$ that minimize the maximum predicted half-width across pixels, subject to the constraints $\\sum_{i=1}^P n_i = N_{\\text{total}}$ and $n_i \\ge n_{0,i}$.\n2. Compute the predicted maximum half-width value\n   $$H_{\\max} \\equiv \\max_{1 \\le i \\le P} \\left( z_{1-\\alpha/2}\\sqrt{\\frac{s_i^2}{n_i}} \\right),$$\n   using the derived allocation $\\{n_i\\}$.\n3. For integer feasibility, if the derived optimal allocation has non-integer values, convert it to integers while preserving the constraints and ensuring that the objective is not worsened by any single-sample reassignment. Your method must be based on a principled marginal-improvement argument justified by the CLT-based objective.\n\nInput is embedded in the code via a fixed test suite. For each test case $t$, you are given: the per-pixel variance estimates $(s_1^2,\\dots,s_P^2)$, the pilot sample counts $(n_{0,1},\\dots,n_{0,P})$, the total budget $N_{\\text{total}}$, and the significance level $\\alpha$. Your program must output, for each test case, the scalar $H_{\\max}$ rounded to six decimal places.\n\nTest suite (five cases):\n- Case A: $P=1$, $(s_1^2) = (4.0)$, $(n_{0,1}) = (0)$, $N_{\\text{total}} = 100$, $\\alpha = 0.05$.\n- Case B: $P=2$, $(s_1^2,s_2^2) = (1.0,1.0)$, $(n_{0,1},n_{0,2}) = (0,0)$, $N_{\\text{total}} = 100$, $\\alpha = 0.05$.\n- Case C: $P=2$, $(s_1^2,s_2^2) = (4.0,1.0)$, $(n_{0,1},n_{0,2}) = (0,0)$, $N_{\\text{total}} = 100$, $\\alpha = 0.05$.\n- Case D: $P=3$, $(s_1^2,s_2^2,s_3^2) = (4.0,1.0,0.25)$, $(n_{0,1},n_{0,2},n_{0,3}) = (5,5,5)$, $N_{\\text{total}} = 30$, $\\alpha = 0.10$.\n- Case E: $P=3$, $(s_1^2,s_2^2,s_3^2) = (0.0,2.25,0.25)$, $(n_{0,1},n_{0,2},n_{0,3}) = (2,2,2)$, $N_{\\text{total}} = 12$, $\\alpha = 0.01$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated Python-style list of floating-point numbers in the order of cases A through E, each rounded to six decimal places and enclosed in square brackets. For example, a valid output line has the form $[\\text{A},\\text{B},\\text{C},\\text{D},\\text{E}]$, where each symbol denotes the rounded value for that case.", "solution": "The problem asks for an integer sample allocation $\\{n_i\\}_{i=1}^P$ that minimizes the maximum predicted confidence interval half-width across a set of $P$ pixels, subject to a total sample budget $N_{\\text{total}}$ and per-pixel minimum sample counts $\\{n_{0,i}\\}$. This is a minimax optimization problem grounded in the Central Limit Theorem (CLT) and the Law of Large Numbers (LLN).\n\n### Step 1: Formulation of the Optimization Problem\n\nThe problem is to determine the integer sample counts $n_1, n_2, \\dots, n_P$ that solve:\n$$\n\\begin{aligned}\n& \\underset{\\{n_i\\}}{\\text{minimize}} & & \\max_{1 \\le i \\le P} H_i \\\\\n& \\text{subject to} & & \\sum_{i=1}^P n_i = N_{\\text{total}} \\\\\n& & & n_i \\ge n_{0,i} \\quad \\text{for all } i \\in \\{1, \\dots, P\\} \\\\\n& & & n_i \\in \\mathbb{Z}^+\n\\end{aligned}\n$$\nwhere $H_i$ is the confidence interval half-width for pixel $i$. The CLT provides the approximation for large $n_i$:\n$$H_i = z_{1-\\alpha/2}\\sqrt{\\frac{\\sigma_i^2}{n_i}}$$\nWe use the unbiased variance estimates $s_i^2$ from a pilot pass as plug-in estimators for the true unknown variances $\\sigma_i^2$, a substitution justified by the LLN. The objective function thus becomes:\n$$\\text{minimize} \\max_{1 \\le i \\le P} \\left( z_{1-\\alpha/2}\\sqrt{\\frac{s_i^2}{n_i}} \\right)$$\nSince $z_{1-\\alpha/2}$ is a positive constant for a given confidence level $1-\\alpha$, minimizing the maximum half-width is equivalent to minimizing the maximum of the term $\\sqrt{s_i^2/n_i}$, which is in turn equivalent to minimizing:\n$$\\max_{1 \\le i \\le P} \\left( \\frac{s_i^2}{n_i} \\right)$$\nThis term, $s_i^2/n_i$, represents the estimated variance of the sample mean $\\bar{L}_{i,n_i}$.\n\n### Step 2: Optimal Allocation Strategy for Real-Valued $n_i$\n\nThe core principle for solving a minimax problem of this form is to equalize the terms being maximized. To minimize $\\max_i(E_i)$, where $E_i = s_i^2/n_i$ is a decreasing function of the resource $n_i$, the optimal solution is achieved when all non-zero error terms are equal:\n$$\\frac{s_1^2}{n_1} = \\frac{s_2^2}{n_2} = \\dots = \\frac{s_P^2}{n_P} = C$$\nfor some constant $C$ (for all pixels with $s_i^2 > 0$). This gives a relationship for the optimal (real-valued) allocation:\n$$n_i = \\frac{s_i^2}{C}$$\nThis shows that the optimal number of samples $n_i$ is directly proportional to the variance $s_i^2$. We can find the constant of proportionality using the total budget constraint, $\\sum n_i = N_{\\text{total}}$:\n$$\\sum_{i=1}^P \\frac{s_i^2}{C} = N_{\\text{total}} \\implies \\frac{1}{C} \\sum_{i=1}^P s_i^2 = N_{\\text{total}} \\implies \\frac{1}{C} = \\frac{N_{\\text{total}}}{\\sum_{j=1}^P s_j^2}$$\nSubstituting this back, we get the ideal real-valued allocation (ignoring the $n_i \\ge n_{0,i}$ constraint for a moment):\n$$n_i^* = N_{\\text{total}} \\frac{s_i^2}{\\sum_{j=1}^P s_j^2}$$\n\n### Step 3: Incorporating Minimum Sample Constraints\n\nThe constraint $n_i \\ge n_{0,i}$ must be incorporated. The allocation $n_i^*$ might violate this for some pixels. If $n_i^* < n_{0,i}$ for a pixel $i$, we are forced to allocate at least $n_{0,i}$ samples to it. For such a pixel, we \"lock\" its sample count at $n_i = n_{0,i}$. The remaining budget is then optimally re-allocated among the remaining \"active\" pixels according to the same principle. This leads to an iterative algorithm:\n1.  Initialize the set of active pixels $A = \\{1, \\dots, P\\}$ and the budget $N = N_{\\text{total}}$.\n2.  In a loop, calculate the ideal allocation $n_i^{\\text{ideal}} = N \\frac{s_i^2}{\\sum_{j \\in A} s_j^2}$ for all $i \\in A$.\n3.  Identify the set of pixels $V \\subseteq A$ for which $n_i^{\\text{ideal}} < n_{0,i}$.\n4.  If $V$ is empty, the current allocation $n_i^{\\text{ideal}}$ for $i \\in A$ is optimal and respects all minimums. The final real-valued allocation is found.\n5.  If $V$ is not empty, for each $i \\in V$, fix its allocation to $n_i = n_{0,i}$. Remove these pixels from $A$ and reduce the budget $N$ by $\\sum_{i \\in V} n_{0,i}$. Repeat the loop with the smaller active set and reduced budget.\n\nThis iterative process yields a real-valued allocation $\\{n_i^{\\text{real}}\\}$ that satisfies all constraints.\n\n### Step 4: Integer Sample Allocation\n\nThe derived real-valued allocation $\\{n_i^{\\text{real}}\\}$ must be converted to an integer allocation $\\{n_i\\}$ that sums to $N_{\\text{total}}$. A principled method, consistent with the objective of minimizing the maximum error, is as follows:\n1.  Initialize the integer allocation by taking the floor of the real solution: $n_i' = \\lfloor n_i^{\\text{real}} \\rfloor$. This satisfies $n_i' \\ge n_{0,i}$ as $n_i^{\\text{real}} \\ge n_{0,i}$ and $n_{0,i}$ is an integer.\n2.  Calculate the remaining number of samples to be distributed: $R = N_{\\text{total}} - \\sum_{i=1}^P n_i'$.\n3.  Distribute these $R$ samples one by one. In each of the $R$ steps, add a single sample to the pixel which currently has the largest error term $s_i^2/n_i'$. This is a greedy approach that directly tackles the minimax objective at each step. If $n_i'=0$ for a pixel with $s_i^2>0$, its error is infinite, ensuring it receives a sample first.\n\n### Step 5: Final Calculation\n\nOnce the final integer allocation $\\{n_i\\}$ is determined, the maximum half-width is calculated by finding the maximum error term across all pixels and multiplying by the appropriate normal quantile:\n$$ H_{\\max} = \\max_{1 \\le i \\le P} \\left( z_{1-\\alpha/2}\\sqrt{\\frac{s_i^2}{n_i}} \\right) = z_{1-\\alpha/2} \\sqrt{\\max_{1 \\le i \\le P} \\left(\\frac{s_i^2}{n_i}\\right)} $$\nwhere $z_{1-\\alpha/2}$ is the $(1-\\alpha/2)$ quantile of the standard normal distribution, found using `scipy.stats.norm.ppf(1 - \\alpha/2)`. If a sample count $n_i$ for a pixel with $s_i^2>0$ is zero, its half-width is considered infinite; however, the allocation algorithm prevents this if $N_{\\text{total}}$ is sufficient.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Main solver function to process all test cases and print the results.\n    \"\"\"\n    # Test suite (five cases): P, s^2, n_0, N_total, alpha\n    test_cases = [\n        # Case A: P=1, (s_1^2)=(4.0), (n_0,1)=(0), N_total=100, alpha=0.05\n        (1, np.array([4.0]), np.array([0]), 100, 0.05),\n        # Case B: P=2, (s_1^2,s_2^2)=(1.0,1.0), (n_0,1,n_0,2)=(0,0), N_total=100, alpha=0.05\n        (2, np.array([1.0, 1.0]), np.array([0, 0]), 100, 0.05),\n        # Case C: P=2, (s_1^2,s_2^2)=(4.0,1.0), (n_0,1,n_0,2)=(0,0), N_total=100, alpha=0.05\n        (2, np.array([4.0, 1.0]), np.array([0, 0]), 100, 0.05),\n        # Case D: P=3, (s_1^2,s_2^2,s_3^2)=(4.0,1.0,0.25), (n_0,...)=(5,5,5), N_total=30, alpha=0.10\n        (3, np.array([4.0, 1.0, 0.25]), np.array([5, 5, 5]), 30, 0.10),\n        # Case E: P=3, (s_1^2,s_2^2,s_3^2)=(0.0,2.25,0.25), (n_0,...)=(2,2,2), N_total=12, alpha=0.01\n        (3, np.array([0.0, 2.25, 0.25]), np.array([2, 2, 2]), 12, 0.01),\n    ]\n\n    results = []\n    for P, s2_vals, n0_vals, N_total, alpha in test_cases:\n        result = _calculate_max_half_width(P, s2_vals, n0_vals, N_total, alpha)\n        results.append(result)\n\n    # Format the final output string\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\ndef _calculate_max_half_width(P, s2_vals, n0_vals, N_total, alpha):\n    \"\"\"\n    Calculates the maximum confidence interval half-width for a single test case.\n    \"\"\"\n    \n    # --- Step 1: Real-valued allocation with constraints ---\n    n_real = np.zeros(P)\n    active_indices = list(range(P))\n    budget = float(N_total)\n    \n    # Iteratively lock pixels that don't meet their minimum sample count n_0\n    for _ in range(P + 1):  # Loop guard to prevent infinite loops\n        if not active_indices:\n            break\n            \n        sum_s2_active = sum(s2_vals[i] for i in active_indices)\n        \n        # If all remaining active pixels have zero variance, their allocation is minimal.\n        if sum_s2_active == 0:\n            for i in active_indices:\n                n_real[i] = n0_vals[i]\n                budget -= n_real[i]\n            # Remaining budget for zero-variance pixels can be distributed arbitrarily.\n            # To be deterministic, we add it to the first such pixel.\n            if len(active_indices) > 0 and budget > 0:\n                n_real[active_indices[0]] += budget\n            break\n\n        n_ideal = {i: budget * s2_vals[i] / sum_s2_active for i in active_indices}\n        \n        violators = {i for i in active_indices if n_ideal[i] < n0_vals[i]}\n\n        if not violators:\n            for i in active_indices:\n                n_real[i] = n_ideal[i]\n            break\n        \n        newly_locked_indices = []\n        for i in violators:\n            n_real[i] = float(n0_vals[i])\n            budget -= n_real[i]\n            newly_locked_indices.append(i)\n        \n        active_indices = [i for i in active_indices if i not in newly_locked_indices]\n        \n    # --- Step 2: Convert real allocation to integer allocation ---\n    n_alloc = np.floor(n_real).astype(int)\n    \n    # Distribute remainder samples using a greedy approach\n    remainder_samples = N_total - np.sum(n_alloc)\n    \n    for _ in range(remainder_samples):\n        errors = np.zeros(P)\n        for i in range(P):\n            if s2_vals[i] > 0:\n                if n_alloc[i] == 0:\n                    errors[i] = np.inf\n                else:\n                    errors[i] = s2_vals[i] / n_alloc[i]\n            else:\n                errors[i] = -np.inf # Ensure zero-variance pixels are never chosen\n\n        # Find pixel with max error to give the next sample\n        idx_to_increment = np.argmax(errors)\n        n_alloc[idx_to_increment] += 1\n        \n    # --- Step 3: Calculate the maximum half-width ---\n    z_val = norm.ppf(1 - alpha / 2.0)\n    \n    max_error_term = 0.0\n    for i in range(P):\n        # A pixel with no samples and non-zero variance would have infinite error,\n        # but the algorithm ensures this doesn't happen if N_total is sufficient.\n        if n_alloc[i] > 0:\n            error_term = s2_vals[i] / n_alloc[i]\n            if error_term > max_error_term:\n                max_error_term = error_term\n    \n    h_max = z_val * np.sqrt(max_error_term)\n    return h_max\n\nsolve()\n```", "id": "3153099"}]}