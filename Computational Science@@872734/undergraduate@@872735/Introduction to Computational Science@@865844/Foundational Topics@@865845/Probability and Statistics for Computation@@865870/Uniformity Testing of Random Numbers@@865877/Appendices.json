{"hands_on_practices": [{"introduction": "A truly uniform sequence of random numbers should exhibit no systematic patterns. One way to probe for such patterns is to observe how the sequence behaves in a Monte Carlo calculation with a known, simple answer. This exercise leverages the fact that the integral of a sine wave over any number of full periods is zero. You will build a formal statistical test from this observation, using fundamental tools like the Central Limit Theorem to quantify when the Monte Carlo estimate's deviation from zero is statistically significant, thereby revealing hidden non-uniformity.", "problem": "Consider a sequence of independent and identically distributed (i.i.d.) random variables $U_1,U_2,\\dots,U_n$ intended to be samples from the continuous uniform distribution on the unit interval $[0,1]$. For any fixed positive integer $k$, define the Monte Carlo estimator of the integral $$\\int_0^1 \\sin(2\\pi k x)\\,dx$$ by the empirical average $$\\widehat{\\mu}_k = \\frac{1}{n}\\sum_{i=1}^n \\sin(2\\pi k U_i),$$ where the sine function is evaluated in radians. Under perfect uniformity, the integral equals $0$, so a well-designed estimator should concentrate near $0$ as $n$ grows large. Your task is to design and implement a uniformity test that uses the deviations of $\\widehat{\\mu}_k$ from $0$ to flag non-uniformity.\n\nStarting from fundamental definitions in probability and statistics and without using any pre-given shortcut formulas, derive a decision rule that:\n- Models the sampling distribution of $\\widehat{\\mu}_k$ under the null hypothesis that $U_i$ are i.i.d. $\\mathrm{Uniform}[0,1]$.\n- Computes, from first principles, the variance of $\\sin(2\\pi k U)$ for integer $k$ under the uniform distribution on $[0,1]$.\n- Uses the Central Limit Theorem (CLT) to approximate the distribution of $\\widehat{\\mu}_k$ and obtain a threshold for declaring a statistically significant deviation from $0$.\n- Controls the family-wise error rate at a pre-specified significance level $\\alpha$ when testing multiple frequencies $k$ simultaneously.\n- Flags non-uniformity if any tested frequency $k$ exhibits a statistically significant deviation according to your derived threshold.\n\nImplement a complete, runnable program that performs this test on the following test suite. In all cases, angles must be interpreted in radians, and the significance level $\\alpha$ is expressed as a decimal (not with a percentage sign). Each case specifies the sample size $n$, the list of frequencies $\\{k_j\\}$ to be tested, the significance level $\\alpha$, a random seed for reproducibility, and a method for generating the sequence $\\{U_i\\}$:\n\n- Case $1$ (happy path, i.i.d. uniform): $n=5000$, $\\{k_j\\} = \\{1,2,3,4,5\\}$, $\\alpha=0.01$, seed $42$, generator: i.i.d. $\\mathrm{Uniform}[0,1]$.\n- Case $2$ (non-uniform with a sine tilt): $n=4000$, $\\{k_j\\} = \\{1,2,3,4\\}$, $\\alpha=0.01$, seed $123$, generator: acceptance-rejection sampling from a density proportional to $1+\\varepsilon\\sin(2\\pi x)$ on $[0,1]$ with $\\varepsilon=0.3$. Use a uniform envelope and accept a proposal $x$ with probability $\\frac{1+\\varepsilon\\sin(2\\pi x)}{1+\\varepsilon}$; repeat until $n$ samples are accepted.\n- Case $3$ (boundary condition, small sample, i.i.d. uniform): $n=200$, $\\{k_j\\} = \\{1\\}$, $\\alpha=0.01$, seed $987$, generator: i.i.d. $\\mathrm{Uniform}[0,1]$.\n- Case $4$ (non-uniform mixture with a point mass): $n=3000$, $\\{k_j\\} = \\{1,2,3\\}$, $\\alpha=0.01$, seed $7$, generator: for each index $i$, draw $B_i$ from a Bernoulli distribution with success probability $p=0.2$ and set $U_i = 0.9$ if $B_i=1$; otherwise draw $U_i$ i.i.d. $\\mathrm{Uniform}[0,1]$.\n\nYour program must, for each case, output a boolean indicating whether the sequence passes the uniformity test ($\\text{True}$ for \"no significant deviation detected\" and $\\text{False}$ for \"flagged as non-uniform\"). The final output must be a single line containing the four booleans in order for the four cases, formatted as a comma-separated list enclosed in square brackets (e.g., \"[True,False,True,False]\").", "solution": "The problem requires the formulation and implementation of a statistical test to assess the uniformity of a sequence of random numbers $U_1, U_2, \\dots, U_n$ on the interval $[0,1]$. The test is based on the Monte Carlo estimators of the integrals of sinusoidal functions, $\\widehat{\\mu}_k = \\frac{1}{n}\\sum_{i=1}^n \\sin(2\\pi k U_i)$, for a set of positive integer frequencies $\\{k_j\\}$.\n\nThe null hypothesis, $H_0$, is that the samples $U_i$ are independent and identically distributed (i.i.d.) from the continuous uniform distribution on $[0,1]$, denoted $U_i \\sim \\text{i.i.d. } \\mathrm{Uniform}[0,1]$. The test must flag the sequence as non-uniform if, for any tested frequency $k_j$, the deviation of $\\widehat{\\mu}_{k_j}$ from its expected value under $H_0$ is statistically significant. The derivation must proceed from first principles.\n\n**Step 1: Mean and Variance of the Test Statistic under the Null Hypothesis**\n\nLet $X_i^{(k)} = \\sin(2\\pi k U_i)$ for a fixed positive integer $k$. The estimator is the sample mean $\\widehat{\\mu}_k = \\frac{1}{n} \\sum_{i=1}^n X_i^{(k)}$.\nUnder $H_0$, the probability density function of $U_i$ is $f_U(u) = 1$ for $u \\in [0,1]$ and $0$ otherwise.\n\nFirst, we compute the expected value of $X_i^{(k)}$:\n$$\n\\mu_X = E[X_i^{(k)}] = E[\\sin(2\\pi k U_i)] = \\int_{-\\infty}^{\\infty} \\sin(2\\pi k u) f_U(u) \\,du = \\int_0^1 \\sin(2\\pi k u) \\cdot 1 \\,du\n$$\nEvaluating the integral:\n$$\n\\mu_X = \\left[ -\\frac{\\cos(2\\pi k u)}{2\\pi k} \\right]_0^1 = -\\frac{\\cos(2\\pi k)}{2\\pi k} - \\left(-\\frac{\\cos(0)}{2\\pi k}\\right)\n$$\nSince $k$ is a positive integer, $\\cos(2\\pi k) = 1$ and $\\cos(0) = 1$.\n$$\n\\mu_X = -\\frac{1}{2\\pi k} + \\frac{1}{2\\pi k} = 0\n$$\nThe expected value of the estimator $\\widehat{\\mu}_k$ is therefore $E[\\widehat{\\mu}_k] = E\\left[\\frac{1}{n}\\sum_{i=1}^n X_i^{(k)}\\right] = \\frac{1}{n}\\sum_{i=1}^n E[X_i^{(k)}] = 0$.\n\nNext, we compute the variance of $X_i^{(k)}$ from first principles:\n$$\n\\sigma_X^2 = \\text{Var}(X_i^{(k)}) = E\\left[(X_i^{(k)} - \\mu_X)^2\\right] = E\\left[(X_i^{(k)})^2\\right] - (\\mu_X)^2 = E[\\sin^2(2\\pi k U_i)] - 0^2\n$$\nWe compute the expectation by integrating:\n$$\nE[\\sin^2(2\\pi k U_i)] = \\int_0^1 \\sin^2(2\\pi k u) \\,du\n$$\nUsing the trigonometric identity $\\sin^2(\\theta) = \\frac{1 - \\cos(2\\theta)}{2}$:\n$$\n\\int_0^1 \\frac{1 - \\cos(2 \\cdot 2\\pi k u)}{2} \\,du = \\frac{1}{2} \\int_0^1 (1 - \\cos(4\\pi k u)) \\,du\n$$\n$$\n= \\frac{1}{2} \\left[ u - \\frac{\\sin(4\\pi k u)}{4\\pi k} \\right]_0^1 = \\frac{1}{2} \\left( \\left(1 - \\frac{\\sin(4\\pi k)}{4\\pi k}\\right) - \\left(0 - \\frac{\\sin(0)}{4\\pi k}\\right) \\right)\n$$\nSince $k$ is an integer, $\\sin(4\\pi k) = 0$ and $\\sin(0) = 0$.\n$$\n\\sigma_X^2 = E[\\sin^2(2\\pi k U_i)] = \\frac{1}{2}(1 - 0) = \\frac{1}{2}\n$$\nThe variance of the estimator $\\widehat{\\mu}_k$ is then calculated. Because the $U_i$ are i.i.d., the $X_i^{(k)}$ are also i.i.d.\n$$\n\\text{Var}(\\widehat{\\mu}_k) = \\text{Var}\\left(\\frac{1}{n}\\sum_{i=1}^n X_i^{(k)}\\right) = \\frac{1}{n^2} \\sum_{i=1}^n \\text{Var}(X_i^{(k)}) = \\frac{1}{n^2} (n \\cdot \\sigma_X^2) = \\frac{\\sigma_X^2}{n} = \\frac{1}{2n}\n$$\n\n**Step 2: Distribution of the Test Statistic via the Central Limit Theorem (CLT)**\n\nThe Central Limit Theorem states that for a sufficiently large sample size $n$, the distribution of the sample mean $\\widehat{\\mu}_k$ of i.i.d. random variables is approximately normal.\n$$\n\\widehat{\\mu}_k \\approx \\mathcal{N}\\left(E[\\widehat{\\mu}_k], \\text{Var}(\\widehat{\\mu}_k)\\right) \\implies \\widehat{\\mu}_k \\approx \\mathcal{N}\\left(0, \\frac{1}{2n}\\right)\n$$\nTo construct a hypothesis test, we define a standardized test statistic $Z_k$ which, under $H_0$, follows a standard normal distribution, $\\mathcal{N}(0,1)$.\n$$\nZ_k = \\frac{\\widehat{\\mu}_k - E[\\widehat{\\mu}_k]}{\\sqrt{\\text{Var}(\\widehat{\\mu}_k)}} = \\frac{\\widehat{\\mu}_k - 0}{\\sqrt{1/(2n)}} = \\widehat{\\mu}_k \\sqrt{2n}\n$$\nThus, under $H_0$, $Z_k \\approx \\mathcal{N}(0,1)$.\n\n**Step 3: Handling Multiple Comparisons and Formulating the Decision Rule**\n\nThe problem requires testing a set of $m$ frequencies, denoted $\\{k_1, k_2, \\dots, k_m\\}$. This involves performing $m$ simultaneous hypothesis tests. If we test each hypothesis at a significance level $\\alpha$, the probability of making at least one Type I error (a false positive), known as the family-wise error rate (FWER), would be significantly higher than $\\alpha$.\n\nTo control the FWER at a specified level $\\alpha$, we use the Bonferroni correction. This method adjusts the significance level for each individual test to $\\alpha' = \\alpha/m$. By the union bound, this ensures that the FWER is less than or equal to $\\alpha$.\n\nFor a single test at significance $\\alpha'$, the decision rule is to reject $H_0$ if the observed test statistic is in the rejection region. For a two-tailed test on a standard normal variable $Z$, the rejection region is $|Z| > z_{\\text{crit}}$, where $z_{\\text{crit}}$ is the critical value such that $P(|Z| > z_{\\text{crit}}) = \\alpha'$. This critical value is given by $z_{\\text{crit}} = \\Phi^{-1}(1 - \\alpha'/2)$, where $\\Phi^{-1}$ is the quantile function (inverse CDF) of the standard normal distribution.\n\nCombining these elements, the final decision rule for the family of tests is as follows:\n1.  Let the set of frequencies be $\\{k_1, k_2, \\dots, k_m\\}$. The number of tests is $m$.\n2.  The overall significance level is $\\alpha$.\n3.  The Bonferroni-corrected significance level for each test is $\\alpha' = \\alpha / m$.\n4.  The critical value for all tests is $z_{\\text{crit}} = \\Phi^{-1}(1 - \\frac{\\alpha'}{2}) = \\Phi^{-1}(1 - \\frac{\\alpha}{2m})$.\n5.  For each frequency $k_j$ in the set:\n    a. Generate the sequence of $n$ samples $U_1, \\dots, U_n$.\n    b. Calculate the estimator $\\widehat{\\mu}_{k_j} = \\frac{1}{n}\\sum_{i=1}^n \\sin(2\\pi k_j U_i)$.\n    c. Calculate the test statistic $Z_{k_j} = \\widehat{\\mu}_{k_j} \\sqrt{2n}$.\n6.  The overall null hypothesis $H_0$ (that the sequence is uniform) is rejected if there exists *any* $j \\in \\{1, \\dots, m\\}$ such that $|Z_{k_j}| > z_{\\text{crit}}$. If this condition is met, we flag the sequence as non-uniform (`False`).\n7.  If for all $j \\in \\{1, \\dots, m\\}$, we have $|Z_{k_j}| \\le z_{\\text{crit}}$, we fail to reject $H_0$ and conclude that the sequence passes the uniformity test (`True`).\n\nThis comprehensive procedure forms the basis for the implementation.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Performs uniformity tests on four specified cases and prints the results.\n    \"\"\"\n    test_cases = [\n        {'n': 5000, 'k_list': [1, 2, 3, 4, 5], 'alpha': 0.01, 'seed': 42, 'generator': 'uniform'},\n        {'n': 4000, 'k_list': [1, 2, 3, 4], 'alpha': 0.01, 'seed': 123, 'generator': 'sine_tilt', 'params': {'epsilon': 0.3}},\n        {'n': 200, 'k_list': [1], 'alpha': 0.01, 'seed': 987, 'generator': 'uniform'},\n        {'n': 3000, 'k_list': [1, 2, 3], 'alpha': 0.01, 'seed': 7, 'generator': 'point_mass_mixture', 'params': {'p': 0.2, 'loc': 0.9}},\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        n = case['n']\n        k_list = case['k_list']\n        alpha = case['alpha']\n        seed = case['seed']\n        generator = case['generator']\n        params = case.get('params', {})\n\n        np.random.seed(seed)\n\n        # Step 1: Generate the sequence of random numbers U\n        samples = np.array([])\n        if generator == 'uniform':\n            samples = np.random.uniform(0, 1, n)\n        \n        elif generator == 'sine_tilt':\n            # Acceptance-rejection sampling from a density proportional to 1 + ε*sin(2*pi*x)\n            epsilon = params['epsilon']\n            # The normalizing constant for the envelope is M = 1 + ε\n            M = 1.0 + epsilon\n            generated_samples = []\n            while len(generated_samples)  n:\n                # Propose from a uniform distribution\n                x_proposal = np.random.uniform(0, 1)\n                # Generate a uniform random number for the acceptance check\n                u_check = np.random.uniform(0, 1)\n                # Calculate the acceptance probability\n                prob_accept = (1.0 + epsilon * np.sin(2 * np.pi * x_proposal)) / M\n                if u_check  prob_accept:\n                    generated_samples.append(x_proposal)\n            samples = np.array(generated_samples)\n\n        elif generator == 'point_mass_mixture':\n            # Mixture model: with probability p, U_i = loc; otherwise, U_i is uniform\n            p = params['p']\n            loc = params['loc']\n            # Generate Bernoulli trials to decide between the two components\n            bernoulli_draws = np.random.binomial(1, p, n)\n            # Generate uniform samples for the continuous part\n            uniform_draws = np.random.uniform(0, 1, n)\n            # Combine them: if B=1, use loc; if B=0, use uniform_draw\n            samples = (1 - bernoulli_draws) * uniform_draws + bernoulli_draws * loc\n            \n        # Step 2: Set up the hypothesis test parameters\n        m = len(k_list)  # Number of simultaneous tests\n        alpha_prime = alpha / m  # Bonferroni corrected significance level\n        \n        # Calculate the two-tailed critical value from the standard normal distribution\n        z_crit = norm.ppf(1 - alpha_prime / 2)\n\n        # Step 3: Perform the test for each frequency k\n        is_uniform = True  # Assume uniform until a deviation is found\n        for k in k_list:\n            # Calculate the Monte Carlo estimator\n            mu_hat_k = np.mean(np.sin(2 * np.pi * k * samples))\n            \n            # Calculate the test statistic Z_k\n            z_k = mu_hat_k * np.sqrt(2 * n)\n            \n            # Check if the statistic exceeds the critical value\n            if np.abs(z_k)  z_crit:\n                is_uniform = False\n                break  # A single failure is enough to reject uniformity\n\n        results.append(is_uniform)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3201350"}, {"introduction": "After building a test from scratch, it is equally important to master the application of standard, powerful statistical tools. This practice applies established goodness-of-fit tests, like the Kolmogorov-Smirnov and Pearson's Chi-Square, to the classic sequence $x_n = \\{n\\alpha\\}$. This exercise not only allows you to empirically verify the theoretical property of equidistribution and its dependence on the rationality of $\\alpha$, but also cleverly reveals the unique strengths and weaknesses of these common tests, demonstrating scenarios where one might fail to detect non-uniformity that the other easily captures.", "problem": "Consider the sequence $x_n = \\{n\\alpha\\}$ where $\\{y\\}$ denotes the fractional part of $y$, that is, the unique value in $[0,1)$ satisfying $y = \\lfloor y \\rfloor + \\{y\\}$. You will investigate the uniformity of the points $\\{x_n\\}_{n=1}^N$ on the interval $[0,1)$ by formalizing two independent tests based on fundamental definitions of uniform distribution and empirical approximation.\n\nFundamental base:\n- A sequence $\\{x_n\\}$ in $[0,1)$ is said to be uniformly distributed if, for every subinterval $[a,b) \\subseteq [0,1)$, the fraction of indices $n \\in \\{1,2,\\dots,N\\}$ with $x_n \\in [a,b)$ approaches $b-a$ as $N \\to \\infty$.\n- The Empirical Cumulative Distribution Function (ECDF) at a point $t \\in [0,1)$ is the fraction of the sample less than or equal to $t$.\n- The Pearson Chi-Square test compares observed bin counts to the counts predicted by a uniform distribution across equal-width bins.\n\nTasks:\n1. For given parameters $(\\alpha, N, B, \\gamma)$, construct the finite sequence $\\{x_n\\}_{n=1}^N$ with $x_n = \\{n\\alpha\\}$ using standard floating-point arithmetic and reduction modulo $1$.\n2. Implement two formal tests of uniformity on $[0,1)$:\n   - Kolmogorov–Smirnov (KS) test: quantify the deviation between the ECDF of the sample and the identity function on $[0,1)$, and obtain a `$p$-value` under the null hypothesis of a continuous uniform distribution on $[0,1)$.\n   - Pearson Chi-Square test on a histogram with $B$ equal-width bins on $[0,1)$: compute observed frequencies and compare them to the expected frequency $N/B$ per bin, and obtain a `$p$-value` under the null hypothesis of a uniform distribution.\n3. Declare a case \"uniform\" if and only if both tests individually fail to reject at significance level $\\gamma$; that is, both `$p$-values` are greater than or equal to $\\gamma$. Otherwise, declare the case \"non-uniform\".\n4. Apply this procedure to the following test suite of parameter values $(\\alpha,N,B,\\gamma)$:\n   - Case $1$: $\\alpha = \\sqrt{2}$, $N = 10000$, $B = 20$, $\\gamma = 0.01$.\n   - Case $2$: $\\alpha = \\frac{1+\\sqrt{5}}{2}$, $N = 12000$, $B = 50$, $\\gamma = 0.01$.\n   - Case $3$: $\\alpha = \\frac{1}{2}$, $N = 2000$, $B = 20$, $\\gamma = 0.01$.\n   - Case $4$: $\\alpha = \\frac{1}{4}$, $N = 2000$, $B = 20$, $\\gamma = 0.01$.\n   - Case $5$: $\\alpha = \\frac{3}{8}$, $N = 16000$, $B = 8$, $\\gamma = 0.01$.\n   - Case $6$: $\\alpha = \\frac{3}{8}$, $N = 16000$, $B = 16$, $\\gamma = 0.01$.\n\nDesign for coverage:\n- Cases $1$ and $2$ probe irrational $\\alpha$, where theoretical equidistribution suggests acceptance by both tests for sufficiently large $N$.\n- Cases $3$ and $4$ probe rational $\\alpha$ with small denominators, where periodicity yields strong non-uniformity detectable by both tests.\n- Case $5$ probes a rational $\\alpha$ whose support aligns exactly with $B$ equal-width bins, potentially causing the histogram-based test to return observed counts equal to expected counts, while the KS test still detects the stepwise deviation from a continuous uniform distribution.\n- Case $6$ probes the same rational $\\alpha$ with a bin count that does not align with the period, making histogram-based non-uniformity apparent.\n\nFinal output specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each entry corresponds to one case and is a boolean indicating \"uniform\" ($\\text{True}$) or \"non-uniform\" ($\\text{False}$) according to Task $3$. For example, the output should look like $[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4,\\text{result}_5,\\text{result}_6]$.", "solution": "The problem statement is first subjected to a rigorous validation process.\n\n### Step 1: Extract Givens\n- **Sequence Definition**: $x_n = \\{n\\alpha\\}$, where $\\{y\\}$ is the fractional part of $y$, for $n \\in \\{1, 2, \\dots, N\\}$.\n- **Uniform Distribution Definition**: For any subinterval $[a,b) \\subseteq [0,1)$, the fraction of points $x_n$ in $[a,b)$ tends to $b-a$ as $N \\to \\infty$.\n- **Test 1**: Kolmogorov–Smirnov (KS) test, comparing the Empirical Cumulative Distribution Function (ECDF) of the sample to the CDF of a continuous uniform distribution on $[0,1)$, which is the identity function $F(t)=t$. A `$p$-value` is to be obtained.\n- **Test 2**: Pearson Chi-Square test on a histogram with $B$ equal-width bins. Observed frequencies are compared to the expected frequency $N/B$. A `$p$-value` is to be obtained.\n- **Decision Criterion**: A case is declared \"uniform\" if and only if both the KS `$p$-value` and the Chi-Square `$p$-value` are greater than or equal to a given significance level $\\gamma$. Otherwise, it is \"non-uniform\".\n- **Parameter Sets**:\n    1. $(\\alpha, N, B, \\gamma) = (\\sqrt{2}, 10000, 20, 0.01)$\n    2. $(\\alpha, N, B, \\gamma) = (\\frac{1+\\sqrt{5}}{2}, 12000, 50, 0.01)$\n    3. $(\\alpha, N, B, \\gamma) = (\\frac{1}{2}, 2000, 20, 0.01)$\n    4. $(\\alpha, N, B, \\gamma) = (\\frac{1}{4}, 2000, 20, 0.01)$\n    5. $(\\alpha, N, B, \\gamma) = (\\frac{3}{8}, 16000, 8, 0.01)$\n    6. $(\\alpha, N, B, \\gamma) = (\\frac{3}{8}, 16000, 16, 0.01)$\n- **Output Format**: A single line representing a list of boolean values, e.g., $[\\text{True},\\text{False},\\dots]$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the validation criteria.\n\n- **Scientifically Grounded**: The problem is fundamentally sound. It is based on the well-established mathematical theory of uniform distribution, specifically Weyl's criterion for sequences of the form $\\{n\\alpha\\}$. The statistical tests proposed, the Kolmogorov-Smirnov test and the Pearson Chi-Square test, are standard, valid methods for goodness-of-fit testing.\n- **Well-Posed**: The problem is well-posed. All parameters $(\\alpha, N, B, \\gamma)$ are explicitly provided for each case. The tasks are defined without ambiguity, and the criterion for classifying a sequence as \"uniform\" or \"non-uniform\" is precise and objective. A unique solution exists for each test case.\n- **Objective**: The problem is stated in objective, formal language. The definitions and procedures are standard in mathematics and statistics, leaving no room for subjective interpretation.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A complete solution will be provided.\n\n### Solution Derivation\nThe task is to assess the uniformity of six finite sequences generated by the rule $x_n = \\{n\\alpha\\}$ for $n = 1, \\dots, N$. This assessment is performed using two independent statistical tests, and a final verdict is reached based on a clear decision rule.\n\n**1. Sequence Generation**\nFor each parameter set $(\\alpha, N, B, \\gamma)$, we generate the sequence $\\{x_n\\}_{n=1}^N$. The term $x_n$ is the fractional part of $n\\alpha$. This is equivalent to calculating $n\\alpha \\pmod 1$. Computationally, for an array of indices $n = 1, 2, \\ldots, N$, we can generate the entire sequence vectorially.\n\n**2. Uniformity Tests**\nThe null hypothesis, $H_0$, for both tests is that the sample $\\{x_n\\}$ is drawn from a continuous uniform distribution on the interval $[0,1)$.\n\n- **Kolmogorov–Smirnov (KS) Test**: This test compares the Empirical Cumulative Distribution Function (ECDF) of the data, $F_N(t)$, with the cumulative distribution function (CDF) of the hypothesized distribution, $F(t)$. For the uniform distribution on $[0,1)$, the CDF is $F(t) = t$. The KS statistic, $D_N$, is the maximum absolute difference between these two functions:\n$$D_N = \\sup_{t \\in [0,1)} |F_N(t) - F(t)|$$\nwhere $F_N(t)$ is the proportion of sample points less than or equal to $t$. A large value of $D_N$ suggests the data does not follow the hypothesized distribution. We compute the `$p$-value`, $p_{KS}$, which is the probability of observing a test statistic at least as extreme as $D_N$ under $H_0$.\n\n- **Pearson's Chi-Square ($\\chi^2$) Test**: This test is applied to binned data. The interval $[0,1)$ is divided into $B$ disjoint subintervals (bins) of equal width, $1/B$. For each bin $i=1, \\dots, B$, we count the number of data points, $O_i$, that fall into it. Under the null hypothesis of uniformity, the expected number of points in each bin is $E_i = N/B$. The $\\chi^2$ statistic measures the discrepancy between observed and expected frequencies:\n$$\\chi^2 = \\sum_{i=1}^{B} \\frac{(O_i - E_i)^2}{E_i}$$\nThis statistic follows a $\\chi^2$ distribution with $B-1$ degrees of freedom. We compute the corresponding `$p$-value`, $p_{\\chi^2}$.\n\n**3. Decision Rule**\nFor each case, we determine the outcome based on the provided significance level $\\gamma = 0.01$. The sequence is classified as \"uniform\" if and only if both tests fail to reject the null hypothesis at this level. Mathematically, the condition is:\n$$ (p_{KS} \\ge \\gamma) \\land (p_{\\chi^2} \\ge \\gamma) $$\nIf this condition holds, the result is $\\text{True}$; otherwise, it is $\\text{False}$.\n\n**4. Analysis of Test Cases**\n\n- **Case 1  2**: $\\alpha=\\sqrt{2}$ and $\\alpha=\\frac{1+\\sqrt{5}}{2}$ (the golden ratio). Both are irrational numbers. According to Weyl's criterion, the infinite sequence $\\{n\\alpha\\}$ is uniformly distributed on $[0,1)$. For large $N$ ($10000$ and $12000$), the finite sequences should approximate a uniform distribution well. Therefore, we expect both $p_{KS}$ and $p_{\\chi^2}$ to be greater than $\\gamma=0.01$, leading to a \"uniform\" ($\\text{True}$) classification.\n\n- **Case 3  4**: $\\alpha=1/2$ and $\\alpha=1/4$. Both are rational numbers with small denominators. The sequence $\\{n\\alpha\\}$ is periodic and visits only a small, finite set of values. For $\\alpha=1/2$, the sequence is $\\{0.5, 0, 0.5, 0, \\dots\\}$. For $\\alpha=1/4$, the sequence is $\\{0.25, 0.5, 0.75, 0, \\dots\\}$. These sequences are patently non-uniform. Both the KS test (detecting the large gaps in the ECDF) and the $\\chi^2$ test (detecting highly uneven bin counts) should strongly reject $H_0$, yielding `$p$-values` close to $0$. The classification for both cases should be \"non-uniform\" ($\\text{False}$).\n\n- **Case 5**: $\\alpha=3/8$ and $B=8$. Here, $\\alpha$ is rational with denominator $8$. The sequence consists of $8$ distinct values: $\\{0, 1/8, 2/8, \\dots, 7/8\\}$, each appearing $N/8 = 16000/8 = 2000$ times. The number of bins, $B=8$, aligns perfectly with the denominator. The bins are $[0, 1/8), [1/8, 2/8), \\dots$. The points in the sequence are $\\{0, 3/8, 6/8, 1/8, 4/8, 7/8, 2/8, 5/8\\}$. Each of the $8$ values $\\{k/8 \\mid k=0,\\dots,7\\}$ falls into a unique bin. As each value appears $2000$ times, the observed count for each bin is $O_i = 2000$. The expected count is $E_i = N/B = 16000/8 = 2000$. Thus, $O_i = E_i$ for all $i$, yielding a $\\chi^2$ statistic of $0$ and a $p_{\\chi^2}$ of $1$. The $\\chi^2$ test fails to detect the non-uniformity. However, the ECDF is a step function with $8$ large steps, a significant deviation from the linear CDF of a uniform distribution. The KS test should capture this, yielding a very small $p_{KS}$. Since $p_{KS}  0.01$, the combined result is \"non-uniform\" ($\\text{False}$).\n\n- **Case 6**: $\\alpha=3/8$ and $B=16$. The sequence is the same as in Case 5, containing only values of the form $k/8$. When binned into $16$ bins of width $1/16$, the points $\\{0, 2/16, 4/16, \\dots, 14/16\\}$ will only occupy bins with even indices. All bins with odd indices will be empty. The observed counts will be highly non-uniform, leading to a large $\\chi^2$ statistic and a $p_{\\chi^2}$ near $0$. The KS test will also detect the discrete nature of the data. Thus, both tests should reject $H_0$, resulting in a \"non-uniform\" ($\\text{False}$) classification.\n\nThe implementation will proceed by calculating these values for each case and applying the decision rule.", "answer": "```python\nimport numpy as np\nfrom scipy import stats\n\ndef solve():\n    \"\"\"\n    Performs uniformity testing on sequences x_n = {n*alpha} for six test cases.\n    \"\"\"\n    test_cases = [\n        # Case 1: alpha is irrational (sqrt(2))\n        (np.sqrt(2), 10000, 20, 0.01),\n        # Case 2: alpha is irrational (golden ratio)\n        ((1 + np.sqrt(5)) / 2, 12000, 50, 0.01),\n        # Case 3: alpha is rational (1/2), small period\n        (1/2, 2000, 20, 0.01),\n        # Case 4: alpha is rational (1/4), small period\n        (1/4, 2000, 20, 0.01),\n        # Case 5: alpha is rational (3/8), B matches period\n        (3/8, 16000, 8, 0.01),\n        # Case 6: alpha is rational (3/8), B does not match period\n        (3/8, 16000, 16, 0.01),\n    ]\n\n    results = []\n    for alpha, N, B, gamma in test_cases:\n        # Task 1: Construct the finite sequence\n        n_values = np.arange(1, N + 1)\n        x_sequence = (n_values * alpha) % 1\n\n        # Task 2: Implement two formal tests of uniformity\n        \n        # Kolmogorov-Smirnov test\n        # H0: The sample is drawn from a continuous uniform distribution on [0, 1)\n        # The CDF for 'uniform' in scipy.stats.kstest is the identity function on [0,1].\n        ks_statistic, p_value_ks = stats.kstest(x_sequence, 'uniform')\n\n        # Pearson Chi-Square test\n        # H0: The sample is drawn from a uniform distribution.\n        # 1. Create a histogram with B equal-width bins on [0, 1).\n        observed_frequencies, _ = np.histogram(x_sequence, bins=B, range=(0, 1))\n        \n        # 2. Compute the p-value.\n        # stats.chisquare by default assumes expected frequencies are uniform,\n        # which is exactly sum(observed_frequencies) / len(observed_frequencies) = N/B.\n        chi2_statistic, p_value_chi2 = stats.chisquare(f_obs=observed_frequencies)\n\n        # Task 3: Declare \"uniform\" or \"non-uniform\"\n        # \"uniform\" if and only if both tests fail to reject H0 at level gamma.\n        is_uniform = (p_value_ks = gamma) and (p_value_chi2 = gamma)\n        results.append(is_uniform)\n\n    # Final output specification\n    # A single line with a comma-separated list of booleans enclosed in square brackets.\n    # Python's str() on a boolean produces \"True\" or \"False\", which is what is needed.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3201353"}, {"introduction": "While many statistical tests operate on a fixed batch of data, many real-world applications require monitoring a continuous stream of numbers and detecting problems as they arise. This final practice introduces a more dynamic, sequential approach to uniformity testing. You will design a real-time monitor by constructing a random walk from the incoming numbers and deriving dynamic \"alarm\" boundaries from Hoeffding's concentration inequality. This method provides a powerful framework for flagging a data stream as non-uniform the moment its cumulative deviation becomes statistically improbable, a vital technique in online quality control and data analysis.", "problem": "Consider a sequence of independent Uniform random numbers on the unit interval, denoted by $U_1, U_2, \\dots, U_N$, where each $U_i$ is independent and identically distributed (i.i.d.) as $\\mathrm{Uniform}[0,1]$. The fundamental base is that $U_i$ has expectation $\\mathbb{E}[U_i] = \\tfrac{1}{2}$ and is bounded in the interval $[0,1]$. Define the centered partial sums $S_k = \\sum_{i=1}^{k} \\left(U_i - \\tfrac{1}{2}\\right)$, which form a zero-mean bounded random walk under the null hypothesis of uniformity.\n\nYour task is to design a sequential uniformity monitor that triggers a flag the first time $k$ such that the cumulative sum $S_k$ leaves a prescribed acceptance region bounded by symmetric time-varying thresholds $\\pm h_k$. The acceptance boundaries $\\{h_k\\}_{k=1}^N$ must be derived from well-tested concentration principles for bounded independent variables using Hoeffding's inequality and a union bound argument so that, under the null hypothesis that $U_i$ are i.i.d. $\\mathrm{Uniform}[0,1]$, the probability that any boundary is crossed at some time $k \\in \\{1,2,\\dots,N\\}$ is at most a preassigned level $\\alpha \\in (0,1)$.\n\nThe monitor must operate as follows: for each time $k = 1, 2, \\dots, N$, compare $S_k$ to the boundaries $\\pm h_k$ and stop immediately when $S_k \\ge h_k$ or $S_k \\le -h_k$. The monitor outputs the signed index of the first crossing time: if the upper boundary is crossed first at time $k^\\star$, output the integer $k^\\star$; if the lower boundary is crossed first at time $k^\\star$, output the integer $-k^\\star$; if no crossing occurs for all $k \\le N$, output the integer $0$.\n\nImplement this monitor for the following test suite. Each test case specifies the sequence length $N$, the false alarm level $\\alpha$, and how to generate the sequence $U_1, \\dots, U_N$. Random sequences must be generated deterministically using the specified seed to ensure reproducibility.\n\nTest suite:\n1. Case A (happy path): $N = 500$, $\\alpha = 0.01$, $U_i$ generated as i.i.d. $\\mathrm{Uniform}[0,1]$ with pseudorandom generator seed $123$.\n2. Case B (positive bias): $N = 200$, $\\alpha = 0.01$, $U_i$ generated as $U_i = \\min\\{1, V_i + 0.2\\}$ where $V_i$ are i.i.d. $\\mathrm{Uniform}[0,1]$ with seed $456$.\n3. Case C (negative bias): $N = 200$, $\\alpha = 0.01$, $U_i$ generated as $U_i = \\max\\{0, V_i - 0.2\\}$ where $V_i$ are i.i.d. $\\mathrm{Uniform}[0,1]$ with seed $789$.\n4. Case D (boundary condition, short sequence): $N = 1$, $\\alpha = 0.01$, $U_1 = 0.9$ deterministically.\n5. Case E (edge case, perfectly centered): $N = 300$, $\\alpha = 0.01$, $U_i = \\tfrac{1}{2}$ deterministically for all $i$.\n\nYour program must compute the signed first crossing index for each case, as defined above, using boundaries derived from the concentration inequality approach with overall false alarm level $\\alpha$ across the horizon $N$ under the null hypothesis. The final output format must be a single line containing the results for the five cases as a comma-separated list enclosed in square brackets, with no spaces, in the order of the cases listed above. For example, if the results were integers $r_1, r_2, r_3, r_4, r_5$, the output must be printed exactly as $[r_1,r_2,r_3,r_4,r_5]$.", "solution": "The problem statement has been critically examined and is determined to be valid. It is scientifically grounded in probability theory and statistics, well-posed with a clear objective and sufficient information, and formulated with objective, unambiguous language. The task is to implement a sequential uniformity test for random numbers based on a random walk of centered partial sums, with stopping boundaries derived from Hoeffding's inequality and a union bound argument to control the overall false alarm probability.\n\nThe core of the problem is to construct time-varying boundaries $\\pm h_k$ for a sequence of cumulative sums $S_k = \\sum_{i=1}^{k} \\left(U_i - \\frac{1}{2}\\right)$, where under the null hypothesis, $\\{U_i\\}_{i=1}^N$ are independent and identically distributed (i.i.d.) as $\\mathrm{Uniform}[0,1]$. The boundaries must ensure that the probability of the random walk crossing them at any point up to time $N$ is no more than a specified level $\\alpha$.\n$$ P\\left(\\bigcup_{k=1}^{N} \\{|S_k| \\ge h_k\\}\\right) \\le \\alpha $$\n\nWe begin by defining the random variables $X_i = U_i - \\frac{1}{2}$. Under the null hypothesis, the $X_i$ are i.i.d. with expectation $\\mathbb{E}[X_i] = 0$. Since $U_i \\in [0,1]$, each $X_i$ is bounded in the interval $[-\\frac{1}{2}, \\frac{1}{2}]$. The range of each $X_i$ is $(\\frac{1}{2}) - (-\\frac{1}{2}) = 1$. The sum is $S_k = \\sum_{i=1}^k X_i$.\n\nWe apply Hoeffding's inequality, which provides a bound on the tail probability of a sum of bounded, independent random variables. For our sum $S_k$, the inequality is:\n$$ P(S_k \\ge t) \\le \\exp\\left( - \\frac{2t^2}{\\sum_{i=1}^k (b_i - a_i)^2} \\right) $$\nWith $b_i-a_i = 1$ for all $i$, the denominator becomes $\\sum_{i=1}^k 1^2 = k$. Thus,\n$$ P(S_k \\ge t) \\le \\exp\\left( - \\frac{2t^2}{k} \\right) $$\nBy symmetry of the setup ($X_i$ vs $-X_i$), the same bound applies for $P(S_k \\le -t)$. Using a union bound for a single time step $k$, we get a two-sided bound:\n$$ P(|S_k| \\ge t) = P(S_k \\ge t \\text{ or } S_k \\le -t) \\le 2 \\exp\\left( - \\frac{2t^2}{k} \\right) $$\n\nTo control the overall false alarm rate over the entire horizon $k=1, \\dots, N$, we use the union bound (also known as Boole's inequality) across all time steps. Let $A_k$ be the event $|S_k| \\ge h_k$.\n$$ P(\\text{any crossing}) = P\\left(\\bigcup_{k=1}^{N} A_k\\right) \\le \\sum_{k=1}^{N} P(A_k) = \\sum_{k=1}^{N} P(|S_k| \\ge h_k) $$\nWe can substitute our Hoeffding bound for each term in the sum:\n$$ \\sum_{k=1}^{N} P(|S_k| \\ge h_k) \\le \\sum_{k=1}^{N} 2 \\exp\\left( - \\frac{2h_k^2}{k} \\right) $$\nTo ensure the total probability is bounded by $\\alpha$, we need to select the thresholds $\\{h_k\\}$ such that this sum is less than or equal to $\\alpha$. A straightforward and common approach is to set the bound on the probability for each individual test $P(|S_k| \\ge h_k)$ to be a constant fraction of $\\alpha$, such as $\\alpha/N$. This is an application of the Bonferroni correction.\nWe set:\n$$ 2 \\exp\\left( - \\frac{2h_k^2}{k} \\right) = \\frac{\\alpha}{N} $$\nSolving this equation for $h_k$ gives us the explicit formula for the boundaries:\n$$ \\exp\\left( - \\frac{2h_k^2}{k} \\right) = \\frac{\\alpha}{2N} $$\n$$ - \\frac{2h_k^2}{k} = \\ln\\left(\\frac{\\alpha}{2N}\\right) $$\n$$ \\frac{2h_k^2}{k} = -\\ln\\left(\\frac{\\alpha}{2N}\\right) = \\ln\\left(\\frac{2N}{\\alpha}\\right) $$\n$$ h_k^2 = \\frac{k}{2} \\ln\\left(\\frac{2N}{\\alpha}\\right) $$\n$$ h_k = \\sqrt{\\frac{k}{2} \\ln\\left(\\frac{2N}{\\alpha}\\right)} $$\nWith this choice, the sum of the probability bounds is $\\sum_{k=1}^N \\frac{\\alpha}{N} = \\alpha$, satisfying the design requirement.\n\nThe monitoring procedure is then implemented as follows:\n1. For a given test case ($N$, $\\alpha$, and sequence generation rule), pre-compute the constant factor for the boundaries: $C = \\sqrt{\\frac{1}{2} \\ln(\\frac{2N}{\\alpha})}$. The boundary at time $k$ is then $h_k = C\\sqrt{k}$.\n2. Initialize the cumulative sum $S_0 = 0$.\n3. Iterate from $k=1$ to $N$:\n    a. Generate or retrieve the $k$-th value, $U_k$.\n    b. Update the centered sum: $S_k = S_{k-1} + (U_k - \\frac{1}{2})$.\n    c. Compare $S_k$ with the boundaries $\\pm h_k$.\n    d. If $S_k \\ge h_k$, stop and report the crossing time as $k$.\n    e. If $S_k \\le -h_k$, stop and report the crossing time as $-k$.\n4. If the loop completes without any boundary crossing, report $0$.\n\nThis procedure is deterministic once the sequence of $U_i$ values is fixed, and it will be applied to each of the specified test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_monitor(N, alpha, U_sequence):\n    \"\"\"\n    Runs the sequential uniformity monitor for a given sequence.\n\n    Args:\n        N (int): The maximum length of the sequence to monitor.\n        alpha (float): The desired overall false alarm probability.\n        U_sequence (np.ndarray): The sequence of numbers to test.\n\n    Returns:\n        int: The signed index of the first crossing time, or 0 if no crossing occurs.\n    \"\"\"\n    # Defensive check in case N=0 or alpha is invalid, although problem constraints\n    # make this unlikely. The log would be undefined for N=0.\n    if N == 0:\n        return 0\n    if not (0  alpha  1):\n        raise ValueError(\"alpha must be in (0,1)\")\n\n    # Pre-compute the constant part of the boundary calculation\n    # h_k = sqrt(k/2 * ln(2N/alpha)) = C * sqrt(k)\n    boundary_constant = np.sqrt(0.5 * np.log(2 * N / alpha))\n\n    # Initialize the centered cumulative sum\n    S_k = 0.0\n\n    # Iterate through the sequence, updating the sum and checking boundaries\n    for k_idx, u_val in enumerate(U_sequence):\n        k = k_idx + 1  # Time index is 1-based\n        S_k += u_val - 0.5\n        h_k = boundary_constant * np.sqrt(k)\n\n        if S_k = h_k:\n            return k\n        if S_k = -h_k:\n            return -k\n\n    # If the loop completes without crossing, return 0\n    return 0\n\ndef solve():\n    \"\"\"\n    Solves the problem by running the monitor on all specified test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'id': 'A', 'N': 500, 'alpha': 0.01, 'gen_type': 'uniform', 'seed': 123},\n        {'id': 'B', 'N': 200, 'alpha': 0.01, 'gen_type': 'positive_bias', 'seed': 456},\n        {'id': 'C', 'N': 200, 'alpha': 0.01, 'gen_type': 'negative_bias', 'seed': 789},\n        {'id': 'D', 'N': 1, 'alpha': 0.01, 'gen_type': 'deterministic', 'U_seq': np.array([0.9])},\n        {'id': 'E', 'N': 300, 'alpha': 0.01, 'gen_type': 'deterministic', 'U_seq': np.full(300, 0.5)}\n    ]\n\n    results = []\n    for case in test_cases:\n        N = case['N']\n        alpha = case['alpha']\n        \n        if case['gen_type'] == 'uniform':\n            rng = np.random.default_rng(seed=case['seed'])\n            U = rng.uniform(0, 1, size=N)\n        elif case['gen_type'] == 'positive_bias':\n            rng = np.random.default_rng(seed=case['seed'])\n            V = rng.uniform(0, 1, size=N)\n            U = np.minimum(1.0, V + 0.2)\n        elif case['gen_type'] == 'negative_bias':\n            rng = np.random.default_rng(seed=case['seed'])\n            V = rng.uniform(0, 1, size=N)\n            U = np.maximum(0.0, V - 0.2)\n        elif case['gen_type'] == 'deterministic':\n            U = case['U_seq']\n        \n        result = run_monitor(N, alpha, U)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3201429"}]}