{"hands_on_practices": [{"introduction": "One of the most common pitfalls in numerical computation is catastrophic cancellation, which occurs when subtracting two nearly equal numbers, leading to a dramatic loss of relative precision. This exercise explores this phenomenon through the function $d(x) = 1 - \\frac{\\sin(x)}{x}$, which approaches zero for small values of $x$. You will analyze why the direct computation is numerically unstable and then develop a robust alternative using a Taylor series expansion, a fundamental technique for creating stable algorithms [@problem_id:3212241].", "problem": "Consider the function $f(x) = \\begin{cases}\\dfrac{\\sin x}{x}, & x \\neq 0,\\\\ 1, & x = 0.\\end{cases}$ evaluated in radians. The aim is to study loss of significance for small $x$ when computing $f(x)$ using standard floating-point arithmetic and to design a numerically stable alternative for accurately capturing the small decrement $d(x) = 1 - f(x)$.\n\nStart from the following foundational bases:\n- The Taylor series for the sine function about $x = 0$: $\\sin x = x - \\dfrac{x^{3}}{3!} + \\dfrac{x^{5}}{5!} - \\cdots$.\n- The induced series for $f(x)$ and the associated decrement $d(x)$:\n$$f(x) = 1 - \\dfrac{x^{2}}{3!} + \\dfrac{x^{4}}{5!} - \\cdots,\\quad d(x) = 1 - f(x) = \\dfrac{x^{2}}{3!} - \\dfrac{x^{4}}{5!} + \\dfrac{x^{6}}{7!} - \\cdots.$$\n- The standard floating-point model: for a basic operation $\\circ \\in \\{+,-,\\times,\\div\\}$, $\\operatorname{fl}(a \\circ b) = (a \\circ b)(1 + \\delta)$ with $|\\delta| \\le u$, where $u$ is the unit roundoff of the working precision.\n\nTasks:\n1. Using the above bases, reason about why directly computing $\\operatorname{fl}(\\sin x)/\\operatorname{fl}(x)$ has small relative error for $f(x)$ when $x$ is small, yet can completely lose information about the decrement $d(x)$ due to subtractive cancellation in forming $1 - \\operatorname{fl}(\\sin x)/\\operatorname{fl}(x)$. Derive a scale for the threshold in terms of $u$ at which the naive computation cannot reliably resolve $d(x)$ (express this threshold asymptotically in $u$).\n2. Design a stable series-based evaluation that:\n   - For $|x|$ below a threshold $\\,\\tau\\,$ based on $u$, computes $d(x)$ via its alternating series and then forms $f(x) = 1 - d(x)$ only at the end, using a numerically stable summation strategy.\n   - For $|x| \\ge \\tau$, uses the direct evaluation $f(x) = \\sin(x)/x$.\n   - Handles the removable singularity at $x = 0$ by returning $f(0) = 1$.\n3. Implement a complete, runnable program that:\n   - Uses standard double-precision floating-point arithmetic for the primary computations.\n   - Uses a high-precision reference based on a truncated series computed with arbitrary precision arithmetic to approximate the true values of $f(x)$ and $d(x)$ to many correct digits.\n   - For each test input $x$, computes three error metrics:\n     - The relative error of the naive $f(x)$: $\\left|\\dfrac{f_{\\text{naive}} - f_{\\text{ref}}}{f_{\\text{ref}}}\\right|$.\n     - The relative error of the naive decrement $d_{\\text{naive}} = 1 - f_{\\text{naive}}$ against $d_{\\text{ref}}$: $\\left|\\dfrac{d_{\\text{naive}} - d_{\\text{ref}}}{d_{\\text{ref}}}\\right|$ (define this as $0$ if $d_{\\text{ref}} = 0$).\n     - The relative error of the stable series decrement $d_{\\text{stable}}$ against $d_{\\text{ref}}$: $\\left|\\dfrac{d_{\\text{stable}} - d_{\\text{ref}}}{d_{\\text{ref}}}\\right|$ (define this as $0$ if $d_{\\text{ref}} = 0$).\n   - Uses radians and no physical units.\n\nTest suite:\n- Evaluate the program for the inputs $x \\in \\{0,\\;10^{-12},\\;10^{-10},\\;10^{-8},\\;-10^{-7},\\;10^{-7},\\;10^{-4},\\;10^{-1}\\}$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of lists, where each inner list corresponds to one test input and contains the three error metrics in the order specified above. For example: \"[[e11,e12,e13],[e21,e22,e23],...]\" with all entries as floating-point numbers. The angle unit is radians, and no percentage symbols are to be used anywhere in the computation or output.", "solution": "The problem is subjected to validation against the established criteria.\n\n### Step 1: Extract Givens\n- **Function definition**: $f(x) = \\begin{cases}\\dfrac{\\sin x}{x}, & x \\neq 0,\\\\ 1, & x = 0.\\end{cases}$ evaluated in radians.\n- **Decrement definition**: $d(x) = 1 - f(x)$.\n- **Taylor series for sine**: $\\sin x = x - \\dfrac{x^{3}}{3!} + \\dfrac{x^{5}}{5!} - \\cdots$.\n- **Series for $f(x)$ and $d(x)$**: $f(x) = 1 - \\dfrac{x^{2}}{3!} + \\dfrac{x^{4}}{5!} - \\cdots$, and $d(x) = \\dfrac{x^{2}}{3!} - \\dfrac{x^{4}}{5!} + \\dfrac{x^{6}}{7!} - \\cdots$.\n- **Floating-point model**: $\\operatorname{fl}(a \\circ b) = (a \\circ b)(1 + \\delta)$ with $|\\delta| \\le u$, where $u$ is the unit roundoff.\n- **Task 1**: Analyze why direct computation of $f(x)$ has small relative error but computing $d(x)$ via $1 - f(x)$ suffers from catastrophic cancellation for small $x$. Derive the threshold scale in terms of $u$.\n- **Task 2**: Design a stable hybrid algorithm using a series expansion for $|x| < \\tau$ and direct evaluation for $|x| \\ge \\tau$.\n- **Task 3**: Implement a program to compute three specified error metrics (relative error of naive $f(x)$, naive $d(x)$, and stable $d(x)$) against a high-precision reference for a given test suite.\n- **Execution Environment and Precision**: Primary computations in double-precision, reference in arbitrary-precision.\n- **Test Suite**: $x \\in \\{0,\\;10^{-12},\\;10^{-10},\\;10^{-8},\\;-10^{-7},\\;10^{-7},\\;10^{-4},\\;10^{-1}\\}$.\n- **Output Format**: A single line string representing a list of lists of error metrics: `[[e11,e12,e13],[e21,e22,e23],...]`.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is a standard exercise in numerical analysis focusing on catastrophic cancellation, a core concept in scientific computing.\n- **Scientifically Grounded (Critical)**: The problem is fundamentally based on Taylor series expansions and the standard model of floating-point arithmetic, which are cornerstones of numerical methods. It is scientifically sound.\n- **Well-Posed**: The problem is well-posed. It clearly defines the function, the quantities to be computed, the methods to be analyzed, and the expected output. A unique and meaningful solution exists.\n- **Objective (Critical)**: The language is precise and mathematical. There are no subjective or opinion-based statements.\nThe problem is self-contained and consistent. The request for an \"arbitrary precision\" reference is satisfied by using the `decimal` module from Python's standard library, which is permitted. All other criteria are met.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n---\n\n### Analysis of Numerical Error and Algorithm Design\n\nThis solution addresses the three tasks outlined in the problem statement: analyzing the numerical instability, designing a stable algorithm, and implementing it for verification.\n\n#### 1. Analysis of Numerical Errors in Naive Computation\n\nWe analyze the errors in computing $f(x)$ and $d(x)$ for small values of $x$ using standard floating-point arithmetic.\n\n**Error in computing $f(x)$:**\nThe \"naive\" computation of $f(x)$ for $x \\neq 0$ is $f_{\\text{naive}}(x) = \\operatorname{fl}(\\sin(x) / x)$. Let's model the floating-point errors. The evaluation of the sine function and the division introduce relative errors bounded by the unit roundoff $u$.\nFirst, the sine function is computed: $\\operatorname{fl}(\\sin x) = (\\sin x)(1 + \\delta_1)$, where $|\\delta_1| \\le u$.\nThen, the division is performed:\n$$ \\operatorname{fl}\\left(\\frac{\\operatorname{fl}(\\sin x)}{x}\\right) = \\left(\\frac{(\\sin x)(1 + \\delta_1)}{x}\\right)(1 + \\delta_2) = \\frac{\\sin x}{x}(1 + \\delta_1)(1 + \\delta_2) $$\nwhere $|\\delta_2| \\le u$. Assuming $x$ is an exact floating-point number.\nLet $\\hat{f}(x)$ denote the computed value. Expanding the error terms, we get:\n$$ \\hat{f}(x) = f(x)(1 + \\delta_1 + \\delta_2 + \\delta_1\\delta_2) \\approx f(x)(1 + \\delta_{\\text{f}}) $$\nwhere $\\delta_{\\text{f}} = \\delta_1 + \\delta_2$. The total relative error in computing $f(x)$ is:\n$$ \\left|\\frac{\\hat{f}(x) - f(x)}{f(x)}\\right| \\approx |\\delta_{\\text{f}}| \\le |\\delta_1| + |\\delta_2| \\le 2u $$\nThis relative error is small, on the order of the unit roundoff $u$. Therefore, the direct computation of $f(x)$ is numerically stable across its domain.\n\n**Error in computing $d(x) = 1 - f(x)$:**\nThe naive computation of the decrement is $\\hat{d}(x) = \\operatorname{fl}(1 - \\hat{f}(x))$. For small $x$, we know from the Taylor series that $f(x) = 1 - \\frac{x^2}{6} + O(x^4)$, which is very close to $1$. The subtraction $1 - \\hat{f}(x)$ is a classic example of catastrophic cancellation, where two nearly equal numbers are subtracted, leading to a potential loss of relative precision.\n\nLet's analyze this more formally. The computed decrement is:\n$$ \\hat{d}(x) = \\operatorname{fl}(1 - \\hat{f}(x)) = (1 - \\hat{f}(x))(1 + \\delta_3) \\quad \\text{with } |\\delta_3| \\le u $$\nSubstituting the expression for $\\hat{f}(x)$:\n$$ \\hat{d}(x) = (1 - f(x)(1 + \\delta_{\\text{f}}))(1 + \\delta_3) = (1 - f(x) - f(x)\\delta_{\\text{f}})(1 + \\delta_3) $$\nSince $d(x) = 1 - f(x)$, this becomes:\n$$ \\hat{d}(x) = (d(x) - f(x)\\delta_{\\text{f}})(1 + \\delta_3) = d(x) - f(x)\\delta_{\\text{f}} + d(x)\\delta_3 - f(x)\\delta_{\\text{f}}\\delta_3 $$\nThe absolute error is $\\hat{d}(x) - d(x) \\approx -f(x)\\delta_{\\text{f}} + d(x)\\delta_3$.\nThe relative error in $d(x)$ is:\n$$ \\frac{\\hat{d}(x) - d(x)}{d(x)} \\approx \\frac{-f(x)\\delta_{\\text{f}} + d(x)\\delta_3}{d(x)} = -\\frac{f(x)}{d(x)}\\delta_{\\text{f}} + \\delta_3 $$\nFor small $x$, we have $f(x) \\approx 1$ and $d(x) \\approx x^2/6$. The error is dominated by the first term:\n$$ \\left|\\frac{\\hat{d}(x) - d(x)}{d(x)}\\right| \\approx \\left|-\\frac{f(x)}{d(x)}\\delta_{\\text{f}}\\right| \\approx \\frac{1}{x^2/6}|\\delta_{\\text{f}}| = \\frac{6|\\delta_{\\text{f}}|}{x^2} $$\nUsing the bound $|\\delta_{\\text{f}}| \\le 2u$, the relative error is bounded by:\n$$ \\left|\\frac{\\hat{d}(x) - d(x)}{d(x)}\\right| \\lesssim \\frac{12u}{x^2} $$\nThe naive computation becomes unreliable when this relative error is of order $1$ or greater. This happens when $12u/x^2 \\approx 1$, which gives the threshold:\n$$ |x| \\approx \\sqrt{12u} $$\nFor double-precision arithmetic, $u = 2^{-53} \\approx 1.11 \\times 10^{-16}$. The threshold is $|x| \\approx \\sqrt{12 \\times 1.11 \\times 10^{-16}} \\approx 3.65 \\times 10^{-8}$. For values of $|x|$ at or below this scale, the naive computation of $d(x)$ loses most or all of its significant digits. The threshold is asymptotically of order $O(\\sqrt{u})$.\n\n#### 2. Design of a Stable Algorithm\n\nTo overcome the catastrophic cancellation, we design a hybrid algorithm that avoids the problematic subtraction for small $|x|$.\nThe algorithm is as follows:\n- A threshold $\\tau$ is chosen based on the error analysis. A practical choice for double precision is $\\tau = 10^{-7}$, which is slightly larger than the theoretical onset of total significance loss ($\\approx 3.65 \\times 10^{-8}$), providing a safe margin.\n- **For $|x| < \\tau$**: Instead of computing $f(x)$ first, we directly compute the decrement $d(x)$ using its Taylor series expansion:\n  $$ d(x) = \\frac{x^{2}}{3!} - \\frac{x^{4}}{5!} + \\frac{x^{6}}{7!} - \\cdots = \\sum_{k=1}^{\\infty} (-1)^{k-1} \\frac{x^{2k}}{(2k+1)!} $$\n  This is an alternating series whose terms decrease rapidly in magnitude for $|x| < 1$. We can compute $d(x)$ accurately by summing a few terms. This approach is stable because it constructs the small value $d(x)$ by summing other small, accurately-represented values, rather than as the difference of two large, nearly-equal numbers. Once an accurate value for $d_{\\text{stable}}(x)$ is found, $f(x)$ can be computed as $f_{\\text{stable}}(x) = 1 - d_{\\text{stable}}(x)$. This final subtraction is not subject to catastrophic cancellation because $d_{\\text{stable}}(x)$ is a small, accurately computed number.\n- **For $|x| \\ge \\tau$**: The naive computation $f(x) = \\sin(x)/x$ is used. In this regime, $f(x)$ is not sufficiently close to $1$ for the subtraction $1-f(x)$ to cause a catastrophic loss of precision in the context of double-precision numbers. The series evaluation would also become less efficient and potentially less accurate as more terms are needed for convergence.\n- **For $x=0$**: The function is handled as a special case, returning $f(0) = 1$ and $d(0)=0$ as per the definition.\n\n#### 3. Implementation and Verification\n\nThe implementation will consist of three main parts:\n1.  A high-precision reference calculation for $f_{\\text{ref}}$ and $d_{\\text{ref}}$ using Python's `decimal` module. The series for $d(x)$ is well-suited for this, summed to a high number of terms to ensure convergence to a precision of $50$ digits.\n2.  The naive computation functions, $f_{\\text{naive}}(x) = \\sin(x)/x$ and $d_{\\text{naive}}(x) = 1 - f_{\\text{naive}}(x)$.\n3.  The stable computation function, which implements the hybrid algorithm described above to find $d_{\\text{stable}}(x)$.\n\nFor each input $x$ from the test suite, we will compute the three requested relative errors:\n- $\\epsilon_1 = \\left|\\dfrac{f_{\\text{naive}} - f_{\\text{ref}}}{f_{\\text{ref}}}\\right|$\n- $\\epsilon_2 = \\left|\\dfrac{d_{\\text{naive}} - d_{\\text{ref}}}{d_{\\text{ref}}}\\right|$\n- $\\epsilon_3 = \\left|\\dfrac{d_{\\text{stable}} - d_{\\text{ref}}}{d_{\\text{ref}}}\\right|$\n\nFor the case $x=0$, $d_{\\text{ref}}=0$, and the error metrics $\\epsilon_2$ and $\\epsilon_3$ are defined to be $0$.", "answer": "```python\nimport numpy as np\nfrom decimal import Decimal, getcontext\n\ndef solve():\n    \"\"\"\n    Solves the numerical analysis problem of computing f(x) = sin(x)/x and d(x) = 1-f(x).\n    It compares a naive method with a stable, series-based method against a high-precision reference.\n    \"\"\"\n\n    # Set precision for the high-precision reference calculation. 50 digits is sufficient.\n    getcontext().prec = 50\n\n    def reference_d(x_str: str) -> Decimal:\n        \"\"\"Computes d(x) to high precision using its Taylor series.\"\"\"\n        if x_str == '0':\n            return Decimal(0)\n\n        x = Decimal(x_str)\n        x_sq = x * x\n        \n        # d(x) = sum_{k=1 to inf} (-1)^(k-1) * x^(2k) / (2k+1)!\n        # term_{k} = (-1)^(k-1) * x^(2k) / (2k+1)!\n        # The ratio term_{k+1} / term_{k} = -x^2 / ((2k+2)(2k+3))\n\n        k = 1\n        term = x_sq / Decimal(6)  # First term (k=1)\n        d_val = term\n        \n        # Sum until the next term is smaller than the context precision\n        while abs(term) > Decimal('1e-50'):\n            k += 1\n            term *= -x_sq / Decimal((2 * k) * (2 * k + 1))\n            d_val += term\n        \n        return d_val\n\n    def naive_f(x: float) -> float:\n        \"\"\"Computes f(x) = sin(x)/x naively.\"\"\"\n        if x == 0.0:\n            return 1.0\n        return np.sin(x) / x\n\n    def stable_d(x: float, threshold: float) -> float:\n        \"\"\"\n        Computes d(x) using a stable hybrid algorithm.\n        \"\"\"\n        if x == 0.0:\n            return 0.0\n        \n        if abs(x) < threshold:\n            # For small |x|, use the Taylor series for d(x) to avoid cancellation.\n            # d(x) = x^2/6 - x^4/120 + x^6/5040 - ...\n            x_sq = x * x\n            \n            # term_{k+1} = term_{k} * (-x^2) / ((2k+2)(2k+3))\n            k = 1\n            term = x_sq / 6.0  # First term (k=1)\n            d_val = term\n            \n            # Sum until convergence at double precision\n            for k_iter in range(2, 15): # 15 iterations is more than enough\n                term *= -x_sq / ((2 * k_iter) * (2 * k_iter + 1))\n                d_val_prev = d_val\n                d_val += term\n                if d_val == d_val_prev:\n                    break\n            return d_val\n        else:\n            # For larger |x|, direct computation is stable enough.\n            f_val = np.sin(x) / x\n            return 1.0 - f_val\n\n    # Define test cases from the problem statement\n    test_cases_str = ['0', '1e-12', '1e-10', '1e-8', '-1e-7', '1e-7', '1e-4', '1e-1']\n    test_cases_float = [float(x) for x in test_cases_str]\n    \n    # Threshold for switching to series expansion, based on O(sqrt(u)) analysis\n    # For double precision, u ~ 10^-16, sqrt(u) ~ 10^-8. 10^-7 is a safe choice.\n    threshold = 1e-7\n\n    all_results = []\n    for x_str, x_float in zip(test_cases_str, test_cases_float):\n        # 1. High-precision reference calculation\n        d_ref_dec = reference_d(x_str)\n        f_ref_dec = Decimal(1) - d_ref_dec\n        d_ref = float(d_ref_dec)\n        f_ref = float(f_ref_dec)\n\n        # 2. Naive computation\n        f_naive_val = naive_f(x_float)\n        d_naive_val = 1.0 - f_naive_val\n\n        # 3. Stable computation for the decrement\n        d_stable_val = stable_d(x_float, threshold)\n\n        # 4. Compute error metrics\n        if x_float == 0.0:\n            err_f_naive = 0.0\n            err_d_naive = 0.0\n            err_d_stable = 0.0\n        else:\n            # Relative error of naive f(x)\n            err_f_naive = abs((f_naive_val - f_ref) / f_ref) if f_ref != 0 else 0.0\n            \n            if d_ref == 0.0:\n                # This branch should not be taken for x != 0\n                err_d_naive = 0.0 if d_naive_val == 0.0 else float('inf')\n                err_d_stable = 0.0 if d_stable_val == 0.0 else float('inf')\n            else:\n                # Relative error of naive d(x)\n                err_d_naive = abs((d_naive_val - d_ref) / d_ref)\n                # Relative error of stable d(x)\n                err_d_stable = abs((d_stable_val - d_ref) / d_ref)\n        \n        all_results.append([err_f_naive, err_d_naive, err_d_stable])\n\n    # Format output as a string representing a list of lists, without spaces.\n    formatted_results = [f\"[{r[0]},{r[1]},{r[2]}]\" for r in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3212241"}, {"introduction": "Beyond the limitations of floating-point arithmetic, numerical errors can also arise from the formulas we use to approximate mathematical operations. This is known as truncation error. This practice problem serves as a cautionary tale, demonstrating how a standard numerical method—the central difference formula for the second derivative—can fail spectacularly. By choosing a step size $h$ that unintentionally aligns with the periodic nature of the function, the formula yields a significantly incorrect result, highlighting the critical importance of understanding the interplay between numerical methods and the characteristics of the problem at hand [@problem_id:2200124].", "problem": "In numerical analysis, the accuracy of finite difference formulas can be highly sensitive to the choice of the step size, especially for oscillatory functions. This problem explores a specific case of this sensitivity.\n\nConsider the function $f(x) = \\cos(kx) + 2k^2 x^2$, where $k$ is a positive real constant representing the wavenumber. We wish to evaluate the second derivative of this function at the point $x=0$.\n\nThe second-order central difference formula provides an approximation for the second derivative of a function $f(x)$ at a point $x_0$:\n$$\nf''(x_0) \\approx \\frac{f(x_0+h) - 2f(x_0) + f(x_0-h)}{h^2}\n$$\nwhere $h$ is the step size.\n\nSuppose you use this formula to approximate $f''(0)$, but you choose a step size $h$ that is exactly equal to the period of the oscillatory component of the function, i.e., $h = \\frac{2\\pi}{k}$.\n\nCalculate the ratio of the value obtained from the numerical approximation to the true analytical value of the second derivative $f''(0)$. Give your answer as an analytic expression.", "solution": "We have $f(x)=\\cos(kx)+2k^{2}x^{2}$. The analytical second derivative is obtained by differentiating twice:\n$$\nf'(x)=-k\\sin(kx)+4k^{2}x,\\qquad f''(x)=-k^{2}\\cos(kx)+4k^{2}.\n$$\nEvaluating at $x=0$ gives\n$$\nf''(0)=-k^{2}\\cos(0)+4k^{2}=-k^{2}+4k^{2}=3k^{2}.\n$$\n\nThe second-order central difference approximation at $x_{0}=0$ with step size $h$ is\n$$\nD^{(2)}_{h}f(0)=\\frac{f(h)-2f(0)+f(-h)}{h^{2}}.\n$$\nChoose $h=\\frac{2\\pi}{k}$, which is the period of $\\cos(kx)$. Then\n$$\nf(0)=\\cos(0)+0=1,\n$$\n$$\nf(h)=\\cos(kh)+2k^{2}h^{2}=\\cos(2\\pi)+2k^{2}\\left(\\frac{2\\pi}{k}\\right)^{2}=1+8\\pi^{2},\n$$\n$$\nf(-h)=\\cos(-kh)+2k^{2}h^{2}=\\cos(2\\pi)+8\\pi^{2}=1+8\\pi^{2}.\n$$\nTherefore, the numerator simplifies to\n$$\nf(h)-2f(0)+f(-h)=(1+8\\pi^{2})-2\\cdot 1+(1+8\\pi^{2})=16\\pi^{2},\n$$\nand since $h^{2}=\\left(\\frac{2\\pi}{k}\\right)^{2}=\\frac{4\\pi^{2}}{k^{2}}$, the finite-difference approximation is\n$$\nD^{(2)}_{h}f(0)=\\frac{16\\pi^{2}}{4\\pi^{2}/k^{2}}=4k^{2}.\n$$\nThe requested ratio of the numerical approximation to the true value is\n$$\n\\frac{4k^{2}}{3k^{2}}=\\frac{4}{3}.\n$$", "answer": "$$\\boxed{\\frac{4}{3}}$$", "id": "2200124"}, {"introduction": "The challenge of minimizing round-off error is so fundamental that solutions have been engineered directly into modern processors. This exercise introduces the Fused Multiply-Add (FMA) instruction, a hardware feature that computes expressions like $a \\times b + c$ with a single rounding, mitigating the error accumulation that occurs in separate operations. Through a series of hands-on coding tests involving polynomial evaluation and dot products, you will quantify the substantial accuracy gains provided by FMA and appreciate how hardware architecture plays a crucial role in robust scientific computing [@problem_id:3165813].", "problem": "Consider floating-point computation in a modern processor that supports fused multiply-add (FMA). Treat the floating-point operations using the standard rounding model: each primitive arithmetic operation returns the exact real result multiplied by a factor of the form $1 + \\delta$, where $\\delta$ is a small number whose magnitude is bounded by the machine precision $ \\epsilon $. In the absence of FMA, a compound operation such as $a\\times b + c$ undergoes separate rounding at multiplication and at addition. With FMA, $a\\times b + c$ is executed as a single operation with a single rounding. Catastrophic cancellation occurs when two numbers of nearly equal magnitude and opposite sign are added, causing severe loss of significant digits. Round-off error arises from finite precision rounding in arithmetic, and truncation error arises from approximations to continuous or infinite processes; in this problem, truncation error does not appear, but round-off and cancellation do.\n\nYou are to evaluate specific polynomials and dot products, measure numerical errors with and without using FMA, and quantify how FMA mitigates cancellation. Let the absolute error be defined as $E_{\\text{abs}} = \\lvert \\hat{y} - y \\rvert$ and the relative error as $E_{\\text{rel}} = \\frac{\\lvert \\hat{y} - y \\rvert}{\\lvert y \\rvert}$, where $\\hat{y}$ is the computed value and $y$ is a high-precision reference value. When $y = 0$, use absolute error; otherwise, use relative error.\n\nThe task is:\n\n1. Evaluate a polynomial using Horner’s scheme both with standard multiply-then-add and with FMA at given inputs. The polynomials are $p_1(x) = (x - 1)^8$ and $p_2(x) = (x - 1)^{12}$. Also include a baseline evaluation for $p_3(x) = (x - 1)^8$ at a value far from cancellation.\n\n2. Compute dot products of specified vector pairs with and without FMA accumulation.\n\n3. For each test case, compute the improvement factor $I = \\frac{E_{\\text{noFMA}}}{E_{\\text{FMA}}}$, using relative error when $y \\neq 0$ and absolute error when $y = 0$.\n\nUse a high-precision reference $y$ computed via arbitrary precision arithmetic to approximate the exact real arithmetic result for comparison. Then compute the improvement factor for each test case.\n\nTest Suite:\n- Polynomial tests via Horner’s method:\n  - $T_1$: $p_1(x)$ at $x = 1 + 10^{-8}$.\n  - $T_2$: $p_2(x)$ at $x = 1 + 10^{-10}$.\n  - $T_3$: $p_3(x)$ at $x = 2$.\n- Dot product tests:\n  - $T_4$: $v^{(A)} = [10^8, 10^8, 10^8, 10^8, 10^8, 10^8, 10^8, 10^8]$ and $w^{(A)} = [10^{-8}, -10^{-8}, 10^{-8}, -10^{-8}, 10^{-8}, -10^{-8}, 10^{-8}, -10^{-8}]$.\n  - $T_5$: $v^{(B)} = [10^{16}, 1, -10^{16}, 1, 10^{16}, -1]$ and $w^{(B)} = [10^{-16}, 1, 10^{-16}, 1, -10^{-16}, 1]$.\n  - $T_6$: $v^{(C)}$ of length $100$ with entries $v_i = (-1)^i \\cdot 10^8$ and $w^{(C)}$ of length $100$ with entries $w_i = 10^{-8}$, for $i = 1, \\dots, 100$.\n\nImplementation requirements:\n- Evaluate polynomials using Horner’s scheme in the form $y \\leftarrow y \\cdot x + a_i$, where $a_i$ are coefficients ordered from highest degree to constant term. The FMA variant should implement this update using a single fused operation $y \\leftarrow \\mathrm{fma}(y, x, a_i)$, that is, $y \\leftarrow (y \\cdot x + a_i)$ with one rounding.\n- Evaluate dot products as $\\sum_{i=1}^{n} v_i w_i$; the FMA variant should accumulate with $y \\leftarrow \\mathrm{fma}(v_i, w_i, y)$.\n\nReference values:\n- Obtain $y$ using arbitrary precision arithmetic applied to the same mathematical expressions, with numbers specified in decimal form (for instance, $10^{-8}$) so that the reference approximates exact real arithmetic closely.\n\nFinal Output:\n- Your program should produce a single line of output containing the six improvement factors for $T_1$ through $T_6$ as a comma-separated list enclosed in square brackets, in order $[I_{T_1}, I_{T_2}, I_{T_3}, I_{T_4}, I_{T_5}, I_{T_6}]$, each formatted to $12$ significant digits (for example, $[1.23456789012,2.0,3.14159265359, ...]$).", "solution": "The user's request is to solve a numerical analysis problem involving the comparison of standard floating-point arithmetic with arithmetic using Fused Multiply-Add (FMA) capabilities. The goal is to quantify the improvement FMA offers in mitigating numerical errors, specifically round-off error and catastrophic cancellation.\n\n### Problem Validation\n\nFirst, I will validate the problem statement according to the required procedure.\n\n#### Step 1: Extract Givens\n\n-   **Floating-Point Model**: Each primitive arithmetic operation is subject to rounding, modeled as multiplication by a factor of $(1 + \\delta)$, where $|\\delta| \\le \\epsilon$ ($\\epsilon$ is the machine precision).\n-   **Standard Computation**: An operation like $a \\times b + c$ involves two rounding steps: one for the multiplication and one for the addition, i.e., $fl(fl(a \\times b) + c)$.\n-   **FMA Computation**: An operation like $a \\times b + c$ is executed as a single unit with only one rounding step, i.e., $fl(a \\times b + c)$.\n-   **Error Metrics**: Absolute error $E_{\\text{abs}} = |\\hat{y} - y|$ and relative error $E_{\\text{rel}} = \\frac{|\\hat{y} - y|}{|y|}$. Relative error is used unless the true value $y=0$, in which case absolute error is used.\n-   **Improvement Factor**: $I = \\frac{E_{\\text{noFMA}}}{E_{\\text{FMA}}}$, where $E$ is the error (relative or absolute as appropriate).\n-   **Reference Value ($y$)**: A high-precision value computed using arbitrary-precision arithmetic to serve as a proxy for the exact real result.\n-   **Tasks**:\n    1.  Evaluate polynomials $p_1(x) = (x - 1)^8$ and $p_2(x) = (x - 1)^{12}$ using Horner's scheme, with and without FMA. A baseline case $p_3(x) = (x-1)^8$ is also tested.\n    2.  Compute dot products of specified vector pairs, with and without FMA accumulation.\n-   **Implementations**:\n    -   Horner's scheme update: $y \\leftarrow y \\cdot x + a_i$ (standard) and $y \\leftarrow \\mathrm{fma}(y, x, a_i)$ (FMA).\n    -   Dot product accumulation: $y \\leftarrow y + v_i \\cdot w_i$ (standard loop) and $y \\leftarrow \\mathrm{fma}(v_i, w_i, y)$ (FMA loop).\n-   **Test Suite**:\n    -   $T_1$: $p_1(x) = (x - 1)^8$ at $x = 1 + 10^{-8}$.\n    -   $T_2$: $p_2(x) = (x - 1)^{12}$ at $x = 1 + 10^{-10}$.\n    -   $T_3$: $p_3(x) = (x - 1)^8$ at $x = 2$.\n    -   $T_4$: $v^{(A)} = [10^8, \\dots, 10^8]$ (8 elements), $w^{(A)} = [10^{-8}, -10^{-8}, \\dots]$.\n    -   $T_5$: $v^{(B)} = [10^{16}, 1, -10^{16}, 1, 10^{16}, -1]$, $w^{(B)} = [10^{-16}, 1, 10^{-16}, 1, -10^{-16}, 1]$.\n    -   $T_6$: `v_i = (-1)^i \\cdot 10^8`, `w_i = 10^{-8}` for $i=1, \\dots, 100$.\n-   **Output Format**: A single-line list of six improvement factors, `[I_T1, ..., I_T6]`, formatted to $12$ significant digits.\n\n#### Step 2: Validate Using Extracted Givens\n\n-   **Scientifically Grounded**: The problem is fundamentally sound. It addresses core concepts of numerical analysis: floating-point arithmetic, round-off error, catastrophic cancellation, Horner's method, and the FMA instruction. The error models and definitions are standard.\n-   **Well-Posed**: The problem is well-posed. The inputs, required computations, and desired output are all explicitly defined. The tasks lead to a unique and meaningful set of numerical results. The derivation of polynomial coefficients from $(x-1)^n$ is a standard application of the binomial theorem and not a missing piece of information.\n-   **Objective**: The problem is stated in precise, objective, and technical language, free from ambiguity or subjective claims.\n\nThe problem does not exhibit any of the invalidity flaws. It is a well-formulated, standard problem in an introductory computational science or numerical analysis course.\n\n#### Step 3: Verdict and Action\n\nThe problem is **valid**. A solution will be provided.\n\n### Principle-Based Solution Design\n\nThe core of this problem lies in the difference between fused and non-fused computations.\n\n1.  **Fundamental Principle**: A standard floating-point operation like $z = a \\times b + c$ is executed as two separate operations: a multiplication followed by an addition. Each operation incurs a rounding error.\n    -   $\\hat{p} = fl(a \\times b) = (a \\times b)(1+\\delta_1)$\n    -   $\\hat{z} = fl(\\hat{p} + c) = (\\hat{p} + c)(1+\\delta_2)$\n    The Fused Multiply-Add (FMA) instruction, available on modern processors, performs this entire operation with a single rounding:\n    -   $\\hat{z}_{\\text{FMA}} = fl(a \\times b + c) = (a \\times b + c)(1+\\delta_3)$\n    FMA computes the product $a \\times b$ to a higher intermediate precision before adding $c$, thereby reducing the total round-off error. This is especially impactful in two scenarios:\n    a.  When the product $a \\times b$ is itself not exactly representable as a standard float, FMA prevents the loss of information that would occur from rounding it before the addition.\n    b.  When $fl(a \\times b)$ and $c$ are nearly equal in magnitude and opposite in sign, their sum suffers from catastrophic cancellation. FMA can mitigate this by adding $c$ to the un-rounded, high-precision product $a \\times b$.\n\n2.  **Polynomial Evaluation (Horner's Method)**: For a polynomial $P(x) = \\sum_{i=0}^{n} c_i x^i$, Horner's method is an efficient evaluation algorithm defined by the recurrence: $y_n = c_n$, $y_k = y_{k+1} \\cdot x + c_k$ for $k = n-1, \\dots, 0$.\n    The polynomials $(x-1)^n$, when expanded, have large binomial coefficients with alternating signs. For example, $(x-1)^8 = x^8 - 8x^7 + 28x^6 - 56x^5 + 70x^4 - 56x^3 + 28x^2 - 8x + 1$. When evaluating this near $x=1$, the intermediate terms in Horner's method become large, and their additions and subtractions lead to significant catastrophic cancellation. FMA performs each step $y \\cdot x + c_k$ with higher precision, preserving significant digits and yielding a much more accurate result. Test case $T_3$, at $x=2$, serves as a baseline where no such cancellation occurs.\n\n3.  **Dot Product Evaluation**: The dot product $\\sum_{i=1}^{n} v_i w_i$ is a sum of products. A naive implementation involves a loop that accumulates the sum.\n    -   _Without FMA_: `sum = sum + (v_i * w_i)`. This involves a rounding after the product `v_i * w_i` and another after the addition to `sum`.\n    -   _With FMA_: `sum = fma(v_i, w_i, sum)`. Here, the full high-precision product of `v_i` and `w_i` is calculated and then added to `sum` before a single final rounding.\n    Test cases $T_4, T_5, T_6$ are designed such that the exact sum is zero, but the individual terms are not. This is a classic scenario for catastrophic cancellation in summation, where FMA's ability to avoid intermediate rounding of products becomes crucial for accuracy.\n\n4.  **Handling of Edge Cases**: In computing the improvement factor $I = E_{\\text{noFMA}} / E_{\\text{FMA}}$, division by zero can occur if $E_{\\text{FMA}} = 0$.\n    -   If both $E_{\\text{noFMA}}$ and $E_{\\text{FMA}}$ are zero (as in $T_3$), the methods are equally accurate, and the improvement factor is logically $1$.\n    -   If $E_{\\text{FMA}}=0$ but $E_{\\text{noFMA}} > 0$ (as in $T_5$), the improvement is theoretically infinite. To provide a numerical output as required, we can use a proxy for the FMA error. A sensible choice is the smallest positive normalized float, `numpy.finfo(float).tiny`, representing the limit of floating-point resolution. This yields a very large, finite number for the improvement factor.\n    -   If $E_{\\text{noFMA}}=0$ but $E_{\\text{FMA}} > 0$ (as in $T_4, T_6$), this indicates a scenario where the standard method fortuitously produces the exact answer while the FMA method accumulates a small error. In this case, the improvement factor is correctly computed as $0$.\n\nThe implementation will use Python's `math` module for standard operations and `math.fma` for fused operations. High-precision reference values will be computed using the `decimal` module.", "answer": "```python\nimport math\nimport decimal\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes numerical errors with and without Fused Multiply-Add (FMA)\n    and calculates the improvement factor for a suite of test cases.\n    \"\"\"\n    # Set precision for high-accuracy reference calculations.\n    decimal.getcontext().prec = 100\n\n    # --- Helper Functions ---\n\n    def get_poly_coeffs(n):\n        \"\"\"Computes coefficients for the expanded form of (x-1)^n.\"\"\"\n        coeffs = []\n        # Horner's method requires coefficients from highest degree (c_n) down to c_0.\n        # The coefficient of x^k in (x-1)^n is C(n, n-k) * (-1)^(n-k).\n        # We order them for c_n, c_{n-1}, ..., c_0\n        for i in range(n, -1, -1):\n            k = n - i\n            coeff = math.comb(n, k) * ((-1)**k)\n            coeffs.append(float(coeff))\n        return coeffs\n\n    def horner_nofma(coeffs, x):\n        \"\"\"Evaluates a polynomial using Horner's scheme with standard operators.\"\"\"\n        y = 0.0\n        for c in coeffs:\n            y = y * x + c\n        return y\n\n    def horner_fma(coeffs, x):\n        \"\"\"Evaluates a polynomial using Horner's scheme with FMA.\"\"\"\n        y = 0.0\n        for c in coeffs:\n            y = math.fma(y, x, c)\n        return y\n\n    def dot_nofma(v, w):\n        \"\"\"Computes a dot product with standard operators.\"\"\"\n        s = 0.0\n        for i in range(len(v)):\n            s += v[i] * w[i]\n        return s\n\n    def dot_fma(v, w):\n        \"\"\"Computes a dot product using FMA for accumulation.\"\"\"\n        s = 0.0\n        for i in range(len(v)):\n            s = math.fma(v[i], w[i], s)\n        return s\n\n    def get_improvement(y_ref, y_nofma, y_fma):\n        \"\"\"\n        Calculates the improvement factor I = E_noFMA / E_FMA.\n        Handles edge cases like division by zero.\n        \"\"\"\n        y_ref_f = float(y_ref)\n\n        if y_ref_f == 0.0:\n            err_nofma = abs(y_nofma)\n            err_fma = abs(y_fma)\n        else:\n            err_nofma = abs(y_nofma - y_ref_f) / abs(y_ref_f)\n            err_fma = abs(y_fma - y_ref_f) / abs(y_ref_f)\n\n        if err_fma == err_nofma:\n            return 1.0\n        \n        if err_fma == 0.0:\n            # Improvement is theoretically infinite. Use a very large number\n            # by dividing by the smallest possible float value as a proxy for error.\n            if y_ref_f == 0.0:\n                err_fma_proxy = np.finfo(float).tiny\n            else:\n                err_fma_proxy = np.finfo(float).tiny / abs(y_ref_f)\n            \n            if err_fma_proxy == 0.0: # Avoid division by zero if y_ref is huge\n                return np.inf\n\n            return err_nofma / err_fma_proxy\n\n        return err_nofma / err_fma\n\n    # --- Test Case Definitions ---\n\n    test_cases = [\n        # T1: p1(x) = (x-1)^8 at x = 1 + 1e-8\n        {\n            'eval_func_nofma': lambda: horner_nofma(get_poly_coeffs(8), 1.0 + 1e-8),\n            'eval_func_fma': lambda: horner_fma(get_poly_coeffs(8), 1.0 + 1e-8),\n            'ref_val': (decimal.Decimal('1') + decimal.Decimal('1e-8') - decimal.Decimal('1'))**8\n        },\n        # T2: p2(x) = (x-1)^12 at x = 1 + 1e-10\n        {\n            'eval_func_nofma': lambda: horner_nofma(get_poly_coeffs(12), 1.0 + 1e-10),\n            'eval_func_fma': lambda: horner_fma(get_poly_coeffs(12), 1.0 + 1e-10),\n            'ref_val': (decimal.Decimal('1') + decimal.Decimal('1e-10') - decimal.Decimal('1'))**12\n        },\n        # T3: p3(x) = (x-1)^8 at x = 2\n        {\n            'eval_func_nofma': lambda: horner_nofma(get_poly_coeffs(8), 2.0),\n            'eval_func_fma': lambda: horner_fma(get_poly_coeffs(8), 2.0),\n            'ref_val': (decimal.Decimal('2') - decimal.Decimal('1'))**8\n        },\n        # T4: Dot product vA . wA\n        {\n            'eval_func_nofma': lambda: dot_nofma([1e8] * 8, [1e-8, -1e-8] * 4),\n            'eval_func_fma': lambda: dot_fma([1e8] * 8, [1e-8, -1e-8] * 4),\n            'ref_val': decimal.Decimal('0')\n        },\n        # T5: Dot product vB . wB\n        {\n            'v': [1e16, 1.0, -1e16, 1.0, 1e16, -1.0],\n            'w': [1e-16, 1.0, 1e-16, 1.0, -1e-16, 1.0],\n            'eval_func_nofma': lambda: dot_nofma([1e16, 1.0, -1e16, 1.0, 1e16, -1.0], [1e-16, 1.0, 1e-16, 1.0, -1e-16, 1.0]),\n            'eval_func_fma': lambda: dot_fma([1e16, 1.0, -1e16, 1.0, 1e16, -1.0], [1e-16, 1.0, 1e-16, 1.0, -1e-16, 1.0]),\n            'ref_val': decimal.Decimal('0')\n        },\n        # T6: Dot product vC . wC\n        {\n            'v': [(-1.0)**i * 1e8 for i in range(1, 101)],\n            'w': [1e-8] * 100,\n            'eval_func_nofma': lambda: dot_nofma([(-1.0)**i * 1e8 for i in range(1, 101)], [1e-8] * 100),\n            'eval_func_fma': lambda: dot_fma([(-1.0)**i * 1e8 for i in range(1, 101)], [1e-8] * 100),\n            'ref_val': decimal.Decimal('0')\n        }\n    ]\n\n    # --- Main Execution Loop ---\n\n    results = []\n    for case in test_cases:\n        y_ref = case['ref_val']\n        y_nofma = case['eval_func_nofma']()\n        y_fma = case['eval_func_fma']()\n        \n        improvement = get_improvement(y_ref, y_nofma, y_fma)\n        results.append(improvement)\n\n    # Final print statement in the exact required format.\n    # The spec asks for 12 significant digits. Using .12g is robust.\n    result_str = ','.join(format(res, '.12g') for res in results)\n    print(f\"[{result_str}]\")\n\nsolve()\n```", "id": "3165813"}]}