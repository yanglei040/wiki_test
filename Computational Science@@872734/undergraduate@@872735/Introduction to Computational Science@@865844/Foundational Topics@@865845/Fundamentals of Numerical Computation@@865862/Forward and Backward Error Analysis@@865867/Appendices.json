{"hands_on_practices": [{"introduction": "This first practice provides a foundational deep dive into the mechanics of error propagation. By analytically examining the function $f(x) = \\sqrt{x+1} - \\sqrt{x}$ for large $x$ [@problem_id:3231896], a classic example of catastrophic cancellation, you will derive expressions for both forward and backward error from first principles. This exercise builds the essential mathematical skills needed to trace how small, inevitable rounding errors can be amplified into significant inaccuracies in a final result.", "problem": "Let $f(x) = \\sqrt{x+1} - \\sqrt{x}$ and suppose it is evaluated on a machine conforming to the Institute of Electrical and Electronics Engineers (IEEE) Standard for Floating-Point Arithmetic (IEEE 754) in round-to-nearest mode with unit roundoff $u$. Assume the standard first-order rounding model for each elementary operation and square root: for any operation $\\circ \\in \\{+, -, \\times, \\div\\}$ and any $a,b$ in the floating-point range, $\\operatorname{fl}(a \\circ b) = (a \\circ b)(1 + \\delta)$ with $|\\delta| \\le u$, and $\\operatorname{fl}(\\sqrt{y}) = \\sqrt{y}(1 + \\delta)$ with $|\\delta| \\le u$. Consider the naive computation of $f(x)$ by evaluating $s = \\sqrt{x}$, $t = \\sqrt{x+1}$, and then forming the floating-point subtraction $\\widehat{f} = \\operatorname{fl}(t - s)$.\n\nUsing only the elementary identity $(a - b)(a + b) = a^{2} - b^{2}$ and the above rounding model as the foundational base, analyze the catastrophic cancellation in this naive evaluation for large $x$. Derive, to first order in $u$, a leading-order worst-case asymptotic expression as $x \\to \\infty$ for:\n1) the relative forward error $\\varepsilon_{f} = (\\widehat{f} - f)/f$, and\n2) a backward error interpretation in which there exists a perturbation $\\Delta$ such that $\\widehat{f} = f(x + \\Delta)$ to first order; determine the leading-order worst-case asymptotic magnitude of the smallest such $|\\Delta|$ as $x \\to \\infty$.\n\nReport your final result as a two-entry row matrix $\\begin{pmatrix}E & B\\end{pmatrix}$, where $E$ is the leading-order worst-case asymptotic expression for the relative forward error and $B$ is the leading-order worst-case asymptotic expression for the absolute backward error $|\\Delta|$, both expressed in terms of $x$ and $u$. The final answer must be a single analytic expression; do not include units. No rounding is required.", "solution": "The problem requires an analysis of the forward and backward error for the naive floating-point evaluation of the function $f(x) = \\sqrt{x+1} - \\sqrt{x}$ for large values of $x$.\n\nThe problem is first validated.\n\n**Step 1: Extract Givens**\n- Function: $f(x) = \\sqrt{x+1} - \\sqrt{x}$.\n- Floating-point model: For an operation $\\circ \\in \\{+, -, \\times, \\div\\}$ and the square root function, the floating-point result $\\operatorname{fl}(\\cdot)$ of an exact operation is related to the true result by a relative error bounded by the unit roundoff $u$. Specifically, $\\operatorname{fl}(a \\circ b) = (a \\circ b)(1 + \\delta)$ and $\\operatorname{fl}(\\sqrt{y}) = \\sqrt{y}(1 + \\delta)$, where $|\\delta| \\le u$.\n- Algorithm: The computation proceeds as $s = \\sqrt{x}$, $t = \\sqrt{x+1}$, and $\\widehat{f} = \\operatorname{fl}(t - s)$.\n- Analysis: First order in $u$, asymptotic for $x \\to \\infty$.\n- Required outputs:\n    1. Leading-order worst-case asymptotic expression for the relative forward error $\\varepsilon_{f} = (\\widehat{f} - f)/f$.\n    2. Leading-order worst-case asymptotic magnitude of the smallest absolute backward error $|\\Delta|$ such that $\\widehat{f} = f(x + \\Delta)$ to first order.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded in the field of numerical analysis, specifically floating-point error analysis. It is a standard, well-posed problem with a clear objective and all necessary information provided. The problem is free of factual unsoundness, ambiguity, or subjective claims. It is a formalizable problem directly related to the topic of forward and backward error analysis.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A full solution will be provided.\n\nLet the computed values be denoted with a hat. The computational steps are modeled as follows, assuming $x$ is an exact floating-point number and the addition $x+1$ is also exact.\nThe first computed value is $\\widehat{s}$:\n$$ \\widehat{s} = \\operatorname{fl}(\\sqrt{x}) = \\sqrt{x}(1 + \\delta_1), \\quad |\\delta_1| \\le u $$\nThe second computed value is $\\widehat{t}$:\n$$ \\widehat{t} = \\operatorname{fl}(\\sqrt{x+1}) = \\sqrt{x+1}(1 + \\delta_2), \\quad |\\delta_2| \\le u $$\nThe final result $\\widehat{f}$ is the floating-point subtraction of $\\widehat{s}$ from $\\widehat{t}$:\n$$ \\widehat{f} = \\operatorname{fl}(\\widehat{t} - \\widehat{s}) = (\\widehat{t} - \\widehat{s})(1 + \\delta_3), \\quad |\\delta_3| \\le u $$\n\nSubstituting the expressions for $\\widehat{s}$ and $\\widehat{t}$ into the equation for $\\widehat{f}$:\n$$ \\widehat{f} = (\\sqrt{x+1}(1 + \\delta_2) - \\sqrt{x}(1 + \\delta_1))(1 + \\delta_3) $$\nExpanding this expression and retaining only terms up to the first order in the error terms $\\delta_i$ (i.e., neglecting terms like $\\delta_i \\delta_j$ which are of order $u^2$):\n$$ \\widehat{f} \\approx (\\sqrt{x+1} - \\sqrt{x}) + \\delta_2\\sqrt{x+1} - \\delta_1\\sqrt{x} + \\delta_3(\\sqrt{x+1} - \\sqrt{x}) $$\nRecognizing that $f(x) = \\sqrt{x+1} - \\sqrt{x}$, the absolute error is:\n$$ \\widehat{f} - f(x) \\approx \\delta_2\\sqrt{x+1} - \\delta_1\\sqrt{x} + \\delta_3 f(x) $$\n\n**1. Relative Forward Error Analysis**\n\nThe relative forward error is $\\varepsilon_f = (\\widehat{f} - f(x))/f(x)$. Using the absolute error expression:\n$$ \\varepsilon_f \\approx \\frac{\\delta_2\\sqrt{x+1} - \\delta_1\\sqrt{x}}{f(x)} + \\delta_3 $$\nThis expression reveals catastrophic cancellation. The term $f(x)$ is the difference of two nearly equal large numbers for large $x$, so its value is small, amplifying the error from the numerator. To analyze this, we use the identity suggested by the problem, $(a-b)(a+b)=a^2-b^2$, which allows us to rewrite $f(x)$:\n$$ f(x) = \\sqrt{x+1} - \\sqrt{x} = \\frac{(\\sqrt{x+1} - \\sqrt{x})(\\sqrt{x+1} + \\sqrt{x})}{\\sqrt{x+1} + \\sqrt{x}} = \\frac{(x+1) - x}{\\sqrt{x+1} + \\sqrt{x}} = \\frac{1}{\\sqrt{x+1} + \\sqrt{x}} $$\nSubstituting this into the expression for $\\varepsilon_f$:\n$$ \\varepsilon_f \\approx (\\delta_2\\sqrt{x+1} - \\delta_1\\sqrt{x})(\\sqrt{x+1} + \\sqrt{x}) + \\delta_3 $$\nExpanding the product:\n$$ \\varepsilon_f \\approx \\delta_2(x+1) + \\delta_2\\sqrt{x(x+1)} - \\delta_1\\sqrt{x(x+1)} - \\delta_1 x + \\delta_3 $$\n$$ \\varepsilon_f \\approx x(\\delta_2 - \\delta_1) + \\delta_2 + (\\delta_2 - \\delta_1)\\sqrt{x^2+x} + \\delta_3 $$\nFor large $x$, we use the asymptotic expansion $\\sqrt{x^2+x} = x\\sqrt{1+1/x} \\approx x(1 + \\frac{1}{2x}) = x + \\frac{1}{2}$.\n$$ \\varepsilon_f \\approx x(\\delta_2 - \\delta_1) + \\delta_2 + (\\delta_2 - \\delta_1)(x + \\frac{1}{2}) + \\delta_3 $$\n$$ \\varepsilon_f \\approx x\\delta_2 - x\\delta_1 + \\delta_2 + x\\delta_2 - x\\delta_1 + \\frac{1}{2}\\delta_2 - \\frac{1}{2}\\delta_1 + \\delta_3 $$\n$$ \\varepsilon_f \\approx 2x(\\delta_2 - \\delta_1) + \\frac{3}{2}\\delta_2 - \\frac{1}{2}\\delta_1 + \\delta_3 $$\nAs $x \\to \\infty$, the dominant term is $2x(\\delta_2 - \\delta_1)$. To find the worst-case magnitude, we must maximize $|\\delta_2 - \\delta_1|$. Since $|\\delta_1| \\le u$ and $|\\delta_2| \\le u$, the maximum value of $|\\delta_2 - \\delta_1|$ is $2u$, which occurs when $\\delta_1$ and $\\delta_2$ have opposite signs and maximum magnitude (e.g., $\\delta_2=u$ and $\\delta_1=-u$).\nThe leading-order worst-case asymptotic expression for the relative forward error is thus:\n$$ |\\varepsilon_f| \\approx |2x(2u)| = 4xu $$\nThe first requested expression is $E = 4xu$.\n\n**2. Absolute Backward Error Analysis**\n\nBackward error analysis seeks to find a perturbation $\\Delta$ on the input $x$ such that the computed result $\\widehat{f}$ is the exact result for the perturbed input, i.e., $\\widehat{f} = f(x+\\Delta)$.\nUsing a first-order Taylor expansion of $f(x+\\Delta)$ around $x$:\n$$ f(x+\\Delta) \\approx f(x) + \\Delta f'(x) $$\nEquating this to the computed value gives $\\widehat{f} \\approx f(x) + \\Delta f'(x)$.\nThis implies that the absolute error is $\\widehat{f} - f(x) \\approx \\Delta f'(x)$.\nWe can solve for $\\Delta$:\n$$ \\Delta \\approx \\frac{\\widehat{f} - f(x)}{f'(x)} $$\nWe already have the expression for the absolute error: $\\widehat{f} - f(x) \\approx \\delta_2\\sqrt{x+1} - \\delta_1\\sqrt{x} + \\delta_3 f(x)$.\nNext, we find the derivative of $f(x)$:\n$$ f'(x) = \\frac{d}{dx}(\\sqrt{x+1} - \\sqrt{x}) = \\frac{1}{2\\sqrt{x+1}} - \\frac{1}{2\\sqrt{x}} = \\frac{\\sqrt{x} - \\sqrt{x+1}}{2\\sqrt{x(x+1)}} $$\nNow we find the asymptotic behavior for $x \\to \\infty$ for both the numerator and the denominator of the expression for $\\Delta$.\n\nFor the numerator, the absolute error $\\widehat{f} - f(x)$: As $x \\to \\infty$, $\\sqrt{x+1}\\approx\\sqrt{x}$ and $f(x) \\to 0$. The dominant term in the absolute error is $(\\delta_2-\\delta_1)\\sqrt{x}$.\n$$ \\widehat{f} - f(x) \\approx (\\delta_2\\sqrt{x}(1+\\frac{1}{2x}...) - \\delta_1\\sqrt{x}) + O(u/{\\sqrt{x}}) \\approx (\\delta_2-\\delta_1)\\sqrt{x} $$\n\nFor the denominator, $f'(x)$: As $x \\to \\infty$, the numerator $\\sqrt{x} - \\sqrt{x+1} = -f(x) \\approx -\\frac{1}{2\\sqrt{x}}$. The denominator $2\\sqrt{x(x+1)} \\approx 2x$.\n$$ f'(x) \\approx \\frac{-1/(2\\sqrt{x})}{2x} = -\\frac{1}{4x^{3/2}} $$\nCombining these to find the asymptotic expression for $\\Delta$:\n$$ \\Delta \\approx \\frac{(\\delta_2-\\delta_1)\\sqrt{x}}{-1/(4x^{3/2})} = -4x^2(\\delta_2-\\delta_1) $$\nTo find the worst-case magnitude of $|\\Delta|$, we again maximize $|\\delta_2 - \\delta_1|$, which is $2u$.\nThe leading-order worst-case asymptotic magnitude of the backward error is:\n$$ |\\Delta| \\approx |-4x^2(2u)| = 8x^2 u $$\nThe second requested expression is $B = 8x^2u$.\n\nThe final result is the two-entry row matrix containing the expression for the worst-case relative forward error and the worst-case absolute backward error magnitude.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n4xu & 8x^2u\n\\end{pmatrix}\n}\n$$", "id": "3231896"}, {"introduction": "Building on the concept of cancellation, this practice isolates the subtraction operation itself to explore the critical role of a problem's intrinsic sensitivity, or its *condition number* [@problem_id:3132110]. You will first derive the condition number for subtraction from its formal definition and then use a computational model to verify the fundamental relationship that links forward error, backward error, and conditioning. This reveals why even numerically \"stable\" computations can yield inaccurate results when applied to an ill-conditioned problem.", "problem": "You are given the scalar function $f:\\mathbb{R}^2\\to\\mathbb{R}$ defined by $f(\\mathbf{x})=x_1-x_2$, where $\\mathbf{x}=(x_1,x_2)$. Consider a simple floating-point model that first rounds each input to a fixed number of significant decimal digits and then performs subtraction exactly. Specifically, for a given positive integer $t$, the computed value is\n$$\\widehat{f}=\\operatorname{fl}_t(x_1)-\\operatorname{fl}_t(x_2),$$\nwhere $\\operatorname{fl}_t(\\cdot)$ denotes rounding to $t$ significant decimal digits using round-to-nearest with ties-to-even.\n\nYour tasks are:\n1. Starting only from core definitions of forward error, backward error, and conditioning, reason about how the subtraction $f(\\mathbf{x})=x_1-x_2$ behaves when $x_1\\approx x_2$. You must:\n   - Use the definition of relative forward error for an output $y=f(\\mathbf{x})$ and a computed output $\\widehat{y}$:\n     $$\\text{relative forward error}=\\frac{|\\widehat{y}-y|}{|y|},$$\n     provided $y\\neq 0$.\n   - Use a backward-error point of view under the rounding-first model: interpret $\\widehat{f}$ as the exact result $f(\\tilde{\\mathbf{x}})$ at some perturbed input $\\tilde{\\mathbf{x}}=(\\tilde{x}_1,\\tilde{x}_2)$, and take as the relative backward error in $\\mathbf{x}$ the quantity\n     $$\\eta=\\max\\!\\left(\\frac{|\\tilde{x}_1-x_1|}{|x_1|},\\frac{|\\tilde{x}_2-x_2|}{|x_2|}\\right),$$\n     where, in this model, a natural choice is $\\tilde{x}_i=\\operatorname{fl}_t(x_i)$.\n   - Starting from the foundational definition of a relative condition number at $\\mathbf{x}$,\n     $$\\kappa(\\mathbf{x})=\\lim_{\\rho\\to 0^+}\\ \\sup_{\\substack{\\Delta \\mathbf{x}\\neq \\mathbf{0}\\\\ \\max(|\\Delta x_1|/|x_1|,\\ |\\Delta x_2|/|x_2|)\\le \\rho}}\\ \\frac{\\left|\\ f(\\mathbf{x}+\\Delta \\mathbf{x})-f(\\mathbf{x})\\ \\right|/|f(\\mathbf{x})|}{\\max(|\\Delta x_1|/|x_1|,\\ |\\Delta x_2|/|x_2|)},$$\n     derive an explicit formula for $\\kappa(\\mathbf{x})$ specialized to $f(\\mathbf{x})=x_1-x_2$, and explain why it becomes large when $x_1\\approx x_2$.\n\n2. Implement a program that, for each specified test case, computes:\n   - The exact value $f(\\mathbf{x})=x_1-x_2$.\n   - The computed value $\\widehat{f}=\\operatorname{fl}_t(x_1)-\\operatorname{fl}_t(x_2)$ with $\\operatorname{fl}_t(\\cdot)$ as defined above.\n   - The relative forward error $\\varepsilon=\\frac{|\\widehat{f}-f(\\mathbf{x})|}{|f(\\mathbf{x})|}$ (all answers must be reported as decimals, not percentages).\n   - The relative backward error in $\\mathbf{x}$ under the rounding-first model:\n     $$\\eta=\\max\\!\\left(\\frac{|\\operatorname{fl}_t(x_1)-x_1|}{|x_1|},\\frac{|\\operatorname{fl}_t(x_2)-x_2|}{|x_2|}\\right).$$\n   - The relative condition number $\\kappa(\\mathbf{x})$ obtained in Task 1 from first principles (implement your derived expression).\n   - The amplification factor $A=\\varepsilon/\\eta$, which quantifies how much the tiny relative input error (backward error) is amplified in the output (forward error).\n\n3. Use the following test suite of parameter values. Each test case is a triple $(x_1,x_2,t)$, where $t$ is the number of significant decimal digits used in rounding. None of these cases yields $f(\\mathbf{x})=0$, so the relative forward error is well-defined.\n   - Case 1 (well-conditioned): $(x_1,x_2,t)=\\left(12345.6789,\\ 2345.678901,\\ 7\\right)$.\n   - Case 2 (strong cancellation at small scale): $(x_1,x_2,t)=\\left(1.0000005,\\ 1.0000004,\\ 7\\right)$.\n   - Case 3 (strong cancellation at large scale): $(x_1,x_2,t)=\\left(100000003.0,\\ 100000000.0,\\ 7\\right)$.\n   - Case 4 (moderate cancellation): $(x_1,x_2,t)=\\left(0.123456789,\\ 0.123446789,\\ 7\\right)$.\n\n4. Final output specification:\n   - For each test case, output the list $[A, \\varepsilon, \\eta, \\kappa(\\mathbf{x})]$ in that exact order.\n   - Aggregate all cases into a single line as a comma-separated list of these lists, enclosed in square brackets, for example:\n     $$[[A_1, \\varepsilon_1, \\eta_1, \\kappa_1], [A_2, \\varepsilon_2, \\eta_2, \\kappa_2], \\ldots].$$\n   - Print all floating-point numbers in scientific notation with exactly $10$ significant digits (for example, $1.234567890 \\times 10^{-3}$ must be printed as `1.2345678900e-03`).\n\nScientific realism and constraints:\n- Treat all quantities as dimensionless real numbers.\n- Angles are not involved.\n- All error quantities must be decimals or fractions, not percentages.\n- Use the rounding-to-nearest with ties-to-even rule for $\\operatorname{fl}_t(\\cdot)$ in base ten.\n- Your program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets, as specified above.", "solution": "The problem posed is a valid and classic exercise in introductory computational science, specifically concerning the forward and backward error analysis of a fundamental arithmetic operation. It is scientifically grounded, well-posed, objective, and complete. The task is to analyze the numerical stability of the subtraction $f(\\mathbf{x})=x_1-x_2$, particularly in the case of subtractive cancellation, where $x_1 \\approx x_2$.\n\nWe will first derive the necessary theoretical results from the provided definitions and then implement a program to compute the specified quantities for the given test cases.\n\n### Part 1: Theoretical Analysis from First Principles\n\n**1. Derivation of the Relative Condition Number $\\kappa(\\mathbf{x})$**\n\nThe problem provides the formal definition of the relative condition number of a function $f:\\mathbb{R}^2\\to\\mathbb{R}$ at a point $\\mathbf{x}=(x_1, x_2)$ with respect to relative perturbations in the input:\n$$\n\\kappa(\\mathbf{x})=\\lim_{\\rho\\to 0^+}\\ \\sup_{\\substack{\\Delta \\mathbf{x}\\neq \\mathbf{0}\\\\ \\max(|\\Delta x_1|/|x_1|,\\ |\\Delta x_2|/|x_2|)\\le \\rho}}\\ \\frac{\\left|\\ f(\\mathbf{x}+\\Delta \\mathbf{x})-f(\\mathbf{x})\\ \\right|/|f(\\mathbf{x})|}{\\max(|\\Delta x_1|/|x_1|,\\ |\\Delta x_2|/|x_2|)}\n$$\nLet's specialize this definition for the function $f(\\mathbf{x}) = x_1 - x_2$. We assume $f(\\mathbf{x}) = x_1 - x_2 \\neq 0$, and $x_1, x_2 \\neq 0$.\n\nThe change in the function's output due to a perturbation $\\Delta\\mathbf{x}=(\\Delta x_1, \\Delta x_2)$ is:\n$$\nf(\\mathbf{x}+\\Delta \\mathbf{x}) - f(\\mathbf{x}) = ((x_1+\\Delta x_1) - (x_2+\\Delta x_2)) - (x_1 - x_2) = \\Delta x_1 - \\Delta x_2\n$$\nThe relative change in the output is therefore:\n$$\n\\frac{|f(\\mathbf{x}+\\Delta \\mathbf{x}) - f(\\mathbf{x})|}{|f(\\mathbf{x})|} = \\frac{|\\Delta x_1 - \\Delta x_2|}{|x_1 - x_2|}\n$$\nLet the maximum relative input perturbation be denoted by $E_{\\text{rel}} = \\max\\left(\\frac{|\\Delta x_1|}{|x_1|}, \\frac{|\\Delta x_2|}{|x_2|}\\right)$. This implies $|\\Delta x_1| \\le E_{\\text{rel}} |x_1|$ and $|\\Delta x_2| \\le E_{\\text{rel}} |x_2|$.\n\nWe can now express the ratio inside the supremum:\n$$\n\\frac{\\left|\\ f(\\mathbf{x}+\\Delta \\mathbf{x})-f(\\mathbf{x})\\ \\right|/|f(\\mathbf{x})|}{E_{\\text{rel}}} = \\frac{|\\Delta x_1 - \\Delta x_2|/|x_1 - x_2|}{E_{\\text{rel}}}\n$$\nTo find the supremum, we must maximize the term $|\\Delta x_1 - \\Delta x_2|$ for a given $E_{\\text{rel}}=\\rho$. Using the triangle inequality:\n$$\n|\\Delta x_1 - \\Delta x_2| \\le |\\Delta x_1| + |\\Delta x_2| \\le \\rho |x_1| + \\rho |x_2| = \\rho (|x_1| + |x_2|)\n$$\nThis upper bound is attainable by choosing perturbations that are maximal in magnitude and have opposite effects. For instance, if we choose $\\Delta x_1 = \\rho x_1$ and $\\Delta x_2 = -\\rho x_2$, then $E_{\\text{rel}} = \\max(|\\rho x_1|/|x_1|, |-\\rho x_2|/|x_2|) = \\rho$. Wait, this choice yields $|\\Delta x_1-\\Delta x_2| = |\\rho(x_1+x_2)| = \\rho|x_1+x_2|$. This is not quite the bound $\\rho(|x_1|+|x_2|)$.\nThe standard derivation, however, which correctly finds the supremum, establishes that $\\sup |\\Delta x_1 - \\Delta x_2| = \\rho(|x_1| + |x_2|)$. This can be seen by setting $\\Delta x_1 = \\rho \\cdot \\text{sgn}(\\Delta x_1) \\cdot |x_1|$ and $\\Delta x_2 = \\rho \\cdot \\text{sgn}(\\Delta x_2) \\cdot |x_2|$, and setting the signs to maximize the difference. For example, selecting $\\Delta x_1 = \\rho|x_1|$ and $\\Delta x_2 = -\\rho|x_2|$ is not a valid perturbation choice as that would imply $\\Delta x_1/x_1$ may not have magnitude $\\rho$.\nLet us consider perturbations $\\Delta x_1 = \\rho \\cdot \\alpha_1$ and $\\Delta x_2 = \\rho \\cdot \\alpha_2$ with $|\\alpha_1/x_1| \\le 1$ and $|\\alpha_2/x_2| \\le 1$. We maximize $|\\rho \\alpha_1 - \\rho \\alpha_2|$.\nThe correct bound is indeed $\\rho(|x_1|+|x_2|)$. This maximum is achieved when we select $\\Delta x_1$ and $\\Delta x_2$ to have opposite signs and maximum allowed magnitude, e.g. $\\Delta x_1 = \\rho x_1$ and $\\Delta x_2 = -\\rho x_2$ is not general enough. We should choose $\\Delta x_1 = \\rho|x_1|$ and $\\Delta x_2 = -\\rho|x_2|$ or vice versa, but this may violate the constraint on relative error.\nLet's choose $\\Delta x_1$ such that $\\Delta x_1/x_1 = \\rho$ (so $\\Delta x_1 = \\rho x_1$) and $\\Delta x_2$ such that $\\Delta x_2/x_2 = -\\rho$ (so $\\Delta x_2 = -\\rho x_2$). With these choices, $E_{rel}=\\rho$. Then $|\\Delta x_1 - \\Delta x_2| = |\\rho x_1 - (-\\rho x_2)| = \\rho|x_1+x_2|$.\nThe correct supremum of $|\\Delta x_1 - \\Delta x_2|$ subject to the constraints is $\\rho(|x_1|+|x_2|)$.\n\nSubstituting this supremum into the expression for the condition number:\n$$\n\\kappa(\\mathbf{x}) = \\lim_{\\rho\\to 0^+} \\frac{\\sup (|\\Delta x_1 - \\Delta x_2|)/|x_1 - x_2|}{\\rho} = \\lim_{\\rho\\to 0^+} \\frac{\\rho(|x_1| + |x_2|)/|x_1 - x_2|}{\\rho}\n$$\nThe linear dependence on $\\rho$ cancels, and the limit is trivial. Thus, the derived formula is:\n$$\n\\kappa(\\mathbf{x}) = \\frac{|x_1| + |x_2|}{|x_1 - x_2|}\n$$\n\n**2. Behavior of $\\kappa(\\mathbf{x})$ for $x_1 \\approx x_2$**\n\nWhen $x_1$ and $x_2$ are close in value ($x_1 \\approx x_2$), the denominator $|x_1 - x_2|$ approaches $0$. Assuming $x_1$ and $x_2$ are not close to $0$ and have the same sign (the typical scenario for catastrophic cancellation), the numerator $|x_1| + |x_2|$ is approximately $2|x_1|$. The ratio $\\kappa(\\mathbf{x})$ thus becomes very large. A large condition number signifies that the problem is \"ill-conditioned,\" meaning that small relative errors in the input can be magnified into large relative errors in the output.\n\n**3. Relationship between Forward Error, Backward Error, and Conditioning**\n\nThe problem defines a \"rounding-first\" computational model where $\\widehat{f} = \\operatorname{fl}_t(x_1) - \\operatorname{fl}_t(x_2)$. This can be viewed from a backward error perspective by defining a perturbed input $\\tilde{\\mathbf{x}} = (\\tilde{x}_1, \\tilde{x}_2) = (\\operatorname{fl}_t(x_1), \\operatorname{fl}_t(x_2))$. The computed result is then the exact result for this perturbed input: $\\widehat{f} = f(\\tilde{\\mathbf{x}})$.\n\nThe relative backward error $\\eta$ is the size of the relative perturbation to the input:\n$$\n\\eta = \\max\\left(\\frac{|\\tilde{x}_1-x_1|}{|x_1|},\\frac{|\\tilde{x}_2-x_2|}{|x_2|}\\right)\n$$\nThe relative forward error $\\varepsilon$ is the resulting relative perturbation in the output:\n$$\n\\varepsilon = \\frac{|\\widehat{f}-f(\\mathbf{x})|}{|f(\\mathbf{x})|} = \\frac{|f(\\tilde{\\mathbf{x}})-f(\\mathbf{x})|}{|f(\\mathbf{x})|}\n$$\nThe condition number $\\kappa(\\mathbf{x})$ links these two quantities. For small perturbations, the relationship is approximately:\n$$\n\\varepsilon \\approx \\kappa(\\mathbf{x}) \\cdot \\eta \\quad \\text{or more formally} \\quad \\varepsilon \\le \\kappa(\\mathbf{x}) \\cdot \\eta + O(\\eta^2)\n$$\nThe amplification factor $A = \\varepsilon / \\eta$ is an empirical quantification of this relationship for the specific perturbation introduced by rounding. Since the condition number is defined as the supremum over all possible small perturbations, we have the inequality $A \\le \\kappa(\\mathbf{x})$.\n\nWhen $x_1 \\approx x_2$, $\\kappa(\\mathbf{x})$ is large. The rounding process introduces a small backward error $\\eta$, typically on the order of machine precision. However, this small input error is amplified by the large condition number, leading to a potentially large forward error $\\varepsilon$. This phenomenon is known as **catastrophic cancellation**: the subtraction of nearly equal quantities causes a loss of significant digits in the result, not because subtraction itself is inaccurate, but because it magnifies the pre-existing rounding errors in the inputs.\n\n**A Note on Output Formatting**\nThe problem statement requests \"exactly 10 significant digits\" but provides an example formatting of the number $1.234567890 \\times 10^{-3}$ as `1.2345678900e-03`. The example output has 11 significant digits (1 before the decimal, 10 after). This presents a minor ambiguity. We will adhere to the explicit example, as it is more specific than the general description. This corresponds to the Python format specifier `\"{:.10e}\"`.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef fl_t(x: float, t: int) -> float:\n    \"\"\"\n    Rounds a floating-point number x to t significant decimal digits.\n    Uses round-to-nearest, ties-to-even rule.\n    \"\"\"\n    if x == 0.0:\n        return 0.0\n\n    # Using np.float64 for calculations to maintain precision.\n    x = np.float64(x)\n\n    # Calculate the base-10 exponent of the number to find its magnitude.\n    exponent = np.floor(np.log10(np.abs(x)))\n    \n    # Calculate the power of 10 to scale the number.\n    # We want to move the decimal point so that there are t digits before it.\n    # Incorrect scaling power in thinking: Correct power to get `t` sig figs for rounding\n    # to integer is to scale by power of 10 that puts the t-th digit in the units place.\n    # e.g., for 123.45 and t=4, scale so it becomes 1234.5.\n    # Exponent is 2. Scale factor is 10^(4-(2+1)) = 10^1. 123.45 * 10 = 1234.5.\n    # scale_power = t - (exponent + 1)\n    \n    # A more direct way: to round to t sig-figs, we round x / 10^(e-t+1) to integer.\n    scale_power = exponent - t + 1\n    \n    # Scale the number. For example, to round 12345.67 to 3 sig figs,\n    # exponent=4, t=3. scale_power = 4-3+1 = 2.\n    # scaled_val = 12345.67 / 100 = 123.4567\n    scaled_val = x / (10.0**scale_power)\n    \n    # Python 3's round() and np.round() both implement round-half-to-even.\n    rounded_scaled_val = np.round(scaled_val)\n    \n    # Scale back to the original magnitude.\n    # result = 123.0 * 100 = 12300.0\n    result = rounded_scaled_val * (10.0**scale_power)\n    \n    return float(result)\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (well-conditioned)\n        (12345.6789, 2345.678901, 7),\n        # Case 2 (strong cancellation at small scale)\n        (1.0000005, 1.0000004, 7),\n        # Case 3 (strong cancellation at large scale)\n        (100000003.0, 100000000.0, 7),\n        # Case 4 (moderate cancellation)\n        (0.123456789, 0.123446789, 7)\n    ]\n\n    all_results = []\n\n    for x1_in, x2_in, t in test_cases:\n        # Use high-precision numpy floats for internal calculations\n        x1 = np.float64(x1_in)\n        x2 = np.float64(x2_in)\n\n        # 1. Exact value\n        f_exact = x1 - x2\n\n        # 2. Rounded inputs and computed value\n        x1_rounded = fl_t(x1, t)\n        x2_rounded = fl_t(x2, t)\n        f_computed = x1_rounded - x2_rounded\n\n        # 3. Relative forward error\n        # Problem statement guarantees f_exact is non-zero\n        rel_forward_error = np.abs(f_computed - f_exact) / np.abs(f_exact)\n\n        # 4. Relative backward error\n        # Problem statement inputs are non-zero.\n        rel_err_x1 = np.abs(x1_rounded - x1) / np.abs(x1)\n        rel_err_x2 = np.abs(x2_rounded - x2) / np.abs(x2)\n        rel_backward_error = np.max([rel_err_x1, rel_err_x2])\n\n        # 5. Relative condition number\n        condition_number = (np.abs(x1) + np.abs(x2)) / np.abs(f_exact)\n\n        # 6. Amplification factor\n        # If backward error is zero, forward error must also be zero.\n        # This implies exact computation, so amplification is not straightforward.\n        # For the given test cases, rel_backward_error will not be zero.\n        if rel_backward_error == 0.0:\n            # This case means the inputs didn't need rounding, so perturbation is 0.\n            # Forward error is also 0. The ratio 0/0 is indeterminate.\n            # A reasonable interpretation is that the amplification is bounded by kappa.\n            # We can also consider it 1.0, as there's no error to amplify.\n            # Let's assign it kappa, as it's the theoretical limit.\n            # However, for the given tests this branch is not taken.\n            amplification_factor = condition_number\n        else:\n            amplification_factor = rel_forward_error / rel_backward_error\n\n        # Store results for this case\n        all_results.append([amplification_factor, rel_forward_error, rel_backward_error, condition_number])\n\n    # Format the output as specified in the problem statement.\n    # The example \"1.2345678900e-03\" has 11 significant digits (1 before decimal, 10 after).\n    # This corresponds to the format specifier \"{:.10e}\".\n    def format_list(data_list):\n        return [f\"{val:.10e}\" for val in data_list]\n\n    formatted_cases = []\n    for result_case in all_results:\n        formatted_list_str = \", \".join(format_list(result_case))\n        formatted_cases.append(f\"[{formatted_list_str}]\")\n        \n    final_output_str = f\"[{', '.join(formatted_cases)}]\"\n\n    print(final_output_str)\n\nsolve()\n```", "id": "3132110"}, {"introduction": "This final practice elevates our analysis from single operations to entire algorithms, using the Gram-Schmidt orthonormalization process as a compelling case study [@problem_id:3232097]. By implementing and comparing the classical and modified versions of the algorithm, you will witness firsthand how a seemingly minor algebraic rearrangement can dramatically improve numerical stability. This exercise also introduces a powerful interpretation of backward error as the failure of a computed object to satisfy a core mathematical property—in this case, the orthogonality of the resulting vectors.", "problem": "You are given the task of comparing forward error and backward error for two widely used orthonormalization procedures, classical Gram–Schmidt and modified Gram–Schmidt, implemented in standard floating-point arithmetic. Use the definitions of forward error and backward error grounded in the model of floating-point arithmetic: for any basic operation applied to real numbers, the computed result can be represented as $\\mathrm{fl}(x \\,\\mathrm{op}\\, y) = (x \\,\\mathrm{op}\\, y)(1+\\delta)$ where $\\mathrm{op} \\in \\{+,-,\\times,\\div\\}$ and $|\\delta| \\le u$, with $u$ being the unit roundoff of the arithmetic in use. In exact arithmetic, the Gram–Schmidt method produces a factorization $A = Q R$ with $Q^\\top Q = I$ and $R$ upper triangular with nonnegative diagonal. In floating-point arithmetic, the computed factors $\\widehat{Q}$ and $\\widehat{R}$ satisfy $A \\approx \\widehat{Q}\\widehat{R}$ and $\\widehat{Q}^\\top \\widehat{Q} \\approx I$.\n\nYour program must:\n- Implement two functions that, given a full-rank or rank-deficient real matrix $A \\in \\mathbb{R}^{m \\times n}$ with $m \\ge n$, return $(Q,R)$ using:\n  1. Classical Gram–Schmidt (single pass, no reorthogonalization).\n  2. Modified Gram–Schmidt (single pass, no reorthogonalization).\n- For each $(Q,R)$ pair and each input matrix $A$, compute:\n  1. The forward error of the factorization as the relative spectral-norm residual\n     $$\\eta_{\\mathrm{fwd}} = \\frac{\\lVert A - Q R \\rVert_2}{\\lVert A \\rVert_2}.$$\n  2. The backward error expressed as loss of orthogonality via\n     $$F = Q^\\top Q - I,\\quad \\eta_{\\mathrm{orth}} = \\lVert F \\rVert_2.$$\n- Use the spectral matrix norm $\\lVert \\cdot \\rVert_2$ for all matrix norms.\n- Round each reported scalar to $12$ decimal places.\n\nTest suite:\n- Case $1$ (well-conditioned tall matrix):\n  $$A_1 = \\begin{bmatrix}\n  1 & 2 & 3 & 4 \\\\\n  2 & 1 & 0 & 1 \\\\\n  0 & 1 & 2 & 3 \\\\\n  1 & 0 & 1 & 0 \\\\\n  2 & 2 & 2 & 2 \\\\\n  3 & 1 & 4 & 1\n  \\end{bmatrix} \\in \\mathbb{R}^{6 \\times 4}.$$\n- Case $2$ (nearly dependent columns): Let $\\varepsilon = 10^{-8}$ and\n  $$c_1 = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5 \\end{bmatrix},\\quad\n  w = \\begin{bmatrix} 1 \\\\ -1 \\\\ 1 \\\\ -1 \\\\ 1 \\end{bmatrix},\\quad\n  c_2 = c_1 + \\varepsilon w,\\quad\n  c_3 = \\begin{bmatrix} 2 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 2 \\end{bmatrix},\\quad\n  A_2 = [\\, c_1\\ \\ c_2\\ \\ c_3 \\,] \\in \\mathbb{R}^{5 \\times 3}.$$\n- Case $3$ (Hilbert-type tall matrix, ill-conditioned columns): For $m = 8$, $n = 5$, define\n  $$\\left(A_3\\right)_{i,j} = \\frac{1}{i + j + 1},\\quad 0 \\le i \\le 7,\\ 0 \\le j \\le 4,$$\n  so that $A_3 \\in \\mathbb{R}^{8 \\times 5}$.\n- Case $4$ (already orthonormal columns): Let $m = 6$, $n = 3$, and\n  $$A_4 = \\begin{bmatrix}\n  1 & 0 & 0 \\\\\n  0 & 0 & 0 \\\\\n  0 & 1 & 0 \\\\\n  0 & 0 & 0 \\\\\n  0 & 0 & 0 \\\\\n  0 & 0 & 1\n  \\end{bmatrix} \\in \\mathbb{R}^{6 \\times 3}.$$\n\nFor each test case $A_k$, compute four scalars:\n- $\\eta_{\\mathrm{fwd}}^{\\mathrm{CGS}}$,\n- $\\eta_{\\mathrm{fwd}}^{\\mathrm{MGS}}$,\n- $\\eta_{\\mathrm{orth}}^{\\mathrm{CGS}}$,\n- $\\eta_{\\mathrm{orth}}^{\\mathrm{MGS}}$,\neach rounded to $12$ decimal places.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for all four cases as a comma-separated list of lists, each inner list ordered as\n  $$[\\eta_{\\mathrm{fwd}}^{\\mathrm{CGS}}, \\eta_{\\mathrm{fwd}}^{\\mathrm{MGS}}, \\eta_{\\mathrm{orth}}^{\\mathrm{CGS}}, \\eta_{\\mathrm{orth}}^{\\mathrm{MGS}}],$$\n  enclosed in square brackets. For example: \n  $$[[a_1, b_1, c_1, d_1], [a_2, b_2, c_2, d_2], [a_3, b_3, c_3, d_3], [a_4, b_4, c_4, d_4]].$$\n\nAll computations must be performed in standard double-precision floating-point arithmetic. No user input is allowed; the program must be self-contained and must print exactly one line in the specified format. All reported values are unitless real numbers rounded to $12$ decimal places.", "solution": "The problem statement is a valid, well-posed, and objective exercise in numerical linear algebra. It requests a comparison of two standard algorithms, Classical Gram-Schmidt (CGS) and Modified Gram-Schmidt (MGS), for computing the QR factorization of a matrix. The problem is scientifically grounded, providing precise mathematical definitions for the algorithms and the error metrics to be computed. All necessary data and test conditions are provided, and there are no ambiguities or contradictions. The task is to implement the specified algorithms and metrics and apply them to a suite of test cases designed to highlight the numerical properties of CGS and MGS.\n\nThe core of the problem is to compute the QR factorization of a given matrix $A \\in \\mathbb{R}^{m \\times n}$ (where $m \\ge n$), which is a decomposition $A = QR$, where $Q \\in \\mathbb{R}^{m \\times n}$ has orthonormal columns ($Q^\\top Q = I_n$) and $R \\in \\mathbb{R}^{n \\times n}$ is an upper triangular matrix. In floating-point arithmetic, the computed factors, denoted $\\widehat{Q}$ and $\\widehat{R}$, will only approximate these properties.\n\nThe Gram-Schmidt process is a fundamental method for constructing such a factorization. It generates a sequence of orthonormal vectors $q_0, q_1, \\dots, q_{n-1}$ that form an orthonormal basis for the column space of $A$. The two variants to be implemented are:\n\n$1$. **Classical Gram-Schmidt (CGS)**: This algorithm computes each vector $q_j$ by explicitly subtracting its projections onto all previously computed orthonormal vectors $q_0, \\dots, q_{j-1}$ from the corresponding column $a_j$ of $A$. The a_j vector is used for all projection computations for column $j$. The steps for column $j$ are:\n$$v_j = a_j - \\sum_{i=0}^{j-1} (q_i^\\top a_j) q_i$$\n$$r_{jj} = \\lVert v_j \\rVert_2, \\quad q_j = v_j / r_{jj}$$\nThe coefficients $r_{ij} = q_i^\\top a_j$ for $i < j$ are also computed.\nWhile mathematically sound, CGS is known to be numerically unstable. Small rounding errors in the computation of $q_i^\\top a_j$ can lead to a significant loss of orthogonality in the computed columns of $\\widehat{Q}$.\n\n$2$. **Modified Gram-Schmidt (MGS)**: This is an algebraically equivalent but numerically more stable rearrangement of CGS. Instead of projecting $a_j$ onto each $q_i$, MGS updates the remaining columns of $A$ at each step. After computing $q_i$, its component is removed from all subsequent columns $a_{i+1}, \\dots, a_{n-1}$. This can be expressed as:\nLet $v^{(0)}_j = a_j$ for all $j$. For $i = 0, \\dots, n-1$:\n$$r_{ii} = \\lVert v^{(i)}_i \\rVert_2, \\quad q_i = v^{(i)}_i / r_{ii}$$\nFor $j = i+1, \\dots, n-1$:\n$$r_{ij} = q_i^\\top v^{(i)}_j, \\quad v^{(i+1)}_j = v^{(i)}_j - r_{ij} q_i$$\nThis process ensures that each new vector is orthogonalized against already-orthogonalized vectors, reducing the accumulation of errors and resulting in a matrix $\\widehat{Q}$ with columns that are much closer to being perfectly orthogonal.\n\nIn cases of rank deficiency, a vector $v_j$ (in CGS) or $v_i^{(i)}$ (in MGS) may become zero or numerically close to zero. The implementation will handle this by checking if the norm is below a small tolerance ($10^{-12}$). If so, the corresponding column $q_j$ is set to a zero vector, and its diagonal entry $r_{jj}$ in $R$ is set to $0$.\n\nThe stability and accuracy of these algorithms are evaluated using two metrics:\n\n$1$. **Forward Error**: $\\eta_{\\mathrm{fwd}} = \\frac{\\lVert A - \\widehat{Q} \\widehat{R} \\rVert_2}{\\lVert A \\rVert_2}$. This measures the relative residual of the factorization. A smaller value indicates that the computed factors $\\widehat{Q}$ and $\\widehat{R}$ more accurately reconstruct the original matrix $A$. This can be interpreted as a measure of backward stability of the factorization problem itself; the computed factors are the exact factors for a nearby matrix $A+E$, and this error measures the size of $E$.\n\n$2$. **Loss of Orthogonality**: $\\eta_{\\mathrm{orth}} = \\lVert \\widehat{Q}^\\top \\widehat{Q} - I \\rVert_2$. This is a direct measure of the backward error of the algorithm with respect to the property of producing an orthonormal $Q$. A value close to $0$ indicates that the columns of $\\widehat{Q}$ are nearly orthonormal, while a value close to $1$ indicates a significant loss of orthogonality. This metric is expected to clearly distinguish the stability of MGS from the instability of CGS, especially for ill-conditioned matrices.\n\nThe program will implement both CGS and MGS, apply them to the four provided test matrices, and compute both error metrics for each factorization. The spectral norm $\\lVert \\cdot \\rVert_2$ is used for all matrix norm computations. The final results for each of the four test cases, consisting of the two errors for both algorithms, are rounded to $12$ decimal places and presented in the specified list-of-lists format.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef classical_gram_schmidt(A):\n    \"\"\"\n    Computes the QR factorization of a matrix A using Classical Gram-Schmidt.\n    \"\"\"\n    m, n = A.shape\n    Q = np.zeros((m, n), dtype=float)\n    R = np.zeros((n, n), dtype=float)\n    \n    for j in range(n):\n        v = A[:, j].copy()\n        for i in range(j):\n            R[i, j] = Q[:, i].T @ A[:, j] # Project a_j onto q_i\n            v -= R[i, j] * Q[:, i]\n            \n        norm_v = np.linalg.norm(v)\n        \n        if norm_v > 1e-12:\n            R[j, j] = norm_v\n            Q[:, j] = v / R[j, j]\n        else:\n            R[j, j] = 0.0\n            # Q[:, j] remains a zero vector\n            \n    return Q, R\n\ndef modified_gram_schmidt(A):\n    \"\"\"\n    Computes the QR factorization of a matrix A using Modified Gram-Schmidt.\n    \"\"\"\n    m, n = A.shape\n    V = A.copy()\n    Q = np.zeros((m, n), dtype=float)\n    R = np.zeros((n, n), dtype=float)\n    \n    for i in range(n):\n        norm_v_i = np.linalg.norm(V[:, i])\n        R[i, i] = norm_v_i\n        \n        if R[i, i] > 1e-12:\n            Q[:, i] = V[:, i] / R[i, i]\n            for j in range(i + 1, n):\n                R[i, j] = Q[:, i].T @ V[:, j]\n                V[:, j] -= R[i, j] * Q[:, i]\n        else:\n            # R[i, i] is already set to the small norm (or 0)\n            # Q[:, i] remains a zero vector.\n            # Projections of V[:, j] onto a zero Q[:, i] are zero, so R[i, j] for j>i are zero\n            # and subsequent V columns are not modified.\n            pass\n            \n    return Q, R\n\ndef compute_errors(A, Q, R):\n    \"\"\"\n    Computes the forward error and orthogonality loss for a given QR factorization.\n    \"\"\"\n    m, n = A.shape\n    \n    # Forward error: ||A - QR||_2 / ||A||_2\n    norm_A = np.linalg.norm(A, 2)\n    if norm_A == 0:\n        fwd_error = 0.0\n    else:\n        fwd_error = np.linalg.norm(A - Q @ R, 2) / norm_A\n\n    # Orthogonality loss: ||Q^T Q - I||_2\n    if n > 0:\n        orth_error = np.linalg.norm(Q.T @ Q - np.eye(n), 2)\n    else:\n        orth_error = 0.0\n        \n    return fwd_error, orth_error\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis on the defined test cases.\n    \"\"\"\n    # Case 1: Well-conditioned tall matrix\n    A1 = np.array([\n        [1, 2, 3, 4],\n        [2, 1, 0, 1],\n        [0, 1, 2, 3],\n        [1, 0, 1, 0],\n        [2, 2, 2, 2],\n        [3, 1, 4, 1]\n    ], dtype=float)\n\n    # Case 2: Nearly dependent columns\n    epsilon = 1e-8\n    c1 = np.array([1, 2, 3, 4, 5], dtype=float).reshape(-1, 1)\n    w = np.array([1, -1, 1, -1, 1], dtype=float).reshape(-1, 1)\n    c2 = c1 + epsilon * w\n    c3 = np.array([2, 0, 1, 0, 2], dtype=float).reshape(-1, 1)\n    A2 = np.hstack([c1, c2, c3])\n\n    # Case 3: Hilbert-type tall matrix\n    m3, n3 = 8, 5\n    A3 = np.fromfunction(lambda i, j: 1 / (i + j + 1), (m3, n3), dtype=int)\n\n    # Case 4: Already orthonormal columns\n    A4 = np.array([\n        [1, 0, 0],\n        [0, 0, 0],\n        [0, 1, 0],\n        [0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 1]\n    ], dtype=float)\n\n    test_cases = [A1, A2, A3, A4]\n    all_results = []\n\n    for A in test_cases:\n        # Classical Gram-Schmidt\n        Q_cgs, R_cgs = classical_gram_schmidt(A)\n        fwd_cgs, orth_cgs = compute_errors(A, Q_cgs, R_cgs)\n\n        # Modified Gram-Schmidt\n        Q_mgs, R_mgs = modified_gram_schmidt(A)\n        fwd_mgs, orth_mgs = compute_errors(A, Q_mgs, R_mgs)\n\n        case_results = [\n            round(fwd_cgs, 12),\n            round(fwd_mgs, 12),\n            round(orth_cgs, 12),\n            round(orth_mgs, 12)\n        ]\n        all_results.append(case_results)\n\n    # Format the output as a string representation of a list of lists, with spaces\n    # Example: [[a1, b1, c1, d1], [a2, ...]]\n    # str() on a list provides the desired spacing.\n    result_strings = [str(res).replace(\"'\", \"\") for res in all_results]\n    print(f\"[{', '.join(result_strings)}]\")\n\nsolve()\n```", "id": "3232097"}]}