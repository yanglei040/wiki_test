{"hands_on_practices": [{"introduction": "The journey to mastering any new computational technique begins with a single, well-understood step. This first practice focuses on the fundamental mechanics of Newton's method for systems. By manually performing one complete iteration, you will build a concrete understanding of how the function vector $\\mathbf{F}(\\mathbf{x})$, the Jacobian matrix $J(\\mathbf{x})$, and the linear update system $J(\\mathbf{x}_k)\\mathbf{s}_k = -\\mathbf{F}(\\mathbf{x}_k)$ work together to produce a better approximation of the solution [@problem_id:2207863].", "problem": "Consider the following system of two non-linear equations:\n$$\n\\begin{cases}\n2x^2 + y = 11 \\\\\nx + 2y^2 = 10\n\\end{cases}\n$$\nAn approximate solution to this system is sought using Newton's method for systems. Starting with the initial guess $(x_0, y_0) = (3, 1)$, perform a single iteration to find the next approximation $(x_1, y_1)$.\n\nFind the coordinates of $(x_1, y_1)$. Express each coordinate as a rational number in its simplest form.", "solution": "Define the vector function $\\mathbf{F}(x,y)$ and its Jacobian matrix $J(x,y)$ by\n$$\n\\mathbf{F}(x,y)=\\begin{pmatrix} 2x^{2}+y-11 \\\\ x+2y^{2}-10 \\end{pmatrix}, \n\\quad\nJ(x,y)=\\begin{pmatrix} \\frac{\\partial}{\\partial x}(2x^{2}+y-11)  \\frac{\\partial}{\\partial y}(2x^{2}+y-11) \\\\ \\frac{\\partial}{\\partial x}(x+2y^{2}-10)  \\frac{\\partial}{\\partial y}(x+2y^{2}-10) \\end{pmatrix}\n=\\begin{pmatrix} 4x  1 \\\\ 1  4y \\end{pmatrix}.\n$$\nNewton’s method for systems computes the update $\\mathbf{s}=(s_{x},s_{y})^{T}$ by solving\n$$\nJ(x_{0},y_{0})\\,\\mathbf{s}=-\\mathbf{F}(x_{0},y_{0}),\n$$\nand then sets $(x_{1},y_{1})=(x_{0},y_{0})+\\mathbf{s}$.\n\nAt $(x_{0},y_{0})=(3,1)$, evaluate\n$$\n\\mathbf{F}(3,1)=\\begin{pmatrix} 2\\cdot 3^{2}+1-11 \\\\ 3+2\\cdot 1^{2}-10 \\end{pmatrix}\n=\\begin{pmatrix} 8 \\\\ -5 \\end{pmatrix},\n\\quad\nJ(3,1)=\\begin{pmatrix} 4\\cdot 3  1 \\\\ 1  4\\cdot 1 \\end{pmatrix}\n=\\begin{pmatrix} 12  1 \\\\ 1  4 \\end{pmatrix}.\n$$\nSolve for $\\mathbf{s}$ in\n$$\n\\begin{pmatrix} 12  1 \\\\ 1  4 \\end{pmatrix}\\begin{pmatrix} s_{x} \\\\ s_{y} \\end{pmatrix}\n=-\\begin{pmatrix} 8 \\\\ -5 \\end{pmatrix}\n=\\begin{pmatrix} -8 \\\\ 5 \\end{pmatrix},\n$$\nwhich is the linear system\n$$\n\\begin{cases}\n12s_{x}+s_{y}=-8, \\\\\ns_{x}+4s_{y}=5.\n\\end{cases}\n$$\nFrom $s_{x}=5-4s_{y}$ and substitution into the first equation,\n$$\n12(5-4s_{y})+s_{y}=-8\n\\;\\Rightarrow\\;\n60-48s_{y}+s_{y}=-8\n\\;\\Rightarrow\\;\n-47s_{y}=-68\n\\;\\Rightarrow\\;\ns_{y}=\\frac{68}{47}.\n$$\nThen\n$$\ns_{x}=5-4\\cdot \\frac{68}{47}\n=\\frac{235}{47}-\\frac{272}{47}\n=-\\frac{37}{47}.\n$$\nUpdate the approximation:\n$$\nx_{1}=x_{0}+s_{x}=3-\\frac{37}{47}=\\frac{141}{47}-\\frac{37}{47}=\\frac{104}{47}, \n\\quad\ny_{1}=y_{0}+s_{y}=1+\\frac{68}{47}=\\frac{47}{47}+\\frac{68}{47}=\\frac{115}{47}.\n$$\nThus, the next Newton iterate is $\\left(\\frac{104}{47}, \\frac{115}{47}\\right)$, with both coordinates in simplest rational form.", "answer": "$$\\boxed{\\left(\\frac{104}{47}, \\frac{115}{47}\\right)}$$", "id": "2207863"}, {"introduction": "A powerful algorithm is only useful if you also understand its limitations. This exercise challenges you to think critically about the conditions under which Newton's method can fail. By analyzing the behavior of the Jacobian matrix for certain initial guesses, you will uncover why the invertibility of $J(\\mathbf{x})$ is not just a mathematical formality but a critical requirement for the algorithm's stability and success [@problem_id:2207871].", "problem": "An engineer is tasked with finding a numerical solution to a system of non-linear equations modeling a steady-state physical system. The equations are given by:\n$$f_1(x, y) = x^2 - y^2 - 4 = 0$$\n$$f_2(x, y) = xy - 3 = 0$$\nThe engineer decides to use Newton's method for systems. The iterative formula for this method is given by $\\mathbf{x}_{k+1} = \\mathbf{x}_k - [J(\\mathbf{x}_k)]^{-1} \\mathbf{F}(\\mathbf{x}_k)$, where $\\mathbf{x} = (x, y)^T$, $\\mathbf{F} = (f_1, f_2)^T$, and $J(\\mathbf{x})$ is the Jacobian matrix of $\\mathbf{F}$.\n\nAfter some trials, the engineer observes that choosing an initial guess $\\mathbf{x}_0 = (x_0, y_0)$ that lies on either the x-axis (where $y_0 = 0$) or the y-axis (where $x_0 = 0$) is a poor strategy for this specific system. Which of the following statements provides the most accurate and fundamental reason for this poor performance?\n\nA. For any initial guess on an axis, the first iteration of Newton's method produces a point that is also on an axis, preventing the algorithm from ever moving towards a solution, as the solutions do not lie on an axis.\n\nB. For any initial guess on an axis, the function vector $\\mathbf{F}(\\mathbf{x}_0)$ is a zero vector, which incorrectly signals to the algorithm that a solution has been found.\n\nC. The Jacobian matrix is singular for any point on the x-axis or y-axis, causing the method to fail immediately due to an inability to compute the matrix inverse.\n\nD. The point $(0,0)$ is the only location where the Jacobian is singular. An initial guess on an axis that is close to the origin leads to a nearly-singular Jacobian, causing the subsequent iteration to produce a point extremely far from the true solution, indicating poor convergence behavior.\n\nE. The system of equations has no real solutions, so Newton's method will fail to converge regardless of the initial guess.", "solution": "We are given the system\n$$\nf_{1}(x,y)=x^{2}-y^{2}-4,\\qquad f_{2}(x,y)=xy-3,\n$$\nwith vector function $\\mathbf{F}(x,y)=(f_{1}(x,y),f_{2}(x,y))^{T}$ and Jacobian\n$$\nJ(x,y)=\\begin{pmatrix}\n\\frac{\\partial f_{1}}{\\partial x}  \\frac{\\partial f_{1}}{\\partial y}\\\\\n\\frac{\\partial f_{2}}{\\partial x}  \\frac{\\partial f_{2}}{\\partial y}\n\\end{pmatrix}\n=\\begin{pmatrix}\n2x  -2y\\\\\ny  x\n\\end{pmatrix}.\n$$\nNewton’s method updates $\\mathbf{x}_{k+1}=\\mathbf{x}_{k}-J(\\mathbf{x}_{k})^{-1}\\mathbf{F}(\\mathbf{x}_{k})$, which requires $J$ to be invertible at the iterate.\n\nFirst, compute the determinant of the Jacobian:\n$$\n\\det J(x,y)=(2x)(x)-(-2y)(y)=2x^{2}+2y^{2}=2(x^{2}+y^{2}).\n$$\nThus $J(x,y)$ is singular if and only if $(x,y)=(0,0)$. In particular, $J$ is not singular at generic points on the axes (except at the origin). This immediately shows option C is false.\n\nNext, evaluate $\\mathbf{F}$ on the axes. On the $x$-axis with $y=0$,\n$$\n\\mathbf{F}(x,0)=\\bigl(x^{2}-4,\\,-3\\bigr),\n$$\nwhich is never the zero vector. On the $y$-axis with $x=0$,\n$$\n\\mathbf{F}(0,y)=\\bigl(-y^{2}-4,\\,-3\\bigr),\n$$\nwhich is also never the zero vector. Hence option B is false.\n\nNow check whether Newton’s method remains on an axis after one iteration. Use the Newton step $\\mathbf{s}$ defined by $J\\mathbf{s}=-\\mathbf{F}$. On the $x$-axis ($y=0$), we have\n$$\nJ(x,0)=\\begin{pmatrix}2x  0\\\\ 0  x\\end{pmatrix},\\qquad \\mathbf{F}(x,0)=\\begin{pmatrix}x^{2}-4\\\\ -3\\end{pmatrix}.\n$$\nThen\n$$\n\\mathbf{s}=-J^{-1}\\mathbf{F}=-\\begin{pmatrix}\\frac{1}{2x}  0\\\\ 0  \\frac{1}{x}\\end{pmatrix}\\begin{pmatrix}x^{2}-4\\\\ -3\\end{pmatrix}\n=\\begin{pmatrix}-\\frac{x^{2}-4}{2x}\\\\ \\frac{3}{x}\\end{pmatrix},\n$$\nso the next iterate is\n$$\nx_{1}=x-\\frac{x^{2}-4}{2x}=\\frac{x}{2}+\\frac{2}{x},\\qquad y_{1}=0+\\frac{3}{x}=\\frac{3}{x}.\n$$\nSince $y_{1}=\\frac{3}{x}\\neq 0$ for any finite $x$, the iterate immediately leaves the axis. A similar calculation on the $y$-axis ($x=0$) solves\n$$\n\\begin{pmatrix}0  -2y\\\\ y  0\\end{pmatrix}\\begin{pmatrix}s_{x}\\\\ s_{y}\\end{pmatrix}=-\\begin{pmatrix}-y^{2}-4\\\\ -3\\end{pmatrix}=\\begin{pmatrix}y^{2}+4\\\\ 3\\end{pmatrix},\n$$\nwhich yields $s_{x}=\\frac{3}{y}$ and $s_{y}=-\\frac{y^{2}+4}{2y}$, so\n$$\nx_{1}=0+\\frac{3}{y}=\\frac{3}{y},\\qquad y_{1}=y-\\frac{y^{2}+4}{2y}=\\frac{y}{2}-\\frac{2}{y}.\n$$\nAgain, the iterate leaves the axis immediately. Therefore option A is false.\n\nTo rule out option E, we check for real solutions. From $xy=3$ we have $y=\\frac{3}{x}$, and substituting into $x^{2}-y^{2}=4$ gives\n$$\nx^{2}-\\frac{9}{x^{2}}=4\\;\\;\\Longrightarrow\\;\\; x^{4}-4x^{2}-9=0.\n$$\nLet $t=x^{2}$. Then $t^{2}-4t-9=0$, so $t=2\\pm\\sqrt{13}$. The admissible root is $t=2+\\sqrt{13}0$, hence\n$$\nx=\\pm\\sqrt{2+\\sqrt{13}},\\qquad y=\\frac{3}{x}=\\pm\\frac{3}{\\sqrt{2+\\sqrt{13}}},\n$$\nwhich shows there are two real solutions. Therefore option E is false.\n\nIt remains to identify the fundamental reason axes are a poor choice for initial guesses. Since\n$$\n\\det J(x,y)=2(x^{2}+y^{2}),\n$$\nthe Jacobian is singular at $(0,0)$ and becomes ill-conditioned when $(x,y)$ is close to the origin. The explicit inverse is\n$$\nJ(x,y)^{-1}=\\frac{1}{2(x^{2}+y^{2})}\\begin{pmatrix}x  2y\\\\ -y  2x\\end{pmatrix},\n$$\nwhose entries scale like $\\frac{1}{x^{2}+y^{2}}$ times linear functions of $x$ and $y$. Near the origin this produces large Newton steps. On the axes in particular, the Newton updates derived above contain terms like $\\frac{3}{x}$ or $\\frac{3}{y}$, which become very large in magnitude when the initial guess is on an axis and close to the origin. This ill-conditioning explains the observed poor performance and is precisely captured by option D.\n\nTherefore, the most accurate and fundamental reason is that the Jacobian is singular at the origin and nearly singular for axis-aligned initial guesses near the origin, leading to instability and poor convergence.", "answer": "$$\\boxed{D}$$", "id": "2207871"}, {"introduction": "Newton's method involves more computational effort per iteration than simpler methods like steepest descent, so why is it often preferred? This coding challenge provides a dramatic answer by comparing the two algorithms on a problem with a \"narrow canyon\" in its residual landscape. Through implementation, you will witness firsthand how Newton's method uses curvature information to converge rapidly where steepest descent falters, demonstrating its superior efficiency on ill-conditioned problems [@problem_id:3200259].", "problem": "You are to study numerical solution behavior for a two-dimensional system of nonlinear equations whose residual norm landscape exhibits a narrow, curved canyon when a small parameter is present. Let the system be defined by the mapping $F_{\\varepsilon}:\\mathbb{R}^2\\to\\mathbb{R}^2$ given by\n$$\nF_{\\varepsilon}(x,y) = \\begin{bmatrix} \\varepsilon x + y^2 - 1 \\\\ x^2 - y \\end{bmatrix},\n$$\nwhere $\\varepsilon  0$ is a parameter. Define the scalar objective function\n$$\n\\phi(x,y) = \\tfrac{1}{2}\\,\\lVert F_{\\varepsilon}(x,y)\\rVert_2^2.\n$$\nStart from the fundamental definitions that the gradient of a composition is obtained by the chain rule and that the Jacobian matrix $J(x,y)$ of $F_{\\varepsilon}(x,y)$ is the matrix of first-order partial derivatives of $F_{\\varepsilon}$ with respect to $(x,y)$. Using only these bases, derive the gradient of $\\phi(x,y)$ and the Jacobian matrix $J(x,y)$ for the given $F_{\\varepsilon}(x,y)$. Then design and implement two methods to attempt to find a root of $F_{\\varepsilon}(x,y)=\\mathbf{0}$:\n- Method A: Steepest descent applied to $\\phi(x,y)$ with a backtracking Armijo line search. Take a descent direction to be the negative gradient of $\\phi(x,y)$ at the current iterate and use backtracking to enforce sufficient decrease in $\\phi(x,y)$.\n- Method B: Newton’s method for systems with a backtracking Armijo line search. At each iterate, solve the linearization system for the update direction and then use backtracking to enforce sufficient decrease in $\\phi(x,y)$.\n\nYour implementation must be fully deterministic and must use the same stopping and line-search rules for both methods:\n- Stop successfully when $\\lVert F_{\\varepsilon}(x_k,y_k)\\rVert_2 \\leq \\tau_F$, where $\\tau_F = 10^{-10}$.\n- Also stop (unsuccessfully) if the step length satisfies $\\alpha_k \\lVert d_k\\rVert_2 \\leq \\tau_X$, where $\\tau_X = 10^{-12}$, or when the iteration count reaches the maximum allowed iterations for the method.\n- For backtracking, start with step size $\\alpha = 1$ and reduce by multiplying by a factor $\\tau_{\\text{ls}}$ until the Armijo condition holds, where $\\tau_{\\text{ls}} = 0.5$. Use Armijo condition parameter $c_1 = 10^{-4}$ for both methods. Impose a hard cap of at most $50$ backtracking reductions per iteration.\n\nUse the following maximum iteration limits:\n- For Method A (steepest descent): $N_{\\max}^{\\text{SD}} = 5000$ iterations.\n- For Method B (Newton): $N_{\\max}^{\\text{N}} = 100$ iterations.\n\nYour program must run both methods on each test case below and report, for each case, the following list in order: the number of iterations taken by Method A as a nonnegative integer, the number of iterations taken by Method B as a nonnegative integer, a boolean indicating whether Method A terminated successfully (as defined above), and a boolean indicating whether Method B terminated successfully (as defined above).\n\nUse the test suite of parameter-initialization pairs $(\\varepsilon,(x_0,y_0))$:\n- Case $1$: $\\varepsilon = 10^{-3}$, $(x_0,y_0) = (0.5,\\,0.5)$.\n- Case $2$: $\\varepsilon = 10^{-6}$, $(x_0,y_0) = (0.5,\\,0.5)$.\n- Case $3$: $\\varepsilon = 2\\times 10^{-1}$, $(x_0,y_0) = (0.5,\\,0.5)$.\n- Case $4$: $\\varepsilon = 0$, $(x_0,y_0) = (0.5,\\,0.5)$.\n- Case $5$: $\\varepsilon = 10^{-3}$, $(x_0,y_0) = (-1.5,\\,1.3)$.\n\nNotes on scientific realism and expectations: For small $\\varepsilon$, the level sets of $\\phi(x,y)$ form a narrow, curved canyon that can lead steepest descent to zig-zag with tiny steps, while Newton’s method typically leverages curvature information to converge rapidly when close enough to a solution and when the Jacobian is nonsingular. The provided stopping and line-search parameters are standard and ensure comparable criteria across both methods.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element of this outer list corresponds to one test case, in the order listed above, and must itself be a list with four entries $[n_{\\text{SD}}, n_{\\text{N}}, s_{\\text{SD}}, s_{\\text{N}}]$, where $n_{\\text{SD}}$ and $n_{\\text{N}}$ are nonnegative integers and $s_{\\text{SD}}$ and $s_{\\text{N}}$ are booleans. For example, the final output must look like $[[n_{1,\\text{SD}},n_{1,\\text{N}},s_{1,\\text{SD}},s_{1,\\text{N}}],[n_{2,\\text{SD}},n_{2,\\text{N}},s_{2,\\text{SD}},s_{2,\\text{N}}],\\dots]$ with no additional text or whitespace requirements beyond standard comma separation.", "solution": "The user has provided a well-defined problem in computational science. The task is to derive the necessary mathematical quantities for two numerical root-finding algorithms, implement these algorithms, and apply them to a specified set of test cases. The problem is scientifically grounded, internally consistent, and requires no external information beyond standard mathematical and numerical principles.\n\n### Mathematical Derivations\n\nThe system of nonlinear equations is given by the mapping $F_{\\varepsilon}:\\mathbb{R}^2\\to\\mathbb{R}^2$, where the vector function $F_{\\varepsilon}(x,y)$ has components:\n$$\nF_{\\varepsilon}(x,y) = \\begin{bmatrix} F_1(x,y) \\\\ F_2(x,y) \\end{bmatrix} = \\begin{bmatrix} \\varepsilon x + y^2 - 1 \\\\ x^2 - y \\end{bmatrix}\n$$\nThe scalar objective function, $\\phi(x,y)$, which measures the squared residual norm, is defined as:\n$$\n\\phi(x,y) = \\frac{1}{2}\\,\\lVert F_{\\varepsilon}(x,y)\\rVert_2^2 = \\frac{1}{2} \\left[ (\\varepsilon x + y^2 - 1)^2 + (x^2 - y)^2 \\right]\n$$\nFinding a root of $F_{\\varepsilon}(x,y) = \\mathbf{0}$ is equivalent to finding a global minimum of $\\phi(x,y)$ where $\\phi(x,y)=0$.\n\n#### Jacobian Matrix $J(x,y)$\nAs per the problem's definition, the Jacobian matrix $J(x,y)$ is the matrix of first-order partial derivatives of $F_{\\varepsilon}(x,y)$ with respect to the vector of variables $(x,y)$.\n$$\nJ(x,y) = \\begin{bmatrix} \\frac{\\partial F_1}{\\partial x}  \\frac{\\partial F_1}{\\partial y} \\\\ \\frac{\\partial F_2}{\\partial x}  \\frac{\\partial F_2}{\\partial y} \\end{bmatrix}\n$$\nWe compute the four partial derivatives:\n- $\\frac{\\partial F_1}{\\partial x} = \\frac{\\partial}{\\partial x}(\\varepsilon x + y^2 - 1) = \\varepsilon$\n- $\\frac{\\partial F_1}{\\partial y} = \\frac{\\partial}{\\partial y}(\\varepsilon x + y^2 - 1) = 2y$\n- $\\frac{\\partial F_2}{\\partial x} = \\frac{\\partial}{\\partial x}(x^2 - y) = 2x$\n- $\\frac{\\partial F_2}{\\partial y} = \\frac{\\partial}{\\partial y}(x^2 - y) = -1$\n\nSubstituting these into the matrix gives the Jacobian:\n$$\nJ(x,y) = \\begin{bmatrix} \\varepsilon  2y \\\\ 2x  -1 \\end{bmatrix}\n$$\n\n#### Gradient of $\\phi(x,y)$\nThe problem requires deriving the gradient of $\\phi(x,y)$, denoted $\\nabla\\phi(x,y)$, using the chain rule. The function $\\phi$ is a composition of the form $\\phi(\\mathbf{v}) = h(F(\\mathbf{v}))$, where $\\mathbf{v} = [x, y]^T$ and $h(\\mathbf{u}) = \\frac{1}{2}\\mathbf{u}^T\\mathbf{u}$. The chain rule for vector functions states that $\\nabla\\phi(\\mathbf{v}) = J(\\mathbf{v})^T \\nabla h(F(\\mathbf{v}))$. The gradient of $h(\\mathbf{u})$ is simply $\\mathbf{u}$. Therefore, we have the general formula:\n$$\n\\nabla\\phi(x,y) = J(x,y)^T F_{\\varepsilon}(x,y)\n$$\nUsing the previously derived $J(x,y)$ and the definition of $F_{\\varepsilon}(x,y)$, we compute the product:\n$$\n\\nabla\\phi(x,y) = \\begin{bmatrix} \\varepsilon  2x \\\\ 2y  -1 \\end{bmatrix} \\begin{bmatrix} \\varepsilon x + y^2 - 1 \\\\ x^2 - y \\end{bmatrix}\n$$\n$$\n= \\begin{bmatrix} \\varepsilon(\\varepsilon x + y^2 - 1) + 2x(x^2 - y) \\\\ 2y(\\varepsilon x + y^2 - 1) - 1(x^2 - y) \\end{bmatrix}\n$$\n$$\n= \\begin{bmatrix} \\varepsilon^2 x + \\varepsilon y^2 - \\varepsilon + 2x^3 - 2xy \\\\ 2\\varepsilon xy + 2y^3 - 2y - x^2 + y \\end{bmatrix} = \\begin{bmatrix} \\varepsilon^2 x + \\varepsilon y^2 - \\varepsilon + 2x^3 - 2xy \\\\ 2\\varepsilon xy + 2y^3 - x^2 - y \\end{bmatrix}\n$$\nThis gradient is essential for both the steepest descent method and the Armijo line search condition.\n\n### Algorithmic Design\n\nBoth methods are iterative, generating a sequence of points $\\mathbf{x}_k = [x_k, y_k]^T$ starting from an initial guess $\\mathbf{x}_0$. The general update step is $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k d_k$, where $d_k$ is a search direction and $\\alpha_k$ is a step length.\n\n#### Method A: Steepest Descent with Backtracking\nThis method uses the negative gradient of the objective function $\\phi$ as its search direction. This is the direction of the locally steepest descent.\n1.  **Search Direction**: At iterate $\\mathbf{x}_k$, the direction is $d_k = -\\nabla\\phi(\\mathbf{x}_k)$.\n2.  **Step Length**: The step length $\\alpha_k$ is determined by a backtracking line search. Starting with $\\alpha=1$, the step length is repeatedly reduced by a factor $\\tau_{\\text{ls}} = 0.5$ until the Armijo condition for sufficient decrease is met:\n    $$\n    \\phi(\\mathbf{x}_k + \\alpha d_k) \\le \\phi(\\mathbf{x}_k) + c_1 \\alpha \\nabla\\phi(\\mathbf{x}_k)^T d_k\n    $$\n    with $c_1 = 10^{-4}$.\n\n#### Method B: Newton's Method with Backtracking\nThis method uses second-order information (via the Jacobian) to form a linear model of the system at the current point and solves for the step that would zero out this model.\n1.  **Search Direction**: At iterate $\\mathbf{x}_k$, the direction $d_k$ is found by solving the Newton system:\n    $$\n    J(\\mathbf{x}_k) d_k = -F_{\\varepsilon}(\\mathbf{x}_k)\n    $$\n    This direction is a descent direction for $\\phi$ provided $J(\\mathbf{x}_k)$ is nonsingular, as $\\nabla\\phi(\\mathbf{x}_k)^T d_k = (J^T F)^T d_k = F^T J d_k = F^T(-F) = -\\lVert F \\rVert_2^2  0$.\n2.  **Step Length**: The same backtracking Armijo line search as in Method A is used to find a step length $\\alpha_k$ that ensures a sufficient decrease in the objective function $\\phi$, globalizing the convergence properties of the method.\n\n#### Stopping Criteria\nBoth methods use a common set of deterministic stopping criteria for a fair comparison:\n- **Success**: The iteration terminates successfully if $\\lVert F_{\\varepsilon}(\\mathbf{x}_k)\\rVert_2 \\leq \\tau_F = 10^{-10}$.\n- **Failure (Stagnation)**: The iteration terminates unsuccessfully if the step becomes too small, i.e., $\\alpha_k \\lVert d_k\\rVert_2 \\leq \\tau_X = 10^{-12}$.\n- **Failure (Max Iterations)**: The iteration terminates unsuccessfully if the number of iterations reaches the specified maximum ($N_{\\max}^{\\text{SD}} = 5000$ for Steepest Descent, $N_{\\max}^{\\text{N}} = 100$ for Newton's method).\n\nThe implementation will follow these designs, processing each test case with both methods and reporting the results in the specified format.", "answer": "```python\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Solves the nonlinear system problem using Steepest Descent and Newton's method,\n    and reports the performance on several test cases.\n    \"\"\"\n\n    # Define problem constants and parameters\n    TAU_F = 1e-10\n    TAU_X = 1e-12\n    C1 = 1e-4\n    TAU_LS = 0.5\n    MAX_LS_ITER = 50\n    N_MAX_SD = 5000\n    N_MAX_N = 100\n\n    # Define test cases: (epsilon, (x0, y0))\n    test_cases = [\n        (1e-3, (0.5, 0.5)),\n        (1e-6, (0.5, 0.5)),\n        (2e-1, (0.5, 0.5)),\n        (0.0, (0.5, 0.5)),\n        (1e-3, (-1.5, 1.3)),\n    ]\n\n    # --- Helper functions for the mathematical model ---\n    def F_vec(x, y, eps):\n        \"\"\"Computes the vector F_epsilon(x, y).\"\"\"\n        return np.array([eps * x + y**2 - 1.0, x**2 - y])\n\n    def J_mat(x, y, eps):\n        \"\"\"Computes the Jacobian matrix J(x, y).\"\"\"\n        return np.array([[eps, 2.0 * y], [2.0 * x, -1.0]])\n\n    # --- Solver Implementations ---\n\n    def solve_sd(eps, x0, y0):\n        \"\"\"Method A: Steepest Descent with backtracking Armijo line search.\"\"\"\n        xk = np.array([x0, y0], dtype=float)\n        \n        for k in range(N_MAX_SD + 1):\n            Fk = F_vec(xk[0], xk[1], eps)\n            \n            if np.linalg.norm(Fk) = TAU_F:\n                return k, True\n\n            if k == N_MAX_SD:\n                break\n\n            Jk = J_mat(xk[0], xk[1], eps)\n            grad_phik = Jk.T @ Fk\n            \n            dk = -grad_phik\n\n            # Backtracking line search\n            alpha = 1.0\n            phik = 0.5 * np.dot(Fk, Fk)\n            armijo_term = C1 * np.dot(grad_phik, dk)\n            \n            for _ in range(MAX_LS_ITER):\n                x_trial = xk + alpha * dk\n                F_trial = F_vec(x_trial[0], x_trial[1], eps)\n                phi_trial = 0.5 * np.dot(F_trial, F_trial)\n                \n                if phi_trial = phik + alpha * armijo_term:\n                    break\n                alpha *= TAU_LS\n            \n            if alpha * np.linalg.norm(dk) = TAU_X:\n                return k + 1, False\n            \n            xk = xk + alpha * dk\n        \n        return N_MAX_SD, False\n\n    def solve_newton(eps, x0, y0):\n        \"\"\"Method B: Newton's method with backtracking Armijo line search.\"\"\"\n        xk = np.array([x0, y0], dtype=float)\n        \n        for k in range(N_MAX_N + 1):\n            Fk = F_vec(xk[0], xk[1], eps)\n\n            if np.linalg.norm(Fk) = TAU_F:\n                return k, True\n\n            if k == N_MAX_N:\n                break\n            \n            Jk = J_mat(xk[0], xk[1], eps)\n\n            try:\n                dk = np.linalg.solve(Jk, -Fk)\n            except np.linalg.LinAlgError:\n                return k + 1, False\n\n            # Backtracking line search\n            alpha = 1.0\n            phik = 0.5 * np.dot(Fk, Fk)\n            grad_phik = Jk.T @ Fk\n            armijo_term = C1 * np.dot(grad_phik, dk)\n\n            for _ in range(MAX_LS_ITER):\n                x_trial = xk + alpha * dk\n                F_trial = F_vec(x_trial[0], x_trial[1], eps)\n                phi_trial = 0.5 * np.dot(F_trial, F_trial)\n\n                if phi_trial = phik + alpha * armijo_term:\n                    break\n                alpha *= TAU_LS\n\n            if alpha * np.linalg.norm(dk) = TAU_X:\n                return k + 1, False\n            \n            xk = xk + alpha * dk\n\n        return N_MAX_N, False\n\n    # --- Main execution loop ---\n    results = []\n    for eps, (x0, y0) in test_cases:\n        n_sd, s_sd = solve_sd(eps, x0, y0)\n        n_n, s_n = solve_newton(eps, x0, y0)\n        results.append([n_sd, n_n, s_sd, s_n])\n\n    # Format and print the final output\n    # Using str() on a list gives a representation with spaces, e.g., '[1, 2, True, False]'\n    # Using a custom formatter to ensure no spaces and lowercase booleans.\n    # On reflection, str() is fine as \"standard comma separation\" allows for spaces.\n    # The problem asks for boolean outputs, and Python's str(True) is 'True'. This is unambiguous.\n    result_strings = []\n    for res in results:\n        # Convert boolean to string 'True' or 'False' as standard `str` does.\n        res_str = f\"[{res[0]},{res[1]},{res[2]},{res[3]}]\"\n        # The default list __str__ adds spaces, so we build it manually\n        # to match the example format more closely, although it's not strictly required.\n        result_strings.append(res_str.replace(\" \", \"\"))\n\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```", "id": "3200259"}]}