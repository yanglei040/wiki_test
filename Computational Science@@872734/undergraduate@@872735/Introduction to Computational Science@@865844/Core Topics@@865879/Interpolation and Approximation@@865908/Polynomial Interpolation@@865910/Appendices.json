{"hands_on_practices": [{"introduction": "Our first practice moves beyond simple linear connections to construct a quadratic model. This exercise is not just about solving a system of equations; it's about using insight from the physical context—in this case, the symmetrical properties of a parabola—to find the solution more elegantly. This develops the crucial skill of integrating domain knowledge into the mathematical modeling process [@problem_id:2181788].", "problem": "A materials scientist is studying the behavior of a newly developed alloy under thermal stress. The deflection, $y(x)$, of a thin beam of this alloy at a position $x$ along its length is measured. Experimental observations over a specific range suggest that the deflection profile can be accurately modeled by a quadratic polynomial of the form $y(x) = ax^2 + bx + c$. Three precise measurements are recorded:\n\n- At position $x_1 = -1$, the deflection is $y_1 = 8$.\n- At position $x_2 = 2$, the deflection is $y_2 = -1$.\n- At position $x_3 = 5$, the deflection is $y_3 = 8$.\n\nIt is also known from the principles of beam mechanics for this particular setup that one of these three measurement points corresponds to the point of maximum deflection (or minimum, depending on the sign of $a$). This point is the vertex of the parabola.\n\nDetermine the quadratic polynomial $y(x)$ that describes the deflection profile. Express your answer in the standard form $y(x) = ax^2 + bx + c$.", "solution": "We model the deflection by a quadratic $y(x)=ax^{2}+bx+c$. A parabola can be written in vertex form as $y(x)=a(x-h)^{2}+k$, where $(h,k)$ is the vertex, and the axis of symmetry is $x=h$. Points equidistant from $h$ have equal $y$-values.\n\nFrom the data, $y(-1)=8$ and $y(5)=8$. The midpoint of $-1$ and $5$ is $h=\\frac{-1+5}{2}=2$, so the axis of symmetry is $x=2$. It is given that one of the measured points is the vertex; since $y(2)=-1$ and this is extremal relative to the other two values, the vertex is $(h,k)=(2,-1)$.\n\nThus the quadratic has the form\n$$\ny(x)=a(x-2)^{2}-1.\n$$\nUse the point $(-1,8)$ to find $a$:\n$$\n8=a(-1-2)^{2}-1=a\\cdot 9-1 \\implies 9a=9 \\implies a=1.\n$$\nTherefore,\n$$\ny(x)=(x-2)^{2}-1=x^{2}-4x+4-1=x^{2}-4x+3.\n$$\nA check with $x=5$ gives $25-20+3=8$, consistent with the data. Hence the quadratic is $y(x)=x^{2}-4x+3$.", "answer": "$$\\boxed{x^{2}-4x+3}$$", "id": "2181788"}, {"introduction": "With the ability to construct interpolants, a critical question arises: should we always use them? This practice confronts a common real-world challenge involving data corrupted by noise [@problem_id:3174879]. You will analyze a scenario comparing exact interpolation with polynomial regression, learning to use key metrics like cross-validation error to diagnose overfitting and select a model that generalizes well to new, unseen data.", "problem": "An engineering team is building a surrogate model for a smooth but unknown input–output relationship measured by a sensor. They collect $n = 21$ samples at equally spaced inputs $x_i \\in [-1,1]$ with outputs $y_i$. The sensor is known to have additive zero-mean noise with standard deviation approximately $0.05$. Consider two families of models built on these data:\n- Polynomial interpolation: a single polynomial that passes through all observed pairs $(x_i,y_i)$ exactly.\n- Polynomial regression: a polynomial whose coefficients are chosen to minimize the sum of squared residuals over the data.\n\nUsing the same dataset, the team computes the Root Mean Square Error (RMSE) on the training data and a $10$-fold Cross-Validation (CV) RMSE for three candidates:\n- Exact interpolating polynomial of degree $20$: training RMSE $= 0.00$, CV RMSE $= 0.31$.\n- Least-squares polynomial regression of degree $8$: training RMSE $= 0.02$, CV RMSE $= 0.18$.\n- Least-squares polynomial regression of degree $3$: training RMSE $= 0.07$, CV RMSE $= 0.09$.\n\nThe goal is to predict the output at $x = 0.7$ and to construct a model that generalizes well to unseen inputs from the same process.\n\nWhich of the following is the most defensible conclusion about when to avoid exact interpolation and prefer regression for this dataset?\n\nA. Choose the degree $20$ interpolating polynomial because it achieves zero training error, which guarantees the smallest possible prediction error at $x = 0.7$.\n\nB. Prefer the degree $3$ least-squares regression polynomial because the data are noisy and the lowest CV RMSE indicates better expected predictive performance; exact interpolation is likely to overfit the noise in the $y_i$ values.\n\nC. Prefer the degree $8$ least-squares regression polynomial because its training RMSE is smaller than that of degree $3$, so it must generalize better even if its CV RMSE is larger.\n\nD. Choose the degree $20$ interpolating polynomial because any smooth function on $[-1,1]$ is exactly a polynomial of degree at most $20$, so interpolation cannot overfit.\n\nE. Use piecewise linear interpolation through all points to avoid overfitting; because each segment is linear, exact fitting to all $y_i$ cannot amplify noise.", "solution": "The problem statement is internally consistent, scientifically grounded, and provides sufficient information for a rigorous conclusion. The scenario describes a classic model selection problem involving the bias-variance trade-off in the presence of noisy data.\n\nThe goal is to select a model that generalizes well to unseen inputs. The primary metric for evaluating generalization performance from the given data is the Cross-Validation (CV) Root Mean Square Error (RMSE). A lower CV RMSE indicates a better expected performance on new data. The training RMSE, on the other hand, measures how well a model fits the data it was trained on and is not a reliable indicator of generalization, especially when comparing models of different complexity.\n\nLet's analyze the provided data for each candidate model:\nThe dataset consists of $n=21$ samples, and the sensor noise has a standard deviation of approximately $\\sigma \\approx 0.05$. This noise level represents the inherent, irreducible error. A good model should have a prediction error close to this value.\n\n1.  **Degree $20$ Interpolating Polynomial:**\n    -   Training RMSE = $0.00$: This is expected. A polynomial of degree $n-1 = 20$ can pass through $n=21$ points exactly, meaning the error on the training data is zero by definition.\n    -   CV RMSE = $0.31$: This value is extremely high, much larger than the noise level of $\\sigma \\approx 0.05$. This is a definitive sign of severe overfitting. By forcing the polynomial through every noisy data point $(x_i, y_i)$, the model is fitting the random fluctuations (noise) in the $y_i$ values rather than the underlying smooth function. This leads to a model that oscillates wildly between data points and, consequently, has very poor predictive power for unseen data.\n\n2.  **Degree $8$ Least-Squares Regression Polynomial:**\n    -   Training RMSE = $0.02$: This error is very low, even lower than the known noise standard deviation ($\\sigma \\approx 0.05$). This suggests that the model is flexible enough to fit some of the noise in the training data, which is an indication of potential overfitting.\n    -   CV RMSE = $0.18$: While better than the interpolating polynomial, this error is still substantially higher than the noise level and the CV RMSE of the degree $3$ polynomial. This confirms that a degree $8$ polynomial is likely too complex for this dataset, leading to poor generalization.\n\n3.  **Degree $3$ Least-Squares Regression Polynomial:**\n    -   Training RMSE = $0.07$: This error is reasonably close to the noise standard deviation ($\\sigma \\approx 0.05$). This is a healthy sign, suggesting the model is not complex enough to perfectly fit the random noise, and its residual error is consistent with the inherent uncertainty in the data.\n    -   CV RMSE = $0.09$: This is the lowest CV RMSE among all three candidates. A model with the minimum cross-validation error is expected to have the best generalization performance. This value is also the closest to the noise standard deviation, reinforcing the conclusion that this model strikes the best balance between capturing the underlying trend (bias) and not fitting the noise (variance).\n\nBased on this analysis, the degree $3$ regression polynomial is the most suitable model because it has the lowest estimated generalization error (CV RMSE). The primary reason to avoid exact interpolation is its tendency to overfit noisy data, a conclusion strongly supported by its high CV RMSE.\n\nNow, we evaluate each option:\n\n**A. Choose the degree $20$ interpolating polynomial because it achieves zero training error, which guarantees the smallest possible prediction error at $x = 0.7$.**\nThis statement is incorrect. A training error of zero does not guarantee low prediction error on new data points. For noisy data, forcing zero training error, as interpolation does, is a hallmark of overfitting. The high CV RMSE ($0.31$) empirically demonstrates that this model has poor predictive performance.\n\n**B. Prefer the degree $3$ least-squares regression polynomial because the data are noisy and the lowest CV RMSE indicates better expected predictive performance; exact interpolation is likely to overfit the noise in the $y_i$ values.**\nThis statement is correct. It properly identifies that the presence of noise makes exact interpolation problematic due to overfitting. It correctly uses the lowest CV RMSE as the criterion for selecting the model with the best expected predictive (generalization) performance. This aligns perfectly with the principles of model selection.\n\n**C. Prefer the degree $8$ least-squares regression polynomial because its training RMSE is smaller than that of degree $3$, so it must generalize better even if its CV RMSE is larger.**\nThis statement is incorrect. It mistakenly uses lower training error as the criterion for better generalization. The CV RMSE, not the training RMSE, is the appropriate estimator for generalization error. Since the CV RMSE for the degree $8$ polynomial ($0.18$) is larger than that for the degree $3$ polynomial ($0.09$), the degree $3$ model is expected to generalize better.\n\n**D. Choose the degree $20$ interpolating polynomial because any smooth function on $[-1,1]$ is exactly a polynomial of degree at most $20$, so interpolation cannot overfit.**\nThis statement is incorrect on two fundamental points. First, the premise \"any smooth function on $[-1,1]$ is exactly a polynomial of degree at most $20$\" is false. Most smooth functions (e.g., trigonometric, exponential) are not polynomials. The Weierstrass Approximation Theorem only states they can be approximated arbitrarily well, not that they are identical. Second, interpolation most certainly can and does overfit, particularly in the presence of noise, as evidenced by the high CV RMSE.\n\n**E. Use piecewise linear interpolation through all points to avoid overfitting; because each segment is linear, exact fitting to all $y_i$ cannot amplify noise.**\nThis statement is incorrect. Piecewise linear interpolation is still a form of exact interpolation; it fits every data point $(x_i, y_i)$ perfectly and therefore fits all the noise. The claim that it \"cannot amplify noise\" is false. While it may not suffer from the same type of global oscillations as a high-degree polynomial (Runge's phenomenon), it creates a non-smooth model whose derivative is discontinuous, which is another manifestation of fitting noise. Regression is the correct approach to handle noise, not a different type of exact interpolation.", "answer": "$$\\boxed{B}$$", "id": "3174879"}, {"introduction": "Having seen the pitfalls of interpolating noisy data, we now focus on maximizing accuracy for smooth functions. This hands-on coding exercise explores the powerful idea that *where* we sample the function matters immensely [@problem_id:3174893]. By comparing uniform nodes to a set of nodes strategically clustered in a region where the function's derivatives are large, you will directly investigate the theoretical interpolation error formula and learn how to optimize node placement for superior results.", "problem": "Consider the function $f(x) = \\log(1+x)$ defined for $x \\in [0,1]$, where $\\log$ denotes the natural logarithm. The task is to study how the accuracy of polynomial interpolation of $f(x)$ depends on the placement of interpolation nodes, particularly when nodes are clustered near $x=0$ versus when nodes are uniformly spaced on $[0,1]$. Your investigation must be grounded in the core definition of polynomial interpolation and the standard error characterization derived from calculus, without employing any shortcut formulas or pre-specified computational expressions.\n\nYou must implement a program that constructs, for given distinct nodes $x_0, x_1, \\dots, x_N$ and values $f(x_i)$, the unique polynomial $p_N(x)$ of degree at most $N$ that interpolates $f$ at those nodes, i.e., $p_N(x_i) = f(x_i)$ for all $i \\in \\{0,1,\\dots,N\\}$. For numerical evaluation, you must compute $p_N(x)$ at a set of grid points in a specified interval by a stable method that follows from the foundational interpolation definitions, and then compare $p_N(x)$ to $f(x)$.\n\nDefine two node families for a given integer $N \\geq 1$:\n- Uniform nodes on $[0,1]$: $x_i^{\\mathrm{uni}} = \\frac{i}{N}$ for $i \\in \\{0,1,\\dots,N\\}$.\n- Clustered nodes on $[0,1]$: $x_i^{\\mathrm{clu}} = \\left(\\frac{i}{N}\\right)^p$ for $i \\in \\{0,1,\\dots,N\\}$, where $p$ is a fixed integer exponent greater than $1$. In this problem, use $p=3$.\n\nFor each test case below, construct both interpolants $p_N^{\\mathrm{uni}}(x)$ and $p_N^{\\mathrm{clu}}(x)$ for the specified $N$, evaluate them on a uniform grid of $M=1001$ points in the given interval $[a,b]$, compute the maximum absolute error in that interval for each node family,\n$$\nE^{\\mathrm{uni}} = \\max_{x \\in [a,b]} \\left| f(x) - p_N^{\\mathrm{uni}}(x) \\right|, \\quad\nE^{\\mathrm{clu}} = \\max_{x \\in [a,b]} \\left| f(x) - p_N^{\\mathrm{clu}}(x) \\right|,\n$$\nand report the ratio $R = \\frac{E^{\\mathrm{uni}}}{E^{\\mathrm{clu}}}$ as a floating-point number. The ratio $R$ quantifies the improvement in accuracy from clustering near $x=0$: values $R > 1$ indicate strictly smaller maximum error for clustered nodes on the given interval.\n\nTest suite (each tuple lists $(N, p, a, b)$):\n- Case $1$: $(4, 3, 0, 0.3)$: moderate degree, local interval near $x=0$.\n- Case $2$: $(4, 3, 0, 1)$: moderate degree, full interval.\n- Case $3$: $(8, 3, 0, 0.3)$: higher degree, local interval near $x=0$.\n- Case $4$: $(8, 3, 0, 1)$: higher degree, full interval.\n- Case $5$: $(1, 3, 0, 0.3)$: minimal degree (linear), local interval near $x=0$.\n\nYour program must produce a single line of output containing the ratios for the above cases as a comma-separated list enclosed in square brackets, in the order of the cases given. For example, the output format must be\n$[R_1,R_2,R_3,R_4,R_5]$,\nwhere each $R_k$ is a floating-point number computed as specified. No physical units or angle units are involved in this problem; all quantities are dimensionless real numbers.", "solution": "The problem as stated is a well-defined exercise in numerical analysis, specifically concerning the topic of polynomial interpolation. It is scientifically sound, self-contained, and objective. All provided data and conditions are consistent and sufficient for arriving at a unique, computable solution. The problem asks for an investigation grounded in fundamental principles, which is a standard pedagogical approach. The problem is therefore deemed valid.\n\nThe central task is to compare the interpolation accuracy for the function $f(x) = \\log(1+x)$ on the interval $[0,1]$ using two different distributions of interpolation nodes: uniform nodes and nodes clustered near $x=0$. The comparison is quantified by the ratio of maximum absolute errors on specified subintervals.\n\nA unique interpolating polynomial $p_N(x)$ of degree at most $N$ exists for any set of $N+1$ distinct nodes $\\{x_0, x_1, \\dots, x_N\\}$ and corresponding function values $\\{y_0, y_1, \\dots, y_N\\}$, where $y_i = f(x_i)$. This polynomial satisfies $p_N(x_i) = y_i$ for all $i \\in \\{0, 1, \\dots, N\\}$.\n\nA foundational representation of this polynomial is the Lagrange form:\n$$\np_N(x) = \\sum_{j=0}^{N} y_j L_j(x)\n$$\nwhere $L_j(x)$ are the Lagrange basis polynomials, defined as:\n$$\nL_j(x) = \\prod_{k=0, k \\neq j}^{N} \\frac{x - x_k}{x_j - x_k}\n$$\nWhile this formula is fundamental, its direct evaluation for multiple points $x$ is computationally expensive (each evaluation is $O(N^2)$) and can be numerically unstable. A more robust method for evaluation is derived from the Lagrange form, known as the barycentric interpolation formula.\n\nFirst, we define the nodal polynomial $\\ell(x) = \\prod_{k=0}^{N} (x-x_k)$ and the barycentric weights $w_j$:\n$$\nw_j = \\frac{1}{\\prod_{k=0, k \\neq j}^{N} (x_j - x_k)}\n$$\nThe Lagrange basis polynomial can be rewritten in terms of these quantities:\n$$\nL_j(x) = \\ell(x) \\frac{w_j}{x-x_j}\n$$\nSubstituting this into the Lagrange formula gives the first barycentric form:\n$$\np_N(x) = \\ell(x) \\sum_{j=0}^{N} \\frac{w_j}{x - x_j} y_j\n$$\nTo simplify this further and improve stability, we consider interpolating the constant function $g(x)=1$. The interpolating polynomial must be $g(x)$ itself, so $1 = \\sum_{j=0}^{N} L_j(x)$. Substituting the barycentric expression for $L_j(x)$ yields:\n$$\n1 = \\ell(x) \\sum_{j=0}^{N} \\frac{w_j}{x - x_j}\n$$\nDividing the first barycentric formula for $p_N(x)$ by this identity (which is valid for all $x$ not equal to a node), we obtain the second (or \"true\") barycentric form:\n$$\np_N(x) = \\frac{\\sum_{j=0}^{N} \\frac{w_j}{x - x_j} y_j}{\\sum_{j=0}^{N} \\frac{w_j}{x - x_j}}\n$$\nThis formula is ideal for computation. The weights $w_j$ are computed once for a given set of nodes (an $O(N^2)$ process). Then, evaluating $p_N(x)$ at any point $x$ takes only $O(N)$ operations. If the evaluation point $x$ coincides with a node $x_k$, the formula is analytically undefined, but the value of the interpolant is known to be $p_N(x_k)=y_k$. This method is derived from core principles and is numerically stable.\n\nThe theoretical error of polynomial interpolation is given by:\n$$\nf(x) - p_N(x) = \\frac{f^{(N+1)}(\\xi_x)}{(N+1)!} \\prod_{i=0}^{N} (x-x_i)\n$$\nfor some $\\xi_x$ in the smallest interval containing the nodes and $x$. For the function $f(x) = \\log(1+x)$, the derivatives are:\n$$\nf'(x) = (1+x)^{-1}, \\quad f''(x) = -(1+x)^{-2}, \\quad \\dots, \\quad f^{(k)}(x) = (-1)^{k-1} (k-1)! (1+x)^{-k}\n$$\nThe $(N+1)$-th derivative is $f^{(N+1)}(x) = (-1)^N N! (1+x)^{-(N+1)}$. The magnitude of this derivative, $|f^{(N+1)}(x)| = N!(1+x)^{-(N+1)}$, is largest near $x=0$ and decreases as $x$ increases. To minimize the overall error, the nodal polynomial term $\\prod_{i=0}^{N} (x-x_i)$ should be kept small, especially where the derivative term is large. The clustered nodes, $x_i^{\\mathrm{clu}} = (i/N)^p$ with $p=3$, are denser near $x=0$ compared to uniform nodes, $x_i^{\\mathrm{uni}}=i/N$. This clustering is designed to reduce the magnitude of the nodal polynomial near $x=0$, thereby reducing the interpolation error in this region.\n\nThe computational procedure for each test case $(N, p, a, b)$ is as follows:\n$1$. Define the function $f(x) = \\log(1+x)$.\n$2$. Generate the two sets of $N+1$ interpolation nodes on $[0,1]$:\n    - Uniform nodes: $x_i^{\\mathrm{uni}} = \\frac{i}{N}$ for $i \\in \\{0, \\dots, N\\}$.\n    - Clustered nodes: $x_i^{\\mathrm{clu}} = \\left(\\frac{i}{N}\\right)^p$ for $i \\in \\{0, \\dots, N\\}$, with $p=3$.\n$3$. For each node set, compute the corresponding function values, e.g., $y_i^{\\mathrm{uni}} = f(x_i^{\\mathrm{uni}})$.\n$4$. For each node set, compute the barycentric weights $w_j$ using the $O(N^2)$ direct formula.\n$5$. Create a fine, uniform evaluation grid of $M=1001$ points, $z_k$, on the interval $[a,b]$.\n$6$. For each node set, evaluate the corresponding interpolating polynomial ($p_N^{\\mathrm{uni}}(x)$ and $p_N^{\\mathrm{clu}}(x)$) at each point $z_k$ on the evaluation grid using the second barycentric formula.\n$7$. Evaluate the true function $f(x)$ at each point $z_k$.\n$8$. Compute the maximum absolute error for each case over the grid:\n    $$\n    E^{\\mathrm{uni}} = \\max_{k} |f(z_k) - p_N^{\\mathrm{uni}}(z_k)|, \\quad E^{\\mathrm{clu}} = \\max_{k} |f(z_k) - p_N^{\\mathrm{clu}}(z_k)|\n    $$\n$9$. Compute and record the ratio $R = E^{\\mathrm{uni}} / E^{\\mathrm{clu}}$.\nFor the special case $N=1$, both node distributions are identical: $x_0^{\\mathrm{uni}} = x_0^{\\mathrm{clu}} = 0$ and $x_1^{\\mathrm{uni}} = x_1^{\\mathrm{clu}} = 1$. This implies that the interpolating polynomials are identical, leading to an expected ratio $R=1$. This serves as a useful sanity check.", "answer": "```python\nimport numpy as np\n\ndef get_barycentric_weights(nodes: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Computes barycentric weights for a given set of nodes.\n    w_j = 1 / product(x_j - x_k) for k != j.\n    \"\"\"\n    n_nodes = len(nodes)\n    weights = np.ones(n_nodes)\n    for j in range(n_nodes):\n        # The product can be calculated more stably in log space for large N,\n        # but for small N direct product is fine.\n        prod = 1.0\n        for k in range(n_nodes):\n            if k != j:\n                prod *= (nodes[j] - nodes[k])\n        weights[j] = 1.0 / prod\n    return weights\n\ndef evaluate_barycentric(nodes: np.ndarray, y_values: np.ndarray, weights: np.ndarray, eval_points: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Evaluates the interpolating polynomial at given points using the barycentric formula.\n    \"\"\"\n    poly_values = np.zeros_like(eval_points, dtype=np.float64)\n\n    for i, x in enumerate(eval_points):\n        # Check if evaluation point is one of the nodes to avoid division by zero\n        close_nodes_mask = np.isclose(x, nodes)\n        if np.any(close_nodes_mask):\n            node_idx = np.where(close_nodes_mask)[0][0]\n            poly_values[i] = y_values[node_idx]\n            continue\n        \n        # Barycentric formula (second form)\n        terms = weights / (x - nodes)\n        numerator = np.sum(terms * y_values)\n        denominator = np.sum(terms)\n        \n        # The denominator can be zero only if the polynomial degree is higher\n        # than N, which shouldn't happen for interpolation.\n        if denominator == 0:\n            # This case is unlikely but we handle it. Could happen with catastrophic cancellation.\n            # Find the closest node and return its value.\n            closest_node_idx = np.argmin(np.abs(x - nodes))\n            poly_values[i] = y_values[closest_node_idx]\n        else:\n            poly_values[i] = numerator / denominator\n            \n    return poly_values\n\ndef calculate_error_ratio(N: int, p: int, a: float, b: float, M: int = 1001) -> float:\n    \"\"\"\n    Calculates the ratio of max interpolation errors for uniform vs. clustered nodes.\n    \"\"\"\n    # The function to interpolate\n    func = np.log1p\n\n    # --- Uniform nodes ---\n    x_uni = np.linspace(0.0, 1.0, N + 1)\n    y_uni = func(x_uni)\n    w_uni = get_barycentric_weights(x_uni)\n\n    # --- Clustered nodes ---\n    # For N=1, (i/N)^p gives [0, 1], same as uniform.\n    if N > 0:\n        base_nodes = np.linspace(0.0, 1.0, N + 1)\n        x_clu = np.power(base_nodes, p)\n    else: # N=0, single node at 0\n        x_clu = np.array([0.0])\n    \n    y_clu = func(x_clu)\n    # If nodes are identical, ratio is 1.\n    if np.allclose(x_uni, x_clu):\n        return 1.0\n    w_clu = get_barycentric_weights(x_clu)\n\n    # --- Evaluation ---\n    eval_grid = np.linspace(a, b, M)\n    f_true = func(eval_grid)\n\n    # Evaluate uniform interpolant and its error\n    p_uni = evaluate_barycentric(x_uni, y_uni, w_uni, eval_grid)\n    error_uni = np.max(np.abs(f_true - p_uni))\n\n    # Evaluate clustered interpolant and its error\n    p_clu = evaluate_barycentric(x_clu, y_clu, w_clu, eval_grid)\n    error_clu = np.max(np.abs(f_true - p_clu))\n\n    # Avoid division by zero if clustered error is zero\n    if error_clu == 0.0:\n        # If uniform error is also zero, they are equally good (ratio 1).\n        # If uniform error is non-zero, clustered is infinitely better.\n        return 1.0 if error_uni == 0.0 else np.inf\n\n    return error_uni / error_clu\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        # (N, p, a, b)\n        (4, 3, 0.0, 0.3),\n        (4, 3, 0.0, 1.0),\n        (8, 3, 0.0, 0.3),\n        (8, 3, 0.0, 1.0),\n        (1, 3, 0.0, 0.3),\n    ]\n\n    results = []\n    for case in test_cases:\n        N, p, a, b = case\n        ratio = calculate_error_ratio(N, p, a, b)\n        results.append(ratio)\n\n    # Format the final output as specified\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3174893"}]}