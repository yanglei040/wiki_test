{"hands_on_practices": [{"introduction": "Let's begin by confronting the practical consequences of ill-conditioning head-on. This first exercise makes the abstract threat of ill-conditioning tangible, demonstrating how the choice of numerical precision can dramatically affect the accuracy of a solution for a sensitive system [@problem_id:3141607]. By solving the same system using both single-precision ($32$-bit) and double-precision ($64$-bit) floating-point numbers, you will gain a direct understanding of how ill-conditioned matrices amplify small rounding errors.", "problem": "You are to write a complete program that empirically studies the numerical behavior of solving the linear system $A x = b$ when $A$ is ill-conditioned, by comparing solutions computed in single precision and double precision. The goal is to determine, for each test case, the minimal precision needed to achieve a specified target relative error in the solution $x$. Your study must start from the following foundational base:\n\n- The definition of a linear system $A x = b$, where $A$ is a square matrix, $x$ is a vector of unknowns, and $b$ is a known right-hand side vector.\n- The concept of floating-point arithmetic and its rounding behavior, where operations are modeled as $\\mathrm{fl}(a \\,\\circ\\, b) = (a \\,\\circ\\, b) (1 + \\delta)$ with $|\\delta| \\le u$, and $u$ is the unit roundoff of the floating-point format.\n- The definition of matrix condition number in a given norm, $\\kappa(A) = \\|A\\| \\cdot \\|A^{-1}\\|$, and the understanding that larger $\\kappa(A)$ implies greater sensitivity of the solution $x$ to perturbations in $A$ and $b$.\n\nYour program must implement the following tasks, expressed purely in mathematical and algorithmic terms:\n\n1. For each test case with dimension $n$ and target tolerance $\\tau$, construct the Hilbert matrix $A \\in \\mathbb{R}^{n \\times n}$ defined by $A_{ij} = \\frac{1}{i + j - 1}$ for $1 \\le i,j \\le n$. The Hilbert matrix is a classical example of an ill-conditioned matrix.\n2. Choose a ground-truth solution $x^\\star \\in \\mathbb{R}^n$ as the vector of ones, i.e., $x^\\star = (1,1,\\dots,1)^\\top$. Compute $b = A x^\\star$ in double precision to establish a reference right-hand side.\n3. Solve $A x = b$ twice:\n   - Once in single precision (floating-point $32$-bit), by casting $A$ and $b$ to single precision and using a standard direct solver.\n   - Once in double precision (floating-point $64$-bit), by casting $A$ and $b$ to double precision and using the same solver.\n4. For each precision, compute the relative forward error in the solution,\n   $$\n   e_{\\mathrm{rel}} = \\frac{\\|x_{\\mathrm{computed}} - x^\\star\\|_2}{\\|x^\\star\\|_2}.\n   $$\n5. Determine the minimal precision that achieves the target tolerance $\\tau$: if the single-precision relative error is less than or equal to $\\tau$, choose $32$; else if the double-precision relative error is less than or equal to $\\tau$, choose $64$; otherwise, if neither meets the tolerance, output $-1$ to indicate that neither tested precision achieves the required accuracy under the given ill-conditioning.\n\nYour program must use the following test suite of parameter values $(n, \\tau)$ designed to probe different aspects of numerical stability:\n\n- A general case where single precision suffices: $(n, \\tau) = (3, 10^{-4})$.\n- A case where single precision fails but double precision succeeds: $(n, \\tau) = (8, 10^{-5})$.\n- A strongly ill-conditioned case where even double precision fails to meet a strict tolerance: $(n, \\tau) = (12, 10^{-8})$.\n- A boundary case with moderate ill-conditioning but extremely tight tolerance: $(n, \\tau) = (5, 10^{-12})$.\n\nThere are no physical units or angle units in this problem. All numerical tolerances, dimensions, and outputs are dimensionless real numbers.\n\nFinal output format specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the same order as the test cases above. Each result must be an integer among $\\{32, 64, -1\\}$, representing the minimal precision required.\n- For example, an output could look like $\\left[32,64,-1,-1\\right]$, but must reflect the actual outcomes computed by your program for the specified test cases.", "solution": "The problem requires an empirical investigation into the effects of floating-point precision on the solution of ill-conditioned linear systems. Specifically, we are tasked with determining the minimum floating-point precision—either $32$-bit (single precision) or $64$-bit (double precision)—required to solve the system $A x = b$ to within a specified relative error tolerance $\\tau$, where $A$ is a Hilbert matrix.\n\nThe process for each test case, defined by a dimension $n$ and a tolerance $\\tau$, is as follows.\n\n**Step 1: System Construction**\nFirst, we construct the components of the linear system $A x = b$. The matrix $A$ is an $n \\times n$ Hilbert matrix, whose entries are defined by the formula:\n$$\nA_{ij} = \\frac{1}{i + j - 1} \\quad \\text{for } 1 \\le i, j \\le n\n$$\nHilbert matrices are notoriously ill-conditioned, meaning their condition number $\\kappa(A) = \\|A\\| \\|A^{-1}\\|$ grows very rapidly with the dimension $n$. This property makes them excellent candidates for studying numerical instability.\n\nThe ground-truth solution, denoted $x^\\star$, is defined as a vector of ones of dimension $n$:\n$$\nx^\\star = (1, 1, \\dots, 1)^\\top \\in \\mathbb{R}^n\n$$\nAll elements of this vector are exactly representable in standard floating-point formats.\n\n**Step 2: Right-Hand Side Vector Calculation**\nUsing the defined $A$ and $x^\\star$, we compute the right-hand side vector $b = A x^\\star$. To ensure that $b$ is as accurate as possible and serves as a reliable reference, this calculation is performed in double precision ($64$-bit floating-point arithmetic). This minimizes the introduction of error into the problem setup itself; any significant errors observed later can then be more confidently attributed to the subsequent solution process. Let us denote the double-precision matrix as $A_{64}$ and the resulting vector as $b_{64}$.\n\n**Step 3: Solution in Single Precision**\nTo simulate a lower-precision computation, we first cast the double-precision matrix $A_{64}$ and vector $b_{64}$ into single precision ($32$-bit) format. Let these be $A_{32}$ and $b_{32}$. The conversion process itself can introduce rounding errors, especially for the entries of the Hilbert matrix which are fractions.\n\nWe then solve the linear system $A_{32} x_{32} = b_{32}$ for the unknown vector $x_{32}$ using a standard numerical solver. The resulting vector, $x_{32}$, is the solution computed in single precision.\n\nNext, we evaluate the accuracy of this solution by computing the relative forward error, $e_{32}$. The error is measured with respect to the known ground-truth solution $x^\\star$ using the Euclidean ($L_2$) norm:\n$$\ne_{32} = \\frac{\\|x_{32} - x^\\star\\|_2}{\\|x^\\star\\|_2}\n$$\nThe norm of the true solution, $\\|x^\\star\\|_2$, is simply $\\sqrt{n}$ since all its components are $1$.\n\n**Step 4: Solution in Double Precision**\nWe repeat the solution process, but this time entirely in double precision. We solve the system $A_{64} x_{64} = b_{64}$ for the vector $x_{64}$. This computation benefits from a larger mantissa and smaller unit roundoff compared to the single-precision case, which is expected to yield a more accurate result for an ill-conditioned system.\n\nSimilarly, we compute the relative forward error for the double-precision solution:\n$$\ne_{64} = \\frac{\\|x_{64} - x^\\star\\|_2}{\\|x^\\star\\|_2}\n$$\n\n**Step 5: Minimal Precision Determination**\nThe final step is to compare the computed errors, $e_{32}$ and $e_{64}$, against the user-specified tolerance $\\tau$. The decision logic is as follows:\n- If $e_{32} \\le \\tau$, single precision is deemed sufficient. The result for the test case is $32$.\n- If $e_{32} > \\tau$ but $e_{64} \\le \\tau$, single precision fails while double precision succeeds. The result is $64$.\n- If neither precision achieves the target tolerance (i.e., $e_{32} > \\tau$ and $e_{64} > \\tau$), then both tested precisions are inadequate for the given level of ill-conditioning and the strictness of the tolerance. The result is $-1$.\n\nThis procedure is systematically applied to each $(n, \\tau)$ pair in the provided test suite, and the sequence of results is reported. The test cases are specifically chosen to probe scenarios where the condition number of the $n \\times n$ Hilbert matrix is low enough for single precision to suffice, high enough to require double precision, or so high that even double precision fails to meet a stringent tolerance.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Empirically studies the numerical behavior of solving ill-conditioned\n    linear systems by comparing single and double precision results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (3, 1e-4),    # General case, single precision should suffice.\n        (8, 1e-5),    # Single precision fails, double precision succeeds.\n        (12, 1e-8),   # Strongly ill-conditioned, even double precision fails.\n        (5, 1e-12),   # Moderate ill-conditioning, very tight tolerance.\n    ]\n\n    results = []\n    for n, tau in test_cases:\n        # Step 1: Construct the n x n Hilbert matrix A and ground-truth solution x_star.\n        # All initial constructions are done in double precision (float64) to maintain accuracy.\n        \n        # Create the ground-truth solution vector x_star = [1, 1, ..., 1]^T.\n        x_star = np.ones(n, dtype='float64')\n\n        # Construct the Hilbert matrix A_ij = 1 / (i + j - 1).\n        # We use 'i' and 'j' indices from 1 to n.\n        i = np.arange(1, n + 1, dtype='float64').reshape(-1, 1)\n        j = np.arange(1, n + 1, dtype='float64').reshape(1, -1)\n        A_64 = 1.0 / (i + j - 1)\n\n        # Step 2: Compute the right-hand side b = A * x_star in double precision.\n        b_64 = A_64 @ x_star\n\n        # Step 3: Solve in single precision (32-bit).\n        # Cast the matrix and vector to float32.\n        A_32 = A_64.astype('float32')\n        b_32 = b_64.astype('float32')\n        \n        # Solve the system Ax = b.\n        x_32_computed = np.linalg.solve(A_32, b_32)\n        \n        # Compute the relative forward error for the single-precision solution.\n        # The norm of x_star is sqrt(n).\n        norm_x_star = np.linalg.norm(x_star)\n        err_32 = np.linalg.norm(x_32_computed - x_star) / norm_x_star\n\n        # Step 4: Solve in double precision (64-bit).\n        # The matrix and vector are already in float64.\n        x_64_computed = np.linalg.solve(A_64, b_64)\n\n        # Compute the relative forward error for the double-precision solution.\n        err_64 = np.linalg.norm(x_64_computed - x_star) / norm_x_star\n        \n        # Step 5: Determine the minimal precision needed.\n        if err_32 <= tau:\n            results.append(32)\n        elif err_64 <= tau:\n            results.append(64)\n        else:\n            results.append(-1)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3141607"}, {"introduction": "Having observed the dangers of ill-conditioning, we now turn to proactive solutions. This practice explores regularization, a class of techniques designed to stabilize ill-posed problems by introducing additional information or constraints [@problem_id:3141553]. You will implement two common augmentation strategies—Tikhonov regularization and the addition of new measurements—to methodically improve a system's condition number and analyze the important concept of diminishing returns.", "problem": "Consider a linear system represented by a real matrix $A \\in \\mathbb{R}^{m \\times n}$. The numerical sensitivity of solving linear systems or least-squares problems involving $A$ can be quantified using the condition number in the Euclidean operator norm (also called the $2$-norm), defined for a full-rank matrix by $\\kappa_2(A) = \\dfrac{\\sigma_{\\max}(A)}{\\sigma_{\\min}(A)}$, where $\\sigma_{\\max}(A)$ and $\\sigma_{\\min}(A)$ denote the largest and smallest singular values from the singular value decomposition (SVD) of $A$. In many computational workflows, $A$ is augmented with additional constraints or measurements to improve conditioning. Two common augmentation strategies are:\n- Additional measurements: appending new measurement rows to $A$, forming a taller matrix.\n- Quadratic constraints (Tikhonov regularization): appending weighted identity rows $\\sqrt{\\lambda} I$ to $A$, which is equivalent to imposing a quadratic penalty on the solution.\n\nFrom the fundamental definitions of the singular value decomposition (SVD) and the condition number, augmentations alter the singular values of $A$ by changing its row space alignment and by lifting small singular values through constraints. However, improvements in $\\kappa_2(A)$ often exhibit diminishing returns: after a certain step, each further augmentation yields a smaller relative improvement.\n\nWrite a program that, for each given test case, performs the following steps:\n1. Construct the base matrix $A$ exactly as specified.\n2. Build a sequence of augmented matrices $\\{A^{(k)}\\}$, starting with the unaugmented $A^{(0)} = A$ and then applying augmentation steps indexed by $k = 1, 2, \\dots, K$ according to the test case definition. For measurement augmentation, form $A^{(k)}$ by vertically stacking the first $k$ specified new measurement rows onto $A$. For quadratic constraint augmentation with parameter $\\lambda$, form\n$$\nA_{\\text{aug}}(\\lambda) = \\begin{bmatrix} A \\\\ \\sqrt{\\lambda} I_n \\end{bmatrix},\n$$\nand define the sequence by the provided list of $\\lambda$ values, with $A^{(0)}$ corresponding to $\\lambda = 0$ and subsequent $A^{(k)}$ corresponding to the $k$-th nonzero $\\lambda$ in the list.\n3. For each $A^{(k)}$, compute $\\kappa_2(A^{(k)}) = \\dfrac{\\sigma_{\\max}(A^{(k)})}{\\sigma_{\\min}(A^{(k)})}$ using the singular value decomposition. If $\\sigma_{\\min}(A^{(k)}) = 0$, treat $\\kappa_2(A^{(k)})$ as $+\\infty$.\n4. Compute the stepwise relative improvements $r_k$ for $k = 1, \\dots, K$ by\n$$\nr_k = \\max\\left(0, \\dfrac{\\kappa_2\\left(A^{(k-1)}\\right) - \\kappa_2\\left(A^{(k)}\\right)}{\\kappa_2\\left(A^{(k-1)}\\right)}\\right).\n$$\n5. Using a threshold $\\tau$ provided in the test case, define the diminishing returns index $D$ for the sequence as the smallest augmentation step index $i \\in \\{1, \\dots, K\\}$ such that $r_j < \\tau$ holds for all $j \\in \\{i, i+1, \\dots, K\\}$. If no such index exists, set $D = K$.\n\nYour program must process the following test suite and output the diminishing returns index $D$ for each case:\n\n- Test Case $1$ (happy path, strongly ill-conditioned square matrix with Tikhonov augmentation):\n  - Base matrix $A \\in \\mathbb{R}^{5 \\times 5}$ is the Hilbert matrix with entries $A_{ij} = \\dfrac{1}{i + j - 1}$ for $i, j \\in \\{1, 2, 3, 4, 5\\}$.\n  - Augmentation strategy: quadratic constraints with identity regularization. Use the sequence of $\\lambda$ values $\\left[0, 10^{-8}, 10^{-6}, 10^{-4}, 10^{-2}, 10^{-1}, 1\\right]$.\n  - Threshold: $\\tau = 0.02$.\n\n- Test Case $2$ (edge case, nearly collinear columns with redundant measurements):\n  - Base matrix $A \\in \\mathbb{R}^{5 \\times 3}$ with columns\n    $$\n    c_1 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix},\\quad\n    c_2 = c_1 + 10^{-4} \\begin{bmatrix} 1 \\\\ -1 \\\\ 1 \\\\ -1 \\\\ 1 \\end{bmatrix},\\quad\n    c_3 = c_1 + 2 \\cdot 10^{-4} \\begin{bmatrix} -1 \\\\ 1 \\\\ -1 \\\\ 1 \\\\ -1 \\end{bmatrix}.\n    $$\n    Form $A = [c_1\\ c_2\\ c_3]$.\n  - Augmentation strategy: additional measurements. Append the following measurement rows one-by-one to $A$ to form $A^{(k)}$ for $k = 1, 2, 3, 4$:\n    $$\n    r_1 = \\begin{bmatrix} 1 & 1 & 1 \\end{bmatrix},\\quad\n    r_2 = \\begin{bmatrix} 1 + 10^{-6} & 1 - 10^{-6} & 1 \\end{bmatrix},\\quad\n    r_3 = \\begin{bmatrix} 1 - 2 \\cdot 10^{-6} & 1 + 2 \\cdot 10^{-6} & 1 \\end{bmatrix},\\quad\n    r_4 = \\begin{bmatrix} 1 + 5 \\cdot 10^{-7} & 1 + 5 \\cdot 10^{-7} & 1 + 5 \\cdot 10^{-7} \\end{bmatrix}.\n    $$\n  - Threshold: $\\tau = 0.02$.\n\n- Test Case $3$ (boundary case, already well-conditioned with near-orthonormal columns):\n  - Base matrix $A \\in \\mathbb{R}^{6 \\times 4}$ defined by\n    $$\n    A = \\begin{bmatrix}\n    1 & 0 & 0 & 0 \\\\\n    0 & 1 & 0 & 0 \\\\\n    0 & 0 & 1 & 0 \\\\\n    0 & 0 & 0 & 1 \\\\\n    0 & 0 & 0 & 0 \\\\\n    0 & 0 & 0 & 0\n    \\end{bmatrix}.\n    $$\n    The columns of $A$ are orthonormal in $\\mathbb{R}^6$.\n  - Augmentation strategy: additional measurements. Append the following measurement rows one-by-one to $A$ to form $A^{(k)}$ for $k = 1, 2, 3$:\n    $$\n    r_1 = \\begin{bmatrix} 1 & 0 & 0 & 0 \\end{bmatrix},\\quad\n    r_2 = \\begin{bmatrix} 0 & 1 & 0 & 0 \\end{bmatrix},\\quad\n    r_3 = \\begin{bmatrix} 0 & 0 & 1 & 0 \\end{bmatrix}.\n    $$\n  - Threshold: $\\tau = 0.02$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the diminishing returns indices for the three test cases as a comma-separated list enclosed in square brackets, for example $[d_1,d_2,d_3]$, where each $d_i$ is the integer diminishing returns index for Test Case $i$ according to the rule above. No physical units are involved, and no angles are used. All outputs are integers.", "solution": "The problem is valid as it is scientifically grounded in numerical linear algebra, well-posed with all necessary definitions and data, and objective in its formulation. The task is to analyze the diminishing returns of matrix augmentation on the condition number for three specific cases.\n\nThe core of this problem lies in understanding the condition number, $\\kappa_2(A)$, of a matrix $A \\in \\mathbb{R}^{m \\times n}$. The condition number quantifies the sensitivity of the solution of a linear system $Ax=b$ to perturbations in $A$ or $b$. A large condition number signifies an ill-conditioned problem, where small input errors can lead to large output errors. For a full-rank matrix, it is defined as the ratio of the largest to the smallest singular values:\n$$\n\\kappa_2(A) = \\frac{\\sigma_{\\max}(A)}{\\sigma_{\\min}(A)}\n$$\nThe singular values are obtained from the Singular Value Decomposition (SVD) of $A$. If $\\sigma_{\\min}(A) = 0$, the matrix is rank-deficient, and the condition number is considered to be infinite.\n\nThe problem investigates two common techniques for improving conditioning (i.e., reducing $\\kappa_2(A)$):\n1.  **Tikhonov Regularization**: This method transforms the original matrix $A$ into an augmented matrix $A_{\\text{aug}}(\\lambda)$:\n    $$\n    A_{\\text{aug}}(\\lambda) = \\begin{bmatrix} A \\\\ \\sqrt{\\lambda} I_n \\end{bmatrix}\n    $$\n    where $I_n$ is the $n \\times n$ identity matrix and $\\lambda > 0$ is the regularization parameter. This is equivalent to solving a modified least-squares problem $\\min_x \\|Ax-b\\|_2^2 + \\lambda \\|x\\|_2^2$. This augmentation directly affects the singular values. The singular values of $A_{\\text{aug}}(\\lambda)$, let's call them $\\sigma_i'$, are related to the singular values of $A$, $\\sigma_i$, by $\\sigma_i' = \\sqrt{\\sigma_i^2 + \\lambda}$. This transformation \"lifts\" all singular values, but has the most pronounced effect on the smallest ones, thus typically reducing the ratio $\\sigma_{\\max}'/\\sigma_{\\min}'$ and improving the condition number.\n\n2.  **Additional Measurements**: This method involves appending new measurement rows to the matrix $A$, forming a taller matrix. Each new row provides additional constraints on the solution vector $x$. If the new information is not redundant, it can help disambiguate near-linear dependencies among the columns of $A$, which are the source of ill-conditioning. This can increase the smaller singular values, thereby improving the condition number.\n\nThe algorithm to be implemented follows these steps for each test case:\n1.  **Matrix Construction**: The base matrix $A$ is constructed as specified. Let its dimensions be $m \\times n$.\n2.  **Sequential Augmentation**: A sequence of matrices $\\{A^{(k)}\\}_{k=0}^K$ is generated. $A^{(0)}$ is the base matrix $A$.\n    - For Tikhonov regularization, the sequence $A^{(k)}$ for $k > 0$ corresponds to using the $k$-th non-zero $\\lambda$ value from the provided list to form $A_{\\text{aug}}(\\lambda)$.\n    - For additional measurements, $A^{(k)}$ is formed by vertically stacking the first $k$ new measurement rows onto the base matrix $A$.\n3.  **Condition Number Calculation**: For each matrix $A^{(k)}$ in the sequence, its singular values are computed. The condition number $\\kappa_2(A^{(k)})$ is calculated as $\\sigma_{\\max}(A^{(k)}) / \\sigma_{\\min}(A^{(k)})$. If $\\sigma_{\\min}(A^{(k)})$ is computationally indistinguishable from zero, $\\kappa_2(A^{(k)})$ is treated as infinite.\n4.  **Relative Improvement Calculation**: For each augmentation step from $k=1$ to $K$, the stepwise relative improvement $r_k$ is computed using the formula:\n    $$\n    r_k = \\max\\left(0, \\frac{\\kappa_2(A^{(k-1)}) - \\kappa_2(A^{(k)})}{\\kappa_2(A^{(k-1)})}\\right)\n    $$\n    Special care is taken for cases where $\\kappa_2(A^{(k-1)})$ is infinite. If $\\kappa_2(A^{(k-1)}) = \\infty$ and $\\kappa_2(A^{(k)})$ is finite, the improvement is maximal, so $r_k=1$. If both are infinite, there is no improvement, so $r_k=0$. The $\\max(0, \\dots)$ component ensures that if an augmentation worsens the conditioning, the relative improvement is recorded as $0$.\n5.  **Diminishing Returns Index Determination**: The final step is to find the diminishing returns index, $D$. This is defined as the smallest augmentation step index $i \\in \\{1, \\dots, K\\}$ such that all subsequent relative improvements $r_j$ (for $j \\ge i$) are below a given threshold $\\tau$. If no such index $i$ exists, $D$ is set to $K$. This is found by iterating $i$ from $1$ to $K$ and, for each $i$, checking if the condition $r_j  \\tau$ holds for all $j \\in \\{i, i+1, \\dots, K\\}$. The first $i$ for which this is true is the result. If the loop completes without finding such an $i$, the default value $D=K$ is used.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import hilbert\n\ndef compute_condition_number(matrix):\n    \"\"\"\n    Computes the 2-norm condition number of a matrix using SVD.\n    Returns np.inf for rank-deficient matrices.\n    \"\"\"\n    try:\n        singular_values = np.linalg.svd(matrix, compute_uv=False)\n        s_min = np.min(singular_values)\n        if np.isclose(s_min, 0):\n            return np.inf\n        s_max = np.max(singular_values)\n        return s_max / s_min\n    except np.linalg.LinAlgError:\n        return np.inf\n\ndef solve():\n    \"\"\"\n    Processes all test cases and calculates the diminishing returns index for each.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"type\": \"tikhonov\",\n            \"A_base_gen\": lambda: hilbert(5),\n            \"params\": [0, 1e-8, 1e-6, 1e-4, 1e-2, 1e-1, 1],\n            \"threshold\": 0.02\n        },\n        {\n            \"type\": \"measurement\",\n            \"A_base_gen\": lambda: np.column_stack([\n                np.ones(5),\n                np.ones(5) + 1e-4 * np.array([1, -1, 1, -1, 1]),\n                np.ones(5) + 2e-4 * np.array([-1, 1, -1, 1, -1])\n            ]),\n            \"params\": [\n                np.array([1, 1, 1]),\n                np.array([1 + 1e-6, 1 - 1e-6, 1]),\n                np.array([1 - 2e-6, 1 + 2e-6, 1]),\n                np.array([1 + 5e-7, 1 + 5e-7, 1 + 5e-7])\n            ],\n            \"threshold\": 0.02\n        },\n        {\n            \"type\": \"measurement\",\n            \"A_base_gen\": lambda: np.vstack([np.eye(4), np.zeros((2, 4))]),\n            \"params\": [\n                np.array([1, 0, 0, 0]),\n                np.array([0, 1, 0, 0]),\n                np.array([0, 0, 1, 0])\n            ],\n            \"threshold\": 0.02\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        A_base = case[\"A_base_gen\"]()\n        m_base, n_base = A_base.shape\n        params = case[\"params\"]\n        tau = case[\"threshold\"]\n\n        # Step 2: Build sequence of augmented matrices and compute condition numbers\n        condition_numbers = []\n        if case[\"type\"] == \"tikhonov\":\n            # The list of lambdas defines the sequence of matrices A^(k)\n            # A^(0) is for lambda=0, A^(1) for the next lambda, etc.\n            K = len(params) - 1\n            for lam in params:\n                if lam == 0:\n                    A_aug = A_base\n                else:\n                    reg_term = np.sqrt(lam) * np.eye(n_base)\n                    A_aug = np.vstack([A_base, reg_term])\n                condition_numbers.append(compute_condition_number(A_aug))\n        else: # measurement\n            # The list of rows defines the augmentations for A^(k), k=1..K\n            K = len(params)\n            # A^(0) is the base matrix\n            condition_numbers.append(compute_condition_number(A_base))\n            A_current = A_base\n            for row_to_append in params:\n                A_current = np.vstack([A_current, row_to_append])\n                condition_numbers.append(compute_condition_number(A_current))\n\n        # Step 4: Compute stepwise relative improvements\n        relative_improvements = []\n        for k in range(1, K + 1):\n            kappa_prev = condition_numbers[k-1]\n            kappa_curr = condition_numbers[k]\n            \n            if np.isinf(kappa_prev):\n                r_k = 0.0 if np.isinf(kappa_curr) else 1.0\n            elif kappa_prev  0:\n                r_k = (kappa_prev - kappa_curr) / kappa_prev\n            else: # kappa_prev is 0 or NaN, should not happen for a matrix\n                r_k = 0.0\n            \n            relative_improvements.append(max(0, r_k))\n\n        # Step 5: Find the diminishing returns index D\n        # Default value per problem spec\n        D = K if K  0 else 0 \n        if K  0:\n            for i in range(1, K + 1):\n                # Check if r_j  tau for all j from i to K\n                is_diminishing = True\n                for j in range(i, K + 1):\n                    # relative_improvements is 0-indexed, so r_j is at index j-1\n                    if relative_improvements[j - 1] = tau:\n                        is_diminishing = False\n                        break\n                if is_diminishing:\n                    D = i\n                    break\n        \n        results.append(D)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3141553"}, {"introduction": "Regularization provides a powerful fix for ill-conditioning, but this stability comes at a price. This final exercise delves into the fundamental bias-variance trade-off that underpins such methods, connecting numerical linear algebra with core statistical principles [@problem_id:3141620]. Using the Truncated Singular Value Decomposition (SVD), you will quantitatively analyze how reducing noise sensitivity (variance) systematically shifts the solution away from the true value (bias), revealing the nature of stabilized solutions.", "problem": "Consider the linear model $y = A x_{\\mathrm{true}} + \\varepsilon$, where $A \\in \\mathbb{R}^{n \\times n}$, $x_{\\mathrm{true}} \\in \\mathbb{R}^{n}$ is an unknown vector, and $\\varepsilon \\in \\mathbb{R}^{n}$ represents measurement noise with independent and identically distributed components of zero mean and variance $\\sigma_{\\varepsilon}^2$. Let $A$ be factorized by the Singular Value Decomposition (SVD), namely the Singular Value Decomposition (SVD) representation $A = U \\Sigma V^{\\top}$, where $U \\in \\mathbb{R}^{n \\times n}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthonormal matrices, and $\\Sigma \\in \\mathbb{R}^{n \\times n}$ is diagonal with nonnegative entries $\\sigma_1 \\ge \\sigma_2 \\ge \\cdots \\ge \\sigma_n \\ge 0$.\n\nIn the presence of small singular values, direct inversion to solve for an estimate of $x_{\\mathrm{true}}$ can be ill-conditioned due to noise amplification. A common stabilization is the truncated SVD estimator, which retains only the first $k$ singular components. Using the definitions of bias and variance in statistical estimation, where the bias of an estimator $\\hat{x}$ is defined as $\\mathrm{bias}(\\hat{x}) = \\mathbb{E}[\\hat{x}] - x_{\\mathrm{true}}$ and the variance of an estimator $\\hat{x}$ is quantified by the expected squared deviation from its mean, derive how truncating the small singular values influences the bias and variance of the solution, and implement a program that computes, for specified values of $k$ and $\\sigma_{\\varepsilon}^2$, the squared norm of the bias and the variance term of the truncated SVD estimator.\n\nUse the following instance to ground the computation:\n- Let $n = 6$, and let $A$ be the diagonal matrix with diagonal entries $\\sigma_1 = 10^{0}$, $\\sigma_2 = 10^{-1}$, $\\sigma_3 = 10^{-2}$, $\\sigma_4 = 10^{-3}$, $\\sigma_5 = 10^{-4}$, $\\sigma_6 = 10^{-5}$, so that $A = \\mathrm{diag}(10^{0}, 10^{-1}, 10^{-2}, 10^{-3}, 10^{-4}, 10^{-5})$.\n- Let $x_{\\mathrm{true}} = [\\,1,\\,0.5,\\,0.25,\\,0.125,\\,0.0625,\\,0.03125\\,]^{\\top}$.\n- Assume the noise vector $\\varepsilon$ has components distributed as zero-mean with variance $\\sigma_{\\varepsilon}^2$ and is independent of $x_{\\mathrm{true}}$.\n\nStarting from the fundamental definitions and the SVD structure, the program must:\n- Compute the SVD of $A$ to obtain $(U,\\Sigma,V)$.\n- Form the truncated SVD estimator that retains the first $k$ singular values.\n- Using the estimator and the definitions of bias and variance, compute the squared $\\ell_2$ norm of the bias, $\\|\\mathrm{bias}(\\hat{x}_k)\\|_2^2$, and the variance term for $\\hat{x}_k$ under the given noise model, expressed in terms of the singular values and singular vectors obtained from SVD.\n\nTest suite:\nEvaluate the computations for the following parameter values $(k, \\sigma_{\\varepsilon}^2)$:\n1. $(k, \\sigma_{\\varepsilon}^2) = (3, 10^{-6})$.\n2. $(k, \\sigma_{\\varepsilon}^2) = (0, 10^{-6})$.\n3. $(k, \\sigma_{\\varepsilon}^2) = (6, 10^{-6})$.\n4. $(k, \\sigma_{\\varepsilon}^2) = (2, 10^{-10})$.\n5. $(k, \\sigma_{\\varepsilon}^2) = (4, 10^{-4})$.\n\nRequired final output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes an inner list with two floating-point numbers in the order $[\\|\\mathrm{bias}(\\hat{x}_k)\\|_2^2, \\mathrm{variance\\ term}]$. For example, the output should look like $[[b_1,v_1],[b_2,v_2],\\dots]$, with the five inner lists ordered exactly as the test suite above.", "solution": "The problem is valid. It is a well-posed, scientifically grounded problem in computational science and numerical linear algebra, providing all necessary information for a unique solution.\n\nThe objective is to derive and compute the squared norm of the bias and the variance for the truncated Singular Value Decomposition (SVD) estimator of a system of linear equations. The linear model is given by $y = A x_{\\mathrm{true}} + \\varepsilon$, where $A \\in \\mathbb{R}^{n \\times n}$, $x_{\\mathrm{true}} \\in \\mathbb{R}^{n}$ is the true solution vector, and $\\varepsilon \\in \\mathbb{R}^{n}$ is a noise vector with i.i.d. components, each having a mean of $0$ and variance $\\sigma_{\\varepsilon}^2$.\n\nThe SVD of matrix $A$ is $A = U \\Sigma V^{\\top}$, where $U$ and $V$ are orthonormal matrices and $\\Sigma$ is a diagonal matrix of singular values $\\sigma_1 \\ge \\dots \\ge \\sigma_n \\ge 0$. The standard least-squares solution $\\hat{x} = A^{-1}y = V \\Sigma^{-1} U^{\\top} y$ is susceptible to noise amplification when some $\\sigma_i$ are small.\n\nThe truncated SVD estimator, $\\hat{x}_k$, mitigates this by using a truncated pseudoinverse $A_k^\\dagger$ which only inverts the first $k$ largest singular values.\nThe truncated pseudoinverse is defined as $A_k^\\dagger = V \\Sigma_k^\\dagger U^{\\top}$, where $\\Sigma_k^\\dagger$ is a diagonal matrix with entries:\n$$\n(\\Sigma_k^\\dagger)_{ii} =\n\\begin{cases}\n1/\\sigma_i  \\text{if } i \\le k \\\\\n0  \\text{if } i > k\n\\end{cases}\n$$\nThe estimator is then $\\hat{x}_k = A_k^\\dagger y$. Substituting $y = A x_{\\mathrm{true}} + \\varepsilon$:\n$$\n\\hat{x}_k = A_k^\\dagger (A x_{\\mathrm{true}} + \\varepsilon) = (V \\Sigma_k^\\dagger U^{\\top}) (U \\Sigma V^{\\top} x_{\\mathrm{true}} + \\varepsilon)\n$$\nUsing the orthonormality of $U$ ($U^{\\top}U = I$), we get:\n$$\n\\hat{x}_k = V \\Sigma_k^\\dagger \\Sigma V^{\\top} x_{\\mathrm{true}} + V \\Sigma_k^\\dagger U^{\\top} \\varepsilon\n$$\nThe product $\\Sigma_k^\\dagger \\Sigma$ is a diagonal matrix, let's call it $P_k$, with entries $(P_k)_{ii} = 1$ for $i \\le k$ and a value of $0$ for $i > k$. So, $\\hat{x}_k$ simplifies to:\n$$\n\\hat{x}_k = V P_k V^{\\top} x_{\\mathrm{true}} + V \\Sigma_k^\\dagger U^{\\top} \\varepsilon\n$$\n\n**1. Derivation of the Bias**\n\nThe bias of the estimator $\\hat{x}_k$ is defined as $\\mathrm{bias}(\\hat{x}_k) = \\mathbb{E}[\\hat{x}_k] - x_{\\mathrm{true}}$. We first compute the expected value of $\\hat{x}_k$. Since $\\mathbb{E}[\\varepsilon] = 0$, the expectation of the second term is zero:\n$$\n\\mathbb{E}[\\hat{x}_k] = \\mathbb{E}[V P_k V^{\\top} x_{\\mathrm{true}} + V \\Sigma_k^\\dagger U^{\\top} \\varepsilon] = V P_k V^{\\top} x_{\\mathrm{true}} + V \\Sigma_k^\\dagger U^{\\top} \\mathbb{E}[\\varepsilon] = V P_k V^{\\top} x_{\\mathrm{true}}\n$$\nThe bias is therefore:\n$$\n\\mathrm{bias}(\\hat{x}_k) = V P_k V^{\\top} x_{\\mathrm{true}} - x_{\\mathrm{true}}\n$$\nUsing the identity $I = V V^{\\top}$, we can write $x_{\\mathrm{true}} = V V^{\\top} x_{\\mathrm{true}}$:\n$$\n\\mathrm{bias}(\\hat{x}_k) = V P_k V^{\\top} x_{\\mathrm{true}} - V V^{\\top} x_{\\mathrm{true}} = V (P_k - I) V^{\\top} x_{\\mathrm{true}}\n$$\nThe matrix $(P_k - I)$ is diagonal with entries $(P_k-I)_{ii} = 0$ for $i \\le k$ and $(P_k-I)_{ii} = -1$ for $i > k$.\nThe squared $\\ell_2$ norm of the bias is $\\|\\mathrm{bias}(\\hat{x}_k)\\|_2^2$. Since $V$ is an orthonormal matrix, it preserves the $\\ell_2$ norm, i.e., $\\|Vz\\|_2 = \\|z\\|_2$ for any vector $z$.\n$$\n\\|\\mathrm{bias}(\\hat{x}_k)\\|_2^2 = \\|V (P_k - I) V^{\\top} x_{\\mathrm{true}}\\|_2^2 = \\|(P_k - I) V^{\\top} x_{\\mathrm{true}}\\|_2^2\n$$\nLet $z = V^{\\top} x_{\\mathrm{true}}$. The components of $z$ are $z_i = v_i^{\\top} x_{\\mathrm{true}}$, where $v_i$ is the $i$-th column of $V$. The vector $(P_k - I)z$ has components $0$ for $i \\le k$ and $-z_i$ for $i > k$. Its squared norm is:\n$$\n\\|\\mathrm{bias}(\\hat{x}_k)\\|_2^2 = \\sum_{i=k+1}^{n} (-z_i)^2 = \\sum_{i=k+1}^{n} (v_i^{\\top} x_{\\mathrm{true}})^2\n$$\nThis shows that the bias arises from discarding the components of $x_{\\mathrm{true}}$ that project onto the right singular vectors corresponding to the truncated singular values.\n\n**2. Derivation of the Variance**\n\nThe variance term is defined as the expected squared deviation from the mean: $\\mathrm{Var}(\\hat{x}_k) = \\mathbb{E}[\\|\\hat{x}_k - \\mathbb{E}[\\hat{x}_k]\\|_2^2]$.\nFrom our previous expressions:\n$$\n\\hat{x}_k - \\mathbb{E}[\\hat{x}_k] = V \\Sigma_k^\\dagger U^{\\top} \\varepsilon\n$$\nThe variance term is then:\n$$\n\\mathrm{Var}(\\hat{x}_k) = \\mathbb{E}[\\|V \\Sigma_k^\\dagger U^{\\top} \\varepsilon\\|_2^2]\n$$\nAgain, since $V$ is an isometry, this simplifies to:\n$$\n\\mathrm{Var}(\\hat{x}_k) = \\mathbb{E}[\\|\\Sigma_k^\\dagger U^{\\top} \\varepsilon\\|_2^2]\n$$\nLet's define a new random vector $\\eta = U^{\\top} \\varepsilon$. The covariance of $\\eta$ is $\\mathrm{Cov}(\\eta) = U^{\\top} \\mathrm{Cov}(\\varepsilon) U$. Given that the components of $\\varepsilon$ are i.i.d. with variance $\\sigma_{\\varepsilon}^2$, its covariance matrix is $\\mathrm{Cov}(\\varepsilon) = \\sigma_{\\varepsilon}^2 I$. Thus:\n$$\n\\mathrm{Cov}(\\eta) = U^{\\top} (\\sigma_{\\varepsilon}^2 I) U = \\sigma_{\\varepsilon}^2 U^{\\top} U = \\sigma_{\\varepsilon}^2 I\n$$\nThis means the components $\\eta_i$ are also uncorrelated, with mean $0$ and variance $\\sigma_{\\varepsilon}^2$.\nThe vector $\\Sigma_k^\\dagger \\eta$ has components $(\\Sigma_k^\\dagger \\eta)_i = (1/\\sigma_i)\\eta_i$ for $i \\le k$ and $0$ for $i > k$. Its squared norm is $\\sum_{i=1}^{k} (\\eta_i/\\sigma_i)^2$.\nTaking the expectation:\n$$\n\\mathrm{Var}(\\hat{x}_k) = \\mathbb{E}\\left[\\sum_{i=1}^{k} \\frac{\\eta_i^2}{\\sigma_i^2}\\right] = \\sum_{i=1}^{k} \\frac{\\mathbb{E}[\\eta_i^2]}{\\sigma_i^2}\n$$\nSince $\\eta_i$ has mean $0$ and variance $\\sigma_{\\varepsilon}^2$, we have $\\mathbb{E}[\\eta_i^2] = \\mathrm{Var}(\\eta_i) + (\\mathbb{E}[\\eta_i])^2 = \\sigma_{\\varepsilon}^2 + 0 = \\sigma_{\\varepsilon}^2$.\nThe final expression for the variance term is:\n$$\n\\mathrm{Var}(\\hat{x}_k) = \\sum_{i=1}^{k} \\frac{\\sigma_{\\varepsilon}^2}{\\sigma_i^2} = \\sigma_{\\varepsilon}^2 \\sum_{i=1}^{k} \\frac{1}{\\sigma_i^2}\n$$\nThis shows that the variance is due to the amplification of noise by the inverse of the singular values that are retained.\n\n**3. Application to the Specific Problem Instance**\n\nFor the given problem, the matrix $A$ is diagonal: $A = \\mathrm{diag}(10^{0}, 10^{-1}, \\dots, 10^{-5})$. For a non-negative diagonal matrix, the SVD is trivial: $U=I$, $V=I$, and $\\Sigma=A$. The singular values $\\sigma_i$ are the diagonal entries of $A$, and the right singular vectors $v_i$ are the standard basis vectors $e_i$.\n\nWith these simplifications, our formulas become:\n- **Squared Bias Norm**: Since $v_i = e_i$, the projection $v_i^{\\top} x_{\\mathrm{true}}$ is simply the $i$-th component of $x_{\\mathrm{true}}$, denoted $x_{\\mathrm{true},i}$.\n  $$\n  \\|\\mathrm{bias}(\\hat{x}_k)\\|_2^2 = \\sum_{i=k+1}^{n} (x_{\\mathrm{true},i})^2\n  $$\n- **Variance Term**: The formula remains unchanged but we use the specific singular values.\n  $$\n  \\mathrm{Var}(\\hat{x}_k) = \\sigma_{\\varepsilon}^2 \\sum_{i=1}^{k} \\frac{1}{\\sigma_i^2}\n  $$\n\nThe specific values are:\n- $n=6$\n- $\\sigma_i = 10^{-(i-1)}$ for $i=1, \\dots, 6$.\n- $x_{\\mathrm{true}} = [1, 0.5, 0.25, 0.125, 0.0625, 0.03125]^{\\top} = [2^{0}, 2^{-1}, 2^{-2}, 2^{-3}, 2^{-4}, 2^{-5}]^{\\top}$.\n\nWe can now compute the required quantities for each test case by applying these formulas. For a given $k$ and $\\sigma_{\\varepsilon}^2$, we sum the squared components of $x_{\\mathrm{true}}$ from index $k$ to $n-1$ (using $0$-based indexing for implementation) for the bias, and we compute the weighted sum of inverse squared singular values up to index $k-1$ for the variance.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the squared bias norm and variance for the truncated SVD estimator\n    for a given linear system and a set of test cases.\n    \"\"\"\n    # Define the parameters of the problem instance.\n    n = 6\n    # Singular values: sigma_i = 10^-(i-1) for i=1,...,6\n    sigmas = np.array([10.0**(-i) for i in range(n)])\n    \n    # True solution vector: x_true_i = (1/2)^(i-1) for i=1,...,6\n    # using 0-based indexing for numpy array\n    x_true = np.array([0.5**i for i in range(n)])\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (k, sigma_epsilon_squared)\n        (3, 10**-6),\n        (0, 10**-6),\n        (6, 10**-6),\n        (2, 10**-10),\n        (4, 10**-4),\n    ]\n\n    results = []\n    for k, sigma_eps_sq in test_cases:\n        # Calculate the squared L2 norm of the bias.\n        # Bias is due to truncating components from k to n.\n        # In 0-based indexing, this corresponds to components from index k to n-1.\n        # If k=n, the slice x_true[n:] is empty, and the sum is correctly 0.\n        # If k=0, the slice x_true[0:] is the whole array, giving ||x_true||^2.\n        bias_sq_norm = np.sum(x_true[k:]**2)\n\n        # Calculate the variance term.\n        # Variance is due to noise amplification by the kept singular values (1 to k).\n        # In 0-based indexing, this corresponds to sigmas from index 0 to k-1.\n        # If k=0, the slice sigmas[:0] is empty, and the sum is correctly 0.\n        if k  0:\n            variance_term = sigma_eps_sq * np.sum(1.0 / sigmas[:k]**2)\n        else:\n            variance_term = 0.0\n        \n        results.append([bias_sq_norm, variance_term])\n\n    # Format the output string to exactly match the required format:\n    # [[b_1,v_1],[b_2,v_2],...] with no spaces.\n    inner_strings = [f\"[{b},{v}]\" for b, v in results]\n    output_string = f\"[{','.join(inner_strings)}]\"\n    \n    # Final print statement in the exact required format.\n    print(output_string)\n\nsolve()\n```", "id": "3141620"}]}