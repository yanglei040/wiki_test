{"hands_on_practices": [{"introduction": "The theoretical guarantee of convergence for iterative methods often rests on complex analysis. However, a powerful and easy-to-check *sufficient* condition exists: strict diagonal dominance. This exercise provides hands-on practice in verifying this property for a given matrix, a crucial first step in quickly assessing whether methods like Jacobi or Gauss-Seidel are guaranteed to work [@problem_id:1369798].", "problem": "A square matrix $M$ with entries $m_{ij}$ is called strictly diagonally dominant if for every row $i$, the absolute value of the diagonal entry, $|m_{ii}|$, is strictly greater than the sum of the absolute values of all other entries in that row. This condition can be expressed as $|m_{ii}|  \\sum_{j \\neq i} |m_{ij}|$. The convergence of certain iterative methods for solving linear systems, like the Jacobi and Gauss-Seidel methods, is guaranteed if the coefficient matrix is strictly diagonally dominant.\n\nConsider the following system of linear equations:\n$$\n\\begin{cases}\n5x_1 + 2x_2 - x_3 = 9 \\\\\n12x_1 + 8x_2 - 3x_3 = 20 \\\\\nx_1 - 4x_2 + 6x_3 = -1\n\\end{cases}\n$$\n\nLet $A$ be the coefficient matrix of this system. A new linear system is created by taking the original system and scaling the second equation by a factor of $1/10$. Let the coefficient matrix of this new system be denoted by $A'$.\n\nWhich of the following statements is true regarding the matrix $A'$?\n\nA. The matrix $A'$ is strictly diagonally dominant.\n\nB. The matrix $A'$ is not strictly diagonally dominant because the first row fails the condition.\n\nC. The matrix $A'$ is not strictly diagonally dominant because the second row fails the condition.\n\nD. The matrix $A'$ is not strictly diagonally dominant because the third row fails the condition.\n\nE. The matrix $A'$ is not strictly diagonally dominant because both the first and second rows fail the condition.", "solution": "The problem asks us to determine if a new coefficient matrix $A'$, obtained by scaling one row of an original matrix $A$, is strictly diagonally dominant.\n\nFirst, let's identify the original coefficient matrix $A$ from the given system of linear equations:\n$$\nA = \\begin{pmatrix} 5  2  -1 \\\\ 12  8  -3 \\\\ 1  -4  6 \\end{pmatrix}\n$$\n\nThe problem states that a new linear system is created by scaling the second equation by a factor of $1/10$. This operation corresponds to multiplying every element in the second row of the coefficient matrix $A$ by $1/10$. This gives us the new coefficient matrix $A'$.\n\n$$\nA' = \\begin{pmatrix} 5  2  -1 \\\\ 12 \\times \\frac{1}{10}  8 \\times \\frac{1}{10}  -3 \\times \\frac{1}{10} \\\\ 1  -4  6 \\end{pmatrix} = \\begin{pmatrix} 5  2  -1 \\\\ 1.2  0.8  -0.3 \\\\ 1  -4  6 \\end{pmatrix}\n$$\n\nNow, we must check if $A'$ is strictly diagonally dominant. According to the definition provided, for each row $i$, we must verify if $|a'_{ii}|  \\sum_{j \\neq i} |a'_{ij}|$. Let's check this condition for each row of $A'$.\n\n**Row 1:**\nWe check if $|a'_{11}|  |a'_{12}| + |a'_{13}|$.\nThe diagonal element is $a'_{11} = 5$. The sum of the absolute values of the off-diagonal elements is $|a'_{12}| + |a'_{13}| = |2| + |-1| = 2 + 1 = 3$.\nThe condition is $|5|  3$, which simplifies to $5  3$. This is true. So, the first row satisfies the condition.\n\n**Row 2:**\nWe check if $|a'_{22}|  |a'_{21}| + |a'_{23}|$.\nThe diagonal element is $a'_{22} = 0.8$. The sum of the absolute values of the off-diagonal elements is $|a'_{21}| + |a'_{23}| = |1.2| + |-0.3| = 1.2 + 0.3 = 1.5$.\nThe condition is $|0.8|  1.5$, which simplifies to $0.8  1.5$. This is false. Therefore, the second row fails the condition for strict diagonal dominance.\n\nSince the condition has failed for at least one row, we can immediately conclude that the matrix $A'$ is not strictly diagonally dominant. The failure occurred in the second row.\n\n**Row 3 (for completeness):**\nWe check if $|a'_{33}|  |a'_{31}| + |a'_{32}|$.\nThe diagonal element is $a'_{33} = 6$. The sum of the absolute values of the off-diagonal elements is $|a'_{31}| + |a'_{32}| = |1| + |-4| = 1 + 4 = 5$.\nThe condition is $|6|  5$, which simplifies to $6  5$. This is true. So, the third row satisfies the condition.\n\nThe matrix $A'$ is not strictly diagonally dominant because the condition fails for the second row. Therefore, the correct statement is that the matrix is not strictly diagonally dominant because the second row fails the condition. This corresponds to option C.\n\nIt is a general property that scaling a row by a non-zero constant $c$ does not change whether that row satisfies the strict diagonal dominance condition. Let the original row be $(a_{i1}, \\dots, a_{in})$. The condition is $|a_{ii}|  \\sum_{j \\neq i} |a_{ij}|$. After scaling, the new row is $(ca_{i1}, \\dots, ca_{in})$. The condition for the new row is $|ca_{ii}|  \\sum_{j \\neq i} |ca_{ij}|$. This is equivalent to $|c||a_{ii}|  \\sum_{j \\neq i} |c||a_{ij}|$, which simplifies to $|c||a_{ii}|  |c|\\sum_{j \\neq i} |a_{ij}|$. Since $c=1/10 \\neq 0$, we can divide by $|c|$, which gives $|a_{ii}|  \\sum_{j \\neq i} |a_{ij}|$, the original condition. Thus, if a row initially fails the condition, it will continue to fail after scaling. In our original matrix $A$, for row 2, we had $|8|  |12| + |-3|$, or $8  15$, which was false. The row failed initially, and it must fail after scaling.", "answer": "$$\\boxed{C}$$", "id": "1369798"}, {"introduction": "Building on the concept of diagonal dominance, it's important to recognize the distinction between row-based and column-based definitions. While strict *row* diagonal dominance guarantees convergence for the Jacobi and Gauss-Seidel methods, understanding both types deepens your analytical toolkit. This practice will sharpen your ability to inspect a matrix and differentiate between these important, but distinct, properties [@problem_id:2166709].", "problem": "In numerical linear algebra, certain properties of a matrix can guarantee the convergence of iterative methods used to solve systems of linear equations. One such property is diagonal dominance.\n\nA square matrix $A$ of size $n \\times n$ with entries $a_{ij}$ is defined as **strictly row diagonally dominant** if for every row $i$, the absolute value of the diagonal entry is strictly greater than the sum of the absolute values of all other entries in that row. Mathematically, this is expressed as:\n$$|a_{ii}|  \\sum_{j \\neq i} |a_{ij}| \\quad \\text{for all } i = 1, 2, \\dots, n$$\n\nSimilarly, the matrix $A$ is defined as **strictly column diagonally dominant** if for every column $j$, the absolute value of the diagonal entry is strictly greater than the sum of the absolute values of all other entries in that column. Mathematically, this is expressed as:\n$$|a_{jj}|  \\sum_{i \\neq j} |a_{ij}| \\quad \\text{for all } j = 1, 2, \\dots, n$$\n\nConsider the following 3x3 matrices. Which one of these matrices is strictly column diagonally dominant but is **not** strictly row diagonally dominant?\n\nA.\n$A_A = \\begin{pmatrix} 10  1  2 \\\\ 3  9  4 \\\\ 1  2  8 \\end{pmatrix}$\n\nB.\n$A_B = \\begin{pmatrix} 10  3  8 \\\\ 4  8  5 \\\\ 7  2  12 \\end{pmatrix}$\n\nC.\n$A_C = \\begin{pmatrix} 10  3  6 \\\\ 4  8  5 \\\\ 5  2  12 \\end{pmatrix}$\n\nD.\n$A_D = \\begin{pmatrix} 10  4  5 \\\\ 3  8  2 \\\\ 6  5  12 \\end{pmatrix}$", "solution": "We use the definitions:\n- Strict row diagonal dominance (SRDD): for each row index $i$, $|a_{ii}|  \\sum_{j \\neq i} |a_{ij}|$.\n- Strict column diagonal dominance (SCDD): for each column index $j$, $|a_{jj}|  \\sum_{i \\neq j} |a_{ij}|$.\nAll entries are nonnegative, so $|a_{ij}| = a_{ij}$.\n\nMatrix $A_{A} = \\begin{pmatrix} 10  1  2 \\\\ 3  9  4 \\\\ 1  2  8 \\end{pmatrix}$.\nRow checks:\n- Row $1$: $10  1 + 2 = 3$.\n- Row $2$: $9  3 + 4 = 7$.\n- Row $3$: $8  1 + 2 = 3$.\nThus SRDD holds.\nColumn checks:\n- Column $1$: $10  3 + 1 = 4$.\n- Column $2$: $9  1 + 2 = 3$.\n- Column $3$: $8  2 + 4 = 6$.\nThus SCDD holds.\nConclusion: $A_{A}$ is both SRDD and SCDD, not the requested case.\n\nMatrix $A_{B} = \\begin{pmatrix} 10  3  8 \\\\ 4  8  5 \\\\ 7  2  12 \\end{pmatrix}$.\nRow checks:\n- Row $1$: $10  3 + 8 = 11$ is false, so SRDD fails.\nColumn checks:\n- Column $1$: $10  4 + 7 = 11$ is false, so SCDD fails.\nConclusion: $A_{B}$ is neither SCDD nor SRDD, not the requested case.\n\nMatrix $A_{C} = \\begin{pmatrix} 10  3  6 \\\\ 4  8  5 \\\\ 5  2  12 \\end{pmatrix}$.\nRow checks:\n- Row $1$: $10  3 + 6 = 9$ holds.\n- Row $2$: $8  4 + 5 = 9$ is false, so SRDD fails.\n- Row $3$: $12  5 + 2 = 7$ holds.\nThus not SRDD.\nColumn checks:\n- Column $1$: $10  4 + 5 = 9$ holds.\n- Column $2$: $8  3 + 2 = 5$ holds.\n- Column $3$: $12  6 + 5 = 11$ holds.\nThus SCDD holds.\nConclusion: $A_{C}$ is strictly column diagonally dominant but not strictly row diagonally dominant.\n\nMatrix $A_{D} = \\begin{pmatrix} 10  4  5 \\\\ 3  8  2 \\\\ 6  5  12 \\end{pmatrix}$.\nRow checks:\n- Row $1$: $10  4 + 5 = 9$.\n- Row $2$: $8  3 + 2 = 5$.\n- Row $3$: $12  6 + 5 = 11$.\nThus SRDD holds.\nColumn checks:\n- Column $1$: $10  3 + 6 = 9$ holds.\n- Column $2$: $8  4 + 5 = 9$ is false, so SCDD fails.\nConclusion: $A_{D}$ is SRDD but not SCDD, not the requested case.\n\nTherefore, the only matrix that is strictly column diagonally dominant but not strictly row diagonally dominant is $A_{C}$.", "answer": "$$\\boxed{C}$$", "id": "2166709"}, {"introduction": "While diagonal dominance is a convenient shortcut, the definitive test for the convergence of any stationary iterative method is the spectral radius of its iteration matrix being less than one. This coding exercise takes you to the heart of convergence analysis, requiring you to implement and compare the Jacobi and Gauss-Seidel methods. You will construct a scenario where one method converges and the other diverges, demonstrating empirically the predictive power of the spectral radius criterion [@problem_id:3205095].", "problem": "Design and implement a complete, runnable program that constructs and analyzes linear systems to evaluate the robustness and stability of iterative solvers. Use the fundamental base that iterative methods for linear systems are derived from matrix splittings and that their convergence is governed by the spectral radius criterion. Specifically, for a linear system with matrix splitting, the Jacobi method and the Gauss-Seidel method are defined using matrices built from the coefficient matrix. The convergence of either method is ensured when the spectral radius of the associated iteration matrix is strictly less than one. The program must demonstrate a case where the Jacobi iteration diverges while the Gauss-Seidel iteration converges, explain the mechanism from first principles, and verify it numerically.\n\nUse the following foundational definitions and facts.\n\n- For a square matrix $A \\in \\mathbb{R}^{n \\times n}$, define the diagonal part $D$, the strictly lower triangular part $L$, and the strictly upper triangular part $U$ such that $A = D + L + U$.\n- The Jacobi iteration is defined by the update $x^{k+1} = D^{-1}\\left(b - (L + U)x^{k}\\right)$, which can be written as $x^{k+1} = T_{J} x^{k} + c_{J}$ with iteration matrix $T_{J} = -D^{-1}(L+U)$.\n- The Gauss-Seidel iteration is defined by the update $x^{k+1} = (D + L)^{-1}\\left(b - U x^{k}\\right)$, which can be written as $x^{k+1} = T_{GS} x^{k} + c_{GS}$ with iteration matrix $T_{GS} = -(D+L)^{-1}U$.\n- A well-tested convergence fact is that an iterative method $x^{k+1} = T x^{k} + c$ converges for any initial vector if and only if the spectral radius $\\rho(T)  1$. Another well-tested fact is that for any symmetric positive definite matrix (SPD), the Gauss-Seidel method converges for any right-hand side $b$.\n\nYour program must:\n- For each test case, construct $D$, $L$, $U$, form $T_{J}$ and $T_{GS}$, compute their spectral radii $\\rho(T_{J})$ and $\\rho(T_{GS})$, run both iterative methods from $x^{0} = 0$ for a maximum of $N$ iterations with tolerance $\\varepsilon$ on the residual $\\|b - A x^{k}\\|_{2}$, and report whether each method converged.\n- Use a residual tolerance $\\varepsilon = 10^{-10}$ and a maximum iteration count $N = 500$. No physical units are involved.\n\nTest suite. Analyze the following four linear systems, specified by their coefficient matrices $A$ and right-hand sides $b$:\n\n- Case $1$ (target case: Jacobi diverges, Gauss-Seidel converges): \n  $$A_{1} = \\begin{bmatrix} \\tfrac{3}{2}  1  1 \\\\ 1  \\tfrac{3}{2}  1 \\\\ 1  1  \\tfrac{3}{2} \\end{bmatrix}, \\quad b_{1} = \\begin{bmatrix} 1 \\\\ -1 \\\\ 2 \\end{bmatrix}.$$\n- Case $2$ (strictly diagonally dominant, both converge):\n  $$A_{2} = \\begin{bmatrix} 4  1  1 \\\\ 1  4  1 \\\\ 1  1  4 \\end{bmatrix}, \\quad b_{2} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}.$$\n- Case $3$ (singular and ill-posed, both fail to converge under the given criteria):\n  $$A_{3} = \\begin{bmatrix} 1  1  1 \\\\ 1  1  1 \\\\ 1  1  1 \\end{bmatrix}, \\quad b_{3} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}.$$\n- Case $4$ (borderline for Jacobi with $\\rho(T_{J}) = 1$, Gauss-Seidel converges):\n  $$A_{4} = \\begin{bmatrix} 2  1  1 \\\\ 1  2  1 \\\\ 1  1  2 \\end{bmatrix}, \\quad b_{4} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}.$$\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must be a list of the form $[\\rho(T_{J}), \\rho(T_{GS}), \\text{JacobiConverged}, \\text{GaussSeidelConverged}]$, where $\\rho(T_{J})$ and $\\rho(T_{GS})$ are floating-point numbers and the convergence indicators are booleans. For example, the output must look like \n$$[\\,[\\rho_{1J}, \\rho_{1GS}, \\text{bool}, \\text{bool}], [\\rho_{2J}, \\rho_{2GS}, \\text{bool}, \\text{bool}], [\\rho_{3J}, \\rho_{3GS}, \\text{bool}, \\text{bool}], [\\rho_{4J}, \\rho_{4GS}, \\text{bool}, \\text{bool}]\\,].$$", "solution": "The core principle is that classical stationary iterative methods arise from matrix splittings of the linear system and converge when their iteration matrices contract the error; this is ensured if the spectral radius is strictly less than one. Starting from the fundamental definition $A = D + L + U$ where $D$ is the diagonal of $A$, $L$ is the strictly lower triangular part, and $U$ is the strictly upper triangular part, the Jacobi and Gauss-Seidel methods are defined by\n$$x^{k+1} = D^{-1}\\left(b - (L + U)x^{k}\\right), \\quad x^{k+1} = (D + L)^{-1}\\left(b - U x^{k}\\right),$$\nrespectively. These can be expressed as stationary iterations $x^{k+1} = T x^{k} + c$ with $T_{J} = -D^{-1}(L+U)$ for Jacobi and $T_{GS} = -(D+L)^{-1} U$ for Gauss-Seidel. A well-tested fact in numerical analysis is that such stationary iterations converge for any initial vector $x^{0}$ if and only if $\\rho(T)  1$, where $\\rho(T)$ denotes the spectral radius, the maximum magnitude of the eigenvalues of $T$. Another well-tested and fundamental result is that the Gauss-Seidel method converges for any symmetric positive definite (SPD) matrix $A$.\n\nWe now explain how to construct a matrix where the Jacobi iteration diverges while the Gauss-Seidel iteration converges, and why this happens. Consider the $n \\times n$ matrix family with constant diagonal and constant off-diagonal entries,\n$$A(\\alpha,\\beta) = \\alpha I + \\beta ( \\mathbf{1}\\mathbf{1}^{\\top} - I ),$$\nthat is, $A(\\alpha,\\beta)$ has diagonal entries equal to $\\alpha$ and off-diagonal entries equal to $\\beta$. For $n = 3$, this is\n$$A(\\alpha,\\beta) = \\begin{bmatrix} \\alpha  \\beta  \\beta \\\\ \\beta  \\alpha  \\beta \\\\ \\beta  \\beta  \\alpha \\end{bmatrix}.$$\nThis matrix is symmetric by construction. Its eigenstructure is well known: the eigenvalues are\n$$\\lambda_{1} = \\alpha - \\beta \\quad \\text{(with multiplicity } n-1 \\text{)}, \\qquad \\lambda_{2} = \\alpha + (n-1)\\beta \\quad \\text{(with multiplicity } 1 \\text{)}.$$\nTherefore, $A(\\alpha,\\beta)$ is symmetric positive definite precisely when $\\alpha - \\beta  0$, that is, $\\alpha  \\beta$, since both $\\lambda_{1}$ and $\\lambda_{2}$ are then strictly positive.\n\nFor the Jacobi method, we compute its iteration matrix. Since $D = \\alpha I$ and $L+U = \\beta(\\mathbf{1}\\mathbf{1}^{\\top} - I)$ for this family, the Jacobi iteration matrix is\n$$T_{J} = -D^{-1}(L+U) = -\\frac{\\beta}{\\alpha} (\\mathbf{1}\\mathbf{1}^{\\top} - I).$$\nThe eigenvalues of $\\mathbf{1}\\mathbf{1}^{\\top} - I$ are $(n-1)$ for the eigenvector $\\mathbf{1}$ and $-1$ for any vector orthogonal to $\\mathbf{1}$. Consequently, the eigenvalues of $T_{J}$ are\n$$\\mu_{1} = -\\frac{\\beta}{\\alpha} (n-1), \\qquad \\mu_{2} = \\frac{\\beta}{\\alpha} \\quad \\text{(with multiplicity } n-1 \\text{)}.$$\nThe spectral radius is thus\n$$\\rho(T_{J}) = \\max\\left\\{ \\left| -\\frac{\\beta}{\\alpha}(n-1) \\right|, \\left| \\frac{\\beta}{\\alpha} \\right| \\right\\} = \\frac{\\beta}{\\alpha}(n-1).$$\nFor $n = 3$, we obtain $\\rho(T_{J}) = \\dfrac{2\\beta}{\\alpha}$. The Jacobi method will diverge if $\\rho(T_{J}) \\geq 1$, that is, if $\\dfrac{2\\beta}{\\alpha} \\geq 1$. If we choose $\\alpha = \\tfrac{3}{2}$ and $\\beta = 1$, then $\\alpha  \\beta$ and $A(\\alpha,\\beta)$ is SPD, but\n$$\\rho(T_{J}) = \\frac{2 \\cdot 1}{\\tfrac{3}{2}} = \\frac{4}{3}  1,$$\nso the Jacobi method diverges according to the spectral radius criterion.\n\nFor the Gauss-Seidel method, the fundamental convergence result states that for any SPD matrix $A$, Gauss-Seidel converges. Since $A\\left(\\tfrac{3}{2}, 1\\right)$ is SPD, Gauss-Seidel converges even though Jacobi diverges. This shows the increased robustness of Gauss-Seidel relative to Jacobi in this setting.\n\nTo provide additional context on why such divergence cannot occur in the $2 \\times 2$ case under the standard ordering, consider $A = \\begin{bmatrix} a  b \\\\ c  d \\end{bmatrix}$ with $D = \\operatorname{diag}(a,d)$, $L = \\begin{bmatrix} 0  0 \\\\ c  0 \\end{bmatrix}$, and $U = \\begin{bmatrix} 0  b \\\\ 0  0 \\end{bmatrix}$. The Jacobi iteration matrix is\n$$T_{J} = -D^{-1}(L+U) = \\begin{bmatrix} 0  -\\frac{b}{a} \\\\ -\\frac{c}{d}  0 \\end{bmatrix},$$\nwhose eigenvalues satisfy $\\lambda^{2} = \\dfrac{bc}{ad}$, hence $\\rho(T_{J}) = \\sqrt{\\left|\\dfrac{bc}{ad}\\right|}$. The Gauss-Seidel iteration matrix is\n$$T_{GS} = -(D+L)^{-1}U = \\begin{bmatrix} 0  -\\frac{b}{a} \\\\ 0  \\frac{bc}{ad} \\end{bmatrix},$$\nwhich is upper triangular and has eigenvalues $0$ and $\\dfrac{bc}{ad}$, hence $\\rho(T_{GS}) = \\left|\\dfrac{bc}{ad}\\right|$. Therefore, in $2 \\times 2$, both methods converge or diverge together under the same inequality $\\left|\\dfrac{bc}{ad}\\right|  1$, and one cannot construct a counterexample of Jacobi diverging while Gauss-Seidel converging. This explains why the requested example requires dimension $n \\geq 3$.\n\nThe test suite is designed to cover several facets:\n- Case $1$ provides the primary scenario where Jacobi diverges ($\\rho(T_{J})  1$) while Gauss-Seidel converges due to $A$ being SPD.\n- Case $2$ is strictly diagonally dominant, a situation where both methods are known to converge; here, $\\rho(T_{J})$ and $\\rho(T_{GS})$ will be well below $1$.\n- Case $3$ is singular and ill-posed; neither method should satisfy the spectral radius convergence criterion, and numerically the residual will not reach the specified tolerance.\n- Case $4$ is the borderline where $\\rho(T_{J}) = 1$; Jacobi typically fails to converge (no contraction), while Gauss-Seidel converges since $A$ is SPD.\n\nThe program computes $T_{J}$ and $T_{GS}$ directly from $D$, $L$, and $U$, evaluates their spectral radii via eigenvalues, and performs iterations from $x^{0} = 0$ up to $N = 500$ steps with tolerance $\\varepsilon = 10^{-10}$ using the residual norm $\\|b - A x^{k}\\|_{2}$. The results are formatted exactly as required: each case contributes a list $[\\rho(T_{J}), \\rho(T_{GS}), \\text{JacobiConverged}, \\text{GaussSeidelConverged}]$, and the final output is a single line list of these case results. This ties the theoretical convergence criteria to practical numerical behavior, illustrating algorithm robustness and stability with clear quantitative checks.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef spectral_radius(mat: np.ndarray) - float:\n    \"\"\"Compute spectral radius (max absolute eigenvalue) of a square matrix.\"\"\"\n    eigvals = np.linalg.eigvals(mat)\n    return float(np.max(np.abs(eigvals)))\n\ndef jacobi_iteration(A: np.ndarray, b: np.ndarray, max_iter: int = 500, tol: float = 1e-10) - bool:\n    \"\"\"Run Jacobi iteration starting from x0=0, return True if residual norm = tol.\"\"\"\n    n = A.shape[0]\n    D = np.diag(np.diag(A))\n    L = np.tril(A, -1)\n    U = np.triu(A, 1)\n    # Precompute D^{-1}\n    try:\n        D_inv = np.linalg.inv(D)\n    except np.linalg.LinAlgError:\n        return False\n    x = np.zeros(n, dtype=float)\n    for _ in range(max_iter):\n        x = D_inv @ (b - (L + U) @ x)\n        r = b - A @ x\n        if np.linalg.norm(r, 2) = tol:\n            return True\n    return False\n\ndef gauss_seidel_iteration(A: np.ndarray, b: np.ndarray, max_iter: int = 500, tol: float = 1e-10) - bool:\n    \"\"\"Run Gauss-Seidel iteration starting from x0=0, return True if residual norm = tol.\"\"\"\n    n = A.shape[0]\n    D = np.diag(np.diag(A))\n    L = np.tril(A, -1)\n    U = np.triu(A, 1)\n    DL = D + L\n    try:\n        DL_inv = np.linalg.inv(DL)\n    except np.linalg.LinAlgError:\n        return False\n    x = np.zeros(n, dtype=float)\n    for _ in range(max_iter):\n        x = DL_inv @ (b - U @ x)\n        r = b - A @ x\n        if np.linalg.norm(r, 2) = tol:\n            return True\n    return False\n\ndef solve():\n    # Define the test cases from the problem statement.\n    A1 = np.array([[1.5, 1.0, 1.0],\n                   [1.0, 1.5, 1.0],\n                   [1.0, 1.0, 1.5]], dtype=float)\n    b1 = np.array([1.0, -1.0, 2.0], dtype=float)\n\n    A2 = np.array([[4.0, 1.0, 1.0],\n                   [1.0, 4.0, 1.0],\n                   [1.0, 1.0, 4.0]], dtype=float)\n    b2 = np.array([1.0, 2.0, 3.0], dtype=float)\n\n    A3 = np.array([[1.0, 1.0, 1.0],\n                   [1.0, 1.0, 1.0],\n                   [1.0, 1.0, 1.0]], dtype=float)\n    b3 = np.array([1.0, 1.0, 1.0], dtype=float)\n\n    A4 = np.array([[2.0, 1.0, 1.0],\n                   [1.0, 2.0, 1.0],\n                   [1.0, 1.0, 2.0]], dtype=float)\n    b4 = np.array([0.0, 0.0, 1.0], dtype=float)\n\n    test_cases = [\n        (A1, b1),\n        (A2, b2),\n        (A3, b3),\n        (A4, b4),\n    ]\n\n    results = []\n    for A, b in test_cases:\n        D = np.diag(np.diag(A))\n        L = np.tril(A, -1)\n        U = np.triu(A, 1)\n        # Iteration matrices\n        # Use safe inverses; if singular, mark spectral radii as np.nan\n        try:\n            TJ = -np.linalg.inv(D) @ (L + U)\n            rho_J = spectral_radius(TJ)\n        except np.linalg.LinAlgError:\n            rho_J = float('nan')\n        try:\n            TGS = -np.linalg.inv(D + L) @ U\n            rho_GS = spectral_radius(TGS)\n        except np.linalg.LinAlgError:\n            rho_GS = float('nan')\n\n        jacobi_conv = jacobi_iteration(A, b, max_iter=500, tol=1e-10)\n        gs_conv = gauss_seidel_iteration(A, b, max_iter=500, tol=1e-10)\n\n        results.append([rho_J, rho_GS, jacobi_conv, gs_conv])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(str(item) for item in results)}]\")\n\nsolve()\n```", "id": "3205095"}]}