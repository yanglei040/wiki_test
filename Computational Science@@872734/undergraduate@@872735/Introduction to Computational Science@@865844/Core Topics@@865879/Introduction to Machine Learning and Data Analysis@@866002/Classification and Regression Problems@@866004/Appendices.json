{"hands_on_practices": [{"introduction": "A critical assumption of Ordinary Least Squares (OLS) regression is homoscedasticity, meaning the variance of the errors is constant across all levels of the independent variables. This exercise provides a hands-on workflow for diagnosing and addressing heteroscedasticity, a common issue where this assumption is violated. You will first use a formal statistical test to classify a dataset as heteroscedastic and then apply Weighted Least Squares (WLS) to build a more efficient model, a process that involves a secondary regression to model the error variance itself [@problem_id:3107027].", "problem": "You are asked to write a complete, runnable program that, for several synthetic datasets, performs three tasks grounded in the classical linear model: classify the presence of heteroscedasticity of the error term conditional variance $\\operatorname{Var}(\\epsilon \\mid x)$ using residual diagnostics formalized by a Lagrange Multiplier principle, estimate a parametric variance function $\\sigma^{2}(x)$ by regression, and refit the regression by weighted least squares. Your implementation must follow a principle-based method derived from fundamental definitions and well-tested results, as specified below.\n\nThe base model is the simple linear regression\n$$\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i,\\quad i=1,\\dots,n,\n$$\nwhere the errors satisfy $\\mathbb{E}[\\epsilon_i \\mid x_i] = 0$. Under homoscedasticity, the conditional variance is constant, $\\operatorname{Var}(\\epsilon_i \\mid x_i) = \\sigma^2$, while under heteroscedasticity it varies with $x_i$, $\\operatorname{Var}(\\epsilon_i \\mid x_i) = \\sigma^2(x_i)$. Ordinary least squares (OLS) chooses coefficients that minimize the sum of squared residuals,\n$$\n(\\widehat{\\beta}_0,\\widehat{\\beta}_1) = \\arg\\min_{\\beta_0,\\beta_1} \\sum_{i=1}^{n} \\left(y_i - \\beta_0 - \\beta_1 x_i\\right)^2.\n$$\nDefine residuals $r_i = y_i - \\widehat{\\beta}_0 - \\widehat{\\beta}_1 x_i$.\n\nTask A (Heteroscedasticity classification via residual plots formalized as a test): Use the Breusch–Pagan Lagrange Multiplier approach as a formal proxy for residual-pattern diagnostics. Regress the squared residuals on the regressors and a constant,\n$$\nr_i^2 = \\delta_0 + \\delta_1 x_i + u_i,\n$$\nand compute the coefficient of determination $R^2$ of this auxiliary regression. The Breusch–Pagan test statistic is\n$$\n\\mathrm{LM} = n R^2,\n$$\nwhich, under the null hypothesis of homoscedasticity, follows approximately a chi-squared distribution with $k$ degrees of freedom, where $k$ is the number of non-constant regressors in the auxiliary regression. For this problem, $k = 1$. Classify a dataset as heteroscedastic if the chi-squared survival probability satisfies\n$$\np\\text{-value} = \\Pr\\left(\\chi^2_k \\ge \\mathrm{LM}\\right) < 0.05.\n$$\n\nTask B (Variance function regression): Using the OLS residuals, fit a positive variance model by regressing the log-squared residuals on a fixed feature of $x$:\n$$\n\\log\\left(r_i^2 + \\varepsilon\\right) = \\gamma_0 + \\gamma_1 \\,\\log\\!\\left(1 + x_i^2\\right) + \\eta_i,\n$$\nwhere $\\varepsilon$ is a small positive constant to avoid taking the logarithm of zero. Then define the fitted variance at each $x_i$ as\n$$\n\\widehat{\\sigma}_i^2 = \\exp\\!\\left(\\widehat{\\gamma}_0 + \\widehat{\\gamma}_1 \\,\\log\\!\\left(1 + x_i^2\\right)\\right).\n$$\nThis ensures $\\widehat{\\sigma}_i^2 > 0$ for all $i$.\n\nTask C (Weighted least squares refit): With weights $w_i = 1/\\widehat{\\sigma}_i^2$, compute the weighted least squares (WLS) estimator that minimizes\n$$\n\\sum_{i=1}^{n} w_i \\left(y_i - \\beta_0 - \\beta_1 x_i\\right)^2,\n$$\nand report the refitted coefficients. Also compute the mean squared error (MSE) of OLS and WLS fits, defined as\n$$\n\\mathrm{MSE}_{\\mathrm{OLS}} = \\frac{1}{n} \\sum_{i=1}^{n} r_i^2,\\quad\n\\mathrm{MSE}_{\\mathrm{WLS}} = \\frac{1}{n} \\sum_{i=1}^{n} \\left(y_i - \\widehat{\\beta}^{\\mathrm{WLS}}_0 - \\widehat{\\beta}^{\\mathrm{WLS}}_1 x_i\\right)^2.\n$$\n\nImplementation details you must follow:\n- Use the following test suite with a fixed random seed for reproducibility. For each case, draw $x_i$ independently and uniformly as specified, generate $\\epsilon_i$ as independent Gaussian noise with mean $0$ and the specified standard deviation function, and set $y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i$.\n- Random seed: use seed value $1337$ for all random generation.\n- For each dataset, use the exact parameters:\n\n1. Case $1$ (homoscedastic, “happy path”): $n = 200$, $\\beta_0 = 1.25$, $\\beta_1 = -0.75$, $x_i \\sim \\mathrm{Uniform}[-2,2]$, noise standard deviation $\\sigma(x) \\equiv 0.5$ (constant).\n2. Case $2$ (clearly heteroscedastic, increasing with $|x|$): $n = 200$, $\\beta_0 = 0.5$, $\\beta_1 = 1.5$, $x_i \\sim \\mathrm{Uniform}[-3,3]$, noise standard deviation $\\sigma(x) = 0.3 + 0.7|x|$.\n3. Case $3$ (edge case with moderate sample size and curved variance): $n = 60$, $\\beta_0 = 0.0$, $\\beta_1 = 2.0$, $x_i \\sim \\mathrm{Uniform}[-2,2]$, noise standard deviation $\\sigma(x) = 0.2 + 0.4(x+1)^2$.\n\n- In the variance-regression step, use $\\varepsilon = 10^{-8}$.\n- In the Breusch–Pagan auxiliary regression, include an intercept and the regressor $x_i$. Use $k=1$ degrees of freedom for the chi-squared reference distribution.\n\nRequired outputs for each dataset:\n- A list of the form\n$$\n[\\text{hetero\\_boolean},\\; p\\_value,\\; \\widehat{\\beta}^{\\mathrm{OLS}}_0,\\; \\widehat{\\beta}^{\\mathrm{OLS}}_1,\\; \\widehat{\\beta}^{\\mathrm{WLS}}_0,\\; \\widehat{\\beta}^{\\mathrm{WLS}}_1,\\; \\mathrm{MSE}_{\\mathrm{OLS}},\\; \\mathrm{MSE}_{\\mathrm{WLS}}].\n$$\n- The boolean must be capitalized as in Python ($\\mathrm{True}$ or $\\mathrm{False}$). All floats must be rounded to exactly $6$ decimal places.\n\nFinal program requirements:\n- Your program must construct all datasets internally using the above test suite, perform Tasks A–C for each dataset, and produce a single line of output containing a list with three elements (one per test case), where each element is the list described above. The format must be a single line:\n\"[$\\dots$]\" with comma-separated entries and no extra text. For example: \"[[True,0.012345,1.234000, ...],[...],[...]]\".\n- No user input, no external files, and no network access are permitted. The only allowed libraries are Numerical Python (NumPy) and Scientific Python (SciPy).", "solution": "The user-provided problem is assessed to be **valid**. It is a well-posed, scientifically grounded, and objective problem in computational statistics. It outlines a standard procedure for identifying and correcting for heteroscedasticity in a simple linear regression context. All necessary parameters, models, and evaluation criteria are specified unambiguously.\n\nThe solution will be developed by systematically implementing the three specified tasks for each of the three synthetic datasets. The methodology relies on fundamental principles of linear regression, statistical testing, and weighted least squares estimation.\n\n### Principle-Based Method\n\nThe core of the problem is to address heteroscedasticity, a condition where the variance of the error term $\\epsilon_i$ is not constant across observations. The standard Ordinary Least Squares (OLS) estimator, while still unbiased under heteroscedasticity, is no longer the Best Linear Unbiased Estimator (BLUE). The provided tasks outline a common workflow to diagnose and mitigate this issue.\n\n#### 1. Data Generation and Initial OLS Fit\n\nFor each test case, we first generate a synthetic dataset according to the specified model:\n$$\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i,\\quad i=1,\\dots,n\n$$\nThe independent variable $x_i$ is drawn from a uniform distribution, and the error term $\\epsilon_i$ is drawn from a normal distribution $\\mathcal{N}(0, \\sigma^2(x_i))$, where the standard deviation $\\sigma(x_i)$ is specific to each case.\n\nWe begin by fitting this model using Ordinary Least Squares (OLS). The OLS estimators $(\\widehat{\\beta}^{\\mathrm{OLS}}_0, \\widehat{\\beta}^{\\mathrm{OLS}}_1)$ are found by minimizing the sum of squared residuals. In matrix form, with a design matrix $\\mathbf{X}$ (where the first column is all ones and the second is the vector of $x_i$ values) and response vector $\\mathbf{y}$, the OLS coefficient vector $\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{OLS}}$ is given by:\n$$\n\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{OLS}} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}\n$$\nFrom this fit, we compute the OLS residuals $r_i = y_i - (\\widehat{\\beta}^{\\mathrm{OLS}}_0 + \\widehat{\\beta}^{\\mathrm{OLS}}_1 x_i)$. These residuals are crucial for the subsequent diagnostic and modeling steps.\n\n#### 2. Task A: Heteroscedasticity Classification (Breusch-Pagan Test)\n\nThe first task is to formally test for the presence of heteroscedasticity. The Breusch-Pagan test is based on the idea that if heteroscedasticity is present, the squared residuals $r_i^2$ (which are proxies for the true error variance $\\sigma_i^2$) should be systematically related to the independent variables.\n\nWe perform an auxiliary OLS regression:\n$$\nr_i^2 = \\delta_0 + \\delta_1 x_i + u_i\n$$\nThe test statistic is the Lagrange Multiplier (LM) statistic, calculated using the coefficient of determination, $R^2$, from this auxiliary regression:\n$$\n\\mathrm{LM} = n R^2\n$$\nUnder the null hypothesis of homoscedasticity, this statistic follows a chi-squared distribution with $k$ degrees of freedom, where $k$ is the number of independent variables in the auxiliary regression (excluding the constant). In this problem, $k=1$.\n\nWe then compute the $p$-value:\n$$\np\\text{-value} = \\Pr\\left(\\chi^2_1 \\ge \\mathrm{LM}\\right)\n$$\nA small $p$-value (specifically, $p < 0.05$ as per the problem) provides evidence against the null hypothesis, leading us to classify the dataset as heteroscedastic.\n\n#### 3. Task B: Parametric Variance Function Regression\n\nIf heteroscedasticity is suspected (or even if not, for the purpose of this exercise), we proceed to model the variance function $\\sigma^2(x_i)$. The problem specifies a flexible and robust log-linear model to ensure the estimated variance is always positive. We regress the logarithm of the squared residuals on a function of $x_i$:\n$$\n\\log\\left(r_i^2 + \\varepsilon\\right) = \\gamma_0 + \\gamma_1 \\,\\log\\!\\left(1 + x_i^2\\right) + \\eta_i\n$$\nHere, $\\varepsilon = 10^{-8}$ is a small constant to prevent taking the logarithm of zero in cases where a residual is exactly zero. After estimating the parameters $\\widehat{\\gamma}_0$ and $\\widehat{\\gamma}_1$ via OLS, we can form an estimate of the variance for each observation:\n$$\n\\widehat{\\sigma}_i^2 = \\exp\\!\\left(\\widehat{\\gamma}_0 + \\widehat{\\gamma}_1 \\,\\log\\!\\left(1 + x_i^2\\right)\\right)\n$$\n\n#### 4. Task C: Weighted Least Squares (WLS) Refit\n\nWith the estimated variances $\\widehat{\\sigma}_i^2$ in hand, we can improve upon the initial OLS fit by using Weighted Least Squares (WLS). WLS accounts for heteroscedasticity by assigning a weight to each observation that is inversely proportional to its error variance. The weights are defined as $w_i = 1/\\widehat{\\sigma}_i^2$. Observations with smaller variance (and thus more information) receive higher weight.\n\nThe WLS estimator is the vector $\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{WLS}}$ that minimizes the weighted sum of squared residuals:\n$$\n\\sum_{i=1}^{n} w_i \\left(y_i - \\beta_0 - \\beta_1 x_i\\right)^2\n$$\nIn matrix form, with $\\mathbf{W}$ being a diagonal matrix of the weights $w_i$, the WLS solution is:\n$$\n\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{WLS}} = (\\mathbf{X}^T \\mathbf{W} \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{W} \\mathbf{y}\n$$\nThis estimator is efficient under the assumed form of heteroscedasticity.\n\nFinally, we compute the Mean Squared Error (MSE) for both the OLS and WLS fits to compare their performance on the original scale:\n$$\n\\mathrm{MSE}_{\\mathrm{OLS}} = \\frac{1}{n} \\sum_{i=1}^{n} r_i^2 \\quad \\text{and} \\quad \\mathrm{MSE}_{\\mathrm{WLS}} = \\frac{1}{n} \\sum_{i=1}^{n} \\left(y_i - \\widehat{\\beta}^{\\mathrm{WLS}}_0 - \\widehat{\\beta}^{\\mathrm{WLS}}_1 x_i\\right)^2\n$$\n\nThis completes the entire pipeline. The implementation will follow these steps precisely for each test case, gathering the required\neight output values: the boolean classification, the $p$-value, the two OLS coefficients, the two WLS coefficients, and the two MSE values.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef solve():\n    \"\"\"\n    Solves the problem by performing OLS, Breusch-Pagan test, variance modeling,\n    and WLS for three synthetic datasets.\n    \"\"\"\n    \n    # Define test cases as per the problem description.\n    test_cases = [\n        # Case 1: Homoscedastic\n        {'n': 200, 'beta0': 1.25, 'beta1': -0.75, 'x_range': [-2, 2], 'sigma_func': lambda x: 0.5},\n        # Case 2: Clearly heteroscedastic\n        {'n': 200, 'beta0': 0.5, 'beta1': 1.5, 'x_range': [-3, 3], 'sigma_func': lambda x: 0.3 + 0.7 * np.abs(x)},\n        # Case 3: Edge case, curved variance\n        {'n': 60, 'beta0': 0.0, 'beta1': 2.0, 'x_range': [-2, 2], 'sigma_func': lambda x: 0.2 + 0.4 * (x + 1)**2}\n    ]\n\n    # Global constants and random number generator\n    RANDOM_SEED = 1337\n    EPSILON_LOG = 1e-8\n    rng = np.random.default_rng(RANDOM_SEED)\n\n    all_results = []\n\n    for case in test_cases:\n        # --- Data Generation ---\n        n = case['n']\n        beta0_true, beta1_true = case['beta0'], case['beta1']\n        x = rng.uniform(case['x_range'][0], case['x_range'][1], size=n)\n        sigma = case['sigma_func'](x)\n        epsilon = rng.normal(0, sigma, size=n)\n        y = beta0_true + beta1_true * x + epsilon\n\n        # --- Task A: OLS and Breusch-Pagan Test ---\n        \n        # 1. Initial OLS fit\n        X_ols = np.c_[np.ones(n), x]\n        try:\n            beta_ols = np.linalg.inv(X_ols.T @ X_ols) @ X_ols.T @ y\n        except np.linalg.LinAlgError:\n            # Fallback to lstsq if matrix is singular, though unlikely for this problem.\n            beta_ols, _, _, _ = np.linalg.lstsq(X_ols, y, rcond=None)\n\n        res_ols = y - X_ols @ beta_ols\n        squared_res = res_ols**2\n        \n        # 2. Auxiliary regression for Breusch-Pagan test: r_i^2 on x_i\n        X_aux = np.c_[np.ones(n), x]\n        try:\n            delta = np.linalg.inv(X_aux.T @ X_aux) @ X_aux.T @ squared_res\n        except np.linalg.LinAlgError:\n            delta, _, _, _ = np.linalg.lstsq(X_aux, squared_res, rcond=None)\n            \n        y_pred_aux = X_aux @ delta\n        ss_res_aux = np.sum((squared_res - y_pred_aux)**2)\n        ss_tot_aux = np.sum((squared_res - np.mean(squared_res))**2)\n        \n        # Guard against ss_tot_aux being zero\n        r_squared_aux = 1 - (ss_res_aux / ss_tot_aux) if ss_tot_aux > 0 else 0.0\n\n        # 3. LM statistic and p-value\n        lm_stat = n * r_squared_aux\n        p_value = chi2.sf(lm_stat, df=1)\n        is_heteroscedastic = p_value < 0.05\n        \n        # --- Task B: Variance Function Regression ---\n        log_squared_res = np.log(squared_res + EPSILON_LOG)\n        log_x_feature = np.log(1 + x**2)\n        \n        X_var = np.c_[np.ones(n), log_x_feature]\n        try:\n            gamma = np.linalg.inv(X_var.T @ X_var) @ X_var.T @ log_squared_res\n        except np.linalg.LinAlgError:\n            gamma, _, _, _ = np.linalg.lstsq(X_var, log_squared_res, rcond=None)\n\n        sigma2_hat = np.exp(X_var @ gamma)\n        \n        # --- Task C: Weighted Least Squares Refit ---\n        weights = 1.0 / sigma2_hat\n        W = np.diag(weights)\n        \n        try:\n            # Direct WLS formula\n            X_wls = X_ols\n            beta_wls = np.linalg.inv(X_wls.T @ W @ X_wls) @ X_wls.T @ W @ y\n        except np.linalg.LinAlgError:\n            # Fallback using transformed OLS, which is more stable\n            sqrt_w = np.sqrt(weights)\n            y_prime = y * sqrt_w\n            X_prime = X_wls * sqrt_w[:, np.newaxis]\n            beta_wls, _, _, _ = np.linalg.lstsq(X_prime, y_prime, rcond=None)\n\n        # --- MSE Calculation ---\n        mse_ols = np.mean(res_ols**2)\n        res_wls = y - X_wls @ beta_wls\n        mse_wls = np.mean(res_wls**2)\n        \n        # --- Collect and Format Results ---\n        result_list = [\n            is_heteroscedastic,\n            p_value,\n            beta_ols[0],\n            beta_ols[1],\n            beta_wls[0],\n            beta_wls[1],\n            mse_ols,\n            mse_wls,\n        ]\n        all_results.append(result_list)\n\n    # Final print statement in the exact required format.\n    output_str = \"[\"\n    for i, res in enumerate(all_results):\n        formatted_res = (\n            f\"[{res[0]},\"\n            f\"{res[1]:.6f},\"\n            f\"{res[2]:.6f},\"\n            f\"{res[3]:.6f},\"\n            f\"{res[4]:.6f},\"\n            f\"{res[5]:.6f},\"\n            f\"{res[6]:.6f},\"\n            f\"{res[7]:.6f}]\"\n        )\n        output_str += formatted_res\n        if i < len(all_results) - 1:\n            output_str += \",\"\n    output_str += \"]\"\n    \n    print(output_str)\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3107027"}, {"introduction": "Choosing the right model complexity is central to machine learning, representing a trade-off between bias and variance. A model that is too complex may overfit the training data, leading to a large generalization gap—the difference between its performance on training data and on new, unseen data. This practice demonstrates a powerful technique for model selection by treating the generalization gap itself as a quantity to be modeled [@problem_id:3107026]. By regressing this gap against model complexity (polynomial degree), you will create a smoothed estimate of the expected generalization error and classify the optimal model that balances fit and complexity.", "problem": "You are given observations from polynomial regression models of degree $d$ fitted to the same dataset. For each test case, you are provided, for a set of degrees $d \\in \\mathbb{N}$, the empirical training Mean Squared Error (MSE) $E_{\\text{train}}(d)$ and the empirical cross-validation MSE $E_{\\text{val}}(d)$. Your task is to predict overfitting risk by regressing the generalization gap $g(d)$ versus $d$, and to classify the optimal degree that minimizes an estimate of the expected generalization error.\n\nFundamental base and definitions:\n- The generalization error at complexity $d$ can be expressed as $E_{\\text{gen}}(d) = E_{\\text{train}}(d) + g(d)$, where $g(d)$ is the generalization gap defined by $g(d) = E_{\\text{test}}(d) - E_{\\text{train}}(d)$. In practice, $E_{\\text{test}}(d)$ is unknown and cross-validation is used as a well-tested proxy, so we estimate the gap as $g(d) \\approx E_{\\text{val}}(d) - E_{\\text{train}}(d)$.\n- Overfitting risk increasing with model complexity is reflected by a positive slope of $g(d)$ as a function of $d$.\n\nAlgorithmic requirement:\n- For each test case, compute $g(d_k) = E_{\\text{val}}(d_k) - E_{\\text{train}}(d_k)$ at each observed degree $d_k$.\n- Fit a straight line $g(d) \\approx a + b\\,d$ to the observed pairs $\\{(d_k, g(d_k))\\}$ using Ordinary Least Squares (OLS).\n- Form the estimated expected generalization error at each observed degree as $\\widehat{E}_{\\text{gen}}(d_k) = E_{\\text{train}}(d_k) + \\widehat{a} + \\widehat{b}\\,d_k$.\n- Classify the optimal degree $\\widehat{d}^{\\star}$ as the integer degree from the provided set that minimizes $\\widehat{E}_{\\text{gen}}(d_k)$. If multiple degrees attain the same minimum within numerical tolerance, choose the smallest such degree (prefer lower complexity).\n- Report two quantities per test case: the estimated slope $\\widehat{b}$ of the gap-versus-degree regression, rounded to $4$ decimal places, and the classified optimal degree $\\widehat{d}^{\\star}$ as an integer.\n\nTest suite:\n- Test case $1$: $d = [1,2,3,4,5]$, $E_{\\text{train}} = [9.0,6.0,4.5,4.0,3.8]$, $E_{\\text{val}} = [10.0,7.0,5.0,5.2,6.0]$.\n- Test case $2$: $d = [1,2,3,4]$, $E_{\\text{train}} = [5.0,4.2,3.6,3.3]$, $E_{\\text{val}} = [5.0,4.2,3.6,3.3]$.\n- Test case $3$: $d = [1,2,3,4,5,6]$, $E_{\\text{train}} = [12.0,8.0,6.0,5.2,5.0,4.9]$, $E_{\\text{val}} = [11.5,7.3,5.1,4.1,3.7,3.4]$.\n- Test case $4$: $d = [1,10]$, $E_{\\text{train}} = [5.0,0.1]$, $E_{\\text{val}} = [5.1,6.0]$.\n- Test case $5$: $d = [2,3,4]$, $E_{\\text{train}} = [8.0,6.0,5.0]$, $E_{\\text{val}} = [8.7,6.6,5.4]$.\n\nOutput specification:\n- For each test case, output a two-element list $[\\widehat{b}, \\widehat{d}^{\\star}]$, where $\\widehat{b}$ is the slope rounded to $4$ decimal places, and $\\widehat{d}^{\\star}$ is an integer.\n- Your program should produce a single line of output containing one list that aggregates the per-test-case outputs in order, with no spaces. Each element must itself be a two-element list as described. The final printout must be exactly one line.\n\nNo physical units are involved. All angles, if any, are irrelevant here. No percentages are required.\n\nYour program must be a complete, runnable program that defines the test cases above internally and produces the specified single-line output.", "solution": "### Step 1: Extract Givens\n- **Models**: Polynomial regression models of degree $d \\in \\mathbb{N}$.\n- **Data per test case**: A set of degrees $d_k$, corresponding training Mean Squared Error (MSE) $E_{\\text{train}}(d_k)$, and cross-validation MSE $E_{\\text{val}}(d_k)$.\n- **Definitions**:\n    - Generalization error: $E_{\\text{gen}}(d) = E_{\\text{train}}(d) + g(d)$.\n    - Generalization gap: $g(d) = E_{\\text{test}}(d) - E_{\\text{train}}(d)$.\n    - Empirical generalization gap estimate: $g(d) \\approx E_{\\text{val}}(d) - E_{\\text{train}}(d)$.\n- **Algorithmic Requirements**:\n    1.  Compute the empirical gap for each degree: $g(d_k) = E_{\\text{val}}(d_k) - E_{\\text{train}}(d_k)$.\n    2.  Fit a straight line $g(d) \\approx a + b\\,d$ to the data points $\\{(d_k, g(d_k))\\}$ using Ordinary Least Squares (OLS) to find the estimated intercept $\\widehat{a}$ and slope $\\widehat{b}$.\n    3.  Estimate the expected generalization error at each degree: $\\widehat{E}_{\\text{gen}}(d_k) = E_{\\text{train}}(d_k) + \\widehat{a} + \\widehat{b}\\,d_k$.\n    4.  Classify the optimal degree $\\widehat{d}^{\\star}$ as the integer degree from the provided set that minimizes $\\widehat{E}_{\\text{gen}}(d_k)$. The tie-breaking rule is to choose the smallest degree if multiple degrees yield the same minimum.\n    5.  Report the slope $\\widehat{b}$ rounded to $4$ decimal places and the integer optimal degree $\\widehat{d}^{\\star}$.\n- **Test Suite**:\n    - **Test case 1**: $d = [1,2,3,4,5]$, $E_{\\text{train}} = [9.0,6.0,4.5,4.0,3.8]$, $E_{\\text{val}} = [10.0,7.0,5.0,5.2,6.0]$.\n    - **Test case 2**: $d = [1,2,3,4]$, $E_{\\text{train}} = [5.0,4.2,3.6,3.3]$, $E_{\\text{val}} = [5.0,4.2,3.6,3.3]$.\n    - **Test case 3**: $d = [1,2,3,4,5,6]$, $E_{\\text{train}} = [12.0,8.0,6.0,5.2,5.0,4.9]$, $E_{\\text{val}} = [11.5,7.3,5.1,4.1,3.7,3.4]$.\n    - **Test case 4**: $d = [1,10]$, $E_{\\text{train}} = [5.0,0.1]$, $E_{\\text{val}} = [5.1,6.0]$.\n    - **Test case 5**: $d = [2,3,4]$, $E_{\\text{train}} = [8.0,6.0,5.0]$, $E_{\\text{val}} = [8.7,6.6,5.4]$.\n- **Output Specification**: A single-line list of two-element lists $[\\widehat{b}, \\widehat{d}^{\\star}]$ for each test case, with no spaces in the final output string.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is evaluated against the validation criteria.\n- **Scientifically Grounded**: The problem is based on fundamental concepts in statistical learning theory, including the bias-variance trade-off, overfitting, training error, validation error, and generalization error. The use of Ordinary Least Squares to model the relationship between model complexity and the generalization gap is a standard and sound analytical technique.\n- **Well-Posed**: The problem is well-posed. For each test case, the number of distinct data points for the OLS regression is at least $2$, ensuring a unique solution for the line parameters. The search for the optimal degree is a minimization over a finite set, which is guaranteed to have a solution. The tie-breaking rule ensures this solution is unique.\n- **Objective**: The problem is stated using precise mathematical definitions and a clear, unambiguous algorithmic procedure. The inputs are numerical, and the required outputs are well-defined.\n- **Flaw Checklist**: The problem does not violate any of the specified flaws. It is scientifically sound, formalizable, complete, realistic, and well-posed. It requires non-trivial computation and reasoning.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\nThe problem requires an analysis of model performance as a function of complexity, specifically the degree $d$ of a polynomial regression model. The core of the analysis lies in understanding and modeling the generalization gap, $g(d)$, which represents the difference between a model's performance on unseen data (approximated by validation error, $E_{\\text{val}}$) and its performance on the training data ($E_{\\text{train}}$). A growing gap with increasing complexity is a hallmark of overfitting.\n\nThe specified algorithm aims to create a smoothed estimate of the generalization error to make a more robust model selection decision than simply picking the model with the lowest validation error. The validation error itself can be noisy, and modeling the trend of the generalization gap can help filter out this noise.\n\nThe procedure for each test case is as follows:\n\n1.  **Compute the Empirical Generalization Gap**: For each given polynomial degree $d_k$, we calculate the empirical generalization gap, $g(d_k)$, using the provided training and validation errors:\n    $$g(d_k) = E_{\\text{val}}(d_k) - E_{\\text{train}}(d_k)$$\n\n2.  **Model the Generalization Gap via OLS**: We hypothesize a linear relationship between the model complexity $d$ and the generalization gap $g(d)$. We fit a line, $\\widehat{g}(d) = \\widehat{a} + \\widehat{b}d$, to the set of observed points $\\{(d_k, g(d_k))\\}$ using Ordinary Least Squares (OLS). The slope $\\widehat{b}$ and intercept $\\widehat{a}$ are chosen to minimize the sum of squared differences $\\sum_k (g(d_k) - (\\widehat{a} + \\widehat{b}d_k))^2$. For a set of $n$ points, the OLS estimators are given by:\n    $$ \\widehat{b} = \\frac{\\sum_{k=1}^{n} (d_k - \\bar{d})(g_k - \\bar{g})}{\\sum_{k=1}^{n} (d_k - \\bar{d})^2} $$\n    $$ \\widehat{a} = \\bar{g} - \\widehat{b}\\bar{d} $$\n    where $\\bar{d} = \\frac{1}{n}\\sum_k d_k$ and $\\bar{g} = \\frac{1}{n}\\sum_k g(d_k)$ are the sample means. The slope $\\widehat{b}$ serves as a direct measure of overfitting risk; a positive $\\widehat{b}$ indicates that the gap between training and validation performance widens as model complexity increases.\n\n3.  **Estimate the Generalization Error**: Using the linear model for the gap, we construct a smoothed estimate of the true generalization error, $\\widehat{E}_{\\text{gen}}(d_k)$:\n    $$ \\widehat{E}_{\\text{gen}}(d_k) = E_{\\text{train}}(d_k) + \\widehat{g}(d_k) = E_{\\text{train}}(d_k) + \\widehat{a} + \\widehat{b}d_k $$\n    This estimate combines the directly observed training error, which reflects how well the model fits the data, with a regularized estimate of the generalization penalty, which accounts for complexity.\n\n4.  **Determine the Optimal Degree**: The optimal degree, $\\widehat{d}^{\\star}$, is selected as the degree from the given set $\\{d_k\\}$ that minimizes our estimated generalization error, $\\widehat{E}_{\\text{gen}}(d_k)$.\n    $$ \\widehat{d}^{\\star} = \\underset{d_k}{\\arg\\min} \\{ \\widehat{E}_{\\text{gen}}(d_k) \\} $$\n    The problem specifies that if a tie occurs, the smallest degree among those that achieve the minimum error should be chosen. This reflects the principle of parsimony (Occam's razor): when all else is equal, prefer the simpler model.\n\nFinally, for each test case, we report the calculated slope $\\widehat{b}$ (a measure of overfitting risk) and the determined optimal degree $\\widehat{d}^{\\star}$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the model selection problem for all test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1\n        {'d': [1, 2, 3, 4, 5], \n         'E_train': [9.0, 6.0, 4.5, 4.0, 3.8], \n         'E_val': [10.0, 7.0, 5.0, 5.2, 6.0]},\n        # Test case 2\n        {'d': [1, 2, 3, 4], \n         'E_train': [5.0, 4.2, 3.6, 3.3], \n         'E_val': [5.0, 4.2, 3.6, 3.3]},\n        # Test case 3\n        {'d': [1, 2, 3, 4, 5, 6], \n         'E_train': [12.0, 8.0, 6.0, 5.2, 5.0, 4.9], \n         'E_val': [11.5, 7.3, 5.1, 4.1, 3.7, 3.4]},\n        # Test case 4\n        {'d': [1, 10], \n         'E_train': [5.0, 0.1], \n         'E_val': [5.1, 6.0]},\n        # Test case 5\n        {'d': [2, 3, 4], \n         'E_train': [8.0, 6.0, 5.0], \n         'E_val': [8.7, 6.6, 5.4]},\n    ]\n\n    results_as_strings = []\n    \n    for case in test_cases:\n        # Extract and convert data to numpy arrays for vectorized operations\n        d = np.array(case['d'], dtype=float)\n        E_train = np.array(case['E_train'], dtype=float)\n        E_val = np.array(case['E_val'], dtype=float)\n        \n        # Step 1: Compute the empirical generalization gap\n        g = E_val - E_train\n        \n        # Step 2: Fit a straight line g(d) = a + b*d using OLS.\n        # np.polyfit with degree 1 returns the coefficients [b, a] for slope and intercept.\n        b_hat, a_hat = np.polyfit(d, g, 1)\n        \n        # Step 3: Form the estimated expected generalization error\n        # E_gen_hat(d) = E_train(d) + (a_hat + b_hat*d)\n        E_gen_hat = E_train + a_hat + b_hat * d\n        \n        # Step 4: Classify the optimal degree d_star\n        # np.argmin finds the index of the first occurrence of the minimum value.\n        # Since the input degrees 'd' are sorted, this satisfies the tie-breaking rule\n        # of choosing the smallest degree.\n        min_error_index = np.argmin(E_gen_hat)\n        d_star = int(d[min_error_index])\n        \n        # Format the result for this case as a string \"[b,d_star]\" with no spaces\n        # and b rounded to 4 decimal places.\n        case_result_str = f\"[{b_hat:.4f},{d_star}]\"\n        results_as_strings.append(case_result_str)\n\n    # Final print statement in the exact required format.\n    # The output is a single list containing the results for all test cases.\n    print(f\"[{','.join(results_as_strings)}]\")\n\nsolve()\n```", "id": "3107026"}, {"introduction": "In modern data science, we often face high-dimensional problems where the number of features far exceeds the number of observations. In such scenarios, standard regression fails, and regularization becomes essential. This practice delves into the LASSO method for sparse signal recovery, a powerful technique that simultaneously performs feature selection and regression [@problem_id:3106944]. You will engage in two tasks: first, classifying reconstructions as under-regularized, over-regularized, or well-balanced based on their sparsity and data fidelity, and second, using regression to build a predictive model for the optimal regularization parameter, $\\lambda$, itself.", "problem": "You are given a sparse linear inverse problem from compressed sensing, modeled as $y = A x^{\\star} + e$, where $A \\in \\mathbb{R}^{m \\times n}$ is a known sensing matrix with $m < n$, $x^{\\star} \\in \\mathbb{R}^{n}$ is a $k$-sparse signal, and $e \\in \\mathbb{R}^{m}$ is additive noise with independent and identically distributed Gaussian entries of mean $0$ and variance $\\sigma^2$. The reconstruction $\\hat{x} \\in \\mathbb{R}^{n}$ is obtained by solving the Least Absolute Shrinkage and Selection Operator (LASSO) problem\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\ \\frac{1}{2} \\|A x - y\\|_2^2 + \\lambda \\|x\\|_1,\n$$\nwhere $\\lambda > 0$ is the regularization parameter. You will use the residual norm $r(\\hat{x}) = \\|A \\hat{x} - y\\|_2$ and the sparsity level $s(\\hat{x}) = \\|\\hat{x}\\|_0$ (the number of nonzero entries) to classify reconstructions as under- or over-regularized, and you will also regress an approximation to the optimal $\\lambda$ using a data-driven linear model.\n\nFundamental bases:\n- The forward model $y = A x^{\\star} + e$ and definitions of the norms $\\|\\cdot\\|_2$, $\\|\\cdot\\|_1$, and $\\|\\cdot\\|_0$.\n- The LASSO objective above and the fact that the gradient of the data-fit term $\\frac{1}{2}\\|A x - y\\|_2^2$ is $A^{\\top}(A x - y)$.\n- Basic Gaussian noise concentration indicating that a consistency threshold for the residual can be based on $\\|e\\|_2 \\approx \\sigma \\sqrt{m}$ for moderate $m$.\n\nPart A (Classification task):\n- A reconstruction is considered under-regularized if it fits the noise too closely and is insufficiently sparse. Operationally, declare a reconstruction under-regularized if both $r(\\hat{x}) < \\tau_r$ and $s(\\hat{x}) > k_{\\mathrm{ref}}$.\n- A reconstruction is considered over-regularized if it is too sparse to fit the data within the expected noise level. Operationally, declare a reconstruction over-regularized if both $r(\\hat{x}) > \\tau_r$ and $s(\\hat{x}) \\le k_{\\mathrm{ref}}$.\n- Otherwise, declare the reconstruction well-balanced.\n- Use the residual threshold $\\tau_r = c_r \\, \\sigma \\sqrt{m}$ with $c_r = 1.15$, and use $k_{\\mathrm{ref}} = k^{\\star}$, the true sparsity used to generate $x^{\\star}$.\n- Encode the classification as an integer label: under-regularized $\\to -1$, over-regularized $\\to +1$, well-balanced $\\to 0$.\n\nImplement the reconstructions using the Iterative Shrinkage-Thresholding Algorithm (ISTA), which is a gradient-descent method with soft-thresholding for LASSO. Let $L$ denote the Lipschitz constant of the gradient of the data-fit term, which equals the largest eigenvalue of $A^{\\top}A$, equivalently the square of the spectral norm of $A$. With a step size $\\alpha \\in (0, 1/L]$, the ISTA iteration is\n$$\nx^{(t+1)} = \\mathcal{S}_{\\alpha \\lambda}\\Big(x^{(t)} - \\alpha A^{\\top}(A x^{(t)} - y)\\Big),\n$$\nwhere $\\mathcal{S}_{\\theta}(z) = \\mathrm{sign}(z)\\max\\{|z| - \\theta, 0\\}$ is applied elementwise.\n\nUse the following single data instance for classification:\n- Measurement dimensions: $m = 40$, $n = 80$.\n- True sparsity: $k^{\\star} = 5$.\n- Noise level: $\\sigma = 0.02$.\n- Random seed: $42$ for matrix, signal, and noise generation.\n- Sensing matrix $A$: entries drawn from a standard normal distribution and then column-normalized to unit $\\ell_2$-norm.\n- True signal $x^{\\star}$: support chosen uniformly at random of size $k^{\\star}$; nonzero entries drawn from a standard normal distribution.\n- Noise $e$: entries drawn independently from a normal distribution with mean $0$ and variance $\\sigma^2$.\n- Measurements: $y = A x^{\\star} + e$.\n- Candidate regularization parameters to test: $\\lambda \\in \\{ 0.001, 0.02, 0.2 \\}$.\n\nFor each of the three $\\lambda$ values above, reconstruct $\\hat{x}$ using ISTA and classify each reconstruction using the rule described. This produces three integer labels.\n\nPart B (Regression task):\n- Your goal is to learn a simple linear regression model to predict the approximately optimal regularization parameter $\\lambda^{\\star}$ from problem features. For a given instance, define the near-optimal $\\lambda^{\\star}$ as the minimizer over a small grid $\\Lambda$ of the reconstruction error\n$$\n\\mathrm{RMSE}(\\lambda) = \\frac{\\| \\hat{x}_{\\lambda} - x^{\\star} \\|_2}{\\sqrt{n}},\n$$\nwhere $\\hat{x}_{\\lambda}$ is the ISTA solution at regularization $\\lambda$ on that instance. Evaluate over the grid\n$$\n\\Lambda = \\{ 0.005, 0.01, 0.02, 0.05, 0.1, 0.2 \\}.\n$$\n- Use a linear model\n$$\n\\lambda^{\\star} \\approx \\beta_0 + \\beta_1 \\, \\sigma + \\beta_2 \\, \\sqrt{2 \\log n} + \\beta_3 \\, \\frac{m}{n},\n$$\nfit by ordinary least squares on a training set of simulated instances constructed analogously to Part A (same generative process), with the following training suite:\n    - Case $1$: $(m, n, k^{\\star}, \\sigma, \\text{seed}) = (40, 80, 5, 0.01, 1)$\n    - Case $2$: $(m, n, k^{\\star}, \\sigma, \\text{seed}) = (40, 80, 5, 0.05, 2)$\n    - Case $3$: $(m, n, k^{\\star}, \\sigma, \\text{seed}) = (40, 120, 5, 0.02, 3)$\n    - Case $4$: $(m, n, k^{\\star}, \\sigma, \\text{seed}) = (40, 60, 5, 0.03, 4)$\n    - Case $5$: $(m, n, k^{\\star}, \\sigma, \\text{seed}) = (40, 100, 5, 0.04, 5)$\n    - Case $6$: $(m, n, k^{\\star}, \\sigma, \\text{seed}) = (40, 90, 5, 0.015, 6)$\n- After fitting $(\\beta_0, \\beta_1, \\beta_2, \\beta_3)$, predict $\\lambda$ on the following three test instances (generated analogously):\n    - Test $1$: $(m, n, k^{\\star}, \\sigma, \\text{seed}) = (40, 85, 5, 0.025, 101)$\n    - Test $2$: $(m, n, k^{\\star}, \\sigma, \\text{seed}) = (40, 110, 5, 0.035, 102)$\n    - Test $3$: $(m, n, k^{\\star}, \\sigma, \\text{seed}) = (40, 70, 5, 0.012, 103)$\n- Ensure predicted $\\lambda$ values are strictly positive by clipping at a small positive floor if needed.\n\nAngle units do not apply. There are no physical units to report for the outputs.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must be ordered as\n$$\n[\\ \\ell_1, \\ \\ell_2, \\ \\ell_3, \\ \\hat{\\lambda}_1, \\ \\hat{\\lambda}_2, \\ \\hat{\\lambda}_3 \\ ],\n$$\nwhere $\\ell_i$ are the integer classification labels for the three $\\lambda$ in Part A (in the order $\\lambda = 0.001, 0.02, 0.2$), and $\\hat{\\lambda}_j$ are the three regression predictions for the test instances in Part B, each rounded to $6$ decimal places. For example, an admissible output would look like $[ -1,0,1,0.012345,0.067890,0.004321 ]$. The exact numeric values must be computed by your program by following the procedures above.", "solution": "The user's request is a two-part computational problem involving classification and regression tasks in the context of sparse signal recovery using the LASSO method.\n\nThe problem is first validated to be scientifically grounded, well-posed, objective, and complete. It is based on standard principles of compressed sensing, numerical optimization, and statistical learning. The procedures and parameters are specified in sufficient detail to allow for a unique and verifiable solution.\n\nThe solution is implemented in two main parts, corresponding to the two tasks described.\n\n**Part A: Classification of LASSO Reconstructions**\n\nThis part involves classifying reconstructions obtained for three different values of the regularization parameter $\\lambda$.\n\n1.  **Data Generation**: A single instance of a sparse recovery problem is generated according to the specified parameters: dimensions $m=40$, $n=80$; true sparsity $k^{\\star}=5$; and noise level $\\sigma=0.02$. A random number generator seeded with $42$ ensures reproducibility. The process involves:\n    *   Creating a sensing matrix $A \\in \\mathbb{R}^{40 \\times 80}$ with entries drawn from a standard normal distribution, followed by column normalization to unit $\\ell_2$-norm.\n    *   Generating a $k^{\\star}$-sparse signal $x^{\\star} \\in \\mathbb{R}^{80}$ with its support chosen uniformly at random and non-zero values drawn from a standard normal distribution.\n    *   Adding Gaussian noise $e \\in \\mathbb{R}^{40}$ with mean $0$ and standard deviation $\\sigma=0.02$ to form the measurement vector $y = A x^{\\star} + e$.\n\n2.  **Signal Reconstruction**: For each candidate regularization parameter $\\lambda \\in \\{0.001, 0.02, 0.2\\}$, the corresponding signal estimate $\\hat{x}$ is computed by solving the LASSO optimization problem. The chosen algorithm is the Iterative Shrinkage-Thresholding Algorithm (ISTA), a first-order method. The ISTA update rule is given by:\n    $$\n    x^{(t+1)} = \\mathcal{S}_{\\alpha \\lambda}\\Big(x^{(t)} - \\alpha A^{\\top}(A x^{(t)} - y)\\Big)\n    $$\n    Here, $\\mathcal{S}_{\\theta}(z) = \\mathrm{sign}(z)\\max\\{|z| - \\theta, 0\\}$ is the element-wise soft-thresholding operator. The step size $\\alpha$ is set to $1/L$, where $L$ is the Lipschitz constant of the gradient of the data-fit term. $L$ is the largest eigenvalue of $A^{\\top}A$, which is equivalent to the squared spectral norm of $A$, i.e., $L = \\|A\\|_2^2$. A fixed number of $5000$ iterations is used to ensure convergence.\n\n3.  **Classification**: Each reconstruction $\\hat{x}$ is classified based on two metrics: its residual norm, $r(\\hat{x}) = \\|A \\hat{x} - y\\|_2$, and its sparsity level, $s(\\hat{x}) = \\|\\hat{x}\\|_0$. The sparsity is computed by counting the number of elements with magnitude greater than a small threshold of $10^{-8}$. The classification rules use a residual threshold $\\tau_r = c_r \\sigma \\sqrt{m}$ with $c_r = 1.15$ and a sparsity reference $k_{\\mathrm{ref}} = k^{\\star}$.\n    *   A label of $-1$ (under-regularized) is assigned if $r(\\hat{x}) < \\tau_r$ and $s(\\hat{x}) > k_{\\mathrm{ref}}$.\n    *   A label of $+1$ (over-regularized) is assigned if $r(\\hat{x}) > \\tau_r$ and $s(\\hat{x}) \\le k_{\\mathrm{ref}}$.\n    *   A label of $0$ (well-balanced) is assigned otherwise.\n    This process yields three integer labels, one for each value of $\\lambda$.\n\n**Part B: Regression Model for Optimal Regularization**\n\nThis part involves learning a linear model to predict an approximately optimal $\\lambda$ based on problem characteristics.\n\n1.  **Training Data Generation and Ground Truth**: A training set of six instances is generated, with parameters $(m, n, k^{\\star}, \\sigma, \\text{seed})$ provided in the problem. For each instance, we determine the \"ground-truth\" optimal regularization parameter, $\\lambda^{\\star}$. This is done by performing a grid search over the set $\\Lambda = \\{0.005, 0.01, 0.02, 0.05, 0.1, 0.2\\}$. For each $\\lambda \\in \\Lambda$, the signal $\\hat{x}_{\\lambda}$ is reconstructed using ISTA, and the one that minimizes the root-mean-squared error, $\\mathrm{RMSE}(\\lambda) = \\|\\hat{x}_{\\lambda} - x^{\\star}\\|_2 / \\sqrt{n}$, is chosen as $\\lambda^{\\star}$.\n\n2.  **Linear Model Fitting**: A linear regression model is fitted to predict $\\lambda^{\\star}$. The model form is:\n    $$\n    \\lambda^{\\star} \\approx \\beta_0 + \\beta_1 \\, \\sigma + \\beta_2 \\, \\sqrt{2 \\log n} + \\beta_3 \\, \\frac{m}{n}\n    $$\n    For each of the six training instances, a feature vector $[1, \\sigma, \\sqrt{2 \\log n}, m/n]$ and the corresponding target $\\lambda^{\\star}$ are generated. This creates a design matrix $Z \\in \\mathbb{R}^{6 \\times 4}$ and a target vector $\\vec{\\lambda}^{\\star} \\in \\mathbb{R}^{6}$. The coefficient vector $\\beta = [\\beta_0, \\beta_1, \\beta_2, \\beta_3]^{\\top}$ is estimated using Ordinary Least Squares (OLS) by solving the normal equations, which is implemented via `numpy.linalg.lstsq`.\n\n3.  **Prediction**: The trained model is used to predict the optimal $\\lambda$ for three new test instances. For each test instance, its feature vector is constructed, and the prediction is calculated as $\\hat{\\lambda} = \\beta_0 + \\beta_1 \\sigma + \\beta_2 \\sqrt{2 \\log n} + \\beta_3 (m/n)$. To ensure physical plausibility, the predicted value is clipped at a small positive floor ($10^{-9}$), i.e., $\\hat{\\lambda} = \\max(\\hat{\\lambda}, 10^{-9})$.\n\nFinally, the three classification labels from Part A and the three rounded regression predictions from Part B are concatenated into a single list to produce the final output, formatted as specified.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves a two-part problem on LASSO reconstruction: classification and regression.\n    \"\"\"\n\n    # ========= Helper Functions =========\n    \n    def soft_threshold(z, theta):\n        \"\"\"Element-wise soft-thresholding operator.\"\"\"\n        return np.sign(z) * np.maximum(np.abs(z) - theta, 0)\n\n    def ista_solve(A, y, lambda_reg, max_iter=5000):\n        \"\"\"\n        Solves the LASSO problem using the Iterative Shrinkage-Thresholding Algorithm (ISTA).\n        \"\"\"\n        m, n = A.shape\n        # The Lipschitz constant L is the squared spectral norm of A.\n        L = np.linalg.norm(A, 2)**2\n        # The step size alpha must be in (0, 1/L]. We choose the largest safe value.\n        alpha = 1.0 / L\n        \n        # Initialize the solution vector x\n        x = np.zeros(n)\n        \n        # ISTA iterations\n        for _ in range(max_iter):\n            grad = A.T @ (A @ x - y)\n            x_update = x - alpha * grad\n            x = soft_threshold(x_update, alpha * lambda_reg)\n            \n        return x\n\n    def generate_instance(m, n, k_star, sigma, seed):\n        \"\"\"\n        Generates a sparse signal recovery problem instance (A, x_star, y).\n        \"\"\"\n        rng = np.random.default_rng(seed)\n        \n        # Generate sensing matrix A with normalized columns\n        A = rng.standard_normal(size=(m, n))\n        A /= np.linalg.norm(A, axis=0)\n        \n        # Generate k-sparse signal x_star\n        x_star = np.zeros(n)\n        support = rng.choice(n, size=k_star, replace=False)\n        x_star[support] = rng.standard_normal(size=k_star)\n        \n        # Generate noise e and measurements y\n        e = rng.standard_normal(size=m) * sigma\n        y = A @ x_star + e\n        \n        return A, x_star, y\n\n    # ========= Part A: Classification =========\n\n    def solve_part_a():\n        \"\"\"\n        Performs classification of LASSO reconstructions.\n        \"\"\"\n        m, n, k_star, sigma, seed = 40, 80, 5, 0.02, 42\n        lambda_candidates = [0.001, 0.02, 0.2]\n        \n        A, _, y = generate_instance(m, n, k_star, sigma, seed)\n        \n        c_r = 1.15\n        tau_r = c_r * sigma * np.sqrt(m)\n        k_ref = k_star\n        \n        labels = []\n        for lambda_val in lambda_candidates:\n            x_hat = ista_solve(A, y, lambda_val)\n            \n            r_hat = np.linalg.norm(A @ x_hat - y)\n            s_hat = np.sum(np.abs(x_hat) > 1e-8)\n            \n            if r_hat < tau_r and s_hat > k_ref:\n                label = -1  # Under-regularized\n            elif r_hat > tau_r and s_hat <= k_ref:\n                label = 1   # Over-regularized\n            else:\n                label = 0   # Well-balanced\n            \n            labels.append(label)\n            \n        return labels\n\n    # ========= Part B: Regression =========\n\n    def solve_part_b():\n        \"\"\"\n        Trains a linear model to predict the optimal lambda and makes predictions.\n        \"\"\"\n        training_suite = [\n            (40, 80, 5, 0.01, 1),\n            (40, 80, 5, 0.05, 2),\n            (40, 120, 5, 0.02, 3),\n            (40, 60, 5, 0.03, 4),\n            (40, 100, 5, 0.04, 5),\n            (40, 90, 5, 0.015, 6)\n        ]\n        lambda_grid = [0.005, 0.01, 0.02, 0.05, 0.1, 0.2]\n        \n        # --- Training Phase ---\n        feature_matrix = []\n        target_lambdas = []\n        \n        for m, n, k_star, sigma, seed in training_suite:\n            A, x_star, y = generate_instance(m, n, k_star, sigma, seed)\n            \n            min_rmse = float('inf')\n            best_lambda = -1\n            \n            for lambda_val in lambda_grid:\n                x_hat = ista_solve(A, y, lambda_val)\n                rmse = np.linalg.norm(x_hat - x_star) / np.sqrt(n)\n                if rmse < min_rmse:\n                    min_rmse = rmse\n                    best_lambda = lambda_val\n            \n            features = [1.0, sigma, np.sqrt(2 * np.log(n)), m / n]\n            feature_matrix.append(features)\n            target_lambdas.append(best_lambda)\n            \n        Z = np.array(feature_matrix)\n        targets = np.array(target_lambdas)\n        beta, _, _, _ = np.linalg.lstsq(Z, targets, rcond=None)\n        \n        # --- Prediction Phase ---\n        test_suite = [\n            (40, 85, 5, 0.025, 101),\n            (40, 110, 5, 0.035, 102),\n            (40, 70, 5, 0.012, 103)\n        ]\n        \n        predictions = []\n        for m, n, k_star, sigma, _ in test_suite:\n            test_features = np.array([1.0, sigma, np.sqrt(2 * np.log(n)), m / n])\n            lambda_pred = test_features @ beta\n            lambda_pred = max(lambda_pred, 1e-9)\n            predictions.append(round(lambda_pred, 6))\n            \n        return predictions\n\n    # ========= Main Execution =========\n    \n    part_a_results = solve_part_a()\n    part_b_results = solve_part_b()\n    \n    final_results = part_a_results + part_b_results\n    \n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n```", "id": "3106944"}]}