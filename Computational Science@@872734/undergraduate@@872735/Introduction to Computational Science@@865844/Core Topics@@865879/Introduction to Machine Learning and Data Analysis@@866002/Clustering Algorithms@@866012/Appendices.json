{"hands_on_practices": [{"introduction": "The $k$-means algorithm is fundamentally an iterative process that refines cluster assignments and centroids until a stable state is reached. A crucial question in any practical implementation is deciding when to stop these iterations. In this exercise, you will explore this question by deriving and comparing two of the most common stopping criteria: the stabilization of cluster assignments and the diminishing returns in the reduction of the objective function, $J$. This practice [@problem_id:3107749] provides deep insight into the convergence behavior of $k$-means and the practical trade-offs between computational effort and the precision of the final clustering solution.", "problem": "You are tasked with deriving principled stopping criteria for the $k$-means clustering algorithm and implementing a program to compare their iteration counts on a small test suite. Work from first principles suitable for Introduction to Computational Science at the intermediate undergraduate level.\n\nStart from the following fundamental base:\n- The Euclidean norm on $\\mathbb{R}^d$ is defined by $\\|\\boldsymbol{x}\\| = \\sqrt{\\sum_{j=1}^d x_j^2}$ and the squared Euclidean distance between points $x$ and $y$ is $\\|x - y\\|^2$.\n- For a given partition (assignment) of points $\\{x_i\\}_{i=1}^n$ into $k$ clusters, the within-cluster sum of squared distances objective is\n$$\nJ(\\{\\mu_j\\}_{j=1}^k, \\{a_i\\}_{i=1}^n) = \\sum_{i=1}^n \\left\\| x_i - \\mu_{a_i} \\right\\|^2,\n$$\nwhere $a_i \\in \\{1,\\dots,k\\}$ is the cluster index assigned to $x_i$ and $\\mu_j \\in \\mathbb{R}^d$ is the center of cluster $j$.\n- For fixed assignments, the minimizer of the objective over $\\mu_j$ is the arithmetic mean of the points in each cluster, that is, if $C_j = \\{ i : a_i = j \\}$, then $\\mu_j^\\star = \\frac{1}{|C_j|} \\sum_{i \\in C_j} x_i$, provided $|C_j| > 0$.\n\nUsing these facts, first explain why the standard $k$-means iterative procedure consisting of alternating between:\n(1) assigning each point to its nearest current center (under squared Euclidean distance), and\n(2) updating each center to the mean of its assigned points,\nproduces a sequence of objective values $\\{J_t\\}_{t \\ge 0}$ that is monotonically nonincreasing and bounded below by $0$. From this, derive and justify a stopping criterion based on relative improvement per iteration:\n$$\n\\frac{\\Delta J_t}{J_{t-1}} < \\epsilon,\n$$\nwhere $\\Delta J_t = J_{t-1} - J_t$, and $\\epsilon > 0$ is a user-specified tolerance. To ensure numerical robustness when $J_{t-1}$ is $0$, use a denominator of $\\max(J_{t-1}, \\delta)$ with a small positive constant $\\delta$. Separately, derive and justify a stopping criterion based on assignment stability, defined as no change in any $a_i$ between two consecutive iterations.\n\nImplement both stopping criteria and compare their iteration counts on the following test suite. In all cases, use $k$-means with squared Euclidean distance and deterministic $k$-means++ initialization with a fixed random seed to ensure reproducibility. Count iterations as complete assignment-update cycles. For the relative improvement count, evaluate the ratio using the sequence from the run that proceeds until assignment stability; if no ratio satisfies the inequality before stability is reached, report the assignment stability iteration count for the relative-improvement-based count.\n\nLet $\\delta = 10^{-12}$ and $T_{\\max}$ be the maximum number of iterations beyond which the algorithm should stop to prevent infinite loops.\n\nTest Suite:\n- Test case $1$ (well-separated clusters):\n  - Data points in $\\mathbb{R}^2$: $x_1 = (0,0)$, $x_2 = (0.4,-0.2)$, $x_3 = (-0.3,0.1)$, $x_4 = (5.0,5.0)$, $x_5 = (5.2,4.7)$, $x_6 = (4.8,5.3)$, $x_7 = (5.1,5.1)$, $x_8 = (-0.2,-0.1)$.\n  - Number of clusters: $k = 2$.\n  - Relative improvement tolerance: $\\epsilon = 10^{-4}$.\n  - Maximum iterations: $T_{\\max} = 100$.\n- Test case $2$ (slow improvement due to overlapping clusters):\n  - Data points in $\\mathbb{R}^2$: $x_1 = (0.0,0.0)$, $x_2 = (0.1,0.0)$, $x_3 = (0.0,0.1)$, $x_4 = (0.2,0.2)$, $x_5 = (0.3,0.3)$, $x_6 = (1.0,1.0)$, $x_7 = (1.1,1.2)$, $x_8 = (0.9,1.05)$, $x_9 = (1.2,0.9)$, $x_{10} = (1.05,1.1)$.\n  - Number of clusters: $k = 2$.\n  - Relative improvement tolerance: $\\epsilon = 10^{-6}$.\n  - Maximum iterations: $T_{\\max} = 200$.\n- Test case $3$ (degenerate case with identical points):\n  - Data points in $\\mathbb{R}^2$: $x_1 = (1.0,1.0)$, $x_2 = (1.0,1.0)$, $x_3 = (1.0,1.0)$, $x_4 = (1.0,1.0)$, $x_5 = (1.0,1.0)$, $x_6 = (1.0,1.0)$.\n  - Number of clusters: $k = 3$.\n  - Relative improvement tolerance: $\\epsilon = 10^{-9}$.\n  - Maximum iterations: $T_{\\max} = 50$.\n\nImplementation details to ensure scientific realism and reproducibility:\n- Use deterministic $k$-means++ initialization with a fixed seed, selecting the first center uniformly at random and subsequent centers with probability proportional to squared distance to the nearest already chosen center.\n- Use squared Euclidean distances throughout.\n- After each assignment step, if any cluster is empty, reassign a deterministic selection of points to empty clusters to ensure each cluster has at least one point before updating centers. A deterministic rule is required; for example, for each empty cluster, move the point with the largest index among those currently assigned to the largest cluster. This guarantees progress without introducing randomness.\n- Define $J_t$ as the objective value computed after the update step in iteration $t$ with the then-current assignments and centers.\n- Define the iteration count as the number of assignment-update cycles executed.\n\nYour program should compute, for each test case, two integers: the number of iterations until the relative improvement criterion first holds and the number of iterations until assignment stability is reached. The final output must be a single line containing a comma-separated list enclosed in square brackets, flattening the pairs in test suite order as\n$[i_{1,\\mathrm{rel}}, i_{1,\\mathrm{stab}}, i_{2,\\mathrm{rel}}, i_{2,\\mathrm{stab}}, i_{3,\\mathrm{rel}}, i_{3,\\mathrm{stab}}]$.", "solution": "The problem asks for a principled derivation of two stopping criteria for the $k$-means algorithm and an implementation to compare their performance on a given test suite. The problem is well-posed, scientifically sound, and provides sufficient detail for a unique, reproducible solution.\n\nFirst, we derive and justify the properties of the $k$-means algorithm and the stopping criteria from the provided first principles.\n\nThe $k$-means algorithm is an iterative procedure designed to partition a set of $n$ data points $\\{x_i\\}_{i=1}^n$ in $\\mathbb{R}^d$ into $k$ disjoint clusters. It aims to minimize the within-cluster sum of squared distances (WCSS), given by the objective function:\n$$\nJ(\\{\\mu_j\\}_{j=1}^k, \\{a_i\\}_{i=1}^n) = \\sum_{i=1}^n \\left\\| x_i - \\mu_{a_i} \\right\\|^2\n$$\nwhere $a_i$ is the index of the cluster to which point $x_i$ is assigned, and $\\mu_j$ is the centroid of cluster $j$. The standard algorithm, also known as Lloyd's algorithm, alternates between two steps: an assignment step and an update step.\n\n**Monotonicity and Boundedness of the Objective Function**\n\nLet the state of the algorithm after $t-1$ complete iterations be characterized by the set of centroids $\\{\\mu_j^{(t-1)}\\}_{j=1}^k$. The objective value is $J_{t-1}$. An iteration $t$ consists of two steps:\n\n1.  **Assignment Step**: Each data point $x_i$ is assigned to the cluster with the nearest centroid, minimizing its contribution to the WCSS. The new assignment $a_i^{(t)}$ for point $x_i$ is found by:\n    $$\n    a_i^{(t)} = \\arg\\min_{j \\in \\{1,\\dots,k\\}} \\| x_i - \\mu_j^{(t-1)} \\|^2\n    $$\n    By construction, this new assignment cannot increase the objective function value for the given centroids. Let $J'$ be the objective value after this step, using the new assignments $\\{a_i^{(t)}\\}$ but the old centroids $\\{\\mu_j^{(t-1)}\\}$. We have:\n    $$\n    J' = \\sum_{i=1}^n \\| x_i - \\mu_{a_i^{(t)}}^{(t-1)} \\|^2 \\le \\sum_{i=1}^n \\| x_i - \\mu_{a_i^{(t-1)}}^{(t-1)} \\|^2 = J_{t-1}\n    $$\n    This inequality holds because each term in the sum on the left is less than or equal to the corresponding term on the right due to the nature of the $\\arg\\min$ operation.\n\n2.  **Update Step**: The centroid of each cluster is updated to be the arithmetic mean of all data points assigned to it. For each cluster $j$, the new centroid $\\mu_j^{(t)}$ is calculated as:\n    $$\n    \\mu_j^{(t)} = \\frac{1}{|C_j^{(t)}|} \\sum_{i \\in C_j^{(t)}} x_i, \\quad \\text{where } C_j^{(t)} = \\{i : a_i^{(t)} = j\\}\n    $$\n    As stated in the problem, this choice of $\\mu_j^{(t)}$ is the one that minimizes the sum of squared distances for the points in cluster $C_j^{(t)}$. Therefore, this step must also decrease or keep constant the objective function value compared to its value after the assignment step:\n    $$\n    J_t = \\sum_{i=1}^n \\| x_i - \\mu_{a_i^{(t)}}^{(t)} \\|^2 \\le \\sum_{i=1}^n \\| x_i - \\mu_{a_i^{(t)}}^{(t-1)} \\|^2 = J'\n    $$\n\nCombining both steps, we have $J_t \\le J' \\le J_{t-1}$, which proves that the sequence of objective function values $\\{J_t\\}_{t \\ge 0}$ is monotonically non-increasing. Furthermore, since $J$ is a sum of squared norms, it is inherently non-negative, i.e., $J_t \\ge 0$ for all $t$. A monotonically non-increasing sequence that is bounded below is guaranteed to converge to a limit. This convergence is the foundation for the stopping criteria.\n\n**Stopping Criterion 1: Relative Improvement**\n\nSince the sequence of objective values $\\{J_t\\}$ converges, the improvement in each step, $\\Delta J_t = J_{t-1} - J_t$, will approach $0$. A simple criterion could be to stop when $\\Delta J_t$ falls below some absolute threshold. However, this is not scale-invariant; the magnitude of $J$ depends on the scale of the data coordinates. A more robust criterion is the relative improvement, which normalizes the decrease by the objective value itself:\n$$\n\\frac{J_{t-1} - J_t}{J_{t-1}} < \\epsilon\n$$\nfor some small user-specified tolerance $\\epsilon > 0$. This criterion measures the fractional improvement and is dimensionless. To avoid division by zero or numerical instability when $J_{t-1}$ is close to $0$, the denominator is stabilized using a small positive constant $\\delta$, as specified. The final, robust criterion is:\n$$\n\\frac{\\Delta J_t}{\\max(J_{t-1}, \\delta)} < \\epsilon\n$$\nThis criterion signals that the algorithm has reached a point where further iterations yield diminishing returns in terms of optimizing the objective function.\n\n**Stopping Criterion 2: Assignment Stability**\n\nThe $k$-means algorithm converges to a fixed point, which is a local minimum of the objective function, when the cluster assignments no longer change between iterations. Let us assume that in iteration $t$, the assignment step yields a set of assignments $\\{a_i^{(t)}\\}$ that is identical to the assignments from the previous iteration, $\\{a_i^{(t-1)}\\}$.\n$$\na_i^{(t)} = a_i^{(t-1)} \\quad \\forall i \\in \\{1, \\dots, n\\}\n$$\nIf the assignments are unchanged, the sets of points $C_j$ for each cluster $j$ are also unchanged. Consequently, the update step will produce the exact same centroids as in the previous iteration:\n$$\n\\mu_j^{(t)} = \\frac{1}{|C_j^{(t)}|} \\sum_{i \\in C_j^{(t)}} x_i = \\frac{1}{|C_j^{(t-1)}|} \\sum_{i \\in C_j^{(t-1)}} x_i = \\mu_j^{(t-1)}\n$$\nSince both the assignments and the centroids are identical to the previous state, the algorithm has reached a fixed point. All subsequent iterations will produce the same result. Therefore, checking for the condition of no change in any point's cluster assignment is a definitive test for convergence to a local optimum. This is called the assignment stability criterion.\n\n**Implementation Plan**\n\nThe implementation will follow the problem's detailed specifications. For each test case, a single run of the $k$-means algorithm is performed until assignment stability is reached or a maximum number of iterations, $T_{\\max}$, is exceeded.\n1.  **Initialization**: Centers are initialized using a deterministic version of $k$-means++, where a fixed random seed (we use $0$) makes the probabilistic selection process reproducible.\n2.  **Iteration Loop**: The main loop performs assignment-update cycles.\n3.  **Assignment**: The `scipy.spatial.distance.cdist` function is used to efficiently compute the squared Euclidean distances between all points and centers.\n4.  **Empty Clusters**: After assignment, a check for empty clusters is performed. If any are found, a deterministic rule is applied: for each empty cluster, the point with the largest index from the currently largest cluster is reassigned to it. This ensures all $k$ centroids can be updated.\n5.  **State Tracking**: The assignments from the previous iteration are stored to check for stability. The objective function value $J_t$ is calculated after each update step.\n6.  **Criteria Evaluation**: During the run, we track the iteration count.\n    -   `iter_stab`: The number of full cycles completed before the assignments stabilize. This is recorded when, at the start of iteration $t$, the assignments are found to be identical to those from iteration $t-1$. The count is then $t-1$.\n    -   `iter_rel`: The first iteration $t$ for which the relative improvement drops below $\\epsilon$. It is calculated after $J_t$ is computed.\n7.  **Final Counts**: The run continues until stability or $T_{\\max}$. The final value for `iter_rel` is taken as the first iteration that satisfied the condition. If the condition was never met, `iter_rel` is set to the final `iter_stab` value, as per the problem's instruction.\n\nThis design ensures both stopping criteria are evaluated based on the exact same algorithmic trajectory, providing a fair comparison.", "answer": "```python\nimport numpy as np\nfrom scipy.spatial.distance import cdist\n\ndef kmeans_plusplus_init(X, k, seed):\n    \"\"\"Deterministic k-means++ initialization.\"\"\"\n    rng = np.random.default_rng(seed)\n    n_samples, n_features = X.shape\n    centers = np.zeros((k, n_features))\n    \n    # 1. Choose the first center uniformly at random (from indices)\n    first_center_idx = rng.choice(n_samples)\n    centers[0] = X[first_center_idx]\n    \n    # 2. Choose remaining k-1 centers\n    for j in range(1, k):\n        # Calculate squared distances to the nearest existing center\n        dist_sq = cdist(X, centers[:j], 'sqeuclidean')\n        min_dist_sq = np.min(dist_sq, axis=1)\n        \n        # Calculate probabilities\n        prob_sum = np.sum(min_dist_sq)\n        if prob_sum == 0:\n            # Handle degenerate case (e.g., all points identical)\n            # Fallback to uniform probability\n            prob = np.full(n_samples, 1/n_samples)\n        else:\n            prob = min_dist_sq / prob_sum\n        \n        # Choose next center based on probabilities\n        next_center_idx = rng.choice(n_samples, p=prob)\n        centers[j] = X[next_center_idx]\n        \n    return centers\n\ndef handle_empty_clusters(assignments, k):\n    \"\"\"Deterministically handle empty clusters by reassigning points.\"\"\"\n    assignments = assignments.copy()\n    cluster_indices, counts = np.unique(assignments, return_counts=True)\n    \n    if len(cluster_indices) < k:\n        all_clusters = set(range(k))\n        assigned_clusters = set(cluster_indices)\n        empty_clusters = sorted(list(all_clusters - assigned_clusters))\n        \n        for j_empty in empty_clusters:\n            # Recalculate largest cluster each time\n            current_counts = np.bincount(assignments, minlength=k)\n            j_largest = np.argmax(current_counts)\n            \n            # Find point with the largest index in the largest cluster\n            indices_in_largest = np.where(assignments == j_largest)[0]\n            point_to_move_idx = np.max(indices_in_largest)\n            \n            # Reassign the point\n            assignments[point_to_move_idx] = j_empty\n            \n    return assignments\n\ndef run_kmeans_comparison(X, k, epsilon, T_max, delta, seed=0):\n    \"\"\"\n    Runs k-means until stability and reports iteration counts for two stopping criteria.\n    \"\"\"\n    n_samples, _ = X.shape\n    \n    # 1. Initialization\n    centers = kmeans_plusplus_init(X, k, seed)\n    \n    # Calculate an initial objective value to compare against J_1.\n    # This J_0 is based on initial centers and the assignments they induce.\n    initial_assignments = np.argmin(cdist(X, centers, 'sqeuclidean'), axis=1)\n    J_prev = np.sum((X - centers[initial_assignments])**2)\n    \n    assignments_prev = np.full(n_samples, -1, dtype=int)\n    \n    iter_rel = None\n    iter_stab = T_max # Default if not reached\n    \n    for t in range(1, T_max + 1):\n        # 2. Assignment step\n        assignments_curr_prime = np.argmin(cdist(X, centers, 'sqeuclidean'), axis=1)\n        \n        # 3. Handle empty clusters (after assignment, before stability check and update)\n        assignments_curr = handle_empty_clusters(assignments_curr_prime, k)\n\n        # 4. Stability check\n        if np.array_equal(assignments_curr, assignments_prev):\n            iter_stab = t - 1\n            break\n        \n        # 5. Update centers\n        new_centers = np.zeros_like(centers)\n        for j in range(k):\n            # This is guaranteed to be non-empty due to handle_empty_clusters\n            points_in_cluster = X[assignments_curr == j]\n            new_centers[j] = np.mean(points_in_cluster, axis=0)\n        centers = new_centers\n\n        # 6. Calculate objective J_t\n        J_curr = np.sum((X - centers[assignments_curr])**2)\n        \n        # 7. Relative improvement check\n        if iter_rel is None:\n            denominator = max(J_prev, delta)\n            relative_improvement = (J_prev - J_curr) / denominator\n            if relative_improvement < epsilon:\n                iter_rel = t\n        \n        # 8. Update state for next iteration\n        J_prev = J_curr\n        assignments_prev = assignments_curr\n    \n    # If the loop finished due to T_max, iter_stab is already T_max.\n    # If stability was reached, `break` was triggered.\n    else: # This `else` belongs to the `for` loop, runs if no `break`\n        iter_stab = T_max\n\n    # \"if no ratio satisfies the inequality before stability is reached, report \n    # the assignment stability iteration count\"\n    if iter_rel is None:\n        iter_rel = iter_stab\n        \n    return iter_rel, iter_stab\n\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    delta = 1e-12\n    # Using a fixed seed for reproducibility as requested.\n    fixed_seed = 0\n\n    test_cases = [\n        {\n            \"data\": np.array([\n                [0.0, 0.0], [0.4, -0.2], [-0.3, 0.1], [5.0, 5.0], \n                [5.2, 4.7], [4.8, 5.3], [5.1, 5.1], [-0.2, -0.1]\n            ]),\n            \"k\": 2, \"epsilon\": 1e-4, \"T_max\": 100\n        },\n        {\n            \"data\": np.array([\n                [0.0, 0.0], [0.1, 0.0], [0.0, 0.1], [0.2, 0.2], [0.3, 0.3], \n                [1.0, 1.0], [1.1, 1.2], [0.9, 1.05], [1.2, 0.9], [1.05, 1.1]\n            ]),\n            \"k\": 2, \"epsilon\": 1e-6, \"T_max\": 200\n        },\n        {\n            \"data\": np.array([\n                [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], \n                [1.0, 1.0], [1.0, 1.0], [1.0, 1.0]\n            ]),\n            \"k\": 3, \"epsilon\": 1e-9, \"T_max\": 50\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        iter_rel, iter_stab = run_kmeans_comparison(\n            case[\"data\"], case[\"k\"], case[\"epsilon\"], case[\"T_max\"], delta, seed=fixed_seed\n        )\n        all_results.extend([iter_rel, iter_stab])\n\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```", "id": "3107749"}, {"introduction": "At its core, the $k$-means algorithm partitions data based on Euclidean distance, making it highly sensitive to the scale of the features in your dataset. This exercise provides a hands-on exploration of this sensitivity by examining how scaling individual features alters the geometry of the problem. You will derive the analytical form of the decision boundary between two clusters and programmatically observe how this boundary shifts and rotates as you apply different scaling factors, leading to changes in cluster assignments [@problem_id:3107771]. This practice powerfully illustrates why feature scaling is not just a perfunctory step, but a critical part of preparing data for distance-based algorithms.", "problem": "You will examine how per-feature scaling alters cluster assignments in the $k$-means algorithm in a two-dimensional setting, using only the assignment step with fixed centroids. The fundamental base for your derivation must be the standard definition of squared Euclidean distance and the $k$-means assignment rule. Begin from these fundamental definitions and derive how the decision boundary between two fixed centroids changes under diagonal feature scaling.\n\nDefinitions and setup:\n- Consider a dataset in $\\mathbb{R}^2$ consisting of the six points $x_i \\in \\mathbb{R}^2$:\n  - $x_1 = (0.0, 0.0)$\n  - $x_2 = (0.0, 3.0)$\n  - $x_3 = (3.0, 0.0)$\n  - $x_4 = (2.9, 3.0)$\n  - $x_5 = (1.0, 2.0)$\n  - $x_6 = (2.0, 1.0)$\n- Use $k = 2$ clusters with fixed centroids $c_1 = (0.0, 0.0)$ and $c_2 = (2.9, 3.0)$.\n- For any scaling vector $s \\in \\mathbb{R}^2$ with strictly positive entries, define the diagonal scaling transform $S = \\mathrm{diag}(s_1, s_2)$ and the scaled squared distance $d_s^2(x, c) = \\|S(x - c)\\|_2^2$.\n- The $k$-means assignment rule under scaling $s$ assigns a point $x$ to the centroid $c_j$ that minimizes $d_s^2(x, c_j)$.\n\nTasks:\n1. Starting from the definition of squared Euclidean distance and the $k$-means assignment rule, derive the analytic equation of the decision boundary between $c_1$ and $c_2$ as a function of $s$. Express the boundary in the original coordinates $(x, y)$ and write it in the linear form $A(s)\\,x + B(s)\\,y + C(s) = 0$, explicitly identifying $A(s)$, $B(s)$, and $C(s)$ in terms of $s$, $c_1$, and $c_2$.\n2. Using your derived boundary, implement a program that:\n   - Computes the baseline assignments for all six points using the unscaled case $s = (1, 1)$.\n   - For each specified scaling vector $s$, reassigns all points using only the assignment step with the fixed centroids $c_1$ and $c_2$ under the scaled distance $d_s^2(\\cdot,\\cdot)$, and counts how many points have assignments different from the baseline.\n   - Computes the orientation of the boundary’s normal vector $\\theta(s) = \\mathrm{atan2}(B(s), A(s))$ in radians, normalized to the interval $[0, \\pi)$.\n   - Computes the signed distance of the boundary from the origin, given by $d_0(s) = \\dfrac{C(s)}{\\sqrt{A(s)^2 + B(s)^2}}$.\n3. Report the pairwise assignment changes and the boundary characteristics for a test suite of scaling vectors.\n\nTest suite:\n- Use the following five scaling vectors $s = (s_1, s_2)$:\n  - $s = (1.0, 1.0)$\n  - $s = (3.0, 1.0)$\n  - $s = (1.0, 3.0)$\n  - $s = (0.5, 2.0)$\n  - $s = (2.0, 0.5)$\n\nFinal output format:\n- For each scaling vector in the order listed above, output a list of the form $[\\text{flips}, \\theta, d_0]$ where $\\text{flips}$ is the integer number of points whose assignment differs from the baseline at $s = (1.0, 1.0)$, $\\theta$ is the boundary normal angle in radians, and $d_0$ is the signed distance of the boundary from the origin.\n- Express $\\theta$ in radians and round both $\\theta$ and $d_0$ to $6$ decimal places.\n- Aggregate the results for the five test cases into a single line as a comma-separated list enclosed in square brackets, with no spaces, for example:\n  - $[[\\text{flips}_1,\\theta_1,d_{0,1}],[\\text{flips}_2,\\theta_2,d_{0,2}],\\dots]$\n\nNotes:\n- There are no physical units involved.\n- Angles must be expressed in radians.\n- All numerical outputs that are real numbers must be rounded to $6$ decimal places.", "solution": "The problem requires an analysis of how per-feature scaling affects the assignment step in the $k$-means algorithm. This involves deriving the equation for the decision boundary between two fixed centroids under a scaled distance metric and then applying this to a specific dataset.\n\nThe solution is presented in three parts:\n1.  Derivation of the general decision boundary equation.\n2.  Calculation of the boundary's geometric characteristics.\n3.  Application to the specific problem data and outline of the computational procedure.\n\n**1. Derivation of the Decision Boundary Equation**\n\nThe assignment of a point $x \\in \\mathbb{R}^2$ to one of two clusters with centroids $c_1$ and $c_2$ is determined by finding which centroid is closer. The decision boundary is the locus of points equidistant from both centroids. Under a scaled distance metric, this condition is expressed as:\n$$\nd_s^2(x, c_1) = d_s^2(x, c_2)\n$$\nwhere $d_s^2(x, c)$ is the scaled squared Euclidean distance, defined as $d_s^2(x, c) = \\|S(x - c)\\|_2^2$. Here, $x = (x, y)$, $c_j = (c_{jx}, c_{jy})$, and $S = \\mathrm{diag}(s_1, s_2)$ is the diagonal scaling matrix with $s_1 > 0$ and $s_2 > 0$.\n\nThe transformation $S(x-c)$ is given by:\n$$\nS(x-c) = \\begin{pmatrix} s_1 & 0 \\\\ 0 & s_2 \\end{pmatrix} \\begin{pmatrix} x - c_x \\\\ y - c_y \\end{pmatrix} = \\begin{pmatrix} s_1(x - c_x) \\\\ s_2(y - c_y) \\end{pmatrix}\n$$\nThe squared Euclidean norm is then:\n$$\nd_s^2(x, c) = \\|S(x - c)\\|_2^2 = (s_1(x - c_x))^2 + (s_2(y - c_y))^2 = s_1^2(x - c_x)^2 + s_2^2(y - c_y)^2\n$$\nSubstituting this into the decision boundary condition:\n$$\ns_1^2(x - c_{1x})^2 + s_2^2(y - c_{1y})^2 = s_1^2(x - c_{2x})^2 + s_2^2(y - c_{2y})^2\n$$\nTo find the equation of the line, we expand the squared terms:\n$$\ns_1^2(x^2 - 2xc_{1x} + c_{1x}^2) + s_2^2(y^2 - 2yc_{1y} + c_{1y}^2) = s_1^2(x^2 - 2xc_{2x} + c_{2x}^2) + s_2^2(y^2 - 2yc_{2y} + c_{2y}^2)\n$$\nThe quadratic terms, $s_1^2x^2$ and $s_2^2y^2$, appear on both sides and cancel out. We are left with an equation that is linear in $x$ and $y$:\n$$\n-2s_1^2xc_{1x} + s_1^2c_{1x}^2 - 2s_2^2yc_{1y} + s_2^2c_{1y}^2 = -2s_1^2xc_{2x} + s_1^2c_{2x}^2 - 2s_2^2yc_{2y} + s_2^2c_{2y}^2\n$$\nRearranging the terms to group coefficients of $x$, $y$, and the constant terms, we get:\n$$\n(2s_1^2c_{2x} - 2s_1^2c_{1x})x + (2s_2^2c_{2y} - 2s_2^2c_{1y})y + (s_1^2c_{1x}^2 + s_2^2c_{1y}^2 - s_1^2c_{2x}^2 - s_2^2c_{2y}^2) = 0\n$$\nThis equation is in the desired linear form $A(s)x + B(s)y + C(s) = 0$. The coefficients are explicitly identified as functions of the scaling vector $s = (s_1, s_2)$ and the centroid coordinates:\n$$\nA(s) = 2s_1^2(c_{2x} - c_{1x})\n$$\n$$\nB(s) = 2s_2^2(c_{2y} - c_{1y})\n$$\n$$\nC(s) = s_1^2(c_{1x}^2 - c_{2x}^2) + s_2^2(c_{1y}^2 - c_{2y}^2) = \\|Sc_1\\|_2^2 - \\|Sc_2\\|_2^2\n$$\n\n**2. Calculation of Boundary Characteristics**\n\nUsing the coefficients $A(s)$, $B(s)$, and $C(s)$, we can compute the geometric properties of the decision boundary.\n\nThe orientation of the boundary's normal vector, $\\theta(s)$, is the angle of the vector $(A(s), B(s))$ with respect to the positive x-axis. It is given by the two-argument arctangent function:\n$$\n\\theta(s) = \\mathrm{atan2}(B(s), A(s))\n$$\nThe problem requires this angle to be normalized to the interval $[0, \\pi)$. Since the `atan2` function returns a value in $(-\\pi, \\pi]$, a simple adjustment `if \\theta < 0: \\theta = \\theta + \\pi` correctly maps any negative result into the desired range, corresponding to choosing the normal vector that points into the upper half-plane.\n\nThe signed distance of the boundary from the origin, $d_0(s)$, is given by the standard formula for the distance from a point $(x_0, y_0)$ to a line $Ax + By + C = 0$, which is $\\frac{Ax_0 + By_0 + C}{\\sqrt{A^2+B^2}}$. For the origin $(x_0, y_0) = (0, 0)$, this simplifies to:\n$$\nd_0(s) = \\frac{C(s)}{\\sqrt{A(s)^2 + B(s)^2}}\n$$\nThe sign of $d_0(s)$ indicates which side of the line the origin lies on.\n\n**3. Application and Computational Procedure**\n\nThe specific data provided are:\n- Points $x_1 = (0.0, 0.0)$, $x_2 = (0.0, 3.0)$, $x_3 = (3.0, 0.0)$, $x_4 = (2.9, 3.0)$, $x_5 = (1.0, 2.0)$, $x_6 = (2.0, 1.0)$.\n- Centroids $c_1 = (0.0, 0.0)$ and $c_2 = (2.9, 3.0)$.\n\nThe computational procedure is as follows:\nFirst, we establish the baseline assignments. This is done using the unscaled distance, which corresponds to the scaling vector $s = (1.0, 1.0)$. For each point $x_i$, we compute $d_s^2(x_i, c_1)$ and $d_s^2(x_i, c_2)$ and assign the point to the centroid yielding the minimum distance. These assignments serve as the reference.\n\nSecond, for each scaling vector $s$ in the provided test suite, we repeat the assignment process:\n- For each point $x_i$, calculate the scaled squared distances $d_s^2(x_i, c_1)$ and $d_s^2(x_i, c_2)$.\n- Assign $x_i$ to the closer centroid.\n- Compare the new assignment for each point to its baseline assignment. The number of points whose assignment has changed is counted as `flips`.\n\nThird, for each scaling vector $s$, we compute the boundary characteristics:\n- We substitute the specific coordinates of $c_1=(0,0)$ and $c_2=(2.9, 3.0)$ and the components of $s=(s_1,s_2)$ into the derived formulas for $A(s)$, $B(s)$, and $C(s)$:\n  $A(s) = 2s_1^2(2.9 - 0) = 5.8s_1^2$\n  $B(s) = 2s_2^2(3.0 - 0) = 6.0s_2^2$\n  $C(s) = s_1^2(0^2 - 2.9^2) + s_2^2(0^2 - 3.0^2) = -8.41s_1^2 - 9.0s_2^2$\n- We then compute $\\theta(s)$ and $d_0(s)$ using these coefficients.\n\nFinally, the results for each scaling vector—the number of flips, the normalized angle $\\theta(s)$, and the signed distance $d_0(s)$—are collected and formatted as specified. All floating-point values are rounded to $6$ decimal places.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of analyzing k-means assignment changes under feature scaling.\n    \"\"\"\n    # Define the dataset, centroids, and scaling vectors from the problem statement.\n    points = np.array([\n        [0.0, 0.0],\n        [0.0, 3.0],\n        [3.0, 0.0],\n        [2.9, 3.0],\n        [1.0, 2.0],\n        [2.0, 1.0]\n    ])\n\n    centroids = np.array([\n        [0.0, 0.0],\n        [2.9, 3.0]\n    ])\n\n    test_cases = [\n        (1.0, 1.0),\n        (3.0, 1.0),\n        (1.0, 3.0),\n        (0.5, 2.0),\n        (2.0, 0.5)\n    ]\n\n    def get_assignments(points, centroids, s_vector):\n        \"\"\"\n        Calculates cluster assignments for a given scaling vector.\n        \"\"\"\n        s = np.array(s_vector)\n        s_sq = s**2\n        \n        # Broadcasting to compute distances for all points to all centroids\n        # diff shape: (num_points, num_centroids, num_dims)\n        diff = points[:, np.newaxis, :] - centroids[np.newaxis, :, :]\n        \n        # scaled_sq_diffs shape: (num_points, num_centroids, num_dims)\n        # Broadcasting s_sq over the first two dimensions\n        scaled_sq_diffs = diff**2 * s_sq\n        \n        # dist_sq shape: (num_points, num_centroids)\n        # Sum over the dimensions axis\n        dist_sq = np.sum(scaled_sq_diffs, axis=2)\n        \n        # assignments shape: (num_points,)\n        # Find the index of the minimum distance for each point\n        assignments = np.argmin(dist_sq, axis=1)\n        return assignments\n\n    # Calculate baseline assignments for s = (1.0, 1.0)\n    baseline_assignments = get_assignments(points, centroids, test_cases[0])\n    \n    results = []\n    \n    # Process each test case\n    for s_vector in test_cases:\n        s1, s2 = s_vector\n        \n        # 1. Reassign points and count flips\n        current_assignments = get_assignments(points, centroids, s_vector)\n        flips = np.sum(current_assignments != baseline_assignments)\n        \n        # 2. Compute boundary characteristics\n        c1x, c1y = centroids[0]\n        c2x, c2y = centroids[1]\n\n        # Using derived formulas for A(s), B(s), C(s)\n        A = 2 * s1**2 * (c2x - c1x)\n        B = 2 * s2**2 * (c2y - c1y)\n        C = s1**2 * (c1x**2 - c2x**2) + s2**2 * (c1y**2 - c2y**2)\n        \n        # Compute orientation theta\n        theta = np.arctan2(B, A)\n        \n        # Normalize theta to the interval [0, pi)\n        # atan2 returns in (-pi, pi], so adding pi to negative values is sufficient\n        if theta < 0:\n            theta += np.pi\n\n        # Compute signed distance d0\n        norm = np.sqrt(A**2 + B**2)\n        d0 = C / norm\n        \n        # Format the result entry for the current test case\n        formatted_entry = f\"[{flips},{theta:.6f},{d0:.6f}]\"\n        results.append(formatted_entry)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3107771"}, {"introduction": "While powerful, the standard $k$-means algorithm is not without its biases. Its objective is to minimize the total within-cluster sum of squares (WCSS), which can inadvertently cause the algorithm to favor large, dense clusters, sometimes at the expense of accurately identifying smaller, more distinct ones. This practice [@problem_id:3107780] guides you through building synthetic datasets to expose this very bias, allowing you to quantitatively measure how $k$-means can misassign points from smaller clusters or \"waste\" centroids on larger ones. Furthermore, you will implement a weighted variant of the algorithm, demonstrating how modifying the objective function can serve as a principled remedy to this inherent limitation.", "problem": "You are to implement, from first principles, the $k$-means clustering algorithm and a weighted variant, and use them to analyze how the standard $k$-means objective biases solutions toward large clusters in synthetic data where cluster sizes differ by orders of magnitude.\n\nStart from the following core definitions. Given data points $\\{x_i \\in \\mathbb{R}^d\\}_{i=1}^n$, a prescribed number of clusters $k$, cluster assignments $z_i \\in \\{1,\\dots,k\\}$, and centroids $\\{c_j \\in \\mathbb{R}^d\\}_{j=1}^k$, the standard $k$-means objective minimizes the Within-Cluster Sum of Squares (WCSS) defined as\n$$\nJ(\\{c_j\\}, \\{z_i\\}) = \\sum_{i=1}^n \\| x_i - c_{z_i} \\|_2^2,\n$$\nwhere $\\|\\cdot\\|_2$ denotes the Euclidean norm. The weighted $k$-means objective introduces nonnegative point weights $\\{w_i\\}_{i=1}^n$ and minimizes\n$$\nJ_w(\\{c_j\\}, \\{z_i\\}) = \\sum_{i=1}^n w_i \\| x_i - c_{z_i} \\|_2^2.\n$$\nImplement $k$-means using the iterative assignment-update paradigm: assignment by nearest centroid, and centroid update by minimizing the objective over $\\{c_j\\}$. The weighted variant must update centroids to the weighted mean of assigned points. Use $k$-means++ initialization to select initial centroids.\n\nConstruct synthetic datasets in two dimensions by sampling independently from isotropic Gaussian distributions $\\mathcal{N}(\\mu, \\sigma^2 I)$ with specified centers and standard deviations. The datasets must be generated using a fixed random seed for reproducibility. For evaluation, use the known data-generating labels to compute the following metrics:\n\n- Define the small-cluster misassignment rate as the fraction of points originating from the small cluster that are assigned to a centroid mapped to the large cluster’s true center. Define the large-cluster misassignment rate analogously. To map each learned centroid to a true center, choose the nearest true center in $\\ell_2$ distance.\n- Define the bias score $B$ as\n$$\nB = \\frac{\\text{misassigned small-cluster points}}{n_s} - \\frac{\\text{misassigned large-cluster points}}{n_\\ell},\n$$\nwhere $n_s$ and $n_\\ell$ are the sizes of the small and large clusters, respectively.\n- For cases with $k > 2$, define the duplication count $D$ for the large cluster as the number of learned centroids whose nearest true center is the large cluster's true center.\n\nYour program must implement the algorithms and compute the following test suite. In all cases, data are two-dimensional, and angles are not applicable. There are no physical units.\n\nTest 1 (happy path):\n- Data: two clusters with sizes $n_s = 60$, $n_\\ell = 600$, means $\\mu_s = (-5, 0)$, $\\mu_\\ell = (5, 0)$, and standard deviations $\\sigma_s = 1$, $\\sigma_\\ell = 1$.\n- Clustering: standard $k$-means with $k = 2$ and $k$-means++ initialization.\n- Random seed: $42$ for data generation and initialization.\n- Output: the bias score $B$ rounded to three decimals.\n\nTest 2 (overlap-induced bias):\n- Data: two clusters with sizes $n_s = 60$, $n_\\ell = 600$, means $\\mu_s = (0.1, 0)$, $\\mu_\\ell = (0, 0)$, and standard deviations $\\sigma_s = 1$, $\\sigma_\\ell = 1$.\n- Clustering: standard $k$-means with $k = 2$ and $k$-means++ initialization.\n- Random seed: $43$ for data generation and initialization.\n- Output: the bias score $B$ rounded to three decimals.\n\nTest 3 (extreme size difference, extra centroid allocation):\n- Data: two clusters with sizes $n_s = 50$, $n_\\ell = 5000$, means $\\mu_s = (3, 0)$, $\\mu_\\ell = (0, 0)$, and standard deviations $\\sigma_s = 0.3$, $\\sigma_\\ell = 1.5$.\n- Clustering: standard $k$-means with $k = 3$ and $k$-means++ initialization.\n- Random seed: $44$ for data generation and initialization.\n- Output: the duplication count $D$ for the large cluster as an integer.\n\nTest 4 (remedy via inverse-size weighting):\n- Data: identical to Test $3$.\n- Clustering: weighted $k$-means with $k = 3$ and $k$-means++ initialization, with weights $w_i = 1/n_s$ for points originating from the small cluster and $w_i = 1/n_\\ell$ for points originating from the large cluster.\n- Random seed: $44$ for data generation and initialization.\n- Output: the duplication count $D$ for the large cluster as an integer.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order $[B_{\\text{Test 1}}, B_{\\text{Test 2}}, D_{\\text{Test 3}}, D_{\\text{Test 4}}]$. The bias scores must be rounded to three decimals, and the duplication counts must be integers, producing a list of the form $[b_1,b_2,d_3,d_4]$.", "solution": "The problem requires the implementation of the standard and a weighted $k$-means clustering algorithm to analyze the inherent bias of the standard method towards large clusters. The implementation will be from first principles, including the $k$-means++ initialization scheme.\n\n### Algorithmic Formulation\n\nGiven a set of $n$ data points $\\{x_i \\in \\mathbb{R}^d\\}_{i=1}^n$ and a specified number of clusters $k$, the goal of $k$-means is to find cluster assignments $z_i \\in \\{1, \\dots, k\\}$ and centroids $\\{c_j \\in \\mathbb{R}^d\\}_{j=1}^k$ that partition the data by minimizing an objective function.\n\nThe standard $k$-means algorithm minimizes the Within-Cluster Sum of Squares (WCSS), defined as:\n$$\nJ(\\{c_j\\}, \\{z_i\\}) = \\sum_{i=1}^n \\| x_i - c_{z_i} \\|_2^2\n$$\nwhere $c_{z_i}$ is the centroid of the cluster to which point $x_i$ is assigned.\n\nThe weighted $k$-means variant modifies this objective by introducing non-negative weights $\\{w_i\\}_{i=1}^n$ for each data point:\n$$\nJ_w(\\{c_j\\}, \\{z_i\\}) = \\sum_{i=1}^n w_i \\| x_i - c_{z_i} \\|_2^2\n$$\n\nBoth objectives are typically minimized using an iterative procedure known as Lloyd's algorithm, which alternates between two steps until convergence:\n\n1.  **Assignment Step**: With the centroids $\\{c_j\\}$ held fixed, the objective is minimized by assigning each data point $x_i$ to the nearest centroid. This is equivalent for both standard and weighted $k$-means, as the weights $w_i > 0$ do not affect the choice of the closest centroid for a given point $x_i$. The assignment rule is:\n    $$\n    z_i \\leftarrow \\underset{j \\in \\{1, \\dots, k\\}}{\\mathrm{argmin}} \\| x_i - c_j \\|_2^2\n    $$\n\n2.  **Update Step**: With the assignments $\\{z_i\\}$ held fixed, the objective is minimized by updating each centroid $c_j$ to be the new center of its assigned points. Let $C_j = \\{i \\mid z_i = j\\}$ be the set of indices of points assigned to cluster $j$.\n    *   For **standard $k$-means**, we minimize $J_j = \\sum_{i \\in C_j} \\| x_i - c_j \\|_2^2$ with respect to $c_j$. Setting the gradient to zero, $\\nabla_{c_j} J_j = \\sum_{i \\in C_j} -2(x_i - c_j) = 0$, yields the update rule:\n        $$\n        c_j \\leftarrow \\frac{1}{|C_j|} \\sum_{i \\in C_j} x_i\n        $$\n        The new centroid is the geometric mean of the points in the cluster.\n    *   For **weighted $k$-means**, we minimize $J_{w,j} = \\sum_{i \\in C_j} w_i \\| x_i - c_j \\|_2^2$. Setting the gradient to zero, $\\nabla_{c_j} J_{w,j} = \\sum_{i \\in C_j} -2 w_i (x_i - c_j) = 0$, yields the update rule:\n        $$\n        c_j \\leftarrow \\frac{\\sum_{i \\in C_j} w_i x_i}{\\sum_{i \\in C_j} w_i}\n        $$\n        The new centroid is the weighted geometric mean of the points in the cluster.\n\nTo mitigate sensitivity to initial centroid placement, the **$k$-means++** algorithm is used for initialization. It selects initial centroids sequentially, with each subsequent centroid chosen from the data points with a probability proportional to its squared distance to the nearest existing centroid. This biases the initialization towards placing centroids far from each other, which often leads to better and more consistent results.\n\n### Synthetic Data and Evaluation\n\nThe analysis is performed on synthetic two-dimensional data generated from a mixture of two isotropic Gaussian distributions, $\\mathcal{N}(\\mu, \\sigma^2 I)$. We use a fixed random seed for reproducibility in both data generation and $k$-means++ initialization.\n\nTo quantify the algorithm's behavior, we use the ground-truth labels from the data generation process.\n- Learned cluster centroids (with arbitrary labels $1, \\dots, k$) are first mapped to the true cluster centers ($\\mu_s, \\mu_\\ell$) by finding the nearest true center in Euclidean distance.\n- The **bias score** $B$ is defined as the difference between the small-cluster misassignment rate and the large-cluster misassignment rate:\n  $$\n  B = \\frac{\\text{misassigned small-cluster points}}{n_s} - \\frac{\\text{misassigned large-cluster points}}{n_\\ell}\n  $$\n  A positive score indicates a bias against the small cluster.\n- When $k > 2$, the **duplication count** $D$ for the large cluster is the number of learned centroids that are mapped to the large cluster's true center. This metric quantifies the tendency of $k$-means to allocate extra centroids to larger, more spread-out clusters.\n\nThe specific test cases will demonstrate how cluster overlap, extreme size differences, and inverse-size weighting influence these metrics, providing a quantitative look into the behavior of the $k$-means algorithm. The implementation will be performed in Python, using `numpy` for numerical computation and `scipy` for efficient distance calculations.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.spatial.distance import cdist\n\ndef generate_data(ns, nl, mu_s, mu_l, sigma_s, sigma_l, seed):\n    \"\"\"Generates a 2D dataset from two isotropic Gaussian distributions.\"\"\"\n    rng = np.random.default_rng(seed)\n    dim = len(mu_s)\n    \n    small_cluster = rng.multivariate_normal(mu_s, np.eye(dim) * sigma_s**2, size=ns)\n    large_cluster = rng.multivariate_normal(mu_l, np.eye(dim) * sigma_l**2, size=nl)\n    \n    X = np.vstack([small_cluster, large_cluster])\n    # True labels: 0 for small cluster, 1 for large cluster\n    y_true = np.array([0] * ns + [1] * nl) \n    true_centers = np.array([mu_s, mu_l])\n    \n    return X, y_true, true_centers\n\ndef kmeans_plusplus_init(X, k, seed):\n    \"\"\"Initializes k centroids using the k-means++ algorithm.\"\"\"\n    rng = np.random.default_rng(seed)\n    n_samples, n_features = X.shape\n    centroids = np.empty((k, n_features))\n    \n    # 1. Choose first centroid uniformly at random from data points.\n    centroids[0] = X[rng.choice(n_samples)]\n    \n    # For subsequent centroids\n    for i in range(1, k):\n        # 2. Calculate squared distances to the nearest existing centroid.\n        sq_dists = cdist(X, centroids[:i, :], 'sqeuclidean')\n        min_sq_dists = np.min(sq_dists, axis=1)\n        \n        # 3. Choose the next centroid with probability proportional to D(x)^2.\n        if np.sum(min_sq_dists) > 0:\n            probs = min_sq_dists / np.sum(min_sq_dists)\n            next_idx = rng.choice(n_samples, p=probs)\n        else:\n            # Handle cases where all points are duplicates or already chosen.\n            next_idx = rng.choice(n_samples)\n            \n        centroids[i] = X[next_idx]\n        \n    return centroids\n\ndef perform_kmeans(X, k, seed, weights=None, max_iter=100, tol=1e-6):\n    \"\"\"Performs standard or weighted k-means clustering.\"\"\"\n    centroids = kmeans_plusplus_init(X, k, seed)\n    \n    for i in range(max_iter):\n        # Assignment step: find the closest centroid for each point.\n        sq_dists = cdist(X, centroids, 'sqeuclidean')\n        assignments = np.argmin(sq_dists, axis=1)\n        \n        new_centroids = np.copy(centroids)\n        # Update step: recompute centroids based on new assignments.\n        for j in range(k):\n            assigned_points_mask = (assignments == j)\n            if np.any(assigned_points_mask):\n                if weights is None:\n                    # Standard k-means: centroid is the mean.\n                    new_centroids[j] = np.mean(X[assigned_points_mask], axis=0)\n                else:\n                    # Weighted k-means: centroid is the weighted mean.\n                    assigned_weights = weights[assigned_points_mask]\n                    new_centroids[j] = np.average(X[assigned_points_mask], axis=0, weights=assigned_weights)\n        \n        # Check for convergence: if centroids stop moving.\n        if np.sum((new_centroids - centroids)**2) < tol:\n            break\n            \n        centroids = new_centroids\n        \n    return centroids, assignments\n\ndef map_centroids_to_true_centers(centroids, true_centers):\n    \"\"\"Maps each learned centroid to the index of the nearest true center.\"\"\"\n    dists = cdist(centroids, true_centers)\n    mapping = np.argmin(dists, axis=1)\n    return mapping\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'ns': 60, 'nl': 600, 'mu_s': np.array([-5.0, 0.0]), 'mu_l': np.array([5.0, 0.0]), 'sigma_s': 1.0, 'sigma_l': 1.0, 'k': 2, 'seed': 42, 'type': 'bias'},\n        {'ns': 60, 'nl': 600, 'mu_s': np.array([0.1, 0.0]), 'mu_l': np.array([0.0, 0.0]), 'sigma_s': 1.0, 'sigma_l': 1.0, 'k': 2, 'seed': 43, 'type': 'bias'},\n        {'ns': 50, 'nl': 5000, 'mu_s': np.array([3.0, 0.0]), 'mu_l': np.array([0.0, 0.0]), 'sigma_s': 0.3, 'sigma_l': 1.5, 'k': 3, 'seed': 44, 'type': 'duplication_std'},\n        {'ns': 50, 'nl': 5000, 'mu_s': np.array([3.0, 0.0]), 'mu_l': np.array([0.0, 0.0]), 'sigma_s': 0.3, 'sigma_l': 1.5, 'k': 3, 'seed': 44, 'type': 'duplication_weighted'}\n    ]\n\n    results = []\n\n    for params in test_cases:\n        X, y_true, true_centers = generate_data(\n            params['ns'], params['nl'], \n            params['mu_s'], params['mu_l'],\n            params['sigma_s'], params['sigma_l'],\n            params['seed']\n        )\n        \n        weights = None\n        if params['type'] == 'duplication_weighted':\n            weights = np.zeros_like(y_true, dtype=float)\n            # small cluster points have label 0, large cluster points have label 1\n            weights[y_true == 0] = 1.0 / params['ns']\n            weights[y_true == 1] = 1.0 / params['nl']\n\n        centroids, assignments = perform_kmeans(X, params['k'], params['seed'], weights=weights)\n\n        centroid_to_true_map = map_centroids_to_true_centers(centroids, true_centers)\n\n        if params['type'] == 'bias':\n            # Map each point's assignment to its effective true cluster label\n            mapped_assignments = centroid_to_true_map[assignments]\n            \n            # Small cluster has label 0, large cluster has label 1\n            # Misassigned small point: true label is 0, assigned to centroid mapped to 1\n            misassigned_small_mask = (y_true == 0) & (mapped_assignments == 1)\n            # Misassigned large point: true label is 1, assigned to centroid mapped to 0\n            misassigned_large_mask = (y_true == 1) & (mapped_assignments == 0)\n            \n            num_misassigned_small = np.sum(misassigned_small_mask)\n            num_misassigned_large = np.sum(misassigned_large_mask)\n            \n            ns, nl = params['ns'], params['nl']\n            \n            bias_score = (num_misassigned_small / ns) - (num_misassigned_large / nl)\n            results.append(f\"{bias_score:.3f}\")\n\n        elif 'duplication' in params['type']:\n            # Large cluster has true index 1\n            large_cluster_true_index = 1\n            duplication_count = np.sum(centroid_to_true_map == large_cluster_true_index)\n            results.append(str(duplication_count))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3107780"}]}