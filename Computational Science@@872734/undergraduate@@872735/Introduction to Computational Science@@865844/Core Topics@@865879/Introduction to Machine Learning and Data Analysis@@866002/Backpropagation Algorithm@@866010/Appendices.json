{"hands_on_practices": [{"introduction": "To truly understand backpropagation, we must build it. This exercise guides you through implementing reverse-mode automatic differentiation, the general algorithm of which backpropagation is a special case. By creating a system that records computations on a \"tape\" and then replays it in reverse to accumulate gradients according to the chain rule, you will demystify the core mechanics of how modern deep learning frameworks compute derivatives [@problem_id:3100018].", "problem": "You are tasked with implementing reverse-mode Automatic Differentiation (AD) for a small scalar expression language and demonstrating how to compute adjoint variables, denoted by $\\bar{x} = \\partial L / \\partial x$, through a reverse replay of a tape of operations. Reverse-mode AD is synonymous with the backpropagation algorithm in computational graphs. Your implementation must start from first principles, specifically the chain rule of differentiation for composed functions, and the definition of a computational graph as a directed acyclic graph of primitive operations with a scalar loss $L$ at the root. You must design a tape structure that records the forward execution of primitive operations and then replay this tape in reverse to accumulate adjoints using the chain rule.\n\nYour small expression language must support scalar variables and constants, and the following primitive operations: binary addition $+$, binary subtraction $-$, binary multiplication $\\cdot$, binary division $\\div$, unary sine $\\sin(\\cdot)$, unary exponential $\\exp(\\cdot)$, and unary natural logarithm $\\log(\\cdot)$. All angles in trigonometric functions must be in radians. The domain constraints must be respected, for example the input to $\\log(\\cdot)$ must be strictly positive. You must design the computational tape to record each non-leaf operation with its operands and forward value to ensure a correct reverse replay.\n\nYour program must:\n- Build an internal computational graph and tape when evaluating a scalar loss $L$.\n- Compute the adjoint for each input variable $x_i$, that is $\\partial L / \\partial x_i$, via a single reverse replay of the tape starting from $\\bar{L} = \\partial L / \\partial L = 1$.\n- Produce, for each test case, a list whose first element is the scalar loss value $L$ and whose subsequent elements are the adjoints for the variables in the order they were introduced.\n\nStart from fundamental principles only: the chain rule for composed functions, the definition of adjoints $\\bar{v} = \\partial L / \\partial v$ for intermediate values $v$, and the semantics of a computational graph. Do not rely on pre-packaged differentiation formulas that skip the derivation path; instead derive and implement the local partial derivatives needed for the reverse replay using basic calculus for each primitive operation.\n\nImplement and run the following test suite. In each case, define the variables in the specified order, construct the expression using the primitive operations, and compute the outputs. All angles are in radians, and there are no physical units involved in this problem.\n\n- Test case $1$ (general composition): variables $x, y$, loss $L = \\sin(x \\cdot y) + \\exp(y)$, with $x = 0.5$, $y = -1.0$. Output format for this case: $[L, \\partial L / \\partial x, \\partial L / \\partial y]$.\n- Test case $2$ (boundary with zero and constants): variable $x$, loss $L = x \\cdot 0 + \\sin(0) + \\log(1)$, with $x = 2.0$. Output format: $[L, \\partial L / \\partial x]$.\n- Test case $3$ (repeated variable usage): variable $x$, loss $L = (x \\cdot x) \\cdot x$, with $x = 2.0$. Output format: $[L, \\partial L / \\partial x]$.\n- Test case $4$ (division and logarithm): variables $x, y$, loss $L = x \\div y + \\log(y)$, with $x = 1.0$, $y = 1.5$. Output format: $[L, \\partial L / \\partial x, \\partial L / \\partial y]$.\n- Test case $5$ (nested unary composition): variable $x$, loss $L = \\exp(\\sin(x))$, with $x = 0.0$. Output format: $[L, \\partial L / \\partial x]$.\n\nYour program should produce a single line of output containing the results of all test cases as a comma-separated list enclosed in square brackets, with each test case result itself being a comma-separated list enclosed in square brackets. For example, an output for two test cases would look like $[[L_1,\\partial L_1/\\partial x_1,\\dots],[L_2,\\partial L_2/\\partial x_1,\\dots]]$. Your final output must follow this format exactly, using standard floating-point numbers.", "solution": "The problem requires the implementation of reverse-mode Automatic Differentiation (AD), colloquially known as backpropagation, from first principles. This method computes the gradient of a scalar loss function $L$ with respect to a set of input variables $x_i$ by first performing a forward evaluation of the expression for $L$ to compute intermediate values and record a computational graph, followed by a reverse traversal of this graph to propagate gradients based on the chain rule.\n\n**Fundamental Principles: The Chain Rule and Adjoints**\n\nThe foundation of reverse-mode AD is the chain rule of calculus. If a scalar loss $L$ is a function of an intermediate variable $v_j$, which itself is a function of other variables $v_i$, the gradient of $L$ with respect to $v_i$ is given by the sum of contributions through all paths from $v_i$ to $L$. For a single path $L \\to v_j \\to v_i$, the chain rule states:\n$$\n\\frac{\\partial L}{\\partial v_i} = \\frac{\\partial L}{\\partial v_j} \\frac{\\partial v_j}{\\partial v_i}\n$$\nIn the language of AD, we define the \"adjoint\" of a variable $v$ as $\\bar{v} \\equiv \\frac{\\partial L}{\\partial v}$. Using this notation, the chain rule becomes:\n$$\n\\bar{v}_i = \\bar{v}_j \\frac{\\partial v_j}{\\partial v_i}\n$$\nThe reverse-mode AD algorithm leverages this relationship by first computing the value of $L$ and then propagating the adjoints backward from $L$ to the input variables. The process starts by seeding the adjoint of the loss function itself, $\\bar{L} = \\frac{\\partial L}{\\partial L} = 1$.\n\n**The Computational Graph and Tape**\n\nAny scalar expression can be decomposed into a sequence of primitive operations (e.g., addition, multiplication, sine). This decomposition naturally forms a Directed Acyclic Graph (DAG), where nodes represent numerical values (input variables, constants, and intermediate results) and edges represent the primitive operations.\n\nThe forward evaluation of the expression, from inputs to the final loss $L$, is used to construct this graph. For our implementation, we use a \"tape\" data structure, which is a linearized representation of the graph. The tape is an ordered list of operations recorded during the forward pass. Each entry on the tape stores the type of operation, references to its input nodes, and a reference to its output node. This recording ensures we have the complete structure and all necessary intermediate values for the reverse pass.\n\n**The Forward Pass: Evaluation and Tape Recording**\n\nThe forward pass proceeds as follows:\n$1$. Input variables and constants are initialized as the starting nodes in our graph.\n$2$. The expression is evaluated sequentially. Each time a primitive operation is applied, two things happen:\n    a. The numerical result of the operation is computed and stored as a new node in the graph.\n    b. An entry is added to the tape, recording the operation type, its input node(s), and the newly created output node.\n\nFor example, for the expression $z = x \\cdot y$, we would compute the value of $z$ using the current values of $x$ and $y$, create a new node for $z$, and record `('mul', [x_node, y_node], z_node)` on the tape.\n\n**The Reverse Pass: Adjoint Accumulation**\n\nOnce the forward pass is complete and the final loss value $L$ is computed, the reverse pass begins. It traverses the tape in the reverse order of its creation.\n$1$. An array of adjoints, corresponding to each node in the graph, is initialized to zero.\n$2$. The adjoint of the final loss node is set to $1$, i.e., $\\bar{L} = 1$.\n$3$. For each operation $z = f(x_1, \\dots, x_n)$ on the tape (processed in reverse order):\n    a. We retrieve the already computed adjoint of the output, $\\bar{z}$.\n    b. We use the chain rule to calculate the contribution of $\\bar{z}$ to the adjoints of the inputs. The adjoint of each input $x_i$ is updated by accumulating this contribution:\n    $$\n    \\bar{x}_i \\mathrel{+}= \\bar{z} \\cdot \\frac{\\partial z}{\\partial x_i}\n    $$\n    The use of accumulation ($\\mathrel{+}=$) is crucial because a single variable might be used in multiple operations (i.e., it can be a parent to multiple children in the graph). Its total adjoint is the sum of the gradient signals flowing back from all its children. Reversing the tape guarantees that a node's adjoint ($\\bar{z}$) is fully calculated before it is propagated to its own inputs ($x_i$).\n\n**Adjoint Update Rules for Primitive Operations**\n\nThe local partial derivatives $\\frac{\\partial z}{\\partial x_i}$ are known for each primitive operation. The values of the inputs required for these derivatives (e.g., for $z = x \\cdot y$, $\\frac{\\partial z}{\\partial x} = y$) are available from the forward pass.\n\n- **Addition:** $z = x + y$\n  - $\\frac{\\partial z}{\\partial x} = 1$, $\\frac{\\partial z}{\\partial y} = 1$.\n  - Update rules: $\\bar{x} \\mathrel{+}= \\bar{z}$, $\\bar{y} \\mathrel{+}= \\bar{z}$.\n\n- **Subtraction:** $z = x - y$\n  - $\\frac{\\partial z}{\\partial x} = 1$, $\\frac{\\partial z}{\\partial y} = -1$.\n  - Update rules: $\\bar{x} \\mathrel{+}= \\bar{z}$, $\\bar{y} \\mathrel{+}= -\\bar{z}$.\n\n- **Multiplication:** $z = x \\cdot y$\n  - $\\frac{\\partial z}{\\partial x} = y$, $\\frac{\\partial z}{\\partial y} = x$.\n  - Update rules: $\\bar{x} \\mathrel{+}= \\bar{z} \\cdot y$, $\\bar{y} \\mathrel{+}= \\bar{z} \\cdot x$.\n\n- **Division:** $z = x \\div y$\n  - $\\frac{\\partial z}{\\partial x} = \\frac{1}{y}$, $\\frac{\\partial z}{\\partial y} = -\\frac{x}{y^2}$.\n  - Update rules: $\\bar{x} \\mathrel{+}= \\bar{z} \\cdot \\frac{1}{y}$, $\\bar{y} \\mathrel{+}= \\bar{z} \\cdot \\left(-\\frac{x}{y^2}\\right)$.\n\n- **Sine:** $z = \\sin(x)$\n  - $\\frac{dz}{dx} = \\cos(x)$.\n  - Update rule: $\\bar{x} \\mathrel{+}= \\bar{z} \\cdot \\cos(x)$.\n\n- **Exponential:** $z = \\exp(x)$\n  - $\\frac{dz}{dx} = \\exp(x) = z$.\n  - Update rule: $\\bar{x} \\mathrel{+}= \\bar{z} \\cdot z$.\n\n- **Natural Logarithm:** $z = \\log(x)$\n  - $\\frac{dz}{dx} = \\frac{1}{x}$.\n  - Update rule: $\\bar{x} \\mathrel{+}= \\bar{z} \\cdot \\frac{1}{x}$.\n\n**Implementation Design**\n\nThe implementation uses two main classes: `Graph` and `Node`. The `Graph` class manages the state of the computation: it stores the `values` of all nodes, the operation `tape`, and the computed `adjoints`. The `Node` class acts as a wrapper around a node's ID, providing an intuitive interface by overloading Python's arithmetic operators (`+`, `*`, etc.). When an operation like `c = a + b` is performed on `Node` objects, it transparently calls a method on the associated `Graph` object, which performs the forward calculation, records the operation on the tape, and returns a new `Node` for the result `c`. This object-oriented design allows for the construction of expressions in a natural way while correctly building the computational graph in the background. After the final loss `Node` is computed, a call to `Graph.compute_gradients()` executes the reverse pass as described above.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# A consistent execution environment requires no user input and all dependencies declared.\n\n# Define unary functions that can operate on Node objects or raw numbers\ndef sin(node):\n    \"\"\"Computes sine, handling both Node objects and numeric types.\"\"\"\n    if isinstance(node, Node):\n        return node.graph.sin(node)\n    return np.sin(node)\n\ndef exp(node):\n    \"\"\"Computes exponential, handling both Node objects and numeric types.\"\"\"\n    if isinstance(node, Node):\n        return node.graph.exp(node)\n    return np.exp(node)\n\ndef log(node):\n    \"\"\"Computes natural logarithm, handling both Node objects and numeric types.\"\"\"\n    if isinstance(node, Node):\n        return node.graph.log(node)\n    return np.log(node)\n\nclass Graph:\n    \"\"\"Manages the computational graph, tape, and differentiation process.\"\"\"\n    def __init__(self):\n        # A linearized list of operations representing the computational graph.\n        # Each entry is a tuple: (op_type, [input_node_ids], output_node_id)\n        self.tape = []\n        # Stores the numerical value of each node computed during the forward pass.\n        self.values = []\n        # Stores the adjoint (dL/dv) for each node, computed during the reverse pass.\n        self.adjoints = None\n\n    def _add_node(self, value):\n        \"\"\"Adds a new node (value) to the graph and returns its ID.\"\"\"\n        node_id = len(self.values)\n        self.values.append(value)\n        return node_id\n\n    def variable(self, value):\n        \"\"\"Creates a variable node, which is a leaf in the graph.\"\"\"\n        node_id = self._add_node(value)\n        return Node(self, node_id)\n\n    def _promote_to_node(self, other):\n        \"\"\"Promotes a numeric constant to a Node to allow operations like `x + 5`.\"\"\"\n        if not isinstance(other, Node):\n            # Treat numeric constants as new variable nodes in the graph.\n            return self.variable(other)\n        return other\n\n    # Methods for primitive operations (Forward Pass)\n    def add(self, n1, n2):\n        res_val = self.values[n1.node_id] + self.values[n2.node_id]\n        res_id = self._add_node(res_val)\n        self.tape.append(('add', [n1.node_id, n2.node_id], res_id))\n        return Node(self, res_id)\n\n    def sub(self, n1, n2):\n        res_val = self.values[n1.node_id] - self.values[n2.node_id]\n        res_id = self._add_node(res_val)\n        self.tape.append(('sub', [n1.node_id, n2.node_id], res_id))\n        return Node(self, res_id)\n\n    def mul(self, n1, n2):\n        res_val = self.values[n1.node_id] * self.values[n2.node_id]\n        res_id = self._add_node(res_val)\n        self.tape.append(('mul', [n1.node_id, n2.node_id], res_id))\n        return Node(self, res_id)\n\n    def truediv(self, n1, n2):\n        res_val = self.values[n1.node_id] / self.values[n2.node_id]\n        res_id = self._add_node(res_val)\n        self.tape.append(('div', [n1.node_id, n2.node_id], res_id))\n        return Node(self, res_id)\n\n    def sin(self, n1):\n        res_val = np.sin(self.values[n1.node_id])\n        res_id = self._add_node(res_val)\n        self.tape.append(('sin', [n1.node_id], res_id))\n        return Node(self, res_id)\n\n    def exp(self, n1):\n        res_val = np.exp(self.values[n1.node_id])\n        res_id = self._add_node(res_val)\n        self.tape.append(('exp', [n1.node_id], res_id))\n        return Node(self, res_id)\n\n    def log(self, n1):\n        value = self.values[n1.node_id]\n        if value = 0:\n            raise ValueError(\"Domain error: input to log must be positive.\")\n        res_val = np.log(value)\n        res_id = self._add_node(res_val)\n        self.tape.append(('log', [n1.node_id], res_id))\n        return Node(self, res_id)\n\n    def compute_gradients(self, loss_node):\n        \"\"\"Performs the reverse pass to compute gradients for all nodes.\"\"\"\n        num_nodes = len(self.values)\n        self.adjoints = np.zeros(num_nodes)\n        self.adjoints[loss_node.node_id] = 1.0  # Seed the reverse pass\n\n        # Replay the tape in reverse to propagate adjoints\n        for op_type, input_ids, output_id in reversed(self.tape):\n            adjoint_out = self.adjoints[output_id]\n            \n            if adjoint_out == 0.0:  # Optimization: no gradient to propagate\n                continue\n\n            # Apply the chain rule based on the operation\n            if op_type == 'add':\n                self.adjoints[input_ids[0]] += adjoint_out\n                self.adjoints[input_ids[1]] += adjoint_out\n            elif op_type == 'sub':\n                self.adjoints[input_ids[0]] += adjoint_out\n                self.adjoints[input_ids[1]] -= adjoint_out\n            elif op_type == 'mul':\n                val0 = self.values[input_ids[0]]\n                val1 = self.values[input_ids[1]]\n                self.adjoints[input_ids[0]] += adjoint_out * val1\n                self.adjoints[input_ids[1]] += adjoint_out * val0\n            elif op_type == 'div':\n                val0 = self.values[input_ids[0]]\n                val1 = self.values[input_ids[1]]\n                self.adjoints[input_ids[0]] += adjoint_out / val1\n                self.adjoints[input_ids[1]] -= adjoint_out * val0 / (val1**2)\n            elif op_type == 'sin':\n                val0 = self.values[input_ids[0]]\n                self.adjoints[input_ids[0]] += adjoint_out * np.cos(val0)\n            elif op_type == 'exp':\n                out_val = self.values[output_id] # d/dx(exp(x)) = exp(x)\n                self.adjoints[input_ids[0]] += adjoint_out * out_val\n            elif op_type == 'log':\n                val0 = self.values[input_ids[0]]\n                self.adjoints[input_ids[0]] += adjoint_out / val0\n\nclass Node:\n    \"\"\"A node in the computational graph, with overloaded operators.\"\"\"\n    def __init__(self, graph, node_id):\n        self.graph = graph\n        self.node_id = node_id\n\n    @property\n    def value(self):\n        \"\"\"Get the node's numerical value from its graph.\"\"\"\n        return self.graph.values[self.node_id]\n        \n    @property\n    def adjoint(self):\n        \"\"\"Get the node's adjoint after the reverse pass.\"\"\"\n        if self.graph.adjoints is None:\n            raise RuntimeError(\"Gradients not computed yet. Call `compute_gradients` on the loss node first.\")\n        return self.graph.adjoints[self.node_id]\n    \n    # Left-side binary operators\n    def __add__(self, other):\n        other = self.graph._promote_to_node(other)\n        return self.graph.add(self, other)\n\n    def __sub__(self, other):\n        other = self.graph._promote_to_node(other)\n        return self.graph.sub(self, other)\n\n    def __mul__(self, other):\n        other = self.graph._promote_to_node(other)\n        return self.graph.mul(self, other)\n\n    def __truediv__(self, other):\n        other = self.graph._promote_to_node(other)\n        return self.graph.truediv(self, other)\n\n    # Right-side binary operators (for expressions like `5 + x`)\n    def __radd__(self, other):\n        other = self.graph._promote_to_node(other)\n        return self.graph.add(other, self)\n\n    def __rsub__(self, other):\n        other = self.graph._promote_to_node(other)\n        return self.graph.sub(other, self)\n\n    def __rmul__(self, other):\n        other = self.graph._promote_to_node(other)\n        return self.graph.mul(other, self)\n\n    def __rtruediv__(self, other):\n        other = self.graph._promote_to_node(other)\n        return self.graph.truediv(other, self)\n\n    def __repr__(self):\n        return f\"Node(id={self.node_id}, value={self.value:.4f})\"\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: L = sin(x*y) + exp(y), x=0.5, y=-1.0\n        {'vars': {'x': 0.5, 'y': -1.0}, 'expr': lambda x, y: sin(x * y) + exp(y)},\n        # Case 2: L = x*0 + sin(0) + log(1), x=2.0\n        {'vars': {'x': 2.0}, 'expr': lambda x: x * 0.0 + sin(0.0) + log(1.0)},\n        # Case 3: L = (x*x)*x, x=2.0\n        {'vars': {'x': 2.0}, 'expr': lambda x: (x * x) * x},\n        # Case 4: L = x/y + log(y), x=1.0, y=1.5\n        {'vars': {'x': 1.0, 'y': 1.5}, 'expr': lambda x, y: x / y + log(y)},\n        # Case 5: L = exp(sin(x)), x=0.0\n        {'vars': {'x': 0.0}, 'expr': lambda x: exp(sin(x))},\n    ]\n\n    all_results = []\n    for case in test_cases:\n        g = Graph()\n        # Create variable nodes in the specified order (determined by dict insertion order in Python 3.7+)\n        var_nodes = {name: g.variable(val) for name, val in case['vars'].items()}\n        \n        # Build the graph by executing the expression\n        loss_node = case['expr'](**var_nodes)\n        \n        # Compute gradients via reverse-mode AD\n        g.compute_gradients(loss_node)\n        \n        # Collect results: [L, dL/dx1, dL/dx2, ...]\n        case_result = [loss_node.value]\n        for node in var_nodes.values():\n            case_result.append(node.adjoint)\n        \n        all_results.append(f\"[{','.join(map(str, case_result))}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n\n```", "id": "3100018"}, {"introduction": "Once we can compute gradients, we must learn to interpret them. This practice moves from the mechanics of calculation to the goal of optimization by analyzing a simple linear neuron [@problem_id:3099996]. You will use backpropagation to find a point where the gradient of the loss is zero and then compute the Hessian matrix to determine if this point represents a local minimum, connecting the abstract derivative to the tangible geometry of the loss landscape.", "problem": "You are given a single-neuron model with a linear activation, defined by the parametric function $f(x; \\theta) = W x + b$, where $\\theta = (W, b)$, $W \\in \\mathbb{R}$, and $b \\in \\mathbb{R}$. The training set consists of three input-output pairs $(x_i, y_i)$ for $i = 1, 2, 3$, specifically $(x_1, y_1) = (0, 1)$, $(x_2, y_2) = (1, 3)$, and $(x_3, y_3) = (2, 5)$. The empirical risk is the half-sum of squared errors, defined by\n$$\nJ(\\theta) = \\frac{1}{2} \\sum_{i=1}^{3} \\left(f(x_i; \\theta) - y_i\\right)^{2}.\n$$\nStarting from the fundamental definition of the chain rule of calculus and the definition of the gradient and Hessian (the matrix of second-order partial derivatives), do the following:\n1. Choose parameters $W$ and $b$ that exactly fit the three data points, meaning that $f(x_i; \\theta) = y_i$ for every $i \\in \\{1, 2, 3\\}$.\n2. Using backpropagation (that is, the chain rule applied to the computational graph of the model), derive the gradient $\\nabla_{\\theta} J(\\theta)$ and evaluate it at the exact-fit parameters you chose in part $1$.\n3. Derive the Hessian $H(\\theta)$ of $J(\\theta)$ with respect to $\\theta$ and evaluate it at the exact-fit parameters. Compute the minimum eigenvalue $\\lambda_{\\min}(H)$.\n4. Based on the sign of $\\lambda_{\\min}(H)$, briefly state whether the exact-fit point is a local minimum or a saddle point for $J(\\theta)$.\n\nProvide your final answer as the exact value of $\\lambda_{\\min}(H)$ at the solution. No rounding is required.", "solution": "The problem has been validated and is scientifically sound, well-posed, objective, and contains sufficient information for a unique solution.\n\nThe task is to analyze the empirical risk function $J(\\theta)$ for a single linear neuron model $f(x; \\theta) = W x + b$ with parameters $\\theta = (W, b)$. The risk is defined as the half-sum of squared errors over three data points: $(x_1, y_1) = (0, 1)$, $(x_2, y_2) = (1, 3)$, and $(x_3, y_3) = (2, 5)$. The risk function is:\n$$\nJ(W, b) = \\frac{1}{2} \\sum_{i=1}^{3} \\left( (Wx_i + b) - y_i \\right)^{2}\n$$\nSubstituting the given data points:\n$$\nJ(W, b) = \\frac{1}{2} \\left[ (W(0) + b - 1)^{2} + (W(1) + b - 3)^{2} + (W(2) + b - 5)^{2} \\right]\n$$\n$$\nJ(W, b) = \\frac{1}{2} \\left[ (b - 1)^{2} + (W + b - 3)^{2} + (2W + b - 5)^{2} \\right]\n$$\n\n**1. Find the exact-fit parameters $\\theta^* = (W, b)$**\n\nFor an exact fit, the model must satisfy $f(x_i; \\theta) = y_i$ for all $i \\in \\{1, 2, 3\\}$. This yields a system of linear equations for $W$ and $b$:\n\\begin{enumerate}\n    \\item For $(x_1, y_1) = (0, 1)$: $W(0) + b = 1 \\implies b = 1$.\n    \\item For $(x_2, y_2) = (1, 3)$: $W(1) + b = 3 \\implies W + b = 3$.\n    \\item For $(x_3, y_3) = (2, 5)$: $W(2) + b = 5 \\implies 2W + b = 5$.\n\\end{enumerate}\nSubstituting $b = 1$ from the first equation into the second gives $W + 1 = 3$, which implies $W = 2$.\nWe must verify that these values satisfy the third equation: $2W + b = 2(2) + 1 = 4 + 1 = 5$, which is consistent with $y_3 = 5$.\nThus, the parameters for an exact fit are $W = 2$ and $b = 1$. Let us denote this point as $\\theta^* = (2, 1)$.\n\n**2. Derive and evaluate the gradient $\\nabla_{\\theta} J(\\theta)$ at $\\theta^*$**\n\nThe gradient of $J(\\theta)$ with respect to $\\theta = (W, b)$ is $\\nabla_{\\theta} J = \\begin{pmatrix} \\frac{\\partial J}{\\partial W} \\\\ \\frac{\\partial J}{\\partial b} \\end{pmatrix}$.\nUsing the chain rule, as specified by the backpropagation methodology, we define the error for each point as $e_i(\\theta) = f(x_i; \\theta) - y_i = Wx_i + b - y_i$. The loss is $J = \\frac{1}{2} \\sum_{i=1}^3 e_i^2$.\nThe partial derivatives are:\n$$\n\\frac{\\partial J}{\\partial W} = \\sum_{i=1}^{3} \\frac{\\partial J}{\\partial e_i} \\frac{\\partial e_i}{\\partial W} = \\sum_{i=1}^{3} e_i \\cdot x_i = \\sum_{i=1}^{3} (Wx_i + b - y_i) x_i\n$$\n$$\n\\frac{\\partial J}{\\partial b} = \\sum_{i=1}^{3} \\frac{\\partial J}{\\partial e_i} \\frac{\\partial e_i}{\\partial b} = \\sum_{i=1}^{3} e_i \\cdot 1 = \\sum_{i=1}^{3} (Wx_i + b - y_i)\n$$\nAt the exact-fit point $\\theta^* = (2, 1)$, by definition, the error terms are zero: $e_i(\\theta^*) = Wx_i + b - y_i = 0$ for all $i$.\nTherefore, evaluating the gradient at $\\theta^*$:\n$$\n\\frac{\\partial J}{\\partial W}\\bigg|_{\\theta^*} = \\sum_{i=1}^{3} (0) x_i = 0\n$$\n$$\n\\frac{\\partial J}{\\partial b}\\bigg|_{\\theta^*} = \\sum_{i=1}^{3} (0) = 0\n$$\nThe gradient at the exact-fit point is the zero vector: $\\nabla_{\\theta} J(\\theta^*) = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$. This confirms that $\\theta^*$ is a critical point of the loss function $J(\\theta)$.\n\n**3. Derive the Hessian $H(\\theta)$ and compute its minimum eigenvalue**\n\nThe Hessian matrix $H(\\theta)$ contains the second-order partial derivatives of $J(\\theta)$:\n$$\nH(\\theta) = \\begin{pmatrix} \\frac{\\partial^2 J}{\\partial W^2}  \\frac{\\partial^2 J}{\\partial W \\partial b} \\\\ \\frac{\\partial^2 J}{\\partial b \\partial W}  \\frac{\\partial^2 J}{\\partial b^2} \\end{pmatrix}\n$$\nWe compute these by differentiating the first-order partial derivatives:\n$$\n\\frac{\\partial^2 J}{\\partial W^2} = \\frac{\\partial}{\\partial W} \\left[ \\sum_{i=1}^{3} (Wx_i + b - y_i) x_i \\right] = \\sum_{i=1}^{3} x_i^2\n$$\n$$\n\\frac{\\partial^2 J}{\\partial b \\partial W} = \\frac{\\partial}{\\partial b} \\left[ \\sum_{i=1}^{3} (Wx_i + b - y_i) x_i \\right] = \\sum_{i=1}^{3} x_i\n$$\n$$\n\\frac{\\partial^2 J}{\\partial b^2} = \\frac{\\partial}{\\partial b} \\left[ \\sum_{i=1}^{3} (Wx_i + b - y_i) \\right] = \\sum_{i=1}^{3} 1 = 3\n$$\nNote that $\\frac{\\partial^2 J}{\\partial W \\partial b} = \\frac{\\partial^2 J}{\\partial b \\partial W}$, as expected. The Hessian is constant and does not depend on $W$ or $b$. We evaluate the sums using the given inputs $x_1=0$, $x_2=1$, $x_3=2$:\n$$\n\\sum_{i=1}^{3} x_i^2 = 0^2 + 1^2 + 2^2 = 0 + 1 + 4 = 5\n$$\n$$\n\\sum_{i=1}^{3} x_i = 0 + 1 + 2 = 3\n$$\nThe Hessian matrix is:\n$$\nH = \\begin{pmatrix} 5  3 \\\\ 3  3 \\end{pmatrix}\n$$\nThe eigenvalues $\\lambda$ of $H$ are the roots of the characteristic equation $\\det(H - \\lambda I) = 0$:\n$$\n\\det \\begin{pmatrix} 5-\\lambda  3 \\\\ 3  3-\\lambda \\end{pmatrix} = (5-\\lambda)(3-\\lambda) - (3)(3) = 0\n$$\n$$\n15 - 8\\lambda + \\lambda^2 - 9 = 0\n$$\n$$\n\\lambda^2 - 8\\lambda + 6 = 0\n$$\nUsing the quadratic formula $\\lambda = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$:\n$$\n\\lambda = \\frac{8 \\pm \\sqrt{(-8)^2 - 4(1)(6)}}{2} = \\frac{8 \\pm \\sqrt{64 - 24}}{2} = \\frac{8 \\pm \\sqrt{40}}{2}\n$$\nSimplifying $\\sqrt{40} = \\sqrt{4 \\cdot 10} = 2\\sqrt{10}$:\n$$\n\\lambda = \\frac{8 \\pm 2\\sqrt{10}}{2} = 4 \\pm \\sqrt{10}\n$$\nThe two eigenvalues are $\\lambda_1 = 4 + \\sqrt{10}$ and $\\lambda_2 = 4 - \\sqrt{10}$. The minimum eigenvalue is $\\lambda_{\\min}(H) = 4 - \\sqrt{10}$.\n\n**4. Classify the critical point $\\theta^*$**\n\nTo classify the critical point $\\theta^*$, we examine the signs of the eigenvalues of the Hessian matrix evaluated at that point. Since $H$ is constant, we use the eigenvalues just computed.\nWe know that $3 = \\sqrt{9}  \\sqrt{10}  \\sqrt{16} = 4$.\nTherefore, the minimum eigenvalue $\\lambda_{\\min}(H) = 4 - \\sqrt{10}$ is positive, as $4  \\sqrt{10}$.\nThe maximum eigenvalue $\\lambda_{\\max}(H) = 4 + \\sqrt{10}$ is also clearly positive.\nSince both eigenvalues of the Hessian are positive, the Hessian matrix is positive definite. According to the second partial derivative test, a critical point at which the Hessian is positive definite is a local minimum. For this quadratic loss function, it is the unique global minimum. The exact-fit point is a local minimum.\nThe final answer is the value of the minimum eigenvalue.", "answer": "$$\n\\boxed{4 - \\sqrt{10}}\n$$", "id": "3099996"}, {"introduction": "How can you trust that your implementation of backpropagation is correct for a complex model? This final practice introduces the indispensable professional technique of gradient checking [@problem_id:3100954]. By comparing the analytical gradient computed by backpropagation to a numerical approximation via finite differences, you will learn how to rigorously verify and debug your code, a crucial skill for building reliable machine learning systems.", "problem": "Construct a verification of the backpropagation algorithm by comparing an analytically derived gradient of a two-layer neural network to a finite-difference approximation. The objective is to numerically confirm that the discrepancy between the two gradients behaves as an order-$\\epsilon$ truncation error when using a forward finite difference, that is, the error is $\\mathcal{O}(\\epsilon)$.\n\nUse the following purely mathematical setup.\n\n- Network architecture and data:\n  - Inputs have dimension $d = 3$, the hidden layer has $h = 3$ units with hyperbolic tangent activation, and the output layer has dimension $o = 1$ with a linear output.\n  - Given a mini-batch of size $n = 4$, the input matrix $X \\in \\mathbb{R}^{4 \\times 3}$ and target vector $y \\in \\mathbb{R}^{4 \\times 1}$ are:\n    $$\n    X =\n    \\begin{bmatrix}\n    0.2  -0.1  0.4 \\\\\n    -0.5  0.3  0.1 \\\\\n    0.0  -0.2  0.2 \\\\\n    0.1  0.4  -0.3\n    \\end{bmatrix}, \\quad\n    y =\n    \\begin{bmatrix}\n    0.5 \\\\ -0.1 \\\\ 0.2 \\\\ 0.0\n    \\end{bmatrix}.\n    $$\n  - Parameters for Test Case A are fixed as:\n    $$\n    W_1 =\n    \\begin{bmatrix}\n    0.3  -0.1  0.2 \\\\\n    -0.4  0.5  0.1 \\\\\n    0.2  0.3  -0.2\n    \\end{bmatrix}, \\quad\n    b_1 =\n    \\begin{bmatrix}\n    0.05 \\\\ -0.02 \\\\ 0.01\n    \\end{bmatrix}, \\quad\n    W_2 =\n    \\begin{bmatrix}\n    0.6  -0.7  0.2\n    \\end{bmatrix}, \\quad\n    b_2 =\n    \\begin{bmatrix}\n    0.03\n    \\end{bmatrix}.\n    $$\n    For Test Case B, use the same shapes but scale every entry of $W_1, b_1, W_2, b_2$ by a factor of $0.1$.\n\n- Forward model and loss:\n  - For each row $x_i^\\top$ of $X$, define the hidden pre-activation $z_{1,i}^\\top = x_i^\\top W_1^\\top + b_1^\\top$, hidden activation $h_i^\\top = \\tanh(z_{1,i}^\\top)$, output pre-activation $z_{2,i} = h_i^\\top W_2^\\top + b_2$, and prediction $\\hat{y}_i = z_{2,i}$.\n  - Define the mean squared error loss\n    $$\n    L(W_1,b_1,W_2,b_2) = \\frac{1}{2n} \\sum_{i=1}^{n} \\left( \\hat{y}_i - y_i \\right)^2.\n    $$\n\n- Analytical gradient via backpropagation:\n  - Using multivariable calculus, the chain rule, and the derivative identity $\\frac{d}{dz}\\tanh(z) = 1 - \\tanh^2(z)$, derive the gradient of $L$ with respect to all parameters and implement it.\n  - Flatten the parameter set into a single vector $\\theta \\in \\mathbb{R}^{p}$ with $p = 16$ using the following order and memory layout:\n    1. Flatten $W_1 \\in \\mathbb{R}^{3 \\times 3}$ in row-major order.\n    2. Append $b_1 \\in \\mathbb{R}^{3}$.\n    3. Flatten $W_2 \\in \\mathbb{R}^{1 \\times 3}$ in row-major order.\n    4. Append $b_2 \\in \\mathbb{R}^{1}$.\n\n- Finite-difference approximation:\n  - For a given $\\epsilon  0$ and the standard basis vector $e_k$ in $\\mathbb{R}^p$, approximate the $k$-th component of $\\nabla_{\\theta} L$ by the forward difference\n    $$\n    \\frac{L(\\theta + \\epsilon e_k) - L(\\theta)}{\\epsilon}.\n    $$\n  - Use the list of step sizes\n    $$\n    \\mathcal{E} = \\left[ 10^{-1}, \\; 3 \\cdot 10^{-2}, \\; 10^{-2}, \\; 3 \\cdot 10^{-3}, \\; 10^{-3}, \\; 3 \\cdot 10^{-4}, \\; 10^{-4} \\right].\n    $$\n\n- Error metric and order verification:\n  - For each $\\epsilon \\in \\mathcal{E}$, compute the Euclidean norm of the difference between the analytical gradient and the finite-difference gradient,\n    $$\n    \\mathrm{err}(\\epsilon) = \\left\\| \\nabla_{\\theta} L - g_{\\mathrm{FD}}(\\epsilon) \\right\\|_2.\n    $$\n  - For consecutive $\\epsilon_i  \\epsilon_{i+1}$, compute the empirical order\n    $$\n    s_i = \\frac{\\log\\left( \\mathrm{err}(\\epsilon_i) / \\mathrm{err}(\\epsilon_{i+1}) \\right)}{\\log\\left( \\epsilon_i / \\epsilon_{i+1} \\right)}.\n    $$\n  - Define two boolean checks per test case:\n    1. Let $s_{\\mathrm{med}}$ be the median of $\\{ s_i \\}$. Define $\\mathrm{pass\\_order}$ to be true if $0.8 \\le s_{\\mathrm{med}} \\le 1.2$.\n    2. Define $\\mathrm{pass\\_mono}$ to be true if $\\mathrm{err}(\\epsilon)$ is strictly decreasing over the first $5$ values of $\\mathcal{E}$, that is, for $\\epsilon \\in \\{ 10^{-1}, 3\\cdot 10^{-2}, 10^{-2}, 3\\cdot 10^{-3}, 10^{-3} \\}$.\n\n- Test suite:\n  - Two test cases are specified by the parameter sets:\n    - Test Case A: the parameters exactly as given above.\n    - Test Case B: the same parameter shapes with every entry scaled by $0.1$ relative to Test Case A.\n  - In both cases, use the same $X$, $y$, and the same $\\mathcal{E}$.\n\n- Required program behavior and final output format:\n  - Your program must implement the forward model, derive and compute the analytical gradient via backpropagation from first principles, compute the finite-difference gradients for each $\\epsilon \\in \\mathcal{E}$, and evaluate the error norms and empirical orders.\n  - For each test case, produce a list with four entries: \n    $[ \\mathrm{err}(\\min \\mathcal{E}), \\; s_{\\mathrm{med}}, \\; \\mathrm{pass\\_order}, \\; \\mathrm{pass\\_mono} ]$.\n  - The final program output must be a single line containing a list with the two per-case lists, formatted exactly as a comma-separated list enclosed in square brackets, for example:\n    $$\n    \\left[ [a_1, a_2, a_3, a_4], [b_1, b_2, b_3, b_4] \\right]\n    $$\n    where each $a_j$ and $b_j$ is a boolean or a floating-point number. No other text must be printed.\n  - There are no physical units involved in this problem.\n\nYour implementation must be self-contained and must not read input. It must use the specified numerical values exactly as provided above.", "solution": "The objective is to numerically verify the correctness of the backpropagation algorithm for a two-layer neural network. This is achieved by comparing the analytically computed gradient with a numerical approximation obtained via the finite-difference method. The primary verification criterion is to confirm that the error between the analytical and numerical gradients decreases linearly with the finite-difference step size $\\epsilon$, characteristic of a forward-difference scheme's first-order truncation error, $\\mathcal{O}(\\epsilon)$.\n\n### Mathematical Model and Loss Function\n\nThe neural network architecture consists of an input layer, one hidden layer, and an output layer.\n- Input $X \\in \\mathbb{R}^{n \\times d}$ ($n=4, d=3$)\n- Hidden layer with $h=3$ units and $\\tanh$ activation.\n- Output layer with $o=1$ unit and linear activation.\n- Parameters: $W_1 \\in \\mathbb{R}^{h \\times d}$, $b_1 \\in \\mathbb{R}^{h \\times 1}$, $W_2 \\in \\mathbb{R}^{o \\times h}$, $b_2 \\in \\mathbb{R}^{o \\times 1}$.\n\nThe forward propagation of a mini-batch $X$ is defined by the following matrix operations:\n1.  **Hidden Layer Pre-activation**: The linear transformation for the hidden layer is given by $Z_1 = X W_1^\\top + \\mathbf{1}b_1^\\top$, where $\\mathbf{1} \\in \\mathbb{R}^{n \\times 1}$ is a vector of ones and its product with $b_1^\\top$ is handled via broadcasting. The resulting matrix $Z_1 \\in \\mathbb{R}^{n \\times h}$.\n2.  **Hidden Layer Activation**: The hyperbolic tangent activation function is applied element-wise: $H = \\tanh(Z_1)$, where $H \\in \\mathbb{R}^{n \\times h}$.\n3.  **Output Layer Pre-activation**: A second linear transformation produces the output pre-activations: $Z_2 = H W_2^\\top + \\mathbf{1}b_2^\\top$. The resulting matrix $Z_2 \\in \\mathbb{R}^{n \\times o}$.\n4.  **Prediction**: The network output is linear, so the prediction $\\hat{Y}$ is equal to the pre-activation: $\\hat{Y} = Z_2 \\in \\mathbb{R}^{n \\times o}$.\n\nThe performance of the network is quantified by the mean squared error (MSE) loss function, averaged over the mini-batch:\n$$\nL = \\frac{1}{2n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i)^2 = \\frac{1}{2n} \\|\\hat{Y} - Y\\|_F^2\n$$\nwhere $Y \\in \\mathbb{R}^{n \\times o}$ is the matrix of true target values.\n\n### Analytical Gradient via Backpropagation\n\nThe core of this task is to derive the gradient of the loss $L$ with respect to each parameter ($W_1, b_1, W_2, b_2$) using the multivariable chain rule. This process is known as backpropagation. We denote the gradient of the loss with respect to a matrix $M$ as $\\delta_M = \\frac{\\partial L}{\\partial M}$.\n\n1.  **Gradient at the Output**: The gradient of the loss with respect to the network's predictions $\\hat{Y}$ is:\n    $$\n    \\delta_{\\hat{Y}} = \\frac{\\partial L}{\\partial \\hat{Y}} = \\frac{1}{n} (\\hat{Y} - Y)\n    $$\n    Since $\\hat{Y} =Z_2$, we have $\\delta_{Z_2} = \\delta_{\\hat{Y}}$.\n\n2.  **Gradients for the Output Layer ($W_2, b_2$)**:\n    Using the chain rule on $Z_2 = H W_2^\\top + \\mathbf{1}b_2^\\top$:\n    $$\n    \\delta_{W_2} = \\frac{\\partial L}{\\partial W_2} = \\delta_{Z_2}^\\top H\n    $$\n    $$\n    \\delta_{b_2} = \\frac{\\partial L}{\\partial b_2} = (\\mathrm{sum}(\\delta_{Z_2}, \\text{axis}=0))^\\top = \\delta_{Z_2}^\\top \\mathbf{1}\n    $$\n\n3.  **Propagating the Gradient to the Hidden Layer**:\n    The gradient is propagated back to the hidden layer's activations $H$:\n    $$\n    \\delta_H = \\frac{\\partial L}{\\partial H} = \\delta_{Z_2} W_2\n    $$\n    Next, the gradient is propagated through the $\\tanh$ activation function, using $\\frac{d}{dz}\\tanh(z) = 1 - \\tanh^2(z)$:\n    $$\n    \\delta_{Z_1} = \\frac{\\partial L}{\\partial Z_1} = \\delta_H \\odot (1 - H^2)\n    $$\n    where $\\odot$ denotes the element-wise (Hadamard) product.\n\n4.  **Gradients for the Hidden Layer ($W_1, b_1$)**:\n    Finally, using the chain rule on $Z_1 = X W_1^\\top + \\mathbf{1}b_1^\\top$:\n    $$\n    \\delta_{W_1} = \\frac{\\partial L}{\\partial W_1} = \\delta_{Z_1}^\\top X\n    $$\n    $$\n    \\delta_{b_1} = \\frac{\\partial L}{\\partial b_1} = (\\mathrm{sum}(\\delta_{Z_1}, \\text{axis}=0))^\\top = \\delta_{Z_1}^\\top \\mathbf{1}\n    $$\n\nThese matrix-form equations provide a complete algorithm for computing the analytical gradient.\n\n### Numerical Verification\n\nTo verify the analytical gradient, we compare it against a numerical approximation.\n\n- **Parameter Vectorization**: All network parameters ($W_1, b_1, W_2, b_2$) are flattened and concatenated into a single vector $\\theta \\in \\mathbb{R}^{p}$, with $p=16$. The specified order is $W_1$ (row-major), $b_1$, $W_2$ (row-major), and $b_2$.\n\n- **Finite-Difference Approximation**: The gradient is approximated using the first-order forward-difference formula. The $k$-th component of the gradient vector is estimated as:\n  $$\n  g_{\\mathrm{FD},k}(\\epsilon) = \\frac{L(\\theta + \\epsilon e_k) - L(\\theta)}{\\epsilon}\n  $$\n  where $e_k$ is the $k$-th standard basis vector and $\\epsilon$ is a small step size.\n\n- **Error Analysis and Order Verification**:\n  The discrepancy between the analytical gradient $\\nabla_\\theta L$ and the finite-difference approximation $g_{\\mathrm{FD}}(\\epsilon)$ is measured by the Euclidean norm of their difference:\n  $$\n  \\mathrm{err}(\\epsilon) = \\| \\nabla_\\theta L - g_{\\mathrm{FD}}(\\epsilon) \\|_2\n  $$\n  For a first-order method, this error is expected to be proportional to $\\epsilon$, i.e., $\\mathrm{err}(\\epsilon) \\approx C\\epsilon$. This implies that the ratio of errors for two step sizes $\\epsilon_i$ and $\\epsilon_{i+1}$ should be approximately equal to the ratio of the step sizes themselves. To quantify this relationship, we compute the empirical order of convergence $s_i$:\n  $$\n  s_i = \\frac{\\log(\\mathrm{err}(\\epsilon_i) / \\mathrm{err}(\\epsilon_{i+1}))}{\\log(\\epsilon_i / \\epsilon_{i+1})}\n  $$\n  A value of $s_i \\approx 1$ confirms the expected first-order convergence, thus validating the analytical gradient implementation. We use the median of the computed $s_i$ values for robustness. The verification is considered successful if this median order $s_{\\mathrm{med}}$ is within the range $[0.8, 1.2]$ and if the error is monotonically decreasing for the initial, larger values of $\\epsilon$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Constructs and solves the backpropagation verification problem.\n    \"\"\"\n    \n    # --- Givens from the problem statement ---\n    X = np.array([\n        [0.2, -0.1, 0.4],\n        [-0.5, 0.3, 0.1],\n        [0.0, -0.2, 0.2],\n        [0.1, 0.4, -0.3]\n    ])\n    y = np.array([\n        [0.5],\n        [-0.1],\n        [0.2],\n        [0.0]\n    ])\n    n = X.shape[0]  # Mini-batch size, n=4\n\n    # Parameters for Test Case A\n    W1_a = np.array([\n        [0.3, -0.1, 0.2],\n        [-0.4, 0.5, 0.1],\n        [0.2, 0.3, -0.2]\n    ])\n    b1_a = np.array([[0.05], [-0.02], [0.01]])\n    W2_a = np.array([[0.6, -0.7, 0.2]])\n    b2_a = np.array([[0.03]])\n    \n    test_cases_params = [\n        (W1_a, b1_a, W2_a, b2_a),\n        (W1_a * 0.1, b1_a * 0.1, W2_a * 0.1, b2_a * 0.1) # Test Case B\n    ]\n\n    epsilons = [1e-1, 3e-2, 1e-2, 3e-3, 1e-3, 3e-4, 1e-4]\n    \n    # --- Helper functions for parameter manipulation ---\n    def params_to_vec(W1, b1, W2, b2):\n        return np.concatenate([\n            W1.flatten(),\n            b1.flatten(),\n            W2.flatten(),\n            b2.flatten()\n        ])\n\n    def vec_to_params(theta):\n        W1 = theta[0:9].reshape(3, 3)\n        b1 = theta[9:12].reshape(3, 1)\n        W2 = theta[12:15].reshape(1, 3)\n        b2 = theta[15:16].reshape(1, 1)\n        return W1, b1, W2, b2\n        \n    # --- Core functions for forward/backward pass ---\n    def forward_pass(W1, b1, W2, b2):\n        Z1 = X @ W1.T + b1.T\n        H = np.tanh(Z1)\n        Z2 = H @ W2.T + b2\n        Y_hat = Z2\n        loss = (1 / (2 * n)) * np.sum((Y_hat - y)**2)\n        return loss, Z1, H, Y_hat\n\n    def analytical_gradient(W1, b1, W2, b2):\n        _loss, _Z1, H, Y_hat = forward_pass(W1, b1, W2, b2)\n        \n        # Backward pass\n        dZ2 = (1 / n) * (Y_hat - y)\n        dW2 = dZ2.T @ H\n        db2 = np.sum(dZ2, axis=0, keepdims=True)\n        \n        dH = dZ2 @ W2\n        dZ1 = dH * (1 - H**2)\n        dW1 = dZ1.T @ X\n        db1 = np.sum(dZ1, axis=0, keepdims=True).T\n        \n        return params_to_vec(dW1, db1, dW2, db2)\n\n    def run_verification(params):\n        W1, b1, W2, b2 = params\n        \n        # 1. Compute analytical gradient\n        grad_analytic = analytical_gradient(W1, b1, W2, b2)\n        \n        # 2. Compute finite-difference gradient for each epsilon\n        theta_base = params_to_vec(W1, b1, W2, b2)\n        loss_base, _, _, _ = forward_pass(W1, b1, W2, b2)\n        \n        errors = []\n        for eps in epsilons:\n            grad_fd = np.zeros_like(theta_base)\n            for k in range(len(theta_base)):\n                theta_p = np.copy(theta_base)\n                theta_p[k] += eps\n                \n                W1_p, b1_p, W2_p, b2_p = vec_to_params(theta_p)\n                loss_p, _, _, _ = forward_pass(W1_p, b1_p, W2_p, b2_p)\n                \n                grad_fd[k] = (loss_p - loss_base) / eps\n            \n            error = np.linalg.norm(grad_analytic - grad_fd)\n            errors.append(error)\n\n        # 3. Compute empirical orders\n        orders = []\n        for i in range(len(epsilons) - 1):\n            s_i = np.log(errors[i] / errors[i+1]) / np.log(epsilons[i] / epsilons[i+1])\n            orders.append(s_i)\n        \n        s_med = np.median(orders)\n        \n        # 4. Perform boolean checks\n        pass_order = 0.8 = s_med = 1.2\n        pass_mono = all(errors[i]  errors[i+1] for i in range(4))\n\n        # 5. Collect results\n        err_min_eps = errors[-1]\n        \n        return [err_min_eps, s_med, bool(pass_order), bool(pass_mono)]\n\n    # --- Main execution loop ---\n    final_results = []\n    for case_params in test_cases_params:\n        result = run_verification(case_params)\n        final_results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n\n```", "id": "3100954"}]}