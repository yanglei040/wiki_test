{"hands_on_practices": [{"introduction": "This first exercise guides you through the process of building an adaptive integrator from fundamental principles. You will start with the simple trapezoidal rule and derive a local error estimator by comparing an approximation on a single interval to a more refined one on its two halves [@problem_id:3284319]. This hands-on implementation is crucial for understanding how adaptive algorithms intelligently focus computational effort, forming the foundation of efficient numerical integration.", "problem": "You are tasked with building a principled, self-contained program that implements an adaptive numerical integration scheme grounded in first principles for the trapezoidal rule. The core objective is to approximate the definite integral $\\int_{a}^{b} f(x)\\,dx$ for a series of test functions using an adaptive refinement strategy that estimates local error by comparing the trapezoidal approximation on the whole interval and the sum of trapezoidal approximations on its two halves.\n\nBegin from the definition of the definite integral as the limit of Riemann sums and the construction of the linear interpolant of $f(x)$ between $x=a$ and $x=b$. The trapezoidal rule for a single interval arises by integrating this linear interpolant over $[a,b]$. Your algorithm must:\n- On any subinterval $[a,b]$ with width $h=b-a$, compute the trapezoidal approximation on the whole interval and the combined trapezoidal approximation on its two halves $[a,m]$ and $[m,b]$ with $m=(a+b)/2$.\n- Use only the comparison between these two approximations (the whole interval versus the sum over two halves) to design a local error estimator, deduced from sound reasoning about how the local truncation error scales with interval width when the interval is halved. Do not assume or use any shortcut formulas not derived from this error-scaling reasoning.\n- Accept a subinterval if the estimated local error is below a prescribed tolerance, optionally using a bias-reduced corrected estimate derived from the same error-scaling principle; otherwise, split the interval and recurse on each half.\n- Ensure termination through a maximum recursion depth parameter $D_{\\max}$ and handle degenerate intervals with $a=b$ correctly.\n\nAngle units for any trigonometric function must be in radians. There are no physical units in this problem. All numeric tolerances in the test suite are absolute tolerances.\n\nImplement your program to evaluate the following test suite. For each test case, compute the integral approximation using your adaptive trapezoidal scheme with the given tolerance, and aggregate the results into a single line of output in the specified format.\n\nTest Suite:\n1. $f(x)=\\sin(x)$ on $[0,\\pi]$ with tolerance $10^{-12}$.\n2. $f(x)=e^{-x^{2}}$ on $[0,1]$ with tolerance $10^{-12}$.\n3. $f(x)=\\dfrac{1}{1+x^{2}}$ on $[-5,5]$ with tolerance $10^{-10}$.\n4. $f(x)=|x|$ on $[-1,1]$ with tolerance $10^{-10}$.\n5. $f(x)=\\dfrac{\\sin(100x)}{1+x^{2}}$ on $[0,1]$ with tolerance $10^{-8}$.\n6. $f(x)=5$ on $[2,5]$ with tolerance $10^{-12}$.\n7. $f(x)=\\sin(x)$ on $[1,1]$ (zero-length interval) with tolerance $10^{-12}$.\n\nDesign for coverage:\n- The first case is a smooth periodic function over one full period.\n- The second case has a bell-shaped integrand with rapidly decaying tails within the interval.\n- The third case tests rational integrands over a symmetric large interval.\n- The fourth case tests a non-differentiable integrand at the midpoint.\n- The fifth case is oscillatory with moderate damping.\n- The sixth case is constant and should terminate immediately.\n- The seventh case is a boundary case with zero-length interval.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with no additional whitespace or text. For example: \"[r1,r2,r3,r4,r5,r6,r7]\". Each $r_{i}$ must be a floating-point number representing the integral approximation for the corresponding test case, computed by your adaptive trapezoidal method.", "solution": "The problem requires the development of an adaptive numerical integration scheme based on the trapezoidal rule. The core of the task is to derive an error estimation and refinement strategy from first principles, specifically by comparing a coarse approximation on an interval with a more refined one.\n\n### Principle-Based Derivation\n\nLet the definite integral to be approximated be $I = \\int_{a}^{b} f(x)\\,dx$.\n\n**1. The Trapezoidal Rule from First Principles**\n\nThe trapezoidal rule approximates the integrand $f(x)$ with a linear polynomial $p_1(x)$ that interpolates the function at the endpoints of the interval $[a, b]$. The coordinates of these points are $(a, f(a))$ and $(b, f(b))$. The linear interpolant is given by:\n$$p_1(x) = f(a) + \\frac{f(b) - f(a)}{b-a}(x - a)$$\nThe integral of this linear polynomial over the interval $[a, b]$ provides the trapezoidal approximation, denoted as $T(a,b)$. Let $h = b - a$ be the width of the interval.\n$$T(a,b) = \\int_{a}^{b} p_1(x) \\,dx = \\int_{a}^{b} \\left( f(a) + \\frac{f(b) - f(a)}{h}(x - a) \\right) \\,dx$$\n$$= \\left[ f(a)x + \\frac{f(b) - f(a)}{h} \\left( \\frac{x^2}{2} - ax \\right) \\right]_{a}^{b}$$\n$$= f(a)(b-a) + \\frac{f(b) - f(a)}{h} \\left( \\left(\\frac{b^2}{2} - ab\\right) - \\left(\\frac{a^2}{2} - a^2\\right) \\right)$$\n$$= f(a)h + \\frac{f(b) - f(a)}{h} \\left( \\frac{b^2 - 2ab + a^2}{2} \\right) = f(a)h + \\frac{f(b) - f(a)}{h} \\frac{(b-a)^2}{2}$$\n$$= f(a)h + (f(b) - f(a))\\frac{h}{2} = \\frac{h}{2}(2f(a) + f(b) - f(a)) = \\frac{h}{2}(f(a) + f(b))$$\nThis is the single-panel trapezoidal rule. Let's call this coarse approximation $S_1$.\n$$S_1 = \\frac{h}{2}(f(a) + f(b))$$\n\n**2. Error Estimation via Refinement**\n\nTo estimate the error, we compare $S_1$ with a more accurate approximation, $S_2$, obtained by splitting the interval $[a, b]$ into two subintervals of equal width, $[a, m]$ and $[m, b]$, where $m = (a+b)/2$. The width of each subinterval is $h/2$. The approximation $S_2$ is the sum of the trapezoidal rule applied to each subinterval:\n$$S_2 = T(a, m) + T(m, b) = \\frac{h/2}{2}(f(a) + f(m)) + \\frac{h/2}{2}(f(m) + f(b))$$\n$$S_2 = \\frac{h}{4}(f(a) + 2f(m) + f(b))$$\n\nThe local truncation error for the trapezoidal rule on an interval of width $w$ is given by $E(w) = -\\frac{w^3}{12}f''(\\xi)$ for some $\\xi$ in the interval, assuming $f$ is twice continuously differentiable. This shows that the error is proportional to the cube of the interval width, i.e., $E(w) \\approx Cw^3$.\n\nThe true integral $I$ can be related to our approximations $S_1$ and $S_2$ as follows:\n$I = S_1 + E(h) \\approx S_1 + Ch^3$\n$I = S_2 + E(h/2) + E(h/2) \\approx S_2 + 2C(h/2)^3 = S_2 + \\frac{Ch^3}{4}$\n\nWe now have a system of two equations for the two unknowns, $I$ and $C$:\n$I - S_1 \\approx Ch^3$\n$I - S_2 \\approx \\frac{Ch^3}{4}$\n\nSubtracting the second equation from the first yields:\n$(I - S_2) - (I - S_1) \\approx \\frac{Ch^3}{4} - Ch^3 \\implies S_1 - S_2 \\approx -\\frac{3}{4}Ch^3$\n\nThis allows us to express the unknown term $Ch^3$ in terms of our computed quantities $S_1$ and $S_2$:\n$Ch^3 \\approx \\frac{4}{3}(S_2 - S_1)$\n\nThe error in the more accurate approximation, $E_2 = I - S_2$, can now be estimated.\n$E_2 \\approx \\frac{Ch^3}{4} \\approx \\frac{1}{4} \\left( \\frac{4}{3}(S_2 - S_1) \\right) = \\frac{1}{3}(S_2 - S_1)$\n\nThe absolute local error for the refined approximation $S_2$ can thus be estimated as:\n$$\\text{err} \\approx \\frac{1}{3}|S_2 - S_1|$$\nThis estimator is derived solely from the comparison of the two approximations and the scaling property of the local error, as required.\n\n**3. Adaptive Algorithm and Bias Reduction**\n\nThe adaptive algorithm proceeds recursively. For a given interval $[a, b]$ and an absolute tolerance $\\tau$:\n1.  Calculate $S_1$, $S_2$, and the error estimate $\\text{err} = \\frac{1}{3}|S_2 - S_1|$.\n2.  If $\\text{err} \\lt \\tau$, the interval is considered adequately approximated. The process for this branch terminates.\n3.  If $\\text{err} \\ge \\tau$, the interval is split into $[a, m]$ and $[m, b]$. The algorithm is then called recursively on each subinterval, with the tolerance budget split accordingly, typically $\\tau/2$ for each. The results from the recursive calls are summed.\n\nThe problem mentions using a \"bias-reduced corrected estimate\". This is an application of Richardson extrapolation. A better estimate for the true integral $I$ can be obtained by correcting $S_2$ with our error estimate $E_2$:\n$$I \\approx S_2 + E_2 \\approx S_2 + \\frac{1}{3}(S_2 - S_1) = \\frac{4S_2 - S_1}{3}$$\nThis corrected value is, in fact, Simpson's rule for the interval $[a,b]$:\n$$\\frac{4}{3} \\left( \\frac{h}{4}(f(a) + 2f(m) + f(b)) \\right) - \\frac{1}{3} \\left( \\frac{h}{2}(f(a) + f(b)) \\right) = \\frac{h}{3}(f(a) + 2f(m) + f(b)) - \\frac{h}{6}(f(a) + f(b))$$\n$$= \\frac{h}{6} (2f(a) + 4f(m) + 2f(b) - f(a) - f(b)) = \\frac{h}{6}(f(a) + 4f(m) + f(b))$$\nWhen an interval is accepted (i.e., $\\text{err} \\lt \\tau$), returning this higher-order Simpson's rule approximation provides a more accurate result for the same number of function evaluations.\n\n**4. Implementation Structure and Termination**\n\nThe algorithm is implemented as a recursive function. A wrapper function initializes the process.\n- **Base Cases for Recursion:**\n    1.  If $a = b$, the integral is $0$.\n    2.  A maximum recursion depth, $D_{\\max}$, is imposed to guarantee termination, even if the tolerance criterion is never met (e.g., for certain pathological functions or insufficient floating-point precision). If this depth is reached, the current best estimate for the subinterval is returned.\n\n- **Recursive Step:**\n    An internal function, `_adaptive_trapezoid(f, a, b, tol, fa, fb, depth)`, will perform the main logic. Passing `fa=f(a)` and `fb=f(b)` as arguments avoids redundant function evaluations at shared endpoints between parent and child intervals. If the error criterion is not met, it makes two recursive calls:\n    `_adaptive_trapezoid(f, a, m, tol/2, fa, fm, depth+1) + _adaptive_trapezoid(f, m, b, tol/2, fm, fb, depth+1)`\n    where $m=(a+b)/2$ and $fm=f(m)$. This structure efficiently and robustly implements the adaptive integration scheme.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the adaptive integration problem for the given test suite.\n    \"\"\"\n\n    MAX_DEPTH = 50\n\n    def _adaptive_trapezoid(f, a, b, tol, fa, fb, depth):\n        \"\"\"\n        Recursive helper function for adaptive trapezoidal integration.\n\n        This function approximates the integral of f(x) from a to b. It estimates\n        the error by comparing a one-panel trapezoid rule with a two-panel rule.\n        If the error is too large, it recursively calls itself on the two halves\n        of the interval.\n\n        Args:\n            f (callable): The function to integrate.\n            a (float): The start of the integration interval.\n            b (float): The end of the integration interval.\n            tol (float): The absolute tolerance for this subinterval.\n            fa (float): The value of f(a), passed to avoid re-computation.\n            fb (float): The value of f(b), passed to avoid re-computation.\n            depth (int): The current recursion depth.\n\n        Returns:\n            float: The approximated integral value for the interval [a, b].\n        \"\"\"\n        # Base case 1: Zero-length interval\n        if a == b:\n            return 0.0\n\n        # Base case 2: Maximum recursion depth reached\n        if depth > MAX_DEPTH:\n            # Reached depth limit, return best available coarse estimate.\n            # A warning could be printed here in a real application.\n            h = b - a\n            return (h / 2.0) * (fa + fb)\n\n        h = b - a\n        m = (a + b) / 2.0\n        fm = f(m)\n\n        # S1: Coarse approximation (1 trapezoid over [a,b])\n        s1 = (h / 2.0) * (fa + fb)\n\n        # S2: Finer approximation (2 trapezoids over [a,m] and [m,b])\n        s2 = (h / 4.0) * (fa + 2.0 * fm + fb)\n\n        # Estimate the error of the more accurate approximation, S2.\n        # This is derived from Richardson extrapolation, where error(S2) ~ (S2-S1)/3\n        error_estimate = abs(s2 - s1) / 3.0\n\n        if error_estimate < tol:\n            # Error is within tolerance. Return the bias-reduced (Simpson's rule) value.\n            # This is S2 + error_estimate, which is more accurate.\n            return s2 + (s2 - s1) / 3.0\n        else:\n            # Error is too large. Split the interval and recurse.\n            # The tolerance is split between the two sub-intervals.\n            left_integral = _adaptive_trapezoid(f, a, m, tol / 2.0, fa, fm, depth + 1)\n            right_integral = _adaptive_trapezoid(f, m, b, tol / 2.0, fm, fb, depth + 1)\n            return left_integral + right_integral\n\n    def adaptive_integrator(f, a, b, tol):\n        \"\"\"\n        Wrapper function to start the adaptive integration process.\n        \"\"\"\n        # Initial call to the recursive helper function.\n        # Pre-calculates f(a) and f(b) for efficiency.\n        return _adaptive_trapezoid(f, a, b, tol, f(a), f(b), 0)\n\n    # Test Suite Definition\n    test_cases = [\n        {'func': lambda x: np.sin(x), 'interval': (0, np.pi), 'tol': 1e-12},\n        {'func': lambda x: np.exp(-x**2), 'interval': (0, 1), 'tol': 1e-12},\n        {'func': lambda x: 1.0 / (1.0 + x**2), 'interval': (-5, 5), 'tol': 1e-10},\n        {'func': lambda x: np.abs(x), 'interval': (-1, 1), 'tol': 1e-10},\n        {'func': lambda x: np.sin(100 * x) / (1.0 + x**2), 'interval': (0, 1), 'tol': 1e-8},\n        {'func': lambda x: 5.0, 'interval': (2, 5), 'tol': 1e-12}, # Use 5.0 to ensure float\n        {'func': lambda x: np.sin(x), 'interval': (1, 1), 'tol': 1e-12},\n    ]\n\n    results = []\n    for case in test_cases:\n        f = case['func']\n        a, b = case['interval']\n        tol = case['tol']\n        \n        # Handle the zero-length interval case explicitly in the wrapper for clarity,\n        # although the recursion also handles it.\n        if a == b:\n            result = 0.0\n        else:\n            result = adaptive_integrator(f, a, b, tol)\n        \n        results.append(result)\n\n    # Format the final output string exactly as required.\n    # The repr() function provides a high-precision string representation of floats.\n    print(f\"[{','.join(map(repr, results))}]\")\n\nsolve()\n\n```", "id": "3284319"}, {"introduction": "While theoretically elegant, adaptive algorithms must be designed to handle imperfect real-world functions. This practice addresses the critical challenge of integrating functions with jump discontinuities, a scenario where a naive algorithm can get trapped in infinite recursion [@problem_id:3203393]. By implementing a stopping criterion tied to the limitations of floating-point arithmetic, you will learn how to build robust numerical software that terminates correctly even for ill-behaved integrands.", "problem": "You are to design and implement an error-controlled adaptive quadrature algorithm that is robust in the presence of a jump discontinuity by introducing a principled stopping criterion tied to floating-point resolution. The context is the numerical approximation of a definite integral of a bounded function over a closed interval. The foundational base for this task consists of: (i) the definition of the Riemann integral as the limit of Riemann sums for a bounded function on a closed interval, (ii) interpolation-based composite quadrature rules derived from polynomial interpolation on equally spaced nodes, and (iii) the properties of floating-point arithmetic in double precision, particularly the concept of machine epsilon.\n\nTask requirements:\n- Implement an adaptive panel-refinement quadrature based on polynomial interpolation at three nodes per panel. Your recursion must be guided by an error indicator computed by comparing a single panel with its two child panels. Do not hard-code a maximum recursion depth; your method must be controlled by a numerical error tolerance and the robust stopping criterion described below.\n- Construct and use a robust stopping criterion that prevents unbounded refinement near a discontinuity by enforcing a minimum panel width tied to double-precision machine epsilon. Let the double-precision machine epsilon be denoted by $\\varepsilon_{\\mathrm{mach}}$, obtained programmatically. Define the minimum panel width as\n$$\nh_{\\min} \\;=\\; \\beta \\cdot \\max\\!\\big(1,\\lvert a \\rvert,\\lvert b \\rvert\\big) \\cdot \\varepsilon_{\\mathrm{mach}},\n$$\nwith $\\beta = 256$, where $[a,b]$ is the current panel. If a panel of width $h$ satisfies $h \\le h_{\\min}$, you must terminate refinement on that panel and accept the child-panel approximation without further subdivision. Additionally, if due to floating-point resolution the midpoint equals either endpoint, refinement must also be terminated on that panel.\n- Integrand: use the Heaviside step function shifted to the discontinuity at $x=\\frac{1}{2}$,\n$$\nf(x) \\;=\\; H(x - 0.5) \\;=\\; \n\\begin{cases}\n0, & x \\lt 0.5,\\\\\n0, & x = 0.5,\\\\\n1, & x \\gt 0.5,\n\\end{cases}\n$$\nwhere the value at the discontinuity is set to $0$ to force nontrivial refinement behavior.\n- Error tolerance: denote the user-specified tolerance by $\\tau$ and enforce it in the adaptive recursion via the child-versus-parent panel comparison. Your algorithm should return both the numerical integral approximation and a boolean indicator stating whether the minimum-width cap was triggered at least once during the recursion for that integral evaluation.\n\nTest suite:\nEvaluate the integral of $f(x)$ over each of the following intervals and tolerances:\n1. $[0,1]$ with $\\tau = 10^{-8}$.\n2. $[0,1]$ with $\\tau = 10^{-20}$.\n3. $[0,0.49]$ with $\\tau = 10^{-8}$.\n4. $[0.51,1]$ with $\\tau = 10^{-8}$.\n\nAnswer specification:\n- For each test case, return two outputs: the approximated integral value (a float) and the boolean indicator (true if the minimum-width cap was triggered, false otherwise).\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order:\n$$\n[\\text{I}_1,\\text{C}_1,\\text{I}_2,\\text{C}_2,\\text{I}_3,\\text{C}_3,\\text{I}_4,\\text{C}_4],\n$$\nwhere $\\text{I}_k$ is the integral approximation (float) and $\\text{C}_k$ is the cap-trigger indicator (boolean) for test case $k$.\nNo physical units are involved, and no angles appear; all quantities are dimensionless.", "solution": "The problem statement is assessed to be valid. It is a well-posed, scientifically grounded problem in the field of numerical analysis. It provides a complete and consistent set of requirements for designing and implementing an adaptive quadrature algorithm with a robust stopping criterion for handling a function with a jump discontinuity. All parameters, the function to be integrated, and the test cases are specified unambiguously.\n\nThe solution is developed by synthesizing principles of numerical integration, error estimation, and floating-point arithmetic. The algorithm is a recursive, adaptive quadrature method based on Simpson's rule, augmented with a stopping criterion that ensures termination in the presence of discontinuities by respecting the limits of double-precision floating-point resolution.\n\nThe core components of the algorithm are as follows:\n\n1.  **Base Quadrature Rule: Simpson's Rule**\n    The problem specifies a quadrature rule based on polynomial interpolation at three nodes per panel. For a panel $[a, b]$, the standard choice of nodes is the endpoints $a$, $b$, and the midpoint $m = (a+b)/2$. The unique quadratic polynomial interpolating the function $f(x)$ at these points, $(a, f(a))$, $(m, f(m))$, and $(b, f(b))$, is integrated exactly to yield Simpson's rule. The approximation of the integral of $f(x)$ over $[a, b]$, denoted $S(a,b)$, is given by:\n    $$\n    S(a, b) = \\frac{b-a}{6} \\left( f(a) + 4f\\left(\\frac{a+b}{2}\\right) + f(b) \\right)\n    $$\n    Simpson's rule is exact for polynomials of degree up to $3$.\n\n2.  **Adaptive Refinement with Error Estimation**\n    The principle of adaptive quadrature is to refine panels only where the function is \"difficult\" to integrate, i.e., where the estimated integration error is large. This is achieved through a recursive process. For a given panel $[a, b]$, we compute a coarse approximation, $I_{\\text{parent}} = S(a, b)$. We then subdivide the panel at its midpoint $m = (a+b)/2$ into two child panels, $[a, m]$ and $[m, b]$. A more refined approximation, $I_{\\text{children}}$, is computed by summing the Simpson's rule results on each child panel:\n    $$\n    I_{\\text{children}} = S(a, m) + S(m, b)\n    $$\n    The error of the coarse approximation, $S(a,b)$, is of order $\\mathcal{O}((b-a)^5)$, while the error of the refined approximation, $I_{\\text{children}}$, is of order $\\mathcal{O}((m-a)^5 + (b-m)^5) = \\mathcal{O}(2 \\cdot (\\frac{b-a}{2})^5) = \\frac{1}{16}\\mathcal{O}((b-a)^5)$. This implies that the error of the refined approximation is approximately $1/16$ of the error of the coarse one. The difference between the two computed values can thus be used to estimate the error of the more accurate approximation, $I_{\\text{children}}$:\n    $$\n    E_{\\text{children}} \\approx \\frac{1}{15} |I_{\\text{children}} - I_{\\text{parent}}|\n    $$\n    The refinement is terminated for the panel $[a,b]$ if this estimated error is within a specified tolerance, $\\tau_{\\text{panel}}$. A common strategy, which we adopt, is to check if $|I_{\\text{children}} - I_{\\text{parent}}| \\lt 15 \\cdot \\tau_{\\text{panel}}$. The global tolerance for the entire integration domain, denoted $\\tau$, is distributed to sub-panels. For bisection, a simple and effective method is to allocate half of the parent's tolerance to each child, so at each level of recursion for a panel, its tolerance is halved for its children.\n\n3.  **Robust Stopping Criterion for Discontinuities**\n    For a function with a jump discontinuity, like the specified Heaviside function $f(x) = H(x - 0.5)$, the error near the discontinuity at $x=0.5$ does not decrease at the expected rate with panel refinement. The error estimate will consistently fail the tolerance check, leading to unbounded recursion as the algorithm attempts to resolve the jump. To guarantee termination, a robust stopping criterion based on the limits of floating-point arithmetic is necessary. The specified criterion has two components, checked at the beginning of each recursive step for a panel $[a, b]$ of width $h=b-a$:\n    \n    a. **Minimum Panel Width ($h_{min}$)**: Refinement is halted if the panel width $h$ becomes smaller than a prescribed minimum, $h_{min}$. This minimum is defined relative to the scale of the interval endpoints and the machine precision, $\\varepsilon_{\\mathrm{mach}}$ (the smallest number such that $1.0 + \\varepsilon_{\\mathrm{mach}} > 1.0$ in floating-point arithmetic).\n    $$\n    h_{\\min} = \\beta \\cdot \\max(1, |a|, |b|) \\cdot \\varepsilon_{\\mathrm{mach}}\n    $$\n    With the given parameter $\\beta=256$, this establishes a floor for panel width, preventing infinite subdivision. If $h \\le h_{min}$, we cease recursion.\n\n    b. **Floating-Point Resolution Limit**: As $a$ and $b$ become very close, their midpoint $m = (a+b)/2$ may be computationally indistinguishable from $a$ or $b$. If $m=a$ or $m=b$, further subdivision is mathematically impossible in the given floating-point system. This condition serves as a fundamental backstop.\n\n    If either of these conditions is met, refinement on the current panel is terminated, and the refined approximation, $I_{\\text{children}}$, is accepted as the result for that panel. A boolean flag is returned to indicate that this special termination was triggered.\n\n4.  **Algorithm Synthesis**\n    The complete algorithm is implemented as a recursive function. A main entry function, `adaptive_quadrature(func, a, b, tol)`, sets up the initial panel and tolerance and calls a recursive helper function.\n    The recursive helper, `_recursive_quad(func, a, b, tol_sub, ...)`, performs the following steps for its given panel:\n    i. Check the robust stopping criterion. If met, return the child-panel approximation $I_{\\text{children}}$ and a `true` flag.\n    ii. Compute the parent $I_{\\text{parent}}$ and child $I_{\\text{children}}$ approximations.\n    iii. Calculate the error estimate $|I_{\\text{children}} - I_{\\text{parent}}|$.\n    iv. If the error is within the panel's tolerance ($< 15 \\cdot \\tau_{sub}$), return $I_{\\text{children}}$ and a `false` flag.\n    v. Otherwise, recursively call the function for the two child-panels, $[a, m]$ and $[m, b]$, each with half the tolerance, $\\tau_{sub}/2$.\n    vi. Sum the integral results from the two child calls. The `cap_triggered` status is the logical OR of the statuses from the child calls, ensuring that if the cap is triggered in any sub-problem, the final result reflects this.\n\nThis design correctly implements all requirements, providing a robust numerical integrator capable of handling the specified test cases, including the challenging scenario of a very small tolerance on an interval containing a discontinuity, which is designed to specifically test the stopping criterion.", "answer": "```python\nimport numpy as np\n\n# A global constant for double-precision machine epsilon is defined for convenience.\n_EPS = np.finfo(float).eps\n\ndef f(x: float) -> float:\n    \"\"\"\n    Implements the integrand f(x) = H(x - 0.5), a Heaviside step function.\n    The value at the discontinuity x=0.5 is explicitly defined as 0.\n    \"\"\"\n    if x < 0.5:\n        return 0.0\n    elif x > 0.5:\n        return 1.0\n    else:  # x == 0.5\n        return 0.0\n\ndef _recursive_quad(func, a, b, tol, fa, fm, fb, I_panel):\n    \"\"\"\n    Recursive helper function for the adaptive quadrature algorithm.\n\n    This function performs one step of the adaptive refinement. It checks stopping criteria,\n    estimates error, and decides whether to accept the current approximation or to\n    recurse on sub-panels.\n\n    Args:\n        func: The function to integrate.\n        a, b: The endpoints of the current panel.\n        tol: The error tolerance for the current panel.\n        fa, fm, fb: Pre-computed function values at a, m=(a+b)/2, and b.\n        I_panel: The Simpson's rule approximation on the parent panel [a, b].\n\n    Returns:\n        A tuple (integral_value, cap_triggered), where integral_value is the\n        numerical approximation of the integral on [a, b], and cap_triggered\n        is a boolean indicating if the robust stopping criterion was met.\n    \"\"\"\n    h = b - a\n    m = (a + b) / 2.0\n\n    # 1. Robust Stopping Criterion\n    # This prevents unbounded recursion near discontinuities.\n    h_min = 256.0 * max(1.0, abs(a), abs(b)) * _EPS\n    cap_triggered_this_level = (h <= h_min) or (m == a) or (m == b)\n\n    # 2. Compute Refined Approximation\n    # Subdivide the panel and apply Simpson's rule to each child.\n    ml = (a + m) / 2.0\n    mr = (m + b) / 2.0\n    fml = func(ml)\n    fmr = func(mr)\n\n    # Simpson's rule over child panels [a, m] and [m, b]\n    # Note: width of each child panel is h/2. The (h/2)/6 factor is (b-a)/12.\n    I_left = (h / 12.0) * (fa + 4.0 * fml + fm)\n    I_right = (h / 12.0) * (fm + 4.0 * fmr + fb)\n    I_children = I_left + I_right\n\n    if cap_triggered_this_level:\n        # If the panel is too small, accept the child approximation and terminate.\n        return I_children, True\n\n    # 3. Error Estimation and Recursion Decision\n    error_est = abs(I_children - I_panel)\n\n    # The error in the refined sum (I_children) is approx. 1/15 of the difference.\n    # We stop if the estimated error is less than the allocated tolerance.\n    # The check is written as `error_est < 15 * tol` for numerical stability.\n    if error_est < 15.0 * tol:\n        # The problem asks to accept the child-panel approximation.\n        # A more advanced version would add a correction term: I_children + error_est / 15.0\n        return I_children, False\n    else:\n        # If error is too large, recurse on the two child panels.\n        # The tolerance is distributed (halved for each child).\n        tol_sub = tol / 2.0\n        integral_left, cap_left = _recursive_quad(func, a, m, tol_sub, fa, fml, fm, I_left)\n        integral_right, cap_right = _recursive_quad(func, m, b, tol_sub, fm, fmr, fb, I_right)\n\n        # Combine results and propagate the cap_triggered flag.\n        return integral_left + integral_right, cap_left or cap_right\n\ndef adaptive_quadrature(func, a, b, tol):\n    \"\"\"\n    Top-level function for error-controlled adaptive quadrature.\n\n    Args:\n        func: The function to integrate.\n        a, b: The overall integration interval [a, b].\n        tol: The absolute error tolerance for the entire interval.\n\n    Returns:\n        A tuple (integral_value, cap_triggered).\n    \"\"\"\n    h = b - a\n    if h == 0.0:\n        return 0.0, False\n    \n    m = (a + b) / 2.0\n    fa, fm, fb = func(a), func(m), func(b)\n\n    # Initial Simpson's rule approximation over the whole interval [a, b]\n    I_panel = h / 6.0 * (fa + 4.0 * fm + fb)\n\n    return _recursive_quad(func, a, b, tol, fa, fm, fb, I_panel)\n\ndef solve():\n    \"\"\"\n    Executes the test suite and prints the final answer in the specified format.\n    \"\"\"\n    test_cases = [\n        (0.0, 1.0, 1e-8),    # Case 1: Standard run with discontinuity\n        (0.0, 1.0, 1e-20),   # Case 2: Tiny tolerance, should trigger h_min cap\n        (0.0, 0.49, 1e-8),   # Case 3: Smooth region (f(x)=0)\n        (0.51, 1.0, 1e-8)    # Case 4: Smooth region (f(x)=1)\n    ]\n\n    results = []\n    for a, b, tol in test_cases:\n        integral, cap_triggered = adaptive_quadrature(f, a, b, tol)\n        results.append(integral)\n        # Format boolean as lowercase 'true'/'false' string as per problem implications\n        results.append(str(cap_triggered).lower())\n\n    # Format the final output as a single comma-separated list in brackets.\n    output_str = \",\".join(map(str, results))\n    print(f\"[{output_str}]\")\n\nsolve()\n```", "id": "3203393"}, {"introduction": "Adaptivity in numerical methods is a flexible concept that extends beyond just refining spatial grids. This exercise introduces a powerful alternative strategy: adaptive-order quadrature, where the polynomial degree of the integration rule itself is varied to match the local complexity of the function [@problem_id:3203569]. You will use high-order Gauss-Legendre rules to efficiently handle highly curved regions, providing insight into a different dimension of adaptive algorithm design.", "problem": "You are to design and implement an adaptive-order quadrature algorithm that computes definite integrals by varying the polynomial degree of the quadrature rule itself over subintervals, using low order where the integrand is approximately linear and higher order where the integrand is highly curved. The method must be grounded in first principles of numerical integration and produce results for a test suite of functions.\n\nFundamental base and definitions to employ: The definite integral of a function $f$ over an interval $[a,b]$ is defined as the limit of Riemann sums, \n$$\n\\int_a^b f(x)\\,dx = \\lim_{n\\to\\infty} \\sum_{k=1}^n f(x_k^\\ast)\\,\\Delta x_k,\n$$\nwhere $[a,b]$ is partitioned into subintervals of lengths $\\Delta x_k$ and sample points $x_k^\\ast$ are taken within each subinterval. Polynomial interpolation of $f$ yields quadrature rules that approximate the integral by exactly integrating polynomials up to some degree. Gaussian Quadrature (GQ), and in particular the Gauss–Legendre Quadrature (GLQ), uses orthogonal polynomials to choose nodes and weights on $[-1,1]$ such that the $n$-point rule exactly integrates polynomials up to degree $2n-1$; applying an affine change of variables maps the rule to an arbitrary interval $[a,b]$.\n\nTarget: Implement an algorithm that, for each subinterval of $[a,b]$, adapts the order $n$ of the Gauss–Legendre Quadrature rule to meet a specified local error tolerance, using a low order in approximately linear regions and a higher order in regions of high curvature. The global tolerance $T$ is apportioned across subintervals so that the sum of local errors does not exceed $T$.\n\nAlgorithmic specification to follow:\n- Partition $[a,b]$ into $m$ uniform subintervals of width $\\Delta = (b-a)/m$.\n- On each subinterval $[x_\\ell, x_r]$, compute two nested quadrature approximations using Gauss–Legendre Quadrature:\n  - A low-order approximation with $n$ points (beginning with a small initial order).\n  - A higher-order approximation with $2n$ points.\n- Use the absolute difference between these two approximations, denoted by $E = \\left| I_{2n} - I_n \\right|$, as a local error estimator. If $E$ is less than or equal to the local tolerance $T_{\\text{local}}$, accept the higher-order approximation $I_{2n}$ for that subinterval; otherwise, double $n$ and repeat, up to a specified maximum order.\n- The local tolerance must be set proportional to the subinterval width so that $\\sum E \\le T$. A simple choice is $T_{\\text{local}} = T \\cdot \\frac{\\Delta}{b-a}$.\n\nConstraints and requirements:\n- Use Gauss–Legendre Quadrature for all subinterval approximations.\n- Use an affine transformation to map nodes from $[-1,1]$ to $[x_\\ell, x_r]$ and scale weights appropriately.\n- Start with a small initial order (for example, $n=2$) and adapt up to a maximum order (for example, $n=64$) as needed to meet the local tolerance.\n- The implementation must assume angles are in radians where trigonometric functions appear.\n- There are no external inputs; all parameters and functions are embedded in the program.\n\nTest suite:\nCompute the following integrals with the given global tolerance $T$:\n1. $f_1(x) = x$ on $[0,1]$ with $T = 10^{-12}$. The exact value is $\\int_0^1 x\\,dx = \\frac{1}{2}$.\n2. $f_2(x) = \\sin(50 x)$ on $[0,1]$ with $T = 10^{-8}$. Angles are in radians. The exact value is $\\int_0^1 \\sin(50x)\\,dx = \\frac{1 - \\cos(50)}{50}$.\n3. $f_3(x) = e^{-x^2}$ on $[-2,2]$ with $T = 10^{-8}$. The exact value is $\\int_{-2}^{2} e^{-x^2}\\,dx = \\sqrt{\\pi}\\,\\mathrm{erf}(2)$, where $\\mathrm{erf}$ denotes the error function.\n4. $f_4(x) = |x|$ on $[-1,1]$ with $T = 10^{-10}$. The exact value is $\\int_{-1}^{1} |x|\\,dx = 1$.\n5. $f_5(x) = \\max(0, x - 0.5)$ on $[0,1]$ with $T = 10^{-12}$. The exact value is $\\int_0^1 \\max(0, x - 0.5)\\,dx = \\frac{1}{8}$.\n\nImplementation details:\n- Use $m$ uniform subintervals for all test cases; choose $m$ sufficiently large to allow accurate adaptation across diverse integrands. Set $m$ to a fixed value in your implementation.\n- Use the Gauss–Legendre nodes and weights on $[-1,1]$ and map them to each subinterval $[x_\\ell,x_r]$ via $x = \\frac{x_r - x_\\ell}{2}\\xi + \\frac{x_r + x_\\ell}{2}$ and weight scaling by $\\frac{x_r - x_\\ell}{2}$.\n- For each subinterval, begin with $n = 2$ and double until the local error estimate is below $T_{\\text{local}}$ or the maximum order is reached.\n- Sum accepted subinterval contributions to produce the integral estimate for each function.\n\nRequired final output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The results must be ordered as $[I_1, I_2, I_3, I_4, I_5]$ corresponding to the five test cases above. Each $I_k$ must be printed as a floating-point number rounded to ten decimal places. For example, a valid output format is $[0.5000000000,0.1234567890,0.9876543210,1.0000000000,0.1250000000]$.", "solution": "The problem requires the design and implementation of an adaptive-order numerical quadrature algorithm. The core of the method is to approximate a definite integral by partitioning the domain and, for each segment, dynamically selecting the order of a Gauss-Legendre Quadrature (GLQ) rule to meet a local error tolerance. This approach is efficient, as it allocates more computational effort (higher-order rules) only to regions where the integrand exhibits complex behavior, such as high curvature, while using less effort (lower-order rules) in regions where the integrand is smooth or nearly linear.\n\nThe definite integral of a function $f(x)$ over an interval $[a,b]$ is given by\n$$\nI = \\int_a^b f(x)\\,dx\n$$\nWe approximate this integral by first partitioning the interval $[a,b]$ into $m$ smaller, uniform subintervals, $[x_{j-1}, x_j]$ for $j=1, 2, \\dots, m$, where $x_j = a + j \\cdot \\Delta$ and the width of each subinterval is $\\Delta = (b-a)/m$. The total integral is the sum of the integrals over these subintervals:\n$$\nI = \\sum_{j=1}^{m} \\int_{x_{j-1}}^{x_j} f(x)\\,dx\n$$\nWe will select a fixed value of $m=100$ for the implementation, which is sufficiently large to resolve the features of the test integrands.\n\nFor each subinterval, we use Gauss-Legendre Quadrature to approximate the integral. An $n$-point GLQ rule approximates the integral of a function $g(\\xi)$ over the canonical interval $[-1,1]$ as a weighted sum of function evaluations at specific points, called nodes:\n$$\n\\int_{-1}^1 g(\\xi)\\,d\\xi \\approx \\sum_{i=1}^n w_i g(\\xi_i)\n$$\nHere, $\\xi_i$ are the $n$ roots of the Legendre polynomial of degree $n$, $P_n(\\xi)$, and $w_i$ are the corresponding weights. This rule is exact for all polynomials of degree up to $2n-1$.\n\nTo apply this rule to an arbitrary subinterval $[x_\\ell, x_r]$, we use an affine transformation that maps $\\xi \\in [-1,1]$ to $x \\in [x_\\ell, x_r]$:\n$$\nx(\\xi) = \\frac{x_r - x_\\ell}{2}\\xi + \\frac{x_r + x_\\ell}{2}\n$$\nThe differential element transforms as $dx = \\frac{x_r - x_\\ell}{2} d\\xi$. The integral over $[x_\\ell, x_r]$ becomes:\n$$\n\\int_{x_\\ell}^{x_r} f(x)\\,dx = \\frac{x_r - x_\\ell}{2} \\int_{-1}^1 f\\left(\\frac{x_r - x_\\ell}{2}\\xi + \\frac{x_r + x_\\ell}{2}\\right) d\\xi\n$$\nApplying the $n$-point GLQ rule yields the approximation:\n$$\nI_n(f; x_\\ell, x_r) = \\frac{x_r - x_\\ell}{2} \\sum_{i=1}^n w_i^{(n)} f\\left(\\frac{x_r - x_\\ell}{2}\\xi_i^{(n)} + \\frac{x_r + x_\\ell}{2}\\right)\n$$\nwhere the superscripts on the weights $w_i^{(n)}$ and nodes $\\xi_i^{(n)}$ explicitly denote their dependence on the order $n$.\n\nThe core of the adaptive-order algorithm lies in the error estimation and order selection process for each subinterval. A global error tolerance $T$ for the entire integral $\\int_a^b f(x)\\,dx$ is provided. This tolerance is apportioned across the $m$ subintervals. We use a simple distribution, assigning a local tolerance of $T_{\\text{local}} = T \\cdot \\frac{\\Delta}{b-a} = T/m$ to each subinterval.\n\nOn a given subinterval $[x_\\ell, x_r]$, the algorithm proceeds as follows:\n$1$. Initialize the base quadrature order, $n$, to a small value, as specified, $n=2$.\n$2$. Compute two approximations: a lower-order one, $I_n$, and a higher-order one, $I_{2n}$. These rules are nested in terms of accuracy, with $I_{2n}$ typically being much more accurate than $I_n$.\n$3$. Estimate the local error of the lower-order approximation as the absolute difference between the two results: $E = |I_{2n} - I_n|$. This heuristic is based on the assumption that $I_{2n}$ is a much better approximation to the true integral value, so the difference primarily reflects the error in $I_n$.\n$4$. Compare the error estimate $E$ with the local tolerance $T_{\\text{local}}$.\n    - If $E \\le T_{\\text{local}}$, the approximation is deemed sufficiently accurate. We accept the more accurate result, $I_{2n}$, as the value for the subinterval integral and terminate the process for this subinterval.\n    - If $E > T_{\\text{local}}$, the error is too large. We double the base order ($n \\to 2n$) and repeat from step $2$. This process continues until the tolerance is met or a specified maximum base order is reached. In our implementation, the sequence of base orders will be $n \\in \\{2, 4, 8, 16, 32, 64\\}$. The maximum base order is $n=64$. If the tolerance is still not met with the pair $(I_{64}, I_{128})$, we accept the best available estimate, $I_{128}$, for that subinterval.\n\nFinally, the total integral $I$ is computed by summing the accepted integral values from all $m$ subintervals:\n$$\nI_{\\text{approx}} = \\sum_{j=1}^{m} I^{(j)}\n$$\nwhere $I^{(j)}$ is the accepted integral value for the $j$-th subinterval $[x_{j-1}, x_j]$. This procedure ensures that computational resources are adaptively focused on the portions of the domain where the integrand is most challenging to integrate numerically. For functions with singularities or sharp features, such as $f_4(x)=|x|$ or $f_5(x)=\\max(0, x-0.5)$, the fixed grid partitioning with $m=100$ conveniently places the non-differentiable points at the boundaries of subintervals, making the integrand smooth within each subinterval and thus well-suited for GLQ.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main execution function to run the test suite and print results.\n    \"\"\"\n\n    # Pre-compute Gauss-Legendre nodes and weights to avoid redundant calculations.\n    # The adaptive algorithm uses orders n and 2n.\n    # Base orders n are {2, 4, 8, 16, 32, 64}.\n    # This requires nodes/weights for {2, 4, 8, 16, 32, 64, 128}.\n    gl_orders = [2, 4, 8, 16, 32, 64, 128]\n    gl_cache = {n: np.polynomial.legendre.leggauss(n) for n in gl_orders}\n    \n    # --- Test Suite Definition ---\n    # Each test case is a dictionary containing the integrand, interval, and tolerance.\n    test_cases = [\n        {\n            \"func\": lambda x: x,\n            \"interval\": (0, 1),\n            \"tolerance\": 1e-12\n        },\n        {\n            \"func\": lambda x: np.sin(50 * x),\n            \"interval\": (0, 1),\n            \"tolerance\": 1e-8\n        },\n        {\n            \"func\": lambda x: np.exp(-x**2),\n            \"interval\": (-2, 2),\n            \"tolerance\": 1e-8\n        },\n        {\n            \"func\": lambda x: np.abs(x),\n            \"interval\": (-1, 1),\n            \"tolerance\": 1e-10\n        },\n        {\n            \"func\": lambda x: np.maximum(0, x - 0.5),\n            \"interval\": (0, 1),\n            \"tolerance\": 1e-12\n        }\n    ]\n\n    results = []\n    # Number of uniform subintervals to partition the main interval.\n    m_subintervals = 100\n\n    for case in test_cases:\n        integral_value = adaptive_order_quadrature(\n            f=case[\"func\"],\n            a=case[\"interval\"][0],\n            b=case[\"interval\"][1],\n            T=case[\"tolerance\"],\n            m=m_subintervals,\n            gl_cache=gl_cache\n        )\n        results.append(f\"{integral_value:.10f}\")\n\n    print(f\"[{','.join(results)}]\")\n\n\ndef _apply_gl_quad(f, a, b, n, gl_cache):\n    \"\"\"\n    Applies n-point Gauss-Legendre quadrature for f over [a, b].\n    \"\"\"\n    nodes, weights = gl_cache[n]\n    \n    # Affine transformation from [-1, 1] to [a, b]\n    mapped_nodes = 0.5 * (b - a) * nodes + 0.5 * (b + a)\n    mapped_weights_factor = 0.5 * (b - a)\n    \n    return mapped_weights_factor * np.sum(weights * f(mapped_nodes))\n\ndef _integrate_subinterval(f, xl, xr, T_local, gl_cache):\n    \"\"\"\n    Integrates over a single subinterval [xl, xr] using adaptive order.\n    \"\"\"\n    n = 2  # Initial base order\n    max_base_order = 64\n\n    while n <= max_base_order:\n        # Compute low and high order approximations\n        I_n = _apply_gl_quad(f, xl, xr, n, gl_cache)\n        I_2n = _apply_gl_quad(f, xl, xr, 2 * n, gl_cache)\n\n        # Estimate error\n        error_est = np.abs(I_2n - I_n)\n\n        if error_est <= T_local:\n            return I_2n  # Accept the higher-order approximation\n\n        # If error is too large, increase order\n        n *= 2\n\n    # If max order is reached and tolerance is not met, return the best estimate.\n    # The last computation was with n = max_base_order, so we return I_2n.\n    return _apply_gl_quad(f, xl, xr, 2 * max_base_order, gl_cache)\n\ndef adaptive_order_quadrature(f, a, b, T, m, gl_cache):\n    \"\"\"\n    Computes the definite integral of f from a to b using an adaptive-order\n    Gauss-Legendre quadrature on a fixed grid of m subintervals.\n\n    Args:\n        f: The integrand function.\n        a: The lower limit of integration.\n        b: The upper limit of integration.\n        T: The global error tolerance.\n        m: The number of uniform subintervals.\n        gl_cache: A dictionary with pre-computed GL nodes and weights.\n\n    Returns:\n        The approximate value of the integral.\n    \"\"\"\n    if a == b:\n        return 0.0\n\n    total_integral = 0.0\n    \n    # Apportion global tolerance across subintervals\n    T_local = T / m\n    \n    # Create the grid of subintervals\n    subinterval_bounds = np.linspace(a, b, m + 1)\n\n    for i in range(m):\n        xl, xr = subinterval_bounds[i], subinterval_bounds[i+1]\n        \n        sub_integral = _integrate_subinterval(f, xl, xr, T_local, gl_cache)\n        total_integral += sub_integral\n        \n    return total_integral\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3203569"}]}