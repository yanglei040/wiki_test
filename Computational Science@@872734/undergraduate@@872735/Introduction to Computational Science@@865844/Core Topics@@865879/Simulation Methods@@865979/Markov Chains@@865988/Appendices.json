{"hands_on_practices": [{"introduction": "The first step in working with Markov chains is understanding how to predict the system's state after a finite number of steps. This exercise provides a concrete scenario of an AI character in a video game to practice this fundamental skill. By calculating the probability of the character being in the 'Attack' state after three cycles, you will apply the core principle of Markovian evolution: the future state distribution is found by multiplying the current distribution by the transition matrix [@problem_id:1639029].", "problem": "An engineer is programming the behavior of a non-player character (NPC) guard for a video game. The NPC's state is updated in discrete time steps, or \"update cycles\". The NPC can be in one of three states: 'Patrol', 'Alert', or 'Attack'. The transitions between these states are probabilistic and depend only on the current state.\n\n- If the NPC is in the 'Patrol' state, it is following a predefined route. In any given update cycle, there is an 80% chance it remains in the 'Patrol' state and a 20% chance it hears a suspicious sound and transitions to the 'Alert' state. It cannot transition directly from 'Patrol' to 'Attack'.\n\n- If the NPC is in the 'Alert' state, it is actively investigating a disturbance. In any given update cycle, there is a 30% chance it finds nothing and returns to the 'Patrol' state, a 40% chance it continues investigating and remains in the 'Alert' state, and a 30% chance it spots the player and transitions to the 'Attack' state.\n\n- If the NPC is in the 'Attack' state, it is engaged in combat with the player. In any given update cycle, there is a 10% chance it loses track of the player and returns to the 'Patrol' state. Otherwise, it remains in the 'Attack' state. It cannot transition from 'Attack' back to 'Alert'.\n\nThe NPC is initialized and begins its first update cycle in the 'Patrol' state. What is the probability that the NPC is in the 'Attack' state at the end of the third update cycle? Round your final answer to four significant figures.", "solution": "Model the NPC's state as a time-homogeneous Markov chain with states ordered as $[P, A, T]$ for Patrol, Alert, Attack. The one-step transition matrix $M$ is\n$$\nM=\\begin{pmatrix}\n0.8 & 0.2 & 0 \\\\\n0.3 & 0.4 & 0.3 \\\\\n0.1 & 0 & 0.9\n\\end{pmatrix}.\n$$\nLet the row vector $p_{n}$ denote the state distribution at the end of the $n$th update cycle. The evolution is given by the Markov property and matrix multiplication:\n$$\np_{n+1}=p_{n}M.\n$$\nThe NPC starts in Patrol, so before the first update cycle $p_{0}=(1,0,0)$. After the first update cycle,\n$$\np_{1}=p_{0}M=(1,0,0)M=(0.8,\\,0.2,\\,0).\n$$\nAfter the second update cycle,\n$$\np_{2}=p_{1}M=(0.8,\\,0.2,\\,0)M,\n$$\nwhose components are\n$$\n\\begin{aligned}\np_{2}(P)&=0.8\\cdot 0.8+0.2\\cdot 0.3+0\\cdot 0.1=0.64+0.06=0.70,\\\\\np_{2}(A)&=0.8\\cdot 0.2+0.2\\cdot 0.4+0\\cdot 0=0.16+0.08=0.24,\\\\\np_{2}(T)&=0.8\\cdot 0+0.2\\cdot 0.3+0\\cdot 0.9=0.06.\n\\end{aligned}\n$$\nAfter the third update cycle,\n$$\np_{3}=p_{2}M,\n$$\nand the probability of being in Attack is, by the law of total probability over the current state,\n$$\np_{3}(T)=p_{2}(P)\\cdot 0+p_{2}(A)\\cdot 0.3+p_{2}(T)\\cdot 0.9=0.24\\cdot 0.3+0.06\\cdot 0.9=0.072+0.054=0.126.\n$$\nRounded to four significant figures, this is $0.1260$.", "answer": "$$\\boxed{0.1260}$$", "id": "1639029"}, {"introduction": "While predicting short-term behavior is useful, many applications of Markov chains involve understanding the long-run equilibrium of a system. This problem introduces the concept of the stationary distribution, which describes the probability of finding the system in each state after it has been running for a long time. By modeling user navigation on a website, you will learn to set up and solve the system of linear equations $\\pi = \\pi P$ to find this crucial long-term distribution, a technique essential for analyzing everything from web traffic to chemical reactions [@problem_id:1639030].", "problem": "A simplified model for user navigation on a small commercial website is described by a discrete-time Markov chain. The website consists of three distinct pages: a Home page (State 1), a Blog page (State 2), and a Contact page (State 3). The browsing behavior is modeled by the following transition probabilities:\n\n- A user on the Home page (State 1) will navigate to the Blog page (State 2) with probability $p_1$ or to the Contact page (State 3) with probability $1-p_1$.\n- A user on the Blog page (State 2) will return to the Home page (State 1) with probability $p_2$, navigate to the Contact page (State 3) with probability $p_3$, or remain on the Blog page with the remaining probability.\n- A user on the Contact page (State 3) will always return to the Home page (State 1).\n\nAll parameters $p_1, p_2, p_3$ are constants in the interval $(0, 1)$, and it is given that $p_2 + p_3 < 1$. This set of conditions ensures the Markov chain is ergodic, meaning it has a unique stationary distribution.\n\nAssuming a user has been browsing the website for a very long time, the process reaches a stationary state. Determine the stationary distribution $\\pi = (\\pi_1, \\pi_2, \\pi_3)$, where $\\pi_i$ is the long-term probability of finding the user on page $i$. Express your answer as a row vector in terms of the parameters $p_1$, $p_2$, and $p_3$.", "solution": "Let the transition matrix, with states ordered as Home (1), Blog (2), Contact (3), be\n$$\nP=\\begin{pmatrix}\n0 & p_{1} & 1-p_{1} \\\\\np_{2} & 1-p_{2}-p_{3} & p_{3} \\\\\n1 & 0 & 0\n\\end{pmatrix}.\n$$\nA stationary distribution $\\pi=(\\pi_{1},\\pi_{2},\\pi_{3})$ satisfies $\\pi=\\pi P$ and $\\pi_{1}+\\pi_{2}+\\pi_{3}=1$. Writing the stationarity equations componentwise gives\n$$\n\\pi_{1}=p_{2}\\pi_{2}+\\pi_{3},\\quad\n\\pi_{2}=p_{1}\\pi_{1}+(1-p_{2}-p_{3})\\pi_{2},\\quad\n\\pi_{3}=(1-p_{1})\\pi_{1}+p_{3}\\pi_{2}.\n$$\nFrom the second equation,\n$$\n(p_{2}+p_{3})\\pi_{2}=p_{1}\\pi_{1}\\quad\\Rightarrow\\quad \\pi_{2}=\\frac{p_{1}}{p_{2}+p_{3}}\\pi_{1}.\n$$\nSubstituting this into the third equation yields\n$$\n\\pi_{3}=(1-p_{1})\\pi_{1}+p_{3}\\cdot\\frac{p_{1}}{p_{2}+p_{3}}\\pi_{1}\n=\\frac{(1-p_{1})p_{2}+p_{3}}{p_{2}+p_{3}}\\pi_{1}.\n$$\nApply the normalization condition:\n$$\n\\pi_{1}+\\pi_{2}+\\pi_{3}\n=\\pi_{1}\\left[1+\\frac{p_{1}}{p_{2}+p_{3}}+\\frac{(1-p_{1})p_{2}+p_{3}}{p_{2}+p_{3}}\\right]\n=1.\n$$\nSolving for $\\pi_1$ gives:\n$$\n\\pi_{1}=\\frac{p_{2}+p_{3}}{p_{1}+2p_{2}+2p_{3}-p_{1}p_{2}}.\n$$\nTherefore,\n$$\n\\pi_{2}=\\frac{p_{1}}{p_{2}+p_{3}}\\pi_{1}=\\frac{p_{1}}{p_{1}+2p_{2}+2p_{3}-p_{1}p_{2}},\\quad\n\\pi_{3}=\\frac{(1-p_{1})p_{2}+p_{3}}{p_{2}+p_{3}}\\pi_{1}=\\frac{(1-p_{1})p_{2}+p_{3}}{p_{1}+2p_{2}+2p_{3}-p_{1}p_{2}}.\n$$\nThus the stationary distribution, as a row vector, is\n$$\n\\left(\\frac{p_{2}+p_{3}}{p_{1}+2p_{2}+2p_{3}-p_{1}p_{2}},\\ \\frac{p_{1}}{p_{1}+2p_{2}+2p_{3}-p_{1}p_{2}},\\ \\frac{(1-p_{1})p_{2}+p_{3}}{p_{1}+2p_{2}+2p_{3}-p_{1}p_{2}}\\right).\n$$", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{p_{2}+p_{3}}{p_{1}+2p_{2}+2p_{3}-p_{1}p_{2}}&\\frac{p_{1}}{p_{1}+2p_{2}+2p_{3}-p_{1}p_{2}}&\\frac{(1-p_{1})p_{2}+p_{3}}{p_{1}+2p_{2}+2p_{3}-p_{1}p_{2}}\\end{pmatrix}}$$", "id": "1639030"}, {"introduction": "This final practice elevates our analysis from simple description to active intervention, a core task in computational science. Here, you will model student attention states not just to predict outcomes, but to evaluate and choose the best strategy to improve them. This exercise integrates both short-term analysis (expected time in a state) and long-term analysis (stationary distribution) into a computational framework, challenging you to use the Markov chain model as a tool for optimization and decision-making [@problem_id:3158351].", "problem": "Consider a Discrete-Time Markov Chain (DTMC) with a finite state space $\\mathcal{S} = \\{\\text{Attentive}, \\text{Distracted}, \\text{Absent}\\}$ indexed as $0, 1, 2$ respectively. Let $P$ be the state transition matrix where $P_{ij}$ is the probability of transitioning from state $i$ to state $j$ in one time step. Let $\\pi_0$ be an initial distribution over states written as a row vector, and let $H$ be a positive integer horizon representing the number of time steps to evaluate. We define the expected number of time steps in which the process is in the Attentive state over the horizon $H$ as the sum of the probabilities of being in the Attentive state at each time step.\n\nAn intervention is modeled as a modification to $P$ by shifting probability mass within a specified row from a source column to a target column by a nonnegative amount $\\delta$, while preserving row stochasticity. This is represented as a quadruple $(r, s, t, \\delta)$ meaning: in row $r$, decrease $P_{r s}$ by $\\delta$ (capped at the available $P_{r s}$ if necessary) and increase $P_{r t}$ by the same $\\delta$. Multiple such shifts may be applied sequentially for a given intervention. All indices are $0$-based. After modification, any small numerical deviations should be corrected so each row sums to $1$.\n\nYour tasks for each test case are:\n1. Compute the expected number of Attentive time steps over the horizon $H$ using the baseline matrix $P$ and initial distribution $\\pi_0$.\n2. For a given set of candidate interventions, select the single intervention that maximizes the expected number in item $1$. Break ties by selecting the intervention with the larger long-run Attentive fraction (defined in item $3$), and if still tied, select the smallest index intervention.\n3. Compute the long-run Attentive fraction for both the baseline $P$ and the selected intervention. The long-run fraction is the Attentive component of a stationary distribution $\\pi_\\infty$ satisfying $\\pi_\\infty P = \\pi_\\infty$ and $\\sum_i (\\pi_\\infty)_i = 1$.\n\nYou must implement this using the foundational definitions of Markov chains and linearity of expectation. No heuristic shortcuts are permitted. All answers are to be returned as real numbers without percentage signs; angles do not appear in this problem. The final output must be a single line containing a list of per-test-case results, where each test-case result is a list in the format $[E_{\\text{base}}, E_{\\text{best}}, \\pi_{\\infty,\\text{att}}^{\\text{base}}, \\pi_{\\infty,\\text{att}}^{\\text{best}}, \\text{best\\_index}]$. Round all floating-point values to $6$ decimal places. The final line must be exactly this list format, with no additional text.\n\nUse the following test suite. For each test case, the states are ordered as $[0=\\text{Attentive}, 1=\\text{Distracted}, 2=\\text{Absent}]$.\n\nTest case $1$ (general classroom dynamics):\n- $P = \\begin{bmatrix} 0.6 & 0.3 & 0.1 \\\\ 0.4 & 0.4 & 0.2 \\\\ 0.2 & 0.3 & 0.5 \\end{bmatrix}$\n- $\\pi_0 = \\begin{bmatrix} 1.0 & 0.0 & 0.0 \\end{bmatrix}$\n- $H = 10$\n- Candidate interventions:\n  - Index $0$: $(1,1,0,0.1)$\n  - Index $1$: $(2,2,0,0.1)$\n  - Index $2$: $(0,2,0,0.05)$ and $(0,1,0,0.05)$\n\nTest case $2$ (near-attentive stability from distracted start):\n- $P = \\begin{bmatrix} 0.95 & 0.05 & 0.0 \\\\ 0.7 & 0.3 & 0.0 \\\\ 0.3 & 0.2 & 0.5 \\end{bmatrix}$\n- $\\pi_0 = \\begin{bmatrix} 0.0 & 1.0 & 0.0 \\end{bmatrix}$\n- $H = 5$\n- Candidate interventions:\n  - Index $0$: $(1,1,0,0.15)$\n  - Index $1$: $(2,2,0,0.2)$\n  - Index $2$: $(0,1,0,0.04)$\n\nTest case $3$ (absent state absorbing in baseline):\n- $P = \\begin{bmatrix} 0.5 & 0.4 & 0.1 \\\\ 0.3 & 0.3 & 0.4 \\\\ 0.0 & 0.0 & 1.0 \\end{bmatrix}$\n- $\\pi_0 = \\begin{bmatrix} 0.5 & 0.5 & 0.0 \\end{bmatrix}$\n- $H = 8$\n- Candidate interventions:\n  - Index $0$: $(1,2,0,0.2)$\n  - Index $1$: $(0,2,0,0.1)$\n  - Index $2$: $(2,2,0,0.05)$\n\nTest case $4$ (short horizon, mixed initial):\n- $P = \\begin{bmatrix} 0.7 & 0.2 & 0.1 \\\\ 0.2 & 0.7 & 0.1 \\\\ 0.1 & 0.2 & 0.7 \\end{bmatrix}$\n- $\\pi_0 = \\begin{bmatrix} \\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3} \\end{bmatrix}$\n- $H = 1$\n- Candidate interventions:\n  - Index $0$: $(0,1,0,0.1)$\n  - Index $1$: $(1,2,0,0.1)$\n  - Index $2$: $(2,2,0,0.2)$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case, and each such element is itself a comma-separated list $[E_{\\text{base}}, E_{\\text{best}}, \\pi_{\\infty,\\text{att}}^{\\text{base}}, \\pi_{\\infty,\\text{att}}^{\\text{best}}, \\text{best\\_index}]$ with floating-point values rounded to $6$ decimal places.", "solution": "The user wants a solution to a problem involving Discrete-Time Markov Chains (DTMCs). I will first validate the problem statement and then, if valid, provide a reasoned solution.\n\n### Step 1: Extract Givens\n- **Model**: A Discrete-Time Markov Chain (DTMC) with a state space $\\mathcal{S} = \\{\\text{Attentive}, \\text{Distracted}, \\text{Absent}\\}$ indexed by $\\{0, 1, 2\\}$.\n- **Parameters**: A state transition matrix $P$, an initial distribution row vector $\\pi_0$, and a positive integer horizon $H$.\n- **Metric of Interest**: The expected number of time steps in the Attentive state over horizon $H$, defined as the sum of the probabilities of being in the Attentive state at each time step.\n- **Intervention**: A modification to $P$ defined by a quadruple $(r, s, t, \\delta)$, where probability mass $\\delta$ is shifted from $P_{rs}$ to $P_{rt}$ in row $r$. The shift is capped by the available probability $P_{rs}$. Rows are re-normalized to sum to $1$ after modification.\n- **Tasks**:\n  1.  Compute the expected number of Attentive steps ($E_{\\text{base}}$) for the baseline $P$.\n  2.  Select the best intervention from a candidate set, which maximizes this expectation. Ties are broken first by the long-run Attentive fraction, then by the smallest intervention index.\n  3.  Compute the long-run Attentive fraction for the baseline ($\\pi_{\\infty,\\text{att}}^{\\text{base}}$) and the best intervention's matrix ($\\pi_{\\infty,\\text{att}}^{\\text{best}}$). This is the Attentive-state component of the stationary distribution $\\pi_\\infty$ satisfying $\\pi_\\infty P = \\pi_\\infty$.\n- **Output**: For each test case, a list $[E_{\\text{base}}, E_{\\text{best}}, \\pi_{\\infty,\\text{att}}^{\\text{base}}, \\pi_{\\infty,\\text{att}}^{\\text{best}}, \\text{best\\_index}]$, with all floating-point values rounded to $6$ decimal places.\n- **Test Cases**: Four test cases are provided with specific values for $P$, $\\pi_0$, $H$, and candidate interventions.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded (Critical)**: The problem is rooted in the standard theory of DTMCs. The concepts of transition matrices, state distributions over time, expected values, and stationary distributions are fundamental to this field. The problem is scientifically sound.\n2.  **Well-Posed**: The problem is well-defined. The objectives are quantitative and clear. The metric for expectation and the criteria for selecting the best intervention, including tie-breaking rules, are unambiguously specified, ensuring a unique solution can be determined.\n3.  **Objective (Critical)**: The problem statement uses precise, formal mathematical language, devoid of any subjectivity or ambiguity.\n4.  **Completeness and Consistency**: All required data for each test case ($P, \\pi_0, H$, and interventions) are provided. The definitions are consistent with standard literature on Markov chains. The provided transition matrices are valid stochastic matrices (rows sum to $1$).\n\n### Step 3: Verdict and Action\nThe problem is **valid**. I will proceed to design and implement a solution.\n\n### Solution Design\n\nThe solution will be structured into three core computational functions, which will be applied to each test case.\n\n**1. Expected Number of Attentive Time Steps**\nLet $\\pi_k$ be the state distribution at time step $k$. It is a row vector where $(\\pi_k)_j = P(X_k = j)$. The evolution of the distribution is given by $\\pi_k = \\pi_{k-1} P = \\pi_0 P^k$.\nThe problem defines the expected number of Attentive steps over a horizon $H$ as the sum of probabilities of being in the Attentive state (state $0$) at each step. Assuming time steps $k \\in \\{0, 1, \\dots, H-1\\}$, this is:\n$$E_{\\text{att}} = \\sum_{k=0}^{H-1} P(X_k = 0) = \\sum_{k=0}^{H-1} (\\pi_k)_0 = \\sum_{k=0}^{H-1} (\\pi_0 P^k)_0$$\nThis can be computed iteratively: initialize $\\pi_{\\text{current}} = \\pi_0$ and a running sum. In a loop for $H$ steps, add $(\\pi_{\\text{current}})_0$ to the sum and update $\\pi_{\\text{current}} \\leftarrow \\pi_{\\text{current}} P$.\n\n**2. Stationary Distribution and Long-Run Fraction**\nThe stationary distribution $\\pi_\\infty$ is a probability distribution that is invariant under the transition matrix $P$, i.e., $\\pi_\\infty P = \\pi_\\infty$. This can be rewritten as a system of homogeneous linear equations:\n$$\\pi_\\infty (P - I) = \\mathbf{0}$$\nwhere $I$ is the identity matrix and $\\mathbf{0}$ is a zero vector. To obtain a unique solution, we add the constraint that the elements of $\\pi_\\infty$ must sum to $1$: $\\sum_i (\\pi_\\infty)_i = 1$.\nThis system can be solved using standard linear algebra. Transposing the equation gives $(P^T - I^T)\\pi_\\infty^T = \\mathbf{0}^T$. We can form a matrix $A = P^T - I$, replace one of its rows (e.g., the last one) with a row of all ones to enforce the summation constraint, and form a corresponding right-hand-side vector $\\mathbf{b}$ (e.g., $[0, 0, \\dots, 1]^T$). The system $A \\pi_\\infty^T = \\mathbf{b}$ can then be solved for $\\pi_\\infty^T$. The long-run Attentive fraction is simply the first component, $(\\pi_\\infty)_0$. This method works for both irreducible chains and reducible chains with transient states leading to absorbing states, as seen in the test cases.\n\n**3. Intervention Application and Selection**\nAn intervention consists of one or more shifts $(r, s, t, \\delta)$. For each shift, the probability mass is moved within row $r$. The amount of mass moved, $\\Delta$, is $\\min(\\delta, P_{rs})$. The matrix is updated as $P'_{rs} = P_{rs} - \\Delta$ and $P'_{rt} = P_{rt} + \\Delta$. After all shifts prescribed for a candidate intervention are applied, the modified rows must be re-normalized to ensure they sum to $1$.\n\nFor each test case, the overall procedure is:\n1.  Calculate $E_{\\text{base}}$ and $\\pi_{\\infty,\\text{att}}^{\\text{base}}$ using the baseline matrix $P$.\n2.  For each candidate intervention (indexed $0, 1, \\dots$):\n    a. Apply the specified shifts to a copy of $P$ to create a new matrix, $P_{\\text{new}}$.\n    b. Calculate $E_{\\text{new}}$ and $\\pi_{\\infty,\\text{att}}^{\\text{new}}$ using $P_{\\text{new}}$.\n    c. Store these results along with the intervention index.\n3.  Compare the results for all candidates based on the specified criteria:\n    a. Primary: Maximize the expected Attentive steps, $E$.\n    b. Tie-breaker 1: Maximize the long-run Attentive fraction, $\\pi_{\\infty,\\text{att}}$.\n    c. Tie-breaker 2: Select the smallest intervention index.\n4.  This is efficiently accomplished by sorting the candidates' results in descending order of $E$, then descending order of $\\pi_{\\infty,\\text{att}}$, and finally ascending order of the index. The top entry after sorting is the best intervention.\n5.  Extract the statistics for the best intervention ($E_{\\text{best}}, \\pi_{\\infty,\\text{att}}^{\\text{best}}, \\text{best\\_index}$).\n6.  Assemble the five required values and format them as specified. This process is repeated for all test cases.", "answer": "$$\\boxed{\\text{[[4.659972,4.869032,0.444444,0.500000,0],[3.805500,3.957750,0.933333,0.941176,0],[2.573100,2.696320,0.000000,0.222222,2],[0.333333,0.366667,0.333333,0.400000,0]]}}$$", "id": "3158351"}]}