{"hands_on_practices": [{"introduction": "The foundation of Monte Carlo integration lies in its remarkable simplicity and power to handle complex, high-dimensional domains. This first practice exercise provides a direct application of this core principle. By estimating the total mass of a substance within a non-standard, three-dimensional region, you will see how averaging function values at a few random points can yield a reasonable approximation of a challenging triple integral, showcasing the method's intuitive \"hit-or-miss\" nature [@problem_id:1376816].", "problem": "A materials scientist is studying a new alloy created in a cubical mold of side length 1 meter. The coordinate system is aligned with the mold, such that it occupies the region defined by $0 \\le x \\le 1$, $0 \\le y \\le 1$, and $0 \\le z \\le 1$. The concentration of a special hardening agent, $C$, is found to be non-zero only in a specific sub-region of the mold defined by the inequalities $0 \\le z \\le y \\le x \\le 1$. Within this sub-region, the concentration at a point $(x,y,z)$ is described by the function $C(x,y,z) = k x y z$, where $k$ is a constant. Outside this region, the concentration is zero. The total mass of the agent in the mold is given by the integral of the concentration function over the entire volume of the 1-meter-cubed mold.\n\nTo estimate this total mass, an automated measurement system probes the concentration at a set of $N=5$ sample points, which are assumed to be representative of a uniform random sampling within the unit cube. The coordinates of these five points are:\n$P_1 = (0.8, 0.7, 0.2)$\n$P_2 = (0.9, 0.5, 0.6)$\n$P_3 = (0.6, 0.8, 0.3)$\n$P_4 = (0.5, 0.4, 0.3)$\n$P_5 = (0.7, 0.9, 0.8)$\n\nGiven the concentration constant $k = 4.8 \\text{ kg/m}^6$, calculate the numerical estimate for the total mass of the hardening agent in the mold based on this set of five sample points. Express your answer in kilograms (kg) and round to three significant figures.", "solution": "The total mass is the volume integral of the concentration over the unit cube:\n$$\nM=\\iiint_{[0,1]^{3}} C(x,y,z)\\,dV.\n$$\nWith uniform random sampling over a domain of volume $V=1$, the Monte Carlo estimator with $N$ samples $\\{P_{i}\\}_{i=1}^{N}$ is\n$$\n\\widehat{M}=\\frac{V}{N}\\sum_{i=1}^{N} C(P_{i})=\\frac{1}{N}\\sum_{i=1}^{N} C(P_{i}).\n$$\nHere $C(x,y,z)=k\\,x y z$ if $0 \\le z \\le y \\le x \\le 1$ and $C=0$ otherwise. Evaluate the indicator $0 \\le z \\le y \\le x \\le 1$ for each sample:\n- $P_{1}=(0.8,0.7,0.2)$: $0.2 \\le 0.7 \\le 0.8 \\le 1$ is true, so contributes $k\\cdot 0.8\\cdot 0.7\\cdot 0.2=0.112\\,k$.\n- $P_{2}=(0.9,0.5,0.6)$: $0.6 \\le 0.5$ is false, contributes $0$.\n- $P_{3}=(0.6,0.8,0.3)$: $0.8 \\le 0.6$ is false, contributes $0$.\n- $P_{4}=(0.5,0.4,0.3)$: $0.3 \\le 0.4 \\le 0.5 \\le 1$ is true, contributes $k\\cdot 0.5\\cdot 0.4\\cdot 0.3=0.06\\,k$.\n- $P_{5}=(0.7,0.9,0.8)$: $0.9 \\le 0.7$ is false, contributes $0$.\nTherefore\n$$\n\\sum_{i=1}^{5} C(P_{i})=(0.112+0.06)\\,k=0.172\\,k,\n$$\nand\n$$\n\\widehat{M}=\\frac{1}{5}\\cdot 0.172\\,k=0.0344\\,k.\n$$\nSubstituting $k=4.8$ gives\n$$\n\\widehat{M}=0.0344\\times 4.8=0.16512,\n$$\nwhich, rounded to three significant figures, is $0.165$ kilograms.", "answer": "$$\\boxed{0.165}$$", "id": "1376816"}, {"introduction": "While powerful, the convergence of the basic \"crude\" Monte Carlo estimator can be slow. To achieve higher precision with the same computational effort, we use variance reduction techniques. This exercise challenges you to theoretically derive and compare the effectiveness of two fundamental methods: antithetic variates and control variates. By calculating the exact variance reduction each method achieves for the integral of $f(x) = e^x$, you will gain a deep, quantitative understanding of how these techniques work and why they are essential tools in computational science [@problem_id:3253427].", "problem": "Consider the integral $I=\\int_{0}^{1} e^{x}\\,dx$. You will approximate $I$ using Monte Carlo (MC) integration with samples $U\\sim \\mathrm{Uniform}(0,1)$ so that $f(U)=e^{U}$ and $\\mathbb{E}[f(U)]=I$. For a fair comparison under a fixed computational budget of $2n$ evaluations of $f(x)$, consider the following three MC estimators:\n\n- Crude MC: Use $2n$ independent samples $U_{1},\\dots,U_{2n}$ and the sample mean of $f(U_{i})$.\n- Antithetic variates: Use $n$ independent draws $U_{1},\\dots,U_{n}$, form $A_{i}=\\tfrac{1}{2}\\big(f(U_{i})+f(1-U_{i})\\big)$, and average the $A_{i}$.\n- Control variates: Take $g(x)=x$ with known mean $\\int_{0}^{1} x\\,dx=\\tfrac{1}{2}$. Using $2n$ independent samples $U_{1},\\dots,U_{2n}$, form the control variate estimator with an optimally chosen coefficient $\\beta$ (do not assume a formula for $\\beta$; determine it from first principles).\n\nUsing only the definitions of expectation, variance, covariance, and independence (together with basic calculus for the necessary integrals), derive the exact variances of these three estimators as functions of $n$, and then compare antithetic variates versus the optimal control variate under the equal-cost budget of $2n$ evaluations of $f$. Which option below correctly describes their relative variance reductions with respect to crude MC at the same cost?\n\nA. Under equal cost, antithetic variates achieve a variance ratio $R_{\\mathrm{AV}}\\approx 0.0323$ relative to crude MC, while the optimal control variate with $g(x)=x$ achieves $R_{\\mathrm{CV}}\\approx 0.0163$; thus control variates reduce variance more.\n\nB. Under equal cost, antithetic variates achieve $R_{\\mathrm{AV}}\\approx 0.0163$ and the optimal control variate with $g(x)=x$ achieves $R_{\\mathrm{CV}}\\approx 0.0323$; thus antithetic variates reduce variance more.\n\nC. Under equal cost, antithetic and control variates yield exactly the same reduction, with $R_{\\mathrm{AV}}=R_{\\mathrm{CV}}\\approx 0.0240$.\n\nD. Under equal cost, neither method reduces variance relative to crude MC, i.e., $R_{\\mathrm{AV}}\\geq 1$ and $R_{\\mathrm{CV}}\\geq 1$.", "solution": "The problem is to compare the variance reduction of antithetic variates and control variates for the Monte Carlo approximation of the integral $I=\\int_{0}^{1} e^{x}\\,dx$. The comparison must be made under an equal computational budget of $2n$ evaluations of the function $f(x)=e^x$. Let $U$ be a random variable with distribution $\\mathrm{Uniform}(0,1)$.\n\nFirst, we compute the necessary moments and covariances.\nThe true value of the integral is $I = \\mathbb{E}[f(U)] = \\int_{0}^{1} e^x \\,dx = [e^x]_0^1 = e^1 - e^0 = e - 1$.\n\nThe variance of $f(U) = e^U$ requires the second moment:\n$\\mathbb{E}[f(U)^2] = \\mathbb{E}[e^{2U}] = \\int_{0}^{1} e^{2x} \\,dx = \\left[\\frac{1}{2} e^{2x}\\right]_0^1 = \\frac{1}{2}(e^2 - 1)$.\nThe variance of $f(U)$ is denoted by $\\sigma_f^2$:\n$\\sigma_f^2 = \\mathrm{Var}(f(U)) = \\mathbb{E}[f(U)^2] - (\\mathbb{E}[f(U)])^2 = \\frac{1}{2}(e^2 - 1) - (e - 1)^2$\n$\\sigma_f^2 = \\frac{1}{2}e^2 - \\frac{1}{2} - (e^2 - 2e + 1) = -\\frac{1}{2}e^2 + 2e - \\frac{3}{2}$.\n\nFor the control variate estimator, we use $g(x)=x$.\nThe mean of $g(U) = U$ is $\\mu_g = \\mathbb{E}[U] = \\int_{0}^{1} x \\,dx = \\frac{1}{2}$.\nThe variance of $g(U)$, denoted $\\sigma_g^2$, requires $\\mathbb{E}[U^2] = \\int_{0}^{1} x^2 \\,dx = \\frac{1}{3}$.\n$\\sigma_g^2 = \\mathrm{Var}(U) = \\mathbb{E}[U^2] - (\\mathbb{E}[U])^2 = \\frac{1}{3} - \\left(\\frac{1}{2}\\right)^2 = \\frac{1}{3} - \\frac{1}{4} = \\frac{1}{12}$.\n\nThe covariance between $f(U)$ and $g(U)$ is $\\sigma_{fg} = \\mathrm{Cov}(f(U), g(U)) = \\mathbb{E}[f(U)g(U)] - \\mathbb{E}[f(U)]\\mathbb{E}[g(U)]$.\n$\\mathbb{E}[f(U)g(U)] = \\mathbb{E}[Ue^U] = \\int_{0}^{1} xe^x \\,dx$.\nUsing integration by parts ($\\int u dv = uv - \\int v du$ with $u=x, dv=e^x dx$):\n$\\int_{0}^{1} xe^x \\,dx = [xe^x]_0^1 - \\int_{0}^{1} e^x \\,dx = (1 \\cdot e^1 - 0) - [e^x]_0^1 = e - (e-1) = 1$.\nSo, $\\sigma_{fg} = 1 - (e-1)\\left(\\frac{1}{2}\\right) = 1 - \\frac{e}{2} + \\frac{1}{2} = \\frac{3-e}{2}$.\n\nNow we analyze the three estimators.\n\n1.  **Crude Monte Carlo Estimator**\n    The estimator uses $2n$ samples: $\\hat{I}_{\\mathrm{Crude}} = \\frac{1}{2n} \\sum_{i=1}^{2n} e^{U_i}$.\n    Since the samples $U_i$ are independent and identically distributed (i.i.d.), the variance is:\n    $\\mathrm{Var}(\\hat{I}_{\\mathrm{Crude}}) = \\frac{1}{(2n)^2} \\sum_{i=1}^{2n} \\mathrm{Var}(e^{U_i}) = \\frac{2n}{(2n)^2} \\sigma_f^2 = \\frac{1}{2n} \\sigma_f^2$.\n    $\\mathrm{Var}(\\hat{I}_{\\mathrm{Crude}}) = \\frac{1}{2n} \\left(-\\frac{1}{2}e^2 + 2e - \\frac{3}{2}\\right)$. This is our baseline variance.\n\n2.  **Antithetic Variates (AV) Estimator**\n    This estimator uses $n$ pairs of samples $(U_i, 1-U_i)$. The total cost is $2n$ evaluations of $f(x)$.\n    $\\hat{I}_{\\mathrm{AV}} = \\frac{1}{n} \\sum_{i=1}^{n} A_i$, where $A_i = \\frac{1}{2}(e^{U_i} + e^{1-U_i})$.\n    Since $U_i$ are i.i.d., the terms $A_i$ are also i.i.d. The variance is:\n    $\\mathrm{Var}(\\hat{I}_{\\mathrm{AV}}) = \\frac{1}{n} \\mathrm{Var}(A_1) = \\frac{1}{n} \\mathrm{Var}\\left(\\frac{1}{2}(e^U + e^{1-U})\\right) = \\frac{1}{4n} \\mathrm{Var}(e^U + e^{1-U})$.\n    $\\mathrm{Var}(e^U + e^{1-U}) = \\mathrm{Var}(e^U) + \\mathrm{Var}(e^{1-U}) + 2\\mathrm{Cov}(e^U, e^{1-U})$.\n    Since $1-U \\sim \\mathrm{Uniform}(0,1)$, $\\mathrm{Var}(e^{1-U}) = \\mathrm{Var}(e^U) = \\sigma_f^2$.\n    The covariance is $\\mathrm{Cov}(e^U, e^{1-U}) = \\mathbb{E}[e^U e^{1-U}] - \\mathbb{E}[e^U]\\mathbb{E}[e^{1-U}]$.\n    $\\mathbb{E}[e^U e^{1-U}] = \\mathbb{E}[e^{U+1-U}] = \\mathbb{E}[e] = e$.\n    $\\mathbb{E}[e^{1-U}] = \\mathbb{E}[e^U] = e-1$.\n    $\\mathrm{Cov}(e^U, e^{1-U}) = e - (e-1)^2 = e - (e^2-2e+1) = -e^2+3e-1$.\n    $\\mathrm{Var}(\\hat{I}_{\\mathrm{AV}}) = \\frac{1}{4n} (2\\sigma_f^2 + 2\\mathrm{Cov}(e^U, e^{1-U})) = \\frac{1}{2n}(\\sigma_f^2 + \\mathrm{Cov}(e^U, e^{1-U}))$.\n    Substituting the expressions for $\\sigma_f^2$ and the covariance:\n    $\\mathrm{Var}(\\hat{I}_{\\mathrm{AV}}) = \\frac{1}{2n} \\left( \\left(-\\frac{1}{2}e^2 + 2e - \\frac{3}{2}\\right) + (-e^2+3e-1) \\right) = \\frac{1}{2n} \\left(-\\frac{3}{2}e^2 + 5e - \\frac{5}{2}\\right)$.\n\n    The variance ratio $R_{\\mathrm{AV}}$ is:\n    $R_{\\mathrm{AV}} = \\frac{\\mathrm{Var}(\\hat{I}_{\\mathrm{AV}})}{\\mathrm{Var}(\\hat{I}_{\\mathrm{Crude}})} = \\frac{\\frac{1}{2n} (-\\frac{3}{2}e^2 + 5e - \\frac{5}{2})}{\\frac{1}{2n} (-\\frac{1}{2}e^2 + 2e - \\frac{3}{2})} = \\frac{-3e^2+10e-5}{-e^2+4e-3}$.\n    Using $e \\approx 2.71828$:\n    $R_{\\mathrm{AV}} \\approx \\frac{-3(7.389056) + 10(2.71828) - 5}{-7.389056 + 4(2.71828) - 3} = \\frac{0.015632}{0.484064} \\approx 0.03229$. Thus, $R_{\\mathrm{AV}} \\approx 0.0323$.\n\n3.  **Control Variates (CV) Estimator**\n    This estimator uses $2n$ samples: $\\hat{I}_{\\mathrm{CV}}(\\beta) = \\frac{1}{2n}\\sum_{i=1}^{2n} (f(U_i) - \\beta(g(U_i) - \\mu_g))$.\n    The variance of a single term is minimized to find the optimal $\\beta$.\n    $\\mathrm{Var}(f(U) - \\beta(g(U)-\\mu_g)) = \\mathrm{Var}(f(U) - \\beta g(U)) = \\mathrm{Var}(f(U)) - 2\\beta\\mathrm{Cov}(f(U), g(U)) + \\beta^2\\mathrm{Var}(g(U))$.\n    This is $\\sigma_f^2 - 2\\beta\\sigma_{fg} + \\beta^2\\sigma_g^2$. To find the minimum, we differentiate with respect to $\\beta$ and set to $0$:\n    $-2\\sigma_{fg} + 2\\beta\\sigma_g^2 = 0 \\implies \\beta^* = \\frac{\\sigma_{fg}}{\\sigma_g^2}$.\n    The minimum variance is $\\sigma_f^2 - 2\\frac{\\sigma_{fg}}{\\sigma_g^2}\\sigma_{fg} + (\\frac{\\sigma_{fg}}{\\sigma_g^2})^2\\sigma_g^2 = \\sigma_f^2 - \\frac{\\sigma_{fg}^2}{\\sigma_g^2}$.\n    The variance of the estimator is $\\mathrm{Var}(\\hat{I}_{\\mathrm{CV}}) = \\frac{1}{2n} \\left( \\sigma_f^2 - \\frac{\\sigma_{fg}^2}{\\sigma_g^2} \\right)$.\n    Substituting the expressions:\n    $\\frac{\\sigma_{fg}^2}{\\sigma_g^2} = \\frac{((3-e)/2)^2}{1/12} = 12 \\frac{(3-e)^2}{4} = 3(9-6e+e^2) = 27-18e+3e^2$.\n    $\\mathrm{Var}(\\hat{I}_{\\mathrm{CV}}) = \\frac{1}{2n} \\left( (-\\frac{1}{2}e^2 + 2e - \\frac{3}{2}) - (3e^2-18e+27) \\right)$\n    $\\mathrm{Var}(\\hat{I}_{\\mathrm{CV}}) = \\frac{1}{2n} \\left(-\\frac{7}{2}e^2 + 20e - \\frac{57}{2}\\right)$.\n\n    The variance ratio $R_{\\mathrm{CV}}$ is:\n    $R_{\\mathrm{CV}} = \\frac{\\mathrm{Var}(\\hat{I}_{\\mathrm{CV}})}{\\mathrm{Var}(\\hat{I}_{\\mathrm{Crude}})} = \\frac{\\sigma_f^2 - \\sigma_{fg}^2/\\sigma_g^2}{\\sigma_f^2} = \\frac{-\\frac{7}{2}e^2+20e-\\frac{57}{2}}{-\\frac{1}{2}e^2+2e-\\frac{3}{2}} = \\frac{-7e^2+40e-57}{-e^2+4e-3}$.\n    Using $e \\approx 2.71828$:\n    $R_{\\mathrm{CV}} \\approx \\frac{-7(7.389056) + 40(2.71828) - 57}{0.484064} = \\frac{0.007888}{0.484064} \\approx 0.016295$. Thus, $R_{\\mathrm{CV}} \\approx 0.0163$.\n\nComparison and Option Evaluation:\nWe have derived $R_{\\mathrm{AV}} \\approx 0.0323$ and $R_{\\mathrm{CV}} \\approx 0.0163$.\nA smaller variance ratio indicates a larger variance reduction. Since $0.0163 < 0.0323$, the control variate method is more effective for this problem.\n\nA. Under equal cost, antithetic variates achieve a variance ratio $R_{\\mathrm{AV}}\\approx 0.0323$ relative to crude MC, while the optimal control variate with $g(x)=x$ achieves $R_{\\mathrm{CV}}\\approx 0.0163$; thus control variates reduce variance more.\nThis option matches our derived values and conclusion. **Correct**.\n\nB. Under equal cost, antithetic variates achieve $R_{\\mathrm{AV}}\\approx 0.0163$ and the optimal control variate with $g(x)=x$ achieves $R_{\\mathrm{CV}}\\approx 0.0323$; thus antithetic variates reduce variance more.\nThis option swaps the numerical values for the two methods. **Incorrect**.\n\nC. Under equal cost, antithetic and control variates yield exactly the same reduction, with $R_{\\mathrm{AV}}=R_{\\mathrm{CV}}\\approx 0.0240$.\nOur calculations show that the variance ratios are not equal. **Incorrect**.\n\nD. Under equal cost, neither method reduces variance relative to crude MC, i.e., $R_{\\mathrm{AV}}\\geq 1$ and $R_{\\mathrm{CV}}\\geq 1$.\nBoth calculated ratios are significantly less than $1$, indicating substantial variance reduction. For antithetic variates, the variance is reduced because $f(x)=e^x$ is monotonic, leading to negative correlation. For control variates, the variance is reduced because $f(x)=e^x$ and $g(x)=x$ are positively correlated. **Incorrect**.", "answer": "$$\\boxed{A}$$", "id": "3253427"}, {"introduction": "One of the most powerful applications of Monte Carlo methods is in estimating the probabilities of rare events, a task where naive sampling is computationally infeasible. This advanced practice introduces importance sampling, a technique that cleverly biases the sampling process towards the rare event region to obtain an efficient estimate. By tackling a high-dimensional problem from risk analysis, you will learn how to design an effective proposal distribution and use its results to accurately quantify a probability that would otherwise be nearly impossible to find [@problem_id:1376872].", "problem": "In many fields, such as in the modeling of communication systems or risk analysis, it is necessary to estimate the probability of rare events. Consider a system whose state is described by a vector $Z = (Z_1, Z_2, \\dots, Z_{10})$ of 10 independent and identically distributed random variates, each following a standard normal distribution, $Z_i \\sim \\mathcal{N}(0,1)$. A failure event is defined to occur if the squared L2-norm of this state vector, $\\|Z\\|^2 = \\sum_{i=1}^{10} Z_i^2$, exceeds a critical threshold $c=60$. The probability of this event, $P = P(\\|Z\\|^2 > 60)$, is extremely small and difficult to estimate using standard Monte Carlo methods.\n\nTo obtain a more efficient estimate, we will employ importance sampling. We will draw samples $X = (X_1, X_2, \\dots, X_{10})$ from a proposal distribution $q(x)$ instead of the original distribution $p(z)$. The chosen proposal distribution is a multivariate normal distribution where the components are independent and identically distributed, $X_i \\sim \\mathcal{N}(0, \\sigma^2)$, with a variance $\\sigma^2$ to be determined.\n\nYour task is to calculate the importance sampling estimate of the probability $P$. First, determine the appropriate value for the variance $\\sigma^2$ of the proposal distribution. A common and effective heuristic for this choice is to set the expectation of the quantity of interest (in this case, $\\|X\\|^2$) under the proposal distribution equal to the threshold value $c$.\n\nAfter determining $\\sigma^2$, use the results of a pre-computed simulation. In a numerical experiment, a total of $N = 5.0 \\times 10^5$ samples were drawn from the proposal distribution $q(x)$ with the value of $\\sigma^2$ you determined. For this collection of samples, the sum of the importance weights, but only for those samples that satisfy the rare event condition $\\|X\\|^2 > 60$, was calculated and found to be 0.617.\n\nBased on this information, calculate the numerical estimate for the probability $P$. Express your final answer in scientific notation, rounded to three significant figures.", "solution": "The problem asks for an importance sampling-based estimate of the probability $P = P(\\sum_{i=1}^{10} Z_i^2 > 60)$, where $Z_i \\sim \\mathcal{N}(0,1)$ are independent random variates.\n\nThis probability can be written as an expectation:\n$$P = E_p\\left[\\mathbb{I}\\left(\\sum_{i=1}^{10} Z_i^2 > 60\\right)\\right]$$\nwhere $\\mathbb{I}(\\cdot)$ is the indicator function and $E_p[\\cdot]$ denotes the expectation with respect to the original probability distribution $p(z) = \\prod_{i=1}^{10} \\frac{1}{\\sqrt{2\\pi}} \\exp(-z_i^2/2)$.\n\nIn importance sampling, we re-write this expectation with respect to a proposal distribution $q(x)$:\n$$P = E_q\\left[\\mathbb{I}\\left(\\sum_{i=1}^{10} X_i^2 > 60\\right) \\frac{p(X)}{q(X)}\\right]$$\nwhere $X \\sim q(x)$. The term $w(X) = p(X)/q(X)$ is the importance weight. A Monte Carlo estimate of $P$ using $N$ samples $\\{X^{(j)}\\}_{j=1}^N$ drawn from $q(x)$ is given by:\n$$\\hat{P}_N = \\frac{1}{N} \\sum_{j=1}^N \\mathbb{I}\\left(\\|X^{(j)}\\|^2 > 60\\right) w(X^{(j)})$$\n\nThe first step is to determine the variance $\\sigma^2$ of the proposal distribution $q(x) = \\prod_{i=1}^{10} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp(-x_i^2/(2\\sigma^2))$. The problem states the heuristic is to set $E_q[\\|X\\|^2] = 60$.\n\nFor a single component $X_i \\sim \\mathcal{N}(0, \\sigma^2)$, its expectation is $E_q[X_i] = 0$ and its variance is $\\text{Var}_q(X_i) = \\sigma^2$. The expectation of its square is $E_q[X_i^2] = \\text{Var}_q(X_i) + (E_q[X_i])^2 = \\sigma^2 + 0^2 = \\sigma^2$.\n\nBy the linearity of expectation, the expectation of the sum of squares is:\n$$E_q[\\|X\\|^2] = E_q\\left[\\sum_{i=1}^{10} X_i^2\\right] = \\sum_{i=1}^{10} E_q[X_i^2] = 10 \\sigma^2$$\nApplying the heuristic, we set this equal to the threshold $c=60$:\n$$10 \\sigma^2 = 60 \\implies \\sigma^2 = 6$$\n\nThe second step is to find the expression for the importance weight function $w(x) = p(x)/q(x)$.\n$$p(x) = \\prod_{i=1}^{10} \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x_i^2}{2}\\right) = (2\\pi)^{-5} \\exp\\left(-\\frac{1}{2}\\sum_{i=1}^{10} x_i^2\\right) = (2\\pi)^{-5} \\exp\\left(-\\frac{\\|x\\|^2}{2}\\right)$$\n$$q(x) = \\prod_{i=1}^{10} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{x_i^2}{2\\sigma^2}\\right) = (2\\pi\\sigma^2)^{-5} \\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^{10} x_i^2\\right) = (2\\pi\\sigma^2)^{-5} \\exp\\left(-\\frac{\\|x\\|^2}{2\\sigma^2}\\right)$$\n\nThe ratio is:\n$$w(x) = \\frac{p(x)}{q(x)} = \\frac{(2\\pi)^{-5}}{(2\\pi\\sigma^2)^{-5}} \\frac{\\exp(-\\|x\\|^2/2)}{\\exp(-\\|x\\|^2/(2\\sigma^2))} = (\\sigma^2)^5 \\exp\\left(-\\frac{\\|x\\|^2}{2} + \\frac{\\|x\\|^2}{2\\sigma^2}\\right)$$\n$$w(x) = (\\sigma^2)^5 \\exp\\left(-\\frac{\\|x\\|^2}{2} \\left(1 - \\frac{1}{\\sigma^2}\\right)\\right)$$\nSubstituting $\\sigma^2 = 6$:\n$$w(x) = 6^5 \\exp\\left(-\\frac{\\|x\\|^2}{2} \\left(1 - \\frac{1}{6}\\right)\\right) = 7776 \\exp\\left(-\\frac{5}{12}\\|x\\|^2\\right)$$\n\nThe third step is to use the provided simulation data to compute the estimate. The estimator is:\n$$\\hat{P}_N = \\frac{1}{N} \\sum_{j=1}^N \\mathbb{I}(\\|X^{(j)}\\|^2 > 60) w(X^{(j)})$$\nThe sum is over all $N$ samples, but the indicator function $\\mathbb{I}(\\cdot)$ is zero for any sample that does not meet the condition $\\|X^{(j)}\\|^2 > 60$. Therefore, the sum is equivalent to summing the weights of only those samples that fall into the rare event region.\nThe problem states that this sum is given:\n$$\\sum_{j \\text{ s.t. } \\|X^{(j)}\\|^2 > 60} w(X^{(j)}) = 0.617$$\nThis is the value of the numerator in the estimator, summed over all samples:\n$$\\sum_{j=1}^N \\mathbb{I}(\\|X^{(j)}\\|^2 > 60) w(X^{(j)}) = 0.617$$\n\nThe total number of samples is $N = 5.0 \\times 10^5$. Plugging these values into the estimator formula:\n$$\\hat{P}_N = \\frac{0.617}{5.0 \\times 10^5}$$\n\nFinally, we calculate the numerical value:\n$$\\hat{P}_N = \\frac{0.617}{500000} = 0.000001234 = 1.234 \\times 10^{-6}$$\nRounding to three significant figures, we get $1.23 \\times 10^{-6}$.", "answer": "$$\\boxed{1.23 \\times 10^{-6}}$$", "id": "1376872"}]}