{"hands_on_practices": [{"introduction": "The heart of any Gibbs sampler is its set of full conditional distributions. This first exercise provides practice in the essential skill of deriving these conditionals directly from a given joint probability distribution. Mastering this step is the necessary foundation for constructing any Gibbs sampler from scratch. [@problem_id:1338680]", "problem": "Consider two interacting binary random variables, $X$ and $Y$, which can each take a value of either 0 or 1. Their joint probability mass function is specified by an unnormalized relationship of the form $P(X=x, Y=y) \\propto f(x, y)$, where the function $f(x, y)$ is defined for $x, y \\in \\{0, 1\\}$. This type of distribution is fundamental in modeling interacting systems in fields like statistical mechanics and machine learning.\n\nFor a specific system, the interaction is described by the function:\n$$f(x, y) = \\exp(5xy - 2x - y)$$\nSo, the joint probability is given by:\n$$P(X=x, Y=y) \\propto \\exp(5xy - 2x - y)$$\n\nYour task is to determine the conditional probability that the variable $X$ takes the value 1, given the state of the variable $Y$. Derive a symbolic expression for $P(X=1 | Y=y)$ as a function of $y$.", "solution": "We are given a joint distribution specified up to normalization as $P(X=x, Y=y) \\propto f(x,y)$ with $f(x,y) = \\exp(5xy - 2x - y)$. For discrete random variables, the conditional probability is given by\n$$\nP(X=1 \\mid Y=y) = \\frac{P(X=1, Y=y)}{\\sum_{x \\in \\{0,1\\}} P(X=x, Y=y)}.\n$$\nBecause $P$ is proportional to $f$, the common proportionality constant cancels in the ratio, so we can compute using $f$ directly:\n$$\nP(X=1 \\mid Y=y) = \\frac{f(1,y)}{f(0,y) + f(1,y)}.\n$$\nCompute the required terms:\n$$\nf(1,y) = \\exp(5 \\cdot 1 \\cdot y - 2 \\cdot 1 - y) = \\exp(4y - 2),\n$$\n$$\nf(0,y) = \\exp(5 \\cdot 0 \\cdot y - 2 \\cdot 0 - y) = \\exp(-y).\n$$\nHence,\n$$\nP(X=1 \\mid Y=y) = \\frac{\\exp(4y - 2)}{\\exp(-y) + \\exp(4y - 2)}.\n$$\nDivide numerator and denominator by $\\exp(4y - 2)$ to simplify:\n$$\nP(X=1 \\mid Y=y) = \\frac{1}{1 + \\exp\\big((-y) - (4y - 2)\\big)} = \\frac{1}{1 + \\exp(2 - 5y)}.\n$$\nThis gives the desired symbolic expression as a function of $y$.", "answer": "$$\\boxed{\\frac{1}{1 + \\exp(2 - 5y)}}$$", "id": "1338680"}, {"introduction": "Once the conditional distributions are known, the next step is to use them to generate samples from the target distribution. This practice problem simulates one \"half-step\" of a Gibbs sampler, demonstrating how to update one variable based on the current state of the system. This is the fundamental mechanical process that drives the sampler's exploration of the state space. [@problem_id:1338677]", "problem": "A two-dimensional random variable $(X, Y)$ is characterized by its conditional distributions. The conditional distribution of $X$ given $Y=y$ is a continuous uniform distribution on the interval $[0, y]$. The conditional distribution of $Y$ given $X=x$ is a Poisson distribution with a mean of $x$.\n\nA Markov chain Monte Carlo (MCMC) simulation based on the Gibbs sampler is used to generate samples from the joint distribution of $(X, Y)$. The iterative process for generating the sequence of samples $(X^{(t)}, Y^{(t)})$ for $t = 0, 1, 2, \\dots$ is defined as follows:\n1.  Sample $X^{(t+1)}$ from the conditional distribution of $X$ given $Y=Y^{(t)}$.\n2.  Sample $Y^{(t+1)}$ from the conditional distribution of $Y$ given $X=X^{(t+1)}$.\n\nThe process starts from the initial state $(X^{(0)}, Y^{(0)}) = (4, 4)$.\n\nCalculate the probability $P(1  X^{(1)}  2.5)$. Express your answer as an exact fraction.", "solution": "We are given the conditional distributions:\n- For any $y0$, $X \\mid Y=y \\sim \\text{Uniform}(0,y)$, so the conditional density is\n$$\nf_{X\\mid Y}(x \\mid y) = \\begin{cases}\n\\frac{1}{y},  0 \\leq x \\leq y, \\\\\n0,  \\text{otherwise}.\n\\end{cases}\n$$\n- For any $x \\geq 0$, $Y \\mid X=x \\sim \\text{Poisson}(x)$.\n\nIn the Gibbs sampler, step 1 updates $X^{(1)}$ from the conditional distribution of $X$ given $Y=Y^{(0)}$. Since the process starts at $(X^{(0)}, Y^{(0)})=(4,4)$, we have\n$$\nX^{(1)} \\mid Y^{(0)}=4 \\sim \\text{Uniform}(0,4),\n$$\nso its conditional density is $f_{X^{(1)} \\mid Y^{(0)}}(x \\mid 4) = \\frac{1}{4}$ for $0 \\leq x \\leq 4$ and $0$ otherwise.\n\nTherefore,\n$$\nP(1  X^{(1)}  2.5) = \\int_{1}^{2.5} \\frac{1}{4} \\, dx = \\frac{1}{4}\\left(2.5 - 1\\right) = \\frac{1}{4} \\cdot 1.5 = \\frac{3}{8}.\n$$\nBecause $Y^{(0)}=4$ is deterministic in this setup, this conditional probability equals the unconditional $P(1  X^{(1)}  2.5)$.", "answer": "$$\\boxed{\\frac{3}{8}}$$", "id": "1338677"}, {"introduction": "A Gibbs sampler is only effective if it can freely explore the entire region of non-zero probability. This final exercise presents a cautionary scenario where the sampler gets stuck and fails to converge to the correct distribution. Understanding such failure modes is a critical skill for any practitioner, as it highlights the importance of ensuring the underlying Markov chain is irreducible. [@problem_id:1338719]", "problem": "A monitoring system for a specialized chemical reactor tracks two correlated temperature readings, $X$ and $Y$. Due to a strict physical constraint imposed by the reactor's design, the joint probability density function $p(x, y)$ for these two readings is non-zero only on the line segment defined by the equation $x + y = 10$, where $x$ and $y$ are both non-negative (i.e., $x \\ge 0$ and $y \\ge 0$). Along this accessible line segment, the probability distribution is uniform.\n\nA data scientist decides to use a Gibbs sampler to generate synthetic data points $(X, Y)$ that follow this distribution. The sampler is initialized at the point $(X_0, Y_0) = (4.5, 5.5)$. The sampling procedure for generating the next state $(X_{t}, Y_{t})$ from the current state $(X_{t-1}, Y_{t-1})$ for $t = 1, 2, 3, \\dots$ is as follows:\n1. Draw a new value for the first variable, $X_{t}$, from the conditional distribution $p(x | Y=Y_{t-1})$.\n2. Draw a new value for the second variable, $Y_{t}$, from the conditional distribution $p(y | X=X_{t})$.\n\nDetermine the state of the sampler, $(X_{100}, Y_{100})$, after 100 full iterations. Express your answer as a pair of numbers.", "solution": "The joint support is the line segment $S=\\{(x,y): x\\ge 0,\\ y\\ge 0,\\ x+y=10\\}$. Since $p(x,y)$ is uniform along this one-dimensional set, for any $y\\in[0,10]$ the conditional $p(x\\mid Y=y)$ concentrates all its mass on the unique $x$ satisfying $x+y=10$, i.e.,\n$$\np(x\\mid Y=y)=\\delta\\!\\left(x-(10-y)\\right),\n$$\nand similarly, for any $x\\in[0,10]$,\n$$\np(y\\mid X=x)=\\delta\\!\\left(y-(10-x)\\right).\n$$\nTherefore, the Gibbs updates are deterministic:\n$$\nX_{t}=10-Y_{t-1},\\qquad Y_{t}=10-X_{t}.\n$$\nStarting from $(X_{0},Y_{0})=(4.5,5.5)$, the first iteration yields\n$$\nX_{1}=10-5.5=4.5,\\qquad Y_{1}=10-4.5=5.5,\n$$\nso the state is unchanged. By induction, if $(X_{t},Y_{t})=(4.5,5.5)$, then\n$$\nX_{t+1}=10-5.5=4.5,\\qquad Y_{t+1}=10-4.5=5.5,\n$$\nhence the chain remains fixed at $(4.5,5.5)$ for all $t$. In particular,\n$$\n(X_{100},Y_{100})=(4.5,5.5).\n$$", "answer": "$$\\boxed{(4.5, 5.5)}$$", "id": "1338719"}]}