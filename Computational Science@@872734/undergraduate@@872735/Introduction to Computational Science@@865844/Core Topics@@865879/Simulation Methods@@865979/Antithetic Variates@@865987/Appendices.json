{"hands_on_practices": [{"introduction": "To build a solid foundation, our first practice involves a direct, analytical calculation of the variance reduction achieved by antithetic variates. By working with a simple monotonic function, $f(x) = (1+x)^2$, we can make the abstract concept of negative correlation concrete. This exercise [@problem_id:2188199] will demonstrate the ideal conditions under which antithetic sampling is highly effective and will walk you through the core mechanics of quantifying its performance.", "problem": "In computational science, Monte Carlo methods are frequently used to approximate the value of a definite integral, $I = \\int_{0}^{1} f(x) \\,dx$. A key aspect of these methods is the reduction of the estimator's variance to improve accuracy.\n\nConsider the integral of the function $f(x) = (1+x)^2$ over the interval $[0, 1]$. We will compare two Monte Carlo estimators for this integral, both based on a total of $N$ function evaluations, where $N$ is an even integer.\n\n1.  **Standard Monte Carlo Estimator ($\\hat{I}_{\\text{std}}$):**\n    This estimator is formed by drawing $N$ independent and identically distributed random numbers $X_1, X_2, \\ldots, X_N$ from a Uniform(0,1) distribution. The estimator is given by:\n    $$ \\hat{I}_{\\text{std}} = \\frac{1}{N} \\sum_{i=1}^{N} f(X_i) $$\n\n2.  **Antithetic Variates Estimator ($\\hat{I}_{\\text{anti}}$):**\n    This estimator is formed by drawing $M = N/2$ independent random numbers $U_1, U_2, \\ldots, U_M$ from a Uniform(0,1) distribution. For each $U_i$, we form a pair of 'antithetic' samples, $(U_i, 1-U_i)$. The estimator is given by:\n    $$ \\hat{I}_{\\text{anti}} = \\frac{1}{M} \\sum_{i=1}^{M} \\frac{f(U_i) + f(1-U_i)}{2} $$\n\nYour task is to analytically determine the theoretical variance reduction achieved by using antithetic variates for this specific function. Calculate the exact value of the ratio $R = \\frac{\\text{Var}(\\hat{I}_{\\text{anti}})}{\\text{Var}(\\hat{I}_{\\text{std}})}$.", "solution": "Let $X\\sim \\text{Uniform}(0,1)$ and $f(x)=(1+x)^{2}=1+2x+x^{2}$. The standard Monte Carlo estimator with $N$ IID samples has variance\n$$\n\\text{Var}(\\hat{I}_{\\text{std}})=\\frac{\\text{Var}(f(X))}{N}.\n$$\nCompute $\\text{Var}(f(X))$ by evaluating $E[f(X)]$ and $E[f(X)^{2}]$. Using $E[X]=\\frac{1}{2}$, $E[X^{2}]=\\frac{1}{3}$, $E[X^{3}]=\\frac{1}{4}$, $E[X^{4}]=\\frac{1}{5}$,\n$$\nE[f(X)]=E[1+2X+X^{2}]=1+2\\cdot \\frac{1}{2}+\\frac{1}{3}=\\frac{7}{3},\n$$\nand\n$$\nf(X)^{2}=(1+2X+X^{2})^{2}=1+4X+6X^{2}+4X^{3}+X^{4},\n$$\nso\n$$\nE[f(X)^{2}]=1+4\\cdot \\frac{1}{2}+6\\cdot \\frac{1}{3}+4\\cdot \\frac{1}{4}+\\frac{1}{5}=\\frac{31}{5}.\n$$\nThus\n$$\n\\text{Var}(f(X))=E[f(X)^{2}]-\\left(E[f(X)]\\right)^{2}=\\frac{31}{5}-\\frac{49}{9}=\\frac{34}{45}.\n$$\n\nFor the antithetic estimator, define $M=\\frac{N}{2}$ and $Y=\\frac{f(U)+f(1-U)}{2}$ with $U\\sim \\text{Uniform}(0,1)$. Then\n$$\n\\text{Var}(\\hat{I}_{\\text{anti}})=\\frac{\\text{Var}(Y)}{M}.\n$$\nUse\n$$\n\\text{Var}(Y)=\\frac{1}{4}\\text{Var}\\big(f(U)+f(1-U)\\big)=\\frac{1}{4}\\left(2\\,\\text{Var}(f(X))+2\\,\\text{Cov}(f(U),f(1-U))\\right)\n=\\frac{1}{2}\\left(\\text{Var}(f(X))+\\text{Cov}(f(U),f(1-U))\\right).\n$$\nTherefore the variance ratio is\n$$\nR=\\frac{\\text{Var}(\\hat{I}_{\\text{anti}})}{\\text{Var}(\\hat{I}_{\\text{std}})}\n=\\frac{\\frac{\\text{Var}(Y)}{M}}{\\frac{\\text{Var}(f(X))}{N}}\n=\\frac{\\text{Var}(Y)}{\\text{Var}(f(X))}\\cdot \\frac{N}{M}\n=\\frac{\\frac{1}{2}\\left(\\text{Var}(f(X))+\\text{Cov}(f(U),f(1-U))\\right)}{\\text{Var}(f(X))}\\cdot 2\n=1+\\frac{\\text{Cov}(f(U),f(1-U))}{\\text{Var}(f(X))}.\n$$\n\nIt remains to compute $\\text{Cov}(f(U),f(1-U))$. Since $f(1-U)=(2-U)^{2}=4-4U+U^{2}$,\n$$\nf(U)f(1-U)=(1+2U+U^{2})(4-4U+U^{2})=4+4U-3U^{2}-2U^{3}+U^{4}.\n$$\nHence\n$$\nE[f(U)f(1-U)]=4+4\\cdot \\frac{1}{2}-3\\cdot \\frac{1}{3}-2\\cdot \\frac{1}{4}+\\frac{1}{5}=\\frac{47}{10},\n$$\nand with $E[f(U)]=\\frac{7}{3}$,\n$$\n\\text{Cov}(f(U),f(1-U))=E[f(U)f(1-U)]-\\left(E[f(U)]\\right)^{2}=\\frac{47}{10}-\\frac{49}{9}=-\\frac{67}{90}.\n$$\nTherefore,\n$$\nR=1+\\frac{-\\frac{67}{90}}{\\frac{34}{45}}=1-\\frac{67}{90}\\cdot \\frac{45}{34}=1-\\frac{67}{68}=\\frac{1}{68}.\n$$\nThus the antithetic variates estimator achieves a variance that is a factor $\\frac{1}{68}$ of the standard Monte Carlo estimator for this $f$.", "answer": "$$\\boxed{\\frac{1}{68}}$$", "id": "2188199"}, {"introduction": "While the previous exercise showed the power of antithetic variates, it's crucial to understand its limitations. This next practice explores how the method's performance is intrinsically linked to the properties of the integrand, particularly its symmetry. By analyzing the function $f(x) = \\sin(k \\pi x)$, we will discover scenarios where antithetic sampling can range from perfectly efficient to being actively detrimental, doubling the variance of our estimate [@problem_id:3285865]. This serves as an important lesson in carefully assessing a problem before applying a variance reduction technique.", "problem": "Consider estimating the integral $I(k) = \\int_{0}^{1} \\sin(k \\pi x) \\, dx$ for a fixed positive integer $k$ using Monte Carlo (MC) methods with antithetic variates (AV). Let $\\{U_i\\}_{i=1}^{n}$ be independent and identically distributed samples from $\\mathrm{Uniform}(0,1)$, with $n$ even. Define the crude Monte Carlo estimator by $\\widehat{I}_{\\mathrm{MC}} = \\frac{1}{n} \\sum_{i=1}^{n} f(U_i)$ with $f(x) = \\sin(k \\pi x)$. Define the antithetic estimator by pairing samples as $Y_i = \\frac{f(U_i) + f(1 - U_i)}{2}$ for $i = 1, \\dots, n/2$, and setting $\\widehat{I}_{\\mathrm{AV}} = \\frac{1}{n/2} \\sum_{i=1}^{n/2} Y_i$. Using only the fundamental definitions of expectation, variance, and covariance, and basic trigonometric identities, derive an exact closed-form expression (in terms of $k$ only) for the variance ratio\n$$\nR(k) = \\frac{\\mathrm{Var}(\\widehat{I}_{\\mathrm{AV}})}{\\mathrm{Var}(\\widehat{I}_{\\mathrm{MC}})}.\n$$\nExpress your final result as a single simplified analytical expression in $k$. No numerical rounding is required.", "solution": "The objective is to compute the ratio $R(k) = \\frac{\\mathrm{Var}(\\widehat{I}_{\\mathrm{AV}})}{\\mathrm{Var}(\\widehat{I}_{\\mathrm{MC}})}$. We begin by expressing the variances of the two estimators.\n\nLet $U$ be a random variable with distribution $\\mathrm{Uniform}(0,1)$. The random variables $f(U_i)$ are i.i.d. with a common variance $\\mathrm{Var}(f(U))$.\nThe variance of the crude Monte Carlo estimator is:\n$$\n\\mathrm{Var}(\\widehat{I}_{\\mathrm{MC}}) = \\mathrm{Var}\\left(\\frac{1}{n} \\sum_{i=1}^{n} f(U_i)\\right) = \\frac{1}{n^2} \\sum_{i=1}^{n} \\mathrm{Var}(f(U_i)) = \\frac{n}{n^2} \\mathrm{Var}(f(U)) = \\frac{1}{n} \\mathrm{Var}(f(U)).\n$$\nThe antithetic estimator is an average of $n/2$ i.i.d. random variables $Y_i = \\frac{f(U_i) + f(1 - U_i)}{2}$. Let $Y$ be a random variable representing any of the $Y_i$.\nThe variance of the antithetic variates estimator is:\n$$\n\\mathrm{Var}(\\widehat{I}_{\\mathrm{AV}}) = \\mathrm{Var}\\left(\\frac{1}{n/2} \\sum_{i=1}^{n/2} Y_i\\right) = \\frac{1}{(n/2)^2} \\sum_{i=1}^{n/2} \\mathrm{Var}(Y_i) = \\frac{n/2}{(n/2)^2} \\mathrm{Var}(Y) = \\frac{2}{n} \\mathrm{Var}(Y).\n$$\nThe variance of $Y$ is:\n$$\n\\mathrm{Var}(Y) = \\mathrm{Var}\\left(\\frac{f(U) + f(1-U)}{2}\\right) = \\frac{1}{4} \\mathrm{Var}(f(U) + f(1-U)).\n$$\nUsing the formula for the variance of a sum of two random variables, $\\mathrm{Var}(A+B) = \\mathrm{Var}(A) + \\mathrm{Var}(B) + 2\\mathrm{Cov}(A,B)$, we get:\n$$\n\\mathrm{Var}(Y) = \\frac{1}{4} \\left( \\mathrm{Var}(f(U)) + \\mathrm{Var}(f(1-U)) + 2\\mathrm{Cov}(f(U), f(1-U)) \\right).\n$$\nSince $U \\sim \\mathrm{Uniform}(0,1)$, the random variable $V = 1-U$ also follows a $\\mathrm{Uniform}(0,1)$ distribution. Therefore, $\\mathrm{Var}(f(1-U)) = \\mathrm{Var}(f(V)) = \\mathrm{Var}(f(U))$.\nSubstituting this back, we have:\n$$\n\\mathrm{Var}(Y) = \\frac{1}{4} \\left( 2\\mathrm{Var}(f(U)) + 2\\mathrm{Cov}(f(U), f(1-U)) \\right) = \\frac{1}{2} \\left( \\mathrm{Var}(f(U)) + \\mathrm{Cov}(f(U), f(1-U)) \\right).\n$$\nTherefore, the variance of the antithetic estimator is:\n$$\n\\mathrm{Var}(\\widehat{I}_{\\mathrm{AV}}) = \\frac{2}{n} \\mathrm{Var}(Y) = \\frac{1}{n} \\left( \\mathrm{Var}(f(U)) + \\mathrm{Cov}(f(U), f(1-U)) \\right).\n$$\nNow we can write the variance ratio $R(k)$:\n$$\nR(k) = \\frac{\\mathrm{Var}(\\widehat{I}_{\\mathrm{AV}})}{\\mathrm{Var}(\\widehat{I}_{\\mathrm{MC}})} = \\frac{\\frac{1}{n} \\left( \\mathrm{Var}(f(U)) + \\mathrm{Cov}(f(U), f(1-U)) \\right)}{\\frac{1}{n} \\mathrm{Var}(f(U))} = 1 + \\frac{\\mathrm{Cov}(f(U), f(1-U))}{\\mathrm{Var}(f(U))}.\n$$\nTo evaluate this ratio, we must analyze the covariance term. The core of this analysis lies in the symmetry properties of the function $f(x) = \\sin(k \\pi x)$. Let's evaluate $f(1-x)$:\n$$\nf(1-x) = \\sin(k \\pi (1-x)) = \\sin(k \\pi - k \\pi x).\n$$\nUsing the trigonometric identity $\\sin(A-B) = \\sin(A)\\cos(B) - \\cos(A)\\sin(B)$:\n$$\nf(1-x) = \\sin(k \\pi)\\cos(k \\pi x) - \\cos(k \\pi)\\sin(k \\pi x).\n$$\nSince $k$ is an integer, $\\sin(k \\pi) = 0$ and $\\cos(k \\pi) = (-1)^k$. The expression simplifies to:\n$$\nf(1-x) = 0 - (-1)^k \\sin(k \\pi x) = -(-1)^k f(x).\n$$\nThis relationship is pivotal. It implies that the random variables $f(1-U)$ and $f(U)$ are related by $f(1-U) = -(-1)^k f(U)$.\n\nWe can now analyze the covariance $\\mathrm{Cov}(f(U), f(1-U))$:\n$$\n\\mathrm{Cov}(f(U), f(1-U)) = \\mathrm{Cov}(f(U), -(-1)^k f(U)).\n$$\nUsing the property $\\mathrm{Cov}(X, cY) = c\\mathrm{Cov}(X, Y)$, with $c = -(-1)^k$:\n$$\n\\mathrm{Cov}(f(U), f(1-U)) = -(-1)^k \\mathrm{Cov}(f(U), f(U)) = -(-1)^k \\mathrm{Var}(f(U)).\n$$\nWe can now substitute this directly into our expression for the ratio $R(k)$:\n$$\nR(k) = 1 + \\frac{-(-1)^k \\mathrm{Var}(f(U))}{\\mathrm{Var}(f(U))}.\n$$\nProvided that $\\mathrm{Var}(f(U)) \\neq 0$, we can cancel the variance terms. The variance $\\mathrm{Var}(f(U)) = \\mathrm{Var}(\\sin(k\\pi U))$ is zero only if $\\sin(k\\pi U)$ is a constant, which is not the case for a positive integer $k$. Thus, the cancellation is valid.\n$$\nR(k) = 1 - (-1)^k.\n$$\nThis single expression elegantly captures the behavior for both even and odd $k$.\n\nFor completeness, we can detail the two cases:\n1.  If $k$ is an even integer, $k = 2m$ for some integer $m \\ge 1$. Then $(-1)^k = 1$. The ratio is $R(k) = 1 - 1 = 0$. This occurs because for even $k$, $f(1-x) = -f(x)$, meaning the function is odd with respect to the point $x=1/2$. The antithetic pair average is $Y_i = \\frac{f(U_i) + f(1-U_i)}{2} = \\frac{f(U_i) - f(U_i)}{2} = 0$. The estimator $\\widehat{I}_{\\mathrm{AV}}$ is identically zero, hence its variance is zero. This corresponds to perfect variance reduction.\n\n2.  If $k$ is an odd integer, $k = 2m-1$ for some integer $m \\ge 1$. Then $(-1)^k = -1$. The ratio is $R(k) = 1 - (-1) = 2$. This occurs because for odd $k$, $f(1-x) = -(-1)f(x) = f(x)$, meaning the function is even with respect to the point $x=1/2$. The antithetic pair average is $Y_i = \\frac{f(U_i) + f(1-U_i)}{2} = \\frac{f(U_i) + f(U_i)}{2} = f(U_i)$. The estimator becomes $\\widehat{I}_{\\mathrm{AV}} = \\frac{1}{n/2} \\sum_{i=1}^{n/2} f(U_i)$. Its variance is $\\mathrm{Var}(\\widehat{I}_{\\mathrm{AV}}) = \\frac{1}{n/2}\\mathrm{Var}(f(U)) = \\frac{2}{n}\\mathrm{Var}(f(U))$. The ratio becomes $\\frac{2/n \\cdot \\mathrm{Var}(f(U))}{1/n \\cdot \\mathrm{Var}(f(U))} = 2$. In this scenario, the antithetic sampling technique actually doubles the variance compared to the crude Monte Carlo estimator that uses the same number of function evaluations ($n$).\n\nThe derivation did not require the explicit calculation of $\\mathrm{Var}(f(U))$, but instead relied solely on the symmetry property of the integrand. The final expression is a function of $k$ only.", "answer": "$$\n\\boxed{1 - (-1)^k}\n$$", "id": "3285865"}, {"introduction": "A skilled computational scientist must not only know how to use a tool, but also when to use it over other alternatives. This final practice places antithetic variates in a broader context by performing a head-to-head comparison with another powerful technique: control variates. By estimating the integral of $f(x) = \\exp(x)$, we will quantitatively analyze the efficiency of both methods [@problem_id:3253427], developing the practical judgment needed to select the most appropriate variance reduction strategy for a given computational problem.", "problem": "Consider the integral $I=\\int_{0}^{1} e^{x}\\,dx$. You will approximate $I$ using Monte Carlo (MC) integration with samples $U\\sim \\mathrm{Uniform}(0,1)$ so that $f(U)=e^{U}$ and $\\mathbb{E}[f(U)]=I$. For a fair comparison under a fixed computational budget of $2n$ evaluations of $f(x)$, consider the following three MC estimators:\n\n- Crude MC: Use $2n$ independent samples $U_{1},\\dots,U_{2n}$ and the sample mean of $f(U_{i})$.\n- Antithetic variates: Use $n$ independent draws $U_{1},\\dots,U_{n}$, form $A_{i}=\\tfrac{1}{2}\\big(f(U_{i})+f(1-U_{i})\\big)$, and average the $A_{i}$.\n- Control variates: Take $g(x)=x$ with known mean $\\int_{0}^{1} x\\,dx=\\tfrac{1}{2}$. Using $2n$ independent samples $U_{1},\\dots,U_{2n}$, form the control variate estimator with an optimally chosen coefficient $\\beta$ (do not assume a formula for $\\beta$; determine it from first principles).\n\nUsing only the definitions of expectation, variance, covariance, and independence (together with basic calculus for the necessary integrals), derive the exact variances of these three estimators as functions of $n$, and then compare antithetic variates versus the optimal control variate under the equal-cost budget of $2n$ evaluations of $f$. Which option below correctly describes their relative variance reductions with respect to crude MC at the same cost?\n\nA. Under equal cost, antithetic variates achieve a variance ratio $R_{\\mathrm{AV}}\\approx 0.0323$ relative to crude MC, while the optimal control variate with $g(x)=x$ achieves $R_{\\mathrm{CV}}\\approx 0.0163$; thus control variates reduce variance more.\n\nB. Under equal cost, antithetic variates achieve $R_{\\mathrm{AV}}\\approx 0.0163$ and the optimal control variate with $g(x)=x$ achieves $R_{\\mathrm{CV}}\\approx 0.0323$; thus antithetic variates reduce variance more.\n\nC. Under equal cost, antithetic and control variates yield exactly the same reduction, with $R_{\\mathrm{AV}}=R_{\\mathrm{CV}}\\approx 0.0240$.\n\nD. Under equal cost, neither method reduces variance relative to crude MC, i.e., $R_{\\mathrm{AV}}\\geq 1$ and $R_{\\mathrm{CV}}\\geq 1$.", "solution": "The problem is to compare the variance reduction of antithetic variates and control variates for the Monte Carlo approximation of the integral $I=\\int_{0}^{1} e^{x}\\,dx$. The comparison must be made under an equal computational budget of $2n$ evaluations of the function $f(x)=e^x$. Let $U$ be a random variable with distribution $\\mathrm{Uniform}(0,1)$.\n\nFirst, we compute the necessary moments and covariances.\nThe true value of the integral is $I = \\mathbb{E}[f(U)] = \\int_{0}^{1} e^x \\,dx = [e^x]_0^1 = e^1 - e^0 = e - 1$.\n\nThe variance of $f(U) = e^U$ requires the second moment:\n$\\mathbb{E}[f(U)^2] = \\mathbb{E}[e^{2U}] = \\int_{0}^{1} e^{2x} \\,dx = \\left[\\frac{1}{2} e^{2x}\\right]_0^1 = \\frac{1}{2}(e^2 - 1)$.\nThe variance of $f(U)$ is denoted by $\\sigma_f^2$:\n$\\sigma_f^2 = \\mathrm{Var}(f(U)) = \\mathbb{E}[f(U)^2] - (\\mathbb{E}[f(U)])^2 = \\frac{1}{2}(e^2 - 1) - (e - 1)^2$\n$\\sigma_f^2 = \\frac{1}{2}e^2 - \\frac{1}{2} - (e^2 - 2e + 1) = -\\frac{1}{2}e^2 + 2e - \\frac{3}{2}$.\n\nFor the control variate estimator, we use $g(x)=x$.\nThe mean of $g(U) = U$ is $\\mu_g = \\mathbb{E}[U] = \\int_{0}^{1} x \\,dx = \\frac{1}{2}$.\nThe variance of $g(U)$, denoted $\\sigma_g^2$, requires $\\mathbb{E}[U^2] = \\int_{0}^{1} x^2 \\,dx = \\frac{1}{3}$.\n$\\sigma_g^2 = \\mathrm{Var}(U) = \\mathbb{E}[U^2] - (\\mathbb{E}[U])^2 = \\frac{1}{3} - \\left(\\frac{1}{2}\\right)^2 = \\frac{1}{3} - \\frac{1}{4} = \\frac{1}{12}$.\n\nThe covariance between $f(U)$ and $g(U)$ is $\\sigma_{fg} = \\mathrm{Cov}(f(U), g(U)) = \\mathbb{E}[f(U)g(U)] - \\mathbb{E}[f(U)]\\mathbb{E}[g(U)]$.\n$\\mathbb{E}[f(U)g(U)] = \\mathbb{E}[Ue^U] = \\int_{0}^{1} xe^x \\,dx$.\nUsing integration by parts ($\\int u dv = uv - \\int v du$ with $u=x, dv=e^x dx$):\n$\\int_{0}^{1} xe^x \\,dx = [xe^x]_0^1 - \\int_{0}^{1} e^x \\,dx = (1 \\cdot e^1 - 0) - [e^x]_0^1 = e - (e-1) = 1$.\nSo, $\\sigma_{fg} = 1 - (e-1)\\left(\\frac{1}{2}\\right) = 1 - \\frac{e}{2} + \\frac{1}{2} = \\frac{3-e}{2}$.\n\nNow we analyze the three estimators.\n\n1.  **Crude Monte Carlo Estimator**\n    The estimator uses $2n$ samples: $\\hat{I}_{\\mathrm{Crude}} = \\frac{1}{2n} \\sum_{i=1}^{2n} e^{U_i}$.\n    Since the samples $U_i$ are independent and identically distributed (i.i.d.), the variance is:\n    $\\mathrm{Var}(\\hat{I}_{\\mathrm{Crude}}) = \\frac{1}{(2n)^2} \\sum_{i=1}^{2n} \\mathrm{Var}(e^{U_i}) = \\frac{2n}{(2n)^2} \\sigma_f^2 = \\frac{1}{2n} \\sigma_f^2$.\n    $\\mathrm{Var}(\\hat{I}_{\\mathrm{Crude}}) = \\frac{1}{2n} \\left(-\\frac{1}{2}e^2 + 2e - \\frac{3}{2}\\right)$. This is our baseline variance.\n\n2.  **Antithetic Variates (AV) Estimator**\n    This estimator uses $n$ pairs of samples $(U_i, 1-U_i)$. The total cost is $2n$ evaluations of $f(x)$.\n    $\\hat{I}_{\\mathrm{AV}} = \\frac{1}{n} \\sum_{i=1}^{n} A_i$, where $A_i = \\frac{1}{2}(e^{U_i} + e^{1-U_i})$.\n    Since $U_i$ are i.i.d., the terms $A_i$ are also i.i.d. The variance is:\n    $\\mathrm{Var}(\\hat{I}_{\\mathrm{AV}}) = \\frac{1}{n} \\mathrm{Var}(A_1) = \\frac{1}{n} \\mathrm{Var}\\left(\\frac{1}{2}(e^U + e^{1-U})\\right) = \\frac{1}{4n} \\mathrm{Var}(e^U + e^{1-U})$.\n    $\\mathrm{Var}(e^U + e^{1-U}) = \\mathrm{Var}(e^U) + \\mathrm{Var}(e^{1-U}) + 2\\mathrm{Cov}(e^U, e^{1-U})$.\n    Since $1-U \\sim \\mathrm{Uniform}(0,1)$, $\\mathrm{Var}(e^{1-U}) = \\mathrm{Var}(e^U) = \\sigma_f^2$.\n    The covariance is $\\mathrm{Cov}(e^U, e^{1-U}) = \\mathbb{E}[e^U e^{1-U}] - \\mathbb{E}[e^U]\\mathbb{E}[e^{1-U}]$.\n    $\\mathbb{E}[e^U e^{1-U}] = \\mathbb{E}[e^{U+1-U}] = \\mathbb{E}[e] = e$.\n    $\\mathbb{E}[e^{1-U}] = \\mathbb{E}[e^U] = e-1$.\n    $\\mathrm{Cov}(e^U, e^{1-U}) = e - (e-1)^2 = e - (e^2-2e+1) = -e^2+3e-1$.\n    $\\mathrm{Var}(\\hat{I}_{\\mathrm{AV}}) = \\frac{1}{4n} (2\\sigma_f^2 + 2\\mathrm{Cov}(e^U, e^{1-U})) = \\frac{1}{2n}(\\sigma_f^2 + \\mathrm{Cov}(e^U, e^{1-U}))$.\n    Substituting the expressions for $\\sigma_f^2$ and the covariance:\n    $\\mathrm{Var}(\\hat{I}_{\\mathrm{AV}}) = \\frac{1}{2n} \\left( \\left(-\\frac{1}{2}e^2 + 2e - \\frac{3}{2}\\right) + (-e^2+3e-1) \\right) = \\frac{1}{2n} \\left(-\\frac{3}{2}e^2 + 5e - \\frac{5}{2}\\right)$.\n\n    The variance ratio $R_{\\mathrm{AV}}$ is:\n    $R_{\\mathrm{AV}} = \\frac{\\mathrm{Var}(\\hat{I}_{\\mathrm{AV}})}{\\mathrm{Var}(\\hat{I}_{\\mathrm{Crude}})} = \\frac{\\frac{1}{2n} (-\\frac{3}{2}e^2 + 5e - \\frac{5}{2})}{\\frac{1}{2n} (-\\frac{1}{2}e^2 + 2e - \\frac{3}{2})} = \\frac{-3e^2+10e-5}{-e^2+4e-3}$.\n    Using $e \\approx 2.71828$:\n    $R_{\\mathrm{AV}} \\approx \\frac{-3(7.389056) + 10(2.71828) - 5}{-7.389056 + 4(2.71828) - 3} = \\frac{0.015632}{0.484064} \\approx 0.03229$. Thus, $R_{\\mathrm{AV}} \\approx 0.0323$.\n\n3.  **Control Variates (CV) Estimator**\n    This estimator uses $2n$ samples: $\\hat{I}_{\\mathrm{CV}}(\\beta) = \\frac{1}{2n}\\sum_{i=1}^{2n} (f(U_i) - \\beta(g(U_i) - \\mu_g))$.\n    The variance of a single term is minimized to find the optimal $\\beta$.\n    $\\mathrm{Var}(f(U) - \\beta(g(U)-\\mu_g)) = \\mathrm{Var}(f(U) - \\beta g(U)) = \\mathrm{Var}(f(U)) - 2\\beta\\mathrm{Cov}(f(U), g(U)) + \\beta^2\\mathrm{Var}(g(U))$.\n    This is $\\sigma_f^2 - 2\\beta\\sigma_{fg} + \\beta^2\\sigma_g^2$. To find the minimum, we differentiate with respect to $\\beta$ and set to $0$:\n    $-2\\sigma_{fg} + 2\\beta\\sigma_g^2 = 0 \\implies \\beta^* = \\frac{\\sigma_{fg}}{\\sigma_g^2}$.\n    The minimum variance is $\\sigma_f^2 - 2\\frac{\\sigma_{fg}}{\\sigma_g^2}\\sigma_{fg} + (\\frac{\\sigma_{fg}}{\\sigma_g^2})^2\\sigma_g^2 = \\sigma_f^2 - \\frac{\\sigma_{fg}^2}{\\sigma_g^2}$.\n    The variance of the estimator is $\\mathrm{Var}(\\hat{I}_{\\mathrm{CV}}) = \\frac{1}{2n} \\left( \\sigma_f^2 - \\frac{\\sigma_{fg}^2}{\\sigma_g^2} \\right)$.\n    Substituting the expressions:\n    $\\frac{\\sigma_{fg}^2}{\\sigma_g^2} = \\frac{((3-e)/2)^2}{1/12} = 12 \\frac{(3-e)^2}{4} = 3(9-6e+e^2) = 27-18e+3e^2$.\n    $\\mathrm{Var}(\\hat{I}_{\\mathrm{CV}}) = \\frac{1}{2n} \\left( (-\\frac{1}{2}e^2 + 2e - \\frac{3}{2}) - (27-18e+3e^2) \\right)$\n    $\\mathrm{Var}(\\hat{I}_{\\mathrm{CV}}) = \\frac{1}{2n} \\left(-\\frac{7}{2}e^2 + 20e - \\frac{57}{2}\\right)$.\n\n    The variance ratio $R_{\\mathrm{CV}}$ is:\n    $R_{\\mathrm{CV}} = \\frac{\\mathrm{Var}(\\hat{I}_{\\mathrm{CV}})}{\\mathrm{Var}(\\hat{I}_{\\mathrm{Crude}})} = \\frac{\\sigma_f^2 - \\sigma_{fg}^2/\\sigma_g^2}{\\sigma_f^2} = \\frac{-\\frac{7}{2}e^2+20e-\\frac{57}{2}}{-\\frac{1}{2}e^2+2e-\\frac{3}{2}} = \\frac{-7e^2+40e-57}{-e^2+4e-3}$.\n    Using $e \\approx 2.71828$:\n    $R_{\\mathrm{CV}} \\approx \\frac{-7(7.389056) + 40(2.71828) - 57}{0.484064} = \\frac{0.007888}{0.484064} \\approx 0.016295$. Thus, $R_{\\mathrm{CV}} \\approx 0.0163$.\n\nComparison and Option Evaluation:\nWe have derived $R_{\\mathrm{AV}} \\approx 0.0323$ and $R_{\\mathrm{CV}} \\approx 0.0163$.\nA smaller variance ratio indicates a larger variance reduction. Since $0.0163  0.0323$, the control variate method is more effective for this problem.\n\nA. Under equal cost, antithetic variates achieve a variance ratio $R_{\\mathrm{AV}}\\approx 0.0323$ relative to crude MC, while the optimal control variate with $g(x)=x$ achieves $R_{\\mathrm{CV}}\\approx 0.0163$; thus control variates reduce variance more.\nThis option matches our derived values and conclusion. **Correct**.\n\nB. Under equal cost, antithetic variates achieve $R_{\\mathrm{AV}}\\approx 0.0163$ and the optimal control variate with $g(x)=x$ achieves $R_{\\mathrm{CV}}\\approx 0.0323$; thus antithetic variates reduce variance more.\nThis option swaps the numerical values for the two methods. **Incorrect**.\n\nC. Under equal cost, antithetic and control variates yield exactly the same reduction, with $R_{\\mathrm{AV}}=R_{\\mathrm{CV}}\\approx 0.0240$.\nOur calculations show that the variance ratios are not equal. **Incorrect**.\n\nD. Under equal cost, neither method reduces variance relative to crude MC, i.e., $R_{\\mathrm{AV}}\\geq 1$ and $R_{\\mathrm{CV}}\\geq 1$.\nBoth calculated ratios are significantly less than $1$, indicating substantial variance reduction. For antithetic variates, the variance is reduced because $f(x)=e^x$ is monotonic, leading to negative correlation. For control variates, the variance is reduced because $f(x)=e^x$ and $g(x)=x$ are positively correlated. **Incorrect**.", "answer": "$$\\boxed{A}$$", "id": "3253427"}]}