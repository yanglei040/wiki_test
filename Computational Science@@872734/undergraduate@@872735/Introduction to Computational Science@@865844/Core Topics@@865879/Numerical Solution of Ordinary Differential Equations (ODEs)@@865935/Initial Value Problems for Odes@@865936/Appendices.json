{"hands_on_practices": [{"introduction": "Many real-world systems, from chemical reactions to circuit simulations, are described by 'stiff' ordinary differential equations, where different processes occur on vastly different time scales. Explicit numerical methods often struggle with stiffness, requiring impractically small time steps to remain stable. This exercise (Problem 3282680) provides a hands-on demonstration of this critical concept by comparing the behavior of the simple explicit forward Euler method with its stable implicit counterpart, the backward Euler method, on a classic stiff problem.", "problem": "Consider the initial value problem for an ordinary differential equation (ODE)\n$$\n\\frac{dy}{dt} = -100\\,(y - \\cos t), \\quad y(0) = 1,\n$$\nwhere $t$ is in radians. You will compare two single-step time-integration methods to illustrate stiffness: the explicit forward Euler method and the implicit backward Euler method. Your task is to derive the update rules from first principles and then implement a program that quantifies the difference in their behavior for various time steps.\n\nStart from the fundamental definition of the derivative, namely \n$$\n\\frac{dy}{dt} = \\lim_{h \\to 0} \\frac{y(t+h) - y(t)}{h},\n$$\nand approximate it with a finite difference appropriate for each method. Use only this definition and the idea of evaluating the right-hand side $f(t,y)$ either at the known time (for an explicit step) or at the advanced time (for an implicit step) to construct, by algebra, the respective one-step update rules that map $y_n \\approx y(t_n)$ to $y_{n+1} \\approx y(t_{n+1})$, where $t_{n+1} = t_n + h$. Do not assume any prefabricated formulas.\n\nThen, for each method, apply your derived update rule to advance the numerical solution from $t=0$ to $t=T$ using a fixed step size $h$ such that $T/h$ is an integer. Use the following specification:\n\n- The model is the given ODE with $f(t,y) = -100\\,(y - \\cos t)$, $t$ in radians, and initial condition $y(0)=1$.\n- The time horizon is $T=1$.\n- For each $h$ in the test suite below, perform exactly $N = T/h$ steps starting from $y_0 = 1$.\n\nCompute the exact solution $y(t)$ analytically by solving the linear ODE with integrating factor. Use this exact solution to compute the absolute error at the final time $t=T$ for both methods:\n$$\nE_{\\mathrm{FE}}(h) = \\left|y_{\\mathrm{FE}}(T;h) - y(T)\\right|, \\quad E_{\\mathrm{BE}}(h) = \\left|y_{\\mathrm{BE}}(T;h) - y(T)\\right|.\n$$\nFor each test case, report the ratio\n$$\nR(h) = \\frac{E_{\\mathrm{FE}}(h)}{E_{\\mathrm{BE}}(h)}.\n$$\n\nTest suite:\n- Case $1$: $h=0.005$, $T=1$.\n- Case $2$: $h=0.02$, $T=1$ (boundary of forward Euler linear stability for eigenvalue $-100$).\n- Case $3$: $h=0.05$, $T=1$ (forward Euler unstable regime).\n- Case $4$: $h=0.1$, $T=1$ (strongly unstable for forward Euler).\n\nYour program must:\n- Implement both methods using only the update rules you derive from the derivative definition.\n- Implement the exact solution to the ODE in closed form.\n- For each case, return $R(h)$ as a floating-point number.\n\nFinal output format:\n- Produce a single line of output containing the four results as a comma-separated list enclosed in square brackets, with no spaces, in the order of the cases above, for example, $[r_1,r_2,r_3,r_4]$.\n- Each $r_k$ must be rounded to $6$ significant digits, using scientific notation if necessary.\n\nAll numerical outputs are unitless because the problem is posed in nondimensional form, and angles are in radians. The only accepted output types are floating-point numbers as specified, aggregated into the single required list.", "solution": "The problem statement is a valid exercise in numerical methods for ordinary differential equations (ODEs). It is scientifically grounded, well-posed, objective, and contains all necessary information to derive and implement a solution.\n\nThe problem requires a three-part analysis: first, deriving the analytical solution to the given initial value problem (IVP); second, deriving the update rules for the forward and backward Euler methods from first principles; and third, implementing these methods to compute the ratio of their numerical errors for a given set of time steps.\n\n### 1. Analytical Solution of the ODE\n\nThe given ODE is a first-order linear non-homogeneous differential equation:\n$$\n\\frac{dy}{dt} = -100(y - \\cos t)\n$$\nThis can be rewritten in the standard form $\\frac{dy}{dt} + P(t)y = Q(t)$:\n$$\n\\frac{dy}{dt} + 100y = 100\\cos t\n$$\nHere, $P(t) = 100$ and $Q(t) = 100\\cos t$. We solve this using an integrating factor, $\\mu(t)$.\n$$\n\\mu(t) = \\exp\\left(\\int P(t) dt\\right) = \\exp\\left(\\int 100 dt\\right) = e^{100t}\n$$\nMultiplying the standard form of the ODE by $\\mu(t)$ gives:\n$$\ne^{100t}\\frac{dy}{dt} + 100e^{100t}y = 100e^{100t}\\cos t\n$$\nThe left side is the derivative of the product $y(t)\\mu(t)$:\n$$\n\\frac{d}{dt}\\left(y \\cdot e^{100t}\\right) = 100e^{100t}\\cos t\n$$\nIntegrating both sides with respect to $t$:\n$$\ny \\cdot e^{100t} = \\int 100e^{100t}\\cos t \\,dt + C\n$$\nThe integral $\\int e^{at}\\cos(bt) dt$ has a standard form:\n$$\n\\int e^{at}\\cos(bt) dt = \\frac{e^{at}}{a^2+b^2}(a\\cos(bt) + b\\sin(bt))\n$$\nIn our case, $a=100$ and $b=1$. Substituting these values:\n$$\n\\int 100e^{100t}\\cos t \\,dt = 100 \\left[ \\frac{e^{100t}}{100^2+1^2}(100\\cos t + 1\\sin t) \\right] = \\frac{100e^{100t}}{10001}(100\\cos t + \\sin t)\n$$\nSubstituting this back into the equation for $y \\cdot e^{100t}$:\n$$\ny \\cdot e^{100t} = \\frac{100e^{100t}}{10001}(100\\cos t + \\sin t) + C\n$$\nDividing by $e^{100t}$ to solve for $y(t)$:\n$$\ny(t) = \\frac{10000}{10001}\\cos t + \\frac{100}{10001}\\sin t + Ce^{-100t}\n$$\nWe use the initial condition $y(0)=1$ to find the constant $C$:\n$$\n1 = y(0) = \\frac{10000}{10001}\\cos(0) + \\frac{100}{10001}\\sin(0) + Ce^{0}\n$$\n$$\n1 = \\frac{10000}{10001}(1) + \\frac{100}{10001}(0) + C(1) \\implies C = 1 - \\frac{10000}{10001} = \\frac{1}{10001}\n$$\nThus, the exact analytical solution is:\n$$\ny(t) = \\frac{10000}{10001}\\cos t + \\frac{100}{10001}\\sin t + \\frac{1}{10001}e^{-100t}\n$$\n\n### 2. Derivation of Numerical Update Rules\n\nWe start with the finite difference approximation of the derivative, where $h$ is a finite time step size:\n$$\n\\frac{dy}{dt}\\bigg|_{t=t_n} \\approx \\frac{y(t_{n+1}) - y(t_n)}{h} \\approx \\frac{y_{n+1} - y_n}{h}\n$$\nwhere $y_n \\approx y(t_n)$ and $t_{n+1} = t_n + h$. Let the ODE be $\\frac{dy}{dt} = f(t,y)$, where $f(t,y) = -100(y - \\cos t)$.\n\n#### Forward Euler (Explicit) Method\nThe forward Euler method approximates the derivative at the current time step $t_n$ using the function value $f(t_n, y_n)$.\n$$\n\\frac{y_{n+1} - y_n}{h} = f(t_n, y_n)\n$$\nSolving for $y_{n+1}$ gives the general update rule:\n$$\ny_{n+1} = y_n + h f(t_n, y_n)\n$$\nSubstituting our specific function $f(t,y)$:\n$$\ny_{n+1} = y_n + h(-100(y_n - \\cos(t_n)))\n$$\n$$\ny_{n+1} = y_n - 100h y_n + 100h \\cos(t_n)\n$$\n$$\ny_{n+1} = (1 - 100h)y_n + 100h \\cos(t_n)\n$$\nThis is the explicit update rule for the forward Euler method.\n\n#### Backward Euler (Implicit) Method\nThe backward Euler method approximates the derivative using the function value at the next time step, $f(t_{n+1}, y_{n+1})$.\n$$\n\\frac{y_{n+1} - y_n}{h} = f(t_{n+1}, y_{n+1})\n$$\nThis yields an equation that is implicit in the unknown $y_{n+1}$:\n$$\ny_{n+1} = y_n + h f(t_{n+1}, y_{n+1})\n$$\nSubstituting our specific function $f(t,y)$:\n$$\ny_{n+1} = y_n + h(-100(y_{n+1} - \\cos(t_{n+1})))\n$$\nWe must now solve this equation for $y_{n+1}$:\n$$\ny_{n+1} = y_n - 100h y_{n+1} + 100h \\cos(t_{n+1})\n$$\n$$\ny_{n+1} + 100h y_{n+1} = y_n + 100h \\cos(t_{n+1})\n$$\n$$\ny_{n+1}(1 + 100h) = y_n + 100h \\cos(t_{n+1})\n$$\n$$\ny_{n+1} = \\frac{y_n + 100h \\cos(t_{n+1})}{1 + 100h}\n$$\nThis is the explicit form of the update rule for the (originally implicit) backward Euler method for this linear ODE.\n\n### 3. Computational Strategy\n\nFor each given time step $h$, we will perform the following steps:\n1.  Set the initial condition $y_0 = 1$ and final time $T=1$.\n2.  Calculate the number of steps $N = T/h$.\n3.  Simulate the system from $t=0$ to $t=T$ using the derived forward Euler update rule to find the numerical solution at the final time, $y_{\\mathrm{FE}}(T;h)$.\n4.  Simulate the system again from $t=0$ to $t=T$ using the derived backward Euler update rule to find its numerical solution, $y_{\\mathrm{BE}}(T;h)$.\n5.  Calculate the exact solution at the final time, $y(T)$, using the analytical formula.\n6.  Compute the absolute errors for both methods: $E_{\\mathrm{FE}}(h) = |y_{\\mathrm{FE}}(T;h) - y(T)|$ and $E_{\\mathrm{BE}}(h) = |y_{\\mathrm{BE}}(T;h) - y(T)|$.\n7.  Calculate the ratio $R(h) = E_{\\mathrm{FE}}(h) / E_{\\mathrm{BE}}(h)$.\n8.  The calculated ratios for all test cases will be collected and formatted as specified. The large eigenvalue-like term ($-100$) makes the problem stiff, and we expect the forward Euler method to become unstable for $h > 2/100 = 0.02$, leading to a dramatic increase in its error and the ratio $R(h)$. The backward Euler method is A-stable and will not exhibit this instability.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the stiff ODE problem by comparing forward and backward Euler methods.\n    \"\"\"\n    \n    # Test cases defined by the problem statement.\n    # Each case is a specific value for the time step h.\n    test_cases = [\n        0.005,  # Case 1: h=0.005\n        0.02,   # Case 2: h=0.02 (stability boundary)\n        0.05,   # Case 3: h=0.05 (unstable FE)\n        0.1,    # Case 4: h=0.1 (strongly unstable FE)\n    ]\n\n    T = 1.0  # Final time\n    y0 = 1.0  # Initial condition y(0) = 1\n\n    def y_exact(t):\n        \"\"\"\n        Computes the analytical solution of the ODE at time t.\n        y(t) = (1/10001) * [10000*cos(t) + 100*sin(t) + exp(-100t)]\n        \"\"\"\n        return (10000.0 * np.cos(t) + 100.0 * np.sin(t) + np.exp(-100.0 * t)) / 10001.0\n\n    y_true_at_T = y_exact(T)\n    results = []\n\n    for h in test_cases:\n        # Number of steps N must be an integer, as T/h is guaranteed to be one.\n        N = int(T / h)\n\n        # 1. Forward Euler Method\n        # y_{n+1} = (1 - 100h)y_n + 100h*cos(t_n)\n        y_fe = y0\n        t = 0.0\n        for _ in range(N):\n            y_fe = (1.0 - 100.0 * h) * y_fe + 100.0 * h * np.cos(t)\n            t += h\n        y_fe_final = y_fe\n\n        # 2. Backward Euler Method\n        # y_{n+1} = (y_n + 100h*cos(t_{n+1})) / (1 + 100h)\n        y_be = y0\n        t = 0.0\n        for _ in range(N):\n            t += h # t is now t_{n+1}\n            y_be = (y_be + 100.0 * h * np.cos(t)) / (1.0 + 100.0 * h)\n        y_be_final = y_be\n\n        # 3. Error Calculation\n        err_fe = np.abs(y_fe_final - y_true_at_T)\n        err_be = np.abs(y_be_final - y_true_at_T)\n\n        # 4. Ratio Calculation\n        # The problem might imply a case where err_be is zero.\n        # However, for these first-order methods on this problem, it won't be exactly zero.\n        ratio = err_fe / err_be\n        results.append(ratio)\n        \n    # Format the results to 6 significant digits and join them into a single string.\n    # The 'g' format specifier is ideal for handling significant digits and\n    # automatically choosing between fixed-point and scientific notation.\n    formatted_results = [f\"{r:.6g}\" for r in results]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3282680"}, {"introduction": "A numerical solution is only useful if it respects the fundamental physical and mathematical properties of the system it models, such as the preservation of positive quantities like concentration or population. However, standard numerical methods do not automatically guarantee this, and can sometimes produce unphysical negative values. In this practice (Problem 3144090), you will analyze why and when several common explicit methods fail to preserve positivity and explore strategies, from step-size restrictions to variable transformations, to enforce this crucial constraint.", "problem": "Consider the initial value problem (IVP) for an ordinary differential equation (ODE) given by $$y'(t)=-k y(t),\\quad y(0)=y_0,$$ where $k0$ and $y_00$. The exact solution is strictly positive for all $t\\ge 0$. In computational practice, explicit time-stepping methods approximate the continuous dynamics by discrete updates. This problem asks you to analyze positivity preservation (avoiding negative values) of the numerical solution under several explicit methods and to formulate conditions or modifications that guarantee nonnegative approximations.\n\nStarting from the fundamental definition of the derivative, $$y'(t)=\\lim_{h\\to 0}\\frac{y(t+h)-y(t)}{h},$$ and the notion that explicit methods approximate slopes using available information at or within the current step, derive, for the ODE $$y'=-k\\,y,$$ the one-step amplification or update relations for each of the following explicit schemes:\n\n- Forward Euler (explicit first-order method).\n- Explicit midpoint method (explicit second-order Runge–Kutta).\n- A classical explicit third-order Runge–Kutta method (with three stages and weights chosen for third-order accuracy).\n- Classical explicit fourth-order Runge–Kutta method.\n- Two-step Adams–Bashforth method of order $2$ (requiring one-step initialization for $y_1$).\n\nUsing only first principles and core definitions of these methods (no shortcut formulas), derive the discrete updates for $y_{n+1}$ in terms of $y_n$ (and $y_{n-1}$ where applicable) for the test equation $$y'=-k\\,y$$ and express any resulting amplification factor or recurrence coefficients as functions of the dimensionless step parameter $$a=k\\,h,$$ where $h0$ is the time step. Based on those discrete relations, establish conditions under which the numerical solution remains nonnegative for all steps when starting from $y_00$.\n\nFurther, propose either:\n- modified step-size restrictions (an upper bound on $h$ as a function of $k$), or\n- a method variant grounded in a valid transformation of variables,\nthat guarantees $$y_n\\ge 0$$ for all $n$.\n\nAfter completing the derivations and analysis, implement a program that evaluates positivity (nonnegativity) for specified parameter sets by simulating the discrete schemes for a finite number of steps and checking whether all computed iterates satisfy $$y_n\\ge 0.$$\n\nYour program must implement the following numerical methods for the ODE $$y'=-k\\,y$$:\n- Forward Euler.\n- Explicit midpoint (second-order Runge–Kutta).\n- Explicit third-order Runge–Kutta (classical).\n- Explicit fourth-order Runge–Kutta (classical).\n- Adams–Bashforth of order $2$ (with the explicit midpoint method used to compute $y_1$).\n- A positivity-preserving variant based on the logarithmic transformation $$u=\\ln(y)$$, which converts the ODE to $$u'=-k$$ and yields an exact affine update in $u$, followed by reconstruction of $$y=\\exp(u)$$.\n\nSimulation rule: for a given method, simulate $N$ uniform steps of size $h$ from $t=0$ to $t=N\\,h$ starting with $y_00$, and declare the result for that test case to be boolean $$\\text{True}$$ if every simulated iterate satisfies $$y_n\\ge 0$$ (including $n=0$), otherwise $$\\text{False}$$.\n\nTest suite to cover general and edge cases:\n1. Forward Euler with $$k=2.0$$, $$h=0.5$$, $$N=5$$, $$y_0=1.0$$.\n2. Forward Euler with $$k=2.0$$, $$h=0.6$$, $$N=5$$, $$y_0=1.0$$.\n3. Explicit midpoint with $$k=100.0$$, $$h=1.0$$, $$N=5$$, $$y_0=1.0$$.\n4. Explicit third-order Runge–Kutta with $$k=1.0$$, $$h=1.5$$, $$N=5$$, $$y_0=1.0$$.\n5. Explicit third-order Runge–Kutta with $$k=1.0$$, $$h=2.0$$, $$N=5$$, $$y_0=1.0$$.\n6. Explicit fourth-order Runge–Kutta with $$k=5.0$$, $$h=10.0$$, $$N=5$$, $$y_0=1.0$$.\n7. Adams–Bashforth of order $$2$$ with $$k=3.0$$, $$h=\\frac{2}{9}$$, $$N=10$$, $$y_0=1.0$$.\n8. Adams–Bashforth of order $$2$$ with $$k=3.0$$, $$h=0.4$$, $$N=10$$, $$y_0=1.0$$.\n9. Log-transformed variant with $$k=1000.0$$, $$h=10.0$$, $$N=5$$, $$y_0=0.5$$.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the same order as the test suite, for example $$[\\text{True},\\text{False},\\dots]$$.", "solution": "The problem requires an analysis of positivity preservation for several explicit numerical methods applied to the initial value problem (IVP) for the test equation\n$$y'(t) = -k\\,y(t), \\quad y(0)=y_0,$$\nwhere $k0$ and $y_00$. The exact solution is $y(t) = y_0 \\exp(-kt)$, which remains strictly positive for all $t \\ge 0$. We will derive the discrete update relations for each method, establish conditions on the dimensionless step parameter $a=kh$ that guarantee the numerical solution $y_n$ remains nonnegative, and propose a variant that is unconditionally positivity-preserving.\n\nA numerical method applied to this stable test equation yields a discrete recurrence of the form $y_{n+1} = G(a) y_n$ for a one-step method, or a multi-step linear recurrence for a multi-step method. The function $G(a)$ is the amplification factor. For the numerical solution to remain nonnegative, given $y_n \\ge 0$, we require that the update rule produces $y_{n+1} \\ge 0$. For one-step methods, this translates to the condition $G(a) \\ge 0$.\n\nWe derive the update relations and positivity conditions for each specified method from first principles.\n\n**1. Forward Euler Method**\nThe Forward Euler method approximates the solution at step $n+1$ using the tangent at step $n$:\n$$y_{n+1} = y_n + h\\,y'(t_n) = y_n + h\\,f(t_n, y_n).$$\nFor the given ODE, $f(t,y) = -k\\,y$, so we have:\n$$y_{n+1} = y_n + h(-k\\,y_n) = (1 - k\\,h) y_n.$$\nThe amplification factor is $G(a) = 1 - a$, where $a=k\\,h$. For positivity preservation, we require $y_{n+1} \\ge 0$. Since we start with $y_0  0$ and proceed by induction assuming $y_n  0$, the condition becomes:\n$$G(a) = 1 - a \\ge 0 \\implies a \\le 1.$$\nThe step-size restriction is thus $h \\le 1/k$.\n\n**2. Explicit Midpoint Method (Second-Order Runge-Kutta)**\nThis is a two-stage explicit Runge-Kutta method. The update is computed using the slope at the midpoint of the interval, estimated using an initial Euler step.\n$$k_1 = f(t_n, y_n) = -k\\,y_n$$\n$$k_2 = f(t_n + h/2, y_n + \\frac{h}{2} k_1) = -k \\left( y_n + \\frac{h}{2} (-k\\,y_n) \\right) = -k \\left( 1 - \\frac{k\\,h}{2} \\right) y_n$$\n$$y_{n+1} = y_n + h\\,k_2 = y_n + h \\left[ -k \\left( 1 - \\frac{k\\,h}{2} \\right) y_n \\right] = \\left[ 1 - k\\,h + \\frac{(k\\,h)^2}{2} \\right] y_n.$$\nThe amplification factor is $G(a) = 1 - a + a^2/2$. To check for positivity, we inspect this quadratic in $a$. The discriminant is $\\Delta = (-1)^2 - 4(1)(1/2) = 1 - 2 = -1  0$. Since the leading coefficient ($1/2$) is positive, the quadratic $G(a)$ is strictly positive for all real values of $a$.\nTherefore, the explicit midpoint method is unconditionally positivity-preserving for this ODE; no restriction on the step size $h$ is needed to maintain nonnegativity.\n\n**3. Classical Explicit Third-Order Runge-Kutta Method (RK3)**\nWe use the classical three-stage method of Kutta:\n$$k_1 = f(t_n, y_n) = -k\\,y_n$$\n$$k_2 = f(t_n + h/2, y_n + \\frac{h}{2} k_1) = -k \\left( y_n + \\frac{h}{2} (-k\\,y_n) \\right) = -k \\left( 1 - \\frac{a}{2} \\right) y_n$$\n$$k_3 = f(t_n + h, y_n - h\\,k_1 + 2h\\,k_2) = -k \\left( y_n - h(-k\\,y_n) + 2h(-k(1-a/2)y_n) \\right)$$\n$$k_3 = -k \\left( 1 + a - 2a(1-a/2) \\right) y_n = -k \\left( 1 + a - 2a + a^2 \\right) y_n = -k \\left( 1 - a + a^2 \\right) y_n$$\n$$y_{n+1} = y_n + \\frac{h}{6} (k_1 + 4k_2 + k_3)$$\n$$y_{n+1} = y_n + \\frac{h}{6} \\left[ -k\\,y_n - 4k(1-a/2)y_n - k(1-a+a^2)y_n \\right]$$\n$$y_{n+1} = y_n \\left[ 1 - \\frac{a}{6} (1 + 4(1-a/2) + (1-a+a^2)) \\right]$$\n$$y_{n+1} = y_n \\left[ 1 - \\frac{a}{6} (1 + 4 - 2a + 1 - a + a^2) \\right] = y_n \\left[ 1 - \\frac{a}{6} (6 - 3a + a^2) \\right]$$\n$$y_{n+1} = \\left( 1 - a + \\frac{a^2}{2} - \\frac{a^3}{6} \\right) y_n.$$\nThe amplification factor is $G(a) = 1 - a + a^2/2 - a^3/6$. This polynomial is the order-3 Taylor approximation of $\\exp(-a)$. Unlike the quadratic for RK2, this cubic polynomial has a real root. We require $G(a) \\ge 0$. Numerically solving for the smallest positive root of $G(a)=0$ yields $a_{crit} \\approx 1.756$.\nThe condition for positivity is $a \\le a_{crit}$, or $k\\,h \\lesssim 1.756$.\n\n**4. Classical Explicit Fourth-Order Runge-Kutta Method (RK4)**\nThis is the most common four-stage RK method. For the test equation $y'=-k\\,y$, its amplification factor is known to be the order-4 Taylor approximation of $\\exp(-a)$:\n$$G(a) = 1 - a + \\frac{a^2}{2!} - \\frac{a^3}{3!} + \\frac{a^4}{4!} = 1 - a + \\frac{a^2}{2} - \\frac{a^3}{6} + \\frac{a^4}{24}.$$\nThis can be verified by a tedious but straightforward calculation similar to the RK3 case. We analyze the positivity of this quartic polynomial. The derivative is $G'(a) = -1 + a - a^2/2 + a^3/6 = -G_{RK3}(a)$. The extrema of $G_{RK4}(a)$ occur at the roots of $G_{RK3}(a)$. The first positive root of $G_{RK3}(a) = 0$ is $a \\approx 1.756$. The global minimum of $G_{RK4}(a)$ for $a0$ can be shown to be positive. Therefore, $G(a)0$ for all $a \\ge 0$.\nLike the RK2 method, the classical RK4 method is unconditionally positivity-preserving for this ODE.\n\n**5. Two-Step Adams-Bashforth Method (AB2)**\nThis linear multi-step method uses information from steps $n$ and $n-1$:\n$$y_{n+1} = y_n + h \\left( \\frac{3}{2} f_n - \\frac{1}{2} f_{n-1} \\right),$$\nwhere $f_i = f(t_i, y_i) = -k\\,y_i$. Substituting this gives the recurrence relation:\n$$y_{n+1} = y_n + h \\left( \\frac{3}{2}(-k\\,y_n) - \\frac{1}{2}(-k\\,y_{n-1}) \\right)$$\n$$y_{n+1} = \\left( 1 - \\frac{3}{2}k\\,h \\right) y_n + \\left( \\frac{1}{2}k\\,h \\right) y_{n-1}.$$\nIn terms of $a=k\\,h$, this is:\n$$y_{n+1} = \\left( 1 - \\frac{3}{2}a \\right) y_n + \\frac{a}{2} y_{n-1}.$$\nFor the solution to be guaranteed to remain nonnegative, we can impose a sufficient condition that $y_{n+1}$ is a convex combination of previous nonnegative values, $y_n$ and $y_{n-1}$. This requires the coefficients in the recurrence to be non-negative.\nThe coefficient $\\frac{a}{2}$ is always non-negative since $k0$ and $h0$. The other coefficient requires:\n$$1 - \\frac{3}{2}a \\ge 0 \\implies 1 \\ge \\frac{3}{2}a \\implies a \\le \\frac{2}{3}.$$\nThus, a sufficient condition for the AB2 method to preserve positivity is $k\\,h \\le 2/3$. If this condition is violated, the presence of a negative coefficient can lead to negative solutions.\n\n**6. Positivity-Preserving Variant: Logarithmic Transformation**\nA robust method to guarantee positivity is to transform the dependent variable. Let $u(t) = \\ln(y(t))$. The dynamics of $u(t)$ are governed by:\n$$u'(t) = \\frac{d}{dt} \\ln(y(t)) = \\frac{1}{y(t)} y'(t).$$\nSubstituting $y'(t) = -k\\,y(t)$:\n$$u'(t) = \\frac{1}{y(t)} (-k\\,y(t)) = -k.$$\nThe new IVP for $u$ is $u' = -k$ with $u(0)=\\ln(y_0)$. This linear ODE can be solved exactly over a time step $h$:\n$$u(t_{n+1}) = u(t_n) - k\\,h.$$\nThe numerical scheme is simply $u_{n+1} = u_n - a$. Starting from $u_0 = \\ln(y_0)$, the discrete solution for $u$ is exact. The solution for $y$ is reconstructed by the inverse transformation:\n$$y_n = \\exp(u_n).$$\nSince the exponential function $\\exp(\\cdot)$ yields a strictly positive value for any finite real argument, $y_n$ is guaranteed to be positive for all $n$, regardless of the step size $h$. This method is therefore unconditionally positivity-preserving.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by implementing and testing various numerical methods\n    for positivity preservation on the ODE y' = -k*y.\n    \"\"\"\n\n    def check_positivity(y_values):\n        \"\"\"Checks if all values in the sequence are non-negative.\"\"\"\n        for y in y_values:\n            if y  0.0:\n                return False\n        return True\n\n    def forward_euler(k, h, N, y0):\n        y = np.zeros(N + 1)\n        y[0] = y0\n        if y[0]  0: return False\n        \n        for n in range(N):\n            y[n+1] = y[n] * (1.0 - k * h)\n            if y[n+1]  0.0:\n                return False\n        return True\n\n    def explicit_midpoint(k, h, N, y0):\n        y = np.zeros(N + 1)\n        y[0] = y0\n        if y[0]  0: return False\n        \n        a = k * h\n        amp_factor = 1.0 - a + 0.5 * a**2\n        \n        for n in range(N):\n            y[n+1] = y[n] * amp_factor\n            if y[n+1]  0.0:\n                return False\n        return True\n\n    def rk3_classical(k, h, N, y0):\n        y = np.zeros(N + 1)\n        y[0] = y0\n        if y[0]  0: return False\n        \n        a = k * h\n        amp_factor = 1.0 - a + 0.5 * a**2 - (1.0/6.0) * a**3\n        \n        for n in range(N):\n            y[n+1] = y[n] * amp_factor\n            if y[n+1]  0.0:\n                return False\n        return True\n\n    def rk4_classical(k, h, N, y0):\n        y = np.zeros(N + 1)\n        y[0] = y0\n        if y[0]  0: return False\n        \n        a = k * h\n        amp_factor = 1.0 - a + 0.5 * a**2 - (1.0/6.0) * a**3 + (1.0/24.0) * a**4\n\n        for n in range(N):\n            y[n+1] = y[n] * amp_factor\n            if y[n+1]  0.0:\n                return False\n        return True\n\n    def ab2(k, h, N, y0):\n        if N == 0:\n            return y0 = 0.0\n\n        y = np.zeros(N + 1)\n        y[0] = y0\n        if y[0]  0.0: return False\n\n        # Use explicit midpoint for the first step y1\n        a = k * h\n        amp_factor_midpoint = 1.0 - a + 0.5 * a**2\n        y[1] = y[0] * amp_factor_midpoint\n        if y[1]  0.0: return False\n\n        for n in range(1, N):\n            f_n = -k * y[n]\n            f_n_minus_1 = -k * y[n-1]\n            y[n+1] = y[n] + h * (1.5 * f_n - 0.5 * f_n_minus_1)\n            if y[n+1]  0.0:\n                return False\n        return True\n\n    def log_transform(k, h, N, y0):\n        if y0 = 0:  # Log is not defined for non-positive y0\n            return False\n            \n        u = np.zeros(N + 1)\n        u[0] = np.log(y0)\n        \n        # This loop computes u_n, but reconstruction y_n = exp(u_n) is always  0.\n        # So we can just return True if y0  0.\n        # The loop is included for completeness but is not strictly necessary for the check.\n        for n in range(N):\n            u[n+1] = u[n] - k * h\n        \n        # Reconstructed y values are always positive\n        # y = np.exp(u)\n        # return check_positivity(y)\n        return True\n    \n    # Map method names to functions\n    methods = {\n        \"forward_euler\": forward_euler,\n        \"explicit_midpoint\": explicit_midpoint,\n        \"rk3\": rk3_classical,\n        \"rk4\": rk4_classical,\n        \"ab2\": ab2,\n        \"log_transform\": log_transform\n    }\n\n    # Test cases from the problem statement.\n    test_suite = [\n        (\"forward_euler\",     {'k': 2.0, 'h': 0.5, 'N': 5, 'y0': 1.0}),\n        (\"forward_euler\",     {'k': 2.0, 'h': 0.6, 'N': 5, 'y0': 1.0}),\n        (\"explicit_midpoint\", {'k': 100.0, 'h': 1.0, 'N': 5, 'y0': 1.0}),\n        (\"rk3\",               {'k': 1.0, 'h': 1.5, 'N': 5, 'y0': 1.0}),\n        (\"rk3\",               {'k': 1.0, 'h': 2.0, 'N': 5, 'y0': 1.0}),\n        (\"rk4\",               {'k': 5.0, 'h': 10.0, 'N': 5, 'y0': 1.0}),\n        (\"ab2\",               {'k': 3.0, 'h': 2.0/9.0, 'N': 10, 'y0': 1.0}),\n        (\"ab2\",               {'k': 3.0, 'h': 0.4, 'N': 10, 'y0': 1.0}),\n        (\"log_transform\",     {'k': 1000.0, 'h': 10.0, 'N': 5, 'y0': 0.5}),\n    ]\n\n    results = []\n    for method_name, params in test_suite:\n        func = methods[method_name]\n        result = func(**params)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3144090"}, {"introduction": "Choosing the right numerical solver for a given problem is a crucial skill involving trade-offs between accuracy, computational cost, and stability. To make this choice systematically, we need a robust framework for benchmarking solver performance that goes beyond simple runtime comparisons. This exercise (Problem 3144094) guides you through creating such a benchmark, using professional metrics like work-precision diagrams to evaluate how efficiently different solvers achieve a desired accuracy on problems with distinct characteristics.", "problem": "You are to design and implement a self-contained benchmarking program for Initial Value Problems (IVPs) of Ordinary Differential Equations (ODEs) with known analytic solutions. The goal is to benchmark different numerical solvers across IVPs that exhibit qualitatively distinct behaviors and to compute evaluation metrics beyond raw runtime, specifically work–precision relationships. Your program must produce a single-line output aggregating the requested metrics for a small test suite that covers oscillatory dynamics, pure exponential decay, and stiffness arising from multiple time scales.\n\nFoundational base and context: An Ordinary Differential Equation (ODE) IVP is defined by an equation of the form $y'(t) = f(t,y)$ together with an initial condition $y(t_0) = y_0$, where $y(t)$ is the unknown function and $f$ is a given function. Under standard regularity assumptions on $f$, existence and uniqueness of the solution follow from classical results such as the Picard–Lindelöf theorem. For benchmarking, we rely on IVPs whose analytic solutions are derived from these fundamental definitions and well-tested facts like separation of variables and linear constant-coefficient systems.\n\nYou must implement and benchmark the following three IVPs:\n\n1. Oscillatory case (simple harmonic oscillator): Define the two-dimensional system\n$$\n\\begin{cases}\nu'(t) = v(t), \\\\\nv'(t) = -\\omega^2 u(t),\n\\end{cases}\n$$\nwith initial condition $u(0) = 1$ and $v(0) = 0$, frequency parameter $\\omega = 5$ (the angle argument of trigonometric functions is in radians), and final time $T = 2$. This IVP has an analytic solution obtained by standard linear system methods that you must use to compute errors.\n\n2. Exponential decay case: Define the scalar IVP\n$$\ny'(t) = -\\lambda\\,y(t),\n$$\nwith initial condition $y(0) = 1$, decay parameter $\\lambda = 2$, and final time $T = 5$. This IVP has an analytic solution obtained by separation of variables that you must use to compute errors.\n\n3. Stiff multiscale decay case: Define the two-dimensional, decoupled system\n$$\n\\begin{cases}\ny_1'(t) = -\\lambda_1\\,y_1(t), \\\\\ny_2'(t) = -\\lambda_2\\,y_2(t),\n\\end{cases}\n$$\nwith initial condition $y_1(0) = 1$, $y_2(0) = 1$, parameters $\\lambda_1 = 1000$ and $\\lambda_2 = 1$, and final time $T = 1$. This IVP exhibits stiffness due to widely separated decay rates and has analytic solutions for each component that you must use to compute errors.\n\nFor each IVP, benchmark two numerical methods as follows:\n\n- For the oscillatory and exponential decay cases, use a nonstiff explicit Runge–Kutta method of order $4$–$5$ (denoted as \"RK45\") and a stiff-appropriate implicit method (denoted as \"Radau\").\n- For the stiff multiscale decay case, use \"RK45\" and a backward differentiation formula method (denoted as \"BDF\").\n\nFor each solver on each IVP, run the solver with relative tolerances $r_\\mathrm{tol} \\in \\{10^{-3}, 10^{-5}, 10^{-7}\\}$. Choose the absolute tolerance $a_\\mathrm{tol}$ to be $a_\\mathrm{tol} = 10^{-3}\\, r_\\mathrm{tol}$ so that relative error dominates while providing a floor for near-zero components. For each run, record:\n- The number of right-hand-side evaluations $N_{\\mathrm{fev}}$ performed by the solver.\n- The global error at the final time, defined as\n$$\nE = \\begin{cases}\n|y_{\\mathrm{num}}(T) - y_{\\mathrm{exact}}(T)|,  \\text{for scalar state}, \\\\\n\\left\\|y_{\\mathrm{num}}(T) - y_{\\mathrm{exact}}(T)\\right\\|_2,  \\text{for vector state},\n\\end{cases}\n$$\nwhere $\\|\\cdot\\|_2$ denotes the Euclidean norm.\n\nBeyond runtime, compute two metrics per solver per IVP:\n- Work–precision slope $s$: Given pairs $\\left(N_{\\mathrm{fev}}^{(i)}, E^{(i)}\\right)$ over the tolerance set, define $x_i = \\log_{10}\\left(N_{\\mathrm{fev}}^{(i)}\\right)$ and $y_i = \\log_{10}\\left(E^{(i)}\\right)$, and compute the best-fit line $y = s x + b$ by least squares. Report the slope $s$ as the work–precision slope. This characterizes how error decreases as computational work increases.\n- Minimal work to achieve target accuracy $W_{\\min}$: Given a target accuracy threshold $\\varepsilon_\\mathrm{target} = 10^{-4}$, define\n$$\nW_{\\min} = \\min\\left\\{\\, N_{\\mathrm{fev}}^{(i)} \\,\\middle|\\, E^{(i)} \\le \\varepsilon_\\mathrm{target} \\,\\right\\}.\n$$\nIf no tested tolerance yields $E^{(i)} \\le \\varepsilon_\\mathrm{target}$, report $W_{\\min} = -1$.\n\nTest suite coverage requirements:\n- The oscillatory case tests step-size control under rapidly changing phase while preserving amplitude.\n- The exponential decay case is the general happy path with smooth monotone dynamics.\n- The stiff multiscale case provides an edge case with very different time scales, challenging explicit methods.\n\nFinal output format:\nYour program must produce a single line of output containing twelve comma-separated numbers enclosed in square brackets in the exact order below:\n1. Oscillatory case, \"RK45\": $W_{\\min}$, then $s$.\n2. Oscillatory case, \"Radau\": $W_{\\min}$, then $s$.\n3. Exponential decay case, \"RK45\": $W_{\\min}$, then $s$.\n4. Exponential decay case, \"Radau\": $W_{\\min}$, then $s$.\n5. Stiff multiscale case, \"RK45\": $W_{\\min}$, then $s$.\n6. Stiff multiscale case, \"BDF\": $W_{\\min}$, then $s$.\n\nAll angles in trigonometric functions are in radians. No physical units are required beyond the time parameter. The final answers for each metric must be reported as numbers (integers for $W_{\\min}$ and floats for $s$). Your program must be complete and runnable without user input.", "solution": "The accuracy of the numerical solvers is measured against known analytic solutions. These are derived using standard methods for linear ODEs.\n\n**1. Analytic Solutions of the Initial Value Problems**\n\n**a. Oscillatory Case (Simple Harmonic Oscillator)**\nThe system is given by:\n$$\n\\begin{cases}\nu'(t) = v(t), \\\\\nv'(t) = -\\omega^2 u(t),\n\\end{cases}\n\\quad \\text{with} \\quad\n\\begin{cases}\nu(0) = 1, \\\\\nv(0) = 0.\n\\end{cases}\n$$\nThis is a second-order linear ODE $u''(t) + \\omega^2 u(t) = 0$. The general solution is $u(t) = C_1 \\cos(\\omega t) + C_2 \\sin(\\omega t)$. The second state variable is $v(t) = u'(t) = -C_1 \\omega \\sin(\\omega t) + C_2 \\omega \\cos(\\omega t)$. Applying the initial conditions:\n$u(0) = C_1 = 1$.\n$v(0) = C_2 \\omega = 0 \\implies C_2 = 0$ (since $\\omega=5 \\neq 0$).\nThe exact solution is therefore:\n$$\n\\vec{y}_{\\mathrm{exact}}(t) = \\begin{pmatrix} u(t) \\\\ v(t) \\end{pmatrix} = \\begin{pmatrix} \\cos(\\omega t) \\\\ -\\omega \\sin(\\omega t) \\end{pmatrix}\n$$\nFor $\\omega=5$ and final time $T=2$, the exact solution is $\\vec{y}_{\\mathrm{exact}}(2) = (\\cos(10), -5\\sin(10))$.\n\n**b. Exponential Decay Case**\nThe scalar IVP is:\n$$\ny'(t) = -\\lambda y(t), \\quad \\text{with} \\quad y(0) = 1.\n$$\nThis is a separable first-order linear ODE. Integration of $\\frac{dy}{y} = -\\lambda dt$ yields $\\ln|y| = -\\lambda t + C$, or $y(t) = K e^{-\\lambda t}$. The initial condition $y(0)=1$ gives $K=1$.\nThe exact solution is:\n$$\ny_{\\mathrm{exact}}(t) = e^{-\\lambda t}\n$$\nFor $\\lambda=2$ and final time $T=5$, the exact solution is $y_{\\mathrm{exact}}(5) = e^{-10}$.\n\n**c. Stiff Multiscale Decay Case**\nThe system is given by two decoupled exponential decay equations:\n$$\n\\begin{cases}\ny_1'(t) = -\\lambda_1 y_1(t), \\\\\ny_2'(t) = -\\lambda_2 y_2(t),\n\\end{cases}\n\\quad \\text{with} \\quad\n\\begin{cases}\ny_1(0) = 1, \\\\\ny_2(0) = 1.\n\\end{cases}\n$$\nFollowing the logic from the exponential decay case, each component has a solution of the form $y_i(t) = e^{-\\lambda_i t}$.\nThe exact vector solution is:\n$$\n\\vec{y}_{\\mathrm{exact}}(t) = \\begin{pmatrix} y_1(t) \\\\ y_2(t) \\end{pmatrix} = \\begin{pmatrix} e^{-\\lambda_1 t} \\\\ e^{-\\lambda_2 t} \\end{pmatrix}\n$$\nFor $\\lambda_1=1000$, $\\lambda_2=1$, and final time $T=1$, the exact solution is $\\vec{y}_{\\mathrm{exact}}(1) = (e^{-1000}, e^{-1})$. Note that $e^{-1000}$ underflows to $0$ in standard double-precision floating-point arithmetic.\n\n**2. Numerical Benchmarking and Metric Calculation**\n\nFor each of the six (IVP, solver) combinations, the following procedure is executed:\n- A loop runs over the set of relative tolerances $r_\\mathrm{tol} \\in \\{10^{-3}, 10^{-5}, 10^{-7}\\}$.\n- For each $r_\\mathrm{tol}$, the absolute tolerance is set to $a_\\mathrm{tol} = 10^{-3} r_\\mathrm{tol}$.\n- The `scipy.integrate.solve_ivp` function is called with the appropriate right-hand-side function, time span, initial condition, method, and tolerances.\n- From the solver's output, two pieces of data are extracted: the number of function evaluations, $N_{\\mathrm{fev}}$, and the numerical solution at the final time, $\\vec{y}_{\\mathrm{num}}(T)$.\n- The global error $E$ is computed as the Euclidean norm of the difference between the numerical and exact solutions at $T$, i.e., $E = \\|\\vec{y}_{\\mathrm{num}}(T) - \\vec{y}_{\\mathrm{exact}}(T)\\|_2$. This single definition accommodates both scalar and vector problems.\n- This process yields three pairs of $(N_{\\mathrm{fev}}^{(i)}, E^{(i)})$, one for each tolerance level.\n\nWith this data, the two evaluation metrics are computed:\n\n**a. Minimal Work ($W_{\\min}$)**\nThe minimal work to achieve a target accuracy $\\varepsilon_{\\mathrm{target}} = 10^{-4}$ is found by examining the collected data:\n$$\nW_{\\min} = \\min\\left\\{\\, N_{\\mathrm{fev}}^{(i)} \\,\\middle|\\, E^{(i)} \\le \\varepsilon_{\\mathrm{target}} \\,\\right\\}\n$$\nIf none of the runs achieve the target accuracy, $W_{\\min}$ is set to $-1$ as specified.\n\n**b. Work-Precision Slope ($s$)**\nThe work-precision slope characterizes the efficiency of the solver. It is the slope of a line fitted to the data on a log-log plot.\n1. The data is transformed: $x_i = \\log_{10}(N_{\\mathrm{fev}}^{(i)})$ and $y_i = \\log_{10}(E^{(i)})$.\n2. A linear model $y = sx + b$ is fitted to the three points $(x_i, y_i)$ using the method of least squares.\n3. The slope $s$ is calculated. The `numpy.polyfit` function provides a robust and convenient way to perform this linear regression. The slope is the first coefficient returned by `np.polyfit(x, y, 1)`.\n\nThe final program implements this entire workflow, systematically processing each case and assembling the twelve resulting metrics in the specified order for output.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.integrate import solve_ivp\n\ndef solve():\n    \"\"\"\n    Designs and implements a self-contained benchmarking program for ODE IVPs.\n    \"\"\"\n\n    # --- Define Right-Hand-Side (RHS) functions for the ODEs ---\n    def f_oscillatory(t, y, omega):\n        u, v = y\n        return [v, -omega**2 * u]\n\n    def f_exponential(t, y, lam):\n        return -lam * y\n\n    def f_stiff(t, y, lam1, lam2):\n        y1, y2 = y\n        return [-lam1 * y1, -lam2 * y2]\n\n    # --- Define analytic solutions for error calculation ---\n    def sol_oscillatory(t, omega):\n        return np.array([np.cos(omega * t), -omega * np.sin(omega * t)])\n\n    def sol_exponential(t, lam):\n        return np.array([np.exp(-lam * t)])\n\n    def sol_stiff(t, lam1, lam2):\n        return np.array([np.exp(-lam1 * t), np.exp(-lam2 * t)])\n\n    # --- Define the test suite ---\n    test_cases_config = [\n        {\n            \"name\": \"Oscillatory\",\n            \"fun\": f_oscillatory,\n            \"t_span\": [0, 2],\n            \"y0\": np.array([1.0, 0.0]),\n            \"params\": {\"omega\": 5.0},\n            \"exact_sol\": sol_oscillatory,\n            \"solvers\": [\"RK45\", \"Radau\"]\n        },\n        {\n            \"name\": \"Exponential Decay\",\n            \"fun\": f_exponential,\n            \"t_span\": [0, 5],\n            \"y0\": np.array([1.0]),\n            \"params\": {\"lam\": 2.0},\n            \"exact_sol\": sol_exponential,\n            \"solvers\": [\"RK45\", \"Radau\"]\n        },\n        {\n            \"name\": \"Stiff Multiscale\",\n            \"fun\": f_stiff,\n            \"t_span\": [0, 1],\n            \"y0\": np.array([1.0, 1.0]),\n            \"params\": {\"lam1\": 1000.0, \"lam2\": 1.0},\n            \"exact_sol\": sol_stiff,\n            \"solvers\": [\"RK45\", \"BDF\"]\n        }\n    ]\n\n    # --- Define benchmark parameters ---\n    tolerances = [1e-3, 1e-5, 1e-7]\n    target_accuracy = 1e-4\n\n    final_results = []\n\n    # --- Main benchmarking loop ---\n    for config in test_cases_config:\n        for solver_name in config[\"solvers\"]:\n            \n            run_data = [] # Stores (nfev, error) for each tolerance run\n\n            for rtol in tolerances:\n                atol = 1e-3 * rtol\n                \n                # Run the solver\n                sol = solve_ivp(\n                    config[\"fun\"],\n                    config[\"t_span\"],\n                    config[\"y0\"],\n                    method=solver_name,\n                    rtol=rtol,\n                    atol=atol,\n                    args=tuple(config[\"params\"].values())\n                )\n\n                # Get numerical and exact solutions at the final time\n                y_num_final = sol.y[:, -1]\n                t_final = config[\"t_span\"][1]\n                y_exact_final = config[\"exact_sol\"](t_final, **config[\"params\"])\n\n                # Calculate global error and store data\n                error = np.linalg.norm(y_num_final - y_exact_final)\n                nfev = sol.nfev\n                run_data.append({\"nfev\": nfev, \"error\": error})\n\n            # --- Calculate metrics for the (IVP, solver) pair ---\n\n            # 1. Minimal work to achieve target accuracy (W_min)\n            eligible_runs = [r[\"nfev\"] for r in run_data if r[\"error\"] = target_accuracy]\n            W_min = int(min(eligible_runs)) if eligible_runs else -1\n\n            # 2. Work-precision slope (s)\n            nfev_vals = np.array([r[\"nfev\"] for r in run_data])\n            error_vals = np.array([r[\"error\"] for r in run_data])\n\n            # Use only points where error is positive for log-log plot\n            valid_points = error_vals  0\n            if np.sum(valid_points) = 2:\n                log_nfev = np.log10(nfev_vals[valid_points])\n                log_error = np.log10(error_vals[valid_points])\n                \n                # Check for constant nfev, which would make slope undefined\n                if np.all(log_nfev == log_nfev[0]):\n                    slope = np.nan # Undefined slope\n                else:\n                    # Fit a line y = slope*x + intercept and get the slope\n                    slope = np.polyfit(log_nfev, log_error, 1)[0]\n            else:\n                # Not enough data for a meaningful fit\n                slope = np.nan\n\n            final_results.append(W_min)\n            final_results.append(slope)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n```", "id": "3144094"}]}