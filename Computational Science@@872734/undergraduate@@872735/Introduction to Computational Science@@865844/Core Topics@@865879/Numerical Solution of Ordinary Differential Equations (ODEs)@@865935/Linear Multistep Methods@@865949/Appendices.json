{"hands_on_practices": [{"introduction": "To understand the accuracy of any numerical method, we must first analyze the error it introduces in a single step. This exercise focuses on the fundamental concept of Local Truncation Error (LTE), which quantifies this single-step error under the idealized assumption that all previous calculations were perfectly accurate. By deriving the LTE for the second-order Adams-Bashforth method on a simple problem with a known solution, you will gain a concrete understanding of how a method's order of accuracy is formally defined and calculated [@problem_id:3153704].", "problem": "Consider the initial value problem $y'(t) = f(t) = t^{2}$ with $y(0) = 0$, whose exact antiderivative is known. Using the two-step Adams–Bashforth (AB2) method, define the local truncation error at the $(n+1)$-th step as the difference between the exact one-step increment and the method’s predicted increment when the previous values are exact. Starting from the fundamental identity $y(t+h) - y(t) = \\int_{t}^{t+h} f(s)\\,ds$, derive, in closed form, the symbolic expression for the local truncation error $\\tau_{n+1}$ as a function of the step size $h$ and the grid point $t_{n}$. Then verify the order by computing $\\tau_{n+1}$ numerically for $h = 0.2$ at any $t_{n}$ and rounding your numerical value to five significant figures. Provide the exact symbolic expression for $\\tau_{n+1}$ as your final answer (no units).", "solution": "The problem requires the derivation of the local truncation error for the two-step Adams-Bashforth (AB2) method applied to the initial value problem $y'(t) = f(t) = t^2$ with $y(0) = 0$.\n\nFirst, we establish the necessary definitions. The grid points are defined as $t_k = k h$ for a constant step size $h > 0$. The given ordinary differential equation is $y'(t) = f(t, y(t))$, where the function $f$ is independent of $y$ and is given by $f(t) = t^2$.\n\nThe two-step Adams-Bashforth (AB2) method generates the approximation $y_{n+1}$ to the exact solution $y(t_{n+1})$ using the formula:\n$$y_{n+1} = y_n + h \\left( \\frac{3}{2} f_n - \\frac{1}{2} f_{n-1} \\right)$$\nwhere $f_k = f(t_k, y_k)$. For the specific problem given, $f_k = t_k^2$.\n\nThe local truncation error at step $n+1$, denoted $\\tau_{n+1}$, is defined as the difference between the exact one-step increment and the method's predicted increment, assuming all previous values are exact. That is, we assume $y_n = y(t_n)$ and $y_{n-1} = y(t_{n-1})$. The error is thus:\n$$\\tau_{n+1} = \\left( y(t_{n+1}) - y(t_n) \\right) - \\left( y_{n+1} - y_n \\right)$$\nSubstituting the AB2 formula for the numerical increment $(y_{n+1} - y_n)$, we have:\n$$\\tau_{n+1} = \\left( y(t_{n+1}) - y(t_n) \\right) - h \\left( \\frac{3}{2} f(t_n) - \\frac{1}{2} f(t_{n-1}) \\right)$$\nwhere we use $f(t_k)$ since we assume past points lie on the exact solution curve.\n\nThe problem requires the derivation to start from the fundamental identity for the exact increment, which is obtained by integrating the differential equation $y'(s) = f(s)$ from $t_n$ to $t_{n+1}$:\n$$y(t_{n+1}) - y(t_n) = \\int_{t_n}^{t_{n+1}} f(s) \\, ds$$\nFor $f(s) = s^2$, the exact increment is:\n$$y(t_{n+1}) - y(t_n) = \\int_{t_n}^{t_{n+1}} s^2 \\, ds = \\left[ \\frac{s^3}{3} \\right]_{t_n}^{t_{n+1}} = \\frac{t_{n+1}^3 - t_n^3}{3}$$\nUsing $t_{n+1} = t_n + h$, we expand the expression:\n$$y(t_{n+1}) - y(t_n) = \\frac{(t_n + h)^3 - t_n^3}{3} = \\frac{(t_n^3 + 3t_n^2h + 3t_nh^2 + h^3) - t_n^3}{3} = t_n^2h + t_nh^2 + \\frac{h^3}{3}$$\n\nNext, we evaluate the numerical increment predicted by the AB2 method.\nThe AB2 increment is $h \\left( \\frac{3}{2} f(t_n) - \\frac{1}{2} f(t_{n-1}) \\right)$. With $f(t) = t^2$, $t_n$, and $t_{n-1} = t_n - h$, this becomes:\n$$\\text{AB2 increment} = h \\left( \\frac{3}{2} t_n^2 - \\frac{1}{2} (t_n - h)^2 \\right)$$\nExpanding the squared term:\n$$\\text{AB2 increment} = h \\left( \\frac{3}{2} t_n^2 - \\frac{1}{2} (t_n^2 - 2t_nh + h^2) \\right) = h \\left( \\frac{3}{2} t_n^2 - \\frac{1}{2} t_n^2 + t_nh - \\frac{1}{2} h^2 \\right)$$\nSimplifying the terms inside the parenthesis:\n$$\\text{AB2 increment} = h \\left( t_n^2 + t_nh - \\frac{1}{2}h^2 \\right) = t_n^2h + t_nh^2 - \\frac{h^3}{2}$$\n\nNow, we compute the local truncation error $\\tau_{n+1}$ by subtracting the numerical increment from the exact increment:\n$$\\tau_{n+1} = \\left( t_n^2h + t_nh^2 + \\frac{h^3}{3} \\right) - \\left( t_n^2h + t_nh^2 - \\frac{h^3}{2} \\right)$$\nThe terms involving $t_n$ cancel out:\n$$\\tau_{n+1} = \\frac{h^3}{3} - \\left(-\\frac{h^3}{2}\\right) = \\frac{h^3}{3} + \\frac{h^3}{2} = \\left(\\frac{2+3}{6}\\right)h^3 = \\frac{5}{6}h^3$$\nThe symbolic expression for the local truncation error is $\\tau_{n+1} = \\frac{5}{6}h^3$. It is noteworthy that for this specific problem, where $y'''(t) = f''(t)$ is a constant, the local truncation error is independent of the grid point $t_n$.\n\nTo verify this result numerically, we use $h = 0.2$ as requested. The symbolic formula gives:\n$$\\tau_{n+1} = \\frac{5}{6}(0.2)^3 = \\frac{5}{6}(0.008) = \\frac{0.04}{6} = \\frac{0.02}{3} \\approx 0.0066666...$$\nLet's confirm this by direct calculation at an arbitrary grid point, for instance $t_n = 1.0$. This implies $t_{n-1} = 0.8$ and $t_{n+1} = 1.2$.\nThe exact increment is:\n$$y(1.2)-y(1.0) = \\int_{1.0}^{1.2} s^2 \\, ds = \\frac{1.2^3 - 1.0^3}{3} = \\frac{1.728 - 1.0}{3} = \\frac{0.728}{3}$$\nThe AB2 increment is:\n$$h \\left( \\frac{3}{2} t_n^2 - \\frac{1}{2} t_{n-1}^2 \\right) = 0.2 \\left( \\frac{3}{2}(1.0)^2 - \\frac{1}{2}(0.8)^2 \\right) = 0.2 \\left( 1.5 - \\frac{1}{2}(0.64) \\right) = 0.2(1.5 - 0.32) = 0.2(1.18) = 0.236$$\nThe numerical error is:\n$$\\tau_{n+1} = \\frac{0.728}{3} - 0.236 = \\frac{0.728 - 3 \\times 0.236}{3} = \\frac{0.728 - 0.708}{3} = \\frac{0.02}{3}$$\nThis numerical computation perfectly matches the result from the symbolic formula. Rounded to five significant figures, the numerical value is $0.0066667$. The fact that the error is proportional to $h^3$ confirms that the method is second-order, which is characteristic of the AB2 method.\nThe final required answer is the symbolic expression for the local truncation error.", "answer": "$$\\boxed{\\frac{5}{6}h^{3}}$$", "id": "3153704"}, {"introduction": "While Local Truncation Error tells us about the error in one step, our ultimate goal is to control the global error accumulated over the entire integration interval. There is a crucial theoretical link between the order of the local error, typically $O(h^{p+1})$, and the order of the global error, which is $O(h^p)$. This coding practice allows you to numerically verify this fundamental relationship for the family of Adams-Bashforth methods, providing a powerful, hands-on demonstration of convergence theory in action [@problem_id:2410045].", "problem": "Write a program that, for a family of uniform step sizes and method orders, numerically demonstrates the asymptotic orders of the global error and the local truncation error of the $p$-step Adams–Bashforth (AB) method. Consider the initial value problem $y^{\\prime}(t)=-y(t)$ for $t \\in [0,1]$ with $y(0)=1$, whose exact solution is $y(t)=e^{-t}$. For a given integer order $p \\in \\{1,2,3,4\\}$, define the uniform grid $t_n=n h$ with $h \\in \\{1/8,1/16,1/32,1/64\\}$ and $n=0,1,\\dots,N$, where $N=1/h$. For each pair $(p,h)$, apply a $p$-step Adams–Bashforth method on this grid to compute a numerical approximation $y_N$ to $y(1)$, using, for initialization, the exact solution values at the first $p$ grid points, that is, $y(t_0),y(t_1),\\dots,y(t_{p-1})$. For the same $(p,h)$, define the local one-step residual for the exact solution, for indices $n=p-1,p,\\dots,N-1$, by\n$$\nr_{n+1} \\equiv y(t_{n+1}) - y(t_n) - h \\sum_{j=0}^{p-1} \\beta_j f\\!\\left(t_{n-j}, y(t_{n-j})\\right),\n$$\nwhere $f(t,y)=-y$ and $\\{\\beta_j\\}_{j=0}^{p-1}$ are the $p$-step Adams–Bashforth weights associated with the method. For each fixed $p$, compute the observed global error order as the slope of the best-fit line of $\\log(E(h))$ versus $\\log(h)$ over the set of step sizes, where $E(h)=\\lvert y_N - y(1)\\rvert$, and compute the observed local truncation error order as the slope of the best-fit line of $\\log(R(h))$ versus $\\log(h)$, where $R(h)=\\max_{n=p-1,\\dots,N-1} \\lvert r_{n+1}\\rvert$. Report these observed orders for each $p \\in \\{1,2,3,4\\}$ to numerically demonstrate that the global error is $O(h^p)$ and the local truncation error is $O(h^{p+1})$.\n\nTest suite and answer specification:\n- Parameters to test:\n  - Orders $p \\in \\{1,2,3,4\\}$.\n  - Step sizes $h \\in \\{1/8,1/16,1/32,1/64\\}$.\n- For each $p$, output two floating-point numbers:\n  - The observed global error order based on the four values of $E(h)$.\n  - The observed local truncation error order based on the four values of $R(h)$.\n- Required final output format:\n  - Your program should produce a single line containing a comma-separated list enclosed in square brackets with eight numbers corresponding to $p=1,2,3,4$ in order, flattened as $[g_1,\\ell_1,g_2,\\ell_2,g_3,\\ell_3,g_4,\\ell_4]$, where $g_p$ is the observed global error order for order $p$, and $\\ell_p$ is the observed local truncation error order for order $p$. Each number must be rounded to three decimal places.\n- There are no physical units or angles to report in this problem.\n\nYour program must be self-contained and must not read any input. It must compute and print the results as specified above.", "solution": "The problem presented is a standard exercise in the numerical analysis of ordinary differential equations. It is scientifically sound, well-posed, and contains all necessary information to proceed. The task is to numerically demonstrate the theoretical orders of convergence for the global error and the local truncation error of the $p$-step Adams–Bashforth methods for orders $p=1, 2, 3, 4$. The problem is valid.\n\nThe specified initial value problem (IVP) is given by\n$$\ny^{\\prime}(t) = -y(t), \\quad t \\in [0, 1]\n$$\nwith the initial condition $y(0) = 1$. The function on the right-hand side is $f(t,y) = -y$. The analytical solution to this IVP is known to be $y(t) = e^{-t}$.\n\nThe general form of a $p$-step Adams–Bashforth (AB) method for approximating the solution to an IVP $y^{\\prime} = f(t,y)$ on a uniform grid $t_n = n h$ is\n$$\ny_{n+1} = y_n + h \\sum_{j=0}^{p-1} \\beta_j f(t_{n-j}, y_{n-j})\n$$\nThis formula is applied for $n \\geq p-1$. The method requires $p$ starting values, $y_0, y_1, \\dots, y_{p-1}$. The problem dictates that these shall be taken from the exact solution, i.e., $y_k = y(t_k) = e^{-kh}$ for $k=0, 1, \\dots, p-1$. The coefficients $\\{\\beta_j\\}_{j=0}^{p-1}$ for the methods of orders $p=1, 2, 3, 4$ are standard:\nFor $p=1$ (Forward Euler):\n$$\n\\beta_0 = 1\n$$\nFor $p=2$:\n$$\n\\beta_0 = \\frac{3}{2}, \\quad \\beta_1 = -\\frac{1}{2}\n$$\nFor $p=3$:\n$$\n\\beta_0 = \\frac{23}{12}, \\quad \\beta_1 = -\\frac{16}{12}, \\quad \\beta_2 = \\frac{5}{12}\n$$\nFor $p=4$:\n$$\n\\beta_0 = \\frac{55}{24}, \\quad \\beta_1 = -\\frac{59}{24}, \\quad \\beta_2 = \\frac{37}{24}, \\quad \\beta_3 = -\\frac{9}{24}\n$$\nFor each pair $(p,h)$, two quantities must be computed. First, the global error at the final time $t=1$. This is defined as $E(h) = \\lvert y_N - y(1)\\rvert$, where $N=1/h$, $y_N$ is the numerical approximation at $t_N=1$, and $y(1)=e^{-1}$ is the exact value. A $p$-step AB method is known to have a global error of order $p$, so we expect $E(h) \\propto h^p$.\n\nSecond, we analyze the local one-step residual, which is defined by substituting the exact solution $y(t)$ into the one-step formula:\n$$\nr_{n+1} \\equiv y(t_{n+1}) - y(t_n) - h \\sum_{j=0}^{p-1} \\beta_j f(t_{n-j}, y(t_{n-j}))\n$$\nThe problem defines the measure of the local truncation error as $R(h) = \\max_{n=p-1,\\dots,N-1} \\lvert r_{n+1}\\rvert$. For a $p$-step AB method, this residual is theoretically of order $p+1$, meaning $r_{n+1} = O(h^{p+1})$. Consequently, we expect to observe $R(h) \\propto h^{p+1}$. Note that this is distinct from the standard definition of local truncation error, which is $T_{n+1} = r_{n+1}/h$ and would be of order $p$. The problem is self-consistent in its definition.\n\nTo numerically verify these orders of convergence, we assume the relationships $E(h) \\approx C_g h^{g_p}$ and $R(h) \\approx C_l h^{\\ell_p}$, where $g_p$ and $\\ell_p$ are the observed orders of convergence for a method of order $p$. By taking the natural logarithm, we obtain linear relationships:\n$$\n\\log(E(h)) \\approx \\log(C_g) + g_p \\log(h)\n$$\n$$\n\\log(R(h)) \\approx \\log(C_l) + \\ell_p \\log(h)\n$$\nFor each fixed $p$, we compute the values of $E(h)$ and $R(h)$ for the set of step sizes $h \\in \\{1/8, 1/16, 1/32, 1/64\\}$. We then perform a linear regression on the sets of points $(\\log(h), \\log(E(h)))$ and $(\\log(h), \\log(R(h)))$. The slopes of these best-fit lines provide the estimates for the observed orders, $g_p$ and $\\ell_p$, respectively. The slope $m$ of a best-fit line for a set of points $(x_i, y_i)$ is calculated using the formula from least squares regression:\n$$\nm = \\frac{M \\sum_{i=1}^M x_i y_i - (\\sum_{i=1}^M x_i)(\\sum_{i=1}^M y_i)}{M \\sum_{i=1}^M x_i^2 - (\\sum_{i=1}^M x_i)^2}\n$$\nwhere $M=4$ is the number of data points corresponding to the four step sizes.\n\nThe algorithm proceeds as follows: for each order $p \\in \\{1, 2, 3, 4\\}$, we iterate through the given step sizes $h$. For each $h$, we compute the numerical solution up to $t=1$ to find $E(h)$, and we compute the set of residuals $\\{r_{n+1}\\}$ to find $R(h)$. After collecting the data for all step sizes, we compute the slopes $g_p$ and $\\ell_p$. The final output will consist of the sequence of these eight computed values, rounded as specified.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Computes the observed orders of convergence for global and local truncation\n    errors for Adams-Bashforth methods of orders p=1, 2, 3, 4.\n    \"\"\"\n    p_orders = [1, 2, 3, 4]\n    h_steps = np.array([1/8, 1/16, 1/32, 1/64], dtype=np.float64)\n\n    # Pre-computed Adams-Bashforth coefficients (beta_0, beta_1, ...)\n    ab_coeffs = {\n        1: np.array([1.0], dtype=np.float64),\n        2: np.array([3/2, -1/2], dtype=np.float64),\n        3: np.array([23/12, -16/12, 5/12], dtype=np.float64),\n        4: np.array([55/24, -59/24, 37/24, -9/24], dtype=np.float64)\n    }\n\n    # The IVP is y' = -y, so f(t, y) = -y.\n    # The exact solution is y(t) = exp(-t).\n    y_exact_func = lambda t: np.exp(-t)\n    f = lambda t, y: -y\n    \n    final_results = []\n\n    for p in p_orders:\n        log_h_vals = []\n        log_global_errors = []\n        log_local_errors = []\n\n        for h in h_steps:\n            N = int(1/h)\n            # Grid points: t_0, t_1, ..., t_N\n            t = np.linspace(0.0, 1.0, N + 1, dtype=np.float64)\n\n            # --- Global Error Calculation ---\n            y_numerical = np.zeros(N + 1, dtype=np.float64)\n            # Initialize first p values with the exact solution\n            y_numerical[:p] = y_exact_func(t[:p])\n\n            for n in range(p - 1, N):\n                # History of f(t,y) values: f_n, f_{n-1}, ..., f_{n-p+1}\n                # Since f(t,y)=-y, values are -y_numerical[n], -y_numerical[n-1],...\n                f_history = -y_numerical[n - p + 1 : n + 1]\n                # Invert for correct order in dot product with betas\n                f_history_reversed = f_history[::-1]\n                \n                y_numerical[n + 1] = y_numerical[n] + h * np.sum(ab_coeffs[p] * f_history_reversed)\n\n            global_error = np.abs(y_numerical[N] - y_exact_func(1.0))\n            \n            log_h_vals.append(np.log(h))\n            log_global_errors.append(np.log(global_error))\n\n            # --- Local Truncation Error (Residual) Calculation ---\n            y_exact_vals = y_exact_func(t)\n            residuals = []\n            for n in range(p - 1, N):\n                # History of f(t, y(t)) values based on exact solution\n                f_exact_history = f(None, y_exact_vals[n - p + 1 : n + 1])\n                f_exact_history_reversed = f_exact_history[::-1]\n                \n                # Definition of residual r_{n+1}\n                r_n_plus_1 = (y_exact_vals[n + 1] - y_exact_vals[n] - \n                              h * np.sum(ab_coeffs[p] * f_exact_history_reversed))\n                residuals.append(r_n_plus_1)\n            \n            # Max norm of the residual vector\n            local_error_norm = np.max(np.abs(np.array(residuals)))\n            log_local_errors.append(np.log(local_error_norm))\n        \n        # --- Order Calculation using Linear Regression ---\n        # np.polyfit(_x, _y, 1) returns [slope, intercept] of the best-fit line\n        # The slope corresponds to the order of convergence.\n        global_order = np.polyfit(log_h_vals, log_global_errors, 1)[0]\n        local_order = np.polyfit(log_h_vals, log_local_errors, 1)[0]\n        \n        final_results.extend([global_order, local_order])\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{val:.3f}' for val in final_results)}]\")\n\nsolve()\n```", "id": "2410045"}, {"introduction": "Beyond accuracy, stability is a paramount concern for linear multistep methods. A unique feature of these methods is the presence of \"parasitic roots\" in their characteristic polynomial, which do not correspond to the true solution of the differential equation. This exercise provides a fascinating look into how these parasitic roots, even in a theoretically stable method, can interact with and amplify small perturbations like machine roundoff error. By simulating a simple case with injected noise, you will uncover how the multistep nature of the recurrence can lead to unexpected error propagation, a critical insight for designing robust numerical simulations [@problem_id:3153656].", "problem": "You are asked to investigate how additive roundoff-scale perturbations propagate under a linear multistep method, using the fourth-order Adams-Bashforth (AB4) method as the stepping scheme. Work with the scalar initial value problem defined by the linear test equation $y'=\\lambda y$ with $\\lambda=0$, so that the exact solution is constant. The fundamental base for your derivation and design must be the general $k$-step linear multistep method definition, the time-discretized update relation derived from it for Adams-Bashforth methods, and stochastic properties of independent, zero-mean perturbations. You must not assume any special formula beyond these foundations in the problem statement.\n\nYour task is to derive from first principles the AB4 coefficients using the standard Adams-Bashforth construction and to analyze the discrete dynamics induced by the method when applied to the homogeneous problem $y' = 0$. Then, to model machine precision, inject independent and identically distributed additive perturbations of magnitude proportional to a given machine precision parameter $\\epsilon$ into the computed $y_n$ after each step. Specifically, treat the perturbation at each step as a Gaussian random variable with zero mean and standard deviation $\\epsilon$, independent across steps. This setup isolates how parasitic components of the multistep recurrence are affected by small errors in the dependent variable sequence.\n\nStarting from the general linear multistep method definition,\n- a $k$-step linear multistep method has the form $\\sum_{j=0}^{k}\\alpha_j y_{n+j} = h\\sum_{j=0}^{k}\\beta_j f_{n+j}$ where $f_{n+j} = f(t_{n+j},y_{n+j})$,\n- Adams-Bashforth methods correspond to choosing $\\alpha_k=1$, $\\alpha_{k-1}=-1$, $\\alpha_j=0$ for all other $j$, and $\\beta_j$ the quadrature weights obtained from integrating the Lagrange interpolant of the right-hand side over one step.\n\nUse this base to:\n1) Derive the characteristic polynomial and its roots for the AB4 method applied to $y'=0$, and interpret these roots as principal and parasitic modes.\n2) Construct an algorithm that advances $y_n$ using AB4 for general $\\lambda$ and step size $h$, and, in each step, adds an independent perturbation $\\eta_{n+1}\\sim\\mathcal{N}(0,\\epsilon^2)$ to the just-computed $y_{n+1}$ to model machine-precision-scale contamination. You may bootstrap the history using consistent lower-order Adams-Bashforth schemes for the first few steps; ensure your approach is correct for any $\\lambda$, though the tests will use $\\lambda=0$.\n3) For the case $\\lambda=0$, quantify numerically the empirical growth scale by computing the metric\n$$G(N,\\epsilon)=\\frac{\\max_{0\\le n\\le N}\\lvert y_n\\rvert}{\\epsilon\\sqrt{N}},$$\nand report $G(N,\\epsilon)$ for several $(N,\\epsilon)$ pairs. The normalization by $\\epsilon\\sqrt{N}$ is chosen based on the scaling of a partial sum of independent, zero-mean perturbations.\n\nNo physical units apply. Angles are not used. All numerical values must be expressed as real numbers.\n\nTest suite and answer specification:\n- Use time step $h=1$ in all cases.\n- Use initial value $y_0=0$ in all cases.\n- Use the following five test cases, each specified by $(\\epsilon, N, \\text{seed})$:\n  - Case 1: $(\\epsilon = 2.220446049250313\\times 10^{-16},\\, N = 200000,\\, \\text{seed} = 12345)$,\n  - Case 2: $(\\epsilon = 10^{-12},\\, N = 200000,\\, \\text{seed} = 12345)$,\n  - Case 3: $(\\epsilon = 10^{-8},\\, N = 200000,\\, \\text{seed} = 12345)$,\n  - Case 4: $(\\epsilon = 10^{-8},\\, N = 1000,\\, \\text{seed} = 98765)$,\n  - Case 5: $(\\epsilon = 10^{-8},\\, N = 1,\\, \\text{seed} = 42)$.\n- For each case, run the simulation with $\\lambda=0$ and report the single float $G(N,\\epsilon)$ defined above.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\"). The list must contain the five $G(N,\\epsilon)$ values in the order of Cases $1$ through $5$.\n\nYour program must be completely self-contained and produce deterministic results by initializing the random number generator with the provided seed for each case.", "solution": "The problem statement has been meticulously reviewed and is determined to be **valid**. It is a well-posed, scientifically grounded problem in the field of numerical analysis, specifically concerning the stability and error propagation characteristics of linear multistep methods. The problem is self-contained, with all necessary data, definitions, and conditions provided for a unique and verifiable solution.\n\nThe task is to analyze the propagation of additive roundoff-scale perturbations in the fourth-order Adams-Bashforth (AB4) method when applied to the homogeneous initial value problem $y' = \\lambda y$ with $\\lambda=0$. This involves deriving the method's properties from first principles, constructing a simulation that includes a stochastic error model, and quantifying the resulting error growth.\n\n**1. Derivation of the Fourth-Order Adams-Bashforth (AB4) Method**\n\nA general $k$-step linear multistep method (LMM) for solving the ordinary differential equation (ODE) $y'(t) = f(t, y(t))$ has the form:\n$$\n\\sum_{j=0}^{k} \\alpha_j y_{n+j} = h \\sum_{j=0}^{k} \\beta_j f_{n+j}\n$$\nwhere $y_n$ is the numerical approximation to the solution $y(t_n)$ at time $t_n = t_0 + nh$, $h$ is the constant step size, and $f_{n+j} = f(t_{n+j}, y_{n+j})$.\n\nThe Adams-Bashforth (AB) methods are an explicit family of LMMs defined by the choices:\n$$\n\\alpha_k = 1, \\quad \\alpha_{k-1} = -1, \\quad \\alpha_j = 0 \\text{ for } j  k-1, \\quad \\text{and} \\quad \\beta_k = 0\n$$\nThe update rule for an AB method is thus $y_{n+k} - y_{n+k-1} = h \\sum_{j=0}^{k-1} \\beta_j f_{n+j}$. This structure arises from integrating the ODE from $t_{n+k-1}$ to $t_{n+k}$:\n$$\ny(t_{n+k}) - y(t_{n+k-1}) = \\int_{t_{n+k-1}}^{t_{n+k}} f(t, y(t)) dt\n$$\nThe integral is approximated by replacing $f(t, y(t))$ with a polynomial $P(t)$ that interpolates the known values $f_n, f_{n+1}, \\dots, f_{n+k-1}$ at past time steps.\n\nFor the AB4 method, we have $k=4$. We construct a polynomial $P_3(t)$ of degree $3$ that passes through the points $(t_n, f_n), (t_{n+1}, f_{n+1}), (t_{n+2}, f_{n+2}), (t_{n+3}, f_{n+3})$. The update rule is:\n$$\ny_{n+4} = y_{n+3} + \\int_{t_{n+3}}^{t_{n+4}} P_3(t) dt\n$$\nUsing Newton's backward difference formula for the interpolating polynomial relative to the point $t_{n+3}$, with a normalized coordinate $s = (t - t_{n+3})/h$, we write:\n$$\nP_3(t_{n+3} + sh) = f_{n+3} + s\\nabla f_{n+3} + \\frac{s(s+1)}{2!} \\nabla^2 f_{n+3} + \\frac{s(s+1)(s+2)}{3!} \\nabla^3 f_{n+3}\n$$\nwhere $\\nabla$ is the backward difference operator. The integral becomes:\n$$\n\\int_{t_{n+3}}^{t_{n+4}} P_3(t) dt = h \\int_0^1 P_3(t_{n+3} + sh) ds = h \\left( \\gamma_0 f_{n+3} + \\gamma_1 \\nabla f_{n+3} + \\gamma_2 \\nabla^2 f_{n+3} + \\gamma_3 \\nabla^3 f_{n+3} \\right)\n$$\nThe coefficients $\\gamma_j$ are given by integrating the polynomial basis functions:\n$\\gamma_0 = \\int_0^1 1 ds = 1$\n$\\gamma_1 = \\int_0^1 s ds = 1/2$\n$\\gamma_2 = \\int_0^1 \\frac{s(s+1)}{2} ds = 5/12$\n$\\gamma_3 = \\int_0^1 \\frac{s(s+1)(s+2)}{6} ds = 3/8$\n\nTo find the coefficients $\\beta_j$ in the standard LMM form, we expand the backward differences in terms of function values:\n$\\nabla f_{n+3} = f_{n+3} - f_{n+2}$\n$\\nabla^2 f_{n+3} = f_{n+3} - 2f_{n+2} + f_{n+1}$\n$\\nabla^3 f_{n+3} = f_{n+3} - 3f_{n+2} + 3f_{n+1} - f_n$\n\nSubstituting these into the integrated expression and collecting terms for each $f_j$:\n\\begin{itemize}\n    \\item $f_{n+3}: h(\\gamma_0 + \\gamma_1 + \\gamma_2 + \\gamma_3) = h(1 + 1/2 + 5/12 + 3/8) = h(55/24)$\n    \\item $f_{n+2}: h(-\\gamma_1 - 2\\gamma_2 - 3\\gamma_3) = h(-1/2 - 10/12 - 9/8) = h(-59/24)$\n    \\item $f_{n+1}: h(\\gamma_2 + 3\\gamma_3) = h(5/12 + 9/8) = h(37/24)$\n    \\item $f_n: h(-\\gamma_3) = h(-3/8) = h(-9/24)$\n\\end{itemize}\nThus, the fourth-order Adams-Bashforth (AB4) method is:\n$$\ny_{n+4} = y_{n+3} + \\frac{h}{24} \\left( 55f_{n+3} - 59f_{n+2} + 37f_{n+1} - 9f_n \\right)\n$$\n\n**2. Analysis for the Homogeneous Equation $y'=0$**\n\nFor the homogeneous ODE $y' = 0$, we have $f(t, y) = 0$, which implies $f_{n+j} = 0$ for all $j$. The AB4 method simplifies to the linear homogeneous difference equation:\n$$\ny_{n+4} - y_{n+3} = 0\n$$\nTo analyze its stability, we examine its characteristic polynomial $\\rho(z)$, which is obtained by substituting the ansatz $y_n = z^n$:\n$$\nz^{n+4} - z^{n+3} = 0 \\implies z^3(z-1) = 0\n$$\nThe roots of the characteristic polynomial are $z_1 = 1$ (with multiplicity $1$) and $z_2 = z_3 = z_4 = 0$ (with multiplicity $3$).\n\nThese roots determine the behavior of the numerical solution.\n- The **principal root** is $z_1 = 1$. It corresponds to the exact solution of $y' = 0$, which is $y(t) = \\text{constant}$. A numerical solution component proportional to $(1)^n$ correctly represents this constant behavior.\n- The **parasitic roots** are $z_2, z_3, z_4 = 0$. They are artifacts of the four-step method. Since their magnitudes are all strictly less than $1$, the method is **strongly stable**. Any components of the numerical solution that arise from these roots will decay to zero, meaning they do not cause instability.\n\n**3. Modeling Perturbation Propagation**\n\nWe model machine-precision error by adding an independent perturbation $\\eta_{n+1} \\sim \\mathcal{N}(0, \\epsilon^2)$ after each step. The computational process for a single step is:\n1.  Compute an intermediate value $y_{n+1}^*$ using the LMM formula.\n2.  Obtain the final value $y_{n+1}$ by adding the perturbation: $y_{n+1} = y_{n+1}^* + \\eta_{n+1}$.\n\nFor the AB4 method applied to $y'=0$ with $\\lambda=0$, the deterministic update is $y_{n+4}^* = y_{n+3}$. The full update with perturbation becomes:\n$$\ny_{n+4} = y_{n+3} + \\eta_{n+4} \\quad \\text{for } n \\ge 3\n$$\nThis is a first-order recurrence. To start the AB4 method, we need four values, $y_0, y_1, y_2, y_3$. We are given $y_0=0$. We generate the required history using lower-order AB methods, applying the same perturbation model:\n- **Step 1 (AB1/Forward Euler)**: $y_1^* = y_0 + hf_0 = 0$. So, $y_1 = 0 + \\eta_1 = \\eta_1$.\n- **Step 2 (AB2)**: $y_2^* = y_1 + \\frac{h}{2}(3f_1 - f_0) = y_1$. So, $y_2 = y_1 + \\eta_2 = \\eta_1 + \\eta_2$.\n- **Step 3 (AB3)**: $y_3^* = y_2 + \\frac{h}{12}(23f_2 - 16f_1 + 5f_0) = y_2$. So, $y_3 = y_2 + \\eta_3 = \\eta_1 + \\eta_2 + \\eta_3$.\n\nBy induction, the recurrence relation $y_{n+1} = y_n + \\eta_{n+1}$ holds for all $n \\ge 0$. This implies that the numerical solution is the cumulative sum of the perturbations:\n$$\ny_n = \\sum_{i=1}^{n} \\eta_i\n$$\nThis process is a discrete-time **random walk**. Each $\\eta_i$ is an independent random variable with mean $0$ and variance $\\epsilon^2$. The value $y_n$ is therefore a random variable with mean $\\mathbb{E}[y_n] = 0$ and variance $\\text{Var}(y_n) = \\sum_{i=1}^n \\text{Var}(\\eta_i) = n\\epsilon^2$.\n\n**4. Algorithmic Design and Metric**\n\nThe algorithm must implement the general AB4 method with bootstrapping, as it must be correct for any $\\lambda$. The simulation proceeds as follows:\n1.  Initialize $y_0=0$ and arrays for $y_n$ and $f_n$.\n2.  Use AB1, AB2, and AB3 for the first three steps ($n=0, 1, 2$) to compute $y_1, y_2, y_3$, adding the respective noise terms $\\eta_1, \\eta_2, \\eta_3$.\n3.  For subsequent steps ($n \\ge 3$), apply the AB4 formula to compute $y_{n+1}$, adding $\\eta_{n+1}$. For the specified test cases with $\\lambda=0$, all $f_j$ terms will be zero.\n4.  After completing $N$ steps, compute the metric:\n$$\nG(N,\\epsilon) = \\frac{\\max_{0\\le n\\le N}\\lvert y_n\\rvert}{\\epsilon\\sqrt{N}}\n$$\nThe normalization factor $\\epsilon\\sqrt{N}$ is the standard deviation of the final point of the random walk, $\\sigma_{y_N} = \\sqrt{\\text{Var}(y_N)} = \\sqrt{N\\epsilon^2}$. This metric quantifies the peak absolute excursion of the random walk, normalized by the characteristic scale of its fluctuations at the endpoint. For large $N$, this value is expected to be a statistical constant, largely independent of $\\epsilon$ and $N$.\n\nThe provided code will execute this simulation for the specified test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_simulation(epsilon, N, seed, lambd, h, y0):\n    \"\"\"\n    Runs the AB4 simulation with additive noise for a given set of parameters.\n\n    Args:\n        epsilon (float): Standard deviation of the additive Gaussian noise.\n        N (int): Total number of steps.\n        seed (int): Seed for the random number generator.\n        lambd (float): Parameter for the ODE y' = lambda*y.\n        h (float): Step size.\n        y0 (float): Initial condition y(0).\n\n    Returns:\n        float: The computed metric G(N, epsilon).\n    \"\"\"\n    # Initialize the random number generator for deterministic results\n    rng = np.random.default_rng(seed)\n\n    # Allocate arrays for the solution y and the function values f\n    # Array size is N+1 to store values from y_0 to y_N\n    y = np.zeros(N + 1)\n    f = np.zeros(N + 1)\n\n    # Set initial condition\n    y[0] = y0\n    f[0] = lambd * y[0]\n    \n    # Handle the edge case of N=0, though a test case for this is not provided.\n    # The max would be over a single element {y_0}.\n    # The problem asks for max over 0 = n = N.\n    if N == 0:\n        if epsilon == 0 and y0 == 0:\n            return 0.0\n        # Denominator would be 0, so the metric is ill-defined.\n        # But all test cases have N = 1.\n        return np.inf\n\n    # Pre-generate all noise terms for efficiency.\n    # We need N noise terms for steps 1 through N.\n    noise = rng.normal(loc=0.0, scale=epsilon, size=N)\n\n    # Bootstrapping steps for the AB4 method history\n    \n    # Step 1: n=0, compute y_1 using AB1 (Forward Euler)\n    if N = 1:\n        y_star = y[0] + h * f[0]\n        y[1] = y_star + noise[0]\n        f[1] = lambd * y[1]\n\n    # Step 2: n=1, compute y_2 using AB2\n    if N = 2:\n        y_star = y[1] + (h / 2.0) * (3.0 * f[1] - 1.0 * f[0])\n        y[2] = y_star + noise[1]\n        f[2] = lambd * y[2]\n\n    # Step 3: n=2, compute y_3 using AB3\n    if N = 3:\n        y_star = y[2] + (h / 12.0) * (23.0 * f[2] - 16.0 * f[1] + 5.0 * f[0])\n        y[3] = y_star + noise[2]\n        f[3] = lambd * y[3]\n\n    # Main loop for n from 3 to N-1, applying AB4\n    for n in range(3, N):\n        y_star = y[n] + (h / 24.0) * (55.0 * f[n] - 59.0 * f[n-1] + 37.0 * f[n-2] - 9.0 * f[n-3])\n        y[n+1] = y_star + noise[n]\n        f[n+1] = lambd * y[n+1]\n\n    # Calculate the metric G(N, epsilon)\n    max_abs_y = np.max(np.abs(y))\n    \n    denominator = epsilon * np.sqrt(N)\n    \n    if denominator == 0.0:\n        # This case happens if epsilon is 0. If y is always 0, G should be 0.\n        return 0.0 if max_abs_y == 0.0 else np.inf\n        \n    G = max_abs_y / denominator\n    return G\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (epsilon, N, seed)\n    test_cases = [\n        (2.220446049250313e-16, 200000, 12345),\n        (1e-12, 200000, 12345),\n        (1e-8, 200000, 12345),\n        (1e-8, 1000, 98765),\n        (1e-8, 1, 42),\n    ]\n\n    # Common parameters for all test cases\n    lambd = 0.0\n    h = 1.0\n    y0 = 0.0\n\n    results = []\n    for epsilon, N, seed in test_cases:\n        result = run_simulation(epsilon, N, seed, lambd, h, y0)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3153656"}]}