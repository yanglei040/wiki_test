{"hands_on_practices": [{"introduction": "The core idea behind many predictor-corrector schemes is to first take an ambitious step towards the objective, and then 'correct' this step to ensure critical constraints are met. This exercise provides a tangible demonstration of this principle in the context of constrained optimization [@problem_id:3163808]. By first performing a simple gradient descent step that ignores the constraint, and then projecting the resulting point back onto the feasible manifold, you will gain a geometric intuition for how these powerful methods balance progress with feasibility.", "problem": "Consider the equality-constrained optimization problem of minimizing the quadratic objective subject to a smooth manifold constraint. Let the objective be given by $f(x) = \\frac{1}{2}\\|x - c\\|^{2}$ with $x \\in \\mathbb{R}^{2}$ and $c = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$, and let the constraint be $h(x) = x_{1}^{2} + x_{2}^{2} - 1 = 0$, which defines the unit circle. Suppose you are at the feasible point $x^{(0)} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$.\n\nA predictor-corrector scheme proceeds by first taking an unconstrained predictor step based on the gradient of the objective, and then correcting to restore feasibility by projecting onto the constraint manifold using a first-order local approximation. Specifically:\n\n1. Use an unconstrained gradient step to form the predictor $x^{\\mathrm{pred}} = x^{(0)} - \\alpha \\nabla f(x^{(0)})$ with step size $\\alpha = \\frac{1}{4}$.\n2. Using the first-order Taylor expansion of the constraint about $x^{\\mathrm{pred}}$, $h(x^{\\mathrm{pred}} + \\delta) \\approx h(x^{\\mathrm{pred}}) + J_{h}(x^{\\mathrm{pred}})\\,\\delta$, determine the minimal-norm correction $\\delta$ that restores feasibility, where $J_{h}(x)$ denotes the Jacobian of $h$ at $x$. Then form the corrected iterate $x^{\\mathrm{corr}} = x^{\\mathrm{pred}} + \\delta$.\n3. Compute the objective value $f(x^{\\mathrm{corr}})$.\n\nExpress your final numerical answer for $f(x^{\\mathrm{corr}})$ exactly as a rational number. Do not round.", "solution": "The user has provided a well-posed optimization problem that requires the application of a single predictor-corrector step. The problem is scientifically grounded, self-contained, and objective. We will proceed with the solution by following the specified steps.\n\nThe objective function is $f(x) = \\frac{1}{2}\\|x - c\\|^{2}$ where $x = \\begin{pmatrix}x_1 \\\\ x_2\\end{pmatrix} \\in \\mathbb{R}^{2}$ and $c = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$. The constraint is $h(x) = x_{1}^{2} + x_{2}^{2} - 1 = 0$. The initial point is $x^{(0)} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$.\n\n**Step 1: Predictor Step**\n\nFirst, we compute the gradient of the objective function, $\\nabla f(x)$.\nThe objective function can be written as $f(x) = \\frac{1}{2}((x_1 - 1)^2 + (x_2 - 0)^2)$.\nThe gradient is given by:\n$$ \\nabla f(x) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\end{pmatrix} = \\begin{pmatrix} x_1 - 1 \\\\ x_2 \\end{pmatrix} = x - c $$\nWe evaluate the gradient at the initial point $x^{(0)} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$:\n$$ \\nabla f(x^{(0)}) = x^{(0)} - c = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix} - \\begin{pmatrix}1 \\\\ 0\\end{pmatrix} = \\begin{pmatrix}-1 \\\\ 1\\end{pmatrix} $$\nThe predictor step is an unconstrained gradient step with step size $\\alpha = \\frac{1}{4}$:\n$$ x^{\\mathrm{pred}} = x^{(0)} - \\alpha \\nabla f(x^{(0)}) $$\nSubstituting the values:\n$$ x^{\\mathrm{pred}} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix} - \\frac{1}{4} \\begin{pmatrix}-1 \\\\ 1\\end{pmatrix} = \\begin{pmatrix}0 + \\frac{1}{4} \\\\ 1 - \\frac{1}{4}\\end{pmatrix} = \\begin{pmatrix}\\frac{1}{4} \\\\ \\frac{3}{4}\\end{pmatrix} $$\n\n**Step 2: Corrector Step**\n\nThe corrector step aims to restore feasibility by moving from $x^{\\mathrm{pred}}$ by a minimal-norm correction vector $\\delta$ such that the first-order approximation of the constraint is satisfied.\nThe linearized constraint equation is $h(x^{\\mathrm{pred}}) + J_{h}(x^{\\mathrm{pred}})\\,\\delta = 0$.\n\nFirst, we evaluate the constraint function at $x^{\\mathrm{pred}}$:\n$$ h(x^{\\mathrm{pred}}) = \\left(\\frac{1}{4}\\right)^{2} + \\left(\\frac{3}{4}\\right)^{2} - 1 = \\frac{1}{16} + \\frac{9}{16} - 1 = \\frac{10}{16} - 1 = \\frac{5}{8} - \\frac{8}{8} = -\\frac{3}{8} $$\nNext, we compute the Jacobian of the constraint function $h(x)$. The Jacobian is a $1 \\times 2$ matrix:\n$$ J_h(x) = \\begin{pmatrix} \\frac{\\partial h}{\\partial x_1} & \\frac{\\partial h}{\\partial x_2} \\end{pmatrix} = \\begin{pmatrix} 2x_1 & 2x_2 \\end{pmatrix} $$\nWe evaluate the Jacobian at $x^{\\mathrm{pred}}$:\n$$ J_h(x^{\\mathrm{pred}}) = \\begin{pmatrix} 2\\left(\\frac{1}{4}\\right) & 2\\left(\\frac{3}{4}\\right) \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} & \\frac{3}{2} \\end{pmatrix} $$\nThe minimal-norm correction $\\delta$ that solves the linear system $J_h(x^{\\mathrm{pred}})\\,\\delta = -h(x^{\\mathrm{pred}})$ is given by the formula:\n$$ \\delta = J_h(x^{\\mathrm{pred}})^T \\left( J_h(x^{\\mathrm{pred}}) J_h(x^{\\mathrm{pred}})^T \\right)^{-1} (-h(x^{\\mathrm{pred}})) $$\nLet $J = J_h(x^{\\mathrm{pred}})$. We compute the term $JJ^T$:\n$$ JJ^T = \\begin{pmatrix} \\frac{1}{2} & \\frac{3}{2} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{3}{2} \\end{pmatrix} = \\left(\\frac{1}{2}\\right)^2 + \\left(\\frac{3}{2}\\right)^2 = \\frac{1}{4} + \\frac{9}{4} = \\frac{10}{4} = \\frac{5}{2} $$\nThe inverse is $(JJ^T)^{-1} = \\frac{2}{5}$.\nNow we can calculate $\\delta$:\n$$ \\delta = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{3}{2} \\end{pmatrix} \\left( \\frac{2}{5} \\right) \\left( -(-\\frac{3}{8}) \\right) = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{3}{2} \\end{pmatrix} \\left( \\frac{2}{5} \\cdot \\frac{3}{8} \\right) = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{3}{2} \\end{pmatrix} \\left( \\frac{6}{40} \\right) = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{3}{2} \\end{pmatrix} \\left( \\frac{3}{20} \\right) $$\n$$ \\delta = \\begin{pmatrix} \\frac{3}{40} \\\\ \\frac{9}{40} \\end{pmatrix} $$\nThe corrected iterate $x^{\\mathrm{corr}}$ is found by adding $\\delta$ to $x^{\\mathrm{pred}}$:\n$$ x^{\\mathrm{corr}} = x^{\\mathrm{pred}} + \\delta = \\begin{pmatrix} \\frac{1}{4} \\\\ \\frac{3}{4} \\end{pmatrix} + \\begin{pmatrix} \\frac{3}{40} \\\\ \\frac{9}{40} \\end{pmatrix} $$\nTo perform the addition, we use a common denominator of $40$:\n$$ x^{\\mathrm{corr}} = \\begin{pmatrix} \\frac{10}{40} + \\frac{3}{40} \\\\ \\frac{30}{40} + \\frac{9}{40} \\end{pmatrix} = \\begin{pmatrix} \\frac{13}{40} \\\\ \\frac{39}{40} \\end{pmatrix} $$\n\n**Step 3: Compute the Objective Value**\n\nFinally, we compute the objective value at the corrected point $x^{\\mathrm{corr}}$.\n$$ f(x^{\\mathrm{corr}}) = \\frac{1}{2} \\|x^{\\mathrm{corr}} - c\\|^2 $$\nFirst, calculate the vector difference $x^{\\mathrm{corr}} - c$:\n$$ x^{\\mathrm{corr}} - c = \\begin{pmatrix} \\frac{13}{40} \\\\ \\frac{39}{40} \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{13}{40} - \\frac{40}{40} \\\\ \\frac{39}{40} - 0 \\end{pmatrix} = \\begin{pmatrix} -\\frac{27}{40} \\\\ \\frac{39}{40} \\end{pmatrix} $$\nNext, compute the squared Euclidean norm of this vector:\n$$ \\|x^{\\mathrm{corr}} - c\\|^2 = \\left(-\\frac{27}{40}\\right)^2 + \\left(\\frac{39}{40}\\right)^2 = \\frac{27^2}{40^2} + \\frac{39^2}{40^2} = \\frac{729}{1600} + \\frac{1521}{1600} = \\frac{729 + 1521}{1600} = \\frac{2250}{1600} $$\nThe objective value is half of this quantity:\n$$ f(x^{\\mathrm{corr}}) = \\frac{1}{2} \\left( \\frac{2250}{1600} \\right) = \\frac{2250}{3200} $$\nWe simplify the fraction by dividing the numerator and denominator by their greatest common divisor.\n$$ f(x^{\\mathrm{corr}}) = \\frac{225}{320} = \\frac{45}{64} $$\nThe final answer is required as an exact rational number.", "answer": "$$ \\boxed{\\frac{45}{64}} $$", "id": "3163808"}, {"introduction": "Predictor-corrector schemes are not limited to handling explicit constraints; they are also fundamental to managing the dynamics of an iterative process. This practice explores a momentum-based optimization method, where the 'prediction' incorporates past movement to accelerate convergence [@problem_id:3163746]. Your task is to analyze the stability of this scheme, determining the conditions under which the corrective update avoids overshooting the minimum, which provides crucial insight into the interplay between step size, momentum, and algorithm stability.", "problem": "Consider the momentum-based predictor-corrector scheme for minimizing a smooth function $f(x)$, defined by the predictor step $v_{k+1}=\\beta v_{k}-\\alpha \\nabla f(x_{k})$ and the corrector step $x_{k+1}=x_{k}+v_{k+1}$, where $\\alpha>0$ is the step size and $\\beta \\geq 0$ is the momentum coefficient. Assume a strictly convex quadratic objective $f(x)=\\frac{1}{2}\\lambda x^{2}$ with curvature parameter $\\lambda>0$, so that the unique minimizer is at $x^{\\star}=0$ and the gradient is $\\nabla f(x)=\\lambda x$. Start from $x_{0}>0$ and $v_{0}=0$. Define an overshoot at a correction step to mean that the iterate crosses the minimizer, i.e., $x_{k+1}<0$ when $x_{k}>0$. Using only the fundamental properties of quadratic objectives and the stated update rules, derive the largest step size $\\alpha^{\\star}(\\beta,\\lambda)$ such that neither the first correction $x_{1}$ nor the second correction $x_{2}$ overshoots (that is, $x_{1}\\geq 0$ and $x_{2}\\geq 0$). Express your final answer as a closed-form analytic expression in terms of $\\beta$ and $\\lambda$. No rounding is required.", "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and complete.\n\n### Step 1: Extract Givens\n-   **Predictor-Corrector Scheme**: A momentum-based method for minimizing a function $f(x)$.\n-   **Predictor Step**: $v_{k+1}=\\beta v_{k}-\\alpha \\nabla f(x_{k})$.\n-   **Corrector Step**: $x_{k+1}=x_{k}+v_{k+1}$.\n-   **Step Size**: $\\alpha>0$.\n-   **Momentum Coefficient**: $\\beta \\geq 0$.\n-   **Objective Function**: A strictly convex quadratic, $f(x)=\\frac{1}{2}\\lambda x^{2}$.\n-   **Curvature Parameter**: $\\lambda>0$.\n-   **Unique Minimizer**: $x^{\\star}=0$.\n-   **Gradient**: $\\nabla f(x)=\\lambda x$.\n-   **Initial Conditions**: $x_{0}>0$ and $v_{0}=0$.\n-   **Overshoot Definition**: An iterate crosses the minimizer, i.e., $x_{k+1}<0$ when $x_{k}>0$.\n-   **Constraint**: Find the largest step size $\\alpha^{\\star}(\\beta, \\lambda)$ such that neither the first correction ($x_1$) nor the second correction ($x_2$) overshoots. This translates to the conditions $x_{1}\\geq 0$ and $x_{2}\\geq 0$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem describes a momentum-based optimization method (a variant of the Heavy Ball method) applied to a simple, strongly convex quadratic function. This is a standard and fundamental analysis in the field of numerical optimization. All terms are well-defined, and the initial conditions and constraints are clearly stated. The problem is self-contained, mathematically consistent, and objective. It is scientifically sound and well-posed, asking for the maximum parameter value that satisfies a set of derived inequalities.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be derived.\n\n### Solution Derivation\n\nThe update rules for the given predictor-corrector scheme are:\n$$v_{k+1} = \\beta v_{k} - \\alpha \\nabla f(x_{k})$$\n$$x_{k+1} = x_{k} + v_{k+1}$$\n\nFor the specified objective function $f(x)=\\frac{1}{2}\\lambda x^{2}$, the gradient is $\\nabla f(x) = \\lambda x$. Substituting this into the predictor step gives:\n$$v_{k+1} = \\beta v_{k} - \\alpha \\lambda x_{k}$$\n\nWe can express the state update as a coupled linear recurrence. We will calculate the first two iterates, $x_1$ and $x_2$, starting from the initial conditions $x_0 > 0$ and $v_0 = 0$.\n\n**First Correction Step ($k=0$):**\nWe compute $v_1$ and then $x_1$.\n$$v_1 = \\beta v_0 - \\alpha \\lambda x_0 = \\beta(0) - \\alpha \\lambda x_0 = -\\alpha \\lambda x_0$$\n$$x_1 = x_0 + v_1 = x_0 - \\alpha \\lambda x_0 = (1 - \\alpha \\lambda) x_0$$\n\nThe first non-overshoot condition is $x_1 \\geq 0$. Since we are given $x_0 > 0$, this requires:\n$$1 - \\alpha \\lambda \\geq 0$$\n$$\\alpha \\lambda \\leq 1$$\nSince $\\lambda > 0$, this gives our first constraint on the step size $\\alpha$:\n$$\\alpha \\leq \\frac{1}{\\lambda}$$\n\n**Second Correction Step ($k=1$):**\nNext, we compute $v_2$ and then $x_2$, using the values of $x_1$ and $v_1$ we just found.\n$$v_2 = \\beta v_1 - \\alpha \\lambda x_1 = \\beta(-\\alpha \\lambda x_0) - \\alpha \\lambda ((1 - \\alpha \\lambda) x_0)$$\n$$v_2 = [-\\alpha \\lambda \\beta - \\alpha \\lambda (1 - \\alpha \\lambda)] x_0$$\n$$v_2 = [-\\alpha \\lambda \\beta - \\alpha \\lambda + (\\alpha \\lambda)^2] x_0$$\n$$v_2 = [(\\alpha \\lambda)^2 - \\alpha \\lambda (\\beta+1)] x_0$$\n\nNow, we compute $x_2$:\n$$x_2 = x_1 + v_2 = (1 - \\alpha \\lambda) x_0 + [(\\alpha \\lambda)^2 - \\alpha \\lambda (\\beta+1)] x_0$$\n$$x_2 = [1 - \\alpha \\lambda + (\\alpha \\lambda)^2 - \\alpha \\lambda \\beta - \\alpha \\lambda] x_0$$\n$$x_2 = [(\\alpha \\lambda)^2 - (2+\\beta) \\alpha \\lambda + 1] x_0$$\n\nThe second non-overshoot condition is $x_2 \\geq 0$. Since $x_0 > 0$, this requires the polynomial in $\\alpha\\lambda$ to be non-negative:\n$$(\\alpha \\lambda)^2 - (2+\\beta) \\alpha \\lambda + 1 \\geq 0$$\n\nLet's analyze this quadratic inequality by defining a variable $u = \\alpha \\lambda$. The inequality becomes:\n$$u^2 - (2+\\beta) u + 1 \\geq 0$$\n\nThis is a quadratic in $u$ representing an upward-opening parabola. The inequality is satisfied when $u$ is outside the roots of the corresponding equation $u^2 - (2+\\beta) u + 1 = 0$. We find the roots using the quadratic formula:\n$$u = \\frac{-(-(2+\\beta)) \\pm \\sqrt{(-(2+\\beta))^2 - 4(1)(1)}}{2(1)}$$\n$$u = \\frac{2+\\beta \\pm \\sqrt{(2+\\beta)^2 - 4}}{2}$$\n$$u = \\frac{2+\\beta \\pm \\sqrt{4 + 4\\beta + \\beta^2 - 4}}{2}$$\n$$u = \\frac{2+\\beta \\pm \\sqrt{\\beta^2 + 4\\beta}}{2}$$\n\nLet the two roots be $u_1$ and $u_2$, with $u_1 \\leq u_2$:\n$$u_1 = \\frac{2+\\beta - \\sqrt{\\beta^2 + 4\\beta}}{2}$$\n$$u_2 = \\frac{2+\\beta + \\sqrt{\\beta^2 + 4\\beta}}{2}$$\nThe inequality $u^2 - (2+\\beta) u + 1 \\geq 0$ holds if $u \\leq u_1$ or $u \\geq u_2$.\n\n**Combining the Constraints:**\nWe need to find the largest $\\alpha > 0$ that satisfies both non-overshoot conditions. In terms of $u = \\alpha \\lambda$:\n1.  From $x_1 \\geq 0$: $u \\leq 1$.\n2.  From $x_2 \\geq 0$: $u \\leq u_1$ or $u \\geq u_2$.\n\nWe need to satisfy both conditions simultaneously. Let's compare the bounds.\nFirst, we check if $u_1 \\leq 1$:\n$$\\frac{2+\\beta - \\sqrt{\\beta^2 + 4\\beta}}{2} \\leq 1$$\n$$2+\\beta - \\sqrt{\\beta^2 + 4\\beta} \\leq 2$$\n$$\\beta \\leq \\sqrt{\\beta^2 + 4\\beta}$$\nSince $\\beta \\geq 0$, both sides are non-negative, so we can square both sides:\n$$\\beta^2 \\leq \\beta^2 + 4\\beta$$\n$$0 \\leq 4\\beta$$\nThis is true for all $\\beta \\geq 0$. Thus, $u_1 \\leq 1$.\n\nSecond, we check if $u_2 \\geq 1$:\n$$\\frac{2+\\beta + \\sqrt{\\beta^2 + 4\\beta}}{2} \\geq 1$$\n$$2+\\beta + \\sqrt{\\beta^2 + 4\\beta} \\geq 2$$\n$$\\beta + \\sqrt{\\beta^2 + 4\\beta} \\geq 0$$\nThis is also true for all $\\beta \\geq 0$, as both terms are non-negative. Thus, $u_2 \\geq 1$.\n\nWe are looking for the largest $u$ such that ($u \\leq 1$) AND ($u \\leq u_1$ or $u \\geq u_2$).\nSince $u_1 \\leq 1 \\leq u_2$, the intersection of these conditions is $u \\leq u_1$. The region $u \\geq u_2$ is incompatible with $u \\leq 1$ (except for the single point $u=1$ when $u_2=1$, which occurs when $\\beta=0$).\nThe combined condition for any $\\beta \\geq 0$ simplifies to:\n$$u \\leq u_1$$\n\nTo find the largest permissible step size $\\alpha^{\\star}$, we take the largest possible value for $u = \\alpha \\lambda$, which is $u_1$:\n$$\\alpha^{\\star} \\lambda = u_1 = \\frac{2+\\beta - \\sqrt{\\beta^2 + 4\\beta}}{2}$$\n\nSolving for $\\alpha^{\\star}$ gives the final expression:\n$$\\alpha^{\\star}(\\beta, \\lambda) = \\frac{2+\\beta - \\sqrt{\\beta^2 + 4\\beta}}{2\\lambda}$$\nThis can be written as:\n$$\\alpha^{\\star}(\\beta, \\lambda) = \\frac{2+\\beta - \\sqrt{\\beta(\\beta+4)}}{2\\lambda}$$\nThis is the largest step size that guarantees neither the first nor the second correction step overshoots the minimizer at $x^{\\star}=0$.", "answer": "$$\n\\boxed{\\frac{2+\\beta - \\sqrt{\\beta(\\beta+4)}}{2\\lambda}}\n$$", "id": "3163746"}, {"introduction": "In sophisticated algorithms like Interior-Point Methods, the predictor-corrector pattern is essential for navigating complex boundaries. The 'predictor' step calculates a promising search direction, but a full step might violate feasibility constraints. This exercise focuses on the 'corrector' phase: implementing the 'fraction-to-the-boundary' rule, a vital safeguard that shortens the step to maintain strict feasibility [@problem_id:3163783]. Mastering this concept is key to understanding how practical optimization solvers robustly handle bound constraints.", "problem": "Consider a bound-constrained convex optimization problem with simple box constraints, where the variable vector $x \\in \\mathbb{R}^{n}$ is required to satisfy $0 < x < u$ componentwise, and the method uses a logarithmic barrier of the form $\\phi(x) = f(x) - \\mu \\sum_{i=1}^{n} \\left( \\ln(x_{i}) + \\ln(u_{i} - x_{i}) \\right)$ for some barrier parameter $\\mu > 0$. In the Interior-Point Method (IPM), a Mehrotra-style predictor-corrector step proposes a tentative displacement $d \\in \\mathbb{R}^{n}$ from the current strictly feasible point $x$, but the predictor may heavily violate the bounds if taken with unit step length. To preserve strict feasibility of both the primal variables $x$ and the associated upper-bound slacks $s = u - x$ at the next iterate $x^{+} = x + \\alpha d$ and $s^{+} = s - \\alpha d$, the acceptance step length $\\alpha$ must be chosen to keep $x^{+} > 0$ and $s^{+} > 0$ componentwise.\n\nStarting from the fundamental requirement that the logarithmic barrier is only defined on the interior, namely $x_{i} > 0$ and $s_{i} = u_{i} - x_{i} > 0$ for all $i$, derive the largest step length $\\alpha_{\\max}$ that maintains strict positivity for both $x^{+}$ and $s^{+}$ along the given direction $d$. Then, apply the fraction-to-the-boundary safeguard with parameter $\\tau \\in (0,1)$ to obtain a corrected step length $\\alpha_{\\text{safe}}$ suitable for use in the predictor-corrector scheme.\n\nUse the following data for a three-dimensional instance:\n- $x = (4,\\, 1.5,\\, 6)$,\n- $u = (5,\\, 2,\\, 7)$,\n- $d = (-3,\\, -1,\\, 2)$,\n- $\\tau = 0.8$.\n\nCompute the safeguarded step length $\\alpha_{\\text{safe}}$. Provide your final answer as a single real number. If any rounding were needed, you would round to four significant figures; however, if an exact value is obtained, report it exactly.", "solution": "The problem is first validated to ensure it is scientifically sound, well-posed, and objective.\n\n### Step 1: Extract Givens\n- **Problem Context**: A bound-constrained convex optimization problem.\n- **Constraints**: $0 < x < u$ componentwise, for $x \\in \\mathbb{R}^{n}$.\n- **Barrier Function**: $\\phi(x) = f(x) - \\mu \\sum_{i=1}^{n} \\left( \\ln(x_{i}) + \\ln(u_{i} - x_{i}) \\right)$ for $\\mu > 0$.\n- **Method**: Interior-Point Method (IPM) with a predictor-corrector step.\n- **Current Primal Iterate**: A strictly feasible point $x$.\n- **Search Direction**: $d \\in \\mathbb{R}^{n}$.\n- **Upper-Bound Slacks**: $s = u - x$.\n- **Next Iterates**: $x^{+} = x + \\alpha d$ and $s^{+} = s - \\alpha d$.\n- **Feasibility Requirement**: $x^{+} > 0$ and $s^{+} > 0$, componentwise.\n- **Step Lengths**: $\\alpha_{\\max}$ is the largest step length maintaining strict positivity, and $\\alpha_{\\text{safe}}$ is a safeguarded step.\n- **Safeguard Rule**: $\\alpha_{\\text{safe}} = \\tau \\alpha_{\\max}$ with $\\tau \\in (0,1)$.\n- **Instance Data ($n=3$):**\n  - $x = (4,\\, 1.5,\\, 6)$\n  - $u = (5,\\, 2,\\, 7)$\n  - $d = (-3,\\, -1,\\, 2)$\n  - $\\tau = 0.8$\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is scientifically grounded, describing a standard \"fraction-to-the-boundary\" rule used in Interior-Point Methods for optimization. It is well-posed, providing all necessary data to compute a unique solution. The data is consistent; the initial point $x$ is strictly feasible as required, since $0 < 4 < 5$, $0 < 1.5 < 2$, and $0 < 6 < 7$. There are no ambiguities, contradictions, or violations of scientific principles.\n\n### Step 3: Verdict and Action\nThe problem is valid. A solution will be derived.\n\n### Solution Derivation\n\nThe goal is to compute the safeguarded step length $\\alpha_{\\text{safe}}$. This requires first determining $\\alpha_{\\max}$, the maximum step length $\\alpha > 0$ that maintains strict feasibility for the next iterate. Strict feasibility means that for every component $i \\in \\{1, 2, \\dots, n\\}$, the conditions $x_i^{+} > 0$ and $s_i^{+} > 0$ must hold.\n\nThe next primal iterate is $x^{+} = x + \\alpha d$. The condition $x^{+} > 0$ implies $x_i + \\alpha d_i > 0$ for each component $i$.\n- If $d_i \\ge 0$, since $x_i > 0$ (by initial feasibility) and we seek $\\alpha > 0$, this inequality $x_i + \\alpha d_i > 0$ is always satisfied and imposes no upper bound on $\\alpha$.\n- If $d_i < 0$, the inequality can be rearranged to $\\alpha (-d_i) < x_i$. Since $-d_i > 0$, we can divide to obtain an upper bound on the step length: $\\alpha < \\frac{x_i}{-d_i}$.\n\nThe next slack iterate is $s^{+} = s - \\alpha d$, where $s = u - x$. The condition $s^{+} > 0$ implies $s_i - \\alpha d_i > 0$ for each component $i$.\n- If $d_i \\le 0$, since $s_i > 0$ (by initial feasibility) and $\\alpha > 0$, the term $-\\alpha d_i$ is non-negative, so the inequality $s_i - \\alpha d_i > 0$ is always satisfied and imposes no upper bound on $\\alpha$.\n- If $d_i > 0$, the inequality can be rearranged to $\\alpha d_i < s_i$. This gives an upper bound on the step length: $\\alpha < \\frac{s_i}{d_i}$.\n\nTo satisfy all these conditions simultaneously for all $i$, $\\alpha$ must be smaller than the minimum of all derived upper bounds. The term $\\alpha_{\\max}$ in this context refers to the step length that takes one component of either $x^{+}$ or $s^{+}$ to its boundary value of $0$. Therefore, $\\alpha_{\\max}$ is the minimum of all the limiting step ratios.\n\n$$\n\\alpha_{\\max} = \\min \\left( \\min_{i: d_i < 0} \\left\\{ \\frac{x_i}{-d_i} \\right\\}, \\min_{i: d_i > 0} \\left\\{ \\frac{s_i}{d_i} \\right\\} \\right)\n$$\n\nNow, we apply this formula to the given three-dimensional data:\n- $x = (4, 1.5, 6)$, so $x_1=4$, $x_2=1.5$, $x_3=6$.\n- $u = (5, 2, 7)$, so $u_1=5$, $u_2=2$, $u_3=7$.\n- $d = (-3, -1, 2)$, so $d_1=-3$, $d_2=-1$, $d_3=2$.\n\nFirst, we compute the slack variables $s = u - x$:\n- $s_1 = u_1 - x_1 = 5 - 4 = 1$.\n- $s_2 = u_2 - x_2 = 2 - 1.5 = 0.5$.\n- $s_3 = u_3 - x_3 = 7 - 6 = 1$.\n\nNext, we calculate the step limits for each component that provides a bound.\nThe set of indices where $d_i < 0$ is $\\{1, 2\\}$.\n- For $i=1$: The limit is $\\frac{x_1}{-d_1} = \\frac{4}{-(-3)} = \\frac{4}{3}$.\n- For $i=2$: The limit is $\\frac{x_2}{-d_2} = \\frac{1.5}{-(-1)} = 1.5 = \\frac{3}{2}$.\n\nThe set of indices where $d_i > 0$ is $\\{3\\}$.\n- For $i=3$: The limit is $\\frac{s_3}{d_3} = \\frac{1}{2}$.\n\nNow, we find $\\alpha_{\\max}$ by taking the minimum of these computed limits:\n$$\n\\alpha_{\\max} = \\min\\left(\\frac{4}{3}, \\frac{3}{2}, \\frac{1}{2}\\right)\n$$\nComparing the values: $\\frac{4}{3} \\approx 1.333\\dots$, $\\frac{3}{2} = 1.5$, and $\\frac{1}{2} = 0.5$. The minimum is $\\frac{1}{2}$.\n$$\n\\alpha_{\\max} = 0.5\n$$\n\nFinally, we apply the fraction-to-the-boundary safeguard using the given parameter $\\tau = 0.8$:\n$$\n\\alpha_{\\text{safe}} = \\tau \\alpha_{\\max}\n$$\nSubstituting the values:\n$$\n\\alpha_{\\text{safe}} = 0.8 \\times 0.5 = 0.4\n$$\nThe result is an exact decimal value.", "answer": "$$\\boxed{0.4}$$", "id": "3163783"}]}