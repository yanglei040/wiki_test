{"hands_on_practices": [{"introduction": "The best way to understand a new numerical method is to apply it by hand. This first exercise provides a direct, concrete application of Heun's method for a single time step. By calculating the result and comparing it to the midpoint method, another second-order scheme, you will gain a clear grasp of the predictor-corrector sequence and how different methods can yield different approximations even when they share the same order of accuracy [@problem_id:2200959].", "problem": "Consider the initial value problem (IVP) given by the ordinary differential equation $y'(t) = ty$ with the initial condition $y(1) = 2$.\n\nWe wish to approximate the value of $y(1.2)$ using a single step of size $h = 0.2$. Two different second-order Runge-Kutta methods will be used: the midpoint method and Heun's method (also known as the improved Euler method or the explicit trapezoidal rule).\n\nLet $y_M$ be the numerical approximation of $y(1.2)$ obtained using the midpoint method, and let $y_H$ be the numerical approximation of $y(1.2)$ obtained using Heun's method.\n\nCalculate the absolute difference $|y_H - y_M|$. Provide the exact numerical value of this difference.", "solution": "We have the IVP $y'(t)=f(t,y)=ty$ with $y(1)=2$, step size $h=\\frac{1}{5}$, and we seek approximations at $t_{1}=1+h=\\frac{6}{5}$.\n\nFor the midpoint (explicit midpoint) method, the update is\n$$\ny_{n+1}=y_{n}+h\\,f\\!\\left(t_{n}+\\frac{h}{2},\\,y_{n}+\\frac{h}{2}f(t_{n},y_{n})\\right).\n$$\nWith $t_{0}=1$, $y_{0}=2$:\n- $k_{1}=f(1,2)=1\\cdot 2=2$.\n- Midpoint arguments: $t_{0}+\\frac{h}{2}=1+\\frac{1}{10}=\\frac{11}{10}$ and $y_{0}+\\frac{h}{2}k_{1}=2+\\frac{1}{10}\\cdot 2=\\frac{11}{5}$.\n- $k_{2}=f\\!\\left(\\frac{11}{10},\\frac{11}{5}\\right)=\\frac{11}{10}\\cdot\\frac{11}{5}=\\frac{121}{50}$.\nThus\n$$\ny_{M}=y_{1}=2+h\\,k_{2}=2+\\frac{1}{5}\\cdot\\frac{121}{50}=2+\\frac{121}{250}=\\frac{621}{250}.\n$$\n\nFor Heun’s method (explicit trapezoidal rule), the update is\n$$\ny_{n+1}=y_{n}+\\frac{h}{2}\\left(f(t_{n},y_{n})+f\\!\\left(t_{n}+h,\\,y_{n}+h f(t_{n},y_{n})\\right)\\right).\n$$\nWith $t_{0}=1$, $y_{0}=2$:\n- $k_{1}=f(1,2)=2$.\n- Predictor at the end: $t_{0}+h=1+\\frac{1}{5}=\\frac{6}{5}$ and $y_{0}+h k_{1}=2+\\frac{1}{5}\\cdot 2=\\frac{12}{5}$.\n- $k_{2}=f\\!\\left(\\frac{6}{5},\\frac{12}{5}\\right)=\\frac{6}{5}\\cdot\\frac{12}{5}=\\frac{72}{25}$.\nThus\n$$\ny_{H}=y_{1}=2+\\frac{h}{2}(k_{1}+k_{2})=2+\\frac{1}{10}\\left(2+\\frac{72}{25}\\right)\n=2+\\frac{1}{10}\\cdot\\frac{122}{25}=2+\\frac{122}{250}=\\frac{311}{125}.\n$$\n\nCompute the absolute difference:\n$$\n|y_{H}-y_{M}|=\\left|\\frac{311}{125}-\\frac{621}{250}\\right|=\\left|\\frac{622}{250}-\\frac{621}{250}\\right|=\\frac{1}{250}.\n$$", "answer": "$$\\boxed{\\frac{1}{250}}$$", "id": "2200959"}, {"introduction": "A crucial characteristic of any numerical integrator is its order of accuracy, which describes how the error decreases as the step size $h$ gets smaller. For Heun's method, the global error is theoretically proven to be second-order, or $\\mathcal{O}(h^2)$. This computational exercise guides you through the process of numerically verifying this theoretical result by measuring the global error for a range of step sizes and analyzing the data on a log-log scale [@problem_id:3259641].", "problem": "Consider the initial value problem (IVP) $y'(t) = f(t,y(t))$ with $y(t_0) = y_0$ where $f$ is continuously differentiable with respect to both arguments on a closed interval $[t_0,T]$. The global discretization error at time $T$ for a one-step method with uniform step size $h$ and $N = (T - t_0)/h$ steps is defined as $E(h) = \\lvert y_N - y(T) \\rvert$, where $y_N$ is the numerical approximation to $y(T)$ produced by the method.\n\nHeun's method (also known as the explicit trapezoidal method) is the one-step method obtained by applying the trapezoidal quadrature rule to the integral representation $y(t_{n+1}) = y(t_n) + \\int_{t_n}^{t_{n+1}} f(s,y(s)) \\, ds$ and approximating the second function evaluation using a forward Euler predictor. The resulting step from $(t_n,y_n)$ to $(t_{n+1},y_{n+1})$ with uniform step size $h$ is\n$$\ny_{n+1} = y_n + \\frac{h}{2}\\left(f(t_n,y_n) + f\\!\\left(t_{n+1}, y_n + h\\,f(t_n,y_n)\\right)\\right).\n$$\nAssume $f$ and the exact solution $y$ are smooth enough so that the local truncation error of Heun's method is well-defined.\n\nYour task is to numerically demonstrate, across a small test suite of smooth problems, that the global error $E(h)$ of Heun's method at the final time $T$ scales like $\\mathcal{O}(h^2)$. To do this, for each test case below:\n- Implement Heun's method on $[t_0,T]$ with uniform step sizes $h = (T - t_0)/N$ for a list of $N$ values.\n- Compute the global error $E(h)$ at $T$ using the known exact solution $y(T)$.\n- Estimate the observed order $p$ as the slope of the best-fit line in the least-squares sense for the pairs $\\big(\\log(h), \\log(E(h))\\big)$ over the provided set of step sizes. Concretely, if $(x_i,y_i) = \\big(\\log(h_i),\\log(E(h_i))\\big)$, find the line $y \\approx \\alpha + p\\,x$ that minimizes $\\sum_i (y_i - \\alpha - p\\,x_i)^2$ and return the slope $p$.\n\nUse the following test suite. When trigonometric functions appear, interpret all angles in radians.\n\n- Test Case $1$ (linear, exponentially decaying): $f(t,y) = -y$, $t_0 = 0$, $y_0 = 1$, $T = 1$, exact solution $y(t) = e^{-t}$, step counts $N \\in \\{10, 20, 40, 80\\}$.\n- Test Case $2$ (multiplicative oscillation): $f(t,y) = y\\cos(t)$, $t_0 = 0$, $y_0 = 1$, $T = \\pi$, exact solution $y(t) = \\exp(\\sin(t))$, step counts $N \\in \\{20, 40, 80, 160\\}$.\n- Test Case $3$ (nonlinear growth on a short interval): $f(t,y) = y^2$, $t_0 = 0$, $y_0 = 1$, $T = 0.5$, exact solution $y(t) = \\frac{1}{1 - t}$, step counts $N \\in \\{20, 40, 80, 160\\}$.\n- Test Case $4$ (pure forcing): $f(t,y) = \\sin(t)$, $t_0 = 0$, $y_0 = 0$, $T = 2$, exact solution $y(t) = 1 - \\cos(t)$, step counts $N \\in \\{20, 40, 80, 160\\}$.\n\nYour program must:\n- Implement Heun's method as described.\n- For each test case, compute the list of $(h, E(h))$ values using the specified $N$ values, estimate the slope $p$ as described, and collect the four slopes.\n- Produce a single line of output containing these four slopes as a comma-separated list enclosed in square brackets, for example, $[p_1,p_2,p_3,p_4]$. Each $p_i$ must be a floating-point number.\n\nThere are no physical units involved. All trigonometric evaluations must use angles in radians. The program must be self-contained and must not require any input. The final outputs are the four floating-point slopes corresponding to the four test cases.", "solution": "The problem requires a numerical demonstration of the convergence order of Heun's method for several initial value problems (IVPs). The theoretical global error for a one-step method of order $p$, at a fixed time $T$ and using a step size $h$, is expected to behave as $E(h) \\approx C h^p$ for some constant $C$ as $h \\to 0$. Heun's method is known to be a second-order method, so we expect to find $p \\approx 2$.\n\nTo verify this numerically, we can analyze the relationship between the error $E(h)$ and the step size $h$. Taking the natural logarithm of the error expression gives:\n$$ \\log(E(h)) \\approx \\log(C) + p \\log(h) $$\nThis equation has the form of a line, $y = \\alpha + p x$, where $y = \\log(E(h))$, $x = \\log(h)$, and the intercept is $\\alpha = \\log(C)$. The slope of this line is the order of convergence, $p$. The problem specifies that we must estimate this slope $p$ by performing a linear least-squares regression on the set of data points $(\\log(h_i), \\log(E(h_i)))$ generated from a sequence of step sizes $h_i$.\n\nThe overall algorithm proceeds as follows for each test case:\n1.  Define the IVP parameters: the function $f(t,y)$, initial time $t_0$, initial value $y_0$, final time $T$, and the exact solution $y(t)$.\n2.  For a given list of step counts $\\{N_1, N_2, \\dots, N_m\\}$:\n    a. For each $N_i$, calculate the step size $h_i = (T - t_0) / N_i$.\n    b. Solve the IVP from $t_0$ to $T$ using Heun's method with $N_i$ steps to obtain the numerical approximation $y_{N_i}$. The iteration formula for Heun's method is:\n    $$ y_{n+1} = y_n + \\frac{h}{2}\\left(f(t_n,y_n) + f\\!\\left(t_{n+1}, y_n + h f(t_n,y_n)\\right)\\right) $$\n    where $t_{n+1} = t_n + h$. This is applied $N_i$ times, starting from $(t_0, y_0)$.\n    c. Calculate the true solution at the final time, $y(T)$.\n    d. Compute the global error $E(h_i) = |y_{N_i} - y(T)|$.\n    e. Store the pair $(h_i, E(h_i))$.\n3.  After generating the set of $m$ data points $\\{(h_i, E(h_i))\\}$, transform them by taking logarithms to get $\\{(\\log(h_i), \\log(E(h_i)))\\}$. Let $x_i = \\log(h_i)$ and $y_i = \\log(E(h_i))$.\n4.  Estimate the slope $p$ of the best-fit line through the points $(x_i, y_i)$. For a simple linear regression model $y = \\alpha + px$, the slope $p$ which minimizes the sum of squared errors $\\sum_{i=1}^m (y_i - (\\alpha + px_i))^2$ is given by:\n$$ p = \\frac{\\sum_{i=1}^m (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^m (x_i - \\bar{x})^2} $$\nwhere $\\bar{x} = \\frac{1}{m}\\sum_{i=1}^m x_i$ and $\\bar{y} = \\frac{1}{m}\\sum_{i=1}^m y_i$ are the sample means of the $x$ and $y$ data, respectively. This formula is equivalent to computing the covariance of $x$ and $y$ divided by the variance of $x$.\n\nThis procedure is implemented for each of the four test cases provided. The implementation uses Python and the `numpy` library. `numpy` provides implementations for the necessary mathematical functions (e.g., `exp`, `sin`, `cos`, `log`, `pi`) and facilitates array operations for the least-squares calculation. A function is created to implement Heun's method, and another to compute the least-squares slope. The main part of the program iterates through the test cases, computes the errors for the specified step counts, estimates the convergence order $p$ for each case, and collects these values for the final output. The estimated values for $p$ are expected to be close to $2$, confirming the second-order accuracy of Heun's method for these smooth problems.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by implementing Heun's method, calculating global errors,\n    and estimating the order of convergence for four different IVPs.\n    \"\"\"\n\n    def heun_solver(f, t0, y0, T, N):\n        \"\"\"\n        Solves an IVP y'(t) = f(t, y) using Heun's method.\n\n        Args:\n            f: The function f(t, y).\n            t0: Initial time.\n            y0: Initial value.\n            T: Final time.\n            N: Number of steps.\n\n        Returns:\n            The numerical approximation of y(T).\n        \"\"\"\n        h = (T - t0) / N\n        t = float(t0)\n        y = float(y0)\n        for _ in range(N):\n            k1 = f(t, y)\n            k2 = f(t + h, y + h * k1)\n            y = y + (h / 2.0) * (k1 + k2)\n            t = t + h\n        return y\n\n    def estimate_order(h_values, E_values):\n        \"\"\"\n        Estimates the order of convergence p from step sizes h and errors E.\n        This is done by finding the slope of the best-fit line for log(E) vs. log(h).\n\n        Args:\n            h_values: A numpy array of step sizes.\n            E_values: A numpy array of corresponding global errors.\n\n        Returns:\n            The estimated order of convergence p.\n        \"\"\"\n        # Take the natural logarithm of step sizes and errors\n        log_h = np.log(h_values)\n        log_E = np.log(E_values)\n        \n        # We want to find the slope p of the line y = alpha + p*x that best fits\n        # the data (x, y) = (log_h, log_E) in the least-squares sense.\n        # The formula for the slope is p = Cov(x, y) / Var(x).\n        x = log_h\n        y = log_E\n        x_mean = np.mean(x)\n        y_mean = np.mean(y)\n        \n        numerator = np.sum((x - x_mean) * (y - y_mean))\n        denominator = np.sum((x - x_mean)**2)\n        \n        p = numerator / denominator\n        return p\n\n    # Define the test suite from the problem statement.\n    test_cases = [\n        {\n            \"f\": lambda t, y: -y,\n            \"y_exact\": lambda t: np.exp(-t),\n            \"t0\": 0.0,\n            \"y0\": 1.0,\n            \"T\": 1.0,\n            \"N_values\": [10, 20, 40, 80]\n        },\n        {\n            \"f\": lambda t, y: y * np.cos(t),\n            \"y_exact\": lambda t: np.exp(np.sin(t)),\n            \"t0\": 0.0,\n            \"y0\": 1.0,\n            \"T\": np.pi,\n            \"N_values\": [20, 40, 80, 160]\n        },\n        {\n            \"f\": lambda t, y: y**2,\n            \"y_exact\": lambda t: 1.0 / (1.0 - t),\n            \"t0\": 0.0,\n            \"y0\": 1.0,\n            \"T\": 0.5,\n            \"N_values\": [20, 40, 80, 160]\n        },\n        {\n            \"f\": lambda t, y: np.sin(t),\n            \"y_exact\": lambda t: 1.0 - np.cos(t),\n            \"t0\": 0.0,\n            \"y0\": 0.0,\n            \"T\": 2.0,\n            \"N_values\": [20, 40, 80, 160]\n        }\n    ]\n\n    estimated_orders = []\n    \n    # Process each test case\n    for case in test_cases:\n        h_values = []\n        errors = []\n        \n        # Calculate the exact solution at the final time T once\n        y_exact_at_T = case[\"y_exact\"](case[\"T\"])\n        \n        # Run the simulation for each specified number of steps N\n        for N in case[\"N_values\"]:\n            t0, y0, T = case[\"t0\"], case[\"y0\"], case[\"T\"]\n            \n            # Calculate step size\n            h = (T - t0) / N\n            \n            # Get numerical solution using Heun's method\n            y_numerical_at_T = heun_solver(case[\"f\"], t0, y0, T, N)\n            \n            # Calculate global error\n            error = np.abs(y_numerical_at_T - y_exact_at_T)\n            \n            h_values.append(h)\n            errors.append(error)\n        \n        # Estimate the order of convergence p\n        p = estimate_order(np.array(h_values), np.array(errors))\n        estimated_orders.append(p)\n        \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, estimated_orders))}]\")\n\nsolve()\n```", "id": "3259641"}, {"introduction": "While Heun's method is a significant improvement over the first-order Euler method, how does it compare to more powerful, higher-order methods? This advanced practice tackles the practical question of computational efficiency by pitting Heun's method against the classical fourth-order Runge-Kutta (RK4) method. By determining the total computational work needed for each method to reach a specific accuracy target, you will uncover the fundamental trade-off between a method's complexity per step and its overall efficiency for high-precision problems [@problem_id:2428159].", "problem": "A single-variable Initial Value Problem (IVP) is specified by an Ordinary Differential Equation (ODE) of the form $y^{\\prime}(t) = f(t,y(t))$ with an initial condition $y(t_{0}) = y_{0}$. The exact solution $y(t)$ satisfies the integral identity $y(t_{n+1}) = y(t_{n}) + \\int_{t_{n}}^{t_{n+1}} f(\\tau,y(\\tau)) \\, d\\tau$. A numerical one-step method approximates $y(t)$ at a discrete grid $t_{n} = t_{0} + n h$ with a constant step size $h = (T - t_{0})/N$ and produces an approximation $y_{n} \\approx y(t_{n})$. The global error at the final time $T$ is defined as $e_{\\mathrm{global}} = |y_{N} - y(T)|$.\n\nYour task is to implement and compare the efficiency of two time-integration methods for IVPs:\n- The explicit predictor–corrector scheme commonly known as Heun’s method.\n- The classical fourth-order Runge–Kutta method (Fourth-Order Runge–Kutta (RK4)).\n\nEfficiency must be measured in terms of the total number of right-hand-side evaluations of $f(t,y)$ required to meet a target global error tolerance $\\varepsilon$ at the final time $T$, using uniform time steps. For each method, determine the minimal number of steps $N$ such that $e_{\\mathrm{global}} \\le \\varepsilon$. Then compute the total number of function evaluations as $2N$ for Heun’s method and $4N$ for RK4.\n\nYou must:\n1. Implement both methods for scalar IVPs with constant step size.\n2. For each method, find the minimal $N$ that achieves $e_{\\mathrm{global}} \\le \\varepsilon$ by:\n   - Starting from $N = 1$ (so $h = (T - t_{0})/N$), repeatedly doubling $N$ until the inequality holds.\n   - Then performing an integer binary search between the last failing and first passing $N$ to find the smallest $N$ that satisfies $e_{\\mathrm{global}} \\le \\varepsilon$.\n   - If no $N$ up to a safe cap $N_{\\max}$ satisfies the tolerance, you must report failure. Use $N_{\\max} = 2^{20}$.\n3. Compute the efficiency ratio $R = \\dfrac{\\text{function evaluations by Heun}}{\\text{function evaluations by RK4}}$ for each test case, rounded to $3$ decimal places.\n\nUse the following test suite with exact solutions. In all cases, use $t_{0} = 0$ and state variables are dimensionless. For trigonometric functions, use radians.\n\n- Test A (exponential decay): $f(t,y) = -2 y$, $y(0) = 1$, exact $y(t) = e^{-2 t}$, $T = 1$, $\\varepsilon = 10^{-6}$.\n- Test B (exponential growth): $f(t,y) = y$, $y(0) = 1$, exact $y(t) = e^{t}$, $T = 1$, $\\varepsilon = 10^{-6}$.\n- Test C (forced linear): $f(t,y) = \\cos(t) - y$, $y(0) = 1$, exact $y(t) = \\dfrac{1}{2}\\left(\\sin t + \\cos t + e^{-t}\\right)$, $T = 10$, $\\varepsilon = 10^{-5}$.\n- Test D (logistic): $f(t,y) = y\\,(1 - y)$, $y(0) = 0.1$, exact $y(t) = \\dfrac{1}{1 + 9 e^{-t}}$, $T = 5$, $\\varepsilon = 10^{-6}$.\n\nFinal output specification:\n- For each test case, output the efficiency ratio $R$ as a float rounded to $3$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[r_{A},r_{B},r_{C},r_{D}]$).\n\nThere are no physical units, so no unit conversion is required. Angles, where present, are in radians.", "solution": "The problem requires a quantitative comparison of the computational efficiency of two numerical schemes for solving first-order Ordinary Differential Equations (ODEs): Heun's method and the classical fourth-order Runge-Kutta (RK4) method. Efficiency is defined as the total number of evaluations of the right-hand-side function, $f(t,y)$, required to achieve a prescribed global error tolerance, $\\varepsilon$, at a final time, $T$. This is a standard problem in computational science, designed to illustrate the trade-off between the complexity of a method, measured in function evaluations per step, and its accuracy, characterized by its order of convergence.\n\nWe consider the Initial Value Problem (IVP) given by $y'(t) = f(t,y(t))$ with the initial condition $y(t_0)=y_0$ on the time interval $[t_0, T]$. The numerical solution is computed on a uniform grid $t_n = t_0 + n h$ for $n \\in \\{0, 1, \\dots, N\\}$, with a constant step size $h = (T - t_0)/N$. The global error at the final time $T$ is defined as $e_{\\mathrm{global}} = |y_N - y(T)|$, where $y_N$ is the numerical approximation and $y(T)$ is the exact solution.\n\nHeun's method is a second-order predictor-corrector scheme. For each step from $t_n$ to $t_{n+1}$, it involves two stages:\n1.  **Predictor:** An explicit Euler step provides a first-order accurate guess for the solution at the next time point: $\\tilde{y}_{n+1} = y_n + h f(t_n, y_n)$.\n2.  **Corrector:** This preliminary result is used to evaluate the slope at the end of the interval, $f(t_{n+1}, \\tilde{y}_{n+1})$. The final update is calculated using an average of the slopes at the beginning and the predicted end of the step, which is geometrically equivalent to applying the trapezoidal rule for integration: $y_{n+1} = y_n + \\frac{h}{2} [f(t_n, y_n) + f(t_{n+1}, \\tilde{y}_{n+1})]$.\nThis method performs precisely $2$ function evaluations per time step. Its global error is of order $2$, which means $e_{\\mathrm{global}} = O(h^2)$.\n\nThe classical fourth-order Runge-Kutta (RK4) method is a more sophisticated one-step method that achieves higher accuracy by using a weighted average of four slope evaluations within each step. The update rule from $y_n$ to $y_{n+1}$ is given by:\n$$y_{n+1} = y_n + \\frac{h}{6}(k_1 + 2k_2 + 2k_3 + k_4)$$\nwhere the intermediate slopes are evaluated as follows:\n$$k_1 = f(t_n, y_n)$$\n$$k_2 = f\\left(t_n + \\frac{h}{2}, y_n + \\frac{h}{2}k_1\\right)$$\n$$k_3 = f\\left(t_n + \\frac{h}{2}, y_n + \\frac{h}{2}k_2\\right)$$\n$$k_4 = f(t_n + h, y_n + h k_3)$$\nThis method requires exactly $4$ function evaluations per time step. Its global error is of order $4$, so $e_{\\mathrm{global}} = O(h^4)$.\n\nTo determine the efficiency of each method, we must find the minimal number of steps, $N$, that satisfies the global error condition $|y_N - y(T)| \\le \\varepsilon$. The problem specifies a two-phase search algorithm for this purpose.\n1.  **Phase 1: Exponential Search.** We start with $N=1$ and repeatedly double $N$ (i.e., $N=1, 2, 4, 8, \\dots$) until we find a value, let it be $N_{\\mathrm{pass}}$, for which the error tolerance is met. The value of $N$ from the preceding iteration, $N_{\\mathrm{fail}}$, is also recorded. This procedure efficiently identifies an interval $[N_{\\mathrm{fail}}, N_{\\mathrm{pass}}]$ that is guaranteed to contain the minimal required $N$. The search is considered to have failed if the tolerance cannot be met for any $N$ up to a maximum of $N_{\\max} = 2^{20}$.\n2.  **Phase 2: Binary Search.** A standard integer binary search is then performed within the interval bracketed by the last failing and first passing number of steps to precisely pinpoint the smallest integer $N_{\\min}$ for which the global error is no greater than $\\varepsilon$.\n\nThe theoretical behavior of the global error for a method of order $p$ is $e_{\\mathrm{global}} \\approx C h^p = C \\left(\\frac{T-t_0}{N}\\right)^p$, where $C$ is a constant that depends on the ODE, its derivatives, and the specific method. To satisfy the tolerance $\\varepsilon$, we require $C \\left(\\frac{T-t_0}{N}\\right)^p \\le \\varepsilon$, which implies that the number of steps must scale as $N \\ge (T-t_0) \\left(\\frac{C}{\\varepsilon}\\right)^{1/p}$.\nFor Heun's method ($p=2$), the total number of function evaluations is $E_{\\mathrm{Heun}} = 2 N_{\\mathrm{Heun}} \\propto \\varepsilon^{-1/2}$.\nFor the RK4 method ($p=4$), the total number of evaluations is $E_{\\mathrm{RK4}} = 4 N_{\\mathrm{RK4}} \\propto \\varepsilon^{-1/4}$.\nThe efficiency ratio is therefore $R = \\frac{E_{\\mathrm{Heun}}}{E_{\\mathrm{RK4}}} \\propto \\frac{\\varepsilon^{-1/2}}{\\varepsilon^{-1/4}} = \\varepsilon^{-1/4}$. This relationship demonstrates that for small \"varepsilon\", the ratio $R$ is expected to be significantly greater than $1$. This indicates the superior efficiency of the higher-order RK4 method for reaching high accuracy, despite its higher computational cost per step. The exact value of $R$ depends on the problem-specific constants embodied in $C$, which are determined numerically by executing the specified algorithm for each test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem, implementing and comparing Heun's and RK4 methods.\n    \"\"\"\n\n    def heun_solver(f, y0, t0, T, N):\n        \"\"\"\n        Solves a scalar IVP y'=f(t,y) using Heun's method with a constant step size.\n        \"\"\"\n        h = (T - t0) / N\n        t = t0\n        y = y0\n        for _ in range(N):\n            # Predictor step\n            f1 = f(t, y)\n            y_tilde = y + h * f1\n            # Corrector step\n            t_next = t + h\n            f2 = f(t_next, y_tilde)\n            y = y + (h / 2.0) * (f1 + f2)\n            t = t_next\n        return y\n\n    def rk4_solver(f, y0, t0, T, N):\n        \"\"\"\n        Solves a scalar IVP y'=f(t,y) using the classical RK4 method with a constant step size.\n        \"\"\"\n        h = (T - t0) / N\n        t = t0\n        y = y0\n        for _ in range(N):\n            k1 = f(t, y)\n            k2 = f(t + h / 2.0, y + h * k1 / 2.0)\n            k3 = f(t + h / 2.0, y + h * k2 / 2.0)\n            k4 = f(t + h, y + h * k3)\n            y = y + (h / 6.0) * (k1 + 2 * k2 + 2 * k3 + k4)\n            t += h\n        return y\n\n    def find_min_N(solver, f, y0, t0, T, y_exact_T, tol, N_max):\n        \"\"\"\n        Finds the minimum number of steps N for a given solver to achieve the desired tolerance.\n        \"\"\"\n        \n        # Helper function to compute error for a given N\n        def get_error(num_steps):\n            y_approx = solver(f, y0, t0, T, num_steps)\n            return np.abs(y_approx - y_exact_T)\n\n        # First, check if N=1 is sufficient\n        if get_error(1) = tol:\n            return 1\n            \n        # Exponential search to find the bracketing interval for N\n        n_fail = 1\n        n_pass = 2\n        while n_pass = N_max:\n            if get_error(n_pass) = tol:\n                break\n            n_fail = n_pass\n            n_pass *= 2\n        else:\n            # This 'else' belongs to the 'while' loop, executed if the loop finishes without a break.\n            return -1 # Indicates failure to meet tolerance within N_max\n\n        # Binary search to find the minimal N\n        low = n_fail + 1\n        high = n_pass\n        min_N = n_pass\n\n        while low = high:\n            mid = low + (high - low) // 2\n            if mid == 0:  # Safety check, should not be reached with low starting  0\n                low = 1\n                continue\n                \n            if get_error(mid) = tol:\n                min_N = mid\n                high = mid - 1\n            else:\n                low = mid + 1\n        \n        return min_N\n\n    # Test cases as defined in the problem statement.\n    test_cases = [\n        # Test A (exponential decay)\n        {\n            \"f\": lambda t, y: -2.0 * y, \"y0\": 1.0, \"t0\": 0.0, \"T\": 1.0,\n            \"y_exact_func\": lambda t: np.exp(-2.0 * t), \"tol\": 1e-6, \"name\": \"A\"\n        },\n        # Test B (exponential growth)\n        {\n            \"f\": lambda t, y: y, \"y0\": 1.0, \"t0\": 0.0, \"T\": 1.0,\n            \"y_exact_func\": lambda t: np.exp(t), \"tol\": 1e-6, \"name\": \"B\"\n        },\n        # Test C (forced linear)\n        {\n            \"f\": lambda t, y: np.cos(t) - y, \"y0\": 1.0, \"t0\": 0.0, \"T\": 10.0,\n            \"y_exact_func\": lambda t: 0.5 * (np.sin(t) + np.cos(t) + np.exp(-t)), \"tol\": 1e-5, \"name\": \"C\"\n        },\n        # Test D (logistic)\n        {\n            \"f\": lambda t, y: y * (1.0 - y), \"y0\": 0.1, \"t0\": 0.0, \"T\": 5.0,\n            \"y_exact_func\": lambda t: 1.0 / (1.0 + 9.0 * np.exp(-t)), \"tol\": 1e-6, \"name\": \"D\"\n        }\n    ]\n\n    N_max = 2**20\n    results = []\n\n    for case in test_cases:\n        f = case[\"f\"]\n        y0 = case[\"y0\"]\n        t0 = case[\"t0\"]\n        T = case[\"T\"]\n        y_exact_func = case[\"y_exact_func\"]\n        tol = case[\"tol\"]\n        \n        y_exact_T = y_exact_func(T)\n\n        # Find minimal N for Heun's method\n        N_heun = find_min_N(heun_solver, f, y0, t0, T, y_exact_T, tol, N_max)\n        if N_heun == -1:\n            raise RuntimeError(f\"Heun's method failed to converge for case {case['name']}\")\n        evals_heun = 2 * N_heun\n\n        # Find minimal N for RK4 method\n        N_rk4 = find_min_N(rk4_solver, f, y0, t0, T, y_exact_T, tol, N_max)\n        if N_rk4 == -1:\n            raise RuntimeError(f\"RK4 method failed to converge for case {case['name']}\")\n        evals_rk4 = 4 * N_rk4\n\n        # Calculate and round the efficiency ratio\n        ratio = evals_heun / evals_rk4\n        results.append(round(ratio, 3))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2428159"}]}