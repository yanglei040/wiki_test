{"hands_on_practices": [{"introduction": "Amdahl's Law provides a powerful model for parallel speedup, but its parameters are not just abstract variables; they must be determined from real-world performance. This first exercise challenges you to work as a computational scientist, taking experimental timing data from a simulation and using it to infer a fundamental characteristic of the program: its serial fraction. By connecting abstract theory to concrete measurements, you will learn to quantify the inherent scalability limits of a workload, a critical first step in any performance analysis task [@problem_id:3097164].", "problem": "A parallel Monte Carlo simulation estimates a physical quantity by aggregating independent stochastic trials. Assume a fixed workload where the total execution time on $N$ identical processing cores can be partitioned into a core-invariant serial component and an ideally parallelizable component whose execution time scales inversely with $N$. Measurements on the same cluster yield wall-clock times $T_1$, $T_4$, and $T_{16}$ for $N=1$, $N=4$, and $N=16$ cores, respectively, for the same input size: $T_1 = 80 \\text{ s}$, $T_4 = 26 \\text{ s}$, $T_{16} = 12.5 \\text{ s}$. Assume cache effects, input/output overheads, and contention are negligible up to $N=16$ and that the decomposition into serial and parallel portions remains constant across $N$. Using only the fundamental definitions that a fixed fraction of the workload time is serial and the remaining fraction is perfectly parallelizable under ideal conditions for the parallel portion, infer the serial fraction $1-p$ of the workload from these measurements. Then, using first-principles reasoning grounded in those definitions, explain why the observed execution time $T_{64}$ stops improving substantially despite adding up to $N=64$ cores on the same workload.\n\nExpress your final answer for the serial fraction $1-p$ as a decimal fraction rounded to four significant figures. Do not use a percentage sign.", "solution": "The problem statement has been validated and is deemed scientifically grounded, well-posed, and objective. It provides a consistent set of data to analyze a standard model of parallel computing performance.\n\nThe problem requires us to model the total execution time, $T_N$, of a fixed workload on $N$ identical processing cores. According to the problem's first principles, this time is the sum of a core-invariant serial component and a perfectly parallelizable component.\n\nLet $T_s$ be the execution time of the serial part of the workload. This time is constant, independent of the number of cores $N$.\nLet $T_{p,1}$ be the execution time of the parallelizable part of the workload on a single core ($N=1$).\nThe total execution time on a single core is therefore $T_1 = T_s + T_{p,1}$.\n\nFor $N$ cores, the serial part still takes time $T_s$. The parallelizable part, being \"ideally parallelizable,\" has its execution time reduced by a factor of $N$. The time for the parallel part on $N$ cores is $T_{p,N} = \\frac{T_{p,1}}{N}$.\n\nThe total execution time on $N$ cores, $T_N$, is given by the sum:\n$$T_N = T_s + T_{p,N} = T_s + \\frac{T_{p,1}}{N}$$\n\nThe problem asks for the \"serial fraction\", which we denote by $s$. This is the fraction of the total single-core execution time that is spent on the serial part.\n$$s = \\frac{T_s}{T_1}$$\nSimilarly, the parallel fraction, $p$, is the fraction of time spent on the parallel part on a single core.\n$$p = \\frac{T_{p,1}}{T_1}$$\nFrom $T_1 = T_s + T_{p,1}$, it follows that $s + p = 1$, or $p = 1 - s$. The problem asks for the value of $1-p$, which is equal to $s$.\n\nWe can express $T_s$ and $T_{p,1}$ in terms of $T_1$ and $s$:\n$T_s = s \\cdot T_1$\n$T_{p,1} = (1-s) \\cdot T_1$\n\nSubstituting these into the equation for $T_N$ gives the governing equation for this system, commonly known as Amdahl's Law:\n$$T_N = s \\cdot T_1 + \\frac{(1-s) \\cdot T_1}{N}$$\n\nWe are given the following data:\n$T_1 = 80 \\text{ s}$\n$T_4 = 26 \\text{ s}$\n$T_{16} = 12.5 \\text{ s}$\n\nWe can use the data for $N=4$ to an solve for the unknown serial fraction $s$.\nSubstituting $N=4$, $T_4 = 26$, and $T_1 = 80$:\n$$26 = s \\cdot 80 + \\frac{(1-s) \\cdot 80}{4}$$\n$$26 = 80s + 20(1-s)$$\n$$26 = 80s + 20 - 20s$$\n$$26 - 20 = 60s$$\n$$6 = 60s$$\n$$s = \\frac{6}{60} = 0.1$$\n\nTo verify the consistency of the model and the data, we can perform the same calculation using the data for $N=16$:\nSubstituting $N=16$, $T_{16} = 12.5$, and $T_1 = 80$:\n$$12.5 = s \\cdot 80 + \\frac{(1-s) \\cdot 80}{16}$$\n$$12.5 = 80s + 5(1-s)$$\n$$12.5 = 80s + 5 - 5s$$\n$$12.5 - 5 = 75s$$\n$$7.5 = 75s$$\n$$s = \\frac{7.5}{75} = 0.1$$\nBoth measurements yield an identical serial fraction $s=0.1$, confirming the validity of the model for the given data. The serial fraction $1-p$ is $s = 0.1$. The problem requires this to be expressed to four significant figures, which is $0.1000$.\n\nThe second part of the task is to explain why the execution time stops improving substantially for $N=64$. This is a direct consequence of the serial portion of the workload. Using our derived value of $s=0.1$ and the given $T_1=80 \\text{ s}$, we can determine the absolute time for the serial component:\n$$T_s = s \\cdot T_1 = 0.1 \\cdot 80 \\text{ s} = 8 \\text{ s}$$\nThe governing equation is $T_N = T_s + \\frac{T_{p,1}}{N}$. As the number of cores $N$ increases, the parallel term $\\frac{T_{p,1}}{N}$ decreases and approaches zero. However, the total execution time $T_N$ can never be less than the constant serial time $T_s$.\n$$\\lim_{N \\to \\infty} T_N = \\lim_{N \\to \\infty} \\left(T_s + \\frac{T_{p,1}}{N}\\right) = T_s + 0 = 8 \\text{ s}$$\nThis means the execution time is bounded below by $8 \\text{ s}$, regardless of how many cores are used. This bound is the fundamental reason for the diminishing returns in performance.\n\nLet's quantify this by calculating the predicted time for $N=64$ and comparing the time reductions. First, we find $T_{p,1}$:\n$$T_{p,1} = (1-s) \\cdot T_1 = (1 - 0.1) \\cdot 80 \\text{ s} = 0.9 \\cdot 80 \\text{ s} = 72 \\text{ s}$$\nSo, our specific model is $T_N = 8 + \\frac{72}{N}$.\nLet's predict $T_{64}$:\n$$T_{64} = 8 + \\frac{72}{64} = 8 + \\frac{9}{8} = 8 + 1.125 = 9.125 \\text{ s}$$\n\nNow consider the time saved by successive quadrupling of cores:\n-   Time saved from $N=1$ to $N=4$: $T_1 - T_4 = 80 - 26 = 54 \\text{ s}$.\n-   Time saved from $N=4$ to $N=16$: $T_4 - T_{16} = 26 - 12.5 = 13.5 \\text{ s}$.\n-   Time saved from $N=16$ to $N=64$: $T_{16} - T_{64} = 12.5 - 9.125 = 3.375 \\text{ s}$.\n\nEach quadrupling of computational resources yields a progressively smaller reduction in execution time. At $N=16$, the execution time is already $12.5 \\text{ s}$, which is only $4.5 \\text{ s}$ away from the theoretical minimum of $8 \\text{ s}$. When moving to $N=64$, we are parallelizing this remaining small fraction of the total time, resulting in a modest improvement of only $3.375 \\text{ s}$. Because the total time $T_N$ is asymptotically approaching the serial time $T_s$, the benefit of adding more cores diminishes rapidly. This saturation of performance is a hallmark of problems with a non-zero serial fraction.", "answer": "$$\n\\boxed{0.1000}\n$$", "id": "3097164"}, {"introduction": "In the world of high-performance computing, it is common to encounter bold claims of massive speedups from new hardware or algorithms. Amdahl's Law, however, serves as an essential tool for conducting a \"reality check\" on these claims by revealing the strict theoretical constraints on performance. This practice will sharpen your critical thinking skills by tasking you to use the law not just for calculation, but for evaluation, helping you determine the plausibility of a performance report by calculating the nearly perfect degree of parallelization it would require [@problem_id:3097177].", "problem": "A research team claims that a data-processing kernel achieves a speedup of $100$ times when run on $N = 128$ identical processor cores of a High-Performance Computing (HPC) cluster. You are asked to evaluate this claim using the reasoning behind Amdahlâ€™s law without invoking any pre-derived shortcut formulas.\n\nStart from the following fundamental base:\n- Let the single-core execution time be $T_{1}$, decomposed into a strictly serial (non-parallelizable) component $T_{\\mathrm{s}}$ and a parallelizable component $T_{\\mathrm{p}}$, so that $T_{1} = T_{\\mathrm{s}} + T_{\\mathrm{p}}$.\n- On $N$ identical cores under idealized parallel execution (no overhead and perfect load balancing), only the parallelizable component is accelerated, yielding an $N$-core execution time $T_{N} = T_{\\mathrm{s}} + \\frac{T_{\\mathrm{p}}}{N}$.\n- Speedup is defined as $S(N) = \\frac{T_{1}}{T_{N}}$.\n- Define the parallel fraction $p$ by $p = \\frac{T_{\\mathrm{p}}}{T_{1}}$ and the serial fraction $s$ by $s = \\frac{T_{\\mathrm{s}}}{T_{1}}$, with $s = 1 - p$.\n\nUsing only these definitions and the claim $S(128) = 100$, derive the required value of the parallel fraction $p$ that would make the claim true under the idealized model above. Then, given prior measurements on comparable kernels that consistently report a serial fraction in the range $s \\in [0.01, 0.03]$, discuss the plausibility of the claimed speedup.\n\nExpress your final value of $p$ as a simplified fraction. Do not round. No units are required for $p$.", "solution": "The problem is evaluated as valid. It is scientifically grounded in the principles of Amdahl's law, well-posed with sufficient information to derive a unique solution, and framed using objective, formal language. The problem is a standard exercise in applying computational science principles.\n\nThe task is to determine the parallel fraction $p$ required to achieve a claimed speedup and then to assess the plausibility of this claim. The derivation must start from the provided fundamental definitions.\n\nThe given definitions are:\n- Single-core execution time: $T_{1} = T_{\\mathrm{s}} + T_{\\mathrm{p}}$\n- $N$-core execution time: $T_{N} = T_{\\mathrm{s}} + \\frac{T_{\\mathrm{p}}}{N}$\n- Speedup: $S(N) = \\frac{T_{1}}{T_{N}}$\n- Serial fraction: $s = \\frac{T_{\\mathrm{s}}}{T_{1}}$\n- Parallel fraction: $p = \\frac{T_{\\mathrm{p}}}{T_{1}}$, with $p + s = 1$.\n\nWe begin by expressing the speedup $S(N)$ in terms of the parallel fraction $p$ and the number of cores $N$. First, we manipulate the expression for the $N$-core execution time, $T_N$:\n$$T_{N} = T_{\\mathrm{s}} + \\frac{T_{\\mathrm{p}}}{N}$$\nUsing the definitions $T_{\\mathrm{s}} = s T_1$ and $T_{\\mathrm{p}} = p T_1$, we substitute these into the equation for $T_N$:\n$$T_{N} = s T_1 + \\frac{p T_1}{N} = T_1 \\left(s + \\frac{p}{N}\\right)$$\nNow, substitute this expression for $T_N$ into the definition of speedup $S(N)$:\n$$S(N) = \\frac{T_{1}}{T_{N}} = \\frac{T_{1}}{T_1 \\left(s + \\frac{p}{N}\\right)} = \\frac{1}{s + \\frac{p}{N}}$$\nUsing the relationship $s = 1 - p$, we obtain Amdahl's law in its standard form, expressed solely in terms of $p$ and $N$:\n$$S(N) = \\frac{1}{(1-p) + \\frac{p}{N}}$$\nThe problem states that a speedup of $S(N) = 100$ is achieved with $N = 128$ cores. We substitute these values into the derived equation to solve for the required parallel fraction $p$:\n$$100 = \\frac{1}{(1-p) + \\frac{p}{128}}$$\nTo solve for $p$, we take the reciprocal of both sides:\n$$(1-p) + \\frac{p}{128} = \\frac{1}{100} = 0.01$$\nNow, we rearrange the terms to isolate $p$:\n$$1 - p + \\frac{p}{128} = 0.01$$\n$$1 - 0.01 = p - \\frac{p}{128}$$\n$$0.99 = p \\left(1 - \\frac{1}{128}\\right)$$\n$$0.99 = p \\left(\\frac{128}{128} - \\frac{1}{128}\\right)$$\n$$0.99 = p \\left(\\frac{127}{128}\\right)$$\nSolving for $p$:\n$$p = 0.99 \\times \\frac{128}{127} = \\frac{99}{100} \\times \\frac{128}{127} = \\frac{99 \\times 128}{100 \\times 127}$$\nWe calculate the product in the numerator and denominator:\n$$p = \\frac{12672}{12700}$$\nTo express this as a simplified fraction, we find the greatest common divisor. Both numbers are divisible by $4$:\n$$12672 \\div 4 = 3168$$\n$$12700 \\div 4 = 3175$$\nSo, the fraction becomes:\n$$p = \\frac{3168}{3175}$$\nThe prime factorization of the denominator is $3175 = 5^2 \\times 127$. The numerator $3168$ is not divisible by $5$ or $127$. Thus, the fraction is in its simplest form. This is the value of the parallel fraction required to validate the claim under the idealized model.\n\nNext, we evaluate the plausibility of this result. The problem states that for comparable kernels, the measured serial fraction $s$ is consistently in the range $s \\in [0.01, 0.03]$. We must calculate the serial fraction $s$ that corresponds to our derived parallel fraction $p$:\n$$s = 1 - p = 1 - \\frac{3168}{3175} = \\frac{3175 - 3168}{3175} = \\frac{7}{3175}$$\nTo compare this value to the given range, we convert it to a decimal:\n$$s = \\frac{7}{3175} \\approx 0.0022047...$$\nThe calculated required serial fraction is $s \\approx 0.0022$. This value is significantly smaller than the lower bound of the empirically observed range, which is $0.01$. The required serial fraction is less than one-fourth of the most optimistic typical measurements ($0.0022$ vs $0.01$).\n\nTherefore, the claim of a $100$-fold speedup on $128$ cores is highly implausible. To achieve such a speedup, the data-processing kernel would need to be almost entirely parallelizable, with a serial component that is far smaller than what is typically found in comparable applications. The analysis, which is based on an idealized model that neglects real-world overheads (such as inter-processor communication, memory contention, and thread management), already indicates that the claim is questionable. In a real system, these overheads would further reduce the achievable speedup, making the claim even less likely to be true.", "answer": "$$\\boxed{\\frac{3168}{3175}}$$", "id": "3097177"}, {"introduction": "Optimizing a parallel application involves making difficult choices about where to invest your programming effort. Is it more valuable to attack the small, stubborn serial bottleneck or to further refine the already-parallelized bulk of the code? This exercise models that strategic dilemma, extending Amdahl's Law to compare two distinct optimization strategies. You will discover that the best path forward is not universal but depends on the number of processors, $N$, and by calculating a \"break-even\" point, you learn how to make informed, data-driven decisions in performance engineering [@problem_id:3097220].", "problem": "A scientific computing application consists of a strictly serial portion and a perfectly parallelizable portion. On a single processor, normalize the total runtime to $1$, and let the fraction of work that is parallelizable be $p$ and the strictly serial fraction be $1-p$. Assume ideal load balance and no overheads so that, on $N$ identical processors, only the parallelizable work can be divided evenly among the $N$ processors, while the serial work cannot be accelerated.\n\nAn algorithm designer proposes two distinct algorithmic improvements, each of which changes the amount of work but not the hardware:\n- Serial-optimized variant: the strictly serial work is reduced by a factor $k$ (making it $k$ times faster), with no change to the parallelizable work.\n- Parallel-optimized variant: the parallelizable work is reduced by a factor $r$ (making it $r$ times faster), with no change to the serial work.\n\nLet $p = 0.88$, $k = 2.5$, and $r = 1.6$.\n\nTasks:\n1. Starting from the definition of speedup $S(N)$ as the ratio of the single-processor runtime to the $N$-processor runtime, and from the decomposition of the work into serial and parallelizable parts, derive expressions for the baseline runtime $T_{0}(N)$ and speedup $S_{0}(N)$, the serial-optimized runtime $T_{S}(N)$ and speedup $S_{S}(N)$, and the parallel-optimized runtime $T_{P}(N)$ and speedup $S_{P}(N)$, all as functions of $N$, $p$, $k$, and $r$.\n2. Evaluate $S_{0}(8)$, $S_{S}(8)$, and $S_{P}(8)$ numerically and state which of the two algorithmic improvements (serial-optimized or parallel-optimized) yields the larger speedup at $N=8$. Report these three speedups rounded to four significant figures.\n3. Determine the processor count $N^{*}$ (not necessarily an integer) at which the two improved variants achieve equal speedup, that is, $S_{S}(N^{*}) = S_{P}(N^{*})$. Provide $N^{*}$ rounded to four significant figures. Provide $N^{*}$ as your final answer.", "solution": "The solution proceeds by addressing the three tasks outlined in the problem statement.\n\n**Task 1: Derive expressions for runtimes and speedups.**\n\nThe total single-processor runtime for the baseline application is normalized to $1$. This runtime is composed of a serial part, with duration $1-p$, and a parallelizable part, with duration $p$. Thus, the single-processor runtime is $T(1) = (1-p) + p = 1$.\n\nWhen using $N$ processors, the serial part's duration remains unchanged, while the parallelizable part's duration is reduced by a factor of $N$. The speedup, $S(N)$, is defined as the ratio of the original single-processor runtime, $T_{0}(1)=1$, to the runtime on $N$ processors, $T(N)$. This definition provides a consistent basis for comparing the different variants, as it measures the total improvement over the original, unoptimized, single-processor execution.\n\n**Baseline Variant (Subscript 0):**\nThe runtime on $N$ processors, $T_{0}(N)$, is the sum of the serial time and the scaled parallel time:\n$$T_{0}(N) = (1-p) + \\frac{p}{N}$$\nThe speedup, $S_{0}(N)$, is the ratio of the original single-processor time, $T_{0}(1)=1$, to this new runtime:\n$$S_{0}(N) = \\frac{T_{0}(1)}{T_{0}(N)} = \\frac{1}{(1-p) + \\frac{p}{N}}$$\n\n**Serial-Optimized Variant (Subscript S):**\nIn this variant, the serial work, originally $1-p$, is reduced by a factor of $k$. The new duration of the serial part is $\\frac{1-p}{k}$. The parallelizable work, $p$, is unchanged.\nThe runtime on $N$ processors, $T_{S}(N)$, is the sum of the new serial time and the scaled parallel time:\n$$T_{S}(N) = \\frac{1-p}{k} + \\frac{p}{N}$$\nThe corresponding speedup, $S_{S}(N)$, is measured against the original single-processor time:\n$$S_{S}(N) = \\frac{T_{0}(1)}{T_{S}(N)} = \\frac{1}{\\frac{1-p}{k} + \\frac{p}{N}}$$\n\n**Parallel-Optimized Variant (Subscript P):**\nIn this variant, the parallelizable work, originally $p$, is reduced by a factor of $r$. The new amount of parallelizable work is $\\frac{p}{r}$. The serial work, $1-p$, is unchanged.\nThe runtime on $N$ processors, $T_{P}(N)$, is the sum of the serial time and the new, scaled parallel time:\n$$T_{P}(N) = (1-p) + \\frac{p/r}{N} = (1-p) + \\frac{p}{rN}$$\nThe corresponding speedup, $S_{P}(N)$, is measured against the original single-processor time:\n$$S_{P}(N) = \\frac{T_{0}(1)}{T_{P}(N)} = \\frac{1}{(1-p) + \\frac{p}{rN}}$$\n\n**Task 2: Evaluate speedups at $N=8$.**\n\nWe are given $p = 0.88$, $k = 2.5$, $r = 1.6$, and we evaluate the speedups for $N=8$. The serial fraction is $1-p = 1 - 0.88 = 0.12$.\n\nBaseline speedup $S_{0}(8)$:\n$$S_{0}(8) = \\frac{1}{(1-0.88) + \\frac{0.88}{8}} = \\frac{1}{0.12 + 0.11} = \\frac{1}{0.23} \\approx 4.347826...$$\nRounded to four significant figures, $S_{0}(8) \\approx 4.348$.\n\nSerial-optimized speedup $S_{S}(8)$:\n$$S_{S}(8) = \\frac{1}{\\frac{1-0.88}{2.5} + \\frac{0.88}{8}} = \\frac{1}{\\frac{0.12}{2.5} + 0.11} = \\frac{1}{0.048 + 0.11} = \\frac{1}{0.158} \\approx 6.329113...$$\nRounded to four significant figures, $S_{S}(8) \\approx 6.329$.\n\nParallel-optimized speedup $S_{P}(8)$:\n$$S_{P}(8) = \\frac{1}{(1-0.88) + \\frac{0.88}{1.6 \\times 8}} = \\frac{1}{0.12 + \\frac{0.88}{12.8}} = \\frac{1}{0.12 + 0.06875} = \\frac{1}{0.18875} \\approx 5.298013...$$\nRounded to four significant figures, $S_{P}(8) \\approx 5.298$.\n\nComparing the two improvements at $N=8$:\n$S_{S}(8) \\approx 6.329$ and $S_{P}(8) \\approx 5.298$.\nSince $6.329 > 5.298$, the serial-optimized variant yields the larger speedup at $N=8$.\n\n**Task 3: Determine the processor count $N^{*}$ for equal speedup.**\n\nWe need to find the value of $N = N^{*}$ where $S_{S}(N^{*}) = S_{P}(N^{*})$.\n$$\\frac{1}{\\frac{1-p}{k} + \\frac{p}{N^{*}}} = \\frac{1}{(1-p) + \\frac{p}{rN^{*}}}$$\nFor the expressions to be equal, their denominators must be equal:\n$$\\frac{1-p}{k} + \\frac{p}{N^{*}} = (1-p) + \\frac{p}{rN^{*}}$$\nTo solve for $N^{*}$, we first gather terms involving $N^{*}$ on one side and constant terms on the other.\n$$\\frac{p}{N^{*}} - \\frac{p}{rN^{*}} = (1-p) - \\frac{1-p}{k}$$\nFactor out common terms on both sides:\n$$\\frac{p}{N^{*}}\\left(1 - \\frac{1}{r}\\right) = (1-p)\\left(1 - \\frac{1}{k}\\right)$$\nSimplify the terms in the parentheses:\n$$\\frac{p}{N^{*}}\\left(\\frac{r-1}{r}\\right) = (1-p)\\left(\\frac{k-1}{k}\\right)$$\nNow, we can isolate $N^{*}$:\n$$N^{*} = \\frac{p}{1-p} \\cdot \\frac{\\left(\\frac{r-1}{r}\\right)}{\\left(\\frac{k-1}{k}\\right)} = \\frac{p}{1-p} \\cdot \\frac{k(r-1)}{r(k-1)}$$\nSubstituting the given values $p=0.88$, $k=2.5$, and $r=1.6$:\n$$N^{*} = \\frac{0.88}{1-0.88} \\cdot \\frac{2.5(1.6-1)}{1.6(2.5-1)}$$\n$$N^{*} = \\frac{0.88}{0.12} \\cdot \\frac{2.5(0.6)}{1.6(1.5)}$$\n$$N^{*} = \\frac{0.88}{0.12} \\cdot \\frac{1.5}{2.4}$$\n$$N^{*} = \\frac{0.88 \\times 1.5}{0.12 \\times 2.4} = \\frac{1.32}{0.288}$$\n$$N^{*} = 4.58333...$$\nRounding to four significant figures, we get $N^{*} \\approx 4.583$. This is the processor count where both optimization strategies yield the same overall speedup. For $N < N^{*}$, the parallel-optimized variant is superior, and for $N > N^{*}$, the serial-optimized variant is superior. Our result from Task 2 ($S_S(8)>S_P(8)$ for $N=8 > 4.583$) is consistent with this finding.", "answer": "$$\\boxed{4.583}$$", "id": "3097220"}]}