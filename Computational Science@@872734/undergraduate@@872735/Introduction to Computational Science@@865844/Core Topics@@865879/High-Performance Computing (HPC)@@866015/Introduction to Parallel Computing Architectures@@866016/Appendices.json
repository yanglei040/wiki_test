{"hands_on_practices": [{"introduction": "Effective parallel programming requires understanding how software interacts with underlying hardware architecture. This first practice explores a common but subtle performance pitfall in shared-memory systems known as \"false sharing\". By analyzing a scenario where multiple threads update logically independent counters [@problem_id:3145329], you will learn how data layout at the byte level can trigger costly, unnecessary communication through the cache coherence protocol, demonstrating a crucial link between data structures and parallel efficiency.", "problem": "A shared-memory multicore system uses an invalidation-based cache coherence protocol consistent with Modified, Exclusive, Shared, Invalid (MESI). The coherence unit is a cache line of size $L = 64$ bytes. Consider $T$ threads pinned one per core on a processor whose private caches also have line size $L = 64$ bytes. Each thread $i$ increments a per-thread counter in a shared array $c[\\,]$ at index $i$ inside a tight loop. The type of $c[\\,]$ is a $64$-bit unsigned integer, so each element has size $s = 8$ bytes. The array base is aligned to $L = 64$ bytes. The inter-thread communication pattern requires no sharing of the counters’ logical values during the loop; each thread only updates its own counter and a reduction is performed after the loop to combine results.\n\nEmpirically, the increment loop achieves lower throughput than expected, and performance improves when threads are reduced to $T = 1$. You are asked to reason from first principles of cache coherence to explain whether the observed slowdown is consistent with false sharing and, if so, to determine which data layout redesigns guarantee elimination of cache line ping-pong due to false sharing, without changing the loop’s semantics.\n\nSelect all options that, under these assumptions, guarantee that distinct threads updating their own counters will not cause false sharing during the increment loop for any $T \\ge 1$.\n\nA. Pad each counter so that successive counters are separated by at least $L = 64$ bytes, and align the array base to $L = 64$ bytes, so each thread’s counter resides on a distinct cache line.\n\nB. Keep the original contiguous array of $s = 8$-byte counters but align only the array base to $L = 64$ bytes; rely on base alignment to prevent false sharing.\n\nC. Replace the struct-of-arrays layout with an array-of-structs in which each per-thread struct contains the counter and is padded so the struct size is exactly $L = 64$ bytes; align the array base to $L = 64$ bytes.\n\nD. Use atomic increment operations (e.g., atomic fetch-and-add) on the original contiguous array of $s = 8$-byte counters to prevent cache line ping-pong.\n\nE. Use a struct-of-arrays layout that groups all counters contiguously to improve spatial locality for the counter field; rely on improved locality to reduce coherence traffic.\n\nProvide your selection and justify it by deriving the conditions under which false sharing occurs for the original layout and the conditions required to avoid it, grounded in the core definitions of cache lines and invalidation-based coherence.", "solution": "The problem statement describes a classic performance issue in parallel computing known as false sharing. It is scientifically grounded, well-posed, and provides sufficient, consistent information to derive a solution from first principles of computer architecture.\n\n**1. Validation of the Problem Statement**\n\nThe givens are:\n- A shared-memory multicore system with $T$ threads, one per core.\n- An invalidation-based cache coherence protocol, consistent with MESI.\n- The coherence unit (and private cache line size) is $L = 64$ bytes.\n- Each thread $i$ increments its own counter, $c[i]$, in a tight loop.\n- The counters are $64$-bit unsigned integers, so their size is $s = 8$ bytes.\n- The array $c$ is contiguous in memory, and its base address is aligned to a $64$-byte boundary.\n- There is no logical need for threads to share counter values during the loop.\n\nThe observed performance degradation for $T > 1$ is consistent with the phenomenon of false sharing. Let us analyze why from first principles.\n\n**2. Derivation from First Principles**\n\nCache coherence protocols operate on the granularity of a cache line. In an invalidation-based protocol like MESI, when a core writes to a memory location, it must first gain exclusive ownership of the cache line containing that location. This is typically achieved by sending an invalidation signal over the interconnect to all other cores that may have a copy of that same cache line. Those cores must then mark their copies as invalid (I state). Any subsequent attempt by those cores to read from or write to that line will result in a cache miss, forcing a fetch of the updated line from memory or directly from the modifying core's cache.\n\nThe problem arises when multiple distinct data items, updated by different threads, happen to reside on the same physical cache line. This is called **false sharing**. The data items are logically independent, but their physical proximity in memory couples their performance through the cache coherence mechanism.\n\nIn the given scenario:\n- The size of a single counter element is $s = 8$ bytes.\n- The size of a cache line is $L = 64$ bytes.\n- Therefore, the number of counters that can fit within a single cache line is $N = L/s = 64/8 = 8$.\n\nThe array $c$ is contiguous and its base is aligned to a $64$-byte boundary. This means that the memory addresses for the first $8$ counters, $c[0], c[1], \\dots, c[7]$, will occupy the same cache line. Specifically, if the base address of $c$ is $A_{base}$, then $c[i]$ is at address $A_{base} + i \\times s$. For $i \\in \\{0, 1, \\dots, 7\\}$, these addresses all fall within the first cache line, which spans memory $[A_{base}, A_{base} + L - 1]$.\n\nLet's consider two threads, Thread $0$ and Thread $1$, running on different cores and updating $c[0]$ and $c[1]$ respectively.\n1. Thread $0$ executes `c[0]++`. This is a read-modify-write operation. To perform the write, Core $0$ must acquire exclusive ownership of the cache line. Let's say it brings the line into its private cache in the 'Modified' (M) state.\n2. Thread $1$ executes `c[1]++`. Since $c[1]$ is on the same cache line, Core $1$ must also acquire exclusive ownership. It sends a request for the line.\n3. This request causes Core $0$ to lose exclusive ownership. It might write the line back to memory and/or forward it to Core $1$. Core $0$'s copy of the line is invalidated (I state).\n4. Core $1$ now has the line in the 'Modified' state and increments $c[1]$.\n5. When Thread $0$ loops and attempts to increment $c[0]$ again, it finds its copy is invalid, resulting in a cache miss. It must re-acquire the cache line, which in turn invalidates the copy in Core $1$'s cache.\n\nThis back-and-forth transfer of the cache line between cores is known as \"cache line ping-pong.\" It is a costly process that involves stalls and traffic on the memory bus or interconnect, explaining the observed low throughput. The issue disappears for $T=1$ because there is no other core to contend with for the cache line.\n\nTo **guarantee** the elimination of false sharing, we must ensure that no two counters modified by different threads reside on the same cache line. Since each thread $i$ modifies only $c[i]$, the condition is that for any two distinct threads $i$ and $j$, $c[i]$ and $c[j]$ must be on different cache lines. This can be achieved by structuring the data such that each `c[i]` is located in a memory block of at least $L=64$ bytes that is not shared with any other $c[j]$.\n\n**3. Evaluation of Options**\n\n**A. Pad each counter so that successive counters are separated by at least $L = 64$ bytes, and align the array base to $L = 64$ bytes, so each thread’s counter resides on a distinct cache line.**\nThis approach explicitly enforces the necessary condition. If the base address of the array is aligned to $64$ bytes, and each element is placed at an offset that is a multiple of $64$ bytes, then each element will start at the beginning of a new cache line. For example, the address of $c[i]$ would be $A_{base} + i \\times P$, where $P \\ge L = 64$. Since a cache line cannot span across two elements, and each element's start is at least $L$ bytes from the next, each $c[i]$ will be on a unique cache line (relative to other counters). This layout directly prevents false sharing.\nVerdict: **Correct**.\n\n**B. Keep the original contiguous array of $s = 8$-byte counters but align only the array base to $L = 64$ bytes; rely on base alignment to prevent false sharing.**\nThis describes the original problematic setup. As demonstrated in the derivation, aligning the base of a contiguous array of small elements does not prevent those elements from sharing a cache line. The first $8$ counters, $c[0]$ through $c[7]$, will still occupy the first cache line, leading to false sharing among the first $8$ threads.\nVerdict: **Incorrect**.\n\n**C. Replace the struct-of-arrays layout with an array-of-structs in which each per-thread struct contains the counter and is padded so the struct size is exactly $L = 64$ bytes; align the array base to $L = 64$ bytes.**\nThis is an alternative, and common, implementation of the strategy in option A. An array of structs like `struct { uint64_t counter; char padding[56]; } data[T];` would have a `sizeof` equal to $64$ bytes. Since the array is contiguous and its base is aligned to $64$ bytes, the address of the $i$-th struct, `data[i]`, will be $A_{base} + i \\times 64$. This means each struct, and thus each counter within it, will begin on a new cache line boundary. The counter for thread $i$ is on a different cache line from the counter for thread $j$ ($i \\neq j$). This guarantees the elimination of false sharing.\nVerdict: **Correct**.\n\n**D. Use atomic increment operations (e.g., atomic fetch-and-add) on the original contiguous array of $s = 8$-byte counters to prevent cache line ping-pong.**\nAtomic operations ensure that a read-modify-write sequence is performed as an indivisible unit from the perspective of the CPU's instruction stream. This is crucial for preventing data races in cases of *true sharing* (multiple threads updating the same variable). However, they do not alter the underlying memory layout or the behavior of the cache coherence protocol. An atomic write to $c[i]$ is still a write to a memory location within a cache line. If other threads are writing to other variables (like $c[j]$) on the same cache line, the hardware must still invalidate their copies to give the writing core exclusive access. Atomic operations do not and cannot prevent the cache line ping-pong caused by false sharing.\nVerdict: **Incorrect**.\n\n**E. Use a struct-of-arrays layout that groups all counters contiguously to improve spatial locality for the counter field; rely on improved locality to reduce coherence traffic.**\nThe initial problem\nsetup *is* a struct-of-arrays layout (or its equivalent for a simple array), where all counters are grouped contiguously. This design provides excellent spatial locality for a *sequential* access pattern (one thread iterating over all counters). However, for the given *parallel* access pattern where each thread repeatedly accesses its own single, distinct counter, this very spatial locality is the cause of false sharing. Placing the counters contiguously ensures they will share cache lines, which in turn *maximizes*, rather than reduces, coherence traffic.\nVerdict: **Incorrect**.", "answer": "$$\\boxed{AC}$$", "id": "3145329"}, {"introduction": "After exploring micro-architectural effects, we now zoom out to model the performance of the entire system. The Roofline model is an insightful tool that establishes the ultimate performance bounds of a given hardware architecture, determined by its peak computational rate and memory bandwidth. This exercise [@problem_id:3145387] challenges you to apply this model to predict the maximum achievable speedup for a parallel application, illustrating the critical concept of memory-bound versus compute-bound limitations and why parallel scaling eventually saturates.", "problem": "A programmer is evaluating a shared-memory multicore system using the roofline performance perspective for a memory-bound kernel. The system has up to $N_{\\max} = 32$ identical cores, each with a per-core peak compute rate of $P_{c} = 80$ giga floating-point operations per second (GFLOP/s). The memory subsystem provides a sustained bandwidth cap of $B = 60$ gigabytes per second (GB/s), but a single core sustains only $b_{1} = 12$ GB/s due to limited parallelism in memory accesses. The kernel has constant arithmetic intensity $I = 0.8$ floating-point operations per byte. Assume the following are valid: arithmetic intensity is defined as floating-point operations per byte moved; sustained performance is bounded by both the compute peak and the product of arithmetic intensity with the achievable bandwidth; and the achievable bandwidth scales approximately linearly with the number of active cores until reaching the cap $B$. The programmer defines parallel speedup $S(N)$ as the ratio of the achievable performance with $N$ cores to the achievable performance with one core. Under these constraints, deduce the maximum possible speedup $S_{\\max}$ over all integer $N$ with $1 \\leq N \\leq N_{\\max}$, and provide it as a dimensionless decimal number. No rounding is needed; provide the exact value.", "solution": "The problem asks for the maximum possible parallel speedup $S_{\\max}$ for a memory-bound kernel on a shared-memory multicore system. The analysis will be conducted using the roofline performance model, which posits that the achievable performance is the minimum of the peak compute performance and the peak memory performance.\n\nFirst, let us define the achievable performance for $N$ cores, denoted as $P(N)$. According to the roofline model:\n$$ P(N) = \\min(P_{\\text{compute}}(N), P_{\\text{memory}}(N)) $$\nwhere $P_{\\text{compute}}(N)$ is the total peak compute rate for $N$ cores, and $P_{\\text{memory}}(N)$ is the performance achievable given the memory bandwidth for $N$ cores.\n\nThe givens are:\n- Maximum number of cores: $N_{\\max} = 32$\n- Per-core peak compute rate: $P_{c} = 80$ GFLOP/s\n- System sustained memory bandwidth cap: $B = 60$ GB/s\n- Single-core sustained memory bandwidth: $b_{1} = 12$ GB/s\n- Kernel arithmetic intensity: $I = 0.8$ FLOP/byte\n\nThe total peak compute rate for $N$ identical cores is the sum of their individual peak rates:\n$$ P_{\\text{compute}}(N) = N \\times P_{c} $$\n\nThe peak memory performance is the product of the kernel's arithmetic intensity $I$ and the achievable memory bandwidth for $N$ cores, $B(N)$:\n$$ P_{\\text{memory}}(N) = I \\times B(N) $$\n\nThe problem states that the achievable bandwidth $B(N)$ scales linearly with the number of active cores $N$ until it reaches the system cap $B$. The single-core bandwidth is $b_1$. Therefore, the linearly scaled bandwidth is $N \\times b_1$. This leads to the following model for $B(N)$:\n$$ B(N) = \\min(N \\times b_1, B) $$\n\nCombining these expressions, the overall performance $P(N)$ is:\n$$ P(N) = \\min(N \\times P_{c}, I \\times \\min(N \\times b_1, B)) $$\nUsing the property $\\min(a, \\min(b, c)) = \\min(a, b, c)$, we can write:\n$$ P(N) = \\min(N \\times P_{c}, N \\times I \\times b_1, I \\times B) $$\n\nLet's substitute the given values into the terms of the expression for $P(N)$:\n- $N \\times P_{c} = N \\times 80$ GFLOP/s\n- $N \\times I \\times b_1 = N \\times 0.8 \\frac{\\text{FLOP}}{\\text{byte}} \\times 12 \\frac{\\text{GB}}{\\text{s}} = N \\times 9.6$ GFLOP/s\n- $I \\times B = 0.8 \\frac{\\text{FLOP}}{\\text{byte}} \\times 60 \\frac{\\text{GB}}{\\text{s}} = 48$ GFLOP/s\n\nSo, the performance in GFLOP/s is:\n$$ P(N) = \\min(80N, 9.6N, 48) $$\nFor any positive number of cores $N \\ge 1$, we have $9.6N  80N$. Thus, the term $80N$ is never the minimum, and the expression simplifies to:\n$$ P(N) = \\min(9.6N, 48) $$\nThis confirms that for the given arithmetic intensity, the kernel's performance is always limited by the memory subsystem, not the peak computational capability of the cores, for any number of cores $N$ in the specified range.\n\nThe problem defines the parallel speedup $S(N)$ as the ratio of the achievable performance with $N$ cores to that with one core. First, we must calculate the single-core performance, $P(1)$:\n$$ P(1) = \\min(9.6 \\times 1, 48) = \\min(9.6, 48) = 9.6 \\text{ GFLOP/s} $$\n\nNow, we can express the speedup function $S(N)$:\n$$ S(N) = \\frac{P(N)}{P(1)} = \\frac{\\min(9.6N, 48)}{9.6} $$\nWe can distribute the division into the $\\min$ function:\n$$ S(N) = \\min\\left(\\frac{9.6N}{9.6}, \\frac{48}{9.6}\\right) $$\n$$ S(N) = \\min(N, 5) $$\n\nThe problem asks for the maximum possible speedup, $S_{\\max}$, for an integer number of cores $N$ in the range $1 \\leq N \\leq N_{\\max}$, where $N_{\\max} = 32$.\nThe function $S(N) = \\min(N, 5)$ behaves as follows:\n- For $N  5$, $S(N) = N$. The speedup is linear and increases with $N$.\n- For $N \\ge 5$, $S(N) = 5$. The speedup saturates at a constant value of $5$.\n\nThe function $S(N)$ is a monotonically non-decreasing function of $N$. Its maximum value over the domain $1 \\leq N \\leq 32$ will be the value it reaches at saturation.\n- $S(1) = 1$\n- $S(2) = 2$\n- $S(3) = 3$\n- $S(4) = 4$\n- $S(5) = 5$\n- $S(6) = 5$\n- ...\n- $S(32) = 5$\n\nThe maximum value of $S(N)$ for $N$ in the given range is $5$. This maximum is first achieved with $N=5$ cores and is maintained for any greater number of cores up to $N_{\\max}=32$. This limitation on speedup is a classic example of Amdahl's Law, where the bottleneck (in this case, the system memory bus bandwidth $B$) limits the achievable parallel performance improvement.\n\nThe maximum possible speedup $S_{\\max}$ is therefore $5$.", "answer": "$$\\boxed{5}$$", "id": "3145387"}, {"introduction": "Building on our understanding of hardware bottlenecks, we now turn to the software strategies used to manage parallel work. The way iterations of a parallel loop are distributed among processor cores can have a profound impact on performance, especially when the work per iteration is uneven. This practice [@problem_id:3145384] presents a classic trade-off between static and dynamic scheduling, asking you to analyze how different strategies balance the goals of minimizing scheduling overhead and adapting to workload imbalances.", "problem": "Consider a loop with $N$ independent iterations to run on $p$ identical Central Processing Unit (CPU) cores using Open Multi-Processing (OpenMP). Each iteration $i$ has a runtime $T_i$ drawn independently from a heavy-tailed mixture: with probability $q$ it is a “heavy” iteration taking $t_H$ time units, and with probability $1 - q$ it is a “light” iteration taking $t_L$ time units. Assume $t_H \\gg t_L$ and that the mixture model is a reasonable surrogate for a heavy tail. The loop is scheduled either statically (each core gets a fixed contiguous block of iterations) or dynamically (each core repeatedly pulls the next available chunk of size $c$ from a shared queue until all iterations are done). Each dynamic chunk acquisition incurs a scheduling overhead $t_s$ on the acquiring core; static scheduling overhead is negligible.\n\nIn this specific scenario, take $N = 400$, $p = 4$, $q = 0.02$, $t_L = 1\\,\\mathrm{ms}$, $t_H = 100\\,\\mathrm{ms}$, and $t_s = 0.2\\,\\mathrm{ms}$. For dynamic scheduling, consider $c = 1$ and $c = 10$. Assume the runtime system is otherwise ideal: no contention beyond the per-chunk overhead $t_s$, and cores execute iterations at the stated times.\n\nUse the foundational definitions that govern parallel loop execution:\n- The makespan $T_{\\mathrm{par}}$ is the time until the last core finishes its assigned work.\n- Under perfect load balance and negligible overheads, $T_{\\mathrm{par}}$ is approximately the total work divided by $p$, that is $T_{\\mathrm{par}} \\approx \\left(\\sum_{i=1}^{N} T_i\\right)/p$.\n- Under static scheduling, each core receives exactly $N/p$ iterations, so its runtime equals the sum of $T_i$ over its assigned block; the makespan is determined by the core with the largest such sum.\n- Under dynamic scheduling, the runtime includes both the executed iteration times and the cumulative scheduling overhead of the acquired chunks; chunking affects both load balance granularity and overhead.\n\nBased on these definitions and the given parameters, which statements are most accurate about expected load balance and scheduling overhead trade-offs?\n\nA. Static scheduling achieves near-perfect load balance because each core gets exactly $N/p$ iterations, so its expected makespan is close to $\\left(\\mathbb{E}[T]\\,N\\right)/p$, and it outperforms dynamic scheduling with $c = 1$.\n\nB. Dynamic scheduling with $c = 1$ achieves significantly better expected load balance than static scheduling, so its expected makespan is closer to $\\left(\\sum_{i=1}^{N} T_i\\right)/p$, even after accounting for scheduling overhead $t_s$ per iteration.\n\nC. Dynamic scheduling with $c = 10$ reduces scheduling overhead by a factor of $10$ compared to $c = 1$ while preserving most of the load-balance benefits of dynamic scheduling, so its expected makespan is lower than with $c = 1$.\n\nD. Randomly permuting the iteration order before static assignment makes static scheduling effectively equivalent to dynamic scheduling with $c = 1$, yielding an expected makespan close to $\\left(\\sum_{i=1}^{N} T_i\\right)/p$ without overhead.\n\nSelect all that apply.", "solution": "We begin by establishing the baseline parameters for our analysis based on the provided data.\n- Total number of iterations: $N = 400$\n- Number of processor cores: $p = 4$\n- Probability of a heavy iteration: $q = 0.02$\n- Runtime of a light iteration: $t_L = 1\\,\\mathrm{ms}$\n- Runtime of a heavy iteration: $t_H = 100\\,\\mathrm{ms}$\n- Scheduling overhead for a dynamic chunk: $t_s = 0.2\\,\\mathrm{ms}$\n\nFirst, we calculate the expected total computational work, $W_{total}$. The expected number of heavy iterations is $N \\times q = 400 \\times 0.02 = 8$. The expected number of light iterations is $N \\times (1-q) = 400 \\times (1 - 0.02) = 392$.\nThe total expected work is the sum of the times for all iterations:\n$$W_{total} = (Nq) t_H + (N(1-q)) t_L = (8 \\times 100\\,\\mathrm{ms}) + (392 \\times 1\\,\\mathrm{ms}) = 800\\,\\mathrm{ms} + 392\\,\\mathrm{ms} = 1192\\,\\mathrm{ms}$$\nFor the purpose of comparing schedulers, we will treat this expected work as the actual work to be performed, i.e., $\\sum_{i=1}^{N} T_i = 1192\\,\\mathrm{ms}$.\n\nThe ideal makespan, $T_{ideal}$, assumes perfect load balance and zero overhead. It serves as a theoretical lower bound on the expected makespan.\n$$T_{ideal} = \\frac{W_{total}}{p} = \\frac{1192\\,\\mathrm{ms}}{4} = 298\\,\\mathrm{ms}$$\n\nNow, we analyze each scheduling strategy.\n\n**Static Scheduling**\nUnder static scheduling, the $N=400$ iterations are divided into $p=4$ contiguous blocks, with each core receiving $N/p = 100$ iterations. The overhead is negligible. The makespan is determined by the core that receives the block with the most work.\nThe workload is \"heavy-tailed\" because $t_H \\gg t_L$. This means the makespan is dominated by the distribution of the $8$ heavy iterations among the $4$ cores.\n- **Best case (perfect balance):** If the $8$ heavy iterations are distributed evenly, with each core receiving $8/4=2$ heavy iterations, the work for each core would be identical: $W_{core} = (2 \\times t_H) + ((100-2) \\times t_L) = (2 \\times 100) + (98 \\times 1) = 298\\,\\mathrm{ms}$. The makespan would be $T_{static} = 298\\,\\mathrm{ms}$, which is the ideal makespan.\n- **Likely/Worst case (imbalance):** The iterations' runtimes are independent, so there is no guarantee of an even distribution. It is highly probable that the distribution will be uneven. For instance, consider a plausible scenario where the distribution of heavy iterations is $(3, 2, 2, 1)$. The makespan will be determined by the core with $3$ heavy iterations: $W_{max} = (3 \\times 100) + (97 \\times 1) = 397\\,\\mathrm{ms}$. In a more skewed case like $(4, 2, 1, 1)$, the makespan would be $W_{max} = (4 \\times 100) + (96 \\times 1) = 496\\,\\mathrm{ms}$.\nBecause of the high probability of such imbalances, the *expected* makespan for static scheduling will be significantly higher than the ideal $T_{ideal} = 298\\,\\mathrm{ms}$.\n\n**Dynamic Scheduling with Chunk Size $c=1$**\nEach processor requests one iteration at a time from a central queue.\n- **Load Balance:** This fine-grained approach provides near-perfect load balancing. Whenever a core finishes an iteration (whether it be light or heavy), it immediately starts a new one. The work is distributed almost perfectly evenly among the cores.\n- **Overhead:** There is an overhead $t_s$ for each chunk acquired. Since $c=1$, there are $N=400$ chunks in total. The total scheduling overhead is $O_{c=1} = N \\times t_s = 400 \\times 0.2\\,\\mathrm{ms} = 80\\,\\mathrm{ms}$.\n- **Makespan:** The total effort is the computational work plus the overhead: $W_{total} + O_{c=1} = 1192\\,\\mathrm{ms} + 80\\,\\mathrm{ms} = 1272\\,\\mathrm{ms}$. Due to the excellent load balancing, the expected makespan is well-approximated by this total effort divided by the number of cores.\n$$E[T_{dyn, c=1}] \\approx \\frac{W_{total} + O_{c=1}}{p} = \\frac{1272\\,\\mathrm{ms}}{4} = 318\\,\\mathrm{ms}$$\nThis is higher than the ideal $298\\,\\mathrm{ms}$ due to overhead, but it is substantially better than the imbalanced static cases (e.g., $397\\,\\mathrm{ms}$ or $496\\,\\mathrm{ms}$).\n\n**Dynamic Scheduling with Chunk Size $c=10$**\nEach processor requests a chunk of $10$ iterations at a time.\n- **Load Balance:** The granularity is coarser than with $c=1$. This might lead to some load imbalance, but it is still far superior to static scheduling as there are $N/c = 400/10 = 40$ chunks to be distributed dynamically.\n- **Overhead:** The number of chunks is reduced by a factor of $10$. The total scheduling overhead is $O_{c=10} = (N/c) \\times t_s = 40 \\times 0.2\\,\\mathrm{ms} = 8\\,\\mathrm{ms}$. This is a significant reduction from the $80\\,\\mathrm{ms}$ overhead with $c=1$.\n- **Makespan:** The total effort is $W_{total} + O_{c=10} = 1192\\,\\mathrm{ms} + 8\\,\\mathrm{ms} = 1200\\,\\mathrm{ms}$. An idealized makespan for this case would be:\n$$E[T_{dyn, c=10}]_{ideal} = \\frac{W_{total} + O_{c=10}}{p} = \\frac{1200\\,\\mathrm{ms}}{4} = 300\\,\\mathrm{ms}$$\nThe actual makespan may be slightly higher due to the coarser granularity (a core might be idle at the end while another finishes a long-running chunk), but the reduction in overhead is substantial ($80\\,\\mathrm{ms}$ vs $8\\,\\mathrm{ms}$). The total overhead was reduced by $72\\,\\mathrm{ms}$, which reduces the average work per core by $72/4 = 18\\,\\mathrm{ms}$. For the $c=10$ case to be slower than the $c=1$ case, the load imbalance penalty would need to exceed this $18\\,\\mathrm{ms}$ gain. This is unlikely with $40$ chunks distributed among $4$ cores. Thus, we expect $E[T_{dyn, c=10}]  E[T_{dyn, c=1}]$.\n\nLet us now evaluate the given options based on this analysis.\n\n**A. Static scheduling achieves near-perfect load balance because each core gets exactly $N/p$ iterations, so its expected makespan is close to $\\left(\\mathbb{E}[T]\\,N\\right)/p$, and it outperforms dynamic scheduling with $c = 1$.**\nThis statement is flawed on multiple grounds. First, giving each core the same *number* of iterations does not ensure balanced *workload* when iteration times vary widely, as is the case here. This leads to poor load balance, not \"near-perfect\". Second, because of this poor load balance, the expected makespan is significantly *larger* than the ideal time $(\\mathbb{E}[T]\\,N)/p = 298\\,\\mathrm{ms}$. Third, our analysis shows static scheduling is expected to be slower (e.g., $397\\,\\mathrm{ms}$ or more) than dynamic with $c=1$ (approx. $318\\,\\mathrm{ms}$).\n**Verdict: Incorrect.**\n\n**B. Dynamic scheduling with $c = 1$ achieves significantly better expected load balance than static scheduling, so its expected makespan is closer to $\\left(\\sum_{i=1}^{N} T_i\\right)/p$, even after accounting for scheduling overhead $t_s$ per iteration.**\nThis statement is accurate. Dynamic scheduling with $c=1$ is designed to achieve excellent load balance, which is a significant improvement over static scheduling for this type of workload. The term $(\\sum_{i=1}^{N} T_i)/p$ represents the ideal average work per core, $T_{ideal}=298\\,\\mathrm{ms}$. While the dynamic scheduling makespan ($E[T_{dyn, c=1}] \\approx 318\\,\\mathrm{ms}$) includes overhead, the penalty from this overhead ($318 - 298 = 20\\,\\mathrm{ms}$) is much smaller than the penalty from the severe load imbalance expected with static scheduling (e.g., $397 - 298 = 99\\,\\mathrm{ms}$). Therefore, the makespan under dynamic scheduling is indeed much closer to the ideal parallel time than the makespan under static scheduling.\n**Verdict: Correct.**\n\n**C. Dynamic scheduling with $c = 10$ reduces scheduling overhead by a factor of $10$ compared to $c = 1$ while preserving most of the load-balance benefits of dynamic scheduling, so its expected makespan is lower than with $c = 1$.**\nThis statement accurately describes the trade-off. The overhead for $c=10$ is $8\\,\\mathrm{ms}$, and for $c=1$ it is $80\\,\\mathrm{ms}$, so the reduction is indeed a factor of $10$. While the load balancing is not as perfect as with $c=1$, it is still very effective compared to static scheduling, thus \"preserving most of the load-balance benefits\" is a fair description. The significant reduction in overhead ($72\\,\\mathrm{ms}$ total) leads to a lower total workload. As calculated, $E[T_{dyn, c=10}]$ is expected to be around $300\\,\\mathrm{ms}$, which is lower than $E[T_{dyn, c=1}] \\approx 318\\,\\mathrm{ms}$. The gain from reduced overhead outweighs the minor loss in load-balancing efficiency.\n**Verdict: Correct.**\n\n**D. Randomly permuting the iteration order before static assignment makes static scheduling effectively equivalent to dynamic scheduling with $c = 1$, yielding an expected makespan close to $(\\sum_{i=1}^{N} T_i)/p$ without overhead.**\nRandomly permuting the iterations before a static assignment helps to avoid worst-case scenarios where heavy iterations are clustered, but it does not eliminate the statistical likelihood of imbalance. One core can still be assigned more heavy iterations than others by chance. This is fundamentally different from dynamic scheduling, which *adapts* to the workload at runtime. Thus, it is not \"effectively equivalent\" to dynamic scheduling. Consequently, the makespan is still subject to significant variance and the expected makespan will be notably larger than the ideal $(\\sum_{i=1}^{N} T_i)/p$ due to this imbalance. The claim of equivalence is false.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{BC}$$", "id": "3145384"}]}