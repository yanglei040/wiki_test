{"hands_on_practices": [{"introduction": "The parallel prefix sum (or scan) is a cornerstone algorithm with wide applications, from sorting to solving recurrence relations. Analyzing its performance forces us to confront the fundamental differences between shared-memory and distributed-memory programming models. This practice guides you through a formal analysis using the work-span model, helping you connect algorithmic structure to performance by modeling communication and synchronization costs explicitly. By comparing a thread-based implementation with an MPI-based one, you will gain a quantitative understanding of how architectural differences influence the constant factors that often dominate real-world execution time [@problem_id:3169829].", "problem": "You are studying the parallel prefix sum (scan) over $N$ double-precision elements using two programming models: a thread-based shared-memory implementation and a process-based Message Passing Interface (MPI) implementation. Assume the associative operation is addition. Your goal is to compare the total work and the critical path length (span) under a simple cost model, and then quantify the constant-factor difference in the $\\Theta(\\log_2 P)$ span terms arising from the different coordination patterns.\n\nFundamental bases and assumptions:\n- A sequential scan over $N$ items performs $N-1$ additions.\n- In the fork-join model, the work $T_1$ is the total number of primitive operations performed across all processing elements, and the span $T_{\\infty}$ is the length of the longest chain of dependent operations (the critical path).\n- A balanced binary tree of $P$ leaves has height $\\log_2 P$.\n\nTwo implementations and their cost models:\n- Thread-based shared-memory implementation: Data are partitioned into $P$ contiguous blocks, one per thread. Threads compute local scans of their blocks, then perform an inter-thread scan on the $P$ block totals using a balanced tree up-sweep followed by a down-sweep, and finally add per-block offsets to local elements. Each tree level incurs one barrier of latency $b$ and a constant-time local combine cost $c$ on the critical path.\n- MPI-based implementation using recursive doubling: Each process first performs a local scan of its block, then participates in $\\log_2 P$ rounds of point-to-point message exchanges (recursive doubling) to compute the prefix of block totals. Each round incurs a message latency $L$, a per-byte transfer time $g$ for a payload of $s$ bytes (one partial sum), and a local combine cost $c$ on the critical path.\n\nIn comparing spans, focus on the coordination component that yields the $\\Theta(\\log_2 P)$ term. You may ignore the identical local per-block scan and offset-application contributions in both models when forming the ratio, because those contributions cancel and do not affect the $\\Theta(\\log_2 P)$ coordination term or its constant factors.\n\nGiven parameters:\n- Number of processing elements $P = 1024$.\n- Thread barrier latency $b = 0.3$ microseconds.\n- Local combine cost $c = 0.01$ microseconds.\n- MPI per-message latency $L = 4.8$ microseconds.\n- MPI inverse bandwidth $g = 0.003$ microseconds per byte.\n- Payload size $s = 8$ bytes.\n\nTasks:\n1) Using the core definitions above, characterize symbolically the leading-order work $T_1$ for each implementation in terms of $N$ and $P$, retaining any asymptotically non-negligible dependence on $P$ that differs between the two.\n2) Derive the asymptotic form and constant factors of the coordination span $T_{\\infty}$ as a function of $P$ for each implementation under the given cost model, justifying the $\\Theta(\\log_2 P)$ dependence from first principles.\n3) Using the given numerical parameters, compute the ratio $R$ of the coordination spans,\n$$\nR \\equiv \\frac{T_{\\infty}^{\\text{threads}}}{T_{\\infty}^{\\text{MPI}}}.\n$$\nRound your answer to four significant figures. Report $R$ as a pure number with no units.", "solution": "The problem requires an analysis of two parallel prefix sum implementations, one using shared-memory threads and the other using MPI. The analysis involves determining the total work ($T_1$) and the coordination span ($T_{\\infty}$) for each, followed by a numerical comparison of their coordination spans.\n\n### Task 1: Characterization of Work ($T_1$)\n\nThe work, $T_1$, represents the total number of additions performed across all processing elements. The input consists of $N$ elements, partitioned into $P$ blocks of size $N/P$.\n\n**1. Thread-based Shared-Memory Implementation:**\nThe algorithm proceeds in three stages. We calculate the work for each stage.\n- **Local Scans:** Each of the $P$ threads performs a sequential prefix sum on its local block of $N/P$ elements. A sequential scan on $k$ items takes $k-1$ additions. Thus, the work for one thread is $(N/P) - 1$. For all $P$ threads, the total work is $W_{\\text{local}} = P \\left( \\frac{N}{P} - 1 \\right) = N - P$.\n- **Inter-thread Scan:** A parallel scan is performed on the $P$ block totals (the last element of each local scan) using a balanced binary tree structure involving an up-sweep and a down-sweep (Blelloch scan). The work for a parallel scan on $P$ elements using this method is $2(P-1)$ additions. So, $W_{\\text{inter}} = 2(P-1)$.\n- **Offset Application:** Each of the threads from $1$ to $P-1$ adds its corresponding prefix sum of block totals to all $N/P$ elements in its local block. Thread $0$ requires no offset. This results in $(P-1)$ threads each performing $N/P$ additions. The total work is $W_{\\text{offset}} = (P-1)\\frac{N}{P} = N - \\frac{N}{P}$.\n\nThe total work for the thread-based model, $T_{1}^{\\text{threads}}$, is the sum of the work in all stages:\n$$T_{1}^{\\text{threads}} = W_{\\text{local}} + W_{\\text{inter}} + W_{\\text{offset}}$$\n$$T_{1}^{\\text{threads}} = (N-P) + 2(P-1) + \\left(N - \\frac{N}{P}\\right)$$\n$$T_{1}^{\\text{threads}} = 2N + P - \\frac{N}{P} - 2$$\n\n**2. MPI-based Implementation:**\nThis algorithm also has three stages.\n- **Local Scans:** This stage is identical to the thread-based model. The work is $W_{\\text{local}} = N - P$.\n- **Inter-process Scan (Recursive Doubling):** The prefix sums of the $P$ block totals are computed using a recursive doubling algorithm. This requires $\\log_2 P$ rounds of communication. In each round $k$ (where $k$ ranges from $0$ to $\\log_2 P - 1$), processes $i \\in [2^k, P-1]$ receive a partial sum and perform one addition. The number of additions in round $k$ is $P - 2^k$. The total work is the sum over all rounds:\n$$W_{\\text{inter}} = \\sum_{k=0}^{\\log_2 P - 1} (P - 2^k) = P \\sum_{k=0}^{\\log_2 P - 1} 1 - \\sum_{k=0}^{\\log_2 P - 1} 2^k = P\\log_2 P - (2^{\\log_2 P} - 1) = P\\log_2 P - P + 1$$\n- **Offset Application:** This stage is identical to the thread-based model. The work is $W_{\\text{offset}} = N - \\frac{N}{P}$.\n\nThe total work for the MPI-based model, $T_{1}^{\\text{MPI}}$, is:\n$$T_{1}^{\\text{MPI}} = W_{\\text{local}} + W_{\\text{inter}} + W_{\\text{offset}}$$\n$$T_{1}^{\\text{MPI}} = (N-P) + (P\\log_2 P - P + 1) + \\left(N - \\frac{N}{P}\\right)$$\n$$T_{1}^{\\text{MPI}} = 2N + P\\log_2 P - 2P - \\frac{N}{P} + 1$$\n\n### Task 2: Derivation of Coordination Span ($T_{\\infty}$)\n\nThe span, $T_{\\infty}$, represents the critical path length. We are asked to analyze the coordination component only.\n\n**1. Thread-based Coordination Span:**\nThe coordination phase consists of an up-sweep and a down-sweep on a balanced binary tree of height $\\log_2 P$.\n- **Up-sweep:** The critical path goes from a leaf to the root, traversing $\\log_2 P$ levels. Each level requires synchronization (a barrier of latency $b$) followed by a local combine operation (cost $c$). The span for one level is $b+c$. Thus, the span for the up-sweep is $(\\log_2 P)(b+c)$.\n- **Down-sweep:** The critical path goes from the root to a leaf, again traversing $\\log_2 P$ levels. Each level incurs the same cost of $b+c$. The span for the down-sweep is also $(\\log_2 P)(b+c)$.\n\nThe total coordination span, $T_{\\infty}^{\\text{threads}}$, is the sum of the spans of the two sequential phases:\n$$T_{\\infty}^{\\text{threads}} = (\\log_2 P)(b+c) + (\\log_2 P)(b+c) = 2(b+c)\\log_2 P$$\nThis has the required $\\Theta(\\log_2 P)$ dependence, justified by the traversal of a tree of depth $\\log_2 P$ twice, once up and once down.\n\n**2. MPI-based Coordination Span:**\nThe coordination phase uses recursive doubling, which consists of $\\log_2 P$ sequential rounds.\n- **Span of one round:** In each round, a critical-path process waits for a message, receives it, and performs a local combination. The time for this is the sum of the message latency $L$, the transfer time $g \\cdot s$ (where $s$ is the payload size and $g$ is the inverse bandwidth), and the local combine cost $c$. So, the span per round is $L+gs+c$.\n\nThe total coordination span, $T_{\\infty}^{\\text{MPI}}$, is the span per round multiplied by the number of rounds:\n$$T_{\\infty}^{\\text{MPI}} = (L+gs+c)\\log_2 P$$\nThis has the required $\\Theta(\\log_2 P)$ dependence, justified by the $\\log_2 P$ sequential stages of communication inherent to the recursive doubling algorithm.\n\n### Task 3: Computation of the Ratio $R$\n\nThe ratio $R$ of the coordination spans is defined as:\n$$R = \\frac{T_{\\infty}^{\\text{threads}}}{T_{\\infty}^{\\text{MPI}}}$$\nSubstituting the expressions derived in Task 2:\n$$R = \\frac{2(b+c)\\log_2 P}{(L+gs+c)\\log_2 P}$$\nThe $\\log_2 P$ terms cancel, yielding:\n$$R = \\frac{2(b+c)}{L+gs+c}$$\nNow, we substitute the given numerical parameters:\n- $b = 0.3 \\, \\mu s$\n- $c = 0.01 \\, \\mu s$\n- $L = 4.8 \\, \\mu s$\n- $g = 0.003 \\, \\mu s / \\text{byte}$\n- $s = 8 \\, \\text{bytes}$\n\nThe numerator is:\n$$2(b+c) = 2(0.3 + 0.01) = 2(0.31) = 0.62$$\nThe denominator is:\n$$L+gs+c = 4.8 + (0.003 \\times 8) + 0.01 = 4.8 + 0.024 + 0.01 = 4.834$$\nThe ratio $R$ is:\n$$R = \\frac{0.62}{4.834} \\approx 0.1282579644$$\nRounding to four significant figures, we get:\n$$R \\approx 0.1283$$", "answer": "$$\\boxed{0.1283}$$", "id": "3169829"}, {"introduction": "Launching more threads than available hardware cores—a practice known as oversubscription—can severely degrade performance due to system-level overheads. This exercise explores the trade-offs between parallelism and the costs of context switching, thread migration, and cache contention. By implementing a computational model, you will quantify the slowdown caused by these effects and understand the benefits of thread affinity, which binds threads to specific cores. This practice provides a concrete framework for reasoning about one of the most common performance pitfalls and developing strategies to choose an optimal number of threads [@problem_id:3169824].", "problem": "You are asked to design and implement a self-contained computational experiment that models the impact of oversubscription in a threading context and the role of thread affinity on performance. The model must be based on fundamental definitions of throughput, speedup, and scheduling, and must not rely on properties of any particular programming language runtime. Your experiment must quantify slowdown when the number of threads exceeds available cores and propose an adaptive strategy to cap the number of threads to avoid oversubscription.\n\nFundamental basis, definitions, and modeling assumptions:\n- A Central Processing Unit (CPU) with $c$ cores can run up to $c$ independent threads simultaneously without preemptive time sharing.\n- A thread performs $W$ units of work, where each unit takes $1$ abstract time unit when executed uninterrupted on a dedicated core. The total workload is $t$ identical threads, each with $W$ units.\n- Speedup is defined as $S(t) = T(1) \\,/\\, T(t)$, where $T(t)$ is the makespan (the time to finish all $t$ threads).\n- The ideal batch makespan under perfect scaling and no overhead is $$T_{\\text{ideal}}(t,c,W) = \\max\\!\\left(W, \\frac{t\\,W}{c}\\right).$$ This follows from the observation that when $t \\le c$ all threads can run concurrently and finish in $W$, and when $t > c$ the CPU time is shared across $c$ cores, yielding a lower bound of $(t\\,W)/c$ for total completion time under perfect pipelining.\n- Under time-sliced scheduling, each active thread is given a quantum of $q$ work units per slice, incurring a context-switch overhead of $s$ time units whenever switching between different threads on the same core. When only a single thread runs on a core (i.e., no oversubscription on that core), assume no context switch overhead ($s$ applies only when $k > 1$), where $k$ is the number of threads assigned to a core.\n- With thread affinity (pinned scheduling), each thread is assigned to a fixed core. If $t > c$, each core hosts $k = \\lceil t / c \\rceil$ threads and cycles through them round-robin. The number of slices per thread is $L = \\lceil W / q \\rceil$.\n- Without thread affinity (unpinned scheduling), threads may migrate between cores. Migration introduces additional expected overhead per slice of $p \\cdot a$ time units, where $p$ is the probability of a migration per slice and $a$ is the cost of a migration. Unpinned scheduling also degrades effective per-slice execution due to lost locality and contention. Model this cache and contention penalty as a multiplicative factor $$r(t,c,\\beta) = 1 + \\beta \\cdot \\max\\!\\left(\\frac{t}{c} - 1,\\, 0\\right),$$ where $\\beta \\ge 0$ is a given penalty coefficient. The effective time to execute $q$ work units in a slice becomes $q \\cdot r(t,c,\\beta)$.\n- The pinned makespan is modeled by $$T_{\\text{pinned}}(t,c,W,q,s) \\approx L \\cdot k \\cdot \\big(q + s_{\\text{eff}}\\big),$$ where $k = \\lceil t / c \\rceil$, $L = \\lceil W / q \\rceil$, and $s_{\\text{eff}} = s$ if $k > 1$ and $s_{\\text{eff}} = 0$ otherwise.\n- The unpinned makespan is modeled by $$T_{\\text{unpinned}}(t,c,W,q,s,p,a,\\beta) \\approx L \\cdot k \\cdot \\big(q \\cdot r(t,c,\\beta) + s_{\\text{eff}} + p \\cdot a\\big),$$ using the same $k$, $L$, and $s_{\\text{eff}}$.\n- Define slowdown relative to the ideal batch completion time by $$\\text{slowdown}(t) = \\frac{T_{\\text{actual}}(t)}{T_{\\text{ideal}}(t,c,W)}.$$ This dimensionless quantity captures inefficiency due to oversubscription and overhead.\n\nTasks:\n- Implement the above model to compute, for each test case, the pinned slowdown and the unpinned slowdown at the provided thread count $t$.\n- Propose an adaptive capping strategy: given a maximum allowable thread count $t_{\\max}$ and fixed parameters $(c,W,q,s,p,a,\\beta)$, choose $$t^{\\star} = \\operatorname*{arg\\,min}_{u \\in \\{1,2,\\dots,t_{\\max}\\}} T_{\\text{unpinned}}(u,c,W,q,s,p,a,\\beta).$$ Report the recommended cap as $\\min(c, t^{\\star})$ to avoid oversubscription by design.\n- Your program must implement these computations in a deterministic way and produce the outputs described below.\n\nUnits and numerical representation:\n- All times are in abstract time units based on the above definitions. Slowdowns are dimensionless.\n- Report all floating-point slowdowns rounded to $6$ decimal places.\n\nTest suite:\nProvide results for the following parameter sets, which cover a general case, boundary conditions, and edge cases. Each test case is a tuple $(c,t,W,q,s,p,a,\\beta)$.\n\n- Case $1$ (happy path, moderate oversubscription): $(c,t,W,q,s,p,a,\\beta) = (4,8,1000,50,2,0.1,5,0.05)$.\n- Case $2$ (boundary, $t = c$): $(c,t,W,q,s,p,a,\\beta) = (4,4,1000,50,2,0.1,5,0.05)$.\n- Case $3$ (heavy oversubscription): $(c,t,W,q,s,p,a,\\beta) = (4,32,1000,20,2,0.3,5,0.1)$.\n- Case $4$ (serial baseline): $(c,t,W,q,s,p,a,\\beta) = (4,1,1000,50,2,0.0,5,0.05)$.\n- Case $5$ (single core with oversubscription): $(c,t,W,q,s,p,a,\\beta) = (1,8,1000,50,2,0.1,5,0.05)$.\n\nRequired final output format:\n- For each test case, output a list of three values: $[\\text{slowdown}_{\\text{pinned}}, \\text{slowdown}_{\\text{unpinned}}, t_{\\text{cap}}]$, where the slowdowns are floats rounded to $6$ decimal places, and $t_{\\text{cap}}$ is an integer given by $\\min(c,t^{\\star})$ with $t^{\\star}$ computed over $u \\in \\{1,2,\\dots,t\\}$ for that case.\n- Aggregate all test case results into a single list in the given order. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[[x_1,y_1,z_1],[x_2,y_2,z_2],\\dots]$ with no extra whitespace or text.", "solution": "The posed problem requires the design and implementation of a computational experiment to model and quantify the performance effects of thread oversubscription and scheduling policies (pinned vs. unpinned). The analysis is to be based on a set of provided mathematical models for execution time (makespan). The problem is well-defined, scientifically grounded in the principles of computational performance modeling, and internally consistent. All variables, constants, and functional relationships are explicitly provided, enabling a direct and formal implementation. The approximate nature of the models, indicated by `$\\approx$`, is understood to mean that for the purposes of this problem, the expressions are to be treated as exact equalities.\n\nThe solution proceeds in three logical steps:\n$1$. Implementing the mathematical models for ideal, pinned, and unpinned makespans.\n$2$. Computing the slowdown for both pinned and unpinned scheduling relative to the ideal makespan for each specified test case.\n$3$. Determining a recommended thread cap, $t_{\\text{cap}}$, by finding the thread count $t^{\\star}$ that minimizes the unpinned makespan and then constraining this value by the number of cores $c$.\n\nFirst, we formalize the component functions based on the provided definitions. Let $c$ be the number of cores, $t$ the number of threads, $W$ the work per thread, $q$ the time quantum, and $s$ the context-switch overhead.\n\nThe number of slices per thread, $L$, is the total work $W$ divided by the work per slice $q$, rounded up to the nearest integer:\n$$L = \\lceil W / q \\rceil$$\nThe maximum number of threads assigned to any single core, $k$, assuming an even distribution, is:\n$$k = \\lceil t / c \\rceil$$\nThe effective context-switch overhead, $s_{\\text{eff}}$, applies only when a core manages more than one thread ($k > 1$):\n$$s_{\\text{eff}} = \\begin{cases} s & \\text{if } k > 1 \\\\ 0 & \\text{if } k \\le 1 \\end{cases}$$\n\nWith these fundamental quantities, we can define the makespan models. The ideal makespan, $T_{\\text{ideal}}$, represents the theoretical minimum time, assuming perfect parallelism and no overhead. It is the greater of the single-thread execution time ($W$) and the time if all work ($t \\cdot W$) were perfectly distributed across $c$ cores:\n$$T_{\\text{ideal}}(t,c,W) = \\max\\left(W, \\frac{t \\cdot W}{c}\\right)$$\n\nThe makespan for pinned scheduling, $T_{\\text{pinned}}$, where each thread is fixed to a core, is determined by the most heavily loaded core. This core runs $k$ threads in a round-robin fashion. The total time is the number of slices per thread ($L$) multiplied by the number of threads on the core ($k$) times the duration of a slice, which includes execution ($q$) and potential context-switching ($s_{\\text{eff}}$):\n$$T_{\\text{pinned}}(t,c,W,q,s) = L \\cdot k \\cdot (q + s_{\\text{eff}})$$\n\nThe makespan for unpinned scheduling, $T_{\\text{unpinned}}$, incorporates additional overheads. Let $p$ be the migration probability, $a$ the migration cost, and $\\beta$ the contention penalty coefficient. A multiplicative penalty factor, $r$, models performance degradation due to lost cache locality and contention when threads exceed cores:\n$$r(t,c,\\beta) = 1 + \\beta \\cdot \\max\\left(\\frac{t}{c} - 1, 0\\right)$$\nThe unpinned makespan includes the costs of slice execution scaled by contention ($q \\cdot r$), context switching ($s_{\\text{eff}}$), and expected migration overhead ($p \\cdot a$):\n$$T_{\\text{unpinned}}(t,c,W,q,s,p,a,\\beta) = L \\cdot k \\cdot (q \\cdot r(t,c,\\beta) + s_{\\text{eff}} + p \\cdot a)$$\n\nThe performance degradation is quantified by the slowdown, defined as the ratio of the actual makespan to the ideal makespan:\n$$\\text{slowdown}(t) = \\frac{T_{\\text{actual}}(t)}{T_{\\text{ideal}}(t,c,W)}$$\nWe will compute this for both $T_{\\text{pinned}}$ and $T_{\\text{unpinned}}$.\n\nFinally, we must propose an adaptive threading cap. This requires finding the optimal number of threads, $t^{\\star}$, that minimizes the unpinned makespan for a given set of system parameters. The search is conducted over the range of a an allowable thread count, $u \\in \\{1, 2, \\dots, t_{\\max}\\}$, where $t_{\\max}$ is specified as the thread count $t$ from the corresponding test case.\n$$t^{\\star} = \\operatorname*{arg\\,min}_{u \\in \\{1, 2, \\dots, t\\}} T_{\\text{unpinned}}(u,c,W,q,s,p,a,\\beta)$$\nAn analytical examination of the $T_{\\text{unpinned}}$ model reveals that for $u \\in \\{1, \\dots, c\\}$, the makespan is constant, as overheads from oversubscription ($s$ and the $\\beta$-dependent term) are zero. For $u > c$, both the factor $k = \\lceil u/c \\rceil$ and the per-slice time increase, leading to a monotonically increasing makespan. Consequently, the minimum time occurs for any $u \\in \\{1, \\dots, c\\}$, and the `argmin` is $t^{\\star}=1$. The recommended cap, $t_{\\text{cap}}$, is designed to prevent oversubscription by taking the minimum of $c$ and this optimal value, $t^{\\star}$:\n$$t_{\\text{cap}} = \\min(c, t^{\\star})$$\n\nThe computational procedure involves implementing these functions, iterating through each test case tuple $(c,t,W,q,s,p,a,\\beta)$, calculating the pinned and unpinned slowdowns at the given $t$, determining $t_{\\text{cap}}$ via the minimization search, and reporting the three resulting values: $[\\text{slowdown}_{\\text{pinned}}, \\text{slowdown}_{\\text{unpinned}}, t_{\\text{cap}}]$. All floating-point results are rounded to $6$ decimal places as required.", "answer": "[[1.040000,1.100000,1],[1.000000,1.010000,1],[1.100000,1.875000,1],[1.000000,1.000000,1],[1.040000,1.400000,1]]", "id": "3169824"}, {"introduction": "Lock-free algorithms promise high performance by avoiding the overhead and contention of traditional locks, but they introduce subtle failure modes like livelock. Livelock occurs when threads are perpetually active but make no forward progress, often due to repeated conflicts. This exercise models this phenomenon in a lock-free queue and explores a standard mitigation technique: probabilistic backoff. Through this analytical exercise, you will derive the expected system throughput as a function of contention, gaining insight into the delicate balance required for scalable concurrent systems and demonstrating how a probabilistic approach can solve complex contention problems [@problem_id:3169794].", "problem": "Consider a multi-producer single-consumer lock-free queue implemented with a pointer-based linked list. Each enqueue operation appends a node by updating a shared tail pointer using a hardware primitive Compare-And-Swap (CAS), defined as follows: Compare-And-Swap (CAS) takes a memory address and two values, and atomically assigns the memory address to the new value if its current contents equal the expected old value, returning whether it succeeded. Assume that each CAS attempt that overlaps in time with another CAS attempt on the same tail pointer fails due to contention, and that successful CAS updates complete one enqueue.\n\nConstruct a minimal livelock scenario under high contention as follows. Two threads, labeled $A$ and $B$, attempt to enqueue simultaneously. Each reads the current tail pointer, prepares its new node, and issues a CAS to update the tail pointer to that node. Because these CAS operations overlap, both fail. Without any backoff or delay, both threads immediately retry, read the same tail, and again issue CAS at nearly the same time, repeatedly failing. This repeated mutual interference without progress constitutes livelock.\n\nTo mitigate livelock, propose a probabilistic backoff strategy: after a failed CAS, a thread defers its next attempt by choosing a random delay measured in discrete time slots, so that in any given slot it attempts CAS independently with probability $r$ that can be adjusted by the backoff policy and adapts in response to contention. Assume $N$ identical producer threads, time divided into equal slots, and that attempts in each slot are independent across threads.\n\nDefine the contention probability $p$ as the probability that, conditioned on a given thread attempting CAS in a slot, at least one other thread also attempts CAS in that same slot (thus causing a collision for that attempt if it occurs). Under the independence and identical-attempt-probability assumptions, there is a fixed relationship between the per-thread attempt probability $r$ and the contention probability $p$.\n\nStarting only from the definitions above and standard probability laws for independent Bernoulli trials, derive a closed-form analytic expression for the expected enqueue throughput per slot, $T(p)$, as a function of $p$ and $N$. Express your final answer symbolically as a single simplified expression. Do not provide any numerical approximation or rounding.", "solution": "The user wants me to solve the problem.\nI need to first validate the problem statement.\n\n**Step 1: Extract Givens**\n- System: A multi-producer single-consumer lock-free queue implemented with a pointer-based linked list.\n- Enqueue mechanism: Appending a node by updating a shared tail pointer using a Compare-And-Swap (CAS) hardware primitive.\n- Contention model: A CAS attempt fails if it overlaps in time with another CAS on the same pointer. A successful CAS completes one enqueue.\n- Livelock example: Two threads repeatedly and simultaneously attempt CAS, causing mutual interference and repeated failure without progress.\n- Mitigation strategy: A probabilistic backoff where a thread, after a failed CAS, attempts again in any given subsequent time slot with an independent probability $r$.\n- System parameters: $N$ identical producer threads.\n- Time model: Time is divided into discrete, equal slots.\n- Independence assumption: Attempts in each slot are independent across threads.\n- Definition of contention probability, $p$: \"the probability that, conditioned on a given thread attempting CAS in a slot, at least one other thread also attempts CAS in that same slot\".\n- Desired result: A closed-form analytic expression for the expected enqueue throughput per slot, $T(p)$, as a function of $p$ and $N$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is firmly grounded in computer science, specifically in the analysis of concurrent algorithms. Lock-free queues, Compare-And-Swap (CAS), livelock, and probabilistic backoff are standard concepts. The use of a probabilistic model based on Bernoulli trials to analyze system throughput is a classic and valid technique in performance engineering.\n- **Well-Posed:** The problem provides a clear definition of all variables ($N, r, p$), the system's behavior, and the quantity to be derived ($T(p)$). The relationship between $r$ and $p$ is explicitly stated to be fixed and derivable from the assumptions, making the problem self-contained and solvable. A unique, meaningful solution is expected.\n- **Objective:** The problem is stated in precise, technical language, free from ambiguity or subjective claims.\n\nThe problem does not violate any of the invalidity criteria. It is a standard, well-defined problem in the performance analysis of concurrent systems.\n\n**Step 3: Verdict and Action**\n- The problem is **valid**.\n- I will now proceed with the derivation of the solution.\n\nThe objective is to derive the expected enqueue throughput per slot, $T$, which is the expected number of successful enqueue operations per time slot.\n\nAccording to the problem description, a successful enqueue occurs if and only if exactly one thread attempts a CAS operation in a given time slot. If zero threads attempt, there is no enqueue. If two or more threads attempt, their CAS operations collide, and all fail, resulting in no enqueue.\n\nLet $r$ be the probability that a single thread attempts a CAS in a given slot. We have $N$ identical and independent threads. Let $K$ be the random variable representing the number of threads that attempt a CAS in a given slot. Since each thread's attempt is an independent Bernoulli trial with success probability $r$, the total number of attempts $K$ follows a binomial distribution with parameters $N$ and $r$.\nThe probability mass function of $K$ is given by:\n$$P(K=k) = \\binom{N}{k} r^k (1-r)^{N-k}$$\nfor $k \\in \\{0, 1, \\dots, N\\}$.\n\nThe throughput $T$ is the expected number of successful enqueues. A success occurs only when $K=1$. Therefore, the expected throughput is:\n$$T = 1 \\cdot P(K=1) + 0 \\cdot P(K \\neq 1) = P(K=1)$$\nSubstituting $k=1$ into the binomial probability mass function, we express the throughput as a function of $r$ and $N$:\n$$T(r) = \\binom{N}{1} r^1 (1-r)^{N-1} = N r (1-r)^{N-1}$$\n\nThe problem requires the throughput $T$ to be expressed as a function of the contention probability $p$ and $N$. We must therefore establish a relationship between $r$ and $p$.\n\nThe contention probability $p$ is defined as \"the probability that, conditioned on a given thread attempting CAS in a slot, at least one other thread also attempts CAS in that same slot\".\n\nLet's consider a specific thread, say thread $i$. The conditioning event is \"$A_i$: thread $i$ attempts CAS\". The event of interest is \"$C_i$: at least one of the other $N-1$ threads attempts CAS\".\nSo, by definition, $p = P(C_i | A_i)$.\nSince the threads' attempts are independent, the event $A_i$ provides no information about the behavior of the other $N-1$ threads. Therefore, the conditional probability is equal to the unconditional probability:\n$$p = P(C_i | A_i) = P(C_i)$$\nThe event $C_i$ is that at least one of the other $N-1$ threads attempts a CAS. It is simpler to calculate the probability of the complement event, $\\neg C_i$, which is that none of the other $N-1$ threads attempt a CAS.\nEach of these $N-1$ threads attempts with probability $r$ and does not attempt with probability $1-r$. The probability that a single one of these threads does not attempt is $1-r$. Since they are all independent, the probability that none of them attempt is:\n$$P(\\neg C_i) = (1-r)^{N-1}$$\nThe probability $p$ is then:\n$$p = P(C_i) = 1 - P(\\neg C_i) = 1 - (1-r)^{N-1}$$\nThis provides the required relationship between $p$ and $r$. The a priori assumption $N \\ge 2$ (implied by \"multi-producer\" and the livelock scenario) ensures that $N-1 \\ge 1$, making this expression well-defined.\n\nNow we can express $T$ in terms of $p$ and $N$. We have the two equations:\n$$1) \\quad T = N r (1-r)^{N-1}$$\n$$2) \\quad p = 1 - (1-r)^{N-1}$$\n\nFrom equation (2), we can directly express the term $(1-r)^{N-1}$ in terms of $p$:\n$$(1-r)^{N-1} = 1-p$$\nNext, we solve equation (2) for $r$.\n$$(1-r)^{N-1} = 1-p$$\n$$1-r = (1-p)^{\\frac{1}{N-1}}$$\n$$r = 1 - (1-p)^{\\frac{1}{N-1}}$$\n\nNow, we substitute these expressions for $r$ and $(1-r)^{N-1}$ back into the equation for $T$:\n$$T(p) = N \\cdot r \\cdot (1-r)^{N-1} = N \\left( 1 - (1-p)^{\\frac{1}{N-1}} \\right) (1-p)$$\n\nThis is the closed-form analytic expression for the expected throughput per slot, $T(p)$, as a function of the contention probability $p$ and the number of threads $N$. The expression is simplified as requested.", "answer": "$$\\boxed{N(1-p)\\left(1 - (1-p)^{\\frac{1}{N-1}}\\right)}$$", "id": "3169794"}]}