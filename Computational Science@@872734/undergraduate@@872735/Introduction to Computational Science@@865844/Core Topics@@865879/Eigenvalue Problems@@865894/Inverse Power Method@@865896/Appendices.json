{"hands_on_practices": [{"introduction": "The inverse power method is an elegant iterative algorithm, but at its heart, it relies on a single, repeated algebraic operation. This first exercise [@problem_id:1395843] isolates that core step: solving the linear system $(A-\\sigma I)y_{k+1} = x_k$. Mastering this calculation is the first step toward understanding how the method iteratively refines an initial guess into a precise eigenvector.", "problem": "In a numerical algorithm, a sequence of vectors is generated starting from an initial vector $x_0$. The first unnormalized vector in this sequence, denoted as $y_1$, is found by solving the linear system $(A-\\sigma I)y_1 = x_0$, where $A$ is a square matrix, $\\sigma$ is a scalar shift, and $I$ is the identity matrix of the same dimension as $A$.\n\nGiven the matrix $A = \\begin{pmatrix} 3  -1 \\\\ -1  3 \\end{pmatrix}$, the shift $\\sigma = 1.5$, and the initial vector $x_0 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$, determine the components of the vector $y_1$. Present your answer as a row matrix where each component is an exact fraction or decimal.", "solution": "We are asked to solve the linear system $(A-\\sigma I) y_{1} = x_{0}$ for $y_{1}$, where $A = \\begin{pmatrix} 3  -1 \\\\ -1  3 \\end{pmatrix}$, $\\sigma = \\frac{3}{2}$, and $x_{0} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\n\nFirst compute the shifted matrix:\n$$\nA - \\sigma I = \\begin{pmatrix} 3  -1 \\\\ -1  3 \\end{pmatrix} - \\frac{3}{2} \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{2}  -1 \\\\ -1  \\frac{3}{2} \\end{pmatrix}.\n$$\nDenote $M = A - \\sigma I$. Then $y_{1} = M^{-1} x_{0}$. For a $2 \\times 2$ matrix $M = \\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix}$, we use $M^{-1} = \\frac{1}{\\det(M)} \\begin{pmatrix} d  -b \\\\ -c  a \\end{pmatrix}$. Here $a = d = \\frac{3}{2}$ and $b = c = -1$, so\n$$\n\\det(M) = \\left(\\frac{3}{2}\\right)\\left(\\frac{3}{2}\\right) - (-1)(-1) = \\frac{9}{4} - 1 = \\frac{5}{4},\n$$\nand\n$$\n\\operatorname{adj}(M) = \\begin{pmatrix} \\frac{3}{2}  1 \\\\ 1  \\frac{3}{2} \\end{pmatrix}.\n$$\nTherefore,\n$$\nM^{-1} = \\frac{1}{\\frac{5}{4}} \\begin{pmatrix} \\frac{3}{2}  1 \\\\ 1  \\frac{3}{2} \\end{pmatrix} = \\frac{4}{5} \\begin{pmatrix} \\frac{3}{2}  1 \\\\ 1  \\frac{3}{2} \\end{pmatrix}.\n$$\nMultiplying by $x_{0}$,\n$$\ny_{1} = M^{-1} x_{0} = \\frac{4}{5} \\begin{pmatrix} \\frac{3}{2}  1 \\\\ 1  \\frac{3}{2} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\frac{4}{5} \\begin{pmatrix} \\frac{3}{2} \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{6}{5} \\\\ \\frac{4}{5} \\end{pmatrix}.\n$$\nThus the components of $y_{1}$ are $\\frac{6}{5}$ and $\\frac{4}{5}$, which we present as a row matrix.", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{6}{5}  \\frac{4}{5} \\end{pmatrix}}$$", "id": "1395843"}, {"introduction": "Before we can fully appreciate the power of using a strategic shift, it's essential to understand what the inverse power method does in its most basic, unshifted form. This problem [@problem_id:1395849] presents a conceptual scenario to test your intuition about the method's convergence. By analyzing the eigenvalues of a system, you can predict the outcome of the algorithm, revealing its inherent tendency to find the eigenvector associated with the eigenvalue of smallest magnitude.", "problem": "A computational physicist is modeling the stability of a quantum system. The system's Hamiltonian is represented by a large, complex matrix $H$. While the full matrix is difficult to work with, a theoretical analysis has revealed that its four distinct energy eigenvalues are $\\{-6.5, -1.2, 0.9, 4.8\\}$, measured in some arbitrary energy units.\n\nTo find the ground state eigenvector of the system, which corresponds to the eigenvalue with the lowest energy, the physicist considers using an iterative numerical method. However, due to a misunderstanding, they implement the standard (unshifted) inverse power method on the matrix $H$. They use a randomly generated initial vector that has a non-zero component in the direction of each of the eigenvectors.\n\nAssuming the numerical method converges without issues of precision or floating-point errors, the eigenvector it finds will correspond to which of the given eigenvalues?\n\nA. $-6.5$\n\nB. $-1.2$\n\nC. $0.9$\n\nD. $4.8$", "solution": "Let $H$ be diagonalizable with distinct eigenvalues $\\{\\lambda_{i}\\}$ and corresponding eigenvectors $\\{v_{i}\\}$. Let the initial vector be $x_{0}=\\sum_{i} c_{i} v_{i}$ with $c_{i} \\neq 0$ for all $i$.\n\nThe standard power method applied to a matrix $A$ converges (under the usual nondefectiveness and separation assumptions) to the eigenvector associated with the eigenvalue of $A$ having largest magnitude. The inverse power method on $H$ is precisely the power method applied to $H^{-1}$.\n\nIf $\\lambda_{i}$ are the eigenvalues of $H$, then the eigenvalues of $H^{-1}$ are\n$$\n\\mu_{i}=\\frac{1}{\\lambda_{i}},\n$$\nwith the same eigenvectors $v_{i}$. After $k$ steps of inverse iteration (ignoring normalization for clarity), one has\n$$\n(H^{-1})^{k} x_{0}=(H^{-1})^{k} \\sum_{i} c_{i} v_{i}=\\sum_{i} c_{i} \\left(\\frac{1}{\\lambda_{i}}\\right)^{k} v_{i}.\n$$\nAs $k \\to \\infty$, the term with the largest $|\\mu_{i}|=|1/\\lambda_{i}|$ dominates, i.e., the term with the smallest $|\\lambda_{i}|$. Therefore, the unshifted inverse power method converges to the eigenvector corresponding to the eigenvalue of $H$ with minimal absolute value.\n\nGiven the eigenvalues $\\{-6.5, -1.2, 0.9, 4.8\\}$, their absolute values are $\\{6.5, 1.2, 0.9, 4.8\\}$. The smallest absolute value is $0.9$, hence the method converges to the eigenvector associated with $\\lambda=0.9$.\n\nTherefore, among the options provided, the correct choice is C.", "answer": "$$\\boxed{C}$$", "id": "1395849"}, {"introduction": "The true power of this method comes from using a shift, $\\sigma$, to target specific eigenvalues. In theory, the iteration converges to the eigenvector whose eigenvalue is closest to $\\sigma$. This exercise [@problem_id:1395866] explores a critical exception to this rule, demonstrating that the composition of the initial vector is just as important as the choice of shift. Understanding this interplay is key to effectively applying the method in practice and troubleshooting unexpected results.", "problem": "Consider the real-valued matrix $A$ given by:\n$$A = \\begin{pmatrix} 2.5  -1.5  0 \\\\ -1.5  2.5  0 \\\\ 0  0  5 \\end{pmatrix}$$\nThis matrix has eigenvalues $\\lambda_1 = 1$, $\\lambda_2 = 4$, and $\\lambda_3 = 5$. The corresponding un-normalized eigenvectors are $v_1 = [1, 1, 0]^T$, $v_2 = [1, -1, 0]^T$, and $v_3 = [0, 0, 1]^T$, respectively.\n\nThe inverse power method is used to find an eigenvalue-eigenvector pair of $A$. The method starts with an initial vector $x_0$ and uses a shift $\\sigma$. The iterative step is defined by solving $(A - \\sigma I) y_{k+1} = x_k$ for $y_{k+1}$, and then normalizing to get the next vector $x_{k+1} = y_{k+1} / \\|y_{k+1}\\|$. As $k \\to \\infty$, the vector $x_k$ converges to an eigenvector of $A$.\n\nSuppose the method is executed with a shift of $\\sigma = 0.9$ and an initial vector of $x_0 = [1, -1, 1]^T$. The sequence of vectors $\\{x_k\\}$ will converge to an eigenvector of $A$. Determine the eigenvalue corresponding to this eigenvector.", "solution": "The inverse power method with shift $\\sigma$ applies the iteration\n$$\n(A-\\sigma I) y_{k+1} = x_{k}, \\quad x_{k+1} = \\frac{y_{k+1}}{\\|y_{k+1}\\|}.\n$$\nLet the eigenpairs of $A$ be $(\\lambda_{i}, v_{i})$ for $i \\in \\{1,2,3\\}$, with $\\lambda_{1}=1$, $\\lambda_{2}=4$, $\\lambda_{3}=5$ and $v_{1} = [1, 1, 0]^{T}$, $v_{2} = [1, -1, 0]^{T}$, $v_{3} = [0, 0, 1]^{T}$.\n\nDecompose the initial vector in the eigenbasis:\n$$\nx_{0} = \\alpha_{1} v_{1} + \\alpha_{2} v_{2} + \\alpha_{3} v_{3}.\n$$\nMatching components, for the first two entries we solve\n$$\n\\alpha_{1} + \\alpha_{2} = 1, \\quad \\alpha_{1} - \\alpha_{2} = -1,\n$$\nwhich yields $\\alpha_{1} = 0$, $\\alpha_{2} = 1$. For the third entry we have $\\alpha_{3} = 1$. Thus\n$$\nx_{0} = 0 \\cdot v_{1} + 1 \\cdot v_{2} + 1 \\cdot v_{3}.\n$$\n\nSince $(A-\\sigma I) v_{i} = (\\lambda_{i} - \\sigma) v_{i}$, it follows that\n$$\n(A-\\sigma I)^{-1} v_{i} = \\frac{1}{\\lambda_{i} - \\sigma} \\, v_{i}.\n$$\nAfter $k$ inverse iterations (before normalization), the vector is proportional to\n$$\n\\sum_{i=1}^{3} \\alpha_{i} (\\lambda_{i} - \\sigma)^{-k} v_{i}.\n$$\nHence, as $k \\to \\infty$, the component with the largest magnitude factor $\\left|(\\lambda_{i} - \\sigma)^{-1}\\right|$ among those with $\\alpha_{i} \\neq 0$ dominates.\n\nWith $\\sigma = 0.9$, we have\n$$\n\\left|\\frac{1}{\\lambda_{2} - \\sigma}\\right| = \\frac{1}{|4 - 0.9|} = \\frac{1}{3.1}, \\qquad\n\\left|\\frac{1}{\\lambda_{3} - \\sigma}\\right| = \\frac{1}{|5 - 0.9|} = \\frac{1}{4.1}.\n$$\nSince $\\frac{1}{3.1}  \\frac{1}{4.1}$ and $\\alpha_{2}, \\alpha_{3} \\neq 0$ but $\\alpha_{1} = 0$, the sequence $\\{x_{k}\\}$ converges to the eigenvector $v_{2}$ corresponding to $\\lambda_{2} = 4$.\n\nTherefore, the eigenvalue corresponding to the limiting eigenvector is $4$.", "answer": "$$\\boxed{4}$$", "id": "1395866"}]}