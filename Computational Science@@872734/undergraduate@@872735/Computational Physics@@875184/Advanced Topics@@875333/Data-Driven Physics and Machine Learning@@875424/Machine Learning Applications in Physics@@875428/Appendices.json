{"hands_on_practices": [{"introduction": "One of the most powerful applications of machine learning in science is its ability to uncover underlying physical laws from observational data. This exercise guides you through a simplified symbolic regression task: rediscovering Kepler's Third Law from simulated astronomical data [@problem_id:2410557]. By transforming the power-law hypothesis into a linear equation, you will use ordinary least squares—a cornerstone of data analysis—to determine the precise exponent governing orbital motion, illustrating how data can reveal fundamental principles of nature.", "problem": "You are tasked with constructing a minimal symbolic regression pipeline to recover a power-law relationship between orbital period and semi-major axis from simulated astronomical data, using only standard numerical tools. The physical setting is a two-body system in which a small body orbits a central mass under Newtonian gravity. The fundamental base you must use consists of Newton's law of universal gravitation and the definition of centripetal acceleration, combined with kinematic relations linking orbital speed and period for circular motion. From these, a power-law linking the orbital period and semi-major axis follows. You must derive the form of this power-law from these principles and then implement a machine learning model that discovers the exponent of the power-law directly from noisy simulated data.\n\nYou must perform the following tasks:\n\n1. Derive, from first principles, the relationship between the orbital period $P$ in seconds and the semi-major axis $a$ in meters for a circular orbit around a point mass $M$ in kilograms, using Newton's law of universal gravitation with gravitational constant $G$ in SI units. The derivation must start from the equality of gravitational and centripetal acceleration and well-tested kinematic facts. Do not assume any pre-known target formula relating $P$ and $a$.\n\n2. Design a symbolic regression model restricted to the hypothesis class of power laws of the form $P = C a^n$, where $C$ and $n$ are constants. Show how to transform this into a linear model by taking natural logarithms so that you can estimate $n$ via ordinary least squares on the transformed variables. Assume a multiplicative noise model in $P$ that becomes additive, zero-mean noise in $\\ln P$.\n\n3. Implement a complete, runnable program that:\n   - Simulates datasets according to the physical model with the following details:\n     - Use the exact formula for the orbital period $P$ implied by the derivation in task $1$.\n     - Sample $a$ values log-uniformly between specified bounds $a_{\\min}$ and $a_{\\max}$ for each dataset.\n     - Generate noisy observations by adding zero-mean Gaussian noise with standard deviation $\\sigma$ to $\\ln P$ (equivalently, multiplicative log-normal noise on $P$). To ensure determinism and eliminate finite-sample bias in the estimated slope, the noise must be constructed to have zero sample covariance with $\\ln a$. To achieve this, if $\\epsilon_{\\text{raw}} \\sim \\mathcal{N}(0,\\sigma^2)$ denotes the initial noise vector and $x=\\ln a$ with $x_c = x - \\overline{x}$, use\n       $$\\epsilon \\leftarrow \\epsilon_{\\text{raw}} - \\frac{x_c^\\top \\epsilon_{\\text{raw}}}{x_c^\\top x_c} x_c,$$\n       and then set $\\ln P_{\\text{obs}} = \\ln P_{\\text{true}} + \\epsilon$ and $P_{\\text{obs}} = \\exp(\\ln P_{\\text{obs}})$.\n     - Use the following physical constants (in SI units): gravitational constant $G = 6.67430 \\times 10^{-11}\\ \\text{m}^3\\ \\text{kg}^{-1}\\ \\text{s}^{-2}$, Solar mass $M_\\odot = 1.98847 \\times 10^{30}\\ \\text{kg}$, Jupiter mass $M_J = 1.89813 \\times 10^{27}\\ \\text{kg}$, astronomical unit $\\mathrm{AU} = 1.495978707 \\times 10^{11}\\ \\text{m}$.\n     - Use a fixed random seed so that results are reproducible.\n   - Fits the model $\\ln P = \\beta_0 + n \\ln a$ by ordinary least squares and returns the estimated exponent $n$ for each dataset.\n   - Rounds each reported exponent to three decimal places.\n   - Produces the final output as a single line containing a comma-separated list of the exponents for all test cases enclosed in square brackets.\n\nTest Suite:\nSimulate four datasets with the following parameter sets. In all cases, report the discovered exponent $n$ (dimensionless) rounded to three decimal places.\n\n- Case $1$ (happy path, Solar mass, broad dynamic range, low noise):\n  - $M = 1.0 \\times M_\\odot$\n  - $a_{\\min} = 0.3 \\times \\mathrm{AU}$, $a_{\\max} = 5.0 \\times \\mathrm{AU}$\n  - Number of samples $N = 64$\n  - Log-noise standard deviation $\\sigma = 0.02$\n\n- Case $2$ (heavier star, very broad dynamic range, higher noise):\n  - $M = 5.0 \\times M_\\odot$\n  - $a_{\\min} = 0.1 \\times \\mathrm{AU}$, $a_{\\max} = 10.0 \\times \\mathrm{AU}$\n  - Number of samples $N = 128$\n  - Log-noise standard deviation $\\sigma = 0.05$\n\n- Case $3$ (lighter star, narrow dynamic range, low noise):\n  - $M = 0.1 \\times M_\\odot$\n  - $a_{\\min} = 0.5 \\times \\mathrm{AU}$, $a_{\\max} = 1.0 \\times \\mathrm{AU}$\n  - Number of samples $N = 40$\n  - Log-noise standard deviation $\\sigma = 0.02$\n\n- Case $4$ (Jupiter as central mass, meter-scale inputs):\n  - $M = 1.0 \\times M_J$\n  - $a_{\\min} = 1.0 \\times 10^{7}\\ \\text{m}$, $a_{\\max} = 1.0 \\times 10^{9}\\ \\text{m}$\n  - Number of samples $N = 50$\n  - Log-noise standard deviation $\\sigma = 0.03$\n\nFinal Output Format:\nYour program should produce a single line of output containing the four rounded exponents in order for cases $1$ through $4$, as a comma-separated list enclosed in square brackets. For example, the output format must be exactly like\n`[1.500,1.500,1.500,1.500]`\nwith each value rounded to three decimal places.\n\nAll physical quantities must be handled in SI units internally. The final reported exponents are dimensionless; no physical units are required for the final outputs. Angles do not appear in this task. The final outputs are floats rounded to three decimal places and aggregated into a single list on one line as specified.", "solution": "The problem has been analyzed and is determined to be valid. It is scientifically grounded in Newtonian mechanics, well-posed with a clear objective and methodology, and free from ambiguity or contradiction. We shall proceed with a complete solution.\n\nThe task is to derive the physical law governing orbital motion from first principles and then use this understanding to construct a computational model that recovers the law's parameters from simulated, noisy data. This process is a microcosm of the scientific method itself, bridging theoretical physics with data analysis.\n\n### Part 1: Derivation of the Orbital Period-Radius Relationship\n\nWe begin from fundamental principles of classical mechanics to derive the relationship between the orbital period $P$ and the semi-major axis $a$ for a body in a circular orbit around a central mass $M$. For a circular orbit, the semi-major axis is simply the constant orbital radius, $r = a$.\n\n1.  **Force Balance**: An object in a stable circular orbit is subject to a constant gravitational force, which provides the necessary centripetal force to maintain the circular path. We equate the expressions for Newton's law of universal gravitation, $F_g$, and the centripetal force, $F_c$.\n    $$F_g = F_c$$\n    Let $m$ be the mass of the orbiting body, $M$ be the mass of the central body, $v$ be the orbital velocity, and $a$ be the orbital radius. The gravitational constant is $G$.\n    $$G \\frac{M m}{a^2} = \\frac{m v^2}{a}$$\n\n2.  **Isolating Velocity**: The mass of the orbiting body, $m$, cancels, which is a manifestation of the equivalence principle. We can then rearrange the equation to solve for the square of the orbital velocity, $v^2$.\n    $$v^2 = \\frac{G M}{a}$$\n\n3.  **Kinematic Relation**: The orbital velocity $v$ is the circumference of the orbit, $2 \\pi a$, divided by the time it takes to complete one orbit, the period $P$.\n    $$v = \\frac{2 \\pi a}{P}$$\n\n4.  **Substitution and Final Form**: We substitute this kinematic expression for $v$ back into the equation derived from the force balance.\n    $$\\left(\\frac{2 \\pi a}{P}\\right)^2 = \\frac{G M}{a}$$\n    $$\\frac{4 \\pi^2 a^2}{P^2} = \\frac{G M}{a}$$\n    We now rearrange the algebra to isolate the period $P$. First, we solve for $P^2$.\n    $$P^2 = \\left(\\frac{4 \\pi^2}{G M}\\right) a^3$$\n    Finally, taking the square root of both sides gives the explicit relationship between $P$ and $a$:\n    $$P = \\sqrt{\\frac{4 \\pi^2}{G M}} a^{3/2}$$\n    This is Kepler's Third Law for circular orbits. This equation is of the form $P = C a^n$, where the constant coefficient is $C = \\sqrt{4 \\pi^2 / (G M)}$ and the exponent is $n = 3/2 = 1.5$. This theoretically derived exponent $n=1.5$ is the quantity our machine learning model must recover.\n\n### Part 2: Symbolic Regression Model via Linearization\n\nThe task requires a symbolic regression model restricted to the hypothesis class of power laws, $P = C a^n$. While more complex symbolic regression might involve genetic algorithms to search a space of mathematical expressions, for this restricted class, a simpler and more direct method is available through linearization.\n\n1.  **Logarithmic Transformation**: We apply the natural logarithm, $\\ln$, to both sides of the power-law equation.\n    $$\\ln(P) = \\ln(C a^n)$$\n    Using the properties of logarithms, $\\ln(xy) = \\ln(x) + \\ln(y)$ and $\\ln(x^k) = k \\ln(x)$, we transform the equation:\n    $$\\ln(P) = \\ln(C) + \\ln(a^n) = \\ln(C) + n \\ln(a)$$\n\n2.  **Linear Model Formulation**: This transformed equation is a linear equation. Let us define new variables: $y = \\ln(P)$, $x = \\ln(a)$, the intercept $\\beta_0 = \\ln(C)$, and the slope $\\beta_1 = n$. The model becomes:\n    $$y = \\beta_0 + \\beta_1 x$$\n    This is a simple linear regression model. The original exponent $n$ is now the slope of the line in log-log space.\n\n3.  **Noise Model**: The problem specifies a multiplicative noise model for $P$, which is common for physical quantities that are strictly positive. An observed period, $P_{obs}$, is related to the true period, $P_{true}$, by $P_{obs} = P_{true} \\cdot e^{\\epsilon_{raw}}$, where $\\epsilon_{raw}$ is a random variable drawn from a Gaussian distribution $\\mathcal{N}(0, \\sigma^2)$. Applying the logarithm to the observation gives:\n    $$\\ln(P_{obs}) = \\ln(P_{true} \\cdot e^{\\epsilon_{raw}}) = \\ln(P_{true}) + \\ln(e^{\\epsilon_{raw}}) = \\ln(P_{true}) + \\epsilon_{raw}$$\n    Thus, a multiplicative log-normal noise model in the original space becomes a simple additive Gaussian noise model in the log-transformed space.\n\n4.  **Parameter Estimation**: The unknown parameters $\\beta_0$ and $\\beta_1$ (our exponent $n$) can be estimated using ordinary least squares (OLS). The OLS estimator for the slope $\\beta_1$ for a set of $N$ data points $(x_i, y_i)$ is given by:\n    $$\\hat{\\beta_1} = n_{est} = \\frac{\\sum_{i=1}^{N} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{N} (x_i - \\bar{x})^2} = \\frac{\\text{Cov}(x, y)}{\\text{Var}(x)}$$\n    The problem specifies a non-standard but deterministic noise generation procedure. The noise vector $\\epsilon$ added to $\\ln(P_{true})$ is made to be orthogonal to the centered predictor vector $x_c = x - \\bar{x}$. This means the sample covariance between $x$ and $\\epsilon$ is exactly zero. As shown in the derivation of the OLS estimator, the error term in the estimate of the slope is proportional to this sample covariance. By forcing this covariance to zero, the OLS estimator for the slope will recover the true parameter $n = 1.5$ exactly, irrespective of the noise variance $\\sigma$ or the sample size $N$, provided the computations are done with sufficient numerical precision. The noise will only affect the estimate of the intercept $\\beta_0$. This construction provides a precise analytical test of the implementation's correctness.\n\n### Part 3: Implementation Outline\n\nThe program will execute the following steps for each test case:\n1.  Define all necessary physical constants in SI units: $G$, $M_\\odot$, $M_J$, $\\mathrm{AU}$.\n2.  Set a fixed random seed for reproducibility.\n3.  For each case, establish the parameters: central mass $M$, semi-major axis bounds $a_{min}$ and $a_{max}$, number of samples $N$, and noise standard deviation $\\sigma$. All inputs, such as AU, are converted to SI units (meters).\n4.  Generate $N$ samples of $a$ log-uniformly between $a_{min}$ and $a_{max}$. This is achieved by creating a geometric progression using `numpy.logspace`.\n5.  Calculate the \"true\" orbital period $P_{true}$ for each $a$ using the derived formula $P = \\sqrt{4\\pi^2 / (GM)} a^{3/2}$.\n6.  Transform the data into log-space: $x = \\ln(a)$ and $y_{true} = \\ln(P_{true})$.\n7.  Generate the noise vector $\\epsilon$. First, a raw noise vector $\\epsilon_{raw}$ is drawn from $\\mathcal{N}(0, \\sigma^2)$. Then, it is orthogonalized with respect to the centered predictor vector $x_c = x - \\bar{x}$ using the provided formula: $\\epsilon \\leftarrow \\epsilon_{raw} - \\frac{x_c^\\top \\epsilon_{raw}}{x_c^\\top x_c} x_c$.\n8.  Create the \"observed\" log-period data: $y_{obs} = y_{true} + \\epsilon$.\n9.  Calculate the slope of the best-fit line for the data $(x, y_{obs})$ using the OLS formula. This slope is the estimated exponent $n$.\n10. Round the estimated $n$ to three decimal places and store it.\n11. After processing all cases, format the list of exponents into the required string `\"[n1,n2,n3,n4]\"`.\nThis procedure will be encapsulated in a Python program using the `numpy` library.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of recovering the power-law exponent in Kepler's Third Law\n    from simulated data using a linearized model and ordinary least squares.\n    \"\"\"\n\n    # Physical constants in SI units\n    G = 6.67430e-11  # m^3 kg^-1 s^-2\n    M_SOLAR = 1.98847e30  # kg\n    M_JUPITER = 1.89813e27  # kg\n    AU = 1.495978707e11  # m\n\n    # Test cases defined in the problem statement\n    test_cases = [\n        {\n            \"M\": 1.0 * M_SOLAR,\n            \"a_min\": 0.3 * AU,\n            \"a_max\": 5.0 * AU,\n            \"N\": 64,\n            \"sigma\": 0.02,\n        },\n        {\n            \"M\": 5.0 * M_SOLAR,\n            \"a_min\": 0.1 * AU,\n            \"a_max\": 10.0 * AU,\n            \"N\": 128,\n            \"sigma\": 0.05,\n        },\n        {\n            \"M\": 0.1 * M_SOLAR,\n            \"a_min\": 0.5 * AU,\n            \"a_max\": 1.0 * AU,\n            \"N\": 40,\n            \"sigma\": 0.02,\n        },\n        {\n            \"M\": 1.0 * M_JUPITER,\n            \"a_min\": 1.0e7, # Already in meters\n            \"a_max\": 1.0e9, # Already in meters\n            \"N\": 50,\n            \"sigma\": 0.03,\n        },\n    ]\n\n    # Set a fixed random seed for reproducibility\n    np.random.seed(42)\n\n    results = []\n\n    for case in test_cases:\n        M = case[\"M\"]\n        a_min = case[\"a_min\"]\n        a_max = case[\"a_max\"]\n        N = case[\"N\"]\n        sigma = case[\"sigma\"]\n\n        # 1. Simulate dataset: sample semi-major axis `a` log-uniformly\n        a_samples = np.logspace(np.log10(a_min), np.log10(a_max), N)\n\n        # 2. Calculate true orbital period `P` using the derived formula\n        # P^2 = (4 * pi^2 / (G * M)) * a^3\n        C_squared = (4 * np.pi**2) / (G * M)\n        p_true_samples = np.sqrt(C_squared * a_samples**3)\n\n        # 3. Transform to log-space for linear regression\n        # ln(P) = ln(C_sqrt) + (3/2) * ln(a)\n        x = np.log(a_samples)  # ln(a)\n        y_true = np.log(p_true_samples) # ln(P_true)\n\n        # 4. Generate and orthogonalize noise\n        # Generate raw Gaussian noise\n        eps_raw = np.random.normal(loc=0.0, scale=sigma, size=N)\n        \n        # Center the predictor variable x = ln(a)\n        x_centered = x - np.mean(x)\n\n        # Orthogonalize the noise vector with respect to the centered predictor vector\n        # This ensures the sample covariance between x and the final noise is zero.\n        # eps_ortho = eps_raw - proj_of_eps_raw_onto_x_centered\n        # projection = (x_c.T @ eps_raw / x_c.T @ x_c) * x_c\n        dot_product_xc_eps = np.dot(x_centered, eps_raw)\n        dot_product_xc_xc = np.dot(x_centered, x_centered)\n        \n        # Handle case where x_centered has zero variance (e.g., N=1 or all x are same)\n        if dot_product_xc_xc == 0:\n            eps_ortho = eps_raw\n        else:\n            projection_scalar = dot_product_xc_eps / dot_product_xc_xc\n            eps_ortho = eps_raw - projection_scalar * x_centered\n\n        # 5. Create the observed log-period data with the orthogonalized noise\n        y_obs = y_true + eps_ortho\n\n        # 6. Fit the model ln(P) = beta_0 + n * ln(a) using Ordinary Least Squares\n        # We only need the slope 'n' (beta_1).\n        # The OLS formula for the slope is Cov(x, y) / Var(x).\n        # We use ddof=0 for sample covariance/variance, not unbiased estimates.\n        cov_matrix = np.cov(x, y_obs, ddof=0)\n        # The slope is cov(x,y) / var(x)\n        estimated_n = cov_matrix[0, 1] / cov_matrix[0, 0]\n\n        # 7. Round the result to three decimal places\n        rounded_n = round(estimated_n, 3)\n        results.append(str(rounded_n))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2410557"}, {"introduction": "Beyond discovering laws, machine learning excels at parameterizing complex physical models whose functional form is known from theory. In this practice, you will work with the Semi-Empirical Mass Formula (SEMF), a cornerstone of nuclear physics that approximates the binding energy of an atomic nucleus [@problem_id:2410513]. By engineering features based on the physical terms of the formula—volume, surface, Coulomb, and others—you will see how a linear regression model can learn the coefficients that represent the strengths of these underlying nuclear forces.", "problem": "You are tasked with constructing a complete, runnable program that learns to predict the nuclear binding energy from the proton number and neutron number using a data-driven approach grounded in first principles. The target quantity is the total binding energy in mega-electronvolts (MeV) of a nucleus with proton number $Z$ and neutron number $N$. The physical ground-truth mapping for training and evaluation is defined by the Semi-Empirical Mass Formula (SEMF), also known as the Weizsäcker formula, with specified coefficients. The objective is to train a model solely from data generated by this formula and then produce predictions for a prescribed test suite of nuclei.\n\nPhysics definition of the target:\nFor a nucleus with proton number $Z$, neutron number $N$, and mass number $A=Z+N$, the total binding energy $B(Z,N)$ in mega-electronvolts is given by\n$$\nB(Z,N) \\;=\\; a_v A \\;-\\; a_s A^{2/3} \\;-\\; a_c \\frac{Z(Z-1)}{A^{1/3}} \\;-\\; a_a \\frac{(A-2Z)^2}{A} \\;+\\; \\delta(A,Z,N),\n$$\nwhere the pairing term is\n$$\n\\delta(A,Z,N) \\;=\\; \n\\begin{cases}\n+\\dfrac{a_p}{\\sqrt{A}},  \\text{$Z$ and $N$ even},\\\\[6pt]\n-\\dfrac{a_p}{\\sqrt{A}},  \\text{$Z$ and $N$ odd},\\\\[6pt]\n0,  \\text{$A$ odd}.\n\\end{cases}\n$$\nUse the following SEMF coefficients (all in mega-electronvolts): $a_v=15.8$, $a_s=18.3$, $a_c=0.714$, $a_a=23.2$, $a_p=12.0$. All outputs must be expressed in mega-electronvolts (MeV).\n\nData generation for training:\n- Construct a training set by evaluating $B(Z,N)$ on the grid with $Z \\in \\{2,3,\\dots,60\\}$ and $N \\in \\{2,3,\\dots,90\\}$. The values of $Z$ and $N$ are integers.\n- Each training example consists of an input pair $(Z,N)$ and the corresponding target $B(Z,N)$ computed from the formula above, with $A=Z+N$, using the given coefficients.\n\nLearning task:\n- Train a model (you may choose any architecture) to approximate the mapping $(Z,N)\\mapsto B(Z,N)$ from the training data generated exactly as specified.\n\nTest suite to evaluate the trained model:\nProvide predictions for the following eight nuclei, each given as a pair $(Z,N)$:\n- $(1,1)$\n- $(2,2)$\n- $(3,3)$\n- $(8,9)$\n- $(26,30)$\n- $(50,70)$\n- $(82,126)$\n- $(92,146)$\n\nAnswer specification and units:\n- For each test case, output the model’s predicted total binding energy $B(Z,N)$ in mega-electronvolts (MeV).\n- Express each result as a floating-point decimal number rounded to three decimal places.\n- Angles are not involved in this problem, so no angle unit applies.\n\nFinal output format:\n- Your program must produce a single line of output containing the eight results as a comma-separated list enclosed in square brackets, for example, $[x_1,x_2,\\dots,x_8]$, where each $x_i$ is the rounded floating-point result in MeV corresponding to the $i$-th test case in the order listed above.\n\nDesign for coverage in the test suite:\n- The set includes light nuclei (e.g., $(1,1)$ and $(2,2)$), an odd-odd even-mass nucleus $(3,3)$, an odd-mass nucleus $(8,9)$ where the pairing term vanishes, mid-mass $(26,30)$, and heavy magic-number and near-magic-number nuclei $(82,126)$ and $(92,146)$, as well as a mid-heavy case $(50,70)$. This ensures coverage of different pairing regimes, mass scales, and structural regimes.\n\nYour submission must be a complete program that performs training on the specified grid and prints the predictions for the test suite in the exact format required. No user input is allowed at runtime. All computations must be in mega-electronvolts (MeV), and the final printed numbers must be rounded to three decimal places.", "solution": "The problem statement has been evaluated and is deemed valid. It is scientifically grounded, well-posed, objective, and contains all necessary information to proceed with a solution. It is based on the well-established Semi-Empirical Mass Formula (SEMF) from nuclear physics and poses a clear, solvable computational task.\n\nThe problem requires the construction of a model to predict the nuclear binding energy $B(Z, N)$ for a nucleus with proton number $Z$ and neutron number $N$. The ground truth for this learning task is explicitly defined by the SEMF, also known as the Weizsäcker formula. The binding energy $B$ is given in units of mega-electronvolts (MeV) by:\n$$\nB(Z,N) = a_v A - a_s A^{2/3} - a_c \\frac{Z(Z-1)}{A^{1/3}} - a_a \\frac{(A-2Z)^2}{A} + \\delta(A,Z,N)\n$$\nwhere $A = Z+N$ is the mass number. The coefficients are provided as $a_v=15.8$, $a_s=18.3$, $a_c=0.714$, $a_a=23.2$, and $a_p=12.0$, all in units of MeV. The pairing term $\\delta(A,Z,N)$ is defined as:\n$$\n\\delta(A,Z,N) = \n\\begin{cases}\n+a_p/\\sqrt{A},  \\text{if $Z$ and $N$ are even} \\\\\n-a_p/\\sqrt{A},  \\text{if $Z$ and $N$ are odd} \\\\\n0,  \\text{if $A$ is odd}\n\\end{cases}\n$$\n\nThe task is framed as a machine learning problem: to train a model on data generated from this formula and then use it for prediction. The most rigorous and appropriate \"architecture\" for this task is a linear regression model, as the SEMF is inherently a linear combination of physically motivated basis functions (features) derived from $Z$ and $N$.\n\nLet us define a feature vector $\\boldsymbol{x}$ with $5$ components, corresponding to the five terms of the SEMF:\n1.  **Volume Term Feature**: $x_1 = A$\n2.  **Surface Term Feature**: $x_2 = A^{2/3}$\n3.  **Coulomb Term Feature**: $x_3 = \\frac{Z(Z-1)}{A^{1/3}}$\n4.  **Asymmetry Term Feature**: $x_4 = \\frac{(A-2Z)^2}{A}$\n5.  **Pairing Term Feature**: $x_5 = p(Z,N)A^{-1/2}$, where $p(Z,N) = +1$ for even-even nuclei, $-1$ for odd-odd nuclei, and $0$ for odd-A nuclei.\n\nWith these features, the binding energy can be expressed as a linear model:\n$$\nB(\\boldsymbol{x}) = \\boldsymbol{w}^T \\boldsymbol{x} = w_1 x_1 + w_2 x_2 + w_3 x_3 + w_4 x_4 + w_5 x_5\n$$\nThe ideal weight vector $\\boldsymbol{w}_{\\text{ideal}}$ corresponding to the SEMF is $\\boldsymbol{w}_{\\text{ideal}} = [a_v, -a_s, -a_c, -a_a, a_p]^T = [15.8, -18.3, -0.714, -23.2, 12.0]^T$.\n\nThe procedure is as follows:\n1.  **Data Generation**: We generate a training set of input-output pairs. The inputs are pairs of integers $(Z,N)$ from the grid defined by $Z \\in \\{2, 3, \\dots, 60\\}$ and $N \\in \\{2, 3, \\dots, 90\\}$. For each pair $(Z_i, N_i)$, we construct the feature vector $\\boldsymbol{x}_i$ and compute the corresponding \"true\" binding energy $y_i = B(Z_i, N_i)$ using the given SEMF formula and coefficients.\n\n2.  **Model Training**: The training process consists of finding the optimal weight vector $\\boldsymbol{w}$ that best fits the training data. Given the feature matrix $\\boldsymbol{X}$ (where each row is a feature vector $\\boldsymbol{x}_i^T$) and the target vector $\\boldsymbol{y}$ (containing the values $y_i$), we solve the ordinary least squares problem. The solution is given by the normal equation:\n$$\n\\boldsymbol{w} = (\\boldsymbol{X}^T \\boldsymbol{X})^{-1} \\boldsymbol{X}^T \\boldsymbol{y}\n$$\nThis linear system is solved to obtain the learned model parameters $\\boldsymbol{w}$. A numerically stable method, such as one provided by `numpy.linalg.lstsq`, is used for this computation.\n\n3.  **Prediction**: Once the model is trained (i.e., the weight vector $\\boldsymbol{w}$ is determined), we can predict the binding energy for any nucleus $(Z_{\\text{test}}, N_{\\text{test}})$. We first construct the feature vector $\\boldsymbol{x}_{\\text{test}}$ for the test nucleus. The predicted binding energy $\\hat{B}$ is then calculated as the dot product:\n$$\n\\hat{B} = \\boldsymbol{w}^T \\boldsymbol{x}_{\\text{test}}\n$$\nThis procedure is applied to each of the eight specified test nuclei. The resulting predictions are rounded to three decimal places as required. Since the chosen model architecture perfectly matches the form of the data-generating function and the data is noise-free, the learned weights $\\boldsymbol{w}$ will be nearly identical to $\\boldsymbol{w}_{\\text{ideal}}$, and the model's predictions will faithfully reproduce the output of the SEMF. This holds true even for test nuclei outside the training range, demonstrating the generalization capability of a model grounded in correct physical principles.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Constructs a model to predict nuclear binding energy based on the Semi-Empirical Mass Formula (SEMF).\n    The program generates training data from the SEMF, trains a linear model, and predicts the binding\n    energy for a specified set of test nuclei.\n    \"\"\"\n\n    # SEMF coefficients (in MeV)\n    COEFFS = {\n        'a_v': 15.8,\n        'a_s': 18.3,\n        'a_c': 0.714,\n        'a_a': 23.2,\n        'a_p': 12.0\n    }\n\n    def get_pairing_factor(Z, N):\n        \"\"\"Calculates the sign of the pairing term.\"\"\"\n        if Z % 2 == 0 and N % 2 == 0:\n            return 1.0  # even-even\n        if Z % 2 != 0 and N % 2 != 0:\n            return -1.0  # odd-odd\n        return 0.0  # odd-A\n\n    def get_semf_features(Z, N):\n        \"\"\"\n        Calculates the 5 feature terms of the SEMF for a given nucleus (Z, N).\n        \"\"\"\n        if Z  0 or N  0 or (Z == 0 and N == 0):\n            return np.zeros(5)\n\n        A = float(Z + N)\n\n        # Handle cases where A is zero to avoid division by zero, though not expected here.\n        if A == 0:\n            return np.zeros(5)\n\n        # 1. Volume term feature\n        f1 = A\n        # 2. Surface term feature\n        f2 = A**(2/3)\n        # 3. Coulomb term feature\n        f3 = Z * (Z - 1) / (A**(1/3))\n        # 4. Asymmetry term feature\n        f4 = (A - 2 * Z)**2 / A\n        # 5. Pairing term feature\n        f5 = get_pairing_factor(Z, N) / np.sqrt(A)\n\n        return np.array([f1, f2, f3, f4, f5])\n\n    def get_semf_binding_energy(Z, N, coeffs):\n        \"\"\"\n        Calculates the ground-truth binding energy using the SEMF formula.\n        \"\"\"\n        features = get_semf_features(Z, N)\n        \n        # The ideal weights include the signs from the formula\n        ideal_weights = np.array([\n            coeffs['a_v'],\n            -coeffs['a_s'],\n            -coeffs['a_c'],\n            -coeffs['a_a'],\n            coeffs['a_p']\n        ])\n        \n        return np.dot(features, ideal_weights)\n\n    # --- 1. Data Generation ---\n    # Generate training data from the specified grid.\n    Z_range = range(2, 61)  # Z from 2 to 60\n    N_range = range(2, 91)  # N from 2 to 90\n    \n    X_train_list = []\n    y_train_list = []\n\n    for Z_val in Z_range:\n        for N_val in N_range:\n            features = get_semf_features(Z_val, N_val)\n            target = get_semf_binding_energy(Z_val, N_val, COEFFS)\n            X_train_list.append(features)\n            y_train_list.append(target)\n\n    X_train = np.array(X_train_list)\n    y_train = np.array(y_train_list)\n\n    # --- 2. Model Training ---\n    # Train a linear regression model by solving for the weights.\n    # weights = (X^T X)^-1 X^T y\n    # np.linalg.lstsq is a numerically stable way to do this.\n    weights, _, _, _ = np.linalg.lstsq(X_train, y_train, rcond=None)\n\n    # --- 3. Prediction ---\n    # Test suite of nuclei (Z, N)\n    test_cases = [\n        (1, 1),\n        (2, 2),\n        (3, 3),\n        (8, 9),\n        (26, 30),\n        (50, 70),\n        (82, 126),\n        (92, 146)\n    ]\n    \n    results = []\n    for Z_test, N_test in test_cases:\n        test_features = get_semf_features(Z_test, N_test)\n        # Predict binding energy using the trained model (learned weights)\n        prediction = np.dot(test_features, weights)\n        # Round the result to three decimal places\n        rounded_prediction = round(prediction, 3)\n        results.append(rounded_prediction)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2410513"}, {"introduction": "Machine learning potentials are revolutionizing molecular simulation, but their predictive power hinges on a critical concept: generalization. This exercise explores the challenge of *domain mismatch* by having you build a simple potential for water [@problem_id:2457471]. You will train a model on data mimicking a dense, liquid 'bulk' environment and then test its ability to describe an isolated water dimer in a vacuum. This practice provides a crucial lesson on why a model trained in one physical context may fail in another, emphasizing the need for careful validation.", "problem": "You will implement a complete, runnable program that constructs a simple one-dimensional machine learning potential trained on synthetic \"periodic bulk water\" data and then evaluates its ability to predict the vacuum potential energy curve of a water dimer as a function of the oxygen–oxygen separation. The goal is to demonstrate, through explicit numerical evaluation, how a model trained on periodic bulk data can fail when applied to the out-of-domain vacuum dimer potential energy curve.\n\nFoundational starting point: use a physically motivated pair-interaction representation for the vacuum dimer potential energy that captures the competition between short-range Pauli repulsion and long-range dispersion attraction. Use a kernel ridge regression model to represent a one-dimensional machine learning potential, with a Gaussian kernel. The program must be fully self-contained.\n\nDefinitions and specifications:\n\n1) Vacuum water dimer potential energy model. For an oxygen–oxygen separation $R$ in ångström, define the vacuum reference energy $E_{\\mathrm{vac}}(R)$ by\n$$\nE_{\\mathrm{vac}}(R) \\equiv A_{\\mathrm{rep}} \\, e^{-b_{\\mathrm{rep}} R} \\;-\\; \\frac{C_6}{R^6},\n$$\nwith parameters\n- $A_{\\mathrm{rep}} = 295400$ in $\\mathrm{kJ/mol}$,\n- $b_{\\mathrm{rep}} = 3.2$ in $\\mathrm{\\AA^{-1}}$,\n- $C_6 = 25000$ in $\\mathrm{kJ \\, \\AA^6/mol}$.\nAll energies must be expressed in $\\mathrm{kJ/mol}$ and all distances $R$ in $\\mathrm{\\AA}$.\n\n2) Emulating periodic bulk training data. To emulate that the training data originates from periodic bulk water (and thus includes many-body environmental stabilization absent in vacuum), define an environmental stabilization term\n$$\n\\Delta_{\\mathrm{env}}(R) \\equiv -A_{\\mathrm{env}} \\exp\\!\\left(-\\frac{(R - R_{\\mathrm{env}})^2}{2 s_{\\mathrm{env}}^2}\\right),\n$$\nwith\n- $A_{\\mathrm{env}} = 8.0$ in $\\mathrm{kJ/mol}$,\n- $R_{\\mathrm{env}} = 2.75$ in $\\mathrm{\\AA}$,\n- $s_{\\mathrm{env}} = 0.15$ in $\\mathrm{\\AA}$.\nThe \"bulk-labeled\" training energies are\n$$\nE_{\\mathrm{bulk}}(R) \\equiv E_{\\mathrm{vac}}(R) + \\Delta_{\\mathrm{env}}(R).\n$$\n\n3) Training set construction. Construct a training set $\\{(R_i, y_i)\\}_{i=1}^{N}$ by sampling $N$ points uniformly in the interval $[R_{\\min}, R_{\\max}]$ with\n- $R_{\\min} = 2.5$ in $\\mathrm{\\AA}$,\n- $R_{\\max} = 3.0$ in $\\mathrm{\\AA}$,\n- $N = 40$,\nand set $y_i \\equiv E_{\\mathrm{bulk}}(R_i)$.\n\n4) Machine Learning Potential (Kernel Ridge Regression). Use kernel ridge regression with a Gaussian kernel to obtain a model $\\widehat{E}(R)$ trained on the above data. Specifically, define the kernel\n$$\nK(R, R') \\equiv \\exp\\!\\left(-\\frac{(R - R')^2}{2 \\ell^2}\\right),\n$$\nwith length scale\n- $\\ell = 0.25$ in $\\mathrm{\\AA}$,\nand ridge regularization parameter\n- $\\lambda = 10^{-6}$ (dimensionless).\nLet $\\mathbf{K} \\in \\mathbb{R}^{N \\times N}$ be the Gram matrix with entries $K_{ij} \\equiv K(R_i, R_j)$. The kernel ridge regression solution for the coefficient vector $\\boldsymbol{\\alpha} \\in \\mathbb{R}^{N}$ is\n$$\n\\boldsymbol{\\alpha} = \\left(\\mathbf{K} + \\lambda \\mathbf{I}\\right)^{-1} \\mathbf{y},\n$$\nwhere $\\mathbf{y} = (y_1, \\dots, y_N)^\\top$. The model prediction at a new separation $R$ is\n$$\n\\widehat{E}(R) = \\sum_{j=1}^{N} \\alpha_j \\, K(R, R_j).\n$$\n\n5) Evaluation against vacuum reference. Evaluate the model $\\widehat{E}(R)$ against the vacuum reference $E_{\\mathrm{vac}}(R)$ for a specified set of test separations and also by comparing minima locations on a grid. All energies must be reported in $\\mathrm{kJ/mol}$ and distances in $\\mathrm{\\AA}$.\n\nTest Suite:\n\n- Pointwise tests at the oxygen–oxygen separations\n$$\nR_{\\mathrm{test}} \\in \\{ 2.4, \\; 2.85, \\; 3.4, \\; 5.0 \\} \\; \\mathrm{\\AA}.\n$$\nFor each $R$ in the set, compute the absolute energy error\n$$\n\\varepsilon(R) \\equiv \\left| \\widehat{E}(R) - E_{\\mathrm{vac}}(R) \\right|\n$$\nin $\\mathrm{kJ/mol}$.\n\n- Minima-location test on a grid. On a uniform grid\n$$\nR \\in [2.2, \\; 5.0] \\; \\mathrm{\\AA}\n$$\nwith grid spacing\n$$\n\\Delta R = 0.001 \\; \\mathrm{\\AA},\n$$\ncompute\n$$\nR_{\\min}^{\\mathrm{vac}} \\equiv \\operatorname*{arg\\,min}_{R} E_{\\mathrm{vac}}(R), \\quad\nR_{\\min}^{\\mathrm{ml}} \\equiv \\operatorname*{arg\\,min}_{R} \\widehat{E}(R),\n$$\nand report the signed deviation\n$$\n\\Delta R_{\\min} \\equiv R_{\\min}^{\\mathrm{ml}} - R_{\\min}^{\\mathrm{vac}}\n$$\nin $\\mathrm{\\AA}$.\n\nRequired final output format:\n\n- Your program must produce a single line of output containing a list with $5$ entries:\n  1) $\\varepsilon(2.4)$,\n  2) $\\varepsilon(2.85)$,\n  3) $\\varepsilon(3.4)$,\n  4) $\\varepsilon(5.0)$,\n  5) $\\Delta R_{\\min}$.\n- The first four entries must be floats in $\\mathrm{kJ/mol}$ rounded to exactly $3$ decimal places. The last entry must be a float in $\\mathrm{\\AA}$ rounded to exactly $3$ decimal places.\n- The output must be a comma-separated list enclosed in square brackets, for example\n$$\n[\\varepsilon(2.4),\\varepsilon(2.85),\\varepsilon(3.4),\\varepsilon(5.0),\\Delta R_{\\min}]\n$$\nwith all five numbers shown as decimals having exactly three digits after the decimal point.\n\nConstraints and notes:\n\n- The program must implement the definitions above exactly, without any external data.\n- All energies must be expressed in $\\mathrm{kJ/mol}$, all distances in $\\mathrm{\\AA}$.\n- No random numbers are needed.\n- Angles are not used in this reduced one-dimensional model; all angle quantities are irrelevant.\n- Your code must be standalone and must not read any input.", "solution": "The problem is scientifically sound and computationally well-posed, designed to demonstrate the crucial issue of domain mismatch in machine learning potentials. The solution involves implementing the specified physical models and the kernel ridge regression algorithm, training the model on \"bulk\" data, and then evaluating its performance against the \"vacuum\" reference.\n\nThe objective is to construct a one-dimensional machine learning potential $\\widehat{E}(R)$ trained on synthetic \"bulk\" data and evaluate its predictive accuracy for the \"vacuum\" potential energy curve $E_{\\mathrm{vac}}(R)$. The core of the problem lies in the discrepancy between the training data distribution, which includes an environmental stabilization term $\\Delta_{\\mathrm{env}}(R)$, and the target test distribution, which does not.\n\n1.  **Define Potential Energy Functions**: First, we implement the analytical forms of the potentials. The vacuum potential $E_{\\mathrm{vac}}(R)$ is defined as\n    $$\n    E_{\\mathrm{vac}}(R) = A_{\\mathrm{rep}} \\, e^{-b_{\\mathrm{rep}} R} - \\frac{C_6}{R^6}\n    $$\n    with the given parameters $A_{\\mathrm{rep}} = 295400 \\, \\mathrm{kJ/mol}$, $b_{\\mathrm{rep}} = 3.2 \\, \\mathrm{\\AA^{-1}}$, and $C_6 = 25000 \\, \\mathrm{kJ \\, \\AA^6/mol}$.\n    The environmental stabilization term, representing the effect of a condensed-phase environment, is\n    $$\n    \\Delta_{\\mathrm{env}}(R) = -A_{\\mathrm{env}} \\exp\\!\\left(-\\frac{(R - R_{\\mathrm{env}})^2}{2 s_{\\mathrm{env}}^2}\\right)\n    $$\n    with parameters $A_{\\mathrm{env}} = 8.0 \\, \\mathrm{kJ/mol}$, $R_{\\mathrm{env}} = 2.75 \\, \\mathrm{\\AA}$, and $s_{\\mathrm{env}} = 0.15 \\, \\mathrm{\\AA}$.\n    The training data is generated using the \"bulk\" potential, which is the sum of these two components:\n    $$\n    E_{\\mathrm{bulk}}(R) = E_{\\mathrm{vac}}(R) + \\Delta_{\\mathrm{env}}(R)\n    $$\n    This term $\\Delta_{\\mathrm{env}}(R)$ introduces a stabilization (a negative energy contribution) centered at $R = 2.75 \\, \\mathrm{\\AA}$, which is the physically correct location for the first peak of the oxygen-oxygen radial distribution function in liquid water. The machine learning model will learn this feature from the training data.\n\n2.  **Construct Training Set**: A training set $\\{ (R_i, y_i) \\}_{i=1}^{N}$ is generated. The input points $R_i$ are $N=40$ uniformly spaced separations in the interval $[R_{\\min}, R_{\\max}] = [2.5, 3.0] \\, \\mathrm{\\AA}$. The target values are the corresponding \"bulk\" energies, $y_i = E_{\\mathrm{bulk}}(R_i)$. This training region, $[2.5, 3.0] \\, \\mathrm{\\AA}$, is narrow and centered around the environmental stabilization minimum at $R=2.75 \\, \\mathrm{\\AA}$.\n\n3.  **Train the Kernel Ridge Regression Model**: We employ kernel ridge regression (KRR) with a Gaussian kernel to model the potential energy surface. The Gaussian kernel is given by\n    $$\n    K(R, R') = \\exp\\!\\left(-\\frac{(R - R')^2}{2 \\ell^2}\\right)\n    $$\n    with length scale $\\ell = 0.25 \\, \\mathrm{\\AA}$. The KRR model predicts the energy at a new point $R$ as a linear combination of kernel functions centered at the training points $R_j$:\n    $$\n    \\widehat{E}(R) = \\sum_{j=1}^{N} \\alpha_j K(R, R_j)\n    $$\n    The coefficients $\\boldsymbol{\\alpha} = (\\alpha_1, \\dots, \\alpha_N)^\\top$ are found by solving the regularized linear system:\n    $$\n    (\\mathbf{K} + \\lambda \\mathbf{I}) \\boldsymbol{\\alpha} = \\mathbf{y}\n    $$\n    Here, $\\mathbf{K}$ is the $N \\times N$ Gram matrix with entries $K_{ij} = K(R_i, R_j)$, $\\mathbf{y} = (y_1, \\dots, y_N)^\\top$ is the vector of training targets, $\\mathbf{I}$ is the identity matrix, and $\\lambda = 10^{-6}$ is the Tikhonov regularization parameter. This linear system is solved numerically.\n\n4.  **Evaluate the Model**: The trained model $\\widehat{E}(R)$ is now evaluated against the true vacuum potential $E_{\\mathrm{vac}}(R)$, not the bulk potential it was trained on. This tests its ability to generalize outside its training distribution.\n\n    a.  **Pointwise Error**: The absolute error $\\varepsilon(R) = | \\widehat{E}(R) - E_{\\mathrm{vac}}(R) |$ is computed at four specified test points: $R_{\\mathrm{test}} \\in \\{ 2.4, 2.85, 3.4, 5.0 \\} \\, \\mathrm{\\AA}$. Points such as $R=2.4 \\, \\mathrm{\\AA}$ and $R=3.4 \\, \\mathrm{\\AA}$ are outside the training interval $[2.5, 3.0] \\, \\mathrm{\\AA}$, testing extrapolation. The point $R=2.85 \\, \\mathrm{\\AA}$ is inside the interval, testing interpolation. The point $R=5.0 \\, \\mathrm{\\AA}$ tests behavior at long range. We expect significant errors because the model $\\widehat{E}(R)$ has learned the stabilization feature from $\\Delta_{\\mathrm{env}}(R)$, which is absent in the target function $E_{\\mathrm{vac}}(R)$.\n\n    b.  **Minima Location Error**: To assess the model's ability to predict structural properties, we compare the location of the potential energy minimum. We define a fine grid of $R$ values from $2.2 \\, \\mathrm{\\AA}$ to $5.0 \\, \\mathrm{\\AA}$ with a step of $\\Delta R = 0.001 \\, \\mathrm{\\AA}$. We evaluate both $E_{\\mathrm{vac}}(R)$ and $\\widehat{E}(R)$ on this grid and find their respective minima, $R_{\\min}^{\\mathrm{vac}}$ and $R_{\\min}^{\\mathrm{ml}}$. The deviation $\\Delta R_{\\min} = R_{\\min}^{\\mathrm{ml}} - R_{\\min}^{\\mathrm{vac}}$ is calculated. Since the training data includes an artefactual stabilization centered at $R = 2.75 \\, \\mathrm{\\AA}$, the learned potential $\\widehat{E}(R)$ will have its minimum shifted towards this value compared to the true vacuum minimum of $E_{\\mathrm{vac}}(R)$, resulting in a non-zero $\\Delta R_{\\min}$.\n\nThe final implementation will carry out these steps numerically and format the results as requested.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Constructs and evaluates a 1D machine learning potential for a water dimer.\n    The model is trained on synthetic \"bulk\" data and tested against the \"vacuum\" potential,\n    demonstrating the effect of distributional shift.\n    \"\"\"\n\n    # --- 1. Definitions and Parameters ---\n\n    # Parameters for the vacuum potential E_vac(R)\n    A_rep = 295400.0  # kJ/mol\n    b_rep = 3.2      # A^-1\n    C6 = 25000.0     # kJ * A^6 / mol\n\n    # Parameters for the environmental stabilization term Delta_env(R)\n    A_env = 8.0      # kJ/mol\n    R_env = 2.75     # A\n    s_env = 0.15     # A\n\n    # Training set parameters\n    N = 40\n    R_min = 2.5      # A\n    R_max = 3.0      # A\n\n    # Kernel Ridge Regression parameters\n    length_scale = 0.25  # A\n    lambda_reg = 1e-6    # dimensionless\n\n    # --- 2. Potential Energy Functions ---\n\n    def E_vac(R):\n        \"\"\"Calculates the vacuum reference energy E_vac(R).\"\"\"\n        return A_rep * np.exp(-b_rep * R) - C6 / (R**6)\n\n    def Delta_env(R):\n        \"\"\"Calculates the environmental stabilization term Delta_env(R).\"\"\"\n        return -A_env * np.exp(-((R - R_env)**2) / (2 * s_env**2))\n\n    def E_bulk(R):\n        \"\"\"Calculates the 'bulk-labeled' training energy E_bulk(R).\"\"\"\n        return E_vac(R) + Delta_env(R)\n\n    # --- 3. Training Set Construction ---\n\n    R_train = np.linspace(R_min, R_max, N)\n    y_train = E_bulk(R_train)\n\n    # --- 4. Machine Learning Potential (Kernel Ridge Regression) ---\n\n    def gaussian_kernel(R1, R2, l):\n        \"\"\"\n        Computes the Gaussian kernel between two sets of points.\n        Handles broadcasting for vector-matrix operations.\n        \"\"\"\n        # Ensure R1 and R2 are numpy arrays for broadcasting\n        R1 = np.asarray(R1)\n        R2 = np.asarray(R2)\n        \n        # If R1 is a vector and R2 is a vector, we want a matrix of distances\n        if R1.ndim == 1 and R2.ndim == 1:\n            R1 = R1[:, np.newaxis] # Makes R1 a column vector\n        \n        dist_sq = (R1 - R2)**2\n        return np.exp(-dist_sq / (2 * l**2))\n\n    # Construct the Gram matrix K\n    K_matrix = gaussian_kernel(R_train, R_train, length_scale)\n\n    # Solve for the KRR coefficients alpha\n    # (K + lambda*I) * alpha = y  =  alpha = solve(K + lambda*I, y)\n    A = K_matrix + lambda_reg * np.eye(N)\n    alpha = np.linalg.solve(A, y_train)\n\n    def E_hat(R):\n        \"\"\"\n        Predicts the energy using the trained KRR model.\n        R can be a single value or a numpy array.\n        \"\"\"\n        kernel_vec = gaussian_kernel(R, R_train, length_scale)\n        return kernel_vec @ alpha\n\n    # --- 5. Evaluation against Vacuum Reference ---\n\n    # a) Pointwise tests\n    R_test = np.array([2.4, 2.85, 3.4, 5.0])\n    E_hat_test = E_hat(R_test)\n    E_vac_test = E_vac(R_test)\n    errors = np.abs(E_hat_test - E_vac_test)\n    \n    # b) Minima-location test\n    R_grid_step = 0.001\n    R_grid = np.arange(2.2, 5.0 + R_grid_step, R_grid_step)\n\n    # Evaluate both potentials on the grid\n    E_vac_grid = E_vac(R_grid)\n    E_hat_grid = E_hat(R_grid)\n\n    # Find the arguments of the minima\n    idx_min_vac = np.argmin(E_vac_grid)\n    R_min_vac = R_grid[idx_min_vac]\n\n    idx_min_ml = np.argmin(E_hat_grid)\n    R_min_ml = R_grid[idx_min_ml]\n\n    # Calculate the signed deviation\n    delta_R_min = R_min_ml - R_min_vac\n\n    # --- 6. Final Output Formatting ---\n    results = [\n        errors[0],      # epsilon(2.4)\n        errors[1],      # epsilon(2.85)\n        errors[2],      # epsilon(3.4)\n        errors[3],      # epsilon(5.0)\n        delta_R_min     # Delta_R_min\n    ]\n\n    # Format output according to the problem specification\n    formatted_results = [f\"{res:.3f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\n\nif __name__ == \"__main__\":\n    solve()\n\n```", "id": "2457471"}]}