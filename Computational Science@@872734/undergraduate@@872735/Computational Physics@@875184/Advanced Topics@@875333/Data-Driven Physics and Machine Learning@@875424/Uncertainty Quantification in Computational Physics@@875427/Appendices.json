{"hands_on_practices": [{"introduction": "This exercise provides hands-on practice with the fundamental technique of uncertainty propagation. By analyzing a simple pendulum, you will learn how to quantify the uncertainty in a model's output when its inputs are uncertain and, importantly, correlated. This problem uses the classic first-order Taylor series method, a cornerstone of analytical error analysis [@problem_id:2448343].", "problem": "Write a complete program that, for an ideal small-angle simple pendulum, quantifies the uncertainty in its oscillation period when the pendulum length and the local gravitational acceleration are jointly uncertain and correlated. The period is modeled by the mapping $T:\\mathbb{R}_{>0}^{2}\\to\\mathbb{R}_{>0}$ defined by $T(L,g)=2\\pi\\sqrt{L/g}$. Assume that the input vector $(L,g)$ is jointly normally distributed with mean vector $(\\mu_L,\\mu_g)$, standard deviations $(\\sigma_L,\\sigma_g)$, and correlation coefficient $\\rho\\in[-1,1]$. For each parameter set, determine the first-order approximation to the mean period and its corresponding first-order standard deviation obtained by linearizing $T$ about $(\\mu_L,\\mu_g)$. Express all period values and standard deviations in seconds, rounded to $6$ decimal places.\n\nUse the following test suite, where $L$ is in meters and $g$ is in meters per second squared, and where each tuple is $(\\mu_L,\\sigma_L,\\mu_g,\\sigma_g,\\rho)$:\n- Test $1$: $(1.000,0.005,9.80665,0.020,0.000)$.\n- Test $2$: $(2.000,0.010,9.81000,0.050,0.900)$.\n- Test $3$: $(0.500,0.002,9.78000,0.030,-0.900)$.\n- Test $4$: $(1.500,0.000001,9.81000,0.000001,0.000)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must itself be a two-element list containing the approximate mean period and the corresponding first-order standard deviation, both in seconds and rounded to $6$ decimal places. For example, the overall output format must be of the form $[[\\mu_{T,1},\\sigma_{T,1}],[\\mu_{T,2},\\sigma_{T,2}],[\\mu_{T,3},\\sigma_{T,3}],[\\mu_{T,4},\\sigma_{T,4}]]$ with no additional text.", "solution": "The problem statement has been rigorously validated and is found to be scientifically sound, well-posed, and objective. It provides a complete and consistent setup for uncertainty quantification using established principles of physics and statistics. We may therefore proceed with the solution.\n\nThe problem requires the quantification of uncertainty in the period $T$ of a simple pendulum, where $T$ is a nonlinear function of the pendulum length $L$ and the local gravitational acceleration $g$. The mapping is given by:\n$$\nT(L, g) = 2\\pi\\sqrt{\\frac{L}{g}}\n$$\nThe input parameters $(L, g)$ are treated as a bivariate random variable, jointly normally distributed with mean vector $\\boldsymbol{\\mu} = (\\mu_L, \\mu_g)^T$ and a covariance matrix $\\Sigma$. The covariance matrix is defined by the standard deviations $\\sigma_L$ and $\\sigma_g$, and the correlation coefficient $\\rho$:\n$$\n\\Sigma = \\begin{pmatrix} \\sigma_L^2 & \\rho \\sigma_L \\sigma_g \\\\ \\rho \\sigma_L \\sigma_g & \\sigma_g^2 \\end{pmatrix}\n$$\nWe are tasked with finding the first-order approximations for the mean $\\mu_T$ and standard deviation $\\sigma_T$ of the period $T$. This is achieved by applying the law of propagation of uncertainty, which is based on a first-order Taylor series expansion of the function $T(L, g)$ around the mean values $(\\mu_L, \\mu_g)$.\n\nFirst, the first-order approximation of the mean of the output, $\\mu_T$, is simply the function evaluated at the means of the inputs. This is the zeroth-order term in the Taylor expansion of the expected value. Higher-order terms, which involve the Hessian of the function, are neglected in this first-order approach.\n$$\n\\mu_T \\approx T(\\mu_L, \\mu_g) = 2\\pi\\sqrt{\\frac{\\mu_L}{\\mu_g}}\n$$\nLet us denote this evaluation at the mean as $T_0 = T(\\mu_L, \\mu_g)$.\n\nNext, we determine the first-order approximation for the variance of the output, $\\sigma_T^2$. For a general function $Y = f(X_1, X_2)$, the variance is given by:\n$$\n\\sigma_Y^2 \\approx \\left(\\frac{\\partial f}{\\partial X_1}\\right)^2 \\sigma_{X_1}^2 + \\left(\\frac{\\partial f}{\\partial X_2}\\right)^2 \\sigma_{X_2}^2 + 2 \\left(\\frac{\\partial f}{\\partial X_1}\\right) \\left(\\frac{\\partial f}{\\partial X_2}\\right) \\text{Cov}(X_1, X_2)\n$$\nwhere the partial derivatives are evaluated at the mean values $(\\mu_{X_1}, \\mu_{X_2})$.\n\nFor our specific function $T(L, g) = 2\\pi L^{1/2} g^{-1/2}$, we must compute the partial derivatives with respect to $L$ and $g$.\nThe partial derivative with respect to $L$ is:\n$$\n\\frac{\\partial T}{\\partial L} = 2\\pi \\left(\\frac{1}{2} L^{-1/2} g^{-1/2}\\right) = \\pi \\frac{1}{\\sqrt{Lg}}\n$$\nThe partial derivative with respect to $g$ is:\n$$\n\\frac{\\partial T}{\\partial g} = 2\\pi L^{1/2} \\left(-\\frac{1}{2} g^{-3/2}\\right) = -\\pi \\frac{\\sqrt{L}}{g\\sqrt{g}} = -\\pi \\frac{\\sqrt{L}}{\\sqrt{g^3}}\n$$\nEvaluating these derivatives at the mean point $(\\mu_L, \\mu_g)$ can be simplified by relating them to $T_0$:\n$$\n\\left. \\frac{\\partial T}{\\partial L} \\right|_{(\\mu_L, \\mu_g)} = \\pi \\frac{1}{\\sqrt{\\mu_L \\mu_g}} = \\left(2\\pi\\sqrt{\\frac{\\mu_L}{\\mu_g}}\\right) \\left(\\frac{1}{2\\mu_L}\\right) = \\frac{T_0}{2\\mu_L}\n$$\n$$\n\\left. \\frac{\\partial T}{\\partial g} \\right|_{(\\mu_L, \\mu_g)} = -\\pi \\frac{\\sqrt{\\mu_L}}{\\sqrt{\\mu_g^3}} = \\left(2\\pi\\sqrt{\\frac{\\mu_L}{\\mu_g}}\\right) \\left(-\\frac{1}{2\\mu_g}\\right) = -\\frac{T_0}{2\\mu_g}\n$$\nNow, substitute these sensitivity coefficients into the variance propagation formula. The covariance term is $\\text{Cov}(L, g) = \\rho\\sigma_L\\sigma_g$.\n$$\n\\sigma_T^2 \\approx \\left(\\frac{T_0}{2\\mu_L}\\right)^2 \\sigma_L^2 + \\left(-\\frac{T_0}{2\\mu_g}\\right)^2 \\sigma_g^2 + 2\\left(\\frac{T_0}{2\\mu_L}\\right)\\left(-\\frac{T_0}{2\\mu_g}\\right)(\\rho\\sigma_L\\sigma_g)\n$$\nFactoring out common terms yields:\n$$\n\\sigma_T^2 \\approx \\frac{T_0^2}{4} \\left[ \\frac{\\sigma_L^2}{\\mu_L^2} + \\frac{\\sigma_g^2}{\\mu_g^2} - 2\\rho\\frac{\\sigma_L\\sigma_g}{\\mu_L\\mu_g} \\right]\n$$\nThis expression can be written more elegantly using relative standard deviations (coefficients of variation), $c_L = \\sigma_L/\\mu_L$ and $c_g = \\sigma_g/\\mu_g$:\n$$\n\\sigma_T^2 \\approx \\frac{T_0^2}{4} \\left[ c_L^2 + c_g^2 - 2\\rho c_L c_g \\right]\n$$\nThe first-order standard deviation $\\sigma_T$ is the square root of the variance:\n$$\n\\sigma_T \\approx \\sqrt{\\sigma_T^2} = \\frac{T_0}{2} \\sqrt{ \\left(\\frac{\\sigma_L}{\\mu_L}\\right)^2 + \\left(\\frac{\\sigma_g}{\\mu_g}\\right)^2 - 2\\rho \\left(\\frac{\\sigma_L}{\\mu_L}\\right) \\left(\\frac{\\sigma_g}{\\mu_g}\\right) }\n$$\nThis final formula provides the required first-order standard deviation of the oscillation period. The algorithm for implementation is as follows:\n$1$. For each test case $(\\mu_L, \\sigma_L, \\mu_g, \\sigma_g, \\rho)$, first compute the approximate mean period $T_0 = 2\\pi\\sqrt{\\mu_L/\\mu_g}$.\n$2$. Compute the relative standard deviations for $L$ and $g$.\n$3$. Substitute these values into the derived formula for $\\sigma_T$.\n$4$. Round both $T_0$ and $\\sigma_T$ to the specified $6$ decimal places.\nThis procedure will be implemented for all provided test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes first-order approximations for the mean and standard deviation\n    of a pendulum's period given uncertain inputs.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each tuple is (mu_L, sigma_L, mu_g, sigma_g, rho).\n    test_cases = [\n        (1.000, 0.005, 9.80665, 0.020, 0.000),\n        (2.000, 0.010, 9.81000, 0.050, 0.900),\n        (0.500, 0.002, 9.78000, 0.030, -0.900),\n        (1.500, 0.000001, 9.81000, 0.000001, 0.000),\n    ]\n\n    results = []\n    for case in test_cases:\n        mu_L, sigma_L, mu_g, sigma_g, rho = case\n\n        # Step 1: Calculate the first-order approximation for the mean period (T_0)\n        # mu_T is approximated by T(mu_L, mu_g)\n        mu_T = 2 * np.pi * np.sqrt(mu_L / mu_g)\n\n        # Step 2: Calculate the first-order standard deviation of the period\n        # Formula derived from first-order Taylor expansion (propagation of uncertainty)\n        # sigma_T = (T_0 / 2) * sqrt( (sigma_L/mu_L)^2 + (sigma_g/mu_g)^2 - 2*rho*(sigma_L/mu_L)*(sigma_g/mu_g) )\n        \n        # Avoid division by zero, though problem constraints ensure mu_L and mu_g > 0\n        if mu_L == 0 or mu_g == 0:\n            # This case is not expected based on the problem description\n            sigma_T = float('inf')\n        else:\n            rel_std_L = sigma_L / mu_L\n            rel_std_g = sigma_g / mu_g\n            \n            variance_term = (rel_std_L**2) + (rel_std_g**2) - (2 * rho * rel_std_L * rel_std_g)\n            \n            # The term inside sqrt must be non-negative.\n            # It represents the variance of a linear combination of correlated variables\n            # and is guaranteed to be non-negative.\n            if variance_term  0:\n                # This could happen due to floating point inaccuracies, clamp to 0\n                variance_term = 0\n\n            sigma_T = (mu_T / 2) * np.sqrt(variance_term)\n\n        # Step 3: Round results to 6 decimal places\n        mu_T_rounded = round(mu_T, 6)\n        sigma_T_rounded = round(sigma_T, 6)\n        \n        results.append([mu_T_rounded, sigma_T_rounded])\n\n    # Final print statement in the exact required format.\n    # Convert the list of lists to the specified string format\n    # e.g., [[val1, val2],[val3, val4]]\n    result_str = \",\".join(map(str, results))\n    print(f\"[{result_str}]\")\n\nsolve()\n```", "id": "2448343"}, {"introduction": "Many models in computational physics are too slow to run thousands of times for a standard uncertainty analysis. This practice introduces a powerful and widely used solution: surrogate modeling. You will first build a fast and accurate polynomial \"surrogate\" for a materials science model and then use it to efficiently propagate input uncertainties through Monte Carlo simulation, a core workflow in modern UQ [@problem_id:2448322].", "problem": "Create a complete program that constructs and validates a fast surrogate for the Dirac-point energy shift under small in-plane strain in a two-dimensional crystalline material and uses it to propagate input uncertainty. Work in a low-strain regime where the effect of strain can be captured by a smooth function of the independent small-strain tensor components. Use the following physically motivated reference model (taken as the expensive ground-truth model to be emulated) for the Dirac-point energy shift:\n$$\n\\Delta E_{\\mathrm{ref}}(\\varepsilon_{xx},\\varepsilon_{yy},\\varepsilon_{xy}) \\;=\\; D\\,(\\varepsilon_{xx}+\\varepsilon_{yy}) \\;+\\; \\alpha\\,(\\varepsilon_{xx}-\\varepsilon_{yy})^{2} \\;+\\; \\beta\\,\\varepsilon_{xy}^{2},\n$$\nwith $D = 5.0$ in electronvolt, $\\alpha = 15.0$ in electronvolt, and $\\beta = 10.0$ in electronvolt. The strain components $\\varepsilon_{xx}$, $\\varepsilon_{yy}$, and $\\varepsilon_{xy}$ are dimensionless and assumed small in magnitude.\n\nStarting from the definition of least-squares regression and Monte Carlo estimation, implement the following steps without using any problem-specific shortcut formulas:\n\n- Build a second-order polynomial surrogate of $\\Delta E_{\\mathrm{ref}}$ in variables $(\\varepsilon_{xx},\\varepsilon_{yy},\\varepsilon_{xy})$ by fitting a linear model in the full set of monomials up to degree $2$ using ordinary least squares. Concretely, represent\n$$\n\\widehat{\\Delta E}(\\boldsymbol{\\varepsilon}) \\;=\\; w_{0} \\;+\\; w_{1}\\,\\varepsilon_{xx} \\;+\\; w_{2}\\,\\varepsilon_{yy} \\;+\\; w_{3}\\,\\varepsilon_{xy} \\;+\\; w_{4}\\,\\varepsilon_{xx}^{2} \\;+\\; w_{5}\\,\\varepsilon_{yy}^{2} \\;+\\; w_{6}\\,\\varepsilon_{xy}^{2} \\;+\\; w_{7}\\,\\varepsilon_{xx}\\varepsilon_{yy} \\;+\\; w_{8}\\,\\varepsilon_{xx}\\varepsilon_{xy} \\;+\\; w_{9}\\,\\varepsilon_{yy}\\varepsilon_{xy},\n$$\nand determine the coefficients $w_{j}$ by minimizing the sum of squared residuals over a training set.\n\n- Generate a training set by drawing $M$ independent samples of $(\\varepsilon_{xx},\\varepsilon_{yy},\\varepsilon_{xy})$ uniformly from the cube $[-s,s]^{3}$ with $M = 2000$ and $s = 0.02$, evaluate $\\Delta E_{\\mathrm{ref}}$ on these samples, and compute the least-squares fit.\n\n- Use the surrogate $\\widehat{\\Delta E}$ to propagate input uncertainty in strain for each test case described below. In each case, the strain vector is modeled as a Gaussian random vector $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})$ with mean $\\boldsymbol{\\mu}$ and covariance matrix $\\boldsymbol{\\Sigma}$ provided; the three components correspond, in order, to $(\\varepsilon_{xx},\\varepsilon_{yy},\\varepsilon_{xy})$. Use a Monte Carlo sample size $N = 100000$ to estimate:\n    - the mean value $\\mathbb{E}[\\widehat{\\Delta E}]$ in electronvolt,\n    - the standard deviation $\\sqrt{\\mathrm{Var}[\\widehat{\\Delta E}]}$ in electronvolt, and\n    - the exceedance probability $\\mathbb{P}\\left(|\\widehat{\\Delta E}| > T\\right)$ as a decimal, for threshold $T = 0.02$ in electronvolt.\n\nIf $\\boldsymbol{\\Sigma}$ is the zero matrix, treat the distribution as deterministic at $\\boldsymbol{\\mu}$. All energies must be expressed in electronvolt, and all probabilities must be expressed as decimals (not percentages). Report all outputs rounded to six decimal places.\n\nTest suite to evaluate your program:\n\n- Case $1$ (isotropic mean with small independent uncertainty):\n    - $\\boldsymbol{\\mu}_{1} = [0.005,\\, 0.005,\\, 0.0]$\n    - $\\boldsymbol{\\Sigma}_{1} = \\mathrm{diag}([0.001^{2},\\, 0.001^{2},\\, 0.0005^{2}])$\n- Case $2$ (pure shear mean with correlation between $\\varepsilon_{xx}$ and $\\varepsilon_{yy}$):\n    - $\\boldsymbol{\\mu}_{2} = [0.0,\\, 0.0,\\, 0.005]$\n    - Correlation matrix\n      $$\n      \\mathbf{R}_{2} \\;=\\; \\begin{bmatrix}\n      1  -0.5  0 \\\\\n      -0.5  1  0 \\\\\n      0  0  1\n      \\end{bmatrix}\n      $$\n      with standard deviations $[0.001,\\, 0.001,\\, 0.001]$, so\n      $$\n      \\boldsymbol{\\Sigma}_{2} \\;=\\; \\mathrm{diag}([0.001,\\,0.001,\\,0.001])\\;\\mathbf{R}_{2}\\;\\mathrm{diag}([0.001,\\,0.001,\\,0.001]).\n      $$\n- Case $3$ (deterministic anisotropic mean, zero uncertainty):\n    - $\\boldsymbol{\\mu}_{3} = [0.01,\\, 0.0,\\, 0.0]$\n    - $\\boldsymbol{\\Sigma}_{3} = \\mathbf{0}$ (the $3 \\times 3$ zero matrix)\n- Case $4$ (correlated anisotropic mean and uncertainty):\n    - $\\boldsymbol{\\mu}_{4} = [0.003,\\, 0.007,\\, -0.002]$\n    - Correlation matrix\n      $$\n      \\mathbf{R}_{4} \\;=\\; \\begin{bmatrix}\n      1  0.7  -0.3 \\\\\n      0.7  1  0.2 \\\\\n      -0.3  0.2  1\n      \\end{bmatrix}\n      $$\n      with standard deviations $[0.002,\\, 0.0025,\\, 0.0015]$, so\n      $$\n      \\boldsymbol{\\Sigma}_{4} \\;=\\; \\mathrm{diag}([0.002,\\, 0.0025,\\, 0.0015])\\;\\mathbf{R}_{4}\\;\\mathrm{diag}([0.002,\\, 0.0025,\\, 0.0015]).\n      $$\n\nProgram requirements and final output format:\n\n- The program must be self-contained and produce results for the four cases above using the stated $M$, $s$, $N$, $D$, $\\alpha$, $\\beta$, and $T$.\n- Use a fixed random seed to ensure reproducibility.\n- Your program should produce a single line of output containing the results as a comma-separated list of four lists, each inner list ordered as $[\\text{mean in eV},\\, \\text{standard deviation in eV},\\, \\text{probability as decimal}]$ and rounded to six decimal places, with no extra whitespace. For example: `[[a,b,c],[d,e,f],[g,h,i],[j,k,l]]` where $a$ through $l$ denote floating-point numbers rounded to six decimals.", "solution": "The problem statement submitted for analysis is deemed valid. It is scientifically grounded in the principles of computational materials science and statistics, is mathematically well-posed, and is defined with objective, unambiguous precision. All necessary parameters, models, and conditions for a unique and verifiable solution are provided. The problem is a standard exercise in uncertainty quantification involving surrogate modeling and Monte Carlo simulation. We shall proceed with the solution.\n\nThe task is to construct a polynomial surrogate model for a given physical function and then use this surrogate to perform uncertainty propagation. The process is divided into two main stages: surrogate construction and uncertainty analysis.\n\n**1. Surrogate Model Construction**\n\nThe ground-truth model for the Dirac-point energy shift $\\Delta E_{\\mathrm{ref}}$ as a function of the strain tensor components $\\boldsymbol{\\varepsilon} = (\\varepsilon_{xx}, \\varepsilon_{yy}, \\varepsilon_{xy})$ is given by:\n$$\n\\Delta E_{\\mathrm{ref}}(\\varepsilon_{xx},\\varepsilon_{yy},\\varepsilon_{xy}) = D\\,(\\varepsilon_{xx}+\\varepsilon_{yy}) + \\alpha\\,(\\varepsilon_{xx}-\\varepsilon_{yy})^{2} + \\beta\\,\\varepsilon_{xy}^{2}\n$$\nwhere the constants are $D = 5.0\\, \\text{eV}$, $\\alpha = 15.0\\, \\text{eV}$, and $\\beta = 10.0\\, \\text{eV}$.\n\nWe are tasked to construct a second-order polynomial surrogate model, $\\widehat{\\Delta E}(\\boldsymbol{\\varepsilon})$, of the form:\n$$\n\\widehat{\\Delta E}(\\boldsymbol{\\varepsilon}) = \\mathbf{x}(\\boldsymbol{\\varepsilon})^T \\mathbf{w}\n$$\nwhere $\\mathbf{w} = [w_0, w_1, \\dots, w_9]^T$ is the vector of coefficients to be determined, and $\\mathbf{x}(\\boldsymbol{\\varepsilon})$ is the vector of polynomial basis functions (monomials) up to degree $2$:\n$$\n\\mathbf{x}(\\boldsymbol{\\varepsilon})^T = [1, \\varepsilon_{xx}, \\varepsilon_{yy}, \\varepsilon_{xy}, \\varepsilon_{xx}^{2}, \\varepsilon_{yy}^{2}, \\varepsilon_{xy}^{2}, \\varepsilon_{xx}\\varepsilon_{yy}, \\varepsilon_{xx}\\varepsilon_{xy}, \\varepsilon_{yy}\\varepsilon_{xy}]\n$$\nThese coefficients $\\mathbf{w}$ are found using Ordinary Least Squares (OLS) regression. First, a training set of $M=2000$ points is generated by drawing strain vectors $\\boldsymbol{\\varepsilon}^{(i)}$ independently and uniformly from the cube $[-s, s]^3$, where $s=0.02$. The corresponding energy shifts $y^{(i)} = \\Delta E_{\\mathrm{ref}}(\\boldsymbol{\\varepsilon}^{(i)})$ are computed.\n\nThis forms a linear system $\\mathbf{y} \\approx \\mathbf{X}\\mathbf{w}$, where $\\mathbf{y}$ is the column vector of the $M$ computed energy shifts and $\\mathbf{X}$ is the $M \\times 10$ design matrix, with each row being $\\mathbf{x}(\\boldsymbol{\\varepsilon}^{(i)})^T$. OLS determines $\\mathbf{w}$ by minimizing the sum of squared residuals, $\\|\\mathbf{y} - \\mathbf{X}\\mathbf{w}\\|_2^2$. The solution is given by the normal equations:\n$$\n\\mathbf{w} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\n$$\nNumerically, this is solved robustly using methods like QR decomposition, as implemented in standard linear algebra software (e.g., `numpy.linalg.lstsq`).\n\nTo validate the surrogate, we can expand the analytical form of $\\Delta E_{\\mathrm{ref}}$:\n$$\n\\Delta E_{\\mathrm{ref}} = 5\\varepsilon_{xx} + 5\\varepsilon_{yy} + 15(\\varepsilon_{xx}^2 - 2\\varepsilon_{xx}\\varepsilon_{yy} + \\varepsilon_{yy}^2) + 10\\varepsilon_{xy}^2\n$$\n$$\n\\Delta E_{\\mathrm{ref}} = 5\\varepsilon_{xx} + 5\\varepsilon_{yy} + 15\\varepsilon_{xx}^2 + 15\\varepsilon_{yy}^2 + 10\\varepsilon_{xy}^2 - 30\\varepsilon_{xx}\\varepsilon_{yy}\n$$\nComparing this to the surrogate's functional form, the theoretical coefficients are:\n- $w_0 = 0$ (constant term)\n- $w_1 = 5$ ($\\varepsilon_{xx}$ term)\n- $w_2 = 5$ ($\\varepsilon_{yy}$ term)\n- $w_3 = 0$ ($\\varepsilon_{xy}$ term)\n- $w_4 = 15$ ($\\varepsilon_{xx}^2$ term)\n- $w_5 = 15$ ($\\varepsilon_{yy}^2$ term)\n- $w_6 = 10$ ($\\varepsilon_{xy}^2$ term)\n- $w_7 = -30$ ($\\varepsilon_{xx}\\varepsilon_{yy}$ term)\n- $w_8 = 0$ ($\\varepsilon_{xx}\\varepsilon_{xy}$ term)\n- $w_9 = 0$ ($\\varepsilon_{yy}\\varepsilon_{xy}$ term)\nThe coefficients obtained from the least-squares fit on the training data must closely match these theoretical values, which confirms the correctness of the surrogate construction.\n\n**2. Uncertainty Propagation via Monte Carlo Simulation**\n\nWith the validated surrogate model $\\widehat{\\Delta E}$ in hand, we propagate the uncertainty from the input strain components to the output energy shift. For each test case, the strain $\\boldsymbol{\\varepsilon}$ is a random vector following a multivariate normal distribution $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$.\n\nWe use Monte Carlo simulation with a sample size of $N=100000$. The procedure is as follows:\n1. Generate $N$ random samples $\\boldsymbol{\\varepsilon}^{(j)}$ from the specified distribution $\\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$.\n2. For each sample, evaluate the surrogate model to obtain an energy shift: $\\widehat{y}^{(j)} = \\widehat{\\Delta E}(\\boldsymbol{\\varepsilon}^{(j)})$.\n3. Use the resulting population of output samples $\\{\\widehat{y}^{(1)}, \\dots, \\widehat{y}^{(N)}\\}$ to estimate the required statistics:\n   - Mean: $\\mathbb{E}[\\widehat{\\Delta E}] \\approx \\bar{y} = \\frac{1}{N}\\sum_{j=1}^{N} \\widehat{y}^{(j)}$\n   - Standard Deviation: $\\sqrt{\\mathrm{Var}[\\widehat{\\Delta E}]} \\approx \\sqrt{\\frac{1}{N}\\sum_{j=1}^{N} (\\widehat{y}^{(j)} - \\bar{y})^2}$\n   - Exceedance Probability: $\\mathbb{P}(|\\widehat{\\Delta E}| > T) \\approx \\frac{1}{N} \\sum_{j=1}^{N} \\mathbf{1}_{|\\widehat{y}^{(j)}| > T}$, where $T=0.02\\, \\text{eV}$ and $\\mathbf{1}$ is the indicator function.\n\nFor test case 3, where the covariance matrix $\\boldsymbol{\\Sigma}_3$ is the zero matrix, the distribution is deterministic. The strain is fixed at its mean value $\\boldsymbol{\\varepsilon} = \\boldsymbol{\\mu}_3$. In this scenario, the energy shift is a single value $\\widehat{\\Delta E}(\\boldsymbol{\\mu}_3)$, the standard deviation is exactly $0$, and the exceedance probability is either $1$ or $0$ depending on whether $|\\widehat{\\Delta E}(\\boldsymbol{\\mu}_3)|$ exceeds the threshold $T$.\n\nThe code in the final answer implements this entire procedure, from data generation and model fitting to the uncertainty analysis for all four specified test cases, and formats the output as required.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Constructs a polynomial surrogate for Dirac-point energy shift and uses it\n    for uncertainty quantification.\n    \"\"\"\n    # 1. Define constants and setup\n    D = 5.0  # eV\n    alpha = 15.0  # eV\n    beta = 10.0  # eV\n    M = 2000  # Number of training samples\n    s = 0.02  # Range for uniform sampling of strain\n    N = 100000  # Number of Monte Carlo samples for UQ\n    T = 0.02  # Threshold for exceedance probability (eV)\n    SEED = 42  # Fixed seed for reproducibility\n    rng = np.random.default_rng(SEED)\n\n    # 2. Reference (ground-truth) model\n    def delta_E_ref(eps_samples):\n        # eps_samples is an array of shape (n_samples, 3) for [exx, eyy, exy]\n        exx, eyy, exy = eps_samples[:, 0], eps_samples[:, 1], eps_samples[:, 2]\n        return D * (exx + eyy) + alpha * (exx - eyy)**2 + beta * exy**2\n\n    # 3. Generate training data\n    train_eps = rng.uniform(low=-s, high=s, size=(M, 3))\n    train_y = delta_E_ref(train_eps)\n\n    # 4. Construct surrogate model via Ordinary Least Squares\n    def build_design_matrix(eps_samples):\n        # eps_samples is an array of shape (n_samples, 3)\n        n_samples = eps_samples.shape[0]\n        exx, eyy, exy = eps_samples[:, 0], eps_samples[:, 1], eps_samples[:, 2]\n        # Basis: 1, exx, eyy, exy, exx^2, eyy^2, exy^2, exx*eyy, exx*exy, eyy*exy\n        X = np.stack([\n            np.ones(n_samples),\n            exx,\n            eyy,\n            exy,\n            exx**2,\n            eyy**2,\n            exy**2,\n            exx * eyy,\n            exx * exy,\n            eyy * exy,\n        ], axis=1)\n        return X\n\n    X_train = build_design_matrix(train_eps)\n    \n    # Solve for weights w using numpy's numerically stable lstsq solver\n    w, residuals, rank, svals = np.linalg.lstsq(X_train, train_y, rcond=None)\n\n    # Define the surrogate model function\n    def delta_E_hat(eps_samples, weights):\n        # eps_samples can be shape (3,) or (n_samples, 3)\n        if eps_samples.ndim == 1:\n            eps_samples = eps_samples.reshape(1, -1)\n        X = build_design_matrix(eps_samples)\n        return X @ weights\n\n    # 5. Define test cases\n    test_cases_params = [\n        {\n            \"mu\": np.array([0.005, 0.005, 0.0]),\n            \"Sigma\": np.diag([0.001**2, 0.001**2, 0.0005**2])\n        },\n        {\n            \"mu\": np.array([0.0, 0.0, 0.005]),\n            \"Sigma\": np.diag([0.001, 0.001, 0.001]) @ \n                     np.array([[1, -0.5, 0], [-0.5, 1, 0], [0, 0, 1]]) @ \n                     np.diag([0.001, 0.001, 0.001])\n        },\n        {\n            \"mu\": np.array([0.01, 0.0, 0.0]),\n            \"Sigma\": np.zeros((3, 3))\n        },\n        {\n            \"mu\": np.array([0.003, 0.007, -0.002]),\n            \"Sigma\": np.diag([0.002, 0.0025, 0.0015]) @ \n                     np.array([[1, 0.7, -0.3], [0.7, 1, 0.2], [-0.3, 0.2, 1]]) @ \n                     np.diag([0.002, 0.0025, 0.0015])\n        }\n    ]\n\n    all_results = []\n\n    # 6. Propagate uncertainty for each case\n    for case in test_cases_params:\n        mu, Sigma = case[\"mu\"], case[\"Sigma\"]\n\n        # Handle the deterministic case separately for correctness\n        if np.all(Sigma == 0):\n            y_hat_val = delta_E_hat(mu, w)[0]\n            mean_val = y_hat_val\n            std_val = 0.0\n            exceed_prob = 1.0 if np.abs(y_hat_val) > T else 0.0\n        else:\n            # Generate MC samples from the input distribution\n            mc_eps_samples = rng.multivariate_normal(mu, Sigma, size=N)\n            # Evaluate the surrogate on these samples\n            y_hat_samples = delta_E_hat(mc_eps_samples, w)\n\n            # Estimate statistics\n            mean_val = np.mean(y_hat_samples)\n            std_val = np.std(y_hat_samples)\n            exceed_prob = np.sum(np.abs(y_hat_samples) > T) / N\n        \n        all_results.append([mean_val, std_val, exceed_prob])\n\n    # 7. Format and print the final output\n    formatted_results = []\n    for res_list in all_results:\n        # Format each number to 6 decimal places as a string\n        formatted_sublist = [f\"{val:.6f}\" for val in res_list]\n        # Create the string representation of the sublist\n        formatted_results.append(f\"[{','.join(formatted_sublist)}]\")\n    \n    # Create the final string representation of the list of lists\n    final_output_str = f\"[{','.join(formatted_results)}]\"\n    print(final_output_str)\n\nsolve()\n```", "id": "2448322"}, {"introduction": "Uncertainty quantification is not just about propagating uncertainties forward; it's also about learning from noisy data. This exercise delves into the \"inverse problem\" using a Bayesian approach to infer the most probable underlying rule of a cellular automaton given a single, imperfect observation. This practice provides a concrete example of how to reason about model uncertainty and perform model selection in a probabilistic framework [@problem_id:2448360].", "problem": "You are given a one-dimensional, binary-state, radius-$1$ cellular automaton of lattice length $L$ with periodic boundary conditions. A cellular automaton rule is an element $r \\in \\{0,1,\\dots,255\\}$ from the set of Wolfram elementary rules. For any rule $r$, define the local update function $f_r:\\{0,1\\}^3 \\to \\{0,1\\}$ as follows. Write $r$ in binary as an $8$-bit string $(b_7,b_6,b_5,b_4,b_3,b_2,b_1,b_0)$, where each $b_k \\in \\{0,1\\}$ and $r = \\sum_{k=0}^{7} b_k 2^k$. The output bit for a neighborhood $(\\ell,c,\\rho) \\in \\{0,1\\}^3$ is given by $f_r(\\ell,c,\\rho) = b_{4\\ell + 2c + \\rho}$. Let the state of the automaton at discrete time $t$ be $\\mathbf{x}^{(t)} \\in \\{0,1\\}^L$, updated synchronously by\n$$\nx^{(t+1)}_i = f_r\\!\\bigl(x^{(t)}_{i-1},\\,x^{(t)}_{i},\\,x^{(t)}_{i+1}\\bigr), \\quad i \\in \\{0,1,\\dots,L-1\\},\n$$\nwith periodic boundary conditions $x^{(t)}_{-1} \\equiv x^{(t)}_{L-1}$ and $x^{(t)}_{L} \\equiv x^{(t)}_{0}$.\n\nYou observe a single noisy final state $\\tilde{\\mathbf{y}} \\in \\{0,1\\}^L$ after $T$ time steps from a known initial state $\\mathbf{x}^{(0)}$. The observational noise model is independent bit flips: conditioned on the true final state $\\mathbf{y} = \\mathbf{x}^{(T)}$, each bit is flipped independently with probability $p \\in [0,1]$. That is, for each site $i$,\n$$\n\\mathbb{P}\\!\\left(\\tilde{y}_i \\mid y_i\\right) =\n\\begin{cases}\n1-p,  \\text{if } \\tilde{y}_i = y_i,\\\\\np,  \\text{if } \\tilde{y}_i \\neq y_i.\n\\end{cases}\n$$\nAssume a uniform prior over rules on $\\{0,1,\\dots,255\\}$. Using Bayes’ theorem, compute for each candidate rule $r$ the posterior probability $\\mathbb{P}(r \\mid \\tilde{\\mathbf{y}}, \\mathbf{x}^{(0)}, T, p)$ up to a common normalization constant, and determine the maximum a posteriori rule index $r_{\\mathrm{MAP}}$. In the event of posterior ties, choose the smallest rule index.\n\nYour program must implement this probabilistic model exactly as specified above and, for each test case provided below, return the single integer $r_{\\mathrm{MAP}}$.\n\nTest suite. For each case, all vectors are ordered as $\\bigl[x_0,x_1,\\dots,x_{L-1}\\bigr]$.\n\n- Case A (general): $L = 8$, $T = 3$, $p = 0.1$, initial $\\mathbf{x}^{(0)} = [\\,0,0,0,1,0,0,0,0\\,]$, observed $\\tilde{\\mathbf{y}} = [\\,0,1,0,1,0,1,0,0\\,]$.\n- Case B (deterministic observation, $T=1$): $L = 8$, $T = 1$, $p = 0$, initial $\\mathbf{x}^{(0)} = [\\,1,0,1,0,1,0,1,0\\,]$, observed $\\tilde{\\mathbf{y}} = [\\,0,1,0,1,0,1,0,1\\,]$.\n- Case C (uninformative noise): $L = 5$, $T = 2$, $p = 0.5$, initial $\\mathbf{x}^{(0)} = [\\,0,0,1,0,0\\,]$, observed $\\tilde{\\mathbf{y}} = [\\,1,1,1,1,1\\,]$.\n- Case D (no evolution, $T=0$): $L = 7$, $T = 0$, $p = 0.2$, initial $\\mathbf{x}^{(0)} = [\\,1,1,0,0,1,0,0\\,]$, observed $\\tilde{\\mathbf{y}} = [\\,1,0,0,0,1,0,0\\,]$.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list of the four integers for Cases A, B, C, and D, in that order, enclosed in square brackets, for example, `[1,2,3,4]`. No spaces are permitted in the output line. All quantities are dimensionless, and any angles, if present, must be in radians; however, no angles are used here. The required outputs are integers only.", "solution": "We formulate the Bayesian inference problem from first principles. A one-dimensional, binary-state, radius-$1$ cellular automaton with periodic boundary conditions evolves by a deterministic local rule $f_r:\\{0,1\\}^3 \\to \\{0,1\\}$ determined by the Wolfram rule index $r \\in \\{0,1,\\dots,255\\}$. Writing $r$ in binary as $(b_7,b_6,\\dots,b_0)$ with $b_k \\in \\{0,1\\}$ such that $r = \\sum_{k=0}^{7} b_k 2^k$, the local update is $f_r(\\ell,c,\\rho) = b_{4\\ell+2c+\\rho}$, where $(\\ell,c,\\rho)$ are the left, center, and right bits of the neighborhood. For a lattice of length $L$, the global synchronous update is\n$$\nx^{(t+1)}_i = f_r\\!\\bigl(x^{(t)}_{i-1},\\,x^{(t)}_{i},\\,x^{(t)}_{i+1}\\bigr), \\quad i=0,1,\\dots,L-1,\n$$\nwith periodic boundary conditions $x^{(t)}_{-1}=x^{(t)}_{L-1}$ and $x^{(t)}_{L}=x^{(t)}_{0}$. Given an initial configuration $\\mathbf{x}^{(0)} \\in \\{0,1\\}^L$ and a time horizon $T \\in \\mathbb{N}_0$, the predicted noiseless final state under rule $r$ is $\\mathbf{y}_r = \\mathbf{x}^{(T)}$ obtained by $T$ iterations of the update.\n\nThe observation model specifies that the observed vector $\\tilde{\\mathbf{y}}$ is generated from the true final state $\\mathbf{y}_r$ by independent bit flips with probability $p \\in [0,1]$. Hence the likelihood for rule $r$ is\n$$\n\\mathcal{L}(r) = \\mathbb{P}(\\tilde{\\mathbf{y}} \\mid r, \\mathbf{x}^{(0)}, T, p) = \\prod_{i=0}^{L-1} \\left[(1-p)\\,\\mathbb{I}\\{\\tilde{y}_i = y_{r,i}\\} + p\\,\\mathbb{I}\\{\\tilde{y}_i \\neq y_{r,i}\\}\\right].\n$$\nLet $m_r$ be the number of matches, i.e., $m_r = \\sum_{i=0}^{L-1} \\mathbb{I}\\{\\tilde{y}_i = y_{r,i}\\}$, so the number of mismatches is $L - m_r$. Because the bits are independent and identically distributed given $\\mathbf{y}_r$, the likelihood simplifies to\n$$\n\\mathcal{L}(r) = (1-p)^{m_r} \\, p^{L - m_r},\n$$\nwith limiting conventions for $p \\in \\{0,1\\}$: when $p=0$ the likelihood is $1$ if $m_r=L$ and $0$ otherwise, and when $p=1$ the likelihood is $1$ if $m_r=0$ and $0$ otherwise.\n\nAssuming a uniform prior $\\mathbb{P}(r) = 1/256$, Bayes’ theorem gives the posterior\n$$\n\\mathbb{P}(r \\mid \\tilde{\\mathbf{y}}, \\mathbf{x}^{(0)}, T, p) = \\frac{\\mathcal{L}(r)\\,\\mathbb{P}(r)}{\\sum_{r'=0}^{255} \\mathcal{L}(r')\\,\\mathbb{P}(r')} \\propto \\mathcal{L}(r).\n$$\nTherefore, the maximum a posteriori (MAP) estimate is\n$$\nr_{\\mathrm{MAP}} \\in \\arg\\max_{r \\in \\{0,\\dots,255\\}} \\mathcal{L}(r),\n$$\nwith ties broken by selecting the smallest $r$. To avoid numerical underflow for $p \\in (0,1)$ and $L$ moderate, computation should be carried out via the log-likelihood,\n$$\n\\log \\mathcal{L}(r) = m_r \\log(1-p) + (L - m_r) \\log p,\n$$\nwith the appropriate limiting cases handled explicitly for $p \\in \\{0,1\\}$.\n\nAlgorithmic design from principles:\n- For each test case, enumerate all $256$ rules $r$.\n- For each $r$, compute $\\mathbf{y}_r$ by iterating the synchronous update $T$ times from $\\mathbf{x}^{(0)}$, using the local rule $f_r$ and periodic boundary conditions.\n- Compute $m_r$ as the count of indices $i \\in \\{0,\\dots,L-1\\}$ for which $y_{r,i} = \\tilde{y}_i$.\n- Compute $\\log \\mathcal{L}(r)$ using the expression above, handling $p=0$ and $p=1$ via their limits.\n- Select the $r$ that maximizes $\\log \\mathcal{L}(r)$; if multiple rules attain the same value within an exact or numerical tolerance, choose the smallest $r$.\n\nEdge cases in the test suite are covered as follows:\n- Case A has $p = 0.1 \\in (0,1)$ and $T=3$, representing a typical inference scenario.\n- Case B has $p = 0$ and $T=1$. In this case, only rules that produce an exact match $\\mathbf{y}_r = \\tilde{\\mathbf{y}}$ have nonzero likelihood. For the provided initial pattern $\\mathbf{x}^{(0)} = [\\,1,0,1,0,1,0,1,0\\,]$, the neighborhoods that occur at time $t=0$ are only $\\{010,101\\}$. The observed $\\tilde{\\mathbf{y}} = [\\,0,1,0,1,0,1,0,1\\,]$ imposes $f_r(0,1,0) = 0$ and $f_r(1,0,1) = 1$. The smallest $r$ consistent with these two outputs is obtained by setting $b_{2} = 0$ and $b_{5} = 1$ and all other $b_k = 0$, yielding $r = 2^5 = 32$.\n- Case C has $p = 0.5$. For any $r$, $\\log \\mathcal{L}(r) = L \\log(0.5)$ is constant, so the posterior is uniform and the tie-breaking rule selects $r_{\\mathrm{MAP}} = 0$.\n- Case D has $T = 0$. The predicted final state equals the initial state for all $r$, so the likelihood depends only on $\\mathbf{x}^{(0)}$ and $\\tilde{\\mathbf{y}}$, not on $r$, and the posterior is uniform. The tie-breaking rule selects $r_{\\mathrm{MAP}} = 0$.\n\nThe program enumerates all rules, computes the log-likelihoods as above, applies the tie-breaking rule, and prints the four integers $[r_{\\mathrm{MAP},A}, r_{\\mathrm{MAP},B}, r_{\\mathrm{MAP},C}, r_{\\mathrm{MAP},D}]$ in a single line without spaces.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef rule_to_table(rule_index: int) -> np.ndarray:\n    \"\"\"\n    Convert a Wolfram elementary cellular automaton rule index (0..255)\n    into a lookup table of length 8, where table[k] is the output bit\n    for neighborhood with decimal code k = 4*left + 2*center + right.\n    \"\"\"\n    # Bits b0..b7 where b_k is (rule_index >> k)  1\n    table = np.array([(rule_index >> k)  1 for k in range(8)], dtype=np.uint8)\n    return table\n\ndef evolve_once(state: np.ndarray, table: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Perform one synchronous update of a 1D binary CA with periodic boundary\n    conditions using the provided rule table.\n    \"\"\"\n    # Periodic neighbors using numpy roll\n    left = np.roll(state, 1)\n    center = state\n    right = np.roll(state, -1)\n    # Neighborhood code: 4*left + 2*center + right\n    codes = (left  2) | (center  1) | right\n    next_state = table[codes]\n    return next_state\n\ndef evolve(state0: np.ndarray, rule_index: int, T: int) -> np.ndarray:\n    \"\"\"\n    Evolve the CA for T steps from initial state0 under rule_index.\n    \"\"\"\n    state = state0.copy()\n    if T = 0:\n        return state\n    table = rule_to_table(rule_index)\n    for _ in range(T):\n        state = evolve_once(state, table)\n    return state\n\ndef log_likelihood(observed: np.ndarray, predicted: np.ndarray, p: float) -> float:\n    \"\"\"\n    Compute log-likelihood under independent bit-flip model with probability p.\n    Handle limiting cases p=0 and p=1 exactly.\n    \"\"\"\n    L = observed.size\n    matches = int(np.sum(observed == predicted))\n    mismatches = L - matches\n    if p == 0.0:\n        return 0.0 if mismatches == 0 else float(\"-inf\")\n    if p == 1.0:\n        return 0.0 if matches == 0 else float(\"-inf\")\n    # General case 0  p  1\n    return matches * np.log(1.0 - p) + mismatches * np.log(p)\n\ndef map_rule_for_case(L: int, T: int, p: float, x0_list, yobs_list) -> int:\n    \"\"\"\n    Compute the MAP rule index (0..255) for the given case parameters,\n    using uniform prior and tie-breaking by smallest index.\n    \"\"\"\n    x0 = np.array(x0_list, dtype=np.uint8)\n    yobs = np.array(yobs_list, dtype=np.uint8)\n    assert x0.size == L and yobs.size == L\n    best_rule = 0\n    best_loglike = float(\"-inf\")\n    tol = 1e-12\n    for r in range(256):\n        ypred = evolve(x0, r, T)\n        ll = log_likelihood(yobs, ypred, p)\n        if ll > best_loglike + tol:\n            best_loglike = ll\n            best_rule = r\n        elif abs(ll - best_loglike) = tol and r  best_rule:\n            best_rule = r\n    return best_rule\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each test case: (L, T, p, x0, y_obs)\n    test_cases = [\n        (8, 3, 0.1, [0,0,0,1,0,0,0,0], [0,1,0,1,0,1,0,0]),  # Case A\n        (8, 1, 0.0, [1,0,1,0,1,0,1,0], [0,1,0,1,0,1,0,1]),  # Case B\n        (5, 2, 0.5, [0,0,1,0,0],       [1,1,1,1,1]),        # Case C\n        (7, 0, 0.2, [1,1,0,0,1,0,0],   [1,0,0,0,1,0,0]),    # Case D\n    ]\n\n    results = []\n    for L, T, p, x0, yobs in test_cases:\n        result = map_rule_for_case(L, T, p, x0, yobs)\n        results.append(result)\n\n    # Final print statement in the exact required format: no spaces.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2448360"}]}