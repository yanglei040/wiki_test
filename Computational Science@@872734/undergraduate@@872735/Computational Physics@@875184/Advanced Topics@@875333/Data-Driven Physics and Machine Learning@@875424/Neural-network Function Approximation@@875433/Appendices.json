{"hands_on_practices": [{"introduction": "Neural networks are often called \"universal function approximators,\" but this power comes with a significant challenge: the curse of dimensionality. This exercise provides a direct, computational experience with this phenomenon, demonstrating how the amount of data required to approximate a function to a fixed accuracy can grow dramatically with the input dimension [@problem_id:2417291]. By implementing a simple random-feature model, you will empirically measure this explosion in sample complexity, gaining a crucial intuition for why naive, brute-force approximation is often infeasible in high-dimensional scientific problems.", "problem": "You are asked to empirically demonstrate the curse of dimensionality in neural-network function approximation by measuring how the required number of training samples grows with input dimension to achieve a fixed approximation accuracy. Implement a complete, runnable program that does the following.\n\nFrom first principles, consider the following setup.\n\n- Domain and target function:\n  - For each input dimension $D \\in \\{\\,1,2,4,8\\,\\}$, define the domain as the unit hypercube $[0,1]^D$.\n  - Define the target function $f_D:[0,1]^D \\to \\mathbb{R}$ by\n    $$f_D(\\mathbf{x}) \\;=\\; \\prod_{i=1}^{D} \\sqrt{2}\\,\\sin\\!\\big(2\\pi x_i\\big),$$\n    where angles are in radians. The multiplicative factor $\\sqrt{2}$ ensures that the mean squared magnitude of $f_D$ under the uniform distribution does not vanish or blow up with $D$.\n\n- Neural-network model and training protocol:\n  - Use a one-hidden-layer feedforward neural network with Rectified Linear Unit (ReLU) activation. The hidden layer has $M$ fixed random features, and only the output layer is trained by ridge-regularized linear least squares. This is a valid neural-network function approximator and a well-known approximation to kernel machines using random features.\n  - Let $\\phi:\\mathbb{R}^D \\to \\mathbb{R}^M$ be the hidden feature map defined by\n    $$\\phi_j(\\mathbf{x}) \\;=\\; \\max\\!\\big(0,\\,\\mathbf{w}_j^\\top \\mathbf{x} + b_j\\big), \\quad j\\in\\{1,\\dots,M\\},$$\n    where each $\\mathbf{w}_j \\in \\mathbb{R}^D$ and $b_j \\in \\mathbb{R}$ are drawn independently from a standard normal distribution. The overall model is\n    $$\\hat{f}_D(\\mathbf{x}) \\;=\\; \\mathbf{a}^\\top \\phi(\\mathbf{x}) + c,$$\n    where $\\mathbf{a}\\in\\mathbb{R}^M$ and $c\\in\\mathbb{R}$ are trained by minimizing ridge-regularized empirical mean squared error on a training set of size $N$.\n  - Training reduces to solving the ridge-regularized normal equations\n    $$(\\Phi^\\top \\Phi + \\lambda I)\\,\\boldsymbol{\\theta} \\;=\\; \\Phi^\\top \\mathbf{y},$$\n    where $\\Phi\\in\\mathbb{R}^{N\\times(M+1)}$ is the design matrix whose $n$-th row is $[\\phi(\\mathbf{x}^{(n)})^\\top,\\;1]$, $\\mathbf{y}\\in\\mathbb{R}^N$ contains targets $f_D(\\mathbf{x}^{(n)})$, $\\boldsymbol{\\theta} \\in \\mathbb{R}^{M+1}$ concatenates $\\mathbf{a}$ and $c$, and $\\lambda>0$ is the ridge regularization strength. Use a fixed $\\lambda$ for all experiments.\n  - Use independent, identically distributed (i.i.d.) samples from the uniform distribution on $[0,1]^D$ for training and validation.\n\n- Accuracy criterion:\n  - Measure the root mean squared error (RMSE) on an i.i.d. validation set:\n    $$\\mathrm{RMSE} \\;=\\; \\sqrt{\\frac{1}{N_{\\mathrm{val}}}\\sum_{n=1}^{N_{\\mathrm{val}}}\\big(\\hat{f}_D(\\mathbf{x}^{(n)}_{\\mathrm{val}})-f_D(\\mathbf{x}^{(n)}_{\\mathrm{val}})\\big)^2}.$$\n  - Declare that a given training set size $N$ is sufficient if $\\mathrm{RMSE} \\le \\varepsilon$ on the validation set.\n\n- Fixed hyperparameters and reproducibility:\n  - Use $M=256$ random hidden units.\n  - Use ridge parameter $\\lambda=10^{-3}$.\n  - Use validation set size $N_{\\mathrm{val}}=2000$.\n  - Use candidate training sizes $N \\in \\{\\,32,64,128,256,512,1024,2048\\,\\}$ and select the smallest $N$ that meets the accuracy criterion.\n  - Use accuracy threshold $\\varepsilon=0.20$.\n  - Use independent random number generators with fixed seeds so that results are exactly reproducible. Angles in the sine are in radians.\n\n- Test suite and output specification:\n  - For each $D \\in \\{\\,1,2,4,8\\,\\}$, with fixed $M$, $\\lambda$, $N_{\\mathrm{val}}$, and $\\varepsilon$ as above, determine the minimal $N$ from the candidate list that achieves $\\mathrm{RMSE}\\le\\varepsilon$. If no candidate $N$ achieves the threshold for a given $D$, return $-1$ for that $D$.\n  - Your program should produce a single line of output containing the results as a comma-separated list of integers enclosed in square brackets, in the order of $D=\\{\\,1,2,4,8\\,\\}$, for example:\n    $$[N_1,N_2,N_3,N_4],$$\n    where each $N_k$ is either the minimal sufficient training size for that dimension or $-1$ if none suffices.\n\nScientific realism and foundational base: You must reason from the definition of mean squared approximation error, the structure of a one-hidden-layer neural network with ReLU activation, and the properties of random features and ridge-regularized least squares. Do not use any data-dependent heuristics beyond what is specified. All angles must be in radians. No physical units are involved in this problem. Express any internal percentages, if any arise, as decimals or fractions, not with the percent sign.", "solution": "This problem demonstrates the curse of dimensionality by measuring the growth in the number of required training samples ($N$) as the input dimension ($D$) increases, for a fixed approximation accuracy ($\\varepsilon$). The solution involves setting up a systematic experiment using a random feature model.\n\n**1. Target Function and Domain**\nFor each dimension $D \\in \\{1, 2, 4, 8\\}$, the target function is defined on the unit hypercube $[0,1]^D$:\n$$f_D(\\mathbf{x}) = \\prod_{i=1}^{D} \\sqrt{2}\\,\\sin(2\\pi x_i)$$\nThe $\\sqrt{2}$ factor normalizes the function such that its average squared value over the domain is 1, independent of $D$. This ensures that any observed increase in approximation difficulty is due to the growing dimensionality and functional complexity, not a change in the function's overall magnitude.\n\n**2. Neural Network Model: Random Features**\nA one-hidden-layer ReLU network is used. To simplify and speed up training, we use a random feature model. The parameters of the hidden layer, weights $\\mathbf{w}_j$ and biases $b_j$, are initialized from a standard normal distribution and then held fixed. The model is:\n$$\\hat{f}_D(\\mathbf{x}) = \\mathbf{a}^\\top \\phi(\\mathbf{x}) + c$$\nwhere $\\phi(\\mathbf{x})$ is the vector of ReLU feature activations: $\\phi_j(\\mathbf{x}) = \\max(0, \\mathbf{w}_j^\\top \\mathbf{x} + b_j)$. This reduces the problem to training only the output layer parameters, $\\mathbf{a}$ and $c$.\n\n**3. Training via Ridge-Regularized Least Squares**\nSince the model is linear in the parameters to be trained ($\\mathbf{a}, c$), we can find the optimal parameters by solving a standard linear least-squares problem, with ridge regularization to prevent overfitting. Given a training set, we form a design matrix $\\Phi$ where each row corresponds to a sample and consists of the feature vector $\\phi(\\mathbf{x}^{(n)})$ augmented with a 1 (for the bias term $c$). The solution for the parameters $\\boldsymbol{\\theta} = [\\mathbf{a}^\\top, c]^\\top$ is found by solving the normal equations:\n$$(\\Phi^\\top \\Phi + \\lambda I)\\,\\boldsymbol{\\theta} = \\Phi^\\top \\mathbf{y}$$\nwhere $\\mathbf{y}$ is the vector of true function values and $\\lambda$ is the regularization strength. This provides a fast, deterministic, and convex training procedure.\n\n**4. Experimental Protocol**\nFor each dimension $D$:\n1.  A fixed set of $M=256$ random features is generated.\n2.  A large, fixed validation set is generated.\n3.  The candidate training set sizes $N$ are tested in increasing order.\n4.  For each $N$, a model is trained on a new training set of that size.\n5.  The model's accuracy is evaluated by computing the Root Mean Squared Error (RMSE) on the validation set.\n6.  The first (smallest) $N$ that achieves an RMSE below the threshold $\\varepsilon=0.20$ is recorded. If no size is sufficient, -1 is recorded.\n\nThis protocol systematically measures the sample complexity. The expected outcome is that the required $N$ will increase significantly with $D$, illustrating how quickly high-dimensional spaces become difficult to sample and learn from.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef target_function(X):\n    \"\"\"\n    Compute f_D(x) = prod_i sqrt(2) * sin(2*pi*x_i) for a batch of inputs X.\n    X: array of shape (N, D)\n    Returns: array of shape (N,)\n    \"\"\"\n    return np.prod(np.sqrt(2.0) * np.sin(2.0 * np.pi * X), axis=1)\n\ndef generate_random_features(D, M, seed):\n    \"\"\"\n    Generate random ReLU feature parameters (W, b) for dimension D and M features.\n    W: shape (M, D), entries ~ N(0,1)\n    b: shape (M,), entries ~ N(0,1)\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    W = rng.standard_normal(size=(M, D))\n    b = rng.standard_normal(size=(M,))\n    return W, b\n\ndef relu_features(X, W, b):\n    \"\"\"\n    Compute ReLU features Phi = max(0, X @ W^T + b) for batch X.\n    X: (N, D), W: (M, D), b: (M,)\n    Returns: Phi of shape (N, M)\n    \"\"\"\n    Z = X @ W.T + b  # broadcasting b over rows\n    return np.maximum(0.0, Z)\n\ndef ridge_solve(Phi, y, lam):\n    \"\"\"\n    Solve (Phi^T Phi + lam I) theta = Phi^T y for theta.\n    Phi should already include a bias column (i.e., augmented with ones).\n    \"\"\"\n    # Normal equations with Tikhonov regularization\n    A = Phi.T @ Phi\n    # Add ridge on all parameters (including bias for simplicity)\n    A.flat[::A.shape[0]+1] += lam  # add lam to diagonal\n    b = Phi.T @ y\n    theta = np.linalg.solve(A, b)\n    return theta\n\ndef rmse(y_true, y_pred):\n    return float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n\ndef minimal_training_size_for_dimension(D, M, lam, eps, N_candidates, N_val,\n                                        seed_features, seed_train_base, seed_val_base):\n    \"\"\"\n    For a given input dimension D, find the minimal N in N_candidates such that\n    RMSE <= eps on a validation set, using fixed random features and ridge training.\n    Returns the minimal N, or -1 if none suffices.\n    \"\"\"\n    # Fixed random features for this D\n    W, b = generate_random_features(D, M, seed_features + D)\n\n    # Fixed validation set for this D\n    rng_val = np.random.default_rng(seed_val_base + D)\n    X_val = rng_val.random(size=(N_val, D))\n    y_val = target_function(X_val)\n    Phi_val = relu_features(X_val, W, b)\n    Phi_val_aug = np.hstack([Phi_val, np.ones((N_val, 1))])\n\n    for N in N_candidates:\n        rng_train = np.random.default_rng(seed_train_base + D + N)\n        X_train = rng_train.random(size=(N, D))\n        y_train = target_function(X_train)\n        Phi_train = relu_features(X_train, W, b)\n        Phi_train_aug = np.hstack([Phi_train, np.ones((N, 1))])\n\n        theta = ridge_solve(Phi_train_aug, y_train, lam)\n        y_pred_val = Phi_val_aug @ theta\n        e = rmse(y_val, y_pred_val)\n        if e <= eps:\n            return N\n    return -1\n\ndef solve():\n    # Experimental settings as specified\n    dims = [1, 2, 4, 8]              # D values\n    M = 256                          # number of random hidden units\n    lam = 1e-3                       # ridge regularization strength\n    eps = 0.20                       # RMSE threshold\n    N_candidates = [32, 64, 128, 256, 512, 1024, 2048]\n    N_val = 2000\n\n    # Fixed seeds for reproducibility\n    seed_features = 12345\n    seed_train_base = 54321\n    seed_val_base = 99999\n\n    results = []\n    for D in dims:\n        N_min = minimal_training_size_for_dimension(\n            D=D, M=M, lam=lam, eps=eps, N_candidates=N_candidates, N_val=N_val,\n            seed_features=seed_features, seed_train_base=seed_train_base, seed_val_base=seed_val_base\n        )\n        results.append(N_min)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2417291"}, {"introduction": "A powerful strategy for overcoming the challenges of high-dimensional approximation is to embed prior knowledge about a problem into the network's structure, a concept known as inductive bias. This practice explores this idea by tasking you with learning the solution operator for a translation-invariant partial differential equation [@problem_id:2417315]. You will compare a generic, fully connected network with a convolutional neural network (CNN), whose architecture inherently respects translation symmetry, revealing how the right inductive bias can enable astounding data efficiency and generalization.", "problem": "Consider the one-dimensional, translation-invariant elliptic partial differential equation (PDE) on the periodic domain given by $-u''(x) + a\\,u(x) = f(x)$ with periodic boundary conditions on $[0,1)$. Discretize the problem on a uniform grid of $N$ points with spacing $h = 1/N$. The standard centered second-difference yields a circulant linear system for the discrete solution vector $u \\in \\mathbb{R}^N$ and forcing vector $f \\in \\mathbb{R}^N$. The discrete operator is diagonalized by the discrete Fourier transform: each mode indexed by integer $k \\in \\{0,1,\\dots,N-1\\}$ has eigenvalue $\\lambda_k = a + \\dfrac{4}{h^2}\\sin^2\\!\\left(\\dfrac{\\pi k}{N}\\right)$. Consequently, the exact discrete solution is obtained by dividing the Fourier coefficients of $f$ by $\\lambda_k$ and transforming back to physical space.\n\nIn this problem you will implement from first principles two neural network surrogates for the solution operator $T: f \\mapsto u$, trained only on a single input-output example, and compare their performance to demonstrate the effect of inductive bias for translation-invariant operators:\n\n- A fully connected multilayer perceptron (MLP) with no hidden layers (i.e., a single linear map), represented by a dense matrix $W \\in \\mathbb{R}^{N \\times N}$ acting as $u_{\\text{MLP}} = W f$.\n\n- A one-dimensional circular convolutional neural network (CNN) with one linear convolutional layer having an odd-length kernel of size $K$, represented by a kernel vector $w \\in \\mathbb{R}^{K}$ that acts via circular convolution $u_{\\text{CNN}}[i] = \\sum_{s=-m}^{m} w[s+m]\\; f[(i - s) \\bmod N]$, where $m = (K-1)/2$.\n\nBoth models will be trained by minimizing the mean-squared error loss between the model output and the exact discrete solution produced by the Fourier diagonalization described above. Use simple gradient descent with fixed learning rates and a fixed number of iterations. Initialize all trainable parameters to zero. Angles in any trigonometric expressions must be in radians.\n\nYour implementation must respect the following fixed, scientifically consistent parameter choices and dataset definitions:\n\n- Grid size: $N = 64$ and spacing $h = 1/N$.\n\n- PDE parameter: $a = 1$.\n\n- CNN kernel size: $K = 31$ (so $m = 15$).\n\n- Training set: a single training pair $(f^{\\text{train}}, u^{\\text{train}})$ where $f^{\\text{train}}$ is the unit impulse at index $0$ (i.e., $f^{\\text{train}}[0] = 1$ and $f^{\\text{train}}[i] = 0$ for all $i \\neq 0$) and $u^{\\text{train}}$ is the exact discrete solution for that forcing computed by Fourier diagonalization.\n\n- Optimization details: use constant-step gradient descent with $T_{\\text{MLP}} = 2000$ iterations and learning rate $\\eta_{\\text{MLP}} = 0.2$ for the MLP, and $T_{\\text{CNN}} = 2000$ iterations and learning rate $\\eta_{\\text{CNN}} = 0.05$ for the CNN. For the MLP with loss $L = \\dfrac{1}{N} \\|W f - u\\|_2^2$, update by $W \\leftarrow W - \\eta_{\\text{MLP}} \\dfrac{2}{N} (W f - u) f^\\top$. For the CNN with loss $L = \\dfrac{1}{N} \\|w \\circledast f - u\\|_2^2$, where $\\circledast$ denotes circular convolution as defined above, update by $w[s+m] \\leftarrow w[s+m] - \\eta_{\\text{CNN}} \\dfrac{2}{N} \\sum_{i=0}^{N-1} \\big( (w \\circledast f)[i] - u[i] \\big)\\, f[(i - s) \\bmod N]$ for all $s \\in \\{-m,\\dots,m\\}$.\n\nDesign a test suite to assess translation generalization and frequency response using four fixed forcings $f^{(j)}$ and their exact discrete solutions $u^{(j)}$:\n\n- Case $1$: $f^{(1)}$ is the unit impulse at index $0$.\n\n- Case $2$: $f^{(2)}$ is the unit impulse at index $16$.\n\n- Case $3$: $f^{(3)}[i] = \\cos\\!\\left(2 \\pi \\cdot 4 \\cdot \\dfrac{i}{N}\\right)$ for all $i \\in \\{0,\\dots,N-1\\}$ (angles in radians).\n\n- Case $4$: $f^{(4)}[i] = 1$ for all $i \\in \\{0,\\dots,N-1\\}$.\n\nFor each case, compute the relative $\\ell_2$ error of each model with respect to the exact solution, defined by $\\varepsilon_{\\text{model}} = \\dfrac{\\|u_{\\text{model}} - u^{(j)}\\|_2}{\\|u^{(j)}\\|_2}$, for the MLP and CNN respectively.\n\nYour program must implement all of the above, using the specified hyperparameters, and produce a single line of output containing the eight floating-point results in the following order: $[\\varepsilon_{\\text{MLP}}^{(1)}, \\varepsilon_{\\text{CNN}}^{(1)}, \\varepsilon_{\\text{MLP}}^{(2)}, \\varepsilon_{\\text{CNN}}^{(2)}, \\varepsilon_{\\text{MLP}}^{(3)}, \\varepsilon_{\\text{CNN}}^{(3)}, \\varepsilon_{\\text{MLP}}^{(4)}, \\varepsilon_{\\text{CNN}}^{(4)}]$. The line must be printed as a comma-separated list enclosed in square brackets, with no additional text.\n\nAll quantities are dimensionless under the chosen nondimensionalization of the PDE. Ensure your code is a complete, runnable program that requires no input and uses only the allowed libraries. The final answer must be expressed as floats in the specified single-line format.", "solution": "The problem requires comparing a Multilayer Perceptron (MLP) and a Convolutional Neural Network (CNN) as surrogate models for the solution operator of a one-dimensional, translation-invariant elliptic PDE. The solution involves establishing a ground truth via Fourier analysis, defining and training both neural network models on an impulse response, and then evaluating their generalization performance on various test inputs.\n\n**1. Exact Discrete Solution via Fourier Method**\n\nThe governing PDE, $-u''(x) + a\\,u(x) = f(x)$, when discretized with centered finite differences on a periodic grid, becomes a linear system $L\\mathbf{u} = \\mathbf{f}$, where $L$ is a circulant matrix. Circulant matrices are diagonalized by the Discrete Fourier Transform (DFT). This means that in the Fourier domain, the operation becomes a simple element-wise division. The exact discrete solution $\\mathbf{u}$ for a given forcing term $\\mathbf{f}$ can be computed as:\n$$ \\mathbf{u} = \\text{IDFT}\\left(\\frac{\\text{DFT}(\\mathbf{f})}{\\mathbf{\\lambda}}\\right) $$\nwhere IDFT is the Inverse Discrete Fourier Transform, and $\\mathbf{\\lambda}$ is the vector of eigenvalues of the discrete operator, given by $\\lambda_k = a + \\frac{4}{h^2}\\sin^2(\\frac{\\pi k}{N})$ for each Fourier mode $k$. This method provides the exact target solutions for training and evaluation.\n\n**2. The Multilayer Perceptron (MLP) Model**\n\nThe MLP model is a linear map represented by a dense $N \\times N$ matrix $W$, so $\\mathbf{u}_{\\text{MLP}} = W \\mathbf{f}$. It has $N^2$ parameters and no built-in structural assumptions. It is trained on a single pair $(\\mathbf{f}^{\\text{train}}, \\mathbf{u}^{\\text{train}})$, where $\\mathbf{f}^{\\text{train}}$ is a unit impulse at index 0. The gradient descent update only modifies the first column of $W$, because the input $\\mathbf{f}^{\\text{train}}$ is zero everywhere else. The MLP thus learns a mapping for an impulse at index 0 but has no mechanism to infer the response for an impulse at any other location. This lack of an appropriate *inductive bias* (translation invariance) means it is expected to fail on any test case that is not identical to the training input.\n\n**3. The Convolutional Neural Network (CNN) Model**\n\nThe CNN model implements the solution operator as a circular convolution with a kernel $\\mathbf{w}$ of size $K$: $\\mathbf{u}_{\\text{CNN}} = \\mathbf{w} \\circledast \\mathbf{f}$. This model has only $K$ parameters. The convolution operation is inherently translation-equivariant, which is the correct inductive bias for this problem, as the underlying PDE operator is linear and translation-invariant (LTI).\n\nWhen trained on the impulse-response pair $(\\mathbf{f}^{\\text{train}}, \\mathbf{u}^{\\text{train}})$, the CNN learns to shape its kernel $\\mathbf{w}$ into an approximation of the system's true impulse response (Green's function). Since the response of any LTI system to an arbitrary input is the convolution of that input with the impulse response, the trained CNN should be able to accurately predict the solution for a wide range of inputs, demonstrating excellent generalization from a single training example.\n\n**4. Evaluation and Expected Outcome**\n\nThe models are tested on four cases: the training impulse, a translated impulse, a cosine wave, and a constant function. The MLP is expected to have low error only on the first case and high error on the others. The CNN is expected to achieve low error across all test cases, clearly demonstrating the dramatic benefit of incorporating correct physical symmetries into the model architecture. The relative $\\ell_2$ error is used to quantify this performance difference.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport scipy.fft\n\ndef solve():\n    \"\"\"\n    Implements and compares MLP and CNN surrogates for a 1D elliptic PDE.\n    \"\"\"\n    # -- 1. Define constants and problem parameters --\n    N = 64\n    h = 1.0 / N\n    a = 1.0\n    K = 31\n    m = (K - 1) // 2\n    T_MLP = 2000\n    eta_MLP = 0.2\n    T_CNN = 2000\n    eta_CNN = 0.05\n\n    # -- 2. Define helper functions --\n    def exact_solver(f_vec):\n        \"\"\"\n        Computes the exact discrete solution using Fourier diagonalization.\n        \"\"\"\n        k = np.arange(N)\n        lambda_k = a + (4 / h**2) * np.sin(np.pi * k / N)**2\n        # Handle the case where a mode might be numerically zero, though not here.\n        # Add a small epsilon to the denominator for stability if needed.\n        f_hat = scipy.fft.fft(f_vec)\n        u_hat = f_hat / lambda_k\n        u_vec = scipy.fft.ifft(u_hat)\n        # The solution for a real forcing must be real.\n        return u_vec.real\n\n    def circular_conv(w_kernel, f_vec):\n        \"\"\"\n        Performs 1D circular convolution.\n        w_kernel has length K, f_vec has length N.\n        \"\"\"\n        # Embed the odd-length kernel into a full N-length vector for FFT-based convolution\n        # Center of kernel w_kernel[m] goes to kernel_full[0]\n        # w_kernel[m+s] for s in [-m, m] goes to kernel_full[s mod N]\n        kernel_full = np.zeros_like(f_vec, dtype=np.float64)\n        s_vals = np.arange(-m, m + 1)\n        for s_idx, s in enumerate(s_vals):\n            kernel_full[s % N] = w_kernel[s_idx]\n        \n        # Use convolution theorem for efficiency\n        return scipy.fft.ifft(scipy.fft.fft(kernel_full) * scipy.fft.fft(f_vec)).real\n\n    # -- 3. Generate training data --\n    f_train = np.zeros(N)\n    f_train[0] = 1.0\n    u_train = exact_solver(f_train)\n\n    # -- 4. Train the MLP model --\n    W = np.zeros((N, N))\n    for _ in range(T_MLP):\n        u_pred_mlp = W @ f_train\n        error_mlp = u_pred_mlp - u_train\n        # Gradient update for L = (1/N) * ||Wf - u||^2\n        grad_W = (2.0 / N) * np.outer(error_mlp, f_train)\n        W -= eta_MLP * grad_W\n\n    # -- 5. Train the CNN model --\n    w = np.zeros(K)\n    for _ in range(T_CNN):\n        # Use direct summation for gradient calculation as it's simpler to implement from the formula\n        u_pred_cnn = np.zeros_like(f_train)\n        s_vals = np.arange(-m, m + 1)\n        for s_idx, s in enumerate(s_vals):\n            u_pred_cnn += w[s_idx] * np.roll(f_train, s)\n\n        error_cnn = u_pred_cnn - u_train\n        grad_w = np.zeros(K)\n        \n        # Gradient for L = (1/N) * ||w*f - u||^2\n        # dL/dw_j = (2/N) * sum_i ( (w*f)_i - u_i ) * f_{i-j_s}\n        for s_idx, s in enumerate(s_vals):\n            # The sum is the dot product of the error and the shifted input\n            grad_w[s_idx] = np.dot(error_cnn, np.roll(f_train, s))\n        \n        grad_w *= (2.0 / N)\n        w -= eta_CNN * grad_w\n\n    # -- 6. Define test cases and evaluate models --\n    # Case 1: Unit impulse at index 0 (training case)\n    f1 = np.zeros(N); f1[0] = 1.0\n    # Case 2: Unit impulse at index 16\n    f2 = np.zeros(N); f2[16] = 1.0\n    # Case 3: Cosine wave\n    i_indices = np.arange(N)\n    f3 = np.cos(2 * np.pi * 4 * i_indices / N)\n    # Case 4: Constant function (zero-frequency wave)\n    f4 = np.ones(N)\n\n    test_forcings = [f1, f2, f3, f4]\n    test_solutions = [exact_solver(f) for f in test_forcings]\n    \n    results = []\n    for f_test, u_exact in zip(test_forcings, test_solutions):\n        # Prevent division by zero, although not expected here\n        norm_u_exact = np.linalg.norm(u_exact)\n        if norm_u_exact == 0:\n            norm_u_exact = 1.0\n\n        # MLP prediction and error\n        u_mlp = W @ f_test\n        err_mlp = np.linalg.norm(u_mlp - u_exact) / norm_u_exact\n        results.append(err_mlp)\n        \n        # CNN prediction and error\n        u_cnn = circular_conv(w, f_test)\n        err_cnn = np.linalg.norm(u_cnn - u_exact) / norm_u_exact\n        results.append(err_cnn)\n\n    # -- 7. Print final results in the specified format --\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2417315"}, {"introduction": "Beyond architectural choices, physical principles can be directly enforced during training by incorporating them into the loss function—the core idea behind Physics-Informed Neural Networks (PINNs). This exercise guides you through constructing and evaluating the components of a PINN loss for the viscous Burgers' equation, including a novel term that encourages the solution to respect a known Lie symmetry (a Galilean boost) [@problem_id:2417275]. This practice offers a glimpse into an advanced technique where the network learns not just from data points, but from the fundamental laws and symmetries of physics themselves.", "problem": "You are asked to formalize and compute a physics-informed neural network loss for a one-dimensional viscous Burgers equation that explicitly includes a term encouraging equivariance under a known Lie symmetry of the equation. Your program must implement the precise definitions given below and return the total loss values for a specified test suite.\n\nThe target partial differential equation is the one-dimensional viscous Burgers equation on the space-time domain $\\{(x,t) \\mid x \\in [-1,1],\\ t \\in [0,1]\\}$:\n$$\nu_t(x,t) + u(x,t)\\,u_x(x,t) - \\nu\\,u_{xx}(x,t) = 0,\n$$\nwhere $\\nu > 0$ is the kinematic viscosity. Consider the Cauchy problem consisting of this equation together with the initial condition\n$$\nu(x,0) = 0 \\quad \\text{for all } x \\in [-1,1],\n$$\nand Dirichlet boundary conditions\n$$\nu(-1,t) = 0, \\quad u(1,t) = 0 \\quad \\text{for all } t \\in [0,1].\n$$\n\nA known Lie symmetry of the viscous Burgers equation is the Galilean boost: for any constant $c \\in \\mathbb{R}$, if $u(x,t)$ is a solution, then\n$$\nu'(x,t) = u(x - c\\,t, t) + c\n$$\nis also a solution. To encourage a neural-network approximation to respect this symmetry, introduce a symmetry-consistency penalty defined by averaging the squared deviation from this equivariance relation over a prescribed set of points $(x,t)$.\n\nYou will use a fixed single-hidden-layer neural network approximation $u_\\theta(x,t)$ with hyperbolic tangent activation (where $\\tanh$ denotes the hyperbolic tangent):\n$$\nu_\\theta(x,t) = W_2 \\,\\tanh\\!\\bigl(W_1 \\,[x,\\ t]^\\top + b_1 \\bigr) + b_2,\n$$\nwhere $W_1 \\in \\mathbb{R}^{3 \\times 2}$, $b_1 \\in \\mathbb{R}^3$, $W_2 \\in \\mathbb{R}^{1 \\times 3}$, and $b_2 \\in \\mathbb{R}$. All derivatives $u_t(x,t)$, $u_x(x,t)$, and $u_{xx}(x,t)$ should be computed exactly by differentiating this parametric function with respect to $x$ and $t$.\n\nUse the following fixed parameter values for the neural network:\n- $W_1 = \\begin{bmatrix} 1.2 & -0.7 \\\\ -0.3 & 0.9 \\\\ 0.5 & 1.1 \\end{bmatrix}$,\n- $b_1 = \\begin{bmatrix} 0.1 \\\\ -0.2 \\\\ 0.3 \\end{bmatrix}$,\n- $W_2 = \\begin{bmatrix} 0.8 & -0.5 & 0.3 \\end{bmatrix}$,\n- $b_2 = -0.1$.\n\nDefine the following mean-squared loss components:\n- The partial differential equation residual loss\n$$\n\\mathcal{L}_{\\mathrm{pde}} = \\frac{1}{N_{\\mathrm{p}}} \\sum_{(x,t) \\in \\mathcal{P}} \\bigl( u_t(x,t) + u(x,t)\\,u_x(x,t) - \\nu\\,u_{xx}(x,t) \\bigr)^2,\n$$\nwhere the set of partial differential equation collocation points is\n$$\n\\mathcal{P} = \\{(-0.8,\\,0.2),\\ (0,\\,0.5),\\ (0.9,\\,0.9),\\ (0.5,\\,0.1)\\},\n$$\nand $N_{\\mathrm{p}} = 4$.\n- The initial condition loss\n$$\n\\mathcal{L}_{\\mathrm{ic}} = \\frac{1}{N_{\\mathrm{ic}}} \\sum_{x \\in \\mathcal{X}_{\\mathrm{ic}}} \\bigl(u(x,0)\\bigr)^2,\n$$\nwhere\n$$\n\\mathcal{X}_{\\mathrm{ic}} = \\{-1,\\ -0.3,\\ 0.2,\\ 1\\},\n$$\nand $N_{\\mathrm{ic}} = 4$.\n- The boundary condition loss\n$$\n\\mathcal{L}_{\\mathrm{bc}} = \\frac{1}{N_{\\mathrm{b}}} \\sum_{t \\in \\mathcal{T}_{\\mathrm{bc}}} \\Bigl( \\bigl(u(-1,t)\\bigr)^2 + \\bigl(u(1,t)\\bigr)^2 \\Bigr),\n$$\nwhere\n$$\n\\mathcal{T}_{\\mathrm{bc}} = \\{0,\\ 0.4,\\ 1\\},\n$$\nand $N_{\\mathrm{b}} = 6$.\n- The symmetry-consistency loss for a given boost parameter $c \\in \\mathbb{R}$\n$$\n\\mathcal{L}_{\\mathrm{sym}}(c) = \\frac{1}{N_{\\mathrm{s}}} \\sum_{(x,t) \\in \\mathcal{S}} \\Bigl( u(x,t) - \\bigl(u(x - c\\,t, t) + c\\bigr) \\Bigr)^2,\n$$\nwhere\n$$\n\\mathcal{S} = \\{(-0.6,\\,0.3),\\ (0.2,\\,0.7),\\ (0.95,\\,0.5),\\ (-1,\\,1)\\},\n$$\nand $N_{\\mathrm{s}} = 4$.\n\nUsing the positive weights\n$$\nw_{\\mathrm{pde}} = 1,\\quad w_{\\mathrm{ic}} = 10,\\quad w_{\\mathrm{bc}} = 10,\\quad w_{\\mathrm{sym}} = 1,\n$$\ndefine the total loss\n$$\n\\mathcal{L}_{\\mathrm{total}}(\\nu,c) = w_{\\mathrm{pde}}\\,\\mathcal{L}_{\\mathrm{pde}} + w_{\\mathrm{ic}}\\,\\mathcal{L}_{\\mathrm{ic}} + w_{\\mathrm{bc}}\\,\\mathcal{L}_{\\mathrm{bc}} + w_{\\mathrm{sym}}\\,\\mathcal{L}_{\\mathrm{sym}}(c).\n$$\n\nYour program must compute and report $\\mathcal{L}_{\\mathrm{total}}(\\nu,c)$ for each item of the following test suite, with the same neural network and collocation sets:\n- Test $1$: $(\\nu,c) = (0.1,\\,0)$,\n- Test $2$: $(\\nu,c) = (0.1,\\,0.5)$,\n- Test $3$: $(\\nu,c) = (0.01,\\,-0.75)$.\n\nFinal output format: Your program should produce a single line of output containing the three total loss values for Tests $1$–$3$ as a comma-separated list enclosed in square brackets, each value rounded to six decimal places (for example, $[0.123456,0.234567,0.345678]$). Angles do not appear in this problem. There are no physical units in this problem; all quantities are dimensionless.", "solution": "The problem requires the computation of a composite loss function for a Physics-Informed Neural Network (PINN). The solution involves two main parts: first, deriving the analytical expressions for the partial derivatives of the given neural network function $u_\\theta(x,t)$; and second, implementing the four specified loss components ($\\mathcal{L}_{\\mathrm{pde}}$, $\\mathcal{L}_{\\mathrm{ic}}$, $\\mathcal{L}_{\\mathrm{bc}}$, $\\mathcal{L}_{\\mathrm{sym}}$) and summing them with the given weights.\n\n**1. Analytical Derivatives of the Neural Network**\n\nThe neural network is defined as $u_\\theta(x,t) = W_2 \\,\\tanh\\!\\bigl(W_1 \\,[x,\\ t]^\\top + b_1 \\bigr) + b_2$. Let the input be $p = [x, t]^\\top$, the pre-activation be $z(p) = W_1 p + b_1$, and the hidden activation be $a(z) = \\tanh(z)$. The output is then $u_\\theta(p) = W_2 a(z) + b_2$.\n\nTo compute the PDE residual, we need the partial derivatives $u_t$, $u_x$, and $u_{xx}$. These are found by applying the chain rule. The derivative of the activation function is $\\frac{d}{ds}\\tanh(s) = 1 - \\tanh^2(s)$.\n\nThe first-order partial derivatives are components of the gradient $\\nabla u_\\theta = [\\frac{\\partial u_\\theta}{\\partial x}, \\frac{\\partial u_\\theta}{\\partial t}]$. Let's denote the columns of $W_1$ as $W_{1x}$ and $W_{1t}$, such that $W_1 = [W_{1x}, W_{1t}]$. The derivatives are:\n$$\nu_x(x,t) = \\frac{\\partial u_\\theta}{\\partial x} = \\sum_{j=1}^3 (W_2)_{1j} \\cdot (1-\\tanh^2(z_j)) \\cdot (W_1)_{j1}\n$$\n$$\nu_t(x,t) = \\frac{\\partial u_\\theta}{\\partial t} = \\sum_{j=1}^3 (W_2)_{1j} \\cdot (1-\\tanh^2(z_j)) \\cdot (W_1)_{j2}\n$$\nIn vector notation, using element-wise product $\\odot$: $u_x = W_2 \\cdot ((1-a^2) \\odot W_{1x})$ and $u_t = W_2 \\cdot ((1-a^2) \\odot W_{1t})$.\n\nThe second-order partial derivative $u_{xx}$ is found by differentiating $u_x$ with respect to $x$:\n$$\n\\frac{\\partial u_x}{\\partial x} = \\sum_{j=1}^3 (W_2)_{1j} (W_1)_{j1} \\frac{\\partial}{\\partial x}(1 - \\tanh^2(z_j))\n$$\nUsing the chain rule again on the inner term: $\\frac{\\partial}{\\partial x}(1 - \\tanh^2(z_j)) = -2 \\tanh(z_j) (1 - \\tanh^2(z_j)) \\frac{\\partial z_j}{\\partial x} = -2 a_j (1 - a_j^2) (W_1)_{j1}$.\nSubstituting this back, we get:\n$$\nu_{xx}(x,t) = -2 \\sum_{j=1}^3 (W_2)_{1j} (W_1)_{j1}^2 a_j (1 - a_j^2)\n$$\n\n**2. Computation of Loss Components**\n\nWith these analytical derivatives, the program will execute the following steps for each test case $(\\nu, c)$:\n1.  **PDE Loss ($\\mathcal{L}_{\\mathrm{pde}}$):** For each point $(x,t)$ in $\\mathcal{P}$, compute $u_\\theta, u_t, u_x, u_{xx}$. Calculate the PDE residual $R_{pde} = u_t + u_\\theta u_x - \\nu u_{xx}$. The loss is the mean of $R_{pde}^2$ over all points in $\\mathcal{P}$.\n2.  **Initial Condition Loss ($\\mathcal{L}_{\\mathrm{ic}}$):** For each point $x$ in $\\mathcal{X}_{\\mathrm{ic}}$, compute $u_\\theta(x,0)$. The loss is the mean of $(u_\\theta(x,0))^2$ over all points.\n3.  **Boundary Condition Loss ($\\mathcal{L}_{\\mathrm{bc}}$):** For each time $t$ in $\\mathcal{T}_{\\mathrm{bc}}$, compute $u_\\theta(-1,t)$ and $u_\\theta(1,t)$. The loss is the mean of $(u_\\theta(-1,t))^2 + (u_\\theta(1,t))^2$ over all time points (i.e., average of 6 squared terms).\n4.  **Symmetry Loss ($\\mathcal{L}_{\\mathrm{sym}}$):** For each point $(x,t)$ in $\\mathcal{S}$, compute $u_\\theta(x,t)$ and $u_\\theta(x-ct, t)$. Calculate the symmetry residual $R_{sym} = u_\\theta(x,t) - (u_\\theta(x-ct, t) + c)$. The loss is the mean of $R_{sym}^2$ over all points in $\\mathcal{S}$.\n5.  **Total Loss ($\\mathcal{L}_{\\mathrm{total}}$):** Combine the individual losses using the specified weights: $\\mathcal{L}_{\\mathrm{total}} = w_{\\mathrm{pde}}\\mathcal{L}_{\\mathrm{pde}} + w_{\\mathrm{ic}}\\mathcal{L}_{\\mathrm{ic}} + w_{\\mathrm{bc}}\\mathcal{L}_{\\mathrm{bc}} + w_{\\mathrm{sym}}\\mathcal{L}_{\\mathrm{sym}}(c)$.\n\nThe final output is a list of the computed total loss values for the three test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    # Define fixed network parameters\n    W1 = np.array([[1.2, -0.7], [-0.3, 0.9], [0.5, 1.1]])\n    b1 = np.array([0.1, -0.2, 0.3]).reshape(3, 1)\n    W2 = np.array([0.8, -0.5, 0.3]).reshape(1, 3)\n    b2 = np.array([-0.1])\n\n    # Define collocation point sets\n    P_points = [(-0.8, 0.2), (0.0, 0.5), (0.9, 0.9), (0.5, 0.1)]\n    X_ic_points = [-1.0, -0.3, 0.2, 1.0]\n    T_bc_points = [0.0, 0.4, 1.0]\n    S_points = [(-0.6, 0.3), (0.2, 0.7), (0.95, 0.5), (-1.0, 1.0)]\n\n    # Define loss component weights\n    w_pde = 1.0\n    w_ic = 10.0\n    w_bc = 10.0\n    w_sym = 1.0\n\n    # Test cases to evaluate\n    test_cases = [\n        (0.1, 0.0),    # Test 1\n        (0.1, 0.5),    # Test 2\n        (0.01, -0.75)  # Test 3\n    ]\n\n    memo_u = {}\n    memo_derivs = {}\n\n    def get_u_and_derivatives(x, t):\n        \"\"\"Computes u and its partial derivatives at (x, t) using analytical formulas.\"\"\"\n        if (x, t) in memo_derivs:\n            return memo_derivs[(x, t)]\n\n        p = np.array([x, t]).reshape(2, 1)\n        \n        # Forward pass\n        z = W1 @ p + b1\n        a = np.tanh(z)\n        u_val = (W2 @ a + b2).item()\n        \n        # Derivatives calculation\n        d_tanh_z = 1 - a**2  # Derivative of tanh(z) w.r.t. z\n        \n        # First-order derivatives\n        W1_x = W1[:, 0].reshape(3, 1)\n        W1_t = W1[:, 1].reshape(3, 1)\n        \n        u_x_val = (W2 @ (d_tanh_z * W1_x)).item()\n        u_t_val = (W2 @ (d_tanh_z * W1_t)).item()\n        \n        # Second-order derivative u_xx\n        u_xx_vec_contrib = -2 * a * d_tanh_z * (W1_x**2)\n        u_xx_val = (W2 @ u_xx_vec_contrib).item()\n        \n        result = (u_val, u_t_val, u_x_val, u_xx_val)\n        memo_derivs[(x, t)] = result\n        memo_u[(x, t)] = u_val\n        return result\n\n    def get_u(x, t):\n        \"\"\"Computes u(x, t) using memoization.\"\"\"\n        if (x, t) in memo_u:\n            return memo_u[(x, t)]\n        \n        u_val, _, _, _ = get_u_and_derivatives(x, t)\n        return u_val\n\n    def calculate_loss_pde(nu):\n        \"\"\"Calculates the PDE residual loss.\"\"\"\n        loss_sum = 0.0\n        for x, t in P_points:\n            u, u_t, u_x, u_xx = get_u_and_derivatives(x, t)\n            residual = u_t + u * u_x - nu * u_xx\n            loss_sum += residual**2\n        return loss_sum / len(P_points)\n\n    def calculate_loss_ic():\n        \"\"\"Calculates the initial condition loss.\"\"\"\n        loss_sum = 0.0\n        for x in X_ic_points:\n            u_val = get_u(x, 0.0)\n            loss_sum += u_val**2\n        return loss_sum / len(X_ic_points)\n\n    def calculate_loss_bc():\n        \"\"\"Calculates the boundary condition loss.\"\"\"\n        loss_sum = 0.0\n        for t in T_bc_points:\n            u_neg1 = get_u(-1.0, t)\n            u_pos1 = get_u(1.0, t)\n            loss_sum += u_neg1**2 + u_pos1**2\n        return loss_sum / (2 * len(T_bc_points))\n\n    def calculate_loss_sym(c):\n        \"\"\"Calculates the symmetry-consistency loss.\"\"\"\n        loss_sum = 0.0\n        for x, t in S_points:\n            u_original = get_u(x, t)\n            x_prime = x - c * t\n            u_transformed = get_u(x_prime, t)\n            u_prime = u_transformed + c\n            residual = u_original - u_prime\n            loss_sum += residual**2\n        return loss_sum / len(S_points)\n\n    results = []\n    for nu, c in test_cases:\n        # Clear memoization for each test case as nu/c change dependencies\n        memo_derivs.clear()\n        memo_u.clear()\n        \n        l_pde = calculate_loss_pde(nu)\n        l_ic = calculate_loss_ic()\n        l_bc = calculate_loss_bc()\n        l_sym = calculate_loss_sym(c)\n        \n        total_loss = w_pde * l_pde + w_ic * l_ic + w_bc * l_bc + w_sym * l_sym\n        results.append(f\"{total_loss:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2417275"}]}