{"hands_on_practices": [{"introduction": "Our exploration begins with a foundational application of histogram reweighting to one of the most celebrated models in statistical physics: the Ising model. This exercise challenges you to move beyond single-temperature analysis by combining simulated energy histograms from different temperatures to reconstruct the model's specific heat curve, $C(T)$ [@problem_id:2401585]. By implementing the core iterative equations of the multiple histogram method, you will learn how to extract a continuous thermodynamic response from a small number of discrete simulations, a powerful technique for efficiently studying phase transitions.", "problem": "Consider the two-dimensional ferromagnetic Ising model on a square lattice of linear size $L$ with periodic boundary conditions, coupling $J=1$, and Boltzmann constant set to $k_B=1$. The Hamiltonian is $H = -\\sum_{\\langle ij \\rangle} s_i s_j$ with $s_i \\in \\{-1,+1\\}$ and the sum over nearest neighbors on the square lattice. Let $N=L^2$ be the number of spins and $N_b=2N$ be the number of bonds. For $L=4$ (so $N=16$), the possible total energies are the nine values $-32,-24,-16,-8,0,8,16,24,32$ (in units of $J$). In the canonical ensemble at inverse temperature $\\beta=1/T$, the probability of observing energy $E$ is $p_\\beta(E) \\propto g(E)\\,\\exp(-\\beta E)$, where $g(E)$ is the density of states (degeneracy). The specific heat per spin is defined by $C(T)=\\beta^2\\,\\mathrm{Var}_\\beta(E)/N$.\n\nYou are given binned Monte Carlo energy histograms for the $L=4$ system at three temperatures: one below the critical temperature, one near it, and one above it. Each histogram reports counts for the same nine energy bins listed above, in that order. The three temperatures and their histograms (each summing to $100000$ samples) are:\n- Temperature $T_1 = 2.0$, counts (aligned with energies in order $-32$ to $32$): $[36000, 42000, 18000, 3000, 800, 100, 60, 30, 10]$.\n- Temperature $T_2 = 2.269$, counts: $[6000, 14000, 24000, 23000, 17000, 9000, 4000, 2000, 1000]$.\n- Temperature $T_3 = 3.0$, counts: $[1000, 4000, 9000, 16000, 21000, 20000, 16000, 9000, 4000]$.\n\nStarting from first principles of the canonical ensemble and without assuming any prior knowledge of the density of states $g(E)$, use histogram reweighting to estimate $g(E)$ from the three histograms together and then compute the specific heat per spin $C(T)$ at a set of target temperatures. Your method must rely only on the provided histograms and must not use any external data or further simulations. You must:\n- Justify and implement a statistically sound combination of multiple histograms collected at different temperatures to infer $g(E)$ up to a multiplicative constant, by enforcing consistency with the canonical probabilities.\n- From the inferred $g(E)$, compute the partition function at any target temperature $T$ as a discrete sum over the nine energies, and evaluate the energy mean and variance to obtain $C(T)$.\n\nAssume that all histograms are drawn from independent samples (you may ignore autocorrelation corrections). All calculations are in units where $k_B=1$ and $J=1$, and the required output is the specific heat per spin, which is dimensionless in these units.\n\nTest suite: compute $C(T)$ for the following target temperatures (in units of $J$): $[1.8, 2.0, 2.269, 2.5, 3.0, 3.2]$. This set includes a temperature slightly below the lowest provided histogram, two temperatures coinciding with provided histograms, one near the critical temperature, and two above the highest provided histogram.\n\nYour program must produce a single line of output containing the six results as a comma-separated list enclosed in square brackets, with each value rounded to six decimal places (for example, \"[0.123456,0.234567,0.345678,0.456789,0.567890,0.678901]\"). All outputs are specific heats per spin in units with $k_B=1$.", "solution": "The problem presented is scientifically sound, self-contained, and well-posed. It requires the application of the multiple histogram reweighting method, a standard and powerful technique in computational statistical physics, to analyze simulated data for the two-dimensional Ising model. We will therefore proceed with a rigorous solution.\n\nThe basis for this analysis is the canonical ensemble of statistical mechanics. For a system at a fixed inverse temperature $\\beta = 1/T$ (with Boltzmann constant $k_B=1$), the probability $p_\\beta(E)$ of observing a microstate with energy $E$ is given by:\n$$\np_\\beta(E) = \\frac{g(E) e^{-\\beta E}}{Z(\\beta)}\n$$\nHere, $g(E)$ is the density of states (or degeneracy), which is the number of distinct microstates corresponding to the energy $E$. The partition function, $Z(\\beta)$, serves as the normalization constant and is defined by the sum over all possible energy levels $E_i$:\n$$\nZ(\\beta) = \\sum_{i} g(E_i) e^{-\\beta E_i}\n$$\nThe problem provides data from $R=3$ independent Monte Carlo simulations, performed at inverse temperatures $\\beta_j=1/T_j$ for $j \\in \\{1, 2, 3\\}$. For each simulation $j$, we are given a histogram of energies, which contains the counts $n_{ij}$ of observing the energy $E_i$ out of a total of $M_j$ samples. The empirical probability is thus $n_{ij}/M_j$. This provides an estimate for the true canonical probability:\n$$\n\\frac{n_{ij}}{M_j} \\approx p_{\\beta_j}(E_i) = \\frac{g(E_i) e^{-\\beta_j E_i}}{Z(\\beta_j)}\n$$\nFrom this relation, one can derive an estimate for the density of states $g(E_i)$ using data from a single simulation $j$:\n$$\ng(E_i) \\propto \\frac{n_{ij}}{M_j} e^{\\beta_j E_i}\n$$\nThe proportionality constant involves the unknown partition function $Z(\\beta_j)$. A more robust estimate for $g(E_i)$ can be obtained by combining the data from all $R$ simulations. The statistically optimal way to do this is via the multiple histogram method, also known as the Weighted Histogram Analysis Method (WHAM). This method yields an improved estimate for $g(E_i)$ that is consistent with all datasets simultaneously. The combined estimate for the density of states is given by:\n$$\ng(E_i) = \\frac{\\sum_{j=1}^{R} n_{ij}}{\\sum_{j=1}^{R} M_j e^{f_j - \\beta_j E_i}}\n$$\nwhere the parameters $f_j$ are related to the dimensionless free energies of the simulations, $F_j = -T_j \\ln Z(\\beta_j)$, such that $Z(\\beta_j) \\propto e^{-f_j}$. These parameters must be determined self-consistently. Substituting the expression for $g(E_i)$ back into the definition of the partition function $Z(\\beta_k) \\propto e^{-f_k}$ gives the set of self-consistency equations:\n$$\ne^{-f_k} = \\sum_{i} g(E_i) e^{-\\beta_k E_i} = \\sum_{i} \\frac{\\sum_{j=1}^{R} n_{ij}}{\\sum_{j=1}^{R} M_j e^{f_j - \\beta_j E_i}} e^{-\\beta_k E_i} \\quad \\text{for } k=1, \\dots, R\n$$\nThese $R$ coupled nonlinear equations for the $R$ unknowns $\\{f_1, \\dots, f_R\\}$ can be solved iteratively. It is important to note that if $\\{f_j\\}$ is a solution, then $\\{f_j + C\\}$ is also a solution for any constant $C$. This gauge freedom is fixed by setting one of the parameters to a constant, for example, $f_1=0$. To ensure numerical stability, the iterative solution is best implemented using logarithms, particularly the `log-sum-exp` technique to handle sums of exponentials.\n\nThe iterative procedure is as follows:\n1. Initialize the parameters, for instance, $f_j = 0$ for all $j=1, \\dots, R$.\n2. In each iteration, compute an updated estimate for the logarithm of the density of states, $\\ln g(E_i)$, for all energy levels $i$:\n   $$\n   \\ln g(E_i) = \\ln\\left(\\sum_{j=1}^{R} n_{ij}\\right) - \\ln\\left(\\sum_{j=1}^{R} M_j e^{f_j - \\beta_j E_i}\\right)\n   $$\n3. Use the new $\\ln g(E_i)$ to re-calculate the free energy parameters:\n   $$\n   f_k^{\\text{new}} = -\\ln\\left(\\sum_i e^{\\ln g(E_i) - \\beta_k E_i}\\right)\n   $$\n4. Normalize the new set $\\{f_k^{\\text{new}}\\}$, for instance by shifting all values so that the first element is zero: $f_k \\leftarrow f_k^{\\text{new}} - f_1^{\\text{new}}$.\n5. Repeat steps 2-4 until the values of $\\{f_j\\}$ converge to within a desired tolerance.\n\nOnce the converged values of $f_j$ are obtained, we have the final estimate for $\\ln g(E_i)$ (up to an irrelevant additive constant). This allows for the calculation of any thermodynamic observable at any target inverse temperature $\\beta_{\\text{target}}$. The expectation value of a quantity $A(E)$ is:\n$$\n\\langle A \\rangle_{\\beta_{\\text{target}}} = \\frac{\\sum_i A(E_i) g(E_i) e^{-\\beta_{\\text{target}} E_i}}{\\sum_i g(E_i) e^{-\\beta_{\\text{target}} E_i}} = \\frac{\\sum_i A(E_i) e^{\\ln g(E_i) - \\beta_{\\text{target}} E_i}}{\\sum_i e^{\\ln g(E_i) - \\beta_{\\text{target}} E_i}}\n$$\nThe problem requires the computation of the specific heat per spin, $C(T)$, defined as:\n$$\nC(T) = \\frac{\\beta^2}{N} \\text{Var}_\\beta(E) = \\frac{\\beta^2}{N} \\left(\\langle E^2 \\rangle_\\beta - \\langle E \\rangle_\\beta^2\\right)\n$$\nwhere $N=16$ is the number of spins. We will compute the expectation values $\\langle E \\rangle$ and $\\langle E^2 \\rangle$ using the reweighted density of states for each of the specified target temperatures and subsequently determine $C(T)$. The provided data consists of histograms for $T \\in \\{2.0, 2.269, 3.0\\}$ on a lattice with $L=4$, and the energy levels $E_i \\in \\{-32, -24, -16, -8, 0, 8, 16, 24, 32\\}$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import logsumexp\n\ndef solve():\n    \"\"\"\n    Solves the problem of calculating specific heat using the multiple histogram \n    reweighting method for the 2D Ising model.\n    \"\"\"\n    # Define problem parameters and givens\n    L = 4\n    N_spins = L * L\n    # The nine possible energy levels for the L=4 system\n    energies = np.array([-32, -24, -16, -8, 0, 8, 16, 24, 32], dtype=float)\n\n    # Simulation temperatures and their corresponding inverse temperatures (beta)\n    sim_temps = np.array([2.0, 2.269, 3.0])\n    sim_betas = 1.0 / sim_temps\n\n    # Histogram counts from the three simulations\n    counts = np.array([\n        [36000, 42000, 18000, 3000, 800, 100, 60, 30, 10],      # T=2.0\n        [6000, 14000, 24000, 23000, 17000, 9000, 4000, 2000, 1000], # T=2.269\n        [1000, 4000, 9000, 16000, 21000, 20000, 16000, 9000, 4000]  # T=3.0\n    ], dtype=float)\n\n    num_samples_per_sim = 100000.0\n    R, K = counts.shape  # R = number of simulations, K = number of energy bins\n\n    # Total counts for each energy level, summed across all simulations\n    total_counts_per_energy = np.sum(counts, axis=0)\n    \n    # --- Multiple Histogram Reweighting (WHAM) ---\n    # Iteratively solve for the free-energy-related parameters f_j.\n    # The parameters f are related to the log of the partition functions.\n    f = np.zeros(R)\n\n    # Precompute terms for efficiency\n    beta_E_matrix = np.outer(sim_betas, energies)  # Shape (R, K)\n    log_num_samples = np.log(num_samples_per_sim)\n\n    max_iter = 1000\n    tolerance = 1e-12\n\n    for _ in range(max_iter):\n        f_old = f.copy()\n\n        # Step 1: Calculate the log of the density of states, g(E), using the current f_j.\n        # log g(E_i) = log(sum_j n_ij) - log(sum_j M_j exp(f_j - beta_j E_i))\n        f_broadcast = np.tile(f, (K, 1)).T\n        log_denominators = logsumexp(log_num_samples + f_broadcast - beta_E_matrix, axis=0)\n        log_g = np.log(total_counts_per_energy) - log_denominators\n\n        # Step 2: Update f_k using the self-consistency equation.\n        # exp(-f_k) = sum_i g(E_i) exp(-beta_k E_i)\n        # We work with logs: f_k = -log(sum_i exp(log g_i - beta_k E_i))\n        log_g_broadcast = np.tile(log_g, (R, 1))\n        f_new_unnormalized = -logsumexp(log_g_broadcast - beta_E_matrix, axis=1)\n\n        # Normalize the new f_k to fix the gauge freedom (set f_1 = 0)\n        f = f_new_unnormalized - f_new_unnormalized[0]\n\n        # Check for convergence\n        if np.max(np.abs(f - f_old)) < tolerance:\n            break\n            \n    # The last computed log_g is the final estimate from the converged f_j.\n\n    # --- Calculation of Specific Heat at Target Temperatures ---\n    target_temps = np.array([1.8, 2.0, 2.269, 2.5, 3.0, 3.2])\n    target_betas = 1.0 / target_temps\n\n    results = []\n    for beta in target_betas:\n        # Calculate log of terms in the partition function sum: log(g(E_i)) - beta * E_i\n        w = log_g - beta * energies\n\n        # Use logsumexp trick for numerical stability to compute expectation values\n        w_max = np.max(w)\n        exp_w_shifted = np.exp(w - w_max)\n        \n        # Denominator for expectation values is sum_i exp(w_i - w_max)\n        denominator = np.sum(exp_w_shifted)\n        \n        # Calculate <E>\n        mean_E_numerator = np.sum(energies * exp_w_shifted)\n        mean_E = mean_E_numerator / denominator\n        \n        # Calculate <E^2>\n        mean_E2_numerator = np.sum(energies**2 * exp_w_shifted)\n        mean_E2 = mean_E2_numerator / denominator\n        \n        # Variance of energy\n        var_E = mean_E2 - mean_E**2\n        \n        # Specific heat per spin: C = beta^2 * Var(E) / N\n        C_per_spin = (beta**2 * var_E) / N_spins\n        results.append(C_per_spin)\n\n    # Print the final results in the specified format\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```", "id": "2401585"}, {"introduction": "Having implemented the reweighting algorithm in a specific case, we now turn to its theoretical underpinnings to appreciate its full generality. This practice is a theoretical derivation that asks you to construct the optimal estimator for an observable by combining data from entirely different statistical ensemblesâ€”the canonical ($NVT$) and grand canonical ($\\mu VT$) ensembles [@problem_id:2401647]. Working from first principles, you will derive the generalized formula that forms the basis of modern methods like the Multistate Bennett Acceptance Ratio (MBAR), revealing how reweighting serves as a universal framework for integrating information from disparate simulations.", "problem": "Consider a classical many-particle system of identical particles in a fixed volume $V$, described by potential energy $U(\\mathbf{x})$ for configuration $\\mathbf{x}$. Two independent simulations are performed:\n\n1. Simulation $A$ is in the canonical ensemble (constant number, volume, temperature; $NVT$) at inverse temperature $\\beta_{A}$ with a fixed particle number $N_{A}$. It produces $M_{A}$ independent samples $\\{\\mathbf{x}_{n}\\}_{n=1}^{M_{A}}$.\n2. Simulation $B$ is in the grand canonical ensemble (constant chemical potential, volume, temperature; $\\mu VT$) at inverse temperature $\\beta_{B}$ and chemical potential $\\mu_{B}$. It produces $M_{B}$ independent samples $\\{(N_{m},\\mathbf{x}_{m})\\}_{m=1}^{M_{B}}$ of particle number $N$ and configuration $\\mathbf{x}$.\n\nDefine the unnormalized probability densities\n- for simulation $A$ (viewed on the joint state space $(N,\\mathbf{x})$) by $q_{A}(N,\\mathbf{x}) \\equiv \\frac{1}{N!}\\,\\exp\\!\\big(-\\beta_{A} U(\\mathbf{x})\\big)\\,\\delta_{N,N_{A}}$,\n- for simulation $B$ by $q_{B}(N,\\mathbf{x}) \\equiv \\frac{z_{B}^{N}}{N!}\\,\\exp\\!\\big(-\\beta_{B} U(\\mathbf{x})\\big)$ with activity $z_{B} \\equiv \\exp(\\beta_{B}\\mu_{B})/\\Lambda_{B}^{3}$, where $\\Lambda_{B}$ is the thermal de Broglie wavelength at $\\beta_{B}$,\n- and for a target grand canonical state $(\\beta_{*},\\mu_{*},V)$ by $q_{*}(N,\\mathbf{x}) \\equiv \\frac{z_{*}^{N}}{N!}\\,\\exp\\!\\big(-\\beta_{*} U(\\mathbf{x})\\big)$ with $z_{*} \\equiv \\exp(\\beta_{*}\\mu_{*})/\\Lambda_{*}^{3}$.\n\nLet the corresponding normalization constants (partition functions) be $Z_{A} \\equiv \\sum_{N}\\int q_{A}(N,\\mathbf{x})\\,\\mathrm{d}\\mathbf{x}$ and $\\Xi_{B} \\equiv \\sum_{N}\\int q_{B}(N,\\mathbf{x})\\,\\mathrm{d}\\mathbf{x}$, and define reduced free energies $f_{A} \\equiv -\\ln Z_{A}$ and $f_{B} \\equiv -\\ln \\Xi_{B}$.\n\nUsing only first principles definitions of ensemble averages and probability densities, derive a closed-form analytic expression that combines all samples from simulations $A$ and $B$ to estimate the expectation value $\\langle A \\rangle_{*}$ of an arbitrary observable $A(N,\\mathbf{x})$ in the target grand canonical state $(\\beta_{*},\\mu_{*},V)$. Your final expression must be written explicitly in terms of the sample set $\\{(N_{n},\\mathbf{x}_{n})\\}_{n=1}^{M}$ with $M \\equiv M_{A}+M_{B}$ (where for simulation $A$ samples one has $N_{n}=N_{A}$), the unnormalized densities $q_{A}$, $q_{B}$, $q_{*}$, the counts $M_{A}$ and $M_{B}$, and the reduced free energies $f_{A}$ and $f_{B}$. Do not assume any particular functional form for $A(N,\\mathbf{x})$ beyond measurability. Express your final answer as a single closed-form analytic expression. No numerical evaluation or rounding is required, and no units are needed.", "solution": "The problem requires the derivation of a closed-form expression for the expectation value $\\langle A \\rangle_{*}$ of an observable $A(N,\\mathbf{x})$ in a target grand canonical ensemble. This estimator must optimally combine samples from two independent simulations, one canonical ($A$) and one grand canonical ($B$). The derivation will proceed from the fundamental definition of an ensemble average and the principles of importance sampling.\n\nThe expectation value of the observable $A(N,\\mathbf{x})$ in the target state '$*$', characterized by the unnormalized probability density $q_{*}(N,\\mathbf{x})$, is defined as the ratio of two integrals over the entire state space of particle number $N$ and configuration $\\mathbf{x}$. We denote the composite integration and summation measure as $\\mathrm{d}\\mu(N,\\mathbf{x}) \\equiv \\sum_{N=0}^{\\infty} \\int \\mathrm{d}\\mathbf{x}$. The expectation value is given by:\n$$\n\\langle A \\rangle_{*} = \\frac{\\int A(N,\\mathbf{x}) q_{*}(N,\\mathbf{x}) \\, \\mathrm{d}\\mu(N,\\mathbf{x})}{\\int q_{*}(N,\\mathbf{x}) \\, \\mathrm{d}\\mu(N,\\mathbf{x})}\n$$\nThe denominator is the partition function of the target state, $\\Xi_{*} = \\int q_{*}(N,\\mathbf{x}) \\, \\mathrm{d}\\mu(N,\\mathbf{x})$.\n\nThe available data consists of a set of $M_{A}$ samples $\\{\\mathbf{x}_{n}\\}_{n=1}^{M_A}$ drawn from the canonical probability distribution $p_{A}(N,\\mathbf{x}) = Z_{A}^{-1} q_{A}(N,\\mathbf{x})$, and $M_{B}$ samples $\\{(N_{m},\\mathbf{x}_{m})\\}_{m=1}^{M_B}$ drawn from the grand canonical distribution $p_{B}(N,\\mathbf{x}) = \\Xi_{B}^{-1} q_{B}(N,\\mathbf{x})$. Here, $Z_{A} = \\exp(-f_{A})$ and $\\Xi_{B} = \\exp(-f_{B})$ are the normalization constants (partition functions) for simulations $A$ and $B$, respectively. For samples from simulation $A$, the particle number is fixed at $N_{A}$. We combine these into a single set of $M = M_{A} + M_{B}$ samples, denoted $\\{(N_n, \\mathbf{x}_n)\\}_{n=1}^{M}$.\n\nTo estimate $\\langle A \\rangle_{*}$, we can employ importance sampling. The most effective approach is to consider all samples as being drawn from an optimal mixture distribution that best represents the total sampling effort. The probability of observing a given state $(N,\\mathbf{x})$ is proportional to the sum of probabilities of observing it in each simulation, weighted by the number of samples from that simulation. This defines an effective, unnormalized sampling density:\n$$\n\\rho_{\\text{eff}}(N,\\mathbf{x}) \\propto M_{A} p_{A}(N,\\mathbf{x}) + M_{B} p_{B}(N,\\mathbf{x})\n$$\nSubstituting the expressions for $p_{A}$ and $p_{B}$, we have:\n$$\n\\rho_{\\text{eff}}(N,\\mathbf{x}) \\propto M_{A} Z_{A}^{-1} q_{A}(N,\\mathbf{x}) + M_{B} \\Xi_{B}^{-1} q_{B}(N,\\mathbf{x})\n$$\nUsing the provided reduced free energies $f_{A} = -\\ln Z_{A}$ and $f_{B} = -\\ln \\Xi_{B}$, this becomes:\n$$\n\\rho_{\\text{eff}}(N,\\mathbf{x}) \\propto M_{A} \\exp(f_{A}) q_{A}(N,\\mathbf{x}) + M_{B} \\exp(f_{B}) q_{B}(N,\\mathbf{x})\n$$\n\nWe can now rewrite the expression for $\\langle A \\rangle_{*}$ by multiplying and dividing the integrands by a weighting function proportional to this effective density. Let the weighting function be $W(N,\\mathbf{x}) = [M_{A} \\exp(f_{A}) q_{A}(N,\\mathbf{x}) + M_{B} \\exp(f_{B}) q_{B}(N,\\mathbf{x})]^{-1}$. The numerator of $\\langle A \\rangle_{*}$ can be written as an expectation value with respect to the mixture distribution:\n$$\n\\int A(N,\\mathbf{x}) q_{*}(N,\\mathbf{x}) \\, \\mathrm{d}\\mu(N,\\mathbf{x}) = \\int \\frac{A(N,\\mathbf{x}) q_{*}(N,\\mathbf{x})}{W(N,\\mathbf{x})^{-1}} \\, W(N,\\mathbf{x})^{-1} \\, \\mathrm{d}\\mu(N,\\mathbf{x})\n$$\nThe integral on the right is an expectation of the function $A(N,\\mathbf{x}) q_{*}(N, \\mathbf{x}) W(N, \\mathbf{x})$ with respect to a distribution proportional to $W(N, \\mathbf{x})^{-1} = \\rho_{\\text{eff}}$. Such an expectation can be estimated by a sample average over all $M$ collected data points. The same treatment applies to the denominator of $\\langle A \\rangle_{*}$.\n\nTherefore, the estimator for $\\langle A \\rangle_{*}$ is the ratio of two sample averages:\n$$\n\\langle A \\rangle_{*} \\approx \\frac{\\sum_{n=1}^{M} A(N_{n}, \\mathbf{x}_{n}) q_{*}(N_{n}, \\mathbf{x}_{n}) W(N_{n}, \\mathbf{x}_{n})}{\\sum_{n=1}^{M} q_{*}(N_{n}, \\mathbf{x}_{n}) W(N_{n}, \\mathbf{x}_{n})}\n$$\nSubstituting the expression for the weight function $W(N_{n}, \\mathbf{x}_{n})$ yields:\n$$\n\\langle A \\rangle_{*} \\approx \\frac{\\sum_{n=1}^{M} \\frac{A(N_{n}, \\mathbf{x}_{n}) q_{*}(N_{n}, \\mathbf{x}_{n})}{M_{A} \\exp(f_{A}) q_{A}(N_{n}, \\mathbf{x}_{n}) + M_{B} \\exp(f_{B}) q_{B}(N_{n}, \\mathbf{x}_{n})}}{\\sum_{n=1}^{M} \\frac{q_{*}(N_{n}, \\mathbf{x}_{n})}{M_{A} \\exp(f_{A}) q_{A}(N_{n}, \\mathbf{x}_{n}) + M_{B} \\exp(f_{B}) q_{B}(N_{n}, \\mathbf{x}_{n})}}\n$$\nThis is the final closed-form expression. For each sample $(N_{n}, \\mathbf{x}_{n})$ in the combined set, we evaluate the functions $q_{A}$, $q_{B}$, and $q_{*}$ and combine them according to this formula. For a sample originating from simulation $A$, $N_n=N_A$. For a sample from simulation $B$ with particle number $N_n \\neq N_A$, the term $q_A(N_n, \\mathbf{x}_n)$ will be zero due to the Kronecker delta $\\delta_{N,N_A}$ in its definition. The expression correctly handles all cases and optimally combines all available data as per the principles of maximum likelihood estimation, upon which this method, known as the Multistate Bennett Acceptance Ratio (MBAR) or a generalized form of the Weighted Histogram Analysis Method (WHAM), is founded. The question does not require finding the values of $f_A$ and $f_B$, but to express the final answer in terms of them, which has been done.", "answer": "$$\n\\boxed{\\frac{\\sum_{n=1}^{M_{A}+M_{B}} \\frac{A(N_{n}, \\mathbf{x}_{n}) q_{*}(N_{n}, \\mathbf{x}_{n})}{M_{A} \\exp(f_{A}) q_{A}(N_{n}, \\mathbf{x}_{n}) + M_{B} \\exp(f_{B}) q_{B}(N_{n}, \\mathbf{x}_{n})}}{\\sum_{n=1}^{M_{A}+M_{B}} \\frac{q_{*}(N_{n}, \\mathbf{x}_{n})}{M_{A} \\exp(f_{A}) q_{A}(N_{n}, \\mathbf{x}_{n}) + M_{B} \\exp(f_{B}) q_{B}(N_{n}, \\mathbf{x}_{n})}}}\n$$", "id": "2401647"}, {"introduction": "To conclude our hands-on practices, we tackle a comprehensive project that mirrors a realistic research workflow: reconstructing a two-dimensional free energy surface. This problem requires you to implement the WHAM algorithm to analyze umbrella sampling data where the reaction coordinates are non-orthogonal, a common complexity in molecular systems [@problem_id:2401621]. You will manage the entire process from generating synthetic data to implementing the 2D WHAM iteration and validating your computed free energy surface against an exact analytical solution, consolidating your skills in a challenging and practical application.", "problem": "You are asked to implement a complete, reproducible program that constructs a two-dimensional free energy surface using the Weighted Histogram Analysis Method (WHAM) when the reaction coordinates are not orthogonal. Your implementation must be based on first principles from statistical mechanics and probability, and you must not rely on any specialized WHAM library. The program must generate synthetic umbrella-sampled data from a known unbiased system, combine histograms using statistically consistent weights, and validate the result against an analytical reference. All energies must be expressed in units of $k_\\mathrm{B} T$ and are therefore dimensionless. Angles do not appear. The final output must be a single line containing a list of floating-point numbers as specified below.\n\nThe fundamental basis for your derivation and implementation must be:\n- The Boltzmann distribution $p(\\mathbf{z}) \\propto \\exp\\{-\\beta U(\\mathbf{z})\\}$, where $\\beta = 1/(k_\\mathrm{B} T)$ and $\\mathbf{z} = (x,y)^\\top$ denotes the microscopic coordinates.\n- The principle of umbrella sampling with added bias potentials, which shifts sampling to improve coverage of regions of interest.\n- Discrete histogram reweighting founded on statistical consistency and maximum-likelihood arguments.\n\nYou must construct a model in which the unbiased potential energy is a two-dimensional quadratic form with a coupling term,\n$$\nU_0(\\mathbf{z}) = \\tfrac{1}{2}\\,\\mathbf{z}^\\top \\mathbf{K}\\,\\mathbf{z}, \\quad \\mathbf{K} = \\begin{pmatrix} k_x & k_{xy} \\\\ k_{xy} & k_y \\end{pmatrix},\n$$\nand the reaction coordinates are linear and non-orthogonal,\n$$\n\\mathbf{s}(\\mathbf{z}) = \\begin{pmatrix} s_1 \\\\ s_2 \\end{pmatrix} = \\mathbf{A}\\,\\mathbf{z}, \\quad \\mathbf{A} = \\begin{pmatrix} 1 & 0 \\\\ 1 & \\alpha \\end{pmatrix},\n$$\nso that $s_1 = x$ and $s_2 = x + \\alpha y$, with $\\alpha \\neq 0$. For each umbrella window $i$, the bias is harmonic in both reaction coordinates,\n$$\nU^{(i)}_\\mathrm{b}(\\mathbf{s}) = \\tfrac{1}{2}\\,k_1\\,\\big(s_1 - c^{(i)}_1\\big)^2 + \\tfrac{1}{2}\\,k_2\\,\\big(s_2 - c^{(i)}_2\\big)^2.\n$$\nThe total biased energy in window $i$ is $U_0(\\mathbf{z}) + U^{(i)}_\\mathrm{b}(\\mathbf{s}(\\mathbf{z}))$. Because both $U_0$ and $U^{(i)}_\\mathrm{b}$ are quadratic and $\\mathbf{s}$ is linear, the biased distribution in $\\mathbf{z}$ is Gaussian and may be sampled exactly.\n\nYour program must:\n- For each specified test case, generate independent samples in each window $i$ by sampling from the Gaussian distribution proportional to $\\exp\\{-\\beta[U_0(\\mathbf{z}) + U^{(i)}_\\mathrm{b}(\\mathbf{s}(\\mathbf{z}))]\\}$ with exactly the given random seed. You must derive the associated Gaussian mean and covariance from first principles; do not use black-box samplers other than a multivariate normal with your derived parameters.\n- Discretize the $\\mathbf{s}$-space on a uniform grid with the provided bounds and number of bins, build a separate two-dimensional histogram for each window in $\\mathbf{s}$, and then combine the histograms using a statistically consistent WHAM fixed-point iteration to estimate the unbiased probability $P(\\mathbf{s})$ on the grid.\n- Convert $P(\\mathbf{s})$ to a free energy $F(\\mathbf{s}) = -\\ln P(\\mathbf{s})$ (in units of $k_\\mathrm{B} T$), and shift $F(\\mathbf{s})$ so that its minimum over the visited grid bins is zero.\n- Compute an analytical reference free energy $F_\\mathrm{ref}(\\mathbf{s})$ based on the exact Gaussian transformation of the unbiased distribution under $\\mathbf{s} = \\mathbf{A}\\mathbf{z}$, and shift it so that its minimum over the same visited bins is zero.\n- Calculate the root-mean-square error (RMSE) between $F(\\mathbf{s})$ and $F_\\mathrm{ref}(\\mathbf{s})$ over those grid bins whose total count across all windows is at least a specified threshold (use threshold count $\\geq 5$). The RMSE is a single floating-point number per test case.\n\nTest suite. Implement the above for the following three cases. In all cases, use $\\beta = 1$ (i.e., $k_\\mathrm{B} T$ as energy unit) and the same unbiased stiffness matrix,\n- Unbiased stiffness: $\\mathbf{K} = \\begin{pmatrix} 1.5 & 0.3 \\\\ 0.3 & 1.0 \\end{pmatrix}$.\n\nCase $1$ (happy path):\n- Reaction coordinate non-orthogonality: $\\alpha = 0.5$.\n- Umbrella spring constants: $k_1 = 4.0$, $k_2 = 2.0$.\n- Umbrella centers: $\\{c^{(i)}_1\\} \\in \\{-1.2, 0.0, 1.2\\}$ and $\\{c^{(i)}_2\\} \\in \\{-1.2, 0.0, 1.2\\}$, forming a $3 \\times 3$ grid of windows.\n- Samples per window: $N_i = 4000$ for all $i$.\n- Histogram grid in $\\mathbf{s}$: $s_1 \\in [-2.0, 2.0]$ with $41$ bins; $s_2 \\in [-2.0, 2.0]$ with $41$ bins.\n- Random seed: $123$.\n\nCase $2$ (stronger non-orthogonality and stiffer umbrellas):\n- Reaction coordinate non-orthogonality: $\\alpha = 0.9$.\n- Umbrella spring constants: $k_1 = 6.0$, $k_2 = 6.0$.\n- Umbrella centers: $\\{c^{(i)}_1\\} \\in \\{-1.8, 0.0, 1.8\\}$ and $\\{c^{(i)}_2\\} \\in \\{-1.8, 0.0, 1.8\\}$, forming a $3 \\times 3$ grid.\n- Samples per window: $N_i = 1500$ for all $i$.\n- Histogram grid in $\\mathbf{s}$: $s_1 \\in [-3.0, 3.0]$ with $45$ bins; $s_2 \\in [-3.0, 3.0]$ with $45$ bins.\n- Random seed: $456$.\n\nCase $3$ (edge case with fewer windows and fewer samples):\n- Reaction coordinate non-orthogonality: $\\alpha = 0.2$.\n- Umbrella spring constants: $k_1 = 3.0$, $k_2 = 3.0$.\n- Umbrella centers: $\\{c^{(i)}_1\\} \\in \\{-1.2, 1.2\\}$ and $\\{c^{(i)}_2\\} \\in \\{-1.2, 1.2\\}$, forming a $2 \\times 2$ grid.\n- Samples per window: $N_i = 600$ for all $i$.\n- Histogram grid in $\\mathbf{s}$: $s_1 \\in [-2.0, 2.0]$ with $35$ bins; $s_2 \\in [-2.0, 2.0]$ with $35$ bins.\n- Random seed: $789$.\n\nAlgorithmic details you must honor:\n- For sampling in each window $i$, derive the Gaussian mean and covariance of the biased distribution in $\\mathbf{z}$ of the form $\\mathcal{N}(\\boldsymbol{\\mu}^{(i)}, \\boldsymbol{\\Sigma}^{(i)})$ implied by $U_0(\\mathbf{z}) + U^{(i)}_\\mathrm{b}(\\mathbf{s}(\\mathbf{z}))$, and draw exactly $N_i$ samples using the stated seed.\n- Build separate two-dimensional histograms in $(s_1,s_2)$ for each window using the exact bin edges described above.\n- Implement a WHAM fixed-point iteration on the discrete grid that solves for a set of offsets $\\{f_i\\}$ and the unbiased probability $P(\\mathbf{s})$ on the grid, using stable numerical procedures. Normalize $P(\\mathbf{s})$ to sum to $1$ over all visited bins.\n- Construct $F(\\mathbf{s}) = -\\ln P(\\mathbf{s})$ and shift it so that $\\min F = 0$ over the visited bins.\n- Construct the analytical reference $F_\\mathrm{ref}(\\mathbf{s})$ implied by the unbiased Gaussian and the linear transformation $\\mathbf{s} = \\mathbf{A}\\mathbf{z}$, shifted so that $\\min F_\\mathrm{ref} = 0$ over the same visited bins.\n- Compute the RMSE between $F(\\mathbf{s})$ and $F_\\mathrm{ref}(\\mathbf{s})$ over bins whose total count across all windows is at least $5$.\n\nFinal output format:\n- Your program must produce a single line of output containing the three RMSE values for cases $1$, $2$, and $3$, respectively, rounded to six decimals, as a comma-separated list enclosed in square brackets, for example, $[0.012345,0.067890,0.123456]$.\n\nExecution constraints:\n- The program must be self-contained, require no user input, and read no external files.\n- Use the programming language and library constraints specified separately.", "solution": "The problem requires the implementation of the Weighted Histogram Analysis Method (WHAM) to reconstruct a two-dimensional free energy surface from synthetically generated umbrella sampling data. The reaction coordinates are specified as non-orthogonal linear combinations of the underlying microscopic coordinates. The solution must be derived from first principles of statistical mechanics, implemented without reliance on specialized external libraries for WHAM, and validated against an exact analytical result. All energies are dimensionless, expressed in units of $k_\\mathrm{B} T$, which is equivalent to setting $\\beta = (k_\\mathrm{B} T)^{-1} = 1$.\n\n### 1. Theoretical Framework\n\n**1.1. System Definition**\nThe unbiased potential energy of the system is a quadratic form in the Cartesian coordinates $\\mathbf{z} = (x, y)^\\top$:\n$$\nU_0(\\mathbf{z}) = \\frac{1}{2}\\mathbf{z}^\\top \\mathbf{K}\\,\\mathbf{z}\n$$\nwhere $\\mathbf{K}$ is a symmetric positive-definite stiffness matrix. The corresponding unbiased probability distribution is a multivariate normal (Gaussian) distribution with zero mean and covariance matrix $\\mathbf{\\Sigma_z} = \\mathbf{K}^{-1}$:\n$$\np_0(\\mathbf{z}) \\propto \\exp(-U_0(\\mathbf{z})) \\implies \\mathbf{z} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{K}^{-1})\n$$\n\nThe reaction coordinates $\\mathbf{s} = (s_1, s_2)^\\top$ are a linear transformation of $\\mathbf{z}$:\n$$\n\\mathbf{s}(\\mathbf{z}) = \\mathbf{A}\\,\\mathbf{z}\n$$\nwhere $\\mathbf{A}$ is an invertible matrix, signifying that the reaction coordinates are not orthogonal in general.\n\nUmbrella sampling is performed in a set of $M$ simulation windows, indexed by $i = 1, \\dots, M$. In each window $i$, a harmonic bias potential $U^{(i)}_\\mathrm{b}(\\mathbf{s})$ is added to restrain the system near a center $\\mathbf{c}^{(i)} = (c^{(i)}_1, c^{(i)}_2)^\\top$ in reaction coordinate space:\n$$\nU^{(i)}_\\mathrm{b}(\\mathbf{s}) = \\frac{1}{2} (\\mathbf{s} - \\mathbf{c}^{(i)})^\\top \\mathbf{K}_\\mathrm{b} (\\mathbf{s} - \\mathbf{c}^{(i)})\n$$\nwhere $\\mathbf{K}_\\mathrm{b} = \\mathrm{diag}(k_1, k_2)$ is the diagonal matrix of harmonic force constants.\n\n**1.2. Biased Distribution for Data Generation**\nThe total potential energy in window $i$ is $U^{(i)}(\\mathbf{z}) = U_0(\\mathbf{z}) + U^{(i)}_\\mathrm{b}(\\mathbf{s}(\\mathbf{z}))$. To generate samples, we must determine the parameters of the resulting probability distribution $p^{(i)}(\\mathbf{z}) \\propto \\exp(-U^{(i)}(\\mathbf{z}))$. Substituting the definitions, we have:\n$$\nU^{(i)}(\\mathbf{z}) = \\frac{1}{2}\\mathbf{z}^\\top \\mathbf{K}\\,\\mathbf{z} + \\frac{1}{2} (\\mathbf{A}\\mathbf{z} - \\mathbf{c}^{(i)})^\\top \\mathbf{K}_\\mathrm{b} (\\mathbf{A}\\mathbf{z} - \\mathbf{c}^{(i)})\n$$\nExpanding the second term gives:\n$$\nU^{(i)}_\\mathrm{b}(\\mathbf{s}(\\mathbf{z})) = \\frac{1}{2}\\mathbf{z}^\\top (\\mathbf{A}^\\top \\mathbf{K}_\\mathrm{b} \\mathbf{A}) \\mathbf{z} - \\mathbf{z}^\\top (\\mathbf{A}^\\top \\mathbf{K}_\\mathrm{b} \\mathbf{c}^{(i)}) + \\frac{1}{2}(\\mathbf{c}^{(i)})^\\top \\mathbf{K}_\\mathrm{b} \\mathbf{c}^{(i)}\n$$\nCombining terms, the total potential is:\n$$\nU^{(i)}(\\mathbf{z}) = \\frac{1}{2}\\mathbf{z}^\\top (\\mathbf{K} + \\mathbf{A}^\\top \\mathbf{K}_\\mathrm{b} \\mathbf{A}) \\mathbf{z} - \\mathbf{z}^\\top (\\mathbf{A}^\\top \\mathbf{K}_\\mathrm{b} \\mathbf{c}^{(i)}) + \\mathrm{const.}\n$$\nThis is the exponent of a Gaussian distribution. A general Gaussian density for $\\mathbf{z}$ with mean $\\boldsymbol{\\mu}$ and covariance $\\mathbf{\\Sigma}$ is proportional to $\\exp(-\\frac{1}{2}(\\mathbf{z}-\\boldsymbol{\\mu})^\\top\\mathbf{\\Sigma}^{-1}(\\mathbf{z}-\\boldsymbol{\\mu}))$, which expands to $\\exp(-\\frac{1}{2}\\mathbf{z}^\\top\\mathbf{\\Sigma}^{-1}\\mathbf{z} + \\mathbf{z}^\\top\\mathbf{\\Sigma}^{-1}\\boldsymbol{\\mu} + \\mathrm{const.})$. By comparing this form with $\\exp(-U^{(i)}(\\mathbf{z}))$, we identify the inverse covariance and the mean of the biased distribution in window $i$:\n$$\n\\mathbf{\\Sigma}^{(i)^{-1}} = \\mathbf{K} + \\mathbf{A}^\\top \\mathbf{K}_\\mathrm{b} \\mathbf{A}\n$$\n$$\n\\mathbf{\\Sigma}^{(i)^{-1}} \\boldsymbol{\\mu}^{(i)} = \\mathbf{A}^\\top \\mathbf{K}_\\mathrm{b} \\mathbf{c}^{(i)}\n$$\nSolving for the parameters yields the sampling distribution for $\\mathbf{z}$ in window $i$:\n$$\n\\mathbf{z} \\sim \\mathcal{N}(\\boldsymbol{\\mu}^{(i)}, \\mathbf{\\Sigma}^{(i)})\n$$\nwith covariance $\\mathbf{\\Sigma}^{(i)} = (\\mathbf{K} + \\mathbf{A}^\\top \\mathbf{K}_\\mathrm{b} \\mathbf{A})^{-1}$ and mean $\\boldsymbol{\\mu}^{(i)} = \\mathbf{\\Sigma}^{(i)} (\\mathbf{A}^\\top \\mathbf{K}_\\mathrm{b} \\mathbf{c}^{(i)})$.\n\n**1.3. Analytical Reference Free Energy**\nThe unbiased distribution of the reaction coordinates $\\mathbf{s}$ can be found from the properties of linear transformations of Gaussian variables. Since $\\mathbf{z} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{K}^{-1})$ and $\\mathbf{s} = \\mathbf{A}\\mathbf{z}$, the distribution of $\\mathbf{s}$ is also Gaussian, $\\mathbf{s} \\sim \\mathcal{N}(\\boldsymbol{\\mu_s}, \\mathbf{\\Sigma_s})$, with mean $\\boldsymbol{\\mu_s} = \\mathbf{A}\\boldsymbol{\\mu_z} = \\mathbf{0}$ and covariance $\\mathbf{\\Sigma_s} = \\mathbf{A} \\mathbf{\\Sigma_z} \\mathbf{A}^\\top = \\mathbf{A} \\mathbf{K}^{-1} \\mathbf{A}^\\top$.\n\nThe probability density is $p(\\mathbf{s}) \\propto \\exp(-\\frac{1}{2}\\mathbf{s}^\\top \\mathbf{\\Sigma_s}^{-1} \\mathbf{s})$. The analytical free energy is defined as $F(\\mathbf{s}) = -\\ln p(\\mathbf{s})$ (in units of $k_\\mathrm{B} T$). Thus, up to an irrelevant additive constant:\n$$\nF_\\mathrm{ref}(\\mathbf{s}) = \\frac{1}{2}\\mathbf{s}^\\top (\\mathbf{A} \\mathbf{K}^{-1} \\mathbf{A}^\\top)^{-1} \\mathbf{s}\n$$\nThis provides the theoretical ground truth against which the WHAM reconstruction is compared.\n\n**1.4. WHAM Implementation**\nData from all $M$ windows are combined using WHAM. The reaction coordinate space $\\mathbf{s}$ is discretized into a grid of bins, indexed by $j$. Let $H_{ij}$ be the number of samples from window $i$ falling into bin $j$, and $N_i = \\sum_j H_{ij}$ be the total samples from window $i$. The goal of WHAM is to find the unbiased probability $P_j$ for each bin $j$ and the dimensionless free energy offset $f_i$ for each window $i$. These are determined by the self-consistent equations:\n$$\nP_j = \\frac{\\sum_{i=1}^M H_{ij}}{\\sum_{i=1}^M N_i \\exp(f_i - U^{(i)}_\\mathrm{b}(\\mathbf{s}_j))}\n$$\n$$\n\\exp(-f_i) = \\sum_j P_j \\exp(-U^{(i)}_\\mathrm{b}(\\mathbf{s}_j))\n$$\nwhere $\\mathbf{s}_j$ is the coordinate of the center of bin $j$. These equations are solved iteratively. For numerical stability, the calculations are performed in logarithmic space.\n\nThe iterative procedure is as follows:\n1. Initialize $f_i = 0$ for all $i=1, \\dots, M$.\n2. Repeatedly update $\\{f_i\\}$ and $\\{P_j\\}$ until convergence:\n   a. For each visited bin $j$ (where $\\sum_i H_{ij} > 0$), calculate an unnormalized probability $P'_j$ using the current $\\{f_i\\}$:\n      $$\n      \\ln P'_j = \\ln\\left(\\sum_i H_{ij}\\right) - \\mathrm{logsumexp}_i(\\ln N_i + f_i - U^{(i)}_\\mathrm{b}(\\mathbf{s}_j))\n      $$\n   b. Normalize the probabilities over the set of all visited bins:\n      $$\n      \\ln P_j = \\ln P'_j - \\mathrm{logsumexp}_{k \\in \\text{visited}} (\\ln P'_k)\n      $$\n   c. Update the free energy offsets $\\{f_i\\}$ using the new probabilities $\\{P_j\\}$:\n      $$\n      f_i^{\\text{new}} = -\\mathrm{logsumexp}_{j \\in \\text{visited}} (\\ln P_j - U^{(i)}_\\mathrm{b}(\\mathbf{s}_j))\n      $$\n   d. To prevent numerical drift, anchor the free energies, e.g., $f_i \\leftarrow f_i^{\\text{new}} - f_1^{\\text{new}}$.\n   e. Check for convergence, for instance, by requiring the maximum absolute change in any $f_i$ to be below a small tolerance (e.g., $10^{-9}$).\n\n**1.5. Free Energy Surface and Error Calculation**\nOnce the converged probabilities $P_j$ are obtained, the free energy surface is computed as $F_j = -\\ln P_j$. This surface is defined only for visited bins; $F_j$ is infinite otherwise. For comparison, the surface is shifted so its minimum value over the visited bins is zero.\n\nThe analytical reference free energy $F_{\\mathrm{ref}, j}$ is evaluated at the bin centers $\\mathbf{s}_j$. It is then shifted by subtracting its minimum value calculated over the same set of visited bins. This ensures a consistent reference for the error calculation.\n\nThe Root-Mean-Square Error (RMSE) is calculated to quantify the accuracy of the reconstruction. The comparison is restricted to bins with a total sample count $\\sum_i H_{ij}$ of at least $5$, to exclude statistically unreliable regions. Let $\\mathcal{J}$ be the set of such bins. The RMSE is:\n$$\n\\text{RMSE} = \\sqrt{\\frac{1}{|\\mathcal{J}|} \\sum_{j \\in \\mathcal{J}} (F_j - F_{\\mathrm{ref}, j})^2}\n$$\nThe procedure is applied to each of the three test cases specified in the problem statement.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import logsumexp\n\ndef run_analysis(K_unbiased, alpha, k_bias, c1_centers, c2_centers, n_samples_per_window, s_lims, n_bins, seed):\n    \"\"\"\n    Performs a complete WHAM analysis for a single test case.\n    \"\"\"\n    # 1. Setup system parameters\n    rng = np.random.default_rng(seed)\n    A = np.array([[1, 0], [1, alpha]])\n    K_b = np.diag(k_bias)\n    \n    # Generate the grid of umbrella window centers\n    window_centers = np.array(np.meshgrid(c1_centers, c2_centers)).T.reshape(-1, 2)\n    n_windows = len(window_centers)\n    \n    # Pre-calculate matrices for sampling distribution\n    # These are constant across all windows for this problem\n    # Sigma_inv = K + A.T @ K_b @ A\n    Sigma_inv_biased = K_unbiased + A.T @ K_b @ A\n    Sigma_biased = np.linalg.inv(Sigma_inv_biased)\n    \n    # Setup histogram grid\n    s1_edges = np.linspace(s_lims[0][0], s_lims[0][1], n_bins[0] + 1)\n    s2_edges = np.linspace(s_lims[1][0], s_lims[1][1], n_bins[1] + 1)\n    s1_centers = (s1_edges[:-1] + s1_edges[1:]) / 2.0\n    s2_centers = (s2_edges[:-1] + s2_edges[1:]) / 2.0\n    \n    hist_shape = (n_bins[0], n_bins[1])\n    n_bins_total = n_bins[0] * n_bins[1]\n\n    # All histograms and related data will be stored flattened for easier processing\n    all_histograms = np.zeros((n_windows, n_bins_total))\n    \n    # 2. Generate samples and build histograms\n    for i in range(n_windows):\n        c_i = window_centers[i]\n        \n        # Derive mean of the biased distribution for window i\n        mu_biased = Sigma_biased @ A.T @ K_b @ c_i\n        \n        # Sample from the biased distribution in z-space\n        z_samples = rng.multivariate_normal(mu_biased, Sigma_biased, size=n_samples_per_window)\n        \n        # Transform samples to reaction coordinate space s\n        s_samples = z_samples @ A.T\n        \n        # Build 2D histogram for this window\n        hist, _, _ = np.histogram2d(s_samples[:, 0], s_samples[:, 1], bins=[s1_edges, s2_edges])\n        all_histograms[i, :] = hist.flatten()\n\n    # 3. WHAM Calculation\n    S1_grid, S2_grid = np.meshgrid(s1_centers, s2_centers, indexing='ij')\n    s_coords = np.vstack([S1_grid.ravel(), S2_grid.ravel()]).T\n\n    # Pre-calculate bias energies for all bins and all windows\n    U_bias_all = np.zeros((n_windows, n_bins_total))\n    for i in range(n_windows):\n        c_i = window_centers[i]\n        ds1 = s_coords[:, 0] - c_i[0]\n        ds2 = s_coords[:, 1] - c_i[1]\n        U_bias_all[i, :] = 0.5 * k_bias[0] * ds1**2 + 0.5 * k_bias[1] * ds2**2\n        \n    total_counts = np.sum(all_histograms, axis=0)\n    visited_mask = total_counts > 0\n    \n    counts_visited = total_counts[visited_mask]\n    U_bias_visited = U_bias_all[:, visited_mask]\n    n_samples_per_window_arr = np.full(n_windows, n_samples_per_window)\n\n    f = np.zeros(n_windows)\n    \n    # WHAM fixed-point iteration\n    for iteration in range(2000): # More than enough for convergence\n        f_old = np.copy(f)\n        \n        # Calculate unnormalized probabilities P' in log space\n        log_numer = np.log(counts_visited)\n        \n        arg_logsumexp_i = np.log(n_samples_per_window_arr)[:, np.newaxis] + f[:, np.newaxis] - U_bias_visited\n        log_denom = logsumexp(arg_logsumexp_i, axis=0)\n        \n        log_P_unnorm = log_numer - log_denom\n        \n        # Normalize probabilities P over visited bins\n        log_Z_P = logsumexp(log_P_unnorm)\n        log_P = log_P_unnorm - log_Z_P\n        \n        # Update free energy offsets f\n        arg_logsumexp_j = log_P[np.newaxis, :] - U_bias_visited\n        f = -logsumexp(arg_logsumexp_j, axis=1)\n        f -= f[0] # Anchor the free energies\n        \n        # Check for convergence\n        if np.max(np.abs(f - f_old)) < 1e-9:\n            break\n            \n    # 4. Construct Free Energy Surface (FES)\n    F_wham = np.full(n_bins_total, np.inf)\n    F_wham[visited_mask] = -log_P\n    F_wham -= np.min(F_wham[visited_mask])\n\n    # 5. Construct Analytical Reference FES\n    K_inv_unbiased = np.linalg.inv(K_unbiased)\n    Sigma_s = A @ K_inv_unbiased @ A.T\n    K_s = np.linalg.inv(Sigma_s)\n    \n    F_ref = 0.5 * np.sum((s_coords @ K_s) * s_coords, axis=1)\n    F_ref -= np.min(F_ref[visited_mask])\n    \n    # 6. Calculate RMSE\n    rmse_mask = total_counts >= 5\n    if np.sum(rmse_mask) > 0:\n        diff_sq = (F_wham[rmse_mask] - F_ref[rmse_mask])**2\n        rmse = np.sqrt(np.mean(diff_sq))\n    else:\n        rmse = 0.0 # Or nan, depending on convention for no valid bins.\n        \n    return rmse\n\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and run the WHAM analysis.\n    \"\"\"\n    K_unbiased = np.array([[1.5, 0.3], [0.3, 1.0]])\n\n    test_cases = [\n        # Case 1\n        {\n            \"alpha\": 0.5, \"k_bias\": [4.0, 2.0],\n            \"c1_centers\": [-1.2, 0.0, 1.2], \"c2_centers\": [-1.2, 0.0, 1.2],\n            \"n_samples_per_window\": 4000,\n            \"s_lims\": [[-2.0, 2.0], [-2.0, 2.0]], \"n_bins\": [41, 41],\n            \"seed\": 123\n        },\n        # Case 2\n        {\n            \"alpha\": 0.9, \"k_bias\": [6.0, 6.0],\n            \"c1_centers\": [-1.8, 0.0, 1.8], \"c2_centers\": [-1.8, 0.0, 1.8],\n            \"n_samples_per_window\": 1500,\n            \"s_lims\": [[-3.0, 3.0], [-3.0, 3.0]], \"n_bins\": [45, 45],\n            \"seed\": 456\n        },\n        # Case 3\n        {\n            \"alpha\": 0.2, \"k_bias\": [3.0, 3.0],\n            \"c1_centers\": [-1.2, 1.2], \"c2_centers\": [-1.2, 1.2],\n            \"n_samples_per_window\": 600,\n            \"s_lims\": [[-2.0, 2.0], [-2.0, 2.0]], \"n_bins\": [35, 35],\n            \"seed\": 789\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        rmse = run_analysis(K_unbiased=K_unbiased, **case)\n        results.append(rmse)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```", "id": "2401621"}]}