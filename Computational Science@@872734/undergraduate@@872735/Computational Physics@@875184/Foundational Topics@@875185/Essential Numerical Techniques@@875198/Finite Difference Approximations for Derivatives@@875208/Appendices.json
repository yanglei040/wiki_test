{"hands_on_practices": [{"introduction": "The foundation of numerical methods lies in verifying their correctness and understanding their performance. This first exercise guides you through a fundamental validation procedure: empirically measuring the order of accuracy for the common centered-difference approximations of the first and second derivatives [@problem_id:2391581]. By comparing the numerical error's convergence rate against the theoretical prediction from Taylor series analysis, you will develop a critical skill for building and trusting computational tools.", "problem": "Design and implement a fully reproducible numerical experiment to empirically verify the order of accuracy of symmetric finite difference approximations for derivatives. Use only the following foundational base: Taylor series expansions of a sufficiently smooth function about a point, and the definitions of the first and second derivatives as limits. Your tasks are:\n\n1. Construct symmetric, centered finite difference approximations for the first derivative and the second derivative of a smooth function $f(x)$ at a point $x_0$ using a step size $h$. The approximations must use values of $f$ evaluated at $x_0 \\pm h$ (and optionally $x_0$ for the second derivative). Do not assume or quote any pre-existing error formulas; instead, base your reasoning on Taylor series expansions about $x_0$ to justify the expected truncation error behavior.\n\n2. For a given function $f(x)$ with known derivatives, define the true first derivative $f'(x)$ and the true second derivative $f''(x)$ at $x_0$. For a sequence of step sizes $h_k = h_0/2^k$, $k = 0,1,\\dots,n-1$, compute the absolute error $e_k = \\lvert A(h_k) - T \\rvert$ where $A(h_k)$ is the numerical approximation of the target derivative at $x_0$ using step size $h_k$ and $T$ is the corresponding true derivative value. For each consecutive pair of errors, compute the observed order of accuracy\n$$\np_k = \\log_2\\left( \\frac{e_k}{e_{k+1}} \\right).\n$$\nReport as the scalar result for the test case the last available $p_k$ in the refinement sequence (i.e., the one corresponding to the two smallest step sizes), which empirically approximates the asymptotic order.\n\n3. Implement the above as a complete program that runs the following test suite. Each test case specifies a target derivative order, a function $f(x)$, an evaluation point $x_0$, an initial step size $h_0$, and a number of refinements $n$. The functions and their true derivatives must be treated exactly as specified below.\n\n- Test case 1 (happy path, first derivative): Target derivative is $f'(x)$ with $f(x) = \\sin(x)$, $x_0 = 0.37$ (in radians), $h_0 = 0.2$, $n = 5$.\n- Test case 2 (different smoothness profile, first derivative): Target derivative is $f'(x)$ with $f(x) = \\exp(\\sin(x))$, $x_0 = -0.8$ (in radians), $h_0 = 0.2$, $n = 5$. Use the exact identity $f'(x) = \\exp(\\sin(x)) \\cos(x)$.\n- Test case 3 (happy path, second derivative): Target derivative is $f''(x)$ with $f(x) = \\sin(x)$, $x_0 = 0.37$ (in radians), $h_0 = 0.2$, $n = 5$. Use the exact identity $f''(x) = -\\sin(x)$.\n- Test case 4 (nontrivial rational function, second derivative): Target derivative is $f''(x)$ with $f(x) = \\frac{1}{1+x^2}$, $x_0 = 0.9$, $h_0 = 0.2$, $n = 5$. Use the exact identity $f''(x) = \\frac{6x^2 - 2}{(1+x^2)^3}$.\n- Test case 5 (edge case with large magnitude function values, first derivative): Target derivative is $f'(x)$ with $f(x) = \\exp(x)$, $x_0 = 5.0$, $h_0 = 0.4$, $n = 5$. Use the exact identity $f'(x) = \\exp(x)$.\n\n4. Your implementation must:\n- For target $f'(x)$, use a symmetric, centered stencil based only on $x_0 \\pm h$.\n- For target $f''(x)$, use a symmetric, centered stencil based only on $x_0 \\pm h$ and $x_0$.\n- Use the refinement strategy $h_k = h_0/2^k$ for $k = 0,1,\\dots,n-1$.\n- Compute absolute errors and the sequence of observed orders $p_k$.\n- Return the final scalar result per test case as the last $p_k$ in the sequence.\n- If an error difference happens to be zero at the smallest scales, use the most recent nonzero neighboring pair to compute $p_k$.\n\n5. Final output format:\nYour program should produce a single line of output containing the results as a comma-separated list of floating-point numbers enclosed in square brackets (for example, \"[r1,r2,r3,r4,r5]\"). Each number must be the final observed order for the corresponding test case, rounded to six decimal places.\n\nAngles must be interpreted in radians. No physical units are involved in this problem. The expected outcome is that the observed orders for both the first-derivative and second-derivative symmetric schemes are close to $2$, consistent with a truncation error proportional to $h^2$.", "solution": "The problem statement is a valid, well-posed exercise in numerical analysis. It is scientifically grounded in the theory of Taylor series and finite difference methods, contains no contradictions, and provides all necessary information to construct a unique, verifiable solution. We will therefore proceed with the derivation and implementation.\n\nThe objective is to derive symmetric finite difference formulas for the first and second derivatives of a sufficiently smooth function $f(x)$, analyze their truncation error, and empirically verify the theoretical order of accuracy through a numerical experiment.\n\nFirst, we derive the finite difference approximation for the first derivative, $f'(x)$. Let $f(x)$ be at least three times continuously differentiable. The Taylor series expansions of $f(x)$ around a point $x_0$ for steps $+h$ and $-h$ are given by:\n$$\nf(x_0 + h) = f(x_0) + h f'(x_0) + \\frac{h^2}{2!} f''(x_0) + \\frac{h^3}{3!} f'''(x_0) + O(h^4) \\quad (1)\n$$\n$$\nf(x_0 - h) = f(x_0) - h f'(x_0) + \\frac{h^2}{2!} f''(x_0) - \\frac{h^3}{3!} f'''(x_0) + O(h^4) \\quad (2)\n$$\nSubtracting equation $(2)$ from equation $(1)$ eliminates the terms with even powers of $h$:\n$$\nf(x_0 + h) - f(x_0 - h) = 2h f'(x_0) + \\frac{2h^3}{6} f'''(x_0) + O(h^5)\n$$\nSolving for $f'(x_0)$ yields:\n$$\nf'(x_0) = \\frac{f(x_0 + h) - f(x_0 - h)}{2h} - \\frac{h^2}{6} f'''(x_0) + O(h^4)\n$$\nThis gives the symmetric centered finite difference approximation for the first derivative:\n$$\nA_{f'}(h) = \\frac{f(x_0 + h) - f(x_0 - h)}{2h}\n$$\nThe truncation error, defined as the difference between the approximation and the true value, $E_{trunc} = A_{f'}(h) - f'(x_0)$, is therefore:\n$$\nE_{trunc} = -\\frac{h^2}{6} f'''(x_0) + O(h^4)\n$$\nThe leading term of the error is proportional to $h^2$, which means the approximation is second-order accurate.\n\nNext, we derive the approximation for the second derivative, $f''(x)$. Let $f(x)$ be at least four times continuously differentiable. Adding equations $(1)$ and $(2)$ eliminates the terms with odd powers of $h$:\n$$\nf(x_0 + h) + f(x_0 - h) = 2f(x_0) + h^2 f''(x_0) + \\frac{2h^4}{24} f''''(x_0) + O(h^6)\n$$\nSolving for $f''(x_0)$ yields:\n$$\nf''(x_0) = \\frac{f(x_0 + h) - 2f(x_0) + f(x_0 - h)}{h^2} - \\frac{h^2}{12} f''''(x_0) + O(h^4)\n$$\nThis gives the symmetric centered finite difference approximation for the second derivative:\n$$\nA_{f''}(h) = \\frac{f(x_0 + h) - 2f(x_0) + f(x_0 - h)}{h^2}\n$$\nThe truncation error for this approximation is:\n$$\nE_{trunc} = -\\frac{h^2}{12} f''''(x_0) + O(h^4)\n$$\nThe leading term of the error is again proportional to $h^2$, so this approximation is also second-order accurate.\n\nTo empirically verify the order of accuracy, we analyze the behavior of the absolute error as the step size $h$ is refined. For an approximation of order $p$, the absolute error $e(h) = |A(h) - T|$, where $T$ is the true derivative value, behaves as $e(h) \\approx C h^p$ for some constant $C$ and sufficiently small $h$.\nConsider a sequence of step sizes $h_k = h_0 / 2^k$ for $k=0, 1, 2, \\dots$. The corresponding errors are $e_k = e(h_k)$ and $e_{k+1} = e(h_{k+1})$. The ratio of consecutive errors is:\n$$\n\\frac{e_k}{e_{k+1}} \\approx \\frac{C h_k^p}{C h_{k+1}^p} = \\frac{C (h_0/2^k)^p}{C (h_0/2^{k+1})^p} = \\frac{(1/2^k)^p}{(1/2^{k+1})^p} = \\left(\\frac{2^{k+1}}{2^k}\\right)^p = 2^p\n$$\nBy taking the base-$2$ logarithm, we can compute the observed order of accuracy, $p_k$:\n$$\np_k = \\log_2\\left(\\frac{e_k}{e_{k+1}}\\right)\n$$\nAs $k$ increases, $h_k$ decreases, and $p_k$ should converge to the theoretical order $p$. For the derived second-order schemes, we expect $p_k \\to 2$.\n\nThe numerical experiment is implemented as follows. For each test case:\n1.  A sequence of $n$ step sizes is generated: $h_k = h_0 / 2^k$ for $k=0, \\dots, n-1$.\n2.  The true value of the derivative, $T$, is computed at the point $x_0$.\n3.  For each step size $h_k$, the corresponding numerical approximation $A(h_k)$ is calculated using either the first- or second-derivative formula.\n4.  A sequence of absolute errors is computed: $e_k = |A(h_k) - T|$.\n5.  A sequence of observed orders is computed: $p_k = \\log_2(e_k / e_{k+1})$ for $k=0, \\dots, n-2$.\n6.  The result is the last validly computed order, $p_{n-2}$, which corresponds to the comparison between the two smallest step sizes, $h_{n-2}$ and $h_{n-1}$. This provides an empirical estimate of the asymptotic order of accuracy.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Designs and implements a numerical experiment to verify the order of accuracy\n    of symmetric finite difference approximations for first and second derivatives.\n    \"\"\"\n\n    # Define the functions, their true derivatives, and parameters for each test case.\n    test_cases = [\n        {\n            # Test case 1: f'(x) for sin(x)\n            \"target_order\": 1,\n            \"f\": lambda x: np.sin(x),\n            \"true_deriv_func\": lambda x: np.cos(x),\n            \"x0\": 0.37,\n            \"h0\": 0.2,\n            \"n\": 5,\n        },\n        {\n            # Test case 2: f'(x) for exp(sin(x))\n            \"target_order\": 1,\n            \"f\": lambda x: np.exp(np.sin(x)),\n            \"true_deriv_func\": lambda x: np.exp(np.sin(x)) * np.cos(x),\n            \"x0\": -0.8,\n            \"h0\": 0.2,\n            \"n\": 5,\n        },\n        {\n            # Test case 3: f''(x) for sin(x)\n            \"target_order\": 2,\n            \"f\": lambda x: np.sin(x),\n            \"true_deriv_func\": lambda x: -np.sin(x),\n            \"x0\": 0.37,\n            \"h0\": 0.2,\n            \"n\": 5,\n        },\n        {\n            # Test case 4: f''(x) for 1/(1+x^2)\n            \"target_order\": 2,\n            \"f\": lambda x: 1.0 / (1.0 + x**2),\n            \"true_deriv_func\": lambda x: (6.0 * x**2 - 2.0) / (1.0 + x**2)**3,\n            \"x0\": 0.9,\n            \"h0\": 0.2,\n            \"n\": 5,\n        },\n        {\n            # Test case 5: f'(x) for exp(x) with large function magnitude\n            \"target_order\": 1,\n            \"f\": lambda x: np.exp(x),\n            \"true_deriv_func\": lambda x: np.exp(x),\n            \"x0\": 5.0,\n            \"h0\": 0.4,\n            \"n\": 5,\n        },\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        target_order = case[\"target_order\"]\n        f = case[\"f\"]\n        true_deriv_func = case[\"true_deriv_func\"]\n        x0 = case[\"x0\"]\n        h0 = case[\"h0\"]\n        n = case[\"n\"]\n\n        # Calculate the true value of the derivative\n        true_value = true_deriv_func(x0)\n\n        # Generate step sizes and compute errors\n        errors = []\n        for k in range(n):\n            h = h0 / (2**k)\n            \n            if target_order == 1:\n                # Symmetric centered difference for the first derivative\n                approx_val = (f(x0 + h) - f(x0 - h)) / (2.0 * h)\n            elif target_order == 2:\n                # Symmetric centered difference for the second derivative\n                approx_val = (f(x0 + h) - 2.0 * f(x0) + f(x0 - h)) / (h**2)\n            else:\n                # This case should not be reached based on the problem statement\n                raise ValueError(\"Invalid target derivative order.\")\n\n            # Compute and store the absolute error\n            errors.append(np.abs(approx_val - true_value))\n\n        # Compute observed orders of accuracy\n        observed_orders = []\n        for k in range(n - 1):\n            e_k = errors[k]\n            e_k_plus_1 = errors[k + 1]\n            \n            # Ensure errors are non-zero to avoid division by zero\n            # or log of zero. This also handles the specified edge case.\n            if e_k  0 and e_k_plus_1  0:\n                p_k = np.log2(e_k / e_k_plus_1)\n                observed_orders.append(p_k)\n\n        # The result for the test case is the last available observed order\n        if observed_orders:\n            final_p = observed_orders[-1]\n        else:\n            # Handle case where no valid order could be computed (e.g., all errors are zero)\n            # For this problem set, this fallback is not expected to be needed.\n            final_p = np.nan\n            \n        results.append(final_p)\n\n    # Format the final output as a comma-separated list of numbers rounded to six decimal places.\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```", "id": "2391581"}, {"introduction": "Moving from clean, analytical functions to real-world data introduces the challenge of noise. This practice explores the critical, and often problematic, interaction between finite difference schemes and noisy signals [@problem_id:2392343]. You will quantify how numerical differentiation amplifies measurement noise and see firsthand the trade-off between reducing truncation error with a small step size $h$ and the resulting explosion of noise-related error.", "problem": "You are given a time series model for one-dimensional position as a function of time with additive measurement noise. The position is a smooth, known function of time with superimposed zero-mean, independent noise at each sample. Your task is to design and implement a program that, for a set of test cases, constructs the noisy position data, estimates the velocity and acceleration using finite differences, and quantitatively analyzes the amplification of measurement noise by the differentiation operators.\n\nBase your reasoning on the following fundamental definitions and facts only: the derivative of a function is the limit of the difference quotient, linear operators acting on independent random variables produce outputs whose variances add according to the squares of the operator weights, and a Taylor series expansion can be used to derive the local truncation error order of a finite difference stencil. Do not use any other pre-packaged formulas beyond these principles.\n\nUse the following signal model. The noiseless position is\n$$\nx(t) = A \\sin(2\\pi f_1 t) + C \\sin(2\\pi f_2 t) + D t^2,\n$$\nwith parameters\n$$\nA = 1.0\\ \\text{m},\\quad C = 0.5\\ \\text{m},\\quad f_1 = 1.0\\ \\text{Hz},\\quad f_2 = 3.0\\ \\text{Hz},\\quad D = 0.05\\ \\text{m/s}^2.\n$$\nAngles in trigonometric functions are in radians. The exact velocity and acceleration are, respectively,\n$$\nv(t) = \\frac{dx}{dt} = 2\\pi f_1 A \\cos(2\\pi f_1 t) + 2\\pi f_2 C \\cos(2\\pi f_2 t) + 2 D t,\n$$\n$$\na(t) = \\frac{d^2 x}{dt^2} = - (2\\pi f_1)^2 A \\sin(2\\pi f_1 t) - (2\\pi f_2)^2 C \\sin(2\\pi f_2 t) + 2 D.\n$$\n\nSampling and noise model. For each test case, sample uniformly at times\n$$\nt_n = n \\,\\Delta t,\\quad n=0,1,\\dots,N-1,\\quad N = \\left\\lfloor \\frac{T}{\\Delta t}\\right\\rfloor + 1,\\quad T = 10\\ \\text{s},\n$$\nand form noisy measurements\n$$\nx_n^{\\text{noisy}} = x(t_n) + \\eta_n,\\quad \\eta_n \\sim \\mathcal{N}(0,\\sigma_x^2)\\ \\text{i.i.d.},\n$$\nwith a fixed random seed equal to $12345$ for reproducibility. All distances are in meters and time in seconds.\n\nFinite difference requirements. From first principles, derive and implement second-order accurate finite difference schemes for the first and second derivatives as follows:\n- For interior points, use a centered, second-order accurate stencil.\n- At the two boundaries, use one-sided, second-order accurate stencils.\nYour implementation must produce arrays $v_n^{\\text{FD}}$ and $a_n^{\\text{FD}}$ of the same length as the input $x_n$.\n\nNoise amplification analysis. Let a finite difference derivative at index $i$ be the linear combination\n$$\ny_i = \\sum_{j} w_{i,j} x_j,\n$$\nwhere $y_i$ represents either the first or second derivative estimate, and $w_{i,j}$ are the finite difference weights divided by the appropriate power of $\\Delta t$. Using only linearity of expectation and independence of the noise samples, derive and compute:\n- The empirical Root Mean Square (RMS) noise amplification for the first derivative,\n$$\ng_v^{\\text{emp}} = \\sqrt{\\frac{1}{N}\\sum_{i=0}^{N-1} \\left( v_i^{\\text{FD}}[x^{\\text{noisy}}] - v_i^{\\text{FD}}[x^{\\text{clean}}]\\right)^2 },\n$$\nand for the second derivative,\n$$\ng_a^{\\text{emp}} = \\sqrt{\\frac{1}{N}\\sum_{i=0}^{N-1} \\left( a_i^{\\text{FD}}[x^{\\text{noisy}}] - a_i^{\\text{FD}}[x^{\\text{clean}}]\\right)^2 }.\n$$\n- The theoretical RMS noise amplification predicted by the weights,\n$$\ng^{\\text{theory}} = \\sigma_x \\sqrt{ \\frac{1}{N} \\sum_{i=0}^{N-1} \\left( \\sum_{j} w_{i,j}^2 \\right) }.\n$$\nCompute $g_v^{\\text{theory}}$ and $g_a^{\\text{theory}}$ using the actual weights used at each index, including boundary stencils.\n\nPerformance metrics. For each test case, compute:\n- The RMS error of the velocity estimate relative to the exact velocity,\n$$\nE_v = \\sqrt{\\frac{1}{N}\\sum_{i=0}^{N-1} \\left( v_i^{\\text{FD}}[x^{\\text{noisy}}] - v(t_i) \\right)^2 } \\ \\text{in m/s}.\n$$\n- The RMS error of the acceleration estimate relative to the exact acceleration,\n$$\nE_a = \\sqrt{\\frac{1}{N}\\sum_{i=0}^{N-1} \\left( a_i^{\\text{FD}}[x^{\\text{noisy}}] - a(t_i) \\right)^2 } \\ \\text{in m/s}^2.\n$$\n- The ratios\n$$\nR_v = \\frac{ g_v^{\\text{emp}} }{ g_v^{\\text{theory}} },\\qquad R_a = \\frac{ g_a^{\\text{emp}} }{ g_a^{\\text{theory}} },\n$$\nwhich indicate how well the empirical noise amplification matches the theoretical prediction.\n\nTest suite. Run your program on the following four test cases, which vary sampling interval and noise level:\n- Case $1$: $\\Delta t = 0.01\\ \\text{s}$, $\\sigma_x = 0.001\\ \\text{m}$.\n- Case $2$: $\\Delta t = 0.1\\ \\text{s}$, $\\sigma_x = 0.001\\ \\text{m}$.\n- Case $3$: $\\Delta t = 0.01\\ \\text{s}$, $\\sigma_x = 0.01\\ \\text{m}$.\n- Case $4$: $\\Delta t = 0.001\\ \\text{s}$, $\\sigma_x = 0.001\\ \\text{m}$.\n\nFinal output format. Your program should produce a single line of output containing the results aggregated as a comma-separated list enclosed in square brackets. For each case in the order $1$ through $4$, append the four floating-point numbers in the following order: $E_v$ (in m/s), $E_a$ (in m/s$^2$), $R_v$ (dimensionless), $R_a$ (dimensionless). Each number must be printed in scientific notation with exactly six significant figures. For example, the overall output format is\n$$\n[\\ E_{v,1},\\ E_{a,1},\\ R_{v,1},\\ R_{a,1},\\ E_{v,2},\\ E_{a,2},\\ R_{v,2},\\ R_{a,2},\\ E_{v,3},\\ E_{a,3},\\ R_{v,3},\\ R_{a,3},\\ E_{v,4},\\ E_{a,4},\\ R_{v,4},\\ R_{a,4}\\ ].\n$$", "solution": "We begin from first principles. The first derivative of a function $x(t)$ at time $t$ is defined by the limit\n$$\n\\frac{dx}{dt}(t) = \\lim_{\\Delta t \\to 0} \\frac{x(t+\\Delta t) - x(t-\\Delta t)}{2\\Delta t},\n$$\nwhich suggests symmetric (centered) difference quotients for finite but small $\\Delta t$. The second derivative is defined by\n$$\n\\frac{d^2x}{dt^2}(t) = \\lim_{\\Delta t \\to 0} \\frac{x(t+\\Delta t) - 2x(t) + x(t-\\Delta t)}{\\Delta t^2}.\n$$\nUsing Taylor series expansions around $t_i = i \\Delta t$,\n$$\nx(t_{i\\pm 1}) = x(t_i) \\pm \\Delta t\\, x'(t_i) + \\frac{\\Delta t^2}{2} x''(t_i) \\pm \\frac{\\Delta t^3}{6} x^{(3)}(t_i) + \\frac{\\Delta t^4}{24} x^{(4)}(t_i) + \\mathcal{O}(\\Delta t^5),\n$$\nwe can derive second-order accurate centered stencils for interior points:\n$$\nx'(t_i) = \\frac{x_{i+1} - x_{i-1}}{2\\Delta t} + \\mathcal{O}(\\Delta t^2), \\quad\nx''(t_i) = \\frac{x_{i+1} - 2 x_i + x_{i-1}}{\\Delta t^2} + \\mathcal{O}(\\Delta t^2),\n$$\nwhere $x_i = x(t_i)$. At the boundaries, one cannot form symmetric differences; instead, a Taylor expansion using forward points yields one-sided, second-order accurate stencils. For the first derivative at the left boundary $i=0$,\n$$\nx'(t_0) = \\frac{-3 x_0 + 4 x_1 - x_2}{2\\Delta t} + \\mathcal{O}(\\Delta t^2),\n$$\nand analogously for the right boundary $i=N-1$,\n$$\nx'(t_{N-1}) = \\frac{3 x_{N-1} - 4 x_{N-2} + x_{N-3}}{2\\Delta t} + \\mathcal{O}(\\Delta t^2).\n$$\nFor the second derivative, forward and backward second-order accurate one-sided stencils are\n$$\nx''(t_0) = \\frac{2 x_0 - 5 x_1 + 4 x_2 - x_3}{\\Delta t^2} + \\mathcal{O}(\\Delta t^2), \\quad\nx''(t_{N-1}) = \\frac{2 x_{N-1} - 5 x_{N-2} + 4 x_{N-3} - x_{N-4}}{\\Delta t^2} + \\mathcal{O}(\\Delta t^2).\n$$\nThese formulas are obtained by solving systems of equations that match the Taylor expansions term-by-term to eliminate lower-order error terms, ensuring second-order accuracy.\n\nNoise amplification analysis rests on linearity. Let the finite difference operator mapping $\\{x_j\\}$ to a derivative estimate $\\{y_i\\}$ be linear:\n$$\ny_i = \\sum_{j} w_{i,j} x_j,\n$$\nwhere $w_{i,j}$ are the operator weights, including normalization by the appropriate power of $\\Delta t$ dictated by the derivative order. Suppose the measurements include additive zero-mean, independent noise $\\eta_j$ with variance $\\mathbb{V}[\\eta_j] = \\sigma_x^2$. Due to linearity and independence,\n$$\n\\mathbb{E}[y_i] = \\sum_{j} w_{i,j} \\mathbb{E}[x_j], \\quad\n\\mathbb{V}[y_i] = \\sum_{j} w_{i,j}^2 \\,\\mathbb{V}[\\eta_j] = \\sigma_x^2 \\sum_{j} w_{i,j}^2.\n$$\nTherefore, the Root Mean Square (RMS) of the noise component at index $i$ equals\n$$\n\\sqrt{\\mathbb{V}[y_i]} = \\sigma_x \\sqrt{\\sum_{j} w_{i,j}^2}.\n$$\nA global RMS across all indices, consistent with the empirical RMS used in practice, is\n$$\ng^{\\text{theory}} = \\sigma_x \\sqrt{\\frac{1}{N}\\sum_{i=0}^{N-1} \\left(\\sum_{j} w_{i,j}^2\\right)}.\n$$\nThis expression includes boundary effects naturally via index-dependent weights.\n\nTwo consequences follow:\n- For the first derivative, the weights scale like $1/\\Delta t$, so $g_v^{\\text{theory}} \\propto \\sigma_x/\\Delta t$; that is, reducing $\\Delta t$ increases the noise amplification in the velocity estimate if the position noise level per sample is fixed.\n- For the second derivative, the weights scale like $1/\\Delta t^2$, so $g_a^{\\text{theory}} \\propto \\sigma_x/\\Delta t^2$, implying even stronger amplification of noise.\n\nThe empirical Root Mean Square (RMS) noise amplification can be isolated by comparing the derivative operator applied to noisy data and to clean data, which cancels deterministic truncation error:\n$$\ng^{\\text{emp}} = \\sqrt{\\frac{1}{N}\\sum_{i=0}^{N-1} \\left( y_i[x^{\\text{noisy}}] - y_i[x^{\\text{clean}}] \\right)^2 }.\n$$\nBecause $y_i[\\cdot]$ is linear and $x^{\\text{noisy}} = x^{\\text{clean}} + \\eta$, the difference equals $y_i[\\eta]$, whose RMS matches the theoretical expression derived above in the limit of many samples. Hence, the ratios\n$$\nR_v = \\frac{g_v^{\\text{emp}}}{g_v^{\\text{theory}}},\\qquad R_a = \\frac{g_a^{\\text{emp}}}{g_a^{\\text{theory}}},\n$$\nshould be close to $1$ when the empirical averages are representative.\n\nAlgorithmic design:\n- Construct time samples $t_i = i \\Delta t$ for $i=0,\\dots,N-1$ with $T=10$ s.\n- Compute the clean position $x(t_i)$, and the exact velocity $v(t_i)$ and acceleration $a(t_i)$ from the given analytic expressions.\n- Generate additive noise $\\eta_i \\sim \\mathcal{N}(0,\\sigma_x^2)$ using a fixed seed to ensure reproducibility, and form $x^{\\text{noisy}}_i = x(t_i) + \\eta_i$.\n- Implement second-order accurate finite difference estimators for the first and second derivatives that apply centered stencils in the interior and one-sided stencils at the boundaries, yielding $v^{\\text{FD}}$ and $a^{\\text{FD}}$ for both clean and noisy inputs.\n- Compute performance metrics: $E_v$ and $E_a$ as RMS errors against the exact derivatives. Compute $g_v^{\\text{emp}}$ and $g_a^{\\text{emp}}$ as RMS differences between the finite difference outputs on noisy and clean data. Compute $g_v^{\\text{theory}}$ and $g_a^{\\text{theory}}$ via the weights by summing squared weights at each index and averaging, then multiply by $\\sigma_x$ and take a square root. Finally, compute $R_v$ and $R_a$ as ratios of empirical to theoretical amplification.\n- Repeat for the four specified test cases. Print the consolidated results in the required single-line, bracketed, comma-separated list, with scientific notation and six significant figures.\n\nExpected trends:\n- Increasing $\\sigma_x$ by a factor of $10$ should increase $g^{\\text{emp}}$ and the noise-dominated components of $E_v$ and $E_a$ by a factor of $10$.\n- Increasing $\\Delta t$ should reduce $g_v^{\\text{theory}}$ roughly like $1/\\Delta t$ and $g_a^{\\text{theory}}$ roughly like $1/\\Delta t^2$; however, truncation error grows like $\\mathcal{O}(\\Delta t^2)$, so $E_v$ and $E_a$ may not decrease monotonically with $\\Delta t$ because of the trade-off between truncation error and noise amplification.\n- The ratios $R_v$ and $R_a$ should be close to $1$, validating the linear noise amplification analysis.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef position(t, A=1.0, C=0.5, f1=1.0, f2=3.0, D=0.05):\n    # x(t) in meters\n    return A * np.sin(2*np.pi*f1*t) + C * np.sin(2*np.pi*f2*t) + D * t**2\n\ndef velocity_true(t, A=1.0, C=0.5, f1=1.0, f2=3.0, D=0.05):\n    # v(t) in m/s\n    return 2*np.pi*f1*A * np.cos(2*np.pi*f1*t) + 2*np.pi*f2*C * np.cos(2*np.pi*f2*t) + 2*D*t\n\ndef acceleration_true(t, A=1.0, C=0.5, f1=1.0, f2=3.0, D=0.05):\n    # a(t) in m/s^2\n    return -(2*np.pi*f1)**2 * A * np.sin(2*np.pi*f1*t) - (2*np.pi*f2)**2 * C * np.sin(2*np.pi*f2*t) + 2*D\n\ndef fd_first_derivative(x, dt):\n    \"\"\"\n    Second-order accurate finite difference for first derivative.\n    - One-sided 3-point at boundaries (order 2).\n    - Centered 3-point in interior (order 2).\n    Returns y of same length as x.\n    \"\"\"\n    n = x.size\n    y = np.empty_like(x, dtype=float)\n    if n  3:\n        raise ValueError(\"Need at least 3 points for second-order first derivative\")\n    # Left boundary: (-3 x0 + 4 x1 - x2)/(2 dt)\n    y[0] = (-3.0*x[0] + 4.0*x[1] - 1.0*x[2]) / (2.0*dt)\n    # Interior\n    y[1:-1] = (x[2:] - x[:-2]) / (2.0*dt)\n    # Right boundary: (3 xN-1 - 4 xN-2 + xN-3)/(2 dt)\n    y[-1] = (3.0*x[-1] - 4.0*x[-2] + 1.0*x[-3]) / (2.0*dt)\n    return y\n\ndef fd_second_derivative(x, dt):\n    \"\"\"\n    Second-order accurate finite difference for second derivative.\n    - One-sided 4-point at boundaries (order 2).\n    - Centered 3-point in interior (order 2).\n    Returns y of same length as x.\n    \"\"\"\n    n = x.size\n    y = np.empty_like(x, dtype=float)\n    if n  4:\n        raise ValueError(\"Need at least 4 points for second derivative with second-order accuracy\")\n    # Left boundary: (2 x0 - 5 x1 + 4 x2 - x3)/dt^2\n    y[0] = (2.0*x[0] - 5.0*x[1] + 4.0*x[2] - 1.0*x[3]) / (dt*dt)\n    # Interior\n    y[1:-1] = (x[2:] - 2.0*x[1:-1] + x[:-2]) / (dt*dt)\n    # Right boundary: (2 xN-1 - 5 xN-2 + 4 xN-3 - xN-4)/dt^2\n    y[-1] = (2.0*x[-1] - 5.0*x[-2] + 4.0*x[-3] - 1.0*x[-4]) / (dt*dt)\n    return y\n\ndef weights_sqsum_first(n, dt):\n    \"\"\"\n    For each index i, compute sum_j w_{i,j}^2 for the first derivative operator.\n    Returns an array s of length n with s[i] = sum_j w_{i,j}^2.\n    \"\"\"\n    s = np.zeros(n, dtype=float)\n    inv = 1.0 / (2.0*dt)\n    # Left boundary: (-3, 4, -1)/(2 dt)\n    coeffs = np.array([-3.0, 4.0, -1.0]) * inv\n    s[0] = np.sum(coeffs**2)\n    # Interior: [-1, +1] at i-1 and i+1\n    c = np.array([-1.0, 1.0]) * inv\n    val = np.sum(c**2)\n    s[1:-1] = val\n    # Right boundary: (1, -4, 3)/(2 dt) applied to (i-2, i-1, i)\n    coeffs = np.array([1.0, -4.0, 3.0]) * inv\n    s[-1] = np.sum(coeffs**2)\n    return s\n\ndef weights_sqsum_second(n, dt):\n    \"\"\"\n    For each index i, compute sum_j w_{i,j}^2 for the second derivative operator.\n    Returns an array s of length n with s[i] = sum_j w_{i,j}^2.\n    \"\"\"\n    s = np.zeros(n, dtype=float)\n    inv2 = 1.0 / (dt*dt)\n    # Left boundary: (2, -5, 4, -1)/dt^2\n    coeffs = np.array([2.0, -5.0, 4.0, -1.0]) * inv2\n    s[0] = np.sum(coeffs**2)\n    # Interior: (1, -2, 1)/dt^2\n    c = np.array([1.0, -2.0, 1.0]) * inv2\n    val = np.sum(c**2)\n    s[1:-1] = val\n    # Right boundary: (-1, 4, -5, 2)/dt^2 applied to (i-3, i-2, i-1, i)\n    coeffs = np.array([-1.0, 4.0, -5.0, 2.0]) * inv2\n    s[-1] = np.sum(coeffs**2)\n    return s\n\ndef rms(x):\n    return np.sqrt(np.mean(np.square(x)))\n\ndef format_float(x):\n    # Scientific notation with exactly six significant figures\n    return f\"{x:.6e}\"\n\ndef run_case(dt, sigma_x, rng):\n    T = 10.0\n    N = int(np.floor(T/dt)) + 1\n    t = np.linspace(0.0, dt*(N-1), N)\n    x_clean = position(t)\n    # Generate noise with given sigma\n    noise = rng.normal(loc=0.0, scale=sigma_x, size=N)\n    x_noisy = x_clean + noise\n\n    # True derivatives\n    v_true = velocity_true(t)\n    a_true = acceleration_true(t)\n\n    # Finite difference estimates\n    v_fd_clean = fd_first_derivative(x_clean, dt)\n    a_fd_clean = fd_second_derivative(x_clean, dt)\n    v_fd_noisy = fd_first_derivative(x_noisy, dt)\n    a_fd_noisy = fd_second_derivative(x_noisy, dt)\n\n    # RMS errors against exact\n    E_v = rms(v_fd_noisy - v_true)\n    E_a = rms(a_fd_noisy - a_true)\n\n    # Empirical noise amplification (difference noisy-clean)\n    g_v_emp = rms(v_fd_noisy - v_fd_clean)\n    g_a_emp = rms(a_fd_noisy - a_fd_clean)\n\n    # Theoretical noise amplification from weights\n    s1 = weights_sqsum_first(N, dt)\n    s2 = weights_sqsum_second(N, dt)\n    g_v_theory = sigma_x * np.sqrt(np.mean(s1))\n    g_a_theory = sigma_x * np.sqrt(np.mean(s2))\n\n    # Ratios (avoid division by zero, though here not zero)\n    R_v = g_v_emp / g_v_theory if g_v_theory  0 else np.nan\n    R_a = g_a_emp / g_a_theory if g_a_theory  0 else np.nan\n\n    return E_v, E_a, R_v, R_a\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each tuple: (dt [s], sigma_x [m])\n    test_cases = [\n        (0.01, 0.001),   # Case 1\n        (0.1, 0.001),    # Case 2\n        (0.01, 0.01),    # Case 3\n        (0.001, 0.001),  # Case 4\n    ]\n\n    rng = np.random.default_rng(12345)\n\n    results = []\n    for dt, sigma_x in test_cases:\n        E_v, E_a, R_v, R_a = run_case(dt, sigma_x, rng)\n        results.extend([E_v, E_a, R_v, R_a])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(format_float(x) for x in results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2392343"}, {"introduction": "Finite differences are not just for estimating derivatives of known functions; their true power lies in solving differential equations. In this exercise, you will tackle a nonlinear boundary value problem that describes the shape of a loaded hanging cable [@problem_id:2392377]. By discretizing the governing equation, you will transform a second-order ODE into a system of nonlinear algebraic equations and solve it using Newton's method, a powerful combination of techniques central to computational science.", "problem": "Consider the nonlinear second-order ordinary differential equation for a loaded, hanging cable (catenary-type shape) given by $y''(x) = a \\sqrt{1 + (y'(x))^2}$ defined on a closed interval $[x_L, x_R]$, with Dirichlet boundary conditions $y(x_L) = y_L$ and $y(x_R) = y_R$, where $a  0$ is a given constant. Your task is to construct a numerical approximation to $y(x)$ on a uniform grid using a second-order accurate finite difference scheme and to solve the resulting nonlinear algebraic system.\n\nStart from fundamental Taylor expansions about a point $x_i$ to define second-order accurate centered difference approximations for $y'(x_i)$ and $y''(x_i)$ on a uniform grid with spacing $h = (x_R - x_L)/N$, where $N$ is the number of subintervals and the grid nodes are $x_i = x_L + i h$ for $i = 0, 1, \\dots, N$. Use these approximations to discretize the differential equation at each interior node $x_i$ for $i = 1, \\dots, N-1$ into a nonlinear algebraic system for the unknown nodal values $y_i \\approx y(x_i)$, with $y_0 = y_L$ and $y_N = y_R$ enforced directly. Solve the nonlinear system using a Newton method with an analytically derived Jacobian.\n\nThe numerical method must be designed using a valid base of well-tested formulas: Taylor expansions for smooth functions, the centered difference approximation for first and second derivatives, and Newton’s method for systems of nonlinear equations. Do not assume or use any pre-derived formulas beyond these bases.\n\nVerification requirement: Independently obtain the exact analytic solution of the ordinary differential equation by separation of variables and integration, determine the constants from the boundary conditions, and evaluate the exact solution at the grid nodes. Compute, for each test case, the maximum absolute error over all grid nodes between the numerical solution and the exact solution. No physical units are required for this problem; all quantities are treated as dimensionless.\n\nTest suite. Your program must run the following four test cases and produce the requested outputs in the specified format:\n- Test case $1$: $x_L = -0.5$, $x_R = 0.5$, $a = 2.0$, $N = 200$, $y_L = 0.27154031740762185$, $y_R = 0.27154031740762185$.\n- Test case $2$: $x_L = 0.0$, $x_R = 1.0$, $a = 1.5$, $N = 150$, $y_L = 0.0$, $y_R = 1.5577727726083314$.\n- Test case $3$: $x_L = -0.2$, $x_R = 0.2$, $a = 8.0$, $N = 40$, $y_L = 0.19718305889936066$, $y_R = 0.19718305889936066$.\n- Test case $4$: $x_L = -1.0$, $x_R = 1.0$, $a = 0.3$, $N = 10$, $y_L = 0.1511283804295349$, $y_R = 0.1511283804295349$.\n\nFinal output specification. Your program must produce a single line containing a Python-style list with four floating-point numbers, each being the maximum absolute error for one of the test cases in the order listed above. The numbers must be printed in scientific notation with $10$ significant digits. For example, an output line should look like $[1.2345678900e-06,2.3456789000e-07,3.4567890000e-08,4.5678900000e-09]$ (this is only an example).\n\nAngles are not involved. All results are dimensionless real numbers. No user input is required; all parameters are fixed as above. The program must implement the numerical method as described and compute the error against the exact solution determined from the same differential equation and boundary conditions.", "solution": "We derive a finite difference scheme for the nonlinear ordinary differential equation $y''(x) = a \\sqrt{1 + (y'(x))^2}$ with Dirichlet boundary conditions $y(x_L) = y_L$ and $y(x_R) = y_R$ on a uniform grid. The derivation starts from Taylor series, which are well-tested and fundamental for constructing finite difference formulas.\n\nLet the grid nodes be $x_i = x_L + i h$ for $i = 0, 1, \\dots, N$, with uniform spacing $h = (x_R - x_L)/N$. For a sufficiently smooth function $y(x)$, the Taylor expansions about $x_i$ are\n$$\ny(x_{i \\pm 1}) = y(x_i) \\pm h y'(x_i) + \\frac{h^2}{2} y''(x_i) \\pm \\frac{h^3}{6} y^{(3)}(x_i) + \\frac{h^4}{24} y^{(4)}(x_i) + \\mathcal{O}(h^5).\n$$\nFrom these, the classical second-order centered difference approximations follow:\n$$\ny'(x_i) = \\frac{y(x_{i+1}) - y(x_{i-1})}{2 h} + \\mathcal{O}(h^2),\n\\qquad\ny''(x_i) = \\frac{y(x_{i+1}) - 2 y(x_i) + y(x_{i-1})}{h^2} + \\mathcal{O}(h^2).\n$$\nWe denote the grid-based unknowns by $y_i \\approx y(x_i)$, with $y_0 = y_L$ and $y_N = y_R$ known from the boundary conditions. Substituting the centered differences into the differential equation at interior nodes $i = 1, \\dots, N-1$ yields the nonlinear system\n$$\nF_i(\\mathbf{y}) \\equiv \\frac{y_{i+1} - 2 y_i + y_{i-1}}{h^2} - a \\sqrt{1 + \\left(\\frac{y_{i+1} - y_{i-1}}{2 h}\\right)^2} = 0,\n\\quad i = 1, \\dots, N-1,\n$$\nwhere $\\mathbf{y} = (y_1, y_2, \\dots, y_{N-1})^\\top$ collects the interior unknowns. The residual vector $\\mathbf{F}(\\mathbf{y})$ has length $N-1$.\n\nTo solve $\\mathbf{F}(\\mathbf{y}) = \\mathbf{0}$, we apply Newton’s method, which is a standard, well-tested algorithm for nonlinear systems. Given an initial guess $\\mathbf{y}^{(0)}$ (for instance, the straight line interpolating between the boundary values),\n$$\n\\mathbf{y}^{(k+1)} = \\mathbf{y}^{(k)} + \\Delta \\mathbf{y}, \\quad \\text{where} \\quad J(\\mathbf{y}^{(k)}) \\Delta \\mathbf{y} = - \\mathbf{F}(\\mathbf{y}^{(k)}),\n$$\nand $J(\\mathbf{y})$ is the Jacobian matrix with entries $J_{ij}(\\mathbf{y}) = \\partial F_i / \\partial y_j$. Because the residual at node $i$ only depends on $y_{i-1}$, $y_i$, and $y_{i+1}$, the Jacobian is tridiagonal. Define at each interior node $i$ the discrete slope\n$$\ns_i \\equiv \\frac{y_{i+1} - y_{i-1}}{2 h}.\n$$\nThen, by differentiation,\n$$\n\\frac{\\partial F_i}{\\partial y_i} = -\\frac{2}{h^2},\n$$\n$$\n\\frac{\\partial F_i}{\\partial y_{i+1}} = \\frac{1}{h^2} - a \\cdot \\frac{s_i}{\\sqrt{1 + s_i^2}} \\cdot \\frac{1}{2 h},\n$$\n$$\n\\frac{\\partial F_i}{\\partial y_{i-1}} = \\frac{1}{h^2} + a \\cdot \\frac{s_i}{\\sqrt{1 + s_i^2}} \\cdot \\frac{1}{2 h}.\n$$\nThese formulas follow from the chain rule applied to $\\sqrt{1 + s_i^2}$ with $\\partial s_i/\\partial y_{i+1} = 1/(2h)$ and $\\partial s_i/\\partial y_{i-1} = -1/(2h)$. For rows adjacent to the boundary, if $y_{i-1}$ or $y_{i+1}$ is a boundary value (i.e., $i=1$ or $i=N-1$), its contribution is treated as a known constant in $F_i$, and the corresponding Jacobian column is omitted since the associated variable is not part of $\\mathbf{y}$.\n\nConvergence of Newton’s method is monitored using either the residual norm $\\|\\mathbf{F}(\\mathbf{y}^{(k)})\\|$ or the update norm $\\|\\Delta \\mathbf{y}\\|$ relative to $\\|\\mathbf{y}^{(k)}\\|$. A typical stopping criterion is\n$$\n\\frac{\\|\\Delta \\mathbf{y}\\|_\\infty}{\\max(1, \\|\\mathbf{y}^{(k)}\\|_\\infty)}  \\varepsilon\n\\quad \\text{or} \\quad\n\\|\\mathbf{F}(\\mathbf{y}^{(k)})\\|_\\infty  \\varepsilon,\n$$\nwith a tolerance such as $\\varepsilon = 10^{-12}$.\n\nFor verification, we use the exact analytic solution of the differential equation, which can be obtained by separation of variables. Let $p(x) = y'(x)$, then\n$$\n\\frac{dp}{dx} = a \\sqrt{1 + p^2}\n\\quad \\Rightarrow \\quad\n\\int \\frac{dp}{\\sqrt{1 + p^2}} = \\int a \\, dx\n\\quad \\Rightarrow \\quad\n\\operatorname{arsinh}(p) = a x + C_1,\n$$\nso that\n$$\np(x) = \\sinh(a x + C_1).\n$$\nIntegrating once more,\n$$\ny(x) = \\int p(x) \\, dx = \\frac{1}{a} \\cosh(a x + C_1) + C_2.\n$$\nThe constants $C_1$ and $C_2$ are determined from the boundary conditions $y(x_L) = y_L$ and $y(x_R) = y_R$. Subtracting the two boundary conditions gives a single nonlinear equation in $C_1$:\n$$\ny_R - y_L = \\frac{1}{a}\\left[\\cosh(a x_R + C_1) - \\cosh(a x_L + C_1)\\right],\n$$\nwhich is strictly monotone in $C_1$ because $\\sinh$ is strictly increasing, ensuring a unique solution for $C_1$. This can be solved robustly with a bracketed root-finding method such as the Brent method. Then $C_2$ follows from\n$$\nC_2 = y_L - \\frac{1}{a} \\cosh(a x_L + C_1),\n$$\nand the exact solution evaluates to\n$$\ny_{\\text{exact}}(x) = \\frac{1}{a} \\cosh(a x + C_1) + C_2.\n$$\nWe then compute the maximum absolute nodal error\n$$\nE_\\infty = \\max_{0 \\le i \\le N} \\left| y_i - y_{\\text{exact}}(x_i) \\right|.\n$$\n\nAlgorithmic design summary:\n1. For each test case, construct the uniform grid with $N+1$ nodes and spacing $h$.\n2. Initialize the interior unknowns $\\mathbf{y}^{(0)}$ by linear interpolation between $(x_L, y_L)$ and $(x_R, y_R)$.\n3. Assemble the residual $\\mathbf{F}(\\mathbf{y})$ and tridiagonal Jacobian $J(\\mathbf{y})$ using the formulas above.\n4. Apply Newton’s method to solve $J \\Delta \\mathbf{y} = -\\mathbf{F}$ and update $\\mathbf{y}$ until convergence.\n5. Independently compute $C_1$ and $C_2$ from the boundary conditions and evaluate $y_{\\text{exact}}(x_i)$ at all nodes.\n6. Report $E_\\infty$ for each test case.\n\nAccuracy expectations:\n- The centered difference discretization introduces a local truncation error of order $\\mathcal{O}(h^2)$; therefore, for sufficiently smooth solutions and a converged Newton solve, we expect the nodal error to scale like $\\mathcal{O}(h^2)$ as $N$ increases.\n- The Jacobian is tridiagonal, so the linear solve per Newton iteration is $\\mathcal{O}(N)$ with specialized solvers, or $\\mathcal{O}(N^3)$ with a dense solver; given the modest $N$ in the test suite, a dense solve is acceptable.\n\nTest suite details:\n- Test case $1$: $x_L = -0.5$, $x_R = 0.5$, $a = 2.0$, $N = 200$, $y_L = 0.27154031740762185$, $y_R = 0.27154031740762185$.\n- Test case $2$: $x_L = 0.0$, $x_R = 1.0$, $a = 1.5$, $N = 150$, $y_L = 0.0$, $y_R = 1.5577727726083314$.\n- Test case $3$: $x_L = -0.2$, $x_R = 0.2$, $a = 8.0$, $N = 40$, $y_L = 0.19718305889936066$, $y_R = 0.19718305889936066$.\n- Test case $4$: $x_L = -1.0$, $x_R = 1.0$, $a = 0.3$, $N = 10$, $y_L = 0.1511283804295349$, $y_R = 0.1511283804295349$.\n\nFinal output: A single line containing a Python-style list with four floating-point numbers in scientific notation with $10$ significant digits, namely $[E_{\\infty,1}, E_{\\infty,2}, E_{\\infty,3}, E_{\\infty,4}]$ in the order of the test cases above.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import brentq\n\ndef assemble_residual_and_jacobian(y, a, h, yL, yR):\n    \"\"\"\n    Assemble residual vector F and tridiagonal Jacobian J for the nonlinear system\n    arising from central-difference discretization of y'' = a*sqrt(1+(y')^2).\n    The vector y contains only interior unknowns y[1..N-1].\n    \"\"\"\n    N_minus_1 = y.size  # number of interior points\n    N = N_minus_1 + 1   # number of intervals\n    # Construct the full y with boundaries for convenience\n    y_full = np.empty(N+1)\n    y_full[0] = yL\n    y_full[-1] = yR\n    y_full[1:-1] = y\n\n    F = np.zeros(N_minus_1)\n    J = np.zeros((N_minus_1, N_minus_1))\n\n    inv_h2 = 1.0 / (h * h)\n    inv_2h = 1.0 / (2.0 * h)\n\n    for idx in range(N_minus_1):\n        i = idx + 1  # grid index in full array\n        ym1 = y_full[i - 1]\n        yi = y_full[i]\n        yp1 = y_full[i + 1]\n\n        s = (yp1 - ym1) * inv_2h\n        sqrt_term = np.sqrt(1.0 + s * s)\n\n        # Residual at interior node i\n        F[idx] = (ym1 - 2.0 * yi + yp1) * inv_h2 - a * sqrt_term\n\n        # Jacobian entries\n        # Diagonal dF/dy_i\n        J[idx, idx] = -2.0 * inv_h2\n\n        # Off-diagonals if neighbors are interior unknowns\n        common = a * s / sqrt_term * inv_2h  # a * s / sqrt(1+s^2) * (1/(2h))\n\n        # dF/dy_{i-1}\n        if idx - 1 = 0:\n            J[idx, idx - 1] = inv_h2 + common\n        # dF/dy_{i+1}\n        if idx + 1  N_minus_1:\n            J[idx, idx + 1] = inv_h2 - common\n\n    return F, J\n\ndef newton_solve(a, xL, xR, yL, yR, N, tol=1e-12, maxiter=50):\n    \"\"\"\n    Solve the discretized nonlinear system by Newton's method.\n    Returns the nodal coordinates x and the solution y at all nodes.\n    \"\"\"\n    x = np.linspace(xL, xR, N + 1)\n    h = (xR - xL) / N\n\n    # Initial guess: straight line between boundaries\n    y0 = np.linspace(yL, yR, N + 1)\n    y_int = y0[1:-1].copy()\n\n    for _ in range(maxiter):\n        F, J = assemble_residual_and_jacobian(y_int, a, h, yL, yR)\n        # Solve J * delta = -F\n        try:\n            delta = np.linalg.solve(J, -F)\n        except np.linalg.LinAlgError:\n            # Fall back to least squares in rare singular cases\n            delta, *_ = np.linalg.lstsq(J, -F, rcond=None)\n\n        y_int_new = y_int + delta\n\n        # Check convergence\n        denom = max(1.0, np.linalg.norm(y_int, ord=np.inf))\n        if np.linalg.norm(delta, ord=np.inf) / denom  tol:\n            y_int = y_int_new\n            break\n\n        y_int = y_int_new\n\n    # Construct full y\n    y = np.empty(N + 1)\n    y[0] = yL\n    y[-1] = yR\n    y[1:-1] = y_int\n    return x, y\n\ndef exact_solution_params(a, xL, xR, yL, yR):\n    \"\"\"\n    Solve for constants C1, C2 in y_exact(x) = (1/a) cosh(a x + C1) + C2\n    given boundary conditions y(xL) = yL, y(xR) = yR.\n    Uses a robust bracketed root solve for C1, then computes C2.\n    \"\"\"\n    # Define the monotone function of C1 from the boundary difference\n    def f(C1):\n        return (1.0 / a) * (np.cosh(a * xR + C1) - np.cosh(a * xL + C1)) - (yR - yL)\n\n    # Find a bracket where f changes sign; for this problem, [-50, 50] is safe\n    C1 = brentq(f, -50.0, 50.0, maxiter=1000, xtol=1e-14, rtol=1e-14)\n    C2 = yL - (1.0 / a) * np.cosh(a * xL + C1)\n    return C1, C2\n\ndef exact_solution_on_grid(a, x, xL, xR, yL, yR):\n    C1, C2 = exact_solution_params(a, xL, xR, yL, yR)\n    return (1.0 / a) * np.cosh(a * x + C1) + C2\n\ndef max_abs_error(y_num, y_ex):\n    return float(np.max(np.abs(y_num - y_ex)))\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (xL, xR, a, N, yL, yR)\n    test_cases = [\n        (-0.5, 0.5, 2.0, 200, 0.27154031740762185, 0.27154031740762185),\n        (0.0, 1.0, 1.5, 150, 0.0, 1.5577727726083314),\n        (-0.2, 0.2, 8.0, 40, 0.19718305889936066, 0.19718305889936066),\n        (-1.0, 1.0, 0.3, 10, 0.1511283804295349, 0.1511283804295349),\n    ]\n\n    results = []\n    for xL, xR, a, N, yL, yR in test_cases:\n        x, y_num = newton_solve(a, xL, xR, yL, yR, N, tol=1e-12, maxiter=50)\n        y_ex = exact_solution_on_grid(a, x, xL, xR, yL, yR)\n        err = max_abs_error(y_num, y_ex)\n        results.append(err)\n\n    # Final print statement in the exact required format: scientific notation with 10 significant digits\n    formatted = \",\".join(f\"{val:.10e}\" for val in results)\n    print(f\"[{formatted}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2392377"}]}