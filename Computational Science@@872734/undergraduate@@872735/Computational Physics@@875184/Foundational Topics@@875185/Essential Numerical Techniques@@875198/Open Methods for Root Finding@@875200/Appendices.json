{"hands_on_practices": [{"introduction": "Open methods like Newton's method offer rapid convergence, but this speed comes with a crucial caveat: convergence is not guaranteed and depends sensitively on the initial guess. This exercise provides a hands-on analytical exploration of the \"basin of attraction,\" the set of starting points that converge to a specific root. By analyzing the iterative map for a simple polynomial [@problem_id:1677773], you will uncover the boundaries that separate these basins, gaining a deeper appreciation for the rich and sometimes complex dynamics that govern the behavior of iterative methods.", "problem": "Newton's method is a popular algorithm for finding the roots of a differentiable function $f(x)$. Starting with an initial guess $x_0$, the method generates a sequence of approximations using the iterative formula:\n$$x_{k+1} = N(x_k) = x_k - \\frac{f(x_k)}{f'(x_k)}$$\nThis process can be viewed as a discrete dynamical system. For a given function, the set of all initial points $x_0$ that lead to a sequence converging to a particular root is called the basin of attraction for that root.\n\nConsider the function $f(x) = x^3 - x$. This function has three distinct real roots. The real number line can be partitioned into the basins of attraction for these three roots. Due to the symmetry of the function, the basin of attraction for the root at $x=0$ is a single connected open interval of the form $(-\\beta, \\beta)$ for some positive constant $\\beta$. The basin for the root at $x=1$ includes the interval $(\\beta, \\infty)$, and the basin for the root at $x=-1$ includes the interval $(-\\infty, -\\beta)$. The boundary points $\\pm\\beta$ themselves form a periodic orbit and do not converge to any of the three roots.\n\nDetermine the exact positive value of this boundary point, $\\beta$.", "solution": "Compute the Newton map for $f(x)=x^{3}-x$. Using $N(x)=x-\\frac{f(x)}{f'(x)}$ with $f'(x)=3x^{2}-1$, we obtain\n$$\nN(x)=x-\\frac{x^{3}-x}{3x^{2}-1}\n=\\frac{x(3x^{2}-1)-(x^{3}-x)}{3x^{2}-1}\n=\\frac{3x^{3}-x-x^{3}+x}{3x^{2}-1}\n=\\frac{2x^{3}}{3x^{2}-1}.\n$$\nThe map $N$ is odd, since $N(-x)=-N(x)$. Its fixed points satisfy $N(x)=x$, which reduces to $f(x)=0$, so the only fixed points are $x\\in\\{-1,0,1\\}$.\n\nBy the given symmetry, the immediate basin of attraction of $0$ is $(-\\beta,\\beta)$ with boundary points $\\pm\\beta$ that do not converge to any root and form a periodic orbit. Because $N$ is odd, the only possible nontrivial periodic orbit consisting of the two boundary points is a $2$-cycle with\n$$\nN(\\beta)=-\\beta\\quad\\text{and}\\quad N(-\\beta)=\\beta.\n$$\nImposing $N(\\beta)=-\\beta$ with $\\beta>0$ and $\\beta\\neq 0$ gives\n$$\n\\frac{2\\beta^{3}}{3\\beta^{2}-1}=-\\beta.\n$$\nMultiplying by $3\\beta^{2}-1$ (which is nonzero at the solution we find) and simplifying,\n$$\n2\\beta^{3}=-\\beta(3\\beta^{2}-1)\\quad\\Longrightarrow\\quad 2\\beta^{3}=-3\\beta^{3}+\\beta,\n$$\n$$\n5\\beta^{3}-\\beta=0\\quad\\Longrightarrow\\quad \\beta\\left(5\\beta^{2}-1\\right)=0.\n$$\nExcluding $\\beta=0$, we obtain\n$$\n\\beta^{2}=\\frac{1}{5}\\quad\\Longrightarrow\\quad \\beta=\\frac{1}{\\sqrt{5}}.\n$$\nAt this $\\beta$, $3\\beta^{2}-1=\\frac{3}{5}-1=-\\frac{2}{5}\\neq 0$, so the computation is valid. Moreover, the cycle is repelling since\n$$\nN'(x)=\\frac{6x^{2}(x^{2}-1)}{(3x^{2}-1)^{2}},\\quad N'(\\beta)=\\frac{6\\cdot\\frac{1}{5}\\left(\\frac{1}{5}-1\\right)}{\\left(3\\cdot\\frac{1}{5}-1\\right)^{2}}=-6,\n$$\nso the boundary points do not converge to any root, as required. Therefore, the exact positive boundary value is $\\beta=\\frac{1}{\\sqrt{5}}$.", "answer": "$$\\boxed{\\frac{1}{\\sqrt{5}}}$$", "id": "1677773"}, {"introduction": "This practice moves from theory to implementation, challenging you to build and contrast two fundamental open root-finding algorithms: Newton's method and the secant method. You will apply these methods to find the roots of high-degree Chebyshev polynomials, a non-trivial task that serves as a rigorous test of your implementation's accuracy and stability [@problem_id:2422749]. By comparing your numerical results against the known analytical solutions, you will gain practical experience in evaluating the performance and potential pitfalls of these powerful techniques.", "problem": "You are asked to design and implement a complete, runnable program that numerically computes the roots of the Chebyshev polynomial of the first kind and quantifies their agreement with the known analytical result. Your implementation must use open methods for root finding (for example, Newton’s method and the secant method), and must not rely on bracketing methods. All angles, when used, must be expressed in radians.\n\nStarting point and fundamental base: Use the following fundamental facts and definitions without providing any shortcut formulas in the problem statement. The Chebyshev polynomials of the first kind, denoted by $T_n(x)$, form a sequence of orthogonal polynomials on the interval $[-1,1]$ with respect to the weight $(1-x^2)^{-1/2}$. They satisfy the three-term recurrence relation\n$$\nT_0(x)=1,\\quad T_1(x)=x,\\quad T_{k+1}(x)=2x\\,T_k(x)-T_{k-1}(x),\n$$\nand the angle-doubling identity for cosine implies the representation\n$$\nT_n(\\cos\\theta)=\\cos(n\\theta),\n$$\nfor any real $\\theta$. The derivative is related to the Chebyshev polynomials of the second kind, $U_n(x)$, by\n$$\n\\frac{d}{dx}T_n(x)=n\\,U_{n-1}(x),\n$$\nwhere $U_n(x)$ obeys the recurrence\n$$\nU_0(x)=1,\\quad U_1(x)=2x,\\quad U_{k+1}(x)=2x\\,U_k(x)-U_{k-1}(x).\n$$\nFrom these bases and the fact that $T_n(\\cos\\theta)=\\cos(n\\theta)$, the analytical roots of $T_n(x)$ in $(-1,1)$ are known in closed form; you must use this well-tested fact to construct reference values for comparison, with all angles expressed in radians.\n\nOpen methods to implement: Implement at least two open methods for root finding for a scalar function $f(x)$: Newton’s method given by the iteration\n$$\nx_{m+1}=x_m-\\frac{f(x_m)}{f'(x_m)},\n$$\nand the secant method given by the iteration\n$$\nx_{m+1}=x_m - f(x_m)\\,\\frac{x_m-x_{m-1}}{f(x_m)-f(x_{m-1})}.\n$$\nThese formulas must be derived in your solution from first principles, not stated as mere recipes.\n\nNumerical task: For each specified test case, do the following.\n- Define $f(x)=T_n(x)$ for the given degree $n$ using the stable three-term recurrence for $T_n(x)$.\n- For Newton’s method, define $f'(x)=n\\,U_{n-1}(x)$ using the stable three-term recurrence for $U_{n-1}(x)$.\n- Generate initial guesses for each of the $n$ distinct roots in $(-1,1)$. You may use the known exact expression for the $n$ roots as initial seeds, possibly with small perturbations. For the secant method, provide two initial values per root.\n- Apply the chosen open method to converge to each root. Use a stopping criterion based on either $|f(x)|\\le \\varepsilon$ or $|x_{m+1}-x_m|\\le \\varepsilon$ where $\\varepsilon$ is a prescribed tolerance.\n- Compute the analytical roots for comparison using the known exact expression (angles in radians), and sort both the numerical and analytical roots in ascending order.\n- For each test case, report a single real number equal to the maximum absolute difference between corresponding numerical and analytical roots across all $n$ roots.\n\nTest suite: Your program must run the following four test cases without any external input, producing results in double precision.\n- Case A (happy path, smallest nontrivial degree): $n=1$, method = Newton, tolerance $\\varepsilon=10^{-13}$.\n- Case B (moderate degree): $n=5$, method = Newton, tolerance $\\varepsilon=10^{-13}$.\n- Case C (higher degree, clustering near endpoints): $n=32$, method = Newton, tolerance $\\varepsilon=10^{-13}$.\n- Case D (higher degree using a different open method): $n=50$, method = secant, tolerance $\\varepsilon=10^{-13}$.\n\nAngle unit requirement: Whenever you compute analytical roots from the closed-form expression, treat all angles in radians.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[r_A,r_B,r_C,r_D]\"). Each entry must be a floating-point number equal to the maximum absolute difference for the corresponding test case in the order A, B, C, D.\n\nAll computations must be performed in dimensionless units. No user interaction is allowed; the program must be completely self-contained and deterministic.", "solution": "The problem as stated is subjected to validation and is found to be scientifically sound, well-posed, and objective. It contains no logical contradictions, provides all necessary data for a unique solution, and falls squarely within the domain of computational physics and numerical analysis. Therefore, I will proceed with a complete solution. The task is to compute the roots of Chebyshev polynomials of the first kind using specified open methods and to quantify the accuracy of these numerical results against the known analytical solution.\n\nFirst, the required iterative formulas for the open root-finding methods must be derived from fundamental principles.\n\nDerivation of Newton's Method\n\nLet $x^{*}$ be a root of a differentiable function $f(x)$, such that $f(x^{*}) = 0$. Let $x_m$ be a current approximation of the root $x^{*}$. We seek a better approximation, $x_{m+1}$. We can express $f(x)$ using a first-order Taylor series expansion around the point $x_m$:\n$$\nf(x) \\approx f(x_m) + f'(x_m)(x - x_m)\n$$\nWe are looking for the value of $x$ for which $f(x) = 0$. Let this value be our next approximation, $x_{m+1}$. By substituting $x = x_{m+1}$ into the Taylor expansion and setting it to zero, we obtain:\n$$\n0 \\approx f(x_m) + f'(x_m)(x_{m+1} - x_m)\n$$\nAssuming that the derivative $f'(x_m)$ is non-zero, one can solve for $x_{m+1}$:\n$$\nf'(x_m)(x_{m+1} - x_m) = -f(x_m)\n$$\n$$\nx_{m+1} = x_m - \\frac{f(x_m)}{f'(x_m)}\n$$\nThis is the well-known iteration formula for Newton's method. Its convergence is typically quadratic, provided the initial guess $x_0$ is sufficiently close to the root and the root is simple.\n\nDerivation of the Secant Method\n\nA practical limitation of Newton's method is the requirement for an analytical expression for the derivative, $f'(x)$. When the derivative is unavailable or expensive to compute, it can be approximated. The secant method replaces the derivative with a finite difference approximation based on the two most recent iterates, $x_m$ and $x_{m-1}$. The derivative $f'(x_m)$ is approximated by the slope of the secant line connecting the points $(x_{m-1}, f(x_{m-1}))$ and $(x_m, f(x_m))$:\n$$\nf'(x_m) \\approx \\frac{f(x_m) - f(x_{m-1})}{x_m - x_{m-1}}\n$$\nSubstituting this approximation into the Newton's method formula yields the secant method iteration:\n$$\nx_{m+1} = x_m - \\frac{f(x_m)}{\\left(\\frac{f(x_m) - f(x_{m-1})}{x_m - x_{m-1}}\\right)} = x_m - f(x_m)\\frac{x_m - x_{m-1}}{f(x_m) - f(x_{m-1})}\n$$\nThis method requires two initial guesses, $x_0$ and $x_1$, and exhibits superlinear convergence, which is slower than Newton's method but faster than linear.\n\nApplication to Chebyshev Polynomials\n\nThe function for which we must find the roots is the Chebyshev polynomial of the first kind, $T_n(x)$.\n\nAnalytical Roots of $T_n(x)$\nThe problem provides the identity $T_n(\\cos\\theta) = \\cos(n\\theta)$. We seek roots $x$ in the interval $(-1, 1)$ where $T_n(x) = 0$. By setting $x = \\cos\\theta$ for $\\theta \\in (0, \\pi)$, we transform the problem into finding $\\theta$ such that:\n$$\n\\cos(n\\theta) = 0\n$$\nThe general solution for this trigonometric equation is $n\\theta = (k + \\frac{1}{2})\\pi$, where $k$ is an integer. All angles must be in radians, as specified. This gives:\n$$\n\\theta_k = \\frac{(k + \\frac{1}{2})\\pi}{n} = \\frac{(2k+1)\\pi}{2n}\n$$\nTo obtain $n$ distinct roots in the interval $x \\in (-1, 1)$, we need $n$ distinct angles $\\theta_k$ in the interval $(0, \\pi)$. Choosing $k = 0, 1, \\dots, n-1$ yields $n$ unique angles in this range. The corresponding roots $x_k$ are:\n$$\nx_k = \\cos\\left(\\frac{(2k+1)\\pi}{2n}\\right), \\quad k = 0, 1, \\dots, n-1\n$$\nThese are the $n$ analytical roots of $T_n(x)$ and will serve as the reference values for our accuracy assessment.\n\nNumerical Evaluation of $T_n(x)$ and its Derivative\nTo implement the root-finding methods, we need to evaluate $T_n(x)$ and, for Newton's method, its derivative.\nThe function $f(x) = T_n(x)$ is evaluated using the provided three-term recurrence relation:\n$$\nT_0(x)=1, \\quad T_1(x)=x, \\quad T_{k+1}(x)=2x\\,T_k(x)-T_{k-1}(x)\n$$\nThis recurrence is numerically stable for $x \\in [-1, 1]$.\nFor Newton's method, the derivative $f'(x)$ is given by $\\frac{d}{dx}T_n(x) = n\\,U_{n-1}(x)$, where $U_{n-1}(x)$ is the Chebyshev polynomial of the second kind of degree $n-1$. It is evaluated using its own three-term recurrence:\n$$\nU_0(x)=1, \\quad U_1(x)=2x, \\quad U_{k+1}(x)=2x\\,U_k(x)-U_{k-1}(x)\n$$\nBoth polynomials will be implemented via iterative algorithms rather than recursion for efficiency.\n\nComputational Procedure\nFor each test case specified by a degree $n$, a method, and a tolerance $\\varepsilon$, the following steps are executed:\n$1$. The $n$ analytical roots, $x_k^{\\text{exact}}$, are calculated using the formula derived above. These are sorted in ascending order to serve as a reference.\n$2$. For each of the $n$ roots, a numerical search is initiated.\n    - An initial guess (or guesses) is generated from the corresponding analytical root. This ensures that each search converges to a unique, pre-identified root, allowing for a direct comparison of accuracy.\n    - For Newton's method, the initial guess is set to $x_0 = x_k^{\\text{exact}}$.\n    - For the secant method, two initial guesses are required. We use $x_1 = x_k^{\\text{exact}}$ and a slightly perturbed point $x_0 = x_1 - \\delta$, where $\\delta$ is a small constant (e.g., $\\delta = 10^{-8}$).\n$3$. The chosen iterative method (Newton or secant) is applied until a stopping criterion is met. The criteria are $|f(x_m)| \\le \\varepsilon$ or, as a safeguard, $|x_{m+1} - x_m| \\le \\varepsilon$. A maximum number of iterations is also imposed to prevent infinite loops in cases of non-convergence.\n$4$. The numerically computed roots are collected and sorted in ascending order.\n$5$. The maximum absolute difference between the sorted numerical roots and the sorted analytical roots is computed: $\\max_{k} |x_k^{\\text{numerical}} - x_k^{\\text{exact}}|$. This single value quantifies the accuracy of the method for the given test case.\n\nThis procedure is repeated for all four test cases:\n- Case A: $n=1$, Newton's method, $\\varepsilon=10^{-13}$.\n- Case B: $n=5$, Newton's method, $\\varepsilon=10^{-13}$.\n- Case C: $n=32$, Newton's method, $\\varepsilon=10^{-13}$.\n- Case D: $n=50$, secant method, $\\varepsilon=10^{-13}$.\n\nThe final output will be a list containing the four maximum absolute difference values, one for each case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# No scipy is needed as the methods are implemented from first principles.\n\ndef solve():\n    \"\"\"\n    Main solver function that orchestrates the entire process.\n    It runs the specified test cases and prints the final result.\n    \"\"\"\n\n    def T(n, x):\n        \"\"\"\n        Computes the Chebyshev polynomial of the first kind, T_n(x),\n        using the three-term recurrence relation.\n        \"\"\"\n        if n == 0:\n            return 1.0\n        if n == 1:\n            return x\n        \n        T_k_minus_1 = 1.0  # T_0\n        T_k = x            # T_1\n        for _ in range(2, n + 1):\n            T_k_plus_1 = 2.0 * x * T_k - T_k_minus_1\n            T_k_minus_1 = T_k\n            T_k = T_k_plus_1\n        return T_k\n\n    def U(n, x):\n        \"\"\"\n        Computes the Chebyshev polynomial of the second kind, U_n(x),\n        using the three-term recurrence relation.\n        \"\"\"\n        if n == 0:\n            return 1.0\n        if n == 1:\n            return 2.0 * x\n            \n        U_k_minus_1 = 1.0  # U_0\n        U_k = 2.0 * x      # U_1\n        for _ in range(2, n + 1):\n            U_k_plus_1 = 2.0 * x * U_k - U_k_minus_1\n            U_k_minus_1 = U_k\n            U_k = U_k_plus_1\n        return U_k\n\n    def newton_method(f, df, x0, tol, max_iter=50):\n        \"\"\"\n        Finds a root of f(x) using Newton's method.\n        f: function\n        df: derivative of the function\n        x0: initial guess\n        tol: tolerance for stopping\n        max_iter: maximum number of iterations\n        \"\"\"\n        x = x0\n        for _ in range(max_iter):\n            fx = f(x)\n            if abs(fx) <= tol:\n                return x\n            \n            dfx = df(x)\n            if dfx == 0:\n                # Derivative is zero, method fails. Unlikely for this problem.\n                break\n                \n            x_next = x - fx / dfx\n            if abs(x_next - x) <= tol:\n                return x_next\n            x = x_next\n        return x\n\n    def secant_method(f, x0, x1, tol, max_iter=50):\n        \"\"\"\n        Finds a root of f(x) using the secant method.\n        f: function\n        x0, x1: two initial guesses\n        tol: tolerance for stopping\n        max_iter: maximum number of iterations\n        \"\"\"\n        fx0 = f(x0)\n        fx1 = f(x1)\n        for _ in range(max_iter):\n            if abs(fx1) <= tol:\n                return x1\n            \n            # Avoid division by zero\n            if (fx1 - fx0) == 0:\n                break\n            \n            x_next = x1 - fx1 * (x1 - x0) / (fx1 - fx0)\n            \n            if abs(x_next - x1) <= tol:\n                return x_next\n            \n            x0, x1 = x1, x_next\n            fx0, fx1 = fx1, f(x1)\n        return x1\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'n': 1, 'method': 'newton', 'tol': 1e-13},  # Case A\n        {'n': 5, 'method': 'newton', 'tol': 1e-13},  # Case B\n        {'n': 32, 'method': 'newton', 'tol': 1e-13}, # Case C\n        {'n': 50, 'method': 'secant', 'tol': 1e-13}, # Case D\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        n = case['n']\n        method = case['method']\n        tol = case['tol']\n\n        # 1. Compute analytical roots for reference and seeding\n        # Formula: x_k = cos((2k+1)pi / (2n)) for k=0, ..., n-1\n        k = np.arange(n)\n        analytical_roots = np.cos((2 * k + 1) * np.pi / (2 * n))\n        analytical_roots.sort() # Ensure ascending order\n\n        numerical_roots = []\n\n        # 2. Find roots numerically\n        for i in range(n):\n            initial_seed = analytical_roots[i]\n            \n            if method == 'newton':\n                # Define function f(x) = T_n(x) and its derivative f'(x) = n*U_{n-1}(x)\n                f = lambda x: T(n, x)\n                df = lambda x: n * U(n - 1, x) if n > 0 else 0.0\n                root = newton_method(f, df, initial_seed, tol)\n                numerical_roots.append(root)\n            \n            elif method == 'secant':\n                f = lambda x: T(n, x)\n                # Two initial guesses: the analytical root and a slightly perturbed version\n                x1 = initial_seed\n                x0 = initial_seed - 1e-8 # Small perturbation\n                root = secant_method(f, x0, x1, tol)\n                numerical_roots.append(root)\n        \n        numerical_roots = np.array(numerical_roots)\n        numerical_roots.sort()\n\n        # 3. Compute the maximum absolute difference\n        max_abs_diff = np.max(np.abs(numerical_roots - analytical_roots))\n        results.append(max_abs_diff)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.15e}' for r in results)}]\")\n\nsolve()\n```", "id": "2422749"}, {"introduction": "Real-world root-finding routines must be both fast and reliable. This advanced practice guides you in creating a \"hybrid\" algorithm that achieves this balance by combining the strengths of different methods. You will design a solver that intelligently switches between the safety of bisection and the speed of the secant and Newton's methods, based on how close the iteration is to the final solution [@problem_id:2402195]. This exercise synthesizes the concepts of convergence and stability into a practical, robust tool, mirroring the design of professional numerical libraries.", "problem": "Design and implement a ternary hybrid root-finding algorithm that combines interval-halving, slope-based extrapolation, and linearized local correction to solve scalar nonlinear equations. The goal is to compute an approximation to a real root of a continuous function $f(x)$ within a given bracketing interval $[a,b]$ where $f(a)\\,f(b)\\le 0$, using a principled switching scheme between three phases: a conservative interval-halving phase when far from the root, a slope-based approach phase when reasonably close, and a final polishing phase based on a linearized local model. All function arguments that involve trigonometric functions must be expressed in radians.\n\nStarting from the fundamental base below, derive and justify a robust switching strategy, specify acceptance and fallback conditions needed to retain safety, and implement a complete program that executes this strategy on a provided test suite.\n\nFundamental base:\n- Definition of a root: a point $x^\\ast$ such that $f(x^\\ast)=0$.\n- Continuity and the Intermediate Value Theorem (IVT): if $f$ is continuous on $[a,b]$ with $f(a)\\,f(b)\\le 0$, then there exists at least one root in $[a,b]$.\n- First-order Taylor expansion for differentiable functions: for a differentiable $f$ at $x_k$, $f(x)\\approx f(x_k)+f'(x_k)\\,(x-x_k)$ for $x$ near $x_k$.\n- Finite-difference approximation of a derivative over an interval: $f'(x)\\approx \\dfrac{f(x_2)-f(x_1)}{x_2-x_1}$ for distinct $x_1$ and $x_2$.\n\nRequirements for the ternary hybrid switching scheme:\n- Bracketing invariant: maintain $[a,b]$ with $f(a)\\,f(b)\\le 0$ at all times.\n- Phase I (interval halving): when the current interval length $L=b-a$ is large relative to the initial length $L_0=b_0-a_0$, choose the midpoint candidate $m=(a+b)/2$. This phase should be used when $L>\\tau_{\\text{far}}\\,L_0$.\n- Phase II (slope-based approach): when the interval is moderately small but not tiny, use a linear model constructed from the current bracketing data to propose a candidate point strictly inside $[a,b]$. This phase should be used when $\\tau_{\\text{near}}\\,L_0 < L \\le \\tau_{\\text{far}}\\,L_0$.\n- Phase III (local polishing): when the interval is very small, use a first-order local linearization around a current interior point to propose a refinement step. Accept such a step only if it remains inside $[a,b]$ and the local slope magnitude is not too small; otherwise, fall back to a safer Phase II or Phase I step.\n- Acceptance and fallback policy: any candidate outside $[a,b]$ must be rejected; if the local slope magnitude is below a specified threshold, avoid the local linearization step; if the linear-model step is not admissible, revert to interval halving.\n- Bracket update: after computing a candidate $x_c$, evaluate $f(x_c)$. If $f(a)\\,f(x_c)\\le 0$, set $b\\leftarrow x_c$; otherwise set $a\\leftarrow x_c$. This preserves the bracketing invariant.\n\nStopping criteria:\n- Stop if either the absolute residual at the current midpoint $|f(m)|$ is at most $f_{\\text{tol}}$ or the interval length $L=b-a$ is at most $x_{\\text{tol}}$.\n- Use $x_{\\text{tol}}=10^{-12}$ and $f_{\\text{tol}}=10^{-12}$.\n- Use at most $N_{\\max}=200$ iterations.\n\nSwitch thresholds and safeguards:\n- Use $\\tau_{\\text{far}}=0.25$ and $\\tau_{\\text{near}}=10^{-3}$.\n- For the local linearization, require $|f'(x)|\\ge \\delta_{\\min}$ with $\\delta_{\\min}=10^{-14}$ to attempt a polishing step.\n\nYour program must implement the above algorithm and apply it to the following test suite of functions and brackets. All angles must be in radians. Each function in the test suite is continuously differentiable on the specified interval. Provide analytic first derivatives $f'(x)$ in your implementation.\n\nTest suite:\n- Case $1$: $f_1(x)=x^3-x-2$ on $[1,2]$, with $f_1'(x)=3x^2-1$.\n- Case $2$: $f_2(x)=\\cos(x)-x$ on $[0,1]$, with $f_2'(x)=-\\sin(x)-1$.\n- Case $3$: $f_3(x)=e^{x}-3x$ on $[1,2]$, with $f_3'(x)=e^{x}-3$.\n- Case $4$: $f_4(x)=x-\\tanh(x)$ on $[-2,2]$, with $f_4'(x)=1-\\operatorname{sech}^2(x)=1-\\dfrac{1}{\\cosh^2(x)}$.\n\nFinal output specification:\n- For each case, output the approximated root as a floating-point number rounded to exactly $10$ decimal places.\n- Aggregate the four results in order as a single Python-style list literal with no embedded spaces, i.e., the single line should be of the form $[r_1,r_2,r_3,r_4]$, where each $r_i$ has exactly $10$ digits after the decimal point.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3,r_4]$). No user input is allowed; all constants and functions must be defined within the program. The answer has no physical units and no percentages are involved. Angles in trigonometric functions must be in radians.", "solution": "The posed problem requires the design and implementation of a ternary hybrid algorithm for finding a root of a scalar nonlinear function $f(x)$ within a specified bracketing interval $[a, b]$. A critical validation of the problem statement confirms its scientific and mathematical soundness. It is based on established principles of numerical analysis—the Intermediate Value Theorem, Taylor's theorem, and iterative methods—and provides a complete, consistent, and unambiguous set of requirements. Therefore, the problem is deemed valid, and we proceed with the derivation and implementation of the required algorithm.\n\nThe core principle of a robust root-finding algorithm is to combine the guaranteed convergence of a safe method, such as interval halving (bisection), with the rapid convergence of faster, open methods like the Secant or Newton-Raphson methods. The proposed hybrid strategy formalizes this by partitioning the solution process into three phases based on the relative width of the current search interval.\n\n**1. Bracketing Invariant and Fundamental Principle**\n\nThe algorithm's foundation is the maintenance of a bracketing interval $[a, b]$ such that the root $x^\\ast$ is always contained within it. This is ensured by the Intermediate Value Theorem, which states that for a continuous function $f$, if $f(a)$ and $f(b)$ have opposite signs, i.e., $f(a)f(b) \\le 0$, there must exist at least one root $x^\\ast \\in [a, b]$.\n\nAt each iteration, we propose a candidate root $x_c \\in (a, b)$. We then evaluate $f(x_c)$ and update the bracket to maintain the invariant:\n- If $f(a)f(x_c) \\le 0$, the root must be in $[a, x_c]$, so we set the new interval to $[a, x_c]$ by updating $b \\leftarrow x_c$.\n- Otherwise, it must be that $f(x_c)f(b) \\le 0$, so the root is in $[x_c, b]$, and we update $a \\leftarrow x_c$.\nThis procedure guarantees that the interval $[a, b]$ always brackets a root and its length, $L = b-a$, systematically decreases.\n\n**2. The Ternary Hybrid Switching Scheme**\n\nThe choice of the candidate point $x_c$ depends on the current interval length $L$ relative to the initial length $L_0 = b_0 - a_0$. This defines the three-phase strategy.\n\n**Phase I: Interval Halving (Bisection Method)**\n\nThis is the most conservative and globally convergent phase. The candidate point is simply the midpoint of the interval:\n$$\nx_c = m = \\frac{a+b}{2}\n$$\nThe bisection method exhibits linear convergence, with the interval length being halved at each step. It is unconditionally safe as long as the bracketing invariant is maintained. Due to its slow convergence, it is most appropriate when the interval is large and we are far from the root.\n**Activation Condition:** This phase is used when the interval is large, $L > \\tau_{\\text{far}} L_0$, or as a fallback if the more aggressive methods fail. The given threshold is $\\tau_{\\text{far}} = 0.25$.\n\n**Phase II: Slope-Based Extrapolation (Secant Method)**\n\nWhen the interval has been reduced sufficiently, we can accelerate convergence by using a linear model of the function based on the two endpoints $(a, f(a))$ and $(b, f(b))$. The root of the line (the secant) passing through these points gives the next candidate:\n$$\nx_c = b - f(b) \\frac{b-a}{f(b)-f(a)} = \\frac{a f(b) - b f(a)}{f(b) - f(a)}\n$$\nThe Secant method has a superlinear convergence rate of order $\\phi \\approx 1.618$.\n**Activation Condition:** This phase is used for moderately small intervals, where $\\tau_{\\text{near}} L_0 < L \\le \\tau_{\\text{far}} L_0$, with thresholds $\\tau_{\\text{near}} = 10^{-3}$ and $\\tau_{\\text{far}} = 0.25$.\n**Safeguards:** A Secant step is accepted only if it is \"admissible,\" meaning the proposed candidate $x_c$ lies strictly within the current bracket, i.e., $x_c \\in (a, b)$. If $x_c$ falls outside this range, the step is rejected, and the algorithm must fall back to the safer bisection method.\n\n**Phase III: Local Linearization (Newton-Raphson Method)**\n\nWhen the interval is very small, we assume we are close enough to the root for a local model based on Taylor's theorem to be highly accurate. The first-order Taylor expansion of $f(x)$ around a point $x_k$ is:\n$$\nf(x) \\approx f(x_k) + f'(x_k)(x-x_k)\n$$\nSetting $f(x)=0$ and solving for $x$ yields the Newton-Raphson update rule:\n$$\nx_{k+1} = x_k - \\frac{f(x_k)}{f'(x_k)}\n$$\nThis method offers quadratic convergence for simple roots. For our algorithm, we select the current interval midpoint $m=(a+b)/2$ as the point of linearization, $x_k=m$. The candidate is:\n$$\nx_c = m - \\frac{f(m)}{f'(m)}\n$$\n**Activation Condition:** This polishing phase is activated for very small intervals, $L \\le \\tau_{\\text{near}} L_0$.\n**Safeguards:** The Newton-Raphson step is powerful but can be unstable. Two critical safeguards are required:\n1.  **Admissibility:** As with the Secant method, the candidate $x_c$ must lie within the bracket $(a, b)$.\n2.  **Derivative Magnitude:** A very small derivative, $|f'(m)| \\approx 0$, indicates a tangent that is nearly horizontal, which can cause the step to be enormous and non-convergent. Therefore, we only attempt a Newton step if the derivative magnitude is above a minimum threshold: $|f'(m)| \\ge \\delta_{\\min}$, with $\\delta_{\\min}=10^{-14}$.\nIf either safeguard fails, the algorithm must fall back to a safer method—first to Phase II (Secant), and if that also fails, to Phase I (Bisection).\n\n**3. Integrated Algorithm and Fallback Logic**\n\nThe complete algorithm integrates these three phases with a clear fallback hierarchy to ensure robustness. In each iteration, after checking the stopping criteria ($L \\le x_{\\text{tol}}$ or $|f(m)| \\le f_{\\text{tol}}$), a candidate $x_c$ is determined as follows:\n\n1.  Calculate current interval length $L = b-a$.\n2.  **Attempt Phase III (Newton):** If $L \\le \\tau_{\\text{near}} L_0$, compute the derivative $f'(m)$ at the midpoint $m$. If $|f'(m)| \\ge \\delta_{\\min}$, compute the Newton candidate $x_c$. If $a < x_c < b$, accept it.\n3.  **Attempt Phase II (Secant):** If a Newton candidate was not accepted and $L \\le \\tau_{\\text{far}} L_0$, compute the Secant candidate $x_c$. To prevent numerical instability, this should only be done if the denominator $|f(b)-f(a)|$ is not excessively small. If $a < x_c < b$, accept it.\n4.  **Use Phase I (Bisection):** If neither a Newton nor a Secant step was accepted, fall back to the bisection candidate $x_c = m$. This step is always valid and ensures progress.\n\nOnce a valid $x_c$ is chosen, the bracketing interval $[a, b]$ is updated, and the process repeats until a stopping criterion is met or the maximum number of iterations, $N_{\\max}=200$, is exceeded. The final solution is taken as the midpoint of the last computed interval.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and run the hybrid root-finding algorithm.\n    The results are printed in the specified format.\n    \"\"\"\n\n    def hybrid_solver(f, df, a, b, xtol, ftol, n_max, tau_far, tau_near, delta_min):\n        \"\"\"\n        Implements the ternary hybrid root-finding algorithm.\n\n        Args:\n            f (callable): The function for which to find a root.\n            df (callable): The analytic derivative of the function f.\n            a (float): The lower bound of the bracketing interval.\n            b (float): The upper bound of the bracketing interval.\n            xtol (float): Tolerance for the interval length.\n            ftol (float): Tolerance for the function value at the midpoint.\n            n_max (int): Maximum number of iterations.\n            tau_far (float): Threshold for switching to/from bisection.\n            tau_near (float): Threshold for switching to/from Newton's method.\n            delta_min (float): Minimum slope magnitude for Newton/Secant steps.\n\n        Returns:\n            float: The approximated root.\n        \"\"\"\n        a0, b0 = float(a), float(b)\n        L0 = b0 - a0\n        \n        fa = f(a0)\n        fb = f(b0)\n\n        if fa * fb > 0:\n            # Per problem spec, this should not happen for the given test cases.\n            raise ValueError(\"Root not bracketed or function has same sign at endpoints.\")\n\n        for _ in range(n_max):\n            L = b0 - a0\n            # Use a stable midpoint calculation\n            m = a0 + (b0 - a0) / 2.0\n            fm = f(m)\n\n            # Check stopping criteria\n            if L <= xtol or abs(fm) <= ftol:\n                return m\n\n            candidate_found = False\n            xc = None\n\n            # Phase III: Newton-Raphson Method\n            if L <= tau_near * L0:\n                dfm = df(m)\n                if abs(dfm) >= delta_min:\n                    xc_newton = m - fm / dfm\n                    # Admissibility check\n                    if a0 < xc_newton < b0:\n                        xc = xc_newton\n                        candidate_found = True\n\n            # Phase II: Secant Method (also serves as fallback for Phase III)\n            if not candidate_found and L <= tau_far * L0:\n                # Denominator safety check\n                if abs(fb - fa) >= delta_min:\n                    xc_secant = (a0 * fb - b0 * fa) / (fb - fa)\n                    # Admissibility check\n                    if a0 < xc_secant < b0:\n                        xc = xc_secant\n                        candidate_found = True\n\n            # Phase I: Bisection Method (ultimate fallback)\n            if not candidate_found:\n                xc = m\n            \n            fc = f(xc)\n\n            # Update bracket\n            if fa * fc <= 0:\n                b0 = xc\n                fb = fc\n            else:\n                a0 = xc\n                fa = fc\n        \n        # Return best guess if max iterations reached\n        return (a0 + b0) / 2.0\n\n    # Test Suite Definition\n    test_cases = [\n        {'func': lambda x: x**3 - x - 2, 'dfunc': lambda x: 3*x**2 - 1, 'bracket': (1, 2)},\n        {'func': lambda x: np.cos(x) - x, 'dfunc': lambda x: -np.sin(x) - 1, 'bracket': (0, 1)},\n        {'func': lambda x: np.exp(x) - 3*x, 'dfunc': lambda x: np.exp(x) - 3, 'bracket': (1, 2)},\n        {'func': lambda x: x - np.tanh(x), 'dfunc': lambda x: 1 - (1/np.cosh(x))**2, 'bracket': (-2, 2)}\n    ]\n\n    # Algorithm Parameters\n    params = {\n        'xtol': 1e-12,\n        'ftol': 1e-12,\n        'n_max': 200,\n        'tau_far': 0.25,\n        'tau_near': 1e-3,\n        'delta_min': 1e-14\n    }\n\n    results = []\n    for case in test_cases:\n        root = hybrid_solver(\n            f=case['func'],\n            df=case['dfunc'],\n            a=case['bracket'][0],\n            b=case['bracket'][1],\n            **params\n        )\n        results.append(f\"{root:.10f}\")\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2402195"}]}