{"hands_on_practices": [{"introduction": "The Gaussian probability density function is a cornerstone of statistics and is found everywhere in the physical sciences, from describing measurement errors to modeling the distribution of particle velocities. While its formula appears simple, a direct implementation in code is fragile and can easily fail due to numerical overflow or underflow when dealing with extreme values. This first practice challenges you to engineer a robust Gaussian function by converting the calculation into logarithmic space, a fundamental technique for extending the dynamic range of numerical computations. [@problem_id:2423348]", "problem": "You are asked to implement a numerically robust program that evaluates the Gaussian (normal) probability density function for given parameters while handling extreme cases that would otherwise cause numerical underflow or overflow in finite-precision arithmetic.\n\nLet the Gaussian probability density function be defined for real $x$, real $\\mu$, and strictly positive $\\sigma$ by\n$$\n\\operatorname{pdf}(x,\\mu,\\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\!\\left(-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^{2}\\right).\n$$\nAll computations must assume the standard double-precision floating-point format as specified by the Institute of Electrical and Electronics Engineers (IEEE) $754$. You must ensure the following output policy for representability:\n- If the exact mathematical value of $\\operatorname{pdf}(x,\\mu,\\sigma)$ is strictly smaller than the smallest strictly positive representable double-precision number $2^{-1074}$, your program must output the floating-point value $0.0$ for that test case.\n- If the exact mathematical value of $\\operatorname{pdf}(x,\\mu,\\sigma)$ is strictly larger than the largest finite representable double-precision number $(2-2^{-52})\\times 2^{1023}$, your program must output the floating-point value corresponding to positive infinity for that test case.\n- Otherwise, your program must output the finite floating-point value of $\\operatorname{pdf}(x,\\mu,\\sigma)$ in standard double precision.\n\nNo physical units are involved in this problem. Angles are not used.\n\nTest suite and required output format:\n- Evaluate $\\operatorname{pdf}(x,\\mu,\\sigma)$ for the following ordered list of parameter triples $(x,\\mu,\\sigma)$:\n  1. $(0,0,1)$,\n  2. $(8,0,1)$,\n  3. $(38,0,1)$,\n  4. $(40,0,1)$,\n  5. $(0,0,10^{-320})$,\n  6. $(0,0,10^{50})$,\n  7. $\\left(-10^{308},\\,10^{308},\\,1\\right)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the same order as the test suite, for example, $[r_1,r_2,\\dots,r_7]$ where each $r_k$ is a floating-point number consistent with the representability policy above. Do not print any additional characters or lines.", "solution": "The problem as stated is to implement a numerically robust evaluation of the Gaussian probability density function, $\\operatorname{pdf}(x,\\mu,\\sigma)$, given by the formula\n$$\n\\operatorname{pdf}(x,\\mu,\\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\!\\left(-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^{2}\\right),\n$$\nfor real-valued parameters $x$ and $\\mu$, and a strictly positive real parameter $\\sigma$. The implementation must correctly handle cases that would lead to numerical overflow or underflow in a naive direct computation using standard double-precision floating-point arithmetic.\n\nThe problem is scientifically grounded, well-posed, objective, and contains all necessary information for a unique solution. It is a classic problem in computational science demonstrating essential techniques for maintaining numerical stability. Therefore, the problem is valid, and we proceed to its solution.\n\nThe fundamental principle for developing a robust algorithm is to avoid operations that can catastrophically fail at the limits of the floating-point number range. A naive implementation of the formula involves several such potential failures:\n$1$. If $\\sigma$ is very small (approaching zero), the pre-factor $\\frac{1}{\\sigma\\sqrt{2\\pi}}$ can overflow.\n$2$. If $\\sigma$ is very large, the pre-factor can underflow to $0.0$, losing all precision.\n$3$. The standardized variable $z = \\frac{x-\\mu}{\\sigma}$ can itself overflow if $|x-\\mu|$ is large and $\\sigma$ is small.\n$4$. If $|z|$ is large, the term $z^2$ can overflow, even if $z$ itself is representable.\n$5$. If $|z|$ is large, the exponential term $\\exp(-0.5 z^2)$ can underflow to $0.0$.\n\nTo mitigate these issues, we reformulate the calculation in logarithmic space. This is a standard and powerful technique in numerical methods. Instead of computing the $\\operatorname{pdf}$ directly, we first compute its natural logarithm, which we denote as $L$:\n$$\nL = \\log(\\operatorname{pdf}(x,\\mu,\\sigma)) = \\log\\left(\\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\!\\left(-\\frac{1}{2}z^2\\right)\\right)\n$$\nUsing the properties of logarithms, this expression simplifies to a sum of terms:\n$$\nL = \\log\\left(\\frac{1}{\\sigma\\sqrt{2\\pi}}\\right) + \\log\\left(\\exp\\!\\left(-\\frac{1}{2}z^2\\right)\\right) \\\\\nL = -\\log(\\sigma\\sqrt{2\\pi}) - \\frac{1}{2}z^2 \\\\\nL = -\\log(\\sigma) - \\frac{1}{2}\\log(2\\pi) - \\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2\n$$\nThis formulation replaces the multiplication of the pre-factor and the exponential term with the addition of their logarithms. Addition is far less prone to spurious overflow and underflow than multiplication, thereby extending the dynamic range of inputs for which a meaningful result can be computed.\n\nThe algorithmic procedure is as follows:\n$1$. For a given set of parameters $(x, \\mu, \\sigma)$, we first calculate the terms of the log-PDF, $L$. The constant term $\\frac{1}{2}\\log(2\\pi)$ can be pre-computed.\n$2$. We calculate $\\log(\\sigma)$ and the standardized variable $z = \\frac{x-\\mu}{\\sigma}$. It is critical to note that for extreme values, such as test case $7$, the intermediate calculation of $x-\\mu$ may overflow. However, IEEE $754$ arithmetic, as implemented in `numpy`, correctly handles this by propagating an infinity (`inf`). For instance, if $x-\\mu$ overflows to $-\\infty$, then $z$ becomes $-\\infty$, $z^2$ becomes $+\\infty$, the exponent term $-0.5z^2$ becomes $-\\infty$, and the total log-PDF $L$ becomes $-\\infty$. This leads to a final result of $\\exp(-\\infty) = 0.0$, which is the mathematically correct limit. Thus, a special check for the overflow of $z^2$ is not required, as the standard `inf` propagation handles it correctly.\n$3$. We then compute the total log-PDF: $L = (-\\log(\\sigma) - \\frac{1}{2}\\log(2\\pi)) - \\frac{1}{2}z^2$.\n$4$. Before computing the final result by exponentiation, we must compare $L$ to the specified thresholds, also transformed into log-space. The problem defines overflow and underflow bounds based on the limitations of double-precision numbers.\n    - **Overflow condition**: The result overflows if it is strictly larger than the largest finite double-precision number, $D_{\\text{max}} = (2-2^{-52})\\times 2^{1023}$. This is equivalent to checking if $L > \\log(D_{\\text{max}})$. If this condition is met, the function must return positive infinity.\n    - **Underflow condition**: The result underflows if it is strictly smaller than the smallest positive (subnormal) double-precision number, $D_{\\text{min\\_sub}} = 2^{-1074}$. This is equivalent to checking if $L  \\log(D_{\\text{min\\_sub}}) = -1074\\log(2)$. If this condition is met, the function must return $0.0$.\n$5$. If $L$ falls between these two logarithmic thresholds, the result is numerically representable and stable to compute. We then calculate the final PDF value as $\\exp(L)$.\n\nThis design, based on the principle of logarithmic computation, ensures that the implementation is robust, accurate, and adheres to the specified policies for handling extreme numerical values. It correctly processes all test cases, from standard inputs to those designed to cause overflow or underflow in naive implementations.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef robust_gaussian_pdf(x: float, mu: float, sigma: float) -> float:\n    \"\"\"\n    Computes the Gaussian probability density function in a numerically robust way.\n\n    Args:\n        x: The point at which to evaluate the PDF.\n        mu: The mean of the distribution.\n        sigma: The standard deviation of the distribution (must be > 0).\n\n    Returns:\n        The value of the PDF, handling overflow and underflow per IEEE 754 spec.\n    \"\"\"\n    # Pre-calculated constant for efficiency and clarity.\n    # log(sqrt(2*pi))\n    LOG_SQRT_2PI = 0.5 * np.log(2.0 * np.pi)\n\n    # Thresholds for overflow and underflow in log-space, as per the problem.\n    # log(DBL_MAX)\n    LOG_MAX_FLOAT = np.log(np.finfo(np.double).max)\n    # log(2**-1074)\n    LOG_MIN_SUBNORMAL = -1074.0 * np.log(2.0)\n\n    # Per the problem statement, sigma is strictly positive. A check for sigma = 0\n    # would be good practice in a general-purpose library, but it is not\n    # required for the given test suite.\n    \n    # Standardized variable z. Numpy's handling of large numbers will correctly\n    # propagate 'inf' if (x - mu) overflows, which leads to the correct\n    # final result of 0.0.\n    z = (x - mu) / sigma\n    \n    # Calculate the PDF in log-space to prevent intermediate over/underflow.\n    # log(pdf) = -log(sigma) - log(sqrt(2*pi)) - 0.5 * z**2\n    log_pdf = -np.log(sigma) - LOG_SQRT_2PI - 0.5 * z**2\n    \n    # Check against the specified representability thresholds.\n    if log_pdf > LOG_MAX_FLOAT:\n        return np.inf\n    \n    if log_pdf  LOG_MIN_SUBNORMAL:\n        return 0.0\n        \n    # If within representable range, compute the final value.\n    return np.exp(log_pdf)\n\n\ndef solve():\n    \"\"\"\n    Runs the test suite and prints the results in the required format.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (0.0, 0.0, 1.0),\n        (8.0, 0.0, 1.0),\n        (38.0, 0.0, 1.0),\n        (40.0, 0.0, 1.0),\n        (0.0, 0.0, 1e-320),\n        (0.0, 0.0, 1e50),\n        (-1e308, 1e308, 1.0),\n    ]\n\n    results = []\n    for case in test_cases:\n        x_val, mu_val, sigma_val = case\n        result = robust_gaussian_pdf(x_val, mu_val, sigma_val)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # The default string conversion for floats, including 'inf', is correct here.\n    print(f\"[{','.join(map(str, results))}]\")\n\n# Execute the solution.\nsolve()\n```", "id": "2423348"}, {"introduction": "The random walk is a powerful mathematical model for a vast range of stochastic processes, from the diffusion of molecules to price fluctuations in financial markets. Calculating the probability of a walker's final position requires computing binomial coefficients, which involve factorials that grow astonishingly fast and will quickly overflow any standard floating-point data type. This exercise demonstrates how to tame these enormous numbers by using the logarithm of the Gamma function, $\\ln(\\Gamma(z))$, allowing for stable and accurate probability calculations even for walks of millions of steps. [@problem_id:2423389]", "problem": "You are to write a complete program that computes, for a one-dimensional discrete-time random walk on a lattice, the probability of being at a specified position after a given number of steps. At each time step, the position changes by $+1$ with probability $p$ and by $-1$ with probability $1-p$. Let $X_N$ denote the position after $N$ steps, starting from the origin at step $0$. For a given triple $(N,x,p)$ with $N \\in \\mathbb{Z}_{\\ge 0}$, $x \\in \\mathbb{Z}$, and $p \\in [0,1]$, compute the probability $\\mathbb{P}(X_N = x)$. Your program must robustly handle overflow and underflow that arise in the evaluation of combinatorial terms and powers, and it must return $0.0$ whenever the final probability underflows in double-precision floating-point arithmetic.\n\nThe following conditions must be enforced by first principles:\n- If $|x|  N$, then the event is impossible and the probability must be $0.0$.\n- If $N + x$ is odd, then the event is impossible and the probability must be $0.0$.\n- For the boundary cases $p = 0$ and $p = 1$, the exact probabilities are well-defined: when $p = 0$, the probability is $1.0$ if and only if $x = -N$, and is $0.0$ otherwise; when $p = 1$, the probability is $1.0$ if and only if $x = +N$, and is $0.0$ otherwise.\n\nNo physical units are involved. All answers must be returned as dimensionless real numbers. Angles are not used.\n\nYour program must evaluate the probability for each of the following test cases and aggregate the results:\n- $(N,x,p) = (\\,10,\\,2,\\,0.5\\,)$\n- $(N,x,p) = (\\,9,\\,2,\\,0.5\\,)$\n- $(N,x,p) = (\\,1000,\\,1000,\\,0.5\\,)$\n- $(N,x,p) = (\\,5000,\\,5000,\\,0.5\\,)$\n- $(N,x,p) = (\\,1000000,\\,0,\\,0.5\\,)$\n- $(N,x,p) = (\\,200000,\\,200,\\,0.501\\,)$\n- $(N,x,p) = (\\,1000,\\, -1000,\\, 1.0\\times 10^{-12}\\,)$\n- $(N,x,p) = (\\,1000,\\, 1000,\\, 1.0\\times 10^{-12}\\,)$\n- $(N,x,p) = (\\,50,\\, 50,\\, 1.0\\,)$\n\nFinal output format:\n- Your program must produce a single line containing a comma-separated list of the $9$ probabilities in the same order as above, enclosed in square brackets.\n- Each probability must be formatted in scientific notation with exactly $12$ significant digits after the decimal point, for example: $[\\,1.234000000000\\mathrm{e}{-02},\\ldots\\,]$.\n- No additional text or lines are permitted in the output.", "solution": "The problem as stated is valid. It is scientifically grounded in the principles of probability theory and statistical mechanics, specifically the binomial distribution which governs the discrete-time random walk. It is well-posed, objective, and contains all necessary information for a unique solution. The core challenge is computational, involving the handling of numerical overflow and underflow, which is a standard topic in computational physics.\n\nWe are tasked with computing the probability $\\mathbb{P}(X_N = x)$ of a one-dimensional random walk being at position $x$ after $N$ steps, starting from the origin $X_0 = 0$. At each step, the position changes by $+1$ with probability $p$ and by $-1$ with probability $q = 1-p$.\n\nLet $N_+$ be the number of steps to the right (position increases by $1$) and $N_-$ be the number of steps to the left (position decreases by $1$). The total number of steps is $N$.\n$$N_+ + N_- = N$$\nThe final position $x$ is given by the difference between the number of right and left steps.\n$$N_+ - N_- = x$$\nThis system of two linear equations can be solved for $N_+$ and $N_-$. Adding the two equations yields $2N_+ = N+x$, and subtracting the second from the first yields $2N_- = N-x$. Thus, we have:\n$$N_+ = \\frac{N+x}{2}$$\n$$N_- = \\frac{N-x}{2}$$\nFor $N_+$ and $N_-$ to be valid counts, they must be non-negative integers. This imposes two fundamental constraints:\n1.  Parity constraint: The sum $N+x$ (and consequently $N-x$) must be an even number. If $N+x$ is odd, it is impossible to reach position $x$ in $N$ steps, so the probability is $0$. This is equivalent to the condition that $N$ and $x$ must have the same parity.\n2.  Boundary constraint: The number of steps in either direction cannot be negative. This requires $N+x \\ge 0$ and $N-x \\ge 0$, which is equivalent to $|x| \\le N$. If the target position $x$ is more distant from the origin than the number of steps $N$, it is an impossible event, and the probability is $0$.\n\nIf these conditions are met, the problem is equivalent to finding the probability of obtaining exactly $N_+$ successes in $N$ Bernoulli trials, where the probability of success (a step to the right) is $p$. This is described by the binomial distribution. The number of distinct paths leading to position $x$ is the number of ways to choose $N_+$ steps to the right out of a total of $N$ steps, which is given by the binomial coefficient $\\binom{N}{N_+}$. The probability of any single such path is $p^{N_+} (1-p)^{N_-}$.\n\nTherefore, the total probability is:\n$$\\mathbb{P}(X_N = x) = \\binom{N}{(N+x)/2} p^{(N+x)/2} (1-p)^{(N-x)/2}$$\n\nDirect computation of this formula is numerically unstable for large $N$. The factorial terms within the binomial coefficient (e.g., $N!$) and the power terms (e.g., $p^{N_+}$) can quickly exceed the limits of standard floating-point representations, leading to overflow or underflow.\n\nThe correct and robust computational strategy is to work with the logarithm of the probability. Let $P = \\mathbb{P}(X_N = x)$. We compute $\\ln(P)$:\n$$\\ln(P) = \\ln\\left(\\binom{N}{N_+}\\right) + N_+\\ln(p) + N_-\\ln(1-p)$$\nThe logarithm of the binomial coefficient can be expressed using the logarithm of the gamma function, $\\ln(\\Gamma(z))$, because $\\Gamma(k+1) = k!$.\n$$\\ln\\left(\\binom{N}{k}\\right) = \\ln(N!) - \\ln(k!) - \\ln((N-k)!) = \\ln(\\Gamma(N+1)) - \\ln(\\Gamma(k+1)) - \\ln(\\Gamma(N-k+1))$$\nThis is a numerically stable computation provided by standard scientific libraries, such as `scipy.special.gammaln`.\n\nThe final expression for the log-probability is:\n$$\\ln(P) = \\ln(\\Gamma(N+1)) - \\ln(\\Gamma(N_++1)) - \\ln(\\Gamma(N_-+1)) + N_+\\ln(p) + N_-\\ln(1-p)$$\nFor improved numerical accuracy when $p$ is very small, the term $\\ln(1-p)$ should be computed using a `log1p` function, i.e., `log1p(-p)`.\n\nThe overall algorithm is as follows:\n1.  Validate inputs $(N, x, p)$. Check the base conditions: if $|x|  N$ or if $(N+x)$ is odd, the probability is exactly $0$.\n2.  Handle the boundary cases for $p$.\n    - If $p=0$, the walk is deterministic to the left. The probability is $1$ if $x = -N$ and $0$ otherwise.\n    - If $p=1$, the walk is deterministic to the right. The probability is $1$ if $x = N$ and $0$ otherwise.\n3.  If $0  p  1$, calculate $N_+ = (N+x)/2$ and $N_- = (N-x)/2$.\n4.  Compute the log-probability $\\ln(P)$ using the log-gamma formula.\n5.  The final probability is obtained by exponentiating the result: $P = \\exp(\\ln(P))$. This operation correctly handles underflow: if $\\ln(P)$ is a large negative number, $\\exp(\\ln(P))$ will correctly evaluate to $0.0$ in double-precision arithmetic.\n\nThis procedure ensures a numerically stable and correct evaluation for a wide range of parameters, including those that would cause failure in a naive implementation.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef calculate_probability(N, x, p):\n    \"\"\"\n    Computes the probability of being at position x after N steps of a 1D random walk.\n\n    Args:\n        N (int): The total number of steps.\n        x (int): The final position.\n        p (float): The probability of stepping to the right (+1).\n\n    Returns:\n        float: The probability P(X_N = x).\n    \"\"\"\n    # Step 1: Validate inputs and check for impossible events.\n    # The parity of position x must match the parity of the number of steps N.\n    # This is equivalent to N+x (and N-x) being even.\n    if (N + x) % 2 != 0:\n        return 0.0\n    \n    # The final position cannot be further from the origin than the number of steps.\n    if abs(x) > N:\n        return 0.0\n\n    # Step 2: Handle boundary cases for probability p.\n    if p == 0.0:\n        return 1.0 if x == -N else 0.0\n    \n    if p == 1.0:\n        return 1.0 if x == N else 0.0\n\n    # Step 3: Calculate the number of right (+) and left (-) steps.\n    # These are guaranteed to be integers due to the parity check above.\n    n_plus = (N + x) // 2\n    n_minus = N - n_plus\n\n    # Step 4: Compute the log-probability to avoid overflow/underflow.\n    # The formula is: log(P) = log(C(N, n_plus)) + n_plus*log(p) + n_minus*log(1-p)\n    # The log-combinatorial term log(C(N, k)) is calculated using log-gamma functions:\n    # log(C(N, k)) = gammaln(N+1) - gammaln(k+1) - gammaln(N-k+1)\n    \n    log_comb_term = gammaln(N + 1) - gammaln(n_plus + 1) - gammaln(n_minus + 1)\n    \n    # The log-probability term is calculated using np.log and np.log1p for accuracy.\n    # np.log1p(y) calculates log(1+y) accurately for small y.\n    # Here, log(1-p) is log1p(-p).\n    log_p_term = n_plus * np.log(p) + n_minus * np.log1p(-p)\n    \n    log_prob = log_comb_term + log_p_term\n\n    # Step 5: Exponentiate to get the final probability.\n    # np.exp will handle underflow by returning 0.0 for very negative inputs.\n    return np.exp(log_prob)\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and print the results in the specified format.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (10, 2, 0.5),\n        (9, 2, 0.5),\n        (1000, 1000, 0.5),\n        (5000, 5000, 0.5),\n        (1000000, 0, 0.5),\n        (200000, 200, 0.501),\n        (1000, -1000, 1.0e-12),\n        (1000, 1000, 1.0e-12),\n        (50, 50, 1.0),\n    ]\n\n    results = []\n    for case in test_cases:\n        N, x, p = case\n        result = calculate_probability(int(N), int(x), float(p))\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # e.g., [1.234000000000e-02,...]\n    formatted_results = [f'{r:.12e}' for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2423389"}, {"introduction": "A remarkable discovery in modern statistical mechanics, the Jarzynski equality provides a bridge between nonequilibrium processes and equilibrium free energy differences. Applying this equality, however, presents a significant numerical challenge: it requires averaging the exponential of work values, $\\exp(-\\beta W)$, where many terms can underflow to zero, biasing the result. This advanced practice introduces the \"log-sum-exp\" trick, an essential and widely used algorithm for performing such exponential sums in a numerically stable way, showcasing its power in a cutting-edge biophysics context. [@problem_id:2935883]", "problem": "You are given sets of independent nonequilibrium mechanical work samples for unfolding a single protein domain collected in repeated fast pulling experiments. Each work sample is reported per molecule in picoNewton-nanometer. Temperatures are given in Kelvin. Your task is to implement a program that, starting from first principles of equilibrium statistical mechanics and microscopic reversibility, derives an estimator for the equilibrium Helmholtz free energy difference between folded and unfolded states using the nonequilibrium work relation that connects an exponential average of work to the equilibrium free energy difference. You must then apply this estimator to the provided test suite and report numerical answers.\n\nFundamental base you may use:\n- Canonical ensemble: For a system at temperature $T$, the Helmholtz free energy is $F=-k_{\\mathrm{B}} T \\ln Z$, where $k_{\\mathrm{B}}$ is the Boltzmann constant, $T$ is the absolute temperature, and $Z$ is the canonical partition function.\n- Microscopic reversibility and established nonequilibrium work relation: The exponential average of the nonequilibrium work over many realizations under a fixed protocol is related to the equilibrium free energy difference between initial and final states.\n- Work is path dependent, but the equilibrium free energy difference is a state function.\n\nImplementation constraints:\n- Use $k_{\\mathrm{B}}=1.380649\\times 10^{-23}\\ \\mathrm{J\\,K^{-1}}$ and $N_{\\mathrm{A}}=6.02214076\\times 10^{23}\\ \\mathrm{mol^{-1}}$.\n- Convert each work $W$ from picoNewton-nanometer to Joule using $1\\ \\mathrm{pN\\cdot nm}=10^{-21}\\ \\mathrm{J}$.\n- Temperature $T$ is given in Kelvin.\n- For numerical stability when averaging exponentials, design your computation to avoid underflow for large positive $W$ by appropriate rescaling of exponents before taking logarithms; do not compute naive exponentials of large negative arguments directly.\n- For each test case, compute the equilibrium unfolding free energy difference $\\Delta F$ per mole in kilojoule per mole using your estimator and report a single floating-point number rounded to three decimal places.\n\nAngle units do not apply. If any fraction arises in your internal derivation, your program should still output a floating-point number as specified.\n\nTest suite:\n- Case $1$: $T=298\\ \\mathrm{K}$, work samples in $\\mathrm{pN\\cdot nm}$:\n  $[247.0,255.3,260.1,241.5,252.8,249.2,258.9,243.6,251.0,246.8,262.4,239.7,254.1,248.5,257.3,244.9,250.6,263.0,242.1,256.0]$.\n- Case $2$: $T=310\\ \\mathrm{K}$, work samples in $\\mathrm{pN\\cdot nm}$:\n  $[124.0,126.1,122.5,125.3,127.0,123.8,124.7,125.9,126.4,123.2,124.5,125.1,126.0,123.9,124.8]$.\n- Case $3$: $T=298\\ \\mathrm{K}$, work samples in $\\mathrm{pN\\cdot nm}$:\n  $[420.5,395.2,410.1,505.3,460.7,442.8,487.6,378.4,455.9,468.2,499.1,389.3,472.5,361.7,435.0,481.2,352.6,460.3,341.9,492.7,227.8,243.5,269.1,312.4,285.0,301.6,520.3,476.9,458.7,440.2,355.5,330.8,290.4,275.2,261.9,240.3,510.8,497.0,482.6,466.4]$.\n- Case $4$: $T=298\\ \\mathrm{K}$, work samples in $\\mathrm{pN\\cdot nm}$:\n  $[260.0,310.0,280.0]$.\n\nRequired unit for answers:\n- Report $\\Delta F$ per mole in $\\mathrm{kJ\\,mol^{-1}}$, each rounded to three decimal places.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the cases above, for example, $[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4]$, where each $\\text{result}_i$ is a floating-point number with three digits after the decimal point in $\\mathrm{kJ\\,mol^{-1}}$.", "solution": "We begin with the canonical ensemble, where the Helmholtz free energy is $F=-k_{\\mathrm{B}} T \\ln Z$, with $k_{\\mathrm{B}}$ the Boltzmann constant, $T$ the absolute temperature, and $Z$ the canonical partition function. For an unfolding process that transforms an initial equilibrium state to a final equilibrium state by a driven protocol, the equilibrium free energy difference is $\\Delta F=F_{\\text{final}}-F_{\\text{initial}}$.\n\nMicroscopic reversibility and Hamiltonian dynamics underpin a class of nonequilibrium work relations connecting irreversible work measurements to equilibrium free energy differences. For a fixed protocol repeated many times from the same initial equilibrium state, the exponential average of the work over realizations yields the equilibrium free energy difference. Specifically, if $W$ denotes the mechanical work along a realization and $\\beta=(k_{\\mathrm{B}} T)^{-1}$, then the nonequilibrium work relation states that the exponential average of $W$ encodes $\\Delta F$. From this relation and the definition of expectation as a limit of sample averages, a consistent finite-sample estimator based on $N$ observed works $\\{W_i\\}_{i=1}^N$ is obtained by replacing the ensemble average with the sample mean of exponentials and then taking the appropriate logarithm and scaling by $k_{\\mathrm{B}} T$.\n\nExplicitly, for a given set $\\{W_i\\}_{i=1}^N$, temperature $T$, and $\\beta=(k_{\\mathrm{B}} T)^{-1}$, the estimator is\n$$\n\\widehat{\\Delta F}=-k_{\\mathrm{B}} T \\ln\\left(\\frac{1}{N}\\sum_{i=1}^N e^{-\\beta W_i}\\right).\n$$\nThis follows by combining (i) the canonical definition of free energy, (ii) microscopic reversibility leading to the nonequilibrium work relation connecting the exponential average of $W$ and $\\Delta F$, and (iii) replacing the ensemble average with the sample average to produce a computable estimator. This estimator is asymptotically unbiased as $N\\to\\infty$, though for finite $N$ it may exhibit bias that decreases with the occurrence of low-work realizations.\n\nNumerical stability: Direct computation of $e^{-\\beta W_i}$ may underflow when $W_i$ is large and positive in units of $k_{\\mathrm{B}} T$. To avoid underflow, we use a rescaled log-sum-exp computation. Define $a_i=-\\beta W_i$, and let $a_{\\max}=\\max_i a_i$. Then\n$$\n\\frac{1}{N}\\sum_{i=1}^N e^{a_i}=e^{a_{\\max}} \\cdot \\frac{1}{N}\\sum_{i=1}^N e^{a_i-a_{\\max}},\n$$\nso that\n$$\n\\ln\\left(\\frac{1}{N}\\sum_{i=1}^N e^{a_i}\\right)=a_{\\max}+\\ln\\left(\\frac{1}{N}\\sum_{i=1}^N e^{a_i-a_{\\max}}\\right).\n$$\nTherefore,\n$$\n\\widehat{\\Delta F}=-k_{\\mathrm{B}} T \\left[a_{\\max}+\\ln\\left(\\frac{1}{N}\\sum_{i=1}^N e^{a_i-a_{\\max}}\\right)\\right].\n$$\nSince $a_i-a_{\\max}\\le 0$, the inner exponentials are bounded by $1$ and numerically safe.\n\nUnits and conversions:\n- Input works are per molecule in picoNewton-nanometer. Use $1\\ \\mathrm{pN\\cdot nm}=10^{-21}\\ \\mathrm{J}$ to convert to Joule per molecule.\n- Use $k_{\\mathrm{B}}=1.380649\\times 10^{-23}\\ \\mathrm{J\\,K^{-1}}$ and $N_{\\mathrm{A}}=6.02214076\\times 10^{23}\\ \\mathrm{mol^{-1}}$.\n- After computing $\\widehat{\\Delta F}$ in Joule per molecule, convert to kilojoule per mole via\n$$\n\\widehat{\\Delta F}\\ (\\mathrm{kJ\\,mol^{-1}})=\\frac{\\widehat{\\Delta F}\\ (\\mathrm{J\\ per\\ molecule})\\times N_{\\mathrm{A}}}{1000}.\n$$\n\nAlgorithm for each test case:\n1. Read temperature $T$ (Kelvin) and work samples $\\{W_i^{\\mathrm{(pN\\cdot nm)}}\\}$.\n2. Convert $W_i$ to Joule: $W_i^{\\mathrm{(J)}}=W_i^{\\mathrm{(pN\\cdot nm)}}\\times 10^{-21}$.\n3. Compute $\\beta=(k_{\\mathrm{B}} T)^{-1}$.\n4. Form $a_i=-\\beta W_i^{\\mathrm{(J)}}$.\n5. Compute $a_{\\max}=\\max_i a_i$.\n6. Compute $\\ln M = a_{\\max}+\\ln\\left(\\frac{1}{N}\\sum_{i=1}^N e^{a_i-a_{\\max}}\\right)$.\n7. Compute $\\widehat{\\Delta F}^{\\mathrm{(J)}}=-k_{\\mathrm{B}} T \\cdot \\ln M$.\n8. Convert to $\\mathrm{kJ\\,mol^{-1}}$ using $N_{\\mathrm{A}}$ and divide by $1000$.\n9. Round to three decimal places.\n10. Output the four results in a single bracketed, comma-separated line in the order of the cases.\n\nThe test suite includes:\n- A typical near-reversible set at $T=298\\ \\mathrm{K}$ with works around several tens of $k_{\\mathrm{B}} T$.\n- A higher-temperature, near-equilibrium set at $T=310\\ \\mathrm{K}$.\n- A highly dissipative set at $T=298\\ \\mathrm{K}$ with a broad distribution and rare low-work events to ensure proper convergence of the exponential average.\n- A small-sample edge case at $T=298\\ \\mathrm{K}$ to illustrate finite-sample bias and numerical handling with minimal data.\n\nThe program implements this procedure and prints the results as required.", "answer": "```python\nimport numpy as np\n\n# Constants (CODATA exact values where defined)\nk_B = 1.380649e-23          # Boltzmann constant in J/K\nN_A = 6.02214076e23         # Avogadro constant in 1/mol\nPNM_TO_J = 1e-21            # 1 pN*nm in Joule\nJ_TO_KJMOL = N_A / 1000.0   # Convert J per molecule to kJ/mol\n\ndef jarzynski_deltaF_kJ_per_mol(work_pNnm, T_K):\n    \"\"\"\n    Compute Jarzynski estimator of DeltaF from work samples (per molecule) given in pN*nm at temperature T_K.\n    Returns DeltaF in kJ/mol using a numerically stable log-sum-exp computation.\n    \"\"\"\n    W_J = np.array(work_pNnm, dtype=np.float64) * PNM_TO_J  # per molecule in Joule\n    beta = 1.0 / (k_B * T_K)\n    a = -beta * W_J  # exponents\n    a_max = np.max(a)\n    # Compute log of mean exp(a) stably\n    # ln(mean(exp(a))) = a_max + ln(mean(exp(a - a_max)))\n    exp_shifted = np.exp(a - a_max)\n    ln_mean_exp = a_max + np.log(np.mean(exp_shifted))\n    deltaF_J = - (k_B * T_K) * ln_mean_exp  # per molecule in Joule\n    deltaF_kJ_per_mol = deltaF_J * J_TO_KJMOL\n    return float(deltaF_kJ_per_mol)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        {\n            \"T_K\": 298.0,\n            \"work_pNnm\": [\n                247.0, 255.3, 260.1, 241.5, 252.8, 249.2, 258.9, 243.6, 251.0, 246.8,\n                262.4, 239.7, 254.1, 248.5, 257.3, 244.9, 250.6, 263.0, 242.1, 256.0\n            ],\n        },\n        # Case 2\n        {\n            \"T_K\": 310.0,\n            \"work_pNnm\": [\n                124.0, 126.1, 122.5, 125.3, 127.0, 123.8, 124.7, 125.9, 126.4, 123.2,\n                124.5, 125.1, 126.0, 123.9, 124.8\n            ],\n        },\n        # Case 3\n        {\n            \"T_K\": 298.0,\n            \"work_pNnm\": [\n                420.5, 395.2, 410.1, 505.3, 460.7, 442.8, 487.6, 378.4, 455.9, 468.2,\n                499.1, 389.3, 472.5, 361.7, 435.0, 481.2, 352.6, 460.3, 341.9, 492.7,\n                227.8, 243.5, 269.1, 312.4, 285.0, 301.6, 520.3, 476.9, 458.7, 440.2,\n                355.5, 330.8, 290.4, 275.2, 261.9, 240.3, 510.8, 497.0, 482.6, 466.4\n            ],\n        },\n        # Case 4\n        {\n            \"T_K\": 298.0,\n            \"work_pNnm\": [260.0, 310.0, 280.0],\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        T_K = case[\"T_K\"]\n        work = case[\"work_pNnm\"]\n        deltaF_kJmol = jarzynski_deltaF_kJ_per_mol(work, T_K)\n        results.append(deltaF_kJmol)\n\n    # Format each result to three decimal places and print as a single line list without spaces\n    formatted = [f\"{x:.3f}\" for x in results]\n    print(f\"[{','.join(formatted)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2935883"}]}