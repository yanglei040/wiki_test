{"hands_on_practices": [{"introduction": "Understanding the order of accuracy is not just a theoretical exercise; it's a practical skill that can be verified numerically. This first practice provides a foundational method for estimating the order of a numerical scheme, like the forward Euler method, by comparing the errors produced at two different step sizes. By applying the fundamental error scaling relationship $E(h) \\approx K h^p$, you will empirically confirm the method's behavior and build a core competency for analyzing any numerical integrator [@problem_id:1695635].", "problem": "Consider the initial value problem defined by the ordinary differential equation $y'(t) = \\frac{1}{1+t}$ with the initial condition $y(0) = 0$. We wish to numerically investigate the accuracy of the forward Euler method for this problem.\n\nThe forward Euler method approximates the solution by iterating the formula $y_{n+1} = y_n + h \\cdot f(t_n, y_n)$, where $h$ is the step size, $t_n = t_0 + nh$, and $f(t,y) = y'(t)$.\n\nThe global error of a numerical method at a fixed time $T$ is expected to follow the relationship $E(h) \\approx K h^p$, where $E(h) = |y_{exact}(T) - y_h(T)|$ is the absolute error for a given step size $h$, $y_h(T)$ is the numerical approximation at time $T$, $y_{exact}(T)$ is the true value of the solution at time $T$, $K$ is a constant, and $p$ is the order of accuracy of the method.\n\nYour task is to estimate the order of accuracy, $p$. To do this, first calculate the numerical approximations for $y(2)$ using two different step sizes: $h_1 = 1.0$ and $h_2 = 0.5$. Then, using these two numerical results and the exact solution, compute an estimate for $p$.\n\nExpress your final answer for the estimated order of accuracy $p$ as a numerical value rounded to three significant figures.", "solution": "We first determine the exact solution. Solving $y'(t)=\\frac{1}{1+t}$ gives $y(t)=\\ln(1+t)+C$. Using $y(0)=0$ yields $C=0$, so $y_{\\text{exact}}(t)=\\ln(1+t)$ and hence $y_{\\text{exact}}(2)=\\ln 3$.\n\nApply forward Euler with $h_{1}=1$. Starting at $t_{0}=0$, $y_{0}=0$:\n$$\ny_{1}=y_{0}+1\\cdot \\frac{1}{1+t_{0}}=0+1=1,\\quad t_{1}=1,\n$$\n$$\ny_{2}=y_{1}+1\\cdot \\frac{1}{1+t_{1}}=1+\\frac{1}{2}=\\frac{3}{2},\\quad t_{2}=2.\n$$\nThus $y_{h_{1}}(2)=\\frac{3}{2}$ and the error is\n$$\nE(h_{1})=\\left|\\ln 3-\\frac{3}{2}\\right|\\approx 0.4013877113.\n$$\n\nApply forward Euler with $h_{2}=0.5$. Starting at $t_{0}=0$, $y_{0}=0$ and $t_{n}=0+0.5n$:\n$$\ny_{1}=0+0.5\\cdot \\frac{1}{1+0}=0.5,\\quad\ny_{2}=0.5+0.5\\cdot \\frac{1}{1+0.5}= \\frac{5}{6},\n$$\n$$\ny_{3}=\\frac{5}{6}+0.5\\cdot \\frac{1}{1+1}= \\frac{13}{12},\\quad\ny_{4}=\\frac{13}{12}+0.5\\cdot \\frac{1}{1+1.5}= \\frac{77}{60}.\n$$\nThus $y_{h_{2}}(2)=\\frac{77}{60}$ and the error is\n$$\nE(h_{2})=\\left|\\ln 3-\\frac{77}{60}\\right|\\approx 0.1847210447.\n$$\n\nAssuming $E(h)\\approx K h^{p}$, we estimate $p$ by\n$$\np \\approx \\frac{\\ln\\!\\left(\\frac{E(h_{1})}{E(h_{2})}\\right)}{\\ln\\!\\left(\\frac{h_{1}}{h_{2}}\\right)}\n= \\frac{\\ln\\!\\left(\\frac{0.4013877113}{0.1847210447}\\right)}{\\ln(2)}\n\\approx \\frac{\\ln(2.17294)}{\\ln(2)}\n\\approx 1.12,\n$$\nrounded to three significant figures.", "answer": "$$\\boxed{1.12}$$", "id": "1695635"}, {"introduction": "While simple methods like Euler are instructive, real-world problems often demand higher-order schemes. This practice moves from manual calculation to automated verification by having you implement a general Runge-Kutta solver that uses Butcher tableaus to define the integration scheme. By testing a suite of methods from first to fourth order, you will see directly how the complexity of a method's coefficients translates into faster convergence and higher accuracy [@problem_id:2376768].", "problem": "Write a complete program that, given several explicit Runge–Kutta (RK) methods in the form of Butcher tableaus, numerically verifies each method’s order of accuracy by testing on a manufactured solution of an initial value problem for an Ordinary Differential Equation (ODE). The manufactured solution is defined as follows: let the exact solution be $y(t)=\\exp(\\sin t)$ for $t \\in [0,1]$, where angles are in radians. The ODE is $y'(t)=f(t,y(t))$ with $f(t,y)=\\cos(t)\\,y$, the initial condition is $y(0)=1$, and the final time is $T=1$. Use the global error at the final time $t=T$ to quantify accuracy.\n\nUse the following explicit Runge–Kutta methods, each specified by its Butcher tableau $(A,b,c)$:\n- Method $1$ (Explicit Euler, expected order $1$): $s=1$, $A=\\begin{bmatrix}0\\end{bmatrix}$, $b=\\begin{bmatrix}1\\end{bmatrix}$, $c=\\begin{bmatrix}0\\end{bmatrix}$.\n- Method $2$ (Explicit Midpoint, expected order $2$): $s=2$, $A=\\begin{bmatrix}0  0\\\\ \\tfrac{1}{2}  0\\end{bmatrix}$, $b=\\begin{bmatrix}0  1\\end{bmatrix}$, $c=\\begin{bmatrix}0  \\tfrac{1}{2}\\end{bmatrix}$.\n- Method $3$ (Kutta’s third-order method, expected order $3$): $s=3$, $A=\\begin{bmatrix}0  0  0\\\\ \\tfrac{1}{2}  0  0\\\\ -1  2  0\\end{bmatrix}$, $b=\\begin{bmatrix}\\tfrac{1}{6}  \\tfrac{2}{3}  \\tfrac{1}{6}\\end{bmatrix}$, $c=\\begin{bmatrix}0  \\tfrac{1}{2}  1\\end{bmatrix}$.\n- Method $4$ (Classical RK4, expected order $4$): $s=4$, $A=\\begin{bmatrix}0  0  0  0\\\\ \\tfrac{1}{2}  0  0  0\\\\ 0  \\tfrac{1}{2}  0  0\\\\ 0  0  1  0\\end{bmatrix}$, $b=\\begin{bmatrix}\\tfrac{1}{6}  \\tfrac{1}{3}  \\tfrac{1}{3}  \\tfrac{1}{6}\\end{bmatrix}$, $c=\\begin{bmatrix}0  \\tfrac{1}{2}  \\tfrac{1}{2}  1\\end{bmatrix}$.\n\nFor each method, perform time integration on $[0,1]$ using uniform time steps of size $h=1/N$ for $N$ in the test suite $\\{10,20,40,80,160,320\\}$. For each $N$, compute the numerical approximation $y_N$ at $t=1$, compute the global error $E(h)=\\lvert y_N - y(1)\\rvert$, and then estimate the observed order $p$ as the least-squares slope of the line fitting the data $(\\log h,\\log E(h))$ across all values of $N$ in the test suite. Use the natural logarithm for $\\log$.\n\nYour program must output, in a single line, a comma-separated list enclosed in square brackets containing the four estimated orders $(p_1,p_2,p_3,p_4)$ for Methods $1$ through $4$, respectively, each rounded to two decimal places. No other text should be printed.\n\nTest Suite and Answer Specification:\n- The test suite consists of the four methods above, each tested with $N \\in \\{10,20,40,80,160,320\\}$.\n- The final answers are the four floats $p_1$, $p_2$, $p_3$, $p_4$, each being the observed order estimate for the corresponding method.\n- The final output format must be exactly a single line of the form $\\texttt{[p1,p2,p3,p4]}$, where each $p_k$ is rounded to two decimal places and printed as a decimal number.", "solution": "The problem statement has been rigorously analyzed. It is scientifically grounded, well-posed, and contains all necessary information for a unique and meaningful solution. The specified ordinary differential equation, its manufactured analytical solution, the definitions of the Runge-Kutta methods via their Butcher tableaus, and the procedure for numerical verification of the order of accuracy are all standard, correct, and self-consistent. The problem is valid. We will now construct the solution.\n\nThe fundamental task is to solve an initial value problem (IVP) of the form:\n$$ y'(t) = f(t, y(t)), \\quad y(t_0) = y_0 $$\nfor $t \\in [t_0, T]$. The problem provides the specific function $f(t, y) = \\cos(t) y$, the initial condition $y(0) = 1$, and the time interval $[0, 1]$. The exact solution is given as $y(t) = \\exp(\\sin t)$, which is readily verified by differentiation: $y'(t) = \\exp(\\sin t) \\cdot \\cos(t) = y(t)\\cos(t)$, and checking the initial condition: $y(0) = \\exp(\\sin 0) = \\exp(0) = 1$.\n\nAn $s$-stage explicit Runge-Kutta (RK) method approximates the solution by stepping forward in time with a step size $h$. From the solution $y_n$ at time $t_n$, the solution $y_{n+1}$ at time $t_{n+1} = t_n + h$ is calculated. The method is defined by a set of coefficients arranged in a Butcher tableau:\n$$\n\\begin{array}{c|c}\nc  A \\\\\n\\hline\n   b^T\n\\end{array} \\quad \\text{where } c \\in \\mathbb{R}^s, b \\in \\mathbb{R}^s, A \\in \\mathbb{R}^{s \\times s}\n$$\nFor an explicit method, the matrix $A$ is strictly lower triangular, meaning $a_{ij} = 0$ for $j \\ge i$. The computation proceeds in stages. First, the $s$ stage derivatives, $k_i$, are computed for $i=1, 2, \\dots, s$:\n$$ k_i = f\\left(t_n + c_i h, y_n + h \\sum_{j=1}^{i-1} a_{ij} k_j\\right) $$\nThe solution is then advanced using a weighted average of these stage derivatives:\n$$ y_{n+1} = y_n + h \\sum_{i=1}^{s} b_i k_i $$\n\nThe accuracy of a numerical method is characterized by its order of convergence, $p$. For a method of order $p$, the global error at a fixed final time $T$, denoted $E(h)$, is expected to decrease with the step size $h$ according to the relationship:\n$$ E(h) = |y_N - y(T)| \\approx C h^p $$\nwhere $y_N$ is the numerical solution at $T=Nh$ and $C$ is a constant that depends on the method and the problem, but not on $h$.\n\nTo numerically verify the order $p$, we can transform this relationship by taking the natural logarithm of both sides:\n$$ \\ln(E(h)) \\approx \\ln(C) + p \\ln(h) $$\nThis equation is of the form $Y = mX + B$, where $Y = \\ln(E(h))$, $X = \\ln(h)$, the slope is $m = p$, and the intercept is $B = \\ln(C)$. This linear relationship implies that a plot of $\\ln(E(h))$ versus $\\ln(h)$ will approximate a straight line whose slope is the order of the method, $p$.\n\nThe specified procedure is as follows:\n$1$. For each of the four given RK methods, a series of numerical integrations must be performed over the interval $[0, 1]$.\n$2$. The integrations will use a sequence of decreasing step sizes $h = 1/N$, for $N \\in \\{10, 20, 40, 80, 160, 320\\}$.\n$3$. For each integration with a specific step size $h$, the numerical approximation at the final time, $y_N$, is computed.\n$4$. The global error is calculated as $E(h) = |y_N - y(1)|$, where the exact value is $y(1) = \\exp(\\sin 1)$.\n$5$. After computing the errors for all step sizes, the data pairs $(\\ln(h), \\ln(E(h)))$ are collected.\n$6$. A linear least-squares regression is performed on these data points. The slope of the resulting best-fit line provides the experimental estimate of the order of accuracy, $p$. The slope $p$ for a set of data points $(x_i, y_i)$ is given by:\n$$ p = \\frac{\\sum_{i=1}^{M} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{M} (x_i - \\bar{x})^2} $$\nwhere $x_i = \\ln(h_i)$, $y_i = \\ln(E(h_i))$, $\\bar{x}$ and $\\bar{y}$ are the mean values, and $M=6$ is the number of step sizes in the test suite.\n\nThis procedure will be implemented for each of the four Butcher tableaus provided, yielding four estimated orders of accuracy, $(p_1, p_2, p_3, p_4)$, which are expected to be close to their theoretical values of $1, 2, 3,$ and $4$, respectively. The final result will be these four values, rounded to two decimal places.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Numerically verifies the order of accuracy of several explicit Runge-Kutta methods.\n    \"\"\"\n    # Define the Ordinary Differential Equation and its analytical solution\n    f = lambda t, y: np.cos(t) * y\n    t_start = 0.0\n    y_start = 1.0\n    t_end = 1.0\n    \n    # Pre-calculate the exact solution at the final time for error computation\n    y_exact_final = np.exp(np.sin(t_end))\n\n    # Define the Butcher tableaus for the four RK methods\n    methods = [\n        {\n            # Method 1: Explicit Euler (Order 1)\n            'A': np.array([[0.0]]),\n            'b': np.array([1.0]),\n            'c': np.array([0.0])\n        },\n        {\n            # Method 2: Explicit Midpoint (Order 2)\n            'A': np.array([[0.0, 0.0], [0.5, 0.0]]),\n            'b': np.array([0.0, 1.0]),\n            'c': np.array([0.0, 0.5])\n        },\n        {\n            # Method 3: Kutta's third-order method (Order 3)\n            'A': np.array([[0.0, 0.0, 0.0], [0.5, 0.0, 0.0], [-1.0, 2.0, 0.0]]),\n            'b': np.array([1.0/6.0, 2.0/3.0, 1.0/6.0]),\n            'c': np.array([0.0, 0.5, 1.0])\n        },\n        {\n            # Method 4: Classical RK4 (Order 4)\n            'A': np.array([\n                [0.0, 0.0, 0.0, 0.0],\n                [0.5, 0.0, 0.0, 0.0],\n                [0.0, 0.5, 0.0, 0.0],\n                [0.0, 0.0, 1.0, 0.0]\n            ]),\n            'b': np.array([1.0/6.0, 1.0/3.0, 1.0/3.0, 1.0/6.0]),\n            'c': np.array([0.0, 0.5, 0.5, 1.0])\n        }\n    ]\n\n    # Test suite of step counts\n    N_values = [10, 20, 40, 80, 160, 320]\n    h_values = np.array([1.0 / N for N in N_values])\n\n    estimated_orders = []\n\n    for method in methods:\n        A, b, c = method['A'], method['b'], method['c']\n        s = len(b)  # Number of stages\n        errors = []\n\n        for N in N_values:\n            h = (t_end - t_start) / N\n            y_current = y_start\n            \n            # Time integration loop\n            for n in range(N):\n                t_n = t_start + n * h\n                k_stages = np.zeros(s)\n                \n                # Calculate stage derivatives k_i\n                for i in range(s):\n                    stage_sum = 0.0\n                    for j in range(i):\n                        stage_sum += A[i, j] * k_stages[j]\n                    \n                    y_stage_input = y_current + h * stage_sum\n                    t_stage_input = t_n + c[i] * h\n                    k_stages[i] = f(t_stage_input, y_stage_input)\n                \n                # Update solution\n                y_current += h * np.dot(b, k_stages)\n            \n            # Store the global error at t=T\n            errors.append(np.abs(y_current - y_exact_final))\n\n        # Use natural logarithm for the log-log plot\n        log_h = np.log(h_values)\n        log_E = np.log(np.array(errors))\n        \n        # Perform linear regression (polynomial fit of degree 1)\n        # The slope of the line is the estimated order of accuracy\n        # np.polyfit returns [slope, intercept]\n        slope = np.polyfit(log_h, log_E, 1)[0]\n        estimated_orders.append(slope)\n        \n    # Format the output as specified: [p1,p2,p3,p4]\n    formatted_results = [f'{p:.2f}' for p in estimated_orders]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2376768"}, {"introduction": "Having seen that higher-order methods converge more quickly, a natural question arises: why are their coefficients so specific? This final practice delves into the heart of Runge-Kutta theory by exploring the consequences of altering the carefully tuned coefficients of the classical fourth-order method. By empirically measuring the degraded order of accuracy after these perturbations, you will gain a profound appreciation for the \"order conditions\" that underpin the power of high-order numerical schemes [@problem_id:2422943].", "problem": "You are given the standard explicit classical fourth-order Runge–Kutta method for the scalar ordinary differential equation (ODE) $y'(t) = f(t,y(t))$, with initial condition $y(0) = y_0$, specified by its Butcher tableau:\n$$\n\\begin{array}{c|cccc}\n0    0    0    0    0 \\\\\n\\dfrac{1}{2}  \\dfrac{1}{2}  0    0    0 \\\\\n\\dfrac{1}{2}  0    \\dfrac{1}{2}  0    0 \\\\\n1    0    0    1    0 \\\\\n\\hline\n     \\dfrac{1}{6}  \\dfrac{1}{3}  \\dfrac{1}{3}  \\dfrac{1}{6}\n\\end{array}\n$$\nwhere the Runge–Kutta coefficients are $a_{ij}$ (strictly lower triangular for an explicit method), stage abscissae $c_i = \\sum_{j=1}^{i-1} a_{ij}$, and weights $b_i$.\n\nConsider the initial value problem (IVP) for the scalar ODE\n$$\ny'(t) = y(t), \\quad y(0) = 1,\n$$\nwhose exact solution is $y(t) = e^{t}$. Let the final time be $T=1$. For a stepsize $h = T/N$ with integer $N \\ge 1$, let $y_N$ denote the numerical approximation at time $T$ produced by a single-step explicit Runge–Kutta method with $N$ uniform steps of size $h$. Define the global error at $T$ by\n$$\nE(h) = \\lvert y_N - y(T) \\rvert.\n$$\n\nThe order of accuracy $p$ is defined by the asymptotic relation $E(h) \\sim C h^{p}$ as $h \\to 0$, for some constant $C \\neq 0$. In this problem, you will empirically estimate the order $p$ after altering one or more coefficients of the Butcher tableau.\n\nUse the following test suite of four cases. In all cases, apply the same initial value problem and final time as specified above, and use the uniform step counts\n$$\nN \\in \\{20,\\,40,\\,80,\\,160,\\,320,\\,640\\}.\n$$\nFor each case, construct the Runge–Kutta method by starting from the standard tableau above and then applying the stated modification(s). No other coefficients are to be changed beyond what is explicitly stated.\n\n- Case A (baseline): No modification; use the classical fourth-order coefficients exactly as given.\n\n- Case B (weight inconsistency): Modify the weight $b_2$ to $b_2 + \\delta$ with $\\delta = 10^{-3}$; leave all other coefficients unchanged.\n\n- Case C (internal coefficient perturbation): Modify the internal coefficient $a_{32}$ to $a_{32} + \\delta$ with $\\delta = 10^{-3}$; leave all other coefficients unchanged.\n\n- Case D (weights adjusted to preserve first and second order conditions): With $\\delta = 10^{-3}$, modify the weights by $b_1 \\leftarrow b_1 - \\dfrac{\\delta}{2}$, $b_2 \\leftarrow b_2 + \\delta$, $b_3 \\leftarrow b_3$ (unchanged), $b_4 \\leftarrow b_4 - \\dfrac{\\delta}{2}$.\n\nFor each case, estimate the empirical order $p$ as the slope of the least-squares linear fit to the data points $\\big(\\ln h,\\, \\ln E(h)\\big)$ over the provided set of $N$ values, where $\\ln$ denotes the natural logarithm. Report each estimated $p$ rounded to $3$ decimal places.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order [Case A, Case B, Case C, Case D]. For example, the output format must be exactly like\n$$\n[\\text{pA},\\text{pB},\\text{pC},\\text{pD}],\n$$\nwhere each of $\\text{pA}$, $\\text{pB}$, $\\text{pC}$, $\\text{pD}$ is a floating-point number rounded to $3$ decimal places.", "solution": "We seek the empirical order of accuracy for explicit Runge–Kutta discretizations when one or more coefficients are perturbed from those of the standard fourth-order method. The method acts on the initial value problem $y'(t) = y(t)$ with $y(0) = 1$ and exact solution $y(t) = e^{t}$. We integrate to final time $T=1$ with stepsize $h = T/N$ for $N \\in \\{20,\\,40,\\,80,\\,160,\\,320,\\,640\\}$, and compute the global error $E(h) = \\lvert y_N - e^{1} \\rvert$ for each $h$.\n\nDefinition of empirical order. If a method has order $p$, then as $h \\to 0$ its global error satisfies $E(h) \\sim C h^{p}$ with constant $C \\ne 0$. Taking natural logarithms yields $\\ln E(h) = \\ln C + p \\ln h + o(1)$. Hence, the slope of the least-squares best-fit line to the points $\\big(\\ln h, \\ln E(h)\\big)$ over a set of sufficiently small $h$ estimates $p$. This estimator is consistent with the order definition because linear regression on the linearized model identifies the exponent $p$ as the slope.\n\nRunge–Kutta formulation. An $s$-stage explicit Runge–Kutta method advances $y_n \\mapsto y_{n+1}$ by computing stage derivatives $k_i$ according to\n$$\nk_i = f\\!\\left(t_n + c_i h,\\; y_n + h \\sum_{j=1}^{i-1} a_{ij} k_j\\right), \\quad i=1,\\dots,s,\n$$\nand then\n$$\ny_{n+1} = y_n + h \\sum_{i=1}^{s} b_i k_i.\n$$\nFor explicit methods, $a_{ij} = 0$ for $j \\ge i$ and $c_i = \\sum_{j=1}^{i-1} a_{ij}$. The classical fourth-order method has $s=4$ and the Butcher tableau provided in the problem statement.\n\nOrder conditions and effects of perturbations. The order conditions for explicit Runge–Kutta methods can be derived by matching Taylor series expansions of the exact and numerical solutions (for example, via rooted tree analysis). The first few conditions are:\n- Order $1$: $\\sum_{i=1}^{s} b_i = 1$.\n- Order $2$: $\\sum_{i=1}^{s} b_i c_i = \\dfrac{1}{2}$.\n- Order $3$: two independent conditions, including $\\sum_{i=1}^{s} b_i c_i^2 = \\dfrac{1}{3}$ and $\\sum_{i,j} b_i a_{ij} c_j = \\dfrac{1}{6}$.\n- Order $4$: several additional conditions.\n\nThe unperturbed tableau satisfies all conditions up to order $4$. Perturbations can violate some of these, degrading the order.\n\n- Case A (baseline): All conditions remain satisfied; the global error scales as $E(h) \\propto h^{4}$, so the estimated slope $p$ should be close to $4$.\n\n- Case B (weight inconsistency $b_2 \\to b_2 + \\delta$): The sum of weights becomes $\\sum b_i = 1 + \\delta$, violating the order $1$ condition. This destroys consistency, and the leading local defect is $\\mathcal{O}(h)$ times a nonzero constant error in the first Taylor coefficient, yielding a global error that does not vanish as $h \\to 0$. Thus the empirical convergence rate tends to $p \\approx 0$.\n\n- Case C (internal coefficient perturbation $a_{32} \\to a_{32} + \\delta$): The stage abscissa $c_3 = \\sum_{j3} a_{3j}$ becomes $c_3 = \\dfrac{1}{2} + \\delta$, altering $\\sum b_i c_i$ away from $\\dfrac{1}{2}$ even though the weights are unchanged. As a result, the order $2$ condition is generally violated while order $1$ remains intact; the method becomes first-order, so $E(h) \\propto h^{1}$ and $p \\approx 1$.\n\n- Case D (weights adjusted to preserve first and second order conditions): With $\\delta = 10^{-3}$, set $b_1 \\leftarrow b_1 - \\dfrac{\\delta}{2}$, $b_2 \\leftarrow b_2 + \\delta$, $b_3$ unchanged, and $b_4 \\leftarrow b_4 - \\dfrac{\\delta}{2}$. These modifications satisfy\n$$\n\\sum_i b_i \\ \\text{unchanged}, \\qquad \\sum_i b_i c_i \\ \\text{unchanged},\n$$\nso the order $1$ and order $2$ conditions still hold. However, the third-order condition $\\sum_i b_i c_i^2 = \\dfrac{1}{3}$ is altered because $c_1 = 0$, $c_2 = c_3 = \\dfrac{1}{2}$, and $c_4 = 1$ yield a net change\n$$\n\\Delta\\!\\left(\\sum_i b_i c_i^2\\right) = \\left(+\\delta\\right)\\left(\\dfrac{1}{2}\\right)^2 + \\left(-\\dfrac{\\delta}{2}\\right)\\cdot 1^2 = \\dfrac{\\delta}{4} - \\dfrac{\\delta}{2} = -\\dfrac{\\delta}{4} \\ne 0,\n$$\nthus violating a third-order condition while maintaining those of orders $1$ and $2$. Therefore the empirical order degrades to approximately $p \\approx 2$.\n\nNumerical estimation procedure. For each case, we compute $E(h)$ for the specified set of $N$ values, form the data $\\left(\\ln h, \\ln E(h)\\right)$, and compute the slope $p$ of the least-squares linear fit. This slope estimates the order. We then round each $p$ to $3$ decimal places and report in the order [Case A, Case B, Case C, Case D] as required.\n\nRunning the computation for the given initial value problem and coefficient perturbations typically yields values close to $[4,\\,0,\\,1,\\,2]$ (within small deviations due to finite-$h$ effects), each reported to three decimal places.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef rk_explicit_step(f, t, y, h, a, b):\n    \"\"\"\n    Perform one step of an explicit Runge-Kutta method defined by coefficients a, b.\n    a: (s x s) strictly lower triangular matrix (entries a[i,j] for j  i)\n    b: length-s weights\n    c: computed as row sums of a (since explicit)\n    \"\"\"\n    s = len(b)\n    # Compute stage abscissae c_i = sum_{j i} a_{ij}\n    c = np.sum(a, axis=1)\n    k = [None] * s\n    for i in range(s):\n        yi = y\n        if i  0:\n            acc = 0.0\n            for j in range(i):\n                acc += a[i, j] * k[j]\n            yi = y + h * acc\n        ti = t + c[i] * h\n        k[i] = f(ti, yi)\n    incr = 0.0\n    for i in range(s):\n        incr += b[i] * k[i]\n    return y + h * incr\n\ndef integrate_rk(f, y0, T, N, a, b):\n    h = T / N\n    t = 0.0\n    y = y0\n    for _ in range(N):\n        y = rk_explicit_step(f, t, y, h, a, b)\n        t += h\n    return y\n\ndef empirical_order(hs, errs):\n    # Use least-squares fit of log(err) vs log(h); slope is the order\n    x = np.log(hs)\n    y = np.log(errs)\n    # Fit y = p*x + c\n    p, c = np.polyfit(x, y, 1)\n    return p\n\ndef make_base_tableau():\n    # Classical RK4\n    a = np.array([\n        [0.0, 0.0, 0.0, 0.0],\n        [0.5, 0.0, 0.0, 0.0],\n        [0.0, 0.5, 0.0, 0.0],\n        [0.0, 0.0, 1.0, 0.0]\n    ], dtype=float)\n    b = np.array([1/6, 1/3, 1/3, 1/6], dtype=float)\n    return a, b\n\ndef solve():\n    # Problem data\n    f = lambda t, y: y\n    y_exact = lambda t: np.exp(t)\n    y0 = 1.0\n    T = 1.0\n    Ns = np.array([20, 40, 80, 160, 320, 640], dtype=int)\n    hs = T / Ns.astype(float)\n    delta = 1e-3\n\n    results = []\n\n    # Case A: baseline (no changes)\n    aA, bA = make_base_tableau()\n    errsA = []\n    for N in Ns:\n        yN = integrate_rk(f, y0, T, N, aA, bA)\n        err = abs(yN - y_exact(T))\n        errsA.append(err)\n    pA = empirical_order(hs, np.array(errsA))\n    results.append(pA)\n\n    # Case B: weight inconsistency b2 += delta\n    aB, bB = make_base_tableau()\n    bB = bB.copy()\n    bB[1] += delta\n    errsB = []\n    for N in Ns:\n        yN = integrate_rk(f, y0, T, N, aB, bB)\n        err = abs(yN - y_exact(T))\n        errsB.append(err)\n    pB = empirical_order(hs, np.array(errsB))\n    results.append(pB)\n\n    # Case C: internal coefficient perturbation a32 += delta\n    aC, bC = make_base_tableau()\n    aC = aC.copy()\n    aC[2, 1] += delta  # a32 in 1-based indexing\n    errsC = []\n    for N in Ns:\n        yN = integrate_rk(f, y0, T, N, aC, bC)\n        err = abs(yN - y_exact(T))\n        errsC.append(err)\n    pC = empirical_order(hs, np.array(errsC))\n    results.append(pC)\n\n    # Case D: weights adjusted to preserve sum b and sum b c\n    aD, bD = make_base_tableau()\n    bD = bD.copy()\n    bD[0] -= delta/2.0\n    bD[1] += delta\n    # b3 unchanged\n    bD[3] -= delta/2.0\n    errsD = []\n    for N in Ns:\n        yN = integrate_rk(f, y0, T, N, aD, bD)\n        err = abs(yN - y_exact(T))\n        errsD.append(err)\n    pD = empirical_order(hs, np.array(errsD))\n    results.append(pD)\n\n    # Format results rounded to 3 decimals\n    results_str = \"[\" + \",\".join(f\"{p:.3f}\" for p in results) + \"]\"\n    print(results_str)\n\nsolve()\n```", "id": "2422943"}]}