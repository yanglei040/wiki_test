{"hands_on_practices": [{"introduction": "A common pitfall for programmers new to scientific computing is using floating-point variables in loop conditions that rely on exact equality. This practice demonstrates why this seemingly simple approach can fail unexpectedly. By simulating a loop and comparing the computed floating-point sequence against its theoretical, exact counterpart, you will directly observe the effects of representation and round-off errors. This exercise [@problem_id:2447428] is fundamental to understanding why you must use tolerance-based comparisons instead of exact equality checks in floating-point arithmetic.", "problem": "You are given real scalars $s$, $h$, and $t$, and a positive integer $M$. Consider the sequence $\\{x_n\\}_{n\\ge 0}$ defined by $x_0 = s$ and $x_{n+1} = x_n + h$ for $n \\ge 0$. A naive termination condition that checks whether $x_n = t$ may be unreliable when $x_n$ is computed in binary floating-point arithmetic due to representation and rounding effects. Your task is to write a program that, for each provided test case, simulates the floating-point sequence and reports whether the equality $x_n = t$ ever holds within a bounded number of simulated iterations.\n\nFor each test case, perform the following steps:\n1. Initialize $x_0 = s$ in double-precision floating-point arithmetic.\n2. For $n = 0,1,2,\\dots$, check whether $x_n = t$ exactly in floating-point arithmetic. If equality holds for some $n \\le M$, record the smallest such $n$ and terminate the simulation for that test case.\n3. If no equality occurs for $0 \\le n \\le M$, terminate the simulation after $M$ increments.\n4. Let $n_{\\mathrm{exec}}$ denote the number of increments actually performed by your simulation for the test case, where $n_{\\mathrm{exec}}$ equals the smallest $n$ for which equality is observed (if any), or $M$ otherwise.\n5. Compute the theoretical exact value $x_{\\mathrm{exact}} = s + n_{\\mathrm{exec}}\\,h$ using exact real arithmetic, not using floating-point rounding at intermediate steps.\n6. Let $x_{\\mathrm{float}}$ denote the floating-point value obtained by your simulation after $n_{\\mathrm{exec}}$ increments. Compute the absolute error $e = |x_{\\mathrm{float}} - x_{\\mathrm{exact}}|$ as a real number.\n\nFor each test case, your program must output a list with the following fields in order:\n- A boolean indicating whether there exists an $n \\in \\{0,1,\\dots,M\\}$ such that $x_n = t$ in floating-point arithmetic.\n- The smallest such $n$ if it exists; otherwise the integer $-1$.\n- The integer $n_{\\mathrm{exec}}$.\n- The floating-point value $x_{\\mathrm{float}}$ after $n_{\\mathrm{exec}}$ increments.\n- The theoretical exact value $x_{\\mathrm{exact}}$ converted to a floating-point number for reporting.\n- The absolute error $e$ as a floating-point number.\n\nUse the following five test cases. In all test cases, angles do not appear, and there are no physical units. The real numbers written in decimal (such as $0.1$) denote exact real values in base ten, and expressions involving powers of two (such as $2^{-55}$) denote exact real values.\n\n- Test case 1 (canonical failure of equality with decimal step):\n  $s = 0.0$, $h = 0.1$, $t = 1.0$, $M = 12$.\n- Test case 2 (exactly representable step and reachable target):\n  $s = 0.0$, $h = 0.125$, $t = 1.0$, $M = 8$.\n- Test case 3 (step does not subdivide the interval):\n  $s = 0.0$, $h = 0.3$, $t = 1.0$, $M = 10$.\n- Test case 4 (negative step toward zero with decimal step):\n  $s = 1.0$, $h = -0.1$, $t = 0.0$, $M = 12$.\n- Test case 5 (increments below one unit in the last place until accumulation):\n  $s = 1.0$, $h = 2^{-55}$, $t = 1 + 2^{-52}$, $M = 8$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a list for one test case in the same order as above. For example, the output must have the form\n$[r_1,r_2,r_3,r_4,r_5]$\nwhere each $r_i$ is the list corresponding to test case $i$, and there must be no spaces anywhere in the line. Each list $r_i$ must be of the form\n$[\\text{hit}, n_{\\min}, n_{\\mathrm{exec}}, x_{\\mathrm{float}}, x_{\\mathrm{exact}}, e]$\nwith the field types defined above. All numeric outputs must be represented as base-ten floats or integers on this line.", "solution": "The problem statement has been subjected to rigorous validation and is determined to be valid. It is scientifically grounded in the principles of numerical computation, well-posed with a clear and deterministic procedure, and objective in its formulation. It presents a standard but fundamental problem in computational engineering concerning the limitations of floating-point arithmetic. We shall now proceed with the solution.\n\nThe core of this problem lies in the fundamental distinction between exact real arithmetic and computer-based floating-point arithmetic, governed by the IEEE $754$ standard for double-precision numbers. When simulating the sequence $x_{n+1} = x_n + h$, two primary sources of error arise:\n\n$1$. **Representation Error**: Many decimal fractions, such as $0.1$ or $0.3$, do not have an exact finite representation in binary. They are stored as the nearest representable binary floating-point number. For example, the decimal $0.1$ is an infinite repeating fraction in binary ($0.0001100110011\\dots_2$), which must be truncated to fit the $52$-bit significand of a double-precision float. This introduces an initial error before any computation begins. Conversely, numbers that are finite sums of powers of two, such as $0.125 = 1/8 = 2^{-3}$, are represented exactly.\n\n$2$. **Round-off Error**: Each arithmetic operation, in this case, addition, is performed with finite precision. The mathematically exact result of $x_n + h$ is rounded to the nearest representable floating-point number. This small error, introduced at each step, can accumulate over many iterations, causing the computed sequence $x_n$ to diverge from its theoretical path.\n\nThe provided problem requires a simulation that demonstrates these effects. The methodology is as follows:\n\nFirst, we must meticulously handle the input values. The provided test case parameters $s$, $h$, and $t$ are defined as exact real numbers. To compute the theoretical value $x_{\\mathrm{exact}}$, we must use a high-precision arithmetic library that avoids standard floating-point inaccuracies. For this purpose, Python's `decimal` module is employed, configured with a sufficiently high precision to handle all calculations as if they were exact. The inputs for each test case are converted to these high-precision objects.\n\nSecond, a simulation is performed for each test case using standard double-precision floating-point arithmetic, which is represented in Python by the `float` type or `numpy.float64`. The simulation adheres to the following algorithm:\n$1$. Initialize the floating-point sequence value $x_{\\mathrm{float}} \\leftarrow \\text{float}(s)$. Initialize `hit` to `False` and `n_min` to $-1$.\n$2$. Iterate with an index $n$ from $0$ to $M$, inclusive. In each iteration, the current value $x_n$ of the sequence is represented by $x_{\\mathrm{float}}$.\n$3$. At each step $n$, perform an exact equality check: if $x_{\\mathrm{float}} == \\text{float}(t)$.\n$4$. If equality holds, the target has been hit. We set `hit` to `True`, record the current index as $n_{\\mathrm{min}} = n$, set the number of increments performed as $n_{\\mathrm{exec}} = n$, store the current value of $x_{\\mathrm{float}}$ as the final one, and terminate the simulation loop for this test case.\n$5$. If equality does not hold and $n  M$, the sequence is advanced by one step: $x_{\\mathrm{float}} \\leftarrow x_{\\mathrm{float}} + \\text{float}(h)$.\n$6$. If the loop completes without finding an equality (i.e., for all $n \\in \\{0, 1, \\dots, M\\}$), we set $n_{\\mathrm{exec}} = M$. The final value of $x_{\\mathrm{float}}$ is the result after $M$ increments.\n\nThird, after the simulation determines the values of $n_{\\mathrm{exec}}$ and the final $x_{\\mathrm{float}}$, we calculate the theoretical quantities.\n$1$. The exact final value is computed using the high-precision `Decimal` objects: $x_{\\mathrm{exact}} = s_{\\mathrm{exact}} + n_{\\mathrm{exec}} \\cdot h_{\\mathrm{exact}}$.\n$2$. The absolute error is then $e = |x_{\\mathrm{float}} - \\text{float}(x_{\\mathrm{exact}})|$.\n\nFinally, the six required output fields (`hit`, $n_{\\mathrm{min}}$, $n_{\\mathrm{exec}}$, $x_{\\mathrm{float}}$, $\\text{float}(x_{\\mathrm{exact}})$, and $e$) are collected into a list for each test case.\n\nThe specific test cases are designed to illustrate different behaviors:\n- **Cases $1$, $3$, and $4$**: These use step sizes ($h = \\pm 0.1$, $h = 0.3$) that are not exactly representable in binary. The accumulation of representation and round-off errors will cause the floating-point sequence to miss the target value $t$ exactly. Thus, we expect `hit` to be `False`.\n- **Case $2$**: Here, $s, h, t$ (`0.0`, `0.125`, `1.0`) are all exactly representable, as $h=2^{-3}$. All additions are exact. The sequence will exactly hit the target $t=1.0$ at $n=8$. Thus, we expect `hit` to be `True`.\n- **Case $5$**: This case is subtle. The start value is $s=1.0$. The unit in the last place (ULP) for $1.0$ is $2^{-52}$. The increment is $h = 2^{-55}$, which is smaller than half the ULP of $1.0$. Due to round-to-nearest-even rules in IEEE $754$, the operation $1.0 + 2^{-55}$ rounds back to $1.0$. The simulated value of $x_n$ will therefore remain stuck at $1.0$ and will never reach the target $t = 1.0 + 2^{-52}$. However, the exact sum does accumulate. This demonstrates a significant round-off error (absorption) where small increments are lost. We expect `hit` to be `False`, and a non-zero error $e$ equal to the sum of the lost increments.", "answer": "```python\nimport numpy as np\nfrom decimal import Decimal, getcontext\n\ndef solve():\n    \"\"\"\n    Simulates a floating-point sequence and compares it with exact arithmetic.\n    \"\"\"\n    # Set a high precision for the Decimal module for \"exact\" calculations.\n    getcontext().prec = 100\n\n    def run_case_simulation(s_exact, h_exact, t_exact, M):\n        \"\"\"\n        Runs the simulation for a single test case.\n        \"\"\"\n        # Convert exact Decimal inputs to double-precision floats for simulation.\n        s_f = np.float64(s_exact)\n        h_f = np.float64(h_exact)\n        t_f = np.float64(t_exact)\n\n        x_n_float = s_f\n        hit = False\n        n_min = -1\n        \n        # Loop from n=0 to M, checking the sequence value x_n at each step.\n        for n in range(M + 1):\n            # Per problem, check for exact floating-point equality.\n            if x_n_float == t_f:\n                hit = True\n                n_min = n\n                n_exec = n\n                x_float_final = x_n_float\n                break\n            \n            # If not hit and not the last iteration, perform one increment.\n            if n  M:\n                x_n_float += h_f\n        else:  # This 'else' clause executes if the 'for' loop completes without a 'break'.\n            n_exec = M\n            # The final value is the result after M increments.\n            x_float_final = x_n_float\n\n        # Calculate the theoretical exact value and the absolute error.\n        x_exact_val = s_exact + Decimal(n_exec) * h_exact\n        error = np.abs(x_float_final - np.float64(x_exact_val))\n\n        return [hit, n_min, n_exec, x_float_final, np.float64(x_exact_val), error]\n\n    # Define test cases using Decimal for exact representation of inputs.\n    test_cases = [\n        # Test case 1: Canonical failure with decimal step.\n        (Decimal('0.0'), Decimal('0.1'), Decimal('1.0'), 12),\n        # Test case 2: Exactly representable step and reachable target.\n        (Decimal('0.0'), Decimal('0.125'), Decimal('1.0'), 8),\n        # Test case 3: Step does not subdivide the interval.\n        (Decimal('0.0'), Decimal('0.3'), Decimal('1.0'), 10),\n        # Test case 4: Negative step toward zero with decimal step.\n        (Decimal('1.0'), Decimal('-0.1'), Decimal('0.0'), 12),\n        # Test case 5: Increments below one ULP until accumulation.\n        (Decimal('1.0'), Decimal(1) / (Decimal(2)**55), Decimal(1) + Decimal(1) / (Decimal(2)**52), 8)\n    ]\n    \n    all_results = []\n    for case_params in test_cases:\n        s, h, t, M = case_params\n        result = run_case_simulation(s, h, t, M)\n        all_results.append(result)\n\n    # Format the output string to be a list of lists with no spaces.\n    # Each inner list is manually formatted to avoid spaces from default str(list).\n    result_strings = []\n    for res in all_results:\n        # Note: res[0] is a boolean, str(res[0]) is 'True' or 'False'.\n        res_str = f\"[{res[0]},{res[1]},{res[2]},{res[3]},{res[4]},{res[5]}]\"\n        result_strings.append(res_str)\n    \n    final_output = f\"[{','.join(result_strings)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "2447428"}, {"introduction": "Calculating the variance of a dataset is a routine task in data analysis, but different mathematical formulas for variance can have vastly different numerical properties. This exercise guides you to compare the \"one-pass\" raw-moment formula, $\\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$, with the more robust \"two-pass\" centered-moment formula. You will see a stark example of *catastrophic cancellation*, a significant loss of precision that occurs when subtracting two large, nearly-equal numbers, which plagues the one-pass method when the data's standard deviation is small compared to its mean. This practice [@problem_id:2447454] highlights the crucial principle that mathematical equivalence does not guarantee numerical equivalence and trains you to select algorithms based on their stability.", "problem": "You are to implement a complete, runnable program that demonstrates the impact of round-off error, and specifically catastrophic cancellation, in the computation of variance for datasets with a large mean and small deviations, comparing two computational formulas. The foundational base is the definition of variance as the second central moment of a real-valued random variable and the standard floating-point rounding model. Use the following facts as the starting point:\n- The variance of a real-valued random variable $X$ with finite second moment is defined by the second central moment: $\\operatorname{Var}(X) = \\mathbb{E}\\big[(X - \\mu)^2\\big]$, where $\\mu = \\mathbb{E}[X]$.\n- For real numbers, the second raw moment satisfies $\\mathbb{E}[X^2] = \\operatorname{Var}(X) + \\mu^2$.\n- Floating-point arithmetic in the Institute of Electrical and Electronics Engineers (IEEE) binary64 format (often called double precision) approximately obeys the rounding model $ \\operatorname{fl}(a \\,\\circ\\, b) = (a \\,\\circ\\, b)(1 + \\delta)$ with $|\\delta| \\le \\epsilon_{\\text{mach}}$, where $\\epsilon_{\\text{mach}}$ is the machine epsilon, and $\\circ$ is an arithmetic operation. Subtracting nearly equal numbers can cause catastrophic cancellation, whereby significant digits are lost.\n\nYour program must:\n- Construct specified datasets whose elements have a large mean and small deviations.\n- Compute the variance using two methods in IEEE binary64 arithmetic:\n  1. The raw-moment one-pass form: compute $\\mathbb{E}[X]$ and $\\mathbb{E}[X^2]$ and then form $\\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$.\n  2. The centered two-pass form: first compute $\\mu = \\mathbb{E}[X]$, then compute $\\mathbb{E}[(X-\\mu)^2]$ in a second pass.\n- Compute a high-precision reference variance using decimal arithmetic with sufficiently high precision so that round-off in the operations is negligible compared to IEEE binary64. Use the central-moment definition $\\mathbb{E}[(X-\\mu)^2]$ in high precision.\n- Quantify the absolute error of each floating-point method relative to the high-precision reference.\n- Produce a single line of output containing, for each dataset in the test suite and in order, a list of five values: the one-pass variance, the two-pass variance, the high-precision reference variance, the absolute error of the one-pass result, and the absolute error of the two-pass result.\n\nAll datasets are purely numeric; there are no physical units in this problem. Angles are not involved.\n\nTest suite to cover the happy path, boundary emphasis, and edge cases for catastrophic cancellation:\n- Test $1$ (symmetric small deviations around a large mean, integers to avoid input quantization): Let $M = 10^{8}$ and $D = \\{-3,-1,0,1,3\\}$. The dataset is $X = \\{ M + d \\mid d \\in D\\}$. The true variance for exact reals equals the average of squared deviations, which is $4$.\n- Test $2$ (non-symmetric small deviations with nonzero mean of deviations): Let $M = 10^{8}$ and $D = \\{0,1,2,3,4\\}$. The dataset is $X = \\{ M + d \\mid d \\in D\\}$. The true variance for exact reals equals $2$.\n- Test $3$ (larger sample with tiny, smoothly varying deviations): Let $M = 10^{8}$ and $D = \\left\\{ \\frac{k-500}{1000} \\;\\middle|\\; k=0,1,\\dots,999 \\right\\}$. The dataset is $X = \\{ M + d \\mid d \\in D\\}$. The true variance for exact reals is the population variance of this arithmetic grid; it is close to $1/12$ minus the square of the small mean of deviations and must be computed exactly by your high-precision routine.\n- Test $4$ (extreme small deviations near the limit of resolution relative to the mean): Let $M = 10^{8}$ and $D = \\{10^{-8},-10^{-8}\\}$. The dataset is $X = \\{ M + d \\mid d \\in D\\}$. The true variance for exact reals equals $10^{-16}$.\n\nHigh-precision reference requirement:\n- Build the reference using base-ten decimal arithmetic with at least $p = 100$ digits of precision. Construct the dataset using exact decimal values for $M$ and $D$ as specified above (e.g., use $M = 100000000$ exactly and rational deviations such as $(k-500)/1000$ as exact decimals). Compute the population variance using the two-pass central-moment definition $\\mathbb{E}[(X-\\mu)^2]$, where $\\mathbb{E}[\\cdot]$ is the arithmetic mean over the finite set.\n\nFloating-point computations requirement:\n- Use IEEE binary64 (double precision) via standard arrays in a numerical library to compute the one-pass raw-moment and two-pass centered-moment population variances. Do not apply any compensated summation or numerically stabilized tricks; use straightforward means and sums in the obvious ways so that truncation and round-off errors are visible.\n\nFinal output format:\n- Your program should produce a single line of output containing a Python-like list of four inner lists, one per test case in the order Tests $1$ through $4$. Each inner list must have the five floats: $[\\text{var\\_one\\_pass}, \\text{var\\_two\\_pass}, \\text{var\\_ref}, \\text{abs\\_err\\_one}, \\text{abs\\_err\\_two}]$. For example, a syntactically valid output line looks like $[[v_{11},v_{12},v_{13},e_{11},e_{12}],[v_{21},v_{22},v_{23},e_{21},e_{22}],\\dots]$ with numeric values filled in by your program.\n\nThere must be no user input and no external files. The program must fully determine the test data, perform the computations, and print the single required output line. The outputs must be floating-point numbers.", "solution": "The user has presented a problem in computational engineering that requires an analysis of numerical stability in variance calculations. The problem is valid, well-posed, and scientifically grounded. It addresses a fundamental issue in numerical methods: the loss of precision due to catastrophic cancellation in floating-point arithmetic.\n\nThe core task is to compare two formulas for computing the population variance of a dataset $X = \\{x_1, x_2, \\dots, x_N\\}$:\n\n1.  **The one-pass (or raw-moment) formula:** This method is derived from the algebraic identity $\\operatorname{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$. Computationally, it involves a single pass through the data to compute the sum of values and the sum of squares, from which the means are calculated. The formula is:\n    $$ \\sigma^2 = \\frac{1}{N}\\sum_{i=1}^{N} x_i^2 - \\left(\\frac{1}{N}\\sum_{i=1}^{N} x_i\\right)^2 $$\n    While mathematically exact for real numbers, this formula is numerically unstable when the standard deviation $\\sigma$ is small compared to the mean $\\mu = \\mathbb{E}[X]$. The two terms, $\\mathbb{E}[X^2]$ and $(\\mathbb{E}[X])^2$, become very close to each other. Specifically, $\\mathbb{E}[X^2] = \\sigma^2 + \\mu^2$, so the formula amounts to computing $\\sigma^2 = (\\sigma^2 + \\mu^2) - \\mu^2$. When evaluated using finite-precision floating-point arithmetic, such as IEEE binary64, this involves the subtraction of two very large, nearly identical numbers. This operation is a classic example of catastrophic cancellation. The leading digits of the two numbers cancel out, resulting in a loss of most or all significant digits of the small difference. The rounding errors in the computation of $\\mathbb{E}[X^2]$ and $(\\mathbb{E}[X])^2$, which are on the order of $\\mu^2 \\epsilon_{\\text{mach}}$, become the dominant part of the final result, potentially yielding a variance that is highly inaccurate, or even negative.\n\n2.  **The two-pass (or centered-moment) formula:** This method adheres more closely to the definition of variance as the mean of the squared deviations from the mean. It requires two passes over the data.\n    $$ \\mu = \\frac{1}{N}\\sum_{i=1}^{N} x_i $$\n    $$ \\sigma^2 = \\frac{1}{N}\\sum_{i=1}^{N} (x_i - \\mu)^2 $$\n    In the first pass, the mean $\\mu$ is computed. In the second pass, this computed mean is used to find the squared deviations $(x_i - \\mu)^2$, which are then averaged. This method is far more numerically robust. The subtraction $x_i - \\mu$ is still performed, but the result is a set of small numbers (the deviations). Squaring and summing these small numbers does not involve the subtraction of large quantities. The primary source of error is the initial computation of $\\mu$. The error in the computed mean, $\\hat{\\mu}$, propagates to the deviations. However, this error is typically small. For data with a large mean $M$ and small deviations, the error in the computed mean is on the order of $M\\epsilon_{\\text{mach}}$. As long as this error is small relative to the magnitude of the true deviations, the two-pass algorithm yields an accurate result.\n\nFor this problem, a high-precision reference calculation is also required. This will be performed using the Python `decimal` module, with a precision of $p=100$ digits. At this level of precision, the rounding errors are negligible compared to those in standard binary64 floating-point arithmetic, providing a \"ground truth\" against which the two other methods can be compared.\n\nThe program will be structured to process four specific test cases. Each case uses a dataset with a large mean ($M=10^8$) and small deviations, designed to expose the numerical flaws of the one-pass formula.\n\n-   **Test 1  2:** Deviations are small integers. The two-pass method is expected to be highly accurate. The one-pass method is expected to fail due to catastrophic cancellation.\n-   **Test 3:** A larger dataset with smoothly varying, small rational deviations. Similar behavior is expected.\n-   **Test 4:** Deviations are extremely small ($d = \\pm 10^{-8}$), near the resolution limit of binary64 relative to the mean. For $x = M+d$, the value of $d$ is smaller than the smallest possible increment for a number of magnitude $M$ (which is $M \\cdot \\epsilon_{\\text{mach}} \\approx 10^8 \\cdot 2.22 \\times 10^{-16} = 2.22 \\times 10^{-8}$). As a result, $fl(M+d)$ will be rounded to $M$ itself. This demonstrates a different source of error: loss of information in the initial data representation, which will cause both floating-point methods to compute a variance of $0$.\n\nThe implementation will proceed by defining a function that takes a dataset, computes the variance using the three methods (one-pass float, two-pass float, and high-precision reference), calculates the absolute errors for the float methods, and returns the results. This function will be called for each test case, and the collected results will be formatted into the specified output string.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom decimal import Decimal, getcontext\n\ndef solve():\n    \"\"\"\n    Computes and compares variance using three different methods to demonstrate\n    truncation and round-off errors.\n    \"\"\"\n    # Set the precision for decimal arithmetic to 100 digits, as required.\n    getcontext().prec = 100\n\n    def analyze_dataset(M_str: str, D_list: list):\n        \"\"\"\n        Performs the full analysis for a given dataset definition.\n\n        Args:\n            M_str: The large mean component as a string for exact Decimal conversion.\n            D_list: A list of deviation values (as Decimal objects).\n\n        Returns:\n            A list containing the five required values:\n            [var_one_pass, var_two_pass, var_ref, abs_err_one, abs_err_two]\n        \"\"\"\n        # 1. High-precision reference calculation using the decimal module.\n        # This serves as the ground truth.\n        M_dec = Decimal(M_str)\n        X_dec = [M_dec + d for d in D_list]\n        N_dec = Decimal(len(X_dec))\n        \n        # Use two-pass formula for the reference calculation.\n        mu_dec = sum(X_dec) / N_dec\n        var_ref = sum([(x - mu_dec)**2 for x in X_dec]) / N_dec\n\n        # 2. Floating-point calculations using numpy (IEEE binary64).\n        # Construct the dataset using standard float64.\n        # Note: float() conversion from Decimal can introduce small errors,\n        # but the dominant error source is the variance algorithm itself.\n        X_fp = np.array([float(x) for x in X_dec], dtype=np.float64)\n        \n        # Method 1: One-pass (raw-moment) formula. Prone to catastrophic cancellation.\n        # sigma^2 = E[X^2] - (E[X])^2\n        mean_of_squares = np.mean(X_fp**2)\n        square_of_mean = np.mean(X_fp)**2\n        var_one_pass = mean_of_squares - square_of_mean\n        \n        # Method 2: Two-pass (centered-moment) formula. More numerically stable.\n        # sigma^2 = E[(X - E[X])^2]\n        mu_fp = np.mean(X_fp)\n        var_two_pass = np.mean((X_fp - mu_fp)**2)\n        \n        # 3. Quantify absolute errors.\n        abs_err_one = abs(var_one_pass - float(var_ref))\n        abs_err_two = abs(var_two_pass - float(var_ref))\n        \n        return [var_one_pass, var_two_pass, float(var_ref), abs_err_one, abs_err_two]\n\n    # --- Define and run all test cases ---\n    test_cases_defs = [\n        # Test 1: Symmetric small integer deviations. True Var = 4.\n        {'M': '1e8', 'D': [Decimal(s) for s in ['-3', '-1', '0', '1', '3']]},\n        \n        # Test 2: Non-symmetric small integer deviations. True Var = 2.\n        {'M': '1e8', 'D': [Decimal(s) for s in ['0', '1', '2', '3', '4']]},\n        \n        # Test 3: Larger sample with small rational deviations.\n        {'M': '1e8', 'D': [(Decimal(k) - Decimal(500)) / Decimal(1000) for k in range(1000)]},\n        \n        # Test 4: Extremely small deviations at the limit of float64 resolution. True Var = 1e-16.\n        {'M': '1e8', 'D': [Decimal('1e-8'), Decimal('-1e-8')]}\n    ]\n\n    all_results = []\n    for case in test_cases_defs:\n        result = analyze_dataset(case['M'], case['D'])\n        all_results.append(result)\n\n    # Final print statement in the exact required format.\n    # The format is a list of lists, e.g., [[v1,v2,...],[v1,v2,...]]\n    formatted_inner_lists = []\n    for res_list in all_results:\n        # Format each inner list as \"[v1,v2,v3,e1,e2]\"\n        formatted_list_str = f\"[{','.join(map(str, res_list))}]\"\n        formatted_inner_lists.append(formatted_list_str)\n    \n    # Join the inner lists into the final output format \"[ [...], [...], ... ]\"\n    final_output = f\"[{','.join(formatted_inner_lists)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "2447454"}, {"introduction": "When summing a long sequence of numbers, especially in financial applications with millions of small returns, the slow accumulation of round-off error can render the final result meaningless. This hands-on exercise introduces a powerful technique to counteract this problem: compensated summation. You will implement and test the Kahan summation algorithm, which cleverly tracks and reincorporates the low-order bits that are lost in each addition. By comparing its accuracy against a naive summation, you will learn a practical and essential tool [@problem_id:2427731] for writing high-precision code that handles large-scale numerical aggregation reliably.", "problem": "You are given a task in computational economics and finance to quantify the impact of floating-point round-off error when aggregating long series of small asset returns and to mitigate it using a compensated summation algorithm. Consider a sequence of real-valued returns $\\{r_i\\}_{i=1}^n$, where the mathematically exact cumulative return is the real sum $S = \\sum_{i=1}^n r_i$. In actual computation with binary floating-point arithmetic, the operation of addition is modeled by the standard floating-point rounding model: for any two real numbers $a$ and $b$, the computed value of their sum is $\\operatorname{fl}(a+b) = (a+b)(1+\\delta)$ with $|\\delta| \\le u$, where $u$ is the unit round-off, for example $u = 2^{-53}$ for binary64 (double-precision) arithmetic. In long sums with heterogeneous magnitudes and signs, naive left-to-right summation accumulates round-off error and can lose low-order bits, especially when adding very small values to a much larger partial sum, a phenomenon relevant when aggregating small daily returns over long horizons.\n\nYour task is to write a complete, runnable program that:\n- Implements two methods to sum a given ordered sequence of returns in floating-point arithmetic:\n  1. A naive left-to-right summation that accumulates in a single scalar.\n  2. A compensated summation method due to Kahan that tracks and corrects for low-order bits lost to rounding without changing the order of the sequence.\n- Computes a high-precision baseline for the mathematically exact sum $S$ using exact rational arithmetic by interpreting each return as a decimal fraction with a fixed denominator $D = 10^{16}$ and summing the numerators as integers. Specifically, for each decimal return $r_i$ with at most $16$ digits after the decimal point, interpret $r_i$ as the exact rational $\\frac{\\lfloor r_i \\cdot D \\rceil}{D}$, where rounding is exact for the values provided below (no rounding is needed for the given test suite). Then compute $S = \\frac{1}{D}\\sum_{i=1}^n (r_i \\cdot D)$ exactly in integers and convert $S$ to a floating-point number only for the purpose of reporting absolute errors.\n- For each test sequence, reports two absolute errors as floating-point numbers: $|S_{\\text{naive}} - S|$ and $|S_{\\text{Kahan}} - S|$, and a boolean indicating whether the Kahan-compensated error is strictly smaller than the naive error.\n\nImportant instructions:\n- Treat returns as unit-free decimal numbers (do not use a percentage sign). No physical units or angle units are involved.\n- All summations must be performed in the specified order; no reordering is permitted.\n- The final output must aggregate the results for all test cases into a single line containing a comma-separated list enclosed in square brackets. Each element of this list must itself be a list of the form $[e_n, e_k, b]$, where $e_n$ is the absolute error of naive summation (a floating-point number), $e_k$ is the absolute error of Kahan-compensated summation (a floating-point number), and $b$ is a boolean equal to $\\text{True}$ if $e_k  e_n$ and $\\text{False}$ otherwise. For example: $[[e_{n,1}, e_{k,1}, b_1],[e_{n,2}, e_{k,2}, b_2],\\dots]$.\n\nTest suite and coverage:\n- Use the following four ordered sequences, each specified compactly as a list of segments $(v, c)$ meaning the value $v$ appears $c$ times consecutively in that order. All values are decimal strings and all counts are nonnegative integers. Every value has at most $16$ digits after the decimal point, so the common denominator $D = 10^{16}$ is valid for exact rational summation.\n\n  1. Happy-path but cancellation-prone sequence (small increments around large offsets):\n     - $\\left(\"1.0\",\\, 1\\right)$, then $\\left(\"1e-16\",\\, 1000000\\right)$, then $\\left(\"-1.0\",\\, 1\\right)$. The mathematically exact sum is $S = 10^{6}\\cdot 10^{-16} = 10^{-10}$.\n  2. Slightly imbalanced small positive and negative returns around zero:\n     - $\\left(\"1e-8\",\\, 100000\\right)$, then $\\left(\"-1e-8\",\\, 99999\\right)$. The mathematically exact sum is $S = 10^{-8}$.\n  3. Boundary case with all zeros:\n     - $\\left(\"0.0\",\\, 50000\\right)$. The mathematically exact sum is $S = 0$.\n  4. Mixed magnitudes with severe cancellation and many tiny terms:\n     - $\\left(\"1e-16\",\\, 300000\\right)$, then $\\left(\"1.0\",\\, 1\\right)$, then $\\left(\"-1.0\",\\, 1\\right)$, then $\\left(\"1e-16\",\\, 300000\\right)$, then $\\left(\"-1e-16\",\\, 600000\\right)$. The mathematically exact sum is $S = 0$.\n\nRequirements:\n- Implement both summation methods and the exact rational baseline as specified.\n- For each of the four test sequences, compute and report the triple $[|S_{\\text{naive}}-S|,\\, |S_{\\text{Kahan}}-S|,\\, (|S_{\\text{Kahan}}-S|  |S_{\\text{naive}}-S|)]$.\n- Your program should produce a single line of output containing the list of these four triples in the exact format: $[[e_{n,1},e_{k,1},b_1],[e_{n,2},e_{k,2},b_2],[e_{n,3},e_{k,3},b_3],[e_{n,4},e_{k,4},b_4]]$.", "solution": "We begin from the standard floating-point error model for addition. Let $a$ and $b$ be real numbers and suppose we compute their sum in binary floating-point with rounding to nearest. The computed result satisfies\n$$\n\\operatorname{fl}(a+b) \\;=\\; (a+b)(1+\\delta), \\quad |\\delta| \\le u,\n$$\nwhere $u$ is the unit round-off. For the binary64 format, one has $u = 2^{-53} \\approx 1.11 \\times 10^{-16}$. In summing a long sequence $\\{r_i\\}_{i=1}^n$, the naive left-to-right algorithm updates a running total $s$ by $s \\leftarrow \\operatorname{fl}(s + r_i)$ for $i=1,\\dots,n$. The cumulative forward error of naive summation can be bounded by well-known results of backward error analysis: roughly, the absolute error grows on the order of $u$ times the sum of magnitudes, that is\n$$\n|s_{\\text{naive}} - S| \\lesssim \\gamma_n \\sum_{i=1}^n |r_i|, \\quad \\gamma_n = \\frac{nu}{1 - nu},\n$$\nas long as $nu  1$, highlighting an $O(nu)$ growth in error and the sensitivity to the ordering and cancellation structure. In particular, when adding very small $r_i$ into a much larger running total $s$, if $|r_i|  \\tfrac{1}{2}\\operatorname{ulp}(s)$, then $s + r_i$ rounds back to $s$ and the low-order bits of $r_i$ are effectively dropped, a phenomenon that becomes economically relevant in aggregating many small daily returns when the horizon is long and offsets arise from large gains or losses.\n\nTo mitigate this loss, the Kahan compensated summation introduces an auxiliary variable $c$ that tracks a running compensation for the low-order bits lost to rounding. The idea is to pre-subtract the compensation from the next addend and to update the compensation by the rounding error observed in the last addition. Specifically, let $s$ be the running sum and $c$ the compensation, both initialized to $0$. For each addend $x$, define the compensated addend $y = x - c$ and form the tentative sum $t = s + y$. The new compensation is the part of $y$ that was not captured in $t$, which algebraically is $c \\leftarrow (t - s) - y$, and the running sum is updated as $s \\leftarrow t$. In exact arithmetic, $(t - s) = y$, so $c$ would remain zero, but in floating-point arithmetic $(t - s)$ may differ from $y$ by the rounding of $s + y$, and this difference is stored in $c$ to be fed back on the next step. This produces a first-order correction for the lost low-order bits and dramatically reduces the error in many cancellation-prone scenarios, with error often bounded essentially by a constant multiple of $u$ independent of $n$.\n\nFor benchmarking, we require a baseline for the mathematically exact sum $S = \\sum_{i=1}^n r_i$. Since the test sequences are expressed as decimal strings with at most $16$ fractional digits, each $r_i$ is exactly representable as a rational number with denominator $D = 10^{16}$. Therefore, if $r_i$ is written as a decimal string, then $r_i \\cdot D$ is an integer, and the exact sum is\n$$\nS \\;=\\; \\frac{1}{D} \\sum_{i=1}^n \\left(r_i \\cdot D\\right), \\qquad D = 10^{16}.\n$$\nWe can compute the numerator $\\sum_{i=1}^n (r_i \\cdot D)$ exactly using integer arithmetic by parsing each decimal string $r_i$ and multiplying by $D$, and only at the end convert $S$ to a floating-point number for the purpose of computing absolute errors $|S_{\\text{naive}} - S|$ and $|S_{\\text{Kahan}} - S|$.\n\nAlgorithmic design:\n- Implement a generator that iterates over each test sequence in the provided order, yielding floating-point values for the naive and Kahan summations.\n- Implement the naive summation as repeated floating-point addition.\n- Implement the Kahan compensated summation with variables $s$ and $c$ initialized to $0$, and the updates\n  $$\n  y \\leftarrow x - c,\\quad t \\leftarrow s + y,\\quad c \\leftarrow (t - s) - y,\\quad s \\leftarrow t.\n  $$\n- Implement the exact rational baseline by summing integer numerators with denominator $D = 10^{16}$: for each segment $(v, c)$, compute the integer $n_v = v \\cdot D$ exactly and add $c \\cdot n_v$ to the integer accumulator. The exact sum is $S = N / D$ with $N$ the integer total. This is valid for the given test suite because each $v$ has at most $16$ digits after the decimal point, so $v \\cdot D$ is an integer.\n- For each test case, compute the absolute errors $e_n = |S_{\\text{naive}} - S|$ and $e_k = |S_{\\text{Kahan}} - S|$, and the boolean $b = (e_k  e_n)$.\n- Produce the final single-line output as a list of the four triples $[e_n, e_k, b]$ for the four specified sequences, in order.\n\nTest suite interpretation:\n1. Sequence $1$: $\\left(\"1.0\",\\, 1\\right)$, $\\left(\"1e-16\",\\, 1000000\\right)$, $\\left(\"-1.0\",\\, 1\\right)$. Here $S = 10^6 \\cdot 10^{-16} = 10^{-10}$. Naive summation will often lose each $10^{-16}$ when added to $1.0$, yielding a result close to $0$ after the final $-1.0$, while Kahan summation recovers the low-order bits in $c$, yielding a result close to $10^{-10}$.\n2. Sequence $2$: $\\left(\"1e-8\",\\, 100000\\right)$, $\\left(\"-1e-8\",\\, 99999\\right)$. Here $S = 10^{-8}$. Both methods should perform well; the absolute errors are expected to be very small compared to $S$.\n3. Sequence $3$: $\\left(\"0.0\",\\, 50000\\right)$. Here $S = 0$. Both methods should yield exactly $0$ in floating-point arithmetic, resulting in zero absolute errors.\n4. Sequence $4$: $\\left(\"1e-16\",\\, 300000\\right)$, $\\left(\"1.0\",\\, 1\\right)$, $\\left(\"-1.0\",\\, 1\\right)$, $\\left(\"1e-16\",\\, 300000\\right)$, $\\left(\"-1e-16\",\\, 600000\\right)$. Here $S = 0$. The long run of tiny terms interspersed with large cancellation stresses both methods; the compensated method reduces the accumulation of round-off.\n\nThe program must implement these steps and print a single line in the exact format:\n$$\n\\left[\\,[e_{n,1},e_{k,1},b_1],\\,[e_{n,2},e_{k,2},b_2],\\,[e_{n,3},e_{k,3},b_3],\\,[e_{n,4},e_{k,4},b_4]\\,\\right].\n$$", "answer": "```python\nimport math\nfrom decimal import Decimal, getcontext\n\n# No external input; all parameters are embedded per the problem statement.\n\ndef naive_sum(seq_iter):\n    \"\"\"Naive left-to-right summation over an iterator of floats.\"\"\"\n    s = 0.0\n    for x in seq_iter:\n        s += x\n    return s\n\ndef kahan_sum(seq_iter):\n    \"\"\"Kahan compensated summation over an iterator of floats.\"\"\"\n    s = 0.0\n    c = 0.0\n    for x in seq_iter:\n        y = x - c\n        t = s + y\n        c = (t - s) - y\n        s = t\n    return s\n\ndef float_generator_from_segments(segments):\n    \"\"\"\n    Yield floats in the specified order for segments specified as\n    a list of tuples: (value_string, count_int).\n    \"\"\"\n    for v_str, cnt in segments:\n        x = float(v_str)\n        for _ in range(cnt):\n            yield x\n\ndef exact_sum_from_segments(segments, D=10**16):\n    \"\"\"\n    Compute the exact rational sum as N/D where D=10^16, by summing integer numerators.\n    Each value v_str must be a decimal with at most 16 digits after the decimal point.\n    Returns the exact sum as a float for error comparison.\n    \"\"\"\n    # Use high precision Decimal to convert values exactly, then multiply by D to get integer.\n    getcontext().prec = 50\n    D_dec = Decimal(D)\n    N = 0  # integer numerator\n    for v_str, cnt in segments:\n        v_dec = Decimal(v_str)\n        n_v = int((v_dec * D_dec).to_integral_exact())  # exact integer for given test suite\n        if cnt:\n            N += n_v * cnt\n    # Convert to float for error measurement; loss here is negligible vs measured errors\n    return N / D\n\ndef compute_case(segments):\n    \"\"\"Compute absolute errors for naive and Kahan sums against exact rational baseline.\"\"\"\n    true_sum = exact_sum_from_segments(segments)\n    nsum = naive_sum(float_generator_from_segments(segments))\n    ksum = kahan_sum(float_generator_from_segments(segments))\n    en = abs(nsum - true_sum)\n    ek = abs(ksum - true_sum)\n    return [en, ek, ek  en]\n\ndef solve():\n    # Define the test cases as per the problem statement.\n    test_cases = [\n        # 1) (\"1.0\", 1), (\"1e-16\", 1000000), (\"-1.0\", 1)\n        [(\"1.0\", 1), (\"1e-16\", 1_000_000), (\"-1.0\", 1)],\n        # 2) (\"1e-8\", 100000), (\"-1e-8\", 99999)\n        [(\"1e-8\", 100_000), (\"-1e-8\", 99_999)],\n        # 3) (\"0.0\", 50000)\n        [(\"0.0\", 50_000)],\n        # 4) (\"1e-16\", 300000), (\"1.0\", 1), (\"-1.0\", 1), (\"1e-16\", 300000), (\"-1e-16\", 600000)\n        [(\"1e-16\", 300_000), (\"1.0\", 1), (\"-1.0\", 1), (\"1e-16\", 300_000), (\"-1e-16\", 600_000)],\n    ]\n\n    results = []\n    for segments in test_cases:\n        res = compute_case(segments)\n        results.append(res)\n\n    # Print in the exact required single-line format.\n    # Convert booleans and floats to their standard string representations.\n    def fmt(item):\n        if isinstance(item, list):\n            return \"[\" + \",\".join(fmt(x) for x in item) + \"]\"\n        if isinstance(item, bool):\n            return \"True\" if item else \"False\"\n        # float or int\n        return str(item)\n\n    print(fmt(results))\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2427731"}]}