{"hands_on_practices": [{"introduction": "The power of the Bulirsch-Stoer method comes from Richardson extrapolation, a technique for accelerating the convergence of a sequence of approximations. To understand this core mechanism in isolation, we first apply it not to a differential equation, but to the familiar problem of summing a geometric series. This exercise demonstrates how to combine several low-accuracy partial sums to produce a dramatically improved estimate of the infinite sum, providing a concrete feel for the mathematics of extrapolation before we embed it within an ODE solver.", "problem": "Let $x$ be a real parameter satisfying $|x|1$. Define the geometric series sum $S(x)$ and its $N$-th partial sum $S_{N}(x)$ by\n$$\nS(x) = \\sum_{k=0}^{\\infty} x^{k}, \\qquad S_{N}(x) = \\sum_{k=0}^{N} x^{k}.\n$$\nConsider the mapping $h \\mapsto S(h)$ defined on the discrete set $\\{h = 1/N : N \\in \\mathbb{N}\\}$ by $S(h) := S_{1/h}(x) = S_{N}(x)$, and think of $h$ as a step-size parameter. In the spirit of the Bulirsch–Stoer extrapolation method, construct the unique quadratic polynomial $P_{2}(h)$ that interpolates the three data points $\\big(h_{1},S(h_{1})\\big)$, $\\big(h_{2},S(h_{2})\\big)$, and $\\big(h_{3},S(h_{3})\\big)$ at\n$$\nh_{1}=\\frac{1}{N}, \\qquad h_{2}=\\frac{1}{2N}, \\qquad h_{3}=\\frac{1}{3N},\n$$\nwhere $N \\in \\mathbb{N}$ is fixed. Determine $P_{2}(0)$ in a fully simplified, closed-form analytic expression in terms of $x$ and $N$ only.\n\nAnswer specification:\n- Provide your final answer as a single closed-form analytic expression.\n- No rounding is required and no units are involved.", "solution": "The problem statement must first be validated for scientific and logical integrity.\n\nStep 1: Extract Givens\n- A real parameter $x$ with the constraint $|x|1$.\n- The geometric series sum is defined as $S(x) = \\sum_{k=0}^{\\infty} x^{k}$.\n- The $N$-th partial sum is defined as $S_{N}(x) = \\sum_{k=0}^{N} x^{k}$.\n- A function $S(h)$ is defined on a discrete set of points $h = 1/M$ for $M \\in \\mathbb{N}$ by the relation $S(h) := S_{1/h}(x)$.\n- A unique quadratic polynomial, denoted $P_{2}(h)$, interpolates three specific points:\n  - $(h_{1}, S(h_{1}))$ where $h_{1} = \\frac{1}{N}$\n  - $(h_{2}, S(h_{2}))$ where $h_{2} = \\frac{1}{2N}$\n  - $(h_{3}, S(h_{3}))$ where $h_{3} = \\frac{1}{3N}$\n- The integer $N$ is a fixed natural number, $N \\in \\mathbb{N}$.\n- The objective is to determine the value of this polynomial at $h=0$, i.e., $P_{2}(0)$, as a simplified, closed-form analytic expression in terms of $x$ and $N$.\n\nStep 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is formulated within the standard mathematical framework of analysis and numerical methods. Geometric series, partial sums, and polynomial interpolation are fundamental concepts. The idea of extrapolating a sequence of approximations to its limit, which is central to the Bulirsch–Stoer method, is a well-established technique in computational science. The condition $|x|1$ correctly ensures the convergence of the geometric series. The problem is scientifically sound.\n- **Well-Posed:** A unique quadratic polynomial is determined by three distinct points. For any $N \\in \\mathbb{N}$, the values $h_{1} = 1/N$, $h_{2} = 1/(2N)$, and $h_{3} = 1/(3N)$ are distinct. Therefore, a unique interpolating polynomial $P_2(h)$ exists. The task is to find its value at a specific point, $h=0$, which is a uniquely defined quantity. The problem is well-posed.\n- **Objective:** The problem statement is precise, quantitative, and devoid of subjective or ambiguous language.\n\nStep 3: Verdict and Action\nThe problem is valid. It is a well-defined mathematical exercise in polynomial interpolation and extrapolation. I will proceed with the solution.\n\nThe task is to find the value $P_{2}(0)$ where $P_{2}(h)$ is the unique quadratic polynomial that passes through the three points $(h_1, A_1)$, $(h_2, A_2)$, and $(h_3, A_3)$, where $h_k$ are the given step sizes and $A_k = S(h_k)$ are the corresponding values of the partial sums.\nLet $h_1 = \\frac{1}{N}$, $h_2 = \\frac{1}{2N}$, and $h_3 = \\frac{1}{3N}$.\nThe corresponding function values are:\n$A_1 = S(h_1) = S_N(x) = \\sum_{k=0}^{N} x^k$\n$A_2 = S(h_2) = S_{2N}(x) = \\sum_{k=0}^{2N} x^k$\n$A_3 = S(h_3) = S_{3N}(x) = \\sum_{k=0}^{3N} x^k$\n\nWe seek to evaluate the interpolating polynomial at $h=0$. This is an extrapolation, as $h=0$ lies outside the interval defined by the interpolation points. A direct method is to use the Lagrange interpolation formula for $P_2(h)$ and then set $h=0$.\nThe Lagrange form of the interpolating polynomial is:\n$$\nP_2(h) = A_1 \\frac{(h-h_2)(h-h_3)}{(h_1-h_2)(h_1-h_3)} + A_2 \\frac{(h-h_1)(h-h_3)}{(h_2-h_1)(h_2-h_3)} + A_3 \\frac{(h-h_1)(h-h_2)}{(h_3-h_1)(h_3-h_2)}\n$$\nEvaluating at $h=0$, we obtain:\n$$\nP_2(0) = A_1 \\frac{h_2 h_3}{(h_1-h_2)(h_1-h_3)} + A_2 \\frac{h_1 h_3}{(h_2-h_1)(h_2-h_3)} + A_3 \\frac{h_1 h_2}{(h_3-h_1)(h_3-h_2)}\n$$\nThis expresses $P_2(0)$ as a linear combination of $A_1$, $A_2$, and $A_3$. We must compute the coefficients. Let $h_0 = 1/N$ for simplicity. Then $h_1=h_0$, $h_2=h_0/2$, $h_3=h_0/3$.\n\nThe coefficient of $A_1$:\n$$\nc_1 = \\frac{(\\frac{h_0}{2})(\\frac{h_0}{3})}{(h_0-\\frac{h_0}{2})(h_0-\\frac{h_0}{3})} = \\frac{\\frac{1}{6}h_0^2}{(\\frac{1}{2}h_0)(\\frac{2}{3}h_0)} = \\frac{\\frac{1}{6}h_0^2}{\\frac{1}{3}h_0^2} = \\frac{1}{2}\n$$\nThe coefficient of $A_2$:\n$$\nc_2 = \\frac{(h_0)(\\frac{h_0}{3})}{(\\frac{h_0}{2}-h_0)(\\frac{h_0}{2}-\\frac{h_0}{3})} = \\frac{\\frac{1}{3}h_0^2}{(-\\frac{1}{2}h_0)(\\frac{1}{6}h_0)} = \\frac{\\frac{1}{3}h_0^2}{-\\frac{1}{12}h_0^2} = -4\n$$\nThe coefficient of $A_3$:\n$$\nc_3 = \\frac{(h_0)(\\frac{h_0}{2})}{(\\frac{h_0}{3}-h_0)(\\frac{h_0}{3}-\\frac{h_0}{2})} = \\frac{\\frac{1}{2}h_0^2}{(-\\frac{2}{3}h_0)(-\\frac{1}{6}h_0)} = \\frac{\\frac{1}{2}h_0^2}{\\frac{1}{9}h_0^2} = \\frac{9}{2}\n$$\nThus, the extrapolated value is:\n$$\nP_2(0) = \\frac{1}{2}A_1 - 4A_2 + \\frac{9}{2}A_3\n$$\nThe condition that the sum of coefficients is $c_1+c_2+c_3 = \\frac{1}{2}-4+\\frac{9}{2} = 5-4=1$ is satisfied, as expected for an extrapolation formula.\n\nNext, we substitute the expressions for $A_k = S_{kN}(x)$. The sum of a finite geometric series is given by $S_M(x) = \\frac{1-x^{M+1}}{1-x}$.\nThe terms are:\n$A_1 = S_N(x) = \\frac{1-x^{N+1}}{1-x}$\n$A_2 = S_{2N}(x) = \\frac{1-x^{2N+1}}{1-x}$\n$A_3 = S_{3N}(x) = \\frac{1-x^{3N+1}}{1-x}$\n\nSubstituting these into the expression for $P_2(0)$:\n$$\nP_2(0) = \\frac{1}{2}\\left(\\frac{1-x^{N+1}}{1-x}\\right) - 4\\left(\\frac{1-x^{2N+1}}{1-x}\\right) + \\frac{9}{2}\\left(\\frac{1-x^{3N+1}}{1-x}\\right)\n$$\nWe can factor out the common denominator $\\frac{1}{1-x}$:\n$$\nP_2(0) = \\frac{1}{1-x} \\left[ \\frac{1}{2}(1-x^{N+1}) - 4(1-x^{2N+1}) + \\frac{9}{2}(1-x^{3N+1}) \\right]\n$$\nNow, we simplify the expression inside the brackets. First, collect the constant terms:\n$$\n\\frac{1}{2} - 4 + \\frac{9}{2} = \\frac{10}{2} - 4 = 5 - 4 = 1\n$$\nNext, collect the terms involving powers of $x$:\n$$\n-\\frac{1}{2}x^{N+1} + 4x^{2N+1} - \\frac{9}{2}x^{3N+1}\n$$\nCombining these results, the expression in the brackets becomes:\n$$\n1 - \\frac{1}{2}x^{N+1} + 4x^{2N+1} - \\frac{9}{2}x^{3N+1}\n$$\nTherefore, the final closed-form expression for $P_2(0)$ is:\n$$\nP_2(0) = \\frac{1 - \\frac{1}{2}x^{N+1} + 4x^{2N+1} - \\frac{9}{2}x^{3N+1}}{1-x}\n$$\nThis expression is a function of only $x$ and $N$ and is in its simplest analytical form.\nAs a check, in the limit $N \\to \\infty$, since $|x|1$, we have $x^{N+1} \\to 0$, $x^{2N+1} \\to 0$, and $x^{3N+1} \\to 0$. The expression for $P_2(0)$ approaches $\\frac{1}{1-x}$, which is the true sum of the series $S(x)$. This is the correct behavior for a valid extrapolation scheme.", "answer": "$$\n\\boxed{\\frac{1 - \\frac{1}{2}x^{N+1} + 4x^{2N+1} - \\frac{9}{2}x^{3N+1}}{1-x}}\n$$", "id": "2378481"}, {"introduction": "Having grasped the essence of extrapolation, we now integrate it into a full-featured solver. This practice challenges you to implement the complete Bulirsch-Stoer algorithm, combining the symmetric modified midpoint method with adaptive step-sizing and Richardson extrapolation. By simulating the helical motion of a charged particle in a magnetic field, you will see the method's remarkable efficiency and precision firsthand, while also investigating whether it conserves physical quantities like the orbital radius.", "problem": "Implement a complete, runnable program that integrates the motion of a charged particle in a uniform magnetic field using the Bulirsch–Stoer (BS) extrapolation method. The physical model must be based on the Lorentz force law and the kinematic definition of velocity as time derivative of position. Use the following fundamental laws as the base:\n\n- The Lorentz force: $\\mathbf{F} = q\\,\\mathbf{v} \\times \\mathbf{B}$.\n- Newton's second law: $m\\,\\dfrac{d\\mathbf{v}}{dt} = \\mathbf{F}$.\n- Kinematics: $\\dfrac{d\\mathbf{x}}{dt} = \\mathbf{v}$.\n\nAssume a spatially uniform, time-independent magnetic field $\\mathbf{B}$ pointing along the $z$-axis, i.e., $\\mathbf{B} = (0,0,B_0)$, with no electric field. Let the particle mass be $m$ in kilograms, charge $q$ in coulombs, position $\\mathbf{x}(t)$ in meters, velocity $\\mathbf{v}(t)$ in meters per second, and time $t$ in seconds. All physical quantities must be treated in the International System of Units (SI). All angles, where applicable, must be in radians.\n\nYour program must implement the Bulirsch–Stoer extrapolation algorithm with the modified midpoint method and Richardson extrapolation to advance the state $(\\mathbf{x},\\mathbf{v})$ in time. The implementation must be from first principles following the above fundamental definitions, without using any pre-built ordinary differential equation (ODE) solvers. Clarify that Ordinary Differential Equation (ODE) refers to a system of equations of the form $\\dfrac{d\\mathbf{y}}{dt} = \\mathbf{f}(t,\\mathbf{y})$. The code must be numerically stable and must adapt step sizes to meet a relative tolerance parameter $r_{\\text{tol}}$.\n\nThe exact circular motion in a uniform magnetic field has a constant Larmor radius (gyroradius). For $\\mathbf{B} = (0,0,B_0)$, the guiding-center position is\n$$\n\\mathbf{R}_{\\mathrm{gc}} = \\mathbf{x} + \\frac{m}{q B_0^2}\\,\\mathbf{v} \\times \\mathbf{B},\n$$\nwhich is constant in time under exact dynamics, and the orbital radius is\n$$\nr_{\\mathrm{L}} = \\left\\| \\mathbf{x} - \\mathbf{R}_{\\mathrm{gc}} \\right\\|.\n$$\nUse this to assess whether the numerical method conserves the orbital radius exactly.\n\nTest suite specification (all constants and initial conditions in SI units):\n- Use electron parameters $q = -1.602176634\\times 10^{-19}$ C and $m = 9.1093837015\\times 10^{-31}$ kg, magnetic field magnitude $B_0 = 1$ Tesla, initial position $\\mathbf{x}(0) = (0,0,0)$ meters, initial velocity $\\mathbf{v}(0) = (1.0\\times 10^{6}, 0, 0)$ meters per second. Define the cyclotron frequency $\\omega_c = \\dfrac{|q| B_0}{m}$ and cyclotron period $T = \\dfrac{2\\pi}{\\omega_c}$, where $\\pi$ is the circle constant.\n\n- Your program must evaluate the trajectory at uniformly spaced times and compute the maximum relative deviation of the radius from the initial Larmor radius,\n$$\n\\max_{t \\in [0,t_{\\text{final}}]} \\frac{\\left| r_{\\mathrm{L}}(t) - r_{\\mathrm{L}}(0) \\right|}{r_{\\mathrm{L}}(0)},\n$$\nusing a fixed number of sample points per period. The relative deviation is dimensionless; report it as a floating-point number.\n\nProvide four test cases to probe different aspects:\n- Test A (happy path): $t_{\\text{final}} = T$ with $r_{\\text{tol}} = 10^{-10}$, using $400$ samples per period.\n- Test B (tighter tolerance): $t_{\\text{final}} = T$ with $r_{\\text{tol}} = 10^{-12}$, using $400$ samples per period.\n- Test C (looser tolerance): $t_{\\text{final}} = T$ with $r_{\\text{tol}} = 10^{-8}$, using $400$ samples per period.\n- Test D (accumulation over many periods): $t_{\\text{final}} = 10\\,T$ with $r_{\\text{tol}} = 10^{-10}$, using $400$ samples per period.\n\nAll computations of $T$ must use radians. The final outputs for the four tests are the four floating-point values of the maximum relative radius deviation for Tests A–D in the order specified above. Additionally, compute a boolean that answers the conceptual question: “Does the Bulirsch–Stoer method conserve the circular-orbit radius exactly?” Define this boolean to be true if and only if all four computed maximum relative deviations are exactly equal to $0$ within machine arithmetic, and false otherwise.\n\nFinal output format requirement:\n- Your program should produce a single line of output containing a list with the five results in order, enclosed in square brackets and comma-separated, for example, $[a,b,c,d,e]$, where $a$, $b$, $c$, and $d$ are floating-point numbers (maximum relative deviations for Tests A–D), and $e$ is a boolean answering the conservation question as defined above. No other text should be printed.\n\nThe program must be fully self-contained and runnable as is, with no user input. Ensure scientific realism and numerical consistency in all chosen values and computations. Express all physical parameters in SI units, and produce dimensionless outputs for the relative deviations. The conceptual justification of whether exact conservation is achieved will be assessed separately; here you must compute the numerical evidence and the boolean as specified.", "solution": "The user requests the implementation of the Bulirsch-Stoer (BS) extrapolation method to solve for the trajectory of a charged particle in a uniform magnetic field. The problem is scientifically sound, well-posed, and provides all necessary information for a complete solution.\n\nThe motion of a particle with mass $m$ and charge $q$ is governed by Newton's second law, where the force is the Lorentz force. Given a uniform magnetic field $\\mathbf{B} = (0, 0, B_0)$ and no electric field, the system is described by:\n$$ \\frac{d\\mathbf{x}}{dt} = \\mathbf{v} $$\n$$ m\\,\\frac{d\\mathbf{v}}{dt} = q(\\mathbf{v} \\times \\mathbf{B}) $$\nThis forms a system of first-order ordinary differential equations (ODEs). Let the state vector be $\\mathbf{y}(t) = (\\mathbf{x}(t), \\mathbf{v}(t)) = (x, y, z, v_x, v_y, v_z)$. The system can be written as $\\frac{d\\mathbf{y}}{dt} = \\mathbf{f}(t, \\mathbf{y})$. The components of $\\mathbf{f}$ are:\n$$\n\\mathbf{f}(t, \\mathbf{y}) =\n\\begin{pmatrix}\nv_x \\\\\nv_y \\\\\nv_z \\\\\n\\frac{q B_0}{m} v_y \\\\\n-\\frac{q B_0}{m} v_x \\\\\n0\n\\end{pmatrix}\n$$\nThe dimensionless quantity $\\omega_c = \\frac{|q| B_0}{m}$ is the cyclotron frequency. The problem can be solved by numerically integrating this system of ODEs from the initial state $\\mathbf{y}(0) = (\\mathbf{x}(0), \\mathbf{v}(0))$.\n\nThe Bulirsch-Stoer method is an advanced numerical technique for solving ODEs that combines three key ideas:\n1.  **Modified Midpoint Method**: This method is used to obtain a low-order approximation of the solution over a large interval $H$ from $t$ to $t+H$. It does so by dividing $H$ into $n$ smaller substeps of size $h = H/n$. The sequence of states $z_0, z_1, \\dots, z_n$ is computed as follows:\n    $$ z_0 = \\mathbf{y}(t) $$\n    $$ z_1 = z_0 + h \\cdot \\mathbf{f}(t, z_0) $$\n    $$ z_{m+1} = z_{m-1} + 2h \\cdot \\mathbf{f}(t+mh, z_m) \\quad \\text{for } m=1, \\dots, n-1 $$\n    A final smoothing step yields the estimate for $\\mathbf{y}(t+H)$:\n    $$ \\mathbf{y}_n(H) = \\frac{1}{2}\\left(z_n + z_{n-1} + h \\cdot \\mathbf{f}(t+H, z_n)\\right) $$\n    The error of this method has a series expansion in even powers of the substep size, i.e., $\\mathbf{y}_{true}(t+H) - \\mathbf{y}_n(H) = \\sum_{i=1}^{\\infty} c_i h^{2i} = \\sum_{i=1}^{\\infty} c_i (H/n)^{2i}$.\n\n2.  **Richardson Extrapolation**: This technique leverages the known error structure to systematically eliminate error terms. We compute estimates $\\mathbf{y}_n(H)$ for a sequence of increasing substep counts, e.g., $n \\in \\{2, 4, 6, 8, \\dots\\}$, and then extrapolate the results to the limit $n \\to \\infty$ (or equivalently, $h \\to 0$). This is a form of polynomial extrapolation in the variable $h^2$. We construct a tableau, often using Neville's algorithm, where the diagonal entries represent progressively higher-order approximations. Let $T_{k,1}$ be the result from the modified midpoint method with $n_k$ substeps. The extrapolation tableau is:\n    $$ T_{k,j} = T_{k,j-1} + \\frac{T_{k,j-1} - T_{k-1,j-1}}{(n_k/n_{k-j+1})^2 - 1} $$\n    The entry $T_{k,k}$ is a high-order estimate of the true solution.\n\n3.  **Adaptive Step-Size Control**: The BS method is computationally intensive for a single step. To maintain efficiency, the overall step size $H$ is adapted to meet a specified tolerance with minimal effort. After a step from $t$ to $t+H$ is successfully computed using $k$ extrapolations, yielding a solution $T_{k,k}$ with an estimated error $\\epsilon$, the next step size $H_{next}$ is chosen to keep the error per unit step near the desired tolerance. A common control strategy is:\n    $$ H_{next} = H \\cdot S \\cdot \\left(\\frac{\\text{tol}}{\\epsilon}\\right)^{1/(2k+1)} $$\n    where $S$ is a safety factor (e.g., $0.9$), `tol` is the target tolerance for the step, and $2k$ is the approximate order of the method. If a step fails (error exceeds tolerance), it is recomputed with a smaller $H$.\n\nThe orbital radius, or Larmor radius, for motion in the $xy$-plane is given by $r_L = \\frac{v_\\perp}{\\omega_c} = \\frac{\\sqrt{v_x^2 + v_y^2}}{\\omega_c}$. Analytically, for this system, both kinetic energy and the magnitude of perpendicular velocity are conserved, meaning $r_L$ is a constant of motion. However, the BS method, being a general-purpose numerical integrator, is not a \"geometric integrator\" and does not inherently conserve such quantities exactly. The numerical solution will exhibit small deviations from the true constant radius due to truncation and floating-point errors. The magnitude of this deviation serves as a practical measure of the method's accuracy and stability. We compute the maximum relative deviation to test the implementation. The conceptual question of exact conservation is answered by checking if these numerically computed deviations are identically zero, which is not expected.\n\nThe implementation will consist of a driver function that manages the adaptive stepping from one user-specified output time to the next. At its core, this driver calls a function that performs a single Bulirsch-Stoer step, which in turn uses the modified midpoint method and Richardson extrapolation until the local error tolerance is met. The Larmor radius is calculated at each output point to determine the maximum relative error over the entire integration interval.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the Bulirsch-Stoer simulation for all test cases.\n    \"\"\"\n    # Physical constants (SI units)\n    Q = -1.602176634e-19  # Electron charge in Coulombs\n    M = 9.1093837015e-31   # Electron mass in kg\n    B0 = 1.0               # Magnetic field in Tesla\n\n    # Initial conditions (SI units)\n    X0 = np.array([0.0, 0.0, 0.0])\n    V0 = np.array([1.0e6, 0.0, 0.0])\n    Y0 = np.concatenate([X0, V0])\n\n    # Derived parameters\n    OMEGA_C = np.abs(Q) * B0 / M\n    T_PERIOD = 2.0 * np.pi / OMEGA_C\n\n    # ODE function: dy/dt = f(t, y)\n    q_over_m = Q / M\n    def lorentz_force_ode(t, y):\n        \"\"\"\n        Describes the motion of a charged particle in a uniform B-field.\n        y = [x, y, z, vx, vy, vz]\n        \"\"\"\n        vx, vy = y[3], y[4]\n        ax = q_over_m * B0 * vy\n        ay = -q_over_m * B0 * vx\n        return np.array([y[3], y[4], y[5], ax, ay, 0.0])\n\n    # Test cases from the problem statement.\n    # (t_final_factor, rtol, samples_per_period)\n    test_cases_spec = [\n        (1.0, 1e-10, 400),  # Test A\n        (1.0, 1e-12, 400),  # Test B\n        (1.0, 1e-8, 400),   # Test C\n        (10.0, 1e-10, 400), # Test D\n    ]\n\n    all_results = []\n    \n    # --- Bulirsch-Stoer Implementation ---\n\n    def _modified_midpoint(func, y0, t, H, n):\n        \"\"\"\n        Performs n steps of the modified midpoint method over interval H.\n        \"\"\"\n        h = H / n\n        \n        # First step\n        z0 = y0\n        k1 = func(t, z0)\n        z1 = z0 + h * k1\n        \n        # Main loop\n        z_prev = z0\n        z_curr = z1\n        t_curr = t + h\n        for _ in range(1, n):\n            k = func(t_curr, z_curr)\n            z_next = z_prev + 2.0 * h * k\n            z_prev = z_curr\n            z_curr = z_next\n            t_curr += h\n        \n        # Final smoothing step\n        k_final = func(t + H, z_curr)\n        y_final = 0.5 * (z_prev + z_curr + h * k_final)\n        \n        return y_final\n\n    def _bs_step(func, y, t, H_try, rtol, atol_abs):\n        \"\"\"\n        Performs a single adaptive Bulirsch-Stoer step.\n        \"\"\"\n        MAX_K = 8  # Maximum order of extrapolation\n        N_SEQ = [2 * (k + 1) for k in range(MAX_K)]  # Sequence of substeps: 2, 4, 6,...\n        \n        table = np.zeros((MAX_K, MAX_K, len(y)))\n        \n        for k in range(MAX_K):\n            n_k = N_SEQ[k]\n            y_est = _modified_midpoint(func, y, t, H_try, n_k)\n            table[k, 0] = y_est\n\n            # Richardson extrapolation (Neville's algorithm)\n            for j in range(1, k + 1):\n                n_k_minus_j = N_SEQ[k-j]\n                factor = (n_k / n_k_minus_j)**2\n                num = table[k, j-1] - table[k-1, j-1]\n                den = factor - 1.0\n                table[k, j] = table[k, j-1] + num / den\n\n            # Check for convergence\n            y_new = table[k, k]\n            \n            # error estimation\n            if k == 0:\n                err_est = np.inf\n            else:\n                y_prev = table[k-1, k-1]\n                err_est = np.linalg.norm(y_new - y_prev)\n\n            atol = atol_abs\n            y_scale = np.linalg.norm(y) \n            tol = atol + rtol * y_scale\n\n            if err_est  tol:\n                # Converged\n                order = 2 * (k + 1)\n                return y_new, err_est, order, True\n\n        # Did not converge\n        return y, np.inf, 0, False\n\n    def integrate_bs(func, y0, t_eval, rtol):\n        \"\"\"\n        Main Bulirsch-Stoer ODE integrator with adaptive step size.\n        \"\"\"\n        t_start = t_eval[0]\n        t_end = t_eval[-1]\n        \n        t = t_start\n        y = y0\n        \n        results = [y0]\n        \n        # Initial step size guess (heuristic)\n        H = 0.1 * (t_end - t_start)\n        \n        # Safety and step control factors\n        S = 0.9 \n        MIN_FACTOR = 0.2\n        MAX_FACTOR = 5.0\n\n        for i in range(1, len(t_eval)):\n            t_out = t_eval[i]\n            while t  t_out:\n                H_try = min(H, t_out - t)\n                \n                # Absolute tolerance for a step is relative to current state\n                atol_step = rtol * np.linalg.norm(y)\n\n                y_new, err, order, success = _bs_step(func, y, t, H_try, rtol, atol_step)\n\n                if success:\n                    # Step successful, update state and next step size\n                    t += H_try\n                    y = y_new\n                    \n                    if err == 0.0:\n                        factor = MAX_FACTOR\n                    else:\n                        factor = S * (atol_step / err)**(1.0 / (order + 1))\n                    \n                    H = H_try * min(MAX_FACTOR, max(MIN_FACTOR, factor))\n\n                else:\n                    # Step failed, reduce step size and retry\n                    H = H_try * 0.25\n        \n            results.append(y)\n\n        return np.array(results)\n\n    # Execute test cases\n    deviations = []\n    for t_factor, rtol, samples_per_period in test_cases_spec:\n        t_final = t_factor * T_PERIOD\n        num_samples = int(samples_per_period * t_factor) + 1\n        t_eval = np.linspace(0.0, t_final, num_samples)\n        \n        y_traj = integrate_bs(lorentz_force_ode, Y0, t_eval, rtol)\n\n        # Calculate Larmor radius at each sample point\n        v_perp_sq = y_traj[:, 3]**2 + y_traj[:, 4]**2\n        rL_t = np.sqrt(v_perp_sq) / OMEGA_C\n\n        # Calculate initial Larmor radius\n        rL0 = rL_t[0]\n\n        # Calculate max relative deviation\n        if rL0 == 0.0:\n            max_rel_dev = np.max(np.abs(rL_t))\n        else:\n            max_rel_dev = np.max(np.abs(rL_t - rL0) / rL0)\n        \n        deviations.append(max_rel_dev)\n\n    # Conceptual question: Does the BS method conserve the radius exactly?\n    # True if and only if all deviations are *exactly* 0.0\n    is_conserved_exactly = all(d == 0.0 for d in deviations)\n\n    final_results = deviations + [is_conserved_exactly]\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n```", "id": "2378480"}, {"introduction": "No numerical method is universally applicable, and a true expert understands a tool's limitations as well as its strengths. This final practice explores a critical edge case: an ordinary differential equation that violates the Lipschitz condition, a key requirement for guaranteeing a unique solution. Investigating how a high-precision solver behaves when faced with this problem will reveal the profound importance of the underlying mathematical theory and teach you to identify situations where even the most sophisticated algorithms can fail.", "problem": "Consider the initial value problem for an Ordinary Differential Equation (ODE) given by $y^{\\prime}(t) = \\sqrt{y(t)}$ for $t \\ge 0$ with the initial condition $y(0)=0$. Let the smooth positive branch be defined as the family of functions $y(t) = \\left(\\dfrac{t + C}{2}\\right)^{2}$ with constant $C \\in \\mathbb{R}$ chosen to satisfy the given initial condition. For a general initial condition at $t=t_{0}$ with value $y(t_{0})=y_{0} \\ge 0$, select $C = 2\\sqrt{y_{0}} - t_{0}$ so that the branch is $y(t) = \\left(\\dfrac{t + 2\\sqrt{y_{0}} - t_{0}}{2}\\right)^{2}$ for all $t \\ge t_{0}$. Design a high-accuracy time integration program to approximate the terminal value $y(T)$ for the test suite described below, using a deterministic method of your choice that controls the absolute error to a specified tolerance. Your program must also compute, for each test case, the absolute deviation between the computed $y(T)$ and the smooth positive branch value evaluated at $T$ with the constant $C$ determined by the corresponding initial condition. No physical units are involved in this problem.\n\nTest Suite:\n- Case A (locking at the equilibrium): $t_{0}=0$, $y_{0}=0$, $T=1$, absolute tolerance $10^{-12}$.\n- Case B (vanishingly small positive start): $t_{0}=0$, $y_{0}=10^{-12}$, $T=1$, absolute tolerance $10^{-12}$.\n- Case C (away from the non-Lipschitz point): $t_{0}=10^{-6}$, $y_{0}=\\dfrac{(10^{-6})^{2}}{4}$, $T=1$, absolute tolerance $10^{-12}$.\n- Case D (short-time target at the equilibrium): $t_{0}=0$, $y_{0}=0$, $T=10^{-3}$, absolute tolerance $10^{-12}$.\n\nFor each case, your program must return two real numbers: the computed $y(T)$ and the absolute deviation from the smooth positive branch value at $T$. The final output must be a single line containing a comma-separated flat list of these eight numbers in the order A then B then C then D, each reported as a floating-point number. Concretely, your program should produce a single line of output containing\n$[y_{A}(T),\\ \\lvert y_{A}(T)-y^{\\mathrm{branch}}_{A}(T)\\rvert,\\ y_{B}(T),\\ \\lvert y_{B}(T)-y^{\\mathrm{branch}}_{B}(T)\\rvert,\\ y_{C}(T),\\ \\lvert y_{C}(T)-y^{\\mathrm{branch}}_{C}(T)\\rvert,\\ y_{D}(T),\\ \\lvert y_{D}(T)-y^{\\mathrm{branch}}_{D}(T)\\rvert]$,\nwhere $y^{\\mathrm{branch}}(T) = \\left(\\dfrac{T + 2\\sqrt{y_{0}} - t_{0}}{2}\\right)^{2}$ for the corresponding $(t_{0},y_{0},T)$ of each case. The angle unit is not applicable. Percentages are not applicable; all quantities are real numbers.", "solution": "The problem statement is valid. It presents a well-defined numerical experiment concerning the solution of an Ordinary Differential Equation (ODE).\n\nThe core of this problem lies in the behavior of numerical integrators when faced with an ODE that violates the Lipschitz condition. The given ODE is $y^{\\prime}(t) = \\sqrt{y(t)}$. Its right-hand side, $f(t, y) = \\sqrt{y}$, is not Lipschitz continuous with respect to $y$ at $y=0$. This is because its partial derivative, $\\frac{\\partial f}{\\partial y} = \\frac{1}{2\\sqrt{y}}$, is unbounded as $y \\to 0^{+}$. The Picard–Lindelöf theorem, which guarantees existence and uniqueness of the solution to an initial value problem (IVP), requires the function to be Lipschitz continuous. The failure of this condition for the given ODE at $y=0$ permits the existence of multiple solutions.\n\nFor the IVP with the initial condition $y(0)=0$, at least two distinct solutions exist for $t \\ge 0$:\n$1$. The trivial solution, $y_{1}(t) = 0$.\n$2$. The \"smooth positive branch\" solution, which for this specific initial condition corresponds to $y_{2}(t) = (t/2)^{2}$.\n\nA deterministic numerical method, by its very nature, computes the next state as a function of the current state. If the initial state is an equilibrium point, where the derivative is zero (i.e., $f(t_0, y_0) = 0$), the calculated increment will be zero. Consequently, the numerical solution will remain at the equilibrium point for all subsequent steps. For Cases A and D, the initial condition is $(t_0, y_0) = (0, 0)$, which is an equilibrium point since $y'(0) = \\sqrt{0} = 0$. Therefore, a standard deterministic integrator is expected to produce the trivial solution, $y(T)=0$.\n\nFor Cases B and C, the initial value $y_0$ is a small positive number. The derivative $y'(t_0) = \\sqrt{y_0}$ is non-zero, so the numerical integration will proceed, moving away from the $y=0$ axis. In these cases, the solver is expected to track the unique positive solution branch determined by the initial conditions.\n\nTo solve this problem, a high-order adaptive step-size Runge-Kutta method is appropriate due to the strict absolute tolerance requirement of $10^{-12}$. I will use the `DOP853` method, an 8th-order Dormand-Prince method, available through the `scipy.integrate.solve_ivp` function. This method is deterministic and provides robust error control. Both the absolute tolerance (`atol`) and relative tolerance (`rtol`) will be set to $10^{-12}$ to ensure high accuracy.\n\nThe implementation proceeds as follows:\n$1$. The ODE function is defined as `ode_func(t, y)`, which returns $\\sqrt{\\max(0, y)}$ to prevent domain errors for the square root that might arise from small negative values produced by numerical noise.\n$2$. A function is created to compute the analytical value of the smooth positive branch, $y^{\\mathrm{branch}}(t) = \\left(\\frac{t + 2\\sqrt{y_{0}} - t_{0}}{2}\\right)^{2}$.\n$3$. The four test cases are iterated. For each case, `scipy.integrate.solve_ivp` is called with the corresponding parameters ($t_0$, $y_0$, $T$, and tolerance).\n$4$. The numerically computed terminal value, $y(T)$, is extracted from the solver's output.\n$5$. The value of the smooth positive branch, $y^{\\mathrm{branch}}(T)$, is calculated using the analytical formula.\n$6$. The absolute deviation, $|y(T) - y^{\\mathrm{branch}}(T)|$, is computed.\n$7$. The computed $y(T)$ and the deviation are stored. Finally, all eight resulting numbers are formatted into a single flat list as required.", "answer": "```python\nimport numpy as np\nfrom scipy.integrate import solve_ivp\n\ndef solve():\n    \"\"\"\n    Solves the ODE y'(t) = sqrt(y(t)) for a suite of test cases,\n    comparing numerical results to a specified analytical branch.\n    \"\"\"\n\n    def ode_func(t, y):\n        \"\"\"\n        Defines the right-hand side of the ODE y'(t) = sqrt(y(t)).\n        The input y is a 1-element array.\n        np.maximum is used to prevent domain errors from numerical noise.\n        \"\"\"\n        return np.sqrt(np.maximum(0, y[0]))\n\n    def get_branch_solution(t, t0, y0):\n        \"\"\"\n        Calculates the value of the \"smooth positive branch\" solution.\n        The constant C is determined by the initial condition (t0, y0).\n        \"\"\"\n        # C = 2*sqrt(y0) - t0\n        # y(t) = ((t + C)/2)^2\n        val = (t + 2 * np.sqrt(y0) - t0) / 2\n        return val**2\n\n    # Test Suite: (t0, y0, T, tolerance)\n    test_cases = [\n        # Case A: locking at the equilibrium\n        (0.0, 0.0, 1.0, 1e-12),\n        # Case B: vanishingly small positive start\n        (0.0, 1e-12, 1.0, 1e-12),\n        # Case C: away from the non-Lipschitz point\n        (1e-6, (1e-6)**2 / 4.0, 1.0, 1e-12),\n        # Case D: short-time target at the equilibrium\n        (0.0, 0.0, 1e-3, 1e-12),\n    ]\n\n    results = []\n    for case in test_cases:\n        t0, y0, T, tol = case\n        \n        # Integrate the ODE\n        sol = solve_ivp(\n            fun=ode_func,\n            t_span=(t0, T),\n            y0=[y0],\n            method='DOP853',  # High-order deterministic method\n            atol=tol,\n            rtol=tol\n        )\n        \n        # Extract the computed value at the terminal time T\n        computed_y_T = sol.y[0, -1]\n        \n        # Calculate the corresponding value from the smooth positive branch\n        branch_y_T = get_branch_solution(T, t0, y0)\n        \n        # Calculate the absolute deviation\n        deviation = np.abs(computed_y_T - branch_y_T)\n        \n        results.append(computed_y_T)\n        results.append(deviation)\n    \n    # Format the final output as a comma-separated list in a single line\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2378472"}]}