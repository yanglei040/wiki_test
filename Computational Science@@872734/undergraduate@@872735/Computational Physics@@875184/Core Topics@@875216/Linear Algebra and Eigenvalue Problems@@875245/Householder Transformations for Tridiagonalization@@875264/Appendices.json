{"hands_on_practices": [{"introduction": "Moving from theory to practice requires careful attention to the nuances of computation. This first exercise explores the critical issue of numerical stability in constructing a Householder vector. While two algebraic formulas for the reflection vector may be equivalent on paper, this problem illustrates how one choice can lead to catastrophic cancellation errors in floating-point arithmetic, demonstrating why the standard algorithm includes a specific sign choice to ensure accuracy. [@problem_id:2402000]", "problem": "Consider the real symmetric matrix\n$$\nA(\\delta)=\n\\begin{pmatrix}\n2 & 1 & \\delta & 0\\\\\n1 & 3 & 0 & 0\\\\\n\\delta & 0 & 4 & 0\\\\\n0 & 0 & 0 & 5\n\\end{pmatrix},\n$$\nwhere $\\delta>0$ is a real parameter with $\\delta \\ll 1$. In the first step of Householder tridiagonalization, one constructs a Householder reflector $H=I-2uu^T$ to act on the trailing $(n-1)$-dimensional subspace so that the first column below the diagonal is annihilated. Let $x\\in\\mathbb{R}^{3}$ denote the subvector formed from the entries of the first column of $A(\\delta)$ below the $(1,1)$ position, namely $x=A(\\delta)_{2:4,1}$. A standard construction of the Householder vector uses $v=x\\pm \\alpha e_1$ with $\\alpha=\\|x\\|_2$ and $e_1=(1,0,0)^T$, followed by normalization $u=v/\\|v\\|_2$. Here, the two choices of sign correspond to two algebraically equivalent but numerically distinct reflectors.\n\nDefine $v_{\\text{right}}=x+\\alpha e_1$ and $v_{\\text{wrong}}=x-\\alpha e_1$. Compute the exact, closed-form analytic expression for\n$$\n\\kappa(\\delta)=\\frac{\\left| \\left(v_{\\text{wrong}}\\right)_1 \\right|}{\\left| \\left(v_{\\text{right}}\\right)_1 \\right|},\n$$\nwhere $\\left(v\\right)_1$ denotes the first component of the vector $v$. Your final answer must be a single simplified analytic expression in terms of $\\delta$ only, with no numerical approximation.", "solution": "The problem as stated is scientifically grounded, well-posed, objective, and self-contained. It is a standard exercise in numerical linear algebra concerning the numerical stability of constructing Householder reflectors. We proceed with the solution.\n\nThe given matrix is\n$$\nA(\\delta)=\n\\begin{pmatrix}\n2 & 1 & \\delta & 0\\\\\n1 & 3 & 0 & 0\\\\\n\\delta & 0 & 4 & 0\\\\\n0 & 0 & 0 & 5\n\\end{pmatrix}\n$$\nwhere $\\delta > 0$ is a real parameter.\n\nThe first step of Householder tridiagonalization targets the first column of the matrix. The vector $x$ required for constructing the reflector is the subvector of the first column of $A(\\delta)$ that lies below the main diagonal element $A(\\delta)_{1,1}$. The first column of $A(\\delta)$ is $(2, 1, \\delta, 0)^T$. Therefore, the subvector $x \\in \\mathbb{R}^3$ is given by\n$$\nx = \\begin{pmatrix} 1 \\\\ \\delta \\\\ 0 \\end{pmatrix}.\n$$\nThe construction of the Householder vector $v$ is based on $v = x \\pm \\alpha e_1$, where $\\alpha = \\|x\\|_2$ and $e_1$ is the first standard basis vector in $\\mathbb{R}^3$, $e_1 = (1, 0, 0)^T$. First, we compute the Euclidean norm $\\alpha$ of the vector $x$:\n$$\n\\alpha = \\|x\\|_2 = \\sqrt{1^2 + \\delta^2 + 0^2} = \\sqrt{1 + \\delta^2}.\n$$\nThe problem defines two possible choices for the unnormalized Householder vector:\n$$\nv_{\\text{right}} = x + \\alpha e_1\n$$\n$$\nv_{\\text{wrong}} = x - \\alpha e_1\n$$\nThe standard \"correct\" choice in numerical algorithms is to select the sign in $x \\pm \\alpha e_1$ to match the sign of the first component of $x$, in order to avoid subtractive cancellation. Since $(x)_1 = 1 > 0$, the numerically stable choice is $v_{\\text{right}}$. The problem asks for a ratio involving both choices.\n\nLet us explicitly construct these vectors:\n$$\nv_{\\text{right}} = \\begin{pmatrix} 1 \\\\ \\delta \\\\ 0 \\end{pmatrix} + \\sqrt{1 + \\delta^2} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 + \\sqrt{1 + \\delta^2} \\\\ \\delta \\\\ 0 \\end{pmatrix}\n$$\n$$\nv_{\\text{wrong}} = \\begin{pmatrix} 1 \\\\ \\delta \\\\ 0 \\end{pmatrix} - \\sqrt{1 + \\delta^2} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 - \\sqrt{1 + \\delta^2} \\\\ \\delta \\\\ 0 \\end{pmatrix}\n$$\nWe need to compute the ratio $\\kappa(\\delta)$, defined as\n$$\n\\kappa(\\delta)=\\frac{\\left| \\left(v_{\\text{wrong}}\\right)_1 \\right|}{\\left| \\left(v_{\\text{right}}\\right)_1 \\right|}.\n$$\nThe first components are $(v_{\\text{right}})_1 = 1 + \\sqrt{1 + \\delta^2}$ and $(v_{\\text{wrong}})_1 = 1 - \\sqrt{1 + \\delta^2}$.\n\nWe evaluate the absolute values of these components. Since $\\delta > 0$, we have $\\delta^2 > 0$, which implies $1 + \\delta^2 > 1$ and therefore $\\sqrt{1 + \\delta^2} > 1$.\nThe term $(v_{\\text{right}})_1 = 1 + \\sqrt{1 + \\delta^2}$ is strictly positive, so its absolute value is itself:\n$$\n\\left| \\left(v_{\\text{right}}\\right)_1 \\right| = 1 + \\sqrt{1 + \\delta^2}.\n$$\nThe term $(v_{\\text{wrong}})_1 = 1 - \\sqrt{1 + \\delta^2}$ is strictly negative. Its absolute value is the negation of the term:\n$$\n\\left| \\left(v_{\\text{wrong}}\\right)_1 \\right| = - (1 - \\sqrt{1 + \\delta^2}) = \\sqrt{1 + \\delta^2} - 1.\n$$\nNow, we can write the expression for $\\kappa(\\delta)$:\n$$\n\\kappa(\\delta) = \\frac{\\sqrt{1 + \\delta^2} - 1}{\\sqrt{1 + \\delta^2} + 1}.\n$$\nThis expression is correct, but it can be simplified further, particularly to obtain a form that is more robust for numerical computation when $\\delta$ is small (as suggested by $\\delta \\ll 1$). This is achieved by rationalizing the numerator. We multiply the numerator and the denominator by the conjugate of the numerator, which is $(\\sqrt{1 + \\delta^2} + 1)$:\n$$\n\\kappa(\\delta) = \\left( \\frac{\\sqrt{1 + \\delta^2} - 1}{\\sqrt{1 + \\delta^2} + 1} \\right) \\times \\left( \\frac{\\sqrt{1 + \\delta^2} + 1}{\\sqrt{1 + \\delta^2} + 1} \\right).\n$$\nThe numerator simplifies to a difference of squares:\n$$\n(\\sqrt{1 + \\delta^2} - 1)(\\sqrt{1 + \\delta^2} + 1) = (\\sqrt{1 + \\delta^2})^2 - 1^2 = (1 + \\delta^2) - 1 = \\delta^2.\n$$\nThe denominator becomes a square:\n$$\n(\\sqrt{1 + \\delta^2} + 1)(\\sqrt{1 + \\delta^2} + 1) = (\\sqrt{1 + \\delta^2} + 1)^2.\n$$\nCombining these results gives the final, simplified, closed-form expression for $\\kappa(\\delta)$:\n$$\n\\kappa(\\delta) = \\frac{\\delta^2}{(\\sqrt{1 + \\delta^2} + 1)^2}.\n$$\nThis form avoids the subtractive cancellation present in the numerator of the intermediate expression, which is the entire point of the distinction between $v_{\\text{right}}$ and $v_{\\text{wrong}}$.", "answer": "$$\\boxed{\\frac{\\delta^2}{(\\sqrt{1 + \\delta^2} + 1)^2}}$$", "id": "2402000"}, {"introduction": "Beyond accuracy, efficiency is a cornerstone of computational science. This practice delves into the computational cost of applying a Householder transformation, comparing a naive, brute-force matrix multiplication with the elegant, structure-exploiting method used in high-performance libraries. By analyzing the floating-point operation (flop) counts, you will quantify the immense speed-up gained by avoiding the explicit formation of the Householder matrix, a key insight into practical algorithm design. [@problem_id:2402001]", "problem": "Consider a real symmetric dense matrix $A \\in \\mathbb{R}^{n \\times n}$ and a Householder reflector $H_k = I - \\tau v v^T$ built at step $k$ of a tridiagonalization procedure, where $v \\in \\mathbb{R}^{m}$ has its first $k$ entries equal to zero when embedded in $\\mathbb{R}^{n}$ and $m = n - k$ with $1 \\le k \\le n - 2$. The similarity update acts only on the trailing principal submatrix of dimension $m \\times m$, denoted $A_t$, producing $A_t' = H_k A_t H_k$.\n\nCompare the leading-order floating-point operation (flop) counts, under the model that each scalar addition or multiplication counts as one flop, for the following two implementation strategies to compute $A_t'$:\n\n- Explicit method: form $H_k$ explicitly as an $m \\times m$ matrix and compute $A_t' = (H_k A_t) H_k$ by two dense matrix-matrix multiplications.\n- Optimized vector-based method: exploit symmetry of $A_t$ and the rank-$1$ structure of $H_k$ to apply the similarity without forming $H_k$ explicitly, using one symmetric matrix-vector product with $A_t$ and one symmetric rank-$2$ update to $A_t$, plus operations whose costs are of lower order in $m$.\n\nLet $F_{\\mathrm{exp}}(n,k)$ and $F_{\\mathrm{opt}}(n,k)$ denote the leading-order flop counts for the explicit and optimized methods, respectively, in terms of $n$ and $k$, ignoring all lower-order terms in $m$. Define the ratio $R(n,k) = \\frac{F_{\\mathrm{exp}}(n,k)}{F_{\\mathrm{opt}}(n,k)}$.\n\nWhat is the simplified analytic expression for $R(n,k)$ in terms of $n$ and $k$? Provide your answer as a closed-form expression. No rounding is required, and no units are involved.", "solution": "The problem requires a comparison of the leading-order floating-point operation (flop) counts for two distinct methods of applying a Householder similarity transformation to a symmetric matrix sub-block. The goal is to find the ratio of these flop counts.\n\nFirst, I will validate the problem statement.\n\n### Step 1: Extract Givens\n- A real symmetric dense matrix $A \\in \\mathbb{R}^{n \\times n}$.\n- A Householder reflector $H_k = I - \\tau v v^T$ at step $k$.\n- $v \\in \\mathbb{R}^{m}$ where $m = n - k$ and $1 \\le k \\le n - 2$.\n- The update is applied to the trailing submatrix $A_t$ of dimension $m \\times m$, yielding $A_t' = H_k A_t H_k$.\n- Flop model: one flop for each scalar addition or multiplication.\n- Explicit method: Form $H_k$ explicitly ($m \\times m$) and compute $A_t' = (H_k A_t) H_k$ via two dense matrix-matrix multiplications.\n- Optimized vector-based method: Exploit symmetry of $A_t$ and the rank-$1$ structure of $H_k$, using one symmetric matrix-vector product and one symmetric rank-$2$ update, ignoring lower-order terms.\n- Notation: $F_{\\mathrm{exp}}(n,k)$ and $F_{\\mathrm{opt}}(n,k)$ for leading-order flop counts.\n- Objective: Find the ratio $R(n,k) = \\frac{F_{\\mathrm{exp}}(n,k)}{F_{\\mathrm{opt}}(n,k)}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is evaluated against the required criteria.\n\n- **Scientifically Grounded:** The problem is a standard topic in numerical linear algebra, specifically the analysis of algorithms for eigenvalue computation. The methods described (explicit formation vs. vector-based application of Householder reflectors) are well-established and represent a fundamental trade-off in computational science. The problem is scientifically sound.\n- **Well-Posed:** The problem provides clear definitions for the two computational strategies and the accounting model for flops. It requests a specific, derivable quantity (the ratio of leading-order flop counts). The existence of a unique, meaningful solution is assured.\n- **Objective:** The language is technical, precise, and free of any subjective or ambiguous terminology.\n- **Incompleteness or Contradiction:** The problem is self-contained. All necessary information, including matrix properties, dimensions, and descriptions of the algorithms, is provided. There are no contradictions.\n\n### Step 3: Verdict and Action\nThe problem is valid. A rigorous solution follows.\n\nThe analysis will focus on the leading-order flop count in terms of the submatrix dimension $m = n-k$, as operations of order $O(m)$ or lower are explicitly to be ignored in comparison to terms of order $O(m^2)$ or higher.\n\n**Analysis of the Explicit Method ($F_{\\mathrm{exp}}$)**\n\nThis method involves two steps for the update $A_t' = H_k A_t H_k$. The matrix $A_t \\in \\mathbb{R}^{m \\times m}$ is symmetric, and the Householder reflector $H_k = I - \\tau v v^T$ is also symmetric.\n$1$. Formation of $H_k$: This involves an outer product $v v^T$ ($m^2$ multiplications), scaling by $-\\tau$ ($m^2$ multiplications), and adding the identity matrix ($m$ additions). The total cost is $O(m^2)$.\n$2$. Matrix multiplications: The problem states to compute $A_t'$ via two dense matrix-matrix multiplications.\n- First, compute the intermediate matrix $C = H_k A_t$. A standard algorithm for the product of two dense $m \\times m$ matrices requires $m$ multiplications and $m-1$ additions for each of the $m^2$ entries, totaling $m^2(2m-1) = 2m^3 - m^2$ flops. The leading-order cost is $2m^3$.\n- Second, compute $A_t' = C H_k$. This is another dense matrix-matrix multiplication, with a leading-order cost of $2m^3$ flops.\n\nThe cost of forming $H_k$, being $O(m^2)$, is a lower-order term compared to the matrix multiplications and is thus ignored. The total leading-order flop count for the explicit method is the sum of the costs of the two matrix products.\n$$F_{\\mathrm{exp}}(n,k) \\approx 2m^3 + 2m^3 = 4m^3$$\nSubstituting $m = n-k$, we get:\n$$F_{\\mathrm{exp}}(n,k) = 4(n-k)^3$$\n\n**Analysis of the Optimized Vector-Based Method ($F_{\\mathrm{opt}}$)**\n\nThis method avoids forming $H_k$ and applies the transformation using vector and matrix-vector operations. The update $A_t' = H_k A_t H_k = (I - \\tau v v^T) A_t (I - \\tau v v^T)$ is performed in a way that exploits the symmetric and rank-one structure. The overall operation is equivalent to a symmetric rank-$2$ update to $A_t$. The procedure, as described, involves two primary steps plus lower-order work.\n\n$1$. Symmetric matrix-vector product: Compute the vector $p = A_t v$.\n- For each component $p_i = \\sum_{j=1}^{m} (A_t)_{ij} v_j$, this requires $m$ multiplications and $m-1$ additions.\n- Since there are $m$ components in $p$, the total flop count is $m \\times (m + m-1) = 2m^2 - m$.\n- The leading-order cost for this step is $2m^2$. Exploiting the symmetry of $A_t$ does not reduce the leading-order flop count for this operation.\n\n$2$. Symmetric rank-$2$ update: The update takes the form $A_t' = A_t - (v w^T + w v^T)$, where the vector $w$ is constructed from $v$ and $p$ in $O(m)$ flops (a lower-order cost which we ignore, as per the instruction). To perform this update efficiently, we exploit the symmetry of the result. We only need to compute the elements of the lower (or upper) triangle of $A_t'$, which number $\\frac{m(m+1)}{2}$.\n- For each element $(A_t')_{ij}$ with $i \\ge j$, the update is $(A_t')_{ij} = (A_t)_{ij} - v_i w_j - w_i v_j$.\n- This computation requires $2$ multiplications ($v_i w_j$ and $w_i v_j$) and $2$ subtractions, totaling $4$ flops per element.\n- The total flop count for this update is $4 \\times \\frac{m(m+1)}{2} = 2m^2 + 2m$.\n- The leading-order cost for this step is $2m^2$.\n\nThe total leading-order flop count for the optimized method is the sum of the costs from these two main steps.\n$$F_{\\mathrm{opt}}(n,k) \\approx 2m^2 + 2m^2 = 4m^2$$\nSubstituting $m = n-k$:\n$$F_{\\mathrm{opt}}(n,k) = 4(n-k)^2$$\n\n**Calculation of the Ratio $R(n,k)$**\n\nThe ratio of the leading-order flop counts is:\n$$R(n,k) = \\frac{F_{\\mathrm{exp}}(n,k)}{F_{\\mathrm{opt}}(n,k)} = \\frac{4(n-k)^3}{4(n-k)^2}$$\nSimplifying the expression gives:\n$$R(n,k) = n-k$$\nThis result demonstrates the significant computational advantage of the structure-exploiting method, an advantage that grows linearly with the size of the submatrix being processed.", "answer": "$$\\boxed{n-k}$$", "id": "2402001"}, {"introduction": "To complete our hands-on exploration, we will reverse the process to solidify our understanding of the similarity transformation itself. Given the final tridiagonal matrix $T$ and the accumulated orthogonal matrix $Q$, this problem challenges you to reconstruct the original dense symmetric matrix $A$. This practice reinforces the fundamental relationship $A = Q T Q^T$ and provides a concrete check on your understanding of how these components relate to one another. [@problem_id:2401977]", "problem": "Consider a real symmetric matrix $A \\in \\mathbb{R}^{3 \\times 3}$ that has been reduced to a symmetric tridiagonal matrix $T$ by an orthogonal similarity transformation obtained from a sequence of Householder reflectors, so that $Q^T A Q = T$, where $Q \\in \\mathbb{R}^{3 \\times 3}$ is orthogonal. You are given\n$$\nT \\;=\\; \\begin{pmatrix}\n2 & 1 & 0 \\\\\n1 & 3 & 1 \\\\\n0 & 1 & 4\n\\end{pmatrix},\n\\qquad\nQ \\;=\\; \\begin{pmatrix}\n\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} & 0 \\\\\n-\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix}.\n$$\nDetermine the exact value of the $(1,3)$ entry of the original matrix $A$. Your final answer must be given as a closed-form analytic expression with no rounding.", "solution": "The problem statement has been subjected to rigorous validation and is determined to be valid. It is scientifically grounded, well-posed, objective, and contains sufficient information for a unique solution. The context lies within standard numerical linear algebra, a fundamental tool in computational physics and other STEM disciplines. The task is to determine a specific entry of a matrix $A$ given its tridiagonal form $T$ and the orthogonal transformation matrix $Q$.\n\nThe governing relation is the similarity transformation:\n$$\nQ^T A Q = T\n$$\nwhere $A \\in \\mathbb{R}^{3 \\times 3}$ is a real symmetric matrix, $T \\in \\mathbb{R}^{3 \\times 3}$ is a symmetric tridiagonal matrix, and $Q \\in \\mathbb{R}^{3 \\times 3}$ is an orthogonal matrix. Our goal is to find the entry $A_{13}$.\n\nTo find $A$, we must isolate it from the given equation. Since $Q$ is an orthogonal matrix, its inverse is equal to its transpose, i.e., $Q^{-1} = Q^T$. Consequently, the inverse of its transpose is the matrix itself: $(Q^T)^{-1} = Q$.\n\nWe begin by left-multiplying the equation by $Q$:\n$$\nQ (Q^T A Q) = Q T\n$$\nUsing the associative property of matrix multiplication:\n$$\n(Q Q^T) A Q = Q T\n$$\nSince $Q Q^T = I$, where $I$ is the $3 \\times 3$ identity matrix, this simplifies to:\n$$\nI A Q = Q T\n$$\n$$\nA Q = Q T\n$$\nNext, we right-multiply by $Q^T$:\n$$\n(A Q) Q^T = (Q T) Q^T\n$$\n$$\nA (Q Q^T) = Q T Q^T\n$$\n$$\nA I = Q T Q^T\n$$\nThis yields the explicit expression for the matrix $A$:\n$$\nA = Q T Q^T\n$$\nThe problem requires the determination of the entry $A_{13}$, which is the element in the first row and third column of $A$. According to the rules of matrix multiplication, this element is the dot product of the first row of the matrix product $QT$ and the third column of the matrix $Q^T$.\n\nFirst, let us denote the product $M = Q T$. The first row of $M$, which we denote as $M_{1*}$, is calculated by multiplying the first row of $Q$ by the matrix $T$:\n$$\nM_{1*} = Q_{1*} T\n$$\nThe provided matrices are:\n$$\nT \\;=\\; \\begin{pmatrix}\n2 & 1 & 0 \\\\\n1 & 3 & 1 \\\\\n0 & 1 & 4\n\\end{pmatrix},\n\\qquad\nQ \\;=\\; \\begin{pmatrix}\n\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} & 0 \\\\\n-\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix}\n$$\nThe first row of $Q$ is $Q_{1*} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} & 0 \\end{pmatrix}$.\nCalculating $M_{1*}$:\n$$\nM_{1*} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} & 0 \\end{pmatrix} \\begin{pmatrix}\n2 & 1 & 0 \\\\\n1 & 3 & 1 \\\\\n0 & 1 & 4\n\\end{pmatrix}\n$$\n$$\nM_{1*} = \\begin{pmatrix}\n\\frac{1}{\\sqrt{2}}(2) + \\frac{1}{\\sqrt{2}}(1) + 0(0) &,&\n\\frac{1}{\\sqrt{2}}(1) + \\frac{1}{\\sqrt{2}}(3) + 0(1) &,&\n\\frac{1}{\\sqrt{2}}(0) + \\frac{1}{\\sqrt{2}}(1) + 0(4)\n\\end{pmatrix}\n$$\n$$\nM_{1*} = \\begin{pmatrix}\n\\frac{3}{\\sqrt{2}} &,&\n\\frac{4}{\\sqrt{2}} &,&\n\\frac{1}{\\sqrt{2}}\n\\end{pmatrix}\n$$\nNow, we need the third column of $Q^T$. The matrix $Q^T$ is:\n$$\nQ^T \\;=\\; \\begin{pmatrix}\n\\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}} & 0 \\\\\n\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix}\n$$\nThe third column of $Q^T$, which we denote as $(Q^T)_{*3}$, is $\\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}$.\n\nFinally, we calculate $A_{13}$ by taking the dot product of $M_{1*}$ and $(Q^T)_{*3}$:\n$$\nA_{13} = M_{1*} \\cdot (Q^T)_{*3} = \\begin{pmatrix} \\frac{3}{\\sqrt{2}} & \\frac{4}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}\n$$\n$$\nA_{13} = \\left(\\frac{3}{\\sqrt{2}}\\right)(0) + \\left(\\frac{4}{\\sqrt{2}}\\right)(0) + \\left(\\frac{1}{\\sqrt{2}}\\right)(1)\n$$\n$$\nA_{13} = \\frac{1}{\\sqrt{2}}\n$$\nTo present the answer in a conventional form with a rationalized denominator:\n$$\nA_{13} = \\frac{1}{\\sqrt{2}} \\times \\frac{\\sqrt{2}}{\\sqrt{2}} = \\frac{\\sqrt{2}}{2}\n$$\nThis is the exact value of the $(1,3)$ entry of the original matrix $A$.", "answer": "$$\\boxed{\\frac{\\sqrt{2}}{2}}$$", "id": "2401977"}]}