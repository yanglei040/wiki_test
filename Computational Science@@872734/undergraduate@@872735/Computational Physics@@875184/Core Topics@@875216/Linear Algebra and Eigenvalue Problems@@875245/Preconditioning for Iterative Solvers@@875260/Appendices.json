{"hands_on_practices": [{"introduction": "Preconditioning is a cornerstone technique for accelerating iterative solvers, but a common misconception is that any preconditioner is better than no preconditioner. This exercise serves as a crucial reality check. By working through a concrete calculation, you will demonstrate that a seemingly reasonable preconditioner can, in fact, worsen the conditioning of a system, leading to slower convergence. This practice underscores the importance of the 2-norm condition number, $\\kappa_2(A)$, for non-symmetric matrices and proves that careful analysis, rather than blind application, is key to successful preconditioning [@problem_id:2429417].", "problem": "In many computational physics applications, such as upwind discretizations of steady convection-diffusion operators, the resulting linear systems can be nonsymmetric with strongly unbalanced diagonal entries. A common diagonal (Jacobi) preconditioner scales the rows by the inverse of the diagonal entries. While preconditioning is often intended to improve convergence of iterative solvers, it does not universally reduce the matrix condition number in the matrix $2$-norm.\n\nStarting from the core definitions:\n- The matrix $2$-norm condition number of a nonsingular matrix $X$ is $\\kappa_{2}(X) = \\sigma_{\\max}(X)/\\sigma_{\\min}(X)$, where $\\sigma_{\\max}(X)$ and $\\sigma_{\\min}(X)$ are the largest and smallest singular values of $X$, respectively.\n- The singular values of $X$ are the square roots of the eigenvalues of $X^{\\mathsf{T}}X$.\n- The Jacobi preconditioner $P$ is the diagonal matrix formed from the diagonal of $A$, and the left-preconditioned operator is $P^{-1}A$.\n\nConsider the explicit $2 \\times 2$ matrix\n$$\nA = \\begin{pmatrix}\n1  10 \\\\\n0.19  2\n\\end{pmatrix},\n$$\nwhich is a simple model for a locally upwinded transport operator with disparate diagonal scaling. Let $P = \\operatorname{diag}(A) = \\operatorname{diag}(1, 2)$ be the Jacobi preconditioner, and define $B = P^{-1} A$.\n\nUsing only the definitions above (no other formulas may be assumed), compute the ratio\n$$\nR \\equiv \\frac{\\kappa_{2}(P^{-1}A)}{\\kappa_{2}(A)} = \\frac{\\kappa_{2}(B)}{\\kappa_{2}(A)}.\n$$\nReport the final numerical value of $R$ rounded to four significant figures. The answer is dimensionless; do not include any units.", "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded in numerical linear algebra, well-posed with all necessary information provided, and objective in its formulation. No inconsistencies, ambiguities, or factual errors are present. We proceed with the computation as requested.\n\nThe task is to compute the ratio $R = \\frac{\\kappa_{2}(B)}{\\kappa_{2}(A)}$, where $A = \\begin{pmatrix} 1  10 \\\\ 0.19  2 \\end{pmatrix}$ and $B = P^{-1}A$ with $P = \\operatorname{diag}(A)$. The computation will be performed in two stages: first for the matrix $A$, then for the matrix $B$.\n\nFirst, we determine the condition number $\\kappa_2(A)$. According to the provided definition, the singular values of $A$ are the square roots of the eigenvalues of $A^{\\mathsf{T}}A$.\nThe transpose of $A$ is $A^{\\mathsf{T}} = \\begin{pmatrix} 1  0.19 \\\\ 10  2 \\end{pmatrix}$.\nWe compute the product $A^{\\mathsf{T}}A$:\n$$\nA^{\\mathsf{T}}A = \\begin{pmatrix} 1  0.19 \\\\ 10  2 \\end{pmatrix} \\begin{pmatrix} 1  10 \\\\ 0.19  2 \\end{pmatrix} = \\begin{pmatrix} 1^2 + 0.19^2  1 \\cdot 10 + 0.19 \\cdot 2 \\\\ 10 \\cdot 1 + 2 \\cdot 0.19  10^2 + 2^2 \\end{pmatrix} = \\begin{pmatrix} 1.0361  10.38 \\\\ 10.38  104 \\end{pmatrix}\n$$\nThe eigenvalues, denoted $\\lambda$, of $A^{\\mathsf{T}}A$ are the roots of the characteristic equation $\\det(A^{\\mathsf{T}}A - \\lambda I) = 0$:\n$$\n(1.0361 - \\lambda)(104 - \\lambda) - (10.38)^2 = 0\n$$\n$$\n\\lambda^2 - (1.0361 + 104)\\lambda + (1.0361 \\cdot 104 - 10.38^2) = 0\n$$\n$$\n\\lambda^2 - 105.0361\\lambda + (107.7544 - 107.7444) = 0\n$$\n$$\n\\lambda^2 - 105.0361\\lambda + 0.01 = 0\n$$\nUsing the quadratic formula, the eigenvalues are $\\lambda = \\frac{105.0361 \\pm \\sqrt{105.0361^2 - 4(0.01)}}{2}$.\nThe two eigenvalues are $\\lambda_{\\max, A} \\approx 105.0360048$ and $\\lambda_{\\min, A} \\approx 9.52054 \\times 10^{-5}$.\nThe singular values of $A$ are $\\sigma_{\\max}(A) = \\sqrt{\\lambda_{\\max, A}}$ and $\\sigma_{\\min}(A) = \\sqrt{\\lambda_{\\min, A}}$.\nThe condition number of $A$ is therefore:\n$$\n\\kappa_2(A) = \\frac{\\sigma_{\\max}(A)}{\\sigma_{\\min}(A)} = \\sqrt{\\frac{\\lambda_{\\max, A}}{\\lambda_{\\min, A}}} \\approx \\sqrt{\\frac{105.0360048}{9.52054 \\times 10^{-5}}} \\approx \\sqrt{1103256} \\approx 1050.36\n$$\n\nNext, we determine the condition number $\\kappa_2(B)$. The preconditioner is $P = \\operatorname{diag}(A) = \\begin{pmatrix} 1  0 \\\\ 0  2 \\end{pmatrix}$.\nIts inverse is $P^{-1} = \\begin{pmatrix} 1  0 \\\\ 0  0.5 \\end{pmatrix}$.\nThe preconditioned matrix $B$ is:\n$$\nB = P^{-1}A = \\begin{pmatrix} 1  0 \\\\ 0  0.5 \\end{pmatrix} \\begin{pmatrix} 1  10 \\\\ 0.19  2 \\end{pmatrix} = \\begin{pmatrix} 1  10 \\\\ 0.095  1 \\end{pmatrix}\n$$\nWe follow the same procedure for $B$. The transpose is $B^{\\mathsf{T}} = \\begin{pmatrix} 1  0.095 \\\\ 10  1 \\end{pmatrix}$.\nThe product $B^{\\mathsf{T}}B$ is:\n$$\nB^{\\mathsf{T}}B = \\begin{pmatrix} 1  0.095 \\\\ 10  1 \\end{pmatrix} \\begin{pmatrix} 1  10 \\\\ 0.095  1 \\end{pmatrix} = \\begin{pmatrix} 1^2 + 0.095^2  1 \\cdot 10 + 0.095 \\cdot 1 \\\\ 10 \\cdot 1 + 1 \\cdot 0.095  10^2 + 1^2 \\end{pmatrix} = \\begin{pmatrix} 1.009025  10.095 \\\\ 10.095  101 \\end{pmatrix}\n$$\nThe eigenvalues, denoted $\\mu$, of $B^{\\mathsf{T}}B$ are the roots of the characteristic equation $\\det(B^{\\mathsf{T}}B - \\mu I) = 0$:\n$$\n(1.009025 - \\mu)(101 - \\mu) - (10.095)^2 = 0\n$$\n$$\n\\mu^2 - (1.009025 + 101)\\mu + (1.009025 \\cdot 101 - 10.095^2) = 0\n$$\n$$\n\\mu^2 - 102.009025\\mu + (101.911525 - 101.909025) = 0\n$$\n$$\n\\mu^2 - 102.009025\\mu + 0.0025 = 0\n$$\nThe eigenvalues are $\\mu = \\frac{102.009025 \\pm \\sqrt{102.009025^2 - 4(0.0025)}}{2}$.\nThe two eigenvalues are $\\mu_{\\max, B} \\approx 102.0090005$ and $\\mu_{\\min, B} \\approx 2.45075 \\times 10^{-5}$.\nThe condition number of $B$ is:\n$$\n\\kappa_2(B) = \\frac{\\sigma_{\\max}(B)}{\\sigma_{\\min}(B)} = \\sqrt{\\frac{\\mu_{\\max, B}}{\\mu_{\\min, B}}} \\approx \\sqrt{\\frac{102.0090005}{2.45075 \\times 10^{-5}}} \\approx \\sqrt{4162166.5} \\approx 2040.14\n$$\n\nFinally, we compute the ratio $R$:\n$$\nR = \\frac{\\kappa_2(B)}{\\kappa_2(A)} \\approx \\frac{2040.14}{1050.36} \\approx 1.94233\n$$\nRounding the final result to four significant figures gives $1.942$.", "answer": "$$\n\\boxed{1.942}\n$$", "id": "2429417"}, {"introduction": "While some preconditioners are fixed, others, like the Symmetric Successive Over-Relaxation (SSOR) method, include tunable parameters that allow us to optimize their performance. This exercise moves beyond simply applying a preconditioner to actively tailoring it for a specific problem. You will construct the SSOR preconditioner for a small but representative matrix and then use calculus to find the optimal relaxation parameter, $\\omega$, that minimizes the spectral condition number. This hands-on analysis reveals the mathematical principles behind tuning advanced numerical methods for peak efficiency [@problem_id:2429375].", "problem": "Consider the one-dimensional Poisson boundary value problem on the unit interval with homogeneous Dirichlet boundary conditions, discretized by second-order centered finite differences on a uniform grid with exactly two interior points. The resulting linear system is $A \\mathbf{u} = \\mathbf{f}$, where the coefficient matrix $A \\in \\mathbb{R}^{2 \\times 2}$ is symmetric positive definite and given by\n$$\nA = \\begin{pmatrix}\n2  -1 \\\\\n-1  2\n\\end{pmatrix}.\n$$\nLet $A$ be split as $A = D - L - U$, where $D$ is the diagonal of $A$, and $L$ and $U$ are the strictly lower and strictly upper triangular parts of $A$ with the sign convention that makes $L$ and $U$ entrywise nonnegative, i.e., $L = -\\operatorname{tril}(A,-1)$ and $U = -\\operatorname{triu}(A,1)$. Define the Symmetric Successive Over-Relaxation (SSOR) preconditioner $M_{\\omega}$ with relaxation parameter $\\omega \\ge 0$ by\n$$\nM_{\\omega} = (D + \\omega L)\\, D^{-1}\\, (D + \\omega U).\n$$\nFor the preconditioned operator $B_{\\omega} = M_{\\omega}^{-1} A$, the spectral condition number in the Euclidean norm is $\\kappa_{2}(B_{\\omega}) = \\lambda_{\\max}(B_{\\omega})/\\lambda_{\\min}(B_{\\omega})$, where $\\lambda_{\\max}$ and $\\lambda_{\\min}$ denote the largest and smallest eigenvalues, respectively.\n\nDetermine the value of the relaxation parameter $\\omega \\ge 0$ that minimizes $\\kappa_{2}(B_{\\omega})$. Provide your answer as a single real number. No rounding is required.", "solution": "The problem is to find the value of the relaxation parameter $\\omega \\ge 0$ that minimizes the spectral condition number $\\kappa_{2}(B_{\\omega})$ of the preconditioned matrix $B_{\\omega} = M_{\\omega}^{-1} A$, where $A$ is a given $2 \\times 2$ matrix and $M_{\\omega}$ is the SSOR preconditioner.\n\nFirst, we must identify the matrices $D$, $L$, and $U$ from the given matrix $A$.\nThe coefficient matrix is\n$$\nA = \\begin{pmatrix} 2  -1 \\\\ -1  2 \\end{pmatrix}\n$$\nThe problem specifies the splitting $A = D - L - U$, where $D$ is the diagonal of $A$, $L = -\\operatorname{tril}(A,-1)$ is derived from the strictly lower triangular part of $A$, and $U = -\\operatorname{triu}(A,1)$ is derived from the strictly upper triangular part of $A$.\nFrom the definition of $A$, we have:\n$$\nD = \\begin{pmatrix} 2  0 \\\\ 0  2 \\end{pmatrix} = 2I\n$$\n$$\nL = -\\begin{pmatrix} 0  0 \\\\ -1  0 \\end{pmatrix} = \\begin{pmatrix} 0  0 \\\\ 1  0 \\end{pmatrix}\n$$\n$$\nU = -\\begin{pmatrix} 0  -1 \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} 0  1 \\\\ 0  0 \\end{pmatrix}\n$$\nThe inverse of $D$ is $D^{-1} = \\frac{1}{2}I = \\begin{pmatrix} 1/2  0 \\\\ 0  1/2 \\end{pmatrix}$.\n\nNext, we construct the SSOR preconditioner matrix $M_{\\omega}$ using its definition:\n$$\nM_{\\omega} = (D + \\omega L) D^{-1} (D + \\omega U)\n$$\nSubstituting the matrices $D$, $L$, $U$, and $D^{-1}$:\n$$\nM_{\\omega} = \\left( \\begin{pmatrix} 2  0 \\\\ 0  2 \\end{pmatrix} + \\omega \\begin{pmatrix} 0  0 \\\\ 1  0 \\end{pmatrix} \\right) \\begin{pmatrix} 1/2  0 \\\\ 0  1/2 \\end{pmatrix} \\left( \\begin{pmatrix} 2  0 \\\\ 0  2 \\end{pmatrix} + \\omega \\begin{pmatrix} 0  1 \\\\ 0  0 \\end{pmatrix} \\right)\n$$\n$$\nM_{\\omega} = \\begin{pmatrix} 2  0 \\\\ \\omega  2 \\end{pmatrix} \\begin{pmatrix} 1/2  0 \\\\ 0  1/2 \\end{pmatrix} \\begin{pmatrix} 2  \\omega \\\\ 0  2 \\end{pmatrix}\n$$\n$$\nM_{\\omega} = \\begin{pmatrix} 1  0 \\\\ \\omega/2  1 \\end{pmatrix} \\begin{pmatrix} 2  \\omega \\\\ 0  2 \\end{pmatrix}\n$$\n$$\nM_{\\omega} = \\begin{pmatrix} 2  \\omega \\\\ \\omega  \\frac{\\omega^2}{2} + 2 \\end{pmatrix}\n$$\nTo find the preconditioned matrix $B_{\\omega} = M_{\\omega}^{-1} A$, we need the inverse of $M_{\\omega}$. The determinant of $M_{\\omega}$ is:\n$$\n\\det(M_{\\omega}) = 2 \\left( 2 + \\frac{\\omega^2}{2} \\right) - \\omega^2 = 4 + \\omega^2 - \\omega^2 = 4\n$$\nSince $\\det(M_{\\omega}) \\neq 0$, the inverse exists and is given by:\n$$\nM_{\\omega}^{-1} = \\frac{1}{4} \\begin{pmatrix} 2 + \\frac{\\omega^2}{2}  -\\omega \\\\ -\\omega  2 \\end{pmatrix}\n$$\nNow, we can compute the preconditioned matrix $B_{\\omega}$:\n$$\nB_{\\omega} = M_{\\omega}^{-1} A = \\frac{1}{4} \\begin{pmatrix} 2 + \\frac{\\omega^2}{2}  -\\omega \\\\ -\\omega  2 \\end{pmatrix} \\begin{pmatrix} 2  -1 \\\\ -1  2 \\end{pmatrix}\n$$\n$$\nB_{\\omega} = \\frac{1}{4} \\begin{pmatrix} 2(2 + \\frac{\\omega^2}{2}) + \\omega  -(2 + \\frac{\\omega^2}{2}) - 2\\omega \\\\ -2\\omega - 2  \\omega + 4 \\end{pmatrix}\n$$\n$$\nB_{\\omega} = \\frac{1}{4} \\begin{pmatrix} 4 + \\omega^2 + \\omega  -2 - \\frac{\\omega^2}{2} - 2\\omega \\\\ -2\\omega - 2  \\omega + 4 \\end{pmatrix}\n$$\nTo find the condition number, we need the eigenvalues of $B_{\\omega}$, which we denote by $\\lambda$. The characteristic equation is $\\det(B_{\\omega} - \\lambda I) = 0$. This is equivalent to $\\det(4B_{\\omega} - 4\\lambda I) = 0$. Let $\\mu = 4\\lambda$ be an eigenvalue of the matrix $C = 4B_{\\omega}$. The characteristic equation for $C$ is $\\mu^2 - \\operatorname{Tr}(C)\\mu + \\det(C) = 0$.\nThe trace of $C$ is:\n$$\n\\operatorname{Tr}(C) = (4 + \\omega^2 + \\omega) + (\\omega + 4) = \\omega^2 + 2\\omega + 8\n$$\nThe determinant of $C$ is:\n$$\n\\det(C) = \\det(4B_{\\omega}) = 4^2 \\det(B_{\\omega}) = 16 \\det(M_{\\omega}^{-1} A) = 16 \\frac{\\det(A)}{\\det(M_{\\omega})}\n$$\nWe have $\\det(A) = 2 \\times 2 - (-1) \\times (-1) = 3$ and $\\det(M_{\\omega}) = 4$.\n$$\n\\det(C) = 16 \\times \\frac{3}{4} = 12\n$$\nThe characteristic equation for $\\mu$ is:\n$$\n\\mu^2 - (\\omega^2 + 2\\omega + 8)\\mu + 12 = 0\n$$\nSolving for $\\mu$ using the quadratic formula:\n$$\n\\mu = \\frac{(\\omega^2 + 2\\omega + 8) \\pm \\sqrt{(\\omega^2 + 2\\omega + 8)^2 - 4(12)}}{2}\n$$\nLet $g(\\omega) = \\omega^2 + 2\\omega + 8$. The eigenvalues of $C$ are $\\mu_{1,2} = \\frac{g(\\omega) \\pm \\sqrt{g(\\omega)^2 - 48}}{2}$.\nThe eigenvalues of $B_{\\omega}$ are $\\lambda_{1,2} = \\frac{\\mu_{1,2}}{4}$:\n$$\n\\lambda_{\\max}(\\omega) = \\frac{g(\\omega) + \\sqrt{g(\\omega)^2 - 48}}{8}\n$$\n$$\n\\lambda_{\\min}(\\omega) = \\frac{g(\\omega) - \\sqrt{g(\\omega)^2 - 48}}{8}\n$$\nThe spectral condition number is $\\kappa_2(B_{\\omega}) = \\frac{\\lambda_{\\max}(\\omega)}{\\lambda_{\\min}(\\omega)}$:\n$$\n\\kappa_2(B_{\\omega}) = \\frac{g(\\omega) + \\sqrt{g(\\omega)^2 - 48}}{g(\\omega) - \\sqrt{g(\\omega)^2 - 48}}\n$$\nTo find the value of $\\omega$ that minimizes $\\kappa_2(B_{\\omega})$, we analyze its dependence on $g(\\omega)$. Let $f(g) = \\frac{g + \\sqrt{g^2 - c}}{g - \\sqrt{g^2 - c}}$ where $g = g(\\omega)$ and $c=48$. We check the derivative of $f(g)$ with respect to $g$:\n$$\n\\frac{df}{dg} = \\frac{d}{dg} \\left( \\frac{(g + \\sqrt{g^2 - c})^2}{g^2 - (g^2 - c)} \\right) = \\frac{1}{c} \\frac{d}{dg} (g + \\sqrt{g^2 - c})^2\n$$\n$$\n\\frac{df}{dg} = \\frac{2}{c} (g + \\sqrt{g^2 - c}) \\left( 1 + \\frac{g}{\\sqrt{g^2 - c}} \\right)\n$$\nFor $\\omega \\ge 0$, $g(\\omega) = \\omega^2+2\\omega+8 \\ge 8$. Since $c=48$, we have $g^2 \\ge 64  48$, so the terms are real. As $g  0$ and $\\sqrt{g^2-c}  0$, all factors in the expression for $\\frac{df}{dg}$ are positive. Therefore, $\\frac{df}{dg}  0$, which means that $\\kappa_2(B_{\\omega})$ is a monotonically increasing function of $g(\\omega)$.\nTo minimize $\\kappa_2(B_{\\omega})$, we must minimize the function $g(\\omega) = \\omega^2 + 2\\omega + 8$ for the domain $\\omega \\ge 0$.\nThe function $g(\\omega)$ is a parabola opening upwards. Its vertex is found by setting its derivative to zero:\n$$\n\\frac{dg}{d\\omega} = 2\\omega + 2 = 0 \\implies \\omega = -1\n$$\nThe minimum of the unconstrained function $g(\\omega)$ occurs at $\\omega = -1$. However, the problem restricts the domain to $\\omega \\ge 0$. On the interval $[0, \\infty)$, the derivative $\\frac{dg}{d\\omega} = 2\\omega + 2$ is always positive (since $2\\omega + 2 \\ge 2$ for $\\omega \\ge 0$). This means that $g(\\omega)$ is a strictly increasing function on its domain $[0, \\infty)$.\nTherefore, the minimum value of $g(\\omega)$ for $\\omega \\ge 0$ occurs at the lower boundary of the domain, which is $\\omega = 0$.\n\nThe value of the relaxation parameter $\\omega \\ge 0$ that minimizes $\\kappa_{2}(B_{\\omega})$ is $\\omega = 0$.", "answer": "$$\\boxed{0}$$", "id": "2429375"}, {"introduction": "Theory provides the foundation, but computational physics is ultimately an experimental science. This final practice moves from pen-and-paper analysis to a practical coding implementation, tackling a more realistic 2D anisotropic diffusion problem. You will implement the Preconditioned Conjugate Gradient (PCG) method and compare the performance of several key preconditioners, including the powerful Incomplete Cholesky (IC) factorization. This exercise provides invaluable hands-on experience with the practical challenges of numerical methods, including the critical impact of finite-precision arithmetic on the stability and utility of your preconditioner [@problem_id:2427808].", "problem": "You are given a sequence of symmetric positive definite linear systems of the form $A \\mathbf{x} = \\mathbf{b}$ arising from a $2$-dimensional, $5$-point finite difference discretization of an anisotropic diffusion operator on a square grid with Dirichlet boundary conditions. Let $m$ denote the number of interior points per coordinate direction (so the dimension is $n = m^2$). The coefficient matrix $A$ is formed as $A = I_m \\otimes T_x + T_y \\otimes I_m$, where $T_x$ and $T_y$ are tridiagonal matrices with constant diagonals defined by $T_x = \\text{tridiag}(-a_x, 2 a_x, -a_x)$ and $T_y = \\text{tridiag}(-a_y, 2 a_y, -a_y)$, for strictly positive $a_x$ and $a_y$. The right-hand side $b$ is the vector of all ones in $\\mathbb{R}^n$. Your task is to implement a Preconditioned Conjugate Gradient method with different preconditioners and investigate how floating point precision (single versus double versus mixed) affects preconditioner stability and utility as measured by iteration counts to a fixed accuracy.\n\nStarting from fundamental bases:\n- The Conjugate Gradient method solves $A \\mathbf{x} = \\mathbf{b}$ for symmetric positive definite $A$ by iteratively minimizing the $A$-energy norm of the error over expanding Krylov subspaces generated by $A$ and the residual, using orthogonality of residuals and $A$-conjugacy of directions. \n- A left preconditioner $M$ that is symmetric positive definite transforms the system to $M^{-1} A \\mathbf{x} = M^{-1} \\mathbf{b}$, changing the inner product to $\\langle u, v \\rangle_M = u^\\top M v$, which in practice reduces the condition number of the operator applied in the Krylov process and thus improves convergence rates when $M$ is a good approximation to $A$.\n- A standard sparse approximate factorization for symmetric positive definite matrices is incomplete Cholesky factorization with zero fill, denoted $\\text{IC}(0)$, which computes a lower triangular factor $L$ constrained to the lower-triangular sparsity pattern of $A$ such that $L L^\\top \\approx A$. Applying the preconditioner corresponds to computing $z = M^{-1} r$ by two triangular solves $L y = r$, $L^\\top z = y$.\n\nImplement the following preconditioners:\n- No preconditioner: $M = I$.\n- Jacobi preconditioner: $M = \\mathrm{diag}(A)$.\n- Incomplete Cholesky with zero fill computed in double precision: $M = L_{64} L_{64}^\\top$, where $L_{64}$ is computed using $64$-bit floating point arithmetic.\n- Incomplete Cholesky with zero fill computed in single precision with single-precision triangular solves: $M = L_{32} L_{32}^\\top$, where $L_{32}$ is computed using $32$-bit floating point arithmetic and applied using $32$-bit arithmetic in the triangular solves.\n- Mixed-precision incomplete Cholesky: compute $L_{32}$ as above but apply triangular solves in $64$-bit arithmetic (by casting the stored factor to $64$-bit once before solves).\n\nAll iterations, vector inner products, and convergence tests of the Preconditioned Conjugate Gradient method must be performed in $64$-bit arithmetic. Use the relative residual stopping criterion $\\|r_k\\|_2 / \\|b\\|_2 \\le \\tau$, with $\\tau = 10^{-8}$. If the method fails to satisfy the stopping criterion within $k_{\\max} = 5000$ iterations, return $5000$ as the iteration count for that run.\n\nFor $\\text{IC}(0)$, if at any step a nonpositive diagonal is encountered due to finite precision effects, stabilize it by adding a small positive diagonal shift proportional to the local diagonal magnitude. Specifically, if the tentative value at step $i$ computed in the factorization is not strictly positive, replace it with $d_i + \\delta$, where $d_i$ is the diagonal entry of $A$ at row $i$, and $\\delta$ is a small positive quantity on the order of $\\sqrt{\\varepsilon}$ (where $\\varepsilon$ is machine epsilon in the working precision). Record how many such stabilizations occur when computing $L_{32}$.\n\nTest suite. For each of the following parameter sets $(m, a_x, a_y)$:\n- Case $1$: $m = 16$, $a_x = 1$, $a_y = 1$.\n- Case $2$: $m = 16$, $a_x = 1$, $a_y = 10^{-3}$.\n- Case $3$: $m = 16$, $a_x = 1$, $a_y = 10^{-6}$.\n\ndo the following for each case:\n- Assemble $A$ in compressed sparse row form.\n- Run the Preconditioned Conjugate Gradient method for each of the five preconditioners defined above, all with the same $b$ (the vector of ones), tolerance $\\tau = 10^{-8}$, and maximum iterations $k_{\\max} = 5000$.\n- Record the number of iterations taken to satisfy the stopping criterion for each preconditioner, and record the stabilization count used only for the single-precision incomplete Cholesky computation.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one case and is itself a list of six integers in the order\n$[\\text{iters\\_none}, \\text{iters\\_jacobi}, \\text{iters\\_ic64}, \\text{iters\\_ic32}, \\text{iters\\_icmix}, \\text{stabilizations\\_ic32}]$.\nFor example, an output for three cases has the form\n$[[n_{1,1}, n_{1,2}, n_{1,3}, n_{1,4}, n_{1,5}, s_1],[n_{2,1}, n_{2,2}, n_{2,3}, n_{2,4}, n_{2,5}, s_2],[n_{3,1}, n_{3,2}, n_{3,3}, n_{3,4}, n_{3,5}, s_3]]$\nwhere each $n_{i,j}$ is an integer iteration count and each $s_i$ is the integer stabilization count for single-precision $\\text{IC}(0)$ in case $i$.\n\nAngle units do not apply. Physical units do not apply. All numerical quantities in the answer must be unitless.", "solution": "The problem posed is a well-defined exercise in computational science, requiring the implementation and comparison of preconditioning strategies for the Conjugate Gradient method. It is scientifically grounded, internally consistent, and requires no information beyond what is provided. The problem is valid.\n\nWe are tasked with solving the linear system $A \\mathbf{x} = \\mathbf{b}$, where the matrix $A \\in \\mathbb{R}^{n \\times n}$ with $n=m^2$ arises from a $5$-point finite difference discretization of an anisotropic diffusion operator on a uniform $m \\times m$ grid of interior points. The matrix $A$ is symmetric positive definite (SPD) and is given by the Kronecker sum $A = I_m \\otimes T_x + T_y \\otimes I_m$. The matrices $T_x, T_y \\in \\mathbb{R}^{m \\times m}$ are tridiagonal, specified as $T_x = \\text{tridiag}(-a_x, 2 a_x, -a_x)$ and $T_y = \\text{tridiag}(-a_y, 2 a_y, -a_y)$, with $a_x, a_y  0$. The right-hand side vector $\\mathbf{b}$ is the vector of all ones.\n\nThe solution will proceed in three stages:\n1.  Construction of the matrix $A$ in a sparse format suitable for efficient computation.\n2.  Implementation of the Preconditioned Conjugate Gradient (PCG) algorithm.\n3.  Implementation of the various preconditioners, with particular attention to the Incomplete Cholesky factorization and its numerical stability under different floating-point precisions.\n\n**1. Matrix Assembly**\n\nThe matrix $A$ is constructed using the Kronecker product. Given $m$, $a_x$, and $a_y$, the matrices $T_x$ and $T_y$ are assembled as sparse tridiagonal matrices. Using the properties of the Kronecker product, $I_m \\otimes T_x$ yields a block-diagonal matrix where each block is $T_x$. Similarly, $T_y \\otimes I_m$ produces a block-tridiagonal matrix where the diagonal blocks are $2a_y I_m$ and the off-diagonal blocks are $-a_y I_m$. Their sum gives the matrix $A$, which has a block-tridiagonal structure and contains at most $5$ non-zero elements per row. All computations for assembling $A$ are performed in $64$-bit precision (double precision). The resulting matrix is stored in the Compressed Sparse Row (CSR) format.\n\n**2. The Preconditioned Conjugate Gradient (PCG) Method**\n\nThe PCG method is an iterative algorithm for solving SPD linear systems. It improves upon the Conjugate Gradient method by using a preconditioner $M \\approx A$ to transform the system, accelerating convergence by clustering the eigenvalues of the iteration matrix $M^{-1}A$. The algorithm, performed entirely in $64$-bit arithmetic, is as follows:\n\nGiven an initial guess $x_0$ (we use $x_0 = \\mathbf{0}$), a tolerance $\\tau = 10^{-8}$, and a maximum of $k_{\\max} = 5000$ iterations:\n1.  Initialize residual: $r_0 = \\mathbf{b} - A x_0$.\n2.  Apply preconditioner: $z_0 = M^{-1} r_0$.\n3.  Set initial search direction: $p_0 = z_0$.\n4.  For $k=0, 1, 2, \\dots$ until convergence or $k_{\\max}$:\n    a. Compute step size: $\\alpha_k = \\frac{r_k^\\top z_k}{p_k^\\top A p_k}$.\n    b. Update solution: $x_{k+1} = x_k + \\alpha_k p_k$.\n    c. Update residual: $r_{k+1} = r_k - \\alpha_k A p_k$.\n    d. Check for convergence: if $\\|r_{k+1}\\|_2 / \\|\\mathbf{b}\\|_2 \\le \\tau$, stop and return $k+1$.\n    e. Apply preconditioner: $z_{k+1} = M^{-1} r_{k+1}$.\n    f. Update search direction: $\\beta_k = \\frac{r_{k+1}^\\top z_{k+1}}{r_k^\\top z_k}$, then $p_{k+1} = z_{k+1} + \\beta_k p_k$.\n\nThe core of the investigation lies in the implementation of the preconditioning step $z = M^{-1} r$.\n\n**3. Preconditioner Implementations**\n\nWe implement five different choices for the preconditioner $M$.\n\n-   **No Preconditioner**: $M = I$, the identity matrix. The operation $M^{-1}r$ is simply $z=r$. This corresponds to the standard Conjugate Gradient method.\n-   **Jacobi Preconditioner**: $M = \\mathrm{diag}(A)$. This is a simple diagonal scaling. For the given matrix $A$, the diagonal is constant and equal to $2(a_x+a_y)$. The operation $M^{-1}r$ is a component-wise division: $z_i = r_i / (2(a_x+a_y))$.\n-   **Incomplete Cholesky Factorization with Zero Fill (IC(0))**: This preconditioner computes an approximate Cholesky factorization $M = LL^\\top \\approx A$, where the lower triangular factor $L$ is constrained to have the same sparsity pattern as the lower part of $A$. The preconditioning step $z = M^{-1}r$ is performed by solving two triangular systems: $Ly=r$ (forward substitution) and $L^\\top z=y$ (backward substitution).\n\n    The IC(0) factor $L$ is computed via the following algorithm. For $i=0, \\dots, n-1$:\n    $$l_{ij} = \\frac{1}{l_{jj}} \\left( a_{ij} - \\sum_{k=0}^{j-1} l_{ik} l_{jk} \\right) \\quad \\text{for } j  i \\text{ and } a_{ij} \\ne 0$$\n    $$l_{ii} = \\sqrt{a_{ii} - \\sum_{k=0}^{i-1} l_{ik}^2}$$\n    For the graph of the $5$-point stencil matrix, which contains no cycles of length $3$, the sum $\\sum_{kj} l_{ik} l_{jk}$ is always zero. This simplifies the computation of off-diagonal elements to $l_{ij} = a_{ij} / l_{jj}$.\n\n    In finite precision arithmetic, the term under the square root for $l_{ii}$ can become non-positive. To prevent breakdown, we stabilize the factorization. If $a_{ii} - \\sum_{ki} l_{ik}^2 \\le 0$, we replace the diagonal value with $d_i + \\delta$, where $d_i$ is the original diagonal entry of $A$ and $\\delta = \\sqrt{\\varepsilon}$, with $\\varepsilon$ being the machine epsilon of the working precision.\n\n    We implement three variants of the IC(0) preconditioner:\n    1.  **Double Precision IC(0)**: The factor $L_{64}$ is computed and applied entirely in $64$-bit arithmetic.\n    2.  **Single Precision IC(0)**: The matrix $A$ is cast to $32$-bit precision. The factor $L_{32}$ is computed in $32$-bit precision (with stabilization using $\\varepsilon_{32}$), and the number of stabilizations is recorded. The triangular solves for $z=M^{-1}r$ are also performed in $32$-bit precision: the $64$-bit residual $r$ is cast to $32$-bit, the solves are performed, and the resulting $32$-bit vector $z$ is cast back to $64$-bit.\n    3.  **Mixed Precision IC(0)**: The factor $L_{32}$ is computed as in the single-precision case. However, it is then cast to a $64$-bit matrix before the PCG iteration begins. The triangular solves are subsequently performed entirely in $64$-bit arithmetic. This approach aims to benefit from the faster computation of the $L_{32}$ factor while maintaining the numerical stability of $64$-bit solves.\n\nThe following Python code implements this complete procedure for the specified test cases.", "answer": "```python\nimport numpy as np\nimport scipy.sparse as sp\nfrom scipy.sparse.linalg import spsolve_triangular\n\ndef assemble_A(m, ax, ay):\n    \"\"\"\n    Assembles the 2D anisotropic diffusion matrix A using Kronecker products.\n    A = I_m kron T_x + T_y kron I_m\n    \"\"\"\n    n = m * m\n    \n    # Define T_x and T_y matrices\n    tx_diagonals = [-ax * np.ones(m - 1), 2 * ax * np.ones(m), -ax * np.ones(m - 1)]\n    ty_diagonals = [-ay * np.ones(m - 1), 2 * ay * np.ones(m), -ay * np.ones(m - 1)]\n    \n    Tx = sp.diags(tx_diagonals, [-1, 0, 1], shape=(m, m), format='csr', dtype=np.float64)\n    Ty = sp.diags(ty_diagonals, [-1, 0, 1], shape=(m, m), format='csr', dtype=np.float64)\n    \n    Im = sp.eye(m, dtype=np.float64)\n    \n    A = sp.kron(Im, Tx) + sp.kron(Ty, Im)\n    return A.tocsr()\n\ndef ic0_factor(A, precision):\n    \"\"\"\n    Computes the Incomplete Cholesky factorization with zero fill (IC(0)).\n    This implementation is correct for matrices where the graph has no cycles of length 3,\n    such as those from a 5-point finite difference stencil.\n    \n    Args:\n        A (csr_matrix): The symmetric positive definite matrix to factorize.\n        precision (dtype): The floating-point precision (np.float32 or np.float64).\n    \n    Returns:\n        L (csr_matrix): The lower triangular IC(0) factor.\n        stabilization_count (int): Number of times stabilization was applied.\n    \"\"\"\n    n = A.shape[0]\n    # Use LIL format for easier, albeit slower, element-wise modification\n    L = A.copy().tolil().astype(precision)\n    A_diag_orig = A.diagonal().astype(precision)\n    \n    eps = np.finfo(precision).eps\n    delta = np.sqrt(eps)\n    stabilization_count = 0\n\n    for i in range(n):\n        # The sum over k in the formula for l_ij is zero for 5-point stencil graph\n        # because there are no triangles (cycles of length 3).\n        # So, l_ij = a_ij / l_jj\n        row_i_cols_lt_i = sorted([j for j in L.rows[i] if j  i])\n        \n        for j in row_i_cols_lt_i:\n            if L[j, j] != 0:\n                L[i, j] = L[i, j] / L[j, j]\n\n        # Compute diagonal element l_ii\n        # l_ii^2 = a_ii - sum_{ki} l_ik^2\n        sum_sq = 0.0\n        for k in row_i_cols_lt_i:\n             sum_sq += L[i, k]**2\n        \n        diag_val = L[i, i] - sum_sq # L[i,i] still holds original A[i,i]\n\n        if diag_val = 0:\n            diag_val = A_diag_orig[i] + delta\n            stabilization_count += 1\n        \n        L[i, i] = np.sqrt(diag_val)\n\n    # Return a clean lower-triangular CSR matrix\n    return sp.tril(L, format='csr'), stabilization_count\n\ndef pcg(A, b, precon_func, tol, max_iter):\n    \"\"\"\n    Preconditioned Conjugate Gradient solver.\n    All vector operations and tests are in 64-bit precision.\n    \"\"\"\n    n = A.shape[0]\n    x = np.zeros(n, dtype=np.float64)\n    r = b - A.dot(x)\n    z = precon_func(r)\n    p = z.copy()\n    \n    rs_old = np.dot(r, z)\n    b_norm = np.linalg.norm(b)\n    \n    for i in range(max_iter):\n        Ap = A.dot(p)\n        alpha = rs_old / np.dot(p, Ap)\n        \n        x += alpha * p\n        r -= alpha * Ap\n        \n        if np.linalg.norm(r) / b_norm = tol:\n            return i + 1\n            \n        z = precon_func(r)\n        rs_new = np.dot(r, z)\n        \n        beta = rs_new / rs_old\n        p = z + beta * p\n        \n        rs_old = rs_new\n        \n    return max_iter\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        (16, 1.0, 1.0),\n        (16, 1.0, 1e-3),\n        (16, 1.0, 1e-6),\n    ]\n\n    results = []\n    \n    for m, ax, ay in test_cases:\n        case_results = []\n        A_64 = assemble_A(m, ax, ay)\n        n = m * m\n        b_64 = np.ones(n, dtype=np.float64)\n        tol = 1e-8\n        max_iter = 5000\n\n        # No preconditioner\n        precon_none = lambda r: r\n        iters_none = pcg(A_64, b_64, precon_none, tol, max_iter)\n        case_results.append(iters_none)\n\n        # Jacobi preconditioner\n        A_diag_64 = A_64.diagonal()\n        precon_jacobi = lambda r: r / A_diag_64\n        iters_jacobi = pcg(A_64, b_64, precon_jacobi, tol, max_iter)\n        case_results.append(iters_jacobi)\n\n        # IC(0) 64-bit\n        L64, _ = ic0_factor(A_64, np.float64)\n        precon_ic64 = lambda r: spsolve_triangular(L64.T, spsolve_triangular(L64, r, lower=True), lower=False)\n        iters_ic64 = pcg(A_64, b_64, precon_ic64, tol, max_iter)\n        case_results.append(iters_ic64)\n\n        # IC(0) 32-bit (factorization and solves in float32)\n        A_32 = A_64.astype(np.float32)\n        L32, stab_count = ic0_factor(A_32, np.float32)\n        def precon_ic32(r):\n            r_32 = r.astype(np.float32)\n            y_32 = spsolve_triangular(L32, r_32, lower=True)\n            z_32 = spsolve_triangular(L32.T, y_32, lower=False)\n            return z_32.astype(np.float64)\n        iters_ic32 = pcg(A_64, b_64, precon_ic32, tol, max_iter)\n        case_results.append(iters_ic32)\n\n        # Mixed-precision IC(0) (factor in float32, solves in float64)\n        L32_as_64 = L32.astype(np.float64)\n        precon_icmix = lambda r: spsolve_triangular(L32_as_64.T, spsolve_triangular(L32_as_64, r, lower=True), lower=False)\n        iters_icmix = pcg(A_64, b_64, precon_icmix, tol, max_iter)\n        case_results.append(iters_icmix)\n\n        case_results.append(stab_count)\n        results.append(case_results)\n\n    # Format the final output as specified\n    output_str = \"[\" + \",\".join([str(r) for r in results]) + \"]\"\n    print(output_str.replace(\" \", \"\"))\n\n\nsolve()\n```", "id": "2427808"}]}