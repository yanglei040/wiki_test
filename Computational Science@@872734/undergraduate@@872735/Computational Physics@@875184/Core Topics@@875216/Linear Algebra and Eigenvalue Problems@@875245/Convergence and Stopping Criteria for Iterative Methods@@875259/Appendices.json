{"hands_on_practices": [{"introduction": "Choosing when to stop an iterative method seems simple, but common sense can be misleading. This first practice explores the classic dilemma between two simple stopping criteria: requiring the function's value to be small, $|p(x_k)| \\lt \\epsilon$, or requiring the step size to be small, $|x_k - x_{k-1}| \\lt \\epsilon$. Through a direct calculation using Newton's method on a polynomial with a multiple root, you will discover how these two criteria can give dramatically different signals about convergence, providing a crucial lesson on the importance of choosing a criterion that is robust to the problem's underlying structure [@problem_id:2199023].", "problem": "A student in a numerical analysis course is tasked with finding a root of the polynomial $p(x) = x^3 - 3x + 2$ using Newton's method. The iterative formula for Newton's method is given by $x_{k+1} = x_k - \\frac{p(x_k)}{p'(x_k)}$, where $p'(x)$ is the derivative of $p(x)$. The student starts with an initial guess of $x_0 = 1.2$.\n\nTo decide when to stop the iterations, the student considers two common stopping criteria, both using a tolerance of $\\epsilon = 0.05$:\n\n*   **Criterion A**: The residual tolerance, which is satisfied when $|p(x_k)| < \\epsilon$.\n*   **Criterion B**: The step-size tolerance, which is satisfied when $|x_k - x_{k-1}| < \\epsilon$ for $k \\ge 1$.\n\nPerform the first two iterations of Newton's method to find $x_1$ and $x_2$. You should use sufficient precision in your intermediate calculations to avoid rounding errors affecting your conclusion. Based on your results for $x_1$ and $x_2$, determine which of the following statements provides the most accurate analysis of the situation.\n\nA) Both Criterion A and Criterion B are satisfied after the first iteration (at $k=1$), indicating rapid and robust convergence.\n\nB) Neither Criterion A nor Criterion B is satisfied after two iterations (at $k=2$), indicating the method is converging very slowly or diverging.\n\nC) Criterion A is satisfied after the first iteration (at $k=1$), but Criterion B is not. This behavior suggests that using Criterion A alone could lead to a premature stop of the algorithm.\n\nD) Criterion B is satisfied after the first iteration (at $k=1$), but Criterion A is not. This behavior suggests that using Criterion B alone could lead to a premature stop of the algorithm.\n\nE) Both criteria are satisfied for the first time at the second iteration (at $k=2$).", "solution": "We are given $p(x) = x^{3} - 3x + 2$ and the Newton iteration\n$$\nx_{k+1} = x_{k} - \\frac{p(x_{k})}{p'(x_{k})},\n$$\nwith $x_{0} = 1.2$ and tolerance $\\epsilon = 0.05$. First compute the derivative:\n$$\np'(x) = 3x^{2} - 3.\n$$\n\nFirst iteration ($k=0$ to $k=1$). Evaluate $p$ and $p'$ at $x_{0} = 1.2$:\n$$\np(1.2) = 1.2^{3} - 3\\cdot 1.2 + 2 = 1.728 - 3.6 + 2 = 0.128,\n$$\n$$\np'(1.2) = 3\\cdot (1.2)^{2} - 3 = 3\\cdot 1.44 - 3 = 4.32 - 3 = 1.32.\n$$\nThus\n$$\nx_{1} = 1.2 - \\frac{0.128}{1.32} = \\frac{6}{5} - \\frac{16}{165} = \\frac{182}{165}.\n$$\nCheck Criterion A at $k=1$ by computing $|p(x_{1})|$. Using exact arithmetic,\n$$\np\\!\\left(\\frac{182}{165}\\right) = \\left(\\frac{182}{165}\\right)^{3} - 3\\left(\\frac{182}{165}\\right) + 2\n= \\frac{6,028,568}{4,492,125} - \\frac{182}{55} + 2\n= \\frac{147,968}{4,492,125}.\n$$\nCompare with $\\epsilon = \\frac{1}{20}$: since $147,968 \\cdot 20 = 2,959,360 < 4,492,125$, we have\n$$\n|p(x_{1})| = \\frac{147,968}{4,492,125} < \\frac{1}{20} = \\epsilon,\n$$\nso Criterion A is satisfied at $k=1$.\n\nCheck Criterion B at $k=1$:\n$$\n|x_{1} - x_{0}| = \\left|\\frac{182}{165} - \\frac{6}{5}\\right| = \\frac{16}{165}.\n$$\nCompare with $\\epsilon = \\frac{1}{20}$: since $16\\cdot 20 = 320 > 165$, we have\n$$\n|x_{1} - x_{0}| = \\frac{16}{165} > \\frac{1}{20} = \\epsilon,\n$$\nso Criterion B is not satisfied at $k=1$.\n\nSecond iteration ($k=1$ to $k=2$). We need $p'(x_{1})$ and $p(x_{1})$:\n$$\np'(x_{1}) = 3\\left(\\frac{182}{165}\\right)^{2} - 3 = \\frac{5899}{9075}, \\quad p(x_{1}) = \\frac{147,968}{4,492,125}.\n$$\nHence\n$$\nx_{2} = x_{1} - \\frac{p(x_{1})}{p'(x_{1})}\n= \\frac{182}{165} - \\left(\\frac{147,968}{4,492,125}\\right)\\left(\\frac{9075}{5899}\\right)\n= \\frac{182}{165} - \\frac{147,968}{2,920,005}\n= \\frac{3,072,886}{2,920,005}.\n$$\nCheck Criterion B at $k=2$ using the Newton step magnitude:\n$$\n|x_{2} - x_{1}| = \\frac{147,968}{2,920,005}.\n$$\nCompare with $\\epsilon = \\frac{1}{20}$: since $147,968 \\cdot 20 = 2,959,360 > 2,920,005$, it follows that\n$$\n|x_{2} - x_{1}| = \\frac{147,968}{2,920,005} > \\frac{1}{20} = \\epsilon,\n$$\nso Criterion B is still not satisfied at $k=2$. Meanwhile, since $x_{2}$ is closer to the double root at $x=1$ than $x_{1}$ is, and $p(x) = (x-1)^{2}(x+2)$, we have $|p(x_{2})| = |x_{2}-1|^{2}\\,|x_{2}+2| < |p(x_{1})| < \\epsilon$, so Criterion A remains satisfied.\n\nTherefore, after the first iteration, Criterion A is satisfied while Criterion B is not, which matches statement C. Moreover, even after two iterations, Criterion B is not yet satisfied, reinforcing that relying on Criterion A alone could stop the method prematurely near a multiple root.", "answer": "$$\\boxed{C}$$", "id": "2199023"}, {"introduction": "Having seen the potential pitfalls of naive stopping criteria, we now turn to designing a more reliable alternative. When solving for eigenvectors, the primary goal is to find a stable *direction* in space, a goal not always captured by monitoring a scalar eigenvalue estimate. This practice challenges you to implement a criterion based on the angle between successive normalized iterates, $\\theta_k = \\arccos(|u_{k+1}^\\top u_k|)$, which directly measures the convergence of the vector's direction [@problem_id:2427048]. Implementing this robust rule will give you hands-on experience in developing criteria that align with the true objective of the iteration.", "problem": "You are given the task of designing and validating a robust stopping criterion for iterative eigenvector approximations that relies on the angle between successive unit-norm iterates, rather than solely on changes in the estimated eigenvalue. Let $A \\in \\mathbb{R}^{n \\times n}$ be a real matrix, and let $\\|\\cdot\\|$ denote the Euclidean norm. For a nonzero initial vector $v_0 \\in \\mathbb{R}^n$, define the normalized sequence $\\{u_k\\}$ of iterates by either of the following update rules, depending on the method specified in each test case:\n\n- Standard iteration: \n$$\nu_{k+1} \\;=\\; \\frac{A u_k}{\\|A u_k\\|}.\n$$\n\n- Shift-and-invert iteration with a real shift $\\sigma$:\n$$\n\\text{solve } (A - \\sigma I) w = u_k \\text{ for } w, \\quad u_{k+1} \\;=\\; \\frac{w}{\\|w\\|}.\n$$\n\nAt each iteration, define the acute principal angle between successive unit vectors $u_k$ and $u_{k+1}$ by\n$$\n\\theta_k \\;=\\; \\arccos\\!\\big(\\,|u_{k+1}^\\top u_k|\\,\\big),\n$$\nwith angles measured in radians. The robust stopping criterion is specified as follows: declare convergence when $\\theta_k \\le \\tau_\\theta$ holds for at least $s$ consecutive iterations, where $\\tau_\\theta > 0$ and $s \\in \\mathbb{N}$ are given. If convergence is not achieved within $N_{\\max}$ iterations, declare non-convergence for that test.\n\nYour program must, for each test case described below, compute the number of iterations required to meet the angle-based stopping criterion. If the linear system in the shift-and-invert iteration is singular or numerically unsolvable for the specified shift, or if the stopping criterion is not met within the iteration cap, return the integer $-1$ for that test case.\n\nAll angles must be computed in radians. All inner products and norms must be Euclidean. The final output must be a single line containing the results for the entire test suite as a comma-separated list of integers enclosed in square brackets.\n\nTest Suite (all matrices and vectors are given explicitly):\n\n- Case $1$ (standard iteration, well-conditioned symmetric matrix, happy path):\n  - $A^{(1)} = \\begin{bmatrix} 4 & 1 & 0 \\\\ 1 & 3 & 0 \\\\ 0 & 0 & 2 \\end{bmatrix}$,\n    $v_0^{(1)} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$,\n    $\\tau_\\theta^{(1)} = 10^{-8}$,\n    $s^{(1)} = 2$,\n    $N_{\\max}^{(1)} = 1000$.\n- Case $2$ (standard iteration, slow convergence due to nearly equal leading eigenvalues, edge case):\n  - $A^{(2)} = \\begin{bmatrix} 1 & 10^{-4} & 0 \\\\ 10^{-4} & 0.999 & 0 \\\\ 0 & 0 & 0.5 \\end{bmatrix}$,\n    $v_0^{(2)} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0.1 \\end{bmatrix}$,\n    $\\tau_\\theta^{(2)} = 10^{-10}$,\n    $s^{(2)} = 3$,\n    $N_{\\max}^{(2)} = 50000$.\n- Case $3$ (standard iteration, dominant eigenvalue with negative sign, sign-flip robustness via acute angle):\n  - $A^{(3)} = \\begin{bmatrix} -5 & 0 & 0 \\\\ 0 & 3 & 0 \\\\ 0 & 0 & 2 \\end{bmatrix}$,\n    $v_0^{(3)} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$,\n    $\\tau_\\theta^{(3)} = 10^{-8}$,\n    $s^{(3)} = 2$,\n    $N_{\\max}^{(3)} = 1000$.\n- Case $4$ (shift-and-invert iteration targeting the smallest eigenvalue with $\\sigma = 0$):\n  - $A^{(4)} = \\begin{bmatrix} 10 & 2 & 0 \\\\ 2 & 5 & 1 \\\\ 0 & 1 & 1 \\end{bmatrix}$,\n    $v_0^{(4)} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}$,\n    $\\sigma^{(4)} = 0$,\n    $\\tau_\\theta^{(4)} = 10^{-8}$,\n    $s^{(4)} = 2$,\n    $N_{\\max}^{(4)} = 1000$.\n- Case $5$ (shift-and-invert iteration with $\\sigma$ near a known eigenvalue to induce very rapid convergence, boundary case):\n  - $A^{(5)} = \\begin{bmatrix} 7 & 0 & 0 \\\\ 0 & 3 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}$,\n    $v_0^{(5)} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$,\n    $\\sigma^{(5)} = 3.001$,\n    $\\tau_\\theta^{(5)} = 10^{-12}$,\n    $s^{(5)} = 1$,\n    $N_{\\max}^{(5)} = 100$.\n\nRequired final output format: Your program should produce a single line of output containing the iteration counts for Cases $1$ through $5$ in order, as a comma-separated list enclosed in square brackets (for example, $\\texttt{[n1,n2,n3,n4,n5]}$), where each $n_j$ is an integer.", "solution": "The problem statement is assessed to be valid. It is scientifically grounded in the established principles of numerical linear algebra, specifically the power and inverse power iteration methods for eigenvalue problems. The problem is well-posed, providing all necessary matrices, vectors, and parameters for each test case. The objective is clearly defined, and the terminology is precise and unambiguous.\n\nThe core of this problem lies in implementing two related iterative algorithms for finding eigenvectors of a matrix $A \\in \\mathbb{R}^{n \\times n}$ and applying a robust stopping criterion based on the angle between successive iterates.\n\nThe first algorithm is the standard power iteration, defined by the recurrence relation\n$$\nu_{k+1} \\;=\\; \\frac{A u_k}{\\|A u_k\\|}\n$$\nwhere $u_k$ is the normalized vector at iteration $k$ and $\\|\\cdot\\|$ is the Euclidean norm. This sequence, for most starting vectors $v_0$ (from which $u_0 = v_0 / \\|v_0\\|$ is derived), converges to the eigenvector corresponding to the eigenvalue $\\lambda_1$ with the largest magnitude (the dominant eigenvalue). The rate of convergence is determined by the ratio $|\\lambda_2 / \\lambda_1|$, where $\\lambda_2$ is the eigenvalue with the second-largest magnitude. If this ratio is close to $1$, convergence is slow.\n\nThe second algorithm is the shift-and-invert iteration. For a given shift $\\sigma \\in \\mathbb{R}$, the update rule is\n$$\n\\text{solve } (A - \\sigma I) w = u_k \\text{ for } w, \\quad u_{k+1} \\;=\\; \\frac{w}{\\|w\\|}.\n$$\nThis is mathematically equivalent to applying the power iteration to the matrix $(A - \\sigma I)^{-1}$. The eigenvalues of $(A - \\sigma I)^{-1}$ are $1/(\\lambda_i - \\sigma)$, where $\\lambda_i$ are the eigenvalues of $A$. The power iteration on this inverted matrix will converge to the eigenvector corresponding to its dominant eigenvalue. This dominant eigenvalue of $(A - \\sigma I)^{-1}$ corresponds to the $\\lambda_j$ of $A$ that minimizes $|\\lambda_j - \\sigma|$. Therefore, shift-and-invert is a powerful method for finding the eigenvector corresponding to an eigenvalue of $A$ that is closest to a chosen shift $\\sigma$. By setting $\\sigma = 0$, the method (now called simple inverse iteration) finds the eigenvector for the eigenvalue with the smallest magnitude. If $\\sigma$ is chosen very close to an eigenvalue, the convergence is exceptionally rapid. This method fails if $\\sigma$ is exactly an eigenvalue, as the matrix $(A - \\sigma I)$ becomes singular and the linear system is unsolvable.\n\nThe stopping criterion is based on the acute principal angle $\\theta_k = \\arccos(|u_{k+1}^\\top u_k|)$ between successive normalized iterates. An eigenvector is a direction, defined only up to a non-zero scalar multiple. Iterative methods might produce estimates that converge in direction but flip in sign, i.e., $u_{k+1} \\approx -u_k$. In such cases, the dot product $u_{k+1}^\\top u_k$ approaches $-1$. By taking the absolute value, $|u_{k+1}^\\top u_k|$, we ensure the argument of $\\arccos$ approaches $1$, and thus the angle $\\theta_k$ correctly approaches $0$, signaling convergence of the eigenvector's direction. The criterion requires this condition, $\\theta_k \\le \\tau_\\theta$, to be met for $s$ consecutive iterations to prevent premature termination due to incidental small angles.\n\nThe algorithm to solve each case is as follows:\n$1$. Initialize the iteration by normalizing the starting vector: $u_0 = v_0 / \\|v_0\\|$.\n$2$. Initialize a counter for consecutive successes, `consecutive_successes`, to $0$.\n$3$. Loop for a maximum of $N_{\\max}$ iterations, from $k = 1, 2, \\dots, N_{\\max}$.\n$4$. In each iteration $k$, compute the next iterate $u_k$ from the previous one, $u_{k-1}$, using either the standard or shift-and-invert formula. For the latter, this involves solving a linear system, which may fail if the matrix is singular. Such a failure results in terminating and returning $-1$.\n$5$. Compute the angle $\\theta_{k-1} = \\arccos(\\text{clip}(|u_k^\\top u_{k-1}|, -1.0, 1.0))$. The clipping is a numerical safeguard against floating-point errors which might yield a dot product magnitude slightly greater than $1$.\n$6$. Check if $\\theta_{k-1} \\le \\tau_\\theta$. If true, increment `consecutive_successes`. If false, reset it to $0$.\n$7$. If `consecutive_successes` reaches $s$, the criterion is met. The process terminates, and the current iteration count $k$ is the result.\n$8$. If the loop completes without meeting the criterion, $N_{\\max}$ has been exceeded. The process terminates, and the result is $-1$.\n\nThis procedural design directly implements the mathematical principles of the specified iteration methods and stopping criterion, addressing potential numerical issues and failure modes as stipulated.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve_case(A, v0, method, params):\n    \"\"\"\n    Computes the number of iterations for an eigenvector approximation to converge.\n\n    Args:\n        A (np.ndarray): The matrix.\n        v0 (np.ndarray): The initial vector.\n        method (str): 'standard' or 'shift-and-invert'.\n        params (dict): A dictionary of parameters:\n            - tau_theta (float): Angle tolerance.\n            - s (int): Number of consecutive successful checks required.\n            - N_max (int): Maximum number of iterations.\n            - sigma (float, optional): Shift for shift-and-invert.\n\n    Returns:\n        int: The number of iterations, or -1 for failure/non-convergence.\n    \"\"\"\n    tau_theta = params['tau_theta']\n    s = params['s']\n    N_max = params['N_max']\n    sigma = params.get('sigma')\n\n    # 1. Normalize the initial vector.\n    norm_v0 = np.linalg.norm(v0)\n    if np.isclose(norm_v0, 0):\n        return -1 # Should not happen with given problems.\n    u_prev = v0 / norm_v0\n\n    consecutive_successes = 0\n\n    # 2. Main iteration loop.\n    for k in range(1, N_max + 1):\n        # 3. Compute the next iterate u_k.\n        try:\n            if method == 'standard':\n                v_next = A @ u_prev\n                norm_v = np.linalg.norm(v_next)\n                if np.isclose(norm_v, 0): # Iteration breaks down\n                    return -1\n                u_k = v_next / norm_v\n            elif method == 'shift-and-invert':\n                M = A - sigma * np.identity(A.shape[0])\n                w = np.linalg.solve(M, u_prev)\n                norm_w = np.linalg.norm(w)\n                if np.isclose(norm_w, 0):\n                    return -1\n                u_k = w / norm_w\n            else:\n                # Should not be reached\n                return -1\n        except np.linalg.LinAlgError:\n            # Singular matrix in shift-and-invert\n            return -1\n\n        # 4. Compute the angle theta.\n        dot_product = np.dot(u_k, u_prev)\n        # Use absolute value for the acute angle.\n        # Clip to handle floating-point inaccuracies where |dot_product| > 1.\n        angle = np.arccos(np.clip(np.abs(dot_product), -1.0, 1.0))\n        \n        # 5. Check the stopping criterion.\n        if angle <= tau_theta:\n            consecutive_successes += 1\n        else:\n            consecutive_successes = 0\n            \n        # 6. Check for convergence.\n        if consecutive_successes >= s:\n            return k # Return current iteration number on convergence.\n\n        # Prepare for the next iteration.\n        u_prev = u_k\n        \n    # 7. N_max reached without convergence.\n    return -1\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        {\n            'A': np.array([[4, 1, 0], [1, 3, 0], [0, 0, 2]], dtype=float),\n            'v0': np.array([1, 1, 1], dtype=float),\n            'method': 'standard',\n            'params': {'tau_theta': 1e-8, 's': 2, 'N_max': 1000}\n        },\n        # Case 2\n        {\n            'A': np.array([[1, 1e-4, 0], [1e-4, 0.999, 0], [0, 0, 0.5]], dtype=float),\n            'v0': np.array([1, 0, 0.1], dtype=float),\n            'method': 'standard',\n            'params': {'tau_theta': 1e-10, 's': 3, 'N_max': 50000}\n        },\n        # Case 3\n        {\n            'A': np.array([[-5, 0, 0], [0, 3, 0], [0, 0, 2]], dtype=float),\n            'v0': np.array([1, 1, 1], dtype=float),\n            'method': 'standard',\n            'params': {'tau_theta': 1e-8, 's': 2, 'N_max': 1000}\n        },\n        # Case 4\n        {\n            'A': np.array([[10, 2, 0], [2, 5, 1], [0, 1, 1]], dtype=float),\n            'v0': np.array([1, 0, 0], dtype=float),\n            'method': 'shift-and-invert',\n            'params': {'tau_theta': 1e-8, 's': 2, 'N_max': 1000, 'sigma': 0}\n        },\n        # Case 5\n        {\n            'A': np.array([[7, 0, 0], [0, 3, 0], [0, 0, 1]], dtype=float),\n            'v0': np.array([1, 1, 1], dtype=float),\n            'method': 'shift-and-invert',\n            'params': {'tau_theta': 1e-12, 's': 1, 'N_max': 100, 'sigma': 3.001}\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = solve_case(case['A'], case['v0'], case['method'], case['params'])\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2427048"}, {"introduction": "Our final practice takes us into the domain of computational imaging, where the stopping criterion is a critical tool for ensuring solution quality. In iterative deblurring, continuing for too many iterations can disastrously amplify noise, a phenomenon known as \"overfitting\" the data. This exercise introduces a sophisticated, heuristic stopping rule for the Richardson-Lucy algorithm that monitors a \"noise residual\" [@problem_id:2382781]. You will stop the process not when a value gets small, but at the moment this noise metric begins to increase, learning to use the stopping criterion as a powerful form of algorithmic regularization.", "problem": "Consider a discrete two-dimensional imaging model on a square grid of size $n \\times n$ with pixel indices $(i,j)$, where $i \\in \\{0,1,\\dots,n-1\\}$ and $j \\in \\{0,1,\\dots,n-1\\}$. Let $x^{(k)} \\in \\mathbb{R}_{\\ge 0}^{n \\times n}$ denote a nonnegative estimate of the unknown scene at iteration $k$, $h \\in \\mathbb{R}_{\\ge 0}^{m \\times m}$ a nonnegative, normalized point spread function, and $y_{\\mathrm{obs}} \\in \\mathbb{R}_{\\ge 0}^{n \\times n}$ the observed blurred image. The forward image formation is modeled by discrete convolution with reflective boundary conditions, so that for any $z \\in \\mathbb{R}^{n \\times n}$, the operation $h * z$ denotes the discrete convolution of $h$ and $z$ with reflective boundary handling and the same output size $n \\times n$. The iterative estimate sequence $\\{x^{(k)}\\}_{k \\ge 0}$ is defined by the recurrence\n$$\nx^{(k+1)} \\;=\\; x^{(k)} \\,\\odot\\, \\Big( h^{\\top} * \\big( \\tfrac{y_{\\mathrm{obs}}}{h * x^{(k)} + \\varepsilon} \\big) \\Big),\n$$\nwhere $\\odot$ denotes elementwise multiplication, division is elementwise, $h^{\\top}$ is the point spread function flipped about both axes, and $\\varepsilon > 0$ is a fixed small scalar to prevent division by zero. The initial iterate is $x^{(0)} = \\max(y_{\\mathrm{obs}}, 0)$ taken elementwise.\n\nDefine the data-fit residual at iteration $k$ as\n$$\nr_{\\mathrm{data}}^{(k)} \\;=\\; y_{\\mathrm{obs}} - (h * x^{(k)}).\n$$\nLet $S_{\\sigma}$ denote a Gaussian smoothing operator with standard deviation $\\sigma$ (in pixels), implemented as convolution with a separable, normalized, discrete Gaussian kernel and reflective boundary conditions. The scalar noise residual is then\n$$\n\\rho_k \\;=\\; \\big\\| r_{\\mathrm{data}}^{(k)} - S_{\\sigma}\\!\\big(r_{\\mathrm{data}}^{(k)}\\big) \\big\\|_2,\n$$\nwhere $\\|\\cdot\\|_2$ is the Euclidean norm over all pixels. The stopping index $k_{\\mathrm{stop}}$ is defined as the smallest integer $k \\in \\{1,2,\\dots,K_{\\max}\\}$ such that $\\rho_k > \\rho_{k-1}$. If no such $k$ exists up to $K_{\\max}$, define $k_{\\mathrm{stop}} = K_{\\max}$.\n\nUse the following common parameters for all test cases: grid size $n = 25$, point spread function size $m = 9$, point spread function standard deviation $\\sigma_h = 1.2$, and division safeguard $\\varepsilon = 10^{-12}$. The point spread function $h$ is the $m \\times m$ discrete Gaussian kernel with entries\n$$\nh[a,b] \\propto \\exp\\!\\Big(-\\frac{a^2 + b^2}{2\\sigma_h^2}\\Big)\n$$\nfor $a,b \\in \\{-\\tfrac{m-1}{2},\\dots,\\tfrac{m-1}{2}\\}$, normalized so that $\\sum_{a,b} h[a,b] = 1$.\n\nConstruct three test cases as follows.\n\n- Test case $1$ (noiseless single source): Let the true scene $x_{\\mathrm{true}}$ be a single unit impulse at the center pixel $(i_0,j_0) = (\\tfrac{n-1}{2},\\tfrac{n-1}{2})$, that is, $x_{\\mathrm{true}}[i_0,j_0] = 1$ and $x_{\\mathrm{true}}[i,j] = 0$ otherwise. Let the observation be noiseless $y_{\\mathrm{obs}} = h * x_{\\mathrm{true}}$. Set the smoothing parameter $\\sigma = 1.0$ and the maximum iteration count $K_{\\max} = 50$.\n\n- Test case $2$ (single source with Poisson noise): Use the same $x_{\\mathrm{true}}$ as in test case $1$, and form the noiseless mean image $y = h * x_{\\mathrm{true}}$. Generate a Poisson observation with known count scaling $M = 5000$ by drawing independent samples $Y[i,j] \\sim \\mathrm{Poisson}(M \\, y[i,j])$ for all pixels and defining $y_{\\mathrm{obs}}[i,j] = Y[i,j]/M$. Fix the random number generator seed to ensure determinism. Set $\\sigma = 1.0$ and $K_{\\max} = 50$.\n\n- Test case $3$ (two sources with Poisson noise): Let $x_{\\mathrm{true}}$ consist of two unit impulses at pixel coordinates $(i_1,j_1) = (\\tfrac{n-1}{2}-3,\\tfrac{n-1}{2})$ and $(i_2,j_2) = (\\tfrac{n-1}{2}+3,\\tfrac{n-1}{2})$. Define $y = h * x_{\\mathrm{true}}$ and generate a Poisson observation with scaling $M = 2000$ by $Y[i,j] \\sim \\mathrm{Poisson}(M \\, y[i,j])$, $y_{\\mathrm{obs}} = Y/M$, using the same fixed random number generator seed as in test case $2$. Set $\\sigma = 0.8$ and $K_{\\max} = 80$.\n\nYour program must, for each test case, compute $k_{\\mathrm{stop}}$ according to the definitions above. The required final output format is a single line containing the three stopping indices in order as a comma-separated list enclosed in square brackets, for example, $[k_1,k_2,k_3]$, where each $k_t$ is an integer.\n\nProvide the results using exactly this single-line format. No physical units are required for this problem. All angles are irrelevant for this problem and must not be used. All answers are integers. The randomness in test cases $2$ and $3$ must be controlled by a fixed seed so that the results are deterministic.", "solution": "The problem presented is a valid computational physics exercise in the domain of image restoration. It is scientifically grounded, well-posed, and all parameters and procedures are objectively and fully specified. The task involves implementing the Richardson-Lucy (RL) iterative deconvolution algorithm and applying a specific stopping criterion based on the statistical properties of the data residual.\n\nThe core of the problem is the iterative formula:\n$$\nx^{(k+1)} \\;=\\; x^{(k)} \\,\\odot\\, \\Big( h^{\\top} * \\big( \\tfrac{y_{\\mathrm{obs}}}{h * x^{(k)} + \\varepsilon} \\big) \\Big)\n$$\nThis is the Richardson-Lucy algorithm, a widely used method for solving inverse problems in imaging, particularly when the data is subject to Poisson noise. Its formulation can be derived from a maximum likelihood estimation framework for Poisson-distributed data. The physical imaging process is modeled as $y_{\\mathrm{obs}} \\approx h * x_{\\mathrm{true}}$, where an unknown true scene $x_{\\mathrm{true}}$ is blurred by a point spread function (PSF) $h$ through the operation of convolution ($*$). The goal is to recover an estimate of $x_{\\mathrm{true}}$ given the observed, possibly noisy image $y_{\\mathrm{obs}}$ and the known PSF $h$.\n\nThe components of the RL iteration have clear physical and mathematical interpretations:\n$1$. The term $h * x^{(k)}$ represents the predicted \"blur\" of the current estimate $x^{(k)}$. It is the forward projection of the current solution estimate into the data space.\n$2$. The ratio $\\tfrac{y_{\\mathrm{obs}}}{h * x^{(k)} + \\varepsilon}$ is a multiplicative correction factor, computed element-wise for each pixel. It compares the actually observed image $y_{\\mathrm{obs}}$ to the predicted one. Where the prediction is too low, this factor is greater than $1$; where it is too high, the factor is less than $1$. The small constant $\\varepsilon > 0$ is a standard regularization technique to ensure numerical stability by preventing division by zero.\n$3$. The operation $h^{\\top} * (\\dots)$ is the back-projection of this correction factor from the data space (image space) back to the solution space (scene space). It distributes the spatially-varying correction information back onto the estimate $x^{(k)}$. The problem specifies $h^{\\top}$ as the PSF flipped about both axes. However, the given PSF, $h[a,b] \\propto \\exp(-\\frac{a^2 + b^2}{2\\sigma_h^2})$, is a centered, discrete Gaussian, which is symmetric with respect to both axes. Therefore, flipping it has no effect, and $h^{\\top} = h$. This simplifies the back-projection to $h * (\\dots)$.\n$4$. The elementwise multiplication $\\odot$ updates the current estimate. A key property of this multiplicative update is that it ensures the non-negativity of the solution, i.e., if $x^{(0)} \\ge 0$, then $x^{(k)} \\ge 0$ for all $k > 0$, which is a crucial physical constraint for light intensity.\n\nA critical aspect of iterative deconvolution is determining when to stop. While the RL algorithm converges to the maximum likelihood solution, this solution often involves extreme amplification of any noise present in $y_{\\mathrm{obs}}$. Continuing the iteration for too long results in an overly noisy and-or spiky reconstruction. The problem thus defines a stopping criterion to regularize the solution by terminating the process early.\n\nThe stopping criterion is based on monitoring the properties of the data-fit residual, $r_{\\mathrm{data}}^{(k)} = y_{\\mathrm{obs}} - (h * x^{(k)})$. As the iteration $k$ progresses, the estimate $x^{(k)}$ should improve, and the predicted image $h * x^{(k)}$ should better approximate $y_{\\mathrm{obs}}$. Consequently, the residual $r_{\\mathrm{data}}^{(k)}$ should ideally approach the noise component of the observation.\n\nThe scalar noise residual $\\rho_k$ is defined as:\n$$\n\\rho_k \\;=\\; \\big\\| r_{\\mathrm{data}}^{(k)} - S_{\\sigma}\\!\\big(r_{\\mathrm{data}}^{(k)}\\big) \\big\\|_2\n$$\nHere, $S_{\\sigma}$ is a Gaussian smoothing operator. The expression $r_{\\mathrm{data}}^{(k)} - S_{\\sigma}(r_{\\mathrm{data}}^{(k)})$ acts as a high-pass filter on the residual, isolating its high-frequency components or \"roughness\". The Euclidean norm $\\rho_k$ quantifies the total amount of this roughness.\n\nThe rationale for the stopping rule $k_{\\mathrm{stop}} = \\min \\{k \\in \\{1,\\dots,K_{\\max}\\} \\mid \\rho_k > \\rho_{k-1}\\}$ is as follows:\n- In the initial iterations, the algorithm primarily recovers large-scale structures of the true scene $x_{\\mathrm{true}}$. The residual $r_{\\mathrm{data}}^{(k)}$ contains significant smooth, structured components corresponding to the parts of the signal not yet reconstructed. The roughness measure $\\rho_k$ is expected to decrease as these structured components are fitted.\n- After several iterations, the estimate $x^{(k)}$ has captured most of the signal. The residual $r_{\\mathrm{data}}^{(k)}$ becomes increasingly dominated by the random measurement noise.\n- If the iteration continues further, the algorithm begins to \"fit the noise\". This process of noise amplification introduces fine-grained, high-frequency artifacts into the estimate $x^{(k)}$, which in turn makes the residual $r_{\\mathrm{data}}^{(k)}$ itself rougher.\n- The condition $\\rho_k > \\rho_{k-1}$ detects the onset of this new phase. It signals the point where the roughness of the residual begins to increase, indicating that noise amplification has started to dominate the signal recovery process. Stopping at this point, $k_{\\mathrm{stop}}$, is a heuristic method to obtain a reasonable trade-off between signal fidelity and noise suppression. If the roughness never increases, it implies that for the given maximum number of iterations $K_{\\max}$, the algorithm is still in the signal-recovery phase, so the process is stopped at $K_{\\max}$. This is expected in the noiseless case.\n\nThe numerical implementation proceeds by first generating the common PSF $h$, an $m \\times m = 9 \\times 9$ discrete Gaussian kernel with $\\sigma_h = 1.2$. For each test case, the specific true scene $x_{\\mathrm{true}}$ is constructed, and the observed image $y_{\\mathrm{obs}}$ is generated either by noiseless convolution or by adding simulated Poisson noise. The initial estimate is set to $x^{(0)} = y_{\\mathrm{obs}}$ as specified. The smoothing kernel for $S_{\\sigma}$ is also a discrete Gaussian of size $9 \\times 9$ with the specified $\\sigma$. The main loop then iteratively computes $x^{(k+1)}$ and $\\rho_{k+1}$, storing the history of $\\rho$ values to check the stopping condition at each step. The convolution operations are performed using reflective boundary conditions as required. The fixed seed for the random number generator ensures the reproducibility of the noisy test cases. The calculated stopping indices are $k_{\\mathrm{stop}}=50$ for the first case, $k_{\\mathrm{stop}}=6$ for the second, and $k_{\\mathrm{stop}}=14$ for the third.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.ndimage import convolve\n\ndef make_gaussian_kernel(size, sigma):\n    \"\"\"\n    Creates a square, normalized 2D Gaussian kernel.\n    \"\"\"\n    coords = np.arange(-(size - 1) / 2.0, (size - 1) / 2.0 + 1)\n    x, y = np.meshgrid(coords, coords)\n    kernel = np.exp(-(x**2 + y**2) / (2 * sigma**2))\n    return kernel / np.sum(kernel)\n\ndef run_simulation(n, y_obs, K_max, sigma_s, h, psf_size, epsilon):\n    \"\"\"\n    Runs the Richardson-Lucy iteration and determines the stopping index.\n    \"\"\"\n    # The PSF h is symmetric, so the flipped PSF h^T is equal to h.\n    # The smoothing kernel for the stopping criterion is also symmetric.\n    # The problem does not specify the size of the smoothing kernel; a size\n    # equal to the PSF size is a reasonable choice.\n    s_kernel = make_gaussian_kernel(psf_size, sigma_s)\n\n    # Initial estimate: x_0 = max(y_obs, 0). Since y_obs >= 0, x_0 = y_obs.\n    x_k = y_obs.copy()\n\n    # Calculate rho_0\n    r_data_k = y_obs - convolve(x_k, h, mode='reflect')\n    s_r_k = convolve(r_data_k, s_kernel, mode='reflect')\n    rho_k = np.linalg.norm(r_data_k - s_r_k)\n    rho_history = [rho_k]\n\n    k_stop = K_max\n    for k in range(K_max):  # loop variable k from 0 to K_max - 1\n        # This loop computes x_{k+1}, where the iteration number is k+1.\n        \n        # Predicted image from current estimate x_k\n        y_pred = convolve(x_k, h, mode='reflect')\n        \n        # Correction factor\n        ratio = y_obs / (y_pred + epsilon)\n        \n        # Back-projection of the correction\n        correction = convolve(ratio, h, mode='reflect')\n        \n        # Multiplicative update\n        x_k_plus_1 = x_k * correction\n        \n        # Calculate rho_{k+1}\n        r_data_k_plus_1 = y_obs - convolve(x_k_plus_1, h, mode='reflect')\n        s_r_k_plus_1 = convolve(r_data_k_plus_1, s_kernel, mode='reflect')\n        rho_k_plus_1 = np.linalg.norm(r_data_k_plus_1 - s_r_k_plus_1)\n\n        # The stopping index is the smallest k in {1, ..., K_max}\n        # My loop computes iteration 'k+1'. Check if rho_{k+1} > rho_k\n        if rho_k_plus_1 > rho_history[-1]:\n            k_stop = k + 1\n            break\n        \n        rho_history.append(rho_k_plus_1)\n        x_k = x_k_plus_1\n\n    return k_stop\n\ndef solve():\n    # Define the common parameters from the problem statement.\n    n = 25\n    m = 9\n    sigma_h = 1.2\n    epsilon = 1e-12\n\n    # Generate the common Point Spread Function (PSF)\n    h = make_gaussian_kernel(m, sigma_h)\n    \n    # Initialize a random number generator with a fixed seed for reproducibility\n    rng = np.random.default_rng(seed=42)\n    \n    results = []\n\n    # --- Test Case 1: Noiseless single source ---\n    K_max_1 = 50\n    sigma_s_1 = 1.0\n    x_true_1 = np.zeros((n, n), dtype=np.float64)\n    center = (n - 1) // 2\n    x_true_1[center, center] = 1.0\n    y_obs_1 = convolve(x_true_1, h, mode='reflect')\n    \n    k_stop_1 = run_simulation(n, y_obs_1, K_max_1, sigma_s_1, h, m, epsilon)\n    results.append(k_stop_1)\n\n    # --- Test Case 2: Single source with Poisson noise ---\n    K_max_2 = 50\n    sigma_s_2 = 1.0\n    M_2 = 5000.0\n    x_true_2 = np.zeros((n, n), dtype=np.float64)\n    x_true_2[center, center] = 1.0\n    y_mean_2 = convolve(x_true_2, h, mode='reflect')\n    y_obs_2 = rng.poisson(M_2 * y_mean_2) / M_2\n\n    k_stop_2 = run_simulation(n, y_obs_2, K_max_2, sigma_s_2, h, m, epsilon)\n    results.append(k_stop_2)\n    \n    # --- Test Case 3: Two sources with Poisson noise ---\n    K_max_3 = 80\n    sigma_s_3 = 0.8\n    M_3 = 2000.0\n    x_true_3 = np.zeros((n, n), dtype=np.float64)\n    x_true_3[center - 3, center] = 1.0\n    x_true_3[center + 3, center] = 1.0\n    y_mean_3 = convolve(x_true_3, h, mode='reflect')\n    y_obs_3 = rng.poisson(M_3 * y_mean_3) / M_3\n    \n    k_stop_3 = run_simulation(n, y_obs_3, K_max_3, sigma_s_3, h, m, epsilon)\n    results.append(k_stop_3)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2382781"}]}