{"hands_on_practices": [{"introduction": "How can we obtain perfect randomness from a biased source? This exercise explores the elegant von Neumann rejection method, a classic algorithm for extracting unbiased bits from a biased coin. By analyzing pairs of flips, you will derive and verify how this technique leverages basic probability to purify randomness, a foundational concept in simulation and information theory. [@problem_id:2433264]", "problem": "You are given access to a simulated biased coin whose flips are independent and identically distributed (i.i.d.). Each flip returns heads with probability $p \\in (0,1)$, $p \\neq 0.5$, and tails otherwise. Your task is to use the von Neumann rejection method to extract unbiased bits from this biased coin and to verify the unbiasedness and efficiency both analytically and numerically. The program you write must be a complete, runnable program. For reproducibility, all randomness must be generated from a single seeded random number generator (RNG), with the seed fixed to $314159$.\n\nStart from the following fundamental base: independence of i.i.d. Bernoulli trials, the multiplication rule for probabilities, conditional probability, and the law of large numbers.\n\nImplement the following:\n\n- Derivation tasks:\n  1. Consider two consecutive independent coin flips $X_1$ and $X_2$, where $X_i \\in \\{0,1\\}$ with $\\mathbb{P}(X_i = 1) = p$ denoting heads and $\\mathbb{P}(X_i = 0) = 1 - p$ denoting tails. The von Neumann extractor reads non-overlapping pairs $(X_1, X_2), (X_3, X_4), \\dots$ and does the following: if a pair is $(1,0)$ interpret it as an output bit $1$; if a pair is $(0,1)$ interpret it as an output bit $0$; if a pair is $(0,0)$ or $(1,1)$ discard the pair and produce no output. From the independence of $X_1$ and $X_2$ and the basic rules of probability, derive a general expression for the probability that an output bit is produced from a single pair, and show that the output bit is unbiased in the sense that its expected value is $0.5$.\n  2. From first principles, derive a general expression for the expected number of output bits per input flip.\n\n- Computational tasks:\n  1. Implement a function that, given parameters $p$ and an even number of flips $N$ (if an odd $N$ is provided, discard the last flip to keep pairs non-overlapping), simulates $N$ flips of the biased coin using a single seeded RNG and applies the von Neumann extractor to produce an array of output bits. The simulation must use the seed $314159$ for all test cases so that results are reproducible.\n  2. For each test case below, compute:\n     - The empirical absolute bias of the extracted bits, defined as $\\left|\\bar{Y} - 0.5\\right|$, where $\\bar{Y}$ is the sample mean of the extracted bits. If no bits are extracted, define $\\bar{Y} = 0.5$ by convention for this metric.\n     - The absolute error between the empirical output rate (extracted bits per input flip actually used, where the number of input flips used is the largest even number $\\leq N$) and your derived theoretical expected rate from the derivation step.\n  3. Use the following fixed test suite of parameter values:\n     - Case $1$: $p = 0.2$, $N = 200000$.\n     - Case $2$: $p = 0.8$, $N = 200000$.\n     - Case $3$: $p = 0.01$, $N = 300000$.\n     - Case $4$: $p = 0.49$, $N = 200000$.\n\n- Final output format:\n  Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case in the order given above, output two floats rounded to $6$ decimal places: first the empirical absolute bias, then the absolute error in the output rate. Therefore, the final output must contain $8$ floats in total in the order\n  $[\\text{bias}_1,\\text{rate\\_err}_1,\\text{bias}_2,\\text{rate\\_err}_2,\\text{bias}_3,\\text{rate\\_err}_3,\\text{bias}_4,\\text{rate\\_err}_4]$.\n\nThere are no physical units involved in this problem. Angles are not used. All results are to be reported as plain decimal numbers. The program must not read any input and must not write any output other than the single required line. The code must be fully self-contained and runnable.", "solution": "The problem statement is subjected to validation.\n\nStep 1: Extract Givens\n- A simulated biased coin with independent and identically distributed (i.i.d.) flips.\n- Probability of heads (denoted as $1$) is $p$, where $p \\in (0,1)$ and $p \\neq 0.5$. Probability of tails (denoted as $0$) is $1-p$.\n- The von Neumann rejection method is defined as follows: for non-overlapping pairs of flips $(X_{2k-1}, X_{2k})$, if the pair is $(1,0)$, output a bit $1$; if the pair is $(0,1)$, output a bit $0$; if the pair is $(0,0)$ or $(1,1)$, discard the pair.\n- A single seeded random number generator (RNG) with seed $314159$ must be used for all simulations.\n- Foundational principles to be used: independence of i.i.d. Bernoulli trials, multiplication rule for probabilities, conditional probability, and the law of large numbers.\n- Derivation Tasks:\n  1. Derive the probability that an output bit is produced from a single pair.\n  2. Show that the output bit is unbiased, i.e., its expected value is $0.5$.\n  3. Derive the expected number of output bits per input flip.\n- Computational Tasks:\n  1. Implement a function to simulate $N$ flips (or $N-1$ if $N$ is odd) and apply the extractor.\n  2. For each test case, compute the empirical absolute bias $|\\bar{Y} - 0.5|$ (where $\\bar{Y}=0.5$ if no bits are produced) and the absolute error between the empirical and theoretical output rates.\n  3. Test Suite:\n     - Case 1: $p = 0.2$, $N = 200000$.\n     - Case 2: $p = 0.8$, $N = 200000$.\n     - Case 3: $p = 0.01$, $N = 300000$.\n     - Case 4: $p = 0.49$, $N = 200000$.\n- Output Format: A single line string `[bias_1,rate_err_1,...,bias_4,rate_err_4]` with float values rounded to $6$ decimal places.\n\nStep 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem describes the von Neumann extractor, a well-established and correct algorithm in computational physics and computer science for randomness extraction. The underlying principles are fundamental probability theory. The problem is scientifically sound.\n- **Well-Posed:** The problem is clearly defined. The algorithm, inputs, and required outputs are all specified unambiguously. A unique numerical result is guaranteed due to the fixed RNG seed.\n- **Objective:** The problem is stated in objective, mathematical terms, free of any subjectivity or ambiguity.\n- **Flaw Checklist:** The problem does not violate any of the listed invalidity criteria. It is complete, consistent, realistic, and verifiable.\n\nStep 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\nThe foundation of this problem lies in the properties of i.i.d. Bernoulli trials. Let $X_i$ be a random variable representing the outcome of the $i$-th coin flip, where $X_i=1$ for heads and $X_i=0$ for tails. The problem states that $\\mathbb{P}(X_i = 1) = p$ and $\\mathbb{P}(X_i = 0) = 1-p$ for all $i$. The flips are independent.\n\nThe von Neumann method considers non-overlapping pairs of flips, $(X_1, X_2)$. Due to independence, the probability of each of the four possible outcomes for a pair is given by the multiplication rule:\n- $\\mathbb{P}(X_1=0, X_2=0) = \\mathbb{P}(X_1=0)\\mathbb{P}(X_2=0) = (1-p)(1-p) = (1-p)^2$\n- $\\mathbb{P}(X_1=0, X_2=1) = \\mathbb{P}(X_1=0)\\mathbb{P}(X_2=1) = (1-p)p$\n- $\\mathbb{P}(X_1=1, X_2=0) = \\mathbb{P}(X_1=1)\\mathbb{P}(X_2=0) = p(1-p)$\n- $\\mathbb{P}(X_1=1, X_2=1) = \\mathbb{P}(X_1=1)\\mathbb{P}(X_2=1) = p^2$\n\nThe sum of these probabilities is $(1-p)^2 + p(1-p) + p(1-p) + p^2 = (1-2p+p^2) + 2p - 2p^2 + p^2 = 1$, as expected.\n\nFirst, we derive the probability that a single pair of flips produces an output bit. Let $E$ be the event that an output is produced. This occurs if the outcome is either $(0,1)$ or $(1,0)$. These are disjoint events. Thus, the probability of $E$ is the sum of their individual probabilities:\n$$\n\\mathbb{P}(E) = \\mathbb{P}((X_1=0, X_2=1) \\cup (X_1=1, X_2=0)) = \\mathbb{P}(X_1=0, X_2=1) + \\mathbb{P}(X_1=1, X_2=0)\n$$\nSubstituting the probabilities derived above:\n$$\n\\mathbb{P}(E) = p(1-p) + p(1-p) = 2p(1-p)\n$$\nThis is the general expression for the probability that a single pair yields an output bit.\n\nNext, we must show that the output bit, let us call it $Y$, is unbiased. This means showing that $\\mathbb{P}(Y=1) = \\mathbb{P}(Y=0) = 0.5$. The value of $Y$ is determined conditional on the event $E$ having occurred.\nBy the definition of the method, $Y=1$ if the pair is $(1,0)$, and $Y=0$ if the pair is $(0,1)$. Using the formula for conditional probability, $\\mathbb{P}(A|B) = \\mathbb{P}(A \\cap B)/\\mathbb{P}(B)$:\n$$\n\\mathbb{P}(Y=1) = \\mathbb{P}((X_1=1, X_2=0) | E) = \\frac{\\mathbb{P}((X_1=1, X_2=0) \\cap E)}{\\mathbb{P}(E)}\n$$\nSince the event $(X_1=1, X_2=0)$ is a case where an output is produced, the intersection $(X_1=1, X_2=0) \\cap E$ is simply the event $(X_1=1, X_2=0)$ itself. Therefore:\n$$\n\\mathbb{P}(Y=1) = \\frac{\\mathbb{P}(X_1=1, X_2=0)}{\\mathbb{P}(E)} = \\frac{p(1-p)}{2p(1-p)}\n$$\nThe problem specifies that $p \\in (0,1)$, which ensures that $p(1-p) \\neq 0$. We can safely cancel this term:\n$$\n\\mathbb{P}(Y=1) = \\frac{1}{2} = 0.5\n$$\nSimilarly, for the output bit $Y=0$:\n$$\n\\mathbb{P}(Y=0) = \\mathbb{P}((X_1=0, X_2=1) | E) = \\frac{\\mathbb{P}(X_1=0, X_2=1)}{\\mathbb{P}(E)} = \\frac{p(1-p)}{2p(1-p)} = \\frac{1}{2} = 0.5\n$$\nThe expected value of the output bit $Y$ is:\n$$\n\\mathbb{E}[Y] = 1 \\cdot \\mathbb{P}(Y=1) + 0 \\cdot \\mathbb{P}(Y=0) = 1 \\cdot (0.5) + 0 \\cdot (0.5) = 0.5\n$$\nThis formally proves that the output bits are unbiased. The key insight of von Neumann is that the events $(1,0)$ and $(0,1)$ have equal probability $p(1-p)$, irrespective of the bias $p$.\n\nFinally, we derive the expected number of output bits per input flip, which represents the efficiency or theoretical rate of the extractor. Each pair consists of $2$ input flips. Let $N_{out}$ be the number of bits produced by a single pair. $N_{out}$ is a random variable that takes the value $1$ if event $E$ occurs and $0$ otherwise. The expected value of $N_{out}$ is:\n$$\n\\mathbb{E}[N_{out}] = 1 \\cdot \\mathbb{P}(N_{out}=1) + 0 \\cdot \\mathbb{P}(N_{out}=0) = \\mathbb{P}(E) = 2p(1-p)\n$$\nThis is the expected number of output bits for every $2$ input flips. To find the expected number of output bits per single input flip, we divide by $2$:\n$$\n\\text{Theoretical Rate} = \\frac{\\mathbb{E}[N_{out}]}{2} = \\frac{2p(1-p)}{2} = p(1-p)\n$$\nThis expression indicates that the efficiency is maximized when $p=0.5$ (value is $0.25$) and approaches $0$ as $p$ approaches $0$ or $1$.\n\nFor the computational task, a simulation is implemented. A single RNG is seeded with $314159$ to ensure reproducibility. For each test case $(p, N)$, a sequence of $N_{used} = N - (N \\pmod 2)$ biased flips is generated. These flips are grouped into $N_{used}/2$ pairs. We count the number of occurrences of $(1,0)$ pairs (let this be $n_{10}$) and $(0,1)$ pairs (let this be $n_{01}$).\nThe total number of extracted bits is $n_{out} = n_{10} + n_{01}$.\nThe sample mean of the output bits is $\\bar{Y} = n_{10} / n_{out}$. If $n_{out}=0$, we use $\\bar{Y}=0.5$ by convention. The empirical absolute bias is then $|\\bar{Y} - 0.5|$.\nThe empirical output rate is $n_{out} / N_{used}$. The absolute error in the rate is the absolute difference between this empirical rate and the theoretical rate $p(1-p)$. The law of large numbers suggests that as $N \\to \\infty$, the empirical bias and rate error should converge to $0$. The provided code will compute these quantities for the specified test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the von Neumann extractor problem by deriving theoretical values,\n    running simulations, and comparing empirical results to theory.\n    \"\"\"\n    \n    # Define the fixed test suite of parameter values.\n    test_cases = [\n        (0.2, 200000),  # Case 1\n        (0.8, 200000),  # Case 2\n        (0.01, 300000), # Case 3\n        (0.49, 200000), # Case 4\n    ]\n\n    # All randomness must be generated from a single seeded RNG.\n    seed = 314159\n    rng = np.random.default_rng(seed)\n\n    results = []\n    \n    for p, N in test_cases:\n        # --- Simulation ---\n        \n        # The number of input flips used must be an even number.\n        # Discard the last flip if N is odd.\n        num_flips_used = N - (N % 2)\n        \n        # Simulate biased coin flips. Heads (1) with probability p.\n        # This is an efficient way to generate Bernoulli trials.\n        flips = (rng.random(size=num_flips_used) < p).astype(np.int8)\n        \n        # Reshape the 1D array of flips into a 2D array of non-overlapping pairs.\n        pairs = flips.reshape(-1, 2)\n        \n        # Count the number of (1, 0) and (0, 1) pairs using vectorized operations.\n        # (1, 0) pairs yield an output bit of 1.\n        num_10 = np.sum((pairs[:, 0] == 1) & (pairs[:, 1] == 0))\n        \n        # (0, 1) pairs yield an output bit of 0.\n        num_01 = np.sum((pairs[:, 0] == 0) & (pairs[:, 1] == 1))\n        \n        # Total number of extracted unbiased bits.\n        num_output_bits = num_10 + num_01\n        \n        # --- Metric Calculation ---\n        \n        # 1. Empirical Absolute Bias\n        if num_output_bits == 0:\n            # If no bits are extracted, by convention, the sample mean is 0.5.\n            sample_mean_y = 0.5\n        else:\n            # The sample mean is the number of 1s divided by the total number of bits.\n            sample_mean_y = num_10 / num_output_bits\n            \n        empirical_absolute_bias = abs(sample_mean_y - 0.5)\n        \n        # 2. Absolute Error in Output Rate\n        if num_flips_used == 0:\n            empirical_rate = 0.0\n        else:\n            # Empirical rate is the number of output bits per input flip used.\n            empirical_rate = num_output_bits / num_flips_used\n        \n        # Theoretical rate is p*(1-p) output bits per input flip.\n        theoretical_rate = p * (1.0 - p)\n        \n        rate_error = abs(empirical_rate - theoretical_rate)\n\n        # Append formatted results to the list.\n        results.append(f\"{empirical_absolute_bias:.6f}\")\n        results.append(f\"{rate_error:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2433264"}, {"introduction": "Randomly shuffling a set of items is a common task, but subtle algorithmic flaws can lead to significant bias. This practice challenges you to implement the correct Fisher-Yates shuffle alongside intentionally flawed versions to see the difference firsthand. You will use the chi-squared goodness-of-fit test to statistically quantify the bias, developing a critical tool for validating randomized algorithms in your own work. [@problem_id:2433283]", "problem": "You are asked to design and implement a program that probes the principle that an unbiased random permutation should make every item equally likely to appear in any position. Your focus is the first position. This exercise ties directly to Monte Carlo methodology in computational physics, where the fidelity of a pseudorandom number generator (PRNG) determines the validity of stochastic simulation results.\n\nStarting point and definitions:\n- A pseudorandom number generator (PRNG) produces a deterministic sequence of numbers that approximates samples from prescribed probability distributions when initialized with a seed.\n- A permutation of $N$ distinct labels is a bijection on the set $\\{0,1,\\dots,N-1\\}$. A permutation is uniform if each of the $N!$ possible permutations has probability $1/N!$.\n- If permutations are uniform, then for any fixed position (in particular, the first position), each label must appear with probability $1/N$.\n- To test whether empirical counts are consistent with a hypothesized discrete uniform distribution, one may use the classical chi-squared goodness-of-fit test: form the chi-squared statistic from observed counts and expected counts, and compare to the chi-squared distribution with appropriate degrees of freedom to obtain a $p$-value. The null hypothesis is rejected for sufficiently small $p$-values at a preselected significance level $\\alpha$.\n\nYour tasks:\n1) Implement three permutation generators on the list of labels $[0,1,\\dots,N-1]$:\n   - Method U (unbiased Fisher–Yates): Iterate an index $i$ from $N-1$ down to $1$. For each $i$, draw an integer $j$ uniformly from $\\{0,1,\\dots,i\\}$ and swap the items at indices $i$ and $j$. This is known to produce uniform permutations.\n   - Method P$(c)$ (partial Fisher–Yates): Define $m=\\lfloor c\\,N\\rfloor$ with $0<c<1$. Perform only the last $m$ swaps of Method U, that is, for $k=0,1,\\dots,m-1$ set $i=N-1-k$, draw $j$ uniformly from $\\{0,1,\\dots,i\\}$, and swap indices $i$ and $j$. This intentionally leaves the leading segment under-randomized.\n   - Method M$(b)$ (biased modulo-first-pick): Draw a single integer $x$ uniformly from $\\{0,1,\\dots,2^b-1\\}$ and set $f = x \\bmod N$. Swap indices $0$ and $f$. Then complete positions $1$ through $N-1$ by applying the Fisher–Yates procedure restricted to indices $i=N-1$ down to $1$, drawing $j$ uniformly from $\\{1,2,\\dots,i\\}$ and swapping indices $i$ and $j$. This forces the first element selection to come from a discrete pool of size $2^b$, which generally biases the first-position choice unless $N$ divides $2^b$.\n\n2) For a given method, integer $N \\ge 2$, trial count $T$, significance level $\\alpha$ (expressed as a decimal), and seed $s$, perform the following experiment:\n   - Initialize the PRNG with the integer seed $s$.\n   - Repeat the following $T$ times independently:\n     - Generate one permutation of $[0,1,\\dots,N-1]$ with the selected method.\n     - Record the label occupying index $0$ by incrementing its count.\n   - Let $\\mathbf{O}=(O_0,\\dots,O_{N-1})$ be the observed counts and $\\mathbf{E}=(E,\\dots,E)$ with $E=T/N$ the expected counts under the null hypothesis $H_0$ that the first-position label distribution is uniform on $\\{0,1,\\dots,N-1\\}$.\n   - Compute the chi-squared test statistic based on $\\mathbf{O}$ and $\\mathbf{E}$ and the corresponding $p$-value using the chi-squared distribution with $N-1$ degrees of freedom.\n   - Report the rounded $p$-value (to six decimal places) and an indicator $d$ that equals $1$ if the null hypothesis is rejected at level $\\alpha$ (that is, if $p<\\alpha$), and equals $0$ otherwise.\n\n3) Implement your solution as a complete, runnable program that uses no input and prints results for the following test suite, where each case is of the form $(\\text{method}, N, T, \\alpha, s, \\text{parameter})$ and “parameter” is interpreted according to the method as described above:\n   - Case 1 (happy path, unbiased): $(\\text{\"U\"},\\, 10,\\, 100000,\\, 0.01,\\, 10101,\\, \\text{None})$.\n   - Case 2 (edge under-randomization near the front): $(\\text{\"P\"},\\, 20,\\, 80000,\\, 1\\times 10^{-12},\\, 20202,\\, c=0.2)$.\n   - Case 3 (explicit modulo bias in first pick): $(\\text{\"M\"},\\, 10,\\, 120000,\\, 1\\times 10^{-6},\\, 30303,\\, b=8)$.\n   - Case 4 (small $N$ boundary, unbiased): $(\\text{\"U\"},\\, 2,\\, 60000,\\, 0.01,\\, 40404,\\, \\text{None})$.\n   - Case 5 (strong modulo bias with small keyspace): $(\\text{\"M\"},\\, 7,\\, 120000,\\, 1\\times 10^{-6},\\, 50505,\\, b=3)$.\n\nOutput format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element must itself be a two-element list of the form $[p, d]$, where $p$ is the $p$-value rounded to six decimal places and $d$ is the integer decision indicator defined above. For example, a valid output line looks like:\n  [[0.532114,0],[0.000000,1],[0.123456,0],[0.774321,0],[0.000000,1]]\n\nNotes:\n- There are no physical units involved, so no unit conversions are required.\n- Angles are not involved.\n- Percentages, where present (such as the significance level), are already specified as decimals and must be treated as such.", "solution": "The problem requires an empirical validation of three permutation algorithms by testing if they produce a uniform distribution of labels in the first position of a permuted array. This is a fundamental check in computational physics and Monte Carlo methods, where the quality of pseudorandomness is paramount for the validity of simulation results. The validation will be conducted using the chi-squared goodness-of-fit test.\n\nThe overall procedure for each test case involves a fixed number of trials, $T$. In each trial, an initial array of labels, $[0, 1, \\dots, N-1]$, is permuted using one of the specified methods. The label that ends up at index $0$ is recorded. After $T$ trials, we obtain a frequency distribution of observed counts $\\mathbf{O} = (O_0, O_1, \\dots, O_{N-1})$ for each label at the first position.\n\nThe null hypothesis, $H_0$, states that the permutation method is unbiased with respect to the first position, meaning every label $k \\in \\{0, \\dots, N-1\\}$ has a probability of $1/N$ of appearing there. Under $H_0$, the expected count for each label is $E_k = T/N$. The chi-squared test statistic, $\\chi^2$, is computed as:\n$$ \\chi^2 = \\sum_{k=0}^{N-1} \\frac{(O_k - E_k)^2}{E_k} $$\nThis statistic is compared against the $\\chi^2$ distribution with $df = N-1$ degrees of freedom. The $p$-value, representing the probability of observing a statistic at least as extreme as the one computed if $H_0$ were true, is determined from the survival function of this distribution: $p = P(\\chi^2_{df} \\ge \\chi^2_{\\text{observed}})$. At a given significance level $\\alpha$, $H_0$ is rejected if $p < \\alpha$.\n\nBelow is the analysis of each permutation method. All algorithms are implemented using a pseudorandom number generator (PRNG) initialized with a specific seed $s$ for reproducibility.\n\nMethod U (unbiased Fisher–Yates): This algorithm iterates an index $i$ from $N-1$ down to $1$. In each step, it selects a random index $j$ uniformly from $\\{0, 1, \\dots, i\\}$ and swaps the elements at positions $i$ and $j$. This canonical algorithm is proven to generate every possible permutation of $N$ elements with equal probability $1/N!$. Consequently, the probability of any given label appearing at any given position is exactly $1/N$. This method serves as our baseline for an unbiased permutation, and we anticipate that the statistical test will not reject the null hypothesis (i.e., will yield a large $p$-value).\n\nMethod P($c$) (partial Fisher–Yates): This method is designed to be biased. It executes only a fraction of the swaps of a full Fisher-Yates shuffle. Given a parameter $c \\in (0, 1)$, it performs $m = \\lfloor cN \\rfloor$ swaps. The problem's description \"for $k=0,1,\\dots,m-1$ set $i=N-1-k$\" dictates that these are the swaps for the largest indices, $i=N-1, N-2, \\dots, N-m$. This leaves the prefix of the array, corresponding to small indices, under-randomized. A label starting in the prefix is less likely to be moved. Specifically, the label $0$ (initially at index $0$) remains at its position unless it is selected as the index $j$ in one of the swaps. The probability of it *not* being chosen is $\\prod_{i=N-m}^{N-1} (1 - \\frac{1}{i+1}) = \\frac{N-m}{N} = 1-c$. This creates a significant bias: the label $0$ is far more likely to appear at index $0$ than any other label. We expect the test to strongly reject $H_0$ with a $p$-value close to $0$.\n\nMethod M($b$) (biased modulo-first-pick): This method introduces a bias in the very first step. It selects the element for the first position from a non-uniform distribution. An integer $x$ is drawn uniformly from $\\{0, 1, \\dots, 2^b-1\\}$, and an index $f = x \\pmod N$ is computed. The element at index $f$ (which is the label $f$) is swapped into index $0$. The subsequent shuffling step, as described, shuffles the elements at indices $1$ to $N-1$ among themselves, leaving the element at index $0$ untouched. Therefore, the distribution of the final label at index $0$ is precisely the distribution of $f$. This distribution is uniform only if $N$ divides $2^b$. If not, let $2^b = qN+r$. Labels $\\{0, \\dots, r-1\\}$ are chosen with probability $(q+1)/2^b$, while labels $\\{r, \\dots, N-1\\}$ are chosen with probability $q/2^b$. This discrepancy, however small, introduces a bias that should be detectable with a sufficiently large number of trials $T$. For the given test cases, $N$ does not divide $2^b$, so we expect to reject $H_0$. The magnitude of the bias, and thus the resulting $p$-value, depends on how much the probabilities deviate from $1/N$.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef permute_U(arr, rng):\n    \"\"\"\n    Generates a uniform permutation using the Fisher-Yates shuffle.\n    \"\"\"\n    n = len(arr)\n    for i in range(n - 1, 0, -1):\n        j = rng.integers(0, i, endpoint=True)\n        arr[i], arr[j] = arr[j], arr[i]\n    return arr\n\ndef permute_P(arr, rng, c):\n    \"\"\"\n    Generates a biased permutation using a partial Fisher-Yates shuffle.\n    \"\"\"\n    n = len(arr)\n    m = int(c * n)\n    for k in range(m):\n        i = n - 1 - k\n        # The swap is meaningful only for i > 0\n        if i > 0:\n            j = rng.integers(0, i, endpoint=True)\n            arr[i], arr[j] = arr[j], arr[i]\n    return arr\n\ndef permute_M(arr, rng, b):\n    \"\"\"\n    Generates a biased permutation using a modulo-biased first pick.\n    \"\"\"\n    n = len(arr)\n    # Draw x uniformly from {0, ..., 2^b - 1}\n    keyspace = 2**b\n    x = rng.integers(0, keyspace - 1, endpoint=True)\n    f = x % n\n    \n    # Swap element at index f into the first position\n    arr[0], arr[f] = arr[f], arr[0]\n    \n    # Shuffle the rest of the array (indices 1..N-1) among themselves\n    # The problem specifies j is drawn from {1, ..., i}\n    for i in range(n - 1, 0, -1):\n        j = rng.integers(1, i, endpoint=True)\n        arr[i], arr[j] = arr[j], arr[i]\n    return arr\n\ndef run_experiment(method, N, T, s, param):\n    \"\"\"\n    Runs the simulation to collect counts of labels at the first position.\n    \"\"\"\n    rng = np.random.default_rng(seed=s)\n    counts = np.zeros(N, dtype=np.int64)\n    \n    permute_func = None\n    if method == \"U\":\n        permute_func = lambda arr: permute_U(arr, rng)\n    elif method == \"P\":\n        permute_func = lambda arr: permute_P(arr, rng, c=param)\n    elif method == \"M\":\n        permute_func = lambda arr: permute_M(arr, rng, b=param)\n    else:\n        raise ValueError(f\"Unknown permutation method: {method}\")\n\n    for _ in range(T):\n        p = list(range(N))\n        permuted_p = permute_func(p)\n        first_element_label = permuted_p[0]\n        counts[first_element_label] += 1\n        \n    return counts\n\ndef perform_chi2_test(observed_counts, N, T, alpha):\n    \"\"\"\n    Performs the chi-squared goodness-of-fit test.\n    \"\"\"\n    # Under H0, expected count for each category is uniform.\n    expected_count = T / N\n    \n    # Calculate the chi-squared statistic\n    chisq_stat = np.sum((observed_counts - expected_count)**2 / expected_count)\n    \n    # Degrees of freedom for a GOF test is k-1.\n    df = N - 1\n    \n    # Calculate p-value using the survival function (1 - CDF).\n    p_value = chi2.sf(chisq_stat, df)\n    \n    # Decision: reject H0 if p < alpha\n    decision = 1 if p_value < alpha else 0\n    \n    return p_value, decision\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        # (method, N, T, alpha, seed, parameter)\n        (\"U\", 10, 100000, 0.01, 10101, None),\n        (\"P\", 20, 80000, 1e-12, 20202, 0.2),\n        (\"M\", 10, 120000, 1e-6, 30303, 8),\n        (\"U\", 2, 60000, 0.01, 40404, None),\n        (\"M\", 7, 120000, 1e-6, 50505, 3),\n    ]\n\n    results = []\n    for case in test_cases:\n        method, N, T, alpha, seed, param = case\n        \n        # 1. Run simulation to get observed counts\n        observed_counts = run_experiment(method, N, T, seed, param)\n        \n        # 2. Perform chi-squared test to get p-value and decision\n        p_value, decision = perform_chi2_test(observed_counts, N, T, alpha)\n        \n        results.append((p_value, decision))\n    \n    # 3. Format output string precisely as required\n    result_strings = [f\"[{p:.6f},{d}]\" for p, d in results]\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```", "id": "2433283"}, {"introduction": "Many physical processes, from particle decays to molecular orientations, involve random directions in three-dimensional space. This practice guides you through the correct method for generating points uniformly on the surface of a sphere, a non-trivial task that highlights a common pitfall in sampling. You will apply the inverse transform sampling method and use a two-dimensional chi-squared test to verify the uniformity of your distribution, mastering a technique essential for realistic physical modeling. [@problem_id:2433291]", "problem": "You are to write a complete, runnable program that generates points uniformly distributed on the surface of the unit sphere in three-dimensional space and assesses their uniformity using a two-dimensional histogram over spherical coordinates. All angles must be in radians. The surface of the unit sphere is parameterized by spherical coordinates with polar angle $\\theta \\in [0,\\pi]$ and azimuthal angle $\\phi \\in [0,2\\pi)$. The uniform surface measure has surface area element $\\mathrm{d}A = \\sin\\theta\\,\\mathrm{d}\\theta\\,\\mathrm{d}\\phi$ and total surface area $4\\pi$. For a given set of generated points, construct a two-dimensional histogram by partitioning $\\theta$ into $B_\\theta$ equal-width bins and $\\phi$ into $B_\\phi$ equal-width bins. The $\\theta$-bin edges are $\\theta_i = i\\,\\Delta\\theta$ with $\\Delta\\theta = \\pi/B_\\theta$ for $i=0,1,\\dots,B_\\theta$, and the $\\phi$-bin edges are $\\phi_j = j\\,\\Delta\\phi$ with $\\Delta\\phi = 2\\pi/B_\\phi$ for $j=0,1,\\dots,B_\\phi$. Use half-open intervals $[\\theta_i,\\theta_{i+1})$ and $[\\phi_j,\\phi_{j+1})$ for all bins except that the final bin in each dimension is closed on the right to include the endpoint $\\theta=\\pi$ and $\\phi=2\\pi$.\n\nFor testing uniformity, let $O_{i,j}$ denote the observed count in bin $(i,j)$, and let $E_{i,j}$ denote the expected count under the uniform surface measure, obtained from first principles by integrating the surface area element over the bin region and normalizing by the total surface area. That is, the expected probability for bin $(i,j)$ is the ratio of the bin’s surface area to $4\\pi$, and $E_{i,j}$ equals this probability times the total number of generated points $N$. Use the Pearson chi-squared statistic\n$$\n\\chi^2 = \\sum_{i=0}^{B_\\theta-1}\\sum_{j=0}^{B_\\phi-1} \\frac{\\left(O_{i,j}-E_{i,j}\\right)^2}{E_{i,j}},\n$$\nsumming only over bins with $E_{i,j} > 0$. The degrees of freedom are $k-1$, where $k$ is the number of bins with $E_{i,j} > 0$. For a specified significance level $\\alpha \\in (0,1)$, accept the null hypothesis of uniform surface distribution if $\\chi^2 \\leq q$, where $q$ is the $(1-\\alpha)$-quantile of the chi-squared distribution with $k-1$ degrees of freedom.\n\nYour program must, for each test case, generate exactly $N$ points on the unit sphere with a pseudo-random number generator initialized by the given integer seed, compute $\\theta$ and $\\phi$ for each point with the convention $\\phi \\in [0,2\\pi)$, build the two-dimensional histogram as specified, compute $E_{i,j}$ from first principles as described, evaluate $\\chi^2$, determine the critical value $q$ for the given $\\alpha$, and report a boolean indicating whether $\\chi^2 \\leq q$.\n\nAngle unit requirement: Report and compute all angles in radians.\n\nTest suite specification:\n- Case $1$: seed $= 12345$, $N = 40000$, $B_\\theta = 10$, $B_\\phi = 20$, $\\alpha = 0.01$.\n- Case $2$: seed $= 54321$, $N = 1280$, $B_\\theta = 8$, $B_\\phi = 16$, $\\alpha = 0.05$.\n- Case $3$: seed $= 20231102$, $N = 50000$, $B_\\theta = 2$, $B_\\phi = 3$, $\\alpha = 0.01$.\n- Case $4$: seed $= 777$, $N = 100000$, $B_\\theta = 18$, $B_\\phi = 36$, $\\alpha = 0.05$.\n\nRequired final output format: Your program should produce a single line of output containing the boolean results for the test cases, in order, as a comma-separated list enclosed in square brackets, with no spaces. For example, an output with four booleans must look like\n$[{\\tt True},{\\tt False},{\\tt True},{\\tt True}]$.", "solution": "The problem is subjected to validation.\n\n**Step 1: Extract Givens**\n\n*   **Objective**: Generate $N$ points uniformly distributed on the surface of a unit sphere in $3$-dimensional space and test for uniformity.\n*   **Coordinates**: Spherical coordinates with polar angle $\\theta \\in [0, \\pi]$ and azimuthal angle $\\phi \\in [0, 2\\pi)$.\n*   **Surface Area Element**: $\\mathrm{d}A = \\sin\\theta\\,\\mathrm{d}\\theta\\,\\mathrm{d}\\phi$.\n*   **Histogram**: $B_\\theta \\times B_\\phi$ bins over $(\\theta, \\phi)$.\n    *   $\\theta$-bins: $B_\\theta$ equal-width bins on $[0, \\pi]$.\n    *   $\\phi$-bins: $B_\\phi$ equal-width bins on $[0, 2\\pi)$.\n*   **Statistical Test**: Pearson's chi-squared test.\n*   **Observed Counts ($O_{i,j}$)**: Number of points in bin $(i,j)$.\n*   **Expected Counts ($E_{i,j}$)**: $N \\times (\\text{Area of bin } (i,j)) / (4\\pi)$, where the bin area is $\\int_{\\phi_j}^{\\phi_{j+1}} \\int_{\\theta_i}^{\\theta_{i+1}} \\sin\\theta\\,\\mathrm{d}\\theta\\,\\mathrm{d}\\phi$.\n*   **Chi-Squared Statistic**: $\\chi^2 = \\sum_{i,j} \\frac{(O_{i,j}-E_{i,j})^2}{E_{i,j}}$ over bins with $E_{i,j} > 0$.\n*   **Degrees of Freedom**: $k-1$, where $k$ is the number of bins with $E_{i,j} > 0$.\n*   **Hypothesis Decision**: For a significance level $\\alpha$, accept uniformity if $\\chi^2 \\leq q$, where $q$ is the $(1-\\alpha)$-quantile of the $\\chi^2_{k-1}$ distribution.\n*   **Test Cases**:\n    *   Case $1$: seed $= 12345$, $N = 40000$, $B_\\theta = 10$, $B_\\phi = 20$, $\\alpha = 0.01$.\n    *   Case $2$: seed $= 54321$, $N = 1280$, $B_\\theta = 8$, $B_\\phi = 16$, $\\alpha = 0.05$.\n    *   Case $3$: seed $= 20231102$, $N = 50000$, $B_\\theta = 2$, $B_\\phi = 3$, $\\alpha = 0.01$.\n    *   Case $4$: seed $= 777$, $N = 100000$, $B_\\theta = 18$, $B_\\phi = 36$, $\\alpha = 0.05$.\n\n**Step 2: Validate Using Extracted Givens**\n\n*   **Scientifically Grounded**: The problem is based on standard mathematical and statistical methods used in computational physics, namely, generation of random variates for a given distribution, spherical coordinate systems, and goodness-of-fit testing using the chi-squared statistic. All premises and formulas are factually and scientifically sound.\n*   **Well-Posed**: The problem is well-posed. It provides all necessary parameters and defines a clear, deterministic procedure (given a specific pseudorandom number generator) that leads to a unique boolean result for each test case.\n*   **Objective**: The language is objective and precise, with no reliance on subjective interpretation.\n*   **Completeness**: The problem is complete. While it does not explicitly state the algorithm for generating points on the sphere, selecting a correct, standard algorithm is an implicit and fundamental part of the task.\n\n**Step 3: Verdict and Action**\n\nThe problem statement is **valid**. It is a rigorous and well-defined computational task. A solution will be provided.\n\n**Principle-Based Solution Design**\n\nThe task requires the implementation of a statistical test for the uniformity of points on a sphere. This involves three main stages: generating the points according to the specified distribution, binning them to obtain observed frequencies, and comparing these to theoretically derived expected frequencies.\n\n**1. Generation of Uniformly Distributed Points on a Sphere**\nA naive approach of sampling $\\theta$ and $\\phi$ from uniform distributions on their respective ranges, $[0, \\pi]$ and $[0, 2\\pi)$, is incorrect as it leads to a higher density of points near the poles. The correct method must account for the surface area element $\\mathrm{d}A = \\sin\\theta\\,\\mathrm{d}\\theta\\,\\mathrm{d}\\phi$. The probability density function $p(\\theta, \\phi)$ for a point to be at $(\\theta, \\phi)$ must be proportional to the surface area element. Normalizing over the entire sphere (total area $4\\pi$) gives the joint probability density function:\n$$p(\\theta, \\phi) = \\frac{1}{4\\pi}\\sin\\theta, \\quad \\theta \\in [0, \\pi], \\phi \\in [0, 2\\pi)$$\nThe marginal probability densities are found by integrating over the other variable:\n$$p(\\theta) = \\int_0^{2\\pi} p(\\theta, \\phi) \\,\\mathrm{d}\\phi = \\int_0^{2\\pi} \\frac{1}{4\\pi}\\sin\\theta \\,\\mathrm{d}\\phi = \\frac{2\\pi}{4\\pi}\\sin\\theta = \\frac{1}{2}\\sin\\theta$$\n$$p(\\phi) = \\int_0^\\pi p(\\theta, \\phi) \\,\\mathrm{d}\\theta = \\int_0^\\pi \\frac{1}{4\\pi}\\sin\\theta \\,\\mathrm{d}\\theta = \\frac{1}{4\\pi}[-\\cos\\theta]_0^\\pi = \\frac{2}{4\\pi} = \\frac{1}{2\\pi}$$\nSince $p(\\theta, \\phi) = p(\\theta)p(\\phi)$, the variables $\\theta$ and $\\phi$ are independent and can be sampled separately. We use the inverse transform sampling method. Let $U_1$ and $U_2$ be two independent random variables uniformly distributed on $[0,1)$.\n\nFor $\\phi$, the cumulative distribution function (CDF) is $F_\\phi(\\phi') = \\int_0^{\\phi'} \\frac{1}{2\\pi} \\mathrm{d}\\phi = \\frac{\\phi'}{2\\pi}$. Setting $F_\\phi(\\phi) = U_1$ yields $\\phi = 2\\pi U_1$.\n\nFor $\\theta$, the CDF is $F_\\theta(\\theta') = \\int_0^{\\theta'} \\frac{1}{2}\\sin\\theta \\mathrm{d}\\theta = \\frac{1}{2}[-\\cos\\theta]_0^{\\theta'} = \\frac{1}{2}(1 - \\cos\\theta')$. Setting $F_\\theta(\\theta) = U_2$ gives $\\frac{1}{2}(1-\\cos\\theta) = U_2$, which rearranges to $\\cos\\theta = 1-2U_2$. Since $1-2U_2$ is uniform on $[-1,1]$, we can equivalently use a variable $v = 2U_2-1$, which is also uniform on $[-1,1]$. Thus, we generate $\\cos\\theta$ uniformly from $[-1,1]$ and then find $\\theta$:\n$$\\phi = 2\\pi U_1$$\n$$\\theta = \\arccos(2U_2-1)$$\nThis procedure generates $N$ pairs $(\\theta_k, \\phi_k)$ for $k=1, \\dots, N$.\n\n**2. Observed and Expected Frequencies**\nThe generated points are binned into a $2$-dimensional histogram of size $B_\\theta \\times B_\\phi$. The number of points falling into bin $(i,j)$, defined by the region $[\\theta_i, \\theta_{i+1}) \\times [\\phi_j, \\phi_{j+1})$, gives the observed frequency $O_{i,j}$.\n\nThe expected frequency $E_{i,j}$ for bin $(i,j)$ is $N$ times the probability of a point falling in that bin. This probability is the ratio of the bin's surface area to the total surface area of the sphere, $4\\pi$. The surface area $A_{ij}$ of bin $(i,j)$ is calculated by integrating the surface area element:\n$$A_{i,j} = \\int_{\\phi_j}^{\\phi_{j+1}} \\int_{\\theta_i}^{\\theta_{i+1}} \\sin\\theta\\,\\mathrm{d}\\theta\\,\\mathrm{d}\\phi = \\left(\\int_{\\phi_j}^{\\phi_{j+1}} \\mathrm{d}\\phi\\right) \\left(\\int_{\\theta_i}^{\\theta_{i+1}} \\sin\\theta\\,\\mathrm{d}\\theta\\right)$$\nThe bin edges are $\\theta_i = i\\Delta\\theta$ with $\\Delta\\theta = \\pi/B_\\theta$ and $\\phi_j = j\\Delta\\phi$ with $\\Delta\\phi = 2\\pi/B_\\phi$. Evaluating the integrals gives:\n$$A_{i,j} = \\Delta\\phi \\cdot [-\\cos\\theta]_{\\theta_i}^{\\theta_{i+1}} = \\frac{2\\pi}{B_\\phi} (\\cos\\theta_i - \\cos\\theta_{i+1})$$\nThe expected count $E_{i,j}$ is then:\n$$E_{i,j} = N \\cdot \\frac{A_{i,j}}{4\\pi} = N \\cdot \\frac{(2\\pi/B_\\phi) (\\cos\\theta_i - \\cos\\theta_{i+1})}{4\\pi} = \\frac{N}{2B_\\phi}(\\cos(i\\pi/B_\\theta) - \\cos((i+1)\\pi/B_\\theta))$$\nNote that $E_{i,j}$ is independent of the index $j$. Also, since $\\cos(x)$ is strictly decreasing on $[0, \\pi]$, $\\cos(i\\pi/B_\\theta) > \\cos((i+1)\\pi/B_\\theta)$ for all $i$, which means $E_{i,j} > 0$ for all bins.\n\n**3. Chi-Squared Goodness-of-Fit Test**\nWith observed counts $O_{i,j}$ and expected counts $E_{i,j}$, the Pearson's chi-squared statistic is computed:\n$$\\chi^2 = \\sum_{i=0}^{B_\\theta-1} \\sum_{j=0}^{B_\\phi-1} \\frac{(O_{i,j}-E_{i,j})^2}{E_{i,j}}$$\nThe number of bins is $k = B_\\theta B_\\phi$. Since the uniform distribution has no parameters estimated from the data, the number of degrees of freedom for the $\\chi^2$ distribution is $k-1$.\nThe null hypothesis $H_0$, that the points are uniformly distributed, is tested against the critical value $q$ from the $\\chi^2$ distribution. $q$ is the $(1-\\alpha)$-quantile for a given significance level $\\alpha$. If the computed $\\chi^2$ statistic is less than or equal to $q$, we do not reject $H_0$. The program will return a boolean value indicating if $\\chi^2 \\leq q$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef run_case(seed, N, B_theta, B_phi, alpha):\n    \"\"\"\n    Generates points on a unit sphere, performs a chi-squared test for uniformity,\n    and returns the result of the hypothesis test.\n\n    Args:\n        seed (int): The seed for the random number generator.\n        N (int): The total number of points to generate.\n        B_theta (int): The number of bins for the polar angle theta.\n        B_phi (int): The number of bins for the azimuthal angle phi.\n        alpha (float): The significance level for the chi-squared test.\n\n    Returns:\n        bool: True if the null hypothesis of uniformity is accepted, False otherwise.\n    \"\"\"\n    # 1. Generate points using inverse transform sampling\n    # This method correctly generates points uniformly distributed on the sphere's surface.\n    rng = np.random.default_rng(seed)\n    # Generate N random numbers from a uniform distribution on [0, 1)\n    u1 = rng.random(size=N)\n    u2 = rng.random(size=N)\n\n    # Transform uniform variates to spherical coordinates (theta, phi)\n    # phi is uniform in [0, 2*pi)\n    phi = 2 * np.pi * u1\n    # cos(theta) is uniform in [-1, 1]\n    cos_theta = 2 * u2 - 1\n    # theta is in [0, pi]\n    theta = np.arccos(cos_theta)\n    \n    # 2. Compute observed counts O_ij using a 2D histogram\n    theta_edges = np.linspace(0, np.pi, B_theta + 1)\n    phi_edges = np.linspace(0, 2 * np.pi, B_phi + 1)\n    \n    # np.histogram2d bins the data. The returned histogram H[i, j] corresponds\n    # to theta_edges[i] <= theta < theta_edges[i+1] and phi_edges[j] <= phi < phi_edges[j+1].\n    # The first argument 'x' is theta, second 'y' is phi.\n    # The resulting O_ij matrix has shape (B_theta, B_phi).\n    O_ij, _, _ = np.histogram2d(theta, phi, bins=[theta_edges, phi_edges])\n    \n    # 3. Compute expected counts E_ij from first principles\n    # E_ij is independent of j, so we first compute a 1D array for E_i.\n    i_indices = np.arange(B_theta)\n    delta_theta = np.pi / B_theta\n    theta_i = i_indices * delta_theta\n    theta_i_plus_1 = (i_indices + 1) * delta_theta\n\n    # Analytical formula for expected count in a bin\n    cos_diff = np.cos(theta_i) - np.cos(theta_i_plus_1)\n    E_i_base = (N / (2 * B_phi)) * cos_diff\n    \n    # Expand to a 2D matrix of shape (B_theta, B_phi) to match O_ij\n    E_ij = np.tile(E_i_base, (B_phi, 1)).T\n\n    # 4. Perform the Pearson's chi-squared test\n    # All E_ij are > 0 as cos(x) is strictly decreasing on [0, pi].\n    # The sum is over all bins.\n    squared_diff = (O_ij - E_ij)**2\n    chi_squared_stat = np.sum(squared_diff / E_ij)\n    \n    # Degrees of freedom: k - 1, where k is the number of bins\n    k = B_theta * B_phi\n    dof = k - 1\n    \n    # Critical value q is the (1-alpha) quantile of the chi-squared distribution\n    q = chi2.ppf(1 - alpha, df=dof)\n    \n    # Decision: Accept H0 if chi_squared_stat is not in the critical region\n    return chi_squared_stat <= q\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results in the required format.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (seed, N, B_theta, B_phi, alpha)\n        (12345, 40000, 10, 20, 0.01),\n        (54321, 1280, 8, 16, 0.05),\n        (20231102, 50000, 2, 3, 0.01),\n        (777, 100000, 18, 36, 0.05),\n    ]\n\n    results = []\n    for case in test_cases:\n        seed, N, B_theta, B_phi, alpha = case\n        result = run_case(seed, N, B_theta, B_phi, alpha)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # e.g., \"[True,False,True,True]\"\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2433291"}]}