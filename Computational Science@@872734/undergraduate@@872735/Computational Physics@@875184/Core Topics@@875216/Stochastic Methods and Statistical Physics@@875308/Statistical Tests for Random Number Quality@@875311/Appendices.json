{"hands_on_practices": [{"introduction": "What if your only source of randomness is flawed? This practice explores a classic solution: the von Neumann randomness extractor. You will implement this elegant algorithm to transform a biased sequence, like from a weighted coin, into a perfectly unbiased one, demonstrating a fundamental principle of information theory. By applying a suite of standard statistical tests, you will then verify that the extracted sequence meets the criteria for high-quality randomness [@problem_id:2442648].", "problem": "You are given an infinite sequence of independent and identically distributed coin flips, where each flip is Heads with probability $p$ and Tails with probability $1 - p$. Represent Heads as $1$ and Tails as $0$. Define the von Neumann extractor as follows: partition the raw flips into disjoint consecutive pairs and map each pair according to the rule\n- $(1, 0) \\mapsto 1$,\n- $(0, 1) \\mapsto 0$,\n- $(0, 0)$ and $(1, 1)$ produce no output,\ncontinuing until exactly $N$ output bits have been produced.\n\nUsing only the assumption of independence for the raw flips, generate the von Neumann extracted output sequence of length $N$ from the biased source and assess the statistical quality of the resulting output using the following three tests, each producing a two-sided $p$-value:\n1. Frequency (monobit) test: If $k$ of the $N$ output bits are equal to $1$, compute the exact two-sided binomial $p$-value under the null hypothesis $H_0\\!:\\, \\Pr(1)=0.5$.\n2. Serial test on non-overlapping adjacent pairs: Using the first $2 \\lfloor N/2 \\rfloor$ bits of the output, form $\\lfloor N/2 \\rfloor$ non-overlapping pairs and count the occurrences of $(0,0)$, $(0,1)$, $(1,0)$, $(1,1)$. Under the null hypothesis of independent fair bits, the expected proportion of each pair is $1/4$. Compute the chi-squared goodness-of-fit $p$-value with $3$ degrees of freedom.\n3. Runs test for independence in a binary sequence with $\\Pr(1)=\\Pr(0)=1/2$: Let $n_1$ and $n_0$ denote the counts of ones and zeros, respectively, in the $N$ output bits, and let $R$ be the total number of runs of identical bits. Under the null hypothesis of independence with fixed $n_1$ and $n_0$, use the mean\n$$\\mu_R \\,=\\, 1 \\,+\\, \\frac{2 n_1 n_0}{N}$$\nand variance\n$$\\sigma_R^2 \\,=\\, \\frac{2 n_1 n_0 \\left(2 n_1 n_0 - N\\right)}{N^2 (N-1)}$$\nto form the normal approximation statistic\n$$Z \\,=\\, \\frac{R - \\mu_R}{\\sigma_R},$$\nand compute the corresponding two-sided $p$-value.\n\nFor reproducibility, use a fixed integer seed $s$ for the pseudorandom number generator that produces the raw biased flips. For test case index $i$ (starting from $0$), use the seed $s + i$.\n\nTest Suite. For each parameter pair $(p, N)$ below, generate the von Neumann extracted output and compute the three $p$-values described above:\n- Case $0$: $(p, N) = (0.30, 8000)$,\n- Case $1$: $(p, N) = (0.49, 12000)$,\n- Case $2$: $(p, N) = (0.90, 6000)$,\n- Case $3$: $(p, N) = (0.50, 10000)$,\nwith base seed $s = 20240913$.\n\nFinal Output Format. Your program should produce a single line of output containing the results as a comma-separated list of four items enclosed in square brackets. Each item corresponds to one test case and is itself a list of three decimal numbers equal to the frequency-test $p$-value, the serial-test $p$-value, and the runs-test $p$-value, each rounded to six decimal places. For example, the overall format must be\n$[[p_{0,1},p_{0,2},p_{0,3}],[p_{1,1},p_{1,2},p_{1,3}],[p_{2,1},p_{2,2},p_{2,3}],[p_{3,1},p_{3,2},p_{3,3}]]$,\nwhere $p_{i,j}$ denotes the $j$-th test’s $p$-value for case $i$. No additional text should be printed.", "solution": "The problem statement is evaluated as valid. It is scientifically grounded, well-posed, and objective. The task is a standard exercise in computational statistics, specifically concerning the quality assessment of random numbers generated via a well-established randomness extraction technique. All parameters, methods, and test statistics are clearly and correctly defined.\n\nThe solution is developed in three stages: first, the generation of a debiased binary sequence using the von Neumann extractor; second, the application of three distinct statistical tests to this sequence; and third, the implementation of this process for the specified test cases.\n\n**1. The Von Neumann Extractor**\n\nThe foundation of this problem is the von Neumann randomness extractor. It takes a sequence of independent and identically distributed (i.i.d.) biased bits, $X_i$, where the probability of a one is $\\Pr(X_i=1)=p$ and a zero is $\\Pr(X_i=0)=1-p$, with $p \\in (0,1)$. The extractor processes the input stream by taking non-overlapping pairs of bits, $(X_{2j-1}, X_{2j})$.\n\nThe operational rules are:\n-   If the pair is $(1,0)$, output a $1$.\n-   If the pair is $(0,1)$, output a $0$.\n-   If the pair is $(0,0)$ or $(1,1)$, produce no output and proceed to the next pair.\n\nThe theoretical justification for this method's effectiveness is based on simple probability. The probabilities of the generating pairs are $\\Pr(1,0) = p(1-p)$ and $\\Pr(0,1) = (1-p)p$. These probabilities are identical. The probability of a pair being discarded is $\\Pr(0,0) + \\Pr(1,1) = (1-p)^2 + p^2$.\n\nThe conditional probability of producing a $1$, given that an output bit is generated, is:\n$$ \\Pr(\\text{output}=1 | \\text{output is generated}) = \\frac{\\Pr(1,0)}{\\Pr(1,0) + \\Pr(0,1)} = \\frac{p(1-p)}{p(1-p) + (1-p)p} = \\frac{1}{2} $$\nSimilarly, the conditional probability of producing a $0$ is also $1/2$. Thus, the output sequence is perfectly unbiased. Further, because the input bits are i.i.d., the non-overlapping pairs are independent, which ensures that the resulting output bits are also independent of each other. The procedure therefore transforms a biased i.i.d. source into an unbiased i.i.d. source, which should pass statistical tests for randomness.\n\nThe simulation generates these output bits by creating a stream of pseudorandom bits with bias $p$, processing them in pairs according to the rules, and collecting the output until a sequence of the desired length $N$ is formed. For reproducibility, the pseudorandom number generator is seeded with $s+i$ for test case index $i$, where the base seed is $s=20240913$.\n\n**2. Statistical Quality Assessment**\n\nThe quality of the generated $N$-bit sequence is assessed using three standard statistical tests. Each test evaluates a specific null hypothesis ($H_0$) related to ideal randomness and yields a $p$-value. The $p$-value represents the probability of observing a result at least as extreme as the one measured, assuming $H_0$ is true.\n\n**2.1. Frequency (Monobit) Test**\nThis test checks for the most fundamental property of an unbiased binary sequence: an equal number of zeros and ones on average.\n-   $H_0$: The probability of a one is $1/2$.\n-   Let $k$ be the number of ones in the $N$-bit sequence. Under $H_0$, $k$ follows a binomial distribution, $B(N, 0.5)$.\n-   The two-sided $p$-value is the probability of observing a count of ones as far or farther from the expected mean $N/2$ than $k$. This is calculated using the exact binomial probability mass function.\n\n**2.2. Serial Test on Pairs**\nThis test examines for short-range correlations by checking if adjacent bits are independent.\n-   $H_0$: The bits are independent and fair. This implies that each of the four possible non-overlapping pairs—$(0,0)$, $(0,1)$, $(1,0)$, $(1,1)$—should occur with equal probability, $1/4$.\n-   The first $2 \\lfloor N/2 \\rfloor$ bits are formed into $M = \\lfloor N/2 \\rfloor$ pairs. The number of occurrences of each pair type ($c_{00}, c_{01}, c_{10}, c_{11}$) is counted.\n-   A chi-squared ($\\chi^2$) goodness-of-fit test is performed to compare the observed counts to the expected count of $M/4$ for each pair type. The $\\chi^2$ statistic is:\n$$ \\chi^2 = \\sum_{ij \\in \\{00,01,10,11\\}} \\frac{(c_{ij} - M/4)^2}{M/4} $$\n-   This statistic is compared against a $\\chi^2$ distribution with $4-1=3$ degrees of freedom to obtain the $p$-value.\n\n**2.3. Runs Test**\nThis test checks for independence by analyzing the number of \"runs\" in the sequence, where a run is a maximal consecutive subsequence of identical values. An unusual number of runs can indicate clustering (too few runs) or rapid oscillation (too many runs).\n-   $H_0$: The sequence is independent, with fixed counts of ones ($n_1$) and zeros ($n_0$).\n-   Let $R$ be the total observed number of runs. For large $N$, the distribution of $R$ can be approximated by a normal distribution with the specified mean $\\mu_R = 1 + \\frac{2 n_1 n_0}{N}$ and variance $\\sigma_R^2 = \\frac{2 n_1 n_0 (2 n_1 n_0 - N)}{N^2 (N-1)}$.\n-   The test statistic $Z = (R - \\mu_R) / \\sigma_R$ is computed.\n-   The two-sided $p$-value is derived from the standard normal distribution as $2 \\times \\Phi(-|Z|)$, where $\\Phi$ is the standard normal cumulative distribution function.\n\nThe implementation of these three tests will produce three $p$-values for each test case, which are then formatted as required.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import binomtest, chisquare, norm\n\ndef generate_von_neumann_sequence(p, N, seed):\n    \"\"\"\n    Generates a sequence of N unbiased bits from a biased source using the\n    von Neumann extractor.\n\n    Args:\n        p (float): The probability of '1' in the source sequence.\n        N (int): The desired length of the output sequence.\n        seed (int): The seed for the random number generator.\n\n    Returns:\n        np.ndarray: A numpy array of length N containing the unbiased bits.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    output_list = []\n    \n    # Heuristic for chunk size to generate raw bits.\n    # A larger chunk is more efficient than many small ones.\n    # 20000 raw bits = 10000 pairs.\n    chunk_size = 20000\n\n    while len(output_list) < N:\n        # Generate a chunk of biased bits (True for 1, False for 0).\n        raw_bits = rng.random(size=chunk_size) < p\n        \n        for i in range(0, chunk_size, 2):\n            b1, b2 = raw_bits[i], raw_bits[i+1]\n            \n            # Check for (1,0) or (0,1) pairs.\n            if b1 != b2:\n                # Map (1,0) to 1 and (0,1) to 0.\n                output_list.append(1 if b1 else 0)\n                if len(output_list) == N:\n                    break\n    \n    return np.array(output_list, dtype=np.int8)\n\ndef frequency_test(sequence):\n    \"\"\"\n    Performs the frequency (monobit) test.\n    \n    Args:\n        sequence (np.ndarray): The binary sequence to test.\n        \n    Returns:\n        float: The two-sided p-value.\n    \"\"\"\n    N = len(sequence)\n    if N == 0:\n        return 1.0\n    k = np.sum(sequence)\n    # The binomtest function computes the exact two-sided binomial test p-value.\n    result = binomtest(k, n=N, p=0.5, alternative='two-sided')\n    return result.pvalue\n\ndef serial_test(sequence):\n    \"\"\"\n    Performs the serial test on non-overlapping adjacent pairs.\n    \n    Args:\n        sequence (np.ndarray): The binary sequence to test.\n        \n    Returns:\n        float: The chi-squared p-value.\n    \"\"\"\n    N = len(sequence)\n    M = N // 2\n    if M == 0:\n        return 1.0 # Not enough data for pairs.\n\n    # Form non-overlapping pairs from the first 2*M bits.\n    seq_pairs = sequence[:2*M].reshape(M, 2)\n    \n    # A mapping to count pairs: (0,0)->0, (0,1)->1, (1,0)->2, (1,1)->3\n    pair_values = 2 * seq_pairs[:, 0] + seq_pairs[:, 1]\n    observed_counts = np.bincount(pair_values, minlength=4)\n    \n    # Under H0, expected count for each pair is M/4.\n    expected_count = M / 4.0\n    \n    if expected_count == 0:\n        return 1.0\n        \n    # The chisquare test compares observed vs expected frequencies.\n    # Degrees of freedom is k-1 = 4-1 = 3 by default.\n    _, p_value = chisquare(f_obs=observed_counts, f_exp=[expected_count]*4)\n    return p_value\n\ndef runs_test(sequence):\n    \"\"\"\n    Performs the runs test for independence.\n    \n    Args:\n        sequence (np.ndarray): The binary sequence to test.\n        \n    Returns:\n        float: The normal approximation p-value.\n    \"\"\"\n    N = len(sequence)\n    n1 = np.sum(sequence)\n    n0 = N - n1\n\n    # If the sequence is monolithic, the test is not applicable.\n    # This implies extreme non-randomness. p-value should be 0.\n    if n1 == 0 or n0 == 0:\n        return 0.0\n\n    # Count the number of runs R.\n    R = np.sum(sequence[:-1] != sequence[1:]) + 1\n    \n    # Calculate mean and variance under the null hypothesis.\n    mu_R = 1 + (2.0 * n1 * n0) / N\n    \n    numerator_sigma2 = 2.0 * n1 * n0 * (2.0 * n1 * n0 - N)\n    denominator_sigma2 = float(N**2) * (N - 1)\n    \n    # Variance can be non-positive only in extreme cases not expected here.\n    if denominator_sigma2 == 0 or numerator_sigma2 <= 0:\n        return 0.0\n        \n    sigma2_R = numerator_sigma2 / denominator_sigma2\n    sigma_R = np.sqrt(sigma2_R)\n    \n    # Calculate Z-score and two-sided p-value from standard normal distribution.\n    Z = (R - mu_R) / sigma_R\n    p_value = 2 * norm.sf(abs(Z)) # sf is the survival function, 1-cdf\n    \n    return p_value\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        (0.30, 8000),\n        (0.49, 12000),\n        (0.90, 6000),\n        (0.50, 10000),\n    ]\n    base_seed = 20240913\n\n    all_results = []\n    for i, (p, N) in enumerate(test_cases):\n        seed = base_seed + i\n        \n        # 1. Generate the sequence\n        sequence = generate_von_neumann_sequence(p, N, seed)\n        \n        # 2. Run the statistical tests\n        p_freq = frequency_test(sequence)\n        p_serial = serial_test(sequence)\n        p_runs = runs_test(sequence)\n        \n        all_results.append([p_freq, p_serial, p_runs])\n\n    # 3. Format the output as specified\n    formatted_cases = []\n    for pvals in all_results:\n        formatted_pvals = [f\"{p:.6f}\" for p in pvals]\n        formatted_cases.append(f'[{\",\".join(formatted_pvals)}]')\n    \n    print(f'[{\",\".join(formatted_cases)}]')\n\nsolve()\n```", "id": "2442648"}, {"introduction": "A random number generator can pass a battery of standard tests with flying colors and still be dangerously flawed. This exercise is a crucial cautionary tale about hidden correlations between dimensions. You will construct a special generator that appears perfectly uniform in one dimension but fails spectacularly when used for a simple two-dimensional Monte Carlo simulation, teaching an unforgettable lesson about the limitations of 1D statistical tests [@problem_id:2442681].", "problem": "You are to construct and analyze a deterministic Pseudorandom Number Generator (PRNG) that, for each provided seed, produces a finite sequence of real numbers in the open interval $(0,1)$ which passes a specified statistical battery while producing a systematically incorrect outcome in a simple Monte Carlo (MC) physical simulation. The program you write must implement the generator, apply the tests, run the simulation, and report a boolean result for each test case. All definitions and thresholds are given below.\n\nA PRNG with seed $s$ produces a sequence $\\{x_n\\}_{n=1}^N$ with each $x_n \\in (0,1)$. For a fixed seed $s$, define three hypothesis tests under the null hypothesis $\\mathcal{H}_0$ that the sequence is independent and identically distributed (i.i.d.) with common distribution $\\mathrm{Uniform}(0,1)$:\n\n- One-sample Kolmogorov–Smirnov (KS) test: Let $F_N(x)$ denote the empirical distribution function of $\\{x_n\\}_{n=1}^N$ and $F(x)=x$ denote the cumulative distribution function of $\\mathrm{Uniform}(0,1)$. Define the KS statistic $D_N=\\sup_{x\\in[0,1]} \\left| F_N(x) - F(x) \\right|$. The associated $p$-value is computed under $\\mathcal{H}_0$.\n\n- Chi-square test for equal-probability bins: Partition $(0,1)$ into $B$ equal-width bins. Let $O_j$ be the observed count in bin $j$ for $j=1,\\dots,B$, and let $E_j=N/B$ be the expected count under $\\mathcal{H}_0$. The chi-square statistic is $\\chi^2=\\sum_{j=1}^B \\frac{(O_j-E_j)^2}{E_j}$. The associated $p$-value is computed under a chi-square distribution with $B-1$ degrees of freedom under $\\mathcal{H}_0$.\n\n- Sample mean test: Let $\\bar{X}=\\frac{1}{N}\\sum_{n=1}^N x_n$. Under $\\mathcal{H}_0$, by the Central Limit Theorem, $Z=\\sqrt{12N}\\,(\\bar{X}-\\tfrac{1}{2})$ is approximately standard normal. The two-sided $p$-value is computed from the standard normal distribution.\n\nDefine a pass criterion threshold $\\varepsilon$ with $0<\\varepsilon<\\tfrac{1}{2}$, and say that the statistical battery passes if and only if all three $p$-values lie strictly within the open interval $(\\varepsilon,1-\\varepsilon)$.\n\nIndependently of the above tests, define a physics-motivated MC estimator of $\\pi$ via the classical dart-throwing method on the unit square. Using the same PRNG and the same seed $s$, form $M$ consecutive pairs from a stream $(x_1,x_2,\\dots)$ as $(x_{2k-1},x_{2k})$ for $k=1,\\dots,M$. Define\n$$\n\\widehat{\\pi} = 4\\, \\frac{1}{M}\\sum_{k=1}^M \\mathbf{1}\\!\\left\\{ x_{2k-1}^2 + x_{2k}^2 \\le 1 \\right\\},\n$$\nwhere $\\mathbf{1}\\{\\cdot\\}$ is the indicator function. Given an absolute error tolerance $\\tau>0$, declare that the MC simulation fails if and only if $|\\widehat{\\pi} - \\pi| > \\tau$. All angles involved are implicitly in radians through the use of $\\pi$; no explicit angles are computed.\n\nYour program must implement a single deterministic PRNG design of your choosing and evaluate it on the following test suite. For each test case, the program must compute a boolean value that is true if and only if the PRNG passes the statistical battery and fails the MC simulation, and false otherwise.\n\nThe test suite consists of the following parameter sets $(s, N, B, M, \\varepsilon, \\tau)$:\n\n- Case $1$: $(s, N, B, M, \\varepsilon, \\tau) = (123456789, 100000, 100, 50000, 10^{-9}, 10^{-2})$.\n- Case $2$: $(s, N, B, M, \\varepsilon, \\tau) = (0, 50000, 50, 20000, 10^{-9}, 10^{-2})$.\n- Case $3$: $(s, N, B, M, \\varepsilon, \\tau) = (987654321098765432, 120000, 200, 60000, 10^{-9}, 10^{-2})$.\n\nYour program should produce a single line of output containing the three boolean results for the cases above as a comma-separated list enclosed in square brackets (for example, $[{\\rm True},{\\rm False},{\\rm True}]$). No other output should be produced. All reported values are unitless; no physical units are required in the output. All internal computations must be deterministic and require no user input.", "solution": "The problem as stated is valid. It is a well-posed and scientifically grounded exercise in computational physics that probes the critical distinction between one-dimensional and multi-dimensional properties of pseudorandom number sequences. An unsophisticated battery of one-dimensional statistical tests is insufficient to certify a generator for multi-dimensional applications, such as Monte Carlo simulations. We shall construct a generator that is deliberately flawed in two dimensions, yet whose one-dimensional marginal distribution remains uniform, thereby satisfying the stated criteria.\n\nThe core of the problem is to design a single deterministic Pseudorandom Number Generator (PRNG) that, for a given seed $s$, produces a sequence $\\{x_n\\}$ that simultaneously:\n$1$. Passes a statistical battery of three tests (Kolmogorov-Smirnov, Chi-square, sample mean), meaning the $p$-value for each test falls within the interval $(\\varepsilon, 1-\\varepsilon)$.\n$2$. Fails a Monte Carlo (MC) estimation of $\\pi$, meaning the estimated value $\\widehat{\\pi}$ differs from the true value $\\pi$ by more than a tolerance $\\tau$.\n\nWe will construct a PRNG with a specific, fatal correlation between adjacent numbers. Let a high-quality, cryptographically secure PRNG, such as the PCG64 generator provided by the `numpy` library, be used as a base source of randomness. Let this base generator be seeded with the given integer seed $s$ to produce a sequence of i.i.d. values $\\{u_k\\}$ from a $\\mathrm{Uniform}(0,1)$ distribution.\n\nOur flawed PRNG will then construct the target sequence $\\{x_n\\}$ by forming pairs:\n$$ x_{2k-1} = u_k $$\n$$ x_{2k} = 1 - u_k $$\nfor $k = 1, 2, 3, \\dots$.\n\nLet us analyze the properties of this sequence $\\{x_n\\}$.\n\nFirst, consider the one-dimensional statistical properties. The sequence $\\{x_n\\}$ is a mixture of two sequences, $\\{u_k\\}$ and $\\{1-u_k\\}$. If $u_k \\sim \\mathrm{Uniform}(0,1)$, then it is a basic result of probability theory that the transformed variable $1-u_k$ is also distributed as $\\mathrm{Uniform}(0,1)$. The combined sequence $\\{x_n\\}$ therefore has a marginal cumulative distribution function that is an equal-weighted average of the uniform CDF with itself, which is simply the uniform CDF. Thus, the marginal distribution of $\\{x_n\\}$ is $\\mathrm{Uniform}(0,1)$. Consequently, it is expected to appear uniform to any one-dimensional test.\n- The **Kolmogorov-Smirnov test** compares the empirical CDF of $\\{x_n\\}_{n=1}^N$ to the uniform CDF $F(x)=x$. As the marginal distribution is uniform, the empirical CDF will converge to the true CDF, and the test is expected to pass with a high $p$-value.\n- The **Chi-square test** examines the frequency of numbers falling into $B$ bins. A uniform marginal distribution implies that counts should be evenly distributed, so this test is also expected to pass.\n- The **Sample mean test** relies on the mean of the sequence. The expectation is $E[x_n] = \\frac{1}{2} E[u_k] + \\frac{1}{2} E[1-u_k] = \\frac{1}{2}(\\frac{1}{2}) + \\frac{1}{2}(1-\\frac{1}{2}) = \\frac{1}{2}$. The sample mean $\\bar{X}$ will converge to $\\frac{1}{2}$, and the test statistic $Z=\\sqrt{12N}\\,(\\bar{X}-\\tfrac{1}{2})$ will be close to $0$, yielding a $p$-value close to $1$.\nGiven the extremely permissive pass-fail threshold of $\\varepsilon=10^{-9}$, it is virtually certain that all three statistical tests will pass.\n\nSecond, consider the two-dimensional Monte Carlo simulation. The method estimates $\\pi$ by sampling points $(x,y)$ in the unit square and finding the fraction that falls within the unit circle. Our generator produces pairs $(x_{2k-1}, x_{2k}) = (u_k, 1-u_k)$. These points are not uniformly distributed in the unit square; they all lie perfectly on the line segment $y = 1-x$ for $x \\in (0,1)$. This is a catastrophic failure of two-dimensional uniformity.\n\nThe MC estimator is $\\widehat{\\pi} = 4\\, \\frac{1}{M}\\sum_{k=1}^M \\mathbf{1}\\!\\left\\{ x_{2k-1}^2 + x_{2k}^2 \\le 1 \\right\\}$. Substituting our correlated pairs, the condition inside the indicator function becomes:\n$$ u_k^2 + (1-u_k)^2 \\le 1 $$\nExpanding the expression gives:\n$$ u_k^2 + 1 - 2u_k + u_k^2 \\le 1 $$\n$$ 2u_k^2 - 2u_k \\le 0 $$\n$$ 2u_k(u_k - 1) \\le 0 $$\nSince the base generator produces $u_k \\in [0,1)$, the term $u_k$ is non-negative and the term $(u_k-1)$ is non-positive. Their product is therefore always non-positive, and the inequality is always satisfied.\n\nEvery single pair $(x_{2k-1}, x_{2k})$ generated will be counted as a \"hit\" inside the circle. The sum in the estimator becomes $\\sum_{k=1}^M 1 = M$. The estimator for $\\pi$ will therefore be:\n$$ \\widehat{\\pi} = 4 \\frac{M}{M} = 4 $$\nThe MC simulation fails if $|\\widehat{\\pi} - \\pi| > \\tau$. In our case, this is $|4 - \\pi| > 10^{-2}$. Since $\\pi \\approx 3.14159$, the absolute error is $|4 - \\pi| \\approx 0.8584$, which is significantly larger than the tolerance $\\tau=10^{-2}$. The MC simulation is thus guaranteed to fail.\n\nThe final boolean result for each test case is true if and only if the statistical battery passes (which we expect) and the MC simulation fails (which is guaranteed). The implementation will proceed by generating the sequence as described, applying the specified statistical tests using functions from the `scipy.stats` library, and calculating the MC simulation result.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import kstest, chisquare, norm\n\ndef solve():\n    \"\"\"\n    Implements a flawed PRNG, evaluates it against a statistical battery and a\n    Monte Carlo simulation, and reports the combined outcome for several test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (s, N, B, M, epsilon, tau)\n        (123456789, 100000, 100, 50000, 1e-9, 1e-2),\n        (0, 50000, 50, 20000, 1e-9, 1e-2),\n        (987654321098765432, 120000, 200, 60000, 1e-9, 1e-2),\n    ]\n\n    results = []\n    for s, N, B, M, epsilon, tau in test_cases:\n        # Step 0: Determine total sequence length required.\n        # Length must be sufficient for both tests and MC simulation, and be even for pairing.\n        L = max(N, 2 * M)\n        if L % 2 != 0:\n            L += 1\n\n        # Step 1: Implement the PRNG with a 2D flaw.\n        # Use numpy's default PRNG (PCG64) as the base generator.\n        rng = np.random.default_rng(s)\n        # Generate L/2 base numbers.\n        base_rands = rng.random(size=L // 2)\n        # Construct the flawed sequence x_n where x_2k = 1 - x_{2k-1}.\n        x_sequence = np.empty(L, dtype=np.float64)\n        x_sequence[0::2] = base_rands\n        x_sequence[1::2] = 1.0 - base_rands\n\n        # Step 2: Perform the statistical battery on the first N numbers.\n        x_for_tests = x_sequence[:N]\n\n        # 2a: One-sample Kolmogorov–Smirnov test.\n        # Test against the standard uniform distribution on (0,1).\n        ks_result = kstest(x_for_tests, 'uniform')\n        p_ks = ks_result.pvalue\n\n        # 2b: Chi-square test for equal-probability bins.\n        observed_counts, _ = np.histogram(x_for_tests, bins=B, range=(0.0, 1.0))\n        # With f_exp not provided, chisquare computes it as sum(f_obs)/len(f_obs),\n        # which is N/B, as required.\n        chi2_result = chisquare(f_obs=observed_counts)\n        p_chi2 = chi2_result.pvalue\n\n        # 2c: Sample mean test.\n        mean_val = np.mean(x_for_tests)\n        z_stat = np.sqrt(12.0 * N) * (mean_val - 0.5)\n        # Two-sided p-value from standard normal survival function.\n        p_mean = 2.0 * norm.sf(np.abs(z_stat))\n\n        # Check if the statistical battery passes.\n        stat_pass = (p_ks > epsilon and p_ks < 1.0 - epsilon) and \\\n                    (p_chi2 > epsilon and p_chi2 < 1.0 - epsilon) and \\\n                    (p_mean > epsilon and p_mean < 1.0 - epsilon)\n\n        # Step 3: Perform the Monte Carlo simulation on the first 2M numbers.\n        x_for_mc = x_sequence[:(2 * M)]\n        x_coords = x_for_mc[0::2]\n        y_coords = x_for_mc[1::2]\n\n        # Count \"hits\" inside the unit circle.\n        hits = np.sum(x_coords**2 + y_coords**2 <= 1.0)\n        # Calculate pi estimate.\n        pi_estimate = 4.0 * hits / M\n\n        # Check if the MC simulation fails.\n        mc_fail = np.abs(pi_estimate - np.pi) > tau\n\n        # Step 4: Determine the final boolean result for the case.\n        # The result is True iff the battery passes AND the simulation fails.\n        final_result = stat_pass and mc_fail\n        results.append(final_result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2442681"}, {"introduction": "How can we design tests that are sensitive to the multi-dimensional flaws we discovered in the previous practice? This exercise moves beyond standard batteries, guiding you to build a test inspired by a physical model: the 2D random walk. By analyzing the winding number of walk trajectories, you will create a sensitive probe for isotropy, showing how physical intuition can be used to design powerful, custom-made validation tools [@problem_id:2442651].", "problem": "You are tasked with designing a statistical quality test for a Random Number Generator (RNG) using the winding number of a two-dimensional (2D) random walk about the origin. Consider the following experiment. For each test case, generate an ensemble of independent 2D random walks, each consisting of unit-length steps with independently and identically distributed step direction angles drawn from the continuous uniform distribution on the interval $\\left[0,2\\pi\\right)$, using the given seed to initialize the pseudo-RNG. Each walk starts at the origin $(0,0)$.\n\nMathematical definitions to be used:\n- Let a single walk have $N$ steps, with step angles $\\{\\phi_{j}\\}_{j=1}^{N}$ in radians. The positions $\\{(x_{j},y_{j})\\}_{j=1}^{N}$ evolve according to\n$$\nx_{j} = \\sum_{i=1}^{j} \\cos(\\phi_{i}), \\quad y_{j} = \\sum_{i=1}^{j} \\sin(\\phi_{i}).\n$$\n- Let $\\psi_{j} = \\operatorname{atan2}(y_{j},x_{j})$ be the polar angle of the position vector after step $j$. Define the unwrapped angle sequence $\\{\\tilde{\\psi}_{j}\\}_{j=1}^{N}$ by choosing it to be continuous with increments $\\tilde{\\psi}_{j}-\\tilde{\\psi}_{j-1} \\in (-\\pi,\\pi]$ for $j\\ge 2$, and $\\tilde{\\psi}_{1}=\\psi_{1}$. The total continuous angular change over the walk is\n$$\n\\Delta = \\tilde{\\psi}_{N}-\\tilde{\\psi}_{1}.\n$$\n- The winding number $W$ of the walk about the origin is the integer\n$$\nW = \\operatorname{round}\\!\\left(\\frac{\\Delta}{2\\pi}\\right),\n$$\nwhere $\\operatorname{round}(\\cdot)$ denotes rounding to the nearest integer.\n\nStatistical null hypothesis:\n- Under ideal isotropy and independence, the distribution of $W$ over many independent walks is symmetric about zero. Formally, for all integers $k>0$, one has $\\mathbb{P}(W=k)=\\mathbb{P}(W=-k)$.\n\nTest statistic and $p$-value:\n- Given $M$ independent walks with winding numbers $\\{W^{(m)}\\}_{m=1}^{M}$, let $n_{k}$ denote the count of walks with $W=k$. Consider all positive integers $k$ such that $n_{k}+n_{-k}>0$; denote the number of such pairs by $\\nu$. Define the Pearson chi-squared statistic for symmetry:\n$$\nT = \\sum_{\\substack{k>0 \\\\ n_{k}+n_{-k}>0}} \\frac{\\left(n_{k}-n_{-k}\\right)^{2}}{n_{k}+n_{-k}}.\n$$\nUnder the null hypothesis, $T$ is approximately distributed as a chi-squared distribution with $\\nu$ degrees of freedom. The $p$-value is then\n$$\np = \\Pr\\!\\left(\\chi^{2}_{\\nu} \\ge T \\right).\n$$\nIf $\\nu=0$ (no nonzero winding numbers observed), define the reported $p$-value by the convention $p=1.0$.\n\nRequired output:\n- For each test case, compute the $p$-value as defined above. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each $p$-value must be rounded to $6$ decimal places.\n\nTest suite:\n- Use the following test cases, each specified as a tuple $(\\text{seed},N,M)$:\n  - $(\\,20231123,\\,2000,\\,2000\\,)$\n  - $(\\,7,\\,300,\\,400\\,)$\n  - $(\\,123456789,\\,1,\\,1000\\,)$\n  - $(\\,999,\\,50,\\,200\\,)$\n\nFinal output format:\n- A single line containing a list of the four rounded $p$-values in the order of the test suite, for example, \"[r1,r2,r3,r4]\" where each \"ri\" is a decimal rounded to $6$ places.", "solution": "The problem statement has been analyzed and is determined to be valid. It is scientifically grounded, mathematically well-posed, and provides a complete and unambiguous specification for the required computation. The task is to implement a statistical test for the quality of a pseudo-random number generator (PRNG) based on the symmetry properties of winding numbers of two-dimensional random walks.\n\nThe solution is implemented by following a precise algorithmic procedure for each test case, which is specified by a tuple $(\\text{seed}, N, M)$. A test case involves generating an ensemble of $M$ independent random walks, each of length $N$, using a PRNG initialized with the given `seed`.\n\nFirst, the PRNG is initialized. For each of the $M$ walks in the ensemble, a sequence of $N$ step angles, $\\{\\phi_{j}\\}_{j=1}^{N}$, is generated. Each angle $\\phi_j$ is an independent sample drawn from the continuous uniform distribution over the interval $[0, 2\\pi)$.\n\nFor a single walk, the trajectory in the plane is a sequence of positions $\\{(x_{j},y_{j})\\}_{j=1}^{N}$, starting from $(x_0, y_0) = (0,0)$. The position after step $j$ is calculated by the cumulative sum of the step vectors' components:\n$$\nx_{j} = \\sum_{i=1}^{j} \\cos(\\phi_{i}), \\quad y_{j} = \\sum_{i=1}^{j} \\sin(\\phi_{i})\n$$\nThe polar angle of the walker's position at each step is given by $\\psi_{j} = \\operatorname{atan2}(y_{j},x_{j})$. The problem requires tracking the total continuous angular displacement around the origin. This is achieved by \"unwrapping\" the sequence of polar angles $\\{\\psi_j\\}$. The unwrapped angle sequence, $\\{\\tilde{\\psi}_{j}\\}_{j=1}^{N}$, is constructed such that $\\tilde{\\psi}_{1} = \\psi_{1}$ and subsequent values are chosen to maintain continuity by ensuring that the incremental change between steps, $\\tilde{\\psi}_{j}-\\tilde{\\psi}_{j-1}$, lies within the interval $(-\\pi, \\pi]$. This is accomplished by adding or subtracting integer multiples of $2\\pi$ to $\\psi_j$ as needed.\n\nThe total angular change over the entire walk is the difference between the final and initial unwrapped angles:\n$$\n\\Delta = \\tilde{\\psi}_{N}-\\tilde{\\psi}_{1}\n$$\nThe winding number $W$ for the walk is then defined as the total number of full revolutions around the origin, obtained by rounding the total angular change in units of $2\\pi$ to the nearest integer:\n$$\nW = \\operatorname{round}\\!\\left(\\frac{\\Delta}{2\\pi}\\right)\n$$\nThis procedure is repeated for all $M$ walks, yielding an ensemble of $M$ winding numbers, $\\{W^{(m)}\\}_{m=1}^{M}$.\n\nThe statistical test is then performed on this ensemble of winding numbers. The null hypothesis, $H_0$, is that the PRNG is isotropic, which implies a symmetric distribution of winding numbers around $0$, i.e., $\\mathbb{P}(W=k) = \\mathbb{P}(W=-k)$ for any integer $k>0$.\nTo test this hypothesis, the counts $n_k$ of each observed integer winding number $k$ are tallied from the ensemble. The test statistic is a Pearson's chi-squared statistic, computed over pairs of non-zero, opposite-signed winding numbers:\n$$\nT = \\sum_{\\substack{k>0 \\\\ n_{k}+n_{-k}>0}} \\frac{\\left(n_{k}-n_{-k}\\right)^{2}}{n_{k}+n_{-k}}\n$$\nThe number of terms in this sum defines the degrees of freedom, $\\nu$, for the chi-squared distribution. Specifically, $\\nu$ is the count of positive integers $k$ for which the total count $n_{k}+n_{-k}$ is greater than $0$. Under the null hypothesis, the statistic $T$ is approximately distributed as a chi-squared random variable with $\\nu$ degrees of freedom, $\\chi^2_{\\nu}$.\n\nThe $p$-value, which represents the probability of observing a test statistic at least as extreme as $T$ under the null hypothesis, is calculated as the survival function of the $\\chi^2_{\\nu}$ distribution:\n$$\np = \\Pr\\!\\left(\\chi^{2}_{\\nu} \\ge T \\right)\n$$\nA small $p$-value (typically less than a significance level like $0.05$ or $0.01$) would provide evidence to reject the null hypothesis and suggest a defect (anisotropy) in the PRNG.\nA special case occurs when no non-zero winding numbers are observed across the entire ensemble. In this situation, $\\nu=0$ and the test statistic $T=0$. This provides no evidence against the null hypothesis of symmetry; thus, the $p$-value is defined as $1.0$.\n\nThis entire procedure is applied to each of the specified test cases. The final output is a list of the computed $p$-values, rounded to $6$ decimal places. The implementation utilizes the `numpy` library for efficient vectorized computation of the walk trajectories and the `scipy` library for calculating the $p$-value from the chi-squared distribution.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef solve():\n    \"\"\"\n    Solves the random walk winding number problem for the given test suite.\n    \"\"\"\n    test_cases = [\n        (20231123, 2000, 2000),\n        (7, 300, 400),\n        (123456789, 1, 1000),\n        (999, 50, 200),\n    ]\n\n    results = []\n    for seed, N, M in test_cases:\n        # Initialize the random number generator with the specified seed for reproducibility\n        rng = np.random.default_rng(seed)\n        \n        winding_numbers = []\n        \n        for _ in range(M):\n            # For N=1, the winding number is always 0. This is a deterministic shortcut.\n            if N == 1:\n                winding_numbers.append(0)\n                continue\n\n            # Step 1: Generate N i.i.d. step angles uniformly in [0, 2*pi)\n            phi = rng.uniform(0.0, 2.0 * np.pi, N)\n            \n            # Step 2: Calculate the trajectory (x_j, y_j)\n            # np.cumsum provides the sums needed for positions at each step\n            x = np.cumsum(np.cos(phi))\n            y = np.cumsum(np.sin(phi))\n            \n            # Step 3: Calculate the polar angles psi_j = atan2(y_j, x_j)\n            psi = np.arctan2(y, x)\n            \n            # Step 4: Unwrap the angle sequence to get continuous angular change\n            # np.unwrap ensures that jumps are smaller than pi, matching the problem\n            psi_unwrap = np.unwrap(psi)\n            \n            # Step 5: Calculate total angular change and the winding number W\n            # Delta is the difference between the last and first unwrapped angles\n            delta_psi = psi_unwrap[-1] - psi_unwrap[0]\n            W = np.round(delta_psi / (2.0 * np.pi))\n            winding_numbers.append(int(W))\n\n        # Step 6: Statistical analysis\n        # Count occurrences of each winding number\n        unique_W, counts_W = np.unique(winding_numbers, return_counts=True)\n        counts = dict(zip(unique_W, counts_W))\n\n        # Find positive k's where n_k + n_{-k} > 0\n        positive_keys = sorted([k for k in counts.keys() if k > 0])\n        \n        T = 0.0\n        nu = 0\n        \n        # A set to keep track of processed positive k values to avoid double counting\n        processed_k = set()\n        \n        all_keys = sorted(counts.keys())\n\n        for k in all_keys:\n            if k > 0 and k not in processed_k:\n                n_k = counts.get(k, 0)\n                n_minus_k = counts.get(-k, 0)\n                \n                if n_k + n_minus_k > 0:\n                    nu += 1\n                    T += (n_k - n_minus_k)**2 / (n_k + n_minus_k)\n                \n                processed_k.add(k)\n        \n        # Step 7: Calculate the p-value\n        if nu == 0:\n            # If no non-zero winding numbers are observed, there is no evidence against symmetry.\n            p_value = 1.0\n        else:\n            # Survival function of the chi-squared distribution\n            p_value = chi2.sf(T, df=nu)\n            \n        results.append(p_value)\n\n    # Final print statement in the exact required format\n    formatted_results = [f\"{p:.6f}\" for p in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2442651"}]}