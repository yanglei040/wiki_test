{"hands_on_practices": [{"introduction": "Power-law distributions are fundamental across many areas of physics, describing phenomena from the energy spectra of cosmic rays to the sizes of avalanches in self-organized critical systems. This first exercise [@problem_id:2403909] provides essential practice in applying the inverse transform method to a truncated power-law, a common task in computational modeling. You will derive and implement the analytical inverse cumulative distribution function, which requires careful handling of the special case where the exponent $\\alpha=1$.", "problem": "You are tasked with constructing a program that generates independent random variates from a truncated power-law probability distribution used in computational physics. The distribution is defined on a finite interval by the probability density function (PDF)\n$$\np(x) = C\\,x^{-\\alpha} \\quad \\text{for } x \\in [x_{\\min}, x_{\\max}],\n$$\nand $p(x)=0$ otherwise, where $C$ is the normalization constant that makes the total probability over $[x_{\\min}, x_{\\max}]$ equal to $1$. The parameters satisfy $x_{\\min}  0$, $x_{\\max}  x_{\\min}$, and $\\alpha  0$. The domain variable is real-valued $x \\in \\mathbb{R}$. The goal is to produce synthetic data that are exactly distributed according to this truncated density.\n\nYour program must:\n- Treat $C$ implicitly through the exact definition of the target distribution given above, ensuring that the generated variates are distributed according to the density $p(x)$ on $[x_{\\min}, x_{\\max}]$.\n- For each test case, generate exactly $N$ independent variates according to this distribution, using a single pseudorandom number generator initialized once at the very start with the fixed integer seed $20231102$, and then consuming the pseudorandom numbers sequentially across the test cases in the order listed below.\n- For each test case, after generating the $N$ variates, compute the following three summary statistics:\n  1. The minimum value among the $N$ variates.\n  2. The median defined as the $\\left(\\frac{N+1}{2}\\right)$-th order statistic (note that each $N$ below is odd, so this is an exact order statistic without interpolation).\n  3. The maximum value among the $N$ variates.\n- Round each of the three reported values for every test case to exactly six decimal places.\n- Aggregate the results for all test cases into a single line of output containing a comma-separated list of lists, each inner list corresponding to one test case in the same order as below.\n\nTest Suite (five test cases):\n- Case 1: $\\alpha = 2.5$, $x_{\\min} = 1.0$, $x_{\\max} = 10.0$, $N = 9999$.\n- Case 2: $\\alpha = 1.0$, $x_{\\min} = 0.1$, $x_{\\max} = 100.0$, $N = 9999$.\n- Case 3: $\\alpha = 0.3$, $x_{\\min} = 1.0$, $x_{\\max} = 1000000.0$, $N = 7777$.\n- Case 4: $\\alpha = 5.0$, $x_{\\min} = 0.5$, $x_{\\max} = 2.0$, $N = 5555$.\n- Case 5: $\\alpha = 0.9$, $x_{\\min} = 2.0$, $x_{\\max} = 3.0$, $N = 3333$.\n\nFinal Output Format:\n- Your program should produce a single line of output containing the results as a comma-separated list of five inner lists, each inner list containing the three rounded values for the corresponding test case in the order [minimum, median, maximum]. The exact required format is:\n[\n[min_case1,median_case1,max_case1],\n[min_case2,median_case2,max_case2],\n[min_case3,median_case3,max_case3],\n[min_case4,median_case4,max_case4],\n[min_case5,median_case5,max_case5]\n]\nAll numbers appearing in the output must be rounded to six decimal places as specified. No additional text must be printed.", "solution": "The problem statement is examined and found to be valid. It is scientifically grounded, well-posed, objective, and provides a complete and consistent set of givens for a solvable problem in computational physics. The task is to generate random variates from a truncated power-law distribution, which is a standard procedure. The method of inverse transform sampling is exact for this purpose and will be employed.\n\nThe fundamental principle of the inverse transform sampling method is that if a random variable $U$ is uniformly distributed on the interval $[0, 1]$, then the random variable $X$ defined by the transformation $X = F^{-1}(U)$ will be distributed according to the probability density function $p(x)$ whose cumulative distribution function (CDF) is $F(x)$. Here, $F^{-1}$ is the inverse CDF, also known as the quantile function.\n\nThe procedure, therefore, is to first derive the CDF $F(x)$ from the given probability density function (PDF), $p(x)$, and then to find its inverse, $F^{-1}(u)$.\n\nThe PDF is given by:\n$$\np(x) = C\\,x^{-\\alpha} \\quad \\text{for } x \\in [x_{\\min}, x_{\\max}]\n$$\nwhere $C$ is a normalization constant. The CDF, $F(x)$, for a value $x$ in the support $[x_{\\min}, x_{\\max}]$ is the probability $P(X \\le x)$, given by the integral of the PDF from $x_{\\min}$ to $x$.\n$$\nF(x) = \\int_{x_{\\min}}^{x} p(t) \\,dt\n$$\nTo ensure that $F(x_{\\max}) = 1$, the integral must be normalized by the total probability over the entire support.\n$$\nF(x) = \\frac{\\int_{x_{\\min}}^{x} C t^{-\\alpha} \\,dt}{\\int_{x_{\\min}}^{x_{\\max}} C t^{-\\alpha} \\,dt}\n$$\nThe constant $C$ cancels, which is consistent with the problem's directive to treat $C$ implicitly. We are left with computing the integrals of $t^{-\\alpha}$. The form of the integral depends on whether $\\alpha=1$.\n\nCase 1: $\\alpha \\neq 1$\nThe indefinite integral of $t^{-\\alpha}$ is $\\frac{t^{1-\\alpha}}{1-\\alpha}$. The definite integral is:\n$$\n\\int_{a}^{b} t^{-\\alpha} \\,dt = \\left[ \\frac{t^{1-\\alpha}}{1-\\alpha} \\right]_{a}^{b} = \\frac{b^{1-\\alpha} - a^{1-\\alpha}}{1-\\alpha}\n$$\nSubstituting this into the expression for the CDF:\n$$\nF(x) = \\frac{\\frac{x^{1-\\alpha} - x_{\\min}^{1-\\alpha}}{1-\\alpha}}{\\frac{x_{\\max}^{1-\\alpha} - x_{\\min}^{1-\\alpha}}{1-\\alpha}} = \\frac{x^{1-\\alpha} - x_{\\min}^{1-\\alpha}}{x_{\\max}^{1-\\alpha} - x_{\\min}^{1-\\alpha}}\n$$\n\nCase 2: $\\alpha = 1$\nThe PDF is $p(x) \\propto x^{-1}$. The indefinite integral of $t^{-1}$ is $\\ln(t)$. The definite integral is:\n$$\n\\int_{a}^{b} t^{-1} \\,dt = \\left[ \\ln(t) \\right]_{a}^{b} = \\ln(b) - \\ln(a) = \\ln(b/a)\n$$\nSubstituting this into the expression for the CDF:\n$$\nF(x) = \\frac{\\ln(x) - \\ln(x_{\\min})}{\\ln(x_{\\max}) - \\ln(x_{\\min})} = \\frac{\\ln(x/x_{\\min})}{\\ln(x_{\\max}/x_{\\min})}\n$$\n\nNext, we must find the inverse CDF, $F^{-1}(u)$, by setting $u = F(x)$ for $u \\in [0, 1]$ and solving for $x$.\n\nCase 1: $\\alpha \\neq 1$\n$$\nu = \\frac{x^{1-\\alpha} - x_{\\min}^{1-\\alpha}}{x_{\\max}^{1-\\alpha} - x_{\\min}^{1-\\alpha}}\n$$\n$$\nu(x_{\\max}^{1-\\alpha} - x_{\\min}^{1-\\alpha}) = x^{1-\\alpha} - x_{\\min}^{1-\\alpha}\n$$\n$$\nx^{1-\\alpha} = x_{\\min}^{1-\\alpha} + u(x_{\\max}^{1-\\alpha} - x_{\\min}^{1-\\alpha})\n$$\n$$\nx = F^{-1}(u) = \\left[ x_{\\min}^{1-\\alpha} + u(x_{\\max}^{1-\\alpha} - x_{\\min}^{1-\\alpha}) \\right]^{\\frac{1}{1-\\alpha}}\n$$\n\nCase 2: $\\alpha = 1$\n$$\nu = \\frac{\\ln(x) - \\ln(x_{\\min})}{\\ln(x_{\\max}) - \\ln(x_{\\min})}\n$$\n$$\nu(\\ln(x_{\\max}) - \\ln(x_{\\min})) = \\ln(x) - \\ln(x_{\\min})\n$$\n$$\n\\ln(x) = \\ln(x_{\\min}) + u(\\ln(x_{\\max}) - \\ln(x_{\\min}))\n$$\nUsing properties of logarithms, this can be simplified:\n$$\n\\ln(x) = (1-u)\\ln(x_{\\min}) + u\\ln(x_{\\max}) = \\ln(x_{\\min}^{1-u} x_{\\max}^{u})\n$$\nExponentiating both sides gives the quantile function:\n$$\nx = F^{-1}(u) = x_{\\min}^{1-u} x_{\\max}^{u} = x_{\\min} \\left(\\frac{x_{\\max}}{x_{\\min}}\\right)^u\n$$\n\nThe computational algorithm is as follows:\n1. Initialize a pseudorandom number generator with the specified seed.\n2. For each test case $(\\alpha, x_{\\min}, x_{\\max}, N)$:\n   a. Generate $N$ independent random numbers $u_1, u_2, \\dots, u_N$ from the uniform distribution on $[0, 1]$.\n   b. For each $u_i$, calculate the corresponding variate $x_i = F^{-1}(u_i)$ using the appropriate formula based on the value of $\\alpha$.\n   c. From the resulting sample of $N$ variates $\\{x_i\\}$, compute the minimum, the median (which for odd $N$ is the $((N+1)/2)$-th order statistic), and the maximum.\n   d. Round the three statistics to six decimal places.\n3. Collate and format the results as specified.\nThis procedure will generate variates that are guaranteed to follow the desired truncated power-law distribution.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Generates random variates from a truncated power-law distribution using\n    inverse transform sampling and computes summary statistics for several test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (alpha, x_min, x_max, N)\n        (2.5, 1.0, 10.0, 9999),\n        (1.0, 0.1, 100.0, 9999),\n        (0.3, 1.0, 1000000.0, 7777),\n        (5.0, 0.5, 2.0, 5555),\n        (0.9, 2.0, 3.0, 3333),\n    ]\n\n    # Initialize a single pseudorandom number generator as required.\n    # The generator's state will be updated sequentially across all test cases.\n    rng = np.random.default_rng(20231102)\n\n    all_results = []\n    \n    for case in test_cases:\n        alpha, x_min, x_max, N = case\n\n        # Generate N uniform random numbers in [0, 1).\n        u = rng.random(size=N)\n\n        # Apply the inverse transform sampling formula.\n        # Two distinct formulas are required depending on whether alpha is 1.\n        if alpha == 1.0:\n            # Quantile function for alpha = 1: x = x_min * (x_max / x_min)^u\n            samples = x_min * (x_max / x_min)**u\n        else:\n            # Quantile function for alpha != 1:\n            # x = [x_min^(1-alpha) + u * (x_max^(1-alpha) - x_min^(1-alpha))]^(1/(1-alpha))\n            one_minus_alpha = 1.0 - alpha\n            x_min_pow = x_min**one_minus_alpha\n            x_max_pow = x_max**one_minus_alpha\n            \n            inner_term = x_min_pow + u * (x_max_pow - x_min_pow)\n            samples = inner_term**(1.0 / one_minus_alpha)\n\n        # Compute the required summary statistics.\n        # N is always odd, so np.median correctly calculates the ((N+1)/2)-th order statistic.\n        min_val = np.min(samples)\n        median_val = np.median(samples)\n        max_val = np.max(samples)\n\n        # Round the statistics to exactly six decimal places and store them.\n        all_results.append([\n            round(min_val, 6),\n            round(median_val, 6),\n            round(max_val, 6)\n        ])\n\n    # Format the results into the exact string format required.\n    # The output must be a single line.\n    # Example format: [[min1,med1,max1],[min2,med2,max2],...]\n    # Using .6f ensures that six decimal places are always shown.\n    formatted_cases = []\n    for res in all_results:\n        case_str = f\"[{res[0]:.6f},{res[1]:.6f},{res[2]:.6f}]\"\n        formatted_cases.append(case_str)\n    \n    final_output = f\"[{','.join(formatted_cases)}]\"\n\n    # Final print statement in the exact required format.\n    print(final_output)\n\nsolve()\n```", "id": "2403909"}, {"introduction": "Many physical systems are best described by mixture models, which are probabilistic models composed of multiple distinct distributions. This practice [@problem_id:2403899] demonstrates how to extend the inverse transform method to sample from a mixed distribution containing both a continuous component and a discrete point mass, represented by a Dirac delta function. By strategically partitioning a single uniform random variate, you will learn the composition method, a versatile and powerful technique for simulating outcomes from complex, multi-modal systems.", "problem": "You are to design and implement an inverse transform sampler for a probability distribution consisting of a point mass (Dirac delta) mixed with a continuous component. The target probability density is\n$$\np(x) = w\\,\\delta(x - x_0) + (1 - w)\\,f(x),\n$$\nwhere $w \\in [0,1]$, $x_0 \\in \\mathbb{R}$, and $f(x)$ is a proper probability density function on its domain. Your derivation must start from the foundational definition that if $U$ is a random variable uniformly distributed on $[0,1]$, then $X = F^{-1}(U)$ has cumulative distribution function $F(x)$, where $F^{-1}$ is the generalized inverse of $F$. You must not assume any shortcut formulas; derive what the generalized inverse sampling rule implies when the cumulative distribution function has a jump discontinuity due to a Dirac delta term, and show how this leads to a practical algorithm that uses a single uniform variate per sample.\n\nImplement a single program that:\n- Derives, implements, and uses a sampler that draws samples from $p(x)$ by transforming a single uniform random number $U \\sim \\mathrm{Uniform}(0,1)$ per sample.\n- Supports the following choices for the continuous component $f(x)$ (you must derive and use the corresponding inverse cumulative distribution functions from first principles):\n  1. Exponential distribution with rate parameter $\\lambda  0$ on $x \\ge 0$.\n  2. Uniform distribution on $[a,b]$ with $a  b$.\n  3. Logistic distribution on $x \\in \\mathbb{R}$ with location $\\mu$ and scale $s  0$.\n\nFor each supported continuous distribution, derive and use analytic expressions for the first and second moments of $f(x)$, and then derive the mixture’s mean and variance using the law of the unconscious statistician and basic properties of mixtures.\n\nYour program must hard-code and execute the following test suite. For each test case, it must:\n- Generate $N$ samples from the specified $p(x)$ using only one independent uniform variate per generated sample.\n- Compute the empirical point mass, empirical mean, and empirical variance, defined respectively as:\n  - The empirical point mass at $x_0$, equal to the fraction of generated samples exactly equal to $x_0$.\n  - The empirical mean $\\hat{\\mu} = \\frac{1}{N}\\sum_{i=1}^N x_i$.\n  - The empirical variance $\\hat{\\sigma}^2 = \\frac{1}{N}\\sum_{i=1}^N (x_i - \\hat{\\mu})^2$.\n- Compute the theoretical point mass $w$, the theoretical mixture mean, and the theoretical mixture variance using your derived formulas.\n- For each metric (point mass, mean, variance), output a boolean indicating whether the empirical value is within a given absolute tolerance of the theoretical value.\n\nUse the following test suite with fixed seeds for reproducibility:\n- Case A (general mixture on $\\mathbb{R}_{\\ge 0}$): $w = 0.3$, $x_0 = 1.5$, $f$ is exponential with rate $\\lambda = 2.0$, $N = 80000$, seed $314159$.\n- Case B (boundary condition $w = 0$): $w = 0$, $x_0 = -100.0$, $f$ is uniform on $[a,b]$ with $a = 2.0$, $b = 5.0$, $N = 60000$, seed $271828$.\n- Case C (boundary condition $w = 1$): $w = 1$, $x_0 = -2.0$, $f$ is logistic with $\\mu = 0.0$, $s = 1.5$, $N = 40000$, seed $161803$.\n- Case D (mixture with dominant point mass): $w = 0.95$, $x_0 = 0.0$, $f$ is logistic with $\\mu = -1.0$, $s = 0.8$, $N = 80000$, seed $141421$.\n\nUse the following absolute tolerances:\n- Point mass tolerance $\\tau_p = 0.01$.\n- Mean tolerance $\\tau_m = 0.02$.\n- Variance tolerance $\\tau_v = 0.05$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case, output six items in the following order: empirical point mass (float), empirical mean (float), empirical variance (float), then three booleans indicating the pass/fail of the point mass, mean, and variance tolerance checks. Concatenate the outputs of all test cases in order A, B, C, D into one flat list. For example, the overall structure is\n$$\n[\\hat{p}_A, \\hat{\\mu}_A, \\hat{\\sigma}^2_A, b^p_A, b^\\mu_A, b^\\sigma_A, \\hat{p}_B, \\hat{\\mu}_B, \\hat{\\sigma}^2_B, b^p_B, b^\\mu_B, b^\\sigma_B, \\ldots ].\n$$\nNo user input is required and no physical units are involved. Angles are not used. The output must be fully determined by the fixed seeds given above.", "solution": "The problem statement is subjected to validation.\n\n**Step 1: Extract Givens**\n- **Target Probability Density Function (PDF):** $p(x) = w\\,\\delta(x - x_0) + (1 - w)\\,f(x)$, where $w \\in [0,1]$, $x_0 \\in \\mathbb{R}$, and $f(x)$ is a proper PDF.\n- **Fundamental Principle:** The sampler must be derived from the inverse transform sampling principle, where $X = F^{-1}(U)$ for a uniform variate $U \\sim \\mathrm{Uniform}(0,1)$ and a cumulative distribution function (CDF) $F(x)$. $F^{-1}$ is the generalized inverse CDF. The sampler must use a single uniform variate per sample.\n- **Continuous Components $f(x)$:**\n  1. Exponential distribution with rate $\\lambda  0$ on $x \\ge 0$.\n  2. Uniform distribution on $[a,b]$ with $a  b$.\n  3. Logistic distribution on $x \\in \\mathbb{R}$ with location $\\mu$ and scale $s  0$.\n- **Required Derivations:**\n  1. The sampling algorithm itself, from first principles.\n  2. Inverse CDFs for each specified $f(x)$.\n  3. First and second moments for each $f(x)$.\n  4. Mean and variance of the mixture distribution $p(x)$.\n- **Test Suite:**\n  - **Case A:** $w = 0.3$, $x_0 = 1.5$, $f(x)$ is Exponential($\\lambda = 2.0$), $N = 80000$, seed $314159$.\n  - **Case B:** $w = 0$, $x_0 = -100.0$, $f(x)$ is Uniform($a = 2.0, b = 5.0$), $N = 60000$, seed $271828$.\n  - **Case C:** $w = 1$, $x_0 = -2.0$, $f(x)$ is Logistic($\\mu = 0.0, s = 1.5$), $N = 40000$, seed $161803$.\n  - **Case D:** $w = 0.95$, $x_0 = 0.0$, $f(x)$ is Logistic($\\mu = -1.0, s = 0.8$), $N = 80000$, seed $141421$.\n- **Tolerances:** Point mass $\\tau_p = 0.01$, mean $\\tau_m = 0.02$, variance $\\tau_v = 0.05$.\n- **Output Format:** A single flat list of floats and booleans: $[\\hat{p}_A, \\hat{\\mu}_A, \\hat{\\sigma}^2_A, b^p_A, b^\\mu_A, b^\\sigma_A, \\ldots]$.\n\n**Step 2: Validate Using Extracted Givens**\n1.  **Scientifically Grounded:** The problem is based on the well-established statistical methods of mixture models and inverse transform sampling. The specified distributions (Dirac delta, Exponential, Uniform, Logistic) are standard in probability theory. The problem is fundamentally sound.\n2.  **Well-Posed:** The problem defines a clear objective (implement and test a sampler) with all necessary parameters, constraints, and test cases provided. The existence of a solution is guaranteed by the theory of probability. The use of fixed seeds ensures a unique, reproducible output.\n3.  **Objective:** The problem is stated in precise mathematical and computational terms, free of subjectivity or ambiguity.\n\n**Step 3: Verdict and Action**\nThe problem is **valid**. It is scientifically sound, well-posed, objective, and complete. A full solution will be provided.\n\n**Derivation and Solution**\n\nThe objective is to construct a sampling algorithm for the mixed probability distribution with density $p(x) = w\\,\\delta(x - x_0) + (1 - w)\\,f(x)$, where $w$ is the weight of the point mass at $x_0$ and $f(x)$ is a continuous probability density. The derivation must adhere to the principle of inverse transform sampling.\n\n**1. General Sampling Algorithm from First Principles**\n\nThe cumulative distribution function (CDF) $P(x)$ for the density $p(x)$ is given by the integral:\n$$\nP(x) = \\int_{-\\infty}^{x} p(t) \\,dt = \\int_{-\\infty}^{x} \\left[ w\\,\\delta(t - x_0) + (1 - w)\\,f(t) \\right] dt\n$$\nBy linearity of the integral, we have:\n$$\nP(x) = w \\int_{-\\infty}^{x} \\delta(t - x_0) \\,dt + (1 - w) \\int_{-\\infty}^{x} f(t) \\,dt\n$$\nThe integral of the Dirac delta function is the Heaviside step function, $H(z)$, which is $1$ for $z \\ge 0$ and $0$ for $z  0$. The integral of the density $f(t)$ is its own CDF, which we denote as $F_f(x)$. Therefore, the mixture CDF is:\n$$\nP(x) = w\\,H(x - x_0) + (1-w)\\,F_f(x)\n$$\nThis CDF has a jump discontinuity of size $w$ at $x = x_0$. The principle of inverse transform sampling states that if $U$ is a random variable uniformly distributed on $[0,1]$, then the random variable $X = P^{-1}(U)$ has the CDF $P(x)$. Direct algebraic inversion to find $P^{-1}$ is complicated due to the step function and the piecewise nature of $P(x)$.\n\nHowever, we can construct an algorithm and then prove that it correctly generates samples according to $P(x)$, thereby demonstrating it as a valid implementation of the inverse transform method. This approach is known as the composition method. The mixture can be interpreted as a two-step process: first, choose a component (either the Dirac delta or the continuous distribution $f(x)$) with probabilities $w$ and $1-w$ respectively; second, draw a sample from the chosen component. This can be implemented with a single uniform variate.\n\nThe proposed algorithm is as follows:\n1.  Generate a single uniform random number $U \\sim \\mathrm{Uniform}(0,1)$.\n2.  If $U \\le w$, set the sample $X = x_0$. This branch is chosen with probability $w$.\n3.  If $U  w$, draw a sample from the density $f(x)$. This branch is chosen with probability $1-w$. To do this with the same variate $U$, we must transform it to a new uniform variate on $[0,1]$. Let $U' = \\frac{U-w}{1-w}$. If $U \\sim \\mathrm{Uniform}(w, 1]$, then $U' \\sim \\mathrm{Uniform}(0,1]$. We then set the sample $X = F_f^{-1}(U')$, where $F_f^{-1}$ is the inverse CDF of $f(x)$.\n\nTo prove this algorithm is correct, we derive the CDF of the random variable $X$ it produces. Let this computed CDF be $P_X(z) = \\mathrm{Pr}(X \\le z)$. Using the law of total probability, conditioning on the value of $U$:\n$$\nP_X(z) = \\mathrm{Pr}(X \\le z | U \\le w)\\mathrm{Pr}(U \\le w) + \\mathrm{Pr}(X \\le z | U  w)\\mathrm{Pr}(U  w)\n$$\nThe probabilities of the conditions are $\\mathrm{Pr}(U \\le w) = w$ and $\\mathrm{Pr}(U  w) = 1-w$.\n- If $U \\le w$, the algorithm sets $X = x_0$. Thus, $\\mathrm{Pr}(X \\le z | U \\le w) = \\mathrm{Pr}(x_0 \\le z) = H(z - x_0)$.\n- If $U  w$, the algorithm sets $X = F_f^{-1}(U')$. The variable $U' = \\frac{U-w}{1-w}$ is uniform on $(0,1]$. By the inverse transform theorem, the variable $X$ generated in this branch has the CDF $F_f(z)$. Thus, $\\mathrm{Pr}(X \\le z | U  w) = F_f(z)$.\n\nSubstituting these back into the equation for $P_X(z)$:\n$$\nP_X(z) = H(z - x_0) \\cdot w + F_f(z) \\cdot (1-w)\n$$\nThis is identical to the target CDF $P(x)$. The algorithm is therefore a correct implementation of sampling from $p(x)$ and is derived from the principles of inverse transform sampling.\n\n**2. Analysis of Continuous Components $f(x)$**\n\n**2.1. Exponential Distribution**\n- **PDF:** $f(x) = \\lambda e^{-\\lambda x}$ for $x \\ge 0$, and $0$ otherwise.\n- **CDF:** $F_f(x) = \\int_0^x \\lambda e^{-\\lambda t} dt = [-e^{-\\lambda t}]_0^x = 1 - e^{-\\lambda x}$ for $x \\ge 0$.\n- **Inverse CDF:** Let $u = F_f(x)$. Then $u = 1 - e^{-\\lambda x} \\implies e^{-\\lambda x} = 1 - u \\implies -\\lambda x = \\ln(1-u) \\implies x = -\\frac{\\ln(1-u)}{\\lambda}$. This is $F_f^{-1}(u)$.\n- **Moments:**\n  - Mean: $E_f[X] = \\int_0^\\infty x \\lambda e^{-\\lambda x} dx = \\frac{1}{\\lambda}$.\n  - Second Moment: $E_f[X^2] = \\int_0^\\infty x^2 \\lambda e^{-\\lambda x} dx = \\frac{2}{\\lambda^2}$.\n\n**2.2. Uniform Distribution**\n- **PDF:** $f(x) = \\frac{1}{b-a}$ for $x \\in [a,b]$, and $0$ otherwise.\n- **CDF:** $F_f(x) = \\int_a^x \\frac{1}{b-a} dt = \\frac{x-a}{b-a}$ for $x \\in [a,b]$.\n- **Inverse CDF:** Let $u = F_f(x)$. Then $u = \\frac{x-a}{b-a} \\implies u(b-a) = x-a \\implies x = a + u(b-a)$. This is $F_f^{-1}(u)$.\n- **Moments:**\n  - Mean: $E_f[X] = \\int_a^b x \\frac{1}{b-a} dx = \\frac{1}{b-a} [\\frac{x^2}{2}]_a^b = \\frac{b^2-a^2}{2(b-a)} = \\frac{a+b}{2}$.\n  - Second Moment: $E_f[X^2] = \\int_a^b x^2 \\frac{1}{b-a} dx = \\frac{1}{b-a} [\\frac{x^3}{3}]_a^b = \\frac{b^3-a^3}{3(b-a)} = \\frac{a^2+ab+b^2}{3}$.\n\n**2.3. Logistic Distribution**\n- **PDF:** $f(x) = \\frac{e^{-(x-\\mu)/s}}{s(1+e^{-(x-\\mu)/s})^2}$ for $x \\in \\mathbb{R}$.\n- **CDF:** $F_f(x) = \\frac{1}{1+e^{-(x-\\mu)/s}}$.\n- **Inverse CDF:** Let $u = F_f(x)$. Then $u = \\frac{1}{1+e^{-(x-\\mu)/s}} \\implies \\frac{1}{u} = 1+e^{-(x-\\mu)/s} \\implies \\frac{1-u}{u} = e^{-(x-\\mu)/s} \\implies \\ln(\\frac{1-u}{u}) = -\\frac{x-\\mu}{s} \\implies x = \\mu - s\\ln(\\frac{1-u}{u}) = \\mu + s\\ln(\\frac{u}{1-u})$. This is $F_f^{-1}(u)$.\n- **Moments:**\n  - Mean: $E_f[X] = \\mu$.\n  - Variance: $\\mathrm{Var}_f(X) = \\frac{\\pi^2 s^2}{3}$.\n  - Second Moment: $E_f[X^2] = \\mathrm{Var}_f(X) + (E_f[X])^2 = \\frac{\\pi^2 s^2}{3} + \\mu^2$.\n\n**3. Mixture Moments**\n\nThe moments of the mixture distribution $p(x)$ are derived using the law of the unconscious statistician.\n\n- **Mean of the Mixture ($E[X]$):**\n$$\nE[X] = \\int_{-\\infty}^{\\infty} x\\, p(x) \\,dx = \\int_{-\\infty}^{\\infty} x [w\\,\\delta(x - x_0) + (1 - w)\\,f(x)] \\,dx\n$$\n$$\nE[X] = w \\int_{-\\infty}^{\\infty} x\\,\\delta(x - x_0)\\,dx + (1-w) \\int_{-\\infty}^{\\infty} x\\,f(x)\\,dx\n$$\nBy the sifting property of the Dirac delta, the first integral is $x_0$. The second integral is the mean of $f(x)$, $E_f[X]$.\n$$\nE[X] = w\\,x_0 + (1-w)\\,E_f[X]\n$$\n\n- **Second Moment of the Mixture ($E[X^2]$):**\n$$\nE[X^2] = \\int_{-\\infty}^{\\infty} x^2 p(x) \\,dx = w \\int_{-\\infty}^{\\infty} x^2 \\delta(x - x_0) \\,dx + (1-w) \\int_{-\\infty}^{\\infty} x^2 f(x)\\,dx\n$$\nThe first integral is $x_0^2$. The second integral is the second moment of $f(x)$, $E_f[X^2]$.\n$$\nE[X^2] = w\\,x_0^2 + (1-w)\\,E_f[X^2]\n$$\n\n- **Variance of the Mixture ($\\mathrm{Var}(X)$):**\nThe variance is given by the standard formula, $\\mathrm{Var}(X) = E[X^2] - (E[X])^2$.\n$$\n\\mathrm{Var}(X) = \\left( w\\,x_0^2 + (1-w)\\,E_f[X^2] \\right) - \\left( w\\,x_0 + (1-w)\\,E_f[X] \\right)^2\n$$\nThese formulas allow for the calculation of the theoretical mean and variance for each test case.\nThe theoretical point mass probability at $x_0$ is simply $w$.\n\nThe implementation will now follow from these derivations.", "answer": "```python\nimport numpy as np\nimport math\n\n# Use a fixed version of the default generator for reproducibility across numpy versions.\n# PCG64 is available in numpy 1.23.5.\nGenerator = np.random.PCG64\n\nclass Distribution:\n    \"\"\"Abstract base class for continuous probability distributions.\"\"\"\n    def inverse_cdf(self, u: np.ndarray) - np.ndarray:\n        raise NotImplementedError\n\n    def mean(self) - float:\n        raise NotImplementedError\n\n    def second_moment(self) - float:\n        raise NotImplementedError\n\nclass Exponential(Distribution):\n    \"\"\"Exponential distribution f(x) = lambda * exp(-lambda * x).\"\"\"\n    def __init__(self, lam: float):\n        if not lam  0:\n            raise ValueError(\"Rate parameter lambda must be positive.\")\n        self.lam = float(lam)\n\n    def inverse_cdf(self, u: np.ndarray) - np.ndarray:\n        # Derived from u = 1 - exp(-lambda * x)\n        # Using 1-u. For a uniform variate u, 1-u is also uniform.\n        # This form is common, but log(u) is numerically more stable if u can be 1.\n        # Since u is U', it's from (0,1), so no risk of u=1 for inverse_cdf.\n        return -np.log(1.0 - u) / self.lam\n\n    def mean(self) - float:\n        return 1.0 / self.lam\n\n    def second_moment(self) - float:\n        return 2.0 / self.lam**2\n\nclass Uniform(Distribution):\n    \"\"\"Uniform distribution f(x) = 1/(b-a) on [a, b].\"\"\"\n    def __init__(self, a: float, b: float):\n        if not a  b:\n            raise ValueError(\"Parameter 'a' must be less than 'b'.\")\n        self.a = float(a)\n        self.b = float(b)\n\n    def inverse_cdf(self, u: np.ndarray) - np.ndarray:\n        # Derived from u = (x - a) / (b - a)\n        return self.a + u * (self.b - self.a)\n\n    def mean(self) - float:\n        return (self.a + self.b) / 2.0\n\n    def second_moment(self) - float:\n        # E[X^2] = Var(X) + E[X]^2\n        var = ((self.b - self.a)**2) / 12.0\n        mean = self.mean()\n        return var + mean**2\n\nclass Logistic(Distribution):\n    \"\"\"Logistic distribution.\"\"\"\n    def __init__(self, mu: float, s: float):\n        if not s  0:\n            raise ValueError(\"Scale parameter 's' must be positive.\")\n        self.mu = float(mu)\n        self.s = float(s)\n\n    def inverse_cdf(self, u: np.ndarray) - np.ndarray:\n        # Derived from u = 1 / (1 + exp(-(x-mu)/s))\n        # This is x = mu + s * log(u / (1-u))\n        # The input u will be in (0, 1), so no division by zero or log(0).\n        return self.mu + self.s * np.log(u / (1.0 - u))\n\n    def mean(self) - float:\n        return self.mu\n\n    def second_moment(self) - float:\n        # E[X^2] = Var(X) + E[X]^2\n        var = (math.pi**2 * self.s**2) / 3.0\n        return var + self.mu**2\n\ndef solve():\n    \"\"\"\n    Main function to execute the test suite for the mixed distribution sampler.\n    \"\"\"\n    test_cases = [\n        {'name': 'A', 'w': 0.3, 'x0': 1.5, 'f_type': 'exp', 'f_params': {'lam': 2.0}, 'N': 80000, 'seed': 314159},\n        {'name': 'B', 'w': 0.0, 'x0': -100.0, 'f_type': 'unif', 'f_params': {'a': 2.0, 'b': 5.0}, 'N': 60000, 'seed': 271828},\n        {'name': 'C', 'w': 1.0, 'x0': -2.0, 'f_type': 'logis', 'f_params': {'mu': 0.0, 's': 1.5}, 'N': 40000, 'seed': 161803},\n        {'name': 'D', 'w': 0.95, 'x0': 0.0, 'f_type': 'logis', 'f_params': {'mu': -1.0, 's': 0.8}, 'N': 80000, 'seed': 141421},\n    ]\n\n    tolerances = {'pm': 0.01, 'mean': 0.02, 'var': 0.05}\n    \n    dist_map = {\n        'exp': Exponential,\n        'unif': Uniform,\n        'logis': Logistic,\n    }\n\n    final_results = []\n\n    for case in test_cases:\n        w = case['w']\n        x0 = case['x0']\n        N = case['N']\n        seed = case['seed']\n        \n        f_dist = dist_map[case['f_type']](**case['f_params'])\n        \n        # --- Generate Samples ---\n        rng = np.random.Generator(Generator(seed))\n        U = rng.random(size=N)\n        samples = np.zeros(N)\n\n        # Boolean mask for the point mass component\n        is_point_mass = U = w\n        \n        # Assign point mass samples\n        samples[is_point_mass] = x0\n        \n        # Handle continuous component\n        # The condition (1-w)  0 avoids division by zero when w=1\n        num_continuous = np.sum(~is_point_mass)\n        if num_continuous  0:\n            # Rescale uniform variates for the continuous part\n            U_cont = U[~is_point_mass]\n            U_prime = (U_cont - w) / (1.0 - w)\n            samples[~is_point_mass] = f_dist.inverse_cdf(U_prime)\n\n        # --- Empirical Metrics ---\n        # The fraction of samples exactly equal to x0\n        emp_point_mass = np.sum(samples == x0) / N\n        emp_mean = np.mean(samples)\n        # np.var with ddof=0 calculates population variance (1/N), as required\n        emp_var = np.var(samples)\n\n        # --- Theoretical Metrics ---\n        theo_point_mass = w\n        \n        E_f_X = f_dist.mean()\n        E_f_X2 = f_dist.second_moment()\n        \n        theo_mean = w * x0 + (1.0 - w) * E_f_X\n        E_mix_X2 = w * x0**2 + (1.0 - w) * E_f_X2\n        theo_var = E_mix_X2 - theo_mean**2\n        \n        # --- Tolerance Checks ---\n        pm_ok = abs(emp_point_mass - theo_point_mass)  tolerances['pm']\n        mean_ok = abs(emp_mean - theo_mean)  tolerances['mean']\n        var_ok = abs(emp_var - theo_var)  tolerances['var']\n        \n        # Collect results for this case\n        case_results = [\n            emp_point_mass,\n            emp_mean,\n            emp_var,\n            pm_ok,\n            mean_ok,\n            var_ok\n        ]\n        final_results.extend(case_results)\n\n    # Format output as a single flat list\n    # The map converts booleans to 'True'/'False' strings as required for joining.\n    formatted_results = []\n    for item in final_results:\n        if isinstance(item, bool):\n            formatted_results.append(str(item))\n        else:\n            # Rounding for clean output, not required but good practice.\n            formatted_results.append(f\"{item:.6f}\")\n\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2403899"}, {"introduction": "The power of the inverse transform method extends beyond cases where the inverse cumulative distribution function (CDF) can be found analytically. This final exercise [@problem_id:2403893] brings the technique into the realm of quantum mechanics, where you will simulate the probable location of a particle in the first excited state of a one-dimensional infinite potential well. Since the CDF for this system is a transcendental function, you must employ a numerical root-finding algorithm to compute its inverse, a critical skill for applying sampling methods to many realistic physical models.", "problem": "A non-relativistic quantum particle is confined in a one-dimensional infinite potential well on the interval $[0,L]$ with perfectly rigid walls. The normalized stationary state wavefunctions are given by $\\psi_n(x) = \\sqrt{\\frac{2}{L}} \\sin\\!\\left(\\frac{n\\pi x}{L}\\right)$ for $x \\in [0,L]$ and integer $n \\ge 1$. Consider the first excited state with $n = 2$, so that the probability density on $[0,L]$ is $f(x) = \\lvert \\psi_2(x) \\rvert^2$. All trigonometric arguments must be interpreted in radians.\n\nTask:\n1. Starting from first principles, derive the cumulative distribution function $F(x)$ associated with $f(x)$, defined by $F(x) = \\int_{0}^{x} f(t)\\,dt$ for $x \\in [0,L]$. Prove that $F(x)$ is strictly increasing on $[0,L]$ and hence has a unique inverse $G(u)$ on $u \\in [0,1]$ such that $F(G(u)) = u$.\n2. Construct a program that, for each test case listed below, performs the following:\n   - Computes the function $F(x)$ and its inverse $G(u)$ such that for any $u \\in [0,1]$, the value $x = G(u)$ satisfies $\\lvert F(x) - u \\rvert \\le 10^{-12}$.\n   - Generates $N$ independent pseudorandom variates $u_1,\\dots,u_N$ that are uniformly distributed on the open interval $(0,1)$ using a pseudorandom number generator initialized with the specified integer seed for that test case.\n   - Transforms the uniform variates to samples $x_i = G(u_i)$ that follow the distribution with density $f(x)$ on $[0,L]$.\n   - Computes the following three summary statistics for the samples:\n     - The sample mean $m_N = \\frac{1}{N}\\sum_{i=1}^{N} x_i$ (express your answer in the same length unit as $L$).\n     - The sample second moment $s_N^{(2)} = \\frac{1}{N}\\sum_{i=1}^{N} x_i^2$ (express your answer in the square of the length unit of $L$).\n     - The Kolmogorov–Smirnov statistic $D_N$ for the target cumulative distribution function $F$, defined by\n       $$D_N = \\max\\!\\left(\\max_{1\\le i\\le N}\\left(\\frac{i}{N} - F(x_{(i)})\\right), \\max_{1\\le i\\le N}\\left(F(x_{(i)}) - \\frac{i-1}{N}\\right)\\right),$$\n       where $x_{(1)} \\le \\cdots \\le x_{(N)}$ is the non-decreasing ordering of the samples. The statistic $D_N$ is dimensionless.\n3. Your program must produce a single line of output containing the results for all test cases as a comma-separated list of sublists, where each sublist contains the three values $[m_N, s_N^{(2)}, D_N]$ in that order. Each floating-point result in the output must be rounded to $6$ decimal places. For example, the required overall format is\n   $$[[m_{N,1}, s^{(2)}_{N,1}, D_{N,1}], [m_{N,2}, s^{(2)}_{N,2}, D_{N,2}], \\ldots]$$\n   and the program must print exactly one line containing this list with the brackets and commas as shown.\n\nTest Suite:\n- Case $1$: $L = 1.0$, $N = 1000$, seed $= 12345$.\n- Case $2$: $L = 5.0$, $N = 2000$, seed $= 20231102$.\n- Case $3$: $L = 2.0$, $N = 1$, seed $= 42$.\n\nAngle unit specification: all angles appearing inside trigonometric functions must be in radians.\n\nPhysical units: report $m_N$ in the same unit as $L$, $s_N^{(2)}$ in the square of that unit, and $D_N$ as a unitless number.\n\nFinal Output Format: Your program should produce a single line of output containing the results as a comma-separated list of sublists, each sublist of the form $[m_N,s_N^{(2)},D_N]$, enclosed in square brackets, for example: $[[a,b,c],[d,e,f],[g,h,i]]$.", "solution": "The problem requires the application of the inverse transform sampling method to generate random variates from a distribution defined by the quantum mechanical probability density of a particle in a one-dimensional infinite potential well. The steps involve analytical derivation of the probability and cumulative distribution functions, followed by a numerical implementation to generate samples and compute specified statistics. The problem is scientifically grounded and well-posed.\n\nThe system is a particle in an infinite potential well on the interval $[0, L]$. The stationary state wavefunctions are $\\psi_n(x) = \\sqrt{\\frac{2}{L}} \\sin\\left(\\frac{n\\pi x}{L}\\right)$ for an integer quantum number $n \\ge 1$. We are concerned with the first excited state, which corresponds to $n=2$.\n\n**1. Probability Density Function (PDF)**\n\nThe probability density function $f(x)$ is given by the squared modulus of the wavefunction, $f(x) = |\\psi_2(x)|^2$.\nFor $n=2$, the wavefunction is:\n$$ \\psi_2(x) = \\sqrt{\\frac{2}{L}} \\sin\\left(\\frac{2\\pi x}{L}\\right) $$\nThe corresponding PDF for $x \\in [0, L]$ is:\n$$ f(x) = \\left| \\sqrt{\\frac{2}{L}} \\sin\\left(\\frac{2\\pi x}{L}\\right) \\right|^2 = \\frac{2}{L} \\sin^2\\left(\\frac{2\\pi x}{L}\\right) $$\nUsing the trigonometric power-reduction identity $\\sin^2(\\theta) = \\frac{1 - \\cos(2\\theta)}{2}$, we can rewrite the PDF as:\n$$ f(x) = \\frac{2}{L} \\left( \\frac{1 - \\cos\\left(2 \\cdot \\frac{2\\pi x}{L}\\right)}{2} \\right) = \\frac{1}{L} \\left( 1 - \\cos\\left(\\frac{4\\pi x}{L}\\right) \\right) $$\nThis form is more convenient for integration.\n\n**2. Cumulative Distribution Function (CDF)**\n\nThe cumulative distribution function $F(x)$ is defined as the integral of the PDF from $0$ to $x$:\n$$ F(x) = \\int_{0}^{x} f(t) \\,dt = \\int_{0}^{x} \\frac{1}{L} \\left( 1 - \\cos\\left(\\frac{4\\pi t}{L}\\right) \\right) dt $$\nWe perform the integration term by term:\n$$ F(x) = \\frac{1}{L} \\left[ \\int_{0}^{x} 1 \\,dt - \\int_{0}^{x} \\cos\\left(\\frac{4\\pi t}{L}\\right) dt \\right] $$\n$$ F(x) = \\frac{1}{L} \\left[ t - \\frac{L}{4\\pi} \\sin\\left(\\frac{4\\pi t}{L}\\right) \\right]_{0}^{x} $$\nEvaluating at the limits gives:\n$$ F(x) = \\frac{1}{L} \\left[ \\left( x - \\frac{L}{4\\pi} \\sin\\left(\\frac{4\\pi x}{L}\\right) \\right) - \\left( 0 - \\frac{L}{4\\pi} \\sin(0) \\right) \\right] $$\n$$ F(x) = \\frac{x}{L} - \\frac{1}{4\\pi} \\sin\\left(\\frac{4\\pi x}{L}\\right) $$\nAs a check, we evaluate the CDF at the boundaries: $F(0) = 0$ and $F(L) = \\frac{L}{L} - \\frac{1}{4\\pi}\\sin(4\\pi) = 1 - 0 = 1$. This confirms the normalization.\n\n**3. Invertibility of the CDF**\n\nTo apply inverse transform sampling, the CDF $F(x)$ must have a unique inverse. A continuous function has a unique inverse if it is strictly monotonic. We can prove that $F(x)$ is strictly increasing on $[0, L]$ by examining its derivative. By the Fundamental Theorem of Calculus, $F'(x) = f(x)$.\n$$ F'(x) = f(x) = \\frac{2}{L} \\sin^2\\left(\\frac{2\\pi x}{L}\\right) $$\nThe term $\\frac{2}{L}$ is positive. The term $\\sin^2(\\theta)$ is non-negative for all real $\\theta$. It is equal to $0$ only when $\\sin(\\theta)=0$. For $\\theta = \\frac{2\\pi x}{L}$ with $x \\in [0, L]$, this occurs at $x=0$, $x=L/2$, and $x=L$.\nSince $F'(x) \\ge 0$ for all $x \\in [0, L]$ and $F'(x)$ is zero only at a finite number of isolated points, the function $F(x)$ is strictly increasing over the entire interval $[0, L]$. Therefore, a unique inverse function $G(u) = F^{-1}(u)$ exists for $u \\in [0, 1]$.\n\n**4. The Inverse Transform Sampling Algorithm**\n\nThe inverse transform method consists of two steps:\n1. Generate a random variate $u$ from the uniform distribution on $[0, 1]$, denoted $u \\sim U(0, 1)$.\n2. Compute the sample $x$ by applying the inverse CDF: $x = G(u) = F^{-1}(u)$.\n\nThe equation we must solve for $x$ is $F(x) = u$, or:\n$$ \\frac{x}{L} - \\frac{1}{4\\pi} \\sin\\left(\\frac{4\\pi x}{L}\\right) = u $$\nThis is a transcendental equation that cannot be solved for $x$ in a simple closed form. We must employ a numerical root-finding algorithm. For a given $u$, we seek the root $x$ of the function $h(x) = F(x) - u = 0$ on the interval $[0, L]$. Since $h(0) = F(0) - u = -u  0$ and $h(L) = F(L) - u = 1 - u  0$ for $u \\in (0, 1)$, the root is bracketed in $[0, L]$. A robust and efficient algorithm like Brent's method is suitable for finding this root to the required precision.\n\n**5. Computation of Summary Statistics**\n\nFor a set of $N$ samples $\\{x_1, x_2, \\ldots, x_N\\}$ generated by this method, we compute the following statistics:\n- **Sample Mean ($m_N$)**: The arithmetic average of the samples.\n$$ m_N = \\frac{1}{N} \\sum_{i=1}^{N} x_i $$\n- **Sample Second Moment ($s_N^{(2)}$)**: The arithmetic average of the squares of the samples.\n$$ s_N^{(2)} = \\frac{1}{N} \\sum_{i=1}^{N} x_i^2 $$\n- **Kolmogorov–Smirnov (KS) Statistic ($D_N$)**: This statistic measures the maximum distance between the empirical distribution function (EDF) of the samples and the theoretical cumulative distribution function $F(x)$. For a sorted sample set $x_{(1)} \\le x_{(2)} \\le \\cdots \\le x_{(N)}$, the statistic is defined as:\n$$ D_N = \\max\\left( \\max_{1 \\le i \\le N}\\left(\\frac{i}{N} - F(x_{(i)})\\right), \\max_{1 \\le i \\le N}\\left(F(x_{(i)}) - \\frac{i-1}{N}\\right) \\right) $$\nThe algorithm will be implemented for each test case, generating $N$ samples using the specified seed, calculating these three statistics, and reporting the results in the required format.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import brentq\n\ndef solve():\n    \"\"\"\n    Solves the inverse transform sampling problem for a particle in a box.\n    \"\"\"\n    test_cases = [\n        # (L, N, seed)\n        (1.0, 1000, 12345),\n        (5.0, 2000, 20231102),\n        (2.0, 1, 42),\n    ]\n\n    def F_cdf(x, L):\n        \"\"\"\n        Computes the theoretical cumulative distribution function (CDF) F(x).\n        F(x) = x/L - sin(4*pi*x/L) / (4*pi)\n        \"\"\"\n        arg = 4.0 * np.pi * x / L\n        return (x / L) - (np.sin(arg) / (4.0 * np.pi))\n\n    all_results = []\n\n    for L, N, seed in test_cases:\n        # 1. Initialize the pseudorandom number generator with the specified seed.\n        rng = np.random.default_rng(seed)\n\n        # 2. Generate N independent pseudorandom variates uniformly distributed on (0,1).\n        u_samples = rng.uniform(0.0, 1.0, size=N)\n\n        # 3. For each uniform variate u, find the corresponding x by inverting the CDF.\n        # This requires solving F(x) - u = 0 for x.\n        x_samples = np.zeros(N)\n        for i, u in enumerate(u_samples):\n            # The function to find the root of.\n            func_to_solve = lambda x: F_cdf(x, L) - u\n            \n            # Use Brent's method to find the root in the interval [0, L].\n            # The root is guaranteed to be in this interval as F(0)=0 and F(L)=1.\n            # h(0) = -u  0 and h(L) = 1-u  0 for u in (0,1).\n            x_samples[i] = brentq(func_to_solve, 0.0, L)\n\n        # 4. Compute the required summary statistics.\n        # Sample mean\n        m_N = np.mean(x_samples)\n\n        # Sample second moment\n        s2_N = np.mean(np.square(x_samples))\n\n        # Kolmogorov-Smirnov statistic\n        if N  0:\n            x_sorted = np.sort(x_samples)\n            F_values = F_cdf(x_sorted, L)\n            \n            # i/N for i=1,...,N\n            i_over_N = np.arange(1, N + 1) / N\n            # (i-1)/N for i=1,...,N\n            i_minus_1_over_N = np.arange(0, N) / N\n            \n            # D_N = max(max(i/N - F(x_i)), max(F(x_i) - (i-1)/N))\n            max1 = np.max(i_over_N - F_values)\n            max2 = np.max(F_values - i_minus_1_over_N)\n            D_N = np.max([max1, max2])\n        else:\n            # Although not in the test suite, handle the N=0 case gracefully.\n            m_N, s2_N, D_N = 0.0, 0.0, 0.0\n\n        all_results.append([m_N, s2_N, D_N])\n\n    # 5. Format the final output string exactly as specified.\n    # Each sublist is formatted, then joined by commas, then enclosed in brackets.\n    sublist_strs = [f\"[{res[0]:.6f},{res[1]:.6f},{res[2]:.6f}]\" for res in all_results]\n    final_output_str = f\"[{','.join(sublist_strs)}]\"\n\n    print(final_output_str)\n\nsolve()\n```", "id": "2403893"}]}