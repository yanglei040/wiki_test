{"hands_on_practices": [{"introduction": "The first step in understanding correlated data is to compute the autocorrelation function directly from a time series. This practice provides a foundational exercise where you will generate data from a simple, exactly solvable physical system—a collection of harmonic oscillators—and then implement the standard formulas to calculate the normalized autocorrelation function, $\\rho(\\ell)$, and the integrated autocorrelation time, $\\tau_{\\mathrm{int}}$. By building the analysis from the ground up, you will gain a concrete understanding of how a system's intrinsic dynamics manifest as statistical correlations in your measurements. [@problem_id:2442392]", "problem": "You are asked to construct a self-contained program that generates and analyzes deterministic time series for the instantaneous temperature of a microcanonical (constant Number, Volume, and Energy) ensemble (NVE) molecular dynamics (MD) system. Consider a system modeled as $M$ noninteracting, classical, one-dimensional harmonic oscillators with unit mass. For oscillator $j \\in \\{1,\\dots,M\\}$, the angular frequency is $\\omega_j$, the initial phase is $\\phi_j$, and the total energy is $E_j$. The position and velocity satisfy the equations of motion $\\ddot{x}_j(t) + \\omega_j^2 x_j(t) = 0$ with $m_j = 1$. The instantaneous kinetic energy of oscillator $j$ is $K_j(t) = \\tfrac{1}{2} v_j(t)^2$, and for harmonic motion this yields $K_j(t) = E_j \\sin^2(\\omega_j t + \\phi_j)$. The total kinetic energy is $K(t) = \\sum_{j=1}^M K_j(t)$.\n\nDefine the instantaneous temperature time series by\n$$\nT_{\\mathrm{inst}}(t) = \\frac{2}{f k_{\\mathrm{B}}}\\,K(t),\n$$\nwhere $f$ is the number of velocity degrees of freedom and $k_{\\mathrm{B}}$ is the Boltzmann constant. Use dimensionless units with $k_{\\mathrm{B}} = 1$ and $f = M$. Let the discrete sampling times be $t_n = n\\,\\Delta t$ for $n \\in \\{0,1,\\dots,N-1\\}$.\n\nFor a given discrete time series $\\{T_{\\mathrm{inst}}(t_n)\\}_{n=0}^{N-1}$, define the sample mean $\\mu = \\frac{1}{N} \\sum_{n=0}^{N-1} T_{\\mathrm{inst}}(t_n)$, the discrete autocovariance\n$$\n\\Gamma(\\ell) = \\frac{1}{N - \\ell} \\sum_{n=0}^{N-1-\\ell} \\big(T_{\\mathrm{inst}}(t_n) - \\mu\\big)\\,\\big(T_{\\mathrm{inst}}(t_{n+\\ell}) - \\mu\\big), \\quad \\ell \\in \\{0,1,\\dots,N-1\\},\n$$\nand the normalized autocorrelation\n$$\n\\rho(\\ell) = \\frac{\\Gamma(\\ell)}{\\Gamma(0)}.\n$$\nDefine the statistical inefficiency\n$$\ng = 1 + 2 \\sum_{\\ell=1}^{L^\\star} \\rho(\\ell),\n$$\nwhere $L^\\star$ is the largest nonnegative integer such that $\\rho(\\ell)  0$ for all $\\ell \\in \\{1,2,\\dots,L^\\star\\}$ and either $L^\\star = N-1$ or $\\rho(L^\\star+1) \\le 0$. Define the integrated autocorrelation time by\n$$\n\\tau_{\\mathrm{int}} = \\frac{\\Delta t}{2}\\, g,\n$$\nso that the variance of the sample mean scales as in the standard continuous-time relation.\n\nYour program must, for each test case below, generate $T_{\\mathrm{inst}}(t_n)$ from first principles using the model above, compute $\\rho(\\ell)$ until the first nonpositive value, and then compute $g$ and $\\tau_{\\mathrm{int}}$ using the definitions above.\n\nTest suite (all quantities are dimensionless):\n- Case $\\mathcal{A}$:\n  - $M = 1$, $N = 4096$, $\\Delta t = 0.01$,\n  - $\\omega_1 = 1.5$, $\\phi_1 = 0.37$, $E_1 = 1.0$.\n- Case $\\mathcal{B}$:\n  - $M = 2$, $N = 4096$, $\\Delta t = 0.01$,\n  - $(\\omega_1,\\omega_2) = \\big(1.0, \\sqrt{2}\\big)$,\n  - $(\\phi_1,\\phi_2) = (0.1, 1.3)$,\n  - $(E_1,E_2) = (1.0, 0.6)$.\n- Case $\\mathcal{C}$:\n  - $M = 10$, $N = 8192$, $\\Delta t = 0.005$,\n  - $\\omega_j$ linearly spaced over the closed interval $[0.5, 2.0]$ for $j \\in \\{1,\\dots,10\\}$,\n  - $\\phi_j = \\big(j \\pi / 7\\big) \\bmod (2\\pi)$ for $j \\in \\{1,\\dots,10\\}$,\n  - $E_j = 1.0$ for all $j \\in \\{1,\\dots,10\\}$.\n\nFor each case, report the pair $\\big[\\tau_{\\mathrm{int}}, g\\big]$. Express $\\tau_{\\mathrm{int}}$ in the same time unit as $t$ and $\\Delta t$. Your program should produce a single line of output containing the results as a comma-separated outer list of inner lists with no spaces, in the format $[[\\tau_{\\mathrm{int}}^{(\\mathcal{A})},g^{(\\mathcal{A})}],[\\tau_{\\mathrm{int}}^{(\\mathcal{B})},g^{(\\mathcal{B})}],[\\tau_{\\mathrm{int}}^{(\\mathcal{C})},g^{(\\mathcal{C})}]]$.", "solution": "The problem statement has been subjected to rigorous validation and is determined to be valid. It is scientifically grounded, mathematically well-posed, and provides a complete and consistent set of definitions and data required for a unique solution. We may therefore proceed with the derivation and implementation of the solution.\n\nThe problem requires the computation of the statistical inefficiency, $g$, and the integrated autocorrelation time, $\\tau_{\\mathrm{int}}$, for a time series of instantaneous temperature, $T_{\\mathrm{inst}}(t)$. This time series is generated from a deterministic model of $M$ non-interacting, one-dimensional classical harmonic oscillators.\n\nThe instantaneous temperature is defined in dimensionless units ($k_{\\mathrm{B}}=1$) with $f=M$ degrees of freedom as:\n$$\nT_{\\mathrm{inst}}(t) = \\frac{2}{M} K(t)\n$$\nwhere $K(t)$ is the total kinetic energy of the system. For a system of $M$ harmonic oscillators with unit mass ($m_j=1$), initial parameters $(\\omega_j, \\phi_j, E_j)$, the total kinetic energy is given by the sum of individual kinetic energies:\n$$\nK(t) = \\sum_{j=1}^{M} K_j(t) = \\sum_{j=1}^{M} E_j \\sin^2(\\omega_j t + \\phi_j)\n$$\nThus, the expression for the instantaneous temperature becomes:\n$$\nT_{\\mathrm{inst}}(t) = \\frac{2}{M} \\sum_{j=1}^{M} E_j \\sin^2(\\omega_j t + \\phi_j)\n$$\nThis function is sampled at discrete times $t_n = n\\,\\Delta t$ for $n \\in \\{0, 1, \\dots, N-1\\}$, yielding a discrete time series $\\{T_n\\} \\equiv \\{T_{\\mathrm{inst}}(t_n)\\}$.\n\nThe subsequent analysis of this time series follows a standard statistical procedure. First, we compute the sample mean of the series:\n$$\n\\mu = \\frac{1}{N} \\sum_{n=0}^{N-1} T_n\n$$\nNext, we compute the discrete, unbiased autocovariance function $\\Gamma(\\ell)$ for a time lag of $\\ell$ steps:\n$$\n\\Gamma(\\ell) = \\frac{1}{N - \\ell} \\sum_{n=0}^{N-1-\\ell} \\big(T_n - \\mu\\big)\\,\\big(T_{n+\\ell} - \\mu\\big)\n$$\nThis is normalized by the variance, $\\Gamma(0)$, to obtain the normalized autocorrelation function, $\\rho(\\ell)$:\n$$\n\\rho(\\ell) = \\frac{\\Gamma(\\ell)}{\\Gamma(0)}\n$$\nThe statistical inefficiency, $g$, quantifies the number of simulation steps required to obtain an independent sample. It is calculated by integrating the autocorrelation function:\n$$\ng = 1 + 2 \\sum_{\\ell=1}^{L^\\star} \\rho(\\ell)\n$$\nThe summation cutoff, $L^\\star$, is defined as the largest integer for which the autocorrelation $\\rho(\\ell)$ remains positive for all preceding lags, i.e., $\\rho(\\ell)  0$ for all $\\ell \\in \\{1, 2, \\dots, L^\\star\\}$. The summation must terminate at the first lag where the autocorrelation function becomes non-positive. If $\\rho(\\ell)  0$ for all $\\ell \\in \\{1, \\dots, N-1\\}$, then $L^\\star = N-1$.\n\nFinally, the integrated autocorrelation time, $\\tau_{\\mathrm{int}}$, is determined from the statistical inefficiency and the simulation time step, $\\Delta t$:\n$$\n\\tau_{\\mathrm{int}} = \\frac{\\Delta t}{2}\\, g\n$$\nThis value represents the characteristic time over which the system's \"memory\" persists.\n\nThe algorithmic procedure to solve the problem for each test case is as follows:\n1.  Initialize the parameters for the specific case: $M, N, \\Delta t$, and the sets of oscillator parameters $\\{\\omega_j\\}$, $\\{\\phi_j\\}$, and $\\{E_j\\}$.\n2.  Generate the discrete time vector $t_n = n\\Delta t$ for $n = 0, \\dots, N-1$.\n3.  Construct the instantaneous temperature time series $\\{T_n\\}$ by evaluating the analytical expression for $T_{\\mathrm{inst}}(t_n)$. This is a vectorized operation for efficiency.\n4.  Calculate the sample mean $\\mu$ of the time series $\\{T_n\\}$.\n5.  Center the time series by subtracting the mean: $T'_n = T_n - \\mu$.\n6.  Calculate the variance $\\Gamma(0) = \\frac{1}{N}\\sum_{n=0}^{N-1} (T'_n)^2$.\n7.  Iterate for lag $\\ell$ from $1$ to $N-1$. In each step:\n    a. Compute the autocovariance sum $\\sum_{n=0}^{N-1-\\ell} T'_n T'_{n+\\ell}$.\n    b. Calculate $\\Gamma(\\ell)$ by dividing by the normalization factor $N-\\ell$.\n    c. Calculate $\\rho(\\ell) = \\Gamma(\\ell) / \\Gamma(0)$.\n    d. If $\\rho(\\ell) \\le 0$, the condition for termination is met. Set $L^\\star = \\ell-1$ and break the loop. The sum for $g$ is complete.\n    e. If $\\rho(\\ell)  0$, add it to a running sum for the calculation of $g$. If the loop completes without breaking, $L^\\star=N-1$.\n8.  Compute the statistical inefficiency $g = 1 + 2 \\sum_{\\ell=1}^{L^\\star} \\rho(\\ell)$.\n9.  Compute the integrated autocorrelation time $\\tau_{\\mathrm{int}} = \\frac{\\Delta t}{2} g$.\n10. Store the resulting pair $[\\tau_{\\mathrm{int}}, g]$.\n\nThis procedure is implemented for each of the three test cases provided. The numerical implementation relies on the `numpy` library for efficient array manipulations. The final results are then formatted into a single string as specified.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for all test cases and prints the final result.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A\n        {\n            \"M\": 1, \"N\": 4096, \"dt\": 0.01,\n            \"omegas\": np.array([1.5]),\n            \"phis\": np.array([0.37]),\n            \"Es\": np.array([1.0]),\n        },\n        # Case B\n        {\n            \"M\": 2, \"N\": 4096, \"dt\": 0.01,\n            \"omegas\": np.array([1.0, np.sqrt(2)]),\n            \"phis\": np.array([0.1, 1.3]),\n            \"Es\": np.array([1.0, 0.6]),\n        },\n        # Case C\n        {\n            \"M\": 10, \"N\": 8192, \"dt\": 0.005,\n            \"omegas\": np.linspace(0.5, 2.0, 10),\n            \"phis\": (np.arange(1, 11) * np.pi / 7) % (2 * np.pi),\n            \"Es\": np.ones(10),\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        result = calculate_autocorrelation_properties(case)\n        results.append(result)\n\n    # Format the output string to be exactly as required, without spaces within lists.\n    output_str = f\"[{','.join(f'[{tau},{g}]' for tau, g in results)}]\"\n    print(output_str)\n\ndef calculate_autocorrelation_properties(params):\n    \"\"\"\n    Calculates tau_int and g for a single test case.\n\n    Args:\n        params (dict): A dictionary containing all parameters for the case.\n\n    Returns:\n        tuple: A tuple containing (tau_int, g).\n    \"\"\"\n    M = params[\"M\"]\n    N = params[\"N\"]\n    dt = params[\"dt\"]\n    omegas = params[\"omegas\"]\n    phis = params[\"phis\"]\n    Es = params[\"Es\"]\n\n    # 1. Generate the time series for instantaneous temperature\n    t = np.arange(N) * dt\n    T_inst = np.zeros(N)\n\n    # Sum contributions from each oscillator\n    for j in range(M):\n        arg = omegas[j] * t + phis[j]\n        T_inst += Es[j] * np.sin(arg)**2\n    \n    T_inst *= (2.0 / M)\n    \n    # 2. Calculate sample mean and center the series\n    mu = np.mean(T_inst)\n    T_prime = T_inst - mu\n\n    # 3. Calculate variance Gamma(0)\n    # The problem defines Gamma(l) with 1/(N-l) prefactor.\n    # For l=0, Gamma(0) = (1/N) * sum((T_n - mu)^2).\n    # np.var calculates (1/N) * sum((x-mean(x))^2), which matches.\n    gamma_0 = np.var(T_inst)\n\n    if gamma_0 == 0:\n        # If variance is zero, the series is constant.\n        # Autocorrelation is undefined, but g=1 and tau_int = dt/2 is a reasonable limit.\n        g = 1.0\n        tau_int = 0.5 * dt * g\n        return tau_int, g\n\n    # 4. Find L_star and sum rho(l)\n    sum_rho = 0.0\n    # L_star is largest l = N-2 s.t. rho(1...l)  0\n    # but the problem formulation uses l in {1,...,L_star}.\n    # We sum rho(l) for l  0 as long as rho(l) is positive.\n    for l in range(1, N):\n        # Calculate autocovariance Gamma(l)\n        # Gamma(l) = 1/(N-l) * sum_{n=0}^{N-l-1} (T_n-mu)(T_{n+l}-mu)\n        numerator = np.dot(T_prime[0:N-l], T_prime[l:N])\n        gamma_l = numerator / (N - l)\n        \n        # Calculate normalized autocorrelation rho(l)\n        rho_l = gamma_l / gamma_0\n        \n        if rho_l > 0:\n            sum_rho += rho_l\n        else:\n            # First non-positive rho(l) found, stop summation.\n            break\n            \n    # 5. Calculate g and tau_int\n    # g = 1 + 2 * sum_{l=1}^{L_star} rho(l)\n    g = 1.0 + 2.0 * sum_rho\n    \n    # tau_int = (dt/2) * g\n    tau_int = 0.5 * dt * g\n    \n    return tau_int, g\n\nsolve()\n```", "id": "2442392"}, {"introduction": "While simple physical models are instructive, the time series from complex simulations, such as those near a phase transition, can show diverse correlation structures. This exercise challenges you to apply your analysis skills to different types of synthetic data that mimic these behaviors, including monotonic and oscillatory correlations. You will calculate the integrated autocorrelation time, $\\tau_{\\mathrm{int}}$, for each and use it to determine the effective number of independent samples, $N_{\\mathrm{eff}}$, a crucial metric for judging the statistical quality of simulation data. [@problem_id:2442361]", "problem": "Consider a time series representing the virial pressure from an equilibrium Molecular Dynamics (MD) simulation near a liquid-gas critical point, where large fluctuations and long-time correlations are expected. Let the discrete-time series be denoted by $\\{p_t\\}_{t=1}^N$. For any finite series, define the sample mean $\\bar{p}$, the lag-$k$ sample autocovariance $C(k)$, the normalized sample autocorrelation function $\\rho(k)$, the integrated autocorrelation time $\\tau_{\\mathrm{int}}$, the statistical inefficiency $g$, and the effective number of uncorrelated samples $N_{\\mathrm{eff}}$. Note: This problem uses a common convention where the \"integrated autocorrelation time\" $\\tau_{\\mathrm{int}}$ is defined to be equal to the statistical inefficiency $g$. The relevant formulas are:\n$$\n\\bar{p} \\equiv \\frac{1}{N}\\sum_{t=1}^{N} p_t,\n\\quad\nC(k) \\equiv \\frac{1}{N-k}\\sum_{t=1}^{N-k} \\left(p_t-\\bar{p}\\right)\\left(p_{t+k}-\\bar{p}\\right) \\ \\text{for} \\ k=0,1,\\dots,N-1,\n$$\n$$\n\\rho(k) \\equiv \\frac{C(k)}{C(0)},\n\\quad\n\\tau_{\\mathrm{int}} \\equiv g \\equiv 1 + 2 \\sum_{k=1}^{M} \\rho(k),\n\\quad\nN_{\\mathrm{eff}} \\equiv \\frac{N}{g}.\n$$\nThe truncation index $M$ is defined as the largest nonnegative integer for which all earlier lags are strictly positive, specifically:\n$$\nM \\equiv \\max \\left\\{ m \\in \\{0,1,\\dots,N-1\\}: \\rho(k)  0 \\ \\text{for all} \\ k \\in \\{1,2,\\dots,m\\} \\right\\}.\n$$\nIf no nonpositive $\\rho(k)$ occurs, take $M \\equiv N-1$. All quantities in this problem are dimensionless, and any angles must be interpreted in radians.\n\nYou are to compute $\\tau_{\\mathrm{int}}$ and $N_{\\mathrm{eff}}$ for a small test suite of synthetic virial pressure series constructed from linear stochastic recurrences driven by a deterministic pseudo-random “thermal noise” defined below. Each series length is $N$, and an initial transient of length $B$ is discarded to ensure approximate stationarity. Specifically, define the pseudo-random number generator and noise as follows:\n- Linear Congruential Generator (LCG): integers $X_{n+1} \\equiv (a X_n + c) \\bmod m$ with $m \\equiv 2^{64}$, $a \\equiv 6364136223846793005$, $c \\equiv 1442695040888963407$, and seed $X_0$ specified per case. Uniform deviates are $U_n \\equiv X_n / m \\in [0,1)$.\n- Standard normal deviates $\\epsilon_t$ are produced in order via the Box-Muller transform: for independent uniforms $U_{2j},U_{2j+1}$, define\n$$\nZ_{2j} \\equiv \\sqrt{-2 \\ln U_{2j}}\\ \\cos(2\\pi U_{2j+1}), \\quad\nZ_{2j+1} \\equiv \\sqrt{-2 \\ln U_{2j}}\\ \\sin(2\\pi U_{2j+1}),\n$$\nand take $\\epsilon_t$ as the sequence $\\{Z_0, Z_1, Z_2, \\dots\\}$.\n\nConstruct the following three test cases, each yielding a series $\\{p_t\\}_{t=1}^{N}$ after discarding the first $B$ samples generated by the given recurrence. In all cases, the innovation noise is $\\epsilon_t$ as defined above, and the initial conditions for any required lagged values are set to zero.\n- Case A (long correlation, “happy path”): Autoregressive order-$1$ recurrence\n$$\np_t = \\phi\\, p_{t-1} + \\epsilon_t,\n$$\nwith parameters $N \\equiv 2048$, $B \\equiv 1024$, $\\phi \\equiv 0.95$, seed $X_0 \\equiv 1$.\n- Case B (boundary, nearly uncorrelated): Autoregressive order-$1$ recurrence\n$$\np_t = \\phi\\, p_{t-1} + \\epsilon_t,\n$$\nwith parameters $N \\equiv 2048$, $B \\equiv 0$, $\\phi \\equiv 0.0$, seed $X_0 \\equiv 2$.\n- Case C (edge case, oscillatory correlations): Autoregressive order-$2$ recurrence\n$$\np_t = 2 r \\cos(\\omega)\\, p_{t-1} - r^2\\, p_{t-2} + \\epsilon_t,\n$$\nwith parameters $N \\equiv 2048$, $B \\equiv 1024$, $r \\equiv 0.9$, $\\omega \\equiv 0.2$ (radians), seed $X_0 \\equiv 3$.\n\nCompute, for each case, the normalized sample autocorrelation function $\\rho(k)$, the integrated autocorrelation time $\\tau_{\\mathrm{int}}$ using the truncation rule above, and the corresponding effective sample size $N_{\\mathrm{eff}}$. Report, for each case, the pair $\\left[\\tau_{\\mathrm{int}}, N_{\\mathrm{eff}}\\right]$ rounded to $4$ decimal places.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list of three bracketed pairs in the order A, B, C, enclosed in square brackets. For example, a syntactically valid output line must look like\n$$\n\\bigl[[\\tau_A, N_{\\mathrm{eff},A}],[\\tau_B, N_{\\mathrm{eff},B}],[\\tau_C, N_{\\mathrm{eff},C}]\\bigr],\n$$\nwhere each numeric entry is rounded to $4$ decimal places and expressed as a decimal (not a fraction). No additional text should be printed.", "solution": "The central objective is to estimate time correlations and the statistical inefficiency of correlated samples from first principles, starting from the definitions of sample moments and correlations. For a discrete stationary series $\\{p_t\\}_{t=1}^{N}$, the sample mean is $\\bar{p} \\equiv \\frac{1}{N}\\sum_{t=1}^{N} p_t$. The lag-$k$ sample autocovariance is defined by\n$$\nC(k) \\equiv \\frac{1}{N-k}\\sum_{t=1}^{N-k} \\left(p_t-\\bar{p}\\right)\\left(p_{t+k}-\\bar{p}\\right),\n$$\nwith $C(0)$ equal to the sample variance. The normalized autocorrelation function is\n$$\n\\rho(k) \\equiv \\frac{C(k)}{C(0)}.\n$$\nThe integrated autocorrelation time is defined by\n$$\n\\tau_{\\mathrm{int}} \\equiv 1 + 2 \\sum_{k=1}^{M} \\rho(k),\n$$\nwhere $M$ is a truncation index that must be specified to obtain a finite-sample estimate. The adopted definition selects $M$ as the largest nonnegative integer such that all $\\rho(k)$ for $k \\in \\{1,\\dots,M\\}$ are strictly positive, i.e., $M$ is one less than the first lag at which $\\rho(k)$ becomes nonpositive; if no such lag exists, set $M \\equiv N-1$. This choice ensures the summation includes only the reliably positive portion of the empirical correlation function, preventing spurious cancellation by noisy negative estimates at long lag. The statistical inefficiency is $g \\equiv \\tau_{\\mathrm{int}}$, and the effective number of uncorrelated samples is\n$$\nN_{\\mathrm{eff}} \\equiv \\frac{N}{g}.\n$$\n\nSynthetic virial pressure series are generated by linear stochastic recurrences driven by a deterministic pseudo-random “thermal noise” to ensure reproducibility. The noise $\\epsilon_t$ is constructed from a Linear Congruential Generator (LCG) with modulus $m \\equiv 2^{64}$, multiplier $a \\equiv 6364136223846793005$, increment $c \\equiv 1442695040888963407$, and case-specific seed $X_0$. Uniform deviates are $U_n \\equiv X_n/m$. Pairs of uniforms produce independent standard normal variates via the Box-Muller transform:\n$$\nZ_{2j} \\equiv \\sqrt{-2 \\ln U_{2j}}\\ \\cos(2\\pi U_{2j+1}), \\quad\nZ_{2j+1} \\equiv \\sqrt{-2 \\ln U_{2j}}\\ \\sin(2\\pi U_{2j+1}),\n$$\nand $\\epsilon_t$ is the sequence $\\{Z_0, Z_1, \\dots\\}$. The series are then defined by:\n- Case A: $p_t = \\phi p_{t-1} + \\epsilon_t$ with $\\phi \\equiv 0.95$, and the first $B \\equiv 1024$ transient samples discarded.\n- Case B: $p_t = \\phi p_{t-1} + \\epsilon_t$ with $\\phi \\equiv 0.0$ (white noise), $B \\equiv 0$.\n- Case C: $p_t = 2 r \\cos(\\omega) p_{t-1} - r^2 p_{t-2} + \\epsilon_t$ with $r \\equiv 0.9$, $\\omega \\equiv 0.2$ radians, discarding $B \\equiv 1024$.\n\nFor each case, after generating $N+B$ samples and discarding the first $B$, we obtain the length-$N$ stationary series. To compute the empirical autocovariance efficiently and exactly with the provided definition, we subtract the mean to form $x_t \\equiv p_t - \\bar{p}$, compute the linear (non-circular) autocorrelation via convolution of $x_t$ with itself, and divide each lag by the number of contributing terms $(N-k)$ to match $C(k)$:\n$$\nS(k) \\equiv \\sum_{t=1}^{N-k} x_t x_{t+k}, \\quad C(k) \\equiv \\frac{S(k)}{N-k}, \\quad \\rho(k) \\equiv \\frac{C(k)}{C(0)}.\n$$\nNumerically, $S(k)$ can be obtained by the inverse discrete Fourier transform of the power spectrum of zero-mean data with zero padding to avoid circular wrap-around; the unbiased normalization by $(N-k)$ is then applied. The truncation index $M$ is found as $M \\equiv \\max\\{m \\ge 0 : \\rho(k)  0 \\ \\forall k \\in \\{1,\\dots,m\\}\\}$, i.e., $M$ is one less than the first nonpositive lag; if there is no nonpositive lag, take $M \\equiv N-1$. Finally,\n$$\n\\tau_{\\mathrm{int}} \\equiv 1 + 2 \\sum_{k=1}^{M} \\rho(k), \\quad g \\equiv \\tau_{\\mathrm{int}}, \\quad N_{\\mathrm{eff}} \\equiv \\frac{N}{g}.\n$$\n\nQualitatively, for Case A with $\\phi \\equiv 0.95$, the theoretical normalized autocorrelation of an autoregressive order-$1$ process decays geometrically as $\\rho(k) \\approx \\phi^k$, giving an infinite-sample integrated autocorrelation time $\\tau_{\\mathrm{int}} \\approx 1 + \\frac{2\\phi}{1-\\phi}$, which is large when $\\phi$ is close to $1$, implying a small $N_{\\mathrm{eff}}$. For Case B with $\\phi \\equiv 0.0$, $\\rho(k)$ fluctuates around zero for $k \\ge 1$, typically yielding $M \\equiv 0$ and thus $\\tau_{\\mathrm{int}} \\approx 1$ and $N_{\\mathrm{eff}} \\approx N$. For Case C, the autoregressive order-$2$ model with complex-conjugate roots produces a damped oscillatory autocorrelation, which crosses zero at a moderate lag; therefore, $M$ is finite and $\\tau_{\\mathrm{int}}$ is intermediate, leading to $N_{\\mathrm{eff}}$ that is smaller than $N$ but much larger than in Case A.\n\nThe program implements the deterministic noise generator, constructs each time series with its specified parameters and burn-in, computes $C(k)$, $\\rho(k)$, $\\tau_{\\mathrm{int}}$, and $N_{\\mathrm{eff}}$ according to the definitions, rounds each numeric output to $4$ decimal places, and prints a single line containing\n$$\n\\bigl[[\\tau_A, N_{\\mathrm{eff},A}],[\\tau_B, N_{\\mathrm{eff},B}],[\\tau_C, N_{\\mathrm{eff},C}]\\bigr].\n$$", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\n# Deterministic Linear Congruential Generator (LCG) parameters\nLCG_M = 1  64\nLCG_A = 6364136223846793005\nLCG_C = 1442695040888963407\n\ndef lcg_uniforms(seed: int, count: int):\n    \"\"\"\n    Generate 'count' uniforms in [0,1) using a 64-bit LCG with given seed.\n    Returns a list of floats.\n    \"\"\"\n    x = seed  (LCG_M - 1)\n    us = []\n    for _ in range(count):\n        x = (LCG_A * x + LCG_C)  (LCG_M - 1)\n        u = x / LCG_M\n        us.append(u)\n    return us\n\ndef box_muller_normals(seed: int, n: int):\n    \"\"\"\n    Generate n standard normal variates using Box-Muller from the LCG uniforms.\n    Seed is for the LCG.\n    \"\"\"\n    # Need 2 * ceil(n/2) uniforms\n    pairs = (n + 1) // 2\n    u = lcg_uniforms(seed, 2 * pairs)\n    normals = []\n    for j in range(pairs):\n        u1 = u[2 * j]\n        u2 = u[2 * j + 1]\n        # Guard against log(0)\n        if u1 == 0.0:\n            u1 = 1.0 / LCG_M\n        r = math.sqrt(-2.0 * math.log(u1))\n        theta = 2.0 * math.pi * u2\n        z0 = r * math.cos(theta)\n        z1 = r * math.sin(theta)\n        normals.append(z0)\n        normals.append(z1)\n    return np.array(normals[:n], dtype=float)\n\ndef generate_ar1(phi: float, N: int, burn: int, seed: int):\n    \"\"\"\n    Generate AR(1): p_t = phi * p_{t-1} + epsilon_t\n    epsilon_t ~ N(0,1), via deterministic Box-Muller using LCG with given seed.\n    Start from p_0 = 0. Discard burn-in samples and return last N.\n    \"\"\"\n    total = N + burn\n    eps = box_muller_normals(seed, total)\n    p_prev = 0.0\n    series = np.empty(total, dtype=float)\n    for t in range(total):\n        p_t = phi * p_prev + eps[t]\n        series[t] = p_t\n        p_prev = p_t\n    if burn > 0:\n        series = series[burn:]\n    return series\n\ndef generate_ar2_osc(r: float, omega: float, N: int, burn: int, seed: int):\n    \"\"\"\n    Generate AR(2) with complex-conjugate roots r * exp(± i omega):\n      p_t = 2 r cos(omega) p_{t-1} - r^2 p_{t-2} + epsilon_t\n    epsilon_t ~ N(0,1), via deterministic Box-Muller using LCG with given seed.\n    Start with p_{-1} = 0, p_0 = 0. Discard burn-in and return last N.\n    \"\"\"\n    total = N + burn\n    eps = box_muller_normals(seed, total)\n    a1 = 2.0 * r * math.cos(omega)\n    a2 = - (r ** 2)\n    p_im2 = 0.0  # p_{t-2}\n    p_im1 = 0.0  # p_{t-1}\n    series = np.empty(total, dtype=float)\n    for t in range(total):\n        p_t = a1 * p_im1 + a2 * p_im2 + eps[t]\n        series[t] = p_t\n        p_im2, p_im1 = p_im1, p_t\n    if burn > 0:\n        series = series[burn:]\n    return series\n\ndef unbiased_acf(x: np.ndarray):\n    \"\"\"\n    Compute unbiased autocovariance and normalized autocorrelation rho(k)\n    for k = 0..N-1 using FFT-based linear autocorrelation.\n    Returns rho as a 1D numpy array of length N.\n    \"\"\"\n    N = x.shape[0]\n    x_centered = x - np.mean(x)\n    # Next power of two for zero-padding to avoid circular correlation\n    nfft = 1  (2 * N - 1).bit_length()\n    # Real FFT\n    f = np.fft.rfft(x_centered, n=nfft)\n    # Power spectrum and inverse FFT to get linear autocorrelation sequence\n    acf_full = np.fft.irfft(f * np.conj(f), n=nfft).real[:N]\n    # Unbiased normalization\n    counts = np.arange(N, 0, -1)  # N, N-1, ..., 1\n    cov_unbiased = acf_full / counts\n    var0 = cov_unbiased[0]\n    if var0 = 0:\n        # Degenerate case: constant series or numerical issue; return zeros except rho(0)=1\n        rho = np.zeros(N, dtype=float)\n        rho[0] = 1.0\n        return rho\n    rho = cov_unbiased / var0\n    return rho\n\ndef integrated_autocorrelation_time(rho: np.ndarray):\n    \"\"\"\n    Compute tau_int = 1 + 2 sum_{k=1}^M rho(k),\n    where M is one less than the first nonpositive lag; if none, M = N-1.\n    \"\"\"\n    N = rho.shape[0]\n    # Find first k >= 1 such that rho[k] = 0\n    nonpos = np.where(rho[1:] = 0.0)[0]\n    if nonpos.size > 0:\n        first_nonpos_k = int(nonpos[0]) + 1  # adjust for slicing offset\n        M = first_nonpos_k - 1\n    else:\n        M = N - 1\n    if M = 0:\n        return 1.0  # sum is empty, tau_int = 1\n    s = float(np.sum(rho[1:M+1]))\n    tau_int = 1.0 + 2.0 * s\n    # Ensure non-negative due to possible tiny numerical noise\n    if tau_int  0.0:\n        tau_int = 0.0\n    return tau_int\n\ndef analyze_series(p: np.ndarray):\n    \"\"\"\n    Given a series p, compute rho, tau_int, and N_eff with the specified definitions.\n    \"\"\"\n    N = p.shape[0]\n    rho = unbiased_acf(p)\n    tau = integrated_autocorrelation_time(rho)\n    g = tau\n    # Protect against division by zero\n    if g = 0:\n        neff = float('inf')\n    else:\n        neff = N / g\n    return tau, neff\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Case A: AR(1) with phi = 0.95, N=2048, B=1024, seed=1\n    N_A, B_A, phi_A, seed_A = 2048, 1024, 0.95, 1\n    series_A = generate_ar1(phi_A, N_A, B_A, seed_A)\n\n    # Case B: AR(1) with phi = 0.0, N=2048, B=0, seed=2\n    N_B, B_B, phi_B, seed_B = 2048, 0, 0.0, 2\n    series_B = generate_ar1(phi_B, N_B, B_B, seed_B)\n\n    # Case C: AR(2) with r=0.9, omega=0.2, N=2048, B=1024, seed=3\n    N_C, B_C, r_C, omega_C, seed_C = 2048, 1024, 0.9, 0.2, 3\n    series_C = generate_ar2_osc(r_C, omega_C, N_C, B_C, seed_C)\n\n    results = []\n    for series in (series_A, series_B, series_C):\n        tau, neff = analyze_series(series)\n        # Round to 4 decimal places\n        tau_r = f\"{tau:.4f}\"\n        neff_r = f\"{neff:.4f}\"\n        results.append(f\"[{tau_r},{neff_r}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2442361"}, {"introduction": "Directly summing the autocorrelation function can be numerically unstable, especially for long-tailed correlations. The blocking method offers a more robust and widely used approach to estimate the statistical error of a sample mean. This practice guides you through implementing the blocking algorithm and using its output not just for error estimation, but as a powerful diagnostic tool to quantitatively assess whether a simulation has reached equilibrium—a critical step in any serious computational study. [@problem_id:2442379]", "problem": "You are given the task of implementing the blocking (also called binning) method to estimate the standard error of the sample mean from correlated data produced by a Markov Chain Monte Carlo (MCMC) simulation and to design a quantitative criterion, based solely on the blocking curve, to detect lack of equilibrium (non-stationarity). The fundamental base you may assume includes: the definition of the autocovariance function of a stationary time series, the definition of the integrated autocorrelation time, the fact that the variance of the sample mean of independent and identically distributed (i.i.d.) data is the population variance divided by the sample size, and the empirical observation that the blocking method estimates the variance of the sample mean by treating block averages as approximately independent when the block size is larger than the autocorrelation time. You must not assume any shortcut formulas beyond these core definitions.\n\nDefinitions to use as a starting point:\n- Let $\\{X_t\\}_{t=1}^N$ be a scalar time series. For a stationary process with mean $\\mu$ and autocovariance function $\\gamma(k)=\\mathbb{E}[(X_t-\\mu)(X_{t+k}-\\mu)]$, the statistical inefficiency is $g=1+2\\sum_{k=1}^{\\infty}\\rho(k)$ where $\\rho(k)=\\gamma(k)/\\gamma(0)$ is the autocorrelation function. Note: In this problem, we will use the term integrated autocorrelation time, $\\tau_{\\mathrm{int}}$, to refer to the statistical inefficiency ($g$). With this convention, the variance of the sample mean $\\bar{X}=\\frac{1}{N}\\sum_{t=1}^N X_t$ behaves as $\\mathrm{Var}(\\bar{X})\\approx \\frac{\\sigma^2\\tau_{\\mathrm{int}}}{N}$ where $\\sigma^2=\\gamma(0)$.\n- The blocking method partitions the data into $m$ non-overlapping consecutive blocks of equal size $b$ (so that $m=\\lfloor N/b\\rfloor$) and computes the block averages. For sufficiently large $b$, the block averages are approximately uncorrelated and their sample variance, divided by $m$, estimates the variance of the overall mean.\n\nYour program must:\n1) Construct synthetic MCMC-like time series according to the following rules. All chains are real-valued autoregressive processes of order one (AR(1)) with parameter $\\phi$ and time-dependent mean $\\mu_t$ given by the recursion $X_{t+1}=\\phi X_t + (1-\\phi)\\mu_t + \\eta_{t+1}$ where $\\eta_{t}$ are independent Gaussian random variables with zero mean and variance chosen such that the stationary variance (when $\\mu_t$ is constant and $|\\phi|1$) is approximately unity. Explicitly, you must use $\\mathrm{Var}(\\eta_t)=1-\\phi^2$ so that for constant $\\mu_t$, the process has stationary variance close to $1$.\n2) For a given series $\\{X_t\\}_{t=1}^N$, implement the blocking estimator as follows. For block sizes $b=2^k$ with $k\\in\\{0,1,2,\\dots\\}$, include only those $b$ such that the number of full blocks $m=\\lfloor N/b\\rfloor$ is at least $8$. For each such $b$, compute the $m$ consecutive block averages and then compute the unbiased sample variance of the block averages. The estimated variance of the overall mean at block size $b$ is then the sample variance of the block averages divided by $m$. The corresponding standard error is the square root of that variance. This produces a sequence of standard error estimates as a function of $b$ (the blocking curve).\n3) Devise a quantitative criterion, based exclusively on the shape of the blocking curve at the largest accessible block sizes, to decide whether the chain has reached equilibrium. You must formalize the criterion using precise, testable inequalities. For this problem, define a “plateau” at the end of the blocking curve as follows: take the last $K$ valid block sizes (use $K=4$, or all available if fewer than $4$ exist), compute the median of their standard error estimates and the relative range, defined as $(\\max-\\min)/\\text{median}$ over those $K$ points. Declare the chain equilibrated if and only if the relative range over these tail points is less than or equal to a threshold $R_\\star$, with $R_\\star=0.15$, and there are at least $3$ tail points. Otherwise, declare it non-equilibrated.\n4) Using the plateau standard error estimate (taken as the median over the last $K$ block sizes), together with the sample variance $s^2$ of the raw series, construct an estimator of the integrated autocorrelation time $\\tau_{\\mathrm{int}}$ by comparing the plateau variance of the mean to the i.i.d. variance-of-the-mean baseline. Your estimator must be a function of $N$, the plateau standard error, and $s^2$ only, and must be consistent with the fundamental definitions above.\n\nTest suite. You must run your code on the following four test cases, each defined by $(N,\\phi,\\mu_t,\\text{seed},X_1)$:\n- Case A (stationary, long chain): $N=65536$, $\\phi=0.9$, constant mean $\\mu_t\\equiv 0$, seed $=101$, initial value $X_1=0$.\n- Case B (slow relaxation, short chain, far-from-equilibrium start): $N=8192$, $\\phi=0.9995$, constant mean $\\mu_t\\equiv 0$, seed $=202$, initial value $X_1=20$.\n- Case C (stationary, short chain): $N=2048$, $\\phi=0.5$, constant mean $\\mu_t\\equiv 0$, seed $=303$, initial value $X_1=0$.\n- Case D (piecewise-constant mean shift): $N=65536$, $\\phi=0.9$, mean $\\mu_t=0$ for $t\\le N/2$ and $\\mu_t=1$ for $t N/2$, seed $=404$, initial value $X_1=0$.\n\nFor each case, your program must output:\n- A boolean indicating whether the chain is declared equilibrated by your plateau criterion.\n- A floating-point estimate of $\\tau_{\\mathrm{int}}$ constructed from the plateau standard error and the raw sample variance as specified in item $4$.\n- A floating-point plateau standard error estimate as specified in item $3$.\n\nNumerical reporting requirements:\n- All floating-point outputs must be rounded to six decimal places.\n- There are no physical or angular units in this problem.\n- The final output must be a single line containing a JSON array of four elements, one per test case, where each element is a JSON array of the form $[\\text{equilibrated}, \\tau_{\\mathrm{int}}, \\text{SE}]$, with the boolean as a JSON boolean and the two floating-point numbers as decimals rounded to six places. For example, a valid output format is $[[\\text{true},1.234000,0.056000],[\\dots],\\dots]$.\n\nYour program should produce a single line of output containing the results as a JSON array of arrays, in the format described above, with no additional text. The program must be completely deterministic given the seeds above and must not require any user input.", "solution": "The problem is well-posed, scientifically sound, and provides a complete, self-contained specification for a computational task. It is therefore valid. I will proceed with the solution.\n\nThe task requires the implementation of the blocking method to analyze correlated time series data, specifically to estimate the standard error of the sample mean and to devise a criterion for detecting non-stationarity. Furthermore, an estimator for the integrated autocorrelation time must be derived and implemented.\n\nFirst, we address the generation of synthetic time series. The problem specifies an autoregressive process of order one, or AR(1), defined by the recurrence relation:\n$$X_{t+1} = \\phi X_t + (1-\\phi)\\mu_t + \\eta_{t+1}$$\nHere, $\\{X_t\\}_{t=1}^N$ is the time series of length $N$, $\\phi$ is the autoregressive parameter, $\\mu_t$ is a potentially time-dependent mean, and $\\eta_{t+1}$ are independent and identically distributed random variables drawn from a Gaussian distribution with mean zero, $\\eta_t \\sim \\mathcal{N}(0, \\sigma_\\eta^2)$. The variance of the noise is specified as $\\sigma_\\eta^2 = 1-\\phi^2$. This choice is deliberate; for a stationary process where $\\mu_t$ is a constant $\\mu$ and $|\\phi|1$, the centered process $Y_t = X_t - \\mu$ follows $Y_t = \\phi Y_{t-1} + \\eta_t$. Its stationary variance is $\\mathrm{Var}(Y_t) = \\mathrm{Var}(\\eta_t) / (1-\\phi^2)$. With our choice of $\\mathrm{Var}(\\eta_t) = 1-\\phi^2$, the stationary variance of the process becomes $\\sigma^2 = \\mathrm{Var}(X_t) = 1$. This normalizes the intrinsic scale of the fluctuations.\n\nNext, we implement the blocking method. For a stationary time series $\\{X_t\\}_{t=1}^N$, the sample mean is $\\bar{X} = \\frac{1}{N}\\sum_{t=1}^N X_t$. The variance of this sample mean is not simply $\\sigma^2/N$ as for independent data, but is amplified by correlations. The correct asymptotic formula is:\n$$\\mathrm{Var}(\\bar{X}) \\approx \\frac{\\sigma^2 \\tau_{\\mathrm{int}}}{N}$$\nwhere $\\sigma^2$ is the process variance and $\\tau_{\\mathrm{int}}$ is the integrated autocorrelation time (used here to mean the statistical inefficiency, as defined in the problem). The blocking method provides an estimate of $\\mathrm{Var}(\\bar{X})$ directly. The data is partitioned into $m = \\lfloor N/b \\rfloor$ non-overlapping blocks of size $b$. For each block $j \\in \\{1, \\dots, m\\}$, we compute the block average:\n$$\\bar{X}_j^{(b)} = \\frac{1}{b} \\sum_{i=1}^{b} X_{(j-1)b+i}$$\nWhen the block size $b$ is significantly larger than $\\tau_{\\mathrm{int}}$, the block averages $\\{\\bar{X}_j^{(b)}\\}_{j=1}^m$ become approximately independent and identically distributed. We can thus treat them as a set of $m$ independent samples. The variance of the mean of these samples, which is an estimator for $\\mathrm{Var}(\\bar{X})$, is given by the standard formula for the variance of a sample mean:\n$$V_b = \\frac{s^2_{\\text{blocks}}(b)}{m}$$\nwhere $s^2_{\\text{blocks}}(b)$ is the unbiased sample variance of the block averages:\n$$s^2_{\\text{blocks}}(b) = \\frac{1}{m-1} \\sum_{j=1}^{m} \\left(\\bar{X}_j^{(b)} - \\left(\\frac{1}{m}\\sum_{l=1}^m \\bar{X}_l^{(b)}\\right)\\right)^2$$\nThe standard error of the mean for block size $b$ is then $SE_b = \\sqrt{V_b}$. This procedure is performed for block sizes $b=2^k$ for $k=0, 1, 2, \\dots$, so long as the number of blocks $m$ is at least $8$. This generates the \"blocking curve\", a plot of $SE_b$ versus $b$.\n\nThe shape of the blocking curve is used to diagnose non-stationarity. For a stationary process, as $b$ increases, $SE_b$ initially rises (as short-range correlations are averaged out) and then converges to a plateau when $b \\gg \\tau_{\\mathrm{int}}$. For a non-stationary process, such as one with a drift or trend, the variance between blocks continues to increase with $b$, and $SE_b$ does not plateau. We formalize a plateau criterion as follows:\n1.  Consider the last $K$ valid block sizes, where $K$ is the minimum of $4$ and the total number of valid block sizes.\n2.  Let the corresponding standard error estimates be $\\{S_1, S_2, \\dots, S_K\\}$.\n3.  Compute the median, $S_{\\text{median}} = \\text{median}(S_1, \\dots, S_K)$.\n4.  Compute the relative range, $R = \\frac{\\max(S_1, \\dots, S_K) - \\min(S_1, \\dots, S_K)}{S_{\\text{median}}}$. A check for $S_{\\text{median}}0$ is prudent.\n5.  The process is declared \"equilibrated\" (stationary) if and only if $R \\le R_\\star = 0.15$ and the number of tail points $K$ is at least $3$.\nThe plateau standard error estimate, $SE_{\\text{plateau}}$, is defined as $S_{\\text{median}}$ regardless of whether the equilibrium criterion is met.\n\nFinally, we construct an estimator for the integrated autocorrelation time, $\\hat{\\tau}_{\\mathrm{int}}$. We start from the fundamental relation $\\mathrm{Var}(\\bar{X}) \\approx \\frac{\\sigma^2 \\tau_{\\mathrm{int}}}{N}$. We have two estimators from our data:\n- An estimate of $\\mathrm{Var}(\\bar{X})$ from the blocking method's plateau: $(SE_{\\text{plateau}})^2$.\n- An estimate of the process variance $\\sigma^2$ from the raw data's sample variance: $s^2 = \\frac{1}{N-1}\\sum_{t=1}^N (X_t - \\bar{X})^2$.\nSubstituting these into the relation gives:\n$$(SE_{\\text{plateau}})^2 \\approx \\frac{s^2 \\hat{\\tau}_{\\mathrm{int}}}{N}$$\nSolving for our estimator $\\hat{\\tau}_{\\mathrm{int}}$ yields:\n$$\\hat{\\tau}_{\\mathrm{int}} = \\frac{N \\cdot (SE_{\\text{plateau}})^2}{s^2}$$\nThis formula connects the macroscopic estimate of the mean's variance with the microscopic sample variance to extract the statistical inefficiency factor $\\tau_{\\mathrm{int}}$.\n\nThe overall algorithm proceeds by generating the time series for each test case, applying the blocking analysis to obtain the blocking curve, using the tail of this curve to test for equilibrium and find $SE_{\\text{plateau}}$, and finally combining this with the raw sample variance to compute $\\hat{\\tau}_{\\mathrm{int}}$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the blocking method to estimate standard error and autocorrelation time\n    for AR(1) processes and applies a quantitative criterion for non-stationarity.\n    \"\"\"\n\n    def generate_ar1_series(N, phi, mu_func, seed, X1):\n        \"\"\"Generates an AR(1) time series.\"\"\"\n        rng = np.random.default_rng(seed)\n        noise_std = np.sqrt(1 - phi**2) if (1 - phi**2) > 0 else 0.0\n        # Generate all noise values at once for efficiency\n        eta = rng.normal(loc=0.0, scale=noise_std, size=N - 1)\n        \n        X = np.zeros(N)\n        X[0] = X1\n        \n        for t in range(N - 1):\n            # mu_t influences the state at t+1\n            mu_t_val = mu_func(t)\n            X[t+1] = phi * X[t] + (1 - phi) * mu_t_val + eta[t]\n            \n        return X\n\n    test_cases = [\n        # Case A: Stationary, long chain\n        {'N': 65536, 'phi': 0.9, 'mu_func': lambda t: 0.0, 'seed': 101, 'X1': 0.0},\n        # Case B: Slow relaxation, far-from-equilibrium start\n        {'N': 8192, 'phi': 0.9995, 'mu_func': lambda t: 0.0, 'seed': 202, 'X1': 20.0},\n        # Case C: Stationary, short chain\n        {'N': 2048, 'phi': 0.5, 'mu_func': lambda t: 0.0, 'seed': 303, 'X1': 0.0},\n        # Case D: Piecewise-constant mean shift (non-stationary)\n        {'N': 65536, 'phi': 0.9, 'mu_func': lambda t, N_val=65536: 1.0 if (t + 1) > N_val / 2 else 0.0, 'seed': 404, 'X1': 0.0}\n    ]\n\n    results = []\n\n    for case in test_cases:\n        N = case['N']\n        phi = case['phi']\n        mu_func = case['mu_func']\n        seed = case['seed']\n        X1 = case['X1']\n\n        # 1. Generate time series\n        series = generate_ar1_series(N, phi, mu_func, seed, X1)\n        \n        # 2. Implement blocking estimator\n        se_values = []\n        k = 0\n        while True:\n            b = 2**k\n            m = N // b\n            if m  8:\n                break\n            \n            # Truncate data to fit full blocks\n            data_to_block = series[:m*b]\n            # Reshape into m blocks of size b\n            blocks = data_to_block.reshape((m, b))\n            # Compute block averages\n            block_averages = blocks.mean(axis=1)\n            \n            # Unbiased sample variance of block averages\n            var_of_block_averages = np.var(block_averages, ddof=1)\n            \n            # Estimated variance of the overall mean\n            var_of_mean = var_of_block_averages / m\n            se = np.sqrt(var_of_mean)\n            se_values.append(se)\n            \n            k += 1\n\n        # 3. Apply equilibrium criterion\n        num_points = len(se_values)\n        K = min(4, num_points)\n        \n        is_equilibrated = False\n        se_plateau = 0.0\n        \n        if K > 0:\n            tail_points = np.array(se_values[-K:])\n            se_plateau = np.median(tail_points)\n\n            if K >= 3:\n                R_star = 0.15\n                se_min = np.min(tail_points)\n                se_max = np.max(tail_points)\n                \n                relative_range = 0.0\n                if se_plateau > 0:\n                    relative_range = (se_max - se_min) / se_plateau\n                \n                if relative_range = R_star:\n                    is_equilibrated = True\n\n        # 4. Estimate tau_int\n        raw_sample_var = np.var(series, ddof=1)\n        tau_int = 0.0\n        if raw_sample_var > 0:\n            tau_int = (N * se_plateau**2) / raw_sample_var\n        \n        results.append(f\"[{str(is_equilibrated).lower()},{tau_int:.6f},{se_plateau:.6f}]\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2442379"}]}