{"hands_on_practices": [{"introduction": "Percolation theory is not limited to discrete lattices; it beautifully describes connectivity in continuous space, with applications from porous media to wireless networks. This first exercise challenges you to simulate continuum percolation by randomly placing overlapping disks and to pinpoint the critical area density, $\\eta_c$, where a system-spanning cluster first emerges. This practice builds a complete simulation from the ground up, teaching you how to model random geometric systems and numerically locate their critical points using Monte Carlo methods, efficient data structures, and a bisection search—a fundamental skillset in computational physics. [@problem_id:2426247]", "problem": "Write a complete program that estimates the critical area density in continuum percolation for overlapping disks using first-principles simulation and principled statistical inference. Model a two-dimensional Boolean model on a square domain of side length $L$, where disk centers are distributed as a spatial Poisson point process of intensity $\\rho$ (points per unit area), and each disk has the same radius $r$. Two disks are considered connected if and only if the Euclidean distance between their centers is less than or equal to $2r$. The system forms an undirected graph whose connected components are the percolation clusters. Define the dimensionless area density as $\\eta = \\rho \\pi r^2$. A spanning event (left-right) occurs if there exists at least one connected component that intersects both the left boundary ($x \\le r$) and the right boundary ($x \\ge L - r$) of the square.\n\nStarting from the core definitions of the spatial Poisson point process and geometric connectivity, design a Monte Carlo (MC) estimator for the percolation probability at a given $\\eta$, and then locate an estimate of the critical area density $\\eta_c$ where the spanning probability equals $0.5$ in a finite system. Use bisection on $\\eta$ over a bracket $[\\eta_{\\mathrm{lo}}, \\eta_{\\mathrm{hi}}]$ to find $\\eta_c$ as the midpoint of the final bracket after a fixed number of iterations. In each evaluation of the spanning probability at a fixed $\\eta$, generate an independent realization by drawing the number of disks $N$ from a Poisson distribution with mean $\\rho L^2$, where $\\rho = \\eta / (\\pi r^2)$, then placing $N$ disk centers independently and uniformly in $[0,L]\\times[0,L]$, and testing for left-right spanning using connectivity of overlapping disks.\n\nAlgorithmic requirements:\n- Use Disjoint Set Union (DSU) with path compression and union by rank to maintain connectivity of disks.\n- For neighbor search, you must avoid all-pairs distance checks. Use a spatial search method with sub-quadratic expected complexity (for example, a space-partitioning tree) to enumerate only pairs of disks whose distance is at most $2r$.\n- Treat boundary contact geometrically: a disk with center coordinate $x$ touches the left boundary if $x \\le r$, and touches the right boundary if $x \\ge L - r$.\n- Use a fixed random seed for each test case to ensure reproducibility.\n\nNumerical and statistical specifications:\n- All quantities are dimensionless; no physical units are required.\n- Angles are not involved in this task.\n- In each spanning-probability evaluation at a given $\\eta$, use exactly $T$ independent trials and estimate the spanning probability as the sample mean of the $T$ spanning indicators. Use this estimator inside the bisection decision rule.\n- After the prescribed number of bisection iterations $I$, report $\\eta_c$ as the midpoint of the final bracket.\n- To ensure numerical stability and runtime feasibility, you must implement the required neighbor search using an efficient spatial structure and the DSU-based connectivity.\n\nTest suite:\nRun your program for the following three test cases. For each case, output the estimated $\\eta_c$ rounded to three decimals.\n\n- Case $1$ (happy path, moderate $N$): $L = 1$, $r = 0.05$, seed $s = 12345$, number of bisection iterations $I = 8$, trials per evaluation $T = 128$, bracket $[\\eta_{\\mathrm{lo}}, \\eta_{\\mathrm{hi}}] = [0.7, 1.5]$.\n- Case $2$ (different scale, smaller $N$): $L = 1$, $r = 0.07$, seed $s = 54321$, number of bisection iterations $I = 8$, trials per evaluation $T = 128$, bracket $[\\eta_{\\mathrm{lo}}, \\eta_{\\mathrm{hi}}] = [0.7, 1.5]$.\n- Case $3$ (larger disks, stronger finite-size effects): $L = 1$, $r = 0.09$, seed $s = 999$, number of bisection iterations $I = 8$, trials per evaluation $T = 128$, bracket $[\\eta_{\\mathrm{lo}}, \\eta_{\\mathrm{hi}}] = [0.7, 1.5]$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each entry rounded to three decimals, in the same order as the test cases. For example, the output must look like\n$[x_1,x_2,x_3]$\nwhere each $x_j$ is the rounded estimate for the corresponding case, with no additional text before or after the line.\n\nConstraints:\n- You must implement the DSU connectivity and the spatial neighbor search as specified.\n- You must not read any input; the test cases are hard-coded.\n- The only allowed libraries are the Python standard library, NumPy, and SciPy, as specified in the execution environment.", "solution": "The user-provided problem statement is subjected to validation.\n\n### Step 1: Extract Givens\n- **Model**: Two-dimensional Boolean model on a square domain of side length $L$.\n- **Disk Centers**: Distributed as a spatial Poisson point process of intensity $\\rho$ (points per unit area).\n- **Disks**: All disks have a fixed radius $r$.\n- **Connectivity**: Two disks are connected if the Euclidean distance between their centers is less than or equal to $2r$.\n- **Dimensionless Area Density**: $\\eta = \\rho \\pi r^2$.\n- **Spanning Event**: A connected component of disks intersects both the left boundary ($x \\le r$) and the right boundary ($x \\ge L - r$).\n- **Objective**: Estimate the critical area density $\\eta_c$ where the spanning probability is $0.5$.\n- **Method for finding $\\eta_c$**: Bisection method on $\\eta$ over a given bracket $[\\eta_{\\mathrm{lo}}, \\eta_{\\mathrm{hi}}]$ for a fixed number of iterations $I$. The final estimate is the midpoint of the final bracket.\n- **Method for Spanning Probability Estimation**: A Monte Carlo estimator. For a given $\\eta$, perform $T$ independent trials. The estimate is the fraction of trials that exhibit a spanning event.\n- **Single Trial Generation**:\n    1. Calculate $\\rho = \\eta / (\\pi r^2)$.\n    2. Draw the number of disks $N$ from a Poisson distribution with mean $\\rho L^2$.\n    3. Place $N$ disk centers independently and uniformly in $[0,L]\\times[0,L]$.\n- **Algorithmic requirements**:\n    - Connectivity tracking must use Disjoint Set Union (DSU) with path compression and union by rank.\n    - Neighbor search must use a sub-quadratic method (e.g., space partitioning) to find pairs with distance $\\le 2r$. All-pairs checks are forbidden.\n- **Test Case 1**: $L = 1$, $r = 0.05$, seed $s = 12345$, iterations $I = 8$, trials $T = 128$, bracket $[\\eta_{\\mathrm{lo}}, \\eta_{\\mathrm{hi}}] = [0.7, 1.5]$.\n- **Test Case 2**: $L = 1$, $r = 0.07$, seed $s = 54321$, iterations $I = 8$, trials $T = 128$, bracket $[\\eta_{\\mathrm{lo}}, \\eta_{\\mathrm{hi}}] = [0.7, 1.5]$.\n- **Test Case 3**: $L = 1$, $r = 0.09$, seed $s = 999$, iterations $I = 8$, trials $T = 128$, bracket $[\\eta_{\\mathrm{lo}}, \\eta_{\\mathrm{hi}}] = [0.7, 1.5]$.\n- **Output**: A single line with a comma-separated list of $\\eta_c$ estimates for each case, rounded to three decimals, e.g., `[x_1,x_2,x_3]`.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the validation criteria.\n- **Scientifically Grounded**: The problem describes the continuum percolation of disks, also known as a Boolean model or a random geometric graph. This is a canonical model in statistical physics and probability theory, used to study connectivity in random systems. All definitions—spatial Poisson point process, dimensionless density $\\eta$, and the concept of a critical threshold $\\eta_c$—are standard and factually correct. The theoretical value of $\\eta_c$ for infinite systems of overlapping disks is a well-studied quantity, known to be approximately $1.12$. The problem is scientifically sound.\n- **Well-Posed**: The problem is specified with mathematical and algorithmic precision. It requests an *estimate* of $\\eta_c$ for a finite system using a fully defined numerical procedure: Monte Carlo simulation embedded within a bisection search. All parameters ($L, r, I, T$, seed, bracket) are provided for each test case. The endpoints of the numerical procedure (DSU, spatial search) are explicitly constrained. This structure guarantees that a unique numerical result can be obtained.\n- **Objective**: The problem is stated in the objective language of mathematics and computer science. There are no subjective, ambiguous, or opinion-based clauses.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. It is a standard, well-posed problem in computational statistical physics that is scientifically sound and objective. A solution will be provided.\n\n### Solution Design\n\nThe problem requires the estimation of the critical area density $\\eta_c$ for continuum percolation. The solution is constructed by implementing the specified numerical scheme, which combines a bisection search with a Monte Carlo estimator for the spanning probability.\n\n**1. Bisection Method for Locating $\\eta_c$**\nThe spanning probability, $P(\\eta)$, is a monotonically increasing function of the area density $\\eta$. The critical density $\\eta_c$ is defined as the value where $P(\\eta_c) = 0.5$. The bisection method is a robust root-finding algorithm for monotonic functions.\nWe start with a bracket $[\\eta_{\\mathrm{lo}}, \\eta_{\\mathrm{hi}}]$ that is known to contain $\\eta_c$. The algorithm proceeds for a fixed number of iterations, $I$:\n1. Compute the midpoint density $\\eta_{\\mathrm{mid}} = (\\eta_{\\mathrm{lo}} + \\eta_{\\mathrm{hi}}) / 2$.\n2. Estimate the spanning probability $P(\\eta_{\\mathrm{mid}})$ using a Monte Carlo simulation.\n3. If the estimated probability is greater than $0.5$, it implies $\\eta_{\\mathrm{mid}}$ is likely above the critical threshold. The upper bound is updated: $\\eta_{\\mathrm{hi}} = \\eta_{\\mathrm{mid}}$.\n4. If the estimated probability is less than or equal to $0.5$, $\\eta_{\\mathrm{mid}}$ is likely at or below the threshold. The lower bound is updated: $\\eta_{\\mathrm{lo}} = \\eta_{\\mathrm{mid}}$.\nAfter $I$ iterations, the estimate for $\\eta_c$ is the midpoint of the final, narrowed bracket: $(\\eta_{\\mathrm{lo}} + \\eta_{\\mathrm{hi}}) / 2$.\n\n**2. Monte Carlo Estimation of Spanning Probability**\nTo evaluate $P(\\eta)$ at a given density $\\eta$, we perform $T$ independent trials. For each trial:\n1. The physical density of disk centers $\\rho$ is calculated from the dimensionless density $\\eta$ as $\\rho = \\eta / (\\pi r^2)$.\n2. The domain is a square of area $L^2$. The expected number of disks is $\\lambda = \\rho L^2$. For a spatial Poisson point process, the number of points $N$ in a fixed domain follows a Poisson distribution. We draw $N$ from $\\text{Poisson}(\\lambda)$.\n3. We generate the positions of these $N$ disks by drawing their center coordinates $(x, y)$ independently and uniformly from the domain $[0,L] \\times [0,L]$.\n4. We then check if this realization of the system contains a \"spanning cluster\".\nThe estimate for $P(\\eta)$ is the number of spanning trials divided by the total number of trials $T$.\n\n**3. Spanning Cluster Identification**\nThis is the core of a single trial analysis. A spanning cluster is a connected component of disks that simultaneously touches the left ($x=0$) and right ($x=L$) boundaries of the domain.\n- **Connectivity**: We use a Disjoint Set Union (DSU) data structure, implemented with path compression and union by rank for optimal performance. Each of the $N$ disks is an element in the DSU. To handle boundaries, we introduce two virtual nodes: a \"left-boundary\" node and a \"right-boundary\" node.\n- **Boundary Conditions**: A disk with center $(x,y)$ and radius $r$ touches the left boundary if $x \\le r$ and the right boundary if $x \\ge L-r$. We iterate through all disks: if a disk touches the left boundary, we perform a union operation between that disk and the left-boundary node. Similarly for the right boundary.\n- **Neighbor Search**: To find which pairs of disks are connected (center distance $\\le 2r$), an all-pairs check ($O(N^2)$) is inefficient and forbidden. We implement a grid-based spatial search, which has an expected time complexity of $O(N)$ for uniformly distributed points.\n    1. The domain $[0,L]\\times[0,L]$ is partitioned into a grid of square cells. The side length of a cell must be at least $2r$. We choose it to be exactly $2r$.\n    2. Each disk is placed into a list corresponding to the cell its center falls in.\n    3. To find connections, we iterate through each disk $i$. Any disk $j$ that is connected to $i$ must have its center within a circle of radius $2r$ around the center of $i$. Due to our choice of cell size, the center of disk $j$ must lie either in the same cell as $i$ or in one of the 8 immediately adjacent cells.\n    4. To avoid redundant checks, for each cell, we check for connections between disks within that cell, and between disks in that cell and those in 4 of its 8 neighbors (e.g., right, bottom-left, bottom, bottom-right). For each identified pair of disks $(i, j)$ whose centers are separated by a distance $\\le 2r$, we perform `dsu.union(i, j)`.\n- **Spanning Check**: After all potential connections (disk-to-boundary and disk-to-disk) have been processed, a spanning cluster exists if and only if the left-boundary and right-boundary virtual nodes belong to the same connected component in the DSU. This is checked by `dsu.find(left_node) == dsu.find(right_node)`.\n\nThis completes the design. The implementation will follow this logic precisely, ensuring all constraints from the problem statement are met.", "answer": "```python\nimport numpy as np\n\nclass DSU:\n    \"\"\"Disjoint Set Union data structure with path compression and union by rank.\"\"\"\n    def __init__(self, n):\n        self.parent = np.arange(n)\n        self.rank = np.zeros(n, dtype=np.int32)\n\n    def find(self, i):\n        \"\"\"Finds the representative of the set containing element i with path compression.\"\"\"\n        if self.parent[i] == i:\n            return i\n        self.parent[i] = self.find(self.parent[i])\n        return self.parent[i]\n\n    def union(self, i, j):\n        \"\"\"Merges the sets containing elements i and j using union by rank.\"\"\"\n        root_i = self.find(i)\n        root_j = self.find(j)\n        if root_i != root_j:\n            if self.rank[root_i]  self.rank[root_j]:\n                self.parent[root_i] = root_j\n            elif self.rank[root_i]  self.rank[root_j]:\n                self.parent[root_j] = root_i\n            else:\n                self.parent[root_j] = root_i\n                self.rank[root_i] += 1\n            return True\n        return False\n\ndef has_spanning_cluster(points, L, r):\n    \"\"\"\n    Checks for a left-right spanning cluster in a single realization of disks.\n    Uses DSU for connectivity and a grid-based spatial search for neighbors.\n    \"\"\"\n    N = len(points)\n    if N == 0:\n        return False\n\n    # N disks (0 to N-1), plus two virtual nodes for left/right boundaries\n    left_boundary_node = N\n    right_boundary_node = N + 1\n    dsu = DSU(N + 2)\n\n    # Connect disks to virtual boundary nodes\n    for i in range(N):\n        x = points[i, 0]\n        if x = r:\n            dsu.union(i, left_boundary_node)\n        if x = L - r:\n            dsu.union(i, right_boundary_node)\n\n    # Early exit if no disks are near both boundaries initially\n    if dsu.find(left_boundary_node) == dsu.find(right_boundary_node):\n        return True\n\n    # Spatial search using a grid to find neighboring disks\n    # The connection distance is 2r, so cell size should be = 2r\n    cell_size = 2.0 * r\n    grid_dim = int(np.ceil(L / cell_size))\n    if grid_dim == 0: grid_dim = 1\n    \n    grid = [[] for _ in range(grid_dim * grid_dim)]\n    \n    for i in range(N):\n        x, y = points[i]\n        ix = min(int(x / cell_size), grid_dim - 1)\n        iy = min(int(y / cell_size), grid_dim - 1)\n        grid[iy * grid_dim + ix].append(i)\n\n    dist_sq_threshold = (2.0 * r)**2\n\n    for iy in range(grid_dim):\n        for ix in range(grid_dim):\n            current_cell_idx = iy * grid_dim + ix\n            points_in_cell = grid[current_cell_idx]\n            \n            # 1. Pairs WITHIN the current cell\n            for i in range(len(points_in_cell)):\n                for j in range(i + 1, len(points_in_cell)):\n                    p1_idx = points_in_cell[i]\n                    p2_idx = points_in_cell[j]\n                    dist_sq = np.sum((points[p1_idx] - points[p2_idx])**2)\n                    if dist_sq = dist_sq_threshold:\n                        dsu.union(p1_idx, p2_idx)\n\n            # 2. Pairs with neighboring cells (4 directions to avoid double counting)\n            neighbor_deltas = [(1, 0), (0, 1), (1, 1), (-1, 1)] # Right, Bottom, Bottom-Right, Bottom-Left\n            for dix, diy in neighbor_deltas:\n                nix, niy = ix + dix, iy + diy\n                if 0 = nix  grid_dim and 0 = niy  grid_dim:\n                    neighbor_cell_idx = niy * grid_dim + nix\n                    points_in_neighbor_cell = grid[neighbor_cell_idx]\n                    for p1_idx in points_in_cell:\n                        for p2_idx in points_in_neighbor_cell:\n                            dist_sq = np.sum((points[p1_idx] - points[p2_idx])**2)\n                            if dist_sq = dist_sq_threshold:\n                                dsu.union(p1_idx, p2_idx)\n\n    return dsu.find(left_boundary_node) == dsu.find(right_boundary_node)\n\ndef estimate_spanning_prob(eta, L, r, T, rng):\n    \"\"\"Estimates the spanning probability for a given area density eta using T Monte Carlo trials.\"\"\"\n    rho = eta / (np.pi * r**2)\n    lambda_N = rho * L**2\n    \n    spanning_count = 0\n    for _ in range(T):\n        N = rng.poisson(lambda_N)\n        if N == 0:\n            continue\n        \n        points = rng.uniform(0, L, size=(N, 2))\n        \n        if has_spanning_cluster(points, L, r):\n            spanning_count += 1\n            \n    return spanning_count / T\n\ndef solve_case(L, r, seed, I, T, eta_bracket):\n    \"\"\"\n    Finds the critical area density eta_c for one set of parameters using bisection.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    eta_lo, eta_hi = eta_bracket\n\n    for _ in range(I):\n        eta_mid = (eta_lo + eta_hi) / 2.0\n        p_span = estimate_spanning_prob(eta_mid, L, r, T, rng)\n        \n        if p_span  0.5:\n            eta_hi = eta_mid\n        else:\n            eta_lo = eta_mid\n            \n    return (eta_lo + eta_hi) / 2.0\n\ndef solve():\n    \"\"\"Main function to run all test cases and print results.\"\"\"\n    test_cases = [\n        # Case 1 (happy path, moderate N)\n        {'L': 1, 'r': 0.05, 'seed': 12345, 'I': 8, 'T': 128, 'eta_bracket': [0.7, 1.5]},\n        # Case 2 (different scale, smaller N)\n        {'L': 1, 'r': 0.07, 'seed': 54321, 'I': 8, 'T': 128, 'eta_bracket': [0.7, 1.5]},\n        # Case 3 (larger disks, stronger finite-size effects)\n        {'L': 1, 'r': 0.09, 'seed': 999, 'I': 8, 'T': 128, 'eta_bracket': [0.7, 1.5]},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = solve_case(\n            L=case['L'],\n            r=case['r'],\n            seed=case['seed'],\n            I=case['I'],\n            T=case['T'],\n            eta_bracket=case['eta_bracket']\n        )\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join([f'{res:.3f}' for res in results])}]\")\n\nsolve()\n```", "id": "2426247"}, {"introduction": "Having explored how to find a critical point, we now investigate the universal properties that emerge precisely *at* this threshold. A cornerstone of percolation theory is that the distribution of finite cluster sizes follows a power law, independent of many microscopic details of the system. This practice focuses on Fisher's law, which states that the number of clusters of size $s$, denoted $n_s$, scales as $n_s \\sim s^{-\\tau}$ at the critical probability $p_c$. By simulating percolation on a square lattice and fitting the resulting data, you will numerically estimate the universal critical exponent $\\tau$ using robust statistical methods, gaining hands-on experience with cluster-labeling algorithms and the principles of critical phenomena. [@problem_id:2426213]", "problem": "You will write a complete, runnable program that uses numerical simulation to study site percolation on a two-dimensional square lattice at the critical point. The goal is to determine the distribution of finite cluster sizes, denoted by $n_s$, at the percolation threshold $p_c$, and to verify that it follows Fisher’s law $n_s \\sim s^{-\\tau}$ by estimating the exponent $\\tau$ from simulated data.\n\nStart from the core definitions and facts of percolation theory and statistical estimation, without assuming any specialized percolation formulas beyond those definitions. The foundational bases you may use are:\n\n- The definition of site percolation on a square lattice: each site is independently occupied with probability $p$ and empty with probability $1-p$, where $p \\in [0,1]$ is a decimal number (not a percentage). Nearest-neighbor connectivity is defined along the lattice directions.\n- The definition of a cluster as a maximally connected set of occupied sites under nearest-neighbor connectivity on the lattice.\n- The percolation threshold $p_c$ for site percolation on the infinite two-dimensional square lattice is approximately $p_c \\approx 0.592746$, a widely accepted empirical constant derived from high-precision studies.\n- The notion of a spanning cluster on a finite $L \\times L$ system with free boundaries: a cluster that connects the top to the bottom or the left to the right boundary. Such clusters should be excluded when forming the distribution of finite cluster sizes at criticality.\n- Statistical estimation principles that allow fitting a power-law model to empirical data above a lower cutoff $s_{\\min}$ in a principled way, derived from likelihood maximization and asymptotic arguments, and acknowledging that cluster sizes are integer-valued.\n\nYour program must:\n\n1. Generate $R$ independent realizations of site percolation on an $L \\times L$ square lattice with free boundary conditions at occupation probability $p = p_c$, where $p_c = 0.592746$. In each realization, identify all clusters using only nearest-neighbor connectivity.\n2. Detect and exclude spanning clusters, defined as any cluster that touches both the top and bottom boundaries or both the left and right boundaries.\n3. Aggregate the sizes of all remaining (finite, non-spanning) clusters across the $R$ realizations into a multiset $\\{s_i\\}$.\n4. From the aggregated multiset, determine the empirical finite-size cluster-size distribution $n_s$ in the conceptual sense that $n_s$ is proportional to the frequency of clusters of size $s$ per site at $p = p_c$. You do not need to output $n_s$ itself; it serves as the basis for the estimation task in item $5$.\n5. Fit the tail of $n_s$ above a lower cutoff $s_{\\min}$ to a power law $n_s \\sim s^{-\\tau}$ by deriving and applying a statistically well-founded estimator for $\\tau$ based on first principles of likelihood for power-law tails. Use only data with $s \\ge s_{\\min}$ for the fit.\n6. Compare your estimate $\\hat{\\tau}$ to the theoretical two-dimensional value $\\tau_{\\mathrm{2D}} = \\frac{187}{91}$ and compute the absolute error $|\\hat{\\tau} - \\tau_{\\mathrm{2D}}|$. For verification, declare a case as passed if this absolute error is strictly less than a specified tolerance $\\epsilon$.\n\nYour implementation must use a logically sound and computationally efficient cluster-labeling algorithm suitable for large $L$ (for example, a single-pass labeling with dynamic equivalence resolution). The program must be self-contained, produce reproducible results given fixed seeds, and avoid any reliance on external input or files.\n\nTest Suite:\nUse the following three parameter sets. Each parameter set is a tuple $(L, R, p, s_{\\min}, \\text{seed}, \\epsilon)$ with all numbers given explicitly.\n\n- Case $1$: $(L, R, p, s_{\\min}, \\text{seed}, \\epsilon) = (\\,64,\\,200,\\,0.592746,\\,8,\\,12345,\\,0.35\\,)$.\n- Case $2$: $(L, R, p, s_{\\min}, \\text{seed}, \\epsilon) = (\\,96,\\,150,\\,0.592746,\\,10,\\,67890,\\,0.35\\,)$.\n- Case $3$: $(L, R, p, s_{\\min}, \\text{seed}, \\epsilon) = (\\,128,\\,120,\\,0.592746,\\,12,\\,424242,\\,0.35\\,)$.\n\nFor each case, your program must compute:\n- The estimated exponent $\\hat{\\tau}$ as a floating-point number.\n- The absolute error $|\\hat{\\tau} - \\frac{187}{91}|$ as a floating-point number.\n- A boolean indicating whether the case passes the verification criterion $|\\hat{\\tau} - \\frac{187}{91}|  \\epsilon$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results of all three cases aggregated into one list in the following order:\n$[\\hat{\\tau}_1, \\text{err}_1, \\text{pass}_1, \\hat{\\tau}_2, \\text{err}_2, \\text{pass}_2, \\hat{\\tau}_3, \\text{err}_3, \\text{pass}_3]$,\nwhere $\\text{err}_k = |\\hat{\\tau}_k - \\frac{187}{91}|$. The line must be exactly a Python-style list literal with comma-separated values and no additional text.", "solution": "The problem as stated is valid. It is a well-defined computational exercise in the field of statistical physics, specifically percolation theory. All parameters are provided, the objectives are clear, and the underlying physical and mathematical principles are sound. I will now proceed with a complete solution.\n\nThe objective is to numerically verify Fisher's scaling law for the finite cluster size distribution, $n_s$, at the critical percolation threshold, $p_c$, on a two-dimensional square lattice. The law posits that $n_s \\sim s^{-\\tau}$, where $s$ is the cluster size and $\\tau$ is a universal critical exponent. For two dimensions, theory predicts $\\tau = \\frac{187}{91}$. Our task is to estimate $\\tau$ from simulated data and compare it to this theoretical value.\n\nThe procedure is structured as follows:\n1.  A Monte Carlo simulation is performed to generate multiple realizations of a percolating system.\n2.  An algorithm is implemented to identify all clusters of connected sites in each realization.\n3.  Spanning clusters, which are artifacts of the finite system size, are identified and excluded from the analysis.\n4.  The sizes of all remaining finite clusters are aggregated.\n5.  A statistically robust method, derived from the principle of maximum likelihood, is used to estimate the exponent $\\tau$ from the tail of the observed cluster size distribution.\n\nA detailed exposition of each step is provided below.\n\n**1. Simulation of the Percolation System**\n\nWe consider a two-dimensional square lattice of size $L \\times L$ with free boundary conditions. Each site on this lattice is designated as \"occupied\" with a probability $p$ or \"empty\" with probability $1-p$, independent of all other sites. The problem is studied at the critical percolation threshold $p = p_c \\approx 0.592746$. For each set of parameters $(L, R, p_c, \\text{seed})$, we generate $R$ independent lattice configurations. A pseudorandom number generator, initialized with the specified seed, is used to assign the state of each site, ensuring reproducibility. A site at coordinates $(i, j)$ is occupied if a drawn random variate $u \\in [0, 1)$ is less than $p_c$. This process generates a binary matrix representing the occupied sites on the lattice.\n\n**2. Algorithmic Identification of Clusters**\n\nA cluster is a set of occupied sites that are connected through paths of nearest neighbors. To identify these clusters, we employ a standard and efficient algorithm based on a single pass with a Disjoint-Set Union (DSU) or Union-Find data structure. This is a variant of the Hoshen-Kopelman algorithm.\n\nThe algorithm proceeds as follows:\n- An integer matrix, `labels`, of size $L \\times L$, is initialized to all zeros. A variable, `next_label`, is initialized to $1$.\n- A DSU data structure is initialized to manage equivalences between labels.\n- The lattice sites are scanned row by row, from left to right. At each occupied site $(i, j)$:\n    - We inspect its already processed nearest neighbors: the site above, $(i-1, j)$, and the site to the left, $(i, j-1)$.\n    - If there are no occupied, labeled neighbors, the current site is the start of a new cluster. It is assigned `labels[i, j] = next_label`, and `next_label` is incremented. A new set is created in the DSU for this label.\n    - If there is one or more occupied, labeled neighbors, the current site belongs to one of their clusters. It is assigned the minimum of the labels of its neighbors. The DSU's `union` operation is then called to record that all neighbor labels are equivalent.\n- After this single pass, the `labels` matrix contains preliminary labels, and the DSU structure contains all the necessary equivalence information. A second pass is then performed over the `labels` matrix. The label of each site is replaced by the canonical representative of its equivalence class, which is found using the DSU's `find` operation with path compression.\nThe result is a matrix where all sites belonging to the same cluster are marked with the same unique integer label.\n\n**3. Exclusion of Spanning Clusters**\n\nAt the critical point $p_c$ on a finite lattice, there is a non-zero probability of forming a \"spanning\" or \"percolating\" cluster. These are macroscopic clusters whose properties are heavily influenced by the finite size of the system and do not belong to the finite cluster distribution $n_s$. We must identify and exclude them. A cluster is defined as spanning if it connects the top boundary of the lattice to the bottom boundary, or the left boundary to the right boundary.\n\nTo implement this, we identify the set of unique cluster labels present on each of the four boundaries of the final `labels` matrix: $S_{\\text{top}}$, $S_{\\text{bottom}}$, $S_{\\text{left}}$, and $S_{\\text{right}}$.\nA cluster with label $k$ is spanning if $k \\in (S_{\\text{top}} \\cap S_{\\text{bottom}}) \\cup (S_{\\text{left}} \\cap S_{\\text{right}})$.\nThe sizes of all clusters whose labels fall into this spanning set are excluded from the subsequent statistical analysis.\n\n**4. Statistical Estimation of the Power-Law Exponent $\\tau$**\n\nAfter aggregating the sizes $\\{s_i\\}$ of all finite, non-spanning clusters from all $R$ realizations, we fit the tail of the distribution to the power law $n_s \\sim s^{-\\tau}$. This implies that the probability of observing a cluster of size $s$, given that $s \\ge s_{\\min}$, follows a probability distribution $P(s) \\propto s^{-\\tau}$. We require an estimator for $\\tau$.\n\nWe derive the Maximum Likelihood Estimator (MLE) for the exponent of a continuous power-law distribution, which serves as an excellent and widely used approximation for the discrete case, especially for a non-small lower cutoff $s_{\\min}$.\nThe probability density function (PDF) for a variable $s$ following a power law with exponent $\\tau$ for $s \\ge s_{\\min}$ is:\n$$ P(s | \\tau, s_{\\min}) = (\\tau - 1)s_{\\min}^{\\tau - 1}s^{-\\tau} $$\nThis is derived by normalizing the distribution such that $\\int_{s_{\\min}}^{\\infty} P(s) ds = 1$. This requires $\\tau  1$.\n\nGiven a set of $N$ observed cluster sizes $\\{s_i\\}_{i=1}^N$ such that $s_i \\ge s_{\\min}$, the likelihood function is the product of the probabilities of observing each size:\n$$ \\mathcal{L}(\\tau | \\{s_i\\}) = \\prod_{i=1}^{N} P(s_i | \\tau, s_{\\min}) = \\prod_{i=1}^{N} (\\tau - 1)s_{\\min}^{\\tau - 1}s_i^{-\\tau} $$\nIt is more convenient to work with the log-likelihood, $\\ln \\mathcal{L}$:\n$$ \\ln \\mathcal{L} = \\sum_{i=1}^{N} \\ln \\left( (\\tau - 1)s_{\\min}^{\\tau - 1}s_i^{-\\tau} \\right) = N \\ln(\\tau - 1) + N(\\tau - 1)\\ln(s_{\\min}) - \\tau \\sum_{i=1}^{N} \\ln(s_i) $$\nTo find the value of $\\tau$ that maximizes this function, we differentiate with respect to $\\tau$ and set the result to zero:\n$$ \\frac{\\partial \\ln \\mathcal{L}}{\\partial \\tau} = \\frac{N}{\\tau - 1} + N\\ln(s_{\\min}) - \\sum_{i=1}^{N} \\ln(s_i) = 0 $$\nSolving for $\\tau$ gives the estimator $\\hat{\\tau}$:\n$$ \\frac{N}{\\hat{\\tau} - 1} = \\sum_{i=1}^{N} \\ln(s_i) - N\\ln(s_{\\min}) = \\sum_{i=1}^{N} (\\ln(s_i) - \\ln(s_{\\min})) = \\sum_{i=1}^{N} \\ln\\left(\\frac{s_i}{s_{\\min}}\\right) $$\n$$ \\hat{\\tau} = 1 + N \\left[ \\sum_{i=1}^{N} \\ln\\left(\\frac{s_i}{s_{\\min}}\\right) \\right]^{-1} $$\nThis is the formula implemented in the code to calculate the estimated exponent $\\hat{\\tau}$ from the collected cluster sizes greater than or equal to the specified cutoff $s_{\\min}$. The theoretical value is $\\tau_{\\mathrm{2D}} = \\frac{187}{91}$. The absolute error $|\\hat{\\tau} - \\tau_{\\mathrm{2D}}|$ is computed and compared against the tolerance $\\epsilon$ to determine if the test case passes.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\nclass DSU:\n    \"\"\"A Disjoint-Set Union data structure for cluster labeling.\"\"\"\n\n    def __init__(self):\n        # The parent array maps a label to its parent in the set.\n        # It is represented as a dictionary for sparse label numbers.\n        self.parent = {}\n\n    def find(self, i):\n        \"\"\"Finds the representative of the set containing element i with path compression.\"\"\"\n        if i not in self.parent:\n            self.parent[i] = i\n            return i\n        \n        path = []\n        while self.parent[i] != i:\n            path.append(i)\n            i = self.parent[i]\n        \n        # Path compression\n        for node in path:\n            self.parent[node] = i\n        return i\n\n    def union(self, i, j):\n        \"\"\"Merges the sets containing elements i and j.\"\"\"\n        root_i = self.find(i)\n        root_j = self.find(j)\n        if root_i != root_j:\n            # A simple union rule: the smaller root becomes the parent.\n            if root_i  root_j:\n                self.parent[root_j] = root_i\n            else:\n                self.parent[root_i] = root_j\n\ndef _label_clusters(grid: np.ndarray) - np.ndarray:\n    \"\"\"\n    Identifies and labels clusters on a 2D grid using a two-pass algorithm with a DSU structure.\n\n    Args:\n        grid: A boolean numpy array where True indicates an occupied site.\n\n    Returns:\n        A numpy array of the same shape with integer labels for each cluster.\n    \"\"\"\n    L = grid.shape[0]\n    labels = np.zeros_like(grid, dtype=np.int32)\n    next_label = 1\n    dsu = DSU()\n\n    # First pass: Scan the grid and assign preliminary labels.\n    for i in range(L):\n        for j in range(L):\n            if grid[i, j]:\n                # Check neighbors (top and left)\n                top_neighbor = labels[i - 1, j] if i  0 else 0\n                left_neighbor = labels[i, j - 1] if j  0 else 0\n\n                neighbor_labels = []\n                if top_neighbor  0:\n                    neighbor_labels.append(top_neighbor)\n                if left_neighbor  0:\n                    neighbor_labels.append(left_neighbor)\n\n                if not neighbor_labels:\n                    # New cluster\n                    labels[i, j] = next_label\n                    dsu.find(next_label) # Initialize the new label in DSU\n                    next_label += 1\n                else:\n                    # Part of an existing cluster\n                    min_label = min(neighbor_labels)\n                    labels[i, j] = min_label\n                    for label in neighbor_labels:\n                        dsu.union(min_label, label)\n\n    # Second pass: Resolve label equivalences.\n    # A vector mapping old labels to their canonical root is more efficient\n    # than iterating through the grid again, but this is conceptually clear.\n    final_labels = np.zeros_like(labels)\n    for i in range(L):\n        for j in range(L):\n            if labels[i, j]  0:\n                final_labels[i, j] = dsu.find(labels[i, j])\n\n    return final_labels\n\ndef _estimate_tau(sizes: np.ndarray, s_min: int) - float:\n    \"\"\"\n    Estimates the power-law exponent tau using the Maximum Likelihood Estimator.\n\n    Args:\n        sizes: A numpy array of all finite cluster sizes.\n        s_min: The minimum size to include in the fit.\n\n    Returns:\n        The estimated exponent tau_hat.\n    \"\"\"\n    sizes_above_min = sizes[sizes = s_min]\n    n = len(sizes_above_min)\n\n    if n == 0:\n        return np.nan # No data to fit\n\n    # MLE formula: tau = 1 + n / sum(ln(s_i / s_min))\n    log_sum = np.sum(np.log(sizes_above_min / s_min))\n    \n    if log_sum = 0:\n        # This case occurs if all s_i = s_min, implying an infinitely steep exponent.\n        # Or if n=0, though checked above.\n        return np.inf\n\n    tau_hat = 1.0 + n / log_sum\n    return tau_hat\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (L, R, p, s_min, seed, epsilon)\n        (64, 200, 0.592746, 8, 12345, 0.35),\n        (96, 150, 0.592746, 10, 67890, 0.35),\n        (128, 120, 0.592746, 12, 424242, 0.35),\n    ]\n\n    TAU_2D = 187.0 / 91.0\n    all_results = []\n\n    for L, R, p, s_min, seed, epsilon in test_cases:\n        rng = np.random.default_rng(seed)\n        all_finite_cluster_sizes = []\n\n        for _ in range(R):\n            # 1. Generate lattice\n            grid = rng.random((L, L))  p\n\n            # 2. Identify all clusters\n            labeled_grid = _label_clusters(grid)\n            \n            # Find unique non-zero labels and their counts (sizes)\n            unique_labels, sizes = np.unique(labeled_grid[labeled_grid  0], return_counts=True)\n            if unique_labels.size == 0:\n                continue\n            \n            cluster_sizes = dict(zip(unique_labels, sizes))\n\n            # 3. Detect and exclude spanning clusters\n            top_boundary_labels = set(np.unique(labeled_grid[0, :]))\n            bottom_boundary_labels = set(np.unique(labeled_grid[L - 1, :]))\n            left_boundary_labels = set(np.unique(labeled_grid[:, 0]))\n            right_boundary_labels = set(np.unique(labeled_grid[:, L - 1]))\n            \n            # Remove background label 0\n            for s in [top_boundary_labels, bottom_boundary_labels, left_boundary_labels, right_boundary_labels]:\n                s.discard(0)\n\n            v_spanning = top_boundary_labels.intersection(bottom_boundary_labels)\n            h_spanning = left_boundary_labels.intersection(right_boundary_labels)\n            spanning_labels = v_spanning.union(h_spanning)\n\n            # 4. Aggregate sizes of finite, non-spanning clusters\n            for label, size in cluster_sizes.items():\n                if label not in spanning_labels:\n                    all_finite_cluster_sizes.append(size)\n        \n        # 5. Fit power law and estimate tau\n        tau_hat = _estimate_tau(np.array(all_finite_cluster_sizes), s_min)\n\n        # 6. Compare with theory and check verification\n        error = abs(tau_hat - TAU_2D)\n        passed = error  epsilon\n\n        all_results.extend([tau_hat, error, passed])\n\n    # Final print statement in the exact required format.\n    # Custom mapping for booleans and floats to get precise string representation.\n    def format_val(v):\n        if isinstance(v, bool):\n            return str(v)\n        if isinstance(v, float):\n            return f\"{v:.7f}\" # Ensure sufficient precision\n        return str(v)\n\n    print(f\"[{','.join(map(format_val, all_results))}]\")\n\nsolve()\n```", "id": "2426213"}, {"introduction": "At the critical point, the spanning cluster is an infinitely complex and tenuous object. Not all of its sites are equally important for connectivity; many form \"dangling ends\" that do not contribute to carrying a current across the system. This advanced exercise teaches you to distinguish the full spanning cluster from its essential current-carrying \"backbone.\" You will implement a pruning algorithm to isolate this core structure and then use the box-counting method to measure the fractal dimensions of both the full cluster ($d_f$) and its much sparser backbone ($d_b$), revealing that they are distinct fractals with different universal dimensions. [@problem_id:2426194]", "problem": "You are asked to write a complete, runnable program that, for two-dimensional site percolation on a square lattice, identifies the backbone of the left–right spanning cluster (a finite-size proxy for the infinite cluster) and estimates its fractal dimension by box-counting. You will compare this estimate to that of the full spanning cluster. The task must be solved by first principles: employ definitions from percolation theory and graph connectivity without using any prepackaged percolation libraries.\n\nDefinitions and fundamental base to be used:\n- Consider a square lattice of linear size $L$ with open boundaries, indexed by integer coordinates $(i,j)$ with $i \\in \\{0,\\dots,L-1\\}$ and $j \\in \\{0,\\dots,L-1\\}$. Site percolation with occupation probability $p$ means each site is occupied independently with probability $p$ and vacant with probability $1-p$.\n- Two occupied sites are nearest-neighbor connected if they differ by exactly one in one coordinate and are equal in the other (four-neighbor connectivity). A cluster is a maximal set of mutually connected occupied sites.\n- A left–right spanning cluster is a cluster that contains at least one occupied site in the left boundary column $j=0$ and at least one occupied site in the right boundary column $j=L-1$. In an undirected lattice, the set of sites that are simultaneously connected to the left boundary and to the right boundary can be obtained as the intersection of two connectivity floods.\n- The backbone of a left–right spanning cluster is the subset of its sites that are not part of any dangling dead-end trees when current would be driven from left to right. One constructive way to define it is: in the subgraph induced by the spanning cluster, repeatedly remove all sites of degree $\\le 1$ that are not on the left or right boundary (where the degree is counted within this subgraph using four-neighbor connections), until no further removal is possible. The remaining sites constitute the backbone that can carry a left–right path.\n- The fractal dimension $d$ of a set embedded in the plane can be estimated by the box-counting method: cover the $L \\times L$ lattice by a grid of non-overlapping boxes of side $s$ (with $s$ a divisor of $L$), count the number $N(s)$ of boxes that intersect the set, and fit the scaling law $N(s) \\propto s^{-d}$; equivalently, perform a linear least-squares fit of $\\log N(s)$ versus $\\log (1/s)$ to estimate $d$ as the slope.\n\nRequirements:\n1. Generate site percolation configurations on an $L \\times L$ grid using independent identically distributed Bernoulli trials with parameter $p$. Use four-neighbor connectivity and open boundaries.\n2. Identify the left–right spanning cluster as follows: compute the set $A$ of occupied sites connected to the left boundary and the set $B$ of occupied sites connected to the right boundary, each by a breadth-first search (BFS) or depth-first search (DFS). The spanning cluster sites are those in $A \\cap B$. If $A \\cap B$ is empty, there is no left–right spanning cluster.\n3. Identify the backbone of the spanning cluster by iterative pruning (burning algorithm): within the subgraph induced by $A \\cap B$, define the degree of a site as the number of its four-neighbor occupied neighbors that also lie in $A \\cap B$. Repeatedly remove all sites with degree $\\le 1$ that are not on the columns $j=0$ or $j=L-1$ (these boundary sites are protected and must never be pruned), updating degrees after each removal, until convergence. The remaining set is the backbone.\n4. Estimate the fractal dimension $d_f$ of the full spanning cluster and the fractal dimension $d_b$ of its backbone by box-counting. Use box sizes $s$ that are positive integers dividing $L$ with $1 \\le s \\le L/2$ and with at least $3$ distinct scales included. For each $s$, tile the grid with $L/s$ boxes along each axis and count the number $N(s)$ of boxes that contain at least one site from the set under study. Perform an ordinary least-squares fit of $\\log N(s)$ versus $\\log (1/s)$ to estimate $d$. If the set is empty, define its fractal dimension as $0.0$. For numerical robustness, you may clamp the estimated $d$ to the interval $[0,2]$.\n5. For reproducibility, use a fixed Pseudorandom Number Generator (PRNG) seed as provided in each test case.\n\nTest suite:\n- Case $1$: $(L,p,\\text{seed}) = (64,0.62,1)$.\n- Case $2$: $(L,p,\\text{seed}) = (64,0.55,2)$.\n- Case $3$: $(L,p,\\text{seed}) = (96,1.0,0)$.\n- Case $4$: $(L,p,\\text{seed}) = (64,0.70,3)$.\n\nOutput specification:\n- For each case, if there is no left–right spanning cluster (that is, $A \\cap B$ is empty), return the pair $[0.000,0.000]$ for that case.\n- Otherwise, compute and return the pair $[d_f,d_b]$, where $d_f$ is the fractal dimension of the full spanning cluster $A \\cap B$ and $d_b$ is that of its backbone, each rounded to three decimal places.\n- Your program should produce a single line of output containing the results for the four cases as a comma-separated list of pairs enclosed in square brackets, in the order of the test suite, for example: $[[d_{f,1},d_{b,1}],[d_{f,2},d_{b,2}],[d_{f,3},d_{b,3}],[d_{f,4},d_{b,4}]]$. No additional text should be printed.", "solution": "The problem statement has been rigorously validated and is determined to be sound. It poses a well-defined computational problem within the domain of statistical physics, specifically percolation theory. All definitions, parameters, and algorithmic requirements are clear, consistent, and scientifically grounded. The task is to implement a sequence of standard algorithms: lattice generation via Bernoulli trials, cluster identification using graph search, backbone extraction via iterative pruning, and fractal dimension estimation using the box-counting method. We shall proceed with a complete solution.\n\nThe methodology is executed in four a sequential stages for each test case $(L, p, \\text{seed})$.\n\n**1. Generation of the Percolation Lattice**\nA square lattice of size $L \\times L$ is modeled as a two-dimensional array. Each site $(i,j)$, where $i, j \\in \\{0, \\dots, L-1\\}$, is designated as occupied or vacant based on a random process. We use a pseudorandom number generator (PRNG) initialized with the given seed to ensure reproducibility. For each site, a random number $r \\in [0, 1)$ is drawn from a uniform distribution. The site is marked 'occupied' if $r  p$ and 'vacant' otherwise, where $p$ is the specified occupation probability. This constitutes a set of independent Bernoulli trials, resulting in a specific configuration of occupied sites on which the analysis is performed.\n\n**2. Identification of the Left-Right Spanning Cluster**\nA left-right spanning cluster is a connected path of occupied sites from the left boundary ($j=0$) to the right boundary ($j=L-1$). To identify the sites forming such a cluster, we employ two separate graph traversal searches. Connectivity is defined by nearest neighbors (a $4$-neighbor von Neumann neighborhood).\n\nFirst, we find the set $A$ of all occupied sites that are connected to any occupied site on the left boundary. This is achieved by initiating a Breadth-First Search (BFS) starting simultaneously from all occupied sites in the column $j=0$. The BFS explores the graph of occupied sites, identifying all reachable locations.\n\nSecond, we similarly find the set $B$ of all occupied sites connected to the right boundary ($j=L-1$) by initiating another BFS starting from all occupied sites in that column.\n\nThe set of sites belonging to the left-right spanning cluster, which we denote as $C_{span}$, is the intersection of these two sets: $C_{span} = A \\cap B$. If this intersection is empty ($A \\cap B = \\emptyset$), no spanning cluster exists for the given configuration.\n\n**3. Extraction of the Cluster Backbone**\nThe backbone of the spanning cluster is the subset of its sites essential for maintaining the connection from left to right. It excludes 'dangling ends' or 'dead-end trees'. We identify the backbone using an iterative pruning algorithm, also known as a burning algorithm.\n\nLet $S_0 = C_{span}$ be the initial set of candidate backbone sites. The algorithm proceeds in steps $k=0, 1, 2, \\dots$:\n- For each site $s \\in S_k$, we compute its degree, defined as the number of its nearest neighbors that are also in $S_k$.\n- A site $s=(i,j) \\in S_k$ is identified as prunable if its degree is less than or equal to $1$ (i.e., it is a leaf or an isolated site within $S_k$) AND it is not on the left or right boundaries (i.e., $j \\neq 0$ and $j \\neq L-1$). Sites on the boundary columns are protected reservoirs for the hypothetical current and are never removed.\n- A new set of sites $P_k$ containing all prunable sites is formed.\n- If $P_k$ is empty, the process has converged. The backbone is the final set $S_k$.\n- Otherwise, the next set of candidates is $S_{k+1} = S_k \\setminus P_k$. The process repeats with $k \\leftarrow k+1$.\n\nThe set of sites remaining after this iterative removal constitutes the backbone, $C_{backbone}$.\n\n**4. Estimation of Fractal Dimension**\nThe fractal dimension $d$ of a set of points is estimated using the box-counting method. This method is based on the scaling hypothesis that the number of boxes $N(s)$ of linear size $s$ needed to cover the set scales as a power law: $N(s) \\propto s^{-d}$.\n\nTo estimate $d$, we take the logarithm of this relation: $\\log N(s) = -d \\log s + \\text{const}$, which is equivalent to $\\log N(s) = d \\log(1/s) + \\text{const}$. This reveals a linear relationship between $\\log N(s)$ and $\\log(1/s)$, where the slope is the fractal dimension $d$.\n\nThe algorithm is as follows for a given set of sites (either $C_{span}$ or $C_{backbone}$):\n- If the set is empty, its dimension is defined as $0.0$.\n- A series of box sizes $\\{s_k\\}$ is chosen. As per the requirements, these are integer divisors of $L$ such that $1 \\le s_k \\le L/2$.\n- For each $s_k$, the $L \\times L$ grid is tiled by non-overlapping boxes of size $s_k \\times s_k$. We count the number of boxes, $N(s_k)$, that contain at least one site from the set.\n- We then perform an ordinary least-squares linear regression on the data points $(\\log(1/s_k), \\log N(s_k))$. The slope of the resulting best-fit line provides the estimate for the fractal dimension $d$. For numerical stability, we use natural logarithms.\n- The final estimated dimension is clamped to the physically meaningful interval $[0, 2]$.\n\nThis procedure is applied to both the full spanning cluster $C_{span}$ to find its dimension $d_f$, and to its backbone $C_{backbone}$ to find its dimension $d_b$. The resulting pair $[d_f, d_b]$ is calculated for each test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import linregress\nfrom collections import deque\n\ndef solve():\n    \"\"\"\n    Main function to run the percolation analysis for all test cases.\n    \"\"\"\n    test_cases = [\n        (64, 0.62, 1),\n        (64, 0.55, 2),\n        (96, 1.0, 0),\n        (64, 0.70, 3),\n    ]\n\n    results = []\n    for L, p, seed in test_cases:\n        result_pair = run_percolation_analysis(L, p, seed)\n        results.append(result_pair)\n\n    # Final print statement in the exact required format.\n    formatted_results = [f\"[{r[0]:.3f},{r[1]:.3f}]\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\n\ndef run_percolation_analysis(L, p, seed):\n    \"\"\"\n    Executes the full analysis for a single test case.\n    \"\"\"\n    # 1. Generate percolation configuration\n    rng = np.random.default_rng(seed)\n    grid = rng.random((L, L))  p\n\n    # 2. Identify spanning cluster\n    \n    # Find sites connected to the left boundary\n    left_starts = {(i, 0) for i in range(L) if grid[i, 0]}\n    cluster_A = _run_bfs(grid, L, left_starts)\n\n    # Find sites connected to the right boundary\n    right_starts = {(i, L - 1) for i in range(L) if grid[i, L - 1]}\n    cluster_B = _run_bfs(grid, L, right_starts)\n\n    # The spanning cluster is the intersection\n    spanning_cluster = cluster_A.intersection(cluster_B)\n\n    if not spanning_cluster:\n        return [0.0, 0.0]\n\n    # 3. Identify backbone\n    backbone = _find_backbone(spanning_cluster, L)\n\n    # 4. Estimate fractal dimensions\n    d_f = _calculate_fractal_dimension(spanning_cluster, L)\n    d_b = _calculate_fractal_dimension(backbone, L)\n\n    return [d_f, d_b]\n\n\ndef _run_bfs(grid, L, start_nodes):\n    \"\"\"\n    Performs a Breadth-First Search on the grid from a set of start nodes.\n    \"\"\"\n    if not start_nodes:\n        return set()\n\n    q = deque(start_nodes)\n    visited = set(start_nodes)\n\n    while q:\n        r, c = q.popleft()\n\n        for dr, dc in [(0, 1), (0, -1), (1, 0), (-1, 0)]:\n            nr, nc = r + dr, c + dc\n            \n            if 0 = nr  L and 0 = nc  L and \\\n               grid[nr, nc] and (nr, nc) not in visited:\n                visited.add((nr, nc))\n                q.append((nr, nc))\n    \n    return visited\n\n\ndef _find_backbone(cluster_sites, L):\n    \"\"\"\n    Identifies the backbone of a cluster by iterative pruning.\n    \"\"\"\n    backbone_candidates = cluster_sites.copy()\n\n    while True:\n        to_prune = set()\n        \n        # Calculate degrees for all sites in the current candidate set\n        degrees = {}\n        for r, c in backbone_candidates:\n            degree = 0\n            for dr, dc in [(0, 1), (0, -1), (1, 0), (-1, 0)]:\n                nr, nc = r + dr, c + dc\n                if (nr, nc) in backbone_candidates:\n                    degree += 1\n            degrees[(r, c)] = degree\n        \n        # Identify prunable sites\n        for (r,c), degree in degrees.items():\n            if degree = 1 and c != 0 and c != L - 1:\n                to_prune.add((r, c))\n\n        if not to_prune:\n            break\n        \n        backbone_candidates.difference_update(to_prune)\n    \n    return backbone_candidates\n\n\ndef _calculate_fractal_dimension(site_set, L):\n    \"\"\"\n    Estimates the fractal dimension of a set of sites using box-counting.\n    \"\"\"\n    if not site_set:\n        return 0.0\n\n    # Determine box sizes s: divisors of L, with 1 = s = L/2\n    box_sizes = [s for s in range(1, L // 2 + 1) if L % s == 0]\n    \n    if len(box_sizes)  3:\n        # Not enough scales for a meaningful fit, might happen for small L.\n        # Although problem constraints (L=64,96) ensure this is not an issue.\n        return 0.0\n\n    site_coords = np.array(list(site_set))\n    log_N_s = []\n    log_inv_s = []\n\n    for s in box_sizes:\n        # Map site coordinates to box coordinates\n        box_coords = site_coords // s\n        \n        # Count unique boxes containing sites\n        num_boxes = len(np.unique(box_coords, axis=0))\n        \n        if num_boxes  0:\n            log_N_s.append(np.log(num_boxes))\n            log_inv_s.append(np.log(1.0 / s))\n    \n    if len(log_inv_s)  2:\n        # Need at least 2 points for a line fit\n        return 0.0\n\n    # Perform linear regression\n    # log(N(s)) = slope * log(1/s) + intercept\n    slope, _, _, _, _ = linregress(log_inv_s, log_N_s)\n    \n    # Clamp dimension to the interval [0, 2]\n    dimension = np.clip(slope, 0.0, 2.0)\n    \n    return dimension\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2426194"}]}