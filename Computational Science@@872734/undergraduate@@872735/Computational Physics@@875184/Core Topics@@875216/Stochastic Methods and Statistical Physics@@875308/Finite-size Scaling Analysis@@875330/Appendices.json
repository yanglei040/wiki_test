{"hands_on_practices": [{"introduction": "The first step in any finite-size scaling analysis is to acquire data from systems of finite size. For sufficiently small systems, we can do this exactly by enumerating all possible configurations, which provides a direct and error-free look at finite-size effects. This practice [@problem_id:2394487] challenges you to implement such an exact enumeration for the cornerstone 2D Ising model. By calculating the magnetic susceptibility $\\chi(T,L)$ and finding the temperature where it peaks, you will determine the pseudo-critical temperature $T_c(L)$ and observe firsthand how it is influenced by system size and boundary conditions.", "problem": "Consider the ferromagnetic two-dimensional Ising model on an $L\\times L$ square lattice with spin variables $s_i\\in\\{-1,+1\\}$ and coupling constant $J0$ in zero external field. The Hamiltonian is\n$$\n\\mathcal{H}(\\{s\\})=-J\\sum_{\\langle i,j\\rangle}s_is_j,\n$$\nwhere the sum is over nearest-neighbor pairs $\\langle i,j\\rangle$, each pair counted once. Two boundary conditions are to be considered: open boundary conditions and periodic boundary conditions. Under open boundary conditions, neighbors are only those sites that are adjacent within the $L\\times L$ grid without any wrapping. Under periodic boundary conditions, neighbors include wrap-around connections so that the lattice is topologically a torus.\n\nFor a given temperature $T0$ and system size $L$, define the partition function\n$$\nZ(T,L)=\\sum_{\\{s\\}}\\exp\\left(-\\frac{\\mathcal{H}(\\{s\\})}{k_{\\mathrm{B}}T}\\right),\n$$\nand the thermodynamic averages of the total magnetization $M(\\{s\\})=\\sum_i s_i$ by\n$$\n\\langle M\\rangle_{T,L}=\\frac{1}{Z(T,L)}\\sum_{\\{s\\}} M(\\{s\\})\\exp\\left(-\\frac{\\mathcal{H}(\\{s\\})}{k_{\\mathrm{B}}T}\\right),\n$$\n$$\n\\langle M^2\\rangle_{T,L}=\\frac{1}{Z(T,L)}\\sum_{\\{s\\}} M(\\{s\\})^2\\exp\\left(-\\frac{\\mathcal{H}(\\{s\\})}{k_{\\mathrm{B}}T}\\right).\n$$\nThe zero-field magnetic susceptibility per spin is defined by\n$$\n\\chi(T,L)=\\frac{1}{k_{\\mathrm{B}}TN}\\left(\\langle M^2\\rangle_{T,L}-\\langle M\\rangle_{T,L}^2\\right),\\quad N=L^2.\n$$\n\nFor this problem, adopt dimensionless units $J=1$ and $k_{\\mathrm{B}}=1$, so that temperature is measured in units of $J/k_{\\mathrm{B}}$. For each specified pair of $(L,\\text{boundary})$, define the finite-size pseudo-critical temperature $T_c(L)$ operationally as the element of the discrete temperature set\n$$\n\\mathcal{T}=\\{T\\,\\mid\\,T=1.00+0.02\\,n,\\ n\\in\\mathbb{Z},\\ 0\\le n\\le 150\\}\n$$\nat which $\\chi(T,L)$ attains its maximum value. If $\\chi(T,L)$ attains its maximum at multiple temperatures in $\\mathcal{T}$, take $T_c(L)$ to be the smallest such temperature in $\\mathcal{T}$.\n\nYour task is to compute $T_c(L)$ for each case in the following test suite:\n- Case $1$: $L=2$, open boundary conditions.\n- Case $2$: $L=2$, periodic boundary conditions.\n- Case $3$: $L=3$, open boundary conditions.\n- Case $4$: $L=3$, periodic boundary conditions.\n- Case $5$: $L=4$, open boundary conditions.\n- Case $6$: $L=4$, periodic boundary conditions.\n\nAll results must be expressed in units of $J/k_{\\mathrm{B}}$ and rounded to $3$ decimal places. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets in the order of Cases $1$ through $6$ (for example, $[x_1,x_2,x_3,x_4,x_5,x_6]$), where each $x_i$ is a floating-point number as specified.", "solution": "The problem statement has been rigorously validated. It is found to be scientifically grounded, well-posed, and objective. It is rooted in the standard framework of statistical mechanics, specifically the two-dimensional Ising model, which is a cornerstone of the field. All parameters, definitions, and procedures are specified with sufficient clarity and precision to permit a unique, verifiable solution. The problem is therefore deemed valid and a direct computational approach is appropriate.\n\nThe task is to compute the finite-size pseudo-critical temperature, $T_c(L)$, for the ferromagnetic 2D Ising model on an $L \\times L$ lattice. This temperature is defined as the point within a specified discrete set, $\\mathcal{T}$, where the zero-field magnetic susceptibility per spin, $\\chi(T,L)$, is maximized.\n\nThe system sizes specified ($L=2, 3, 4$) are small, rendering the problem solvable by exact enumeration. The total number of possible spin configurations, or microstates, is $2^N$, where $N=L^2$. For the largest system, $L=4$, this amounts to $2^{16} = 65536$ states, a number that is computationally trivial to handle.\n\nThe methodical approach is as follows:\n\nFirst, for each specified case defined by system size $L$ and boundary condition type (open or periodic), we will construct the exact density of states, denoted by $g(E, M)$. This function counts the number of distinct spin configurations $\\{\\text{s}\\}$ that have a specific total energy $E$ and a specific total magnetization $M$. The energy is calculated from the Hamiltonian with dimensionless units $J=1$:\n$$ E = \\mathcal{H}(\\{s\\}) = -\\sum_{\\langle i,j \\rangle} s_i s_j $$\nAnd the magnetization is the sum of all spins:\n$$ M = \\sum_{i=1}^{N} s_i $$\nThe density of states is computed by systematically iterating through all $2^N$ possible configurations, calculating $(E,M)$ for each, and incrementing the corresponding counter.\n\nSecond, with the density of states $g(E,M)$ established, we can efficiently compute the necessary thermodynamic quantities for any temperature $T$. The partition function, $Z(T,L)$, is given by a sum over all unique energy and magnetization pairs:\n$$ Z(T,L) = \\sum_{E, M} g(E,M) \\exp\\left(-\\frac{E}{T}\\right) $$\nHere, we have used the dimensionless setting where the Boltzmann constant $k_{\\mathrm{B}}=1$.\n\nThird, we compute the magnetic susceptibility per spin, $\\chi(T,L)$. The general definition is:\n$$ \\chi(T,L) = \\frac{1}{T N} \\left( \\langle M^2 \\rangle_{T,L} - \\langle M \\rangle_{T,L}^2 \\right) $$\nDue to the spin-flip symmetry of the Hamiltonian in zero external magnetic field, the average magnetization $\\langle M \\rangle_{T,L}$ is identically zero for any finite system at non-zero temperature. The expression for susceptibility thus simplifies to:\n$$ \\chi(T,L) = \\frac{1}{T N} \\langle M^2 \\rangle_{T,L} $$\nThe thermal average of the squared magnetization, $\\langle M^2 \\rangle_{T,L}$, is calculated as:\n$$ \\langle M^2 \\rangle_{T,L} = \\frac{1}{Z(T,L)} \\sum_{E, M} g(E,M) M^2 \\exp\\left(-\\frac{E}{T}\\right) $$\n\nFinally, to determine $T_c(L)$, we compute $\\chi(T,L)$ for every temperature $T$ in the provided discrete set $\\mathcal{T} = \\{T \\mid T=1.00+0.02n, n \\in \\{0, 1, \\dots, 150\\}\\}$. The value of $T_c(L)$ is the temperature at which $\\chi(T,L)$ reaches its maximum. The problem specifies that in case of a tie, the smallest temperature that yields the maximum susceptibility is to be chosen. This rule is strictly followed by iterating through temperatures in ascending order and updating the candidate for $T_c(L)$ only when a strictly greater value of $\\chi$ is found.\n\nThis complete algorithm is implemented for each of the six test cases to produce the required results.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom collections import defaultdict\n\ndef get_dos(L, boundary_type):\n    \"\"\"\n    Computes the exact density of states g(E, M) for an L x L Ising lattice.\n\n    Args:\n        L (int): The linear size of the lattice.\n        boundary_type (str): 'obc' for open or 'pbc' for periodic boundaries.\n\n    Returns:\n        defaultdict: A dictionary mapping (energy, magnetization) tuples to counts.\n    \"\"\"\n    N = L * L\n    dos = defaultdict(int)\n\n    # Iterate through all 2^N possible spin configurations.\n    # Each integer 'i' from 0 to 2^N - 1 represents a unique configuration.\n    for i in range(2**N):\n        # Generate the spin configuration from the integer 'i'.\n        # Bit 'j' of 'i' corresponds to spin s_j. 0 - -1, 1 - +1.\n        spins = np.array([2 * ((i  j)  1) - 1 for j in range(N)])\n        \n        energy = 0\n        # Calculate energy by summing over nearest-neighbor interactions.\n        # To avoid double counting, we only sum interactions with 'right'\n        # and 'down' neighbors for each spin.\n        for idx in range(N):\n            s_i = spins[idx]\n            y, x = divmod(idx, L)\n\n            # Interaction with the right neighbor\n            if boundary_type == 'pbc':\n                idx_right = y * L + (x + 1) % L\n                energy -= s_i * spins[idx_right]\n            elif x  L - 1: # OBC\n                idx_right = y * L + x + 1\n                energy -= s_i * spins[idx_right]\n\n            # Interaction with the down neighbor\n            if boundary_type == 'pbc':\n                idx_down = ((y + 1) % L) * L + x\n                energy -= s_i * spins[idx_down]\n            elif y  L - 1: # OBC\n                idx_down = (y + 1) * L + x\n                energy -= s_i * spins[idx_down]\n\n        magnetization = np.sum(spins)\n        dos[(energy, magnetization)] += 1\n    \n    return dos\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    \"\"\"\n    test_cases = [\n        (2, 'obc'), (2, 'pbc'),\n        (3, 'obc'), (3, 'pbc'),\n        (4, 'obc'), (4, 'pbc'),\n    ]\n\n    # Define the discrete set of temperatures according to the problem statement.\n    # T = 1.00 + 0.02*n for n from 0 to 150.\n    temperatures = np.linspace(1.0, 1.00 + 0.02 * 150, 151)\n    \n    final_results = []\n\n    for L, bc in test_cases:\n        N = L * L\n        dos = get_dos(L, bc)\n        \n        max_chi = -1.0\n        tc = -1.0\n        \n        for T in temperatures:\n            # Using dimensionless units J=1, k_B=1.\n            Z = 0.0       # Partition function\n            M2_sum = 0.0  # Sum for M^2 numerator\n            \n            # Sum over the density of states to compute Z and M^2\n            for (energy, mag), count in dos.items():\n                boltzmann_factor = np.exp(-energy / T)\n                Z += count * boltzmann_factor\n                M2_sum += count * mag**2 * boltzmann_factor\n            \n            # Avoid division by zero, although not expected here for T  0.\n            if Z  0:\n                avg_M2 = M2_sum / Z\n            else:\n                avg_M2 = 0\n            \n            # Susceptibility per spin: chi = (1/(T*N)) * (M^2 - M^2)\n            # In zero field, M = 0.\n            chi = avg_M2 / (T * N)\n            \n            # Find T that maximizes chi.\n            # If chi == max_chi, keep the existing tc since it's smaller.\n            # This handles the tie-breaking rule.\n            if chi  max_chi:\n                max_chi = chi\n                tc = T\n\n        # Format result to 3 decimal places as specified.\n        # This ensures outputs like 2.4 are printed as 2.400.\n        final_results.append(f\"{tc:.3f}\")\n\n    # Print the final list of results in the required format.\n    print(f\"[{','.join(final_results)}]\")\n\nsolve()\n```", "id": "2394487"}, {"introduction": "Once pseudo-critical temperatures $T_c(L)$ are obtained for several system sizes $L$, the central goal of finite-size scaling is often to extrapolate these results to the thermodynamic limit ($L \\to \\infty$). FSS theory predicts that the deviation of $T_c(L)$ from the true critical temperature $T_c$ vanishes as a power law, $|T_c(L) - T_c| \\sim L^{-1/\\nu}$. This exercise [@problem_id:2394503] puts you in the role of a computational physicist analyzing simulation data; you will use this scaling relation to perform a regression analysis and extract an accurate estimate of $T_c$ for the infinite system.", "problem": "You are tasked with implementing a principled Finite-Size Scaling (FSS) analysis to estimate the thermodynamic-limit phase boundary of a two-dimensional ($2\\mathrm{D}$) Ising model on a square lattice with competing nearest-neighbor ($J_1$) and next-nearest-neighbor ($J_2$) interactions. Consider the ferromagnetic regime with $J_1  0$ and $J_2 \\ge 0$, where for $J_2/J_1$ below a threshold the finite-temperature phase transition is continuous and in the standard two-dimensional Ising universality class. Temperatures are to be expressed in units of $J_1/k_{\\mathrm{B}}$.\n\nStarting from the fundamental definition that the equilibrium correlation length $\\xi$ diverges near a continuous phase transition as $\\xi \\sim \\xi_0 \\lvert t \\rvert^{-\\nu}$, where $t = (T - T_{\\mathrm{c}})/T_{\\mathrm{c}}$ is the reduced temperature and $\\nu$ is the correlation-length critical exponent, and using the well-tested fact that for the two-dimensional Ising universality class one has $\\nu = 1$, reason from first principles how a finite system of linear size $L$ imposes a cutoff on $\\xi$ that rounds and shifts thermodynamic singularities. In particular, for a pseudocritical temperature $T^*_L$ defined by the location of the susceptibility maximum at finite $L$, justify an estimator for the thermodynamic critical temperature $T_{\\mathrm{c}}$ in the limit $L \\to \\infty$. Your estimator must follow from the logic that the singularity is rounded when $\\xi(T) \\sim \\mathcal{O}(L)$ and may include subleading analytic corrections consistent with this framework.\n\nImplement a program that, given finite-size pseudocritical temperatures $T^*_L$ for several $L$ at fixed $J_2/J_1$, fits a model consistent with the FSS reasoning and then extrapolates $T^*_L$ to $L \\to \\infty$ to estimate $T_{\\mathrm{c}}(J_2/J_1)$. For numerical stability and to account for subleading corrections, you must use a polynomial in $L^{-1}$ up to quadratic order to model $T^*_L$ as a function of $L$, and extract the $L \\to \\infty$ limit as the intercept in this representation. Report all temperatures in units of $J_1/k_{\\mathrm{B}}$, rounded to $9$ decimal places.\n\nTest suite and data (all temperatures are in units of $J_1/k_{\\mathrm{B}}$):\n- Use system sizes $L \\in \\{10, 20, 40, 80\\}$.\n- For $J_2/J_1 = 0.0$, the pseudocritical temperatures are $2.394185314$, $2.330435314$, $2.299497814$, $2.284263439$ for $L = 10$, $20$, $40$, $80$, respectively.\n- For $J_2/J_1 = 0.1$, the pseudocritical temperatures are $2.16795863574$, $2.11570863574$, $2.09014613574$, $2.07750551074$ for $L = 10$, $20$, $40$, $80$, respectively.\n- For $J_2/J_1 = 0.2$, the pseudocritical temperatures are $1.94273195748$, $1.90123195748$, $1.88085695748$, $1.87076320748$ for $L = 10$, $20$, $40$, $80$, respectively.\n\nYour program must:\n- Implement a regression of $T^*_L$ versus a polynomial basis in $L^{-1}$ up to quadratic order to estimate the intercept corresponding to $L \\to \\infty$ at each $J_2/J_1$ value.\n- Produce the final output as a single line containing a comma-separated list of the three estimated $T_{\\mathrm{c}}$ values (in units of $J_1/k_{\\mathrm{B}}$), rounded to $9$ decimal places, enclosed in square brackets.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, \"[x0,x1,x2]\". Replace \"x0,x1,x2\" with your three floating-point results, each shown with exactly $9$ digits after the decimal point and no spaces.", "solution": "The problem presented is a standard exercise in computational statistical mechanics, specifically in the application of finite-size scaling theory to extrapolate numerical data from finite systems to the thermodynamic limit. The problem is scientifically grounded, well-posed, and contains all necessary information for a unique solution. I will therefore proceed with a principled derivation of the solution methodology.\n\nThe cornerstone of the theory of continuous phase transitions is the divergence of the equilibrium correlation length, $\\xi$, as the temperature $T$ approaches the critical temperature $T_{\\mathrm{c}}$. This divergence is described by a power law:\n$$\n\\xi(T) \\sim \\xi_0 \\lvert t \\rvert^{-\\nu}\n$$\nwhere $t = (T - T_{\\mathrm{c}})/T_{\\mathrm{c}}$ is the reduced temperature, $\\xi_0$ is a non-universal amplitude, and $\\nu$ is a universal critical exponent. For the specified problem, the system belongs to the two-dimensional ($2\\mathrm{D}$) Ising universality class, for which it is an exact result that the correlation length exponent is $\\nu = 1$. Thus, near $T_{\\mathrm{c}}$, the correlation length behaves as:\n$$\n\\xi(T) \\sim \\xi_0 \\lvert T - T_{\\mathrm{c}} \\rvert^{-1}\n$$\n\nIn any practical simulation or experiment, the system size is finite, characterized by a linear dimension $L$. A finite system cannot sustain fluctuations on a length scale larger than its own size. This imposes an effective cutoff on the correlation length, $\\xi_L(T) \\le L$. The sharp singularity present at $T_{\\mathrm{c}}$ in the thermodynamic limit ($L \\to \\infty$) is consequently rounded and shifted in a finite system. Response functions, such as the magnetic susceptibility, which diverge at $T_{\\mathrm{c}}$ for $L \\to \\infty$, will instead exhibit a finite peak at a system-size-dependent pseudocritical temperature, denoted $T^*_L$.\n\nThe finite-size scaling hypothesis posits that this rounding effect becomes dominant when the thermodynamic correlation length $\\xi(T)$ becomes comparable to the system size $L$. The location of the peak, $T^*_L$, is therefore determined by the condition:\n$$\n\\xi(T^*_L) \\sim L\n$$\nSubstituting the scaling form for $\\xi$ with $\\nu = 1$ gives:\n$$\n\\xi_0 \\lvert T^*_L - T_{\\mathrm{c}} \\rvert^{-1} \\sim L\n$$\nRearranging this relationship reveals how the pseudocritical temperature $T^*_L$ approaches the true critical temperature $T_{\\mathrm{c}}$ as $L$ increases:\n$$\n\\lvert T^*_L - T_{\\mathrm{c}} \\rvert \\sim L^{-1}\n$$\nThis implies that a plot of $T^*_L$ versus $L^{-1}$ should be linear for large $L$, and the intercept at $L^{-1} = 0$ (i.e., $L \\to \\infty$) will yield the thermodynamic critical temperature $T_{\\mathrm{c}}$. We can write this as a formal expansion:\n$$\nT^*_L = T_{\\mathrm{c}} + c_1 L^{-1} + \\dots\n$$\nwhere $c_1$ is a non-universal constant.\n\nMore advanced scaling theories account for subleading corrections, which arise from irrelevant scaling fields or analytic background contributions. The general form for the shift of the pseudocritical temperature is:\n$$\nT^*_L = T_{\\mathrm{c}} + a_{1} L^{-1/\\nu} + a_{2} L^{-(\\omega + 1/\\nu)} + \\dots\n$$\nwhere $\\omega  0$ is the leading correction-to-scaling exponent. For the $2\\mathrm{D}$ Ising universality class with $\\nu=1$, this becomes:\n$$\nT^*_L = T_{\\mathrm{c}} + a_{1} L^{-1} + a_{2} L^{-(1+\\omega)} + \\dots\n$$\nThe problem specifies using a polynomial in $L^{-1}$ up to quadratic order as a model. This is a pragmatic and robust approach, especially when the value of $\\omega$ is not known with high precision or when the data are not of sufficient quality to resolve multiple non-integer exponents. We thus adopt the fitting function:\n$$\nT^*_L = C_0 + C_1 L^{-1} + C_2 L^{-2}\n$$\nIn this model, the parameter $C_0$ represents the extrapolated value in the thermodynamic limit, $L \\to \\infty$, and is therefore our estimator for the true critical temperature, $T_{\\mathrm{c}}$. The term $C_1 L^{-1}$ captures the leading finite-size scaling behavior, and the term $C_2 L^{-2}$ provides an effective representation of the leading subleading corrections, whatever their precise origin.\n\nTo determine the coefficients $C_0, C_1,$ and $C_2$, we perform a linear least-squares regression. For each dataset corresponding to a fixed ratio $J_2/J_1$, we are given a set of measurements $(L_i, T^*_{L_i})$. We seek the vector of coefficients $\\mathbf{C} = [C_0, C_1, C_2]^T$ that minimizes the sum of squared residuals:\n$$\n\\chi^2 = \\sum_{i} \\left( T^*_{L_i} - (C_0 + C_1 L_i^{-1} + C_2 L_i^{-2}) \\right)^2\n$$\nThis is a standard linear algebra problem, solvable by constructing a design matrix $\\mathbf{A}$ where each row corresponds to a system size $L_i$ and is given by $[1, L_i^{-1}, L_i^{-2}]$, and a vector of observations $\\mathbf{b}$ with elements $T^*_{L_i}$. The solution is found by solving the normal equations $(\\mathbf{A}^T \\mathbf{A})\\mathbf{C} = \\mathbf{A}^T \\mathbf{b}$. The resulting coefficient $C_0$ is the desired estimate for $T_{\\mathrm{c}}$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Performs a finite-size scaling analysis to estimate the critical temperature\n    of a 2D Ising model with J1-J2 interactions.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    # Each case includes system sizes L and corresponding pseudocritical\n    # temperatures T_star. Temperatures are in units of J1/k_B.\n    test_cases = [\n        {\n            # Case for J2/J1 = 0.0\n            'L': np.array([10, 20, 40, 80], dtype=np.float64),\n            'T_star': np.array([2.394185314, 2.330435314, 2.299497814, 2.284263439], dtype=np.float64)\n        },\n        {\n            # Case for J2/J1 = 0.1\n            'L': np.array([10, 20, 40, 80], dtype=np.float64),\n            'T_star': np.array([2.16795863574, 2.11570863574, 2.09014613574, 2.07750551074], dtype=np.float64)\n        },\n        {\n            # Case for J2/J1 = 0.2\n            'L': np.array([10, 20, 40, 80], dtype=np.float64),\n            'T_star': np.array([1.94273195748, 1.90123195748, 1.88085695748, 1.87076320748], dtype=np.float64)\n        }\n    ]\n\n    estimated_tc_values = []\n\n    for case in test_cases:\n        L = case['L']\n        T_star = case['T_star']\n\n        # The fitting model is a polynomial in L^-1 up to quadratic order:\n        # T_star(L) = C0 + C1 * L^-1 + C2 * L^-2\n        # We need to solve a linear least-squares problem for the coefficients [C0, C1, C2].\n        # C0 is the estimate for T_c in the thermodynamic limit (L - infinity).\n\n        # Construct the design matrix 'A' for the linear system A*x = b.\n        # The columns of A are the basis functions: L^0, L^-1, L^-2.\n        L_inv = 1.0 / L\n        A = np.vstack([np.ones_like(L), L_inv, L_inv**2]).T\n        \n        # The vector of observations 'b' contains the T_star values.\n        b = T_star\n        \n        # Solve the linear least squares problem A*x = b for x = [C0, C1, C2].\n        # numpy.linalg.lstsq returns the solution vector as the first element.\n        coeffs = np.linalg.lstsq(A, b, rcond=None)[0]\n        \n        # The extrapolated critical temperature Tc is the intercept, C0.\n        tc_estimate = coeffs[0]\n        estimated_tc_values.append(tc_estimate)\n\n    # Format the results as specified: a list of floats rounded to 9 decimal places,\n    # comma-separated, and enclosed in square brackets. No spaces.\n    formatted_results = [f\"{tc:.9f}\" for tc in estimated_tc_values]\n    output_string = f\"[{','.join(formatted_results)}]\"\n    \n    # Final print statement in the exact required format.\n    print(output_string)\n\nsolve()\n```", "id": "2394503"}, {"introduction": "Data collapse is arguably the most powerful and visually compelling confirmation of the finite-size scaling hypothesis, showing that data from different system sizes and temperatures can be unified onto a single master curve. But what if the critical exponents required for the rescaling are unknown? This advanced practice [@problem_id:2394510] introduces a modern, automated approach to solve this problem by treating data collapse as an optimization task. You will implement an algorithm to find the best-fit exponents by systematically minimizing a quantitative measure of the collapse quality, a technique directly applicable to real research data.", "problem": "You are given a generic finite-size scaling problem focused on collapsing multiple curves of an observable onto a single master curve by rescaling axes. The scientific context is the finite-size scaling hypothesis near a continuous phase transition. Let an observable be denoted by $m(L,t)$, where $L$ is a system size and $t$ is a dimensionless reduced control parameter (for example, a reduced temperature). The finite-size scaling hypothesis states that there exist exponent combinations $p$ and $q$ and a dimensionless scaling function $F(\\cdot)$ such that the rescaled variables\n$$\nx = t \\, L^{p}, \\quad y = m(L,t) \\, L^{q}\n$$\ncollapse data from different $L$ onto a common, size-independent master curve $y = F(x)$, when the correct exponents $p$ and $q$ are used. In the thermodynamic limit, this statement is supported by renormalization group arguments and is part of the standard finite-size scaling paradigm in computational physics.\n\nYour task is to implement an algorithm that automatically performs data collapse by numerically optimizing the choice of $p$ and $q$ to minimize the area between the upper and lower envelopes of the collapsed curves. To make this algorithmic objective precise: given a collection of curves indexed by $j$, after rescaling to $(x_j, y_j)$ as above, define their common domain $[x_{\\min}, x_{\\max}]$ as the intersection of all individual $x$-intervals. On a uniform grid of $N_g$ points in $[x_{\\min}, x_{\\max}]$, linearly interpolate each curve $y_j(x)$, compute at each grid point the spread $w(x) = \\max_j y_j(x) - \\min_j y_j(x)$, and define the objective to be the domain-averaged spread\n$$\n\\mathcal{A}(p,q) = \\frac{1}{x_{\\max} - x_{\\min}} \\int_{x_{\\min}}^{x_{\\max}} \\left[ \\max_j y_j(x) - \\min_j y_j(x) \\right] \\, dx.\n$$\nNumerically approximate the integral using the trapezoidal rule. If the common domain is empty or negligibly small, treat $\\mathcal{A}(p,q)$ as very large to discourage such parameter choices. The algorithm must search for $p$ and $q$ that minimize $\\mathcal{A}(p,q)$.\n\nAlgorithmic and implementation requirements:\n- Use a derivative-free numerical optimizer suitable for nonconvex problems.\n- Use a uniform grid of $N_g = 401$ points to evaluate the integral and the envelope width.\n- Use linear interpolation for $y_j(x)$ over the common domain.\n- Constrain the search for $p$ and $q$ to reasonable bounds: $p \\in [0.5, 1.5]$, $q \\in [0.05, 0.25]$.\n- No physical units are involved; all variables are dimensionless.\n\nTest suite:\nConstruct synthetic datasets according to\n$$\nm(L,t) = L^{-q_{\\mathrm{true}}} \\, F\\!\\left(t \\, L^{p_{\\mathrm{true}}}\\right),\n$$\nwith the scaling function\n$$\nF(u) = \\left[ 1 + (a\\,u)^2 \\right]^{-b},\n$$\nwhich is smooth and positive for all real $u$. For noisy cases, add independent Gaussian noise with zero mean and standard deviation equal to a specified fraction times the median of the noiseless $m(L,t)$ values of that dataset. Use a fixed random seed per case for reproducibility.\n\nProvide three test cases as follows. For each case, construct $n_L$ system sizes $\\{L\\}$, a uniform grid of $N_t$ points for $t$ over $[t_{\\min}, t_{\\max}]$, and then compute $m(L,t)$ as above.\n\n- Case $1$ (clean, canonical exponents):\n  - Parameters: $p_{\\mathrm{true}} = 1.0$, $q_{\\mathrm{true}} = 0.125$, $a = 2.0$, $b = 0.5$.\n  - System sizes: $L \\in \\{16, 32, 64, 128\\}$ (that is, $n_L = 4$).\n  - Grid: $t \\in [-0.30, 0.30]$ with $N_t = 241$ uniformly spaced points.\n  - Noise: none (standard deviation equal to $0$), seed not applicable.\n\n- Case $2$ (noisy, same exponents as case $1$):\n  - Parameters: $p_{\\mathrm{true}} = 1.0$, $q_{\\mathrm{true}} = 0.125$, $a = 2.0$, $b = 0.5$.\n  - System sizes: $L \\in \\{16, 32, 64, 128\\}$.\n  - Grid: $t \\in [-0.30, 0.30]$ with $N_t = 241$ uniformly spaced points.\n  - Noise: Gaussian with standard deviation equal to $0.02$ times the median of the noiseless $m(L,t)$ values for each $L$, seed $123$.\n\n- Case $3$ (different exponents and moderate noise):\n  - Parameters: $p_{\\mathrm{true}} = 0.9$, $q_{\\mathrm{true}} = 0.2$, $a = 1.5$, $b = 0.8$.\n  - System sizes: $L \\in \\{12, 24, 48, 96\\}$.\n  - Grid: $t \\in [-0.40, 0.40]$ with $N_t = 241$ uniformly spaced points.\n  - Noise: Gaussian with standard deviation equal to $0.01$ times the median of the noiseless $m(L,t)$ values for each $L$, seed $456$.\n\nYour program must:\n- Implement the synthetic data generation as specified above for each test case.\n- Implement the objective $\\mathcal{A}(p,q)$ as the domain-averaged envelope width on a grid of $N_g = 401$ points using linear interpolation and trapezoidal integration, returning a large penalty when the common domain is empty or extremely small.\n- Numerically minimize $\\mathcal{A}(p,q)$ over $p \\in [0.5, 1.5]$ and $q \\in [0.05, 0.25]$ to estimate $(p, q)$ for each case.\n- Produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each result must be a list of two floats $[p_{\\mathrm{est}}, q_{\\mathrm{est}}]$ rounded to exactly three decimal places. For the three cases, the output must have the form\n  $$\n  \\left[ [p_{\\mathrm{est}}^{(1)}, q_{\\mathrm{est}}^{(1)}], [p_{\\mathrm{est}}^{(2)}, q_{\\mathrm{est}}^{(2)}], [p_{\\mathrm{est}}^{(3)}, q_{\\mathrm{est}}^{(3)}] \\right].\n  $$\nNo additional text should be printed other than this single line containing the list.", "solution": "We begin from the core finite-size scaling hypothesis: for an observable $m(L,t)$ near a continuous phase transition, there exist exponent combinations $p$ and $q$ and a scaling function $F(\\cdot)$ such that $m(L,t) = L^{-q} F(t L^{p})$. This statement rests on renormalization group considerations and the assumption that the only relevant scale at criticality is the ratio of the system size to the correlation length, which justifies data collapse when variables are properly rescaled.\n\nTo detect the correct exponents computationally, we define a quantitative measure of how well curves collapse when rescaled. If we rescale each curve using $x = t L^{p}$ and $y = m L^{q}$, the perfect collapse would produce a single curve $y = F(x)$ that is independent of $L$. A practical, model-agnostic way to quantify the quality of the collapse is to compare the spread of $y$ among the different curves at the same $x$. The upper envelope at a given $x$ is $\\max_j y_j(x)$ and the lower envelope is $\\min_j y_j(x)$, where $j$ indexes curves at different $L$. The spread at $x$ is the difference $\\max_j y_j(x) - \\min_j y_j(x)$. We aggregate this spread over a domain of $x$ values common to all curves to avoid extrapolation beyond data support.\n\nSince the data are discrete, we proceed numerically. For a given guess $(p,q)$, we:\n- Compute $x_j = t_j L_j^{p}$ and $y_j = m_j L_j^{q}$ for each curve $j$.\n- Determine the common domain $[x_{\\min}, x_{\\max}]$ as the intersection of all intervals covered by the rescaled $x_j$; that is, $x_{\\min} = \\max_j \\min(x_j)$ and $x_{\\max} = \\min_j \\max(x_j)$.\n- If the common domain is empty or negligibly small, we return a large penalty to discourage such parameter choices.\n- Otherwise, construct a uniform grid of $N_g$ points on $[x_{\\min}, x_{\\max}]$.\n- For each curve $j$, linearly interpolate $y_j$ as a function of $x_j$ onto this grid using stable monotonic interpolation (the data are naturally monotone in $x$ because $t$ is monotone and $L^{p}  0$). This gives $y_j(x)$ on the common grid for all $j$.\n- At each grid point $x$, compute the spread $w(x) = \\max_j y_j(x) - \\min_j y_j(x)$.\n- Compute the objective as the mean envelope width over the domain,\n$$\n\\mathcal{A}(p,q) = \\frac{1}{x_{\\max} - x_{\\min}} \\int_{x_{\\min}}^{x_{\\max}} w(x) \\, dx,\n$$\nwhich we approximate by the trapezoidal rule over the uniform grid. Normalizing by $(x_{\\max} - x_{\\min})$ prevents the algorithm from artificially reducing the objective by collapsing the domain to a vanishingly small interval, thus making the measure comparable across different $(p,q)$.\n\nWith this objective in hand, we select a derivative-free optimizer suitable for nonconvex landscapes. Methods like Powell’s method or Nelder–Mead as implemented in standard scientific libraries are appropriate because the objective involves interpolation and an integral that make derivatives inconvenient or noisy. We enforce simple bound constraints $p \\in [0.5, 1.5]$ and $q \\in [0.05, 0.25]$, which reflect typical ranges for many models and prevent pathological rescalings.\n\nFor testing and verification, we generate synthetic data from a known scaling function and known exponents. Specifically, we use\n$$\nm(L,t) = L^{-q_{\\mathrm{true}}} \\, F(t L^{p_{\\mathrm{true}}}), \\quad F(u) = \\left[1 + (a u)^2 \\right]^{-b}.\n$$\nWe provide three test cases: a clean dataset with $p_{\\mathrm{true}} = 1.0$, $q_{\\mathrm{true}} = 0.125$, $a = 2.0$, $b = 0.5$; a noisy dataset with the same exponents and noise standard deviation equal to $0.02$ times the median signal level per $L$; and a third dataset with different exponents $p_{\\mathrm{true}} = 0.9$, $q_{\\mathrm{true}} = 0.2$, parameters $a = 1.5$, $b = 0.8$, and moderate noise equal to $0.01$ times the median signal level per $L$. For the noisy cases we use fixed random seeds to ensure reproducibility. The grid in $t$ is uniform, and $L$ values are chosen so that the rescaled domains overlap sufficiently for the true exponents.\n\nThe algorithm proceeds as follows for each test case:\n- Generate $\\{(t_j, m_j, L_j)\\}$ for each $L_j$ in the specified set using the given $p_{\\mathrm{true}}$, $q_{\\mathrm{true}}$, $a$, and $b$, and add noise where required.\n- Define the objective function $\\mathcal{A}(p,q)$ as the mean envelope width computed on a uniform grid of $N_g = 401$ points over the common $x$ domain.\n- Minimize $\\mathcal{A}(p,q)$ with respect to $(p,q)$ using a derivative-free optimizer with box constraints $p \\in [0.5, 1.5]$ and $q \\in [0.05, 0.25]$, possibly from multiple initial guesses to improve robustness.\n- Round the resulting estimates $(p_{\\mathrm{est}}, q_{\\mathrm{est}})$ to three decimal places.\n\nFinally, we aggregate the three $(p_{\\mathrm{est}}, q_{\\mathrm{est}})$ pairs into a single list and print it as a single line in the required format:\n$$\n\\left[ [p_{\\mathrm{est}}^{(1)}, q_{\\mathrm{est}}^{(1)}], [p_{\\mathrm{est}}^{(2)}, q_{\\mathrm{est}}^{(2)}], [p_{\\mathrm{est}}^{(3)}, q_{\\mathrm{est}}^{(3)}] \\right].\n$$\nThis approach is principled because it derives directly from the finite-size scaling hypothesis and operationalizes data collapse via an envelope-based spread measure, while the numerical implementation leverages standard interpolation and quadrature combined with robust optimization.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\n\n# Finite-size scaling data collapse by minimizing the average envelope width.\n\ndef scaling_function(u, a, b):\n    # F(u) = [1 + (a u)^2]^{-b}\n    return (1.0 + (a * u) ** 2) ** (-b)\n\ndef generate_synthetic_case(L_values, t_min, t_max, N_t, p_true, q_true, a, b, noise_frac=0.0, seed=None):\n    \"\"\"\n    Generate synthetic data for a single case:\n    m(L,t) = L^{-q_true} * F(t * L^{p_true}), F(u) = [1 + (a u)^2]^{-b}\n    Optionally add Gaussian noise with std = noise_frac * median(noiseless_m(L,t)) per L.\n    \"\"\"\n    t_grid = np.linspace(t_min, t_max, N_t)\n    datasets = []\n    rng = np.random.default_rng(seed) if seed is not None else None\n    for L in L_values:\n        u = t_grid * (L ** p_true)\n        m_clean = (L ** (-q_true)) * scaling_function(u, a, b)\n        if noise_frac  0 and rng is not None:\n            sigma = noise_frac * np.median(m_clean)\n            noise = rng.normal(loc=0.0, scale=sigma, size=m_clean.shape)\n            m_noisy = m_clean + noise\n        else:\n            m_noisy = m_clean.copy()\n        datasets.append((t_grid.copy(), m_noisy, float(L)))\n    return datasets\n\ndef average_envelope_width(params, datasets, Ng=401, penalty=1e6):\n    \"\"\"\n    Compute the average envelope width over the common x-domain for given (p, q).\n    Uses linear interpolation and trapezoidal integration on a uniform grid.\n    Returns a large penalty if the common domain is empty or too small.\n    \"\"\"\n    p, q = params\n    # Rescale each dataset\n    x_list = []\n    y_list = []\n    for (t, m, L) in datasets:\n        x = t * (L ** p)\n        y = m * (L ** q)\n        # Ensure increasing x order for interpolation\n        # t_grid is monotonic, but if p is negative (not allowed) or pathological bounds, this prevents issues.\n        idx = np.argsort(x)\n        x_list.append(x[idx])\n        y_list.append(y[idx])\n\n    # Determine common domain\n    x_mins = [xi[0] for xi in x_list]\n    x_maxs = [xi[-1] for xi in x_list]\n    x_min_common = max(min(xi) for xi in x_list)\n    x_max_common = min(max(xi) for xi in x_list)\n\n    if not np.isfinite(x_min_common) or not np.isfinite(x_max_common):\n        return penalty\n    if x_max_common = x_min_common:\n        return penalty\n\n    # Uniform grid on the common domain\n    grid = np.linspace(x_min_common, x_max_common, Ng)\n    # Interpolate y on the grid\n    Y = np.empty((len(y_list), Ng))\n    for i, (xi, yi) in enumerate(zip(x_list, y_list)):\n        # np.interp assumes xi is ascending\n        Y[i, :] = np.interp(grid, xi, yi)\n    # Envelope width at each grid point\n    width = np.max(Y, axis=0) - np.min(Y, axis=0)\n    domain_length = x_max_common - x_min_common\n    # Average envelope width: integral divided by domain length\n    avg_width = np.trapz(width, grid) / domain_length\n    # Numerical safety\n    if not np.isfinite(avg_width):\n        return penalty\n    return float(avg_width)\n\ndef estimate_exponents(datasets, bounds=((0.5, 1.5), (0.05, 0.25))):\n    \"\"\"\n    Estimate (p, q) by minimizing the average envelope width with respect to params.\n    Uses Powell's method with bounds and multiple starting points for robustness.\n    \"\"\"\n    # Multiple initial guesses to reduce risk of local minima\n    initial_guesses = [\n        np.array([1.0, 0.1]),\n        np.array([0.8, 0.2]),\n        np.array([1.2, 0.15]),\n    ]\n    best_val = np.inf\n    best_params = None\n    for x0 in initial_guesses:\n        res = minimize(\n            average_envelope_width,\n            x0=x0,\n            args=(datasets,),\n            method=\"Powell\",\n            bounds=bounds,\n            options={\"xtol\": 1e-4, \"ftol\": 1e-6, \"maxiter\": 200, \"disp\": False},\n        )\n        if res.fun  best_val:\n            best_val = res.fun\n            best_params = res.x\n    # Clip to bounds for numerical safety\n    p_est = float(np.clip(best_params[0], bounds[0][0], bounds[0][1]))\n    q_est = float(np.clip(best_params[1], bounds[1][0], bounds[1][1]))\n    return p_est, q_est\n\ndef solve():\n    # Define the three test cases\n\n    # Case 1: Clean, canonical exponents\n    case1 = generate_synthetic_case(\n        L_values=[16, 32, 64, 128],\n        t_min=-0.30, t_max=0.30, N_t=241,\n        p_true=1.0, q_true=0.125, a=2.0, b=0.5,\n        noise_frac=0.0, seed=None\n    )\n\n    # Case 2: Noisy, same exponents as case 1\n    case2 = generate_synthetic_case(\n        L_values=[16, 32, 64, 128],\n        t_min=-0.30, t_max=0.30, N_t=241,\n        p_true=1.0, q_true=0.125, a=2.0, b=0.5,\n        noise_frac=0.02, seed=123\n    )\n\n    # Case 3: Different exponents, moderate noise\n    case3 = generate_synthetic_case(\n        L_values=[12, 24, 48, 96],\n        t_min=-0.40, t_max=0.40, N_t=241,\n        p_true=0.9, q_true=0.2, a=1.5, b=0.8,\n        noise_frac=0.01, seed=456\n    )\n\n    # Estimate exponents for each case\n    results_pairs = []\n    for datasets in [case1, case2, case3]:\n        p_est, q_est = estimate_exponents(datasets)\n        # Round to three decimals for final output format\n        results_pairs.append([round(p_est, 3), round(q_est, 3)])\n\n    # Format output as a single-line list of lists with exactly three decimals\n    formatted = \"[\" + \",\".join(\n        \"[\" + \",\".join(f\"{v:.3f}\" for v in pair) + \"]\" for pair in results_pairs\n    ) + \"]\"\n    print(formatted)\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2394510"}]}