{"hands_on_practices": [{"introduction": "The size and structure of a virtual address space are not arbitrary; they are direct consequences of fundamental hardware design choices. This exercise challenges you to work backward from the parameters of a multi-level page table—such as the number of levels, page size, and entry size—to derive the total virtual address width from first principles. By comparing two different hypothetical architectures, you will gain a deeper appreciation for the design trade-offs between page table overhead and the size of the addressable virtual memory. [@problem_id:3620292]", "problem": "A computer system uses demand-paged virtual memory implemented by a Memory Management Unit (MMU). The MMU translates a process’s logical (virtual) addresses to physical addresses using a uniform multi-level page table scheme. Each level of the page table is a page-sized array of fixed-size page-table entries. For a given architecture, let $L$ denote the number of page-table levels (including the leaf), let $E$ denote the number of entries per page-table page, and let $P$ denote the page size in bytes. The virtual address of a memory reference is split into a page offset and $L$ indices, one per level, that select the corresponding entry within each level.\n\nTwo architectures are proposed:\n\n- Architecture $\\mathsf{A}$: $L_{\\mathsf{A}} = 4$, $P_{\\mathsf{A}} = 4 \\times 2^{10}$ bytes, and each page-table entry is $8$ bytes, with the design choice that $E_{\\mathsf{A}} = P_{\\mathsf{A}} / 8$.\n\n- Architecture $\\mathsf{B}$: $L_{\\mathsf{B}} = 3$, $P_{\\mathsf{B}} = 16 \\times 2^{10}$ bytes, and each page-table entry is $8$ bytes, with the design choice that $E_{\\mathsf{B}} = P_{\\mathsf{B}} / 8$.\n\nAssume that a page-table entry stores enough information to locate the next-level page table (or the physical page at the leaf), and that there are no hardware-imposed constraints beyond the multi-level scheme described. Start from the core definitions of paging (page offset, page number, and multi-level indexing) and the fact that the number of bits needed to encode $N$ distinct values is $\\log_{2}(N)$, and derive, from first principles, the virtual-address width for each architecture. Use only these fundamentals and do not assume or cite any architecture-specific “shortcut” formulas.\n\nTo ground the comparison with concrete numbers, consider mapping a single contiguous region of logical memory of size $S = 256 \\times 2^{20}$ bytes for one process. For each architecture, determine the number of last-level page tables allocated, the number of upper-level page tables allocated, and the total page-table memory overhead in bytes. Briefly explain the design trade-offs observed (for example, impacts on page-table memory overhead and internal fragmentation).\n\nReport only the difference in virtual-address width between Architecture $\\mathsf{A}$ and Architecture $\\mathsf{B}$ as your final answer. Express your final answer as an integer number of bits.", "solution": "The problem is valid as it is scientifically grounded in the principles of virtual memory management, is well-posed with all necessary information provided, and is stated objectively. We can proceed with a solution derived from first principles.\n\nA virtual address is partitioned into two main components: a page number and a page offset. The number of bits required for the page offset, $w_{\\text{offset}}$, is determined by the page size, $P$. To uniquely address each byte within a page of size $P$ bytes, we require $w_{\\text{offset}} = \\log_2(P)$ bits.\n\nThe remaining bits of the virtual address constitute the page number. In a multi-level paging scheme with $L$ levels, the page number is further subdivided into $L$ indices, where each index is used to select an entry in a page table at a specific level. The problem states that each page table is itself a page-sized array containing a number of page-table entries, $E$. To select one of the $E$ entries in a page table, an index of width $w_{\\text{index}} = \\log_2(E)$ bits is required.\n\nSince the scheme is uniform, each of the $L$ indices has the same width, $w_{\\text{index}}$. The total width of the page number, $w_{\\text{pn}}$, is the sum of the widths of all indices, which is $w_{\\text{pn}} = L \\times w_{\\text{index}} = L \\times \\log_2(E)$.\n\nTherefore, the total virtual-address width, $w_{\\text{vaddr}}$, is the sum of the page number width and the page offset width:\n$$w_{\\text{vaddr}} = w_{\\text{pn}} + w_{\\text{offset}} = (L \\times \\log_2(E)) + \\log_2(P)$$\n\nWe are given that a page table is page-sized and each page-table entry (PTE) has a fixed size. Let the PTE size be $S_{\\text{PTE}}$. The number of entries per page-table, $E$, is the page size $P$ divided by the PTE size $S_{\\text{PTE}}$.\n$$E = \\frac{P}{S_{\\text{PTE}}}$$\nSubstituting this into our equation for $w_{\\text{vaddr}}$:\n$$w_{\\text{vaddr}} = L \\times \\log_2\\left(\\frac{P}{S_{\\text{PTE}}}\\right) + \\log_2(P)$$\nThis is the general formula derived from first principles. Now we apply it to each architecture.\n\nFor both architectures, the page-table entry size is $S_{\\text{PTE}} = 8$ bytes, which is $2^3$ bytes.\n\n**Architecture $\\mathsf{A}$**\nThe given parameters are:\n- Number of levels, $L_{\\mathsf{A}} = 4$.\n- Page size, $P_{\\mathsf{A}} = 4 \\times 2^{10} = 2^2 \\times 2^{10} = 2^{12}$ bytes.\n- Page-table entry size, $S_{\\text{PTE}} = 8 = 2^3$ bytes.\n\nFirst, we calculate the number of entries per page-table page for architecture $\\mathsf{A}$:\n$$E_{\\mathsf{A}} = \\frac{P_{\\mathsf{A}}}{S_{\\text{PTE}}} = \\frac{2^{12}}{2^3} = 2^9$$\nThe number of bits for an index into this page table is:\n$$w_{\\text{index, A}} = \\log_2(E_{\\mathsf{A}}) = \\log_2(2^9) = 9 \\text{ bits}$$\nThe number of bits for the page offset is:\n$$w_{\\text{offset, A}} = \\log_2(P_{\\mathsf{A}}) = \\log_2(2^{12}) = 12 \\text{ bits}$$\nThe total virtual-address width for architecture $\\mathsf{A}$ is:\n$$w_{\\text{vaddr, A}} = (L_{\\mathsf{A}} \\times w_{\\text{index, A}}) + w_{\\text{offset, A}} = (4 \\times 9) + 12 = 36 + 12 = 48 \\text{ bits}$$\n\n**Architecture $\\mathsf{B}$**\nThe given parameters are:\n- Number of levels, $L_{\\mathsf{B}} = 3$.\n- Page size, $P_{\\mathsf{B}} = 16 \\times 2^{10} = 2^4 \\times 2^{10} = 2^{14}$ bytes.\n- Page-table entry size, $S_{\\text{PTE}} = 8 = 2^3$ bytes.\n\nFirst, we calculate the number of entries per page-table page for architecture $\\mathsf{B}$:\n$$E_{\\mathsf{B}} = \\frac{P_{\\mathsf{B}}}{S_{\\text{PTE}}} = \\frac{2^{14}}{2^3} = 2^{11}$$\nThe number of bits for an index into this page table is:\n$$w_{\\text{index, B}} = \\log_2(E_{\\mathsf{B}}) = \\log_2(2^{11}) = 11 \\text{ bits}$$\nThe number of bits for the page offset is:\n$$w_{\\text{offset, B}} = \\log_2(P_{\\mathsf{B}}) = \\log_2(2^{14}) = 14 \\text{ bits}$$\nThe total virtual-address width for architecture $\\mathsf{B}$ is:\n$$w_{\\text{vaddr, B}} = (L_{\\mathsf{B}} \\times w_{\\text{index, B}}) + w_{\\text{offset, B}} = (3 \\times 11) + 14 = 33 + 14 = 47 \\text{ bits}$$\n\n**Difference in Virtual-Address Width**\nThe difference is $w_{\\text{vaddr, A}} - w_{\\text{vaddr, B}} = 48 - 47 = 1$ bit.\n\n**Analysis of Page-Table Overhead**\nWe now analyze the memory overhead for mapping a contiguous region of size $S = 256 \\times 2^{20}$ bytes.\n$S = 256 \\times 2^{20} = 2^8 \\times 2^{20} = 2^{28}$ bytes.\n\n**For Architecture $\\mathsf{A}$ ($P_{\\mathsf{A}} = 2^{12}$ bytes, $E_{\\mathsf{A}} = 2^9$ entries/table, $L_{\\mathsf{A}}=4$ levels):**\nNumber of pages required to map the region:\n$$N_{\\text{pages, A}} = \\frac{S}{P_{\\mathsf{A}}} = \\frac{2^{28}}{2^{12}} = 2^{16} = 65536 \\text{ pages}$$\nThese pages are pointed to by entries in the last-level (leaf) page tables. These are level-4 page tables in a 4-level scheme.\nNumber of last-level ($L4$) page tables:\n$$N_{L4, \\mathsf{A}} = \\left\\lceil \\frac{N_{\\text{pages, A}}}{E_{\\mathsf{A}}} \\right\\rceil = \\left\\lceil \\frac{2^{16}}{2^9} \\right\\rceil = 2^7 = 128 \\text{ tables}$$\nThese $128$ tables require $128$ pointers in the level-3 page tables.\nNumber of level-3 ($L3$) page tables:\n$$N_{L3, \\mathsf{A}} = \\left\\lceil \\frac{N_{L4, \\mathsf{A}}}{E_{\\mathsf{A}}} \\right\\rceil = \\left\\lceil \\frac{128}{2^9} \\right\\rceil = \\left\\lceil \\frac{2^7}{2^9} \\right\\rceil = 1 \\text{ table}$$\nThis single $L3$ table requires $1$ pointer in a level-2 page table.\nNumber of level-2 ($L2$) page tables:\n$$N_{L2, \\mathsf{A}} = \\left\\lceil \\frac{N_{L3, \\mathsf{A}}}{E_{\\mathsf{A}}} \\right\\rceil = \\left\\lceil \\frac{1}{2^9} \\right\\rceil = 1 \\text{ table}$$\nThis single $L2$ table requires $1$ pointer in a level-1 page table.\nNumber of level-1 ($L1$) page tables (the top-level directory):\n$$N_{L1, \\mathsf{A}} = \\left\\lceil \\frac{N_{L2, \\mathsf{A}}}{E_{\\mathsf{A}}} \\right\\rceil = \\left\\lceil \\frac{1}{2^9} \\right\\rceil = 1 \\text{ table}$$\n- Number of last-level page tables: $128$.\n- Number of upper-level page tables ($L1, L2, L3$): $1 + 1 + 1 = 3$.\n- Total page tables allocated: $128 + 3 = 131$.\n- Total page-table memory overhead: $131 \\times P_{\\mathsf{A}} = 131 \\times 2^{12} = 131 \\times 4096 = 536576$ bytes.\n\n**For Architecture $\\mathsf{B}$ ($P_{\\mathsf{B}} = 2^{14}$ bytes, $E_{\\mathsf{B}} = 2^{11}$ entries/table, $L_{\\mathsf{B}}=3$ levels):**\nNumber of pages required to map the region:\n$$N_{\\text{pages, B}} = \\frac{S}{P_{\\mathsf{B}}} = \\frac{2^{28}}{2^{14}} = 2^{14} = 16384 \\text{ pages}$$\nThese pages are pointed to by entries in the last-level (leaf) page tables. These are level-3 page tables in a 3-level scheme.\nNumber of last-level ($L3$) page tables:\n$$N_{L3, \\mathsf{B}} = \\left\\lceil \\frac{N_{\\text{pages, B}}}{E_{\\mathsf{B}}} \\right\\rceil = \\left\\lceil \\frac{2^{14}}{2^{11}} \\right\\rceil = 2^3 = 8 \\text{ tables}$$\nThese $8$ tables require $8$ pointers in the level-2 page tables.\nNumber of level-2 ($L2$) page tables:\n$$N_{L2, \\mathsf{B}} = \\left\\lceil \\frac{N_{L3, \\mathsf{B}}}{E_{\\mathsf{B}}} \\right\\rceil = \\left\\lceil \\frac{8}{2^{11}} \\right\\rceil = 1 \\text{ table}$$\nThis single $L2$ table requires $1$ pointer in a level-1 page table.\nNumber of level-1 ($L1$) page tables (the top-level directory):\n$$N_{L1, \\mathsf{B}} = \\left\\lceil \\frac{N_{L2, \\mathsf{B}}}{E_{\\mathsf{B}}} \\right\\rceil = \\left\\lceil \\frac{1}{2^{11}} \\right\\rceil = 1 \\text{ table}$$\n- Number of last-level page tables: $8$.\n- Number of upper-level page tables ($L1, L2$): $1 + 1 = 2$.\n- Total page tables allocated: $8 + 2 = 10$.\n- Total page-table memory overhead: $10 \\times P_{\\mathsf{B}} = 10 \\times 2^{14} = 10 \\times 16384 = 163840$ bytes.\n\n**Brief Explanation of Design Trade-offs**\n- **Page-Table Memory Overhead**: Architecture $\\mathsf{B}$ has a significantly lower memory overhead for its page tables ($163840$ bytes) compared to Architecture $\\mathsf{A}$ ($536576$ bytes). This is a direct consequence of its larger page size ($16 \\text{ KiB}$ vs. $4 \\text{ KiB}$) and shallower page table hierarchy ($3$ levels vs. $4$ levels). Larger pages mean fewer pages are needed to cover the same memory region, which in turn means fewer page table entries and fewer last-level page tables are required.\n- **Internal Fragmentation**: Architecture $\\mathsf{A}$ is superior in terms of internal fragmentation. With its smaller page size of $4 \\text{ KiB}$, the average wasted space at the end of a memory allocation is smaller (on average $2 \\text{ KiB}$) than with Architecture $\\mathsf{B}$'s $16 \\text{ KiB}$ pages (on average $8 \\text{ KiB}$). For processes with many small, non-page-aligned memory segments, this can lead to substantial memory waste in Architecture $\\mathsf{B}$.\nThe trade-off is classic: smaller pages reduce internal fragmentation but increase the size of the page tables and the pressure on translation lookaside buffers (TLBs), while larger pages reduce page table overhead and improve TLB effectiveness at the cost of increased internal fragmentation.\n\nThe final answer required is only the difference in virtual-address width.\n$$w_{\\text{vaddr, A}} - w_{\\text{vaddr, B}} = 48 - 47 = 1$$", "answer": "$$\\boxed{1}$$", "id": "3620292"}, {"introduction": "Modern processors use huge pages as a critical optimization to reduce pressure on the Translation Lookaside Buffer (TLB) and minimize page table overhead when managing large memory regions. However, this performance gain comes with a constraint: huge pages require strict memory alignment for both virtual and physical addresses. This practice presents a concrete remapping task that forces you to navigate these alignment rules, calculating the practical efficiency gains of using huge pages versus standard 4 KiB pages and illustrating why this feature is powerful but not universally applicable. [@problem_id:3620275]", "problem": "Consider a $64$-bit system that uses demand-paged virtual memory with support for both base pages of size $4$ KiB and huge pages of size $2$ MiB. The Memory Management Unit (MMU) translates Virtual Address (VA) to Physical Address (PA) through multi-level page tables. Each page-mapping entry (either a Page Table Entry (PTE) for a $4$ KiB page or a Page Directory Entry (PDE) with the Page Size bit set for a $2$ MiB page) maps exactly one page and is the unit of remapping. For this problem, define one \"PTE update\" to mean writing one page-mapping entry of either kind.\n\nAssume the following physical and virtual memory facts:\n- A $4$ KiB page is $2^{12}$ bytes and a $2$ MiB page is $2^{21}$ bytes. A kibibyte (KiB) is $2^{10}$ bytes and a mebibyte (MiB) is $2^{20}$ bytes.\n- A $2$ MiB mapping requires the VA base address and the PA base address to be individually aligned to $2$ MiB boundaries, and the physical frames covered by the mapping must be physically contiguous.\n- A $4$ KiB mapping requires the VA base and PA base be individually aligned to $4$ KiB boundaries, and the physical frame must be present; no larger alignment is required.\n\nYou must remap a single large contiguous VA region of size $513$ MiB to a new set of Physical Frame Numbers (PFNs) that are physically contiguous over the entire $513$ MiB span. The VA base of the region is misaligned by exactly $1$ MiB relative to the nearest $2$ MiB boundary, and the PA base of the target PFN span is also misaligned by exactly $1$ MiB relative to a $2$ MiB boundary. All addresses involved are aligned to $4$ KiB boundaries, and the multi-level page tables already exist; count only the page-mapping entry writes needed to perform the remap, not any pointer-level entries or table allocations.\n\nCompute the minimal number of page-mapping entry updates required under two independent strategies:\n- Strategy A: Use $2$ MiB pages wherever the alignment rules allow and use $4$ KiB pages for any remainder that cannot be covered by $2$ MiB pages.\n- Strategy B: Use only $4$ KiB pages for the entire $513$ MiB region.\n\nReport your final answer as a pair $(\\text{Strategy A}, \\text{Strategy B})$ in that order. The final answer must be a single entity and contain no units.", "solution": "The problem is assessed to be valid. It is scientifically grounded in the principles of operating system memory management, well-posed with sufficient and consistent data, and objectively stated.\n\nWe are asked to compute the number of page-mapping entry updates required to remap a contiguous virtual address (VA) region of size $513$ MiB to a contiguous physical address (PA) region of the same size, under two different paging strategies.\n\nLet's define the key parameters from the problem statement:\n-   Total size of the memory region to be remapped: $S_{total} = 513 \\text{ MiB}$.\n-   Base page size: $S_{base} = 4 \\text{ KiB} = 4 \\times 2^{10} \\text{ bytes} = 2^2 \\times 2^{10} \\text{ bytes} = 2^{12} \\text{ bytes}$.\n-   Huge page size: $S_{huge} = 2 \\text{ MiB} = 2 \\times 2^{20} \\text{ bytes} = 2^{21} \\text{ bytes}$.\n-   The VA base address of the region is misaligned by $1 \\text{ MiB}$ relative to a $2 \\text{ MiB}$ boundary.\n-   The PA base address of the target region is also misaligned by $1 \\text{ MiB}$ relative to a $2 \\text{ MiB}$ boundary.\n-   An \"update\" is defined as writing a single page-mapping entry, which can be for either a $4 \\text{ KiB}$ page or a $2 \\text{ MiB}$ page.\n\nFirst, we analyze Strategy B, as it is simpler.\n\n**Strategy B: Use only $4$ KiB pages**\n\nIn this strategy, the entire $513 \\text{ MiB}$ region is mapped using only $4 \\text{ KiB}$ pages. The number of required page-mapping entry updates, $N_B$, is the total size of the region divided by the size of a single base page. The misalignments are irrelevant here because the problem states all addresses are aligned to $4 \\text{ KiB}$ boundaries, which is the only requirement for using $4 \\text{ KiB}$ pages.\n\nWe can express the sizes in bytes to perform the calculation:\n$S_{total} = 513 \\text{ MiB} = 513 \\times 2^{20} \\text{ bytes}$.\n$S_{base} = 4 \\text{ KiB} = 2^{12} \\text{ bytes}$.\n\nThe number of updates is:\n$$N_B = \\frac{S_{total}}{S_{base}} = \\frac{513 \\times 2^{20}}{2^{12}} = 513 \\times 2^{20-12} = 513 \\times 2^8$$\n$$N_B = 513 \\times 256$$\nTo compute this, we can write $513$ as $512 + 1 = 2^9 + 1$:\n$$N_B = (2^9 + 1) \\times 2^8 = 2^9 \\times 2^8 + 1 \\times 2^8 = 2^{17} + 2^8 = 131072 + 256 = 131328$$\nSo, for Strategy B, $131,328$ updates are required.\n\n**Strategy A: Use $2$ MiB pages wherever possible**\n\nThis strategy requires careful analysis of the alignment constraints. A $2 \\text{ MiB}$ huge page can only be used if both its VA base and its corresponding PA base are aligned to a $2 \\text{ MiB}$ boundary.\n\nLet the VA base of the region be $V_{start}$ and the PA base be $P_{start}$. The problem states they are misaligned by $1 \\text{ MiB}$ relative to a $2 \\text{ MiB}$ boundary. This can be expressed mathematically:\n$$V_{start} \\pmod{2 \\text{ MiB}} = 1 \\text{ MiB}$$\n$$P_{start} \\pmod{2 \\text{ MiB}} = 1 \\text{ MiB}$$\n\nThe VA region spans from $V_{start}$ to $V_{start} + 513 \\text{ MiB} - 1$.\nBecause $V_{start}$ is not aligned to a $2 \\text{ MiB}$ boundary, we cannot use a $2 \\text{ MiB}$ page at the start of the region. We must first map a portion of the region using $4 \\text{ KiB}$ pages until we reach a VA that is $2 \\text{ MiB}$-aligned.\n\nThe first $2 \\text{ MiB}$-aligned VA address greater than $V_{start}$ is $V_{align} = V_{start} + (2 \\text{ MiB} - 1 \\text{ MiB}) = V_{start} + 1 \\text{ MiB}$. The VA segment from $V_{start}$ to $V_{align}-1$ has a size of $1 \\text{ MiB}$. This initial segment must be mapped using $4 \\text{ KiB}$ pages.\nThe number of updates for this initial $1 \\text{ MiB}$ segment, $N_{A,1}$, is:\n$$N_{A,1} = \\frac{1 \\text{ MiB}}{4 \\text{ KiB}} = \\frac{2^{20} \\text{ bytes}}{2^{12} \\text{ bytes}} = 2^{20-12} = 2^8 = 256$$\n\nNow, we consider the remaining portion of the region. The total size is $513 \\text{ MiB}$, and we have already mapped $1 \\text{ MiB}$. The remaining size is $513 \\text{ MiB} - 1 \\text{ MiB} = 512 \\text{ MiB}$.\nThis remaining region starts at VA $V_{align} = V_{start} + 1 \\text{ MiB}$. We have already established that this VA is $2 \\text{ MiB}$-aligned.\n\nNext, we must check the alignment of the corresponding PA. The physical memory is contiguous, so the PA corresponding to $V_{align}$ is $P_{align} = P_{start} + 1 \\text{ MiB}$. We check its alignment:\n$$P_{align} \\pmod{2 \\text{ MiB}} = (P_{start} + 1 \\text{ MiB}) \\pmod{2 \\text{ MiB}}$$\nSince $P_{start} \\pmod{2 \\text{ MiB}} = 1 \\text{ MiB}$, we have:\n$$(1 \\text{ MiB} + 1 \\text{ MiB}) \\pmod{2 \\text{ MiB}} = 2 \\text{ MiB} \\pmod{2 \\text{ MiB}} = 0$$\nThe PA base of the remaining region, $P_{align}$, is also $2 \\text{ MiB}$-aligned.\n\nThe remaining region has a size of $512 \\text{ MiB}$. Since both its VA and PA bases are $2 \\text{ MiB}$-aligned, and the physical memory is contiguous, we can map this entire portion using $2 \\text{ MiB}$ huge pages. The number of huge pages needed, $N_{A,2}$, is:\n$$N_{A,2} = \\frac{512 \\text{ MiB}}{2 \\text{ MiB}} = 256$$\n\nThe total number of updates for Strategy A, $N_A$, is the sum of the updates for the initial misaligned part and the main aligned part:\n$$N_A = N_{A,1} + N_{A,2} = 256 + 256 = 512$$\n\nThus, for Strategy A, $512$ updates are required.\n\nThe final answer is the pair of results for Strategy A and Strategy B.\n- Strategy A: $512$ updates.\n- Strategy B: $131,328$ updates.\nThe pair is $(512, 131328)$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n512 & 131328\n\\end{pmatrix}\n}\n$$", "id": "3620275"}, {"introduction": "Copy-on-Write (COW) is a clever and widely used operating system optimization that defers expensive memory copies until they are absolutely necessary, most notably during process creation via `fork()`. While this mechanism is highly effective, it has a hidden cost that is realized on the first write to a shared page. This problem uses a probabilistic model to explore the expected memory overhead under random write patterns, revealing how certain workloads can inadvertently trigger a cascade of page duplications and undermine the very efficiency COW aims to provide. [@problem_id:3620286]", "problem": "Consider a system implementing Copy-On-Write (COW) for anonymous memory pages following the semantics of a forked process. A parent process allocates an anonymous region consisting of $P$ pages, each of size $s$ bytes, in its virtual address space. After a fork, the child and parent initially share the same physical frames for these pages, and the Memory Management Unit (MMU) maps the child’s virtual addresses to these shared physical frames. Copy-On-Write (COW) means that the first time the child writes to any shared page, the kernel allocates a new physical frame and copies the entire page content; subsequent writes by the same process to the same page do not allocate additional frames.\n\nAssume the parent process remains read-only, while the child process performs $k$ writes, each writing a single byte to a virtual address selected independently and uniformly at random from the child’s $P$-page region. Each write selects an address uniformly across all addresses in the region, equivalently selecting one of the $P$ pages uniformly at random. The MMU enforces page granularity, so any first write by the child to a shared page triggers duplication of the entire page of size $s$ bytes, regardless of the fact that the write modifies only a single byte.\n\nIgnoring all caching, Translation Lookaside Buffer (TLB) behavior, and other kernel overheads, and assuming no other processes touch this region, derive the expected total increase in physical memory consumption (in bytes) due solely to page duplications caused by the child’s $k$ writes. Express your final answer as a single closed-form analytic expression in terms of $P$, $k$, and $s$. State your final result in bytes. No rounding is required.", "solution": "The user wants me to solve the problem by following a strict validation and formatting protocol.\n\n### Step 1: Extract Givens\n- A region of anonymous memory consists of $P$ pages.\n- Each page has a size of $s$ bytes.\n- A child process performs $k$ writes.\n- Each write is to a virtual address selected independently and uniformly at random from the $P$-page region.\n- The parent process remains read-only.\n- The first write by the child to a shared page triggers duplication of the entire page, increasing physical memory consumption by $s$ bytes.\n- Subsequent writes by the child to the same page do not cause further duplication.\n- The problem asks for the expected total increase in physical memory consumption in bytes.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded (Critical)**: The problem is firmly grounded in the principles of operating systems, specifically virtual memory management and the Copy-On-Write (COW) optimization. This is a standard topic in computer science and engineering curricula. The probabilistic model used is a classic application within this domain. The problem is scientifically sound.\n2.  **Well-Posed**: The problem asks for an expected value, which is a well-defined mathematical quantity. All necessary variables ($P$, $s$, $k$) are provided to derive a closed-form solution. The problem is structured such that a unique, meaningful solution exists.\n3.  **Objective (Critical)**: The language is precise and unambiguous. The description of the COW mechanism and the random process for writes is clear and objective.\n4.  **Incomplete or Contradictory Setup**: The problem is self-contained. It clearly states the assumptions (e.g., ignoring caches, TLB, other overheads) to simplify the model, which is appropriate for a theoretical problem. There are no contradictions.\n5.  **Unrealistic or Infeasible**: The scenario is a standard, albeit simplified, model of process memory behavior after a `fork()` system call. The parameters are symbolic, so no specific values are physically implausible.\n6.  **Ill-Posed or Poorly Structured**: The problem is well-structured. The random selection process is clearly defined as \"independently and uniformly at random.\"\n7.  **Pseudo-Profound, Trivial, or Tautological**: The problem requires a non-trivial application of probability theory (specifically, the use of indicator random variables for an occupancy problem) to a computer systems context. It is not trivial or tautological.\n8.  **Outside Scientific Verifiability**: The problem is a mathematical derivation based on a well-defined model. Its solution is verifiable through logical and mathematical reasoning.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. I will proceed with the solution.\n\nThe problem asks for the expected total increase in physical memory consumption due to the child process's writes. An increase in physical memory occurs only when the child writes to a page for the very first time, triggering a Copy-On-Write (COW) event. Each such event duplicates one page, increasing memory usage by $s$ bytes.\n\nLet $D$ be the random variable representing the number of distinct pages written to by the child process out of the total $P$ pages. The total increase in physical memory consumption, which we can denote by $M$, is directly proportional to $D$. Specifically, for each of the $D$ unique pages that are written to, a copy is made, consuming $s$ bytes of new physical memory. Therefore, the total memory increase is given by:\n$$M = s \\cdot D$$\n\nThe problem asks for the expected value of this memory increase, $E[M]$. Using the linearity of expectation, we have:\n$$E[M] = E[s \\cdot D] = s \\cdot E[D]$$\n\nOur task is now reduced to finding the expected number of distinct pages written to, $E[D]$, after $k$ independent and uniformly random writes.\n\nThis is a classic occupancy problem. We can solve it by defining indicator random variables. For each page $i$, where $i \\in \\{1, 2, \\dots, P\\}$, let $I_i$ be an indicator random variable such that:\n$$\nI_i =\n\\begin{cases}\n1 & \\text{if page } i \\text{ is written to at least once in } k \\text{ writes} \\\\\n0 & \\text{if page } i \\text{ is never written to in } k \\text{ writes}\n\\end{cases}\n$$\n\nThe total number of distinct pages written to, $D$, is the sum of these indicator variables:\n$$D = \\sum_{i=1}^{P} I_i$$\n\nBy the linearity of expectation, the expected value of $D$ is the sum of the expected values of the individual indicator variables:\n$$E[D] = E\\left[\\sum_{i=1}^{P} I_i\\right] = \\sum_{i=1}^{P} E[I_i]$$\n\nThe expected value of an indicator variable is equal to the probability of the event it indicates. Thus, $E[I_i] = P(I_i = 1)$, which is the probability that page $i$ is written to at least once.\n\nIt is easier to calculate the complementary probability, $P(I_i = 0)$, which is the probability that page $i$ is never written to in any of the $k$ writes.\n\nFor a single write, the child process selects one of the $P$ pages uniformly at random.\nThe probability of selecting a specific page $i$ is $\\frac{1}{P}$.\nThe probability of *not* selecting page $i$ in a single write is $1 - \\frac{1}{P}$.\n\nSince the $k$ writes are independent events, the probability that page $i$ is not selected in any of these $k$ writes is the product of the individual probabilities:\n$$P(I_i = 0) = \\left(1 - \\frac{1}{P}\\right)^k$$\n\nNow we can find the probability that page $i$ is written to at least once:\n$$P(I_i = 1) = 1 - P(I_i = 0) = 1 - \\left(1 - \\frac{1}{P}\\right)^k$$\n\nTherefore, the expected value of the indicator variable is:\n$$E[I_i] = 1 - \\left(1 - \\frac{1}{P}\\right)^k$$\n\nSince the page selection is uniform, this probability is the same for all pages $i = 1, \\dots, P$. We can now substitute this back into the expression for $E[D]$:\n$$E[D] = \\sum_{i=1}^{P} \\left(1 - \\left(1 - \\frac{1}{P}\\right)^k\\right)$$\n\nBecause the term being summed is constant with respect to the index $i$, the summation simplifies to multiplying the term by $P$:\n$$E[D] = P \\left(1 - \\left(1 - \\frac{1}{P}\\right)^k\\right)$$\n\nFinally, we can find the expected total increase in physical memory consumption, $E[M]$, by substituting our result for $E[D]$ into the initial equation:\n$$E[M] = s \\cdot E[D] = s P \\left(1 - \\left(1 - \\frac{1}{P}\\right)^k\\right)$$\n\nThis is the final closed-form analytic expression for the expected increase in physical memory consumption in bytes.", "answer": "$$\n\\boxed{s P \\left(1 - \\left(1 - \\frac{1}{P}\\right)^k\\right)}\n$$", "id": "3620286"}]}