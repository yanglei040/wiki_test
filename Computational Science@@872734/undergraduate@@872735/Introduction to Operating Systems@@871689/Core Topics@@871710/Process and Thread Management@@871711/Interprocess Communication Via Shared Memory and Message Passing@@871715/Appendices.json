{"hands_on_practices": [{"introduction": "Effective shared memory programming requires an understanding of the underlying hardware. This first practice explores the critical concept of \"false sharing,\" a performance pitfall where logically independent variables cause contention simply because they occupy the same cache line. By analyzing how to place data to avoid this issue, you will gain a practical understanding of how memory layout interacts with cache coherence protocols to dictate the performance of parallel applications [@problem_id:3650171].", "problem": "A systems team is evaluating two designs for interprocess communication between two cooperating processes: message passing through operating system primitives and shared memory with in-place updates. To reduce system call overhead typical of message passing, they consider shared memory for two frequently updated counters. The hardware is a shared-memory multiprocessor with write-invalidate cache coherence and fixed cache line size $L$ bytes. A shared memory segment is created and mapped at a virtual base address chosen by the operating system; the mapping’s base address modulo $L$ is not controllable by user space and can be any residue in $\\{0,1,\\dots,L-1\\}$. Each counter is stored in a scalar object of size $s$ bytes (with $1 \\leq s \\leq L$), and the counters are placed at byte offsets $o_1$ and $o_2$ from the start of the shared memory segment, with $o_2 \\geq o_1$ and the two counters not overlapping in address.\n\nUse the following fundamental definitions as the basis of your reasoning:\n\n- A cache line is the unit of coherence; if two memory locations lie in the same cache line, writes by different processors to those locations can cause false sharing, manifesting as coherence traffic even when the locations are distinct.\n- An address $x$ resides in cache line index $\\left\\lfloor x / L \\right\\rfloor$. A contiguous memory region $[x, x + s - 1]$ occupies all cache lines whose indices are in the set $\\left\\{\\left\\lfloor \\frac{y}{L} \\right\\rfloor : y \\in [x, x + s - 1]\\right\\}$.\n\nLet $B$ denote the (unknown) virtual base address of the mapping, and define the two counter address ranges as $[B + o_1, B + o_1 + s - 1]$ and $[B + o_2, B + o_2 + s - 1]$. The goal is to choose the separation $d = o_2 - o_1$ to minimize cache line contention (eliminate false sharing) regardless of the value of $B \\pmod L$.\n\nDerive, from the above definitions and without assuming any special alignment of $B$, a closed-form expression for the minimal byte separation $d$ as a function of $L$ and $s$ such that, for every possible residue of $B \\pmod L$, the two counters do not occupy any common cache line. Provide your final expression for $d$ in bytes. No numerical evaluation or rounding is required; express your final result symbolically in terms of $L$ and $s$.", "solution": "The problem requires the derivation of the minimal byte separation, $d$, between two data objects (counters) in shared memory to guarantee they do not occupy any common cache line, regardless of the memory mapping's base address alignment. This condition must prevent false sharing.\n\nLet the cache line size be $L$ bytes. The two counters each have a size of $s$ bytes, where $1 \\le s \\le L$. The shared memory segment is mapped at a virtual base address $B$. The counters are placed at offsets $o_1$ and $o_2$ from this base address. The separation is $d = o_2 - o_1$. We assume, without loss of generality, that $o_2 \\ge o_1$.\n\nThe first counter occupies the contiguous memory address range $[B+o_1, B+o_1+s-1]$. Let us denote its start address as $A_1 = B+o_1$. The address range is $[A_1, A_1+s-1]$.\nThe second counter occupies the address range $[B+o_2, B+o_2+s-1]$. Its start address is $A_2 = B+o_2 = B+o_1+d = A_1+d$. The address range is $[A_2, A_2+s-1]$.\n\nAccording to the problem statement, an address $x$ resides in the cache line with index $\\lfloor x/L \\rfloor$. Two memory locations cause contention if they map to the same cache line index. To completely eliminate contention between the two counters, the set of cache line indices occupied by the first counter must be disjoint from the set of cache line indices for the second counter.\n\nThe set of cache line indices for the first counter, $C_1$, is given by all indices for addresses $y_1$ in its range: $C_1 = \\{\\lfloor y_1/L \\rfloor : y_1 \\in [A_1, A_1+s-1]\\}$. Since the function $\\lfloor \\cdot \\rfloor$ is non-decreasing, this set of indices is a contiguous range. The minimum index is $\\lfloor A_1/L \\rfloor$ and the maximum index is $\\lfloor (A_1+s-1)/L \\rfloor$.\n\nSimilarly, the set of cache line indices for the second counter, $C_2$, is $C_2 = \\{\\lfloor y_2/L \\rfloor : y_2 \\in [A_2, A_2+s-1]\\}$. The minimum index is $\\lfloor A_2/L \\rfloor$ and the maximum index is $\\lfloor (A_2+s-1)/L \\rfloor$.\n\nSince $o_2 \\ge o_1$, we have $A_2 \\ge A_1$. To ensure the sets of cache line indices $C_1$ and $C_2$ are disjoint, the maximum index in $C_1$ must be strictly less than the minimum index in $C_2$. This gives the necessary and sufficient condition:\n$$\n\\max(C_1) < \\min(C_2)\n$$\n$$\n\\lfloor \\frac{A_1+s-1}{L} \\rfloor < \\lfloor \\frac{A_2}{L} \\rfloor\n$$\nSubstituting $A_2 = A_1+d$, the condition becomes:\n$$\n\\lfloor \\frac{A_1+s-1}{L} \\rfloor < \\lfloor \\frac{A_1+d}{L} \\rfloor\n$$\nThis inequality must hold for any possible base address $B$. The problem states that the base address modulo $L$, which we denote as $B \\pmod L$, can be any residue in $\\{0, 1, \\dots, L-1\\}$. The start address of the first counter is $A_1 = B+o_1$. The offset $o_1$ is a fixed chosen value, while $B$ varies. The alignment of $A_1$ relative to a cache line boundary is given by $A_1 \\pmod L = (B+o_1) \\pmod L$. As $B \\pmod L$ can take any value from $0$ to $L-1$, so too can $(B+o_1) \\pmod L$ by an appropriate choice of $B$. Therefore, we must ensure the inequality holds for any possible alignment of $A_1$.\n\nLet $A_1 = kL+r$, where $k$ is an integer representing the base cache line index and $r \\in \\{0, 1, \\dots, L-1\\}$ is the byte offset within that line. Substituting this into the inequality:\n$$\n\\lfloor \\frac{kL+r+s-1}{L} \\rfloor < \\lfloor \\frac{kL+r+d}{L} \\rfloor\n$$\nUsing the property $\\lfloor z+n \\rfloor = \\lfloor z \\rfloor + n$ for integer $n$:\n$$\nk + \\lfloor \\frac{r+s-1}{L} \\rfloor < k + \\lfloor \\frac{r+d}{L} \\rfloor\n$$\n$$\n\\lfloor \\frac{r+s-1}{L} \\rfloor < \\lfloor \\frac{r+d}{L} \\rfloor\n$$\nThis is equivalent to requiring:\n$$\n\\lfloor \\frac{r+s-1}{L} \\rfloor + 1 \\le \\lfloor \\frac{r+d}{L} \\rfloor\n$$\nThis condition must be true for all possible residues $r \\in \\{0, 1, \\dots, L-1\\}$. To find the minimum $d$ that satisfies this for all $r$, we must analyze the worst-case scenario for $r$. Let's solve for $d$:\n$$\n\\frac{r+d}{L} \\ge \\lfloor \\frac{r+d}{L} \\rfloor \\ge \\lfloor \\frac{r+s-1}{L} \\rfloor + 1\n$$\nThis implies $r+d \\ge L \\left( \\lfloor \\frac{r+s-1}{L} \\rfloor + 1 \\right)$, so:\n$$\nd \\ge L \\left( \\lfloor \\frac{r+s-1}{L} \\rfloor + 1 \\right) - r\n$$\nTo find the minimum $d$ that works for all $r$, we must find the maximum value of the right-hand side expression over all possible $r$:\n$$\nd_{min} = \\max_{r \\in \\{0, \\dots, L-1\\}} \\left( L \\left( \\lfloor \\frac{r+s-1}{L} \\rfloor + 1 \\right) - r \\right)\n$$\nWe analyze the expression in two parts, based on the value of $\\lfloor (r+s-1)/L \\rfloor$.\nGiven $1 \\le s \\le L$ and $0 \\le r \\le L-1$, the term $r+s-1$ can range from $s-1$ to $2L-2$. So, $\\lfloor (r+s-1)/L \\rfloor$ can be either $0$ or $1$.\n\nCase 1: $r+s-1 < L$, which means $r < L-s+1$.\nIn this case, $\\lfloor (r+s-1)/L \\rfloor = 0$. The expression becomes $L(0+1) - r = L-r$. To maximize this value for $r$ in the range $[0, L-s]$, we choose the smallest $r$, which is $r=0$. The maximum value is $L-0=L$.\n\nCase 2: $r+s-1 \\ge L$, which means $r \\ge L-s+1$.\nIn this case, $\\lfloor (r+s-1)/L \\rfloor = 1$. The expression becomes $L(1+1) - r = 2L-r$. To maximize this value for $r$ in the range $[L-s+1, L-1]$, we choose the smallest $r$, which is $r=L-s+1$. The maximum value is $2L - (L-s+1) = L+s-1$.\n\nThe overall minimum required separation $d$ is the maximum of the requirements from these two cases:\n$$\nd_{min} = \\max(L, L+s-1)\n$$\nSince $s \\ge 1$, we have $s-1 \\ge 0$, which implies $L+s-1 \\ge L$. Therefore, the maximum of the two values is $L+s-1$.\n\nThe minimal byte separation required is $d = L+s-1$. This guarantees that for any possible alignment of the shared memory segment, the two counters will reside in completely separate cache lines, thus eliminating false sharing. The condition that the counters do not overlap, $d \\ge s$, is also satisfied since $L \\ge 1$ implies $L+s-1 \\ge s$.", "answer": "$$ \\boxed{L+s-1} $$", "id": "3650171"}, {"introduction": "Beyond performance, ensuring correctness is paramount in concurrent systems. This exercise transitions from hardware layout to the logical ordering of operations, a common source of subtle bugs. You will investigate a classic data race in a producer-consumer scenario caused by memory reordering on weakly-ordered architectures and discover how release-acquire semantics establish the necessary \"happens-before\" relationship to guarantee correctness [@problem_id:3650245]. This is a fundamental skill for writing reliable lock-free code.", "problem": "A single-producer/single-consumer (SPSC) queue is implemented as a ring buffer in shared memory mapped into two independent processes on a symmetric multiprocessor with a weak memory model. The producer process writes an element into slot index $i$ and then advances a shared index $tail$; the consumer process reads $tail$ to decide if data is available and then reads the element from the corresponding slot. All shared variables reside in ordinary shared memory without any explicit ordering primitives. The platform guarantees cache coherence (that is, per-location program order is preserved), but permits reordering of memory operations to different addresses by both the compiler and the Central Processing Unit (CPU).\n\nProducer pseudo-steps in program order:\n- Step $1$: store the payload value $v$ to $buf[i]$.\n- Step $2$: store the new index to $tail$ (for example, set $tail := i + 1 \\bmod N$, where $N$ is the ring capacity).\n\nConsumer pseudo-steps in program order:\n- Step $1$: load $t := tail$.\n- Step $2$: load $r := buf[j]$ for the appropriate $j$ determined by the consumer’s local $head$.\n\nIn testing, the consumer sometimes observes that $t$ indicates that a new element is available (that is, $t$ has advanced), but the payload read $r$ is the old value that was present before the producer’s write of $v$. Assume that $i$ and $j$ are the corresponding matching indices for a single element transfer.\n\nUsing only the following foundational bases:\n- The definition of cache coherence and per-location program order (that is, writes to the same address are observed in program order by all processors).\n- The definition of the happens-before relation and the effect of release/acquire synchronization: a release operation on a shared variable makes all prior writes in the releasing thread happen-before a matching acquire operation that reads-from that release, thereby preventing reordering and ensuring visibility of those writes to the acquiring thread.\n- The fact that weakly ordered architectures may reorder stores to different addresses and loads to different addresses in the absence of explicit ordering constraints.\n\nSelect the single option that simultaneously:\n- Provides a concrete, architecture-plausible sequence of events (naming the reordering categories) that explains how the consumer can see $tail$ updated but not the corresponding payload.\n- Proposes a minimal change that is sufficient to eliminate the bug on all weakly ordered architectures while preserving the non-blocking SPSC design.\n\nA. Diagnosis: The producer’s store to $tail$ can become visible before its prior store to $buf[i]$ (a store-followed-by-store reordering to different addresses), and the consumer’s load of $buf[j]$ can be issued and serviced after its load of $tail$ but still return the old value because the producer’s store to $buf[i]$ is not yet globally visible. A concrete interleaving is: producer executes Step $1$ then Step $2$ in program order, but other cores observe Step $2$ before Step $1$; consumer executes Step $1$ (sees advanced $tail$) and then Step $2$ (sees stale $buf[j]$). Minimal fix: make the producer’s store to $tail$ a release operation and the consumer’s load of $tail$ an acquire operation (for example, use C $11$ atomics with $memory\\_order\\_release$ for the producer’s $tail$ store and $memory\\_order\\_acquire$ for the consumer’s $tail$ load, or equivalently place a store-release barrier between producer Step $1$ and Step $2$ and a load-acquire barrier between consumer Step $1$ and Step $2$). This establishes a happens-before edge from the payload store to the consumer’s payload load.\n\nB. Diagnosis: The anomaly is caused solely by compiler reordering, not by hardware. Minimal fix: insert a compiler-only barrier (for example, an optimization fence) between producer Step $1$ and Step $2$; no change to the consumer.\n\nC. Diagnosis: The anomaly is caused solely by the consumer reordering its loads (load-followed-by-load reordering). Minimal fix: add an acquire fence or acquire semantics only on the consumer after reading $tail$; no changes on the producer.\n\nD. Diagnosis: The anomaly is caused by false sharing of $tail$ and the payload slot. Minimal fix: pad the data structure so that $tail$ and $buf[i]$ are on different cache lines; no memory ordering changes are needed.\n\nE. Diagnosis: The producer’s stores can be observed out of order, but consumer-side reordering is irrelevant. Minimal fix: insert a single full fence on the producer between Step $1$ and Step $2$; no ordering change on the consumer is needed.\n\nChoose the correct option.", "solution": "The user requires a critical validation of the problem statement, followed by a detailed derivation of the solution and an evaluation of all provided options.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- **System**: A single-producer/single-consumer (SPSC) queue implemented as a ring buffer in shared memory.\n- **Environment**: Two independent processes, symmetric multiprocessor (SMP), weak memory model.\n- **Shared Variables**: $buf$, the ring buffer; $tail$, a shared index. These reside in \"ordinary shared memory without any explicit ordering primitives.\"\n- **Memory Guarantees**:\n    - Cache coherence is guaranteed.\n    - Per-location program order is preserved (writes to the same address are observed in program order).\n    - Compiler and Central Processing Unit (CPU) are permitted to reorder memory operations to different addresses.\n- **Producer Logic (Program Order)**:\n    1. Store payload $v$ to $buf[i]$.\n    2. Store new index to $tail$ (e.g., $tail := i + 1 \\bmod N$).\n- **Consumer Logic (Program Order)**:\n    1. Load $t := tail$.\n    2. Load $r := buf[j]$ (where $j$ is derived from the consumer's local $head$ index).\n- **Observed Anomaly**: The consumer reads an advanced value of $t$ but reads an old, stale value of the payload $r$ from $buf[j]$, where indices $i$ and $j$ correspond to the same element transfer.\n- **Foundational Bases**: The solution must be based on the definitions of cache coherence, the happens-before relation with release/acquire semantics, and the properties of weakly ordered architectures.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded**: The problem is a classic and fundamental scenario in concurrent programming and systems design. The concepts described—weak memory models, memory reordering by compilers and CPUs, cache coherence, and the need for explicit memory ordering primitives (fences/barriers)—are core principles of modern computer architecture. The observed anomaly is a well-documented data race that occurs in naive lock-free implementations.\n- **Well-Posed**: The problem is clearly defined. It specifies the algorithm, the hardware/compiler environment, the observed failure mode, and the theoretical principles to be used for the analysis. The question asks for a specific diagnosis and a minimal, sufficient fix, which is a standard format for problems in this domain.\n- **Objective**: The problem statement is expressed using precise, technical language. There are no subjective or ambiguous terms. The \"pseudo-steps\" provide a clear, formal description of the program order of memory operations.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. It is scientifically sound, well-posed, and objective. It represents a real and important challenge in the field of operating systems and concurrent programming. The solution process can proceed.\n\n### Solution Derivation\n\nThe core issue arises from the \"weak memory model,\" which permits the reordering of memory operations to different addresses. The producer writes to two distinct memory locations: a slot in the buffer, $buf[i]$, and the shared index, $tail$. The consumer reads from these same two locations (with a matching index $j$).\n\nThe program order for the producer is:\n$1$. `store v, buf[i]`\n$2$. `store (i+1 mod N), tail`\n\nThe program order for the consumer is:\n$1$. `load t, tail`\n$2$. `load r, buf[j]`\n\nThe anomaly occurs when the consumer reads the new value of $tail$ but the old value of $buf[j]$. This implies that, from the consumer's perspective, the effect of the producer's second store has become visible, but the effect of the producer's first store has not. On a weakly ordered architecture, the memory system is permitted to make the write to $tail$ globally visible before the write to $buf[i]$ is visible, even if the producer's CPU executed them in program order. This phenomenon is often called Store-Store reordering. A producer core might write both values to its local store buffer, but the memory subsystem could commit the write to $tail$ to the shared cache/memory before committing the write to $buf[i]$.\n\nTo prevent this, a specific ordering must be enforced. The write to the data ($buf[i]$) must be guaranteed to be visible to all other processors *before* the write to the \"flag\" variable ($tail$) that indicates the data is ready. This creates a synchronization point.\n\nThe foundational principle of the happens-before relation via release/acquire semantics directly addresses this.\n- A **release operation** ensures that all memory writes in the thread's program order that precede the release operation are completed and visible to other threads before the release itself.\n- An **acquire operation** ensures that if it reads the value from a release operation, then all memory writes that happened-before that release are now visible to the current thread. It also prevents subsequent memory reads in the current thread from being reordered to before the acquire.\n\nThe correct and minimal fix is to apply these semantics to the synchronization variable, $tail$.\n\n1.  **Producer**: The store to $tail$ must be a **release store**. This makes the prior store to $buf[i]$ happen-before the store to $tail$. In effect, it \"publishes\" the data write.\n    - `store v, buf[i]`\n    - `store_release (i+1 mod N), tail`\n\n2.  **Consumer**: The load from $tail$ must be an **acquire load**. This ensures that if the consumer sees the updated $tail$ (thereby synchronizing with the producer's release), it is also guaranteed to see the updated $buf[i]$. It also prevents the load from $buf[j]$ from being reordered to occur before the load from $tail$.\n    - `load_acquire t, tail`\n    - `load r, buf[j]`\n\nThis release-acquire pairing establishes a happens-before edge from the producer's write to $buf[i]$ to the consumer's read from $buf[j]$, thus correctly solving the data race. This is the minimal fix because both sides must participate to establish the synchronization; a one-sided fix is insufficient on all general weak memory models.\n\n### Option-by-Option Analysis\n\n**A. Diagnosis: The producer’s store to $tail$ can become visible before its prior store to $buf[i]$ (a store-followed-by-store reordering to different addresses)... Minimal fix: make the producer’s store to $tail$ a release operation and the consumer’s load of $tail$ an acquire operation...**\n- **Analysis**: This option provides a precise and complete diagnosis. It correctly identifies that the reordering of the producer's stores (Store-Store reordering) is the root cause from the memory system's perspective. The proposed fix, using a release store on the producer and an acquire load on the consumer, is the canonical, correct, and minimal solution for this problem. It establishes the necessary happens-before relationship as described in the derivation above. The mention of C$11$ atomics is a correct and concrete example of implementing this.\n- **Verdict**: **Correct**.\n\n**B. Diagnosis: The anomaly is caused solely by compiler reordering, not by hardware. Minimal fix: insert a compiler-only barrier...**\n- **Analysis**: This diagnosis is incorrect. The problem explicitly states that the CPU on a weak memory model can also reorder operations. A compiler-only barrier (optimization fence) does not emit CPU instructions to constrain hardware reordering. Therefore, the bug would persist even with this fix. The diagnosis incorrectly limits the source of reordering.\n- **Verdict**: **Incorrect**.\n\n**C. Diagnosis: The anomaly is caused solely by the consumer reordering its loads... Minimal fix: add an acquire fence or acquire semantics only on the consumer...**\n- **Analysis**: The diagnosis is incomplete and thus incorrect as a sole explanation. While consumer-side Load-Load reordering can be a contributing factor on some architectures, the primary issue is the lack of guaranteed visibility ordering of the producer's writes. The proposed fix is insufficient. An acquire operation on the consumer must pair with a release operation on the producer to have a guaranteed synchronization effect. Without the producer's release, the acquire operation has nothing to synchronize with, and there is no guarantee that the producer's data write will be visible.\n- **Verdict**: **Incorrect**.\n\n**D. Diagnosis: The anomaly is caused by false sharing... Minimal fix: pad the data structure...**\n- **Analysis**: This diagnosis is incorrect. False sharing is a performance issue caused by multiple CPUs contending for the same cache line when writing to different variables. It does not cause the correctness issue of reordered visibility described in the problem. The anomaly is about the *order* of memory updates becoming visible, not cache line contention. The fix, padding, addresses false sharing but does nothing to enforce memory ordering.\n- **Verdict**: **Incorrect**.\n\n**E. Diagnosis: The producer’s stores can be observed out of order, but consumer-side reordering is irrelevant. Minimal fix: insert a single full fence on the producer...**\n- **Analysis**: The diagnosis is incomplete because consumer-side reordering can be relevant on certain weak architectures (e.g., ARM). The proposed fix is both not minimal and insufficient. It is not minimal because a full fence is a stronger (and more expensive) barrier than the required release semantic. It is insufficient because a producer-side fence does not prevent the consumer's CPU from reordering its own independent loads. A portable solution requires a barrier or acquire load on the consumer side as well.\n- **Verdict**: **Incorrect**.", "answer": "$$\\boxed{A}$$", "id": "3650245"}, {"introduction": "Building on the principles of correctness and low-level performance, we now turn to higher-level design trade-offs for scalability. A single lock can create a bottleneck, but how many locks are too many? This practice guides you through modeling the trade-off between reducing lock contention by partitioning data and the increasing management overhead of using more locks, allowing you to derive an optimal design mathematically [@problem_id:3650193]. This approach of modeling and optimizing is a key technique in performance engineering.", "problem": "A multicore program uses a shared memory region to coordinate work among $n$ threads. To avoid the high contention caused by a single global lock, the region can be partitioned into $s$ equal-sized sections, each protected by its own lock. Each operation maps uniformly at random to exactly one section (for example, by hashing a key to a section index), acquires that section’s lock, performs a constant-time critical update, and releases the lock. Assume the following empirically measured and theoretically motivated facts hold under a steady workload:\n\n- With a single lock (that is, $s=1$), the expected additional time per operation due purely to waiting for the lock (beyond the constant-time critical work and other unrelated costs) is $\\gamma$ (in time units). This $\\gamma$ captures the combined effect of the arrival rate, service time, and scheduler behavior under the current workload.\n- When the shared region is partitioned into $s$ independent, equally likely sections, and the mapping to sections is uniform and independent across operations, the contention-induced waiting time per operation scales inversely with $s$ because the contention is split evenly across the $s$ independent locks.\n- The presence of more locks incurs per-operation lock-management overhead due to larger lock arrays and cache effects. This overhead grows linearly with the number of sections, with a measured marginal cost $h$ (in time units) per section per operation. Thus, increasing $s$ by $1$ increases the expected per-operation lock-management overhead by $h$.\n\nStarting only from the above facts and the standard additivity of independent expected costs, model the expected per-operation time spent on synchronization (waiting plus lock-management overhead) as a function of $s$, $\\gamma$, and $h$. Treat $s$ as a positive real variable and ignore any constant terms that do not depend on $s$. Derive the value of $s$ that minimizes this expected synchronization time.\n\nGive your final answer as a single closed-form analytic expression in terms of $\\gamma$ and $h$. No rounding is required. $s$ is dimensionless; do not include units in your final expression.", "solution": "The problem requires us to model the expected per-operation synchronization time as a function of the number of sections, $s$, and then find the value of $s$ that minimizes this time. Let $T(s)$ represent the total expected per-operation synchronization time. The problem states that this time consists of two components: a contention-induced waiting time, which we will denote as $W(s)$, and a lock-management overhead, denoted as $O(s)$.\n\nThe total synchronization time is the sum of these two components:\n$$T(s) = W(s) + O(s)$$\n\nFirst, we must establish the functional form of $W(s)$. The problem states that the \"contention-induced waiting time per operation scales inversely with $s$\". This relationship can be expressed mathematically as $W(s) = \\frac{k}{s}$, where $k$ is a constant of proportionality. We are also given the condition that \"with a single lock (that is, $s=1$), the expected additional time per operation due purely to waiting for the lock ... is $\\gamma$\". This allows us to determine the constant $k$. By setting $s=1$ in our model for $W(s)$, we get $W(1) = \\frac{k}{1} = k$. Since we are given $W(1) = \\gamma$, it follows that $k = \\gamma$. Therefore, the contention-induced waiting time is given by:\n$$W(s) = \\frac{\\gamma}{s}$$\n\nNext, we establish the functional form of $O(s)$. The problem states that the \"per-operation lock-management overhead...grows linearly with the number of sections, with a measured marginal cost $h...$ per section per operation\". A linear growth with a marginal cost $h$ means that the part of the overhead that depends on $s$ is given by the function $O(s) = h \\cdot s$. The instruction to \"ignore any constant terms that do not depend on $s$\" validates this model, as we are only concerned with the terms that vary with $s$ to find the optimal value of $s$. Thus, we have:\n$$O(s) = h \\cdot s$$\n\nCombining these two components, we obtain the total expected synchronization time as a function of $s$:\n$$T(s) = \\frac{\\gamma}{s} + h \\cdot s$$\n\nTo find the value of $s$ that minimizes $T(s)$, we can use calculus, treating $s$ as a positive real variable as specified. We compute the first derivative of $T(s)$ with respect to $s$ and set it to zero to find the critical points.\n\nThe derivative of $T(s)$ is:\n$$\\frac{dT}{ds} = \\frac{d}{ds} \\left( \\frac{\\gamma}{s} + h \\cdot s \\right) = \\frac{d}{ds}(\\gamma s^{-1}) + \\frac{d}{ds}(hs)$$\n$$\\frac{dT}{ds} = -\\gamma s^{-2} + h = -\\frac{\\gamma}{s^2} + h$$\n\nSetting the derivative to zero yields the condition for the minimum:\n$$-\\frac{\\gamma}{s^2} + h = 0$$\n$$h = \\frac{\\gamma}{s^2}$$\n\nWe can now solve for $s$. Since $s$ must be positive (it represents the number of sections), we can rearrange the equation as follows:\n$$s^2 = \\frac{\\gamma}{h}$$\nTaking the positive square root gives the optimal value of $s$:\n$$s = \\sqrt{\\frac{\\gamma}{h}}$$\nThe parameters $\\gamma$ and $h$ are time costs, so they are positive real numbers. Their ratio is a positive, dimensionless quantity, so its square root is a well-defined positive real number.\n\nTo verify that this critical point corresponds to a minimum, we examine the second derivative of $T(s)$:\n$$\\frac{d^2T}{ds^2} = \\frac{d}{ds} \\left( -\\frac{\\gamma}{s^2} + h \\right) = \\frac{d}{ds}(-\\gamma s^{-2})$$\n$$\\frac{d^2T}{ds^2} = -(-2)\\gamma s^{-3} = \\frac{2\\gamma}{s^3}$$\n\nGiven that $\\gamma > 0$ (a waiting time) and we are considering $s > 0$ (number of sections), the second derivative $\\frac{d^2T}{ds^2}$ is strictly positive for all $s$ in the domain of interest. A positive second derivative indicates that the function $T(s)$ is strictly convex for $s > 0$, which confirms that the value of $s$ we found corresponds to a unique global minimum.", "answer": "$$\\boxed{\\sqrt{\\frac{\\gamma}{h}}}$$", "id": "3650193"}]}