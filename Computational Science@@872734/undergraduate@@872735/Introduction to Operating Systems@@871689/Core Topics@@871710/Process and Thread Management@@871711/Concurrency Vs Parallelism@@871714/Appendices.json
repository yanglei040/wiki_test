{"hands_on_practices": [{"introduction": "A common misconception is that more threads always lead to more speed. This exercise explores the fundamental limit to parallel speedup described by Amdahl's Law. By analyzing a hypothetical server with a significant serial bottleneck, you will quantify how even a small non-parallelizable section can severely cap the performance gains from adding more processor cores [@problem_id:3626997].", "problem": "A server application runs on a multicore Central Processing Unit (CPU) under an Operating System (OS) scheduler. The application spawns $N$ worker threads that process client requests. Each request consists of two phases: a local computation phase that does not access any shared state, followed by a critical section that updates a shared log protected by a mutual exclusion lock. Because the lock allows only one thread to execute the critical section at a time, the critical section is inherently serial even in the presence of many runnable threads. Assume the OS scheduler yields many interleavings among the $N$ threads, so concurrency (interleavings) is high, but the critical section dominates request time.\n\nSuppose the measured time per request on a single core is as follows: the local computation phase takes $c = 3$ milliseconds, and the critical section takes $s = 7$ milliseconds. Assume ideal parallel scaling for the local computation phase across $M$ identical cores, no overheads from synchronization beyond the serialization of the critical section itself, and that the critical section does not benefit from additional cores.\n\nStarting from the foundational definitions that speedup $S(M)$ is the ratio $T(1)/T(M)$, that the parallelizable fraction $p$ of the single-core execution time is the portion of work that can run concurrently across multiple cores, and that the serial fraction $1 - p$ does not accelerate with more cores, derive the expression for $S(M)$ in terms of $p$ and $M$. Then, compute the parallelizable fraction $p$ for this workload from $c$ and $s$, and evaluate the speedup for $M \\in \\{1, 2, 8\\}$. Express your final answer as a row matrix containing the three speedups corresponding to $M = 1$, $M = 2$, and $M = 8$. Do not include any units in your final numerical values.", "solution": "The problem requires the derivation of the speedup formula known as Amdahl's Law and its application to a specific workload on a multi-core processor.\n\nFirst, we perform the problem validation.\n\n**Step 1: Extract Givens**\n- Application spawns $N$ worker threads.\n- Each request has a local computation phase and a critical section phase.\n- Critical section is protected by a mutual exclusion lock, making it inherently serial.\n- Time for local computation per request on a single core: $c = 3$ milliseconds.\n- Time for critical section per request on a single core: $s = 7$ milliseconds.\n- Ideal parallel scaling is assumed for the local computation phase across $M$ cores.\n- The critical section does not benefit from additional cores.\n- No synchronization overheads beyond serialization.\n- Speedup definition: $S(M) = T(1)/T(M)$, where $T(M)$ is the execution time on $M$ cores.\n- Parallelizable fraction of single-core execution time: $p$.\n- Serial fraction of single-core execution time: $1-p$.\n- Task: Derive the expression for $S(M)$ in terms of $p$ and $M$.\n- Task: Compute $p$ from $c$ and $s$.\n- Task: Evaluate $S(M)$ for $M \\in \\{1, 2, 8\\}$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded**: The problem is a classic application of Amdahl's Law, a fundamental principle in parallel computing used to predict the theoretical speedup when using multiple processors. All concepts (critical section, parallel fraction, speedup) are standard in computer science and operating systems.\n- **Well-Posed**: The problem provides all necessary definitions and data ($c$, $s$) to derive the formula and calculate the required values. The objectives are stated clearly. A unique solution exists.\n- **Objective**: The problem is stated in precise, technical language, free from subjective or ambiguous terms.\n\nThe problem is valid as it is scientifically sound, well-posed, and objective. We may proceed with the solution.\n\n**Derivation of Speedup $S(M)$**\n\nLet $T(1)$ be the total execution time of a task on a single processor core. This total time can be partitioned into a serial component, which cannot be parallelized, and a parallelizable component.\n\nThe serial fraction is given as $1-p$. The time spent on the serial part is therefore $(1-p)T(1)$. This time is constant regardless of the number of cores $M$.\n\nThe parallelizable fraction is given as $p$. The time spent on the parallelizable part on a single core is $pT(1)$. With ideal parallel scaling, this work can be distributed across $M$ cores, reducing the time for this portion to $\\frac{pT(1)}{M}$.\n\nThe total execution time on $M$ cores, $T(M)$, is the sum of the time for the serial part and the time for the parallelized part:\n$$T(M) = (1-p)T(1) + \\frac{pT(1)}{M}$$\nWe can factor out $T(1)$:\n$$T(M) = T(1) \\left( (1-p) + \\frac{p}{M} \\right)$$\nThe speedup $S(M)$ is defined as the ratio of the single-core execution time to the multi-core execution time:\n$$S(M) = \\frac{T(1)}{T(M)}$$\nSubstituting the expression for $T(M)$:\n$$S(M) = \\frac{T(1)}{T(1) \\left( (1-p) + \\frac{p}{M} \\right)}$$\n$$S(M) = \\frac{1}{(1-p) + \\frac{p}{M}}$$\nThis is the general expression for speedup in terms of the parallelizable fraction $p$ and the number of processors $M$, known as Amdahl's Law.\n\n**Calculation of the Parallelizable Fraction $p$**\n\nFor the given workload, the total execution time for a single request on a single core, $T(1)$, is the sum of the local computation time $c$ and the critical section time $s$.\n$$T(1) = c + s$$\nThe local computation is the part of the work that can be run in parallel. Thus, its duration $c$ corresponds to the parallelizable portion of the single-core execution time. The critical section is inherently serial, so its duration $s$ corresponds to the serial portion.\n\nThe parallelizable fraction $p$ is the ratio of the parallelizable time to the total time:\n$$p = \\frac{c}{c+s}$$\nSubstituting the given values $c=3$ and $s=7$:\n$$p = \\frac{3}{3+7} = \\frac{3}{10} = 0.3$$\nThe serial fraction is $1-p = 1 - 0.3 = 0.7$. This corresponds to $\\frac{s}{c+s} = \\frac{7}{10} = 0.7$, which is consistent.\n\n**Evaluation of Speedup for $M \\in \\{1, 2, 8\\}$**\n\nNow we use the derived speedup formula with $p=0.3$:\n$$S(M) = \\frac{1}{(1-0.3) + \\frac{0.3}{M}} = \\frac{1}{0.7 + \\frac{0.3}{M}}$$\n\nCase 1: $M=1$\n$$S(1) = \\frac{1}{0.7 + \\frac{0.3}{1}} = \\frac{1}{0.7 + 0.3} = \\frac{1}{1} = 1$$\nThis is the expected baseline speedup.\n\nCase 2: $M=2$\n$$S(2) = \\frac{1}{0.7 + \\frac{0.3}{2}} = \\frac{1}{0.7 + 0.15} = \\frac{1}{0.85}$$\nTo express this as an exact fraction:\n$$S(2) = \\frac{1}{\\frac{85}{100}} = \\frac{100}{85} = \\frac{20}{17}$$\n\nCase 3: $M=8$\n$$S(8) = \\frac{1}{0.7 + \\frac{0.3}{8}}$$\nTo express this as an exact fraction, we convert the denominator to a common base:\n$$0.7 = \\frac{7}{10} = \\frac{56}{80}$$\n$$\\frac{0.3}{8} = \\frac{\\frac{3}{10}}{8} = \\frac{3}{80}$$\nSo, the denominator is:\n$$\\frac{56}{80} + \\frac{3}{80} = \\frac{59}{80}$$\nTherefore, the speedup is:\n$$S(8) = \\frac{1}{\\frac{59}{80}} = \\frac{80}{59}$$\n\nThe three computed speedup values are $S(1)=1$, $S(2)=\\frac{20}{17}$, and $S(8)=\\frac{80}{59}$.", "answer": "$$\\boxed{\\begin{pmatrix} 1 & \\frac{20}{17} & \\frac{80}{59} \\end{pmatrix}}$$", "id": "3626997"}, {"introduction": "The choice of a concurrency model has profound implications for system performance, as different models impose different overheads. This practice moves from abstract theory to a concrete architectural trade-off: a classic thread-per-connection model versus a modern asynchronous I/O design. You will use Little's Law, $L = \\lambda W$, to quantify the performance impact of these overheads and determine which approach is more efficient for a CPU-bound server at scale [@problem_id:3627006].", "problem": "A network server maintains $10\\,000$ long-lived Transmission Control Protocol (TCP) connections and handles one outstanding request per connection at any given time. The server runs on a symmetric multiprocessing machine with $4$ identical cores. Each request requires base Central Processing Unit (CPU) work of $0.9 \\times 10^{-3}$ seconds in user space. Two implementation designs are considered:\n\n- Thread-per-connection: Each request is handled by its own thread. The concurrency overhead consists of context switches and scheduler bookkeeping. Each context switch costs $5 \\times 10^{-6}$ seconds, and there are $12$ context switches per request on average due to blocking and unblocking on Input/Output (I/O). Scheduler bookkeeping adds a fixed $60 \\times 10^{-6}$ seconds per request.\n- Asynchronous I/O: Requests are handled by a single event loop using non-blocking I/O. The concurrency overhead consists of event loop poll/dispatch work. There are $8$ event loop poll/dispatch operations per request, each costing $3 \\times 10^{-6}$ seconds, plus a fixed $15 \\times 10^{-6}$ seconds per request for dispatch bookkeeping.\n\nAssume the system is CPU-bound at saturation, and ignore all non-CPU bottlenecks. Let $L$ denote the number of outstanding requests in the system, $\\lambda$ the system-wide request completion rate (throughput), and $W$ the average time a request spends in the system (latency). Use Little’s Law, $L = \\lambda W$, together with the parallelism ceiling imposed by the $4$ cores, to derive the ratio of the saturated average time in the system for thread-per-connection relative to asynchronous I/O, $\\frac{W_{\\text{thread}}}{W_{\\text{async}}}$.\n\nRound your final ratio to four significant figures. Express the answer as a pure number with no units.", "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n### Step 1: Extract Givens\n- Number of TCP connections: $N_{\\text{conn}} = 10\\,000$\n- Outstanding requests per connection: $1$\n- Number of identical CPU cores: $N_{\\text{cores}} = 4$\n- Base CPU work per request: $T_{\\text{base}} = 0.9 \\times 10^{-3} \\text{ s}$\n- **Thread-per-connection model:**\n  - Cost per context switch: $T_{\\text{cs}} = 5 \\times 10^{-6} \\text{ s}$\n  - Average context switches per request: $N_{\\text{cs}} = 12$\n  - Scheduler bookkeeping cost per request: $T_{\\text{sched}} = 60 \\times 10^{-6} \\text{ s}$\n- **Asynchronous I/O model:**\n  - Number of event loop operations per request: $N_{\\text{poll}} = 8$\n  - Cost per event loop operation: $T_{\\text{poll}} = 3 \\times 10^{-6} \\text{ s}$\n  - Dispatch bookkeeping cost per request: $T_{\\text{dispatch}} = 15 \\times 10^{-6} \\text{ s}$\n- **System State and Laws:**\n  - The system is CPU-bound at saturation.\n  - Total number of outstanding requests: $L$\n  - System-wide request completion rate (throughput): $\\lambda$\n  - Average time a request spends in the system (latency): $W$\n  - Little’s Law: $L = \\lambda W$\n- **Objective:**\n  - Calculate the ratio $\\frac{W_{\\text{thread}}}{W_{\\text{async}}}$ and round to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is a standard performance analysis question in computer science, specifically in the context of operating systems and network servers.\n- **Scientifically Grounded:** The problem uses established concepts like concurrency models (threading vs. asynchronous I/O), context switching overhead, CPU-bound systems, and Little's Law. The provided values for CPU times and overheads are plausible. The model is a simplified but valid representation of a real-world performance trade-off.\n- **Well-Posed:** The problem provides all necessary information to calculate the total CPU work per request for both models. The assumption of a CPU-bound saturated system allows for the determination of maximum throughput. Little's Law provides the necessary relationship to find the latency. The objective is clear and a unique solution is achievable.\n- **Objective:** The problem is stated using precise, quantitative, and unbiased language.\n\nThe problem does not violate any of the invalidity criteria. It is complete, consistent, and formalizable.\n\n### Step 3: Verdict and Action\nThe problem is valid. A solution will be provided.\n\nThe core of the problem is to determine the average latency, $W$, for each model. The latency depends on the total number of requests in the system, $L$, and the system's throughput, $\\lambda$, as described by Little's Law, $L = \\lambda W$.\n\nFirst, we determine the total number of outstanding requests, $L$. The system maintains $N_{\\text{conn}} = 10\\,000$ connections, each with one outstanding request. Therefore, the total number of requests in the system at any time is constant:\n$$L = N_{\\text{conn}} \\times 1 = 10\\,000$$\n\nNext, we calculate the total CPU service time required to process a single request for each model. This service time, denoted by $S$, is the sum of the base CPU work and all concurrency-related overheads.\n\nFor the thread-per-connection model, the service time, $S_{\\text{thread}}$, is the sum of the base work, the total context switch cost, and the scheduler bookkeeping cost.\n$$S_{\\text{thread}} = T_{\\text{base}} + N_{\\text{cs}} \\times T_{\\text{cs}} + T_{\\text{sched}}$$\nSubstituting the given values:\n$$S_{\\text{thread}} = (0.9 \\times 10^{-3}) + (12 \\times 5 \\times 10^{-6}) + (60 \\times 10^{-6})$$\nTo facilitate addition, we express all terms in seconds with the same exponent:\n$$S_{\\text{thread}} = (900 \\times 10^{-6}) + (60 \\times 10^{-6}) + (60 \\times 10^{-6})$$\n$$S_{\\text{thread}} = (900 + 60 + 60) \\times 10^{-6} = 1020 \\times 10^{-6} \\text{ s} = 1.02 \\times 10^{-3} \\text{ s}$$\n\nFor the asynchronous I/O model, the service time, $S_{\\text{async}}$, is the sum of the base work, the total event loop polling cost, and the dispatch bookkeeping cost.\n$$S_{\\text{async}} = T_{\\text{base}} + N_{\\text{poll}} \\times T_{\\text{poll}} + T_{\\text{dispatch}}$$\nSubstituting the given values:\n$$S_{\\text{async}} = (0.9 \\times 10^{-3}) + (8 \\times 3 \\times 10^{-6}) + (15 \\times 10^{-6})$$\nAgain, expressing all terms in a common form:\n$$S_{\\text{async}} = (900 \\times 10^{-6}) + (24 \\times 10^{-6}) + (15 \\times 10^{-6})$$\n$$S_{\\text{async}} = (900 + 24 + 15) \\times 10^{-6} = 939 \\times 10^{-6} \\text{ s} = 0.939 \\times 10^{-3} \\text{ s}$$\n\nThe problem states the system is CPU-bound at saturation. This means the $N_{\\text{cores}} = 4$ CPUs are the sole bottleneck and are fully utilized ($100\\%$). The total processing capacity of the system is $N_{\\text{cores}}$ CPU-seconds per second of real time. The maximum system-wide throughput, $\\lambda$, is this total capacity divided by the CPU service time per request, $S$.\n$$\\lambda = \\frac{N_{\\text{cores}}}{S}$$\n\nNow we can use Little's Law, $L = \\lambda W$, to find the average latency $W$:\n$$W = \\frac{L}{\\lambda} = \\frac{L}{N_{\\text{cores}} / S} = \\frac{L \\cdot S}{N_{\\text{cores}}}$$\n\nWe can write the latencies for the two models as:\n$$W_{\\text{thread}} = \\frac{L \\cdot S_{\\text{thread}}}{N_{\\text{cores}}}$$\n$$W_{\\text{async}} = \\frac{L \\cdot S_{\\text{async}}}{N_{\\text{cores}}}$$\n\nThe problem asks for the ratio $\\frac{W_{\\text{thread}}}{W_{\\text{async}}}$.\n$$\\frac{W_{\\text{thread}}}{W_{\\text{async}}} = \\frac{\\frac{L \\cdot S_{\\text{thread}}}{N_{\\text{cores}}}}{\\frac{L \\cdot S_{\\text{async}}}{N_{\\text{cores}}}}$$\nSince $L$ and $N_{\\text{cores}}$ are the same for both scenarios, they cancel out, simplifying the ratio to the ratio of the service times:\n$$\\frac{W_{\\text{thread}}}{W_{\\text{async}}} = \\frac{S_{\\text{thread}}}{S_{\\text{async}}}$$\n\nSubstituting the calculated service times:\n$$\\frac{W_{\\text{thread}}}{W_{\\text{async}}} = \\frac{1020 \\times 10^{-6}}{939 \\times 10^{-6}} = \\frac{1020}{939}$$\nPerforming the division:\n$$\\frac{1020}{939} \\approx 1.08626198...$$\n\nThe problem requires rounding the final answer to four significant figures. The first four significant figures are $1.086$. The fifth digit is $2$, which is less than $5$, so we round down (i.e., truncate).\n$$\\frac{W_{\\text{thread}}}{W_{\\text{async}}} \\approx 1.086$$", "answer": "$$\\boxed{1.086}$$", "id": "3627006"}, {"introduction": "Properly provisioning a system is a critical engineering task that requires balancing multiple constraints. This exercise challenges you to derive a rule for sizing a thread pool for a workload that mixes CPU work with blocking I/O. The goal is to find a principle that provides enough concurrency to hide I/O latency while also providing enough work to fully utilize the available parallel hardware (CPU cores), synthesizing the core concepts of this topic [@problem_id:3627021].", "problem": "A server uses a fixed-size thread pool to handle independent client requests. Each request alternates between compute phases on the Central Processing Unit (CPU) and blocking Input/Output (I/O) waits. Let $C$ denote the mean CPU compute time per request and $W$ the mean blocking time per request, with $C, W > 0$, and model each thread as cycling between compute (busy on a core) and wait (blocked on I/O). The machine has $M$ identical CPU cores available to execute compute-ready threads. Requests arrive at an average rate $\\lambda$ requests per second. The system operator targets a steady-state average response time no greater than $R^\\star$ seconds. The thread pool size is $k$ threads.\n\nFrom first principles, propose a rule for choosing $k$ that simultaneously captures:\n- the concurrency requirement to cover I/O latency (so that enough requests can be in-flight to achieve the target average response time), and\n- the parallelism limit imposed by the $M$ cores (so that compute throughput is not starved, nor cores left idle due to blocking).\n\nSelect the single best option.\n\nA. Choose $k = \\lceil M \\rceil$.\n\nB. Choose $k = \\left\\lceil M \\cdot \\left(1 + \\frac{W}{C}\\right) \\right\\rceil$.\n\nC. Choose $k = \\left\\lceil \\max\\!\\left( M \\cdot \\left(1 + \\frac{W}{C}\\right),\\, \\lambda \\cdot R^\\star \\right) \\right\\rceil$.\n\nD. Choose $k = \\left\\lceil \\lambda \\cdot (C + W) \\right\\rceil$.\n\nE. Choose $k = \\left\\lceil \\frac{M}{1 + \\frac{W}{C}} \\right\\rceil$.", "solution": "The user wants me to solve a problem about sizing a thread pool.\n\n### Step 1: Validate the Problem Statement\n\nI will begin by validating the problem statement as per the instructions.\n\n#### Extract Givens\n- A server uses a fixed-size thread pool of size $k$.\n- The server handles independent client requests.\n- Each request alternates between compute phases (CPU) and blocking Input/Output (I/O) waits.\n- $C$: mean CPU compute time per request, with $C > 0$.\n- $W$: mean blocking I/O wait time per request, with $W > 0$.\n- The system has $M$ identical CPU cores.\n- $\\lambda$: average arrival rate of requests (requests per second).\n- $R^\\star$: target steady-state average response time (seconds).\n\n#### Validate Using Extracted Givens\nThe problem asks for a rule to choose the thread pool size $k$ to satisfy two requirements:\n1.  A concurrency requirement to cover I/O latency and meet the response time target.\n2.  A parallelism limit to effectively utilize the $M$ CPU cores.\n\nThe problem statement is scientifically grounded, well-posed, and objective. It describes a classic and fundamental problem in computer systems performance modeling. The parameters ($C$, $W$, $M$, $\\lambda$, $R^\\star$) are standard metrics used in queuing theory and system analysis. The model, which uses mean values for CPU and I/O time, is a common and valid simplification for first-order analysis. The problem is not ambiguous, contradictory, or based on false premises. It requires the derivation of a sizing heuristic from first principles, which is a standard engineering task.\n\n#### Verdict\nThe problem statement is **valid**. I will proceed with the derivation and solution.\n\n### Step 2: Derivation from First Principles\n\nThe problem requires a rule for determining the thread pool size, $k$, that simultaneously addresses two distinct constraints: one related to hardware utilization (parallelism) and one related to service level objectives (concurrency).\n\n**1. Parallelism Limit: Saturating the CPU Cores**\n\nThe first goal is to choose $k$ such that the $M$ CPU cores are kept busy and not left idle due to threads blocking on I/O.\n- Let's consider a single thread. In one complete cycle of its work on a request, it spends $C$ time units in a runnable state (either computing or ready to compute) and $W$ time units in a blocked state (waiting for I/O).\n- The total time for one cycle is $C+W$.\n- The fraction of time a thread is runnable (i.e., requires a CPU core) is $f_{cpu} = \\frac{C}{C+W}$.\n- The fraction of time a thread is blocked on I/O is $f_{io} = \\frac{W}{C+W}$.\n\nWith a pool of $k$ threads, the average number of threads that are in the runnable state at any given time is $k \\cdot f_{cpu} = k \\cdot \\frac{C}{C+W}$.\nTo fully saturate all $M$ cores, we need the number of runnable threads to be at least equal to the number of cores, $M$. The ideal point to aim for is where the number of runnable threads is exactly $M$.\n$$\nk \\cdot \\frac{C}{C+W} = M\n$$\nSolving for $k$ gives the number of threads required to keep the CPUs saturated:\n$$\nk_{cpu} = M \\cdot \\frac{C+W}{C} = M \\cdot \\left(1 + \\frac{W}{C}\\right)\n$$\nThis formula, often known as a utilization-based sizing rule, provides the number of threads needed to ensure that, on average, there are exactly $M$ threads ready to run, thus keeping the $M$ cores fully utilized. Any fewer threads, and cores would be idle whenever more than $k-M$ threads become blocked. This formula captures the parallelism limit.\n\n**2. Concurrency Requirement: Meeting the Response Time Target**\n\nThe second goal is to have enough threads to handle the incoming workload and achieve an average response time of $R^\\star$ or less.\n- This can be analyzed using Little's Law, a fundamental theorem in queuing theory, which states that the long-term average number of customers in a stable system, $L$, is equal to the long-term average effective arrival rate, $\\lambda$, multiplied by the average time a customer spends in the system, $R$.\n$$\nL = \\lambda \\cdot R\n$$\n- In our context, \"customers\" are the client requests. The \"system\" is the server. The time a request spends in the system is its response time.\n- The system must support an arrival rate of $\\lambda$ and aims for an average response time of $R^\\star$.\n- Applying Little's Law, the average number of concurrent requests in the system to meet this target is:\n$$\nL^\\star = \\lambda \\cdot R^\\star\n$$\n- Each request that is \"in-flight\" (being processed by the server) requires a thread from the pool. Therefore, to support an average of $L^\\star$ concurrent requests, the thread pool must have at least that many threads.\n$$\nk_{latency} \\ge \\lambda \\cdot R^\\star\n$$\nThis formula captures the concurrency requirement driven by the service level objective.\n\n**3. Combining the Requirements**\n\nThe problem asks for a single rule that captures *both* requirements. The thread pool must be large enough to satisfy the stricter of the two constraints.\n- It must be large enough to keep the CPUs busy: $k \\ge M \\cdot \\left(1 + \\frac{W}{C}\\right)$.\n- It must be large enough to support the required concurrency for the latency target: $k \\ge \\lambda \\cdot R^\\star$.\n\nTo satisfy both conditions simultaneously, $k$ must be greater than or equal to the maximum of the two required values:\n$$\nk \\ge \\max\\left( M \\cdot \\left(1 + \\frac{W}{C}\\right),\\, \\lambda \\cdot R^\\star \\right)\n$$\nSince the thread pool size $k$ must be an integer, the smallest integer value for $k$ that satisfies this condition is the ceiling of the expression.\n$$\nk = \\left\\lceil \\max\\left( M \\cdot \\left(1 + \\frac{W}{C}\\right),\\, \\lambda \\cdot R^\\star \\right) \\right\\rceil\n$$\nThis composite rule ensures the system is provisioned to handle both the hardware-level parallelism and the application-level concurrency targets.\n\n### Step 3: Option-by-Option Analysis\n\nNow, I will evaluate each option based on the derived principles.\n\n**A. Choose $k = \\lceil M \\rceil$.**\nAs $M$ (the number of cores) is an integer, this simplifies to $k=M$. This rule sets the number of threads equal to the number of available cores. This is an appropriate strategy only for workloads that are purely CPU-bound (i.e., $W=0$). For the given problem, where requests have blocking I/O waits ($W > 0$), if all $M$ threads block on I/O simultaneously, the CPU cores will become idle. This fails to satisfy the requirement of keeping the cores busy. Therefore, this option is insufficient.\n**Verdict: Incorrect.**\n\n**B. Choose $k = \\left\\lceil M \\cdot \\left(1 + \\frac{W}{C}\\right) \\right\\rceil$.**\nThis formula corresponds exactly to the $k_{cpu}$ derived to keep the $M$ cores saturated. It correctly addresses the parallelism limit. However, it completely ignores the second requirement concerning the arrival rate $\\lambda$ and the target response time $R^\\star$. If the concurrency level required to meet the service target, $\\lambda \\cdot R^\\star$, is significantly larger than $M \\cdot (1 + W/C)$, this rule would lead to a thread pool that is too small, causing requests to be rejected or to queue for excessively long times before being assigned a thread, thus failing to meet the response time target. As it only captures one of the two requirements, it is incomplete.\n**Verdict: Incorrect.**\n\n**C. Choose $k = \\left\\lceil \\max\\!\\left( M \\cdot \\left(1 + \\frac{W}{C}\\right),\\, \\lambda \\cdot R^\\star \\right) \\right\\rceil$.**\nThis formula incorporates both lower bounds that we derived from first principles. The term $M \\cdot (1 + W/C)$ ensures the pool is large enough to keep the hardware utilized. The term $\\lambda \\cdot R^\\star$ ensures the pool is large enough to support the number of concurrent requests needed to meet the performance target. By taking the maximum of these two terms, the rule guarantees that the chosen pool size $k$ is sufficient to meet both the hardware-centric requirement and the service-level-centric requirement. This rule robustly and simultaneously captures both aspects of the problem.\n**Verdict: Correct.**\n\n**D. Choose $k = \\left\\lceil \\lambda \\cdot (C + W) \\right\\rceil$.**\nThe term $C+W$ represents the total ideal service time of a request, excluding any queuing delays. The expression $\\lambda \\cdot (C+W)$ is an application of Little's Law to find the number of requests in the system if the response time were exactly equal to the service time. This is the definition of the system's offered load. While this value is related to the required concurrency, it completely ignores the number of cores $M$, failing to address the parallelism limit. Furthermore, it assumes the target response time $R^\\star$ is equal to the ideal service time $C+W$, which is generally not the case, as $R^\\star$ must typically account for queuing delays.\n**Verdict: Incorrect.**\n\n**E. Choose $k = \\left\\lceil \\frac{M}{1 + \\frac{W}{C}} \\right\\rceil$.**\nThis formula can be rewritten as $k = \\lceil M \\cdot \\frac{C}{C+W} \\rceil$. Since $C, W > 0$, the fraction $\\frac{C}{C+W}$ is always less than $1$. Therefore, this rule would suggest a thread pool size $k \\le M$. If there are fewer threads than cores, it is impossible to ever fully utilize all $M$ cores. This directly contradicts the goal of saturating the CPUs and is conceptually the inverse of the correct logic for handling I/O-bound tasks.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{C}$$", "id": "3627021"}]}