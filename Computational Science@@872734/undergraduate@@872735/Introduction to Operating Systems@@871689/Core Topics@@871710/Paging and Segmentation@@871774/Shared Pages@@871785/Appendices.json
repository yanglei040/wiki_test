{"hands_on_practices": [{"introduction": "The fundamental motivation for using shared pages in an operating system is to conserve physical memory. This exercise will help you quantify this advantage by calculating the total memory required for a group of processes under two scenarios: one where read-only code pages are shared, and one where they are not. By comparing these two figures, you will gain a concrete understanding of the significant efficiency gains provided by page sharing. [@problem_id:3622953]", "problem": "An Operating System (OS) uses demand paging with a fixed page size, and physical memory is organized into frames of the same size. Under paging, each virtual memory page that is resident occupies exactly one physical frame. A single program is launched as a set of concurrent processes, all executing the same read-only code segment that has been compiled into $n$ code pages. Each process also has $m$ private data pages that can be written and therefore cannot be shared. Assume all $k$ processes run concurrently and that every code and data page of each process is resident in memory at the same time. Ignore all kernel pages, page table storage, and other metadata overhead. Assume the only pages that may be shared are read-only code pages and that sharing does not create duplicate physical frames for shared pages.\n\nBased on these definitions and assumptions, determine the total number of physical frames required in two scenarios:\n- With code-page sharing across processes.\n- With no sharing of any pages across processes.\n\nExpress your final answer as a single row matrix whose first entry is the total frames with sharing and whose second entry is the total frames without sharing. No rounding is required.", "solution": "The problem statement is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\nThe givens are:\n- The system uses demand paging with a fixed page size equal to the physical frame size.\n- A virtual page resident in memory occupies exactly one physical frame.\n- There are $k$ concurrent processes.\n- All processes execute the same program.\n- The program's compiled code consists of $n$ read-only code pages.\n- Each process has $m$ private, writable data pages.\n- All $n$ code pages and $m$ data pages for each of the $k$ processes are resident in physical memory simultaneously.\n- Only read-only code pages may be shared.\n- Sharing a page means a single physical frame is used for that page across all processes.\n- All overhead (kernel pages, page tables, etc.) is to be ignored.\n\nThe problem is valid. It describes a standard, simplified model of paged virtual memory management, a fundamental topic in operating systems. The concepts of pages, frames, shared read-only code, and private data are well-established. The problem is self-contained, with all necessary variables ($n$, $m$, $k$) and assumptions provided. The objective is clear: to calculate the total number of physical frames required under two distinct, well-defined scenarios. The premises are logically consistent and do not violate any principles of computer science or mathematics.\n\nWe will now proceed with the solution by analyzing the two specified scenarios.\n\nScenario 1: With code-page sharing.\n\nIn this scenario, the read-only code pages can be shared among all $k$ processes.\nThe total number of code pages for the program is $n$. Because these pages are read-only and shared, only one copy of these $n$ pages needs to be loaded into physical memory. All $k$ processes will map their virtual addresses for these code pages to the same set of $n$ physical frames.\nTherefore, the number of physical frames required for the code segment is $n$.\n\nEach of the $k$ processes has its own private data segment consisting of $m$ pages. These pages are writable and therefore cannot be shared. Each process requires its own distinct set of $m$ physical frames to store its private data. Since there are $k$ processes running concurrently, the total number of frames required for all data pages is the number of processes multiplied by the number of data pages per process.\nThe number of physical frames required for the data segments is $k \\times m$, or $km$.\n\nThe total number of physical frames required in the system is the sum of the frames for the shared code and the frames for the private data of all processes.\nTotal frames (with sharing) = (Frames for code) + (Frames for data)\nTotal frames (with sharing) = $n + km$.\n\nScenario 2: With no sharing of any pages.\n\nIn this scenario, no pages are shared between processes, including the read-only code pages. Each process must have its own separate copy of all its pages loaded into physical memory.\nEach of the $k$ processes requires a full copy of the $n$ code pages. Therefore, the total number of physical frames required for the code segments of all $k$ processes is the number of processes multiplied by the number of code pages per program.\nThe number of physical frames required for the code segments is $k \\times n$, or $kn$.\n\nAs in the first scenario, the $m$ data pages for each process are private and cannot be shared. So, for the $k$ processes, the total number of frames required for data remains the same.\nThe number of physical frames required for the data segments is $k \\times m$, or $km$.\n\nThe total number of physical frames required in this non-sharing scenario is the sum of the frames for all code copies and the frames for all private data.\nTotal frames (no sharing) = (Frames for all code copies) + (Frames for all data copies)\nTotal frames (no sharing) = $kn + km$.\nThis can also be expressed as $k(n+m)$.\n\nThe final answer is required as a single row matrix where the first entry is the total frames with sharing and the second entry is the total frames without sharing.\nFirst entry: $n + km$\nSecond entry: $kn + km$", "answer": "$$\\boxed{\\begin{pmatrix} n + km & kn + km \\end{pmatrix}}$$", "id": "3622953"}, {"introduction": "Beyond sharing static code, modern operating systems use the Copy-on-Write (COW) technique to efficiently create new processes. This practice challenges you to interpret real-world performance counters to diagnose how effectively COW is working for different programs. By analyzing the relationship between initially shared pages and the number of COW faults, you will learn to distinguish efficient use of COW from inefficient scenarios where the benefits of sharing are lost. [@problem_id:3629088]", "problem": "A server on an Operating System (OS) uses Copy-On-Write (COW) during process creation via the fork system call. Immediately after the fork operation, the parent and child processes run concurrently for a fixed interval while the OS exposes page-fault counters. The OS reports the following, for each program variant $\\mathsf{A}$, $\\mathsf{B}$, and $\\mathsf{C}$: the number of virtual pages initially shared between parent and child $S$ (measured at the moment right after fork), the number of minor page faults $m$, the number of major page faults $M$, and the number of copy-on-write page faults $c$. The measured tuples are:\n- For $\\mathsf{A}$: $(S_{\\mathsf{A}}, m_{\\mathsf{A}}, M_{\\mathsf{A}}, c_{\\mathsf{A}}) = (100000, 1500, 2, 1200)$.\n- For $\\mathsf{B}$: $(S_{\\mathsf{B}}, m_{\\mathsf{B}}, M_{\\mathsf{B}}, c_{\\mathsf{B}}) = (100000, 60000, 0, 58000)$.\n- For $\\mathsf{C}$: $(S_{\\mathsf{C}}, m_{\\mathsf{C}}, M_{\\mathsf{C}}, c_{\\mathsf{C}}) = (100000, 500, 1, 20)$.\n\nAssume a standard virtual memory implementation in which copy-on-write faults are resolved without disk input/output and are, therefore, classified as minor page faults, and assume that each page can incur at most one copy-on-write fault on the first write that breaks sharing.\n\nWhich student-facing diagnostic, aimed at evaluating copy-on-write efficiency and proposing a practical heuristic, is best supported by the above counters?\n\nA. Program $\\mathsf{C}$ is copy-on-write efficient while program $\\mathsf{B}$ is copy-on-write inefficient; a practical heuristic is: if $c$ is much smaller than $S$ and $c$ is a small fraction of $m$, then copy-on-write preserves sharing well, but if $c$ approaches $S$, sharing largely collapses.\n\nB. Program $\\mathsf{B}$ is copy-on-write efficient because a high $m$ means many faults were handled without disk input/output; therefore, copy-on-write preserved performance and memory sharing.\n\nC. Program $\\mathsf{A}$ is copy-on-write inefficient because $M$ is nonzero; major page faults are the direct indicator of pages being copied due to writes under copy-on-write.\n\nD. No conclusion about copy-on-write efficiency can be drawn from $m$ and $c$ because copy-on-write faults do not correspond to first writes on shared pages, so these counters are not informative.", "solution": "The user provides a problem concerning the interpretation of performance counters related to the Copy-On-Write (COW) mechanism in an operating system. I am asked to validate the problem, and if valid, derive the correct conclusion and evaluate the given options.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n- **Mechanism:** The OS uses Copy-On-Write (COW) during process creation via the `fork` system call.\n- **Scenario:** After `fork`, the parent and child processes run concurrently.\n- **Counters:**\n    - $S$: number of virtual pages initially shared between parent and child.\n    - $m$: number of minor page faults.\n    - $M$: number of major page faults.\n    - $c$: number of copy-on-write page faults.\n- **Data (Tuples $(S, m, M, c)$):**\n    - Program $\\mathsf{A}$: $(100000, 1500, 2, 1200)$\n    - Program $\\mathsf{B}$: $(100000, 60000, 0, 58000)$\n    - Program $\\mathsf{C}$: $(100000, 500, 1, 20)$\n- **Assumptions:**\n    1. Copy-on-write faults are classified as minor page faults (resolved without disk I/O).\n    2. Each shared page can incur at most one copy-on-write fault on the first write that breaks sharing.\n- **Question:** Which student-facing diagnostic, evaluating COW efficiency and proposing a heuristic, is best supported by the counters?\n\n**Step 2: Validate Using Extracted Givens**\n\n- **Scientific Groundedness:** The problem is based on fundamental and standard concepts of virtual memory management in operating systems, including the `fork` system call, copy-on-write, and the distinction between minor and major page faults. The assumptions provided are standard for such an implementation. The problem is scientifically sound.\n- **Well-Posedness:** The problem provides quantitative data and asks for the best qualitative interpretation among a set of choices. This is a standard analysis task. A unique best answer is expected to exist based on a correct understanding of the underlying principles. The problem is well-posed.\n- **Objectivity:** The problem is stated using objective, quantitative data and technical definitions. The question asks for the \"best supported\" diagnostic, which requires an objective analysis of the data against established principles.\n- **Incomplete or Contradictory Setup:** The provided data and assumptions are self-consistent.\n    - Assumption 1 implies $c \\le m$. For $\\mathsf{A}$, $1200 \\le 1500$. For $\\mathsf{B}$, $58000 \\le 60000$. For $\\mathsf{C}$, $20 \\le 500$. This holds.\n    - The number of COW faults ($c$) cannot exceed the number of initially shared pages ($S$). For all programs, $c \\le 100000$. This holds.\n    The setup is complete and consistent.\n- **Unrealistic or Infeasible:** The numerical values are within a realistic range for a modern application's memory footprint and page fault activity. A memory space of $100000$ pages (assuming $4$ KiB pages) corresponds to approximately $400$ MiB, which is reasonable.\n- **Ill-Posed or Poorly Structured:** The term \"copy-on-write efficiency\" has a standard interpretation in this context: it is high when few pages are actually copied, and low when many are. The problem is not ambiguous.\n\n**Step 3: Verdict and Action**\n\nThe problem statement is valid. It is scientifically sound, well-posed, objective, and its data is internally consistent. I will proceed with the solution derivation.\n\n### Solution Derivation\n\nThe core principle of Copy-On-Write (COW) for the `fork` system call is to defer the expensive operation of copying a parent process's memory pages to its child. Initially, both processes share the same physical pages, but these pages are marked as read-only. A write attempt by either process triggers a page fault, known as a COW fault. The OS then allocates a new physical page, copies the content of the shared page to it, and updates the page table of the writing process to point to the new, private page, which is now marked as writable.\n\nThe efficiency of COW is a measure of how well this optimization works. High efficiency means that few writes occur to the shared address space, so few pages need to be copied. The system saves both memory (by not duplicating pages) and time (by not performing the copy). Low efficiency means that one or both processes write to a large fraction of the shared pages soon after the `fork`, forcing many copies and largely negating the benefit of COW.\n\nThe primary metric for COW efficiency is the ratio of the number of pages that were actually copied ($c$) to the number of pages that were initially shared ($S$). A small ratio $\\frac{c}{S}$ indicates high efficiency, while a ratio approaching $1$ indicates low efficiency.\n\nLet's analyze the data for each program:\n\n- **Program $\\mathsf{A}$:** $(S_{\\mathsf{A}}, m_{\\mathsf{A}}, M_{\\mathsf{A}}, c_{\\mathsf{A}}) = (100000, 1500, 2, 1200)$\n  - The ratio of COW faults to shared pages is $\\frac{c_{\\mathsf{A}}}{S_{\\mathsf{A}}} = \\frac{1200}{100000} = 0.012$.\n  - Only $1.2\\%$ of the shared pages were written to, triggering a copy. This indicates **very high COW efficiency**.\n  - The number of major faults, $M_{\\mathsf{A}} = 2$, indicates that $2$ pages had to be read from disk, a process unrelated to the COW mechanism itself.\n\n- **Program $\\mathsf{B}$:** $(S_{\\mathsf{B}}, m_{\\mathsf{B}}, M_{\\mathsf{B}}, c_{\\mathsf{B}}) = (100000, 60000, 0, 58000)$\n  - The ratio of COW faults to shared pages is $\\frac{c_{\\mathsf{B}}}{S_{\\mathsf{B}}} = \\frac{58000}{100000} = 0.58$.\n  - A substantial $58\\%$ of the shared pages were copied. This indicates **low COW efficiency**. The workload of this program is such that it defeats much of the benefit of using COW. The OS had to perform work nearly equivalent to copying more than half of the shared memory space.\n\n- **Program $\\mathsf{C}$:** $(S_{\\mathsf{C}}, m_{\\mathsf{C}}, M_{\\mathsf{C}}, c_{\\mathsf{C}}) = (100000, 500, 1, 20)$\n  - The ratio of COW faults to shared pages is $\\frac{c_{\\mathsf{C}}}{S_{\\mathsf{C}}} = \\frac{20}{100000} = 0.0002$.\n  - A minuscule $0.02\\%$ of the shared pages were copied. This indicates **extremely high COW efficiency**. This is the ideal use case for COW, where the vast majority of shared pages are never written to.\n\n### Option-by-Option Analysis\n\n**A. Program $\\mathsf{C}$ is copy-on-write efficient while program $\\mathsf{B}$ is copy-on-write inefficient; a practical heuristic is: if $c$ is much smaller than $S$ and $c$ is a small fraction of $m$, then copy-on-write preserves sharing well, but if $c$ approaches $S$, sharing largely collapses.**\n\n- The statement \"Program $\\mathsf{C}$ is copy-on-write efficient\" is correct, as $\\frac{c_{\\mathsf{C}}}{S_{\\mathsf{C}}}$ is extremely small.\n- The statement \"program $\\mathsf{B}$ is copy-on-write inefficient\" is correct, as $\\frac{c_{\\mathsf{B}}}{S_{\\mathsf{B}}}$ is large.\n- The proposed heuristic has two parts. First, \"if $c$ is much smaller than $S$... then copy-on-write preserves sharing well\". This is the fundamental principle for evaluating COW efficiency and is correct. Second, \"if $c$ approaches $S$, sharing largely collapses\". This correctly describes COW inefficiency. The intermediate clause \"and $c$ is a small fraction of $m$\" is an observation true for program $\\mathsf{C}$ ($c_{\\mathsf{C}}/m_{\\mathsf{C}} = 20/500 = 0.04$) but not a necessary condition for efficiency (for program $\\mathsf{A}$, $c_{\\mathsf{A}}/m_{\\mathsf{A}} = 1200/1500 = 0.8$, yet it is also efficient). However, the overall diagnostic accurately contrasts the efficient and inefficient cases and provides a sound heuristic based on the correct primary metric ($c$ vs $S$).\n- **Verdict: Correct.** This option provides the most accurate and well-supported diagnosis based on the data.\n\n**B. Program $\\mathsf{B}$ is copy-on-write efficient because a high $m$ means many faults were handled without disk input/output; therefore, copy-on-write preserved performance and memory sharing.**\n\n- The claim that program $\\mathsf{B}$ is COW efficient is false, as demonstrated by the high value of $\\frac{c_{\\mathsf{B}}}{S_{\\mathsf{B}}}$.\n- The reasoning is flawed. While a high number of minor faults ($m$) is preferable to major faults ($M$), a high number of COW faults ($c$), which are a subset of minor faults, is a direct measure of COW *inefficiency*. The goal of COW is to *avoid* page copies and the associated faults.\n- The conclusion that \"copy-on-write preserved... memory sharing\" is directly contradicted by the data: $58000$ pages were copied, meaning sharing was broken for them.\n- **Verdict: Incorrect.**\n\n**C. Program $\\mathsf{A}$ is copy-on-write inefficient because $M$ is nonzero; major page faults are the direct indicator of pages being copied due to writes under copy-on-write.**\n\n- The claim that program $\\mathsf{A}$ is COW inefficient is false. With $\\frac{c_{\\mathsf{A}}}{S_{\\mathsf{A}}} = 0.012$, it is very efficient.\n- The reasoning is fundamentally incorrect. Major page faults ($M$) are caused by references to pages not resident in physical memory (i.e., they are on disk). COW faults are explicitly defined as occurring on writes to pages that *are* resident in memory but are shared. The problem statement and standard OS principles classify COW faults as minor faults, not major faults. The counter for COW copies is $c$, not $M$.\n- **Verdict: Incorrect.**\n\n**D. No conclusion about copy-on-write efficiency can be drawn from $m$ and $c$ because copy-on-write faults do not correspond to first writes on shared pages, so these counters are not informative.**\n\n- The premise \"copy-on-write faults do not correspond to first writes on shared pages\" is false. It directly contradicts the definition of the COW mechanism and Assumption 2 given in the problem statement.\n- Consequently, the conclusion that \"no conclusion... can be drawn\" and that the counters \"are not informative\" is also false. The counters $c$ and $S$ are the most direct and informative measures of COW efficiency.\n- **Verdict: Incorrect.**", "answer": "$$\\boxed{A}$$", "id": "3629088"}, {"introduction": "The abstraction of a shared page relies on underlying data structures, like reference counts, to manage its lifecycle. This exercise explores the critical performance challenges that arise when managing these counts in a multicore system, where contention for a single shared counter can become a bottleneck. By comparing a naive atomic update approach with a more sophisticated per-core batching design, you will analyze how algorithmic choices can drastically improve scalability by reducing cache coherence traffic. [@problem_id:3682481]", "problem": "A single shared physical page has a global reference count stored in a single memory word that resides in a single cache line. Consider a symmetric multiprocessor with $N$ identical Central Processing Unit (CPU) cores. On core $i \\in \\{1,\\dots,N\\}$, reference count updates (increments or decrements) targeting this page arrive according to an independent homogeneous Poisson process of rate $\\lambda$ (updates per second). The cache coherence protocol is write-invalidate: any write to the shared counter requires the writing core to obtain exclusive ownership of the cache line, which moves the line to that core if a different core last wrote to it. Define contention as the expected number of inter-core cache line ownership transfers per unit time at the memory location of the shared counter.\n\nTwo designs for maintaining the shared reference count are considered.\n\nDesign A (direct atomic): Every update performs a single atomic fetch-and-add operation directly on the shared counter.\n\nDesign B (per-core batching): Each core accumulates updates in a per-core local counter without touching the shared counter. Every period $T$ (seconds), each core merges its local counter into the shared counter by performing exactly one atomic fetch-and-add that applies the accumulated delta since its previous merge. Assume merge epochs across cores are independent with identical mean period $T$, and that the identity of the core performing successive atomic operations on the shared counter is independent and identically distributed due to independent arrivals and merge offsets.\n\nUsing only fundamental properties of independent Poisson processes and basic probability, and treating the system in steady state, derive a closed-form expression for the contention reduction factor $R(T)$ defined as\n$$\nR(T) \\equiv \\frac{\\text{expected inter-core cache line ownership transfers per unit time under Design A}}{\\text{expected inter-core cache line ownership transfers per unit time under Design B}}.\n$$\nExpress your final answer as a simplified analytic expression in terms of $\\lambda$ and $T$ only. No numerical evaluation is required, and no units should be included in the final expression.", "solution": "The user wants me to solve the following problem:\nA single shared physical page has a global reference count stored in a single memory word that resides in a single cache line. Consider a symmetric multiprocessor with $N$ identical Central Processing Unit (CPU) cores. On core $i \\in \\{1,\\dots,N\\}$, reference count updates (increments or decrements) targeting this page arrive according to an independent homogeneous Poisson process of rate $\\lambda$ (updates per second). The cache coherence protocol is write-invalidate: any write to the shared counter requires the writing core to obtain exclusive ownership of the cache line, which moves the line to that core if a different core last wrote to it. Define contention as the expected number of inter-core cache line ownership transfers per unit time at the memory location of the shared counter.\n\nTwo designs for maintaining the shared reference count are considered.\n\nDesign A (direct atomic): Every update performs a single atomic fetch-and-add operation directly on the shared counter.\n\nDesign B (per-core batching): Each core accumulates updates in a per-core local counter without touching the shared counter. Every period $T$ (seconds), each core merges its local counter into the shared counter by performing exactly one atomic fetch-and-add that applies the accumulated delta since its previous merge. Assume merge epochs across cores are independent with identical mean period $T$, and that the identity of the core performing successive atomic operations on the shared counter is independent and identically distributed due to independent arrivals and merge offsets.\n\nUsing only fundamental properties of independent Poisson processes and basic probability, and treating the system in steady state, derive a closed-form expression for the contention reduction factor $R(T)$ defined as\n$$\nR(T) \\equiv \\frac{\\text{expected inter-core cache line ownership transfers per unit time under Design A}}{\\text{expected inter-core cache line ownership transfers per unit time under Design B}}.\n$$\nExpress your final answer as a simplified analytic expression in terms of $\\lambda$ and $T$ only. No numerical evaluation is required, and no units should be included in the final expression.\n\n### Step 1: Extract Givens\n- Number of CPU cores: $N$.\n- Cores are indexed $i \\in \\{1, \\dots, N\\}$.\n- For each core, updates arrive via an independent homogeneous Poisson process of rate $\\lambda$.\n- Cache coherence protocol: write-invalidate.\n- Contention is defined as the expected number of inter-core cache line ownership transfers per unit time.\n- Design A: Every update is a direct atomic fetch-and-add on the shared counter.\n- Design B: Each core batches updates locally and merges them into the shared counter with a single atomic fetch-and-add. The mean period for merging is $T$ for each core. Merge epochs are independent across cores.\n- Assumption: The identity of the core performing successive atomic operations on the shared counter is independent and identically distributed (i.i.d.).\n- Objective: Derive the contention reduction factor $R(T) = \\frac{\\text{Contention}_A}{\\text{Contention}_B}$ in terms of $\\lambda$ and $T$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is a well-defined exercise in performance modeling and probability theory, grounded in fundamental concepts of computer architecture and operating systems (e.g., cache coherence, atomic operations, contention). The use of Poisson processes is a standard and appropriate modeling technique for arrival events. The problem provides all necessary information and a clear, objective set of assumptions, including the critical simplification that the sequence of cores performing atomic operations is i.i.d. This assumption makes the problem analytically tractable. The problem is self-contained, scientifically sound, and does not violate any of the invalidity criteria.\n\n### Step 3: Verdict and Action\nThe problem is deemed valid. A solution will be derived.\n\nWe begin by formalizing the definition of contention. Contention is the expected number of inter-core cache line ownership transfers per unit time. This can be expressed as the product of two quantities: the total rate of atomic operations on the shared counter, and the probability that any given atomic operation results in an inter-core transfer.\nLet $\\Lambda_{ops}$ be the total rate of atomic operations on the shared counter across all $N$ cores.\nLet $C_k$ denote the identity of the core performing the $k$-th atomic operation. An inter-core transfer occurs if the core performing the current operation, $C_k$, is different from the core that performed the previous operation, $C_{k-1}$. Let $P_{transfer}$ be the probability of this event, i.e., $P_{transfer} = P(C_k \\neq C_{k-1})$.\nThe contention can then be written as:\n$$\n\\text{Contention} = \\Lambda_{ops} \\times P_{transfer}\n$$\nThe problem states that \"the identity of the core performing successive atomic operations on the shared counter is independent and identically distributed\". Due to the symmetric nature of the system (all cores are identical), the probability that any specific core $i$ performs an atomic operation is uniformly distributed.\n$$\nP(C_k = i) = \\frac{1}{N} \\quad \\text{for any } i \\in \\{1, \\dots, N\\}\n$$\nThe \"independent\" part of the i.i.d. assumption means that $P(C_k = i | C_{k-1} = j) = P(C_k = i)$.\nThe probability that two successive operations are performed by the *same* core is:\n$$\nP(C_k = C_{k-1}) = \\sum_{i=1}^{N} P(C_k = i \\text{ and } C_{k-1} = i)\n$$\nDue to independence:\n$$\nP(C_k = C_{k-1}) = \\sum_{i=1}^{N} P(C_k = i) P(C_{k-1} = i) = \\sum_{i=1}^{N} \\left(\\frac{1}{N}\\right) \\left(\\frac{1}{N}\\right) = N \\left(\\frac{1}{N^2}\\right) = \\frac{1}{N}\n$$\nTherefore, the probability of an inter-core cache line ownership transfer for any given atomic operation is:\n$$\nP_{transfer} = P(C_k \\neq C_{k-1}) = 1 - P(C_k = C_{k-1}) = 1 - \\frac{1}{N} = \\frac{N-1}{N}\n$$\nThis probability is the same for both Design A and Design B, as it depends only on the i.i.d. assumption. Now, we calculate the contention for each design.\n\nFor Design A (direct atomic):\nEvery update is an atomic operation on the shared counter. On each of the $N$ cores, updates arrive as an independent Poisson process with rate $\\lambda$. By the property of superposition of independent Poisson processes, the total stream of updates to the shared counter is also a Poisson process with a rate equal to the sum of the individual rates.\nThe total rate of atomic operations for Design A is:\n$$\n\\Lambda_A = \\sum_{i=1}^{N} \\lambda = N\\lambda\n$$\nThe contention for Design A, denoted $\\text{Contention}_A$, is:\n$$\n\\text{Contention}_A = \\Lambda_A \\times P_{transfer} = (N\\lambda) \\left(\\frac{N-1}{N}\\right) = \\lambda(N-1)\n$$\n\nFor Design B (per-core batching):\nEach core performs one atomic merge operation in a mean period of $T$ seconds. This corresponds to a rate of $1/T$ atomic operations per core.\nThe total rate of atomic operations on the shared counter is the sum of the rates from all $N$ cores:\n$$\n\\Lambda_B = \\sum_{i=1}^{N} \\frac{1}{T} = \\frac{N}{T}\n$$\nThe information about the Poisson process of updates (rate $\\lambda$) determines the *value* of the batched count that is atomically added, but not the *rate* of the atomic operations themselves, which is dictated solely by the merge period $T$.\nThe contention for Design B, denoted $\\text{Contention}_B$, is:\n$$\n\\text{Contention}_B = \\Lambda_B \\times P_{transfer} = \\left(\\frac{N}{T}\\right) \\left(\\frac{N-1}{N}\\right) = \\frac{N-1}{T}\n$$\n\nFinally, we compute the contention reduction factor $R(T)$. This is the ratio of the contention under Design A to that under Design B.\n$$\nR(T) = \\frac{\\text{Contention}_A}{\\text{Contention}_B} = \\frac{\\lambda(N-1)}{(N-1)/T}\n$$\nAssuming a multiprocessor system where $N > 1$ (otherwise contention is not meaningful), the term $(N-1)$ is non-zero and can be canceled from the numerator and denominator.\n$$\nR(T) = \\frac{\\lambda}{\\frac{1}{T}} = \\lambda T\n$$\nThe result is independent of the number of cores $N$. The value $\\lambda T$ represents the average number of updates that are batched together by a single core in one merge period.", "answer": "$$\n\\boxed{\\lambda T}\n$$", "id": "3682481"}]}