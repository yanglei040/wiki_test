{"hands_on_practices": [{"introduction": "A file system's capacity is defined not just by its total storage size, but also by its ability to manage metadata for a large number of files. This exercise explores this crucial trade-off by asking you to model the upper limits of two different metadata designs: one with a statically allocated table for file metadata (inodes) and one that allocates them dynamically. By working from first principles, you will quantify the advantages of a dynamic approach and gain insight into the fundamental resource constraints that govern file system architecture [@problem_id:3642840].", "problem": "A storage stack implements two contrasting on-disk metadata designs at the file organization layer within a single partition of total size $P$ bytes, understood through the classical layering where the logical file layer uses index nodes (inodes) to name and locate file data and the directory layer stores directory entries that bind names to inode identifiers. Consider the following two designs.\n\nFixed-inode design. The file system (FS) preallocates an on-disk inode table of size $I$ bytes. Every index node (inode) occupies $i_s$ bytes. Exactly $r$ inodes are permanently unavailable to users because they are reserved for system objects and the root directory; the remaining inodes, if any, can be used for user-created regular files. Non-inode fixed overheads (for example, superblock, journaling structures, and allocation bitmaps), excluding the inode table itself, occupy $F_f$ bytes and do not scale with the number of user files. Each user-created regular file must appear in a directory; the directory layer stores exactly $d$ bytes per file as directory entry payload for that file, and these directory bytes are placed in the same general data region that also stores ordinary file contents. You may assume that to maximize the count of distinct files, files of size $0$ bytes are created so that no data blocks are consumed beyond directory entries, and that the root directory and other system objects are already accounted for in the $r$ reserved inodes and $F_f$ bytes.\n\nDynamic-inode design. The FS does not preallocate a fixed-size inode table. Instead, inodes are carved on demand from the general free space region. Each user-created inode still stores $i_s$ bytes of inode state and additionally incurs an amortized allocation bookkeeping cost of $a$ bytes that scales linearly with the number of user inodes. Non-inode fixed overheads for this design (including any precreated system objects such as the root directory and their metadata) occupy $F_d$ bytes and do not scale with the number of user files. The per-file directory entry cost remains exactly $d$ bytes, stored in the same general free space region. As above, assume all user files have size $0$ bytes.\n\nWorking from first principles of the file and directory layers, and without invoking any prepackaged formulas, derive the tightest possible upper bounds consistent with the stated model on the maximum number of distinct user regular files that can be created under each design. Then, define the multiplicative improvement factor $\\gamma$ to be the ratio of the dynamic-inode upper bound to the fixed-inode upper bound, using only algebraic operators together with the standard floor and minimum operators as needed to capture discreteness and shared-space coupling. Express your final answer as a single closed-form analytic expression for $\\gamma$ in terms of $P$, $I$, $i_s$, $r$, $F_f$, $F_d$, $a$, and $d$. State no units in your final expression. Assume parameters are such that all denominators used are nonzero and the bounds are positive.", "solution": "The problem requires the derivation of the tightest possible upper bounds on the number of user-creatable files for two different on-disk metadata designs, and then to compute the ratio of these bounds. Let us proceed by analyzing each design from first principles. Let $N$ denote the number of distinct user regular files. The analysis assumes files of size $0$ bytes are created to maximize the file count by minimizing data block consumption, thereby focusing the analysis on metadata overhead.\n\nFirst, we analyze the **Fixed-inode design**.\nIn this design, two primary resources constrain the number of files: the finite number of preallocated inodes and the finite amount of space for directory entries. The maximum number of files, let us call it $N_{fixed}$, will be the minimum of the bounds imposed by these two constraints.\n\n1.  **Inode Constraint**: The total size of the preallocated inode table is $I$ bytes, and each inode occupies $i_s$ bytes. The total number of inodes that can be stored in this table is the integer part of the ratio of these sizes, which is $\\lfloor \\frac{I}{i_s} \\rfloor$. The problem states that $r$ of these inodes are reserved for system use. Therefore, the maximum number of inodes available for user files is the total number of inodes minus the reserved count. This imposes an upper bound on $N_{fixed}$:\n    $$ N_{fixed} \\le \\left\\lfloor \\frac{I}{i_s} \\right\\rfloor - r $$\n\n2.  **Space Constraint**: The total partition size is $P$ bytes. The space consumed by fixed overheads consists of the inode table itself ($I$ bytes) and other non-inode structures ($F_f$ bytes). The remaining space constitutes the general data region, available for storing file contents and directory entries. The size of this region is $P - I - F_f$. According to the model, each of the $N_{fixed}$ user files requires $d$ bytes for its directory entry, which are stored in this data region. Since we assume user files have size $0$, the total space consumed by these directory entries is $N_{fixed} \\cdot d$. This consumption cannot exceed the available space in the data region.\n    $$ N_{fixed} \\cdot d \\le P - I - F_f $$\n    This yields a second upper bound on $N_{fixed}$ by dividing by $d$ and taking the floor, as a non-integer number of files cannot be created:\n    $$ N_{fixed} \\le \\left\\lfloor \\frac{P - I - F_f}{d} \\right\\rfloor $$\n\nThe number of user files $N_{fixed}$ must satisfy both constraints simultaneously. The tightest possible upper bound is therefore the most restrictive of these two limits. This is expressed using the minimum operator:\n$$ N_{fixed} = \\min\\left( \\left\\lfloor \\frac{I}{i_s} \\right\\rfloor - r, \\left\\lfloor \\frac{P - I - F_f}{d} \\right\\rfloor \\right) $$\n\nNext, we analyze the **Dynamic-inode design**.\nIn this design, there is no preallocated inode table. Instead, inodes and directory entries are both allocated from a single general free space region. This means there is only one primary resource constraint: the total available space. Let $N_{dynamic}$ be the maximum number of user files in this design.\n\n1.  **Space Constraint**: The total partition size is $P$ bytes. Non-inode fixed overheads occupy $F_d$ bytes. The remaining space, $P - F_d$, is the general free space region from which all per-file metadata is allocated. For each user file created, a certain amount of space is consumed from this pool. This per-file cost includes:\n    - The inode itself: $i_s$ bytes.\n    - The amortized allocation bookkeeping for the inode: $a$ bytes.\n    - The directory entry: $d$ bytes.\n    - The file data has a size of $0$ bytes.\n    The total space consumed per user file is $i_s + a + d$ bytes. For $N_{dynamic}$ files, the total space consumed is $N_{dynamic} \\cdot (i_s + a + d)$. This total must not exceed the available free space.\n    $$ N_{dynamic} \\cdot (i_s + a + d) \\le P - F_d $$\n    Solving for $N_{dynamic}$ and taking the floor to ensure an integer number of files gives the tightest upper bound:\n    $$ N_{dynamic} = \\left\\lfloor \\frac{P - F_d}{i_s + a + d} \\right\\rfloor $$\n\nFinally, we define the multiplicative improvement factor $\\gamma$ as the ratio of the dynamic-inode upper bound to the fixed-inode upper bound.\n$$ \\gamma = \\frac{N_{dynamic}}{N_{fixed}} $$\nSubstituting the derived expressions for $N_{dynamic}$ and $N_{fixed}$ yields the final closed-form expression for $\\gamma$.\n$$ \\gamma = \\frac{\\left\\lfloor \\frac{P - F_d}{i_s + a + d} \\right\\rfloor}{\\min\\left( \\left\\lfloor \\frac{I}{i_s} \\right\\rfloor - r, \\left\\lfloor \\frac{P - I - F_f}{d} \\right\\rfloor \\right)} $$\nThis expression is formulated using only the given parameters and the allowed algebraic, floor, and minimum operators, satisfying all conditions of the problem statement. The assumptions that denominators are non-zero and bounds are positive ensure this expression is well-defined.", "answer": "$$\n\\boxed{\\frac{\\left\\lfloor \\frac{P - F_d}{i_s + a + d} \\right\\rfloor}{\\min\\left( \\left\\lfloor \\frac{I}{i_s} \\right\\rfloor - r, \\left\\lfloor \\frac{P - I - F_f}{d} \\right\\rfloor \\right)}}\n$$", "id": "3642840"}, {"introduction": "The performance of reading from a file is heavily influenced by how its data blocks are organized on the storage device. This problem contrasts the random-access performance of a classic linked-list allocation scheme (the File Allocation Table, or FAT) with a modern extent-based approach. You will build a mathematical model that incorporates the probabilistic nature of caching to derive the expected access time, providing a clear, quantitative understanding of why modern file systems have evolved beyond simple linked structures [@problem_id:3642743].", "problem": "An operating system’s file subsystem uses a layering in which the file mapping layer translates a file’s logical block index to an on-disk location before the storage layer issues the input/output operation. Consider two alternative mapping organizations for a file: File Allocation Table (FAT) and an extent-based mapping. In the File Allocation Table (FAT) organization, a file is a singly linked list of clusters, and the directory entry stores the starting cluster number. Let each cluster contain $C$ logical blocks. A random access to logical block index $b \\in \\mathbb{Z}_{\\ge 0}$ within the file requires traversing the FAT chain from the starting cluster to the cluster containing block $b$. Assume the following cost model for the mapping layer and storage layer:\n\n- Accessing a FAT entry that is already in the in-memory cache costs $t_{H}$ time units.\n- Accessing a FAT entry that is not in the cache costs $t_{F}$ time units (to fetch from storage into memory).\n- Independently for each needed FAT entry during a traversal, the probability that it is in the cache is $p \\in [0,1]$.\n- The cost to read the target data block from the storage device after mapping completes is $t_{B}$ time units.\n- The directory entry’s starting cluster number is assumed known in memory at cost $0$.\n- In the extent-based organization, the mapping layer performs an $O(1)$ extent lookup with cost $t_{E}$ time units and then the storage layer reads the target data block with the same cost $t_{B}$ time units.\n\nStarting from the basic definitions of how the File Allocation Table (FAT) chain traversal computes the cluster containing a given logical block and the linearity of expectation for independent cache-hit events, derive a closed-form expression for the ratio $R(b)$ of the expected total time to perform a random access to logical block $b$ in the FAT organization to the total time in the extent-based organization. Express your final answer as a single simplified analytic expression in terms of $b$, $C$, $p$, $t_{H}$, $t_{F}$, $t_{B}$, and $t_{E}$. No numerical evaluation is required. The final answer must be a single expression; do not provide an inequality or an equation other than the requested expression. If you choose to present any intermediate quantities, they must not appear in the final answer. Do not include units in the final answer.", "solution": "The problem requires the derivation of a closed-form expression for the ratio $R(b)$ of the expected total time for a random access to logical block $b$ in a File Allocation Table (FAT) organization to the total time for the same access in an extent-based organization. We will approach this by first determining the expression for the total time in the extent-based case, then deriving the expected total time for the FAT-based case, and finally computing their ratio.\n\nFirst, let us analyze the total time for an access in the extent-based organization. The problem states that the mapping layer performs an extent lookup with a cost of $t_{E}$ time units, which is described as an $O(1)$ operation. Following this, the storage layer reads the target data block, which incurs a cost of $t_{B}$ time units. These two operations are sequential. Therefore, the total time for an access in the extent-based organization, which we denote as $T_{\\text{extent}}$, is the sum of these costs:\n$$T_{\\text{extent}} = t_{E} + t_{B}$$\nThis time is deterministic.\n\nNext, we analyze the expected total time for an access in the FAT organization. The access to a logical block with index $b \\in \\mathbb{Z}_{\\ge 0}$ involves two main steps: first, the mapping layer must translate the logical block index to a physical disk location by traversing the FAT chain, and second, the storage layer reads the actual data block.\n\nThe logical blocks of a file are grouped into clusters, where each cluster contains $C$ logical blocks. The blocks are indexed starting from $0$. Thus, cluster $0$ of the file contains logical blocks $0, 1, \\dots, C-1$. Cluster $1$ contains logical blocks $C, C+1, \\dots, 2C-1$. In general, file-relative cluster $k$ contains logical blocks with indices from $kC$ to $(k+1)C-1$. To find the file-relative cluster index $k$ that contains the logical block $b$, we must find the integer $k$ such that $kC \\le b < (k+1)C$. Dividing by $C$ gives $k \\le \\frac{b}{C} < k+1$. This implies that $k$ is the integer part of $\\frac{b}{C}$, which is given by the floor function:\n$$k = \\left\\lfloor \\frac{b}{C} \\right\\rfloor$$\nIn the FAT organization, a file is a singly linked list of clusters. The directory entry contains the address of the first cluster (cluster $0$ in the file's sequence), which is assumed to be known in memory at zero cost. To find the location of the target cluster at index $k$, the file system must traverse the linked list starting from the first cluster. This requires $k$ sequential lookups in the FAT. For instance, to find the second cluster (index $1$), one must read the FAT entry corresponding to the first cluster (index $0$). To find the third cluster (index $2$), one must read the FAT entry for the second cluster, and so on. Therefore, to reach the cluster at index $k = \\lfloor \\frac{b}{C} \\rfloor$, a total of $N = \\lfloor \\frac{b}{C} \\rfloor$ FAT entry accesses are necessary.\n\nLet $X_i$ be the random variable representing the cost of the $i$-th FAT access in the traversal chain, for $i \\in \\{1, 2, \\dots, N\\}$. The cost of each access depends on whether the corresponding FAT entry is in the in-memory cache.\n- The cost is $t_H$ if the entry is in the cache (a \"hit\"). This occurs with probability $p$.\n- The cost is $t_F$ if the entry is not in the cache (a \"miss\"). This occurs with probability $1-p$.\n\nThe expected cost of a single FAT access, $E[X_i]$, is the weighted average of these two outcomes:\n$$E[X_i] = p \\cdot t_{H} + (1-p) \\cdot t_{F}$$\nThe problem states that the cache hit/miss events are independent for each required FAT entry. Thus, the expected cost $E[X_i]$ is the same for all $i=1, \\dots, N$. Let us denote this common expected cost by $E[X_{\\text{access}}]$.\n\nThe total time for the mapping layer traversal is the sum of the costs of the $N$ individual accesses, $T_{\\text{mapping-FAT}} = \\sum_{i=1}^{N} X_i$. By the linearity of expectation, the expected total mapping time is the sum of the expected costs of each access:\n$$E[T_{\\text{mapping-FAT}}] = E\\left[\\sum_{i=1}^{N} X_i\\right] = \\sum_{i=1}^{N} E[X_i]$$\nSince $E[X_i]$ is constant for all $i$ and $N = \\lfloor \\frac{b}{C} \\rfloor$, this simplifies to:\n$$E[T_{\\text{mapping-FAT}}] = N \\cdot E[X_{\\text{access}}] = \\left\\lfloor \\frac{b}{C} \\right\\rfloor (p t_{H} + (1-p) t_{F})$$\nAfter the mapping layer determines the physical location of the data block, the storage layer performs the read operation, which has a constant cost of $t_B$. The total expected time for the FAT-based access, $E[T_{\\text{FAT}}(b)]$, is the sum of the expected mapping time and the block read time:\n$$E[T_{\\text{FAT}}(b)] = E[T_{\\text{mapping-FAT}}] + t_{B} = \\left\\lfloor \\frac{b}{C} \\right\\rfloor (p t_{H} + (1-p) t_{F}) + t_{B}$$\n\nFinally, we can compute the ratio $R(b)$ of the expected total time for the FAT organization to the total time for the extent-based organization:\n$$R(b) = \\frac{E[T_{\\text{FAT}}(b)]}{T_{\\text{extent}}}$$\nSubstituting the derived expressions for the numerator and the denominator, we obtain the final closed-form expression:\n$$R(b) = \\frac{\\left\\lfloor \\frac{b}{C} \\right\\rfloor (p t_{H} + (1-p) t_{F}) + t_{B}}{t_{E} + t_{B}}$$\nThis expression is simplified and contains only the variables specified in the problem statement.", "answer": "$$\\boxed{\\frac{\\left\\lfloor \\frac{b}{C} \\right\\rfloor \\left(p t_{H} + (1-p) t_{F}\\right) + t_{B}}{t_{E} + t_{B}}}$$", "id": "3642743"}, {"introduction": "Operating systems provide a powerful logical abstraction over the on-disk file system, but this abstraction comes with intricate rules that have profound security implications. This problem presents a practical sandboxing scenario to test your understanding of the POSIX path resolution model, particularly the subtle but critical interactions between the `chroot` jail, the process's current working directory ($cwd$), and the modern `openat` system call. By analyzing these scenarios, you will uncover common security pitfalls and appreciate the robust safety provided by file-descriptor-based operations over path-based ones [@problem_id:3642749].", "problem": "Consider a process on a Unix-like system adhering to the Portable Operating System Interface (POSIX). Let the process initially have its current working directory $cwd$ equal to $\"/etc\"$ and its root directory $root$ equal to $\"/\"$. The process later creates a directory intended as a sandbox at the absolute path $\"/srv/sandbox\"$, which contains entries $\"a\"$ (a directory) and $\"b\"$ (a regular file). For realism, suppose that under the initial $cwd$ of $\"/etc\"$ there also exist entries named $\"a\"$ (a directory) and $\"b\"$ (a regular file). The process may invoke the change root operation `chroot` to set its new $root$, and may invoke the change directory operations `chdir` or `fchdir` to set its $cwd$. The `openat` system call resolves a pathname relative to either a supplied directory file descriptor or, if the special constant `AT_FDCWD` is passed, relative to the process’s $cwd$. Assume there are no mount namespaces or bind mounts altering visibility, and ignore permission errors.\n\nUse only the following widely accepted base facts about path resolution: under POSIX, a relative path is resolved starting at $cwd$, an absolute path is resolved starting at $root$; the components $\".\"$ and $\"..\"$ are interpreted at path resolution time so that $x/..\"$ refers to the parent of the directory $x$ actually denotes after any symbolic link resolution; `chroot` sets $root$ but does not change $cwd$; when $root$ is set, path traversal cannot ascend above $root$ via $\"..\"$ during resolution, but resolution does not retroactively relocate a $cwd$ outside $root$ into $root$.\n\nThe process wishes to open the pathname $\"a/../b\"$ using `openat`. Which of the following choices correctly state conditions that guarantee the opened path is confined to the subtree under the sandbox $\"/srv/sandbox\"$ (that is, the final resolved path cannot name anything outside $\"/srv/sandbox\"$)? Select all that apply.\n\nA. Invoke `chroot(\"/srv/sandbox\")` and then directly call `openat(AT_FDCWD, \"a/../b\", O_RDONLY)` without changing $cwd$. Confinement is guaranteed because $\"..\"$ cannot escape the new $root$.\n\nB. Invoke `chroot(\"/srv/sandbox\")` followed immediately by `chdir(\"/\")`, then call `openat(AT_FDCWD, \"a/../b\", O_RDONLY)`. Confinement is guaranteed.\n\nC. Instead of using `AT_FDCWD`, open a directory file descriptor $d\\_root$ referring to $\"/srv/sandbox\"$ (for example, via `d_root = open(\"/srv/sandbox\", O_PATH | O_DIRECTORY)`), and then call `openat(d_root, \"a/../b\", O_RDONLY)`. Confinement is guaranteed regardless of $cwd$.\n\nD. Invoke `chroot(\"/srv/sandbox\")` followed by `chdir(\"/\")`, but suppose that inside the sandbox $\"a\"$ is a symbolic link to $\"/\"$. Then `openat(AT_FDCWD, \"a/../b\", O_RDONLY)` can resolve outside the sandbox, so confinement is not guaranteed.\n\nE. Invoke `chroot(\"/srv/sandbox\")` while keeping open a pre-existing directory file descriptor to $\"/etc\"$, then use `fchdir` to set $cwd$ to that directory before calling `openat(AT_FDCWD, \"a/../b\", O_RDONLY)`. Confinement is still guaranteed because the $root$ barrier prevents escape via $\"..\"$.", "solution": "The problem statement is a well-defined exercise in computational systems theory, specifically concerning the semantics of file system path resolution in a POSIX-compliant operating system. It provides a clear initial state, a set of defined operations, and a collection of explicit rules governing system behavior. These rules are consistent with the established behavior of Unix-like systems. The problem is scientifically grounded, well-posed, objective, and does not exhibit any logical contradictions or factual unsoundness. It is therefore valid for analysis.\n\nThe central task is to determine which scenarios guarantee that resolving the relative path `\"a/../b\"` results in a file path confined within the `\"/srv/sandbox\"` directory subtree. The resolution of the path `\"a/../b\"` from a starting directory, let's call it $D_{start}$, simplifies. The component `\"a\"` leads to a subdirectory $D_{start}\\text{/a}$. The component `\"../\"` then leads to the parent of $D_{start}\\text{/a}$, which is $D_{start}$ itself (assuming `\"a\"` is a simple directory and not a symbolic link that complicates resolution, a case we will handle separately). Finally, the component `\"b\"` is resolved within $D_{start}$, resulting in the path $D_{start}\\text{/b}$. Thus, the path `\"a/../b\"` is effectively equivalent to `\"b\"` resolved from the starting directory.\n\nConfinement to the `\"/srv/sandbox\"` subtree is therefore guaranteed if and only if the starting directory for path resolution, $D_{start}$, is either `\"/srv/sandbox\"` itself or one of its subdirectories. The system call in question is `openat`, and the starting directory is determined by its first argument.\n\nLet's evaluate each choice based on these principles and the provided base facts. The initial state is $root = \\text{\"/\"}$ and $cwd = \\text{\"/etc\"}$.\n\n**Option A Evaluation**\n1.  The process invokes `chroot(\"/srv/sandbox\")`. According to the provided rules, this sets the process's $root$ directory to `\"/srv/sandbox\"`. Crucially, this operation \"does not change $cwd$\".\n2.  The $cwd$ remains a reference to the directory `\"/etc\"`. The rule \"resolution does not retroactively relocate a $cwd$ outside $root$ into $root$\" confirms that the process's $cwd$ is still the directory located at `\"/etc\"` in the global filesystem, which is outside the new $root$ jail.\n3.  The process then calls `openat(AT_FDCWD, \"a/../b\", O_RDONLY)`. The special constant `AT_FDCWD` directs the system call to resolve the path relative to the current working directory, $cwd$.\n4.  Since $cwd$ is `\"/etc\"`, the path resolution starts there. It resolves `\"a/../b\"` to `\"/etc/b\"`.\n5.  The final path `\"/etc/b\"` is not within the `\"/srv/sandbox\"` subtree. Confinement is not achieved.\n6.  The reasoning provided in the option, \"because `..` cannot escape the new $root$\", is fallacious. The path resolution begins and remains entirely outside the new $root$, so the traversal restriction on `\"..\"` is never engaged.\n\nThe conditions stated do not guarantee confinement.\nVerdict: **Incorrect**.\n\n**Option B Evaluation**\n1.  The process invokes `chroot(\"/srv/sandbox\")`, setting its $root$ to `\"/srv/sandbox\"`. As before, $cwd$ remains `\"/etc\"`.\n2.  The process then invokes `chdir(\"/\")`. According to the rule \"an absolute path is resolved starting at $root$\", the path `\"/\"` is resolved relative to the process's new $root$ directory, `\"/srv/sandbox\"`. Thus, `chdir(\"/\")` successfully changes the $cwd$ to `\"/srv/sandbox\"`.\n3.  The process state is now $root = \\text{\"/srv/sandbox\"}$ and $cwd = \\text{\"/srv/sandbox\"}$.\n4.  The call `openat(AT_FDCWD, \"a/../b\", O_RDONLY)` resolves the path starting from the $cwd$, which is `\"/srv/sandbox\"`.\n5.  Resolution of `\"a/../b\"` from `\"/srv/sandbox\"` proceeds as follows: the `\"/srv/sandbox/a\"` directory is found, `\"../\"` resolves to its parent `\"/srv/sandbox\"`, and `\"b\"` resolves to the file `\"/srv/sandbox/b\"`.\n6.  The final path is `\"/srv/sandbox/b\"`, which is confined to the sandbox subtree.\n\nThe conditions stated do guarantee confinement.\nVerdict: **Correct**.\n\n**Option C Evaluation**\n1.  This option bypasses the use of `chroot` and `AT_FDCWD` for the open operation. Instead, a file descriptor $d\\_root$ is obtained for the directory `\"/srv/sandbox\"` via a call like `d_root = open(\"/srv/sandbox\", O_PATH | O_DIRECTORY)`.\n2.  The process then calls `openat(d_root, \"a/../b\", O_RDONLY)`. This form of `openat` resolves the given pathname relative to the directory associated with the file descriptor $d\\_root$.\n3.  The starting directory for resolution is explicitly `\"/srv/sandbox\"`, regardless of the process's $cwd$ or $root$.\n4.  As in the analysis for Option B, resolving `\"a/../b\"` from the starting directory `\"/srv/sandbox\"` yields the final path `\"/srv/sandbox/b\"`.\n5.  This path is confined to the sandbox. The claim \"Confinement is guaranteed regardless of $cwd$\" is accurate. This is a standard and robust technique for performing path-based operations within a specific directory without altering the process-wide state like $cwd$ or $root$.\n\nThe conditions stated do guarantee confinement.\nVerdict: **Correct**.\n\n**Option D Evaluation**\n1.  This option presents a scenario and makes a claim about its outcome. We must evaluate the truth of the claim.\n2.  The setup is the same as in Option B: `chroot(\"/srv/sandbox\")` followed by `chdir(\"/\")`. As established, this sets $root = \\text{\"/srv/sandbox\"}$ and $cwd = \\text{\"/srv/sandbox\"}$.\n3.  The new condition is that `\"a\"` inside the sandbox (`\"/srv/sandbox/a\"`) is a symbolic link to `\"/\"`.\n4.  The call is `openat(AT_FDCWD, \"a/../b\", O_RDONLY)`, starting from $cwd = \\text{\"/srv/sandbox\"}$.\n5.  Path resolution begins with `\"a\"`. At `\"/srv/sandbox\"`, `\"a\"` is found to be a symbolic link to `\"/\"`.\n6.  The target of the link, `\"/\"`, is an absolute path. Its resolution must therefore start from the process's current $root$, which is `\"/srv/sandbox\"`. Thus, the symbolic link resolves to the directory `\"/srv/sandbox\"`.\n7.  The path component `\"a\"` has effectively resolved to `\"/srv/sandbox\"`. Now, the next component, `\"..\"`, is processed. This refers to the parent of the directory just resolved.\n8.  We must find the parent of `\"/srv/sandbox\"`. However, the rule \"path traversal cannot ascend above $root$ via `..`\" applies. Since `\"/srv/sandbox\"` is the process's $root$, attempting to ascend via `\"../\"` results in `\"/srv/sandbox\"` itself.\n9.  So, the path `\"a/..\"` resolves to `\"/srv/sandbox\"`.\n10. Finally, the component `\"b\"` is resolved relative to this result, yielding `\"/srv/sandbox/b\"`.\n11. The final path is confined to the sandbox. The option's claim that the path \"can resolve outside the sandbox\" and that \"confinement is not guaranteed\" is therefore false.\n\nThe statement made in this option is factually incorrect.\nVerdict: **Incorrect**.\n\n**Option E Evaluation**\n1.  The process invokes `chroot(\"/srv/sandbox\")`, setting $root = \\text{\"/srv/sandbox\"}$.\n2.  It then uses `fchdir` to set its $cwd$ using a file descriptor that was opened to `\"/etc\"` *before* the chroot. This action explicitly sets the $cwd$ to be the directory inode corresponding to `\"/etc\"`.\n3.  The resulting state is $root = \\text{\"/srv/sandbox\"}$ and $cwd$ points to the directory `\"/etc\"`, which is outside the $root$ jail. This is a state identical to that in Option A.\n4.  The call `openat(AT_FDCWD, \"a/../b\", O_RDONLY)` resolves the path relative to the $cwd$, `\"/etc\"`.\n5.  The path resolves to `\"/etc/b\"`, which is not confined to the `\"/srv/sandbox\"` subtree.\n6.  The claim \"Confinement is still guaranteed\" is false. The reasoning \"because the $root$ barrier prevents escape\" is again flawed, as the resolution never enters the jail to begin with.\n\nThe conditions stated do not guarantee confinement.\nVerdict: **Incorrect**.\n\nIn summary, only options B and C describe scenarios that correctly and robustly guarantee confinement of the file operation to the `\"/srv/sandbox\"` subtree.", "answer": "$$\\boxed{BC}$$", "id": "3642749"}]}