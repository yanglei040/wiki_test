{"hands_on_practices": [{"introduction": "This practice provides a hands-on look at the CPU overhead involved in the LFS write path. By breaking down the cost of a single write into its constituent parts—such as checksum calculation, metadata handling, and amortized costs—you can build a performance model from first principles. This exercise [@problem_id:3654765] is crucial for understanding how low-level design choices impact the overall system performance.", "problem": "A Log-Structured File System (LFS) maintains a per-segment summary to accelerate crash recovery and cleaning. Consider an implementation that appends user writes into an in-memory segment buffer of size $S = 2\\,\\mathrm{MiB}$, with file system block size $B = 4096\\,\\mathrm{bytes}$. When the segment is full, the system emits the segment and its summary. Each appended block contributes one summary entry. The per-entry work consists of computing a Cyclic Redundancy Check (CRC), formatting metadata, and inserting an entry into an in-memory hash table used for quick lookup during cleaning. The implementation is profiled on a given machine, and the following per-operation costs are measured:\n\n- Computing a $32$-bit CRC over a byte stream costs $c_{\\mathrm{crc}} = 3.2$ cycles per byte.\n- Formatting and storing the fixed-size summary metadata of $m = 32\\,\\mathrm{bytes}$ costs $c_{\\mathrm{meta}} = 0.8$ cycles per byte.\n- Fixed per-entry formatting overhead (branching, bounds checks, and pointer chasing) costs $c_{\\mathrm{fmt}} = 150$ cycles per entry.\n- Inserting the entry into the in-memory hash table has expected cost $c_{\\mathrm{hash}} = 75$ cycles per entry at the observed load factor.\n\nThere is also a fixed per-segment header assembly cost of $c_{\\mathrm{seg}} = 10000$ cycles that must be amortized across all entries in the segment. Additionally, each user write has a fixed per-write synchronization overhead of $c_{\\mathrm{w}} = 300$ cycles.\n\nAssume the average user write size is $w = 12\\,\\mathrm{KiB}$ and that writes align to block boundaries so that the number of blocks per write is exactly $n = w / B$. Assume the segment is densely filled so that the amortized per-entry share of $c_{\\mathrm{seg}}$ is uniform across all entries in the segment, and that all costs add linearly because the steps are serialized on the Central Processing Unit (CPU).\n\nDerive, from first principles and the given measurements, a closed-form expression for the expected CPU cycles per write, $C_{\\mathrm{cpu}}$, and then evaluate it numerically for the provided parameters. Express the final result in cycles and round your answer to four significant figures.", "solution": "The problem requires the derivation of a closed-form expression for the expected CPU cycles per user write, denoted as $C_{\\mathrm{cpu}}$, and its subsequent numerical evaluation. The derivation will be based on first principles, breaking down the total cost into its constituent components as described in the problem statement. The principle of linearity allows us to sum these individual costs.\n\nFirst, let us define the given parameters symbolically and numerically:\n- Segment buffer size: $S = 2\\,\\mathrm{MiB} = 2 \\times (1024)^2\\,\\mathrm{bytes} = 2 \\times 2^{20}\\,\\mathrm{bytes} = 2097152\\,\\mathrm{bytes}$.\n- File system block size: $B = 4096\\,\\mathrm{bytes} = 4 \\times 1024\\,\\mathrm{bytes} = 2^{12}\\,\\mathrm{bytes}$.\n- Average user write size: $w = 12\\,\\mathrm{KiB} = 12 \\times 1024\\,\\mathrm{bytes} = 12288\\,\\mathrm{bytes}$.\n- Summary metadata size per entry: $m = 32\\,\\mathrm{bytes}$.\n- CRC cost per byte: $c_{\\mathrm{crc}} = 3.2$ cycles/byte.\n- Metadata storage cost per byte: $c_{\\mathrm{meta}} = 0.8$ cycles/byte.\n- Fixed per-entry formatting overhead: $c_{\\mathrm{fmt}} = 150$ cycles/entry.\n- Hash table insertion cost per entry: $c_{\\mathrm{hash}} = 75$ cycles/entry.\n- Fixed per-segment header assembly cost: $c_{\\mathrm{seg}} = 10000$ cycles/segment.\n- Fixed per-write synchronization overhead: $c_{\\mathrm{w}} = 300$ cycles/write.\n\nThe total cost per write, $C_{\\mathrm{cpu}}$, is the sum of the fixed per-write overhead and the costs associated with processing the blocks that constitute the write. A single user write has a size of $w$ and is composed of $n$ blocks of size $B$. Given the assumption of block alignment, the number of blocks per write is:\n$$n = \\frac{w}{B}$$\nThe total cost can be expressed as:\n$$C_{\\mathrm{cpu}} = c_{\\mathrm{w}} + n \\cdot C_{\\mathrm{block}}$$\nwhere $C_{\\mathrm{block}}$ is the total CPU cost associated with processing a single block.\n\nThe cost per block, $C_{\\mathrm{block}}$, is the sum of several components. The problem states that each appended block contributes one summary entry. Therefore, the per-block costs are equivalent to the per-entry costs. These costs can be categorized as follows:\n\n1.  **Cost of CRC computation**: This cost is proportional to the block size $B$.\n    $$C_{\\mathrm{crc\\_block}} = c_{\\mathrm{crc}} \\cdot B$$\n2.  **Cost of summary entry generation**: This includes formatting and storing the metadata of size $m$, the fixed per-entry formatting overhead, and the hash table insertion cost.\n    $$C_{\\mathrm{summary\\_entry}} = (c_{\\mathrm{meta}} \\cdot m) + c_{\\mathrm{fmt}} + c_{\\mathrm{hash}}$$\n3.  **Amortized per-segment cost**: The fixed cost per segment, $c_{\\mathrm{seg}}$, must be distributed over all the blocks within that segment. First, we calculate the number of blocks, $N_{B/S}$, that can fit into a single segment of size $S$.\n    $$N_{B/S} = \\frac{S}{B}$$\n    The amortized cost per block (or per entry) is then:\n    $$C_{\\mathrm{amortized\\_seg}} = \\frac{c_{\\mathrm{seg}}}{N_{B/S}} = \\frac{c_{\\mathrm{seg}}}{S/B} = \\frac{c_{\\mathrm{seg}} \\cdot B}{S}$$\n\nThe total cost per block, $C_{\\mathrm{block}}$, is the sum of these components:\n$$C_{\\mathrm{block}} = C_{\\mathrm{crc\\_block}} + C_{\\mathrm{summary\\_entry}} + C_{\\mathrm{amortized\\_seg}}$$\n$$C_{\\mathrm{block}} = (c_{\\mathrm{crc}} \\cdot B) + (c_{\\mathrm{meta}} \\cdot m + c_{\\mathrm{fmt}} + c_{\\mathrm{hash}}) + \\frac{c_{\\mathrm{seg}} \\cdot B}{S}$$\n\nNow, we substitute this expression for $C_{\\mathrm{block}}$ back into the equation for $C_{\\mathrm{cpu}}$:\n$$C_{\\mathrm{cpu}} = c_{\\mathrm{w}} + n \\cdot \\left( c_{\\mathrm{crc}} \\cdot B + c_{\\mathrm{meta}} \\cdot m + c_{\\mathrm{fmt}} + c_{\\mathrm{hash}} + \\frac{c_{\\mathrm{seg}} \\cdot B}{S} \\right)$$\nSubstituting $n = w/B$, we obtain the final closed-form expression for the expected CPU cycles per write:\n$$C_{\\mathrm{cpu}} = c_{\\mathrm{w}} + \\frac{w}{B} \\left( c_{\\mathrm{crc}} \\cdot B + c_{\\mathrm{meta}} \\cdot m + c_{\\mathrm{fmt}} + c_{\\mathrm{hash}} + \\frac{c_{\\mathrm{seg}} \\cdot B}{S} \\right)$$\n\nNext, we evaluate this expression numerically using the provided parameters.\n\nFirst, calculate the number of blocks per write, $n$:\n$$n = \\frac{w}{B} = \\frac{12288\\,\\mathrm{bytes}}{4096\\,\\mathrm{bytes}} = 3$$\n\nNow, we calculate the components of the cost per block, $C_{\\mathrm{block}}$:\n- CRC cost per block:\n$C_{\\mathrm{crc\\_block}} = c_{\\mathrm{crc}} \\cdot B = 3.2 \\cdot 4096 = 13107.2$ cycles.\n- Summary entry cost (excluding amortized cost):\n$C_{\\mathrm{summary\\_entry}} = (c_{\\mathrm{meta}} \\cdot m) + c_{\\mathrm{fmt}} + c_{\\mathrm{hash}} = (0.8 \\cdot 32) + 150 + 75 = 25.6 + 150 + 75 = 250.6$ cycles.\n- Amortized segment cost per block:\nFirst, find the number of blocks per segment, $N_{B/S}$:\n$$N_{B/S} = \\frac{S}{B} = \\frac{2097152\\,\\mathrm{bytes}}{4096\\,\\mathrm{bytes}} = 512$$\nThen, the amortized cost is:\n$$C_{\\mathrm{amortized\\_seg}} = \\frac{c_{\\mathrm{seg}}}{N_{B/S}} = \\frac{10000}{512} = 19.53125 \\text{ cycles.}$$\n\nThe total cost per block is the sum of these components:\n$$C_{\\mathrm{block}} = 13107.2 + 250.6 + 19.53125 = 13377.33125 \\text{ cycles.}$$\n\nFinally, we calculate the total CPU cycles per write, $C_{\\mathrm{cpu}}$:\n$$C_{\\mathrm{cpu}} = c_{\\mathrm{w}} + n \\cdot C_{\\mathrm{block}} = 300 + 3 \\cdot 13377.33125$$\n$$C_{\\mathrm{cpu}} = 300 + 40131.99375 = 40431.99375 \\text{ cycles.}$$\n\nThe problem requires rounding the final answer to four significant figures. The calculated value is $40431.99375$. The first four significant digits are $4, 0, 4, 3$. The fifth digit is $1$, so we round down.\n$$C_{\\mathrm{cpu}} \\approx 40430$$\nTo express this unambiguously with four significant figures, we can use scientific notation: $4.043 \\times 10^4$.", "answer": "$$\\boxed{4.043 \\times 10^4}$$", "id": "3654765"}, {"introduction": "A key challenge in LFS design is managing the cleaning process to reclaim space without introducing excessive overhead, a phenomenon known as write amplification. This exercise [@problem_id:3654742] guides you through building a probabilistic model to determine the optimal cleaning threshold. You will use principles of conditional expectation to derive a cost function for device traffic and find the utilization threshold $u_t$ that minimizes it, offering insight into a fundamental LFS performance trade-off.", "problem": "A Log-Structured File System (LFS) cleaner reclaims space by selecting segments below a utilization threshold. Consider the following principled model grounded in definitions of segment cleaning and probabilistic selection.\n\nA large pool of segments has independent utilizations modeled by a random variable $U$ that is uniform on $[0,1]$. When the cleaner runs, it selects uniformly among segments with $U \\le u_t$, where $u_t \\in (0,1)$ is a fixed trigger threshold. Cleaning a segment with utilization $u$ reads $1$ segment and writes the fraction $u$ of live data to new segments, thereby freeing a fraction $1-u$ of space. The act of writing new user data itself always incurs exactly $1$ segment’s worth of device traffic per unit of data written (normalized to $1$). Additionally, before cleaning, the system may need to scan candidate segments to find those with $U \\le u_t$. Each candidate segment examined during scanning incurs a normalized, workload-dependent device-traffic overhead of $c_m$, where $m \\in \\{\\text{hot}, \\text{cold}\\}$ denotes the workload mode. Assume the expected number of candidates examined to find an eligible segment equals the reciprocal of the eligibility probability.\n\nWorkload varies stochastically across time in two modes:\n- With probability $p$, the system is in a hot mode incurring scanning cost $c_{\\mathrm{hot}}$ per candidate.\n- With probability $1-p$, the system is in a cold mode incurring scanning cost $c_{\\mathrm{cold}}$ per candidate.\n\nAssume $p = \\frac{1}{3}$, $c_{\\mathrm{hot}} = 2$, and $c_{\\mathrm{cold}} = \\frac{1}{2}$. Segment sizes and all device traffic quantities are normalized so that the above units are dimensionless.\n\nStarting from the definitions above and basic probability (in particular, conditional expectation and the expected count of trials before success in a Bernoulli process), derive the expected total device traffic per unit of new user data, $E[W(u_t)]$, as a function of $u_t$, where the expectation is taken over the workload mode randomness. Then, determine the value of $u_t \\in (0,1)$ that minimizes $E[W(u_t)]$. Express your final answer for the optimal threshold $u_t^{\\star}$ as a single closed-form expression.", "solution": "The objective is to find the optimal utilization threshold $u_t^{\\star}$ that minimizes the expected total device traffic per unit of new user data, denoted as $E[W(u_t)]$. This quantity is commonly known as the write amplification factor.\n\nFirst, let's establish the relationship between the components of device traffic. The total device traffic is the sum of traffic from writing new user data and traffic from the cleaning process. The problem normalizes the traffic for writing new user data to $1$ unit of traffic per unit of data. To write $1$ unit of new data, $1$ unit of free space is required. This space must be reclaimed by the cleaner. The total traffic per unit of new data, $W$, can be expressed as:\n$$\nW = 1 + (\\text{Cleaning traffic incurred to free } 1 \\text{ unit of space})\n$$\nThe cleaning traffic itself is composed of two parts: scanning for eligible segments and the read/write operations for cleaning a segment.\n\nLet $U$ be the random variable representing segment utilization, with $U \\sim \\text{Uniform}[0, 1]$.\nA segment is eligible for cleaning if its utilization is at or below the threshold $u_t$, where $u_t \\in (0, 1)$. The probability of a randomly chosen segment being eligible is:\n$$\nP(U \\le u_t) = \\int_0^{u_t} 1 \\, du = u_t\n$$\nThe utilization of a segment that is selected for cleaning, let's call it $U_s$, follows the distribution of $U$ conditioned on $U \\le u_t$. This conditional distribution is Uniform on $[0, u_t]$, with a probability density function $f_{U_s}(u) = \\frac{1}{u_t}$ for $u \\in [0, u_t]$.\n\nThe expected utilization of a cleaned segment is:\n$$\nE[U_s] = E[U | U \\le u_t] = \\int_0^{u_t} u \\cdot f_{U_s}(u) \\, du = \\int_0^{u_t} u \\frac{1}{u_t} \\, du = \\frac{1}{u_t} \\left[ \\frac{u^2}{2} \\right]_0^{u_t} = \\frac{u_t}{2}\n$$\nCleaning a segment with utilization $u$ frees $1-u$ units of space. The expected space freed per cleaned segment is:\n$$\nE[\\text{Space Freed}] = E[1 - U_s] = 1 - E[U_s] = 1 - \\frac{u_t}{2}\n$$\nThe traffic cost of cleaning a segment with utilization $u$ is $1$ (for reading the segment) plus $u$ (for writing the live data). The expected read/write traffic for cleaning one segment is:\n$$\nE[\\text{Read/Write Traffic}] = E[1 + U_s] = 1 + E[U_s] = 1 + \\frac{u_t}{2}\n$$\nNext, we consider the scanning cost. The process of finding an eligible segment is a sequence of Bernoulli trials with success probability $p_{succ} = u_t$. The expected number of trials (segments to scan) to find the first success is the mean of a geometric distribution, which is $1/p_{succ} = 1/u_t$.\nThe scanning cost per candidate segment is $c_m$, where $m \\in \\{\\text{hot}, \\text{cold}\\}$. The expected scanning traffic to find one eligible segment, conditional on the workload mode $m$, is:\n$$\nE[\\text{Scanning Traffic} | m] = \\frac{1}{u_t} \\cdot c_m = \\frac{c_m}{u_t}\n$$\nThe total expected traffic to clean one segment, conditional on mode $m$, is the sum of the expected scanning and read/write traffic:\n$$\nC_{seg}(u_t, m) = E[\\text{Scanning Traffic} | m] + E[\\text{Read/Write Traffic}] = \\frac{c_m}{u_t} + 1 + \\frac{u_t}{2}\n$$\nTo free $1$ unit of space, the expected number of segments that must be cleaned is:\n$$\nN_{seg} = \\frac{1}{E[\\text{Space Freed}]} = \\frac{1}{1 - \\frac{u_t}{2}}\n$$\nThe cleaning traffic incurred to free $1$ unit of space, conditional on mode $m$, is the product of the number of segments and the cost per segment:\n$$\nT_{clean}(u_t, m) = N_{seg} \\cdot C_{seg}(u_t, m) = \\frac{1}{1 - \\frac{u_t}{2}} \\left( \\frac{c_m}{u_t} + 1 + \\frac{u_t}{2} \\right)\n$$\nThe total traffic per unit of new data (write amplification), conditional on mode $m$, is $W(u_t, m) = 1 + T_{clean}(u_t, m)$:\n$$\nW(u_t, m) = 1 + \\frac{\\frac{c_m}{u_t} + 1 + \\frac{u_t}{2}}{1 - \\frac{u_t}{2}} = \\frac{(1 - \\frac{u_t}{2}) + (\\frac{c_m}{u_t} + 1 + \\frac{u_t}{2})}{1 - \\frac{u_t}{2}} = \\frac{2 + \\frac{c_m}{u_t}}{1 - \\frac{u_t}{2}}\n$$\nThe problem asks for the expected value of this quantity over the workload modes. Let the random variable for the cost be $C_M$. With probability $p$, $C_M = c_{\\mathrm{hot}}$, and with probability $1-p$, $C_M = c_{\\mathrm{cold}}$. The expected cost is $E[C_M] = p \\cdot c_{\\mathrm{hot}} + (1-p) \\cdot c_{\\mathrm{cold}}$. Let's denote this average cost by $\\bar{c}$.\nUsing the law of total expectation:\n$$\nE[W(u_t)] = p \\cdot W(u_t, \\text{hot}) + (1-p) \\cdot W(u_t, \\text{cold})\n$$\n$$\nE[W(u_t)] = p \\left(\\frac{2 + \\frac{c_{\\mathrm{hot}}}{u_t}}{1 - \\frac{u_t}{2}}\\right) + (1-p) \\left(\\frac{2 + \\frac{c_{\\mathrm{cold}}}{u_t}}{1 - \\frac{u_t}{2}}\\right) = \\frac{2p + \\frac{p c_{\\mathrm{hot}}}{u_t} + 2(1-p) + \\frac{(1-p)c_{\\mathrm{cold}}}{u_t}}{1 - \\frac{u_t}{2}}\n$$\n$$\nE[W(u_t)] = \\frac{2 + \\frac{p c_{\\mathrm{hot}} + (1-p)c_{\\mathrm{cold}}}{u_t}}{1 - \\frac{u_t}{2}} = \\frac{2 + \\frac{\\bar{c}}{u_t}}{1 - \\frac{u_t}{2}}\n$$\nNow, substitute the given numerical values: $p = 1/3$, $c_{\\mathrm{hot}} = 2$, and $c_{\\mathrm{cold}} = 1/2$.\n$$\n\\bar{c} = \\frac{1}{3}(2) + \\left(1-\\frac{1}{3}\\right)\\left(\\frac{1}{2}\\right) = \\frac{2}{3} + \\frac{2}{3} \\cdot \\frac{1}{2} = \\frac{2}{3} + \\frac{1}{3} = 1\n$$\nSo, the function to minimize is:\n$$\nf(u_t) = E[W(u_t)] = \\frac{2 + \\frac{1}{u_t}}{1 - \\frac{u_t}{2}} = \\frac{\\frac{2u_t+1}{u_t}}{\\frac{2-u_t}{2}} = \\frac{2(2u_t+1)}{u_t(2-u_t)} = \\frac{4u_t+2}{2u_t - u_t^2}\n$$\nTo find the minimum, we differentiate $f(u_t)$ with respect to $u_t$ and set the derivative to $0$. Using the quotient rule, where $g(u_t) = 4u_t+2$ and $h(u_t) = 2u_t - u_t^2$:\n$$\n\\frac{df}{du_t} = \\frac{(4)(2u_t - u_t^2) - (4u_t+2)(2 - 2u_t)}{(2u_t - u_t^2)^2}\n$$\nSetting the numerator to zero to find critical points:\n$$\n4(2u_t - u_t^2) - (4u_t+2)(2 - 2u_t) = 0\n$$\n$$\n(8u_t - 4u_t^2) - (8u_t - 8u_t^2 + 4 - 4u_t) = 0\n$$\n$$\n8u_t - 4u_t^2 - (4u_t - 8u_t^2 + 4) = 0\n$$\n$$\n8u_t - 4u_t^2 - 4u_t + 8u_t^2 - 4 = 0\n$$\n$$\n4u_t^2 + 4u_t - 4 = 0\n$$\n$$\nu_t^2 + u_t - 1 = 0\n$$\nWe solve this quadratic equation for $u_t$:\n$$\nu_t = \\frac{-1 \\pm \\sqrt{1^2 - 4(1)(-1)}}{2(1)} = \\frac{-1 \\pm \\sqrt{5}}{2}\n$$\nSince the threshold $u_t$ must be in the interval $(0, 1)$, we take the positive root:\n$$\nu_t^{\\star} = \\frac{-1 + \\sqrt{5}}{2}\n$$\nThe value $\\sqrt{5}$ is approximately $2.236$, so $u_t^{\\star} \\approx \\frac{-1+2.236}{2} \\approx 0.618$, which lies in the domain $(0,1)$. The second derivative test or analysis of the first derivative's sign shows this is a minimum. The first derivative's numerator, $4(u_t^2 + u_t - 1)$, is a parabola opening upwards, negative between its roots and positive elsewhere. Thus, the function $f(u_t)$ decreases before $u_t^{\\star}$ and increases after, confirming a minimum.", "answer": "$$\\boxed{\\frac{-1 + \\sqrt{5}}{2}}$$", "id": "3654742"}, {"introduction": "Beyond performance, a file system's primary duty is to maintain data integrity and consistency, even across system crashes. This practice [@problem_id:3654795] delves into the critical task of ensuring that file system operations are atomic, using the example of creating a POSIX hard link. You will evaluate different implementation strategies in a Copy-On-Write (CoW) LFS to understand how to correctly manage metadata updates and preserve invariants through mechanisms like atomic commit records.", "problem": "Consider a Log-Structured File System (LFS) that implements Copy-On-Write (CoW) for metadata updates, including inode modifications. The system aims to preserve the Portable Operating System Interface (POSIX) hard link semantics in the presence of crashes and concurrent updates. The fundamental base you may assume includes the following well-tested facts and core definitions: a hard link creates a new directory entry pointing to an existing inode; the POSIX link count of an inode equals the number of directory entries that reference that inode; a system call is atomic with respect to crash consistency if the post-crash state is equivalent to some serial execution in which the system call either fully occurred or did not occur; and CoW writes produce new versions of logical objects that become visible only once committed.\n\nLet an inode be identified by an immutable inode number $i$. Let the set of directory entries at time $t$ be $\\mathcal{D}(t) \\subseteq \\text{Names} \\times \\text{InodeNumbers}$, where a directory entry $(n, i)$ means name $n$ maps to inode $i$. Let the link count stored within the inode’s metadata be $\\ell_i(t) \\in \\mathbb{N}$ at time $t$. POSIX requires the invariant\n$$\\ell_i(t) = \\left| \\{ (n, j) \\in \\mathcal{D}(t) \\mid j = i \\} \\right|,$$\nand requires that the state after a crash at any time $t$ is equivalent to a serial history in which any completed system calls are fully reflected.\n\nThe LFS appends records to a sequential log. Each logical metadata change is encoded as an append-only record. Records are applied to produce a new filesystem state via a deterministic replay function $S(\\mathcal{R})$, where $\\mathcal{R}$ is the set of records deemed committed. A commit record $C$ may be used to demarcate an atomic group of records; only groups terminated by a valid $C$ are considered committed. Assume the storage can lose a suffix of the log on crash, but does not reorder or corrupt individual records already flushed.\n\nYou are given four candidate designs for implementing the $link(n, i)$ system call that must satisfy the POSIX invariant and atomicity requirements under crash and concurrency. The initial pre-call state at time $t_0$ has $\\ell_i(t_0) = k$ and no directory entry $(n, i)$ in $\\mathcal{D}(t_0)$. The goal of $link(n, i)$ is to make $\\ell_i$ become $k+1$ and add $(n, i)$, with atomic crash consistency.\n\n- A. The implementation appends an intent record $L_{\\text{intent}}(n, i)$, then appends a CoW inode update record $I(i, \\ell_i \\mapsto k+1)$ derived from a read of the current inode state, then appends a directory entry record $D(n \\mapsto i)$, and finally appends a commit record $C$. During replay, a group is applied only if $C$ is present; otherwise none of $L_{\\text{intent}}, I, D$ are applied. Concurrent link operations against the same inode serialize via the log: each operation reads the latest committed $\\ell_i$, and if on replay two $I$ records for the same group would conflict, only groups with $C$ are applied and non-committed intents are ignored.\n- B. The implementation appends the directory entry record $D(n \\mapsto i)$ and flushes it, then appends the CoW inode update record $I(i, \\ell_i \\mapsto k+1)$ in a later segment without a grouping commit record. It relies on the fact that both records will eventually be written to the log and on mount the system trusts the inode’s stored link count $\\ell_i$ without recomputation.\n- C. The implementation appends the CoW inode update record $I(i, \\ell_i \\mapsto k+1)$ first, flushes it, and later appends the directory entry record $D(n \\mapsto i)$, both in the same segment with a segment checksum but no explicit commit record. It relies on the assumption that a segment is written atomically and that partial segment writes will either include both records or neither.\n- D. The implementation appends only a directory entry record $D(n \\mapsto i)$ and an intent record $L_{\\text{intent}}(n, i)$, deferring the inode link count increment to a background refcount reconciler that periodically scans directories to recompute $\\ell_i$ as $\\left| \\{ (n, j) \\in \\mathcal{D} \\mid j = i \\} \\right|$. It does not use commit records and permits $D$ to be visible to applications immediately after $link(n, i)$ returns.\n\nWhich option(s) ensure that, for any crash at time $t$, the post-crash state $S(\\mathcal{R}(t))$ satisfies both the POSIX hard link invariant $\\ell_i(t) = \\left| \\{ (n, j) \\in \\mathcal{D}(t) \\mid j = i \\} \\right|$ and the atomicity requirement that $link(n, i)$ either is fully reflected as $\\ell_i = k+1$ with $(n, i) \\in \\mathcal{D}$ or not reflected at all?\n\nChoose all that apply.", "solution": "### Solution Derivation\n\nThe task is to evaluate four proposed designs for implementing `link(n, i)`. A correct design must guarantee that the two constituent updates—creating the directory entry `D(n \\mapsto i)` and incrementing the inode's link count `I(i, \\ell_i \\mapsto k+1)`—are performed atomically, not just with respect to crashes but also in the presence of concurrent operations on the same inode. This atomicity must preserve the POSIX invariant $\\ell_i(t) = |\\{\\text{directory entries pointing to } i\\}|$.\n\n#### Option-by-Option Analysis\n\n**A. The implementation appends an intent record $L_{\\text{intent}}(n, i)$, then appends a CoW inode update record $I(i, \\ell_i \\mapsto k+1)$, then a directory entry record $D(n \\mapsto i)$, and finally a commit record $C$.**\n\n- **Crash Consistency:** This design groups all necessary records for the `link` operation into a single transaction demarcated by a commit record $C$. The problem defines that only record groups terminated by a valid $C$ are considered committed.\n    - If a crash occurs at any point before the record $C$ is durably written to the log, the group is incomplete. Upon recovery, the replay function $S(\\mathcal{R})$ will not apply any records from this partial group. The filesystem state will revert to the pre-call state ($\\ell_i = k$, no new link), satisfying the \"did not occur\" aspect of atomicity. The POSIX invariant remains valid.\n    - If the crash occurs after $C$ is durably written, the entire group is considered committed. The replay function will apply both the inode update $I(i, \\ell_i \\mapsto k+1)$ and the directory entry update $D(n \\mapsto i)$. The resulting state has $\\ell_i = k+1$ and the new directory entry exists, making the true link count $k+1$. This satisfies the \"fully occurred\" aspect of atomicity, and the POSIX invariant is upheld.\n- **Concurrency:** The update to the link count ($\\ell_i \\leftarrow \\ell_i + 1$) is a read-modify-write operation, which is vulnerable to race conditions under concurrency. For example, two concurrent `link` operations on the same inode might both read $\\ell_i=k$ and both write updates to $\\ell_i=k+1$, leading to an incorrect final count of $k+1$ instead of $k+2$. The description for option A states: \"Concurrent link operations against the same inode serialize via the log: each operation reads the latest committed $\\ell_i$\". This is a crucial assertion. It implies that the system provides a serialization mechanism (e.g., in-memory locks on inodes, not detailed but whose effect is guaranteed) that prevents two operations from reading the same value of $\\ell_i$ before one of them has committed. The description also mentions a conflict resolution strategy on replay, which, however imprecisely worded, suggests a mechanism like optimistic concurrency control to abort transactions based on stale reads. This explicit, albeit imperfect, handling of concurrency makes the design robust.\n\n**Verdict on A: Correct.** This design correctly uses an atomic commit group for crash consistency and asserts a serialization mechanism to handle concurrent updates, thereby satisfying all problem requirements.\n\n**B. The implementation appends the directory entry record $D(n \\mapsto i)$ and flushes it, then appends the CoW inode update record $I(i, \\ell_i \\mapsto k+1)$ in a later segment without a grouping commit record.**\n\n- **Crash Consistency:** The two essential updates, $D$ and $I$, are written separately without any atomic grouping mechanism. The problem's crash model allows for the loss of a log suffix. Therefore, a crash can occur after the record $D(n \\mapsto i)$ has been flushed to stable storage but before the record $I(i, \\ell_i \\mapsto k+1)$ has been written.\n    - In this crash scenario, recovery would replay the log and apply the $D$ record, creating the new directory entry. The true number of links to inode $i$ would become $k+1$. However, the $I$ record was lost, so the stored link count $\\ell_i$ in the inode's metadata remains $k$. The resulting state is $\\ell_i = k$ and $|\\{\\text{links}\\}| = k+1$.\n- This directly violates the POSIX invariant. The operation is not atomic, as the system is left in a partially updated, inconsistent state.\n\n**Verdict on B: Incorrect.** This design fails to provide atomicity and breaks the POSIX invariant in a common crash scenario.\n\n**C. The implementation appends the CoW inode update record $I(i, \\ell_i \\mapsto k+1)$ first... and later appends the directory entry record $D(n \\mapsto i)$, both in the same segment with a segment checksum but no explicit commit record. It relies on the assumption that a segment is written atomically...**\n\n- **Crash Consistency:** This design replaces the logical commit record $C$ with a physical grouping mechanism: a checksummed segment. In LFS, a checksum over a segment can be used to validate its integrity. During recovery, an incomplete or corrupt segment (due to a crash during its write) would fail its checksum verification and be discarded. This effectively makes the segment an atomic unit of writing: either the entire segment is applied, or none of it is. This correctly ensures atomicity for a single operation against crashes.\n- **Concurrency:** This design suffers from the same potential read-modify-write race condition as discussed for option A. Two concurrent `link` operations on the same inode could both read $\\ell_i = k$ and generate updates to $\\ell_i = k+1$. If both of their respective atomic segments are successfully written to the log, replay would result in a final stored link count of $\\ell_i = k+1$, while the true number of links would be $k+2$. Unlike option A, the description for option C provides no statement or mechanism for handling this concurrency issue. The responsibility of a correct solution is to handle *both* crashes and concurrency.\n\n**Verdict on C: Incorrect.** While providing crash atomicity for a single operation, this design fails to address the specified requirement of handling concurrent updates, leading to a race condition that violates the POSIX invariant.\n\n**D. The implementation appends only a directory entry record $D(n \\mapsto i)$... deferring the inode link count increment to a background refcount reconciler... It does not use commit records and permits $D$ to be visible... immediately.**\n\n- **POSIX Invariant:** This design explicitly and intentionally creates a state that violates the POSIX invariant. Immediately after the `link(n,i)` call returns, and before the background reconciler runs, the new directory entry exists (true link count is $k+1$), but the stored link count `\\ell_i` is still $k$. This violation occurs even without a crash. The problem requires the POSIX invariant to hold in the post-crash state $S(\\mathcal{R}(t))$, which is the state immediately after log replay. The action of a future background process is not part of $S(\\mathcal{R}(t))$. Therefore, after a crash and replay, the system will be in a state where $\\ell_i \\neq |\\{\\text{links}\\}|$.\n\n**Verdict on D: Incorrect.** This design fundamentally violates the POSIX invariant as a standard part of its operation, which is explicitly disallowed by the problem's requirements.", "answer": "$$\\boxed{A}$$", "id": "3654795"}]}