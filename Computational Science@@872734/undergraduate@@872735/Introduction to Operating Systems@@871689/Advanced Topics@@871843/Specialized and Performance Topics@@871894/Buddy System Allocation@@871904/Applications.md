## Applications and Interdisciplinary Connections

The [buddy system](@entry_id:637828), with its elegant principles of binary [recursive partitioning](@entry_id:271173) and expedient coalescing, extends its utility far beyond the introductory examples of memory management. Its hierarchical structure and predictable behavior make it a versatile and powerful tool that has been adapted and applied in a wide array of contexts, from the foundational layers of the operating system to the frontiers of [computer architecture](@entry_id:174967), system security, and even as an abstract paradigm for managing non-spatial resources. This chapter explores these diverse applications, demonstrating how the core mechanisms of the [buddy system](@entry_id:637828) are leveraged to solve complex, real-world problems.

### Core Operating System Functions

The most direct and fundamental application of the [buddy system](@entry_id:637828) is in the management of physical memory within the operating system kernel. During the system's bootstrap phase, the [buddy allocator](@entry_id:747005) is often one of the first [memory management](@entry_id:636637) services to become active. It takes control of large, contiguous regions of RAM and serves the initial, critical allocation requests required to bring the rest of the system online. This includes allocating space for the kernel's own code and data segments (the text and bss), loading the initial RAM disk (initrd) which contains necessary drivers and utilities, and setting up temporary page tables for initializing virtual memory. As the boot process completes, temporary structures like the initrd and initial page tables are deallocated. The [buddy system](@entry_id:637828)'s efficient coalescing mechanism ensures that these freed blocks are immediately merged with their buddies, progressively reconstructing larger free blocks and combating fragmentation from the earliest moments of the system's life [@problem_id:3624799].

Beyond general-purpose page allocation, the [buddy system](@entry_id:637828) is also employed to manage specific, structured resources within the kernel that have unique requirements. One such example is the allocation of thread stacks. For security and stability, modern operating systems often place unmapped "guard pages" at the boundaries of a stack to detect overflow conditions. A [buddy allocator](@entry_id:747005) can be adapted to this need by reserving a block large enough to contain the desired stack size plus the guard pages. For instance, a request for an 18 KiB stack might be satisfied by a 32 KiB buddy block, providing ample space for the stack and its guards. The power-of-two sizing proves advantageous when a thread's stack needs to grow. The kernel can attempt an efficient in-place expansion by coalescing the current block with its buddy, provided the buddy is entirely free. If not, the system can fall back to allocating a new, larger block and migrating the stack, a process seamlessly managed by the [buddy system](@entry_id:637828)'s allocation and free mechanisms [@problem_id:3624788].

The [buddy system](@entry_id:637828)'s signal advantage over other allocators, such as the [slab allocator](@entry_id:635042), is its ability to guarantee physically contiguous blocks of memory spanning multiple pages. This capability is indispensable for devices that use Direct Memory Access (DMA). A DMA-capable device, such as a high-performance network card or disk controller, needs to read or write to a buffer in [main memory](@entry_id:751652) that forms a single, unbroken range of physical addresses. A [slab allocator](@entry_id:635042), which typically carves up single pages into smaller objects, cannot satisfy a DMA buffer request larger than one page. The [buddy system](@entry_id:637828), by its very design, is the natural solution. A request for a 64 KiB DMA buffer on a system with 4 KiB pages would be translated into a request for a contiguous block of 16 pages (an order-4 block, if the base page is order-0), which the [buddy system](@entry_id:637828) can provide by construction [@problem_id:3683586].

However, the primary trade-off for the [buddy system](@entry_id:637828)'s speed and structural simplicity is [internal fragmentation](@entry_id:637905). Because every allocation is rounded up to the next power of two, a significant portion of allocated memory may go unused. For example, a sequence of requests for 62 KiB, 90 KiB, 35 KiB, and 10 KiB would require allocations of 64 KiB, 128 KiB, 64 KiB, and 16 KiB, respectively. In this scenario, 197 KiB of memory is requested, but 272 KiB is allocated, resulting in 75 KiB of wasted space, a waste fraction of about 28%. This effect is also observed when the [buddy allocator](@entry_id:747005) serves as the backing store for higher-level [data structures](@entry_id:262134) like [dynamic arrays](@entry_id:637218), whose [geometric growth](@entry_id:174399) policies (e.g., doubling capacity) can interact with the allocator's rounding to create significant unused space within allocated blocks [@problem_id:3628282] [@problem_id:3230274]. Sophisticated memory management subsystems often use the [buddy system](@entry_id:637828) for large, multi-page allocations where its properties are essential, while employing other allocators like the slab system for small, fixed-size objects to mitigate this [internal fragmentation](@entry_id:637905).

### Advanced Architectures and Virtualization

The [buddy system](@entry_id:637828)'s role is not confined to physical [memory management](@entry_id:636637) alone; it is a crucial enabler for performance optimizations in modern computer architectures, particularly in the realm of [virtual memory](@entry_id:177532). The Translation Lookaside Buffer (TLB) is a small, fast hardware cache that stores recent virtual-to-physical address translations. A TLB miss is costly, requiring a walk of the [page table structure](@entry_id:753083) in memory. To improve TLB efficiency, modern CPUs support "[huge pages](@entry_id:750413)" (e.g., 2 MiB or 1 GiB instead of the standard 4 KiB). A single huge page can cover a large memory region with just one TLB entry. The [buddy system](@entry_id:637828) is fundamental to this process, as creating a 2 MiB huge page requires a free, 2 MiB-aligned, physically contiguous block of memory. In a [buddy system](@entry_id:637828) with a 4 KiB base page, this corresponds exactly to a free block of order 9 ($2^9 \times 4 \text{ KiB} = 2 \text{ MiB}$). The allocator's ability to provide these large, aligned blocks is therefore a direct prerequisite for the operating system's ability to leverage [huge pages](@entry_id:750413) and enhance application performance by increasing TLB coverage [@problem_id:3624806].

In multi-socket systems with Non-Uniform Memory Access (NUMA) architectures, memory access latency depends on the physical location of the memory relative to the executing processor. To manage this, operating systems maintain separate [buddy system](@entry_id:637828) instances for each NUMA node's local memory. When a process requests memory, the allocator faces a policy decision: it can satisfy the request from the local node, which offers the lowest latency. However, if this requires splitting a large, high-order free block, it may fragment the local memory, making it difficult to satisfy future requests for large contiguous blocks (like [huge pages](@entry_id:750413)) on that node. The alternative is to serve the request from a remote node, incurring a higher access latency for the application but preserving the large blocks on the local node. Modern kernels implement sophisticated policies that weigh these trade-offs, sometimes opting for a higher-latency remote allocation to protect valuable, high-order local blocks from fragmentation [@problem_id:3624849].

This concept of managing distinct memory pools with buddy allocators extends to the growing field of heterogeneous memory systems, which combine fast but volatile DRAM with slower but persistent Non-Volatile Memory (NVM). An OS can manage the DRAM and NVM pools with separate buddy allocators. A key challenge is ensuring that "hot" (frequently accessed) data resides in DRAM while "cold" (infrequently accessed) data is moved to NVM. The system can track the access intensity of memory objects and use this information to drive migration policies. For example, if a hot new object requires a large block of DRAM that is currently unavailable, the system can evaluate migrating a cold object out of DRAM and into an available block in the NVM pool. This decision involves a [cost-benefit analysis](@entry_id:200072), comparing the one-time migration cost against the cumulative performance benefit of placing the hot object in low-latency memory. The buddy allocators for each pool provide the mechanism for finding and managing the free space needed to orchestrate these data movements [@problem_id:3624828].

In virtualized environments, the [buddy system](@entry_id:637828)'s structural properties inform [memory reclamation](@entry_id:751879) strategies. A hypervisor can reclaim memory from a guest Virtual Machine (VM) through a process called "ballooning," where a driver inside the guest allocates and "pins" memory, allowing the [hypervisor](@entry_id:750489) to reclaim the underlying host physical frames. If the [hypervisor](@entry_id:750489)'s goal is to create a free huge page (e.g., an order-9 block), a naive, random reclamation of pages is unlikely to succeed. A more intelligent policy can be implemented: the [hypervisor](@entry_id:750489) identifies the host physical "pageblock" (an aligned 2 MiB region) that is closest to being entirely free and directs the VM balloon driver to target its reclamation efforts specifically on the remaining occupied pages within that block. By surgically freeing just the right pages, the system can efficiently consolidate a free order-9 block, making it available for THP promotion, without resorting to costly full-[memory compaction](@entry_id:751850) [@problem_id:3624812]. Real-world kernels like Linux use these watermark-based and locality-aware [heuristics](@entry_id:261307) to guide [buddy allocator](@entry_id:747005) behavior, preventing low-priority allocations from causing fragmentation that would hinder high-priority goals like huge page formation [@problem_id:3683554].

### Security and Storage Systems

The predictable address structure of the [buddy system](@entry_id:637828) has important, and sometimes subtle, implications for system security. In systems that employ full [memory encryption](@entry_id:751857) using a cipher like AES-GCM, it is a cryptographic imperative that the Initialization Vector (IV) never be reused with the same key for different plaintexts. A naive IV derivation scheme, for example one based solely on the physical address of the data being written, is insecure in a system using a [buddy allocator](@entry_id:747005). Consider a physical address that is first part of a large, order-$(o+1)$ block. After this block is freed and split, the same physical address may become part of a smaller, order-$o$ block. If the IV is derived only from the address, an IV collision would occur. Even including the block's order in the IV is insufficient, as a block of a specific address and order can be allocated, freed, and later re-allocated. The per-line write counters often used in these schemes are typically only unique *within* a single allocation lifetime. A robust solution requires a component that can distinguish between different temporal instances of an allocation. A secure strategy incorporates a per-allocation "epoch"—a unique random number generated each time a block is allocated from the free list. This epoch, when included in the IV derivation, ensures that even if the same physical block is reused over time, the IVs will be unique, thus preserving confidentiality [@problem_id:3624819].

The core concept of the [buddy system](@entry_id:637828)—a hierarchy of power-of-two sized blocks that can be split and merged—is general enough to be adapted for resources other than RAM. One compelling application is in the management of erase blocks in a Flash Translation Layer (FTL) for solid-state drives. Flash memory has a finite number of erase cycles, making wear leveling a critical concern. One can model a storage device's address space as a collection of buddy groups of erase blocks. Coalescing two adjacent, free buddy groups into a larger "superblock" is permitted only if two conditions are met: first, all constituent blocks must be free (potentially requiring garbage collection to move valid data elsewhere), and second, the wear counts of the blocks must be similar. For instance, the policy might require the difference between the maximum and minimum erase counts within the group to be below a certain threshold. This modification adapts the classic coalescing rule to the physical constraints of the underlying hardware, balancing the need for large, contiguous write areas with the long-term goal of distributing wear evenly across the device [@problem_id:3624787].

### The Buddy System as a General Resource Allocation Paradigm

The principles of the [buddy system](@entry_id:637828) can be abstracted and applied to the management of any resource that is fungible and recursively divisible. CPU time, for example, can be managed with a time-analogue of the [buddy system](@entry_id:637828). A scheduler might manage a fixed-length time frame (e.g., 16 ms) as a single resource block. When processes request CPU time, their requests are rounded up to the nearest power-of-two [time quantum](@entry_id:756007) (e.g., a 3 ms request is granted a 4 ms slice). This leads to a concept of "time-[internal fragmentation](@entry_id:637905)"—the allocated but unused time within a quantum. At the end of the frame, all time slots become "free" and can be conceptually coalesced back into the original full frame. While this approach can lead to fairness issues if some processes are deferred due to block size unavailability, it demonstrates the versatility of the buddy model for scheduling discrete, divisible resources [@problem_id:3624783].

A similar analogy applies to network bandwidth allocation. An Internet Service Provider (ISP) could manage the capacity of a core link (e.g., 1024 Mbps) using a [buddy system](@entry_id:637828). A customer's request for a certain bitrate is rounded up to the nearest power of two and allocated from the link's capacity. When a customer's flow terminates, its bandwidth block is freed. If its buddy block is also free, the two are coalesced into a larger block of available capacity. This allows the ISP to efficiently track and allocate bandwidth in discrete, hierarchical chunks, simplifying management at the cost of some potential bandwidth fragmentation [@problem_id:3624863]. These examples highlight that the [buddy system](@entry_id:637828) is not merely a [memory allocation](@entry_id:634722) algorithm but a fundamental pattern for resource management, applicable wherever a resource can be hierarchically partitioned and recombined.