{"hands_on_practices": [{"introduction": "A core design pattern in Asymmetric Multiprocessing (AMP) is the master-worker model, where one core handles privileged operations for many others. This exercise allows you to quantify a fundamental performance trade-off of this design: the latency incurred when a worker core forwards a system call to the master. By modeling the master core as a simple queue, you will apply principles from queueing theory to predict system-wide latency, a crucial skill for analyzing and designing responsive operating systems. [@problem_id:3621335]", "problem": "Consider an Asymmetric Multiprocessing (AMP) Operating System (OS) design used in a teaching lab. In this AMP system, there is a single dedicated master core that executes all privileged kernel system calls, and there are $n$ identical worker cores that execute user-level threads. When a user thread running on a worker core issues a system call, the worker forwards the system call to the master core. The forwarding incurs a measured software overhead $c$ on the worker to marshal arguments and trigger an inter-processor interrupt, and when the master completes the system call it sends the result back to the originating worker, incurring an additional overhead $c$ on the return path. Assume the following modeling assumptions as a fundamental base for analysis:\n\n- Each worker core generates system call requests according to an independent Poisson process with rate $\\lambda$ (requests per second).\n- The master core serves system call requests one at a time, with independent and identically distributed exponential service times having rate $\\mu$ (requests per second).\n- Superposition of independent Poisson processes yields a Poisson process with rate equal to the sum of individual rates, and the master can be modeled as an $M/M/1$ queue that is stable if and only if the total arrival rate is strictly less than the service rate.\n- All queues are First-In First-Out (FIFO), and the forwarding overheads $c$ on the forward and return paths are deterministic and do not overlap with service at the master.\n\nDefine the end-to-end latency of a system call as the elapsed time from the instant the worker initiates the system call forwarding until the instant the worker receives the return value. In the lab, the measured and configured parameters are:\n- Number of worker cores: $n = 6$.\n- Per-core system call arrival rate: $\\lambda = 800$ requests per second.\n- Master service rate: $\\mu = 6000$ requests per second.\n- One-way forwarding overhead: $c = 8$ microseconds.\n\nTasks:\n- Starting from the given modeling assumptions, derive symbolic expressions for:\n  1. The total arrival rate to the master, the traffic intensity, and the expected number of requests in the master’s queue.\n  2. The expected end-to-end system call latency as a function of $n$, $\\lambda$, $\\mu$, and $c$.\n- Then, using the provided numerical values, compute the expected end-to-end system call latency. Express your final numerical answer in milliseconds and round your answer to four significant figures.", "solution": "The problem asks for an analysis of system call latency in an Asymmetric Multiprocessing (AMP) system. The system is modeled using queueing theory, where the master core is treated as an $M/M/1$ queue. We are tasked with deriving symbolic expressions for key performance metrics and then calculating the numerical value of the end-to-end latency.\n\nFirst, we will derive the symbolic expressions as requested. The system consists of $n$ worker cores, each generating system call requests according to a Poisson process with rate $\\lambda$. These requests are all directed to a single master core.\n\n1.  **Total Arrival Rate, Traffic Intensity, and Expected Queue Length**\n\nThe total arrival rate to the master core, denoted by $\\Lambda$, is the sum of the rates from the $n$ independent worker cores. Based on the superposition property of Poisson processes:\n$$ \\Lambda = \\sum_{i=1}^{n} \\lambda = n\\lambda $$\nThe master core serves these requests with an exponential service time distribution at a rate of $\\mu$. The traffic intensity, $\\rho$, is the ratio of the total arrival rate to the service rate. It represents the fraction of time the server (master core) is busy.\n$$ \\rho = \\frac{\\Lambda}{\\mu} = \\frac{n\\lambda}{\\mu} $$\nFor the queueing system to be stable, the arrival rate must be strictly less than the service rate, which means $\\rho  1$.\n\nThe expected number of requests in the master's queue, $L_q$, for a stable $M/M/1$ system is given by the formula:\n$$ L_q = \\frac{\\rho^2}{1-\\rho} $$\nSubstituting the expression for $\\rho$, we get:\n$$ L_q = \\frac{\\left(\\frac{n\\lambda}{\\mu}\\right)^2}{1 - \\frac{n\\lambda}{\\mu}} = \\frac{(n\\lambda)^2}{\\mu(\\mu - n\\lambda)} $$\n\n2.  **Expected End-to-End System Call Latency**\n\nThe end-to-end latency of a system call, $T_{e2e}$, is defined as the time from the moment a worker initiates forwarding until it receives the result. This total time is the sum of three distinct, non-overlapping components:\n- The initial forwarding overhead, $c$.\n- The time spent in the master's system, $W$. This includes both the time waiting in the queue and the service time.\n- The return path overhead, $c$.\n\nThus, the total latency for a single request is $T_{e2e} = c + W + c = 2c + W$. We are interested in the expected value of this latency, $E[T_{e2e}]$. Since $c$ is a deterministic constant, we have:\n$$ E[T_{e2e}] = E[2c + W] = 2c + E[W] $$\n$E[W]$ is the expected time a request spends in the $M/M/1$ system (also known as the expected sojourn time). For a stable $M/M/1$ queue, this is given by:\n$$ E[W] = \\frac{1}{\\mu - \\Lambda} $$\nSubstituting $\\Lambda = n\\lambda$ into this expression gives:\n$$ E[W] = \\frac{1}{\\mu - n\\lambda} $$\nFinally, we can write the symbolic expression for the expected end-to-end latency:\n$$ E[T_{e2e}] = 2c + \\frac{1}{\\mu - n\\lambda} $$\n\nNow, we proceed to the numerical calculation using the provided parameter values:\n- Number of worker cores: $n = 6$\n- Per-core system call arrival rate: $\\lambda = 800 \\text{ s}^{-1}$\n- Master service rate: $\\mu = 6000 \\text{ s}^{-1}$\n- One-way forwarding overhead: $c = 8 \\text{ µs} = 8 \\times 10^{-6} \\text{ s}$\n\nFirst, we calculate the total arrival rate $\\Lambda$:\n$$ \\Lambda = n\\lambda = 6 \\times 800 \\text{ s}^{-1} = 4800 \\text{ s}^{-1} $$\nWe check the stability condition:\n$$ \\rho = \\frac{\\Lambda}{\\mu} = \\frac{4800}{6000} = 0.8 $$\nSince $\\rho = 0.8  1$, the system is stable, and our model is applicable.\n\nNext, we compute the expected time in the master's system, $E[W]$:\n$$ E[W] = \\frac{1}{\\mu - n\\lambda} = \\frac{1}{6000 - 4800} \\text{ s} = \\frac{1}{1200} \\text{ s} $$\nAs a decimal, $\\frac{1}{1200} \\approx 0.0008333... \\text{ s}$.\n\nNow, we calculate the expected end-to-end latency, $E[T_{e2e}]$:\n$$ E[T_{e2e}] = 2c + E[W] = 2 \\times (8 \\times 10^{-6} \\text{ s}) + \\frac{1}{1200} \\text{ s} $$\n$$ E[T_{e2e}] = 1.6 \\times 10^{-5} \\text{ s} + \\frac{1}{1200} \\text{ s} $$\n$$ E[T_{e2e}] = 0.000016 \\text{ s} + 0.0008333... \\text{ s} = 0.000849333... \\text{ s} $$\nThe problem requires the answer to be in milliseconds (ms). We convert the result from seconds to milliseconds:\n$$ E[T_{e2e}] = 0.000849333... \\text{ s} \\times 1000 \\frac{\\text{ms}}{\\text{s}} = 0.849333... \\text{ ms} $$\nFinally, we round the answer to four significant figures. The first four significant figures are $8, 4, 9, 3$. The following digit is $3$, which is less than $5$, so we do not round up.\n$$ E[T_{e2e}] \\approx 0.8493 \\text{ ms} $$", "answer": "$$\\boxed{0.8493}$$", "id": "3621335"}, {"introduction": "The primary motivation for AMP systems is to boost efficiency and performance by assigning tasks to specialized cores. This hands-on practice explores how a smart, \"aware\" scheduler can unlock this potential by routing tasks based on their characteristics, such as their I/O versus CPU-bound nature. By comparing the system throughput of a specialized scheduler against a simple, \"agnostic\" one, you will directly measure the performance gains achievable through intelligent software design in a heterogeneous environment. [@problem_id:3621298]", "problem": "An operating systems laboratory explores Asymmetric Multiprocessing (AMP), in which a heterogeneous multiprocessor platform contains specialized worker cores. Consider an AMP system with $2$ worker cores and a separate dispatch core whose overhead can be neglected for steady-state throughput. One worker, denoted $W_{\\mathrm{IO}}$, is tuned for Input/Output (I/O)-bound tasks; the other, denoted $W_{\\mathrm{CPU}}$, is tuned for CPU-bound tasks. The lab asks students to design a scheduler that uses the I/O wait ratio $\\omega$ (defined as the long-run fraction of time a task is not runnable because it waits on I/O) to prefer $W_{\\mathrm{IO}}$ for I/O-bound tasks and $W_{\\mathrm{CPU}}$ for CPU-bound tasks. The system runs at a high multiprogramming level with a very large number of independent tasks so that the ready queue is effectively never empty.\n\nModel the ready CPU work as a stream of independent “compute quanta” of uniform size. Each quantum belongs to either an I/O-bound task or a CPU-bound task. Across a long observation window, the measured classification driven by $\\omega$ yields that a fraction $p$ of all ready quanta are I/O-bound and a fraction $1-p$ are CPU-bound, where $p=\\frac{4}{7}$. The per-quantum processing times (service times) on the two workers, in milliseconds, are:\n- On $W_{\\mathrm{IO}}$: $t_{\\mathrm{IO},i}=3\\,\\mathrm{ms}$ for I/O-bound quanta; $t_{\\mathrm{IO},c}=7\\,\\mathrm{ms}$ for CPU-bound quanta.\n- On $W_{\\mathrm{CPU}}$: $t_{\\mathrm{CPU},i}=6\\,\\mathrm{ms}$ for I/O-bound quanta; $t_{\\mathrm{CPU},c}=4\\,\\mathrm{ms}$ for CPU-bound quanta.\n\nAssume there is no shared bottleneck other than the two workers, no interference between workers, and that routing decisions do not introduce additional delay. Also assume perfect classification when the $\\omega$-aware policy is used: all I/O-bound quanta are identified and can be routed to $W_{\\mathrm{IO}}$, and all CPU-bound quanta to $W_{\\mathrm{CPU}}$. Under the baseline “$\\omega$-agnostic” policy, each worker receives the same mixture with I/O-bound fraction $p$.\n\nStarting only from fundamental definitions such as “throughput equals the long-run number of completed quanta divided by elapsed time” and the fact that, for a saturated single server with independent service times, the long-run throughput equals the reciprocal of the long-run average service time, derive and compute the throughput difference\n$$\\Delta = \\text{throughput}_{\\omega\\text{-aware}} - \\text{throughput}_{\\omega\\text{-agnostic}}.$$\nExpress your final answer as an exact value in quanta per second. Do not round.", "solution": "The problem requires the calculation of the throughput difference between two scheduling policies on an Asymmetric Multiprocessing (AMP) system. We begin by establishing the fundamental principles and definitions provided. The total system throughput, which we will denote as $R$, is defined as the long-run number of completed quanta divided by the elapsed time. The system consists of two worker cores, $W_{\\mathrm{IO}}$ and $W_{\\mathrm{CPU}}$, operating in parallel. In a saturated state, the throughput of a single server is the reciprocal of its long-run average service time.\n\nLet $p$ be the fraction of I/O-bound quanta, given as $p = \\frac{4}{7}$. Consequently, the fraction of CPU-bound quanta is $1 - p = 1 - \\frac{4}{7} = \\frac{3}{7}$. The service times for I/O-bound ($i$) and CPU-bound ($c$) quanta on each worker are given in milliseconds ($ms$):\n- On $W_{\\mathrm{IO}}$: $t_{\\mathrm{IO},i} = 3\\,\\mathrm{ms}$ and $t_{\\mathrm{IO},c} = 7\\,\\mathrm{ms}$.\n- On $W_{\\mathrm{CPU}}$: $t_{\\mathrm{CPU},i} = 6\\,\\mathrm{ms}$ and $t_{\\mathrm{CPU},c} = 4\\,\\mathrm{ms}$.\n\nFirst, we analyze the \"$\\omega$-agnostic\" policy.\nUnder this policy, both workers receive the same statistical mixture of quanta. Since the ready queue is never empty, both workers are saturated. The total throughput of the system is the sum of the individual throughputs of the two workers.\nFor a single worker, the throughput is the reciprocal of its average service time, $\\bar{t}$. The average service time is the weighted average of the service times for I/O-bound and CPU-bound quanta.\n\nFor worker $W_{\\mathrm{IO}}$, the average service time is:\n$$ \\bar{t}_{\\mathrm{IO}, \\text{agnostic}} = p \\cdot t_{\\mathrm{IO},i} + (1-p) \\cdot t_{\\mathrm{IO},c} = \\left(\\frac{4}{7}\\right)(3) + \\left(\\frac{3}{7}\\right)(7) = \\frac{12}{7} + \\frac{21}{7} = \\frac{33}{7}\\,\\mathrm{ms} $$\nThe throughput of worker $W_{\\mathrm{IO}}$ is:\n$$ R_{\\mathrm{IO}, \\text{agnostic}} = \\frac{1}{\\bar{t}_{\\mathrm{IO}, \\text{agnostic}}} = \\frac{1}{33/7} = \\frac{7}{33}\\,\\text{quanta/ms} $$\n\nFor worker $W_{\\mathrm{CPU}}$, the average service time is:\n$$ \\bar{t}_{\\mathrm{CPU}, \\text{agnostic}} = p \\cdot t_{\\mathrm{CPU},i} + (1-p) \\cdot t_{\\mathrm{CPU},c} = \\left(\\frac{4}{7}\\right)(6) + \\left(\\frac{3}{7}\\right)(4) = \\frac{24}{7} + \\frac{12}{7} = \\frac{36}{7}\\,\\mathrm{ms} $$\nThe throughput of worker $W_{\\mathrm{CPU}}$ is:\n$$ R_{\\mathrm{CPU}, \\text{agnostic}} = \\frac{1}{\\bar{t}_{\\mathrm{CPU}, \\text{agnostic}}} = \\frac{1}{36/7} = \\frac{7}{36}\\,\\text{quanta/ms} $$\n\nThe total throughput for the $\\omega$-agnostic policy is the sum of the individual throughputs:\n$$ \\text{throughput}_{\\omega\\text{-agnostic}} = R_{\\mathrm{IO}, \\text{agnostic}} + R_{\\mathrm{CPU}, \\text{agnostic}} = \\frac{7}{33} + \\frac{7}{36} = 7 \\left( \\frac{1}{33} + \\frac{1}{36} \\right) = 7 \\left( \\frac{12 + 11}{396} \\right) = 7 \\left( \\frac{23}{396} \\right) = \\frac{161}{396}\\,\\text{quanta/ms} $$\n\nNext, we analyze the \"$\\omega$-aware\" policy.\nUnder this policy, perfect classification routes all I/O-bound quanta to $W_{\\mathrm{IO}}$ and all CPU-bound quanta to $W_{\\mathrm{CPU}}$. We model this as two parallel processing pipelines. Consider a large number of total quanta, $N$. The number of I/O-bound quanta is $N_i = pN$, and the number of CPU-bound quanta is $N_c = (1-p)N$.\nThe time required for $W_{\\mathrm{IO}}$ to process all $N_i$ quanta is:\n$$ T_{\\mathrm{IO}} = N_i \\cdot t_{\\mathrm{IO},i} = (pN) \\cdot t_{\\mathrm{IO},i} $$\nThe time required for $W_{\\mathrm{CPU}}$ to process all $N_c$ quanta is:\n$$ T_{\\mathrm{CPU}} = N_c \\cdot t_{\\mathrm{CPU},c} = ((1-p)N) \\cdot t_{\\mathrm{CPU},c} $$\nSince the two workers operate in parallel, the total time $T$ to process the entire batch of $N$ quanta is determined by the slower of the two pipelines (the bottleneck):\n$$ T = \\max(T_{\\mathrm{IO}}, T_{\\mathrm{CPU}}) = \\max((pN) \\cdot t_{\\mathrm{IO},i}, ((1-p)N) \\cdot t_{\\mathrm{CPU},c}) $$\nThe total system throughput, based on its fundamental definition, is $R = N/T$:\n$$ \\text{throughput}_{\\omega\\text{-aware}} = \\frac{N}{\\max((pN) \\cdot t_{\\mathrm{IO},i}, ((1-p)N) \\cdot t_{\\mathrm{CPU},c})} = \\frac{1}{\\max(p \\cdot t_{\\mathrm{IO},i}, (1-p) \\cdot t_{\\mathrm{CPU},c})} $$\nWe now compute the values inside the $\\max$ function:\n$$ p \\cdot t_{\\mathrm{IO},i} = \\frac{4}{7} \\cdot 3 = \\frac{12}{7}\\,\\mathrm{ms} $$\n$$ (1-p) \\cdot t_{\\mathrm{CPU},c} = \\frac{3}{7} \\cdot 4 = \\frac{12}{7}\\,\\mathrm{ms} $$\nThe system is perfectly balanced under this policy, meaning neither worker is a bottleneck relative to the other. Therefore:\n$$ \\text{throughput}_{\\omega\\text{-aware}} = \\frac{1}{12/7} = \\frac{7}{12}\\,\\text{quanta/ms} $$\n\nFinally, we compute the throughput difference, $\\Delta$:\n$$ \\Delta = \\text{throughput}_{\\omega\\text{-aware}} - \\text{throughput}_{\\omega\\text{-agnostic}} = \\frac{7}{12} - \\frac{161}{396} $$\nTo subtract the fractions, we find a common denominator, which is $396 = 12 \\times 33$.\n$$ \\Delta = \\frac{7 \\cdot 33}{12 \\cdot 33} - \\frac{161}{396} = \\frac{231}{396} - \\frac{161}{396} = \\frac{231 - 161}{396} = \\frac{70}{396} $$\nSimplifying the fraction gives:\n$$ \\Delta = \\frac{35}{198}\\,\\text{quanta/ms} $$\nThe problem asks for the answer in quanta per second. Since there are $1000$ milliseconds in a second, we multiply our result by $1000$:\n$$ \\Delta_{\\text{quanta/s}} = \\frac{35}{198} \\times 1000 = \\frac{35000}{198} = \\frac{17500}{99} $$\nThe fraction $\\frac{17500}{99}$ is in simplest form.", "answer": "$$\\boxed{\\frac{17500}{99}}$$", "id": "3621298"}, {"introduction": "In dynamic AMP systems, migrating tasks between different types of cores is essential for load balancing and power management. However, this migration is not without its costs, especially due to differences in hardware resources like cache sizes. This exercise provides a practical method for quantifying the performance penalty of \"cache thrashing\" that occurs when a task with a large memory footprint is moved to a core with a smaller cache, offering a vital lesson in the hidden costs of task migration. [@problem_id:3621294]", "problem": "Consider an Asymmetric Multiprocessing (AMP) system consisting of a \"big\" core and a \"little\" core. The big core has a private level-two cache (L2) of capacity $2\\,\\text{MiB}$, while the little core has a private L2 of capacity $512\\,\\text{KiB}$. Both cores share a last-level cache (LLC). You are designing a laboratory measurement to quantify the cache-related performance impact of migrating a memory-intensive task from the big core to the little core, focusing on the incremental cycles lost due to cache thrash after migration.\n\nThe task repeatedly scans an array whose working set is $W = 1.5\\,\\text{MiB}$, with contiguous 64-byte lines. After a cross-core context switch to the little core, the task executes two passes over the array during its time slice. Define the lost cycles $L$ as the additional stall cycles incurred on the second pass because the working set does not fit in the little core’s L2, relative to a hypothetical baseline in which the little core’s L2 could hold the working set and the second pass would be served by L2 hits. For scientific realism, assume:\n\n- The cache line size is $B = 64\\,\\text{B}$.\n- The little core’s clock frequency is $f_{L} = 1.5 \\times 10^{9}\\,\\text{cycles/s}$.\n- The little core’s L2 hit latency (hypothetical baseline when the working set fits) is $c_{\\mathrm{L2}} = 14$ cycles.\n- Measured with hardware performance counters during the second pass after migration, the probability that an L2 miss hits in the shared Last-Level Cache (LLC) is $p = 0.75$, and the probability of an LLC miss going to main memory is $1 - p$.\n- The measured LLC hit time is $t_{\\mathrm{LLC}} = 35\\,\\text{ns}$, and the measured main memory access time is $t_{\\mathrm{MEM}} = 120\\,\\text{ns}$.\n\nUsing fundamental definitions from memory hierarchy performance and cycle-time conversion (cycles equal frequency times time), compute $L$ as a single real-valued number representing the additional cycles lost on the second pass due to cache thrash on the little core, relative to the L2-hit baseline on that same core. Round your final answer to four significant figures. Express the final answer in cycles.", "solution": "The objective is to compute the lost cycles, $L$, incurred during the second pass of a task over its working set after migrating to a core with an insufficient L2 cache size. The lost cycles are the difference between the memory access cost in the actual thrashing scenario and a hypothetical baseline where all accesses are L2 hits.\n\nFirst, we determine the total number of memory accesses in a single pass. The task's working set size is $W = 1.5\\,\\text{MiB}$ and the cache line size is $B = 64\\,\\text{B}$. The number of unique cache lines in the working set, $N_{\\text{lines}}$, is:\n$$N_{\\text{lines}} = \\frac{W}{B} = \\frac{1.5 \\times 2^{20}\\,\\text{B}}{64\\,\\text{B}} = \\frac{1.5 \\times 2^{20}}{2^6} = 1.5 \\times 2^{14} = 1.5 \\times 16384 = 24576$$\nA full pass over the array involves $N_{\\text{lines}}$ cache line accesses.\n\nNext, we analyze the hypothetical baseline scenario for the second pass. In this baseline, the little core's L2 cache is assumed to be large enough to hold the entire $1.5\\,\\text{MiB}$ working set. After the first pass, the entire working set would be resident in the L2 cache. Therefore, during the second pass, every one of the $N_{\\text{lines}}$ accesses would be an L2 hit. The latency for an L2 hit is given as $c_{\\mathrm{L2}} = 14$ cycles.\nThe total cycles for the second pass in the baseline scenario, $C_{\\text{baseline}}$, is:\n$$C_{\\text{baseline}} = N_{\\text{lines}} \\times c_{\\mathrm{L2}}$$\n\nNow, we analyze the actual scenario. The little core’s L2 cache capacity is $C_L = 512\\,\\text{KiB} = 0.5\\,\\text{MiB}$. Since the working set size $W = 1.5\\,\\text{MiB}$ is greater than $C_L$, the working set cannot fit. As the task scans the array sequentially, it continuously evicts older cache lines to make room for new ones. By the time the first pass completes, the L2 cache contains only the last $0.5\\,\\text{MiB}$ of the working set. When the second pass begins, it starts from the beginning of the array, for which the cache lines have been evicted. Consequently, every access during the second pass results in an L2 cache miss, a condition known as cache thrashing.\n\nThe cost of each L2 miss must be calculated. This cost is a weighted average of an LLC hit and a main memory access. We first convert the given time-based latencies into cycles using the little core's frequency, $f_{L} = 1.5 \\times 10^{9}\\,\\text{cycles/s}$.\n\nThe LLC hit latency in cycles, $c_{\\mathrm{LLC}}$, is:\n$$c_{\\mathrm{LLC}} = t_{\\mathrm{LLC}} \\times f_{L} = (35 \\times 10^{-9}\\,\\text{s}) \\times (1.5 \\times 10^{9}\\,\\text{cycles/s}) = 52.5\\,\\text{cycles}$$\nThe main memory access latency in cycles, $c_{\\mathrm{MEM}}$, is:\n$$c_{\\mathrm{MEM}} = t_{\\mathrm{MEM}} \\times f_{L} = (120 \\times 10^{-9}\\,\\text{s}) \\times (1.5 \\times 10^{9}\\,\\text{cycles/s}) = 180\\,\\text{cycles}$$\n\nThe probability of an L2 miss hitting in the LLC is $p = 0.75$. The average cost of an L2 miss, $c_{\\text{miss,L2}}$, is therefore:\n$$c_{\\text{miss,L2}} = p \\times c_{\\mathrm{LLC}} + (1 - p) \\times c_{\\mathrm{MEM}}$$\n$$c_{\\text{miss,L2}} = (0.75 \\times 52.5) + (0.25 \\times 180) = 39.375 + 45 = 84.375\\,\\text{cycles}$$\n\nIn the actual scenario, every access on the second pass is an L2 miss. The total cycles for the second pass, $C_{\\text{actual}}$, is:\n$$C_{\\text{actual}} = N_{\\text{lines}} \\times c_{\\text{miss,L2}}$$\n\nThe lost cycles, $L$, are defined as the additional cycles incurred, which is the difference between the actual and baseline scenarios:\n$$L = C_{\\text{actual}} - C_{\\text{baseline}} = (N_{\\text{lines}} \\times c_{\\text{miss,L2}}) - (N_{\\text{lines}} \\times c_{\\mathrm{L2}}) = N_{\\text{lines}} \\times (c_{\\text{miss,L2}} - c_{\\mathrm{L2}})$$\nThis formula represents the total number of accesses multiplied by the per-access cycle penalty of missing in L2 versus hitting in L2.\n\nSubstituting the calculated values:\n$$L = 24576 \\times (84.375 - 14) = 24576 \\times 70.375$$\n$$L = 1729536\\,\\text{cycles}$$\n\nThe problem requires the answer to be rounded to four significant figures.\n$$L = 1729536 \\approx 1.730 \\times 10^6$$\nThe trailing zero in $1.730$ is significant.", "answer": "$$\\boxed{1.730 \\times 10^{6}}$$", "id": "3621294"}]}