{"hands_on_practices": [{"introduction": "A key design choice in multicore processors involves balancing the number of cores against their individual speed, a decision governed by physical constraints like power and heat. This practice explores a scenario where you must decide between using many cores at a lower frequency or fewer cores at a higher frequency, given that both options provide the same total computational throughput. By analyzing the trade-offs in terms of energy consumption and task latency, you will gain insight into the crucial Energy-Delay Product ($EDP$) metric used to evaluate overall system efficiency. [@problem_id:3659923]", "problem": "A server has $k$ identical general-purpose cores and supports Dynamic Voltage and Frequency Scaling (DVFS). You must choose between two operating points to execute a batch of independent, ready-at-time-zero tasks using a work-conserving scheduler with negligible overhead and perfect load balancing:\n- Configuration A: run $k$ cores at frequency $f$.\n- Configuration B: run $k/2$ cores at frequency $2f$.\n\nAssume the following fundamental definitions and well-tested models:\n- Each core executes $f$ cycles per second when clocked at frequency $f$; a task requiring $L$ core-cycles takes time $L/f$ on one core (ignoring memory stalls).\n- The dynamic power per active core is given by $P_{\\text{dyn}} = C_{\\text{sw}} V^2 f$, where $C_{\\text{sw}}$ is an effective switching capacitance-and-activity constant, $V$ is the supply voltage, and $f$ is the clock frequency.\n- In the operating range of interest, DVFS supplies voltage proportional to frequency: $V = \\beta f$.\n- The static (leakage) power per active core over this range is approximately constant and equal to $P_s$.\n- Inactive cores are fully power-gated and draw zero power. There is no other platform power to consider.\n\nYou are given the following parameters:\n- $k = 16$ cores.\n- $f = 2$ GHz (use $f$ in GHz in all formulas; treat $C_{\\text{sw}}$ and $\\beta$ as given below so that power is in watts when $f$ is in GHz and $V$ in volts).\n- $M = 160$ tasks, each requiring $L = 10^9$ core-cycles.\n- $C_{\\text{sw}} = 0.2$ W/(V$^2$·GHz), $\\beta = 1$ V/GHz, and $P_s = 1$ W per active core.\n\nStarting from the definitions above and standard scheduling reasoning for batch processing of identical, independent tasks:\n1. Establish that both configurations offer identical throughput for this workload and derive the total makespan in each configuration.\n2. Derive the mean task completion time (average latency across the $M$ tasks) under each configuration by reasoning about batch completions under a work-conserving scheduler that always keeps all available cores busy until all tasks finish.\n3. Using the power model, derive and compute the total energy to complete all $M$ tasks under each configuration.\n4. Define the Energy-Delay Product (EDP) of a configuration as the product of its total energy and its mean task completion time. Compute the ratio\n$$R = \\frac{\\text{EDP}_{\\text{B}}}{\\text{EDP}_{\\text{A}}}.$$\n\nExpress the final ratio $R$ as a unitless real number, rounded to four significant figures.", "solution": "The problem is evaluated to be scientifically grounded, well-posed, objective, and complete. All necessary parameters and models are provided to derive a unique, meaningful solution. We proceed with the solution, following the four specified steps.\n\nThe core of this problem is to analyze the trade-offs between two processor configurations, A and B, in terms of performance (makespan, latency) and energy consumption. Let's denote quantities related to configuration A with a subscript A, and those for configuration B with a subscript B.\n\nThe given parameters are:\n- Total number of available cores: $k = 16$.\n- Base frequency: $f = 2 \\text{ GHz}$.\n- Total number of tasks: $M = 160$.\n- Work per task: $L = 10^9$ core-cycles.\n- Dynamic power constant: $C_{\\text{sw}} = 0.2 \\text{ W/(V}^2 \\cdot \\text{GHz)}$.\n- Voltage-frequency scaling factor: $\\beta = 1 \\text{ V/GHz}$.\n- Static power per active core: $P_s = 1 \\text{ W}$.\n\nConfiguration A uses $k_A = k = 16$ cores at frequency $f_A = f = 2 \\text{ GHz}$.\nConfiguration B uses $k_B = k/2 = 8$ cores at frequency $f_B = 2f = 4 \\text{ GHz}$.\n\nFirst, we establish the power model for a single active core. The total power is the sum of dynamic and static power:\n$P_{\\text{core}} = P_{\\text{dyn}} + P_s$.\nGiven $P_{\\text{dyn}} = C_{\\text{sw}} V^2 f_{\\text{core}}$ and $V = \\beta f_{\\text{core}}$, we can substitute $V$ into the power equation:\n$P_{\\text{dyn}} = C_{\\text{sw}} (\\beta f_{\\text{core}})^2 f_{\\text{core}} = C_{\\text{sw}} \\beta^2 f_{\\text{core}}^3$.\nThus, the power for a single core operating at frequency $f_{\\text{core}}$ is:\n$$P_{\\text{core}}(f_{\\text{core}}) = C_{\\text{sw}} \\beta^2 f_{\\text{core}}^3 + P_s$$\n\n**1. Throughput and Makespan**\n\nThroughput, in this context, is the total rate of work completion. The total work for the entire batch of tasks is $W_{\\text{total}} = M \\times L$ core-cycles. The aggregate processing rate of a configuration with $k_{\\text{cores}}$ running at frequency $f_{\\text{core}}$ is $R_{\\text{agg}} = k_{\\text{cores}} \\times f_{\\text{core}}$ cycles/second.\n\nFor Configuration A:\nThe aggregate processing rate is $R_{\\text{agg,A}} = k_A \\times f_A = k \\times f$.\n\nFor Configuration B:\nThe aggregate processing rate is $R_{\\text{agg,B}} = k_B \\times f_B = (k/2) \\times (2f) = k \\times f$.\n\nSince $R_{\\text{agg,A}} = R_{\\text{agg,B}}$, both configurations offer identical aggregate processing rates and thus identical throughput for this workload.\n\nThe makespan, $T_{\\text{makespan}}$, is the total time required to complete all $M$ tasks. It is calculated as the total work divided by the aggregate processing rate.\n$$T_{\\text{makespan}} = \\frac{W_{\\text{total}}}{R_{\\text{agg}}} = \\frac{M \\times L}{k \\times f}$$\nSince the aggregate rate is the same for both, their makespans are identical.\n$$T_{\\text{makespan,A}} = T_{\\text{makespan,B}} = \\frac{160 \\times 10^9 \\text{ cycles}}{16 \\times (2 \\times 10^9 \\text{ cycles/s})} = \\frac{160}{32} \\text{ s} = 5 \\text{ s}$$\n\n**2. Mean Task Completion Time**\n\nThe mean task completion time (or average latency) depends on how tasks are completed over the makespan. With a work-conserving scheduler and perfect load balancing, tasks are completed in batches. The size of each batch is equal to the number of active cores, and all tasks in a batch complete simultaneously.\n\nFor Configuration A:\n- Number of active cores: $k_A = 16$.\n- Time to complete one task on one core: $T_{\\text{task},A} = L/f_A = L/f = (10^9)/(2 \\times 10^9) = 0.5 \\text{ s}$.\n- Number of batches: $N_{\\text{batches},A} = M/k_A = 160/16 = 10$.\nThe first batch of $16$ tasks finishes at time $1 \\times T_{\\text{task},A}$. The second batch finishes at $2 \\times T_{\\text{task},A}$, and so on, until the last batch ($j=10$) finishes at $10 \\times T_{\\text{task},A} = 5 \\text{ s}$ (the makespan).\nThe sum of completion times for all $M$ tasks is $\\sum C_i = k_A \\sum_{j=1}^{N_{\\text{batches},A}} (j \\cdot T_{\\text{task},A})$.\nThe mean completion time, $\\bar{T}_A$, is this sum divided by $M = k_A \\cdot N_{\\text{batches},A}$:\n$$\\bar{T}_A = \\frac{k_A \\cdot T_{\\text{task},A} \\sum_{j=1}^{N_{\\text{batches},A}} j}{k_A \\cdot N_{\\text{batches},A}} = T_{\\text{task},A} \\frac{N_{\\text{batches},A}(N_{\\text{batches},A}+1)}{2 N_{\\text{batches},A}} = T_{\\text{task},A} \\frac{N_{\\text{batches},A}+1}{2}$$\nSubstituting the values:\n$$\\bar{T}_A = (0.5 \\text{ s}) \\times \\frac{10+1}{2} = 0.5 \\times 5.5 = 2.75 \\text{ s}$$\n\nFor Configuration B:\n- Number of active cores: $k_B = 8$.\n- Time to complete one task on one core: $T_{\\text{task},B} = L/f_B = L/(2f) = (10^9)/(4 \\times 10^9) = 0.25 \\text{ s}$.\n- Number of batches: $N_{\\text{batches},B} = M/k_B = 160/8 = 20$.\nUsing the same formula for the mean completion time:\n$$\\bar{T}_B = T_{\\text{task},B} \\frac{N_{\\text{batches},B}+1}{2}$$\nSubstituting the values:\n$$\\bar{T}_B = (0.25 \\text{ s}) \\times \\frac{20+1}{2} = 0.25 \\times 10.5 = 2.625 \\text{ s}$$\n\n**3. Total Energy**\n\nThe total energy to complete the workload is the total power consumed multiplied by the makespan. Inactive cores are power-gated and consume zero power.\n\nFor Configuration A:\n- Power per core: $P_{\\text{core},A} = C_{\\text{sw}} \\beta^2 f_A^3 + P_s = C_{\\text{sw}} \\beta^2 f^3 + P_s$.\n- $P_{\\text{core},A} = (0.2)(1^2)(2^3) + 1 = 0.2 \\times 8 + 1 = 1.6 + 1 = 2.6 \\text{ W}$.\n- Total power: $P_{\\text{total},A} = k_A \\times P_{\\text{core},A} = 16 \\times 2.6 = 41.6 \\text{ W}$.\n- Total energy: $E_A = P_{\\text{total},A} \\times T_{\\text{makespan},A} = 41.6 \\text{ W} \\times 5 \\text{ s} = 208 \\text{ J}$.\n\nFor Configuration B:\n- Power per core: $P_{\\text{core},B} = C_{\\text{sw}} \\beta^2 f_B^3 + P_s = C_{\\text{sw}} \\beta^2 (2f)^3 + P_s = 8 C_{\\text{sw}} \\beta^2 f^3 + P_s$.\n- $P_{\\text{core},B} = 8 \\times (0.2)(1^2)(2^3) + 1 = 8 \\times 1.6 + 1 = 12.8 + 1 = 13.8 \\text{ W}$.\n- Total power: $P_{\\text{total},B} = k_B \\times P_{\\text{core},B} = 8 \\times 13.8 = 110.4 \\text{ W}$.\n- Total energy: $E_B = P_{\\text{total},B} \\times T_{\\text{makespan},B} = 110.4 \\text{ W} \\times 5 \\text{ s} = 552 \\text{ J}$.\n\n**4. Energy-Delay Product (EDP) Ratio**\n\nThe problem defines the Energy-Delay Product (EDP) as the product of total energy and mean task completion time. We need to compute the ratio $R = \\text{EDP}_B / \\text{EDP}_A$.\n\nFirst, we compute the EDP for each configuration:\n$$EDP_A = E_A \\times \\bar{T}_A = 208 \\text{ J} \\times 2.75 \\text{ s} = 572 \\text{ J} \\cdot \\text{s}$$\n$$EDP_B = E_B \\times \\bar{T}_B = 552 \\text{ J} \\times 2.625 \\text{ s} = 1449 \\text{ J} \\cdot \\text{s}$$\n\nNow, we compute the ratio $R$:\n$$R = \\frac{\\text{EDP}_B}{\\text{EDP}_A} = \\frac{1449}{572}$$\n$$R \\approx 2.53321678...$$\nRounding to four significant figures, the ratio is $2.533$.", "answer": "$$\\boxed{2.533}$$", "id": "3659923"}, {"introduction": "In a multicore system, threads must often wait for exclusive access to shared resources, but not all waiting is equal. This exercise tackles the classic dilemma of spin-waiting versus blocking: should a thread burn CPU cycles actively polling for a lock, or should it yield the processor and incur the overhead of a context switch? You will use the memoryless property of the exponential distribution, a powerful tool in performance modeling, to determine the optimal waiting strategy based on the expected wait time. [@problem_id:3659912]", "problem": "A multicore server with $16$ hardware cores runs a user-space mutual exclusion lock that protects a short critical section. Threads contend for the lock under First-In First-Out (FIFO) ordering; when a thread arrives and finds the lock held, it must choose whether to spin-wait on a core or to voluntarily block, incurring a context switch. Empirically, the total cost of blocking and later waking up (including scheduler dispatch and cache working-set effects) is measured as $S$ seconds. The critical section hold times are independent and identically distributed and follow an exponential distribution with rate parameter $\\lambda$ (so the mean hold time is $1/\\lambda$ seconds). Assume the system has enough runnable cores that a spinning thread is not preempted while waiting.\n\nConsider a particular arriving thread that observes $q$ contenders ahead of it in the lock order, where the current holder counts toward $q$. The scheduling policy is: spin if the expected remaining wait time $E[W(q)]$ is less than or equal to $S$, otherwise block. Starting from the definitions of the exponential distribution and independence of successive lock holds, derive $E[W(q)]$ from first principles and then determine the threshold queue position $q^{*}$ that satisfies $E[W(q^{*})] = S$. Using $\\lambda = 3.3 \\times 10^{5} \\ \\text{s}^{-1}$ and $S = 10 \\ \\mu\\text{s}$, compute $q^{*}$. Round your final numeric answer for $q^{*}$ to four significant figures. The final answer must be a single real number without units.", "solution": "The problem is first validated to ensure it is scientifically sound, self-contained, and well-posed.\n\n**Step 1: Extract Givens**\n- Number of hardware cores: $16$.\n- Synchronization: User-space mutual exclusion lock with First-In First-Out (FIFO) ordering.\n- Cost of blocking and waking up (context switch): $S$ seconds.\n- Critical section hold times are independent and identically distributed (i.i.d.).\n- Distribution of hold times: Exponential with rate parameter $\\lambda$.\n- Mean hold time: $1/\\lambda$ seconds.\n- Assumption: A spinning thread is not preempted.\n- An arriving thread observes $q$ contenders ahead, including the current lock holder.\n- Scheduling policy: Spin if the expected remaining wait time $E[W(q)]$ is less than or equal to $S$; otherwise, block.\n- Objective 1: Derive $E[W(q)]$ from first principles.\n- Objective 2: Find the threshold queue position $q^{*}$ where $E[W(q^{*})] = S$.\n- Objective 3: Compute $q^{*}$ for $\\lambda = 3.3 \\times 10^{5} \\ \\text{s}^{-1}$ and $S = 10 \\ \\mu\\text{s}$.\n- Rounding: The final numerical answer for $q^*$ must have four significant figures.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is a standard exercise in queueing theory applied to computer systems performance analysis, a core topic in operating systems.\n- **Scientific Grounding**: The model uses the exponential distribution for service times, a common and justified practice in performance modeling. The trade-off between spin-waiting and blocking is a fundamental concept in multicore synchronization. The problem is scientifically and mathematically sound.\n- **Well-Posedness**: All necessary parameters ($\\lambda$, $S$) and conditions are provided. The objectives are clearly stated and lead to a unique, derivable solution.\n- **Objectivity**: The problem is stated in precise, quantitative terms, free from ambiguity or subjective claims.\n\nThe number of cores ($16$) is provided, but the assumption that \"a spinning thread is not preempted\" makes this information extraneous to the calculation for an individual thread's decision. This assumption simplifies the model by ensuring that a spinning thread's wait time is determined solely by lock contention, not by interference from the OS scheduler. The problem is therefore well-defined and internally consistent.\n\n**Step 3: Verdict and Action**\nThe problem is deemed **valid**. A full solution will be provided.\n\n**Derivation of the Expected Wait Time $E[W(q)]$**\n\nLet $T$ be the random variable representing the critical section hold time. The problem states that $T$ follows an exponential distribution with rate parameter $\\lambda$. The probability density function (PDF) of $T$ is given by:\n$$f_T(t) = \\lambda \\exp(-\\lambda t) \\quad \\text{for } t \\ge 0$$\nThe expected value of $T$ is $E[T] = 1/\\lambda$.\n\nAn arriving thread observes $q$ contenders ahead of it. According to the FIFO ordering and the problem statement, these $q$ contenders consist of:\n1. One thread currently holding the lock.\n2. $q-1$ threads waiting in the queue ahead of the arriving thread.\n\nThe total wait time for the arriving thread, $W(q)$, is the sum of the remaining time for the current lock holder's critical section and the full critical section times for all $q-1$ threads ahead in the queue.\nLet $T_{\\text{rem}}$ be the remaining time for the current lock holder.\nLet $T_1, T_2, \\dots, T_{q-1}$ be the full critical section hold times for the $q-1$ threads in the queue.\nSince the hold times are i.i.d., each $T_i \\sim \\text{Exp}(\\lambda)$.\nThe total wait time is:\n$$W(q) = T_{\\text{rem}} + \\sum_{i=1}^{q-1} T_i$$\n\nBy the linearity of expectation, the expected total wait time is:\n$$E[W(q)] = E[T_{\\text{rem}}] + E\\left[\\sum_{i=1}^{q-1} T_i\\right] = E[T_{\\text{rem}}] + \\sum_{i=1}^{q-1} E[T_i]$$\n\nThe crucial step is to determine $E[T_{\\text{rem}}]$. This requires using the **memoryless property** of the exponential distribution. This property states that the remaining lifetime of an exponentially distributed variable is independent of its past. Formally, for any $s, t  0$:\n$$P(T  t+s | T  s) = P(T  t)$$\nTo prove this from first principles, we use the definition of conditional probability and the survival function $P(Tx) = \\int_x^\\infty \\lambda \\exp(-\\lambda \\tau) d\\tau = \\exp(-\\lambda x)$:\n$$P(T  t+s | T  s) = \\frac{P(T  t+s \\text{ and } T  s)}{P(T  s)} = \\frac{P(T  t+s)}{P(T  s)} = \\frac{\\exp(-\\lambda(t+s))}{\\exp(-\\lambda s)} = \\exp(-\\lambda t) = P(T  t)$$\nThis confirms that the distribution of the remaining time $T_{\\text{rem}}$ is the same as the original distribution of $T$, regardless of how long the lock has already been held. Therefore, $T_{\\text{rem}}$ also follows an exponential distribution with rate $\\lambda$.\nConsequently, its expected value is:\n$$E[T_{\\text{rem}}] = E[T] = \\frac{1}{\\lambda}$$\n\nThe expected hold times for the $q-1$ threads waiting in the queue are simply $E[T_i] = 1/\\lambda$ for each $i \\in \\{1, \\dots, q-1\\}$.\nThe sum of their expectations is:\n$$\\sum_{i=1}^{q-1} E[T_i] = \\sum_{i=1}^{q-1} \\frac{1}{\\lambda} = (q-1) \\frac{1}{\\lambda}$$\n\nCombining these results, we find the expected total wait time:\n$$E[W(q)] = E[T_{\\text{rem}}] + \\sum_{i=1}^{q-1} E[T_i] = \\frac{1}{\\lambda} + \\frac{q-1}{\\lambda} = \\frac{1 + q - 1}{\\lambda} = \\frac{q}{\\lambda}$$\n\n**Determination of the Threshold Queue Position $q^{*}$**\n\nThe scheduling policy dictates that a thread should spin if its expected wait time is no more than the cost of blocking, i.e., spin if $E[W(q)] \\le S$. The threshold position $q^{*}$ is the point of indifference, where the expected cost of spinning equals the fixed cost of blocking.\nWe set $E[W(q^{*})] = S$.\nSubstituting our derived expression for the expected wait time:\n$$\\frac{q^{*}}{\\lambda} = S$$\nSolving for $q^{*}$, we get:\n$$q^{*} = S \\lambda$$\n\n**Computation of an Explicit Numerical Value for $q^{*}$**\n\nThe problem provides the following values:\n- $S = 10 \\ \\mu\\text{s} = 10 \\times 10^{-6} \\ \\text{s}$\n- $\\lambda = 3.3 \\times 10^{5} \\ \\text{s}^{-1}$\n\nWe can now compute the numerical value of $q^{*}$:\n$$q^{*} = (10 \\times 10^{-6} \\ \\text{s}) \\times (3.3 \\times 10^{5} \\ \\text{s}^{-1})$$\n$$q^{*} = 10 \\times 3.3 \\times 10^{-6} \\times 10^{5}$$\n$$q^{*} = 33 \\times 10^{-1}$$\n$$q^{*} = 3.3$$\n\nThe problem requires the final answer to be rounded to four significant figures.\n$$q^{*} = 3.300$$\nThis means a thread should choose to spin if it observes $3$ or fewer contenders ahead (since $q$ must be an integer, the policy is to spin if $q \\le \\lfloor q^* \\rfloor = 3$). The value $q^* = 3.3$ represents the exact break-even point in the model.", "answer": "$$\\boxed{3.300}$$", "id": "3659912"}, {"introduction": "Understanding system performance begins with correctly interpreting its vital signs, but metrics like \"load average\" can be deceiving. This practice demystifies the relationship between the system load average ($L$) and CPU utilization ($U$), particularly on a multicore machine. You will analyze a hypothetical workload to see how tasks blocked on certain types of I/O can inflate the load average, demonstrating the important principle that a high load does not always mean the CPUs are busy. [@problem_id:3659898]", "problem": "Consider a multicore system with $m$ identical cores and a preemptive time-sharing scheduler. Define the following quantities precisely:\n- System load average $L$: the exponentially weighted time-average of the number of tasks that are either running on a core or in uninterruptible sleep (the so-called uninterruptible sleep “$D$-state”) during a measurement window. Assume that over sufficiently long and stationary intervals, this equals the long-run time-average of those counted tasks.\n- Central Processing Unit (CPU) utilization $U$: the fraction of aggregate core-time spent executing user-space or kernel threads (i.e., not the idle loop) over the same window.\n\nAssume negligible context-switch overhead and identical tasks. Now consider $N$ independent tasks that each alternate deterministically between a compute phase of duration $\\tau_c$ and an input/output (Input/Output; I/O) phase of duration $\\tau_{io}$. Let the compute duty fraction be $\\alpha = \\tau_c / (\\tau_c + \\tau_{io})$. Two cases are considered:\n- Case A: The I/O phase places tasks in uninterruptible sleep ($D$-state).\n- Case B: The I/O phase places tasks in interruptible sleep (not $D$-state), so sleeping tasks are not counted in $L$.\n\nSuppose $m=4$, $N=20$, $\\tau_c=10\\,\\mathrm{ms}$, and $\\tau_{io}=90\\,\\mathrm{ms}$, and the workload is steady-state.\n\nWhich of the following statements are correct about interpreting $L$ on an $m$-core system and the relationship of $L/m$ to $U$, including explanations for anomalies when many tasks are blocked?\n\nA. On an $m$-core system with predominantly CPU-bound (always runnable) tasks, CPU utilization $U$ is approximately $\\min\\!\\left(1,\\; L/m\\right)$.\n\nB. In Case A with the given parameters, the ratio $L/m$ may significantly exceed $1$ while $U$ remains well below $1$.\n\nC. In Case B with the given parameters, $L/m$ closely approximates $U$ provided the workload is stationary and context-switch overhead is negligible.\n\nD. The load average $L$ counts only runnable tasks; tasks blocked on input/output in any sleep state do not contribute to $L$.\n\nE. If $L/m  1$, it necessarily implies that all cores are fully utilized (i.e., $U=1$), regardless of workload composition.", "solution": "The user wants me to follow the instructions for `solver_agent`.\n\n### Problem Validation\n\n#### Step 1: Extract Givens\n\n-   System: $m$ identical cores, preemptive time-sharing scheduler.\n-   System load average $L$: exponentially weighted time-average of the number of tasks that are either running on a core or in uninterruptible sleep ($D$-state). Assumed to equal the long-run time-average for stationary intervals.\n-   CPU utilization $U$: the fraction of aggregate core-time spent executing user-space or kernel threads (i.e., not the idle loop).\n-   Assumptions: negligible context-switch overhead, identical tasks.\n-   Workload: $N$ independent tasks.\n-   Task behavior: deterministic alternation between a compute phase of duration $\\tau_c$ and an I/O phase of duration $\\tau_{io}$.\n-   Compute duty fraction: $\\alpha = \\tau_c / (\\tau_c + \\tau_{io})$.\n-   Case A: The I/O phase places tasks in uninterruptible sleep ($D$-state).\n-   Case B: The I/O phase places tasks in interruptible sleep (not $D$-state).\n-   Parameters: $m=4$, $N=20$, $\\tau_c=10\\,\\mathrm{ms}$, $\\tau_{io}=90\\,\\mathrm{ms}$.\n-   Condition: The workload is in steady-state.\n-   Question: Evaluate the correctness of statements about the interpretation of $L$ and the relationship of $L/m$ to $U$.\n\n#### Step 2: Validate Using Extracted Givens\n\n-   **Scientific Grounding**: The problem is well-grounded in the principles of operating systems and performance analysis. The definitions of CPU utilization ($U$) and load average ($L$) are consistent with those used in real-world systems, specifically the Linux kernel's definition of load average which includes tasks in the `TASK_UNINTERRUPTIBLE` state. The model of tasks alternating between compute and I/O is a standard queueing theory model used for performance evaluation.\n-   **Well-Posedness**: The problem is well-posed. It provides all necessary definitions, parameters, and simplifying assumptions (steady-state, negligible overhead) to permit a unique, quantitative analysis of the two cases described. The question asks for an evaluation of specific statements based on this analysis, which is a clear and solvable task.\n-   **Objectivity**: The language is precise, quantitative, and free of subjective or ambiguous terminology. The definitions for $L$ and $U$ are given explicitly, removing any potential for misinterpretation.\n\nThe problem does not exhibit any of the invalidating flaws. It is not scientifically unsound, non-formalizable, incomplete, unrealistic, ill-posed, or trivial. The distinction between interruptible and uninterruptible sleep and its effect on the load average is a conceptually important topic in operating systems.\n\n#### Step 3: Verdict and Action\n\nThe problem statement is valid. Proceeding with the solution.\n\n### Principle-Based Derivation\n\nFirst, we establish the theoretical framework based on the provided definitions and model.\nA single task has a cycle time of $T = \\tau_c + \\tau_{io}$.\nThe fraction of time a task requires a CPU (its compute duty fraction) is $\\alpha = \\frac{\\tau_c}{\\tau_c + \\tau_{io}}$.\nWith $N$ independent, identical tasks, the total demand for CPU resources, measured in core-equivalents, is the product of the number of tasks and their individual CPU demand:\n$$\n\\text{Total CPU Demand} = N \\alpha\n$$\nThe system has $m$ cores, providing a total of $m$ core-equivalents of processing capacity.\nThe CPU utilization, $U$, is the ratio of the total demand to the total capacity, but it cannot exceed $1$ (i.e., $100\\%$). Therefore,\n$$\nU = \\min\\left(1, \\frac{N \\alpha}{m}\\right)\n$$\nThe system load average, $L$, is the time-average number of tasks in the run queue (ready to run), currently running, or in uninterruptible sleep ($D$-state).\n\nNow, we apply the specific parameters:\n-   $m = 4$\n-   $N = 20$\n-   $\\tau_c = 10\\,\\mathrm{ms}$\n-   $\\tau_{io} = 90\\,\\mathrm{ms}$\n\nFrom these, we calculate the cycle time and duty fraction:\n$$\nT = \\tau_c + \\tau_{io} = 10\\,\\mathrm{ms} + 90\\,\\mathrm{ms} = 100\\,\\mathrm{ms}\n$$\n$$\n\\alpha = \\frac{\\tau_c}{T} = \\frac{10\\,\\mathrm{ms}}{100\\,\\mathrm{ms}} = 0.1\n$$\nThe total CPU demand from the $N=20$ tasks is:\n$$\nN \\alpha = 20 \\times 0.1 = 2.0 \\text{ core-equivalents}\n$$\nSince the total demand ($2.0$) is less than the system's capacity ($m=4$), the system is not CPU-saturated. The CPU utilization is:\n$$\nU = \\frac{N \\alpha}{m} = \\frac{2.0}{4} = 0.5\n$$\nIn steady state, the average number of tasks in the compute phase (runnable) is $N \\alpha = 2$.\nThe average number of tasks in the I/O phase is $N(1-\\alpha) = 20 \\times (1-0.1) = 18$.\n\nWe now analyze the two cases for the load average $L$.\n\n**Case A: I/O phase is uninterruptible sleep ($D$-state)**\nAccording to the definition, $L$ includes tasks in the compute phase (running or ready) and tasks in uninterruptible sleep.\n$$\nL_A = (\\text{average # of tasks in compute phase}) + (\\text{average # of tasks in } D\\text{-state I/O})\n$$\n$$\nL_A = (N \\alpha) + (N (1-\\alpha)) = N = 20\n$$\nThe normalized load average is $L_A/m = 20/4 = 5$.\nIn this case, we have $U=0.5$ and $L/m=5$.\n\n**Case B: I/O phase is interruptible sleep (not $D$-state)**\nAccording to the definition, $L$ only includes tasks in the compute phase, as interruptible sleep states are not counted.\n$$\nL_B = (\\text{average # of tasks in compute phase})\n$$\n$$\nL_B = N \\alpha = 2\n$$\nThe normalized load average is $L_B/m = 2/4 = 0.5$.\nIn this case, we have $U=0.5$ and $L/m=0.5$.\n\n### Option-by-Option Analysis\n\n**A. On an m-core system with predominantly CPU-bound (always runnable) tasks, CPU utilization U is approximately $\\min\\!\\left(1,\\; L/m\\right)$.**\nFor CPU-bound tasks, they are nearly always in a runnable state (either running or ready in the run queue). Their time in any sleep state is negligible. In this scenario, the load average $L$ (which counts runnable tasks) becomes a direct measure of the number of tasks demanding CPU resources. Let this number be $N$. So, $L \\approx N$. The total CPU demand is $N$ core-equivalents. The utilization $U$ is the realized portion of this demand, given the capacity $m$, so $U = \\min(1, N/m)$. Substituting $L \\approx N$, we get $U \\approx \\min(1, L/m)$. This statement correctly describes the relationship for a CPU-bound workload.\n**Verdict: Correct.**\n\n**B. In Case A with the given parameters, the ratio L/m may significantly exceed 1 while U remains well below 1.**\nOur analysis of Case A yielded $L/m = 5$ and $U=0.5$. The ratio $5$ significantly exceeds $1$, while the utilization $0.5$ is well below $1$. This situation arises because the load average $L$ is inflated by the $18$ tasks in uninterruptible sleep, which do not consume CPU resources. The statement accurately describes this calculated outcome.\n**Verdict: Correct.**\n\n**C. In Case B with the given parameters, L/m closely approximates U provided the workload is stationary and context-switch overhead is negligible.**\nOur analysis of Case B with the given parameters yielded $L/m = 0.5$ and $U=0.5$. In this specific instance, $L/m$ is not just an approximation but is exactly equal to $U$. The conditions mentioned (stationary workload, negligible overhead) are precisely the assumptions that allow for this exact calculation. Therefore, the statement that it \"closely approximates\" is true. This holds because the system is not CPU-saturated ($N\\alpha \\le m$) and the I/O-blocked tasks are not counted in $L$.\n**Verdict: Correct.**\n\n**D. The load average L counts only runnable tasks; tasks blocked on input/output in any sleep state do not contribute to L.**\nThis statement directly contradicts the problem's explicit definition of $L$: \"...the number of tasks that are either running on a core or in uninterruptible sleep (the so-called uninterruptible sleep '$D$-state').\" Tasks blocked on I/O in the $D$-state do contribute to $L$.\n**Verdict: Incorrect.**\n\n**E. If L/m  1, it necessarily implies that all cores are fully utilized (i.e., U=1), regardless of workload composition.**\nThis is a strong, universal claim that is falsified by a counterexample. Our analysis of Case A provides such a counterexample. In Case A, $L/m = 5$, which is greater than $1$. However, the CPU utilization $U$ was only $0.5$, not $1$. The high load average was caused by I/O-bound tasks in uninterruptible sleep, not by an excess of CPU demand. Therefore, a high load average does not necessarily imply high CPU utilization.\n**Verdict: Incorrect.**\n\nFinal conclusion: Statements A, B, and C are correct.", "answer": "$$\\boxed{ABC}$$", "id": "3659898"}]}