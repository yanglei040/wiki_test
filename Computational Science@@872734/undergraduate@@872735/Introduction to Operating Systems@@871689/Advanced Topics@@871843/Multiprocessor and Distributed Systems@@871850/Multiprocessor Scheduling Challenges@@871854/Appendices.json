{"hands_on_practices": [{"introduction": "At the heart of any operating system scheduler lies the run queue, the data structure holding all threads ready for execution. This exercise delves into a fundamental engineering trade-off in scheduler design: the choice between a simple, unsorted list and a more complex binary heap. By using concrete performance parameters, you will quantitatively determine the crossover point where the logarithmic scalability of a heap outweighs the low overhead of a list, a critical analysis for building efficient and scalable systems.", "problem": "A symmetric multiprocessor operating system with $p$ identical central processing unit (CPU) cores makes a scheduling decision every time a time slice expires on a core. Let the time slice be $\\tau$ seconds, so the system performs approximately $p / \\tau$ scheduling decisions per second when there is always work to run. Each scheduling decision picks the runnable thread with the highest dynamic priority from a global run queue of average length $l$ and then reinserts the preempted thread back into the run queue (assume long-running CPU-bound workloads so that each decision performs one selection and one reinsertion). The total per-decision scheduler overhead is the sum of the selection and reinsertion times. Two data-structure designs are considered for the global run queue:\n\n- Unsorted list:\n  - Selection (scan) time scales linearly in $l$: $t_{\\text{scan}}(l) = \\alpha \\, l + \\beta$.\n  - Reinsertion time is constant: $t_{\\text{enq}} = \\delta$.\n\n- Binary heap keyed by priority:\n  - Selection time scales logarithmically in $l$: $t_{\\text{sel,heap}}(l) = \\alpha_{h} \\, \\log_{2}(l) + \\beta_{h}$.\n  - Reinsertion time also scales logarithmically: $t_{\\text{enq,heap}}(l) = \\delta_{h} \\, \\log_{2}(l)$.\n\nHere $\\log_{2}(\\,\\cdot\\,)$ denotes the base-$2$ logarithm. Constants are obtained from microbenchmarks under the target platform:\n- $\\alpha = 0.03$ microseconds per element,\n- $\\beta = 0.20$ microseconds,\n- $\\delta = 0.05$ microseconds,\n- $\\alpha_{h} = 0.28$ microseconds,\n- $\\beta_{h} = 0.50$ microseconds,\n- $\\delta_{h} = 0.12$ microseconds.\n\nUsing only these definitions and facts, derive the per-decision overhead expressions for both designs and determine the run-queue length $l^{\\star}$ (treated as a real value) at which both designs have equal per-decision overhead. Report $l^{\\star}$ rounded to four significant figures. Because $l^{\\star}$ is a dimensionless count, no unit is required in the final answer.", "solution": "The objective is to determine the run-queue length $l^{\\star}$ at which the per-decision scheduler overhead of two different data-structure designs becomes equal. A scheduling decision consists of one selection and one reinsertion. The total per-decision overhead is the sum of the times for these two operations.\n\nFirst, we formulate the total per-decision overhead, $T(l)$, as a function of the average run-queue length $l$ for each design.\n\nFor the unsorted list design, the overhead $T_{\\text{list}}(l)$ is the sum of the selection (scan) time and the reinsertion (enqueue) time.\nThe selection time is given as $t_{\\text{scan}}(l) = \\alpha l + \\beta$.\nThe reinsertion time is given as a constant, $t_{\\text{enq}} = \\delta$.\nTherefore, the total overhead for the unsorted list is:\n$$T_{\\text{list}}(l) = t_{\\text{scan}}(l) + t_{\\text{enq}} = (\\alpha l + \\beta) + \\delta = \\alpha l + (\\beta + \\delta)$$\n\nFor the binary heap design, the overhead $T_{\\text{heap}}(l)$ is the sum of the selection time and the reinsertion time.\nThe selection time is given as $t_{\\text{sel,heap}}(l) = \\alpha_{h} \\log_{2}(l) + \\beta_{h}$.\nThe reinsertion time is given as $t_{\\text{enq,heap}}(l) = \\delta_{h} \\log_{2}(l)$.\nTherefore, the total overhead for the binary heap is:\n$$T_{\\text{heap}}(l) = t_{\\text{sel,heap}}(l) + t_{\\text{enq,heap}}(l) = (\\alpha_{h} \\log_{2}(l) + \\beta_{h}) + (\\delta_{h} \\log_{2}(l))$$\n$$T_{\\text{heap}}(l) = (\\alpha_{h} + \\delta_{h}) \\log_{2}(l) + \\beta_{h}$$\n\nThe problem asks for the run-queue length $l^{\\star}$ at which these two overheads are equal. We find this by setting $T_{\\text{list}}(l^{\\star}) = T_{\\text{heap}}(l^{\\star})$:\n$$\\alpha l^{\\star} + (\\beta + \\delta) = (\\alpha_{h} + \\delta_{h}) \\log_{2}(l^{\\star}) + \\beta_{h}$$\n\nNow, we substitute the provided numerical values for the constants:\n$\\alpha = 0.03$ $\\mu$s/element\n$\\beta = 0.20$ $\\mu$s\n$\\delta = 0.05$ $\\mu$s\n$\\alpha_{h} = 0.28$ $\\mu$s\n$\\beta_{h} = 0.50$ $\\mu$s\n$\\delta_{h} = 0.12$ $\\mu$s\n\nAll time constants are in microseconds ($\\mu$s), so we can treat them as dimensionless numbers for the purpose of solving the equation for the dimensionless quantity $l^{\\star}$.\n$$0.03 l^{\\star} + (0.20 + 0.05) = (0.28 + 0.12) \\log_{2}(l^{\\star}) + 0.50$$\n$$0.03 l^{\\star} + 0.25 = 0.40 \\log_{2}(l^{\\star}) + 0.50$$\nTo find the root $l^{\\star}$, we can rearrange this equation into the form $f(l) = 0$:\n$$f(l) = 0.03 l - 0.40 \\log_{2}(l) - 0.25 = 0$$\n\nThis is a transcendental equation, which cannot be solved for $l$ using elementary algebraic operations. We must find the root using numerical methods. Let's evaluate $f(l)$ at some test values to locate the root.\nFor $l=64$:\n$f(64) = 0.03(64) - 0.40 \\log_{2}(64) - 0.25 = 1.92 - 0.40(6) - 0.25 = 1.92 - 2.40 - 0.25 = -0.73$\nFor $l=128$:\n$f(128) = 0.03(128) - 0.40 \\log_{2}(128) - 0.25 = 3.84 - 0.40(7) - 0.25 = 3.84 - 2.80 - 0.25 = 0.79$\n\nSince $f(64)  0$ and $f(128) > 0$, and the function $f(l)$ is continuous for $l>0$, a root $l^{\\star}$ must exist in the interval $(64, 128)$. We can use a numerical root-finding algorithm, such as the bisection method or Newton's method, to find a more precise value.\n\nLet's use Newton's method. The derivative of $f(l)$ is required:\n$f'(l) = \\frac{d}{dl} \\left( 0.03 l - 0.40 \\frac{\\ln(l)}{\\ln(2)} - 0.25 \\right) = 0.03 - \\frac{0.40}{l \\ln(2)}$.\nThe iterative formula is $l_{n+1} = l_n - \\frac{f(l_n)}{f'(l_n)}$.\nLet's start with an initial guess of $l_0 = 100$.\n$f(100) = 0.03(100) - 0.40 \\log_{2}(100) - 0.25 \\approx 3 - 0.40(6.643856) - 0.25 \\approx 3 - 2.65754 - 0.25 = 0.09246$\n$f'(100) = 0.03 - \\frac{0.40}{100 \\ln(2)} \\approx 0.03 - \\frac{0.40}{69.3147} \\approx 0.03 - 0.00577 = 0.02423$\n$l_1 = 100 - \\frac{0.09246}{0.02423} \\approx 100 - 3.8159 = 96.1841$\n\nLet's perform a second iteration with $l_1 = 96.1841$:\n$f(96.1841) = 0.03(96.1841) - 0.40 \\log_{2}(96.1841) - 0.25 \\approx 2.885523 - 0.40(6.588079) - 0.25 = 2.885523 - 2.635232 - 0.25 \\approx 0.000291$\n$f'(96.1841) = 0.03 - \\frac{0.40}{96.1841 \\ln(2)} \\approx 0.03 - \\frac{0.40}{66.650} \\approx 0.03 - 0.00600 = 0.02400$\n$l_2 = 96.1841 - \\frac{0.000291}{0.02400} \\approx 96.1841 - 0.01213 = 96.17197$\n\nThe value has converged rapidly. Let's check the function value at $l=96.172$:\n$f(96.172) = 0.03(96.172) - 0.40 \\log_{2}(96.172) - 0.25 \\approx 2.88516 - 0.40(6.58793) - 0.25 = 2.88516 - 2.635172 - 0.25 = -0.000012$\nThis value is sufficiently close to zero. Thus, $l^{\\star} \\approx 96.172$.\n\nThe problem requires the answer to be rounded to four significant figures.\nThe value is $96.172...$. The first four significant figures are $9$, $6$, $1$, and $7$. The fifth digit is $2$, which is less than $5$, so we round down.\n$l^{\\star} \\approx 96.17$.\nThis means that for a run queue length of approximately $96$, the two designs have roughly the same overhead. For shorter queues, the unsorted list is more efficient, while for longer queues, the binary heap is superior.", "answer": "$$\n\\boxed{96.17}\n$$", "id": "3661227"}, {"introduction": "Modern multiprocessor systems often feature Non-Uniform Memory Access (NUMA) architectures, creating a core scheduling challenge: balancing CPU load versus maintaining data locality. This conceptual problem requires you to act as an OS designer, evaluating different load-balancing strategies for a realistic workload on a NUMA machine. Your goal is to maximize performance by selecting the policy that best preserves locality—keeping threads close to their data—while still ensuring high CPU utilization in the face of dynamic events like I/O blocking.", "problem": "A dual-socket multiprocessor has $N=8$ cores per socket. Each socket has a shared Last Level Cache (LLC) of capacity $C=16$ MiB and local Dynamic Random-Access Memory (DRAM). The machine implements Non-Uniform Memory Access (NUMA) with a first-touch allocation policy: a page is placed in the DRAM local to the socket on which it is first touched. A set of $T=16$ identical user threads executes a barrier-synchronized loop. In each iteration, a thread repeatedly traverses a private working set of size $w=1$ MiB and then participates in a barrier, after which it scans a read-mostly shared dataset of size $D=4$ MiB used by all threads in the same process. Short blocking events due to input/output cause a random subset of threads (on average $25\\%$ of them) to block for approximately $5$ ms; to maintain near-full utilization during these events, the Operating System (OS) must perform load balancing at intervals of approximately $1$ ms within whatever balancing domain is chosen.\n\nAssume the following foundational facts and definitions:\n- A cache hit occurs when the referenced data remains in the cache since its last access; capacity-induced misses occur when the aggregate actively used data exceeds the cache capacity.\n- If the aggregate working set actively contending for a given LLC during the relevant reuse window is less than the LLC capacity, the hit probability is high; if it exceeds capacity, the hit probability degrades due to evictions.\n- Under first-touch placement, migrating a thread to a different socket makes its subsequent page accesses remote until the pages are migrated; remote DRAM accesses are slower than local DRAM accesses.\n- The goal of “maximizing locality” here means maximizing the expected fraction of memory references that are either LLC hits or, on an LLC miss, are served by local DRAM rather than remote DRAM.\n\nYou are evaluating scheduler balancing scope choices $S$ to maximize locality under the stated utilization requirement. Consider these candidate policies:\n\nA. System-wide per-core balancing: a single global domain across both sockets rebalance run queues every $1$ ms with no preference for socket locality.\n\nB. Per-socket balancing: independent balancing domains per socket rebalance every $1$ ms within each socket; rare cross-socket rebalancing occurs only every $100$ ms to correct long-lived imbalance.\n\nC. No rebalancing after an initial random placement: each thread is pinned to its initially chosen core permanently.\n\nD. Cross-socket paired-core balancing: balance is performed every $1$ ms within fixed core pairs that span sockets (for each index $i$, core $i$ of socket $0$ is paired with core $i$ of socket $1$), allowing frequent cross-socket migrations within each pair.\n\nWhich choice of $S$ best maximizes locality as defined above while satisfying the utilization requirement?\n\nSelect one:\n\nA. System-wide per-core balancing\n\nB. Per-socket balancing\n\nC. No rebalancing after initial random placement\n\nD. Cross-socket paired-core balancing", "solution": "The problem requires finding the optimal scheduling policy that balances high CPU utilization with maximum memory locality on a Non-Uniform Memory Access (NUMA) system.\n\nFirst, let's analyze the workload and hardware configuration. The system has $2$ sockets with $8$ cores each, totaling $16$ cores. There are $16$ threads. An ideal initial placement assigns $8$ threads to each socket. Let's analyze the memory footprint per socket for this placement:\n- Each thread has a private working set of $w=1$ MiB. For $8$ threads, this is $8 \\times 1 = 8$ MiB.\n- All threads access a shared dataset of $D=4$ MiB.\n- The total active data per socket is $8 \\text{ MiB} + 4 \\text{ MiB} = 12 \\text{ MiB}$.\n- The Last Level Cache (LLC) capacity per socket is $C=16$ MiB.\n\nSince the total active data per socket ($12$ MiB) fits within the LLC capacity ($16$ MiB), threads will have a high cache hit rate as long as they remain on the same socket. Due to the first-touch memory policy, each thread's private data will also reside in the DRAM local to its socket, maximizing NUMA locality. The challenge arises because I/O blocking creates idle cores, and the OS must load balance to maintain utilization.\n\nNow, let's evaluate each policy:\n\n**A. System-wide per-core balancing:** This policy treats all $16$ cores as a single pool and migrates threads freely between sockets to balance load. This is catastrophic for locality. A thread moved from socket 0 to socket 1 will suffer slow remote memory accesses for all its private data and will lose the benefit of the warm cache on socket 0. Frequent migrations ($1$ ms intervals) will lead to constant cache and NUMA thrashing. This fails to maximize locality.\n\n**B. Per-socket balancing:** This policy creates two balancing domains, one for each socket. Frequent load balancing ($1$ ms) happens only *within* a socket's $8$ cores. This is the ideal approach. A thread moved between cores on the same socket continues to access the same shared LLC and local DRAM. This preserves both cache and NUMA locality perfectly. The rare cross-socket migration ($100$ ms) handles severe, long-term imbalances without causing constant thrashing. This policy successfully maintains high utilization while maximizing locality.\n\n**C. No rebalancing after an initial random placement:** This policy provides the best possible locality by pinning threads to cores. However, it fails the problem's explicit requirement to maintain high utilization. When a thread blocks, its core becomes idle, leading to an average of $25\\%$ of cores being unused.\n\n**D. Cross-socket paired-core balancing:** This policy creates rigid pairs of cores across sockets. It allows frequent migration between the two cores in a pair. This is a limited and flawed form of cross-socket migration. It still destroys NUMA and cache locality just like policy A whenever a thread moves to the other socket in its pair. It is also an ineffective way to balance the system's overall load.\n\n**Conclusion:** Per-socket balancing (B) is the only policy that correctly balances the competing goals. It keeps frequent migrations within the hardware's locality domains (the sockets), thus preserving cache and NUMA performance while still reacting to load imbalances to keep cores busy.", "answer": "$$\\boxed{B}$$", "id": "3661196"}, {"introduction": "Simultaneous Multithreading (SMT) improves processor throughput but introduces a subtle challenge: threads sharing a core's resources interfere with each other, affecting their actual performance. This practice explores how such hardware-level contention can undermine a scheduler's fairness goals. You will use a quantitative model to calculate the deviation between the intended, weight-based CPU allocation and the realized share received by each thread, providing insight into why modern schedulers must be aware of complex hardware behaviors.", "problem": "A system has $N=3$ identical processor cores, each supporting two Simultaneous Multithreading (SMT) hardware threads. The operating system employs weighted fairness: in the absence of interference, a runnable software thread $i$ with weight $w_i$ should receive a fraction of total processor capacity equal to $w_i$ divided by the sum of all runnable threads’ weights. Under SMT resource sharing, however, each hardware thread $k$ exhibits a measured share scalar $s_k \\in (0,1]$ that multiplies the realized service rate of any software thread bound to it, reflecting contention for shared pipeline and cache resources. Assume the scheduler continues to allocate time slices in proportion to $w_i$, but the realized service rate becomes proportional to $w_i s_{k(i)}$, where $k(i)$ denotes the hardware thread hosting software thread $i$. Realized capacity fractions are then the $w_i s_{k(i)}$ values normalized by their sum across all runnable threads.\n\nConsider five runnable software threads $i \\in \\{1,2,3,4,5\\}$ with weights $w_1=1.0$, $w_2=2.0$, $w_3=1.5$, $w_4=1.0$, and $w_5=3.0$. They are pinned to hardware threads on the three cores as follows: thread $1$ to hardware thread $k=1$, thread $2$ to $k=2$, thread $3$ to $k=3$, thread $4$ to $k=4$, and thread $5$ to $k=5$; the sibling $k=6$ is idle. The measured share scalars for the active hardware threads are $s_1=0.58$, $s_2=0.62$, $s_3=0.55$, $s_4=0.60$, and $s_5=0.98$.\n\nDefine the ideal weighted share for thread $i$ as $p_i = \\frac{w_i}{\\sum_{j=1}^{5} w_j}$ and the realized share under SMT contention as $f_i = \\frac{w_i s_{k(i)}}{\\sum_{j=1}^{5} w_j s_{k(j)}}$. Using the sum of squared deviations from the ideal,\n$$L \\;=\\; \\sum_{i=1}^{5} \\left(f_i - p_i\\right)^{2},$$\ncompute $L$ for this system. Express the final value of $L$ as a pure number with no units and round your answer to four significant figures.", "solution": "The problem asks to compute the fairness loss, $L$, which is the sum of squared deviations between the ideal and realized CPU shares for five threads.\n\n**Step 1: Calculate the ideal shares ($p_i$).**\nThe ideal share for a thread is its weight divided by the total weight.\nTotal weight $\\sum w_j = 1.0 + 2.0 + 1.5 + 1.0 + 3.0 = 8.5$.\nThe ideal shares are:\n$p_1 = 1.0 / 8.5 \\approx 0.117647$\n$p_2 = 2.0 / 8.5 \\approx 0.235294$\n$p_3 = 1.5 / 8.5 \\approx 0.176471$\n$p_4 = 1.0 / 8.5 \\approx 0.117647$\n$p_5 = 3.0 / 8.5 \\approx 0.352941$\n\n**Step 2: Calculate the realized shares ($f_i$).**\nThe realized share is based on the effective weight, which is the thread's weight multiplied by its SMT share scalar, $w_i s_{k(i)}$.\nFirst, we find the effective weights:\n$w_1 s_1 = 1.0 \\times 0.58 = 0.58$\n$w_2 s_2 = 2.0 \\times 0.62 = 1.24$\n$w_3 s_3 = 1.5 \\times 0.55 = 0.825$\n$w_4 s_4 = 1.0 \\times 0.60 = 0.60$\n$w_5 s_5 = 3.0 \\times 0.98 = 2.94$\n\nNext, we sum these effective weights:\n$\\sum w_j s_j = 0.58 + 1.24 + 0.825 + 0.60 + 2.94 = 6.185$.\n\nNow, we can find the realized shares by normalizing the effective weights:\n$f_1 = 0.58 / 6.185 \\approx 0.093775$\n$f_2 = 1.24 / 6.185 \\approx 0.200485$\n$f_3 = 0.825 / 6.185 \\approx 0.133387$\n$f_4 = 0.60 / 6.185 \\approx 0.096993$\n$f_5 = 2.94 / 6.185 \\approx 0.475344$\n\n**Step 3: Compute the fairness loss ($L$).**\nThe fairness loss $L$ is the sum of the squared differences $(f_i - p_i)^2$.\n$d_1 = f_1 - p_1 \\approx 0.093775 - 0.117647 = -0.023872$\n$d_2 = f_2 - p_2 \\approx 0.200485 - 0.235294 = -0.034809$\n$d_3 = f_3 - p_3 \\approx 0.133387 - 0.176471 = -0.043084$\n$d_4 = f_4 - p_4 \\approx 0.096993 - 0.117647 = -0.020654$\n$d_5 = f_5 - p_5 \\approx 0.475344 - 0.352941 = 0.122403$\n\nNow, square these deviations and sum them:\n$d_1^2 \\approx 0.00056987$\n$d_2^2 \\approx 0.00121167$\n$d_3^2 \\approx 0.00185623$\n$d_4^2 \\approx 0.00042659$\n$d_5^2 \\approx 0.01498250$\n\n$L = \\sum d_i^2 \\approx 0.00056987 + 0.00121167 + 0.00185623 + 0.00042659 + 0.01498250$\n$L \\approx 0.01904686$\n\nRounding the final result to four significant figures gives $0.01905$.", "answer": "$$\\boxed{0.01905}$$", "id": "3661255"}]}