{"hands_on_practices": [{"introduction": "Understanding the performance of concurrent programs begins with quantifying the overhead of communication. In modern multicore processors, this communication is often implicit, managed by the cache coherence protocol. This first exercise [@problem_id:3625552] provides a model to calculate the fundamental cost of \"true sharing,\" where multiple threads repeatedly contend for the same piece of data. By modeling thread activity as a Poisson process, you will derive the expected rate of cache line invalidations, providing a clear link between the number of threads and the resulting coherence traffic.", "problem": "A shared-memory program runs on a multiprocessor with $T$ hardware threads pinned one per Central Processing Unit (CPU) core. All cores share a single physically indexed, physically tagged cache-coherent hierarchy that implements an invalidation-based Modified–Exclusive–Shared–Invalid (MESI) protocol. The program maintains a single $64$-bit counter that resides entirely within one cache line and is incremented using an atomic read–modify–write operation (for example, atomic fetch-and-add) by each thread.\n\nAssume the following simplified but realistic model of coherence and timing:\n- Each atomic increment is implemented by a Read For Ownership (RFO) transaction that brings the cache line into the requester’s cache in the Modified state, invalidating any other valid copy. After completing its increment, the requester retains the line in the Modified state.\n- There are no speculative reads of the line except as part of the RFO for the increment, and there are no other sharers; at any instant, at most one core holds a valid copy, namely the current owner in the Modified state.\n- When a different core issues an RFO while another core owns the line in the Modified state, the directory sends exactly one invalidation to the current owner to revoke its copy before transferring ownership to the requester. Count this as exactly one invalidation. If the same core that already owns the line issues another increment, there are no invalidations.\n- Each thread issues increments as an independent Poisson process with rate $r$ increments per second. All processes are independent and stationary, and the superposition of these processes therefore has the usual properties of a Poisson process.\n\nStarting from the core definitions of invalidation-based coherence and the standard properties of independent Poisson processes (superposition, memorylessness, and source selection proportional to rates), derive the steady-state expected count of invalidation messages per second caused by this workload, as a function of $T$ and $r$. Express your final answer as a single closed-form analytic expression in terms of $T$ and $r$. Do not round. State your final result in the unit “per second.”", "solution": "The problem asks for the steady-state expected count of invalidation messages per second for a shared counter being atomically incremented by $T$ threads. The threads are running on $T$ separate cores, and each thread's increment requests follow an independent Poisson process with rate $r$.\n\nFirst, we establish the total rate of atomic increment operations across all cores. The problem states that each of the $T$ threads issues increments as an independent Poisson process with a rate of $r$ increments per second. A fundamental property of independent Poisson processes is that their superposition is also a Poisson process whose rate is the sum of the individual rates. Let $R_{total}$ be the rate of the superposed process, which represents the total rate of atomic increments in the system.\n\n$$R_{total} = \\sum_{i=1}^{T} r = Tr$$\n\nThis is the total expected number of atomic increments (and thus, Read For Ownership, or RFO, requests) per second across the entire system.\n\nNext, we must determine the condition under which one of these RFO requests generates an invalidation message. According to the problem description:\n1. An RFO from a core that does not own the cache line causes one invalidation to be sent to the current owner.\n2. The core that performs an increment retains the line in the Modified (M) state, becoming the new owner.\n3. An RFO from the core that already owns the line causes no invalidations.\n\nTherefore, an invalidation event occurs if and only if the core issuing an RFO request is different from the core that issued the immediately preceding RFO request.\n\nWe need to find the probability that a randomly chosen increment operation from the superposed stream of all increments was issued by a core different from the one that issued the previous increment. Let's denote the core that issues the $k$-th increment as $C_k$. An invalidation occurs at the time of the $k$-th increment if and only if $C_k \\neq C_{k-1}$. We are interested in the probability $P(C_k \\neq C_{k-1})$.\n\nThe problem states that we should use the standard properties of independent Poisson processes. One such property, often called the \"source selection\" or \"competing processes\" property, states that for a superposed process, the probability that any given event originates from a specific constituent process $i$ is proportional to its rate $r_i$. In our case, all constituent processes (one for each core) have the same rate $r$. The probability, $p_i$, that a given increment event comes from core $i$ is:\n\n$$p_i = \\frac{r}{\\sum_{j=1}^{T} r} = \\frac{r}{Tr} = \\frac{1}{T}$$\n\nThis means that any single increment event is equally likely to come from any of the $T$ cores.\n\nAnother crucial property of a Poisson process is memorylessness. This implies that the identity of the core generating the $k$-th event is statistically independent of the identity of the core that generated the $(k-1)$-th event.\n\nLet's assume the $(k-1)$-th increment was performed by some core, say Core A. Core A is now the owner of the cache line. The next increment, the $k$-th one, will now occur. The probability that this $k$-th increment is also issued by Core A is, from our previous result, $P(C_k = \\text{Core A}) = \\frac{1}{T}$.\n\nAn invalidation will be triggered if this $k$-th increment is issued by any core *other than* Core A. Since the events are independent, the probability of this is:\n\n$$P_{inval} = P(C_k \\neq \\text{Core A}) = 1 - P(C_k = \\text{Core A}) = 1 - \\frac{1}{T} = \\frac{T-1}{T}$$\n\nThis is the probability that any single atomic increment operation will cause an invalidation.\n\nTo find the expected number of invalidations per second, which we denote as $I_{total}$, we multiply the total rate of increment operations by the probability that any given increment causes an invalidation.\n\n$$I_{total} = R_{total} \\times P_{inval}$$\n\nSubstituting the expressions for $R_{total}$ and $P_{inval}$:\n\n$$I_{total} = (Tr) \\times \\left( \\frac{T-1}{T} \\right)$$\n\nThe factor of $T$ cancels out, yielding the final expression for the expected rate of invalidations:\n\n$$I_{total} = r(T-1)$$\n\nThe unit of $r$ is increments per second per thread, and $T$ is the number of threads (unitless), so the unit of the final expression is indeed invalidations per second, as required.\n\nFor example, if $T=1$, there is only one core, which can never invalidate another's copy. The formula gives $I_{total} = r(1-1) = 0$, which is correct. If $T$ is very large, almost every increment comes from a new core, so the invalidation rate should approach the total increment rate $Tr$. The formula gives $r(T-1) = Tr - r$, which is indeed close to $Tr$ for large $T$.", "answer": "$$\\boxed{r(T-1)}$$", "id": "3625552"}, {"introduction": "Cache coherence protocols operate on cache lines, not individual variables. This architectural detail can lead to a subtle but severe performance issue known as \"false sharing,\" where unrelated data items happen to occupy the same cache line, causing unnecessary invalidations. This practice [@problem_id:3625510] moves from theory to application, challenging you to write a program that arithmetically models and quantifies false sharing for different memory layouts. By comparing Array-of-Structures (AoS) and Structure-of-Arrays (SoA), you will gain a concrete understanding of how data organization impacts parallel performance.", "problem": "Consider a simplified multi-core cache model for a physics simulation. The memory is byte-addressed, and each central processing unit core has a private cache that uses fixed-size cache lines of $L$ bytes. The system employs a write-invalidate cache coherence protocol: when a thread writes any byte within a cache line, all other cores' copies of that line are invalidated. False sharing occurs when multiple threads write distinct words located within the same cache line; although no true data dependency exists, the coherence protocol still forces invalidations.\n\nYou will analyze how the memory layout affects false sharing when multiple threads update a single hot field across a large collection of bodies. Two layouts are used: Array of Structures (AoS) and Structure of Arrays (SoA). In AoS, each body is represented by a structure of size $s$ bytes with the hot field at byte offset $o$ within the structure. In SoA, the hot field of all bodies is stored in one contiguous array where each element occupies $f$ bytes. An alignment variant is also considered: \"AoS-aligned\", where the hot field for each body is placed so that its address starts a new cache line (effectively one hot field per cache line). All addresses can be modeled in purely arithmetic terms without simulating real hardware.\n\nAssume the following programming model:\n- There are $N$ bodies indexed by $i = 0, 1, \\dots, N-1$.\n- There are $T$ threads that partition the bodies contiguously and disjointly. Thread $k$ (where $k \\in \\{0, 1, \\dots, T-1\\}$) processes the subrange from $i = \\left\\lfloor \\frac{kN}{T} \\right\\rfloor$ to $i = \\left\\lfloor \\frac{(k+1)N}{T} \\right\\rfloor - 1$, inclusive. Empty subranges are possible when $T > N$.\n- Each thread writes the hot field once per body in its subrange during a single iteration.\n\nDefine the effective address of the hot field for body index $i$ under three layouts:\n- AoS: $a_{\\text{AoS}}(i) = i \\cdot s + o$.\n- SoA: $a_{\\text{SoA}}(i) = i \\cdot f$.\n- AoS-aligned: $a_{\\text{ALGN}}(i) = i \\cdot L$.\n\nDefine the cache line index function for any byte address $a$ as $\\ell(a) = \\left\\lfloor \\frac{a}{L} \\right\\rfloor$. Under the write-invalidate protocol and contiguous partitions, any potential false sharing in this single iteration must arise on partition boundaries. Specifically, between adjacent threads $k$ and $k+1$, inspect whether the last body index $i_{\\text{end}} = \\left\\lfloor \\frac{(k+1)N}{T} \\right\\rfloor - 1$ of thread $k$ and the first body index $i_{\\text{start}} = \\left\\lfloor \\frac{(k+1)N}{T} \\right\\rfloor$ of thread $k+1$ map to the same cache line when using the hot field addresses. When they map to the same cache line, both threads write into that cache line, creating exactly one shared cache line on that boundary in this iteration. If either thread’s subrange is empty, no boundary sharing occurs at that boundary.\n\nYour task is to implement a complete program that, for each test case below, computes the count of shared cache lines (an integer) for AoS, SoA, and AoS-aligned layouts, respectively, using the model above. For each boundary between adjacent threads, count at most one shared cache line, and sum across all boundaries. The program should not spawn threads; it should perform purely arithmetic computation as specified.\n\nUse the following test suite of parameter sets $(N, T, L, s, o, f)$, with all quantities in bytes except $N$ and $T$ which are counts:\n1. $(N, T, L, s, o, f) = (1000, 4, 64, 48, 0, 8)$ is a general case with a typical cache line size and double-precision hot field.\n2. $(N, T, L, s, o, f) = (1000, 1, 64, 48, 0, 8)$ is a boundary condition with a single thread.\n3. $(N, T, L, s, o, f) = (1024, 8, 64, 64, 0, 64)$ is an edge case where each hot field element occupies an entire cache line.\n4. $(N, T, L, s, o, f) = (7, 3, 64, 24, 8, 8)$ is a small-size edge case with nontrivial structure offset.\n\nYour program must produce a single line of output containing the results as a comma-separated list of lists, one list per test case, where each inner list is $[\\text{AoS}, \\text{SoA}, \\text{ALGN}]$ and each element is the integer count of shared cache lines for that layout. For example, an output with two test cases would look like $[[x_1,y_1,z_1],[x_2,y_2,z_2]]$.\n\nThe final output must be exactly one line in the format described, with no additional text.", "solution": "The problem requires an analysis of false sharing, a performance degradation phenomenon in shared-memory multiprocessor systems. False sharing occurs when multiple threads access distinct data variables that happen to reside on the same cache line. In a system with a write-invalidate cache coherence protocol, a write by one thread to its variable will invalidate the entire cache line in other threads' caches, even if those threads only access different, unrelated variables on that same line. This forces subsequent accesses by other threads to fetch the line from main memory, incurring a significant performance penalty.\n\nThe problem provides a deterministic, arithmetic model to quantify the number of shared cache lines at thread partition boundaries for three different data layouts: Array of Structures (AoS), Structure of Arrays (SoA), and a specially aligned Array of Structures (AoS-aligned). The solution is a direct implementation of the specified model.\n\nThe algorithmic approach is as follows:\n\nFor each test case, specified by the parameters $(N, T, L, s, o, f)$, we compute the number of shared cache lines for each of the three memory layouts. The parameters are:\n- $N$: The number of bodies in the simulation.\n- $T$: The number of threads processing the bodies.\n- $L$: The size of a cache line in bytes.\n- $s$: The size of the full structure for one body in the AoS layout, in bytes.\n- $o$: The byte offset of the \"hot field\" within the structure in the AoS layout.\n- $f$: The size of the \"hot field\" element in the SoA layout, in bytes.\n\nThe core of the analysis is to examine the $T-1$ boundaries between adjacent thread partitions. For each boundary between thread $k$ and thread $k+1$ (where $k$ ranges from $0$ to $T-2$), we perform the following steps:\n\n1.  **Identify Partition Boundaries**: The problem states that $N$ bodies are partitioned contiguously among $T$ threads. Thread $k$ is responsible for the range of body indices from $i_{k, \\text{start}} = \\left\\lfloor \\frac{kN}{T} \\right\\rfloor$ to $i_{k, \\text{end}} = \\left\\lfloor \\frac{(k+1)N}{T} \\right\\rfloor - 1$.\n    A boundary is shared if both threads on either side of it have work to do. A thread $m$'s partition is empty if its start index is greater than its end index, which occurs if $\\lfloor mN/T \\rfloor > \\lfloor(m+1)N/T\\rfloor - 1$, or equivalently $\\lfloor mN/T \\rfloor \\ge \\lfloor(m+1)N/T\\rfloor$. This can happen if $T > N$. If the partition for either thread $k$ or thread $k+1$ is empty, no sharing occurs at this boundary.\n\n2.  **Locate Critical Accesses**: False sharing, in this model, can only occur between the last body processed by thread $k$ and the first body processed by thread $k+1$. The indices of these two bodies are:\n    - Last body for thread $k$: $i_1 = \\left\\lfloor \\frac{(k+1)N}{T} \\right\\rfloor - 1$.\n    - First body for thread $k+1$: $i_2 = \\left\\lfloor \\frac{(k+1)N}{T} \\right\\rfloor$.\n    Note that $i_2 = i_1 + 1$.\n\n3.  **Calculate Memory Addresses**: For each of the three layouts, we calculate the effective memory addresses of the hot fields for bodies $i_1$ and $i_2$ using the provided functions:\n    - **AoS**: $a_{\\text{AoS}}(i) = i \\cdot s + o$. The addresses are $a_1 = i_1 \\cdot s + o$ and $a_2 = i_2 \\cdot s + o$.\n    - **SoA**: $a_{\\text{SoA}}(i) = i \\cdot f$. The addresses are $a_1 = i_1 \\cdot f$ and $a_2 = i_2 \\cdot f$.\n    - **AoS-aligned (ALGN)**: $a_{\\text{ALGN}}(i) = i \\cdot L$. The addresses are $a_1 = i_1 \\cdot L$ and $a_2 = i_2 \\cdot L$.\n\n4.  **Determine Cache Line Indices**: The cache line index for any memory address $a$ is given by $\\ell(a) = \\left\\lfloor \\frac{a}{L} \\right\\rfloor$. We compute this for the addresses of the hot fields of bodies $i_1$ and $i_2$ for each layout.\n\n5.  **Count Shared Lines**: For a given layout, a shared cache line is counted at the boundary between threads $k$ and $k+1$ if the hot fields for bodies $i_1$ and $i_2$ map to the same cache line index. That is, if $\\ell(a_1) = \\ell(a_2)$. We increment the respective counter for that layout.\n\nThe total number of shared lines for each layout is the sum of counts over all $T-1$ boundaries.\n\nA crucial observation for the AoS-aligned case: the address of the hot field for body $i$ is $a_{\\text{ALGN}}(i) = i \\cdot L$. The corresponding cache line is $\\ell(a_{\\text{ALGN}}(i)) = \\lfloor(i \\cdot L) / L\\rfloor = i$. At a boundary, we compare the cache lines for bodies $i_1$ and $i_2$. The cache line indices are $i_1$ and $i_2$, respectively. Since $i_2 = i_1 + 1$, it is impossible for $i_1 = i_2$. Therefore, the number of shared cache lines for the AoS-aligned layout is deterministically $0$ under this model. This serves as an analytical validation of the computational result.\n\nThe final program implements this logic arithmetically, iterating through the test cases and, for each, iterating through the thread boundaries to compute the total counts.", "answer": "```c\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <math.h>\n\n// A struct to hold the parameters for a single test case.\ntypedef struct {\n    long N; // Number of bodies\n    long T; // Number of threads\n    long L; // Cache line size in bytes\n    long s; // AoS struct size in bytes\n    long o; // AoS hot field offset in bytes\n    long f; // SoA hot field size in bytes\n} TestCase;\n\nint main(void) {\n    // Define the test cases from the problem statement.\n    TestCase test_cases[] = {\n        {1000, 4, 64, 48, 0, 8},\n        {1000, 1, 64, 48, 0, 8},\n        {1024, 8, 64, 64, 0, 64},\n        {7, 3, 64, 24, 8, 8},\n    };\n\n    // Calculate the number of test cases.\n    int num_cases = sizeof(test_cases) / sizeof(test_cases[0]);\n    int results[num_cases][3]; // [AoS, SoA, ALGN] for each case\n\n    // Calculate the result for each test case.\n    for (int i = 0; i < num_cases; ++i) {\n        long N = test_cases[i].N;\n        long T = test_cases[i].T;\n        long L = test_cases[i].L;\n        long s = test_cases[i].s;\n        long o = test_cases[i].o;\n        long f = test_cases[i].f;\n\n        int aos_shares = 0;\n        int soa_shares = 0;\n        int algn_shares = 0;\n\n        // Iterate over the T-1 boundaries between adjacent threads.\n        for (long k = 0; k < T - 1; ++k) {\n            // Determine the workload for thread k and thread k+1.\n            long part_k_start = (k * N) / T;\n            long part_k_end = ((k + 1) * N) / T - 1;\n            \n            long part_k1_start = ((k + 1) * N) / T;\n            long part_k1_end = ((k + 2) * N) / T - 1;\n            \n            // If either partition is empty, no sharing occurs at this boundary.\n            if (part_k_start > part_k_end || part_k1_start > part_k1_end) {\n                continue;\n            }\n\n            // Get the indices of the last body of thread k and the first of thread k+1.\n            long i1 = part_k_end;\n            long i2 = part_k1_start;\n\n            // Check for sharing in AoS layout.\n            long long addr_aos1 = i1 * s + o;\n            long long addr_aos2 = i2 * s + o;\n            if ((addr_aos1 / L) == (addr_aos2 / L)) {\n                aos_shares++;\n            }\n\n            // Check for sharing in SoA layout.\n            long long addr_soa1 = i1 * f;\n            long long addr_soa2 = i2 * f;\n            if ((addr_soa1 / L) == (addr_soa2 / L)) {\n                soa_shares++;\n            }\n            \n            // Check for sharing in AoS-aligned layout.\n            // By definition, this is guaranteed to be false since i1 != i2.\n            // Address of body i is i * L, so cache line is simply i.\n            // We compare line i1 and i2, which are never equal.\n            // The code below will calculate 0, but is included for completeness.\n            long long addr_algn1 = i1 * L;\n            long long addr_algn2 = i2 * L;\n            if ((addr_algn1 / L) == (addr_algn2 / L)) {\n                algn_shares++;\n            }\n        }\n\n        results[i][0] = aos_shares;\n        results[i][1] = soa_shares;\n        results[i][2] = algn_shares;\n    }\n\n    // Print the results in the EXACT REQUIRED format before the final return statement.\n    printf(\"[\");\n    for (int i = 0; i < num_cases; ++i) {\n        printf(\"[%d,%d,%d]\", results[i][0], results[i][1], results[i][2]);\n        if (i < num_cases - 1) {\n            printf(\",\");\n        }\n    }\n    printf(\"]\");\n\n    return EXIT_SUCCESS;\n}\n```", "id": "3625510"}, {"introduction": "After identifying performance issues like false sharing, the next step is to devise a solution and analyze its consequences. A common fix for false sharing is to insert padding to force contentious data onto separate cache lines. However, this is not a free lunch; it increases the memory footprint, which can lead to higher cache miss rates. This final practice [@problem_id:3625495] asks you to model this exact trade-off. You will derive the \"break-even\" point where the performance benefit of eliminating coherence invalidations is exactly offset by the cost of increased L1 cache pressure, a core skill in performance-aware software design.", "problem": "A software queue shared by two Central Processing Unit (CPU) cores has two counters in memory: a head index, written only by the consumer core, and a tail index, written only by the producer core. Let the cache line size be $B$ bytes. If the head and tail variables reside in the same cache line, each write by either core causes a coherence invalidation visible to the other core. Let the producer update rate be $r_{t}$ writes per second and the consumer update rate be $r_{h}$ writes per second. Each cross-core coherence invalidation induces a stall of $t_{i}$ cycles on the writing core due to cache line ownership transfer under a typical Modified-Exclusive-Shared-Invalid (MESI) protocol.\n\nTo reduce invalidations, the data structure is optionally padded so that the tail is placed $L$ bytes after the head within the same memory object. Assume the starting offset of the head within a cache line is uniformly random over $\\{0,1,\\dots,B-1\\}$ across executions; under this assumption, when $0 \\le L \\le B$, the probability that head and tail reside on different cache lines is $L/B$, and it saturates to $1$ for $L \\ge B$.\n\nPadding by $L$ bytes increases the steady-state working set observed by the Level-1 Data Cache (L1D) by $L$ bytes. Assume a uniform access model in which the aggregate L1D line reference rate from both cores is $R$ cache lines per second. Let the L1D capacity be $C_{1}$ bytes, and let the stall penalty of an extra L1D miss that falls back to the next level be $t_{m}$ cycles. In this model, near steady state, the fractional increase in miss probability caused by the extra $L$ bytes is approximated by $L/C_{1}$, so the expected additional stall due to padding is $(t_{m} R L)/C_{1}$ cycles per second.\n\nDefine the expected total stall per second as the sum of coherence invalidation stall and padding-induced miss stall. Using only the definitions and assumptions above, derive a closed-form expression for the break-even coherence invalidation penalty per write, $t_{i}^{\\star}$ (in cycles), for which padding to exactly one cache line separation ($L=B$) yields the same expected total stall per second as no padding ($L=0$). Express your final answer symbolically in terms of $B$, $r_{h}$, $r_{t}$, $R$, $t_{m}$, and $C_{1}$. State your final result as a single analytic expression. Express the final answer in cycles. No rounding is required.", "solution": "The problem requires the derivation of a break-even coherence invalidation penalty, denoted as $t_{i}^{\\star}$, under a specific performance model. The break-even point is defined as the value of the penalty for which the total expected performance stall per second is identical for two configurations: one with no data structure padding ($L=0$ bytes) and one with padding equal to exactly one cache line size ($L=B$ bytes).\n\nThe total expected stall per second, which we can denote as a function $S(L)$, is defined as the sum of two components: the stall from coherence invalidations, $S_{coh}(L)$, and the stall from additional cache misses induced by padding, $S_{pad}(L)$.\n$$ S(L) = S_{coh}(L) + S_{pad}(L) $$\n\nFirst, let us formulate the expression for the coherence stall, $S_{coh}(L)$. Coherence invalidations, and the associated stalls, occur only when the head and tail counters reside on the same cache line, a situation known as false sharing. A write to the head by the consumer core or a write to the tail by the producer core will invalidate the cache line for the other core if they both have a copy. The total rate of writes that can potentially cause such invalidations is the sum of the individual write rates, $r_{h} + r_{t}$. Each such event incurs a stall of $t_{i}$ cycles.\n\nThe problem states that the probability of the head and tail residing on *different* cache lines is $P(\\text{different}) = L/B$ for $0 \\le L \\le B$. Consequently, the probability of them residing on the *same* cache line is $P(\\text{same}) = 1 - P(\\text{different})$.\n$$ P(\\text{same}) = 1 - \\frac{L}{B} \\quad \\text{for } 0 \\le L \\le B $$\nThe expected stall per second due to coherence invalidations is the total rate of writes multiplied by the probability of false sharing, all multiplied by the stall per invalidation.\n$$ S_{coh}(L) = (r_{h} + r_{t}) t_{i} P(\\text{same}) = (r_{h} + r_{t}) t_{i} \\left(1 - \\frac{L}{B}\\right) $$\n\nNext, we consider the padding-induced miss stall, $S_{pad}(L)$. The problem statement provides a direct model for this component: \"the expected additional stall due to padding is $(t_{m} R L)/C_{1}$ cycles per second.\"\n$$ S_{pad}(L) = \\frac{t_{m} R L}{C_{1}} $$\n\nCombining these two components, the total expected stall per second is:\n$$ S(L) = (r_{h} + r_{t}) t_{i} \\left(1 - \\frac{L}{B}\\right) + \\frac{t_{m} R L}{C_{1}} $$\nThis expression is valid for the range of padding $0 \\le L \\le B$.\n\nWe are asked to find the break-even value $t_{i}^{\\star}$ where the total stall with no padding ($L=0$) equals the total stall with padding of one cache line ($L=B$). Let's evaluate $S(L)$ at these two points.\n\nFor the case of no padding, $L=0$:\n$$ S(L=0) = (r_{h} + r_{t}) t_{i} \\left(1 - \\frac{0}{B}\\right) + \\frac{t_{m} R \\cdot 0}{C_{1}} $$\n$$ S(L=0) = (r_{h} + r_{t}) t_{i} (1 - 0) + 0 $$\n$$ S(L=0) = (r_{h} + r_{t}) t_{i} $$\nIn this case, false sharing always occurs, and there is no stall from padding.\n\nFor the case of padding by one cache line size, $L=B$:\n$$ S(L=B) = (r_{h} + r_{t}) t_{i} \\left(1 - \\frac{B}{B}\\right) + \\frac{t_{m} R B}{C_{1}} $$\n$$ S(L=B) = (r_{h} + r_{t}) t_{i} (1 - 1) + \\frac{t_{m} R B}{C_{1}} $$\n$$ S(L=B) = 0 + \\frac{t_{m} R B}{C_{1}} $$\n$$ S(L=B) = \\frac{t_{m} R B}{C_{1}} $$\nIn this case, false sharing is completely eliminated, but the system incurs the maximum padding-induced stall penalty considered in this model.\n\nThe break-even condition is met when $S(L=0) = S(L=B)$, with the coherence penalty being $t_{i}^{\\star}$:\n$$ (r_{h} + r_{t}) t_{i}^{\\star} = \\frac{t_{m} R B}{C_{1}} $$\n\nTo find the expression for $t_{i}^{\\star}$, we solve for it by dividing both sides by the total write rate, $(r_{h} + r_{t})$.\n$$ t_{i}^{\\star} = \\frac{t_{m} R B}{C_{1} (r_{h} + r_{t})} $$\nThis is the closed-form expression for the break-even coherence invalidation penalty per write, expressed symbolically in terms of the given parameters.", "answer": "$$\\boxed{\\frac{t_{m} R B}{C_{1} (r_{h} + r_{t})}}$$", "id": "3625495"}]}