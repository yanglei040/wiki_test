{"hands_on_practices": [{"introduction": "At the heart of many high-performance applications lies a single-threaded event loop, responsible for processing I/O events. To reason about the capacity of such a system, we must first understand its fundamental limits. This exercise models an event-driven server to derive its maximum sustainable throughput, connecting abstract concepts like CPU quotas to the concrete performance a developer can expect. By calculating this limit, you will gain a first-principles understanding of how system resources constrain application performance [@problem_id:3621630].", "problem": "Consider an event-driven server built around an Asynchronous Input/Output (I/O) interface. The server runs a single-threaded event loop that pulls completions from a queue and processes each completion by executing a finite sequence of user-space operations and non-blocking system calls. Assume the following:\n\n- The event loop is scheduled on a single Central Processing Unit (CPU) core under a strict quota enforced by the operating system scheduler: over any wall-clock interval of length $t$ seconds, the event loop receives at most $B \\cdot t$ seconds of CPU time, where $B$ is a fixed constant with $0 < B \\leq 1$.\n- Each completed I/O event, once dequeued, requires a deterministic amount of CPU processing time $c$ (seconds per event) in the event loop before it is considered fully handled.\n- The completion queue is sufficiently large that it will only begin to drop events if the long-term average number of events arriving per unit time exceeds the long-term average number of events the loop can process per unit time.\n- The event loop itself never blocks in a way that yields the CPU during event processing, and any polling overhead is already included in $c$.\n\nStarting only from these assumptions and the core definition that a queue is stable if its long-run arrival rate is no greater than its long-run service rate, derive a closed-form expression for the maximum sustainable event arrival rate $\\lambda_{\\max}$ (events per second) such that the system does not drop events over the long run. Express your final answer as events per second. No numerical substitution is required; provide the expression in terms of $B$ and $c$ only.", "solution": "The problem asks for the maximum sustainable event arrival rate, denoted as $\\lambda_{\\max}$, for a single-threaded, event-driven server. The stability of the system is predicated on the condition that the long-run average arrival rate does not exceed the long-run average service rate. Therefore, the maximum sustainable arrival rate is equal to the system's maximum long-run average service rate. Let us denote the service rate, in events per second, by $\\mu$. The objective is to find $\\lambda_{\\max} = \\mu$.\n\nThe problem provides the following key parameters and constraints:\n1.  The server's event loop is scheduled on a single CPU core.\n2.  The CPU time allocated to the event loop is limited by a quota. Over any wall-clock interval of duration $t$ seconds, the event loop is granted at most $B \\cdot t$ seconds of CPU time. The constant $B$ is a dimensionless fraction representing the CPU share, where $0  B \\leq 1$.\n3.  Each event requires a fixed amount of CPU time, $c$ seconds, for its processing. The units of $c$ are seconds per event.\n\nOur goal is to determine the maximum number of events the server can process per unit of wall-clock time. Let us consider an arbitrary, sufficiently long interval of wall-clock time, $t$.\n\nDuring this interval of $t$ seconds, the total amount of available CPU time for the event loop is, by definition, at most $B \\cdot t$ seconds. Let's call this available CPU time $T_{CPU}$. So, we have:\n$$T_{CPU} \\leq B \\cdot t$$\nTo find the maximum sustainable rate, we must consider the maximum processing capacity, which corresponds to the case where the event loop fully utilizes its CPU quota. Thus, we take the available CPU time to be exactly:\n$$T_{CPU} = B \\cdot t$$\n\nThe processing of a single event consumes $c$ seconds of CPU time. Let $N$ be the total number of events that can be processed using the available CPU time $T_{CPU}$. The relationship between $N$, $T_{CPU}$, and $c$ is:\n$$T_{CPU} = N \\cdot c$$\n\nBy substituting the expression for $T_{CPU}$ from the CPU quota, we get:\n$$B \\cdot t = N \\cdot c$$\n\nWe can now solve for $N$, the total number of events processed during the wall-clock time interval $t$:\n$$N = \\frac{B \\cdot t}{c}$$\n\nThe average service rate, $\\mu$, is defined as the number of events processed per unit of wall-clock time. Therefore, we can calculate $\\mu$ by dividing the number of events processed, $N$, by the duration of the wall-clock time interval, $t$:\n$$\\mu = \\frac{N}{t}$$\n\nSubstituting the expression for $N$:\n$$\\mu = \\frac{\\frac{B \\cdot t}{c}}{t}$$\n\nSimplifying the expression by canceling the term $t$ from the numerator and the denominator yields the long-run average service rate:\n$$\\mu = \\frac{B}{c}$$\n\nThe units of this expression are consistent. The parameter $B$ is dimensionless (seconds of CPU time per second of wall-clock time), and $c$ has units of seconds per event. Thus, the units of $\\mu$ are:\n$$\\frac{[\\text{dimensionless}]}{[\\text{seconds} / \\text{event}]} = \\frac{1}{\\text{seconds} / \\text{event}} = \\frac{\\text{events}}{\\text{second}}$$\nThis confirms that our expression represents an event rate, as required.\n\nAccording to the problem's stability criterion, the queue will not grow indefinitely (and thus will not drop events in the long run) if the arrival rate $\\lambda$ is no greater than the service rate $\\mu$. The maximum sustainable arrival rate, $\\lambda_{\\max}$, is therefore equal to the system's maximum service rate, $\\mu$.\n$$\\lambda_{\\max} = \\mu$$\n\nTherefore, the closed-form expression for the maximum sustainable event arrival rate is:\n$$\\lambda_{\\max} = \\frac{B}{c}$$\nThis expression represents the fundamental capacity limit of the server, dictated by its fractional CPU allocation and the per-event processing cost.", "answer": "$$\\boxed{\\frac{B}{c}}$$", "id": "3621630"}, {"introduction": "A primary benefit of asynchronous I/O is the ability to overlap slow I/O operations with CPU-intensive computation, preventing expensive processor cores from sitting idle. This practice models a common application pipeline where data is fetched via I/O and then processed by the CPU. Your task is to determine the ideal level of I/O concurrency required to fully \"hide\" the I/O latency, ensuring the system's throughput is limited only by its computational power. This exercise is fundamental to designing balanced systems that maximize hardware utilization [@problem_id:3621649].", "problem": "Consider an application that processes a stream of independent requests using an Asynchronous Input/Output (AIO) interface. Each request follows a two-stage pipeline: an I/O stage and a compute stage. The I/O stage consists of submitting a read operation to the AIO subsystem and waiting for it to complete; the compute stage consists of performing deterministic computation on the data. Assume the following:\n- The AIO subsystem schedules up to $k$ outstanding I/O operations concurrently, each with deterministic latency $l$, and with no throughput degradation or queuing delay up to this level of concurrency.\n- The compute stage runs on a machine with $m$ identical central processing unit (CPU) cores, each capable of processing one request at a time. The compute time per request is deterministic and equal to $c$.\n- Scheduling overheads, data transfer bandwidth limits, memory limits, and contention effects besides the latency $l$ are negligible. The system reaches a steady state in which the arrival rate to the compute stage equals the completion rate of I/O operations.\n\nUsing only first principles from operating systems and queueing theory—specifically, the basic definition of throughput and Little’s law—derive the exact closed-form expression for the minimal integer concurrency level $k^{\\*}$ that ensures the CPU compute stage never waits for I/O completions in steady state. In other words, find the smallest $k^{\\*}$ such that the I/O latency is fully hidden and the long-run throughput is limited by the compute stage rather than by the I/O stage. Express your answer as a symbolic expression in terms of $m$, $c$, and $l$. The final answer must be a single analytic expression; do not provide an inequality or an equation.", "solution": "The objective is to find the minimum integer concurrency level, denoted as $k^{\\*}$, such that the throughput of the system is limited by the compute stage, not the I/O stage. This means a continuous supply of completed I/O requests must be available to the compute cores, ensuring they are never idle waiting for data.\n\nLet $\\lambda$ represent the steady-state throughput of the system, defined as the number of requests processed per unit time.\n\nFirst, we determine the maximum possible throughput of the compute stage, $\\lambda_{\\text{compute}}$. The compute stage has $m$ identical cores, and each core takes time $c$ to process a single request. The processing rate of a single core is $\\frac{1}{c}$ requests per unit time. Since the cores operate in parallel on independent requests, their capacities add up. Therefore, the maximum throughput of the compute stage is:\n$$\n\\lambda_{\\text{compute}} = m \\times \\frac{1}{c} = \\frac{m}{c}\n$$\n\nNext, we determine the maximum possible throughput of the I/O stage, $\\lambda_{\\text{I/O}}$. The problem directs us to use Little's Law, which relates the average number of items in a stable system ($N$), the average arrival/departure rate (throughput, $\\lambda$), and the average time an item spends in the system ($T$):\n$$\nN = \\lambda T\n$$\nIn the context of our AIO subsystem:\n- $N$ is the number of concurrent, outstanding I/O operations. To maximize I/O throughput, we must sustain as many concurrent operations as the system allows, which is $k$. Thus, we analyze the system at its capacity, where $N = k$.\n- $T$ is the time a request spends in the I/O stage, which is the deterministic latency $l$.\n- $\\lambda$ is the throughput of the I/O stage, $\\lambda_{\\text{I/O}}$.\n\nApplying Little's Law to the I/O subsystem:\n$$\nk = \\lambda_{\\text{I/O}} \\cdot l\n$$\nSolving for the I/O throughput gives:\n$$\n\\lambda_{\\text{I/O}} = \\frac{k}{l}\n$$\n\nThe condition that the compute stage is the bottleneck and never waits for I/O means that the I/O stage must be able to supply requests at a rate at least as great as the rate at which the compute stage can process them. This translates to the following inequality:\n$$\n\\lambda_{\\text{I/O}} \\ge \\lambda_{\\text{compute}}\n$$\n\nSubstituting the expressions for $\\lambda_{\\text{I/O}}$ and $\\lambda_{\\text{compute}}$ into the inequality:\n$$\n\\frac{k}{l} \\ge \\frac{m}{c}\n$$\n\nWe are looking for the minimal integer concurrency level $k^{\\*}$ that satisfies this condition. To find $k$, we rearrange the inequality:\n$$\nk \\ge \\frac{m \\cdot l}{c}\n$$\n\nSince $k$ must be an integer, the smallest integer value that satisfies this inequality is the ceiling of the right-hand side. The ceiling function, $\\lceil x \\rceil$, gives the smallest integer greater than or equal to $x$. Therefore, the minimal integer concurrency level $k^{\\*}$ is:\n$$\nk^{*} = \\left\\lceil \\frac{m \\cdot l}{c} \\right\\rceil\n$$\nThis expression represents the number of requests that must be \"in flight\" in the I/O system to cover the total compute work done during the time it takes for one I/O operation to complete. The ceiling function correctly handles the requirement that $k^{\\*}$ must be an integer.", "answer": "$$\n\\boxed{\\left\\lceil \\frac{m \\cdot l}{c} \\right\\rceil}\n$$", "id": "3621649"}, {"introduction": "While asynchronous interfaces offer great power and performance, they also introduce subtle complexities, especially when mixed with traditional buffered I/O libraries like the C Standard I/O stream. This exercise presents a scenario where both paradigms operate on the same file, exploring how their different assumptions about state management can lead to desynchronization. By analyzing how the kernel's view of the file position can diverge from the library's buffered state, you will learn to identify and avoid critical bugs that can lead to data corruption or data loss [@problem_id:3621602].", "problem": "A program manipulates a regular file using both the C Standard Input/Output library stream and asynchronous interfaces. The file is opened once with a single open file description, yielding a file descriptor $fd$, and then wrapped into a C Standard Input/Output stream $FILE$ via $fdopen$. Assume the following widely accepted and well-tested base facts and definitions hold:\n- The operating system kernel maintains, for each open file description, a current file offset $o$ that is advanced by sequential reads and writes.\n- A call to the system call $read$ on a descriptor that uses the shared file position reads from the current offset $o$ and advances $o$ by the number of bytes actually read.\n- A call to the system call $pread$ reads from an explicit offset without changing $o$.\n- The function $dup$ produces a new file descriptor that refers to the same open file description and therefore shares the current offset $o$.\n- The C Standard Input/Output stream $FILE$ maintains a user-space buffer (assume capacity $B = 8192$ bytes) to amortize system calls. The function $fread$ can fill this buffer with a single or multiple underlying $read$ calls and then serve application reads from this buffer without consulting the kernel again until the buffer is exhausted. The $FILE$ object’s internal notion of the next byte to deliver is independent from the kernel’s page cache and depends on the previously fetched data and the program’s subsequent reads from the stream.\n- In Linux $io\\_uring$, an operation submitted with offset $-1$ uses and advances the shared file position $o$ (file-position semantics). In contrast, $POSIX$ Asynchronous Input/Output operations such as $aio\\_read$ use an explicit offset (from $aiocb \\rightarrow aio\\_offset$) and do not change $o$.\n- Asynchronous operations may complete at any time relative to $fread$ calls unless the program enforces ordering.\n\nConsider a file of size $S = 16384$ bytes containing deterministic bytes $b[0..S-1]$. The program performs sequential reads using $fread$ on the $FILE$ stream but also submits asynchronous operations on the same underlying file. Suppose $fread$ initially fills its buffer by performing one underlying $read$ of $4096$ bytes at offset $o = 0$, then returns $2048$ bytes to the caller, leaving $2048$ bytes unread in the user-space buffer. Immediately after returning those $2048$ bytes, the program submits an asynchronous operation. After the asynchronous operation completes, the program calls $fread$ again expecting the next $2048$ bytes to be the sequence $b[2048..4095]$.\n\nWhich of the following scenarios can cause the state of $fread$ and the kernel’s notion of file position and buffered data to become desynchronized, such that the subsequent $fread$ either skips bytes, duplicates bytes, or otherwise returns a sequence not equal to the next contiguous bytes $b[2048..4095]$? Select all that apply.\n\nA. The program submits an $io\\_uring$ read with operation $IORING\\_OP\\_READ$ on $fd$ using offset $-1$ to read $4096$ bytes into an unrelated memory buffer. The asynchronous read completes before the next call to $fread$.\n\nB. The program submits a $POSIX$ Asynchronous Input/Output $aio\\_read$ on $fd$ with $aiocb \\rightarrow aio\\_offset = 4096$ to read $4096$ bytes into an unrelated memory buffer. The asynchronous read completes before the next call to $fread$.\n\nC. The program submits an $io\\_uring$ write with operation $IORING\\_OP\\_WRITE$ on $fd$ using offset $-1$ to write $4096$ bytes at the current file position $o$. The data being written overlaps the region that $fread$ would read next. The asynchronous write completes before the next call to $fread$.\n\nD. The program first duplicates the descriptor with $fd2 = dup(fd)$, then submits an $io\\_uring$ read $IORING\\_OP\\_READ$ on $fd2$ using offset $-1$ to read $4096$ bytes. The asynchronous read completes before the next call to $fread$.\n\nE. The program opens the same pathname a second time, obtaining a new, independent file descriptor $fd3$ (not via $dup$), and submits a $pread$ equivalent asynchronous read of $4096$ bytes at offset $4096$ on $fd3$. The asynchronous read completes before the next call to $fread$.\n\nGive your reasoning from the base definitions above and the concurrent nature of asynchronous completion, focusing on how and why $fread$’s user-space buffer state and the kernel’s open file description offset $o$ can diverge, and whether the kernel’s page cache contents seen by $fread$ can differ from the program’s intended sequential view of $b[0..S-1]$ in each scenario. Do not rely on library-specific undocumented behaviors; reason from the standardized semantics stated.", "solution": "The core of the problem lies in the management of the single kernel file offset $o$ associated with the open file description shared by the file descriptor $fd$ and the `FILE*` stream. Desynchronization occurs if an external operation modifies $o$ without the C standard library's knowledge.\n\n**Initial State Analysis:**\n1.  The program calls `fread` for the first time. The `FILE*` stream's buffer is empty.\n2.  The `FILE*` stream's implementation issues a system call, `read(fd, internal_buffer, 4096)`, starting at the initial file offset $o = 0$.\n3.  This `read` is successful. The kernel reads $4096$ bytes ($b[0..4095]$) and advances the file offset to $o = 4096$.\n4.  The `FILE*` stream's buffer is now filled with $b[0..4095]$.\n5.  `fread` copies the first $2048$ bytes ($b[0..2047]$) to the application's buffer and returns.\n6.  At this point, just before the asynchronous operation is submitted, the state is:\n    *   Kernel file offset: $o = 4096$.\n    *   `FILE*` stream's buffer: Contains $b[0..4095]$.\n    *   `FILE*` stream's internal position: Points to the next byte to be served from its buffer, which corresponds to the file content at offset $2048$. There are $2048$ bytes ($b[2048..4095]$) remaining in the buffer.\n    *   `FILE*` stream's expectation: It has consumed data up to offset $4095$. It expects the next kernel `read` to occur at offset $4096$.\n\nThe problem is to determine which of the following asynchronous operations, completing before the next buffer-refilling `read`, will alter the kernel offset $o$, thus violating the `FILE*` stream's expectation. Interpreting \"the subsequent fread\" as any `fread` call that occurs after the state has been corrupted (specifically, the one that triggers the next kernel `read`), we analyze the options.\n\n**Option A: The program submits an `io_uring` read with operation `IORING_OP_READ` on `fd` using offset `-1` to read $4096$ bytes into an unrelated memory buffer. The asynchronous read completes before the next call to `fread`.**\n\nAccording to the provided definitions, an `io_uring` operation with `offset = -1` uses and advances the shared file position $o$. The operation is submitted on `fd`, which uses the shared open file description. Before this operation, $o = 4096$. The asynchronous read will therefore commence at offset $4096$ and read $4096$ bytes. Upon completion, it will advance the file offset by $4096$. The new kernel offset will be $o = 4096 + 4096 = 8192$. The `FILE*` stream is unaware of this change. When its buffer is eventually exhausted (after serving the buffered $b[2048..4095]$), it will issue a `read` system call, expecting to get data starting from offset $4096$. Instead, the `read` will start at the current offset $o=8192$, causing it to read $b[8192..]$ and effectively skip the bytes $b[4096..8191]$. This is a desynchronization that leads to data skipping.\n\n*Verdict*: **Correct**.\n\n**Option B: The program submits a `POSIX` Asynchronous Input/Output `aio_read` on `fd` with `aiocb->aio_offset = 4096` to read $4096$ bytes into an unrelated memory buffer. The asynchronous read completes before the next call to `fread`.**\n\nThe problem states that `POSIX` `AIO` operations like `aio_read` use an explicit offset (here, $4096$) and **do not change** the shared file offset $o$. This type of I/O is analogous to `pread`. The operation will read the bytes $b[4096..8191]$, but the kernel file offset $o$ will remain unchanged at $4096$. When the `FILE*` stream later issues its buffer-refilling `read`, it will correctly start at $o=4096$. No desynchronization of the file offset occurs.\n\n*Verdict*: **Incorrect**.\n\n**Option C: The program submits an `io_uring` write with operation `IORING_OP_WRITE` on `fd` using offset `-1` to write $4096$ bytes at the current file position $o$. The data being written overlaps the region that `fread` would read next. The asynchronous write completes before the next call to `fread`.**\n\nSimilar to a read, an `io_uring` write with `offset = -1` uses and advances the shared file position $o$. Before this operation, $o = 4096$. The `io_uring` write will begin at offset $4096$ and advance the offset to $o = 4096 + 4096 = 8192$. This causes the same file offset desynchronization as in option A, leading to data skipping in a future `fread`. Additionally, this operation modifies the file content in the range $[4096, 8191]$, which means the data eventually read by a future `fread` would not be from the original sequence $b$, another form of desynchronization. The primary cause of failure in the sense of skipping bytes is the modification of $o$.\n\n*Verdict*: **Correct**.\n\n**Option D: The program first duplicates the descriptor with `fd2 = dup(fd)`, then submits an `io_uring` read `IORING_OP_READ` on `fd2` using offset `-1` to read $4096$ bytes. The asynchronous read completes before the next call to `fread`.**\n\nThe definition of `dup` states that the new file descriptor ($fd2$) refers to the **same open file description** as the original ($fd$). This means they share all file status flags and, critically, the current file offset $o$. Therefore, performing an operation on `fd2` that modifies $o$ is indistinguishable from performing it on `fd`. This scenario is functionally identical to option A. The `io_uring` read on `fd2` will start at $o = 4096$ and advance it to $o = 8192$. This causes desynchronization and will lead to a future `fread` skipping data.\n\n*Verdict*: **Correct**.\n\n**Option E: The program opens the same pathname a second time, obtaining a new, independent file descriptor `fd3` (not via `dup`), and submits a `pread` equivalent asynchronous read of $4096$ bytes at offset $4096$ on `fd3`. The asynchronous read completes before the next call to `fread`.**\n\nOpening a file path again with `open()` creates a **new and independent** open file description. This new description has its own file offset, which is completely separate from the offset $o$ associated with `fd`. Let's call the offset for `fd3` as $o_3$. Operations on `fd3` will use and modify $o_3$, but will have no effect on $o$. Furthermore, the operation specified is a `pread` equivalent, which uses an explicit offset ($4096$) and would not modify its own file offset ($o_3$) anyway. The kernel file offset $o$ associated with `fd` remains $4096$, and no desynchronization occurs.\n\n*Verdict*: **Incorrect**.", "answer": "$$\\boxed{ACD}$$", "id": "3621602"}]}