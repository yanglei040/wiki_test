{"hands_on_practices": [{"introduction": "In the early days of computing, hardware was far less reliable than it is today. For operating systems running long scientific or data processing jobs, frequent power glitches or component failures posed a constant threat. This exercise takes you back to that era, challenging you to analyze a fault-tolerance technique known as checkpointing, where the system's state is periodically saved to stable storage. By modeling this process [@problem_id:3639697], you will derive a foundational result in reliability engineering that quantifies the trade-off between the overhead of saving progress and the cost of re-doing lost work after a failure.", "problem": "A university research laboratory in 1978 deploys a small multiuser minicomputer whose only mass-storage peripheral available for bulk sequential transfers is Magnetic Tape (MT). Due to frequent power glitches, the operating system team wants a minimal, era-plausible checkpoint/restore mechanism that uses only magnetic tape to bound lost computation and reduce downtime.\n\nYou are asked to do the following:\n\n- Propose a plausible checkpoint/restore design appropriate for a late-1970s monolithic kernel without virtual memory:\n  1) The checkpoint operation must quiesce user processes, flush device queues, and serialize recoverable process state (registers, memory image, process control block, open-file table metadata as identifiers or pathnames, and pending signals) to a dedicated checkpoint tape in a self-describing format with a versioned header and redundancy (e.g., block checksums). The system must stop all application-level writes to disk during the checkpoint window and record a restart-intent marker at tape start to support idempotent restore. \n  2) The restore operation must boot a minimal recovery monitor, mount and rewind the checkpoint tape, validate the header, reconstruct process address spaces and registers, rebind open files by reopening paths in a consistent mode, and reinitialize device drivers to a quiescent state. Any device I/O in flight at checkpoint time must be treated as not executed after restore.\n\nAssume the following quantitative model for your analysis:\n\n- A single job runs indefinitely with periodic coordinated checkpoints every $\\tau$ seconds. Magnetic tape write throughput is $b_{w}$ (bytes per second), read throughput is $b_{r}$ (bytes per second), mount/rewind latency per tape operation is $m$ (seconds), and the serialized checkpoint image size is $s$ (bytes).\n- The time to write a full checkpoint is $C_{w}$, and the time to read and restore from the most recent checkpoint is $R_{r}$. You may assume sequential tape behavior so that $C_{w} = m + \\frac{s}{b_{w}}$ and $R_{r} = m + \\frac{s}{b_{r}}$.\n- Failures follow a Poisson process with rate $\\lambda$ (per second), independently of the checkpoint schedule. Assume $\\lambda \\tau \\ll 1$ so that the probability of more than one failure between checkpoints is negligible and that the failure point within a checkpoint interval can be treated as uniformly distributed on $[0,\\tau]$ for first-order analysis.\n- Define the long-run non-productive fraction of wall-clock time $D(\\tau)$ as the expected fraction of time spent not making net forward progress due to checkpointing, restart after failures, and re-execution of lost computation.\n- Define the data loss risk $L(\\tau)$ as the long-run expected fraction of wall-clock time lost solely to re-execution of rolled-back computation after failures.\n\nTasks:\n\n1) Starting from the above tape and failure model, derive expressions for $C_{w}$ and $R_{r}$ in terms of $m$, $s$, $b_{w}$, and $b_{r}$.\n\n2) Using renewal-reward reasoning and the $\\lambda \\tau \\ll 1$ approximation, derive $D(\\tau)$ and $L(\\tau)$ in terms of $\\lambda$, $\\tau$, $C_{w}$, and $R_{r}$.\n\n3) Determine the checkpoint interval $\\tau^{\\ast}$ that minimizes $D(\\tau)$, and then evaluate both $D(\\tau^{\\ast})$ and $L(\\tau^{\\ast})$ in closed form.\n\nExpress your final answer as a two-element row matrix $\\bigl[D(\\tau^{\\ast}),\\,L(\\tau^{\\ast})\\bigr]$ symbolically in terms of $\\lambda$, $m$, $s$, $b_{w}$, and $b_{r}$. No numerical evaluation is required, and no units should be included in the final boxed answer.", "solution": "The problem is scientifically grounded, well-posed, and objective. It provides a quantitative model based on standard principles of reliability engineering and queuing theory, suitable for mathematical analysis. All required parameters are defined, and the objectives are explicit. We may therefore proceed with a solution.\n\nThe solution is organized according to the three tasks specified in the problem statement.\n\n### Task 1: Expressions for $C_{w}$ and $R_{r}$\n\nThe problem statement provides a model for the duration of checkpoint and restore operations using magnetic tape. A tape operation is modeled as comprising two distinct phases: a mechanical latency and a data transfer period.\n\nThe time to write a full checkpoint, denoted $C_{w}$, consists of the mount and rewind latency, $m$, plus the time required to transfer the checkpoint image of size $s$ at a write throughput of $b_{w}$. Assuming sequential write access, the transfer time is the image size divided by the throughput, $\\frac{s}{b_{w}}$. Therefore, the total time for a checkpoint write operation is:\n$$C_{w} = m + \\frac{s}{b_{w}}$$\n\nSimilarly, the time to read the checkpoint data and perform a system restore, denoted $R_{r}$, consists of the same mechanical latency, $m$, plus the time to read the image of size $s$ at a read throughput of $b_{r}$. The read transfer time is $\\frac{s}{b_{r}}$. Thus, the total time for a restore operation is:\n$$R_{r} = m + \\frac{s}{b_{r}}$$\n\n### Task 2: Derivation of $D(\\tau)$ and $L(\\tau)$\n\nWe derive the long-run non-productive fraction of time, $D(\\tau)$, and the data loss risk, $L(\\tau)$, using a first-order analysis based on the assumption that failures are rare within a checkpoint interval, i.e., $\\lambda \\tau \\ll 1$. This allows us to consider the fractional overhead contributed by each non-productive activity. We analyze the system over a long arbitrary period of wall-clock time, $T_{total}$.\n\n1.  **Time spent checkpointing:** In the absence of failures, the system performs useful computation for a period $\\tau$ and then takes a checkpoint for a duration $C_{w}$. The duration of such a cycle is $\\tau + C_{w}$. The number of checkpoints taken during $T_{total}$ is approximately $\\frac{T_{total}}{\\tau + C_{w}}$. Since the overheads are assumed to be a small fraction of the total time, we can approximate the cycle length by $\\tau$, so the number of checkpoints is approximately $\\frac{T_{total}}{\\tau}$. The total time spent on checkpointing is this number multiplied by the duration of one checkpoint, $C_{w}$.\n    $$T_{checkpoint} \\approx \\frac{T_{total}}{\\tau} C_{w}$$\n    The fraction of time spent checkpointing is $\\frac{T_{checkpoint}}{T_{total}} = \\frac{C_{w}}{\\tau}$.\n\n2.  **Time spent restoring:** Failures occur as a Poisson process with rate $\\lambda$. Over the period $T_{total}$, the expected number of failures is $\\lambda T_{total}$. Each failure necessitates a restore operation, which takes $R_{r}$ seconds. The total time spent on restore operations is the number of failures multiplied by the duration of one restore.\n    $$T_{restore} = (\\lambda T_{total}) R_{r}$$\n    The fraction of time spent restoring is $\\frac{T_{restore}}{T_{total}} = \\lambda R_{r}$.\n\n3.  **Time spent re-executing lost work:** When a failure occurs, the computation performed since the last successful checkpoint is lost and must be re-executed. A failure is assumed to occur at a time $t_{f}$ uniformly distributed in the interval $[0, \\tau]$. The expected amount of lost computation time is therefore $E[t_{f}] = \\frac{\\tau}{2}$. The total time spent on re-executing lost work is the expected number of failures multiplied by the expected time lost per failure.\n    $$T_{re-execute} = (\\lambda T_{total}) \\left(\\frac{\\tau}{2}\\right)$$\n    The fraction of time spent on re-execution is $\\frac{T_{re-execute}}{T_{total}} = \\frac{\\lambda \\tau}{2}$.\n\nThe data loss risk, $L(\\tau)$, is defined as the fraction of time lost solely to re-execution. Based on the above derivation, we have:\n$$L(\\tau) = \\frac{\\lambda \\tau}{2}$$\n\nThe total non-productive fraction of time, $D(\\tau)$, is the sum of the fractions of time spent on all non-productive activities: checkpointing, restoring, and re-executing.\n$$D(\\tau) = \\frac{C_{w}}{\\tau} + \\lambda R_{r} + \\frac{\\lambda \\tau}{2}$$\nThis expression coheres with the sum of the individual overhead fractions and represents a standard first-order model for checkpointing overhead.\n\n### Task 3: Optimal Checkpoint Interval and Associated Overheads\n\nTo find the checkpoint interval $\\tau^{\\ast}$ that minimizes the total non-productive time fraction $D(\\tau)$, we must find the value of $\\tau$ that minimizes the expression derived in Task 2.\n$$D(\\tau) = \\frac{C_{w}}{\\tau} + \\frac{\\lambda \\tau}{2} + \\lambda R_{r}$$\nThe term $\\lambda R_{r}$ is constant with respect to $\\tau$, so we only need to minimize the $\\tau$-dependent part of the function. We find the minimum by taking the derivative of $D(\\tau)$ with respect to $\\tau$ and setting it to zero.\n$$\\frac{dD}{d\\tau} = \\frac{d}{d\\tau} \\left( C_{w}\\tau^{-1} + \\frac{\\lambda}{2}\\tau + \\lambda R_{r} \\right) = -C_{w}\\tau^{-2} + \\frac{\\lambda}{2}$$\nSetting the derivative to zero to find the critical point:\n$$-\\frac{C_{w}}{(\\tau^{\\ast})^2} + \\frac{\\lambda}{2} = 0$$\n$$\\frac{\\lambda}{2} = \\frac{C_{w}}{(\\tau^{\\ast})^2}$$\n$$(\\tau^{\\ast})^2 = \\frac{2 C_{w}}{\\lambda}$$\n$$\\tau^{\\ast} = \\sqrt{\\frac{2 C_{w}}{\\lambda}}$$\nTo confirm this is a minimum, we check the second derivative:\n$$\\frac{d^2D}{d\\tau^2} = \\frac{d}{d\\tau} \\left(-C_{w}\\tau^{-2}\\right) = 2C_{w}\\tau^{-3} = \\frac{2 C_{w}}{\\tau^3}$$\nSince $C_{w} > 0$ and $\\tau > 0$, the second derivative is positive, which confirms that $\\tau^{\\ast}$ corresponds to a local minimum.\n\nNow we evaluate $D(\\tau^{\\ast})$ and $L(\\tau^{\\ast})$ at this optimal interval.\nFirst, for $L(\\tau^{\\ast})$:\n$$L(\\tau^{\\ast}) = \\frac{\\lambda \\tau^{\\ast}}{2} = \\frac{\\lambda}{2} \\sqrt{\\frac{2 C_{w}}{\\lambda}} = \\frac{1}{2}\\sqrt{\\lambda^2 \\frac{2 C_{w}}{\\lambda}} = \\sqrt{\\frac{2 \\lambda^2 C_{w}}{4\\lambda}} = \\sqrt{\\frac{\\lambda C_{w}}{2}}$$\n\nNext, for $D(\\tau^{\\ast})$:\n$$D(\\tau^{\\ast}) = \\frac{C_{w}}{\\tau^{\\ast}} + \\frac{\\lambda \\tau^{\\ast}}{2} + \\lambda R_{r}$$\nWe substitute the expression for $\\tau^{\\ast}$ into the first term:\n$$\\frac{C_{w}}{\\tau^{\\ast}} = \\frac{C_{w}}{\\sqrt{\\frac{2 C_{w}}{\\lambda}}} = C_{w}\\sqrt{\\frac{\\lambda}{2 C_{w}}} = \\sqrt{\\frac{C_{w}^2\\lambda}{2 C_{w}}} = \\sqrt{\\frac{\\lambda C_{w}}{2}}$$\nThe first two terms of $D(\\tau^{\\ast})$ are equal at the optimum: $\\frac{C_{w}}{\\tau^{\\ast}} = L(\\tau^{\\ast}) = \\sqrt{\\frac{\\lambda C_{w}}{2}}$. This signifies that at the optimal interval, the time lost to checkpointing equals the time lost to re-execution.\n$$D(\\tau^{\\ast}) = \\sqrt{\\frac{\\lambda C_{w}}{2}} + \\sqrt{\\frac{\\lambda C_{w}}{2}} + \\lambda R_{r} = 2\\sqrt{\\frac{\\lambda C_{w}}{2}} + \\lambda R_{r} = \\sqrt{4\\frac{\\lambda C_{w}}{2}} + \\lambda R_{r} = \\sqrt{2\\lambda C_{w}} + \\lambda R_{r}$$\n\nFinally, we express these results in terms of the fundamental parameters $\\lambda$, $m$, $s$, $b_{w}$, and $b_{r}$ by substituting the expressions for $C_{w}$ and $R_{r}$ from Task 1.\n$$C_{w} = m + \\frac{s}{b_{w}}$$\n$$R_{r} = m + \\frac{s}{b_{r}}$$\n\nSubstituting for $C_{w}$ into the expressions for $D(\\tau^{\\ast})$ and $L(\\tau^{\\ast})$:\n$$L(\\tau^{\\ast}) = \\sqrt{\\frac{\\lambda}{2} \\left(m + \\frac{s}{b_{w}}\\right)}$$\n\nSubstituting for both $C_{w}$ and $R_{r}$ into the expression for $D(\\tau^{\\ast})$:\n$$D(\\tau^{\\ast}) = \\sqrt{2\\lambda \\left(m + \\frac{s}{b_{w}}\\right)} + \\lambda \\left(m + \\frac{s}{b_{r}}\\right)$$\n\nThe final answer is the two-element row matrix $[D(\\tau^{\\ast}), L(\\tau^{\\ast})]$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\sqrt{2 \\lambda \\left(m + \\frac{s}{b_{w}}\\right)} + \\lambda \\left(m + \\frac{s}{b_{r}}\\right)  \\sqrt{\\frac{\\lambda}{2} \\left(m + \\frac{s}{b_{w}}\\right)}\n\\end{pmatrix}\n}\n$$", "id": "3639697"}, {"introduction": "As operating systems and storage hardware evolved, a new, more subtle threat to data emerged: silent corruption. Simply protecting against system crashes wasn't enough if the underlying disk could silently alter bits without reporting an error. This practice demonstrates the profound evolutionary leap from legacy filesystems, which implicitly trusted the hardware, to modern systems that implement end-to-end data integrity. By applying basic probability theory [@problem_id:3639725], you will quantitatively discover just how dramatically checksums and replication reduce the risk of data loss, a principle that underpins a generation of highly reliable storage systems.", "problem": "Consider the historical evolution from legacy filesystems without end-to-end integrity to modern checksummed filesystems. In a legacy design, the Operating System (OS) does not maintain end-to-end checksums of file data; any silent corruption delivered by the storage device is not detected and the corrupted bytes are returned to applications. In a modern design, the filesystem stores a strong per-block checksum (assume perfect detection) and maintains two independent replicas of each block, such as in Redundant Array of Independent Disks (RAID) level 1 mirroring. If a block read fails checksum verification on one replica, the filesystem reads the other replica and returns correct data provided that at least one replica holds an uncorrupted copy.\n\nModel silent corruption as follows: each block read independently experiences silent corruption with probability $p$, and corruption events are independent across blocks and across replicas. Let the file size be 1 gibibyte (GiB), with blocks of size 4 kibibytes (KiB). Take $p = 1 \\times 10^{-9}$ per block read. Define end-to-end data loss for a single full-file read as the event that the application does not obtain correct data for at least one block, either because corrupted data is returned undetected (legacy design) or because corruption is detected but unrecoverable across both replicas (checksummed design).\n\nStarting only from the core definitions of independence of events, the complement rule, and basic probability multiplication, derive expressions for the end-to-end data loss probability for a single full-file read in both designs:\n- Legacy filesystem without checksums and without replication.\n- Checksummed filesystem with perfect detection and two independent replicas.\n\nThen evaluate the numerical values for the given parameters. Express both probabilities as decimals in standard scientific notation. Round your answers to $4$ significant figures. No percentage signs are permitted; give pure decimal values without units.", "solution": "The problem statement has been evaluated and is deemed valid. It is scientifically grounded, well-posed, and objective. All necessary parameters are provided, and the model, while simplified, adheres to logical and probabilistic principles relevant to computer system reliability.\n\nThe objective is to derive and calculate the end-to-end data loss probability for a single full-file read under two distinct filesystem designs. We begin by establishing the total number of blocks in the file.\n\nLet a gibibyte ($1$ GiB) be $2^{30}$ bytes and a kibibyte ($1$ KiB) be $2^{10}$ bytes. The file size is $1$ GiB, and the block size is $4$ KiB. The number of blocks, $N$, is the ratio of the file size to the block size:\n$$N = \\frac{\\text{File Size}}{\\text{Block Size}} = \\frac{1 \\text{ GiB}}{4 \\text{ KiB}} = \\frac{2^{30} \\text{ bytes}}{4 \\times 2^{10} \\text{ bytes}} = \\frac{2^{30}}{2^2 \\times 2^{10}} = \\frac{2^{30}}{2^{12}} = 2^{18}$$\nCalculating this value gives:\n$$N = 2^{18} = 262144$$\n\nWe are given that a silent corruption event for a single block read occurs with probability $p = 1 \\times 10^{-9}$. These events are independent for each block read.\n\n**Legacy Filesystem (No Checksums, No Replication)**\n\nIn the legacy design, data loss occurs if at least one block is read with silent corruption. The application receives the corrupted data because there are no checksums to detect the error.\n\nLet $P_L$ be the probability of end-to-end data loss for the legacy system. It is more straightforward to first calculate the probability of a successful file read, which is the complementary event. A full-file read is successful if and only if *all* $N$ blocks are read without corruption.\n\nThe probability that a single block is read correctly (i.e., without corruption) is $1-p$.\nSince all block read events are independent, the probability that all $N$ blocks are read correctly is the product of their individual success probabilities:\n$$P(\\text{success}) = (1-p) \\times (1-p) \\times \\dots \\times (1-p) \\quad (N \\text{ times})$$\n$$P(\\text{success}) = (1-p)^N$$\n\nThe event of data loss, $P_L$, is the complement of a successful read. Using the complement rule, $P(A) = 1 - P(A^c)$:\n$$P_L = 1 - P(\\text{success}) = 1 - (1-p)^N$$\n\nSubstituting the given values $N = 262144$ and $p = 1 \\times 10^{-9}$:\n$$P_L = 1 - (1 - 1 \\times 10^{-9})^{262144}$$\n\nFor a small value of $x$, the binomial approximation $(1-x)^n \\approx 1-nx$ can be used. Here, $p$ is very small, so $Np = (262144)(1 \\times 10^{-9}) = 2.62144 \\times 10^{-4}$ is also small. This justifies the approximation:\n$$P_L \\approx 1 - (1 - Np) = Np$$\n$$P_L \\approx 262144 \\times (1 \\times 10^{-9}) = 2.62144 \\times 10^{-4}$$\nRounding to $4$ significant figures, we get:\n$$P_L \\approx 2.621 \\times 10^{-4}$$\n\n**Checksummed Filesystem (Two Replicas)**\n\nIn the modern design, each block has two independent replicas. Data for a block is returned correctly as long as at least one of its replicas is uncorrupted. Data loss for a specific block occurs only if *both* replicas of that block are corrupted upon being read.\n\nLet $p$ be the probability of corruption for a read from a single replica. The reads from the two replicas are independent. The probability that both replicas of a single block are corrupted is:\n$$p_{\\text{block loss}} = p \\times p = p^2$$\nThe probability that a given block is recoverable (i.e., not a block loss event) is the complement:\n$$P(\\text{block recoverable}) = 1 - p^2$$\n\nFor a full-file read to be successful, all $N$ blocks must be recoverable. Since corruption events are independent across blocks, the probability of a successful full-file read is the product of the probabilities of each of the $N$ blocks being recoverable:\n$$P(\\text{success}) = (1 - p^2) \\times (1 - p^2) \\times \\dots \\times (1 - p^2) \\quad (N \\text{ times})$$\n$$P(\\text{success}) = (1 - p^2)^N$$\n\nLet $P_M$ be the probability of end-to-end data loss for the modern system. This is the complement of a successful read:\n$$P_M = 1 - P(\\text{success}) = 1 - (1 - p^2)^N$$\n\nSubstituting the values $N = 262144$ and $p = 1 \\times 10^{-9}$:\n$$p^2 = (1 \\times 10^{-9})^2 = 1 \\times 10^{-18}$$\n$$P_M = 1 - (1 - 1 \\times 10^{-18})^{262144}$$\n\nThe value of $p^2$ is exceedingly small. We can confidently use the binomial approximation $1 - (1-x)^N \\approx Nx$ where $x=p^2$:\n$$P_M \\approx N p^2$$\n$$P_M \\approx 262144 \\times (1 \\times 10^{-18}) = 2.62144 \\times 10^5 \\times 10^{-18} = 2.62144 \\times 10^{-13}$$\nRounding to $4$ significant figures yields:\n$$P_M \\approx 2.621 \\times 10^{-13}$$\n\nThe results demonstrate the profound improvement in data integrity offered by the combination of checksumming and replication. The probability of data loss is reduced by approximately nine orders of magnitude.", "answer": "$$\\boxed{\\begin{pmatrix} 2.621 \\times 10^{-4}  2.621 \\times 10^{-13} \\end{pmatrix}}$$", "id": "3639725"}, {"introduction": "The evolution of operating systems continues into the modern era, where complexity is a defining challenge. Understanding why a sophisticated, multi-layered system is slow or behaving unexpectedly requires deep visibility, or \"observability\". This practice explores the rise of powerful kernel tracing tools by modeling the fundamental trade-off they present: the immense value of the insights they provide versus the performance overhead they impose. By framing this as a calculus-based optimization problem [@problem_id:3639734], you can determine the optimal sampling rate that maximizes the net benefit, reflecting the sophisticated engineering decisions that go into designing modern, high-performance, and diagnosable operating systems.", "problem": "Modern operating systems evolved from coarse event logging to pervasive kernel tracing facilities to balance insight into system behavior with runtime overhead. Consider modeling this trade-off to explain the rise of kernel tracing tools such as DTrace and the Extended Berkeley Packet Filter (eBPF). Let the observability value be a concave function $V(s)$ of the sampling rate $s$ (in samples per second), capturing diminishing returns as more probes are added, and let the probe cost be a convex function $\\kappa(s)$ reflecting increasing overhead due to contention and cache effects. Specifically, suppose\n$$V(s) = \\alpha \\ln\\!\\big(1 + \\beta s\\big), \\quad \\kappa(s) = \\gamma s + \\delta s^{2},$$\nwith parameters $\\alpha = 100$, $\\beta = 5 \\times 10^{-3}$ $\\mathrm{s}$, $\\gamma = 0.1$, and $\\delta = 1 \\times 10^{-4}$. The net benefit is $F(s) = V(s) - \\kappa(s)$.\n\nUsing first principles of calculus-based optimization (marginal benefit equals marginal cost) and assuming $s \\ge 0$, determine the sampling rate $s^{\\star}$ that maximizes $F(s)$. Round your answer to $3$ significant figures and express the sampling rate in samples per second.", "solution": "The evolution toward pervasive kernel tracing can be explained by a formal trade-off between observability and overhead: instrumentation provides diminishing incremental insight as probe density grows, while overhead increases more than linearly due to shared resource contention. A canonical way to capture diminishing returns is a logarithmic utility and a canonical way to capture increasing overhead is a quadratic cost. We formalize the net benefit as\n$$F(s) = V(s) - \\kappa(s) = \\alpha \\ln\\!\\big(1 + \\beta s\\big) - \\gamma s - \\delta s^{2}.$$\nWe seek $s^{\\star} \\ge 0$ that maximizes $F(s)$. By the standard first-order optimality condition for differentiable concave objectives, the maximizer satisfies\n$$\\frac{d}{ds}F(s) = 0,$$\nand the second-order condition verifies that this stationary point is a maximum.\n\nCompute the derivative:\n$$\\frac{d}{ds}F(s) = \\frac{d}{ds}\\left[\\alpha \\ln\\!\\big(1 + \\beta s\\big)\\right] - \\frac{d}{ds}(\\gamma s) - \\frac{d}{ds}(\\delta s^{2}) = \\frac{\\alpha \\beta}{1 + \\beta s} - \\gamma - 2 \\delta s.$$\nSet the derivative to zero:\n$$\\frac{\\alpha \\beta}{1 + \\beta s} - \\gamma - 2 \\delta s = 0.$$\nRearrange to solve for $s$. Multiply both sides by $\\big(1 + \\beta s\\big)$ to clear the denominator:\n$$\\alpha \\beta = \\big(\\gamma + 2 \\delta s\\big)\\big(1 + \\beta s\\big).$$\nExpand the right-hand side:\n$$\\alpha \\beta = \\gamma + \\gamma \\beta s + 2 \\delta s + 2 \\delta \\beta s^{2}.$$\nCollect terms to obtain a quadratic equation in $s$:\n$$2 \\delta \\beta s^{2} + \\big(\\gamma \\beta + 2 \\delta\\big) s + \\gamma - \\alpha \\beta = 0.$$\nWith the given parameters $\\alpha = 100$, $\\beta = 5 \\times 10^{-3}$, $\\gamma = 0.1$, and $\\delta = 1 \\times 10^{-4}$, compute the coefficients:\n- $2 \\delta \\beta = 2 \\times \\big(1 \\times 10^{-4}\\big) \\times \\big(5 \\times 10^{-3}\\big) = 1 \\times 10^{-6}$,\n- $\\gamma \\beta + 2 \\delta = 0.1 \\times \\big(5 \\times 10^{-3}\\big) + 2 \\times \\big(1 \\times 10^{-4}\\big) = 5 \\times 10^{-4} + 2 \\times 10^{-4} = 7 \\times 10^{-4}$,\n- $\\gamma - \\alpha \\beta = 0.1 - 100 \\times \\big(5 \\times 10^{-3}\\big) = 0.1 - 0.5 = -0.4$.\n\nThus the quadratic is\n$$\\big(1 \\times 10^{-6}\\big) s^{2} + \\big(7 \\times 10^{-4}\\big) s - 0.4 = 0.$$\nSolve using the quadratic formula. Let $a = 1 \\times 10^{-6}$, $b = 7 \\times 10^{-4}$, and $c = -0.4$. The discriminant is\n$$\\Delta = b^{2} - 4 a c = \\big(7 \\times 10^{-4}\\big)^{2} - 4 \\times \\big(1 \\times 10^{-6}\\big) \\times \\big(-0.4\\big) = 4.9 \\times 10^{-7} + 1.6 \\times 10^{-6} = 2.09 \\times 10^{-6}.$$\nIts square root is\n$$\\sqrt{\\Delta} = \\sqrt{2.09 \\times 10^{-6}} \\approx 1.446664 \\times 10^{-3}.$$\nThe roots are\n$$s = \\frac{-b \\pm \\sqrt{\\Delta}}{2 a} = \\frac{-7 \\times 10^{-4} \\pm 1.446664 \\times 10^{-3}}{2 \\times 10^{-6}}.$$\nThe negative root is infeasible ($s  0$), so take the positive root:\n$$s^{\\star} = \\frac{-7 \\times 10^{-4} + 1.446664 \\times 10^{-3}}{2 \\times 10^{-6}} = \\frac{7.46664 \\times 10^{-4}}{2 \\times 10^{-6}} \\approx 3.73332 \\times 10^{2}.$$\nVerify the second-order condition:\n$$\\frac{d^{2}}{ds^{2}}F(s) = -\\frac{\\alpha \\beta^{2}}{\\big(1 + \\beta s\\big)^{2}} - 2 \\delta  0,$$\nfor all $s \\ge 0$, since $\\alpha  0$, $\\beta  0$, and $\\delta  0$. Therefore, the stationary point is indeed a global maximum on $s \\ge 0$.\n\nRound $s^{\\star}$ to $3$ significant figures:\n$$s^{\\star} \\approx 373.$$\nInterpretation in the operating systems evolution context: a concave $V(s)$ models the fact that early increases in probe density (as enabled by DTrace and later extended Berkeley Packet Filter) yield substantial insight, but each additional probe contributes less. A convex $\\kappa(s)$ models that overhead grows increasingly steeply with probe density, especially with naive instrumentation; advances such as extended Berkeley Packet Filter reduce $\\gamma$ and $\\delta$, shifting the optimum to higher $s^{\\star}$ while maintaining efficiency. For the provided parameters, the optimal sampling rate is $373$ samples per second.", "answer": "$$\\boxed{373}$$", "id": "3639734"}]}