{"hands_on_practices": [{"introduction": "A hallmark of a quality mobile operating system is a fluid and responsive user interface. This responsiveness hinges on the OS's ability to process events and update the display with minimal delay. This practice delves into the critical path of failure propagation in an Android-like system, modeling the journey from a low-level Inter-Process Communication (IPC) event to a visual change on the screen. By calculating the total time to propagate a failure and its impact on the user experience, you will gain a concrete understanding of the interplay between IPC mechanisms, thread scheduling, message queues, and display refresh cycles that governs perceived performance [@problem_id:3646046].", "problem": "You are analyzing failure propagation in an Android-like handheld operating system that uses Binder Inter-Process Communication (IPC). In this system, clients register Binder death recipients to be notified when a remote service process dies. When a remote service process is killed, the kernel driver marks its Binder node as dead and enqueues a death notification to each registered recipient in the client processes. A client application processes this notification on a Binder thread, which posts a message to its main thread. The main thread uses an event loop (Looper) to process messages sequentially and renders visual changes at discrete display refresh boundaries (vertical synchronization) at a fixed refresh rate.\n\nConsider a single client application with the following characteristics at the moment the remote service process is killed at time $t=0$:\n- The kernel’s Binder driver posts the death notification to the client process in $t_{d} = 0.2$ milliseconds.\n- A Binder thread in the client takes $t_{p} = 0.3$ milliseconds to run the death-recipient callback and post a message to the main thread’s queue. Assume there are sufficient Binder threads so that all death recipients in this client post essentially simultaneously at time $t=t_{d}+t_{p}$.\n- The client application’s main thread has a backlog of $2$ messages taking $2$ and $1$ milliseconds, respectively, to process, for a total of $t_{q}=3$ milliseconds of work from $t=0$ onward, processed continuously.\n- The client has $m=3$ user experience (UX) components that independently registered as Binder death recipients. Each component posts a main-thread message that, when processed, updates the component’s state to reflect the failure. Each such update requires a deterministic $t_{u}=7$ milliseconds of main-thread compute time. The main thread processes these three update messages sequentially after the existing backlog, in any order.\n- The display refreshes at a constant rate of $60$ hertz, so the refresh interval is $T_{v}=\\frac{50}{3}$ milliseconds. The time from $t=0$ until the next refresh boundary is $t_{\\text{next}}=\\frac{10}{3}$ milliseconds. A UI change completed by the main thread becomes visible at the first display refresh boundary strictly after the change completes.\n\nDefine the time to propagate failure, $T_{f}$, as the elapsed time from $t=0$ until the first visible indication of failure appears in the application’s user interface. Define the impact on UX components, $I$, as the total number of display frames missed across all $m$ components before each component’s update becomes visible, where a “missed frame” for a component is counted as one for each refresh interval boundary that passes after $t=0$ and before the component’s update becomes visible.\n\nCompute $T_{f}$ and $I$ under the assumptions above. Express $T_{f}$ in milliseconds and $I$ as a dimensionless count. No rounding is required; provide exact values. Return your final answer as a two-entry row vector $\\left(T_{f}, I\\right)$ as specified.", "solution": "The problem is well-posed and scientifically grounded in the principles of operating systems and event-driven programming. All necessary parameters are provided, and the definitions are unambiguous. We can proceed with a step-by-step calculation.\n\nFirst, we establish the timeline of events leading to the processing of failure notifications on the application's main thread. The remote service is killed at time $t=0$.\n\n1.  **Time of Message Posting to Main Thread:**\n    The kernel's Binder driver posts the death notification to the client process in $t_{d} = 0.2$ milliseconds. A Binder thread in the client then takes $t_{p} = 0.3$ milliseconds to execute its callback and post a message to the main thread's message queue. Therefore, the death notification messages from the $m=3$ UX components are all enqueued on the main thread at time $t_{\\text{post}}$:\n    $$t_{\\text{post}} = t_{d} + t_{p} = 0.2 + 0.3 = 0.5 \\text{ ms}$$\n\n2.  **Main Thread Work Timeline:**\n    At $t=0$, the main thread has a backlog of work that takes $t_{q}=3$ milliseconds to complete. Since it processes this work continuously from $t=0$, the backlog will be cleared at $t=3$ ms. The messages posted at $t_{\\text{post}}=0.5$ ms must wait in the queue until the main thread is free.\n    The main thread will start processing the first of the three death notification messages at time $t_{\\text{start,1}}$:\n    $$t_{\\text{start,1}} = t_{q} = 3 \\text{ ms}$$\n    Each of these messages requires a compute time of $t_{u}=7$ milliseconds. The problem states they are processed sequentially in any order. For the first failure propagation, the order does not matter. The first message processing will be completed at $t_{\\text{complete,1}}$:\n    $$t_{\\text{complete,1}} = t_{\\text{start,1}} + t_{u} = 3 + 7 = 10 \\text{ ms}$$\n\n3.  **Display Refresh Timeline:**\n    The display refreshes at a constant rate of $60$ hertz. The refresh interval, $T_{v}$, is:\n    $$T_{v} = \\frac{1 \\text{ s}}{60} \\times \\frac{1000 \\text{ ms}}{1 \\text{ s}} = \\frac{1000}{60} \\text{ ms} = \\frac{50}{3} \\text{ ms}$$\n    The problem states that the time from $t=0$ until the next refresh boundary is $t_{\\text{next}}=\\frac{10}{3}$ milliseconds. Subsequent refresh boundaries occur at intervals of $T_{v}$. The time of the $k$-th refresh boundary after $t=0$ (for $k=0, 1, 2, \\dots$) is given by $t_{\\text{refresh},k}$:\n    $$t_{\\text{refresh},k} = t_{\\text{next}} + k \\cdot T_{v} = \\frac{10}{3} + k \\frac{50}{3} = \\frac{10 + 50k}{3} \\text{ ms}$$\n    The first few refresh boundaries are:\n    $t_{\\text{refresh},0} = \\frac{10}{3} \\approx 3.33$ ms\n    $t_{\\text{refresh},1} = \\frac{10 + 50}{3} = \\frac{60}{3} = 20$ ms\n    $t_{\\text{refresh},2} = \\frac{10 + 100}{3} = \\frac{110}{3} \\approx 36.67$ ms\n    $t_{\\text{refresh},3} = \\frac{10 + 150}{3} = \\frac{160}{3} \\approx 53.33$ ms\n\n4.  **Compute Time to Propagate Failure, $T_{f}$:**\n    $T_{f}$ is the time until the first visible indication of failure. A UI change becomes visible at the first display refresh boundary strictly after the change is completed by the main thread. The first UI change is completed at $t_{\\text{complete,1}} = 10$ ms. We must find the smallest $t_{\\text{refresh},k}$ such that $t_{\\text{refresh},k} > 10$ ms.\n    $t_{\\text{refresh},0} = \\frac{10}{3} < 10$ ms.\n    $t_{\\text{refresh},1} = 20 > 10$ ms.\n    Therefore, the first update becomes visible at $t=20$ ms.\n    $$T_{f} = 20 \\text{ ms}$$\n\n5.  **Compute Impact on UX Components, $I$:**\n    To compute $I$, we need the visibility time for each of the $m=3$ components. We first find the completion times for all three updates. Since they are processed sequentially, and each takes $t_{u}=7$ ms, their completion times are:\n    $$t_{\\text{complete,1}} = 3 + 7 = 10 \\text{ ms}$$\n    $$t_{\\text{complete,2}} = t_{\\text{complete,1}} + t_{u} = 10 + 7 = 17 \\text{ ms}$$\n    $$t_{\\text{complete,3}} = t_{\\text{complete,2}} + t_{u} = 17 + 7 = 24 \\text{ ms}$$\n    Note that this set of completion times is independent of the order of processing.\n\n    Next, we find the visibility time for each component's update, which is the first refresh boundary strictly after its completion time.\n    - Component 1 (completes at $10$ ms): Visible at $t_{\\text{vis,1}} = t_{\\text{refresh},1} = 20$ ms.\n    - Component 2 (completes at $17$ ms): Visible at $t_{\\text{vis,2}} = t_{\\text{refresh},1} = 20$ ms (since $17 < 20$).\n    - Component 3 (completes at $24$ ms): Visible at $t_{\\text{vis,3}} = t_{\\text{refresh},2} = \\frac{110}{3}$ ms (since $20 < 24 < 110/3$).\n\n    The impact $I$ is the total number of frames missed across all components. A missed frame is a refresh boundary that passes before an update becomes visible.\n    The number of missed frames for component $i$, $I_i$, is the count of boundaries $t_{\\text{refresh},k}$ such that $t_{\\text{refresh},k} < t_{\\text{vis},i}$.\n\n    - For Component 1: $t_{\\text{vis,1}} = 20$ ms. The boundaries before $20$ ms are $\\{t_{\\text{refresh},0} = 10/3 \\text{ ms}\\}$. So, $I_1 = 1$.\n    - For Component 2: $t_{\\text{vis,2}} = 20$ ms. The boundaries before $20$ ms are also $\\{t_{\\text{refresh},0} = 10/3 \\text{ ms}\\}$. So, $I_2 = 1$.\n    - For Component 3: $t_{\\text{vis,3}} = \\frac{110}{3}$ ms. The boundaries before $\\frac{110}{3}$ ms are $\\{t_{\\text{refresh},0} = 10/3 \\text{ ms}, t_{\\text{refresh},1} = 20 \\text{ ms}\\}$. So, $I_3 = 2$.\n\n    The total impact $I$ is the sum of the individual impacts:\n    $$I = I_1 + I_2 + I_3 = 1 + 1 + 2 = 4$$\n\nThe computed values are $T_{f} = 20$ ms and $I=4$.\nThe required output is a two-entry row vector $(T_f, I)$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n20 & 4\n\\end{pmatrix}\n}\n$$", "id": "3646046"}, {"introduction": "Mobile and wearable devices operate under a fundamental constraint: finite battery life. A key responsibility of a mobile OS is to manage power consumption aggressively without compromising essential functionality like connectivity. This exercise puts you in the role of an OS designer optimizing a Bluetooth Low Energy (BLE) connection, a common scenario for wearables and IoT devices. You will use first principles to derive a power model and determine the optimal connection interval that minimizes energy use while satisfying critical constraints on data throughput and latency, illustrating the real-world trade-offs inherent in mobile system design [@problem_id:3646045].", "problem": "A wearable sensor uses Bluetooth Low Energy (BLE) to maintain a connection with a smartphone. The wearable’s operating system schedules a connection event every connection interval of length $\\tau$, during which it sends exactly one notification containing application data, and then returns to deep sleep until the next event. Assume the following scientifically grounded facts and parameters:\n\n- BLE connection events repeat with period $\\tau$ and the worst-case one-way application-layer latency equals the next-event wait, which is $\\tau$ (ignore host and controller processing delays relative to radio timing).\n- Average power over a periodic schedule equals total energy spent in one period divided by the period.\n- Each connection event has a fixed active-phase energy cost $E_{\\mathrm{act}}$ and duration $t_{\\mathrm{act}}$ that do not depend on $\\tau$, and the device otherwise sleeps at power $P_{\\mathrm{slp}}$ during the remainder of the interval.\n- BLE requires the connection interval $\\tau$ to be an integer multiple of $1.25~\\mathrm{ms}$ and to lie within $[7.5~\\mathrm{ms}, 4~\\mathrm{s}]$.\n\nThe wearable must stream sensor data at a sustained application data rate $R$ using one notification per connection event. Each notification carries an application payload of $M$ bytes. To meet the data rate, the achieved throughput per second must be at least $R$. To meet interactivity constraints, the worst-case one-way latency must not exceed $L_0$.\n\nGiven:\n- $E_{\\mathrm{act}} = 60\\ \\mathrm{\\mu J}$, $t_{\\mathrm{act}} = 2.5\\ \\mathrm{ms}$, $P_{\\mathrm{slp}} = 9\\ \\mathrm{\\mu W}$,\n- $M = 27\\ \\mathrm{bytes}$, $R = 750\\ \\mathrm{bytes/s}$,\n- $L_0 = 34\\ \\mathrm{ms}$,\n\nuse first principles (definitions of average power over a period, BLE timing, and basic throughput) to determine the value of the connection interval $\\tau^{\\star}$ that minimizes the average power consumption while satisfying both the latency constraint and the data-rate feasibility and quantization constraints. Express your final $\\tau^{\\star}$ in milliseconds. Do not include any units in your final boxed answer.", "solution": "We start from the definition of average power over a periodic schedule. If a system repeats a cycle of duration $\\tau$ and spends total energy $E_{\\mathrm{tot}}(\\tau)$ in that cycle, the time-averaged power is\n$$\nP(\\tau) = \\frac{E_{\\mathrm{tot}}(\\tau)}{\\tau}.\n$$\nFor the BLE-connected wearable, in each interval of length $\\tau$, there is an active radio phase of fixed duration $t_{\\mathrm{act}}$ and fixed energy cost $E_{\\mathrm{act}}$ that are independent of $\\tau$, plus a sleep phase lasting $\\tau - t_{\\mathrm{act}}$ at power $P_{\\mathrm{slp}}$. The total energy per interval is therefore\n$$\nE_{\\mathrm{tot}}(\\tau) = E_{\\mathrm{act}} + P_{\\mathrm{slp}}\\big(\\tau - t_{\\mathrm{act}}\\big).\n$$\nSubstituting into the average power expression yields\n$$\nP(\\tau) = \\frac{E_{\\mathrm{act}} + P_{\\mathrm{slp}}(\\tau - t_{\\mathrm{act}})}{\\tau}\n= P_{\\mathrm{slp}} + \\frac{E_{\\mathrm{act}} - P_{\\mathrm{slp}}\\,t_{\\mathrm{act}}}{\\tau}.\n$$\nBecause $E_{\\mathrm{act}} > 0$, $P_{\\mathrm{slp}} > 0$, and $t_{\\mathrm{act}} > 0$, the numerator $E_{\\mathrm{act}} - P_{\\mathrm{slp}}\\,t_{\\mathrm{act}}$ is nonnegative in realistic systems. In the given parameters, $E_{\\mathrm{act}} - P_{\\mathrm{slp}}\\,t_{\\mathrm{act}} = 60\\times 10^{-6}\\ \\mathrm{J} - 9\\times 10^{-6}\\ \\mathrm{W}\\cdot 2.5\\times 10^{-3}\\ \\mathrm{s} = 60\\times 10^{-6}\\ \\mathrm{J} - 22.5\\times 10^{-9}\\ \\mathrm{J} > 0$. Hence\n$$\n\\frac{dP}{d\\tau} = -\\frac{E_{\\mathrm{act}} - P_{\\mathrm{slp}}\\,t_{\\mathrm{act}}}{\\tau^{2}} < 0 \\quad \\text{for } \\tau > 0,\n$$\nwhich shows that $P(\\tau)$ is strictly decreasing in $\\tau$ over the feasible domain. Therefore, to minimize average power, we should choose $\\tau$ as large as permitted by the constraints.\n\nWe now formalize the constraints:\n\n1. Latency constraint. Worst-case one-way latency equals the wait until the next connection event, which is $L = \\tau$. The constraint $L \\le L_0$ therefore implies\n$$\n\\tau \\le L_0.\n$$\n\n2. Throughput constraint. One notification per event carries $M$ bytes every $\\tau$ seconds, so the achieved application throughput is\n$$\n\\frac{M}{\\tau}\\ \\mathrm{bytes/s}.\n$$\nTo meet the required rate $R$, we must have\n$$\n\\frac{M}{\\tau} \\ge R \\quad \\Longleftrightarrow \\quad \\tau \\le \\frac{M}{R}.\n$$\n\n3. BLE quantization and range. The BLE connection interval must satisfy\n$$\n\\tau \\in \\{k \\cdot 1.25\\ \\mathrm{ms} \\mid k \\in \\mathbb{Z},\\ 7.5\\ \\mathrm{ms} \\le \\tau \\le 4\\ \\mathrm{s}\\}.\n$$\n\nBecause $P(\\tau)$ decreases with $\\tau$, the optimal $\\tau^{\\star}$ is the largest BLE-permitted value that satisfies both upper bounds from items 1 and 2. Thus, before quantization,\n$$\n\\tau_{\\max} = \\min\\!\\left(L_0,\\ \\frac{M}{R}\\right).\n$$\nWe then quantize $\\tau_{\\max}$ down to the nearest allowed multiple of $1.25\\ \\mathrm{ms}$ that lies within $[7.5\\ \\mathrm{ms}, 4\\ \\mathrm{s}]$.\n\nSubstitute the given values. Compute the throughput-implied upper bound:\n$$\n\\frac{M}{R} = \\frac{27\\ \\mathrm{bytes}}{750\\ \\mathrm{bytes/s}} = 0.036\\ \\mathrm{s} = 36\\ \\mathrm{ms}.\n$$\nThe latency-implied upper bound is\n$$\nL_0 = 34\\ \\mathrm{ms}.\n$$\nTherefore,\n$$\n\\tau_{\\max} = \\min\\!\\big(34\\ \\mathrm{ms},\\ 36\\ \\mathrm{ms}\\big) = 34\\ \\mathrm{ms}.\n$$\nQuantize $34\\ \\mathrm{ms}$ down to the nearest allowed multiple of $1.25\\ \\mathrm{ms}$. Let the quantization step be $\\Delta = 1.25\\ \\mathrm{ms}$. Compute\n$$\nk = \\left\\lfloor \\frac{34\\ \\mathrm{ms}}{1.25\\ \\mathrm{ms}} \\right\\rfloor = \\left\\lfloor 27.2 \\right\\rfloor = 27,\n$$\nso the largest allowed multiple not exceeding $34\\ \\mathrm{ms}$ is\n$$\n\\tau^{\\star} = k \\Delta = 27 \\times 1.25\\ \\mathrm{ms} = 33.75\\ \\mathrm{ms}.\n$$\nThis value lies within the BLE-allowed range $[7.5\\ \\mathrm{ms}, 4\\ \\mathrm{s}]$ and satisfies both the latency and throughput constraints. Since $P(\\tau)$ is strictly decreasing in $\\tau$, this $\\tau^{\\star}$ minimizes the average power subject to the constraints.", "answer": "$$\\boxed{33.75}$$", "id": "3646045"}, {"introduction": "For a mobile application to be reliable, its data must remain consistent even in the face of unexpected crashes or power failures. This problem challenges you to design a transactional update mechanism for application settings, a common source of data corruption that can lead to frustrating crash loops. By evaluating strategies like Write-Ahead Logging (WAL) and analyzing their recovery-time performance, you will explore core database principles of atomicity and durability as they apply in a resource-constrained mobile environment. This design-oriented exercise provides insight into how a robust OS ensures data integrity and prevents application fragility [@problem_id:3646008].", "problem": "A smartphone application persists user configuration in a Shared Preferences (SP) file stored on flash-based Non-Volatile Memory (NVM). The SP content is a small text representation with maximum size $S_{\\max}$. The application sometimes performs multiple preference updates in quick succession and then crashes. Upon restart, it immediately reads the SP file to reconstruct state and may crash again if the SP content is corrupted, causing a crash loop. Assume the operating system’s failure model is that a crash can occur at any time, an incomplete write can result in torn pages, a file system rename operation is atomic, and a successful explicit flush of a file descriptor makes data durable.\n\nYou must design a transactional update mechanism for SP that ensures all-or-nothing atomicity and a bounded rollback time $T_b$ during recovery after a crash. Atomicity requires that, if a crash occurs, the system either presents the pre-update state or the entire post-update state, never a mixture. A bounded rollback time requires that the worst-case recovery time be upper-bounded by a function of the system’s parameters that does not grow without limit during normal operations.\n\nConsider the following parameters and constraints, which are typical of handheld devices:\n\n- Maximum SP size $S_{\\max} = 256\\,\\mathrm{KiB}$.\n- Write-Ahead Logging (WAL) with at most $N_{\\max}$ pending transactions is permitted, where $N_{\\max} = 64$.\n- Each log entry payload size $e = 2\\,\\mathrm{KiB}$, with a per-entry Cyclic Redundancy Check (CRC) to detect corruption.\n- Sequential read throughput $R_r = 200\\,\\mathrm{MiB/s}$, sequential write throughput $R_w = 100\\,\\mathrm{MiB/s}$.\n- Flush latency per explicit flush $f = 3\\,\\mathrm{ms}$.\n- CRC computation time per entry $t_{\\mathrm{crc}} = 0.05\\,\\mathrm{ms}$.\n- Input/Output (I/O) during recovery is sequential (log scanning and writing a reconstructed SP file), and recovery performs the minimal number of explicit flushes required to ensure that the reconstructed consistent SP becomes durable.\n\nWhich option correctly specifies a transactional design that prevents crash loops caused by corrupted SP, guarantees atomicity under the stated failure model, and yields a correct upper bound $T_b$ on worst-case rollback time under recovery using the given parameters?\n\nA. Append-only WAL with per-entry header, payload, and CRC. For each transaction: append the entry, explicitly flush the WAL, then atomically record a commit marker via rename of a small metadata file. On recovery: scan the WAL until the last valid CRC; apply up to $N_{\\max}$ entries to reconstruct a new SP file; explicitly flush the reconstructed SP and perform a minimal metadata flush to make it durable. This design guarantees atomicity because the commit marker only appears after durable WAL append, and prevents crash loops by rejecting corrupted entries via CRC and falling back to the last complete state. The worst-case bound is\n$$\nT_b \\le \\underbrace{\\frac{N_{\\max} e}{R_r}}_{\\text{scan log}} + \\underbrace{N_{\\max} t_{\\mathrm{crc}}}_{\\text{CRC}} + \\underbrace{\\frac{S_{\\max}}{R_w}}_{\\text{write SP}} + \\underbrace{2 f}_{\\text{flushes}} \\,,\n$$\nwhich, with the given values, evaluates to $T_b \\le 12.325\\,\\mathrm{ms}$.\n\nB. In-place updates to the SP file with periodic explicit flushes relying on the file system’s journaling for consistency. On recovery: read the SP file directly and parse. Atomicity is guaranteed by journaling, and rollback time is negligible with $T_b \\approx 0\\,\\mathrm{ms}$ because no extra processing is needed.\n\nC. Append-only WAL with CRC and explicit flush after each append, but with no subsequent reconstruction write. On recovery: scan the WAL and accept the last valid CRC as the committed state without writing a new SP file. The worst-case bound is\n$$\nT_b \\approx \\frac{N_{\\max} e}{R_r} + N_{\\max} t_{\\mathrm{crc}} = 3.825\\,\\mathrm{ms} \\,,\n$$\nbecause only scanning and CRC computation are needed.\n\nD. Double-buffered full-file writes with two SP versions: write a new full SP file, then atomically rename it over the old one without explicit flushes of the new file or directory metadata. On recovery: select the most recent SP by modification time and read it. The worst-case bound is\n$$\nT_b \\le \\frac{S_{\\max}}{R_r} = 1.25\\,\\mathrm{ms} \\,,\n$$\nsince only one file needs to be read and no reconstruction is required.", "solution": "This problem requires evaluating different transactional designs for atomicity and bounded recovery time.\n\n**A. Append-only WAL with commit marker and reconstruction**\n- **Design Analysis**: This option describes a standard and robust Write-Ahead Logging (WAL) protocol. Atomicity is ensured by the sequence: 1) write the change to a log, 2) flush the log to make it durable, and only then 3) mark the transaction as committed. On recovery, the system scans the log, uses CRCs to find all valid, committed entries, and replays them to reconstruct a consistent version of the SP file. This prevents reading a partially written (corrupted) file and thus solves the crash-loop problem. The rollback time is bounded because the log size is capped at $N_{\\max}$ entries.\n- **Rollback Time Calculation**: The worst-case recovery time $T_b$ consists of scanning the full log, checking all CRCs, writing a new SP file, and making the result durable with flushes.\n  - Log scan time: $\\frac{N_{\\max} e}{R_r} = \\frac{64 \\times 2~\\text{KiB}}{200~\\text{MiB/s}} = \\frac{128~\\text{KiB}}{200~\\text{MiB/s}} = \\frac{0.125~\\text{MiB}}{200~\\text{MiB/s}} = 0.000625~\\text{s} = 0.625~\\text{ms}$.\n  - CRC check time: $N_{\\max} t_{\\mathrm{crc}} = 64 \\times 0.05~\\text{ms} = 3.2~\\text{ms}$.\n  - SP write time: $\\frac{S_{\\max}}{R_w} = \\frac{256~\\text{KiB}}{100~\\text{MiB/s}} = \\frac{0.25~\\text{MiB}}{100~\\text{MiB/s}} = 0.0025~\\text{s} = 2.5~\\text{ms}$.\n  - Flush time: A minimal recovery requires flushing the newly written SP file (1 flush) and then flushing the metadata update that makes it the primary version (1 flush). Total: $2f = 2 \\times 3~\\text{ms} = 6~\\text{ms}$.\n  - Total $T_b \\le 0.625 + 3.2 + 2.5 + 6.0 = 12.325~\\text{ms}$.\n- **Verdict**: **Correct**. The design is sound, guarantees atomicity, provides bounded recovery, and the calculation is accurate.\n\n**B. In-place updates to the SP file with periodic explicit flushes**\n- **Design Analysis**: This design is flawed because it relies on file system journaling to provide application-level data atomicity, which is a common misconception. Standard file system journaling ensures metadata consistency but typically does not prevent \"torn writes\" within a file's data blocks during a crash. A crash mid-write could leave the SP file in an inconsistent, unparsable state, failing the atomicity guarantee and leading directly to the crash loop problem.\n- **Verdict**: **Incorrect**. This design fails to guarantee atomicity.\n\n**C. Append-only WAL with no subsequent reconstruction write**\n- **Design Analysis**: This approach correctly uses a WAL but misunderstands its application. The WAL contains a sequence of updates, not the full state itself. To reconstruct the state, one must apply these updates to a base version. The suggestion to \"accept the last valid CRC as the committed state\" is conceptually wrong; the last log entry is a single change, not the entire configuration. This fails to produce a consistent, complete SP file for the application to read.\n- **Verdict**: **Incorrect**. The recovery procedure is flawed and does not correctly reconstruct the atomic state.\n\n**D. Double-buffered full-file writes without explicit flushes**\n- **Design Analysis**: This \"write-then-rename\" pattern can provide atomicity, but *only* if implemented correctly with explicit flushes. The problem states this design operates \"without explicit flushes.\" The `rename` system call is only atomic at the metadata level. Without flushing, a crash could occur after the rename operation returns but before the new file's data and the directory's updated state are durable on NVM. This could leave the file system pointing the SP filename to a partially written or empty file, violating atomicity.\n- **Verdict**: **Incorrect**. The lack of explicit flushes makes the atomicity guarantee void under the specified failure model.", "answer": "$$\\boxed{A}$$", "id": "3646008"}]}