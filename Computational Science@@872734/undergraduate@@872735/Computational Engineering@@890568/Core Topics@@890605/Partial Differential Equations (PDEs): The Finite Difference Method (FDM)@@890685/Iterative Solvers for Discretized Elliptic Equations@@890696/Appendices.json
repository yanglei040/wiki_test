{"hands_on_practices": [{"introduction": "When discretizing partial differential equations on fine grids, the resulting system matrix $A$ can become too large to store in memory. This practice introduces the essential \"matrix-free\" paradigm, where the action of the matrix on a vector, $v \\mapsto Av$, is implemented as a function without ever forming the matrix itself. You will build a matrix-free implementation of the Conjugate Gradient (CG) method, a cornerstone algorithm for solving large, sparse, symmetric positive-definite systems that arise from problems like the Poisson equation [@problem_id:2406207].", "problem": "Consider the linear system arising from the standard five-point finite difference discretization of the Dirichlet problem for the Poisson equation on the unit square. Let the continuous problem be given by $-\\Delta u = f$ on $(0,1)\\times(0,1)$ with $u=0$ on the boundary. For a uniform grid with $n$ interior points per coordinate direction, the grid spacing is $h = \\frac{1}{n+1}$, and the unknowns can be ordered lexicographically into a vector in $\\mathbb{R}^{n^2}$. Define the linear operator $A:\\mathbb{R}^{n^2}\\to\\mathbb{R}^{n^2}$ corresponding to the discrete negative Laplacian with homogeneous Dirichlet boundary conditions by the standard five-point stencil: for each interior grid point with indices $(i,j)$ (where $i\\in\\{1,\\dots,n\\}$ and $j\\in\\{1,\\dots,n\\}$), the action $(A u)_{i,j}$ is\n$$(A u)_{i,j} \\;=\\; \\frac{1}{h^2}\\left(4\\,u_{i,j} - u_{i-1,j} - u_{i+1,j} - u_{i,j-1} - u_{i,j+1}\\right),$$\nwith the convention that $u_{0,j}=u_{n+1,j}=u_{i,0}=u_{i,n+1}=0$ due to the Dirichlet boundary conditions. The vector form of $A u$ is obtained by flattening the two-dimensional array $u_{i,j}$ to a one-dimensional array in row-major order. The operator $A$ is symmetric positive definite.\n\nYour task is to write a complete, runnable program that, for a set of test cases, solves $A u = b$ using a matrix-free implementation of the Conjugate Gradient (CG) method, where the only access to $A$ is through a function that computes $v \\mapsto A v$; no explicit sparse or dense matrix representation of $A$ is allowed. Use the initial guess $u^{(0)}=0$ and terminate when the relative residual norm satisfies\n$$\\frac{\\|r^{(k)}\\|_2}{\\|b\\|_2} \\le \\mathrm{tol},$$\nor when the iteration count exceeds a prescribed maximum. Angles for any trigonometric functions must be in radians.\n\nTo make the right-hand side compatible with a known exact discrete solution, use the manufactured discrete solution\n$$u_{i,j}^{\\star} \\;=\\; \\sin(\\pi x_i)\\,\\sin(\\pi y_j), \\quad x_i = i h,\\; y_j = j h,$$\nand define $b := A u^{\\star}$ by applying the operator $A$ to $u^{\\star}$ as specified above. For each test case, compute the approximate solution $u$ by the matrix-free solver and then verify both of the following:\n- The final relative residual is at most the specified tolerance $\\mathrm{tol}$.\n- The relative $2$-norm error with respect to $u^{\\star}$ satisfies\n$$\\frac{\\|u - u^{\\star}\\|_2}{\\|u^{\\star}\\|_2} \\le 100\\,\\mathrm{tol}.$$\n\nTest Suite:\nEvaluate your program on the following parameter pairs $(n,\\mathrm{tol})$:\n- $(1,\\,10^{-12})$,\n- $(8,\\,10^{-10})$,\n- $(32,\\,10^{-8})$,\n- $(64,\\,10^{-8})$.\n\nFor each test case, return a boolean indicating whether both verification conditions above are satisfied. Use a maximum iteration count equal to $n^2$ for each case.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases listed above. For example, if all test cases pass, the output must be exactly\n\"[True,True,True,True]\".", "solution": "The problem statement is found to be valid. It is scientifically grounded, well-posed, and objective. It presents a standard problem in computational science: solving a linear system derived from a finite difference discretization of a partial differential equation using an iterative method. All parameters and conditions are specified with sufficient clarity to permit a unique and verifiable solution.\n\nThe task is to solve the linear system of equations $A u = b$ using a matrix-free implementation of the Conjugate Gradient (CG) method. This system arises from the five-point finite difference discretization of the Poisson equation, $-\\Delta u = f$, on the unit square $\\Omega = (0,1) \\times (0,1)$ with homogeneous Dirichlet boundary conditions, $u=0$ on $\\partial\\Omega$.\n\nFor a uniform grid with $n$ interior points in each coordinate direction, the grid spacing is $h = \\frac{1}{n+1}$. The discrete operator $A$ represents the negative Laplacian, mapping a function on the grid points to another function. Its action on a grid function $u$, at an interior point $(i,j)$ where $i,j \\in \\{1, \\ldots, n\\}$, is given by the five-point stencil:\n$$ (A u)_{i,j} = \\frac{1}{h^2} \\left( 4u_{i,j} - u_{i-1,j} - u_{i+1,j} - u_{i,j-1} - u_{i,j+1} \\right). $$\nThe boundary conditions $u=0$ are incorporated by setting $u_{i,j}=0$ if $i$ or $j$ is $0$ or $n+1$. The resulting operator $A$ is symmetric and positive definite (SPD), which is a necessary condition for the convergence of the Conjugate Gradient method.\n\nThe problem is constructed using the Method of Manufactured Solutions to provide a case with a known exact discrete solution. The designated exact solution is:\n$$ u_{i,j}^{\\star} = \\sin(\\pi x_i) \\sin(\\pi y_j), \\quad \\text{where } x_i = i h \\text{ and } y_j = j h. $$\nThe right-hand side vector $b$ is then defined as $b = A u^{\\star}$, which is computed by applying the discrete operator $A$ to the known solution $u^{\\star}$.\n\nThe core of the solution is the Conjugate Gradient algorithm. The algorithm is iterative and is particularly suitable for large, sparse systems where the matrix $A$ is SPD. A key requirement here is that the implementation must be \"matrix-free,\" meaning the matrix $A$ is never explicitly constructed or stored. Instead, its action $v \\mapsto Av$ is provided by a function.\n\nThe algorithm proceeds as follows:\n$1$. Initialize the solution vector $u^{(0)} = 0$.\n$2$. Compute the initial residual $r^{(0)} = b - A u^{(0)} = b$.\n$3$. Set the initial search direction $p^{(0)} = r^{(0)}$.\n$4$. For $k = 0, 1, 2, \\dots$ until convergence:\n    a. Compute the matrix-vector product $v^{(k)} = A p^{(k)}$.\n    b. Compute the step size $\\alpha_k = \\frac{r^{(k)T} r^{(k)}}{p^{(k)T} v^{(k)}}$.\n    c. Update the solution: $u^{(k+1)} = u^{(k)} + \\alpha_k p^{(k)}$.\n    d. Update the residual: $r^{(k+1)} = r^{(k)} - \\alpha_k v^{(k)}$.\n    e. Check for convergence: if $\\frac{\\|r^{(k+1)}\\|_2}{\\|b\\|_2} \\le \\mathrm{tol}$, terminate.\n    f. Compute the improvement factor for the search direction: $\\beta_k = \\frac{r^{(k+1)T} r^{(k+1)}}{r^{(k)T} r^{(k)}}$.\n    g. Update the search direction: $p^{(k+1)} = r^{(k+1)} + \\beta_k p^{(k)}$.\n\nThe implementation encapsulates the action of $A$ in a dedicated function. This function accepts a one-dimensional vector of size $n^2$, internally reshapes it into an $n \\times n$ two-dimensional grid representing the solution values at the interior points. To apply the five-point stencil efficiently, this grid is padded with zeros to represent the homogeneous Dirichlet boundary conditions. The stencil operation is then applied using vectorized array operations on this padded grid. The resulting $n \\times n$ grid is flattened back into a one-dimensional vector of size $n^2$ and returned.\n\nFor each test case defined by a pair $(n, \\mathrm{tol})$, the program first sets up the grid, the matrix-free operator function, and the manufactured solution $u^{\\star}$ with its corresponding right-hand side $b$. Then, the CG solver is invoked with a maximum of $n^2$ iterations.\n\nUpon termination of the solver, two verification conditions are checked:\n$1$. The final relative residual norm $\\frac{\\|r^{(\\text{final})}\\|_2}{\\|b\\|_2}$ must be less than or equal to the prescribed tolerance $\\mathrm{tol}$.\n$2$. The relative error between the computed solution $u$ and the exact discrete solution $u^{\\star}$, measured in the Euclidean norm, $\\frac{\\|u - u^{\\star}\\|_2}{\\|u^{\\star}\\|_2}$, must be no more than $100 \\times \\mathrm{tol}$.\n\nA boolean result, `True` if both conditions are met and `False` otherwise, is determined for each test case. The final output is a list of these boolean values.", "answer": "```python\nimport numpy as np\n\ndef create_operator(n, h):\n    \"\"\"\n    Creates a matrix-free function for the 5-point discrete Laplacian operator A.\n\n    Args:\n        n (int): Number of interior grid points per dimension.\n        h (float): Grid spacing.\n\n    Returns:\n        A function that computes the matrix-vector product A*u.\n    \"\"\"\n    def A_op(u_vec):\n        \"\"\"\n        Applies the discrete Laplacian operator A to a vector u_vec.\n\n        Args:\n            u_vec (np.ndarray): A 1D vector of size n*n representing grid values.\n\n        Returns:\n            np.ndarray: The result of A*u_vec as a 1D vector.\n        \"\"\"\n        if n == 0:\n            return np.array([])\n        \n        # Reshape the 1D vector to a 2D grid\n        u_grid = u_vec.reshape((n, n))\n        \n        # Pad the grid with zeros to handle boundary conditions\n        u_padded = np.zeros((n + 2, n + 2))\n        u_padded[1:-1, 1:-1] = u_grid\n        \n        # Apply the 5-point stencil using vectorized operations\n        # The stencil is (4*u_ij - u_{i-1,j} - u_{i+1,j} - u_{i,j-1} - u_{i,j+1})\n        Au_grid = (4 * u_padded[1:-1, 1:-1] -\n                   u_padded[0:-2, 1:-1] -  # u_{i-1,j}\n                   u_padded[2:, 1:-1]   -  # u_{i+1,j}\n                   u_padded[1:-1, 0:-2] -  # u_{i,j-1}\n                   u_padded[1:-1, 2:])      # u_{i,j+1}\n        \n        # Scale by 1/h^2\n        Au_grid /= h**2\n        \n        # Flatten the resulting 2D grid back to a 1D vector\n        return Au_grid.flatten()\n    \n    return A_op\n\ndef cg_solver(A_op, b, tol, max_iter):\n    \"\"\"\n    Solves A*u = b using the Conjugate Gradient method.\n\n    Args:\n        A_op (callable): Matrix-free operator for A.\n        b (np.ndarray): Right-hand side vector.\n        tol (float): Convergence tolerance for the relative residual.\n        max_iter (int): Maximum number of iterations.\n\n    Returns:\n        tuple: A tuple containing the solution vector u and the final residual vector r.\n    \"\"\"\n    u = np.zeros_like(b)\n    r = b.copy()\n    p = r.copy()\n    rs_old = np.dot(r, r)\n    \n    b_norm = np.linalg.norm(b)\n    if b_norm == 0:\n        return u, r  # Trivial case: if b is zero, solution is zero.\n\n    for _ in range(max_iter):\n        Ap = A_op(p)\n        alpha = rs_old / np.dot(p, Ap)\n        \n        u += alpha * p\n        r -= alpha * Ap\n        \n        rs_new = np.dot(r, r)\n        \n        if np.sqrt(rs_new) / b_norm = tol:\n            break\n            \n        p = r + (rs_new / rs_old) * p\n        rs_old = rs_new\n        \n    return u, r\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and produce the final output.\n    \"\"\"\n    test_cases = [\n        (1, 1e-12),\n        (8, 1e-10),\n        (32, 1e-8),\n        (64, 1e-8),\n    ]\n\n    results = []\n    for n, tol in test_cases:\n        h = 1.0 / (n + 1)\n        max_iter = n * n\n        \n        # 1. Create the matrix-free operator for A\n        A_op = create_operator(n, h)\n        \n        # 2. Create the manufactured solution u_star and right-hand side b = A*u_star\n        # The problem states u*_ij = sin(pi*x_i)*sin(pi*y_j) where x_i=ih, y_j=jh.\n        # This implies relating the first grid index to x and second to y.\n        i_coords = np.arange(1, n + 1) * h\n        j_coords = np.arange(1, n + 1) * h\n        # Use 'ij' indexing so grid[i,j] corresponds to (i_coords[i], j_coords[j])\n        xx, yy = np.meshgrid(i_coords, j_coords, indexing='ij')\n        \n        u_star_grid = np.sin(np.pi * xx) * np.sin(np.pi * yy)\n        u_star_vec = u_star_grid.flatten()\n        \n        b_vec = A_op(u_star_vec)\n        \n        # 3. Solve the system A*u = b using the CG solver\n        u_sol, r_final = cg_solver(A_op, b_vec, tol, max_iter)\n        \n        # 4. Perform verification\n        b_norm = np.linalg.norm(b_vec)\n        u_star_norm = np.linalg.norm(u_star_vec)\n\n        # Verification 1: Final relative residual\n        final_rel_res = np.linalg.norm(r_final) / b_norm if b_norm > 0 else 0.0\n        check1 = final_rel_res = tol\n\n        # Verification 2: Relative error with respect to manufactured solution\n        rel_error = np.linalg.norm(u_sol - u_star_vec) / u_star_norm if u_star_norm > 0 else 0.0\n        check2 = rel_error = 100 * tol\n        \n        # Both checks must pass\n        results.append(check1 and check2)\n\n    # Print the final result in the required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2406207"}, {"introduction": "The Conjugate Gradient method is powerful, but its convergence rate is highly dependent on the condition number of the system matrix $A$. To accelerate convergence for ill-conditioned problems, we use preconditioning, which transforms the original system into an equivalent one that is easier for the iterative method to solve. This exercise guides you through implementing a Preconditioned Conjugate Gradient (PCG) solver using the Symmetric Successive Over-Relaxation (SSOR) method as a preconditioner, allowing you to directly compare its performance against the unpreconditioned CG solver and witness the profound impact of preconditioning [@problem_id:2406195].", "problem": "Consider the elliptic boundary value problem for the scalar field $u(x,y)$ on the unit square $\\Omega = (0,1)\\times(0,1)$ with homogeneous Dirichlet boundary conditions:\n$$\n-\\Delta u(x,y) = f(x,y)\\ \\text{in}\\ \\Omega,\\qquad u(x,y)=0\\ \\text{on}\\ \\partial\\Omega.\n$$\nDiscretize this problem using the standard five-point finite difference method on a uniform grid with $n$ interior points in each coordinate direction. Let $h = \\frac{1}{n+1}$ denote the grid spacing. At each interior grid point $(i,j)$, where $i,j\\in\\{1,2,\\dots,n\\}$ and $(x_i,y_j) = (ih,jh)$, the discrete equation is\n$$\n4\\,u_{i,j} - u_{i-1,j} - u_{i+1,j} - u_{i,j-1} - u_{i,j+1} = h^2 f(x_i,y_j).\n$$\nAfter lexicographic ordering of the unknowns, this yields a linear system $A \\mathbf{u} = \\mathbf{b}$ of dimension $N \\times N$ with $N = n^2$, where $A$ is symmetric positive definite. In this problem, take $f(x,y) \\equiv 1$, so that each entry of $\\mathbf{b}$ equals $h^2$.\n\nDefine the Symmetric Successive Over-Relaxation (SSOR) preconditioner for a given relaxation parameter $\\omega \\in (0,2)$ as follows. Let the matrix $A$ be split as $A = D + L + U$, where $D$ is the diagonal of $A$, $L$ is the strict lower triangular part of $A$, and $U$ is the strict upper triangular part of $A$. The SSOR preconditioner $M(\\omega)$ is\n$$\nM(\\omega) = (D + \\omega L)\\, D^{-1}\\, (D + \\omega U).\n$$\n\nYour task is to implement a program that, for each specified test case $(n,\\omega)$, solves the system $A \\mathbf{u} = \\mathbf{b}$ using:\n- the Conjugate Gradient (CG) method without preconditioning, and\n- the preconditioned Conjugate Gradient method using the SSOR preconditioner $M(\\omega)$.\n\nFor both solvers, use the zero vector as the initial guess and stop when the relative residual norm satisfies\n$$\n\\frac{\\|\\mathbf{r}_k\\|_2}{\\|\\mathbf{b}\\|_2} \\le 10^{-8},\n$$\nor when the iteration count reaches the maximum allowed iterations $N = n^2$, whichever occurs first. Report, for each test case, the number of iterations required by unpreconditioned CG and SSOR-preconditioned CG to satisfy the stopping criterion.\n\nUse real arithmetic. All results are dimensionless; no physical units are required.\n\nTest suite:\n- Case $1$: $(n,\\omega) = (16, 1.0)$.\n- Case $2$: $(n,\\omega) = (16, 1.5)$.\n- Case $3$: $(n,\\omega) = (32, 1.5)$.\n- Case $4$: $(n,\\omega) = (8, 1.9)$.\n- Case $5$: $(n,\\omega) = (8, 0.5)$.\n\nYour program should produce a single line of output containing the results for all cases as a comma-separated list of lists, where each inner list contains exactly two integers corresponding to $[\\text{iterations of unpreconditioned CG}, \\text{iterations of SSOR-preconditioned CG}]$ for that case, in the exact order of the test suite. For example, the output format must be\n$$\n[[k_1^{\\mathrm{CG}},k_1^{\\mathrm{SSOR}}],[k_2^{\\mathrm{CG}},k_2^{\\mathrm{SSOR}}],\\dots,[k_5^{\\mathrm{CG}},k_5^{\\mathrm{SSOR}}]].\n$$", "solution": "The problem statement is critically examined and found to be valid. It is a standard, well-posed problem in the field of computational engineering, specifically concerning the iterative solution of linear systems arising from the discretization of elliptic partial differential equations. All definitions, parameters, and objectives are stated with scientific and mathematical precision. No contradictions, ambiguities, or unsound premises are present.\n\nThe task is to solve the linear system $A\\mathbf{u}=\\mathbf{b}$ using two methods: the Conjugate Gradient (CG) method and the Preconditioned Conjugate Gradient (PCG) method with a Symmetric Successive Over-Relaxation (SSOR) preconditioner. The system results from a five-point finite difference discretization of the Poisson equation, $-\\Delta u = 1$, on a unit square with homogeneous Dirichlet boundary conditions.\n\nThe solution is implemented by adhering to the following principled design:\n\n1.  **System Characterization**: The matrix $A$ resulting from the five-point stencil with lexicographic ordering is a large, sparse, block-tridiagonal, symmetric positive-definite (SPD) matrix. Its diagonal entries are $4$, and off-diagonal entries corresponding to grid neighbors are $-1$. The vector $\\mathbf{b}$ has all entries equal to $h^2$, where $h = \\frac{1}{n+1}$ is the grid spacing.\n\n2.  **Matrix-Free Implementation**: To handle the large dimension $N=n^2$ efficiently, the matrix $A$ is not constructed explicitly. Instead, a function is implemented to compute the matrix-vector product $A\\mathbf{v}$. This function reshapes the $N$-dimensional vector $\\mathbf{v}$ into an $n \\times n$ grid, applies the five-point stencil operator while enforcing the zero boundary conditions, and returns the resulting $N$-dimensional vector.\n\n3.  **Conjugate Gradient (CG) Algorithm**: A standard implementation of the CG method is employed. This method is appropriate for SPD systems like $A\\mathbf{u}=\\mathbf{b}$. Starting from an initial guess $\\mathbf{u}_0 = \\mathbf{0}$, the algorithm generates a sequence of iterates that minimizes the $A$-norm of the error. The process terminates when the relative L2-norm of the residual, $\\frac{\\|\\mathbf{r}_k\\|_2}{\\|\\mathbf{b}\\|_2}$, falls below a tolerance of $10^{-8}$ or the number of iterations reaches the maximum of $N$.\n\n4.  **SSOR Preconditioning**: The SSOR preconditioner is given by $M(\\omega) = (D + \\omega L) D^{-1} (D + \\omega U)$, where $A = D+L+U$ is the splitting of $A$ into its diagonal ($D$), strict lower triangular ($L$), and strict upper triangular ($U$) parts. For this problem, $D=4I$. The core of the PCG algorithm is the application of the inverse preconditioner, i.e., solving the system $M\\mathbf{z}=\\mathbf{r}$ for $\\mathbf{z}$. This is performed in two steps:\n    a. A forward substitution to solve $(D + \\omega L) \\mathbf{v} = \\mathbf{r}$ for an intermediate vector $\\mathbf{v}$. On the grid, this corresponds to a forward sweep:\n    $$v_{i,j} = \\frac{1}{4} (r_{i,j} + \\omega(v_{i-1,j} + v_{i,j-1}))$$\n    b. A backward substitution to solve $(D + \\omega U) \\mathbf{z} = D \\mathbf{v}$ for the result $\\mathbf{z}$. On the grid, this is a backward sweep:\n    $$z_{i,j} = v_{i,j} + \\frac{\\omega}{4}(z_{i+1,j} + z_{i,j+1})$$\n    A dedicated function implements these two sweeps to compute $\\mathbf{z} = M^{-1}\\mathbf{r}$.\n\n5.  **Preconditioned Conjugate Gradient (PCG) Algorithm**: The PCG method is implemented by integrating the SSOR preconditioner solve step into the standard CG framework. This is equivalent to applying CG to the better-conditioned system $M^{-1}A\\mathbf{u} = M^{-1}\\mathbf{b}$. The stopping criteria are identical to those for the unpreconditioned CG method.\n\nFor each test case $(n, \\omega)$, the number of iterations required by both CG and PCG are computed and recorded. The final output aggregates these results into the specified list-of-lists format.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef _matvec_A(v_1d, n):\n    \"\"\"\n    Computes the matrix-vector product A*v for the 2D Poisson problem.\n    The matrix A is not formed explicitly.\n    \"\"\"\n    v_2d = v_1d.reshape((n, n))\n    # Pad with 0 for homogeneous Dirichlet boundary conditions.\n    v_padded = np.pad(v_2d, pad_width=1, mode='constant', constant_values=0)\n    # Apply the 5-point stencil corresponding to the negative Laplacian.\n    av_2d = (4 * v_2d \n             - v_padded[1:-1, 0:-2]  # Left neighbor\n             - v_padded[1:-1, 2:]    # Right neighbor\n             - v_padded[0:-2, 1:-1]  # Bottom neighbor\n             - v_padded[2:, 1:-1])   # Top neighbor\n    return av_2d.flatten()\n\ndef _solve_ssor(r_1d, n, omega):\n    \"\"\"\n    Solves the SSOR preconditioning system Mz=r, returning z = M^-1 * r.\n    The preconditioner is M = (D + w*L) * D^-1 * (D + w*U), with D=4I.\n    This is solved via two sweeps: a forward substitution followed by a backward substitution.\n    \"\"\"\n    r_2d = r_1d.reshape((n, n))\n    \n    # --- Step 1: Forward substitution ---\n    # Solves (D + w*L)v = r, which is (4I + w*L)v = r.\n    # On the grid, this is: 4*v_ij - w*v_{i-1,j} - w*v_{i,j-1} = r_ij\n    v_2d = np.zeros((n, n))\n    for j in range(n):\n        for i in range(n):\n            v_left = v_2d[i - 1, j] if i > 0 else 0.0\n            v_down = v_2d[i, j - 1] if j > 0 else 0.0\n            v_2d[i, j] = (r_2d[i, j] + omega * (v_left + v_down)) / 4.0\n            \n    # --- Step 2: Backward substitution ---\n    # Solves (D + w*U)z = D*v, which is (4I + w*U)z = 4v.\n    # On the grid, this is: 4*z_ij - w*z_{i+1,j} - w*z_{i,j+1} = 4*v_ij\n    z_2d = np.zeros((n, n))\n    for j in range(n - 1, -1, -1):\n        for i in range(n - 1, -1, -1):\n            z_right = z_2d[i + 1, j] if i  n - 1 else 0.0\n            z_up = z_2d[i, j + 1] if j  n - 1 else 0.0\n            z_2d[i, j] = v_2d[i, j] + (omega / 4.0) * (z_right + z_up)\n            \n    return z_2d.flatten()\n\ndef _run_cg(n, b, b_norm, tol, max_iter):\n    \"\"\"Runs the unpreconditioned Conjugate Gradient solver.\"\"\"\n    x = np.zeros_like(b)\n    r = np.copy(b)  # Since x_0 is zero, r_0 = b - A*0 = b\n    p = np.copy(r)\n    rs_old = np.dot(r, r)\n\n    if np.sqrt(rs_old) / b_norm = tol:\n        return 0\n\n    for i in range(max_iter):\n        ap = _matvec_A(p, n)\n        alpha = rs_old / np.dot(p, ap)\n        x += alpha * p\n        r -= alpha * ap\n        rs_new = np.dot(r, r)\n        \n        if np.sqrt(rs_new) / b_norm = tol:\n            return i + 1\n            \n        p = r + (rs_new / rs_old) * p\n        rs_old = rs_new\n\n    return max_iter\n\ndef _run_pcg(n, omega, b, b_norm, tol, max_iter):\n    \"\"\"Runs the Preconditioned Conjugate Gradient solver with SSOR.\"\"\"\n    x = np.zeros_like(b)\n    r = np.copy(b)  # Since x_0 is zero, r_0 = b\n    \n    if b_norm == 0 or np.linalg.norm(r) / b_norm = tol:\n        return 0\n\n    z = _solve_ssor(r, n, omega)\n    p = np.copy(z)\n    rz_old = np.dot(r, z)\n    \n    for i in range(max_iter):\n        ap = _matvec_A(p, n)\n        alpha = rz_old / np.dot(p, ap)\n        x += alpha * p\n        r -= alpha * ap\n        \n        if np.linalg.norm(r) / b_norm = tol:\n            return i + 1\n            \n        z = _solve_ssor(r, n, omega)\n        rz_new = np.dot(r, z)\n        \n        if rz_old == 0:\n            return i + 1\n\n        p = z + (rz_new / rz_old) * p\n        rz_old = rz_new\n    \n    return max_iter\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, omega)\n        (16, 1.0),\n        (16, 1.5),\n        (32, 1.5),\n        (8, 1.9),\n        (8, 0.5),\n    ]\n\n    results = []\n    for case in test_cases:\n        n, omega = case\n        \n        # Setup problem parameters\n        N = n * n\n        h = 1.0 / (n + 1)\n        tol = 1e-8\n        max_iter = N\n\n        # Right-hand side vector b\n        b = (h**2) * np.ones(N)\n        b_norm = np.linalg.norm(b)\n\n        # Run unpreconditioned CG\n        cg_iters = _run_cg(n, b, b_norm, tol, max_iter)\n        \n        # Run SSOR-preconditioned CG\n        pcg_iters = _run_pcg(n, omega, b, b_norm, tol, max_iter)\n        \n        results.append([cg_iters, pcg_iters])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2406195"}, {"introduction": "The efficiency of an iterative solver is deeply connected to the properties of the underlying physical problem being modeled. This practice moves from implementation to analysis, exploring how geometric features, such as a high-aspect-ratio domain, influence solver performance. By analyzing the spectral radius of the weighted Jacobi iteration matrix for a discretized Poisson equation on an elongated rectangle, you will gain a concrete understanding of how physical anisotropy can lead to an ill-conditioned system and significantly slow down convergence [@problem_id:2406162].", "problem": "You are given a two-dimensional, second-order elliptic boundary value problem on a rectangle with homogeneous Dirichlet boundary conditions. The computational domain is the rectangle $\\Omega = (0, L_x) \\times (0, L_y)$ with $L_x > 0$ and $L_y > 0$. The governing equation is $ - \\partial_{xx} u - \\partial_{yy} u = f$ in $\\Omega$ with $u = 0$ on $\\partial\\Omega$. Discretize $\\Omega$ using a uniform tensor-product grid with $N_x$ interior points in the $x$-direction and $N_y$ interior points in the $y$-direction, where $N_x$ and $N_y$ are positive integers. Let the mesh spacings be $h_x = L_x/(N_x+1)$ and $h_y = L_y/(N_y+1)$. Using the standard five-point finite difference discretization at each interior grid node $(i,j)$ for $i \\in \\{1,\\dots,N_x\\}$ and $j \\in \\{1,\\dots,N_y\\}$, the discrete operator is\n$$\n\\frac{-u_{i-1,j} + 2 u_{i,j} - u_{i+1,j}}{h_x^2} + \\frac{-u_{i,j-1} + 2 u_{i,j} - u_{i,j+1}}{h_y^2} = f_{i,j}.\n$$\nOrdering the interior unknowns $u_{i,j}$ in any fixed lexicographic convention yields a symmetric positive definite linear system $A \\, \\mathbf{u} = \\mathbf{b}$ with a sparse matrix $A$ of size $N \\times N$ where $N = N_x N_y$. Denote by $D$ the diagonal of $A$, and consider the weighted Jacobi stationary iteration with weight $w \\in (0,1]$ for solving $A \\, \\mathbf{u} = \\mathbf{b}$,\n$$\n\\mathbf{u}^{(k+1)} = \\mathbf{u}^{(k)} + w \\, D^{-1} \\left(\\mathbf{b} - A \\, \\mathbf{u}^{(k)}\\right).\n$$\nThe corresponding iteration matrix is\n$T_J(w) = I - w \\, D^{-1} A.$\nFor a given choice of parameters, define the asymptotic error reduction factor per iteration as the spectral radius $\\rho\\!\\left(T_J(w)\\right)$, that is, the maximum absolute value of the eigenvalues of $T_J(w)$. For a tolerance $\\tau = 10^{-6}$, define the minimal iteration count $n$ required to reduce the worst-case initial error by a factor at most $\\tau$ as the smallest nonnegative integer $n$ such that $\\rho\\!\\left(T_J(w)\\right)^n \\le \\tau$. If $\\rho\\!\\left(T_J(w)\\right) \\ge 1$, define $n = -1$.\n\nYour task is to write a program that, for each parameter set in the test suite below, constructs the matrix $A$ that arises from the five-point finite difference method with homogeneous Dirichlet boundary conditions, computes the spectral radius $\\rho\\!\\left(T_J(w)\\right)$ for the specified $w$ (or the $w$ that minimizes the spectral radius, as specified), and returns the iteration count $n$ defined above for $\\tau = 10^{-6}$.\n\nTest suite:\n- Case $1$ (square domain, reference): $L_x = 1.0$, $L_y = 1.0$, $N_x = 30$, $N_y = 30$, $w = 2/3$.\n- Case $2$ (moderately high aspect ratio): $L_x = 10.0$, $L_y = 1.0$, $N_x = 30$, $N_y = 30$, $w = 2/3$.\n- Case $3$ (very high aspect ratio): $L_x = 100.0$, $L_y = 1.0$, $N_x = 30$, $N_y = 30$, $w = 2/3$.\n- Case $4$ (weight optimization on high aspect ratio): $L_x = 10.0$, $L_y = 1.0$, $N_x = 30$, $N_y = 30$, and $w$ is to be chosen in the interval $(0,1]$ to minimize $\\rho\\!\\left(T_J(w)\\right)$.\n\nFor Cases $1$â€“$3$, the result for each case is the ordered pair $[\\rho, n]$ where $\\rho$ is a floating-point number and $n$ is an integer. For Case $4$, the result is the ordered triple $[w_\\star, \\rho_\\star, n_\\star]$ where $w_\\star \\in (0,1]$ minimizes the spectral radius, $\\rho_\\star$ is the minimized spectral radius, and $n_\\star$ is the corresponding iteration count for $\\tau = 10^{-6}$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for the four cases as a comma-separated list enclosed in square brackets with no spaces. The format is\n$[[\\rho_1,n_1],[\\rho_2,n_2],[\\rho_3,n_3],[w_\\star,\\rho_\\star,n_\\star]]$,\nwhere each $\\rho_k$ and $\\rho_\\star$ is a floating-point number and each $n_k$ and $n_\\star$ is an integer. There are no physical units involved, and all angles, if any, are to be interpreted in radians, although no angles appear in this problem.", "solution": "The problem statement submitted for analysis is deemed valid. It is scientifically grounded in the fields of numerical analysis and computational engineering, specifically concerning the finite difference discretization of the Poisson equation and the convergence properties of stationary iterative methods. The problem is well-posed, objective, and self-contained, providing all necessary information for a unique and meaningful solution. There are no logical contradictions, factual inaccuracies, or ambiguities.\n\nThe task is to determine the convergence behavior of the weighted Jacobi iteration for a discretized two-dimensional Poisson problem. This involves calculating the spectral radius of the iteration matrix and the number of iterations required to achieve a specified error tolerance. A direct, brute-force construction of the system matrix $A$ and numerical computation of its eigenvalues, while possible for the given dimensions ($N = N_x N_y = 30 \\times 30 = 900$), is computationally inefficient and demonstrates a lack of theoretical insight. A superior approach, grounded in the analytical properties of the discretized operator, will be employed.\n\nThe linear system arises from the five-point finite difference discretization of $-\\nabla^2 u = f$ on $\\Omega = (0, L_x) \\times (0, L_y)$ with homogeneous Dirichlet boundary conditions. The discrete equation at an interior node $(i,j)$ is:\n$$\n\\left(\\frac{2}{h_x^2} + \\frac{2}{h_y^2}\\right)u_{i,j} - \\frac{1}{h_x^2}u_{i+1,j} - \\frac{1}{h_x^2}u_{i-1,j} - \\frac{1}{h_y^2}u_{i,j+1} - \\frac{1}{h_y^2}u_{i,j-1} = f_{i,j}\n$$\nThis defines the entries of the system matrix $A$. The matrix $A$ is symmetric and positive definite. The diagonal entries of $A$ are all equal to a constant, $d$, given by:\n$$\nd = \\frac{2}{h_x^2} + \\frac{2}{h_y^2}\n$$\nTherefore, the diagonal matrix $D$ is $D = dI$, where $I$ is the identity matrix.\n\nThe weighted Jacobi iteration matrix is given by $T_J(w) = I - wD^{-1}A$. Substituting $D = dI$, we get:\n$$\nT_J(w) = I - \\frac{w}{d}A\n$$\nThe eigenvalues $\\mu(w)$ of $T_J(w)$ are related to the eigenvalues $\\lambda$ of $A$ by the expression $\\mu(w) = 1 - (w/d)\\lambda$. The spectral radius is $\\rho(T_J(w)) = \\max|\\mu(w)|$.\n\nThe eigenvalues of the matrix $A$ corresponding to the two-dimensional discrete Laplacian with homogeneous Dirichlet boundary conditions are known analytically:\n$$\n\\lambda_{p,q} = \\frac{4}{h_x^2} \\sin^2\\left(\\frac{p\\pi}{2(N_x+1)}\\right) + \\frac{4}{h_y^2} \\sin^2\\left(\\frac{q\\pi}{2(N_y+1)}\\right)\n$$\nfor $p \\in \\{1, \\dots, N_x\\}$ and $q \\in \\{1, \\dots, N_y\\}$.\n\nThe eigenvalues of $A$ are all positive, bounded by $\\lambda_{\\min} = \\lambda_{1,1}$ and $\\lambda_{\\max} = \\lambda_{N_x, N_y}$. The spectral radius of $T_J(w)$ is therefore determined by the mapping of these extremal eigenvalues:\n$$\n\\rho(T_J(w)) = \\max\\left( \\left|1 - \\frac{w}{d}\\lambda_{\\min}\\right|, \\left|1 - \\frac{w}{d}\\lambda_{\\max}\\right| \\right)\n$$\nAll test cases specify $N_x=N_y=30$. This leads to a critical simplification. Let $N_g = N_x = N_y = 30$. The extremal eigenvalues of $A$ are:\n$$\n\\lambda_{\\min} = \\lambda_{1,1} = \\left(\\frac{4}{h_x^2} + \\frac{4}{h_y^2}\\right) \\sin^2\\left(\\frac{\\pi}{2(N_g+1)}\\right) = 2d \\sin^2\\left(\\frac{\\pi}{2(N_g+1)}\\right)\n$$\n$$\n\\lambda_{\\max} = \\lambda_{N_g,N_g} = \\left(\\frac{4}{h_x^2} + \\frac{4}{h_y^2}\\right) \\cos^2\\left(\\frac{\\pi}{2(N_g+1)}\\right) = 2d \\cos^2\\left(\\frac{\\pi}{2(N_g+1)}\\right)\n$$\nNote that $\\sin^2\\left(\\frac{N_g\\pi}{2(N_g+1)}\\right) = \\cos^2\\left(\\frac{\\pi}{2} - \\frac{N_g\\pi}{2(N_g+1)}\\right) = \\cos^2\\left(\\frac{\\pi}{2(N_g+1)}\\right)$.\nIt follows that the ratios $\\lambda_{\\min}/d$ and $\\lambda_{\\max}/d$ are independent of the mesh spacings $h_x$ and $h_y$, and thus independent of the domain's aspect ratio $L_x/L_y$. Specifically,\n$$\n\\frac{\\lambda_{\\min}}{d} = 2\\sin^2\\left(\\frac{\\pi}{2(N_g+1)}\\right) \\quad \\text{and} \\quad \\frac{\\lambda_{\\max}}{d} = 2\\cos^2\\left(\\frac{\\pi}{2(N_g+1)}\\right)\n$$\nThis implies that for a fixed $w$, the spectral radius $\\rho(T_J(w))$ will be identical for Cases $1$, $2$, and $3$.\n\nFor these cases, $w = 2/3$. Using the identity $1-x = \\cos(2\\arccos\\sqrt{x})$, the spectral radius of the unweighted Jacobi matrix ($w=1$) is $\\rho(J) = \\rho(T_J(1)) = \\max(|\\lambda_{\\min}/d-1|, |\\lambda_{\\max}/d-1|) = \\cos(\\pi/(N_g+1))$. The eigenvalues of $T_J(w)$ are $(1-w)+w\\mu_J$, where $\\mu_J$ are eigenvalues of $J$. The spectrum of $J$ is real and lies in $[-\\rho(J), \\rho(J)]$. Thus, the spectral radius of $T_J(w)$ for $w \\in (0,1]$ is $1-w+w\\rho(J)$. For $w=2/3$, we have:\n$$\n\\rho = \\rho(T_J(2/3)) = 1 - \\frac{2}{3} + \\frac{2}{3}\\rho(J) = \\frac{1}{3} + \\frac{2}{3}\\cos\\left(\\frac{\\pi}{N_g+1}\\right)\n$$\nWith $N_g=30$, $\\rho = 1/3 + (2/3)\\cos(\\pi/31) \\approx 0.99658006$.\n\nFor Case $4$, we must find $w_\\star \\in (0,1]$ that minimizes $\\rho(T_J(w))$. The objective function is $\\rho(w) = 1-w+w\\cos(\\pi/(N_g+1))$. Since $\\cos(\\pi/(N_g+1))  1$, this function is strictly decreasing for $w0$. Therefore, the minimum on the interval $(0,1]$ is achieved at the right boundary, $w_\\star = 1$. The corresponding minimal spectral radius is:\n$$\n\\rho_\\star = \\rho(T_J(1)) = \\cos\\left(\\frac{\\pi}{N_g+1}\\right)\n$$\nWith $N_g=30$, $\\rho_\\star = \\cos(\\pi/31) \\approx 0.99487009$.\n\nThe minimal iteration count $n$ to reduce the error by a factor $\\tau = 10^{-6}$ is the smallest non-negative integer satisfying $\\rho^n \\le \\tau$. Since $\\rho \\in (0,1)$, this is equivalent to $n \\ln\\rho \\le \\ln\\tau$, which gives $n \\ge \\ln\\tau/\\ln\\rho$. The smallest integer $n$ is thus:\n$$\nn = \\left\\lceil \\frac{\\ln\\tau}{\\ln\\rho} \\right\\rceil\n$$\nFor Cases $1-3$, with $\\rho \\approx 0.99658006$:\n$$\nn = \\left\\lceil \\frac{\\ln(10^{-6})}{\\ln(0.99658006)} \\right\\rceil = \\left\\lceil \\frac{-13.81551}{-0.0034257} \\right\\rceil = \\lceil 4035.266 \\rceil = 4036\n$$\nFor Case $4$, with $\\rho_\\star \\approx 0.99487009$:\n$$\nn_\\star = \\left\\lceil \\frac{\\ln(10^{-6})}{\\ln(0.99487009)} \\right\\rceil = \\left\\lceil \\frac{-13.81551}{-0.0051412} \\right\\rceil = \\lceil 2687.201 \\rceil = 2688\n$$\nThese analytical results will be computed numerically in the final program.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the elliptic BVP problem as specified.\n\n    The solution leverages the analytical eigenvalues of the discrete Laplacian operator\n    to compute the spectral radius of the weighted Jacobi iteration matrix. This avoids\n    the construction and spectral analysis of large matrices.\n    \"\"\"\n    \n    # Test cases defined in the problem statement\n    test_cases = [\n        # Case 1: square domain, reference\n        {'Lx': 1.0, 'Ly': 1.0, 'Nx': 30, 'Ny': 30, 'w': 2/3, 'optimize_w': False},\n        # Case 2: moderately high aspect ratio\n        {'Lx': 10.0, 'Ly': 1.0, 'Nx': 30, 'Ny': 30, 'w': 2/3, 'optimize_w': False},\n        # Case 3: very high aspect ratio\n        {'Lx': 100.0, 'Ly': 1.0, 'Nx': 30, 'Ny': 30, 'w': 2/3, 'optimize_w': False},\n        # Case 4: weight optimization\n        {'Lx': 10.0, 'Ly': 1.0, 'Nx': 30, 'Ny': 30, 'w': None, 'optimize_w': True},\n    ]\n\n    tau = 1e-6\n    results = []\n\n    for i, case in enumerate(test_cases):\n        Lx, Ly = case['Lx'], case['Ly']\n        Nx, Ny = case['Nx'], case['Ny']\n        w = case['w']\n        optimize_w = case['optimize_w']\n\n        # Sanity check: the analytical solution relies on Nx=Ny\n        if Nx != Ny:\n            raise ValueError(\"Analytical solution provided only for Nx = Ny\")\n\n        # Ng is the common number of interior points in each direction.\n        Ng = Nx\n        \n        # Spectral radius of the unweighted Jacobi matrix J = I - D^-1*A\n        # For Nx=Ny, rho(J) = cos(pi / (Ng + 1)) independent of Lx, Ly.\n        rho_J = np.cos(np.pi / (Ng + 1))\n\n        if optimize_w:\n            # The function to minimize is rho(T_J(w)) = 1-w+w*rho(J) for w in (0,1].\n            # This is a decreasing function of w, so the minimum on (0,1] is at w=1.\n            w_star = 1.0\n            rho_star = rho_J\n            n_star = int(np.ceil(np.log(tau) / np.log(rho_star)))\n            results.append(f\"[{w_star},{rho_star},{n_star}]\")\n        else:\n            # For a fixed w, the spectral radius of the weighted Jacobi iteration T_J(w)\n            # is rho(T_J(w)) = max(|1-w-w*rho(J)|, 1-w+w*rho(J)).\n            # For w in (0,1], this simplifies to 1-w+w*rho(J).\n            rho = 1.0 - w + w * rho_J\n            \n            # If rho >= 1, the method diverges or does not converge strictly.\n            if rho >= 1.0:\n                n = -1\n            else:\n                n = int(np.ceil(np.log(tau) / np.log(rho)))\n            \n            results.append(f\"[{rho},{n}]\")\n\n    # Final print statement in the exact required format without spaces.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2406162"}]}