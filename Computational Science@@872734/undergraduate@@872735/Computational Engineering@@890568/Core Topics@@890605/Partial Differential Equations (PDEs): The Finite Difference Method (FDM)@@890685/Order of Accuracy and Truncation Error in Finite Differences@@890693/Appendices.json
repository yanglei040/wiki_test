{"hands_on_practices": [{"introduction": "While many common finite difference formulas are found in textbooks, a computational scientist must often derive custom schemes tailored to specific problems, especially at domain boundaries. This practice exercise guides you through the fundamental process of constructing a high-order, one-sided approximation using the method of undetermined coefficients and Taylor series [@problem_id:2421879]. Mastering this technique allows you to create accurate numerical stencils for situations where standard centered-difference formulas are not applicable.", "problem": "Let $f$ be a function with continuous derivatives up to order $5$ on an interval $[a,b]$. Consider a uniform grid $x_j = a + j h$ with spacing $h0$, and focus on the left boundary point $x_0 = a$. Using only the values $f(x_0)$, $f(x_1)$, $f(x_2)$, and $f(x_3)$, construct a one-sided finite difference approximation to $f'(x_0)$ that is order-$3$ accurate. Express the approximation in the normalized form\n$$(c_0 f(x_0) + c_1 f(x_1) + c_2 f(x_2) + c_3 f(x_3))/h,$$\nwith rational coefficients $c_0$, $c_1$, $c_2$, and $c_3$. Determine the leading term in the local truncation error, expressed in terms of $h$ and derivatives of $f$ evaluated at $x_0$. Briefly discuss how such a one-sided stencil is employed at a computational boundary in the discretization of a transport or diffusion partial differential equation (PDE) with Dirichlet or Neumann boundary data. Provide the final approximation in its simplest rational-coefficient form. The final answer should be a single closed-form expression for $f'(x_0)$ in terms of $f(x_0)$, $f(x_1)$, $f(x_2)$, $f(x_3)$, and $h$.", "solution": "We seek an approximation of the form\n$$\nf'(x_0) \\approx \\frac{1}{h} \\left( c_0 f(x_0) + c_1 f(x_1) + c_2 f(x_2) + c_3 f(x_3) \\right)\n$$\nwhere $x_j = x_0 + j h$ for $j=1, 2, 3$. We use the method of undetermined coefficients. We expand $f(x_1)$, $f(x_2)$, and $f(x_3)$ in Taylor series around the point $x_0$. Given that $f$ has continuous derivatives up to order $5$, the Taylor expansions are:\n$$\nf(x_1) = f(x_0+h) = f(x_0) + h f'(x_0) + \\frac{h^2}{2} f''(x_0) + \\frac{h^3}{6} f'''(x_0) + \\frac{h^4}{24} f^{(4)}(x_0) + O(h^5)\n$$\n$$\nf(x_2) = f(x_0+2h) = f(x_0) + 2h f'(x_0) + \\frac{(2h)^2}{2} f''(x_0) + \\frac{(2h)^3}{6} f'''(x_0) + \\frac{(2h)^4}{24} f^{(4)}(x_0) + O(h^5)\n$$\n$$\nf(x_3) = f(x_0+3h) = f(x_0) + 3h f'(x_0) + \\frac{(3h)^2}{2} f''(x_0) + \\frac{(3h)^3}{6} f'''(x_0) + \\frac{(3h)^4}{24} f^{(4)}(x_0) + O(h^5)\n$$\nSubstituting these expansions into the linear combination $c_0 f(x_0) + c_1 f(x_1) + c_2 f(x_2) + c_3 f(x_3)$ and grouping terms by derivatives of $f$ at $x_0$:\n\\begin{align*}\n c_0 f(x_0) + c_1 f(x_1) + c_2 f(x_2) + c_3 f(x_3) \\\\\n= c_0 f(x_0) + c_1 \\left( f(x_0) + h f'(x_0) + \\frac{h^2}{2} f''(x_0) + \\dots \\right) \\\\\n\\quad + c_2 \\left( f(x_0) + 2h f'(x_0) + \\frac{4h^2}{2} f''(x_0) + \\dots \\right) \\\\\n\\quad + c_3 \\left( f(x_0) + 3h f'(x_0) + \\frac{9h^2}{2} f''(x_0) + \\dots \\right) \\\\\n= (c_0 + c_1 + c_2 + c_3) f(x_0) \\\\\n\\quad + h (c_1 + 2c_2 + 3c_3) f'(x_0) \\\\\n\\quad + \\frac{h^2}{2} (c_1 + 4c_2 + 9c_3) f''(x_0) \\\\\n\\quad + \\frac{h^3}{6} (c_1 + 8c_2 + 27c_3) f'''(x_0) \\\\\n\\quad + \\frac{h^4}{24} (c_1 + 16c_2 + 81c_3) f^{(4)}(x_0) + O(h^5)\n\\end{align*}\nWe require our approximation for $f'(x_0)$ to be third-order accurate, meaning the error is $O(h^3)$. This means the expression $\\frac{1}{h}(c_0 f_0 + c_1 f_1 + c_2 f_2 + c_3 f_3)$ should match $f'(x_0)$ up to terms of order $h^2$. Therefore, the linear combination $c_0 f_0 + \\dots + c_3 f_3$ must be equal to $h f'(x_0) + O(h^4)$. This imposes the following conditions on the coefficients:\n1.  Coefficient of $f(x_0)$: $c_0 + c_1 + c_2 + c_3 = 0$\n2.  Coefficient of $h f'(x_0)$: $c_1 + 2c_2 + 3c_3 = 1$\n3.  Coefficient of $h^2 f''(x_0)$: $c_1 + 4c_2 + 9c_3 = 0$\n4.  Coefficient of $h^3 f'''(x_0)$: $c_1 + 8c_2 + 27c_3 = 0$\n\nWe solve the system of linear equations for $c_1, c_2, c_3$ first:\n\\begin{align*}\nc_1 + 2c_2 + 3c_3 = 1 \\quad (A) \\\\\nc_1 + 4c_2 + 9c_3 = 0 \\quad (B) \\\\\nc_1 + 8c_2 + 27c_3 = 0 \\quad (C)\n\\end{align*}\nSubtracting $(A)$ from $(B)$:\n$$ (c_1 + 4c_2 + 9c_3) - (c_1 + 2c_2 + 3c_3) = 0 - 1 \\implies 2c_2 + 6c_3 = -1 \\quad (D) $$\nSubtracting $(B)$ from $(C)$:\n$$ (c_1 + 8c_2 + 27c_3) - (c_1 + 4c_2 + 9c_3) = 0 - 0 \\implies 4c_2 + 18c_3 = 0 \\implies 2c_2 + 9c_3 = 0 \\quad (E) $$\nNow, subtracting $(D)$ from $(E)$:\n$$ (2c_2 + 9c_3) - (2c_2 + 6c_3) = 0 - (-1) \\implies 3c_3 = 1 \\implies c_3 = \\frac{1}{3} $$\nSubstitute $c_3 = 1/3$ into $(E)$:\n$$ 2c_2 + 9\\left(\\frac{1}{3}\\right) = 0 \\implies 2c_2 + 3 = 0 \\implies c_2 = -\\frac{3}{2} $$\nSubstitute $c_2 = -3/2$ and $c_3 = 1/3$ into $(A)$:\n$$ c_1 + 2\\left(-\\frac{3}{2}\\right) + 3\\left(\\frac{1}{3}\\right) = 1 \\implies c_1 - 3 + 1 = 1 \\implies c_1 = 3 $$\nFinally, use the first equation to find $c_0$:\n$$ c_0 + c_1 + c_2 + c_3 = 0 \\implies c_0 = -c_1 - c_2 - c_3 = -3 - \\left(-\\frac{3}{2}\\right) - \\frac{1}{3} = -3 + \\frac{3}{2} - \\frac{1}{3} = \\frac{-18+9-2}{6} = -\\frac{11}{6} $$\nThe rational coefficients are:\n$$ c_0 = -\\frac{11}{6}, \\quad c_1 = 3, \\quad c_2 = -\\frac{3}{2}, \\quad c_3 = \\frac{1}{3} $$\nThe finite difference approximation is:\n$$ f'(x_0) \\approx \\frac{1}{h} \\left( -\\frac{11}{6}f(x_0) + 3f(x_1) - \\frac{3}{2}f(x_2) + \\frac{1}{3}f(x_3) \\right) $$\nThis can be written in a cleaner form:\n$$ f'(x_0) \\approx \\frac{-11f(x_0) + 18f(x_1) - 9f(x_2) + 2f(x_3)}{6h} $$\n\n**Local Truncation Error**\nThe local truncation error (LTE) is the difference between the finite difference operator and the true derivative operator. The leading error term is the first non-zero term in the Taylor series expansion that was not canceled. This is the term proportional to $f^{(4)}(x_0)$.\nThe coefficient of the $\\frac{h^4}{24} f^{(4)}(x_0)$ term in the expansion of $c_0 f_0 + \\dots + c_3 f_3$ is:\n$$ \\frac{1}{24}(c_1 \\cdot 1^4 + c_2 \\cdot 2^4 + c_3 \\cdot 3^4) = \\frac{1}{24}(c_1 + 16c_2 + 81c_3) $$\nSubstituting the values of the coefficients:\n$$ \\frac{1}{24} \\left( 3 + 16\\left(-\\frac{3}{2}\\right) + 81\\left(\\frac{1}{3}\\right) \\right) = \\frac{1}{24} (3 - 24 + 27) = \\frac{6}{24} = \\frac{1}{4} $$\nSo the sum is:\n$$ \\sum_{i=0}^{3} c_i f(x_i) = h f'(x_0) + \\frac{h^4}{4} f^{(4)}(x_0) + O(h^5) $$\nThe approximation for $f'(x_0)$ is $D_h f(x_0) = \\frac{1}{h}\\sum c_i f(x_i)$. Therefore,\n$$ D_h f(x_0) = f'(x_0) + \\frac{h^3}{4} f^{(4)}(x_0) + O(h^4) $$\nThe leading term of the local truncation error is LTE $= D_h f(x_0) - f'(x_0)$, which is:\n$$ \\text{LTE} = \\frac{h^3}{4} f^{(4)}(x_0) $$\nThe error is of order $h^3$, so the approximation is indeed order-$3$ accurate.\n\n**Discussion of Application at Computational Boundaries**\nWhen solving partial differential equations (PDEs) numerically on a finite domain, such as a transport equation $u_t + a u_x = 0$ or a diffusion equation $u_t = D u_{xx}$, the domain is discretized into a grid. At interior grid points, symmetric, centered finite difference stencils are generally preferred as they typically offer higher accuracy for a given stencil width. For example, the second-order centered difference $u'(x_i) \\approx (u_{i+1} - u_{i-1})/(2h)$ is very common.\n\nHowever, at the boundaries of the domain (e.g., at $x_0$), a centered stencil requires a \"ghost point\" (e.g., $u_{-1}$) that lies outside the computational domain. A one-sided stencil, such as the one derived here, is essential because it approximates the derivative using only points within or on the boundary of the domain.\n\nThe specific implementation depends on the type of boundary condition given:\n- **Dirichlet Boundary Condition:** The value of the function is specified, e.g., $u(x_0, t) = g(t)$. The value $u_0 = u(x_0,t)$ is known at all times. The PDE does not need to be solved at the boundary node itself. However, the numerical scheme at the adjacent interior node, $x_1$, might require derivative information at $x_0$. More critically, if a physical quantity depending on the derivative, like heat flux $q = -k u_x$, needs to be computed at the boundary, a one-sided formula is indispensable.\n\n- **Neumann Boundary Condition:** The value of the derivative is specified, e.g., $\\frac{\\partial u}{\\partial x}(x_0, t) = p(t)$. Here, the one-sided formula provides a direct algebraic relationship between the boundary derivative and the function values at the grid points. Using the derived third-order formula:\n$$ \\frac{-11 u_0 + 18 u_1 - 9 u_2 + 2 u_3}{6h} = p(t) $$\nThis equation is incorporated into the global system of algebraic equations resulting from the discretization. It provides a constraint that can be used to solve for the unknown boundary value $u_0$ in terms of the interior values and the specified flux $p(t)$.\n\nCrucially, the order of accuracy of the boundary scheme must be compatible with the interior scheme. Using a low-order boundary scheme with a high-order interior scheme can degrade the overall global accuracy of the numerical solution. The third-order one-sided stencil derived here would be appropriate for a numerical method that is third-order or fourth-order accurate in the interior to maintain the desired global convergence rate.", "answer": "$$\n\\boxed{\\frac{-11 f(x_0) + 18 f(x_1) - 9 f(x_2) + 2 f(x_3)}{6h}}\n$$", "id": "2421879"}, {"introduction": "After implementing a numerical method, how can we verify that it achieves its theoretical order of accuracy? This exercise introduces Richardson extrapolation, a powerful technique for analyzing data from a grid refinement study [@problem_id:2421800]. By comparing results from successively finer grids, you can estimate not only the method's convergence rate but also a more accurate approximation of the true, grid-converged solution, a cornerstone of code verification.", "problem": "A scalar quantity $Q$ is computed by a finite-difference discretization on uniform grids of spacing $h$. Assume that, in the asymptotic regime, the leading truncation error behaves as $Q(h) = Q^{\\ast} + C h^{p} + \\mathcal{O}(h^{p+1})$, where $Q^{\\ast}$ is the grid-converged value, $C$ is an $h$-independent constant, and $p$ is the (unknown) order of accuracy. In a grid-refinement study with refinement ratio $r=2$, the following data were recorded for three grids:\n- $h = 0.20$: absolute error $E(h) = 2.0000 \\times 10^{-2}$ and computed approximation $Q(h) = 1.43421356$.\n- $h = 0.10$: absolute error $E(h) = 5.0000 \\times 10^{-3}$ and computed approximation $Q(h) = 1.41921356$.\n- $h = 0.05$: absolute error $E(h) = 1.2500 \\times 10^{-3}$ and computed approximation $Q(h) = 1.41546356$.\n\nUsing only the stated asymptotic error model and the given data, use Richardson extrapolation to estimate both the grid-converged value $Q^{\\ast}$ and the order of accuracy $p$. Report your final answer as a row matrix with $Q^{\\ast}$ first and $p$ second. Round the estimate of $Q^{\\ast}$ to six significant figures. Do not include units.", "solution": "The solution proceeds by first determining the order of accuracy $p$, and then using this value to extrapolate to the grid-converged value $Q^{\\ast}$.\n\nLet us denote the solutions on the three grids as $Q_1 = Q(h_1)$, $Q_2 = Q(h_2)$, and $Q_3 = Q(h_3)$, where $h_1 = 0.20$, $h_2 = 0.10$, and $h_3 = 0.05$. The constant refinement ratio is $r = \\frac{h_1}{h_2} = \\frac{h_2}{h_3} = 2$.\n\nFrom the asymptotic error model, ignoring higher-order terms, we can write:\n$Q_1 \\approx Q^{\\ast} + C h_1^{p}$\n$Q_2 \\approx Q^{\\ast} + C h_2^{p}$\n$Q_3 \\approx Q^{\\ast} + C h_3^{p}$\n\nSubtracting these equations consecutively yields the differences in computed values:\n$Q_1 - Q_2 \\approx C(h_1^p - h_2^p) = C( (r h_2)^p - h_2^p ) = C h_2^p (r^p - 1)$\n$Q_2 - Q_3 \\approx C(h_2^p - h_3^p) = C( (r h_3)^p - h_3^p ) = C h_3^p (r^p - 1)$\n\nThe ratio of these two differences eliminates the constant $C$:\n$$\n\\frac{Q_1 - Q_2}{Q_2 - Q_3} \\approx \\frac{C h_2^p (r^p - 1)}{C h_3^p (r^p - 1)} = \\left(\\frac{h_2}{h_3}\\right)^p = r^p\n$$\nThis allows for the determination of the apparent order of accuracy, $p$:\n$$\np = \\log_r\\left(\\frac{Q_1 - Q_2}{Q_2 - Q_3}\\right) = \\frac{\\ln\\left(\\frac{Q_1 - Q_2}{Q_2 - Q_3}\\right)}{\\ln(r)}\n$$\nSubstituting the given numerical values:\n$Q_1 - Q_2 = 1.43421356 - 1.41921356 = 0.015$\n$Q_2 - Q_3 = 1.41921356 - 1.41546356 = 0.00375$\nThe ratio is $\\frac{0.015}{0.00375} = 4$.\nWith a refinement ratio $r=2$, the order of accuracy is:\n$$\np = \\frac{\\ln(4)}{\\ln(2)} = \\frac{2\\ln(2)}{\\ln(2)} = 2\n$$\nThe order of accuracy is exactly $p=2$.\n\nNext, we use Richardson extrapolation to estimate $Q^{\\ast}$. To obtain the most accurate estimate, we use the solutions from the two finest grids, $Q_2$ and $Q_3$.\nWe have the system of two equations:\n$Q_2 \\approx Q^{\\ast} + C h_2^{p}$\n$Q_3 \\approx Q^{\\ast} + C h_3^{p}$\nWe can eliminate the error term coefficient $C$. From the first equation, $C \\approx \\frac{Q_2 - Q^{\\ast}}{h_2^p}$. Substituting this into the second equation gives:\n$Q_3 \\approx Q^{\\ast} + \\frac{Q_2 - Q^{\\ast}}{h_2^p} h_3^p = Q^{\\ast} + (Q_2 - Q^{\\ast})\\left(\\frac{h_3}{h_2}\\right)^p = Q^{\\ast} + (Q_2 - Q^{\\ast})\\frac{1}{r^p}$\nRearranging to solve for $Q^{\\ast}$:\n$Q_3(r^p) \\approx Q^{\\ast}(r^p) + Q_2 - Q^{\\ast}$\n$Q_3 r^p - Q_2 \\approx Q^{\\ast} (r^p - 1)$\n$$\nQ^{\\ast} \\approx \\frac{Q_3 r^p - Q_2}{r^p - 1}\n$$\nThis is the Richardson extrapolation formula. We now substitute the known values: $Q_2 = 1.41921356$, $Q_3 = 1.41546356$, $r=2$, and $p=2$. Thus, $r^p = 2^2 = 4$.\n$$\nQ^{\\ast} \\approx \\frac{4 \\times 1.41546356 - 1.41921356}{4 - 1}\n$$\nThe calculation proceeds as follows:\n$4 \\times 1.41546356 = 5.66185424$\n$4 Q_3 - Q_2 = 5.66185424 - 1.41921356 = 4.24264068$\n$$\nQ^{\\ast} \\approx \\frac{4.24264068}{3} = 1.41421356\n$$\nThe problem requires this estimate to be rounded to six significant figures. The first six significant figures of $1.41421356$ are $1, 4, 1, 4, 2, 1$. The subsequent digit is $3$, which is less than $5$, so we round down.\nThe rounded estimate is $Q^{\\ast} \\approx 1.41421$.\n\nThe two estimated parameters are the order of accuracy $p=2$ and the grid-converged value $Q^{\\ast} \\approx 1.41421$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1.41421  2\n\\end{pmatrix}\n}\n$$", "id": "2421800"}, {"introduction": "In theory, reducing the step size $h$ should always decrease the truncation error, leading to more accurate results. This hands-on coding problem explores the practical limits of this idea by revealing the battle between truncation error and machine precision [@problem_id:2421884]. You will investigate how, for very small $h$, subtractive cancellation in floating-point arithmetic causes round-off error to dominate, and you will numerically identify the optimal step size that minimizes the total error.", "problem": "Let $f:\\mathbb{R}\\to\\mathbb{R}$ be a smooth function. For a given point $x_0\\in\\mathbb{R}$ and a step size $h0$, define two linear finite-difference operators for approximating the first derivative $f'(x_0)$:\n(1) The forward-difference operator $D_h^{\\mathrm{F}}$ defined by $D_h^{\\mathrm{F}} f(x_0) \\equiv \\dfrac{f(x_0+h)-f(x_0)}{h}$.\n(2) The central-difference operator $D_h^{\\mathrm{C}}$ defined by $D_h^{\\mathrm{C}} f(x_0) \\equiv \\dfrac{f(x_0+h)-f(x_0-h)}{2h}$.\nFor each chosen combination of $f$, $x_0$, and one of the operators above, consider a strictly decreasing geometric sequence of step sizes $\\{h_k\\}_{k=1}^{K}$ given by $h_k = 2^{-k}$ for $k\\in\\{1,2,\\dots,K\\}$, with $K$ a fixed positive integer. For each $h_k$, define the absolute error\n$$\nE(h_k) \\equiv \\left|D_{h_k} f(x_0) - f'(x_0)\\right|.\n$$\nAs $h$ is reduced along the sequence $\\{h_k\\}$, the total error $E(h)$ is influenced by truncation error and floating-point round-off error. In a fixed floating-point arithmetic, there typically exists a smallest error at some step size $h^\\star$ along the sequence, after which further reduction of $h$ causes $E(h)$ to increase. For the purposes of this problem, define $h^\\star$ as the element of $\\{h_k\\}$ at which $E(h_k)$ attains its minimum value over $k\\in\\{1,2,\\dots,K\\}$. If the minimum occurs at multiple indices, choose the largest $h_k$ among those minima as $h^\\star$.\nUse the following settings for the arithmetic model: Institute of Electrical and Electronics Engineers (IEEE) $754$ binary$64$ (commonly known as double precision) and IEEE $754$ binary$32$ (commonly known as single precision). Angles, when present, must be interpreted in radians. No physical units are involved.\nYour task is to write a program that, for each test case in the test suite defined below, computes $h^\\star$ according to the definition above.\nTest suite:\n- Test case $1$: $f(x)=\\exp(x)$, $f'(x)=\\exp(x)$, $x_0=1$, operator $D_h^{\\mathrm{C}}$, arithmetic precision binary$64$, with $K=60$.\n- Test case $2$: $f(x)=\\sin(x)$, $f'(x)=\\cos(x)$, $x_0=1$, operator $D_h^{\\mathrm{F}}$, arithmetic precision binary$64$, with $K=60$.\n- Test case $3$: $f(x)=\\ln(x)$, $f'(x)=1/x$, $x_0=1$, operator $D_h^{\\mathrm{C}}$, arithmetic precision binary$32$, with $K=60$.\n- Test case $4$: $f(x)=\\exp(x)$, $f'(x)=\\exp(x)$, $x_0=1$, operator $D_h^{\\mathrm{F}}$, arithmetic precision binary$32$, with $K=60$.\nAll logarithms are natural logarithms. Ensure $x_0-h_k0$ whenever $f(x)=\\ln(x)$ is evaluated; this is satisfied by the specified data because $x_0=1$ and $h_k\\leq 2^{-1}$ for all $k\\geq 1$.\nFor each test case, output the value of $h^\\star$ as a real number. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases $1$ through $4$; for example, an output with four results must look like $[r_1,r_2,r_3,r_4]$ where each $r_j$ is the computed $h^\\star$ for test case $j$.", "solution": "**Theoretical Foundation**\nThe total error $E(h)$ in numerical differentiation is the sum of two components: truncation error $E_T(h)$ and round-off error $E_R(h)$.\n\n**Truncation Error**\nThe truncation error arises from approximating the true derivative with a discrete formula. Its form is derived from Taylor series expansions.\nFor the **forward-difference operator** $D_h^{\\mathrm{F}}$, the Taylor series of $f$ around $x_0$ is:\n$$ f(x_0+h) = f(x_0) + hf'(x_0) + \\frac{h^2}{2}f''(x_0) + \\mathcal{O}(h^3) $$\nRearranging for the derivative gives:\n$$ D_h^{\\mathrm{F}} f(x_0) = \\frac{f(x_0+h) - f(x_0)}{h} = f'(x_0) + \\frac{h}{2}f''(x_0) + \\mathcal{O}(h^2) $$\nThe leading-order truncation error is $E_T^{\\mathrm{F}}(h) = \\frac{h}{2}f''(x_0)$, which is of order $\\mathcal{O}(h)$. The method is first-order accurate.\n\nFor the **central-difference operator** $D_h^{\\mathrm{C}}$, we use two Taylor expansions:\n$$ f(x_0+h) = f(x_0) + hf'(x_0) + \\frac{h^2}{2}f''(x_0) + \\frac{h^3}{6}f'''(x_0) + \\mathcal{O}(h^4) $$\n$$ f(x_0-h) = f(x_0) - hf'(x_0) + \\frac{h^2}{2}f''(x_0) - \\frac{h^3}{6}f'''(x_0) + \\mathcal{O}(h^4) $$\nSubtracting the second from the first and rearranging yields:\n$$ D_h^{\\mathrm{C}} f(x_0) = \\frac{f(x_0+h) - f(x_0-h)}{2h} = f'(x_0) + \\frac{h^2}{6}f'''(x_0) + \\mathcal{O}(h^4) $$\nThe leading-order truncation error is $E_T^{\\mathrm{C}}(h) = \\frac{h^2}{6}f'''(x_0)$, which is of order $\\mathcal{O}(h^2)$. The method is second-order accurate.\n\n**Round-off Error**\nRound-off error originates from the finite precision of floating-point arithmetic. The evaluation of $f(x)$ produces a value $\\hat{f}(x)$ such that $|\\hat{f}(x) - f(x)| \\lesssim \\epsilon_M |f(x)|$, where $\\epsilon_M$ is the machine epsilon (approximately $2.2 \\times 10^{-16}$ for binary$64$ and $1.2 \\times 10^{-7}$ for binary$32$).\nFor small $h$, both difference formulas involve subtracting nearly equal numbers ($f(x_0+h) \\approx f(x_0)$ and $f(x_0+h) \\approx f(x_0-h)$). This operation, known as subtractive cancellation, amplifies the effect of round-off error. The error in the numerator of the difference formulas is roughly on the order of $\\epsilon_M|f(x_0)|$. When divided by a small denominator ($h$ or $2h$), the resulting round-off error in the derivative approximation becomes significant:\n$$ |E_R(h)| \\approx C \\frac{\\epsilon_M}{h} $$\nwhere the constant $C$ depends on the operator and $|f(x_0)|$.\n\n**Optimal Step Size $h^\\star$**\nThe total error magnitude is dominated by the sum of truncation and round-off error magnitudes:\n$$ E(h) \\approx |E_T(h)| + |E_R(h)| $$\nFor the first-order scheme, $E(h) \\approx A h + B \\frac{\\epsilon_M}{h}$. For the second-order scheme, $E(h) \\approx A h^2 + B \\frac{\\epsilon_M}{h}$. As $h$ decreases, the truncation error term diminishes while the round-off error term grows. This trade-off implies the existence of an optimal step size $h^\\star$ where the total error $E(h)$ is minimized. This is the value the program must find.\n\n**Computational Procedure**\nThe solution requires a direct simulation for each test case.\n1. Define the parameters for each case: the function $f$, its analytical derivative $f'$, the point $x_0$, the operator, and the floating-point precision (binary$32$ or binary$64$). All numerical values used in a calculation must be cast to the specified precision.\n2. For each case, iterate $k$ from $1$ to $K=60$.\n3. In each iteration, compute the step size $h_k = 2^{-k}$.\n4. Calculate the numerical derivative $D_{h_k}f(x_0)$ using the appropriate formula.\n5. Compute the exact derivative value $f'(x_0)$ for reference.\n6. Determine the absolute error $E(h_k) = |D_{h_k}f(x_0) - f'(x_0)|$.\n7. After computing the list of errors $\\{E(h_k)\\}_{k=1}^{K}$, find the minimum value $E_{\\min}$.\n8. Identify all step sizes $h_k$ that produce this minimum error $E_{\\min}$.\n9. According to the problem's tie-breaking rule, select the largest step size from this set as the optimal value $h^\\star$. This corresponds to the smallest index $k$.\nThe following program implements this procedure.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the optimal finite difference step size h_star for a suite of test cases.\n\n    For each case, it iterates through a geometrically decreasing sequence of step sizes h_k,\n    calculates the absolute error of the finite difference approximation against the\n    exact derivative, and finds the step size h_star that minimizes this error. The calculation\n    is performed using the specified floating-point precision (binary32 or binary64).\n    \"\"\"\n    test_cases = [\n        {\n            \"f\": np.exp,\n            \"df\": np.exp,\n            \"x0\": 1.0,\n            \"operator\": \"C\",\n            \"dtype\": np.float64,\n            \"K\": 60,\n        },\n        {\n            \"f\": np.sin,\n            \"df\": np.cos,\n            \"x0\": 1.0,\n            \"operator\": \"F\",\n            \"dtype\": np.float64,\n            \"K\": 60,\n        },\n        {\n            \"f\": np.log,\n            \"df\": lambda x: 1/x,\n            \"x0\": 1.0,\n            \"operator\": \"C\",\n            \"dtype\": np.float32,\n            \"K\": 60,\n        },\n        {\n            \"f\": np.exp,\n            \"df\": np.exp,\n            \"x0\": 1.0,\n            \"operator\": \"F\",\n            \"dtype\": np.float32,\n            \"K\": 60,\n        }\n    ]\n\n    results = []\n\n    for case in test_cases:\n        f = case[\"f\"]\n        df = case[\"df\"]\n        dtype = case[\"dtype\"]\n        x0 = dtype(case[\"x0\"])\n        operator = case[\"operator\"]\n        K = case[\"K\"]\n\n        # Calculate the exact derivative at the specified precision\n        df_exact = df(x0)\n\n        errors_and_hs = []\n\n        # Iterate through the sequence of step sizes h_k = 2^-k\n        for k in range(1, K + 1):\n            k_p = dtype(k)\n            h = dtype(2.0)**(-k_p)\n\n            # Calculate the finite difference approximation\n            if operator == 'C':\n                f_plus = f(x0 + h)\n                f_minus = f(x0 - h)\n                df_approx = (f_plus - f_minus) / (dtype(2.0) * h)\n            elif operator == 'F':\n                f_plus = f(x0 + h)\n                f_zero = f(x0)\n                df_approx = (f_plus - f_zero) / h\n            else:\n                # This path should not be reached with valid problem inputs\n                continue\n\n            # Calculate the absolute error\n            error = np.abs(df_approx - df_exact)\n            errors_and_hs.append((error, h))\n\n        # Find the minimum error achieved\n        if not errors_and_hs:\n            # Handle case of empty list, though not expected here\n            results.append(None) # Or some other indicator of failure\n            continue\n            \n        min_error = min(e for e, h in errors_and_hs)\n        \n        # Find all h values that resulted in the minimum error\n        h_at_min_error = [h for e, h in errors_and_hs if e == min_error]\n        \n        # Per problem spec, choose the largest h if there's a tie\n        h_star = max(h_at_min_error)\n        results.append(h_star)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2421884"}]}