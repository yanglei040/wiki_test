{"hands_on_practices": [{"introduction": "The best way to truly understand an algorithm is to build it from scratch. This exercise guides you through implementing the Cholesky factorization, focusing on a critical aspect: its inherent ability to test for positive-definiteness. By applying your routine to matrices at the boundary of this property, you will gain a practical understanding of how the mathematical conditions for the factorization translate into a clear success or failure in code [@problem_id:2376407].", "problem": "You are to implement and analyze the Cholesky factorization for symmetric positive definite (SPD) systems from first principles. An $n \\times n$ real matrix $A$ is symmetric positive definite (SPD) if $A = A^{\\mathsf{T}}$ and $x^{\\mathsf{T}} A x gt; 0$ for all nonzero vectors $x \\in \\mathbb{R}^n$. A well-tested mathematical fact is that an SPD matrix admits a unique Cholesky factorization $A = L L^{\\mathsf{T}}$ with $L$ lower triangular and all diagonal entries of $L$ strictly positive. Another well-tested fact (Sylvester’s criterion) is that a symmetric matrix is SPD if and only if all leading principal minors are positive.\n\nYour tasks:\n\n1. Implement a routine that attempts to compute the Cholesky factorization $A = L L^{\\mathsf{T}}$ for a given symmetric matrix $A$ without any pivoting. The routine must explicitly check the positivity of each computed squared diagonal pivot, and declare failure if a nonpositive value arises at any step. The implementation must not rely on black-box SPD checks; it should detect failure based purely on the computed intermediate quantities.\n\n2. Use the following parametric family of symmetric matrices:\n   $$ A(r) = \\begin{bmatrix} 1  r \\\\ r  1 \\end{bmatrix}, $$\n   where $r \\in \\mathbb{R}$. This family contains matrices that move from SPD to non-SPD with small changes in the single off-diagonal element $r$. Evaluate your routine on the test suite of parameter values\n   $$ r \\in \\{\\, 0.9,\\; 0.999999,\\; 1.0,\\; 1.000001 \\,\\}. $$\n   Record a boolean for each test indicating whether the Cholesky routine succeeds (return $\\,\\text{True}\\,$) or fails (return $\\,\\text{False}\\,$).\n\n3. As an additional \"happy path\" coverage case, test the following fixed $3 \\times 3$ symmetric matrix known to be SPD by Sylvester’s criterion:\n   $$ B = \\begin{bmatrix} 4  1  1 \\\\ 1  3  1 \\\\ 1  1  2 \\end{bmatrix}. $$\n   Record a boolean indicating whether the Cholesky routine succeeds on $B$.\n\n4. Output specification. Your program must produce a single line of output containing a Python-style list literal with the booleans for all tests in the following order: the four cases $A(r)$ with $r$ equal to $0.9$, $0.999999$, $1.0$, $1.000001$, followed by the single case $B$. For example, the required format is:\n   $$ [\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4,\\text{result}_5] $$\n   where each $\\text{result}_i$ is either $\\text{True}$ or $\\text{False}$.\n\nNotes and constraints:\n\n- Base your reasoning on the definitions of SPD matrices and the existence condition for the Cholesky factorization stated above. Do not assume any unstated properties.\n- There are no physical units involved.\n- Angle units do not apply.\n- The only accepted output is the specified single-line list of booleans in the exact order described.", "solution": "The problem as stated is well-defined, self-contained, and scientifically sound, resting on established principles of numerical linear algebra. It is therefore valid, and we proceed to a full solution. The task requires the implementation of a Cholesky factorization routine and its application to a specified set of test matrices.\n\nThe fundamental principle is the factorization of a symmetric positive-definite (SPD) matrix $A$ into the product $A = LL^{\\mathsf{T}}$, where $L$ is a lower triangular matrix. The elements of $L$, denoted $L_{ij}$, can be computed directly from this equation. For an $n \\times n$ matrix $A$, the element $A_{ij}$ is given by the dot product of the $i$-th row of $L$ and the $j$-th column of $L^{\\mathsf{T}}$ (which is the $j$-th row of $L$):\n$$ A_{ij} = \\sum_{k=1}^{j} L_{ik} L_{jk} \\quad \\text{for } i \\ge j $$\nSince $A$ is symmetric, $A_{ij} = A_{ji}$, and we only need to compute $L_{ij}$ for $i \\ge j$.\n\nThe algorithm computes the columns of $L$ sequentially, from $j=1$ to $j=n$. For each column $j$, we first compute the diagonal element $L_{jj}$ and then the off-diagonal elements $L_{ij}$ for $i  j$.\n\nConsider the diagonal element $A_{jj}$:\n$$ A_{jj} = \\sum_{k=1}^{j} L_{jk}^2 = \\left( \\sum_{k=1}^{j-1} L_{jk}^2 \\right) + L_{jj}^2 $$\nFrom this, we solve for the squared diagonal element of $L$:\n$$ L_{jj}^2 = A_{jj} - \\sum_{k=1}^{j-1} L_{jk}^2 $$\nFor the Cholesky factorization to exist with a real-valued matrix $L$ having positive diagonal entries, the term $L_{jj}^2$ must be strictly positive at every step $j$. If $L_{jj}^2 \\le 0$ for any $j$, the matrix is not positive-definite, and the factorization fails. This is the condition our routine must check. If $L_{jj}^2  0$, we have $L_{jj} = \\sqrt{A_{jj} - \\sum_{k=1}^{j-1} L_{jk}^2}$.\n\nNext, consider the off-diagonal elements $A_{ij}$ for $i  j$:\n$$ A_{ij} = \\sum_{k=1}^{j} L_{ik} L_{jk} = \\left( \\sum_{k=1}^{j-1} L_{ik} L_{jk} \\right) + L_{ij} L_{jj} $$\nSolving for $L_{ij}$, we get:\n$$ L_{ij} = \\frac{1}{L_{jj}} \\left( A_{ij} - \\sum_{k=1}^{j-1} L_{ik} L_{jk} \\right) $$\nThis calculation is possible provided $L_{jj} \\ne 0$, which is guaranteed if $L_{jj}^2  0$.\n\nWe now analyze the specified test cases based on this algorithm.\n\n1.  **Parametric Family $A(r) = \\begin{bmatrix} 1  r \\\\ r  1 \\end{bmatrix}$**\n\n    We apply the algorithm for this $2 \\times 2$ matrix.\n    \n    For column $j=1$:\n    The squared diagonal is $L_{11}^2 = A_{11} = 1$. Since $1  0$, this step succeeds, and we find $L_{11} = 1$.\n    The off-diagonal element is $L_{21} = \\frac{1}{L_{11}} (A_{21}) = \\frac{r}{1} = r$.\n    \n    For column $j=2$:\n    The squared diagonal is $L_{22}^2 = A_{22} - L_{21}^2 = 1 - r^2$.\n    The factorization succeeds if and only if this quantity is strictly positive: $1 - r^2  0$, which is equivalent to $r^2  1$, or $|r|  1$.\n\n    We evaluate the specified values of $r$:\n    - For $r = 0.9$: $L_{22}^2 = 1 - (0.9)^2 = 1 - 0.81 = 0.19  0$. The routine succeeds. Result: $\\text{True}$.\n    - For $r = 0.999999$: $L_{22}^2 = 1 - (0.999999)^2  0$. The value is positive, although small. The routine succeeds. Result: $\\text{True}$.\n    - For $r = 1.0$: $L_{22}^2 = 1 - (1.0)^2 = 0$. This is not strictly positive. The routine must fail. Result: $\\text{False}$.\n    - For $r = 1.000001$: $L_{22}^2 = 1 - (1.000001)^2  0$. The routine must fail. Result: $\\text{False}$.\n\n2.  **Fixed Matrix $B = \\begin{bmatrix} 4  1  1 \\\\ 1  3  1 \\\\ 1  1  2 \\end{bmatrix}$**\n\n    We apply the algorithm to this $3 \\times 3$ matrix.\n\n    For column $j=1$:\n    $L_{11}^2 = B_{11} = 4$. Since $4  0$, we proceed. $L_{11} = 2$.\n    $L_{21} = B_{21} / L_{11} = 1/2$.\n    $L_{31} = B_{31} / L_{11} = 1/2$.\n\n    For column $j=2$:\n    $L_{22}^2 = B_{22} - L_{21}^2 = 3 - (1/2)^2 = 3 - 1/4 = 11/4$. Since $11/4  0$, we proceed. $L_{22} = \\sqrt{11}/2$.\n    $L_{32} = \\frac{1}{L_{22}}(B_{32} - L_{31}L_{21}) = \\frac{1}{\\sqrt{11}/2}(1 - (1/2)(1/2)) = \\frac{2}{\\sqrt{11}}(3/4) = \\frac{3}{2\\sqrt{11}}$.\n\n    For column $j=3$:\n    $L_{33}^2 = B_{33} - (L_{31}^2 + L_{32}^2) = 2 - \\left( (1/2)^2 + \\left(\\frac{3}{2\\sqrt{11}}\\right)^2 \\right) = 2 - \\left( \\frac{1}{4} + \\frac{9}{44} \\right) = 2 - \\left( \\frac{11}{44} + \\frac{9}{44} \\right) = 2 - \\frac{20}{44} = 2 - \\frac{5}{11} = \\frac{17}{11}$.\n    Since $17/11  0$, the final step succeeds.\n\n    As all squared diagonal pivots are strictly positive, the Cholesky factorization for matrix $B$ succeeds. Result: $\\text{True}$.\n\nThe sequence of boolean results for the five test cases in the specified order is therefore: $\\text{True}$, $\\text{True}$, $\\text{False}$, $\\text{False}$, $\\text{True}$. The program in the final answer will implement this logic.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the Cholesky factorization from first principles,\n    tests it on several matrices, and prints the results.\n    \"\"\"\n\n    def cholesky_factorization_attempt(A: np.ndarray) - bool:\n        \"\"\"\n        Attempts to compute the Cholesky factorization of a symmetric matrix A.\n\n        The routine follows the standard column-wise algorithm. It explicitly\n        checks for the strict positivity of the squared diagonal pivots (L_jj^2).\n        If a non-positive pivot is encountered, the matrix is not positive-definite,\n        and the factorization fails.\n\n        Args:\n            A (np.ndarray): The n x n symmetric matrix to factorize.\n\n        Returns:\n            bool: True if the factorization succeeds, False otherwise.\n        \"\"\"\n        n = A.shape[0]\n        L = np.zeros_like(A, dtype=float)\n\n        for j in range(n):\n            # Compute the sum of squares of elements in the j-th row of L up to column j-1.\n            # This corresponds to sum_{k=0}^{j-1} L[j, k]^2\n            s1 = np.dot(L[j, :j], L[j, :j])\n\n            # Compute the squared diagonal element L[j, j]^2.\n            squared_pivot = A[j, j] - s1\n\n            # The core condition for positive-definiteness in Cholesky factorization:\n            # The pivot must be strictly positive.\n            if squared_pivot = 0:\n                return False\n\n            L[j, j] = np.sqrt(squared_pivot)\n\n            # Compute the elements in the j-th column below the diagonal.\n            if j  n - 1:\n                # This corresponds to the sum sum_{k=0}^{j-1} L[i, k] * L[j, k] for each i  j.\n                s2 = np.dot(L[j + 1:n, :j], L[j, :j])\n                L[j + 1:n, j] = (A[j + 1:n, j] - s2) / L[j, j]\n\n        return True\n\n    # 1. Define the parametric test cases A(r).\n    r_values = [0.9, 0.999999, 1.0, 1.000001]\n    test_matrices = [np.array([[1.0, r], [r, 1.0]]) for r in r_values]\n\n    # 2. Define the fixed 3x3 test case B.\n    B_matrix = np.array([\n        [4.0, 1.0, 1.0],\n        [1.0, 3.0, 1.0],\n        [1.0, 1.0, 2.0]\n    ])\n    test_matrices.append(B_matrix)\n\n    # 3. Evaluate the routine on all test cases.\n    results = []\n    for matrix in test_matrices:\n        success = cholesky_factorization_attempt(matrix)\n        results.append(success)\n\n    # 4. Print the final results in the specified format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2376407"}, {"introduction": "Beyond the basic algorithm, understanding how the Cholesky factorization interacts with other matrix operations is key to its application. This practice explores a fundamental algebraic property: how the Cholesky factor of a matrix $A$ relates to the factor of a scaled matrix $DAD$, where $D$ is a diagonal matrix [@problem_id:2376398]. This concept is crucial in areas like preconditioning and coordinate transformations in engineering simulations.", "problem": "Let $A \\in \\mathbb{R}^{n \\times n}$ be symmetric positive-definite (SPD). By definition of the Cholesky factorization, there exists a unique lower-triangular matrix $L \\in \\mathbb{R}^{n \\times n}$ with strictly positive diagonal entries such that $A = L L^{\\top}$. Let $D = \\operatorname{diag}(d_{1}, d_{2}, \\dots, d_{n})$ be a diagonal matrix with $d_{i}  0$ for all $i \\in \\{1, 2, \\dots, n\\}$. Define $B = D A D$.\n\nDetermine, in closed form and in terms of $D$ and $L$, the Cholesky factor of $B$. Give your answer as a single matrix expression.", "solution": "The problem as stated is valid. It is a well-posed question in linear algebra, founded on standard definitions and properties of matrices. All necessary information is provided, and there are no internal contradictions or scientific inaccuracies. We may proceed with the solution.\n\nThe problem requires us to find the Cholesky factor of the matrix $B$, defined as $B = D A D$. Let this Cholesky factor be denoted by $\\tilde{L}$. By definition, $\\tilde{L}$ must be a lower-triangular matrix with strictly positive diagonal entries satisfying the equation $B = \\tilde{L} \\tilde{L}^{\\top}$. The existence and uniqueness of such a matrix $\\tilde{L}$ is guaranteed if and only if $B$ is a symmetric positive-definite (SPD) matrix. We must first verify this property for $B$.\n\nThe matrix $A$ is given to be symmetric, meaning $A^{\\top} = A$. The matrix $D$ is diagonal, so it is also symmetric, $D^{\\top} = D$.\nTo check for the symmetry of $B$, we compute its transpose:\n$$B^{\\top} = (DAD)^{\\top} = D^{\\top} A^{\\top} D^{\\top}$$\nSubstituting the symmetry properties of $A$ and $D$, we obtain:\n$$B^{\\top} = D A D = B$$\nThus, $B$ is a symmetric matrix.\n\nNext, we must verify that $B$ is positive-definite. A matrix $M$ is positive-definite if for any non-zero vector $x \\in \\mathbb{R}^{n}$, the quadratic form $x^{\\top} M x$ is strictly positive. For our matrix $B$, the quadratic form is:\n$$x^{\\top} B x = x^{\\top} (DAD) x$$\nUsing the property $(PQ)^{\\top} = Q^{\\top}P^{\\top}$, we can write this as:\n$$x^{\\top} B x = (Dx)^{\\top} A (Dx)$$\nLet us define a new vector $y = Dx$. The givens state that $D = \\operatorname{diag}(d_{1}, d_{2}, \\dots, d_{n})$ with $d_{i}  0$ for all $i$. A diagonal matrix with strictly positive diagonal entries is invertible. Therefore, if $x$ is a non-zero vector, then $y = Dx$ must also be a non-zero vector. If we assume $y=0$, then $Dx=0$. Since $D$ is invertible, we can multiply by $D^{-1}$ to get $D^{-1}Dx = D^{-1}0$, which implies $x=0$. This contradicts our initial assumption that $x$ is non-zero.\nSo, for any non-zero $x$, we have a corresponding non-zero vector $y$. The quadratic form becomes:\n$$x^{\\top} B x = y^{\\top} A y$$\nWe are given that $A$ is positive-definite, which means $y^{\\top} A y  0$ for any non-zero vector $y$. Since $y=Dx$ is non-zero for any non-zero $x$, it follows that $x^{\\top} B x  0$ for any non-zero $x$.\nThis proves that $B$ is positive-definite.\n\nSince $B$ is symmetric and positive-definite, it has a unique Cholesky factorization. Our task is to construct this factor, $\\tilde{L}$, in terms of $D$ and $L$, where $L$ is the Cholesky factor of $A$ such that $A = L L^{\\top}$.\n\nWe start from the definition of $B$ and substitute the factorization of $A$:\n$$B = D A D = D (L L^{\\top}) D$$\nBy the associativity of matrix multiplication, we can write:\n$$B = (DL) (L^{\\top} D)$$\nAs established earlier, $D$ is a diagonal matrix, so $D = D^{\\top}$. Substituting this into the expression gives:\n$$B = (DL) (L^{\\top} D^{\\top})$$\nUsing the property $(PQ)^{\\top} = Q^{\\top}P^{\\top}$ again, we recognize that $L^{\\top}D^{\\top} = (DL)^{\\top}$. Therefore, we have:\n$$B = (DL) (DL)^{\\top}$$\nThis expression has the form $\\tilde{L} \\tilde{L}^{\\top}$ if we define $\\tilde{L} = DL$. Now, we must verify that this candidate matrix $\\tilde{L}$ has the required properties of a Cholesky factor.\n\n$1$. Is $\\tilde{L}$ a lower-triangular matrix?\nThe matrix $L$ is lower-triangular, meaning its entries $L_{ij} = 0$ for $j  i$. The matrix $D$ is diagonal, with entries $D_{ik} = 0$ for $i \\neq k$ and $D_{ii} = d_i$. The $(i,j)$-th entry of the product $\\tilde{L} = DL$ is given by:\n$$(\\tilde{L})_{ij} = \\sum_{k=1}^{n} D_{ik} L_{kj}$$\nSince $D$ is diagonal, the only non-zero term in the summation is when $k=i$:\n$$(\\tilde{L})_{ij} = D_{ii} L_{ij} = d_i L_{ij}$$\nFor $\\tilde{L}$ to be lower-triangular, its entries must be zero for $j  i$. Since $L$ is lower-triangular, $L_{ij}=0$ for $j  i$. Consequently:\n$$(\\tilde{L})_{ij} = d_i \\cdot 0 = 0 \\quad \\text{for } j  i$$\nThis confirms that $\\tilde{L} = DL$ is a lower-triangular matrix.\n\n$2$. Does $\\tilde{L}$ have strictly positive diagonal entries?\nThe diagonal entries of $\\tilde{L}$ are the entries $(\\tilde{L})_{ii}$. Using the formula derived above:\n$$(\\tilde{L})_{ii} = d_i L_{ii}$$\nWe are given that $d_i  0$ for all $i \\in \\{1, 2, \\dots, n\\}$. We are also given that $L$, being a Cholesky factor, has strictly positive diagonal entries, so $L_{ii}  0$ for all $i$. The product of two strictly positive numbers is strictly positive. Therefore:\n$$(\\tilde{L})_{ii}  0 \\quad \\text{for all } i$$\nThis confirms that the diagonal entries of $\\tilde{L}$ are all strictly positive.\n\nWe have constructed a matrix $\\tilde{L} = DL$ which is lower-triangular with strictly positive diagonal entries and satisfies $B = \\tilde{L} \\tilde{L}^{\\top}$. By the uniqueness of the Cholesky factorization for a symmetric positive-definite matrix, this $\\tilde{L}$ must be the Cholesky factor of $B$.\n\nThe problem asks for the answer as a single matrix expression. The derived expression is $DL$.", "answer": "$$\n\\boxed{DL}\n$$", "id": "2376398"}, {"introduction": "Now that you have a grasp of the factorization process and its properties, it's time to see it in action. This exercise demonstrates how to use the Cholesky factor $L$ to compute the inverse of the original matrix $A$ efficiently and in a numerically stable manner [@problem_id:2376430]. This technique is far superior to direct inversion and showcases one of the primary reasons why Cholesky factorization is a cornerstone of computational engineering.", "problem": "You are given that a real, symmetric positive-definite (SPD) matrix $\\boldsymbol{A} \\in \\mathbb{R}^{n \\times n}$ admits a Cholesky factorization $\\boldsymbol{A} = \\boldsymbol{L}\\boldsymbol{L}^{\\mathsf{T}}$, where $\\boldsymbol{L}$ is a lower-triangular matrix with strictly positive diagonal entries. For each provided test case, you are given only the Cholesky factor $\\boldsymbol{L}$ (not $\\boldsymbol{A}$). Your task is to implement a program that, for each test case, computes the inverse matrix $\\boldsymbol{A}^{-1}$ using only $\\boldsymbol{L}$ and fundamental operations, without directly inverting $\\boldsymbol{A}$. After computing $\\boldsymbol{A}^{-1}$, verify correctness by forming $\\boldsymbol{A} = \\boldsymbol{L}\\boldsymbol{L}^{\\mathsf{T}}$ and evaluating the residual matrix $\\boldsymbol{R} = \\boldsymbol{A}\\boldsymbol{A}^{-1} - \\boldsymbol{I}$, where $\\boldsymbol{I}$ is the identity matrix of appropriate size. For each test case, report the maximum absolute entry of $\\boldsymbol{R}$, defined as $\\max_{i,j} |R_{ij}|$.\n\nUse the following test suite of lower-triangular Cholesky factors $\\boldsymbol{L}$:\n\n- Test case $1$ (boundary size $1 \\times 1$):\n  $$\\boldsymbol{L}_1 = \\begin{bmatrix} 3 \\end{bmatrix}.$$\n\n- Test case $2$ (size $2 \\times 2$):\n  $$\\boldsymbol{L}_2 = \\begin{bmatrix} 2  0 \\\\ 1  \\sqrt{2} \\end{bmatrix}.$$\n\n- Test case $3$ (size $3 \\times 3$):\n  $$\\boldsymbol{L}_3 = \\begin{bmatrix}\n  1.5  0  0 \\\\\n  0.4  2.0  0 \\\\\n  -0.3  0.5  1.2\n  \\end{bmatrix}.$$\n\n- Test case $4$ (size $4 \\times 4$, ill-scaled yet SPD):\n  $$\\boldsymbol{L}_4 = \\begin{bmatrix}\n  10^{-3}  0  0  0 \\\\\n  2\\times 10^{-4}  10^{-1}  0  0 \\\\\n  -10^{-4}  3\\times 10^{-2}  1.0  0 \\\\\n  5\\times 10^{-5}  -2\\times 10^{-2}  4\\times 10^{-1}  10.0\n  \\end{bmatrix}.$$\n\nProgram requirements:\n\n- For each test case, compute $\\boldsymbol{A}^{-1}$ using only the given $\\boldsymbol{L}$ and fundamental linear algebra operations, without directly inverting $\\boldsymbol{A}$.\n- For verification, construct $\\boldsymbol{A} = \\boldsymbol{L}\\boldsymbol{L}^{\\mathsf{T}}$ and compute the residual $\\boldsymbol{R} = \\boldsymbol{A}\\boldsymbol{A}^{-1} - \\boldsymbol{I}$.\n- For each test case, output the scalar $\\rho = \\max_{i,j} |R_{ij}|$.\n- The final output must be a single line containing all four $\\rho$ values, ordered as tests $1$ through $4$, rounded to $12$ decimal places, as a comma-separated list enclosed in square brackets, for example, \"$[r_1,r_2,r_3,r_4]$\".\n\nThere are no physical quantities in this problem, so no physical units are required. All angles, if any, are irrelevant to this task. Your program must not read any input; it should run as provided and produce the required single-line output.", "solution": "The problem statement is valid. It is a well-posed problem in numerical linear algebra, grounded in the established theory of matrix factorizations.\n\nThe task requires the computation of the inverse of a symmetric positive-definite (SPD) matrix $\\boldsymbol{A}$ given its Cholesky factor $\\boldsymbol{L}$, where $\\boldsymbol{A} = \\boldsymbol{L}\\boldsymbol{L}^{\\mathsf{T}}$. A critical constraint is that the inversion must be performed without directly computing the inverse of $\\boldsymbol{A}$. This directive is standard in high-performance scientific computing, as it points towards a method with superior numerical stability and computational efficiency.\n\nThe underlying principle is based on the algebraic properties of matrix inversion and transposition. The inverse of a product of two invertible matrices, $\\boldsymbol{X}$ and $\\boldsymbol{Y}$, follows the rule $(\\boldsymbol{X}\\boldsymbol{Y})^{-1} = \\boldsymbol{Y}^{-1}\\boldsymbol{X}^{-1}$. Furthermore, the inverse of a matrix transpose is equivalent to the transpose of the matrix inverse, expressed as $(\\boldsymbol{X}^{\\mathsf{T}})^{-1} = (\\boldsymbol{X}^{-1})^{\\mathsf{T}}$.\n\nApplying these principles to the Cholesky factorization $\\boldsymbol{A} = \\boldsymbol{L}\\boldsymbol{L}^{\\mathsf{T}}$, we can formally derive an expression for $\\boldsymbol{A}^{-1}$:\n$$\n\\boldsymbol{A}^{-1} = (\\boldsymbol{L}\\boldsymbol{L}^{\\mathsf{T}})^{-1}\n$$\nUsing the rule for the inverse of a product, we get:\n$$\n\\boldsymbol{A}^{-1} = (\\boldsymbol{L}^{\\mathsf{T}})^{-1} \\boldsymbol{L}^{-1}\n$$\nFinally, applying the transpose-inverse commutation rule to the term $(\\boldsymbol{L}^{\\mathsf{T}})^{-1}$ yields the desired expression:\n$$\n\\boldsymbol{A}^{-1} = (\\boldsymbol{L}^{-1})^{\\mathsf{T}} \\boldsymbol{L}^{-1}\n$$\nThis final equation provides the algorithm for computing $\\boldsymbol{A}^{-1}$. The procedure is numerically robust because it operates on the Cholesky factor $\\boldsymbol{L}$. Since $\\boldsymbol{L}$ is a lower-triangular matrix with strictly positive diagonal entries, it is always invertible. Its inverse, $\\boldsymbol{L}^{-1}$, is also lower-triangular and can be computed efficiently using a process equivalent to forward substitution to solve the matrix equation $\\boldsymbol{L}\\boldsymbol{X} = \\boldsymbol{I}$, where $\\boldsymbol{I}$ is the identity matrix.\n\nThis method is preferred because the condition number of the Cholesky factor, $\\kappa(\\boldsymbol{L})$, is the square root of the condition number of the original matrix, $\\kappa(\\boldsymbol{A})$, i.e., $\\kappa(\\boldsymbol{L}) = \\sqrt{\\kappa(\\boldsymbol{A})}$. Inverting $\\boldsymbol{L}$ is therefore a much better-conditioned numerical problem than directly inverting $\\boldsymbol{A}$, which is particularly important for ill-scaled matrices like the one in test case $4$.\n\nThe algorithm implemented to solve this problem is as follows:\n1.  For each given lower-triangular Cholesky factor $\\boldsymbol{L}$, compute its inverse, which we can denote as $\\boldsymbol{M} = \\boldsymbol{L}^{-1}$.\n2.  Calculate the inverse of the original matrix $\\boldsymbol{A}$ using the derived formula: $\\boldsymbol{A}^{-1} = \\boldsymbol{M}^{\\mathsf{T}}\\boldsymbol{M}$.\n3.  For the purpose of verification, explicitly form the matrix $\\boldsymbol{A}$ by computing the product $\\boldsymbol{A} = \\boldsymbol{L}\\boldsymbol{L}^{\\mathsf{T}}$.\n4.  Calculate the residual matrix $\\boldsymbol{R} = \\boldsymbol{A}\\boldsymbol{A}^{-1} - \\boldsymbol{I}$. Ideally, $\\boldsymbol{R}$ should be the zero matrix, but due to floating-point arithmetic, its entries will be small non-zero values.\n5.  The quality of the computed inverse is assessed by finding the maximum absolute entry of the residual matrix, $\\rho = \\max_{i,j} |R_{ij}|$. This value is reported for each test case.\n\nThe provided program executes these steps for each test case using the `numpy` library for the fundamental linear algebra operations.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the inverse of a matrix A from its Cholesky factor L\n    and reports the maximum absolute residual.\n    \"\"\"\n    #\n    # Execution Environment:\n    # language: Python\n    # version: 3.12\n    # libraries:\n    #     - name: numpy, version: 1.23.5\n    #     - name: scipy, version: 1.11.4\n    #\n\n    # Define the test cases from the problem statement.\n    # L1: 1x1 boundary case\n    L1 = np.array([[3.0]])\n\n    # L2: 2x2 case\n    L2 = np.array([\n        [2.0, 0.0],\n        [1.0, np.sqrt(2)]\n    ])\n\n    # L3: 3x3 case\n    L3 = np.array([\n        [1.5, 0.0, 0.0],\n        [0.4, 2.0, 0.0],\n        [-0.3, 0.5, 1.2]\n    ])\n\n    # L4: 4x4 ill-scaled case\n    L4 = np.array([\n        [1.0e-3, 0.0,    0.0,  0.0],\n        [2.0e-4, 1.0e-1, 0.0,  0.0],\n        [-1.0e-4, 3.0e-2, 1.0,  0.0],\n        [5.0e-5, -2.0e-2, 0.4, 10.0]\n    ])\n\n    test_cases = [L1, L2, L3, L4]\n\n    results = []\n    for L in test_cases:\n        # The problem requires computing A_inv from L without inverting a dense A.\n        # The method is A_inv = (L_inv)^T * L_inv, where L_inv is the inverse of L.\n        # Inverting a triangular matrix L is a fundamental and stable operation.\n        \n        # Step 1: Compute the inverse of the lower-triangular matrix L.\n        L_inv = np.linalg.inv(L)\n        \n        # Step 2: Compute A_inv using the formula A_inv = (L_inv.T) @ L_inv.\n        A_inv = L_inv.T @ L_inv\n        \n        # Step 3 (Verification): Form A = L @ L.T\n        A = L @ L.T\n        \n        # Step 4 (Verification): Compute the residual matrix R = A * A_inv - I\n        I = np.identity(A.shape[0])\n        R = A @ A_inv - I\n        \n        # Step 5: Find the maximum absolute value of the entries in R.\n        rho = np.max(np.abs(R))\n        \n        results.append(rho)\n\n    # Final print statement in the exact required format.\n    # Output must be a single line: [r_1,r_2,r_3,r_4] with 12 decimal places.\n    formatted_results = [f\"{r:.12f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2376430"}]}