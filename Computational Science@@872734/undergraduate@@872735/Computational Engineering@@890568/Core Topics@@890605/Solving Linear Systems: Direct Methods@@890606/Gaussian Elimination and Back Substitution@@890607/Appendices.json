{"hands_on_practices": [{"introduction": "The success or failure of Gaussian elimination is deeply connected to the physical properties of the system being modeled. This exercise provides a tangible link between the abstract mathematical concept of a singular matrix and the concrete engineering problem of structural stability. By assembling a stiffness matrix for a simple truss, you will see firsthand how removing a critical member creates a mechanism, resulting in a singular matrix where Gaussian elimination would fail to find a unique solution [@problem_id:2396187].", "problem": "A planar pin-jointed structure consists of a single free joint $P$ at coordinates $(0,0)$ connected to two fixed anchors $A$ and $B$ by straight, axially deformable members that obey Hooke’s law in their axial direction. The fixed anchors are located at $A=(1,0)$ and $B=(1,1)$. Each member has Young’s modulus $E=210\\times 10^{9}$ and cross-sectional area $A=1.0\\times 10^{-4}$, and each member’s axial stiffness equals $k=\\dfrac{EA}{L}$, where $L$ is its undeformed length. The global displacement of $P$ is $u=\\begin{pmatrix}u_x\\\\ u_y\\end{pmatrix}$ in Cartesian components. The fixed anchors do not move.\n\nAssume the member $PB$ is removed from the structure while all other data are unchanged. Using only first principles of axial member behavior and equilibrium, assemble the reduced global stiffness matrix $K\\in\\mathbb{R}^{2\\times 2}$ that relates the nodal force at $P$ to its displacement $u$ for the modified structure, and then compute the determinant of $K$.\n\nProvide your final answer as a single real number. Express the determinant in $\\mathrm{N}^2\\,\\mathrm{m}^{-2}$. No rounding is required.", "solution": "The problem statement is examined and found to be valid. It is a well-posed problem grounded in the fundamental principles of linear structural mechanics, with all necessary information provided and no internal contradictions. It requests the assembly of a stiffness matrix for a simple structure and the calculation of its determinant.\n\nThe modified structure consists of a single plane truss member, which we denote as member $PA$. This member connects a free joint $P$ at coordinates $(0,0)$ to a fixed anchor $A$ at coordinates $(1,0)$. The displacement of joint $P$ is represented by the vector $u = \\begin{pmatrix} u_x \\\\ u_y \\end{pmatrix}$.\n\nFirst, we determine the geometric and material properties of the member $PA$. The undeformed length, $L$, of the member is the Euclidean distance between points $P$ and $A$:\n$$ L = \\sqrt{(1-0)^2 + (0-0)^2} = \\sqrt{1^2} = 1 \\, \\mathrm{m} $$\nThe member is oriented along the global x-axis. The axial stiffness, $k$, of the member is defined as $k = \\frac{EA}{L}$. With the given Young's modulus $E = 210 \\times 10^9 \\, \\mathrm{N/m^2}$ and cross-sectional area $A = 1.0 \\times 10^{-4} \\, \\mathrm{m^2}$, we compute the stiffness:\n$$ k = \\frac{(210 \\times 10^{9} \\, \\mathrm{N/m^2}) \\cdot (1.0 \\times 10^{-4} \\, \\mathrm{m^2})}{1 \\, \\mathrm{m}} = 2.1 \\times 10^7 \\, \\mathrm{N/m} $$\n\nWe now derive the reduced global stiffness matrix $K \\in \\mathbb{R}^{2 \\times 2}$ from first principles. This matrix relates the external forces $F_P = \\begin{pmatrix} F_x \\\\ F_y \\end{pmatrix}$ applied at node $P$ to the nodal displacements $u$. The derivation proceeds in three steps: kinematics, constitutive law, and equilibrium.\n\n1.  Kinematics: We relate the member's elongation, $\\Delta L$, to the nodal displacement $u = (u_x, u_y)$. The new position of node $P$ is $(u_x, u_y)$, while node $A$ remains at $(1,0)$. The new length $L'$ is $L' = \\sqrt{(1-u_x)^2 + (-u_y)^2}$. For small displacements, we linearize this expression. The elongation is $\\Delta L = L' - L = \\sqrt{1 - 2u_x + u_x^2 + u_y^2} - 1$. Using the first-order Taylor approximation $\\sqrt{1+\\epsilon} \\approx 1 + \\frac{1}{2}\\epsilon$, and neglecting second-order terms in displacement ($u_x^2, u_y^2$), we get:\n    $$ \\Delta L \\approx \\sqrt{1-2u_x} - 1 \\approx \\left(1 - \\frac{1}{2}(2u_x)\\right) - 1 = -u_x $$\n\n2.  Constitutive Law: According to Hooke's law, the axial tensile force $T$ in the member is directly proportional to its elongation: $T = k \\cdot \\Delta L$. Substituting the expression for $\\Delta L$:\n    $$ T = k(-u_x) = -k u_x $$\n\n3.  Equilibrium: The force exerted by the member on node $P$, denoted $\\vec{F}_{\\text{member}}$, must be balanced by the external force $F_P = (F_x, F_y)$. If the member is in tension ($T > 0$), it pulls node $P$ towards node $A$, which is in the positive x-direction. Thus, the vector force from the member on the node is $\\vec{F}_{\\text{member}} = (T, 0)$ since the member lies on the x-axis. Substituting the expression for $T$:\n    $$ \\vec{F}_{\\text{member}} = (-k u_x, 0) $$\n    For equilibrium at node $P$, the sum of forces must be zero: $F_P + \\vec{F}_{\\text{member}} = \\vec{0}$.\n    $$ \\begin{pmatrix} F_x \\\\ F_y \\end{pmatrix} + \\begin{pmatrix} -k u_x \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} $$\n    Rearranging gives the relationship between the external force and the displacement:\n    $$ \\begin{pmatrix} F_x \\\\ F_y \\end{pmatrix} = \\begin{pmatrix} k u_x \\\\ 0 \\end{pmatrix} $$\n    This can be expressed in the matrix form $F_P = K u$:\n    $$ \\begin{pmatrix} F_x \\\\ F_y \\end{pmatrix} = \\begin{pmatrix} k & 0 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} u_x \\\\ u_y \\end{pmatrix} $$\nThe reduced global stiffness matrix $K$ is therefore:\n$$ K = \\begin{pmatrix} k & 0 \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 2.1 \\times 10^7 & 0 \\\\ 0 & 0 \\end{pmatrix} \\, \\mathrm{N/m} $$\n\nFinally, we compute the determinant of the matrix $K$. The determinant of a $2 \\times 2$ matrix $\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$ is $ad-bc$.\n$$ \\det(K) = \\det\\begin{pmatrix} 2.1 \\times 10^7 & 0 \\\\ 0 & 0 \\end{pmatrix} = (2.1 \\times 10^7) \\cdot 0 - 0 \\cdot 0 = 0 $$\nThe units of the determinant are $(\\mathrm{N/m})^2 = \\mathrm{N}^2\\,\\mathrm{m}^{-2}$. A determinant of zero signifies that the matrix is singular. This is physically correct, as the structure provides no stiffness in the y-direction and is therefore a mechanism.", "answer": "$$\\boxed{0}$$", "id": "2396187"}, {"introduction": "Computational efficiency is a cornerstone of engineering analysis, and a key principle is to never compute the same thing twice if you can avoid it. This practice demonstrates the power of the $PA=LU$ factorization, the result of Gaussian elimination, as a reusable tool. You will learn how to solve the related system $A^T x = b$ not by re-inverting or re-factorizing, but by cleverly manipulating the existing $L$, $U$, and $P$ matrices, showcasing a more elegant and efficient computational path [@problem_id:2396194].", "problem": "Let $A \\in \\mathbb{R}^{3 \\times 3}$. You are given the Lower-Upper (LU) factorization with row pivoting $P A = L U$, where $P$ is a permutation matrix, $L$ is unit lower triangular, and $U$ is upper triangular:\n$$\nP = \\begin{pmatrix}\n0 & 1 & 0 \\\\\n1 & 0 & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix},\\quad\nL = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n3 & 1 & 0 \\\\\n2 & -1 & 1\n\\end{pmatrix},\\quad\nU = \\begin{pmatrix}\n2 & -1 & 1 \\\\\n0 & 4 & 2 \\\\\n0 & 0 & 3\n\\end{pmatrix}.\n$$\nConsider the linear system $A^{T} x = b$ with\n$$\nb = \\begin{pmatrix} 10 \\\\ 7 \\\\ 8 \\end{pmatrix}.\n$$\nCompute the second component $x_{2}$ of the unique solution $x$. Provide the exact value of $x_{2}$.", "solution": "The problem statement is subjected to validation.\n\n**Step 1: Extract Givens**\nThe given information is:\n- The LU factorization with row pivoting is $P A = L U$, where $A \\in \\mathbb{R}^{3 \\times 3}$.\n- The permutation matrix $P$ is:\n$$\nP = \\begin{pmatrix}\n0 & 1 & 0 \\\\\n1 & 0 & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix}\n$$\n- The unit lower triangular matrix $L$ is:\n$$\nL = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n3 & 1 & 0 \\\\\n2 & -1 & 1\n\\end{pmatrix}\n$$\n- The upper triangular matrix $U$ is:\n$$\nU = \\begin{pmatrix}\n2 & -1 & 1 \\\\\n0 & 4 & 2 \\\\\n0 & 0 & 3\n\\end{pmatrix}\n$$\n- The linear system to be solved is $A^{T} x = b$.\n- The vector $b$ is:\n$$\nb = \\begin{pmatrix} 10 \\\\ 7 \\\\ 8 \\end{pmatrix}\n$$\n- The objective is to compute the second component, $x_{2}$, of the solution vector $x$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, rooted in the standard theory of numerical linear algebra, specifically LU decomposition and solving linear systems. The matrices provided are well-defined: $P$ is a valid permutation matrix, $L$ is a unit lower triangular matrix, and $U$ is an upper triangular matrix with non-zero diagonal elements, which ensures its invertibility. Since $P$, $L$, and $U$ are all invertible, $A = P^{-1}LU$ is also invertible, and consequently $A^T$ is invertible. Therefore, the system $A^T x = b$ has a unique solution. The problem is self-contained, consistent, and well-posed. No flaws are identified.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be provided.\n\n**Solution Derivation**\nThe starting point is the given factorization $P A = L U$. We need to solve the linear system $A^{T} x = b$. First, we must express $A^T$ in terms of the given matrices $P$, $L$, and $U$.\nTaking the transpose of the factorization equation:\n$$\n(P A)^{T} = (L U)^{T}\n$$\nUsing the property that $(XY)^{T} = Y^{T}X^{T}$, we have:\n$$\nA^{T} P^{T} = U^{T} L^{T}\n$$\nTo isolate $A^T$, we right-multiply by the inverse of $P^T$, which is $(P^T)^{-1}$.\n$$\nA^{T} = U^{T} L^{T} (P^{T})^{-1}\n$$\nFor a permutation matrix $P$, its inverse is its transpose, $P^{-1} = P^{T}$. Therefore, $(P^{T})^{-1} = (P^{-1})^{-1} = P$.\nSubstituting this into the expression for $A^T$:\n$$\nA^{T} = U^{T} L^{T} P\n$$\nNow we substitute this into the linear system $A^{T} x = b$:\n$$\n(U^{T} L^{T} P) x = b\n$$\nThis equation can be solved efficiently as a sequence of three simpler problems by introducing intermediate vectors. Let $y = Px$ and $z = L^T y$. The system decomposes into the following steps:\n1.  Solve $U^{T} z = b$ for the vector $z$.\n2.  Solve $L^{T} y = z$ for the vector $y$.\n3.  Solve $y = Px$ for the final solution vector $x$.\n\n**Step 1: Solve $U^{T} z = b$**\nFirst, we find the transpose of $U$:\n$$\nU^{T} = \\begin{pmatrix}\n2 & 0 & 0 \\\\\n-1 & 4 & 0 \\\\\n1 & 2 & 3\n\\end{pmatrix}\n$$\nThe system is $U^{T} z = b$:\n$$\n\\begin{pmatrix}\n2 & 0 & 0 \\\\\n-1 & 4 & 0 \\\\\n1 & 2 & 3\n\\end{pmatrix}\n\\begin{pmatrix}\nz_{1} \\\\\nz_{2} \\\\\nz_{3}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n10 \\\\\n7 \\\\\n8\n\\end{pmatrix}\n$$\nThis is a lower triangular system, solvable by forward substitution.\nFrom the first row: $2 z_{1} = 10 \\implies z_{1} = 5$.\nFrom the second row: $-z_{1} + 4 z_{2} = 7 \\implies -5 + 4 z_{2} = 7 \\implies 4 z_{2} = 12 \\implies z_{2} = 3$.\nFrom the third row: $z_{1} + 2 z_{2} + 3 z_{3} = 8 \\implies 5 + 2(3) + 3 z_{3} = 8 \\implies 11 + 3 z_{3} = 8 \\implies 3 z_{3} = -3 \\implies z_{3} = -1$.\nThus, the vector $z$ is:\n$$\nz = \\begin{pmatrix} 5 \\\\ 3 \\\\ -1 \\end{pmatrix}\n$$\n\n**Step 2: Solve $L^{T} y = z$**\nNext, we find the transpose of $L$:\n$$\nL^{T} = \\begin{pmatrix}\n1 & 3 & 2 \\\\\n0 & 1 & -1 \\\\\n0 & 0 & 1\n\\end{pmatrix}\n$$\nThe system is $L^{T} y = z$:\n$$\n\\begin{pmatrix}\n1 & 3 & 2 \\\\\n0 & 1 & -1 \\\\\n0 & 0 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\ny_{1} \\\\\ny_{2} \\\\\ny_{3}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n5 \\\\\n3 \\\\\n-1\n\\end{pmatrix}\n$$\nThis is an upper triangular system, solvable by back substitution.\nFrom the third row: $y_{3} = -1$.\nFrom the second row: $y_{2} - y_{3} = 3 \\implies y_{2} - (-1) = 3 \\implies y_{2} = 2$.\nFrom the first row: $y_{1} + 3 y_{2} + 2 y_{3} = 5 \\implies y_{1} + 3(2) + 2(-1) = 5 \\implies y_{1} + 4 = 5 \\implies y_{1} = 1$.\nThus, the vector $y$ is:\n$$\ny = \\begin{pmatrix} 1 \\\\ 2 \\\\ -1 \\end{pmatrix}\n$$\n\n**Step 3: Solve $y = Px$ for $x$**\nThe final step is to find $x$ from $y = Px$. This is equivalent to $x = P^{-1}y$. As established earlier, $P^{-1} = P^{T}$. The matrix $P$ is symmetric, so $P^T = P$.\n$$\nx = P y = \\begin{pmatrix}\n0 & 1 & 0 \\\\\n1 & 0 & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n1 \\\\\n2 \\\\\n-1\n\\end{pmatrix}\n$$\nPerforming the matrix-vector multiplication:\n$$\nx_{1} = (0)(1) + (1)(2) + (0)(-1) = 2\n$$\n$$\nx_{2} = (1)(1) + (0)(2) + (0)(-1) = 1\n$$\n$$\nx_{3} = (0)(1) + (0)(2) + (1)(-1) = -1\n$$\nThe solution vector is:\n$$\nx = \\begin{pmatrix} 2 \\\\ 1 \\\\ -1 \\end{pmatrix}\n$$\nThe problem asks for the second component of the solution, which is $x_{2}$.\nThe value of $x_2$ is $1$.", "answer": "$$\\boxed{1}$$", "id": "2396194"}, {"introduction": "In the world of numerical computation, our solutions are often approximations limited by finite-precision arithmetic, especially when dealing with ill-conditioned systems. This hands-on exercise introduces iterative refinement, a powerful and practical technique to improve the accuracy of an initial solution obtained from Gaussian elimination. By calculating the residual and solving for a correction, you will learn a fundamental method for systematically reducing numerical error and achieving more reliable results in your engineering simulations [@problem_id:2396227].", "problem": "A square linear system is given by the pair $(A,b)$, where $A \\in \\mathbb{R}^{n \\times n}$ is nonsingular and $b \\in \\mathbb{R}^n$. For each test case below, an initial approximation $x_0 \\in \\mathbb{R}^n$ is also provided. Define the residual $r \\in \\mathbb{R}^n$ by $r = b - A x_0$. Define the correction $\\delta x \\in \\mathbb{R}^n$ as the unique solution of the linear system $A \\, \\delta x = r$. Define the refined approximation $x_1 \\in \\mathbb{R}^n$ by $x_1 = x_0 + \\delta x$. Your task is to compute $x_1$ for each test case.\n\nAll computations are purely mathematical; no physical units are involved.\n\nUse the following test suite. In each case, $A$ and $b$ specify the linear system, and $x_0$ is the initial approximation.\n\n- Test case $1$ (well-conditioned $3 \\times 3$ system):\n  - $\n    A_1 =\n    \\begin{bmatrix}\n    4 & 2 & 0 \\\\\n    2 & 5 & 1 \\\\\n    0 & 1 & 3\n    \\end{bmatrix}, \\quad\n    b_1 =\n    \\begin{bmatrix}\n    6 \\\\\n    9 \\\\\n    7\n    \\end{bmatrix}, \\quad\n    x_{0,1} =\n    \\begin{bmatrix}\n    1.2 \\\\\n    0.8 \\\\\n    1.9\n    \\end{bmatrix}.\n  $\n- Test case $2$ (requires row pivoting in the first column):\n  - $\n    A_2 =\n    \\begin{bmatrix}\n    0 & 1 & 2 \\\\\n    1 & 0 & 3 \\\\\n    4 & 1 & 8\n    \\end{bmatrix}, \\quad\n    b_2 =\n    \\begin{bmatrix}\n    0 \\\\\n    -2 \\\\\n    -2\n    \\end{bmatrix}, \\quad\n    x_{0,2} =\n    \\begin{bmatrix}\n    0.9 \\\\\n    1.8 \\\\\n    -1.2\n    \\end{bmatrix}.\n  $\n- Test case $3$ (ill-conditioned Hilbert system of size $5$):\n  - $\n    A_3 =\n    \\begin{bmatrix}\n    1 & \\tfrac{1}{2} & \\tfrac{1}{3} & \\tfrac{1}{4} & \\tfrac{1}{5} \\\\\n    \\tfrac{1}{2} & \\tfrac{1}{3} & \\tfrac{1}{4} & \\tfrac{1}{5} & \\tfrac{1}{6} \\\\\n    \\tfrac{1}{3} & \\tfrac{1}{4} & \\tfrac{1}{5} & \\tfrac{1}{6} & \\tfrac{1}{7} \\\\\n    \\tfrac{1}{4} & \\tfrac{1}{5} & \\tfrac{1}{6} & \\tfrac{1}{7} & \\tfrac{1}{8} \\\\\n    \\tfrac{1}{5} & \\tfrac{1}{6} & \\tfrac{1}{7} & \\tfrac{1}{8} & \\tfrac{1}{9}\n    \\end{bmatrix}, \\quad\n    b_3 = A_3 \\cdot\n    \\begin{bmatrix}\n    1 \\\\\n    1 \\\\\n    1 \\\\\n    1 \\\\\n    1\n    \\end{bmatrix}\n    , \\quad\n    x_{0,3} =\n    \\begin{bmatrix}\n    0.9 \\\\\n    1.1 \\\\\n    0.8 \\\\\n    1.2 \\\\\n    0.7\n    \\end{bmatrix}.\n  $\n- Test case $4$ (nearly singular $2 \\times 2$ system with very small leading coefficient):\n  - $\n    A_4 =\n    \\begin{bmatrix}\n    10^{-10} & 1 \\\\\n    1 & 1\n    \\end{bmatrix}, \\quad\n    b_4 = A_4 \\cdot\n    \\begin{bmatrix}\n    1 \\\\\n    1\n    \\end{bmatrix}\n    =\n    \\begin{bmatrix}\n    1.0000000001 \\\\\n    2\n    \\end{bmatrix}\n    , \\quad\n    x_{0,4} =\n    \\begin{bmatrix}\n    0.5 \\\\\n    0.5\n    \\end{bmatrix}.\n  $\n\nYour program must process all four test cases and output the refined approximations $x_1$ for each case. The required final output format is a single line containing the list of refined vectors as a bracketed, comma-separated list of lists, where each component is rounded to $10$ decimal places using standard rounding and written in decimal notation (for example, $1$ should be printed as $1.0000000000$). For instance, an output for two vectors might look like $[[1.0000000000,2.0000000000],[3.5000000000,4.2500000000]]$. Ensure there are no spaces in the printed line.", "solution": "The problem has been analyzed and is determined to be valid. It is a well-posed, scientifically grounded problem in the field of computational engineering, specifically concerning numerical linear algebra. All necessary data and definitions are provided, and there are no internal contradictions or ambiguities. We shall proceed with the solution.\n\nThe problem requires the computation of a single step of iterative refinement for a linear system of equations of the form $A x = b$, where $A \\in \\mathbb{R}^{n \\times n}$ is a nonsingular matrix and $b, x \\in \\mathbb{R}^n$. Given an initial approximation $x_0 \\in \\mathbb{R}^n$, the goal is to compute a refined approximation, $x_1$.\n\nThe procedure for iterative refinement is based on a three-step process designed to reduce the error in the approximation. Let the true solution be denoted by $x$. The error in the initial approximation $x_0$ is $e_0 = x - x_0$. The fundamental principle is to compute an approximation to this error and use it to correct $x_0$.\n\nThe steps are as follows:\n\n1.  **Compute the Residual**: The residual vector, $r \\in \\mathbb{R}^n$, is defined as the difference between the given right-hand side $b$ and the result of applying the matrix $A$ to the current approximation $x_0$.\n    $$\n    r = b - A x_0\n    $$\n    The residual is a measure of how well $x_0$ satisfies the original equation. Note the relationship between the residual and the error:\n    $$\n    r = b - A x_0 = A x - A x_0 = A (x - x_0) = A e_0\n    $$\n    This shows that the residual is the image of the true error under the linear transformation $A$.\n\n2.  **Solve for the Correction**: We seek a correction vector, $\\delta x \\in \\mathbb{R}^n$, which, when added to $x_0$, yields a better approximation to $x$. Based on the relationship derived above, we can solve for the true error $e_0$ by solving the linear system $A e_0 = r$. In practice, we solve for an approximation of the error, which we call the correction $\\delta x$:\n    $$\n    A \\, \\delta x = r\n    $$\n    This is a linear system with the same matrix $A$ as the original problem but with the residual vector $r$ as the right-hand side. The solution $\\delta x$ represents the computed correction needed for $x_0$. The accurate solution of this system is paramount. For a general matrix $A$, a numerically stable method such as Gaussian elimination with partial pivoting (typically implemented via LU decomposition) must be used. This is critical for handling systems where pivots may be zero or very small, and for mitigating the propagation of numerical errors in ill-conditioned systems, such as the Hilbert matrix in Test Case $3$.\n\n3.  **Update the Approximation**: The refined approximation, $x_1 \\in \\mathbb{R}^n$, is obtained by adding the computed correction vector $\\delta x$ to the initial approximation $x_0$.\n    $$\n    x_1 = x_0 + \\delta x\n    $$\n    If the correction system $A\\, \\delta x = r$ is solved with sufficient accuracy, and the residual $r$ is computed with high precision, the new approximation $x_1$ will be closer to the true solution $x$ than $x_0$ was. In the idealized case of exact arithmetic, since $\\delta x = e_0 = x - x_0$, the update yields $x_1 = x_0 + (x-x_0) = x$, the exact solution, in a single step. However, in finite-precision arithmetic, rounding errors in the computation of both $r$ and $\\delta x$ mean that $x_1$ is only an improved, not an exact, solution.\n\nThis three-step procedure will be applied to each of the four test cases provided. For each case, the given matrix $A$, vector $b$, and initial approximation $x_0$ will be used to compute the residual $r$, solve for the correction $\\delta x$, and finally calculate the refined solution $x_1$. The numerical computations will be performed using standard double-precision floating-point arithmetic.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes one step of iterative refinement for four different linear systems.\n    \"\"\"\n    # Define test cases\n    # Test case 1: Well-conditioned 3x3 system\n    A1 = np.array([[4, 2, 0], [2, 5, 1], [0, 1, 3]], dtype=np.float64)\n    b1 = np.array([6, 9, 7], dtype=np.float64)\n    x0_1 = np.array([1.2, 0.8, 1.9], dtype=np.float64)\n\n    # Test case 2: Requires row pivoting\n    A2 = np.array([[0, 1, 2], [1, 0, 3], [4, 1, 8]], dtype=np.float64)\n    b2 = np.array([0, -2, -2], dtype=np.float64)\n    x0_2 = np.array([0.9, 1.8, -1.2], dtype=np.float64)\n\n    # Test case 3: Ill-conditioned 5x5 Hilbert matrix\n    n3 = 5\n    A3 = np.array([[1.0 / (i + j + 1) for j in range(n3)] for i in range(n3)], dtype=np.float64)\n    x_exact_3 = np.ones(n3, dtype=np.float64)\n    b3 = A3 @ x_exact_3\n    x0_3 = np.array([0.9, 1.1, 0.8, 1.2, 0.7], dtype=np.float64)\n\n    # Test case 4: Nearly singular 2x2 system\n    A4 = np.array([[1e-10, 1], [1, 1]], dtype=np.float64)\n    x_exact_4 = np.ones(2, dtype=np.float64)\n    b4 = A4 @ x_exact_4\n    x0_4 = np.array([0.5, 0.5], dtype=np.float64)\n\n    test_cases = [\n        (A1, b1, x0_1),\n        (A2, b2, x0_2),\n        (A3, b3, x0_3),\n        (A4, b4, x0_4),\n    ]\n\n    results = []\n    for A, b, x0 in test_cases:\n        # Step 1: Compute the residual r = b - A*x0\n        r = b - A @ x0\n\n        # Step 2: Solve the linear system A*dx = r for the correction dx\n        delta_x = np.linalg.solve(A, r)\n\n        # Step 3: Compute the refined approximation x1 = x0 + dx\n        x1 = x0 + delta_x\n        results.append(x1)\n\n    # Format the output string as per problem specification\n    # e.g., [[1.0,2.0],[3.5,4.25]] -> \"[[1.0000000000,2.0000000000],[3.5000000000,4.2500000000]]\"\n    # No spaces, 10 decimal places.\n    formatted_vectors = []\n    for vec in results:\n        formatted_components = [f\"{val:.10f}\" for val in vec]\n        formatted_vectors.append(f\"[{','.join(formatted_components)}]\")\n    \n    final_output_string = f\"[{','.join(formatted_vectors)}]\"\n    print(final_output_string)\n\nsolve()\n```", "id": "2396227"}]}