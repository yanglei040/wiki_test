{"hands_on_practices": [{"introduction": "The best way to understand an algorithm is to implement it. This first exercise guides you through coding the Thomas algorithm, not for a generic dense matrix, but using a \"packed\" storage format that is highly memory-efficient [@problem_id:2446352]. This practice builds a foundational understanding of both the algorithm's procedural steps and the data structures that best complement its sparse nature.", "problem": "You are given the task of solving linear systems whose coefficient matrices are tridiagonal. A real tridiagonal matrix $A \\in \\mathbb{R}^{n \\times n}$ is defined by nonzero elements only on the main diagonal and the two adjacent diagonals. You must adopt and use a packed storage layout consisting of exactly three $1$-dimensional arrays:\n- A subdiagonal array $l \\in \\mathbb{R}^{n-1}$,\n- A main diagonal array $d \\in \\mathbb{R}^{n}$,\n- A superdiagonal array $u \\in \\mathbb{R}^{n-1}$.\n\nThe packed storage must encode $A$ with the following indexing conventions:\n- For $i \\in \\{0,1,\\dots,n-2\\}$, $l[i]$ stores the matrix entry $A_{i+1,i}$,\n- For $i \\in \\{0,1,\\dots,n-1\\}$, $d[i]$ stores the matrix entry $A_{i,i}$,\n- For $i \\in \\{0,1,\\dots,n-2\\}$, $u[i]$ stores the matrix entry $A_{i,i+1}$.\n\nYour program must:\n- Construct the packed storage $(l,d,u)$ for each specified test case,\n- Construct a right-hand side vector $b \\in \\mathbb{R}^{n}$ by multiplying the tridiagonal matrix $A$ (encoded by $(l,d,u)$) with a prescribed exact solution vector $x^{\\star} \\in \\mathbb{R}^{n}$,\n- Compute and return a numerical approximation $\\hat{x}$ to the exact solution $x^{\\star}$ for each test case using only the packed storage arrays $(l,d,u)$ and the vector $b$ without materializing $A$ as a dense matrix and without using any external tridiagonal solver from a library,\n- For each test case, compute the maximum absolute error $e = \\max_{0 \\le i \\le n-1} | \\hat{x}_i - x^{\\star}_i |$.\n\nTest suite:\n- Case $1$ (happy path, symmetric strictly diagonally dominant):\n  - Dimension: $n = 5$.\n  - Packed storage representing $A$ with $l[i] = -1$ for $i \\in \\{0,1,2,3\\}$, $d[i] = 2$ for $i \\in \\{0,1,2,3,4\\}$, and $u[i] = -1$ for $i \\in \\{0,1,2,3\\}$.\n  - Exact solution: $x^{\\star} = [1, 2, 3, 4, 5]^{\\mathsf{T}}$.\n  - Right-hand side: $b = A x^{\\star}$.\n- Case $2$ (boundary case $n=1$):\n  - Dimension: $n = 1$.\n  - Packed storage representing $A$ with $d[0] = 3$ (there are no subdiagonal or superdiagonal elements).\n  - Exact solution: $x^{\\star} = [7]^{\\mathsf{T}}$.\n  - Right-hand side: $b = A x^{\\star}$.\n- Case $3$ (zeros on off-diagonals, strictly diagonally dominant):\n  - Dimension: $n = 6$.\n  - Packed storage representing $A$ with $d[i] = 4$ for $i \\in \\{0,1,2,3,4,5\\}$, $u = [1, 0, 1, 0, 1]^{\\mathsf{T}}$, and $l = [0, 1, 0, 1, 0]^{\\mathsf{T}}$.\n  - Exact solution: $x^{\\star} = [1, -1, 2, -2, 3, -3]^{\\mathsf{T}}$.\n  - Right-hand side: $b = A x^{\\star}$.\n- Case $4$ (nonsymmetric, strictly diagonally dominant):\n  - Dimension: $n = 5$.\n  - Packed storage representing $A$ with $d = [5, 5, 5, 5, 5]^{\\mathsf{T}}$, $u = [2, -1, 0.5, -2.5]^{\\mathsf{T}}$, and $l = [-1, 0.5, -0.5, 1.5]^{\\mathsf{T}}$.\n  - Exact solution: $x^{\\star} = [1, 2, -1, 0.5, -2]^{\\mathsf{T}}$.\n  - Right-hand side: $b = A x^{\\star}$.\n\nYour program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each entry is the maximum absolute error $e$ for the corresponding test case. For example, the output format must be exactly of the form $[e_1,e_2,e_3,e_4]$ with no spaces. All values are unitless real numbers. No angles or percentages appear in this problem.", "solution": "The problem statement submitted for analysis is deemed valid. It is a well-posed problem in the domain of numerical linear algebra, specifically concerning the solution of systems of linear equations defined by a tridiagonal matrix. The problem is scientifically grounded, objective, and provides all necessary information to derive a unique and meaningful solution. The specified algorithm, commonly known as the Thomas algorithm or Tridiagonal Matrix Algorithm (TDMA), is the standard and efficient direct method for this class of problems.\n\nThe task is to solve a system of linear equations $A\\hat{x} = b$, where $A \\in \\mathbb{R}^{n \\times n}$ is a tridiagonal matrix, $\\hat{x} \\in \\mathbb{R}^{n}$ is the solution vector to be found, and $b \\in \\mathbb{R}^{n}$ is the right-hand side vector. The matrix $A$ is provided in a packed storage format using three one-dimensional arrays: a subdiagonal $l \\in \\mathbb{R}^{n-1}$, a main diagonal $d \\in \\mathbb{R}^{n}$, and a superdiagonal $u \\in \\mathbb{R}^{n-1}$. The system of equations can be written explicitly using the specified indexing convention:\n$$A_{i,j} = \\begin{cases} d_i & \\text{if } j=i \\\\ u_i & \\text{if } j=i+1 \\\\ l_{j} & \\text{if } j=i-1, \\text{ which means } l_{i-1} \\text{ for } A_{i,i-1} \\\\ 0 & \\text{otherwise} \\end{cases}$$\nThe equations are:\n$$d_0 \\hat{x}_0 + u_0 \\hat{x}_1 = b_0 \\quad (i=0)$$\n$$l_{i-1} \\hat{x}_{i-1} + d_i \\hat{x}_i + u_i \\hat{x}_{i+1} = b_i \\quad (\\text{for } i=1, 2, \\ldots, n-2)$$\n$$l_{n-2} \\hat{x}_{n-2} + d_{n-1} \\hat{x}_{n-1} = b_{n-1} \\quad (i=n-1)$$\n\nThe Thomas algorithm is a manifestation of Gaussian elimination that exploits the sparse, tridiagonal structure of the matrix $A$. It consists of two stages: a forward elimination sweep and a backward substitution sweep.\n\n**1. Forward Elimination**\nThe goal of this stage is to transform the system into an upper bidiagonal form. We systematically eliminate the subdiagonal elements, row by row. This is achieved by modifying the main diagonal and superdiagonal coefficients, as well as the right-hand side vector.\n\nFor the first row ($i=0$), we divide by the diagonal element $d_0$, assuming $d_0 \\neq 0$:\n$$\\hat{x}_0 + \\frac{u_0}{d_0} \\hat{x}_1 = \\frac{b_0}{d_0}$$\nWe define new coefficients for the modified system: $u'_0 = \\frac{u_0}{d_0}$ and $b'_0 = \\frac{b_0}{d_0}$. The equation becomes $\\hat{x}_0 + u'_0 \\hat{x}_1 = b'_0$.\n\nNext, for the second row ($i=1$), the equation is $l_0 \\hat{x}_0 + d_1 \\hat{x}_1 + u_1 \\hat{x}_2 = b_1$. We substitute the expression for $\\hat{x}_0$ from the modified first equation, $\\hat{x}_0 = b'_0 - u'_0 \\hat{x}_1$:\n$$l_0 (b'_0 - u'_0 \\hat{x}_1) + d_1 \\hat{x}_1 + u_1 \\hat{x}_2 = b_1$$\nGrouping terms involving $\\hat{x}_1$ and $\\hat{x}_2$:\n$$(d_1 - l_0 u'_0) \\hat{x}_1 + u_1 \\hat{x}_2 = b_1 - l_0 b'_0$$\nDividing by the new coefficient of $\\hat{x}_1$, we get:\n$$\\hat{x}_1 + \\frac{u_1}{d_1 - l_0 u'_0} \\hat{x}_2 = \\frac{b_1 - l_0 b'_0}{d_1 - l_0 u'_0}$$\nThis gives the recurrence relations for the new coefficients $u'_1$ and $b'_1$.\n\nGeneralizing for a generic row $i$ (from $i=1$ to $n-1$), we start with the equation $l_{i-1} \\hat{x}_{i-1} + d_i \\hat{x}_i + u_i \\hat{x}_{i+1} = b_i$. From the previous step ($i-1$), we have the relation $\\hat{x}_{i-1} = b'_{i-1} - u'_{i-1} \\hat{x}_i$. Substituting this into the current equation yields:\n$$l_{i-1} (b'_{i-1} - u'_{i-1} \\hat{x}_i) + d_i \\hat{x}_i + u_i \\hat{x}_{i+1} = b_i$$\nThis simplifies to:\n$$(d_i - l_{i-1} u'_{i-1}) \\hat{x}_i + u_i \\hat{x}_{i+1} = b_i - l_{i-1} b'_{i-1}$$\nLet the denominator be $m_i = d_i - l_{i-1} u'_{i-1}$. For the algorithm to be stable without pivoting, we require $m_i \\neq 0$ for all $i$. For the strictly diagonally dominant matrices provided in the test suite, this condition is guaranteed.\n\nThe recurrence relations for the modified coefficients (which we denote with primes) are:\n- For $i=0$:\n$$u'_0 = \\frac{u_0}{d_0}$$\n$$b'_0 = \\frac{b_0}{d_0}$$\n- For $i=1, 2, \\ldots, n-2$:\n$$u'_i = \\frac{u_i}{d_i - l_{i-1} u'_{i-1}}$$\n$$b'_i = \\frac{b_i - l_{i-1} b'_{i-1}}{d_i - l_{i-1} u'_{i-1}}$$\n- For the last row, $i=n-1$:\n$$b'_{n-1} = \\frac{b_{n-1} - l_{n-2} b'_{n-2}}{d_{n-1} - l_{n-2} u'_{n-2}}$$\nNote that $u'_{n-1}$ is not required as there is no $u_{n-1}$ term.\n\n**2. Backward Substitution**\nAfter the forward elimination sweep, the original system $A\\hat{x}=b$ is transformed into an equivalent upper bidiagonal system:\n$$\\hat{x}_i + u'_i \\hat{x}_{i+1} = b'_i \\quad (\\text{for } i=0, 1, \\ldots, n-2)$$\n$$\\hat{x}_{n-1} = b'_{n-1}$$\nThis system is solved by starting from the last equation and substituting backwards.\nThe solution is found via the following recurrence:\n$$\\hat{x}_{n-1} = b'_{n-1}$$\n$$\\hat{x}_i = b'_i - u'_i \\hat{x}_{i+1} \\quad (\\text{for } i = n-2, n-3, \\ldots, 0)$$\n\nThe overall procedure for each test case is as follows:\nFirst, construct the right-hand side vector $b$ by computing the matrix-vector product $b = A x^{\\star}$ using the provided packed storage arrays $(l,d,u)$ and the exact solution vector $x^{\\star}$, without materializing the full matrix $A$.\nSecond, apply the described Thomas algorithm to the system $A\\hat{x}=b$ to obtain the numerical solution $\\hat{x}$.\nFinally, compute the maximum absolute error $e = \\max_{0 \\le i \\le n-1} | \\hat{x}_i - x^{\\star}_i |$. Since the computations are performed using floating-point arithmetic, this error serves as a measure of the numerical accuracy of the implementation. For the given test cases with stable matrices, this error is expected to be very close to zero, on the order of machine epsilon.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    It constructs tridiagonal systems, solves them using a custom \n    Thomas algorithm implementation, and computes the maximum absolute error.\n    \"\"\"\n\n    test_cases = [\n        # Case 1: Happy path, symmetric strictly diagonally dominant\n        {\n            \"n\": 5,\n            \"l\": np.array([-1.0, -1.0, -1.0, -1.0]),\n            \"d\": np.array([2.0, 2.0, 2.0, 2.0, 2.0]),\n            \"u\": np.array([-1.0, -1.0, -1.0, -1.0]),\n            \"x_star\": np.array([1.0, 2.0, 3.0, 4.0, 5.0]),\n        },\n        # Case 2: Boundary case n=1\n        {\n            \"n\": 1,\n            \"l\": np.array([]),\n            \"d\": np.array([3.0]),\n            \"u\": np.array([]),\n            \"x_star\": np.array([7.0]),\n        },\n        # Case 3: Zeros on off-diagonals, strictly diagonally dominant\n        {\n            \"n\": 6,\n            \"l\": np.array([0.0, 1.0, 0.0, 1.0, 0.0]),\n            \"d\": np.array([4.0] * 6),\n            \"u\": np.array([1.0, 0.0, 1.0, 0.0, 1.0]),\n            \"x_star\": np.array([1.0, -1.0, 2.0, -2.0, 3.0, -3.0]),\n        },\n        # Case 4: Nonsymmetric, strictly diagonally dominant\n        {\n            \"n\": 5,\n            \"l\": np.array([-1.0, 0.5, -0.5, 1.5]),\n            \"d\": np.array([5.0] * 5),\n            \"u\": np.array([2.0, -1.0, 0.5, -2.5]),\n            \"x_star\": np.array([1.0, 2.0, -1.0, 0.5, -2.0]),\n        },\n    ]\n\n    def construct_b(n, l, d, u, x_star):\n        \"\"\"\n        Constructs the right-hand side vector b = A * x_star using packed storage.\n        \"\"\"\n        if n == 0:\n            return np.array([])\n        \n        b = np.zeros(n, dtype=float)\n        \n        if n == 1:\n            b[0] = d[0] * x_star[0]\n            return b\n\n        # First row\n        b[0] = d[0] * x_star[0] + u[0] * x_star[1]\n        \n        # Middle rows\n        for i in range(1, n - 1):\n            b[i] = l[i-1] * x_star[i-1] + d[i] * x_star[i] + u[i] * x_star[i+1]\n            \n        # Last row\n        b[n-1] = l[n-2] * x_star[n-2] + d[n-1] * x_star[n-1]\n        \n        return b\n\n    def thomas_solver(n, l, d, u, b):\n        \"\"\"\n        Solves a tridiagonal system Ax=b using the Thomas algorithm (TDMA).\n        \n        Args:\n            n (int): The dimension of the system.\n            l (np.ndarray): The subdiagonal (n-1 elements).\n            d (np.ndarray): The main diagonal (n elements).\n            u (np.ndarray): The superdiagonal (n-1 elements).\n            b (np.ndarray): The right-hand side vector (n elements).\n        \n        Returns:\n            np.ndarray: The solution vector x_hat.\n        \"\"\"\n        # Handle the trivial case n=1\n        if n == 1:\n            return np.array([b[0] / d[0]])\n            \n        # Allocate space for modified coefficients\n        u_prime = np.zeros(n - 1, dtype=float)\n        b_prime = np.zeros(n, dtype=float)\n        \n        # Forward elimination sweep\n        # i = 0\n        u_prime[0] = u[0] / d[0]\n        b_prime[0] = b[0] / d[0]\n        \n        # i = 1 to n-2\n        for i in range(1, n - 1):\n            denominator = d[i] - l[i-1] * u_prime[i-1]\n            u_prime[i] = u[i] / denominator\n            b_prime[i] = (b[i] - l[i-1] * b_prime[i-1]) / denominator\n        \n        # i = n-1 (last element of b_prime)\n        denominator_last = d[n-1] - l[n-2] * u_prime[n-2]\n        b_prime[n-1] = (b[n-1] - l[n-2] * b_prime[n-2]) / denominator_last\n\n        # Backward substitution sweep\n        x_hat = np.zeros(n, dtype=float)\n        x_hat[n-1] = b_prime[n-1]\n        for i in range(n - 2, -1, -1):\n            x_hat[i] = b_prime[i] - u_prime[i] * x_hat[i+1]\n            \n        return x_hat\n\n    results = []\n    for case in test_cases:\n        n, l, d, u, x_star = case[\"n\"], case[\"l\"], case[\"d\"], case[\"u\"], case[\"x_star\"]\n        \n        # Construct the right-hand side vector b = A * x_star\n        b = construct_b(n, l, d, u, x_star)\n        \n        # Solve the system A * x_hat = b to find the numerical solution\n        x_hat = thomas_solver(n, l, d, u, b)\n        \n        # Compute the maximum absolute error\n        error = np.max(np.abs(x_hat - x_star))\n        results.append(error)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2446352"}, {"introduction": "Having implemented the algorithm, we now turn to a crucial question: how much faster is it, really? This practice moves from coding to theoretical analysis, asking you to derive the exact number of floating-point operations (FLOPs) [@problem_id:2446371]. By comparing the linear complexity of the Thomas algorithm to the cubic complexity of standard Gaussian elimination, you will gain a profound appreciation for its efficiency in large-scale problems.", "problem": "In computational engineering, a tridiagonal linear system has coefficient matrix $A \\in \\mathbb{R}^{n \\times n}$ with nonzero entries only on the main diagonal and the first sub- and super-diagonals, and a single right-hand side vector $d \\in \\mathbb{R}^{n}$. Consider solving $A x = d$ in two ways:\n\n- Using the Thomas algorithm, implemented in its classical form that performs a single forward sweep eliminating the subdiagonal with scalar multipliers while updating the main diagonal and the right-hand side, followed by a backward substitution on the resulting upper-triangular system. Assume no pivoting.\n- Using standard dense Gaussian elimination (GE) without exploiting sparsity and without pivoting, treating $A$ as a general dense matrix and performing full trailing submatrix updates at each step; then solving the resulting lower-upper (LU) triangular systems with one forward substitution and one backward substitution for the single right-hand side.\n\nAdopt the following floating-point operation (FLOP) model: each floating-point addition, subtraction, multiplication, or division counts as exactly $1$ FLOP; comparisons, assignments, and memory operations are not counted.\n\nDerive the exact total FLOP count as closed-form functions of $n$ for each method, including all steps necessary to compute $x$ for the single right-hand side. Provide your final answer as a row vector containing the Thomas algorithm FLOP count and the dense Gaussian elimination FLOP count, in that order. No rounding is required. Do not include units.", "solution": "The problem requires the derivation of the exact floating-point operation (FLOP) count for solving a tridiagonal linear system $A x = d$ of size $n \\times n$ using two distinct methods: the specialized Thomas algorithm and standard dense Gaussian elimination. The analysis will adhere to the provided FLOP model where each addition, subtraction, multiplication, or division counts as one FLOP.\n\nFirst, let us define the structure of the tridiagonal matrix $A$. The system of equations $A x = d$ can be written as:\n$$ a_i x_{i-1} + b_i x_i + c_i x_{i+1} = d_i $$\nfor $i=1, \\dots, n$, with the understanding that $a_1 = 0$ and $c_n = 0$. The coefficients $a_i$ form the subdiagonal, $b_i$ the main diagonal, and $c_i$ the superdiagonal.\n\n**1. Analysis of the Thomas Algorithm**\n\nThe Thomas algorithm is a highly efficient form of Gaussian elimination tailored for tridiagonal systems. It consists of two sequential stages: a forward elimination sweep followed by a backward substitution sweep.\n\n**Forward Elimination:**\nThe goal of this stage is to eliminate the subdiagonal elements $a_i$, transforming the system into an upper bidiagonal form. This is achieved by iterating from the second row to the last row. For each row $i$ (from $2$ to $n$), we perform the operation $R_i \\leftarrow R_i - m_i R_{i-1}$, where $R_i$ is the $i$-th row of the augmented system $[A|d]$ and $m_i$ is a multiplier chosen to eliminate the subdiagonal element in that row. The original matrix elements $b_i$ and right-hand side elements $d_i$ are modified in this process. Let a prime denote a modified value.\n\nFor each row $i=2, 3, \\dots, n$:\n1.  Compute the multiplier $m_i = \\frac{a_i}{b'_{i-1}}$. This requires $1$ division.\n2.  Update the main diagonal element: $b'_i = b_i - m_i c_{i-1}$. This involves $1$ multiplication and $1$ subtraction, for a total of $2$ FLOPs.\n3.  Update the right-hand side element: $d'_i = d_i - m_i d'_{i-1}$. This involves $1$ multiplication and $1$ subtraction, for a total of $2$ FLOPs.\nThe superdiagonal elements $c_i$ are not altered. Note that for the first step ($i=2$), $b'_{1} = b_1$ and $d'_{1} = d_1$.\n\nThe loop runs from $i=2$ to $n$, which is a total of $n-1$ iterations. Each iteration requires $1 + 2 + 2 = 5$ FLOPs.\nTherefore, the total FLOP count for the forward elimination phase is:\n$$ F_{\\text{forward}} = 5(n-1) $$\n\n**Backward Substitution:**\nAfter forward elimination, the system has the upper bidiagonal form:\n$$ b'_i x_i + c_i x_{i+1} = d'_i $$\nfor $i=1, \\dots, n-1$, and $b'_n x_n = d'_n$ for the last equation. We solve for $x$ by back-substituting, starting from $x_n$.\n\n1.  Solve for $x_n$: $x_n = \\frac{d'_n}{b'_n}$. This requires $1$ division.\n2.  For $i = n-1, n-2, \\dots, 1$: solve for $x_i$ using the already computed $x_{i+1}$:\n    $x_i = \\frac{d'_i - c_i x_{i+1}}{b'_i}$. Each such computation requires $1$ multiplication, $1$ subtraction, and $1$ division, for a total of $3$ FLOPs.\n\nThis loop runs for $n-1$ iterations.\nTherefore, the total FLOP count for the backward substitution phase is:\n$$ F_{\\text{backward}} = 1 + 3(n-1) $$\n\nThe total FLOP count for the Thomas algorithm is the sum of the counts for both phases:\n$$ F_{\\text{Thomas}} = F_{\\text{forward}} + F_{\\text{backward}} = 5(n-1) + (1 + 3(n-1)) = 8(n-1) + 1 = 8n - 7 $$\n\n**2. Analysis of Dense Gaussian Elimination**\n\nThis method treats the tridiagonal matrix $A$ as a general dense matrix, ignoring its sparse structure. The process involves an elimination phase to transform $A$ into an upper triangular matrix $U$ while similarly transforming the right-hand side vector $d$ into $d'$, followed by backward substitution to solve $Ux = d'$.\n\n**Elimination Phase:**\nThis phase proceeds in $n-1$ steps, from $k=1$ to $n-1$. At step $k$, we use the $k$-th row to eliminate the elements below the pivot $A_{kk}$ in the $k$-th column.\n\nFor each step $k = 1, \\dots, n-1$:\n1.  For each row $i$ from $k+1$ to $n$:\n    a. Compute the multiplier $m_{ik} = \\frac{A_{ik}}{A_{kk}}$. This is $1$ division. Note that for a tridiagonal matrix, $A_{ik}=0$ for $i>k+1$, but we are treating it as dense, so we form the general algorithm.\n    b. Update the remaining elements in row $i$: For each column $j$ from $k+1$ to $n$, update $A_{ij} \\leftarrow A_{ij} - m_{ik} A_{kj}$. This is $1$ multiplication and $1$ subtraction, total $2$ FLOPs. The inner loop on $j$ runs for $n-k$ iterations.\n    c. Update the right-hand side vector: $d_i \\leftarrow d_i - m_{ik} d_k$. This costs $2$ FLOPs.\n\nThe number of rows $i$ to be updated is $n-k$.\nThus, the cost for step $k$ is:\n$$ C_k = \\sum_{i=k+1}^{n} \\left( 1 + \\sum_{j=k+1}^{n} (2) + 2 \\right) = (n-k) \\times (1 + 2(n-k) + 2) $$\nThe problem describes LU factorization followed by forward/backward substitution. A more direct way to count for solving $Ax=d$ is to consider the operations on the augmented matrix $[A|d]$.\nAt step $k$:\n-   Multipliers: For each row $i=k+1, \\dots, n$, we compute one multiplier. This is $n-k$ divisions.\n-   Row updates: For each of the $n-k$ rows, we update $n-k$ elements of the matrix and $1$ element of the RHS vector. Each update is a multiplication and a subtraction (2 FLOPs). The elements to be updated are in columns $j=k+1, \\dots, n$ and the RHS. Total columns to update is $(n-k)+1$.\nCost at step $k$: $(n-k)$ divisions + $(n-k) \\times ((n-k)+1) \\times 2$ FLOPs for updates.\nTotal cost is $\\sum_{k=1}^{n-1} \\left( (n-k) + 2(n-k)(n-k+1) \\right)$.\nLet $p = n-k$. As $k$ goes from $1$ to $n-1$, $p$ goes from $n-1$ to $1$.\n$$ F_{\\text{elim}} = \\sum_{p=1}^{n-1} (p + 2p(p+1)) = \\sum_{p=1}^{n-1} (2p^2 + 3p) $$\nUsing standard summation formulas $\\sum_{p=1}^{N} p = \\frac{N(N+1)}{2}$ and $\\sum_{p=1}^{N} p^2 = \\frac{N(N+1)(2N+1)}{6}$ with $N=n-1$:\n$$ \\sum_{p=1}^{n-1} p = \\frac{(n-1)n}{2} $$\n$$ \\sum_{p=1}^{n-1} p^2 = \\frac{(n-1)n(2n-1)}{6} $$\nSo, the total FLOPs for elimination are:\n$$ F_{\\text{elim}} = 2 \\left( \\frac{(n-1)n(2n-1)}{6} \\right) + 3 \\left( \\frac{(n-1)n}{2} \\right) $$\n$$ F_{\\text{elim}} = \\frac{n(n-1)(2n-1)}{3} + \\frac{3n(n-1)}{2} = \\frac{2n(n-1)(2n-1) + 9n(n-1)}{6} $$\n$$ F_{\\text{elim}} = \\frac{n(n-1) [2(2n-1) + 9]}{6} = \\frac{n(n-1)(4n-2+9)}{6} = \\frac{n(n-1)(4n+7)}{6} $$\n$$ F_{\\text{elim}} = \\frac{(n^2-n)(4n+7)}{6} = \\frac{4n^3 + 7n^2 - 4n^2 - 7n}{6} = \\frac{4n^3 + 3n^2 - 7n}{6} $$\n\n**Backward Substitution:**\nAfter elimination, we solve the dense upper triangular system $U x = d'$.\n1.  Solve for $x_n = \\frac{d'_n}{U_{nn}}$. This is $1$ FLOP.\n2.  For $i = n-1, \\dots, 1$:\n    $x_i = \\frac{1}{U_{ii}} \\left( d'_i - \\sum_{j=i+1}^{n} U_{ij} x_j \\right)$.\n    The summation involves $n-i$ multiplications and $n-i-1$ additions. Then there is $1$ subtraction from $d'_i$ and $1$ division. Total FLOPs for each $x_i$: $(n-i) + (n-i-1+1) + 1 = 2(n-i) + 1$.\nThe total for backward substitution is the cost for $x_n$ plus the sum of costs for $x_{n-1}$ down to $x_1$:\n$$ F_{\\text{backward}} = 1 + \\sum_{i=1}^{n-1} [2(n-i)+1] $$\nLet $p=n-i$. The sum becomes $\\sum_{p=1}^{n-1} (2p+1)$.\n$$ F_{\\text{backward}} = 1 + 2\\sum_{p=1}^{n-1} p + \\sum_{p=1}^{n-1} 1 = 1 + 2\\frac{(n-1)n}{2} + (n-1) = 1 + n^2-n + n-1 = n^2 $$\nThe total FLOP count for dense Gaussian elimination is the sum of both phases:\n$$ F_{\\text{GE}} = F_{\\text{elim}} + F_{\\text{backward}} = \\frac{4n^3 + 3n^2 - 7n}{6} + n^2 = \\frac{4n^3 + 3n^2 - 7n + 6n^2}{6} = \\frac{4n^3 + 9n^2 - 7n}{6} $$\n\nIn conclusion, the exact FLOP counts are $8n-7$ for the Thomas algorithm and $\\frac{4n^3 + 9n^2 - 7n}{6}$ for dense Gaussian elimination.", "answer": "$$ \\boxed{ \\begin{pmatrix} 8n - 7 & \\frac{4n^3 + 9n^2 - 7n}{6} \\end{pmatrix} } $$", "id": "2446371"}, {"introduction": "An algorithm that is fast but numerically unstable has limited practical value. This final exercise tackles the critical issue of stability by introducing a pivot check into the forward elimination sweep [@problem_id:2446297]. By implementing a tolerance check for pivots approaching zero, you will learn how to make your solver robust against ill-conditioned or singular matrices, a common challenge in real-world computational engineering.", "problem": "You are given a family of real tridiagonal linear systems of the form $A \\mathbf{x} = \\mathbf{d}$, where $A \\in \\mathbb{R}^{n \\times n}$ has main diagonal entries $\\{b_i\\}_{i=1}^{n}$, subdiagonal entries $\\{a_i\\}_{i=2}^{n}$, and superdiagonal entries $\\{c_i\\}_{i=1}^{n-1}$, so that\n$$\nA =\n\\begin{pmatrix}\nb_1 & c_1 & 0 & \\cdots & 0 \\\\\na_2 & b_2 & c_2 & \\ddots & \\vdots \\\\\n0 & a_3 & b_3 & \\ddots & 0 \\\\\n\\vdots & \\ddots & \\ddots & \\ddots & c_{n-1} \\\\\n0 & \\cdots & 0 & a_n & b_n\n\\end{pmatrix}, \\quad\n\\mathbf{x} =\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ \\vdots \\\\ x_{n-1} \\\\ x_n\n\\end{pmatrix}, \\quad\n\\mathbf{d} =\n\\begin{pmatrix}\nd_1 \\\\ d_2 \\\\ \\vdots \\\\ d_{n-1} \\\\ d_n\n\\end{pmatrix}.\n$$\nFor each system, you are also given a nonnegative tolerance $\\tau \\in \\mathbb{R}$, and you must apply the following rule: define sequences $\\{p_i\\}$, $\\{c'_i\\}$, and $\\{d'_i\\}$ by\n$$\np_1 = b_1,\n$$\nand for $i = 1$ set\n$$\nc'_1 = \\begin{cases}\n\\frac{c_1}{p_1}, & \\text{if } n \\ge 2, \\\\\n\\text{undefined}, & \\text{if } n = 1,\n\\end{cases}\n\\quad\nd'_1 = \\frac{d_1}{p_1}.\n$$\nFor $i = 2, 3, \\ldots, n$, set\n$$\np_i = b_i - a_i \\, c'_{i-1}.\n$$\nAt any index $i \\in \\{1,2,\\ldots,n\\}$, if $|p_i| \\le \\tau$, the computation is deemed to have failed due to singularity or near-singularity, and you must declare failure for that system. If $|p_i| > \\tau$ for all $i$, then for $i = 2, 3, \\ldots, n-1$ define\n$$\nc'_i = \\frac{c_i}{p_i},\n$$\nand for $i = 2, 3, \\ldots, n$ define\n$$\nd'_i = \\frac{d_i - a_i d'_{i-1}}{p_i}.\n$$\nIf no failure was declared, the unique solution $\\mathbf{x}$ is defined by the back-substitution relations\n$$\nx_n = d'_n, \\quad\nx_i = d'_i - c'_i x_{i+1} \\quad \\text{for } i = n-1, n-2, \\ldots, 1.\n$$\n\nImplement a program that, for each test case below, follows the rule above and outputs either the solution vector $\\mathbf{x}$ (as a list of real numbers) if the computation succeeds, or the boolean value $False$ if a failure was declared because there exists an index $i$ with $|p_i| \\le \\tau$.\n\nTest Suite. Each test case is specified by the tuple $(\\{a_i\\}_{i=2}^{n}, \\{b_i\\}_{i=1}^{n}, \\{c_i\\}_{i=1}^{n-1}, \\{d_i\\}_{i=1}^{n}, \\tau)$ written in zero-based array form for implementation, that is, as lists\n$$\na = [a_2, a_3, \\ldots, a_n], \\quad b = [b_1, b_2, \\ldots, b_n], \\quad c = [c_1, c_2, \\ldots, c_{n-1}], \\quad d = [d_1, d_2, \\ldots, d_n],\n$$\nwith $\\tau$ given separately.\n\nProvide results for the following six test cases:\n- Case $1$ (regular, strictly diagonally dominant): $a = [ -1, -1, -1, -1 ]$, $b = [ 2, 2, 2, 2, 2 ]$, $c = [ -1, -1, -1, -1 ]$, $d = [ 1, 1, 1, 1, 1 ]$, $\\tau = 10^{-12}$.\n- Case $2$ (boundary size $n = 1$, failure due to tiny first pivot): $a = [\\,]$ (empty list), $b = [ 10^{-12} ]$, $c = [\\,]$, $d = [ 1 ]$, $\\tau = 10^{-10}$.\n- Case $3$ (mid-computation exact zero pivot): $a = [ 1, 1 ]$, $b = [ 1, 1, 1 ]$, $c = [ 1, 1 ]$, $d = [ 1, 2, 3 ]$, $\\tau = 10^{-12}$.\n- Case $4$ (diagonal matrix): $a = [ 0, 0, 0 ]$, $b = [ 3, 4, 5, 6 ]$, $c = [ 0, 0, 0 ]$, $d = [ 3, 8, 10, 12 ]$, $\\tau = 10^{-12}$.\n- Case $5$ (near-singular but acceptable under strict tolerance): $a = [ 1 ]$, $b = [ 1, 1 + 10^{-9} ]$, $c = [ 1 ]$, $d = [ 1, 1 ]$, $\\tau = 10^{-12}$.\n- Case $6$ (same as Case $5$ but failure under looser tolerance): $a = [ 1 ]$, $b = [ 1, 1 + 10^{-9} ]$, $c = [ 1 ]$, $d = [ 1, 1 ]$, $\\tau = 10^{-8}$.\n\nFinal Output Format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case, output either the solution vector as a list of real numbers or the boolean value $False$ if the rule declares failure. There must be no spaces anywhere in the printed line. For example, an output aggregating three hypothetical per-case results might look like\n$$\n[\\,[0.5,1.0],False,[1.0]\\,].\n$$\nYour program must output exactly one such line containing the six per-case results in order as $[r_1,r_2,r_3,r_4,r_5,r_6]$, with no additional text.", "solution": "The problem statement is subjected to validation.\n\n**Step 1: Extract Givens**\n\nThe problem provides a procedure to solve a real tridiagonal linear system $A \\mathbf{x} = \\mathbf{d}$ of size $n \\times n$.\nThe matrix $A$ has:\n- Main diagonal entries: $\\{b_i\\}_{i=1}^{n}$\n- Subdiagonal entries: $\\{a_i\\}_{i=2}^{n}$\n- Superdiagonal entries: $\\{c_i\\}_{i=1}^{n-1}$\n\nThe algorithm is defined via sequences $\\{p_i\\}$, $\\{c'_i\\}$, and $\\{d'_i\\}$ and a nonnegative tolerance $\\tau$.\n\nForward Elimination:\n1.  Initialize with $i=1$:\n    $p_1 = b_1$.\n    A failure is declared if $|p_1| \\le \\tau$.\n    If successful, compute $c'_1 = c_1/p_1$ (for $n \\ge 2$) and $d'_1 = d_1/p_1$.\n\n2.  Iterate for $i = 2, 3, \\ldots, n$:\n    $p_i = b_i - a_i c'_{i-1}$.\n    A failure is declared if $|p_i| \\le \\tau$.\n    If successful, compute $c'_i = c_i/p_i$ (for $i < n$) and $d'_i = (d_i - a_i d'_{i-1})/p_i$.\n\nBack Substitution:\nIf no failure occurs, the solution $\\mathbf{x}$ is found by:\n$x_n = d'_n$.\n$x_i = d'_i - c'_i x_{i+1}$ for $i = n-1, n-2, \\ldots, 1$.\n\nOutput:\n- The boolean value `False` upon failure.\n- The solution vector $\\mathbf{x}$ upon success.\n\nTest Cases: Six specific instances of $(\\{a_i\\}, \\{b_i\\}, \\{c_i\\}, \\{d_i\\}, \\tau)$ are provided for implementation, using zero-based list formats for the vectors.\n\n**Step 2: Validate Using Extracted Givens**\n\n-   **Scientifically Grounded:** The described algorithm is the Thomas algorithm, a well-established and computationally efficient method in numerical linear algebra for solving tridiagonal systems. It is a specialized form of Gaussian elimination. The use of a tolerance $\\tau$ to check the magnitude of pivots is a standard technique to ensure numerical stability. The problem is scientifically sound.\n-   **Well-Posed:** The algorithm is deterministic and provides a unique outcome (either a solution vector or a failure state) for any valid set of inputs. The conditions for its application and termination are clearly defined.\n-   **Objective:** The problem is stated using precise mathematical language and notation. All data and procedures are objective and formalizable.\n\nThe problem does not violate any of the invalidity criteria. It is scientifically sound, well-posed, complete, and verifiable.\n\n**Step 3: Verdict and Action**\n\nThe problem is **valid**. A solution will be provided.\n\n**Principle-Based Solution**\n\nThe algorithm presented in the problem statement is a direct application of LU decomposition tailored for a tridiagonal matrix structure. This method, known as the Thomas algorithm, achieves high efficiency by avoiding operations on the zero elements of the matrix. The core principle is to factor the tridiagonal matrix $A$ into the product of a lower bidiagonal matrix $L$ and a unit upper bidiagonal matrix $U$, a form known as Crout's factorization, $A = LU$.\n\nLet the matrices $L$ and $U$ be defined as:\n$$\nL =\n\\begin{pmatrix}\np_1 & 0 & 0 & \\cdots & 0 \\\\\na_2 & p_2 & 0 & \\ddots & \\vdots \\\\\n0 & a_3 & p_3 & \\ddots & 0 \\\\\n\\vdots & \\ddots & \\ddots & \\ddots & 0 \\\\\n0 & \\cdots & 0 & a_n & p_n\n\\end{pmatrix}, \\quad\nU =\n\\begin{pmatrix}\n1 & c'_1 & 0 & \\cdots & 0 \\\\\n0 & 1 & c'_2 & \\ddots & \\vdots \\\\\n0 & 0 & 1 & \\ddots & 0 \\\\\n\\vdots & \\ddots & \\ddots & \\ddots & c'_{n-1} \\\\\n0 & \\cdots & 0 & 0 & 1\n\\end{pmatrix}\n$$\nBy equating the entries of $A$ with the product $LU$, we derive the recurrence relations for the unknown elements $p_i$ and $c'_i$.\nThe main diagonal of $A$ gives $b_i = (LU)_{ii}$:\n-   For $i=1$: $b_1 = p_1$.\n-   For $i=2, \\ldots, n$: $b_i = a_i c'_{i-1} + p_i$, which rearranges to $p_i = b_i - a_i c'_{i-1}$.\n\nThe superdiagonal of $A$ gives $c_i = (LU)_{i,i+1}$:\n-   For $i=1, \\ldots, n-1$: $c_i = p_i c'_i$, which gives $c'_i = c_i / p_i$.\n\nThese relations are precisely the forward-elimination steps of the prescribed algorithm for computing $\\{p_i\\}$ and $\\{c'_i\\}$. The elements $\\{p_i\\}$ are the pivots of the Gaussian elimination process. The condition $|p_i| \\le \\tau$ for a small tolerance $\\tau \\ge 0$ is a critical check for numerical stability. If a pivot $p_i$ is zero, the matrix $A$ is singular, and the algorithm fails. If $|p_i|$ is very small, the matrix is ill-conditioned (near-singular), and dividing by $p_i$ would introduce large floating-point errors, destabilizing the solution. The tolerance $\\tau$ provides a practical threshold to declare failure in such cases.\n\nOnce the factorization $A=LU$ is found, the original system $A\\mathbf{x} = \\mathbf{d}$ becomes $LU\\mathbf{x} = \\mathbf{d}$. This is solved in two stages:\n1.  **Forward Substitution:** Let $\\mathbf{y} = U\\mathbf{x}$. Solve the lower triangular system $L\\mathbf{y} = \\mathbf{d}$. This yields:\n    -   $p_1 y_1 = d_1 \\implies y_1 = d_1/p_1$.\n    -   $a_i y_{i-1} + p_i y_i = d_i \\implies y_i = (d_i - a_i y_{i-1})/p_i$ for $i=2, \\ldots, n$.\n    By observing the structure of this recurrence, we see that the vector $\\mathbf{y}$ is identical to the sequence $\\mathbf{d}' = \\{d'_i\\}_{i=1}^n$ defined in the problem. The forward elimination phase thus computes the LU factorization and solves $L\\mathbf{y} = \\mathbf{d}$ simultaneously.\n\n2.  **Backward Substitution:** Solve the unit upper triangular system $U\\mathbf{x} = \\mathbf{y}$ (which is now $U\\mathbf{x} = \\mathbf{d}'$). This yields:\n    -   $x_n = y_n = d'_n$.\n    -   $x_i + c'_i x_{i+1} = y_i = d'_i \\implies x_i = d'_i - c'_i x_{i+1}$ for $i=n-1, \\ldots, 1$.\n    This is identical to the back-substitution phase defined in the problem statement.\n\nIn summary, the provided algorithm is a numerically robust implementation of the Thomas algorithm, grounded in the principles of LU factorization for tridiagonal systems. The logic is sound, and its implementation will yield the correct solution or a failure state as specified.", "answer": "```python\nimport numpy as np\n\ndef solve_tridiagonal(a_vec, b_vec, c_vec, d_vec, tau):\n    \"\"\"\n    Solves a tridiagonal linear system Ax=d using the Thomas algorithm.\n\n    Args:\n        a_vec (list): The subdiagonal entries [a_2, ..., a_n].\n        b_vec (list): The main diagonal entries [b_1, ..., b_n].\n        c_vec (list): The superdiagonal entries [c_1, ..., c_{n-1}].\n        d_vec (list): The right-hand side vector [d_1, ..., d_n].\n        tau (float): The non-negative tolerance for pivot checking.\n\n    Returns:\n        list or bool: The solution vector x as a list of floats, or False if\n                      the computation fails due to a small pivot.\n    \"\"\"\n    n = len(b_vec)\n    if n == 0:\n        return []\n\n    # Handle the n=1 case separately as per the algorithm's structure\n    if n == 1:\n        p1 = b_vec[0]\n        if abs(p1) <= tau:\n            return False\n        return [d_vec[0] / p1]\n\n    # Allocate memory for modified coefficients\n    c_prime = [0.0] * (n - 1)\n    d_prime = [0.0] * n\n\n    # --- Forward Elimination Phase ---\n\n    # Step for i = 1 (Python index 0)\n    p1 = b_vec[0]\n    if abs(p1) <= tau:\n        return False\n    \n    c_prime[0] = c_vec[0] / p1\n    d_prime[0] = d_vec[0] / p1\n\n    # Loop for i = 2 to n (Python index 1 to n-1)\n    for i in range(1, n):\n        # Current mathematical index is i+1, Python index is i\n        # a_vec is 0-indexed and corresponds to math indices a_2, a_3, ...\n        # So a_i (math) corresponds to a_vec[i-2] for i>=2\n        # a_{i+1} (math) corresponds to a_vec[i-1] for i>=1\n        \n        pi = b_vec[i] - a_vec[i-1] * c_prime[i-1]\n        if abs(pi) <= tau:\n            return False\n\n        if i < n - 1:\n            # c_vec is 0-indexed and corresponds to math indices c_1, c_2, ...\n            # c_{i} (math) corresponds to c_vec[i-1] for i>=1\n            # c_{i+1} (math) corresponds to c_vec[i] for i>=1\n            c_prime[i] = c_vec[i] / pi\n        \n        # d_vec is 0-indexed and corresponds to math indices d_1, d_2, ...\n        # d_{i+1} (math) corresponds to d_vec[i] for i>=1\n        d_prime[i] = (d_vec[i] - a_vec[i-1] * d_prime[i-1]) / pi\n\n    # --- Backward Substitution Phase ---\n    x = [0.0] * n\n    x[n - 1] = d_prime[n - 1]\n    \n    # Loop for i = n-1 down to 1 (Python index n-2 down to 0)\n    for i in range(n - 2, -1, -1):\n        x[i] = d_prime[i] - c_prime[i] * x[i + 1]\n\n    return x\n\n\ndef solve():\n    \"\"\"\n    Runs the provided test suite and prints the formatted results.\n    \"\"\"\n    test_cases = [\n        # Case 1: regular, strictly diagonally dominant\n        {'a': [-1., -1., -1., -1.], 'b': [2., 2., 2., 2., 2.], 'c': [-1., -1., -1., -1.], 'd': [1., 1., 1., 1., 1.], 'tau': 1e-12},\n        # Case 2: boundary size n=1, failure due to tiny first pivot\n        {'a': [], 'b': [1e-12], 'c': [], 'd': [1.], 'tau': 1e-10},\n        # Case 3: mid-computation exact zero pivot\n        {'a': [1., 1.], 'b': [1., 1., 1.], 'c': [1., 1.], 'd': [1., 2., 3.], 'tau': 1e-12},\n        # Case 4: diagonal matrix\n        {'a': [0., 0., 0.], 'b': [3., 4., 5., 6.], 'c': [0., 0., 0.], 'd': [3., 8., 10., 12.], 'tau': 1e-12},\n        # Case 5: near-singular but acceptable under strict tolerance\n        {'a': [1.], 'b': [1., 1. + 1e-9], 'c': [1.], 'd': [1., 1.], 'tau': 1e-12},\n        # Case 6: same as Case 5 but failure under looser tolerance\n        {'a': [1.], 'b': [1., 1. + 1e-9], 'c': [1.], 'd': [1., 1.], 'tau': 1e-8},\n    ]\n\n    all_results = []\n    for case in test_cases:\n        result = solve_tridiagonal(case['a'], case['b'], case['c'], case['d'], case['tau'])\n        all_results.append(result)\n\n    # Format the results into a single string with no spaces, as required.\n    string_results = []\n    for res in all_results:\n        if res is False:\n            string_results.append(\"False\")\n        else: # It is a list of numbers\n            list_as_string = \"[\" + \",\".join(map(str, res)) + \"]\"\n            string_results.append(list_as_string)\n            \n    final_output = \"[\" + \",\".join(string_results) + \"]\"\n    print(final_output)\n\nsolve()\n```", "id": "2446297"}]}