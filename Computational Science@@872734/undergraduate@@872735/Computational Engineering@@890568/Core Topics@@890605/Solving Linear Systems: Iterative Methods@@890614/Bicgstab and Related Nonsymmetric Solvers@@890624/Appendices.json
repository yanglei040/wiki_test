{"hands_on_practices": [{"introduction": "To truly grasp the inner workings of an iterative solver, there is no substitute for tracing its execution by hand. This exercise guides you through the BiCGSTAB algorithm step-by-step, using a small $2 \\times 2$ system and exact arithmetic. By manually calculating each residual, search direction, and scalar coefficient, you will build a concrete understanding of how the method systematically reduces the error and converges to the exact solution in a finite number of steps [@problem_id:2374444].", "problem": "You will apply the BiConjugate Gradient Stabilized (BiCGSTAB) method to a nonsymmetric linear system to explicitly illustrate that, in exact arithmetic and with a standard choice of parameters, it requires two iterations to converge starting from the zero initial guess. Consider the linear system with\n- matrix $A = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix}$,\n- right-hand side $b = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$,\n- initial guess $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$,\n- identity preconditioner, and\n- shadow residual $\\hat{r}_0 = r_0$, where $r_0 = b - A x_0$.\n\nAll inner products are the standard Euclidean ones. Work in exact arithmetic (that is, do not round intermediate quantities). Perform the BiCGSTAB iterations step-by-step, starting from the core definitions of residuals, search directions, and scalar coefficients that define the BiCGSTAB recurrence, and show that:\n1. After the first iteration, the residual is nonzero.\n2. After the second iteration, the method reaches the exact solution (residual equal to the zero vector).\n\nExplicitly compute and report all intermediate vectors and scalars used by BiCGSTAB up to convergence to substantiate these claims. Finally, provide the exact value of the first smoothing parameter $\\omega_1$ produced during the first BiCGSTAB iteration for this system. Express your final answer as an exact rational number. Do not round.", "solution": "The problem is first validated for soundness and completeness.\n\n**Step 1: Extract Givens**\n- **Method:** BiConjugate Gradient Stabilized (BiCGSTAB)\n- **Linear System:** $A x = b$\n- **Matrix:** $A = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix}$\n- **Right-hand side:** $b = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$\n- **Initial guess:** $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$\n- **Preconditioner:** Identity matrix ($M=I$)\n- **Shadow residual:** $\\hat{r}_0 = r_0$, where $r_0 = b - A x_0$\n- **Inner product:** Standard Euclidean inner product, denoted as $(u, v) = u^T v$.\n- **Arithmetic:** Exact arithmetic is required.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Grounding:** The problem is a direct and standard application of the BiCGSTAB algorithm, a well-established method in numerical linear algebra for solving nonsymmetric linear systems. It is scientifically sound.\n- **Well-Posedness:** The matrix $A$ has determinant $\\det(A) = (1)(4) - (2)(3) = 4 - 6 = -2 \\neq 0$. Thus, $A$ is invertible, and the linear system $Ax=b$ has a unique solution. The BiCGSTAB algorithm is a well-defined procedure. For a system of size $N \\times N$, Krylov subspace methods like BiCGSTAB are guaranteed to find the exact solution in at most $N$ iterations in exact arithmetic. For this $2 \\times 2$ system, convergence in exactly $2$ iterations is a plausible and verifiable claim.\n- **Objectivity:** The problem is stated using precise mathematical language, free of any subjectivity or ambiguity.\n\n**Verdict:** The problem is valid. The solution is constructed by applying the BiCGSTAB algorithm step-by-step.\n\nThe algorithm is as follows:\n\n**Initialization:**\n1. Given an initial guess $x_0$.\n2. Compute the initial residual $r_0 = b - Ax_0$.\n3. Choose a shadow residual vector $\\hat{r}_0$, such that $(\\hat{r}_0, r_0) \\neq 0$. Here, we are given $\\hat{r}_0 = r_0$.\n4. Set initial parameters for the recurrence: $\\rho_0 = 1$, $\\alpha_0 = 1$, $\\omega_0 = 1$.\n5. Set initial search directions: $p_0 = \\mathbf{0}$, $v_0 = \\mathbf{0}$.\n\n**Main Loop (for $k = 1, 2, \\dots$):**\n1. $\\rho_k = (\\hat{r}_0, r_{k-1})$\n2. $\\beta_k = \\frac{\\rho_k}{\\rho_{k-1}} \\frac{\\alpha_{k-1}}{\\omega_{k-1}}$\n3. $p_k = r_{k-1} + \\beta_k (p_{k-1} - \\omega_{k-1} v_{k-1})$\n4. $v_k = A p_k$\n5. $\\alpha_k = \\frac{\\rho_k}{(\\hat{r}_0, v_k)}$\n6. $s_k = r_{k-1} - \\alpha_k v_k$\n7. $t_k = A s_k$\n8. $\\omega_k = \\frac{(t_k, s_k)}{(t_k, t_k)}$\n9. $x_k = x_{k-1} + \\alpha_k p_k + \\omega_k s_k$\n10. $r_k = s_k - \\omega_k t_k$\n\nWe will now apply this algorithm to the given system.\n\n**Initialization ($k=0$):**\n- The system is defined by $A = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix}$ and $b = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\n- The initial guess is $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\n- The initial residual is $r_0 = b - Ax_0 = b = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\n- The shadow residual is $\\hat{r}_0 = r_0 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\n- The initial parameters are set to $\\rho_0 = 1$, $\\alpha_0 = 1$, $\\omega_0 = 1$.\n- The initial direction vectors are $p_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ and $v_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\n\n**Iteration 1 ($k=1$):**\n1. $\\rho_1 = (\\hat{r}_0, r_0) = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = 1$.\n2. $\\beta_1 = \\frac{\\rho_1}{\\rho_0} \\frac{\\alpha_0}{\\omega_0} = \\frac{1}{1} \\frac{1}{1} = 1$.\n3. $p_1 = r_0 + \\beta_1(p_0 - \\omega_0 v_0) = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + 1 \\left( \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} - 1 \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} \\right) = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\n4. $v_1 = A p_1 = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 3 \\end{pmatrix}$.\n5. The denominator for $\\alpha_1$ is $(\\hat{r}_0, v_1) = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 3 \\end{pmatrix} = 1$. Thus, $\\alpha_1 = \\frac{\\rho_1}{(\\hat{r}_0, v_1)} = \\frac{1}{1} = 1$.\n6. $s_1 = r_0 - \\alpha_1 v_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} - 1 \\begin{pmatrix} 1 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -3 \\end{pmatrix}$.\n7. $t_1 = A s_1 = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ -3 \\end{pmatrix} = \\begin{pmatrix} -6 \\\\ -12 \\end{pmatrix}$.\n8. For $\\omega_1$, we compute the inner products:\n   $(t_1, s_1) = \\begin{pmatrix} -6 & -12 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ -3 \\end{pmatrix} = (0)(-6) + (-3)(-12) = 36$.\n   $(t_1, t_1) = \\begin{pmatrix} -6 & -12 \\end{pmatrix} \\begin{pmatrix} -6 \\\\ -12 \\end{pmatrix} = (-6)^2 + (-12)^2 = 36 + 144 = 180$.\n   So, $\\omega_1 = \\frac{(t_1, s_1)}{(t_1, t_1)} = \\frac{36}{180} = \\frac{1}{5}$.\n9. $x_1 = x_0 + \\alpha_1 p_1 + \\omega_1 s_1 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + 1 \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + \\frac{1}{5} \\begin{pmatrix} 0 \\\\ -3 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -\\frac{3}{5} \\end{pmatrix}$.\n10. $r_1 = s_1 - \\omega_1 t_1 = \\begin{pmatrix} 0 \\\\ -3 \\end{pmatrix} - \\frac{1}{5} \\begin{pmatrix} -6 \\\\ -12 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -3 \\end{pmatrix} - \\begin{pmatrix} -\\frac{6}{5} \\\\ -\\frac{12}{5} \\end{pmatrix} = \\begin{pmatrix} \\frac{6}{5} \\\\ -\\frac{3}{5} \\end{pmatrix}$.\n\nAfter the first iteration, the residual is $r_1 = \\begin{pmatrix} \\frac{6}{5} \\\\ -\\frac{3}{5} \\end{pmatrix}$, which is not the zero vector. This substantiates the first claim.\n\n**Iteration 2 ($k=2$):**\n1. $\\rho_2 = (\\hat{r}_0, r_1) = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} \\frac{6}{5} \\\\ -\\frac{3}{5} \\end{pmatrix} = \\frac{6}{5}$.\n2. $\\beta_2 = \\frac{\\rho_2}{\\rho_1} \\frac{\\alpha_1}{\\omega_1} = \\frac{6/5}{1} \\frac{1}{1/5} = \\frac{6}{5} \\cdot 5 = 6$.\n3. $p_2 = r_1 + \\beta_2(p_1 - \\omega_1 v_1) = \\begin{pmatrix} \\frac{6}{5} \\\\ -\\frac{3}{5} \\end{pmatrix} + 6 \\left( \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} - \\frac{1}{5} \\begin{pmatrix} 1 \\\\ 3 \\end{pmatrix} \\right) = \\begin{pmatrix} \\frac{6}{5} \\\\ -\\frac{3}{5} \\end{pmatrix} + 6 \\begin{pmatrix} \\frac{4}{5} \\\\ -\\frac{3}{5} \\end{pmatrix} = \\begin{pmatrix} \\frac{6}{5} + \\frac{24}{5} \\\\ -\\frac{3}{5} - \\frac{18}{5} \\end{pmatrix} = \\begin{pmatrix} \\frac{30}{5} \\\\ -\\frac{21}{5} \\end{pmatrix} = \\begin{pmatrix} 6 \\\\ -\\frac{21}{5} \\end{pmatrix}$.\n4. $v_2 = A p_2 = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix} \\begin{pmatrix} 6 \\\\ -\\frac{21}{5} \\end{pmatrix} = \\begin{pmatrix} 6 - \\frac{42}{5} \\\\ 18 - \\frac{84}{5} \\end{pmatrix} = \\begin{pmatrix} \\frac{30-42}{5} \\\\ \\frac{90-84}{5} \\end{pmatrix} = \\begin{pmatrix} -\\frac{12}{5} \\\\ \\frac{6}{5} \\end{pmatrix}$.\n5. The denominator for $\\alpha_2$ is $(\\hat{r}_0, v_2) = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} -\\frac{12}{5} \\\\ \\frac{6}{5} \\end{pmatrix} = -\\frac{12}{5}$. Thus, $\\alpha_2 = \\frac{\\rho_2}{(\\hat{r}_0, v_2)} = \\frac{6/5}{-12/5} = -\\frac{1}{2}$.\n6. $s_2 = r_1 - \\alpha_2 v_2 = \\begin{pmatrix} \\frac{6}{5} \\\\ -\\frac{3}{5} \\end{pmatrix} - \\left(-\\frac{1}{2}\\right) \\begin{pmatrix} -\\frac{12}{5} \\\\ \\frac{6}{5} \\end{pmatrix} = \\begin{pmatrix} \\frac{6}{5} \\\\ -\\frac{3}{5} \\end{pmatrix} - \\begin{pmatrix} \\frac{6}{5} \\\\ -\\frac{3}{5} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\n7. Since $s_2 = \\mathbf{0}$, we have $t_2 = A s_2 = A \\mathbf{0} = \\mathbf{0}$.\n8. The formula for $\\omega_2$ becomes $\\frac{(t_2, s_2)}{(t_2, t_2)} = \\frac{0}{0}$. This is a so-called \"lucky breakdown\", which indicates that the exact solution will be found at this step. We can set $\\omega_2 = 0$.\n9. $x_2 = x_1 + \\alpha_2 p_2 + \\omega_2 s_2 = \\begin{pmatrix} 1 \\\\ -\\frac{3}{5} \\end{pmatrix} + \\left(-\\frac{1}{2}\\right) \\begin{pmatrix} 6 \\\\ -\\frac{21}{5} \\end{pmatrix} + 0 \\cdot \\mathbf{0} = \\begin{pmatrix} 1 - 3 \\\\ -\\frac{3}{5} + \\frac{21}{10} \\end{pmatrix} = \\begin{pmatrix} -2 \\\\ -\\frac{6}{10} + \\frac{21}{10} \\end{pmatrix} = \\begin{pmatrix} -2 \\\\ \\frac{15}{10} \\end{pmatrix} = \\begin{pmatrix} -2 \\\\ \\frac{3}{2} \\end{pmatrix}$.\n10. $r_2 = s_2 - \\omega_2 t_2 = \\mathbf{0} - 0 \\cdot \\mathbf{0} = \\mathbf{0}$.\n\nThe residual $r_2$ is the zero vector, which means the algorithm has converged to the exact solution. This substantiates the second claim. To verify, we check the solution:\n$A x_2 = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix} \\begin{pmatrix} -2 \\\\ \\frac{3}{2} \\end{pmatrix} = \\begin{pmatrix} 1(-2) + 2(\\frac{3}{2}) \\\\ 3(-2) + 4(\\frac{3}{2}) \\end{pmatrix} = \\begin{pmatrix} -2 + 3 \\\\ -6 + 6 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = b$.\nThe solution is correct.\n\nThe problem asks for the value of the first smoothing parameter, $\\omega_1$. As calculated in the first iteration, this value is $\\omega_1 = \\frac{1}{5}$.", "answer": "$$\n\\boxed{\\frac{1}{5}}\n$$", "id": "2374444"}, {"introduction": "The behavior of Krylov subspace methods is deeply connected to the spectral properties of the system matrix. This practice explores a fascinating special case where the initial residual is an eigenvector of the matrix $A$. By working through the first iteration of BiCGSTAB under this condition, you will see how the algorithm's steps simplify and directly relate to the corresponding eigenvalue, offering a clearer view into the structure of the underlying Krylov subspace [@problem_id:2374426].", "problem": "Consider the linear system $A x = b$ with \n$$\nA = \\begin{pmatrix}\n2 & 5 \\\\\n0 & 3\n\\end{pmatrix}.\n$$\nChoose the right-hand side $b$ to be an eigenvector of $A$ associated with the eigenvalue $2$, specifically \n$$\nb = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}.\n$$\nAssume the BiConjugate Gradient Stabilized (BiCGSTAB) method is used to solve $A x = b$ with the initial guess $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ and the shadow residual $\\hat{r}_0$ chosen as $\\hat{r}_0 = r_0$, where $r_0 = b - A x_0$. Work in exact arithmetic.\n\nDetermine the scalar $\\alpha$ computed during the first BiCGSTAB iteration. Provide your answer as an exact value with no rounding.", "solution": "The problem is first validated for scientific soundness, completeness, and objectivity.\n\n**Step 1: Extract Givens**\n-   Linear system: $A x = b$.\n-   Matrix: $A = \\begin{pmatrix} 2 & 5 \\\\ 0 & 3 \\end{pmatrix}$.\n-   Right-hand side: $b = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\n-   Premise: $b$ is an eigenvector of $A$ with eigenvalue $\\lambda = 2$.\n-   Method: BiConjugate Gradient Stabilized (BiCGSTAB).\n-   Initial guess: $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\n-   Initial shadow residual: $\\hat{r}_0 = r_0$.\n-   Arithmetic: Exact.\n-   Objective: Determine the scalar $\\alpha$ computed during the first iteration.\n\n**Step 2: Validate Givens**\nThe problem is well-defined within numerical linear algebra. We verify the premise that $b$ is an eigenvector of $A$ with eigenvalue $2$:\n$$\nA b = \\begin{pmatrix} 2 & 5 \\\\ 0 & 3 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} = 2 \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = 2 b.\n$$\nThe premise is correct. The problem is scientifically grounded, self-contained, and objective.\n\n**Step 3: Proceed with Solution**\nWe apply the BiCGSTAB algorithm, using the same 1-indexed formulation as in the previous hands-on practice.\n\n**Initialization ($k=0$):**\n-   Initial guess: $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\n-   Initial residual: $r_0 = b - A x_0 = b = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\n-   Initial shadow residual: $\\hat{r}_0 = r_0 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\n-   Initial parameters: $\\rho_0 = 1, \\alpha_0 = 1, \\omega_0 = 1$.\n-   Initial direction vectors: $p_0 = \\mathbf{0}, v_0 = \\mathbf{0}$.\n\n**First Iteration ($k=1$):**\n1.  Compute $\\rho_1 = (\\hat{r}_0, r_0) = r_0^T r_0 = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = 1$.\n2.  Compute $\\beta_1 = \\frac{\\rho_1}{\\rho_0} \\frac{\\alpha_0}{\\omega_0} = \\frac{1}{1} \\frac{1}{1} = 1$.\n3.  Update the search direction: $p_1 = r_0 + \\beta_1 (p_0 - \\omega_0 v_0) = r_0 + 1(\\mathbf{0} - 1 \\cdot \\mathbf{0}) = r_0 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\n4.  Compute the matrix-vector product: $v_1 = A p_1 = A r_0$. Since $r_0 = b$ is an eigenvector of $A$ with eigenvalue $\\lambda=2$, we have $v_1 = A b = 2b = 2r_0 = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}$.\n5.  Compute the scalar $\\alpha_1$:\n    $$\n    \\alpha_1 = \\frac{\\rho_1}{(\\hat{r}_0, v_1)} = \\frac{(r_0, r_0)}{(r_0, 2r_0)} = \\frac{r_0^T r_0}{2(r_0^T r_0)}\n    $$\n    Since $r_0$ is not the zero vector, $r_0^T r_0 \\neq 0$, and we can cancel this term:\n    $$\n    \\alpha_1 = \\frac{1}{2}\n    $$\nThe scalar $\\alpha$ computed during the first iteration is $\\frac{1}{2}$.", "answer": "$$\\boxed{\\frac{1}{2}}$$", "id": "2374426"}, {"introduction": "While theoretical exercises in exact arithmetic are crucial for understanding, real-world computation happens in finite precision, which introduces new challenges. This hands-on coding problem addresses one of the most important and subtle pitfalls in numerical linear algebra: the divergence between a small residual and a small solution error. By simulating BiCGSTAB on a severely ill-conditioned system, you will witness how a solver can report 'convergence' based on the residual norm, yet yield a solution that is far from correct, a phenomenon known as wrong convergence [@problem_id:2374413].", "problem": "Let $A \\in \\mathbb{R}^{n \\times n}$ be a real, square, nonsingular matrix and let $x_{\\star} \\in \\mathbb{R}^{n}$ be the exact solution to the linear system $A x = b$, where $b = A x_{\\star}$. Consider computing an approximate solution $\\hat{x}$ in IEEE 754 double-precision (binary64) arithmetic by iterating from the initial guess $x_0 = 0$ using the BiConjugate Gradient Stabilized (BiCGSTAB) method. The iteration must terminate when the Euclidean norm of the residual $r_k = b - A x_k$ satisfies\n$$\n\\lVert r_k \\rVert_2 \\le \\max\\left( \\tau_{\\mathrm{rel}} \\lVert b \\rVert_2, \\, \\tau_{\\mathrm{abs}} \\right),\n$$\nor when a prescribed maximum number of iterations $k_{\\max}$ is reached, whichever occurs first. Declare that the method has “converged” if and only if the residual-based stopping condition is met before reaching $k_{\\max}$. After termination, compute the relative forward error\n$$\n\\varepsilon_{\\mathrm{fwd}} = \\frac{\\lVert \\hat{x} - x_{\\star} \\rVert_2}{\\lVert x_{\\star} \\rVert_2}.\n$$\nFor a fixed threshold $\\theta = 10^{-2}$, define a test to “flag wrong convergence” if and only if the method has “converged” and simultaneously $\\varepsilon_{\\mathrm{fwd}} > \\theta$. For each test case, output the integer $1$ if wrong convergence is flagged and $0$ otherwise.\n\nAll arithmetic must be carried out in IEEE 754 double-precision (binary64). Angles are not used, and no physical units are involved.\n\nTest suite. Your program must evaluate exactly the following three cases:\n\n- Case 1 (well-conditioned, nonsymmetric, happy path): Let $n = 10$. Construct $A \\in \\mathbb{R}^{10 \\times 10}$ by first forming a matrix $T$ with entries sampled independently from the uniform distribution on $[-1,1]$ using a fixed pseudorandom seed $s = 123456$, then setting\n$$\nA = T + 10 I_{10},\n$$\nwhere $I_{10}$ is the $10 \\times 10$ identity matrix. Let $x_{\\star} = \\mathbf{1} \\in \\mathbb{R}^{10}$ be the vector of all ones, and set $b = A x_{\\star}$. Use $\\tau_{\\mathrm{rel}} = 10^{-8}$, $\\tau_{\\mathrm{abs}} = 0$, and $k_{\\max} = 200$.\n\n- Case 2 (severely ill-conditioned, nearly singular $2 \\times 2$ system that induces wrong convergence under absolute stopping): Let\n$$\nA = \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 + \\epsilon \\end{bmatrix}, \\quad \\epsilon = 10^{-14}.\n$$\nLet $x_{\\star} = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}$ and set $b = A x_{\\star}$. Use $\\tau_{\\mathrm{rel}} = 0$, $\\tau_{\\mathrm{abs}} = 10^{-12}$, and $k_{\\max} = 200$.\n\n- Case 3 (boundary case, scalar system): Let $n = 1$, $A = [3]$, $x_{\\star} = [2]$, and $b = A x_{\\star} = [6]$. Use $\\tau_{\\mathrm{rel}} = 10^{-14}$, $\\tau_{\\mathrm{abs}} = 0$, and $k_{\\max} = 20$.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered as $[z_1, z_2, z_3]$, where $z_i \\in \\{0,1\\}$ is the integer flag for Case $i$ as defined above. For example, an output line could look like\n$[0,1,0]$.", "solution": "Before proceeding to a solution, the problem statement is rigorously validated.\n\nFirst, we extract the givens presented in the problem statement.\nThe problem concerns the solution of a linear system $A x = b$ where $A \\in \\mathbb{R}^{n \\times n}$ is a real, square, nonsingular matrix, $x_{\\star} \\in \\mathbb{R}^{n}$ is the exact solution, and $b = A x_{\\star}$. An approximate solution $\\hat{x}$ is computed using the BiConjugate Gradient Stabilized (BiCGSTAB) method in IEEE 754 double-precision arithmetic, starting from an initial guess $x_0 = 0$.\n\nThe iteration termination conditions are:\n1.  The Euclidean norm of the residual $r_k = b - A x_k$ satisfies $\\lVert r_k \\rVert_2 \\le \\max\\left( \\tau_{\\mathrm{rel}} \\lVert b \\rVert_2, \\, \\tau_{\\mathrm{abs}} \\right)$.\n2.  A maximum number of iterations $k_{\\max}$ is reached.\n\nThe process has \"converged\" if and only if the residual-based stopping condition (1) is met before reaching $k_{\\max}$.\n\nAfter termination, the relative forward error is computed as $\\varepsilon_{\\mathrm{fwd}} = \\frac{\\lVert \\hat{x} - x_{\\star} \\rVert_2}{\\lVert x_{\\star} \\rVert_2}$.\n\nA test for \"wrong convergence\" is defined. The test is flagged (output $1$) if and only if the method has \"converged\" and simultaneously $\\varepsilon_{\\mathrm{fwd}} > \\theta$, where the threshold is fixed at $\\theta = 10^{-2}$. Otherwise, the output is $0$.\n\nThe test suite consists of three cases:\n-   Case 1: $n = 10$. $A = T + 10 I_{10}$, where $T_{ij} \\sim U[-1,1]$ generated with seed $s = 123456$. $x_{\\star} = \\mathbf{1} \\in \\mathbb{R}^{10}$. $b = A x_{\\star}$. Parameters: $\\tau_{\\mathrm{rel}} = 10^{-8}$, $\\tau_{\\mathrm{abs}} = 0$, $k_{\\max} = 200$.\n-   Case 2: $n = 2$. $A = \\begin{bmatrix} 1  1 \\\\ 1  1 + \\epsilon \\end{bmatrix}$ with $\\epsilon = 10^{-14}$. $x_{\\star} = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}$. $b = A x_{\\star}$. Parameters: $\\tau_{\\mathrm{rel}} = 0$, $\\tau_{\\mathrm{abs}} = 10^{-12}$, $k_{\\max} = 200$.\n-   Case 3: $n = 1$. $A = [3]$, $x_{\\star} = [2]$, $b = A x_{\\star} = [6]$. Parameters: $\\tau_{\\mathrm{rel}} = 10^{-14}$, $\\tau_{\\mathrm{abs}} = 0$, $k_{\\max} = 20$.\n\nNext, we validate these givens. The problem is rooted in numerical linear algebra, a core discipline of computational engineering. The BiCGSTAB method is a standard, well-established algorithm for solving nonsymmetric linear systems. The concepts of residual, forward error, stopping criteria, and machine precision are fundamental to this field. The problem is therefore scientifically grounded. The definitions are precise, objective, and free of ambiguity. Each test case is fully specified with all necessary data ($A$, $x_{\\star}$, and solver parameters), ensuring the problem is self-contained and well-posed. The matrices are confirmed to be nonsingular as required. There are no internal contradictions or scientifically implausible conditions.\n\nThe analysis concludes that the problem is scientifically sound, well-posed, and objective. It is deemed valid, and a solution is constructed.\n\nThe solution requires the implementation of the BiCGSTAB algorithm. For a linear system $A x = b$ and an initial guess $x_0$, the algorithm iteratively refines the solution. We begin with $x_0 = 0$ and the corresponding initial residual $r_0 = b - A x_0 = b$. The standard choice for the initial shadow residual is $\\hat{r}_0 = r_0$. We initialize scalar parameters $\\rho_0 = 1$, $\\alpha_0 = 1$, $\\omega_0 = 1$, and vector parameters $p_0 = 0$, $v_0 = 0$.\n\nFor each iteration $k = 1, 2, \\dots, k_{\\max}$:\n1.  Compute $\\rho_k = \\hat{r}_0^T r_{k-1}$. A breakdown occurs if $\\rho_k = 0$.\n2.  Compute $\\beta = (\\rho_k / \\rho_{k-1}) \\cdot (\\alpha_{k-1} / \\omega_{k-1})$.\n3.  Update the search direction: $p_k = r_{k-1} + \\beta (p_{k-1} - \\omega_{k-1} v_{k-1})$.\n4.  Compute the matrix-vector product $v_k = A p_k$.\n5.  Compute the step length $\\alpha_k = \\rho_k / (\\hat{r}_0^T v_k)$. A breakdown occurs if the denominator is zero.\n6.  Compute an intermediate residual $s = r_{k-1} - \\alpha_k v_k$.\n7.  Compute the matrix-vector product $t = A s$.\n8.  Compute the stabilization parameter $\\omega_k = (t^T s) / (t^T t)$. A breakdown can occur if $t^T t = 0$.\n9.  Update the solution: $x_k = x_{k-1} + \\alpha_k p_k + \\omega_k s$.\n10. Update the residual: $r_k = s - \\omega_k t$.\n11. Check for convergence: test if $\\lVert r_k \\rVert_2 \\le \\max(\\tau_{\\mathrm{rel}} \\lVert b \\rVert_2, \\tau_{\\mathrm{abs}})$. If true, terminate with `converged = True`.\n\nIf the loop completes without meeting the convergence criterion, the process terminates with `converged = False`. The final approximate solution is denoted $\\hat{x}$.\n\nFor each test case, this algorithm will be executed. After termination, we compute the relative forward error $\\varepsilon_{\\mathrm{fwd}} = \\lVert \\hat{x} - x_{\\star} \\rVert_2 / \\lVert x_{\\star} \\rVert_2$. The \"wrong convergence\" flag is set to $1$ if the algorithm reported convergence (`converged = True`) but the solution is poor ($\\varepsilon_{\\mathrm{fwd}}  10^{-2}$). Otherwise, the flag is $0$.\n\nCase 1 represents a typical well-conditioned problem. The condition number of $A$ is small, so we expect BiCGSTAB to converge rapidly and for the small final residual to correspond to a small forward error. The flag should be $0$.\n\nCase 2 is designed to expose a classic pathology. The matrix $A$ is severely ill-conditioned. The true solution is $x_{\\star} = [1, -1]^T$, resulting in a right-hand side vector $b = A x_{\\star} = [0, -\\epsilon]^T$. The initial residual $r_0=b$ has a norm of $\\epsilon = 10^{-14}$. This is smaller than the absolute tolerance $\\tau_{\\mathrm{abs}} = 10^{-12}$. Consequently, the algorithm will terminate at iteration $k=0$ with `converged = True` and $\\hat{x} = x_0 = 0$. The forward error will be $\\varepsilon_{\\mathrm{fwd}} = \\lVert 0 - x_{\\star} \\rVert_2 / \\lVert x_{\\star} \\rVert_2 = 1$, which is greater than $\\theta=10^{-2}$. Thus, the flag for this case will be $1$.\n\nCase 3 is a simple $1 \\times 1$ system. For such a simple case, BiCGSTAB is expected to find the exact solution in a single iteration. The forward error will be $0$ (or near machine precision), so the flag will be $0$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_bicgstab(A, b, x0, tau_rel, tau_abs, k_max):\n    \"\"\"\n    Implements the BiConjugate Gradient Stabilized (BiCGSTAB) method.\n\n    Args:\n        A (np.ndarray): The square matrix of the linear system.\n        b (np.ndarray): The right-hand side vector.\n        x0 (np.ndarray): The initial guess for the solution.\n        tau_rel (float): The relative tolerance for the stopping condition.\n        tau_abs (float): The absolute tolerance for the stopping condition.\n        k_max (int): The maximum number of iterations.\n\n    Returns:\n        tuple: A tuple containing:\n            - np.ndarray: The approximate solution vector.\n            - bool: A flag indicating if the method converged.\n    \"\"\"\n    n = A.shape[0]\n    \n    # Use IEEE 754 double precision (np.float64) for all arithmetic\n    x = x0.copy().astype(np.float64)\n    r = (b - A @ x).astype(np.float64)\n\n    norm_b = np.linalg.norm(b)\n    stop_tol = max(tau_rel * norm_b, tau_abs)\n\n    # Check for convergence at k=0\n    if np.linalg.norm(r) = stop_tol:\n        return x, True\n\n    # BiCGSTAB algorithm setup\n    r_hat0 = r.copy()\n    rho_prev = 1.0\n    alpha = 1.0\n    omega = 1.0\n    p = np.zeros(n, dtype=np.float64)\n    v = np.zeros(n, dtype=np.float64)\n\n    for k in range(1, k_max + 1):\n        rho_curr = np.dot(r_hat0, r)\n        \n        # Breakdown condition 1\n        if rho_curr == 0.0:\n            return x, False\n\n        # On the first iteration, beta calculation uses rho_prev=1, alpha=1, omega=1\n        # which effectively makes p = r, as p_prev and v_prev are zero.\n        beta = (rho_curr / rho_prev) * (alpha / omega)\n        p = r + beta * (p - omega * v)\n\n        v = A @ p\n        \n        r_hat0_dot_v = np.dot(r_hat0, v)\n        \n        # Breakdown condition 2\n        if r_hat0_dot_v == 0.0:\n            return x, False\n        \n        alpha = rho_curr / r_hat0_dot_v\n\n        s = r - alpha * v\n        t = A @ s\n        \n        t_dot_t = np.dot(t, t)\n        \n        # Breakdown condition 3: t is numerically zero.\n        if t_dot_t  np.finfo(np.float64).eps:\n            x = x + alpha * p\n            r = s\n            # The residual is now 's'. Check convergence and return.\n            return x, np.linalg.norm(r) = stop_tol\n\n        omega = np.dot(t, s) / t_dot_t\n\n        x = x + alpha * p + omega * s\n        r = s - omega * t\n        \n        # Check termination condition after iteration k\n        if np.linalg.norm(r) = stop_tol:\n            return x, True\n\n        # Update for next iteration\n        rho_prev = rho_curr\n\n    return x, False # Max iterations reached\n\ndef solve():\n    \"\"\"\n    Sets up and runs the test suite for the BiCGSTAB problem.\n    \"\"\"\n    theta = 1e-2\n\n    # Case 1: Well-conditioned, nonsymmetric\n    n1 = 10\n    seed = 123456\n    rng = np.random.default_rng(seed)\n    T1 = rng.uniform(-1, 1, (n1, n1))\n    A1 = T1 + 10 * np.eye(n1, dtype=np.float64)\n    x_star1 = np.ones(n1, dtype=np.float64)\n    b1 = A1 @ x_star1\n    tau_rel1 = 1e-8\n    tau_abs1 = 0.0\n    k_max1 = 200\n\n    # Case 2: Ill-conditioned, wrong convergence\n    epsilon = 1e-14\n    A2 = np.array([[1, 1], [1, 1 + epsilon]], dtype=np.float64)\n    x_star2 = np.array([1, -1], dtype=np.float64)\n    b2 = A2 @ x_star2\n    tau_rel2 = 0.0\n    tau_abs2 = 1e-12\n    k_max2 = 200\n\n    # Case 3: Scalar system\n    A3 = np.array([[3]], dtype=np.float64)\n    x_star3 = np.array([2], dtype=np.float64)\n    b3 = A3 @ x_star3\n    tau_rel3 = 1e-14\n    tau_abs3 = 0.0\n    k_max3 = 20\n\n    test_cases = [\n        (A1, x_star1, b1, tau_rel1, tau_abs1, k_max1),\n        (A2, x_star2, b2, tau_rel2, tau_abs2, k_max2),\n        (A3, x_star3, b3, tau_rel3, tau_abs3, k_max3),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        A, x_star, b, tau_rel, tau_abs, k_max = case\n        n = A.shape[0]\n        x0 = np.zeros(n, dtype=np.float64)\n        \n        hat_x, converged = run_bicgstab(A, b, x0, tau_rel, tau_abs, k_max)\n        \n        norm_x_star = np.linalg.norm(x_star)\n        \n        # The problem guarantees x_star is not zero, so norm_x_star is non-zero.\n        eps_fwd = np.linalg.norm(hat_x - x_star) / norm_x_star\n\n        flag = 0\n        if converged and eps_fwd > theta:\n            flag = 1\n        \n        results.append(flag)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "2374413"}]}