{"hands_on_practices": [{"introduction": "The spectral radius $\\rho(B_J)$ is the ultimate arbiter of convergence for the Jacobi method, but it is often expensive to compute. A much simpler-to-calculate upper bound is given by the matrix infinity norm, $\\lVert B_J \\rVert_\\infty$. This practice guides you through a numerical experiment to explore the relationship between these two quantities, helping you build crucial intuition about when this convenient bound is reliable and when it can be misleading.", "problem": "Consider solving a linear system with the Jacobi method. Let $A \\in \\mathbb{R}^{n \\times n}$ be a nonsingular matrix with diagonal $D$, strict lower part $L$, and strict upper part $U$, so that $A = D - L - U$. The Jacobi iteration takes the form $x^{(k+1)} = D^{-1}(b + (L + U)x^{(k)})$, with iteration matrix $B_J = D^{-1}(L + U)$. For a given matrix $B$, the spectral radius $\\rho(B)$ is defined as the maximum modulus of its eigenvalues. A core fact from induced matrix norms is that $\\rho(B) \\le \\lVert B \\rVert$ for any induced norm, and in particular $\\rho(B_J) \\le \\lVert B_J \\rVert_\\infty$, where $\\lVert \\cdot \\rVert_\\infty$ is the induced infinity norm (maximum absolute row sum).\n\nDesign a numerical experiment to compare the tightness of the infinity-norm-based upper bound on $\\rho(B_J)$ across different matrices $A$. All computations must be performed purely in $\\mathbb{R}$ (no physical units are involved). Start from the fundamental definitions of the Jacobi iteration matrix and induced infinity norm. For each test case, you must:\n- Construct the specified matrix $A$ using only the given parameters.\n- Form $B_J$ directly from $A$ via $B_J(i,j) = -a_{ij}/a_{ii}$ for $i \\ne j$ and $B_J(i,i) = 0$.\n- Compute the bound as $\\lVert B_J \\rVert_\\infty$ (the maximum absolute row sum of $B_J$).\n- Compute the spectral radius $\\rho(B_J)$ as the maximum modulus of the eigenvalues of $B_J$.\n- Report the nonnegative gap $\\lVert B_J \\rVert_\\infty - \\rho(B_J)$ as a float, which quantifies how loose the bound is (zero means tight).\n- Also report a boolean indicating whether the bound is numerically tight, defined here as $\\lVert B_J \\rVert_\\infty - \\rho(B_J) \\le 10^{-10}$.\n\nYou must implement the following test suite of matrices $A$ (each fully determined by the parameters below), chosen to probe different behaviors of tightness:\n\n- Case 1 (exactly or nearly tight via constant off-diagonal structure): $A \\in \\mathbb{R}^{n \\times n}$ with $a_{ii} = 1$ for all $i$ and $a_{ij} = -s/(n-1)$ for all $i \\ne j$. Parameters: $n = 5$, $s = 0.9$.\n- Case 2 (loose for small tridiagonal Toeplitz): $A = d I - c T$, where $T$ is the tridiagonal matrix with $1$ on the first sub- and super-diagonals and $0$ elsewhere. Parameters: $n = 3$, $d = 1.0$, $c = 0.45$.\n- Case 3 (extremely loose via nilpotent $B_J$): $A = I - U$, where $U$ has a constant value $t$ on the first super-diagonal and zeros elsewhere (so $U$ is strictly upper triangular). Parameters: $n = 6$, $t = 0.9$.\n- Case 4 (near-tight for large tridiagonal Toeplitz): $A = d I - c T$ with the same definition of $T$ as in Case 2. Parameters: $n = 50$, $d = 1.0$, $c = 0.45$.\n\nYour program must:\n- Build each $A$ exactly from its parameters as described, without using any randomness.\n- For each case, compute:\n  1) the bound $\\lVert B_J \\rVert_\\infty$,\n  2) the spectral radius $\\rho(B_J)$,\n  3) the gap $\\lVert B_J \\rVert_\\infty - \\rho(B_J)$,\n  4) the boolean tightness indicator as defined above.\n- Round all floating-point outputs to six decimal places before printing.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a list containing four entries in the order specified above. For example, a valid output with two cases would look like $[[b_1,\\;r_1,\\;g_1,\\;t_1],[b_2,\\;r_2,\\;g_2,\\;t_2]]$, where $b_k$, $r_k$, and $g_k$ are floats (rounded to six decimals) and $t_k$ is a boolean.", "solution": "The problem as stated is valid. It is based on established principles of numerical linear algebra, specifically the convergence analysis of the Jacobi iterative method. All definitions, including the Jacobi iteration matrix $B_J$, the spectral radius $\\rho(B)$, and the induced infinity norm $\\lVert \\cdot \\rVert_\\infty$, are standard. The problem is self-contained, with all necessary parameters provided for each test case, and poses a well-defined computational task. No scientific inconsistencies, ambiguities, or contradictions are present.\n\nThe objective is to conduct a numerical experiment to assess the tightness of the bound $\\rho(B_J) \\le \\lVert B_J \\rVert_\\infty$. For each specified matrix $A$, we must compute the iteration matrix $B_J$, its spectral radius $\\rho(B_J)$, the infinity norm $\\lVert B_J \\rVert_\\infty$, the gap $\\lVert B_J \\rVert_\\infty - \\rho(B_J)$, and a boolean indicator for numerical tightness.\n\nLet us proceed with a formal, step-by-step derivation and implementation plan for each test case.\n\nThe Jacobi iteration matrix $B_J$ is defined as $B_J = D^{-1}(L+U)$, where $A = D-L-U$. Here, $D$ is the diagonal part of $A$, $-L$ is the strictly lower triangular part, and $-U$ is the strictly upper triangular part. An equivalent and computationally convenient formula for $B_J$ is $B_J = I - D^{-1}A$. The components of $B_J$ are given by $(B_J)_{ii} = 0$ for all $i$, and $(B_J)_{ij} = -a_{ij}/a_{ii}$ for $i \\ne j$. This requires $a_{ii} \\ne 0$ for all $i$, a condition that holds for all specified test cases.\n\nThe quantities to be computed are:\n$1$. The infinity norm, $\\lVert B_J \\rVert_\\infty = \\max_{1 \\le i \\le n} \\sum_{j=1}^{n} |(B_J)_{ij}|$.\n$2$. The spectral radius, $\\rho(B_J) = \\max \\{|\\lambda| : \\lambda \\text{ is an eigenvalue of } B_J\\}$.\n$3$. The gap, $g = \\lVert B_J \\rVert_\\infty - \\rho(B_J)$.\n$4$. The tightness boolean, $t = (g \\le 10^{-10})$.\n\nWe will now analyze each case and construct the required matrices and compute the desired values.\n\n**Case 1: Constant Off-Diagonal Structure**\n- Parameters: $n=5$, $s=0.9$.\n- Matrix $A \\in \\mathbb{R}^{5 \\times 5}$ is defined by $a_{ii} = 1$ and $a_{ij} = -s/(n-1) = -0.9/4 = -0.225$ for $i \\ne j$.\n- The Jacobi matrix $B_J$ has elements $(B_J)_{ij} = -a_{ij}/a_{ii} = -(-0.225)/1 = 0.225$ for $i \\ne j$, and $(B_J)_{ii} = 0$.\n- The infinity norm is the maximum absolute row sum. For any row $i$, $\\sum_{j=1}^{n} |(B_J)_{ij}| = (n-1) \\times 0.225 = 4 \\times 0.225 = 0.9$. Thus, $\\lVert B_J \\rVert_\\infty = 0.9$.\n- The matrix $B_J$ can be written as $k(J-I)$, where $k=0.225$, $J$ is the all-ones matrix, and $I$ is the identity matrix. The eigenvalues of $J$ are $\\{n, 0, \\dots, 0\\}$. The eigenvalues of $J-I$ are $\\{n-1, -1, \\dots, -1\\}$. For $n=5$, they are $\\{4, -1, -1, -1, -1\\}$.\n- The eigenvalues of $B_J$ are $k$ times these values: $\\{0.225 \\times 4, 0.225 \\times (-1), \\dots\\} = \\{0.9, -0.225, -0.225, -0.225, -0.225\\}$.\n- The spectral radius is $\\rho(B_J) = \\max(|0.9|, |-0.225|) = 0.9$.\n- The gap is $\\lVert B_J \\rVert_\\infty - \\rho(B_J) = 0.9 - 0.9 = 0$. The bound is exactly tight.\n\n**Case 2: Small Tridiagonal Toeplitz Matrix**\n- Parameters: $n=3$, $d=1.0$, $c=0.45$.\n- Matrix $A = dI - cT$, where $T$ has $1$s on the first sub- and super-diagonals.\n$$A = \\begin{pmatrix} 1.0  -0.45  0 \\\\ -0.45  1.0  -0.45 \\\\ 0  -0.45  1.0 \\end{pmatrix}$$\n- Since $a_{ii} = d = 1.0$, the Jacobi matrix is $B_J = cT$.\n$$B_J = \\begin{pmatrix} 0  0.45  0 \\\\ 0.45  0  0.45 \\\\ 0  0.45  0 \\end{pmatrix}$$\n- The absolute row sums are $\\{0.45, 0.9, 0.45\\}$. The maximum is $\\lVert B_J \\rVert_\\infty = 0.9$.\n- The eigenvalues of such a tridiagonal matrix $T$ of size $n \\times n$ are $\\lambda_k = 2\\cos(\\frac{k\\pi}{n+1})$ for $k=1, \\dots, n$.\n- For $n=3$, the eigenvalues of $T$ are $2\\cos(\\pi/4) = \\sqrt{2}$, $2\\cos(2\\pi/4) = 0$, and $2\\cos(3\\pi/4) = -\\sqrt{2}$.\n- The eigenvalues of $B_J = cT$ are $\\{0.45\\sqrt{2}, 0, -0.45\\sqrt{2}\\}$.\n- The spectral radius is $\\rho(B_J) = |0.45\\sqrt{2}| \\approx 0.636396$.\n- The gap is $0.9 - 0.636396... \\approx 0.263604$. The bound is not tight.\n\n**Case 3: Nilpotent Jacobi Matrix**\n- Parameters: $n=6$, $t=0.9$.\n- Matrix $A = I - U$, where $U$ is a matrix with $t=0.9$ on the first super-diagonal and zeros elsewhere.\n$$A = \\begin{pmatrix} 1  -0.9  0  0  0  0 \\\\ 0  1  -0.9  0  0  0 \\\\ 0  0  1  -0.9  0  0 \\\\ 0  0  0  1  -0.9  0 \\\\ 0  0  0  0  1  -0.9 \\\\ 0  0  0  0  0  1 \\end{pmatrix}$$\n- The diagonal entries are $a_{ii}=1$. The Jacobi matrix is $B_J = I - D^{-1}A = I - I^{-1}(I - U) = U$.\n- So, $B_J$ has $0.9$ on its first super-diagonal and is zero elsewhere.\n- The absolute row sums are $\\{0.9, 0.9, 0.9, 0.9, 0.9, 0\\}$. The maximum is $\\lVert B_J \\rVert_\\infty = 0.9$.\n- $B_J$ is a strictly upper triangular matrix. Its eigenvalues are its diagonal entries, which are all $0$.\n- The spectral radius is $\\rho(B_J) = 0$.\n- The gap is $0.9 - 0 = 0.9$. The bound is extremely loose.\n\n**Case 4: Large Tridiagonal Toeplitz Matrix**\n- Parameters: $n=50$, $d=1.0$, $c=0.45$.\n- The setup is identical to Case $2$, but with $n=50$. $A = dI - cT$ and $B_J = cT$.\n- The absolute row sums are $\\{c, 2c, \\dots, 2c, c\\}$. With $c=0.45$, the sums are $\\{0.45, 0.9, \\dots, 0.9, 0.45\\}$. The maximum is $\\lVert B_J \\rVert_\\infty = 2c = 0.9$.\n- The eigenvalues of $B_J$ are $\\lambda_k = 2c\\cos(\\frac{k\\pi}{n+1})$ for $k=1, \\dots, n$.\n- We have $n=50$, so the eigenvalues are $0.9\\cos(\\frac{k\\pi}{51})$ for $k=1, \\dots, 50$.\n- The spectral radius is $\\rho(B_J) = \\max_{k} |0.9\\cos(\\frac{k\\pi}{51})| = 0.9\\cos(\\frac{\\pi}{51})$.\n- As $n$ becomes large, $\\frac{\\pi}{n+1} \\to 0$ and $\\cos(\\frac{\\pi}{n+1}) \\to 1$. Thus, $\\rho(B_J)$ approaches $\\lVert B_J \\rVert_\\infty$.\n- $\\rho(B_J) = 0.9 \\times \\cos(\\pi/51) \\approx 0.9 \\times 0.998108 \\approx 0.898297$.\n- The gap is $0.9 - 0.898297... \\approx 0.001703$. The bound is near-tight but does not meet the strict criterion of $g \\le 10^{-10}$.\n\nThe implementation will follow these steps for each case, using numerical libraries to construct matrices and compute eigenvalues. The final results will be formatted as specified.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Performs a numerical experiment to compare the tightness of the\n    infinity-norm-based upper bound on the spectral radius of the Jacobi\n    iteration matrix for several test cases.\n    \"\"\"\n\n    test_cases_params = [\n        # Case 1: n=5, s=0.9\n        {'type': 'constant_off_diagonal', 'n': 5, 's': 0.9},\n        # Case 2: n=3, d=1.0, c=0.45\n        {'type': 'tridiagonal', 'n': 3, 'd': 1.0, 'c': 0.45},\n        # Case 3: n=6, t=0.9\n        {'type': 'nilpotent_b_j', 'n': 6, 't': 0.9},\n        # Case 4: n=50, d=1.0, c=0.45\n        {'type': 'tridiagonal', 'n': 50, 'd': 1.0, 'c': 0.45},\n    ]\n\n    all_results = []\n\n    for params in test_cases_params:\n        # Construct matrix A based on the case type and parameters\n        n = params['n']\n        if params['type'] == 'constant_off_diagonal':\n            s = params['s']\n            A = np.full((n, n), -s / (n - 1))\n            np.fill_diagonal(A, 1.0)\n        elif params['type'] == 'tridiagonal':\n            d = params['d']\n            c = params['c']\n            T = np.diag(np.ones(n - 1), k=1) + np.diag(np.ones(n - 1), k=-1)\n            A = d * np.eye(n) - c * T\n        elif params['type'] == 'nilpotent_b_j':\n            t = params['t']\n            U = np.diag(np.full(n - 1, t), k=1)\n            A = np.eye(n) - U\n        else:\n            raise ValueError(\"Unknown test case type\")\n\n        # Construct the Jacobi iteration matrix B_J\n        # B_J = I - D^{-1}A\n        diag_A = np.diag(A)\n        if np.any(diag_A == 0):\n            # This case should not be reached with the given problems.\n            raise ValueError(\"Matrix A has zero on the diagonal, D is not invertible.\")\n        \n        D_inv = np.diag(1.0 / diag_A)\n        B_J = np.eye(n) - D_inv @ A\n\n        # 1. Compute the infinity norm of B_J (the bound)\n        bound = np.linalg.norm(B_J, ord=np.inf)\n\n        # 2. Compute the spectral radius of B_J\n        eigenvalues = np.linalg.eigvals(B_J)\n        spectral_radius = np.max(np.abs(eigenvalues))\n\n        # 3. Compute the gap\n        gap = bound - spectral_radius\n\n        # 4. Determine if the bound is numerically tight\n        is_tight = gap = 1e-10\n\n        all_results.append([bound, spectral_radius, gap, is_tight])\n\n    # Format the final output string as specified\n    formatted_cases = []\n    for res in all_results:\n        b, r, g, t = res\n        # Format floats to 6 decimal places and boolean to string\n        case_str = f\"[{b:.6f},{r:.6f},{g:.6f},{str(t)}]\"\n        formatted_cases.append(case_str)\n    \n    final_output = f\"[{','.join(formatted_cases)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "2384241"}, {"introduction": "Strict diagonal dominance provides a straightforward way to guarantee the convergence of iterative methods. This exercise presents a practical optimization problem: if a matrix is not strictly diagonally dominant, how can we modify it with the smallest possible diagonal perturbation to enforce this desirable property? By solving this, you will gain hands-on experience in algorithmically enforcing convergence conditions and observing their effect on the spectral radius.", "problem": "You are given a real square matrix $A \\in \\mathbb{R}^{n \\times n}$ that is not strictly diagonally dominant by rows. For a diagonal matrix $\\Delta = \\mathrm{diag}(\\delta_1,\\dots,\\delta_n)$, define the Frobenius norm by $\\|\\Delta\\|_{F} = \\sqrt{\\sum_{i=1}^{n} \\delta_i^2}$. For a given positive margin $\\tau \\in \\mathbb{R}_{0}$, a matrix $B$ is said to be $\\tau$-strictly diagonally dominant by rows if $\\left|b_{ii}\\right| \\ge \\sum_{j \\ne i} \\left|b_{ij}\\right| + \\tau$ holds for every row index $i \\in \\{1,\\dots,n\\}$. The goal is to find the diagonal matrix $\\Delta$ of minimal Frobenius norm such that $A + \\Delta$ is $\\tau$-strictly diagonally dominant by rows.\n\nAdditionally, let $D = \\mathrm{diag}(b_{11},\\dots,b_{nn})$ be the diagonal of $B = A+\\Delta$, and let $I$ denote the identity matrix of size $n$. Define the Jacobi iteration matrix $T_J$ associated with $B$ by $T_J = I - D^{-1} B$. The spectral radius (SR) of a square matrix $M$, denoted $\\rho(M)$, is the maximum of the absolute values of the eigenvalues of $M$.\n\nYour program must, for each test matrix below, with the same prescribed margin $\\tau$, compute:\n1. The minimal Frobenius norm $\\|\\Delta\\|_{F}$ among all diagonal matrices $\\Delta$ for which $A+\\Delta$ is $\\tau$-strictly diagonally dominant by rows. In the event of multiple minimizers (which can only occur when $a_{ii} = 0$ for some $i$), choose the minimizer satisfying $a_{ii} + \\delta_i \\ge 0$ for that index $i$.\n2. The spectral radius $\\rho(T_J)$ of the Jacobi iteration matrix built from $B = A+\\Delta$.\n\nAll matrices are real-valued and dimensionless. Use the following test suite with a single common margin $\\tau = 10^{-3}$:\n- Test $1$: $A_1 = \\begin{bmatrix} 1  -2 \\\\ -2  1 \\end{bmatrix}$.\n- Test $2$: $A_2 = \\begin{bmatrix} 4  1  0 \\\\ 0.5  3.5  -0.2 \\\\ 0.1  0.1  1.5 \\end{bmatrix}$.\n- Test $3$: $A_3 = \\begin{bmatrix} 0  0.2 \\\\ -0.3  0 \\end{bmatrix}$.\n- Test $4$: $A_4 = \\begin{bmatrix} -1  0.4  0.6 \\\\ 0.3  -0.9  0.6 \\\\ 0.2  0.5  -0.7 \\end{bmatrix}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets of the form\n$[ \\|\\Delta_1\\|_{F}, \\rho(T_{J,1}), \\|\\Delta_2\\|_{F}, \\rho(T_{J,2}), \\|\\Delta_3\\|_{F}, \\rho(T_{J,3}), \\|\\Delta_4\\|_{F}, \\rho(T_{J,4}) ]$,\nwhere $\\Delta_k$ and $T_{J,k}$ correspond to test $k \\in \\{1,2,3,4\\}$. Each entry must be a real number written in decimal or scientific notation. No angles or physical units are involved, and no rounding to a specific number of digits is required.", "solution": "The problem as stated is subjected to validation and is determined to be valid. It is scientifically grounded in the principles of numerical linear algebra, well-posed, objective, and contains all necessary information for a unique solution. We therefore proceed with a formal derivation.\n\nThe central task is to find a diagonal matrix $\\Delta = \\mathrm{diag}(\\delta_1, \\dots, \\delta_n)$ that minimizes the Frobenius norm $\\|\\Delta\\|_{F}$ subject to the constraint that $B = A + \\Delta$ is $\\tau$-strictly diagonally dominant by rows. The optimization problem is:\n$$\n\\underset{\\delta_1, \\dots, \\delta_n}{\\text{minimize}} \\quad \\|\\Delta\\|_{F}^2 = \\sum_{i=1}^{n} \\delta_i^2\n$$\n$$\n\\text{subject to} \\quad |b_{ii}| \\ge \\sum_{j \\ne i} |b_{ij}| + \\tau \\quad \\text{for all } i \\in \\{1, \\dots, n\\}\n$$\nThe entries of $B$ are related to those of $A$ and $\\Delta$ by $b_{ij} = a_{ij}$ for $i \\ne j$ and $b_{ii} = a_{ii} + \\delta_i$. Let us define the sum of the magnitudes of the off-diagonal elements of $A$ for each row $i$ as $S_i = \\sum_{j \\ne i} |a_{ij}|$. The constraints can then be rewritten as:\n$$\n|a_{ii} + \\delta_i| \\ge S_i + \\tau \\quad \\text{for all } i \\in \\{1, \\dots, n\\}\n$$\nThe objective function to be minimized, $\\sum_{i=1}^{n} \\delta_i^2$, is a sum of non-negative terms, each depending on a single variable $\\delta_i$. The constraints are also decoupled, with each constraint involving only one $\\delta_i$. Consequently, we can minimize the total sum by independently minimizing each term $\\delta_i^2$ for each row $i$. This is equivalent to minimizing $|\\delta_i|$.\n\nFor each row $i$, we must find the value of $\\delta_i$ with the smallest magnitude that satisfies the constraint $|a_{ii} + \\delta_i| \\ge C_i$, where we define the constant $C_i = S_i + \\tau$. Since $\\tau  0$ and $S_i \\ge 0$, we have $C_i  0$. The inequality $|x| \\ge C_i$ defines the valid region for $x=a_{ii}+\\delta_i$ as $(-\\infty, -C_i] \\cup [C_i, \\infty)$. We seek to find a point in this valid region that is closest to $a_{ii}$ in order to minimize the perturbation $|\\delta_i| = |x - a_{ii}|$.\n\nWe analyze the subproblem for each row $i$:\n1.  If the original matrix $A$ already satisfies the condition for row $i$, i.e., $|a_{ii}| \\ge C_i$, then the constraint is met with $\\delta_i = 0$. Since we wish to minimize $|\\delta_i|$, the optimal choice is indeed $\\delta_i = 0$.\n2.  If $|a_{ii}|  C_i$, the condition is not met, and a non-zero $\\delta_i$ is required. We must perturb $a_{ii}$ to become $b_{ii} = a_{ii} + \\delta_i$ such that $|b_{ii}| \\ge C_i$. To minimize $|\\delta_i| = |b_{ii} - a_{ii}|$, we must choose $b_{ii}$ to be the point in the valid set $(-\\infty, -C_i] \\cup [C_i, \\infty)$ that is closest to the value $a_{ii}$.\n    -   If $a_{ii} \\ge 0$, the closest valid point is $b_{ii} = C_i$. This gives $\\delta_i = C_i - a_{ii}$.\n    -   If $a_{ii}  0$, the closest valid point is $b_{ii} = -C_i$. This gives $\\delta_i = -C_i - a_{ii}$.\n    -   A special case arises if $a_{ii} = 0$. The two candidate values are $\\delta_i = C_i$ and $\\delta_i = -C_i$, both having the same magnitude $|C_i|$. The problem specifies that in this event, we must choose the minimizer such that $a_{ii} + \\delta_i \\ge 0$, which means $b_{ii} \\ge 0$. We must therefore choose $b_{ii} = C_i$, which implies $\\delta_i = C_i$. This case is correctly handled by the $a_{ii} \\ge 0$ logic.\n\nThis procedure uniquely determines the optimal value of $\\delta_i$ for each row $i$. The diagonal matrix $\\Delta = \\mathrm{diag}(\\delta_1, \\dots, \\delta_n)$ is thus uniquely determined. The first required value, the minimal Frobenius norm, is then calculated as $\\|\\Delta\\|_{F} = \\sqrt{\\sum_{i=1}^{n} \\delta_i^2}$.\n\nThe second required value is the spectral radius $\\rho(T_J)$ of the Jacobi iteration matrix $T_J$ associated with the newly formed matrix $B = A + \\Delta$. The matrix $B$ is, by construction, $\\tau$-strictly diagonally dominant. Its diagonal entries $b_{ii}$ are guaranteed to be non-zero since $|b_{ii}| \\ge C_i = S_i + \\tau  0$. Therefore, its diagonal part, $D = \\mathrm{diag}(b_{11}, \\dots, b_{nn})$, is invertible.\n\nThe Jacobi matrix is defined as $T_J = I - D^{-1}B$. Its elements are given by:\n$$\n(T_J)_{ij} = \\begin{cases}\n0  \\text{if } i = j \\\\\n-\\frac{b_{ij}}{b_{ii}} = -\\frac{a_{ij}}{b_{ii}}  \\text{if } i \\ne j\n\\end{cases}\n$$\nThe spectral radius $\\rho(T_J)$ is the maximum absolute value among the eigenvalues of $T_J$:\n$$\n\\rho(T_J) = \\max \\{|\\lambda| : \\lambda \\text{ is an eigenvalue of } T_J \\}\n$$\nThe eigenvalues are found by numerically solving the characteristic equation $\\det(T_J - \\lambda I) = 0$.\n\nThe complete algorithm for each test case $(A, \\tau)$ is as follows:\n1.  Initialize an $n$-dimensional vector of zeros for the diagonal entries of $\\Delta$, denoted $(\\delta_1, \\dots, \\delta_n)$.\n2.  For each row $i$ from $1$ to $n$:\n    a. Compute the off-diagonal sum $S_i = \\sum_{j \\ne i} |a_{ij}|$.\n    b. Compute the target magnitude $C_i = S_i + \\tau$.\n    c. If $|a_{ii}|  C_i$, update $\\delta_i$: if $a_{ii} \\ge 0$, set $\\delta_i = C_i - a_{ii}$; otherwise, set $\\delta_i = -C_i - a_{ii}$.\n3.  Calculate the Frobenius norm $\\|\\Delta\\|_{F} = \\sqrt{\\sum_{i=1}^{n} \\delta_i^2}$.\n4.  Construct the matrix $B = A + \\mathrm{diag}(\\delta_1, \\dots, \\delta_n)$.\n5.  Construct the Jacobi matrix $T_J$ with entries $(T_J)_{ij} = -a_{ij}/b_{ii}$ for $i \\ne j$ and zeros on the diagonal.\n6.  Compute the eigenvalues of $T_J$.\n7.  Determine the spectral radius $\\rho(T_J)$ by finding the maximum absolute value of the computed eigenvalues.\n8.  Return $\\|\\Delta\\|_{F}$ and $\\rho(T_J)$.\nThis procedure is implemented for each provided test matrix.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for all test cases and prints the formatted output.\n    \"\"\"\n\n    def process_matrix(A, tau):\n        \"\"\"\n        Calculates the minimal Frobenius norm of Delta and the spectral radius\n        of the Jacobi matrix for a given matrix A and margin tau.\n\n        Args:\n            A (np.ndarray): The input square matrix.\n            tau (float): The positive margin for strict diagonal dominance.\n        \n        Returns:\n            tuple[float, float]: A tuple containing the Frobenius norm of Delta\n                                 and the spectral radius of the Jacobi matrix.\n        \"\"\"\n        n = A.shape[0]\n        delta_diag = np.zeros(n, dtype=float)\n\n        # 1. Determine the optimal diagonal matrix Delta\n        for i in range(n):\n            a_ii = A[i, i]\n            # Sum of absolute values of off-diagonal elements\n            S_i = np.sum(np.abs(A[i, :])) - np.abs(a_ii)\n            C_i = S_i + tau\n\n            if np.abs(a_ii)  C_i:\n                # Condition for modification is met\n                if a_ii = 0:\n                    # This case handles a_ii  0 and a_ii = 0 as per problem specification.\n                    delta_diag[i] = C_i - a_ii\n                else: # a_ii  0\n                    delta_diag[i] = -C_i - a_ii\n            # else, delta_diag[i] remains 0, which is the optimal choice.\n\n        # 2. Compute the Frobenius norm of Delta\n        norm_f_delta = np.linalg.norm(delta_diag)\n\n        # 3. Construct the modified matrix B = A + Delta\n        B = A + np.diag(delta_diag)\n\n        # 4. Construct the Jacobi iteration matrix T_J = I - D^-1 * B\n        b_diag = np.diag(B)\n        \n        # T_J has zeros on the diagonal\n        T_J = np.zeros_like(A, dtype=float)\n        for i in range(n):\n            for j in range(n):\n                if i != j:\n                    T_J[i, j] = -A[i, j] / b_diag[i]\n\n        # 5. Compute the spectral radius of T_J\n        eigenvalues = np.linalg.eigvals(T_J)\n        rho_T_J = np.max(np.abs(eigenvalues))\n\n        return norm_f_delta, rho_T_J\n\n    # Define the test cases from the problem statement.\n    tau = 1e-3\n    test_cases = [\n        np.array([[1, -2], [-2, 1]]),\n        np.array([[4, 1, 0], [0.5, 3.5, -0.2], [0.1, 0.1, 1.5]]),\n        np.array([[0, 0.2], [-0.3, 0]]),\n        np.array([[-1, 0.4, 0.6], [0.3, -0.9, 0.6], [0.2, 0.5, -0.7]])\n    ]\n\n    results = []\n    for A in test_cases:\n        norm_D, rho_Tj = process_matrix(A.astype(float), tau)\n        results.extend([norm_D, rho_Tj])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2384230"}, {"introduction": "Often, a linear system that is not diagonally dominant can be made so simply by reordering its equations, a technique analogous to pivoting. This advanced practice challenges you to implement and compare a standard Gauss-Seidel solver with one that dynamically reorders rows to improve diagonal dominance. This exercise provides a direct look at how algorithmic strategies can dramatically accelerate convergence for practical engineering problems.", "problem": "You are given square linear systems of the form $A x = b$ with $A \\in \\mathbb{R}^{n \\times n}$ and $b \\in \\mathbb{R}^{n}$. Consider the properties of strict diagonal dominance and the spectral radius criterion for convergence of stationary iterations. A matrix $A$ is called strictly diagonally dominant (SDD) if for every row index $i \\in \\{1,\\dots,n\\}$,\n$$\n|a_{i i}| \\gt \\sum_{j \\ne i} |a_{i j}|.\n$$\nLet the Gauss–Seidel method be defined via the splitting $A = D - L - U$ with $D$ the diagonal of $A$, $L$ the strictly lower triangular part of $-A$, and $U$ the strictly upper triangular part of $-A$. Its iteration matrix is\n$$\nT_{\\mathrm{GS}} = (D - L)^{-1} U,\n$$\nand the spectral radius criterion requires\n$$\n\\rho(T_{\\mathrm{GS}}) \\lt 1\n$$\nfor convergence, where $\\rho(\\cdot)$ denotes the spectral radius. For a row permutation matrix $P \\in \\mathbb{R}^{n \\times n}$, the permuted system is $P A x = P b$.\n\nYour task is to implement two solvers for $A x = b$:\n\n- A baseline Gauss–Seidel solver that uses the natural row order of $A$.\n\n- A dynamic row-reordered Gauss–Seidel solver that, at the start of each sweep $k \\in \\{0,1,2,\\dots\\}$, selects a row permutation $P_k$ that maximizes the sum of absolute diagonal entries after permutation, namely\n$$\nP_k \\in \\arg \\max_{P \\in \\mathcal{P}} \\sum_{i=1}^{n} \\left| (P A)_{i i} \\right|,\n$$\nwhere $\\mathcal{P}$ is the set of all $n \\times n$ permutation matrices. The sweep is then performed on $P_k A x = P_k b$.\n\nFor each test case, do all of the following in your program:\n\n1. Initialize with the zero vector $x^{(0)} = 0$. Use the Euclidean norm for residuals. Terminate when $\\lVert A x^{(k)} - b \\rVert_2 \\le \\tau$ with tolerance $\\tau = 10^{-8}$, or when the number of sweeps reaches $N_{\\max} = 1000$. Report convergence as a boolean together with the number of sweeps taken. If any division by zero would occur due to a zero diagonal element in the active system of equations in the baseline solver, declare the baseline solver non-convergent and report the number of sweeps as $-1$.\n\n2. Compute the spectral radius $\\rho_{\\text{base}}$ of $T_{\\mathrm{GS}}$ for the baseline system (unpermuted) using\n$$\nT_{\\mathrm{GS}} = (D - L)^{-1} U.\n$$\nIf $(D - L)$ is singular or any division by zero would be required, set $\\rho_{\\text{base}} = -1.0$.\n\n3. Compute a single best permutation $P^\\star$ that maximizes $\\sum_{i=1}^{n} |(P A)_{i i}|$ (the same optimization used in the dynamic method). Using $P^\\star A$, compute\n$$\n\\rho_{\\text{best}} = \\rho\\!\\left( (D^\\star - L^\\star)^{-1} U^\\star \\right),\n$$\nwhere $D^\\star$, $L^\\star$, and $U^\\star$ are defined from $P^\\star A$ analogously to $D$, $L$, and $U$. If $(D^\\star - L^\\star)$ is singular, set $\\rho_{\\text{best}} = -1.0$.\n\n4. Compute the minimum diagonal dominance ratio before and after the best permutation:\n$$\n\\delta_{\\min}(A) = \\min_{1 \\le i \\le n} \\frac{|a_{i i}|}{\\sum_{j \\ne i} |a_{i j}|}, \\quad\n\\delta_{\\min}(P^\\star A) = \\min_{1 \\le i \\le n} \\frac{|(P^\\star A)_{i i}|}{\\sum_{j \\ne i} |(P^\\star A)_{i j}|}.\n$$\nUse the convention that if the denominator is $0$ and the numerator is nonzero, the ratio is $+\\infty$, and if the numerator is $0$ and the denominator is positive, the ratio is $0$. If both numerator and denominator are $0$, treat the ratio as $+\\infty$.\n\nProvide results for the following test suite of systems $(A, b)$:\n\n- Test case $1$ ($n=3$): \n$$\nA_1 = \\begin{bmatrix}\n4  -1  0 \\\\\n-1  4  -1 \\\\\n0  -1  3\n\\end{bmatrix}, \\quad\nb_1 = \\begin{bmatrix}\n15 \\\\ 10 \\\\ 10\n\\end{bmatrix}.\n$$\n\n- Test case $2$ ($n=3$): \n$$\nA_2 = \\begin{bmatrix}\n0  2  1 \\\\\n3  0  1 \\\\\n1  1  4\n\\end{bmatrix}, \\quad\nb_2 = \\begin{bmatrix}\n3 \\\\ 4 \\\\ 5\n\\end{bmatrix}.\n$$\n\n- Test case $3$ ($n=3$):\n$$\nA_3 = \\begin{bmatrix}\n2  -1  1 \\\\\n-1  2  -1 \\\\\n1  -1  2\n\\end{bmatrix}, \\quad\nb_3 = \\begin{bmatrix}\n1 \\\\ 0 \\\\ 1\n\\end{bmatrix}.\n$$\n\n- Test case $4$ ($n=4$):\n$$\nA_4 = \\begin{bmatrix}\n1  3  0  0 \\\\\n2  0  2  0 \\\\\n0  2  0  3 \\\\\n0  0  4  0\n\\end{bmatrix}, \\quad\nb_4 = \\begin{bmatrix}\n1 \\\\ 2 \\\\ 3 \\\\ 4\n\\end{bmatrix}.\n$$\n\nFinal output format: Your program should produce a single line of output containing a list of results, one per test case, where each per-test-case result is itself a list with the following eight entries in order:\n$[$ $\\text{iters\\_base}$, $\\text{iters\\_dyn}$, $\\text{conv\\_base}$, $\\text{conv\\_dyn}$, $\\rho_{\\text{base}}$, $\\rho_{\\text{best}}$, $\\delta_{\\min}(A)$, $\\delta_{\\min}(P^\\star A)$ $]$.\n\nHere $\\text{iters\\_base}$ and $\\text{iters\\_dyn}$ are integers (use $-1$ if not converged within $N_{\\max}$), $\\text{conv\\_base}$ and $\\text{conv\\_dyn}$ are booleans, and the remaining quantities are floating-point numbers. Your program should print exactly one line in the following format:\n$$\n\\big[ \\text{case1\\_list}, \\text{case2\\_list}, \\text{case3\\_list}, \\text{case4\\_list} \\big],\n$$\nthat is, a single Python-style list literal with comma-separated inner lists in the same order as the test cases above.", "solution": "The problem presented is a well-posed and scientifically grounded exercise in computational engineering, specifically in the field of numerical linear algebra. It requires the implementation and comparison of two variants of the Gauss-Seidel iterative method for solving a system of linear equations $A x = b$. The problem is valid as it is based on established mathematical principles, provides all necessary data and constraints, and requests an objective, verifiable outcome.\n\nThe core task revolves around stationary iterative methods. For a linear system $A x = b$, we decompose the matrix $A$ into a splitting $A = M - N$, where $M$ is an easily invertible matrix. The iteration is then defined by $M x^{(k+1)} = N x^{(k)} + b$. For convergence, the spectral radius of the iteration matrix $T = M^{-1} N$ must be less than one, i.e., $\\rho(T)  1$.\n\nThe Gauss-Seidel method is a specific instance of this framework. The matrix $A$ is split into its diagonal $D$, strictly lower triangular part $-L$, and strictly upper triangular part $-U$, such that $A = D - L - U$. The method is defined by the choice $M = D - L$ and $N = U$. The iteration is thus:\n$$\n(D - L) x^{(k+1)} = U x^{(k)} + b.\n$$\nThe iteration matrix is $T_{\\mathrm{GS}} = (D - L)^{-1} U$. A sufficient, but not necessary, condition for the convergence of the Gauss-Seidel method is that the matrix $A$ is strictly diagonally dominant (SDD). A matrix $A$ is SDD if, for each row $i$, the magnitude of the diagonal element is greater than the sum of the magnitudes of all other elements in that row:\n$$\n|a_{ii}|  \\sum_{j \\neq i} |a_{ij}|.\n$$\nThis condition ensures that $\\rho(T_{\\mathrm{GS}})  1$.\n\nThe problem requires implementation of two solvers:\n\n1.  **Baseline Gauss-Seidel Solver**: This solver applies the method directly to the given system $A x = b$. Its convergence is not guaranteed if $A$ is not SDD or does not possess other favorable properties (like being symmetric positive-definite). If any diagonal element $a_{ii}$ is zero, the term $1/a_{ii}$ in the component-wise update formula is undefined, and the method fails. The update for each component $x_i$ during a sweep is:\n    $$\n    x_i^{(k+1)} = \\frac{1}{a_{ii}} \\left( b_i - \\sum_{ji} a_{ij} x_j^{(k+1)} - \\sum_{ji} a_{ij} x_j^{(k)} \\right).\n    $$\n\n2.  **Dynamic Row-Reordered Gauss-Seidel Solver**: This solver attempts to improve convergence by reordering the equations at the beginning of each sweep $k$. A permutation matrix $P_k$ is chosen to solve the optimization problem:\n    $$\n    P_k \\in \\arg \\max_{P \\in \\mathcal{P}} \\sum_{i=1}^{n} |(P A)_{ii}|,\n    $$\n    where $\\mathcal{P}$ is the set of all $n \\times n$ permutation matrices. This problem can be recognized as the assignment problem (or maximum weight bipartite matching). The goal is to reorder the rows of $A$ to place the elements with the largest possible absolute values onto the main diagonal. Let the permutation of rows be described by a vector $p$, where the $i$-th row of the new matrix is the $p_i$-th row of the old matrix. We want to find a permutation $p$ that maximizes $\\sum_i |A_{p_i, i}|$. This is equivalent to finding an assignment of original rows to new row positions to maximize the sum of absolute values of the resulting diagonal entries. This can be solved efficiently using algorithms like the Hungarian algorithm, which is available in `scipy.optimize.linear_sum_assignment`. The cost matrix $C$ for this function, which finds a minimum cost assignment, must be defined as $C_{ij} = -|A_{j, i}|$, representing the cost of assigning original row $j$ to the $i$-th diagonal position.\n\n    The Gauss-Seidel sweep is then performed on the permuted system $P_k A x = P_k b$. Since the matrix $A$ is constant throughout the process, the optimal permutation $P_k$ will be the same for every sweep, i.e., $P_k = P^\\star$ for all $k$. The dynamic algorithm thus effectively solves the system $P^\\star A x = P^\\star b$ using the standard Gauss-Seidel method. The convergence check, however, must be performed on the original, unpermuted system, i.e., by evaluating the residual norm $\\lVert A x^{(k)} - b \\rVert_2$.\n\nIn addition to implementing the solvers, we must compute several diagnostic quantities:\n-   **Spectral Radii $\\rho_{\\text{base}}$ and $\\rho_{\\text{best}}$**: These are the spectral radii of the Gauss-Seidel iteration matrices for the original system $A$ and the one-time optimally permuted system $P^\\star A$, respectively. They are computed as $\\rho(T) = \\max_i |\\lambda_i(T)|$, where $T = (D-L)^{-1}U$ and $\\lambda_i(T)$ are the eigenvalues of $T$. If the matrix $(D-L)$ is singular (which occurs if any diagonal element of the system matrix is zero), the spectral radius is considered undefined and reported as $-1.0$.\n-   **Minimum Diagonal Dominance Ratios $\\delta_{\\min}$**: This metric quantifies how \"close\" a matrix is to being strictly diagonally dominant. For a matrix $M$, it is defined as:\n    $$\n    \\delta_{\\min}(M) = \\min_{1 \\le i \\le n} \\frac{|m_{ii}|}{\\sum_{j \\ne i} |m_{ij}|}.\n    $$\n    A value $\\delta_{\\min}(M)  1$ implies $M$ is SDD. The specified rules for division by zero ($c/0 = \\infty$ for any $c$, including $c=0$) are handled in the implementation.\n\nThe solution proceeds by first implementing helper functions for these computations: finding the optimal permutation, calculating the spectral radius, and calculating the minimum diagonal dominance ratio. Then, the two Gauss-Seidel solvers are implemented according to the specified termination criteria ($\\lVert A x^{(k)} - b \\rVert_2 \\le 10^{-8}$ or $N_{\\max} = 1000$ sweeps). Finally, a main routine processes each test case, invokes the solvers and metric calculations, and formats the results as required.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import linear_sum_assignment\nfrom scipy.linalg import solve_triangular\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and generate the final output.\n    \"\"\"\n    TOL = 1e-8\n    N_MAX = 1000\n\n    def find_best_permutation(A):\n        \"\"\"Finds the row permutation that maximizes the sum of absolute diagonal entries.\"\"\"\n        n = A.shape[0]\n        # Cost matrix where C[i, j] is the cost of assigning original row j to new row i.\n        # We want to maximize sum(|A_p(i),i|), which is equivalent to minimizing sum(-|A_p(i),i|).\n        # The cost of placing original row j at the new row position i (contributing to new diagonal A'_{ii})\n        # is -|A_{j, i}|.\n        cost_matrix = -np.abs(A.T)\n        row_ind, col_ind = linear_sum_assignment(cost_matrix)\n        # col_ind[i] is the original row index that should be moved to new row i.\n        return col_ind\n\n    def calculate_spectral_radius(A):\n        \"\"\"Computes the spectral radius of the Gauss-Seidel iteration matrix for A.\"\"\"\n        n = A.shape[0]\n        # Check for singularity of D-L, which occurs if any diagonal element is zero.\n        if np.any(np.diag(A) == 0):\n            return -1.0\n        \n        D = np.diag(np.diag(A))\n        L = -np.tril(A, -1)\n        U = -np.triu(A, 1)\n        \n        try:\n            # Solve (D-L)T = U for T = T_GS using forward substitution\n            T_gs = solve_triangular(D - L, U, lower=True)\n            eigenvalues = np.linalg.eigvals(T_gs)\n            return np.max(np.abs(eigenvalues))\n        except np.linalg.LinAlgError:\n            # This case can happen if D-L is numerically singular\n            return -1.0\n\n    def calculate_delta_min(A):\n        \"\"\"Computes the minimum diagonal dominance ratio for A.\"\"\"\n        n = A.shape[0]\n        ratios = []\n        for i in range(n):\n            diag_val = np.abs(A[i, i])\n            off_diag_sum = np.sum(np.abs(A[i, :])) - diag_val\n            if off_diag_sum == 0:\n                ratios.append(np.inf)\n            else:\n                ratios.append(diag_val / off_diag_sum)\n        return np.min(ratios)\n\n    def baseline_gs_solver(A, b):\n        \"\"\"Baseline Gauss-Seidel solver.\"\"\"\n        n = A.shape[0]\n        x = np.zeros(n)\n        \n        if np.any(np.diag(A) == 0):\n            return -1, False\n\n        for k in range(N_MAX):\n            x_old = x.copy()\n            for i in range(n):\n                sigma = np.dot(A[i, :i], x[:i]) + np.dot(A[i, i+1:], x_old[i+1:])\n                x[i] = (b[i] - sigma) / A[i, i]\n            \n            if np.linalg.norm(A @ x - b) = TOL:\n                return k + 1, True\n        \n        return -1, False\n\n    def dynamic_gs_solver(A, b):\n        \"\"\"Dynamic row-reordered Gauss-Seidel solver.\"\"\"\n        n = A.shape[0]\n        x = np.zeros(n)\n\n        # The optimal permutation is constant since A is constant.\n        # However, the problem states to select it \"at the start of each sweep\".\n        for k in range(N_MAX):\n            perm = find_best_permutation(A)\n            A_k = A[perm, :]\n            b_k = b[perm]\n\n            if np.any(np.diag(A_k) == 0):\n                return -1, False\n            \n            x_old = x.copy()\n            for i in range(n):\n                sigma = np.dot(A_k[i, :i], x[:i]) + np.dot(A_k[i, i+1:], x_old[i+1:])\n                x[i] = (b_k[i] - sigma) / A_k[i, i]\n            \n            if np.linalg.norm(A @ x - b) = TOL:\n                return k + 1, True\n                \n        return -1, False\n\n    def process_case(A, b):\n        \"\"\"Process a single test case (A, b).\"\"\"\n        # Solvers\n        iters_base, conv_base = baseline_gs_solver(A, b)\n        iters_dyn, conv_dyn = dynamic_gs_solver(A, b)\n        \n        # Spectral radius for baseline system\n        rho_base = calculate_spectral_radius(A)\n        \n        # Best permutation and related metrics\n        p_star = find_best_permutation(A)\n        A_best = A[p_star, :]\n        rho_best = calculate_spectral_radius(A_best)\n        \n        # Diagonal dominance ratios\n        delta_min_A = calculate_delta_min(A)\n        delta_min_A_best = calculate_delta_min(A_best)\n        \n        return [\n            iters_base, iters_dyn,\n            conv_base, conv_dyn,\n            rho_base, rho_best,\n            delta_min_A, delta_min_A_best\n        ]\n\n    test_cases = [\n        (np.array([[4., -1., 0.], [-1., 4., -1.], [0., -1., 3.]]), \n         np.array([15., 10., 10.])),\n        (np.array([[0., 2., 1.], [3., 0., 1.], [1., 1., 4.]]), \n         np.array([3., 4., 5.])),\n        (np.array([[2., -1., 1.], [-1., 2., -1.], [1., -1., 2.]]), \n         np.array([1., 0., 1.])),\n        (np.array([[1., 3., 0., 0.], [2., 0., 2., 0.], [0., 2., 0., 3.], [0., 0., 4., 0.]]), \n         np.array([1., 2., 3., 4.]))\n    ]\n\n    results = [process_case(A, b) for A, b in test_cases]\n    \n    # Format the final output string as specified.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2384203"}]}