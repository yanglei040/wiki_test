{"hands_on_practices": [{"introduction": "Richardson extrapolation is a powerful, general-purpose \"meta-algorithm\" for improving the accuracy of numerical methods. This first practice provides a hands-on guide to a foundational application: taking the simple, first-order Forward Euler method for solving ordinary differential equations and, with a single step of extrapolation, creating a more accurate second-order method [@problem_id:2433093]. This exercise demonstrates how to get more accuracy for your computational cost without having to design a brand new, more complex algorithm from the ground up.", "problem": "Design and implement a composite numerical method that, given an initial value problem for an ordinary differential equation of the form $y^{\\prime}(t) = f(t,y)$ with initial condition $y(t_0) = y_0$, automatically applies one step of Richardson extrapolation to a base first-order one-step method to accelerate the convergence of the global error at a specified final time $T$. The base method must be the forward Euler method, which advances from $t_n$ to $t_{n+1}$ using a uniform time step $h = (T - t_0)/N$ for some integer $N \\geq 1$. The composite method must perform two runs of the base method with step sizes $h$ and $h/2$ (that is, with $N$ and $2N$ steps, respectively), and then combine the two approximations at the same final time $T$ to cancel the leading-order term in the global error without using any pre-specified higher-order formula. Your implementation must not call any built-in ordinary differential equation solvers.\n\nPrincipled requirements:\n- Start from the fundamental definition of the initial value problem $y^{\\prime}(t) = f(t,y)$ with $y(t_0) = y_0$, and the definition of a one-step method’s global error at time $T$ as the difference between the numerical approximation and the exact solution $y(T)$.\n- Assume only well-tested facts about the order of accuracy: the forward Euler method has a global error that scales linearly with the step size $h$ for sufficiently smooth $f$, i.e., the global error is $\\mathcal{O}(h)$ as $h \\to 0$.\n- Apply a single step of the Richardson idea: use two approximations computed with step sizes $h$ and $h/2$ and combine them so that the leading $\\mathcal{O}(h)$ term in the global error is canceled, resulting in an approximation whose error scales as $\\mathcal{O}(h^2)$ under the same smoothness assumptions. Do not assume or use any “shortcut” formula in the problem statement; instead, your program must implement the general step-halving and cancellation principle.\n\nAngle unit requirement:\n- Whenever trigonometric functions appear, interpret their arguments in radians.\n\nTest suite:\nImplement your composite method and evaluate it on the following four initial value problems. For each case, compute:\n- the base forward Euler approximation at $T$ using $N$ steps,\n- the base forward Euler approximation at $T$ using $2N$ steps,\n- the extrapolated approximation at $T$ constructed by canceling the leading $\\mathcal{O}(h)$ global error term from the two approximations,\n- the absolute global error of the base method with $N$ steps, denoted $E_{\\mathrm{base}} = \\lvert Y_h(T) - y(T) \\rvert$,\n- the absolute global error of the extrapolated approximation, denoted $E_{\\mathrm{extra}} = \\lvert Y_{\\mathrm{extra}}(T) - y(T) \\rvert$,\n- the improvement factor $\\rho = E_{\\mathrm{base}}/E_{\\mathrm{extra}}$.\n\nThe four test cases are:\n1. $f(t,y) = -y$, $t_0 = 0$, $y_0 = 1$, $T = 1$, $N = 10$. Exact solution $y(t) = e^{-t}$.\n2. $f(t,y) = \\sin(t)$, $t_0 = 0$, $y_0 = 0$, $T = 1$, $N = 5$. Exact solution $y(t) = 1 - \\cos(t)$.\n3. $f(t,y) = -15\\,y$, $t_0 = 0$, $y_0 = 1$, $T = 1$, $N = 10$. Exact solution $y(t) = e^{-15 t}$.\n4. $f(t,y) = -y$, $t_0 = 0$, $y_0 = 1$, $T = 1$, $N = 1$. Exact solution $y(t) = e^{-t}$.\n\nAll quantities involving trigonometric functions must use radians. No physical units are involved.\n\nFinal output format:\n- Your program should produce a single line of output containing the four improvement factors $\\rho$ for the test cases, in the same order as listed above, rounded to six decimal places, as a comma-separated list enclosed in square brackets, for example $[\\rho_1,\\rho_2,\\rho_3,\\rho_4]$.\n- Each $\\rho$ must be a floating-point number.\n\nYour submission must be a single, complete, runnable program that adheres to the execution environment described later, takes no input, and writes exactly one line in the format above.", "solution": "The problem is determined to be valid. It is a well-posed and scientifically sound exercise in numerical analysis. We will proceed with a principled derivation and implementation.\n\nThe objective is to construct a numerical method for the initial value problem defined by the ordinary differential equation $y^{\\prime}(t) = f(t,y)$ with the initial condition $y(t_0) = y_0$. The goal is to approximate the solution $y(T)$ at a final time $T$.\n\nThe specified base algorithm is the forward Euler method. This is a one-step method that generates a sequence of approximations $Y_n$ to the true solution $y(t_n)$ at discrete time points $t_n = t_0 + n \\cdot h$. The step size $h$ is uniform, given by $h = (T - t_0)/N$ for a specified number of steps $N$. The recurrence relation for the forward Euler method is derived from a first-order Taylor expansion of the solution around $t_n$:\n$y(t_{n+1}) = y(t_n) + h \\cdot y'(t_n) + \\mathcal{O}(h^2)$.\nBy substituting $y'(t_n) = f(t_n, y(t_n))$ and truncating the higher-order terms, we obtain the numerical scheme:\n$$Y_{n+1} = Y_n + h \\cdot f(t_n, Y_n)$$\nwith $Y_0 = y_0$. After $N$ steps, this procedure yields an approximation $Y_N$ to the true solution $y(T)$. Let us denote this approximation as $Y_h(T)$.\n\nIt is a known result from numerical analysis that for a sufficiently smooth function $f$, the global error of the forward Euler method has an asymptotic expansion in powers of the step size $h$. Let $Y(T)$ be the exact solution at time $T$. The numerical approximation $Y_h(T)$ can be expressed as:\n$$Y_h(T) = Y(T) + C_1 h + C_2 h^2 + C_3 h^3 + \\dots$$\nwhere the coefficients $C_k$ depend on the function $f$ and its derivatives but are independent of $h$. The leading term of the error is $C_1 h$, which establishes that the method has an order of accuracy of $1$, i.e., the global error is $\\mathcal{O}(h)$.\n\nThe task requires applying Richardson extrapolation to improve this accuracy. This is achieved by computing the solution with two different step sizes. We use the given step size $h$ (corresponding to $N$ steps) and a halved step size $h/2$ (corresponding to $2N$ steps). Let the respective approximations at time $T$ be $Y_h(T)$ and $Y_{h/2}(T)$. According to the error expansion, we have:\n$1$. $Y_h(T) = Y(T) + C_1 h + C_2 h^2 + \\mathcal{O}(h^3)$\n$2$. $Y_{h/2}(T) = Y(T) + C_1 (h/2) + C_2 (h/2)^2 + \\mathcal{O}(h^3) = Y(T) + \\frac{1}{2} C_1 h + \\frac{1}{4} C_2 h^2 + \\mathcal{O}(h^3)$\n\nThe objective is to find a linear combination of $Y_h(T)$ and $Y_{h/2}(T)$ that eliminates the leading error term, $C_1 h$. We seek an extrapolated approximation $Y_{\\mathrm{extra}}(T)$ of the form $\\alpha Y_{h/2}(T) + \\beta Y_h(T)$ that is a better approximation of $Y(T)$. For consistency, the approximation must be exact if the base method were exact, which implies $\\alpha + \\beta = 1$. To eliminate the $\\mathcal{O}(h)$ error term, the coefficients of $C_1 h$ in the combined error expansion must sum to zero:\n$\\alpha (\\frac{1}{2}) + \\beta (1) = 0$.\n\nWe have a system of two linear equations for the coefficients $\\alpha$ and $\\beta$:\n$$\n\\begin{cases}\n\\alpha + \\beta = 1 \\\\\n\\frac{1}{2}\\alpha + \\beta = 0\n\\end{cases}\n$$\nSubtracting the second equation from the first yields $\\frac{1}{2}\\alpha = 1$, which gives $\\alpha = 2$. Substituting this into the first equation gives $2 + \\beta = 1$, so $\\beta = -1$.\n\nThus, the extrapolated approximation is given by the formula:\n$$Y_{\\mathrm{extra}}(T) = 2 Y_{h/2}(T) - Y_h(T)$$\nLet us verify the error of this new approximation. The error is $Y_{\\mathrm{extra}}(T) - Y(T)$.\n$Y_{\\mathrm{extra}}(T) - Y(T) = (2 Y_{h/2}(T) - Y_h(T)) - Y(T)$\n$Y_{\\mathrm{extra}}(T) - Y(T) = 2(Y_{h/2}(T) - Y(T)) - (Y_h(T) - Y(T))$\nUsing the error expansions:\n$Y_{\\mathrm{extra}}(T) - Y(T) = 2 \\left( \\frac{1}{2} C_1 h + \\frac{1}{4} C_2 h^2 + \\mathcal{O}(h^3) \\right) - (C_1 h + C_2 h^2 + \\mathcal{O}(h^3))$\n$Y_{\\mathrm{extra}}(T) - Y(T) = (C_1 h + \\frac{1}{2} C_2 h^2 + \\mathcal{O}(h^3)) - (C_1 h + C_2 h^2 + \\mathcal{O}(h^3))$\n$Y_{\\mathrm{extra}}(T) - Y(T) = -\\frac{1}{2} C_2 h^2 + \\mathcal{O}(h^3)$\n\nThe leading error term is now of order $\\mathcal{O}(h^2)$, confirming that the extrapolated method has an order of accuracy of $2$.\n\nThe implementation will consist of a function that performs the forward Euler integration for a given number of steps. This function will be called twice: once with $N$ steps and once with $2N$ steps. The resulting approximations, $Y_h(T)$ and $Y_{h/2}(T)$, are then combined using the derived formula to compute $Y_{\\mathrm{extra}}(T)$. Finally, the absolute global errors $E_{\\mathrm{base}} = \\lvert Y_h(T) - y(T) \\rvert$ and $E_{\\mathrm{extra}} = \\lvert Y_{\\mathrm{extra}}(T) - y(T) \\rvert$ are calculated, and their ratio $\\rho = E_{\\mathrm{base}} / E_{\\mathrm{extra}}$ is determined for each test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of applying Richardson extrapolation to the forward Euler method\n    for several ODE test cases and computes the error improvement factor.\n    \"\"\"\n\n    def forward_euler(f, t0, y0, T, N):\n        \"\"\"\n        Solves y'(t) = f(t, y) using the forward Euler method.\n        \n        Args:\n            f: The function defining the ODE, f(t, y).\n            t0: Initial time.\n            y0: Initial value.\n            T: Final time.\n            N: Number of steps.\n\n        Returns:\n            The numerical approximation of y(T).\n        \"\"\"\n        if N == 0:\n            return y0\n        \n        h = (T - t0) / N\n        t = t0\n        y = y0\n        \n        for _ in range(N):\n            y = y + h * f(t, y)\n            t = t + h\n            \n        return y\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"f\": lambda t, y: -y,\n            \"t0\": 0.0,\n            \"y0\": 1.0,\n            \"T\": 1.0,\n            \"N\": 10,\n            \"y_exact\": lambda t: np.exp(-t)\n        },\n        {\n            \"f\": lambda t, y: np.sin(t),\n            \"t0\": 0.0,\n            \"y0\": 0.0,\n            \"T\": 1.0,\n            \"N\": 5,\n            \"y_exact\": lambda t: 1.0 - np.cos(t)\n        },\n        {\n            \"f\": lambda t, y: -15.0 * y,\n            \"t0\": 0.0,\n            \"y0\": 1.0,\n            \"T\": 1.0,\n            \"N\": 10,\n            \"y_exact\": lambda t: np.exp(-15.0 * t)\n        },\n        {\n            \"f\": lambda t, y: -y,\n            \"t0\": 0.0,\n            \"y0\": 1.0,\n            \"T\": 1.0,\n            \"N\": 1,\n            \"y_exact\": lambda t: np.exp(-t)\n        }\n    ]\n\n    rho_values = []\n\n    for case in test_cases:\n        f = case[\"f\"]\n        t0 = case[\"t0\"]\n        y0 = case[\"y0\"]\n        T = case[\"T\"]\n        N = case[\"N\"]\n        y_exact_func = case[\"y_exact\"]\n\n        # Compute the base approximation with N steps (step size h)\n        Y_h = forward_euler(f, t0, y0, T, N)\n        \n        # Compute the base approximation with 2N steps (step size h/2)\n        Y_h_div_2 = forward_euler(f, t0, y0, T, 2 * N)\n\n        # Compute the extrapolated approximation\n        Y_extra = 2.0 * Y_h_div_2 - Y_h\n\n        # Compute the exact value at the final time T\n        y_exact_at_T = y_exact_func(T)\n\n        # Compute the absolute global error of the base method\n        E_base = np.abs(Y_h - y_exact_at_T)\n        \n        # Compute the absolute global error of the extrapolated approximation\n        E_extra = np.abs(Y_extra - y_exact_at_T)\n\n        # Compute the improvement factor rho\n        if E_extra == 0.0:\n            # If extrapolated error is zero, improvement is infinite (assuming base error is not zero).\n            # This is unlikely with floating point arithmetic for these problems.\n            rho = float('inf') if E_base != 0.0 else 1.0\n        else:\n            rho = E_base / E_extra\n            \n        rho_values.append(rho)\n\n    # Final print statement in the exact required format.\n    formatted_results = [f\"{val:.6f}\" for val in rho_values]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2433093"}, {"introduction": "With the basics of a single extrapolation step covered, we can now appreciate the full power of applying this technique iteratively. This practice delves into a classic and beautiful application: accelerating Archimedes' method for estimating the value of $\\pi$ [@problem_id:2435033]. By implementing a multi-level extrapolation scheme, you will not only gain a more accurate estimate of $\\pi$ but also build an intuition for how powerful algorithms, like Romberg integration, are constructed from simpler components.", "problem": "Design and implement a program that computes accelerated estimates of the mathematical constant $\\pi$ starting from the Archimedean inscribed polygon perimeter sequence and canceling leading even-power error terms using data at successively doubled polygon side counts. Work from the geometric definition: for a unit circle of radius $1$ and a regular $n$-gon inscribed in the circle, the polygon side length is $2 \\sin\\!\\left(\\frac{\\pi}{n}\\right)$ (with angles in radians), so the polygon perimeter is $2 n \\sin\\!\\left(\\frac{\\pi}{n}\\right)$ and the corresponding estimate of $\\pi$ is\n$$\n\\pi_n \\equiv n \\sin\\!\\left(\\frac{\\pi}{n}\\right).\n$$\nYour program must take, for each test case, a base side count $n_0$ and a nonnegative integer depth $L$. Using the sequence $\\{\\pi_{n_0 \\cdot 2^k}\\}_{k=0}^{L}$, construct a triangular array of increasingly accurate estimates that at level $j$ has canceled the first $j$ even-power terms of the asymptotic error expansion in powers of $n^{-2}$. The final estimate to report for a test case is the entry at the bottom of the array at column $L$. For each test case, compute the absolute error of this final estimate with respect to $\\pi$ and return it as a floating-point number.\n\nAll trigonometric arguments must be interpreted in radians. All outputs must be expressed as absolute errors (unitless), rounded to exactly $12$ digits after the decimal point.\n\nUse the following test suite, each specified as a pair $(n_0, L)$:\n- Test $1$: $(6, 0)$.\n- Test $2$: $(6, 1)$.\n- Test $3$: $(6, 2)$.\n- Test $4$: $(12, 3)$.\n- Test $5$: $(1536, 0)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the same order as the test suite. For example, the output format must be exactly of the form\n$$\n[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4,\\text{result}_5],\n$$\nwhere each $\\text{result}_i$ is the absolute error for test $i$, rounded to $12$ digits after the decimal point.", "solution": "The problem posed is a valid and well-defined exercise in numerical analysis, specifically concerning the application of Richardson extrapolation to accelerate the convergence of a sequence. The task requires the computation of an approximation for the constant $\\pi$ based on the perimeters of regular polygons inscribed in a unit circle, a method pioneered by Archimedes. The subsequent steps involve a systematic cancellation of error terms.\n\nThe starting point is the sequence of approximations for $\\pi$, given by $\\pi_n$, derived from a regular $n$-sided polygon inscribed in a unit circle:\n$$\n\\pi_n = n \\sin\\left(\\frac{\\pi}{n}\\right)\n$$\nTo apply Richardson extrapolation, we must first understand the error structure of this approximation. Let $h = 1/n$. The approximation can be viewed as a function of the step size $h$, $A(h) = \\pi_{1/h}$. The error of this approximation, $A(h) - \\pi$, can be analyzed by expanding $\\sin(x)$ in a Taylor series around $x=0$:\n$$\n\\sin(x) = x - \\frac{x^3}{3!} + \\frac{x^5}{5!} - \\frac{x^7}{7!} + \\dots = \\sum_{k=0}^{\\infty} \\frac{(-1)^k}{(2k+1)!} x^{2k+1}\n$$\nSubstituting $x = \\pi/n$ into the formula for $\\pi_n$:\n$$\n\\pi_n = n \\left[ \\left(\\frac{\\pi}{n}\\right) - \\frac{1}{6}\\left(\\frac{\\pi}{n}\\right)^3 + \\frac{1}{120}\\left(\\frac{\\pi}{n}\\right)^5 - \\dots \\right]\n$$\n$$\n\\pi_n = \\pi - \\frac{\\pi^3}{6} \\frac{1}{n^2} + \\frac{\\pi^5}{120} \\frac{1}{n^4} - \\frac{\\pi^7}{5040} \\frac{1}{n^6} + \\dots\n$$\nThis expansion shows that the approximation $\\pi_n$ approaches the true value of $\\pi$ with an error that is a series in even powers of $1/n$. That is,\n$$\n\\pi_n = \\pi + c_1 \\left(\\frac{1}{n^2}\\right) + c_2 \\left(\\frac{1}{n^2}\\right)^2 + c_3 \\left(\\frac{1}{n^2}\\right)^3 + \\dots\n$$\nwhere $c_k$ are constants. This structure is precisely what is required for Richardson extrapolation. The problem specifies a sequence of approximations using polygon side counts that are successively doubled: $n_k = n_0 \\cdot 2^k$ for $k = 0, 1, 2, \\dots, L$. This corresponds to a step size $h_k = 1/n_k$ that is successively halved.\n\nLet $R_{k,0} = \\pi_{n_k}$ be the initial sequence of estimates. We aim to construct a triangular table of estimates $R_{i,j}$ where the second index $j$ indicates the level of extrapolation. An estimate $R_{i,j}$ has an error series that starts with a term of order $(1/n_i)^{2(j+1)}$.\n\nTo eliminate the leading error term, which is of order $n^{-2}$ or $h^2$, we combine two estimates, say $\\pi_n$ and $\\pi_{2n}$.\nLet $A(h) = \\pi_{1/h}$. We have:\n$$\nA(h) = \\pi + c_1 h^2 + c_2 h^4 + \\dots\n$$\n$$\nA(h/2) = \\pi + c_1 (h/2)^2 + c_2 (h/2)^4 + \\dots = \\pi + \\frac{c_1}{4} h^2 + \\frac{c_2}{16} h^4 + \\dots\n$$\nTo cancel the $c_1 h^2$ term, we compute a linear combination:\n$$\n4 A(h/2) - A(h) = (4\\pi + c_1 h^2 + \\dots) - (\\pi + c_1 h^2 + \\dots) = 3\\pi + O(h^4)\n$$\nA more accurate estimate is therefore $\\frac{4A(h/2) - A(h)}{3}$, with an error of order $h^4$.\n\nThis process can be generalized. Let $R_{i, j-1}$ be an estimate with leading error term of order $(1/n_i)^{2j}$. We can combine $R_{i, j-1}$ and $R_{i+1, j-1}$ (which uses a side count of $n_{i+1} = 2n_i$) to produce $R_{i,j}$, which has a leading error of order $(1/n_i)^{2(j+1)}$. The general formula for this extrapolation scheme is:\n$$\nR_{i,j} = \\frac{4^j R_{i+1, j-1} - R_{i, j-1}}{4^j - 1}\n$$\nfor $j \\ge 1$.\n\nThe algorithm consists of the following steps for each test case $(n_0, L)$:\n1.  Generate the initial sequence of $L+1$ estimates, $R_{k,0} = \\pi_{n_k}$ for $k = 0, \\dots, L$, where $n_k = n_0 \\cdot 2^k$. This forms the first column ($j=0$) of our extrapolation table.\n2.  Iteratively compute the subsequent columns of the table. For each column $j$ from $1$ to $L$:\n    - Compute each entry $R_{i,j}$ for $i = 0, \\dots, L-j$ using the formula above.\n3.  The most accurate estimate produced by this method is the single entry in the last column, $R_{0,L}$. This is the final extrapolated value for $\\pi$.\n4.  Calculate the absolute error of this final estimate with respect to the value of $\\pi$ provided by the `numpy` library.\n5.  Format the resulting error as a floating-point number rounded to exactly $12$ decimal places.\n\nAll calculations are performed using standard double-precision floating-point arithmetic.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Richardson extrapolation problem for approximating pi.\n    \"\"\"\n\n    def richardson_pi_accelerator(n0, L):\n        \"\"\"\n        Computes an accelerated estimate of pi using Richardson extrapolation.\n\n        Args:\n            n0 (int): The base number of sides for the inscribed polygon.\n            L (int): The extrapolation depth.\n\n        Returns:\n            float: The most refined estimate of pi.\n        \"\"\"\n        # The Richardson table R[i][j] will store the estimates.\n        # We need L+1 initial estimates, so the table size is (L+1)x(L+1).\n        R = np.zeros((L + 1, L + 1))\n\n        # Step 1: Compute the initial sequence of estimates (the first column of the table).\n        # R[k, 0] corresponds to the estimate from a polygon with n0 * 2^k sides.\n        for k in range(L + 1):\n            n_k = n0 * (2**k)\n            # The approximation of pi from an n-sided polygon is n*sin(pi/n).\n            R[k, 0] = n_k * np.sin(np.pi / n_k)\n        \n        # If L=0, the result is just the initial crude estimate.\n        if L == 0:\n            return R[0, 0]\n\n        # Step 2: Iteratively fill the rest of the Richardson table.\n        # j is the column index, representing the level of extrapolation.\n        for j in range(1, L + 1):\n            # i is the row index.\n            for i in range(L - j + 1):\n                # The error expansion is in powers of n^(-2), or h^2 where h=1/n.\n                # The step size reduction factor is 2 (since n doubles).\n                # The general formula is (r^(p*j) * A_more_accurate - A_less_accurate) / (r^(p*j) - 1).\n                # Here, r=2 and the error order is h^2 so p=2.\n                # However, the structure is in terms of h^2, so the effective order is 1 and the\n                # base is 4. The formula becomes (4^j * ... - ...) / (4^j - 1).\n                power_of_4 = 4.0**j\n                numerator = power_of_4 * R[i + 1, j - 1] - R[i, j - 1]\n                denominator = power_of_4 - 1.0\n                R[i, j] = numerator / denominator\n\n        # Step 3: The most accurate estimate is at the apex of the triangle, R[0, L].\n        return R[0, L]\n\n    test_cases = [\n        (6, 0),\n        (6, 1),\n        (6, 2),\n        (12, 3),\n        (1536, 0),\n    ]\n\n    results = []\n    pi_val = np.pi\n\n    for n0, L in test_cases:\n        pi_estimate = richardson_pi_accelerator(n0, L)\n        absolute_error = abs(pi_val - pi_estimate)\n        # Format the result string to exactly 12 decimal places.\n        results.append(f\"{absolute_error:.12f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2435033"}, {"introduction": "A true expert understands a tool's capabilities as well as its limitations. Richardson extrapolation is not a magic bullet; it relies on the crucial assumption that the error of the base method has a smooth, well-behaved asymptotic expansion. This final practice [@problem_id:2435348] presents a classic case where this assumption is violated—the numerical integration of a function with a \"kink\"—and prompts you to analyze precisely why extrapolation fails to improve accuracy. Mastering this concept will help you avoid subtle pitfalls and apply numerical methods more effectively in real-world scenarios.", "problem": "Consider applying Romberg integration (successive Richardson extrapolation built on the composite trapezoidal rule with step sizes $h, h/2, h/4, \\ldots$) to evaluate the definite integral $\\int_{-1}^{1} |x| \\, dx$. The composite trapezoidal rule on a uniform grid of $N$ subintervals has step size $h = \\frac{2}{N}$. Your task is to reason, starting from the differentiability properties of the integrand and the structure of the composite trapezoidal rule, about whether and why Romberg integration does or does not accelerate convergence for this integral.\n\nWhich statement best explains the behavior?\n\nA. The integrand $|x|$ is not differentiable at $x=0$, so the even-power error expansion assumed by Romberg does not hold. For the composite trapezoidal rule, the global error is parity-dependent: it is $E_N = \\frac{h^2}{4}$ when $N$ is odd (the kink lies strictly inside one panel) and $E_N = 0$ when $N$ is even (the kink coincides with a node and each panel sees a linear function). Because the leading error coefficient is not a fixed constant across refinements, the first Richardson extrapolation from $h$ to $h/2$ can produce a value far from the exact integral.\n\nB. Romberg integration fails because the integral is improper and diverges at $x=0$, so no quadrature rule can be meaningfully applied.\n\nC. Romberg integration fails because $|x|$ is an odd function over symmetric limits, and cancellation near $x=0$ invalidates the extrapolation process.\n\nD. Romberg integration would succeed if the initial step size $h$ were made sufficiently small; the cusp at $x=0$ effectively disappears for small enough $h$, restoring the smooth even-power error expansion that Romberg requires.\n\nE. Romberg integration fails because the composite trapezoidal rule is only first-order accurate globally, and an even-power Richardson extrapolation is therefore inapplicable regardless of smoothness considerations.", "solution": "The problem requires an analysis of the performance of Romberg integration for the definite integral $I = \\int_{-1}^{1} |x| \\, dx$. The core of the analysis rests on the theoretical foundation of Romberg integration and the properties of the integrand $f(x) = |x|$.\n\nFirst, we establish the exact value of the integral. The integrand $f(x) = |x|$ is an even function, so the integral over the symmetric interval $[-1, 1]$ can be calculated as:\n$$ I = \\int_{-1}^{1} |x| \\, dx = 2 \\int_{0}^{1} x \\, dx = 2 \\left[ \\frac{x^2}{2} \\right]_{0}^{1} = 2 \\left( \\frac{1^2}{2} - \\frac{0^2}{2} \\right) = 1 $$\nThe exact value of the integral is $1$.\n\nNext, we analyze the basis of Romberg integration. Romberg integration is a method that uses Richardson extrapolation to accelerate the convergence of the composite trapezoidal rule. The composite trapezoidal rule approximation $T(h)$ for an integral $I$, with step size $h$, has an asymptotic error expansion given by the Euler-Maclaurin formula:\n$$ T(h) = I + C_1 h^2 + C_2 h^4 + C_3 h^6 + \\dots $$\nThis expansion is valid provided the integrand $f(x)$ is sufficiently smooth (i.e., has a sufficient number of continuous derivatives) on the integration interval $[a, b]$. Specifically, for the expansion to contain only even powers of $h$, the integrand and all its odd-order derivatives must vanish at the endpoints $a$ and $b$, or the function must be periodic with period $b-a$. For a general non-periodic function, the expansion is $T(h) = I + C_1 h^2 + C_2 h^4 + \\dots + K_p h^{2p} + O(h^{2p+2})$, which requires the integrand to be at least in the class $C^{2p+2}[a, b]$.\n\nThe integrand in this problem is $f(x) = |x|$. This function is continuous everywhere. However, its first derivative is the signum function, $f'(x) = \\text{sgn}(x)$, which has a jump discontinuity at $x=0$.\n$$ f'(x) = \\begin{cases} -1 & x < 0 \\\\ 1 & x > 0 \\end{cases} $$\nSince $f'(x)$ is not continuous at $x=0$, the function $f(x)=|x|$ is not in the class $C^1[-1, 1]$. Consequently, it is also not in $C^2[-1, 1]$. The fundamental smoothness requirement for the standard Euler-Maclaurin error expansion is violated. The error of the composite trapezoidal rule will not have the form with only even powers of $h$, and therefore, the premise for Romberg integration is invalid.\n\nWe must now analyze the specific behavior of the composite trapezoidal rule for this particular integrand. The rule is applied on a uniform grid of $N$ subintervals over $[-1, 1]$, with step size $h = \\frac{2}{N}$ and nodes $x_k = -1 + k h$ for $k=0, 1, \\dots, N$. The behavior depends critically on whether the point of non-differentiability, $x=0$, is a grid node.\n\nCase 1: $N$ is an even number.\nLet $N=2m$ for some integer $m \\ge 1$. The step size is $h = \\frac{2}{2m} = \\frac{1}{m}$. The grid nodes are $x_k = -1 + \\frac{k}{m}$. The node corresponding to $k=m$ is $x_m = -1 + \\frac{m}{m} = 0$. Thus, when $N$ is even, the point of non-differentiability $x=0$ is always a grid node. The composite trapezoidal rule sums the results over each subinterval. In the subintervals within $[-1, 0]$, the integrand is $f(x)=-x$, a linear function. In the subintervals within $[0, 1]$, the integrand is $f(x)=x$, also a linear function. The trapezoidal rule is exact for linear functions (its error term involves the second derivative of the integrand, which is zero for a linear function). Therefore, the approximation is exact for every subinterval, and the total computed integral is exact. The error $E_N = T_N - I = 0$ when $N$ is even.\n\nCase 2: $N$ is an odd number.\nLet $N=2m+1$ for some integer $m \\ge 0$. The step size is $h = \\frac{2}{2m+1}$. The grid nodes are $x_k = -1 + k \\frac{2}{2m+1}$. For $x_k=0$, we would need $-1 + k \\frac{2}{2m+1} = 0$, which implies $k = \\frac{2m+1}{2}$. This is never an integer. Thus, when $N$ is odd, the point $x=0$ always falls strictly inside one of the subintervals. This subinterval is $[x_m, x_{m+1}]$, where $x_m = -1 + m h = \\frac{-(2m+1)+2m}{2m+1} = \\frac{-1}{2m+1}$ and $x_{m+1} = -1 + (m+1)h = \\frac{-(2m+1)+2m+2}{2m+1} = \\frac{1}{2m+1}$. Notice that this interval is symmetric around $0$ and has length $h$. The error for the composite trapezoidal rule comes exclusively from this single subinterval, as the integrand is linear on all other subintervals.\nThe trapezoidal approximation for this subinterval is:\n$$ T_{[x_m, x_{m+1}]} = \\frac{h}{2} (f(x_m) + f(x_{m+1})) = \\frac{h}{2} \\left(\\left|\\frac{-1}{2m+1}\\right| + \\left|\\frac{1}{2m+1}\\right|\\right) = \\frac{h}{2} \\left(\\frac{1}{2m+1} + \\frac{1}{2m+1}\\right) = \\frac{h}{2} \\left(\\frac{2}{2m+1}\\right) = \\frac{h^2}{2} $$\nThe exact integral over this subinterval is:\n$$ I_{[x_m, x_{m+1}]} = \\int_{-1/(2m+1)}^{1/(2m+1)} |x|\\, dx = 2 \\int_0^{1/(2m+1)} x\\, dx = \\left[x^2\\right]_0^{1/(2m+1)} = \\left(\\frac{1}{2m+1}\\right)^2 = \\left(\\frac{h}{2}\\right)^2 = \\frac{h^2}{4} $$\nThe error for this subinterval, which is the total error $E_N$, is $E_N = T_{[x_m, x_{m+1}]} - I_{[x_m, x_{m+1}]} = \\frac{h^2}{2} - \\frac{h^2}{4} = \\frac{h^2}{4}$. When $N$ is odd, $E_N = \\frac{h^2}{4} \\ne 0$.\n\nThe Richardson extrapolation formula $R_{k,1} = \\frac{4R_{k,0} - R_{k-1,0}}{3}$ assumes that the error $E(h) = C h^2 + O(h^4)$ with a constant $C$. Our analysis shows the error sequence is $E_N=h^2/4$ for odd $N$ and $E_N=0$ for even $N$. As the step size is halved ($h \\to h/2$), the number of intervals is doubled ($N \\to 2N$), which changes the parity of $N$ if $N$ is odd. This means the \"constant\" $C$ in the error term is not constant but alternates. This invalidates the extrapolation. For example, if we start with an odd $N_0$ (error $\\sim h_0^2/4$) and go to $N_1=2N_0$ (error is $0$), extrapolation will produce a poor result.\n\nNow, we evaluate each option.\n\nA. The integrand $|x|$ is not differentiable at $x=0$, so the even-power error expansion assumed by Romberg does not hold. For the composite trapezoidal rule, the global error is parity-dependent: it is $E_N = \\frac{h^2}{4}$ when $N$ is odd (the kink lies strictly inside one panel) and $E_N = 0$ when $N$ is even (the kink coincides with a node and each panel sees a linear function). Because the leading error coefficient is not a fixed constant across refinements, the first Richardson extrapolation from $h$ to $h/2$ can produce a value far from the exact integral.\nThis statement is a perfect summary of our derivation. Every claim made is correct: the non-differentiability, the failure of the error expansion, the parity-dependent error of the trapezoidal rule ($E_N=h^2/4$ for odd $N$, $E_N=0$ for even $N$), the non-constant error coefficient, and the resulting failure of extrapolation.\nVerdict: **Correct**.\n\nB. Romberg integration fails because the integral is improper and diverges at $x=0$, so no quadrature rule can be meaningfully applied.\nThe integral $\\int_{-1}^{1} |x| \\, dx$ is a proper integral. The integrand $|x|$ is continuous and bounded on the finite interval $[-1, 1]$. The integral converges to $1$. The statement is factually false.\nVerdict: **Incorrect**.\n\nC. Romberg integration fails because $|x|$ is an odd function over symmetric limits, and cancellation near $x=0$ invalidates the extrapolation process.\nThe function $f(x)=|x|$ is an even function, because $f(-x) = |-x| = |x| = f(x)$. The premise of the statement is false.\nVerdict: **Incorrect**.\n\nD. Romberg integration would succeed if the initial step size $h$ were made sufficiently small; the cusp at $x=0$ effectively disappears for small enough $h$, restoring the smooth even-power error expansion that Romberg requires.\nThe non-differentiability at $x=0$ is an intrinsic property of the function $|x|$. It does not disappear at any scale. Reducing the step size $h$ does not make the function smoother or restore the required error expansion. The parity-dependent error behavior persists regardless of the scale of $h$.\nVerdict: **Incorrect**.\n\nE. Romberg integration fails because the composite trapezoidal rule is only first-order accurate globally, and an even-power Richardson extrapolation is therefore inapplicable regardless of smoothness considerations.\nThe composite trapezoidal rule is second-order accurate ($O(h^2)$) for sufficiently smooth ($C^2$) functions, not first-order. The failure of Romberg integration in this case is precisely due to smoothness considerations, which this statement dismisses. The premise of the statement is incorrect.\nVerdict: **Incorrect**.", "answer": "$$\\boxed{A}$$", "id": "2435348"}]}