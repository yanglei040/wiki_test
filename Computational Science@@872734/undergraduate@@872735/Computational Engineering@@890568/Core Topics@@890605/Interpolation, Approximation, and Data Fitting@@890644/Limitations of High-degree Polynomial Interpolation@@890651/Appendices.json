{"hands_on_practices": [{"introduction": "The Runge phenomenon is the classic warning against naively using high-degree polynomial interpolation with equally spaced points. This hands-on coding practice moves beyond mere observation, challenging you to quantify this failure by finding the precise \"crossover\" degree where adding more nodes paradoxically makes the overall approximation worse. By computationally pinpointing this threshold, you will gain a concrete understanding of why and when this fundamental limitation of interpolation arises [@problem_id:2436036].", "problem": "Consider the Runge function $f(x)=\\dfrac{1}{1+a x^{2}}$ on the closed interval $[-1,1]$ for a given parameter $a0$. For each integer degree $n \\ge 0$, let $P_{n}(x)$ denote the unique polynomial of degree at most $n$ that interpolates $f(x)$ at the $n+1$ equispaced nodes $x_{k}=-1+\\dfrac{2k}{n}$ for $k=0,1,\\dots,n$. Define the integrated absolute interpolation error\n$$\nE(n)=\\int_{-1}^{1}\\left|f(x)-P_{n}(x)\\right|\\,dx.\n$$\nDefine the crossover degree $N_{\\text{crit}}$ to be the smallest integer $n$ in a prescribed search range such that adding one more equispaced interpolation node strictly increases the integrated error in the following sense:\n$$\nE(n+1)  \\left(1+\\tau\\right) E(n),\n$$\nwhere $\\tau$ is a fixed relative margin. If no such $n$ exists within the prescribed range, set $N_{\\text{crit}}=-1$.\n\nYour task is to compute $N_{\\text{crit}}$ for each of the test cases listed below. The integral defining $E(n)$ must be evaluated numerically to an absolute error no larger than $10^{-6}$ on $[-1,1]$.\n\nTest Suite (each item is a triple $(a,n_{\\min},n_{\\max})$ specifying the Runge parameter and the inclusive degree search range):\n- Case $1$: $(a,n_{\\min},n_{\\max})=(25,2,80)$\n- Case $2$: $(a,n_{\\min},n_{\\max})=(5,2,80)$\n- Case $3$: $(a,n_{\\min},n_{\\max})=(100,2,80)$\n- Case $4$: $(a,n_{\\min},n_{\\max})=(1,2,10)$\n\nUse the relative margin $\\tau=10^{-4}$ in the definition of $N_{\\text{crit}}$. For each case, determine the smallest integer $n$ with $n_{\\min}\\le n  n_{\\max}$ for which $E(n+1)\\left(1+\\tau\\right)E(n)$; if none exists in the specified range, report $-1$.\n\nFinal output format: Your program should produce a single line of output containing the four results in order as a comma-separated list enclosed in square brackets, for example $[N_{\\text{crit},1},N_{\\text{crit},2},N_{\\text{crit},3},N_{\\text{crit},4}]$, where each $N_{\\text{crit},j}$ is an integer.", "solution": "The problem presented is a valid exercise in computational physics and numerical analysis. It is well-posed, scientifically grounded, and contains all necessary information for a unique solution. The task concerns the investigation of Runge's phenomenon for polynomial interpolation of the Runge function, $f(x)=\\dfrac{1}{1+ax^2}$, on the interval $[-1,1]$. Specifically, we are to determine the \"crossover degree\" $N_{\\text{crit}}$, defined as the smallest integer degree $n$ within a specified range $[n_{\\min}, n_{\\max}-1]$ for which the integrated absolute error $E(n+1)$ shows a significant increase over $E(n)$. This phenomenon, where interpolation error with high-degree polynomials on equispaced nodes diverges near the interval boundaries, is a fundamental concept illustrating the limitations of naive interpolation strategies.\n\nThe methodology for solving this problem is a direct computational implementation of the definitions provided. For each test case, characterized by the parameter $a$ and the search range $[n_{\\min}, n_{\\max}]$, we must systematically compute the integrated error $E(n)$ for degrees $n$ and $n+1$ and check the specified condition.\n\nFirst, we define the core quantity, the integrated absolute interpolation error, for a given degree $n$:\n$$\nE(n) = \\int_{-1}^{1} \\left|f(x) - P_{n}(x)\\right|\\,dx\n$$\nHere, $f(x) = \\dfrac{1}{1+ax^2}$ is the function to be interpolated. The term $P_n(x)$ represents the unique interpolating polynomial of degree at most $n$ that passes through $n+1$ specific points on the curve of $f(x)$. These points are determined by the set of $n+1$ equispaced nodes:\n$$\nx_k = -1 + \\frac{2k}{n}, \\quad k = 0, 1, \\dots, n\n$$\nThe corresponding values are $y_k = f(x_k)$. The polynomial $P_n(x)$ can be constructed using these pairs $(x_k, y_k)$. While the Lagrange form is the classical representation, the Barycentric form is numerically more stable and efficient for evaluation, and is a standard choice in high-quality numerical libraries.\n\nThe integral defining $E(n)$ does not, in general, have a simple analytical closed form. Therefore, its evaluation must be performed numerically. A robust adaptive quadrature algorithm, such as the one implemented in `scipy.integrate.quad` which is based on a QUADPACK routine, is suitable. To satisfy the problem's requirement that the integral be evaluated to an absolute error no larger than $10^{-6}$, the numerical integration subroutine must be configured with a sufficiently small tolerance, for instance, an absolute tolerance of $10^{-9}$.\n\nThe overall algorithm proceeds as follows for each test case $(a, n_{\\min}, n_{\\max})$ with the fixed relative margin $\\tau = 10^{-4}$:\n1. Initialize the crossover degree $N_{\\text{crit}} = -1$.\n2. Iterate through integer degrees $n$ from $n_{\\min}$ to $n_{\\max}-1$.\n3. In each iteration, compute the error integrals $E(n)$ and $E(n+1)$. To optimize, the value of $E(n)$ from the previous iteration can be reused. For the first iteration, $n=n_{\\min}$, both $E(n_{\\min})$ and $E(n_{\\min}+1)$ must be computed. For subsequent iterations $n  n_{\\min}$, the value of $E(n)$ will have been computed as $E((n-1)+1)$ in the prior step.\n4. Check the crossover condition:\n   $$\n   E(n+1)  (1+\\tau) E(n)\n   $$\n5. If this condition is met, the current degree $n$ is the smallest integer that satisfies it. We set $N_{\\text{crit}} = n$ and terminate the search for this test case.\n6. If the loop completes without the condition ever being met, $N_{\\text{crit}}$ remains at its initial value of $-1$.\n\nThis computational procedure is repeated for all four test cases, and the resulting values of $N_{\\text{crit}}$ are collected. The parameter $a$ controls the sharpness of the peak in $f(x)$. A larger value of $a$ leads to a more pronounced Runge phenomenon, and we expect the error to start diverging at a lower degree $n$, leading to a smaller $N_{\\text{crit}}$. Conversely, a smaller $a$ results in a smoother function, delaying the onset of divergence, which should correspond to a larger $N_{\\text{crit}}$ or no crossover within the given range.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.interpolate import BarycentricInterpolator\nfrom scipy.integrate import quad\n\ndef solve():\n    \"\"\"\n    Solves for the crossover degree N_crit for the Runge function interpolation error\n    across several test cases.\n    \"\"\"\n\n    def calculate_error(n: int, a: float) -> float:\n        \"\"\"\n        Calculates the integrated absolute interpolation error E(n) for a given degree n and parameter a.\n\n        Args:\n            n: The degree of the interpolating polynomial.\n            a: The parameter of the Runge function.\n\n        Returns:\n            The value of the integral E(n).\n        \"\"\"\n        if n  0:\n            # A degree cannot be negative.\n            raise ValueError(\"Degree n must be non-negative.\")\n\n        # Define the Runge function f(x)\n        runge_func = lambda x: 1.0 / (1.0 + a * x**2)\n\n        # Generate n+1 equispaced nodes in [-1, 1]\n        x_nodes = np.linspace(-1.0, 1.0, n + 1)\n        y_nodes = runge_func(x_nodes)\n\n        # Construct the barycentric interpolating polynomial P_n(x)\n        poly = BarycentricInterpolator(x_nodes, y_nodes)\n\n        # Define the absolute error function |f(x) - P_n(x)|\n        error_func = lambda x: np.abs(runge_func(x) - poly(x))\n\n        # Numerically integrate the error function from -1 to 1.\n        # Set a small absolute tolerance to ensure the result is accurate to 1e-6 as required.\n        integral_val, _ = quad(error_func, -1.0, 1.0, epsabs=1e-9)\n\n        return integral_val\n\n    def find_Ncrit(a: float, n_min: int, n_max: int, tau: float) -> int:\n        \"\"\"\n        Finds the smallest integer n in [n_min, n_max-1] such that E(n+1) > (1+tau)E(n).\n        \n        Args:\n            a: The parameter of the Runge function.\n            n_min: The minimum degree to start the search from.\n            n_max: The exclusive upper bound for the search degree n.\n            tau: The relative margin for error increase.\n            \n        Returns:\n            The crossover degree N_crit, or -1 if not found.\n        \"\"\"\n        N_crit = -1\n\n        # Check for valid range\n        if n_min >= n_max:\n            return N_crit\n\n        # Use an error cache to avoid recomputing integrals\n        error_cache = {}\n\n        def get_E(n_val):\n            if n_val not in error_cache:\n                error_cache[n_val] = calculate_error(n_val, a)\n            return error_cache[n_val]\n        \n        # Loop n from n_min to n_max-1\n        for n in range(n_min, n_max):\n            E_n = get_E(n)\n            E_n_plus_1 = get_E(n + 1)\n\n            if E_n_plus_1 > (1.0 + tau) * E_n:\n                N_crit = n\n                break  # Found the smallest n, terminate search for this case\n        \n        return N_crit\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (25, 2, 80),   # Case 1\n        (5, 2, 80),    # Case 2\n        (100, 2, 80),  # Case 3\n        (1, 2, 10),    # Case 4\n    ]\n    \n    # Relative margin\n    tau = 1e-4\n\n    results = []\n    for a, n_min, n_max in test_cases:\n        result = find_Ncrit(a, n_min, n_max, tau)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2436036"}, {"introduction": "In the real world, data is rarely perfect; it almost always contains some level of noise from measurement error. This practice explores the critical, and often counter-intuitive, interaction between noisy data and high-degree interpolation. You will analyze how the building blocks of a Newton polynomial, the divided differences, respond to small perturbations in the data.", "problem": "In computational engineering practice, one frequently interpolates data with a Newton polynomial built from divided differences. Consider a smooth function $g$ on $[0,1]$ with $g \\in C^{k}([0,1])$ and a bound $|g^{(k)}(x)| \\le M_k$ for all $x \\in [0,1]$, where $k \\in \\{1,2,\\dots,n\\}$. Let $x_i = i h$ for $i=0,1,\\dots,n$ with spacing $h = 1/n$, and suppose we observe noisy samples $y_i = g(x_i) + \\eta_i$, where the noise $\\eta_i$ are independent, zero-mean random variables with variance $\\sigma^2$, and also satisfy $|\\eta_i| \\le \\delta$ almost surely for some $\\delta  0$.\n\nFor a fixed order $k$ and an index $i$ with $0 \\le i \\le n-k$, let $D_{i,k}$ denote the order-$k$ Newton divided difference computed from the consecutive nodes $x_i, x_{i+1}, \\dots, x_{i+k}$ using the observed data $y_i, y_{i+1}, \\dots, y_{i+k}$. Decompose $D_{i,k}$ additively into a deterministic contribution due to $g$ and a random contribution due to the noise.\n\nWhich option best characterizes how the magnitudes of these two contributions scale with $h$ for fixed $k$?\n\nA. For any fixed $k$, the deterministic part is bounded in magnitude by $M_k/k!$, independent of $h$, while the random part has standard deviation that scales like $C_k \\,\\sigma\\, h^{-k}$ for some constant $C_k$ depending only on $k$.\n\nB. For any fixed $k$, both the deterministic and the random parts are bounded independently of $h$.\n\nC. For any fixed $k$, the deterministic part scales like $M_k\\, h^{-k}$, while the random part has standard deviation independent of $h$.\n\nD. For any fixed $k$, the random part has standard deviation that scales like $C\\,\\sigma\\, h^{k}$ for some universal constant $C$ independent of $k$.", "solution": "The problem requires an analysis of the scaling behavior of the components of a Newton divided difference computed from noisy data. We will validate the problem statement and then proceed with a rigorous derivation.\n\nThe problem statement is scientifically sound, well-posed, objective, and self-contained. It presents a standard scenario in numerical analysis concerning the error propagation in finite difference approximations. All provided information is relevant and sufficient for a unique solution. The problem is valid.\n\nLet the order-$k$ divided difference operator for the sequence of nodes $x_i, x_{i+1}, \\dots, x_{i+k}$ be denoted by $\\mathcal{D}_k$. The object of study is $D_{i,k} = \\mathcal{D}_k[y]$. The data points are given by $y_j = g(x_j) + \\eta_j$. Due to the linearity of the divided difference operator, we can decompose $D_{i,k}$ as follows:\n$$ D_{i,k} = \\mathcal{D}_k[g + \\eta] = \\mathcal{D}_k[g] + \\mathcal{D}_k[\\eta] $$\nThe term $\\mathcal{D}_k[g]$ is the deterministic contribution from the underlying function $g$, and $\\mathcal{D}_k[\\eta]$ is the random contribution from the noise $\\eta$. We analyze the magnitude of each term as a function of the spacing $h$.\n\nFirst, consider the deterministic part, which we denote as $D_{i,k}^{(g)} = \\mathcal{D}_k[g]$. This is the divided difference of the function $g$ over the nodes $x_i, x_{i+1}, \\dots, x_{i+k}$. According to the mean value theorem for divided differences, for a function $g \\in C^k([0,1])$ and distinct nodes $z_0, \\dots, z_k$ in an interval $I$, there exists a point $\\xi$ in the interior of the smallest interval containing these nodes such that:\n$$ g[z_0, \\dots, z_k] = \\frac{g^{(k)}(\\xi)}{k!} $$\nIn our case, the nodes are $x_i, \\dots, x_{i+k}$, which lie within $[0,1]$. Thus, there exists a $\\xi \\in (x_i, x_{i+k})$ such that:\n$$ D_{i,k}^{(g)} = g[x_i, \\dots, x_{i+k}] = \\frac{g^{(k)}(\\xi)}{k!} $$\nWe are given the bound $|g^{(k)}(x)| \\le M_k$ for all $x \\in [0,1]$. Therefore, the magnitude of the deterministic part is bounded as:\n$$ |D_{i,k}^{(g)}| = \\left| \\frac{g^{(k)}(\\xi)}{k!} \\right| \\le \\frac{M_k}{k!} $$\nThis bound is a constant for a fixed order $k$ and is independent of the spacing $h$. As $h \\to 0$, $D_{i,k}^{(g)}$ converges to $g^{(k)}(x_i)/k!$, which is a finite, non-zero quantity (in general). Its magnitude does not scale with $h$.\n\nNext, consider the random part, which we denote as $D_{i,k}^{(\\eta)} = \\mathcal{D}_k[\\eta]$. This is the divided difference of the noise samples $\\eta_i, \\dots, \\eta_{i+k}$ over the nodes $x_i, \\dots, x_{i+k}$. The explicit formula for the divided difference is:\n$$ D_{i,k}^{(\\eta)} = \\sum_{j=0}^{k} \\frac{\\eta_{i+j}}{\\prod_{\\substack{m=0 \\\\ m \\ne j}}^{k} (x_{i+j} - x_{i+m})} $$\nThe nodes are $x_{i+j} = (i+j)h$. The denominator is:\n$$ \\prod_{\\substack{m=0 \\\\ m \\ne j}}^{k} ((i+j)h - (i+m)h) = h^k \\prod_{\\substack{m=0 \\\\ m \\ne j}}^{k} (j-m) = h^k (j!) (-1)^{k-j} (k-j)! $$\nSubstituting this into the expression for $D_{i,k}^{(\\eta)}$ gives:\n$$ D_{i,k}^{(\\eta)} = \\sum_{j=0}^{k} \\frac{\\eta_{i+j}}{h^k j! (-1)^{k-j} (k-j)!} = \\frac{1}{k! h^k} \\sum_{j=0}^{k} (-1)^{k-j} \\frac{k!}{j!(k-j)!} \\eta_{i+j} = \\frac{1}{k! h^k} \\sum_{j=0}^{k} (-1)^{k-j} \\binom{k}{j} \\eta_{i+j} $$\nThis is a linear combination of the random variables $\\eta_j$. The magnitude of this random quantity is best characterized by its standard deviation. First, its expected value is zero since $E[\\eta_j]=0$ for all $j$:\n$$ E[D_{i,k}^{(\\eta)}] = \\frac{1}{k! h^k} \\sum_{j=0}^{k} (-1)^{k-j} \\binom{k}{j} E[\\eta_{i+j}] = 0 $$\nThe variance is calculated using the independence of the noise samples $\\eta_j$, for which $Var(\\eta_j) = \\sigma^2$.\n$$ Var(D_{i,k}^{(\\eta)}) = Var\\left( \\frac{1}{k! h^k} \\sum_{j=0}^{k} (-1)^{k-j} \\binom{k}{j} \\eta_{i+j} \\right) = \\left(\\frac{1}{k! h^k}\\right)^2 \\sum_{j=0}^{k} \\left((-1)^{k-j} \\binom{k}{j}\\right)^2 Var(\\eta_{i+j}) $$\n$$ Var(D_{i,k}^{(\\eta)}) = \\frac{\\sigma^2}{(k!)^2 h^{2k}} \\sum_{j=0}^{k} \\binom{k}{j}^2 $$\nUsing the combinatorial identity $\\sum_{j=0}^{k} \\binom{k}{j}^2 = \\binom{2k}{k}$, we obtain:\n$$ Var(D_{i,k}^{(\\eta)}) = \\frac{\\sigma^2 \\binom{2k}{k}}{(k!)^2 h^{2k}} $$\nThe standard deviation, $SD(D_{i,k}^{(\\eta)})$, is the square root of the variance:\n$$ SD(D_{i,k}^{(\\eta)}) = \\sqrt{\\frac{\\sigma^2 \\binom{2k}{k}}{(k!)^2 h^{2k}}} = \\frac{\\sigma \\sqrt{\\binom{2k}{k}}}{k!} h^{-k} $$\nFor a fixed order $k$, the term $C_k = \\frac{\\sqrt{\\binom{2k}{k}}}{k!}$ is a constant that depends only on $k$. Therefore, the standard deviation of the random part scales as:\n$$ SD(D_{i,k}^{(\\eta)}) = C_k \\, \\sigma \\, h^{-k} $$\nThis indicates that the magnitude of the random contribution grows proportionally to $h^{-k}$ as $h \\to 0$.\n\nSummary of findings:\n-   Deterministic part magnitude $|D_{i,k}^{(g)}|$ is bounded by a constant $\\frac{M_k}{k!}$, independent of $h$.\n-   Random part magnitude, represented by its standard deviation $SD(D_{i,k}^{(\\eta)})$, scales as $h^{-k}$.\n\nNow we evaluate the given options.\n\nA. For any fixed $k$, the deterministic part is bounded in magnitude by $M_k/k!$, independent of $h$, while the random part has standard deviation that scales like $C_k \\,\\sigma\\, h^{-k}$ for some constant $C_k$ depending only on $k$.\nThis statement is fully consistent with our derivation. The deterministic part is bounded independently of $h$, and the random part's standard deviation scales as $h^{-k}$. The constant of proportionality includes $\\sigma$ and a factor $C_k$ depending on $k$. This option is **Correct**.\n\nB. For any fixed $k$, both the deterministic and the random parts are bounded independently of $h$.\nThis is incorrect. The random part's standard deviation is proportional to $h^{-k}$ and is not bounded as $h \\to 0$. This option is **Incorrect**.\n\nC. For any fixed $k$, the deterministic part scales like $M_k\\, h^{-k}$, while the random part has standard deviation independent of $h$.\nThis statement inverts the true scaling behavior. The deterministic part is bounded, while the random part's standard deviation depends strongly on $h$. This option is **Incorrect**.\n\nD. For any fixed $k$, the random part has standard deviation that scales like $C\\,\\sigma\\, h^{k}$ for some universal constant $C$ independent of $k$.\nThis statement is incorrect for two reasons. First, the scaling is $h^k$, not $h^{-k}$. Second, the constant $C_k = \\frac{\\sqrt{\\binom{2k}{k}}}{k!}$ is not universal but depends strongly on $k$. This option is **Incorrect**.\n\nTherefore, the only correct characterization is given in option A.", "answer": "$$\\boxed{A}$$", "id": "2409026"}, {"introduction": "Engineers frequently need to estimate derivatives from discrete dataâ€”for example, calculating velocity from a series of position measurements. A tempting approach is to fit an interpolating polynomial and then differentiate it, but this can be a numerically perilous operation. This exercise investigates the propagation of error in numerical differentiation and demonstrates how the choice of interpolation nodes is paramount.", "problem": "A computational engineer records a one-dimensional displacement signal $s(t)$ from a mechanical system over a time interval $[-1,1]$. The signal is known to be sufficiently smooth, specifically $s \\in C^{m}([-1,1])$ for some $m \\ge 2$. The engineer samples $s(t)$ at $n+1$ distinct nodes $x_0, x_1, \\dots, x_n \\in [-1,1]$ and constructs the unique polynomial $p_n(t)$ of degree at most $n$ that interpolates the measured data $\\{(x_i, y_i)\\}_{i=0}^n$, where $y_i = s(x_i) + \\eta_i$. Two noise regimes are considered:\n- Bounded noise: $|\\eta_i| \\le \\varepsilon$ for a fixed $\\varepsilon  0$.\n- Stochastic noise: $\\eta_i$ are independent, zero-mean random variables with variance $\\operatorname{Var}(\\eta_i) = \\sigma^2$.\n\nThe engineer uses the derivative $p_n'(t)$ as a proxy for the true velocity $v(t) = s'(t)$ and, in a force-calculation context, as a proxy for acceleration if differentiated again. Assume the engineer is interested in $p_n'(x_0)$ at a fixed interior point $x_0 \\in (-1,1)$. Consider both the choice of equispaced nodes and Chebyshev nodes of the first kind on $[-1,1]$.\n\nWhich of the following statements about the propagation of error into $p_n'(x_0)$ is true? Select all that apply.\n\nA. For equispaced nodes on $[-1,1]$ and a fixed interior $x_0$, the map from data perturbations $\\{\\eta_i\\}$ with $|\\eta_i| \\le \\varepsilon$ to the derivative error $|p_n'(x_0) - \\tilde p_n'(x_0)|$ (where $\\tilde p_n$ is the interpolant of the perturbed data) has an operator norm that grows without bound as $n$ increases.\n\nB. Switching from equispaced nodes to Chebyshev nodes reduces the worst-case amplification of data perturbations into $p_n'(x_0)$; in particular, the amplification with Chebyshev nodes grows at most polynomially (not exponentially) in $n$.\n\nC. Even with exact data ($\\eta_i = 0$), if $s$ is analytic on $[-1,1]$ and nodes are equispaced, then the maximum pointwise derivative error $\\max_{x \\in [-1,1]} |s'(x) - p_n'(x)|$ necessarily decreases monotonically to $0$ as $n$ increases.\n\nD. Under the stochastic noise model with independent, zero-mean noise of common variance $\\sigma^2$, one has $\\operatorname{Var}(p_n'(x_0)) = \\sigma^2 \\sum_{i=0}^n \\left(\\ell_i'(x_0)\\right)^2$, where $\\ell_i$ are the Lagrange basis polynomials for the chosen nodes. Consequently, the standard deviation of $p_n'(x_0)$ can grow significantly with $n$ for equispaced nodes.", "solution": "The problem statement must first be validated for scientific soundness and consistency.\n\n### Step 1: Extract Givens\n- A displacement signal $s(t)$, which is a function in the class $C^{m}([-1,1])$ for some integer $m \\ge 2$.\n- The time interval is $[-1,1]$.\n- A set of $n+1$ distinct interpolation nodes $\\{x_i\\}_{i=0}^n$ within the interval $[-1,1]$.\n- The measured data are $(x_i, y_i)$ for $i=0, \\dots, n$.\n- The relationship between the measured data and the true signal is $y_i = s(x_i) + \\eta_i$, where $\\eta_i$ is a noise term.\n- Two noise models are considered:\n    1. Bounded noise: $|\\eta_i| \\le \\varepsilon$ for a constant $\\varepsilon  0$.\n    2. Stochastic noise: $\\eta_i$ are independent, zero-mean ($\\mathbb{E}[\\eta_i] = 0$) random variables with common variance $\\operatorname{Var}(\\eta_i) = \\sigma^2$.\n- An interpolating polynomial $p_n(t)$ of degree at most $n$ is constructed through the data points $\\{(x_i, y_i)\\}_{i=0}^n$.\n- The derivative of the interpolant, $p_n'(t)$, is used as an approximation for the true velocity $s'(t)$.\n- The analysis focuses on the error in the derivative at a fixed interior point $x_0 \\in (-1,1)$. The problem appears to imply that this point of evaluation, $x_0$, is also one of the interpolation nodes. This is a notational choice that we will respect.\n- Two sets of nodes are considered: equispaced nodes and Chebyshev nodes of the first kind on $[-1,1]$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem describes a classic scenario in numerical analysis and computational science: the differentiation of noisy data using polynomial interpolation. This task is fundamental to many engineering applications where derivatives must be estimated from sampled measurements.\n\n- **Scientific Grounding**: The problem is firmly rooted in the theory of polynomial interpolation, numerical differentiation, and error analysis. The concepts of Lagrange polynomials, Chebyshev nodes, Runge's phenomenon, and error propagation are all standard, well-established topics in mathematics and engineering. The problem is scientifically sound.\n- **Well-Posedness**: The existence and uniqueness of the interpolating polynomial $p_n(t)$ for $n+1$ distinct nodes is guaranteed. The questions posed are about the asymptotic behavior of error metrics as the number of nodes $n$ increases, which are well-defined mathematical questions.\n- **Objectivity**: The problem is stated in precise, objective mathematical language. There are no subjective or ambiguous terms. The notational choice of using $x_0$ to denote both a specific node and the first element in a sequence of nodes is slightly confusing but does not create a logical contradiction, as nodes can be arbitrarily labeled. For any set of nodes on $[-1,1]$ with $n1$, both equispaced and Chebyshev sets contain interior points. One such point can be designated as $x_0$ for the analysis.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. It is a well-posed, scientifically grounded problem in numerical analysis. The solution can now be derived.\n\nThe interpolating polynomial $p_n(t)$ passing through the points $\\{(x_i, y_i)\\}_{i=0}^n$ can be written using the Lagrange basis as:\n$$p_n(t) = \\sum_{i=0}^n y_i \\ell_i(t)$$\nwhere $\\ell_i(t)$ are the Lagrange basis polynomials, defined by\n$$\\ell_i(t) = \\prod_{j=0, j \\neq i}^n \\frac{t-x_j}{x_i-x_j}$$\nThe derivative of the interpolating polynomial is:\n$$p_n'(t) = \\sum_{i=0}^n y_i \\ell_i'(t)$$\nLet $p_n^*(t)$ be the hypothetical interpolant of the exact, noise-free data, i.e., $p_n^*(t) = \\sum_{i=0}^n s(x_i) \\ell_i(t)$. The polynomial interpolating the noisy data is $p_n(t) = \\sum_{i=0}^n (s(x_i) + \\eta_i) \\ell_i(t)$. The error in the polynomial's derivative due to the data noise $\\eta_i$ is:\n$$\\Delta p_n'(t) \\equiv p_n'(t) - p_n^{*\\prime}(t) = \\sum_{i=0}^n \\eta_i \\ell_i'(t)$$\nThe problem asks about the magnitude of this error at a fixed interior point $x_0 \\in (-1,1)$. The options refer to $\\tilde{p}_n$ as the interpolant of perturbed data; we identify $p_n$ with $\\tilde{p}_n$ from the problem statement and $p_n^*$ with the unperturbed one. Thus, the quantity $|p_n'(x_0) - \\tilde{p}_n'(x_0)|$ in option A corresponds to $|\\Delta p_n'(x_0)| = |\\sum_{i=0}^n \\eta_i \\ell_i'(x_0)|$, where one set of data has $\\eta_i=0$.\n\nNow we evaluate each option.\n\n**A. For equispaced nodes on $[-1,1]$ and a fixed interior $x_0$, the map from data perturbations $\\{\\eta_i\\}$ with $|\\eta_i| \\le \\varepsilon$ to the derivative error $|p_n'(x_0) - \\tilde p_n'(x_0)|$ (where $\\tilde p_n$ is the interpolant of the perturbed data) has an operator norm that grows without bound as $n$ increases.**\n\nThe error in the derivative is $\\Delta p_n'(x_0) = \\sum_{i=0}^n \\eta_i \\ell_i'(x_0)$. Under the bounded noise model, $|\\eta_i| \\le \\varepsilon$. The worst-case error is found by maximizing over all possible perturbations:\n$$\\max_{|\\eta_i| \\le \\varepsilon} |\\Delta p_n'(x_0)| = \\max_{|\\eta_i| \\le \\varepsilon} \\left| \\sum_{i=0}^n \\eta_i \\ell_i'(x_0) \\right| = \\varepsilon \\sum_{i=0}^n |\\ell_i'(x_0)|$$\nThe operator norm of the linear map from the vector of perturbations $\\vec{\\eta} = (\\eta_0, \\dots, \\eta_n)$ (with the $\\ell_\\infty$ norm $\\|\\vec{\\eta}\\|_\\infty = \\max_i |\\eta_i|$) to the scalar output $\\Delta p_n'(x_0)$ is the amplification factor $\\sum_{i=0}^n |\\ell_i'(x_0)|$. This quantity is the condition number for differentiation at point $x_0$.\nFor equispaced nodes on $[-1,1]$, it is a well-established result in numerical analysis that this condition number grows exponentially with $n$. The underlying reason is related to Runge's phenomenon; the derivatives of the Lagrange basis functions become very large, especially near the boundaries, but the growth is exponential even at interior points. Since exponential growth is unbounded, the statement is true.\n**Verdict: Correct.**\n\n**B. Switching from equispaced nodes to Chebyshev nodes reduces the worst-case amplification of data perturbations into $p_n'(x_0)$; in particular, the amplification with Chebyshev nodes grows at most polynomially (not exponentially) in $n$.**\n\nThe worst-case amplification factor is $\\sum_{i=0}^n |\\ell_i'(x_0)|$. As established in A, for equispaced nodes this factor grows exponentially. For Chebyshev nodes of the first kind ($x_i = \\cos(\\frac{(2i+1)\\pi}{2(n+1)})$), the behavior of both the interpolant and its derivatives is much better. The Lebesgue constant $\\Lambda_n = \\max_{x \\in [-1,1]} \\sum_{i=0}^n |\\ell_i(x)|$ grows only as $O(\\log n)$. The condition number for differentiation, $\\sum_{i=0}^n |\\ell_i'(x_0)|$, also behaves much better. For any fixed point $x_0 \\in [-1,1]$, this sum is known to be bounded by a polynomial in $n$. Specifically, for an interior point $x_0 \\in (-1,1)$, the sum grows as $O(n^2)$.\nSwitching from exponential growth for equispaced nodes to polynomial ($O(n^2)$) growth for Chebyshev nodes is a significant reduction in the amplification of errors. The statement is therefore true.\n**Verdict: Correct.**\n\n**C. Even with exact data ($\\eta_i = 0$), if $s$ is analytic on $[-1,1]$ and nodes are equispaced, then the maximum pointwise derivative error $\\max_{x \\in [-1,1]} |s'(x) - p_n'(x)|$ necessarily decreases monotonically to $0$ as $n$ increases.**\n\nThis statement concerns the approximation error, not the propagation of data noise. It claims that for any analytic function $s(t)$, the derivative of the interpolant $p_n'(t)$ based on equispaced nodes converges uniformly and monotonically to the true derivative $s'(t)$. This is false.\nIt is famous that for equispaced nodes, polynomial interpolation does not guarantee uniform convergence for all analytic functions. The classic counterexample is the Runge function, $s(x) = 1/(1+25x^2)$, for which the maximum interpolation error $\\max_{x \\in [-1,1]}|s(x) - p_n(x)|$ diverges as $n \\to \\infty$. The error in the derivative, $|s'(x) - p_n'(x)|$, generally fares even worse, as differentiation amplifies the high-frequency oscillations of the error $s(x) - p_n(x)$. Therefore, uniform convergence of the derivative to zero is not guaranteed.\nThe additional claim that the error \"necessarily decreases monotonically\" is also false. Error in numerical methods rarely exhibits strict monotonic behavior; it often oscillates as the parameter $n$ changes.\n**Verdict: Incorrect.**\n\n**D. Under the stochastic noise model with independent, zero-mean noise of common variance $\\sigma^2$, one has $\\operatorname{Var}(p_n'(x_0)) = \\sigma^2 \\sum_{i=0}^n \\left(\\ell_i'(x_0)\\right)^2$, where $\\ell_i$ are the Lagrange basis polynomials for the chosen nodes. Consequently, the standard deviation of $p_n'(x_0)$ can grow significantly with $n$ for equispaced nodes.**\n\nThe derivative of the interpolant is $p_n'(x_0) = \\sum_{i=0}^n y_i \\ell_i'(x_0)$. The data points are $y_i = s(x_i) + \\eta_i$. Since $s(x_i)$ and $\\ell_i'(x_0)$ are deterministic constants, the variance of $p_n'(x_0)$ is due entirely to the noise terms $\\eta_i$.\n$$\\operatorname{Var}(p_n'(x_0)) = \\operatorname{Var}\\left(\\sum_{i=0}^n (s(x_i) + \\eta_i) \\ell_i'(x_0)\\right) = \\operatorname{Var}\\left(\\sum_{i=0}^n \\eta_i \\ell_i'(x_0)\\right)$$\nSince the noise terms $\\eta_i$ are independent, the variance of the sum is the sum of the variances:\n$$\\operatorname{Var}(p_n'(x_0)) = \\sum_{i=0}^n \\operatorname{Var}(\\eta_i \\ell_i'(x_0)) = \\sum_{i=0}^n (\\ell_i'(x_0))^2 \\operatorname{Var}(\\eta_i)$$\nGiven that $\\operatorname{Var}(\\eta_i) = \\sigma^2$ for all $i$, we get:\n$$\\operatorname{Var}(p_n'(x_0)) = \\sum_{i=0}^n (\\ell_i'(x_0))^2 \\sigma^2 = \\sigma^2 \\sum_{i=0}^n (\\ell_i'(x_0))^2$$\nThe first part of the statement is correct.\nThe standard deviation is $\\operatorname{StdDev}(p_n'(x_0)) = \\sqrt{\\operatorname{Var}(p_n'(x_0))} = \\sigma \\sqrt{\\sum_{i=0}^n (\\ell_i'(x_0))^2}$.\nThe term $\\sqrt{\\sum_{i=0}^n (\\ell_i'(x_0))^2}$ is the Euclidean norm ($\\ell_2$-norm) of the vector of derivative basis functions evaluated at $x_0$. As established in part A, the $\\ell_1$-norm of this vector, $\\sum_{i=0}^n |\\ell_i'(x_0)|$, grows exponentially for equispaced nodes. By the equivalence of norms in a finite-dimensional vector space ($\\|\\cdot\\|_1$ and $\\|\\cdot\\|_2$ on $\\mathbb{R}^{n+1}$), we have $\\|\\mathbf{v}\\|_2 \\le \\|\\mathbf{v}\\|_1 \\le \\sqrt{n+1}\\|\\mathbf{v}\\|_2$. Since $\\|\\mathbf{v}\\|_1$ grows exponentially, and $\\sqrt{n+1}$ grows only polynomially, $\\|\\mathbf{v}\\|_2$ must also grow exponentially. Therefore, the standard deviation of the derivative error grows exponentially, which is \"significantly\". The second part of the statement is also correct.\n**Verdict: Correct.**", "answer": "$$\\boxed{ABD}$$", "id": "2409024"}]}