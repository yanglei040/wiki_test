{"hands_on_practices": [{"introduction": "The QR factorization is not just an algebraic curiosity; it provides a powerful geometric tool. This first practice connects the factorization directly to the concept of orthogonal projection. By using the orthonormal columns of the $Q$ matrix as a basis, we can efficiently find the \"shadow\" of a vector onto a subspace, a fundamental operation in approximation theory and least-squares problems. [@problem_id:1385303]", "problem": "Let the matrix $A$ and the vector $\\mathbf{b}$ be defined as:\n$$A = \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\\\ 1 & 0 \\end{pmatrix}, \\quad \\mathbf{b} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix}$$\nThe columns of $A$ span a subspace of $\\mathbb{R}^3$, which we will denote as $W = \\text{Col}(A)$. The QR factorization of $A$ is $A=QR$, where $Q$ is a matrix with orthonormal columns and $R$ is an upper triangular matrix. The matrix $Q$, whose columns form an orthonormal basis for $W$, is given by:\n$$Q = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{6}} \\\\ 0 & \\frac{2}{\\sqrt{6}} \\\\ \\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{6}} \\end{pmatrix}$$\nDetermine the vector $\\mathbf{p}$, which is the orthogonal projection of $\\mathbf{b}$ onto the subspace $W$. Express your final answer as a row matrix containing the three components of $\\mathbf{p}$, using exact fractions.", "solution": "We are given a matrix $Q$ with orthonormal columns spanning $W=\\text{Col}(A)$. The orthogonal projection $\\mathbf{p}$ of $\\mathbf{b}$ onto $W$ is given by the formula\n$$\n\\mathbf{p} = Q Q^{\\top} \\mathbf{b} = \\sum_{i=1}^{2} (\\mathbf{q}_{i}^{\\top}\\mathbf{b})\\, \\mathbf{q}_{i},\n$$\nwhere $\\mathbf{q}_{1}$ and $\\mathbf{q}_{2}$ are the columns of $Q$.\n\nFrom the given $Q$,\n$$\n\\mathbf{q}_{1} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ 0 \\\\ \\frac{1}{\\sqrt{2}} \\end{pmatrix}, \\quad\n\\mathbf{q}_{2} = \\begin{pmatrix} \\frac{1}{\\sqrt{6}} \\\\ \\frac{2}{\\sqrt{6}} \\\\ -\\frac{1}{\\sqrt{6}} \\end{pmatrix}, \\quad\n\\mathbf{b} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix}.\n$$\nCompute the coefficients:\n$$\n\\mathbf{q}_{1}^{\\top}\\mathbf{b} = \\frac{1}{\\sqrt{2}}\\cdot 1 + 0\\cdot 2 + \\frac{1}{\\sqrt{2}}\\cdot 1 = \\frac{2}{\\sqrt{2}} = \\sqrt{2},\n$$\n$$\n\\mathbf{q}_{2}^{\\top}\\mathbf{b} = \\frac{1}{\\sqrt{6}}\\cdot 1 + \\frac{2}{\\sqrt{6}}\\cdot 2 - \\frac{1}{\\sqrt{6}}\\cdot 1 = \\frac{1+4-1}{\\sqrt{6}} = \\frac{4}{\\sqrt{6}}.\n$$\nThus,\n$$\n\\mathbf{p} = (\\mathbf{q}_{1}^{\\top}\\mathbf{b})\\,\\mathbf{q}_{1} + (\\mathbf{q}_{2}^{\\top}\\mathbf{b})\\,\\mathbf{q}_{2}\n= \\sqrt{2}\\,\\mathbf{q}_{1} + \\frac{4}{\\sqrt{6}}\\,\\mathbf{q}_{2}.\n$$\nCompute each term:\n$$\n\\sqrt{2}\\,\\mathbf{q}_{1} = \\sqrt{2}\\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ 0 \\\\ \\frac{1}{\\sqrt{2}} \\end{pmatrix}\n= \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix},\n\\quad\n\\frac{4}{\\sqrt{6}}\\,\\mathbf{q}_{2} = \\frac{4}{\\sqrt{6}}\\begin{pmatrix} \\frac{1}{\\sqrt{6}} \\\\ \\frac{2}{\\sqrt{6}} \\\\ -\\frac{1}{\\sqrt{6}} \\end{pmatrix}\n= \\begin{pmatrix} \\frac{4}{6} \\\\ \\frac{8}{6} \\\\ -\\frac{4}{6} \\end{pmatrix}\n= \\begin{pmatrix} \\frac{2}{3} \\\\ \\frac{4}{3} \\\\ -\\frac{2}{3} \\end{pmatrix}.\n$$\nAdding these gives\n$$\n\\mathbf{p} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} \\frac{2}{3} \\\\ \\frac{4}{3} \\\\ -\\frac{2}{3} \\end{pmatrix}\n= \\begin{pmatrix} \\frac{5}{3} \\\\ \\frac{4}{3} \\\\ \\frac{1}{3} \\end{pmatrix}.\n$$\nAs required, this is expressed with exact fractions. Interpreting the request for a row matrix, we write the components in a single row.", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{5}{3} & \\frac{4}{3} & \\frac{1}{3} \\end{pmatrix}}$$", "id": "1385303"}, {"introduction": "While the first exercise demonstrated the utility of the $Q$ matrix, the $R$ matrix also provides crucial information about the original matrix $A$. This practice delves into the algebraic consequences of linear dependence within the Gram-Schmidt process. By analyzing a simple case, you will discover how the structure of the upper triangular matrix $R$ directly reveals whether the columns of $A$ are linearly independent, a foundational concept for understanding matrix rank and the solvability of linear systems. [@problem_id:17522]", "problem": "### Problem Statement\n\nThe QR factorization of a matrix $A$ is a decomposition of the matrix into an orthogonal matrix $Q$ and an upper triangular matrix $R$, such that $A=QR$. A common method to find this factorization for a matrix $A$ with column vectors $[\\mathbf{a}_1, \\mathbf{a}_2, \\dots, \\mathbf{a}_n]$ is the Gram-Schmidt process.\n\nThe process constructs a set of orthogonal vectors $[\\mathbf{u}_1, \\mathbf{u}_2, \\dots, \\mathbf{u}_n]$ as follows:\n$\\mathbf{u}_1 = \\mathbf{a}_1$\n$\\mathbf{u}_k = \\mathbf{a}_k - \\sum_{j=1}^{k-1} \\text{proj}_{\\mathbf{u}_j}(\\mathbf{a}_k)$\nwhere the projection of a vector $\\mathbf{v}$ onto a vector $\\mathbf{u}$ is given by:\n$$\\text{proj}_{\\mathbf{u}}(\\mathbf{v}) = \\frac{\\langle \\mathbf{v}, \\mathbf{u} \\rangle}{\\langle \\mathbf{u}, \\mathbf{u} \\rangle} \\mathbf{u}$$\nHere, $\\langle \\mathbf{v}, \\mathbf{u} \\rangle$ denotes the standard Euclidean inner product (dot product).\n\nThe columns of the orthogonal matrix $Q$, denoted $\\mathbf{q}_k$, are the normalized vectors $\\mathbf{q}_k = \\frac{\\mathbf{u}_k}{||\\mathbf{u}_k||}$. The entries of the upper triangular matrix $R$ are then related to these vectors. Specifically, the diagonal elements of $R$ are given by the magnitudes of the orthogonal vectors:\n$$R_{kk} = ||\\mathbf{u}_k||$$\n\nConsider the following $2 \\times 2$ matrix $A$, whose columns are linearly dependent:\n$$A = \\begin{pmatrix} c_1 & k c_1 \\\\ c_2 & k c_2 \\end{pmatrix}$$\nwhere $c_1$ and $c_2$ are non-zero real constants, and $k$ is a non-zero real scalar.\n\nDerive the value of the element $R_{22}$ of the upper triangular matrix $R$ in the QR factorization of $A$.", "solution": "We have $A=[\\mathbf{a}_1, \\mathbf{a}_2]$ with $\\mathbf{a}_1=(c_1,c_2)^\\top$ and $\\mathbf{a}_2=k\\mathbf{a}_1$. We apply the Gram-Schmidt process.\n\nFirst, we set $\\mathbf{u}_1 = \\mathbf{a}_1$.\n\nNext, we compute $\\mathbf{u}_2 = \\mathbf{a}_2 - \\text{proj}_{\\mathbf{u}_1}(\\mathbf{a}_2)$. The projection is given by:\n$$\n\\text{proj}_{\\mathbf{u}_1}(\\mathbf{a}_2) = \\frac{\\langle \\mathbf{a}_2, \\mathbf{u}_1 \\rangle}{\\langle \\mathbf{u}_1, \\mathbf{u}_1 \\rangle} \\mathbf{u}_1\n$$\nWe compute the inner product in the numerator:\n$$\n\\langle \\mathbf{a}_2, \\mathbf{u}_1 \\rangle = \\langle k\\mathbf{a}_1, \\mathbf{a}_1 \\rangle = k \\langle \\mathbf{a}_1, \\mathbf{a}_1 \\rangle\n$$\nSubstituting this back into the projection formula:\n$$\n\\text{proj}_{\\mathbf{u}_1}(\\mathbf{a}_2) = \\frac{k \\langle \\mathbf{a}_1, \\mathbf{a}_1 \\rangle}{\\langle \\mathbf{a}_1, \\mathbf{a}_1 \\rangle} \\mathbf{a}_1 = k \\mathbf{a}_1\n$$\nNow we can find $\\mathbf{u}_2$:\n$$\n\\mathbf{u}_2 = \\mathbf{a}_2 - \\text{proj}_{\\mathbf{u}_1}(\\mathbf{a}_2) = k\\mathbf{a}_1 - k\\mathbf{a}_1 = \\mathbf{0}\n$$\nThe vector $\\mathbf{u}_2$ is the zero vector.\n\nBy definition, the diagonal element $R_{22}$ is the norm of $\\mathbf{u}_2$:\n$$\nR_{22} = \\|\\mathbf{u}_2\\| = \\|\\mathbf{0}\\| = 0.\n$$", "answer": "$$\\boxed{0}$$", "id": "17522"}, {"introduction": "Real-world engineering problems often involve data that arrives sequentially, and recomputing a full QR factorization for every new piece of data is computationally prohibitive. This advanced practice introduces a far more elegant and efficient approach: updating an existing factorization. You will derive and implement a method using orthogonal transformations to incorporate a new row of data directly into the $R$ factor, demonstrating a core technique for building adaptive and recursive algorithms. [@problem_id:2429968]", "problem": "You are given a real matrix $A \\in \\mathbb{R}^{m \\times n}$ with full column rank $n$, together with its thin QR factorization $A = Q R$, where $Q \\in \\mathbb{R}^{m \\times n}$ has orthonormal columns that satisfy $Q^{\\top} Q = I_n$, and $R \\in \\mathbb{R}^{n \\times n}$ is upper triangular with strictly positive diagonal entries. Consider appending a single row $a^{\\top} \\in \\mathbb{R}^{1 \\times n}$ to obtain the augmented matrix $A_{+} \\in \\mathbb{R}^{(m+1) \\times n}$ defined by\n$$\nA_{+} = \\begin{bmatrix} A \\\\ a^{\\top} \\end{bmatrix}.\n$$\nYour task is to, for each specified test case, compute the unique upper triangular matrix $R_{+} \\in \\mathbb{R}^{n \\times n}$ with strictly positive diagonal entries such that\n$$\nR_{+}^{\\top} R_{+} = A_{+}^{\\top} A_{+}.\n$$\nBy definition of the thin QR factorization, this $R_{+}$ is the upper triangular factor associated with the thin QR factorization of $A_{+}$ under the convention of strictly positive diagonal entries. The computation must be based only on the initial factor $R$ and the appended row $a^{\\top}$ through fundamental identities and properties of orthogonal transformations.\n\nFor each test case, define the scalar quantity\n$$\n\\Delta = \\max_{1 \\le i,j \\le n} \\left| \\left(R_{+}\\right)_{ij} - \\left(\\widehat{R}_{+}\\right)_{ij} \\right|,\n$$\nwhere $\\widehat{R}_{+}$ denotes the upper triangular factor (with strictly positive diagonal entries) obtained by directly computing a thin QR factorization of $A_{+}$, and where $\\left(\\cdot\\right)_{ij}$ denotes the entry in row $i$ and column $j$. You must output $\\Delta$ for each test case.\n\nTest Suite:\n- Case $1$:\n  - $A = \\begin{bmatrix}\n  2 & -1 & 0 \\\\\n  1 & 2 & 1 \\\\\n  0 & 1 & 2 \\\\\n  1 & 0 & 1\n  \\end{bmatrix} \\in \\mathbb{R}^{4 \\times 3}$,\n  - $a^{\\top} = \\begin{bmatrix} 1 & -2 & 3 \\end{bmatrix} \\in \\mathbb{R}^{1 \\times 3}$.\n- Case $2$:\n  - $A = \\begin{bmatrix}\n  1 & 0 & 0 \\\\\n  0 & 1 & 0 \\\\\n  0 & 0 & 1\n  \\end{bmatrix} \\in \\mathbb{R}^{3 \\times 3}$,\n  - $a^{\\top} = \\begin{bmatrix} 0 & 0 & 0 \\end{bmatrix} \\in \\mathbb{R}^{1 \\times 3}$.\n- Case $3$:\n  - $A = \\begin{bmatrix}\n  1.0 & 2.0 \\\\\n  2.0 & 4.0001 \\\\\n  3.0 & 6.0002\n  \\end{bmatrix} \\in \\mathbb{R}^{3 \\times 2}$,\n  - $a^{\\top} = \\begin{bmatrix} 4.0 & 8.0003 \\end{bmatrix} \\in \\mathbb{R}^{1 \\times 2}$.\n- Case $4$:\n  - $A = \\begin{bmatrix}\n  3 & 1 & 0 \\\\\n  1 & 3 & 1 \\\\\n  0 & 1 & 3\n  \\end{bmatrix} \\in \\mathbb{R}^{3 \\times 3}$,\n  - $a^{\\top} = \\begin{bmatrix} 1 & 0 & 1 \\end{bmatrix} \\in \\mathbb{R}^{1 \\times 3}$.\n- Case $5$:\n  - $A = \\begin{bmatrix}\n  1 \\\\\n  2 \\\\\n  3 \\\\\n  4 \\\\\n  5\n  \\end{bmatrix} \\in \\mathbb{R}^{5 \\times 1}$,\n  - $a^{\\top} = \\begin{bmatrix} 6 \\end{bmatrix} \\in \\mathbb{R}^{1 \\times 1}$.\n\nAnswer specification and output format:\n- For each case, compute $\\Delta$ as defined above.\n- Your program should produce a single line of output containing the $\\Delta$ values for Cases $1$ through $5$ in order, as a comma-separated list enclosed in square brackets, for example $[x_1,x_2,x_3,x_4,x_5]$, where each $x_k$ is a real number.\n- Each $x_k$ must be printed in scientific notation with exactly $10$ digits after the decimal point.", "solution": "The problem as stated is subjected to validation and is found to be scientifically grounded, well-posed, and objective. It presents a standard task in computational engineering: updating the QR factorization of a matrix to which a row has been appended. All provided data and conditions are consistent and sufficient for arriving at a unique, meaningful solution. Thus, we proceed to the derivation and implementation of the solution.\n\nThe problem requires the computation of the upper triangular factor $R_{+} \\in \\mathbb{R}^{n \\times n}$ from the thin QR factorization of an augmented matrix $A_{+} \\in \\mathbb{R}^{(m+1) \\times n}$. The matrix $A_{+}$ is formed by appending a row $a^{\\top} \\in \\mathbb{R}^{1 \\times n}$ to a matrix $A \\in \\mathbb{R}^{m \\times n}$ which has a known thin QR factorization $A = QR$. The factor $R_{+} \\in \\mathbb{R}^{n \\times n}$ is uniquely defined by the relation $R_{+}^{\\top} R_{+} = A_{+}^{\\top} A_{+}$ in conjunction with the constraint that its diagonal entries must be strictly positive. The computation must leverage the existing factor $R$ and the new row $a^{\\top}$.\n\nFirst, we establish the fundamental algebraic relationship. The normal equations matrix for $A_{+}$ is given by:\n$$\nA_{+}^{\\top} A_{+} = \\begin{bmatrix} A^{\\top} & a \\end{bmatrix} \\begin{bmatrix} A \\\\ a^{\\top} \\end{bmatrix} = A^{\\top} A + a a^{\\top}\n$$\nFrom the initial thin QR factorization $A=QR$, where $Q^{\\top}Q=I_{n}$, we have:\n$$\nA^{\\top} A = (QR)^{\\top}(QR) = R^{\\top}Q^{\\top}QR = R^{\\top}I_{n}R = R^{\\top}R\n$$\nSubstituting this into the expression for $A_{+}^{\\top} A_{+}$, we obtain the target relation for $R_{+}$:\n$$\nR_{+}^{\\top} R_{+} = R^{\\top}R + aa^{\\top}\n$$\nThe term $aa^{\\top}$ represents a rank-one update to the matrix $R^{\\top}R$. We seek the Cholesky factor of the updated matrix. A more direct path to $R_{+}$ can be found by observing that the right-hand side can be expressed as a product:\n$$\nR^{\\top}R + aa^{\\top} = \\begin{bmatrix} R^{\\top} & a \\end{bmatrix} \\begin{bmatrix} R \\\\ a^{\\top} \\end{bmatrix}\n$$\nThis reveals that $R_{+}^{\\top} R_{+}$ is the Gram matrix of the auxiliary matrix $\\begin{bmatrix} R \\\\ a^{\\top} \\end{bmatrix}$. Therefore, $R_{+}$ must be the R-factor of the thin QR factorization of this auxiliary matrix. Let us define this matrix as $M \\in \\mathbb{R}^{(n+1) \\times n}$:\n$$\nM = \\begin{bmatrix} R \\\\ a^{\\top} \\end{bmatrix}\n$$\nIf we find an orthogonal matrix $G \\in \\mathbb{R}^{(n+1) \\times (n+1)}$ such that $GM$ has its last row entirely zero, i.e.,\n$$\nGM = \\begin{bmatrix} R_{+} \\\\ 0_{1 \\times n} \\end{bmatrix}\n$$\nwhere $R_{+}$ is upper triangular, then from the orthogonality of $G$ ($G^{\\top}G=I$), we have:\n$$\nM^{\\top}M = (GM)^{\\top}(GM) = \\begin{bmatrix} R_{+}^{\\top} & 0^{\\top} \\end{bmatrix} \\begin{bmatrix} R_{+} \\\\ 0 \\end{bmatrix} = R_{+}^{\\top}R_{+}\n$$\nThis confirms that the R-factor of $M$ is indeed the desired $R_{+}$.\n\nSince $R$ is already upper triangular, the matrix $M$ is nearly in the required form. Only its last row, $a^{\\top}$, violates the upper triangular structure. We can restore this structure by systematically eliminating the non-zero entries of the last row using a sequence of Givens rotations.\n\nThe procedure is as follows. Let the working matrix be denoted by $\\tilde{M}$, initialized as $\\tilde{M} = M$. We apply a sequence of $n$ Givens rotations, $G_1, G_2, \\ldots, G_n$. For $j=1, 2, \\ldots, n$, the rotation $G_j$ is designed to annihilate the element at position $(n+1, j)$ of the current matrix, by rotating row $j$ with row $n+1$.\n\nSpecifically, for each $j \\in \\{1, \\dots, n\\}$, let the current matrix elements be $\\tilde{m}_{jk}$ and $\\tilde{m}_{n+1,k}$. We construct a Givens rotation in the $(j, n+1)$ plane to zero out the element $\\tilde{m}_{n+1, j}$. Let $x = \\tilde{m}_{jj}$ and $y = \\tilde{m}_{n+1, j}$. The Givens rotation parameters $c$ and $s$ are computed such that:\n$$\n\\begin{bmatrix} c & s \\\\ -s & c \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} = \\begin{bmatrix} \\sqrt{x^2+y^2} \\\\ 0 \\end{bmatrix}\n$$\nThis rotation is then applied to all columns $k = j, \\ldots, n$ of rows $j$ and $n+1$. Applying this sequence for $j=1, \\ldots, n$ successively transforms the matrix $M$:\n$$\nG_n \\cdots G_2 G_1 \\begin{bmatrix} R \\\\ a^{\\top} \\end{bmatrix} = \\begin{bmatrix} R_{+} \\\\ 0_{1 \\times n} \\end{bmatrix}\n$$\nThe resulting $n \\times n$ upper block is the required matrix $R_{+}$. Since the original diagonal entries $R_{ii}$ are strictly positive, and each new diagonal entry $(R_{+})_{jj}$ is computed as $\\sqrt{(\\tilde{m}_{jj})^2 + (\\tilde{m}_{n+1, j})^2}$, the diagonal entries of $R_{+}$ are also guaranteed to be strictly positive.\n\nThis algorithm computes $R_{+}$ using only $R$ and $a^{\\top}$, satisfying the problem constraints. For verification, the resulting $R_{+}$ is compared against $\\widehat{R}_{+}$, which is obtained from a direct thin QR factorization of $A_{+}$, ensuring the diagonal entries are positive. The maximum absolute difference $\\Delta$ between the entries of these two matrices quantifies the numerical consistency of the update procedure.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg.blas import drotg\n\ndef get_positive_diagonal_R(A):\n    \"\"\"\n    Computes the thin QR factorization of A and ensures the R factor has\n    strictly positive diagonal entries.\n    \"\"\"\n    # The problem statement guarantees A has full column rank.\n    if A.shape[1] == 0:\n        return np.empty((0, 0))\n        \n    _, R = np.linalg.qr(A, mode='reduced')\n    \n    n = R.shape[1]\n    for i in range(n):\n        if R[i, i] < 0:\n            R[i, :] *= -1\n            \n    return R\n\ndef solve():\n    \"\"\"\n    Solves the problem for all specified test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (\n            np.array([\n                [2.0, -1.0, 0.0],\n                [1.0, 2.0, 1.0],\n                [0.0, 1.0, 2.0],\n                [1.0, 0.0, 1.0]\n            ]),\n            np.array([1.0, -2.0, 3.0])\n        ),\n        (\n            np.array([\n                [1.0, 0.0, 0.0],\n                [0.0, 1.0, 0.0],\n                [0.0, 0.0, 1.0]\n            ]),\n            np.array([0.0, 0.0, 0.0])\n        ),\n        (\n            np.array([\n                [1.0, 2.0],\n                [2.0, 4.0001],\n                [3.0, 6.0002]\n            ]),\n            np.array([4.0, 8.0003])\n        ),\n        (\n            np.array([\n                [3.0, 1.0, 0.0],\n                [1.0, 3.0, 1.0],\n                [0.0, 1.0, 3.0]\n            ]),\n            np.array([1.0, 0.0, 1.0])\n        ),\n        (\n            np.array([\n                [1.0],\n                [2.0],\n                [3.0],\n                [4.0],\n                [5.0]\n            ]),\n            np.array([6.0])\n        ),\n    ]\n\n    delta_results = []\n\n    for A, a_row in test_cases:\n        m, n = A.shape\n        \n        # Step 1: Obtain the initial R factor with positive diagonals.\n        R = get_positive_diagonal_R(A)\n        \n        # Step 2: Form the augmented matrix M = [R^T, a]^T\n        M_aug = np.vstack([R, a_row.reshape(1, n)])\n        \n        # Step 3: Use Givens rotations to zero out the last row of M_aug.\n        # The working matrix is a copy of M_aug.\n        R_plus_computed_aug = M_aug.copy()\n        \n        for j in range(n):\n            # Elements to be rotated are in column j of rows j and n.\n            x = R_plus_computed_aug[j, j]\n            y = R_plus_computed_aug[n, j]\n            \n            # drotg computes c, s for a Givens rotation.\n            # It's a low-level BLAS function. For floats it's drotg.\n            c, s = drotg(x, y)\n\n            # Define the 2x2 Givens rotation matrix\n            G = np.array([[c, s], [-s, c]])\n            \n            # Apply the rotation to the relevant part of rows j and n.\n            # This affects columns from j to n-1.\n            rows_to_update = R_plus_computed_aug[[j, n], j:]\n            R_plus_computed_aug[[j, n], j:] = G @ rows_to_update\n\n        # The updated R factor is the top n x n block\n        R_plus_computed = R_plus_computed_aug[:n, :]\n        \n        # Step 4: Compute the reference R_plus by direct QR factorization of A_plus.\n        A_plus = np.vstack([A, a_row.reshape(1, n)])\n        R_plus_ref = get_positive_diagonal_R(A_plus)\n        \n        # Step 5: Compute the delta value.\n        delta = np.max(np.abs(R_plus_computed - R_plus_ref))\n        delta_results.append(delta)\n\n    # Final print statement in the exact required format.\n    formatted_results = [\"{:.10e}\".format(res) for res in delta_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2429968"}]}