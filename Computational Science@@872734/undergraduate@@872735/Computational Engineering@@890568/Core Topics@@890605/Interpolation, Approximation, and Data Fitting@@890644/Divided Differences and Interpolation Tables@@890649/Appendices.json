{"hands_on_practices": [{"introduction": "To begin, let's ground our understanding in a common engineering scenario where we have experimental data at discrete points but need to predict values in between. This first practice [@problem_id:2189672] guides you through the fundamental process of constructing a Newton interpolating polynomial from a set of data points by calculating the divided differences. You will then use the resulting coefficients to efficiently estimate a new value, demonstrating the entire workflow from raw data to practical prediction.", "problem": "A materials science lab is characterizing a new metallic alloy intended for high-performance heat exchangers. The thermal conductivity, $k$, of the alloy is measured at several distinct temperatures, $T$. The experimental data are as follows:\n\n- At $T_0 = 100$ K, the conductivity is $k_0 = 400$ W/(m·K).\n- At $T_1 = 200$ K, the conductivity is $k_1 = 390$ W/(m·K).\n- At $T_2 = 400$ K, the conductivity is $k_2 = 350$ W/(m·K).\n- At $T_3 = 600$ K, the conductivity is $k_3 = 300$ W/(m·K).\n\nTo predict the alloy's behavior at intermediate temperatures, an engineer decides to model the thermal conductivity using a cubic interpolating polynomial in Newton's form, $P_3(T)$, that passes through all four data points.\n\nYour task is to determine the coefficients of this polynomial from a divided differences table and then use Horner's nested evaluation method to estimate the thermal conductivity of the alloy at a temperature of $T = 300$ K.\n\nExpress your final answer for the thermal conductivity in W/(m·K), rounded to four significant figures.", "solution": "We are given data points $(T_{0},k_{0})=(100,400)$, $(T_{1},k_{1})=(200,390)$, $(T_{2},k_{2})=(400,350)$, $(T_{3},k_{3})=(600,300)$, and we construct the cubic Newton interpolating polynomial\n$$\nP_{3}(T)=f[T_{0}]+f[T_{0},T_{1}](T-T_{0})+f[T_{0},T_{1},T_{2}](T-T_{0})(T-T_{1})+f[T_{0},T_{1},T_{2},T_{3}](T-T_{0})(T-T_{1})(T-T_{2}).\n$$\nCompute divided differences step by step. First-order:\n$$\nf[T_{0},T_{1}]=\\frac{k_{1}-k_{0}}{T_{1}-T_{0}}=\\frac{390-400}{200-100}=-\\frac{1}{10},\n$$\n$$\nf[T_{1},T_{2}]=\\frac{k_{2}-k_{1}}{T_{2}-T_{1}}=\\frac{350-390}{400-200}=-\\frac{1}{5},\n$$\n$$\nf[T_{2},T_{3}]=\\frac{k_{3}-k_{2}}{T_{3}-T_{2}}=\\frac{300-350}{600-400}=-\\frac{1}{4}.\n$$\nSecond-order:\n$$\nf[T_{0},T_{1},T_{2}]=\\frac{f[T_{1},T_{2}]-f[T_{0},T_{1}]}{T_{2}-T_{0}}=\\frac{-\\frac{1}{5}-\\left(-\\frac{1}{10}\\right)}{400-100}=-\\frac{1}{3000},\n$$\n$$\nf[T_{1},T_{2},T_{3}]=\\frac{f[T_{2},T_{3}]-f[T_{1},T_{2}]}{T_{3}-T_{1}}=\\frac{-\\frac{1}{4}-\\left(-\\frac{1}{5}\\right)}{600-200}=-\\frac{1}{8000}.\n$$\nThird-order:\n$$\nf[T_{0},T_{1},T_{2},T_{3}]=\\frac{f[T_{1},T_{2},T_{3}]-f[T_{0},T_{1},T_{2}]}{T_{3}-T_{0}}=\\frac{-\\frac{1}{8000}-\\left(-\\frac{1}{3000}\\right)}{600-100}=\\frac{1}{2400000}.\n$$\nThus the Newton-form coefficients are\n$$\na_{0}=f[T_{0}]=400,\\quad a_{1}=f[T_{0},T_{1}]=-\\frac{1}{10},\\quad a_{2}=f[T_{0},T_{1},T_{2}]=-\\frac{1}{3000},\\quad a_{3}=f[T_{0},T_{1},T_{2},T_{3}]=\\frac{1}{2400000}.\n$$\nHence\n$$\nP_{3}(T)=400-\\frac{1}{10}(T-100)-\\frac{1}{3000}(T-100)(T-200)+\\frac{1}{2400000}(T-100)(T-200)(T-400).\n$$\nUse Horner’s nested evaluation at $T=300$:\n$$\nb_{2}=a_{2}+(T-T_{2})a_{3}=-\\frac{1}{3000}+(-100)\\cdot\\frac{1}{2400000}=-\\frac{1}{3000}-\\frac{1}{24000}=-\\frac{3}{8000},\n$$\n$$\nb_{1}=a_{1}+(T-T_{1})b_{2}=-\\frac{1}{10}+100\\left(-\\frac{3}{8000}\\right)=-\\frac{1}{10}-\\frac{3}{80}=-\\frac{11}{80},\n$$\n$$\nP_{3}(300)=a_{0}+(T-T_{0})b_{1}=400+200\\left(-\\frac{11}{80}\\right)=400-\\frac{2200}{80}=372.5.\n$$\nRounding to four significant figures gives $372.5$ W/(m·K).", "answer": "$$\\boxed{372.5}$$", "id": "2189672"}, {"introduction": "Beyond simple interpolation, the divided difference table is a powerful analytical tool that can reveal underlying patterns in data, such as whether a set of measurements could have been generated by a simple polynomial function. In this exercise [@problem_id:2386657], you will analyze a given dataset and use the properties of higher-order divided differences to determine the minimal degree of the polynomial that perfectly fits the points. This is a key skill in model selection and validation for surrogate modeling in engineering.", "problem": "A dataset of scalar pairs $(x_i, y_i)$ is recorded at five distinct nodes for use in a surrogate model within computational engineering. The nodes and values are:\n$(-2, -17)$, $(-1, 0)$, $(0, 4)$, $(1, 4)$, $(2, 9)$.\nBased only on the mathematical property of polynomials as reflected in divided differences, select the most precise statement about the minimal degree of a single-variable polynomial that can exactly generate all five data points.\n\nA. The data are consistent with a polynomial of degree at most $2$.\n\nB. The data are consistent with a polynomial of degree at most $3$ but not with any polynomial of degree at most $2$.\n\nC. The data are consistent with a polynomial of degree at most $4$ but not with any polynomial of degree at most $3$.\n\nD. No polynomial of degree at most $4$ can interpolate all the points.", "solution": "The minimal degree of a polynomial that fits a given set of data points can be determined by constructing a Newton divided difference table. For a polynomial of degree $n$, its $n$-th order divided differences are constant, and its $(n+1)$-th order divided differences are zero.\n\nLet the given data points be $(x_i, y_i)$ for $i=0, 1, 2, 3, 4$.\n$x_0 = -2, y_0 = -17$\n$x_1 = -1, y_1 = 0$\n$x_2 = 0, y_2 = 4$\n$x_3 = 1, y_3 = 4$\n$x_4 = 2, y_4 = 9$\n\nThe zeroth-order divided differences are the function values themselves, $f[x_i] = y_i$.\n\nThe first-order divided differences are calculated as:\n$f[x_0, x_1] = \\frac{y_1 - y_0}{x_1 - x_0} = \\frac{0 - (-17)}{-1 - (-2)} = \\frac{17}{1} = 17$\n$f[x_1, x_2] = \\frac{y_2 - y_1}{x_2 - x_1} = \\frac{4 - 0}{0 - (-1)} = \\frac{4}{1} = 4$\n$f[x_2, x_3] = \\frac{y_3 - y_2}{x_3 - x_2} = \\frac{4 - 4}{1 - 0} = \\frac{0}{1} = 0$\n$f[x_3, x_4] = \\frac{y_4 - y_3}{x_4 - x_3} = \\frac{9 - 4}{2 - 1} = \\frac{5}{1} = 5$\n\nThe second-order divided differences are:\n$f[x_0, x_1, x_2] = \\frac{f[x_1, x_2] - f[x_0, x_1]}{x_2 - x_0} = \\frac{4 - 17}{0 - (-2)} = \\frac{-13}{2}$\n$f[x_1, x_2, x_3] = \\frac{f[x_2, x_3] - f[x_1, x_2]}{x_3 - x_1} = \\frac{0 - 4}{1 - (-1)} = \\frac{-4}{2} = -2$\n$f[x_2, x_3, x_4] = \\frac{f[x_3, x_4] - f[x_2, x_3]}{x_4 - x_2} = \\frac{5 - 0}{2 - 0} = \\frac{5}{2}$\n\nThe third-order divided differences are:\n$f[x_0, x_1, x_2, x_3] = \\frac{f[x_1, x_2, x_3] - f[x_0, x_1, x_2]}{x_3 - x_0} = \\frac{-2 - (-\\frac{13}{2})}{1 - (-2)} = \\frac{- \\frac{4}{2} + \\frac{13}{2}}{3} = \\frac{\\frac{9}{2}}{3} = \\frac{3}{2}$\n$f[x_1, x_2, x_3, x_4] = \\frac{f[x_2, x_3, x_4] - f[x_1, x_2, x_3]}{x_4 - x_1} = \\frac{\\frac{5}{2} - (-2)}{2 - (-1)} = \\frac{\\frac{5}{2} + \\frac{4}{2}}{3} = \\frac{\\frac{9}{2}}{3} = \\frac{3}{2}$\n\nThe third-order divided differences are constant and non-zero. This already suggests the minimal degree of the polynomial is $3$. To confirm, we calculate the fourth-order divided difference.\n\nThe fourth-order divided difference is:\n$f[x_0, x_1, x_2, x_3, x_4] = \\frac{f[x_1, x_2, x_3, x_4] - f[x_0, x_1, x_2, x_3]}{x_4 - x_0} = \\frac{\\frac{3}{2} - \\frac{3}{2}}{2 - (-2)} = \\frac{0}{4} = 0$\n\nSince the third-order divided differences are constant and equal to $\\frac{3}{2} \\neq 0$, and the fourth-order divided difference is $0$, the data correspond exactly to a polynomial of degree $3$. The Newton form of this polynomial is $P_3(x) = f[x_0] + f[x_0,x_1](x-x_0) + f[x_0,x_1,x_2](x-x_0)(x-x_1) + f[x_0,x_1,x_2,x_3](x-x_0)(x-x_1)(x-x_2)$. The coefficient of the highest degree term, $x^3$, is $f[x_0,x_1,x_2,x_3] = \\frac{3}{2}$, which is non-zero. Therefore, the minimal degree is exactly $3$.\n\nNow we evaluate the given options.\n\nA. The data are consistent with a polynomial of degree at most $2$.\nThis statement implies that the data could be represented by a polynomial of degree $2$, $1$, or $0$. For this to be true, all third-order divided differences must be zero. Our calculation shows the third-order divided differences are $\\frac{3}{2}$, which is not zero. Thus, the minimal degree of the polynomial must be greater than $2$.\nVerdict: **Incorrect**.\n\nB. The data are consistent with a polynomial of degree at most $3$ but not with any polynomial of degree at most $2$.\nThis statement is composed of two claims. First, \"consistent with a polynomial of degree at most $3$\". This is true if the fourth-order divided differences are zero. We found that $f[x_0, x_1, x_2, x_3, x_4] = 0$, confirming this part. Second, \"not with any polynomial of degree at most $2$\". This is true because the third-order divided differences are non-zero. As both claims are correct, the statement accurately describes that the minimal degree of the interpolating polynomial is exactly $3$.\nVerdict: **Correct**.\n\nC. The data are consistent with a polynomial of degree at most $4$ but not with any polynomial of degree at most $3$.\nThe first part, \"consistent with a polynomial of degree at most $4$\", is trivially true for any five points. A polynomial of degree $3$ is a special case of a polynomial of degree at most $4$ (with the $x^4$ coefficient being zero). However, the second part, \"not with any polynomial of degree at most $3$\", is false. As we have shown, the data are perfectly described by a polynomial of degree $3$.\nVerdict: **Incorrect**.\n\nD. No polynomial of degree at most $4$ can interpolate all the points.\nThis statement contradicts the fundamental theorem of polynomial interpolation, which guarantees that for any $n+1$ distinct points, there exists a unique polynomial of degree at most $n$ that passes through them. Here we have $5$ points, so $n=4$. A unique polynomial of degree at most $4$ must exist. We have explicitly found it to be a polynomial of degree $3$.\nVerdict: **Incorrect**.", "answer": "$$\\boxed{B}$$", "id": "2386657"}, {"introduction": "While polynomial interpolation is a powerful technique, it has important limitations that every computational engineer must understand. This hands-on coding practice [@problem_id:2426405] explores the famous Runge phenomenon, where interpolation with equally spaced nodes can lead to wild oscillations and poor accuracy as the polynomial degree increases. By comparing this standard approach with the more sophisticated use of Chebyshev nodes, you will gain a deep, practical insight into the critical role that node selection plays in achieving stable and reliable numerical approximation.", "problem": "You are to implement polynomial interpolation in Newton form for the Runge function using two node-generation strategies. Start from the fundamental definition that for $n+1$ distinct nodes $\\{x_0,\\dots,x_n\\}$ and function values $\\{f(x_0),\\dots,f(x_n)\\}$, there exists a unique polynomial $p_n(x)$ of degree at most $n$ that satisfies $p_n(x_i)=f(x_i)$ for all $i$. Use the recursive definition of divided differences and the Newton basis to derive a numerically stable evaluation algorithm. The target function is the Runge function $f(x)=\\dfrac{1}{1+25x^2}$ on $[-1,1]$. All angles must be in radians.\n\nYour tasks are:\n- Implement a function to compute the divided differences for nodes $\\{x_i\\}_{i=0}^n$ and values $\\{f(x_i)\\}_{i=0}^n$, producing the Newton-form coefficients $\\{c_0,\\dots,c_n\\}$ for $p_n(x)$.\n- Implement a function to evaluate the Newton-form interpolant at arbitrary $x$ using nested multiplication based on the Newton basis $\\{1,(x-x_0),(x-x_0)(x-x_1),\\dots\\}$ and the previously computed coefficients $\\{c_k\\}_{k=0}^n$.\n- Generate two sets of interpolation nodes on $[-1,1]$ for each degree $n$:\n  - Equally spaced nodes: $x_i=-1+\\dfrac{2i}{n}$ for $i=0,\\dots,n$.\n  - Chebyshev nodes of the first kind (extrema) on $[-1,1]$: $x_i=\\cos\\!\\left(\\dfrac{i\\pi}{n}\\right)$ for $i=0,\\dots,n$. Use radians.\n- For each interpolant, evaluate $p_n(x)$ on a dense grid of $M$ points uniformly spaced on $[-1,1]$ and compute the maximum absolute error $E_{\\max}=\\max_{x\\in\\mathcal{G}}|p_n(x)-f(x)|$, where $\\mathcal{G}$ is the evaluation grid.\n- Do all arithmetic in double precision.\n\nTest suite specification:\n- Use the Runge function $f(x)=\\dfrac{1}{1+25x^2}$ on $[-1,1]$.\n- Use an evaluation grid $\\mathcal{G}$ of $M=10001$ equally spaced points on $[-1,1]$.\n- Use the following degrees (with $n+1$ nodes each): $n\\in\\{0,1,5,10,20\\}$.\n- For each $n$ in the above set, compute two numbers: $E_{\\max}^{\\text{eq}}(n)$ for equally spaced nodes and $E_{\\max}^{\\text{ch}}(n)$ for Chebyshev nodes (with angles in radians).\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain real numbers in the following order:\n  - $\\big[E_{\\max}^{\\text{eq}}(0),E_{\\max}^{\\text{ch}}(0),E_{\\max}^{\\text{eq}}(1),E_{\\max}^{\\text{ch}}(1),E_{\\max}^{\\text{eq}}(5),E_{\\max}^{\\text{ch}}(5),E_{\\max}^{\\text{eq}}(10),E_{\\max}^{\\text{ch}}(10),E_{\\max}^{\\text{eq}}(20),E_{\\max}^{\\text{ch}}(20)\\big]$.\n- The output must be a single line and must not contain any additional text.", "solution": "The problem starts from the core definition of interpolation: given $n+1$ distinct nodes $\\{x_0,\\dots,x_n\\}$ and data values $\\{y_0,\\dots,y_n\\}$ with $y_i=f(x_i)$, there exists a unique polynomial $p_n(x)$ of degree at most $n$ such that $p_n(x_i)=y_i$ for $i=0,\\dots,n$. One constructive representation is the Newton form, which combines two fundamental pieces: divided differences and the Newton basis. The Newton basis is defined recursively as $N_0(x)=1$, $N_k(x)=(x-x_{k-1})N_{k-1}(x)$ for $k\\ge 1$, which yields $N_k(x)=\\prod_{j=0}^{k-1}(x-x_j)$. The interpolant can be expressed as $p_n(x)=\\sum_{k=0}^n c_k N_k(x)$, where the coefficients $c_k$ are the divided differences $c_k=f[x_0,\\dots,x_k]$. The divided differences are determined from the data via the recursion $f[x_i]=y_i$ and, for $k\\ge 1$, $f[x_i,\\dots,x_{i+k}]=\\dfrac{f[x_{i+1},\\dots,x_{i+k}]-f[x_i,\\dots,x_{i+k-1}]}{x_{i+k}-x_i}$. This construction directly encodes the data constraints $p_n(x_i)=y_i$ by induction on $i$, ensuring existence and uniqueness.\n\nAlgorithmic design proceeds as follows. First, compute the divided differences. We can implement the recursion in-place on an array $d$ initialized as $d_i^{(0)}=y_i$. For each order $k=1,\\dots,n$ and indices $i=0,\\dots,n-k$, update $d_i^{(k)}=\\dfrac{d_{i+1}^{(k-1)}-d_i^{(k-1)}}{x_{i+k}-x_i}$. After completing all orders, the Newton coefficients are $c_k=d_0^{(k)}$ for $k=0,\\dots,n$. This is an $O(n^2)$ operation and uses only the core recursion.\n\nSecond, evaluate the interpolant efficiently and stably by nested multiplication (a Horner-like scheme adapted to the Newton basis). Starting with $v=c_n$, accumulate $v \\leftarrow c_{k}+ (x-x_k)\\,v$ for $k=n-1,\\dots,0$. This follows from the basis identity $N_k(x)=(x-x_k)N_{k+1}(x)$ rearranged to express $p_n$ in nested form: $p_n(x)=c_0+(x-x_0)\\left(c_1+(x-x_1)\\left(\\dots+(x-x_{n-1})c_n\\right)\\right)$. This evaluation is $O(n)$ per point and numerically stable relative to a naive basis expansion.\n\nFor node placement, we compare two strategies on $[-1,1]$. Equally spaced nodes use $x_i=-1+\\dfrac{2i}{n}$ for $i=0,\\dots,n$. Chebyshev nodes of the first kind (extrema) are $x_i=\\cos\\!\\left(\\dfrac{i\\pi}{n}\\right)$ for $i=0,\\dots,n$, which cluster near the endpoints and are known to reduce the maximum interpolation error for analytic functions. All trigonometric computations use radians, as specified. The Runge function $f(x)=\\dfrac{1}{1+25x^2}$ is analytic on and around $[-1,1]$, yet it famously exhibits the Runge phenomenon when interpolated with equally spaced nodes: as $n$ increases, the maximum error $E_{\\max}$ may worsen due to oscillations near $x=\\pm 1$. Chebyshev nodes mitigate this by minimizing the Lebesgue constant growth and distributing node density where it is most needed.\n\nTo quantify the behavior, we evaluate on a dense grid $\\mathcal{G}$ of $M=10001$ equispaced points on $[-1,1]$. For each $n\\in\\{0,1,5,10,20\\}$ and each node strategy, we compute the interpolant $p_n(x)$ on $\\mathcal{G}$ and then the maximum absolute error $E_{\\max}=\\max_{x\\in\\mathcal{G}}|p_n(x)-f(x)|$. The program outputs the sequence $[E_{\\max}^{\\text{eq}}(0),E_{\\max}^{\\text{ch}}(0),E_{\\max}^{\\text{eq}}(1),E_{\\max}^{\\text{ch}}(1),E_{\\max}^{\\text{eq}}(5),E_{\\max}^{\\text{ch}}(5),E_{\\max}^{\\text{eq}}(10),E_{\\max}^{\\text{ch}}(10),E_{\\max}^{\\text{eq}}(20),E_{\\max}^{\\text{ch}}(20)]$ on a single line. We expect $E_{\\max}^{\\text{eq}}(n)$ to initially decrease and then increase with $n$ due to the Runge phenomenon, while $E_{\\max}^{\\text{ch}}(n)$ should decrease more steadily.\n\nImplementation details ensure numerical robustness:\n- Divided differences are computed in place using double-precision arrays to avoid unnecessary copies.\n- The evaluation uses vectorized nested multiplication over the grid for efficiency.\n- For the Chebyshev node generator, $n=0$ is handled explicitly, yielding the single node $x_0=\\cos(0)=1$. For $n\\ge 1$, nodes $\\cos\\!\\left(\\dfrac{i\\pi}{n}\\right)$ for $i=0,\\dots,n$ are generated and sorted to ascending order; sorting does not change the interpolating polynomial when coefficients are recomputed for the sorted order, and it yields consistent ordering across strategies.\n\nThis principled approach directly reflects the core definitions of interpolation and divided differences, employs the Newton basis to obtain an efficient algorithm, and computes the requested error metrics on a fixed grid for objective comparison across node sets and degrees.", "answer": "```python\nimport numpy as np\n\ndef runge_function(x: np.ndarray) -> np.ndarray:\n    # f(x) = 1 / (1 + 25 x^2)\n    return 1.0 / (1.0 + 25.0 * x * x)\n\ndef divided_differences(x: np.ndarray, y: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute Newton divided differences coefficients.\n    x: nodes (n+1,)\n    y: values at nodes (n+1,)\n    Returns coefficients c such that p(x) = c0 + c1*(x-x0) + ... in Newton form.\n    \"\"\"\n    n = x.size - 1\n    # Build the divided difference table to find coefficients.\n    # A copy of y values is used to store the table entries.\n    dd_table = y.astype(float).copy()\n    coeffs = [dd_table[0]]\n    # Iterate to compute higher-order differences\n    for k in range(1, n + 1):\n        # Overwrite the previous order differences with the new ones\n        dd_table[: n - k + 1] = (dd_table[1: n - k + 2] - dd_table[: n - k + 1]) / (x[k:] - x[:-k])\n        # The first element of the updated array is the next coefficient\n        coeffs.append(dd_table[0])\n    return np.array(coeffs, dtype=float)\n\ndef newton_evaluate(x_eval: np.ndarray, x_nodes: np.ndarray, coeffs: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Evaluate Newton-form polynomial with given nodes and coefficients at x_eval.\n    Uses nested multiplication (Horner-like) for Newton basis.\n    \"\"\"\n    # Start from highest-order coefficient\n    p = np.full_like(x_eval, fill_value=coeffs[-1], dtype=float)\n    # Iterate backwards over nodes\n    for k in range(len(coeffs) - 2, -1, -1):\n        p = coeffs[k] + (x_eval - x_nodes[k]) * p\n    return p\n\ndef equispaced_nodes(n: int) -> np.ndarray:\n    # n+1 nodes from -1 to 1 inclusive\n    return np.linspace(-1.0, 1.0, n + 1, dtype=float)\n\ndef chebyshev_extrema_nodes(n: int) -> np.ndarray:\n    # Chebyshev nodes of the first kind (extrema): x_i = cos(i*pi/n), i=0..n\n    if n == 0:\n        nodes = np.array([1.0], dtype=float)\n    else:\n        i = np.arange(0, n + 1, dtype=float)\n        nodes = np.cos(np.pi * i / float(n))\n    # Sort ascending for consistency\n    nodes.sort()\n    return nodes\n\ndef max_abs_error_on_grid(n: int, node_strategy: str, grid: np.ndarray) -> float:\n    if node_strategy == \"equispaced\":\n        x_nodes = equispaced_nodes(n)\n    elif node_strategy == \"chebyshev\":\n        x_nodes = chebyshev_extrema_nodes(n)\n    else:\n        raise ValueError(\"Unknown node strategy\")\n\n    y_nodes = runge_function(x_nodes)\n    coeffs = divided_differences(x_nodes, y_nodes)\n    p_vals = newton_evaluate(grid, x_nodes, coeffs)\n    f_vals = runge_function(grid)\n    err = np.abs(p_vals - f_vals)\n    return float(np.max(err))\n\ndef solve():\n    # Define the test cases: degrees n\n    n_values = [0, 1, 5, 10, 20]\n    # Evaluation grid of M=10001 points on [-1,1]\n    M = 10001\n    grid = np.linspace(-1.0, 1.0, M, dtype=float)\n\n    results = []\n    for n in n_values:\n        e_eq = max_abs_error_on_grid(n, \"equispaced\", grid)\n        e_ch = max_abs_error_on_grid(n, \"chebyshev\", grid)\n        results.append(e_eq)\n        results.append(e_ch)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2426405"}]}