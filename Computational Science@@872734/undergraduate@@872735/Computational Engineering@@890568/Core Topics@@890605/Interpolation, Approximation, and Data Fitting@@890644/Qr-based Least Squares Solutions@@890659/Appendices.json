{"hands_on_practices": [{"introduction": "Many real-world problems, from image deblurring to machine learning, lead to ill-conditioned or underdetermined linear systems where a standard least squares approach fails. This practice introduces Tikhonov regularization, a powerful technique to stabilize the solution by penalizing its norm. You will implement a robust, QR-based solver for the equivalent augmented least squares problem, a standard method for tackling such challenges in computational engineering. [@problem_id:2430326]", "problem": "You are to implement a program that computes Tikhonov-regularized least squares solutions using a factorization based on the orthogonal-triangular decomposition, also known as the QR decomposition. The problem addresses ill-posed or ill-conditioned linear systems by solving a regularized least squares problem via an augmented overdetermined system.\n\nStarting point and fundamental base: The regularized least squares problem seeks, for a given matrix $A \\in \\mathbb{R}^{m \\times n}$, data vector $b \\in \\mathbb{R}^{m}$, and regularization parameter $\\lambda \\ge 0$, a vector $x \\in \\mathbb{R}^{n}$ that minimizes the objective function $\\lVert A x - b \\rVert_2^2 + \\lambda^2 \\lVert x \\rVert_2^2$, where $\\lVert \\cdot \\rVert_2$ denotes the Euclidean norm. This can be reformulated as a standard least squares problem by considering the augmented system $\\begin{pmatrix} A \\\\ \\lambda I \\end{pmatrix} x \\approx \\begin{pmatrix} b \\\\ 0 \\end{pmatrix}$, where $I \\in \\mathbb{R}^{n \\times n}$ is the identity matrix and $0 \\in \\mathbb{R}^{n}$ is the zero vector.\n\nYour task is to:\n1. Implement a function that, given $A$, $b$, and $\\lambda$, constructs the augmented matrix $C = \\begin{pmatrix} A \\\\ \\lambda I \\end{pmatrix}$ and the augmented right-hand side $d = \\begin{pmatrix} b \\\\ 0 \\end{pmatrix}$, computes the economy-size QR factorization $C = Q R$ with $Q \\in \\mathbb{R}^{(m+n) \\times n}$ having orthonormal columns and $R \\in \\mathbb{R}^{n \\times n}$ upper triangular, and then solves the triangular system $R x = Q^\\top d$ for $x$ by back substitution. Do not form or solve the normal equations, and do not use singular value decomposition.\n2. For each test case listed below, compute the Euclidean norm of the augmented residual, defined as $\\lVert C x - d \\rVert_2 = \\left\\lVert \\begin{pmatrix} A \\\\ \\lambda I \\end{pmatrix} x - \\begin{pmatrix} b \\\\ 0 \\end{pmatrix} \\right\\rVert_2$, using the solution $x$ obtained by your QR-based method.\n3. Produce a single line of output containing the results as a comma-separated list of floats, enclosed in square brackets, with each float rounded to six decimal places, in the order of the test cases described below.\n\nAlgorithmic constraints:\n- Use an orthogonal-triangular decomposition applied to the augmented system, followed by back substitution on the triangular factor. This leverages orthogonality to avoid squaring the condition number, which would occur when forming normal equations.\n\nTest suite:\nImplement and evaluate your solver on the following four deterministic test cases. In all definitions below, indices start at $0$.\n\n- Test case $1$ (overdetermined, ill-conditioned, unregularized):\n  - Dimensions: $m = 30$, $n = 10$.\n  - Matrix $A \\in \\mathbb{R}^{30 \\times 10}$ defined by entries $A_{i,j} = \\dfrac{1}{i + j + 1}$ for $0 \\le i \\le 29$ and $0 \\le j \\le 9$.\n  - True vector $x_{\\text{true}} \\in \\mathbb{R}^{10}$ defined by $(x_{\\text{true}})_j = 1$ if $j$ is even and $(x_{\\text{true}})_j = -1$ if $j$ is odd, for $0 \\le j \\le 9$.\n  - Noise vector $\\varepsilon \\in \\mathbb{R}^{30}$ defined by $\\varepsilon_i = 10^{-4} \\cos(i)$ for $0 \\le i \\le 29$.\n  - Right-hand side $b = A x_{\\text{true}} + \\varepsilon$.\n  - Regularization parameter $\\lambda = 0$.\n\n- Test case $2$ (same system as test case $1$ but with regularization):\n  - Use the same $A$, $x_{\\text{true}}$, $\\varepsilon$, and $b$ as in test case $1$.\n  - Regularization parameter $\\lambda = 10^{-2}$.\n\n- Test case $3$ (square, rank-deficient, exact data, slight regularization):\n  - Dimensions: $m = 8$, $n = 6$.\n  - Construct columns $c_k \\in \\mathbb{R}^{8}$ as follows:\n    - $c_0$ is the vector with entries $c_{0,i} = 1 + \\dfrac{i}{7}$ for $0 \\le i \\le 7$.\n    - $c_1 = c_0$ (duplicate column to ensure rank deficiency).\n    - $c_2$ has entries $c_{2,i} = \\sin\\!\\left(\\dfrac{\\pi i}{7}\\right)$.\n    - $c_3$ has entries $c_{3,i} = \\cos\\!\\left(\\dfrac{\\pi i}{3}\\right)$.\n    - $c_4 = c_2 + c_3$.\n    - $c_5$ is the constant vector with entries $1$ for all $i$.\n  - Form $A = [c_0\\, c_1\\, c_2\\, c_3\\, c_4\\, c_5] \\in \\mathbb{R}^{8 \\times 6}$.\n  - Define $x_{\\text{true}} \\in \\mathbb{R}^{6}$ with entries $(x_{\\text{true}})_j = [0.5, -0.5, 1.0, -1.0, 0.0, 2.0]_j$ for $0 \\le j \\le 5$.\n  - Right-hand side $b = A x_{\\text{true}}$.\n  - Regularization parameter $\\lambda = 10^{-6}$.\n\n- Test case $4$ (underdetermined, noisy, regularized):\n  - Dimensions: $m = 5$, $n = 10$.\n  - Matrix $A \\in \\mathbb{R}^{5 \\times 10}$ with entries $A_{i,j} = \\sin(i + j) + \\cos(2 i + 3 j)$ for $0 \\le i \\le 4$ and $0 \\le j \\le 9$, where the trigonometric functions take arguments in radians.\n  - True vector $x_{\\text{true}} \\in \\mathbb{R}^{10}$ with entries $(x_{\\text{true}})_j = \\sin(j)$ for $0 \\le j \\le 9$.\n  - Noise vector $\\varepsilon \\in \\mathbb{R}^{5}$ with entries $\\varepsilon_i = 10^{-5} \\sin(i)$ for $0 \\le i \\le 4$.\n  - Right-hand side $b = A x_{\\text{true}} + \\varepsilon$.\n  - Regularization parameter $\\lambda = 10^{-1}$.\n\nOutput specification:\n- For each test case $k \\in \\{1,2,3,4\\}$, compute the scalar $r_k = \\left\\lVert \\begin{pmatrix} A \\\\ \\lambda I \\end{pmatrix} x - \\begin{pmatrix} b \\\\ 0 \\end{pmatrix} \\right\\rVert_2$ using your QR-based solution $x$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order $[r_1, r_2, r_3, r_4]$, with each $r_k$ rounded to six decimal places, for example, $[0.123456,0.234567,0.345678,0.456789]$.\n\nAngle units:\n- All trigonometric functions use radian arguments.\n\nNo physical units are involved in this problem.\n\nYour implementation must be fully deterministic and must not rely on user input or external data. The program must be a complete, runnable script that prints only the required output line.", "solution": "The problem presented is valid and well-posed. It addresses a fundamental task in computational science: the stable solution of ill-conditioned or underdetermined linear systems using Tikhonov regularization. The specified methodology, which relies on the orthogonal-triangular ($QR$) decomposition of an augmented system, is a standard and numerically robust technique.\n\nThe objective is to find a vector $x \\in \\mathbb{R}^{n}$ that minimizes the Tikhonov-regularized objective function:\n$$ \\underset{x}{\\text{minimize}} \\quad \\lVert A x - b \\rVert_2^2 + \\lambda^2 \\lVert x \\rVert_2^2 $$\nwhere $A \\in \\mathbb{R}^{m \\times n}$ is the system matrix, $b \\in \\mathbb{R}^{m}$ is the data vector, and $\\lambda \\ge 0$ is the regularization parameter. The term $\\lambda^2 \\lVert x \\rVert_2^2$ penalizes solutions with large norms, which is crucial for stabilizing the problem when $A$ is ill-conditioned or rank-deficient.\n\nThis objective function can be expressed as the squared Euclidean norm of a single vector, which transforms the problem into a standard linear least squares format. We construct an augmented matrix $C \\in \\mathbb{R}^{(m+n) \\times n}$ and an augmented vector $d \\in \\mathbb{R}^{m+n}$:\n$$ C = \\begin{pmatrix} A \\\\ \\lambda I \\end{pmatrix}, \\quad d = \\begin{pmatrix} b \\\\ 0 \\end{pmatrix} $$\nHere, $I$ is the $n \\times n$ identity matrix, and $0$ is the zero vector in $\\mathbb{R}^{n}$. The original minimization problem is equivalent to:\n$$ \\underset{x}{\\text{minimize}} \\quad \\left\\lVert \\begin{pmatrix} A \\\\ \\lambda I \\end{pmatrix} x - \\begin{pmatrix} b \\\\ 0 \\end{pmatrix} \\right\\rVert_2^2 = \\underset{x}{\\text{minimize}} \\quad \\lVert C x - d \\rVert_2^2 $$\nThe solution $x$ to this least squares problem satisfies the normal equations:\n$$ C^\\top C x = C^\\top d $$\nHowever, the problem statement correctly forbids forming the matrix $C^\\top C$ explicitly. This is because the condition number of $C^\\top C$ is the square of the condition number of $C$, i.e., $\\kappa(C^\\top C) = \\kappa(C)^2$. Forming the normal equations can therefore introduce unnecessary numerical instability, especially for ill-conditioned systems.\n\nA numerically superior method is to use an orthogonal-triangular decomposition of $C$. We compute the economy-size $QR$ factorization of the augmented matrix $C$:\n$$ C = QR $$\nwhere $Q \\in \\mathbb{R}^{(m+n) \\times n}$ is a matrix with orthonormal columns (i.e., $Q^\\top Q = I_n$), and $R \\in \\mathbb{R}^{n \\times n}$ is an upper triangular matrix.\n\nSubstituting $C = QR$ into the least squares problem, we seek to minimize $\\lVert QRx - d \\rVert_2$. Since $Q$ has orthonormal columns, multiplying by $Q^\\top$ preserves the Euclidean norm of the residual component in the column space of $Q$. The solution $x$ is found by solving the transformed system. Substituting into the normal equations gives:\n$$ (QR)^\\top (QR) x = (QR)^\\top d $$\n$$ R^\\top Q^\\top Q R x = R^\\top Q^\\top d $$\n$$ R^\\top R x = R^\\top Q^\\top d $$\nFor the problem to have a unique solution, the matrix $R$ must be invertible. If $\\lambda  0$, the matrix $C$ is guaranteed to have full column rank, even if $A$ does not. This is because the rows corresponding to $\\lambda I$ ensure linear independence of the columns of $C$. Consequently, the upper triangular matrix $R$ will be full rank and thus invertible. If $\\lambda = 0$ and $A$ has full column rank, $R$ is also invertible. In these cases, we can multiply by $(R^\\top)^{-1}$ to obtain the upper triangular system:\n$$ R x = Q^\\top d $$\nThis system is solved efficiently for $x$ using back substitution.\n\nOnce the solution vector $x$ is computed, the final step is to calculate the Euclidean norm of the augmented residual, as required. This is computed directly:\n$$ r = \\lVert C x - d \\rVert_2 = \\left\\lVert \\begin{pmatrix} A \\\\ \\lambda I \\end{pmatrix} x - \\begin{pmatrix} b \\\\ 0 \\end{pmatrix} \\right\\rVert_2 $$\nThis procedure is applied to each of the four specified test cases. The constructions of the matrices and vectors for each case are executed precisely as defined in the problem statement. The final output consists of the computed residual norms.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import solve_triangular\n\ndef solve_regularized_ls_qr(A, b, lam):\n    \"\"\"\n    Solves the Tikhonov-regularized least squares problem min ||Ax-b||^2 + lam^2||x||^2\n    using QR factorization of the augmented system.\n\n    Args:\n        A (np.ndarray): The matrix A of shape (m, n).\n        b (np.ndarray): The vector b of shape (m,).\n        lam (float): The regularization parameter lambda.\n\n    Returns:\n        float: The Euclidean norm of the augmented residual, ||Cx - d||_2.\n    \"\"\"\n    m, n = A.shape\n\n    # 1. Construct the augmented matrix C and augmented vector d.\n    if lam  0:\n        C = np.vstack((A, lam * np.eye(n)))\n    else:\n        # Handle lambda = 0 case to avoid creating a zero matrix and then multiplying\n        C = np.vstack((A, np.zeros((n, n))))\n        \n    d = np.hstack((b, np.zeros(n)))\n\n    # 2. Compute the economy-size QR factorization of C.\n    Q, R = np.linalg.qr(C, mode='reduced')\n\n    # 3. Solve the upper triangular system Rx = Q^T d by back substitution.\n    qT_d = Q.T @ d\n    # SciPy's solver is efficient for this.\n    x = solve_triangular(R, qT_d, check_finite=False)\n\n    # 4. Compute the Euclidean norm of the augmented residual.\n    residual_norm = np.linalg.norm(C @ x - d)\n    \n    return residual_norm\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    test_cases = []\n\n    # Test case 1: overdetermined, ill-conditioned, unregularized\n    m1, n1 = 30, 10\n    lam1 = 0.0\n    i1 = np.arange(m1)[:, np.newaxis]\n    j1 = np.arange(n1)\n    A1 = 1.0 / (i1 + j1 + 1)\n    x_true1 = np.array([(-1)**k for k in range(n1)])\n    epsilon1 = 1e-4 * np.cos(np.arange(m1))\n    b1 = A1 @ x_true1 + epsilon1\n    test_cases.append({'A': A1, 'b': b1, 'lam': lam1})\n\n    # Test case 2: same system as test case 1 but with regularization\n    lam2 = 1e-2\n    test_cases.append({'A': A1, 'b': b1, 'lam': lam2})\n\n    # Test case 3: square, rank-deficient, exact data, slight regularization\n    m3, n3 = 8, 6\n    lam3 = 1e-6\n    i3 = np.arange(m3)\n    c0 = 1 + i3 / 7.0\n    c1 = c0\n    c2 = np.sin(np.pi * i3 / 7.0)\n    c3 = np.cos(np.pi * i3 / 3.0)\n    c4 = c2 + c3\n    c5 = np.ones(m3)\n    A3 = np.column_stack([c0, c1, c2, c3, c4, c5])\n    x_true3 = np.array([0.5, -0.5, 1.0, -1.0, 0.0, 2.0])\n    b3 = A3 @ x_true3\n    test_cases.append({'A': A3, 'b': b3, 'lam': lam3})\n\n    # Test case 4: underdetermined, noisy, regularized\n    m4, n4 = 5, 10\n    lam4 = 1e-1\n    i4 = np.arange(m4)[:, np.newaxis]\n    j4 = np.arange(n4)\n    A4 = np.sin(i4 + j4) + np.cos(2 * i4 + 3 * j4)\n    x_true4 = np.sin(np.arange(n4))\n    epsilon4 = 1e-5 * np.sin(np.arange(m4))\n    b4 = A4 @ x_true4 + epsilon4\n    test_cases.append({'A': A4, 'b': b4, 'lam': lam4})\n\n    results = []\n    for case in test_cases:\n        residual = solve_regularized_ls_qr(case['A'], case['b'], case['lam'])\n        results.append(residual)\n\n    # Final print statement in the exact required format.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2430326"}, {"introduction": "Time series modeling is fundamental to fields like finance and signal processing, and autoregressive (AR) models are a key tool. This exercise challenges you to fit AR models using QR factorization with column pivoting, a numerically superior method for handling the near-linear dependencies that often arise in lagged time series data. Through this task, you will gain hands-on experience with the practical concept of numerical rank and its importance in obtaining stable and meaningful model coefficients. [@problem_id:2430292]", "problem": "You are given time series data and asked to fit an autoregressive (AR) model using a numerically stable linear least squares formulation based on orthogonal-triangular (QR) factorization with column pivoting. The model is the autoregressive (AR) model of order $p$, defined by\n$$\nx_t \\;=\\; \\sum_{i=1}^{p} \\phi_i \\, x_{t-i} \\;+\\; \\varepsilon_t,\n$$\nwhere $x_t$ is the observed scalar time series, $\\phi_i$ are the AR coefficients to be estimated, and $\\varepsilon_t$ is an unobserved disturbance.\n\nFundamental starting points:\n- The least squares estimate minimizes the sum of squared residuals\n$$\n\\min_{\\phi \\in \\mathbb{R}^p} \\;\\left\\| H \\, \\phi - y \\right\\|_2^2,\n$$\nwhere $H$ is the data matrix of lagged observations and $y$ is the vector of current observations. Specifically, for a series $\\{x_1,\\dots,x_N\\}$ and order $p$, define $m = N - p$, then for each $t \\in \\{p+1,\\dots,N\\}$ the row $t-p$ of $H \\in \\mathbb{R}^{m \\times p}$ is $[x_{t-1}, x_{t-2}, \\dots, x_{t-p}]$ and the corresponding entry of $y \\in \\mathbb{R}^{m}$ is $x_t$.\n- Orthogonal-triangular (QR) factorization with column pivoting is a well-tested and numerically stable approach to solve least squares problems, including rank-deficient cases. If $H \\, P = Q \\, R$ with $Q$ orthonormal, $R$ upper-triangular, and $P$ a column permutation, then the least squares solution with minimal Euclidean norm in rank-deficient cases is obtained by truncating to the numerical rank.\n\nYour task is to write a complete program that:\n1. Constructs $H$ and $y$ from each provided time series $\\{x_t\\}$ and AR order $p$ as described above.\n2. Computes the least squares AR coefficient vector $\\hat{\\phi}$ using QR factorization with column pivoting. Determine the numerical rank $r$ by a tolerance\n$$\n\\tau \\;=\\; \\max(m,p)\\, \\epsilon \\, |R_{11}|,\n$$\nwhere $\\epsilon$ is the double precision machine epsilon and $R_{11}$ is the top-left entry of $R$. If $|R_{11}| = 0$, take $r = 0$. If $r  0$, let $R_1 \\in \\mathbb{R}^{r \\times r}$ be the leading triangular block and $Q_1 \\in \\mathbb{R}^{m \\times r}$ the corresponding orthonormal columns; then the minimal-norm solution is\n$$\n\\hat{z}_1 \\;=\\; R_1^{-1} \\, Q_1^\\top \\, y, \n\\quad\n\\hat{z}_2 \\;=\\; 0 \\in \\mathbb{R}^{p-r},\n\\quad\n\\hat{z} \\;=\\; \\begin{bmatrix}\\hat{z}_1 \\\\ \\hat{z}_2\\end{bmatrix},\n\\quad\n\\hat{\\phi} \\;=\\; P \\, \\hat{z}.\n$$\nIf $r = 0$, set $\\hat{\\phi} = 0 \\in \\mathbb{R}^p$.\n3. For each test case, output the estimated coefficient vector $\\hat{\\phi}$, with each component rounded to six decimal places.\n\nTest suite:\n- Test case $1$ (general well-conditioned case, exact AR of order $2$):\n  - Order: $p = 2$.\n  - Time series $\\{x_t\\}_{t=1}^{7}$: $[\\,100.0, 110.0, 80.0, 67.5, 53.75, 43.75, 35.3125\\,]$.\n- Test case $2$ (boundary case with minimal sample for $p = 1$):\n  - Order: $p = 1$.\n  - Time series $\\{x_t\\}_{t=1}^{2}$: $[\\,100.0, 102.0\\,]$.\n- Test case $3$ (nearly rank-deficient design for $p = 2$, geometric growth):\n  - Order: $p = 2$.\n  - Time series $\\{x_t\\}_{t=1}^{8}$: $[\\,100.0, 102.0, 104.04, 106.1208, 108.243216, 110.40808032, 112.6162419264, 114.868566764928\\,]$.\n- Test case $4$ (over-parameterized model $p = 3$ fitted to exact first-order dynamics, rank-deficient):\n  - Order: $p = 3$.\n  - Time series $\\{x_t\\}_{t=1}^{8}$: $[\\,100.0, 50.0, 25.0, 12.5, 6.25, 3.125, 1.5625, 0.78125\\,]$.\n\nFinal output format:\n- Your program should produce a single line of output containing the coefficient vectors for the four test cases, as a comma-separated list enclosed in square brackets. Each coefficient vector must itself be a bracketed, comma-separated list with each floating-point entry rounded to six decimal places. For example:\n$$\n[\\,[\\phi^{(1)}_1,\\dots,\\phi^{(1)}_{p_1}],\\,[\\phi^{(2)}_1,\\dots],\\,[\\phi^{(3)}_1,\\dots],\\,[\\phi^{(4)}_1,\\dots]\\,].\n$$\n- Concretely, the program must print exactly one line of the form\n$$\n[\\,[\\cdot,\\cdot],\\,[\\cdot],\\,[\\cdot,\\cdot],\\,[\\cdot,\\cdot,\\cdot]\\,],\n$$\nwith no spaces between numbers or brackets and each number formatted to six decimal places.", "solution": "The problem requires the estimation of coefficients for an autoregressive (AR) model of order $p$, specified as\n$$\nx_t = \\sum_{i=1}^{p} \\phi_i x_{t-i} + \\varepsilon_t\n$$\nwhere $\\{x_t\\}$ is a time series, $\\phi = [\\phi_1, \\dots, \\phi_p]^\\top$ is the vector of coefficients to be determined, and $\\varepsilon_t$ is a random disturbance term. The estimation is formulated as a linear least squares problem, which must be solved using QR factorization with column pivoting for numerical stability.\n\nFirst, we formalize the problem. Given a time series of length $N$, $\\{x_1, \\dots, x_N\\}$, and an AR order $p$, we construct a linear system. Let $m = N - p$. If $m \\le 0$, there is insufficient data to form the problem; all provided test cases satisfy $m  0$. We define a response vector $y \\in \\mathbb{R}^m$ and a data matrix $H \\in \\mathbb{R}^{m \\times p}$. The vector $y$ consists of the observations to be predicted, and the matrix $H$ contains the lagged observations used as predictors.\n$$\ny = \\begin{bmatrix} x_{p+1} \\\\ x_{p+2} \\\\ \\vdots \\\\ x_N \\end{bmatrix}, \\quad\nH = \\begin{bmatrix}\nx_p  x_{p-1}  \\cdots  x_1 \\\\\nx_{p+1}  x_p  \\cdots  x_2 \\\\\n\\vdots  \\vdots  \\ddots  \\vdots \\\\\nx_{N-1}  x_{N-2}  \\cdots  x_{N-p}\n\\end{bmatrix}\n$$\nThe AR model can then be expressed in matrix form as $y \\approx H\\phi$. The least squares estimate $\\hat{\\phi}$ is the vector that minimizes the sum of squared residuals, which is the squared Euclidean norm of the residual vector:\n$$\n\\hat{\\phi} = \\arg\\min_{\\phi \\in \\mathbb{R}^p} \\| H\\phi - y \\|_2^2\n$$\nA common method to solve this is via the normal equations, $H^\\top H \\phi = H^\\top y$. However, this is numerically unwise. The condition number of $H^\\top H$ is the square of the condition number of $H$, $\\kappa(H^\\top H) = \\kappa(H)^2$. If $H$ is ill-conditioned, $H^\\top H$ will be even more so, leading to large errors in the computed solution due to floating-point arithmetic.\n\nA numerically superior approach is to use an orthogonal factorization of $H$. The problem specifies QR factorization with column pivoting. This computes the decomposition $HP = QR$, where $P$ is a $p \\times p$ permutation matrix, $Q$ is an $m \\times m$ orthogonal matrix (its columns are orthonormal, $Q^\\top Q=I$), and $R$ is an $m \\times p$ upper trapezoidal matrix. The permutation matrix $P$ is chosen to ensure that the diagonal elements of $R$ are non-increasing in magnitude, which is crucial for reliably determining the numerical rank of $H$.\n\nSubstituting $H = QRP^{-1}$ into the objective function, and using the fact that the Euclidean norm is invariant under orthogonal transformations (i.e., $\\|Qz\\|_2 = \\|z\\|_2$), we get:\n$$\n\\| H\\phi - y \\|_2^2 = \\| QRP^{-1}\\phi - y \\|_2^2 = \\| Q(RP^{-1}\\phi - Q^\\top y) \\|_2^2 = \\| RP^{-1}\\phi - Q^\\top y \\|_2^2\n$$\nLet $z = P^{-1}\\phi$, which implies $\\phi = Pz$. The vector $z$ represents the coefficients corresponding to the permuted columns of $H$. The problem becomes minimizing $\\|Rz - Q^\\top y\\|_2^2$.\n\nThe matrix $H$ may be rank-deficient or nearly so, especially if the underlying time series exhibits strong trends or periodicities, causing the lagged observation vectors (columns of $H$) to be nearly linearly dependent. Column-pivoted QR is robust to this situation. We determine the numerical rank $r$ of $H$ by examining the diagonal elements of $R$. A tolerance $\\tau = \\max(m,p) \\cdot \\epsilon \\cdot |R_{11}|$ is used, where $\\epsilon$ is the machine epsilon. The rank $r$ is the number of diagonal elements $|R_{ii}|$ such that $|R_{ii}|  \\tau$. If $|R_{11}|=0$, the matrix is zero, so $r=0$.\n\nBased on rank $r$, we partition $R$, $z$, and $c = Q^\\top y$:\n$$\nR = \\begin{bmatrix} R_{11}  R_{12} \\\\ 0  R_{22} \\end{bmatrix}, \\quad z = \\begin{bmatrix} z_1 \\\\ z_2 \\end{bmatrix}, \\quad c = Q^\\top y = \\begin{bmatrix} c_1 \\\\ c_2 \\end{bmatrix}\n$$\nHere, $R_{11}$ is an $r \\times r$ upper-triangular matrix with non-zero diagonal elements, $z_1 \\in \\mathbb{R}^r$, and $c_1 \\in \\mathbb{R}^r$. The norm to be minimized becomes:\n$$\n\\| Rz - c \\|_2^2 = \\| R_{11}z_1 + R_{12}z_2 - c_1 \\|_2^2 + \\| R_{22}z_2 - c_2 \\|_2^2\n$$\nThe diagonal elements of the $(m-r) \\times (p-r)$ block $R_{22}$ are numerically zero (less than or equal to $\\tau$). For a minimal-norm solution, we set $z_2 = 0 \\in \\mathbb{R}^{p-r}$. This choice nullifies the contribution of the most uncertain components. The problem reduces to solving the full-rank upper-triangular system for $z_1$:\n$$\nR_{11} z_1 = c_1\n$$\nThis system is solved efficiently via back substitution. The resulting solution for the permuted coefficients is $\\hat{z} = [\\hat{z}_1^\\top, 0, \\dots, 0]^\\top \\in \\mathbb{R}^p$.\n\nFinally, the coefficient vector $\\hat{\\phi}$ for the original, unpermuted model is recovered by applying the permutation $P$. If the permutation indices from the QR routine are `piv`, such that `H_perm = H[:, piv]`, then the relationship between the coefficients is $\\hat{\\phi}_{\\text{piv}[k]} = \\hat{z}_k$.\nIn the edge case where the numerical rank $r=0$ (e.g., $H$ is a zero matrix), the minimal norm solution is simply $\\hat{\\phi}=0$.\n\nThe overall algorithm is:\n$1$. From time series $\\{x_t\\}$ and order $p$, construct the matrix $H$ and vector $y$.\n$2$. Compute the QR factorization with column pivoting: $H P = Q R$. Most numerical libraries return $Q$, $R$, and a permutation index vector.\n$3$. Determine the numerical rank $r$ using the specified tolerance $\\tau$.\n$4$. If $r=0$, set $\\hat{\\phi}=0$.\n$5$. If $r0$, solve the $r \\times r$ triangular system $R_{1:r, 1:r} \\, \\hat{z}_{1:r} = (Q^\\top y)_{1:r}$ for $\\hat{z}_{1:r}$.\n$6$. Construct $\\hat{z} \\in \\mathbb{R}^p$ by padding $\\hat{z}_{1:r}$ with $p-r$ zeros.\n$7$. Apply the inverse permutation to $\\hat{z}$ to obtain the final coefficient vector $\\hat{\\phi}$.\nThis procedure yields a robust, numerically stable estimate of the AR coefficients, providing the unique minimal-norm solution in cases of rank deficiency.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import qr, solve_triangular\n\ndef solve():\n    \"\"\"\n    Main function to solve the autoregressive model fitting problem\n    for all test cases as specified.\n    \"\"\"\n\n    # Test cases as defined in the problem description.\n    test_cases = [\n        # (order p, time series x_t)\n        (2, [100.0, 110.0, 80.0, 67.5, 53.75, 43.75, 35.3125]),\n        (1, [100.0, 102.0]),\n        (2, [100.0, 102.0, 104.04, 106.1208, 108.243216, 110.40808032, 112.6162419264, 114.868566764928]),\n        (3, [100.0, 50.0, 25.0, 12.5, 6.25, 3.125, 1.5625, 0.78125]),\n    ]\n\n    results = []\n    for p, x_ts in test_cases:\n        phi_hat = estimate_ar_coeffs(np.array(x_ts, dtype=float), p)\n        results.append(phi_hat)\n\n    # Format the final output string as required.\n    # e.g., [[c1,c2],[c1],[c1,c2],[c1,c2,c3]]\n    # with 6 decimal places and no spaces.\n    output_parts = []\n    for res in results:\n        formatted_coeffs = \",\".join([f\"{c:.6f}\" for c in res])\n        output_parts.append(f\"[{formatted_coeffs}]\")\n    \n    final_output = f\"[{','.join(output_parts)}]\"\n    print(final_output)\n\ndef estimate_ar_coeffs(x, p):\n    \"\"\"\n    Estimates AR(p) coefficients for a time series x using QR factorization\n    with column pivoting.\n\n    Args:\n        x (np.ndarray): The time series data.\n        p (int): The order of the autoregressive model.\n\n    Returns:\n        np.ndarray: The estimated AR coefficients phi_hat.\n    \"\"\"\n    N = len(x)\n    m = N - p\n\n    if m = 0:\n        # Not enough data points to form the least squares problem.\n        # This case is not in the test suite but is a necessary check.\n        return np.zeros(p)\n\n    # 1. Construct the Hankel matrix H and response vector y.\n    # H is m x p, y is m x 1\n    H = np.zeros((m, p))\n    y = x[p:]\n    \n    for i in range(m):\n        # The i-th row of H corresponds to predicting x_{p+i+1}\n        # using [x_{p+i}, x_{p+i-1}, ..., x_{i+1}]\n        # In array indexing, this is y[i] from x[p+i-1:i-1:-1]\n        H[i, :] = x[i:i+p][::-1]\n\n    # 2. Compute QR factorization with column pivoting: H P = Q R\n    Q, R, piv = qr(H, pivoting=True)\n    \n    # 3. Determine numerical rank r.\n    m, p_eff = H.shape\n    eps = np.finfo(float).eps\n    \n    # Check for the case where H is the zero matrix.\n    if np.abs(R[0, 0]) == 0:\n        r = 0\n    else:\n        tau = np.max(H.shape) * eps * np.abs(R[0, 0])\n        diag_R = np.abs(np.diag(R))\n        r = np.sum(diag_R  tau)\n    \n    if r == 0:\n        # If rank is 0, the minimal norm solution is phi_hat = 0.\n        return np.zeros(p)\n\n    # 4. Solve the least squares problem using the factorization.\n    # We want to solve H * phi = y, which is (Q R P^T) * phi = y\n    # Let z = P^T * phi. Then Q R z = y, or R z = Q^T y.\n    # Scipy's output `piv` is such that H[:,piv] = Q @ R.\n    # We solve for z in (H[:,piv]) @ z = y.\n    # The coefficients are then `phi[piv] = z`.\n\n    # Calculate c = Q^T * y\n    c = Q.T @ y\n\n    # 5. Solve the upper triangular system for the first r components of z.\n    # R_sub @ z_sub = c_sub\n    R1 = R[:r, :r]\n    c1 = c[:r]\n    z_hat_r = solve_triangular(R1, c1, lower=False)\n\n    # 6. Construct full z_hat vector with zeros for components beyond rank r.\n    z_hat = np.zeros(p)\n    z_hat[:r] = z_hat_r\n    \n    # 7. Apply inverse permutation to get phi_hat.\n    # The `piv` array maps new column index `j` to old column index `piv[j]`.\n    # z_hat[j] is the coefficient for old column `piv[j]`.\n    # So, phi_hat[piv[j]] = z_hat[j]\n    phi_hat = np.zeros(p)\n    phi_hat[piv] = z_hat\n        \n    return phi_hat\n\nsolve()\n```", "id": "2430292"}, {"introduction": "A numerically stable algorithm is one that does not unduly amplify errors present in the input. While QR factorization is celebrated for its stability, solving a least squares problem is a two-step process: factorization followed by back substitution. This practice focuses on a crucial but often overlooked detail: the back substitution step itself can be a source of instability, even if the triangular matrix $R$ is well-conditioned. By analyzing a carefully crafted example, you will compute the element-growth factor and gain a deeper appreciation for the subtleties of numerical error propagation. [@problem_id:2430289]", "problem": "An overdetermined linear system in computational engineering is solved in the least squares sense via the orthogonal-triangular (QR) factorization: for a full column rank matrix $A \\in \\mathbb{R}^{m \\times n}$ with $m \\geq n$, one computes $A = Q R$ with $Q \\in \\mathbb{R}^{m \\times m}$ orthogonal and $R \\in \\mathbb{R}^{m \\times n}$ upper triangular with its leading $n \\times n$ block nonsingular. The least squares solution $x \\in \\mathbb{R}^{n}$ is obtained by solving the upper triangular system $R_{1:n,1:n} x = Q^{T} b$ by back substitution.\n\nConsider $n = 4$ and suppose the $4 \\times 4$ upper triangular factor is\n$$\nR \\;=\\; \\begin{pmatrix}\n1  1  1  1\\\\\n0  1  1  1\\\\\n0  0  1  1\\\\\n0  0  0  1\n\\end{pmatrix},\n$$\nand the right-hand side for the triangular solve is\n$$\ny \\;=\\; Q^{T} b \\;=\\; \\begin{pmatrix} \\epsilon \\\\ 0 \\\\ -M \\\\ -2M \\end{pmatrix},\n$$\nwith $M = 10^{8}$ and $\\epsilon = 10^{-8}$.\n\nDefine the element-growth factor for back substitution as\n$$\nG \\;=\\; \\max_{1 \\leq i \\leq n} \\frac{|y_{i}| + \\sum_{j=i+1}^{n} |r_{ij}|\\,|x_{j}|}{|r_{ii}|\\,|x_{i}|},\n$$\nwhere $x$ is the exact solution of $R x = y$. Compute $G$ for the data above. Round your answer to four significant figures.", "solution": "The problem posed is a well-defined exercise in numerical linear algebra. It is scientifically sound, self-contained, and requires the application of standard definitions to a concrete example. We proceed with the solution.\n\nThe problem requires the computation of an element-growth factor $G$ for a given upper triangular system $Rx = y$. The system is defined by the matrix $R$ and the vector $y = Q^T b$. a full rank $A \\in \\mathbb{R}^{m \\times n}$, we are effectively examining the subsystem $R_{1:n,1:n}x = (Q^{T}b)_{1:n}$. The problem gives $n=4$ and provides a $4 \\times 4$ upper triangular matrix $R$ and a $4$-component vector $y$.\n\nThe system of equations is $Rx = y$, which is:\n$$\n\\begin{pmatrix}\n1  1  1  1\\\\\n0  1  1  1\\\\\n0  0  1  1\\\\\n0  0  0  1\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3 \\\\ x_4\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\epsilon \\\\ 0 \\\\ -M \\\\ -2M\n\\end{pmatrix}\n$$\nWe solve for the vector $x = (x_1, x_2, x_3, x_4)^T$ using back substitution, starting from the last equation and proceeding upwards.\n\nFrom the fourth row ($i=4$):\n$$\n1 \\cdot x_4 = -2M \\implies x_4 = -2M\n$$\n\nFrom the third row ($i=3$):\n$$\n1 \\cdot x_3 + 1 \\cdot x_4 = -M \\implies x_3 = -M - x_4 = -M - (-2M) = M\n$$\n\nFrom the second row ($i=2$):\n$$\n1 \\cdot x_2 + 1 \\cdot x_3 + 1 \\cdot x_4 = 0 \\implies x_2 = -x_3 - x_4 = -M - (-2M) = M\n$$\n\nFrom the first row ($i=1$):\n$$\n1 \\cdot x_1 + 1 \\cdot x_2 + 1 \\cdot x_3 + 1 \\cdot x_4 = \\epsilon \\implies x_1 = \\epsilon - x_2 - x_3 - x_4 = \\epsilon - M - M - (-2M) = \\epsilon\n$$\n\nThus, the exact solution vector is:\n$$\nx = \\begin{pmatrix}\n\\epsilon \\\\ M \\\\ M \\\\ -2M\n\\end{pmatrix}\n$$\n\nNext, we compute the element-growth factor $G$, defined as:\n$$\nG = \\max_{1 \\leq i \\leq n} G_i, \\quad \\text{where} \\quad G_i = \\frac{|y_{i}| + \\sum_{j=i+1}^{n} |r_{ij}|\\,|x_{j}|}{|r_{ii}|\\,|x_{i}|}\n$$\nFor this problem, $n=4$ and all diagonal elements $r_{ii} = 1$. The formula for each $G_i$ simplifies to:\n$$\nG_i = \\frac{|y_{i}| + \\sum_{j=i+1}^{4} |r_{ij}|\\,|x_{j}|}{|x_{i}|}\n$$\nWe compute $G_i$ for each $i \\in \\{1, 2, 3, 4\\}$.\n\nFor $i=4$:\nThe sum in the numerator is empty and evaluates to $0$.\n$$\nG_4 = \\frac{|y_4|}{|x_4|} = \\frac{|-2M|}{|-2M|} = 1\n$$\n\nFor $i=3$:\n$$\nG_3 = \\frac{|y_3| + |r_{34}|\\,|x_4|}{|x_3|} = \\frac{|-M| + |1| \\cdot |-2M|}{|M|} = \\frac{M + 2M}{M} = \\frac{3M}{M} = 3\n$$\n\nFor $i=2$:\n$$\nG_2 = \\frac{|y_2| + |r_{23}|\\,|x_3| + |r_{24}|\\,|x_4|}{|x_2|} = \\frac{|0| + |1| \\cdot |M| + |1| \\cdot |-2M|}{|M|} = \\frac{M + 2M}{M} = \\frac{3M}{M} = 3\n$$\n\nFor $i=1$:\n$$\nG_1 = \\frac{|y_1| + |r_{12}|\\,|x_2| + |r_{13}|\\,|x_3| + |r_{14}|\\,|x_4|}{|x_1|} = \\frac{|\\epsilon| + |1| \\cdot |M| + |1| \\cdot |M| + |1| \\cdot |-2M|}{|\\epsilon|}\n$$\nGiven that $M = 10^8  0$ and $\\epsilon = 10^{-8}  0$, we have $|\\epsilon| = \\epsilon$, $|M| = M$, and $|-2M|=2M$.\n$$\nG_1 = \\frac{\\epsilon + M + M + 2M}{\\epsilon} = \\frac{\\epsilon + 4M}{\\epsilon} = 1 + \\frac{4M}{\\epsilon}\n$$\n\nThe overall growth factor $G$ is the maximum of these values:\n$$\nG = \\max(G_1, G_2, G_3, G_4) = \\max\\left(1 + \\frac{4M}{\\epsilon}, 3, 3, 1\\right)\n$$\nWe substitute the given numerical values $M = 10^8$ and $\\epsilon = 10^{-8}$:\n$$\nG_1 = 1 + \\frac{4 \\times 10^8}{10^{-8}} = 1 + 4 \\times 10^{16}\n$$\nClearly, $1 + 4 \\times 10^{16}$ is substantially larger than $3$. Therefore,\n$$\nG = 1 + 4 \\times 10^{16} = 40,000,000,000,000,001\n$$\nThe problem requires the answer to be rounded to four significant figures. In scientific notation, the value is $4.0000000000000001 \\times 10^{16}$. The first four significant digits are $4$, $0$, $0$, $0$. The fifth significant digit is $0$, so we round down (i.e., we do not change the fourth digit).\nThe result, rounded to four significant figures, is $4.000 \\times 10^{16}$.", "answer": "$$\n\\boxed{4.000 \\times 10^{16}}\n$$", "id": "2430289"}]}