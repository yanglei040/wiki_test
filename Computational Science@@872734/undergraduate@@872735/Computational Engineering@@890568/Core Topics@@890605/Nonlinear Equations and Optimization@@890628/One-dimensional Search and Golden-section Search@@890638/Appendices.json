{"hands_on_practices": [{"introduction": "Understanding an algorithm deeply begins with building it from scratch. This first practice challenges you to derive the golden-section search from its foundational principles of unimodality and efficient interval reduction, revealing why the golden ratio emerges naturally. You will then implement the algorithm to handle 'black-box' functions, a common scenario in engineering where evaluations correspond to costly simulations or experiments, reinforcing the importance of an efficient search strategy. [@problem_id:2421111]", "problem": "You are tasked with implementing a one-dimensional search algorithm to minimize a scalar function that is only accessible through a separate, delayed black-box simulation. The algorithm must be golden-section search, derived from first principles of unimodality and interval reduction logic. Your program must be fully self-contained and produce the requested outputs without any user interaction.\n\nThe foundational base on which your design must rely includes the following facts and definitions:\n- A function $f(x)$ is unimodal on an interval $[a,b]$ if there exists a point $x^{\\star} \\in [a,b]$ such that $f(x)$ decreases on $[a,x^{\\star}]$ and increases on $[x^{\\star},b]$.\n- A one-dimensional search method aims to reduce an interval of uncertainty $[a,b]$ that provably contains the minimizer $x^{\\star}$, using only function evaluations $f(x)$.\n- The golden-section search is a bracketed search that chooses two interior points $c$ and $d$ within $[a,b]$ according to a constant ratio so that one of the points can be reused in the next iteration, thereby minimizing the number of function evaluations required.\n\nYour tasks:\n1. Derive the choice of the constant ratio for placing two interior points $c$ and $d$ within $[a,b]$ such that after discarding one sub-interval based on a single comparison of $f(c)$ and $f(d)$, one of the two points is reused and the new interval keeps the same proportion as the previous one. Do not assume the ratio; derive it from the invariant that the reduction factor is constant and one interior point is reused at each iteration. Base your derivation on the unimodality property and interval reduction logic.\n2. Implement the golden-section search algorithm resulting from your derivation. Your implementation must:\n   - Work on any unimodal function $f(x)$ supplied through a callable interface.\n   - Use only function evaluations $f(x)$ (no derivatives).\n   - Reuse one function evaluation at each iteration by design.\n   - Terminate when the interval length $(b-a)$ is less than or equal to a tolerance $\\varepsilon$, or when a maximum number of evaluations is reached.\n   - Return the estimate $\\hat{x} = (a+b)/2$ as the final minimizer.\n3. Model the delayed black-box simulation as follows:\n   - The callable returns the exact value of $f(x)$ for a given $x$, but it performs a deterministic, computationally costly internal loop whose results do not alter $f(x)$. This models a delay without changing the mathematics of $f(x)$.\n   - Implement simple memoization so repeated calls at the exact same $x$ do not re-run the internal loop. This reflects the practical need to minimize black-box calls.\n4. Use the following test suite of unimodal functions and intervals. For each test, run your golden-section search with tolerance $\\varepsilon = 10^{-8}$ and a maximum of $N_{\\max} = 200$ function evaluations. For each test, return the final estimate $\\hat{x}$ rounded to $8$ decimal places:\n   - Test A (happy path): $f(x) = (x - 1.234567)^{2} + 3$ on $[a,b] = [-5,5]$. The minimizer is interior.\n   - Test B (asymmetric convex): $f(x) = \\exp(0.3\\,x) + \\exp(-0.7\\,x)$ on $[a,b] = [-2,4]$. The minimizer is interior and not centered.\n   - Test C (flat-bottom unimodal): $f(x) = \\max(|x| - 1, 0)$ on $[a,b] = [-3,3]$. The set of minimizers is the interval $[-1,1]$. Any point $\\hat{x}$ in this set is acceptable; your algorithm should return the midpoint of the final bracket as specified above.\n   - Test D (boundary minimizer): $f(x) = (x + 2)^{2}$ on $[a,b] = [-10,-2]$. The minimizer lies at the boundary.\n5. Final Output Format:\n   - Your program should produce a single line of output containing the four minimizer estimates as a comma-separated list enclosed in square brackets, in the order A, B, C, D, with each value rounded to $8$ decimal places, for example: \"[xA,xB,xC,xD]\".\n   - All outputs are real numbers (floats) with exactly $8$ digits after the decimal point.\n\nNotes:\n- Angles are not involved in this problem.\n- There are no physical units involved.\n- Your program must be complete and runnable as is, with no user input.", "solution": "The problem statement is scrutinized and found to be valid. It is scientifically grounded in the established principles of numerical optimization, is well-posed with all necessary information provided, and is stated in an objective, formal language. The tasks are coherent and build upon one another, from theoretical derivation to practical implementation and testing. There are no contradictions, ambiguities, or factual inaccuracies. Thus, I will proceed with the solution.\n\nThe problem demands the derivation and implementation of the golden-section search algorithm for minimizing a unimodal function on a given interval. This will be addressed in three parts: first, the derivation of the characteristic ratio from first principles; second, the design of the algorithm based on this derivation; and third, the implementation and testing against the specified cases.\n\n**1. Derivation of the Golden-Section Ratio**\n\nLet $f(x)$ be a unimodal function on the interval $[a, b]$. Our goal is to find a smaller interval that is guaranteed to contain the minimizer $x^{\\star}$. The method relies on evaluating the function at two interior points, $c$ and $d$, where $a  c  d  b$.\n\nLet the initial interval be $[a_0, b_0]$ with length $L_0 = b_0 - a_0$. We choose the interior points $c_0$ and $d_0$ to be symmetrically placed with respect to a ratio $\\tau \\in (0, 1)$. Specifically, we define them relative to the interval's endpoints. A common and effective convention is to place them such that the segment $[c_0, d_0]$ is centered within $[a_0, b_0]$, and the ratio of the length of the larger segment to the full interval is $\\tau$.\nLet the points be:\n$c_0 = a_0 + (1-\\tau)(b_0-a_0)$\n$d_0 = a_0 + \\tau(b_0-a_0)$\nFor $c_0  d_0$, we must have $1 - \\tau  \\tau$, which implies $1  2\\tau$ or $\\tau  1/2$. The length of the new interval after one step will be $\\tau L_0$.\n\nWe evaluate $f(c_0)$ and $f(d_0)$ and use the property of unimodality to discard a portion of the interval.\n\nCase 1: $f(c_0)  f(d_0)$.\nDue to unimodality, the minimum cannot be in the sub-interval $(d_0, b_0]$. Therefore, the new interval of uncertainty is $[a_1, b_1] = [a_0, d_0]$. The length of this new interval is $L_1 = d_0 - a_0 = \\tau L_0$.\nThe key requirement of the golden-section search is that one of the interior points from the previous iteration can be reused in the current iteration. The new interval $[a_1, b_1]$ contains the old interior point $c_0$. We must arrange our choice of $\\tau$ such that $c_0$ becomes one of the new interior points, $c_1$ or $d_1$.\nThe new interior points for the interval $[a_1, b_1]$ are:\n$c_1 = a_1 + (1-\\tau)L_1 = a_0 + (1-\\tau)(\\tau L_0)$\n$d_1 = a_1 + \\tau L_1 = a_0 + \\tau(\\tau L_0) = a_0 + \\tau^2 L_0$\nWe want to set one of these equal to the old point $c_0 = a_0 + (1-\\tau)L_0$.\nLet's try setting $d_1 = c_0$:\n$a_0 + \\tau^2 L_0 = a_0 + (1-\\tau)L_0$\n$\\tau^2 = 1 - \\tau$\n$\\tau^2 + \\tau - 1 = 0$\n\nCase 2: $f(c_0) \\geq f(d_0)$.\nBy unimodality, the minimum must be in $[c_0, b_0]$. The new interval is $[a_1, b_1] = [c_0, b_0]$. The length of this new interval is $L_1 = b_0 - c_0 = b_0 - (a_0 + (1-\\tau)L_0) = (b_0-a_0) - (1-\\tau)L_0 = L_0 - (1-\\tau)L_0 = \\tau L_0$.\nThe new interval contains the old interior point $d_0$.\nThe new interior points for $[a_1, b_1]$ are:\n$c_1 = a_1 + (1-\\tau)L_1 = c_0 + (1-\\tau)(\\tau L_0) = (a_0 + (1-\\tau)L_0) + (1-\\tau)\\tau L_0$\n$d_1 = a_1 + \\tau L_1 = c_0 + \\tau(\\tau L_0) = (a_0 + (1-\\tau)L_0) + \\tau^2 L_0$\nWe must equate one of these to the old point $d_0 = a_0 + \\tau L_0$.\nLet's try setting $c_1 = d_0$:\n$(a_0 + (1-\\tau)L_0) + (1-\\tau)\\tau L_0 = a_0 + \\tau L_0$\n$1-\\tau + \\tau - \\tau^2 = \\tau$\n$1 - \\tau^2 = \\tau$\n$\\tau^2 + \\tau - 1 = 0$\n\nBoth cases lead to the same quadratic equation: $\\tau^2 + \\tau - 1 = 0$.\nSolving for $\\tau$ using the quadratic formula, $\\tau = \\frac{-1 \\pm \\sqrt{1^2 - 4(1)(-1)}}{2}$:\n$$ \\tau = \\frac{-1 \\pm \\sqrt{5}}{2} $$\nSince $\\tau$ represents a ratio of lengths, it must be positive. Thus, we take the positive root:\n$$ \\tau = \\frac{\\sqrt{5} - 1}{2} \\approx 0.6180339887... $$\nThis number is the reciprocal of the golden ratio, $\\phi = \\frac{1+\\sqrt{5}}{2}$. The ratio for the smaller segment is $1-\\tau = \\tau^2 = \\frac{3-\\sqrt{5}}{2}$. This choice of $\\tau$ ensures that in each iteration, the interval is reduced by a factor of $\\tau$, and one of the interior points from the previous step is reused, saving one function evaluation per iteration after the initial setup.\n\n**2. Algorithm Design and Implementation**\n\nBased on the derivation, the algorithm proceeds as follows:\n\nLet $\\rho = \\frac{\\sqrt{5}-1}{2}$.\n1.  **Initialization:**\n    Given an interval $[a, b]$, a function $f$, a tolerance $\\varepsilon$, and a maximum number of evaluations $N_{\\max}$.\n    Calculate the initial interior points:\n    $c = b - \\rho(b-a)$\n    $d = a + \\rho(b-a)$\n    Evaluate the function at these two points: $f_c = f(c)$ and $f_d = f(d)$.\n    Initialize an evaluation counter, `eval_count = 2`.\n\n2.  **Iteration:**\n    Loop while the interval length $(b-a)  \\varepsilon$ and `eval_count` $ N_{\\max}$:\n    If $f_c  f_d$:\n        The new interval is $[a, d]$. Set $b \\leftarrow d$.\n        The old point $c$ becomes the new point $d$. Set $d \\leftarrow c$.\n        The corresponding function value is also reused: $f_d \\leftarrow f_c$.\n        Calculate the new point $c$: $c \\leftarrow b - \\rho(b-a)$.\n        Evaluate the function only at the new point $c$: $f_c \\leftarrow f(c)$.\n    Else ($f_c \\geq f_d$):\n        The new interval is $[c, b]$. Set $a \\leftarrow c$.\n        The old point $d$ becomes the new point $c$. Set $c \\leftarrow d$.\n        The corresponding function value is reused: $f_c \\leftarrow f_d$.\n        Calculate the new point $d$: $d \\leftarrow a + \\rho(b-a)$.\n        Evaluate the function only at the new point $d$: $f_d \\leftarrow f(d)$.\n    Increment `eval_count` by $1$.\n\n3.  **Termination:**\n    When the loop terminates, the minimizer is contained within the final interval $[a, b]$. The estimate for the minimizer is the midpoint of this interval: $\\hat{x} = (a+b)/2$.\n\n**3. Black-Box Simulation and Testing**\nA Python class will be used to encapsulate the black-box function. This class will implement memoization using a dictionary to store previously computed values, avoiding re-computation for identical inputs. A deterministic delay will simulate a costly evaluation. The derived algorithm will then be applied to the four test cases specified in the problem statement, using this black-box model.\n\nThe Python implementation in the `final_answer` section will strictly follow this design to compute the minimizers for the given test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport time\n\n# language: Python\n# version: 3.12\n# libraries:\n#     - name: numpy, version: 1.23.5\n#     - name: scipy, version: 1.11.4\n\nclass BlackBoxFunction:\n    \"\"\"\n    A wrapper for a scalar function to simulate a delayed black-box evaluation with memoization.\n    - Simulates computational cost with a deterministic internal loop.\n    - Uses memoization to cache results and avoid re-running the expensive evaluation for the same input.\n    \"\"\"\n    def __init__(self, func):\n        self.func = func\n        self.memo = {}\n        self.evals = 0\n\n    def __call__(self, x):\n        \"\"\"\n        Evaluates the function at x, using cache if available.\n        \"\"\"\n        # Use a tuple representation of x as a key for hashability, handling potential float precision issues by rounding.\n        # For this problem, exact reuse is expected, so no rounding is necessary\n        x_key = x \n        if x_key in self.memo:\n            return self.memo[x_key]\n        else:\n            self.evals += 1\n            # Simulate a costly computation; this loop's result does not affect f(x)\n            # A simple busy-wait loop.\n            _ = sum(i for i in range(1000))\n            result = self.func(x)\n            self.memo[x_key] = result\n            return result\n\ndef golden_section_search(f_wrapped, a, b, tol, max_evals):\n    \"\"\"\n    Performs a golden-section search to find the minimum of a unimodal function.\n\n    Args:\n        f_wrapped: The BlackBoxFunction object to minimize.\n        a: The lower bound of the initial interval.\n        b: The upper bound of the initial interval.\n        tol: The tolerance for the interval length.\n        max_evals: The maximum number of function evaluations allowed.\n\n    Returns:\n        The estimated minimizer x_hat.\n    \"\"\"\n    rho = (np.sqrt(5) - 1) / 2  # Golden ratio conjugate, approx 0.618\n\n    # Initialize the two interior points\n    c = b - rho * (b - a)\n    d = a + rho * (b - a)\n\n    # Perform initial function evaluations\n    fc = f_wrapped(c)\n    fd = f_wrapped(d)\n\n    while (b - a)  tol and f_wrapped.evals  max_evals:\n        if fc  fd:\n            # The new interval is [a, d]\n            b = d\n            # The old c becomes the new d\n            d = c\n            fd = fc # Reuse function value\n            # Calculate the new c and its function value\n            c = b - rho * (b - a)\n            fc = f_wrapped(c)\n        else: # fc = fd\n            # The new interval is [c, b]\n            a = c\n            # The old d becomes the new c\n            c = d\n            fc = fd # Reuse function value\n            # Calculate the new d and its function value\n            d = a + rho * (b - a)\n            fd = f_wrapped(d)\n    \n    # Return the midpoint of the final interval\n    return (a + b) / 2\n\ndef solve():\n    \"\"\"\n    Sets up and runs the test suite for the golden-section search algorithm.\n    \"\"\"\n    # Define test functions\n    f_A = lambda x: (x - 1.234567)**2 + 3\n    f_B = lambda x: np.exp(0.3 * x) + np.exp(-0.7 * x)\n    f_C = lambda x: np.maximum(np.abs(x) - 1, 0)\n    f_D = lambda x: (x + 2)**2\n\n    test_cases = [\n        {\"func\": f_A, \"interval\": [-5.0, 5.0], \"name\": \"A\"},\n        {\"func\": f_B, \"interval\": [-2.0, 4.0], \"name\": \"B\"},\n        {\"func\": f_C, \"interval\": [-3.0, 3.0], \"name\": \"C\"},\n        {\"func\": f_D, \"interval\": [-10.0, -2.0], \"name\": \"D\"},\n    ]\n\n    results = []\n    \n    # Parameters from the problem statement\n    epsilon = 1e-8\n    N_max = 200\n\n    for case in test_cases:\n        f_callable = BlackBoxFunction(case[\"func\"])\n        a, b = case[\"interval\"]\n        \n        minimizer = golden_section_search(f_callable, a, b, tol=epsilon, max_evals=N_max)\n        \n        # Round to 8 decimal places as required for the output format\n        results.append(f\"{minimizer:.8f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2421111"}, {"introduction": "An optimization algorithm is only as powerful as your ability to apply it to real-world problems. This exercise moves from theory to application by tasking you with a classic geometric problem: finding the point on a curve closest to a given target. You will learn to frame this challenge as a one-dimensional search by minimizing a distance function, a key skill in computational engineering and data science. [@problem_id:2398578]", "problem": "You are asked to implement a one-dimensional optimizer using golden-section search to solve a geometric projection problem on a curve. Let the curve be given by a scalar function $f:\\mathbb{R}\\to\\mathbb{R}$ and a fixed target point $(x_0,y_0)\\in\\mathbb{R}^2$. The goal is to find the point on the curve that is closest in Euclidean distance to the target, by minimizing the distance function $D(x)=\\sqrt{(x-x_0)^2+(f(x)-y_0)^2}$ on a closed interval $[a,b]$. Your program must rely only on function evaluations of $f(x)$ and must not use derivatives or higher-order models.\n\nUse the following foundational base:\n- The Euclidean distance between $(x,f(x))$ and $(x_0,y_0)$ is $D(x)=\\sqrt{(x-x_0)^2+(f(x)-y_0)^2}$.\n- A strictly increasing transformation preserves minimizers: if $g$ is strictly increasing and $x^\\ast\\in \\arg\\min_{x\\in[a,b]} h(x)$, then $x^\\ast\\in \\arg\\min_{x\\in[a,b]} g(h(x))$.\n- Golden-section search is a bracketing method that finds a minimizer of a continuous unimodal function on $[a,b]$ using only function evaluations and a fixed ratio to place interior points, repeatedly shrinking the interval until a tolerance criterion is satisfied.\n\nYour task:\n1. Implement golden-section search to minimize the distance $D(x)$ over a given interval $[a,b]$. For numerical stability and efficiency, you may minimize the squared distance $S(x)=(x-x_0)^2+(f(x)-y_0)^2$ instead of $D(x)$, provided you justify why this yields the same minimizer.\n2. Assume $D(x)$ is continuous and unimodal on the provided interval for each test case below. Your implementation must:\n   - Use only function evaluations of $S(x)$ (or $D(x)$).\n   - Terminate when the interval length is less than or equal to a tolerance $\\varepsilon$ or a maximum iteration cap is reached.\n   - Return an approximate minimizer $\\hat{x}$ in $[a,b]$.\n\nTest suite:\nImplement the function $f(x)$ and parameters $(x_0,y_0)$ and $[a,b]$ for each of the following four test cases. There are no physical units involved. Angles are not used. All constants are real numbers.\n\n- Case $1$ (happy path, smooth convex curve):\n  - $f(x)=x^2$\n  - $(x_0,y_0)=(1.0,-1.0)$\n  - $[a,b]=[-2.0,2.0]$\n  - Tolerance $\\varepsilon=10^{-8}$\n\n- Case $2$ (smooth non-polynomial, non-symmetric):\n  - $f(x)=\\mathrm{e}^x$\n  - $(x_0,y_0)=(0.0,0.5)$\n  - $[a,b]=[-2.0,1.0]$\n  - Tolerance $\\varepsilon=10^{-8}$\n\n- Case $3$ (boundary minimum, monotone objective over interval):\n  - $f(x)=0$\n  - $(x_0,y_0)=(2.0,0.7)$\n  - $[a,b]=[-1.0,1.0]$\n  - Tolerance $\\varepsilon=10^{-8}$\n\n- Case $4$ (smooth, gently curved, nonnegative $f$ with negative $y_0$):\n  - $f(x)=\\log(1+x^2)$\n  - $(x_0,y_0)=(0.5,-0.2)$\n  - $[a,b]=[-3.0,3.0]$\n  - Tolerance $\\varepsilon=10^{-8}$\n\nRequired final output format:\n- For each case, compute the approximate minimizer $\\hat{x}$.\n- Round each $\\hat{x}$ to $6$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the cases above, for example $[\\hat{x}_1,\\hat{x}_2,\\hat{x}_3,\\hat{x}_4]$ with each entry rounded to $6$ decimals.\n- The program must be self-contained, must not read any input, and must print only this single line.\n\nConstraints:\n- Do not use derivatives or any external optimization routine.\n- Use only function evaluations of $f(x)$ within the golden-section search.\n- The function evaluations must stay within the closed interval $[a,b]$ for each case.", "solution": "We begin from the Euclidean distance between the point on the curve $(x,f(x))$ and the target $(x_0,y_0)$, given by $D(x)=\\sqrt{(x-x_0)^2+(f(x)-y_0)^2}$. Define the squared distance $S(x)=(x-x_0)^2+(f(x)-y_0)^2$. Because the square-root function is strictly increasing on $[0,\\infty)$, the minimizers of $D(x)$ and $S(x)$ coincide: if $x^\\ast\\in\\arg\\min_{x\\in[a,b]} S(x)$ then $x^\\ast\\in\\arg\\min_{x\\in[a,b]} D(x)$, and vice versa. Therefore, it suffices to minimize $S(x)$ for numerical convenience, since this avoids the square root while preserving the minimizer.\n\nGolden-section search is an interval reduction method for unimodal functions. Suppose $S(x)$ is continuous and unimodal on $[a,b]$. The method selects two interior points $c$ and $d$ within $[a,b]$ such that $acdb$ and the subinterval lengths are in a fixed ratio. Let the golden ratio be $\\varphi=\\frac{1+\\sqrt{5}}{2}$ and its reciprocal $\\tau=\\varphi^{-1}=\\frac{\\sqrt{5}-1}{2}$. The interior points are chosen so that\n$$\nc = b - \\tau\\,(b-a),\\quad d = a + \\tau\\,(b-a),\n$$\nwhich implies $d-a = \\tau\\,(b-a)$ and $b-c = \\tau\\,(b-a)$. The key design principle is that after discarding one subinterval based on comparing $S(c)$ and $S(d)$, one of the interior points becomes an endpoint of the new interval and the other interior point becomes the interior point of the same relative position in the new interval. This reuse of a function evaluation minimizes the number of new evaluations per iteration. The invariance property that enables reuse leads to the defining relation for the ratio parameter. Let the interval shrink from length $L$ to $\\tau L$, and require that the new interior point coincides with one of the old interior points. This yields $\\tau^2 + \\tau - 1 = 0$, whose positive solution is $\\tau = \\frac{\\sqrt{5}-1}{2}$.\n\nAt each iteration, evaluate $S(c)$ and $S(d)$. If $S(c) \\le S(d)$, then the minimizer lies in $[a,d]$, so set $b\\leftarrow d$, $d\\leftarrow c$, and compute a new $c = b - \\tau\\,(b-a)$. Otherwise, the minimizer lies in $[c,b]$, so set $a\\leftarrow c$, $c\\leftarrow d$, and compute a new $d = a + \\tau\\,(b-a)$. Continue until the interval length satisfies $b-a \\le \\varepsilon$ for a given tolerance $\\varepsilon$, or a maximum number of iterations is reached. When terminating, a conservative choice for the reported minimizer $\\hat{x}$ is the point among $\\{a,c,d,b\\}$ with the smallest observed value of $S(x)$, which is robust to cases where the true minimizer is at or near the boundary (for example, a monotone objective on the interval).\n\nFor the test cases:\n\n- Case $1$: $f(x)=x^2$, $(x_0,y_0)=(1.0,-1.0)$, $[a,b]=[-2.0,2.0]$. The function $S(x)=(x-1)^2+(x^2+1)^2$ is smooth and strictly convex, hence unimodal on $[-2.0,2.0]$. Golden-section search converges to the unique minimizer that solves the first-order condition $x-x_0 + (f(x)-y_0)f'(x)=0$, i.e., $x-1 + (x^2+1)\\cdot 2x=0$, which simplifies to $2x^3+3x-1=0$. The algorithm reliably approximates this root.\n\n- Case $2$: $f(x)=\\mathrm{e}^x$, $(x_0,y_0)=(0.0,0.5)$, $[a,b]=[-2.0,1.0]$. The objective $S(x)=x^2+(\\mathrm{e}^x-0.5)^2$ is smooth and strictly convex on the given interval since both $x^2$ and $(\\mathrm{e}^x-0.5)^2$ are convex functions and the sum of convex functions is convex. The algorithm converges to the unique minimizer, which balances the horizontal and vertical deviations.\n\n- Case $3$: $f(x)=0$, $(x_0,y_0)=(2.0,0.7)$, $[a,b]=[-1.0,1.0]$. Here $S(x)=(x-2)^2+(0-0.7)^2=(x-2)^2+0.49$, which is strictly increasing as $x$ decreases on $[-1.0,1.0]$ because its unconstrained minimizer is at $x=2.0$ outside the interval. Therefore, the constrained minimizer lies at the boundary $x=1.0$. Golden-section search, terminating by interval length, will return an approximation arbitrarily close to the right endpoint as $\\varepsilon\\to 0$.\n\n- Case $4$: $f(x)=\\log(1+x^2)$, $(x_0,y_0)=(0.5,-0.2)$, $[a,b]=[-3.0,3.0]$. The objective $S(x)=(x-0.5)^2+(\\log(1+x^2)+0.2)^2$ is smooth and strictly convex on the interval because $\\log(1+x^2)$ is smooth with bounded curvature over any compact domain, and the sum of a strictly convex quadratic and a nonnegative smooth term with nonvanishing curvature near the minimizer maintains unimodality. The algorithm converges to a unique minimizer near where the horizontal displacement and the vertical displacement trade off efficiently.\n\nAlgorithmic details to implement:\n- Define $S(x)=(x-x_0)^2+(f(x)-y_0)^2$ for each case.\n- Implement golden-section search with $\\tau=\\frac{\\sqrt{5}-1}{2}$, interior points $c=b-\\tau(b-a)$ and $d=a+\\tau(b-a)$, and updates based on comparing $S(c)$ and $S(d)$.\n- Terminate when $b-a\\le \\varepsilon$ or a maximum number of iterations is reached, and return the point among $\\{a,c,d,b\\}$ with the smallest observed $S(x)$ as $\\hat{x}$.\n- Apply this procedure to each of the four test cases, and round each $\\hat{x}$ to $6$ decimals.\n\nThe program should output a single line of the form $[\\hat{x}_1,\\hat{x}_2,\\hat{x}_3,\\hat{x}_4]$ with each $\\hat{x}_i$ rounded to $6$ decimal places, in the order of the cases listed above.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef golden_section_search(fun, a, b, tol=1e-8, max_iter=1000):\n    \"\"\"\n    Minimize a unimodal function fun on [a, b] using golden-section search.\n    Only function evaluations are used. Returns an approximate minimizer x*.\n    \"\"\"\n    phi_inv = (np.sqrt(5.0) - 1.0) / 2.0  # 1/phi\n    phi_inv_sq = (3.0 - np.sqrt(5.0)) / 2.0  # 1/phi^2\n    # Initialize interior points\n    c = a + phi_inv_sq * (b - a)\n    d = a + phi_inv * (b - a)\n    fc = fun(c)\n    fd = fun(d)\n\n    it = 0\n    while (b - a)  tol and it  max_iter:\n        it += 1\n        if fc = fd:\n            # Minimum is in [a, d]\n            b, d, fd = d, c, fc\n            c = a + phi_inv_sq * (b - a)\n            fc = fun(c)\n        else:\n            # Minimum is in [c, b]\n            a, c, fc = c, d, fd\n            d = a + phi_inv * (b - a)\n            fd = fun(d)\n\n    # Pick the best among available sampled points to be robust to boundary minima\n    candidates = [(a, fun(a)), (b, fun(b)), (c, fc), (d, fd)]\n    x_best, _ = min(candidates, key=lambda t: t[1])\n    return x_best\n\n# Define the functions f(x) for the test suite\ndef f_quad(x):\n    return x * x\n\ndef f_exp(x):\n    return np.exp(x)\n\ndef f_const0(x):\n    return 0.0\n\ndef f_log1px2(x):\n    # Use numerically stable log1p\n    return np.log1p(x * x)\n\ndef make_squared_distance(f, x0, y0):\n    def S(x):\n        y = f(x)\n        dx = x - x0\n        dy = y - y0\n        return dx * dx + dy * dy\n    return S\n\ndef solve():\n    # Define the test cases as (function, x0, y0, a, b, tol)\n    test_cases = [\n        (f_quad, 1.0, -1.0, -2.0,  2.0, 1e-8),      # Case 1\n        (f_exp,  0.0,  0.5, -2.0,  1.0, 1e-8),      # Case 2\n        (f_const0, 2.0, 0.7, -1.0, 1.0, 1e-8),      # Case 3\n        (f_log1px2, 0.5, -0.2, -3.0, 3.0, 1e-8),    # Case 4\n    ]\n\n    results = []\n    for f, x0, y0, a, b, tol in test_cases:\n        S = make_squared_distance(f, x0, y0)\n        x_star = golden_section_search(S, a, b, tol=tol, max_iter=10000)\n        results.append(f\"{x_star:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2398578"}, {"introduction": "Knowing an algorithm's limitations is as crucial as knowing its strengths. This practice explores a common but potentially flawed technique: solving an equation by minimizing its squared residual. You will analyze why this transformation can create 'spurious' local minima that are not solutions to the original problem, providing a critical lesson on the importance of ensuring the unimodality assumption holds before applying a method like the golden-section search. [@problem_id:2421149]", "problem": "An engineer in computational engineering needs to solve the scalar equation $g(x)=c$ over a closed interval $[a,b]$. To do so, the engineer proposes to minimize the squared residual $f(x)=(g(x)-c)^2$ on $[a,b]$ using Golden-Section Search (GSS). Assume $g$ is continuous on $[a,b]$ and at least once differentiable on $(a,b)$. Select all statements that are correct about the validity of this approach and its pitfalls related to local minima.\n\nA. If $g$ is continuous and strictly monotone on $[a,b]$ and $c\\in g([a,b])$, then $f(x)$ is unimodal on $[a,b]$ with a unique global minimizer at the unique solution of $g(x)=c$, so applying GSS to $f$ on any bracket $[a,b]$ that contains that solution will return a root.\n\nB. The only stationary points of $f(x)$ are the solutions of $g(x)=c$.\n\nC. If $g$ is non-monotone on $[a,b]$, then $f(x)$ can have local minima at points where $g'(x)=0$ and $g(x)\\neq c$, so GSS run on $f$ over such a bracket can converge to a non-root local minimizer depending on the initial bracket.\n\nD. If $c\\notin g([a,b])$, then minimizing $f$ with GSS on $[a,b]$ will not produce a root of $g(x)=c$; instead, it will converge to a point in $[a,b]$ (possibly an endpoint) with nonzero residual.\n\nE. Squaring the residual makes $f(x)$ convex on $[a,b]$ for any differentiable $g$, so no spurious local minima exist and GSS will always find a root if one exists anywhere on the real line.", "solution": "The problem statement is evaluated for validity.\n\n**Step 1: Extract Givens**\n- Equation to solve: $g(x) = c$\n- Domain: Closed interval $[a, b]$\n- Proposed method: Minimize the function $f(x) = (g(x) - c)^2$\n- Optimization algorithm: Golden-Section Search (GSS)\n- Properties of $g(x)$:\n    - Continuous on $[a, b]$\n    - At least once differentiable on $(a, b)$\n- Task: Evaluate statements about the validity and pitfalls of this approach.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem describes a standard technique in numerical analysis: reformulating a root-finding problem as a minimization problem. The functions, concepts (continuity, differentiability, monotonicity, unimodality), and algorithm (GSS) are well-defined within mathematics and computational engineering. The approach is scientifically valid.\n- **Well-Posed:** The problem provides sufficient information to analyze the mathematical properties of the function $f(x)$ based on the given properties of $g(x)$. The questions posed in the options are precise and can be answered with mathematical rigor.\n- **Objective:** The problem and the statements to be evaluated are objective and devoid of subjective claims. Their correctness can be determined through mathematical proof or counterexample.\n\nThe problem does not violate any of the invalidity criteria. It is scientifically sound, well-posed, objective, and formalizable.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A full solution will be derived.\n\n**Derivation of Core Principles**\n\nThe problem proposes to find a root of $g(x) = c$ by minimizing the squared residual function $f(x) = (g(x) - c)^2$. The global minima of $f(x)$ occur where $f(x) = 0$, which is true if and only if $g(x) = c$. Therefore, the global minimizers of $f(x)$ are precisely the roots of $g(x) = c$.\n\nGolden-Section Search (GSS) is an algorithm guaranteed to find the minimum of a function over an interval only if the function is **unimodal** on that interval. A function is unimodal on an interval if it has exactly one local minimum in that interval. If a function has multiple local minima (i.e., is not unimodal), GSS may converge to a local minimum that is not the global minimum, depending on the initial search bracket.\n\nTo understand the behavior of GSS on $f(x)$, we must analyze the stationary points of $f(x)$, which determine its local minima and maxima. Using the chain rule, the first derivative of $f(x)$ is:\n$$ f'(x) = 2(g(x) - c) \\cdot g'(x) $$\nThe stationary points of $f(x)$ are the values of $x$ for which $f'(x) = 0$. This occurs if and only if:\n$$ g(x) - c = 0 \\quad \\text{or} \\quad g'(x) = 0 $$\nThis shows that the stationary points of $f(x)$ consist of two sets:\n$1$. The roots of $g(x) = c$.\n$2$. The stationary points of $g(x)$ itself.\n\nThese latter points, where $g'(x)=0$ but $g(x) \\neq c$, can introduce \"spurious\" local minima in $f(x)$, which are not roots of the original equation.\n\nTo determine if a stationary point $x_0$ is a local minimum, we can examine the second derivative, $f''(x_0)$:\n$$ f''(x) = 2(g'(x))^2 + 2(g(x) - c)g''(x) $$\nAt a stationary point $x_0$ where $g'(x_0) = 0$ and $g(x_0) \\neq c$, the second derivative simplifies to:\n$$ f''(x_0) = 2(g(x_0) - c)g''(x_0) $$\nFor $x_0$ to be a local minimum of $f$, we require $f''(x_0)  0$. This condition can be met, as shown in the analysis of option C.\n\n**Option-by-Option Analysis**\n\n**A. If $g$ is continuous and strictly monotone on $[a,b]$ and $c\\in g([a,b])$, then $f(x)$ is unimodal on $[a,b]$ with a unique global minimizer at the unique solution of $g(x)=c$, so applying GSS to $f$ on any bracket $[a,b]$ that contains that solution will return a root.**\n\nIf $g(x)$ is continuous and strictly monotone on $[a,b]$, and $c$ is in the range of $g$ on this interval, the Intermediate Value Theorem guarantees there is a unique solution $x^*$ in $[a,b]$ such that $g(x^*) = c$. At this point, $f(x^*) = (g(x^*) - c)^2 = 0$. Since $f(x) \\ge 0$ for all $x$, $x^*$ is a global minimizer.\n\nBecause $g(x)$ is strictly monotone, $g'(x)$ does not change sign on $(a, b)$ and is non-zero (except possibly at isolated points that do not constitute local extrema of $g$). The only way for $f'(x) = 2(g(x) - c)g'(x)$ to be zero is if $g(x) - c = 0$, which happens only at $x = x^*$. Thus, $f(x)$ has only one stationary point in the interval.\nLet's assume $g(x)$ is strictly increasing, so $g'(x)  0$.\n- For $x  x^*$, $g(x)  g(x^*) = c$, so $g(x) - c  0$. Thus, $f'(x) = 2(\\text{negative})(\\text{positive})  0$.\n- For $x  x^*$, $g(x)  g(x^*) = c$, so $g(x) - c  0$. Thus, $f'(x) = 2(\\text{positive})(\\text{positive})  0$.\nThis shows $f(x)$ is decreasing for $x  x^*$ and increasing for $x  x^*$. Therefore, $f(x)$ is unimodal on $[a,b]$ with its unique minimum at $x^*$. GSS is designed for unimodal functions and will correctly converge to this unique minimizer.\nVerdict: **Correct**.\n\n**B. The only stationary points of $f(x)$ are the solutions of $g(x)=c$.**\n\nAs derived above, the stationary points of $f(x)$ occur when $f'(x) = 2(g(x) - c) g'(x) = 0$. This equation is satisfied when $g(x) = c$ OR when $g'(x) = 0$. If there exists a point $x_0$ where $g(x)$ has a local extremum (so $g'(x_0) = 0$) and $g(x_0) \\neq c$, then $x_0$ is a stationary point of $f(x)$ but not a solution to $g(x) = c$. For example, let $g(x) = x^2$ and $c=4$. The solutions are $x=\\pm 2$. The function to minimize is $f(x) = (x^2 - 4)^2$. The derivative is $f'(x) = 2(x^2 - 4)(2x) = 4x(x-2)(x+2)$. The stationary points are $x=0$, $x=2$, and $x=-2$. The point $x=0$ is a stationary point of $f(x)$ because $g'(0)=0$, but $g(0) = 0 \\neq 4$, so it is not a solution.\nVerdict: **Incorrect**.\n\n**C. If $g$ is non-monotone on $[a,b]$, then $f(x)$ can have local minima at points where $g'(x)=0$ and $g(x)\\neq c$, so GSS run on $f$ over such a bracket can converge to a non-root local minimizer depending on the initial bracket.**\n\nIf $g(x)$ is non-monotone, there must be at least one point $x_0 \\in (a,b)$ where $g'(x_0)=0$. This point is a stationary point of $f(x)$. We must check if it can be a local minimum. As shown in the general derivation, $f''(x_0) = 2(g(x_0) - c)g''(x_0)$. We can make this positive.\nConsider $g(x) = x^3 - 4x$. Then $g'(x) = 3x^2 - 4$, which is zero at $x_0 = \\pm 2/\\sqrt{3}$. Let's pick $x_0 = 2/\\sqrt{3}$. At this point, $g(x)$ a local minimum: $g(2/\\sqrt{3}) = (8/3\\sqrt{3}) - (8/\\sqrt{3}) = -16/(3\\sqrt{3})$ and $g''(x) = 6x$, so $g''(2/\\sqrt{3}) = 12/\\sqrt{3}  0$.\nLet's find a root for $c  g(x_0)$. For example, let $c = -6$. The equation is $x^3-4x = -6$. The function to minimize is $f(x)=(x^3-4x+6)^2$. A real root $x^*$ exists (e.g., $g(-3)=-15$, $g(-2)=0$, so root is between $-3$ and $-2$).\nAt the stationary point $x_0=2/\\sqrt{3}$ of $g(x)$, we have $g(x_0) = -16/(3\\sqrt{3}) \\approx -3.078$, which is not equal to $c=-6$. The second derivative of $f(x)$ at $x_0$ is $f''(x_0) = 2(g(x_0)-c)g''(x_0) = 2(-16/(3\\sqrt{3}) - (-6)) (12/\\sqrt{3}) = 2(-3.078+6)(6.928)  0$.\nSo, $x_0 = 2/\\sqrt{3}$ is a local minimum of $f(x)$. Since $f(x_0) = (g(x_0)-c)^2  0$ while the global minimum is $f(x^*)=0$, $f(x)$ is not unimodal. GSS started on an interval containing $x_0$ but not $x^*$ (e.g., $[0,2]$) could converge to the local minimizer $x_0$, which is not a root of $g(x)=c$.\nVerdict: **Correct**.\n\n**D. If $c\\notin g([a,b])$, then minimizing $f$ with GSS on $[a,b]$ will not produce a root of $g(x)=c$; instead, it will converge to a point in $[a,b]$ (possibly an endpoint) with nonzero residual.**\n\nThe premise is that $c$ is not in the range of $g(x)$ on the interval $[a,b]$. This means there is no $x \\in [a,b]$ for which $g(x) = c$. Thus, no root exists in the interval. The function $f(x) = (g(x) - c)^2$ is therefore strictly positive for all $x \\in [a,b]$. As a continuous function on a compact set $[a,b]$, $f(x)$ must attain a global minimum value on this interval, and this minimum value will be greater than zero. The point $x_{min}$ where this minimum occurs represents the best \"least-squares\" approximation to a solution in $[a,b]$.\nSince no root exists, GSS cannot produce a root. What it will do is search for a minimum of $f(x)$. GSS iteratively narrows the search interval. Assuming it converges (which it will, as the interval shrinks), it will converge to a point corresponding to a minimum of $f(x)$ within the initial bracket. Since the minimum value of $f(x)$ is strictly positive, the point found will have a non-zero residual, i.e., $f(x_{min})  0$. Such a minimum can occur at an endpoint if, for example, $f(x)$ is monotonic on $[a,b]$. The statement accurately describes that the outcome is a least-squares solution, not a root.\nVerdict: **Correct**.\n\n**E. Squaring the residual makes $f(x)$ convex on $[a,b]$ for any differentiable $g$, so no spurious local minima exist and GSS will always find a root if one exists anywhere on the real line.**\n\nThis statement makes several incorrect claims.\n$1$. **Convexity**: $f(x)$ is not generally convex. As shown above, $f''(x) = 2(g'(x))^2 + 2(g(x) - c)g''(x)$. The second term, $2(g(x) - c)g''(x)$, can be negative and large enough to make $f''(x)  0$. For a counterexample, let $g(x) = \\sin(x)$ and $c=0$. Then $f(x) = \\sin^2(x)$. $f''(x) = 2\\cos(2x)$, which is negative for $x \\in (\\pi/4, 3\\pi/4)$, so $f(x)$ is not convex on this interval.\n$2$. **No spurious local minima**: As demonstrated for option C, spurious (non-root) local minima can and do exist when $g(x)$ is non-monotone. Convexity is a sufficient (but not necessary) condition for unimodality. Since $f(x)$ is not always convex, the claim that no spurious minima exist is false.\n$3$. **Always find a root anywhere**: GSS is a local search method restricted to its initial search bracket $[a, b]$. It has no mechanism to find roots that lie outside this bracket.\nEach part of this statement is false.\nVerdict: **Incorrect**.", "answer": "$$\\boxed{ACD}$$", "id": "2421149"}]}