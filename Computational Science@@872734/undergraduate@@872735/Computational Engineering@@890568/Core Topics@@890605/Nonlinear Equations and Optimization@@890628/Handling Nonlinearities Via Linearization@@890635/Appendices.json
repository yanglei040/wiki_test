{"hands_on_practices": [{"introduction": "The cornerstone of linearizing any nonlinear multivariate function is its Jacobian matrix, which provides the best linear approximation of the function's behavior around a specific point. This first practice provides a crucial, hands-on comparison between two methods for computing this matrix: a numerically exact technique known as forward-mode automatic differentiation and the widely used finite-difference approximation. By implementing both, you will gain a deep, practical understanding of the trade-offs involving accuracy, stability, and computational cost that are central to computational engineering [@problem_id:2398904].", "problem": "You are asked to study how linearization is used to handle nonlinearities by computing the Jacobian matrix of a nonlinear composed mapping in two ways and comparing their numerical precision. Consider the composed function defined by the following components. Let $\\mathbf{x} \\in \\mathbb{R}^{3}$ with components $\\mathbf{x} = [x_{1}, x_{2}, x_{3}]^{\\top}$. Define an intermediate mapping $\\mathbf{g}: \\mathbb{R}^{3} \\rightarrow \\mathbb{R}^{3}$ by\n$$\n\\mathbf{g}(\\mathbf{x}) =\n\\begin{bmatrix}\n\\exp\\!\\big(x_{1} x_{2}\\big) \\\\\n\\sin\\!\\big(x_{2} + x_{3}\\big) \\\\\nx_{1}^{2} + x_{3}\n\\end{bmatrix},\n$$\nand a second mapping $\\mathbf{h}: \\mathbb{R}^{3} \\rightarrow \\mathbb{R}^{2}$ by\n$$\n\\mathbf{h}(\\mathbf{u}) =\n\\begin{bmatrix}\nu_{1} \\cos(u_{2}) + u_{3}^{3} \\\\\n\\ln\\!\\big(1 + u_{1}^{2} + u_{2}^{2}\\big) + \\tanh(u_{3})\n\\end{bmatrix}.\n$$\nAngles for the trigonometric functions $\\sin(\\cdot)$ and $\\cos(\\cdot)$ are to be interpreted in radians. The composed function is $\\mathbf{F} = \\mathbf{h} \\circ \\mathbf{g}: \\mathbb{R}^{3} \\rightarrow \\mathbb{R}^{2}$.\n\nYour task is to compute the Jacobian matrix $\\mathbf{J}_{\\mathbf{F}}(\\mathbf{x}) \\in \\mathbb{R}^{2 \\times 3}$ at several inputs $\\mathbf{x}$ using both:\n- a forward-mode automatic differentiation approach based on dual numbers, and\n- a central finite-difference approximation with two step sizes.\n\nThe Jacobian $\\mathbf{J}_{\\mathbf{F}}(\\mathbf{x})$ is defined by the entries $[\\mathbf{J}_{\\mathbf{F}}(\\mathbf{x})]_{ij} = \\partial F_{i}(\\mathbf{x}) / \\partial x_{j}$. Linearization refers to the first-order approximation of $\\mathbf{F}$ near $\\mathbf{x}$ using this Jacobian.\n\nFor the forward-mode automatic differentiation method, you must implement dual numbers representing pairs $(v, \\dot{v})$ that propagate values and directional derivatives through elementary operations and functions according to first principles (the product rule, the chain rule, and standard derivatives of elementary functions). For the finite difference method, approximate the $j$-th column of the Jacobian using the central difference formula\n$$\n\\frac{\\mathbf{F}(\\mathbf{x} + h \\mathbf{e}_{j}) - \\mathbf{F}(\\mathbf{x} - h \\mathbf{e}_{j})}{2 h},\n$$\nwhere $\\mathbf{e}_{j}$ is the $j$-th canonical basis vector in $\\mathbb{R}^{3}$, and $h$ is a positive scalar step size.\n\nTo quantify the precision difference, compute for each test input and each step size the relative Frobenius-norm error between the finite-difference Jacobian $\\mathbf{J}_{\\mathrm{fd}}$ and the automatic differentiation Jacobian $\\mathbf{J}_{\\mathrm{ad}}$:\n$$\n\\varepsilon_{\\mathrm{rel}} = \\frac{\\left\\| \\mathbf{J}_{\\mathrm{fd}} - \\mathbf{J}_{\\mathrm{ad}} \\right\\|_{F}}{\\max\\!\\left(1, \\left\\| \\mathbf{J}_{\\mathrm{ad}} \\right\\|_{F}\\right)},\n$$\nwhere $\\|\\cdot\\|_{F}$ denotes the Frobenius norm. This normalization avoids division by values smaller than $1$ and keeps the metric well-scaled when the Jacobian norm is very small.\n\nTest suite:\n- Use the following four input vectors $\\mathbf{x}$ (dimensionless):\n  - $\\mathbf{x}^{(1)} = [0.2, -0.3, 0.5]^{\\top}$,\n  - $\\mathbf{x}^{(2)} = [10^{-8}, -10^{-8}, 10^{-8}]^{\\top}$,\n  - $\\mathbf{x}^{(3)} = [1.5, 0.7, -1.2]^{\\top}$,\n  - $\\mathbf{x}^{(4)} = [-2.0, 0.4, 0.3]^{\\top}$.\n- For the central differences, use two step sizes $h$:\n  - $h_{1} = 10^{-6}$,\n  - $h_{2} = 10^{-8}$.\n\nProgram requirements:\n- Implement the dual-number forward-mode automatic differentiation from first principles for the elementary operations and functions appearing in $\\mathbf{F}$.\n- Implement the central finite-difference Jacobian approximation as specified.\n- For each test input $\\mathbf{x}^{(k)}$, compute two values: $\\varepsilon_{\\mathrm{rel}}$ with $h = h_{1}$ and with $h = h_{2}$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets with no spaces. The list must contain the eight floating-point values in the following order:\n  - $\\varepsilon_{\\mathrm{rel}}$ for $\\mathbf{x}^{(1)}$ with $h_{1}$, then with $h_{2}$,\n  - $\\varepsilon_{\\mathrm{rel}}$ for $\\mathbf{x}^{(2)}$ with $h_{1}$, then with $h_{2}$,\n  - $\\varepsilon_{\\mathrm{rel}}$ for $\\mathbf{x}^{(3)}$ with $h_{1}$, then with $h_{2}$,\n  - $\\varepsilon_{\\mathrm{rel}}$ for $\\mathbf{x}^{(4)}$ with $h_{1}$, then with $h_{2}$.\nFor example, the output must look like\n$$\n[\\varepsilon_{1,1},\\varepsilon_{1,2},\\varepsilon_{2,1},\\varepsilon_{2,2},\\varepsilon_{3,1},\\varepsilon_{3,2},\\varepsilon_{4,1},\\varepsilon_{4,2}],\n$$\nwhere each $\\varepsilon_{k,\\ell}$ is a floating-point number. No additional text should be printed.", "solution": "The core task is to compute the Jacobian matrix $\\mathbf{J}_{\\mathbf{F}}(\\mathbf{x})$ of a composed nonlinear function $\\mathbf{F} = \\mathbf{h} \\circ \\mathbf{g}: \\mathbb{R}^{3} \\rightarrow \\mathbb{R}^{2}$ using two distinct methods and to quantify their numerical precision. The methods are forward-mode automatic differentiation (AD) and central finite differences (FD). The AD result, being accurate to machine precision, will serve as the reference against which the FD approximation is compared.\n\n### 1. Analytical Framework\n\nThe composed function is $\\mathbf{F}(\\mathbf{x}) = \\mathbf{h}(\\mathbf{g}(\\mathbf{x}))$, where $\\mathbf{g}: \\mathbb{R}^{3} \\rightarrow \\mathbb{R}^{3}$ and $\\mathbf{h}: \\mathbb{R}^{3} \\rightarrow \\mathbb{R}^{2}$.\n$$\n\\mathbf{g}(\\mathbf{x}) =\n\\begin{bmatrix}\ng_{1}(\\mathbf{x}) \\\\ g_{2}(\\mathbf{x}) \\\\ g_{3}(\\mathbf{x})\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\exp\\!\\big(x_{1} x_{2}\\big) \\\\\n\\sin\\!\\big(x_{2} + x_{3}\\big) \\\\\nx_{1}^{2} + x_{3}\n\\end{bmatrix}\n$$\n$$\n\\mathbf{h}(\\mathbf{u}) =\n\\begin{bmatrix}\nh_{1}(\\mathbf{u}) \\\\ h_{2}(\\mathbf{u})\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nu_{1} \\cos(u_{2}) + u_{3}^{3} \\\\\n\\ln\\!\\big(1 + u_{1}^{2} + u_{2}^{2}\\big) + \\tanh(u_{3})\n\\end{bmatrix}\n$$\nThe Jacobian of the composed function is given by the multivariate chain rule:\n$$\n\\mathbf{J}_{\\mathbf{F}}(\\mathbf{x}) = \\mathbf{J}_{\\mathbf{h}}(\\mathbf{g}(\\mathbf{x})) \\cdot \\mathbf{J}_{\\mathbf{g}}(\\mathbf{x})\n$$\nwhere $\\mathbf{J}_{\\mathbf{h}} \\in \\mathbb{R}^{2 \\times 3}$ and $\\mathbf{J}_{\\mathbf{g}} \\in \\mathbb{R}^{3 \\times 3}$.\n\n### 2. Forward-Mode Automatic Differentiation (AD)\n\nForward-mode AD computes exact derivatives by systematically applying the chain rule at the level of elementary operations. This is achieved through the algebra of dual numbers. A dual number is an ordered pair $(v, \\dot{v})$, representing a value $v$ and its directional derivative $\\dot{v}$. It can be written as $v + \\epsilon \\dot{v}$, where $\\epsilon$ is an infinitesimal with the property $\\epsilon^2 = 0$.\n\nThe rules for arithmetic and elementary functions are derived from the standard rules of differentiation:\n- **Sum Rule**: $(u \\pm w)' = u' \\pm w'$ $\\implies$ $(u_{v}, u_{\\dot{v}}) \\pm (w_{v}, w_{\\dot{v}}) = (u_{v} \\pm w_{v}, u_{\\dot{v}} \\pm w_{\\dot{v}})$\n- **Product Rule**: $(u w)' = u'w + uw'$ $\\implies$ $(u_{v}, u_{\\dot{v}}) \\cdot (w_{v}, w_{\\dot{v}}) = (u_{v} w_{v}, u_{\\dot{v}} w_{v} + u_{v} w_{\\dot{v}})$\n- **Chain Rule**: $(f(u))' = f'(u) u'$ $\\implies$ $f((u_{v}, u_{\\dot{v}})) = (f(u_{v}), f'(u_{v}) u_{\\dot{v}})$\n\nTo compute the $j$-th column of the Jacobian $\\mathbf{J}_{\\mathbf{F}}(\\mathbf{x})$, which contains the partial derivatives with respect to $x_j$, we compute the directional derivative of $\\mathbf{F}$ in the direction of the canonical basis vector $\\mathbf{e}_j$. This is done by setting the input derivatives (the \"seed\") to $\\dot{\\mathbf{x}} = \\mathbf{e}_j$.\n\nThe algorithm consists of the following steps for each column $j = 1, 2, 3$:\n1.  Initialize the input vector $\\mathbf{x}$ as a vector of dual numbers, where $x_{i}$ becomes $(x_{i}, \\delta_{ij})$, with $\\delta_{ij}$ being the Kronecker delta.\n2.  Evaluate the function $\\mathbf{F}$ using dual number arithmetic. The evaluation propagates the values and their derivatives through the composition $\\mathbf{h} \\circ \\mathbf{g}$.\n3.  The result is a vector of dual numbers, $\\mathbf{F}(\\mathbf{x}_{\\text{dual}}) = [(F_{1}, \\dot{F}_{1}), (F_{2}, \\dot{F}_{2})]^{\\top}$. The derivative components $(\\dot{F}_{1}, \\dot{F}_{2})^{\\top}$ form the $j$-th column of the Jacobian: $[\\mathbf{J}_{\\mathbf{F}}]_{ij} = \\dot{F}_i$.\n\nThis process is repeated for each input variable to construct the full Jacobian matrix $\\mathbf{J}_{\\mathrm{ad}}$.\n\n### 3. Central Finite-Difference (FD) Approximation\n\nThe finite difference method approximates derivatives by evaluating the function at perturbed points. The central difference formula for the $j$-th column of the Jacobian is given by:\n$$\n(\\mathbf{J}_{\\mathrm{fd}})_{:,j} = \\frac{\\mathbf{F}(\\mathbf{x} + h \\mathbf{e}_{j}) - \\mathbf{F}(\\mathbf{x} - h \\mathbf{e}_{j})}{2 h}\n$$\nwhere $h$ is a small step size. This formula is derived from the Taylor series expansion of $\\mathbf{F}(\\mathbf{x} \\pm h \\mathbf{e}_{j})$ around $\\mathbf{x}$. The truncation error of this approximation is of order $O(h^2)$, meaning the error is proportional to the square of the step size.\n\nHowever, the total error is a combination of this truncation error and the round-off error from floating-point arithmetic. Round-off error in the numerator's subtraction increases as $h$ decreases, leading to a loss of significance. This creates a trade-off: a smaller $h$ reduces truncation error but increases round-off error. We will investigate this by using two different step sizes, $h_1 = 10^{-6}$ and $h_2 = 10^{-8}$.\n\n### 4. Error Quantification\n\nTo compare the precision of the finite-difference method against the automatic differentiation result, we compute the relative Frobenius-norm error:\n$$\n\\varepsilon_{\\mathrm{rel}} = \\frac{\\left\\| \\mathbf{J}_{\\mathrm{fd}} - \\mathbf{J}_{\\mathrm{ad}} \\right\\|_{F}}{\\max\\!\\left(1, \\left\\| \\mathbf{J}_{\\mathrm{ad}} \\right\\|_{F}\\right)}\n$$\nThe Frobenius norm is defined as $\\|\\mathbf{A}\\|_{F} = \\sqrt{\\sum_{i,j} |A_{ij}|^2}$. The normalization by $\\max(1, \\left\\| \\mathbf{J}_{\\mathrm{ad}} \\right\\|_{F})$ provides a stable error metric, preventing division by values near zero while scaling the error relative to the magnitude of the \"true\" Jacobian.\n\n### 5. Implementation Strategy\n\nThe solution is implemented in Python. A class `Dual` is defined to represent dual numbers and to overload standard arithmetic operators. Global functions for elementary mathematical operations (`exp`, `sin`, `cos`, `ln`, `tanh`) are implemented to be polymorphic, handling both standard floating-point numbers and `Dual` objects. The problem-specific functions $\\mathbf{g}$ and $\\mathbf{h}$ are then written using these polymorphic operations, allowing them to be used seamlessly in both AD and FD computations. The main program iterates through the specified test inputs and step sizes, computing the Jacobians via both methods and calculating the relative error for each case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# A strict Russian professor insists on scientific correctness from first principles.\n\nclass Dual:\n    \"\"\"\n    Represents a dual number for forward-mode automatic differentiation.\n    A dual number has a real part (value) and an infinitesimal part (derivative).\n    d = value + derivative * epsilon, where epsilon^2 = 0.\n    \"\"\"\n    def __init__(self, value, derivative=0.0):\n        self.value = float(value)\n        self.derivative = float(derivative)\n\n    def __repr__(self):\n        return f\"Dual({self.value}, {self.derivative})\"\n\n    def __add__(self, other):\n        if isinstance(other, Dual):\n            return Dual(self.value + other.value, self.derivative + other.derivative)\n        return Dual(self.value + other, self.derivative)\n\n    def __radd__(self, other):\n        return self.__add__(other)\n\n    def __sub__(self, other):\n        if isinstance(other, Dual):\n            return Dual(self.value - other.value, self.derivative - other.derivative)\n        return Dual(self.value - other, self.derivative)\n\n    def __rsub__(self, other):\n        return Dual(other - self.value, -self.derivative)\n\n    def __mul__(self, other):\n        if isinstance(other, Dual):\n            # Product rule: (uv)' = u'v + uv'\n            return Dual(self.value * other.value, \n                        self.derivative * other.value + self.value * other.derivative)\n        return Dual(self.value * other, self.derivative * other)\n\n    def __rmul__(self, other):\n        return self.__mul__(other)\n    \n    def __pow__(self, exponent):\n        if isinstance(exponent, (int, float)):\n            # Power rule for constant exponent: (u^c)' = c*u^(c-1)*u'\n            val = self.value ** exponent\n            der = exponent * (self.value ** (exponent - 1)) * self.derivative\n            return Dual(val, der)\n        raise NotImplementedError(\"Dual to the power of Dual is not implemented.\")\n\n    def __truediv__(self, other):\n        if isinstance(other, Dual):\n            # Quotient rule: (u/v)' = (u'v - uv') / v^2\n            val = self.value / other.value\n            der = (self.derivative * other.value - self.value * other.derivative) / (other.value ** 2)\n            return Dual(val, der)\n        return Dual(self.value / other, self.derivative / other)\n\n    def __rtruediv__(self, other):\n        val = other / self.value\n        der = (-other * self.derivative) / (self.value ** 2)\n        return Dual(val, der)\n    \n    def __neg__(self):\n        return Dual(-self.value, -self.derivative)\n\n# Polymorphic elementary functions that work with both floats and Dual numbers.\ndef exp(d):\n    if not isinstance(d, Dual): return np.exp(d)\n    val = np.exp(d.value)\n    der = val * d.derivative\n    return Dual(val, der)\n\ndef sin(d):\n    if not isinstance(d, Dual): return np.sin(d)\n    val = np.sin(d.value)\n    der = np.cos(d.value) * d.derivative\n    return Dual(val, der)\n\ndef cos(d):\n    if not isinstance(d, Dual): return np.cos(d)\n    val = np.cos(d.value)\n    der = -np.sin(d.value) * d.derivative\n    return Dual(val, der)\n\ndef log(d):\n    if not isinstance(d, Dual): return np.log(d)\n    val = np.log(d.value)\n    der = (1 / d.value) * d.derivative\n    return Dual(val, der)\n\ndef tanh(d):\n    if not isinstance(d, Dual): return np.tanh(d)\n    val = np.tanh(d.value)\n    # Derivative of tanh(x) is sech^2(x) = 1 - tanh^2(x)\n    der = (1 - val**2) * d.derivative\n    return Dual(val, der)\n\n# Problem-specific nonlinear mappings\ndef g(x_vec):\n    \"\"\" Intermediate mapping g: R^3 -> R^3 \"\"\"\n    x1, x2, x3 = x_vec[0], x_vec[1], x_vec[2]\n    return [\n        exp(x1 * x2),\n        sin(x2 + x3),\n        x1**2 + x3\n    ]\n\ndef h(u_vec):\n    \"\"\" Final mapping h: R^3 -> R^2 \"\"\"\n    u1, u2, u3 = u_vec[0], u_vec[1], u_vec[2]\n    return [\n        u1 * cos(u2) + u3**3,\n        log(1 + u1**2 + u2**2) + tanh(u3)\n    ]\n\ndef F(x_vec):\n    \"\"\" Composed mapping F = h(g(x)) \"\"\"\n    return h(g(x_vec))\n\ndef compute_jacobian_ad(x_val):\n    \"\"\"Computes the Jacobian using forward-mode automatic differentiation.\"\"\"\n    n_in = len(x_val)\n    # Dynamically determine output dimension by a sample evaluation\n    n_out = len(F(x_val))\n    J_ad = np.zeros((n_out, n_in))\n    \n    for j in range(n_in):\n        # Create dual number inputs with a seed for the j-th partial derivative\n        x_dual = [Dual(x_val[i], 1.0 if i == j else 0.0) for i in range(n_in)]\n        \n        # Evaluate the function with dual numbers\n        F_dual = F(x_dual)\n        \n        # The derivative part of the output is the j-th column of the Jacobian\n        for i in range(n_out):\n            J_ad[i, j] = F_dual[i].derivative\n            \n    return J_ad\n\ndef compute_jacobian_fd(x_val, h_step):\n    \"\"\"Computes the Jacobian using the central finite-difference formula.\"\"\"\n    x_val = np.array(x_val, dtype=float)\n    n_in = len(x_val)\n    n_out = len(F(x_val))\n    J_fd = np.zeros((n_out, n_in))\n    \n    for j in range(n_in):\n        e_j = np.zeros(n_in)\n        e_j[j] = 1.0\n        \n        x_fwd = x_val + h_step * e_j\n        x_bwd = x_val - h_step * e_j\n        \n        F_fwd = np.array(F(x_fwd))\n        F_bwd = np.array(F(x_bwd))\n        \n        column = (F_fwd - F_bwd) / (2 * h_step)\n        J_fd[:, j] = column\n        \n    return J_fd\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        [0.2, -0.3, 0.5],\n        [1e-8, -1e-8, 1e-8],\n        [1.5, 0.7, -1.2],\n        [-2.0, 0.4, 0.3]\n    ]\n    step_sizes = [1e-6, 1e-8]\n    \n    results = []\n    for x_vec in test_cases:\n        # Compute the \"exact\" Jacobian using Automatic Differentiation\n        J_ad = compute_jacobian_ad(x_vec)\n        norm_J_ad = np.linalg.norm(J_ad, 'fro')\n\n        for h in step_sizes:\n            # Compute the approximate Jacobian using Finite Differences\n            J_fd = compute_jacobian_fd(x_vec, h)\n            \n            # Calculate the relative Frobenius-norm error\n            norm_diff = np.linalg.norm(J_fd - J_ad, 'fro')\n            error = norm_diff / max(1.0, norm_J_ad)\n            results.append(error)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2398904"}, {"introduction": "With a solid grasp of how to compute a linear approximation, we can now use it to solve nonlinear problems. This exercise demonstrates the power of Newton's method, a classic algorithm that iteratively solves a sequence of linear approximations to find the root of a nonlinear equation. You will apply this technique in the exciting context of cryptanalysis, where a hypothetical side-channel leak provides a nonlinear constraint on the prime factors of an RSA modulus, allowing you to break the key by solving for a factor $p$ [@problem_id:2398877].", "problem": "A public-key cryptosystem uses the Rivest–Shamir–Adleman (RSA) modulus defined by $N = p\\,q$, where $p$ and $q$ are unknown primes with $p \\le q$. A side-channel measurement reveals a nonlinear algebraic constraint between the prime factors: the squared-sum $p^2 + q^2$ equals a leaked value $L$. Your task is to compute $p$ by solving a single-variable nonlinear equation derived from the definitions and to implement a Newton linearization procedure to find the root.\n\nFundamental base:\n- By definition of the modulus, $N = p\\,q$.\n- The leak provides $p^2 + q^2 = L$.\n- The goal is to eliminate $q$ and solve for $p$ as the positive root of a scalar nonlinear equation, using Newton’s method (first-order Taylor linearization) with an update based on the derivative of the scalar function with respect to the scalar iterate.\n\nAlgorithmic requirements:\n- Eliminate $q$ using the definition of $N$ to obtain a univariate equation in the unknown $p$ that is consistent with the leak $p^2 + q^2 = L$.\n- Define a continuously differentiable scalar function $f(x)$ such that the true $p$ is a simple root of $f(x) = 0$.\n- Derive the exact derivative $f'(x)$ from first principles (elementary calculus) to support Newton’s method.\n- Implement Newton’s method with the iterate $x_{\\text{new}} = x - f(x)/f'(x)$.\n- Use a strictly positive initial guess $x_0$ equal to $\\sqrt{N}$.\n- Enforce positivity of the iterate and provide a reasonable stopping rule based on the change in $x$ or the residual of $f$ (whichever you prefer). Use a finite maximum iteration count to guarantee termination.\n- After convergence, map the numerical root to the integer factor by rounding only as the final step, and return the smaller prime factor. If the Newton estimate is closer to $q$, use the relation $q = N/p$ to still return the smaller prime $p$.\n\nTest suite:\nYour program must solve the following four independent instances. For each instance, the input is a pair $(N, L)$, and the required output is the smaller prime factor $p$ as an integer.\n\n- Instance A: $N = 17473$, $L = 40130$.\n- Instance B: $N = 272953$, $L = 548210$.\n- Instance C: $N = 119989$, $L = 1539578$.\n- Instance D: $N = 1005973$, $L = 2012090$.\n\nOutput format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered as $[p_A, p_B, p_C, p_D]$. For example, if the four computed smaller factors are $a$, $b$, $c$, and $d$, print $[a,b,c,d]$ on a single line.", "solution": "The solution requires deriving a scalar nonlinear function $f(x)=0$ whose root is the desired prime factor $p$, and then applying Newton's method.\n\n**1. Derivation of the Nonlinear Equation**\n\nWe start with the two given equations:\n1.  $N = p \\cdot q$\n2.  $p^2 + q^2 = L$\n\nFrom the first equation, we express $q$ in terms of $p$ as $q = N/p$ (since $p$ is a prime, $p \\neq 0$). Substituting this into the second equation eliminates $q$:\n$$\np^2 + \\left(\\frac{N}{p}\\right)^2 = L\n$$\n$$\np^2 + \\frac{N^2}{p^2} = L\n$$\nTo clear the denominator, we multiply the entire equation by $p^2$, which yields a quartic equation in $p$:\n$$\np^4 + N^2 = L p^2\n$$\nRearranging this gives the final nonlinear equation:\n$$\np^4 - L p^2 + N^2 = 0\n$$\nWe can now define a continuously differentiable scalar function $f(x)$ whose roots are the prime factors we seek:\n$$\nf(x) = x^4 - L x^2 + N^2\n$$\nThe problem is now reduced to finding the smaller positive root of $f(x) = 0$.\n\n**2. Newton's Method Implementation**\n\nNewton's method finds a root by iteratively solving a linear approximation. The update rule is:\n$$\nx_{k+1} = x_k - \\frac{f(x_k)}{f'(x_k)}\n$$\nTo use this, we need the derivative of $f(x)$, which is found using elementary calculus:\n$$\nf'(x) = \\frac{d}{dx} \\left( x^4 - L x^2 + N^2 \\right) = 4x^3 - 2Lx\n$$\nThe iteration proceeds as follows:\n- **Initial Guess:** We start with $x_0 = \\sqrt{N}$ as specified. Since $p \\le q$ by convention, we have $p \\le \\sqrt{N} \\le q$. This initial guess is therefore located between the two positive roots of $f(x)=0$ and is guaranteed to be closer to the smaller root, $p$. This choice ensures that Newton's method will converge to $p$.\n- **Iteration:** We repeatedly calculate $x_{k+1} = x_k - \\frac{x_k^4 - L x_k^2 + N^2}{4x_k^3 - 2Lx_k}$ until the change in $x_k$ is smaller than a specified tolerance (e.g., $10^{-12}$).\n- **Final Result:** After the iteration converges to a high-precision floating-point root $x^*$, we round it to the nearest integer to get an estimate for the factor, $p_{\\text{est}} = \\text{round}(x^*)$. To be robust, we can calculate the other factor as $q_{\\text{est}} = \\text{round}(N / p_{\\text{est}})$ and return the minimum of the two, which will be the smaller prime factor $p$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for the smaller prime factor p given N=pq and L=p^2+q^2\n    using Newton's method.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (17473, 40130),    # Instance A\n        (272953, 548210),   # Instance B\n        (119989, 1539578),  # Instance C\n        (1005973, 2012090), # Instance D\n    ]\n\n    results = []\n    for N, L in test_cases:\n        # This function implements Newton's method to find a root of\n        # f(x) = x^4 - L*x^2 + N^2 = 0.\n        \n        # Initial guess as specified: x_0 = sqrt(N).\n        # This guess is between the two positive roots p and q,\n        # and closer to the smaller root p.\n        x = np.sqrt(N)\n        \n        # Parameters for Newton's method\n        max_iterations = 100\n        tolerance = 1e-9\n        \n        for i in range(max_iterations):\n            # Evaluate the function f(x) and its derivative f'(x).\n            # f(x) = x^4 - L*x^2 + N^2\n            # f'(x) = 4*x^3 - 2*L*x\n            x_sq = x * x\n            fx = x_sq * x_sq - L * x_sq + N * N\n            \n            dfx = 4.0 * x * x_sq - 2.0 * L * x\n            \n            # Avoid division by zero, though unlikely for this problem's setup.\n            if abs(dfx) < 1e-12:\n                break\n                \n            # Newton's method update step\n            step = fx / dfx\n            x = x - step\n            \n            # Check for convergence\n            if abs(step) < tolerance:\n                break\n        \n        # The iteration converges to a high-precision estimate of one of the roots.\n        # Since the initial guess x_0 = sqrt(N) is closer to p (p <= sqrt(N)),\n        # the method converges to p.\n        x_root = x\n        \n        # Map the numerical root to the integer factor.\n        # Round the result to the nearest integer to get the first factor candidate.\n        factor1 = int(round(x_root))\n        \n        # Compute the other factor candidate using the definition N = pq.\n        # Rounding here handles any small floating-point inaccuracies.\n        factor2 = int(round(N / factor1))\n        \n        # The smaller of the two factors is the required prime p.\n        p = min(factor1, factor2)\n        results.append(p)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2398877"}, {"introduction": "Building on Newton's method, this final practice explores a more advanced concept: the effectiveness of linearization can be dramatically improved by transforming the problem itself. For functions that exhibit extreme growth or large dynamic ranges, such as nested exponentials, a direct linearization may converge slowly or not at all. This exercise will guide you to compare the performance of Newton's method on an equation $f(x)=c$ versus its logarithmic form $\\ln(f(x)) = \\ln(c)$, revealing how a thoughtful reformulation can make a problem much more amenable to a linear approach [@problem_id:2398933].", "problem": "You are given a family of scalar nonlinear equations of the form $f(x)=c$ where $f:\\mathbb{R}\\to (0,\\infty)$ is differentiable and $c>0$. Consider two sequences $\\{x_k\\}_{k\\ge 0}$ that attempt to solve $f(x)=c$ by first-order linearization at the current iterate $x_k$:\n\n- Direct residual linearization of $F(x)\\equiv f(x)-c$: Set $F(x)\\approx F(x_k)+F'(x_k)(x-x_k)$ and define $x_{k+1}$ as the root of this affine approximation. This yields the recursion\n$$\nx_{k+1} = x_k - \\frac{f(x_k)-c}{f'(x_k)}\n$$\n\n- Log-residual linearization of $G(x)\\equiv \\log f(x)-\\log c$ (the natural logarithm): Set $G(x)\\approx G(x_k)+G'(x_k)(x-x_k)$ and define $x_{k+1}$ as the root of this affine approximation. This yields the recursion\n$$\nx_{k+1} = x_k - \\frac{\\log f(x_k)-\\log c}{f'(x_k)/f(x_k)} = x_k - \\frac{f(x_k)\\,\\big(\\log f(x_k)-\\log c\\big)}{f'(x_k)}\n$$\nNote that the second recursion is well-defined only while $f(x_k)>0$.\n\nYour task is to implement a program that, for each test case below, computes both sequences from the same initial guess $x_0$ and reports whether each sequence converges within a given iteration budget, along with the number of iterations taken and the final approximation. Use the following termination rule for both sequences: stop at the first index $k$ such that $|x_{k+1}-x_k|\\le \\varepsilon$, or when the iteration count reaches a maximum $K_{\\max}$. Use $\\varepsilon=10^{-12}$ and $K_{\\max}=50$. If at any step the denominator $f'(x_k)$ is zero, or the expression is undefined (for example, if $f(x_k)\\le 0$ for the log-residual linearization), declare that method as not converged for that test case.\n\nTest suite:\n- Case A (highly nested exponential, moderate target): $f(x)=\\exp(\\exp(x))$, $c=100$, $x_0=2$.\n- Case B (highly nested exponential, extreme target): $f(x)=\\exp(\\exp(x))$, $c=10^{50}$, $x_0=5$.\n- Case C (steep exponential, scaling challenge): $f(x)=\\exp(50\\,x)$, $c=3$, $x_0=1$.\n\nFor each case, report a list in the following order:\n$[$converged by direct residual linearization as a boolean, iteration count for direct residual linearization as an integer, final approximation by direct residual linearization as a real number, converged by log-residual linearization as a boolean, iteration count for log-residual linearization as an integer, final approximation by log-residual linearization as a real number$]$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for all cases aggregated as a comma-separated list enclosed in square brackets, where each element is the list for one case, in the order A, B, C. For example,\n$[$$[$\\text{case A results}$$]$,$[$\\text{case B results}$$]$,$[$\\text{case C results}$$]$$]$.\nAll quantities are unitless real numbers or integers, and booleans must be printed as either $\\text{True}$ or $\\text{False}$.", "solution": "We formalize the two linearization-based recursions from first principles using first-order Taylor expansion. Let $f:\\mathbb{R}\\to (0,\\infty)$ be differentiable and let $c>0$.\n\nDirect residual linearization:\nDefine $F(x)=f(x)-c$. The first-order Taylor polynomial of $F$ around a current iterate $x_k$ is\n$$\nF(x)\\approx F(x_k)+F'(x_k)\\,(x-x_k).\n$$\nSetting the affine approximation to zero and solving for $x$ gives\n$$\nF(x_k)+F'(x_k)\\,(x_{k+1}-x_k)=0 \\;\\;\\Longrightarrow\\;\\; x_{k+1}=x_k-\\frac{F(x_k)}{F'(x_k)}=x_k-\\frac{f(x_k)-c}{f'(x_k)}.\n$$\nThis defines the direct update formula and is precisely the step that solves, at each iteration, the linearized subproblem obtained from the original residual $F$.\n\nLog-residual linearization:\nDefine $G(x)=\\log f(x)-\\log c$, which is well defined provided $f(x)>0$ and $c>0$. The first-order Taylor polynomial around $x_k$ is\n$$\nG(x)\\approx G(x_k)+G'(x_k)\\,(x-x_k),\n$$\nwhere, by the chain rule and quotient rule,\n$$\nG'(x)=\\frac{d}{dx}\\big(\\log f(x)\\big)=\\frac{f'(x)}{f(x)}.\n$$\nSetting the affine approximation to zero and solving for $x$ gives\n$$\nG(x_k)+G'(x_k)\\,(x_{k+1}-x_k)=0\n\\;\\;\\Longrightarrow\\;\\;\nx_{k+1}=x_k-\\frac{G(x_k)}{G'(x_k)}\n= x_k-\\frac{\\log f(x_k)-\\log c}{f'(x_k)/f(x_k)}\n= x_k-\\frac{f(x_k)\\big(\\log f(x_k)-\\log c\\big)}{f'(x_k)}.\n$$\nThis update solves, at each iteration, the linearized subproblem obtained from the log-transformed residual. Because $\\log(\\cdot)$ compresses dynamic range, this transformation can substantially improve numerical conditioning for functions with large growth, such as compositions of exponentials.\n\nTermination and robustness:\nWe use a scale-invariant termination condition based on iterate increments: stop when $|x_{k+1}-x_k|\\le \\varepsilon$, with $\\varepsilon=10^{-12}$, or when a maximum of $K_{\\max}=50$ iterations is reached. If $f'(x_k)=0$, the update is undefined and we declare non-convergence. For the log-residual method, if at any step $f(x_k)\\le 0$, the logarithm is undefined; the method is then declared non-convergent.\n\nApplication to the test suite:\n- Case A: $f(x)=\\exp(\\exp(x))$, $c=100$, $x_0=2$. The exact solution satisfies $\\exp(\\exp(x^\\star))=100$, hence $\\exp(x^\\star)=\\log 100$ and $x^\\star=\\log(\\log 100)$. Here $f'(x)=\\exp(\\exp(x))\\,\\exp(x)$.\n- Case B: $f(x)=\\exp(\\exp(x))$, $c=10^{50}$, $x_0=5$. The exact solution satisfies $x^\\star=\\log(\\log 10^{50})=\\log(50\\log 10)$. The derivative is as in Case A. The log-residual step effectively reduces the nested exponential to a single exponential scale in the update, typically yielding fewer iterations in the presence of extreme targets.\n- Case C: $f(x)=\\exp(50\\,x)$, $c=3$, $x_0=1$. The exact solution is $x^\\star=(\\log 3)/50$. The derivative is $f'(x)=50\\,\\exp(50\\,x)$. The log-residual update exploits that $\\log f(x)=50\\,x$ is linear in $x$, and thus reaches the solution extremely rapidly.\n\nThe program implements both recursions exactly as defined, applies them to each test case with the specified parameters, enforces the termination rules, and reports, for each case, a list containing: convergence boolean for the direct residual method, its iteration count, its final approximation, convergence boolean for the log-residual method, its iteration count, and its final approximation. The final output is a single line containing the list of the three case-specific lists, in the order A, B, C, as required.", "answer": "```python\nimport numpy as np\n\ndef direct_update(f, df, c, x):\n    denom = df(x)\n    if denom == 0.0 or not np.isfinite(denom):\n        return x, False, np.nan\n    step = (f(x) - c) / denom\n    x_new = x - step\n    if not np.isfinite(x_new):\n        return x, False, np.nan\n    return x_new, True, abs(step)\n\ndef log_update(f, df, c, x):\n    fx = f(x)\n    if fx <= 0.0 or not np.isfinite(fx):\n        return x, False, np.nan\n    denom = df(x)\n    if denom == 0.0 or not np.isfinite(denom):\n        return x, False, np.nan\n    # Compute log-residual safely\n    log_res = np.log(fx) - np.log(c)\n    step = fx * log_res / denom\n    x_new = x - step\n    if not np.isfinite(x_new):\n        return x, False, np.nan\n    return x_new, True, abs(step)\n\ndef iterate(method, f, df, c, x0, tol=1e-12, kmax=50):\n    x = float(x0)\n    converged = False\n    last_step = np.inf\n    for k in range(1, kmax + 1):\n        x_new, ok, step_mag = method(f, df, c, x)\n        if not ok or not np.isfinite(step_mag):\n            return False, k-1, x\n        if step_mag <= tol:\n            x = x_new\n            converged = True\n            return True, k, x\n        x = x_new\n        last_step = step_mag\n    return converged, kmax, x\n\ndef case_results(f, df, c, x0, tol=1e-12, kmax=50):\n    conv_d, it_d, x_d = iterate(direct_update, f, df, c, x0, tol, kmax)\n    conv_l, it_l, x_l = iterate(log_update, f, df, c, x0, tol, kmax)\n    return [conv_d, it_d, x_d, conv_l, it_l, x_l]\n\ndef solve():\n    # Define test cases per the problem statement.\n\n    # Case A: f(x) = exp(exp(x)), c = 100, x0 = 2\n    def f1(x):\n        return np.exp(np.exp(x))\n    def df1(x):\n        ex = np.exp(x)\n        return np.exp(ex) * ex  # exp(exp(x)) * exp(x)\n    c1 = 100.0\n    x01 = 2.0\n\n    # Case B: f(x) = exp(exp(x)), c = 1e50, x0 = 5\n    c2 = 1e50\n    x02 = 5.0\n\n    # Case C: f(x) = exp(50 x), c = 3, x0 = 1\n    def f3(x):\n        return np.exp(50.0 * x)\n    def df3(x):\n        return 50.0 * np.exp(50.0 * x)\n    c3 = 3.0\n    x03 = 1.0\n\n    test_cases = [\n        (f1, df1, c1, x01),\n        (f1, df1, c2, x02),\n        (f3, df3, c3, x03),\n    ]\n\n    results = []\n    for f, df, c, x0 in test_cases:\n        res = case_results(f, df, c, x0, tol=1e-12, kmax=50)\n        # Ensure standard Python types for printing\n        conv_d, it_d, x_d, conv_l, it_l, x_l = res\n        results.append([bool(conv_d), int(it_d), float(x_d), bool(conv_l), int(it_l), float(x_l)])\n\n    # Final print statement in the exact required format: list of lists\n    # Use default string conversion to match the specified format.\n    print(str(results).replace(' ', ''))\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2398933"}]}