{"hands_on_practices": [{"introduction": "The BFGS algorithm iteratively builds an approximation of the problem's curvature, but it must start somewhere. This first practice focuses on the crucial choice of the initial inverse Hessian approximation, $H_0$. By evaluating common options, you will understand why selecting the identity matrix ($H_0 = I$) is the standard approach, effectively starting the optimization with a robust steepest descent step before accumulating curvature information [@problem_id:2208648].", "problem": "A team of data scientists is tasked with training a complex machine learning model by minimizing its loss function, $f(x)$, where $x$ is a vector of the model's parameters in $\\mathbb{R}^n$. They decide to use the Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm, a powerful quasi-Newton optimization method. The core of the BFGS algorithm is the iterative update:\n$$x_{k+1} = x_k + \\alpha_k p_k$$\nwhere $\\alpha_k  0$ is a step length found by a line search, and $p_k$ is the search direction given by:\n$$p_k = -H_k g_k$$\nHere, $g_k = \\nabla f(x_k)$ is the gradient of the loss function at the current iterate $x_k$, and $H_k$ is an approximation to the inverse of the Hessian matrix, which is updated at each step.\n\nFor the very first step ($k=0$), the team needs to select an initial matrix, $H_0$. They have no specific prior knowledge about the shape or curvature of the loss function landscape. They are considering several options for the initial inverse Hessian approximation $H_0$. Which of the following choices represents the standard and most justifiable starting point in this common scenario?\n\nA. $H_0 = O$ (the zero matrix). Justification: This represents a state of maximum uncertainty, imposing no initial belief about the curvature of the function.\n\nB. $H_0 = I$ (the identity matrix). Justification: This choice simplifies the initial search direction to be the same as that of the steepest descent method, providing a robust and well-understood starting point.\n\nC. $H_0 = \\epsilon I$ where $\\epsilon$ is a very small positive number (e.g., $10^{-6}$). Justification: This ensures that the first step taken is very small, preventing the algorithm from diverging due to a poor initial guess far from the minimum.\n\nD. $H_0$ is a randomly generated symmetric positive-definite matrix. Justification: This avoids the artificial structure of the identity matrix and may lead to faster convergence if the random matrix happens to align well with the true inverse Hessian.\n\nE. $H_0 = \\frac{1}{g_0^T g_0} I$. Justification: This scales the initial step to have a normalized magnitude, preventing excessively large or small steps based on the initial gradient's scale.", "solution": "We require $H_{0}$ to be symmetric positive definite to ensure a descent direction at the first iteration. Specifically, the BFGS search direction is $p_{k}=-H_{k}g_{k}$, and to guarantee $p_{k}$ is a descent direction when $g_{k}\\neq 0$, we need\n$$\np_{k}^{T}g_{k}=-g_{k}^{T}H_{k}g_{k}0,\n$$\nwhich holds for all nonzero $g_{k}$ if and only if $H_{k}$ is symmetric positive definite.\n\nEvaluate each proposed $H_{0}$:\n\n- Option A: $H_{0}=O$. Then $p_{0}=-H_{0}g_{0}=0$, so no movement occurs. Moreover, $H_{0}$ is singular and not positive definite, violating the requirement above and potentially yielding a rank-deficient first update\n$$\nH_{1}=(I-\\rho s y^{T})H_{0}(I-\\rho y s^{T})+\\rho s s^{T}=\\rho s s^{T},\n$$\nwhich is positive semidefinite but not positive definite for $n1$ (rank one), undermining the algorithm.\n\n- Option B: $H_{0}=I$. Then $H_{0}$ is symmetric positive definite and\n$$\np_{0}=-H_{0}g_{0}=-g_{0},\n$$\nso the first step coincides with steepest descent, a robust default. With a line search enforcing $y_{k}^{T}s_{k}0$, the BFGS update preserves positive definiteness:\n$$\nH_{k+1}=(I-\\rho_{k}s_{k}y_{k}^{T})H_{k}(I-\\rho_{k}y_{k}s_{k}^{T})+\\rho_{k}s_{k}s_{k}^{T},\\quad \\rho_{k}=\\frac{1}{y_{k}^{T}s_{k}},\n$$\nand starting from $H_{0}=I$ is the standard practice recommended in the literature.\n\n- Option C: $H_{0}=\\epsilon I$ with very small $\\epsilon0$. This is symmetric positive definite, but it artificially shrinks the initial direction:\n$$\np_{0}=-\\epsilon g_{0}.\n$$\nAlthough with exact line search the scaling of $p_{0}$ does not affect the minimizer along the ray, practical inexact line searches (e.g., Wolfe conditions with initial trial step of $1$) make this choice lead to an unnecessarily small effective step and poor numerical scaling. It is not the standard default.\n\n- Option D: $H_{0}$ a random symmetric positive-definite matrix. While symmetric positive definite can be enforced, this introduces arbitrary anisotropy and variability without prior information, can yield poor directions, and is not standard practice.\n\n- Option E: $H_{0}=\\frac{1}{g_{0}^{T}g_{0}}I$. This is symmetric positive definite and yields\n$$\np_{0}=-\\frac{1}{g_{0}^{T}g_{0}}g_{0},\n$$\nwhose norm scales as $\\|p_{0}\\|=\\frac{1}{\\|g_{0}\\|}$, coupling the step scaling to the gradient magnitude in a nonstandard way. This is not the customary initialization and can be counterproductive.\n\nTherefore, the standard and most justifiable choice in the absence of prior curvature information is $H_{0}=I$, which makes the first step a steepest descent step and preserves desirable BFGS properties under standard line searches.", "answer": "$$\\boxed{B}$$", "id": "2208648"}, {"introduction": "Building on the foundational choice of initialization, this practice explores a more advanced technique to accelerate initial convergence. While the identity matrix provides a safe starting point, its unit scaling can be suboptimal for ill-conditioned functions. This hands-on coding exercise challenges you to implement and compare the standard initialization with a scaled version that uses a probing step to better match the local curvature, highlighting the practical performance gains on the challenging Rosenbrock function [@problem_id:2431054].", "problem": "Consider unconstrained minimization of a smooth function $f:\\mathbb{R}^n \\to \\mathbb{R}$ using a Quasi-Newton method with the Broyden-Fletcher-Goldfarb-Shanno (BFGS) update. The method maintains an approximation $B_k$ to the inverse Hessian matrix (the inverse of the matrix of second derivatives), and computes a descent direction $p_k$ at iteration $k$ via $p_k = - B_k \\nabla f(x_k)$. The line search then chooses a step length $\\alpha_k$ along $p_k$ to satisfy the strong Wolfe conditions with constants $c_1 = 10^{-4}$ and $c_2 = 0.9$. The initial inverse Hessian approximation $B_0$ strongly influences the first step $p_0$ and hence the initial progress.\n\nYour task is to construct and evaluate a reproducible scenario where the choice $B_0 = I$ (the identity matrix) leads to very slow initial progress compared to a scaled identity $B_0 = \\gamma I$ with\n$$\n\\gamma = \\frac{y_0^\\top s_0}{y_0^\\top y_0}, \\quad s_0 = -\\eta \\, \\nabla f(x_0), \\quad y_0 = \\nabla f(x_0 + s_0) - \\nabla f(x_0),\n$$\nwhere $\\eta  0$ is a small probing step. This scaled choice attempts to tune the initial inverse Hessian to the local curvature measured along the gradient direction.\n\nUse as objective the generalized two-parameter Rosenbrock function in even dimension $n$:\n$$\nf(x) = \\sum_{i=1}^{n/2} \\left[ \\beta\\left(x_{2i} - x_{2i-1}^2\\right)^2 + \\left(1 - x_{2i-1}\\right)^2 \\right],\n$$\nwith gradient components\n$$\n\\frac{\\partial f}{\\partial x_{2i-1}} = -4\\beta\\,x_{2i-1}\\left(x_{2i} - x_{2i-1}^2\\right) + 2\\left(x_{2i-1} - 1\\right), \\quad\n\\frac{\\partial f}{\\partial x_{2i}} = 2\\beta\\left(x_{2i} - x_{2i-1}^2\\right),\n$$\nfor $i=1,\\dots,n/2$.\n\nImplement a single BFGS iteration with two initializations:\n- Case A (unscaled): $B_0 = I$.\n- Case B (scaled): $B_0 = \\gamma I$ with $\\gamma$ given above, using the probing step $s_0 = -\\eta \\nabla f(x_0)$.\n\nFor each case, compute $p_0 = -B_0 \\nabla f(x_0)$ and select $\\alpha_0$ via a strong Wolfe line search with $c_1 = 10^{-4}$ and $c_2 = 0.9$ starting from $\\alpha = 1$. If the strong Wolfe line search fails to return a step length, fall back to Armijo backtracking (sufficient decrease only) with reduction factor $0.5$, the same $c_1$, and the same starting $\\alpha = 1$. Then form $x_1 = x_0 + \\alpha_0 p_0$ for each case. Define the initial progress as the objective reduction $f(x_0) - f(x_1)$.\n\nYour program must compute, for each test case, the ratio\n$$\nR = \\frac{f(x_0) - f(x_1^{\\text{scaled}})}{\\max\\left(f(x_0) - f(x_1^{\\text{identity}}),\\,10^{-30}\\right)},\n$$\nwhere $x_1^{\\text{scaled}}$ is obtained with the scaled $B_0 = \\gamma I$ and $x_1^{\\text{identity}}$ with $B_0 = I$. A value $R  1$ indicates that the scaled initialization made more progress than the unscaled one on the first iteration. Report $R$ rounded to six decimal places.\n\nTest suite:\n- Test 1 (high curvature, two dimensions): $n=2$, $\\beta = 10^4$, $x_0 = (-1.2,\\,1.0)$, $\\eta = 10^{-3}$.\n- Test 2 (moderate curvature, four dimensions): $n=4$, $\\beta = 100$, $x_0 = (-1.2,\\,1.0,\\,-1.2,\\,1.0)$, $\\eta = 10^{-3}$.\n- Test 3 (edge case near minimizer): $n=2$, $\\beta = 100$, $x_0 = (1.0,\\,1.0001)$, $\\eta = 10^{-3}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, \"[r1,r2,r3]\". Each $r_i$ must be the floating-point value of $R$ for the corresponding test, rounded to six decimal places. No other output is permitted. There are no physical units involved and no angles; all quantities are dimensionless real numbers.", "solution": "The posed problem is subjected to rigorous validation. All givens, including the objective function, its gradient, algorithmic parameters, and test cases, have been extracted. The problem is found to be scientifically grounded in the established field of numerical optimization. It is well-posed, with all necessary information provided for a unique, deterministic solution. The language is objective and precise. Therefore, the problem is deemed valid and a solution will be provided.\n\nThe problem requires an analysis of the first iteration of a Quasi-Newton optimization method, specifically the Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm. We are to compare the initial progress for two distinct choices of the initial inverse Hessian approximation, $B_0$.\n\nThe general iterative scheme for a Quasi-Newton method is given by:\n$$ x_{k+1} = x_k + \\alpha_k p_k $$\nwhere $p_k$ is the search direction and $\\alpha_k$ is the step length. The search direction is computed using the current approximation of the inverse Hessian matrix, $B_k$, and the gradient of the objective function, $\\nabla f(x_k)$:\n$$ p_k = -B_k \\nabla f(x_k) $$\nThe step length $\\alpha_k$ is determined by a line search procedure to ensure sufficient decrease in the function value and satisfaction of curvature conditions. The problem specifies the strong Wolfe conditions:\n1. Armijo (sufficient decrease) condition: $f(x_k + \\alpha_k p_k) \\le f(x_k) + c_1 \\alpha_k \\nabla f(x_k)^\\top p_k$\n2. Curvature condition: $|\\nabla f(x_k + \\alpha_k p_k)^\\top p_k| \\le c_2 |\\nabla f(x_k)^\\top p_k|$\nwith specified constants $c_1 = 10^{-4}$ and $c_2 = 0.9$.\n\nThe core of this problem lies in the choice of the initial approximation, $B_0$, which dictates the very first step of the optimization process, $p_0 = -B_0 \\nabla f(x_0)$. We examine two cases.\n\nCase A: The Unscaled Identity Initialization\nThe choice $B_0 = I$, where $I$ is the identity matrix, is the simplest possible. This results in an initial search direction $p_0 = -\\nabla f(x_0)$, which is the direction of steepest descent. While intuitive, this direction can be very inefficient for ill-conditioned problems where the level sets of the function are highly eccentric, leading to slow, zigzagging convergence.\n\nCase B: The Scaled Identity Initialization\nA more sophisticated approach is to scale the initial identity matrix, $B_0 = \\gamma I$. The scaling factor $\\gamma$ is chosen to approximate the curvature of the function. The problem prescribes a specific method to find $\\gamma$ based on a probing step. We first compute a trial step $s_0 = -\\eta \\nabla f(x_0)$ for a small $\\eta  0$. We then measure the change in the gradient, $y_0 = \\nabla f(x_0 + s_0) - \\nabla f(x_0)$. The scaling factor is then given by:\n$$ \\gamma = \\frac{y_0^\\top s_0}{y_0^\\top y_0} $$\nThis formula for $\\gamma$ is derived from finding a scalar that best satisfies the secant equation $s_0 = B_0 y_0$ in a least-squares sense, i.e., by minimizing $\\|\\ s_0 - \\gamma y_0 \\|_2^2$ with respect to $\\gamma$. This seeks to imbue $B_0$ with some information about the function's curvature, potentially leading to a much better-scaled initial step $p_0 = -\\gamma \\nabla f(x_0)$. For this to be a descent direction, we require $\\gamma  0$, which is true if the function is convex enough along the probing direction $s_0$ such that $y_0^\\top s_0  0$.\n\nThe test function is the generalized Rosenbrock function for even dimension $n$:\n$$ f(x) = \\sum_{i=1}^{n/2} \\left[ \\beta\\left(x_{2i} - x_{2i-1}^2\\right)^2 + \\left(1 - x_{2i-1}\\right)^2 \\right] $$\nThis function is a classic benchmark for optimization algorithms due to its non-convexity and the presence of a narrow, parabolic valley. For large $\\beta$, the problem becomes very ill-conditioned, making it an excellent candidate to demonstrate the superiority of a well-scaled initial step over the naive steepest descent direction.\n\nThe computational procedure is as follows:\n1.  For each test case ($n, \\beta, x_0, \\eta$), we will implement the Rosenbrock function and its gradient.\n2.  We will execute one iteration for Case A ($B_0 = I$) and Case B ($B_0 = \\gamma I$).\n3.  For Case B, the factor $\\gamma$ is computed first, requiring an evaluation of the gradient at $x_0$ and at a probed point $x_0 + s_0$. A safeguard is necessary for the case where the denominator $y_0^\\top y_0$ is close to zero.\n4.  For both cases, the search direction $p_0$ is computed. We must verify that it is a descent direction (i.e., $\\nabla f(x_0)^\\top p_0  0$).\n5.  A line search is performed along $p_0$ starting from $\\alpha = 1$ to find a step length $\\alpha_0$ satisfying the strong Wolfe conditions. We will use the `line_search` function from the `scipy.optimize` library.\n6.  As per the problem, if the strong Wolfe line search fails, we must fall back to a manual implementation of Armijo backtracking, where we iteratively reduce $\\alpha$ by a factor of $0.5$ until the sufficient decrease condition is met.\n7.  With $\\alpha_0$ determined, we compute the next iterate $x_1 = x_0 + \\alpha_0 p_0$ and the corresponding function value $f(x_1)$.\n8.  The progress for each case is defined as the reduction in the objective function, $f(x_0) - f(x_1)$.\n9.  Finally, we compute the ratio $R$ of the progress made by the scaled method to that of the unscaled method:\n$$ R = \\frac{f(x_0) - f(x_1^{\\text{scaled}})}{\\max\\left(f(x_0) - f(x_1^{\\text{identity}}),\\,10^{-30}\\right)} $$\nThe denominator is regularized to prevent division by zero or numerical instability. The resulting value of $R$ for each test case will be rounded to six decimal places.\n\nThis procedure will be encapsulated in a Python program, which constitutes the final answer.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import line_search\n\ndef rosenbrock(x, beta):\n    \"\"\"\n    Computes the value of the generalized Rosenbrock function.\n    \"\"\"\n    n = len(x)\n    if n % 2 != 0:\n        raise ValueError(\"Dimension n must be even for the generalized Rosenbrock function.\")\n    \n    val = 0.0\n    for i in range(n // 2):\n        # x-indices are 2*i and 2*i+1, corresponding to problem's x_2i-1 and x_2i for i=1..n/2\n        idx1 = 2 * i\n        idx2 = 2 * i + 1\n        term1 = beta * (x[idx2] - x[idx1]**2)**2\n        term2 = (1 - x[idx1])**2\n        val += term1 + term2\n    return val\n\ndef rosenbrock_grad(x, beta):\n    \"\"\"\n    Computes the gradient of the generalized Rosenbrock function.\n    \"\"\"\n    n = len(x)\n    if n % 2 != 0:\n        raise ValueError(\"Dimension n must be even for the generalized Rosenbrock function.\")\n        \n    grad = np.zeros(n)\n    for i in range(n // 2):\n        idx1 = 2 * i\n        idx2 = 2 * i + 1\n        common_term = 2 * beta * (x[idx2] - x[idx1]**2)\n        grad[idx1] = -2 * x[idx1] * common_term + 2 * (x[idx1] - 1)\n        grad[idx2] = common_term\n    return grad\n\ndef compute_progress(x0, beta, eta, initialization, c1, c2):\n    \"\"\"\n    Computes the progress f(x0) - f(x1) for a single Quasi-Newton iteration.\n    Handles both identity and scaled initializations, and includes line search logic.\n    \"\"\"\n    # Create lambda functions to pass beta parameter\n    f = lambda x: rosenbrock(x, beta)\n    grad = lambda x: rosenbrock_grad(x, beta)\n\n    f0 = f(x0)\n    g0 = grad(x0)\n\n    # If gradient is virtually zero, no progress can be made.\n    if np.linalg.norm(g0)  1e-12:\n        return 0.0\n\n    if initialization == 'identity':\n        p0 = -g0\n    elif initialization == 'scaled':\n        s0 = -eta * g0\n        \n        # Probing step to compute the scaling factor gamma\n        x_probe = x0 + s0\n        g_probe = grad(x_probe)\n        y0 = g_probe - g0\n\n        y0_dot_y0 = np.dot(y0, y0)\n        \n        # Guard against division by zero for ill-defined gamma\n        if y0_dot_y0  1e-20:\n            gamma = 1.0 # Fallback to identity scaling\n        else:\n            y0_dot_s0 = np.dot(y0, s0)\n            gamma = y0_dot_s0 / y0_dot_y0\n        \n        p0 = -gamma * g0\n    else:\n        raise ValueError(f\"Unknown initialization type: {initialization}\")\n\n    # The search direction must be a descent direction.\n    # If gamma = 0, this will not hold, and progress should be zero.\n    pk_dot_g0 = np.dot(g0, p0)\n    if pk_dot_g0 = 0:\n        return 0.0\n\n    # Perform strong Wolfe line search using SciPy\n    alpha, _, _, f_new, _, _ = line_search(\n        f=f,\n        myfprime=grad,\n        xk=x0,\n        pk=p0,\n        gfk=g0,\n        old_fval=f0,\n        c1=c1,\n        c2=c2\n    )\n\n    # Fallback to Armijo backtracking if strong Wolfe search fails\n    if alpha is None:\n        alpha = 1.0\n        rho = 0.5\n        \n        # Limit backtracking steps to prevent infinite loops\n        for _ in range(100):\n            x_new_check = x0 + alpha * p0\n            f_new_check = f(x_new_check)\n            if f_new_check = f0 + c1 * alpha * pk_dot_g0:\n                f_new = f_new_check\n                break\n            alpha *= rho\n        else:\n            # If Armijo loop completes without break, step is negligible.\n            return 0.0\n\n    return f0 - f_new\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, beta, x0, eta)\n        (2, 1e4, np.array([-1.2, 1.0]), 1e-3),\n        (4, 100.0, np.array([-1.2, 1.0, -1.2, 1.0]), 1e-3),\n        (2, 100.0, np.array([1.0, 1.0001]), 1e-3)\n    ]\n    \n    # Line search parameters\n    c1 = 1e-4\n    c2 = 0.9\n\n    results = []\n    for n, beta, x0, eta in test_cases:\n        # Calculate progress for the unscaled (identity) case\n        progress_identity = compute_progress(x0, beta, eta, 'identity', c1, c2)\n        \n        # Calculate progress for the scaled case\n        progress_scaled = compute_progress(x0, beta, eta, 'scaled', c1, c2)\n\n        # Compute the ratio R, with a safeguard for the denominator\n        denominator = max(progress_identity, 1e-30)\n        R = progress_scaled / denominator\n        \n        results.append(round(R, 6))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2431054"}, {"introduction": "The power of BFGS is fully realized in its limited-memory variant, L-BFGS, which makes it suitable for large-scale problems where storing an $n \\times n$ matrix is impossible. Instead of the matrix, L-BFGS stores only the last few vector pairs that define the curvature and uses a clever algorithm to compute the search direction. This culminating exercise guides you to implement the famous L-BFGS two-loop recursion from scratch, revealing the mechanics behind one of the most important algorithms in modern optimization [@problem_id:2431082].", "problem": "You are given finite sequences of curvature pairs $\\{(s_i,y_i)\\}$, an initial scalar $\\gamma$ defining $H_0=\\gamma I$, and a current gradient $g$ of a smooth objective function. Consider the symmetric positive definite linear operator $H$ defined as follows: among all symmetric positive definite matrices that satisfy the secant conditions $H y_i = s_i$ for the most recent $m$ pairs $\\{(s_i,y_i)\\}_{i=k-m+1}^k$ with $s_i^\\top y_i gt; 0$, the operator $H$ is the unique one obtained by starting from $H_0=\\gamma I$ and enforcing these secant conditions in increasing index order in the sense of the Broyden-Fletcher-Goldfarb-Shanno (BFGS) update. For each test case below, compute the search direction $p=-H g$.\n\nAll vectors and matrices are over the real numbers. All components are to be treated as dimensionally consistent pure numbers (no physical units). Use the following test suite; in each case, only the last $m$ pairs are to be enforced.\n\nTest case $1$ (boundary, no curvature pairs):\n- Dimension $n=3$.\n- Memory size $m=0$ (no pairs).\n- Initial scaling $\\gamma = 0.5$.\n- Gradient $g = \\left[1,-2,3\\right]$.\n\nTest case $2$ (single curvature pair):\n- Dimension $n=2$.\n- Memory size $m=1$.\n- Initial scaling $\\gamma = 1$.\n- Curvature pair $\\left(s_1,y_1\\right) = \\left(\\left[1,2\\right],\\left[3,1\\right]\\right)$ with $s_1^\\top y_1 = 5$.\n- Gradient $g = \\left[4,-1\\right]$.\n\nTest case $3$ (multiple pairs spanning the space, exact recovery for a quadratic):\n- Dimension $n=3$.\n- Memory size $m=3$.\n- Initial scaling $\\gamma = 1$.\n- Define $A=\\mathrm{diag}\\!\\left(2,3,4\\right)$. Let $s_1=\\left[1,0,0\\right]$, $s_2=\\left[0,1,0\\right]$, $s_3=\\left[0,0,1\\right]$, and $y_i = A s_i$, that is $y_1=\\left[2,0,0\\right]$, $y_2=\\left[0,3,0\\right]$, $y_3=\\left[0,0,4\\right]$.\n- Gradient $g = \\left[5,-6,7\\right]$.\n\nTest case $4$ (limited memory using only the most recent pairs):\n- Dimension $n=4$.\n- Memory size $m=2$ (use only the last two pairs listed below).\n- Initial scaling $\\gamma = 1$.\n- Define the symmetric positive definite matrix\n$$\nA=\\begin{bmatrix}\n4  1  0  0\\\\\n1  3  0  0\\\\\n0  0  2  0\\\\\n0  0  0  1.5\n\\end{bmatrix}.\n$$\n- Define pairs from $y_i = A s_i$ for\n$s_1=\\left[1,0,0,0\\right]$ with $y_1=\\left[4,1,0,0\\right]$,\n$s_2=\\left[0,1,0,0\\right]$ with $y_2=\\left[1,3,0,0\\right]$,\n$s_3=\\left[0,0,1,0\\right]$ with $y_3=\\left[0,0,2,0\\right]$,\n$s_4=\\left[0,0,0,1\\right]$ with $y_4=\\left[0,0,0,1.5\\right]$.\n- Gradient $g = \\left[1,2,3,4\\right]$.\n- Only the last $m=2$ pairs, namely $\\left(s_3,y_3\\right)$ and $\\left(s_4,y_4\\right)$, are to be enforced.\n\nYour program must compute the corresponding search direction $p=-H g$ for each test case. Output formatting requirement: your program should produce a single line of output containing a list of the four search direction vectors in order, with no whitespace, using decimal notation. Specifically, the output must have the form\n$[[p^{(1)}_1,\\dots,p^{(1)}_{n_1}],[p^{(2)}_1,\\dots,p^{(2)}_{n_2}],[p^{(3)}_1,\\dots,p^{(3)}_{n_3}],[p^{(4)}_1,\\dots,p^{(4)}_{n_4}]]$,\nwhere $p^{(j)}$ is the vector for test case $j$. Each number must be printed as a decimal (for example, $-2$, $-1.5$, or $-2.6666666667$ are acceptable). The final output must be a single line exactly in this bracketed, comma-separated format.", "solution": "The problem statement is critically examined and found to be valid. It is scientifically grounded, well-posed, objective, and internally consistent. It presents a standard computational task from numerical optimization: the calculation of a search direction using the Limited-Memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) algorithm.\n\nThe problem defines a symmetric positive definite matrix $H$ through a constructive procedure. Starting with an initial matrix $H_0 = \\gamma I$, where $\\gamma$ is a positive scalar and $I$ is the identity matrix, a sequence of $m$ BFGS updates is applied using the provided curvature pairs $\\{(s_i, y_i)\\}$. The problem specifies that the updates are applied in increasing order of their indices. The BFGS update formula for the inverse Hessian approximation $H_k$ to obtain $H_{k+1}$ is:\n$$ H_{k+1} = (I - \\rho_k s_k y_k^\\top) H_k (I - \\rho_k y_k s_k^\\top) + \\rho_k s_k s_k^\\top $$\nwhere $\\rho_k = (y_k^\\top s_k)^{-1}$. The condition $s_k^\\top y_k  0$ ensures that $\\rho_k$ is positive and that positive definiteness is maintained throughout the updates.\n\nThe task is to compute the search direction $p = -Hg$, where $g$ is a given gradient vector. The direct computation by first forming the $n \\times n$ matrix $H$ and then performing the matrix-vector multiplication is computationally prohibitive for large-scale problems, for which L-BFGS is designed. The standard and efficient method is the L-BFGS two-loop recursion, which computes the product $Hg$ without explicitly forming $H$. This procedure utilizes only the $m$ stored curvature pairs and the initial scaling factor $\\gamma$. This is the principle guiding the solution design.\n\nThe algorithm to compute $r=Hg$ is as follows. Let the $m$ most recent curvature pairs be denoted $\\{(s_j, y_j)\\}_{j=1}^m$, ordered from the oldest to the newest in the memory window. Let $\\rho_j = (y_j^\\top s_j)^{-1}$ for each pair.\n\n1.  Initialize a vector $q \\leftarrow g$.\n2.  Perform a backward pass (first loop) from the newest pair ($j=m$) to the oldest ($j=1$):\n    For $j = m, m-1, \\dots, 1$:\n    -   Compute and store $\\alpha_j \\leftarrow \\rho_j s_j^\\top q$.\n    -   Update $q \\leftarrow q - \\alpha_j y_j$.\n3.  Scale the intermediate vector by the initial inverse Hessian approximation: $r \\leftarrow H_0 q = \\gamma q$.\n4.  Perform a forward pass (second loop) from the oldest pair ($j=1$) to the newest ($j=m$):\n    For $j = 1, 2, \\dots, m$:\n    -   Compute $\\beta_j \\leftarrow \\rho_j y_j^\\top r$.\n    -   Update $r \\leftarrow r + s_j(\\alpha_j - \\beta_j)$.\n\nThe resulting vector $r$ is the product $Hg$. The final search direction is $p = -r$. This procedure is implemented to solve each of the specified test cases.\n\n-   **Test Case 1**: With memory $m=0$, no curvature pairs are used. The loops are not executed. The calculation simplifies to $p = -H_0 g = -\\gamma g$. For $\\gamma = 0.5$ and $g = [1, -2, 3]^\\top$, we have $p = -0.5 \\times [1, -2, 3]^\\top = [-0.5, 1, -1.5]^\\top$.\n\n-   **Test Case 2**: With $m=1$ pair $(s_1, y_1)$, the two-loop recursion is applied. We calculate $\\rho_1 = (y_1^\\top s_1)^{-1} = 1/5 = 0.2$. The algorithm yields $p = [-1.8, 3.4]^\\top$.\n\n-   **Test Case 3**: With $m=n=3$ and the pairs $(s_i, y_i)$ where $s_i$ are standard basis vectors and $y_i=As_i$ for a diagonal matrix $A$, the L-BFGS procedure is known to recover the exact inverse Hessian, $H=A^{-1}$, after $n$ updates. Thus, the search direction is $p = -A^{-1}g$. For $A=\\mathrm{diag}(2,3,4)$ and $g = [5, -6, 7]^\\top$, this results in $p = -[\\mathrm{diag}(0.5, 1/3, 0.25)] [5, -6, 7]^\\top = [-2.5, 2, -1.75]^\\top$. The two-loop recursion confirms this result.\n\n-   **Test Case 4**: With memory $m=2$, only the last two pairs, $(s_3, y_3)$ and $(s_4, y_4)$, are used. These pairs are orthogonal. The initial matrix is $H_0 = I$ ($\\gamma=1$). The L-BFGS updates effectively modify only the components of the inverse Hessian approximation corresponding to the subspace spanned by $\\{s_3, s_4\\}$. The resulting search direction is calculated as $p = [-1, -2, -1.5, -8/3]^\\top$.", "answer": "```python\nimport numpy as np\n\ndef compute_lbfgs_direction(m, gamma, s_pairs, y_pairs, g):\n    \"\"\"\n    Computes the L-BFGS search direction p = -Hg using the two-loop recursion.\n\n    Args:\n        m (int): The memory size.\n        gamma (float): The initial scaling factor for H_0.\n        s_pairs (list of np.ndarray): List of 's' vectors {s_k}.\n        y_pairs (list of np.ndarray): List of 'y' vectors {y_k}.\n        g (np.ndarray): The current gradient vector.\n\n    Returns:\n        np.ndarray: The search direction vector p.\n    \"\"\"\n    if m == 0:\n        return -gamma * g\n\n    rhos = [1.0 / (y.T @ s) for s, y in zip(s_pairs, y_pairs)]\n    alphas = np.zeros(m)\n    \n    q = g.copy()\n\n    # First loop: from newest to oldest pair\n    for i in range(m - 1, -1, -1):\n        alphas[i] = rhos[i] * (s_pairs[i].T @ q)\n        q = q - alphas[i] * y_pairs[i]\n\n    r = gamma * q\n\n    # Second loop: from oldest to newest pair\n    for i in range(m):\n        beta = rhos[i] * (y_pairs[i].T @ r)\n        r = r + s_pairs[i] * (alphas[i] - beta)\n        \n    p = -r\n    return p\n\ndef solve():\n    \"\"\"\n    Defines the test cases from the problem statement and computes the results.\n    \"\"\"\n    # Test case 1\n    case1 = {\n        \"m\": 0, \"gamma\": 0.5, \"s_pairs\": [], \"y_pairs\": [],\n        \"g\": np.array([1., -2., 3.])\n    }\n\n    # Test case 2\n    case2 = {\n        \"m\": 1, \"gamma\": 1.0,\n        \"s_pairs\": [np.array([1., 2.])],\n        \"y_pairs\": [np.array([3., 1.])],\n        \"g\": np.array([4., -1.])\n    }\n\n    # Test case 3\n    s1_c3 = np.array([1., 0., 0.])\n    s2_c3 = np.array([0., 1., 0.])\n    s3_c3 = np.array([0., 0., 1.])\n    A_c3 = np.diag([2., 3., 4.])\n    y1_c3 = A_c3 @ s1_c3\n    y2_c3 = A_c3 @ s2_c3\n    y3_c3 = A_c3 @ s3_c3\n    case3 = {\n        \"m\": 3, \"gamma\": 1.0,\n        \"s_pairs\": [s1_c3, s2_c3, s3_c3],\n        \"y_pairs\": [y1_c3, y2_c3, y3_c3],\n        \"g\": np.array([5., -6., 7.])\n    }\n\n    # Test case 4\n    A_c4 = np.array([[4., 1., 0., 0.],\n                     [1., 3., 0., 0.],\n                     [0., 0., 2., 0.],\n                     [0., 0., 0., 1.5]])\n    s1_c4 = np.array([1., 0., 0., 0.])\n    s2_c4 = np.array([0., 1., 0., 0.])\n    s3_c4 = np.array([0., 0., 1., 0.])\n    s4_c4 = np.array([0., 0., 0., 1.])\n    y1_c4 = A_c4 @ s1_c4\n    y2_c4 = A_c4 @ s2_c4\n    y3_c4 = A_c4 @ s3_c4\n    y4_c4 = A_c4 @ s4_c4\n    m_c4 = 2\n    case4 = {\n        \"m\": m_c4, \"gamma\": 1.0,\n        \"s_pairs\": [s3_c4, s4_c4], # Only last m=2 pairs\n        \"y_pairs\": [y3_c4, y4_c4],\n        \"g\": np.array([1., 2., 3., 4.])\n    }\n\n    test_cases = [case1, case2, case3, case4]\n    \n    results = []\n    for case in test_cases:\n        p = compute_lbfgs_direction(case[\"m\"], case[\"gamma\"], case[\"s_pairs\"], case[\"y_pairs\"], case[\"g\"])\n        results.append(p)\n\n    str_results = [str(p.tolist()) for p in results]\n    final_output = f\"[{','.join(str_results)}]\"\n    \n    # Remove all whitespace to match the required output format.\n    print(final_output.replace(\" \", \"\"))\n\nsolve()\n```", "id": "2431082"}]}