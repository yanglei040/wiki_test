{"hands_on_practices": [{"introduction": "The quadratic formula is a staple of algebra, offering a direct solution for second-degree polynomial equations. However, when implemented on a computer, this seemingly robust formula can produce highly inaccurate results due to the phenomenon of catastrophic cancellation. This exercise [@problem_id:2435764] challenges you to analyze a specific case where this failure occurs and to reformulate the solution to achieve numerical stability, a crucial skill in computational science.", "problem": "Consider the quadratic polynomial $p(x) = x^2 - 10^8 x + 1$. Derive exact closed-form expressions for both real roots of $p(x) = 0$ from first principles. Then, using the basic model of rounding in finite precision floating-point arithmetic (as in the Institute of Electrical and Electronics Engineers (IEEE) $754$ double precision (binary$64$) standard), explain which of the two exact closed-form expressions is numerically unstable to evaluate and why, identifying the mechanism of catastrophic cancellation. Finally, rewrite the unstable root into an algebraically equivalent exact form that avoids subtractive cancellation. \n\nAnswer requirement: Provide the final answer as exact symbolic expressions for the two roots, without numerical approximation. No rounding is required. Express your final answer as a pair of exact expressions.", "solution": "The problem statement is subjected to validation before any attempt at a solution is made.\n\nFirst, we extract the givens of the problem.\nThe quadratic polynomial is given as $p(x) = x^2 - 10^8 x + 1$.\nThe problem requires finding the roots of the equation $p(x) = 0$.\nThe analysis of numerical stability must be performed within the context of finite precision floating-point arithmetic, specifically referencing the Institute of Electrical and Electronics Engineers (IEEE) $754$ double precision standard.\nThe task involves identifying which root expression derived from the standard quadratic formula is numerically unstable due to \"catastrophic cancellation\", and subsequently deriving an algebraically equivalent, but numerically stable, expression for that root.\n\nNext, we validate the problem statement.\nThe problem is scientifically grounded, as it deals with a fundamental topic in numerical analysis: the loss of precision when subtracting nearly equal numbers. This phenomenon, known as catastrophic cancellation, is a well-understood consequence of floating-point arithmetic. The polynomial and coefficients are mathematically sound.\nThe problem is well-posed. It provides all necessary information ($a=1$, $b=-10^8$, $c=1$) to find the roots and analyze their numerical properties. The question is unambiguous and leads to a unique set of stable expressions for the roots.\nThe problem is objective, stated in precise mathematical terms without any subjective or speculative content.\nTherefore, the problem is deemed valid and a solution will be furnished.\n\nThe quadratic equation to be solved is $x^2 - 10^8 x + 1 = 0$.\nWe apply the standard quadratic formula for the roots of $ax^2 + bx + c = 0$, which are given by $x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$.\nFor the given polynomial, the coefficients are $a=1$, $b=-10^8$, and $c=1$.\nSubstituting these values into the formula yields:\n$$x = \\frac{-(-10^8) \\pm \\sqrt{(-10^8)^2 - 4(1)(1)}}{2(1)}$$\n$$x = \\frac{10^8 \\pm \\sqrt{10^{16} - 4}}{2}$$\nThis gives two exact, real roots:\n$$x_1 = \\frac{10^8 + \\sqrt{10^{16} - 4}}{2}$$\n$$x_2 = \\frac{10^8 - \\sqrt{10^{16} - 4}}{2}$$\n\nNow, we must analyze the numerical stability of these expressions in finite precision arithmetic. The source of potential instability lies in the subtraction of nearly equal quantities.\nLet us examine the magnitude of the terms involved. The term $10^{16}$ is vastly larger than $4$. Consequently, the value of $\\sqrt{10^{16} - 4}$ is very close to $\\sqrt{10^{16}} = 10^8$.\nTo see this more clearly, we can use a binomial approximation.\n$$\\sqrt{10^{16} - 4} = \\sqrt{10^{16}(1 - 4 \\times 10^{-16})} = 10^8 \\sqrt{1 - 4 \\times 10^{-16}}$$\nFor a small value $\\epsilon$, the approximation $(1 - \\epsilon)^{1/2} \\approx 1 - \\frac{1}{2}\\epsilon$ holds. Here, $\\epsilon = 4 \\times 10^{-16}$, which is very small.\nThus, $\\sqrt{10^{16} - 4} \\approx 10^8 (1 - \\frac{1}{2}(4 \\times 10^{-16})) = 10^8 (1 - 2 \\times 10^{-16}) = 10^8 - 2 \\times 10^{-8}$.\nThe value of $\\sqrt{10^{16} - 4}$ is only slightly less than $10^8$.\n\nConsider the expression for $x_1$: $x_1 = \\frac{10^8 + \\sqrt{10^{16} - 4}}{2}$. This expression involves the addition of two large positive numbers of similar magnitude. In floating-point arithmetic, this operation is numerically stable. The relative error of the sum is small, on the order of the machine precision.\n\nConsider the expression for $x_2$: $x_2 = \\frac{10^8 - \\sqrt{10^{16} - 4}}{2}$. This expression involves the subtraction of two very nearly equal numbers. Let $y = \\sqrt{10^{16} - 4}$. In floating-point computation, $10^8$ and the computed value of $y$ will agree in many of their leading significant digits. When the subtraction $10^8 - y$ is performed, these leading digits cancel, and the result is determined by the remaining, less significant digits, which are heavily influenced by the rounding errors incurred when computing $y$. This massive increase in relative error is the phenomenon of catastrophic cancellation. Therefore, the expression for $x_2$ is numerically unstable and would yield a highly inaccurate result if evaluated directly using standard double-precision arithmetic.\n\nTo find a numerically stable expression for $x_2$, we use Vieta's formulas, which relate the coefficients of a polynomial to its roots. For a quadratic equation $ax^2 + bx + c = 0$, the product of the roots is given by $x_1 x_2 = \\frac{c}{a}$.\nFor our equation, this gives $x_1 x_2 = \\frac{1}{1} = 1$.\nWe can compute the stable root $x_1$ accurately using its formula. Then, we can find $x_2$ from the relation $x_2 = \\frac{1}{x_1}$.\nSubstituting the stable expression for $x_1$:\n$$x_2 = \\frac{1}{\\frac{10^8 + \\sqrt{10^{16} - 4}}{2}} = \\frac{2}{10^8 + \\sqrt{10^{16} - 4}}$$\nThis revised expression for $x_2$ involves only the addition of positive numbers and a division, both of which are numerically stable operations. It avoids subtractive cancellation and is therefore the preferred form for numerical computation.\n\nWe verify that this new form is algebraically equivalent to the original unstable form for $x_2$ by rationalizing its denominator:\n$$\\frac{2}{10^8 + \\sqrt{10^{16} - 4}} \\cdot \\frac{10^8 - \\sqrt{10^{16} - 4}}{10^8 - \\sqrt{10^{16} - 4}} = \\frac{2(10^8 - \\sqrt{10^{16} - 4})}{(10^8)^2 - (10^{16} - 4)} = \\frac{2(10^8 - \\sqrt{10^{16} - 4})}{10^{16} - 10^{16} + 4} = \\frac{2(10^8 - \\sqrt{10^{16} - 4})}{4} = \\frac{10^8 - \\sqrt{10^{16} - 4}}{2}$$\nThis confirms the algebraic identity.\n\nThe final required answer consists of the exact symbolic expressions for both roots, rewritten in their numerically stable forms.\nThe larger root is $x_1 = \\frac{10^8 + \\sqrt{10^{16} - 4}}{2}$.\nThe smaller root, in its stable form, is $x_2 = \\frac{2}{10^8 + \\sqrt{10^{16} - 4}}$.", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{10^8 + \\sqrt{10^{16} - 4}}{2}  \\frac{2}{10^8 + \\sqrt{10^{16} - 4}} \\end{pmatrix}}$$", "id": "2435764"}, {"introduction": "Numerical differentiation is a cornerstone of computational modeling, but its accuracy hinges on a careful choice of step size, $h$. This practice [@problem_id:2447368] moves beyond simple cancellation to explore the fundamental trade-off between truncation error, which arises from the mathematical approximation, and round-off error, which is a consequence of finite-precision hardware. By both deriving and empirically finding an optimal step size, you will gain a tangible understanding of how these two error sources interact to define the limits of computational accuracy.", "problem": "Implement a program that investigates truncation and round-off error in the forward-difference numerical differentiation of the exponential function. Consider the derivative of the function $f(x) = \\exp(x)$ at the point $x = 1$. Use the forward-difference approximation $f'(x) \\approx \\dfrac{f(x+h) - f(x)}{h}$. Your tasks are to derive an error model from first principles, use it to obtain a theoretical optimal step size $h$, and then empirically validate this prediction by scanning a range of step sizes $h$ in floating-point arithmetic.\n\nBase your derivation only on the following widely accepted principles:\n- The Taylor expansion of a sufficiently smooth function about a point $x$: $f(x+h) = f(x) + f'(x)h + \\dfrac{1}{2} f''(x) h^2 + \\mathcal{O}(h^3)$ as $h \\to 0$.\n- The standard model for floating-point rounding to nearest in the Institute of Electrical and Electronics Engineers (IEEE) 754 arithmetic: for any basic operation and number $y$, $\\operatorname{fl}(y) = y(1+\\delta)$ with $|\\delta| \\le u$, where $u$ is the unit round-off. For a given machine epsilon $\\varepsilon$ defined as the gap between $1$ and the next representable number, $u = \\varepsilon/2$.\n\nTasks to perform:\n1. Derive from the above principles a leading-order absolute error model for the forward-difference approximation at $x=1$ that combines truncation error and round-off error in terms of $h$, $f(1)$, $f''(1)$, and the machine epsilon $\\varepsilon$. Then, from this model, obtain the theoretical optimal step size $h^\\star$ that minimizes the leading-order error. Express $h^\\star$ explicitly for $f(x) = \\exp(x)$ at $x=1$ in terms of $\\varepsilon$. All quantities are dimensionless.\n2. Implement a program that:\n   - Computes the theoretical $h^\\star$ using the machine epsilon $\\varepsilon$ associated with a specified floating-point data type.\n   - Empirically estimates the optimal $h$ by scanning a logarithmically spaced set of $h$ values in a specified interval and selecting the $h$ that minimizes the absolute error $|D(h) - f'(1)|$, where $D(h) = \\dfrac{f(1+h) - f(1)}{h}$ is computed in the target floating-point data type. The reference value $f'(1)$ must be evaluated with sufficiently high precision so that it does not bias the empirical evaluation. All quantities are dimensionless.\n   - Reports, for each test case, a list of three floats: the theoretical optimal step size $h^\\star$, the empirically optimal step size $h_{\\mathrm{emp}}$, and the minimal absolute error attained across the scan.\n3. Use the following test suite, which varies the floating-point precision and the scan interval to probe different error regimes. For each case, scan $200$ logarithmically spaced values of $h$ over the specified interval:\n   - Test case A (happy path, double precision): data type $\\texttt{float64}$, $h \\in [10^{-16}, 10^{-1}]$.\n   - Test case B (single precision): data type $\\texttt{float32}$, $h \\in [10^{-10}, 10^{0}]$.\n   - Test case C (boundary, round-off dominated window in double precision): data type $\\texttt{float64}$, $h \\in [10^{-20}, 10^{-14}]$.\n4. Final output format: your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a three-element list in the order $[h^\\star, h_{\\mathrm{emp}}, \\min\\_h |D(h) - f'(1)|]$ for one test case. For example, the structure must be exactly like $[[a_1,b_1,c_1],[a_2,b_2,c_2],[a_3,b_3,c_3]]$. All quantities are dimensionless real numbers.\n\nNo external input is required, and all quantities are dimensionless. Angles are not involved, and there are no percentages in the output.", "solution": "The problem statement is subjected to validation prior to any attempt at a solution.\n\nStep 1: Extract Givens\n- Function to differentiate: $f(x) = \\exp(x)$.\n- Point of differentiation: $x = 1$.\n- Numerical differentiation formula: Forward-difference approximation, $f'(x) \\approx \\dfrac{f(x+h) - f(x)}{h}$.\n- Principle 1 (Taylor expansion): $f(x+h) = f(x) + f'(x)h + \\dfrac{1}{2} f''(x) h^2 + \\mathcal{O}(h^3)$ as $h \\to 0$.\n- Principle 2 (Floating-point error model): For a number $y$, its floating-point representation is $\\operatorname{fl}(y) = y(1+\\delta)$ with $\\lvert\\delta\\rvert \\le u$, where $u = \\varepsilon/2$ is the unit round-off and $\\varepsilon$ is the machine epsilon.\n- Task 1: Derive a leading-order absolute error model for the approximation at $x=1$ combining truncation and round-off errors. From this model, derive the theoretical optimal step size $h^\\star$ for $f(x)=\\exp(x)$ at $x=1$ in terms of $\\varepsilon$.\n- Task 2: Implement a program to compute the theoretical $h^\\star$ and find an empirical optimal step size $h_{\\mathrm{emp}}$ by minimizing the absolute error over a scanned range of $h$ values.\n- Task 3: The program must be run for three test cases:\n    - Case A: `float64` precision, $h \\in [10^{-16}, 10^{-1}]$.\n    - Case B: `float32` precision, $h \\in [10^{-10}, 10^{0}]$.\n    - Case C: `float64` precision, $h \\in [10^{-20}, 10^{-14}]$.\n    - For all cases, the scan must use $200$ logarithmically spaced points.\n- Task 4: The final output must be a single string representing a list of lists, with each inner list being $[h^\\star, h_{\\mathrm{emp}}, \\text{min_error}]$. All quantities are dimensionless.\n\nStep 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is firmly located within the standard corpus of numerical analysis and computational engineering. It addresses the fundamental trade-off between truncation error and round-off error in finite difference methods. The function, principles, and error models are all standard and factually correct.\n- **Well-Posed:** The problem is well-defined. It specifies the function, the approximation method, the point of evaluation, the principles for derivation, and the precise tasks for computation and reporting. A unique analytical result for $h^\\star$ can be derived, and the empirical search is clearly specified.\n- **Objective:** The problem is stated in precise, objective language, free of ambiguity or subjective claims.\n\nStep 3: Verdict and Action\nThe problem statement is valid. It is a standard, well-posed problem in computational science. I will proceed with the derivation and implementation.\n\n**Derivation of the Error Model and Optimal Step Size**\n\nThe objective is to analyze the total error in the computed forward-difference approximation $\\hat{D}(h)$ of the derivative of $f(x) = \\exp(x)$ at $x=1$. The total error is the sum of two components: the truncation error, which arises from the approximation of the derivative by a finite difference, and the round-off error, which arises from the finite precision of floating-point arithmetic.\n\n1.  **Truncation Error Analysis**\n    The exact forward-difference quotient is $D(h) = \\dfrac{f(x+h) - f(x)}{h}$. We use the Taylor expansion of $f(x+h)$ around $x$:\n    $$f(x+h) = f(x) + h f'(x) + \\frac{h^2}{2} f''(x) + \\mathcal{O}(h^3)$$\n    Substituting this into the expression for $D(h)$:\n    $$D(h) = \\frac{\\left( f(x) + h f'(x) + \\frac{h^2}{2} f''(x) + \\mathcal{O}(h^3) \\right) - f(x)}{h} = f'(x) + \\frac{h}{2}f''(x) + \\mathcal{O}(h^2)$$\n    The truncation error, $E_{\\text{trunc}}(h)$, is the difference between the approximation and the true derivative:\n    $$E_{\\text{trunc}}(h) = D(h) - f'(x) = \\frac{h}{2}f''(x) + \\mathcal{O}(h^2)$$\n    The leading-order absolute truncation error is therefore $\\lvert\\frac{h}{2}f''(x)\\rvert$.\n\n2.  **Round-off Error Analysis**\n    In floating-point arithmetic, we compute $\\hat{D}(h) = \\operatorname{fl}\\left(\\dfrac{\\operatorname{fl}(f(x+h)) - \\operatorname{fl}(f(x))}{h}\\right)$. Let us analyze the error in the numerator first. Let $y_1 = f(x)$ and $y_2 = f(x+h)$. Their computed values are:\n    $$\\hat{y}_1 = \\operatorname{fl}(y_1) = y_1(1+\\delta_1)$$\n    $$\\hat{y}_2 = \\operatorname{fl}(y_2) = y_2(1+\\delta_2)$$\n    where $\\lvert\\delta_1\\rvert, \\lvert\\delta_2\\rvert \\le u$, and $u = \\varepsilon/2$ is the unit round-off.\n    The subtraction is also subject to rounding:\n    $$\\operatorname{fl}(\\hat{y}_2 - \\hat{y}_1) = (\\hat{y}_2 - \\hat{y}_1)(1+\\delta_3) = (y_2(1+\\delta_2) - y_1(1+\\delta_1))(1+\\delta_3)$$\n    Expanding and retaining only first-order terms in $\\delta_i$:\n    $$\\operatorname{fl}(\\hat{y}_2 - \\hat{y}_1) \\approx (y_2 - y_1) + y_2\\delta_2 - y_1\\delta_1$$\n    The round-off error in the numerator is approximately $y_2\\delta_2 - y_1\\delta_1$. As $h \\to 0$, $y_2 = f(x+h) \\approx f(x) = y_1$. The error in the computed numerator is thus bounded by:\n    $$\\lvert y_2\\delta_2 - y_1\\delta_1 \\rvert \\le \\lvert y_2 \\rvert\\lvert\\delta_2\\rvert + \\lvert y_1 \\rvert\\lvert\\delta_1\\rvert \\approx \\lvert f(x) \\rvert u + \\lvert f(x) \\rvert u = 2u\\lvert f(x) \\rvert = \\varepsilon\\lvert f(x) \\rvert$$\n    This error is then divided by $h$. The round-off error in the final result, $E_{\\text{round}}(h)$, is dominated by the error from the numerator. Thus, the leading-order absolute round-off error is:\n    $$\\lvert E_{\\text{round}}(h) \\rvert \\approx \\frac{\\varepsilon \\lvert f(x) \\rvert}{h}$$\n\n3.  **Total Error and Optimal Step Size**\n    The total absolute error, $\\mathcal{E}(h)$, is the sum of the magnitudes of the leading-order truncation and round-off errors:\n    $$\\mathcal{E}(h) \\approx \\lvert E_{\\text{trunc}}(h) \\rvert + \\lvert E_{\\text{round}}(h) \\rvert = \\frac{h}{2}\\lvert f''(x) \\rvert + \\frac{\\varepsilon \\lvert f(x) \\rvert}{h}$$\n    To find the step size $h^\\star$ that minimizes this total error, we differentiate $\\mathcal{E}(h)$ with respect to $h$ and set the result to zero:\n    $$\\frac{d\\mathcal{E}}{dh} = \\frac{1}{2}\\lvert f''(x) \\rvert - \\frac{\\varepsilon \\lvert f(x) \\rvert}{h^2} = 0$$\n    Solving for $h^2$:\n    $$h^2 = \\frac{2\\varepsilon \\lvert f(x) \\rvert}{\\lvert f''(x) \\rvert}$$\n    This gives the optimal step size:\n    $$h^\\star = \\sqrt{\\frac{2\\varepsilon \\lvert f(x) \\rvert}{\\lvert f''(x) \\rvert}}$$\n\n4.  **Application to $f(x) = \\exp(x)$ at $x=1$**\n    For the given function $f(x) = e^x$, we have $f'(x) = e^x$ and $f''(x) = e^x$. At the point $x=1$:\n    $$f(1) = e^1 = e$$\n    $$f''(1) = e^1 = e$$\n    Since $e > 0$, the absolute value signs can be removed. Substituting these into the formula for $h^\\star$:\n    $$h^\\star = \\sqrt{\\frac{2\\varepsilon \\cdot e}{e}} = \\sqrt{2\\varepsilon}$$\n    This is the theoretical optimal step size for the forward-difference approximation of the derivative of $e^x$ at $x=1$. It depends only on the machine epsilon $\\varepsilon$ of the floating-point arithmetic used. This completes the derivation. The implementation will now proceed to find this $h^\\star$ and compare it with empirically determined values.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Investigates truncation and round-off error in the forward-difference\n    numerical differentiation of f(x) = exp(x) at x = 1.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each case is (data_type_string, h_min, h_max).\n    test_cases = [\n        ('float64', 1e-16, 1e-1),\n        ('float32', 1e-10, 1e0),\n        ('float64', 1e-20, 1e-14),\n    ]\n\n    # Use a high-precision value for the true derivative f'(1) = e.\n    # np.longdouble provides higher precision than float64, preventing a bias\n    # in the error calculation.\n    true_derivative = np.exp(np.longdouble(1))\n\n    results = []\n    \n    for dtype_str, h_min, h_max in test_cases:\n        # 1. Set up the environment for the current test case.\n        if dtype_str == 'float64':\n            dtype = np.float64\n        elif dtype_str == 'float32':\n            dtype = np.float32\n        else:\n            raise ValueError(f\"Unsupported data type: {dtype_str}\")\n\n        # Get machine epsilon for the current data type.\n        eps = np.finfo(dtype).eps\n\n        # 2. Calculate the theoretical optimal step size h_star.\n        # As derived, h_star = sqrt(2 * epsilon).\n        h_star = np.sqrt(2 * eps)\n\n        # 3. Perform the empirical scan to find the optimal h.\n        \n        # Define the point of differentiation and the function f(x) = e^x,\n        # ensuring calculations use the specified data type.\n        x_val = dtype(1.0)\n        f = lambda val: np.exp(val, dtype=dtype)\n        \n        # Generate 200 logarithmically spaced values for h in the given interval.\n        # These values are cast to the target data type.\n        h_values = np.logspace(np.log10(h_min), np.log10(h_max), 200, dtype=dtype)\n        \n        min_abs_error = np.inf\n        h_emp = np.nan\n        \n        for h in h_values:\n            # Ensure h is not zero, which can happen with very small logspace ends.\n            if h == 0:\n                continue\n\n            # Calculate the forward-difference approximation D(h).\n            # The calculation is performed entirely in the target precision.\n            D_h = (f(x_val + h) - f(x_val)) / h\n            \n            # Calculate the absolute error. The subtraction is done with D_h promoted\n            # to the higher precision of true_derivative.\n            abs_error = np.abs(D_h - true_derivative)\n            \n            # Update the minimum error and corresponding h.\n            if abs_error  min_abs_error:\n                min_abs_error = abs_error\n                h_emp = h\n        \n        # Cast the minimum error back to a standard Python float for consistent output.\n        min_abs_error_float = float(min_abs_error)\n        \n        # 4. Store the results for this test case.\n        results.append([h_star, h_emp, min_abs_error_float])\n\n    # Final print statement in the exact required format.\n    # The output should look like [[a1,b1,c1],[a2,b2,c2],[a3,b3,c3]].\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2447368"}, {"introduction": "Summing a sequence of numbers seems trivial, yet it is a task where round-off errors can accumulate to disastrous effect, particularly when dealing with large datasets or numbers of varying magnitudes. This final practice [@problem_id:2447409] introduces a powerful solution: the Kahan summation algorithm, a classic method of compensated summation. By implementing and comparing this algorithm against a naive approach, you will learn how to actively mitigate round-off error and appreciate the profound impact that algorithm choice has on numerical precision.", "problem": "You must write a complete, runnable program that evaluates round-off error in summing sequences of real numbers and demonstrates error reduction using the Kahan summation algorithm. All computations are to be carried out in standard double-precision binary floating-point arithmetic. For each test case, compute two sums of the same sequence: a naive left-to-right floating-point sum and a compensated sum using the Kahan summation algorithm. For each sum, compute the absolute error with respect to a high-accuracy reference sum. Your program must then output a single line containing all absolute errors for all test cases in a specified order and format.\n\nDefine absolute error for a computed sum $\\hat{S}$ relative to a reference value $S^{\\star}$ as $E = \\lvert \\hat{S} - S^{\\star} \\rvert$.\n\nThe test suite consists of the following four sequences:\n\n- Test case $1$ (many tiny increments added to a large baseline):\n  - Sequence $S_1$ has length $N_1 + 1$, where $N_1 = 10^{6}$. The first term is $s^{(1)}_0 = 1$, and the remaining $N_1$ terms are $s^{(1)}_k = 10^{-16}$ for $1 \\le k \\le N_1$.\n\n- Test case $2$ (repeated catastrophic cancellation triplets):\n  - Let $M = 2 \\cdot 10^{5}$. Sequence $S_2$ is the concatenation of $M$ blocks of three terms $(1, 10^{-16}, -1)$.\n\n- Test case $3$ (deterministic pseudo-random small-magnitude values with slight bias):\n  - Let the modulus $m = 2^{64}$, multiplier $a = 6364136223846793005$, increment $c = 1442695040888963407$, and seed $x_0 = 123456789123456789$. Define a linear congruential generator by $x_{k+1} \\equiv a x_k + c \\pmod{m}$ for $k \\ge 0$. Let $N_3 = 5 \\cdot 10^{4}$. For $k = 1, 2, \\dots, N_3$, define $u_k = \\frac{x_k}{m} - \\frac{1}{2}$ and the sequence term $s^{(3)}_k = 10^{-12} u_k + 10^{-16}$. The sequence $S_3$ consists of these $N_3$ terms.\n\n- Test case $4$ (dynamic range and cancellation in a short sequence):\n  - Sequence $S_4$ has five terms: $(10^{16}, 1, -10^{16}, 3, 4 \\cdot 10^{-16})$.\n\nFor each test case $i \\in \\{1,2,3,4\\}$, compute:\n- The naive sum $\\hat{S}^{\\text{naive}}_i$ by left-to-right accumulation in double-precision floating-point arithmetic.\n- The Kahan-compensated sum $\\hat{S}^{\\text{Kahan}}_i$ using the Kahan summation algorithm in double-precision floating-point arithmetic.\n- A high-accuracy reference $S^{\\star}_i$ computed from the mathematical definition of the sequence using exact arithmetic where possible or using base-$10$ arbitrary precision arithmetic with at least $50$ correct decimal digits such that rounding in double-precision does not contaminate $S^{\\star}_i$.\n\nFor each test case $i$, compute the absolute errors $E^{\\text{naive}}_i = \\lvert \\hat{S}^{\\text{naive}}_i - S^{\\star}_i \\rvert$ and $E^{\\text{Kahan}}_i = \\lvert \\hat{S}^{\\text{Kahan}}_i - S^{\\star}_i \\rvert$.\n\nFinal output format:\n- Produce a single line of output containing a comma-separated list enclosed in square brackets. The list must contain the $8$ numbers in the following order:\n  - $E^{\\text{naive}}_1, E^{\\text{Kahan}}_1, E^{\\text{naive}}_2, E^{\\text{Kahan}}_2, E^{\\text{naive}}_3, E^{\\text{Kahan}}_3, E^{\\text{naive}}_4, E^{\\text{Kahan}}_4$.\n- Each number must be rounded to $12$ significant digits, expressed as a decimal (scientific notation is acceptable).\n- Example of the required single-line format (illustrative only): $[e_1,e_2,e_3,e_4,e_5,e_6,e_7,e_8]$.\n\nNo physical units or angle units are involved in this problem. The program must be self-contained and must not require any user input or external files. The results must be reproducible exactly from the definitions above in any modern programming language that adheres to standard double-precision floating-point semantics.", "solution": "The problem statement has been analyzed and is determined to be valid. It is scientifically grounded in the principles of numerical analysis, specifically concerning floating-point arithmetic and round-off error. The problem is well-posed, with all necessary data and definitions provided to compute a unique, verifiable solution. It is objective and free from ambiguity.\n\nThe core of this problem is to demonstrate and quantify the loss of precision that occurs during the summation of floating-point numbers of disparate magnitudes and the mitigation of this error using a compensated summation algorithm.\n\nAll calculations are performed using standard double-precision floating-point arithmetic, which corresponds to the IEEE 754 $64$-bit format. This format has approximately $15$ to $17$ decimal digits of precision. The machine epsilon, $\\epsilon$, which is the smallest number such that $1.0 + \\epsilon  1.0$, is approximately $2.22 \\times 10^{-16}$ for this format. When two numbers of vastly different magnitudes are added, the smaller number may be partially or completely lost. This phenomenon is known as swamping.\n\nThe first method of summation is naive, left-to-right accumulation. For a sequence $s_0, s_1, \\dots, s_N$, the sum $\\hat{S}^{\\text{naive}}$ is computed as $(\\dots((s_0 + s_1) + s_2) + \\dots + s_N)$. This method is highly susceptible to round-off error.\n\nThe second method is the Kahan summation algorithm, a method of compensated summation. It significantly reduces the numerical error in the total obtained by adding a sequence of finite-precision floating-point numbers. The algorithm maintains a running compensation variable, $c$, which accumulates the error that would otherwise be lost. For each term $s_k$ in the sequence, the update rules are:\n$$y_k = s_k - c_{k-1}$$\n$$t_k = \\text{sum}_{k-1} + y_k$$\n$$c_k = (t_k - \\text{sum}_{k-1}) - y_k$$\n$$\\text{sum}_k = t_k$$\nHere, $\\text{sum}_0 = 0$ and $c_0 = 0$. The term $(t_k - \\text{sum}_{k-1})$ recovers the high-order part of $y_k$, and subtracting $y_k$ from this isolates the low-order part (the round-off error), which is stored in $c_k$ and subtracted from the next term $s_{k+1}$.\n\nThe absolute error is defined as $E = \\lvert \\hat{S} - S^{\\star} \\rvert$, where $\\hat{S}$ is a computed sum and $S^{\\star}$ is a high-accuracy reference sum.\n\nAnalysis of Test Cases:\n\nTest Case $1$:\nThe sequence is $s^{(1)}_0 = 1$ followed by $N_1 = 10^6$ terms of $s^{(1)}_k = 10^{-16}$ for $k \\ge 1$.\nThe exact sum is $S^{\\star}_1 = 1 + 10^6 \\times 10^{-16} = 1 + 10^{-10}$.\nIn naive summation, we compute $1 + 10^{-16} + 10^{-16} + \\dots$. The term $10^{-16}$ is very close to the machine epsilon relative to $1.0$. The operation $1.0 + 10^{-16}$ in double-precision arithmetic will suffer from swamping; the result is likely to be rounded back to $1.0$. Thus, most of the small terms will be lost, and $\\hat{S}^{\\text{naive}}_1$ is expected to be very close to $1.0$, resulting in an error close to $10^{-10}$.\nThe Kahan algorithm will capture the lost part $10^{-16}$ in the compensation variable $c$ at each step and reintroduce it, yielding a result $\\hat{S}^{\\text{Kahan}}_1$ that is extremely close to $S^{\\star}_1$. The error $E^{\\text{Kahan}}_1$ should be near machine precision.\n\nTest Case $2$:\nThe sequence consists of $M = 2 \\cdot 10^5$ blocks of $(1, 10^{-16}, -1)$.\nThe exact sum of one block is $1 + 10^{-16} - 1 = 10^{-16}$. The total exact sum is $S^{\\star}_2 = 2 \\cdot 10^5 \\times 10^{-16} = 2 \\cdot 10^{-11}$.\nNaive summation will compute $(1 + 10^{-16}) - 1$. As in the first case, $1 + 10^{-16}$ will likely round to $1.0$, and thus $(1 + 10^{-16}) - 1$ evaluates to $0$. Repeating this for all blocks, $\\hat{S}^{\\text{naive}}_2$ is expected to be $0.0$, leading to an error $E^{\\text{naive}}_2$ of exactly $2 \\cdot 10^{-11}$.\nThe Kahan algorithm will prevent this cancellation error, producing a sum $\\hat{S}^{\\text{Kahan}}_2$ very close to $S^{\\star}_2$, and a much smaller error $E^{\\text{Kahan}}_2$.\n\nTest Case $3$:\nThe sequence consists of $N_3 = 5 \\cdot 10^4$ terms $s^{(3)}_k = 10^{-12} u_k + 10^{-16}$, where $u_k = \\frac{x_k}{m} - \\frac{1}{2}$ and $x_k$ is from an LCG. The values of $u_k$ are pseudo-random in $[-0.5, 0.5)$. The terms $s^{(3)}_k$ are small, with a small positive bias of $10^{-16}$.\nThe exact sum is $S^{\\star}_3 = \\sum_{k=1}^{N_3} (10^{-12} u_k + 10^{-16}) = 10^{-12} \\sum_{k=1}^{N_3} u_k + N_3 \\cdot 10^{-16}$.\nThis sum must be computed using high-precision arithmetic to serve as the reference $S^{\\star}_3$. The LCG states $x_{k+1} \\equiv a x_k + c \\pmod{m}$ are computed using $64$-bit integer arithmetic. The sum $\\sum x_k$ is computed using arbitrary-precision integers, and the final expression for $S^{\\star}_3$ is evaluated using high-precision decimal arithmetic.\nNaive summation will accumulate small round-off errors over the $5 \\cdot 10^4$ additions. The Kahan algorithm is expected to minimize this accumulation, leading to $E^{\\text{Kahan}}_3 \\ll E^{\\text{naive}}_3$.\n\nTest Case $4$:\nThe sequence is $(10^{16}, 1, -10^{16}, 3, 4 \\cdot 10^{-16})$.\nThe exact sum is $S^{\\star}_4 = (10^{16} - 10^{16}) + (1 + 3) + 4 \\cdot 10^{-16} = 4 + 4 \\cdot 10^{-16}$.\nNaive left-to-right summation calculates step-by-step:\n1. $10^{16} + 1 = 10^{16}$ (swamping, since $1$ is less than the unit in the last place of $10^{16}$).\n2. $10^{16} - 10^{16} = 0$.\n3. $0 + 3 = 3$.\n4. $3 + 4 \\cdot 10^{-16} = 3$ (swamping, since $4 \\cdot 10^{-16}$ is smaller than machine epsilon relative to $3$).\nSo, $\\hat{S}^{\\text{naive}}_4 = 3$. The error is $E^{\\text{naive}}_4 = \\lvert 3 - (4 + 4 \\cdot 10^{-16}) \\rvert \\approx 1$.\nThe Kahan summation algorithm is designed to handle this. The loss of $1$ in the first step will be captured by the compensation variable. The final sum $\\hat{S}^{\\text{Kahan}}_4$ should be very close to the true sum $S^{\\star}_4$, resulting in a very small error $E^{\\text{Kahan}}_4$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport decimal\n\ndef solve():\n    \"\"\"\n    Computes and prints round-off errors for naive and Kahan summations\n    for four specific test cases, adhering to the problem specification.\n    \"\"\"\n\n    def naive_sum(sequence):\n        \"\"\"Computes the naive left-to-right sum of a sequence.\"\"\"\n        s = 0.0\n        for x in sequence:\n            s += x\n        return s\n\n    def kahan_sum(sequence):\n        \"\"\"Computes the sum of a sequence using Kahan's algorithm.\"\"\"\n        s = 0.0\n        c = 0.0\n        for x in sequence:\n            y = x - c\n            t = s + y\n            c = (t - s) - y\n            s = t\n        return s\n\n    def generate_test_cases():\n        \"\"\"Generates the sequences for all four test cases.\"\"\"\n        # Test Case 1: Many tiny increments\n        N1 = 10**6\n        seq1 = np.full(N1, 1e-16, dtype=np.float64)\n        seq1 = np.insert(seq1, 0, 1.0)\n        \n        # Test Case 2: Repeated catastrophic cancellation\n        M = 2 * 10**5\n        block = np.array([1.0, 1e-16, -1.0], dtype=np.float64)\n        seq2 = np.tile(block, M)\n        \n        # Test Case 3: LCG-based sequence\n        m = 2**64\n        a = 6364136223846793005\n        c = 1442695040888963407\n        x0 = 123456789123456789\n        N3 = 5 * 10**4\n        \n        seq3 = np.zeros(N3, dtype=np.float64)\n        x_current = x0\n        for k in range(N3):\n            x_current = (a * x_current + c) % m\n            u_k = x_current / m - 0.5\n            seq3[k] = 1e-12 * u_k + 1e-16\n\n        # Test Case 4: Dynamic range and cancellation\n        seq4 = np.array([1e16, 1.0, -1e16, 3.0, 4e-16], dtype=np.float64)\n        \n        return [seq1, seq2, seq3, seq4]\n\n    def get_reference_sums():\n        \"\"\"Computes high-accuracy reference sums for all test cases.\"\"\"\n        # Set precision for Decimal calculations\n        decimal.getcontext().prec = 100\n\n        # Reference Sum 1\n        N1 = 10**6\n        s_star_1 = decimal.Decimal(1) + decimal.Decimal(N1) * decimal.Decimal('1e-16')\n\n        # Reference Sum 2\n        M = 2 * 10**5\n        s_star_2 = decimal.Decimal(M) * decimal.Decimal('1e-16')\n\n        # Reference Sum 3\n        m = 2**64\n        a = 6364136223846793005\n        c = 1442695040888963407\n        x0 = 123456789123456789\n        N3 = 5 * 10**4\n        \n        sum_x = 0\n        x_current = x0\n        for _ in range(N3):\n            x_current = (a * x_current + c) % m\n            sum_x += x_current\n        \n        D_sum_x = decimal.Decimal(sum_x)\n        D_m = decimal.Decimal(m)\n        D_N3 = decimal.Decimal(N3)\n        D_1e_12 = decimal.Decimal('1e-12')\n        D_1e_16 = decimal.Decimal('1e-16')\n        D_half = decimal.Decimal('0.5')\n        \n        sum_u = D_sum_x / D_m - D_N3 * D_half\n        s_star_3 = D_1e_12 * sum_u + D_N3 * D_1e_16\n\n        # Reference Sum 4\n        s_star_4 = decimal.Decimal('4') + decimal.Decimal('4e-16')\n        \n        return [float(s_star_1), float(s_star_2), float(s_star_3), float(s_star_4)]\n\n    sequences = generate_test_cases()\n    reference_sums = get_reference_sums()\n    \n    results = []\n    \n    for i in range(4):\n        seq = sequences[i]\n        s_star = reference_sums[i]\n        \n        # Naive sum and its error\n        s_naive = naive_sum(seq)\n        e_naive = abs(s_naive - s_star)\n        \n        # Kahan sum and its error\n        s_kahan = kahan_sum(seq)\n        e_kahan = abs(s_kahan - s_star)\n        \n        results.extend([e_naive, e_kahan])\n\n    # Format output to 12 significant digits and print\n    formatted_results = [f\"{res:.12g}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2447409"}]}