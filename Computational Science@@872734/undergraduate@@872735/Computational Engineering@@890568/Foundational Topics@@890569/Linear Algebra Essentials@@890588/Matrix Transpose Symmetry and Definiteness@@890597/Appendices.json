{"hands_on_practices": [{"introduction": "In engineering, symmetric positive definite (SPD) matrices often represent systems with inherent stability, like the stiffness matrix of a structure. This exercise explores a fundamental question: if we combine two stable systems, is the resulting system also stable? By proving that the sum of two SPD matrices is also SPD, you will reinforce your understanding of this crucial property and its physical implications [@problem_id:1352981].", "problem": "In linear algebra, a real square matrix $A$ is defined as **symmetric** if it is equal to its transpose, i.e., $A = A^T$. Such a matrix is further defined as **positive definite** if the scalar quantity $x^T A x$ is strictly positive for every non-zero column vector $x$ of appropriate dimension. A fundamental theorem states that a real, symmetric matrix possesses a unique Cholesky factorization of the form $A = LL^T$, where $L$ is a lower triangular matrix with positive diagonal entries, if and only if the matrix $A$ is positive definite.\n\nConsider two $n \\times n$ matrices, $A$ and $B$, which are both known to be symmetric and positive definite. Let $C$ be their sum, $C = A + B$. Based on these properties, which of the following statements about the matrix $C$ is always true?\n\nA. The matrix $C$ will always have a Cholesky factorization.\n\nB. The matrix $C$ will have a Cholesky factorization only if $A$ and $B$ commute (i.e., $AB = BA$).\n\nC. The matrix $C$ will have a Cholesky factorization only if one matrix is a positive scalar multiple of the other (i.e., $A = k B$ for some $k > 0$).\n\nD. The matrix $C$ will never have a Cholesky factorization.\n\nE. Whether $C$ has a Cholesky factorization depends on the specific numerical entries of $A$ and $B$ and cannot be determined in general from the given information.", "solution": "Given $A$ and $B$ are $n \\times n$ real, symmetric, positive definite matrices. Define $C = A + B$.\n\n1) Symmetry of $C$:\nSince $A = A^{T}$ and $B = B^{T}$, it follows that\n$$\nC^{T} = (A + B)^{T} = A^{T} + B^{T} = A + B = C,\n$$\nso $C$ is symmetric.\n\n2) Positive definiteness of $C$:\nBy the definition of positive definiteness, for any nonzero vector $x \\in \\mathbb{R}^{n}$,\n$$\nx^{T} A x > 0 \\quad \\text{and} \\quad x^{T} B x > 0.\n$$\nTherefore,\n$$\nx^{T} C x = x^{T} (A + B) x = x^{T} A x + x^{T} B x > 0 + 0 = 0,\n$$\nfor every nonzero $x$. Hence $C$ is positive definite.\n\n3) Cholesky factorization criterion:\nA real, symmetric matrix has a Cholesky factorization $C = L L^{T}$ with $L$ lower triangular with positive diagonal entries if and only if it is positive definite. Since $C$ is symmetric and positive definite, $C$ has a (unique) Cholesky factorization.\n\nNo additional conditions such as commutativity $AB = BA$ or proportionality $A = k B$ with $k > 0$ are required, and this conclusion does not depend on the specific entries of $A$ and $B$ beyond their symmetry and positive definiteness.\n\nTherefore, the statement that is always true is that $C$ will always have a Cholesky factorization.", "answer": "$$\\boxed{A}$$", "id": "1352981"}, {"introduction": "The quadratic form $x^{\\mathsf{T}} A x$ is central to computational engineering, often representing quantities like energy or error. This problem invites you to investigate a fascinating case where this quantity is zero for all possible states $x$. By deriving the necessary constraint on the matrix $A$, you will uncover the hidden role of its symmetric and skew-symmetric parts and see how abstract properties can inform a constrained optimization problem [@problem_id:2412126].", "problem": "In computational engineering, particularly in the Finite Element Method (FEM), the energy associated with a stiffness-like operator is captured by the quadratic form $x^{\\mathsf{T}} A x$. Consider real matrices $A \\in \\mathbb{R}^{n \\times n}$ and the identity $x^{\\mathsf{T}} A x = 0$ holding for all $x \\in \\mathbb{R}^{n}$.\n\n1. Using only the definitions of transpose, symmetry, and the properties of the scalar $x^{\\mathsf{T}} A x$, derive from first principles the structural constraint on $A$ that is implied by the identity $x^{\\mathsf{T}} A x = 0$ for all $x \\in \\mathbb{R}^{n}$.\n\n2. Now specialize to $\\mathbb{R}^{3 \\times 3}$ and consider the design problem: among all real matrices $A \\in \\mathbb{R}^{3 \\times 3}$ that satisfy $x^{\\mathsf{T}} A x = 0$ for all $x \\in \\mathbb{R}^{3}$ and additionally satisfy the entry constraint $a_{12} = 2$, determine the minimal possible Frobenius norm $\\|A\\|_{F}$, where $\\|A\\|_{F} = \\sqrt{\\sum_{i=1}^{3} \\sum_{j=1}^{3} a_{ij}^{2}}$. Report the minimal norm as an exact value.\n\nYour final answer must be the minimal Frobenius norm as a single real number. No rounding is required.", "solution": "We proceed from first principles.\n\nPart 1. Let $A \\in \\mathbb{R}^{n \\times n}$ and assume $x^{\\mathsf{T}} A x = 0$ for all $x \\in \\mathbb{R}^{n}$. For arbitrary $u, v \\in \\mathbb{R}^{n}$, expand\n\n$$\n(u+v)^{\\mathsf{T}} A (u+v) = u^{\\mathsf{T}} A u + u^{\\mathsf{T}} A v + v^{\\mathsf{T}} A u + v^{\\mathsf{T}} A v.\n$$\n\nBy the hypothesis, $u^{\\mathsf{T}} A u = 0$ and $v^{\\mathsf{T}} A v = 0$ for all $u, v$, so the expansion gives\n\n$$\n0 = (u+v)^{\\mathsf{T}} A (u+v) = u^{\\mathsf{T}} A v + v^{\\mathsf{T}} A u.\n$$\n\nNote that $v^{\\mathsf{T}} A u = (u^{\\mathsf{T}} A^{\\mathsf{T}} v)$. Therefore\n\n$$\n0 = u^{\\mathsf{T}} A v + u^{\\mathsf{T}} A^{\\mathsf{T}} v = u^{\\mathsf{T}} (A + A^{\\mathsf{T}}) v\n$$\n\nfor all $u, v \\in \\mathbb{R}^{n}$. Define the symmetric part $S = \\tfrac{1}{2}(A + A^{\\mathsf{T}})$. Then the above implies $u^{\\mathsf{T}} S v = 0$ for all $u, v$. Choosing $u = e_{i}$ and $v = e_{j}$, the standard basis vectors, yields $e_{i}^{\\mathsf{T}} S e_{j} = s_{ij} = 0$ for all indices, hence $S = 0$. Therefore $A + A^{\\mathsf{T}} = 0$, i.e., $A$ is skew-symmetric. Thus, the identity $x^{\\mathsf{T}} A x = 0$ for all $x$ forces $A$ to be skew-symmetric.\n\nPart 2. We now minimize the Frobenius norm over all $A \\in \\mathbb{R}^{3 \\times 3}$ that are skew-symmetric and satisfy $a_{12} = 2$. A real $3 \\times 3$ skew-symmetric matrix has the form\n\n$$\nA = \\begin{pmatrix}\n0  a_{12}  a_{13} \\\\\n-a_{12}  0  a_{23} \\\\\n-a_{13}  -a_{23}  0\n\\end{pmatrix}.\n$$\n\nImposing $a_{12} = 2$ yields\n\n$$\nA = \\begin{pmatrix}\n0  2  a_{13} \\\\\n-2  0  a_{23} \\\\\n-a_{13}  -a_{23}  0\n\\end{pmatrix},\n$$\n\nwith free parameters $a_{13}, a_{23} \\in \\mathbb{R}$. The Frobenius norm squared is\n\n$$\n\\|A\\|_{F}^{2} = \\sum_{i=1}^{3} \\sum_{j=1}^{3} a_{ij}^{2} = 0^{2} + 2^{2} + a_{13}^{2} + (-2)^{2} + 0^{2} + a_{23}^{2} + (-a_{13})^{2} + (-a_{23})^{2} + 0^{2}.\n$$\n\nThis simplifies to\n\n$$\n\\|A\\|_{F}^{2} = 4 + 4 + 2 a_{13}^{2} + 2 a_{23}^{2} = 8 + 2\\left(a_{13}^{2} + a_{23}^{2}\\right).\n$$\n\nThe expression is minimized by choosing $a_{13} = 0$ and $a_{23} = 0$, which yields the minimal value\n\n$$\n\\|A\\|_{F,\\min} = \\sqrt{8} = 2 \\sqrt{2}.\n$$\n\nThe corresponding minimizing matrix\n\n$$\nA_{\\min} = \\begin{pmatrix}\n0  2  0 \\\\\n-2  0  0 \\\\\n0  0  0\n\\end{pmatrix}\n$$\n\nis real, non-zero, non-symmetric (indeed skew-symmetric), and satisfies $x^{\\mathsf{T}} A x = 0$ for all $x \\in \\mathbb{R}^{3}$ as required. Therefore, the minimal Frobenius norm is $2 \\sqrt{2}$.", "answer": "$$\\boxed{2\\sqrt{2}}$$", "id": "2412126"}, {"introduction": "Theoretical properties like matrix symmetry are most powerful when they lead to tangible computational advantages. This hands-on programming challenge tasks you with translating the concept of symmetry into a memory-efficient storage scheme and a corresponding matrix-vector multiplication algorithm. Successfully implementing and verifying this routine demonstrates how abstract linear algebra directly enables the solution of large-scale engineering problems that would otherwise be computationally intractable [@problem_id:2412069].", "problem": "You are given the task of designing a memory-efficient representation and matrix-vector multiplication routine for a real, sparse, symmetric matrix that stores only its upper-triangular nonzero entries (including the diagonal). You must implement a complete, runnable program that constructs this representation for a provided test suite, computes the matrix-vector product using only the stored data, evaluates symmetry and definiteness properties, and reports a quantitative memory comparison to a naive unsymmetrized coordinate storage.\n\nDefinitions and requirements:\n\n- Let $A \\in \\mathbb{R}^{n \\times n}$ be symmetric, i.e., $A^\\top = A$. The input data for $A$ are given as a list of triplets $(i,j,v)$ with $0 \\le i \\le j \\le n-1$, representing $A_{ij} = v$ and, by symmetry, $A_{ji} = v$ when $i \\ne j$.\n- Given a vector $x \\in \\mathbb{R}^n$, the task is to compute $y = A x$ using only the upper-triangular storage. All arithmetic must adhere to real-number operations.\n- Determine whether $A$ is symmetric positive definite (SPD), defined by $x^\\top A x  0$ for all nonzero $x \\in \\mathbb{R}^n$. Report a boolean indicating whether this property holds.\n- Verify two correctness properties using a dense reconstruction of $A$:\n  1. $y$ computed from the upper-triangular storage equals the dense product $A x$ within an absolute tolerance of $10^{-12}$; report this as a boolean.\n  2. $y$ equals $A^\\top x$ within an absolute tolerance of $10^{-12}$; report this as a boolean. Note that for symmetric $A$, $A^\\top = A$.\n- Quantify memory usage as follows. Consider a naive unsymmetrized coordinate storage (Coordinate list) that stores every nonzero entry $(i,j,v)$ appearing in the full symmetric matrix, i.e., it stores both $(i,j,v)$ and $(j,i,v)$ for every strictly off-diagonal nonzero and a single $(i,i,v)$ for diagonal entries. Assume $64$-bit integers for indices and $64$-bit floats for values, so each stored triplet consumes $24$ bytes. Your upper-triangular storage stores each provided triplet once. For each test case, report the integer number of bytes saved:\n  $$\\text{bytes\\_saved} = \\left(\\text{naive\\_entry\\_count} - \\text{upper\\_entry\\_count}\\right) \\times 24.$$\n- Final output format: For each test case, produce a list\n  $$[\\;y\\_as\\_list\\_of\\_floats,\\; \\text{is\\_spd\\_boolean},\\; \\text{matches\\_dense\\_boolean},\\; \\text{transpose\\_consistency\\_boolean},\\; \\text{bytes\\_saved\\_integer}\\;].$$\n  Your program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, in the same order as the test suite below, for example:\n  $$[\\text{case1\\_result},\\text{case2\\_result},\\ldots].$$\n\nTest suite (indices are zero-based, i.e., they start at $0$; there are no duplicate triplets in any case):\n\n- Case $1$:\n  - $n = 5$\n  - Upper-triangular triplets $(i,j,v)$:\n    - $(0,0,4.0)$, $(1,1,5.0)$, $(2,2,6.0)$, $(3,3,5.0)$, $(4,4,4.0)$\n    - $(0,1,1.0)$, $(1,2,0.5)$, $(2,3,1.0)$, $(3,4,0.5)$, $(0,4,0.2)$\n  - $x = [1.0, 2.0, 3.0, 4.0, 5.0]$\n\n- Case $2$:\n  - $n = 4$\n  - Upper-triangular triplets $(i,j,v)$:\n    - $(0,0,2.0)$, $(1,1,3.0)$, $(2,2,4.0)$, $(3,3,5.0)$\n  - $x = [1.0, 0.0, -1.0, 2.0]$\n\n- Case $3$:\n  - $n = 3$\n  - Upper-triangular triplets $(i,j,v)$:\n    - $(0,0,2.0)$, $(1,1,-1.0)$, $(2,2,3.0)$\n    - $(0,2,0.5)$\n  - $x = [1.0, -2.0, 0.5]$\n\n- Case $4$:\n  - $n = 1$\n  - Upper-triangular triplets $(i,j,v)$:\n    - $(0,0,0.0)$\n  - $x = [3.0]$\n\nYour program must produce the single-line output described above, where each caseâ€™s result is a list with the five items in the specified order.", "solution": "The problem presented is valid, well-posed, and grounded in the fundamental principles of computational linear algebra. It addresses the practical task of implementing a memory-efficient matrix-vector multiplication for a real, sparse, symmetric matrix and subsequently analyzing its properties. We shall proceed with a rigorous, step-by-step solution.\n\nThe core of this problem is the computation of the matrix-vector product $y = Ax$ for a symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$, where only the non-zero entries in the upper triangle (including the diagonal) are stored. Let the set of stored index-value triplets be $U = \\{(i, j, v) \\,|\\, A_{ij} = v, 0 \\le i \\le j  n\\}$.\n\nThe $k$-th component of the product vector $y$ is defined as $y_k = \\sum_{l=0}^{n-1} A_{kl} x_l$. To compute this using only the upper-triangular entries, we must consider how each stored entry contributes to the product. For each stored triplet $(i, j, v) \\in U$:\n\n$1$. If $i = j$, the entry is on the diagonal. This term $A_{ii} = v$ contributes only to the product component $y_i$. The contribution is $v \\cdot x_i$.\n\n$2$. If $i  j$, the entry is in the strict upper triangle. This term $A_{ij} = v$ contributes $v \\cdot x_j$ to the component $y_i$. Due to the symmetry of $A$, we have $A_{ji} = A_{ij} = v$. This corresponding lower-triangular entry contributes $A_{ji} \\cdot x_i = v \\cdot x_i$ to the component $y_j$.\n\nTherefore, a correct and efficient algorithm can be formulated by initializing a zero vector $y \\in \\mathbb{R}^n$ and iterating through the stored triplets:\n- For each diagonal triplet $(i, i, v)$, update $y_i \\leftarrow y_i + v \\cdot x_i$.\n- For each off-diagonal triplet $(i, j, v)$ with $i  j$, perform two updates: $y_i \\leftarrow y_i + v \\cdot x_j$ and $y_j \\leftarrow y_j + v \\cdot x_i$.\nThis procedure correctly computes the full product $Ax$ by leveraging symmetry.\n\nNext, we must determine if $A$ is symmetric positive definite (SPD). A symmetric matrix is, by definition, positive definite if and only if all of its eigenvalues are strictly positive. This provides the most robust criterion for the check. The procedure is as follows:\n$1$. Reconstruct the full dense matrix $A$ from the provided upper-triangular triplets. For each triplet $(i, j, v)$, we set $A_{ij} = v$ and, if $i \\neq j$, also $A_{ji} = v$.\n$2$. Compute the eigenvalues of the resulting symmetric matrix $A$. Specialized numerical routines for symmetric matrices, such as those that compute eigenvalues of a real symmetric matrix, are appropriate here.\n$3$. Check if every computed eigenvalue $\\lambda$ satisfies the condition $\\lambda > 0$. If this holds for all eigenvalues, the matrix is SPD.\n\nThe verification of correctness is straightforward. The product vector $y$ computed via the efficient symmetric algorithm is compared against a reference product $y_{\\text{dense}} = A_{\\text{dense}} x$, where $A_{\\text{dense}}$ is the fully reconstructed matrix. The comparison must be performed using an absolute tolerance of $10^{-12}$. Additionally, because $A$ is symmetric ($A = A^\\top$), the identity $Ax = A^\\top x$ must hold. This provides a second, albeit related, consistency check.\n\nFinally, we quantify the memory savings. The problem specifies that each stored triplet $(i, j, v)$ consumes $24$ bytes ($2 \\times 8$ bytes for integer indices and $8$ bytes for a double-precision float value).\n- The upper-triangular storage requires $\\text{upper\\_entry\\_count}$ triplets, which is the count of input data.\n- A naive coordinate storage would store all non-zero entries explicitly. For each strictly off-diagonal entry $A_{ij} \\neq 0$ ($i \\neq j$), it stores two triplets, $(i, j, A_{ij})$ and $(j, i, A_{ji})$. For each diagonal entry $A_{ii} \\neq 0$, it stores one triplet.\nThe number of off-diagonal triplets in the input is $\\text{num\\_off\\_diagonal}$. The total number of triplets in the naive storage is thus $\\text{naive\\_entry\\_count} = (\\text{upper\\_entry\\_count} - \\text{num\\_off\\_diagonal}) + 2 \\cdot \\text{num\\_off\\_diagonal} = \\text{upper\\_entry\\_count} + \\text{num\\_off\\_diagonal}$.\nThe bytes saved are calculated as $(\\text{naive\\_entry\\_count} - \\text{upper\\_entry\\_count}) \\times 24$, which simplifies to $\\text{num\\_off\\_diagonal} \\times 24$. This reflects the fact that our efficient scheme saves one triplet storage for each non-zero, off-diagonal pair.\n\nThe implementation will process each test case according to these principles, generating the required five-element list for each.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test suite.\n    \"\"\"\n    test_cases = [\n        {\n            \"n\": 5,\n            \"triplets\": [\n                (0, 0, 4.0), (1, 1, 5.0), (2, 2, 6.0), (3, 3, 5.0), (4, 4, 4.0),\n                (0, 1, 1.0), (1, 2, 0.5), (2, 3, 1.0), (3, 4, 0.5), (0, 4, 0.2)\n            ],\n            \"x\": [1.0, 2.0, 3.0, 4.0, 5.0]\n        },\n        {\n            \"n\": 4,\n            \"triplets\": [\n                (0, 0, 2.0), (1, 1, 3.0), (2, 2, 4.0), (3, 3, 5.0)\n            ],\n            \"x\": [1.0, 0.0, -1.0, 2.0]\n        },\n        {\n            \"n\": 3,\n            \"triplets\": [\n                (0, 0, 2.0), (1, 1, -1.0), (2, 2, 3.0),\n                (0, 2, 0.5)\n            ],\n            \"x\": [1.0, -2.0, 0.5]\n        },\n        {\n            \"n\": 1,\n            \"triplets\": [\n                (0, 0, 0.0)\n            ],\n            \"x\": [3.0]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = process_case(case[\"n\"], case[\"triplets\"], case[\"x\"])\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # The string representation of a list of lists.\n    print(str(results).replace(\" \", \"\"))\n\n\ndef process_case(n, triplets, x_vec):\n    \"\"\"\n    Processes a single test case according to the problem specification.\n\n    Args:\n        n (int): The dimension of the matrix.\n        triplets (list of tuple): The upper-triangular non-zero entries (i, j, v).\n        x_vec (list of float): The vector x.\n\n    Returns:\n        list: A list containing [y_as_list, is_spd, matches_dense, transpose_consistency, bytes_saved].\n    \"\"\"\n    x = np.array(x_vec, dtype=float)\n\n    # 1. Efficient matrix-vector multiplication y = Ax from upper-triangular storage\n    y = np.zeros(n, dtype=float)\n    for i, j, v in triplets:\n        if i == j:  # Diagonal element\n            y[i] += v * x[i]\n        else:  # Off-diagonal element, use symmetry\n            y[i] += v * x[j]\n            y[j] += v * x[i]\n\n    # 2. Reconstruct the full dense matrix for verification purposes\n    A_dense = np.zeros((n, n), dtype=float)\n    for i, j, v in triplets:\n        A_dense[i, j] = v\n        if i != j:\n            A_dense[j, i] = v\n\n    # 3. Determine if the matrix is Symmetric Positive Definite (SPD)\n    # A symmetric matrix is SPD iff all its eigenvalues are strictly positive.\n    is_spd = False\n    if n > 0:\n        try:\n            eigenvalues = np.linalg.eigvalsh(A_dense)\n            if np.all(eigenvalues > 0):\n                is_spd = True\n        except np.linalg.LinAlgError:\n            # This case shouldn't happen with the given data but is good practice.\n            is_spd = False\n\n    # 4. Perform correctness and consistency checks\n    tolerance = 1e-12\n    \n    # 4.1. Check against dense matrix-vector product\n    y_dense = A_dense @ x\n    matches_dense = np.allclose(y, y_dense, atol=tolerance, rtol=0)\n\n    # 4.2. Check consistency with A^T x (since A is symmetric, Ax = A^T x)\n    A_transpose = A_dense.T\n    y_transpose = A_transpose @ x\n    transpose_consistency = np.allclose(y, y_transpose, atol=tolerance, rtol=0)\n\n    # 5. Quantify memory savings\n    bytes_per_triplet = 24\n    num_off_diagonal = sum(1 for i, j, v in triplets if i != j)\n    # Savings come from storing each off-diagonal element once instead of twice\n    bytes_saved = num_off_diagonal * bytes_per_triplet\n    \n    # Format the results for the final output\n    return [y.tolist(), is_spd, matches_dense, transpose_consistency, int(bytes_saved)]\n\nsolve()\n```", "id": "2412069"}]}