{"hands_on_practices": [{"introduction": "In many engineering and physical systems, the most interesting vectors are those that satisfy a specific constraint, such as a conservation law. This exercise provides foundational practice in translating such an implicit description of a set of vectors—in this case, a simple balance constraint—into an explicit and tangible basis. By working from first principles, you will solidify your understanding of what defines a subspace and how to construct a minimal set of vectors that spans it, a core skill for analyzing constrained systems [@problem_id:2435999].", "problem": "In a mass-conserving preprocessing step for a $5$-node discretization in computational engineering, a correction vector $\\mathbf{x} \\in \\mathbb{R}^{5}$ must satisfy the balance constraint that the sum of its components is zero. Let\n$$\nS \\equiv \\left\\{ \\mathbf{x} \\in \\mathbb{R}^{5} \\,\\middle|\\, \\sum_{i=1}^{5} x_{i} = 0 \\right\\}.\n$$\nStarting only from the core definitions of subspace, span, and linear independence, and without invoking any prepackaged theorems beyond these definitions, do the following:\n1. Justify from first principles that $S$ is a subspace of $\\mathbb{R}^{5}$.\n2. Construct an explicit basis for $S$ by expressing a general vector in $S$ in terms of free components and rewriting it as a linear combination of a minimal set of fixed vectors. Establish minimality using the definition of linear independence.\n3. Using your construction, determine the dimension of $S$.\n\nReport only the dimension of $S$ as your final answer. No rounding is required and no units are involved.", "solution": "We begin with the definitions. A subset $U \\subseteq \\mathbb{R}^{n}$ is a subspace if and only if it contains the zero vector and is closed under addition and scalar multiplication. A set of vectors $\\{ \\mathbf{v}_{1}, \\dots, \\mathbf{v}_{k} \\}$ spans a subspace $U$ if every $\\mathbf{u} \\in U$ can be written as a linear combination of these vectors. Such a set is linearly independent if the only coefficients $a_{1},\\dots,a_{k}$ satisfying $a_{1}\\mathbf{v}_{1} + \\cdots + a_{k}\\mathbf{v}_{k} = \\mathbf{0}$ are $a_{1} = \\cdots = a_{k} = 0$. A basis is a spanning, linearly independent set, and its cardinality is the dimension.\n\nStep 1: Show that $S$ is a subspace. First, the zero vector $\\mathbf{0} \\in \\mathbb{R}^{5}$ has components summing to $0$, hence $\\mathbf{0} \\in S$. Let $\\mathbf{x}, \\mathbf{y} \\in S$ so that $\\sum_{i=1}^{5} x_{i} = 0$ and $\\sum_{i=1}^{5} y_{i} = 0$. Then\n$$\n\\sum_{i=1}^{5} (x_{i} + y_{i}) = \\sum_{i=1}^{5} x_{i} + \\sum_{i=1}^{5} y_{i} = 0 + 0 = 0,\n$$\nso $\\mathbf{x} + \\mathbf{y} \\in S$. For any scalar $\\alpha \\in \\mathbb{R}$ and $\\mathbf{x} \\in S$,\n$$\n\\sum_{i=1}^{5} (\\alpha x_{i}) = \\alpha \\sum_{i=1}^{5} x_{i} = \\alpha \\cdot 0 = 0,\n$$\nso $\\alpha \\mathbf{x} \\in S$. Therefore $S$ is a subspace of $\\mathbb{R}^{5}$.\n\nStep 2: Construct a basis. Let $\\mathbf{x} = (x_{1}, x_{2}, x_{3}, x_{4}, x_{5})^{\\top} \\in S$. The defining constraint is\n$$\nx_{1} + x_{2} + x_{3} + x_{4} + x_{5} = 0.\n$$\nWe can solve for one component in terms of the others using only algebra. Take $x_{5}$ as dependent:\n$$\nx_{5} = - (x_{1} + x_{2} + x_{3} + x_{4}).\n$$\nThen every $\\mathbf{x} \\in S$ can be parameterized by the four free variables $x_{1}, x_{2}, x_{3}, x_{4} \\in \\mathbb{R}$ as\n$$\n\\mathbf{x} = \\begin{pmatrix} x_{1} \\\\ x_{2} \\\\ x_{3} \\\\ x_{4} \\\\ - (x_{1} + x_{2} + x_{3} + x_{4}) \\end{pmatrix}\n= x_{1} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ -1 \\end{pmatrix}\n+ x_{2} \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ -1 \\end{pmatrix}\n+ x_{3} \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ -1 \\end{pmatrix}\n+ x_{4} \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ -1 \\end{pmatrix}.\n$$\nDefine the four vectors\n$$\n\\mathbf{v}_{1} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ -1 \\end{pmatrix}, \\quad\n\\mathbf{v}_{2} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ -1 \\end{pmatrix}, \\quad\n\\mathbf{v}_{3} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ -1 \\end{pmatrix}, \\quad\n\\mathbf{v}_{4} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ -1 \\end{pmatrix}.\n$$\nBy the parameterization, any $\\mathbf{x} \\in S$ is a linear combination of $\\{ \\mathbf{v}_{1}, \\mathbf{v}_{2}, \\mathbf{v}_{3}, \\mathbf{v}_{4} \\}$, so these vectors span $S$.\n\nTo verify linear independence using the definition, suppose\n$$\na_{1} \\mathbf{v}_{1} + a_{2} \\mathbf{v}_{2} + a_{3} \\mathbf{v}_{3} + a_{4} \\mathbf{v}_{4} = \\mathbf{0}.\n$$\nEquating components yields\n$$\n\\begin{aligned}\n\\text{Component } 1: & \\quad a_{1} = 0, \\\\\n\\text{Component } 2: & \\quad a_{2} = 0, \\\\\n\\text{Component } 3: & \\quad a_{3} = 0, \\\\\n\\text{Component } 4: & \\quad a_{4} = 0, \\\\\n\\text{Component } 5: & \\quad - (a_{1} + a_{2} + a_{3} + a_{4}) = 0,\n\\end{aligned}\n$$\nand the first four equations already force $a_{1} = a_{2} = a_{3} = a_{4} = 0$. Therefore the set $\\{ \\mathbf{v}_{1}, \\mathbf{v}_{2}, \\mathbf{v}_{3}, \\mathbf{v}_{4} \\}$ is linearly independent.\n\nSince the set spans $S$ and is linearly independent, it is a basis for $S$.\n\nStep 3: Determine the dimension. By definition, the dimension of a subspace equals the number of vectors in any basis for it. The basis we constructed has $4$ vectors, hence\n$$\n\\dim(S) = 4.\n$$\nThis completes the construction and determination using only the fundamental definitions.", "answer": "$$\\boxed{4}$$", "id": "2435999"}, {"introduction": "While understanding the space spanned by a set of vectors is crucial, it is often equally important to identify what lies outside that span. This computational exercise challenges you to move from theory to a practical algorithm, designing a method to find a vector in the orthogonal complement of a given subspace. You will use the Singular Value Decomposition (SVD), a cornerstone of numerical linear algebra, to construct a solution that is not only correct in theory but also robust in practice, a vital skill for tasks like error detection and signal decomposition [@problem_id:2435961].", "problem": "You are given $k$ vectors in $\\mathbb{R}^n$ with $k < n$. By definition, the span of a finite set of vectors in $\\mathbb{R}^n$ is the set of all finite linear combinations of those vectors, and the span is a subspace of $\\mathbb{R}^n$. Your task is to design and implement a computational method that, for each provided test case, constructs a vector $\\mathbf{w} \\in \\mathbb{R}^n$ that is guaranteed to lie outside the span of the given vectors. Your method must be justified from first principles, starting from the core definitions of span, subspace, and orthogonality, and must ensure robustness to linear dependence among the input vectors. No physical units are involved. All angles, if any, should be considered in radians; however, no trigonometric quantities are required in this task.\n\nFor each test case, your program must:\n- Accept a set of $k$ vectors in $\\mathbb{R}^n$ with $k < n$ (these are hard-coded in your program; no user input is required).\n- Construct a vector $\\mathbf{w}$ that is guaranteed to be outside the span of the given vectors, based solely on the fundamental definitions of span and orthogonality between subspaces.\n- Compute the rank $r$ of the given set of vectors (the dimension of their span), the nullity $d = n - r$, and verify that $\\mathbf{w}$ is outside the span by computing the residual norm of $\\mathbf{w}$ after orthogonal projection onto the span. Use a numerical tolerance $\\tau = 10^{-10}$ to decide whether $\\mathbf{w}$ is outside the span, by declaring it outside if the residual norm is strictly greater than $\\tau \\cdot \\max(1, \\lVert \\mathbf{w} \\rVert_2)$, where $\\lVert \\cdot \\rVert_2$ is the Euclidean norm.\n- Produce a result for each test case as a list of the form $[r, d, b]$, where $r$ is an integer, $d$ is an integer, and $b$ is a boolean indicating whether $\\mathbf{w}$ is outside the span under the stated criterion.\n\nTest suite (each case lists $n$, $k$, and the $k$ column vectors in $\\mathbb{R}^n$):\n- Case $1$: $n = 3$, $k = 2$, vectors $\\left( $1$, $0$, $0$ \\right)$ and $\\left( $0$, $1$, $0$ \\right)$.\n- Case $2$: $n = 4$, $k = 3$, vectors $\\left( $1$, $1$, $0$, $0$ \\right)$, $\\left( $2$, $2$, $0$, $0$ \\right)$, and $\\left( $0$, $0$, $1$, $0$ \\right)$.\n- Case $3$: $n = 3$, $k = 2$, vectors $\\left( $1$, $0$, $0$ \\right)$ and $\\left( $1$, $1 \\times 10^{-12}$, $0$ \\right)$.\n- Case $4$: $n = 5$, $k = 4$, vectors $\\left( $1$, $0$, $0$, $0$, $0$ \\right)$, $\\left( $0$, $1$, $0$, $0$, $0$ \\right)$, $\\left( $0$, $0$, $1$, $0$, $0$ \\right)$, and $\\left( $0$, $0$, $0$, $1$, $0$ \\right)$.\n- Case $5$: $n = 3$, $k = 1$, vector $\\left( $0$, $0$, $0$ \\right)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list of per-case results enclosed in square brackets. Specifically, the output must be a single line of the form\n$[\\,[r_1,d_1,b_1],[r_2,d_2,b_2],\\dots,[r_5,d_5,b_5]\\,]$\nwith no additional text. Each $r_i$ and $d_i$ must be integers, and each $b_i$ must be a boolean. No user input is permitted, and no file input or output is permitted. The computation must be self-contained and reproducible.", "solution": "The problem as stated is valid. It is a well-posed question in computational linear algebra, firmly grounded in established mathematical principles. All necessary definitions, data, and conditions are provided, and no contradictions or ambiguities are present. The definition of \"nullity\" as $d = n - r$ is explicitly given, precluding any misinterpretation. We may therefore proceed with a formal solution.\n\nLet the given set of $k$ vectors in $\\mathbb{R}^n$ be denoted by $V = \\{\\mathbf{v}_1, \\mathbf{v}_2, \\dots, \\mathbf{v}_k\\}$. The span of these vectors, $\\text{span}(V)$, is the set of all their linear combinations and forms a vector subspace of $\\mathbb{R}^n$. Let us designate this subspace as $S$. By definition, $S = \\{ \\mathbf{s} \\in \\mathbb{R}^n \\mid \\mathbf{s} = \\sum_{i=1}^k c_i \\mathbf{v}_i \\text{ for some scalars } c_i \\in \\mathbb{R} \\}$. The dimension of this subspace, $r = \\dim(S)$, is the rank of the set of vectors. Since the set contains $k$ vectors, it must be that $r \\le k$. The problem statement gives the critical condition $k < n$, which implies $r \\le k < n$. This guarantees that $S$ is a proper subspace of $\\mathbb{R}^n$.\n\nOur task is to construct a vector $\\mathbf{w}$ that is guaranteed to lie outside of $S$. A principled approach relies upon the concept of orthogonality. The fundamental theorem of linear algebra establishes that any vector space $\\mathbb{R}^n$ can be decomposed into the direct sum of a subspace $S$ and its orthogonal complement $S^\\perp$, such that $\\mathbb{R}^n = S \\oplus S^\\perp$. The orthogonal complement $S^\\perp$ is defined as the set of all vectors in $\\mathbb{R}^n$ that are orthogonal to every vector in $S$:\n$$ S^\\perp = \\{ \\mathbf{u} \\in \\mathbb{R}^n \\mid \\mathbf{u}^T \\mathbf{s} = 0 \\text{ for all } \\mathbf{s} \\in S \\} $$\nThe dimensions of these complementary subspaces are related by the equation $\\dim(S) + \\dim(S^\\perp) = \\dim(\\mathbb{R}^n) = n$. The problem defines a quantity $d = n - r$. By substituting $r = \\dim(S)$, we see that $d = n - \\dim(S) = \\dim(S^\\perp)$. The condition $r < n$ implies that $d = \\dim(S^\\perp) > 0$. This is a crucial result: the orthogonal complement $S^\\perp$ is a non-trivial subspace and contains non-zero vectors.\n\nAny non-zero vector $\\mathbf{w} \\in S^\\perp$ is guaranteed to not be an element of $S$. This can be proven by contradiction. Assume that $\\mathbf{w} \\in S^\\perp$, $\\mathbf{w} \\neq \\mathbf{0}$, and also that $\\mathbf{w} \\in S$. As an element of $S^\\perp$, $\\mathbf{w}$ must be orthogonal to every vector in $S$. Since $\\mathbf{w}$ is itself in $S$, it must be orthogonal to itself. This implies the inner product $\\mathbf{w}^T \\mathbf{w} = \\lVert \\mathbf{w} \\rVert_2^2 = 0$, which holds if and only if $\\mathbf{w}$ is the zero vector, $\\mathbf{w} = \\mathbf{0}$. This contradicts the assumption that $\\mathbf{w}$ is non-zero. Therefore, any non-zero vector found in $S^\\perp$ is definitively outside the span $S$.\n\nThe task is now reduced to finding a non-zero vector in $S^\\perp$. Computationally, we can achieve this by first arranging the given vectors $\\mathbf{v}_i$ as the columns of an $n \\times k$ matrix, $A = [\\mathbf{v}_1 | \\mathbf{v}_2 | \\dots | \\mathbf{v}_k]$. The subspace $S$ is then identical to the column space of $A$, written as $\\text{Col}(A)$. Its orthogonal complement, $S^\\perp$, is equivalent to the null space of the transpose of $A$, that is, $S^\\perp = (\\text{Col}(A))^\\perp = \\text{Nul}(A^T)$.\n\nThe most numerically robust method for analyzing these fundamental subspaces is the Singular Value Decomposition (SVD). The SVD of the matrix $A$ is its factorization into $A = U \\Sigma V^T$, where:\n- $U$ is an $n \\times n$ orthogonal matrix whose columns, $\\{\\mathbf{u}_1, \\dots, \\mathbf{u}_n\\}$, form an orthonormal basis for $\\mathbb{R}^n$.\n- $\\Sigma$ is an $n \\times k$ rectangular matrix with non-negative real numbers, the singular values $\\sigma_i$, on its main diagonal.\n- $V$ is a $k \\times k$ orthogonal matrix.\n\nThe rank $r$ of matrix $A$ corresponds to the number of non-zero singular values. The first $r$ columns of the matrix $U$, i.e., $\\{\\mathbf{u}_1, \\dots, \\mathbf{u}_r\\}$, constitute an orthonormal basis for the column space, $\\text{Col}(A) = S$. The subsequent $n-r$ columns of $U$, namely $\\{\\mathbf{u}_{r+1}, \\dots, \\mathbf{u}_n\\}$, form an orthonormal basis for the orthogonal complement, $S^\\perp$.\n\nThis provides a deterministic algorithm for constructing the vector $\\mathbf{w}$. We compute the full SVD of $A$ and select one of the last $n-r$ columns of $U$. We will consistently choose the last column, $\\mathbf{w} = \\mathbf{u}_n$. As columns of an orthogonal matrix, these basis vectors are of unit length, $\\lVert \\mathbf{w} \\rVert_2 = 1$, thus ensuring $\\mathbf{w}$ is non-zero.\n\nThe algorithmic procedure is as follows:\n$1$. Assemble the $n \\times k$ matrix $A$ from the input column vectors.\n$2$. Compute the numerical rank $r$ of $A$. The problem's \"nullity\" is then calculated as $d = n-r$.\n$3$. Perform a full SVD of $A$ to obtain the $n \\times n$ orthogonal matrix $U$.\n$4$. Select the vector $\\mathbf{w}$ to be the last column of $U$, $\\mathbf{w} = U[:, n-1]$.\n$5$. For verification, we must compute the norm of the residual after orthogonally projecting $\\mathbf{w}$ onto $S$. The orthogonal projection operator onto $S$ is $P_S = U_r U_r^T$, where $U_r = [\\mathbf{u}_1 | \\dots | \\mathbf{u}_r]$.\n$6$. For our chosen $\\mathbf{w}=\\mathbf{u}_n$, the projection is $P_S(\\mathbf{w}) = U_r U_r^T \\mathbf{u}_n$. Due to the mutual orthogonality of the columns of $U$, the product $U_r^T \\mathbf{u}_n$ is a zero vector. Consequently, the projection $P_S(\\mathbf{u}_n)$ is also the zero vector, $\\mathbf{0}$.\n$7$. The residual vector is $\\mathbf{w} - P_S(\\mathbf{w}) = \\mathbf{u}_n - \\mathbf{0} = \\mathbf{u}_n$. The norm of this residual is $\\lVert \\mathbf{u}_n \\rVert_2 = 1$.\n$8$. We then apply the specified test: is the residual norm strictly greater than $\\tau \\cdot \\max(1, \\lVert \\mathbf{w} \\rVert_2)$? Substituting the values, we check if $1 > 10^{-10} \\cdot \\max(1, 1)$, which simplifies to $1 > 10^{-10}$. This inequality is true. Therefore, the boolean outcome $b$ is `True`. This logic holds for all test cases where $r < n$, including the trivial case where the input is the zero vector, for which $r=0$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for a predefined test suite, constructing a vector outside the\n    span of a given set of vectors and performing the required analysis.\n    \"\"\"\n\n    # Test suite defined in the problem statement.\n    # Each case is a tuple: (n, list_of_column_vectors)\n    test_cases = [\n        (3, [(1, 0, 0), (0, 1, 0)]),\n        (4, [(1, 1, 0, 0), (2, 2, 0, 0), (0, 0, 1, 0)]),\n        (3, [(1, 0, 0), (1, 1e-12, 0)]),\n        (5, [(1, 0, 0, 0, 0), (0, 1, 0, 0, 0), (0, 0, 1, 0, 0), (0, 0, 0, 1, 0)]),\n        (3, [(0, 0, 0)])\n    ]\n\n    all_results = []\n    \n    # Numerical tolerance for verification check\n    tau = 1e-10\n\n    for n, vectors in test_cases:\n        # Step 1: Construct the n x k matrix A from the input vectors.\n        # The input vectors are column vectors, so we transpose the array.\n        # Handle the edge case of an empty list of vectors.\n        if not vectors or not vectors[0]:\n            A = np.zeros((n, 0))\n        else:\n            A = np.array(vectors).T\n        \n        # Step 2: Compute the rank r and the problem-defined nullity d.\n        # numpy.linalg.matrix_rank uses SVD and is numerically robust.\n        r = np.linalg.matrix_rank(A)\n        d = n - r\n\n        # Step 3: Compute the full SVD of A.\n        # full_matrices=True is essential to get the full n x n U matrix.\n        U, s, Vh = np.linalg.svd(A, full_matrices=True)\n\n        # Step 4: Select vector w from the orthogonal complement's basis.\n        # The last n-r columns of U form a basis for the orthogonal complement.\n        # For a deterministic choice, we take the last column of U.\n        w = U[:, -1]\n\n        # Step 5: Verify that w is outside the span S.\n        # The first r columns of U form an orthonormal basis for the span S.\n        Ur = U[:, :r]\n        \n        # Calculate the orthogonal projection of w onto the span S.\n        # If r is 0, the span is the zero vector, so the projection is zero.\n        # numpy handles matrix multiplication with shape (n, 0) correctly, resulting in a zero vector.\n        projection_w = Ur @ (Ur.T @ w)\n        \n        # Calculate the residual vector and its norm.\n        residual_vector = w - projection_w\n        residual_norm = np.linalg.norm(residual_vector)\n        \n        # Calculate the norm of w for the threshold calculation.\n        w_norm = np.linalg.norm(w)\n\n        # Apply the verification criterion from the problem statement.\n        threshold = tau * max(1.0, w_norm)\n        is_outside = residual_norm > threshold\n        \n        # Store the result [r, d, b] for this case.\n        all_results.append([int(r), int(d), is_outside])\n\n    # Format the final output string exactly as required.\n    # Individual list items are formatted as [r,d,b] without spaces.\n    def format_result_list(res_list):\n        r_val, d_val, b_val = res_list\n        return f\"[{r_val},{d_val},{str(b_val).lower()}]\"\n\n    formatted_results = [format_result_list(res) for res in all_results]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2435961"}, {"introduction": "Modern computational engineering is often flooded with high-dimensional data, where the true underlying structure is much simpler. This advanced practice explores the powerful idea of finding an optimal low-dimensional subspace that best approximates a given set of data points. By minimizing the projection error, you will engage with the core concept behind Principal Component Analysis (PCA), a fundamental technique for dimensionality reduction in machine learning, data analysis, and model order reduction, revealing how abstract subspace concepts lead to powerful data-driven tools [@problem_id:2435976].", "problem": "You are given finite sets of vectors in a real Euclidean space $\\mathbb{R}^n$. For each set, consider the problem of finding a linear subspace $S \\subset \\mathbb{R}^n$ of fixed dimension $k$ that minimizes the total squared Euclidean distance from the given vectors to their orthogonal projections onto $S$. Formally, for a set $V = \\{ \\mathbf{v}_1, \\ldots, \\mathbf{v}_m \\} \\subset \\mathbb{R}^n$ and an integer $k$ with $0 \\le k \\le n$, define the objective\n$$\nE(S) = \\sum_{i=1}^{m} \\left\\| \\mathbf{v}_i - \\mathbf{P}_S(\\mathbf{v}_i) \\right\\|_2^2,\n$$\nwhere $\\mathbf{P}_S$ denotes the orthogonal projection operator onto $S$. The task is, for each provided test case, to compute the minimal possible value of $E(S)$ over all $k$-dimensional subspaces $S \\subset \\mathbb{R}^n$. Distances are Euclidean, and there are no physical units involved.\n\nYour program must process the following test suite. For each case, the ambient space $\\mathbb{R}^n$ is inferred from the length of the vectors, and the subspace dimension $k$ is specified. When $k = 0$, interpret $S$ as the zero subspace $\\{ \\mathbf{0} \\}$.\n\nTest Suite:\n- Case $1$: $V = \\{ (1,2), (2,4), (3,6) \\} \\subset \\mathbb{R}^2$, with $k = 1$.\n- Case $2$: $V = \\{ (1,0,0), (0,1,0), (0,0,1) \\} \\subset \\mathbb{R}^3$, with $k = 2$.\n- Case $3$: $V = \\{ (1,0), (0,1), (0,0) \\} \\subset \\mathbb{R}^2$, with $k = 1$.\n- Case $4$: $V = \\{ (1,2), (-1,-2), (0,0) \\} \\subset \\mathbb{R}^2$, with $k = 0$.\n- Case $5$: $V = \\{ (1,1), (1,0) \\} \\subset \\mathbb{R}^2$, with $k = 1$.\n\nYour program must output, for each case, the minimal total squared projection error, expressed as a real number rounded to six decimal places. The final output format must be a single line containing a comma-separated list of these five numbers enclosed in square brackets, for example, $[a_1,a_2,a_3,a_4,a_5]$, where each $a_i$ is the result for Case $i$, rounded to six decimal places as specified. There is no input; your program should hard-code the test suite and produce the single-line output accordingly.", "solution": "The problem as stated constitutes a well-posed question within the field of linear algebra, specifically concerning the optimal approximation of a set of vectors by a lower-dimensional subspace. This is the foundational problem of Principal Component Analysis (PCA) when data is not centered. We shall proceed with a formal derivation of the solution.\n\nLet the given set of vectors be $V = \\{ \\mathbf{v}_1, \\ldots, \\mathbf{v}_m \\}$, where each $\\mathbf{v}_i \\in \\mathbb{R}^n$. We are tasked with finding a linear subspace $S \\subset \\mathbb{R}^n$ of a fixed dimension $k$ ($0 \\le k \\le n$) that minimizes the objective function:\n$$\nE(S) = \\sum_{i=1}^{m} \\left\\| \\mathbf{v}_i - \\mathbf{P}_S(\\mathbf{v}_i) \\right\\|_2^2\n$$\nHere, $\\mathbf{P}_S$ denotes the orthogonal projection operator onto the subspace $S$. The term $\\mathbf{v}_i - \\mathbf{P}_S(\\mathbf{v}_i)$ is the projection of $\\mathbf{v}_i$ onto the orthogonal complement of $S$, denoted $S^\\perp$. By the Pythagorean theorem, any vector $\\mathbf{v}_i$ can be decomposed into orthogonal components: $\\mathbf{v}_i = \\mathbf{P}_S(\\mathbf{v}_i) + (\\mathbf{v}_i - \\mathbf{P}_S(\\mathbf{v}_i))$. This implies:\n$$\n\\left\\| \\mathbf{v}_i \\right\\|_2^2 = \\left\\| \\mathbf{P}_S(\\mathbf{v}_i) \\right\\|_2^2 + \\left\\| \\mathbf{v}_i - \\mathbf{P}_S(\\mathbf{v}_i) \\right\\|_2^2\n$$\nWe can, therefore, rewrite the objective function as:\n$$\nE(S) = \\sum_{i=1}^{m} \\left( \\left\\| \\mathbf{v}_i \\right\\|_2^2 - \\left\\| \\mathbf{P}_S(\\mathbf{v}_i) \\right\\|_2^2 \\right) = \\left( \\sum_{i=1}^{m} \\left\\| \\mathbf{v}_i \\right\\|_2^2 \\right) - \\left( \\sum_{i=1}^{m} \\left\\| \\mathbf{P}_S(\\mathbf{v}_i) \\right\\|_2^2 \\right)\n$$\nThe first term, $\\sum_{i=1}^{m} \\left\\| \\mathbf{v}_i \\right\\|_2^2$, is the total sum of squared norms of the given vectors. This quantity is a constant with respect to the choice of the subspace $S$. Consequently, minimizing $E(S)$ is equivalent to maximizing the second term, $\\sum_{i=1}^{m} \\left\\| \\mathbf{P}_S(\\mathbf{v}_i) \\right\\|_2^2$, which represents the total squared length of the vectors' projections onto $S$.\n\nLet us construct a data matrix $X \\in \\mathbb{R}^{m \\times n}$ where the $i$-th row is the vector $\\mathbf{v}_i^T$. Let $\\{\\mathbf{u}_1, \\ldots, \\mathbf{u}_k\\}$ be an orthonormal basis for the $k$-dimensional subspace $S$. The projection of a vector $\\mathbf{v}_i$ onto $S$ is $\\mathbf{P}_S(\\mathbf{v}_i) = \\sum_{j=1}^{k} (\\mathbf{v}_i^T \\mathbf{u}_j) \\mathbf{u}_j$. Its squared norm is $\\| \\mathbf{P}_S(\\mathbf{v}_i) \\|_2^2 = \\sum_{j=1}^{k} (\\mathbf{v}_i^T \\mathbf{u}_j)^2$.\n\nThe maximization problem is:\n$$\n\\max_{S: \\dim(S)=k} \\sum_{i=1}^{m} \\sum_{j=1}^{k} (\\mathbf{v}_i^T \\mathbf{u}_j)^2 = \\max_{\\{\\mathbf{u}_j\\} \\text{ orthonormal}} \\sum_{j=1}^{k} \\sum_{i=1}^{m} (\\mathbf{v}_i^T \\mathbf{u}_j)^2\n$$\nThe inner sum can be recognized as the squared norm of the vector $X \\mathbf{u}_j$:\n$$\n\\sum_{i=1}^{m} (\\mathbf{v}_i^T \\mathbf{u}_j)^2 = \\left\\| X \\mathbf{u}_j \\right\\|_2^2 = (X \\mathbf{u}_j)^T (X \\mathbf{u}_j) = \\mathbf{u}_j^T X^T X \\mathbf{u}_j\n$$\nThe problem is thus transformed into maximizing the sum of quadratic forms:\n$$\n\\max_{\\{\\mathbf{u}_1, \\ldots, \\mathbf{u}_k\\} \\text{ orthonormal}} \\sum_{j=1}^{k} \\mathbf{u}_j^T (X^T X) \\mathbf{u}_j\n$$\nThe matrix $A = X^T X \\in \\mathbb{R}^{n \\times n}$ is the scatter matrix (or uncentered covariance matrix). It is symmetric and positive semi-definite. By the Courant-Fischer theorem (or Rayleigh-Ritz principle extended), the maximum value of this sum is obtained when the vectors $\\{\\mathbf{u}_1, \\ldots, \\mathbf{u}_k\\}$ are the orthonormal eigenvectors of $A$ corresponding to its $k$ largest eigenvalues. Let the eigenvalues of $A$ be ordered as $\\lambda_1 \\ge \\lambda_2 \\ge \\ldots \\ge \\lambda_n \\ge 0$. The maximum value of the projected sum of squares is $\\sum_{j=1}^{k} \\lambda_j$.\n\nThe minimal error, $E_{\\min}$, is therefore:\n$$\nE_{\\min} = \\left( \\sum_{i=1}^{m} \\left\\| \\mathbf{v}_i \\right\\|_2^2 \\right) - \\sum_{j=1}^{k} \\lambda_j\n$$\nThe total sum of squares can be related to the eigenvalues of $A$. It is equal to the squared Frobenius norm of $X$, which is also the trace of $X^T X$:\n$$\n\\sum_{i=1}^{m} \\left\\| \\mathbf{v}_i \\right\\|_2^2 = \\sum_{i=1}^{m} \\sum_{j=1}^{n} (X_{ij})^2 = \\|X\\|_F^2 = \\mathrm{Tr}(X^T X)\n$$\nThe trace of a matrix is the sum of its eigenvalues. Therefore, $\\mathrm{Tr}(A) = \\sum_{j=1}^{n} \\lambda_j$. Substituting this into the expression for $E_{\\min}$:\n$$\nE_{\\min} = \\left( \\sum_{j=1}^{n} \\lambda_j \\right) - \\left( \\sum_{j=1}^{k} \\lambda_j \\right) = \\sum_{j=k+1}^{n} \\lambda_j\n$$\nThe minimal total squared projection error is the sum of the $n-k$ smallest eigenvalues of the scatter matrix $X^T X$.\n\nFor numerical computation, it is more stable to use Singular Value Decomposition (SVD). Let the SVD of the data matrix $X$ be $X = U \\Sigma V^T$. The eigenvalues $\\lambda_j$ of $X^T X$ are the squares of the singular values $\\sigma_j$ of $X$. That is, $\\lambda_j = \\sigma_j^2$. The singular values are, by convention, sorted in descending order: $\\sigma_1 \\ge \\sigma_2 \\ge \\ldots \\ge 0$.\nThe minimal error is then given by:\n$$\nE_{\\min} = \\sum_{j=k+1}^{n} \\sigma_j^2\n$$\nComputationally, we form the matrix $X$, compute its singular values $\\sigma_j$, and sum the squares of the singular values from index $k$ to the end (using 0-based indexing for an array of singular values).\n\nLet us apply this to Case 5 as an illustration: $V = \\{ (1,1), (1,0) \\} \\subset \\mathbb{R}^2$, with $k = 1$.\nThe data matrix is $X = \\begin{pmatrix} 1 & 1 \\\\ 1 & 0 \\end{pmatrix}$. Here $m=2, n=2$.\nThe scatter matrix is $X^T X = \\begin{pmatrix} 1 & 1 \\\\ 1 & 0 \\end{pmatrix}^T \\begin{pmatrix} 1 & 1 \\\\ 1 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 1 \\\\ 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\\\ 1 & 0 \\end{pmatrix} = \\begin{pmatrix} 2 & 1 \\\\ 1 & 1 \\end{pmatrix}$.\nThe eigenvalues are given by $\\det(X^T X - \\lambda I) = (2-\\lambda)(1-\\lambda)-1 = \\lambda^2 - 3\\lambda + 1 = 0$.\nThe solutions are $\\lambda = \\frac{3 \\pm \\sqrt{5}}{2}$. So $\\lambda_1 = \\frac{3+\\sqrt{5}}{2}$ and $\\lambda_2 = \\frac{3-\\sqrt{5}}{2}$.\nThe singular values squared are $\\sigma_1^2 = \\lambda_1$ and $\\sigma_2^2 = \\lambda_2$.\nWe need to find $E_{\\min} = \\sum_{j=k+1}^{n} \\sigma_j^2 = \\sum_{j=2}^{2} \\sigma_j^2 = \\sigma_2^2$.\n$E_{\\min} = \\lambda_2 = \\frac{3-\\sqrt{5}}{2} \\approx 0.381966$.\nThis procedure is followed for all test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test suite.\n    For each case, it computes the minimal total squared projection error\n    of a set of vectors onto a k-dimensional subspace.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: V = { (1,2), (2,4), (3,6) }, k = 1\n        {'V': np.array([[1, 2], [2, 4], [3, 6]]), 'k': 1},\n        # Case 2: V = { (1,0,0), (0,1,0), (0,0,1) }, k = 2\n        {'V': np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]), 'k': 2},\n        # Case 3: V = { (1,0), (0,1), (0,0) }, k = 1\n        {'V': np.array([[1, 0], [0, 1], [0, 0]]), 'k': 1},\n        # Case 4: V = { (1,2), (-1,-2), (0,0) }, k = 0\n        {'V': np.array([[1, 2], [-1, -2], [0, 0]]), 'k': 0},\n        # Case 5: V = { (1,1), (1,0) }, k = 1\n        {'V': np.array([[1, 1], [1, 0]]), 'k': 1},\n    ]\n\n    results = []\n    for case in test_cases:\n        X = case['V']\n        k = case['k']\n\n        # The problem is to find a k-dimensional subspace S that minimizes\n        # the sum of squared Euclidean distances from the given vectors to\n        # their orthogonal projections onto S.\n        # This is a classic result in linear algebra, related to Principal\n        # Component Analysis (PCA). The minimum error is the sum of the\n        # squares of the smallest (n-k) singular values of the data matrix X,\n        # where the vectors are rows of X.\n\n        # Compute the singular values of the data matrix X.\n        # np.linalg.svd returns singular values in descending order.\n        # We don't need the U and V* matrices, so compute_uv=False for efficiency.\n        singular_values = np.linalg.svd(X, compute_uv=False)\n\n        # The optimal k-dimensional subspace is spanned by the first k principal\n        # components (right singular vectors of X, or eigenvectors of X^T*X).\n        # The total projection error is the sum of the squared singular values\n        # that are NOT used for projection. These are the singular values\n        # from index k to the end of the list.\n        # Python's 0-based indexing means we slice from index k.\n        \n        # If k is greater or equal to the number of singular values,\n        # the slice s[k:] will be empty, and the sum will correctly be 0.\n        error_values = singular_values[k:]\n        min_squared_error = np.sum(error_values**2)\n        \n        results.append(min_squared_error)\n\n    # Format the results to six decimal places and join them into the specified string format.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2435976"}]}