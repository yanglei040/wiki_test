{"hands_on_practices": [{"introduction": "Vectors and matrices are not just abstract mathematical objects; they are powerful tools for representing and manipulating real-world information. A perfect example is the representation of color in digital imaging, where a simple vector can capture the full spectrum of a pixel. This exercise [@problem_id:2449842] provides a hands-on opportunity to derive a linear transformation that converts color images to grayscale, starting only from fundamental principles of symmetry and invariance. By constructing the transformation matrix and analyzing its null space, you will gain a deeper geometric intuition for how matrices act on vector spaces.", "problem": "In a digital imaging pipeline, each color pixel is represented by a vector in the three-dimensional real vector space $\\mathbb{R}^{3}$, with coordinates $(R,G,B)^{\\top}$ corresponding to Red, Green, and Blue (RGB). Design a linear transformation represented by a matrix $A \\in \\mathbb{R}^{3 \\times 3}$ that maps any RGB vector $x \\in \\mathbb{R}^{3}$ to a grayscale vector $y \\in \\mathbb{R}^{3}$ in which all three components are equal and represent a scalar grayscale intensity. Assume the transformation must satisfy the following physical and symmetry constraints: (i) it is linear, (ii) it preserves the entire gray axis, meaning for any scalar $k \\in \\mathbb{R}$, $A \\,(k \\,\\mathbf{1}) = k \\,\\mathbf{1}$ where $\\mathbf{1} = (1,1,1)^{\\top}$, and (iii) in the absence of any preferred channel, the grayscale intensity is invariant under permutations of the RGB channels. Based solely on these constraints, determine the unique matrix $A$.\n\nAnalyze the geometric meaning of the null space of $A$ within $\\mathbb{R}^{3}$ in terms of RGB color directions. Then, compute the dimension of the null space of $A$. Provide your final answer as the single number equal to the dimension of the null space. No rounding is necessary, and no units are required.", "solution": "The problem statement is subjected to validation.\n\n**Step 1: Extract Givens**\n- Each color pixel is a vector in $\\mathbb{R}^{3}$, $x = (R,G,B)^{\\top}$.\n- The transformation is linear, represented by a matrix $A \\in \\mathbb{R}^{3 \\times 3}$.\n- The transformation maps an RGB vector $x$ to a grayscale vector $y \\in \\mathbb{R}^{3}$ where $y=(g,g,g)^{\\top}$ for some scalar intensity $g$.\n- Constraint (i): The transformation is linear, $y = Ax$.\n- Constraint (ii): The transformation preserves the entire gray axis. For any scalar $k \\in \\mathbb{R}$, $A \\,(k \\,\\mathbf{1}) = k \\,\\mathbf{1}$ where $\\mathbf{1} = (1,1,1)^{\\top}$.\n- Constraint (iii): The grayscale intensity is invariant under permutations of the RGB channels.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, well-posed, and objective. It is a standard problem in linear algebra applied to computational engineering (color space transformation). The provided constraints are mathematically precise and consistent, allowing for the derivation of a unique solution. The problem does not violate any fundamental principles, is not based on false premises, and the terms are well-defined.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will now be derived.\n\n**Derivation of the Transformation Matrix $A$**\n\nLet the transformation matrix be $A = \\begin{pmatrix} a_{11} & a_{12} & a_{13} \\\\ a_{21} & a_{22} & a_{23} \\\\ a_{31} & a_{32} & a_{33} \\end{pmatrix}$.\nThe input vector is $x = (R, G, B)^{\\top}$. The output vector is $y = Ax$.\nThe problem states that the output $y$ must be a grayscale vector, meaning all its components are equal. Let this common value be $g$.\n$$y = Ax = \\begin{pmatrix} a_{11}R + a_{12}G + a_{13}B \\\\ a_{21}R + a_{22}G + a_{23}B \\\\ a_{31}R + a_{32}G + a_{33}B \\end{pmatrix} = \\begin{pmatrix} g \\\\ g \\\\ g \\end{pmatrix}$$\nSince this must hold for any input vector $(R,G,B)^{\\top} \\in \\mathbb{R}^{3}$, the linear forms defining each component of the output must be identical. This implies that the rows of the matrix $A$ must be identical. Let the first row be $(c_1, c_2, c_3)$. Then the matrix $A$ must have the form:\n$$A = \\begin{pmatrix} c_1 & c_2 & c_3 \\\\ c_1 & c_2 & c_3 \\\\ c_1 & c_2 & c_3 \\end{pmatrix}$$\nThe scalar grayscale intensity is given by $g = c_1 R + c_2 G + c_3 B$.\n\nWe now apply Constraint (ii): preservation of the gray axis. For any $k \\in \\mathbb{R}$, a vector on the gray axis is $k \\mathbf{1} = (k,k,k)^{\\top}$.\n$$A(k\\mathbf{1}) = \\begin{pmatrix} c_1 & c_2 & c_3 \\\\ c_1 & c_2 & c_3 \\\\ c_1 & c_2 & c_3 \\end{pmatrix} \\begin{pmatrix} k \\\\ k \\\\ k \\end{pmatrix} = \\begin{pmatrix} k(c_1+c_2+c_3) \\\\ k(c_1+c_2+c_3) \\\\ k(c_1+c_2+c_3) \\end{pmatrix}$$\nThis output must be equal to $k\\mathbf{1} = (k,k,k)^{\\top}$.\n$$k(c_1+c_2+c_3) = k$$\nThis equality must hold for all $k \\in \\mathbb{R}$. For $k \\neq 0$, we can divide by $k$, which gives the condition:\n$$c_1 + c_2 + c_3 = 1$$\n\nNext, we apply Constraint (iii): invariance of the grayscale intensity $g$ under permutations of the RGB channels. The intensity is $g(R,G,B) = c_1 R + c_2 G + c_3 B$.\nLet us consider a permutation of the first two channels, $R$ and $G$. The intensity must remain unchanged.\n$$g(G,R,B) = c_1 G + c_2 R + c_3 B$$\nSetting $g(R,G,B) = g(G,R,B)$:\n$$c_1 R + c_2 G + c_3 B = c_1 G + c_2 R + c_3 B$$\n$$(c_1 - c_2)R + (c_2 - c_1)G = 0$$\n$$(c_1 - c_2)(R - G) = 0$$\nThis must hold for all $R,G \\in \\mathbb{R}$. We can choose $R \\neq G$, which forces $c_1 - c_2 = 0$, so $c_1 = c_2$.\nSimilarly, considering a permutation of the second and third channels, $G$ and $B$:\n$$g(R,B,G) = c_1 R + c_2 B + c_3 G$$\nSetting $g(R,G,B) = g(R,B,G)$:\n$$c_1 R + c_2 G + c_3 B = c_1 R + c_2 B + c_3 G$$\n$$(c_2 - c_3)G + (c_3 - c_2)B = 0$$\n$$(c_2 - c_3)(G - B) = 0$$\nThis must hold for all $G,B \\in \\mathbb{R}$. We can choose $G \\neq B$, which forces $c_2 - c_3 = 0$, so $c_2 = c_3$.\nCombining these results, we find that all three coefficients must be equal: $c_1 = c_2 = c_3$. Let us call this common value $c$.\n\nNow, we combine this result with the condition from Constraint (ii):\n$$c_1 + c_2 + c_3 = c + c + c = 3c = 1$$\nThis yields $c = \\frac{1}{3}$.\nTherefore, the unique matrix $A$ that satisfies all constraints is:\n$$A = \\begin{pmatrix} \\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3} \\\\ \\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3} \\\\ \\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3} \\end{pmatrix}$$\n\n**Analysis and Dimension of the Null Space**\n\nThe null space of $A$, denoted $\\text{Null}(A)$, is the set of all vectors $x = (R,G,B)^{\\top} \\in \\mathbb{R}^{3}$ such that $Ax = \\mathbf{0}$.\n$$Ax = \\begin{pmatrix} \\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3} \\\\ \\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3} \\\\ \\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3} \\end{pmatrix} \\begin{pmatrix} R \\\\ G \\\\ B \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$$\nThis matrix equation simplifies to the single linear equation:\n$$\\frac{1}{3}(R+G+B) = 0$$\nor, equivalently,\n$$R+G+B=0$$\nGeometrically, this equation defines a plane in the three-dimensional space $\\mathbb{R}^{3}$. This plane passes through the origin $(0,0,0)^{\\top}$ and has a normal vector $\\mathbf{n} = (1,1,1)^{\\top}$, which is the direction of the gray axis. In the context of color science, the transformation $A$ projects any color vector onto the gray axis. The intensity of the resulting gray is the average of the $R$, $G$, and $B$ components. The null space consists of all color vectors that are mapped to black (zero intensity), which are those whose components sum to zero. These vectors represent pure chrominance information, orthogonal to the luminance (gray) axis.\n\nTo find the dimension of the null space, we can apply the rank-nullity theorem, which states that for a matrix $A$ with $n$ columns, $\\text{rank}(A) + \\text{dim}(\\text{Null}(A)) = n$.\nIn our case, $A$ is a $3 \\times 3$ matrix, so $n = 3$. The rows of $A$ are all identical and non-zero, so they are linearly dependent. The row space is spanned by the single vector $(\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3})$, so the rank of $A$ is $1$.\n$$\\text{rank}(A) = 1$$\nApplying the rank-nullity theorem:\n$$1 + \\text{dim}(\\text{Null}(A)) = 3$$\nSolving for the dimension of the null space:\n$$\\text{dim}(\\text{Null}(A)) = 3 - 1 = 2$$\nThis result is consistent with the geometric interpretation of the null space as a plane through the origin in $\\mathbb{R}^{3}$, which is a two-dimensional subspace.\nThe dimension of the null space of $A$ is $2$.", "answer": "$$\\boxed{2}$$", "id": "2449842"}, {"introduction": "Beyond performing transformations, matrices can encapsulate the intrinsic properties and relationships within a dataset or a physical system. In statistics and machine learning, the covariance matrix is a cornerstone, describing the variance and inter-correlation of multiple variables. However, not just any matrix can serve this role; it must possess specific mathematical properties. This practice [@problem_id:2449839] challenges you to act as a validator, systematically checking a series of matrices against the required conditions of symmetry and positive definiteness, which are essential for a matrix to be a valid covariance matrix. This exercise will solidify your understanding of these defining properties and their importance in practice.", "problem": "You are given a finite set of real square matrices. A real square matrix $A \\in \\mathbb{R}^{n \\times n}$ is considered a valid covariance matrix in this task if and only if it satisfies all of the following conditions: (i) $A$ has only finite real entries, (ii) $A$ is symmetric, meaning $A = A^{\\top}$, and (iii) $A$ is positive definite, meaning $x^{\\top} A x &gt; 0$ for every nonzero vector $x \\in \\mathbb{R}^{n}$. For each matrix in the test suite below, determine whether it is a valid covariance matrix under these conditions. There are no physical units involved in this task. The required output for the program is a single line containing a comma-separated list of boolean values enclosed in square brackets, in the same order as the test cases (for example: \"[True,False,True]\").\n\nThe test suite consists of the following matrices:\n\nCase $1$:\n$$\nA_1 = \\begin{bmatrix}\n2 & 1 \\\\\n1 & 2\n\\end{bmatrix}\n$$\n\nCase $2$:\n$$\nA_2 = \\begin{bmatrix}\n1 & 2 \\\\\n2 & 1\n\\end{bmatrix}\n$$\n\nCase $3$:\n$$\nA_3 = \\begin{bmatrix}\n1 & 0 \\\\\n0 & 0\n\\end{bmatrix}\n$$\n\nCase $4$:\n$$\nA_4 = \\begin{bmatrix}\n1 & 0 \\\\\n1 & 1\n\\end{bmatrix}\n$$\n\nCase $5$:\n$$\nA_5 = \\begin{bmatrix}\n2 & -1 & 0 \\\\\n-1 & 2 & -1 \\\\\n0 & -1 & 2\n\\end{bmatrix}\n$$\n\nCase $6$:\n$$\nA_6 = \\begin{bmatrix}\n3\n\\end{bmatrix}\n$$\n\nCase $7$:\n$$\nA_7 = \\begin{bmatrix}\n0\n\\end{bmatrix}\n$$\n\nCase $8$:\n$$\nA_8 = \\begin{bmatrix}\n\\text{NaN}\n\\end{bmatrix}\n$$\n\nYour program must evaluate each $A_i$ and produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, \"[True,False,True]\"). The result for each case must be a boolean indicating whether $A_i$ is a valid covariance matrix according to the definition above. The test suite covers general cases, boundary conditions (such as size $1 \\times 1$ and singular matrices), non-symmetric matrices, and matrices with non-finite entries. The final output format must be exactly one line with the list notation as described, with no additional text.", "solution": "The problem requires the validation of a set of real square matrices against a provided definition of a valid covariance matrix. A matrix $A \\in \\mathbb{R}^{n \\times n}$ is defined as a valid covariance matrix if and only if it satisfies the following three conditions simultaneously:\n(i) $A$ has only finite real entries.\n(ii) $A$ is symmetric, which means $A = A^{\\top}$.\n(iii) $A$ is positive definite, which means $x^{\\top} A x > 0$ for every nonzero vector $x \\in \\mathbb{R}^{n}$.\n\nWe will analyze each matrix from the test suite against these three conditions. A matrix must satisfy all three to be considered valid. If any condition fails, the matrix is immediately invalid.\n\nFor condition (iii), we note that a symmetric matrix is positive definite if and only if all of its eigenvalues are strictly positive. This provides a computationally reliable method for verification. If a matrix is not symmetric, the question of it being positive definite under this problem's framework is moot, as it already fails condition (ii).\n\nLet us proceed with the analysis of each case.\n\nCase $1$: $A_1 = \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix}$\n(i) Finite entries: All entries ($2, 1$) are finite real numbers. This condition is satisfied.\n(ii) Symmetry: The matrix is symmetric since the element at row $1$, column $2$ is $1$, which is equal to the element at row $2$, column $1$. Thus, $A_1 = A_1^{\\top}$. This condition is satisfied.\n(iii) Positive definiteness: Since $A_1$ is symmetric, we can inspect its eigenvalues. The characteristic equation is $\\det(A_1 - \\lambda I) = 0$.\n$$ \\det \\begin{pmatrix} 2-\\lambda & 1 \\\\ 1 & 2-\\lambda \\end{pmatrix} = (2-\\lambda)^2 - 1 = \\lambda^2 - 4\\lambda + 3 = (\\lambda-1)(\\lambda-3) = 0 $$\nThe eigenvalues are $\\lambda_1 = 1$ and $\\lambda_2 = 3$. Both are strictly positive. Therefore, $A_1$ is positive definite.\nVerdict for $A_1$: Valid (True).\n\nCase $2$: $A_2 = \\begin{bmatrix} 1 & 2 \\\\ 2 & 1 \\end{bmatrix}$\n(i) Finite entries: All entries ($1, 2$) are finite real numbers. Satisfied.\n(ii) Symmetry: The matrix is symmetric. Satisfied.\n(iii) Positive definiteness: We compute the eigenvalues.\n$$ \\det \\begin{pmatrix} 1-\\lambda & 2 \\\\ 2 & 1-\\lambda \\end{pmatrix} = (1-\\lambda)^2 - 4 = \\lambda^2 - 2\\lambda - 3 = (\\lambda-3)(\\lambda+1) = 0 $$\nThe eigenvalues are $\\lambda_1 = 3$ and $\\lambda_2 = -1$. Since one eigenvalue is negative, the matrix is not positive definite.\nVerdict for $A_2$: Invalid (False).\n\nCase $3$: $A_3 = \\begin{bmatrix} 1 & 0 \\\\ 0 & 0 \\end{bmatrix}$\n(i) Finite entries: All entries ($1, 0$) are finite real numbers. Satisfied.\n(ii) Symmetry: The matrix is symmetric. Satisfied.\n(iii) Positive definiteness: The eigenvalues of a diagonal matrix are its diagonal entries. The eigenvalues are $\\lambda_1 = 1$ and $\\lambda_2 = 0$. A matrix is positive definite only if all eigenvalues are *strictly* positive. The presence of a $0$ eigenvalue means the matrix is positive semi-definite, but not positive definite. For a non-zero vector $x = [0, 1]^{\\top}$, we have $x^{\\top} A_3 x = [0 \\ 1] \\begin{bmatrix} 1 & 0 \\\\ 0 & 0 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = 0$, violating the strict inequality $x^{\\top} A x > 0$.\nVerdict for $A_3$: Invalid (False).\n\nCase $4$: $A_4 = \\begin{bmatrix} 1 & 0 \\\\ 1 & 1 \\end{bmatrix}$\n(i) Finite entries: All entries ($1, 0$) are finite real numbers. Satisfied.\n(ii) Symmetry: The element at row $1$, column $2$ is $0$, while the element at row $2$, column $1$ is $1$. Since $0 \\neq 1$, the matrix is not symmetric ($A_4 \\neq A_4^{\\top}$). The matrix fails this condition.\nVerdict for $A_4$: Invalid (False).\n\nCase $5$: $A_5 = \\begin{bmatrix} 2 & -1 & 0 \\\\ -1 & 2 & -1 \\\\ 0 & -1 & 2 \\end{bmatrix}$\n(i) Finite entries: All entries are finite real numbers. Satisfied.\n(ii) Symmetry: The matrix is symmetric. Satisfied.\n(iii) Positive definiteness: We check the eigenvalues. The characteristic polynomial is $\\det(A_5 - \\lambda I) = (2-\\lambda)((2-\\lambda)^2-1) - (-1)(-(2-\\lambda)) = (2-\\lambda)^3 - 2(2-\\lambda) = (2-\\lambda)(\\lambda^2 - 4\\lambda + 2) = 0$. The roots are $\\lambda_1 = 2$ and, from the quadratic formula on $\\lambda^2 - 4\\lambda + 2 = 0$, $\\lambda = \\frac{4 \\pm \\sqrt{16-8}}{2} = 2 \\pm \\sqrt{2}$. The three eigenvalues are $2$, $2+\\sqrt{2}$, and $2-\\sqrt{2}$. Since $\\sqrt{2} \\approx 1.414$, all eigenvalues ($2$, $\\approx 3.414$, $\\approx 0.586$) are strictly positive. The matrix is positive definite.\nVerdict for $A_5$: Valid (True).\n\nCase $6$: $A_6 = \\begin{bmatrix} 3 \\end{bmatrix}$\n(i) Finite entries: The entry $3$ is a finite real number. Satisfied.\n(ii) Symmetry: A $1 \\times 1$ matrix is trivially symmetric. Satisfied.\n(iii) Positive definiteness: For a $1 \\times 1$ matrix $[a]$, the condition $x^{\\top} A x > 0$ for non-zero $x \\in \\mathbb{R}^1$ becomes $a x^2 > 0$. Since $x \\neq 0$, $x^2 > 0$. The inequality holds if and only if $a > 0$. Here, $a=3$, which is greater than $0$. The single eigenvalue is $3$, which is positive.\nVerdict for $A_6$: Valid (True).\n\nCase $7$: $A_7 = \\begin{bmatrix} 0 \\end{bmatrix}$\n(i) Finite entries: The entry $0$ is a finite real number. Satisfied.\n(ii) Symmetry: A $1 \\times 1$ matrix is trivially symmetric. Satisfied.\n(iii) Positive definiteness: Using the logic from Case $6$, we require $a>0$. Here, $a=0$. The condition is not satisfied, as for any non-zero $x$, $0 \\cdot x^2 = 0$, which is not strictly greater than $0$. The eigenvalue is $0$.\nVerdict for $A_7$: Invalid (False).\n\nCase $8$: $A_8 = \\begin{bmatrix} \\text{NaN} \\end{bmatrix}$\n(i) Finite entries: The entry is 'Not a Number' (NaN), which is not a finite real number. This condition is not satisfied.\nVerdict for $A_8$: Invalid (False).\n\nSummary of results:\n$A_1$: True\n$A_2$: False\n$A_3$: False\n$A_4$: False\n$A_5$: True\n$A_6$: True\n$A_7$: False\n$A_8$: False", "answer": "[True,False,False,False,True,True,False,False]", "id": "2449839"}, {"introduction": "In computational engineering, a crucial lesson is that mathematical equivalence on paper does not guarantee identical performance in a computer. The way we represent and solve a problem can have profound consequences for the accuracy and reliability of the solution due to the finite precision of floating-point arithmetic. The ubiquitous linear least-squares problem offers a classic illustration of this principle. This exercise [@problem_id:2449782] guides you through a comparative analysis of two popular methods: one based on QR decomposition and another on the formation of the normal equations $A^T A \\mathbf{x} = A^T \\mathbf{b}$. By examining the numerical stability and conditioning of each approach, you will uncover why one method is vastly superior for ill-conditioned problems, a vital insight for robust scientific computing.", "problem": "Consider a full column rank matrix $A \\in \\mathbb{R}^{m \\times n}$ with $m \\ge n$ and a vector $\\mathbf{b} \\in \\mathbb{R}^{m}$. The linear least-squares problem seeks $\\mathbf{x}^\\star \\in \\mathbb{R}^{n}$ that minimizes the Euclidean norm $||A\\mathbf{x} - \\mathbf{b}||_2$. Two standard computational approaches are: \n(i) compute an orthogonal-triangular (QR) factorization $A = Q R$ with $Q \\in \\mathbb{R}^{m \\times m}$ orthogonal and $R \\in \\mathbb{R}^{m \\times n}$ upper triangular, and then solve the triangular system for the least-squares solution; \n(ii) form the normal equations $A^T A \\mathbf{x} = A^T \\mathbf{b}$ and solve the resulting symmetric positive definite (SPD) system, for example by Cholesky factorization. Assume all computations are performed in floating-point arithmetic with unit roundoff $u$, and all norms are the matrix or vector $2$-norms. Let $\\sigma_{\\max}$ and $\\sigma_{\\min}$ denote, respectively, the largest and smallest singular values of $A$, and let the $2$-norm condition number be $\\kappa_2(A) = \\sigma_{\\max} / \\sigma_{\\min}$.\n\nWhich of the following statements about numerical stability and conditioning is or are correct?\n\nA. If $A$ has full column rank and computations are performed with unit roundoff $u$, then the forward relative error in the computed solution $\\hat{\\mathbf{x}}$ obtained by the normal equations method is typically on the order of $c\\,\\kappa_2(A)^2\\,u$, whereas a Householder QR-based method yields a forward relative error on the order of $c'\\,\\kappa_2(A)\\,u$, for modest problem- and implementation-dependent constants $c$ and $c'$.\n\nB. For any full column rank $A$, both approaches are equally stable with respect to the computed solution $\\hat{\\mathbf{x}}$ because they minimize the same residual norm $||A\\hat{\\mathbf{x}} - \\mathbf{b}||_2$, so their forward errors are of the same order in $u$.\n\nC. If $\\sigma_{\\max}$ and $\\sigma_{\\min}$ are the extreme singular values of $A$, then $\\kappa_2(A^T A) = \\left(\\dfrac{\\sigma_{\\max}}{\\sigma_{\\min}}\\right)^2 = \\kappa_2(A)^2$.\n\nD. Explicitly forming $A^T A$ can accumulate rounding errors and effectively magnify ill-conditioning by squaring the condition number, whereas a QR method based on orthogonal transformations preserves $2$-norms and is backward stable for the least-squares problem.\n\nE. Because $A^T A$ is SPD in exact arithmetic when $A$ has full column rank, the Cholesky factorization of the computed $A^T A$ in floating-point arithmetic cannot break down, even when $A$ is extremely ill-conditioned.", "solution": "The problem statement is subjected to validation.\n\n### Step 1: Extract Givens\n- A matrix $A \\in \\mathbb{R}^{m \\times n}$ with full column rank, where $m \\ge n$.\n- A vector $\\mathbf{b} \\in \\mathbb{R}^{m}$.\n- The linear least-squares problem: find $\\mathbf{x}^\\star \\in \\mathbb{R}^{n}$ that minimizes $||A\\mathbf{x} - \\mathbf{b}||_2$.\n- Method (i): QR factorization. $A = QR$, where $Q \\in \\mathbb{R}^{m \\times m}$ is orthogonal and $R \\in \\mathbb{R}^{m \\times n}$ is upper triangular.\n- Method (ii): Normal equations. $A^T A \\mathbf{x} = A^T \\mathbf{b}$.\n- Computations are performed in floating-point arithmetic with unit roundoff $u$.\n- All norms are the matrix or vector $2$-norm.\n- $\\sigma_{\\max}$ and $\\sigma_{\\min}$ are the largest and smallest singular values of $A$.\n- The $2$-norm condition number is $\\kappa_2(A) = \\sigma_{\\max} / \\sigma_{\\min}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement describes a fundamental topic in numerical linear algebra and computational engineering: the comparison of two standard algorithms for solving linear least-squares problems.\n- **Scientifically Grounded:** The concepts of QR factorization, normal equations, singular values, condition numbers, and floating-point error analysis are all standard and central to numerical analysis. The problem is based on established scientific principles.\n- **Well-Posed:** The problem asks to evaluate the correctness of several statements regarding the numerical stability and conditioning of these two methods. This is a well-defined question with a unique set of correct answers based on established theory.\n- **Objective:** The language is formal, precise, and free of ambiguity or subjective claims.\n- **Completeness:** All necessary definitions and context are provided to analyze the options.\n\n### Step 3: Verdict and Action\nThe problem statement is scientifically sound, well-posed, objective, and complete. It is therefore valid. I will proceed with the derivation of the solution and evaluation of the options.\n\n### Analysis\nThe central task is to analyze the numerical properties of solving the least-squares problem $\\min_{\\mathbf{x}} ||A\\mathbf{x} - \\mathbf{b}||_2$ via two methods: normal equations and QR factorization.\n\n**Method 1: Normal Equations**\nThis method transforms the least-squares problem into the linear system $A^T A \\mathbf{x} = A^T \\mathbf{b}$. The stability of solving this system depends on the condition number of the matrix $A^T A$. Let the singular value decomposition (SVD) of $A$ be $A = U \\Sigma V^T$, where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is a diagonal matrix of singular values $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_n > 0$. The condition number of $A$ is $\\kappa_2(A) = \\sigma_1 / \\sigma_n \\equiv \\sigma_{\\max} / \\sigma_{\\min}$.\n\nThe matrix of the normal equations is $A^T A = (U \\Sigma V^T)^T (U \\Sigma V^T) = V \\Sigma^T U^T U \\Sigma V^T$. Since $U^T U = I$, this simplifies to $A^T A = V (\\Sigma^T \\Sigma) V^T$. This is the eigendecomposition of the symmetric matrix $A^T A$. The eigenvalues of $A^T A$ are the diagonal entries of $\\Sigma^T \\Sigma$, which are $\\sigma_i^2$ for $i=1, \\dots, n$.\n\nThe condition number of a symmetric positive definite matrix is the ratio of its largest eigenvalue to its smallest eigenvalue. Therefore,\n$$\n\\kappa_2(A^T A) = \\frac{\\lambda_{\\max}(A^T A)}{\\lambda_{\\min}(A^T A)} = \\frac{\\sigma_{\\max}^2}{\\sigma_{\\min}^2} = \\left(\\frac{\\sigma_{\\max}}{\\sigma_{\\min}}\\right)^2 = (\\kappa_2(A))^2\n$$\nThe forward error in the computed solution $\\hat{\\mathbf{x}}$ of a linear system $C\\mathbf{x}=\\mathbf{d}$ is, in general, bounded by a term proportional to $\\kappa_2(C) u$. For the normal equations, the relevant matrix is $A^T A$, so the error is proportional to $\\kappa_2(A^T A) u = (\\kappa_2(A))^2 u$. This squaring of the condition number is a major source of numerical instability, especially when $A$ is ill-conditioned (i.e., $\\kappa_2(A)$ is large).\n\n**Method 2: QR Factorization**\nThis method relies on finding an orthogonal matrix $Q$ such that $A=QR$. The problem becomes $\\min ||QR\\mathbf{x} - \\mathbf{b}||_2$. Since orthogonal transformations preserve the $2$-norm, this is equivalent to $\\min ||Q^T(QR\\mathbf{x} - \\mathbf{b})||_2 = \\min ||R\\mathbf{x} - Q^T\\mathbf{b}||_2$.\nLet $R$ be partitioned as $R = \\begin{pmatrix} R_1 \\\\ 0 \\end{pmatrix}$, where $R_1 \\in \\mathbb{R}^{n \\times n}$ is upper triangular and invertible (since $A$ has full column rank), and let $Q^T\\mathbf{b}$ be partitioned as $Q^T\\mathbf{b} = \\begin{pmatrix} \\mathbf{c}_1 \\\\ \\mathbf{c}_2 \\end{pmatrix}$. The minimization problem becomes\n$$\n\\min \\left\\| \\begin{pmatrix} R_1\\mathbf{x} - \\mathbf{c}_1 \\\\ -\\mathbf{c}_2 \\end{pmatrix} \\right\\|_2^2 = \\min (||R_1\\mathbf{x} - \\mathbf{c}_1||_2^2 + ||\\mathbf{c}_2||_2^2)\n$$\nThis is minimized when $R_1\\mathbf{x} - \\mathbf{c}_1 = \\mathbf{0}$, i.e., by solving the upper triangular system $R_1\\mathbf{x} = \\mathbf{c}_1$.\nThe condition number of $A$ is related to the condition number of $R_1$. Since $A=QR$, we have $\\kappa_2(A) = \\kappa_2(QR) = \\kappa_2(R) = \\kappa_2(R_1)$. The numerical solution involves solving a system with the matrix $R_1$, so the forward error is expected to be proportional to $\\kappa_2(R_1)u = \\kappa_2(A)u$. This method avoids forming $A^TA$ and thus avoids squaring the condition number. QR factorizations performed using Householder transformations or Givens rotations are known to be backward stable.\n\n### Option-by-Option Analysis\n\n**A. If $A$ has full column rank and computations are performed with unit roundoff $u$, then the forward relative error in the computed solution $\\hat{\\mathbf{x}}$ obtained by the normal equations method is typically on the order of $c\\,\\kappa_2(A)^2\\,u$, whereas a Householder QR-based method yields a forward relative error on the order of $c'\\,\\kappa_2(A)\\,u$, for modest problem- and implementation-dependent constants $c$ and $c'$.**\n\nThis statement directly reflects the standard error analysis of the two methods. The method of normal equations solves a system whose effective condition number is $\\kappa_2(A)^2$, leading to a forward error proportional to this quantity. The QR method works with matrices that have a condition number of $\\kappa_2(A)$, leading to a forward error proportional to $\\kappa_2(A)$. The statement accurately summarizes the typical error behavior, excluding terms related to the residual size, which is a common simplification.\n**Verdict: Correct.**\n\n**B. For any full column rank $A$, both approaches are equally stable with respect to the computed solution $\\hat{\\mathbf{x}}$ because they minimize the same residual norm $||A\\hat{\\mathbf{x}} - \\mathbf{b}||_2$, so their forward errors are of the same order in $u$.**\n\nThis statement is fundamentally flawed. While both methods solve the same mathematical problem in exact arithmetic, their behavior in floating-point arithmetic is vastly different. The stability of a numerical algorithm depends on the sequence of computational steps, not just the problem it aims to solve. As shown above, the normal equations method involves an intermediate step (forming $A^TA$) that can catastrophically degrade numerical accuracy by squaring the condition number. The QR method avoids this. Therefore, they are not equally stable.\n**Verdict: Incorrect.**\n\n**C. If $\\sigma_{\\max}$ and $\\sigma_{\\min}$ are the extreme singular values of $A$, then $\\kappa_2(A^T A) = \\left(\\dfrac{\\sigma_{\\max}}{\\sigma_{\\min}}\\right)^2 = \\kappa_2(A)^2$.**\n\nThis is a precise mathematical identity. As derived in the analysis section, the eigenvalues of $A^T A$ are the squares of the singular values of $A$. The $2$-norm condition number of the symmetric positive definite matrix $A^T A$ is the ratio of its largest to its smallest eigenvalue, which is $\\sigma_{\\max}^2 / \\sigma_{\\min}^2$. By definition, $\\kappa_2(A) = \\sigma_{\\max} / \\sigma_{\\min}$. Therefore, $\\kappa_2(A^T A) = (\\sigma_{\\max} / \\sigma_{\\min})^2 = \\kappa_2(A)^2$. The statement is a correct theorem from linear algebra.\n**Verdict: Correct.**\n\n**D. Explicitly forming $A^T A$ can accumulate rounding errors and effectively magnify ill-conditioning by squaring the condition number, whereas a QR method based on orthogonal transformations preserves $2$-norms and is backward stable for the least-squares problem.**\n\nThis statement provides a qualitative but accurate description of the numerical properties of both methods.\n- \"Explicitly forming $A^T A$ can accumulate rounding errors\": True. The computation of the matrix product itself introduces roundoff. For an ill-conditioned $A$, information can be lost. For example, if $\\sigma_{\\min}$ is small enough such that $\\sigma_{\\min}^2$ is smaller than the machine precision relative to $\\sigma_{\\max}^2$, the computed $A^TA$ will be numerically rank-deficient.\n- \"magnify ill-conditioning by squaring the condition number\": True, as proven for option C.\n- \"a QR method based on orthogonal transformations preserves $2$-norms\": True. This is the definition of an orthogonal matrix $Q$: $||Q\\mathbf{v}||_2 = ||\\mathbf{v}||_2$ for any vector $\\mathbf{v}$. This property is key to the stability of the method.\n- \"is backward stable for the least-squares problem\": True. QR factorization via Householder reflections is a cornerstone backward stable algorithm in numerical linear algebra.\nThe statement compiles several correct facts to provide a valid comparison.\n**Verdict: Correct.**\n\n**E. Because $A^T A$ is SPD in exact arithmetic when $A$ has full column rank, the Cholesky factorization of the computed $A^T A$ in floating-point arithmetic cannot break down, even when $A$ is extremely ill-conditioned.**\n\nThis statement is false. In floating-point arithmetic, we compute $\\hat{C} = \\text{fl}(A^T A)$. Due to rounding errors during the matrix multiplication, $\\hat{C}$ is not guaranteed to be symmetric positive definite, even though the exact matrix $A^T A$ is. If $A$ is sufficiently ill-conditioned, some of the smaller eigenvalues of $A^T A$, which are $\\sigma_i^2$, can be perturbed to become zero or negative in the computed matrix $\\hat{C}$. The Cholesky factorization algorithm requires taking square roots of diagonal elements at each step; if a computed diagonal element becomes non-positive, the algorithm fails. This breakdown is a practical risk when using normal equations with ill-conditioned matrices.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{ACD}$$", "id": "2449782"}]}