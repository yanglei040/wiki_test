{"hands_on_practices": [{"introduction": "How can you be sure the numerical method you've implemented is correct? The most fundamental \"sanity check\" in computational science is the convergence study. This practice will equip you with the essential skill of empirically measuring your method's accuracy by estimating its order of convergence, $p$. By analyzing how the error decreases as the mesh size $h$ shrinks, you can verify if your code behaves as theory predicts, a crucial step in building reliable simulations. [@problem_id:2389343]", "problem": "You are given a one-dimensional setting on the closed interval $[0,1]$ with a uniform grid of $N+1$ nodes $x_i = i h$, where $h = 1/N$ and $i \\in \\{0,1,\\dots,N\\}$. The exact smooth function is $u(x) = \\sin(2\\pi x)$. For a numerical method with mesh size $h$, denote the pointwise error by $e_h(x) = u_h(x) - u(x)$. You will use error norms to automatically detect the order of convergence $p$ of the method under the following assumptions.\n\nFundamental base:\n- Discrete $L^1$, $L^2$, and $L^\\infty$ norms on a uniform grid approximate their continuous counterparts using Riemann sums. For a vector of errors $e_i \\approx e_h(x_i)$,\n  - the discrete $L^1$ norm is $\\|e_h\\|_{1,h} = h \\sum_{i=0}^N |e_i|$,\n  - the discrete $L^2$ norm is $\\|e_h\\|_{2,h} = \\sqrt{h \\sum_{i=0}^N e_i^2}$,\n  - the discrete $L^\\infty$ norm is $\\|e_h\\|_{\\infty,h} = \\max_{0 \\le i \\le N} |e_i|$.\n- In the asymptotic regime of sufficiently small $h$, many consistent numerical methods exhibit an error-norm scaling of the form $\\|e_h\\| \\approx C h^p$ for some constants $C > 0$ and $p > 0$ that do not depend on $h$.\n\nTask:\n- Starting from the definitions above and the asymptotic scaling assumption, derive a principled estimator for the order of convergence $p$ that uses only error norms computed at multiple mesh sizes $h$. Your estimator must not assume knowledge of $C$ and must be robust to small higher-order contamination terms in the error.\n- Implement a complete program that:\n  1. Constructs a family of approximations $u_h(x)$ for a set of mesh sizes $h$ and computes the corresponding error norms.\n  2. Applies your estimator to obtain $\\widehat{p}$ from the finest levels in each case.\n  3. Aggregates the results for a given test suite and prints them in the required final output format.\n\nSynthetic “unknown” method model:\n- For each mesh size $h$, the approximate solution is defined as\n  $$u_h(x) = u(x) + C\\,h^p\\,w(x) + \\delta(x;h),$$\n  where $w(x)$ is a smooth “error shape” and $\\delta(x;h)$ is an optional higher-order contamination term modeling pre-asymptotic effects. Both $w(x)$ and $\\delta(x;h)$ are fully specified in the test suite below, but your estimator must not use any prior knowledge of $C$, $p$, $w(x)$, or $\\delta(x;h)$ beyond computing norms of $e_h(x)$.\n\nNorms to compute:\n- For each test case and each $h$, compute the discrete norm $\\|e_h\\|_{q,h}$ corresponding to the specified choice of $q \\in \\{1,2,\\infty\\}$ with the definitions given above, using the uniform grid with $N+1$ points and $h = 1/N$.\n\nEstimator application rule:\n- For each test case, estimate $p$ by applying your estimator to the three finest mesh sizes (i.e., the three smallest $h$ values in that case).\n\nTest suite:\n- Use the following mesh sizes in each case: $N \\in \\{10,20,40,80,160\\}$, so that $h \\in \\{1/10,1/20,1/40,1/80,1/160\\}$.\n- Case $1$ (happy path): $p = 2$, $C = 0.7$, $w(x) = \\cos(\\pi x)$, $\\delta(x;h) \\equiv 0$, norm $L^2$.\n- Case $2$ (different norm): $p = 1$, $C = 1.5$, $w(x) = e^{x}$, $\\delta(x;h) \\equiv 0$, norm $L^1$.\n- Case $3$ (fractional order and $L^\\infty$): $p = 1.5$, $C = 0.3$, $w(x) = \\sin(3\\pi x)$, $\\delta(x;h) \\equiv 0$, norm $L^\\infty$.\n- Case $4$ (pre-asymptotic contamination): $p = 3$, $C = 0.2$, $w(x) = \\cos(5\\pi x)$, $\\delta(x;h) = D\\,h^{p+1}\\,\\sin(7\\pi x)$ with $D = 5.0$, norm $L^2$.\n\nAlgorithmic requirements:\n- Use only the norm values and mesh sizes to estimate $p$ for each case.\n- To mitigate pre-asymptotic effects, apply your estimator only to the finest three mesh sizes, i.e., $N \\in \\{40,80,160\\}$, in every case.\n\nFinal output specification:\n- Your program must produce a single line of output containing a list with the four estimated orders of convergence, in the order of Cases $1$ through $4$, each rounded to $3$ decimal places. The format must be exactly a comma-separated list enclosed in square brackets, for example, $[a,b,c,d]$, where $a$, $b$, $c$, and $d$ are decimal numerals.\n\nNo physical units are involved. All angles are in radians. The final answer must be produced without requiring any user input or external files.", "solution": "**Derivation of the Convergence Order Estimator**\n\nThe problem is to determine the order of convergence, $p$, from the asymptotic relationship for the error norm, $\\|e_h\\|$, as a function of the mesh size, $h$. The governing model is given as:\n$$\n\\|e_h\\| \\approx C h^p\n$$\nwhere $C$ and $p$ are constants independent of $h$ for sufficiently small $h$. The constant $C$ is unknown. To eliminate $C$ and solve for $p$, we can transform the relation into a linear form by taking the natural logarithm of both sides:\n$$\n\\ln(\\|e_h\\|) \\approx \\ln(C h^p) = \\ln(C) + \\ln(h^p) = \\ln(C) + p \\ln(h)\n$$\nThis equation is of the form $Y = A + p X$, where $Y = \\ln(\\|e_h\\|)$, $X = \\ln(h)$, and the intercept $A = \\ln(C)$ is a constant. This reveals a linear relationship between the logarithm of the error norm and the logarithm of the mesh size. The order of convergence, $p$, is the slope of this line.\n\nThe problem requires using data from three mesh refinements, let us denote them $h_1$, $h_2$, and $h_3$, with corresponding computed error norms $E_1$, $E_2$, and $E_3$. This provides us with three data points $(X_i, Y_i) = (\\ln(h_i), \\ln(E_i))$ for $i \\in \\{1, 2, 3\\}$. A robust estimator for the slope $p$ can be obtained using linear least-squares regression. This method finds the line that minimizes the sum of the squared vertical distances from the data points to the line. For a set of $n$ points $(X_i, Y_i)$, the formula for the slope $\\hat{p}$ of the best-fit line is:\n$$\n\\hat{p} = \\frac{n \\sum_{i=1}^n X_i Y_i - \\left(\\sum_{i=1}^n X_i\\right) \\left(\\sum_{i=1}^n Y_i\\right)}{n \\sum_{i=1}^n X_i^2 - \\left(\\sum_{i=1}^n X_i\\right)^2}\n$$\nThis approach is principled as it uses all available information from the three specified mesh levels. It is also robust to small perturbations from the ideal linear model, such as those introduced by higher-order terms in the error (e.g., the $\\delta(x;h)$ term in Case $4$), as the regression process has an averaging effect.\n\n**Implementation Strategy**\n\nThe implementation will follow a systematic procedure for each test case.\n1. Define the parameters for the test case: the true order $p_{true}$, the constant $C$, the error shape function $w(x)$, the contamination term $\\delta(x;h)$, and the norm type $q \\in \\{1, 2, \\infty\\}$.\n2. For the three finest mesh resolutions, $N \\in \\{40, 80, 160\\}$, perform the following steps:\n    a. Calculate the mesh size $h=1/N$.\n    b. Generate the uniform grid of $N+1$ points, $x_i = i h$ for $i=0, \\dots, N$.\n    c. Evaluate the exact solution $u(x_i) = \\sin(2\\pi x_i)$ on the grid.\n    d. Construct the approximate solution $u_h(x_i) = u(x_i) + C\\,h^{p_{true}}\\,w(x_i) + \\delta(x_i;h, p_{true})$ on the grid. Note that the true order $p_{true}$ is used here only to generate the synthetic data, as specified.\n    e. Compute the pointwise error vector $e_i = u_h(x_i) - u(x_i)$.\n    f. Calculate the specified discrete error norm $\\|e_h\\|_{q,h}$ according to the provided formulas.\n3. After computing the three error norms ($E_1, E_2, E_3$) for the three mesh sizes ($h_1, h_2, h_3$), create two vectors: $X = [\\ln(h_1), \\ln(h_2), \\ln(h_3)]$ and $Y = [\\ln(E_1), \\ln(E_2), \\ln(E_3)]$.\n4. Apply a linear regression algorithm to the data $(X, Y)$ to find the slope. A standard numerical library function, such as `numpy.polyfit` with degree $1$, provides an efficient implementation of the least-squares formula, returning the slope as the first coefficient. This slope is the estimated order of convergence, $\\hat{p}$.\n5. This process is repeated for all four test cases. The resulting estimates are collected and formatted for the final output as a list of numbers rounded to $3$ decimal places.\n\nThis method adheres strictly to the problem constraints: it uses only the norm values and mesh sizes for the estimation and applies a principled, robust technique suitable for analyzing convergence data.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by estimating the order of convergence for four test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"case\": 1,\n            \"p_true\": 2.0,\n            \"C\": 0.7,\n            \"w_func\": lambda x: np.cos(np.pi * x),\n            \"delta_func\": lambda x, h, p: 0.0,\n            \"q\": 2\n        },\n        {\n            \"case\": 2,\n            \"p_true\": 1.0,\n            \"C\": 1.5,\n            \"w_func\": lambda x: np.exp(x),\n            \"delta_func\": lambda x, h, p: 0.0,\n            \"q\": 1\n        },\n        {\n            \"case\": 3,\n            \"p_true\": 1.5,\n            \"C\": 0.3,\n            \"w_func\": lambda x: np.sin(3 * np.pi * x),\n            \"delta_func\": lambda x, h, p: 0.0,\n            \"q\": np.inf\n        },\n        {\n            \"case\": 4,\n            \"p_true\": 3.0,\n            \"C\": 0.2,\n            \"w_func\": lambda x: np.cos(5 * np.pi * x),\n            \"delta_func\": lambda x, h, p: 5.0 * h**(p + 1) * np.sin(7 * np.pi * x),\n            \"q\": 2\n        }\n    ]\n\n    results = []\n    # Mesh resolutions to use for the estimator, as per the problem statement.\n    mesh_resolutions_N = [40, 80, 160]\n\n    for case in test_cases:\n        p_true = case[\"p_true\"]\n        C = case[\"C\"]\n        w_func = case[\"w_func\"]\n        delta_func = case[\"delta_func\"]\n        q = case[\"q\"]\n\n        mesh_sizes_h = []\n        error_norms = []\n\n        for N in mesh_resolutions_N:\n            h = 1.0 / N\n            mesh_sizes_h.append(h)\n            \n            # Create the uniform grid from x=0 to x=1 with N+1 points.\n            x = np.linspace(0.0, 1.0, N + 1)\n            \n            # Compute the exact solution u(x) = sin(2*pi*x).\n            u_exact = np.sin(2 * np.pi * x)\n            \n            # Compute the approximate solution using the synthetic model.\n            error_shape = C * h**p_true * w_func(x)\n            contamination = delta_func(x, h, p_true)\n            u_approx = u_exact + error_shape + contamination\n            \n            # Compute the pointwise error.\n            error_vector = u_approx - u_exact\n            \n            # Compute the specified discrete norm of the error.\n            norm = 0.0\n            if q == 1:\n                # Discrete L1 norm: h * sum(|e_i|)\n                norm = h * np.sum(np.abs(error_vector))\n            elif q == 2:\n                # Discrete L2 norm: sqrt(h * sum(e_i^2))\n                norm = np.sqrt(h * np.sum(error_vector**2))\n            elif q == np.inf:\n                # Discrete L-infinity norm: max(|e_i|)\n                norm = np.max(np.abs(error_vector))\n            \n            error_norms.append(norm)\n\n        # Estimate the order of convergence p using linear regression on the log-log data.\n        # The model is log(E) = log(C) + p * log(h).\n        # We find the slope 'p' of the best-fit line for (log(h), log(E)).\n        log_h = np.log(np.array(mesh_sizes_h))\n        log_E = np.log(np.array(error_norms))\n        \n        # np.polyfit with degree 1 fits a line and returns [slope, intercept].\n        # The slope is our estimate for the order of convergence p.\n        p_estimated, _ = np.polyfit(log_h, log_E, 1)        \n        results.append(p_estimated)\n\n    # Final print statement in the exact required format.\n    # The output is a comma-separated list of results rounded to 3 decimal places, inside brackets.\n    print(f\"[{','.join(f'{r:.3f}' for r in results)}]\")\n\nsolve()\n```", "id": "2389343"}, {"introduction": "While global error metrics like the $L^2$ norm are powerful, they can sometimes paint a deceptively rosy picture. This practice serves as a vital cautionary tale, demonstrating how a solution can appear to be \"converging\" in an average sense while exhibiting disastrous, non-physical behavior at specific points. By exploring a stiff ordinary differential equation, you will see firsthand why it is critical to look beyond a single error number and consider different norms to gain a complete understanding of your solution's quality. [@problem_id:2389318]", "problem": "Construct a fully specified computational experiment that demonstrates the contrast between integral-norm convergence and pointwise behavior for a stiff ordinary differential equation. Consider the initial value problem on the interval $[0,1]$:\n- Stiff linear ordinary differential equation: $y'(t) = -\\lambda y(t)$ with stiffness parameter $\\lambda = 100$.\n- Initial condition: $y(0) = 1$.\n- Exact solution: $y(t) = e^{-\\lambda t}$.\n\nFor each uniform time step count $N \\in \\mathbb{N}$, define $h = 1/N$ and the discrete approximation $\\{Y_n\\}_{n=0}^{N}$ by the Forward (Explicit) Euler time-stepping scheme\n$$\nY_{n+1} = Y_n + h(-\\lambda Y_n) = (1 - h\\lambda) Y_n,\\quad Y_0 = 1,\n$$\nand define the continuous, piecewise linear interpolant $y_h:[0,1]\\to\\mathbb{R}$ by linearly interpolating the nodal values $\\{(t_n,Y_n)\\}_{n=0}^{N}$ with $t_n = nh$.\n\nDefine the time-continuous $L^2$ error norm for a given $h$ as\n$$\nE_2(h) = \\left(\\int_0^1 \\lvert y_h(t) - y(t) \\rvert^2\\,dt\\right)^{1/2}.\n$$\nDefine a pointwise oscillation indicator for the discrete trajectory by counting sign changes in the sequence $\\{Y_n\\}_{n=0}^{N}$. Because the exact solution $y(t)$ is strictly positive on $[0,1]$, any sign change of the discrete solution is considered incorrect pointwise behavior.\n\nYour program must, for each test case listed below, evaluate the following two boolean metrics over the three step counts $N$ given in that test case:\n- Strictly decreasing integral-norm error across refinements: return $\\mathrm{True}$ if $E_2(h_1) > E_2(h_2) > E_2(h_3)$ holds for the three step sizes $h_i = 1/N_i$ in the order listed for the test case, and $\\mathrm{False}$ otherwise.\n- Presence of pointwise oscillations: return $\\mathrm{True}$ if at least one of the three discrete solutions $\\{Y_n\\}$, corresponding to the three step counts in the test case, exhibits at least one sign change, and $\\mathrm{False}$ otherwise.\n\nTest Suite (each line is a test case specifying a triple of step counts $N$):\n- Case A (stable but oscillatory regime): $N \\in \\{52,55,59\\}$.\n- Case B (well resolved regime): $N \\in \\{200,400,800\\}$.\n- Case C (stability boundary and beyond): $N \\in \\{50,49,48\\}$.\n- Case D (mixed regime): $N \\in \\{83,100,125\\}$.\n\nYour program must compute $E_2(h)$ exactly for the piecewise linear $y_h$ or by numerical integration with sufficient accuracy so that the boolean comparisons are correct to within an absolute tolerance of $10^{-8}$ on each $E_2(h)$ value, and must implement the sign-change test strictly on the discrete nodal values $\\{Y_n\\}$.\n\nFinal Output Format:\n- Your program should produce a single line of output containing the eight boolean results aggregated as a comma-separated list enclosed in square brackets, in the order\n$$\n[\\text{A\\_dec},\\text{A\\_osc},\\ \\text{B\\_dec},\\text{B\\_osc},\\ \\text{C\\_dec},\\text{C\\_osc},\\ \\text{D\\_dec},\\text{D\\_osc}],\n$$\nwhere, for example, $\\text{A\\_dec}$ is the boolean indicating whether $E_2$ is strictly decreasing across the three $N$ values of Case A, and $\\text{A\\_osc}$ indicates whether any sign change occurs in the discrete trajectories for Case A. The booleans must be printed as either $\\mathrm{True}$ or $\\mathrm{False}$ and nothing else on the line.", "solution": "We formalize the computational experiment entirely from definitions. The ordinary differential equation is $y'(t) = -\\lambda y(t)$ with $\\lambda = 100$, $t \\in [0,1]$, $y(0)=1$. The exact solution is $y(t) = e^{-\\lambda t}$, which is strictly positive and strictly decreasing on $[0,1]$. Stiffness arises because the time scale of decay is $1/\\lambda$, so for $\\lambda=100$ the solution exhibits rapid decay on the small scale $t=\\mathcal{O}(10^{-2})$ relative to the domain length $1$.\n\nFor a uniform grid with $N$ steps, $h = 1/N$ and $t_n = nh$, the Forward (Explicit) Euler scheme is defined from first principles by applying the definition of the derivative to the semidiscrete update:\n$$\n\\frac{Y_{n+1} - Y_n}{h} \\approx y'(t_n) = -\\lambda y(t_n),\n$$\nand by replacing $y(t_n)$ with $Y_n$, we obtain\n$$\nY_{n+1} = Y_n + h(-\\lambda Y_n) = (1 - h\\lambda) Y_n,\\quad Y_0 = 1.\n$$\nThis closed-form recursion yields\n$$\nY_n = (1 - h\\lambda)^n.\n$$\nThe continuous, piecewise linear interpolant $y_h(t)$ is then defined on each subinterval $[t_n,t_{n+1}]$ by the unique linear function matching the nodal values:\n$$\ny_h(t) = \\frac{t_{n+1}-t}{h} Y_n + \\frac{t - t_n}{h} Y_{n+1},\\qquad t \\in [t_n,t_{n+1}].\n$$\n\nWe measure the error in the time-continuous $L^2$ norm,\n$$\nE_2(h) = \\left(\\int_0^1 \\lvert y_h(t) - e^{-\\lambda t} \\rvert^2\\,dt\\right)^{1/2}.\n$$\nThis is a norm on the difference function and, by definition, integrates the squared pointwise error over time. Because $y_h$ is piecewise linear and $y$ is smooth, $E_2(h)$ can be evaluated either by exact integration on each subinterval (expansion of a quadratic minus an exponential with known antiderivatives) or by sufficiently accurate numerical quadrature so that comparisons between different $h$ values are correct within a small absolute tolerance.\n\nWe also quantify incorrect pointwise behavior by detecting sign changes in the discrete sequence $\\{Y_n\\}_{n=0}^{N}$. Since the exact solution satisfies $y(t)>0$ on $[0,1]$, any sign change in the numerical sequence is a nonphysical oscillation. For the Forward Euler scheme on the linear test equation, the amplification factor is\n$$\nG(h\\lambda) = 1 - h\\lambda.\n$$\nIf $0 < h\\lambda < 1$, then $G(h\\lambda) \\in (0,1)$, so $Y_{n+1} = G Y_n$ remains positive and monotonically decreasing, exhibiting no sign changes. If $1 < h\\lambda < 2$, then $G(h\\lambda) \\in (-1,0)$, so $Y_{n+1}$ alternates sign and decays in magnitude, producing pointwise oscillations. If $h\\lambda \\ge 2$, then $\\lvert G(h\\lambda) \\rvert \\ge 1$, and the method is at the stability boundary or unstable, typically yielding persistent or growing oscillations.\n\nTest Suite justification:\n- Case A uses $N \\in \\{52,55,59\\}$, i.e., $h\\lambda \\in \\{100/52,100/55,100/59\\} \\subset (1,2)$, so the discrete solutions are oscillatory but stable. As $N$ increases, $h$ decreases and the $L^2$ error $E_2(h)$ is expected to strictly decrease; the oscillation indicator should be positive because of sign alternation.\n- Case B uses $N \\in \\{200,400,800\\}$, i.e., $h\\lambda \\in \\{0.5,0.25,0.125\\} \\subset (0,1)$, so the method is stable and non-oscillatory. $E_2(h)$ should strictly decrease and the oscillation indicator should be negative (no sign changes).\n- Case C uses $N \\in \\{50,49,48\\}$, i.e., $h\\lambda \\in \\{2.0,\\approx 2.0408,\\approx 2.0833\\}$, spanning the stability boundary and an unstable regime. In this ordering, the $L^2$ errors are not strictly decreasing across the sequence (the first entry at the stability boundary is typically much smaller than the subsequent unstable entries), while the oscillation indicator is positive due to alternation of signs and instability.\n- Case D uses $N \\in \\{83,100,125\\}$, i.e., $h\\lambda \\in \\{\\approx 1.2048,1.0,0.8\\}$, transitioning from oscillatory stable to non-oscillatory stable. The $L^2$ error should strictly decrease and the oscillation indicator should be positive because at least the coarsest choice yields a sign change.\n\nAlgorithmic implementation from these principles:\n- For each $N$, compute the nodal values $\\{Y_n\\}$ via the recurrence and form $y_h$ by linear interpolation on $[0,1]$.\n- Evaluate $E_2(h)$ by numerically integrating $\\lvert y_h(t) - e^{-\\lambda t}\\rvert^2$ on a dense uniform partition of $[0,1]$, e.g., via the composite trapezoid rule with a sufficiently large even number of subintervals to achieve absolute accuracy better than $10^{-8}$.\n- Determine the oscillation indicator by checking whether there exists an index $n$ with $Y_n Y_{n+1} < 0$ (a sign change).\n- For each test case consisting of three $N$ values ordered as specified, compute the booleans for strict decrease of $E_2$ across the triple and the presence of oscillations across the triple.\n\nThe final output is the list\n$$\n[\\text{A\\_dec},\\text{A\\_osc},\\ \\text{B\\_dec},\\text{B\\_osc},\\ \\text{C\\_dec},\\text{C\\_osc},\\ \\text{D\\_dec},\\text{D\\_osc}],\n$$\nprinted on a single line with booleans.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef forward_euler_linear_decay(lmbda: float, N: int):\n    \"\"\"\n    Solve y' = -lambda*y, y(0)=1 on [0,1] with Forward Euler using N steps.\n    Returns time nodes t (size N+1) and solution values Y (size N+1).\n    \"\"\"\n    h = 1.0 / N\n    t = np.linspace(0.0, 1.0, N + 1)\n    Y = np.empty(N + 1, dtype=float)\n    Y[0] = 1.0\n    factor = 1.0 - h * lmbda\n    # Use recurrence: Y_n = factor^n\n    # Iterative to avoid potential under/overflow in pow for large N and debug clarity\n    for n in range(N):\n        Y[n + 1] = Y[n] * factor\n    return t, Y\n\ndef piecewise_linear_interpolant_values(t_nodes: np.ndarray, y_nodes: np.ndarray, t_query: np.ndarray):\n    \"\"\"\n    Evaluate the piecewise linear interpolant through (t_nodes, y_nodes) at t_query.\n    Uses numpy's linear interpolation.\n    Assumes t_nodes is sorted ascending spanning [0,1].\n    \"\"\"\n    return np.interp(t_query, t_nodes, y_nodes)\n\ndef l2_error(lmbda: float, t_nodes: np.ndarray, y_nodes: np.ndarray, num_subintervals: int = 200000):\n    \"\"\"\n    Compute L2(0,1) error norm between piecewise-linear y_h and exact y(t)=exp(-lambda t).\n    Uses composite trapezoidal rule on a uniform grid with num_subintervals subintervals.\n    \"\"\"\n    # Ensure even number of subintervals for symmetric sampling (not required for trapz but good practice)\n    if num_subintervals % 2 == 1:\n        num_subintervals += 1\n    tq = np.linspace(0.0, 1.0, num_subintervals + 1)\n    yh = piecewise_linear_interpolant_values(t_nodes, y_nodes, tq)\n    y_exact = np.exp(-lmbda * tq)\n    err_sq = (yh - y_exact) ** 2\n    integral = np.trapz(err_sq, tq)\n    return np.sqrt(integral)\n\ndef has_sign_change(y_nodes: np.ndarray):\n    \"\"\"\n    Return True if the discrete sequence y_nodes exhibits any sign change between consecutive nodes.\n    \"\"\"\n    # Consider exact zero as non-negative for sign-change detection\n    prod = y_nodes[:-1] * y_nodes[1:]\n    return np.any(prod < 0.0)\n\ndef evaluate_case(lmbda: float, N_list):\n    \"\"\"\n    For a list of three N values, compute:\n    - strictly decreasing L2 error across the list order\n    - presence of oscillations (any sign change) among the three solutions\n    Returns tuple (dec_bool, osc_bool).\n    \"\"\"\n    E2_vals = []\n    osc_flags = []\n    for N in N_list:\n        t_nodes, y_nodes = forward_euler_linear_decay(lmbda, N)\n        E2 = l2_error(lmbda, t_nodes, y_nodes, num_subintervals=200000)\n        E2_vals.append(E2)\n        osc_flags.append(has_sign_change(y_nodes))\n    dec = (E2_vals[0] > E2_vals[1]) and (E2_vals[1] > E2_vals[2])\n    osc = any(osc_flags)\n    return dec, osc\n\ndef solve():\n    # Define parameters\n    lmbda = 100.0  # stiffness parameter\n    # Test cases: lists of N (number of steps); h = 1/N\n    case_A = [52, 55, 59]     # stable but oscillatory (1 < h*lambda < 2)\n    case_B = [200, 400, 800]  # well-resolved (h*lambda << 1)\n    case_C = [50, 49, 48]     # boundary and unstable (h*lambda >= 2), ordered to break monotonic decrease\n    case_D = [83, 100, 125]   # mixed: oscillatory -> boundary -> non-oscillatory\n\n    test_cases = [case_A, case_B, case_C, case_D]\n\n    results = []\n    for case in test_cases:\n        dec, osc = evaluate_case(lmbda, case)\n        results.append(dec)\n        results.append(osc)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2389318"}, {"introduction": "Why can different norms, like the $L^2$ norm and the $H^1$ norm, tell such different stories about the same error? This exercise delves into the elegant mathematical theory that explains this phenomenon: the failure of norm equivalence in infinite-dimensional spaces. Using a sequence of simple \"hat\" functions common in the Finite Element Method, we will construct a scenario where a function's derivative remains large while its magnitude vanishes. This will provide a concrete illustration of why a function sequence can converge in one norm but diverge in another, a foundational concept for advanced error analysis. [@problem_id:2389350]", "problem": "Consider the interval $\\left[0,1\\right]$ and the standard conforming finite element space of continuous, piecewise-linear functions with zero boundary values, which is the typical trial space used in the Finite Element Method (FEM) for the Poisson problem on $\\left[0,1\\right]$. For a uniform mesh with mesh size $h \\in \\left(0,\\tfrac{1}{2}\\right]$ such that $\\tfrac{1}{2h}$ is an integer, let $\\varphi_h$ denote the nodal hat basis function centered at $x=\\tfrac{1}{2}$, that is,\n$$\n\\varphi_h(x) \\;=\\; \\begin{cases}\n1 - \\dfrac{\\left|x - \\tfrac{1}{2}\\right|}{h}, & \\text{if } \\left|x - \\tfrac{1}{2}\\right| \\le h, \\\\[6pt]\n0, & \\text{otherwise.}\n\\end{cases}\n$$\nDefine $u_h(x) = \\alpha_h\\,\\varphi_h(x)$ with the scaling $\\alpha_h > 0$ chosen so that the $H^1$ seminorm of $u_h$ equals $1$, namely $\\lVert u_h' \\rVert_{L^2(0,1)} = 1$. This construction yields a sequence $\\left\\{u_h\\right\\}$ of FEM functions as $h \\to 0$.\n\nYour task is to use this sequence to demonstrate the failure of norm equivalence in infinite-dimensional settings by quantifying how the $L^2$ norm behaves when the $H^1$ seminorm is fixed. For each specified mesh size $h$, compute the following quantities for $u_h$:\n- the $L^2$ norm $\\lVert u_h \\rVert_{L^2(0,1)}$,\n- the $L^\\infty$ norm $\\lVert u_h \\rVert_{L^\\infty(0,1)}$.\n\nNo physical units are involved in this problem.\n\nTest Suite:\nUse the mesh sizes $h \\in \\left\\{\\tfrac{1}{2},\\,\\tfrac{1}{4},\\,\\tfrac{1}{8},\\,\\tfrac{1}{16},\\,\\tfrac{1}{32},\\,\\tfrac{1}{64}\\right\\}$.\n\nAnswer format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each $h$ in the order listed above, output two floating-point numbers in the following order:\n$\\left[\\lVert u_h \\rVert_{L^2(0,1)},\\, \\lVert u_h \\rVert_{L^\\infty(0,1)}\\right]$.\nAll numbers must be rounded to exactly six digits after the decimal point. The final output must therefore contain $12$ numbers and have the form\n$[\\text{L2}_1,\\text{Linfty}_1,\\text{L2}_2,\\text{Linfty}_2,\\ldots,\\text{L2}_6,\\text{Linfty}_6]$,\nwhere $\\text{L2}_k$ and $\\text{Linfty}_k$ correspond to the $k$-th mesh size from the test suite.", "solution": "The objective is to analyze the sequence of finite element functions $u_h(x) = \\alpha_h \\varphi_h(x)$ for a decreasing sequence of mesh sizes $h$. The scaling factor $\\alpha_h$ is determined by the normalization condition that the $H^1$ seminorm is unity, i.e., $\\lVert u_h' \\rVert_{L^2(0,1)} = 1$. We must then compute the $L^2(0,1)$ and $L^\\infty(0,1)$ norms for each function $u_h$ in the sequence.\n\nThe analysis proceeds in three steps: first, the determination of the scaling factor $\\alpha_h$; second, the calculation of the $L^2$ norm; and third, the calculation of the $L^\\infty$ norm.\n\nStep 1: Determination of the scaling factor $\\alpha_h$.\n\nThe scaling factor $\\alpha_h$ is found by enforcing the condition $\\lVert u_h' \\rVert_{L^2(0,1)}^2 = 1$.\nThe function $u_h(x)$ is defined as $u_h(x) = \\alpha_h \\varphi_h(x)$. Its derivative is $u_h'(x) = \\alpha_h \\varphi_h'(x)$.\nThe basis function $\\varphi_h(x)$ is given by\n$$\n\\varphi_h(x) =\n\\begin{cases}\n1 + \\frac{x - 1/2}{h}, & x \\in [1/2 - h, 1/2] \\\\\n1 - \\frac{x - 1/2}{h}, & x \\in [1/2, 1/2 + h] \\\\\n0, & \\text{otherwise}\n\\end{cases}\n$$\nThe (weak) derivative $\\varphi_h'(x)$ is a piecewise constant function:\n$$\n\\varphi_h'(x) =\n\\begin{cases}\n\\frac{1}{h}, & x \\in (1/2 - h, 1/2) \\\\\n-\\frac{1}{h}, & x \\in (1/2, 1/2 + h) \\\\\n0, & \\text{otherwise}\n\\end{cases}\n$$\nThe squared $H^1$ seminorm of $u_h$ is:\n$$\n\\lVert u_h' \\rVert_{L^2(0,1)}^2 = \\int_0^1 (u_h'(x))^2 \\, dx = \\alpha_h^2 \\int_0^1 (\\varphi_h'(x))^2 \\, dx\n$$\nWe compute the integral term:\n$$\n\\int_0^1 (\\varphi_h'(x))^2 \\, dx = \\int_{1/2 - h}^{1/2} \\left(\\frac{1}{h}\\right)^2 \\, dx + \\int_{1/2}^{1/2+h} \\left(-\\frac{1}{h}\\right)^2 \\, dx\n$$\n$$\n= \\frac{1}{h^2} \\int_{1/2 - h}^{1/2} 1 \\, dx + \\frac{1}{h^2} \\int_{1/2}^{1/2+h} 1 \\, dx = \\frac{1}{h^2} \\cdot h + \\frac{1}{h^2} \\cdot h = \\frac{1}{h} + \\frac{1}{h} = \\frac{2}{h}\n$$\nThe normalization condition $\\lVert u_h' \\rVert_{L^2(0,1)}^2 = 1$ thus becomes:\n$$\n\\alpha_h^2 \\left(\\frac{2}{h}\\right) = 1\n$$\nSince $\\alpha_h > 0$ is specified, we solve for $\\alpha_h$:\n$$\n\\alpha_h = \\sqrt{\\frac{h}{2}}\n$$\n\nStep 2: Calculation of the $L^2$ norm $\\lVert u_h \\rVert_{L^2(0,1)}$.\n\nThe squared $L^2$ norm of $u_h$ is given by:\n$$\n\\lVert u_h \\rVert_{L^2(0,1)}^2 = \\int_0^1 (u_h(x))^2 \\, dx = \\alpha_h^2 \\int_0^1 (\\varphi_h(x))^2 \\, dx\n$$\nThe integral is non-zero only on the support of $\\varphi_h$, which is $[1/2 - h, 1/2 + h]$. Due to the symmetry of $\\varphi_h(x)$ around $x=1/2$, we can write:\n$$\n\\int_0^1 (\\varphi_h(x))^2 \\, dx = 2 \\int_{1/2}^{1/2+h} \\left(1 - \\frac{x - 1/2}{h}\\right)^2 \\, dx\n$$\nWe perform a substitution $y = x - 1/2$, so $dy = dx$. The limits of integration change from $x \\in [1/2, 1/2+h]$ to $y \\in [0, h]$.\n$$\n= 2 \\int_0^h \\left(1 - \\frac{y}{h}\\right)^2 \\, dy = 2 \\int_0^h \\left(1 - \\frac{2y}{h} + \\frac{y^2}{h^2}\\right) \\, dy\n$$\n$$\n= 2 \\left[ y - \\frac{y^2}{h} + \\frac{y^3}{3h^2} \\right]_0^h = 2 \\left( h - \\frac{h^2}{h} + \\frac{h^3}{3h^2} \\right) = 2 \\left( h - h + \\frac{h}{3} \\right) = \\frac{2h}{3}\n$$\nNow, we substitute the expressions for $\\alpha_h^2$ and the integral back into the equation for the norm:\n$$\n\\lVert u_h \\rVert_{L^2(0,1)}^2 = \\alpha_h^2 \\cdot \\frac{2h}{3} = \\left(\\frac{h}{2}\\right) \\cdot \\frac{2h}{3} = \\frac{h^2}{3}\n$$\nTaking the square root gives the $L^2$ norm:\n$$\n\\lVert u_h \\rVert_{L^2(0,1)} = \\sqrt{\\frac{h^2}{3}} = \\frac{h}{\\sqrt{3}}\n$$\n\nStep 3: Calculation of the $L^\\infty$ norm $\\lVert u_h \\rVert_{L^\\infty(0,1)}$.\n\nThe $L^\\infty$ norm is the essential supremum of the function's absolute value. For a continuous function like $u_h(x)$, this is simply its maximum absolute value.\n$$\n\\lVert u_h \\rVert_{L^\\infty(0,1)} = \\sup_{x \\in (0,1)} |u_h(x)|\n$$\nSince $\\alpha_h > 0$ and $\\varphi_h(x) \\ge 0$, this simplifies to:\n$$\n\\lVert u_h \\rVert_{L^\\infty(0,1)} = \\alpha_h \\cdot \\sup_{x \\in (0,1)} \\varphi_h(x)\n$$\nThe hat function $\\varphi_h(x)$ attains its maximum value of $1$ at its center, $x=1/2$.\n$$\n\\sup_{x \\in (0,1)} \\varphi_h(x) = \\varphi_h(1/2) = 1\n$$\nTherefore, the $L^\\infty$ norm is equal to the scaling factor $\\alpha_h$:\n$$\n\\lVert u_h \\rVert_{L^\\infty(0,1)} = \\alpha_h \\cdot 1 = \\alpha_h = \\sqrt{\\frac{h}{2}}\n$$\nAs $h \\to 0$, we have $\\lVert u_h' \\rVert_{L^2(0,1)} = 1$ by construction, while $\\lVert u_h \\rVert_{L^2(0,1)} \\sim O(h) \\to 0$ and $\\lVert u_h \\rVert_{L^\\infty(0,1)} \\sim O(\\sqrt{h}) \\to 0$. This demonstrates that the $H^1$ seminorm is not equivalent to the $L^2$ or $L^\\infty$ norms on the infinite-dimensional space which is the completion of the union of these finite element spaces.\n\nThe final formulas to be implemented are:\n- $L^2$ norm: $\\lVert u_h \\rVert_{L^2(0,1)} = \\frac{h}{\\sqrt{3}}$\n- $L^\\infty$ norm: $\\lVert u_h \\rVert_{L^\\infty(0,1)} = \\sqrt{\\frac{h}{2}}$", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the L2 and L-infinity norms for a sequence of FEM functions.\n\n    For each mesh size h in the test suite, this function calculates the norms\n    of a function u_h(x) = alpha_h * phi_h(x), where phi_h is a nodal hat\n    basis function and alpha_h is a scaling factor chosen such that the H1\n    seminorm of u_h is 1.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Mesh sizes h.\n    test_cases = [1/2, 1/4, 1/8, 1/16, 1/32, 1/64]\n\n    results = []\n    for h in test_cases:\n        # The analytical formulas for the norms are derived as:\n        # L2 norm: ||u_h||_L2 = h / sqrt(3)\n        # L-infinity norm: ||u_h||_L_inf = sqrt(h / 2)\n\n        # Calculate the L2 norm for the current h.\n        l2_norm = h / np.sqrt(3)\n\n        # Calculate the L-infinity norm for the current h.\n        linf_norm = np.sqrt(h / 2)\n\n        # Append the rounded results to the list.\n        results.append(round(l2_norm, 6))\n        results.append(round(linf_norm, 6))\n\n    # Format the final output string as required.\n    # The map converts each float to its string representation.\n    # The ':.6f' format specifier ensures exactly six digits after the decimal point.\n    formatted_results = [f\"{num:.6f}\" for num in results]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2389350"}]}