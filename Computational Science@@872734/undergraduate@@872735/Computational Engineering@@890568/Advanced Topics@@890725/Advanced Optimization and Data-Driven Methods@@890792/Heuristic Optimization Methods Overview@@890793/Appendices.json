{"hands_on_practices": [{"introduction": "This practice immerses you in a classic problem from statistical physics: finding the ground state of an Ising model. By implementing an exact search for small systems, you will directly experience the exponential growth in complexity that necessitates heuristic methods for larger, more realistic instances. This exercise provides a concrete foundation for understanding why we turn to algorithms like simulated annealing or genetic algorithms when exact solutions become computationally intractable [@problem_id:2399217].", "problem": "You are given a two-dimensional Ising model on an $N\\times M$ rectangular grid with open boundary conditions. Each lattice site $(i,j)$ carries a binary spin $s_{i,j}\\in\\{-1,+1\\}$. The energy of a spin configuration $s=\\{s_{i,j}\\}$ is\n$$\nE(s)\\;=\\;-\\sum_{i=0}^{N-1}\\sum_{j=0}^{M-2} J^{(x)}_{i,j}\\,s_{i,j}\\,s_{i,j+1}\\;-\\;\\sum_{i=0}^{N-2}\\sum_{j=0}^{M-1} J^{(y)}_{i,j}\\,s_{i,j}\\,s_{i+1,j},\n$$\nwhere $J^{(x)}_{i,j}$ are horizontal couplings for bonds between $(i,j)$ and $(i,j+1)$, and $J^{(y)}_{i,j}$ are vertical couplings for bonds between $(i,j)$ and $(i+1,j)$. The arrays $J^{(x)}$ and $J^{(y)}$ are specified as $N\\times (M-1)$ and $(N-1)\\times M$ real-valued matrices, respectively.\n\nTask. For each of the test cases below, determine the minimal energy\n$$\nE_{\\min}\\;=\\;\\min_{s\\in\\{-1,+1\\}^{N\\times M}} E(s).\n$$\n\nConventions and requirements:\n- Spins must be treated as $s_{i,j}\\in\\{-1,+1\\}$.\n- Only nearest-neighbor interactions explicitly listed by $J^{(x)}$ and $J^{(y)}$ contribute to $E(s)$ (no periodic wrapping).\n- All answers must be reported as integers if exact minima are integers (they are in the provided test suite).\n\nTest suite. Use the following $4$ test cases. Each case is given by $(N,M,J^{(x)},J^{(y)})$:\n1. Case A (boundary condition sanity check):\n   - $N=1$, $M=1$,\n   - $J^{(x)}=[\\,]$ (shape $1\\times 0$), $J^{(y)}=[\\,]$ (shape $0\\times 1$).\n2. Case B (small mixed couplings on a $2\\times 2$ grid):\n   - $N=2$, $M=2$,\n   - $J^{(x)}=\\big[\\,[\\,\\;1\\,\\;\\big],\\;\\big[\\,-1\\,\\;\\big]\\big]$ (shape $2\\times 1$),\n   - $J^{(y)}=\\big[\\,[\\,1,\\,1\\,]\\big]$ (shape $1\\times 2$).\n3. Case C (rectangular $2\\times 3$ grid with antiferromagnetic horizontals and ferromagnetic verticals):\n   - $N=2$, $M=3$,\n   - $J^{(x)}=\\big[\\,[\\,-1,\\,-1\\,],\\;[\\,-1,\\,-1\\,]\\,\\big]$ (shape $2\\times 2$),\n   - $J^{(y)}=\\big[\\,[\\,1,\\,1,\\,1\\,]\\,\\big]$ (shape $1\\times 3$).\n4. Case D (ferromagnetic $3\\times 3$ grid):\n   - $N=3$, $M=3$,\n   - $J^{(x)}=\\big[\\,[\\,1,\\,1\\,],\\;[\\,1,\\,1\\,],\\;[\\,1,\\,1\\,]\\,\\big]$ (shape $3\\times 2$),\n   - $J^{(y)}=\\big[\\,[\\,1,\\,1,\\,1\\,],\\;[\\,1,\\,1,\\,1\\,]\\,\\big]$ (shape $2\\times 3$).\n\nFinal output format. Your program should produce a single line of output containing the minimal energies for Cases A, B, C, and D in this order, aggregated into a comma-separated list enclosed in square brackets, with no spaces. For example, the required format is\n\"[eA,eB,eC,eD]\".", "solution": "The problem statement is subjected to validation before a solution is attempted.\n\n**Step 1: Extract Givens**\n- **Model**: A two-dimensional Ising model on an $N \\times M$ rectangular lattice with open boundary conditions.\n- **Spins**: Binary variables $s_{i,j} \\in \\{-1, +1\\}$ located at each lattice site $(i,j)$.\n- **Energy Function**: The total energy of a configuration $s=\\{s_{i,j}\\}$ is defined as:\n$$\nE(s) = -\\sum_{i=0}^{N-1}\\sum_{j=0}^{M-2} J^{(x)}_{i,j}\\,s_{i,j}\\,s_{i,j+1} - \\sum_{i=0}^{N-2}\\sum_{j=0}^{M-1} J^{(y)}_{i,j}\\,s_{i,j}\\,s_{i+1,j}\n$$\n- **Coupling Matrices**: $J^{(x)}$ is an $N \\times (M-1)$ matrix of horizontal couplings, and $J^{(y)}$ is an $(N-1) \\times M$ matrix of vertical couplings.\n- **Task**: Determine the minimum energy $E_{\\min} = \\min_{s} E(s)$ for four specific test cases.\n- **Test Cases**:\n    1. Case A: $N=1, M=1$, $J^{(x)}$ is $1 \\times 0$ (empty), $J^{(y)}$ is $0 \\times 1$ (empty).\n    2. Case B: $N=2, M=2$, $J^{(x)}=\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$, $J^{(y)}=\\begin{pmatrix} 1 & 1 \\end{pmatrix}$.\n    3. Case C: $N=2, M=3$, $J^{(x)}=\\begin{pmatrix} -1 & -1 \\\\ -1 & -1 \\end{pmatrix}$, $J^{(y)}=\\begin{pmatrix} 1 & 1 & 1 \\end{pmatrix}$.\n    4. Case D: $N=3, M=3$, $J^{(x)}=\\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\\\ 1 & 1 \\end{pmatrix}$, $J^{(y)}=\\begin{pmatrix} 1 & 1 & 1 \\\\ 1 & 1 & 1 \\end{pmatrix}$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Grounding**: The problem is formulated based on the Ising model, a fundamental and rigorously studied model in statistical physics. It is scientifically sound.\n- **Well-Posedness**: The task is to find the minimum of a well-defined function, $E(s)$, over a finite, discrete domain of spin configurations. A minimum value is guaranteed to exist and is unique for a given set of parameters.\n- **Objectivity**: The problem is described using precise mathematical language, containing no subjective or ambiguous statements.\n- **Completeness and Consistency**: All required parameters ($N$, $M$, $J^{(x)}$, $J^{(y)}$) for each test case are provided, and their dimensions are consistent with the energy formula.\n\nThe problem is a standard ground state energy calculation, a classic problem in computational physics. It is related to heuristic optimization as finding the ground state for large, frustrated Ising systems is an NP-hard optimization problem for which heuristics are often employed.\n\n**Step 3: Verdict and Action**\nThe problem is valid. We proceed to the solution.\n\nThe task is to find the minimum energy, $E_{\\min}$, of a specified Ising model. The state space of the system consists of all possible spin configurations $s = \\{s_{i,j}\\}$. For a grid of size $N \\times M$, there are $N \\times M$ individual spins, each of which can be in one of two states, $\\{-1, +1\\}$. The total number of unique configurations is therefore $2^{N \\times M}$.\n\nFinding the ground state of a general Ising model is a computationally hard problem. However, for the small grid sizes given in the test suite, an exhaustive search (brute-force) of the entire configuration space is computationally tractable. This method guarantees finding the absolute minimum energy. The number of states for each case is:\n- Case A: $N=1, M=1 \\implies 2^{1 \\times 1} = 2$ states.\n- Case B: $N=2, M=2 \\implies 2^{2 \\times 2} = 16$ states.\n- Case C: $N=2, M=3 \\implies 2^{2 \\times 3} = 64$ states.\n- Case D: $N=3, M=3 \\implies 2^{3 \\times 3} = 512$ states.\n\nThe algorithm for finding $E_{\\min}$ is as follows:\n1. For each test case defined by $(N, M, J^{(x)}, J^{(y)})$, initialize a variable `min_energy` to a very large value (infinity).\n2. Generate every possible spin configuration for the $N \\times M$ grid. Each configuration is an $N \\times M$ matrix of spin values.\n3. For each configuration, calculate its total energy $E(s)$ using the provided formula: sum the interaction energies $-J \\cdot s \\cdot s'$ for all adjacent spin pairs.\n4. Compare the calculated energy $E(s)$ with `min_energy`. If $E(s)$ is smaller, update `min_energy` to this new value.\n5. After iterating through all $2^{N \\times M}$ configurations, the final value of `min_energy` is the ground state energy $E_{\\min}$.\n\nTo illustrate, consider Case D, the ferromagnetic $3 \\times 3$ grid where all $J$ values are $1$. The energy is $E(s) = -\\sum_{\\langle k,l \\rangle} s_k s_l$, summing over all nearest-neighbor pairs. To minimize $E(s)$, each product $s_k s_l$ must be maximized, i.e., $s_k s_l = +1$. This condition is satisfied if all neighboring spins are aligned. A global configuration where all spins are identical (all $+1$ or all $-1$) satisfies this for every bond simultaneously. The grid has $3 \\times 2 = 6$ horizontal bonds and $3 \\times 2 = 6$ vertical bonds, totaling $12$ bonds. The minimum energy is thus $E_{\\min} = -12 \\times (1) = -12$.\n\nFor cases involving frustration (competing interactions), such as Case B, or mixed interactions like Case C, a simple inspection is not sufficient. An exhaustive search is required to find the true energy minimum. The provided final answer implements this exhaustive search for all cases.", "answer": "```python\nimport numpy as np\nimport itertools\n\ndef find_min_energy(N, M, Jx, Jy):\n    \"\"\"\n    Finds the minimum energy of an Ising model on an N x M grid\n    by performing an exhaustive search over all spin configurations.\n    \"\"\"\n    num_spins = N * M\n    if num_spins == 0:\n        # For N or M being 0, or Case A with N=1, M=1 and empty J matrices,\n        # the energy sums are empty, so the energy is 0.\n        # The loop logic below correctly handles N=1, M=1, but this check\n        # handles more general zero-size cases.\n        return 0\n\n    min_energy = float('inf')\n    spin_values = [-1, 1]\n\n    # Iterate through all 2^(N*M) possible spin configurations.\n    # itertools.product generates the Cartesian product of input iterables.\n    # repeat=num_spins is equivalent to product(spin_values, ..., spin_values).\n    for flat_s in itertools.product(spin_values, repeat=num_spins):\n        s_grid = np.array(flat_s).reshape((N, M))\n        \n        current_energy = 0.0\n\n        # Calculate energy from horizontal interactions\n        if M > 1:\n            for i in range(N):\n                for j in range(M - 1):\n                    current_energy -= Jx[i, j] * s_grid[i, j] * s_grid[i, j + 1]\n\n        # Calculate energy from vertical interactions\n        if N > 1:\n            for i in range(N - 1):\n                for j in range(M):\n                    current_energy -= Jy[i, j] * s_grid[i, j] * s_grid[i + 1, j]\n        \n        if current_energy  min_energy:\n            min_energy = current_energy\n            \n    # The problem statement guarantees integer results\n    return int(min_energy)\n\ndef solve():\n    \"\"\"\n    Defines the test cases and computes the minimum energy for each,\n    then prints the results in the specified format.\n    \"\"\"\n    test_cases = [\n        {\n            \"name\": \"Case A\",\n            \"N\": 1, \"M\": 1,\n            \"Jx\": np.empty((1, 0)),\n            \"Jy\": np.empty((0, 1))\n        },\n        {\n            \"name\": \"Case B\",\n            \"N\": 2, \"M\": 2,\n            \"Jx\": np.array([[1], [-1]]),\n            \"Jy\": np.array([[1, 1]])\n        },\n        {\n            \"name\": \"Case C\",\n            \"N\": 2, \"M\": 3,\n            \"Jx\": np.array([[-1, -1], [-1, -1]]),\n            \"Jy\": np.array([[1, 1, 1]])\n        },\n        {\n            \"name\": \"Case D\",\n            \"N\": 3, \"M\": 3,\n            \"Jx\": np.array([[1, 1], [1, 1], [1, 1]]),\n            \"Jy\": np.array([[1, 1, 1], [1, 1, 1]])\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        min_e = find_min_energy(case[\"N\"], case[\"M\"], case[\"Jx\"], case[\"Jy\"])\n        results.append(min_e)\n\n    # Print the final result in the exact format \"[eA,eB,eC,eD]\"\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2399217"}, {"introduction": "Genetic Algorithms often produce infeasible \"offspring\" when applied to constrained optimization problems. This exercise explores the design of a crucial component—a repair operator—for the classic Set Covering Problem, challenging you to balance the dual goals of restoring feasibility and guiding the search toward low-cost solutions. Mastering the design of effective, cost-aware repair mechanisms is a key skill for successfully applying GAs to a wide range of practical engineering problems [@problem_id:2399283].", "problem": "In the classical set covering problem, one is given a finite ground set $U = \\{1,2,\\dots,m\\}$ and a family of subsets $\\{S_j \\subseteq U\\}_{j=1}^n$ with positive costs $\\{c_j\\}_{j=1}^n$. A binary decision vector $x \\in \\{0,1\\}^n$ represents a selection of subsets, where $x_j = 1$ means $S_j$ is selected. Feasibility requires that every element $i \\in U$ is covered by at least one selected subset, that is, for all $i \\in U$, $\\sum_{j : i \\in S_j} x_j \\ge 1$. The objective is to minimize $\\sum_{j=1}^n c_j x_j$ subject to feasibility. Assume that $\\bigcup_{j=1}^n S_j = U$ so that at least one feasible solution exists.\n\nA Genetic Algorithm (GA) maintains a population of chromosomes $x \\in \\{0,1\\}^n$. Crossover and mutation can produce infeasible offspring that violate one or more covering constraints. A repair operator is applied to map an infeasible chromosome to a feasible one before evaluation.\n\nConsider the specific instance with $U = \\{1,2,3,4,5\\}$, subsets and costs\n- $S_1 = \\{1,2\\}$ with $c_1 = 3$,\n- $S_2 = \\{2,3,4\\}$ with $c_2 = 4$,\n- $S_3 = \\{3,5\\}$ with $c_3 = 2$,\n- $S_4 = \\{1,4,5\\}$ with $c_4 = 5$,\n- $S_5 = \\{2,5\\}$ with $c_5 = 2$,\nand an infeasible offspring chromosome $x = (x_1,x_2,x_3,x_4,x_5) = (1,0,0,0,1)$, which selects $S_1$ and $S_5$ only. The currently covered set is $S_1 \\cup S_5 = \\{1,2,5\\}$; elements $\\{3,4\\}$ are uncovered.\n\nYou are asked to select a repair operator that is soundly justified from first principles (definitions of feasibility and coverage) and balances cost awareness with guaranteed progress to feasibility.\n\nWhich one of the following repair mechanisms both\n(i) guarantees that at each addition step the number of uncovered elements strictly decreases (hence it terminates in at most $|U|$ additions) and never accepts a removal that breaks feasibility, and\n(ii) when applied to the given instance and chromosome $x$, results in the minimum total cost among the listed mechanisms?\n\nA. Greedy uncovered-coverage-per-cost with redundancy elimination:\n- While there exists an uncovered element, choose an index $j \\in \\{1,\\dots,5\\}$ with $x_j = 0$ that minimizes the ratio $c_j / |\\;S_j \\cap \\text{Uncovered}\\;|$, where $\\text{Uncovered} \\subseteq U$ is the current set of uncovered elements (ties broken by larger $|\\;S_j \\cap \\text{Uncovered}\\;|$). Set $x_j \\leftarrow 1$ and update $\\text{Uncovered}$.\n- After feasibility is achieved, attempt to remove redundancy: consider selected sets in nonincreasing order of $c_j / |S_j|$ and set $x_j \\leftarrow 0$ if and only if feasibility is preserved; stop when no removal is possible.\n\nB. Cheapest-first addition without regard to marginal uncovered coverage:\n- While infeasible, repeatedly select the cheapest not-yet-selected set (smallest $c_j$ among $j$ with $x_j = 0$), set $x_j \\leftarrow 1$, and stop once feasibility is achieved; do not remove any set.\n\nC. Overlap-maximizing addition:\n- While infeasible, repeatedly select the not-yet-selected set $S_j$ that maximizes $|\\;S_j \\cap (\\bigcup_{k : x_k = 1} S_k)\\;|$ (ties broken by smallest $c_j$), set $x_j \\leftarrow 1$, and stop once feasibility is achieved; do not remove any set.\n\nD. Penalty-only handling:\n- Do not change $x$; instead, assign a large additive penalty $\\rho \\cdot |\\text{Uncovered}|$ to the objective for fitness evaluation, with $\\rho \\gg \\max_j c_j$.\n\nChoose the single best option that satisfies both (i) and (ii) based on the fundamental definitions above, not on any unstated heuristics or external formulas. Provide only one answer choice.", "solution": "The problem statement has been validated and found to be self-contained, scientifically grounded, and well-posed. We may proceed with the solution.\n\nThe problem asks us to evaluate four repair mechanisms for a Genetic Algorithm solving a specific instance of the Set Covering Problem. The objective is to identify the mechanism that satisfies two conditions:\n(i) it guarantees convergence to a feasible solution by strictly reducing the number of uncovered elements at each addition step and preserving feasibility during any removal step.\n(ii) it produces a repaired solution with the minimum total cost among the four listed mechanisms for the given instance.\n\nThe specific instance is defined by:\n- Ground set $U = \\{1, 2, 3, 4, 5\\}$.\n- Subsets and costs:\n  - $S_1 = \\{1, 2\\}$ with $c_1 = 3$.\n  - $S_2 = \\{2, 3, 4\\}$ with $c_2 = 4$.\n  - $S_3 = \\{3, 5\\}$ with $c_3 = 2$.\n  - $S_4 = \\{1, 4, 5\\}$ with $c_4 = 5$.\n  - $S_5 = \\{2, 5\\}$ with $c_5 = 2$.\n- The initial infeasible chromosome is $x = (1, 0, 0, 0, 1)$. This corresponds to selecting sets $S_1$ and $S_5$.\n\nThe initial state is as follows:\n- Selected sets: $\\{S_1, S_5\\}$.\n- Initial cost: $c_1 + c_5 = 3 + 2 = 5$.\n- Covered set: $C_0 = S_1 \\cup S_5 = \\{1, 2\\} \\cup \\{2, 5\\} = \\{1, 2, 5\\}$.\n- Uncovered set: $\\text{Uncovered}_0 = U \\setminus C_0 = \\{3, 4\\}$.\n\nWe will now analyze each option.\n\n**A. Greedy uncovered-coverage-per-cost with redundancy elimination**\n\nFirst, we evaluate condition (i). The additive phase selects a set $S_j$ that minimizes the ratio $c_j / |S_j \\cap \\text{Uncovered}|$. Since all costs $c_j$ are positive, for the ratio to be finite and minimal, the denominator $|S_j \\cap \\text{Uncovered}|$ must be a positive integer. This means any added set $S_j$ must cover at least one currently uncovered element, thereby strictly decreasing the number of uncovered elements. The removal phase is explicitly constrained to preserve feasibility. Thus, condition (i) is satisfied.\n\nNext, we apply this mechanism to the instance.\n1.  **Additive phase:** The initial set of uncovered elements is $\\text{Uncovered}_0 = \\{3, 4\\}$. We must select a set $S_j$ from $\\{S_2, S_3, S_4\\}$ (those with $x_j=0$) to add. We compute the selection metric $c_j / |S_j \\cap \\text{Uncovered}_0|$ for each:\n    - $S_2$: $c_2 = 4$, $S_2 \\cap \\{3, 4\\} = \\{3, 4\\}$, so $|S_2 \\cap \\{3, 4\\}| = 2$. Ratio is $4/2 = 2$.\n    - $S_3$: $c_3 = 2$, $S_3 \\cap \\{3, 4\\} = \\{3\\}$, so $|S_3 \\cap \\{3, 4\\}| = 1$. Ratio is $2/1 = 2$.\n    - $S_4$: $c_4 = 5$, $S_4 \\cap \\{3, 4\\} = \\{4\\}$, so $|S_4 \\cap \\{3, 4\\}| = 1$. Ratio is $5/1 = 5$.\n    There is a tie between $S_2$ and $S_3$ with the minimum ratio of $2$. The tie-breaking rule is to choose the one with the larger value of $|S_j \\cap \\text{Uncovered}|$. For $S_2$, this value is $2$, and for $S_3$, it is $1$. We therefore select $S_2$.\n    We set $x_2 \\leftarrow 1$. The new set of selected subsets is $\\{S_1, S_2, S_5\\}$. The covered set is $S_1 \\cup S_2 \\cup S_5 = \\{1, 2\\} \\cup \\{2, 3, 4\\} \\cup \\{2, 5\\} = \\{1, 2, 3, 4, 5\\} = U$. The solution is now feasible.\n\n2.  **Redundancy elimination phase:** The current solution is $x=(1, 1, 0, 0, 1)$, selecting $\\{S_1, S_2, S_5\\}$. The total cost is $c_1 + c_2 + c_5 = 3 + 4 + 2 = 9$. We consider removing sets in nonincreasing order of $c_j/|S_j|$:\n    - $S_1$: $c_1/|S_1| = 3/2 = 1.5$.\n    - $S_2$: $c_2/|S_2| = 4/3 \\approx 1.33$.\n    - $S_5$: $c_5/|S_5| = 2/2 = 1$.\n    The order of consideration is $S_1, S_2, S_5$.\n    - Try removing $S_1$: The remaining sets $\\{S_2, S_5\\}$ cover $S_2 \\cup S_5 = \\{2, 3, 4, 5\\}$. Element $1$ becomes uncovered. Removal is rejected.\n    - Try removing $S_2$: The remaining sets $\\{S_1, S_5\\}$ cover $S_1 \\cup S_5 = \\{1, 2, 5\\}$. Elements $3$ and $4$ become uncovered. Removal is rejected.\n    - Try removing $S_5$: The remaining sets $\\{S_1, S_2\\}$ cover $S_1 \\cup S_2 = \\{1, 2, 3, 4\\}$. Element $5$ becomes uncovered. Removal is rejected.\n    No sets can be removed. The final repaired solution is $x = (1, 1, 0, 0, 1)$ with a total cost of $9$.\n\nVerdict for A: This mechanism satisfies condition (i). For the given instance, it produces a feasible solution with a total cost of $9$.\n\n**B. Cheapest-first addition without regard to marginal uncovered coverage**\n\nFirst, we evaluate condition (i). This mechanism adds the cheapest available set, irrespective of whether it covers any currently uncovered elements. It is possible for the cheapest set $S_j$ to be a subset of the already covered set ($S_j \\subseteq \\bigcup_{k:x_k=1} S_k$). In such a case, adding $S_j$ would not decrease the number of uncovered elements. Therefore, this mechanism does not guarantee a strict decrease at each step. Condition (i) is violated.\n\nNext, we apply this mechanism to the instance.\n1.  **Additive phase:** The uncovered set is $\\{3, 4\\}$. Available sets for addition are $\\{S_2, S_3, S_4\\}$ with costs $c_2 = 4, c_3 = 2, c_4 = 5$. The cheapest is $S_3$. We add $S_3$.\n    The selected sets are now $\\{S_1, S_3, S_5\\}$. The covered set is $S_1 \\cup S_3 \\cup S_5 = \\{1, 2, 3, 5\\}$. The uncovered set is $\\{4\\}$. The solution is still infeasible.\n2.  We repeat the process. Available sets for addition are $\\{S_2, S_4\\}$ with costs $c_2 = 4, c_4 = 5$. The cheapest is $S_2$. We add $S_2$.\n    The selected sets are now $\\{S_1, S_2, S_3, S_5\\}$. The covered set is $S_1 \\cup S_2 \\cup S_3 \\cup S_5 = \\{1, 2, 3, 4, 5\\} = U$. The solution is feasible.\nThe mechanism stops. There is no removal phase. The final solution is $x = (1, 1, 1, 0, 1)$.\nThe total cost is $c_1 + c_2 + c_3 + c_5 = 3 + 4 + 2 + 2 = 11$.\n\nVerdict for B: This mechanism violates condition (i). For the given instance, it produces a feasible solution with a total cost of $11$.\n\n**C. Overlap-maximizing addition**\n\nFirst, we evaluate condition (i). This mechanism adds the set that maximizes overlap with the *currently covered* elements. The objective is to reinforce existing coverage, not necessarily to cover new elements. It is possible that the set with maximum overlap, $S_j$, covers no new elements ($S_j \\cap \\text{Uncovered} = \\emptyset$). In this case, the number of uncovered elements does not decrease. Therefore, this mechanism does not guarantee a strict decrease at each step. Condition (i) is violated.\n\nNext, we apply this mechanism to the instance.\n1.  **Additive phase:** The initial covered set is $K_0 = \\{1, 2, 5\\}$, and the uncovered set is $\\{3, 4\\}$. We choose from $\\{S_2, S_3, S_4\\}$ based on maximizing $|S_j \\cap K_0|$.\n    - $S_2$: $|S_2 \\cap K_0| = |\\{2, 3, 4\\} \\cap \\{1, 2, 5\\}| = |\\{2\\}| = 1$.\n    - $S_3$: $|S_3 \\cap K_0| = |\\{3, 5\\} \\cap \\{1, 2, 5\\}| = |\\{5\\}| = 1$.\n    - $S_4$: $|S_4 \\cap K_0| = |\\{1, 4, 5\\} \\cap \\{1, 2, 5\\}| = |\\{1, 5\\}| = 2$.\n    $S_4$ has the maximum overlap. We add $S_4$. The selected sets are now $\\{S_1, S_4, S_5\\}$, and the covered set is $K_1 = S_1 \\cup S_4 \\cup S_5 = \\{1, 2, 4, 5\\}$. The uncovered set is $\\{3\\}$. The solution is still infeasible.\n2.  We repeat the process. The current covered set is $K_1 = \\{1, 2, 4, 5\\}$. Available sets for addition are $\\{S_2, S_3\\}$.\n    - $S_2$: $|S_2 \\cap K_1| = |\\{2, 3, 4\\} \\cap \\{1, 2, 4, 5\\}| = |\\{2, 4\\}| = 2$.\n    - $S_3$: $|S_3 \\cap K_1| = |\\{3, 5\\} \\cap \\{1, 2, 4, 5\\}| = |\\{5\\}| = 1$.\n    $S_2$ has the maximum overlap. We add $S_2$. The selected sets are now $\\{S_1, S_2, S_4, S_5\\}$, and the covered set is $S_1 \\cup S_2 \\cup S_4 \\cup S_5 = \\{1, 2, 3, 4, 5\\} = U$. The solution is feasible.\nThere is no removal phase. The final solution is $x = (1, 1, 0, 1, 1)$.\nThe total cost is $c_1 + c_2 + c_4 + c_5 = 3 + 4 + 5 + 2 = 14$.\n\nVerdict for C: This mechanism violates condition (i). For the given instance, it produces a feasible solution with a total cost of $14$.\n\n**D. Penalty-only handling**\n\nThis mechanism does not alter the chromosome. It simply adds a penalty to the objective function for evaluation. The problem defines a repair operator as a procedure that \"map[s] an infeasible chromosome to a feasible one\". This mechanism fails to do so. It does not contain any \"addition step\" and thus cannot satisfy the guarantee in condition (i). It makes no progress towards feasibility. As such, it is not a repair operator in the sense required by the problem and fails condition (i).\n\nFurthermore, since it does not produce a feasible solution, its cost under the set covering objective $\\sum c_j x_j$ is not comparable to the others which do produce feasible solutions. One might consider its effective cost to be infinite. In any case, it cannot result in the minimum total cost. The penalized cost would be $5 + 2\\rho$, which is very large by definition. So it also fails condition (ii).\n\nVerdict for D: This mechanism violates condition (i) as it does not map an infeasible solution to a feasible one. It also fails condition (ii).\n\n**Conclusion**\n\nWe summarize the findings:\n- **Mechanism A:** Satisfies condition (i). Produces a feasible solution with cost $9$.\n- **Mechanism B:** Violates condition (i). Produces a feasible solution with cost $11$.\n- **Mechanism C:** Violates condition (i). Produces a feasible solution with cost $14$.\n- **Mechanism D:** Violates condition (i) and does not produce a feasible solution.\n\nOnly mechanism A satisfies condition (i).\nFor condition (ii), we compare the costs of the solutions produced by the mechanisms: Cost(A) = $9$, Cost(B) = $11$, Cost(C) = $14$. The minimum cost among these is $9$, achieved by mechanism A. Because mechanism D does not produce a valid, finite-cost feasible solution, it is not a contender for the minimum cost.\nTherefore, mechanism A is the only one that satisfies both conditions (i) and (ii).", "answer": "$$\\boxed{A}$$", "id": "2399283"}, {"introduction": "Many powerful heuristics, like Particle Swarm Optimization (PSO), are naturally formulated for continuous search spaces. This practice challenges you to adapt such an algorithm to a discrete, constrained integer programming problem, a common task in computational engineering. By analyzing different strategies, you will learn to distinguish between ad-hoc fixes and principled methods that preserve the algorithm's core logic, leading to more robust and effective optimization tools [@problem_id:2399268].", "problem": "You are asked to adapt Particle Swarm Optimization (PSO) to a discrete integer programming setting. Consider the constrained optimization problem with decision vector $x \\in \\mathbb{Z}^d$:\nminimize $f(x)$ subject to $0 \\le x_j \\le 5$ for all $j \\in \\{1,\\dots,d\\}$ and $\\sum_{j=1}^d x_j = 12$. In the continuous formulation, a standard PSO maintains, for each particle $i$, a position $x_i^t \\in \\mathbb{R}^d$ and a velocity $v_i^t \\in \\mathbb{R}^d$ updated by\n$$\nv_i^{t+1} = \\omega\\, v_i^{t} + c_1\\, r_{1}^{t} \\odot \\big(p_i^{t} - x_i^{t}\\big) + c_2\\, r_{2}^{t} \\odot \\big(g^{t} - x_i^{t}\\big), \\quad\nx_i^{t+1} = x_i^{t} + v_i^{t+1},\n$$\nwhere $\\omega, c_1, c_2 \\in \\mathbb{R}$ are parameters, $r_1^{t}, r_2^{t} \\in [0,1]^d$ are random vectors sampled componentwise, $\\odot$ denotes componentwise multiplication, $p_i^{t}$ is the personal best of particle $i$, and $g^{t}$ is the global best. You intend to adapt these updates so that, after every iteration, each particle’s position is an integer vector that satisfies the box constraints and the sum constraint. Which of the following strategies yields a well-posed adaptation that ensures every updated position lies in the feasible set $X = \\{x \\in \\mathbb{Z}^d: 0 \\le x_j \\le 5,\\ \\sum_{j=1}^d x_j = 12\\}$ and preserves the interpretation that the discrete update is the nearest feasible counterpart of the continuous attraction step? Select all that apply.\n\nA. After computing the continuous candidate $y_i^{t+1} = x_i^{t} + v_i^{t+1}$, round each component of $y_i^{t+1}$ to the nearest integer and clip it to $[0,5]$. If the sum constraint $\\sum_{j=1}^d x_{i,j}^{t+1} = 12$ is violated, repeatedly pick a random pair of distinct indices $(j,k)$ and adjust $x_{i,j}^{t+1}$ by $+1$ and $x_{i,k}^{t+1}$ by $-1$ (or vice versa, depending on the sign of the discrepancy) while respecting bounds, until the sum equals $12$.\n\nB. Replace real-valued positions by binary vectors in $\\{0,1\\}^d$ and interpret each velocity component via a sigmoid mapping as a flip probability. Update by independently flipping bits according to these probabilities. If $\\sum_{j=1}^d x_{i,j}^{t+1} \\ne 12$, repeatedly resample flips until the equality holds.\n\nC. Define an integer-valued “velocity” by rounding the right-hand side of the standard PSO velocity equation to the nearest integers. Update positions by $x_i^{t+1} = x_i^{t} + v_i^{t+1}$ componentwise and then reduce each component modulo $6$ to enforce $0 \\le x_{i,j}^{t+1} \\le 5$. Ignore the sum constraint during updates.\n\nD. Encode $x$ as a permutation of $\\{1,\\dots,d\\}$ and represent velocity as a sequence of swaps that move a permutation toward its personal and global best permutations. After applying a composed swap sequence to update a particle, decode back to an integer vector by “scaling” the permutation to meet $\\sum_{j=1}^d x_j = 12$ and clipping to $[0,5]$.\n\nE. After computing the continuous candidate $y_i^{t+1} = x_i^{t} + v_i^{t+1}$, map it to an integer feasible vector by solving\n$$\n\\min_{z \\in \\mathbb{Z}^d} \\ \\|z - y_i^{t+1}\\|_2 \\quad \\text{subject to} \\quad 0 \\le z_j \\le 5,\\ \\sum_{j=1}^d z_j = 12,\n$$\nimplemented efficiently by first rounding $y_i^{t+1}$ to nearest integers and then adjusting the rounded vector by distributing the sum-discrepancy over components with largest (in magnitude) fractional parts, while respecting bounds, so that the final $z$ has sum $12$ and remains as close as possible (in Euclidean norm) to $y_i^{t+1}$.", "solution": "The validity of the problem statement must first be established.\n\n### Step 1: Extract Givens\n- The problem is a constrained optimization problem for a function $f(x)$ where the decision vector is $x \\in \\mathbb{Z}^d$.\n- The feasible set $X$ is defined by two types of constraints:\n    1. Box constraints: $0 \\le x_j \\le 5$ for all $j \\in \\{1,\\dots,d\\}$.\n    2. A linear equality constraint (sum constraint): $\\sum_{j=1}^d x_j = 12$.\n- The task is to adapt the standard continuous Particle Swarm Optimization (PSO) algorithm for this discrete domain.\n- The standard continuous PSO update equations are given for a particle $i$ at time step $t$:\n$$\nv_i^{t+1} = \\omega\\, v_i^{t} + c_1\\, r_{1}^{t} \\odot \\big(p_i^{t} - x_i^{t}\\big) + c_2\\, r_{2}^{t} \\odot \\big(g^{t} - x_i^{t}\\big)\n$$\n$$\nx_i^{t+1} = x_i^{t} + v_i^{t+1}\n$$\n- The adaptation must satisfy two conditions:\n    1. The updated position must always belong to the feasible set $X = \\{x \\in \\mathbb{Z}^d: 0 \\le x_j \\le 5, \\sum_{j=1}^d x_j = 12\\}$.\n    2. The adaptation must preserve the interpretation that the discrete update is the \"nearest feasible counterpart\" of the continuous attraction step.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is firmly rooted in the field of computational engineering and heuristic optimization. Adapting continuous metaheuristics like PSO for discrete or constrained problems is a standard and important research topic. The problem formulation uses established mathematical concepts and a well-known algorithm. The constraints define a valid integer programming feasible region. The problem is scientifically sound.\n- **Well-Posed:** The problem provides all necessary information. It clearly defines the search space, the update rules to be adapted, and the criteria for a successful adaptation (feasibility and \"nearest counterpart\"). The term \"nearest feasible counterpart\" is given a clear context by relating it to the continuous attraction step, which points unambiguously to a distance minimization principle. The question is structured to have a unique set of correct answers among the options.\n- **Objective:** The language is precise and technical. The evaluation criteria are objective and can be assessed using mathematical and algorithmic reasoning.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. It is scientifically grounded, well-posed, and objective. I will now proceed with the solution by evaluating each option.\n\nThe core of the problem is to map a continuous candidate solution, $y_i^{t+1} = x_i^{t} + v_i^{t+1}$, to a point $z \\in X$ that is both feasible and conceptually \"close\" to $y_i^{t+1}$. The criterion of being the \"nearest feasible counterpart\" is most rigorously interpreted as finding a vector $z$ that minimizes a distance metric to $y_i^{t+1}$, subject to $z \\in X$. The standard choice for this is the Euclidean distance (or its square), leading to the projection problem: $\\min_{z \\in X} \\|z - y_i^{t+1}\\|_2^2$.\n\n### Option-by-Option Analysis\n\n**Option A:**\nThis strategy first creates an intermediate integer vector by rounding and clipping, which enforces the box constraints $0 \\le x_j \\le 5$ and the integer requirement. However, it then attempts to satisfy the sum constraint $\\sum x_j = 12$ through a series of random adjustments. Specifically, it randomly picks pairs of indices $(j,k)$ and applies updates of the form $(x_j, x_k) \\to (x_j+1, x_k-1)$ or $(x_j-1, x_k+1)$. While this repair mechanism will eventually enforce the sum constraint and maintain the box constraints, the random nature of the adjustments is its critical flaw. By choosing indices randomly, the procedure does not attempt to minimize the deviation from the original continuous candidate $y_i^{t+1}$. The final vector could be arbitrarily far from the true \"nearest\" feasible point, depending on the sequence of random choices. This violates the principle of preserving the \"nearest feasible counterpart\" interpretation. The procedure is ad-hoc, not principled.\n**Verdict: Incorrect.**\n\n**Option B:**\nThis strategy is fundamentally flawed because it redefines the problem. The original problem specifies integer variables $x_j \\in \\{0, 1, 2, 3, 4, 5\\}$. This option proposes using binary vectors, $x_j \\in \\{0, 1\\}$, which is a different search space. It replaces the integer programming problem with a binary programming problem. Furthermore, it proposes a \"rejection sampling\" approach (\"repeatedly resample flips\") to satisfy the sum constraint, which is computationally inefficient and may not be guaranteed to terminate in a reasonable time. This is not a valid adaptation of the given problem.\n**Verdict: Incorrect.**\n\n**Option C:**\nThis strategy fails on two counts. First, it explicitly states to \"ignore the sum constraint\". This means the resulting particles' positions $x_i^{t+1}$ are not guaranteed to be in the feasible set $X$, violating a primary requirement of the adaptation. Second, it uses a modulo operator, $x_{i,j}^{t+1} \\pmod 6$, to enforce the box constraints. The modulo operator creates a \"wrap-around\" effect. For instance, a value of $6$ becomes $0$, and a value of $-1$ becomes $5$. This is not equivalent to finding the nearest integer in the interval $[0,5]$. The standard operation for that would be clipping, i.e., $\\max(0, \\min(5, x_j))$. The wrap-around behavior is inconsistent with the PSO principle of attraction, where a large velocity should move a particle to the boundary of the search space, not teleport it to the opposite side.\n**Verdict: Incorrect.**\n\n**Option D:**\nThis strategy proposes encoding the integer vector $x$ as a permutation. This is a technique suitable for combinatorial optimization problems where the solution is an ordering of elements, such as the Traveling Salesperson Problem or scheduling problems. The problem at hand is an integer programming problem, not a permutation-based one. Using a permutation encoding here is unnatural and forces the problem into a framework for which it is not designed. Moreover, the description is vague, referring to a \"sequence of swaps\" for velocity and \"scaling\" the permutation back to an integer vector. This is not a direct adaptation of the given vector-based PSO update rules and entirely abandons the geometric interpretation of particles moving in a $d$-dimensional space. It fails to preserve the \"continuous attraction step\" concept.\n**Verdict: Incorrect.**\n\n**Option E:**\nThis strategy provides the most rigorous and principled approach. It formalizes the concept of the \"nearest feasible counterpart\" by defining it as the solution to a minimization problem: finding a point $z$ in the feasible set $X$ that minimizes the Euclidean distance to the continuous candidate $y_i^{t+1}$. This is precisely a projection of $y_i^{t+1}$ onto the feasible set $X$. This guarantees that the updated particle position is feasible by construction and is the closest possible feasible point to where the continuous PSO dynamics would have placed it. The option then describes a well-known and efficient greedy algorithm to solve this specific projection problem: first round to the nearest integers, then iteratively adjust the components to satisfy the sum constraint by adding or subtracting $1$ at each step from the component that incurs the minimal increase in the squared Euclidean distance. This corresponds to adjusting components based on their fractional parts or, more accurately, the difference between the rounded and original continuous values. This strategy is well-posed, guarantees feasibility, and perfectly aligns with the \"nearest counterpart\" principle.\n**Verdict: Correct.**", "answer": "$$\\boxed{E}$$", "id": "2399268"}]}