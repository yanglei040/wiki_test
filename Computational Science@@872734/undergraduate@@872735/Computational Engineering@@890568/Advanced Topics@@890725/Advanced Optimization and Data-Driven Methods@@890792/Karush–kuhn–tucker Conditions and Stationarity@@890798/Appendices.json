{"hands_on_practices": [{"introduction": "Understanding the Karush-Kuhn-Tucker (KKT) conditions begins with mastering their application in a simple context. This first exercise provides a foundational walkthrough of the KKT mechanics for a one-dimensional quadratic problem with a single inequality constraint [@problem_id:2167418]. By working through this problem, you will see precisely how the conditions of stationarity, feasibility, and complementary slackness work together to identify the optimal solution, also known as the saddle point of the Lagrangian.", "problem": "In a simplified machine learning scenario, the cost associated with a model parameter, $x$, is described by the quadratic function $f(x) = \\frac{1}{2}x^2$. To prevent underfitting and ensure the model has a minimum level of complexity, a domain expert imposes a constraint that the parameter $x$ must be at least 2. This leads to a constrained optimization problem.\n\nThe Lagrangian function for this problem is defined as $L(x, \\lambda) = f(x) + \\lambda g(x)$, where $g(x) \\le 0$ represents the inequality constraint and $\\lambda \\ge 0$ is the Lagrange multiplier.\n\nYour task is to find the saddle point $(x^*, \\lambda^*)$ of the Lagrangian. This point corresponds to the optimal parameter value $x^*$ that minimizes the cost function subject to the constraint, and the associated optimal Lagrange multiplier $\\lambda^*$.\n\nReport the saddle point as an ordered pair $(x^*, \\lambda^*)$.", "solution": "We are given the convex quadratic objective $f(x) = \\frac{1}{2}x^{2}$ and the inequality constraint $x \\geq 2$. To express this in the standard form $g(x) \\leq 0$, define $g(x) = 2 - x$, so the constraint is $g(x) \\leq 0$. The Lagrangian is\n$$\nL(x, \\lambda) = \\frac{1}{2}x^{2} + \\lambda(2 - x),\n$$\nwith dual feasibility $\\lambda \\geq 0$.\n\nFor a convex problem with an affine constraint, the Karush–Kuhn–Tucker conditions are necessary and sufficient for optimality and for characterizing a saddle point. The KKT conditions are:\n- Primal feasibility: $g(x^{*}) \\leq 0$, i.e., $x^{*} \\geq 2$.\n- Dual feasibility: $\\lambda^{*} \\geq 0$.\n- Stationarity: $\\frac{\\partial L}{\\partial x}(x^{*}, \\lambda^{*}) = 0$.\n- Complementary slackness: $\\lambda^{*} g(x^{*}) = 0$.\n\nCompute the stationarity condition:\n$$\n\\frac{\\partial L}{\\partial x} = x - \\lambda \\quad \\Rightarrow \\quad x^{*} - \\lambda^{*} = 0 \\quad \\Rightarrow \\quad x^{*} = \\lambda^{*}.\n$$\nApply complementary slackness:\n$$\n\\lambda^{*}(2 - x^{*}) = 0.\n$$\nConsider the cases:\n- If $\\lambda^{*} = 0$, then from stationarity $x^{*} = 0$, which violates primal feasibility $x^{*} \\geq 2$; reject this case.\n- Therefore $2 - x^{*} = 0 \\Rightarrow x^{*} = 2$. Then by stationarity $\\lambda^{*} = x^{*} = 2$, which satisfies $\\lambda^{*} \\geq 0$ and $x^{*} \\geq 2$.\n\nThus, the unique KKT point is $(x^{*}, \\lambda^{*}) = (2, 2)$. Because the problem is convex with an affine constraint, this KKT point is the saddle point of the Lagrangian. Equivalently, verifying the saddle property: $L(x, 2)$ is minimized at $x = 2$, and $L(2, \\lambda) = 2$ for all $\\lambda \\geq 0$, with the dual maximizer at $\\lambda^{*} = 2$.", "answer": "$$\\boxed{\\begin{pmatrix} 2  2 \\end{pmatrix}}$$", "id": "2167418"}, {"introduction": "Building on the foundational mechanics, we now apply the KKT conditions to a multi-dimensional problem with a clear geometric meaning: finding the closest point in a given region [@problem_id:2407313]. This practice of projecting a point onto a convex set is a cornerstone of many algorithms in machine learning and signal processing. You will see how the KKT multipliers can be interpreted as indicators of which constraints are \"active\" and how the problem neatly separates into component-wise decisions, providing powerful intuition.", "problem": "Let $p \\in \\mathbb{R}^{3}$ be the point $p = (-1,-2,3)$. Consider the problem of finding the point $x \\in \\mathbb{R}^{3}$ in the first orthant that is closest to $p$ in the Euclidean sense. Formulate this as the constrained optimization problem\n$$\n\\min_{x \\in \\mathbb{R}^{3}} \\; \\frac{1}{2}\\|x - p\\|_{2}^{2} \\quad \\text{subject to} \\quad x_{i} \\ge 0 \\text{ for } i=1,2,3.\n$$\nDetermine the Karush–Kuhn–Tucker (KKT) point $(x^{\\star},\\lambda^{\\star})$, where $\\lambda^{\\star} \\in \\mathbb{R}^{3}$ are the Lagrange multipliers associated with the inequality constraints. Provide your final answer as a single row vector $(x_{1}^{\\star},x_{2}^{\\star},x_{3}^{\\star},\\lambda_{1}^{\\star},\\lambda_{2}^{\\star},\\lambda_{3}^{\\star})$. No rounding is required.", "solution": "We are asked to minimize the squared Euclidean distance from $x$ to $p$ subject to the first-orthant constraints. The optimization problem is\n$$\n\\min_{x \\in \\mathbb{R}^{3}} \\; f(x) = \\frac{1}{2}\\|x - p\\|_{2}^{2} \\quad \\text{subject to} \\quad x_{i} \\ge 0 \\text{ for } i=1,2,3,\n$$\nwith $p = (-1,-2,3)$. We express the inequality constraints in the standard form $g_{i}(x) \\le 0$ by defining\n$$\ng_{i}(x) = -x_{i} \\le 0, \\quad i=1,2,3.\n$$\nThe Lagrangian $L:\\mathbb{R}^{3} \\times \\mathbb{R}^{3} \\to \\mathbb{R}$ is\n$$\nL(x,\\lambda) = \\frac{1}{2}\\|x - p\\|_{2}^{2} + \\sum_{i=1}^{3} \\lambda_{i} g_{i}(x) = \\frac{1}{2}\\|x - p\\|_{2}^{2} - \\lambda^{\\top} x,\n$$\nwhere $\\lambda \\in \\mathbb{R}^{3}$ are the Lagrange multipliers. The Karush–Kuhn–Tucker (KKT) conditions consist of:\n1. Stationarity:\n$$\n\\nabla_{x} L(x,\\lambda) = x - p - \\lambda = 0 \\;\\; \\Longrightarrow \\;\\; \\lambda = x - p.\n$$\n2. Primal feasibility:\n$$\nx_{i} \\ge 0, \\quad i=1,2,3.\n$$\n3. Dual feasibility:\n$$\n\\lambda_{i} \\ge 0, \\quad i=1,2,3.\n$$\n4. Complementary slackness:\n$$\n\\lambda_{i} \\, x_{i} = 0, \\quad i=1,2,3.\n$$\nSubstitute $p = (-1,-2,3)$ into the stationarity relation $\\lambda = x - p$ to obtain componentwise\n$$\n\\lambda_{1} = x_{1} + 1, \\quad \\lambda_{2} = x_{2} + 2, \\quad \\lambda_{3} = x_{3} - 3.\n$$\nWe now enforce complementary slackness and feasibility for each coordinate.\n\nFor $i=1$:\n- If $x_{1}  0$, then $\\lambda_{1} = 0$, which implies $x_{1} + 1 = 0 \\Rightarrow x_{1} = -1$, contradicting $x_{1}  0$.\n- Therefore, $x_{1} = 0$, and then $\\lambda_{1} = 0 + 1 = 1 \\ge 0$.\n\nFor $i=2$:\n- If $x_{2}  0$, then $\\lambda_{2} = 0$, which implies $x_{2} + 2 = 0 \\Rightarrow x_{2} = -2$, contradicting $x_{2}  0$.\n- Therefore, $x_{2} = 0$, and then $\\lambda_{2} = 0 + 2 = 2 \\ge 0$.\n\nFor $i=3$:\n- If $x_{3}  0$, then $\\lambda_{3} = 0$, which implies $x_{3} - 3 = 0 \\Rightarrow x_{3} = 3$, consistent with $x_{3}  0$.\n- This choice satisfies complementary slackness since $\\lambda_{3} x_{3} = 0 \\cdot 3 = 0$ and dual feasibility since $\\lambda_{3} = 0 \\ge 0$.\n\nCollecting the components, the unique KKT point is\n$$\nx^{\\star} = (0,0,3), \\quad \\lambda^{\\star} = (1,2,0).\n$$\nBecause the objective function is strictly convex and the feasible set is convex, these KKT conditions are sufficient for global optimality, confirming that $(x^{\\star},\\lambda^{\\star})$ is the KKT point for the problem.", "answer": "$$\\boxed{\\begin{pmatrix} 0  0  3  1  2  0 \\end{pmatrix}}$$", "id": "2407313"}, {"introduction": "The ultimate goal for a computational engineer is to translate mathematical theory into robust code. This final practice moves from analytical solutions to numerical verification by asking you to implement a KKT condition checker [@problem_id:2407334]. This task is essential for developing optimization solvers, as it forms the basis for defining stopping criteria and validating the output of complex numerical algorithms, highlighting the practical importance of handling numerical tolerances.", "problem": "You are given a concrete nonlinear optimization problem and asked to implement a programmatic verifier for the Karush–Kuhn–Tucker (KKT) optimality conditions. Consider the following minimization problem in computational engineering form: minimize the objective function $f(x) = (x_1 - 1)^2 + (x_2 - 2)^2$ subject to the inequality constraints $g_1(x) = x_1 + x_2 - 2 \\le 0$, $g_2(x) = -x_1 \\le 0$, $g_3(x) = -x_2 \\le 0$, and the equality constraint $h_1(x) = x_1 - x_2 = 0$, where $x \\in \\mathbb{R}^2$. Let $\\lambda \\in \\mathbb{R}^3$ denote the vector of Lagrange multipliers for the inequality constraints and $\\mu \\in \\mathbb{R}$ denote the multiplier for the equality constraint. The KKT verification must include the four canonical components: primal feasibility, dual feasibility, complementary slackness, and stationarity. Use the Euclidean vector norm for stationarity residuals and enforce all checks up to a prescribed tolerance $\\epsilon  0$.\n\nYour task is to write a complete program that implements a function named check_kkt(x, lam, mu, eps) which returns a boolean indicating whether a given triple $(x, \\lambda, \\mu)$ satisfies the KKT conditions for the specified problem within the tolerance $\\epsilon$. The function must perform the following checks:\n- Primal feasibility: all inequality constraints satisfy $g_i(x) \\le \\epsilon$ and equality constraints satisfy $|h_j(x)| \\le \\epsilon$.\n- Dual feasibility: all multipliers for inequality constraints satisfy $\\lambda_i \\ge -\\epsilon$.\n- Complementary slackness: all products satisfy $|\\lambda_i \\, g_i(x)| \\le \\epsilon$.\n- Stationarity: the Euclidean norm of the Lagrangian gradient satisfies $\\|\\nabla f(x) + \\sum_{i=1}^{3} \\lambda_i \\nabla g_i(x) + \\mu \\nabla h_1(x)\\|_2 \\le \\epsilon$.\n\nThe gradients for this problem are:\n- $\\nabla f(x) = \\big(2(x_1 - 1), \\, 2(x_2 - 2)\\big)$,\n- $\\nabla g_1(x) = (1, \\, 1)$, $\\nabla g_2(x) = (-1, \\, 0)$, $\\nabla g_3(x) = (0, \\, -1)$,\n- $\\nabla h_1(x) = (1, \\, -1)$.\n\nYour program must evaluate the function check_kkt on the following test suite of inputs and aggregate the boolean results. For each test case, $x$ is given as a length-$2$ real array, $\\lambda$ is a length-$3$ real array, $\\mu$ is a real scalar, and $\\epsilon$ is a positive real scalar:\n- Test case $1$ (canonical KKT point): $x = [1, 1]$, $\\lambda = [1, 0, 0]$, $\\mu = -1$, $\\epsilon = 10^{-8}$. Expected: True.\n- Test case $2$ (stationarity violated): $x = [1, 1]$, $\\lambda = [0.5, 0, 0]$, $\\mu = -0.5$, $\\epsilon = 10^{-8}$. Expected: False.\n- Test case $3$ (primal infeasible): $x = [1.2, 1.2]$, $\\lambda = [1, 0, 0]$, $\\mu = -1$, $\\epsilon = 10^{-8}$. Expected: False.\n- Test case $4$ (near-KKT within tolerance): $x = [1, 1 + 10^{-7}]$, $\\lambda = [1, -10^{-7}, 0]$, $\\mu = -1 - 10^{-7}$, $\\epsilon = 10^{-6}$. Expected: True.\n- Test case $5$ (complementary slackness violated): $x = [1, 1]$, $\\lambda = [1, 0.1, 0]$, $\\mu = -1$, $\\epsilon = 10^{-8}$. Expected: False.\n\nFinal output format requirement: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[True,False,True,False,True]\"). No additional text must be printed. No physical units are involved. Angles are not involved. Percentages are not involved. The only accepted output types are boolean values aggregated as specified.", "solution": "The problem is to minimize an objective function $f(x)$ subject to a set of inequality and equality constraints. This is a standard nonlinear programming (NLP) problem of the form:\n$$\n\\begin{aligned}\n\\text{minimize} \\quad  f(x) \\\\\n\\text{subject to} \\quad  g_i(x) \\le 0, \\quad i = 1, \\dots, m \\\\\n h_j(x) = 0, \\quad j = 1, \\dots, p\n\\end{aligned}\n$$\nFor the specific case provided:\n-   Objective function: $f(x) = (x_1 - 1)^2 + (x_2 - 2)^2$ where $x = (x_1, x_2) \\in \\mathbb{R}^2$.\n-   Inequality constraints ($m=3$):\n    -   $g_1(x) = x_1 + x_2 - 2 \\le 0$\n    -   $g_2(x) = -x_1 \\le 0$\n    -   $g_3(x) = -x_2 \\le 0$\n-   Equality constraint ($p=1$):\n    -   $h_1(x) = x_1 - x_2 = 0$\n\nThe Karush-Kuhn-Tucker conditions are first-order necessary conditions for a solution in NLP to be optimal, provided some regularity conditions hold. To state these conditions, we first introduce the Lagrangian function, $L(x, \\lambda, \\mu)$, associated with the problem:\n$$\nL(x, \\lambda, \\mu) = f(x) + \\sum_{i=1}^{m} \\lambda_i g_i(x) + \\sum_{j=1}^{p} \\mu_j h_j(x)\n$$\nwhere $\\lambda = (\\lambda_1, \\dots, \\lambda_m)$ are the Lagrange multipliers for the inequality constraints (also known as dual variables) and $\\mu = (\\mu_1, \\dots, \\mu_p)$ are the multipliers for the equality constraints.\n\nFor the given problem, the Lagrangian is:\n$$\nL(x, \\lambda, \\mu) = (x_1 - 1)^2 + (x_2 - 2)^2 + \\lambda_1(x_1 + x_2 - 2) + \\lambda_2(-x_1) + \\lambda_3(-x_2) + \\mu_1(x_1 - x_2)\n$$\nSince there is only one equality constraint, we denote its multiplier by $\\mu$. The KKT conditions for a point $(x^*, \\lambda^*, \\mu^*)$ to be a candidate for optimum are:\n\n1.  **Primal Feasibility**: The point $x^*$ must satisfy all original constraints.\n    -   $g_i(x^*) \\le 0$ for $i \\in \\{1, 2, 3\\}$\n    -   $h_1(x^*) = 0$\n2.  **Dual Feasibility**: The Lagrange multipliers for the inequality constraints must be non-negative.\n    -   $\\lambda_i^* \\ge 0$ for $i \\in \\{1, 2, 3\\}$\n3.  **Complementary Slackness**: For each inequality constraint, either the constraint is active (i.e., holds with equality) or its corresponding multiplier is zero.\n    -   $\\lambda_i^* g_i(x^*) = 0$ for $i \\in \\{1, 2, 3\\}$\n4.  **Stationarity**: The gradient of the Lagrangian with respect to the primal variables $x$ must be zero at the point $(x^*, \\lambda^*, \\mu^*)$.\n    -   $\\nabla_x L(x^*, \\lambda^*, \\mu^*) = 0$\n\nFor numerical verification, these exact conditions must be relaxed using a small, positive tolerance $\\epsilon$. The programmatic checks are structured as follows:\n\n-   **Primal Feasibility Check**:\n    -   $g_i(x) \\le \\epsilon$ for $i \\in \\{1, 2, 3\\}$\n    -   $|h_1(x)| \\le \\epsilon$\n-   **Dual Feasibility Check**:\n    -   $\\lambda_i \\ge -\\epsilon$ for $i \\in \\{1, 2, 3\\}$. This allows for small negative values that are numerically zero.\n-   **Complementary Slackness Check**:\n    -   $|\\lambda_i g_i(x)| \\le \\epsilon$ for $i \\in \\{1, 2, 3\\}$.\n-   **Stationarity Check**:\n    The gradient of the Lagrangian with respect to $x$ is:\n    $$\n    \\nabla_x L(x, \\lambda, \\mu) = \\nabla f(x) + \\sum_{i=1}^{3} \\lambda_i \\nabla g_i(x) + \\mu \\nabla h_1(x)\n    $$\n    Substituting the given gradients:\n    -   $\\nabla f(x) = (2(x_1 - 1), 2(x_2 - 2))$\n    -   $\\nabla g_1(x) = (1, 1)$, $\\nabla g_2(x) = (-1, 0)$, $\\nabla g_3(x) = (0, -1)$\n    -   $\\nabla h_1(x) = (1, -1)$\n    The components of the Lagrangian gradient are:\n    $$\n    \\frac{\\partial L}{\\partial x_1} = 2(x_1 - 1) + \\lambda_1(1) + \\lambda_2(-1) + \\lambda_3(0) + \\mu(1) = 2(x_1 - 1) + \\lambda_1 - \\lambda_2 + \\mu\n    $$\n    $$\n    \\frac{\\partial L}{\\partial x_2} = 2(x_2 - 2) + \\lambda_1(1) + \\lambda_2(0) + \\lambda_3(-1) + \\mu(-1) = 2(x_2 - 2) + \\lambda_1 - \\lambda_3 - \\mu\n    $$\n    The stationarity condition is then that the Euclidean norm of this gradient vector, known as the stationarity residual, must be close to zero:\n    $$\n    \\|\\nabla_x L(x, \\lambda, \\mu)\\|_2 \\le \\epsilon\n    $$\n\nThe implemented function `check_kkt` will systematically perform these four checks. If any single check fails, the function immediately returns `False`. If all checks pass, it returns `True`.\n\nLet us demonstrate with the first test case: $x = [1, 1]$, $\\lambda = [1, 0, 0]$, $\\mu = -1$, and $\\epsilon = 10^{-8}$.\n-   Variables: $x_1 = 1$, $x_2 = 1$, $\\lambda_1 = 1$, $\\lambda_2 = 0$, $\\lambda_3 = 0$, $\\mu = -1$.\n\n-   **Primal Feasibility**:\n    -   $g_1(x) = 1 + 1 - 2 = 0$. $0 \\le 10^{-8}$ is true.\n    -   $g_2(x) = -1$. $-1 \\le 10^{-8}$ is true.\n    -   $g_3(x) = -1$. $-1 \\le 10^{-8}$ is true.\n    -   $h_1(x) = 1 - 1 = 0$. $|0| \\le 10^{-8}$ is true.\n    The condition is satisfied.\n\n-   **Dual Feasibility**:\n    -   $\\lambda_1 = 1 \\ge -10^{-8}$ is true.\n    -   $\\lambda_2 = 0 \\ge -10^{-8}$ is true.\n    -   $\\lambda_3 = 0 \\ge -10^{-8}$ is true.\n    The condition is satisfied.\n\n-   **Complementary Slackness**:\n    -   $|\\lambda_1 g_1(x)| = |1 \\cdot 0| = 0$. $0 \\le 10^{-8}$ is true.\n    -   $|\\lambda_2 g_2(x)| = |0 \\cdot (-1)| = 0$. $0 \\le 10^{-8}$ is true.\n    -   $|\\lambda_3 g_3(x)| = |0 \\cdot (-1)| = 0$. $0 \\le 10^{-8}$ is true.\n    The condition is satisfied.\n\n-   **Stationarity**:\n    -   $\\frac{\\partial L}{\\partial x_1} = 2(1 - 1) + 1 - 0 + (-1) = 0$.\n    -   $\\frac{\\partial L}{\\partial x_2} = 2(1 - 2) + 1 - 0 - (-1) = -2 + 1 + 1 = 0$.\n    -   The gradient vector is $(0, 0)$.\n    -   $\\|\\nabla_x L\\|_2 = \\sqrt{0^2 + 0^2} = 0$. $0 \\le 10^{-8}$ is true.\n    The condition is satisfied.\n\nSince all four conditions are satisfied within the tolerance $\\epsilon$, the point $(x, \\lambda, \\mu)$ is a valid KKT point, and the function correctly returns `True`. The other test cases are evaluated analogously, and the program will aggregate the boolean outcomes.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef check_kkt(x, lam, mu, eps):\n    \"\"\"\n    Verifies the KKT conditions for a given point (x, lam, mu).\n\n    Args:\n        x (np.ndarray): Primal variables, a 2-element array [x1, x2].\n        lam (np.ndarray): Lagrange multipliers for inequality constraints, a 3-element array [lam1, lam2, lam3].\n        mu (float): Lagrange multiplier for the equality constraint.\n        eps (float): Tolerance for all numerical checks.\n\n    Returns:\n        bool: True if the KKT conditions are satisfied, False otherwise.\n    \"\"\"\n    x1, x2 = x[0], x[1]\n    lam1, lam2, lam3 = lam[0], lam[1], lam[2]\n\n    # --- 1. Primal Feasibility ---\n    # g1(x) = x1 + x2 - 2 = 0\n    g1 = x1 + x2 - 2\n    if g1  eps:\n        return False\n\n    # g2(x) = -x1 = 0\n    g2 = -x1\n    if g2  eps:\n        return False\n\n    # g3(x) = -x2 = 0\n    g3 = -x2\n    if g3  eps:\n        return False\n\n    # h1(x) = x1 - x2 = 0\n    h1 = x1 - x2\n    if abs(h1)  eps:\n        return False\n        \n    # --- 2. Dual Feasibility ---\n    # lam_i = 0 for all i\n    if np.any(lam  -eps):\n        return False\n\n    # --- 3. Complementary Slackness ---\n    # lam1 * g1(x) = 0\n    if abs(lam1 * g1)  eps:\n        return False\n\n    # lam2 * g2(x) = 0\n    if abs(lam2 * g2)  eps:\n        return False\n\n    # lam3 * g3(x) = 0\n    if abs(lam3 * g3)  eps:\n        return False\n\n    # --- 4. Stationarity ---\n    # Gradient of the Lagrangian with respect to x must be zero.\n    # grad_L_x1 = 2*(x1 - 1) + lam1 - lam2 + mu\n    # grad_L_x2 = 2*(x2 - 2) + lam1 - lam3 - mu\n    grad_L_x1 = 2 * (x1 - 1) + lam1 - lam2 + mu\n    grad_L_x2 = 2 * (x2 - 2) + lam1 - lam3 - mu\n    \n    grad_L = np.array([grad_L_x1, grad_L_x2])\n    \n    # Check the Euclidean norm of the gradient\n    stationarity_residual = np.linalg.norm(grad_L)\n    \n    if stationarity_residual  eps:\n        return False\n        \n    # If all conditions pass\n    return True\n\ndef solve():\n    \"\"\"\n    Executes the KKT verification for a predefined suite of test cases.\n    \"\"\"\n    test_cases = [\n        # Test case 1 (canonical KKT point)\n        {'x': np.array([1.0, 1.0]), 'lam': np.array([1.0, 0.0, 0.0]), 'mu': -1.0, 'eps': 1e-8},\n        # Test case 2 (stationarity violated)\n        {'x': np.array([1.0, 1.0]), 'lam': np.array([0.5, 0.0, 0.0]), 'mu': -0.5, 'eps': 1e-8},\n        # Test case 3 (primal infeasible)\n        {'x': np.array([1.2, 1.2]), 'lam': np.array([1.0, 0.0, 0.0]), 'mu': -1.0, 'eps': 1e-8},\n        # Test case 4 (near-KKT within tolerance)\n        {'x': np.array([1.0, 1.0 + 1e-7]), 'lam': np.array([1.0, -1e-7, 0.0]), 'mu': -1.0 - 1e-7, 'eps': 1e-6},\n        # Test case 5 (complementary slackness violated)\n        {'x': np.array([1.0, 1.0]), 'lam': np.array([1.0, 0.1, 0.0]), 'mu': -1.0, 'eps': 1e-8},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = check_kkt(case['x'], case['lam'], case['mu'], case['eps'])\n        results.append(result)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2407334"}]}