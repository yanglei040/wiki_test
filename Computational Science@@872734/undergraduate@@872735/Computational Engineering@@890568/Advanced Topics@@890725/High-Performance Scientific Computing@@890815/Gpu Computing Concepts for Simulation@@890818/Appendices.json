{"hands_on_practices": [{"introduction": "Many scientific simulations involve sorting large collections of objects, where each object contains extensive data but is sorted by a simple key. This exercise explores a fundamental GPU optimization: reducing memory bandwidth consumption by separating the sort key from its large data payload. By modeling a memory-bandwidth-bound scenario [@problem_id:2398440], you will quantify the significant performance gains of sorting a lightweight key-index list compared to sorting the full, heavy data structures, a crucial technique for any bandwidth-limited application.", "problem": "You are designing a bandwidth-efficient sorting pipeline for a Graphics Processing Unit (GPU) to sort an array of records for a physics simulation. Each record is a fixed-size struct containing a 32-bit key and a large payload. The simulation requires sorting by the key only. The goal is to design a kernel plan that isolates the memory bandwidth bottleneck and to estimate the execution time using a first-principles model grounded in the roofline perspective.\n\nFundamentals:\n- The roofline model states that, for a memory-bandwidth-bound kernel, the execution time is lower bounded by the ratio of total bytes transferred to the sustained device memory bandwidth. If a kernel moves a total of $X$ bytes through device memory and the sustained bandwidth is $B$ bytes per second, the time satisfies $T \\ge X / B$.\n- A least-significant-digit parallel radix sort of a $w$-bit key with $r$ bits processed per pass requires $p = w / r$ passes.\n- In a global-memory ping-pong implementation of stable parallel radix sort, each pass performs one full read and one full write of the array being sorted.\n\nScenario and definitions:\n- Let $N$ be the number of elements.\n- Let $S$ be the struct size in bytes.\n- Let $K$ be the key size in bytes, with $K = 4$ for a $32$-bit key.\n- Let $I$ be the index size in bytes (use $I = 4$ for $N \\le 2^{32}$).\n- Let $w = 32$ be the key width in bits.\n- Let $r$ be the number of bits processed per radix pass, so the number of passes is $p = w / r$.\n- Let $B$ be the sustained device memory bandwidth in bytes per second.\n- All times must be expressed in seconds. All angles are not applicable. No percentages are used.\n\nTwo strategies:\n- Strategy A (full-struct sort): Perform a stable radix sort directly on the array of structs of size $S$ bytes. Under the ping-pong model and ignoring histogram overhead, each pass reads and writes the entire array, so the total bytes moved is\n$$\nX_{\\mathrm{A}}(N,S,p) = N \\cdot (2 S) \\cdot p.\n$$\nThe bandwidth-bound time estimate is\n$$\nT_{\\mathrm{A}} = \\frac{X_{\\mathrm{A}}}{B}.\n$$\n- Strategy B (keys-and-indices then permute, isolating bandwidth): Materialize a separate array of key-and-index pairs, sort that lighter array, then perform a single permute of the large structs. Assume the materialization performs a one-time read of $K$ bytes per element from the struct and writes $K+I$ bytes per element to the key-index array. The sort of key-index pairs requires, per pass, a read and write of $K+I$ bytes per element. The final struct permutation performs one read and one write of $S$ bytes per element. The total bytes moved is\n$$\nX_{\\mathrm{B}}(N,S,K,I,p) = N \\cdot \\Big( (K + (K+I)) + 2 (K+I) p + 2 S \\Big).\n$$\nThe bandwidth-bound time estimate is\n$$\nT_{\\mathrm{B}} = \\frac{X_{\\mathrm{B}}}{B}.\n$$\nThe predicted speedup of Strategy B over Strategy A is\n$$\n\\text{Speedup} = \\frac{T_{\\mathrm{A}}}{T_{\\mathrm{B}}} = \\frac{X_{\\mathrm{A}}}{X_{\\mathrm{B}}}.\n$$\n\nImplementation requirements:\n- Implement a program that computes $T_{\\mathrm{A}}$, $T_{\\mathrm{B}}$, and the speedup for each of the following test cases. Do not allocate arrays of size $N$; compute the times analytically using the formulas above.\n- Units: Report times $T_{\\mathrm{A}}$ and $T_{\\mathrm{B}}$ in seconds. Format all floating-point outputs with exactly six digits after the decimal point.\n- Also verify the algorithmic transformation on a small synthetic case by constructing a tiny array of structs, sorting by full structs and by keys-and-indices plus permutation (using Central Processing Unit (CPU) code), and checking that the final order of keys matches. This correctness check must produce a boolean.\n- For all cases, use $w = 32$ bits and $K = 4$ bytes. Use $I = 4$ bytes. Use $p = w / r$.\n\nTest suite:\n- Case $1$: $N = 10^9$, $S = 128$ bytes, $r = 8$ bits per pass, $B = 900 \\times 10^9$ bytes per second.\n- Case $2$: $N = 10^9$, $S = 64$ bytes, $r = 4$ bits per pass, $B = 900 \\times 10^9$ bytes per second.\n- Case $3$: $N = 5 \\times 10^7$, $S = 192$ bytes, $r = 8$ bits per pass, $B = 450 \\times 10^9$ bytes per second.\n- Case $4$: $N = 2 \\times 10^8$, $S = 16$ bytes, $r = 8$ bits per pass, $B = 1.6 \\times 10^{12}$ bytes per second.\n\nFinal output specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the following order:\n$[T_{\\mathrm{A}}^{(1)}, T_{\\mathrm{B}}^{(1)}, \\text{Speedup}^{(1)}, T_{\\mathrm{A}}^{(2)}, T_{\\mathrm{B}}^{(2)}, \\text{Speedup}^{(2)}, T_{\\mathrm{A}}^{(3)}, T_{\\mathrm{B}}^{(3)}, \\text{Speedup}^{(3)}, T_{\\mathrm{A}}^{(4)}, T_{\\mathrm{B}}^{(4)}, \\text{Speedup}^{(4)}, \\text{Correct}]$\nwhere the superscript $(i)$ denotes test case $i$, each time and speedup is a float with six digits after the decimal point, and $\\text{Correct}$ is a boolean that is true if the small-case verification shows identical key order after the two strategies. No other text should be printed.", "solution": "The problem statement has been rigorously evaluated and is deemed valid. It is scientifically grounded in the principles of computational performance modeling, specifically the roofline model as applied to GPU memory bandwidth. The problem is well-posed, objective, and provides all necessary data and formulae for a unique solution. We shall proceed with the analytical solution and algorithmic verification as requested.\n\nThe objective is to compare two strategies for sorting an array of $N$ records, each of size $S$ bytes, based on a $w$-bit key. The comparison is performed by estimating the execution time under a memory-bandwidth-bound assumption.\n\nThe key parameters are defined as:\n- $N$: number of records.\n- $S$: size of one record (struct) in bytes.\n- $K$: size of the sort key in bytes, given as $K=4$.\n- $I$: size of an index in bytes, given as $I=4$.\n- $w$: width of the key in bits, given as $w=32$.\n- $r$: number of bits processed per pass of radix sort.\n- $p$: number of passes for the radix sort, calculated as $p=w/r$.\n- $B$: sustained device memory bandwidth in bytes per second.\n\nThe execution time $T$ of a memory-bandwidth-bound kernel is estimated using the roofline model as:\n$$T \\ge \\frac{X}{B}$$\nwhere $X$ represents the total bytes transferred to and from device memory. We will use this as an equality for our estimation, $T = X/B$.\n\n**Strategy A: Full-Struct Sort**\n\nThis strategy performs a stable radix sort directly on the array of records. According to the provided ping-pong model, each of the $p$ passes of the radix sort involves reading the entire array of $N$ records and writing it back to memory.\nThe size of the array is $N \\cdot S$ bytes.\nThe memory traffic for one pass is a full read and a full write: $N \\cdot S + N \\cdot S = 2 N S$ bytes.\nFor $p$ passes, the total memory traffic $X_{\\mathrm{A}}$ is:\n$$X_{\\mathrm{A}}(N, S, p) = N \\cdot (2S) \\cdot p$$\nThe corresponding estimated execution time $T_{\\mathrm{A}}$ is:\n$$T_{\\mathrm{A}} = \\frac{X_{\\mathrm{A}}}{B} = \\frac{2NSp}{B}$$\n\n**Strategy B: Key-Index Sort and Permutation**\n\nThis strategy seeks to minimize memory traffic by sorting a lightweight representation of the data and applying the resulting order to the full records in a final step. The process consists of three stages:\n\n1.  **Materialize Key-Index Pairs:** A temporary array of (key, index) pairs is created. This requires a kernel that reads the key from each of the $N$ original structs and writes out a new pair consisting of the key and the original index of the element.\n    - Bytes read: $N \\cdot K$.\n    - Bytes written: $N \\cdot (K+I)$.\n    - Total traffic for this stage: $N \\cdot K + N \\cdot (K+I) = N \\cdot (2K+I)$.\n\n2.  **Sort Key-Index Pairs:** A stable radix sort is performed on the array of $N$ key-index pairs. Each pair has a size of $(K+I)$ bytes.\n    - Memory traffic for one pass (read and write): $N \\cdot (K+I) + N \\cdot (K+I) = 2N(K+I)$.\n    - Total traffic for $p$ passes: $p \\cdot 2N(K+I)$.\n\n3.  **Permute Full Structs:** The sorted order, now encoded in the sorted key-index array, is used to rearrange the original large structs into a new sorted array. The model simplifies this complex gather operation to one full read and one full write of the struct array.\n    - Total traffic for permutation: $N \\cdot S (\\text{read}) + N \\cdot S (\\text{write}) = 2NS$.\n\nThe total memory traffic for Strategy B, $X_{\\mathrm{B}}$, is the sum of the traffic from all three stages:\n$$X_{\\mathrm{B}}(N, S, K, I, p) = N \\cdot (2K+I) + N \\cdot 2p(K+I) + N \\cdot 2S$$\n$$X_{\\mathrm{B}}(N, S, K, I, p) = N \\cdot \\Big( (2K+I) + 2p(K+I) + 2S \\Big)$$\nThe corresponding estimated execution time $T_{\\mathrm{B}}$ is:\n$$T_{\\mathrm{B}} = \\frac{X_{\\mathrm{B}}}{B}$$\n\n**Speedup Calculation**\n\nThe speedup of Strategy B over Strategy A is the ratio of their execution times:\n$$\\text{Speedup} = \\frac{T_{\\mathrm{A}}}{T_{\\mathrm{B}}} = \\frac{X_{\\mathrm{A}}/B}{X_{\\mathrm{B}}/B} = \\frac{X_{\\mathrm{A}}}{X_{\\mathrm{B}}}$$\n\n**Algorithmic Correctness Verification**\n\nTo verify that Strategy B produces the same sorted result as Strategy A, a small-scale test is performed. An initial array of records with random keys is created. It is sorted using two methods:\n1.  A stable sort is applied directly to the array of records, sorting by the key field.\n2.  An array of (key, original\\_index) pairs is created, stably sorted by key, and the resulting sorted indices are used to permute the original array into a new one.\nThe final sorted arrays from both methods are compared for equality. A boolean value indicates whether the outcomes match.\n\n**Example Calculation (Case 1)**\n\n- Given: $N=10^9$, $S=128$, $r=8$, $B=900 \\times 10^9$.\n- Constants: $w=32$, $K=4$, $I=4$.\n- Number of passes: $p = w/r = 32/8 = 4$.\n\n- **Strategy A:**\n  $X_{\\mathrm{A}} = 10^9 \\cdot (2 \\cdot 128) \\cdot 4 = 1.024 \\times 10^{12}$ bytes.\n  $T_{\\mathrm{A}} = \\frac{1.024 \\times 10^{12}}{900 \\times 10^9} \\approx 1.137778$ s.\n\n- **Strategy B:**\n  $X_{\\mathrm{B}} = 10^9 \\cdot \\Big( (2 \\cdot 4 + 4) + 2 \\cdot 4 \\cdot (4+4) + 2 \\cdot 128 \\Big) = 10^9 \\cdot (12 + 64 + 256) = 3.32 \\times 10^{11}$ bytes.\n  $T_{\\mathrm{B}} = \\frac{3.32 \\times 10^{11}}{900 \\times 10^9} \\approx 0.368889$ s.\n\n- **Speedup:**\n  $\\text{Speedup} = \\frac{T_{\\mathrm{A}}}{T_{\\mathrm{B}}} = \\frac{1.137778}{0.368889} \\approx 3.084337$.\n\nThe same procedure is applied to all test cases, and the results are compiled into the final output.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes performance estimates for two sorting strategies and verifies\n    algorithmic correctness on a small case.\n    \"\"\"\n\n    # --- Part 1: Algorithmic Correctness Verification ---\n\n    def verify_correctness():\n        \"\"\"\n        Verify that sorting keys-and-indices then permuting produces the\n        same result as sorting the full structs directly.\n        \"\"\"\n        # Create a small, synthetic dataset of structs (records)\n        # A struct has a 'key' and a 'payload'\n        num_records = 20\n        # Use a seed for reproducibility\n        rng = np.random.default_rng(seed=42)\n        # Generate some duplicate keys to test for stability\n        keys = rng.integers(0, 10, size=num_records, dtype=np.uint32)\n        \n        # Create the structured array\n        struct_dtype = np.dtype([('key', np.uint32), ('payload', np.uint8, (12,))])\n        structs = np.zeros(num_records, dtype=struct_dtype)\n        structs['key'] = keys\n        # Assign a unique payload to each record to check permutation\n        for i in range(num_records):\n            structs[i]['payload'] = i\n\n        # --- Strategy A: Sort full structs ---\n        # Use a stable sort to respect original order of duplicate keys\n        result_a = np.sort(structs, order='key', kind='stable')\n\n        # --- Strategy B: Sort key-index pairs, then permute ---\n        # 1. Materialize key-index pairs\n        key_index_dtype = np.dtype([('key', np.uint32), ('index', np.uint32)])\n        key_index_pairs = np.empty(num_records, dtype=key_index_dtype)\n        key_index_pairs['key'] = structs['key']\n        key_index_pairs['index'] = np.arange(num_records, dtype=np.uint32)\n\n        # 2. Sort key-index pairs (stably)\n        sorted_key_index_pairs = np.sort(key_index_pairs, order='key', kind='stable')\n\n        # 3. Permute original structs according to sorted indices\n        # This is a gather operation: result_b[i] = structs[old_index]\n        permuted_indices = sorted_key_index_pairs['index']\n        result_b = structs[permuted_indices]\n\n        # 4. Verification: check if the two results are identical\n        return np.array_equal(result_a, result_b)\n\n    correctness_check_passed = verify_correctness()\n\n    # --- Part 2: Performance Estimation for Test Cases ---\n    \n    # Define constants from the problem statement\n    w = 32  # key width in bits\n    K = 4   # key size in bytes\n    I = 4   # index size in bytes\n\n    # Define the test cases\n    test_cases = [\n        # (N, S, r, B)\n        (10**9, 128, 8, 900 * 10**9),\n        (10**9, 64, 4, 900 * 10**9),\n        (5 * 10**7, 192, 8, 450 * 10**9),\n        (2 * 10**8, 16, 8, 1.6 * 10**12),\n    ]\n\n    results = []\n    \n    for N, S, r, B in test_cases:\n        N, S, r, B = float(N), float(S), float(r), float(B)\n        \n        # Number of radix sort passes\n        p = w / r\n\n        # Strategy A: full-struct sort\n        X_A = N * (2 * S) * p\n        T_A = X_A / B\n\n        # Strategy B: keys-and-indices then permute\n        # Per-element traffic: (K read + (K+I) write) + p*(2*(K+I) read/write) + (2*S read/write)\n        # (2K+I) + 2p(K+I) + 2S\n        X_B = N * ((K + (K + I)) + 2 * p * (K + I) + 2 * S)\n        T_B = X_B / B\n        \n        # Speedup\n        speedup = T_A / T_B if T_B > 0 else 0.0\n\n        results.extend([T_A, T_B, speedup])\n\n    # Format results to six decimal places\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    \n    # Append the boolean correctness check result\n    formatted_results.append(str(correctness_check_passed))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2398440"}, {"introduction": "After optimizing an algorithm to reduce overall memory traffic, the next step is to maximize the work done for every byte that is transferred. This practice delves into tiled matrix multiplication, a cornerstone algorithm in high-performance computing that exemplifies the principle of data reuse through the GPU memory hierarchy. By modeling a classic tiled implementation [@problem_id:2398448], you will analyze how loading data into fast on-chip shared memory can dramatically increase a kernel's arithmetic intensity, a key metric for computational efficiency.", "problem": "You are asked to design and implement a program that models the memory traffic and arithmetic intensity of a classic tiled General Matrix Multiply (GEMM) kernel as used in Graphics Processing Unit (GPU) computing for scientific simulation. The setting is the matrix product $C = A \\times B$ with non-square dimensions, where $A \\in \\mathbb{R}^{M \\times K}$, $B \\in \\mathbb{R}^{K \\times N}$, and $C \\in \\mathbb{R}^{M \\times N}$. The assumed numerical type is single-precision floating point ($32$-bit), so each matrix element occupies $4$ bytes in memory. The mathematical base for the reasoning is the definition of matrix multiplication and the counting of floating-point operations from that definition, together with simple tiling concepts used in high-performance computing.\n\nStart from the following fundamental bases:\n- The definition of matrix multiplication: for each $i \\in \\{1,\\dots,M\\}$ and $j \\in \\{1,\\dots,N\\}$, $C_{ij} = \\sum_{k=1}^{K} A_{ik} B_{kj}$.\n- Counting of floating-point operations for matrix multiplication: computing $C = A \\times B$ performs $M \\times N \\times K$ multiply-add pairs (each consisting of one multiplication and one addition), which is $2 \\times M \\times N \\times K$ floating-point operations.\n- In a tiled algorithm, the output matrix $C$ is partitioned into tiles of size $T_M \\times T_N$. For each output tile, the kernel iterates over the $K$ dimension in chunks of size $T_K$, loading one tile of $A$ of size $T_M \\times T_K$ and one tile of $B$ of size $T_K \\times T_N$ from global memory into on-chip shared memory, computing partial sums, and finally writing the $T_M \\times T_N$ results for that tile once to global memory.\n\nAssumptions to model memory movement and parallel decomposition:\n- There is no cache or shared-memory reuse across different thread blocks; only reuse within a block across the $K$-sweep is exploited. Consequently, data fetched for one output tile is not reused by other tiles.\n- The per-block shared memory footprint is dominated by simultaneously resident tiles of $A$ and $B$, i.e., $T_M \\times T_K$ elements for $A$ and $T_K \\times T_N$ elements for $B$.\n- Each matrix element occupies $4$ bytes.\n- Tiling along $M$ and $N$ uses ceiling division: the number of blocks (tiles) along $M$ is $\\lceil M / T_M \\rceil$ and along $N$ is $\\lceil N / T_N \\rceil$. Tiles at the boundaries may be partially filled when dimensions are not divisible by tile sizes.\n- Express all sizes in bytes. No physical units appear. No angles appear. Percentages are not required.\n\nTasks:\n- Derive, from the above bases, expressions for:\n  - The grid dimensions in tiles, namely the number of blocks along $M$ and along $N$.\n  - The total number of blocks.\n  - The per-block shared memory usage in bytes, given that both the $A$-tile and $B$-tile reside simultaneously.\n  - A boolean indicating whether the per-block shared memory usage fits within a given shared-memory budget $S_{\\max}$ (in bytes).\n  - The total global-memory bytes transferred by the kernel under the no-inter-block-reuse assumption, counting all reads and writes for the entire multiplication over all blocks and all $K$-tiles.\n  - The arithmetic intensity, defined as total floating-point operations divided by total global-memory bytes transferred, for the entire multiplication.\n- Implement a program that, for each test case below, computes and outputs:\n  - $\\lceil M / T_M \\rceil$,\n  - $\\lceil N / T_N \\rceil$,\n  - $\\lceil M / T_M \\rceil \\times \\lceil N / T_N \\rceil$,\n  - per-block shared memory in bytes,\n  - the fit boolean relative to $S_{\\max}$,\n  - total global-memory bytes transferred,\n  - arithmetic intensity as a floating-point number rounded to $10^{-6}$.\n- The program must not read any input. It must compute the results for the test suite embedded in the code and print a single line containing all results.\n\nTest suite:\n- Case $1$: $M = 1000$, $K = 750$, $N = 1100$, $T_M = 64$, $T_N = 64$, $T_K = 16$, $S_{\\max} = 49152$ bytes.\n- Case $2$: $M = 1000$, $K = 750$, $N = 1100$, $T_M = 128$, $T_N = 32$, $T_K = 32$, $S_{\\max} = 16384$ bytes.\n- Case $3$: $M = 1000$, $K = 750$, $N = 1100$, $T_M = 32$, $T_N = 128$, $T_K = 32$, $S_{\\max} = 49152$ bytes.\n- Case $4$: $M = 2048$, $K = 256$, $N = 96$, $T_M = 128$, $T_N = 16$, $T_K = 64$, $S_{\\max} = 49152$ bytes.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a list of per-test-case records, where each record is itself a list in the order $[\\lceil M / T_M \\rceil,\\ \\lceil N / T_N \\rceil,\\ \\lceil M / T_M \\rceil \\times \\lceil N / T_N \\rceil,\\ \\text{shared\\_bytes},\\ \\text{fits},\\ \\text{total\\_bytes},\\ \\text{arithmetic\\_intensity}]$.\n- The printed line must be exactly a single Python-style list, for example: $[[\\dots],[\\dots],\\dots]$.\n- The arithmetic intensity must be rounded to $10^{-6}$.", "solution": "The problem presented is a well-defined exercise in the performance modeling of parallel algorithms, specifically the tiled General Matrix Multiply (GEMM) on a Graphics Processing Unit (GPU). It is scientifically grounded in the principles of computational linear algebra and computer architecture. All necessary parameters are provided, and the assumptions are clearly stated, rendering the problem valid and solvable. We proceed with the derivation of the required performance metrics.\n\nThe problem context is the matrix multiplication $C = A \\times B$, where $A \\in \\mathbb{R}^{M \\times K}$, $B \\in \\mathbb{R}^{K \\times N}$, and $C \\in \\mathbb{R}^{M \\times N}$. Each matrix element is a single-precision float, occupying $S_{elem} = 4$ bytes. The algorithm partitions the output matrix $C$ into tiles, or blocks, of size $T_M \\times T_N$.\n\n1.  **Grid Dimensions and Total Blocks**\n    The computational grid is a two-dimensional arrangement of thread blocks, where each block is responsible for computing one $T_M \\times T_N$ tile of the output matrix $C$.\n    The number of blocks required along the $M$ dimension, which we denote as $GridDim_{M}$, is determined by how many tiles of height $T_M$ are needed to cover the $M$ rows of the matrix. This requires ceiling division.\n    $$GridDim_{M} = \\lceil M / T_M \\rceil$$\n    Similarly, the number of blocks along the $N$ dimension, $GridDim_{N}$, is:\n    $$GridDim_{N} = \\lceil N / T_N \\rceil$$\n    The total number of blocks in the grid, $N_{blocks}$, is the product of these two dimensions:\n    $$N_{blocks} = GridDim_{M} \\times GridDim_{N} = \\lceil M / T_M \\rceil \\times \\lceil N / T_N \\rceil$$\n\n2.  **Per-Block Shared Memory Usage**\n    For each output tile, a thread block iterates over the $K$ dimension in steps of size $T_K$. In each step, it loads a micro-tile of $A$ of size $T_M \\times T_K$ and a micro-tile of $B$ of size $T_K \\times T_N$ into its on-chip shared memory. The problem states that the shared memory footprint is dominated by the simultaneous residence of these two tiles.\n    The number of elements for the tile from $A$ is $T_M \\times T_K$.\n    The number of elements for the tile from $B$ is $T_K \\times T_N$.\n    The total required shared memory in bytes, $S_{shared}$, is the sum of the sizes of these two tiles:\n    $$S_{shared} = ( (T_M \\times T_K) + (T_K \\times T_N) ) \\times S_{elem}$$\n    $$S_{shared} = T_K \\times (T_M + T_N) \\times 4$$\n\n3.  **Shared Memory Fit Boolean**\n    This is a simple logical validation. The calculated required shared memory, $S_{shared}$, must not exceed the available shared memory budget per block, $S_{\\max}$. The boolean value, $B_{fit}$, is determined by the following condition:\n    $$B_{fit} = (S_{shared} \\le S_{\\max})$$\n\n4.  **Total Global Memory Bytes Transferred**\n    We analyze the total data movement between the GPU's global memory and the processing units, under the assumption of no data reuse between different thread blocks.\n    -   **Reads from Matrix A:** The entire matrix $A$ (of size $M \\times K$) must be read. Consider the tiling of $C$. For every column of tiles in $C$, the entire matrix $A$ must be loaded. There are $GridDim_{N} = \\lceil N / T_N \\rceil$ such tile columns. Therefore, each element of $A$ is read $\\lceil N / T_N \\rceil$ times.\n        Total bytes read from A: $Bytes_{A} = M \\times K \\times \\lceil N / T_N \\rceil \\times S_{elem}$.\n    -   **Reads from Matrix B:** Similarly, for every row of tiles in $C$, the entire matrix $B$ (of size $K \\times N$) must be read. There are $GridDim_{M} = \\lceil M / T_M \\rceil$ such tile rows. Therefore, each element of $B$ is read $\\lceil M / T_M \\rceil$ times.\n        Total bytes read from B: $Bytes_{B} = K \\times N \\times \\lceil M / T_M \\rceil \\times S_{elem}$.\n    -   **Writes to Matrix C:** The output matrix $C$ (of size $M \\times N$) is computed and written to global memory exactly once.\n        Total bytes written to C: $Bytes_{C} = M \\times N \\times S_{elem}$.\n\n    The total global memory transfer, $B_{total}$, is the sum of all reads and writes:\n    $$B_{total} = Bytes_{A} + Bytes_{B} + Bytes_{C}$$\n    $$B_{total} = (M \\times K \\times \\lceil N / T_N \\rceil + K \\times N \\times \\lceil M / T_M \\rceil + M \\times N) \\times S_{elem}$$\n\n5.  **Arithmetic Intensity**\n    Arithmetic intensity, $I$, is the ratio of total floating-point operations (FLOPs) to total bytes transferred from global memory.\n    The total number of FLOPs, $F$, for a standard matrix multiplication is $2 \\times M \\times N \\times K$, accounting for one multiplication and one addition for each inner-loop product.\n    $$F = 2 \\times M \\times N \\times K$$\n    The arithmetic intensity is therefore:\n    $$I = \\frac{F}{B_{total}} = \\frac{2 \\times M \\times N \\times K}{(M \\times K \\times \\lceil N / T_N \\rceil + K \\times N \\times \\lceil M / T_M \\rceil + M \\times N) \\times 4}$$\n    Simplifying by a factor of $2$:\n    $$I = \\frac{M \\times N \\times K}{2 \\times (M \\times K \\times \\lceil N / T_N \\rceil + K \\times N \\times \\lceil M / T_M \\rceil + M \\times N)}$$\n    This expression quantifies the number of operations performed per byte of data moved from global memory, a critical measure of algorithmic efficiency on memory-bandwidth-bound architectures like GPUs. A higher value is desirable.\n\nThese derived formulae are sufficient to solve the problem for the given test cases. We will now proceed with the implementation.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Calculates performance metrics for a tiled GEMM kernel based on a simplified GPU memory model.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (M, K, N, T_M, T_N, T_K, S_max)\n    test_cases = [\n        (1000, 750, 1100, 64, 64, 16, 49152),\n        (1000, 750, 1100, 128, 32, 32, 16384),\n        (1000, 750, 1100, 32, 128, 32, 49152),\n        (2048, 256, 96, 128, 16, 64, 49152),\n    ]\n\n    results = []\n    \n    # Size of a single-precision float in bytes\n    S_elem = 4\n\n    # Helper function for integer ceiling division\n    def ceil_div(a, b):\n        return (a + b - 1) // b\n\n    for case in test_cases:\n        M, K, N, T_M, T_N, T_K, S_max = case\n\n        # 1. Grid dimensions\n        grid_dim_m = ceil_div(M, T_M)\n        grid_dim_n = ceil_div(N, T_N)\n\n        # 2. Total number of blocks\n        total_blocks = grid_dim_m * grid_dim_n\n\n        # 3. Per-block shared memory usage in bytes\n        shared_bytes = (T_M * T_K + T_K * T_N) * S_elem\n\n        # 4. Shared memory fit boolean\n        fits = shared_bytes = S_max\n\n        # 5. Total global-memory bytes transferred\n        # Bytes read from A: each element of A is read grid_dim_n times\n        bytes_read_A = M * K * grid_dim_n * S_elem\n        # Bytes read from B: each element of B is read grid_dim_m times\n        bytes_read_B = K * N * grid_dim_m * S_elem\n        # Bytes written to C: C is written once\n        bytes_written_C = M * N * S_elem\n        total_bytes = bytes_read_A + bytes_read_B + bytes_written_C\n\n        # 6. Arithmetic intensity\n        # Total floating-point operations\n        flops = 2 * M * N * K\n        # Intensity = FLOPs / Byte\n        # Avoid division by zero, though total_bytes should always be positive for valid inputs\n        if total_bytes > 0:\n            arithmetic_intensity = flops / total_bytes\n        else:\n            arithmetic_intensity = 0.0\n\n        # Round intensity to 10^-6\n        rounded_intensity = round(arithmetic_intensity, 6)\n\n        # Assemble the results for the current case\n        case_result = [\n            grid_dim_m,\n            grid_dim_n,\n            total_blocks,\n            shared_bytes,\n            fits,\n            total_bytes,\n            rounded_intensity\n        ]\n        results.append(case_result)\n\n    # Final print statement in the exact required format.\n    print(results)\n\nsolve()\n```", "id": "2398448"}, {"introduction": "Using fast on-chip shared memory is critical for performance, but *how* you access it matters just as much as *that* you use it. This exercise focuses on a subtle but critical microarchitectural performance hazard: shared memory bank conflicts. Through a scenario involving a non-trivial diagonal memory access pattern [@problem_id:2398488], you will learn to predict and quantify how simultaneous memory requests from threads within a warp can be serialized, degrading performance, and how this can be mitigated by careful data layout and access planning.", "problem": "You are asked to model and quantify bank conflicts in Graphics Processing Unit (GPU) shared memory for a warp under a non-obvious access pattern arising from a diagonal stencil on a two-dimensional grid. Consider a warp of $W$ threads accessing a tile stored in shared memory with $B$ banks. Each element of the tile occupies exactly one bank-width unit, and the base address of the tile is aligned so that the element at linear index $0$ maps to bank $0$. The memory layout is row-major with a leading dimension (row stride) of $L$ elements. For thread lane index $\\ell \\in \\{0,1,\\dots,W-1\\}$, the accessed tile coordinates are given by\n- row index $r(\\ell) = (r_0 + \\ell) \\bmod T_r$,\n- column index $c(\\ell) = (c_0 + \\ell \\cdot d) \\bmod T_c$,\n\nwhere $T_r$ and $T_c$ are the tile dimensions in rows and columns, $r_0$ and $c_0$ are the base row and column offsets within the tile, and $d$ is the diagonal step. The linear element index addressed by thread $\\ell$ is\n$$\n\\mathrm{idx}(\\ell) = r(\\ell)\\cdot L + c(\\ell),\n$$\nand the bank index is\n$$\n\\mathrm{bank}(\\ell) = \\mathrm{idx}(\\ell) \\bmod B.\n$$\n\nAssume the following for conflict modeling:\n- In one memory instruction, each bank can serve exactly one distinct element per cycle. If $m$ threads in the warp simultaneously access $m$ distinct element indices that map to the same bank, these are serialized and take $m$ cycles for that bank.\n- If multiple threads simultaneously access the same element index within the same bank, this is considered a broadcast and takes $1$ cycle for that bank.\n- The conflict degree for the warp for that memory instruction is defined as\n$$\nC = \\max_{b \\in \\{0,1,\\dots,B-1\\}} \\left(\\text{number of distinct }\\mathrm{idx}(\\ell)\\text{ among threads with }\\mathrm{bank}(\\ell)=b\\right).\n$$\n\nYour task is to write a complete program that, for each provided test case, computes the integer conflict degree $C$ as defined above.\n\nUse the following test suite, where each test case is the tuple $(B, W, L, T_r, T_c, d, r_0, c_0)$:\n- Case 1 (diagonal with mild conflicts due to padding): $(32, 32, 33, 32, 32, 1, 0, 0)$.\n- Case 2 (worst-case serialization in a single bank without broadcast): $(32, 32, 32, 32, 32, 0, 0, 0)$.\n- Case 3 (diagonal forming a permutation of banks, no conflicts): $(32, 32, 33, 32, 32, 0, 0, 0)$.\n- Case 4 (degenerate diagonal leading to broadcast in one bank): $(32, 32, 64, 1, 64, 0, 0, 0)$.\n- Case 5 (half-warp diagonal with stride causing even-bank mapping, but no conflicts because each bank is used once): $(32, 16, 33, 16, 16, 1, 0, 0)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the same order as the test cases, for example, $[x_1,x_2,x_3,x_4,x_5]$, where each $x_i$ is the integer conflict degree $C$ for the corresponding test case.", "solution": "We model bank conflicts from first principles of the shared memory banking mechanism. The shared memory is divided into $B$ banks, and each element occupies one bank-width unit. Under these assumptions and with base alignment, the bank mapping for an element with linear index $\\mathrm{idx}$ is $\\mathrm{bank} = \\mathrm{idx} \\bmod B$. For a warp of $W$ threads, each thread lane $\\ell \\in \\{0,1,\\dots,W-1\\}$ accesses the element at tile coordinates\n$$\nr(\\ell) = (r_0 + \\ell) \\bmod T_r,\\quad c(\\ell) = (c_0 + \\ell \\cdot d) \\bmod T_c,\n$$\nso the linear index is\n$$\n\\mathrm{idx}(\\ell) = r(\\ell)\\cdot L + c(\\ell),\n$$\nand the bank is\n$$\n\\mathrm{bank}(\\ell) = \\mathrm{idx}(\\ell) \\bmod B.\n$$\n\nBy definition, the conflict degree $C$ for the warp is the maximum, over all banks, of the number of distinct indices requested that map to that bank in the same instruction:\n$$\nC = \\max_{b \\in \\{0,1,\\dots,B-1\\}} \\left|\\left\\{ \\mathrm{idx}(\\ell) \\,\\big|\\, \\mathrm{bank}(\\ell)=b,\\ \\ell \\in \\{0,1,\\dots,W-1\\} \\right\\}\\right|.\n$$\nThis definition naturally captures the broadcast behavior: if multiple threads address the same $\\mathrm{idx}$ within a bank, they contribute only one to the count for that bank.\n\nWe compute $C$ per test case by enumerating $\\ell \\in \\{0,\\dots,W-1\\}$, evaluating $r(\\ell)$, $c(\\ell)$, $\\mathrm{idx}(\\ell)$, and $\\mathrm{bank}(\\ell)$, grouping by bank, counting distinct indices per bank, and taking the maximum of these counts.\n\nWe can also reason analytically for the provided cases:\n\n- Case $1$: $(B,W,L,T_r,T_c,d,r_0,c_0) = (32,32,33,32,32,1,0,0)$. For $\\ell \\in \\{0,\\dots,31\\}$,\n$r(\\ell) = \\ell \\bmod 32 = \\ell$, $c(\\ell) = \\ell \\bmod 32 = \\ell$,\nso $\\mathrm{idx}(\\ell) = \\ell\\cdot 33 + \\ell = \\ell\\cdot 34$. Then\n$\\mathrm{bank}(\\ell) = (\\ell\\cdot 34) \\bmod 32 = \\ell\\cdot (34 \\bmod 32) \\bmod 32 = \\ell\\cdot 2 \\bmod 32$.\nThus only even banks $0,2,4,\\dots,30$ are used. For $\\ell$ and $\\ell+16$, we have\n$\\mathrm{bank}(\\ell+16) = 2(\\ell+16) \\bmod 32 = (2\\ell + 32) \\bmod 32 = 2\\ell \\bmod 32 = \\mathrm{bank}(\\ell)$,\nand $\\mathrm{idx}(\\ell+16) = (\\ell+16)\\cdot 34 \\neq \\ell\\cdot 34$, so each even bank receives exactly two distinct indices. Therefore $C=2$.\n\n- Case $2$: $(32,32,32,32,32,0,0,0)$. Here $r(\\ell)=\\ell$, $c(\\ell)=0$, so $\\mathrm{idx}(\\ell)=\\ell\\cdot 32$. Then\n$\\mathrm{bank}(\\ell) = (\\ell\\cdot 32) \\bmod 32 = 0$,\nso all threads map to bank $0$ with distinct indices (since $\\ell$ differs). Hence $C=32$.\n\n- Case $3$: $(32,32,33,32,32,0,0,0)$. Here $r(\\ell)=\\ell$, $c(\\ell)=0$, so $\\mathrm{idx}(\\ell)=\\ell\\cdot 33$. Then\n$\\mathrm{bank}(\\ell) = (\\ell\\cdot 33) \\bmod 32 = \\ell\\cdot (33 \\bmod 32) \\bmod 32 = \\ell \\bmod 32 = \\ell$.\nEach bank $b\\in\\{0,\\dots,31\\}$ receives exactly one index. Therefore $C=1$.\n\n- Case $4$: $(32,32,64,1,64,0,0,0)$. Here $T_r=1$ so $r(\\ell)=0$ for all $\\ell$, and $d=0$ so $c(\\ell)=0$. Then $\\mathrm{idx}(\\ell)=0$, $\\mathrm{bank}(\\ell)=0$. All threads access the same element, which is broadcast, so the count of distinct indices in bank $0$ is $1$. Therefore $C=1$.\n\n- Case $5$: $(32,16,33,16,16,1,0,0)$. For $\\ell \\in \\{0,\\dots,15\\}$, $r(\\ell)=\\ell \\bmod 16 = \\ell$, $c(\\ell)=\\ell \\bmod 16 = \\ell$, so $\\mathrm{idx}(\\ell)=\\ell\\cdot 34$, and $\\mathrm{bank}(\\ell)=2\\ell \\bmod 32$. The even banks $0,2,\\dots,30$ receive exactly one index each (since $\\ell$ ranges only over $16$ values), hence $C=1$.\n\nThus, the conflict degrees for the five cases are $[2, 32, 1, 1, 1]$. The program implements the enumerations defined above, computes the sets of distinct indices per bank, and outputs these integers in the specified single-line list format.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef conflict_degree(B, W, L, T_r, T_c, d, r0, c0):\n    # Compute bank mapping and conflict degree C as defined in the problem.\n    # Base address aligned; element size equals bank width unit; bank = idx % B.\n    banks = {}  # bank_id -> set of distinct indices (to model broadcast)\n    for ell in range(W):\n        r = (r0 + ell) % T_r\n        c = (c0 + (ell * d)) % T_c\n        idx = r * L + c\n        bank = idx % B\n        if bank not in banks:\n            banks[bank] = set()\n        banks[bank].add(idx)\n    # Conflict degree is the maximum number of distinct indices per bank.\n    if not banks:\n        return 0\n    return max((len(s) for s in banks.values()), default=0)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each test case: (B, W, L, T_r, T_c, d, r0, c0)\n    test_cases = [\n        (32, 32, 33, 32, 32, 1, 0, 0),  # Case 1\n        (32, 32, 32, 32, 32, 0, 0, 0),  # Case 2\n        (32, 32, 33, 32, 32, 0, 0, 0),  # Case 3\n        (32, 32, 64, 1, 64, 0, 0, 0),   # Case 4\n        (32, 16, 33, 16, 16, 1, 0, 0),  # Case 5\n    ]\n\n    results = []\n    for case in test_cases:\n        B, W, L, T_r, T_c, d, r0, c0 = case\n        result = conflict_degree(B, W, L, T_r, T_c, d, r0, c0)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2398488"}]}