{"hands_on_practices": [{"introduction": "The primary motivation for using Multilevel Monte Carlo (MLMC) is its superior computational efficiency compared to standard Monte-Carlo methods. This exercise provides a direct, quantitative comparison to solidify this core concept. By working through a classic theoretical scenario, you will calculate the computational work required by both methods to achieve a target statistical accuracy, revealing the significant cost savings that MLMC can provide.", "problem": "A computational engineer uses Multilevel Monte Carlo (MLMC) and standard Monte Carlo (MC) to estimate the mean of a dimensionless quantity of interest (QoI) arising from a stochastic partial differential equation solved numerically on a hierarchy of meshes. The numerical approximation at level $l \\in \\{0,1,\\dots,L\\}$ uses mesh size $h_l \\propto 2^{-l}$. The absolute discretization bias satisfies $|\\mathbb{E}[Q - Q_L]| \\leq c_b 2^{-\\alpha L}$ with $c_b = 0.5$ and $\\alpha = 2$. The cost of one sample at level $l$ is $C_l = 2^{2l}$ (in arbitrary but identical units for both methods).\n\nFor MLMC, the level differences are $Y_0 = Q_0$ and $Y_l = Q_l - Q_{l-1}$ for $l \\ge 1$, sampled with independent realizations. The level-difference variances are $V_0 = \\operatorname{Var}(Y_0) = 1.0$ and, for $l \\ge 1$, $V_l = \\operatorname{Var}(Y_l) = 2^{-2l}$. For standard MC on the finest level $L$, the QoI variance is $s^2 = \\operatorname{Var}(Q_L) = 1.0$. Assume all samples are independent across all levels and realizations.\n\nBoth methods must produce a two-sided $95$ percent confidence interval for the continuum mean $\\mathbb{E}[Q]$ of total width $W = 1.0 \\times 10^{-2}$. Use the conservative requirement that the discretization bias magnitude is bounded by $W/4$, and that the statistical half-width equals $W/4$. Take the minimal integer $L$ that satisfies the bias requirement and compare the minimal expected computational work of standard MC and MLMC needed to meet the statistical half-width requirement at that $L$.\n\nWhat is the ratio of the minimal expected work of standard MC to that of MLMC under these assumptions? Round your answer to three significant figures.", "solution": "The problem requires a comparison of the minimal computational work for standard Monte Carlo (MC) and Multilevel Monte Carlo (MLMC) methods to estimate the mean of a quantity of interest, $\\mathbb{E}[Q]$, subject to a specified total error tolerance.\n\nFirst, we validate the problem statement.\nThe givens are:\n- Hierarchy of levels $l \\in \\{0, 1, \\dots, L\\}$.\n- Mesh size $h_l \\propto 2^{-l}$.\n- Discretization bias bound: $|\\mathbb{E}[Q - Q_L]| \\le c_b 2^{-\\alpha L}$ with $c_b = 0.5$ and $\\alpha = 2$.\n- Cost per sample at level $l$: $C_l = 2^{2l}$.\n- MLMC differences: $Y_0 = Q_0$, $Y_l = Q_l - Q_{l-1}$ for $l \\ge 1$.\n- MLMC variances: $V_0 = \\operatorname{Var}(Y_0) = 1.0$, $V_l = \\operatorname{Var}(Y_l) = 2^{-2l}$ for $l \\ge 1$.\n- Standard MC variance: $s^2 = \\operatorname{Var}(Q_L) = 1.0$.\n- Total confidence interval width for $\\mathbb{E}[Q]$: $W = 1.0 \\times 10^{-2}$.\n- Bias constraint: $|\\mathbb{E}[Q - Q_L]| \\le W/4$.\n- Statistical half-width constraint: statistical half-width $= W/4$.\n\nThe problem is scientifically grounded, describing a standard application of Monte Carlo methods in computational engineering. It is well-posed, with a clear objective and sufficient data. The language is objective and precise. Therefore, the problem is deemed valid.\n\nThe solution proceeds in four steps:\n1.  Determine the finest resolution level $L$.\n2.  Calculate the minimal work for standard MC, $\\text{Work}_{MC}$.\n3.  Calculate the minimal work for MLMC, $\\text{Work}_{MLMC}$.\n4.  Compute the ratio $\\frac{\\text{Work}_{MC}}{\\text{Work}_{MLMC}}$.\n\nStep 1: Determine the finest level $L$.\nThe level $L$ is chosen to satisfy the discretization bias constraint. We are given the bias is bounded by $c_b 2^{-\\alpha L}$ and this must be less than or equal to $W/4$. We must find the smallest integer $L$ that satisfies this inequality.\n$$c_b 2^{-\\alpha L} \\le \\frac{W}{4}$$\nSubstituting the given values $c_b = 0.5$, $\\alpha = 2$, and $W = 1.0 \\times 10^{-2}$:\n$$0.5 \\times 2^{-2L} \\le \\frac{1.0 \\times 10^{-2}}{4}$$\n$$0.5 \\times (2^2)^{-L} \\le 0.0025$$\n$$0.5 \\times 4^{-L} \\le 0.0025$$\n$$4^{-L} \\le \\frac{0.0025}{0.5} = 0.005$$\n$$4^L \\ge \\frac{1}{0.005} = 200$$\nWe test integer values for $L$: $4^3 = 64$ and $4^4 = 256$. The smallest integer $L$ satisfying $4^L \\ge 200$ is $L=4$.\n\nStep 2: Minimal work for standard MC.\nThe standard MC estimator uses $N_{MC}$ samples at the finest level $L=4$. The estimator is $\\hat{E}_{MC} = \\frac{1}{N_{MC}} \\sum_{i=1}^{N_{MC}} Q_L^{(i)}$. The variance of this estimator is:\n$$\\operatorname{Var}(\\hat{E}_{MC}) = \\frac{\\operatorname{Var}(Q_L)}{N_{MC}} = \\frac{s^2}{N_{MC}}$$\nWe are given $s^2 = 1.0$.\nThe problem states that the total width of a $95\\%$ confidence interval for $\\mathbb{E}[Q]$ should be $W$. This total error is composed of the discretization bias and the statistical error. The problem provides a budget for each: the bias $|\\mathbb{E}[Q - Q_L]|$ is bounded by $W/4$, and the statistical half-width of the confidence interval for $\\mathbb{E}[Q_L]$ is $W/4$.\nA two-sided $95\\%$ confidence interval has a half-width of $z \\sqrt{\\operatorname{Var}(\\text{Estimator})}$, where $z \\approx 1.96$ is the $z$-score. Therefore, the statistical constraint is:\n$$z \\sqrt{\\operatorname{Var}(\\hat{E}_{MC})} = \\frac{W}{4}$$\n$$z \\sqrt{\\frac{s^2}{N_{MC}}} = \\frac{W}{4}$$\nSolving for the required number of samples $N_{MC}$:\n$$N_{MC} = \\frac{s^2 z^2}{(W/4)^2}$$\nThe computational work (cost) for standard MC is the number of samples multiplied by the cost per sample at level $L$:\n$$\\text{Work}_{MC} = N_{MC} \\times C_L = \\frac{s^2 z^2}{(W/4)^2} C_L$$\nWith $L=4$, the cost per sample is $C_4 = 2^{2 \\times 4} = 2^8 = 256$.\n\nStep 3: Minimal work for MLMC.\nThe MLMC estimator is $\\hat{E}_{MLMC} = \\sum_{l=0}^{L} \\frac{1}{N_l} \\sum_{i=1}^{N_l} Y_l^{(i)}$. Its variance is:\n$$\\operatorname{Var}(\\hat{E}_{MLMC}) = \\sum_{l=0}^{L} \\frac{\\operatorname{Var}(Y_l)}{N_l} = \\sum_{l=0}^{L} \\frac{V_l}{N_l}$$\nThe statistical error constraint is the same as for MC:\n$$z \\sqrt{\\operatorname{Var}(\\hat{E}_{MLMC})} = \\frac{W}{4} \\implies \\sum_{l=0}^{L} \\frac{V_l}{N_l} = \\frac{(W/4)^2}{z^2}$$\nThe total work for MLMC is $\\text{Work}_{MLMC} = \\sum_{l=0}^{L} N_l C_l$. We minimize this work subject to the variance constraint using Lagrange multipliers. The optimal number of samples $N_l$ for each level $l$ is proportional to $\\sqrt{V_l / C_l}$.\nThe resulting minimal work is given by the formula:\n$$\\text{Work}_{MLMC} = \\frac{z^2}{(W/4)^2} \\left( \\sum_{l=0}^{L} \\sqrt{V_l C_l} \\right)^2$$\nWe need to calculate the sum $\\sum_{l=0}^{L} \\sqrt{V_l C_l}$ for $L=4$.\nThe given values are $C_l = 2^{2l}$, $V_0 = 1.0$, and $V_l = 2^{-2l}$ for $l \\ge 1$.\nFor level $l=0$:\n$$\\sqrt{V_0 C_0} = \\sqrt{1.0 \\times 2^{2 \\times 0}} = \\sqrt{1.0 \\times 1} = 1$$\nFor levels $l \\ge 1$:\n$$\\sqrt{V_l C_l} = \\sqrt{2^{-2l} \\times 2^{2l}} = \\sqrt{1} = 1$$\nThe sum for $L=4$ is:\n$$\\sum_{l=0}^{4} \\sqrt{V_l C_l} = \\sqrt{V_0 C_0} + \\sum_{l=1}^{4} \\sqrt{V_l C_l} = 1 + (1+1+1+1) = 5$$\nSo, the minimal work for MLMC is:\n$$\\text{Work}_{MLMC} = \\frac{z^2}{(W/4)^2} (5)^2 = \\frac{25 z^2}{(W/4)^2}$$\n\nStep 4: Compute the ratio.\nWe now compute the ratio of the minimal work of standard MC to that of MLMC.\n$$\\frac{\\text{Work}_{MC}}{\\text{Work}_{MLMC}} = \\frac{\\frac{s^2 z^2}{(W/4)^2} C_L}{\\frac{z^2}{(W/4)^2} \\left( \\sum_{l=0}^{L} \\sqrt{V_l C_l} \\right)^2} = \\frac{s^2 C_L}{\\left( \\sum_{l=0}^{L} \\sqrt{V_l C_l} \\right)^2}$$\nNote that the terms involving $z$ and $W$ cancel out.\nSubstituting the known values: $s^2=1.0$, $L=4$, $C_L=C_4=256$, and $\\sum_{l=0}^4 \\sqrt{V_l C_l} = 5$:\n$$\\frac{\\text{Work}_{MC}}{\\text{Work}_{MLMC}} = \\frac{1.0 \\times 256}{5^2} = \\frac{256}{25} = 10.24$$\nThe problem asks to round the answer to three significant figures. The calculated value of 10.24 has four significant figures. Rounding to three significant figures yields 10.2.", "answer": "$$\\boxed{10.2}$$", "id": "2416330"}, {"introduction": "The efficiency of the MLMC method hinges on an optimal allocation of computational effort across the hierarchy of levels. This allocation is determined by the interplay between how quickly the variance of level differences decays ($\\beta$) and how rapidly the cost per sample grows ($\\gamma$). This hands-on coding practice challenges you to implement the MLMC sample allocation formula and observe how the distribution of samples changes in different regimes, including the \"pathological\" case where $\\beta \\approx \\gamma$.", "problem": "You are studying a scalar Quantity of Interest (QoI) estimated via a multilevel Monte Carlo estimator in the setting of uncertainty propagation. For level $l \\in \\{0,1,\\dots,L\\}$, assume the following model for cost, variance, and bias:\n- The cost per sample at level $l$ is $C_l = c_c \\, 2^{\\gamma l}$, with constants $c_c  0$ and $\\gamma  0$.\n- The variance of the level difference estimator at level $l$ is $V_l = c_v \\, 2^{-\\beta l}$, with constants $c_v  0$ and $\\beta  0$.\n- The absolute bias after truncating at level $L$ is $B_L = c_b \\, 2^{-\\alpha L}$, with constants $c_b  0$ and $\\alpha  0$.\n\nYou are given a mean-square error tolerance $\\varepsilon  0$. Define the minimal truncation level $L$ to be the smallest integer $L \\ge 0$ such that the bias constraint $B_L \\le \\varepsilon / \\sqrt{2}$ holds. Subject to the variance constraint $\\sum_{l=0}^{L} V_l / N_l \\le \\varepsilon^2 / 2$, where $N_l$ is the number of independent samples taken at level $l$, determine the integer sample counts $\\{N_0,\\dots,N_L\\}$ that minimize the total expected cost $\\sum_{l=0}^{L} C_l N_l$. The returned sample counts must be the componentwise ceiling of the corresponding real-valued minimizer of this constrained problem.\n\nFocus on the regime where the variance decay rate $\\beta$ is very close to the cost growth rate $\\gamma$. Analyze how this impacts the distribution of samples $\\{N_l\\}$ across levels.\n\nUse the following parameterized test suite. In all test cases, use $c_b = 1.0$, $c_v = 1.0$, $c_c = 1.0$, and $\\alpha = 1.0$.\n- Test A (balanced rates): $\\beta = 1.0$, $\\gamma = 1.0$, $\\varepsilon = 2^{-4}$.\n- Test B (variance decays slightly slower than cost grows): $\\beta = 0.95$, $\\gamma = 1.0$, $\\varepsilon = 2^{-4}$.\n- Test C (variance decays slightly faster than cost grows): $\\beta = 1.05$, $\\gamma = 1.0$, $\\varepsilon = 2^{-4}$.\n- Test D (coarse tolerance boundary case): $\\beta = 1.0$, $\\gamma = 1.0$, $\\varepsilon = 1.5$.\n\nYour program must, for each test case, compute the integer vector $[N_0,\\dots,N_L]$ as specified. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the bracketed comma-separated list of integers for one test case, in the order A, B, C, D. For example, the required format is $[[n_{A,0},\\dots,n_{A,L_A}],[n_{B,0},\\dots,n_{B,L_B}],[n_{C,0},\\dots,n_{C,L_C}],[n_{D,0},\\dots,n_{D,L_D}]]$ with no spaces.", "solution": "The problem is first subjected to a rigorous validation process.\n\nThe givens are explicitly stated:\nA scalar Quantity of Interest (QoI) is estimated.\nThe level is denoted by $l \\in \\{0, 1, \\dots, L\\}$.\nThe cost per sample at level $l$ is $C_l = c_c \\, 2^{\\gamma l}$, with constants $c_c > 0$ and $\\gamma > 0$.\nThe variance of the level difference estimator at level $l$ is $V_l = c_v \\, 2^{-\\beta l}$, with constants $c_v > 0$ and $\\beta > 0$.\nThe absolute bias after truncating at level $L$ is $B_L = c_b \\, 2^{-\\alpha L}$, with constants $c_b > 0$ and $\\alpha > 0$.\nThe mean-square error tolerance is $\\varepsilon > 0$.\nThe truncation level $L$ is the smallest integer $L \\ge 0$ such that the bias constraint $B_L \\le \\varepsilon / \\sqrt{2}$ holds.\nThe variance constraint is $\\sum_{l=0}^{L} V_l / N_l \\le \\varepsilon^2 / 2$, where $N_l$ is the number of samples at level $l$.\nThe objective is to determine integer sample counts $\\{N_0, \\dots, N_L\\}$ that minimize the total cost $\\sum_{l=0}^{L} C_l N_l$.\nThe final sample counts must be the componentwise ceiling of the real-valued minimizer.\nThe specific regime of interest is when $\\beta$ is close to $\\gamma$.\nThe test suite parameters are: $c_b = 1.0$, $c_v = 1.0$, $c_c = 1.0$, $\\alpha = 1.0$.\nTest A: $\\beta = 1.0$, $\\gamma = 1.0$, $\\varepsilon = 2^{-4}$.\nTest B: $\\beta = 0.95$, $\\gamma = 1.0$, $\\varepsilon = 2^{-4}$.\nTest C: $\\beta = 1.05$, $\\gamma = 1.0$, $\\varepsilon = 2^{-4}$.\nTest D: $\\beta = 1.0$, $\\gamma = 1.0$, $\\varepsilon = 1.5$.\n\nThe problem is found to be valid. It is scientifically grounded in the established theory of Multilevel Monte Carlo (MLMC) methods for uncertainty quantification. The problem is well-posed, constituting a solvable convex optimization problem. The language is objective and mathematically precise. There are no contradictions, missing information, or invalid premises. We may therefore proceed with the derivation of the solution.\n\nThe total Mean Square Error (MSE) is bounded by the sum of the squared bias and the total variance. The problem uses a standard splitting of the total MSE tolerance $\\varepsilon^2$ such that $\\text{Bias}^2 \\le \\varepsilon^2/2$ and $\\text{Variance} \\le \\varepsilon^2/2$.\n\nFirst, we determine the minimal truncation level $L$. The bias constraint is $B_L \\le \\varepsilon / \\sqrt{2}$. Substituting the model for $B_L$:\n$$c_b \\, 2^{-\\alpha L} \\le \\frac{\\varepsilon}{\\sqrt{2}}$$\nSolving for $L$, we take the base-$2$ logarithm of both sides:\n$$-\\alpha L \\le \\log_2\\left(\\frac{\\varepsilon}{c_b\\sqrt{2}}\\right)$$\n$$\\alpha L \\ge -\\log_2\\left(\\frac{\\varepsilon}{c_b\\sqrt{2}}\\right) = \\log_2\\left(\\frac{c_b\\sqrt{2}}{\\varepsilon}\\right)$$\n$$L \\ge \\frac{1}{\\alpha} \\log_2\\left(\\frac{c_b\\sqrt{2}}{\\varepsilon}\\right)$$\nSince $L$ must be the smallest non-negative integer satisfying this inequality, we have:\n$$L = \\max\\left(0, \\left\\lceil \\frac{1}{\\alpha} \\log_2\\left(\\frac{c_b\\sqrt{2}}{\\varepsilon}\\right) \\right\\rceil\\right)$$\nThe $\\max(0, \\dots)$ ensures compliance with the condition $L \\ge 0$, which is relevant if the tolerance $\\varepsilon$ is large.\n\nSecond, we determine the optimal number of samples $N_l$ for $l \\in \\{0, \\dots, L\\}$. We must minimize the total cost $\\mathcal{C} = \\sum_{l=0}^{L} C_l N_l$ subject to the variance constraint $\\sum_{l=0}^{L} V_l/N_l \\le \\varepsilon^2/2$. To minimize a linear cost function for a fixed budget, the constraint must be active, i.e., an equality: $\\sum_{l=0}^{L} V_l/N_l = \\varepsilon^2/2$.\nWe use the method of Lagrange multipliers. The Lagrangian $\\mathcal{L}$ is:\n$$\\mathcal{L}(\\{N_l\\}_{l=0}^L, \\lambda) = \\sum_{l=0}^{L} C_l N_l + \\lambda \\left(\\sum_{l=0}^{L} \\frac{V_l}{N_l} - \\frac{\\varepsilon^2}{2}\\right)$$\nDifferentiating with respect to $N_k$ for a generic level $k$ and setting the result to zero gives the optimal allocation for real-valued $N_k$:\n$$\\frac{\\partial \\mathcal{L}}{\\partial N_k} = C_k - \\lambda \\frac{V_k}{N_k^2} = 0 \\implies N_k = \\sqrt{\\lambda} \\sqrt{\\frac{V_k}{C_k}}$$\nTo find the Lagrange multiplier $\\lambda$, we substitute this expression for $N_l$ back into the variance constraint:\n$$\\sum_{l=0}^{L} \\frac{V_l}{\\sqrt{\\lambda} \\sqrt{V_l/C_l}} = \\frac{1}{\\sqrt{\\lambda}} \\sum_{l=0}^{L} \\sqrt{V_l C_l} = \\frac{\\varepsilon^2}{2}$$\nSolving for $\\sqrt{\\lambda}$:\n$$\\sqrt{\\lambda} = \\frac{2}{\\varepsilon^2} \\sum_{l=0}^{L} \\sqrt{V_l C_l}$$\nSubstituting this back into the expression for $N_k$:\n$$N_k = \\left(\\frac{2}{\\varepsilon^2} \\sum_{l=0}^{L} \\sqrt{V_l C_l}\\right) \\sqrt{\\frac{V_k}{C_k}}$$\nThis provides the optimal real-valued sample counts.\n\nThird, we substitute the given models for cost $C_l = c_c 2^{\\gamma l}$ and variance $V_l = c_v 2^{-\\beta l}$:\n$$\\sqrt{V_l C_l} = \\sqrt{(c_v 2^{-\\beta l})(c_c 2^{\\gamma l})} = \\sqrt{c_v c_c} \\, 2^{(\\gamma-\\beta)l/2}$$\n$$\\sqrt{\\frac{V_k}{C_k}} = \\sqrt{\\frac{c_v 2^{-\\beta k}}{c_c 2^{\\gamma k}}} = \\sqrt{\\frac{c_v}{c_c}} \\, 2^{-(\\beta+\\gamma)k/2}$$\nThe sum term becomes a geometric series:\n$$S = \\sum_{l=0}^{L} \\sqrt{V_l C_l} = \\sqrt{c_v c_c} \\sum_{l=0}^{L} \\left(2^{(\\gamma-\\beta)/2}\\right)^l$$\nIf $\\gamma = \\beta$, the ratio is $1$, and the sum is $S = \\sqrt{c_v c_c} (L+1)$.\nIf $\\gamma \\ne \\beta$, let $r = 2^{(\\gamma-\\beta)/2}$. The sum is $S = \\sqrt{c_v c_c} \\frac{r^{L+1}-1}{r-1}$.\nThe real-valued $N_k$ is then:\n$$N_k = \\frac{2 S}{\\varepsilon^2} \\sqrt{\\frac{c_v}{c_c}} \\, 2^{-(\\beta+\\gamma)k/2}$$\nThe problem states that the integer sample counts are found by taking the componentwise ceiling of this real-valued minimizer: $N_k^{\\text{int}} = \\lceil N_k \\rceil$.\n\nThe analysis for $\\beta \\approx \\gamma$ is instructive. The total cost is $\\mathcal{C} = \\sum C_l N_l = \\sum C_l \\sqrt{\\lambda}\\sqrt{V_l/C_l} = \\sqrt{\\lambda} \\sum \\sqrt{C_l V_l} = \\frac{2}{\\varepsilon^2}S^2$. The total cost is proportional to the square of the sum $S$. The expression $\\sqrt{V_l C_l} \\propto 2^{(\\gamma-\\beta)l/2}$ indicates the computational effort per unit of standard deviation reduction at level $l$. This effort is constant across all levels only when $\\gamma = \\beta$. In this case, the sum $S$ and thus the total cost are minimized for a fixed $L$. If $\\gamma \\ne \\beta$, the cost-effectiveness varies with level, increasing the total work required. The distribution of samples $N_k$ always decays exponentially with $k$ due to the term $2^{-(\\beta+\\gamma)k/2}$, as $\\beta > 0$ and $\\gamma > 0$. Most samples are allocated to the coarser, cheaper levels.\n\nWe now compute the results for the given test cases. For all cases, $c_b=1.0, c_v=1.0, c_c=1.0, \\alpha=1.0$.\nThe formula for $L$ simplifies to $L = \\max(0, \\lceil 0.5 - \\log_2(\\varepsilon) \\rceil)$.\nThe formula for $N_k$ simplifies to $N_k = \\frac{2}{\\varepsilon^2} \\left(\\sum_{l=0}^{L} 2^{(\\gamma - \\beta)l/2}\\right) 2^{-(\\beta + \\gamma)k/2}$.\n\nTest A: $\\beta = 1.0, \\gamma = 1.0, \\varepsilon = 2^{-4}$.\n$L = \\max(0, \\lceil 0.5 - (-4) \\rceil) = \\lceil 4.5 \\rceil = 5$.\n$\\gamma = \\beta$, so the sum is $L+1 = 6$.\n$N_k = \\frac{2}{(2^{-4})^2} (6) 2^{-(1.0+1.0)k/2} = 2 \\cdot 2^8 \\cdot 6 \\cdot 2^{-k} = 12 \\cdot 2^{8-k} = 3 \\cdot 2^{10-k}$.\n$N_0=3072, N_1=1536, N_2=768, N_3=384, N_4=192, N_5=96$.\nThese are integers, so ceiling has no effect. Result: $[3072, 1536, 768, 384, 192, 96]$.\n\nTest B: $\\beta = 0.95, \\gamma = 1.0, \\varepsilon = 2^{-4}$.\n$L=5$. $\\gamma - \\beta = 0.05$. $\\beta + \\gamma = 1.95$.\nLet $r = 2^{0.025}$. The sum is $\\frac{r^6-1}{r-1} \\approx 6.2683$.\n$N_k = \\frac{2}{2^{-8}} (6.2683) 2^{-1.95k/2} = 512 \\cdot (6.2683) \\cdot 2^{-0.975k} \\approx 3209.33 \\cdot 2^{-0.975k}$.\n$N_0 \\approx 3209.33 \\implies \\lceil N_0 \\rceil = 3210$.\n$N_1 \\approx 1632.74 \\implies \\lceil N_1 \\rceil = 1633$.\n$N_2 \\approx 830.61 \\implies \\lceil N_2 \\rceil = 831$.\n$N_3 \\approx 422.58 \\implies \\lceil N_3 \\rceil = 423$.\n$N_4 \\approx 214.97 \\implies \\lceil N_4 \\rceil = 215$.\n$N_5 \\approx 109.37 \\implies \\lceil N_5 \\rceil = 110$.\nResult: $[3210, 1633, 831, 423, 215, 110]$.\n\nTest C: $\\beta = 1.05, \\gamma = 1.0, \\varepsilon = 2^{-4}$.\n$L=5$. $\\gamma - \\beta = -0.05$. $\\beta + \\gamma = 2.05$.\nLet $r = 2^{-0.025}$. The sum is $\\frac{r^6-1}{r-1} \\approx 5.7480$.\n$N_k = \\frac{2}{2^{-8}} (5.7480) 2^{-2.05k/2} = 512 \\cdot (5.7480) \\cdot 2^{-1.025k} \\approx 2942.99 \\cdot 2^{-1.025k}$.\n$N_0 \\approx 2942.99 \\implies \\lceil N_0 \\rceil = 2943$.\n$N_1 \\approx 1445.56 \\implies \\lceil N_1 \\rceil = 1446$.\n$N_2 \\approx 709.97 \\implies \\lceil N_2 \\rceil = 710$.\n$N_3 \\approx 348.72 \\implies \\lceil N_3 \\rceil = 349$.\n$N_4 \\approx 171.27 \\implies \\lceil N_4 \\rceil = 172$.\n$N_5 \\approx 84.11 \\implies \\lceil N_5 \\rceil = 85$.\nResult: $[2943, 1446, 710, 349, 172, 85]$.\n\nTest D: $\\beta = 1.0, \\gamma = 1.0, \\varepsilon = 1.5$.\n$L = \\max(0, \\lceil 0.5 - \\log_2(1.5) \\rceil) = \\max(0, \\lceil 0.5 - 0.585 \\rceil) = \\max(0, \\lceil -0.085 \\rceil) = \\max(0, 0) = 0$.\nThe problem reduces to a single level Monte Carlo ($l=0$). The sum is over a single term.\n$N_0 = \\frac{2 V_0}{\\varepsilon^2} = \\frac{2 \\cdot (1.0 \\cdot 2^{-1.0 \\cdot 0})}{1.5^2} = \\frac{2}{2.25} = \\frac{8}{9} \\approx 0.888...$\n$N_0^{\\text{int}} = \\lceil 8/9 \\rceil = 1$.\nResult: $[1]$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes optimal integer sample counts for Multilevel Monte Carlo (MLMC)\n    based on given cost, variance, and bias models.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'name': 'A', 'beta': 1.0, 'gamma': 1.0, 'eps': 2**(-4)},\n        {'name': 'B', 'beta': 0.95, 'gamma': 1.0, 'eps': 2**(-4)},\n        {'name': 'C', 'beta': 1.05, 'gamma': 1.0, 'eps': 2**(-4)},\n        {'name': 'D', 'beta': 1.0, 'gamma': 1.0, 'eps': 1.5},\n    ]\n\n    # Common parameters for all test cases\n    cb = 1.0\n    cv = 1.0\n    cc = 1.0\n    alpha = 1.0\n    \n    results = []\n    \n    for case in test_cases:\n        beta = case['beta']\n        gamma = case['gamma']\n        eps = case['eps']\n\n        # Step 1: Determine the truncation level L\n        # L must be the smallest non-negative integer satisfying the bias constraint.\n        # This formula derives from: cb * 2**(-alpha*L) = eps / sqrt(2)\n        # L = (1/alpha) * log2(cb*sqrt(2)/eps)\n        log_arg = cb * np.sqrt(2) / eps\n        if log_arg = 1:\n          # If the argument is = 1, log2 is = 0.\n          # The smallest non-negative integer L is 0.\n          L = 0\n        else:\n          L = int(np.ceil((1.0 / alpha) * np.log2(log_arg)))\n        \n        # In this specific problem, the prompt asks for the smallest integer L = 0.\n        # A more direct implementation is max(0, ceil(...))\n        L_val = (1.0 / alpha) * np.log2(cb * np.sqrt(2) / eps)\n        L = max(0, int(np.ceil(L_val)))\n        \n        levels = np.arange(L + 1)\n        \n        # Step 2: Calculate the sum term for the optimal N_l formula\n        # sum_term = sum_{l=0 to L} sqrt(V_l * C_l)\n        # V_l*C_l = (cv * 2**(-beta*l)) * (cc * 2**(gamma*l)) = cv*cc*2**((gamma-beta)*l)\n        # sqrt(V_l*C_l) = sqrt(cv*cc) * 2**((gamma-beta)*l/2)\n        sum_base = np.sqrt(cv * cc)\n        \n        if gamma == beta:\n            sum_val = sum_base * (L + 1)\n        else:\n            r = 2**((gamma - beta) / 2.0)\n            sum_val = sum_base * (r**(L + 1) - 1) / (r - 1)\n\n        # Step 3: Calculate the integer sample counts N_k for k=0,...,L\n        # N_k = (2/eps^2) * sum_val * sqrt(V_k/C_k)\n        # sqrt(V_k/C_k) = sqrt(cv/cc) * 2**(-(beta+gamma)*k/2)\n        \n        Nk_list = []\n        for k in levels:\n            sqrt_vk_ck_ratio = np.sqrt(cv / cc) * (2**(- (beta + gamma) * k / 2.0))\n            Nk_real = (2.0 / eps**2) * sum_val * sqrt_vk_ck_ratio\n            Nk_int = int(np.ceil(Nk_real))\n            Nk_list.append(Nk_int)\n        \n        results.append(Nk_list)\n\n    # Final print statement in the exact required format.\n    # Format: [[n_A_0,...,n_A_L_A],[n_B_0,...,n_B_L_B],...] with no spaces.\n    results_str_parts = []\n    for res_list in results:\n        part = f\"[{','.join(map(str, res_list))}]\"\n        results_str_parts.append(part)\n    \n    final_output = f\"[{','.join(results_str_parts)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "2416356"}, {"introduction": "With the theoretical foundations of MLMC established, this final practice problem brings all the concepts together in a realistic engineering context. You will build a model to estimate signal delay in a VLSI interconnect, where manufacturing variability introduces uncertainty in the physical dimensions of the wire. This comprehensive exercise involves creating a physical model, discretizing it for numerical simulation, and applying the full MLMC estimator to quantify the impact of uncertainty on performance, demonstrating the method's power in a practical setting.", "problem": "You are to implement a complete, runnable program that estimates the expected signal delay in a Very Large Scale Integration (VLSI) interconnect line subject to manufacturing variability using Multilevel Monte Carlo (MLMC). The estimation target is the expected time for the far-end node voltage to reach a specified threshold fraction of a unit step input, expressed in seconds, rounded to floating-point precision.\n\nThe physical model shall be based on the following fundamental principles and core definitions:\n- Ohmâ€™s law in resistive media: for a uniform conductor of length $L$ and cross-sectional area $A$, the resistance is $R = \\rho \\, L / A$, where $\\rho$ is the electrical resistivity.\n- Parallel-plate capacitance: for a plate of area $A$ separated by a dielectric of thickness $h$ and permittivity $\\varepsilon$, the capacitance is $C = \\varepsilon \\, A / h$. A common engineering correction for edge (fringing) fields is to use an effective plate width $W_{\\text{eff}} = W + 2h/\\pi$, where $W$ is the physical width.\n- First-order system step response: a single resistor-capacitor network obeys the linear differential equation $C \\, \\mathrm{d}v/\\mathrm{d}t + (1/R)\\,v = (1/R)\\,u(t)$ for unit step input $u(t)$, and the time to reach a fixed threshold (e.g., $v^\\star = 0.5$) is determined by solving for $t$ in the step response. In a distributed interconnect, the line is modeled as a resistive-capacitive ladder obtained by spatial discretization; the governing semi-discrete linear differential system can be integrated in time using unconditionally stable backward Euler.\n\nMathematical and numerical model:\n- Geometry and materials: an interconnect of length $L$ and rectangular cross section of width $W$ and thickness $T$ runs over a ground plane separated by a dielectric of thickness $h$ and permittivity $\\varepsilon = \\varepsilon_{0}\\varepsilon_{r}$, where $\\varepsilon_{0}$ is the vacuum permittivity and $\\varepsilon_{r}$ is the relative permittivity. Electrical resistivity is $\\rho$.\n- Random inputs: due to manufacturing variability, width $W$ and thickness $T$ are independent, normally distributed with means $\\mu_{W}$ and $\\mu_{T}$ and standard deviations $\\sigma_{W}$ and $\\sigma_{T}$, truncated below at strictly positive lower bounds $w_{\\min}$ and $t_{\\min}$ to enforce physical realism. That is, draw $W \\sim \\max(\\mathcal{N}(\\mu_{W}, \\sigma_{W}^{2}), w_{\\min})$ and $T \\sim \\max(\\mathcal{N}(\\mu_{T}, \\sigma_{T}^{2}), t_{\\min})$ by rejection sampling.\n- Spatial discretization hierarchy: construct $L+1$ levels of uniform spatial grids with $N_{\\ell} = N_{0}\\, 2^{\\ell}$ segments for level $\\ell \\in \\{0,1,\\dots,L\\}$, with $N_{0} \\in \\mathbb{N}$. Let $\\Delta x_{\\ell} = L/N_{\\ell}$. On each level, represent the interconnect as a resistive-capacitive ladder: each segment has resistance $r_{\\text{seg}} = \\rho \\, \\Delta x_{\\ell} /(W T)$, each interior node has capacitance to ground $c_{\\text{node}} = \\varepsilon \\, W_{\\text{eff}} \\, \\Delta x_{\\ell} / h$ with $W_{\\text{eff}} = W + 2h/\\pi$.\n- Temporal integration: apply backward Euler with time step $\\Delta t_{\\ell}$ to the semi-discrete linear system for node voltages. The driver node at the input end is a Dirichlet boundary set to a unit step of amplitude $1$; the far end is open (no load). Use a fixed threshold $v^{\\star} = 0.5$ to define the delay observable $P_{\\ell}(W,T)$ as the first time $t$ such that the far-end node voltage reaches $v^{\\star}$, linearly interpolated between time steps. Choose the simulation horizon $T_{\\max}$ proportional to the diffusive time scale $r' c' L^{2}$ with $r' = \\rho/(W T)$ and $c' = \\varepsilon W_{\\text{eff}}/h$, namely $T_{\\max} = \\alpha \\, r' c' L^{2}$ for a fixed factor $\\alpha0$. Use a level-dependent number of time steps $M_{\\ell} = M_{0}\\,2^{\\ell}$ and $\\Delta t_{\\ell} = T_{\\max}/M_{\\ell}$.\n- Quantity of interest: for any realization $(W,T)$ and level $\\ell$, compute $Y_{\\ell}(W,T) := P_{\\ell}(W,T)$, the estimated $v^{\\star}$-crossing time in seconds.\n\nMultilevel Monte Carlo (MLMC) estimator:\n- Use the telescoping sum representation for the expectation at the finest level $L$: $\\mathbb{E}[Y_{L}] = \\mathbb{E}[Y_{0}] + \\sum_{\\ell=1}^{L} \\mathbb{E}[Y_{\\ell} - Y_{\\ell-1}]$, and estimate each expectation by Monte Carlo sample averages with level-wise couplings that use shared random inputs $(W,T)$ for $Y_{\\ell}$ and $Y_{\\ell-1}$ at the same sample index.\n- Variance-driven sample allocation: let $V_{\\ell}$ denote the variance of $Y_{0}$ for $\\ell=0$ and of $(Y_{\\ell}-Y_{\\ell-1})$ for $\\ell \\ge 1$, and $C_{\\ell}$ the work per sample at level $\\ell$ (proportional to $N_{\\ell} M_{\\ell}$). Given a root-mean-square error tolerance $\\varepsilon$, split it into variance and bias targets. Use pilot estimates of $V_{\\ell}$ and $C_{\\ell}$ to allocate the number of samples $N_{\\ell}$ per level according to the standard asymptotically optimal rule $N_{\\ell} \\propto \\sqrt{V_{\\ell}/C_{\\ell}}$, normalized to achieve the variance target. Control the bias by limiting the maximum level $L$ and by ensuring that the mean of $(Y_{L}-Y_{L-1})$ is sufficiently small relative to $\\varepsilon$.\n\nImplementation requirements:\n- Your program must implement the above model and estimator. It must use a fixed pseudorandom number generator seed to ensure reproducibility across runs.\n- For numerical robustness, ensure $W \\ge w_{\\min}$ and $T \\ge t_{\\min}$ by rejection sampling.\n- The final answers must be expressed in seconds, as floating-point numbers.\n\nTest suite:\nImplement your program to run the MLMC estimator for the following three cases. In all cases, use $v^{\\star} = 0.5$, $\\alpha = 10$, $N_{0} = 8$, $M_{0} = 80$, $L_{\\max} = 3$, $w_{\\min} = \\mu_{W}/3$, and $t_{\\min} = \\mu_{T}/3$. The vacuum permittivity is $\\varepsilon_{0} = 8.8541878128 \\times 10^{-12}\\ \\text{F/m}$. The final output must be a single line containing a Python-style list of the three expected delays in seconds, in the order of the cases below, without any additional text.\n\n- Case A (happy path):\n  - $L = 5.0 \\times 10^{-4}\\ \\text{m}$, $h = 2.0 \\times 10^{-7}\\ \\text{m}$, $\\rho = 1.68 \\times 10^{-8}\\ \\Omega\\cdot\\text{m}$, $\\varepsilon_{r} = 3.9$,\n  - $\\mu_{W} = 1.2 \\times 10^{-7}\\ \\text{m}$, $\\sigma_{W} = 1.2 \\times 10^{-8}\\ \\text{m}$,\n  - $\\mu_{T} = 6.0 \\times 10^{-8}\\ \\text{m}$, $\\sigma_{T} = 8.0 \\times 10^{-9}\\ \\text{m}$,\n  - MLMC tolerance $\\varepsilon = 1.0 \\times 10^{-12}\\ \\text{s}$.\n\n- Case B (thin metal, larger delay; edge case for higher resistance):\n  - $L = 5.0 \\times 10^{-4}\\ \\text{m}$, $h = 2.0 \\times 10^{-7}\\ \\text{m}$, $\\rho = 1.68 \\times 10^{-8}\\ \\Omega\\cdot\\text{m}$, $\\varepsilon_{r} = 3.9$,\n  - $\\mu_{W} = 1.2 \\times 10^{-7}\\ \\text{m}$, $\\sigma_{W} = 1.2 \\times 10^{-8}\\ \\text{m}$,\n  - $\\mu_{T} = 4.0 \\times 10^{-8}\\ \\text{m}$, $\\sigma_{T} = 6.0 \\times 10^{-9}\\ \\text{m}$,\n  - MLMC tolerance $\\varepsilon = 1.5 \\times 10^{-12}\\ \\text{s}$.\n\n- Case C (low-$k$ dielectric, smaller delay; edge case for reduced capacitance):\n  - $L = 5.0 \\times 10^{-4}\\ \\text{m}$, $h = 2.0 \\times 10^{-7}\\ \\text{m}$, $\\rho = 1.68 \\times 10^{-8}\\ \\Omega\\cdot\\text{m}$, $\\varepsilon_{r} = 2.5$,\n  - $\\mu_{W} = 1.2 \\times 10^{-7}\\ \\text{m}$, $\\sigma_{W} = 1.2 \\times 10^{-8}\\ \\text{m}$,\n  - $\\mu_{T} = 6.0 \\times 10^{-8}\\ \\text{m}$, $\\sigma_{T} = 8.0 \\times 10^{-9}\\ \\text{m}$,\n  - MLMC tolerance $\\varepsilon = 1.0 \\times 10^{-12}\\ \\text{s}$.\n\nOutput format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order [Case A result, Case B result, Case C result], for example, \"[resultA,resultB,resultC]\". Each result must be a single floating-point number in seconds.\n\nAll angles, if any, must be in radians. There are no percentages in this problem; any fractional quantities must be expressed as decimals.\n\nYour program must be completely self-contained, must not require any user input, and must not access files or the network. It must adhere to the specified execution environment.", "solution": "The problem requires the estimation of the expected signal propagation delay in a VLSI interconnect using the Multilevel Monte Carlo (MLMC) method. The interconnect's geometric parameters, width $W$ and thickness $T$, are subject to manufacturing variability and are modeled as random variables.\n\nThe solution is developed by first establishing the physical and numerical model for a single realization of the interconnect, and then embedding this model within the MLMC statistical framework.\n\n### 1. Physical and Numerical Model\n\nAn interconnect of length $L$, width $W$, and thickness $T$ over a ground plane is modeled as a distributed resistive-capacitive (RC) line. For the purpose of numerical simulation, this continuous line is discretized into a ladder network of $S_{\\ell}$ segments for a given refinement level $\\ell$.\n\n**1.1. RC Ladder Parameters**\n\nFor a level $\\ell$, the number of uniform spatial segments is $S_{\\ell} = N_{0} 2^{\\ell}$, where $N_{0}$ is the base number of segments. The length of each segment is $\\Delta x_{\\ell} = L / S_{\\ell}$.\n\n- **Resistance per segment ($r_{\\text{seg}}$)**: Based on Ohm's law, the resistance of a single segment is given by:\n$$r_{\\text{seg}} = \\frac{\\rho \\Delta x_{\\ell}}{W T}$$\nwhere $\\rho$ is the electrical resistivity, and $W$ and $T$ are the random width and thickness for a given sample.\n\n- **Capacitance per node ($c_{\\text{node}}$)**: The capacitance to the ground plane is modeled using the parallel-plate formula with a fringe-field correction. The total capacitance of a segment is attributed to its nodes. We adopt the standard $\\Pi$-model, where a segment's capacitance is split equally between its two end nodes. An interior node of the ladder network thus accumulates capacitance from two adjacent segments, while an end node receives capacitance from only one.\nThe effective width $W_{\\text{eff}}$ for capacitance calculation is $W_{\\text{eff}} = W + 2h/\\pi$, where $h$ is the dielectric thickness. The capacitance for a length $\\Delta x_{\\ell}$ is:\n$$c_{\\text{node}} = \\frac{\\varepsilon W_{\\text{eff}} \\Delta x_{\\ell}}{h}$$\nwhere $\\varepsilon = \\varepsilon_{0}\\varepsilon_{r}$ is the dielectric permittivity.\nAccording to the $\\Pi$-model, each interior node ($i=1, \\dots, S_{\\ell}-1$) has a capacitance $C_i = c_{\\text{node}}$, and the far-end node ($i=S_{\\ell}$) has a capacitance $C_{S_{\\ell}} = c_{\\text{node}}/2$.\n\n**1.2. Semi-Discrete System of Equations**\n\nThe RC ladder consists of $S_{\\ell}$ nodes with unknown voltages, labeled $v_1, \\dots, v_{S_{\\ell}}$. The input node $v_0$ is driven by a unit step voltage source, $v_0(t)=1$ for $t \\ge 0$. Applying Kirchhoff's current law at each node $i$ yields a system of first-order ordinary differential equations (ODEs):\n- For an interior node $i \\in \\{1, \\dots, S_{\\ell}-1\\}$:\n$$C_i \\frac{\\mathrm{d}v_i}{\\mathrm{d}t} = \\frac{v_{i-1} - v_i}{r_{\\text{seg}}} - \\frac{v_i - v_{i+1}}{r_{\\text{seg}}}$$\n- For the open-circuited far-end node $i = S_{\\ell}$:\n$$C_{S_{\\ell}} \\frac{\\mathrm{d}v_{S_{\\ell}}}{\\mathrm{d}t} = \\frac{v_{S_{\\ell}-1} - v_{S_{\\ell}}}{r_{\\text{seg}}}$$\n\nThis system can be written in matrix form: $\\mathbf{C} \\frac{\\mathrm{d}\\mathbf{v}}{\\mathrm{d}t} = -\\mathbf{K}\\mathbf{v} + \\mathbf{b}$, where $\\mathbf{v} = [v_1, \\dots, v_{S_{\\ell}}]^T$, $\\mathbf{C}$ is a diagonal matrix of node capacitances, $\\mathbf{K}$ is the symmetric positive definite conductance matrix, and $\\mathbf{b}$ is the source vector from the input $v_0$.\n\n**1.3. Temporal Integration**\n\nThe system of ODEs is integrated in time using the backward Euler method, which is unconditionally stable and thus suitable for potentially stiff RC networks. The time domain is discretized into $M_{\\ell} = M_{0} 2^{\\ell}$ steps of size $\\Delta t_{\\ell} = T_{\\max} / M_{\\ell}$. The simulation horizon $T_{\\max} = \\alpha r' c' L^2$ is chosen to be sufficiently long to observe the delay, where $r' = \\rho/(WT)$ and $c' = \\varepsilon W_{\\text{eff}}/h$.\n\nThe backward Euler update from time step $k$ to $k+1$ is given by:\n$$\\mathbf{C} \\frac{\\mathbf{v}^{k+1} - \\mathbf{v}^k}{\\Delta t_{\\ell}} = -\\mathbf{K}\\mathbf{v}^{k+1} + \\mathbf{b}$$\nRearranging gives a linear system to be solved for $\\mathbf{v}^{k+1}$ at each time step:\n$$(\\mathbf{C} + \\Delta t_{\\ell} \\mathbf{K}) \\mathbf{v}^{k+1} = \\mathbf{C}\\mathbf{v}^k + \\Delta t_{\\ell} \\mathbf{b}$$\nStarting from the initial condition $\\mathbf{v}^0 = \\mathbf{0}$, we iterate this equation. The matrix $\\mathbf{A}_{\\text{BE}} = \\mathbf{C} + \\Delta t_{\\ell} \\mathbf{K}$ is tridiagonal, so the system can be solved efficiently in $O(S_{\\ell})$ operations.\n\nThe quantity of interest, $P_{\\ell}(W,T)$, is the time at which the far-end node voltage $v_{S_{\\ell}}(t)$ first reaches the threshold $v^{\\star}=0.5$. If $v_{S_{\\ell}}$ crosses the threshold between time steps $k$ and $k+1$, the delay is computed by linear interpolation.\n\n### 2. Multilevel Monte Carlo (MLMC) Estimator\n\nThe goal is to compute $\\mathbb{E}[Y_L]$, where $Y_\\ell = P_\\ell(W,T)$ is the delay computed at level $\\ell$ and $L=L_{\\max}$ is the finest level. MLMC is based on the telescoping sum:\n$$\\mathbb{E}[Y_L] = \\mathbb{E}[Y_0] + \\sum_{\\ell=1}^{L} \\mathbb{E}[Y_\\ell - Y_{\\ell-1}]$$\nThe MLMC estimator $\\hat{Y}_{L}^{\\text{MLMC}}$ for $\\mathbb{E}[Y_L]$ is:\n$$\\hat{Y}_{L}^{\\text{MLMC}} = \\frac{1}{K_0}\\sum_{i=1}^{K_0} Y_0^{(i)} + \\sum_{\\ell=1}^{L} \\frac{1}{K_\\ell}\\sum_{i=1}^{K_\\ell} (Y_\\ell^{(i)} - Y_{\\ell-1}^{(i)})$$\nHere, $K_\\ell$ is the number of Monte Carlo samples at level $\\ell$. The crucial aspect is the coupling: for each sample $i$ in the sum for level $\\ell \\ge 1$, the same random inputs $(W^{(i)}, T^{(i)})$ are used to compute both $Y_\\ell^{(i)}$ and $Y_{\\ell-1}^{(i)}$. This ensures that the variance of the difference, $V_\\ell = \\text{Var}[Y_\\ell - Y_{\\ell-1}]$, decreases as the level $\\ell$ increases.\n\n**2.1. Optimal Sample Allocation**\n\nThe total Mean Squared Error (MSE) of the estimator is $\\text{MSE} = \\text{Var}[\\hat{Y}_{L}^{\\text{MLMC}}] + (\\text{Bias})^2 \\le \\varepsilon^2$, where $\\varepsilon$ is the desired root-mean-square error. The variance is $\\text{Var}[\\hat{Y}_{L}^{\\text{MLMC}}] = \\sum_{\\ell=0}^{L} V_\\ell/K_\\ell$, with $V_0 = \\text{Var}[Y_0]$. The bias is due to the finite discretization at level $L$.\n\nWe allocate half of the error budget to variance, $\\sum_{\\ell=0}^{L} V_\\ell/K_\\ell \\le \\varepsilon^2/2$. The number of samples $K_\\ell$ at each level is chosen to minimize the total computational cost, $\\sum_{\\ell=0}^{L} K_\\ell C_\\ell$, where $C_\\ell \\propto S_\\ell M_\\ell$ is the cost per sample at level $\\ell$. The optimal number of samples is given by:\n$$K_\\ell = \\left\\lceil \\frac{2}{\\varepsilon^2} \\left(\\sum_{k=0}^{L} \\sqrt{V_k C_k}\\right) \\sqrt{\\frac{V_\\ell}{C_\\ell}} \\right\\rceil$$\n\n**2.2. Algorithm**\n\nThe practical algorithm proceeds as follows:\n1.  **Pilot Stage**: A small number of samples ($K_{\\text{pilot}}$) are simulated on all levels $\\ell=0, \\dots, L$. This is used to obtain initial estimates for the means $\\mathbb{E}[Y_\\ell - Y_{\\ell-1}]$ and variances $V_\\ell$. The costs $C_\\ell = S_\\ell M_\\ell$ are known a priori.\n2.  **Sample Allocation**: Using the pilot estimates for $V_\\ell$ and the known costs $C_\\ell$, the optimal number of samples $K_\\ell$ for each level is calculated using the formula above.\n3.  **Main Simulation**: For each level $\\ell$, additional simulations are run until the total number of samples reaches $K_\\ell$. The sample means of $Y_0$ and the differences $(Y_\\ell - Y_{\\ell-1})$ are computed.\n4.  **Final Estimate**: The final MLMC estimate is obtained by summing the sample means calculated in the previous step, as per the telescoping sum formula.\n\nThe random inputs $W$ and $T$ are drawn from truncated normal distributions using rejection sampling to ensure they are physically realistic ($W \\ge w_{\\min}, T \\ge t_{\\min}$).", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import solve_banded\nfrom math import pi\n\ndef solve():\n    \"\"\"\n    Main function to solve the VLSI delay estimation problem for all test cases.\n    \"\"\"\n    # Physical and numerical constants\n    EPS0 = 8.8541878128e-12\n    V_STAR = 0.5\n    ALPHA = 10.0\n    N0 = 8\n    M0 = 80\n    L_MAX = 3\n    \n    test_cases = [\n        # Case A\n        {\n            'L': 5.0e-4, 'h': 2.0e-7, 'rho': 1.68e-8, 'eps_r': 3.9,\n            'mu_W': 1.2e-7, 'sigma_W': 1.2e-8,\n            'mu_T': 6.0e-8, 'sigma_T': 8.0e-9,\n            'mlmc_tol': 1.0e-12\n        },\n        # Case B\n        {\n            'L': 5.0e-4, 'h': 2.0e-7, 'rho': 1.68e-8, 'eps_r': 3.9,\n            'mu_W': 1.2e-7, 'sigma_W': 1.2e-8,\n            'mu_T': 4.0e-8, 'sigma_T': 6.0e-9,\n            'mlmc_tol': 1.5e-12\n        },\n        # Case C\n        {\n            'L': 5.0e-4, 'h': 2.0e-7, 'rho': 1.68e-8, 'eps_r': 2.5,\n            'mu_W': 1.2e-7, 'sigma_W': 1.2e-8,\n            'mu_T': 6.0e-8, 'sigma_T': 8.0e-9,\n            'mlmc_tol': 1.0e-12\n        }\n    ]\n\n    results = []\n    for params in test_cases:\n        # Add constants to the parameter dictionary\n        params.update({'v_star': V_STAR, 'alpha': ALPHA, 'N0': N0, 'M0': M0, 'eps0': EPS0})\n        result = mlmc_estimator(params, L_MAX)\n        results.append(result)\n\n    # Print results in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef generate_wt_sample(params, rng):\n    \"\"\"\n    Generates a sample of (W, T) using rejection sampling from a truncated normal distribution.\n    \"\"\"\n    mu_W, sigma_W = params['mu_W'], params['sigma_W']\n    w_min = mu_W / 3.0\n    while True:\n        W = rng.normal(mu_W, sigma_W)\n        if W = w_min:\n            break\n            \n    mu_T, sigma_T = params['mu_T'], params['sigma_T']\n    t_min = mu_T / 3.0\n    while True:\n        T = rng.normal(mu_T, sigma_T)\n        if T = t_min:\n            break\n            \n    return W, T\n\ndef compute_delay(params, W, T, level):\n    \"\"\"\n    Computes the signal delay for a single sample (W, T) at a given discretization level.\n    \"\"\"\n    S_l = params['N0'] * (2**level)\n    M_l = params['M0'] * (2**level)\n    \n    eps = params['eps0'] * params['eps_r']\n    W_eff = W + 2 * params['h'] / pi\n    \n    r_prime = params['rho'] / (W * T)\n    c_prime = eps * W_eff / params['h']\n    \n    delta_x = params['L'] / S_l\n    r_seg = r_prime * delta_x\n    c_node = c_prime * delta_x\n    \n    T_max = params['alpha'] * r_prime * c_prime * params['L']**2\n    delta_t = T_max / M_l\n    \n    if r_seg == 0 or c_node == 0: return 0.0\n    g = 1.0 / r_seg\n\n    # Coefficients for the backward Euler tridiagonal system\n    k1 = delta_t * g / c_node\n    k2 = delta_t * g / (c_node / 2.0)\n    \n    # SciPy's banded solver format: ab[u+i-j, j] = A[i,j]\n    # For u=1, l=1: ab[0,:]=super, ab[1,:]=main, ab[2,:]=sub\n    ab = np.zeros((3, S_l))\n    # Super-diagonal (A[i, i+1]) - ab[0, i+1]\n    ab[0, 1:] = -k1\n    # Main-diagonal (A[i, i]) - ab[1, i]\n    ab[1, 0:S_l-1] = 1.0 + 2.0 * k1\n    ab[1, S_l-1] = 1.0 + k2\n    # Sub-diagonal (A[i+1, i]) - ab[2, i]\n    ab[2, 0:S_l-1] = -k1 # Correcting the last element from the copy-pasted error\n    if S_l > 1:\n        ab[2, S_l-2] = -k2\n\n    # Time stepping\n    v = np.zeros(S_l)\n    \n    for k in range(int(M_l)):\n        v_old = v.copy()\n        \n        rhs = v_old\n        rhs[0] += k1 # Add source term\n        \n        v = solve_banded((1, 1), ab, rhs, check_finite=False)\n        \n        if v[-1] = params['v_star']:\n            v_far_end_old = v_old[-1]\n            v_far_end_new = v[-1]\n            \n            t_old = k * delta_t\n            t_new = (k + 1) * delta_t\n\n            if abs(v_far_end_new - v_far_end_old)  1e-15: return t_new\n            \n            delay = t_old + (t_new - t_old) * (params['v_star'] - v_far_end_old) / (v_far_end_new - v_far_end_old)\n            return delay\n\n    return T_max # Should not be reached with adequate alpha\n\ndef mlmc_estimator(params, L_max):\n    \"\"\"\n    Implements the Multilevel Monte Carlo estimator for a given test case.\n    \"\"\"\n    K_pilot = 200\n    eps = params['mlmc_tol']\n    rng = np.random.default_rng(seed=12345)\n\n    # --- Pilot Stage ---\n    means = [0.0] * (L_max + 1)\n    variances = [0.0] * (L_max + 1)\n    costs = [0.0] * (L_max + 1)\n\n    Y_samples = [np.zeros(K_pilot) for _ in range(L_max + 1)]\n    for i in range(K_pilot):\n        W, T = generate_wt_sample(params, rng)\n        # Generate samples for all levels using the same (W, T)\n        for l in range(L_max + 1):\n            Y_samples[l][i] = compute_delay(params, W, T, l)\n\n    for l in range(L_max + 1):\n        if l == 0:\n            diff_samples = Y_samples[0]\n        else:\n            diff_samples = Y_samples[l] - Y_samples[l-1]\n        \n        means[l] = np.mean(diff_samples)\n        variances[l] = np.var(diff_samples, ddof=1) # Use sample variance\n        S_l = params['N0'] * (2**l)\n        M_l = params['M0'] * (2**l)\n        costs[l] = float(S_l * M_l)\n\n    # --- Sample Allocation ---\n    optimal_K = [0] * (L_max + 1)\n    # Use variance/2 for error budget as per problem desc.\n    sum_sqrt_V_C = sum(np.sqrt(variances[l] * costs[l]) for l in range(L_max + 1) if variances[l] > 0)\n    \n    for l in range(L_max + 1):\n        if variances[l] > 1e-30:\n            # The formula in the solution text had eps^2, but standard MLMC uses eps^2/2 for variance budget\n            num = (2.0 / eps**2) * sum_sqrt_V_C * np.sqrt(variances[l] / costs[l])\n            optimal_K[l] = int(np.ceil(num))\n        else:\n            optimal_K[l] = 0\n    \n    # --- Main Simulation ---\n    final_estimate = 0.0\n    for l in range(L_max + 1):\n        K_l = optimal_K[l]\n        if K_l  2: # Ensure at least a few samples if variance is non-zero\n             K_l = 2 if variances[l] > 1e-30 else 0\n        \n        if K_l = K_pilot:\n             if l == 0:\n                 sum_of_diffs = np.sum(Y_samples[l][:K_l])\n             else:\n                 sum_of_diffs = np.sum(Y_samples[l][:K_l] - Y_samples[l-1][:K_l])\n             if K_l > 0:\n                final_estimate += sum_of_diffs / K_l\n             else: # if K_l is 0 because variance was ~0\n                final_estimate += means[l]\n             continue\n\n        # If more samples are needed than pilot\n        sum_of_diffs = np.sum(Y_samples[l] - (Y_samples[l-1] if l > 0 else 0))\n        for _ in range(K_l - K_pilot):\n            W, T = generate_wt_sample(params, rng)\n            y_fine = compute_delay(params, W, T, l)\n            y_coarse = compute_delay(params, W, T, l-1) if l > 0 else 0.0\n            sum_of_diffs += y_fine - y_coarse\n        \n        final_estimate += sum_of_diffs / K_l\n        \n    return final_estimate\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2416335"}]}