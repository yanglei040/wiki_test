{"hands_on_practices": [{"introduction": "Acoustic tomography is a fundamental inverse problem with applications ranging from medical imaging to geophysical exploration. This exercise provides a hands-on introduction to this field by tasking you with reconstructing a one-dimensional sound speed profile from simulated travel-time measurements. You will implement a Tikhonov-regularized solution with a smoothness prior, allowing you to directly observe how regularization stabilizes the reconstruction, especially when dealing with noisy or incomplete data sets [@problem_id:2405385].", "problem": "Consider a one-dimensional acoustic medium occupying the interval $[0,L]$ with $L = 1000 \\ \\text{m}$. A point source is located at $x=0$. The unknown sound speed field $c(x)$ is assumed piecewise constant over $N$ equal cells of width $\\Delta x = L/N$, with $N = 10$. Let $c_i$ denote the sound speed (in $\\text{m/s}$) in cell $i \\in \\{1,2,\\dots,N\\}$, and let $s_i$ denote the slowness in that cell, defined by $s_i = 1/c_i$ (in $\\text{s/m}$). Travel-time measurements are made by receivers placed at cell boundaries $x_k = k \\, \\Delta x$ for selected $k \\in \\{1,2,\\dots,N\\}$. The travel time to receiver $k$ is defined by\n$$\nt_k \\;=\\; \\int_{0}^{x_k} \\frac{1}{c(x)} \\, dx \\;+\\; \\varepsilon_k \\;=\\; \\sum_{i=1}^{k} \\Delta x \\, s_i \\;+\\; \\varepsilon_k,\n$$\nwhere $\\varepsilon_k$ is an additive perturbation (in $\\text{s}$). Define the measurement matrix $W \\in \\mathbb{R}^{K \\times N}$, where $K$ is the number of receivers used, by\n$$\nW_{k,i} \\;=\\; \\begin{cases}\n\\Delta x,  i \\le k,\\\\\n0,  i  k,\n\\end{cases}\n$$\nfor $k \\in \\{1,\\dots,K\\}$ and $i \\in \\{1,\\dots,N\\}$. Let $\\mathbf{t} \\in \\mathbb{R}^{K}$ collect the $t_k$, and $\\mathbf{s} \\in \\mathbb{R}^{N}$ collect the $s_i$. The discrete forward model is $\\mathbf{t} = W \\mathbf{s} + \\boldsymbol{\\varepsilon}$.\n\nReconstruct $\\mathbf{c}$ by first reconstructing $\\mathbf{s}$ as the unique minimizer of the following regularized least-squares problem with quadratic roughness penalty:\n$$\n\\min_{\\mathbf{s} \\in \\mathbb{R}^{N}} \\; J(\\mathbf{s}) \\;=\\; \\frac{1}{2} \\lVert W \\mathbf{s} - \\mathbf{t} \\rVert_2^2 \\;+\\; \\frac{\\lambda}{2} \\lVert D \\mathbf{s} \\rVert_2^2,\n$$\nwhere $\\lambda \\ge 0$ is a scalar parameter, and $D \\in \\mathbb{R}^{(N-1)\\times N}$ is the first-difference operator defined by\n$$\n(D\\mathbf{s})_i \\;=\\; s_{i+1} - s_i, \\quad \\text{for } i \\in \\{1,\\dots,N-1\\},\n$$\nthat is, $D_{i,i} = -1$, $D_{i,i+1} = 1$, and all other entries of $D$ are zero. After obtaining the minimizer $\\widehat{\\mathbf{s}}$, compute the reconstructed sound speeds cell-wise by $\\widehat{c}_i = 1 / \\widehat{s}_i$ (in $\\text{m/s}$). For evaluation, use the root-mean-square error (in $\\text{m/s}$)\n$$\n\\text{RMSE} \\;=\\; \\sqrt{ \\frac{1}{N} \\sum_{i=1}^{N} \\left( \\widehat{c}_i - c_i^{\\star} \\right)^2 },\n$$\nwith respect to the ground-truth $\\mathbf{c}^{\\star}$ specified below.\n\nUse the following ground-truth and test suite. The ground-truth cell-wise sound speed vector (in $\\text{m/s}$) is\n$$\n\\mathbf{c}^{\\star} \\;=\\; [\\,1480,\\;1480,\\;1500,\\;1520,\\;1550,\\;1550,\\;1530,\\;1500,\\;1490,\\;1480\\,].\n$$\nThe perturbations are deterministic and defined for a given $\\sigma \\ge 0$ by\n$$\n\\varepsilon_k \\;=\\; \\sigma \\, (-1)^k, \\quad \\text{for } k \\in \\{1,\\dots,K\\},\n$$\nwith units of seconds.\n\nTest suite (each row defines one test case with its parameters):\n- Case $1$: $K = 10$, receivers at all boundaries $x_k = k \\Delta x$ for $k \\in \\{1,\\dots,10\\}$, $\\sigma = 0$, $\\lambda = 0$.\n- Case $2$: $K = 6$, receivers at $x_k = k \\Delta x$ for $k \\in \\{1,\\dots,6\\}$, $\\sigma = 0$, $\\lambda = 10^{-2}$.\n- Case $3$: $K = 10$, receivers at $x_k = k \\Delta x$ for $k \\in \\{1,\\dots,10\\}$, $\\sigma = 5 \\times 10^{-4}$, $\\lambda = 10^{-3}$.\n- Case $4$: $K = 10$, receivers at $x_k = k \\Delta x$ for $k \\in \\{1,\\dots,10\\}$, $\\sigma = 5 \\times 10^{-3}$, $\\lambda = 1$.\n\nAll travel times must be expressed in seconds, and all sound speeds must be expressed in meters per second. Angles are not used in this problem.\n\nYour program must, for each test case, assemble $W$ and $D$ exactly as defined, synthesize $\\mathbf{t}$ from $\\mathbf{c}^{\\star}$ and the specified $\\sigma$, compute the unique minimizer $\\widehat{\\mathbf{s}}$ of $J(\\mathbf{s})$, convert to $\\widehat{\\mathbf{c}}$, and report the $\\text{RMSE}$ in meters per second as a floating-point number.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3,r_4]$), where $r_j$ is the $\\text{RMSE}$ (in $\\text{m/s}$) for test case $j$. The order must be Case $1$, Case $2$, Case $3$, Case $4$.", "solution": "The problem statement has been subjected to rigorous validation and is deemed valid. It is scientifically grounded, well-posed, objective, and internally consistent. It describes a standard problem in computational science: a one-dimensional, regularized linear inverse problem for acoustic travel-time tomography. All parameters and required definitions are provided, allowing for a unique and meaningful solution to be computed for each specified test case. We may therefore proceed with the derivation and implementation of the solution.\n\nThe core of the problem is to find the slowness vector $\\mathbf{s} \\in \\mathbb{R}^{N}$ that minimizes the Tikhonov-regularized objective function:\n$$\nJ(\\mathbf{s}) \\;=\\; \\frac{1}{2} \\lVert W \\mathbf{s} - \\mathbf{t} \\rVert_2^2 \\;+\\; \\frac{\\lambda}{2} \\lVert D \\mathbf{s} \\rVert_2^2\n$$\nHere, $\\mathbf{s}$ is the vector of unknown cell slownesses, $\\mathbf{t}$ is the vector of measured travel times, $W$ is the forward model matrix, $D$ is the first-difference operator acting as a regularization matrix, and $\\lambda \\ge 0$ is the regularization parameter.\n\nThe function $J(\\mathbf{s})$ is a quadratic, convex function of $\\mathbf{s}$. Its unique minimizer, which we denote $\\widehat{\\mathbf{s}}$, can be found by computing the gradient of $J(\\mathbf{s})$ with respect to $\\mathbf{s}$ and setting it to the zero vector. The gradient is given by:\n$$\n\\nabla_{\\mathbf{s}} J(\\mathbf{s}) \\;=\\; \\nabla_{\\mathbf{s}} \\left( \\frac{1}{2} (W\\mathbf{s} - \\mathbf{t})^T (W\\mathbf{s} - \\mathbf{t}) \\right) \\;+\\; \\nabla_{\\mathbf{s}} \\left( \\frac{\\lambda}{2} (D\\mathbf{s})^T (D\\mathbf{s}) \\right)\n$$\nUsing standard rules of matrix calculus, the gradients of the two terms are:\n$$\n\\nabla_{\\mathbf{s}} \\left( \\frac{1}{2} \\lVert W \\mathbf{s} - \\mathbf{t} \\rVert_2^2 \\right) = W^T(W\\mathbf{s} - \\mathbf{t})\n$$\n$$\n\\nabla_{\\mathbf{s}} \\left( \\frac{\\lambda}{2} \\lVert D \\mathbf{s} \\rVert_2^2 \\right) = \\lambda D^T D \\mathbf{s}\n$$\nSetting the total gradient to zero yields the equation:\n$$\nW^T(W\\widehat{\\mathbf{s}} - \\mathbf{t}) + \\lambda D^T D \\widehat{\\mathbf{s}} = \\mathbf{0}\n$$\nRearranging the terms to isolate $\\widehat{\\mathbf{s}}$, we obtain the normal equations for this regularized least-squares problem:\n$$\n(W^T W + \\lambda D^T D) \\widehat{\\mathbf{s}} \\;=\\; W^T \\mathbf{t}\n$$\nThis is a system of linear equations of the form $A\\mathbf{x} = \\mathbf{b}$, where the system matrix is $A = (W^T W + \\lambda D^T D)$, the unknown vector is $\\mathbf{x} = \\widehat{\\mathbf{s}}$, and the right-hand side vector is $\\mathbf{b} = W^T \\mathbf{t}$. As established during validation, the matrix $A$ is invertible for all specified test cases, ensuring that a unique solution $\\widehat{\\mathbf{s}}$ exists and can be computed by solving this linear system.\n\nThe computational procedure for each test case is as follows:\n$1$. Define the physical and discretization parameters: $L = 1000 \\ \\text{m}$, $N = 10$, and $\\Delta x = L/N = 100 \\ \\text{m}$.\n$2$. Define the ground-truth sound speed vector $\\mathbf{c}^{\\star} \\in \\mathbb{R}^{N}$ and compute the corresponding ground-truth slowness vector $\\mathbf{s}^{\\star}$ component-wise as $s^{\\star}_i = 1/c^{\\star}_i$.\n$3$. For each test case, specified by parameters $K$, $\\sigma$, and $\\lambda$:\n    a. Construct the measurement matrix $W \\in \\mathbb{R}^{K \\times N}$ according to the definition $W_{k,i} = \\Delta x$ for $i \\le k$ and $W_{k,i} = 0$ for $i  k$, using $1$-based indexing for problem-domain clarity.\n    b. Construct the first-difference operator $D \\in \\mathbb{R}^{(N-1) \\times N}$ where each row $i$ implements the operation $s_{i+1} - s_i$.\n    c. Synthesize the measurement vector $\\mathbf{t} \\in \\mathbb{R}^{K}$. This involves calculating the ideal, unperturbed travel times $\\mathbf{t}_{ideal} = W \\mathbf{s}^{\\star}$ and adding the deterministic perturbation vector $\\boldsymbol{\\varepsilon}$, where $\\varepsilon_k = \\sigma (-1)^k$ for $k \\in \\{1, \\dots, K\\}$. The final measurement vector is $\\mathbf{t} = \\mathbf{t}_{ideal} + \\boldsymbol{\\varepsilon}$.\n    d. Form the system matrix $A = W^T W + \\lambda D^T D$ and the right-hand side vector $\\mathbf{b} = W^T \\mathbf{t}$.\n    e. Solve the linear system $A \\widehat{\\mathbf{s}} = \\mathbf{b}$ to obtain the estimated slowness vector $\\widehat{\\mathbf{s}}$.\n    f. Convert the estimated slowness vector $\\widehat{\\mathbf{s}}$ back to a sound speed vector $\\widehat{\\mathbf{c}}$ via $\\widehat{c}_i = 1/\\widehat{s}_i$.\n    g. Evaluate the reconstruction quality by computing the root-mean-square error ($\\text{RMSE}$) between the reconstructed sound speeds $\\widehat{\\mathbf{c}}$ and the ground-truth sound speeds $\\mathbf{c}^{\\star}$:\n    $$\n    \\text{RMSE} \\;=\\; \\sqrt{ \\frac{1}{N} \\sum_{i=1}^{N} \\left( \\widehat{c}_i - c_i^{\\star} \\right)^2 }\n    $$\nThis procedure is applied to each of the four test cases provided, and the resulting $\\text{RMSE}$ values are reported.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the 1D acoustic tomography inverse problem for four test cases.\n    \"\"\"\n    # Define physical and discretization parameters\n    L = 1000.0  # m\n    N = 10      # number of cells\n    delta_x = L / N # m\n\n    # Ground-truth sound speed vector (m/s)\n    c_star = np.array([1480., 1480., 1500., 1520., 1550., 1550., 1530., 1500., 1490., 1480.])\n    \n    # Ground-truth slowness vector (s/m)\n    s_star = 1.0 / c_star\n\n    # Define the test suite\n    test_cases = [\n        # Case 1: Full data, no noise, no regularization\n        {'K': 10, 'sigma': 0.0, 'lam': 0.0},\n        # Case 2: Incomplete data, no noise, with regularization\n        {'K': 6, 'sigma': 0.0, 'lam': 1e-2},\n        # Case 3: Full data, small noise, with regularization\n        {'K': 10, 'sigma': 5e-4, 'lam': 1e-3},\n        # Case 4: Full data, larger noise, stronger regularization\n        {'K': 10, 'sigma': 5e-3, 'lam': 1.0},\n    ]\n\n    results = []\n\n    for case in test_cases:\n        K = case['K']\n        sigma = case['sigma']\n        lam = case['lam']\n\n        # 1. Construct the measurement matrix W (K x N)\n        # W_ki = delta_x if i = k, 0 otherwise (1-based indexing)\n        # In 0-based code: W[k_idx, i_idx] for i_idx = k_idx\n        W = np.zeros((K, N))\n        for k in range(K):\n            for i in range(k + 1):\n                W[k, i] = delta_x\n\n        # 2. Construct the first-difference operator D ((N-1) x N)\n        # (Ds)_i = s_{i+1} - s_i\n        # This corresponds to a matrix with -1 on the main diagonal and 1 on the first super-diagonal.\n        D = np.eye(N, k=1) - np.eye(N, k=0)\n        D = D[:-1, :] # Keep the first N-1 rows\n\n        # 3. Synthesize the measurement vector t (K x 1)\n        # t = W * s_star + eps\n        t_ideal = W @ s_star\n        \n        # Perturbation vector eps, where eps_k = sigma * (-1)^k for k=1..K\n        k_indices = np.arange(1, K + 1)\n        eps = sigma * ((-1.0)**k_indices)\n        \n        t = t_ideal + eps\n\n        # 4. Form the system matrix A and right-hand side b for the normal equations\n        # (W^T W + lambda * D^T D) * s_hat = W^T * t\n        A = W.T @ W + lam * (D.T @ D)\n        b = W.T @ t\n\n        # 5. Solve the linear system for the estimated slowness s_hat\n        s_hat = np.linalg.solve(A, b)\n\n        # 6. Convert estimated slowness to sound speed\n        c_hat = 1.0 / s_hat\n\n        # 7. Calculate the Root-Mean-Square Error (RMSE)\n        rmse = np.sqrt(np.mean((c_hat - c_star)**2))\n        results.append(rmse)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.10f}' for r in results)}]\")\n\nsolve()\n```", "id": "2405385"}, {"introduction": "While Tikhonov ($L_2$) regularization is a powerful tool, its success hinges on matching its assumptions to the signal's properties. This exercise [@problem_id:2405389] challenges you to compare $L_2$ regularization with $L_1$ regularization, which promotes sparsity, in a scenario where the true signal is known to be non-sparse. By observing how the choice of regularizer impacts the result, you will learn the critical importance of selecting a prior that reflects the underlying physics or structure of the solution.", "problem": "Consider the following deterministic one-dimensional linear inverse problem posed on a periodic grid of length $n=128$. The forward operator $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$ is the circulant matrix that represents circular convolution with a discrete Gaussian kernel of standard deviation $\\sigma$ (measured in grid-index units) that is normalized to have unit sum. Let $\\mathbf{k} \\in \\mathbb{R}^{n}$ denote the first column of $\\mathbf{A}$, with entries\n$$\nk_j \\propto \\exp\\!\\left(-\\frac{d(j)^2}{2\\sigma^2}\\right), \\quad d(j) = \\min\\{j,\\,n-j\\}, \\quad j=0,1,\\ldots,n-1,\n$$\nand then scaled so that $\\sum_{j=0}^{n-1} k_j = 1$. For $\\sigma=0$, interpret the kernel as the Kronecker delta $k_0=1$ and $k_j=0$ for $j\\neq 0$, so that $\\mathbf{A}$ is the identity matrix.\n\nLet the true signal be the constant vector $\\mathbf{x}_{\\mathrm{true}} \\in \\mathbb{R}^{n}$ with entries $(\\mathbf{x}_{\\mathrm{true}})_i = 1$ for all $i$. Let the noiseless data be $\\mathbf{y}=\\mathbf{A}\\mathbf{x}_{\\mathrm{true}}$.\n\nFor a given regularization parameter $\\lambda0$, define two reconstructions $\\widehat{\\mathbf{x}}_{2}$ and $\\widehat{\\mathbf{x}}_{1}$ as the unique minimizers of the convex objectives\n$$\n\\widehat{\\mathbf{x}}_{2} \\in \\underset{\\mathbf{x}\\in\\mathbb{R}^n}{\\arg\\min}\\ \\left\\|\\mathbf{A}\\mathbf{x}-\\mathbf{y}\\right\\|_2^2 + \\lambda \\left\\|\\mathbf{x}\\right\\|_2^2,\n\\qquad\n\\widehat{\\mathbf{x}}_{1} \\in \\underset{\\mathbf{x}\\in\\mathbb{R}^n}{\\arg\\min}\\ \\left\\|\\mathbf{A}\\mathbf{x}-\\mathbf{y}\\right\\|_2^2 + \\lambda \\left\\|\\mathbf{x}\\right\\|_1.\n$$\nFor each reconstruction, define the relative reconstruction error\n$$\ne_p = \\frac{\\left\\|\\widehat{\\mathbf{x}}_{p}-\\mathbf{x}_{\\mathrm{true}}\\right\\|_2}{\\left\\|\\mathbf{x}_{\\mathrm{true}}\\right\\|_2}, \\quad p\\in\\{1,2\\}.\n$$\nDefine the performance gap $g = e_1 - e_2$.\n\nYour task is to compute $g$ for each of the following test cases, where each test case specifies $(\\sigma,\\lambda)$:\n- Test case $1$: $(\\sigma,\\lambda)=(0.0,\\,1.5)$.\n- Test case $2$: $(\\sigma,\\lambda)=(2.0,\\,1.5)$.\n- Test case $3$: $(\\sigma,\\lambda)=(4.0,\\,3.0)$.\n\nRequirements:\n- Use the precise definitions above for $\\mathbf{A}$, $\\mathbf{y}$, $\\widehat{\\mathbf{x}}_{2}$, $\\widehat{\\mathbf{x}}_{1}$, and $e_p$.\n- No measurement noise is present; do not introduce any randomness.\n- Angles do not appear; no angle units are needed.\n- The final outputs must be real numbers.\n\nFinal output format:\n- Your program should produce a single line of output containing the three values of $g$ for the test cases, in order, as a comma-separated list enclosed in square brackets, with each value rounded to exactly six digits after the decimal point (for example, $[0.123456,0.000000,1.500000]$).", "solution": "The problem as stated is valid. It is scientifically grounded in the principles of computational engineering and inverse problems, specifically using Tikhonov ($\\ell_2$) and LASSO ($\\ell_1$) regularization. It is well-posed, with all necessary data and definitions provided to find a unique solution for each reconstruction. The language is objective and mathematically precise. We may proceed with the solution.\n\nThe task is to compute the performance gap $g = e_1 - e_2$ for three test cases. The core of the problem is to find the reconstructions $\\widehat{\\mathbf{x}}_2$ and $\\widehat{\\mathbf{x}}_1$ and their corresponding relative errors $e_2$ and $e_1$.\n\nFirst, we analyze the data vector $\\mathbf{y}$. The forward operator $\\mathbf{A}$ is a circulant matrix generated by the kernel vector $\\mathbf{k} \\in \\mathbb{R}^n$. A property of any circulant matrix is that the sum of the elements in each row is constant across all rows. This sum is equal to the sum of the elements of the generating vector, which in this case is the first column $\\mathbf{k}$. The problem states that the kernel is normalized to have a unit sum:\n$$\n\\sum_{j=0}^{n-1} k_j = 1\n$$\nTherefore, the sum of elements in any row of $\\mathbf{A}$ is $1$. The true signal $\\mathbf{x}_{\\mathrm{true}}$ is a constant vector with all entries equal to $1$. Let us denote this vector by $\\mathbf{1} \\in \\mathbb{R}^n$. The noiseless data $\\mathbf{y}$ is given by $\\mathbf{y} = \\mathbf{A}\\mathbf{x}_{\\mathrm{true}}$. The $i$-th component of $\\mathbf{y}$ is:\n$$\n(\\mathbf{y})_i = (\\mathbf{A}\\mathbf{1})_i = \\sum_{j=0}^{n-1} A_{ij} \\cdot 1 = \\sum_{j=0}^{n-1} A_{ij} = 1\n$$\nThis holds for all $i = 0, 1, \\ldots, n-1$. Thus, the data vector is identical to the true signal:\n$$\n\\mathbf{y} = \\mathbf{1} = \\mathbf{x}_{\\mathrm{true}}\n$$\nThis is a critical simplification that holds irrespective of the standard deviation $\\sigma$ of the Gaussian kernel, as long as the kernel sum is normalized to $1$. This also holds for the special case $\\sigma=0$, where $\\mathbf{A}$ is the identity matrix $\\mathbf{I}$, since $\\mathbf{I}\\mathbf{1} = \\mathbf{1}$.\n\nNext, we solve for the Tikhonov reconstruction $\\widehat{\\mathbf{x}}_2$, which minimizes:\n$$\nJ_2(\\mathbf{x}) = \\left\\|\\mathbf{A}\\mathbf{x}-\\mathbf{y}\\right\\|_2^2 + \\lambda \\left\\|\\mathbf{x}\\right\\|_2^2 = \\left\\|\\mathbf{A}\\mathbf{x}-\\mathbf{1}\\right\\|_2^2 + \\lambda \\left\\|\\mathbf{x}\\right\\|_2^2\n$$\nGiven the constant nature of $\\mathbf{x}_{\\mathrm{true}}$ and $\\mathbf{y}$, we test the ansatz that the solution is also a constant vector, $\\mathbf{x} = c\\mathbf{1}$ for some scalar $c \\in \\mathbb{R}$. Substituting this into the objective function:\n$$\nJ_2(c\\mathbf{1}) = \\left\\|\\mathbf{A}(c\\mathbf{1})-\\mathbf{1}\\right\\|_2^2 + \\lambda \\left\\|c\\mathbf{1}\\right\\|_2^2 = \\left\\|c(\\mathbf{A}\\mathbf{1})-\\mathbf{1}\\right\\|_2^2 + \\lambda c^2 \\left\\|\\mathbf{1}\\right\\|_2^2\n$$\nUsing $\\mathbf{A}\\mathbf{1}=\\mathbf{1}$, $\\|\\mathbf{1}\\|_2^2=n$, the objective becomes a function of $c$:\n$$\nJ_2(c) = \\left\\|c\\mathbf{1}-\\mathbf{1}\\right\\|_2^2 + \\lambda n c^2 = n(c-1)^2 + \\lambda n c^2\n$$\nTo find the minimum, we set the derivative with respect to $c$ to zero:\n$$\n\\frac{dJ_2}{dc} = 2n(c-1) + 2n\\lambda c = 0 \\implies c-1+\\lambda c = 0 \\implies c(1+\\lambda)=1 \\implies c = \\frac{1}{1+\\lambda}\n$$\nThe solution is therefore $\\widehat{\\mathbf{x}}_2 = \\frac{1}{1+\\lambda}\\mathbf{1}$. The relative error $e_2$ is:\n$$\ne_2 = \\frac{\\|\\widehat{\\mathbf{x}}_2 - \\mathbf{x}_{\\mathrm{true}}\\|_2}{\\|\\mathbf{x}_{\\mathrm{true}}\\|_2} = \\frac{\\left\\|\\frac{1}{1+\\lambda}\\mathbf{1} - \\mathbf{1}\\right\\|_2}{\\|\\mathbf{1}\\|_2} = \\frac{\\left| \\frac{1}{1+\\lambda}-1 \\right| \\|\\mathbf{1}\\|_2}{\\|\\mathbf{1}\\|_2} = \\left| \\frac{-\\lambda}{1+\\lambda} \\right| = \\frac{\\lambda}{1+\\lambda}\n$$\n\nNow, we solve for the LASSO reconstruction $\\widehat{\\mathbf{x}}_1$, which minimizes:\n$$\nJ_1(\\mathbf{x}) = \\left\\|\\mathbf{A}\\mathbf{x}-\\mathbf{y}\\right\\|_2^2 + \\lambda \\left\\|\\mathbf{x}\\right\\|_1 = \\left\\|\\mathbf{A}\\mathbf{x}-\\mathbf{1}\\right\\|_2^2 + \\lambda \\left\\|\\mathbf{x}\\right\\|_1\n$$\nApplying the same ansatz $\\mathbf{x}=c\\mathbf{1}$, the objective becomes:\n$$\nJ_1(c\\mathbf{1}) = n(c-1)^2 + \\lambda \\|c\\mathbf{1}\\|_1 = n(c-1)^2 + \\lambda n |c|\n$$\nWe minimize $f(c) = (c-1)^2 + \\lambda|c|$.\nIf $c \\ge 0$, $f(c) = (c-1)^2 + \\lambda c$. The derivative is $f'(c) = 2(c-1)+\\lambda$, which is zero at $c = 1 - \\lambda/2$. If $\\lambda \\le 2$, this minimum is non-negative, so the minimizer for $c \\ge 0$ is $c = 1 - \\lambda/2$. If $\\lambda  2$, the minimum occurs at a negative value, so on $c \\ge 0$, $f(c)$ is strictly increasing, and the minimum is at $c=0$.\nIf $c  0$, $f(c) = (c-1)^2 - \\lambda c$. The derivative is $f'(c) = 2(c-1)-\\lambda$, which is zero at $c = 1 + \\lambda/2$. This contradicts the assumption $c0$ for $\\lambda  0$.\nThus, the global minimum is $c^* = \\max(0, 1 - \\lambda/2)$. This is the soft-thresholding operator $S_{\\lambda/2}(1)$. The solution is $\\widehat{\\mathbf{x}}_1 = \\max(0, 1 - \\lambda/2)\\mathbf{1}$. One can verify this satisfies the subgradient optimality condition for the full problem.\n\nThe relative error $e_1$ is calculated based on $c_1 = \\max(0, 1 - \\lambda/2)$:\n$$\ne_1 = \\frac{\\|\\widehat{\\mathbf{x}}_1 - \\mathbf{x}_{\\mathrm{true}}\\|_2}{\\|\\mathbf{x}_{\\mathrm{true}}\\|_2} = \\frac{\\|c_1\\mathbf{1} - \\mathbf{1}\\|_2}{\\|\\mathbf{1}\\|_2} = |c_1 - 1| = |\\max(0, 1-\\lambda/2) - 1|\n$$\nThis expression simplifies based on the value of $\\lambda$:\nIf $\\lambda \\le 2$: $c_1 = 1 - \\lambda/2$. Then $e_1 = |(1-\\lambda/2) - 1| = |-\\lambda/2| = \\lambda/2$.\nIf $\\lambda  2$: $c_1 = 0$. Then $e_1 = |0 - 1| = 1$.\n\nFinally, we compute the performance gap $g = e_1 - e_2$.\nCase $1$: $\\lambda \\le 2$\n$$\ng = e_1 - e_2 = \\frac{\\lambda}{2} - \\frac{\\lambda}{1+\\lambda} = \\lambda\\left(\\frac{1}{2} - \\frac{1}{1+\\lambda}\\right) = \\lambda\\frac{(1+\\lambda)-2}{2(1+\\lambda)} = \\frac{\\lambda(\\lambda-1)}{2(1+\\lambda)}\n$$\nCase $2$: $\\lambda  2$\n$$\ng = e_1 - e_2 = 1 - \\frac{\\lambda}{1+\\lambda} = \\frac{(1+\\lambda)-\\lambda}{1+\\lambda} = \\frac{1}{1+\\lambda}\n$$\nThe performance gap $g$ is independent of both $\\sigma$ and $n$. We now apply these formulas to the given test cases.\n\nTest Case $1$: $(\\sigma, \\lambda) = (0.0, 1.5)$.\nHere $\\lambda = 1.5$, which satisfies $\\lambda \\le 2$.\n$$\ng = \\frac{1.5(1.5-1)}{2(1+1.5)} = \\frac{1.5(0.5)}{2(2.5)} = \\frac{0.75}{5.0} = 0.15\n$$\n\nTest Case $2$: $(\\sigma, \\lambda) = (2.0, 1.5)$.\nHere $\\lambda = 1.5$, which satisfies $\\lambda \\le 2$. The value of $\\sigma$ is irrelevant.\n$$\ng = \\frac{1.5(1.5-1)}{2(1+1.5)} = 0.15\n$$\n\nTest Case $3$: $(\\sigma, \\lambda) = (4.0, 3.0)$.\nHere $\\lambda = 3.0$, which satisfies $\\lambda  2$.\n$$\ng = \\frac{1}{1+3.0} = \\frac{1}{4.0} = 0.25\n$$\nThe computed values for $g$ are $0.15$, $0.15$, and $0.25$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the performance gap g = e1 - e2 for the given test cases.\n    The problem setup allows for an analytical simplification where the result\n    depends only on the regularization parameter lambda, not on the kernel\n    standard deviation sigma or the grid size n.\n    \"\"\"\n    \n    # Test cases are given as (sigma, lambda) tuples.\n    test_cases = [\n        (0.0, 1.5),\n        (2.0, 1.5),\n        (4.0, 3.0),\n    ]\n\n    results = []\n    for sigma, lam in test_cases:\n        # The analytical formula for the performance gap g is piecewise,\n        # with the condition based on the value of lambda relative to 2.\n        \n        if lam = 2.0:\n            # For lambda = 2, the formula is: g = lambda * (lambda - 1) / (2 * (1 + lambda))\n            g = lam * (lam - 1.0) / (2.0 * (1.0 + lam))\n        else: # lam  2.0\n            # For lambda  2, the formula is: g = 1 / (1 + lambda)\n            g = 1.0 / (1.0 + lam)\n        \n        results.append(g)\n\n    # The problem requires the output to be rounded to exactly six digits after the decimal point.\n    formatted_results = [\"{:.6f}\".format(r) for r in results]\n    \n    # The final output must be in the format [value1,value2,value3].\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2405389"}, {"introduction": "The power of regularization lies in its versatility as a guiding principle. This final exercise [@problem_id:2405441] demonstrates this by framing a creative task—reassembling a shredded document—as an inverse problem solved with a smoothness prior. By finding the permutation of strips that minimizes pixel mismatch at the boundaries, you will see how regularization concepts can be applied to discrete, combinatorial problems, solidifying it as a fundamental problem-solving strategy.", "problem": "You are given a set of vertical strips extracted from a single underlying grayscale image. Each strip is a matrix of pixel intensities with a common height and an integer width. The task is to reconstruct the left-to-right order of the strips by selecting a permutation that minimizes a smoothness-based objective across strip boundaries. This is an inverse problem in which the unknown is the permutation, and the only prior information used is a smoothness regularization that penalizes mismatch of pixel intensities across adjacent strip boundaries.\n\nLet there be $n$ strips, indexed by $i \\in \\{0,1,\\dots,n-1\\}$. Strip $i$ is given as a matrix $S_i \\in \\mathbb{R}^{H \\times W_i}$, where $H$ is the common height in pixels and $W_i$ is the width in pixels. For each strip $S_i$, define the left boundary column $L_i \\in \\mathbb{R}^{H}$ and right boundary column $R_i \\in \\mathbb{R}^{H}$ by\n$$\nL_i = S_i[:,0], \\quad R_i = S_i[:,W_i-1].\n$$\nFor two strips $i$ (on the left) and $j$ (on the right), define the boundary mismatch cost\n$$\nd(i,j) = \\lVert R_i - L_j \\rVert_2^2 = \\sum_{r=0}^{H-1} \\left(R_i[r] - L_j[r]\\right)^2.\n$$\nFor a permutation $\\pi = (\\pi_0,\\pi_1,\\dots,\\pi_{n-1})$ of the strip indices, define the objective\n$$\nJ(\\pi) = \\sum_{t=0}^{n-2} d\\big(\\pi_t,\\pi_{t+1}\\big).\n$$\nFor $n=1$, adopt the convention $J(\\pi)=0$. The reconstruction problem is: find a permutation $\\pi^\\star$ that minimizes $J(\\pi)$. In the event of ties (i.e., multiple permutations achieving the same minimal objective value), choose the lexicographically smallest permutation, where lexicographic order compares index sequences from left to right in the usual way.\n\nYour program must solve the reconstruction for each of the test cases defined below and output the optimal permutations as lists of zero-based indices. No physical units are involved. Angles are not used. The required outputs are lists of integers.\n\nTest Suite:\n- Case A (happy path; strictly increasing column intensities, single-column strips):\n  Height $H=3$. Strips $n=4$, each of width $W_i=1$:\n  $$\n  S_0 = \\begin{bmatrix} 10 \\\\ 10 \\\\ 10 \\end{bmatrix},\\quad\n  S_1 = \\begin{bmatrix} 20 \\\\ 20 \\\\ 20 \\end{bmatrix},\\quad\n  S_2 = \\begin{bmatrix} 30 \\\\ 30 \\\\ 30 \\end{bmatrix},\\quad\n  S_3 = \\begin{bmatrix} 40 \\\\ 40 \\\\ 40 \\end{bmatrix}.\n  $$\n- Case B (boundary condition; single strip):\n  Height $H=3$. Strips $n=1$, width $W_0=1$:\n  $$\n  S_0 = \\begin{bmatrix} 7 \\\\ 7 \\\\ 7 \\end{bmatrix}.\n  $$\n- Case C (edge case with ties; two identical strips and one distinct strip):\n  Height $H=3$. Strips $n=3$, each of width $W_i=1$:\n  $$\n  S_0 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix},\\quad\n  S_1 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix},\\quad\n  S_2 = \\begin{bmatrix} 100 \\\\ 100 \\\\ 100 \\end{bmatrix}.\n  $$\n- Case D (multi-column strips; varying widths):\n  Height $H=3$. Strips $n=3$ with widths $W_0=2$, $W_1=2$, $W_2=2$:\n  $$\n  S_0 = \\begin{bmatrix} 20  50 \\\\ 20  50 \\\\ 20  50 \\end{bmatrix},\\quad\n  S_1 = \\begin{bmatrix} 0  10 \\\\ 0  10 \\\\ 0  10 \\end{bmatrix},\\quad\n  S_2 = \\begin{bmatrix} 55  60 \\\\ 55  60 \\\\ 55  60 \\end{bmatrix}.\n  $$\n\nFinal Output Format:\nYour program should produce a single line of output containing a list of four elements, each being the optimal permutation for the corresponding case (A, B, C, D), represented as a list of zero-based integers. The four lists must be enclosed in a single pair of square brackets and separated by commas, with no spaces anywhere. For example, an output line has the form\n$$\n[ [\\pi_A], [\\pi_B], [\\pi_C], [\\pi_D] ]\n$$\nbut with no spaces and using only square brackets and commas, such as\n$$\n[[0,1,2],[3],[1,0,2],[2,0,1]].\n$$\nYour program must output exactly one such line and nothing else.", "solution": "The problem posed is a combinatorial optimization task to reconstruct the order of image strips. This problem is rigorously defined and scientifically sound, thus it is deemed valid. It is an instance of an inverse problem, where the ordering of strips is the unknown parameter to be determined. The method of solution utilizes a regularization term, specifically a smoothness prior, which penalizes discontinuities between adjacent strips. This is a standard approach in computational science and engineering. The problem can be formally modeled as finding the shortest Hamiltonian path in a complete weighted directed graph.\n\nLet us define the problem within the language of graph theory. The $n$ strips are the vertices of a complete directed graph $G=(V, E)$, where $V=\\{0, 1, \\dots, n-1\\}$. The weight of a directed edge from vertex $i$ to vertex $j$ is given by the boundary mismatch cost, $d(i, j) = \\lVert R_i - L_j \\rVert_2^2$. Here, $R_i$ is the rightmost column vector of strip $S_i$ and $L_j$ is the leftmost column vector of strip $S_j$. A permutation $\\pi = (\\pi_0, \\pi_1, \\dots, \\pi_{n-1})$ of the strip indices corresponds to a Hamiltonian path $P = (\\pi_0, \\pi_1, \\dots, \\pi_{n-1})$ in the graph. The total cost of this path, which is the objective function to be minimized, is $J(\\pi) = \\sum_{t=0}^{n-2} d(\\pi_t, \\pi_{t+1})$.\n\nThe task is to find a permutation $\\pi^\\star$ that minimizes $J(\\pi)$. Since the number of strips $n$ in the provided test cases is small (the maximum is $n=4$), an exhaustive search through all $n!$ possible permutations is a feasible and correct strategy. This brute-force approach guarantees that the globally optimal solution is found. The tie-breaking rule, which mandates selecting the lexicographically smallest permutation in case of multiple optima, ensures a unique solution.\n\nThe algorithm proceeds as follows:\n$1$. For a given set of $n$ strips $\\{S_0, S_1, \\dots, S_{n-1}\\}$, we first extract the left and right boundary columns, $L_i$ and $R_i$, for each strip $i$.\n$2$. A cost matrix $C$ of size $n \\times n$ is computed, where the entry $C_{ij} = d(i,j)$ for $i \\ne j$.\n$3$. The special case of $n=1$ is handled as defined: the only permutation is $(\\pi_0)=(0)$ and the cost is $J=0$.\n$4$. For $n1$, all $n!$ permutations of the indices $\\{0, 1, \\dots, n-1\\}$ are generated. To adhere to the tie-breaking rule, it is advantageous to generate these permutations in lexicographical order.\n$5$. For each permutation $\\pi$, the total cost $J(\\pi)$ is calculated. The permutation that yields the lowest cost is recorded. If subsequent permutations are found with the same minimal cost, they are ignored, as the first one found (due to the lexicographical generation) will satisfy the tie-breaking condition.\n\nLet us apply this procedure to each test case.\n\n**Case A:** $n=4$, $H=3$, $W_i=1$.\nThe strips are single-column vectors: $S_0 = [10, 10, 10]^T$, $S_1 = [20, 20, 20]^T$, $S_2 = [30, 30, 30]^T$, $S_3 = [40, 40, 40]^T$. For single-column strips, $L_i = R_i = S_i[:,0]$. The cost $d(i, j)$ depends on the squared difference of the strip intensities. To minimize the sum of costs, strips with proximate intensities must be placed adjacently. Two sequences are clear candidates: $(\\pi_a) = (0,1,2,3)$ and $(\\pi_b) = (3,2,1,0)$.\n$J(\\pi_a) = d(0,1) + d(1,2) + d(2,3) = 3 \\times (20-10)^2 + 3 \\times (30-20)^2 + 3 \\times (40-30)^2 = 300 + 300 + 300 = 900$.\n$J(\\pi_b) = d(3,2) + d(2,1) + d(1,0) = 3 \\times (30-40)^2 + 3 \\times (20-30)^2 + 3 \\times (10-20)^2 = 300 + 300 + 300 = 900$.\nAny other permutation will involve a larger jump in intensity, yielding a higher cost. For example, $J((0,2,1,3)) = d(0,2) + d(2,1) + d(1,3) = 3 \\times (30-10)^2 + 3 \\times (20-30)^2 + 3 \\times (40-20)^2 = 1200 + 300 + 1200 = 2700$.\nThe minimum cost is $900$. The optimal permutations are $(0,1,2,3)$ and $(3,2,1,0)$. According to the tie-breaking rule, we must choose the lexicographically smallest, which is $(0,1,2,3)$.\n\n**Case B:** $n=1$, $H=3$, $W_0=1$.\nAs per the problem definition for $n=1$, the objective function value is $J=0$. The only possible permutation of a single index $\\{0\\}$ is $(0)$. Thus, the solution is $(0)$.\n\n**Case C:** $n=3$, $H=3$, $W_i=1$.\nStrips are $S_0 = [0,0,0]^T$, $S_1 = [0,0,0]^T$, $S_2 = [100,100,100]^T$. Strips $S_0$ and $S_1$ are identical. As before, $L_i = R_i$. The costs involving distinct strips are $d(0,2) = d(2,0) = d(1,2) = d(2,1) = 3 \\times (100-0)^2 = 30000$. The costs between identical strips are $d(0,1) = d(1,0) = 0$.\nTo minimize the total cost $J(\\pi) = d(\\pi_0, \\pi_1)+d(\\pi_1, \\pi_2)$, one of the transitions must have zero cost, meaning strips $0$ and $1$ must be adjacent.\nThe permutations with adjacent $S_0, S_1$ are:\n- $J((0,1,2)) = d(0,1) + d(1,2) = 0 + 30000 = 30000$.\n- $J((1,0,2)) = d(1,0) + d(0,2) = 0 + 30000 = 30000$.\n- $J((2,0,1)) = d(2,0) + d(0,1) = 30000 + 0 = 30000$.\n- $J((2,1,0)) = d(2,1) + d(1,0) = 30000 + 0 = 30000$.\nPermutations where $S_0, S_1$ are not adjacent yield higher cost: $J((0,2,1)) = J((1,2,0)) = 30000 + 30000 = 60000$.\nThe minimum cost is $30000$. The optimal permutations are $(0,1,2)$, $(1,0,2)$, $(2,0,1)$, and $(2,1,0)$. The lexicographically smallest is $(0,1,2)$.\n\n**Case D:** $n=3$, $H=3$, $W_i=2$.\nThe boundary columns are:\n$L_0=[20,20,20]^T, R_0=[50,50,50]^T$.\n$L_1=[0,0,0]^T, R_1=[10,10,10]^T$.\n$L_2=[55,55,55]^T, R_2=[60,60,60]^T$.\nThe pairwise costs $d(i,j) = \\lVert R_i - L_j \\rVert_2^2$ are:\n$d(0,1) = 3 \\times (50-0)^2 = 7500$.\n$d(0,2) = 3 \\times (50-55)^2 = 75$.\n$d(1,0) = 3 \\times (10-20)^2 = 300$.\n$d(1,2) = 3 \\times (10-55)^2 = 6075$.\n$d(2,0) = 3 \\times (60-20)^2 = 4800$.\n$d(2,1) = 3 \\times (60-0)^2 = 10800$.\nTotal costs for all $3!=6$ permutations are:\n- $J((0,1,2)) = d(0,1)+d(1,2) = 7500+6075=13575$.\n- $J((0,2,1)) = d(0,2)+d(2,1) = 75+10800=10875$.\n- $J((1,0,2)) = d(1,0)+d(0,2) = 300+75=375$.\n- $J((1,2,0)) = d(1,2)+d(2,0) = 6075+4800=10875$.\n- $J((2,0,1)) = d(2,0)+d(0,1) = 4800+7500=12300$.\n- $J((2,1,0)) = d(2,1)+d(1,0) = 10800+300=11100$.\nThe minimum cost is $375$, which is uniquely achieved by the permutation $(1,0,2)$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom itertools import permutations\n\ndef solve():\n    \"\"\"\n    Solves the strip reconstruction problem for a suite of test cases.\n    \"\"\"\n\n    test_cases = [\n        # Case A: happy path; strictly increasing column intensities\n        [\n            np.array([[10], [10], [10]]),\n            np.array([[20], [20], [20]]),\n            np.array([[30], [30], [30]]),\n            np.array([[40], [40], [40]]),\n        ],\n        # Case B: boundary condition; single strip\n        [\n            np.array([[7], [7], [7]]),\n        ],\n        # Case C: edge case with ties; two identical strips\n        [\n            np.array([[0], [0], [0]]),\n            np.array([[0], [0], [0]]),\n            np.array([[100], [100], [100]]),\n        ],\n        # Case D: multi-column strips; varying widths\n        [\n            np.array([[20, 50], [20, 50], [20, 50]]),\n            np.array([[0, 10], [0, 10], [0, 10]]),\n            np.array([[55, 60], [55, 60], [55, 60]]),\n        ],\n    ]\n\n    results = []\n    for strips in test_cases:\n        result = find_optimal_permutation(strips)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # The replacement of spaces is crucial for matching the output format.\n    print(str(results).replace(' ', ''))\n\ndef find_optimal_permutation(strips):\n    \"\"\"\n    Finds the optimal permutation of strips that minimizes boundary mismatch.\n\n    Args:\n        strips (list of np.ndarray): A list of matrices representing image strips.\n\n    Returns:\n        list: The lexicographically smallest optimal permutation of strip indices.\n    \"\"\"\n    n = len(strips)\n\n    # Handle the base case of a single strip.\n    if n == 1:\n        return [0]\n\n    # Extract left and right boundary columns for each strip.\n    left_boundaries = [s[:, 0] for s in strips]\n    right_boundaries = [s[:, -1] for s in strips]\n\n    # Pre-compute the pairwise cost matrix d(i, j).\n    cost_matrix = np.zeros((n, n))\n    for i in range(n):\n        for j in range(n):\n            if i == j:\n                continue\n            # Cost is the squared L2 norm of the difference between\n            # the right boundary of strip i and the left boundary of strip j.\n            diff = right_boundaries[i] - left_boundaries[j]\n            cost_matrix[i, j] = np.dot(diff, diff)\n\n    min_cost = float('inf')\n    best_perm = None\n\n    # Iterate through all permutations of strip indices.\n    # `itertools.permutations` generates them in lexicographical order.\n    for perm in permutations(range(n)):\n        current_cost = 0.0\n        # Calculate the total cost for the current permutation.\n        for t in range(n - 1):\n            current_cost += cost_matrix[perm[t], perm[t+1]]\n        \n        # If a new minimum cost is found, update the best permutation.\n        # Because we iterate in lexicographical order, the first time we\n        # find a permutation with the minimum cost, it will be the\n        # lexicographically smallest one among all optimal permutations.\n        if current_cost  min_cost:\n            min_cost = current_cost\n            best_perm = perm\n\n    return list(best_perm)\n\nsolve()\n```", "id": "2405441"}]}