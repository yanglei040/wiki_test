{"hands_on_practices": [{"introduction": "This first practice exercise grounds the theory of the Kalman filter in a concrete application: estimating the clock error of a GPS satellite. You will implement the filter's core predict-update cycle to track the clock's bias and drift based on noisy pseudorange measurements. This problem [@problem_id:2382578] is fundamental for understanding how to translate the mathematical equations of the filter into a working algorithm for a dynamic system.", "problem": "Consider a Global Positioning System (GPS) satellite whose clock is modeled by an unknown clock bias $b_k$ (in seconds) and an unknown clock drift $d_k$ (in seconds per second) relative to a master clock at discrete times $t_k$ (in seconds). The hidden state is $x_k = \\begin{bmatrix} b_k \\\\ d_k \\end{bmatrix}$. The satellite clock obeys a linear state evolution model\n$$\n\\begin{aligned}\nb_{k+1} &= b_k + d_k \\,\\Delta t_k + w_{b,k},\\\\\nd_{k+1} &= d_k + w_{d,k},\n\\end{aligned}\n$$\nwhere $\\Delta t_k = t_{k+1} - t_k$, the process noises $w_{b,k}$ and $w_{d,k}$ are mutually independent, zero-mean Gaussian with variances $\\operatorname{Var}(w_{b,k}) = q_b \\,\\Delta t_k$ and $\\operatorname{Var}(w_{d,k}) = q_d \\,\\Delta t_k$, respectively. A pseudorange residual measurement $z_k$ (in meters) at time $t_k$ is related to the clock bias by\n$$\nz_k = c\\, b_k + v_k,\n$$\nwhere $c = 299792458$ (in meters per second) is the speed of light and $v_k$ is zero-mean Gaussian with variance $\\operatorname{Var}(v_k) = r^2$ (in meters squared). The initial state is Gaussian with mean $m_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$ and covariance $P_0 = \\operatorname{diag}\\!\\big((10^{-6})^2, (10^{-10})^2\\big)$.\n\nTask: For each test case below, given the time stamps $t_k$, measurements $z_k$, and parameters $r$, $q_b$, and $q_d$, compute the posterior mean of the state at the final time (i.e., the minimum mean-squared error estimate of $\\begin{bmatrix} b_T \\\\ d_T \\end{bmatrix}$ given all measurements up to and including the last one). Express the final estimates in seconds for $b_T$ and in seconds per second for $d_T$, each as floating-point numbers with no unit symbols in the output.\n\nTest Suite:\n- Case A:\n  - Times (seconds): $[0, 1, 2, 3, 4, 5]$.\n  - Measurements (meters): $[34.775095, 36.924783647, 37.874472334, 36.624161021, 38.073849708, 37.823538395]$.\n  - Measurement noise standard deviation (meters): $r = 2.0$.\n  - Process noise intensities: $q_b = 1.0 \\times 10^{-20}$, $q_d = 1.0 \\times 10^{-22}$.\n\n- Case B:\n  - Times (seconds): $[0, 1, 2, 3, 4, 5]$.\n  - Measurements (meters): $[24.18339664, 23.88339664, 24.38339664, 23.68339664, 23.98339664, 24.08339664]$.\n  - Measurement noise standard deviation (meters): $r = 1.0$.\n  - Process noise intensities: $q_b = 1.0 \\times 10^{-22}$, $q_d = 1.0 \\times 10^{-24}$.\n\n- Case C:\n  - Times (seconds): $[0, 2, 3.5, 7.0, 7.5, 10.0]$.\n  - Measurements (meters): $[11.9896229, 17.4296644084, 13.3846955397, 15.7797681794, 14.7647785565, 12.689830442]$.\n  - Measurement noise standard deviation (meters): $r = 5.0$.\n  - Process noise intensities: $q_b = 5.0 \\times 10^{-20}$, $q_d = 1.0 \\times 10^{-20}$.\n\n- Case D:\n  - Times (seconds): $[0, 0.5, 1.0, 1.5, 2.0]$.\n  - Measurements (meters): $[0.05, 0.729481145, 1.50896229, 2.218443435, 2.99792458]$.\n  - Measurement noise standard deviation (meters): $r = 0.1$.\n  - Process noise intensities: $q_b = 1.0 \\times 10^{-22}$, $q_d = 1.0 \\times 10^{-18}$.\n\nFinal Output Format: Your program should produce a single line of output containing the results for the four cases as a list of pairs, in the same order as the cases are listed above. Each pair is the estimated final bias and drift, i.e., $[b_T, d_T]$. The overall output must therefore be a single list of four pairs in square brackets, for example a textual structure like \"[[...,...],[...,...],[...,...],[...,...]]\", with no spaces added or extra text printed. All numerical values in the output represent seconds and seconds per second, as specified above.", "solution": "The problem demands the computation of the posterior mean of a state vector for a linear dynamical system, given a series of measurements. This is a classic state estimation problem, for which the optimal solution in the minimum mean-squared error sense is provided by the Kalman filter. The system is linear, and all noise processes are specified as Gaussian, which are the ideal conditions for the Kalman filter.\n\nThe state of the system at time $t_k$ is given by the vector $x_k = \\begin{bmatrix} b_k \\\\ d_k \\end{bmatrix}$, where $b_k$ is the clock bias in seconds and $d_k$ is the clock drift in seconds per second.\n\nThe state evolution is governed by a discrete-time linear model:\n$$x_{k+1} = F_k x_k + w_k$$\nwhere $F_k$ is the state transition matrix and $w_k$ is the process noise. From the problem statement, we have:\n$$\n\\begin{aligned}\nb_{k+1} &= b_k + d_k \\,\\Delta t_k + w_{b,k} \\\\\nd_{k+1} &= d_k + w_{d,k}\n\\end{aligned}\n$$\nThis allows us to identify the state transition matrix for the time interval $\\Delta t_k = t_{k+1} - t_k$ as:\n$$F_k = \\begin{bmatrix} 1 & \\Delta t_k \\\\ 0 & 1 \\end{bmatrix}$$\nThe process noise vector is $w_k = \\begin{bmatrix} w_{b,k} \\\\ w_{d,k} \\end{bmatrix}$. The noises $w_{b,k}$ and $w_{d,k}$ are independent, zero-mean Gaussian variables. The process noise covariance matrix $Q_k$ is therefore diagonal:\n$$Q_k = \\operatorname{Cov}(w_k) = \\begin{bmatrix} \\operatorname{Var}(w_{b,k}) & 0 \\\\ 0 & \\operatorname{Var}(w_{d,k}) \\end{bmatrix} = \\begin{bmatrix} q_b \\Delta t_k & 0 \\\\ 0 & q_d \\Delta t_k \\end{bmatrix}$$\n\nThe measurement model relates the state $x_k$ to the measurement $z_k$:\n$$z_k = H x_k + v_k$$\nwhere $H$ is the measurement matrix and $v_k$ is the measurement noise. The given relation is $z_k = c\\, b_k + v_k$. This can be written in matrix form with a constant measurement matrix:\n$$H = \\begin{bmatrix} c & 0 \\end{bmatrix}$$\nwith the speed of light $c = 299792458 \\, \\text{m/s}$. The measurement noise $v_k$ is a zero-mean Gaussian with variance $\\operatorname{Var}(v_k) = r^2$. The measurement noise covariance $R$ is thus a scalar:\n$$R = r^2$$\n\nThe Kalman filter algorithm consists of a two-step recursive process: prediction and update. Let $m_{k|k-1}$ and $P_{k|k-1}$ denote the prior mean and covariance of the state $x_k$ given measurements up to time $t_{k-1}$. Let $m_{k|k}$ and $P_{k|k}$ be the posterior mean and covariance given measurements up to time $t_k$.\n\nThe process starts with the initial state distribution at time $t_0$, which is given as a prior before any measurements are considered.\nInitial conditions (at time $t_0$ before the measurement $z_0$):\n$$m_{0|-1} = m_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$$\n$$P_{0|-1} = P_0 = \\begin{bmatrix} (10^{-6})^2 & 0 \\\\ 0 & (10^{-10})^2 \\end{bmatrix} = \\begin{bmatrix} 10^{-12} & 0 \\\\ 0 & 10^{-20} \\end{bmatrix}$$\n\nThe filter then iterates for each measurement from $k=0, \\dots, N$, where $N$ is the index of the final measurement at the final time $t_N$.\n\nFor each time step $k$:\nFirst, if $k>0$, a prediction step is performed to propagate the estimate from time $t_{k-1}$ to $t_k$.\n1.  **Prediction Step** (from $t_{k-1}$ to $t_k$):\n    -   Predicted (prior) mean for time $t_{k}$: $m_{k|k-1} = F_{k-1} m_{k-1|k-1}$\n    -   Predicted (prior) covariance for time $t_{k}$: $P_{k|k-1} = F_{k-1} P_{k-1|k-1} F_{k-1}^T + Q_{k-1}$\n    Here, $F_{k-1}$ and $Q_{k-1}$ are defined using $\\Delta t_{k-1} = t_k - t_{k-1}$. For $k=0$, this step is skipped, and we use $m_{0|-1}$ and $P_{0|-1}$ as the priors.\n\nSecond, an update step incorporates the measurement $z_k$ at time $t_k$ to refine the prior estimate into a posterior estimate.\n2.  **Update Step** (at time $t_k$):\n    -   Innovation (measurement residual): $\\nu_k = z_k - H m_{k|k-1}$\n    -   Innovation covariance: $S_k = H P_{k|k-1} H^T + R$\n    -   Kalman gain: $K_k = P_{k|k-1} H^T S_k^{-1}$\n    -   Updated (posterior) mean: $m_{k|k} = m_{k|k-1} + K_k \\nu_k$\n    -   Updated (posterior) covariance: $P_{k|k} = (I - K_k H) P_{k|k-1}$, where $I$ is the identity matrix.\n\nThe algorithm begins with the given prior for $k=0$, performs an update, then a prediction to $k=1$, an update at $k=1$, and so on, until the final measurement at time $t_N$ is processed. The final output for each test case is the posterior state mean at the final time, $m_{N|N}$, which contains the estimated bias $b_N$ and drift $d_N$. This represents the minimum mean-squared error estimate given all measurements up to and including the last one.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the GPS clock state estimation problem for multiple test cases using a Kalman filter.\n    \"\"\"\n    \n    # Define physical and initial state constants\n    C = 299792458.0  # Speed of light in m/s\n    M0 = np.array([[0.0], [0.0]])  # Initial state mean [b_0, d_0]^T\n    P0 = np.array([[1e-12, 0.0], [0.0, 1e-20]])  # Initial state covariance\n\n    test_cases = [\n        {\n            \"times\": [0, 1, 2, 3, 4, 5],\n            \"measurements\": [34.775095, 36.924783647, 37.874472334, 36.624161021, 38.073849708, 37.823538395],\n            \"r\": 2.0, \"qb\": 1.0e-20, \"qd\": 1.0e-22\n        },\n        {\n            \"times\": [0, 1, 2, 3, 4, 5],\n            \"measurements\": [24.18339664, 23.88339664, 24.38339664, 23.68339664, 23.98339664, 24.08339664],\n            \"r\": 1.0, \"qb\": 1.0e-22, \"qd\": 1.0e-24\n        },\n        {\n            \"times\": [0, 2, 3.5, 7.0, 7.5, 10.0],\n            \"measurements\": [11.9896229, 17.4296644084, 13.3846955397, 15.7797681794, 14.7647785565, 12.689830442],\n            \"r\": 5.0, \"qb\": 5.0e-20, \"qd\": 1.0e-20\n        },\n        {\n            \"times\": [0, 0.5, 1.0, 1.5, 2.0],\n            \"measurements\": [0.05, 0.729481145, 1.50896229, 2.218443435, 2.99792458],\n            \"r\": 0.1, \"qb\": 1.0e-22, \"qd\": 1.0e-18\n        }\n    ]\n\n    def run_kalman_filter(times, measurements, r, qb, qd):\n        \"\"\"\n        Implements the Kalman filter for the given time-varying linear system.\n\n        Args:\n            times (list): List of time stamps t_k.\n            measurements (list): List of measurements z_k.\n            r (float): Measurement noise standard deviation.\n            qb (float): Process noise intensity for bias.\n            qd (float): Process noise intensity for drift.\n\n        Returns:\n            list: The final estimated state [b_T, d_T].\n        \"\"\"\n        # Initialize state mean and covariance\n        m = M0.copy()\n        P = P0.copy()\n\n        # Define constant matrices\n        H = np.array([[C, 0.0]])  # Measurement matrix (1x2)\n        R_val = r**2  # Measurement noise variance (scalar)\n        I = np.identity(2)  # Identity matrix\n\n        # Iterate through each measurement\n        for k in range(len(times)):\n            # --- Prediction step (from t_{k-1} to t_k) ---\n            if k > 0:\n                dt = times[k] - times[k-1]\n                \n                # State transition matrix F\n                F = np.array([[1.0, dt], [0.0, 1.0]])\n                \n                # Process noise covariance matrix Q\n                Q = np.array([[qb * dt, 0.0], [0.0, qd * dt]])\n                \n                # Predict state and covariance\n                m = F @ m\n                P = F @ P @ F.T + Q\n            \n            # --- Update step (at t_k) ---\n            z_k = measurements[k]\n            \n            # Innovation (measurement residual)\n            nu = z_k - (H @ m)[0, 0]  # nu is a scalar\n            \n            # Innovation covariance\n            S = (H @ P @ H.T)[0, 0] + R_val  # S is a scalar\n            \n            # Kalman gain (K = P H' / S)\n            K = (P @ H.T) / S  # K is a 2x1 vector\n            \n            # Update state mean\n            m = m + K * nu\n            \n            # Update state covariance using Joseph form for numerical stability\n            P = (I - K @ H) @ P @ (I - K @ H).T + (K * R_val) @ K.T\n\n        return m.flatten().tolist()\n\n    results = []\n    for case in test_cases:\n        final_state = run_kalman_filter(\n            case[\"times\"], case[\"measurements\"], case[\"r\"], case[\"qb\"], case[\"qd\"]\n        )\n        results.append(final_state)\n    \n    # Format the output string as per requirements: [[b,d],[b,d],...]\n    output_str = f\"[{','.join([f'[{b},{d}]' for b, d in results])}]\"\n    print(output_str)\n\nsolve()\n```", "id": "2382578"}, {"introduction": "A filter is only as good as the data it receives, and real-world sensors can occasionally produce erroneous measurements. This exercise [@problem_id:2382619] introduces a vital quality control technique: the Chi-squared ($\\chi^2$) test on the innovation vector, which is the difference between the actual measurement and its prediction. By implementing this statistical test, you will learn how to automatically detect and reject outliers, a crucial step in building robust and reliable estimation systems.", "problem": "Implement a program that, for each test case in the suite below, computes the decision to accept or reject the measurement according to the rule above. The output for each test case must be a boolean: `True` if the measurement is rejected, and `False` otherwise. No physical units are involved. Angles do not appear. All probabilities and significance levels must be treated as real numbers in $[0,1]$. The test suite specifies distinct scenarios that include a typical case, a clear outlier, a correlated multivariate case, and a boundary case where $z$ equals exactly the critical value and must not be rejected.\n\nImplement a program that, for each test case in the suite below, computes the decision to accept or reject the measurement according to the rule above. The output for each test case must be a boolean: `True` if the measurement is rejected, and `False` otherwise. No physical units are involved. Angles do not appear. All probabilities and significance levels must be treated as real numbers in $[0,1]$. The test suite specifies distinct scenarios that include a typical case, a clear outlier, a correlated multivariate case, and a boundary case where $z$ equals exactly the critical value and must not be rejected.\n\nUse the following test suite, where all matrices are symmetric as required and positive definite where needed:\n\n- Test case $1$ (univariate, typical acceptance):\n  - $\\mathbf{H} = [\\,1.0\\,]$\n  - $\\mathbf{x}_f = [\\,0.0\\,]$\n  - $\\mathbf{P}_f = \\begin{bmatrix} 1.0 \\end{bmatrix}$\n  - $\\mathbf{R} = \\begin{bmatrix} 0.25 \\end{bmatrix}$\n  - $\\mathbf{y} = [\\,0.2\\,]$\n  - $\\alpha = 0.05$\n- Test case $2$ (univariate, clear outlier):\n  - $\\mathbf{H} = [\\,1.0\\,]$\n  - $\\mathbf{x}_f = [\\,0.0\\,]$\n  - $\\mathbf{P}_f = \\begin{bmatrix} 0.5 \\end{bmatrix}$\n  - $\\mathbf{R} = \\begin{bmatrix} 0.1 \\end{bmatrix}$\n  - $\\mathbf{y} = [\\,3.0\\,]$\n  - $\\alpha = 0.01$\n- Test case $3$ (bivariate, correlated, acceptance):\n  - $\\mathbf{H} = \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.0 & 1.0 \\end{bmatrix}$\n  - $\\mathbf{x}_f = \\begin{bmatrix} 1.0 \\\\ -1.0 \\end{bmatrix}$\n  - $\\mathbf{P}_f = \\begin{bmatrix} 1.0 & 0.5 \\\\ 0.5 & 2.0 \\end{bmatrix}$\n  - $\\mathbf{R} = \\begin{bmatrix} 0.2 & 0.1 \\\\ 0.1 & 0.3 \\end{bmatrix}$\n  - $\\mathbf{y} = \\begin{bmatrix} 1.3 \\\\ -0.8 \\end{bmatrix}$\n  - $\\alpha = 0.05$\n- Test case $4$ (univariate, boundary case; do not reject because $z = c$):\n  - $\\mathbf{H} = [\\,1.0\\,]$\n  - $\\mathbf{x}_f = [\\,0.0\\,]$\n  - $\\mathbf{P}_f = \\begin{bmatrix} 0.0 \\end{bmatrix}$\n  - $\\mathbf{R} = \\begin{bmatrix} 1.0 \\end{bmatrix}$\n  - $\\mathbf{y} = [\\,1.959963984540054\\,]$\n  - $\\alpha = 0.05$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., `[result1,result2,result3,result4]`), in the same order as the test cases above. Each `result` must be either `True` or `False`.", "solution": "The problem statement is examined for validity.\n\nStep 1: Extract Givens\nThe problem provides the following definitions and data for a data assimilation context:\n- Forecast state mean: $\\mathbf{x}_f \\in \\mathbb{R}^n$\n- Forecast state covariance: $\\mathbf{P}_f \\in \\mathbb{R}^{n \\times n}$\n- Measurement vector: $\\mathbf{y} \\in \\mathbb{R}^m$\n- Linear observation operator: $\\mathbf{H} \\in \\mathbb{R}^{m \\times n}$\n- Measurement error covariance: $\\mathbf{R} \\in \\mathbb{R}^{m \\times m}$ (zero-mean Gaussian error)\n- Innovation vector: $\\mathbf{v} = \\mathbf{y} - \\mathbf{H}\\mathbf{x}_f$\n- Innovation covariance: $\\mathbf{S} = \\mathbf{H}\\mathbf{P}_f\\mathbf{H}^\\top + \\mathbf{R}$\n- Test statistic: $z = \\mathbf{v}^\\top \\mathbf{S}^{-1} \\mathbf{v}$\n- Distribution of $z$: Chi-squared ($\\chi^2$) with $m$ degrees of freedom.\n- Significance level: $\\alpha \\in (0,1)$\n- Critical value, $c$: The upper $(1-\\alpha)$ quantile of the $\\chi^2$ distribution with $m$ degrees of freedom.\n- Rejection rule: The measurement is rejected if and only if $z > c$.\n\nThe problem provides four distinct test cases with specific numerical values for $\\mathbf{H}$, $\\mathbf{x}_f$, $\\mathbf{P}_f$, $\\mathbf{R}$, $\\mathbf{y}$, and $\\alpha$.\n\nStep 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem describes the innovation consistency check, also known as the Chi-squared test, which is a standard, fundamental procedure in data assimilation, particularly in the context of the Kalman filter and its ensemble variants. The statistical foundation—that the quadratic form $z = \\mathbf{v}^\\top \\mathbf{S}^{-1} \\mathbf{v}$ follows a $\\chi^2$ distribution under the given Gaussian assumptions—is a well-established result in multivariate statistics. The problem is scientifically and mathematically sound.\n- **Well-Posed**: All necessary inputs ($\\mathbf{x}_f, \\mathbf{P}_f, \\mathbf{y}, \\mathbf{H}, \\mathbf{R}, \\alpha$) are provided for each test case. The objective is to compute a boolean decision based on a clear, unambiguous rule ($z > c$). The handling of the boundary case ($z = c$) is explicitly specified, ensuring a unique solution for all possible values of $z$. The provided covariance matrices are symmetric and described as positive definite where required, which guarantees that the innovation covariance $\\mathbf{S}$ is invertible. Thus, the problem is well-posed.\n- **Objective**: The problem is stated using precise mathematical terminology. The evaluation criterion is a strict inequality, free of any subjective interpretation.\n- **Completeness and Consistency**: The problem is self-contained. The dimensions of all matrices and vectors within each test case are consistent for the required matrix operations. For example, in test case $3$, $\\mathbf{H}$ is $2 \\times 2$, $\\mathbf{x}_f$ is $2 \\times 1$, so $\\mathbf{H}\\mathbf{x}_f$ is $2 \\times 1$, which is compatible with the dimension of $\\mathbf{y}$ ($2 \\times 1$). The dimensions for the calculation of $\\mathbf{S}$ are also consistent.\n- **Other criteria**: The problem is formalizable, relevant to computational engineering, realistic in its setup (though simplified), and scientifically verifiable. It does not violate any of the specified invalidity conditions.\n\nStep 3: Verdict and Action\nThe problem is valid. A solution will be constructed.\n\nThe solution requires the implementation of the Chi-squared test for each provided test case. The procedure for each case is as follows:\n\n1.  Identify the dimension of the observation space, $m$, which is the number of rows in the observation operator $\\mathbf{H}$ (or the dimension of the measurement vector $\\mathbf{y}$). This value represents the degrees of freedom for the $\\chi^2$ distribution.\n2.  Compute the innovation vector $\\mathbf{v} = \\mathbf{y} - \\mathbf{H}\\mathbf{x}_f$. This vector represents the discrepancy between the actual measurement $\\mathbf{y}$ and the forecast state projected into observation space, $\\mathbf{H}\\mathbf{x}_f$.\n3.  Compute the innovation covariance matrix $\\mathbf{S} = \\mathbf{H}\\mathbf{P}_f\\mathbf{H}^\\top + \\mathbf{R}$. This matrix quantifies the total expected uncertainty in the innovation, combining the uncertainty from the forecast state (propagated through $\\mathbf{H}$) and the uncertainty from the measurement itself.\n4.  Compute the test statistic $z = \\mathbf{v}^\\top \\mathbf{S}^{-1} \\mathbf{v}$. This is a scalar value that represents the squared Mahalanobis distance of the innovation vector from the origin, normalized by its covariance. It measures how \"surprising\" the innovation is, given the expected uncertainty. The calculation requires computing the inverse of $\\mathbf{S}$.\n5.  Determine the critical value $c$ for the given significance level $\\alpha$. The value $c$ is the upper quantile of the $\\chi^2$ distribution with $m$ degrees of freedom, defined by $P(\\chi^2_m \\le c) = 1 - \\alpha$. This value can be obtained using the percent point function (PPF), also known as the inverse cumulative distribution function, of the $\\chi^2$ distribution.\n6.  Compare the test statistic $z$ with the critical value $c$. According to the specified rule, the measurement is rejected if $z > c$. This yields a boolean result. The boundary case $z = c$ results in non-rejection.\n\nThis algorithm will be applied to each of the four test cases.\n\n- For **Test Case 1** (univariate, typical acceptance):\n  - $m=1$. $\\mathbf{v} = [0.2] - [1.0][0.0] = [0.2]$.\n  - $\\mathbf{S} = [1.0][1.0][1.0]^\\top + [0.25] = [1.25]$.\n  - $z = [0.2]^\\top [1.25]^{-1} [0.2] = 0.2 \\times (1/1.25) \\times 0.2 = 0.032$.\n  - For $\\alpha=0.05$ and $m=1$ degree of freedom, the critical value is $c = \\chi^2_1\\text{.ppf}(0.95) \\approx 3.841$.\n  - Decision: $0.032 > 3.841$ is false. The measurement is accepted.\n\n- For **Test Case 2** (univariate, clear outlier):\n  - $m=1$. $\\mathbf{v} = [3.0] - [1.0][0.0] = [3.0]$.\n  - $\\mathbf{S} = [1.0][0.5][1.0]^\\top + [0.1] = [0.6]$.\n  - $z = [3.0]^\\top [0.6]^{-1} [3.0] = 3.0 \\times (1/0.6) \\times 3.0 = 15.0$.\n  - For $\\alpha=0.01$ and $m=1$ degree of freedom, the critical value is $c = \\chi^2_1\\text{.ppf}(0.99) \\approx 6.635$.\n  - Decision: $15.0 > 6.635$ is true. The measurement is rejected.\n\n- For **Test Case 3** (bivariate, correlated, acceptance):\n  - $m=2$. $\\mathbf{x}_f = \\begin{bmatrix} 1.0 \\\\ -1.0 \\end{bmatrix}$. $\\mathbf{y} = \\begin{bmatrix} 1.3 \\\\ -0.8 \\end{bmatrix}$. $\\mathbf{H} = \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.0 & 1.0 \\end{bmatrix}$.\n  - $\\mathbf{H}\\mathbf{x}_f = \\mathbf{x}_f = \\begin{bmatrix} 1.0 \\\\ -1.0 \\end{bmatrix}$.\n  - $\\mathbf{v} = \\begin{bmatrix} 1.3 \\\\ -0.8 \\end{bmatrix} - \\begin{bmatrix} 1.0 \\\\ -1.0 \\end{bmatrix} = \\begin{bmatrix} 0.3 \\\\ 0.2 \\end{bmatrix}$.\n  - Since $\\mathbf{H}$ is the identity matrix, $\\mathbf{S} = \\mathbf{P}_f + \\mathbf{R} = \\begin{bmatrix} 1.0 & 0.5 \\\\ 0.5 & 2.0 \\end{bmatrix} + \\begin{bmatrix} 0.2 & 0.1 \\\\ 0.1 & 0.3 \\end{bmatrix} = \\begin{bmatrix} 1.2 & 0.6 \\\\ 0.6 & 2.3 \\end{bmatrix}$.\n  - The inverse is $\\mathbf{S}^{-1} = \\frac{1}{(1.2)(2.3) - (0.6)(0.6)} \\begin{bmatrix} 2.3 & -0.6 \\\\ -0.6 & 1.2 \\end{bmatrix} = \\frac{1}{2.4} \\begin{bmatrix} 2.3 & -0.6 \\\\ -0.6 & 1.2 \\end{bmatrix}$.\n  - $z = \\begin{bmatrix} 0.3 & 0.2 \\end{bmatrix} \\frac{1}{2.4} \\begin{bmatrix} 2.3 & -0.6 \\\\ -0.6 & 1.2 \\end{bmatrix} \\begin{bmatrix} 0.3 \\\\ 0.2 \\end{bmatrix} = \\frac{1}{2.4} \\begin{bmatrix} 0.3 & 0.2 \\end{bmatrix} \\begin{bmatrix} 0.57 \\\\ 0.06 \\end{bmatrix} = \\frac{1}{2.4} (0.171 + 0.012) = \\frac{0.183}{2.4} = 0.07625$.\n  - For $\\alpha=0.05$ and $m=2$ degrees of freedom, the critical value is $c = \\chi^2_2\\text{.ppf}(0.95) \\approx 5.991$.\n  - Decision: $0.07625 > 5.991$ is false. The measurement is accepted.\n\n- For **Test Case 4** (univariate, boundary case):\n  - $m=1$. $\\mathbf{v} = [1.959963984540054] - [1.0][0.0] = [1.959963984540054]$.\n  - $\\mathbf{S} = [1.0][0.0][1.0]^\\top + [1.0] = [1.0]$.\n  - $z = [1.959963984540054]^\\top [1.0]^{-1} [1.959963984540054] = (1.959963984540054)^2 \\approx 3.841458820694124$.\n  - For $\\alpha=0.05$ and $m=1$ degree of freedom, the critical value is $c = \\chi^2_1\\text{.ppf}(0.95) \\approx 3.841458820694124$.\n  - The value of $\\mathbf{y}$ is constructed such that $z$ is exactly equal to $c$.\n  - Decision: $z > c$ is false. The measurement is accepted.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef solve():\n    \"\"\"\n    Solves the outlier detection problem for a suite of test cases\n    based on the Chi-squared innovation consistency test.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1 (univariate, typical acceptance)\n        {\n            \"H\": np.array([[1.0]]),\n            \"x_f\": np.array([0.0]),\n            \"P_f\": np.array([[1.0]]),\n            \"R\": np.array([[0.25]]),\n            \"y\": np.array([0.2]),\n            \"alpha\": 0.05\n        },\n        # Test case 2 (univariate, clear outlier)\n        {\n            \"H\": np.array([[1.0]]),\n            \"x_f\": np.array([0.0]),\n            \"P_f\": np.array([[0.5]]),\n            \"R\": np.array([[0.1]]),\n            \"y\": np.array([3.0]),\n            \"alpha\": 0.01\n        },\n        # Test case 3 (bivariate, correlated, acceptance)\n        {\n            \"H\": np.array([[1.0, 0.0], [0.0, 1.0]]),\n            \"x_f\": np.array([1.0, -1.0]),\n            \"P_f\": np.array([[1.0, 0.5], [0.5, 2.0]]),\n            \"R\": np.array([[0.2, 0.1], [0.1, 0.3]]),\n            \"y\": np.array([1.3, -0.8]),\n            \"alpha\": 0.05\n        },\n        # Test case 4 (univariate, boundary case; do not reject because z = c)\n        {\n            \"H\": np.array([[1.0]]),\n            \"x_f\": np.array([0.0]),\n            \"P_f\": np.array([[0.0]]),\n            \"R\": np.array([[1.0]]),\n            \"y\": np.array([1.959963984540054]),\n            \"alpha\": 0.05\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        # Extract matrices and parameters for the current case.\n        H = case[\"H\"]\n        x_f = case[\"x_f\"]\n        P_f = case[\"P_f\"]\n        R = case[\"R\"]\n        y = case[\"y\"]\n        alpha = case[\"alpha\"]\n\n        # Step 1: Determine the degrees of freedom.\n        # m is the dimension of the observation space.\n        m = H.shape[0]\n\n        # Step 2: Compute the innovation vector.\n        # v = y - H * x_f\n        v = y - H @ x_f\n        \n        # Step 3: Compute the innovation covariance matrix.\n        # S = H * P_f * H^T + R\n        S = H @ P_f @ H.T + R\n        \n        # Step 4: Compute the test statistic.\n        # z = v^T * S^-1 * v\n        S_inv = np.linalg.inv(S)\n        # Reshape v to be a column vector for correct matrix multiplication if it's 1D\n        if v.ndim == 1:\n            v_col = v[:, np.newaxis]\n            z = (v_col.T @ S_inv @ v_col)[0, 0]\n        else:\n            z = (v.T @ S_inv @ v)[0, 0]\n\n        # Step 5: Determine the critical value.\n        # c is the upper (1-alpha) quantile of the Chi-squared distribution.\n        c = chi2.ppf(1 - alpha, df=m)\n        \n        # Step 6: Apply the decision rule.\n        # Reject if z > c.\n        is_rejected = z > c\n        \n        # The result must be a standard Python boolean\n        results.append(bool(is_rejected))\n\n    # Format output as a string representation of a list of booleans,\n    # with 'True' and 'False' (capitalized), as per Python's str(bool).\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2382619"}, {"introduction": "Real-world systems often rely on multiple sensors with different sampling rates and characteristics, such as a fast-updating IMU and a slower GPS. This practice [@problem_id:2382633] challenges you to fuse these asynchronous data streams to track the position and velocity of a moving object. You will design a filter that operates on an event-driven timeline, performing predictions between sensor readings and updates whenever new data arrives, reflecting a common and powerful pattern in modern sensor fusion applications.", "problem": "Implement a program that computes the posterior state mean for a one-dimensional point mass subject to asynchronous sensor data, with Inertial Measurement Unit (IMU) and Global Positioning System (GPS) sensors providing data at different, irregular time steps. The state consists of position and velocity. The model is linear and time-varying due to variable time steps. Let the state be $x(t) = \\begin{bmatrix} p(t) \\\\ v(t) \\end{bmatrix}$, with $p(t)$ in meters and $v(t)$ in meters per second. Between any two times $t_{k}$ and $t_{k+1}$, with time step $\\Delta t = t_{k+1} - t_{k}$, the kinematic evolution uses the measured IMU acceleration $u_{k}$ (in meters per second squared) held constant over the interval. The discrete-time evolution is\n$$\nx_{k+1} = F(\\Delta t)\\,x_{k} + G(\\Delta t)\\,u_{k} + w_{k},\n$$\nwhere\n$$\nF(\\Delta t) = \\begin{bmatrix} 1 & \\Delta t \\\\ 0 & 1 \\end{bmatrix}, \\quad G(\\Delta t) = \\begin{bmatrix} \\tfrac{1}{2}\\Delta t^{2} \\\\ \\Delta t \\end{bmatrix},\n$$\nand the process noise $w_{k}$ is zero-mean Gaussian with covariance\n$$\nQ(\\Delta t) = \\sigma_{a}^{2}\\,G(\\Delta t)\\,G(\\Delta t)^{\\mathsf T}.\n$$\nThe GPS provides position measurements at irregular times $t_{m}$ according to\n$$\nz_{m} = H\\,x(t_{m}) + v_{m}, \\quad H = \\begin{bmatrix} 1 & 0 \\end{bmatrix},\n$$\nwith zero-mean Gaussian measurement noise $v_{m}$ of variance $R = \\sigma_{g}^{2}$. The IMU measurement $u_{k}$ is used as the input and its uncertainty is modeled by the process noise above; there is no separate measurement equation for the IMU.\n\nAt the initial time $t_{0}$, the prior state mean and covariance are given as $x(t_{0}) \\sim \\mathcal{N}(m_{0}, P_{0})$.\n\nYour program must:\n- Evolve the state and its covariance over the union of all IMU and GPS timestamps, using the most recent available IMU acceleration value as the input $u_{k}$ for each interval $[t_{k}, t_{k+1})$.\n- When a GPS measurement time is reached, combine it with the current predicted state to produce the posterior state mean and covariance at that time, consistent with the probabilistic model above.\n- Continue until the final event time (the latest time present in the union of the IMU and GPS timestamps) and report the final posterior position mean $p$.\n\nTest suite. For each case below, use the provided values verbatim. All times are in seconds, positions in meters, velocities in meters per second, and accelerations in meters per second squared. Initial covariances are variances on the diagonal. The standard deviations $\\sigma_{a}$ and $\\sigma_{g}$ are in the same units as acceleration and position, respectively.\n\nCase A (general irregular sampling with moderate noise):\n- IMU times $t_{\\text{imu}} = [\\,0.00, 0.07, 0.15, 0.26, 0.38, 0.53, 0.71, 0.93, 1.18\\,]$,\n- IMU accelerations $u = [\\,0.52, 0.48, 0.51, 0.50, 0.49, 0.50, 0.51, 0.50, 0.49\\,]$,\n- GPS times $t_{\\text{gps}} = [\\,0.50, 1.00, 1.20\\,]$,\n- GPS positions $z = [\\,0.064, 0.250, 0.360\\,]$,\n- Initial mean $m_{0} = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix}$,\n- Initial covariance $P_{0} = \\operatorname{diag}([\\,0.25, 0.25\\,])$,\n- Acceleration standard deviation $\\sigma_{a} = 0.05$,\n- GPS position standard deviation $\\sigma_{g} = 0.02$.\n\nCase B (long gap before a single GPS update and higher acceleration noise):\n- IMU times $t_{\\text{imu}} = [\\,0.00, 0.04, 0.09, 0.17, 0.28, 0.44, 0.65, 0.91, 1.30, 1.80, 2.30\\,]$,\n- IMU accelerations $u = [\\,-0.31, -0.29, -0.30, -0.32, -0.28, -0.30, -0.29, -0.31, -0.30, -0.29, -0.31\\,]$,\n- GPS time $t_{\\text{gps}} = [\\,2.30\\,]$,\n- GPS position $z = [\\,-0.800\\,]$,\n- Initial mean $m_{0} = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix}$,\n- Initial covariance $P_{0} = \\operatorname{diag}([\\,1.0, 1.0\\,])$,\n- Acceleration standard deviation $\\sigma_{a} = 0.10$,\n- GPS position standard deviation $\\sigma_{g} = 0.05$.\n\nCase C (edge case with sparse IMU and a large gap, precise GPS):\n- IMU times $t_{\\text{imu}} = [\\,0.00, 0.20, 0.21, 0.70, 1.50\\,]$,\n- IMU accelerations $u = [\\,0.00, 0.00, 1.00, 1.00, 0.00\\,]$,\n- GPS times $t_{\\text{gps}} = [\\,0.50, 1.50\\,]$,\n- GPS positions $z = [\\,0.042, 0.832\\,]$,\n- Initial mean $m_{0} = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix}$,\n- Initial covariance $P_{0} = \\operatorname{diag}([\\,0.04, 0.04\\,])$,\n- Acceleration standard deviation $\\sigma_{a} = 0.05$,\n- GPS position standard deviation $\\sigma_{g} = 0.01$.\n\nFor each case, compute the final posterior position mean at the last event time in meters. Express each result in meters as a floating-point number rounded to six decimal places.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[\\,\\text{result1},\\text{result2},\\text{result3}\\,]$), where the entries are the final posterior position means of Case A, Case B, and Case C, respectively, in meters rounded to six decimal places.", "solution": "The problem presented is valid. It is a well-defined application of a linear Kalman filter for sensor fusion with asynchronous data, a standard task in computational engineering and control theory. The physical model is based on elementary kinematics, and all required parameters and data are provided. No scientific principles are violated.\n\nThe problem requires the estimation of the state of a one-dimensional point mass, where the state is defined by the position $p(t)$ and velocity $v(t)$, denoted by the vector $x(t) = \\begin{bmatrix} p(t) \\\\ v(t) \\end{bmatrix}$. The system is described by a linear time-varying state-space model due to irregular time intervals between events. The process of estimating the state from noisy measurements is a classic filtering problem, for which the Kalman filter provides an optimal solution in the linear-Gaussian case.\n\nThe core of the solution lies in processing all sensor events—from both the Inertial Measurement Unit (IMU) and the Global Positioning System (GPS)—in a single, chronological sequence. This is achieved by creating a unified timeline of unique event timestamps from both sensors. The filter then proceeds from one event time to the next, iteratively applying prediction and update steps.\n\nLet the state estimate at time $t_{k}$ be characterized by a Gaussian distribution with mean $m_{k}$ and covariance $P_{k}$. The filter propagates this estimate to the next event time $t_{k+1}$.\n\nFirst, the **prediction step** evolves the state from time $t_{k}$ to $t_{k+1}$. The time interval is $\\Delta t = t_{k+1} - t_{k}$. The predicted mean $m_{k+1|k}$ and covariance $P_{k+1|k}$ at time $t_{k+1}$ are given by:\n$$\nm_{k+1|k} = F(\\Delta t) m_{k} + G(\\Delta t) u_{k}\n$$\n$$\nP_{k+1|k} = F(\\Delta t) P_{k} F(\\Delta t)^{\\mathsf T} + Q(\\Delta t)\n$$\nwhere $m_k$ and $P_k$ are the posterior mean and covariance from the previous step at time $t_k$. The control input $u_{k}$ is the most recent IMU acceleration measurement available at or before time $t_{k}$. The state transition matrix $F(\\Delta t)$, control-input matrix $G(\\Delta t)$, and process noise covariance $Q(\\Delta t)$ are defined as:\n$$\nF(\\Delta t) = \\begin{bmatrix} 1 & \\Delta t \\\\ 0 & 1 \\end{bmatrix}, \\quad G(\\Delta t) = \\begin{bmatrix} \\frac{1}{2}\\Delta t^2 \\\\ \\Delta t \\end{bmatrix}, \\quad Q(\\Delta t) = \\sigma_{a}^{2} G(\\Delta t) G(\\Delta t)^{\\mathsf T}\n$$\nThe matrix $Q(\\Delta t)$ represents the uncertainty introduced by the acceleration input over the interval $\\Delta t$.\n\nSecond, if a GPS measurement $z_{k+1}$ is available at time $t_{k+1}$, a **measurement update step** is performed to correct the predicted state using this new information. The update equations are:\n$$\ny_{k+1} = z_{k+1} - H m_{k+1|k} \\quad (\\text{Innovation})\n$$\n$$\nS_{k+1} = H P_{k+1|k} H^{\\mathsf T} + R \\quad (\\text{Innovation Covariance})\n$$\n$$\nK_{k+1} = P_{k+1|k} H^{\\mathsf T} S_{k+1}^{-1} \\quad (\\text{Kalman Gain})\n$$\nThe posterior (updated) mean $m_{k+1}$ and covariance $P_{k+1}$ are then computed as:\n$$\nm_{k+1} = m_{k+1|k} + K_{k+1} y_{k+1}\n$$\n$$\nP_{k+1} = (I - K_{k+1} H) P_{k+1|k}\n$$\nHere, $H = \\begin{bmatrix} 1 & 0 \\end{bmatrix}$ is the observation matrix that maps the state to the measured quantity (position), and $R = \\sigma_{g}^{2}$ is the measurement noise variance. The identity matrix is denoted by $I$. If no GPS measurement is available at $t_{k+1}$, the update step is skipped, and the posterior state is simply the predicted state, i.e., $m_{k+1} = m_{k+1|k}$ and $P_{k+1} = P_{k+1|k}$.\n\nThe algorithm proceeds as follows:\n1.  Initialize the state mean $m$ and covariance $P$ with the prior values $m_{0}$ and $P_{0}$ at the initial time $t_{0}$.\n2.  Aggregate all unique timestamps from the IMU ($t_{\\text{imu}}$) and GPS ($t_{\\text{gps}}$) into a single, sorted list of event times.\n3.  Set the initial control input $u$ to the value measured at $t_{0}$.\n4.  Iterate through the sorted event times from $t_0$ to the final time. For each interval $[t_k, t_{k+1})$:\n    a. Calculate $\\Delta t = t_{k+1} - t_k$. If $\\Delta t > 0$, perform the prediction step to propagate the state from $t_k$ to $t_{k+1}$.\n    b. At time $t_{k+1}$, check for a GPS measurement. If one exists, perform the measurement update step.\n    c. At time $t_{k+1}$, check for a new IMU measurement. If one exists, update the control input $u$ to this new value for the subsequent interval.\n5.  After processing all events up to the final timestamp, the first component of the final state mean vector, $m[0]$, is the required posterior position.\n\nThis procedure correctly fuses the information from both asynchronous sensors to produce an optimal estimate of the system's state over time. The implementation requires careful handling of the event timeline and the state propagation between events.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the sensor fusion problem for three test cases.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"name\": \"Case A\",\n            \"t_imu\": np.array([0.00, 0.07, 0.15, 0.26, 0.38, 0.53, 0.71, 0.93, 1.18]),\n            \"u\": np.array([0.52, 0.48, 0.51, 0.50, 0.49, 0.50, 0.51, 0.50, 0.49]),\n            \"t_gps\": np.array([0.50, 1.00, 1.20]),\n            \"z\": np.array([0.064, 0.250, 0.360]),\n            \"m0\": np.array([0.0, 0.0]),\n            \"P0\": np.diag([0.25, 0.25]),\n            \"sigma_a\": 0.05,\n            \"sigma_g\": 0.02,\n        },\n        {\n            \"name\": \"Case B\",\n            \"t_imu\": np.array([0.00, 0.04, 0.09, 0.17, 0.28, 0.44, 0.65, 0.91, 1.30, 1.80, 2.30]),\n            \"u\": np.array([-0.31, -0.29, -0.30, -0.32, -0.28, -0.30, -0.29, -0.31, -0.30, -0.29, -0.31]),\n            \"t_gps\": np.array([2.30]),\n            \"z\": np.array([-0.800]),\n            \"m0\": np.array([0.0, 0.0]),\n            \"P0\": np.diag([1.0, 1.0]),\n            \"sigma_a\": 0.10,\n            \"sigma_g\": 0.05,\n        },\n        {\n            \"name\": \"Case C\",\n            \"t_imu\": np.array([0.00, 0.20, 0.21, 0.70, 1.50]),\n            \"u\": np.array([0.00, 0.00, 1.00, 1.00, 0.00]),\n            \"t_gps\": np.array([0.50, 1.50]),\n            \"z\": np.array([0.042, 0.832]),\n            \"m0\": np.array([0.0, 0.0]),\n            \"P0\": np.diag([0.04, 0.04]),\n            \"sigma_a\": 0.05,\n            \"sigma_g\": 0.01,\n        },\n    ]\n\n    results = [run_kalman_filter(case) for case in test_cases]\n    \n    # Format the final output string exactly as specified.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\ndef run_kalman_filter(case_data):\n    \"\"\"\n    Runs the Kalman filter for a single test case.\n    \"\"\"\n    # Unpack data for the current case\n    t_imu = case_data[\"t_imu\"]\n    u_imu = case_data[\"u\"]\n    t_gps = case_data[\"t_gps\"]\n    z_gps = case_data[\"z\"]\n    m = case_data[\"m0\"].reshape(2, 1)\n    P = case_data[\"P0\"]\n    sigma_a = case_data[\"sigma_a\"]\n    sigma_g = case_data[\"sigma_g\"]\n\n    # Create lookup dictionaries for sensor data\n    imu_data = dict(zip(t_imu, u_imu))\n    gps_data = dict(zip(t_gps, z_gps))\n\n    # Combine and sort all unique event timestamps\n    event_times = sorted(list(set(t_imu) | set(t_gps)))\n\n    current_time = event_times[0]\n    # Initialize control input with the value at the first timestamp\n    u_current = imu_data[current_time]\n\n    H = np.array([[1.0, 0.0]])\n    R = np.array([[sigma_g**2]])\n    I = np.eye(2)\n\n    # Main loop: iterate through consecutive event times\n    for i in range(len(event_times) - 1):\n        next_time = event_times[i+1]\n        delta_t = next_time - current_time\n\n        if delta_t > 1e-9: # Propagate only if time has passed\n            # Prediction step\n            F = np.array([[1.0, delta_t], [0.0, 1.0]])\n            G = np.array([[0.5 * delta_t**2], [delta_t]])\n            Q = (sigma_a**2) * (G @ G.T)\n\n            # Predict state mean and covariance\n            m_pred = F @ m + G * u_current\n            P_pred = F @ P @ F.T + Q\n            \n            m, P = m_pred, P_pred\n\n        # Update step if GPS data is available at the new time\n        if next_time in gps_data:\n            z_meas = gps_data[next_time]\n            \n            # Innovation\n            y = z_meas - H @ m\n            \n            # Innovation covariance\n            S = H @ P @ H.T + R\n            \n            # Kalman gain\n            K = P @ H.T / S[0,0] # S is 1x1\n            \n            # Update state mean and covariance\n            m = m + K * y\n            P = (I - K @ H) @ P\n\n        # Update control input if new IMU data is available\n        if next_time in imu_data:\n            u_current = imu_data[next_time]\n            \n        current_time = next_time\n\n    # The final posterior position is the first element of the mean vector\n    return m[0, 0]\n\nsolve()\n```", "id": "2382633"}]}