{"hands_on_practices": [{"introduction": "The Central Limit Theorem provides a cornerstone result for Monte Carlo methods, telling us that the error of the sample mean estimator typically decreases proportionally to $n^{-1/2}$, where $n$ is the number of samples. While this theoretical rate is universal for many problems, seeing it emerge from data is a powerful learning experience. This first practice ([@problem_id:2414889]) guides you through a numerical experiment to empirically verify this convergence rate for several different probability distributions, reinforcing the foundational theory of Monte Carlo integration.", "problem": "A two-dimensional metal plate occupies the unit square domain with total area equal to $1$. Time is measured in discrete steps indexed by $k \\in \\mathbb{N}$. At each time index $k$, the plate’s boundary temperature is spatially uniform and equal to a random temperature $X_k$ (in Kelvin), where $\\{X_k\\}_{k \\ge 1}$ are independent and identically distributed random variables with a distribution specified per test case. Assume that thermal conduction is sufficiently fast relative to the time between boundary changes so that, at each time index $k$, the plate’s internal temperature field is spatially uniform and equal to the boundary temperature $X_k$. Consequently, the spatially averaged temperature of the plate at time index $k$ equals $X_k$, and the long-time average plate temperature equals the mathematical expectation $\\mu = \\mathbb{E}[X_1]$ (in Kelvin). The estimation of $\\mu$ is a Monte Carlo integration problem over the distribution of $X_1$.\n\nFor each test case below, consider the following definitions. For a positive integer $n$, define the estimator $\\widehat{\\mu}_n = \\frac{1}{n} \\sum_{i=1}^{n} X_i$, where $X_1, \\dots, X_n$ are independent samples drawn from the law of $X_1$ specified by the test case. For a list of sample sizes $\\{n_j\\}_{j=1}^{J}$ and a positive integer $R$, define, for each $n_j$, the empirical mean absolute error\n$$\nM_{n_j} \\;=\\; \\frac{1}{R} \\sum_{r=1}^{R} \\left| \\widehat{\\mu}^{(r)}_{n_j} - \\mu \\right|,\n$$\nwhere $\\widehat{\\mu}^{(r)}_{n_j}$ denotes an independent replicate of $\\widehat{\\mu}_{n_j}$. Define the empirical convergence rate exponent $\\widehat{r}$ as the least-squares slope of the linear fit $y = a + \\widehat{r}\\,x$ to the points $\\left(x_j, y_j\\right)$ with $x_j = \\ln(n_j)$ and $y_j = \\ln(M_{n_j})$, where $\\ln(\\cdot)$ denotes the natural logarithm. The quantity $\\widehat{r}$ is dimensionless. Each test case also specifies a seed $s$ for a pseudorandom number generator to ensure reproducibility.\n\nTest Suite:\n- Test case $1$: $X_1 \\sim \\mathrm{Uniform}([0,1])$. The exact mean is $\\mu = \\frac{1}{2}$ (in Kelvin). Use the sample sizes $n \\in \\{100, 300, 1000, 3000, 10000\\}$, the number of independent replicates $R = 400$, and the seed $s = 12345$.\n- Test case $2$: $X_1 \\sim \\mathrm{Beta}(\\alpha,\\beta)$ with $\\alpha = \\frac{1}{2}$ and $\\beta = \\frac{1}{2}$, supported on $[0,1]$. The exact mean is $\\mu = \\frac{\\alpha}{\\alpha+\\beta} = \\frac{1}{2}$ (in Kelvin). Use the sample sizes $n \\in \\{100, 300, 1000, 3000, 10000\\}$, the number of independent replicates $R = 400$, and the seed $s = 2023$.\n- Test case $3$: $X_1$ is a two-point mixture: $X_1 = 0$ with probability $p = \\frac{9}{10}$ and $X_1 = 1$ with probability $1-p = \\frac{1}{10}$. The exact mean is $\\mu = \\frac{1}{10}$ (in Kelvin). Use the sample sizes $n \\in \\{100, 300, 1000, 3000, 10000\\}$, the number of independent replicates $R = 400$, and the seed $s = 777$.\n- Test case $4$: $X_1 \\sim \\mathrm{Uniform}([0.49, 0.51])$. The exact mean is $\\mu = 0.5$ (in Kelvin). Use the sample sizes $n \\in \\{30, 100, 300, 1000, 3000\\}$, the number of independent replicates $R = 400$, and the seed $s = 4242$.\n\nAngle units are not applicable. All temperatures are in Kelvin. The final reported quantities are the four real numbers $\\widehat{r}_1, \\widehat{r}_2, \\widehat{r}_3, \\widehat{r}_4$, one per test case, each equal to the empirical convergence rate exponent defined above. Express each reported number as a decimal (no percent sign), rounded to $3$ decimal places.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases $1$ through $4$, for example, $[\\widehat{r}_1,\\widehat{r}_2,\\widehat{r}_3,\\widehat{r}_4]$.", "solution": "The problem statement is evaluated and found to be valid. It is scientifically grounded in the theory of Monte Carlo methods, well-posed with all necessary information provided, and objective in its formulation. No inconsistencies, ambiguities, or factual unsoundness are present.\n\nThe problem concerns the numerical estimation of the convergence rate for a Monte Carlo estimator of a mean. The physical framing involving a metal plate is an analogy; the core task is a statistical one. For each test case, we are asked to estimate the mean $\\mu = \\mathbb{E}[X_1]$ of a random variable $X_1$ using the sample mean estimator $\\widehat{\\mu}_n = \\frac{1}{n} \\sum_{i=1}^{n} X_i$, based on $n$ independent and identically distributed samples $X_1, \\dots, X_n$.\n\nThe rate of convergence of this estimator is a central result in computational statistics. According to the Central Limit Theorem (CLT), for a sequence of i.i.d. random variables $\\{X_i\\}$ with finite mean $\\mu$ and finite, non-zero variance $\\sigma^2 = \\mathrm{Var}(X_1)$, the distribution of the sample mean estimator converges to a normal distribution. Specifically, the random variable $\\sqrt{n}(\\widehat{\\mu}_n - \\mu)$ converges in distribution to a normal random variable $\\mathcal{N}(0, \\sigma^2)$ as $n \\to \\infty$.\n\nThis implies that the error of the estimator, $\\widehat{\\mu}_n - \\mu$, is approximately distributed as $\\mathcal{N}(0, \\sigma^2/n)$ for large $n$. The problem defines the mean absolute error, whose theoretical value is $M_n = \\mathbb{E}[|\\widehat{\\mu}_n - \\mu|]$. Using the normal approximation, we can relate $M_n$ to the sample size $n$:\n$$\nM_n = \\mathbb{E}[|\\widehat{\\mu}_n - \\mu|] \\approx \\mathbb{E}\\left[\\left|\\frac{\\sigma}{\\sqrt{n}} Z\\right|\\right] = \\frac{\\sigma}{\\sqrt{n}} \\mathbb{E}[|Z|]\n$$\nwhere $Z \\sim \\mathcal{N}(0,1)$ is a standard normal random variable. The quantity $\\mathbb{E}[|Z|] = \\sqrt{2/\\pi}$ is a constant. Therefore, we have the asymptotic relationship:\n$$\nM_n \\propto n^{-1/2}\n$$\nThis proportionality demonstrates that the mean absolute error of the standard Monte Carlo estimator converges to zero at a rate of $n^{-1/2}$.\n\nThe problem requires us to find an empirical estimate of this convergence rate exponent. This is done by performing a linear least-squares fit to the logarithmically transformed data points $(x_j, y_j)$, where $x_j = \\ln(n_j)$ and $y_j = \\ln(M_{n_j})$. Taking the natural logarithm of the asymptotic relationship gives:\n$$\n\\ln(M_n) \\approx \\ln(C) - \\frac{1}{2} \\ln(n)\n$$\nwhere $C$ is a constant of proportionality. This is a linear equation of the form $y = a + \\widehat{r} x$, with a theoretical slope (convergence rate exponent) of $\\widehat{r} = -1/2 = -0.5$.\n\nThis theoretical rate of $-0.5$ is universal for Monte Carlo integration of functions with respect to distributions that have a finite variance. All four distributions specified in the test cases possess a finite variance:\n$1$. For $X_1 \\sim \\mathrm{Uniform}([0,1])$, $\\sigma^2 = 1/12$.\n$2$. For $X_1 \\sim \\mathrm{Beta}(1/2, 1/2)$, $\\sigma^2 = 1/8$.\n$3$. For the two-point mixture (a Bernoulli trial), $\\sigma^2 = p(1-p) = (1/10)(9/10) = 9/100$.\n$4$. For $X_1 \\sim \\mathrm{Uniform}([0.49, 0.51])$, $\\sigma^2 = (0.51-0.49)^2/12 = (0.02)^2/12 \\approx 3.33 \\times 10^{-5}$.\n\nSince all variances are finite, the theoretical convergence rate exponent is $-0.5$ in all four test cases. The procedure described in the problem is a numerical experiment to verify this theoretical rate. We will implement this procedure for each test case.\n\nThe algorithm for each test case is as follows:\n$1$. Set the pseudorandom number generator seed $s$ for reproducibility.\n$2$. For each sample size $n_j$ in the provided list:\n    a. Perform $R$ independent simulation replicates.\n    b. In each replicate $r \\in \\{1, \\dots, R\\}$, generate $n_j$ random samples from the specified distribution for $X_1$.\n    c. Compute the sample mean $\\widehat{\\mu}^{(r)}_{n_j}$.\n    d. Calculate the absolute error $|\\widehat{\\mu}^{(r)}_{n_j} - \\mu|$, where $\\mu$ is the given exact mean.\n    e. After $R$ replicates, compute the empirical mean absolute error $M_{n_j} = \\frac{1}{R} \\sum_{r=1}^{R} |\\widehat{\\mu}^{(r)}_{n_j} - \\mu|$.\n$3$. Create a set of data points $(x_j, y_j)$ where $x_j = \\ln(n_j)$ and $y_j = \\ln(M_{n_j})$.\n$4$. Perform a simple linear regression on these points to find the slope $\\widehat{r}$ of the best-fit line $y = a + \\widehat{r}x$. This slope is the empirical convergence rate exponent.\n$5$. The result is then rounded to $3$ decimal places.\n\nThis procedure will be executed for all four test cases to obtain the values $\\widehat{r}_1, \\widehat{r}_2, \\widehat{r}_3, \\widehat{r}_4$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Monte Carlo convergence rate problem for all test cases.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"name\": \"Uniform(0,1)\",\n            \"dist_gen\": lambda rng, size: rng.uniform(0, 1, size),\n            \"mu\": 0.5,\n            \"n_values\": [100, 300, 1000, 3000, 10000],\n            \"R\": 400,\n            \"seed\": 12345\n        },\n        {\n            \"name\": \"Beta(0.5, 0.5)\",\n            \"dist_gen\": lambda rng, size: rng.beta(0.5, 0.5, size),\n            \"mu\": 0.5,\n            \"n_values\": [100, 300, 1000, 3000, 10000],\n            \"R\": 400,\n            \"seed\": 2023\n        },\n        {\n            \"name\": \"Two-point mixture\",\n            \"dist_gen\": lambda rng, size: rng.choice([0, 1], size=size, p=[0.9, 0.1]),\n            \"mu\": 0.1,\n            \"n_values\": [100, 300, 1000, 3000, 10000],\n            \"R\": 400,\n            \"seed\": 777\n        },\n        {\n            \"name\": \"Uniform(0.49, 0.51)\",\n            \"dist_gen\": lambda rng, size: rng.uniform(0.49, 0.51, size),\n            \"mu\": 0.5,\n            \"n_values\": [30, 100, 300, 1000, 3000],\n            \"R\": 400,\n            \"seed\": 4242\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        dist_gen = case[\"dist_gen\"]\n        mu = case[\"mu\"]\n        n_values = case[\"n_values\"]\n        R = case[\"R\"]\n        seed = case[\"seed\"]\n\n        rng = np.random.default_rng(seed)\n        \n        log_n_values = []\n        log_M_n_values = []\n\n        for n in n_values:\n            # Store absolute errors for R replicates\n            absolute_errors = np.zeros(R)\n            \n            for r in range(R):\n                # Generate n samples\n                samples = dist_gen(rng, n)\n                # Compute sample mean\n                mu_hat = np.mean(samples)\n                # Compute absolute error\n                absolute_errors[r] = np.abs(mu_hat - mu)\n            \n            # Compute empirical mean absolute error M_n\n            M_n = np.mean(absolute_errors)\n            \n            # Store log-transformed values for regression\n            log_n_values.append(np.log(n))\n            log_M_n_values.append(np.log(M_n))\n            \n        # Perform linear least-squares regression on (log(n), log(M_n))\n        # np.polyfit returns [slope, intercept] for degree 1\n        x = np.array(log_n_values)\n        y = np.array(log_M_n_values)\n        \n        # The slope r_hat is the first element of the result\n        r_hat = np.polyfit(x, y, 1)[0]\n        \n        results.append(r_hat)\n\n    # Format the final output as specified\n    formatted_results = [f\"{r:.3f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2414889"}, {"introduction": "The standard $n^{-1/2}$ convergence rate can be slow. Importance sampling is a powerful variance reduction technique that can significantly accelerate convergence by concentrating samples in the regions that contribute most to the integral. The effectiveness of this method hinges entirely on the choice of the proposal distribution, and a poor choice can even increase the variance. This exercise ([@problem_id:2414928]) asks you to analytically derive and compare the performance of two different proposal distributions, providing insight into how to design an efficient Monte Carlo estimator.", "problem": "Consider the integral\n$$\nI(a) \\;=\\; \\int_{-\\infty}^{\\infty} e^{-x^{2}} \\,\\sin^{2}(a\\,x)\\,dx,\n$$\nwhere $a \\ge 0$ is a real-valued frequency parameter and angles are measured in radians. Let $f_{a}(x) = e^{-x^{2}}\\sin^{2}(a\\,x)$ on $(-\\infty,\\infty)$. Define two absolutely continuous proposal densities on $(-\\infty,\\infty)$:\n$$\nq_{1}(x) \\;=\\; \\frac{1}{\\sqrt{\\pi}}\\,e^{-x^{2}}, \n\\qquad\nq_{2}(x) \\;=\\; \\frac{1}{\\sqrt{2\\pi}}\\,e^{-x^{2}/2}.\n$$\nFor any $a \\ge 0$ and any sample size $N \\in \\mathbb{N}$, consider the standard importance-sampling estimator of $I(a)$ based on independent samples drawn from $q(x) \\in \\{q_{1}(x),q_{2}(x)\\}$. Let the single-sample importance weight be $Y_{q}(a;X) = f_{a}(X)/q(X)$ for $X \\sim q$, and denote the estimator by the sample mean of $N$ independent copies of $Y_{q}(a;X)$. The standard error for a given $q$ and $N$ is defined as the standard deviation of this estimator.\n\nYour tasks are:\n- Derive $I(a)$ exactly.\n- Derive, in closed form, the exact standard error for the estimator when $q(x)=q_{1}(x)$ and when $q(x)=q_{2}(x)$, as functions of $a$ and $N$.\n\nUse your derivations to compute the required outputs for the following test suite of $(a,N)$ pairs:\n- Test case $1$: $a = 100$, $N = 10000$.\n- Test case $2$: $a = 0.5$, $N = 5000$.\n- Test case $3$: $a = 0$, $N = 1234$.\n\nRequired final output format:\n- Your program must produce a single line containing a comma-separated list enclosed in square brackets.\n- For each test case, in the given order, output three real numbers in the following order: the exact value $I(a)$, the exact standard error for $q_{1}(x)$ with the given $N$, and the exact standard error for $q_{2}(x)$ with the given $N$.\n- Thus, the final output must be a single list of length $9$:\n$$\n\\big[ I(a_{1}),\\ \\mathrm{SE}_{q_{1}}(a_{1},N_{1}),\\ \\mathrm{SE}_{q_{2}}(a_{1},N_{1}),\\ I(a_{2}),\\ \\mathrm{SE}_{q_{1}}(a_{2},N_{2}),\\ \\mathrm{SE}_{q_{2}}(a_{2},N_{2}),\\ I(a_{3}),\\ \\mathrm{SE}_{q_{1}}(a_{3},N_{3}),\\ \\mathrm{SE}_{q_{2}}(a_{3},N_{3}) \\big].\n$$\nAll outputs must be real numbers (as decimals). No physical units apply in this problem.", "solution": "The problem as stated is mathematically and scientifically sound. It is well-posed, complete, and devoid of ambiguity. We shall therefore proceed with its solution.\n\nThe task is to derive the exact value of an integral $I(a)$ and the exact standard errors for two different importance-sampling estimators of this integral.\n\nThe integral is given by\n$$I(a) = \\int_{-\\infty}^{\\infty} e^{-x^{2}} \\sin^{2}(a\\,x)\\,dx$$\nwhere $a \\ge 0$. The integrand is $f_{a}(x) = e^{-x^{2}}\\sin^{2}(a\\,x)$.\n\n**Part 1: Exact Derivation of $I(a)$**\n\nTo evaluate $I(a)$, we employ the trigonometric identity $\\sin^2(\\theta) = \\frac{1 - \\cos(2\\theta)}{2}$. Applying this to the integrand gives:\n$$I(a) = \\int_{-\\infty}^{\\infty} e^{-x^{2}} \\left(\\frac{1 - \\cos(2ax)}{2}\\right) dx$$\nWe can separate this into two integrals:\n$$I(a) = \\frac{1}{2} \\int_{-\\infty}^{\\infty} e^{-x^{2}} dx - \\frac{1}{2} \\int_{-\\infty}^{\\infty} e^{-x^{2}} \\cos(2ax)\\,dx$$\nThe first integral is the standard Gaussian integral, whose value is known:\n$$\\int_{-\\infty}^{\\infty} e^{-x^{2}} dx = \\sqrt{\\pi}$$\nThe second integral can be solved by considering the Fourier transform of a Gaussian function. The Fourier transform of $g(x) = e^{-bx^2}$ is given by $\\hat{g}(k) = \\int_{-\\infty}^{\\infty} e^{-bx^2} e^{-ikx} dx = \\sqrt{\\frac{\\pi}{b}} e^{-k^2/(4b)}$. The integral in our expression is the real part of such a transform with a positive sign in the exponent, which yields the same result for a real and even function.\n$$\\int_{-\\infty}^{\\infty} e^{-x^2} \\cos(2ax) dx = \\mathrm{Re}\\left[ \\int_{-\\infty}^{\\infty} e^{-x^2} e^{i(2a)x} dx \\right]$$\nUsing the Fourier transform formula with $b=1$ and $k=2a$, we find:\n$$\\int_{-\\infty}^{\\infty} e^{-x^2} \\cos(2ax) dx = \\sqrt{\\pi} e^{-(2a)^2/4} = \\sqrt{\\pi} e^{-a^2}$$\nSubstituting these results back into the expression for $I(a)$, we obtain:\n$$I(a) = \\frac{1}{2} \\sqrt{\\pi} - \\frac{1}{2} \\sqrt{\\pi} e^{-a^2} = \\frac{\\sqrt{\\pi}}{2} (1 - e^{-a^2})$$\n\n**Part 2: Derivation of the Standard Error**\n\nThe importance sampling estimator for $I(a)$ using $N$ samples $X_i \\sim q(x)$ is $\\hat{I}_N = \\frac{1}{N} \\sum_{i=1}^{N} Y_i$, where $Y_i = \\frac{f_a(X_i)}{q(X_i)}$ are independent and identically distributed random variables. The standard error of this estimator is defined as its standard deviation:\n$$\\mathrm{SE}(\\hat{I}_N) = \\mathrm{Std}\\left( \\frac{1}{N} \\sum_{i=1}^{N} Y_i \\right) = \\frac{\\mathrm{Std}(Y_1)}{\\sqrt{N}} = \\frac{\\sqrt{\\mathrm{Var}(Y_1)}}{\\sqrt{N}}$$\nThe variance of a single sample $Y = Y_q(a;X)$ is given by $\\mathrm{Var}(Y) = \\mathbb{E}_{X \\sim q}[Y^2] - (\\mathbb{E}_{X \\sim q}[Y])^2$.\nThe mean of $Y$ is:\n$$\\mathbb{E}_{X \\sim q}[Y] = \\int_{-\\infty}^{\\infty} \\frac{f_a(x)}{q(x)} q(x) dx = \\int_{-\\infty}^{\\infty} f_a(x) dx = I(a)$$\nThus, we only need to compute the second moment, $\\mathbb{E}_{X \\sim q}[Y^2]$:\n$$\\mathbb{E}_{X \\sim q}[Y^2] = \\int_{-\\infty}^{\\infty} \\left(\\frac{f_a(x)}{q(x)}\\right)^2 q(x) dx = \\int_{-\\infty}^{\\infty} \\frac{f_a(x)^2}{q(x)} dx$$\nWe must perform this calculation for each proposal density $q_1(x)$ and $q_2(x)$.\n\n**Case 1: Proposal Density $q_1(x) = \\frac{1}{\\sqrt{\\pi}} e^{-x^2}$**\n\nThe single-sample weight is:\n$$Y_{q_1}(a;x) = \\frac{f_a(x)}{q_1(x)} = \\frac{e^{-x^2} \\sin^2(ax)}{\\frac{1}{\\sqrt{\\pi}} e^{-x^2}} = \\sqrt{\\pi} \\sin^2(ax)$$\nThe second moment is:\n$$\\mathbb{E}[Y_{q_1}^2] = \\int_{-\\infty}^{\\infty} \\frac{(e^{-x^2} \\sin^2(ax))^2}{\\frac{1}{\\sqrt{\\pi}} e^{-x^2}} dx = \\sqrt{\\pi} \\int_{-\\infty}^{\\infty} e^{-x^2} \\sin^4(ax) dx$$\nWe use the power-reduction identity $\\sin^4(\\theta) = \\frac{3 - 4\\cos(2\\theta) + \\cos(4\\theta)}{8}$.\n$$\\mathbb{E}[Y_{q_1}^2] = \\frac{\\sqrt{\\pi}}{8} \\int_{-\\infty}^{\\infty} e^{-x^2} (3 - 4\\cos(2ax) + \\cos(4ax)) dx$$\nThis breaks into three previously established types of integrals:\n$$\\mathbb{E}[Y_{q_1}^2] = \\frac{\\sqrt{\\pi}}{8} \\left( 3\\int_{-\\infty}^{\\infty} e^{-x^2} dx - 4\\int_{-\\infty}^{\\infty} e^{-x^2}\\cos(2ax)dx + \\int_{-\\infty}^{\\infty} e^{-x^2}\\cos(4ax)dx \\right)$$\n$$\\mathbb{E}[Y_{q_1}^2] = \\frac{\\sqrt{\\pi}}{8} \\left( 3\\sqrt{\\pi} - 4\\sqrt{\\pi}e^{-a^2} + \\sqrt{\\pi}e^{-(2a)^2} \\right) = \\frac{\\pi}{8} (3 - 4e^{-a^2} + e^{-4a^2})$$\nThe variance is $\\mathrm{Var}(Y_{q_1}) = \\mathbb{E}[Y_{q_1}^2] - (I(a))^2$:\n$$\\mathrm{Var}(Y_{q_1}) = \\frac{\\pi}{8} (3 - 4e^{-a^2} + e^{-4a^2}) - \\left(\\frac{\\sqrt{\\pi}}{2}(1-e^{-a^2})\\right)^2$$\n$$\\mathrm{Var}(Y_{q_1}) = \\frac{\\pi}{8} (3 - 4e^{-a^2} + e^{-4a^2}) - \\frac{\\pi}{4}(1 - 2e^{-a^2} + e^{-2a^2})$$\n$$\\mathrm{Var}(Y_{q_1}) = \\frac{\\pi}{8} (3 - 4e^{-a^2} + e^{-4a^2} - 2 + 4e^{-a^2} - 2e^{-2a^2}) = \\frac{\\pi}{8} (1 - 2e^{-2a^2} + e^{-4a^2})$$\nThis simplifies to a perfect square:\n$$\\mathrm{Var}(Y_{q_1}) = \\frac{\\pi}{8} (1 - e^{-2a^2})^2$$\nThe standard error for a sample of size $N$ is:\n$$\\mathrm{SE}_{q_1}(a, N) = \\frac{\\sqrt{\\mathrm{Var}(Y_{q_1})}}{\\sqrt{N}} = \\frac{1}{\\sqrt{N}} \\sqrt{\\frac{\\pi}{8}} (1-e^{-2a^2})$$\nSince $a \\ge 0$, the term $1-e^{-2a^2}$ is non-negative.\n\n**Case 2: Proposal Density $q_2(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-x^2/2}$**\n\nThe second moment integral is:\n$$\\mathbb{E}[Y_{q_2}^2] = \\int_{-\\infty}^{\\infty} \\frac{(e^{-x^2} \\sin^2(ax))^2}{\\frac{1}{\\sqrt{2\\pi}} e^{-x^2/2}} dx = \\sqrt{2\\pi} \\int_{-\\infty}^{\\infty} e^{-2x^2} e^{x^2/2} \\sin^4(ax) dx$$\n$$\\mathbb{E}[Y_{q_2}^2] = \\sqrt{2\\pi} \\int_{-\\infty}^{\\infty} e^{-3x^2/2} \\sin^4(ax) dx$$\nAgain, using $\\sin^4(ax) = \\frac{1}{8} (3 - 4\\cos(2ax) + \\cos(4ax))$, we have:\n$$\\mathbb{E}[Y_{q_2}^2] = \\frac{\\sqrt{2\\pi}}{8} \\int_{-\\infty}^{\\infty} e^{-3x^2/2} (3 - 4\\cos(2ax) + \\cos(4ax)) dx$$\nWe evaluate the integrals using $\\int_{-\\infty}^{\\infty} e^{-bx^2} \\cos(kx) dx = \\sqrt{\\frac{\\pi}{b}} e^{-k^2/(4b)}$ with $b=3/2$:\n\\begin{align*}\n\\mathbb{E}[Y_{q_2}^2] = \\frac{\\sqrt{2\\pi}}{8} \\left( 3\\sqrt{\\frac{\\pi}{3/2}} - 4\\sqrt{\\frac{\\pi}{3/2}}e^{-(2a)^2/(4 \\cdot 3/2)} + \\sqrt{\\frac{\\pi}{3/2}}e^{-(4a)^2/(4 \\cdot 3/2)} \\right) \\\\\n= \\frac{\\sqrt{2\\pi}}{8} \\sqrt{\\frac{2\\pi}{3}} \\left( 3 - 4e^{-4a^2/6} + e^{-16a^2/6} \\right) \\\\\n= \\frac{2\\pi}{8\\sqrt{3}} \\left( 3 - 4e^{-2a^2/3} + e^{-8a^2/3} \\right) = \\frac{\\pi}{4\\sqrt{3}} \\left( 3 - 4e^{-2a^2/3} + e^{-8a^2/3} \\right)\n\\end{align*}\nThe variance is $\\mathrm{Var}(Y_{q_2}) = \\mathbb{E}[Y_{q_2}^2] - (I(a))^2$:\n$$\\mathrm{Var}(Y_{q_2}) = \\frac{\\pi}{4\\sqrt{3}} \\left( 3 - 4e^{-2a^2/3} + e^{-8a^2/3} \\right) - \\left( \\frac{\\sqrt{\\pi}}{2}(1-e^{-a^2}) \\right)^2$$\nThe standard error for a sample size of $N$ is:\n$$\\mathrm{SE}_{q_2}(a, N) = \\frac{1}{\\sqrt{N}} \\sqrt{\\frac{\\pi}{4\\sqrt{3}} (3 - 4e^{-2a^2/3} + e^{-8a^2/3}) - \\frac{\\pi}{4}(1-e^{-a^2})^2}$$\n\n**Summary of Formulas for Computation**\n1.  **Exact Integral**: $I(a) = \\frac{\\sqrt{\\pi}}{2} (1 - e^{-a^2})$\n2.  **Standard Error for $q_1$**: $\\mathrm{SE}_{q_1}(a, N) = \\frac{1}{\\sqrt{N}} \\sqrt{\\frac{\\pi}{8}} (1-e^{-2a^2})$\n3.  **Standard Error for $q_2$**: $\\mathrm{SE}_{q_2}(a, N) = \\frac{1}{\\sqrt{N}} \\sqrt{\\frac{\\pi}{4\\sqrt{3}} (3 - 4e^{-2a^2/3} + e^{-8a^2/3}) - \\frac{\\pi}{4}(1-e^{-a^2})^2}$\n\nThese formulas will be implemented to compute the required values for the given test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the exact integral value and standard errors for two Monte Carlo\n    estimators for a given set of test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (100, 10000),  # Test case 1: (a=100, N=10000)\n        (0.5, 5000),   # Test case 2: (a=0.5, N=5000)\n        (0, 1234),     # Test case 3: (a=0, N=1234)\n    ]\n\n    results = []\n    \n    # Store constants to avoid recomputation\n    PI = np.pi\n    SQRT_PI = np.sqrt(PI)\n\n    for a, N in test_cases:\n        # Task 1: Calculate the exact value of the integral I(a)\n        # I(a) = (sqrt(pi)/2) * (1 - exp(-a^2))\n        a_sq = a**2\n        i_a = (SQRT_PI / 2.0) * (1.0 - np.exp(-a_sq))\n\n        # Task 2: Calculate the standard error for proposal q1(x)\n        # SE_q1(a, N) = (1/sqrt(N)) * sqrt(pi/8) * (1 - exp(-2*a^2))\n        se_q1 = (1.0 / np.sqrt(N)) * np.sqrt(PI / 8.0) * (1.0 - np.exp(-2.0 * a_sq))\n\n        # Task 3: Calculate the standard error for proposal q2(x)\n        # Var(Y_q2) = E[Y_q2^2] - (I(a))^2\n        # E[Y_q2^2] = (pi / (4*sqrt(3))) * (3 - 4*exp(-2*a^2/3) + exp(-8*a^2/3))\n        e_y2_sq_term1 = np.exp(-2.0 * a_sq / 3.0)\n        e_y2_sq_term2 = np.exp(-8.0 * a_sq / 3.0)\n        e_y2_sq = (PI / (4.0 * np.sqrt(3.0))) * (3.0 - 4.0 * e_y2_sq_term1 + e_y2_sq_term2)\n        \n        # (I(a))^2 term\n        i_a_sq = i_a**2\n        \n        var_y_q2 = e_y2_sq - i_a_sq\n        \n        # Ensure variance is non-negative to handle potential floating point inaccuracies\n        if var_y_q2  0:\n            var_y_q2 = 0.0\n            \n        se_q2 = np.sqrt(var_y_q2) / np.sqrt(N)\n        \n        results.extend([i_a, se_q1, se_q2])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2414928"}, {"introduction": "Beyond importance sampling, the control variates method offers another elegant way to reduce the variance of a Monte Carlo estimator. The core idea is to find a function that is a close approximation to our integrand but whose integral can be computed exactly; we then apply Monte Carlo to the *difference* between the two functions, which has a much smaller variance. This hands-on problem ([@problem_id:2414893]) walks you through a practical and widely applicable version of this technique, where the control variate is a polynomial found by a least-squares fit to the original integrand.", "problem": "You are given the task of approximating integrals of functions over the unit hypercube $[0,1]^d$ using Monte Carlo (MC) integration, and of accelerating convergence by using a control variate based on a low-order polynomial approximation of the integrand. For each test case in the test suite below, your program must construct a total-degree polynomial control variate and evaluate its impact on accuracy relative to plain Monte Carlo.\n\nDefinitions and setup:\n- Let $f: [0,1]^d \\to \\mathbb{R}$ be an integrable function, and let the target integral be $I = \\int_{[0,1]^d} f(\\boldsymbol{x}) \\,\\mathrm{d}\\boldsymbol{x}$.\n- A total-degree polynomial of degree at most $k$ in $d$ variables is any polynomial of the form $p(\\boldsymbol{x}) = \\sum_{\\lvert \\boldsymbol{\\alpha} \\rvert \\le k} c_{\\boldsymbol{\\alpha}} \\,\\boldsymbol{x}^{\\boldsymbol{\\alpha}}$, where $\\boldsymbol{\\alpha} \\in \\mathbb{N}_0^d$, $\\lvert \\boldsymbol{\\alpha} \\rvert = \\alpha_1 + \\cdots + \\alpha_d$, and $\\boldsymbol{x}^{\\boldsymbol{\\alpha}} = \\prod_{j=1}^d x_j^{\\alpha_j}$.\n- The exact integral of a monomial over $[0,1]^d$ is $\\int_{[0,1]^d} \\boldsymbol{x}^{\\boldsymbol{\\alpha}} \\,\\mathrm{d}\\boldsymbol{x} = \\prod_{j=1}^d \\frac{1}{\\alpha_j + 1}$, hence for $p(\\boldsymbol{x})$ the exact integral is $\\mu_p = \\int_{[0,1]^d} p(\\boldsymbol{x}) \\,\\mathrm{d}\\boldsymbol{x} = \\sum_{\\lvert \\boldsymbol{\\alpha} \\rvert \\le k} c_{\\boldsymbol{\\alpha}} \\prod_{j=1}^d \\frac{1}{\\alpha_j + 1}$.\n- Angles used inside trigonometric functions must be interpreted in radians.\n\nFor each test case:\n- Use $m_{\\text{fit}}$ independent draws $\\{\\boldsymbol{Z}_j\\}_{j=1}^{m_{\\text{fit}}}$ from the uniform distribution on $[0,1]^d$ to determine the coefficients $\\{c_{\\boldsymbol{\\alpha}}\\}$ of the total-degree polynomial $p$ of degree at most $k$ that best fits the data $\\{(\\boldsymbol{Z}_j, f(\\boldsymbol{Z}_j))\\}$ in the least-squares sense.\n- Use an independent set of $n$ draws $\\{\\boldsymbol{Y}_i\\}_{i=1}^n$ from the uniform distribution on $[0,1]^d$ to compute:\n  - the plain Monte Carlo estimator $\\hat{I}_{\\mathrm{MC}} = \\frac{1}{n} \\sum_{i=1}^n f(\\boldsymbol{Y}_i)$,\n  - the control-variate estimator $\\hat{I}_{\\mathrm{CV}} = \\frac{1}{n} \\sum_{i=1}^n \\big(f(\\boldsymbol{Y}_i) - p(\\boldsymbol{Y}_i)\\big) + \\mu_p$.\n- For each case, compute the absolute error of each estimator using the exact integral $I$ of the given $f$. Then compute the ratio $r = \\frac{\\lvert \\hat{I}_{\\mathrm{CV}} - I \\rvert}{\\lvert \\hat{I}_{\\mathrm{MC}} - I \\rvert}$.\n\nIntegrands and their exact integrals:\n- In one dimension ($d = 1$): $f_1(x) = e^{-x} \\cos(7x) + x^3$, with exact integral\n  $$\n  I_1 = \\int_0^1 e^{-x} \\cos(7x)\\,\\mathrm{d}x + \\int_0^1 x^3\\,\\mathrm{d}x = \\Re\\!\\left(\\frac{e^{-1 + i\\,7} - 1}{-1 + i\\,7}\\right) + \\frac{1}{4}.\n  $$\n- In two dimensions ($d = 2$): $f_2(x,y) = \\sin(\\pi x)\\sin(\\pi y) + x\\,y$, with exact integral\n  $$\n  I_2 = \\left(\\int_0^1 \\sin(\\pi x)\\,\\mathrm{d}x\\right)^2 + \\left(\\int_0^1 x\\,\\mathrm{d}x\\right)\\left(\\int_0^1 y\\,\\mathrm{d}y\\right) = \\frac{4}{\\pi^2} + \\frac{1}{4}.\n  $$\n\nTest suite:\n- Case $1$: $f_1$, $d = 1$, polynomial degree $k = 3$, $m_{\\text{fit}} = 200$, $n = 20000$, seed $= 123456$.\n- Case $2$: $f_2$, $d = 2$, polynomial degree $k = 2$, $m_{\\text{fit}} = 500$, $n = 30000$, seed $= 202311$.\n- Case $3$: $f_1$, $d = 1$, polynomial degree $k = 0$, $m_{\\text{fit}} = 30$, $n = 5000$, seed $= 7777$.\n- Case $4$: $f_2$, $d = 2$, polynomial degree $k = 1$, $m_{\\text{fit}} = 100$, $n = 20000$, seed $= 8888$.\n\nRequirements:\n- For each test case, initialize a pseudorandom number generator with the given seed and use it for all sampling in that test case.\n- The final reported value for each test case is the ratio $r$ defined above, rounded to $6$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets and in the order of the cases listed, for example $[r_1,r_2,r_3,r_4]$.", "solution": "The problem is valid as it is scientifically grounded, well-posed, objective, and provides a complete and consistent set of requirements for a standard problem in computational engineering. The task is to evaluate the effectiveness of a control variate method for Monte Carlo integration, where the control variate is a polynomial approximation of the integrand.\n\nThe fundamental principle behind Monte Carlo integration is the law of large numbers. For a random variable $\\boldsymbol{X}$ uniformly distributed in the unit hypercube $[0,1]^d$, the expected value of $f(\\boldsymbol{X})$ is the integral we wish to compute: $E[f(\\boldsymbol{X})] = \\int_{[0,1]^d} f(\\boldsymbol{x}) \\, \\mathrm{d}\\boldsymbol{x} = I$. By drawing $n$ independent and identically distributed samples $\\{\\boldsymbol{Y}_i\\}_{i=1}^n$ from this distribution, we can form the plain Monte Carlo estimator, which is the sample mean:\n$$\n\\hat{I}_{\\mathrm{MC}} = \\frac{1}{n} \\sum_{i=1}^n f(\\boldsymbol{Y}_i)\n$$\nThe central limit theorem states that the error of this estimator, $\\hat{I}_{\\mathrm{MC}} - I$, is approximately normally distributed with mean $0$ and variance $\\sigma_f^2/n$, where $\\sigma_f^2 = \\text{Var}(f(\\boldsymbol{X}))$. The standard error thus decreases as $n^{-1/2}$, which can be a slow rate of convergence.\n\nTo accelerate convergence, we can use variance reduction techniques. The control variate method is one such technique. It involves finding a function $p(\\boldsymbol{x})$ that is a good approximation to $f(\\boldsymbol{x})$ and whose integral $\\mu_p = \\int_{[0,1]^d} p(\\boldsymbol{x}) \\, \\mathrm{d}\\boldsymbol{x}$ is known analytically. We then rewrite the integral $I$ as:\n$$\nI = \\int_{[0,1]^d} (f(\\boldsymbol{x}) - p(\\boldsymbol{x})) \\, \\mathrm{d}\\boldsymbol{x} + \\int_{[0,1]^d} p(\\boldsymbol{x}) \\, \\mathrm{d}\\boldsymbol{x} = \\int_{[0,1]^d} (f(\\boldsymbol{x}) - p(\\boldsymbol{x})) \\, \\mathrm{d}\\boldsymbol{x} + \\mu_p\n$$\nWe can apply Monte Carlo integration to the new, hopefully better-behaved, integrand $g(\\boldsymbol{x}) = f(\\boldsymbol{x}) - p(\\boldsymbol{x})$. This leads to the control variate estimator:\n$$\n\\hat{I}_{\\mathrm{CV}} = \\frac{1}{n} \\sum_{i=1}^n (f(\\boldsymbol{Y}_i) - p(\\boldsymbol{Y}_i)) + \\mu_p\n$$\nThis is an unbiased estimator of $I$, just like $\\hat{I}_{\\mathrm{MC}}$. Its variance is $\\text{Var}(\\hat{I}_{\\mathrm{CV}}) = \\frac{1}{n}\\text{Var}(f(\\boldsymbol{X}) - p(\\boldsymbol{X}))$. If $p(\\boldsymbol{x})$ is strongly correlated with $f(\\boldsymbol{x})$, the variance of their difference will be small, leading to a more accurate estimator for the same sample size $n$.\n\nThe problem specifies using a total-degree polynomial of degree at most $k$ as the control variate $p(\\boldsymbol{x})$. Such a polynomial is a linear combination of monomial basis functions: $p(\\boldsymbol{x}) = \\sum_{|\\boldsymbol{\\alpha}| \\le k} c_{\\boldsymbol{\\alpha}} \\boldsymbol{x}^{\\boldsymbol{\\alpha}}$. This choice is motivated by the Weierstrass approximation theorem, which ensures that polynomials can approximate any continuous function on a compact domain like $[0,1]^d$. The coefficients $\\{c_{\\boldsymbol{\\alpha}}\\}$ are determined by a least-squares fit. We generate $m_{\\text{fit}}$ samples $\\{\\boldsymbol{Z}_j\\}_{j=1}^{m_{\\text{fit}}}$ and find the coefficients $\\boldsymbol{c}$ that minimize the sum of squared errors $\\sum_{j=1}^{m_{\\text{fit}}} (f(\\boldsymbol{Z}_j) - p(\\boldsymbol{Z}_j))^2$. This is a standard linear regression problem, formulated as minimizing $\\|\\boldsymbol{A}\\boldsymbol{c} - \\boldsymbol{b}\\|_2^2$, where the design matrix $\\boldsymbol{A}$ has entries $A_{j, \\boldsymbol{\\alpha}} = \\boldsymbol{Z}_j^{\\boldsymbol{\\alpha}}$ and the target vector $\\boldsymbol{b}$ has entries $b_j = f(\\boldsymbol{Z}_j)$. This system is solved for $\\boldsymbol{c}$ using numerical linear algebra routines.\n\nOnce the coefficients $\\boldsymbol{c}$ are found, the exact integral of the polynomial, $\\mu_p$, is computed using the provided formula:\n$$\n\\mu_p = \\sum_{|\\boldsymbol{\\alpha}| \\le k} c_{\\boldsymbol{\\alpha}} \\left( \\int_{[0,1]^d} \\boldsymbol{x}^{\\boldsymbol{\\alpha}} \\, \\mathrm{d}\\boldsymbol{x} \\right) = \\sum_{|\\boldsymbol{\\alpha}| \\le k} c_{\\boldsymbol{\\alpha}} \\prod_{j=1}^d \\frac{1}{\\alpha_j + 1}\n$$\n\nThe solution procedure for each test case is as follows:\n1.  Initialize a pseudorandom number generator with the specified seed.\n2.  Define the integrand $f(\\boldsymbol{x})$, its dimension $d$, its exact integral $I$, the polynomial degree $k$, and the sample sizes $m_{\\text{fit}}$ and $n$.\n3.  Generate the set of all multi-indices $\\boldsymbol{\\alpha} \\in \\mathbb{N}_0^d$ such that $|\\boldsymbol{\\alpha}| \\le k$. These define the polynomial basis.\n4.  Generate $m_{\\text{fit}}$ random points $\\{\\boldsymbol{Z}_j\\}_{j=1}^{m_{\\text{fit}}}$ from $U([0,1]^d)$.\n5.  Construct the design matrix $\\boldsymbol{A}$ by evaluating each basis monomial at each point $\\boldsymbol{Z}_j$.\n6.  Construct the target vector $\\boldsymbol{b}$ by evaluating $f(\\boldsymbol{Z}_j)$.\n7.  Solve the least-squares problem $\\boldsymbol{A}\\boldsymbol{c} \\approx \\boldsymbol{b}$ to find the coefficient vector $\\boldsymbol{c}$.\n8.  Calculate $\\mu_p$, the exact integral of the resulting polynomial $p(\\boldsymbol{x})$.\n9.  Generate a second, independent set of $n$ random points $\\{\\boldsymbol{Y}_i\\}_{i=1}^n$ from $U([0,1]^d)$.\n10. Evaluate $f(\\boldsymbol{Y}_i)$ and $p(\\boldsymbol{Y}_i)$ for all points in this new set. The values of $p(\\boldsymbol{Y}_i)$ are found by building a design matrix for the points $\\boldsymbol{Y}$ and multiplying by the coefficient vector $\\boldsymbol{c}$.\n11. Compute the estimators $\\hat{I}_{\\mathrm{MC}}$ and $\\hat{I}_{\\mathrm{CV}}$.\n12. Calculate the absolute errors $|\\hat{I}_{\\mathrm{MC}} - I|$ and $|\\hat{I}_{\\mathrm{CV}} - I|$.\n13. Compute the ratio $r = \\frac{|\\hat{I}_{\\mathrm{CV}} - I|}{|\\hat{I}_{\\mathrm{MC}} - I|}$ and round it to $6$ decimal places. This ratio quantifies the improvement in accuracy from using the control variate. A ratio less than $1$ indicates an improvement. The case $k=0$ serves as a control, where $p(x)$ is a constant, and the ratio $r$ is expected to be $1$, confirming the logic of the estimators.\n\nThis systematic process is applied to each test case, leveraging numerical libraries for random number generation and solving the linear least-squares problem.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport itertools\nfrom typing import List, Tuple, Callable\n\ndef solve():\n    \"\"\"\n    Solves the Monte Carlo integration problem with control variates for all test cases.\n    \"\"\"\n\n    # Define functions and their exact integrals\n    def f1(x: np.ndarray) -> np.ndarray:\n        # x is assumed to be a column vector or a 2D array of shape (N, 1)\n        return np.exp(-x[:, 0]) * np.cos(7 * x[:, 0]) + x[:, 0]**3\n\n    # I1 = Re[ (e^(-1+7i) - 1) / (-1+7i) ] + 1/4\n    # Re part = (1 - e^-1*cos(7) + 7*e^-1*sin(7)) / 50\n    I1_re_part = (1 - np.exp(-1) * np.cos(7) + 7 * np.exp(-1) * np.sin(7)) / 50\n    I1 = I1_re_part + 0.25\n\n    def f2(x: np.ndarray) -> np.ndarray:\n        # x is assumed to be an array of shape (N, 2)\n        return np.sin(np.pi * x[:, 0]) * np.sin(np.pi * x[:, 1]) + x[:, 0] * x[:, 1]\n    \n    # I2 = (integral sin(pi*x) dx)^2 + (integral x dx)^2 = (2/pi)^2 + (1/2)^2\n    I2 = 4 / (np.pi**2) + 0.25\n\n    def generate_total_degree_exponents(d: int, k: int) -> List[Tuple[int, ...]]:\n        \"\"\"\n        Generates all multi-indices alpha for d variables with total degree = k.\n        \"\"\"\n        if d == 1:\n            return [(i,) for i in range(k + 1)]\n        \n        exponents = []\n        for i in range(k + 1):\n            sub_exponents = generate_total_degree_exponents(d - 1, k - i)\n            for sub_exp in sub_exponents:\n                exponents.append((i,) + sub_exp)\n        return exponents\n\n    def evaluate_monomials(points: np.ndarray, exponents: List[Tuple[int, ...]]) -> np.ndarray:\n        \"\"\"\n        Evaluates a basis of monomials at a set of points.\n        points: (n_points, d)\n        exponents: list of alpha tuples\n        Returns: design matrix (n_points, n_exponents)\n        \"\"\"\n        n_points, d = points.shape\n        n_exponents = len(exponents)\n        design_matrix = np.zeros((n_points, n_exponents))\n        \n        for i, alpha in enumerate(exponents):\n            # np.prod(points**alpha, axis=1) is a concise way to compute x^alpha\n            design_matrix[:, i] = np.prod(np.power(points, alpha), axis=1)\n            \n        return design_matrix\n\n    test_cases = [\n        {'f': f1, 'I': I1, 'd': 1, 'k': 3, 'm_fit': 200, 'n': 20000, 'seed': 123456},\n        {'f': f2, 'I': I2, 'd': 2, 'k': 2, 'm_fit': 500, 'n': 30000, 'seed': 202311},\n        {'f': f1, 'I': I1, 'd': 1, 'k': 0, 'm_fit': 30, 'n': 5000, 'seed': 7777},\n        {'f': f2, 'I': I2, 'd': 2, 'k': 1, 'm_fit': 100, 'n': 20000, 'seed': 8888},\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        f, I, d, k, m_fit, n, seed = \\\n            case['f'], case['I'], case['d'], case['k'], case['m_fit'], case['n'], case['seed']\n            \n        rng = np.random.default_rng(seed)\n\n        # Step 1: Generate basis and fit the polynomial control variate\n        exponents = generate_total_degree_exponents(d, k)\n        \n        # Generate samples for fitting\n        Z_points = rng.random((m_fit, d))\n        \n        # Build design matrix and target vector for least-squares\n        A_fit = evaluate_monomials(Z_points, exponents)\n        b_fit = f(Z_points)\n        \n        # Solve for polynomial coefficients\n        c_coeffs, _, _, _ = np.linalg.lstsq(A_fit, b_fit, rcond=None)\n\n        # Step 2: Calculate the exact integral of the polynomial\n        integral_of_monomials = np.array([1.0 / np.prod([exp + 1 for exp in alpha]) for alpha in exponents])\n        mu_p = np.dot(c_coeffs, integral_of_monomials)\n\n        # Step 3: Use an independent sample set for MC estimation\n        Y_points = rng.random((n, d))\n        \n        # Evaluate f and p at the new samples\n        f_Y = f(Y_points)\n        A_eval = evaluate_monomials(Y_points, exponents)\n        p_Y = A_eval @ c_coeffs\n\n        # Step 4: Compute estimators and errors\n        I_hat_mc = np.mean(f_Y)\n        I_hat_cv = np.mean(f_Y - p_Y) + mu_p\n        \n        error_mc = np.abs(I_hat_mc - I)\n        error_cv = np.abs(I_hat_cv - I)\n\n        if error_mc == 0:\n            # This edge case is highly unlikely but robust code should handle it.\n            # If plain MC is perfect, the ratio is either 0 (if CV is also perfect) or infinite.\n            # In the context of performance improvement, if the baseline is perfect, \n            # any non-zero error from the new method is a degradation. If CV is also perfect,\n            # it's no better or worse, ratio could be 1. Let's assume non-zero error for CV.\n            # Given the problem's nature, error_mc will not be zero.\n            ratio = float('inf') if error_cv > 0 else 1.0\n        else:\n            ratio = error_cv / error_mc\n        \n        results.append(round(ratio, 6))\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2414893"}]}