{"hands_on_practices": [{"introduction": "Rayleigh Quotient Iteration (RQI) is celebrated for its remarkably fast, typically cubic, convergence to an eigenpair. However, its powerful performance is not unconditional, and this exercise explores a special case where the iteration becomes trapped in a repeating cycle instead of converging. By working through this carefully constructed scenario [@problem_id:2431711], you will gain a deeper appreciation for how the interplay between a matrix's structure and the initial vector can lead to behaviors that defy the algorithm's typical performance, revealing an important limitation.", "problem": "Consider the Rayleigh quotient iteration (RQI) for a real matrix. Given a unit vector $x_{k} \\in \\mathbb{R}^{n}$, define the Rayleigh quotient $\\mu_{k}$ by\n$$\n\\mu_{k} \\equiv \\frac{x_{k}^{\\mathsf{T}} A x_{k}}{x_{k}^{\\mathsf{T}} x_{k}},\n$$\nand the next iterate $x_{k+1}$ by solving the linear system\n$$\n\\left(A - \\mu_{k} I\\right) y_{k} = x_{k},\n$$\nthen normalizing $x_{k+1} \\equiv \\dfrac{y_{k}}{\\|y_{k}\\|_{2}}$.\n\nLet\n$$\nA = \\begin{pmatrix} 2 & 0 \\\\ 0 & -2 \\end{pmatrix}, \\quad x_{0} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}.\n$$\nAssume exact arithmetic and that $\\left(A - \\mu_{k} I\\right)$ is invertible at each step. Determine the minimal positive integer $p$ such that the RQI-generated sequence $\\{x_{k}\\}_{k \\ge 0}$ satisfies $x_{k+p} = x_{k}$ for all $k \\ge 0$. Provide your answer as a single integer. No rounding is required.", "solution": "The problem requires us to determine the minimal positive integer period $p$ of the sequence $\\{x_{k}\\}_{k \\ge 0}$ generated by the Rayleigh quotient iteration (RQI) for a given matrix $A$ and initial vector $x_{0}$. The problem is well-defined and requires direct computation.\n\nThe RQI algorithm is defined by the following steps, starting with a unit vector $x_{k}$:\n1. Compute the Rayleigh quotient: $\\mu_{k} = x_{k}^{\\mathsf{T}} A x_{k}$. Since $x_k$ is a unit vector, $x_{k}^{\\mathsf{T}} x_{k} = 1$.\n2. Solve the linear system for $y_{k}$: $(A - \\mu_{k} I) y_{k} = x_{k}$.\n3. Normalize to find the next vector in the sequence: $x_{k+1} = \\frac{y_{k}}{\\|y_{k}\\|_{2}}$.\n\nThe given matrix and initial vector are:\n$$\nA = \\begin{pmatrix} 2 & 0 \\\\ 0 & -2 \\end{pmatrix}, \\quad x_{0} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}.\n$$\n\nWe proceed by computing the first few terms of the sequence.\n\n**Step for $k=0$:**\n\nFirst, we compute the Rayleigh quotient $\\mu_0$:\n$$\n\\mu_{0} = x_{0}^{\\mathsf{T}} A x_{0} = \\left( \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 & 1 \\end{pmatrix} \\right) \\begin{pmatrix} 2 & 0 \\\\ 0 & -2 \\end{pmatrix} \\left( \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\right) = \\frac{1}{2} \\begin{pmatrix} 1 & 1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ -2 \\end{pmatrix} = \\frac{1}{2}(2 - 2) = 0.\n$$\nNext, we solve the system $(A - \\mu_{0} I) y_{0} = x_{0}$. With $\\mu_0=0$, this simplifies to $A y_{0} = x_{0}$. The matrix $A$ is diagonal and its inverse is trivial to compute:\n$$\nA^{-1} = \\begin{pmatrix} \\frac{1}{2} & 0 \\\\ 0 & -\\frac{1}{2} \\end{pmatrix}.\n$$\nThe problem statement assumes $(A - \\mu_{k} I)$ is invertible, which is true here since $\\det(A - 0 \\cdot I) = \\det(A) = -4 \\neq 0$.\nWe solve for $y_0$:\n$$\ny_{0} = A^{-1} x_{0} = \\begin{pmatrix} \\frac{1}{2} & 0 \\\\ 0 & -\\frac{1}{2} \\end{pmatrix} \\left( \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\right) = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} \\frac{1}{2} \\\\ -\\frac{1}{2} \\end{pmatrix} = \\frac{1}{2\\sqrt{2}} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}.\n$$\nFinally, we normalize $y_0$ to find $x_1$:\n$$\n\\|y_{0}\\|_{2} = \\sqrt{\\left(\\frac{1}{2\\sqrt{2}}\\right)^2 + \\left(-\\frac{1}{2\\sqrt{2}}\\right)^2} = \\sqrt{\\frac{1}{8} + \\frac{1}{8}} = \\sqrt{\\frac{2}{8}} = \\sqrt{\\frac{1}{4}} = \\frac{1}{2}.\n$$\n$$\nx_{1} = \\frac{y_{0}}{\\|y_{0}\\|_{2}} = \\frac{\\frac{1}{2\\sqrt{2}} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}}{\\frac{1}{2}} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}.\n$$\n\n**Step for $k=1$:**\n\nNow we repeat the process with $x_1$. First, we compute $\\mu_1$:\n$$\n\\mu_{1} = x_{1}^{\\mathsf{T}} A x_{1} = \\left( \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 & -1 \\end{pmatrix} \\right) \\begin{pmatrix} 2 & 0 \\\\ 0 & -2 \\end{pmatrix} \\left( \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} \\right) = \\frac{1}{2} \\begin{pmatrix} 1 & -1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix} = \\frac{1}{2}(2 - 2) = 0.\n$$\nAgain, $\\mu_1 = 0$, so we solve $A y_{1} = x_{1}$:\n$$\ny_{1} = A^{-1} x_{1} = \\begin{pmatrix} \\frac{1}{2} & 0 \\\\ 0 & -\\frac{1}{2} \\end{pmatrix} \\left( \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} \\right) = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{1}{2} \\end{pmatrix} = \\frac{1}{2\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}.\n$$\nWe normalize $y_1$ to find $x_2$:\n$$\n\\|y_{1}\\|_{2} = \\sqrt{\\left(\\frac{1}{2\\sqrt{2}}\\right)^2 + \\left(\\frac{1}{2\\sqrt{2}}\\right)^2} = \\sqrt{\\frac{1}{8} + \\frac{1}{8}} = \\sqrt{\\frac{1}{4}} = \\frac{1}{2}.\n$$\n$$\nx_{2} = \\frac{y_{1}}{\\|y_{1}\\|_{2}} = \\frac{\\frac{1}{2\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}}{\\frac{1}{2}} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}.\n$$\n\n**Identifying the Period:**\n\nBy comparing $x_2$ with $x_0$, we observe that they are identical:\n$$\nx_{2} = x_{0}.\n$$\nSince each term $x_{k+1}$ in the sequence is determined solely by the preceding term $x_{k}$, the fact that $x_2 = x_0$ implies that the sequence is periodic. The sequence of vectors will be $x_0, x_1, x_0, x_1, \\dots$.\nMore formally, if $x_{k+2} = x_k$ holds for $k=0$, then $x_3$ will be generated from $x_2$ in the same way $x_1$ was generated from $x_0$. Since $x_2=x_0$, it must be that $x_3=x_1$. By induction, $x_{k+2} = x_k$ for all $k \\ge 0$.\nThe problem asks for the minimal positive integer $p$ such that $x_{k+p} = x_{k}$ for all $k \\ge 0$. We have found that $p=2$ works.\nWe must check if $p=1$ is a valid period. This would require $x_{k+1} = x_k$ for all $k$. For $k=0$, this means $x_1 = x_0$. However, we calculated:\n$$\nx_{0} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\quad \\text{and} \\quad x_{1} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}.\n$$\nClearly, $x_{1} \\neq x_{0}$. Therefore, the period cannot be $1$.\nThe smallest positive integer $p$ for which the relation holds is $2$.\nThe condition that $(A - \\mu_k I)$ is invertible is always satisfied because we have shown that $\\mu_k = 0$ for all $k \\ge 0$, and $A$ is invertible.\nThus, the minimal period is $p=2$.", "answer": "$$\\boxed{2}$$", "id": "2431711"}, {"introduction": "In any numerical algorithm, every step serves a purpose, though some are more subtle than others. This thought experiment challenges you to consider the consequences of omitting the normalization step in RQI, a practice that is crucial for numerical stability [@problem_id:2431780]. Analyzing this modification reveals the vital distinction between an algorithm's behavior in exact arithmetic versus its practical implementation in finite-precision computation, a core concept in developing robust numerical code.", "problem": "Consider a real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ with simple eigenvalues and a starting vector $v_0 \\neq 0$. The Rayleigh quotient of a nonzero vector $x$ is defined by $\\rho(x) = \\dfrac{x^{\\mathsf{T}} A x}{x^{\\mathsf{T}} x}$. The Rayleigh quotient iteration (RQI) generates a sequence $\\{v_k\\}$ and shifts $\\{\\mu_k\\}$ by computing the shift $\\mu_k = \\rho(v_k)$, solving the shifted linear system $(A - \\mu_k I) w_{k+1} = v_k$ for $w_{k+1}$, and then normalizing via $v_{k+1} = \\dfrac{w_{k+1}}{\\|w_{k+1}\\|}$. Suppose the normalization step is omitted and one instead sets $v_{k+1} = w_{k+1}$ while still defining $\\mu_{k+1} = \\rho(v_{k+1})$ at each step. Assuming exact arithmetic and, separately, considering practical floating-point computation, which statement best describes what happens to the convergence properties of the method?\n\nA) In exact arithmetic, the sequence of directions and Rayleigh quotients is unchanged and local cubic convergence to a simple eigenpair is retained for symmetric $A$; in floating-point arithmetic, omitting normalization can produce very large or very small vector norms, risking overflow or underflow and degrading or stalling convergence in practice.\n\nB) Even in exact arithmetic, the order of convergence necessarily drops from cubic to linear, because normalization is required for any superlinear behavior.\n\nC) The iteration becomes ill-defined, because the Rayleigh quotient is only defined for unit-norm vectors, so omitting normalization makes the shifts meaningless.\n\nD) The method generically diverges unless the initial vector is exactly an eigenvector; normalization is essential for any convergence in both exact and finite-precision settings.", "solution": "The problem statement must be validated before a solution is attempted.\n\n**Step 1: Extract Givens**\n- The matrix $A$ is a real symmetric matrix in $\\mathbb{R}^{n \\times n}$.\n- The eigenvalues of $A$ are simple.\n- The starting vector is $v_0 \\neq 0$.\n- The Rayleigh quotient is defined as $\\rho(x) = \\dfrac{x^{\\mathsf{T}} A x}{x^{\\mathsf{T}} x}$ for a nonzero vector $x$.\n- The standard Rayleigh quotient iteration (RQI) is defined by the sequence: $\\mu_k = \\rho(v_k)$, solve $(A - \\mu_k I) w_{k+1} = v_k$, and normalize $v_{k+1} = \\dfrac{w_{k+1}}{\\|w_{k+1}\\|}$.\n- A modified RQI is proposed where the normalization step is omitted, so that $v_{k+1} = w_{k+1}$. The shift is still computed as $\\mu_{k+1} = \\rho(v_{k+1})$.\n- The question is to describe the effect of this modification on the convergence properties, considering both exact arithmetic and practical floating-point computation.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically and mathematically sound. It poses a question about a standard and important algorithm in numerical linear algebra, the Rayleigh quotient iteration. The matrix properties (real, symmetric, simple eigenvalues) are the typical context in which the convergence of RQI is analyzed. The definition of the algorithm and its modification are stated clearly and unambiguously. The problem is well-posed, objective, and self-contained. It does not violate any scientific principles or contain factual errors.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. A solution will be derived.\n\n**Derivation of Solution**\n\nLet us analyze the behavior of the modified algorithm, first in exact arithmetic and then in the context of floating-point computation.\n\n**Part 1: Exact Arithmetic**\n\nA fundamental property of the Rayleigh quotient $\\rho(x)$ is its scale-invariance. For any nonzero vector $x \\in \\mathbb{R}^n$ and any nonzero scalar $\\alpha \\in \\mathbb{R}$, we have:\n$$\n\\rho(\\alpha x) = \\dfrac{(\\alpha x)^{\\mathsf{T}} A (\\alpha x)}{(\\alpha x)^{\\mathsf{T}} (\\alpha x)} = \\dfrac{\\alpha^2 x^{\\mathsf{T}} A x}{\\alpha^2 x^{\\mathsf{T}} x} = \\dfrac{x^{\\mathsf{T}} A x}{x^{\\mathsf{T}} x} = \\rho(x)\n$$\nThis means that the Rayleigh quotient depends only on the direction of the vector, not its magnitude.\n\nLet $\\{v_k\\}$ denote the sequence of vectors from the standard RQI (with normalization) and let $\\{u_k\\}$ denote the sequence from the modified RQI (without normalization). Let us choose the same starting vector, $v_0 = u_0$.\n\nFor $k=0$:\n- The standard RQI calculates the shift $\\mu_0 = \\rho(v_0)$.\n- The modified RQI calculates the shift $\\sigma_0 = \\rho(u_0)$.\nSince $v_0 = u_0$, it is clear that $\\mu_0 = \\sigma_0$.\n\nThe next iterates are found by solving:\n- Standard RQI: $(A - \\mu_0 I) w_1 = v_0$, followed by normalization $v_1 = \\dfrac{w_1}{\\|w_1\\|}$.\n- Modified RQI: $(A - \\sigma_0 I) u_1 = u_0$.\n\nSince $\\mu_0 = \\sigma_0$ and $v_0 = u_0$, the linear systems are identical. Thus, the solution $u_1$ of the modified method is identical to the intermediate vector $w_1$ of the standard method. So, $u_1 = w_1$, which implies that the normalized vector of the standard method is $v_1 = \\dfrac{u_1}{\\|u_1\\|}$.\n\nNow, let us proceed to the next step, for $k=1$:\n- The standard RQI calculates the shift $\\mu_1 = \\rho(v_1) = \\rho\\left(\\dfrac{u_1}{\\|u_1\\|}\\right)$.\n- The modified RQI calculates the shift $\\sigma_1 = \\rho(u_1)$.\n\nDue to the scale-invariance of the Rayleigh quotient, $\\mu_1 = \\sigma_1$. The shifts are again identical.\n\nBy induction, if we assume $v_k = \\dfrac{u_k}{\\|u_k\\|}$ and $\\mu_{k-1} = \\sigma_{k-1}$, then the subsequent shifts $\\mu_k = \\rho(v_k)$ and $\\sigma_k = \\rho(u_k)$ must be equal due to scale-invariance. The subsequent vectors are given by $w_{k+1} = (A - \\mu_k I)^{-1} v_k$ and $u_{k+1} = (A - \\sigma_k I)^{-1} u_k$. Since $\\mu_k = \\sigma_k$ and $v_k$ is a scalar multiple of $u_k$ (specifically, $v_k = u_k/\\|u_k\\|$), it follows that $w_{k+1}$ is the same scalar multiple of $u_{k+1}$ (specifically, $w_{k+1} = u_{k+1}/\\|u_k\\|$). The next normalized vector is $v_{k+1} = \\dfrac{w_{k+1}}{\\|w_{k+1}\\|} = \\dfrac{u_{k+1}/\\|u_k\\|}{\\|u_{k+1}/\\|u_k\\|\\|} = \\dfrac{u_{k+1}}{\\|u_{k+1}\\|}$.\n\nThe induction holds. The sequence of shifts $\\{\\mu_k\\}$ and the sequence of directions (represented by the unit vectors $\\{v_k\\}$) produced by the standard algorithm are identical to the sequence of shifts $\\{\\sigma_k\\}$ and the sequence of directions $\\{\\frac{u_k}{\\|u_k\\|}\\}$ from the modified algorithm.\n\nThe convergence rate of RQI is determined by the convergence of the shifts to an eigenvalue and the directions to an eigenvector. Since these sequences are mathematically identical in exact arithmetic, the convergence properties are unchanged. For a symmetric matrix $A$ with simple eigenvalues and a starting vector that is not orthogonal to an eigenvector, RQI exhibits local cubic convergence to an eigenpair $(\\lambda, q)$. This property is retained by the unnormalized version in exact arithmetic.\n\n**Part 2: Floating-Point Arithmetic**\n\nIn practice, computations are performed using finite-precision floating-point numbers. The analysis changes significantly. The update step for the unnormalized iteration is $u_{k+1} = (A - \\sigma_k I)^{-1} u_k$. As the iteration converges to an eigenpair $(\\lambda_j, q_j)$, the shift $\\sigma_k = \\rho(u_k)$ rapidly approaches the eigenvalue $\\lambda_j$. Consequently, the matrix $(A - \\sigma_k I)$ becomes nearly singular.\n\nThe norm of the inverse operator, $\\|(A - \\sigma_k I)^{-1}\\|_2$, is equal to $1/\\min_i |\\lambda_i - \\sigma_k|$, where the $\\lambda_i$ are the eigenvalues of $A$. As $\\sigma_k \\to \\lambda_j$, this norm tends to infinity. This causes extreme amplification of the vector $u_k$. The norm of the next iterate, $\\|u_{k+1}\\|$, will be much larger than $\\|u_k\\|$. This rapid, typically exponential, growth in the norm of the vector iterates will quickly lead to the components of $u_k$ exceeding the maximum representable floating-point value, a condition known as **overflow**. Once overflow occurs, the computation breaks down.\n\nConversely, numerical stability is also threatened by vectors with very small magnitudes. If at any point the vector $u_k$ has a norm close to the machine's underflow threshold, the calculation of the next shift $\\sigma_k = \\frac{u_k^{\\mathsf{T}} A u_k}{u_k^{\\mathsf{T}} u_k}$ becomes numerically precarious. The numerator and denominator are squares of small numbers, which can themselves **underflow** to zero, leading to a division by zero or a result of NaN (Not a Number). This would stall the iteration. Even without literal underflow, the catastrophic loss of relative precision in this calculation would corrupt the shift $\\sigma_k$, degrading or destroying the convergence.\n\nTherefore, omitting the normalization step, while theoretically benign in exact arithmetic, is catastrophic in a practical floating-point setting. Normalization is a crucial step for numerical stability, keeping the iterate vector within a well-behaved range of magnitudes, thereby preventing both overflow and underflow and ensuring the accurate calculation of the Rayleigh quotient.\n\n**Evaluation of Options**\n\n- **A) In exact arithmetic, the sequence of directions and Rayleigh quotients is unchanged and local cubic convergence to a simple eigenpair is retained for symmetric A; in floating-point arithmetic, omitting normalization can produce very large or very small vector norms, risking overflow or underflow and degrading or stalling convergence in practice.**\nThis statement is fully consistent with our analysis. The part about exact arithmetic is correct due to the scale-invariance of the Rayleigh quotient. The part about floating-point arithmetic correctly identifies the risk of overflow due to the amplifying effect of the nearly singular inverse, and the risk of underflow-related issues which can corrupt the Rayleigh quotient calculation or halt the process. **Correct.**\n\n- **B) Even in exact arithmetic, the order of convergence necessarily drops from cubic to linear, because normalization is required for any superlinear behavior.**\nThis is incorrect. As demonstrated, the sequence of shifts and directions is unaffected in exact arithmetic, so the cubic convergence rate is preserved. Normalization is not the source of superlinear convergence for RQI. **Incorrect.**\n\n- **C) The iteration becomes ill-defined, because the Rayleigh quotient is only defined for unit-norm vectors, so omitting normalization makes the shifts meaningless.**\nThis is fundamentally incorrect. The Rayleigh quotient $\\rho(x) = \\frac{x^{\\mathsf{T}} A x}{x^{\\mathsf{T}} x}$ is defined for *any* non-zero vector $x$. The normalization is implicit in its definition via the denominator $x^{\\mathsf{T}} x$. The shifts are always well-defined as long as the vector is not the zero vector. **Incorrect.**\n\n- **D) The method generically diverges unless the initial vector is exactly an eigenvector; normalization is essential for any convergence in both exact and finite-precision settings.**\nThis is incorrect. We have shown that in exact arithmetic, the method converges with the same properties as the standard RQI. Normalization is not essential for convergence in theory, but for numerical stability in practice. **Incorrect.**", "answer": "$$\\boxed{A}$$", "id": "2431780"}, {"introduction": "Having explored the conditions for RQI's convergence and the practical importance of its components, it is time to build and test the full algorithm. This practice puts you in the role of a computational engineer, tasking you with implementing RQI for complex Hermitian matrices from the ground up [@problem_id:2431729]. More importantly, you will empirically measure its local order of convergence, providing concrete evidence for the cubic rate that makes RQI such a powerful tool in numerical linear algebra.", "problem": "Implement a program that, for each given complex Hermitian matrix and initial vector, performs Rayleigh quotient iteration to approximate an eigenpair and empirically estimates the local order of convergence. Let $A \\in \\mathbb{C}^{n \\times n}$ be Hermitian, and let the Rayleigh quotient of a nonzero vector $v \\in \\mathbb{C}^n$ be defined by\n$$\n\\mu(v) = \\frac{v^* A v}{v^* v}.\n$$\nGiven an initial nonzero vector $v_0 \\in \\mathbb{C}^n$, define the Rayleigh quotient iteration as the sequence $\\{(v_k,\\mu_k)\\}_{k \\ge 0}$ where $v_k$ is normalized to unit $2$-norm at every step, $\\mu_k = \\mu(v_k)$, and the update is given by solving the shifted linear system\n$$\n(A - \\mu_k I) w_k = v_k,\n$$\nthen setting\n$$\nv_{k+1} = \\frac{w_k}{\\|w_k\\|_2}, \\quad \\mu_{k+1} = \\mu(v_{k+1}).\n$$\nTerminate the iteration when the residual norm satisfies\n$$\n\\|A v_k - \\mu_k v_k\\|_2 \\le \\tau,\n$$\nor when a maximum number of iterations $m$ is reached. In this problem, take $\\tau = 10^{-12}$ and $m = 50$. For each test case, after termination, let $\\lambda_\\star$ be the eigenvalue of $A$ closest (in absolute difference) to the final Rayleigh quotient $\\mu_K$ and let $x_\\star$ be the corresponding unit-norm eigenvector. Define the angle-based error at iteration $k$ by\n$$\ne_k = \\sqrt{1 - \\left|\\langle x_\\star, v_k \\rangle\\right|^2},\n$$\nwhere $\\langle \\cdot, \\cdot \\rangle$ denotes the standard complex inner product on $\\mathbb{C}^n$, so that $e_k = \\sin(\\theta_k)$ where $\\theta_k$ is the principal angle between $v_k$ and $x_\\star$. Use the last three consecutive valid errors $e_{k-1}$, $e_k$, $e_{k+1}$ that are strictly positive and finite to estimate the local order of convergence by\n$$\np \\approx \\frac{\\ln(e_{k+1}/e_k)}{\\ln(e_k/e_{k-1})}.\n$$\nIf fewer than three valid errors are available, return the value $3$ for that test case.\n\nUse the following test suite. For each case, the Hermitian matrix $A$ is defined as $A = \\tfrac{1}{2}(B + B^*)$, where $B$ is given. Each initial vector $v_0$ must be normalized to unit $2$-norm before starting the iteration.\n\n- Test case $1$:\n  - $B_1 = \\begin{bmatrix}\n  4 + 0\\mathrm{i} & 1 + 2\\mathrm{i} & 0 + 0\\mathrm{i} & -1 + 0.5\\mathrm{i} \\\\\n  1 - 2\\mathrm{i} & 3 + 0\\mathrm{i} & -\\mathrm{i} & 2 + 0\\mathrm{i} \\\\\n  0 + 0\\mathrm{i} & \\mathrm{i} & 5 + 0\\mathrm{i} & -2 + \\mathrm{i} \\\\\n  -1 - 0.5\\mathrm{i} & 2 + 0\\mathrm{i} & -2 - \\mathrm{i} & 6 + 0\\mathrm{i}\n  \\end{bmatrix}$,\n  - $v_{0,1} = \\begin{bmatrix} 1 + 0.5\\mathrm{i} \\\\ -0.3 + 0.7\\mathrm{i} \\\\ 0.2 - 0.4\\mathrm{i} \\\\ 0.9 + 0\\mathrm{i} \\end{bmatrix}$.\n\n- Test case $2$:\n  - $B_2 = \\begin{bmatrix}\n  2 + 0\\mathrm{i} & 1 - \\mathrm{i} & 0 + 0.5\\mathrm{i} \\\\\n  1 + \\mathrm{i} & 2.1 + 0\\mathrm{i} & 0.8 - 0.2\\mathrm{i} \\\\\n  0 - 0.5\\mathrm{i} & 0.8 + 0.2\\mathrm{i} & 2.05 + 0\\mathrm{i}\n  \\end{bmatrix}$,\n  - $v_{0,2} = \\begin{bmatrix} 0.6 - 0.1\\mathrm{i} \\\\ -0.2 + 0.5\\mathrm{i} \\\\ 0.7 + 0.3\\mathrm{i} \\end{bmatrix}$.\n\n- Test case $3$:\n  - $B_3 = \\begin{bmatrix}\n  7 + 0\\mathrm{i} & 2 + \\mathrm{i} & 0 + 0\\mathrm{i} & 0.5 - \\mathrm{i} & -1 + 0.2\\mathrm{i} \\\\\n  2 - \\mathrm{i} & 6 + 0\\mathrm{i} & -1 + 2\\mathrm{i} & 0 + 0\\mathrm{i} & 0.3 + 0.1\\mathrm{i} \\\\\n  0 + 0\\mathrm{i} & -1 - 2\\mathrm{i} & 5 + 0\\mathrm{i} & 1 + 0.4\\mathrm{i} & 0 - 0.7\\mathrm{i} \\\\\n  0.5 + \\mathrm{i} & 0 + 0\\mathrm{i} & 1 - 0.4\\mathrm{i} & 4 + 0\\mathrm{i} & 1.2 - 0.6\\mathrm{i} \\\\\n  -1 - 0.2\\mathrm{i} & 0.3 - 0.1\\mathrm{i} & 0.7\\mathrm{i} & 1.2 + 0.6\\mathrm{i} & 3.5 + 0\\mathrm{i}\n  \\end{bmatrix}$,\n  - $v_{0,3} = \\begin{bmatrix} 0.9 + 0.1\\mathrm{i} \\\\ -0.3 + 0.8\\mathrm{i} \\\\ 0.2 - 0.5\\mathrm{i} \\\\ 0.1 + 0.4\\mathrm{i} \\\\ -0.7 + 0.2\\mathrm{i} \\end{bmatrix}$.\n\nFor each test case, produce a single floating-point number equal to the estimated local order $p$ computed from the last available triple of valid errors as specified above. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[\\text{result}_1,\\text{result}_2,\\text{result}_3]$). No physical units are involved. Angles, when implicitly referenced through the sine function in the error definition, are dimensionless and need not be expressed in degrees or radians.", "solution": "The problem is subjected to validation before a solution is attempted.\n\nStep 1: Extract Givens\n- **Algorithm**: Rayleigh quotient iteration.\n- **Matrix**: $A \\in \\mathbb{C}^{n \\times n}$ is a Hermitian matrix.\n- **Rayleigh Quotient**: $\\mu(v) = \\frac{v^* A v}{v^* v}$ for a nonzero vector $v \\in \\mathbb{C}^n$.\n- **Iteration Definition**: Given an initial nonzero vector $v_0 \\in \\mathbb{C}^n$, the sequence is $\\{(v_k,\\mu_k)\\}_{k \\ge 0}$.\n- **Normalization**: $v_k$ is normalized to have a $2$-norm of $1$ at every step.\n- **State Update**: $\\mu_k = \\mu(v_k)$. The update step involves solving the linear system $(A - \\mu_k I) w_k = v_k$. The next iterate is $v_{k+1} = \\frac{w_k}{\\|w_k\\|_2}$, and the next Rayleigh quotient is $\\mu_{k+1} = \\mu(v_{k+1})$.\n- **Termination Conditions**: The iteration stops when the residual norm $\\|A v_k - \\mu_k v_k\\|_2 \\le \\tau$ or when the number of iterations reaches a maximum of $m$.\n- **Constants**: The tolerance is $\\tau = 10^{-12}$, and the maximum number of iterations is $m = 50$.\n- **Convergence Analysis**:\n    - After termination, let $\\mu_K$ be the final Rayleigh quotient. Identify the eigenvalue of $A$, denoted $\\lambda_\\star$, that is closest to $\\mu_K$. Let $x_\\star$ be the corresponding unit-norm eigenvector.\n    - The error at iteration $k$ is defined as $e_k = \\sqrt{1 - \\left|\\langle x_\\star, v_k \\rangle\\right|^2}$, where $\\langle \\cdot, \\cdot \\rangle$ is the standard complex inner product.\n    - The local order of convergence, $p$, is estimated using the last three consecutive, strictly positive, and finite errors: $p \\approx \\frac{\\ln(e_{k+1}/e_k)}{\\ln(e_k/e_{k-1})}$.\n    - If fewer than three such valid errors are available, $p$ is assigned the value of $3$.\n- **Input Data**:\n    - The Hermitian matrix $A$ is constructed as $A = \\tfrac{1}{2}(B + B^*)$ from a given matrix $B$.\n    - Three test cases are provided, each with a matrix $B$ and an initial vector $v_0$.\n    - The initial vector $v_0$ must be normalized to unit $2$-norm before the first iteration.\n- **Output**: A single line containing a comma-separated list of the estimated convergence orders $p$ for each test case, enclosed in square brackets.\n\nStep 2: Validate Using Extracted Givens\n- **Scientific Grounding**: The problem is scientifically sound. Rayleigh quotient iteration is a fundamental and well-established algorithm in numerical linear algebra for computing eigenpairs. The formulas for the Rayleigh quotient, the iterative update, the error metric, and the estimation of the convergence order are all standard in computational engineering and numerical analysis.\n- **Well-Posedness**: The problem is well-posed. It provides a deterministic algorithm, clear termination criteria, and an unambiguous procedure for calculating the required output. For a Hermitian matrix and a generic initial vector not orthogonal to an eigenvector, the iteration is guaranteed to converge, typically to the eigenpair closest to the initial state $(\\mu_0, v_0)$. The convergence is known to be cubic, ensuring a stable and meaningful solution.\n- **Objectivity**: The problem is stated objectively using precise mathematical language, free from ambiguity or subjective claims.\n- **Flaw Analysis**:\n    1.  No scientific unsoundness.\n    2.  The problem is formalizable and directly relevant to the specified topic.\n    3.  The problem is self-contained and provides all necessary information. The definition of the inner product $\\langle \\cdot, \\cdot \\rangle$ might be ambiguous in some contexts, but since the magnitude squared is used, $|\\langle u, w \\rangle|^2$, the result is independent of the convention for which argument is conjugate-linear.\n    4.  The conditions are computationally feasible. The shifted matrix $(A - \\mu_k I)$ can become singular if $\\mu_k$ is an exact eigenvalue. This is a known feature of the algorithm, representing convergence, and can be handled by a robust linear solver or by terminating the iteration.\n    5.  The problem is well-structured and not ill-posed.\n    6.  The result is scientifically verifiable by independent implementation and execution of the specified algorithm.\n\nStep 3: Verdict and Action\nThe problem is valid. A reasoned solution will be provided.\n\nThe problem requires the implementation of the Rayleigh quotient iteration (RQI) algorithm to approximate an eigenpair of a given complex Hermitian matrix and to empirically estimate its order of convergence.\n\nThe Rayleigh quotient iteration is a powerful method derived from inverse iteration. Standard inverse iteration finds the eigenvector corresponding to the eigenvalue closest to a fixed shift $\\sigma$. It computes $v_{k+1}$ by solving $(A - \\sigma I)w_k = v_k$ and normalizing. RQI enhances this by using an adaptive shift: at each step $k$, the shift is updated to be the current best estimate of the eigenvalue, which is the Rayleigh quotient $\\mu_k = \\mu(v_k) = (v_k)^* A v_k / (v_k)^* v_k$. Since we enforce $\\|v_k\\|_2=1$, this simplifies to $\\mu_k = (v_k)^* A v_k$. This adaptive shifting strategy results in exceptionally rapid local convergence. For a Hermitian matrix, the convergence rate is typically cubic, a significant improvement over the linear convergence of the power method or the fixed-shift inverse iteration.\n\nThe procedure for each test case is as follows:\n\n1.  **Initialization**:\n    - Given the matrix $B$, the corresponding Hermitian matrix $A$ is constructed using the formula $A = \\frac{1}{2}(B + B^*)$, where $B^*$ is the conjugate transpose of $B$. This construction guarantees that $A$ is Hermitian, i.e., $A = A^*$.\n    - The provided initial vector $v_{0, \\text{raw}}$ is normalized to have a unit $2$-norm: $v_0 = v_{0, \\text{raw}} / \\|v_{0, \\text{raw}}\\|_2$.\n    - A history of iterate vectors, `vs`, is initialized to store each $v_k$.\n\n2.  **Iterative Process**:\n    - The iteration proceeds for $k = 0, 1, 2, \\dots$ up to a maximum of $m-1 = 49$.\n    - At each step $k$, the current vector $v_k$ is stored.\n    - The Rayleigh quotient $\\mu_k = (v_k)^* A v_k$ is computed.\n    - The termination condition is checked by computing the norm of the residual vector, $r_k = A v_k - \\mu_k v_k$. If $\\|r_k\\|_2 \\le \\tau = 10^{-12}$, the iteration terminates.\n    - If the condition is not met, the next iterate is computed. This involves solving the shifted linear system $(A - \\mu_k I) w_k = v_k$ for the vector $w_k$. The matrix $A - \\mu_k I$ is guaranteed to be Hermitian but may be indefinite. A general linear solver is used for this purpose. If this matrix is numerically singular, it indicates that $\\mu_k$ is an excellent approximation of an eigenvalue, and the iteration can be terminated.\n    - The subsequent vector iterate $v_{k+1}$ is obtained by normalizing $w_k$: $v_{k+1} = w_k / \\|w_k\\|_2$.\n    - The process repeats with the updated vector $v_{k+1}$.\n\n3.  **Post-Iteration Analysis and Convergence Estimation**:\n    - Upon termination at iteration $K$, the final state is described by the approximate eigenpair $(\\mu_K, v_K)$. To estimate the convergence order, we must compare the iterates against the true eigenpair.\n    - The full eigensystem of the matrix $A$ is computed. Since $A$ is Hermitian, its eigenvalues are real, and its eigenvectors form an orthonormal basis.\n    - The true eigenvalue $\\lambda_\\star$ closest to the final computed Rayleigh quotient $\\mu_K$ is identified by minimizing the absolute difference $|\\lambda_j - \\mu_K|$ over all eigenvalues $\\lambda_j$ of $A$.\n    - The corresponding unit-norm eigenvector $x_\\star$ is retrieved.\n    - A sequence of errors $\\{e_k\\}$ is computed for all stored iterates $v_k$ using the formula $e_k = \\sqrt{1 - |(x_\\star)^* v_k|^2}$. This error represents the sine of the angle between the iterate $v_k$ and the true eigenvector $x_\\star$.\n    - To estimate the convergence order $p$, we seek the last three consecutive errors in the sequence, say at indices $j-2, j-1, j$, that are all strictly positive and finite. Let these be $e_{j-2}, e_{j-1}, e_j$. The order $p$ is then estimated as $p \\approx \\frac{\\ln(e_j/e_{j-1})}{\\ln(e_{j-1}/e_{j-2})}$.\n    - If the iteration converges so rapidly that fewer than three such qualifying errors are generated, a definitive estimate cannot be computed. In this scenario, as per the problem specification, we return the theoretically expected value for RQI on a Hermitian matrix, which is $p=3$.\n\nThis comprehensive procedure is applied to each test case to yield the estimated local order of convergence.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements Rayleigh quotient iteration to find an eigenpair of a Hermitian\n    matrix and estimates the local order of convergence.\n    \"\"\"\n\n    test_cases = [\n        (\n            np.array([\n                [4 + 0j, 1 + 2j, 0 + 0j, -1 + 0.5j],\n                [1 - 2j, 3 + 0j, -1j, 2 + 0j],\n                [0 + 0j, 1j, 5 + 0j, -2 + 1j],\n                [-1 - 0.5j, 2 + 0j, -2 - 1j, 6 + 0j]\n            ]),\n            np.array([1 + 0.5j, -0.3 + 0.7j, 0.2 - 0.4j, 0.9 + 0j])\n        ),\n        (\n            np.array([\n                [2 + 0j, 1 - 1j, 0 + 0.5j],\n                [1 + 1j, 2.1 + 0j, 0.8 - 0.2j],\n                [0 - 0.5j, 0.8 + 0.2j, 2.05 + 0j]\n            ]),\n            np.array([0.6 - 0.1j, -0.2 + 0.5j, 0.7 + 0.3j])\n        ),\n        (\n            np.array([\n                [7 + 0j, 2 + 1j, 0 + 0j, 0.5 - 1j, -1 + 0.2j],\n                [2 - 1j, 6 + 0j, -1 + 2j, 0 + 0j, 0.3 + 0.1j],\n                [0 + 0j, -1 - 2j, 5 + 0j, 1 + 0.4j, 0 - 0.7j],\n                [0.5 + 1j, 0 + 0j, 1 - 0.4j, 4 + 0j, 1.2 - 0.6j],\n                [-1 - 0.2j, 0.3 - 0.1j, 0.7j, 1.2 + 0.6j, 3.5 + 0j]\n            ]),\n            np.array([0.9 + 0.1j, -0.3 + 0.8j, 0.2 - 0.5j, 0.1 + 0.4j, -0.7 + 0.2j])\n        )\n    ]\n\n    results = []\n    tau = 1e-12\n    m = 50\n\n    for B, v0_unnormalized in test_cases:\n        # Step 1: Initialization\n        A = 0.5 * (B + B.T.conj())\n        n = A.shape[0]\n        v_current = v0_unnormalized / np.linalg.norm(v0_unnormalized)\n\n        v_history = []\n        \n        # Step 2: Rayleigh Quotient Iteration\n        mu_current = 0.0 # Will be updated in the first loop iteration\n        for k in range(m):\n            v_history.append(v_current)\n            mu_current = np.vdot(v_current, A @ v_current).real\n\n            residual_norm = np.linalg.norm(A @ v_current - mu_current * v_current)\n            if residual_norm <= tau:\n                break\n\n            try:\n                shift_matrix = A - mu_current * np.identity(n)\n                w_next = np.linalg.solve(shift_matrix, v_current)\n            except np.linalg.LinAlgError:\n                # If matrix is singular, mu_current is an exact eigenvalue.\n                # The iteration has converged.\n                break\n            \n            v_next = w_next / np.linalg.norm(w_next)\n            v_current = v_next\n        \n        # After loop termination, mu_final corresponds to the last v_current\n        mu_final = np.vdot(v_current, A @ v_current).real\n\n        # Step 3: Post-processing to find true eigenpair\n        eigvals, eigvecs = np.linalg.eigh(A)\n        idx_closest = np.argmin(np.abs(eigvals - mu_final))\n        lambda_star = eigvals[idx_closest]\n        x_star = eigvecs[:, idx_closest]\n        \n        # Step 4: Calculate error history\n        errors = []\n        for v_k in v_history:\n            # e_k = sin(theta_k) = sqrt(1 - cos^2(theta_k))\n            cos_theta_sq = np.abs(np.vdot(x_star, v_k))**2\n            # Clamp to 1.0 to avoid small negative numbers due to floating point error\n            if cos_theta_sq > 1.0:\n                cos_theta_sq = 1.0\n            error = np.sqrt(1.0 - cos_theta_sq)\n            errors.append(error)\n\n        # Step 5: Estimate convergence order p\n        p = 3.0  # Default value\n        p_val_found = False\n        # Search backwards for the last three consecutive, positive, finite errors\n        for i in range(len(errors) - 1, 1, -1):\n            e_k_plus_1 = errors[i]\n            e_k = errors[i-1]\n            e_k_minus_1 = errors[i-2]\n\n            if (e_k_plus_1 > 0 and np.isfinite(e_k_plus_1) and\n                e_k > 0 and np.isfinite(e_k) and\n                e_k_minus_1 > 0 and np.isfinite(e_k_minus_1)):\n                \n                # Check for non-decreasing error to avoid log(<=1) in denominator\n                if e_k >= e_k_minus_1:\n                    continue\n\n                log_ratio_num = np.log(e_k_plus_1 / e_k)\n                log_ratio_den = np.log(e_k / e_k_minus_1)\n\n                if np.abs(log_ratio_den) > 1e-15:\n                    p = log_ratio_num / log_ratio_den\n                    p_val_found = True\n                    break\n        \n        results.append(p)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2431729"}]}