{"hands_on_practices": [{"introduction": "Before diving into coding, it is crucial to develop an intuition for why certain iterative methods are preferred over others. This analytical exercise [@problem_id:2427129] provides a clear case study on the behavior of different algorithms. You will investigate a scenario where the standard power method fails to converge due to eigenvalues of equal magnitude, while the more sophisticated Rayleigh Quotient Iteration (RQI) successfully pinpoints a specific eigenpair based on the initial guess, demonstrating the method's power and precision.", "problem": "Let $A \\in \\mathbb{R}^{3 \\times 3}$ be the symmetric matrix\n$$\nA=\\begin{pmatrix}\n2  0  0\\\\\n0  -2  0\\\\\n0  0  1\n\\end{pmatrix},\n$$\nand let the initial vector be\n$$\nv_{0}=\\begin{pmatrix}1\\\\1\\\\10\\end{pmatrix}.\n$$\nDefine the standard power iteration by $y_{k+1}=A y_{k} / \\|A y_{k}\\|_{2}$ with $y_{0}=v_{0}$, and define the Rayleigh quotient iteration (RQI) by\n$$\n\\mu_{k}=\\frac{x_{k}^{\\top} A x_{k}}{x_{k}^{\\top} x_{k}},\\quad\nx_{k+1}=\\frac{(A-\\mu_{k} I)^{-1} x_{k}}{\\|(A-\\mu_{k} I)^{-1} x_{k}\\|_{2}},\n$$\ninitialized with $x_{0}=v_{0}$. Analyze, from first principles, whether the sequence $\\{y_{k}\\}$ converges to a single direction as $k \\to \\infty$. Then determine the eigenvalue to which the Rayleigh quotient iteration converges when started from $x_{0}=v_{0}$. Provide only that eigenvalue as your final answer, in exact form. No rounding is required.", "solution": "The problem statement is subjected to validation and is found to be valid. It is scientifically grounded in the field of numerical linear algebra, well-posed with all necessary information provided, and objective in its formulation. We may therefore proceed with a solution.\n\nThe problem requires two distinct analyses: one for the standard power iteration and one for the Rayleigh quotient iteration (RQI).\n\nFirst, we analyze the convergence of the standard power iteration sequence $\\{y_k\\}$. The matrix $A$ is given as\n$$\nA=\\begin{pmatrix}\n2  0  0\\\\\n0  -2  0\\\\\n0  0  1\n\\end{pmatrix}.\n$$\nSince $A$ is a diagonal matrix, its eigenvalues are the diagonal entries, and the corresponding eigenvectors are the standard basis vectors. Let the eigenvalues be $\\lambda_1 = 2$, $\\lambda_2 = -2$, and $\\lambda_3 = 1$. The corresponding orthonormal eigenvectors are\n$$\ne_{1}=\\begin{pmatrix}1\\\\0\\\\0\\end{pmatrix}, \\quad e_{2}=\\begin{pmatrix}0\\\\1\\\\0\\end{pmatrix}, \\quad e_{3}=\\begin{pmatrix}0\\\\0\\\\1\\end{pmatrix}.\n$$\nThe standard power method converges to the eigenvector associated with the eigenvalue of largest magnitude (the dominant eigenvalue), provided such an eigenvalue is unique and the initial vector has a non-zero component along that eigenvector's direction. The magnitudes of the eigenvalues are $|\\lambda_1| = |2| = 2$, $|\\lambda_2| = |-2| = 2$, and $|\\lambda_3| = |1| = 1$. There are two eigenvalues, $\\lambda_1$ and $\\lambda_2$, with the same largest magnitude, $2$. This is a degenerate case for the power method.\n\nTo analyze the behavior, we express the initial vector $y_0 = v_0$ in the basis of eigenvectors:\n$$\ny_{0} = v_{0} = \\begin{pmatrix}1\\\\1\\\\10\\end{pmatrix} = 1 \\cdot e_{1} + 1 \\cdot e_{2} + 10 \\cdot e_{3}.\n$$\nThe $k$-th iteration of the unnormalized power method is given by $A^k y_0$.\n$$\nA^{k} y_{0} = A^{k}(1 \\cdot e_{1} + 1 \\cdot e_{2} + 10 \\cdot e_{3}) = 1 \\cdot A^{k}e_{1} + 1 \\cdot A^{k}e_{2} + 10 \\cdot A^{k}e_{3}.\n$$\nUsing the property that $A^k e_i = \\lambda_i^k e_i$, we get\n$$\nA^{k} y_{0} = 1 \\cdot \\lambda_{1}^{k} e_{1} + 1 \\cdot \\lambda_{2}^{k} e_{2} + 10 \\cdot \\lambda_{3}^{k} e_{3} = (2)^{k} e_{1} + (-2)^{k} e_{2} + 10 \\cdot (1)^{k} e_{3}.\n$$\nWe can factor out the term with the largest magnitude, $(2)^k$:\n$$\nA^{k} y_{0} = (2)^{k} \\left( e_{1} + \\frac{(-2)^{k}}{(2)^{k}} e_{2} + 10 \\frac{1^{k}}{2^{k}} e_{3} \\right) = (2)^{k} \\left( e_{1} + (-1)^{k} e_{2} + 10 \\left(\\frac{1}{2}\\right)^{k} e_{3} \\right).\n$$\nThe normalized vector is $y_k = \\frac{A^k y_0}{\\|A^k y_0\\|_2}$. As $k \\to \\infty$, the term $10 (\\frac{1}{2})^k e_3$ vanishes. The direction of $A^k y_0$ is dominated by the vector $e_1 + (-1)^k e_2$.\nFor even values of $k$, the vector is proportional to $e_1 + e_2 = \\begin{pmatrix}1\\\\1\\\\0\\end{pmatrix}$ (in the limit).\nFor odd values of $k$, the vector is proportional to $e_1 - e_2 = \\begin{pmatrix}1\\\\-1\\\\0\\end{pmatrix}$ (in the limit).\nSince the sequence of vectors $\\{y_k\\}$ alternates between two distinct directions, it does not converge to a single direction.\n\nNext, we analyze the convergence of the Rayleigh quotient iteration. The iteration starts with $x_0 = v_0$. The first step is to compute the initial Rayleigh quotient $\\mu_0$:\n$$\n\\mu_{0} = \\frac{x_{0}^{\\top} A x_{0}}{x_{0}^{\\top} x_{0}}.\n$$\nWith $x_0 = \\begin{pmatrix}1\\\\1\\\\10\\end{pmatrix}$, we compute the numerator and denominator.\n$$\nx_{0}^{\\top} A x_{0} = \\begin{pmatrix}1  1  10\\end{pmatrix} \\begin{pmatrix} 2  0  0\\\\ 0  -2  0\\\\ 0  0  1 \\end{pmatrix} \\begin{pmatrix}1\\\\1\\\\10\\end{pmatrix} = \\begin{pmatrix}1  1  10\\end{pmatrix} \\begin{pmatrix}2\\\\-2\\\\10\\end{pmatrix} = (1)(2) + (1)(-2) + (10)(10) = 100.\n$$\n$$\nx_{0}^{\\top} x_{0} = 1^{2} + 1^{2} + 10^{2} = 1 + 1 + 100 = 102.\n$$\nThus, the initial Rayleigh quotient is\n$$\n\\mu_{0} = \\frac{100}{102} = \\frac{50}{51}.\n$$\nThe Rayleigh quotient iteration for a symmetric matrix is equivalent to inverse iteration with an adaptive shift $\\sigma_k = \\mu_k$. The inverse iteration method with a fixed shift $\\sigma$ converges to the eigenvector corresponding to the eigenvalue closest to $\\sigma$. For RQI, the sequence of Rayleigh quotients $\\{\\mu_k\\}$ typically converges to an eigenvalue, and this convergence is very rapid (cubic, in general) for symmetric matrices. The specific eigenvalue to which it converges is the one closest to the initial Rayleigh quotient $\\mu_0$.\n\nWe must find which of the eigenvalues $\\lambda_1 = 2$, $\\lambda_2 = -2$, $\\lambda_3 = 1$ is closest to $\\mu_0 = \\frac{50}{51}$. We compute the distances:\n$$\n|\\lambda_1 - \\mu_0| = \\left|2 - \\frac{50}{51}\\right| = \\left|\\frac{102 - 50}{51}\\right| = \\frac{52}{51}.\n$$\n$$\n|\\lambda_2 - \\mu_0| = \\left|-2 - \\frac{50}{51}\\right| = \\left|-\\frac{102 + 50}{51}\\right| = \\frac{152}{51}.\n$$\n$$\n|\\lambda_3 - \\mu_0| = \\left|1 - \\frac{50}{51}\\right| = \\left|\\frac{51 - 50}{51}\\right| = \\frac{1}{51}.\n$$\nThe smallest distance is $|\\lambda_3 - \\mu_0| = \\frac{1}{51}$. Therefore, the Rayleigh quotient $\\mu_k$ will converge to the eigenvalue $\\lambda_3 = 1$. The vector sequence $x_k$ will converge to the corresponding eigenvector $e_3$. The problem asks for the eigenvalue to which the iteration converges.", "answer": "$$\n\\boxed{1}\n$$", "id": "2427129"}, {"introduction": "We now move from theoretical analysis to practical implementation. The inverse iteration method is a workhorse for finding an eigenvalue closest to a target value, or \"shift\" $\\sigma$. This practice [@problem_id:2427100] guides you through implementing the algorithm for a sparse tridiagonal matrix, a structure that appears frequently in simulations of physical systems. A key professional skill you will develop here is the use of an efficient linear solver to handle the $(A - \\sigma I)^{-1}$ term, which is fundamental to creating robust and scalable scientific code.", "problem": "Let $A \\in \\mathbb{R}^{n \\times n}$ be the sparse tridiagonal matrix with main diagonal entries equal to $2$ and first sub- and super-diagonal entries equal to $-1$, that is, $A = \\operatorname{tridiag}([-1], [2], [-1])$ with Dirichlet-type structure. For a given real shift $\\sigma \\in \\mathbb{R}$, a nonzero initial vector $x^{(0)} \\in \\mathbb{R}^{n}$, a convergence tolerance $\\varepsilon  0$, and a maximum iteration count $m \\in \\mathbb{N}$, compute an approximation to an eigenvalue $\\lambda \\in \\mathbb{R}$ of $A$ that is closest to $\\sigma$, together with a corresponding unit-norm eigenvector $v \\in \\mathbb{R}^{n}$, by iteratively solving linear systems with the shifted matrix $A - \\sigma I$ and normalizing the iterates. At each iteration, define the scalar approximation using the Rayleigh quotient $\\lambda^{(k)} = (x^{(k)})^{\\top} A x^{(k)}$, and declare convergence when the absolute change in successive Rayleigh quotients satisfies $\\lvert \\lambda^{(k)} - \\lambda^{(k-1)} \\rvert \\le \\varepsilon \\max(1, \\lvert \\lambda^{(k)} \\rvert)$ or when the iteration count reaches $m$. The initial vector is specified by $x^{(0)}_i = i+1$ for indices $i = 0, 1, 2, \\dots, n-1$, and the normalization is with respect to the Euclidean norm $\\lVert \\cdot \\rVert_2$. All computations must be carried out in double-precision floating-point arithmetic. Report, for each test case, only the converged Rayleigh quotient approximation to the eigenvalue, rounded to exactly $10$ decimal places.\n\nTest suite. For each parameter tuple $(n, \\sigma, \\varepsilon, m)$ below, construct $A$ as above and compute the requested approximation:\n- Case $1$: $(n, \\sigma, \\varepsilon, m) = (10, 0.1, 10^{-12}, 100)$.\n- Case $2$: $(n, \\sigma, \\varepsilon, m) = (10, 1.6, 10^{-12}, 100)$.\n- Case $3$: $(n, \\sigma, \\varepsilon, m) = (10, 3.9, 10^{-12}, 100)$.\n- Case $4$: $(n, \\sigma, \\varepsilon, m) = (40, 10.0, 10^{-12}, 200)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the cases above, with each value rounded to exactly $10$ decimal places (for example, [$x_1,x_2,x_3,x_4$]). The final output must therefore be a list of $4$ floating-point numbers.", "solution": "The problem requires the computation of an eigenvalue $\\lambda$ of a specific symmetric tridiagonal matrix $A \\in \\mathbb{R}^{n \\times n}$ that is closest to a given real shift $\\sigma$. The matrix $A$ is defined by its diagonals: the main diagonal entries are all $2$, and the first sub-diagonal and super-diagonal entries are all $-1$. This matrix is a standard finite difference discretization of the second derivative operator with Dirichlet boundary conditions. The method prescribed is the inverse power iteration with shift, also known as shift-and-invert iteration.\n\nThis method is designed to find the eigenvalue of $A$ closest to $\\sigma$. The fundamental principle is that if $\\lambda$ is an eigenvalue of $A$ with corresponding eigenvector $v$, then $(\\lambda - \\sigma)$ is an eigenvalue of the shifted matrix $A - \\sigma I$ with the same eigenvector $v$. Consequently, $(\\lambda - \\sigma)^{-1}$ is an eigenvalue of the inverse matrix $(A - \\sigma I)^{-1}$. The power iteration method, when applied to a matrix $B$, converges to the eigenvector corresponding to the eigenvalue with the largest magnitude. By applying power iteration to $B = (A - \\sigma I)^{-1}$, we find the eigenvector corresponding to its eigenvalue of largest magnitude. This eigenvalue is $(\\lambda_{\\text{closest}} - \\sigma)^{-1}$, where $\\lambda_{\\text{closest}}$ is the eigenvalue of $A$ that is nearest to the shift $\\sigma$, thus minimizing $\\lvert \\lambda - \\sigma \\rvert$.\n\nThe iterative procedure is as follows. We begin with a normalized initial vector $x^{(0)} \\in \\mathbb{R}^n$, where $\\lVert x^{(0)} \\rVert_2 = 1$. The problem specifies an initial vector with components $(x^{(0)}_{\\text{unnorm}})_i = i+1$ for $i=0, \\dots, n-1$; this vector must first be normalized. The first Rayleigh quotient is then computed as $\\lambda^{(0)} = (x^{(0)})^{\\top} A x^{(0)}$.\n\nFor each subsequent iteration $k=1, 2, \\dots, m$:\n$1$. The core step is to solve the linear system of equations $(A - \\sigma I) z^{(k)} = x^{(k-1)}$ for the vector $z^{(k)}$. This is equivalent to applying the inverse matrix, $z^{(k)} = (A - \\sigma I)^{-1} x^{(k-1)}$, but is numerically far more stable and efficient than computing the matrix inverse explicitly. The matrix $A - \\sigma I$ is tridiagonal, so this system can be solved very efficiently in $O(n)$ operations using a specialized algorithm such as the Thomas algorithm.\n\n$2$. The resulting vector $z^{(k)}$ is then normalized to have unit Euclidean norm, yielding the next iterate for the eigenvector: $x^{(k)} = z^{(k)} / \\lVert z^{(k)} \\rVert_2$. The sequence of vectors $x^{(k)}$ converges to the eigenvector $v$ corresponding to the eigenvalue $\\lambda$ of $A$ closest to $\\sigma$.\n\n$3$. The corresponding eigenvalue is approximated at each step using the Rayleigh quotient: $\\lambda^{(k)} = (x^{(k)})^{\\top} A x^{(k)}$. Since $x^{(k)}$ is a unit vector, this simplifies from the general form $\\frac{(x^{(k)})^{\\top} A x^{(k)}}{(x^{(k)})^{\\top} x^{(k)}}$. The sequence $\\lambda^{(k)}$ converges to the eigenvalue $\\lambda$.\n\n$4$. Convergence is declared when the change between successive eigenvalue approximations is sufficiently small. The specified criterion is $\\lvert \\lambda^{(k)} - \\lambda^{(k-1)} \\rvert \\le \\varepsilon \\max(1, \\lvert \\lambda^{(k)} \\rvert)$. This is a mixed relative and absolute error tolerance. The iteration also terminates if the maximum number of iterations, $m$, is reached.\n\nThe implementation will construct the matrix $A - \\sigma I$ in a banded format suitable for an efficient linear solver, such as `scipy.linalg.solve_banded`. The matrix has a main diagonal of $(2-\\sigma)$, and sub- and super-diagonals of $-1$. The Rayleigh quotient calculation will also be optimized for a tridiagonal matrix $A$, where the matrix-vector product $A x^{(k)}$ can be calculated in $O(n)$ time. The entire procedure must be executed using double-precision floating-point arithmetic as required.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import solve_banded\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test suite.\n    \"\"\"\n\n    def apply_A(x: np.ndarray) - np.ndarray:\n        \"\"\"\n        Efficiently computes the matrix-vector product y = Ax for the\n        tridiagonal matrix A = tridiag([-1], [2], [-1]).\n        \"\"\"\n        n = len(x)\n        if n == 0:\n            return np.array([])\n        if n == 1:\n            return np.array([2.0 * x[0]])\n        \n        y = np.zeros_like(x, dtype=np.float64)\n        \n        # First element\n        y[0] = 2.0 * x[0] - x[1]\n        \n        # Middle elements\n        y[1:-1] = -x[0:-2] + 2.0 * x[1:-1] - x[2:]\n        \n        # Last element\n        y[-1] = -x[-2] + 2.0 * x[-1]\n        \n        return y\n\n    def inverse_power_iteration(n: int, sigma: float, epsilon: float, m: int) - float:\n        \"\"\"\n        Computes the eigenvalue of A closest to a shift sigma using the\n        inverse power iteration method.\n\n        Args:\n            n: The dimension of the matrix A.\n            sigma: The real shift.\n            epsilon: The convergence tolerance.\n            m: The maximum number of iterations.\n\n        Returns:\n            The converged eigenvalue approximation.\n        \"\"\"\n        # Construct the banded representation of the shifted matrix B = A - sigma*I\n        # The matrix B is tridiagonal with diagonals [-1, 2-sigma, -1].\n        # For scipy.linalg.solve_banded, the banded matrix `ab` has shape (l+u+1, n).\n        # Here, l=1 (sub-diagonal), u=1 (super-diagonal).\n        # ab[0, 1:] is the super-diagonal\n        # ab[1, :] is the main diagonal\n        # ab[2, :-1] is the sub-diagonal\n        ab = np.zeros((3, n), dtype=np.float64)\n        ab[0, 1:] = -1.0\n        ab[1, :] = 2.0 - sigma\n        ab[2, :-1] = -1.0\n\n        # Initialize the vector x_k\n        # As per problem, initial vector components are x_i = i + 1\n        x_k_unnorm = np.arange(1, n + 1, dtype=np.float64)\n        norm_x = np.linalg.norm(x_k_unnorm)\n        x_k = x_k_unnorm / norm_x\n        \n        # Compute initial Rayleigh quotient lambda_k\n        lambda_k = np.dot(x_k, apply_A(x_k))\n\n        for _ in range(m):\n            lambda_prev = lambda_k\n            \n            # Solve (A - sigma*I) * z = x_k\n            z_k = solve_banded((1, 1), ab, x_k)\n\n            # Normalize to get the next iterate\n            norm_z = np.linalg.norm(z_k)\n            # This case should not be reached with a nonsingular matrix and nonzero vector\n            if norm_z == 0.0:\n                break\n                \n            x_k = z_k / norm_z\n\n            # Compute the new Rayleigh quotient\n            lambda_k = np.dot(x_k, apply_A(x_k))\n\n            # Check for convergence\n            # Criterion: |lambda_k - lambda_{k-1}| = eps * max(1, |lambda_k|)\n            if np.abs(lambda_k - lambda_prev) = epsilon * np.max([1.0, np.abs(lambda_k)]):\n                break\n        \n        return lambda_k\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (10, 0.1, 1e-12, 100),\n        (10, 1.6, 1e-12, 100),\n        (10, 3.9, 1e-12, 100),\n        (40, 10.0, 1e-12, 200),\n    ]\n\n    results = []\n    for case in test_cases:\n        n_val, sigma_val, eps_val, m_val = case\n        result = inverse_power_iteration(n_val, sigma_val, eps_val, m_val)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    formatted_results = [f\"{r:.10f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2427100"}, {"introduction": "This final practice serves as a capstone, challenging you to perform a comparative study of the algorithms discussed. You will implement three distinct methods: the basic power iteration, inverse iteration with a fixed shift, and the powerful Rayleigh Quotient Iteration (RQI) [@problem_id:2427128]. By deploying these algorithms on a series of carefully designed test matrices—including one with very close eigenvalues and an ill-conditioned Hilbert matrix—you will gain a tangible understanding of their relative performance and see firsthand the remarkable cubic convergence rate of RQI.", "problem": "Implement an algorithm to approximate eigenpairs of a real symmetric matrix using inverse iteration with a variable shift given by the Rayleigh quotient. Let the Rayleigh quotient for a nonzero vector $x \\in \\mathbb{R}^n$ be defined as $R(x) = \\dfrac{x^\\mathsf{T} A x}{x^\\mathsf{T} x}$. Consider the following three iterative schemes applied to a real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ with a given nonzero initial vector $x_0 \\in \\mathbb{R}^n$ and tolerance $\\varepsilon  0$:\n\n- Power iteration: $x_{k+1} \\leftarrow \\dfrac{A x_k}{\\lVert A x_k \\rVert_2}$, with the residual norm defined using $\\lambda_k = R(x_k)$ as $\\lVert A x_k - \\lambda_k x_k \\rVert_2$.\n- Inverse iteration with a fixed shift: with $\\sigma_0 = R(x_0)$ fixed, compute $x_{k+1}$ by solving $(A - \\sigma_0 I) y = x_k$ and setting $x_{k+1} \\leftarrow \\dfrac{y}{\\lVert y \\rVert_2}$, with the residual norm defined using $\\lambda_k = R(x_k)$ as $\\lVert A x_k - \\lambda_k x_k \\rVert_2$.\n- Inverse iteration with a variable shift equal to the Rayleigh quotient (Rayleigh quotient iteration): at each iteration, compute $\\sigma_k = R(x_k)$, solve $(A - \\sigma_k I) y = x_k$, and set $x_{k+1} \\leftarrow \\dfrac{y}{\\lVert y \\rVert_2}$, with the residual norm defined using $\\lambda_k = R(x_k)$ as $\\lVert A x_k - \\lambda_k x_k \\rVert_2$.\n\nFor each scheme, stop when the residual norm first satisfies $\\lVert A x_k - \\lambda_k x_k \\rVert_2 \\le \\varepsilon$ or when a prescribed maximum number of iterations is reached. All vector norms are the Euclidean norm, and $I$ denotes the identity matrix of size $n$.\n\nUse the following test suite. In every case, set $n = 5$, the tolerance $\\varepsilon = 10^{-10}$, and the maximum number of iterations to $1000$.\n\n- Test case $\\#1$ (tri-diagonal symmetric positive definite matrix):\n  - Matrix $A_1 \\in \\mathbb{R}^{5 \\times 5}$:\n    $$\n    A_1 =\n    \\begin{bmatrix}\n    6  2  0  0  0 \\\\\n    2  5  2  0  0 \\\\\n    0  2  4  2  0 \\\\\n    0  0  2  3  2 \\\\\n    0  0  0  2  2\n    \\end{bmatrix}.\n    $$\n  - Initial vector $x_0^{(1)} = \\dfrac{1}{\\sqrt{5}} [1, 1, 1, 1, 1]^\\mathsf{T}$.\n\n- Test case $\\#2$ (symmetric matrix with two very close eigenvalues):\n  - Define diagonal matrix $D = \\mathrm{diag}(1, 1 + 10^{-6}, 2, 3, 4)$.\n  - Define the planar rotation with angle $\\theta$ such that $\\cos \\theta = \\dfrac{4}{5}$ and $\\sin \\theta = \\dfrac{3}{5}$, and set\n    $$\n    Q = \\begin{bmatrix}\n    \\cos \\theta  -\\sin \\theta  0  0  0 \\\\\n    \\sin \\theta  \\phantom{-}\\cos \\theta  0  0  0 \\\\\n    0  0  1  0  0 \\\\\n    0  0  0  1  0 \\\\\n    0  0  0  0  1\n    \\end{bmatrix}.\n    $$\n  - Matrix $A_2 = Q^\\mathsf{T} D Q$.\n  - Initial vector $x_0^{(2)} = [1, 0, 0, 0, 0]^\\mathsf{T}$.\n\n- Test case $\\#3$ (Hilbert matrix):\n  - Matrix $A_3 \\in \\mathbb{R}^{5 \\times 5}$ with entries $(A_3)_{ij} = \\dfrac{1}{i + j - 1}$ for $i,j \\in \\{1, 2, 3, 4, 5\\}$.\n  - Initial vector $x_0^{(3)} = \\dfrac{1}{\\sqrt{5}} [1, -1, 1, -1, 1]^\\mathsf{T}$.\n\nFor each test case, run the three schemes independently with the same $A$ and $x_0$ and record the smallest iteration count $k$ at which the residual norm first satisfies $\\lVert A x_k - \\lambda_k x_k \\rVert_2 \\le \\varepsilon$. If convergence does not occur within the maximum number of iterations, record the maximum number of iterations.\n\nYour program must output a single line containing a comma-separated list of $9$ integers enclosed in square brackets, in the following order:\n$[k_{\\mathrm{RQI}}^{(1)}, k_{\\mathrm{fixed}}^{(1)}, k_{\\mathrm{power}}^{(1)}, k_{\\mathrm{RQI}}^{(2)}, k_{\\mathrm{fixed}}^{(2)}, k_{\\mathrm{power}}^{(2)}, k_{\\mathrm{RQI}}^{(3)}, k_{\\mathrm{fixed}}^{(3)}, k_{\\mathrm{power}}^{(3)}]$, where $k_{\\mathrm{RQI}}^{(i)}$ is the iteration count for Rayleigh quotient iteration on test case $i$, $k_{\\mathrm{fixed}}^{(i)}$ is the iteration count for inverse iteration with a fixed shift $\\sigma_0 = R(x_0^{(i)})$, and $k_{\\mathrm{power}}^{(i)}$ is the iteration count for power iteration. The output must be exactly one line in this format, with no additional characters or whitespace beyond those structurally required by the list representation.", "solution": "The problem statement is evaluated to be valid. It is scientifically grounded in the established principles of numerical linear algebra, specifically iterative methods for eigenvalue problems. The problem is well-posed, with all necessary parameters, matrices, initial conditions, and stopping criteria explicitly and unambiguously defined. The language is objective and formal. Therefore, a solution will be provided.\n\nThe problem requires the implementation and comparison of three iterative algorithms for approximating an eigenpair $(\\lambda, v)$ of a real symmetric matrix $A$, where $A v = \\lambda v$. An eigenpair consists of an eigenvalue $\\lambda$ and its corresponding eigenvector $v$. The methods under consideration are power iteration, inverse iteration with a fixed shift, and inverse iteration with a variable shift, also known as Rayleigh quotient iteration (RQI). For a real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$, all its eigenvalues are real, and there exists an orthonormal basis of eigenvectors. The Rayleigh quotient, defined for a nonzero vector $x \\in \\mathbb{R}^n$ as $R(x) = \\frac{x^\\mathsf{T} A x}{x^\\mathsf{T} x}$, provides an estimate for an eigenvalue. If $x$ is an eigenvector, then $R(x)$ is the corresponding exact eigenvalue. For all algorithms, we start with an initial vector $x_0$ and generate a sequence of vectors $\\{x_k\\}$ that converges to an eigenvector, and a sequence of Rayleigh quotients $\\{\\lambda_k = R(x_k)\\}$ that converges to the corresponding eigenvalue.\n\n1.  **Power Iteration**\n\n    The power iteration method is the simplest algorithm for finding the dominant eigenpair of a matrix, i.e., the eigenpair $(\\lambda_1, v_1)$ where $|\\lambda_1|$ is the largest magnitude among all eigenvalues. The iterative step is defined as:\n    $$\n    x_{k+1} = \\frac{A x_k}{\\lVert A x_k \\rVert_2}\n    $$\n    Starting with an initial vector $x_0$ that has a non-zero component in the direction of the dominant eigenvector $v_1$, the sequence $x_k$ converges to $v_1$. The convergence is linear, with a rate determined by the ratio $|\\lambda_2 / \\lambda_1|$, where $\\lambda_2$ is the eigenvalue with the second-largest magnitude. If this ratio is close to $1$, convergence can be very slow. The eigenvalue is approximated at each step by the Rayleigh quotient, $\\lambda_k = R(x_k)$.\n\n2.  **Inverse Iteration with Fixed Shift**\n\n    Inverse iteration is a method to find the eigenpair corresponding to the eigenvalue closest to a given shift $\\sigma$. It applies the power method to the matrix $(A - \\sigma I)^{-1}$. The eigenvalues of $(A - \\sigma I)^{-1}$ are $(\\lambda_i - \\sigma)^{-1}$, where $\\lambda_i$ are the eigenvalues of $A$. The dominant eigenvalue of $(A - \\sigma I)^{-1}$ corresponds to the smallest value of $|\\lambda_i - \\sigma|$, which means $\\lambda_i$ is the eigenvalue of $A$ closest to $\\sigma$. The iterative step is:\n    $$\n    x_{k+1} = \\frac{(A - \\sigma I)^{-1} x_k}{\\lVert (A - \\sigma I)^{-1} x_k \\rVert_2}\n    $$\n    In practice, computing the matrix inverse is avoided. Instead, we solve the linear system $(A - \\sigma I) y_k = x_k$ for $y_k$, and then normalize:\n    $$\n    x_{k+1} = \\frac{y_k}{\\lVert y_k \\rVert_2}\n    $$\n    In this problem, a fixed shift $\\sigma_0 = R(x_0)$ is used throughout the process. Convergence is linear, but the rate is determined by the ratio of the two eigenvalues of $(A-\\sigma_0 I)^{-1}$ with largest magnitude. If $\\sigma_0$ is much closer to one eigenvalue $\\lambda_j$ than to any other, convergence to the eigenvector $v_j$ is very rapid.\n\n3.  **Rayleigh Quotient Iteration (RQI)**\n\n    Rayleigh quotient iteration is a powerful refinement of inverse iteration where the shift is updated at each step using the best current estimate for the eigenvalue: the Rayleigh quotient. The iterative process is defined by:\n    1.  Compute the shift: $\\sigma_k = R(x_k) = \\frac{x_k^\\mathsf{T} A x_k}{x_k^\\mathsf{T} x_k}$.\n    2.  Solve for $y_{k+1}$: $(A - \\sigma_k I) y_{k+1} = x_k$.\n    3.  Normalize: $x_{k+1} = \\frac{y_{k+1}}{\\lVert y_{k+1} \\rVert_2}$.\n\n    For a symmetric matrix, RQI exhibits cubic convergence once the iterate $x_k$ is sufficiently close to an eigenvector. This means that the number of correct digits in the approximation roughly triples with each iteration, leading to extremely fast convergence.\n\n**Stopping Criterion**\n\nFor all three methods, the iteration terminates when the norm of the residual vector, $\\lVert A x_k - \\lambda_k x_k \\rVert_2$, falls below a specified tolerance $\\varepsilon$, where $\\lambda_k = R(x_k)$. This residual measures how well the current approximate pair $( \\lambda_k, x_k )$ satisfies the eigenvalue equation. The iteration count $k$ at which this condition is first met is the desired output. If the condition is not met within a maximum number of iterations, that maximum number is recorded.\n\nThe implementation will proceed by defining three functions, one for each algorithm. Each function will iteratively generate the vector sequence and check the stopping criterion at each step, returning the iteration count. These functions will then be applied to the three specified test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef power_iteration(A, x0, tol, max_iter):\n    \"\"\"\n    Approximates a dominant eigenpair using Power Iteration.\n    \"\"\"\n    x = x0 / np.linalg.norm(x0)\n    for k in range(1, max_iter + 1):\n        # Calculate x_k\n        v = A @ x\n        x_k = v / np.linalg.norm(v)\n\n        # Check residual for x_k\n        # Since x_k is normalized, its L2 norm squared is 1.\n        lam_k = x_k.T @ A @ x_k\n        residual_norm = np.linalg.norm(A @ x_k - lam_k * x_k)\n\n        if residual_norm = tol:\n            return k\n\n        # Prepare for the next iteration\n        x = x_k\n\n    return max_iter\n\ndef inverse_iteration_fixed_shift(A, x0, tol, max_iter):\n    \"\"\"\n    Approximates an eigenpair using inverse iteration with a fixed shift.\n    The shift is the Rayleigh quotient of the initial vector.\n    \"\"\"\n    # Normalize initial vector for stability, although given vectors are normalized.\n    # The problem specifies sigma0 = R(x0), where x0 is the given initial vector.\n    # Since all given x0 are unit norm, x0.T @ x0 = 1.\n    sigma0 = x0.T @ A @ x0\n    \n    try:\n        M = A - sigma0 * np.eye(A.shape[0])\n    except np.linalg.LinAlgError:\n        return max_iter # Fails if shift is an exact eigenvalue\n\n    x = x0 / np.linalg.norm(x0) # Start iteration with normalized vector\n\n    for k in range(1, max_iter + 1):\n        try:\n            # Solve (A - sigma0*I) y = x_{k-1}\n            y = np.linalg.solve(M, x)\n        except np.linalg.LinAlgError:\n            # Shift is an eigenvalue or matrix is numerically singular\n            return max_iter\n\n        x_k = y / np.linalg.norm(y)\n\n        # Check residual for x_k\n        lam_k = x_k.T @ A @ x_k\n        residual_norm = np.linalg.norm(A @ x_k - lam_k * x_k)\n\n        if residual_norm = tol:\n            return k\n\n        x = x_k\n\n    return max_iter\n\ndef rayleigh_quotient_iteration(A, x0, tol, max_iter):\n    \"\"\"\n    Approximates an eigenpair using Rayleigh Quotient Iteration.\n    \"\"\"\n    x = x0 / np.linalg.norm(x0)\n    \n    for k in range(1, max_iter + 1):\n        # Update shift at each step using the Rayleigh quotient of x_{k-1}\n        sigma = x.T @ A @ x\n\n        try:\n            M = A - sigma * np.eye(A.shape[0])\n            # Solve (A - sigma_{k-1}*I) y = x_{k-1}\n            y = np.linalg.solve(M, x)\n        except np.linalg.LinAlgError:\n            # If the shift is an eigenvalue, the previous iterate was the eigenvector.\n            # Its residual should be zero or very small.\n            # The loop condition will have caught it in the previous iteration.\n            # This indicates a numerical breakdown or an exact hit.\n            return k-1 if k  1 else max_iter\n\n        x_k = y / np.linalg.norm(y)\n\n        # Check residual for x_k\n        lam_k = x_k.T @ A @ x_k\n        residual_norm = np.linalg.norm(A @ x_k - lam_k * x_k)\n        \n        if residual_norm = tol:\n            return k\n\n        x = x_k\n\n    return max_iter\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    n = 5\n    tol = 1e-10\n    max_iter = 1000\n\n    # Test Case 1\n    A1 = np.array([\n        [6, 2, 0, 0, 0],\n        [2, 5, 2, 0, 0],\n        [0, 2, 4, 2, 0],\n        [0, 0, 2, 3, 2],\n        [0, 0, 0, 2, 2]\n    ], dtype=float)\n    x0_1 = np.ones(n) / np.sqrt(n)\n\n    # Test Case 2\n    D = np.diag([1.0, 1.0 + 1e-6, 2.0, 3.0, 4.0])\n    cos_theta = 4.0 / 5.0\n    sin_theta = 3.0 / 5.0\n    Q = np.eye(n)\n    Q[0, 0] = cos_theta\n    Q[0, 1] = -sin_theta\n    Q[1, 0] = sin_theta\n    Q[1, 1] = cos_theta\n    A2 = Q.T @ D @ Q\n    x0_2 = np.zeros(n)\n    x0_2[0] = 1.0\n\n    # Test Case 3\n    A3 = np.fromfunction(lambda i, j: 1 / (i + j + 1), (n, n), dtype=int)\n    x0_3 = np.array([1, -1, 1, -1, 1]) / np.sqrt(n)\n    \n    test_cases = [\n        (A1, x0_1),\n        (A2, x0_2),\n        (A3, x0_3)\n    ]\n\n    results = []\n    for A, x0 in test_cases:\n        k_rqi = rayleigh_quotient_iteration(A, x0, tol, max_iter)\n        k_fixed = inverse_iteration_fixed_shift(A, x0, tol, max_iter)\n        k_power = power_iteration(A, x0, tol, max_iter)\n        \n        results.extend([k_rqi, k_fixed, k_power])\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "2427128"}]}