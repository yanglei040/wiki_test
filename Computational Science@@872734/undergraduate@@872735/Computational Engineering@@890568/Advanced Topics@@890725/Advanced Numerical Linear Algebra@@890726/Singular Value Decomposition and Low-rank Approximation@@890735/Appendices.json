{"hands_on_practices": [{"introduction": "The power of SVD in creating low-rank approximations is precisely quantified by the Eckart-Young-Mirsky theorem. This foundational result states that the error of the best rank-$k$ approximation, measured in the spectral norm, is simply the first truncated singular value, $\\sigma_{k+1}$. This practice exercise [@problem_id:1071275] provides a concrete opportunity to apply this theorem, connecting the abstract concept of approximation error to a direct calculation involving the singular values of a specific matrix.", "problem": "Let $A$ be a real $m \\times n$ matrix. The Singular Value Decomposition (SVD) of $A$ is a factorization of the form $A = U\\Sigma V^T$, where $U$ is an $m \\times m$ orthogonal matrix, $V$ is an $n \\times n$ orthogonal matrix, and $\\Sigma$ is an $m \\times n$ rectangular diagonal matrix with non-negative real numbers on the diagonal. The diagonal entries $\\sigma_i$ of $\\Sigma$ are known as the singular values of $A$ and are conventionally ordered such that $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_r > 0$, where $r = \\text{rank}(A)$.\n\nThe truncated SVD provides a low-rank approximation of $A$. The best rank-$k$ approximation of $A$, denoted $A_k$, is given by $A_k = \\sum_{i=1}^k \\sigma_i u_i v_i^T$, where $u_i$ and $v_i$ are the $i$-th columns of $U$ and $V$, respectively.\n\nThe Eckart-Young-Mirsky theorem states that $A_k$ is the optimal rank-$k$ approximation of $A$ with respect to the spectral norm (matrix 2-norm), $\\| \\cdot \\|_2$. The error of this approximation is given by the first truncated singular value:\n$$\n\\|A - A_k\\|_2 = \\sigma_{k+1}\n$$\n\nConsider the 3x3 symmetric Pascal matrix, $P_3$, defined by the binomial coefficients $P_{ij} = \\binom{i+j-2}{i-1}$ for $i,j \\in \\{1, 2, 3\\}$.\n\nDetermine the spectral norm of the error, $\\|P_3 - (P_3)_k\\|_2$, when approximating the matrix $P_3$ with its best rank-$k$ approximation $(P_3)_k$ for $k=2$.", "solution": "1. The Eckart–Young–Mirsky theorem gives \n$$\\|P_3 - (P_3)_k\\|_2 = \\sigma_{k+1},$$ \nwhere $\\sigma_i$ are the singular values of $P_3$, ordered $\\sigma_1\\ge\\sigma_2\\ge\\sigma_3>0$.\n2. For the symmetric matrix \n$$P_3=\\begin{pmatrix}1&1&1\\\\1&2&3\\\\1&3&6\\end{pmatrix}$$ \nthe singular values equal its eigenvalues.  The characteristic polynomial is\n$$\\det(P_3-\\lambda I)=\\lambda^3-9\\lambda^2+9\\lambda-1=0.$$\n3. Factoring out $(\\lambda-1)$ yields\n$$(\\lambda-1)(\\lambda^2-8\\lambda+1),$$\nso the eigenvalues are \n$$\\lambda=1,\\quad \\lambda=4\\pm\\sqrt{15}.$$\n4. Ordering $\\sigma_1=4+\\sqrt{15}\\ge\\sigma_2=1\\ge\\sigma_3=4-\\sqrt{15}$, for $k=2$ the error norm is\n$$\\|P_3-(P_3)_2\\|_2=\\sigma_3=4-\\sqrt{15}.$$", "answer": "$$\\boxed{4-\\sqrt{15}}$$", "id": "1071275"}, {"introduction": "While SVD provides the optimal low-rank approximation in terms of minimizing overall error, its global nature can lead to surprising results in applications like image compression. A rank-1 approximation, being an outer product of two vectors, is generally a dense matrix, meaning it can introduce non-zero values even where the original data was zero. This hands-on exercise [@problem_id:2435626] challenges you to quantify this 'energy leakage' for simple image-like patterns, building crucial intuition about the structural consequences of low-rank approximation.", "problem": "You are given the mathematical task of evaluating how the best rank-$1$ approximation of an image-like matrix may introduce content outside the original nonzero support. Let $A \\in \\mathbb{R}^{m \\times n}$ be a real matrix. Define its best rank-$1$ approximation $A_1$ to be any minimizer of the Frobenius-norm error\n$$\n\\lVert A - X \\rVert_F \\quad \\text{over all matrices } X \\in \\mathbb{R}^{m \\times n} \\text{ with } \\operatorname{rank}(X) = 1.\n$$\nLet $M \\in \\{0,1\\}^{m \\times n}$ denote the support indicator of $A$, defined by $M_{ij} = 1$ if and only if $A_{ij} \\neq 0$, and $M_{ij} = 0$ otherwise. Define the misleading energy ratio $r(A)$ as\n$$\nr(A) \\coloneqq \\begin{cases}\n\\dfrac{\\lVert ( \\mathbf{1} - M ) \\odot A_1 \\rVert_F^2}{\\lVert A_1 \\rVert_F^2},  \\text{if } \\lVert A_1 \\rVert_F \\neq 0, \\\\[6pt]\n0,  \\text{if } \\lVert A_1 \\rVert_F = 0\n\\end{cases}\n$$\nwhere $\\odot$ denotes the Hadamard product and $\\mathbf{1}$ denotes the all-ones matrix of size $m \\times n$. The ratio $r(A)$ quantifies the fraction of the energy of the best rank-$1$ approximation $A_1$ that is located strictly outside the original support of $A$. All computations are to be carried out over the real numbers.\n\nYour task is to compute $r(A)$ for the following test suite of four matrices, each of size $m = n = 8$:\n\n- Test case $1$ ($8 \\times 8$): A vertical bar. Define $A$ by $A_{ij} = 1$ for all row indices $i \\in \\{2,3,4,5,6,7\\}$ and column index $j = 3$, and $A_{ij} = 0$ otherwise.\n\n- Test case $2$ ($8 \\times 8$): A $2 \\times 2$ pattern embedded in the top-left corner. Set the entries\n$$\n\\begin{bmatrix}\nA_{11}  A_{12} \\\\\nA_{21}  A_{22}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1  0 \\\\\n1  1\n\\end{bmatrix},\n$$\nand $A_{ij} = 0$ for all other indices.\n\n- Test case $3$ ($8 \\times 8$): A different $2 \\times 2$ pattern embedded in the top-left corner. Set the entries\n$$\n\\begin{bmatrix}\nA_{11}  A_{12} \\\\\nA_{21}  A_{22}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1  1 \\\\\n0  1\n\\end{bmatrix},\n$$\nand $A_{ij} = 0$ for all other indices.\n\n- Test case $4$ ($8 \\times 8$): The zero matrix. Set $A_{ij} = 0$ for all indices.\n\nAll indices are one-based in the above description. There are no physical units involved. Angles do not appear. Percentages must not be used anywhere in your output.\n\nFinal output format requirement:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases $1$ through $4$. Each number must be rounded to $6$ decimal places as a decimal (for example, $0.123456$). The required format is\n$$\n[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4].\n$$", "solution": "The problem is well-defined and grounded in the established principles of linear algebra. The core of the task lies in applying the Eckart-Young-Mirsky theorem, which provides the solution to the low-rank approximation problem. I will proceed with a formal solution.\n\nThe problem asks for the computation of the misleading energy ratio $r(A)$ for four given matrices. The ratio is defined as:\n$$\nr(A) \\coloneqq \\begin{cases}\n\\dfrac{\\lVert ( \\mathbf{1} - M ) \\odot A_1 \\rVert_F^2}{\\lVert A_1 \\rVert_F^2},  \\text{if } \\lVert A_1 \\rVert_F \\neq 0, \\\\\n0,  \\text{if } \\lVert A_1 \\rVert_F = 0,\n\\end{cases}\n$$\nwhere $A_1$ is the best rank-$1$ approximation of a matrix $A \\in \\mathbb{R}^{m \\times n}$, $M$ is the support indicator of $A$, $\\mathbf{1}$ is the all-ones matrix, and $\\odot$ is the Hadamard product.\n\nThe foundation for finding $A_1$ is the Eckart-Young-Mirsky theorem. For any matrix $A$ with singular value decomposition (SVD) $A = U \\Sigma V^T$, where the singular values $\\sigma_1 \\geq \\sigma_2 \\geq \\dots \\geq 0$ are the diagonal entries of $\\Sigma$, the best rank-$k$ approximation in the Frobenius norm is given by $A_k = \\sum_{i=1}^k \\sigma_i u_i v_i^T$. For this problem, we require the best rank-$1$ approximation, which corresponds to the largest singular value $\\sigma_1$ and its associated left and right singular vectors $u_1$ and $v_1$. Thus,\n$$\nA_1 = \\sigma_1 u_1 v_1^T.\n$$\nThe Frobenius norm of $A_1$ is directly related to $\\sigma_1$. Since $u_1$ and $v_1$ are orthonormal vectors, we have:\n$$\n\\lVert A_1 \\rVert_F^2 = \\lVert \\sigma_1 u_1 v_1^T \\rVert_F^2 = \\sigma_1^2 \\operatorname{Tr}((u_1 v_1^T)^T (u_1 v_1^T)) = \\sigma_1^2 \\operatorname{Tr}(v_1 u_1^T u_1 v_1^T) = \\sigma_1^2 \\operatorname{Tr}(v_1 (u_1^T u_1) v_1^T) = \\sigma_1^2 \\operatorname{Tr}(v_1 v_1^T) = \\sigma_1^2 \\operatorname{Tr}(v_1^T v_1) = \\sigma_1^2.\n$$\nThe denominator in the expression for $r(A)$ is simply $\\sigma_1^2$. The problem reduces to computing the SVD of each matrix $A$, constructing $A_1$, and then calculating the required norms.\n\nAll matrices are of size $m=n=8$. One-based indexing in the problem description is converted to zero-based for computation.\n\n**Test Case 1: Vertical bar**\nThe matrix $A$ has entries $A_{ij} = 1$ for $i \\in \\{1, 2, 3, 4, 5, 6\\}$ and $j=2$ (zero-based), and $A_{ij}=0$ otherwise. This matrix can be written as the outer product of two vectors: $A = c e_2^T$, where $c$ is a column vector with ones in entries $1$ through $6$, and $e_2$ is the standard basis vector for column index $2$. As $A$ is already a rank-$1$ matrix, its best rank-$1$ approximation is itself, i.e., $A_1 = A$. The support indicator $M$ has ones precisely where $A$ has ones. Consequently, the matrix $(\\mathbf{1} - M)$ has zeros where $A_1$ has non-zero values. The Hadamard product $(\\mathbf{1} - M) \\odot A_1$ is therefore the zero matrix. The numerator $\\lVert (\\mathbf{1} - M) \\odot A_1 \\rVert_F^2$ is $0$. Therefore, $r(A) = 0$.\n\n**Test Case 2: $2 \\times 2$ pattern**\nThe matrix $A$ has a non-zero block in the top-left corner given by $A' = \\begin{pmatrix} 1  0 \\\\ 1  1 \\end{pmatrix}$. The singular values of $A$ are those of $A'$ plus additional zeros. To find the singular values of $A'$, we compute the eigenvalues of $A' A'^T$:\n$$\nA' A'^T = \\begin{pmatrix} 1  0 \\\\ 1  1 \\end{pmatrix} \\begin{pmatrix} 1  1 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 1  1 \\\\ 1  2 \\end{pmatrix}.\n$$\nThe characteristic equation is $\\lambda^2 - 3\\lambda + 1 = 0$, yielding eigenvalues $\\lambda = \\frac{3 \\pm \\sqrt{5}}{2}$. The largest singular value squared is $\\sigma_1^2 = \\frac{3+\\sqrt{5}}{2}$, so $\\sigma_1 = \\frac{1+\\sqrt{5}}{2}$, which is the golden ratio $\\phi$. The best rank-$1$ approximation $A_1$ will have its non-zero entries confined to the same $2 \\times 2$ block, which we call $A'_1$.\nThe SVD components for $A'$ are $u'_1 \\propto \\begin{pmatrix} 1 \\\\ \\sigma_1 \\end{pmatrix}$ and $v'_1 \\propto \\begin{pmatrix} \\sigma_1 \\\\ 1 \\end{pmatrix}$.\nThe approximation is $A'_1 = \\frac{\\sigma_1}{1+\\sigma_1^2} \\begin{pmatrix} \\sigma_1  1 \\\\ \\sigma_1^2  \\sigma_1 \\end{pmatrix}$.\nThe original matrix $A$ has a zero at entry $(0,1)$ (zero-based). The support $M$ has $M_{0,1}=0$. The \"misleading\" energy is thus the squared value of the entry $(A_1)_{0,1}$.\n$$\n(A_1)_{0,1} = (A'_1)_{0,1} = \\frac{\\sigma_1}{1+\\sigma_1^2}.\n$$\nThe ratio $r(A)$ is the squared value of this entry divided by the total energy $\\sigma_1^2$:\n$$\nr(A) = \\frac{ \\left( \\frac{\\sigma_1}{1+\\sigma_1^2} \\right)^2 }{ \\sigma_1^2 } = \\frac{1}{(1+\\sigma_1^2)^2}.\n$$\nSubstituting $\\sigma_1^2 = \\frac{3+\\sqrt{5}}{2}$, we get $1+\\sigma_1^2 = \\frac{5+\\sqrt{5}}{2}$.\n$$\nr(A) = \\frac{1}{\\left(\\frac{5+\\sqrt{5}}{2}\\right)^2} = \\frac{4}{(5+\\sqrt{5})^2} = \\frac{4}{30+10\\sqrt{5}} = \\frac{2}{15+5\\sqrt{5}} = \\frac{3-\\sqrt{5}}{10}.\n$$\nNumerically, this value is approximately $0.076393$.\n\n**Test Case 3: Transposed $2 \\times 2$ pattern**\nThe matrix $A^{(3)}$ in this case has its non-zero block as $A'^{(3)} = \\begin{pmatrix} 1  1 \\\\ 0  1 \\end{pmatrix}$, which is the transpose of the block from Case 2, i.e., $A'^{(3)} = (A'^{(2)})^T$. The SVD of a transposed matrix $A^T = (U\\Sigma V^T)^T = V \\Sigma^T U^T$ shows that it has the same singular values. Thus, $\\sigma_1$ for this case is identical to that of Case 2. The best rank-$1$ approximation is $A_1^{(3)} = (A_1^{(2)})^T$. The support $M^{(3)}$ is the transpose of $M^{(2)}$. The zero in the $2 \\times 2$ block of $A^{(3)}$ is at position $(1,0)$, whereas for $A^{(2)}$ it was at $(0,1)$. The misleading energy for $A^{(3)}$ comes from the entry $(A_1^{(3)})_{1,0}$. By the transpose property, $(A_1^{(3)})_{1,0} = (A_1^{(2)})_{0,1}$. This is the same value that generated the misleading energy in Case 2. Since the numerator (squared leaked energy) and the denominator ($\\sigma_1^2$) are identical for both cases, the ratio $r(A)$ must also be identical.\n$$\nr(A) = \\frac{3-\\sqrt{5}}{10} \\approx 0.076393.\n$$\n\n**Test Case 4: The zero matrix**\nThe matrix $A$ is the $8 \\times 8$ zero matrix. All its singular values are $0$. Thus, $\\sigma_1=0$. The best rank-$1$ approximation is $A_1 = 0 \\cdot u_1 v_1^T = 0$. The Frobenius norm $\\lVert A_1 \\rVert_F = 0$. According to the problem's definition for this scenario, $r(A) = 0$.\n\nThe final results are therefore $[0.0, \\frac{3-\\sqrt{5}}{10}, \\frac{3-\\sqrt{5}}{10}, 0.0]$.", "answer": "[0.000000,0.076393,0.076393,0.000000]", "id": "2435626"}, {"introduction": "Singular Value Decomposition is the engine behind Principal Component Analysis (PCA), a cornerstone technique for dimensionality reduction in modern data science and engineering. However, the interpretation of the principal components as directions of maximum variance makes the method sensitive to data quality. This advanced practice problem [@problem_id:2435636] explores the critical concept of robustness by investigating how a single, large outlier can dramatically skew the variance and commandeer the principal components, potentially misleading the entire analysis.", "problem": "Consider a dataset in $\\mathbb{R}^d$ constructed as follows. There are $N$ baseline samples $x_i \\in \\mathbb{R}^d$ of the form $x_i = s_i a$ for $i \\in \\{1,\\dots,N\\}$, where $a \\in \\mathbb{R}^d$ satisfies $\\|a\\|_2 = 1$, the scalars $s_i \\in \\mathbb{R}$ satisfy $\\sum_{i=1}^N s_i = 0$, and $\\sum_{i=1}^N s_i^2$ is finite. An additional outlier sample $x_{N+1} \\in \\mathbb{R}^d$ is appended with $x_{N+1} = M c$, where $c \\in \\mathbb{R}^d$ satisfies $\\|c\\|_2 = 1$ and $a^\\top c = 0$, and $M  0$ is a scalar parameter. Let $X \\in \\mathbb{R}^{(N+1)\\times d}$ be the data matrix whose $(i$-th$)$ row is $x_i^\\top$. Let $\\bar{x} \\in \\mathbb{R}^d$ denote the sample mean $\\bar{x} = \\frac{1}{N+1}\\sum_{i=1}^{N+1} x_i$, and define the column-centered data matrix $X' = X - \\mathbf{1}\\bar{x}^\\top$, where $\\mathbf{1} \\in \\mathbb{R}^{N+1}$ is the all-ones vector. Let the Singular Value Decomposition (SVD) be $X' = U \\Sigma V^\\top$, where $U \\in \\mathbb{R}^{(N+1)\\times (N+1)}$, $\\Sigma \\in \\mathbb{R}^{(N+1)\\times d}$, and $V \\in \\mathbb{R}^{d\\times d}$, with nonnegative singular values in nonincreasing order on the diagonal of $\\Sigma$. In Principal Component Analysis (PCA), the first principal component is the first column of $V$ (the dominant right singular vector), and the largest singular value equals the square root of the dominant eigenvalue of $(X')^\\top X'$.\n\nWhich of the following statements about how the outlier affects the principal components and the low-rank structure of $X'$ are true?\n\nA. For sufficiently large $M$, the first principal component aligns with $c$.\n\nB. After centering by subtracting $\\bar{x}$, the outlier’s contribution cancels, so the first principal component necessarily remains aligned with $a$ for all $M$.\n\nC. There is a switching threshold determined by $M^2 \\frac{N}{N+1} = \\sum_{i=1}^N s_i^2$ at which the first principal component changes alignment between $a$ and $c$.\n\nD. For large $M$, the largest singular value of $X'$ scales as $\\sqrt{\\frac{N}{N+1}}\\,M$ up to lower-order terms in $M$.\n\nE. If $a^\\top c = 0$, then the top two right singular vectors of $X'$ need not be orthogonal.\n\nF. As $M \\to \\infty$, the squared error of the best rank-$1$ approximation (in the Frobenius norm induced by $X'$) converges to $\\sum_{i=1}^N s_i^2$.", "solution": "We begin by analyzing the structure of the centered data matrix, $X'$. The principal components of the data are the eigenvectors of the covariance-like matrix $S = (X')^\\top X'$. The analysis proceeds as follows.\n\nFirst, we compute the sample mean, $\\bar{x}$:\n$$ \\bar{x} = \\frac{1}{N+1} \\sum_{i=1}^{N+1} x_i = \\frac{1}{N+1} \\left( \\sum_{i=1}^N x_i + x_{N+1} \\right) $$\nGiven $x_i = s_i a$ for $i \\in \\{1,\\dots,N\\}$, $x_{N+1} = M c$, and the condition $\\sum_{i=1}^N s_i = 0$, this simplifies to:\n$$ \\bar{x} = \\frac{1}{N+1} \\left( \\left(\\sum_{i=1}^N s_i\\right) a + M c \\right) = \\frac{1}{N+1} (0 \\cdot a + M c) = \\frac{M}{N+1} c $$\n\nNext, we define the centered data points, $x_i' = x_i - \\bar{x}$.\nFor $i \\in \\{1, \\dots, N\\}$:\n$$ x_i' = s_i a - \\frac{M}{N+1} c $$\nFor $i = N+1$:\n$$ x_{N+1}' = M c - \\frac{M}{N+1} c = \\left( M - \\frac{M}{N+1} \\right) c = \\frac{M(N+1) - M}{N+1} c = \\frac{MN}{N+1} c $$\nThe centered data matrix $X'$ has rows $(x_i')^\\top$.\n\nThe principal components are the eigenvectors of $S = (X')^\\top X'$. We compute this matrix:\n$$ S = \\sum_{i=1}^{N+1} x_i' (x_i')^\\top = \\sum_{i=1}^{N} x_i' (x_i')^\\top + x_{N+1}' (x_{N+1}')^\\top $$\nThe sum over the first $N$ points is:\n$$ \\sum_{i=1}^{N} \\left( s_i a - \\frac{M}{N+1} c \\right) \\left( s_i a - \\frac{M}{N+1} c \\right)^\\top $$\n$$ = \\sum_{i=1}^{N} \\left( s_i^2 a a^\\top - \\frac{s_i M}{N+1} a c^\\top - \\frac{s_i M}{N+1} c a^\\top + \\left(\\frac{M}{N+1}\\right)^2 c c^\\top \\right) $$\nUsing $\\sum_{i=1}^N s_i = 0$ and distributing the summation:\n$$ = \\left(\\sum_{i=1}^{N} s_i^2\\right) a a^\\top - \\frac{M}{N+1}\\left(\\sum_{i=1}^{N} s_i\\right) (a c^\\top + c a^\\top) + N \\left(\\frac{M}{N+1}\\right)^2 c c^\\top $$\n$$ = \\left(\\sum_{i=1}^{N} s_i^2\\right) a a^\\top + N \\frac{M^2}{(N+1)^2} c c^\\top $$\nNow, adding the term for $x_{N+1}'$:\n$$ S = \\left(\\sum_{i=1}^{N} s_i^2\\right) a a^\\top + N \\frac{M^2}{(N+1)^2} c c^\\top + \\left(\\frac{MN}{N+1}\\right)^2 c c^\\top $$\n$$ S = \\left(\\sum_{i=1}^{N} s_i^2\\right) a a^\\top + \\left( N \\frac{M^2}{(N+1)^2} + \\frac{M^2 N^2}{(N+1)^2} \\right) c c^\\top $$\n$$ S = \\left(\\sum_{i=1}^{N} s_i^2\\right) a a^\\top + \\frac{M^2 N(1+N)}{(N+1)^2} c c^\\top $$\n$$ S = \\left(\\sum_{i=1}^{N} s_i^2\\right) a a^\\top + \\frac{M^2 N}{N+1} c c^\\top $$\nLet $\\lambda_a = \\sum_{i=1}^N s_i^2$ and $\\lambda_c(M) = M^2 \\frac{N}{N+1}$. The matrix is $S = \\lambda_a a a^\\top + \\lambda_c(M) c c^\\top$.\nSince $a$ and $c$ are orthonormal ($a^\\top c = 0$, $\\|a\\|_2 = 1$, $\\|c\\|_2=1$), they are eigenvectors of $S$:\n$S a = (\\lambda_a a a^\\top + \\lambda_c(M) c c^\\top) a = \\lambda_a a (a^\\top a) = \\lambda_a a$.\n$S c = (\\lambda_a a a^\\top + \\lambda_c(M) c c^\\top) c = \\lambda_c(M) c (c^\\top c) = \\lambda_c(M) c$.\nThe eigenvalues are $\\lambda_a$ and $\\lambda_c(M)$. Any vector $v$ in the orthogonal complement of the span of $\\{a, c\\}$ is an eigenvector with eigenvalue $0$. The principal components are the eigenvectors of $S$ corresponding to nonzero eigenvalues, ordered by the magnitude of the eigenvalue. The first principal component corresponds to the largest eigenvalue.\n\nNow we evaluate each statement.\n\nA. For sufficiently large $M$, the first principal component aligns with $c$.\nThe eigenvalue associated with vector $a$ is $\\lambda_a = \\sum_{i=1}^N s_i^2$, which is a finite constant. The eigenvalue associated with vector $c$ is $\\lambda_c(M) = M^2 \\frac{N}{N+1}$. As $M \\to \\infty$, $\\lambda_c(M)$ grows without bound, while $\\lambda_a$ remains constant. Therefore, for a sufficiently large $M$, $\\lambda_c(M)  \\lambda_a$. The largest eigenvalue will be $\\lambda_c(M)$, and the corresponding eigenvector (the first principal component) will be $c$. This statement is **Correct**.\n\nB. After centering by subtracting $\\bar{x}$, the outlier’s contribution cancels, so the first principal component necessarily remains aligned with $a$ for all $M$.\nThis assertion is false. The contribution of the outlier does not cancel. The centered outlier is $x_{N+1}' = \\frac{MN}{N+1} c$, which is non-zero. As shown in the analysis for option A, for large $M$, the first principal component aligns with $c$, not $a$. This statement is **Incorrect**.\n\nC. There is a switching threshold determined by $M^2 \\frac{N}{N+1} = \\sum_{i=1}^N s_i^2$ at which the first principal component changes alignment between $a$ and $c$.\nThe first principal component is determined by the larger of the two eigenvalues $\\lambda_a$ and $\\lambda_c(M)$. A switch in the identity of the first principal component occurs when these eigenvalues are equal, i.e., $\\lambda_a = \\lambda_c(M)$. Substituting the expressions for the eigenvalues, we get $\\sum_{i=1}^N s_i^2 = M^2 \\frac{N}{N+1}$. This equation defines the threshold value of $M$ for the switch. This statement is **Correct**.\n\nD. For large $M$, the largest singular value of $X'$ scales as $\\sqrt{\\frac{N}{N+1}}\\,M$ up to lower-order terms in $M$.\nThe singular values $\\sigma_j$ of $X'$ are the square roots of the eigenvalues of $S = (X')^\\top X'$. For large $M$, the largest eigenvalue is $\\lambda_{max} = \\lambda_c(M) = M^2 \\frac{N}{N+1}$. The corresponding largest singular value is $\\sigma_1 = \\sqrt{\\lambda_c(M)} = \\sqrt{M^2 \\frac{N}{N+1}} = M \\sqrt{\\frac{N}{N+1}}$. This expression is exactly what the statement claims, with no lower-order terms. This statement is **Correct**.\n\nE. If $a^\\top c = 0$, then the top two right singular vectors of $X'$ need not be orthogonal.\nThe right singular vectors of $X'$ are the columns of the matrix $V$ in the SVD $X' = U \\Sigma V^\\top$. By definition of the Singular Value Decomposition, the matrix $V$ is an orthogonal matrix. Its columns therefore form an orthonormal set of vectors. Consequently, any two distinct right singular vectors must be orthogonal. The statement is a direct contradiction of the properties of SVD. This statement is **Incorrect**.\n\nF. As $M \\to \\infty$, the squared error of the best rank-$1$ approximation (in the Frobenius norm induced by $X'$) converges to $\\sum_{i=1}^N s_i^2$.\nAccording to the Eckart-Young-Mirsky theorem, the squared Frobenius norm error of the best rank-$k$ approximation of $X'$ is the sum of the squares of the singular values from $k+1$ to the rank of the matrix. For a rank-$1$ approximation ($k=1$), this error is $\\sum_{j=2}^{\\text{rank}} \\sigma_j^2$. The squared singular values are the eigenvalues of $S$. As $M \\to \\infty$, the largest eigenvalue is $\\lambda_1 = \\lambda_c(M) = M^2 \\frac{N}{N+1}$ and the second-largest is $\\lambda_2 = \\lambda_a = \\sum_{i=1}^N s_i^2$. All other eigenvalues are $0$. The squared error is therefore $\\lambda_2 + \\lambda_3 + \\dots = \\lambda_a + 0 + \\dots = \\sum_{i=1}^N s_i^2$. This is a constant value. Thus, the error converges to this value. This statement is **Correct**.", "answer": "$$\\boxed{ACDF}$$", "id": "2435636"}]}