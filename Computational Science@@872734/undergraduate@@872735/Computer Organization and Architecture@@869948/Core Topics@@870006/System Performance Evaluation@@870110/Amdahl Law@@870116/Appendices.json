{"hands_on_practices": [{"introduction": "Real-world applications are rarely uniform in their potential for parallelism, often comprising distinct phases with different characteristics. This practice challenges you to generalize Amdahl's Law to such a multi-phase workload [@problem_id:3620142]. By analyzing a program with three distinct phases, you will learn to calculate the overall system speedup by considering the weighted contribution of each phase's unique serial and parallel fractions.", "problem": "A compute-intensive workload is executed on a homogeneous multicore processor. The workload comprises three distinct phases, labeled A, B, and C. When run on a single core, the phase durations obey the ratio $2:5:3$. Let the single-core execution times for phases A, B, and C be $t_A$, $t_B$, and $t_C$, respectively, such that $t_A:t_B:t_C=2:5:3$. Each phase has a fraction of its work that is perfectly parallelizable across cores: for phase A this fraction is $p_A=0.7$, for phase B it is $p_B=0.9$, and for phase C it is $p_C=0.5$. Assume idealized parallel execution with the following properties: the parallelizable fraction of a phase scales linearly with the number of cores $N$, there is no overhead from synchronization or communication, and the non-parallelizable fraction does not accelerate with additional cores.\n\nStarting from the fundamental definition of speedup $S(N)$ as the ratio of single-core execution time to $N$-core execution time, and the assumption that within each phase the parallelizable and non-parallelizable portions contribute additively to the total phase time, derive a closed-form expression for the overall speedup $S(N)$ of the entire workload as a function of the number of cores $N$. Express your final answer as a simplified analytic expression in terms of $N$. No rounding is required.", "solution": "The problem is well-posed, scientifically grounded, and possesses a unique, meaningful solution. It is a direct application of Amdahl's law to a multi-phase computational workload. All necessary data and conditions are provided, and there are no internal contradictions.\n\nThe overall speedup $S(N)$ for a process executed on $N$ cores is defined as the ratio of the execution time on a single core, $T(1)$, to the execution time on $N$ cores, $T(N)$.\n$$S(N) = \\frac{T(1)}{T(N)}$$\nThe total single-core execution time $T(1)$ is the sum of the execution times of the individual phases A, B, and C:\n$$T(1) = t_A + t_B + t_C$$\nThe problem states the ratio of these times is $t_A:t_B:t_C = 2:5:3$. Let the total single-core execution time be a reference value $T_1$. The fractional contribution of each phase to the total single-core execution time can be calculated from this ratio. The sum of the ratio parts is $2+5+3=10$. Therefore, the fractional times are:\n$$f_A = \\frac{t_A}{T(1)} = \\frac{2}{10} = 0.2$$\n$$f_B = \\frac{t_B}{T(1)} = \\frac{5}{10} = 0.5$$\n$$f_C = \\frac{t_C}{T(1)} = \\frac{3}{10} = 0.3$$\nSo, we can express the single-core phase times as $t_A = 0.2 T(1)$, $t_B = 0.5 T(1)$, and $t_C = 0.3 T(1)$.\n\nFor each phase $i$, a fraction $p_i$ is parallelizable and a fraction $(1-p_i)$ is serial. According to the problem statement, the execution time of the serial portion does not change with the number of cores $N$, while the execution time of the parallelizable portion is reduced by a factor of $N$. The execution time for phase $i$ on $N$ cores, $t_i(N)$, is therefore:\n$$t_i(N) = (1 - p_i)t_i + \\frac{p_i t_i}{N}$$\n\nThe total execution time on $N$ cores, $T(N)$, is the sum of the execution times of the three phases on $N$ cores:\n$$T(N) = t_A(N) + t_B(N) + t_C(N)$$\nSubstituting the expression for $t_i(N)$ for each phase:\n$$T(N) = \\left((1 - p_A)t_A + \\frac{p_A t_A}{N}\\right) + \\left((1 - p_B)t_B + \\frac{p_B t_B}{N}\\right) + \\left((1 - p_C)t_C + \\frac{p_C t_C}{N}\\right)$$\nWe can substitute the fractional times $t_i = f_i T(1)$:\n$$T(N) = \\left((1 - p_A)f_A T(1) + \\frac{p_A f_A T(1)}{N}\\right) + \\left((1 - p_B)f_B T(1) + \\frac{p_B f_B T(1)}{N}\\right) + \\left((1 - p_C)f_C T(1) + \\frac{p_C f_C T(1)}{N}\\right)$$\nFactoring out $T(1)$:\n$$T(N) = T(1) \\left[ \\left( f_A(1 - p_A) + \\frac{f_A p_A}{N} \\right) + \\left( f_B(1 - p_B) + \\frac{f_B p_B}{N} \\right) + \\left( f_C(1 - p_C) + \\frac{f_C p_C}{N} \\right) \\right]$$\nThe overall speedup $S(N)$ is then:\n$$S(N) = \\frac{T(1)}{T(N)} = \\frac{1}{ \\left( f_A(1 - p_A) + \\frac{f_A p_A}{N} \\right) + \\left( f_B(1 - p_B) + \\frac{f_B p_B}{N} \\right) + \\left( f_C(1 - p_C) + \\frac{f_C p_C}{N} \\right) }$$\nThis can be rearranged by grouping the terms that are independent of $N$ and those that are proportional to $1/N$:\n$$S(N) = \\frac{1}{ [f_A(1 - p_A) + f_B(1 - p_B) + f_C(1 - p_C)] + \\frac{1}{N}[f_A p_A + f_B p_B + f_C p_C] }$$\nNow, we substitute the given numerical values: $f_A = 0.2$, $f_B = 0.5$, $f_C = 0.3$, and $p_A = 0.7$, $p_B = 0.9$, $p_C = 0.5$.\n\nFirst, calculate the $N$-independent part of the denominator (the overall serial fraction):\n$$f_A(1 - p_A) + f_B(1 - p_B) + f_C(1 - p_C) = (0.2)(1 - 0.7) + (0.5)(1 - 0.9) + (0.3)(1 - 0.5)$$\n$$= (0.2)(0.3) + (0.5)(0.1) + (0.3)(0.5) = 0.06 + 0.05 + 0.15 = 0.26$$\nAlternatively, using fractions: $\\frac{1}{5}(\\frac{3}{10}) + \\frac{1}{2}(\\frac{1}{10}) + \\frac{3}{10}(\\frac{1}{2}) = \\frac{3}{50} + \\frac{1}{20} + \\frac{3}{20} = \\frac{6}{100} + \\frac{5}{100} + \\frac{15}{100} = \\frac{26}{100}$.\n\nNext, calculate the coefficient of the $1/N$ term (the overall parallelizable fraction):\n$$f_A p_A + f_B p_B + f_C p_C = (0.2)(0.7) + (0.5)(0.9) + (0.3)(0.5)$$\n$$= 0.14 + 0.45 + 0.15 = 0.74$$\nAlternatively, using fractions: $\\frac{1}{5}(\\frac{7}{10}) + \\frac{1}{2}(\\frac{9}{10}) + \\frac{3}{10}(\\frac{1}{2}) = \\frac{7}{50} + \\frac{9}{20} + \\frac{3}{20} = \\frac{14}{100} + \\frac{45}{100} + \\frac{15}{100} = \\frac{74}{100}$.\n\nSubstituting these values back into the expression for $S(N)$:\n$$S(N) = \\frac{1}{0.26 + \\frac{0.74}{N}}$$\nTo obtain a simplified rational expression, we multiply the numerator and the denominator by $N$:\n$$S(N) = \\frac{N}{N(0.26 + \\frac{0.74}{N})} = \\frac{N}{0.26N + 0.74}$$\nTo express this with integer coefficients, we can multiply the numerator and denominator by $100$:\n$$S(N) = \\frac{100N}{100(0.26N + 0.74)} = \\frac{100N}{26N + 74}$$\nThis fraction can be simplified by dividing the numerator and denominator by their greatest common divisor, which is $2$:\n$$S(N) = \\frac{50N}{13N + 37}$$\nThis is the final, closed-form expression for the overall speedup of the workload as a function of the number of cores $N$.", "answer": "$$\\boxed{\\frac{50N}{13N + 37}}$$", "id": "3620142"}, {"introduction": "Amdahl's Law is not merely a tool for predicting performance; it is a fundamental guide for system design. This exercise reframes the law as a prescriptive tool, asking you to determine the necessary parallelizable fraction of a workload to achieve a specific speedup target on a given number of cores [@problem_id:3620143]. This approach mirrors the real-world challenges faced by architects who must make design trade-offs to meet performance goals and consider the practical microarchitectural features that enable high levels of parallelism.", "problem": "A shared-memory multiprocessor with $16$ identical cores is being designed to run a scientific workload. Let the single-core execution time be decomposed into a strictly serial component $T_{s}$ and a parallelizable component $T_{p}$, so that $T_{1} = T_{s} + T_{p}$. When running on $N$ cores, assume the serial component remains serial and the parallelizable component is ideally subdivided across cores with no loss of work or load imbalance, so that the multi-core execution time is $T_{N} = T_{s} + T_{p}/N$. Define the speedup on $N$ cores as $S(N) = T_{1}/T_{N}$ and define the parallelizable fraction on one core as $p = T_{p}/T_{1}$. The design target is $S(16) = 10$. Using only these definitions and assumptions, derive the relationship between $S(N)$, $p$, and $N$, solve for the required $p$ that achieves $S(16) = 10$, and report $p$ exactly as a reduced fraction (do not round and do not use a percentage sign).\n\nFinally, based on your derivation and result, discuss in qualitative terms how microarchitectural changes could make it practically feasible to reach such a parallelizable fraction for similar workloads. Provide at least three concrete changes with brief justifications. Acronyms should be defined on first use, for example, Simultaneous Multithreading (SMT).", "solution": "The problem statement has been validated and is deemed sound. It is a well-posed problem grounded in the principles of computer architecture, specifically Amdahl's Law.\n\nThe solution is approached in three parts as requested. First, we derive the general relationship for speedup. Second, we solve for the specific parallelizable fraction required. Third, we discuss microarchitectural implications.\n\n**Part 1: Derivation of the Speedup Formula**\n\nWe are given the following definitions:\nThe single-core execution time is $T_{1} = T_{s} + T_{p}$, where $T_{s}$ is the serial component and $T_{p}$ is the parallelizable component.\nThe multi-core execution time on $N$ cores is $T_{N} = T_{s} + \\frac{T_{p}}{N}$.\nThe speedup is defined as $S(N) = \\frac{T_{1}}{T_{N}}$.\nThe parallelizable fraction is defined as $p = \\frac{T_{p}}{T_{1}}$.\n\nOur goal is to express $S(N)$ as a function of $p$ and $N$. We begin by expressing $T_{s}$ and $T_{p}$ in terms of $T_{1}$ and $p$.\nFrom the definition of the parallelizable fraction, we have:\n$$T_{p} = p \\cdot T_{1}$$\nThe serial fraction of the workload, let's call it $s$, is the remaining portion. Thus, $s = 1 - p$. The serial time component $T_s$ is this fraction of the total single-core time:\n$$T_{s} = (1 - p) \\cdot T_{1}$$\nWe can verify this: $T_s + T_p = (1-p)T_1 + pT_1 = T_1 - pT_1 + pT_1 = T_1$, which is consistent.\n\nNow, we substitute these expressions for $T_{s}$ and $T_{p}$ into the formula for the multi-core execution time $T_{N}$:\n$$T_{N} = ((1 - p) \\cdot T_{1}) + \\frac{p \\cdot T_{1}}{N}$$\nWe can factor out $T_{1}$:\n$$T_{N} = T_{1} \\left( (1 - p) + \\frac{p}{N} \\right)$$\nFinally, we substitute this expression for $T_{N}$ into the speedup formula $S(N)$:\n$$S(N) = \\frac{T_{1}}{T_{1} \\left( (1 - p) + \\frac{p}{N} \\right)}$$\nThe $T_{1}$ terms cancel, yielding the general relationship between $S(N)$, $p$, and $N$, which is the well-known formulation of Amdahl's Law:\n$$S(N) = \\frac{1}{(1 - p) + \\frac{p}{N}}$$\n\n**Part 2: Calculation of the Parallelizable Fraction $p$**\n\nWe are given the design target that a speedup of $S(16) = 10$ must be achieved with $N=16$ cores. We use the derived formula to solve for the required parallelizable fraction $p$.\nSubstitute $N=16$ and $S(16)=10$ into the equation:\n$$10 = \\frac{1}{(1 - p) + \\frac{p}{16}}$$\nTo solve for $p$, we take the reciprocal of both sides:\n$$0.1 = (1 - p) + \\frac{p}{16}$$\nNow, we rearrange the terms to isolate $p$:\n$$0.1 = 1 - p + \\frac{p}{16}$$\n$$p - \\frac{p}{16} = 1 - 0.1$$\n$$p \\left(1 - \\frac{1}{16}\\right) = 0.9$$\n$$p \\left(\\frac{16}{16} - \\frac{1}{16}\\right) = \\frac{9}{10}$$\n$$p \\left(\\frac{15}{16}\\right) = \\frac{9}{10}$$\nNow, we solve for $p$:\n$$p = \\frac{9}{10} \\cdot \\frac{16}{15}$$\n$$p = \\frac{9 \\cdot 16}{10 \\cdot 15} = \\frac{144}{150}$$\nTo report this as a reduced fraction, we find the greatest common divisor of the numerator and denominator. Both are divisible by $6$:\n$$p = \\frac{144 \\div 6}{150 \\div 6} = \\frac{24}{25}$$\nSo, to achieve a speedup of $10$ on $16$ cores, the workload must be $96\\%$ parallelizable. This means the strictly serial portion of the code can only account for $1-p = 1 - \\frac{24}{25} = \\frac{1}{25}$, or $4\\%$, of the total single-core execution time.\n\n**Part 3: Microarchitectural Changes**\n\nAchieving such a high parallelizable fraction ($p = 0.96$) is challenging. The model assumes ideal parallelization of the $T_p$ component and treats $T_s$ as a fixed, immutable quantity. In practice, both the effective serial component and the efficiency of parallel execution are heavily influenced by the processor's microarchitecture. Below are three concrete microarchitectural changes that could help a workload achieve a higher effective parallelizable fraction.\n\n1.  **Aggressive Out-of-Order and Speculative Execution:** The serial component $T_s$ is, by definition, not parallelizable at the thread level. However, it can still contain instruction-level parallelism (ILP). A more powerful out-of-order execution engine can reduce the wall-clock time required for this serial section. This involves increasing the size of key structures like the re-order buffer (ROB), which allows the processor to look further ahead in the instruction stream past blocking instructions, and improving the accuracy of the branch predictor (e.g., using a TAGE (TAgged GEometric history length) predictor). By reducing the execution time of $T_s$ while $T_p$ remains constant, the ratio $T_s/T_1$ decreases, which in turn increases the parallelizable fraction $p = 1 - T_s/T_1$.\n\n2.  **Hardware Acceleration for Synchronization:** A significant source of serialization in many parallel algorithms is the overhead of synchronization (e.g., locks, barriers, atomic operations). These operations can force all but one core to be idle, effectively extending the serial part of the execution. Introducing specialized hardware support can drastically reduce this overhead. For example, Hardware Transactional Memory (HTM) allows sections of code that would typically require locks to be executed speculatively in parallel. The hardware detects data conflicts, and only in the case of a conflict is a rollback and retry necessary. This can convert serially-executed critical sections into parallel ones. Similarly, dedicated hardware units for Atomic Memory Operations (AMOs) can execute these instructions much faster than software-emulated equivalents.\n\n3.  **Advanced Cache Coherence and On-Chip Interconnect:** The model's assumption of ideal speedup for the parallel portion ($T_p/N$) implies that data can be accessed by all cores without contention or latency penalties. This is not realistic. As the core count $N$ increases, contention for shared data in the memory hierarchy becomes a primary performance limiter. A high-performance on-chip network (Network-on-Chip, NoC) with high bandwidth and low latency is critical for efficient inter-core communication. Furthermore, a sophisticated cache coherence protocol is needed to manage shared data. For a $16$-core system, a directory-based protocol is more scalable than a snooping-based protocol. Improvements like larger, partitioned Last-Level Caches (LLCs) or a Non-Uniform Cache Architecture (NUCA) can also reduce average memory access latency by placing data closer to the cores that need it. These features make the parallel execution more efficient, helping the actual performance approach the theoretical ideal of $T_p/N$.", "answer": "$$\\boxed{\\frac{24}{25}}$$", "id": "3620143"}, {"introduction": "Performance measurements in real systems can sometimes yield surprising results, such as speedups that exceed the number of processors used—a phenomenon known as superlinear speedup. This practice delves into this apparent paradox, showing that it does not invalidate Amdahl's Law but instead highlights the importance of the law's underlying assumptions [@problem_id:3620139]. By examining how cache effects can dramatically reduce the cost of computation in a parallel setting, you will learn to think critically about performance models and the real-world factors that influence them.", "problem": "A program processes a large array of records using a fixed algorithm whose control flow and data accesses do not depend on the number of threads. On a single core of a dual-socket Central Processing Unit (CPU), the total runtime is $T_1 = 40$ seconds. Cycle-accurate profiling on this single-core run reports that a fraction $f_s = 0.10$ of $T_1$ is inherently serial (initialization, I/O setup, and a reduction that cannot be parallelized), and the remaining fraction is amenable to data-parallel execution with no algorithmic change. The processor has $2$ sockets; each socket has $8$ cores and a private last-level cache per socket of size $16$ MiB. Dynamic Random-Access Memory (DRAM) latency is approximately $100$ nanoseconds, whereas last-level cache hit latency is approximately $12$ nanoseconds. The working set when the program is run on one core is about $24$ MiB and exhibits a high miss rate in the last-level cache, causing the parallelizable portion to be memory-stall dominated on the single-core baseline.\n\nYou now run the exact same source code with $N = 4$ threads, pinned as $2$ threads per socket (so that each socket works on approximately half of the data). The measured parallel runtime is $T_4 = 7.5$ seconds. Assume negligible synchronization overhead and perfect load balance.\n\nUsing only fundamental definitions such as the definition of speedup $S(N) = T_1/T_N$, and the decomposition of runtime into serial and parallelizable parts under fixed per-operation cost, answer the following. Which option best explains why the measured speedup exceeds the value predicted by applying the serial/parallel decomposition to the single-core profile, and why the foundational bound on ideal parallelism still applies when improvements due to locality are excluded?\n\nA. The predicted speedup from the single-core serial fraction is about $3.08$, whereas the measured speedup is $S(4) = 40/7.5 \\approx 5.33$. This apparent “superlinear” behavior arises because partitioning the data across sockets reduces each socket’s working set from 24 MiB to 12 MiB, which now fits in the 16 MiB last-level cache, drastically reducing memory-stall time in the parallelizable part relative to the single-core baseline. Amdahl’s bound on ideal parallelism still applies if we hold locality constant: for example, by using an equally cache-friendly blocked traversal on the single-core baseline (or by scaling the problem so the per-socket working set again exceeds the last-level cache), we remove the locality advantage, returning to a regime where the maximum speedup is limited solely by the serial fraction.\n\nB. The predicted speedup is about $3.08$, but the measured speedup $5.33$ proves that Amdahl’s law is false on modern hardware with caches. Because caches are part of the machine, there is no meaningful upper bound on speedup for a fixed problem size as $N$ increases.\n\nC. The predicted speedup from the single-core serial fraction is too pessimistic because the serial fraction itself decreases as $N$ grows due to overlapping memory latency across threads. If we replace $f_s$ by $f_s/N$, the prediction will match the measurement, so there is no inconsistency and no special role for cache locality.\n\nD. The observed result is an instance of weak scaling. By invoking Gustafson’s law, the speedup can legitimately exceed both the Amdahl prediction and the processor count, so no additional explanation involving cache effects or exclusion of locality gains is needed.\n\nE. The increase in aggregate cache capacity with more cores mathematically increases the parallel fraction even at fixed problem size, and therefore Amdahl’s law directly permits speedups larger than those predicted from the single-core serial fraction without altering any assumptions or baselines; no special treatment of locality is required.", "solution": "We begin from fundamental definitions. The speedup for $N$ processing elements is defined as\n$$\nS(N) \\equiv \\frac{T_1}{T_N}.\n$$\nLet the single-thread runtime decompose as $T_1 = T_s + T_p$, where $T_s$ is the time spent in code that is inherently serial and $T_p$ is the time spent in code that is parallelizable under fixed per-operation cost. The serial fraction is defined as\n$$\nf_s \\equiv \\frac{T_s}{T_1}, \\quad 0 \\le f_s \\le 1.\n$$\nIf the parallelizable portion scales ideally across $N$ threads without changing the cost per unit of work, then the parallel runtime under these fixed-cost assumptions is\n$$\nT_N = T_s + \\frac{T_p}{N} = f_s T_1 + \\frac{(1-f_s) T_1}{N},\n$$\nwhich implies the well-known bound on speedup obtained from the definition:\n$$\nS(N) = \\frac{T_1}{T_N} = \\frac{1}{f_s + \\frac{1-f_s}{N}}.\n$$\nThis bound rests on the premise that the per-operation cost in the parallelizable region is unchanged by the act of parallelization; it considers only the effect of concurrency on the parallel portion’s wall time.\n\nCompute the predicted speedup from the single-core serial fraction and the measured speedup. We are given $T_1 = 40$ seconds and $f_s = 0.10$. With $N = 4$,\n$$\nS_{\\text{pred}}(4) = \\frac{1}{0.10 + \\frac{0.90}{4}} = \\frac{1}{0.10 + 0.225} = \\frac{1}{0.325} \\approx 3.0769.\n$$\nThe measured parallel runtime is $T_4 = 7.5$ seconds, so the measured speedup is\n$$\nS_{\\text{meas}}(4) = \\frac{40}{7.5} \\approx 5.3333,\n$$\nwhich exceeds both $S_{\\text{pred}}(4)$ and the processor count $N = 4$ (a “superlinear” speedup). Why can this happen without contradicting the bound derived above?\n\nThe bound assumed that the parallel portion’s per-operation cost is unchanged by parallelization. In the given machine and workload, the single-core baseline works on a $24$ MiB working set on one socket with a $16$ MiB last-level cache. Because $24 \\text{ MiB} > 16 \\text{ MiB}$, the last-level cache cannot hold the working set, leading to a high miss rate and frequent Dynamic Random-Access Memory (DRAM) accesses with latency on the order of $100$ nanoseconds. The parallel run uses $N = 4$ threads pinned as $2$ per socket. The data is partitioned, so each socket processes roughly $12$ MiB. Now $12 \\text{ MiB}  16 \\text{ MiB}$, so each socket’s working set fits in its last-level cache, reducing memory access latency to approximately $12$ nanoseconds for most accesses. Consequently, the per-operation cost in the parallelizable portion drops substantially compared to the single-thread baseline. This change violates the fixed-cost assumption in the above derivation and is best interpreted as a locality improvement, not as pure parallelism. As a result, $T_p$ itself shrinks when parallelized, which allows the measured $S(4)$ to exceed the bound computed under fixed-cost assumptions.\n\nWhy does the foundational bound on ideal parallelism still apply when excluding locality gains? If we hold locality constant across the comparison—by ensuring that the single-core baseline enjoys the same cache behavior as the parallel run, or equivalently that the parallel run does not alter the miss rate relative to the single-core baseline—then the per-operation cost does not change and the derivation applies. Two equivalent ways to enforce this are:\n- Use an equally cache-friendly traversal (for example, tiling or domain blocking) on the single-core baseline so that its working set during the parallelizable region also fits in the last-level cache, matching the locality of the parallel execution; or\n- Scale the problem size so that, even after partitioning across sockets, each socket’s working set still exceeds the last-level cache (iso-locality or iso-miss-rate setup), thereby preventing the per-operation cost from dropping in the parallel case.\n\nUnder such iso-locality conditions, the change in runtime from $T_1$ to $T_N$ arises from concurrency alone, and the speedup is bounded by\n$$\nS(N) = \\frac{1}{f_s + \\frac{1-f_s}{N}},\n$$\nwith the $f_s$ measured on the baseline that shares the same locality. The “extra” speedup observed in the original measurement is thus attributable to improved locality (reduced memory-stall time) and is not a violation of the bound on ideal parallelism; it reflects a different cost model rather than a breakdown of the serial/parallel decomposition.\n\nOption-by-option analysis:\n- Option A states the numerical comparison $S_{\\text{pred}}(4) \\approx 3.08$ versus $S_{\\text{meas}}(4) \\approx 5.33$, identifies the mechanism (data partitioning reduces per-socket working set to $12$ MiB, which fits in a $16$ MiB last-level cache, cutting memory-stall time), and correctly explains that Amdahl’s bound applies when locality is held constant by making the single-core baseline equally cache-friendly or by ensuring the parallel run does not change the miss rate. This is Correct.\n- Option B claims Amdahl’s law is false on modern hardware and that there is no meaningful upper bound for fixed-size problems. This misinterprets the situation: the bound is derived under fixed per-operation cost, and the observed excess speedup results from a change in per-operation cost due to locality, not from pure parallelism. This is Incorrect.\n- Option C asserts that the serial fraction $f_s$ decreases as $N$ grows due to overlapping memory latency, effectively replacing $f_s$ by $f_s/N$. The serial fraction pertains to inherently non-parallelizable work; overlapping memory latency in the parallelizable region does not convert serial work into parallel work. This is a modeling error. This is Incorrect.\n- Option D invokes weak scaling and Gustafson’s law, but the experiment is strong scaling with fixed problem size $T_1 = 40$ seconds on one core versus $T_4 = 7.5$ seconds on four threads. Gustafson’s law does not explain superlinear speedup at fixed problem size, nor does it obviate the need to account for cache effects. This is Incorrect.\n- Option E suggests that aggregate cache capacity directly and “mathematically” increases the parallel fraction so that Amdahl’s law permits larger speedups without changing assumptions. The serial/parallel fractions are defined on a baseline under fixed per-operation cost; changing cache behavior changes that cost model. One must either change the baseline or acknowledge that the speedup includes locality gains beyond ideal parallelism. This is Incorrect.\n\nTherefore, the only correct choice is Option A.", "answer": "$$\\boxed{A}$$", "id": "3620139"}]}