{"hands_on_practices": [{"introduction": "The way data is organized in memory is not merely a matter of programming convenience; it has profound implications for performance. This exercise delves into the critical concept of spatial locality by comparing two fundamental data layouts: the Array of Structures (AoS) and the Structure of Arrays (SoA). By meticulously calculating the bandwidth wasted when accessing non-contiguous fields, you will develop a concrete understanding of why aligning data structures with memory access patterns is essential for maximizing memory throughput. [@problem_id:3621491]", "problem": "A streaming kernel traverses an array of structures and, for each structure, reads exactly two $4$-byte scalar fields: one at byte offset $0$ and one at byte offset $32$ within the structure. The array contains $N = 4096$ structures, each structure has total size $48$ bytes, and the base address of the array is aligned to a $64$-byte boundary. The memory system uses a $64$-byte cache line and, on a cache miss, transfers the entire cache line from main memory; cache line transfers are aligned to $64$-byte boundaries and cannot merge across lines. The loop is long enough and scheduled such that it saturates the sustained read bandwidth of the memory subsystem at $S = 51.2$ GB/s (decimal gigabytes per second).\n\nConsider two data layout variants for the same access pattern:\n1. Array of Structures (AoS): a single array stores all fields of each structure contiguously in memory in the $48$-byte record order described above. The kernel reads only the two fields at offsets $0$ and $32$ for every structure and never touches the other bytes.\n2. Structure of Arrays (SoA): two separate arrays store the two fields independently, each as a contiguous array of $4$-byte elements; the kernel streams through both arrays reading each element once. Both arrays are $64$-byte aligned.\n\nStarting from core definitions of memory bandwidth as bytes transferred per unit time and the behavior of cache line fills, and without invoking any pre-derived shortcut results, determine the wasted bandwidth $B$ (in GB/s) attributable specifically to the non-contiguous placement of the two fields in the AoS layout, relative to the SoA layout, under the stated assumptions. Express the final value in GB/s and round your answer to four significant figures.", "solution": "The fundamental quantities are the sustained memory bandwidth $S$ (bytes per unit time) and the bytes actually needed by the program versus the bytes that must be transferred due to $64$-byte cache line granularity. The wasted bandwidth is defined as the fraction of transferred bytes that the computation does not use, multiplied by $S$.\n\nWe analyze the two layouts.\n\nAoS analysis. Each structure has size $48$ bytes, and the two accessed fields are at offsets $0$ and $32$. The base address is aligned to a $64$-byte boundary. Let the structure index be $k \\in \\{0,1,\\dots,N-1\\}$. The byte addresses of the two fields for structure $k$ are\n$$\na_k = 48k + 0,\\qquad b_k = 48k + 32.\n$$\nLet the cache line size be $L = 64$ bytes. The cache line index containing an address $x$ is $\\ell(x) = \\left\\lfloor \\frac{x}{L} \\right\\rfloor$. The memory system transfers, for each distinct cache line index that any of the accessed addresses falls into, $L$ bytes, aligned to $64$-byte boundaries.\n\nThe access pattern to the $48$-byte structures creates a repeating memory footprint. Since the least common multiple of the structure size ($48$) and the cache line size ($64$) is $192$ bytes, the pattern repeats every $192$ bytes. This corresponds to $192 / 48 = 4$ structures and $192 / 64 = 3$ cache lines. Therefore, every 4 structures require exactly 3 cache lines to be fetched.\nSince there are $N = 4096$ structures, and $4096$ is a multiple of $4$, we can calculate the total number of cache lines fetched.\n$$\n\\text{Total Cache Lines} = \\frac{N}{4} \\times 3 = \\frac{4096}{4} \\times 3 = 1024 \\times 3 = 3072 \\text{ lines}.\n$$\nThe total bytes transferred for the AoS layout is:\n$$\nT_{\\text{AoS}} = 3072 \\times L = 3072 \\times 64 \\text{ bytes} = 196608 \\text{ bytes}.\n$$\nThe useful bytes actually used by the computation are two $4$-byte fields for each of the $N$ structures:\n$$\nU = N \\times (4 + 4) \\text{ bytes} = 4096 \\times 8 \\text{ bytes} = 32768 \\text{ bytes}.\n$$\nThe wasted fraction for the AoS layout is:\n$$\nw_{\\text{AoS}} = 1 - \\frac{U}{T_{\\text{AoS}}} = 1 - \\frac{32768}{196608} = 1 - \\frac{1}{6} = \\frac{5}{6}.\n$$\n\nSoA analysis. In the Structure of Arrays (SoA) layout, the two fields reside in two separate contiguous arrays. Each array contains $N=4096$ elements of $4$ bytes each. The total size of each array is $4096 \\times 4 = 16384$ bytes. Since $16384$ is perfectly divisible by the cache line size of $64$ ($16384/64 = 256$), streaming through each array is perfectly efficient. All bytes transferred are useful.\nThe total bytes transferred for the SoA layout is equal to the useful bytes:\n$$\nT_{\\text{SoA}} = U = 32768 \\text{ bytes}.\n$$\nThe wasted fraction for the SoA layout is:\n$$\nw_{\\text{SoA}} = 0.\n$$\n\nThe wasted bandwidth attributable specifically to the AoS layout relative to the SoA layout is the total sustained bandwidth multiplied by the difference in wasted fractions.\n$$\nB_{\\text{wasted}} = S \\cdot (w_{\\text{AoS}} - w_{\\text{SoA}}) = S \\cdot w_{\\text{AoS}}.\n$$\nSubstituting the numerical values:\n$$\nB_{\\text{wasted}} = 51.2 \\text{ GB/s} \\cdot \\frac{5}{6} \\approx 42.666... \\text{ GB/s}.\n$$\nRounded to four significant figures, the wasted bandwidth is $42.67$ GB/s.", "answer": "$$\\boxed{42.67}$$", "id": "3621491"}, {"introduction": "In modern multicore processors, the total memory bandwidth is consumed not only by fetching data from main memory but also by transferring data between processor caches to ensure a coherent view of memory. This practice explores \"false sharing,\" a notorious performance issue where unrelated data items, located within the same cache line, are accessed by different threads. By tracing the states of the MESI coherence protocol, you will quantify the hidden bandwidth cost of this phenomenon and appreciate how data layout is a cornerstone of scalable parallel software. [@problem_id:3621446]", "problem": "A shared-memory multiprocessor has $n$ symmetric cores, each with a private Level 1 (L1) write-back cache that implements the Modified, Exclusive, Shared, Invalid (MESI) protocol with write-invalidate semantics and Read For Ownership (RFO) on stores. The cache line size is $64$ bytes. Consider $n=8$ threads pinned one-to-one to the $8$ cores. Each thread repeatedly updates its own $8$-byte field inside a single shared struct whose $8$ fields are laid out contiguously and aligned so that all $8$ fields reside within the same cache line. A global barrier ensures a deterministic update order: in each iteration, thread $1$ performs one store to its field, then thread $2$ performs one store to its field, and so on up to thread $8$, after which the sequence repeats indefinitely. Assume the cache line never evicts once first touched, and ignore all traffic other than full-line data transfers on coherence events (treat invalidation messages as negligible in data volume). As a baseline, consider the same computation but with each thread’s field placed on a different cache line so that there is no false sharing; after warm-up, this baseline induces zero inter-core data movement per update.\n\nStarting from fundamental definitions of cache coherence and ownership in MESI and without invoking any pre-derived shortcut formulas, determine the steady-state average additional interconnect data volume per update that is caused solely by false sharing in the described scenario, compared to the no–false-sharing baseline. Express your final answer in bytes per update.", "solution": "The problem asks for the steady-state average additional interconnect data volume per update caused by false sharing. We analyze this by tracing the state of the single shared cache line ($CL$) according to the MESI protocol. The cache line size is $L_s = 64$ bytes.\n\nThe baseline case involves no false sharing, meaning each thread accesses a different cache line. After an initial write, each core $i$ holds its line in the **Modified (M)** state. Subsequent writes by thread $i$ are local hits and generate no interconnect traffic. Thus, the baseline data volume is $0$ bytes per update.\n\nIn the false sharing scenario, all $8$ threads operate on the same cache line $CL$. The execution is a strict sequence of stores: Thread 1, Thread 2, ..., Thread 8. We analyze the steady-state behavior.\n\nLet's assume the system is in a steady state. Just before Thread 1 performs its store, Thread 8 must have completed the previous iteration's last store. To do this, Core 8 must have acquired exclusive ownership of $CL$. This means $CL$ is in the **M** state in Core 8's cache ($C_8$) and in the **Invalid (I)** state in all other caches ($C_1, \\dots, C_7$).\n\n1.  **Thread 1 performs a store:**\n    -   Core 1 has $CL$ in state **I**. A store to an invalid line is a write miss, which triggers a Read For Ownership (RFO) request on the interconnect.\n    -   Core 8 snoops this RFO. Since it holds $CL$ in state **M**, it must respond by flushing the entire $64$-byte cache line to the interconnect and invalidating its own copy (transitioning from **M** to **I**).\n    -   Core 1 receives the $64$-byte line, performs its store, and updates its state for $CL$ to **M**.\n    -   The data volume generated by this update is one full cache line transfer: $64$ bytes.\n\n2.  **Thread 2 performs a store:**\n    -   The situation repeats. At this point, Core 1 holds $CL$ in state **M**, and all other cores (including Core 2) hold it in state **I**.\n    -   Core 2 issues an RFO.\n    -   Core 1 responds by flushing the $64$-byte line and invalidating its copy.\n    -   Core 2 receives the line and sets its state to **M**.\n    -   The data volume for this update is again $64$ bytes.\n\nThis exact sequence of events occurs for every single store in the iteration. Each thread's store invalidates the line for all other threads and requires the new writer to fetch the line from the previous writer. This cache line \"ping-pongs\" between the cores.\n\nAn iteration consists of $N=8$ updates. Each update causes a cache-to-cache transfer of one full cache line.\n- Total data volume per iteration = $N \\times L_s = 8 \\times 64 \\text{ bytes} = 512 \\text{ bytes}$.\n- Average data volume per update = $\\frac{\\text{Total volume per iteration}}{\\text{Number of updates}} = \\frac{512 \\text{ bytes}}{8} = 64 \\text{ bytes}$.\n\nThe additional data volume compared to the zero-traffic baseline is therefore $64 - 0 = 64$ bytes per update.", "answer": "$$\n\\boxed{64}\n$$", "id": "3621446"}, {"introduction": "To overcome the ever-widening gap between processor speed and memory latency, modern CPUs employ hardware prefetchers that speculatively fetch data into the cache. While often effective, this strategy carries the risk of \"polluting\" the cache and wasting bandwidth on data that is never used. This problem provides a model to connect high-level performance counters, such as prefetcher accuracy and coverage, to the tangible cost of wasted memory bandwidth. This analysis will equip you with the tools to reason about the trade-offs inherent in advanced memory system optimizations. [@problem_id:3621496]", "problem": "A multicore processor uses a stream prefetcher that is throttled by the memory controller to limit interference in the Dynamic Random Access Memory (DRAM) subsystem. The memory controller enforces a nominal prefetch bandwidth budget of $B_{\\text{pref}}$ (in GB/s). Hardware performance counters report two prefetcher-quality metrics over a long, steady-state interval for a single running application:\n- Accuracy $a$: the fraction of prefetched cache lines that are subsequently accessed by a demand instruction before eviction.\n- Coverage $c$: the fraction of the interval during which the prefetcher is actively issuing prefetches at its budgeted rate.\n\nAssume the following operational model grounded in first principles:\n- Bandwidth is data volume per unit time.\n- When the prefetcher is active, it issues prefetch requests at the enforced budget $B_{\\text{pref}}$.\n- Each prefetched byte is either useful (eventually accessed by a demand instruction) or useless (never accessed by a demand instruction).\n- Accuracy $a$ can be interpreted as the conditional probability that a prefetched byte is useful; therefore, the probability that a prefetched byte is useless is $1 - a$.\n\nUnder this model, derive from these definitions an expression for the average DRAM bandwidth wasted on incorrect prefetches (bytes that are fetched by the prefetcher but never used by demand) over the entire interval, and then compute its value for $a = 0.72$, $c = 0.60$, and $B_{\\text{pref}} = 24$ GB/s. Express the final wasted bandwidth in GB/s and round your answer to four significant figures.", "solution": "The problem requires us to derive an expression for the average DRAM bandwidth wasted on incorrect prefetches, denoted $B_{\\text{wasted}}$, based on the provided definitions.\n\nFirst, let's determine the average total bandwidth consumed by the prefetcher over the entire interval. The prefetcher is active for a fraction of time $c$, during which it consumes bandwidth at a rate of $B_{\\text{pref}}$. For the remaining fraction of time, $1-c$, it is inactive and consumes zero bandwidth. The time-averaged total prefetch bandwidth, $\\bar{B}_{\\text{pref}}$, is therefore:\n$$\n\\bar{B}_{\\text{pref}} = (c \\times B_{\\text{pref}}) + ((1-c) \\times 0) = c \\cdot B_{\\text{pref}}\n$$\nThis represents the average rate at which data is being fetched into the DRAM subsystem due to prefetching activity over the entire observation interval.\n\nNext, we consider the fraction of this prefetched data that is wasted. Accuracy, $a$, is defined as the fraction of prefetched data that is useful. By definition, the fraction of prefetched data that is useless (fetched but never used) is the complement of the accuracy:\n$$\n\\text{Fraction wasted} = 1 - a\n$$\n\nThe average wasted bandwidth, $B_{\\text{wasted}}$, is the average rate at which useless data is transferred. This is the product of the average total prefetch bandwidth and the fraction of that bandwidth that corresponds to useless data.\n$$\nB_{\\text{wasted}} = (\\text{Fraction wasted}) \\times (\\text{Average total prefetch bandwidth})\n$$\nSubstituting our expressions, we get the final formula:\n$$\nB_{\\text{wasted}} = (1 - a) \\cdot \\bar{B}_{\\text{pref}} = (1 - a) \\cdot c \\cdot B_{\\text{pref}}\n$$\n\nNow, we can compute the numerical value using the given parameters: $a = 0.72$, $c = 0.60$, and $B_{\\text{pref}} = 24$ GB/s.\n$$\nB_{\\text{wasted}} = (1 - 0.72) \\cdot 0.60 \\cdot 24 \\text{ GB/s}\n$$\n$$\nB_{\\text{wasted}} = 0.28 \\cdot 0.60 \\cdot 24 \\text{ GB/s}\n$$\n$$\nB_{\\text{wasted}} = 0.168 \\cdot 24 \\text{ GB/s}\n$$\n$$\nB_{\\text{wasted}} = 4.032 \\text{ GB/s}\n$$\nThe result, $4.032$, has four significant figures as required.", "answer": "$$\n\\boxed{4.032}\n$$", "id": "3621496"}]}