{"hands_on_practices": [{"introduction": "To truly grasp how out-of-order (OOO) execution works, there's no substitute for simulating the process step-by-step. This exercise challenges you to trace a sequence of instructions on a superscalar OOO core, comparing the ideal completion time based solely on data dependencies with the actual time achieved under realistic hardware constraints [@problem_id:3662826]. By doing so, you will develop a concrete understanding of how structural hazards and issue width limitations create performance bottlenecks.", "problem": "Consider an instruction-level dataflow graph that models a straight-line program fragment in an Out-of-Order (OOO) superscalar core. Each node $i$ is an instruction with latency $L_i$ (in cycles), and there is a directed edge $j \\rightarrow i$ if $i$ depends on the result of $j$. Assume that operands not produced by any node (e.g., constants) are ready at cycle $0$. The graph has the following nodes and edges, with latencies:\n- Nodes and latencies:\n  - $I_1$: load, $L_1 = 3$\n  - $I_2$: load, $L_2 = 3$\n  - $I_3$: multiply, $L_3 = 3$\n  - $I_4$: integer add, $L_4 = 1$\n  - $I_5$: integer add, $L_5 = 1$\n  - $I_6$: multiply, $L_6 = 3$\n  - $I_7$: integer add, $L_7 = 1$\n  - $I_8$: store, $L_8 = 1$\n- Directed edges (true dependencies):\n  - $I_1 \\rightarrow I_3$, $I_2 \\rightarrow I_3$, $I_2 \\rightarrow I_5$, $I_3 \\rightarrow I_4$, $I_4 \\rightarrow I_6$, $I_5 \\rightarrow I_6$, $I_6 \\rightarrow I_7$, $I_7 \\rightarrow I_8$\n- All other inputs to $I_4$, $I_5$, and $I_7$ are independent constants ready at cycle $0$. Addresses for $I_1$, $I_2$, and $I_8$ are ready at cycle $0$, and the memory system has no cache misses or conflicts.\n\nArchitecture model (fixed for this problem):\n- The core performs Out-of-Order (OOO) execution with register renaming, unbounded Reservation Stations (RS), and an unbounded Reorder Buffer (ROB).\n- Issue width $W = 2$: at most $2$ instructions can be issued per cycle.\n- Functional units (all fully pipelined):\n  - One load unit (for $I_1$, $I_2$) that can start at most one load per cycle; load latency $3$.\n  - Two identical integer arithmetic logic units (ALUs) for adds (for $I_4$, $I_5$, $I_7$); each has latency $1$ and can accept one new add per cycle.\n  - One multiply unit (for $I_3$, $I_6$) with latency $3$; it can accept one new multiply per cycle.\n  - One store unit (for $I_8$) with latency $1$; it can accept one new store per cycle.\n- Timing semantics: if an instruction starts at cycle $t$ with latency $L$, its result becomes available at the start of cycle $t+L$. A dependent instruction may start at cycle $t+L$ provided a matching functional unit can accept it and the issue width is not exceeded in that cycle.\n- Deterministic tie-breaking: in any cycle, among all ready instructions that could be issued given unit availability, the scheduler selects in increasing instruction index order until the per-cycle issue width is reached.\n\nTask:\n1. Compute the earliest start time $\\operatorname{EST}(I_i)$ for each node $I_i$ using only data dependencies and latencies (i.e., assuming unlimited functional units and unlimited issue width). Then compute the critical-path completion time $T_{\\mathrm{CP}}$ for the final instruction $I_8$ under these dependency-only constraints.\n2. Under the given OOO core constraints, compute the actual scheduled completion time $T_{\\mathrm{OOO}}$ of $I_8$ by simulating cycle-by-cycle issue subject to readiness, functional unit availability, and the issue width. Use the timing semantics and tie-breaking rule exactly as specified.\n3. Let $\\Delta = T_{\\mathrm{OOO}} - T_{\\mathrm{CP}}$. What is the value of $\\Delta$? Express your answer in cycles as a single real number. Do not include any unit label in the final numeric answer. No rounding is required.", "solution": "The user wants me to solve a computer architecture problem involving out-of-order execution.\n\n### Step 1: Extract Givens\n- **Nodes and Latencies ($L_i$)**:\n  - $I_1$: load, $L_1 = 3$ cycles\n  - $I_2$: load, $L_2 = 3$ cycles\n  - $I_3$: multiply, $L_3 = 3$ cycles\n  - $I_4$: integer add, $L_4 = 1$ cycle\n  - $I_5$: integer add, $L_5 = 1$ cycle\n  - $I_6$: multiply, $L_6 = 3$ cycles\n  - $I_7$: integer add, $L_7 = 1$ cycle\n  - $I_8$: store, $L_8 = 1$ cycle\n- **Directed Edges (Dependencies)**:\n  - $I_1 \\rightarrow I_3$, $I_2 \\rightarrow I_3$, $I_2 \\rightarrow I_5$, $I_3 \\rightarrow I_4$, $I_4 \\rightarrow I_6$, $I_5 \\rightarrow I_6$, $I_6 \\rightarrow I_7$, $I_7 \\rightarrow I_8$\n- **Initial Conditions**:\n  - Operands not produced by any node are ready at cycle $0$.\n  - Addresses for $I_1$, $I_2$, and $I_8$ are ready at cycle $0$.\n- **Architecture Model**:\n  - Out-of-Order (OOO) execution, unbounded Reservation Stations (RS) and Reorder Buffer (ROB).\n  - Issue width $W = 2$ instructions per cycle.\n  - Functional Units (all fully pipelined):\n    - $1$ load unit (can start $1$ load/cycle).\n    - $2$ integer ALUs (each can start $1$ add/cycle, total $2$ adds/cycle).\n    - $1$ multiply unit (can start $1$ multiply/cycle).\n    - $1$ store unit (can start $1$ store/cycle).\n- **Timing Semantics**:\n  - Instruction starting at cycle $t$ with latency $L$ makes its result available at the start of cycle $t+L$.\n- **Tie-Breaking Rule**:\n  - Among ready instructions, issue in increasing instruction index order.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is a standard exercise in computer architecture, specifically on instruction scheduling in an out-of-order superscalar processor.\n- **Scientifically Grounded**: The problem is based on established principles of computer organization and architecture, including dataflow graphs, critical path analysis, and modeling of OOO execution with resource constraints. All concepts are standard and well-defined.\n- **Well-Posed**: The problem is clearly stated with all necessary parameters (latencies, dependencies, machine model). The deterministic tie-breaking rule ensures a unique solution exists for the scheduling simulation.\n- **Objective**: The language is precise and technical, free of subjectivity or ambiguity.\n\nThe problem is self-contained and internally consistent. It does not violate any scientific principles, is not ill-posed, and is not trivial.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n**Part 1: Critical Path Analysis**\nFirst, we compute the earliest start time, $\\operatorname{EST}(I_i)$, and earliest completion time, $\\operatorname{ECT}(I_i)$, for each instruction, considering only data dependencies. We assume unlimited resources. The $\\operatorname{EST}$ for an instruction is the maximum of the $\\operatorname{ECT}$s of all instructions it depends on. An instruction with no dependencies can start at cycle $0$. The completion time is $\\operatorname{ECT}(I_i) = \\operatorname{EST}(I_i) + L_i$.\n\n- $\\operatorname{EST}(I_1) = 0$. $\\operatorname{ECT}(I_1) = 0 + L_1 = 0 + 3 = 3$.\n- $\\operatorname{EST}(I_2) = 0$. $\\operatorname{ECT}(I_2) = 0 + L_2 = 0 + 3 = 3$.\n- $\\operatorname{EST}(I_3) = \\max(\\operatorname{ECT}(I_1), \\operatorname{ECT}(I_2)) = \\max(3, 3) = 3$. $\\operatorname{ECT}(I_3) = 3 + L_3 = 3 + 3 = 6$.\n- $\\operatorname{EST}(I_5) = \\operatorname{ECT}(I_2) = 3$. $\\operatorname{ECT}(I_5) = 3 + L_5 = 3 + 1 = 4$.\n- $\\operatorname{EST}(I_4) = \\operatorname{ECT}(I_3) = 6$. $\\operatorname{ECT}(I_4) = 6 + L_4 = 6 + 1 = 7$.\n- $\\operatorname{EST}(I_6) = \\max(\\operatorname{ECT}(I_4), \\operatorname{ECT}(I_5)) = \\max(7, 4) = 7$. $\\operatorname{ECT}(I_6) = 7 + L_6 = 7 + 3 = 10$.\n- $\\operatorname{EST}(I_7) = \\operatorname{ECT}(I_6) = 10$. $\\operatorname{ECT}(I_7) = 10 + L_7 = 10 + 1 = 11$.\n- $\\operatorname{EST}(I_8) = \\operatorname{ECT}(I_7) = 11$.\n\nThe critical-path completion time for the final instruction $I_8$ is $T_{\\mathrm{CP}} = \\operatorname{ECT}(I_8) = \\operatorname{EST}(I_8) + L_8 = 11 + 1 = 12$.\n\n**Part 2: OOO Execution Simulation**\nNext, we simulate the execution on the specified OOO core, considering issue width $W=2$ and functional unit constraints. We track the start time `S(i)` and completion time `C(i)` for each instruction $I_i$.\n\n- **Cycle 0**:\n  - Instructions ready with all operands: $I_1, I_2$.\n  - Scheduler considers $I_1$, then $I_2$.\n  - $I_1$ requires the load unit, which is available. Issue $I_1$. $S(I_1)=0$, $C(I_1)=0+3=3$.\n  - $I_2$ requires the load unit. However, the single load unit can only start one instruction per cycle. $I_2$ cannot be issued.\n  - Issued: {$I_1$}.\n\n- **Cycle 1**:\n  - Instruction $I_2$ is still ready.\n  - The load unit is available to start a new operation. Issue $I_2$. $S(I_2)=1$, $C(I_2)=1+3=4$.\n  - Issued: {$I_2$}.\n\n- **Cycles 2-3**:\n  - No instructions are ready. At the start of cycle $3$, $I_1$ completes.\n\n- **Cycle 4**:\n  - At the start of this cycle, $I_2$ completes. Result available from $I_1$ (since cycle $3$) and $I_2$ (now).\n  - $I_3$ becomes ready: its inputs from $I_1$ and $I_2$ are available. Ready time is $\\max(C(I_1), C(I_2)) = \\max(3, 4) = 4$.\n  - $I_5$ becomes ready: its input from $I_2$ is available. Ready time is $C(I_2) = 4$.\n  - Ready queue: {$I_3, I_5$}.\n  - Scheduler considers $I_3$ (needs multiply unit, available) and $I_5$ (needs ALU, available). Both can be issued since $W=2$.\n  - Issue $I_3$: $S(I_3)=4$, $C(I_3)=4+3=7$.\n  - Issue $I_5$: $S(I_5)=4$, $C(I_5)=4+1=5$.\n  - Issued: {$I_3, I_5$}.\n\n- **Cycles 5-6**:\n  - At the start of cycle $5$, $I_5$ completes. No new instructions become ready ($I_6$ is waiting for $I_4$).\n  - No instructions are issued.\n\n- **Cycle 7**:\n  - At the start of this cycle, $I_3$ completes.\n  - $I_4$ becomes ready: its input from $I_3$ is available. Ready time is $C(I_3)=7$.\n  - Issue $I_4$ (needs ALU, available). $S(I_4)=7$, $C(I_4)=7+1=8$.\n  - Issued: {$I_4$}.\n\n- **Cycle 8**:\n  - At the start of this cycle, $I_4$ completes.\n  - $I_6$ becomes ready: its inputs from $I_4$ and $I_5$ are available. Ready time is $\\max(C(I_4), C(I_5)) = \\max(8, 5) = 8$.\n  - Issue $I_6$ (needs multiply unit, available). $S(I_6)=8$, $C(I_6)=8+3=11$.\n  - Issued: {$I_6$}.\n\n- **Cycles 9-10**:\n  - No instructions are issued.\n\n- **Cycle 11**:\n  - At the start of this cycle, $I_6$ completes.\n  - $I_7$ becomes ready: input from $I_6$ is available. Ready time is $C(I_6)=11$.\n  - Issue $I_7$ (needs ALU, available). $S(I_7)=11$, $C(I_7)=11+1=12$.\n  - Issued: {$I_7$}.\n\n- **Cycle 12**:\n  - At the start of this cycle, $I_7$ completes.\n  - $I_8$ becomes ready: input from $I_7$ is available. Ready time is $C(I_7)=12$.\n  - Issue $I_8$ (needs store unit, available). $S(I_8)=12$, $C(I_8)=12+1=13$.\n  - Issued: {$I_8$}.\n\nThe simulation finishes when $I_8$ completes. The actual scheduled completion time is $T_{\\mathrm{OOO}} = C(I_8) = 13$.\n\n**Part 3: Calculation of $\\Delta$**\nThe problem asks for the value of $\\Delta = T_{\\mathrm{OOO}} - T_{\\mathrm{CP}}$.\nUsing the values calculated in the previous parts:\n$T_{\\mathrm{CP}} = 12$\n$T_{\\mathrm{OOO}} = 13$\n$\\Delta = 13 - 12 = 1$.\n\nThe delay is caused by the structural hazard at cycle $0$, where both $I_1$ and $I_2$ compete for the single load unit. This forces $I_2$ to be delayed by one cycle, and this one-cycle delay propagates along the critical path to the final instruction.", "answer": "$$\n\\boxed{1}\n$$", "id": "3662826"}, {"introduction": "Register renaming is the core mechanism that breaks false dependencies to enable out-of-order execution, but it requires a pool of physical registers to store speculative results. This exercise moves from simulation to analytical modeling, asking you to determine the minimum physical register file size needed to sustain a given throughput without stalling [@problem_id:3662897]. This practice bridges the gap between high-level performance goals and the concrete microarchitectural resources required to achieve them.", "problem": "Consider a single-threaded loop running on a register-renamed out-of-order core with in-order commit and precise exceptions. The processor implements classic register renaming with a rename map table and a free list of physical registers. On renaming a destination, a new physical register is allocated immediately, and the previously mapped physical register for that architectural name is returned to the free list only when the redefining instruction commits. The commit stage updates the architectural map in order and frees the previous physical mapping upon commit of the redefining instruction, not earlier.\n\nAssume the following steady-state properties of the loop and machine:\n- The loop body has exactly $k$ distinct scalar temporaries, each represented by a unique architectural register and redefined exactly once per iteration.\n- The loop runs at a sustained initiation interval of $1$ iteration per $1$ cycle in steady state (that is, a new iteration begins renaming every $1$ cycle).\n- For each temporary, the time from its rename (new destination allocation) in a given iteration to the in-order commit of the instruction that redefines it in that iteration is a constant $D$ cycles. You may treat $D$ as an integer and the same for all $k$ temporaries.\n- There are no other architectural registers redefined by the loop besides these $k$ temporaries, and you may ignore all other forms of resource contention (for example, assume the reorder buffer and schedulers are sufficiently large so that only the physical register free list can cause a renaming stall).\n\nUsing only the definitions of register renaming, liveness, and in-order commit provided above, derive from first principles the minimal number of physical registers $R_{phys}$ required to guarantee that the rename stage never stalls due to an empty free list while the loop runs in steady state. Express your answer as a closed-form expression in terms of $k$ and $D$. Do not provide an inequality. No numerical rounding is required, and no units are needed in the final expression.", "solution": "The problem requires the derivation of the minimal number of physical registers, $R_{phys}$, to guarantee that a processor's rename stage does not stall. The system is characterized by a loop running in a steady state on an out-of-order core.\n\nThe fundamental principle for solving this problem is to determine the total number of physical registers that are simultaneously in use (allocated and not yet freed) during this steady state. To prevent stalls, the physical register file must be large enough to accommodate all of them. This number can be found by calculating the total lifetime of a physical register and multiplying it by the rate at which registers are allocated.\n\nLet's first establish the rate of allocation. The problem states:\n- The loop body has $k$ distinct scalar temporaries, each redefined once per iteration.\n- The loop runs at a sustained initiation interval of $1$ iteration per $1$ cycle.\nThis means that every cycle, one new iteration begins, and the rename stage must allocate a new physical register for each of the $k$ redefined architectural registers.\nThus, the rate of allocation is $k$ physical registers per cycle.\n\nNext, let's determine the lifetime of a physical register. The lifetime spans from its allocation at the rename stage to its deallocation, when it is returned to the free list. The problem specifies the deallocation rule: \"the previously mapped physical register for that architectural name is returned to the free list only when the redefining instruction commits.\"\n\nConsider a single architectural register, $A$. Let's track the physical registers mapped to it across consecutive iterations.\n- In iteration $i$, let the instruction that redefines $A$ be renamed at cycle $t_i$. At this time, a new physical register, let's call it $P_i$, is allocated for $A$.\n- In the next iteration, $i+1$, the instruction that redefines $A$ is renamed. Since the initiation interval is $1$, this occurs at cycle $t_{i+1} = t_i + 1$. A new physical register, $P_{i+1}$, is allocated for $A$.\n\nNow, let's determine when $P_i$ is freed. According to the rule, $P_i$ (the \"previously mapped physical register\") is freed when the \"redefining instruction\" (the one from iteration $i+1$) commits.\nThe problem states that the time from an instruction's rename to its commit is a constant $D$ cycles. The instruction from iteration $i+1$ was renamed at cycle $t_{i+1}$. Therefore, it commits at cycle $t_{commit} = t_{i+1} + D$.\nSubstituting $t_{i+1} = t_i + 1$:\n$$t_{commit} = (t_i + 1) + D = t_i + D + 1$$\nThis is the cycle at which the physical register $P_i$ is freed.\n\nThe lifetime of the physical register $P_i$ is the duration from its allocation at cycle $t_i$ until it is freed at cycle $t_i + D + 1$. The number of cycles it remains occupied is:\n$$\\text{Lifetime} = (t_i + D + 1) - t_i = D + 1 \\text{ cycles}$$\n\nWe have now established the two necessary components for our calculation:\n$1$. The rate at which physical registers are consumed from the free list is $k$ registers per cycle.\n$2$. The duration for which each a physical register remains consumed is $D+1$ cycles.\n\nIn a steady-state system, the total number of occupied resources is the product of the arrival rate and the service time. This is an application of Little's Law. The total number of simultaneously live physical registers, $N_{live}$, is:\n$$N_{live} = (\\text{registers allocated per cycle}) \\times (\\text{lifetime in cycles})$$\n$$N_{live} = k \\times (D+1)$$\n\nTo guarantee that the rename stage never stalls due to an empty free list, the total number of available physical registers, $R_{phys}$, must be at least this number. The minimal number required is therefore:\n$$R_{phys} = k(D+1)$$\n\nAn alternative derivation confirms this result by considering a snapshot of the machine's state. At any instant, the physical register file must hold the values for the last committed (architectural) state, plus all in-flight (speculative) states.\n- The committed state for the $k$ loop temporaries requires $k$ physical registers.\n- An instruction is in-flight from its rename until its commit. The time from rename to commit is $D$ cycles. With an initiation interval of $1$, there are always $D$ iterations' worth of instructions that have been renamed but have not yet committed. For example, if an instruction renamed at cycle $t$ commits at $t+D$, then at cycle $T$, the instructions renamed at cycles $T-1, T-2, \\ldots, T-D$ are all in-flight.\n- These $D$ in-flight iterations each allocated $k$ physical registers, accounting for $D \\times k$ speculative registers.\nThe total number of occupied registers is the sum of those for the committed state and the speculative states:\n$$R_{phys} = k_{\\text{committed}} + (D \\times k)_{\\text{speculative}} = k + kD = k(D+1)$$\nBoth derivations conclude that the minimal number of physical registers required is $k(D+1)$.", "answer": "$$\\boxed{k(D+1)}$$", "id": "3662897"}, {"introduction": "Modern processors aggressively speculate on memory dependencies to maximize performance, often executing loads before the addresses of all older stores are known. This can lead to memory ordering violations that require costly replays. This problem introduces a probabilistic model to analyze the performance impact of such hazards, allowing you to quantify the \"replay storm risk\" as a function of memory aliasing probability [@problem_id:3662876].", "problem": "A superscalar out-of-order core uses a Reorder Buffer (ROB) and a Load-Store Queue (LSQ). The core employs a memory-dependence predictor that allows a younger load to execute before all older stores have resolved their effective addresses. Consider the following scientifically realistic hazard scenario. An older store to address $A$ has a late-resolving effective address due to long-latency address generation. A younger load to the same address $A$ issues speculatively and obtains a value from the cache. When the older store finally resolves, the machine detects a memory ordering violation (the load read before an older store to the same address), squashes the younger load and all of its dependent instructions, and marks the load for replay. Due to the predictor’s behavior, the replayed load is permitted to reissue again before the older store’s address has resolved, potentially repeating the same violation. This can lead to a cascade of re-executions known as a replay storm.\n\nModel this as follows. Each speculative execution attempt of the younger load before the older store’s address is known is a Bernoulli trial. On each attempt, with probability $\\beta \\in (0,1)$, the attempt is violating (because it aliases an as-yet-unresolved older store that will later be found to target the same address), and with probability $1-\\beta$, the attempt is non-violating (either because the address dependence does not exist or because the older store has resolved, preventing further violation). When an attempt is violating, the load is replayed; attempts continue until the first non-violating attempt occurs. Assume independence across attempts.\n\nDefine the replay storm risk $\\mathcal{R}_{s}(\\beta)$ to be the expected number of extra executions of the load caused by this hazard event, beyond the ideal single execution that would occur with perfect ordering (that is, count only replays, not the final successful execution). Starting from the fundamental definitions of out-of-order speculation, memory dependence hazards, and Bernoulli trials, derive a closed-form analytic expression for $\\mathcal{R}_{s}(\\beta)$ as a function of $\\beta$.\n\nExpress your final answer as a simplified closed-form expression in terms of $\\beta$ only. No units are required. No numerical rounding is required.", "solution": "The problem statement describes a scenario of memory-ordering violations in a superscalar out-of-order processor and models it as a stochastic process. The validation confirms that the problem is scientifically grounded in the principles of computer architecture, is mathematically well-posed, and uses objective, formalizable language. The premises are consistent and sufficient for deriving a unique solution. Therefore, we may proceed with the derivation.\n\nThe core of the problem lies in modeling the repeated execution attempts of a younger load instruction. Each attempt is described as a Bernoulli trial. Let us define the outcomes of a single trial:\n- A 'failure' corresponds to a violating attempt, which leads to a squash and replay. The probability of this event is given as $\\beta$.\n- A 'success' corresponds to a non-violating attempt, which allows the instruction to commit and the process to terminate. The probability of this event is $1-\\beta$.\n\nThe attempts are independent and repeated until the first success occurs. This is the definition of a process governed by a geometric distribution. Let $N$ be the random variable representing the total number of attempts required to achieve the first success. The probability mass function (PMF) of $N$ is given by:\n$$\nP(N=n) = (\\beta)^{n-1}(1-\\beta), \\quad \\text{for } n = 1, 2, 3, \\dots\n$$\nwhere $n-1$ is the number of failures (violating attempts) before the first success (non-violating attempt) on the $n$-th trial. The parameter of this geometric distribution is the success probability $p = 1-\\beta$.\n\nThe expected value of a geometrically distributed random variable $N$ with success probability $p$ is a standard result in probability theory:\n$$\nE[N] = \\frac{1}{p}\n$$\nIn our context, with $p = 1-\\beta$, the expected total number of executions (both violating and non-violating) is:\n$$\nE[N] = \\frac{1}{1-\\beta}\n$$\n\nThe problem asks for the replay storm risk, $\\mathcal{R}_{s}(\\beta)$, which is defined as the expected number of *extra* executions, i.e., the number of replays. The total number of executions, $N$, is composed of the final successful execution and all the preceding failed executions (replays). If the total number of executions is $N$, then the number of replays is $N-1$.\n\nWe are therefore asked to compute $\\mathcal{R}_{s}(\\beta) = E[N-1]$.\n\nUsing the linearity of the expectation operator, we can write:\n$$\n\\mathcal{R}_{s}(\\beta) = E[N-1] = E[N] - E[1]\n$$\nThe expectation of a constant is the constant itself, so $E[1]=1$. Substituting the expression for $E[N]$, we get:\n$$\n\\mathcal{R}_{s}(\\beta) = \\frac{1}{1-\\beta} - 1\n$$\nTo simplify this into a single closed-form expression, we find a common denominator:\n$$\n\\mathcal{R}_{s}(\\beta) = \\frac{1}{1-\\beta} - \\frac{1-\\beta}{1-\\beta} = \\frac{1 - (1-\\beta)}{1-\\beta}\n$$\nSimplifying the numerator gives:\n$$\n\\mathcal{R}_{s}(\\beta) = \\frac{1 - 1 + \\beta}{1-\\beta} = \\frac{\\beta}{1-\\beta}\n$$\nThis is the closed-form analytic expression for the expected number of replays as a function of the violation probability $\\beta$. The derivation starts from the fundamental definition of Bernoulli trials as specified in the problem, identifies the resulting geometric distribution, and uses its properties to find the desired expected value.", "answer": "$$\n\\boxed{\\frac{\\beta}{1 - \\beta}}\n$$", "id": "3662876"}]}