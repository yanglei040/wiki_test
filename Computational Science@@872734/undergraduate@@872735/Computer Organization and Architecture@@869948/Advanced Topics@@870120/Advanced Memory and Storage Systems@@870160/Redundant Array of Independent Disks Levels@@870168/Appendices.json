{"hands_on_practices": [{"introduction": "The core purpose of RAID is to provide resilience against disk failures. However, the level of protection varies significantly between different RAID configurations. This first exercise challenges you to move beyond simple definitions by systematically analyzing how different RAID levels—RAID 1, 5, 6, and 10—respond to a scenario of two simultaneous disk failures. By counting the specific combinations of failed disks that lead to irrecoverable data loss, you will develop a concrete, first-principles understanding of their distinct fault tolerance capabilities [@problem_id:3675057].", "problem": "An administrator must choose among four Redundant Array of Independent Disks (RAID) configurations for different services. Assume independent disk failures and ideal controllers that implement the standard parity and mirroring semantics of each level, with no latent sector errors and no further failures during rebuild. For each configuration below, suppose exactly $f=2$ physical disks fail simultaneously. Define the failure matrix for a configuration as the mapping from each unordered pair of failed disks to one of two outcomes: recoverable (all logical data blocks can be reconstructed) or irrecoverable (some logical data blocks cannot be reconstructed). Using only first principles about mirroring and parity, determine for each configuration the number $N_{\\mathrm{I}}$ of irrecoverable two-disk pairs and whether rebuild is feasible in the recoverable cases.\n\n- Configuration $\\mathcal{R}_1$: RAID $1$ on $2$ disks (pure mirroring).\n- Configuration $\\mathcal{R}_5$: RAID $5$ on $7$ disks with distributed single parity per stripe.\n- Configuration $\\mathcal{R}_6$: RAID $6$ on $8$ disks with dual independent parity per stripe.\n- Configuration $\\mathcal{R}_{10}$: RAID $10$ on $8$ disks arranged as $4$ mirrored pairs and striped across pairs.\n\nWhich option correctly characterizes the counts $N_{\\mathrm{I}}(\\mathcal{R}_1)$, $N_{\\mathrm{I}}(\\mathcal{R}_5)$, $N_{\\mathrm{I}}(\\mathcal{R}_6)$, $N_{\\mathrm{I}}(\\mathcal{R}_{10})$ and the rebuild feasibility statements?\n\nA. $N_{\\mathrm{I}}(\\mathcal{R}_1)=1$, $N_{\\mathrm{I}}(\\mathcal{R}_5)=21$, $N_{\\mathrm{I}}(\\mathcal{R}_6)=0$, $N_{\\mathrm{I}}(\\mathcal{R}_{10})=4$. In all recoverable cases, rebuild is feasible by copying from surviving mirrors or solving parity equations without data loss.\n\nB. $N_{\\mathrm{I}}(\\mathcal{R}_1)=0$, $N_{\\mathrm{I}}(\\mathcal{R}_5)=15$, $N_{\\mathrm{I}}(\\mathcal{R}_6)=6$, $N_{\\mathrm{I}}(\\mathcal{R}_{10})=28$. Rebuild is not feasible in RAID $6$ even when parity is available.\n\nC. $N_{\\mathrm{I}}(\\mathcal{R}_1)=1$, $N_{\\mathrm{I}}(\\mathcal{R}_5)=0$, $N_{\\mathrm{I}}(\\mathcal{R}_6)=0$, $N_{\\mathrm{I}}(\\mathcal{R}_{10})=4$. RAID $5$ can recover two simultaneous failures using distributed parity.\n\nD. $N_{\\mathrm{I}}(\\mathcal{R}_1)=1$, $N_{\\mathrm{I}}(\\mathcal{R}_5)=21$, $N_{\\mathrm{I}}(\\mathcal{R}_6)=0$, $N_{\\mathrm{I}}(\\mathcal{R}_{10})=12$. RAID $10$ suffers irrecoverable loss whenever the two failed disks lie in the same stripe set even if they are from different mirrored pairs.", "solution": "The user-provided problem is critically validated before any attempt at a solution.\n\n### Step 1: Extract Givens\n- **Task**: For four specified RAID configurations, determine the number of irrecoverable two-disk failure pairs, denoted $N_{\\mathrm{I}}$.\n- **Assumptions**:\n    - Exactly $f=2$ physical disks fail simultaneously.\n    - Disk failures are independent.\n    - Controllers are ideal and implement standard RAID semantics.\n    - No latent sector errors.\n    - No further failures during rebuild.\n- **Definitions**:\n    - **Recoverable**: All logical data blocks can be reconstructed.\n    - **Irrecoverable**: Some logical data blocks cannot be reconstructed.\n- **Configurations**:\n    - $\\mathcal{R}_1$: RAID $1$ on $2$ disks (pure mirroring).\n    - $\\mathcal{R}_5$: RAID $5$ on $7$ disks (distributed single parity).\n    - $\\mathcal{R}_6$: RAID $6$ on $8$ disks (dual independent parity).\n    - $\\mathcal{R}_{10}$: RAID $10$ on $8$ disks, arranged as $4$ mirrored pairs striped across.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is evaluated for validity.\n- **Scientifically Grounded**: The problem is based on the well-defined, standard principles of RAID technology, a core topic in computer systems and operating systems. The concepts of mirroring, parity, striping, and fault tolerance are fundamental and factually sound.\n- **Well-Posed**: The problem is clearly defined. It asks for a specific, quantifiable metric ($N_{\\mathrm{I}}$) for each of four precisely described system configurations under a fixed failure condition ($f=2$). The terms \"recoverable\" and \"irrecoverable\" are explicitly defined. A unique solution can be derived from first principles.\n- **Objective**: The problem is stated in precise, technical language, free from subjectivity or ambiguity.\n- **Conclusion**: The problem does not violate any of the specified invalidity criteria. It is scientifically sound, well-posed, objective, and self-contained for a standard computer science context.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be derived.\n\n### Derivation\nThe number of irrecoverable two-disk failure pairs, $N_{\\mathrm{I}}$, is determined for each configuration by applying the first principles of data redundancy for that RAID level. The total number of ways to choose $2$ disks to fail from a set of $n$ disks is given by the binomial coefficient $\\binom{n}{2} = \\frac{n(n-1)}{2}$.\n\n**Configuration $\\mathcal{R}_1$: RAID $1$ on $n=2$ disks**\n- RAID $1$ with $2$ disks involves one disk being an exact mirror of the other. All data on disk $1$ is duplicated on disk $2$.\n- The total number of disks is $n=2$.\n- The total number of distinct two-disk failure combinations is $\\binom{2}{2} = 1$.\n- This single combination involves the failure of both disks in the array. Since both copies of the data are lost, reconstruction is impossible. The failure is irrecoverable.\n- Therefore, the number of irrecoverable pairs is $N_{\\mathrm{I}}(\\mathcal{R}_1) = 1$.\n\n**Configuration $\\mathcal{R}_5$: RAID $5$ on $n=7$ disks**\n- RAID $5$ stripes data across $n-1=6$ disks and stores a single parity block on the remaining disk for each stripe. The parity block is the XOR sum of the data blocks in that stripe.\n- This single parity information allows the reconstruction of data from exactly one failed disk in a stripe. For a stripe with data blocks $D_1, \\dots, D_6$ and parity $P = D_1 \\oplus \\dots \\oplus D_6$, the loss of any single block (e.g., $D_i$) allows its recovery by XORing the remaining blocks.\n- If two disks fail, two blocks within a given stripe are lost (e.g., $D_i$ and $D_j$). This leaves one equation with two unknowns ($D_i \\oplus D_j = \\text{known value}$), which cannot be solved uniquely.\n- In a RAID $5$ array, data is striped such that any pair of disks will contain blocks belonging to the same stripe. Therefore, the failure of any two disks will render at least one stripe unrecoverable, leading to data loss.\n- The total number of disks is $n=7$.\n- The total number of two-disk failure combinations is $\\binom{7}{2} = \\frac{7 \\times 6}{2} = 21$.\n- Since any two-disk failure is irrecoverable in RAID $5$, all $21$ combinations result in data loss.\n- Therefore, the number of irrecoverable pairs is $N_{\\mathrm{I}}(\\mathcal{R}_5) = 21$.\n\n**Configuration $\\mathcal{R}_6$: RAID $6$ on $n=8$ disks**\n- RAID $6$ extends RAID $5$ by adding a second, independent parity block to each stripe, specifically to protect against dual disk failures. A stripe consists of $n-2=6$ data blocks and $2$ parity blocks.\n- The two parity schemes (e.g., $P$ and $Q$) provide a system of two independent linear equations for the blocks in a stripe. For instance, $P = \\bigoplus D_i$ and $Q = \\bigoplus g^i D_i$ (using operations in a Galois Field, a standard implementation).\n- If any two disks fail, two blocks are lost in each affected stripe. The remaining blocks and the two parity equations form a solvable system of two equations with two unknowns. This allows for the unique reconstruction of the two lost blocks.\n- This protection holds true for any pair of failed disks, whether they hold data or parity.\n- The total number of disks is $n=8$.\n- The total number of two-disk failure combinations is $\\binom{8}{2} = \\frac{8 \\times 7}{2} = 28$.\n- Since RAID $6$ is designed to survive any two-disk failure, all $28$ combinations are recoverable.\n- Therefore, the number of irrecoverable pairs is $N_{\\mathrm{I}}(\\mathcal{R}_6) = 0$.\n\n**Configuration $\\mathcal{R}_{10}$: RAID $10$ on $n=8$ disks**\n- This configuration is a stripe of mirrors (RAID 1+0). There are $4$ mirrored pairs, and data is striped across these pairs. Let the pairs be $(D_{1A}, D_{1B})$, $(D_{2A}, D_{2B})$, $(D_{3A}, D_{3B})$, and $(D_{4A}, D_{4B})$.\n- Data is recoverable as long as at least one disk in each mirrored pair remains operational.\n- An irrecoverable failure occurs if and only if both disks in a single mirrored pair fail simultaneously. This is because all data stored on that pair would be lost, and since data is striped across all pairs, the entire logical volume is compromised.\n- The pairs whose simultaneous failure leads to irrecoverable data loss are precisely the $4$ mirrored pairs: $(D_{1A}, D_{1B})$, $(D_{2A}, D_{2B})$, $(D_{3A}, D_{3B})$, and $(D_{4A}, D_{4B})$.\n- The total number of disks is $n=8$. The total number of two-disk failure combinations is $\\binom{8}{2} = 28$.\n- The number of irrecoverable combinations is the number of mirrored pairs, which is $4$.\n- Therefore, the number of irrecoverable pairs is $N_{\\mathrm{I}}(\\mathcal{R}_{10}) = 4$.\n\n**Rebuild Feasibility**\nThe problem asks whether rebuild is feasible in the recoverable cases.\n- For $\\mathcal{R}_1$ and $\\mathcal{R}_5$, there are no recoverable cases for a two-disk failure, so any statement about them is vacuously true.\n- For $\\mathcal{R}_6$, all $28$ two-disk failure cases are recoverable. Rebuild is feasible by solving the two parity equations for each stripe.\n- For $\\mathcal{R}_{10}$, there are $28 - 4 = 24$ recoverable two-disk failure cases. In these cases, the failed disks belong to different mirrored pairs. Rebuild is feasible by simply copying the data from the surviving member of each affected pair.\n- Thus, the general assertion that \"In all recoverable cases, rebuild is feasible...\" is correct.\n\n### Summary of Results\n- $N_{\\mathrm{I}}(\\mathcal{R}_1) = 1$\n- $N_{\\mathrm{I}}(\\mathcal{R}_5) = 21$\n- $N_{\\mathrm{I}}(\\mathcal{R}_6) = 0$\n- $N_{\\mathrm{I}}(\\mathcal{R}_{10}) = 4$\n- Rebuild is feasible in all recoverable cases, either via parity calculation or mirroring.\n\n### Option-by-Option Analysis\n\n**A. $N_{\\mathrm{I}}(\\mathcal{R}_1)=1$, $N_{\\mathrm{I}}(\\mathcal{R}_5)=21$, $N_{\\mathrm{I}}(\\mathcal{R}_6)=0$, $N_{\\mathrm{I}}(\\mathcal{R}_{10})=4$. In all recoverable cases, rebuild is feasible by copying from surviving mirrors or solving parity equations without data loss.**\n- The calculated values $N_{\\mathrm{I}}(\\mathcal{R}_1)=1$, $N_{\\mathrm{I}}(\\mathcal{R}_5)=21$, $N_{\\mathrm{I}}(\\mathcal{R}_6)=0$, and $N_{\\mathrm{I}}(\\mathcal{R}_{10})=4$ all match our derived results.\n- The statement regarding rebuild feasibility is also consistent with our analysis.\n- **Verdict: Correct.**\n\n**B. $N_{\\mathrm{I}}(\\mathcal{R}_1)=0$, $N_{\\mathrm{I}}(\\mathcal{R}_5)=15$, $N_{\\mathrm{I}}(\\mathcal{R}_6)=6$, $N_{\\mathrm{I}}(\\mathcal{R}_{10})=28$. Rebuild is not feasible in RAID $6$ even when parity is available.**\n- $N_{\\mathrm{I}}(\\mathcal{R}_1)=0$ is incorrect. Our result is $1$.\n- $N_{\\mathrm{I}}(\\mathcal{R}_5)=15$ is incorrect. Our result is $21$.\n- $N_{\\mathrm{I}}(\\mathcal{R}_6)=6$ is incorrect. Our result is $0$.\n- $N_{\\mathrm{I}}(\\mathcal{R}_{10})=28$ is incorrect. Our result is $4$. ($28$ is the total number of pairs).\n- The statement \"Rebuild is not feasible in RAID $6$\" is fundamentally false; this is the primary purpose of RAID $6$'s dual parity.\n- **Verdict: Incorrect.**\n\n**C. $N_{\\mathrm{I}}(\\mathcal{R}_1)=1$, $N_{\\mathrm{I}}(\\mathcal{R}_5)=0$, $N_{\\mathrm{I}}(\\mathcal{R}_6)=0$, $N_{\\mathrm{I}}(\\mathcal{R}_{10})=4$. RAID $5$ can recover two simultaneous failures using distributed parity.**\n- $N_{\\mathrm{I}}(\\mathcal{R}_5)=0$ is incorrect. Our result is $21$.\n- The accompanying statement \"RAID $5$ can recover two simultaneous failures\" is a direct contradiction of the principles of single-parity RAID.\n- **Verdict: Incorrect.**\n\n**D. $N_{\\mathrm{I}}(\\mathcal{R}_1)=1$, $N_{\\mathrm{I}}(\\mathcal{R}_5)=21$, $N_{\\mathrm{I}}(\\mathcal{R}_6)=0$, $N_{\\mathrm{I}}(\\mathcal{R}_{10})=12$. RAID $10$ suffers irrecoverable loss whenever the two failed disks lie in the same stripe set even if they are from different mirrored pairs.**\n- $N_{\\mathrm{I}}(\\mathcal{R}_{10})=12$ is incorrect. Our result is $4$.\n- The statement about RAID $10$ failure is false. A failure of two disks from different mirrored pairs is *recoverable*, as the mirror for each failed disk is still available. The statement describes the opposite.\n- **Verdict: Incorrect.**", "answer": "$$\\boxed{A}$$", "id": "3675057"}, {"introduction": "Choosing a RAID level involves balancing reliability, performance, and cost, with storage capacity being a primary component of cost. This practice explores the crucial trade-off between space efficiency and fault tolerance by comparing RAID 6 and RAID 10. You will derive the space efficiency for each and determine the point at which one becomes more economical, providing a quantitative basis for a common system design decision [@problem_id:3675039].", "problem": "An array is built from $n$ identical disks, each of capacity $s$ bytes, managed by a Redundant Array of Independent Disks (RAID) controller. Define the space efficiency of a RAID configuration as the ratio of usable capacity to raw capacity. Using only core definitions of the relevant RAID levels, derive the space efficiency as a function of $n$ for the following configurations:\n- RAID $6$, which uses two independent parity blocks per stripe and can tolerate any two disk failures.\n- RAID $10$ (striped mirrors), which mirrors data pairwise and then stripes across the mirrored pairs. Assume $n$ is even so that all disks participate in complete mirrored pairs.\n\nAssume negligible metadata overhead, uniform disk sizes, full-stripe writes, and that all disks are active members of the array. From your derived expressions, determine the minimum integer $n$ with $n \\geq 6$ for which RAID $6$ is strictly more space-efficient than RAID $10$. Express your final answer as a unitless integer.\n\nThen, briefly discuss at the cross-over point how the space-efficiency comparison relates to expected performance characteristics for large sequential operations versus small random writes, and how fault tolerance and rebuild behavior differ between the two configurations. Your discussion should make explicit reference to stripe parity computation, write amplification, and the ability to sustain disk failures, but your final answer must be only the computed integer.", "solution": "The problem is assessed to be valid as it is scientifically grounded, well-posed, objective, and contains sufficient, consistent information for a formal solution. It adheres to standard definitions within the field of computer science.\n\nThe analysis proceeds by first deriving expressions for the space efficiency of RAID $6$ and RAID $10$ configurations, and then using these expressions to find the specified minimum number of disks.\n\nLet $n$ be the number of identical disks in the array, and let $s$ be the capacity of each disk in bytes.\n\nThe raw capacity, $C_{raw}$, of the array is the total capacity of all disks combined.\n$$C_{raw} = n \\times s$$\nSpace efficiency, $\\eta$, is defined as the ratio of usable capacity, $C_{usable}$, to raw capacity, $C_{raw}$.\n$$\\eta = \\frac{C_{usable}}{C_{raw}}$$\n\n**RAID 6 Space Efficiency**\nA RAID $6$ array distributes data across $n$ disks. For each stripe of data, it computes and stores two independent parity blocks. This configuration can sustain the failure of any two disks. The storage of two parity blocks per stripe means that the capacity equivalent of two disks is dedicated to storing redundancy information, regardless of the total number of disks, $n$.\nThe number of disks available for data storage is effectively $n-2$.\nTherefore, the usable capacity for RAID $6$, $C_{usable,6}$, is:\n$$C_{usable,6} = (n-2)s$$\nThe space efficiency for RAID $6$, $\\eta_6(n)$, is the ratio of its usable capacity to the raw capacity.\n$$\\eta_6(n) = \\frac{C_{usable,6}}{C_{raw}} = \\frac{(n-2)s}{ns} = \\frac{n-2}{n}$$\nThis expression is valid for $n \\geq 4$, which is satisfied by the problem's constraint $n \\geq 6$.\n\n**RAID 10 Space Efficiency**\nA RAID $10$ array, also known as RAID $1+0$, is a \"stripe of mirrors\". Data is first mirrored onto pairs of disks, and then these mirrored pairs are striped. The problem specifies that $n$ is an even integer, ensuring that all disks can form complete mirrored pairs.\nA single mirrored pair consists of $2$ disks. The data is written to both disks, so the usable capacity of a pair of disks, each of capacity $s$, is only $s$.\nWith a total of $n$ disks, we can form $n/2$ such mirrored pairs.\nThese $n/2$ pairs are then striped. Striping does not add its own capacity overhead; it simply distributes data across the logical devices (the mirrored pairs). Thus, the total usable capacity of the RAID $10$ array, $C_{usable,10}$, is the sum of the usable capacities of the mirrored pairs.\n$$C_{usable,10} = \\left(\\frac{n}{2}\\right)s$$\nThe space efficiency for RAID $10$, $\\eta_{10}(n)$, is the ratio of its usable capacity to the raw capacity.\n$$\\eta_{10}(n) = \\frac{C_{usable,10}}{C_{raw}} = \\frac{\\left(\\frac{n}{2}\\right)s}{ns} = \\frac{1}{2}$$\nThe space efficiency of RAID $10$ is a constant $1/2$, or $50\\%$, for any even number of disks $n \\geq 2$.\n\n**Comparison and Crossover Point**\nThe problem requires finding the minimum integer $n$ with $n \\geq 6$ for which RAID $6$ is strictly more space-efficient than RAID $10$. This can be expressed as the inequality:\n$$\\eta_6(n) > \\eta_{10}(n)$$\nSubstituting the derived expressions:\n$$\\frac{n-2}{n} > \\frac{1}{2}$$\nSince the problem specifies $n \\geq 6$, $n$ is a positive integer. We can multiply both sides of the inequality by $2n$ without changing the direction of the inequality sign.\n$$2(n-2) > n$$\n$$2n - 4 > n$$\n$$n > 4$$\nThe problem asks for the minimum integer $n$ such that $n \\geq 6$ that satisfies this condition. The definition of the RAID $10$ configuration provided is valid only for an even number of disks. Therefore, we must find the minimum even integer $n \\geq 6$ that satisfies $n > 4$. The set of even integers greater than or equal to $6$ is $\\{6, 8, 10, \\ldots\\}$. All members of this set satisfy the condition $n>4$. The minimum value in this set is $6$.\nLet's verify for $n=6$:\n$\\eta_6(6) = \\frac{6-2}{6} = \\frac{4}{6} = \\frac{2}{3}$.\n$\\eta_{10}(6) = \\frac{1}{2}$.\nThe inequality $\\frac{2}{3} > \\frac{1}{2}$ is true, since $4 > 3$. Thus, for $n=6$, RAID $6$ is strictly more space-efficient. Since $6$ is the smallest integer that meets all the problem's criteria ($n \\geq 6$, $n$ is even for the RAID $10$ definition, and the inequality holds), it is the required answer.\n\n**Discussion of Characteristics**\nThe derived crossover point occurs at $n=5$, with RAID $6$ becoming more space-efficient for any $n \\geq 5$. For the allowed even values of $n \\geq 6$, RAID $6$ is always more space-efficient. The discussion focuses on the trade-offs that accompany this greater efficiency.\n\n- **Performance**: For large sequential operations (reads or writes), both RAID $6$ and RAID $10$ can utilize all $n$ disks, leading to high throughput. RAID $10$ may have a slight advantage as it avoids the computational overhead of parity calculations. The most significant performance difference appears with small random writes. RAID $10$ has a very low write penalty, requiring only two write operations per logical write (one to each disk in the mirror). In contrast, RAID $6$ incurs a substantial write penalty due to its Read-Modify-Write cycle. Updating a single block requires reading the old data and both old parity blocks, then writing the new data and both new parity blocks, resulting in six I/O operations for one logical write. This write amplification makes RAID $10$ far superior for random-write-intensive workloads.\n\n- **Fault Tolerance and Rebuild**: RAID $6$ guarantees protection against any two disk failures, providing a higher level of data security. This is its primary advantage. Conversely, RAID $10$ can tolerate at least one disk failure, but its ability to withstand multiple failures depends on which disks fail; if both disks in a single mirror fail, all data is lost. In the best-case scenario, RAID $10$ can sustain up to $n/2$ failures, provided each failure occurs in a different mirrored pair. The rebuild process starkly contrasts the two. Rebuilding a failed drive in a RAID $10$ array is fast and low-impact, as data is simply copied directly from the surviving mirror. A RAID $6$ rebuild is a lengthy, resource-intensive process, requiring reading from all surviving $n-1$ disks to recalculate the missing data, which degrades array performance and increases the window of vulnerability to subsequent failures.", "answer": "$$\\boxed{6}$$", "id": "3675039"}, {"introduction": "A RAID array is most vulnerable during the period it is rebuilding a failed drive. This exercise provides a practical model to quantify this risk, moving from static failure counts to a dynamic analysis of reliability. By calculating the expected rebuild time and the probability of a second failure occurring during this critical window, you will learn to assess the transient risk inherent in a degraded RAID 5 array [@problem_id:3671447].", "problem": "A storage system implements Redundant Array of Independent Disks (RAID) level $5$ (RAID-$5$) across $n$ identical disks. When a single disk fails, the system reconstructs the lost data by reading and processing striped parity and data from all surviving disks. Assume the following model:\n\n- Each surviving disk can devote a fixed rebuild bandwidth of $B_{r}$, measured in megabytes per second, where $1$ megabyte ($\\mathrm{MB}$) is defined as $10^{6}$ bytes.\n- A background workload continuously consumes a fraction $\\phi$ of each surviving disk’s bandwidth, leaving only the remaining fraction available for the rebuild.\n- The amount of data that must be reconstructed onto the replacement disk is $C$ bytes.\n- Disk failures for each individual disk follow an independent Poisson process with a constant rate $\\lambda$ per year, where one year is defined as $365$ days of $24$ hours each.\n\nUsing only the principles that aggregate service rate is the sum of independent channel rates, that time equals workload divided by service rate, and that for an exponential failure process the probability of no events in an interval is the exponential of the negative rate times time, derive from first principles an expression for the expected rebuild time and the probability of at least one additional disk failure among the surviving disks during the rebuild interval.\n\nThen evaluate these for a RAID-$5$ array with $n=8$, $B_{r}=75\\,\\mathrm{MB/s}$, $\\phi=0.3$, $C=10\\times 10^{12}$ bytes, and $\\lambda=0.02$ per year. Express the rebuild time in hours and the failure probability as a unitless decimal fraction. Round both numerical results to four significant figures.", "solution": "The problem requires the derivation of two key metrics for a RAID-$5$ storage system under a specific failure and performance model: the expected rebuild time ($T_r$) and the probability of a subsequent disk failure during this rebuild period ($P_f$). Upon derivation, these expressions are to be evaluated for a given set of parameters.\n\nFirst, we address the validation of the problem statement.\nThe givens are:\n- RAID level: $5$\n- Number of disks: $n$\n- Per-disk rebuild bandwidth: $B_r$ in megabytes per second, where $1$ megabyte is $10^6$ bytes.\n- Background workload fraction: $\\phi$\n- Data to be reconstructed: $C$ bytes.\n- Per-disk failure rate: $\\lambda$ per year, following an independent Poisson process.\n- Time conversion: $1$ year is $365$ days of $24$ hours.\n- Required principles for derivation:\n    1. Aggregate service rate is the sum of independent channel rates.\n    2. Time equals workload divided by service rate.\n    3. For a Poisson process with rate $\\alpha$ over time $t$, the probability of zero events is $\\exp(-\\alpha t)$.\n\nThe problem is scientifically grounded in the fields of computer architecture (RAID systems) and probability theory (Poisson processes). It is well-posed, with all necessary variables and relationships provided to find a unique solution. The language is objective and unambiguous. The provided numerical values ($n=8$, $C=10$ TB, etc.) are realistic for contemporary storage systems. Therefore, the problem is deemed valid and a solution can be constructed.\n\n**1. Derivation of Expected Rebuild Time ($T_r$)**\n\nA RAID-$5$ array with $n$ disks stripes data and parity across all $n$ disks. When one disk fails, there are $n-1$ surviving disks. The lost data is reconstructed by reading the corresponding data and parity blocks from these $n-1$ disks.\n\nEach of the $n-1$ surviving disks contributes to the rebuild process. The total available rebuild bandwidth per disk is given as $B_r$. However, a fraction $\\phi$ of this bandwidth is consumed by a background workload. The remaining fraction available for the rebuild is $(1-\\phi)$.\nThe effective rebuild bandwidth from a single surviving disk, $B_{eff, disk}$, is:\n$$B_{eff, disk} = B_r (1-\\phi)$$\n\nAccording to the principle that the aggregate service rate is the sum of independent channel rates, the total aggregate rebuild bandwidth, $B_{agg}$, for the array is the sum of the effective bandwidths from all $n-1$ surviving disks:\n$$B_{agg} = (n-1) B_{eff, disk} = (n-1) B_r (1-\\phi)$$\n\nThe total workload is the amount of data that must be reconstructed, given as $C$. Using the principle that time equals workload divided by service rate, the expected rebuild time, $T_r$, is:\n$$T_r = \\frac{\\text{Workload}}{\\text{Service Rate}} = \\frac{C}{B_{agg}}$$\nSubstituting the expression for $B_{agg}$, we obtain the final analytical expression for the rebuild time:\n$$T_r = \\frac{C}{(n-1) B_r (1-\\phi)}$$\n\n**2. Derivation of the Probability of an Additional Failure ($P_f$)**\n\nDisk failures are modeled as independent Poisson processes, each with a rate of $\\lambda$ failures per disk per year. For the $n-1$ surviving disks, the total failure rate, $\\lambda_{total}$, is the sum of the individual failure rates, because the sum of independent Poisson processes is also a Poisson process with a rate equal to the sum of the individual rates.\n$$\\lambda_{total} = (n-1)\\lambda$$\n\nWe are interested in the probability of at least one additional disk failure during the rebuild interval, $T_r$. It is simpler to first calculate the probability of zero additional failures, $P_0$, during this interval. Based on the provided principle for a Poisson process, the probability of observing zero events over a time $t$ for a process with rate $\\alpha$ is $\\exp(-\\alpha t)$.\nApplying this to our scenario:\n$$P_0 = \\exp(-\\lambda_{total} T_r)$$\n\nThe event \"at least one failure\" is the complement of the event \"zero failures\". Therefore, the probability $P_f$ is:\n$$P_f = 1 - P_0 = 1 - \\exp(-\\lambda_{total} T_r)$$\n\nSubstituting the derived expressions for $\\lambda_{total}$ and $T_r$:\n$$P_f = 1 - \\exp\\left( -(n-1)\\lambda \\cdot \\frac{C}{(n-1) B_r (1-\\phi)} \\right)$$\nThe term $(n-1)$ in the exponent cancels out, leading to a simplified expression:\n$$P_f = 1 - \\exp\\left( -\\frac{\\lambda C}{B_r (1-\\phi)} \\right)$$\nThis result reveals that, within this model, the probability of a second failure during a rebuild is independent of the number of disks in the array (for $n>1$).\n\n**3. Numerical Evaluation**\n\nWe are given the following parameter values:\n- $n = 8$\n- $B_r = 75\\,\\mathrm{MB/s} = 75 \\times 10^6\\,\\mathrm{bytes/s}$\n- $\\phi = 0.3$\n- $C = 10 \\times 10^{12}\\,\\mathrm{bytes}$\n- $\\lambda = 0.02\\,\\mathrm{year}^{-1}$\n\nWe also use the conversion factors:\n- $1\\,\\mathrm{hour} = 3600\\,\\mathrm{s}$\n- $1\\,\\mathrm{year} = 365 \\times 24 \\times 3600\\,\\mathrm{s} = 31,536,000\\,\\mathrm{s}$\n\n**Calculation of Rebuild Time in Hours:**\nFirst, we calculate $T_r$ in seconds using the derived formula:\n$$T_r = \\frac{C}{(n-1) B_r (1-\\phi)} = \\frac{10 \\times 10^{12}}{(8-1) \\times (75 \\times 10^6) \\times (1-0.3)}$$\n$$T_r = \\frac{10 \\times 10^{12}}{7 \\times 75 \\times 10^6 \\times 0.7} = \\frac{10 \\times 10^{12}}{367.5 \\times 10^6}\\,\\mathrm{s} \\approx 27210.884\\,\\mathrm{s}$$\nTo express this time in hours, we divide by $3600\\,\\mathrm{s/hour}$:\n$$T_{r, \\text{hours}} = \\frac{27210.884}{3600} \\approx 7.558579\\,\\mathrm{hours}$$\nRounding to four significant figures, the rebuild time is $7.559$ hours.\n\n**Calculation of Failure Probability:**\nNext, we calculate $P_f$ using its derived formula. To ensure dimensional consistency in the exponent, we must express $\\lambda$ and the time component $\\frac{C}{B_r(1-\\phi)}$ in compatible units. Let's convert the failure rate $\\lambda$ to units of $\\mathrm{s}^{-1}$:\n$$\\lambda_s = \\frac{0.02}{31,536,000}\\,\\mathrm{s}^{-1}$$\nThe term in the exponent is:\n$$\\frac{\\lambda C}{B_r(1-\\phi)} = \\lambda_s \\times \\frac{C}{B_r(1-\\phi)} = \\frac{0.02}{31,536,000} \\times \\frac{10 \\times 10^{12}}{75 \\times 10^6 \\times 0.7}$$\n$$\\frac{\\lambda C}{B_r(1-\\phi)} = \\left( \\frac{0.02}{31,536,000} \\right) \\times \\left( \\frac{10^{13}}{5.25 \\times 10^7} \\right) \\approx (6.34245 \\times 10^{-10}) \\times (1.90476 \\times 10^5) \\approx 0.00012078$$\nThis exponent is also equal to $\\lambda_{total} \\times T_r$:\n$$\\lambda_{total} = (n-1)\\lambda_s = 7 \\times \\frac{0.02}{31,536,000}\\,\\mathrm{s}^{-1} \\approx 4.4397 \\times 10^{-9}\\,\\mathrm{s}^{-1}$$\n$$\\lambda_{total} T_r \\approx (4.4397 \\times 10^{-9}) \\times (27210.884) \\approx 0.00012078$$\nThe probability of failure is then:\n$$P_f = 1 - \\exp(-0.00012078) \\approx 1 - 0.99987923 \\approx 0.00012077$$\nFor small $x$, $1 - \\exp(-x) \\approx x$. Thus, $P_f \\approx 0.00012078$.\nRounding to four significant figures, the probability is $0.0001208$, which can be written as $1.208 \\times 10^{-4}$.\n\nThe final numerical answers are a rebuild time of $7.559$ hours and a failure probability of $1.208 \\times 10^{-4}$.", "answer": "$$\\boxed{\\begin{pmatrix} 7.559 & 1.208 \\times 10^{-4} \\end{pmatrix}}$$", "id": "3671447"}]}