## Applications and Interdisciplinary Connections

The architectural distinction between Uniform Memory Access (UMA) and Non-Uniform Memory Access (NUMA) extends far beyond the hardware layer, exerting a profound influence on software performance, algorithm design, and [system scalability](@entry_id:755782). While the foundational principles of UMA and NUMA define [memory latency](@entry_id:751862) characteristics, their practical consequences emerge in the complex interplay between software workloads and the underlying physical topology of the machine. This chapter explores these consequences across a diverse range of disciplines, demonstrating how an understanding of NUMA is critical for developing efficient and scalable software in modern multi-socket systems. We will examine applications in core software systems, [parallel programming](@entry_id:753136), scientific computing, and machine learning, illustrating how NUMA-awareness transitions from a mere optimization to a fundamental design principle.

### Core Software Systems and Runtimes

The performance of foundational software, including [operating systems](@entry_id:752938), database engines, and language runtimes, is deeply intertwined with memory access patterns. On NUMA architectures, ignoring [memory locality](@entry_id:751865) can lead to significant performance bottlenecks that are often difficult to diagnose.

**Operating Systems and Schedulers**

Modern [operating systems](@entry_id:752938) employ sophisticated schedulers to manage [load balancing](@entry_id:264055) across multiple cores and sockets. A common and effective strategy is [work-stealing](@entry_id:635381), where an idle thread on one node can "steal" a task from the queue of a busy node. While this approach effectively utilizes compute resources, it introduces a critical trade-off on NUMA systems. A cross-node steal may move a task away from its associated data, leading to a cascade of remote memory accesses and cache misses. The cost of this lost locality can, in some cases, outweigh the benefit of avoiding idle time.

This trade-off can be modeled to create NUMA-aware scheduling policies. By quantifying the incremental time cost of executing a stolen task—which includes penalties from higher cache miss rates and remote DRAM latency—and comparing it to the cost of keeping a thread idle, an [optimal policy](@entry_id:138495) can be derived. For instance, a scheduler might decide to perform a cross-node steal only if the queue length difference between the busy and idle nodes exceeds a certain threshold, $\theta$. This threshold represents the break-even point where the expected cost of waiting becomes greater than the cost of the performance hit from remote access. Formal analysis shows that this optimal threshold is directly proportional to the incremental memory cost of a stolen task and inversely proportional to the cost rate of idle time, providing a principled way to balance load distribution against [memory locality](@entry_id:751865) [@problem_id:3687021].

**Database and Key-Value Systems**

Transactional databases and key-value stores are quintessential systems workloads characterized by intense memory access. In a NUMA environment, the placement of the buffer pool—the in-memory cache of database pages—is critical. A common design involves sharding the buffer pool, with each NUMA node managing its own local pool. A transaction pinned to a specific node will experience low-latency local hits if its required data pages reside in the local pool. However, if the page is in a remote node's buffer pool, the access time increases significantly. The total performance is a probabilistic sum over local hits, remote hits, and misses to persistent storage. Even [concurrency control](@entry_id:747656) mechanisms, such as acquiring latches on pages, can introduce remote memory hits if the shared data structures for managing latches are not carefully partitioned across sockets [@problem_id:3687058].

Similarly, read-heavy workloads in key-value stores are affected by the interaction between read amplification—the phenomenon where a single logical read requires multiple physical memory accesses—and NUMA locality. The total service time for a logical read is the sum of latencies for all underlying DRAM misses. A NUMA-oblivious policy, such as [interleaving](@entry_id:268749) memory pages across all sockets, results in a high probability of remote access for any given miss. In contrast, a NUMA-aware policy that co-locates threads with the data shards they are responsible for can drastically reduce the fraction of remote accesses. The performance benefit of such a policy is magnified by read amplification; as the number of memory accesses per logical operation ($A$) increases, the penalty for each remote access is incurred more times, making [data locality](@entry_id:638066) increasingly valuable [@problem_id:3687065].

**Language Runtimes and Garbage Collection**

Managed language runtimes for languages like Java or Go rely on [automatic memory management](@entry_id:746589), typically a garbage collector (GC), which itself must be NUMA-aware to achieve scalability. A particularly challenging case is a copying garbage collector, which moves live objects to compact the heap. A naive implementation might move an object from one NUMA node to another, which is undesirable as it can break the memory affinity a mutator thread has with that object.

A sophisticated NUMA-aware copying GC can be designed under the constraint that objects are never moved across nodes. In such a design, each node manages its own heap space (e.g., semi-spaces for a Cheney-style collector). During a collection cycle on one node, live objects are evacuated only to other locations within that same node's heap. This satisfies the "no cross-node move" constraint and eliminates logical fragmentation on the local heap. A key challenge is updating pointers to moved objects, especially pointers that reside on other nodes. Eagerly updating all remote pointers could lead to a storm of remote writes, scaling with the number of inter-node references. A more scalable solution is to use an indirection-based approach, such as a Brooks-style forwarding pointer in each object's header. When an object is moved, only its local indirection pointer is updated. All accesses to the object (local or remote) go through this indirection, ensuring they reach the object's new location without requiring widespread remote pointer updates during the GC cycle itself. This localizes the vast majority of GC-related writes, a critical feature for NUMA [scalability](@entry_id:636611) [@problem_id:3687006].

### Concurrent and Parallel Programming

The performance of [parallel algorithms](@entry_id:271337) depends heavily on minimizing communication and [synchronization](@entry_id:263918) overhead. On NUMA systems, these overheads are directly related to the frequency of remote memory accesses.

**Synchronization Primitives**

The choice of [synchronization](@entry_id:263918) primitive can have a dramatic impact on performance. A simple ticket [spinlock](@entry_id:755228), where all waiting threads spin by reading a single [shared memory](@entry_id:754741) location, is notoriously unscalable on NUMA systems. When the lock is released, the write to the shared location invalidates cached copies on all other sockets, causing a flood of high-latency remote read misses as all spinning threads attempt to re-read the value. This creates a "thundering herd" problem that saturates the interconnect.

In contrast, NUMA-aware spinlocks, such as the Mellor-Crummey and Scott (MCS) lock, are designed to avoid this scalability bottleneck. In an MCS lock, each thread spins on a flag in its own private, thread-local node. When a thread releases the lock, it writes only to the flag of its direct successor in the queue. This transforms the broadcast-like invalidation storm of a [ticket lock](@entry_id:755967) into a single, targeted remote write between two nodes. This localization of communication traffic is the key to its superior performance and [scalability](@entry_id:636611) on NUMA architectures, as it reduces inter-socket coherence traffic from scaling with the number of sockets, $O(S)$, to a constant, $O(1)$, per handoff [@problem_id:3687017].

**Lock-Free Data Structures**

Lock-free data structures, which use atomic hardware primitives like Compare-And-Swap (CAS) instead of locks, are often seen as a path to higher scalability. However, they are not immune to NUMA effects. A CAS operation on a [shared memory](@entry_id:754741) location, such as the tail pointer of a [lock-free queue](@entry_id:636621), requires exclusive ownership of the corresponding cache line, effectively serializing all concurrent attempts. On a NUMA system, the latency of this atomic operation depends on whether the cache line is local or remote. If threads from multiple sockets are contending for the same tail pointer, the cache line will frequently move between sockets, and many CAS attempts will incur the higher remote access latency. The resulting throughput of the data structure is thus limited by a weighted average of local and remote CAS latencies, which can be significantly worse than the performance on an idealized UMA machine [@problem_id:3687057].

**Parallel Communication Patterns**

Common [parallel programming](@entry_id:753136) patterns must also be adapted for NUMA. Consider a simple two-stage producer-consumer pipeline where the producer and consumer are pinned to different sockets. They communicate through a shared buffer. The placement of this buffer has first-order performance implications. If the buffer is placed on the producer's node, the producer's writes are fast (local), but the consumer's reads are slow (remote). If placed on the consumer's node, the opposite is true. An interleaved placement results in both stages performing a mix of local and remote accesses. The optimal placement strategy is the one that best balances the total time (computation plus memory access) for each stage, thereby minimizing the pipeline's overall bottleneck. This often means placing the buffer local to the stage with the higher intrinsic computation time, allowing its memory operations to be as fast as possible to avoid exacerbating the bottleneck [@problem_id:3687027].

### High-Performance and Scientific Computing

In scientific and [high-performance computing](@entry_id:169980) (HPC), workloads are often characterized by large data structures and predictable, regular access patterns. NUMA-aware [data placement](@entry_id:748212) and algorithm design are paramount for achieving high efficiency.

**Data Replication for Read-Only Workloads**

For frequently accessed, read-only [data structures](@entry_id:262134) like lookup tables, a powerful strategy on NUMA systems is data replication. If a single copy of a table resides on one node, all other nodes must perform expensive remote accesses to read from it. By replicating the table in the local memory of every node, all accesses become local, maximizing performance. This introduces a clear trade-off: the significant performance gain from eliminating remote latencies comes at the cost of increased memory consumption. A simple performance model can quantify the net throughput gain versus the memory overhead, allowing developers to make an informed decision based on the specific latencies and memory capacities of their system [@problem_id:3686976].

**Structured Grid Computations**

Simulations on [structured grids](@entry_id:272431), such as those for [solving partial differential equations](@entry_id:136409) like the heat equation, are a cornerstone of [scientific computing](@entry_id:143987). These simulations typically use stencil-based methods, where the value of a grid point at the next time step is computed from its neighbors. When the grid is partitioned across NUMA nodes (e.g., by assigning contiguous blocks of rows to each node), points along the partition boundaries will require data from neighboring nodes.

A naive implementation where boundary points directly read from remote memory during the computation phase is highly inefficient, as it involves many fine-grained, high-latency accesses. The standard and most effective NUMA-aware approach is to use **[ghost cells](@entry_id:634508)** (or halo regions). Before the main computation, a communication phase occurs where each node exchanges its boundary data with its neighbors. This data is stored in local [ghost cells](@entry_id:634508). The subsequent computation phase can then proceed with all stencil reads being satisfied from local memory (either the original local data or the now-local [ghost cells](@entry_id:634508)). This strategy consolidates all remote communication into efficient, bulk transfers, dramatically improving performance while preserving the numerical correctness of the scheme [@problem_id:3686997].

**Spectral Methods and the Fast Fourier Transform (FFT)**

Algorithms like the Fast Fourier Transform (FFT) have complex, non-local communication patterns that interact with NUMA architectures in interesting ways. For a one-dimensional FFT on data partitioned into contiguous blocks across nodes, the communication pattern evolves through the stages of the algorithm. In the initial stages, the communication stride is small, and butterflies combine elements that are close together. With a contiguous data layout, these elements reside on the same node, resulting in zero inter-node communication. However, in the later stages, the stride becomes large, and butterflies combine elements that are far apart. At this point, nearly all communications become remote, crossing socket boundaries. A formal analysis of this behavior on a specific interconnect topology, such as a bidirectional ring, allows for the precise calculation of the total communication cost in "element-hops," revealing that for a block-distributed FFT, all communication is back-loaded into the final stages of the algorithm [@problem_id:3687010].

### Data Science and Machine Learning

Modern data science and machine learning workloads, from large-scale data processing to training complex neural networks, present unique challenges and opportunities for NUMA optimization.

**Large-Scale Sorting**

Parallel [sorting algorithms](@entry_id:261019) like [radix sort](@entry_id:636542) involve distributing keys into buckets. When implemented on a NUMA system, the placement of these buckets is critical. If buckets are distributed across nodes, a key processed on one node may need to be written to a bucket owned by another, resulting in a remote write. For a workload with many keys and relatively few buckets, every node will likely need to write to every remote bucket in each pass, leading to a large number of remote communications. A key algorithmic choice is the [radix](@entry_id:754020) size, $r$, which determines the number of buckets ($2^r$) and the number of passes required. A larger [radix](@entry_id:754020) reduces the number of passes but exponentially increases the number of buckets, and thus the potential remote communication partners. For certain data distributions, a smaller [radix](@entry_id:754020) can be optimal, as it minimizes the communication per pass, and this benefit outweighs the cost of performing more passes [@problem_id:3686972].

**Graph Analytics**

Graph algorithms are notorious for their irregular memory access patterns. During a Breadth-First Search (BFS), for example, traversing an edge may require accessing vertex data that resides in remote memory. The number of such remote accesses is highly dependent on how the graph is partitioned across NUMA nodes. A naive partitioning that assigns vertices to nodes randomly or based on vertex ID results in a high probability of remote access for any given edge, proportional to $(P-1)/P$ on a $P$-node system.

A much more effective strategy is to use graph [community detection](@entry_id:143791) algorithms to inform [data placement](@entry_id:748212). By partitioning the graph such that dense communities are co-located on the same NUMA node, the majority of edge traversals become local memory accesses. The graph's **modularity**, $Q$, a measure of the strength of its community structure, provides a direct link between the graph's topology and the potential performance gain. A higher modularity implies that a larger fraction of edges are intra-community, and a NUMA-aware placement can translate this directly into a lower number of remote memory accesses. The expected reduction in remote traversals by using a community-aware placement over a random one can be shown to be directly proportional to the product of the modularity and the total number of edges examined, elegantly connecting a concept from network science to system performance [@problem_id:3687036]. Even for simpler [data structures](@entry_id:262134) like trees, the probability that a node resides in remote memory can be modeled as a function of its depth, allowing for a precise calculation of the expected total remote access cost for a full traversal [@problem_id:3686991].

**Graph Neural Networks (GNNs)**

The training of Graph Neural Networks (GNNs) is a modern workload where NUMA effects are prominent. A key step in GNN training is [message passing](@entry_id:276725), where node [embeddings](@entry_id:158103) (learnable parameters) are updated based on information aggregated from their neighbors. If the graph's node [embeddings](@entry_id:158103) are sharded across NUMA sockets, an edge connecting a source node on one socket to a destination node on another will require a remote gradient update. Similar to graph analytics, a random or naive placement of embeddings leads to a high fraction of remote updates. By applying [graph partitioning](@entry_id:152532) to co-locate densely connected subgraphs on the same socket, the number of cross-socket edges is minimized, which in turn dramatically reduces the number of remote gradient updates. This optimization can lead to significant speedups in training time, demonstrating the critical need for NUMA-aware data sharding in modern machine learning systems [@problem_id:3687066].

### Algorithmic Data Placement

The diverse applications discussed highlight a common theme: intelligent [data placement](@entry_id:748212) is the key to unlocking performance on NUMA systems. While some workloads have regular structures that suggest obvious placement strategies (e.g., domain decomposition for grids), many are irregular. For these, one can formulate [data placement](@entry_id:748212) as an explicit optimization problem. Given an [access matrix](@entry_id:746217) that describes the frequency of access from each NUMA node to each data item, and a [distance matrix](@entry_id:165295) describing the latency between nodes, the goal is to find an assignment of data items to nodes that minimizes the total expected memory access latency.

A greedy heuristic can provide a powerful and practical solution. Such an algorithm first calculates, for each data item, the placement cost on every possible node. It then prioritizes items by their "spread"—the difference between their worst-case and best-case placement cost—and assigns the highest-spread items first to the node that offers them the lowest latency, subject to capacity constraints. This approach ensures that items whose placement is most critical to performance are given their optimal location. When compared to a naive range-based partitioning, this guided approach can yield substantial speedups, transforming an application from being NUMA-pessimal to NUMA-optimal. Of course, on a true UMA system where all memory access latencies are identical, such placement algorithms confer no benefit, underscoring that these techniques are fundamentally about adapting software to the non-uniformity of the hardware [@problem_id:3686995].