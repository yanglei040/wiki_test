{"hands_on_practices": [{"introduction": "This exercise explores the fundamental performance trade-offs of Simultaneous Multithreading (SMT). We will use the classic CPU performance equation to quantify how SMT can both help and hinder performance. By modeling a reduction in stall cycles due to latency hiding and an increase in instruction count due to resource contention, you can calculate the net effect on execution time and gain a tangible understanding of SMT's dual nature [@problem_id:3631114].", "problem": "A single-core Central Processing Unit (CPU) executes Program A at a fixed clock frequency of $f = 3.2~\\text{GHz}$. When the program runs in single-thread mode, the measured per-thread baseline cycles per instruction (CPI) decomposes into a non-stall component of $CPI_{\\text{base}} = 0.90$ and a stall component of $CPI_{\\text{stall},0} = 0.60$. The baseline per-thread instruction count is $IC_{0} = 1.80 \\times 10^{9}$ instructions.\n\nNow consider enabling Simultaneous Multithreading (SMT), also known as hyper-threading, with two hardware threads sharing the core. Due to overlapping of long-latency events, the per-thread stall component of CPI is reduced by $30\\%$ relative to its single-thread value. However, shared-resource contention increases the per-thread instruction count by $1\\%$ relative to its single-thread value. Assume the non-stall component $CPI_{\\text{base}}$ remains unchanged per thread under SMT, and the clock frequency does not change.\n\nCompute the per-thread execution time $T$ for Program A under SMT. Express the final answer in seconds and round to four significant figures.", "solution": "The problem is subjected to validation before a solution is attempted.\n\n### Step 1: Extract Givens\n-   Clock frequency: $f = 3.2~\\text{GHz}$\n-   Single-thread non-stall Cycles Per Instruction (CPI): $CPI_{\\text{base}} = 0.90$\n-   Single-thread stall CPI: $CPI_{\\text{stall},0} = 0.60$\n-   Single-thread baseline instruction count: $IC_{0} = 1.80 \\times 10^{9}$ instructions\n-   Number of Simultaneous Multithreading (SMT) hardware threads: $2$\n-   Reduction in per-thread stall CPI under SMT: $30\\%$\n-   Increase in per-thread instruction count under SMT: $1\\%$\n-   Constant per-thread non-stall CPI: $CPI_{\\text{base}}$ is unchanged.\n-   Constant clock frequency: $f$ is unchanged.\n-   Objective: Compute the per-thread execution time $T$ for Program A under SMT.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed for validity.\n-   **Scientifically Grounded**: The problem utilizes the fundamental CPU performance equation and standard concepts from computer organization and architecture, including CPI, instruction count, clock frequency, and SMT. The described effects of SMT—reducing stall cycles through latency hiding and increasing instruction count due to resource contention—are well-documented and realistic phenomena. The provided numerical values are plausible for modern processors.\n-   **Well-Posed**: The problem is fully specified with all necessary data and constraints to calculate a unique solution. The objective is clearly stated.\n-   **Objective**: The problem is formulated with precise, quantitative, and unbiased language.\n\nThe problem does not violate any of the invalidity criteria. It is scientifically sound, well-posed, and objective.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A solution will be derived.\n\nThe execution time of a program on a CPU is determined by the CPU performance equation:\n$$T = \\frac{IC \\times CPI}{f}$$\nwhere $IC$ is the instruction count, $CPI$ is the average cycles per instruction, and $f$ is the clock frequency. The $CPI$ can be decomposed into a non-stall component, $CPI_{\\text{base}}$, and a stall component, $CPI_{\\text{stall}}$:\n$$CPI = CPI_{\\text{base}} + CPI_{\\text{stall}}$$\n\nFirst, we establish the parameters for the single-thread execution mode. The total CPI in this mode, $CPI_{0}$, is:\n$$CPI_{0} = CPI_{\\text{base}} + CPI_{\\text{stall},0} = 0.90 + 0.60 = 1.50$$\nThe clock frequency is given as $f = 3.2~\\text{GHz} = 3.2 \\times 10^{9}~\\text{Hz}$.\n\nNext, we determine the parameters for a single thread running under SMT. Let these be denoted with the subscript $SMT$.\n\nThe per-thread instruction count under SMT, $IC_{SMT}$, is increased by $1\\%$ relative to the baseline instruction count $IC_{0}$:\n$$IC_{SMT} = IC_{0} \\times (1 + 0.01) = 1.01 \\times IC_{0}$$\nSubstituting the value for $IC_{0}$:\n$$IC_{SMT} = 1.01 \\times (1.80 \\times 10^{9}) = 1.818 \\times 10^{9}~\\text{instructions}$$\n\nThe per-thread stall component of CPI under SMT, $CPI_{\\text{stall},SMT}$, is reduced by $30\\%$ relative to its single-thread value, $CPI_{\\text{stall},0}$:\n$$CPI_{\\text{stall},SMT} = CPI_{\\text{stall},0} \\times (1 - 0.30) = 0.70 \\times CPI_{\\text{stall},0}$$\nSubstituting the value for $CPI_{\\text{stall},0}$:\n$$CPI_{\\text{stall},SMT} = 0.70 \\times 0.60 = 0.42$$\n\nThe non-stall component of CPI, $CPI_{\\text{base}}$, remains unchanged. Therefore, the total per-thread CPI under SMT, $CPI_{SMT}$, is:\n$$CPI_{SMT} = CPI_{\\text{base}} + CPI_{\\text{stall},SMT} = 0.90 + 0.42 = 1.32$$\n\nNow, we can compute the per-thread execution time under SMT, $T_{SMT}$, using the CPU performance equation. The clock frequency $f$ remains unchanged.\n$$T_{SMT} = \\frac{IC_{SMT} \\times CPI_{SMT}}{f}$$\nSubstituting the calculated values:\n$$T_{SMT} = \\frac{(1.818 \\times 10^{9}) \\times 1.32}{3.2 \\times 10^{9}~\\text{Hz}}$$\nThe factor of $10^{9}$ in the numerator and denominator cancels out:\n$$T_{SMT} = \\frac{1.818 \\times 1.32}{3.2}~\\text{s}$$\n$$T_{SMT} = \\frac{2.39976}{3.2}~\\text{s}$$\n$$T_{SMT} = 0.749925~\\text{s}$$\n\nThe problem requires the final answer to be rounded to four significant figures.\nThe fifth significant figure is $2$, which is less than $5$, so we round down.\n$$T_{SMT} \\approx 0.7499~\\text{s}$$", "answer": "$$\n\\boxed{0.7499}\n$$", "id": "3631114"}, {"introduction": "This practice delves into the core mechanism behind SMT's throughput gains: improved utilization of a processor's parallel execution hardware. You will model a superscalar core with multiple execution ports and analyze how co-scheduling two threads with different instruction profiles increases the overall occupancy of these ports. This probabilistic approach demonstrates how SMT fills execution resources that would otherwise lie dormant in single-threaded operation [@problem_id:3677167].", "problem": "A superscalar core that supports Simultaneous Multithreading (SMT) has $3$ disjoint, single-issue execution ports: integer arithmetic port $p_{0}$, integer arithmetic port $p_{1}$, and a memory port $p_{M}$. Each port can issue at most $1$ micro-operation per cycle, and each issued micro-operation requires exactly one of these ports. The hardware issues all ready micro-operations subject to the per-port capacity constraint; if multiple ready micro-operations from different threads simultaneously require the same port in a cycle, exactly one is chosen by fair arbitration, but at most one can be issued on that port in that cycle. Consider the following two threads:\n- Thread A: In any cycle, it has a ready micro-operation with probability $0.92$; conditioned on being ready, the micro-operation requires $p_{0}$ with probability $0.50$, requires $p_{1}$ with probability $0.30$, and requires $p_{M}$ with probability $0.20$.\n- Thread B: In any cycle, it has a ready micro-operation with probability $0.85$; conditioned on being ready, the micro-operation requires $p_{0}$ with probability $0.20$, requires $p_{1}$ with probability $0.60$, and requires $p_{M}$ with probability $0.20$.\n\nAssume independence across threads and across cycles in readiness and operation-type selection. Define the expected per-cycle occupancy of a port as the long-run probability that the port issues a micro-operation in an arbitrary cycle. Compute the expected occupancy of each port for single-threaded execution $T=1$ (only Thread A present) and two-thread SMT $T=2$ (Threads A and B present). Express your final answer as a single row vector\n$$\n\\big[\\,O_{p_{0}}^{(T=1)},\\,O_{p_{1}}^{(T=1)},\\,O_{p_{M}}^{(T=1)},\\,O_{p_{0}}^{(T=2)},\\,O_{p_{1}}^{(T=2)},\\,O_{p_{M}}^{(T=2)}\\,\\big]\n$$\nrounded to four significant figures. No units are required.", "solution": "The problem asks for the expected per-cycle occupancy of three execution ports ($p_0$, $p_1$, $p_M$) under two scenarios: single-threaded execution ($T=1$) with Thread A, and two-thread Simultaneous Multithreading (SMT) execution ($T=2$) with Threads A and B. The expected occupancy is defined as the probability that a port issues a micro-operation in a given cycle.\n\nFirst, let us formalize the given information using probabilistic notation.\nLet $R_A$ be the event that Thread A has a ready micro-operation in a cycle, and $R_B$ be the event that Thread B has a ready micro-operation. The probabilities are given as:\n$$P(R_A) = 0.92$$\n$$P(R_B) = 0.85$$\n\nLet $C_{A,i}$ denote the event that a ready micro-operation from Thread A requires port $p_i$, where $i \\in \\{0, 1, M\\}$. The conditional probabilities are:\n$$P(C_{A,0} | R_A) = 0.50$$\n$$P(C_{A,1} | R_A) = 0.30$$\n$$P(C_{A,M} | R_A) = 0.20$$\nNote that these probabilities sum to $1$, as expected: $0.50 + 0.30 + 0.20 = 1.0$.\n\nSimilarly, let $C_{B,i}$ denote the event that a ready micro-operation from Thread B requires port $p_i$. The conditional probabilities are:\n$$P(C_{B,0} | R_B) = 0.20$$\n$$P(C_{B,1} | R_B) = 0.60$$\n$$P(C_{B,M} | R_B) = 0.20$$\nThese also sum to $1$: $0.20 + 0.60 + 0.20 = 1.0$.\n\nLet $U_{A,i}$ be the event that Thread A requires port $p_i$ in a cycle. This occurs if and only if Thread A has a ready micro-operation AND that operation requires port $p_i$. Thus, $U_{A,i} = R_A \\cap C_{A,i}$. The probability of this joint event is:\n$$P(U_{A,i}) = P(R_A \\cap C_{A,i}) = P(C_{A,i} | R_A) P(R_A)$$\nSimilarly, let $U_{B,i}$ be the event that Thread B requires port $p_i$. The probability is:\n$$P(U_{B,i}) = P(R_B \\cap C_{B,i}) = P(C_{B,i} | R_B) P(R_B)$$\n\nThe problem states that all events are independent across threads and cycles.\n\nLet's calculate the probabilities $P(U_{A,i})$ and $P(U_{B,i})$ for each port $p_i$:\nFor Thread A:\n$$P(U_{A,0}) = 0.50 \\times 0.92 = 0.46$$\n$$P(U_{A,1}) = 0.30 \\times 0.92 = 0.276$$\n$$P(U_{A,M}) = 0.20 \\times 0.92 = 0.184$$\n\nFor Thread B:\n$$P(U_{B,0}) = 0.20 \\times 0.85 = 0.17$$\n$$P(U_{B,1}) = 0.60 \\times 0.85 = 0.51$$\n$$P(U_{B,M}) = 0.20 \\times 0.85 = 0.17$$\n\nNow we can compute the port occupancies for the two scenarios. Let $O_{p_i}^{(T)}$ be the occupancy of port $p_i$ with $T$ threads.\n\nCase 1: Single-Threaded Execution ($T=1$, Thread A only)\nIn this scenario, a port $p_i$ is occupied if and only if Thread A requires it. Therefore, the occupancy is simply the probability $P(U_{A,i})$.\n$$O_{p_0}^{(T=1)} = P(U_{A,0}) = 0.46$$\n$$O_{p_1}^{(T=1)} = P(U_{A,1}) = 0.276$$\n$$O_{p_M}^{(T=1)} = P(U_{A,M}) = 0.184$$\n\nCase 2: Two-Thread SMT Execution ($T=2$, Threads A and B)\nIn the SMT scenario, a port $p_i$ is occupied if at least one of the threads requires it. That is, if either Thread A requires it, OR Thread B requires it. The event that port $p_i$ is occupied is the union $U_{A,i} \\cup U_{B,i}$. The occupancy is the probability of this union.\n$$O_{p_i}^{(T=2)} = P(U_{A,i} \\cup U_{B,i})$$\nUsing the principle of inclusion-exclusion, and the independence of the two threads (which implies $P(U_{A,i} \\cap U_{B,i}) = P(U_{A,i}) P(U_{B,i})$), we have:\n$$O_{p_i}^{(T=2)} = P(U_{A,i}) + P(U_{B,i}) - P(U_{A,i}) P(U_{B,i})$$\nAlternatively, the port is occupied if it is not idle. A port is idle only if both threads do not require it.\n$$O_{p_i}^{(T=2)} = 1 - P(\\text{port } p_i \\text{ is idle}) = 1 - P(\\neg U_{A,i} \\cap \\neg U_{B,i})$$\nBy independence, this is:\n$$O_{p_i}^{(T=2)} = 1 - P(\\neg U_{A,i}) P(\\neg U_{B,i}) = 1 - (1 - P(U_{A,i}))(1 - P(U_{B,i}))$$\nThis formulation is equivalent and computationally stable.\n\nFor port $p_0$:\n$$O_{p_0}^{(T=2)} = 1 - (1 - 0.46)(1 - 0.17) = 1 - (0.54)(0.83) = 1 - 0.4482 = 0.5518$$\n\nFor port $p_1$:\n$$O_{p_1}^{(T=2)} = 1 - (1 - 0.276)(1 - 0.51) = 1 - (0.724)(0.49) = 1 - 0.35476 = 0.64524$$\n\nFor port $p_M$:\n$$O_{p_M}^{(T=2)} = 1 - (1 - 0.184)(1 - 0.17) = 1 - (0.816)(0.83) = 1 - 0.67728 = 0.32272$$\n\nThe problem requires the final answers rounded to four significant figures.\n$O_{p_0}^{(T=1)} = 0.46 \\rightarrow 0.4600$\n$O_{p_1}^{(T=1)} = 0.276 \\rightarrow 0.2760$\n$O_{p_M}^{(T=1)} = 0.184 \\rightarrow 0.1840$\n$O_{p_0}^{(T=2)} = 0.5518 \\rightarrow 0.5518$\n$O_{p_1}^{(T=2)} = 0.64524 \\rightarrow 0.6452$\n$O_{p_M}^{(T=2)} = 0.32272 \\rightarrow 0.3227$\n\nThe final result is the row vector of these six values in the specified order.\n$$\n\\big[\\,O_{p_{0}}^{(T=1)},\\,O_{p_{1}}^{(T=1)},\\,O_{p_{M}}^{(T=1)},\\,O_{p_{0}}^{(T=2)},\\,O_{p_{1}}^{(T=2)},\\,O_{p_{M}}^{(T=2)}\\,\\big]\n$$\n$$\n\\big[\\,0.4600,\\,0.2760,\\,0.1840,\\,0.5518,\\,0.6452,\\,0.3227\\,\\big]\n$$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.4600 & 0.2760 & 0.1840 & 0.5518 & 0.6452 & 0.3227\n\\end{pmatrix}\n}\n$$", "id": "3677167"}, {"introduction": "Correctly applying architectural concepts requires understanding their scope, and this exercise sharpens that skill by tackling a common point of confusion regarding SMT. You are asked to analyze the interaction between SMT threads and a phenomenon known as \"false sharing,\" which typically affects multi-core systems. This thought experiment challenges you to distinguish between intra-core resource contention and inter-core cache coherence, clarifying why principles that apply to multiple cores may not apply to multiple threads on a single core [@problem_id:3641063].", "problem": "Consider a single physical processor core that supports Simultaneous Multithreading (SMT) with $2$ logical threads, a Level $1$ Data Cache (L1D) shared between the logical threads, and a Modified Exclusive Shared Invalid (MESI) cache coherence protocol. Assume the following well-tested facts and definitions as the fundamental base:\n\n- A cache line is the unit of coherence, and the MESI protocol tracks state per cache line, not per thread. Coherence invalidations are only needed when multiple physical cores access the same cache line in conflicting ways; logical threads on the same physical core share the same private L1D contents.\n- The core uses a write-back, write-allocate policy. A store to data already present and writable in the L1D hits in that cache and does not immediately propagate to lower levels.\n- The core has one store-data execution port to the L1D that can retire at most $1$ aligned $w$-byte store per cycle in steady state. Let $w = 8$ bytes for each store.\n- The L1D cache line size is $\\ell = 64$ bytes.\n- The core frequency is $f = 3.0 \\times 10^{9}$ cycles per second.\n\nTwo sibling logical threads (Thread $0$ and Thread $1$) execute on the same physical core. Each thread loops indefinitely, performing stores to its own memory location, without reads, fences, or system calls, and the memory region is not touched by any other physical core or device. Consider two cases:\n\n- Case $\\mathrm{S}$ (Same line): Thread $0$ and Thread $1$ each repeatedly store to distinct $8$-byte words that both reside within the same $\\ell$-byte cache line (for example, offsets $0$ and $8$ within a single line). Each thread stores to the same word on each iteration, so the working set remains one line.\n- Case $\\mathrm{D}$ (Different lines): Thread $0$ and Thread $1$ each repeatedly store to distinct $8$-byte words that reside in different cache lines. Each thread’s working set remains one line, and the two lines are different.\n\nUnder steady state after any initial line allocation, predict the aggregate sustained store throughput $T$ across both threads for Case $\\mathrm{S}$ versus Case $\\mathrm{D}$, given the resource and coherence constraints above. Choose the best option.\n\nA. $T_{\\mathrm{S}} \\approx T_{\\mathrm{D}} \\approx f \\times w = 3.0 \\times 10^{9} \\times 8 \\approx 24 \\,\\text{gigabytes per second}$, because both cases are L1D store hits on one core with no coherence invalidations, and SMT threads share the same single store-data port.\n\nB. $T_{\\mathrm{S}} \\ll T_{\\mathrm{D}}$ (for example, below $1 \\,\\text{gigabyte per second}$ for $T_{\\mathrm{S}}$) because storing to different words in the same cache line induces false sharing between sibling threads, causing frequent coherence invalidations and line ping-pong on the shared L1D.\n\nC. $T_{\\mathrm{D}} > T_{\\mathrm{S}}$ by approximately a factor of $2$ (for example, $T_{\\mathrm{D}} \\approx 48 \\,\\text{gigabytes per second}$ and $T_{\\mathrm{S}} \\approx 24 \\,\\text{gigabytes per second}$), because writing to different lines allows the L1D to use multiple write ports or banks in parallel that are not available when both threads target the same line.\n\nD. $T_{\\mathrm{S}} > T_{\\mathrm{D}}$ (for example, $T_{\\mathrm{S}} \\approx 30 \\,\\text{gigabytes per second}$ and $T_{\\mathrm{D}} \\approx 24 \\,\\text{gigabytes per second}$), because two threads writing to the same line enable cross-thread write-combining that effectively increases the per-cycle store retirement rate beyond the single-port limit.", "solution": "The problem asks to predict the aggregate sustained store throughput for two logical threads running on a single SMT-enabled physical core, under two different memory access patterns.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- Core: Single physical processor core with Simultaneous Multithreading (SMT).\n- Logical Threads: $2$ (Thread $0$, Thread $1$).\n- L1 Data Cache (L1D): Shared between the two logical threads.\n- Cache Coherence Protocol: Modified Exclusive Shared Invalid (MESI).\n- Coherence Scope: Coherence invalidations occur between multiple physical cores; logical threads on the same core share L1D contents without such invalidations.\n- Cache Policy: Write-back, write-allocate.\n- Execution Resource: One store-data execution port to the L1D.\n- Store Port Throughput: At most $1$ aligned $w$-byte store retired per cycle in steady state.\n- Store Size: $w = 8$ bytes.\n- L1D Cache Line Size: $\\ell = 64$ bytes.\n- Core Frequency: $f = 3.0 \\times 10^{9}$ cycles per second.\n- Case S (Same line): Both threads store to distinct $8$-byte words within the same $\\ell=64$-byte cache line.\n- Case D (Different lines): The two threads store to $8$-byte words that reside in different cache lines.\n- Scenario Constraints: Threads loop indefinitely performing stores; memory region is not accessed by other cores; analysis is for steady state after initial line allocation.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded**: The problem is well-grounded in the principles of modern computer architecture. It accurately describes a common configuration: an SMT core with a shared L1D cache, a specific store pipeline limitation, and a standard coherence protocol (MESI). The distinction between inter-core coherence and intra-core resource sharing is a key, and correctly stated, concept.\n- **Well-Posed**: The problem is well-posed. It provides all necessary parameters ($f, w$, store port limit) to calculate a theoretical maximum throughput. The scenarios (Case S and Case D) are clearly defined, allowing for a comparative analysis. A unique, stable solution for the theoretical throughput can be derived from the givens.\n- **Objective**: The problem is stated objectively, using precise technical language and quantitative data. It is free from subjective claims.\n- **Completeness and Consistency**: The problem statement is self-contained and consistent. It provides a simplified but coherent model of a processor core. The explicit constraint of \"one store-data execution port\" is the primary bottleneck, and other details provided are consistent with this model.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. It presents a clear, consistent, and scientifically sound scenario for analysis. I will proceed with the solution.\n\n### Derivation of Solution\n\nThe core of this problem lies in identifying the primary performance bottleneck for the described workload. The problem states that the physical core has **one store-data execution port** that can retire at most **$1$ aligned $w$-byte store per cycle**.\n\nSince both logical threads (Thread $0$ and Thread $1$) execute on the same physical core, they must share this single store-data port. Simultaneous Multithreading allows instructions from both threads to be present in the processor's pipeline, but they still must compete for execution on the finite physical resources. In this case, the crucial shared resource is the store port.\n\nThe aggregate store throughput, $T$, across both threads is therefore limited by the maximum rate at which this single port can service store operations. This maximum rate is given as $1$ store per cycle.\n\nThe throughput in bytes per second can be calculated as:\n$$T = (\\text{stores per cycle}) \\times (\\text{bytes per store}) \\times (\\text{cycles per second})$$\n\nUsing the provided values:\n- Stores per cycle: $1$\n- Bytes per store: $w = 8$ bytes\n- Cycles per second: $f = 3.0 \\times 10^{9} \\text{ s}^{-1}$\n\nSo, the theoretical maximum aggregate throughput is:\n$$T_{\\text{max}} = 1 \\frac{\\text{store}}{\\text{cycle}} \\times 8 \\frac{\\text{bytes}}{\\text{store}} \\times 3.0 \\times 10^{9} \\frac{\\text{cycles}}{\\text{s}} = 24.0 \\times 10^{9} \\frac{\\text{bytes}}{\\text{s}} = 24 \\text{ GB/s}$$\n\nNow we must analyze if Case S and Case D differ from this baseline.\n\n**Case D (Different lines):**\nThread $0$ stores to a location in cache line $L_A$ and Thread $1$ stores to a location in cache line $L_B$, where $L_A \\neq L_B$. In steady state, both stores are hits in the shared L1D cache. The two threads will issue store instructions, which will be interleaved and serialized by the single store-data execution port. The aggregate throughput will be limited by this port, so we expect $T_{\\mathrm{D}} \\approx T_{\\text{max}} = 24 \\text{ GB/s}$.\n\n**Case S (Same line):**\nThread $0$ and Thread $1$ store to different words within the same cache line, $L_C$. The analysis of \"false sharing\" is critical here.\n- The problem explicitly states that \"coherence invalidations are only needed when multiple physical cores access the same cache line... logical threads on the same physical core share the same private L1D contents.\"\n- This means the MESI protocol is not invoked to invalidate the line between the sibling threads, because they are accessing the *very same* physical cache and the *very same* copy of the cache line. There is no \"ping-ponging\" of the line between different caches. The phenomenon of false sharing, which causes severe performance degradation due to coherence traffic between *different cores*, does not apply here.\n\nThe contention in Case S is purely for intra-core resources. Both threads' store operations must be retired through the single store-data port. From the port's perspective, it receives a stream of store requests. The fact that these requests target the same cache line (but different words) does not change the fundamental constraint that it can only service one request per cycle. While complex microarchitectural effects like bank conflicts could exist, the problem's dominant specified constraint is the single retirement port. Therefore, the aggregate throughput is still limited by the port's maximum rate. We expect $T_{\\mathrm{S}} \\approx T_{\\text{max}} = 24 \\text{ GB/s}$.\n\n**Conclusion:**\nBased on the provided model, the single store-data port is the limiting factor in both cases. The type of contention that characterizes inter-core false sharing (coherence invalidations) is absent. Therefore, the aggregate throughput should be approximately the same in both scenarios.\n$$T_{\\mathrm{S}} \\approx T_{\\mathrm{D}} \\approx 24 \\text{ GB/s}$$\n\n### Option-by-Option Analysis\n\n**A. $T_{\\mathrm{S}} \\approx T_{\\mathrm{D}} \\approx f \\times w = 3.0 \\times 10^{9} \\times 8 \\approx 24 \\,\\text{gigabytes per second}$, because both cases are L1D store hits on one core with no coherence invalidations, and SMT threads share the same single store-data port.**\nThis option correctly identifies the single store-data port as the bottleneck for both threads on the SMT core. It correctly calculates the resulting throughput as $f \\times w = (3.0 \\times 10^9) \\times 8 = 24 \\times 10^9$ bytes/sec, or $24$ GB/s. It also correctly states that there are no coherence invalidations, dismissing the misapplication of the \"false sharing\" concept to sibling threads on a single core. The reasoning is sound and aligns perfectly with the derivation.\n**Verdict: Correct**\n\n**B. $T_{\\mathrm{S}} \\ll T_{\\mathrm{D}}$ (for example, below $1 \\,\\text{gigabyte per second}$ for $T_{\\mathrm{S}}$) because storing to different words in the same cache line induces false sharing between sibling threads, causing frequent coherence invalidations and line ping-pong on the shared L1D.**\nThis option is fundamentally flawed. It misinterprets \"false sharing\". As explicitly clarified in the problem statement and as is true for SMT architectures, sibling threads on the same core share the L1D cache. There is only one copy of the cache line, so no MESI-level invalidations or \"ping-ponging\" can occur between them. The premise of the argument is incorrect.\n**Verdict: Incorrect**\n\n**C. $T_{\\mathrm{D}} > T_{\\mathrm{S}}$ by approximately a factor of $2$ (for example, $T_{\\mathrm{D}} \\approx 48 \\,\\text{gigabytes per second}$ and $T_{\\mathrm{S}} \\approx 24 \\,\\text{gigabytes per second}$), because writing to different lines allows the L1D to use multiple write ports or banks in parallel that are not available when both threads target the same line.**\nThis option contradicts a key premise of the problem. It is explicitly stated that the core has \"one store-data execution port\" capable of \"at most $1$ aligned $w$-byte store per cycle\". A throughput of $48$ GB/s would require $2$ stores per cycle, which is impossible given the stated constraints. The existence of multiple write ports is a fabrication not supported by the problem statement.\n**Verdict: Incorrect**\n\n**D. $T_{\\mathrm{S}} > T_{\\mathrm{D}}$ (for example, $T_{\\mathrm{S}} \\approx 30 \\,\\text{gigabytes per second}$ and $T_{\\mathrm{D}} \\approx 24 \\,\\text{gigabytes per second}$), because two threads writing to the same line enable cross-thread write-combining that effectively increases the per-cycle store retirement rate beyond the single-port limit.**\nThis option is also flawed. A throughput of $30$ GB/s would require $30 / (8 \\times 3) = 1.25$ stores to be retired per cycle, which exceeds the hard limit of $1$ store per cycle for the single store port. While advanced microarchitectures might have features like write-combining, they cannot violate the fundamental throughput limit of the execution units that retire instructions. The claim that the retirement rate can be increased \"beyond the single-port limit\" is a contradiction of the given constraints.\n**Verdict: Incorrect**", "answer": "$$\\boxed{A}$$", "id": "3641063"}]}