{"hands_on_practices": [{"introduction": "We begin by quantifying a core performance benefit of modern cache coherence protocols: direct cache-to-cache transfers. This exercise guides you through a probabilistic calculation of average memory access latency, showing how forwarding data between cores avoids costly trips to the last-level cache or main memory. Mastering this analysis is fundamental to understanding the design and performance trade-offs in any shared-memory system.", "problem": "A symmetric shared-memory multiprocessor implements the Modified, Exclusive, Shared, Invalid (MESI) cache-coherence protocol with private level-$1$ (L1) write-back caches and an inclusive shared last-level cache (LLC). Consider a steady-state workload where each memory access is to a single cache line and the following are known, based on measurement and microarchitectural characterization:\n\n- The L1 hit latency is $t_{L1} = 4$ cycles and is identical across all cores in both designs considered below.\n- The L1 miss rate is $m_1 = 0.08$ per access.\n- Conditional on an L1 miss, the probability that the requested line is currently held in a single other coreâ€™s private cache in a state that permits direct forwarding is $p_{\\text{owner}} = 0.35$.\n- If a line is forwarded directly via a cache-to-cache transfer, the end-to-end latency (including directory lookup, coherence messages, and data transmission) is $t_{\\text{c2c}} = 80$ cycles.\n- If a line is not forwarded from another core, the request is served by the LLC or main memory with an effective average latency of $t_{\\text{LLC}} = 140$ cycles.\n\nAssume no queuing, contention, or overlap effects, and that the coherence state distribution is stationary and independent from access to access except as described above. Consider two designs:\n\n- Design A supports cache-to-cache forwarding when possible.\n- Design B does not support cache-to-cache forwarding; in B, every L1 miss is served by the LLC or main memory.\n\nUsing only the fundamental definitions of miss rate as a probability and expected latency as a probability-weighted average over mutually exclusive outcomes, derive from first principles the expected per-access latency reduction (in cycles) of Design A relative to Design B. Express your final answer in cycles and round your answer to $4$ significant figures.", "solution": "The problem asks for the expected per-access latency reduction of a multiprocessor design that supports cache-to-cache forwarding (Design A) relative to one that does not (Design B). The derivation must proceed from first principles, using the definition of expected latency as a probability-weighted average over mutually exclusive outcomes.\n\nLet $E[T]$ denote the expected latency of a memory access. A memory access can result in one of two primary, mutually exclusive outcomes: a hit in the private L1 cache or a miss in the L1 cache. The law of total expectation allows us to write the expected latency as:\n$$\nE[T] = P(\\text{L1 hit}) \\times L_{\\text{hit}} + P(\\text{L1 miss}) \\times L_{\\text{miss}}\n$$\nwhere $P(\\text{L1 hit})$ and $P(\\text{L1 miss})$ are the probabilities of an L1 hit and miss, respectively, and $L_{\\text{hit}}$ and $L_{\\text{miss}}$ are the corresponding total latencies for each outcome.\n\nFrom the problem statement, we are given:\n- The L1 miss rate, $m_1 = 0.08$. This is the probability of an L1 miss, so $P(\\text{L1 miss}) = m_1$.\n- The probability of an L1 hit is therefore $P(\\text{L1 hit}) = 1 - m_1$.\n- The L1 hit latency is $t_{L1} = 4$ cycles. This is the total latency for a hit, so $L_{\\text{hit}} = t_{L1}$.\n\nThe expected latency on an L1 miss, denoted $E[L_{\\text{miss}}]$, depends on the system design. Substituting the known values into the general formula gives:\n$$\nE[T] = (1 - m_1) t_{L1} + m_1 E[L_{\\text{miss}}]\n$$\n\nWe will now analyze the two designs separately.\n\nFor Design B, which does not support cache-to-cache forwarding, any L1 miss must be served by the LLC or main memory. The problem states this effective average latency is $t_{\\text{LLC}} = 140$ cycles. Therefore, for Design B, the latency of any L1 miss is always $t_{\\text{LLC}}$. The expected latency on a miss is simply:\n$$\nE[L_{\\text{miss, B}}] = t_{\\text{LLC}}\n$$\nThe total expected per-access latency for Design B, $E[T_B]$, is:\n$$\nE[T_B] = (1 - m_1) t_{L1} + m_1 t_{\\text{LLC}}\n$$\n\nFor Design A, which supports cache-to-cache forwarding, an L1 miss has two possible subsequent outcomes:\n1. The requested cache line is forwarded from another core's private cache. This occurs with a conditional probability of $p_{\\text{owner}} = 0.35$, and the end-to-end latency for this transfer is $t_{\\text{c2c}} = 80$ cycles.\n2. The line is not forwarded and must be fetched from the LLC or main memory. This occurs with the complementary conditional probability $1 - p_{\\text{owner}}$, and the latency is $t_{\\text{LLC}} = 140$ cycles.\n\nThe expected latency on an L1 miss for Design A, $E[L_{\\text{miss, A}}]$, is the weighted average of these two outcomes:\n$$\nE[L_{\\text{miss, A}}] = p_{\\text{owner}} \\times t_{\\text{c2c}} + (1 - p_{\\text{owner}}) \\times t_{\\text{LLC}}\n$$\nThe total expected per-access latency for Design A, $E[T_A]$, is:\n$$\nE[T_A] = (1 - m_1) t_{L1} + m_1 E[L_{\\text{miss, A}}] = (1 - m_1) t_{L1} + m_1 (p_{\\text{owner}} t_{\\text{c2c}} + (1 - p_{\\text{owner}}) t_{\\text{LLC}})\n$$\n\nThe problem asks for the expected per-access latency reduction of Design A relative to Design B, which is the difference $\\Delta T = E[T_B] - E[T_A]$.\n$$\n\\Delta T = \\left[ (1 - m_1) t_{L1} + m_1 t_{\\text{LLC}} \\right] - \\left[ (1 - m_1) t_{L1} + m_1 (p_{\\text{owner}} t_{\\text{c2c}} + (1 - p_{\\text{owner}}) t_{\\text{LLC}}) \\right]\n$$\nThe term $(1 - m_1) t_{L1}$, representing the latency contribution from L1 hits, is identical in both expressions and thus cancels out. The reduction is entirely due to the different handling of L1 misses.\n$$\n\\Delta T = m_1 t_{\\text{LLC}} - m_1 (p_{\\text{owner}} t_{\\text{c2c}} + (1 - p_{\\text{owner}}) t_{\\text{LLC}})\n$$\nDistributing the term $-m_1$ on the right:\n$$\n\\Delta T = m_1 t_{\\text{LLC}} - m_1 p_{\\text{owner}} t_{\\text{c2c}} - m_1(1 - p_{\\text{owner}}) t_{\\text{LLC}}\n$$\n$$\n\\Delta T = m_1 t_{\\text{LLC}} - m_1 p_{\\text{owner}} t_{\\text{c2c}} - m_1 t_{\\text{LLC}} + m_1 p_{\\text{owner}} t_{\\text{LLC}}\n$$\nThe terms $m_1 t_{\\text{LLC}}$ and $-m_1 t_{\\text{LLC}}$ cancel, yielding:\n$$\n\\Delta T = m_1 p_{\\text{owner}} t_{\\text{LLC}} - m_1 p_{\\text{owner}} t_{\\text{c2c}}\n$$\nFactoring out the common term $m_1 p_{\\text{owner}}$ gives the final symbolic expression for the latency reduction:\n$$\n\\Delta T = m_1 p_{\\text{owner}} (t_{\\text{LLC}} - t_{\\text{c2c}})\n$$\nThis result is intuitively correct: the total latency reduction is the probability of a miss that gets satisfied by a fast cache-to-cache transfer ($m_1 \\times p_{\\text{owner}}$) multiplied by the time saved on each such transfer ($t_{\\text{LLC}} - t_{\\text{c2c}}$).\n\nNow, we substitute the provided numerical values:\n$m_1 = 0.08$\n$p_{\\text{owner}} = 0.35$\n$t_{\\text{LLC}} = 140$ cycles\n$t_{\\text{c2c}} = 80$ cycles\n\n$$\n\\Delta T = 0.08 \\times 0.35 \\times (140 - 80)\n$$\n$$\n\\Delta T = 0.08 \\times 0.35 \\times 60\n$$\n$$\n\\Delta T = 0.028 \\times 60\n$$\n$$\n\\Delta T = 1.68\n$$\nThe latency reduction is $1.68$ cycles per access. The problem requires the answer to be rounded to $4$ significant figures. To express $1.68$ with four significant figures, we write it as $1.680$.", "answer": "$$\\boxed{1.680}$$", "id": "3675527"}, {"introduction": "While efficient sharing is a goal, contention for a single piece of data creates a fundamental performance bottleneck. This practice models a high-contention \"hot spot\" scenario where multiple cores attempt to update the same memory location, forcing their operations into a single queue. By deriving the system's maximum throughput, you will uncover the hard limit imposed by serialization, a critical concept for predicting the scalability of parallel applications [@problem_id:3675634].", "problem": "Consider a shared-memory multiprocessor with $N$ identical cores, a single shared counter variable stored in one cache line, and an invalidation-based cache coherence protocol with Modified, Exclusive, Shared, Invalid (MESI) states. The system enforces sequential consistency, meaning that all memory operations appear in a single total order consistent with program order. An increment of the shared counter is performed by an atomic read-modify-write operation that requires the core to hold the cache line in the Modified state prior to the write.\n\nDefine the cache line handoff latency $t_h$ as the end-to-end time for exclusive ownership of the cache line to transfer from the current owner to a requesting core, including coherence messages, invalidations, data movement, and any arbitration delays, but excluding local execution time of the increment once the line is held in the Modified state. Assume the following stress test configuration:\n- Each core repeatedly performs a single atomic increment of the shared counter and then immediately requests the line again only after some other core has incremented, enforcing a strict round-robin ownership pattern among the $N$ cores.\n- There is always at least one requesting core when a handoff completes, so the line is continuously contended.\n- The local execution time of the increment once the cache line is obtained in the Modified state is negligible compared to $t_h$ and may be treated as zero for the purpose of throughput analysis.\n- No other memory traffic interferes, and the interconnect does not introduce additional queuing beyond what is captured in $t_h$.\n\nUnder these assumptions, derive the steady-state system throughput $X(N, t_h)$ of completed increments per second in closed form. Your final answer must be a single analytic expression in terms of $N$ and $t_h$. Do not provide an inequality or an equation to be solved; provide the final expression. You do not need to round the result. Express the throughput as increments per second.", "solution": "The system throughput, $X$, is defined as the total number of completed increments per unit of time. It can be calculated as the reciprocal of the average time elapsed between two consecutive completed increments in the system.\n\nThe problem states that an increment is an atomic read-modify-write operation that requires the core to hold the cache line in the Modified (M) state. Because only one core can hold the line in the M state at any given time, the increments are serialized across the entire system. This means that at any point in time, only one increment can be in progress.\n\nLet's analyze the sequence of events for the system to complete one increment. Assume at time $T_0$, Core $C_i$ has just completed an increment. It currently holds the cache line in the M state.\n\n1.  Due to the \"continuously contended\" and \"strict round-robin\" assumptions, the next core in the sequence, Core $C_{i+1}$ (indices are modulo $N$), has already issued a request for the cache line.\n2.  The process of transferring exclusive ownership of the cache line from Core $C_i$ to Core $C_{i+1}$ begins. This involves Core $C_i$ responding to the request, invalidating its copy (if necessary under the specific protocol implementation, though it transitions from M), and sending the data to Core $C_{i+1}$. Core $C_{i+1}$ then receives the data and gains ownership in the M state.\n3.  The problem defines the total time for this entire transfer process as the cache line handoff latency, $t_h$.\n4.  Therefore, Core $C_{i+1}$ acquires the line in the M state at time $T_0 + t_h$.\n5.  The problem specifies that the local execution time of the increment, once the line is acquired, is negligible and can be treated as zero. Thus, Core $C_{i+1}$ completes its increment effectively at the same instant it acquires the line, at time $T_1 = T_0 + t_h$.\n\nThis analysis shows that the time elapsed between the completion of one increment (by Core $C_i$) and the completion of the very next increment in the system (by Core $C_{i+1}$) is exactly $t_h$.\n\nThe system completes one increment every $t_h$ seconds. The rate of completion, which is the system throughput $X$, is the reciprocal of the time per increment.\n$$\nX = \\frac{1 \\text{ increment}}{t_h \\text{ seconds}}\n$$\nThus, the throughput is:\n$$\nX(N, t_h) = \\frac{1}{t_h}\n$$\nThe number of cores, $N$, is a parameter of the system that establishes the round-robin pattern and ensures contention. However, in this fully serialized model where the bottleneck is the handoff of the single resource, the overall system throughput is independent of the number of contenders, $N$, as long as $N > 1$ to have a handoff. If we were to analyze the throughput of a single core, it would be $\\frac{1}{N \\cdot t_h}$, since a core must wait for $N-1$ other cores to take their turn before it can increment again, a cycle that takes $N \\cdot t_h$ time. The total system throughput would then be the sum of the throughputs of the $N$ cores, which is $N \\times \\frac{1}{N \\cdot t_h} = \\frac{1}{t_h}$, confirming the result. The function $X(N, t_h)$ is therefore constant with respect to $N$.", "answer": "$$\\boxed{\\frac{1}{t_h}}$$", "id": "3675634"}, {"introduction": "Performance issues in parallel systems are not always due to direct contention for the same data variable; sometimes, they arise from how data is arranged in memory. This exercise introduces the concept of \"false sharing,\" a performance pitfall where threads accessing logically independent data experience slowdowns because their data happens to co-locate on the same cache line. By modeling and minimizing this effect, you will gain practical insight into the critical role of data layout for achieving scalable performance [@problem_id:3675600].", "problem": "Consider a shared-memory multiprocessor with private caches and a write-invalidate cache coherence protocol. The dense result matrix $C$ of a sparse matrix multiplication is stored in row-major order. Let $C$ have $M$ rows and $N$ columns, with each element occupying $s$ bytes. The base address of $C$ is taken as $\\text{base} = 0$ for simplicity. The address of element $(i,j)$ is defined by $\\mathrm{addr}(i,j) = s \\cdot (iN + j)$, and an element maps to cache line index $$\\ell(i,j; L_c) = \\left\\lfloor \\frac{s \\cdot (iN + j)}{L_c} \\right\\rfloor,$$ where $L_c$ is the cache line size in bytes. Assume that multiple threads concurrently write to distinct elements of $C$, as induced by the nonzero distribution of the sparse multiplication. Even though threads write to different words, false sharing occurs when their words reside in the same cache line, prompting coherence invalidations.\n\nWe define the false-sharing cost for a given cache line size $L_c$ as follows. For each cache line index $\\ell$, let $k_{\\ell}$ be the number of distinct threads that write at least one element mapping to $\\ell$. The per-line false-sharing event count is $\\max(0, k_{\\ell} - 1)$, since with a write-invalidate protocol at least $k_{\\ell} - 1$ inter-thread ownership transfers are required for writers to gain exclusive access. The total false-sharing cost is $$F(L_c) = \\sum_{\\ell} \\max(0, k_{\\ell} - 1).$$ Your program must, for each test case, compute $F(L_c)$ for each candidate $L_c$, select the $L_c$ that minimizes $F(L_c)$, and return both the chosen $L_c$ and the minimal cost. In case of a tie, select the smallest $L_c$ among those with minimal $F(L_c)$.\n\nAll units for $L_c$ and $s$ must be in bytes. Angles are not involved. There are no percentages in this problem.\n\nUse the following test suite. In all cases, threads write to distinct elements (no true sharing). The writes are specified by integer pairs $(i,j)$ (row index and column index), and for each test case the candidate cache line sizes are given as a set in bytes.\n\n- Test case $1$:\n  - $M = 4$, $N = 16$, $s = 8$, $T = 3$, candidates $\\{32, 64, 128\\}$.\n  - Thread $0$ writes $\\{(0,0),(0,1),(0,2),(0,3),(1,8),(1,9),(1,10),(1,11)\\}$.\n  - Thread $1$ writes $\\{(0,4),(0,5),(0,6),(0,7),(2,0),(2,1)\\}$.\n  - Thread $2$ writes $\\{(1,0),(1,1),(1,2),(1,3),(2,8),(2,9)\\}$.\n- Test case $2$:\n  - $M = 3$, $N = 2$, $s = 8$, $T = 3$, candidates $\\{16, 32, 64\\}$.\n  - Thread $0$ writes $\\{(0,0),(0,1)\\}$.\n  - Thread $1$ writes $\\{(1,0),(1,1)\\}$.\n  - Thread $2$ writes $\\{(2,0),(2,1)\\}$.\n- Test case $3$:\n  - $M = 1$, $N = 16$, $s = 8$, $T = 4$, candidates $\\{64, 128\\}$.\n  - Thread $0$ writes $\\{(0,0),(0,1),(0,2),(0,3)\\}$.\n  - Thread $1$ writes $\\{(0,4),(0,5),(0,6),(0,7)\\}$.\n  - Thread $2$ writes $\\{(0,8),(0,9),(0,10),(0,11)\\}$.\n  - Thread $3$ writes $\\{(0,12),(0,13),(0,14),(0,15)\\}$.\n- Test case $4$:\n  - $M = 4$, $N = 16$, $s = 8$, $T = 3$, candidates $\\{32, 64, 128\\}$.\n  - Thread $0$ writes $\\{(0,0),(0,8)\\}$.\n  - Thread $1$ writes $\\{(1,0)\\}$.\n  - Thread $2$ writes $\\{(2,0)\\}$.\n\nYour task is to implement a program that, for each test case, computes the optimal cache line size $L_c$ (in bytes) that minimizes the total false-sharing cost $F(L_c)$, along with the minimal cost. The final output format must aggregate the results of all provided test cases into a single line, containing a comma-separated list enclosed in square brackets, where for each test case you output the chosen $L_c$ followed by the corresponding minimal $F(L_c)$ as integers. For example, the format is $[L_{c,1},F(L_{c,1}),L_{c,2},F(L_{c,2}),\\dots]$ with no spaces.", "solution": "The problem is valid as it is scientifically grounded in the principles of computer architecture, specifically cache coherence in shared-memory multiprocessors. It is well-posed, with all necessary data and definitions provided to compute a unique, verifiable solution. The model for false sharing, while a simplification, is a standard and logical approach to quantifying this performance issue.\n\nThe objective is to determine, for each test case, the optimal cache line size $L_c$ from a given set of candidates that minimizes the total false-sharing cost, $F(L_c)$. The cost function is defined as\n$$F(L_c) = \\sum_{\\ell} \\max(0, k_{\\ell} - 1),$$\nwhere the sum is over all cache line indices $\\ell$, and $k_{\\ell}$ is the number of distinct threads writing to line $\\ell$.\n\nThe solution approach involves a direct implementation and evaluation of this cost function for each candidate $L_c$. The procedure for each test case is as follows:\n\n1.  Initialize a minimum cost, $F_{min}$, to a value larger than any possible outcome and an optimal cache line size, $L_{c,opt}$, to a sentinel value.\n\n2.  For each candidate cache line size $L_c$ provided in the test case:\n    a.  Calculate the total false-sharing cost $F(L_c)$. This sub-task is executed by first determining the set of threads that write to each cache line.\n    b.  The address of an element at row $i$ and column $j$ in the $M \\times N$ matrix $C$ is given by $\\mathrm{addr}(i,j) = s \\cdot (iN + j)$, where $s$ is the element size in bytes.\n    c.  The corresponding cache line index is $\\ell(i,j; L_c) = \\left\\lfloor \\frac{\\mathrm{addr}(i,j)}{L_c} \\right\\rfloor = \\left\\lfloor \\frac{s \\cdot (iN + j)}{L_c} \\right\\rfloor$.\n    d.  To track which threads write to which line, we can use an auxiliary data structure. An array, let's call it `line_writers`, indexed by the cache line index $\\ell$, is suitable. The maximum possible cache line index is determined by the maximum address in the matrix, which is for the element $(M-1, N-1)$, and the current $L_c$.\n    e.  For each line index $\\ell$, we need to store the set of unique threads writing to it. Since the number of threads $T$ is small, a bitmask of $T$ bits is an efficient representation. The $t$-th bit of the mask for line $\\ell$ is set to $1$ if thread $t$ writes to an element that maps to line $\\ell$. Let this array of masks be `line_thread_masks`.\n    f.  We iterate through all threads $t \\in \\{0, 1, \\dots, T-1\\}$ and for each write to an element $(i,j)$ performed by thread $t$, we calculate the line index $\\ell = \\ell(i,j; L_c)$ and update the mask for that line, for example, using a bitwise operation like `line_thread_masks[l] |= (1  t)`.\n    g.  After populating the masks for all writes, we compute the total cost $F(L_c)$. We iterate through each entry in `line_thread_masks`. For each line index $\\ell$, we count the number of set bits in its mask, which gives $k_{\\ell}$. The cost for this line is $\\max(0, k_{\\ell} - 1)$, and we sum these costs over all $\\ell$ to get $F(L_c)$.\n    h.  The number of set bits in a mask (population count) can be calculated efficiently.\n\n3.  After computing $F(L_c)$ for the current candidate, we compare it with $F_{min}$.\n    a.  If $F(L_c)  F_{min}$, a new minimum has been found. We update $F_{min} = F(L_c)$ and $L_{c,opt} = L_c$.\n    b.  If $F(L_c) = F_{min}$, we apply the tie-breaking rule: select the smallest $L_c$. Thus, we update $L_{c,opt} = \\min(L_{c,opt}, L_c)$.\n\n4.  After evaluating all candidate $L_c$ values, the pair $(L_{c,opt}, F_{min})$ constitutes the solution for the test case. This process is repeated for all test cases provided. The final results are then aggregated into the specified output format.", "answer": "```c\n#include stdio.h\n#include stdlib.h\n#include string.h\n#include math.h\n#include complex.h\n#include threads.h\n#include stdatomic.h\n\n// A struct to represent a single write operation (coordinate pair).\ntypedef struct {\n    int i, j;\n} WriteCoord;\n\n// A struct to hold all writes for a single thread.\ntypedef struct {\n    const WriteCoord* writes;\n    int num_writes;\n} ThreadWrites;\n\n// A struct to hold the parameters for a single test case.\ntypedef struct {\n    int M, N, s, T;\n    const int* lc_candidates;\n    int num_lc_candidates;\n    const ThreadWrites* thread_writes;\n} TestCase;\n\n// Helper function to count the number of set bits in an unsigned integer (popcount).\nint count_set_bits(unsigned int n) {\n    int count = 0;\n    while (n  0) {\n        n = (n - 1); // Brian Kernighan's algorithm\n        count++;\n    }\n    return count;\n}\n\nint main(void) {\n    // --- Test Case 1 Data ---\n    const WriteCoord tc1_t0_writes[] = {{0,0},{0,1},{0,2},{0,3},{1,8},{1,9},{1,10},{1,11}};\n    const WriteCoord tc1_t1_writes[] = {{0,4},{0,5},{0,6},{0,7},{2,0},{2,1}};\n    const WriteCoord tc1_t2_writes[] = {{1,0},{1,1},{1,2},{1,3},{2,8},{2,9}};\n    const ThreadWrites tc1_threads[] = {\n        {tc1_t0_writes, sizeof(tc1_t0_writes)/sizeof(WriteCoord)},\n        {tc1_t1_writes, sizeof(tc1_t1_writes)/sizeof(WriteCoord)},\n        {tc1_t2_writes, sizeof(tc1_t2_writes)/sizeof(WriteCoord)}\n    };\n    const int tc1_lc_candidates[] = {32, 64, 128};\n\n    // --- Test Case 2 Data ---\n    const WriteCoord tc2_t0_writes[] = {{0,0},{0,1}};\n    const WriteCoord tc2_t1_writes[] = {{1,0},{1,1}};\n    const WriteCoord tc2_t2_writes[] = {{2,0},{2,1}};\n    const ThreadWrites tc2_threads[] = {\n        {tc2_t0_writes, sizeof(tc2_t0_writes)/sizeof(WriteCoord)},\n        {tc2_t1_writes, sizeof(tc2_t1_writes)/sizeof(WriteCoord)},\n        {tc2_t2_writes, sizeof(tc2_t2_writes)/sizeof(WriteCoord)}\n    };\n    const int tc2_lc_candidates[] = {16, 32, 64};\n\n    // --- Test Case 3 Data ---\n    const WriteCoord tc3_t0_writes[] = {{0,0},{0,1},{0,2},{0,3}};\n    const WriteCoord tc3_t1_writes[] = {{0,4},{0,5},{0,6},{0,7}};\n    const WriteCoord tc3_t2_writes[] = {{0,8},{0,9},{0,10},{0,11}};\n    const WriteCoord tc3_t3_writes[] = {{0,12},{0,13},{0,14},{0,15}};\n    const ThreadWrites tc3_threads[] = {\n        {tc3_t0_writes, sizeof(tc3_t0_writes)/sizeof(WriteCoord)},\n        {tc3_t1_writes, sizeof(tc3_t1_writes)/sizeof(WriteCoord)},\n        {tc3_t2_writes, sizeof(tc3_t2_writes)/sizeof(WriteCoord)},\n        {tc3_t3_writes, sizeof(tc3_t3_writes)/sizeof(WriteCoord)}\n    };\n    const int tc3_lc_candidates[] = {64, 128};\n\n    // --- Test Case 4 Data ---\n    const WriteCoord tc4_t0_writes[] = {{0,0},{0,8}};\n    const WriteCoord tc4_t1_writes[] = {{1,0}};\n    const WriteCoord tc4_t2_writes[] = {{2,0}};\n    const ThreadWrites tc4_threads[] = {\n        {tc4_t0_writes, sizeof(tc4_t0_writes)/sizeof(WriteCoord)},\n        {tc4_t1_writes, sizeof(tc4_t1_writes)/sizeof(WriteCoord)},\n        {tc4_t2_writes, sizeof(tc4_t2_writes)/sizeof(WriteCoord)}\n    };\n    const int tc4_lc_candidates[] = {32, 64, 128};\n    \n    // Define the test cases from the problem statement.\n    TestCase test_cases[] = {\n        {4, 16, 8, 3, tc1_lc_candidates, sizeof(tc1_lc_candidates)/sizeof(int), tc1_threads},\n        {3, 2, 8, 3, tc2_lc_candidates, sizeof(tc2_lc_candidates)/sizeof(int), tc2_threads},\n        {1, 16, 8, 4, tc3_lc_candidates, sizeof(tc3_lc_candidates)/sizeof(int), tc3_threads},\n        {4, 16, 8, 3, tc4_lc_candidates, sizeof(tc4_lc_candidates)/sizeof(int), tc4_threads}\n    };\n\n    int num_cases = sizeof(test_cases) / sizeof(test_cases[0]);\n    int results[num_cases * 2];\n\n    for (int i = 0; i  num_cases; ++i) {\n        TestCase tc = test_cases[i];\n        int optimal_lc = -1;\n        int min_cost = -1;\n\n        long max_addr = (long)tc.s * ((long)tc.M * tc.N - 1);\n        \n        int min_candidate_lc = tc.lc_candidates[0];\n        for (int k = 1; k  tc.num_lc_candidates; ++k) {\n            if (tc.lc_candidates[k]  min_candidate_lc) {\n                min_candidate_lc = tc.lc_candidates[k];\n            }\n        }\n\n        long max_lines = (max_addr / min_candidate_lc) + 1;\n        unsigned int* line_thread_masks = (unsigned int*)malloc(max_lines * sizeof(unsigned int));\n        if (line_thread_masks == NULL) { return EXIT_FAILURE; }\n\n        for (int k = 0; k  tc.num_lc_candidates; ++k) {\n            int Lc = tc.lc_candidates[k];\n            long current_num_lines_in_use = (max_addr / Lc) + 1;\n\n            memset(line_thread_masks, 0, max_lines * sizeof(unsigned int));\n\n            for (int t = 0; t  tc.T; ++t) {\n                for (int w = 0; w  tc.thread_writes[t].num_writes; ++w) {\n                    WriteCoord coord = tc.thread_writes[t].writes[w];\n                    long addr = (long)tc.s * ((long)coord.i * tc.N + coord.j);\n                    long line_idx = addr / Lc;\n                    line_thread_masks[line_idx] |= (1U  t);\n                }\n            }\n\n            int current_total_cost = 0;\n            for (long l = 0; l  current_num_lines_in_use; ++l) {\n                int k_l = count_set_bits(line_thread_masks[l]);\n                if (k_l  1) {\n                    current_total_cost += k_l - 1;\n                }\n            }\n\n            if (optimal_lc == -1 || current_total_cost  min_cost) {\n                min_cost = current_total_cost;\n                optimal_lc = Lc;\n            } else if (current_total_cost == min_cost) {\n                if (Lc  optimal_lc) {\n                    optimal_lc = Lc;\n                }\n            }\n        }\n\n        free(line_thread_masks);\n        results[2 * i] = optimal_lc;\n        results[2 * i + 1] = min_cost;\n    }\n\n    printf(\"[\");\n    for (int i = 0; i  num_cases; ++i) {\n        printf(\"%d,%d\", results[2 * i], results[2 * i + 1]);\n        if (i  num_cases - 1) {\n            printf(\",\");\n        }\n    }\n    printf(\"]\");\n\n    return EXIT_SUCCESS;\n}\n```", "id": "3675600"}]}