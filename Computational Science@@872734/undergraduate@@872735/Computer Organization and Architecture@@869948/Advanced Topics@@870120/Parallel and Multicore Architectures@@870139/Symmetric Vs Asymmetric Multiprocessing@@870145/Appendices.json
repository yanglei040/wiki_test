{"hands_on_practices": [{"introduction": "To truly understand the trade-offs between symmetric and asymmetric multiprocessing, we must go beyond qualitative descriptions and learn to model their performance quantitatively. This first practice challenges you to use Amdahl's Law, a cornerstone of performance analysis, to derive the relative speedup between an AMP system with a specialized 'big' core and an SMP system with multiple identical cores [@problem_id:3683281]. By working through this derivation, you will gain a fundamental understanding of how a workload's characteristics—specifically, its parallelizable versus vectorizable fractions—determine which architecture holds the advantage.", "problem": "A single-program workload is to be run on two alternative multicore organizations under the same technology and power budget. Organization A is an asymmetric multiprocessor (AMP) with a single “big” core that accelerates Single Instruction Multiple Data (SIMD) vectorizable operations, and the remaining cores are baseline cores identical to the baseline single-core reference. Organization B is a symmetric multiprocessor (SMP) with $p$ identical baseline cores and no SIMD acceleration.\n\nAssume the following:\n- The baseline single-core execution time of the program is $T_{1}$.\n- The program’s instruction mix has a fraction $f_{vec}$ that is SIMD-vectorizable and only benefits on the big core in the AMP by an ideal acceleration factor $k_{v}$, while the remaining fraction $1 - f_{vec}$ does not benefit from SIMD acceleration.\n- On the SMP, a (possibly different) fraction $f_{par}$ of the program is ideally parallelizable across $p$ identical baseline cores, while the remaining fraction $1 - f_{par}$ is inherently serial and runs on a single core.\n- Ignore all overheads due to synchronization, communication, scheduling, and memory system effects; assume perfect load balancing for the parallelizable portion and ideal SIMD speedup for the vectorizable portion. All baseline cores run at the same frequency and have the same per-instruction performance as the baseline single-core reference; the big core differs only by providing the SIMD acceleration factor $k_{v}$ for the vectorizable fraction.\n\nStarting only from the definitions that execution time is the sum over disjoint code fractions and that speedup is defined as baseline time divided by optimized time, derive a closed-form expression, simplified as far as possible, for the ratio\n$$R \\equiv \\frac{S_{AMP}}{S_{SMP}},$$\nwhere $S_{AMP}$ is the speedup of the AMP when the program runs on the big core, and $S_{SMP}$ is the speedup of the SMP when the program runs with ideal parallelization on $p$ cores. Provide your final answer as a single analytic expression in terms of $f_{vec}$, $k_{v}$, $f_{par}$, and $p$.", "solution": "The problem statement is subjected to validation prior to any attempt at a solution.\n\n**Step 1: Extract Givens**\n- Baseline single-core execution time: $T_{1}$\n- System A (AMP): Asymmetric multiprocessor with one \"big\" core.\n- System B (SMP): Symmetric multiprocessor with $p$ identical baseline cores.\n- AMP program characteristics: A fraction $f_{vec}$ of the program is SIMD-vectorizable and accelerated by a factor $k_{v}$ on the big core. The remaining fraction $1 - f_{vec}$ is not accelerated. The program is specified to run on the big core.\n- SMP program characteristics: A fraction $f_{par}$ of the program is ideally parallelizable across $p$ cores. The remaining fraction $1 - f_{par}$ is inherently serial.\n- Assumptions: All overheads are ignored; load balancing and speedups are ideal.\n- Definitions: Execution time is the sum over disjoint code fractions. Speedup $S$ is defined as baseline time divided by optimized time, $S = T_{baseline} / T_{optimized}$.\n- Objective: Derive a closed-form expression for the ratio $R \\equiv \\frac{S_{AMP}}{S_{SMP}}$ in terms of $f_{vec}$, $k_{v}$, $f_{par}$, and $p$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is assessed as **valid**. It presents a well-posed, scientifically grounded scenario based on Amdahl's Law, a fundamental principle in computer architecture for modeling performance speedup. The two systems, AMP and SMP, are common architectural paradigms. The use of fractions of code ($f_{vec}$, $f_{par}$) and acceleration factors ($k_v$, $p$) is the standard methodology for this type of analysis. The assumptions, while idealizations (e.g., zero overhead), are explicitly stated to define a simplified but coherent theoretical model, which is a common and valid practice in academic problems. The problem is self-contained, objective, and logically consistent, with a clear and uniquely determinable goal.\n\n**Step 3: Solution Derivation**\nThe derivation proceeds by first calculating the execution time and corresponding speedup for each system, followed by computing their ratio.\n\nLet $T_{1}$ be the execution time of the program on a single baseline core.\n\nFirst, we analyze the Asymmetric Multiprocessor (AMP) configuration. The problem states the program runs on the big core. The program consists of a fraction $f_{vec}$ that is vectorizable and a fraction $1 - f_{vec}$ that is not. The execution time of the non-vectorizable part on the big core is the same as on a baseline core, which is $(1 - f_{vec}) T_{1}$. The vectorizable part is accelerated by a factor $k_{v}$, so its execution time on the big core is $\\frac{f_{vec} T_{1}}{k_{v}}$. The total execution time on the AMP, $T_{AMP}$, is the sum of the times for these two disjoint fractions:\n$$T_{AMP} = (1 - f_{vec}) T_{1} + \\frac{f_{vec} T_{1}}{k_{v}}$$\nFactoring out $T_{1}$, we get:\n$$T_{AMP} = T_{1} \\left( 1 - f_{vec} + \\frac{f_{vec}}{k_{v}} \\right)$$\nThe speedup of the AMP, $S_{AMP}$, is the ratio of the baseline time $T_{1}$ to the optimized time $T_{AMP}$:\n$$S_{AMP} = \\frac{T_{1}}{T_{AMP}} = \\frac{T_{1}}{T_{1} \\left( 1 - f_{vec} + \\frac{f_{vec}}{k_{v}} \\right)} = \\frac{1}{1 - f_{vec} + \\frac{f_{vec}}{k_{v}}}$$\n\nNext, we analyze the Symmetric Multiprocessor (SMP) configuration. The program has a serial fraction $1 - f_{par}$ and a parallelizable fraction $f_{par}$. The serial fraction runs on a single core, and its execution time is $(1 - f_{par}) T_{1}$. The parallelizable fraction is ideally distributed across $p$ cores, so its execution time is $\\frac{f_{par} T_{1}}{p}$. The total execution time on the SMP, $T_{SMP}$, is the sum of the times for the serial and parallel phases:\n$$T_{SMP} = (1 - f_{par}) T_{1} + \\frac{f_{par} T_{1}}{p}$$\nFactoring out $T_{1}$:\n$$T_{SMP} = T_{1} \\left( 1 - f_{par} + \\frac{f_{par}}{p} \\right)$$\nThe speedup of the SMP, $S_{SMP}$, is the ratio of the baseline time $T_{1}$ to the optimized time $T_{SMP}$:\n$$S_{SMP} = \\frac{T_{1}}{T_{SMP}} = \\frac{T_{1}}{T_{1} \\left( 1 - f_{par} + \\frac{f_{par}}{p} \\right)} = \\frac{1}{1 - f_{par} + \\frac{f_{par}}{p}}$$\n\nFinally, we compute the required ratio, $R \\equiv \\frac{S_{AMP}}{S_{SMP}}$:\n$$R = \\frac{S_{AMP}}{S_{SMP}} = \\frac{\\frac{1}{1 - f_{vec} + \\frac{f_{vec}}{k_{v}}}}{\\frac{1}{1 - f_{par} + \\frac{f_{par}}{p}}}$$\nThis simplifies to the reciprocal of the denominators:\n$$R = \\frac{1 - f_{par} + \\frac{f_{par}}{p}}{1 - f_{vec} + \\frac{f_{vec}}{k_{v}}}$$\nTo simplify this expression further and eliminate compound fractions, we can find common denominators for the numerator and the denominator of the main fraction.\nThe numerator becomes:\n$$1 - f_{par} + \\frac{f_{par}}{p} = \\frac{p(1 - f_{par}) + f_{par}}{p} = \\frac{p - p f_{par} + f_{par}}{p} = \\frac{p - f_{par}(p - 1)}{p}$$\nThe denominator becomes:\n$$1 - f_{vec} + \\frac{f_{vec}}{k_{v}} = \\frac{k_{v}(1 - f_{vec}) + f_{vec}}{k_{v}} = \\frac{k_{v} - k_{v} f_{vec} + f_{vec}}{k_{v}} = \\frac{k_{v} - f_{vec}(k_{v} - 1)}{k_{v}}$$\nSubstituting these back into the expression for $R$:\n$$R = \\frac{\\frac{p - f_{par}(p - 1)}{p}}{\\frac{k_{v} - f_{vec}(k_{v} - 1)}{k_{v}}}$$\nThis simplifies to the final closed-form expression:\n$$R = \\frac{k_{v} [p - f_{par}(p - 1)]}{p[k_{v} - f_{vec}(k_{v} - 1)]}$$\nThis expression gives the performance ratio of the AMP system relative to the SMP system under the specified idealized conditions.", "answer": "$$\\boxed{\\frac{k_{v} [p - f_{par}(p - 1)]}{p [k_{v} - f_{vec}(k_{v} - 1)]}}$$", "id": "3683281"}, {"introduction": "While Amdahl's Law provides a high-level view, a more detailed analysis requires drilling down into the specifics of core microarchitecture and memory performance. This exercise introduces a model based on Cycles Per Instruction (CPI), which separates a core's raw processing speed (base CPI) from stalls caused by memory access latency [@problem_id:3683318]. You will calculate the performance of different workloads on the 'big' and 'small' cores of an AMP system, learning how to make informed scheduling decisions by matching memory-intensive tasks to the cores best equipped to handle them.", "problem": "A research team is comparing symmetric multiprocessing (SMP) with asymmetric multiprocessing (AMP) using a first-principles analytical model grounded in the definitions of cycles per instruction (CPI) and misses per kilo-instruction (MPKI). Use the following definitions as the base for your derivation: CPI is defined as the average number of processor cycles per retired instruction; MPKI is defined as the number of last-level cache misses per $1000$ retired instructions; and each last-level cache miss incurs a stall of $L$ cycles that is not overlapped with other work.\n\nTwo workloads, $W_{A}$ and $W_{B}$, will be executed. Workload parameters are:\n- $W_{A}$: MPKI $= 8$, instruction count $I_{A} = 2.0 \\times 10^{9}$.\n- $W_{B}$: MPKI $= 30$, instruction count $I_{B} = 1.0 \\times 10^{9}$.\n\nSystem configurations:\n- Symmetric multiprocessing (SMP): two identical cores, each with base CPI (i.e., CPI with a perfect cache) $CPI_{\\text{base,SMP}} = 0.7$ and last-level cache miss stall latency $L_{\\text{SMP}} = 120$ cycles.\n- Asymmetric multiprocessing (AMP): one big core and one small core. The big core has base CPI $CPI_{\\text{base,big}} = 0.5$ and miss stall latency $L_{\\text{big}} = 80$ cycles. The small core has base CPI $CPI_{\\text{base,small}} = 0.9$ and miss stall latency $L_{\\text{small}} = 140$ cycles.\n\nExecution policy:\n- On the SMP system, $W_{A}$ and $W_{B}$ each run on one of the identical cores, so both workloads experience the same $CPI_{\\text{base,SMP}}$ and $L_{\\text{SMP}}$.\n- On the AMP system, $W_{B}$ runs on the big core and $W_{A}$ runs on the small core for the duration of their execution.\n\nAssumptions:\n- Beyond the base CPI, the only source of stalls is last-level cache misses.\n- The miss stall latency $L$ is fully exposed (no overlap), and the expected additional cycles per instruction due to misses follow directly from the definitions of MPKI and $L$.\n- Ignore frequency scaling and assume all CPIs are measured in the same cycle domain.\n\nTasks:\n1. Starting only from the above definitions, derive the per-core CPI for each workload under the SMP system and under the AMP system.\n2. Let the instruction-weighted average CPI of a configuration be defined as $$\\overline{\\text{CPI}} \\equiv \\frac{CPI_{A}\\,I_{A} + CPI_{B}\\,I_{B}}{I_{A} + I_{B}},$$ where $CPI_{A}$ and $CPI_{B}$ are the CPIs of $W_{A}$ and $W_{B}$ under that configuration. Compute the ratio $$R \\equiv \\frac{\\overline{\\text{CPI}}_{\\text{SMP}}}{\\overline{\\text{CPI}}_{\\text{AMP}}}.$$\n\nProvide the final value of $R$ as a single real number, rounded to four significant figures. No units are required.", "solution": "The problem requires a comparative analysis of symmetric multiprocessing (SMP) and asymmetric multiprocessing (AMP) systems by calculating an instruction-weighted average Cycles Per Instruction (CPI) for a given workload mix. The solution will be derived from the first principles provided.\n\nFirst, we must establish a general formula for the total CPI of a core. The problem states that the total CPI is the sum of a base CPI, $CPI_{\\text{base}}$, and a stall component, $CPI_{\\text{stall}}$, which arises exclusively from last-level cache misses.\n$$CPI = CPI_{\\text{base}} + CPI_{\\text{stall}}$$\nThe stall component, $CPI_{\\text{stall}}$, represents the average number of stall cycles per instruction. This can be derived from the provided definitions:\n- MPKI (Misses Per Kilo-Instruction) is the number of misses per $1000$ instructions.\n- $L$ is the latency in cycles for each miss.\n\nThe rate of misses per instruction is therefore $\\frac{\\text{MPKI}}{1000}$. Since each miss incurs a penalty of $L$ cycles, the average number of stall cycles per instruction is the product of the miss rate and the miss latency.\n$$CPI_{\\text{stall}} = \\left(\\frac{\\text{MPKI}}{1000}\\right) \\times L$$\nCombining these, the comprehensive formula for the total CPI is:\n$$CPI = CPI_{\\text{base}} + \\frac{\\text{MPKI} \\times L}{1000}$$\nThis formula will be used to complete the first task of the problem.\n\n**Task 1: Per-Core CPI Calculation**\n\nWe will now apply this formula to calculate the CPI for each workload ($W_{A}$, $W_{B}$) on each system configuration (SMP, AMP).\n\n**Symmetric Multiprocessing (SMP) System:**\nFor the SMP system, both cores are identical with $CPI_{\\text{base,SMP}} = 0.7$ and miss latency $L_{\\text{SMP}} = 120$ cycles. The workloads are characterized by $MPKI_{A} = 8$ and $MPKI_{B} = 30$.\n\nThe CPI for workload $W_{A}$ on the SMP system ($CPI_{A, \\text{SMP}}$) is:\n$$CPI_{A, \\text{SMP}} = CPI_{\\text{base,SMP}} + \\frac{MPKI_{A} \\times L_{\\text{SMP}}}{1000} = 0.7 + \\frac{8 \\times 120}{1000} = 0.7 + \\frac{960}{1000} = 0.7 + 0.96 = 1.66$$\n\nThe CPI for workload $W_{B}$ on the SMP system ($CPI_{B, \\text{SMP}}$) is:\n$$CPI_{B, \\text{SMP}} = CPI_{\\text{base,SMP}} + \\frac{MPKI_{B} \\times L_{\\text{SMP}}}{1000} = 0.7 + \\frac{30 \\times 120}{1000} = 0.7 + \\frac{3600}{1000} = 0.7 + 3.6 = 4.3$$\n\n**Asymmetric Multiprocessing (AMP) System:**\nFor the AMP system, workload $W_{A}$ runs on the small core and $W_{B}$ runs on the big core. The core parameters are distinct.\n- Small core (for $W_{A}$): $CPI_{\\text{base,small}} = 0.9$ and $L_{\\text{small}} = 140$ cycles.\n- Big core (for $W_{B}$): $CPI_{\\text{base,big}} = 0.5$ and $L_{\\text{big}} = 80$ cycles.\n\nThe CPI for workload $W_{A}$ on the AMP system ($CPI_{A, \\text{AMP}}$) is:\n$$CPI_{A, \\text{AMP}} = CPI_{\\text{base,small}} + \\frac{MPKI_{A} \\times L_{\\text{small}}}{1000} = 0.9 + \\frac{8 \\times 140}{1000} = 0.9 + \\frac{1120}{1000} = 0.9 + 1.12 = 2.02$$\n\nThe CPI for workload $W_{B}$ on the AMP system ($CPI_{B, \\text{AMP}}$) is:\n$$CPI_{B, \\text{AMP}} = CPI_{\\text{base,big}} + \\frac{MPKI_{B} \\times L_{\\text{big}}}{1000} = 0.5 + \\frac{30 \\times 80}{1000} = 0.5 + \\frac{2400}{1000} = 0.5 + 2.4 = 2.9$$\n\n**Task 2: Calculation of the Ratio $R$**\n\nThe second task is to compute the ratio $R \\equiv \\frac{\\overline{\\text{CPI}}_{\\text{SMP}}}{\\overline{\\text{CPI}}_{\\text{AMP}}}$. The instruction-weighted average CPI, $\\overline{\\text{CPI}}$, is given by the formula:\n$$\\overline{\\text{CPI}} = \\frac{CPI_{A}\\,I_{A} + CPI_{B}\\,I_{B}}{I_{A} + I_{B}}$$\nThe instruction counts are $I_{A} = 2.0 \\times 10^{9}$ and $I_{B} = 1.0 \\times 10^{9}$.\n\nFirst, we calculate $\\overline{\\text{CPI}}_{\\text{SMP}}$ using the derived CPI values for the SMP system:\n$$\\overline{\\text{CPI}}_{\\text{SMP}} = \\frac{CPI_{A, \\text{SMP}} \\times I_{A} + CPI_{B, \\text{SMP}} \\times I_{B}}{I_{A} + I_{B}}$$\n$$\\overline{\\text{CPI}}_{\\text{SMP}} = \\frac{(1.66) \\times (2.0 \\times 10^{9}) + (4.3) \\times (1.0 \\times 10^{9})}{2.0 \\times 10^{9} + 1.0 \\times 10^{9}}$$\n$$\\overline{\\text{CPI}}_{\\text{SMP}} = \\frac{(1.66 \\times 2.0 + 4.3 \\times 1.0) \\times 10^{9}}{3.0 \\times 10^{9}} = \\frac{3.32 + 4.3}{3.0} = \\frac{7.62}{3.0} = 2.54$$\n\nNext, we calculate $\\overline{\\text{CPI}}_{\\text{AMP}}$ using the derived CPI values for the AMP system:\n$$\\overline{\\text{CPI}}_{\\text{AMP}} = \\frac{CPI_{A, \\text{AMP}} \\times I_{A} + CPI_{B, \\text{AMP}} \\times I_{B}}{I_{A} + I_{B}}$$\n$$\\overline{\\text{CPI}}_{\\text{AMP}} = \\frac{(2.02) \\times (2.0 \\times 10^{9}) + (2.9) \\times (1.0 \\times 10^{9})}{2.0 \\times 10^{9} + 1.0 \\times 10^{9}}$$\n$$\\overline{\\text{CPI}}_{\\text{AMP}} = \\frac{(2.02 \\times 2.0 + 2.9 \\times 1.0) \\times 10^{9}}{3.0 \\times 10^{9}} = \\frac{4.04 + 2.9}{3.0} = \\frac{6.94}{3.0}$$\n\nFinally, we compute the ratio $R$:\n$$R = \\frac{\\overline{\\text{CPI}}_{\\text{SMP}}}{\\overline{\\text{CPI}}_{\\text{AMP}}} = \\frac{2.54}{\\frac{6.94}{3.0}} = \\frac{2.54 \\times 3.0}{6.94} = \\frac{7.62}{6.94}$$\n$$R \\approx 1.0979827089...$$\nAs requested, the value of $R$ must be rounded to four significant figures.\n$$R \\approx 1.098$$", "answer": "$$\\boxed{1.098}$$", "id": "3683318"}, {"introduction": "A key challenge in asymmetric computing is deciding when the benefit of using a powerful, specialized core outweighs the cost of moving the task to it. This final practice simulates a common scenario where a 'big' core offers significant acceleration but requires a fixed overhead for task offloading [@problem_id:3683271]. By deriving the break-even point for kernel size, you will develop the analytical skills needed to create smart scheduling heuristics that maximize the performance of heterogeneous systems.", "problem": "Consider a multicore system that mixes Symmetric Multiprocessing (SMP) and Asymmetric Multiprocessing (AMP) design ideas. The system has two cores: a \"little\" core and a \"big\" core. In the Asymmetric Multiprocessing (AMP) sense, only the big core implements specialized Instruction Set Architecture (ISA) extensions that accelerate a particular kernel. The little core lacks these ISA extensions and can only execute a baseline kernel implementation.\n\nAssume the following scientifically grounded performance model based on first principles:\n- The kernel consists of $n$ extension-eligible operations.\n- The little core executes at a baseline service rate of $\\mu$ operations per second.\n- The big core, when running the ISA-accelerated kernel, achieves a multiplicative speedup factor $k$ relative to the little core, so its service rate is $k \\mu$, with $k > 1$.\n- Offloading the kernel to the big core requires a one-time fixed copy and setup cost $T_{\\text{copy}}$ measured in seconds, covering code/data movement and launch overhead. This cost is independent of $n$.\n- Ignore queuing, contention, and interference; assume dedicated execution on whichever core runs the kernel.\n\nUsing only the fundamental relationship that completion time equals work divided by service rate plus any fixed overhead, define the condition under which offloading \"helps\" as a strictly smaller total completion time when offloading to the big core compared to executing locally on the little core. Derive the exact closed-form symbolic expression for the minimum kernel size $n^{\\*}$ (in operations) above which offloading helps. Express your final answer solely in terms of $k$, $\\mu$, and $T_{\\text{copy}}$. No numerical evaluation is required, and no rounding is permitted. Report $n^{\\*}$ in units of operations (instructions).", "solution": "The problem contrasts Symmetric Multiprocessing (SMP), where cores are identical, with Asymmetric Multiprocessing (AMP), where cores differ. Here, only the big core has specialized Instruction Set Architecture (ISA) extensions, giving it an asymmetric advantage on the given kernel. We model execution using the fundamental law that completion time equals work divided by service rate plus any fixed overhead.\n\nLet the kernel contain $n$ extension-eligible operations. The little core executes at rate $\\mu$ operations per second, so the local execution time on the little core is\n$$\nT_{\\text{local}} = \\frac{n}{\\mu}.\n$$\nIf the kernel is offloaded to the big core, there is a fixed overhead $T_{\\text{copy}}$ to copy and set up code/data. The big core executes the kernel at $k \\mu$ operations per second due to the ISA extension. Therefore, the total offloaded completion time is\n$$\nT_{\\text{offload}} = T_{\\text{copy}} + \\frac{n}{k \\mu}.\n$$\n\nOffloading \"helps\" if the offloaded completion time is strictly less than the local completion time:\n$$\nT_{\\text{offload}} < T_{\\text{local}}.\n$$\nSubstitute the expressions:\n$$\nT_{\\text{copy}} + \\frac{n}{k \\mu} < \\frac{n}{\\mu}.\n$$\nRearrange to isolate $n$ terms on the right-hand side:\n$$\nT_{\\text{copy}} < \\frac{n}{\\mu} - \\frac{n}{k \\mu} = \\frac{n}{\\mu}\\left(1 - \\frac{1}{k}\\right).\n$$\nSolve for $n$:\n$$\nn > \\mu T_{\\text{copy}} \\cdot \\frac{1}{1 - \\frac{1}{k}}.\n$$\nRecognize that $1 - \\frac{1}{k} = \\frac{k - 1}{k}$, hence\n$$\nn > \\mu T_{\\text{copy}} \\cdot \\frac{k}{k - 1}.\n$$\nThe minimum kernel size $n^{\\*}$ at which offloading begins to help (i.e., where equality holds and the two completion times are equal) is\n$$\nn^{\\*} = \\frac{k \\mu T_{\\text{copy}}}{k - 1}.\n$$\n\nThis result is consistent with the intuitive trade-off: larger $k$ (greater acceleration) or larger $\\mu$ (higher baseline rate) reduces the threshold in time units, but, because $n^{\\*}$ is in operations, it increases proportionally with $\\mu T_{\\text{copy}}$ and scales with $\\frac{k}{k - 1}$, reflecting diminishing returns as $k$ grows large. The final expression provides the exact symbolic threshold in terms of $k$, $\\mu$, and $T_{\\text{copy}}$.", "answer": "$$\\boxed{\\frac{k \\mu T_{\\text{copy}}}{k - 1}}$$", "id": "3683271"}]}