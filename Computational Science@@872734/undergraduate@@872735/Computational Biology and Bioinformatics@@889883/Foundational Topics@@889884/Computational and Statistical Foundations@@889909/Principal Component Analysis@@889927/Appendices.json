{"hands_on_practices": [{"introduction": "To truly understand Principal Component Analysis, it's essential to look beyond the \"black box\" of software packages and grasp the underlying mathematical mechanics. This exercise guides you through a manual calculation of the first principal component from a small, hypothetical gene expression dataset. By computing the covariance matrix and finding its dominant eigenvector, you will solidify your understanding of how PCA identifies the primary axis of variation in data [@problem_id:2416060].", "problem": "A gene expression experiment measured log-transformed expression values for $G_1$, $G_2$, and $G_3$ across $S_1$, $S_2$, $S_3$, and $S_4$. The data matrix $X \\in \\mathbb{R}^{4 \\times 3}$ has samples as rows and genes as columns:\n$$\nX \\;=\\; \\begin{pmatrix}\n1 & 2 & 3 \\\\\n2 & 3 & 4 \\\\\n3 & 4 & 5 \\\\\n4 & 5 & 6\n\\end{pmatrix}.\n$$\nTreat samples as independent observations and genes as variables. Using principal component analysis (PCA), compute the first principal component loading vector in gene space by:\n- column-centering $X$,\n- forming the sample covariance matrix across genes with denominator $n-1$ for $n=4$ samples, and\n- taking the unit-norm eigenvector of this covariance matrix corresponding to the largest eigenvalue.\n\nReport the loading vector as a $1 \\times 3$ row matrix ordered as $(G_1, G_2, G_3)$, with the sign chosen so that its first nonzero entry is positive. No rounding is required.", "solution": "We are asked to compute the first principal component loading vector in gene space using the eigen-decomposition of the sample covariance matrix across genes. Let $n=4$ be the number of samples and $p=3$ be the number of genes. The data matrix is $X \\in \\mathbb{R}^{n \\times p}$ with rows as samples and columns as genes.\n\nStep $1$: Column-centering. Compute the column means of $X$:\n$$\n\\bar{x}_{\\cdot 1} \\;=\\; \\frac{1+2+3+4}{4} \\;=\\; 2.5,\\quad\n\\bar{x}_{\\cdot 2} \\;=\\; \\frac{2+3+4+5}{4} \\;=\\; 3.5,\\quad\n\\bar{x}_{\\cdot 3} \\;=\\; \\frac{3+4+5+6}{4} \\;=\\; 4.5.\n$$\nSubtract these means from each column to obtain the centered matrix $Z$:\n$$\nZ \\;=\\; X - \\mathbf{1}\\bar{x}^{\\top}\n\\;=\\;\n\\begin{pmatrix}\n1-2.5 & 2-3.5 & 3-4.5 \\\\\n2-2.5 & 3-3.5 & 4-4.5 \\\\\n3-2.5 & 4-3.5 & 5-4.5 \\\\\n4-2.5 & 5-3.5 & 6-4.5\n\\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n-1.5 & -1.5 & -1.5 \\\\\n-0.5 & -0.5 & -0.5 \\\\\n\\phantom{-}0.5 & \\phantom{-}0.5 & \\phantom{-}0.5 \\\\\n\\phantom{-}1.5 & \\phantom{-}1.5 & \\phantom{-}1.5\n\\end{pmatrix}.\n$$\n\nStep $2$: Sample covariance matrix across genes. Using the denominator $n-1=3$, the sample covariance of genes is\n$$\nS \\;=\\; \\frac{1}{n-1}\\, Z^{\\top} Z \\;=\\; \\frac{1}{3}\\, Z^{\\top} Z.\n$$\nObserve that each row of $Z$ is a scalar multiple of $(1,\\,1,\\,1)$, so all three centered gene columns are identical. Compute $Z^{\\top}Z$ by noting that for any two columns $j$ and $k$, the $(j,k)$ entry equals $\\sum_{i=1}^{n} Z_{ij} Z_{ik}$. Since all three columns are identical, every entry of $Z^{\\top}Z$ equals the sum of squares of a single centered column:\n$$\n\\sum_{i=1}^{4} Z_{i1}^{2} \\;=\\; (-1.5)^{2} + (-0.5)^{2} + (0.5)^{2} + (1.5)^{2} \\;=\\; 2.25 + 0.25 + 0.25 + 2.25 \\;=\\; 5.\n$$\nTherefore,\n$$\nZ^{\\top} Z \\;=\\; 5 \\begin{pmatrix}\n1 & 1 & 1 \\\\\n1 & 1 & 1 \\\\\n1 & 1 & 1\n\\end{pmatrix}\n\\quad\\text{and}\\quad\nS \\;=\\; \\frac{1}{3} Z^{\\top}Z \\;=\\; \\frac{5}{3}\n\\begin{pmatrix}\n1 & 1 & 1 \\\\\n1 & 1 & 1 \\\\\n1 & 1 & 1\n\\end{pmatrix}.\n$$\n\nStep $3$: Eigen-decomposition. Let $J \\in \\mathbb{R}^{3 \\times 3}$ denote the all-ones matrix, i.e., $J_{jk}=1$ for all $j,k$. It is known from first principles that $J$ has rank $1$ with eigenvalues $3$ and $0$ (with multiplicity $2$). A corresponding unit-norm eigenvector for the eigenvalue $3$ is proportional to $(1,\\,1,\\,1)^{\\top}$, specifically\n$$\nv \\;=\\; \\frac{1}{\\sqrt{3}} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}.\n$$\nSince $S = \\frac{5}{3} J$, the eigenvalues of $S$ are $\\lambda_{1} = \\frac{5}{3} \\cdot 3 = 5$ and $\\lambda_{2} = 0$, $\\lambda_{3} = 0$, with the same eigenvectors as $J$. Thus, the first principal component loading vector in gene space is the unit-norm eigenvector associated with $\\lambda_{1}=5$, namely $v$ as above. Choosing the sign so that the first nonzero entry is positive yields\n$$\nv \\;=\\; \\left( \\frac{1}{\\sqrt{3}},\\; \\frac{1}{\\sqrt{3}},\\; \\frac{1}{\\sqrt{3}} \\right).\n$$\n\nTherefore, ordered as $(G_1, G_2, G_3)$ and written as a $1 \\times 3$ row matrix, the first principal component loading vector is\n$$\n\\begin{pmatrix}\n\\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{3}}\n\\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{3}}\\end{pmatrix}}$$", "id": "2416060"}, {"introduction": "While PCA is a powerful tool for finding patterns, the patterns it finds are not always the ones you are looking for. In bioinformatics, it is common for technical artifacts, such as differences in experimental conditions between laboratories, to create more variance than the biological effect under study. This problem presents a classic \"batch effect\" scenario and challenges you to choose the most scientifically rigorous next step, a critical skill for anyone analyzing real-world biological data [@problem_id:2416092].", "problem": "You have an expression data matrix with $p$ genes measured across $n$ biological samples collected in $5$ different laboratories. Each sample has a biological condition label (case or control), and the study design balanced conditions across the $5$ laboratories. After centering and scaling genes, you perform principal component analysis (PCA). A scatterplot of the first two principal components shows that $PC_1$ and $PC_2$ perfectly separate the samples by laboratory, while the case versus control labels do not separate along these axes. You intend to use the data to discover biology related to the condition. What is the most appropriate next step to ensure valid biological interpretation?\n\nA. Conclude that the laboratory is the primary biological driver in the study and proceed with downstream analyses without considering laboratory labels.\n\nB. Investigate and mitigate batch effects attributable to laboratory (for example, include laboratory as a covariate in downstream linear modeling or apply a batch-correction method), then re-run PCA to confirm that technical variation has been addressed before interpreting biological structure.\n\nC. Remove samples from the laboratories that appear at the extremes of $PC_1$ and $PC_2$ until the separation by laboratory disappears, then proceed.\n\nD. Ignore $PC_1$ and $PC_2$ and switch to $PC_3$ and $PC_4$ for visualization and inference, since lower-variance components may capture the biology of interest.\n\nE. Increase the number of retained principal components to $k=20$ so that the biological condition will be represented in at least one component without addressing the laboratory effect.", "solution": "The problem statement will now be subjected to rigorous validation.\n\n### Step 1: Extract Givens\n- Data Source: An expression data matrix.\n- Data Dimensions: $p$ genes, $n$ biological samples.\n- Experimental Design: Samples collected in $5$ different laboratories.\n- Sample Annotation: Each sample has a biological condition label ('case' or 'control').\n- Study Design Constraint: Conditions are balanced across the $5$ laboratories.\n- Preprocessing: Genes are centered and scaled.\n- Analysis Method: Principal component analysis (PCA) is performed.\n- Observation 1: A scatterplot of the first two principal components, $PC_1$ and $PC_2$, shows perfect separation of samples by laboratory.\n- Observation 2: The case versus control labels do not show separation along $PC_1$ and $PC_2$.\n- Objective: To use the data to discover biology related to the condition.\n- Question: What is the most appropriate next step to ensure valid biological interpretation?\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the established criteria.\n\n1.  **Scientifically Grounded**: The scenario described is a classic and frequent occurrence in high-throughput biological research, particularly in genomics and transcriptomics. The presence of strong, systematic variation attributable to the data-generating environment (e.g., laboratory, instrument, reagent batch) is known as a \"batch effect\". PCA is a standard, fundamental technique for exploratory data analysis precisely to detect such effects. The observation that the components with the highest variance ($PC_1$, $PC_2$) correlate with a technical variable (laboratory) instead of the biological variable of interest (condition) is a textbook example of a batch effect confounding a study. The problem is scientifically sound and realistic.\n2.  **Well-Posed**: The question asks for the \"most appropriate next step\". In the context of bioinformatics and statistics, there are established best practices for handling confounding variables and batch effects. The problem is well-posed as it seeks the correct methodological procedure from a set of options, a procedure which is dictated by fundamental statistical principles to avoid drawing spurious conclusions.\n3.  **Objective**: The statement is presented with objective, technical language. It describes a data analysis scenario without subjectivity or bias.\n4.  **Complete and Consistent**: The problem provides all necessary information to understand the context. The goal (discover biology) is clear, and the obstacle (technical variation dominating biological variation) is clearly stated. There are no internal contradictions.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. It describes a common and well-defined challenge in computational biology. Thus, I will proceed to derive the correct solution and evaluate the given options.\n\nThe core issue is that the dominant sources of variation in the dataset, captured by $PC_1$ and $PC_2$, are technical artifacts originating from inter-laboratory differences. These are known as **batch effects**. The goal of the study is to identify biological differences between 'case' and 'control' groups. However, the strong batch effect is masking this biological signal and will confound any downstream statistical analysis, leading to erroneous conclusions where differences between laboratories are mistaken for differences between conditions.\n\nThe fundamental principle of sound experimental data analysis is to account for or remove known sources of unwanted variation before attempting to interpret the signal of interest. Therefore, the correct procedure is to address the batch effect. This can be achieved in two primary ways:\n1.  **Explicit Modeling**: Include the batch variable (in this case, 'laboratory') as a covariate in a statistical model, such as a general linear model. This allows the model to estimate and account for the variance attributable to the laboratory, thereby providing a more accurate estimate of the effect of the biological condition.\n2.  **Data Adjustment**: Apply a dedicated batch correction algorithm (e.g., ComBat, removeBatchEffect from the `limma` package) to adjust the expression matrix directly, aiming to remove the variation associated with the batch while preserving the biological variation.\n\nAfter applying a correction method, it is imperative to verify its efficacy. Re-running PCA on the corrected data is a standard diagnostic step. If the correction was successful, the samples should no longer cluster by laboratory in the top principal components, which would indicate that the dominant source of technical variance has been mitigated. This would then allow for the potential emergence of the biological signal (case vs. control separation) in the principal components or in subsequent downstream analyses.\n\nNow, I will evaluate each option based on this reasoning.\n\n**A. Conclude that the laboratory is the primary biological driver in the study and proceed with downstream analyses without considering laboratory labels.**\nThis conclusion is fundamentally flawed. The laboratory is a *technical* variable, not a biological one. The observed differences are artifacts of experimental procedure, not a biological phenomenon of interest. Proceeding without addressing this confounding effect guarantees that any findings will be biased and likely incorrect, reflecting laboratory-specific signatures instead of case-control biology.\n**Verdict: Incorrect.**\n\n**B. Investigate and mitigate batch effects attributable to laboratory (for example, include laboratory as a covariate in downstream linear modeling or apply a batch-correction method), then re-run PCA to confirm that technical variation has been addressed before interpreting biological structure.**\nThis option describes the precisely correct and standard methodology. It correctly identifies the problem as a \"batch effect\", suggests appropriate and scientifically valid methods for mitigation (modeling as a covariate or using batch-correction tools), and includes the critical step of verification (re-running PCA). This is the only approach that ensures the subsequent search for biological signal is not compromised by technical artifacts.\n**Verdict: Correct.**\n\n**C. Remove samples from the laboratories that appear at the extremes of $PC_1$ and $PC_2$ until the separation by laboratory disappears, then proceed.**\nThis is a crude and scientifically unsound approach. Removing data is a drastic measure that reduces statistical power. Since the problem states perfect separation by $5$ labs, this approach would entail removing entire laboratories from the study. This would unbalance the experimental design and could introduce new biases. It does not solve the underlying issue, as systematic differences likely exist between the remaining laboratories as well. Data removal is not a valid substitute for proper statistical correction of known confounders.\n**Verdict: Incorrect.**\n\n**D. Ignore $PC_1$ and $PC_2$ and switch to $PC_3$ and $PC_4$ for visualization and inference, since lower-variance components may capture the biology of interest.**\nThis is an inadequate and naive solution. While it is possible for biological signals to appear in lower-variance components, the batch effect's influence is not confined to $PC_1$ and $PC_2$. The batch effect permeates the entire dataset. Any statistical test (e.g., differential expression analysis) performed on the raw data matrix will be confounded by the total variance, which is dominated by the batch effect. One cannot simply \"ignore\" the largest sources of variation in the data and expect to obtain valid results. The confounding effect must be formally removed or modeled.\n**Verdict: Incorrect.**\n\n**E. Increase the number of retained principal components to $k=20$ so that the biological condition will be represented in at least one component without addressing the laboratory effect.**\nThis approach fails to address the root problem. Simply considering more principal components does not remove the confounding batch effect. The first principal components will still be dominated by technical noise, and any analysis that uses a combination of these components will remain biased. The goal is not merely to find a component that correlates with biology, but to perform a valid statistical analysis on data where technical noise does not overwhelm the biological signal. This requires correction, not a wider search through uncorrected data dimensions.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{B}$$", "id": "2416092"}, {"introduction": "The principal components of a dataset can sometimes be disproportionately influenced by a few \"high-leverage\" data points or outliers, potentially leading to misleading conclusions. This hands-on coding exercise explores the stability of PCA by examining a scenario where removing a single sample can completely swap the order of the top two principal components. By implementing a formal check for this phenomenon, you will develop an intuition for the robustness of PCA and the importance of identifying influential points in your data [@problem_id:2416110].", "problem": "You are given a definition of Principal Component Analysis (PCA) applied to a gene expression matrix across patients. Let $X \\in \\mathbb{R}^{n \\times m}$ denote a data matrix with $n$ patient samples (rows) and $m$ genes (columns). Define the column-wise centered matrix $\\tilde{X}$ by subtracting the sample mean of each column of $X$. The sample covariance matrix is\n$$\nC \\;=\\; \\frac{1}{n-1}\\,\\tilde{X}^\\top \\tilde{X} \\;\\in\\; \\mathbb{R}^{m \\times m}.\n$$\nLet the top two eigenpairs of $C$ be $(\\lambda_1, v_1)$ and $(\\lambda_2, v_2)$, with $\\lambda_1 \\ge \\lambda_2 \\ge 0$ and $v_1, v_2 \\in \\mathbb{R}^m$ of unit norm. For a given index $r \\in \\{0,1,\\dots,n-1\\}$, let $X^{(-r)}$ denote the matrix obtained by removing the $r$-th row of $X$, and define its covariance $C^{(-r)}$ similarly with top two eigenpairs $(\\lambda_1', v_1')$ and $(\\lambda_2', v_2')$.\n\nFix two thresholds: an eigenvalue tie-tolerance $\\epsilon > 0$ and a direction-alignment threshold $\\tau \\in (0,1)$. Define that removing a single patient sample “completely swaps the order of $PC_1$ and $PC_2$” if and only if all of the following hold:\n- The leading eigenvalues are not tied in either analysis: $|\\lambda_1 - \\lambda_2| > \\epsilon$ and $|\\lambda_1' - \\lambda_2'| > \\epsilon$.\n- The leading directions exchange up to sign: $|v_1^\\top v_2'| \\ge \\tau$ and $|v_2^\\top v_1'| \\ge \\tau$.\n\nUse $\\epsilon = 1\\times 10^{-9}$ and $\\tau = 0.99$.\n\nConstruct a program that, for each test case below, computes whether the removal of the specified sample produces a complete swap according to the definition above, and outputs a list of booleans corresponding to the test cases.\n\nTest suite. Each case consists of a matrix $X^{(i)}$ with $n$ samples by $m$ genes, and a removal index $r^{(i)}$ using $0$-based indexing. All values are real numbers representing gene expression in arbitrary consistent units.\n\n- Case $1$ (designed swap with $m=2$ genes, $n=5$ patients): choose parameters $a=1.0$, $k=1.6$, $p=3.0$. Let\n$$\nX^{(1)} \\;=\\;\n\\begin{bmatrix}\n1.0 & 0.0 \\\\\n-1.0 & 0.0 \\\\\n0.0 & 1.6 \\\\\n0.0 & -1.6 \\\\\n3.0 & 0.0\n\\end{bmatrix},\n\\quad r^{(1)} = 4.\n$$\n\n- Case $2$ (no swap; smaller leverage, same structure): $a=1.0$, $k=1.6$, $p=1.0$.\n$$\nX^{(2)} \\;=\\;\n\\begin{bmatrix}\n1.0 & 0.0 \\\\\n-1.0 & 0.0 \\\\\n0.0 & 1.6 \\\\\n0.0 & -1.6 \\\\\n1.0 & 0.0\n\\end{bmatrix},\n\\quad r^{(2)} = 4.\n$$\n\n- Case $3$ (boundary tie; top eigenvalues equal before removal): $a=1.0$, $k=1.6$, and $p = \\sqrt{\\frac{5}{2}\\,\\big(k^2 - a^2\\big)}$.\n$$\nX^{(3)} \\;=\\;\n\\begin{bmatrix}\n1.0 & 0.0 \\\\\n-1.0 & 0.0 \\\\\n0.0 & 1.6 \\\\\n0.0 & -1.6 \\\\\n\\sqrt{\\tfrac{5}{2}\\,(1.6^2 - 1.0^2)} & 0.0\n\\end{bmatrix},\n\\quad r^{(3)} = 4.\n$$\n\n- Case $4$ (remove a non-leverage sample; no swap expected): reuse the matrix from Case $1$ and remove the first row.\n$$\nX^{(4)} \\;=\\;\n\\begin{bmatrix}\n1.0 & 0.0 \\\\\n-1.0 & 0.0 \\\\\n0.0 & 1.6 \\\\\n0.0 & -1.6 \\\\\n3.0 & 0.0\n\\end{bmatrix},\n\\quad r^{(4)} = 0.\n$$\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example $[b_1,b_2,b_3,b_4]$, where each $b_i$ is the boolean result for Case $i$ in order.", "solution": "The problem statement is subjected to validation.\n\n**Step 1: Extract Givens**\n- Data Matrix: $X \\in \\mathbb{R}^{n \\times m}$, with $n$ patient samples and $m$ genes.\n- Column-wise Centered Matrix: $\\tilde{X}$, obtained by subtracting the sample mean from each column of $X$.\n- Sample Covariance Matrix: $C = \\frac{1}{n-1}\\,\\tilde{X}^\\top \\tilde{X} \\in \\mathbb{R}^{m \\times m}$.\n- Eigenpairs of $C$: $(\\lambda_1, v_1)$ and $(\\lambda_2, v_2)$, where $\\lambda_1 \\ge \\lambda_2 \\ge 0$ and $v_1, v_2$ are unit vectors in $\\mathbb{R}^m$. These correspond to the first two principal components ($PC_1$, $PC_2$).\n- Leave-one-out Matrix: $X^{(-r)}$ is the matrix $X$ with its $r$-th row removed.\n- Leave-one-out Covariance and Eigenpairs: $C^{(-r)}$ is the covariance matrix of $X^{(-r)}$, with top two eigenpairs $(\\lambda_1', v_1')$ and $(\\lambda_2', v_2')$.\n- Eigenvalue Tie-Tolerance: $\\epsilon = 1 \\times 10^{-9}$.\n- Direction-Alignment Threshold: $\\tau = 0.99$.\n- Definition of \"complete swap\": The removal of sample $r$ causes a complete swap if and only if all four of the following conditions hold:\n    1. $|\\lambda_1 - \\lambda_2| > \\epsilon$ (eigenvalues are not tied in the original analysis).\n    2. $|\\lambda_1' - \\lambda_2'| > \\epsilon$ (eigenvalues are not tied in the leave-one-out analysis).\n    3. $|v_1^\\top v_2'| \\ge \\tau$ (original $PC_1$ direction aligns with new $PC_2$ direction).\n    4. $|v_2^\\top v_1'| \\ge \\tau$ (original $PC_2$ direction aligns with new $PC_1$ direction).\n- Test Cases:\n    - Case 1: $X^{(1)} = \\begin{bmatrix} 1.0 & 0.0 \\\\ -1.0 & 0.0 \\\\ 0.0 & 1.6 \\\\ 0.0 & -1.6 \\\\ 3.0 & 0.0 \\end{bmatrix}$, $r^{(1)} = 4$.\n    - Case 2: $X^{(2)} = \\begin{bmatrix} 1.0 & 0.0 \\\\ -1.0 & 0.0 \\\\ 0.0 & 1.6 \\\\ 0.0 & -1.6 \\\\ 1.0 & 0.0 \\end{bmatrix}$, $r^{(2)} = 4$.\n    - Case 3: $X^{(3)} = \\begin{bmatrix} 1.0 & 0.0 \\\\ -1.0 & 0.0 \\\\ 0.0 & 1.6 \\\\ 0.0 & -1.6 \\\\ \\sqrt{\\tfrac{5}{2}\\,(1.6^2 - 1.0^2)} & 0.0 \\end{bmatrix}$, $r^{(3)} = 4$.\n    - Case 4: $X^{(4)} = \\begin{bmatrix} 1.0 & 0.0 \\\\ -1.0 & 0.0 \\\\ 0.0 & 1.6 \\\\ 0.0 & -1.6 \\\\ 3.0 & 0.0 \\end{bmatrix}$, $r^{(4)} = 0$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Grounding**: The problem is well-grounded in statistical linear algebra and its application to bioinformatics, specifically Principal Component Analysis (PCA). The analysis of component stability via leave-one-out resampling is a standard and valid technique to assess the influence of individual data points. The definitions are mathematically rigorous.\n- **Well-Posedness**: The problem is well-posed. All necessary inputs (data matrices, removal indices, tolerance parameters) are provided. The criteria for a \"complete swap\" are defined unambiguously, ensuring a unique boolean outcome for each test case.\n- **Objectivity**: The problem is stated objectively using formal mathematical language, free from subjective or speculative content.\n\nThe problem does not exhibit any of the enumerated flaws (e.g., scientific unsoundness, incompleteness, ambiguity). It is a direct and formalizable computational task.\n\n**Step 3: Verdict and Action**\nThe problem is deemed **valid**. A solution will be furnished.\n\n**Solution Derivation**\nThe objective is to implement an algorithm that verifies, for each provided test case, whether the removal of a specific sample (row) from a data matrix causes a \"complete swap\" of the first two principal components ($PC_1$ and $PC_2$). The procedure for a single test case $(X, r)$ is as follows.\n\n1.  **Analyze the Full Data Matrix $X$**:\n    Let the given matrix be $X$ with $n$ rows and $m$ columns.\n    a. Compute the column means: $\\mu = \\frac{1}{n} \\sum_{i=1}^n x_{i,:}$, where $x_{i,:}$ is the $i$-th row of $X$.\n    b. Center the data: $\\tilde{X} = X - \\mathbf{1}_{n \\times 1} \\mu^\\top$, where $\\mathbf{1}_{n \\times 1}$ is a column vector of ones.\n    c. Compute the sample covariance matrix: $C = \\frac{1}{n-1} \\tilde{X}^\\top \\tilde{X}$.\n    d. Perform eigendecomposition of the symmetric matrix $C$. This yields a set of real eigenvalues and a corresponding orthogonal basis of eigenvectors.\n    e. Identify the two largest eigenvalues, $\\lambda_1 \\ge \\lambda_2$, and their corresponding unit-norm eigenvectors, $v_1$ and $v_2$.\n\n2.  **Analyze the Leave-One-Out Data Matrix $X^{(-r)}$**:\n    Let $X^{(-r)}$ be the matrix obtained by removing row $r$ from $X$. This matrix has $n' = n-1$ rows and $m$ columns.\n    a. Compute the new column means: $\\mu' = \\frac{1}{n'} \\sum_{i \\ne r} x_{i,:}$.\n    b. Center the reduced data: $\\tilde{X}^{(-r)} = X^{(-r)} - \\mathbf{1}_{(n-1) \\times 1} (\\mu')^\\top$.\n    c. Compute the new sample covariance matrix: $C^{(-r)} = \\frac{1}{n'-1} (\\tilde{X}^{(-r)})^\\top \\tilde{X}^{(-r)}$.\n    d. Perform eigendecomposition of $C^{(-r)}$ to find its two largest eigenvalues, $\\lambda_1' \\ge \\lambda_2'$, and corresponding unit-norm eigenvectors, $v_1'$ and $v_2'$.\n\n3.  **Evaluate the \"Complete Swap\" Conditions**:\n    Using the specified thresholds $\\epsilon = 1 \\times 10^{-9}$ and $\\tau = 0.99$, evaluate the four conditions:\n    a. Condition 1 (No tie in original data): Test if $|\\lambda_1 - \\lambda_2| > \\epsilon$.\n    b. Condition 2 (No tie in reduced data): Test if $|\\lambda_1' - \\lambda_2'| > \\epsilon$.\n    c. Condition 3 (Alignment of $PC_1$ and $PC_2'$): Test if $|v_1^\\top v_2'| \\ge \\tau$. The dot product $v_1^\\top v_2'$ measures the cosine of the angle between the two vector directions. The absolute value accounts for the arbitrary sign of eigenvectors.\n    d. Condition 4 (Alignment of $PC_2$ and $PC_1'$): Test if $|v_2^\\top v_1'| \\ge \\tau$.\n\n    A complete swap occurs if and only if all four conditions are true. The procedure will be applied to each of the four test cases provided. Numerical computation will rely on standard libraries for matrix operations and eigendecomposition, ensuring accuracy and adherence to established algorithms. Specifically, for a real symmetric matrix such as the covariance matrix, specialized algorithms exist (e.g., the Jacobi method or QR-based algorithms) which are numerically stable and efficient. The use of `numpy.linalg.eigh` is appropriate as it is designed for Hermitian (or real symmetric) matrices and guarantees real eigenvalues and a complete orthonormal set of eigenvectors, sorted by eigenvalue magnitude.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by analyzing PCA stability for each test case.\n    \"\"\"\n\n    def check_swap(X, r_idx, epsilon, tau):\n        \"\"\"\n        Checks if removing a sample causes a complete swap of PC1 and PC2.\n\n        Args:\n            X (np.ndarray): The data matrix (n_samples, n_features).\n            r_idx (int): The 0-based index of the row to remove.\n            epsilon (float): The eigenvalue tie-tolerance.\n            tau (float): The direction-alignment threshold.\n\n        Returns:\n            bool: True if a complete swap occurs, False otherwise.\n        \"\"\"\n\n        # --- PCA on the full dataset ---\n        n_samples, n_features = X.shape\n        if n_samples <= 1:\n            return False # Covariance is not well-defined.\n\n        # Center the data\n        X_centered = X - X.mean(axis=0)\n        # Compute the sample covariance matrix\n        cov_matrix = (X_centered.T @ X_centered) / (n_samples - 1)\n        \n        # Eigendecomposition of the symmetric covariance matrix\n        # np.linalg.eigh returns eigenvalues in ascending order\n        eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n        \n        # Sort eigenvalues and eigenvectors in descending order\n        sorted_indices = np.argsort(eigenvalues)[::-1]\n        eigenvalues = eigenvalues[sorted_indices]\n        eigenvectors = eigenvectors[:, sorted_indices]\n        \n        # Extract top two eigenpairs\n        lambda1, lambda2 = eigenvalues[0], eigenvalues[1]\n        v1, v2 = eigenvectors[:, 0], eigenvectors[:, 1]\n        \n        # --- PCA on the leave-one-out dataset ---\n        X_loo = np.delete(X, r_idx, axis=0)\n        n_samples_loo, _ = X_loo.shape\n        \n        if n_samples_loo <= 1:\n            return False\n\n        # Center the leave-one-out data\n        X_loo_centered = X_loo - X_loo.mean(axis=0)\n        # Compute the new sample covariance matrix\n        cov_matrix_loo = (X_loo_centered.T @ X_loo_centered) / (n_samples_loo - 1)\n        \n        # Eigendecomposition\n        eigenvalues_loo, eigenvectors_loo = np.linalg.eigh(cov_matrix_loo)\n        \n        # Sort in descending order\n        sorted_indices_loo = np.argsort(eigenvalues_loo)[::-1]\n        eigenvalues_loo = eigenvalues_loo[sorted_indices_loo]\n        eigenvectors_loo = eigenvectors_loo[:, sorted_indices_loo]\n        \n        # Extract top two eigenpairs\n        lambda1_p, lambda2_p = eigenvalues_loo[0], eigenvalues_loo[1]\n        v1_p, v2_p = eigenvectors_loo[:, 0], eigenvectors_loo[:, 1]\n\n        # --- Evaluate the four conditions for a complete swap ---\n        \n        # Condition 1: Eigenvalues are not tied in the original analysis\n        cond1 = abs(lambda1 - lambda2) > epsilon\n        \n        # Condition 2: Eigenvalues are not tied in the leave-one-out analysis\n        cond2 = abs(lambda1_p - lambda2_p) > epsilon\n        \n        # Condition 3: Original PC1 aligns with new PC2\n        cond3 = abs(np.dot(v1, v2_p)) >= tau\n        \n        # Condition 4: Original PC2 aligns with new PC1\n        cond4 = abs(np.dot(v2, v1_p)) >= tau\n        \n        return cond1 and cond2 and cond3 and cond4\n\n    # Define constants from the problem statement\n    epsilon = 1e-9\n    tau = 0.99\n\n    # Define the test cases from the problem statement\n    X1 = np.array([\n        [1.0, 0.0],\n        [-1.0, 0.0],\n        [0.0, 1.6],\n        [0.0, -1.6],\n        [3.0, 0.0]\n    ])\n    r1 = 4\n\n    X2 = np.array([\n        [1.0, 0.0],\n        [-1.0, 0.0],\n        [0.0, 1.6],\n        [0.0, -1.6],\n        [1.0, 0.0]\n    ])\n    r2 = 4\n\n    # Calculate p for Case 3\n    a = 1.0\n    k = 1.6\n    p_val = np.sqrt((5.0 / 2.0) * (k**2 - a**2))\n    X3 = np.array([\n        [1.0, 0.0],\n        [-1.0, 0.0],\n        [0.0, 1.6],\n        [0.0, -1.6],\n        [p_val, 0.0]\n    ])\n    r3 = 4\n\n    X4 = X1.copy() # Reuse matrix from Case 1\n    r4 = 0\n    \n    test_cases = [\n        (X1, r1),\n        (X2, r2),\n        (X3, r3),\n        (X4, r4),\n    ]\n\n    results = []\n    for X, r_idx in test_cases:\n        is_swap = check_swap(X, r_idx, epsilon, tau)\n        # Use lowercase for standard boolean representation (e.g., JSON)\n        results.append(str(is_swap).lower())\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2416110"}]}