{"hands_on_practices": [{"introduction": "To begin our hands-on exploration of simple linear regression, we will start with the most fundamental task: calculating the model's slope parameter. In the context of a bioprocess engineering scenario, this exercise will guide you through estimating the biomass yield coefficient from experimental data [@problem_id:2429490]. By applying the principle of Ordinary Least Squares (OLS), you will derive the slope that best captures the linear relationship between initial nutrient supply and final biomass, a core skill for modeling biological systems.", "problem": "A set of independent batch culture experiments investigated how final biomass depends on the initial glucose supplied. For each experiment, the initial glucose concentration $S_0$ (in $\\mathrm{g/L}$) and the final biomass concentration $X_f$ (in grams of Dry Weight (gDW) per liter, $\\mathrm{gDW/L}$) were measured after glucose was fully consumed. Assume a linear relationship with additive disturbance, modeled as $X_f = \\beta_0 + \\beta_1 S_0 + \\varepsilon$, where $\\varepsilon$ is a zero-mean disturbance, and where the slope $\\beta_1$ equals the biomass yield coefficient $Y_{X/S}$ in $\\mathrm{gDW/g}$ and the intercept $\\beta_0$ represents pre-existing biomass not attributable to the supplied glucose.\n\nFor each dataset below, determine the slope parameter $\\beta_1$ of the line that minimizes the sum of squared residuals between the observed $X_f$ and the model prediction $\\beta_0 + \\beta_1 S_0$. Express each slope in $\\mathrm{gDW/g}$ and round each to exactly six digits after the decimal point. Do not output units, only the numbers.\n\nTest suite (each dataset is given as a set of pairs $(S_0, X_f)$ with $S_0$ in $\\mathrm{g/L}$ and $X_f$ in $\\mathrm{gDW/L}$):\n- Dataset A (general case):\n$$\\{(0, 0.07), (2, 1.04), (4, 2.05), (6, 3.06), (8, 4.03)\\}$$\n- Dataset B (noise-free proportional relationship with zero intercept):\n$$\\{(0, 0.00), (1, 0.45), (2, 0.90), (3, 1.35), (4, 1.80)\\}$$\n- Dataset C (narrow input range with small measurement noise):\n$$\\{(1.9, 1.068), (2.0, 1.116), (2.1, 1.177), (2.2, 1.229), (2.3, 1.285)\\}$$\n- Dataset D (boundary case with the minimal sample size of two distinct points):\n$$\\{(5.0, 2.50), (10.0, 4.90)\\}$$\n\nYour program should produce a single line of output containing the slopes for Datasets A, B, C, and D, respectively, as a comma-separated list enclosed in square brackets, with no spaces, for example, $[a,b,c,d]$, where $a$, $b$, $c$, and $d$ are the computed slopes formatted to six digits after the decimal point.", "solution": "The problem statement has been rigorously validated and is found to be scientifically sound, well-posed, objective, and self-contained. It presents a standard task in data analysis, specifically simple linear regression, applied to a realistic scenario in computational biology. The required data is provided, and the objective is unambiguous. Therefore, a complete solution will be provided.\n\nThe problem requires the determination of the slope parameter, $\\beta_1$, for a simple linear regression model of the form $X_f = \\beta_0 + \\beta_1 S_0 + \\varepsilon$. This is to be done for four separate datasets. The method specified is the minimization of the sum of squared residuals, which is the principle of Ordinary Least Squares (OLS).\n\nLet the independent variable be $x_i = S_{0,i}$ and the dependent variable be $y_i = X_{f,i}$ for a set of $n$ observations. The linear model is $\\hat{y}_i = \\beta_0 + \\beta_1 x_i$, where $\\hat{y}_i$ is the predicted value of $y_i$. The objective is to find the parameters $\\beta_0$ and $\\beta_1$ that minimize the sum of squared residuals ($SSR$):\n$$SSR(\\beta_0, \\beta_1) = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\beta_1 x_i))^2$$\nTo find the minimum of this function, we must compute the partial derivatives with respect to $\\beta_0$ and $\\beta_1$ and set them to zero. This gives us the normal equations.\n\nThe partial derivative with respect to $\\beta_0$ is:\n$$\\frac{\\partial SSR}{\\partial \\beta_0} = \\sum_{i=1}^{n} 2(y_i - \\beta_0 - \\beta_1 x_i)(-1) = -2 \\left( \\sum_{i=1}^{n} y_i - n\\beta_0 - \\beta_1 \\sum_{i=1}^{n} x_i \\right) = 0$$\nThis simplifies to the first normal equation:\n$$n\\beta_0 + \\beta_1 \\sum_{i=1}^{n} x_i = \\sum_{i=1}^{n} y_i$$\n\nThe partial derivative with respect to $\\beta_1$ is:\n$$\\frac{\\partial SSR}{\\partial \\beta_1} = \\sum_{i=1}^{n} 2(y_i - \\beta_0 - \\beta_1 x_i)(-x_i) = -2 \\left( \\sum_{i=1}^{n} x_i y_i - \\beta_0 \\sum_{i=1}^{n} x_i - \\beta_1 \\sum_{i=1}^{n} x_i^2 \\right) = 0$$\nThis simplifies to the second normal equation:\n$$\\beta_0 \\sum_{i=1}^{n} x_i + \\beta_1 \\sum_{i=1}^{n} x_i^2 = \\sum_{i=1}^{n} x_i y_i$$\n\nWe now have a system of two linear equations for the two unknowns, $\\beta_0$ and $\\beta_1$. The problem specifically requests the slope, $\\beta_1$. We can solve this system for $\\beta_1$. From the first normal equation, we express $\\beta_0$ in terms of the sample means $\\bar{x} = \\frac{1}{n}\\sum x_i$ and $\\bar{y} = \\frac{1}{n}\\sum y_i$:\n$$\\beta_0 = \\frac{1}{n} \\left( \\sum_{i=1}^{n} y_i - \\beta_1 \\sum_{i=1}^{n} x_i \\right) = \\bar{y} - \\beta_1 \\bar{x}$$\nSubstituting this expression for $\\beta_0$ into the second normal equation yields:\n$$(\\bar{y} - \\beta_1 \\bar{x}) \\sum_{i=1}^{n} x_i + \\beta_1 \\sum_{i=1}^{n} x_i^2 = \\sum_{i=1}^{n} x_i y_i$$\nSolving for $\\beta_1$:\n$$\\beta_1 \\left( \\sum_{i=1}^{n} x_i^2 - \\bar{x} \\sum_{i=1}^{n} x_i \\right) = \\sum_{i=1}^{n} x_i y_i - \\bar{y} \\sum_{i=1}^{n} x_i$$\n$$\\beta_1 = \\frac{\\sum_{i=1}^{n} x_i y_i - \\bar{y} \\sum_{i=1}^{n} x_i}{\\sum_{i=1}^{n} x_i^2 - \\bar{x} \\sum_{i=1}^{n} x_i} = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2}$$\nA common computational formula derived from this is:\n$$\\beta_1 = \\frac{n \\sum_{i=1}^{n} x_i y_i - (\\sum_{i=1}^{n} x_i)(\\sum_{i=1}^{n} y_i)}{n \\sum_{i=1}^{n} x_i^2 - (\\sum_{i=1}^{n} x_i)^2}$$\nThis formula will be applied to each dataset.\n\n**Dataset A:**\n$x = \\{0, 2, 4, 6, 8\\}$, $y = \\{0.07, 1.04, 2.05, 3.06, 4.03\\}$. Here, $n=5$.\n$\\sum x_i = 20$, $\\sum y_i = 10.25$, $\\sum x_i^2 = 120$, $\\sum x_i y_i = 60.88$.\n$$\\beta_1 = \\frac{5(60.88) - (20)(10.25)}{5(120) - (20)^2} = \\frac{304.4 - 205}{600 - 400} = \\frac{99.4}{200} = 0.497$$\n\n**Dataset B:**\n$x = \\{0, 1, 2, 3, 4\\}$, $y = \\{0.00, 0.45, 0.90, 1.35, 1.80\\}$. Here, $n=5$.\n$\\sum x_i = 10$, $\\sum y_i = 4.5$, $\\sum x_i^2 = 30$, $\\sum x_i y_i = 13.5$.\n$$\\beta_1 = \\frac{5(13.5) - (10)(4.5)}{5(30) - (10)^2} = \\frac{67.5 - 45}{150 - 100} = \\frac{22.5}{50} = 0.45$$\n\n**Dataset C:**\n$x = \\{1.9, 2.0, 2.1, 2.2, 2.3\\}$, $y = \\{1.068, 1.116, 1.177, 1.229, 1.285\\}$. Here, $n=5$.\n$\\sum x_i = 10.5$, $\\sum y_i = 5.875$, $\\sum x_i^2 = 22.15$, $\\sum x_i y_i = 12.3922$.\n$$\\beta_1 = \\frac{5(12.3922) - (10.5)(5.875)}{5(22.15) - (10.5)^2} = \\frac{61.961 - 61.6875}{110.75 - 110.25} = \\frac{0.2735}{0.5} = 0.547$$\n\n**Dataset D:**\n$x = \\{5.0, 10.0\\}$, $y = \\{2.50, 4.90\\}$. Here, $n=2$.\nFor two points, the OLS slope is simply the slope of the line connecting them:\n$$\\beta_1 = \\frac{y_2 - y_1}{x_2 - x_1} = \\frac{4.90 - 2.50}{10.0 - 5.0} = \\frac{2.4}{5.0} = 0.48$$\nUsing the general formula:\n$\\sum x_i = 15.0$, $\\sum y_i = 7.40$, $\\sum x_i^2 = 125.0$, $\\sum x_i y_i = 61.5$.\n$$\\beta_1 = \\frac{2(61.5) - (15.0)(7.40)}{2(125.0) - (15.0)^2} = \\frac{123 - 111}{250 - 225} = \\frac{12}{25} = 0.48$$\n\nThe calculated slopes for datasets A, B, C, and D are $0.497$, $0.45$, $0.547$, and $0.48$, respectively. These must be formatted to six decimal places.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import linregress\n\ndef solve():\n    \"\"\"\n    Calculates the slope of a simple linear regression for several datasets\n    representing biomass yield experiments.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple of two lists: (S0 values, Xf values).\n    test_cases = [\n        # Dataset A (general case)\n        ([0, 2, 4, 6, 8], [0.07, 1.04, 2.05, 3.06, 4.03]),\n        # Dataset B (noise-free proportional relationship with zero intercept)\n        ([0, 1, 2, 3, 4], [0.00, 0.45, 0.90, 1.35, 1.80]),\n        # Dataset C (narrow input range with small measurement noise)\n        ([1.9, 2.0, 2.1, 2.2, 2.3], [1.068, 1.116, 1.177, 1.229, 1.285]),\n        # Dataset D (boundary case with the minimal sample size of two distinct points)\n        ([5.0, 10.0], [2.50, 4.90]),\n    ]\n\n    results = []\n    for s0_values, xf_values in test_cases:\n        # The problem asks to find the slope parameter (beta_1) of the line\n        # that minimizes the sum of squared residuals. This is precisely what\n        # scipy.stats.linregress calculates.\n        \n        # Convert lists to numpy arrays for the regression function.\n        x = np.array(s0_values)\n        y = np.array(xf_values)\n        \n        # Perform the linear regression.\n        # The result object contains 'slope', 'intercept', 'rvalue', 'pvalue', 'stderr'.\n        # We only need the slope.\n        regression_result = linregress(x, y)\n        slope = regression_result.slope\n        \n        # Format the result to exactly six digits after the decimal point.\n        formatted_slope = f\"{slope:.6f}\"\n        results.append(formatted_slope)\n\n    # Final print statement in the exact required format: [a,b,c,d]\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2429490"}, {"introduction": "Beyond simply fitting a line, a crucial part of data analysis is understanding the reliability and stability of your findings. This practice delves into the concept of influential data points and their dramatic effect on statistical significance [@problem_id:2429452]. By observing how a single, high-leverage data point can push a slope's $p$-value across the conventional significance threshold of $0.05$, you will gain a profound appreciation for the importance of diagnostic plots and outlier detection in any regression analysis.", "problem": "You are modeling a linear association in a pharmacogenomic analysis between messenger ribonucleic acid (mRNA) expression and a downstream quantitative response. Consider the following simplified, fully specified scenario that isolates the mathematics of ordinary least squares linear regression while remaining scientifically realistic for computational biology and bioinformatics.\n\nContext and data generation:\n- Predictor $x$ represents $\\log_2$-transformed gene expression (in $\\log_2(\\text{Transcripts Per Million})$, dimensionless). The base sample contains $n = 10$ cell lines with predictor values\n  $$\n  x = (-4.5,-3.5,-2.5,-1.5,-0.5,0.5,1.5,2.5,3.5,4.5).\n  $$\n- Response $y$ represents a standardized protein abundance $z$-score (dimensionless, thus allowed to be negative). To ensure a controlled, reproducible signal-to-noise ratio that produces a borderline association, define\n  $$\n  z_i = x_i^2 - \\overline{x^2}, \\quad \\text{where} \\quad \\overline{x^2} = \\frac{1}{n}\\sum_{i=1}^n x_i^2,\n  $$\n  and form the base response as\n  $$\n  y_i = x_i + \\beta z_i \\quad \\text{with} \\quad \\beta = 0.5175.\n  $$\n  This construction yields a linear trend $y \\approx x$ with structured, orthogonal noise (since $\\sum_{i=1}^n x_i z_i = 0$), producing a slope that is borderline significant under the classical normal-error model.\n\nStatistical model and inference target:\n- Assume the standard simple linear regression model with intercept,\n  $$\n  y_i = \\alpha + \\beta_1 x_i + \\varepsilon_i,\n  $$\n  where the errors $\\varepsilon_i$ are independent and identically distributed as Gaussian with mean $0$ and variance $\\sigma^2$ (homoscedastic normal errors). Under this model, the ordinary least squares slope estimator $\\widehat{\\beta}_1$ has a Student’s $t$-statistic whose two-sided $p$-value quantifies evidence against the null hypothesis $H_0:\\beta_1=0$.\n\nTask:\n- Implement from first principles the ordinary least squares fit with an intercept to compute, for each specified dataset, the two-sided $p$-value for the slope under the homoscedastic normal-error model. You must derive the $t$-statistic using the residual variance estimate and the $(n-2)$ degrees of freedom, and then compute the two-sided $p$-value via the cumulative distribution function of the Student’s $t$-distribution.\n\nTest suite:\nCompute and report the two-sided $p$-values for the slope for the following three datasets:\n1. Base dataset ($n=10$): predictor $x$ as above; response $y$ defined by $y_i = x_i + \\beta z_i$ with $\\beta = 0.5175$ and $z_i = x_i^2 - \\overline{x^2}$.\n2. Base dataset plus one influential point aligned with the trend ($n=11$): add the point $(x_{\\text{new}}, y_{\\text{new}}) = (6.0, 6.0)$ to the base dataset. This emulates a high-leverage sample consistent with the hypothesized biological relationship.\n3. Base dataset plus one influential point opposing the trend ($n=11$): add the point $(x_{\\text{new}}, y_{\\text{new}}) = (6.0, -6.0)$ to the base dataset. This emulates a high-leverage sample that contradicts the hypothesized relationship.\n\nScientific objective to demonstrate:\n- Show concretely how adding one influential data point can change the two-sided slope $p$-value from a borderline value near $0.06$ to a value near $0.04$ (crossing the conventional $0.05$ threshold), as well as how a single contradictory influential point can move the $p$-value in the opposite direction.\n\nAnswer formatting:\n- For each dataset, output the two-sided $p$-value for the slope as a decimal rounded to two decimal places (unitless).\n- Your program should produce a single line of output containing the three results as a comma-separated list enclosed in square brackets (e.g., $[0.06,0.04,0.23]$).", "solution": "The problem statement has been subjected to validation and is deemed valid. It is scientifically grounded, well-posed, objective, and contains all necessary information to derive a unique, meaningful solution using established principles of ordinary least squares linear regression. The provided context and data generation scheme, while synthetic, are constructed to demonstrate a specific, verifiable statistical phenomenon relevant to computational biology and bioinformatics.\n\nThe task is to compute the two-sided $p$-value for the slope coefficient in a simple linear regression model for three distinct datasets. The analysis will be conducted from first principles, consistent with the classical normal-error model.\n\nThe simple linear regression model is given by:\n$$\ny_i = \\alpha + \\beta_1 x_i + \\varepsilon_i\n$$\nwhere $y_i$ is the response variable, $x_i$ is the predictor variable, $\\alpha$ is the intercept, $\\beta_1$ is the slope, and $\\varepsilon_i$ are independent and identically distributed random errors from a normal distribution with mean $0$ and variance $\\sigma^2$, i.e., $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$.\n\nThe ordinary least squares (OLS) method minimizes the residual sum of squares, $\\text{RSS} = \\sum_{i=1}^n (y_i - (\\widehat{\\alpha} + \\widehat{\\beta}_1 x_i))^2$. The OLS estimators for the slope, $\\widehat{\\beta}_1$, and intercept, $\\widehat{\\alpha}$, are given by:\n$$\n\\widehat{\\beta}_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2}\n$$\n$$\n\\widehat{\\alpha} = \\bar{y} - \\widehat{\\beta}_1 \\bar{x}\n$$\nwhere $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i$ and $\\bar{y} = \\frac{1}{n}\\sum_{i=1}^n y_i$ are the sample means.\n\nTo perform hypothesis testing on the slope, we must first estimate the variance of the errors, $\\sigma^2$. The unbiased estimator for $\\sigma^2$ is the mean squared error (or residual variance), $\\widehat{\\sigma}^2$, calculated from the residuals $e_i = y_i - (\\widehat{\\alpha} + \\widehat{\\beta}_1 x_i)$:\n$$\n\\widehat{\\sigma}^2 = \\frac{\\sum_{i=1}^n e_i^2}{n-2} = \\frac{\\text{RSS}}{n-2}\n$$\nThe denominator, $n-2$, represents the degrees of freedom, as two parameters ($\\alpha$ and $\\beta_1$) are estimated from the data.\n\nThe standard error of the slope estimator, $\\text{SE}(\\widehat{\\beta}_1)$, quantifies its sampling variability:\n$$\n\\text{SE}(\\widehat{\\beta}_1) = \\sqrt{\\frac{\\widehat{\\sigma}^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2}}\n$$\n\nThe primary task is to test the null hypothesis $H_0: \\beta_1 = 0$ against the alternative $H_a: \\beta_1 \\neq 0$. This is accomplished using a $t$-statistic, which, under the null hypothesis, follows a Student's $t$-distribution with $n-2$ degrees of freedom. The $t$-statistic is calculated as:\n$$\nt = \\frac{\\widehat{\\beta}_1 - 0}{\\text{SE}(\\widehat{\\beta}_1)} = \\frac{\\widehat{\\beta}_1}{\\text{SE}(\\widehat{\\beta}_1)}\n$$\n\nThe two-sided $p$-value is the probability of observing a test statistic at least as extreme as the one computed, assuming the null hypothesis is true. It is given by:\n$$\np = 2 \\times P(T_{n-2} \\ge |t|)\n$$\nwhere $T_{n-2}$ is a random variable following the Student's $t$-distribution with $n-2$ degrees of freedom. This probability can be computed using the cumulative distribution function (CDF), $F_{n-2}$, of the $t$-distribution:\n$$\np = 2 \\times (1 - F_{n-2}(|t|))\n$$\n\nThe computational procedure for each dataset is as follows:\n1.  For a given set of $n$ data points $(x_i, y_i)$, compute the sample means $\\bar{x}$ and $\\bar{y}$.\n2.  Calculate the OLS slope estimate $\\widehat{\\beta}_1$ and intercept estimate $\\widehat{\\alpha}$.\n3.  Determine the fitted values $\\widehat{y}_i = \\widehat{\\alpha} + \\widehat{\\beta}_1 x_i$ and the residuals $e_i = y_i - \\widehat{y}_i$.\n4.  Compute the residual sum of squares $\\text{RSS} = \\sum_{i=1}^n e_i^2$ and the residual variance $\\widehat{\\sigma}^2 = \\text{RSS} / (n-2)$.\n5.  Compute the standard error of the slope $\\text{SE}(\\widehat{\\beta}_1)$.\n6.  Calculate the $t$-statistic for the slope.\n7.  Determine the two-sided $p$-value using the Student's $t$-distribution with $n-2$ degrees of freedom.\n\nThis procedure will be applied to the three datasets specified.\n\nDataset 1 (Base dataset):\n-   $n = 10$.\n-   Predictor $x = (-4.5, -3.5, -2.5, -1.5, -0.5, 0.5, 1.5, 2.5, 3.5, 4.5)$.\n-   Response $y_i = x_i + \\beta z_i$, with $\\beta = 0.5175$ and $z_i = x_i^2 - \\overline{x^2}$.\n\nDataset 2 (Aligned influential point):\n-   $n = 11$.\n-   Data consists of the base dataset plus the point $(x_{\\text{new}}, y_{\\text{new}}) = (6.0, 6.0)$.\n\nDataset 3 (Opposing influential point):\n-   $n = 11$.\n-   Data consists of the base dataset plus the point $(x_{\\text{new}}, y_{\\text{new}}) = (6.0, -6.0)$.\n\nThe implementation will use numerical libraries to perform these calculations robustly and derive the required $p$-values. The final results demonstrate the sensitivity of statistical significance to high-leverage data points, a critical concept in data analysis.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t\n\ndef solve():\n    \"\"\"\n    Solves the simple linear regression problem for three specified datasets.\n    \"\"\"\n\n    def calculate_p_value(x: np.ndarray, y: np.ndarray) - float:\n        \"\"\"\n        Calculates the two-sided p-value for the slope of a simple linear regression\n        from first principles.\n        \n        Args:\n            x: A numpy array of predictor values.\n            y: A numpy array of response values.\n            \n        Returns:\n            The two-sided p-value for the OLS slope coefficient.\n        \"\"\"\n        # 1. Get sample size n\n        n = len(x)\n        if n = 2:\n            return np.nan # p-value is undefined for n = 2\n\n        # 2. Calculate sample means\n        x_bar = np.mean(x)\n        y_bar = np.mean(y)\n\n        # 3. Calculate OLS slope estimate (beta_1_hat)\n        # Numerator: sum of products of deviations\n        sp_xy = np.sum((x - x_bar) * (y - y_bar))\n        # Denominator: sum of squared deviations for x\n        ss_x = np.sum((x - x_bar)**2)\n        \n        beta_1_hat = sp_xy / ss_x\n        \n        # 4. Calculate OLS intercept estimate (alpha_hat)\n        alpha_hat = y_bar - beta_1_hat * x_bar\n        \n        # 5. Calculate fitted values and residuals\n        y_hat = alpha_hat + beta_1_hat * x\n        residuals = y - y_hat\n        \n        # 6. Calculate Residual Sum of Squares (RSS) and degrees of freedom\n        rss = np.sum(residuals**2)\n        df = n - 2\n        \n        # 7. Calculate residual variance (sigma_hat_squared)\n        sigma2_hat = rss / df\n        \n        # 8. Calculate standard error of the slope\n        se_beta_1 = np.sqrt(sigma2_hat / ss_x)\n        \n        # 9. Calculate the t-statistic\n        if se_beta_1 == 0:\n            return 0.0 if beta_1_hat != 0 else 1.0\n        t_stat = beta_1_hat / se_beta_1\n        \n        # 10. Calculate the two-sided p-value\n        # Using survival function (1 - CDF) for better precision in the tail\n        p_value = 2 * t.sf(np.abs(t_stat), df)\n        \n        return p_value\n\n    # --- Define base data as per the problem ---\n    x_base = np.array([-4.5, -3.5, -2.5, -1.5, -0.5, 0.5, 1.5, 2.5, 3.5, 4.5])\n    n_base = len(x_base)\n    beta_gen = 0.5175\n    \n    # Generate the base response y\n    x_sq_bar = np.mean(x_base**2)\n    z = x_base**2 - x_sq_bar\n    y_base = x_base + beta_gen * z\n\n    # --- Define the three test cases ---\n    test_cases = [\n        # Case 1: Base dataset\n        (x_base, y_base),\n        # Case 2: Base dataset + aligned influential point\n        (np.append(x_base, 6.0), np.append(y_base, 6.0)),\n        # Case 3: Base dataset + opposing influential point\n        (np.append(x_base, 6.0), np.append(y_base, -6.0))\n    ]\n\n    results = []\n    for x_data, y_data in test_cases:\n        p_val = calculate_p_value(x_data, y_data)\n        # Format to two decimal places as a string\n        results.append(f\"{p_val:.2f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2429452"}, {"introduction": "Classical linear regression relies on several key assumptions, such as the normality of residuals, which may not always hold true with real-world biological data. This exercise introduces a powerful and versatile alternative for quantifying uncertainty: the nonparametric bootstrap [@problem_id:2429424]. You will implement this resampling method to construct a confidence interval for the slope relating GC content to gene expression, learning a robust computational technique that is indispensable in modern bioinformatics for its flexibility and fewer underlying assumptions.", "problem": "You are given multiple independent collections of paired observations representing guanine–cytosine content (GC content) and normalized gene expression level for distinct protein-coding genes from a single organism. For each collection, consider the simple linear regression model $y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$, where $x_i$ denotes the GC content (expressed as a fraction in the interval $[0,1]$), $y_i$ denotes the normalized expression level on a log base $2$ Transcripts Per Million (TPM) scale, $\\beta_0$ is the intercept, $\\beta_1$ is the slope, and $\\varepsilon_i$ is an unobserved residual for observation $i$. The slope $\\beta_1$ is defined as the ordinary least squares coefficient that minimizes the sum of squared residuals under this model.\n\nYour task is to use nonparametric bootstrap resampling of paired observations to construct a two-sided confidence interval for the slope $\\beta_1$ at confidence level $0.95$ for each collection. For each collection, treat the observed pairs $\\{(x_i,y_i)\\}_{i=1}^n$ as defining the empirical distribution that places probability mass $1/n$ on each observed pair, draw $B$ bootstrap samples of size $n$ with replacement from these pairs, compute the slope estimate for each bootstrap sample, and then report the percentile interval with lower endpoint at probability $0.025$ and upper endpoint at probability $0.975$ of the bootstrap slope distribution. Use the provided pseudorandom seed for reproducibility for each collection.\n\nTest suite (four distinct collections). In each case, $x$ denotes GC content (fraction) and $y$ denotes log base $2$ TPM. For every case, report the two endpoints of the $0.95$ confidence interval for $\\beta_1$.\n\n- Case $1$ (general increasing association):\n  - $x$: $0.34, 0.36, 0.40, 0.41, 0.43, 0.45, 0.48, 0.50, 0.52, 0.55, 0.57, 0.60$\n  - $y$: $4.10, 4.00, 4.30, 4.50, 4.60, 4.90, 5.00, 5.30, 5.40, 5.80, 6.00, 6.20$\n  - Confidence level: $0.95$; bootstrap replicates: $B=8000$; seed: $13579$.\n\n- Case $2$ (near-zero association):\n  - $x$: $0.31, 0.33, 0.35, 0.38, 0.40, 0.42, 0.44, 0.47, 0.49, 0.51, 0.53, 0.56$\n  - $y$: $5.10, 4.95, 5.05, 5.00, 5.02, 4.98, 5.01, 5.04, 4.96, 5.03, 4.97, 5.00$\n  - Confidence level: $0.95$; bootstrap replicates: $B=8000$; seed: $24680$.\n\n- Case $3$ (general decreasing association):\n  - $x$: $0.30, 0.34, 0.37, 0.39, 0.41, 0.44, 0.46, 0.49, 0.51, 0.54, 0.58, 0.61$\n  - $y$: $7.00, 6.80, 6.60, 6.50, 6.30, 6.10, 6.00, 5.90, 5.70, 5.50, 5.30, 5.20$\n  - Confidence level: $0.95$; bootstrap replicates: $B=8000$; seed: $11235$.\n\n- Case $4$ (smaller sample, increasing association):\n  - $x$: $0.32, 0.36, 0.40, 0.45, 0.50, 0.54, 0.58, 0.62$\n  - $y$: $3.80, 3.95, 4.10, 4.30, 4.45, 4.60, 4.80, 4.95$\n  - Confidence level: $0.95$; bootstrap replicates: $B=8000$; seed: $98765$.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to a test case in the order above and is itself a two-element list holding the lower and upper endpoints of the confidence interval. Each numeric endpoint must be rounded to six decimal places using standard rounding. For example, a valid output shape is $[[\\ell_1,u_1],[\\ell_2,u_2],[\\ell_3,u_3],[\\ell_4,u_4]]$, where $\\ell_k$ and $u_k$ denote the lower and upper endpoints for case $k$, respectively.", "solution": "The problem statement is subjected to rigorous validation before any attempt at a solution.\n\n**Step 1: Extraction of Givens**\n- **Model**: Simple linear regression of the form $y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$.\n- **Independent Variable ($x_i$)**: Guanine–cytosine content (GC content), a fraction in $[0,1]$.\n- **Dependent Variable ($y_i$)**: Normalized gene expression level, on a log base $2$ Transcripts Per Million (TPM) scale.\n- **Parameter of Interest**: The slope coefficient $\\beta_1$, estimated using Ordinary Least Squares (OLS).\n- **Task**: For each of four independent collections of paired observations, construct a two-sided confidence interval for $\\beta_1$ at a confidence level of $0.95$.\n- **Methodology**: Nonparametric bootstrap resampling of paired observations $\\{(x_i, y_i)\\}$. The confidence interval is to be the percentile interval.\n- **Procedure**:\n    1. For each collection of $n$ pairs, draw $B$ bootstrap samples of size $n$ with replacement.\n    2. Compute the OLS slope estimate $\\hat{\\beta}_1^*$ for each bootstrap sample.\n    3. Construct the confidence interval from the quantiles of the resulting bootstrap distribution of slopes. The lower endpoint is the $0.025$ quantile and the upper endpoint is the $0.975$ quantile.\n- **Case-Specific Parameters**:\n    - **Case 1**: Data arrays $x$ and $y$ are provided. $B=8000$ bootstrap replicates. Seed for pseudorandom number generation is $13579$.\n    - **Case 2**: Data arrays $x$ and $y$ are provided. $B=8000$ bootstrap replicates. Seed is $24680$.\n    - **Case 3**: Data arrays $x$ and $y$ are provided. $B=8000$ bootstrap replicates. Seed is $11235$.\n    - **Case 4**: Data arrays $x$ and $y$ are provided. $B=8000$ bootstrap replicates. Seed is $98765$.\n- **Output Format**: A single line containing a list of two-element lists, `[[\\ell_1,u_1],[\\ell_2,u_2],[\\ell_3,u_3],[\\ell_4,u_4]]`, where $\\ell_k$ and $u_k$ are the lower and upper endpoints for case $k$, rounded to six decimal places.\n\n**Step 2: Validation**\n- **Scientific Grounding**: The problem is scientifically well-grounded. The study of the relationship between genomic features like GC content and gene expression is a fundamental topic in computational biology and bioinformatics. Simple linear regression and nonparametric bootstrapping are standard, robust statistical methods used for such analyses. The problem is free from pseudoscience.\n- **Well-Posedness**: The problem is well-posed. It clearly defines the inputs (data, parameters), the statistical procedure (OLS, percentile bootstrap), and the required output format. The provision of seeds ensures a unique and reproducible solution.\n- **Objectivity**: The problem is stated in precise, objective, and quantitative terms. There are no subjective or ambiguous statements.\n- **Completeness and Consistency**: The problem is self-contained. All necessary data, models, and parameters (sample size $n$, number of replicates $B$, confidence level $1-\\alpha$, and random seeds) are explicitly provided for each case. There are no internal contradictions.\n\n**Step 3: Verdict**\nThe problem is deemed **valid**. It is a standard statistical task applied to a realistic biological context, is rigorously defined, and contains all necessary information for its solution. We may therefore proceed.\n\n**Solution Methodology**\n\nThe objective is to construct a $95\\%$ confidence interval for the slope parameter $\\beta_1$ of a simple linear regression model. The method specified is the nonparametric bootstrap, which makes no strong assumptions about the distribution of the error term $\\varepsilon_i$.\n\nThe ordinary least squares (OLS) estimate for the slope, $\\hat{\\beta}_1$, is the statistic of interest. For a sample of $n$ paired observations $\\{(x_i, y_i)\\}_{i=1}^n$, the OLS estimator $\\hat{\\beta}_1$ that minimizes the sum of squared residuals $\\sum_{i=1}^n (y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i))^2$ is given by the formula:\n$$\n\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} = \\frac{\\text{Cov}(x, y)}{\\text{Var}(x)}\n$$\nwhere $\\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i$ and $\\bar{y} = \\frac{1}{n} \\sum_{i=1}^n y_i$ are the sample means of the independent and dependent variables, respectively.\n\nThe core of the solution is the nonparametric bootstrap algorithm, which we apply to each of the four cases:\n$1$. The original dataset of $n$ pairs, $D = \\{(x_1, y_1), \\dots, (x_n, y_n)\\}$, is treated as the empirical distribution function, where each pair has a probability of $1/n$ of being selected.\n$2$. A pseudorandom number generator is initialized with the specified seed for reproducibility.\n$3$. A total of $B=8000$ bootstrap samples are generated. Each bootstrap sample, $D^*_j$ for $j \\in \\{1, \\dots, B\\}$, is created by drawing $n$ pairs with replacement from the original dataset $D$.\n$4$. For each bootstrap sample $D^*_j$, the OLS slope estimate, denoted $\\hat{\\beta}_{1,j}^*$, is calculated using the formula above with the data from $D^*_j$.\n$5$. This process yields a collection of $B$ bootstrap slope estimates, $\\{\\hat{\\beta}_{1,1}^*, \\hat{\\beta}_{1,2}^*, \\dots, \\hat{\\beta}_{1,B}^*\\}$. This collection serves as an empirical approximation of the sampling distribution of the estimator $\\hat{\\beta}_1$.\n\nTo construct the confidence interval, we use the percentile method. For a confidence level of $1-\\alpha = 0.95$, we have $\\alpha=0.05$. The lower and upper bounds of the confidence interval are determined by the $\\alpha/2 = 0.025$ and $1-\\alpha/2 = 0.975$ quantiles of the sorted bootstrap distribution of slopes.\n- Let the sorted list of bootstrap slopes be $\\hat{\\beta}_{1,(1)}^* \\leq \\hat{\\beta}_{1,(2)}^* \\leq \\dots \\leq \\hat{\\beta}_{1,(B)}^*$.\n- The lower endpoint $\\ell$ is the value at the $B \\times (\\alpha/2)$ position of this sorted list.\n- The upper endpoint $u$ is the value at the $B \\times (1-\\alpha/2)$ position.\n\nThe implementation will be carried out in Python using the `numpy` library. For each case, we will:\n- Define the data arrays $x$ and $y$.\n- Instantiate a random number generator with the given seed.\n- Loop $B=8000$ times. In each iteration, generate bootstrap indices, select the corresponding pairs to form a bootstrap sample, and compute the slope $\\hat{\\beta}_1^*$.\n- After the loop, `numpy.quantile` will be used to efficiently compute the $0.025$ and $0.975$ quantiles of the collection of $8000$ slope estimates.\n- The resulting endpoints will be rounded to six decimal places as required. The final output will be formatted into a single string representing a list of lists.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes 95% bootstrap confidence intervals for the slope of a simple linear regression model\n    for four different bioinformatics datasets.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"x\": np.array([0.34, 0.36, 0.40, 0.41, 0.43, 0.45, 0.48, 0.50, 0.52, 0.55, 0.57, 0.60]),\n            \"y\": np.array([4.10, 4.00, 4.30, 4.50, 4.60, 4.90, 5.00, 5.30, 5.40, 5.80, 6.00, 6.20]),\n            \"B\": 8000,\n            \"seed\": 13579\n        },\n        {\n            \"x\": np.array([0.31, 0.33, 0.35, 0.38, 0.40, 0.42, 0.44, 0.47, 0.49, 0.51, 0.53, 0.56]),\n            \"y\": np.array([5.10, 4.95, 5.05, 5.00, 5.02, 4.98, 5.01, 5.04, 4.96, 5.03, 4.97, 5.00]),\n            \"B\": 8000,\n            \"seed\": 24680\n        },\n        {\n            \"x\": np.array([0.30, 0.34, 0.37, 0.39, 0.41, 0.44, 0.46, 0.49, 0.51, 0.54, 0.58, 0.61]),\n            \"y\": np.array([7.00, 6.80, 6.60, 6.50, 6.30, 6.10, 6.00, 5.90, 5.70, 5.50, 5.30, 5.20]),\n            \"B\": 8000,\n            \"seed\": 11235\n        },\n        {\n            \"x\": np.array([0.32, 0.36, 0.40, 0.45, 0.50, 0.54, 0.58, 0.62]),\n            \"y\": np.array([3.80, 3.95, 4.10, 4.30, 4.45, 4.60, 4.80, 4.95]),\n            \"B\": 8000,\n            \"seed\": 98765\n        }\n    ]\n\n    all_results = []\n    confidence_level = 0.95\n    alpha = 1.0 - confidence_level\n    lower_quantile = alpha / 2.0\n    upper_quantile = 1.0 - lower_quantile\n\n    def calculate_slope(x, y):\n        \"\"\"Calculates the OLS slope coefficient.\"\"\"\n        x_mean = np.mean(x)\n        y_mean = np.mean(y)\n        numerator = np.sum((x - x_mean) * (y - y_mean))\n        denominator = np.sum((x - x_mean)**2)\n        # Denominator is non-zero for the given datasets and their bootstrap samples\n        # as x values are not all identical.\n        return numerator / denominator\n\n    for case in test_cases:\n        x_data = case[\"x\"]\n        y_data = case[\"y\"]\n        n = len(x_data)\n        B = case[\"B\"]\n        seed = case[\"seed\"]\n\n        rng = np.random.default_rng(seed)\n        \n        bootstrap_slopes = np.empty(B)\n\n        for i in range(B):\n            indices = rng.choice(n, size=n, replace=True)\n            x_boot = x_data[indices]\n            y_boot = y_data[indices]\n            \n            # The case where all x_boot values are identical is highly improbable\n            # and does not occur with the given seeds and data. We proceed without\n            # an explicit check for denominator == 0 for efficiency.\n            bootstrap_slopes[i] = calculate_slope(x_boot, y_boot)\n        \n        ci_lower = np.quantile(bootstrap_slopes, lower_quantile)\n        ci_upper = np.quantile(bootstrap_slopes, upper_quantile)\n\n        rounded_result = [round(ci_lower, 6), round(ci_upper, 6)]\n        all_results.append(rounded_result)\n\n    # Format the output string exactly as specified.\n    # `map(str, all_results)` produces strings like '[1.23, 4.56]',\n    # which are then joined by commas.\n    print(f\"[{','.join(map(str, all_results))}]\".replace(\" \", \"\"))\n\nsolve()\n```", "id": "2429424"}]}