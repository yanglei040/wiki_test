{"hands_on_practices": [{"introduction": "Before diving into complex iterative algorithms, it is crucial to understand the mathematical foundation of matrix balancing. This exercise grounds the abstract concept of normalization in a concrete, solvable problem. By deriving an analytical solution for a small $3 \\times 3$ matrix, you will see firsthand how the balancing condition translates into a system of equations, providing deep intuition for what large-scale computational methods aim to achieve [@problem_id:2397239].", "problem": "A High-throughput Chromosome Conformation Capture (Hi-C) contact map is a symmetric matrix whose entries represent interaction counts between genomic loci. A common normalization goal is to balance a contact matrix by diagonal rescaling so that all rows (and thus columns, by symmetry) of the rescaled matrix have the same sum. Consider the following symmetric Hi-C contact submatrix of size $3 \\times 3$:\n$$\nA \\;=\\;\n\\begin{pmatrix}\n1 & 1 & 0 \\\\\n1 & 2 & 1 \\\\\n0 & 1 & 1\n\\end{pmatrix}.\n$$\nLet $D = \\mathrm{diag}(x_1, x_2, x_3)$ with $x_1, x_2, x_3 \\in \\mathbb{R}_{>0}$, and define the rescaled matrix $B = D A D$. Determine the unique positive diagonal scaling factors $x_1, x_2, x_3$ such that every row sum of $B$ equals $1$, that is,\n$$\n\\sum_{j=1}^{3} B_{ij} \\;=\\; 1 \\quad \\text{for each } i \\in \\{1,2,3\\}.\n$$\nProvide your final answer as a single row matrix $\\begin{pmatrix} x_1 & x_2 & x_3 \\end{pmatrix}$ in exact form (no approximations, no rounding).", "solution": "The problem as stated is scientifically grounded, well-posed, and objective. It describes a standard matrix balancing procedure used in computational biology for the analysis of Hi-C data. The provided matrix is a valid, symmetric, non-negative, and irreducible matrix, for which a unique positive diagonal scaling solution is known to exist. Therefore, the problem is valid and we may proceed with the solution.\n\nThe problem requires finding a diagonal matrix $D = \\mathrm{diag}(x_1, x_2, x_3)$ with $x_1, x_2, x_3 > 0$ such that the rescaled matrix $B = DAD$ has all its row sums equal to $1$. The given matrix is\n$$\nA =\n\\begin{pmatrix}\n1 & 1 & 0 \\\\\n1 & 2 & 1 \\\\\n0 & 1 & 1\n\\end{pmatrix}\n$$\nThe rescaled matrix $B$ is calculated as $B = DAD$. Its elements $B_{ij}$ are given by the formula $B_{ij} = x_i A_{ij} x_j$.\n$$\nB =\n\\begin{pmatrix}\nx_1 & 0 & 0 \\\\\n0 & x_2 & 0 \\\\\n0 & 0 & x_3\n\\end{pmatrix}\n\\begin{pmatrix}\n1 & 1 & 0 \\\\\n1 & 2 & 1 \\\\\n0 & 1 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 & 0 & 0 \\\\\n0 & x_2 & 0 \\\\\n0 & 0 & x_3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1 \\cdot x_1 x_1 & 1 \\cdot x_1 x_2 & 0 \\cdot x_1 x_3 \\\\\n1 \\cdot x_2 x_1 & 2 \\cdot x_2 x_2 & 1 \\cdot x_2 x_3 \\\\\n0 \\cdot x_3 x_1 & 1 \\cdot x_3 x_2 & 1 \\cdot x_3 x_3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nx_1^2 & x_1 x_2 & 0 \\\\\nx_1 x_2 & 2x_2^2 & x_2 x_3 \\\\\n0 & x_2 x_3 & x_3^2\n\\end{pmatrix}\n$$\nThe condition that all row sums of $B$ equal $1$, i.e., $\\sum_{j=1}^{3} B_{ij} = 1$ for $i \\in \\{1, 2, 3\\}$, produces the following system of non-linear equations:\n$$\n\\begin{cases}\n    x_1^2 + x_1 x_2 = 1 & (1) \\\\\n    x_1 x_2 + 2x_2^2 + x_2 x_3 = 1 & (2) \\\\\n    x_2 x_3 + x_3^2 = 1 & (3)\n\\end{cases}\n$$\nWe must solve this system for positive real numbers $x_1, x_2, x_3$.\nFrom equation $(1)$, we can factor $x_1$:\n$$\nx_1(x_1 + x_2) = 1\n$$\nSince $x_1 > 0$, we can express $x_1 + x_2 = \\frac{1}{x_1}$, which leads to $x_2 = \\frac{1}{x_1} - x_1 = \\frac{1-x_1^2}{x_1}$.\nFor $x_2$ to be positive, we must have $1-x_1^2 > 0$, which implies $0 < x_1 < 1$.\n\nSimilarly, from equation $(3)$, we factor $x_3$:\n$$\nx_3(x_2 + x_3) = 1\n$$\nSince $x_3 > 0$, we have $x_2 + x_3 = \\frac{1}{x_3}$, which leads to $x_2 = \\frac{1}{x_3} - x_3 = \\frac{1-x_3^2}{x_3}$.\nFor $x_2$ to be positive, we must have $1-x_3^2 > 0$, which implies $0 < x_3 < 1$.\n\nBy equating the two expressions for $x_2$, we obtain a key relationship between $x_1$ and $x_3$:\n$$\n\\frac{1}{x_1} - x_1 = \\frac{1}{x_3} - x_3\n$$\n$$\n\\frac{1}{x_1} - \\frac{1}{x_3} = x_1 - x_3\n$$\n$$\n\\frac{x_3 - x_1}{x_1 x_3} = -(x_3 - x_1)\n$$\nThis equation has two possible solutions.\nCase 1: $x_3 - x_1 = 0$, which means $x_1 = x_3$.\nCase 2: If $x_3 - x_1 \\neq 0$, we can divide both sides by $(x_3 - x_1)$, yielding $\\frac{1}{x_1 x_3} = -1$, or $x_1 x_3 = -1$. This is impossible, as the problem specifies that $x_1$ and $x_3$ must be positive real numbers.\nThus, the only valid possibility is $x_1 = x_3$.\n\nWe now substitute $x_3 = x_1$ into the original system. Equation $(3)$ becomes identical to equation $(1)$. Equation $(2)$ simplifies to:\n$$\nx_1 x_2 + 2x_2^2 + x_2 x_1 = 1\n$$\n$$\n2x_1 x_2 + 2x_2^2 = 1\n$$\n$$\n2x_2(x_1 + x_2) = 1\n$$\nWe now have a reduced system of two equations:\n$$\n\\begin{cases}\n    x_1(x_1 + x_2) = 1 & (A) \\\\\n    2x_2(x_1 + x_2) = 1 & (B)\n\\end{cases}\n$$\nSince the right-hand sides are equal to $1$, we can equate the left-hand sides:\n$$\nx_1(x_1 + x_2) = 2x_2(x_1 + x_2)\n$$\nSince $x_1 > 0$ and $x_2 > 0$, the term $(x_1 + x_2)$ is strictly positive. We can divide by it without loss of generality:\n$$\nx_1 = 2x_2\n$$\nNow substitute this relation into equation $(A)$:\n$$\n(2x_2)(2x_2 + x_2) = 1\n$$\n$$\n(2x_2)(3x_2) = 1\n$$\n$$\n6x_2^2 = 1\n$$\n$$\nx_2^2 = \\frac{1}{6}\n$$\nSince $x_2 > 0$, we take the positive square root:\n$$\nx_2 = \\sqrt{\\frac{1}{6}} = \\frac{1}{\\sqrt{6}} = \\frac{\\sqrt{6}}{6}\n$$\nUsing the relationships we derived, we find $x_1$ and $x_3$:\n$$\nx_1 = 2x_2 = 2 \\left( \\frac{\\sqrt{6}}{6} \\right) = \\frac{2\\sqrt{6}}{6} = \\frac{\\sqrt{6}}{3}\n$$\nAnd since $x_3 = x_1$:\n$$\nx_3 = \\frac{\\sqrt{6}}{3}\n$$\nThe unique positive diagonal scaling factors are therefore $x_1 = \\frac{\\sqrt{6}}{3}$, $x_2 = \\frac{\\sqrt{6}}{6}$, and $x_3 = \\frac{\\sqrt{6}}{3}$. These values satisfy the constraints and the original system of equations. The final answer must be presented as a single row matrix.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{\\sqrt{6}}{3} & \\frac{\\sqrt{6}}{6} & \\frac{\\sqrt{6}}{3}\n\\end{pmatrix}\n}\n$$", "id": "2397239"}, {"introduction": "While analytical solutions are insightful for small matrices, real Hi-C data requires iterative computational methods. This practice challenges you to implement Sequential Component Normalization (SCN), an algorithm that balances a matrix by making row norms equal. You will then apply your implementation to investigate a key methodological question in genomics: how do inter-chromosomal contacts affect the bias correction within individual chromosomes [@problem_id:2397211]?", "problem": "You are given the task of formalizing and quantifying how removing all inter-chromosomal contacts affects the intra-chromosomal bias vectors estimated by Sequential Component Normalization (SCN) in the context of High-throughput Chromosome Conformation Capture (Hi-C). Let a contact map be represented by a symmetric, nonnegative matrix $M \\in \\mathbb{R}_{\\ge 0}^{n \\times n}$. The genome is partitioned into two chromosomes with sizes $n_1$ and $n_2$ such that $n = n_1 + n_2$, and the matrix $M$ is block-partitioned accordingly:\n$$\nM = \\begin{bmatrix}\nA & E \\\\\nE^\\top & B\n\\end{bmatrix},\n$$\nwhere $A \\in \\mathbb{R}_{\\ge 0}^{n_1 \\times n_1}$ and $B \\in \\mathbb{R}_{\\ge 0}^{n_2 \\times n_2}$ encode intra-chromosomal contacts for chromosome $1$ and chromosome $2$, respectively, and $E \\in \\mathbb{R}_{\\ge 0}^{n_1 \\times n_2}$ encodes inter-chromosomal contacts.\n\nSequential Component Normalization (SCN) is defined as follows. We seek a positive scaling vector $b \\in \\mathbb{R}_{>0}^n$ such that the diagonally scaled contact map\n$$\nN = D(b) \\, M \\, D(b)\n$$\nhas rows whose Euclidean norms are all equal to $1$, where $D(b)$ denotes the diagonal matrix with $b$ on its diagonal. An iterative procedure starts at $b^{(0)} = \\mathbf{1}$ and applies the fixed-point update\n$$\nb^{(t+1)}_i \\leftarrow \\frac{b^{(t)}_i}{\\sqrt{\\max\\left(\\left\\| \\left(D\\!\\left(b^{(t)}\\right) \\, M \\, D\\!\\left(b^{(t)}\\right)\\right)_{i,\\cdot} \\right\\|_2, \\varepsilon \\right)}}\n\\quad \\text{for all } i \\in \\{1,\\dots,n\\},\n$$\nwith a small $\\varepsilon > 0$ used only to avoid division by zero for rows that are exactly zero. This update is repeated until convergence to a fixed point $b^\\star$, which we call the SCN bias vector for $M$.\n\nThe intra-chromosomal bias vector for chromosome $k \\in \\{1,2\\}$ is defined here as the subvector of $b^\\star$ restricted to the indices belonging to chromosome $k$. Consider two ways to compute the bias vector:\n- The global case: compute $b^\\star$ for the full matrix $M$.\n- The intra-only case: compute $\\tilde{b}^\\star$ for the matrix $\\tilde{M}$ obtained by removing all inter-chromosomal contacts, i.e.,\n$$\n\\tilde{M} = \\begin{bmatrix}\nA & 0 \\\\\n0 & B\n\\end{bmatrix}.\n$$\n\nYour task is to implement the SCN procedure to compute and compare the intra-chromosomal bias vectors obtained in the global and intra-only cases. Quantify the effect of removing inter-chromosomal contacts by reporting, for each chromosome, the Euclidean distance between the corresponding bias subvectors from the two scenarios. Concretely, define, for chromosome $1$,\n$$\nd_1 = \\left\\| \\left(b^\\star\\right)_{1:n_1} - \\left(\\tilde{b}^\\star\\right)_{1:n_1} \\right\\|_2,\n$$\nand, for chromosome $2$,\n$$\nd_2 = \\left\\| \\left(b^\\star\\right)_{n_1+1:n} - \\left(\\tilde{b}^\\star\\right)_{n_1+1:n} \\right\\|_2.\n$$\n\nBase your derivation and implementation on the following fundamental facts and definitions:\n- The contact map is a symmetric nonnegative matrix derived from counts of proximal ligation events between genomic loci, which can be modeled by $M$ as specified above.\n- Diagonal congruence transformations $N = D(b) M D(b)$ preserve symmetry and nonnegativity, and row norms of $N$ change according to the scaling in $b$.\n- When $M$ is block-diagonal, the SCN iteration decouples across blocks because each update of $b_i$ depends only on entries within the corresponding block.\n\nUse the following deterministic test suite, with $n_1 = 3$ and $n_2 = 2$:\n- Intra-chromosomal blocks\n$$\nA = \\begin{bmatrix}\n3.0 & 1.0 & 0.5 \\\\\n1.0 & 2.0 & 0.25 \\\\\n0.5 & 0.25 & 1.5\n\\end{bmatrix}, \\quad\nB = \\begin{bmatrix}\n2.0 & 0.3 \\\\\n0.3 & 1.8\n\\end{bmatrix}.\n$$\n- Inter-chromosomal block parameterization\n$$\nE(\\alpha) = \\alpha \\, u v^\\top, \\quad\nu = \\begin{bmatrix} 0.8 \\\\ 1.0 \\\\ 1.2 \\end{bmatrix}, \\quad\nv = \\begin{bmatrix} 0.7 \\\\ 1.1 \\end{bmatrix}.\n$$\n- Assemble\n$$\nM(\\alpha) = \\begin{bmatrix}\nA & E(\\alpha) \\\\\nE(\\alpha)^\\top & B\n\\end{bmatrix}, \\quad\n\\tilde{M} = \\begin{bmatrix}\nA & 0 \\\\\n0 & B\n\\end{bmatrix}.\n$$\n- Evaluate the distances $(d_1, d_2)$ for the three values $\\alpha \\in \\{ 0.0, 0.2, 1.0 \\}$.\n\nAngle units and physical units are not applicable in this problem. All numeric answers must be reported as floats. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order $[d_1(\\alpha{=}0.0), d_2(\\alpha{=}0.0), d_1(\\alpha{=}0.2), d_2(\\alpha{=}0.2), d_1(\\alpha{=}1.0), d_2(\\alpha{=}1.0)]$, with each float formatted to exactly $12$ decimal places, for example, $[0.000000000000,0.000000000000,0.012345678901,0.098765432109,0.123456000000,0.654321000000]$.", "solution": "The problem is well-defined and scientifically sound, addressing a relevant question in computational genomics regarding the normalization of Hi-C contact maps. The task is to quantify the impact of inter-chromosomal contacts on the estimation of intra-chromosomal biases using the Sequential Component Normalization (SCN) algorithm.\n\nThe SCN algorithm is an iterative procedure designed to find a positive scaling vector $b \\in \\mathbb{R}_{>0}^n$ for a given symmetric, non-negative contact matrix $M \\in \\mathbb{R}_{\\ge 0}^{n \\times n}$. The goal is to obtain a normalized matrix, $N = D(b) \\, M \\, D(b)$, where the Euclidean norm of every row is equal to $1$. The matrix $D(b)$ is a diagonal matrix with the vector $b$ on its diagonal. The iterative update rule for each element $b_i$ of the vector $b$ is given by:\n$$\nb^{(t+1)}_i \\leftarrow \\frac{b^{(t)}_i}{\\sqrt{\\max\\left(\\left\\| \\left(N^{(t)}\\right)_{i,\\cdot} \\right\\|_2, \\varepsilon \\right)}}, \\quad \\text{where } N^{(t)} = D(b^{(t)}) \\, M \\, D(b^{(t)})\n$$\nThe small positive constant $\\varepsilon$ ensures numerical stability by preventing division by zero or the square root of zero. This update rule forms a fixed-point iteration. If the iteration converges to a vector $b^\\star$, it satisfies the condition that, for each row $i$, the Euclidean norm of the corresponding row in the normalized matrix $D(b^\\star) M D(b^\\star)$ is unity, i.e., $\\left\\| (D(b^\\star) M D(b^\\star))_{i,\\cdot} \\right\\|_2 = 1$. The L2-norm of the $i$-th row of $N^{(t)}$ can be explicitly written as:\n$$\n\\left\\| (N^{(t)})_{i,\\cdot} \\right\\|_2 = \\sqrt{\\sum_{j=1}^n \\left( N^{(t)}_{ij} \\right)^2} = \\sqrt{\\sum_{j=1}^n \\left( b_i^{(t)} M_{ij} b_j^{(t)} \\right)^2} = b_i^{(t)} \\sqrt{\\sum_{j=1}^n \\left( M_{ij} b_j^{(t)} \\right)^2}\n$$\nas all elements of $b^{(t)}$ are positive. This formulation guides the implementation of the SCN algorithm, which starts with an initial vector $b^{(0)} = \\mathbf{1}$ (a vector of ones) and proceeds until the change between successive iterates, $\\|b^{(t+1)} - b^{(t)}\\|_2$, is smaller than a specified tolerance.\n\nThe problem requires a comparison between two scenarios for a contact matrix partitioned into two chromosomes of sizes $n_1$ and $n_2$:\n1.  **The Intra-Only Case**: The bias vector, denoted $\\tilde{b}^\\star$, is computed for the block-diagonal matrix $\\tilde{M} = \\begin{bmatrix} A & 0 \\\\ 0 & B \\end{bmatrix}$. Here, $A$ and $B$ represent the intra-chromosomal contacts, and the inter-chromosomal block $E$ is set to zero. Due to the block-diagonal structure of $\\tilde{M}$, the SCN updates for the first $n_1$ components of the bias vector depend only on the matrix $A$, while the updates for the remaining $n_2$ components depend only on $B$. Consequently, the problem decouples. The bias subvector for chromosome $1$, $(\\tilde{b}^\\star)_{1:n_1}$, can be found by applying the SCN algorithm to matrix $A$ alone. Similarly, $(\\tilde{b}^\\star)_{n_1+1:n}$ is obtained by applying SCN to matrix $B$. This provides a baseline representing biases estimated without considering inter-chromosomal effects.\n\n2.  **The Global Case**: The bias vector, denoted $b^\\star$, is computed for the full contact matrix $M(\\alpha) = \\begin{bmatrix} A & E(\\alpha) \\\\ E(\\alpha)^\\top & B \\end{bmatrix}$, where $E(\\alpha) = \\alpha \\, u v^\\top$ parameterizes the strength of inter-chromosomal contacts via the scalar $\\alpha$. For $\\alpha > 0$, the matrix $M(\\alpha)$ is not block-diagonal. The SCN update for any component $b_i$ will depend on all other components $b_j$ across both chromosomes, as the matrix is fully connected. This means the bias estimates for both chromosomes are coupled.\n\nThe implementation will proceed as follows:\nFirst, we compute the baseline intra-only bias subvectors by running the SCN algorithm on matrices $A$ and $B$ separately. Let these be $\\tilde{b}^\\star_A$ and $\\tilde{b}^\\star_B$.\nSecond, for each specified value of $\\alpha \\in \\{ 0.0, 0.2, 1.0 \\}$, we construct the global matrix $M(\\alpha)$. We then run the SCN algorithm on $M(\\alpha)$ to find the global bias vector $b^\\star(\\alpha)$.\nThird, we partition $b^\\star(\\alpha)$ into its chromosomal components, $(b^\\star(\\alpha))_{1:n_1}$ and $(b^\\star(\\alpha))_{n_1+1:n}$.\nFinally, we compute the Euclidean distances $d_1(\\alpha)$ and $d_2(\\alpha)$ to quantify the deviation from the baseline:\n$$\nd_1(\\alpha) = \\left\\| (b^\\star(\\alpha))_{1:n_1} - \\tilde{b}^\\star_A \\right\\|_2\n$$\n$$\nd_2(\\alpha) = \\left\\| (b^\\star(\\alpha))_{n_1+1:n} - \\tilde{b}^\\star_B \\right\\|_2\n$$\nAs a crucial sanity check, for $\\alpha = 0.0$, we have $M(0.0) = \\tilde{M}$, which implies $b^\\star(0.0)$ must be identical to the concatenated vector $[\\tilde{b}^\\star_A, \\tilde{b}^\\star_B]$. Therefore, both $d_1(0.0)$ and $d_2(0.0)$ must be zero. The non-zero distances calculated for $\\alpha > 0$ will measure the magnitude of the perturbation introduced by considering inter-chromosomal interactions.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef scn(M, max_iter=2000, tol=1e-15, epsilon=1e-25):\n    \"\"\"\n    Performs Sequential Component Normalization (SCN) on a symmetric, non-negative matrix M.\n\n    Args:\n        M (np.ndarray): The input contact matrix.\n        max_iter (int): Maximum number of iterations.\n        tol (float): Convergence tolerance.\n        epsilon (float): Small constant to avoid division by zero.\n\n    Returns:\n        np.ndarray: The computed bias vector.\n    \"\"\"\n    n = M.shape[0]\n    b = np.ones(n, dtype=np.float64)\n\n    for _ in range(max_iter):\n        b_old = b.copy()\n\n        # Vectorized calculation of row norms of D(b) @ M @ D(b)\n        # The L2-norm of the i-th row is b_i * sqrt(sum_j (M_ij * b_j)^2)\n        \n        # M_b_ij = M_ij * b_j. b is broadcasted across rows.\n        M_b = M * b\n        \n        # M_b_sq_ij = (M_ij * b_j)^2\n        M_b_sq = np.square(M_b)\n        \n        # sum_sq_i = sum_j (M_ij * b_j)^2\n        sum_sq = M_b_sq.sum(axis=1)\n        \n        # row_norms_i = b_i * sqrt(sum_sq_i)\n        row_norms = b * np.sqrt(sum_sq)\n        \n        # Update rule from the problem statement: b_new = b_old / sqrt(row_norm)\n        b = b / np.sqrt(np.maximum(row_norms, epsilon))\n        \n        # Check for convergence\n        if np.linalg.norm(b - b_old) < tol:\n            break\n            \n    return b\n\ndef solve():\n    \"\"\"\n    Solves the problem by computing and comparing bias vectors.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = {\n        'n1': 3,\n        'n2': 2,\n        'A': np.array([\n            [3.0, 1.0, 0.5],\n            [1.0, 2.0, 0.25],\n            [0.5, 0.25, 1.5]\n        ], dtype=np.float64),\n        'B': np.array([\n            [2.0, 0.3],\n            [0.3, 1.8]\n        ], dtype=np.float64),\n        'u': np.array([0.8, 1.0, 1.2], dtype=np.float64).reshape(3, 1),\n        'v': np.array([0.7, 1.1], dtype=np.float64).reshape(2, 1),\n        'alphas': [0.0, 0.2, 1.0]\n    }\n\n    n1 = test_cases['n1']\n    A = test_cases['A']\n    B = test_cases['B']\n    u = test_cases['u']\n    v = test_cases['v']\n    alphas = test_cases['alphas']\n\n    # --- Intra-only case ---\n    # Since tilde_M is block-diagonal, the problem decouples.\n    # We compute bias vectors for A and B separately.\n    b_tilde_A = scn(A)\n    b_tilde_B = scn(B)\n\n    results = []\n    for alpha in alphas:\n        # --- Global case ---\n        # Construct the inter-chromosomal block E(alpha)\n        E_alpha = alpha * (u @ v.T)\n        \n        # Assemble the full matrix M(alpha)\n        M_alpha = np.block([\n            [A, E_alpha],\n            [E_alpha.T, B]\n        ])\n        \n        # Compute the global bias vector b_star for M(alpha)\n        b_star = scn(M_alpha)\n        \n        # Extract subvectors corresponding to each chromosome\n        b_star_1 = b_star[0:n1]\n        b_star_2 = b_star[n1:]\n        \n        # Calculate the Euclidean distance between global and intra-only bias vectors\n        d1 = np.linalg.norm(b_star_1 - b_tilde_A)\n        d2 = np.linalg.norm(b_star_2 - b_tilde_B)\n        \n        results.append(d1)\n        results.append(d2)\n\n    # Final print statement in the exact required format.\n    formatted_results = [f\"{r:.12f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2397211"}, {"introduction": "The principles of bias correction are not limited to genomics; they are a fundamental tool in data science. This exercise asks you to apply the logic of Hi-C normalization to a novel scenario involving file co-access patterns on a computer system. You will implement a powerful two-stage pipeline: first, using linear regression to model and remove biases from known covariates, and second, using iterative balancing to correct for any remaining latent biases, demonstrating the broad applicability of these techniques [@problem_id:2397168].", "problem": "You are given a symmetric, nonnegative integer matrix that encodes the number of co-accesses between files on a computer system over a fixed observation window. Each diagonal entry is the number of self-access events for a file. You are also given, for each file, two covariates: file size and file age. The task is to construct a program that normalizes the co-access matrix to identify functional associations by removing covariate-driven biases, using principles analogous to High-throughput Chromosome Conformation Capture (Hi-C) contact map normalization in computational biology.\n\nAssume the following fundamental base:\n- Co-access events between files are independent and can be modeled by a Poisson process, so that observed counts arise from a Poisson distribution with an intensity that depends on latent association strength and per-node biases.\n- The per-node bias is multiplicative across file pairs and depends on file covariates through a log-linear model.\n- A balanced symmetric normalization, analogous to doubly stochastic balancing in matrix theory, removes remaining node-specific bias by enforcing equal row and column sums for nonzero rows and columns.\n\nLet the observed count matrix be denoted by $C \\in \\mathbb{N}_0^{n \\times n}$ with $C_{ij} = C_{ji} \\ge 0$, file sizes by $\\{s_i\\}_{i=1}^n$ with $s_i > 0$, and file ages by $\\{a_i\\}_{i=1}^n$ with $a_i > 0$. The latent model is that for each pair $(i,j)$, the expected intensity $E[C_{ij}]$ factors as\n$$\nE[C_{ij}] = \\theta_{ij} \\cdot b_i \\cdot b_j,\n$$\nwhere $\\theta_{ij}$ encodes the covariate-independent association propensity and $b_i$ is a positive bias factor for file $i$. The bias factor obeys a log-linear relationship\n$$\n\\log b_i = \\beta_0 + \\beta_1 \\log s_i + \\beta_2 a_i,\n$$\nwith unknown coefficients $\\beta_0, \\beta_1, \\beta_2 \\in \\mathbb{R}$.\n\nYour program must implement the following normalization pipeline derived from the assumptions above:\n1. Estimate the dependence of total co-access per file on the covariates by fitting a linear model to the logarithm of the row sums of $C$. For each file $i$, define the marginal $m_i = \\sum_{j=1}^n C_{ij}$. Use only indices with $m_i > 0$ when fitting the linear model of the form\n$$\n\\log m_i \\approx \\gamma_0 + \\gamma_1 \\log s_i + \\gamma_2 a_i,\n$$\nby least squares. Use the fitted coefficients to compute a bias estimate $\\hat{b}_i = \\exp(\\gamma_0 + \\gamma_1 \\log s_i + \\gamma_2 a_i)$ for all $i \\in \\{1,\\dots,n\\}$.\n2. Form a covariate-weighted matrix $W$ by multiplicative reweighting:\n$$\nW_{ij} = \\frac{C_{ij}}{\\hat{b}_i \\hat{b}_j}.\n$$\n3. Apply a symmetric iterative proportional fitting (balancing) procedure to find a vector $x \\in \\mathbb{R}_{\\ge 0}^n$ such that the balanced matrix\n$$\nB = \\operatorname{diag}(x) \\, W \\, \\operatorname{diag}(x)\n$$\nhas row sums equal to $1$ for all rows $i$ with $\\sum_{j=1}^n W_{ij} > 0$, and has row sum equal to $0$ for any row that is all zeros. Use the fixed-point iteration\n$$\nx_i^{(t+1)} = \\begin{cases}\n\\displaystyle \\frac{1}{\\sum_{j=1}^n W_{ij} x_j^{(t)}} & \\text{if } \\sum_{j=1}^n W_{ij} > 0,\\\\[1.25em]\n0 & \\text{otherwise},\n\\end{cases}\n$$\ninitialized with $x_i^{(0)} = 1$ if $\\sum_{j=1}^n W_{ij} > 0$ and $x_i^{(0)} = 0$ otherwise. Iterate until convergence where the maximum absolute change $\\max_i \\left| x_i^{(t+1)} - x_i^{(t)} \\right|$ is below a tolerance, or until a pre-specified maximum number of iterations is reached.\n\nImplementation requirements:\n- Implement the steps above precisely. You may choose a reasonable small tolerance (for example, $10^{-9}$) and a maximum number of iterations (for example, $10^4$) for the balancing step.\n- For numerical stability in the regression step, exclude files with $m_i = 0$ from the fit. Do not add arbitrary pseudocounts to $m_i$ for the regression. After fitting, compute $\\hat{b}_i$ for all $i$ using the fitted coefficients.\n- After computing $B$, compute the row sums $r_i = \\sum_{j=1}^n B_{ij}$.\n\nYour program must process the following test suite and output the list of row sums for each case:\n\nTest case A (happy path, heterogeneous counts and covariates):\n- Matrix\n$$\nC^{(A)} =\n\\begin{bmatrix}\n100 & 10 & 5 & 0\\\\\n10 & 80 & 20 & 5\\\\\n5 & 20 & 60 & 15\\\\\n0 & 5 & 15 & 40\n\\end{bmatrix}\n$$\n- Sizes\n$$\ns^{(A)} = \\begin{bmatrix} 1000 & 500 & 200 & 100 \\end{bmatrix}\n$$\n- Ages\n$$\na^{(A)} = \\begin{bmatrix} 5 & 2 & 1 & 0.5 \\end{bmatrix}\n$$\n\nTest case B (edge case with an isolated file having zero co-access):\n- Matrix\n$$\nC^{(B)} =\n\\begin{bmatrix}\n30 & 5 & 0 & 0 & 0\\\\\n5 & 25 & 5 & 0 & 0\\\\\n0 & 5 & 20 & 10 & 0\\\\\n0 & 0 & 10 & 15 & 0\\\\\n0 & 0 & 0 & 0 & 0\n\\end{bmatrix}\n$$\n- Sizes\n$$\ns^{(B)} = \\begin{bmatrix} 300 & 200 & 150 & 100 & 50 \\end{bmatrix}\n$$\n- Ages\n$$\na^{(B)} = \\begin{bmatrix} 1 & 2 & 3 & 4 & 5 \\end{bmatrix}\n$$\n\nTest case C (boundary case with uniform counts but heterogeneous covariates):\n- Matrix\n$$\nC^{(C)} =\n\\begin{bmatrix}\n10 & 10 & 10\\\\\n10 & 10 & 10\\\\\n10 & 10 & 10\n\\end{bmatrix}\n$$\n- Sizes\n$$\ns^{(C)} = \\begin{bmatrix} 10^6 & 10^3 & 10 \\end{bmatrix}\n$$\n- Ages\n$$\na^{(C)} = \\begin{bmatrix} 10 & 5 & 1 \\end{bmatrix}\n$$\n\nFinal output specification:\n- For each test case, compute the balanced matrix $B$ and then the vector of row sums $r = \\left(r_1, \\dots, r_n\\right)$.\n- Round each entry of $r$ to $6$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list of the three row-sum vectors, each enclosed in square brackets. For example, the output format must be\n$$\n[ [r_1^{(A)}, r_2^{(A)}, r_3^{(A)}, r_4^{(A)}], [r_1^{(B)}, \\dots, r_5^{(B)}], [r_1^{(C)}, r_2^{(C)}, r_3^{(C)}] ],\n$$\nwith each $r_i^{(\\cdot)}$ displayed as a decimal number with exactly $6$ places after the decimal point. Do not print any other text.", "solution": "The problem is scientifically grounded, well-posed, and objective. It presents a computational task analogous to Hi-C contact map normalization in bioinformatics, applied to a novel \"file co-access\" scenario. The underlying model, which assumes Poisson-distributed counts with multiplicative biases governed by a log-linear dependence on covariates, is a standard and sound approach in this domain. The prescribed normalization pipeline, involving linear regression to estimate biases followed by iterative matrix balancing, is a well-defined and algorithmically-specified procedure. The provision of specific test cases, parameters, and a required output format makes the problem statement complete and unambiguous.\n\nThe solution proceeds by implementing the three specified steps for each test case:\n\n**Step 1: Covariate Bias Estimation via Linear Regression**\n\nFor each file $i \\in \\{1, \\dots, n\\}$, we first compute its total co-access count, or marginal, $m_i = \\sum_{j=1}^n C_{ij}$. The model assumes that the bias $b_i$ for file $i$ is related to its size $s_i$ and age $a_i$ through the log-linear model $\\log b_i = \\beta_0 + \\beta_1 \\log s_i + \\beta_2 a_i$. The marginals are expected to reflect these biases. We estimate a related set of coefficients $(\\gamma_0, \\gamma_1, \\gamma_2)$ by fitting the linear model\n$$\n\\log m_i \\approx \\gamma_0 + \\gamma_1 \\log s_i + \\gamma_2 a_i\n$$\nThis model is fitted using ordinary least squares, but only on the subset of files for which the marginal count is non-zero, i.e., $\\{i \\mid m_i > 0\\}$. This avoids taking the logarithm of zero and ensures the regression is performed on observable data. If we let $I = \\{i \\mid m_i > 0\\}$, we form a design matrix $X$ where each row is $(1, \\log s_i, a_i)$ for $i \\in I$, and a response vector $y$ with elements $\\log m_i$ for $i \\in I$. The coefficient vector $\\gamma = [\\gamma_0, \\gamma_1, \\gamma_2]^T$ is found by solving the linear system $X\\gamma=y$ in the least-squares sense using `numpy.linalg.lstsq`.\n\nOnce the coefficients $\\gamma$ are determined, the bias estimate $\\hat{b}_i$ for *every* file $i \\in \\{1, \\dots, n\\}$ is computed as:\n$$\n\\hat{b}_i = \\exp(\\gamma_0 + \\gamma_1 \\log s_i + \\gamma_2 a_i)\n$$\n\n**Step 2: Covariate-Corrected Matrix Construction**\n\nThe original count matrix $C$ is then corrected for the estimated covariate-driven biases. A new matrix $W$ is formed by dividing each element $C_{ij}$ by the product of the corresponding bias estimates $\\hat{b}_i$ and $\\hat{b}_j$:\n$$\nW_{ij} = \\frac{C_{ij}}{\\hat{b}_i \\hat{b}_j}\n$$\nThis operation can be expressed in matrix form as $W = \\operatorname{diag}(\\hat{b}^{-1}) \\, C \\, \\operatorname{diag}(\\hat{b}^{-1})$, where $\\hat{b}^{-1}$ is the vector of element-wise reciprocals of $\\hat{b}$. This matrix $W$ represents the interaction propensities with covariate effects removed.\n\n**Step 3: Symmetric Iterative Balancing**\n\nThe final step addresses any remaining latent node-specific biases by applying an iterative balancing algorithm. The goal is to find a diagonal scaling matrix $\\operatorname{diag}(x)$ such that the resulting balanced matrix $B = \\operatorname{diag}(x) \\, W \\, \\operatorname{diag}(x)$ has row (and column) sums equal to $1$ for all rows/columns that are not identically zero. The problem specifies the following fixed-point iteration to find the scaling vector $x \\in \\mathbb{R}_{\\ge 0}^n$:\n$$\nx_i^{(t+1)} = \\begin{cases}\n\\displaystyle \\frac{1}{\\sum_{j=1}^n W_{ij} x_j^{(t)}} & \\text{if } \\sum_{j=1}^n W_{ij} > 0,\\\\\n0 & \\text{otherwise}.\n\\end{cases}\n$$\nThe iteration is initialized with $x_i^{(0)} = 1$ for all non-zero rows $i$ (where $\\sum_j W_{ij} > 0$) and $x_i^{(0)} = 0$ for zero rows. The process continues until the maximum absolute change in the vector $x$, $\\max_i |x_i^{(t+1)} - x_i^{(t)}|$, falls below a specified tolerance of $10^{-9}$, or a maximum of $10^4$ iterations is completed.\n\nFor Test Cases A and B, the matrix $W$ for the connected components is irreducible, and the iterative procedure is expected to converge, yielding a balanced matrix with row sums equal to $1$ for non-zero rows. For Test Case C, due to the uniform counts in matrix $C$, the regression correctly identifies that biases are independent of covariates, resulting in a rank-$1$ matrix $W$. The prescribed iterative scheme oscillates and does not converge to a solution where row sums are $1$. It instead terminates upon reaching the maximum iteration count ($10^4$). Following the instructions precisely, we use the value of $x$ at this final iteration.\n\nFinally, for each test case, the balanced matrix $B$ is computed, and its row sums $r_i = \\sum_{j=1}^n B_{ij}$ are calculated and reported to $6$ decimal places.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the file co-access matrix normalization problem for the given test cases.\n    \"\"\"\n\n    def normalize_matrix(C, s, a, tol=1e-9, max_iter=10000):\n        \"\"\"\n        Implements the complete 3-step normalization pipeline.\n        \n        Args:\n            C (np.ndarray): The symmetric co-access count matrix.\n            s (np.ndarray): The vector of file sizes.\n            a (np.ndarray): The vector of file ages.\n            tol (float): Convergence tolerance for the balancing step.\n            max_iter (int): Maximum number of iterations for the balancing step.\n\n        Returns:\n            np.ndarray: The vector of row sums of the final balanced matrix B.\n        \"\"\"\n        n = C.shape[0]\n\n        # Step 1: Estimate covariate dependence via linear regression\n        m = C.sum(axis=1)\n        valid_indices = np.where(m > 0)[0]\n        \n        # We need at least as many data points as parameters for a determined or over-determined system.\n        # The number of parameters is 3 (gamma_0, gamma_1, gamma_2).\n        if len(valid_indices) < 3:\n            # This case is not in the test suite but is a potential issue.\n            # A simple assumption might be no bias, but we follow the spec.\n            # `lstsq` will handle this by finding a least-norm solution.\n            pass\n\n        log_s_valid = np.log(s[valid_indices])\n        a_valid = a[valid_indices]\n        log_m_valid = np.log(m[valid_indices])\n\n        # Design matrix X for regression: [1, log(s_i), a_i]\n        X = np.vstack([np.ones(len(valid_indices)), log_s_valid, a_valid]).T\n        \n        # Solve for gamma = [gamma_0, gamma_1, gamma_2]\n        gamma, _, _, _ = np.linalg.lstsq(X, log_m_valid, rcond=None)\n        \n        # Compute bias estimates for all files\n        log_b_hat = gamma[0] + gamma[1] * np.log(s) + gamma[2] * a\n        b_hat = np.exp(log_b_hat)\n\n        # Step 2: Form the covariate-weighted matrix W\n        # Using np.outer for efficient calculation of the denominator\n        W = C / np.outer(b_hat, b_hat)\n\n        # Step 3: Apply symmetric iterative proportional fitting (balancing)\n        is_nonzero_row = W.sum(axis=1) > 0\n        x = is_nonzero_row.astype(float)\n        \n        for _ in range(max_iter):\n            x_old = x.copy()\n            \n            # Compute v = W @ x\n            v = W @ x_old\n            \n            # Update x for non-zero rows. \n            # For irreducible non-negative matrices W and positive x, v will be positive.\n            # No special handling for v[i]=0 needed for the given test cases.\n            x[is_nonzero_row] = 1.0 / v[is_nonzero_row]\n\n            # Check for convergence\n            if np.max(np.abs(x - x_old)) < tol:\n                break\n        \n        # Compute the final balanced matrix B and its row sums\n        B = np.diag(x) @ W @ np.diag(x)\n        row_sums = B.sum(axis=1)\n        \n        return row_sums\n\n    # Test cases from the problem statement\n    test_cases = [\n        {\n            \"C\": np.array([\n                [100, 10, 5, 0],\n                [10, 80, 20, 5],\n                [5, 20, 60, 15],\n                [0, 5, 15, 40]\n            ], dtype=float),\n            \"s\": np.array([1000, 500, 200, 100], dtype=float),\n            \"a\": np.array([5, 2, 1, 0.5], dtype=float)\n        },\n        {\n            \"C\": np.array([\n                [30, 5, 0, 0, 0],\n                [5, 25, 5, 0, 0],\n                [0, 5, 20, 10, 0],\n                [0, 0, 10, 15, 0],\n                [0, 0, 0, 0, 0]\n            ], dtype=float),\n            \"s\": np.array([300, 200, 150, 100, 50], dtype=float),\n            \"a\": np.array([1, 2, 3, 4, 5], dtype=float)\n        },\n        {\n            \"C\": np.array([\n                [10, 10, 10],\n                [10, 10, 10],\n                [10, 10, 10]\n            ], dtype=float),\n            \"s\": np.array([1e6, 1e3, 10], dtype=float),\n            \"a\": np.array([10, 5, 1], dtype=float)\n        }\n    ]\n\n    results_formatted = []\n    for case in test_cases:\n        row_sums = normalize_matrix(case[\"C\"], case[\"s\"], case[\"a\"])\n        # Format each row sum to 6 decimal places and join into a string representation of a list\n        formatted_sums = f\"[{','.join([f'{rs:.6f}' for rs in row_sums])}]\"\n        results_formatted.append(formatted_sums)\n\n    # Print the final output in the exact required format\n    print(f\"[{','.join(results_formatted)}]\")\n\nsolve()\n```", "id": "2397168"}]}