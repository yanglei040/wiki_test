{"hands_on_practices": [{"introduction": "Let's begin by considering the foundational assumptions of pseudotime analysis. This exercise [@problem_id:1465922] challenges you to think critically about which biological processes can be appropriately modeled as a simple, directed progression. By contrasting a classic differentiation pathway with the cell cycle, you will grasp the crucial importance of a process being acyclic for standard trajectory inference methods to succeed.", "problem": "In systems biology, single-cell RNA sequencing (scRNA-seq) allows for the profiling of gene expression in individual cells. A common computational technique applied to scRNA-seq data is pseudotime analysis. This method aims to order cells along a trajectory that represents a continuous biological process, such as cellular differentiation, by using transitional expression states. The resulting \"pseudotime\" value for each cell represents its progress through the process.\n\nWhile pseudotime analysis is highly effective for modeling the differentiation of hematopoietic stem cells into various mature blood cell types, it is known to be fundamentally challenging to apply to cells progressing through the cell cycle (G1, S, G2, M phases).\n\nWhich of the following statements provides the most accurate and fundamental reason for this discrepancy?\n\nA. The number of genes that are differentially expressed during the cell cycle is too small to provide the resolution needed to accurately construct a continuous trajectory.\n\nB. The cell cycle is an inherently cyclical process, which violates the core assumption of a directed, acyclic progression that underlies most standard pseudotime inference algorithms.\n\nC. Cellular differentiation processes are typically much slower than the cell cycle, allowing for a more stable capture of intermediate cell states in a scRNA-seq experiment.\n\nD. Gene expression during the cell cycle is primarily random and stochastic, whereas gene expression during differentiation follows a highly deterministic and predictable program.", "solution": "Pseudotime analysis aims to impose an ordering of cells along a one-dimensional trajectory that represents progression through a biological process. Most standard pseudotime inference algorithms (e.g., those based on minimum spanning trees, trees in low-dimensional embeddings, or directed acyclic graph abstractions) assume a directed, acyclic structure: a path or a branching tree where there is a well-defined start and no topological cycles. This assumption is well matched to cellular differentiation, which is typically modeled as a branching process with a direction from progenitor to mature fates and no return to the progenitor state.\n\nBy contrast, the cell cycle is a periodic process with circular topology: cells progress through G1, S, G2, and M phases and return to G1. Topologically, this corresponds to a loop (homeomorphic to a circle), which cannot be embedded as a single directed acyclic trajectory without introducing an arbitrary “cut” that breaks the cycle. This violates the core assumption of most standard pseudotime algorithms and leads to ambiguity in defining a unique start and end, seam artifacts, and misordering near the cut.\n\nEvaluating the options:\n- Option A is not fundamental; numerous genes exhibit robust, phase-specific periodic expression sufficient to recover structure, so the difficulty is not due to too few differentially expressed genes.\n- Option B correctly identifies the core topological mismatch: the cyclic nature of the cell cycle conflicts with the directed, acyclic assumption of standard pseudotime models.\n- Option C concerns timescale and measurement stability, which may affect practical resolution but is not the fundamental reason for the discrepancy.\n- Option D is incorrect; the cell cycle is governed by a well-regulated periodic program rather than being primarily random, and differentiation also involves stochastic elements.\n\nTherefore, the most accurate and fundamental reason is the cyclic topology of the cell cycle violating the directed, acyclic assumption.", "answer": "$$\\boxed{B}$$", "id": "1465922"}, {"introduction": "After confirming that a process is suitable for trajectory inference, a key task is to determine its topology. This hands-on coding practice [@problem_id:2437495] immerses you in the fundamental challenge of model selection: is the underlying biological process a simple linear progression or does it involve a branching decision point? You will implement and compare linear and branching models, using principled statistical criteria like the Akaike Information Criterion (AIC) to make a data-driven choice.", "problem": "You are given a finite set of points in the plane that represent cells embedded in a low-dimensional space obtained from single-cell data. You will compare two nested trajectory models that map each cell to a one-dimensional pseudotime while assuming isotropic Gaussian noise in the direction orthogonal to the trajectory. The two candidate models are: a linear trajectory model and a branching trajectory model with a single bifurcation (two straight branches sharing a junction). Your task is to implement a program that, for each dataset in a test suite, decides which model is preferred by the Akaike Information Criterion (AIC) or the Bayesian Information Criterion (BIC), returning an integer flag for each dataset. The final output must be a single line containing a list of integers corresponding to all datasets.\n\nFundamental modeling assumptions and definitions:\n- Let the dataset be points $\\{x_i\\}_{i=1}^n \\subset \\mathbb{R}^2$, where $x_i = (x_{i1}, x_{i2})^\\top$. Each cell is assumed to lie near a one-dimensional trajectory with isotropic Gaussian noise orthogonal to the trajectory.\n- For the linear model $\\mathcal{M}_{\\text{lin}}$, the trajectory is a single line: a point $m \\in \\mathbb{R}^2$ and a unit direction $d \\in \\mathbb{R}^2$, $\\|d\\|_2 = 1$. The pseudotime of $x_i$ is the scalar $t_i = d^\\top (x_i - m)$. The orthogonal residual is $r_i = (x_i - m) - t_i d$, and the residual sum of squares is $\\text{RSS}_{\\text{lin}} = \\sum_{i=1}^n \\|r_i\\|_2^2$.\n- For the branching model $\\mathcal{M}_{\\text{br}}$, the trajectory is the union of two lines that share a common junction point $m \\in \\mathbb{R}^2$ with two unit directions $d_1, d_2 \\in \\mathbb{R}^2$, $\\|d_j\\|_2 = 1$ for $j \\in \\{1,2\\}$. Each cell is assigned to the branch that yields smaller orthogonal distance. For assignment $a_i \\in \\{1,2\\}$ minimizing the perpendicular distance, the pseudotime is $t_i^{(a_i)} = d_{a_i}^\\top (x_i - m)$ and the orthogonal residual is $r_i^{(a_i)} = (x_i - m) - t_i^{(a_i)} d_{a_i}$. The residual sum of squares is $\\text{RSS}_{\\text{br}} = \\sum_{i=1}^n \\min\\{\\|(x_i - m) - (d_1^\\top (x_i - m)) d_1\\|_2^2,\\|(x_i - m) - (d_2^\\top (x_i - m)) d_2\\|_2^2\\}$.\n- Gaussian noise model and log-likelihood: Assume independent and identically distributed orthogonal deviations with one-dimensional Gaussian noise $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$ in the perpendicular direction, so that the maximum-likelihood estimator of variance is $\\widehat{\\sigma}^2 = RSS/n$. The maximized log-likelihood under either model is\n$$\n\\log L_{\\max} = -\\frac{n}{2}\\left[ \\log(2\\pi \\widehat{\\sigma}^2) + 1 \\right].\n$$\nTo avoid degenerate likelihoods when $\\text{RSS} = 0$, implement a strictly positive floor $\\epsilon$ and use $\\widehat{\\sigma}^2 = \\max\\{\\text{RSS}/n, \\epsilon\\}$ with $\\epsilon = 10^{-12}$.\n- Model selection criteria are defined by\n$$\n\\text{AIC} = 2k - 2 \\log L_{\\max}, \\quad \\text{BIC} = k \\log n - 2 \\log L_{\\max},\n$$\nwhere $k$ is the number of free parameters estimated. Count parameters as follows, including the variance parameter:\n  - For $\\mathcal{M}_{\\text{lin}}$: $k_{\\text{lin}} = 4$ parameters, comprising $m$ ($2$ real degrees of freedom), the line direction $d$ ($1$ angular degree of freedom since $\\|d\\|_2=1$), and the variance $\\sigma^2$ ($1$).\n  - For $\\mathcal{M}_{\\text{br}}$: $k_{\\text{br}} = 5$ parameters, comprising $m$ ($2$), two branch directions $d_1, d_2$ (each contributes $1$ angular degree of freedom, totaling $2$), and the variance $\\sigma^2$ ($1$). Discrete branch assignments are not counted as parameters.\n\nAlgorithmic requirements:\n- Fitting $\\mathcal{M}_{\\text{lin}}$: Estimate $m$ as the sample mean $m = \\frac{1}{n}\\sum_{i=1}^n x_i$. Estimate $d$ as the first principal component direction of the centered data matrix. Compute $\\text{RSS}_{\\text{lin}}$ by summing squared perpendicular distances to the fitted line.\n- Fitting $\\mathcal{M}_{\\text{br}}$: Fix the junction at the same sample mean $m$. Estimate directions $d_1, d_2$ via an iterative direction-clustering procedure on unit vectors $u_i = \\frac{x_i - m}{\\|x_i - m\\|_2}$ for those $i$ with $\\|x_i - m\\|_2 > 0$.\n  - Initialization: use the first two principal directions of the centered data as initial unit directions.\n  - Iteration: alternate between assigning each $u_i$ to the direction $d_j$ that maximizes $|u_i^\\top d_j|$ (reflecting bidirectional lines), and updating each $d_j$ to the normalized signed sum of assigned $u_i$, namely $d_j \\leftarrow \\frac{\\sum_{i \\in S_j} \\operatorname{sign}(u_i^\\top d_j) u_i}{\\left\\|\\sum_{i \\in S_j} \\operatorname{sign}(u_i^\\top d_j) u_i\\right\\|_2}$, for a fixed small number of iterations. If a set $S_j$ is empty in an iteration, retain the previous $d_j$.\n  - Compute $\\text{RSS}_{\\text{br}}$ as the sum over points of the squared perpendicular distances to the closer of the two lines through $m$ with directions $d_1$ and $d_2$.\n- Compute $\\log L_{\\max}$, AIC or BIC as specified per test case, and select the model with the smaller criterion value. In the event of equality up to a numerical tolerance of $10^{-9}$, select the simpler linear model $\\mathcal{M}_{\\text{lin}}$.\n\nAngle unit: All angles must be interpreted in radians.\n\nTest suite (no randomness allowed; generate the exact points as specified):\n- Case $1$ (linear, criterion BIC):\n  - Single-line dataset: $m = (0,0)$, direction with angle $\\theta = \\pi/6$, and pseudotimes $t \\in \\{-2,-1,0,1,2\\}$. Points are $x(t) = m + t(\\cos\\theta, \\sin\\theta)$ for each $t$.\n  - Use BIC for selection.\n- Case $2$ (branching, criterion AIC):\n  - Two-branch dataset: $m = (0,0)$, branch directions with angles $\\theta_1 = \\pi/6$ and $\\theta_2 = -\\pi/6$, and pseudotimes on each branch $t \\in \\{-1, 1\\}$. Points are the union of $\\{ m + t(\\cos\\theta_1, \\sin\\theta_1) : t \\in \\{-1,1\\} \\}$ and $\\{ m + t(\\cos\\theta_2, \\sin\\theta_2) : t \\in \\{-1,1\\} \\}$.\n  - Use AIC for selection.\n- Case $3$ (imbalanced weak branching, criterion BIC):\n  - Two-branch dataset: true junction $m_{\\text{true}} = (0,0)$, branch directions with angles $\\theta_1 = 0$ and $\\theta_2 = \\pi/12$, pseudotimes $t_1 \\in \\{0,1,2,3\\}$ on branch $1$ and $t_2 \\in \\{0.5\\}$ on branch $2$. Points are the union of $\\{ m_{\\text{true}} + t(\\cos\\theta_1, \\sin\\theta_1) : t \\in \\{0,1,2,3\\} \\}$ and $\\{ m_{\\text{true}} + t(\\cos\\theta_2, \\sin\\theta_2) : t \\in \\{0.5\\} \\}$.\n  - Use BIC for selection.\n- Case $4$ (tiny edge case, criterion AIC):\n  - Two points on a line: $x_1 = (0,0)$, $x_2 = (1,0)$.\n  - Use AIC for selection.\n\nAnswer specification and final output format:\n- For each case, output the integer $1$ if the branching model $\\mathcal{M}_{\\text{br}}$ is preferred by the specified information criterion, and $0$ if the linear model $\\mathcal{M}_{\\text{lin}}$ is preferred (including ties).\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[0,1,0,0]\").", "solution": "The user has provided a problem statement that requires the implementation of a model selection procedure to compare a linear trajectory model against a branching trajectory model for single-cell data. The decision is to be made using either the Akaike Information Criterion (AIC) or the Bayesian Information Criterion (BIC). The problem demands a strict validation of its premises before a solution is attempted.\n\n### Problem Validation\n\nFirst, I shall meticulously validate the problem statement according to the specified protocol.\n\n**Step 1: Extract Givens**\n\n-   **Data**: A set of points $\\{x_i\\}_{i=1}^n \\subset \\mathbb{R}^2$.\n-   **Linear Model ($\\mathcal{M}_{\\text{lin}}$)**:\n    -   Trajectory defined by a junction $m \\in \\mathbb{R}^2$ and a unit direction $d \\in \\mathbb{R}^2$.\n    -   $\\text{RSS}_{\\text{lin}} = \\sum_{i=1}^n \\|(x_i - m) - (d^\\top(x_i - m))d\\|_2^2$.\n    -   Number of parameters: $k_{\\text{lin}} = 4$.\n-   **Branching Model ($\\mathcal{M}_{\\text{br}}$)**:\n    -   Trajectory defined by a junction $m \\in \\mathbb{R}^2$ and two unit directions $d_1, d_2 \\in \\mathbb{R}^2$.\n    -   $\\text{RSS}_{\\text{br}} = \\sum_{i=1}^n \\min_{j \\in \\{1,2\\}} \\|(x_i - m) - (d_j^\\top(x_i - m))d_j\\|_2^2$.\n    -   Number of parameters: $k_{\\text{br}} = 5$.\n-   **Statistical Framework**: The problem defines a Gaussian noise model with noise $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$ in the direction perpendicular to the trajectory. It provides formulas for the maximum likelihood variance estimator, the maximized log-likelihood, and the AIC/BIC criteria.\n-   **Model Selection Criteria**:\n    -   $\\text{AIC} = 2k - 2 \\log L_{\\max}$.\n    -   $\\text{BIC} = k \\log n - 2 \\log L_{\\max}$.\n    -   Tie-breaking rule: prefer $\\mathcal{M}_{\\text{lin}}$ if criteria are equal within a tolerance of $10^{-9}$.\n-   **Fitting Algorithms**:\n    -   In both models, $m$ is the sample mean.\n    -   For $\\mathcal{M}_{\\text{lin}}$, $d$ is the first principal component direction of the centered data.\n    -   For $\\mathcal{M}_{\\text{br}}$, $d_1, d_2$ are estimated via an iterative procedure initialized with the first two principal component directions. The number of iterations is specified as a \"fixed small number.\" I will set this to be $10$, a reasonable choice that ensures deterministic behavior.\n-   **Test Suite**: Four specific, deterministically generated datasets are provided for testing.\n-   **Output Format**: A list of integers ($0$ for linear, $1$ for branching) in a specific string format.\n\n**Step 2: Validate Using Extracted Givens**\n\n1.  **Scientifically Grounded**: The problem is well-grounded in computational biology and statistics. Trajectory inference, principal component analysis (PCA), iterative clustering, and model selection via AIC/BIC are all standard and valid concepts.\n2.  **Well-Posed**: The problem is well-posed. The algorithmic steps for fitting both models are explicitly defined and lead to a deterministic outcome. The minor ambiguity of \"a fixed small number of iterations\" is resolved by choosing a specific number, such as $10$. The use of a floor $\\epsilon$ for the variance estimate prevents divisions by zero or logarithms of zero.\n3.  **Objective**: The problem is stated in precise, mathematical terms, free from subjectivity.\n4.  **Flaw Checklist**: The problem does not violate any of the invalidity criteria. It is scientifically sound, formalizable, and relevant. The setup is complete (with the minor iteration count ambiguity resolved) and consistent. The specified test cases are feasible. The problem is well-structured and scientifically verifiable.\n\n**Step 3: Verdict and Action**\n\nThe problem is **valid**. I will proceed to construct the solution.\n\n### Solution\n\nThe solution requires implementing the fitting procedures for both the linear and branching models, calculating the specified information criterion for each, and selecting the model with the lower criterion value. The core of the task lies in correctly implementing the specified algorithms for parameter estimation and residual sum of squares calculation.\n\n**1. Model Fitting and $\\text{RSS}$ Calculation**\n\nFor a a given dataset of $n$ points $X \\in \\mathbb{R}^{n \\times 2}$, the junction point $m$ for both models is estimated as the sample mean:\n$$\nm = \\frac{1}{n} \\sum_{i=1}^n x_i\n$$\nLet $B = X - m^\\top$ be the $n \\times 2$ matrix of centered data points.\n\n**Linear Model ($\\mathcal{M}_{\\text{lin}}$):**\nThe direction vector $d$ is the first principal component of $B$. This is found by performing a Singular Value Decomposition (SVD) on $B = U S V^\\top$. The principal directions are the columns of $V$, so $d$ is the first column of $V$ (or the first row of $V^\\top$, often denoted `Vh` in numerical libraries).\nThe residual sum of squares can be calculated efficiently. For each centered point $b_i = x_i - m$, the squared orthogonal distance to the line defined by $d$ is $\\|b_i\\|^2 - (d^\\top b_i)^2$. Thus:\n$$\n\\text{RSS}_{\\text{lin}} = \\sum_{i=1}^n \\left( \\|b_i\\|^2 - (d^\\top b_i)^2 \\right)\n$$\n\n**Branching Model ($\\mathcal{M}_{\\text{br}}$):**\nThe directions $d_1$ and $d_2$ are found via an iterative algorithm.\n-   **Initialization**: The initial directions $d_1$ and $d_2$ are set to the first and second principal component directions of $B$, respectively.\n-   **Iteration**: The algorithm refines these directions. First, it considers the unit vectors $u_i = b_i / \\|b_i\\|_2$ for all $b_i \\neq 0$. It then alternates between two steps for a fixed number of iterations (I will use $10$):\n    1.  **Assignment**: Each unit vector $u_i$ is assigned to the cluster $S_j$ corresponding to the direction $d_j$ that maximizes the absolute projection $|u_i^\\top d_j|$. This partitions the set of unit vectors into $S_1$ and $S_2$.\n    2.  **Update**: Each direction $d_j$ is updated to be the normalized mean direction of its assigned vectors, accounting for orientation. Let $d_j^{\\text{old}}$ be the direction from the previous step. The new direction is:\n        $$\n        d_j^{\\text{new}} \\leftarrow \\frac{\\sum_{u_i \\in S_j} \\operatorname{sign}(u_i^\\top d_j^{\\text{old}}) u_i}{\\left\\|\\sum_{u_i \\in S_j} \\operatorname{sign}(u_i^\\top d_j^{\\text{old}}) u_i\\right\\|_2}\n        $$\n        If a cluster $S_j$ is empty, $d_j$ remains unchanged.\n-   **$\\text{RSS}$ Calculation**: After the iterative refinement, the final directions $d_1$ and $d_2$ are used to compute $\\text{RSS}_{\\text{br}}$. For each point, the smaller of the two squared orthogonal distances is taken:\n$$\n\\text{RSS}_{\\text{br}} = \\sum_{i=1}^n \\min\\left( \\|b_i\\|^2 - (d_1^\\top b_i)^2, \\|b_i\\|^2 - (d_2^\\top b_i)^2 \\right)\n$$\n\n**2. Model Selection**\n\nFor each model $\\mathcal{M} \\in \\{\\mathcal{M}_{\\text{lin}}, \\mathcal{M}_{\\text{br}}\\}$, we compute its maximized log-likelihood. First, the variance is estimated as $\\widehat{\\sigma}^2 = \\max(\\text{RSS}/n, \\epsilon)$, where $\\epsilon = 10^{-12}$. Then:\n$$\n\\log L_{\\max} = -\\frac{n}{2}\\left( \\log(2\\pi \\widehat{\\sigma}^2) + 1 \\right)\n$$\nUsing the appropriate parameter count $k$ ($k_{\\text{lin}}=4$, $k_{\\text{br}}=5$), the specified information criterion (AIC or BIC) is calculated:\n$$\n\\text{AIC} = 2k - 2 \\log L_{\\max}\n$$\n$$\n\\text{BIC} = k \\log n - 2 \\log L_{\\max}\n$$\nThe model with the lower criterion value is preferred. If $\\text{Criterion}_{\\text{br}} < \\text{Criterion}_{\\text{lin}} - 10^{-9}$, the branching model is chosen (output $1$). Otherwise, including the case of equality, the simpler linear model is chosen (output $0$).\n\nThis structured, principle-based approach ensures that the implementation will faithfully execute the specified tasks and provide a verifiable result for each test case.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the model selection analysis on all test cases.\n    \"\"\"\n    epsilon = 1e-12\n    num_iterations_branching = 10\n    tie_tolerance = 1e-9\n    \n    # --- Helper Functions ---\n    \n    def fit_linear_model(X):\n        \"\"\"Fits the linear model M_lin.\"\"\"\n        n, _ = X.shape\n        if n == 0:\n            return np.zeros(2), np.array([1.0, 0.0]), 0.0\n\n        m = np.mean(X, axis=0)\n        B = X - m\n        \n        if n < 2:\n            return m, np.array([1.0, 0.0]), 0.0\n        \n        # SVD on centered data to get principal components\n        # Vh contains the principal directions as rows\n        try:\n            _, _, Vh = np.linalg.svd(B, full_matrices=False)\n            d = Vh[0, :]\n        except np.linalg.LinAlgError:\n            d = np.array([1.0, 0.0]) # Fallback for singular matrix\n            \n        # Calculate RSS using the efficient formula: sum(||b_i||^2 - (d.b_i)^2)\n        squared_norms = np.sum(B**2, axis=1)\n        projections_sq = (B @ d)**2\n        rss = np.sum(squared_norms - projections_sq)\n        \n        return m, d, rss\n\n    def fit_branching_model(X):\n        \"\"\"Fits the branching model M_br.\"\"\"\n        n, _ = X.shape\n        if n == 0:\n            return np.zeros(2), np.array([1., 0.]), np.array([0., 1.]), 0.0\n\n        m = np.mean(X, axis=0)\n        B = X - m\n        \n        if n < 2:\n            return m, np.array([1., 0.]), np.array([0., 1.]), 0.0\n\n        # Initialize directions with the first two PCs\n        try:\n            _, _, Vh = np.linalg.svd(B, full_matrices=False)\n            d1 = Vh[0, :]\n            d2 = Vh[1, :] if Vh.shape[0] > 1 else np.array([-d1[1], d1[0]])\n        except np.linalg.LinAlgError:\n            d1 = np.array([1.0, 0.0])\n            d2 = np.array([0.0, 1.0])\n\n        # Get unit vectors for points not at the mean\n        norms = np.linalg.norm(B, axis=1)\n        valid_indices = norms > epsilon\n        u_vectors = B[valid_indices] / norms[valid_indices, np.newaxis]\n\n        if u_vectors.shape[0] > 0:\n            # Iterative refinement of directions\n            for _ in range(num_iterations_branching):\n                d1_old, d2_old = d1, d2\n                \n                # Assignment step\n                proj1 = np.abs(u_vectors @ d1)\n                proj2 = np.abs(u_vectors @ d2)\n                \n                assign_to_1 = proj1 >= proj2\n                S1_vectors = u_vectors[assign_to_1]\n                S2_vectors = u_vectors[~assign_to_1]\n\n                # Update step for d1\n                if S1_vectors.shape[0] > 0:\n                    signs1 = np.sign(S1_vectors @ d1_old)\n                    sum_vec1 = np.sum(S1_vectors * signs1[:, np.newaxis], axis=0)\n                    norm1 = np.linalg.norm(sum_vec1)\n                    if norm1 > epsilon:\n                        d1 = sum_vec1 / norm1\n\n                # Update step for d2\n                if S2_vectors.shape[0] > 0:\n                    signs2 = np.sign(S2_vectors @ d2_old)\n                    sum_vec2 = np.sum(S2_vectors * signs2[:, np.newaxis], axis=0)\n                    norm2 = np.linalg.norm(sum_vec2)\n                    if norm2 > epsilon:\n                        d2 = sum_vec2 / norm2\n\n        # Calculate RSS for the branching model\n        squared_norms = np.sum(B**2, axis=1)\n        proj_d1_sq = (B @ d1)**2\n        proj_d2_sq = (B @ d2)**2\n        \n        rss_per_point_1 = squared_norms - proj_d1_sq\n        rss_per_point_2 = squared_norms - proj_d2_sq\n        \n        # Negative rss values can occur from float precision errors, clip at 0\n        rss_br = np.sum(np.minimum(rss_per_point_1, rss_per_point_2).clip(min=0))\n\n        return m, d1, d2, rss_br\n\n    def calculate_criterion(rss, k, n, criterion_type):\n        \"\"\"Calculates AIC or BIC.\"\"\"\n        if n == 0:\n            return float('inf')\n        \n        sigma_sq_hat = max(rss / n, epsilon)\n        log_L_max = - (n / 2) * (np.log(2 * np.pi * sigma_sq_hat) + 1)\n        \n        if criterion_type.upper() == 'AIC':\n            return 2 * k - 2 * log_L_max\n        elif criterion_type.upper() == 'BIC':\n            # log(n) can be problematic for n=1, k log(1) = 0\n            # Problem cases have n>=2.\n            return k * np.log(n) - 2 * log_L_max\n        else:\n            raise ValueError(\"Unknown criterion type\")\n\n    # --- Test Cases ---\n    \n    test_cases_defs = [\n        {\n            \"id\": 1,\n            \"criterion\": \"BIC\",\n            \"points_func\": lambda: np.array([t * np.array([np.cos(np.pi/6), np.sin(np.pi/6)]) for t in [-2, -1, 0, 1, 2]])\n        },\n        {\n            \"id\": 2,\n            \"criterion\": \"AIC\",\n            \"points_func\": lambda: np.vstack([\n                np.array([t * np.array([np.cos(np.pi/6), np.sin(np.pi/6)]) for t in [-1, 1]]),\n                np.array([t * np.array([np.cos(-np.pi/6), np.sin(-np.pi/6)]) for t in [-1, 1]])\n            ])\n        },\n        {\n            \"id\": 3,\n            \"criterion\": \"BIC\",\n            \"points_func\": lambda: np.vstack([\n                np.array([t * np.array([np.cos(0), np.sin(0)]) for t in [0, 1, 2, 3]]),\n                np.array([0.5 * np.array([np.cos(np.pi/12), np.sin(np.pi/12)])])\n            ])\n        },\n        {\n            \"id\": 4,\n            \"criterion\": \"AIC\",\n            \"points_func\": lambda: np.array([[0., 0.], [1., 0.]])\n        }\n    ]\n\n    results = []\n    for case_def in test_cases_defs:\n        X = case_def[\"points_func\"]()\n        criterion_type = case_def[\"criterion\"]\n        n = X.shape[0]\n\n        # Linear Model\n        k_lin = 4\n        _, _, rss_lin = fit_linear_model(X)\n        crit_lin = calculate_criterion(rss_lin, k_lin, n, criterion_type)\n\n        # Branching Model\n        k_br = 5\n        _, _, _, rss_br = fit_branching_model(X)\n        crit_br = calculate_criterion(rss_br, k_br, n, criterion_type)\n\n        # Decision\n        if crit_br < crit_lin - tie_tolerance:\n            results.append(1)\n        else:\n            results.append(0)\n            \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2437495"}, {"introduction": "An inferred trajectory provides a relative ordering of cells but lacks an intrinsic direction of flow. To root the trajectory and understand the biological starting point, we often rely on prior knowledge. This exercise [@problem_id:2437518] translates a key biological heuristic into a quantitative algorithm, using Shannon entropy to identify progenitor cells based on their unique expression patterns of pluripotency markers.", "problem": "You are given a finite set of single cells, each measured for multiple genes, and a designated subset of genes that are pluripotency markers. For each cell, define a probability distribution over the marker set by normalizing its nonnegative marker expression vector with an additive nonnegative pseudocount. The Shannon entropy of this distribution quantifies the concentration of pluripotency marker expression within that cell. Define the root cell set as the subset of cells that minimize this Shannon entropy, with ties resolved by preferring larger total marker expression and then smaller cell index. Your task is to write a complete program that implements this definition exactly and returns the selected index or indices for each of several test cases.\n\nFormally, let there be $N$ cells and $G$ genes. Let $X \\in \\mathbb{R}_{\\ge 0}^{N \\times G}$ be the nonnegative expression matrix with entry $x_{i,g}$ denoting the expression of gene $g \\in \\{0,\\dots,G-1\\}$ in cell $i \\in \\{0,\\dots,N-1\\}$. Let $M \\subseteq \\{0,\\dots,G-1\\}$ be the nonempty set of indices of pluripotency marker genes, and let $|M| = K \\ge 1$. For a given nonnegative pseudocount $\\alpha \\in \\mathbb{R}_{\\ge 0}$, define for each cell $i$ the marker vector $v_i \\in \\mathbb{R}_{\\ge 0}^{K}$ with components $v_{i,g} = x_{i,g}$ for $g \\in M$, the smoothed marker vector $w_i$ with components $w_{i,g} = v_{i,g} + \\alpha$, and the normalization constant $s_i = \\sum_{g \\in M} w_{i,g}$. If $s_i > 0$, define the distribution $p_i$ with components $p_{i,g} = \\dfrac{w_{i,g}}{s_i}$. If $s_i = 0$, define $p_i$ to be the uniform distribution on $M$, that is $p_{i,g} = \\dfrac{1}{K}$ for all $g \\in M$. Define the Shannon entropy (using the natural logarithm) for cell $i$ as\n$$\nH_i = - \\sum_{g \\in M} p_{i,g} \\log p_{i,g},\n$$\nwith the usual convention that for any $q \\in [0,1]$, $q \\log q = 0$ when $q = 0$. Define also the total marker expression without pseudocount as\n$$\nS_i = \\sum_{g \\in M} v_{i,g} = \\sum_{g \\in M} x_{i,g}.\n$$\nGiven a target number of roots $r \\in \\{1,2,\\dots,N\\}$, define the selected root set $R$ to be the subset of $\\{0,\\dots,N-1\\}$ of size $r$ satisfying the following lexicographic minimization: order all cells by increasing $H_i$, breaking ties by decreasing $S_i$, and then breaking any remaining ties by increasing index $i$. Take the first $r$ cells under this order to form $R$. The output for a given test case is either the single integer index in $\\{0,\\dots,N-1\\}$ when $r=1$, or the list of indices in strictly increasing numerical order when $r>1$.\n\nYour program must implement these definitions precisely for the following test suite. In all cases, indices are zero-based.\n\n- Test case $1$ (general case with smoothing):\n  - $X = \\begin{bmatrix}\n  10 & 10 & 0 \\\\\n  20 & 0 & 0 \\\\\n  0 & 20 & 0 \\\\\n  5 & 5 & 10\n  \\end{bmatrix}$,\n  - $M = \\{0,1\\}$,\n  - $\\alpha = 1$,\n  - $r = 1$.\n- Test case $2$ (explicit tie in entropy, resolved by larger total marker expression):\n  - $X = \\begin{bmatrix}\n  8 & 2 \\\\\n  16 & 4 \\\\\n  5 & 5\n  \\end{bmatrix}$,\n  - $M = \\{0,1\\}$,\n  - $\\alpha = 0$,\n  - $r = 1$.\n- Test case $3$ (boundary with all-zero marker vector in a cell handled by smoothing):\n  - $X = \\begin{bmatrix}\n  0 & 0 \\\\\n  0 & 10 \\\\\n  0 & 1\n  \\end{bmatrix}$,\n  - $M = \\{0,1\\}$,\n  - $\\alpha = 1$,\n  - $r = 1$.\n- Test case $4$ (multiple roots requested):\n  - $X = \\begin{bmatrix}\n  10 & 0 & 0 \\\\\n  0 & 10 & 0 \\\\\n  3 & 3 & 3 \\\\\n  6 & 4 & 0 \\\\\n  9 & 1 & 0\n  \\end{bmatrix}$,\n  - $M = \\{0,1,2\\}$,\n  - $\\alpha = 1$,\n  - $r = 2$.\n\nYour program should produce a single line of output containing the results of the four test cases aggregated in order as a comma-separated list enclosed in square brackets. Each element must be either an integer (for $r=1$) or a list of integers in increasing order (for $r>1$). For example, an output may look like\n$[a,b,c,[d,e]]$\nwhere $a$, $b$, $c$ are integers and $[d,e]$ is a list of integers. No additional text should be printed.", "solution": "The problem presents a well-defined computational task grounded in the field of bioinformatics, specifically for identifying root cells in a single-cell dataset. The definitions are mathematically precise, the parameters are fully specified for each test case, and the required selection criteria are unambiguous. The problem is therefore valid and permits a direct algorithmic solution.\n\nThe objective is to identify a set of $r$ root cells from a total of $N$ cells. The selection is based on a lexicographical ordering where cells with the most concentrated expression profile across a predefined set of pluripotency marker genes are prioritized. This concentration is quantified by the Shannon entropy $H_i$ of a probability distribution derived from the cell's marker gene expression levels.\n\nThe procedure is executed as follows: for each cell $i \\in \\{0, \\dots, N-1\\}$, we compute two metrics that form the basis for sorting: the Shannon entropy $H_i$ and the total marker expression $S_i$.\n\nFirst, we compute the Shannon entropy $H_i$ for each cell $i$:\n1.  From the expression matrix $X \\in \\mathbb{R}_{\\ge 0}^{N \\times G}$, we extract the marker expression vector $v_i \\in \\mathbb{R}_{\\ge 0}^{K}$ for cell $i$. Its components are $v_{i,g} = x_{i,g}$ for all gene indices $g$ in the marker set $M$, where $K = |M|$.\n2.  A nonnegative pseudocount $\\alpha \\ge 0$ is added to each component of $v_i$ to form a smoothed vector $w_i$, with components $w_{i,g} = v_{i,g} + \\alpha$. This step ensures that no probabilities are zero when $\\alpha > 0$, avoiding issues with logarithms, and handles the case where a cell has zero expression for all markers.\n3.  The vector $w_i$ is normalized to create a probability distribution $p_i$. The normalization constant is $s_i = \\sum_{g \\in M} w_{i,g}$.\n4.  If $s_i > 0$, the probability distribution $p_i$ has components $p_{i,g} = \\dfrac{w_{i,g}}{s_i}$. In the specific case where $s_i = 0$, which can only happen if $\\alpha=0$ and all marker expressions $x_{i,g}$ for $g \\in M$ are zero, the distribution $p_i$ is defined as the uniform distribution, i.e., $p_{i,g} = \\dfrac{1}{K}$ for all $g \\in M$.\n5.  The Shannon entropy of this distribution is then calculated using the natural logarithm, as specified:\n    $$\n    H_i = - \\sum_{g \\in M} p_{i,g} \\log p_{i,g}\n    $$\n    The convention $q \\log q = 0$ for $q=0$ is respected. A lower value of $H_i$ signifies a more concentrated, less uniform expression profile over the marker genes.\n\nSecond, we compute the total marker expression $S_i$ for each cell $i$. This is the simple sum of the raw, non-smoothed marker expression values:\n$$\nS_i = \\sum_{g \\in M} x_{i,g}\n$$\nThis metric is used as the primary tie-breaker.\n\nWith $H_i$ and $S_i$ computed for all cells, we establish a unique ordering of the cells. The sorting is performed lexicographically based on a tuple of criteria for each cell $i$:\n1.  Primary criterion: Ascending order of Shannon entropy $H_i$.\n2.  Secondary criterion (tie-breaker): Descending order of total marker expression $S_i$.\n3.  Tertiary criterion (final tie-breaker): Ascending order of cell index $i$.\n\nThis sorting process can be implemented by ordering a list of tuples $(H_i, -S_i, i)$ for all $i \\in \\{0, \\dots, N-1\\}$.\n\nFinally, the root set $R$ is formed by selecting the first $r$ cells from this uniquely sorted list. The output for a given test case is the index (for $r=1$) or list of indices (for $r>1$) corresponding to these selected cells. If $r>1$, the indices in the final output must be presented in strictly increasing numerical order.\n\nThe implementation is written in Python, utilizing the `numpy` library for efficient array operations and `scipy.stats.entropy` for a robust calculation of the Shannon entropy. The algorithm follows the steps precisely as described above for each provided test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import entropy\n\ndef find_roots(X_raw, M, alpha, r):\n    \"\"\"\n    Identifies root cells based on Shannon entropy and total marker expression.\n\n    Args:\n        X_raw (list of lists): The N x G gene expression matrix.\n        M (set): The set of marker gene indices.\n        alpha (float): The nonnegative pseudocount.\n        r (int): The number of root cells to select.\n\n    Returns:\n        int or list of int: The index or list of indices of the root cells.\n    \"\"\"\n    X = np.array(X_raw, dtype=float)\n    marker_indices = sorted(list(M))\n    num_cells = X.shape[0]\n    K = len(marker_indices)\n\n    cell_metrics = []\n\n    for i in range(num_cells):\n        # Extract marker vector v_i and calculate total marker expression S_i\n        v_i = X[i, marker_indices]\n        S_i = np.sum(v_i)\n\n        # Calculate smoothed marker vector w_i and normalization constant s_i\n        w_i = v_i + alpha\n        s_i = np.sum(w_i)\n\n        # Define probability distribution p_i\n        if s_i > 0:\n            p_i = w_i / s_i\n        else:\n            # This case occurs iff alpha=0 and all relevant x_i,g are 0.\n            # Define p_i as the uniform distribution on M.\n            p_i = np.full(K, 1.0 / K)\n\n        # Calculate Shannon entropy H_i using natural logarithm\n        H_i = entropy(p_i, base=np.e)\n\n        # Store metrics for lexicographical sorting: (H_i, -S_i, i)\n        # We use -S_i because the tie-breaker is in descending order of S_i.\n        cell_metrics.append((H_i, -S_i, i))\n\n    # Sort cells based on the specified criteria\n    cell_metrics.sort()\n\n    # Select the top r cell indices\n    root_indices = [metric[2] for metric in cell_metrics[:r]]\n\n    # Format the output as per the problem description\n    if r == 1:\n        return root_indices[0]\n    else:\n        # Return indices in strictly increasing numerical order\n        return sorted(root_indices)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"X\": [[10, 10, 0], [20, 0, 0], [0, 20, 0], [5, 5, 10]],\n            \"M\": {0, 1}, \"alpha\": 1, \"r\": 1\n        },\n        {\n            \"X\": [[8, 2], [16, 4], [5, 5]],\n            \"M\": {0, 1}, \"alpha\": 0, \"r\": 1\n        },\n        {\n            \"X\": [[0, 0], [0, 10], [0, 1]],\n            \"M\": {0, 1}, \"alpha\": 1, \"r\": 1\n        },\n        {\n            \"X\": [[10, 0, 0], [0, 10, 0], [3, 3, 3], [6, 4, 0], [9, 1, 0]],\n            \"M\": {0, 1, 2}, \"alpha\": 1, \"r\": 2\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = find_roots(case[\"X\"], case[\"M\"], case[\"alpha\"], case[\"r\"])\n        results.append(result)\n\n    # Helper function to format results into the required string format\n    def format_result(res):\n        if isinstance(res, list):\n            # Format list as '[d,e]' without spaces\n            return f\"[{','.join(map(str, res))}]\"\n        else:\n            return str(res)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(format_result, results))}]\")\n\nsolve()\n```", "id": "2437518"}]}