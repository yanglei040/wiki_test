{"hands_on_practices": [{"introduction": "Metagenome assembly often produces a fragmented collection of sequences called contigs. A crucial next step, known as scaffolding, is to determine the relative order and orientation of these contigs. This practice will guide you through a fundamental scaffolding approach: using paired-end read information to build a graph where contigs are nodes and the number of linking read pairs determines the edge weights, revealing the large-scale structure of the underlying genomes. [@problem_id:2405168]", "problem": "You are given a finite set of contigs and a finite set of paired-end (PE) read mappings at contig ends. Model the scaffold inference as a simple undirected weighted graph. Let the set of contigs be denoted by $C=\\{1,2,\\dots,n\\}$, where each element is an integer identifier. Let the set of paired-end read mappings be denoted by $P$, where each element is an ordered $6$-tuple $\\langle c_1,e_1,s_1,c_2,e_2,s_2\\rangle$ with the following components: $c_1\\in C$, $c_2\\in C$, $e_1\\in\\{\\mathrm{L},\\mathrm{R}\\}$, $e_2\\in\\{\\mathrm{L},\\mathrm{R}\\}$, $s_1\\in\\{+,-\\}$, and $s_2\\in\\{+,-\\}$. The symbol $\\mathrm{L}$ denotes the left end of a contig and $\\mathrm{R}$ denotes the right end of a contig; the symbols $+$ and $-$ denote the forward and reverse read orientations, respectively, relative to the contig reference strand. Assume a standard inward-facing fragment orientation model for paired-end sequencing (commonly termed FR): a valid linkage between distinct contigs arises from a read pair when one mate maps to a right end with forward orientation and the other maps to a left end with reverse orientation. Formally, a read pair $\\langle c_1,e_1,s_1,c_2,e_2,s_2\\rangle$ supports a linkage if and only if $c_1\\neq c_2$ and either $(e_1=\\mathrm{R}\\wedge s_1=+ \\wedge e_2=\\mathrm{L}\\wedge s_2=-)$ or $(e_1=\\mathrm{L}\\wedge s_1=- \\wedge e_2=\\mathrm{R}\\wedge s_2=+)$ holds.\n\nDefine the scaffold graph $G=(V,E)$ with $V=C$. For each unordered pair $\\{u,v\\}$ with $u,v\\in C$ and $u\\neq v$, define the edge weight $w(u,v)$ to be the count of read pairs in $P$ that support a linkage between $u$ and $v$ according to the rule above. Given a nonnegative integer threshold parameter $\\tau$, include an undirected edge between $u$ and $v$ in $E$ if and only if $w(u,v)\\ge \\tau$. The output representation for $G$ must be a list of triples, where each triple is the list $[u,v,w(u,v)]$ with $uv$, and the set of triples is sorted in ascending lexicographic order by $(u,v)$.\n\nYour task is to write a program that, for each of the following test cases, constructs the corresponding scaffold graph and outputs, for each case, the sorted list of edge triples as specified. Aggregate the results for all test cases into a single list in test-case order.\n\nTest case $1$:\n- Contigs: $C=\\{1,2,3\\}$.\n- Threshold: $\\tau=2$.\n- Paired mappings $P$ given as the following $6$-tuples:\n  $\\langle 1,\\mathrm{R},+,2,\\mathrm{L},-\\rangle$,\n  $\\langle 2,\\mathrm{L},-,1,\\mathrm{R},+\\rangle$,\n  $\\langle 1,\\mathrm{R},+,2,\\mathrm{L},-\\rangle$,\n  $\\langle 1,\\mathrm{L},+,2,\\mathrm{R},-\\rangle$,\n  $\\langle 1,\\mathrm{R},+,3,\\mathrm{L},-\\rangle$,\n  $\\langle 1,\\mathrm{R},+,1,\\mathrm{L},-\\rangle$.\n\nTest case $2$:\n- Contigs: $C=\\{1,2\\}$.\n- Threshold: $\\tau=1$.\n- Paired mappings $P$:\n  $\\langle 1,\\mathrm{L},+,2,\\mathrm{R},-\\rangle$,\n  $\\langle 1,\\mathrm{R},+,1,\\mathrm{L},-\\rangle$.\n\nTest case $3$:\n- Contigs: $C=\\{1,2,3,4\\}$.\n- Threshold: $\\tau=1$.\n- Paired mappings $P$:\n  $\\langle 1,\\mathrm{R},+,2,\\mathrm{L},-\\rangle$,\n  $\\langle 2,\\mathrm{L},-,1,\\mathrm{R},+\\rangle$,\n  $\\langle 2,\\mathrm{R},+,3,\\mathrm{L},-\\rangle$,\n  $\\langle 4,\\mathrm{L},-,3,\\mathrm{R},+\\rangle$,\n  $\\langle 3,\\mathrm{R},+,4,\\mathrm{L},-\\rangle$,\n  $\\langle 2,\\mathrm{R},+,4,\\mathrm{L},-\\rangle$,\n  $\\langle 1,\\mathrm{L},+,3,\\mathrm{R},-\\rangle$,\n  $\\langle 3,\\mathrm{R},+,3,\\mathrm{L},-\\rangle$.\n\nTest case $4$:\n- Contigs: $C=\\{1,2,3\\}$.\n- Threshold: $\\tau=3$.\n- Paired mappings $P$:\n  $\\langle 1,\\mathrm{R},+,2,\\mathrm{L},-\\rangle$,\n  $\\langle 2,\\mathrm{L},-,1,\\mathrm{R},+\\rangle$,\n  $\\langle 1,\\mathrm{R},+,2,\\mathrm{L},-\\rangle$,\n  $\\langle 1,\\mathrm{L},+,2,\\mathrm{R},-\\rangle$,\n  $\\langle 3,\\mathrm{R},+,2,\\mathrm{L},-\\rangle$,\n  $\\langle 2,\\mathrm{L},-,3,\\mathrm{R},+\\rangle$.\n\nFinal output format: Your program should produce a single line of output containing a list of length $4$, where the $i$-th element is the list of edge triples $[u,v,w]$ for test case $i$, sorted by $(u,v)$. The overall line must be a single bracketed list with elements separated by commas, for example, $[[\\cdots],[\\cdots],[\\cdots],[\\cdots]]$. All identifiers in the output are integers; do not print any strings, units, or additional text.", "solution": "The problem is valid. It is scientifically grounded in the principles of bioinformatics, specifically genome assembly, and is well-posed with a clear, objective definition of the task.\n\nThe objective is to construct a scaffold graph $G=(V, E)$ based on a set of contigs $C$ and paired-end read mappings $P$. The vertices $V$ of the graph are the contigs themselves. The edges $E$ and their corresponding weights are determined by analyzing the paired-end read data according to a specified biological model.\n\nThe algorithmic procedure to construct this graph is outlined as follows:\n\n1.  **Initialization of Edge Weights**: The graph is undirected, so for every unordered pair of distinct contigs $\\{u, v\\}$ from the set $C=\\{1, 2, \\dots, n\\}$, we need to compute an associated weight $w(u,v)$. A suitable data structure for this task is an associative array (or a dictionary), which maps a canonical representation of the edge, such as an ordered tuple $(\\min(u,v), \\max(u,v))$, to its integer weight. All potential weights are implicitly initialized to $0$.\n\n2.  **Processing Paired-End Mappings**: We iterate through each paired-end read mapping $\\langle c_1,e_1,s_1,c_2,e_2,s_2\\rangle$ provided in the set $P$. For each mapping, we must determine if it constitutes a valid linkage between two different contigs.\n\n3.  **Validation of Linkage Support**: A read pair provides evidence for a scaffold linkage if it adheres to the standard inward-facing fragment orientation model, also known as the FR model. The problem formalizes this condition precisely. A read pair $\\langle c_1,e_1,s_1,c_2,e_2,s_2\\rangle$ supports a linkage between contigs $c_1$ and $c_2$ if and only if two criteria are met:\n    a. The contigs must be distinct, i.e., $c_1 \\neq c_2$. Mappings within the same contig are disregarded for scaffolding.\n    b. The mapping configuration must match one of two symmetrical patterns:\n        i.  $(e_1=\\mathrm{R} \\wedge s_1=+ \\wedge e_2=\\mathrm{L} \\wedge s_2=-)$: One mate maps to the right end of contig $c_1$ with a forward orientation, and the other mate maps to the left end of contig $c_2$ with a reverse orientation.\n        ii. $(e_1=\\mathrm{L} \\wedge s_1=- \\wedge e_2=\\mathrm{R} \\wedge s_2=+)$: The symmetric case where one mate maps to the left end of $c_1$ (reverse) and the other to the right end of $c_2$ (forward).\n    Physically, these patterns suggest that contigs $c_1$ and $c_2$ are adjacent in the source genome, with the right end of one proximal to the left end of the other.\n\n4.  **Weight Aggregation**: If a read mapping is validated as a supporting linkage between contigs $c_1$ and $c_2$, we increment the weight associated with this pair. Using the canonical representation, we update the weight $w(\\min(c_1, c_2), \\max(c_1, c_2))$. This process is repeated for all mappings in $P$, aggregating the total evidence for each potential linkage.\n\n5.  **Graph Construction via Thresholding**: After processing all mappings, we have a complete set of raw edge weights. The final set of edges $E$ for the graph $G$ is determined by a given non-negative integer threshold $\\tau$. An edge $\\{u, v\\}$ is included in $E$ if and only if its aggregated weight $w(u, v) \\ge \\tau$. This step filters out linkages with insufficient evidence, retaining only those supported by a substantial number of read pairs.\n\n6.  **Output Generation**: The final representation of the graph is required as a list of triples $[u, v, w(u,v)]$, where $u  v$. This list must be sorted in ascending lexicographical order based on the pair $(u, v)$. This is accomplished by iterating through the keys of our weight dictionary (which are already canonical and can be sorted), filtering them based on the threshold $\\tau$, and formatting the resulting edges as specified. The results from all test cases are then aggregated into a single list.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport collections\n\ndef solve():\n    \"\"\"\n    Solves the scaffold graph construction problem for the given test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"C\": {1, 2, 3}, \"tau\": 2, \"P\": [\n                (1, 'R', '+', 2, 'L', '-'),\n                (2, 'L', '-', 1, 'R', '+'),\n                (1, 'R', '+', 2, 'L', '-'),\n                (1, 'L', '+', 2, 'R', '-'),\n                (1, 'R', '+', 3, 'L', '-'),\n                (1, 'R', '+', 1, 'L', '-')\n            ]\n        },\n        {\n            \"C\": {1, 2}, \"tau\": 1, \"P\": [\n                (1, 'L', '+', 2, 'R', '-'),\n                (1, 'R', '+', 1, 'L', '-')\n            ]\n        },\n        {\n            \"C\": {1, 2, 3, 4}, \"tau\": 1, \"P\": [\n                (1, 'R', '+', 2, 'L', '-'),\n                (2, 'L', '-', 1, 'R', '+'),\n                (2, 'R', '+', 3, 'L', '-'),\n                (4, 'L', '-', 3, 'R', '+'),\n                (3, 'R', '+', 4, 'L', '-'),\n                (2, 'R', '+', 4, 'L', '-'),\n                (1, 'L', '+', 3, 'R', '-'),\n                (3, 'R', '+', 3, 'L', '-')\n            ]\n        },\n        {\n            \"C\": {1, 2, 3}, \"tau\": 3, \"P\": [\n                (1, 'R', '+', 2, 'L', '-'),\n                (2, 'L', '-', 1, 'R', '+'),\n                (1, 'R', '+', 2, 'L', '-'),\n                (1, 'L', '+', 2, 'R', '-'),\n                (3, 'R', '+', 2, 'L', '-'),\n                (2, 'L', '-', 3, 'R', '+')\n            ]\n        }\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        tau = case[\"tau\"]\n        mappings = case[\"P\"]\n        weights = collections.defaultdict(int)\n\n        for mapping in mappings:\n            c1, e1, s1, c2, e2, s2 = mapping\n            \n            # A read pair supports a linkage if and only if c1 != c2 and \n            # either (e1=R ^ s1=+ ^ e2=L ^ s2=-) or (e1=L ^ s1=- ^ e2=R ^ s2=+).\n            if c1 == c2:\n                continue\n\n            valid_link = False\n            if (e1 == 'R' and s1 == '+' and e2 == 'L' and s2 == '-') or \\\n               (e1 == 'L' and s1 == '-' and e2 == 'R' and s2 == '+'):\n                valid_link = True\n            \n            # The problem logic is symmetrical, check the other direction.\n            elif (e2 == 'R' and s2 == '+' and e1 == 'L' and s1 == '-') or \\\n                 (e2 == 'L' and s2 == '-' and e1 == 'R' and s1 == '+'):\n                 valid_link = True\n\n            if valid_link:\n                u, v = min(c1, c2), max(c1, c2)\n                weights[(u, v)] += 1\n\n        # Filter edges by threshold and sort\n        edge_list = []\n        # Sort keys for lexicographical order\n        sorted_keys = sorted(weights.keys())\n        \n        for u, v in sorted_keys:\n            w = weights[(u, v)]\n            if w >= tau:\n                edge_list.append([u, v, w])\n        \n        all_results.append(edge_list)\n\n    # Format the final output string to be completely space-free within lists.\n    outer_parts = []\n    for case_result in all_results:\n        inner_parts = []\n        for triple in case_result:\n            inner_parts.append(f\"[{triple[0]},{triple[1]},{triple[2]}]\")\n        outer_parts.append(f\"[{','.join(inner_parts)}]\")\n    \n    final_output_string = f\"[{','.join(outer_parts)}]\"\n    print(final_output_string)\n\nsolve()\n```", "id": "2405168"}, {"introduction": "An ideal contig represents a single, contiguous stretch of a real genome, but assembly errors can create \"chimeras\" by incorrectly joining distinct sequences. Detecting these chimeras is a critical quality control step. In this exercise, you will implement a quantitative chimericity score based on the principle that a valid contig should exhibit consistent sequence composition and coverage depth along its length; sudden shifts in these signals are strong indicators of a misassembly. [@problem_id:2405143]", "problem": "You are given the task of defining and computing a quantitative metric that measures whether a contiguous assembled sequence (a contig) is likely to be a chimera by jointly assessing the consistency of its $k$-mer composition and its sequencing coverage along its length. Consider a contig as a string $S$ over the deoxyribonucleic acid (DNA) alphabet $\\{A,C,G,T\\}$, of length $L$, indexed by positions $1,2,\\dots,L$, and a nonnegative per-base coverage vector $D \\in \\mathbb{R}_{\\ge 0}^L$ whose component $D_j$ gives the coverage at position $j$.\n\nDefine the following, for a given positive integer window length $W$ and a positive integer $k$-mer length $k$ with alphabet size $|\\Sigma|=4$:\n1. Partition the contig into $M=\\left\\lfloor \\frac{L}{W} \\right\\rfloor$ non-overlapping windows of equal length $W$: window $i$ spans positions $((i-1)W+1)$ through $(iW)$ for $i \\in \\{1,2,\\dots,M\\}$. If $M2$, define the final score to be $0$ by convention.\n2. For each window $i$, define the $k$-mer count function $c_i: \\Sigma^k \\to \\mathbb{N}_0$ as the number of occurrences of each exact $k$-mer (with stride $1$) entirely contained within the window. Let $N_i=\\max\\{W-k+1,0\\}$ be the number of $k$-mer positions in each window when $W \\ge k$ (and $0$ otherwise). Define the normalized $k$-mer frequency vector $p_i \\in \\mathbb{R}^{4^k}$ by $p_i(w)=\\frac{c_i(w)}{\\max\\{N_i,1\\}}$ for each $w \\in \\Sigma^k$.\n3. Define the composition inconsistency between adjacent windows $i$ and $(i+1)$ as the Euclidean distance\n$$\nd_i=\\left\\|p_i-p_{i+1}\\right\\|_2=\\sqrt{\\sum_{w \\in \\Sigma^k}\\left(p_i(w)-p_{i+1}(w)\\right)^2}.\n$$\nNormalize this using the bound $\\max d_i=\\sqrt{2}$ for distributions, and define the average normalized composition inconsistency\n$$\nI_{\\mathrm{comp}}=\\frac{1}{M-1}\\sum_{i=1}^{M-1}\\frac{d_i}{\\sqrt{2}}.\n$$\n4. Define the mean coverage in window $i$ as\n$$\n\\bar{c}_i=\\frac{1}{W}\\sum_{j=(i-1)W+1}^{iW} D_j.\n$$\nDefine the relative coverage inconsistency between adjacent windows $i$ and $(i+1)$ as\n$$\nr_i=\n\\begin{cases}\n0,  \\text{if } \\bar{c}_i+\\bar{c}_{i+1}=0, \\\\[4pt]\n\\frac{\\left|\\bar{c}_{i+1}-\\bar{c}_i\\right|}{\\bar{c}_{i+1}+\\bar{c}_i},  \\text{otherwise.}\n\\end{cases}\n$$\nDefine the average coverage inconsistency\n$$\nI_{\\mathrm{cov}}=\\frac{1}{M-1}\\sum_{i=1}^{M-1} r_i.\n$$\n5. Define the chimericity score as the convex combination\n$$\n\\mathrm{Score}=\\alpha I_{\\mathrm{comp}}+(1-\\alpha)I_{\\mathrm{cov}},\n$$\nwith $\\alpha=\\frac{1}{2}$.\n\nCompute the chimericity score for each of the following test cases. For each case, unless otherwise stated, use $k=3$, window length $W=30$, non-overlapping windows as above, and $\\alpha=\\frac{1}{2}$. All sequences are over $\\{A,C,G,T\\}$, and all coverages are nonnegative real values. Construct the sequences and coverage vectors exactly as specified:\n\n- Test case $1$ (balanced composition and uniform coverage):\n  - Length $L_1=120$.\n  - Sequence $S_1$: the first $L_1$ symbols of the infinite periodic string $\\text{\"ACGTACGT}\\dots\\text{\"}$.\n  - Coverage $D^{(1)}$: $D^{(1)}_j=30$ for all $j \\in \\{1,\\dots,L_1\\}$.\n\n- Test case $2$ (composition and coverage shift at midpoint):\n  - Length $L_2=120$.\n  - Sequence $S_2$: positions $1$ through $60$ are the first $60$ symbols of the infinite periodic string $\\text{\"GCGCGC}\\dots\\text{\"}$; positions $61$ through $120$ are the first $60$ symbols of the infinite periodic string $\\text{\"ATATAT}\\dots\\text{\"}$.\n  - Coverage $D^{(2)}$: $D^{(2)}_j=40$ for $j \\in \\{1,\\dots,60\\}$ and $D^{(2)}_j=10$ for $j \\in \\{61,\\dots,120\\}$.\n\n- Test case $3$ (short contig, fewer than two windows):\n  - Length $L_3=20$.\n  - Sequence $S_3$: the first $L_3$ symbols of the infinite periodic string $\\text{\"ACGTACGT}\\dots\\text{\"}$.\n  - Coverage $D^{(3)}$: $D^{(3)}_j=20$ for all $j \\in \\{1,\\dots,L_3\\}$.\n\n- Test case $4$ (identical composition across two windows, slightly different coverage):\n  - Length $L_4=60$.\n  - Sequence $S_4$: positions $1$ through $30$ are the first $30$ symbols of the infinite periodic string $\\text{\"ACGTACGT}\\dots\\text{\"}$; positions $31$ through $60$ are the first $30$ symbols of the same infinite periodic string.\n  - Coverage $D^{(4)}$: $D^{(4)}_j=20$ for $j \\in \\{1,\\dots,30\\}$ and $D^{(4)}_j=22$ for $j \\in \\{31,\\dots,60\\}$.\n\n- Test case $5$ (three windows with a composition change in the middle, uniform coverage):\n  - Length $L_5=90$.\n  - Sequence $S_5$: positions $1$ through $30$ are the first $30$ symbols of the infinite periodic string $\\text{\"ATATAT}\\dots\\text{\"}$; positions $31$ through $60$ are the first $30$ symbols of the infinite periodic string $\\text{\"GCGCGC}\\dots\\text{\"}$; positions $61$ through $90$ are the first $30$ symbols of the infinite periodic string $\\text{\"ATATAT}\\dots\\text{\"}$.\n  - Coverage $D^{(5)}$: $D^{(5)}_j=30$ for all $j \\in \\{1,\\dots,90\\}$.\n\nYour program must compute the chimericity score for each test case and produce a single line of output containing the results as a comma-separated list of real numbers in the exact order of the test cases, enclosed in square brackets. Each real number must be rounded to $6$ decimal places using standard rounding to nearest, ties to even are not required. The final output must therefore be of the form\n$[\\text{score}_1,\\text{score}_2,\\text{score}_3,\\text{score}_4,\\text{score}_5]$,\nwith each $\\text{score}_i$ rounded to $6$ decimal places.", "solution": "The problem statement is scientifically grounded, mathematically well-posed, and internally consistent. It provides a complete set of definitions, parameters, and test cases required to compute a deterministic chimericity score for DNA contigs. The defined metric is a plausible approach for identifying chimeric sequences in metagenome assembly by quantifying discontinuities in both sequence composition and sequencing coverage. The problem is therefore deemed valid.\n\nThe solution involves a direct implementation of the provided formulas. For each test case, we compute the chimericity score using the specified parameters: $k$-mer length $k=3$, window size $W=30$, and convex combination weight $\\alpha=\\frac{1}{2}$.\n\nThe core of the algorithm is as follows:\n$1$. The contig of length $L$ is partitioned into $M=\\lfloor L/W \\rfloor$ non-overlapping windows. If $M  2$, the score is $0$ by definition.\n$2$. For each window $i \\in \\{1, \\dots, M\\}$, we compute a normalized $k$-mer frequency vector $p_i$ and the mean coverage $\\bar{c}_i$. The vector $p_i$ has $4^k = 4^3 = 64$ components. The number of $k$-mers in a window is $N_i = W-k+1 = 30-3+1 = 28$.\n$3$. We then compute two forms of inconsistency between adjacent windows $i$ and $i+1$, for $i \\in \\{1, \\dots, M-1\\}$:\n    a. Composition inconsistency: $d_i = \\|p_i - p_{i+1}\\|_2$.\n    b. Relative coverage inconsistency: $r_i = \\frac{|\\bar{c}_{i+1}-\\bar{c}_i|}{\\bar{c}_{i+1}+\\bar{c}_i}$, with $r_i=0$ if $\\bar{c}_i+\\bar{c}_{i+1}=0$.\n$4$. These are averaged over all $M-1$ window transitions to get $I_{\\mathrm{comp}} = \\frac{1}{M-1}\\sum_{i=1}^{M-1}\\frac{d_i}{\\sqrt{2}}$ and $I_{\\mathrm{cov}} = \\frac{1}{M-1}\\sum_{i=1}^{M-1}r_i$.\n$5$. The final score is a weighted sum: $\\mathrm{Score} = \\alpha I_{\\mathrm{comp}} + (1-\\alpha)I_{\\mathrm{cov}}$.\n\nWe now apply this procedure to each test case.\n\n**Test Case 1:**\n- Contig length $L_1=120$, window size $W=30$, so we have $M = \\lfloor 120/30 \\rfloor = 4$ windows.\n- The sequence $S_1$ is a repetition of \"ACGT\". Each window is a cyclic shift of the others. The number of occurrences for each of the $k$-mers \"ACG\", \"CGT\", \"GTA\", and \"TAC\" is $7$ within each window of $28$ total $k$-mers. All other $k$-mer counts are $0$.\n- Consequently, the normalized $k$-mer frequency vectors are identical for all windows: $p_1 = p_2 = p_3 = p_4$. Each of these vectors has $4$ non-zero components, each with value $7/28 = 1/4$.\n- The composition inconsistencies are $d_i = \\|p_i - p_{i+1}\\|_2 = 0$ for $i=1, 2, 3$. This gives $I_{\\mathrm{comp}} = 0$.\n- The coverage $D^{(1)}_j = 30$ is uniform. The mean coverage for every window is $\\bar{c}_i = 30$.\n- The relative coverage inconsistencies are $r_i = 0$ for all $i$. This gives $I_{\\mathrm{cov}} = 0$.\n- The final score is $\\mathrm{Score}_1 = \\frac{1}{2}(0) + \\frac{1}{2}(0) = 0$.\n\n**Test Case 2:**\n- Contig length $L_2=120$, $W=30$, resulting in $M=4$ windows.\n- Windows $1$ and $2$ consist of the sequence \"GCGCGC...\", while windows $3$ and $4$ consist of \"ATATAT...\".\n- For windows $1$ and $2$, the only non-zero $k$-mer frequencies are for \"GCG\" and \"CGC\", each being $14/28 = 1/2$. Thus, $p_1=p_2$.\n- For windows $3$ and $4$, the only non-zero frequencies are for \"ATA\" and \"TAT\", each being $14/28 = 1/2$. Thus, $p_3=p_4$.\n- The composition inconsistencies are:\n    - $d_1 = \\|p_1-p_2\\|_2 = 0$.\n    - $d_2 = \\|p_2-p_3\\|_2 = \\sqrt{(1/2)^2 + (1/2)^2 + (-1/2)^2 + (-1/2)^2} = \\sqrt{4 \\cdot (1/4)} = 1$. The $k$-mer sets are disjoint.\n    - $d_3 = \\|p_3-p_4\\|_2 = 0$.\n- $I_{\\mathrm{comp}} = \\frac{1}{4-1} (\\frac{d_1}{\\sqrt{2}} + \\frac{d_2}{\\sqrt{2}} + \\frac{d_3}{\\sqrt{2}}) = \\frac{1}{3} (0 + \\frac{1}{\\sqrt{2}} + 0) = \\frac{1}{3\\sqrt{2}}$.\n- Coverage: $\\bar{c}_1 = \\bar{c}_2 = 40$ and $\\bar{c}_3 = \\bar{c}_4 = 10$.\n- The relative coverage inconsistencies are:\n    - $r_1 = \\frac{|40-40|}{40+40} = 0$.\n    - $r_2 = \\frac{|10-40|}{10+40} = \\frac{30}{50} = \\frac{3}{5}$.\n    - $r_3 = \\frac{|10-10|}{10+10} = 0$.\n- $I_{\\mathrm{cov}} = \\frac{1}{3} (0 + \\frac{3}{5} + 0) = \\frac{1}{5}$.\n- The final score is $\\mathrm{Score}_2 = \\frac{1}{2} I_{\\mathrm{comp}} + \\frac{1}{2} I_{\\mathrm{cov}} = \\frac{1}{2}(\\frac{1}{3\\sqrt{2}}) + \\frac{1}{2}(\\frac{1}{5}) = \\frac{1}{6\\sqrt{2}} + \\frac{1}{10} \\approx 0.217851$.\n\n**Test Case 3:**\n- Contig length $L_3=20$, $W=30$. The number of windows is $M = \\lfloor 20/30 \\rfloor = 0$.\n- As per the problem definition, if $M  2$, the score is $0$.\n- Thus, $\\mathrm{Score}_3 = 0$.\n\n**Test Case 4:**\n- Contig length $L_4=60$, $W=30$, giving $M = \\lfloor 60/30 \\rfloor = 2$ windows.\n- The sequence in both windows is identical (\"ACGT...\" for $30$ characters). This is identical to the windows in Test Case 1. Therefore, $p_1 = p_2$.\n- The composition inconsistency is $d_1 = \\|p_1-p_2\\|_2 = 0$, giving $I_{\\mathrm{comp}} = \\frac{1}{2-1}\\frac{0}{\\sqrt{2}}=0$.\n- The mean coverages are $\\bar{c}_1=20$ and $\\bar{c}_2=22$.\n- The relative coverage inconsistency is $r_1 = \\frac{|22-20|}{22+20} = \\frac{2}{42} = \\frac{1}{21}$.\n- $I_{\\mathrm{cov}} = \\frac{1}{2-1} r_1 = \\frac{1}{21}$.\n- The final score is $\\mathrm{Score}_4 = \\frac{1}{2}(0) + \\frac{1}{2}(\\frac{1}{21}) = \\frac{1}{42} \\approx 0.023810$.\n\n**Test Case 5:**\n- Contig length $L_5=90$, $W=30$, giving $M = 3$ windows.\n- Window $1$: \"ATATAT...\", Window $2$: \"GCGCGC...\", Window $3$: \"ATATAT...\".\n- From Test Case 2, we know that the $k$-mer profile for \"ATATAT...\" ($p_1, p_3$) is disjoint from that of \"GCGCGC...\" ($p_2$).\n- Therefore, $p_1=p_3$.\n- The composition inconsistencies are:\n    - $d_1 = \\|p_1-p_2\\|_2 = 1$.\n    - $d_2 = \\|p_2-p_3\\|_2 = \\|p_2-p_1\\|_2 = 1$.\n- $I_{\\mathrm{comp}} = \\frac{1}{3-1} (\\frac{d_1}{\\sqrt{2}} + \\frac{d_2}{\\sqrt{2}}) = \\frac{1}{2} (\\frac{1}{\\sqrt{2}} + \\frac{1}{\\sqrt{2}}) = \\frac{1}{\\sqrt{2}}$.\n- The coverage is uniform, $\\bar{c}_1 = \\bar{c}_2 = \\bar{c}_3 = 30$.\n- All relative coverage inconsistencies are $r_i=0$, so $I_{\\mathrm{cov}} = 0$.\n- The final score is $\\mathrm{Score}_5 = \\frac{1}{2}(\\frac{1}{\\sqrt{2}}) + \\frac{1}{2}(0) = \\frac{1}{2\\sqrt{2}} \\approx 0.353553$.", "answer": "```python\nimport numpy as np\nimport itertools\n\ndef compute_chimericity_score(S, D, L, k, W, alpha):\n    \"\"\"\n    Computes the chimericity score for a contig based on k-mer composition and coverage.\n    \"\"\"\n    M = L // W\n    if M  2:\n        return 0.0\n\n    kmer_positions = max(W - k + 1, 0)\n    # The denominator for k-mer frequencies, as per the problem statement\n    # p_i(w) = c_i(w) / max(N_i, 1) where N_i = W-k+1\n    kmer_freq_denominator = float(max(kmer_positions, 1))\n\n    alphabet = 'ACGT'\n    all_kmers = sorted([''.join(p) for p in itertools.product(alphabet, repeat=k)])\n    kmer_to_idx = {kmer: i for i, kmer in enumerate(all_kmers)}\n    num_total_kmers = len(all_kmers)\n\n    window_p_vectors = []\n    window_mean_coverages = []\n\n    for i in range(M):\n        start_idx = i * W\n        end_idx = start_idx + W\n        \n        window_seq = S[start_idx:end_idx]\n        \n        # Calculate k-mer frequency vector p_i\n        counts = np.zeros(num_total_kmers, dtype=float)\n        if kmer_positions  0:\n            for j in range(kmer_positions):\n                kmer = window_seq[j:j+k]\n                # In case of non-standard characters, though the problem restricts to ACGT\n                if kmer in kmer_to_idx:\n                    counts[kmer_to_idx[kmer]] += 1\n        \n        p_vector = counts / kmer_freq_denominator\n        window_p_vectors.append(p_vector)\n        \n        # Calculate mean coverage c_i\n        window_cov_data = D[start_idx:end_idx]\n        mean_cov = np.mean(window_cov_data)\n        window_mean_coverages.append(mean_cov)\n        \n    total_comp_inconsistency = 0.0\n    total_cov_inconsistency = 0.0\n    \n    for i in range(M - 1):\n        # Composition inconsistency\n        p_i = window_p_vectors[i]\n        p_i_plus_1 = window_p_vectors[i+1]\n        d_i = np.linalg.norm(p_i - p_i_plus_1)\n        total_comp_inconsistency += d_i / np.sqrt(2.0)\n        \n        # Coverage inconsistency\n        c_i = window_mean_coverages[i]\n        c_i_plus_1 = window_mean_coverages[i+1]\n        c_sum = c_i + c_i_plus_1\n        if c_sum == 0.0:\n            r_i = 0.0\n        else:\n            r_i = np.abs(c_i_plus_1 - c_i) / c_sum\n        total_cov_inconsistency += r_i\n    \n    I_comp = total_comp_inconsistency / (M - 1)\n    I_cov = total_cov_inconsistency / (M - 1)\n    \n    score = alpha * I_comp + (1 - alpha) * I_cov\n    return score\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and compute scores.\n    \"\"\"\n    k = 3\n    W = 30\n    alpha = 0.5\n\n    # Test case 1\n    L1 = 120\n    S1 = ('ACGT' * (L1 // 4 + 1))[:L1]\n    D1 = np.full(L1, 30.0)\n    \n    # Test case 2\n    L2 = 120\n    S2_part1 = ('GC' * (60 // 2 + 1))[:60]\n    S2_part2 = ('AT' * (60 // 2 + 1))[:60]\n    S2 = S2_part1 + S2_part2\n    D2 = np.concatenate([np.full(60, 40.0), np.full(60, 10.0)])\n    \n    # Test case 3\n    L3 = 20\n    S3 = ('ACGT' * (L3 // 4 + 1))[:L3]\n    D3 = np.full(L3, 20.0)\n    \n    # Test case 4\n    L4 = 60\n    S4_part = ('ACGT' * (30 // 4 + 2))[:30] # +2 ensures enough length\n    S4 = S4_part + S4_part\n    D4 = np.concatenate([np.full(30, 20.0), np.full(30, 22.0)])\n    \n    # Test case 5\n    L5 = 90\n    S5_part_AT = ('AT' * (30 // 2 + 1))[:30]\n    S5_part_GC = ('GC' ' * (30 // 2 + 1))[:30]\n    S5 = S5_part_AT + S5_part_GC + S5_part_AT\n    D5 = np.full(L5, 30.0)\n    \n    test_cases = [\n        (S1, D1, L1, k, W, alpha),\n        (S2, D2, L2, k, W, alpha),\n        (S3, D3, L3, k, W, alpha),\n        (S4, D4, L4, k, W, alpha),\n        (S5, D5, L5, k, W, alpha),\n    ]\n\n    results = []\n    for params in test_cases:\n        score = compute_chimericity_score(*params)\n        results.append(score)\n    \n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2405143"}, {"introduction": "Assembly graphs frequently contain \"bubbles,\" where the graph splits and then rejoins, representing a local sequence variation. The challenge is to distinguish true biological variants, such as a Single Nucleotide Polymorphism (SNP), from simple sequencing errors. This hands-on exercise formalizes this decision by having you implement an algorithm that evaluates multiple lines of evidence—including read depth, allelic balance, and the physical consistency of paired-end reads—to classify these ambiguous structures. [@problem_id:2405181]", "problem": "In metagenomic assembly, a \"bubble\" in a de Bruijn graph can arise either from a sequencing error or from a true Single Nucleotide Polymorphism (SNP). Consider a simplified formalization in which a bubble is modeled as two alternative directed paths between the same pair of vertices. Each path has a length, measured in base pairs (bp). Paired-end reads that span the bubble yield \"inner distances\" that, after alignment and standard adapter/read-length corrections, estimate the path length alone. Assume that, conditional on choosing the correct path, each observed inner distance is an independent sample from a Gaussian distribution centered at the true path length with a known standard deviation.\n\nFor each bubble, you are given:\n- Two path lengths $L_1$ and $L_2$ in bp.\n- Two finite sequences of observed inner distances (in bp), $\\{d_i^{(1)}\\}_{i=1}^{n_1}$ and $\\{d_j^{(2)}\\}_{j=1}^{n_2}$, where the superscript indicates which path the spanning read pair is consistent with.\n- A known standard deviation $\\sigma  0$ (in bp) of the Gaussian measurement noise for inner distances.\n\nDefine the following decision function $D$ that classifies a bubble as a true SNP (returning boolean true) or a sequencing error (returning boolean false):\n\n1. Minimum support per path: require $n_1 \\ge m$ and $n_2 \\ge m$ for a given integer threshold $m \\ge 1$.\n2. Balanced support: define $r = \\dfrac{n_1}{n_1 + n_2}$. Require $\\left| r - \\dfrac{1}{2} \\right| \\le \\varepsilon$ for a given tolerance $\\varepsilon \\in (0, \\tfrac{1}{2})$.\n3. Insert-size consistency: for each path $p \\in \\{1,2\\}$, define $z$-scores $z_i^{(p)} = \\dfrac{d_i^{(p)} - L_p}{\\sigma}$ for $i = 1,\\dots,n_p$. Let $\\bar{z}_p = \\dfrac{1}{n_p} \\sum_{i=1}^{n_p} \\left| z_i^{(p)} \\right|$. Require $\\max\\{\\bar{z}_1, \\bar{z}_2\\} \\le z_{\\mathrm{thr}}$ for a given threshold $z_{\\mathrm{thr}}  0$. If $n_p = 0$ for any $p$, interpret $\\bar{z}_p = +\\infty$.\n\nThe overall classification is:\n- Return true (classify as SNP) if and only if all three requirements above are satisfied.\n- Otherwise, return false (classify as sequencing error).\n\nAll distances are in base pairs (bp). There are no other physical units. The output values are booleans and therefore unitless.\n\nUse the following fixed thresholds for all cases: $\\varepsilon = 0.15$, $z_{\\mathrm{thr}} = 2.0$, and $m = 1$.\n\nYour task is to implement a program that applies the decision function $D$ to each of the following test cases and aggregates the boolean results.\n\nTest suite (each bullet is one case, listing $(L_1, L_2)$, $\\{d_i^{(1)}\\}_{i=1}^{n_1}$, $\\{d_j^{(2)}\\}_{j=1}^{n_2}$, and $\\sigma$):\n\n- Case 1: $(L_1, L_2) = (100, 101)$, $\\{d_i^{(1)}\\} = \\{98, 100, 102, 99\\}$, $\\{d_j^{(2)}\\} = \\{101, 100, 103, 102\\}$, $\\sigma = 3$.\n- Case 2: $(L_1, L_2) = (100, 101)$, $\\{d_i^{(1)}\\} = \\{99, 101, 100, 100, 102, 98, 97, 103, 101, 99\\}$, $\\{d_j^{(2)}\\} = \\{101\\}$, $\\sigma = 3$.\n- Case 3: $(L_1, L_2) = (180, 181)$, $\\{d_i^{(1)}\\} = \\{180, 179, 181, 182, 178, 180\\}$, $\\{d_j^{(2)}\\} = \\{190, 192, 189, 193\\}$, $\\sigma = 2$.\n- Case 4: $(L_1, L_2) = (250, 250)$, $\\{d_i^{(1)}\\} = \\{249\\}$, $\\{d_j^{(2)}\\} = \\{252\\}$, $\\sigma = 5$.\n- Case 5: $(L_1, L_2) = (150, 151)$, $\\{d_i^{(1)}\\} = \\{149, 152\\}$, $\\{d_j^{(2)}\\} = \\{151\\}$, $\\sigma = 5$.\n- Case 6: $(L_1, L_2) = (120, 121)$, $\\{d_i^{(1)}\\} = \\{121, 118, 119\\}$, $\\{d_j^{(2)}\\} = \\{\\}$, $\\sigma = 4$.\n\nRequired final output format: your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets with no spaces, in the order of the test cases above, for example, $\\left[\\text{true},\\text{false},\\dots\\right]$ but using the programming language's canonical boolean literals.", "solution": "The problem statement submitted for analysis is deemed valid. It presents a clear, self-contained, and scientifically grounded computational problem rooted in bioinformatics, specifically metagenomic assembly. The task is to implement a decision function $D$ for classifying bubbles in a de Bruijn graph as either true Single Nucleotide Polymorphisms (SNPs) or sequencing errors. The logic is deterministic and free of contradictions. I will now proceed with a formal solution.\n\nThe decision function $D$ is based on a conjunction of three conditions. A bubble is classified as a SNP (result is true) if and only if all three conditions are met. Otherwise, it is classified as a sequencing error (result is false). The conditions are evaluated using the provided constants: minimum support threshold $m = 1$, balanced support tolerance $\\varepsilon = 0.15$, and insert-size consistency threshold $z_{\\mathrm{thr}} = 2.0$.\n\nThe three conditions are:\n\n1.  **Minimum Support:** This condition requires that the number of reads supporting each of the two alternative paths, $n_1$ and $n_2$, must be greater than or equal to a minimum threshold $m$. The condition is $n_1 \\ge m$ and $n_2 \\ge m$. In this problem, with $m = 1$, this essentially filters out cases where one of the paths has zero supporting reads, which would definitively indicate an error or a structural variation other than a simple SNP. It ensures a minimal level of evidence for both alleles.\n\n2.  **Balanced Support:** For a heterozygous SNP in a diploid organism, the two alleles are expected to be present in roughly equal genomic proportions. Consequently, the number of sequencing reads sampled from each allele should be approximately equal,\n    barring sampling bias. The ratio $r = \\frac{n_1}{n_1 + n_2}$ represents the proportion of reads supporting path $1$. For a balanced SNP, one expects $r \\approx \\frac{1}{2}$. This condition formalizes this expectation by requiring that the deviation of $r$ from $\\frac{1}{2}$ is within a certain tolerance $\\varepsilon$. The condition is $|\\frac{n_1}{n_1 + n_2} - \\frac{1}{2}| \\le \\varepsilon$. An extreme imbalance in read counts (e.g., $n_1 \\gg n_2$) is a strong indicator of a sequencing error, where the low-count path is spurious.\n\n3.  **Insert-Size Consistency:** This condition statistically validates the alignment of reads to their proposed paths. For each path $p \\in \\{1, 2\\}$, the observed inner distances $\\{d_i^{(p)}\\}$ are assumed to be drawn from a Gaussian distribution $\\mathcal{N}(L_p, \\sigma^2)$, where $L_p$ is the true path length and $\\sigma$ is the known standard deviation of the measurement process. The $z$-score, $z_i^{(p)} = \\frac{d_i^{(p)} - L_p}{\\sigma}$, measures how many standard deviations an observation $d_i^{(p)}$ is from the expected mean $L_p$. The metric $\\bar{z}_p = \\frac{1}{n_p} \\sum_{i=1}^{n_p} |z_i^{(p)}|$ is the mean absolute $z$-score, which quantifies the average deviation of the observed distances from the expected path length, in units of standard deviation. If the reads truly belong to path $p$, we expect $\\bar{z}_p$ to be small. The condition $\\max\\{\\bar{z}_1, \\bar{z}_2\\} \\le z_{\\mathrm{thr}}$ requires that both paths exhibit good statistical consistency with their supporting read data. A large $\\bar{z}_p$ suggests that the reads are a poor fit for path $p$, which could happen if $L_p$ is incorrect or the reads originate from a different genomic location. The defined case where $n_p=0$ implies $\\bar{z}_p = +\\infty$ correctly causes this condition to fail, as it should.\n\nWe now apply this decision function to each test case.\n\n**Case 1:**\n- Data: $(L_1, L_2) = (100, 101)$, $\\{d^{(1)}\\} = \\{98, 100, 102, 99\\}$, $\\{d^{(2)}\\} = \\{101, 100, 103, 102\\}$, $\\sigma = 3$.\n- Counts: $n_1 = 4$, $n_2 = 4$.\n1.  **Minimum Support**: $n_1 = 4 \\ge 1$ and $n_2 = 4 \\ge 1$. Condition passes.\n2.  **Balanced Support**: $r = \\frac{4}{4+4} = 0.5$. $|0.5 - 0.5| = 0 \\le 0.15$. Condition passes.\n3.  **Insert-Size Consistency**:\n    - For path $1$, the deviations from $L_1 = 100$ are $\\{-2, 0, 2, -1\\}$. The absolute $z$-scores are $\\{\\frac{2}{3}, 0, \\frac{2}{3}, \\frac{1}{3}\\}$. $\\bar{z}_1 = \\frac{1}{4}(\\frac{2}{3} + 0 + \\frac{2}{3} + \\frac{1}{3}) = \\frac{1}{4}(\\frac{5}{3}) \\approx 0.417$.\n    - For path $2$, the deviations from $L_2 = 101$ are $\\{0, -1, 2, 1\\}$. The absolute $z$-scores are $\\{0, \\frac{1}{3}, \\frac{2}{3}, \\frac{1}{3}\\}$. $\\bar{z}_2 = \\frac{1}{4}(0 + \\frac{1}{3} + \\frac{2}{3} + \\frac{1}{3}) = \\frac{1}{4}(\\frac{4}{3}) \\approx 0.333$.\n    - $\\max\\{\\bar{z}_1, \\bar{z}_2\\} \\approx \\max\\{0.417, 0.333\\} = 0.417 \\le z_{\\mathrm{thr}} = 2.0$. Condition passes.\n- Verdict: **True**.\n\n**Case 2:**\n- Data: $(L_1, L_2) = (100, 101)$, $\\{d^{(1)}\\} = \\{99, \\dots, 99\\}$ (10 elements), $\\{d^{(2)}\\} = \\{101\\}$, $\\sigma = 3$.\n- Counts: $n_1 = 10$, $n_2 = 1$.\n1.  **Minimum Support**: $n_1 = 10 \\ge 1$ and $n_2 = 1 \\ge 1$. Condition passes.\n2.  **Balanced Support**: $r = \\frac{10}{10+1} = \\frac{10}{11} \\approx 0.909$. $|0.909 - 0.5| = 0.409$. Since $0.409  0.15$, this condition fails.\n- Verdict: **False**.\n\n**Case 3:**\n- Data: $(L_1, L_2) = (180, 181)$, $\\{d^{(1)}\\} = \\{180, 179, 181, 182, 178, 180\\}$, $\\{d^{(2)}\\} = \\{190, 192, 189, 193\\}$, $\\sigma = 2$.\n- Counts: $n_1 = 6$, $n_2 = 4$.\n1.  **Minimum Support**: $n_1 = 6 \\ge 1$ and $n_2 = 4 \\ge 1$. Condition passes.\n2.  **Balanced Support**: $r = \\frac{6}{6+4} = 0.6$. $|0.6 - 0.5| = 0.1 \\le 0.15$. Condition passes.\n3.  **Insert-Size Consistency**:\n    - For path $1$, deviations from $L_1 = 180$: $\\{0, -1, 1, 2, -2, 0\\}$. Absolute $z$-scores: $\\{0, 0.5, 0.5, 1.0, 1.0, 0\\}$. $\\bar{z}_1 = \\frac{1}{6}(0+0.5+0.5+1.0+1.0+0) = \\frac{3}{6} = 0.5$.\n    - For path $2$, deviations from $L_2 = 181$: $\\{9, 11, 8, 12\\}$. Absolute $z$-scores: $\\{4.5, 5.5, 4.0, 6.0\\}$. $\\bar{z}_2 = \\frac{1}{4}(4.5+5.5+4.0+6.0) = \\frac{20}{4} = 5.0$.\n    - $\\max\\{\\bar{z}_1, \\bar{z}_2\\} = \\max\\{0.5, 5.0\\} = 5.0$. Since $5.0  z_{\\mathrm{thr}} = 2.0$, this condition fails.\n- Verdict: **False**.\n\n**Case 4:**\n- Data: $(L_1, L_2) = (250, 250)$, $\\{d^{(1)}\\} = \\{249\\}$, $\\{d^{(2)}\\} = \\{252\\}$, $\\sigma = 5$.\n- Counts: $n_1 = 1$, $n_2 = 1$.\n1.  **Minimum Support**: $n_1 = 1 \\ge 1$ and $n_2 = 1 \\ge 1$. Condition passes.\n2.  **Balanced Support**: $r = \\frac{1}{1+1} = 0.5$. $|0.5 - 0.5| = 0 \\le 0.15$. Condition passes.\n3.  **Insert-Size Consistency**:\n    - For path $1$, deviation from $L_1 = 250$: $\\{-1\\}$. Absolute $z$-score: $\\{\\frac{1}{5}\\} = \\{0.2\\}$. $\\bar{z}_1 = 0.2$.\n    - For path $2$, deviation from $L_2 = 250$: $\\{2\\}$. Absolute $z$-score: $\\{\\frac{2}{5}\\} = \\{0.4\\}$. $\\bar{z}_2 = 0.4$.\n    - $\\max\\{\\bar{z}_1, \\bar{z}_2\\} = \\max\\{0.2, 0.4\\} = 0.4 \\le z_{\\mathrm{thr}} = 2.0$. Condition passes.\n- Verdict: **True**.\n\n**Case 5:**\n- Data: $(L_1, L_2) = (150, 151)$, $\\{d^{(1)}\\} = \\{149, 152\\}$, $\\{d^{(2)}\\} = \\{151\\}$, $\\sigma = 5$.\n- Counts: $n_1 = 2$, $n_2 = 1$.\n1.  **Minimum Support**: $n_1 = 2 \\ge 1$ and $n_2 = 1 \\ge 1$. Condition passes.\n2.  **Balanced Support**: $r = \\frac{2}{2+1} = \\frac{2}{3} \\approx 0.667$. $|0.667 - 0.5| \\approx 0.167$. Since $0.167  0.15$, this condition fails.\n- Verdict: **False**.\n\n**Case 6:**\n- Data: $(L_1, L_2) = (120, 121)$, $\\{d^{(1)}\\} = \\{121, 118, 119\\}$, $\\{d^{(2)}\\} = \\{\\}$, $\\sigma = 4$.\n- Counts: $n_1 = 3$, $n_2 = 0$.\n1.  **Minimum Support**: $n_1 = 3 \\ge 1$ but $n_2 = 0  1$. Condition fails.\n- Verdict: **False**.\n\nSummary of results: [True, False, False, True, False, False].", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Applies the decision function D to a suite of test cases for classifying\n    bubbles in a de Bruijn graph as SNPs or sequencing errors.\n    \"\"\"\n\n    # Define the fixed thresholds from the problem statement.\n    m_thr = 1\n    epsilon = 0.15\n    z_thr = 2.0\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        {'L1': 100, 'L2': 101, 'd1': [98, 100, 102, 99], 'd2': [101, 100, 103, 102], 'sigma': 3},\n        # Case 2\n        {'L1': 100, 'L2': 101, 'd1': [99, 101, 100, 100, 102, 98, 97, 103, 101, 99], 'd2': [101], 'sigma': 3},\n        # Case 3\n        {'L1': 180, 'L2': 181, 'd1': [180, 179, 181, 182, 178, 180], 'd2': [190, 192, 189, 193], 'sigma': 2},\n        # Case 4\n        {'L1': 250, 'L2': 250, 'd1': [249], 'd2': [252], 'sigma': 5},\n        # Case 5\n        {'L1': 150, 'L2': 151, 'd1': [149, 152], 'd2': [151], 'sigma': 5},\n        # Case 6\n        {'L1': 120, 'L2': 121, 'd1': [121, 118, 119], 'd2': [], 'sigma': 4},\n    ]\n\n    results = []\n    for case in test_cases:\n        d1 = np.array(case['d1'])\n        d2 = np.array(case['d2'])\n        L1, L2 = case['L1'], case['L2']\n        sigma = case['sigma']\n\n        n1 = len(d1)\n        n2 = len(d2)\n\n        # 1. Minimum support condition\n        cond1 = (n1 >= m_thr) and (n2 >= m_thr)\n        if not cond1:\n            results.append(False)\n            continue\n\n        # 2. Balanced support condition\n        r = n1 / (n1 + n2)\n        cond2 = abs(r - 0.5) = epsilon\n        if not cond2:\n            results.append(False)\n            continue\n            \n        # 3. Insert-size consistency condition\n        # The problem states if np=0, z_bar_p is +inf.\n        # This is handled by the first condition (minimum support),\n        # so we don't need to check for n1=0 or n2=0 here explicitly.\n        \n        # Path 1 z-score calculation\n        z_scores1 = (d1 - L1) / sigma\n        z_bar1 = np.mean(np.abs(z_scores1))\n\n        # Path 2 z-score calculation\n        z_scores2 = (d2 - L2) / sigma\n        z_bar2 = np.mean(np.abs(z_scores2))\n\n        cond3 = max(z_bar1, z_bar2) = z_thr\n        \n        # Overall classification: True if all conditions pass\n        is_snp = cond1 and cond2 and cond3\n        results.append(is_snp)\n\n    # Final print statement in the exact required format.\n    # The str() of a Python boolean (e.g., True) is a capitalized string (\"True\").\n    # This aligns with the instruction to use Python's canonical literals.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2405181"}]}