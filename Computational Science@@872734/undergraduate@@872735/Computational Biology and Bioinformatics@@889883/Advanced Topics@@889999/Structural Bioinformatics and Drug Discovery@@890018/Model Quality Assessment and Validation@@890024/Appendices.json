{"hands_on_practices": [{"introduction": "In computational biology, we often face a choice between competing models. While a more complex model with more parameters might achieve a better fit to the training data, it risks overfitting and performing poorly on new, unseen data. This practice [@problem_id:2406458] introduces the Akaike Information Criterion ($AIC$), a fundamental tool in model selection that formalizes this trade-off. You will apply $AIC$ to compare two Hidden Markov Models for gene finding, learning how to quantitatively balance model likelihood against model complexity to select the model that is more likely to generalize well.", "problem": "You are comparing two Hidden Markov Models (HMMs) for gene finding that were each trained by maximum likelihood on the same annotated genomic training set. Let the two models be denoted $\\mathrm{M}_{1}$ and $\\mathrm{M}_{2}$. The numbers of independently estimated parameters (free parameters) are $k_{1} = 62$ for $\\mathrm{M}_{1}$ and $k_{2} = 95$ for $\\mathrm{M}_{2}$. The maximized log-likelihoods (natural logarithm) of the training data under the fitted models are $\\ell_{1} = -40215.37$ for $\\mathrm{M}_{1}$ and $\\ell_{2} = -40180.00$ for $\\mathrm{M}_{2}$.\n\nUse the Akaike Information Criterion (AIC) to assess relative model quality. The AIC for a fitted model is defined as $\\mathrm{AIC} = 2k - 2\\ell$, where $k$ is the number of free parameters and $\\ell$ is the maximized log-likelihood. Define the Akaike differences $\\Delta_{i} = \\mathrm{AIC}_{i} - \\min_{j} \\mathrm{AIC}_{j}$, and the two-model Akaike weight for model $\\mathrm{M}_{2}$ as\n$$\nw_{2} = \\frac{\\exp\\!\\left(-\\tfrac{1}{2}\\Delta_{2}\\right)}{\\exp\\!\\left(-\\tfrac{1}{2}\\Delta_{1}\\right) + \\exp\\!\\left(-\\tfrac{1}{2}\\Delta_{2}\\right)}.\n$$\n\nCompute $w_{2}$ for the given $\\mathrm{M}_{1}$ and $\\mathrm{M}_{2}$. Express your final answer as a decimal, rounded to $4$ significant figures.", "solution": "The problem presents a task of model selection between two Hidden Markov Models, $\\mathrm{M}_{1}$ and $\\mathrm{M}_{2}$, using the Akaike Information Criterion (AIC). The problem is scientifically grounded, well-posed, and all necessary information is provided. It constitutes a straightforward application of a standard statistical method in computational biology. Thus, we proceed with the calculation.\n\nThe provided parameters are:\n- For model $\\mathrm{M}_{1}$: number of free parameters $k_{1} = 62$ and maximized log-likelihood $\\ell_{1} = -40215.37$.\n- For model $\\mathrm{M}_{2}$: number of free parameters $k_{2} = 95$ and maximized log-likelihood $\\ell_{2} = -40180.00$.\n\nThe Akaike Information Criterion is defined as $\\mathrm{AIC} = 2k - 2\\ell$. First, we compute the AIC for each model.\n\nFor model $\\mathrm{M}_{1}$:\n$$\n\\mathrm{AIC}_{1} = 2k_{1} - 2\\ell_{1} = 2(62) - 2(-40215.37) = 124 + 80430.74 = 80554.74\n$$\n\nFor model $\\mathrm{M}_{2}$:\n$$\n\\mathrm{AIC}_{2} = 2k_{2} - 2\\ell_{2} = 2(95) - 2(-40180.00) = 190 + 80360.00 = 80550.00\n$$\n\nThe AIC measures the trade-off between model fit (likelihood) and model complexity (number of parameters). A lower AIC value indicates a better model. Comparing the two values, we find:\n$$\n\\mathrm{AIC}_{2} = 80550.00  \\mathrm{AIC}_{1} = 80554.74\n$$\nTherefore, model $\\mathrm{M}_{2}$ is preferred over model $\\mathrm{M}_{1}$ according to the AIC. The minimum AIC value is $\\min_{j} \\mathrm{AIC}_{j} = \\mathrm{AIC}_{2} = 80550.00$.\n\nNext, we compute the Akaike differences, $\\Delta_{i} = \\mathrm{AIC}_{i} - \\min_{j} \\mathrm{AIC}_{j}$.\n\nFor model $\\mathrm{M}_{1}$:\n$$\n\\Delta_{1} = \\mathrm{AIC}_{1} - \\min_{j} \\mathrm{AIC}_{j} = 80554.74 - 80550.00 = 4.74\n$$\n\nFor model $\\mathrm{M}_{2}$:\n$$\n\\Delta_{2} = \\mathrm{AIC}_{2} - \\min_{j} \\mathrm{AIC}_{j} = 80550.00 - 80550.00 = 0\n$$\n\nFinally, we compute the Akaike weight for model $\\mathrm{M}_{2}$, $w_{2}$, using the provided formula:\n$$\nw_{2} = \\frac{\\exp(-\\frac{1}{2}\\Delta_{2})}{\\exp(-\\frac{1}{2}\\Delta_{1}) + \\exp(-\\frac{1}{2}\\Delta_{2})}\n$$\nSubstituting the calculated values of $\\Delta_{1}$ and $\\Delta_{2}$:\n$$\nw_{2} = \\frac{\\exp(-\\frac{1}{2} \\cdot 0)}{\\exp(-\\frac{1}{2} \\cdot 4.74) + \\exp(-\\frac{1}{2} \\cdot 0)} = \\frac{\\exp(0)}{\\exp(-2.37) + \\exp(0)}\n$$\nSince $\\exp(0) = 1$, the expression simplifies to:\n$$\nw_{2} = \\frac{1}{\\exp(-2.37) + 1}\n$$\nNow, we compute the numerical value:\n$$\nw_{2} \\approx \\frac{1}{0.0934789... + 1} = \\frac{1}{1.0934789...} \\approx 0.914515...\n$$\nThe problem requires the answer to be rounded to $4$ significant figures. The fifth significant figure is $1$, so we do not round up the fourth.\n$$\nw_{2} \\approx 0.9145\n$$\nThis weight can be interpreted as the probability that $\\mathrm{M}_{2}$ is the better model, given the data and the set of two models.", "answer": "$$\n\\boxed{0.9145}\n$$", "id": "2406458"}, {"introduction": "The Area Under the Receiver Operating Characteristic Curve ($AUC$) is a widely used metric for summarizing classifier performance. However, relying on this single number can be misleading, as models with identical $AUC$ values can have vastly different clinical or practical utility. This exercise [@problem_id:2406412] presents a scenario that highlights this crucial pitfall, forcing a choice between two classifiers based on application-specific constraints. By working through it, you will learn why it is essential to look beyond the $AUC$ and analyze the shape of the ROC curve itself, especially in regions relevant to your problem, such as those with very low False Positive Rates ($FPR$).", "problem": "A clinical laboratory is validating two binary classifiers, $C_1$ and $C_2$, to screen for a rare pathogenic variant from high-throughput sequencing data. Performance is summarized by Receiver Operating Characteristic (ROC) curves, which plot True Positive Rate ($\\mathrm{TPR}$) against False Positive Rate ($\\mathrm{FPR}$). For each classifier, the ROC curve is defined by linear interpolation between the following points:\n\n- For $C_1$: $(0, 0)$, $(0.05, 0.55)$, $(1, 1)$.\n- For $C_2$: $(0, 0)$, $(0.20, 0.70)$, $(1, 1)$.\n\nAssume the evaluation cohort is sufficiently large that these piecewise-linear curves can be treated as exact. A regulator mandates that any deployed screening threshold must satisfy $\\mathrm{FPR} \\le 0.05$.\n\nWhich option is correct?\n\nA. The areas under the ROC curves are equal, and $C_1$ is preferable under the constraint $\\mathrm{FPR} \\le 0.05$.\n\nB. The areas under the ROC curves are equal, and $C_2$ is preferable under the constraint $\\mathrm{FPR} \\le 0.05$.\n\nC. The area under the ROC curve of $C_1$ exceeds that of $C_2$, so $C_1$ is preferable for any constraint.\n\nD. The areas under the ROC curves are equal, so $C_1$ and $C_2$ are clinically indistinguishable under any constraint.", "solution": "The solution requires two main steps: first, calculating the Area Under the ROC Curve (AUC) for each classifier, and second, evaluating their performance specifically under the given constraint on the False Positive Rate.\n\n**Part 1: Calculation of Area Under the ROC Curve (AUC)**\nThe area under a piecewise-linear ROC curve is calculated by summing the areas of the trapezoids formed by the line segments. The area of a trapezoid defined by points $(x_{i-1}, y_{i-1})$ and $(x_i, y_i)$ is given by $(x_i - x_{i-1}) \\times \\frac{y_{i-1} + y_i}{2}$.\n\nFor classifier $C_1$, the points are $(0, 0)$, $(0.05, 0.55)$, and $(1, 1)$.\nThe total area, $\\mathrm{AUC}_1$, is the sum of the areas of two trapezoids.\n- Area of the first trapezoid (from $\\mathrm{FPR}=0$ to $\\mathrm{FPR}=0.05$):\n$$ A_1 = (0.05 - 0) \\times \\frac{0 + 0.55}{2} = 0.05 \\times 0.275 = 0.01375 $$\n- Area of the second trapezoid (from $\\mathrm{FPR}=0.05$ to $\\mathrm{FPR}=1$):\n$$ A_2 = (1 - 0.05) \\times \\frac{0.55 + 1}{2} = 0.95 \\times \\frac{1.55}{2} = 0.95 \\times 0.775 = 0.73625 $$\n- Total AUC for $C_1$:\n$$ \\mathrm{AUC}_1 = A_1 + A_2 = 0.01375 + 0.73625 = 0.75 $$\n\nFor classifier $C_2$, the points are $(0, 0)$, $(0.20, 0.70)$, and $(1, 1)$.\nThe total area, $\\mathrm{AUC}_2$, is also the sum of the areas of two trapezoids.\n- Area of the first trapezoid (from $\\mathrm{FPR}=0$ to $\\mathrm{FPR}=0.20$):\n$$ A_1' = (0.20 - 0) \\times \\frac{0 + 0.70}{2} = 0.20 \\times 0.35 = 0.07 $$\n- Area of the second trapezoid (from $\\mathrm{FPR}=0.20$ to $\\mathrm{FPR}=1$):\n$$ A_2' = (1 - 0.20) \\times \\frac{0.70 + 1}{2} = 0.80 \\times \\frac{1.70}{2} = 0.80 \\times 0.85 = 0.68 $$\n- Total AUC for $C_2$:\n$$ \\mathrm{AUC}_2 = A_1' + A_2' = 0.07 + 0.68 = 0.75 $$\n\nThe calculation confirms that $\\mathrm{AUC}_1 = \\mathrm{AUC}_2 = 0.75$. The areas under the ROC curves are equal.\n\n**Part 2: Evaluation under the Constraint $\\mathrm{FPR} \\le 0.05$**\nThe regulator mandates that only operating points with $\\mathrm{FPR} \\le 0.05$ are permissible. To compare the classifiers under this constraint, we must determine which classifier offers a higher $\\mathrm{TPR}$ for any given $\\mathrm{FPR}$ in the interval $[0, 0.05]$.\n\nFor classifier $C_1$, the ROC curve in this region is the line segment connecting $(0, 0)$ and $(0.05, 0.55)$. The equation for the $\\mathrm{TPR}$ of $C_1$ as a function of $\\mathrm{FPR}$ is:\n$$ \\mathrm{TPR}_1(\\mathrm{FPR}) = \\frac{0.55 - 0}{0.05 - 0} \\times \\mathrm{FPR} = 11 \\times \\mathrm{FPR} \\quad \\text{for } 0 \\le \\mathrm{FPR} \\le 0.05 $$\n\nFor classifier $C_2$, the relevant part of its ROC curve is on the segment connecting $(0, 0)$ and $(0.20, 0.70)$. The equation for the $\\mathrm{TPR}$ of $C_2$ in this region is:\n$$ \\mathrm{TPR}_2(\\mathrm{FPR}) = \\frac{0.70 - 0}{0.20 - 0} \\times \\mathrm{FPR} = 3.5 \\times \\mathrm{FPR} \\quad \\text{for } 0 \\le \\mathrm{FPR} \\le 0.20 $$\n\nNow, we compare the two functions for any $\\mathrm{FPR}$ value in the constrained interval $(0, 0.05]$.\n$$ \\mathrm{TPR}_1(\\mathrm{FPR}) = 11 \\times \\mathrm{FPR} $$\n$$ \\mathrm{TPR}_2(\\mathrm{FPR}) = 3.5 \\times \\mathrm{FPR} $$\nSince $11  3.5$, it follows that for any positive $\\mathrm{FPR}$ within the allowed range, $\\mathrm{TPR}_1(\\mathrm{FPR})  \\mathrm{TPR}_2(\\mathrm{FPR})$. This means that classifier $C_1$ strictly dominates classifier $C_2$ in the operating region of interest. For example, at the maximum allowed $\\mathrm{FPR}$ of $0.05$, $C_1$ achieves a $\\mathrm{TPR}$ of $0.55$, whereas $C_2$ achieves a $\\mathrm{TPR}$ of only $3.5 \\times 0.05 = 0.175$.\n\nTherefore, under the regulatory constraint, classifier $C_1$ is unequivocally preferable.\n\n**Option-by-Option Analysis**\n\nA. The areas under the ROC curves are equal, and $C_1$ is preferable under the constraint $\\mathrm{FPR} \\le 0.05$.\n- As calculated, $\\mathrm{AUC}_1 = \\mathrm{AUC}_2 = 0.75$. The first part of the statement is correct.\n- As demonstrated, for any given $\\mathrm{FPR} \\in (0, 0.05]$, $C_1$ provides a higher $\\mathrm{TPR}$ than $C_2$. Thus, $C_1$ is preferable under the constraint. The second part of the statement is also correct.\n- Verdict: **Correct**.\n\nB. The areas under the ROC curves are equal, and $C_2$ is preferable under the constraint $\\mathrm{FPR} \\le 0.05$.\n- The first part is correct.\n- The second part is incorrect. Classifier $C_1$ is preferable, not $C_2$.\n- Verdict: **Incorrect**.\n\nC. The area under the ROC curve of $C_1$ exceeds that of $C_2$, so $C_1$ is preferable for any constraint.\n- The premise, \"The area under the ROC curve of $C_1$ exceeds that of $C_2$\", is false. The areas are equal.\n- The conclusion, \"so $C_1$ is preferable for any constraint\", is also a logical non sequitur. Even if one AUC were higher, it does not guarantee superiority under every possible constraint.\n- Verdict: **Incorrect**.\n\nD. The areas under the ROC curves are equal, so $C_1$ and $C_2$ are clinically indistinguishable under any constraint.\n- The premise, \"The areas under the ROC curves are equal\", is correct.\n- The conclusion, \"so $C_1$ and $C_2$ are clinically indistinguishable under any constraint\", is a common but profound error in reasoning. Equal overall AUC does not imply equal performance in specific sub-regions of the ROC space. Our analysis shows a clear clinical distinction and preference for $C_1$ under the given constraint. For a different constraint, for example $\\mathrm{FPR} \\ge 0.1$, $C_2$ might be preferable. Thus, they are not indistinguishable.\n- Verdict: **Incorrect**.", "answer": "$$\\boxed{A}$$", "id": "2406412"}, {"introduction": "A truly robust model should not only be accurate but also resilient to small, targeted perturbations in its input. This practice [@problem_id:2406424] delves into this advanced concept by framing model validation as an adversarial challenge: finding the smallest possible change to a DNA sequence that flips a transcription factor binding predictor's output. This exercise introduces the idea of \"adversarial examples\" in a bioinformatics context and requires algorithmic thinking to efficiently probe a model's sensitivities. Mastering this approach is key to understanding a model's potential failure modes and building more trustworthy predictive tools.", "problem": "You are given a formal model of a transcription factor binding site predictor that maps a deoxyribonucleic acid (DNA) sequence of fixed length to a real-valued score. The alphabet is the set $\\{A,C,G,T\\}$. For a sequence $x = (x_1,\\dots,x_L)$ of length $L$, the model score is\n$$\nS(x) = b + \\sum_{i=1}^{L} w_{i, x_i},\n$$\nwhere $b$ is a bias term and $w_{i,\\nu}$ is a position-specific weight assigned to nucleotide $\\nu \\in \\{A,C,G,T\\}$ at position $i \\in \\{1,\\dots,L\\}$. The classifier outputs the label \"bound\" if and only if $S(x) \\ge 0$, and \"unbound\" otherwise. A perturbation consists of replacing the nucleotide at a single position with a different nucleotide; insertions and deletions are not allowed. The size of a perturbation is the number of positions changed, that is, the Hamming distance between the original and perturbed sequence.\n\nYour task is: for each test case, determine the minimal number of single-nucleotide substitutions required to obtain a perturbed sequence $x'$ such that $S(x')  0$. If the original sequence already satisfies $S(x)  0$, output $0$. If it is impossible to achieve $S(x')  0$ by any finite number of substitutions under the given model (that is, even changing every position cannot make the score negative), output $-1$.\n\nUse the following fixed length and weights:\n- Length $L = 6$.\n- For positions $i \\in \\{1,2,3,4,5,6\\}$, the weights $w_{i,\\nu}$ over $\\nu \\in \\{A,C,G,T\\}$ are:\n  - $i=1$: $(w_{1,A}, w_{1,C}, w_{1,G}, w_{1,T}) = (2,-1,1,0)$.\n  - $i=2$: $(w_{2,A}, w_{2,C}, w_{2,G}, w_{2,T}) = (-2,2,0,1)$.\n  - $i=3$: $(w_{3,A}, w_{3,C}, w_{3,G}, w_{3,T}) = (1,1,-1,0)$.\n  - $i=4$: $(w_{4,A}, w_{4,C}, w_{4,G}, w_{4,T}) = (-1,0,2,-2)$.\n  - $i=5$: $(w_{5,A}, w_{5,C}, w_{5,G}, w_{5,T}) = (0,-1,1,1)$.\n  - $i=6$: $(w_{6,A}, w_{6,C}, w_{6,G}, w_{6,T}) = (2,-2,0,0)$.\n\nYour program must process the following test suite of parameter values, where each test case is a pair consisting of a sequence and a bias:\n- Test case $1$: sequence $x^{(1)} = \\text{\"ACAGGA\"}$, bias $b^{(1)} = -5$.\n- Test case $2$: sequence $x^{(2)} = \\text{\"CAGTCC\"}$, bias $b^{(2)} = -5$.\n- Test case $3$: sequence $x^{(3)} = \\text{\"ACACAG\"}$, bias $b^{(3)} = -5$.\n- Test case $4$: sequence $x^{(4)} = \\text{\"CAGTCC\"}$, bias $b^{(4)} = 9$.\n- Test case $5$: sequence $x^{(5)} = \\text{\"ACAGGA\"}$, bias $b^{(5)} = -1$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[r1,r2,r3,r4,r5]\"), where $rj$ is the required minimal number of substitutions for test case $j$ as defined above. Each $rj$ must be an integer.", "solution": "The problem requires us to determine the minimum number of single-nucleotide substitutions needed to change a given DNA sequence $x$ of length $L$ into a new sequence $x'$ such that its score $S(x')$ becomes negative. The score is defined by a linear model $S(x) = b + \\sum_{i=1}^{L} w_{i, x_i}$, where $b$ is a bias term and $w_{i, \\nu}$ are position-specific weights for each nucleotide $\\nu$ at each position $i$. A sequence is classified as \"bound\" if its score is non-negative ($S(x) \\ge 0$) and \"unbound\" if its score is negative ($S(x)  0$). This task is fundamentally an optimization problem: we seek the smallest perturbation, measured in Hamming distance, to flip the classification from \"bound\" to \"unbound\".\n\nFirst, we must calculate the initial score, $S(x)$, for the given sequence $x=(x_1, \\dots, x_L)$ and bias $b$. The formula is $S(x) = b + \\sum_{i=1}^{L} w_{i, x_i}$. The weights $w_{i, \\nu}$ and length $L=6$ are provided. The alphabet of nucleotides is $\\nu \\in \\{A,C,G,T\\}$. If the initial score $S(x)$ is already less than $0$, no substitutions are necessary. In this trivial case, the minimal number of substitutions is $0$.\n\nIf $S(x) \\ge 0$, we must perform one or more substitutions. A substitution at a single position $i$ from nucleotide $x_i$ to a new nucleotide $x'_i$ changes the total score. The original contribution to the score from position $i$ is $w_{i, x_i}$, and the new contribution is $w_{i, x'_i}$. The change in the total score due to this single substitution is precisely $\\Delta S_i = w_{i, x'_i} - w_{i, x_i}$. To reduce the score as much as possible with a single substitution at position $i$, we must choose the new nucleotide $x'_i$ to be the one that has the minimum possible weight at that position. Let this minimum weight be $w_{i, \\text{min}} = \\min_{\\nu \\in \\{A,C,G,T\\}} w_{i, \\nu}$. The maximum possible score reduction achievable by one substitution at position $i$ is therefore $\\Delta_i = w_{i, \\text{min}} - w_{i, x_i}$. By definition, $\\Delta_i \\le 0$. If the original nucleotide $x_i$ already corresponds to the minimum weight at position $i$, then $\\Delta_i = 0$, and no single substitution at this position can decrease the score.\n\nTo find the minimal number of substitutions to make the total score negative, we should employ a greedy strategy. The core principle of this strategy is to always make the choice that provides the most progress toward the goal. In our case, this means applying the substitutions that yield the largest individual score reductions first. Each substitution is an independent event, so this greedy approach is guaranteed to be optimal.\n\nThe algorithm proceeds as follows:\n1. For each position $i$ from $1$ to $L=6$, calculate the maximum possible score reduction $\\Delta_i = w_{i, \\text{min}} - w_{i, x_i}$. This gives us a set of $L$ potential score reductions, $\\{\\Delta_1, \\dots, \\Delta_L\\}$.\n2. Before proceeding, we must consider the case where it is impossible to achieve a negative score. The maximum total reduction possible is the sum of all individual maximum reductions, $\\sum_{i=1}^{L} \\Delta_i$. The lowest achievable score is $S_{\\text{min}} = S(x) + \\sum_{i=1}^{L} \\Delta_i$. If $S_{\\text{min}} \\ge 0$, then no matter how many positions we change (up to all $L$ of them), the score will never become negative. In this scenario, the answer is $-1$.\n3. If a negative score is achievable, we sort the potential reductions in non-decreasing (ascending) order: $\\Delta_{(1)} \\le \\Delta_{(2)} \\le \\dots \\le \\Delta_{(L)}$. These sorted values represent the best, second-best, and so on, reductions we can obtain from single substitutions.\n4. We start with the initial score $S(x)$ and iteratively apply these reductions, starting with the largest in magnitude, $\\Delta_{(1)}$. We maintain a running count of substitutions. After applying $k$ substitutions, the new score is $S_k = S(x) + \\sum_{j=1}^{k} \\Delta_{(j)}$. We find the smallest integer $k$ for which $S_k  0$. This value of $k$ is the minimal number of substitutions required.\n\nThis procedure guarantees finding the minimum number of substitutions because at each step, we choose the substitution that reduces the score by the largest possible amount, thus reaching the target threshold of less than $0$ in the fewest possible steps. Since all weights and the bias are integers, the score $S(x)$ will always be an integer, and the condition $S(x')  0$ is equivalent to $S(x') \\le -1$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for all test cases and prints the final output.\n    \"\"\"\n    # Fixed parameters as defined in the problem statement.\n    # L = 6 is implicitly defined by the dimensions of WEIGHTS.\n    WEIGHTS = np.array([\n        # i=1: (A, C, G, T)\n        [2, -1, 1, 0],\n        # i=2: (A, C, G, T)\n        [-2, 2, 0, 1],\n        # i=3: (A, C, G, T)\n        [1, 1, -1, 0],\n        # i=4: (A, C, G, T)\n        [-1, 0, 2, -2],\n        # i=5: (A, C, G, T)\n        [0, -1, 1, 1],\n        # i=6: (A, C, G, T)\n        [2, -2, 0, 0]\n    ])\n    NUCL_MAP = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n\n    # Pre-calculate the minimum weight for each position for efficiency.\n    MIN_WEIGHTS_PER_POS = np.min(WEIGHTS, axis=1)\n\n    # Test suite of parameter values.\n    test_cases = [\n        (\"ACAGGA\", -5),\n        (\"CAGTCC\", -5),\n        (\"ACACAG\", -5),\n        (\"CAGTCC\", 9),\n        (\"ACAGGA\", -1),\n    ]\n\n    def calculate_min_substitutions(sequence, bias):\n        \"\"\"\n        Calculates the minimal number of substitutions for a single test case.\n        \"\"\"\n        # Step 1: Calculate the initial score.\n        # Since all weights and biases are integers, the score will be an integer.\n        current_score = int(bias)\n        for i, nucl in enumerate(sequence):\n            nucl_idx = NUCL_MAP[nucl]\n            current_score += WEIGHTS[i, nucl_idx]\n\n        # Step 2: If the score is already negative, 0 substitutions are needed.\n        if current_score  0:\n            return 0\n\n        # Step 3: Calculate the maximum potential score reduction for each position.\n        reductions = []\n        for i, nucl in enumerate(sequence):\n            nucl_idx = NUCL_MAP[nucl]\n            current_weight = WEIGHTS[i, nucl_idx]\n            min_weight_at_pos = MIN_WEIGHTS_PER_POS[i]\n            delta = min_weight_at_pos - current_weight\n            reductions.append(delta)\n\n        # Step 4: Check if it's impossible to make the score negative.\n        # The lowest possible score is the current score plus all possible reductions.\n        if current_score + sum(reductions) >= 0:\n            return -1\n\n        # Step 5: Greedily apply substitutions to find the minimum number.\n        # Sort reductions in ascending order to apply most negative ones first.\n        reductions.sort()\n        \n        num_substitutions = 0\n        for delta in reductions:\n            current_score += delta\n            num_substitutions += 1\n            if current_score  0:\n                return num_substitutions\n        \n        # This part of the code should be unreachable due to the impossibility check,\n        # but is included as a safeguard.\n        return -1\n\n\n    results = []\n    for sequence, bias in test_cases:\n        result = calculate_min_substitutions(sequence, bias)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2406424"}]}