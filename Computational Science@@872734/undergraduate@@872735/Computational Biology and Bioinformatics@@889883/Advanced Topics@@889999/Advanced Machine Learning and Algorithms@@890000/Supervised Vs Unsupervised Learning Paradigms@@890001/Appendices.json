{"hands_on_practices": [{"introduction": "To build a solid foundation, our first practice directly contrasts supervised and unsupervised learning on a classic bioinformatics problem: distinguishing protein-coding DNA from non-coding DNA. You will first represent sequences as numerical feature vectors, $\\phi_k(s)$, based on their $k$-mer frequencies. Then, you will train a Support Vector Machine (SVM)—a powerful supervised model—using labeled sequences to learn the distinguishing features, and compare its performance to an unsupervised clustering algorithm that groups sequences based on inherent similarity alone [@problem_id:2432827]. This head-to-head comparison makes the fundamental differences between the two paradigms concrete and highlights the impact of using labeled data.", "problem": "You are given a finite alphabet $\\mathcal{A} = \\{\\mathrm{A}, \\mathrm{C}, \\mathrm{G}, \\mathrm{T}\\}$ and a definition of a feature map from a deoxyribonucleic acid (DNA) sequence $s$ of length $L$ to a vector of normalized $k$-mer frequencies $\\phi_k(s) \\in \\mathbb{R}^{4^k}$. For a given positive integer $k$, let $\\mathcal{M}_k$ be the set of all strings of length $k$ over $\\mathcal{A}$, enumerated in lexicographic order. For each $m \\in \\mathcal{M}_k$, the component $\\left[\\phi_k(s)\\right]_m$ is defined as the count of occurrences of $m$ in $s$ using a sliding window of stride $1$, divided by the total number of $k$-mers in $s$, which is $L - k + 1$ (assume $L \\ge k$ and that no ambiguous characters appear). Formally,\n$$\n\\left[\\phi_k(s)\\right]_m = \\frac{1}{L - k + 1} \\sum_{i=1}^{L-k+1} \\mathbf{1}\\left\\{ s_i s_{i+1} \\cdots s_{i+k-1} = m \\right\\},\n$$\nwhere $\\mathbf{1}\\{\\cdot\\}$ is the indicator function and indices are $1$-based.\n\nA binary label $y \\in \\{-1, +1\\}$ indicates whether a DNA sequence is protein-coding ($+1$) or non-coding ($-1$). Consider a supervised linear classifier defined by a weight vector $w \\in \\mathbb{R}^{4^k}$ and bias $b \\in \\mathbb{R}$ acting on features $\\phi_k(s)$ via the score $f(s) = w^\\top \\phi_k(s) + b$ and the prediction rule $\\hat{y}(s) = \\mathrm{sign}(f(s))$, where $\\mathrm{sign}(z) = +1$ if $z \\ge 0$ and $-1$ otherwise. The model parameters $(w,b)$ are obtained by minimizing the regularized empirical risk with squared hinge loss:\n$$\nJ(w,b) = \\frac{1}{2} \\lVert w \\rVert_2^2 + C \\sum_{i=1}^{N} \\left( \\max\\left(0, 1 - y_i (w^\\top x_i + b) \\right) \\right)^2,\n$$\nwhere $C > 0$ is a given regularization parameter, $\\{(x_i, y_i)\\}_{i=1}^N$ are training examples with $x_i = \\phi_k(s_i)$ and $y_i \\in \\{-1, +1\\}$, and $\\lVert \\cdot \\rVert_2$ denotes the Euclidean norm.\n\nFor an unsupervised baseline, consider partitioning a given set of feature vectors $\\{x_j\\}_{j=1}^M$ (with $x_j = \\phi_k(s_j)$ for $M$ sequences) into exactly two non-empty clusters $\\mathcal{C}_0$ and $\\mathcal{C}_1$ so as to minimize the within-cluster sum of squares:\n$$\nW(\\mathcal{C}_0, \\mathcal{C}_1) = \\sum_{j \\in \\mathcal{C}_0} \\lVert x_j - \\mu_0 \\rVert_2^2 + \\sum_{j \\in \\mathcal{C}_1} \\lVert x_j - \\mu_1 \\rVert_2^2,\n$$\nwhere $\\mu_0$ and $\\mu_1$ are the means of their respective clusters. After obtaining the minimizing partition, define a cluster-based prediction $\\tilde{y}_j \\in \\{-1, +1\\}$ for each item by mapping the two clusters to $\\{-1, +1\\}$ in the way that maximizes the fraction of matches with ground-truth labels on the same items; report that maximal fraction as the unsupervised accuracy expressed as a decimal.\n\nYou are provided with a labeled training set $\\mathcal{D}_{\\mathrm{train}}$ and a labeled test set $\\mathcal{D}_{\\mathrm{test}}$ of DNA sequences. All sequences consist only of symbols from $\\mathcal{A}$.\n\nTraining sequences (with labels $y$ shown in parentheses, where $+1$ denotes coding and $-1$ denotes non-coding):\n- $s_1 = \\texttt{ATGGCGGCCGCGGGCGCCGCGGGCGACGGCTGA}$ $(+1)$\n- $s_2 = \\texttt{ATGCGCGCGCGGGCCGCGGCTGCGGCGTAG}$ $(+1)$\n- $s_3 = \\texttt{ATGGGCGACGGCGGCGACGGCGGCGACTAA}$ $(+1)$\n- $s_4 = \\texttt{ATGGCCGCTGCGGCTGGCGCTGCGGCTTGA}$ $(+1)$\n- $s_5 = \\texttt{ATGGCGGCGGCGGCGGCGGCGGCGGCGGCGTAA}$ $(+1)$\n- $s_6 = \\texttt{ATGGGCGCCGCGGGCGCCGCGGGCGCCTGA}$ $(+1)$\n- $s_7 = \\texttt{TATATAAATAATATATATTTATATAATAATA}$ $(-1)$\n- $s_8 = \\texttt{AAATATATATTTAAATATATATATATAAAA}$ $(-1)$\n- $s_9 = \\texttt{TTTATATATAAATATAATATATTTATAAAT}$ $(-1)$\n- $s_{10} = \\texttt{AATAATAATATATTTATAAATAATATATAT}$ $(-1)$\n- $s_{11} = \\texttt{ATATATAAATATATAATATATAAATATATA}$ $(-1)$\n- $s_{12} = \\texttt{TATATATAAATAAATATATATATAAATATA}$ $(-1)$\n\nTest sequences (with labels):\n- $t_1 = \\texttt{ATGGCGGGCGGGCGACGGCTAA}$ $(+1)$\n- $t_2 = \\texttt{ATGGCCGCGGCTGGCGCTGCGTAG}$ $(+1)$\n- $t_3 = \\texttt{ATGGCGGCGGCGGCGGCGTGA}$ $(+1)$\n- $t_4 = \\texttt{AATATATATATAAATATATATAAATAATA}$ $(-1)$\n- $t_5 = \\texttt{TATATTTATAAATATATATAAATATTTAT}$ $(-1)$\n- $t_6 = \\texttt{AAATAATATATATATAAATAATATATATA}$ $(-1)$\n\nYour tasks are as follows.\n\n1. Supervised classification. For each specified pair $(k, C)$, compute the feature vectors $\\phi_k(s)$ for all $s \\in \\mathcal{D}_{\\mathrm{train}}$, learn $(w,b)$ by minimizing $J(w,b)$, and then compute the test accuracy (as a decimal) on $\\mathcal{D}_{\\mathrm{test}}$ using $\\hat{y}(s) = \\mathrm{sign}(w^\\top \\phi_k(s) + b)$.\n2. Unsupervised clustering baseline. For each specified $k$, compute $\\phi_k(s)$ for all $s \\in \\mathcal{D}_{\\mathrm{test}}$ and find a partition into exactly two non-empty clusters that minimizes $W(\\mathcal{C}_0,\\mathcal{C}_1)$. Map clusters to labels $\\{-1,+1\\}$ to maximize agreement with the true labels on $\\mathcal{D}_{\\mathrm{test}}$, and report that maximal agreement as the accuracy (as a decimal).\n\nTest suite. Run the following five cases in this exact order:\n- Case $1$: supervised with $k = 3$, $C = 1$.\n- Case $2$: supervised with $k = 2$, $C = 1$.\n- Case $3$: supervised with $k = 3$, $C = 0.01$.\n- Case $4$: unsupervised with $k = 3$ on the test set only.\n- Case $5$: unsupervised with $k = 2$ on the test set only.\n\nAnswer specification and output format.\n- For each case, the answer is a single real number equal to the accuracy on $\\mathcal{D}_{\\mathrm{test}}$ expressed as a decimal, rounded to exactly three digits after the decimal point.\n- Your program should produce a single line of output containing the five results as a comma-separated list enclosed in square brackets, in the order of the cases above. For example, an output with five placeholder values should look like $[\\alpha_1,\\alpha_2,\\alpha_3,\\alpha_4,\\alpha_5]$ where each $\\alpha_i$ is a real number rounded to three digits after the decimal point.", "solution": "The problem presented requires the implementation and evaluation of two distinct machine learning paradigms—supervised classification and unsupervised clustering—for the task of discriminating between protein-coding and non-coding deoxyribonucleic acid (DNA) sequences. The problem is scientifically grounded, well-posed, and formally specified. It provides all necessary data and definitions for a complete computational solution. Thus, it is a valid problem. The solution proceeds by first constructing the feature representation, then implementing the specified algorithms for both paradigms.\n\nThe fundamental step for both approaches is the transformation of symbolic DNA sequences into a quantitative, numerical format suitable for algorithmic processing. This is achieved through a feature map $\\phi_k(s)$, which projects a sequence $s$ of length $L$ into a real-valued vector in $\\mathbb{R}^{4^k}$. Each dimension of this vector corresponds to one of the $4^k$ possible strings of length $k$ (termed $k$-mers) over the alphabet $\\mathcal{A} = \\{\\mathrm{A}, \\mathrm{C}, \\mathrm{G}, \\mathrm{T}\\}$. The value of each component is the normalized frequency of the corresponding $k$-mer in the sequence. Specifically, for a $k$-mer $m$, its frequency is its count within a sliding window of size $k$ and stride $1$, divided by the total number of windows, which is $L-k+1$. This establishes a vector space model where each sequence is a point, and the geometric relationships between these points can be exploited by learning algorithms. The set of all $k$-mers, $\\mathcal{M}_k$, is ordered lexicographically, which defines a consistent mapping from a $k$-mer to an index in the feature vector. This can be achieved by treating the characters A, C, G, T as digits in a base-$4$ number system (e.g., $A=0, C=1, G=2, T=3$).\n\nFirst, we address the supervised classification task. We are given a training dataset $\\mathcal{D}_{\\mathrm{train}}$ of $N$ sequences with known labels $y_i \\in \\{-1, +1\\}$. The goal is to learn a linear decision function $f(s) = w^\\top \\phi_k(s) + b$ that can predict the label of a new sequence. The parameters, a weight vector $w \\in \\mathbb{R}^{4^k}$ and a scalar bias $b \\in \\mathbb{R}$, are determined by minimizing a regularized empirical risk function. The specified objective function is:\n$$\nJ(w,b) = \\frac{1}{2} \\lVert w \\rVert_2^2 + C \\sum_{i=1}^{N} \\left( \\max\\left(0, 1 - y_i (w^\\top x_i + b) \\right) \\right)^2\n$$\nHere, $x_i = \\phi_k(s_i)$ is the feature vector for the $i$-th training sequence. The first term, $\\frac{1}{2} \\lVert w \\rVert_2^2$, is an $\\ell_2$-regularizer that penalizes large weights to prevent overfitting. The second term is the sum of squared hinge losses over the training set, which penalizes misclassifications and correct classifications with insufficient margin. The parameter $C > 0$ controls the trade-off between regularization and fitting the training data. This objective function $J(w,b)$ is convex and differentiable everywhere, ensuring that a unique global minimum exists. We can find this minimum using a gradient-based optimization algorithm, such as L-BFGS-B (Limited-memory Broyden–Fletcher–Goldfarb–Shanno with Bounds). To do this, we compute the partial derivatives of $J(w,b)$ with respect to $w$ and $b$. Let $\\mathcal{S}$ be the set of indices for which $y_i(w^\\top x_i + b) < 1$. The gradients are:\n$$\n\\nabla_w J = w - 2C \\sum_{i \\in \\mathcal{S}} y_i (1 - y_i(w^\\top x_i + b)) x_i\n$$\n$$\n\\nabla_b J = -2C \\sum_{i \\in \\mathcal{S}} y_i (1 - y_i(w^\\top x_i + b))\n$$\nStarting with an initial guess (e.g., $w=0, b=0$), the optimizer iteratively updates the parameters to find the optimal $(w^*, b^*)$. Once trained, the model's performance is evaluated on the test set $\\mathcal{D}_{\\mathrm{test}}$ by calculating its accuracy: the fraction of test sequences for which the predicted label $\\hat{y}(s) = \\mathrm{sign}(w^{*\\top} \\phi_k(s) + b^*)$ matches the true label.\n\nSecond, we address the unsupervised clustering baseline. This approach does not use labels during the learning phase. The task is to partition the feature vectors $\\{x_j\\}_{j=1}^M$ derived from the test set $\\mathcal{D}_{\\mathrm{test}}$ into exactly two non-empty clusters, $\\mathcal{C}_0$ and $\\mathcal{C}_1$. The partitioning must minimize the within-cluster sum of squares (WCSS), a standard objective for k-means clustering:\n$$\nW(\\mathcal{C}_0, \\mathcal{C}_1) = \\sum_{j \\in \\mathcal{C}_0} \\lVert x_j - \\mu_0 \\rVert_2^2 + \\sum_{j \\in \\mathcal{C}_1} \\lVert x_j - \\mu_1 \\rVert_2^2\n$$\nwhere $\\mu_0$ and $\\mu_1$ are the centroids (means) of the clusters. While finding the optimal partition for general $k$-means is NP-hard, the number of data points in the test set is small ($M=6$). This allows for an exact solution by enumerating all possible non-trivial partitions. The number of ways to partition a set of $M$ items into two non-empty subsets is given by the Stirling number of the second kind $S(M, 2) = 2^{M-1} - 1$. For $M=6$, this is $2^5 - 1 = 31$ unique partitions. We can iterate through each of these partitions, calculate the corresponding WCSS, and identify the partition that yields the global minimum. After finding the optimal clusters $\\mathcal{C}_0^*$ and $\\mathcal{C}_1^*$, we evaluate their quality by comparing them against the true labels. Since the cluster identities ($0$ and $1$) are arbitrary, we must test both possible mappings to the true labels $\\{-1, +1\\}$: ($\\mathcal{C}_0^* \\to -1, \\mathcal{C}_1^* \\to +1$) and ($\\mathcal{C}_0^* \\to +1, \\mathcal{C}_1^* \\to -1$). The unsupervised accuracy is defined as the maximum accuracy achieved between these two mappings.\n\nThe final procedure involves executing these two algorithmic frameworks for the specified parameters ($k, C$) and reporting the resulting test accuracies, rounded to three decimal places.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Solves the complete problem, including both supervised and unsupervised tasks.\n    \"\"\"\n    # Define the problem data\n    train_seqs = [\n        \"ATGGCGGCCGCGGGCGCCGCGGGCGACGGCTGA\", \"ATGCGCGCGCGGGCCGCGGCTGCGGCGTAG\",\n        \"ATGGGCGACGGCGGCGACGGCGGCGACTAA\", \"ATGGCCGCTGCGGCTGGCGCTGCGGCTTGA\",\n        \"ATGGCGGCGGCGGCGGCGGCGGCGGCGGCGTAA\", \"ATGGGCGCCGCGGGCGCCGCGGGCGCCTGA\",\n        \"TATATAAATAATATATATTTATATAATAATA\", \"AAATATATATTTAAATATATATATATAAAA\",\n        \"TTTATATATAAATATAATATATTTATAAAT\", \"AATAATAATATATTTATAAATAATATATAT\",\n        \"ATATATAAATATATAATATATAAATATATA\", \"TATATATAAATAAATATATATATAAATATA\"\n    ]\n    train_labels = np.array([1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1])\n\n    test_seqs = [\n        \"ATGGCGGGCGGGCGACGGCTAA\", \"ATGGCCGCGGCTGGCGCTGCGTAG\",\n        \"ATGGCGGCGGCGGCGGCGTGA\", \"AATATATATATAAATATATATAAATAATA\",\n        \"TATATTTATAAATATATATAAATATTTAT\", \"AAATAATATATATATAAATAATATATATA\"\n    ]\n    test_labels = np.array([1, 1, 1, -1, -1, -1])\n\n    alphabet_map = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n\n    def get_kmer_index(kmer):\n        \"\"\"Calculates the lexicographical index of a k-mer.\"\"\"\n        index = 0\n        for char in kmer:\n            index = index * 4 + alphabet_map[char]\n        return index\n\n    def phi_k(s, k):\n        \"\"\"Computes the k-mer frequency vector phi_k(s).\"\"\"\n        L = len(s)\n        dim = 4**k\n        if L  k:\n            return np.zeros(dim)\n        \n        counts = np.zeros(dim)\n        num_kmers = L - k + 1\n        \n        for i in range(num_kmers):\n            kmer = s[i:i+k]\n            idx = get_kmer_index(kmer)\n            counts[idx] += 1\n            \n        return counts / num_kmers\n\n    def objective_function(theta, X, y, C):\n        \"\"\"Computes J(w, b) and its gradient for L2-SVM.\"\"\"\n        N, D = X.shape\n        w = theta[:-1]\n        b = theta[-1]\n        \n        margins = y * (X.dot(w) + b)\n        loss_terms = 1 - margins\n        violations_mask = loss_terms > 0\n        \n        squared_hinge_loss = np.sum(loss_terms[violations_mask]**2)\n        objective_value = 0.5 * np.dot(w, w) + C * squared_hinge_loss\n        \n        grad = np.zeros_like(theta)\n        grad[:-1] = w\n        \n        if np.any(violations_mask):\n            common_grad_factor = -2 * C * loss_terms[violations_mask] * y[violations_mask]\n            grad[:-1] += np.dot(common_grad_factor, X[violations_mask, :])\n            grad[-1] += np.sum(common_grad_factor)\n        \n        return objective_value, grad\n\n    def solve_supervised(k, C):\n        \"\"\"Trains the SVM and computes test accuracy.\"\"\"\n        D = 4**k\n        X_train = np.array([phi_k(s, k) for s in train_seqs])\n        \n        initial_theta = np.zeros(D + 1)\n        res = minimize(\n            objective_function,\n            initial_theta,\n            args=(X_train, train_labels, C),\n            jac=True,\n            method='L-BFGS-B'\n        )\n        \n        w_opt, b_opt = res.x[:-1], res.x[-1]\n        \n        X_test = np.array([phi_k(s, k) for s in test_seqs])\n        \n        scores = X_test.dot(w_opt) + b_opt\n        predictions = np.sign(scores)\n        predictions[predictions == 0] = 1 # Per problem: sign(z>=0) = +1\n        \n        accuracy = np.mean(predictions == test_labels)\n        return accuracy\n\n    def solve_unsupervised(k):\n        \"\"\"Performs clustering and computes best-match accuracy.\"\"\"\n        M = len(test_seqs)\n        X_test = np.array([phi_k(s, k) for s in test_seqs])\n        \n        min_wcss = np.inf\n        best_partition = None\n        \n        # Enumerate all non-trivial partitions of M items into 2 clusters.\n        # Fix the first item in cluster 0 and iterate through assignments for the rest.\n        # This gives 2^(M-1) possibilities. The case where all items are in cluster 0\n        # (i=0) is excluded to ensure non-empty clusters.\n        num_items_to_partition = M - 1\n        for i in range(1, 2**num_items_to_partition):\n            c0_indices = [0]\n            c1_indices = []\n            \n            for j in range(num_items_to_partition):\n                if (i >> j)  1:\n                    c1_indices.append(j + 1)\n                else:\n                    c0_indices.append(j + 1)\n\n            C0 = X_test[c0_indices, :]\n            C1 = X_test[c1_indices, :]\n            \n            wcss = np.sum((C0 - np.mean(C0, axis=0))**2) + \\\n                   np.sum((C1 - np.mean(C1, axis=0))**2)\n\n            if wcss  min_wcss:\n                min_wcss = wcss\n                best_partition = (c0_indices, c1_indices)\n\n        c0_indices, c1_indices = best_partition\n        \n        # Mapping 1: C0 -> -1, C1 -> +1\n        pred1 = np.ones(M)\n        pred1[c0_indices] = -1\n        acc1 = np.mean(pred1 == test_labels)\n        \n        # Mapping 2: C0 -> +1, C1 -> -1\n        pred2 = np.ones(M)\n        pred2[c1_indices] = -1\n        acc2 = np.mean(pred2 == test_labels)\n        \n        return max(acc1, acc2)\n\n    # Execute all five test cases\n    results = [\n        solve_supervised(k=3, C=1.0),\n        solve_supervised(k=2, C=1.0),\n        solve_supervised(k=3, C=0.01),\n        solve_unsupervised(k=3),\n        solve_unsupervised(k=2)\n    ]\n    \n    # Format and print the final output\n    formatted_results = [f\"{r:.3f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2432827"}, {"introduction": "Moving beyond simple classification, this exercise explores a sophisticated unsupervised task in genomics: finding co-regulated genes. Instead of partitioning all genes into global groups, biclustering seeks local patterns—subsets of genes that show coherent behavior across a subset of conditions. By calculating the Mean Squared Residue ($\\mathrm{MSR}$) for candidate biclusters [@problem_id:2432840], you will learn to quantify the coherence of these patterns and gain insight into how unsupervised learning can uncover nuanced, hidden structures in high-dimensional biological data without relying on any labels.", "problem": "You are given small real-valued matrices representing gene expression levels, where each row corresponds to a gene and each column corresponds to an experimental condition. Let the matrix be denoted by $X \\in \\mathbb{R}^{G \\times C}$, with entries $x_{ij}$ for row (gene) index $i \\in \\{0,1,\\ldots,G-1\\}$ and column (condition) index $j \\in \\{0,1,\\ldots,C-1\\}$. A bicluster is a pair of index sets $(I,J)$ with $I \\subset \\{0,1,\\ldots,G-1\\}$ and $J \\subset \\{0,1,\\ldots,C-1\\}$, where $|I| \\ge 2$ and $|J| \\ge 2$.\n\nFor any bicluster $(I,J)$, define the Mean Squared Residue (MSR) as follows. Let the submatrix induced by $I$ and $J$ be $\\{x_{ij} : i \\in I, j \\in J\\}$. Define the row means\n$$\n\\bar{x}_{iJ} = \\frac{1}{|J|} \\sum_{j \\in J} x_{ij} \\quad \\text{for each } i \\in I,\n$$\nthe column means\n$$\n\\bar{x}_{Ij} = \\frac{1}{|I|} \\sum_{i \\in I} x_{ij} \\quad \\text{for each } j \\in J,\n$$\nand the overall mean\n$$\n\\bar{x}_{IJ} = \\frac{1}{|I|\\,|J|} \\sum_{i \\in I} \\sum_{j \\in J} x_{ij}.\n$$\nThe residue at $(i,j)$ is\n$$\nr_{ij} = x_{ij} - \\bar{x}_{iJ} - \\bar{x}_{Ij} + \\bar{x}_{IJ},\n$$\nand the Mean Squared Residue (MSR) of $(I,J)$ is\n$$\n\\mathrm{MSR}(I,J) = \\frac{1}{|I|\\,|J|} \\sum_{i \\in I} \\sum_{j \\in J} r_{ij}^2.\n$$\n\nFor a vector of binary labels across conditions, $y \\in \\{0,1\\}^C$, define the label-agreement score of a condition subset $J$ with $y$ as\n$$\nA(J,y) = \\frac{1}{C} \\max\\left\\{ \\sum_{j=0}^{C-1} \\mathbf{1}\\big( \\mathbf{1}(j \\in J) = \\mathbf{1}(y_j = 1) \\big), \\; \\sum_{j=0}^{C-1} \\mathbf{1}\\big( \\mathbf{1}(j \\in J) = \\mathbf{1}(y_j = 0) \\big) \\right\\},\n$$\nwhere $\\mathbf{1}(\\cdot)$ denotes the indicator function.\n\nFor each test case below, you are given:\n- a matrix $X$,\n- a binary label vector $y$ of length $C$,\n- a finite set $\\mathcal{S}$ of candidate biclusters, where each candidate is a pair of index lists $(I,J)$ with strictly increasing elements and satisfying $|I| \\ge 2$ and $|J| \\ge 2$.\n\nYour task for each test case is to select the bicluster $(I^\\star,J^\\star) \\in \\mathcal{S}$ that minimizes $\\mathrm{MSR}(I,J)$. In case of a tie in the $\\mathrm{MSR}$ value, select the lexicographically smallest pair by first comparing $I$ then, if necessary, comparing $J$, using the standard lexicographic order on strictly increasing integer lists.\n\nFor the selected $(I^\\star,J^\\star)$ in each test case, compute:\n- the gene-count $|I^\\star|$ (an integer),\n- the condition-count $|J^\\star|$ (an integer),\n- the minimal $\\mathrm{MSR}(I^\\star,J^\\star)$ as a real number rounded to six digits after the decimal point,\n- the agreement $A(J^\\star,y)$ as a real number rounded to six digits after the decimal point.\n\nYour program should produce a single line of output containing the concatenation of these four values for each test case, in order, as a comma-separated list enclosed in square brackets, with no spaces. In other words, if there are $T$ test cases, and for test case $t$ the four outputs are $(g_t, c_t, m_t, a_t)$, the single-line output must be\n$$\n[ g_1, c_1, m_1, a_1, g_2, c_2, m_2, a_2, \\ldots, g_T, c_T, m_T, a_T ].\n$$\nAll real numbers must be rounded to exactly six digits after the decimal point.\n\nTest Suite:\n\n- Test Case $1$:\n  - $X$ with $G=4$, $C=5$ given by rows:\n    - row $0$: $4,6,3,8,5$\n    - row $1$: $9,5,5,7,6$\n    - row $2$: $10,7,7,9,8$\n    - row $3$: $2,1,4,6,3$\n  - $y$: $0,0,1,1,0$\n  - $\\mathcal{S}$:\n    - $(I,J) = ([1,2],[2,3])$\n    - $(I,J) = ([0,1],[0,1])$\n    - $(I,J) = ([2,3],[1,4])$\n\n- Test Case $2$:\n  - $X$ with $G=3$, $C=3$ given by rows:\n    - row $0$: $1,2,3$\n    - row $1$: $2,3,4$\n    - row $2$: $3,4,5$\n  - $y$: $1,1,0$\n  - $\\mathcal{S}$:\n    - $(I,J) = ([0,1],[0,1])$\n    - $(I,J) = ([1,2],[1,2])$\n    - $(I,J) = ([0,2],[0,2])$\n\n- Test Case $3$:\n  - $X$ with $G=4$, $C=4$ given by rows:\n    - row $0$: $8,9,13,9$\n    - row $1$: $10,11,14,11$\n    - row $2$: $9,9,12,11$\n    - row $3$: $12,13,17,13$\n  - $y$: $1,1,0,1$\n  - $\\mathcal{S}$:\n    - $(I,J) = ([1,3],[0,2])$\n    - $(I,J) = ([0,2],[1,3])$\n    - $(I,J) = ([0,1],[2,3])$\n    - $(I,J) = ([2,3],[0,1])$\n\nAll indices are zero-based. No physical units apply. Angles are not used. Express the final real-valued results as decimal numbers rounded to six digits after the decimal point as specified above. The output must be a single line, exactly in the specified format.", "solution": "The user has provided a computational problem in bioinformatics which requires validation and, if valid, a solution.\n\n### Step 1: Extract Givens\n\nThe problem provides the following definitions and data:\n\n- A gene expression matrix $X \\in \\mathbb{R}^{G \\times C}$, where $G$ is the number of genes and $C$ is the number of experimental conditions.\n- A bicluster is a pair of index sets $(I,J)$ where $I \\subset \\{0,1,\\ldots,G-1\\}$, $J \\subset \\{0,1,\\ldots,C-1\\}$, with $|I| \\ge 2$ and $|J| \\ge 2$.\n- The submatrix induced by $(I,J)$ is $\\{x_{ij} : i \\in I, j \\in J\\}$.\n- The row means for a bicluster: $\\bar{x}_{iJ} = \\frac{1}{|J|} \\sum_{j \\in J} x_{ij}$ for each $i \\in I$.\n- The column means for a bicluster: $\\bar{x}_{Ij} = \\frac{1}{|I|} \\sum_{i \\in I} x_{ij}$ for each $j \\in J$.\n- The overall mean for a bicluster: $\\bar{x}_{IJ} = \\frac{1}{|I|\\,|J|} \\sum_{i \\in I} \\sum_{j \\in J} x_{ij}$.\n- The residue at $(i,j)$ for a bicluster: $r_{ij} = x_{ij} - \\bar{x}_{iJ} - \\bar{x}_{Ij} + \\bar{x}_{IJ}$.\n- The Mean Squared Residue (MSR) of a bicluster $(I,J)$: $\\mathrm{MSR}(I,J) = \\frac{1}{|I|\\,|J|} \\sum_{i \\in I} \\sum_{j \\in J} r_{ij}^2$.\n- A binary label vector for conditions: $y \\in \\{0,1\\}^C$.\n- The label-agreement score of a condition subset $J$ with $y$:\n$$\nA(J,y) = \\frac{1}{C} \\max\\left\\{ \\sum_{j=0}^{C-1} \\mathbf{1}\\big( \\mathbf{1}(j \\in J) = \\mathbf{1}(y_j = 1) \\big), \\; \\sum_{j=0}^{C-1} \\mathbf{1}\\big( \\mathbf{1}(j \\in J) = \\mathbf{1}(y_j = 0) \\big) \\right\\}\n$$\nwhere $\\mathbf{1}(\\cdot)$ is the indicator function.\n- The task is, for each test case, to select the bicluster $(I^\\star,J^\\star)$ from a given finite set of candidates $\\mathcal{S}$ that minimizes $\\mathrm{MSR}(I,J)$.\n- A tie-breaking rule: if MSR values are equal, the lexicographically smallest pair $(I,J)$ is chosen, by first comparing $I$ and then $J$. The index lists $I$ and $J$ are strictly increasing.\n- For the selected bicluster $(I^\\star, J^\\star)$, four values must be computed: $|I^\\star|$, $|J^\\star|$, $\\mathrm{MSR}(I^\\star,J^\\star)$, and $A(J^\\star,y)$.\n- Three specific test cases are provided, each with a matrix $X$, a vector $y$, and a set of candidate biclusters $\\mathcal{S}$.\n- The output format is specified as a single line string: a comma-separated list of the four computed values for each test case, concatenated, enclosed in square brackets. Real numbers must be rounded to six decimal places.\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem statement is subjected to validation.\n\n- **Scientifically Grounded**: The problem is based on biclustering analysis of gene expression data. The Mean Squared Residue (MSR) is a standard coherence measure for biclusters of the additive type, derived from a two-way Analysis of Variance (ANOVA) model. The label-agreement score is a well-defined metric for comparing a feature (the set of conditions in the bicluster) against external class labels. These are established concepts in bioinformatics and computational statistics. The problem is scientifically sound.\n- **Well-Posed**: The task requires selecting an optimal bicluster from a finite set of candidates $\\mathcal{S}$ based on minimizing a well-defined function, MSR. A clear and unambiguous tie-breaking rule (lexicographical order) is provided, which guarantees the existence and uniqueness of a solution. The subsequent calculations are deterministic. The problem is well-posed.\n- **Objective**: All definitions are given in precise mathematical terms. The data are numerical, and the evaluation criteria are objective. The problem is free from subjective or opinion-based statements.\n- **Completeness and Consistency**: Each test case provides all necessary information ($X$, $y$, $\\mathcal{S}$) to perform the required calculations. There are no missing definitions or contradictory constraints.\n- **Feasibility**: The data matrices are small and contain standard real numbers. The calculations are computationally feasible. There are no physically impossible conditions.\n- **Structure**: The problem is structured logically. The terms are defined before use. It does not contain circular reasoning or ambiguity.\n\n### Step 3: Verdict and Action\n\nThe problem is **valid** on all criteria. A reasoned solution will be provided.\n\n### Solution\n\nThe task is to find the optimal bicluster $(I^\\star, J^\\star)$ from a given candidate set $\\mathcal{S}$ for each test case. The optimality criterion is the minimization of the Mean Squared Residue (MSR), with a lexicographical tie-breaking rule. For the selected bicluster, we must compute its dimensions, MSR, and a label-agreement score.\n\nThe procedure for each test case is as follows:\n\n1.  Initialize a variable to store the best bicluster found so far, say $(I_{best}, J_{best})$, and its MSR, $MSR_{min}$. Initialize $MSR_{min}$ to a value larger than any possible MSR (e.g., infinity).\n\n2.  Iterate through each candidate bicluster $(I,J)$ in the provided set $\\mathcal{S}$.\n\n3.  For each candidate $(I,J)$, calculate its MSR. This requires several steps:\n    a.  Extract the submatrix $X_{I,J}$ from the full data matrix $X$. This submatrix has dimensions $|I| \\times |J|$.\n    b.  Calculate the means of each row of $X_{I,J}$, denoted $\\bar{x}_{iJ}$ for $i \\in I$.\n    c.  Calculate the means of each column of $X_{I,J}$, denoted $\\bar{x}_{Ij}$ for $j \\in J$.\n    d.  Calculate the overall mean of all elements in $X_{I,J}$, denoted $\\bar{x}_{IJ}$.\n    e.  For each element $x_{ij}$ in the submatrix (where $i \\in I, j \\in J$), compute the residue $r_{ij} = x_{ij} - \\bar{x}_{iJ} - \\bar{x}_{Ij} + \\bar{x}_{IJ}$.\n    f.  Compute the MSR as the mean of the squared residues: $\\mathrm{MSR}(I,J) = \\frac{1}{|I|\\,|J|} \\sum_{i \\in I} \\sum_{j \\in J} r_{ij}^2$.\n\n4.  Compare the computed $\\mathrm{MSR}(I,J)$ with the current minimum, $MSR_{min}$.\n    a.  If $\\mathrm{MSR}(I,J)  MSR_{min}$, update $MSR_{min} = \\mathrm{MSR}(I,J)$ and set $(I_{best}, J_{best}) = (I,J)$.\n    b.  If $\\mathrm{MSR}(I,J) = MSR_{min}$, apply the tie-breaking rule. Compare the current candidate $(I,J)$ with $(I_{best}, J_{best})$ lexicographically. The pair $(I,J)$ is lexicographically smaller than $(I',J')$ if $I$ is lexicographically smaller than $I'$, or if $I=I'$ and $J$ is lexicographically smaller than $J'$. If $(I,J)$ is smaller, update $(I_{best}, J_{best}) = (I,J)$.\n\n5.  After iterating through all candidates in $\\mathcal{S}$, the final $(I_{best}, J_{best})$ is the optimal bicluster $(I^\\star, J^\\star)$.\n\n6.  Once $(I^\\star, J^\\star)$ is determined, compute the four required output values:\n    a.  Gene count: $|I^\\star|$.\n    b.  Condition count: $|J^\\star|$.\n    c.  Minimal MSR: $\\mathrm{MSR}(I^\\star, J^\\star)$. This value must be rounded to six decimal places.\n    d.  Label-agreement score $A(J^\\star, y)$. The calculation is as follows:\n        i.  Let $C$ be the total number of columns in the original matrix $X$.\n        ii.  Let $v_{J^\\star}$ be a binary vector of length $C$ where the $j$-th element is $1$ if $j \\in J^\\star$ and $0$ otherwise.\n        iii. Calculate the direct agreement count: $S_{direct} = \\sum_{j=0}^{C-1} \\mathbf{1}( (v_{J^\\star})_j = y_j )$. This is the number of positions where the bicluster's condition membership matches the labels in $y$.\n        iv. Calculate the inverted agreement count: $S_{inverted} = \\sum_{j=0}^{C-1} \\mathbf{1}( (v_{J^\\star})_j = (1-y_j) )$. This is the number of positions where the bicluster's condition membership matches the inverted labels.\n        v. The score is $A(J^\\star, y) = \\frac{1}{C} \\max(S_{direct}, S_{inverted})$. This value must be rounded to six decimal places.\n\n7.  Collect these four values for the current test case. Repeat the entire process for all test cases. Finally, format all collected values into a single comma-separated string as specified.\n\nThis procedure is implemented in the Python code provided in the final answer. The use of the `numpy` library facilitates efficient array operations for submatrix extraction and mean calculations. The tie-breaking logic is naturally handled by comparing tuples of the form $(\\mathrm{MSR}, I, J)$, where $I$ and $J$ are themselves tuples of indices.", "answer": "```python\nimport numpy as np\n\ndef calculate_msr(X, I, J):\n    \"\"\"\n    Calculates the Mean Squared Residue (MSR) for a given bicluster.\n    \n    Args:\n        X (np.ndarray): The full data matrix.\n        I (list or tuple): A list of row indices for the bicluster.\n        J (list or tuple): A list of column indices for the bicluster.\n        \n    Returns:\n        float: The MSR value.\n    \"\"\"\n    # Defensive check against empty indices\n    if not I or not J:\n        return float('inf')\n    \n    # Extract submatrix using np.ix_ for advanced indexing\n    submatrix = X[np.ix_(I, J)]\n    \n    num_rows, num_cols = submatrix.shape\n    if num_rows == 0 or num_cols == 0:\n        return float('inf')\n\n    # Calculate means\n    row_means = submatrix.mean(axis=1, keepdims=True)\n    col_means = submatrix.mean(axis=0, keepdims=True)\n    overall_mean = submatrix.mean()\n\n    # Calculate residues using broadcasting\n    residues = submatrix - row_means - col_means + overall_mean\n\n    # Calculate MSR\n    msr = np.mean(residues**2)\n    return msr\n\ndef calculate_agreement_score(J, y, C):\n    \"\"\"\n    Calculates the label-agreement score.\n    \n    Args:\n        J (list or tuple): The list of column indices for the selected bicluster.\n        y (np.ndarray): The binary label vector.\n        C (int): The total number of conditions (columns in X).\n    \n    Returns:\n        float: The label-agreement score.\n    \"\"\"\n    # Create a binary vector for the condition set J\n    v_J = np.zeros(C, dtype=int)\n    if J: # Ensure J is not empty\n        v_J[J] = 1\n\n    # Calculate direct and inverted agreement counts\n    direct_agreement = np.sum(v_J == y)\n    inverted_agreement = np.sum(v_J == (1 - y))\n\n    # The score is the max of the two agreements, normalized by C\n    score = max(direct_agreement, inverted_agreement) / C\n    return score\n\ndef solve():\n    \"\"\"\n    Main function to process all test cases and print the final result.\n    \"\"\"\n    test_cases = [\n        {\n            \"X\": np.array([\n                [4, 6, 3, 8, 5],\n                [9, 5, 5, 7, 6],\n                [10, 7, 7, 9, 8],\n                [2, 1, 4, 6, 3]\n            ], dtype=float),\n            \"y\": np.array([0, 0, 1, 1, 0], dtype=int),\n            \"S\": [\n                ([1, 2], [2, 3]),\n                ([0, 1], [0, 1]),\n                ([2, 3], [1, 4])\n            ]\n        },\n        {\n            \"X\": np.array([\n                [1, 2, 3],\n                [2, 3, 4],\n                [3, 4, 5]\n            ], dtype=float),\n            \"y\": np.array([1, 1, 0], dtype=int),\n            \"S\": [\n                ([0, 1], [0, 1]),\n                ([1, 2], [1, 2]),\n                ([0, 2], [0, 2])\n            ]\n        },\n        {\n            \"X\": np.array([\n                [8, 9, 13, 9],\n                [10, 11, 14, 11],\n                [9, 9, 12, 11],\n                [12, 13, 17, 13]\n            ], dtype=float),\n            \"y\": np.array([1, 1, 0, 1], dtype=int),\n            \"S\": [\n                ([1, 3], [0, 2]),\n                ([0, 2], [1, 3]),\n                ([0, 1], [2, 3]),\n                ([2, 3], [0, 1])\n            ]\n        }\n    ]\n\n    final_results = []\n    \n    for case in test_cases:\n        X, y, S = case[\"X\"], case[\"y\"], case[\"S\"]\n        _, C = X.shape\n\n        best_bicluster_info = (float('inf'), (), ())\n\n        for I, J in S:\n            msr = calculate_msr(X, I, J)\n            # Use tuple comparison for MSR and lexicographical tie-breaking\n            # The structure for comparison is (MSR, I_tuple, J_tuple)\n            current_bicluster_info = (msr, tuple(I), tuple(J))\n\n            if current_bicluster_info  best_bicluster_info:\n                best_bicluster_info = current_bicluster_info\n\n        min_msr, I_star_tuple, J_star_tuple = best_bicluster_info\n        I_star = list(I_star_tuple)\n        J_star = list(J_star_tuple)\n\n        # Compute the four required values for the best bicluster\n        gene_count = len(I_star)\n        condition_count = len(J_star)\n        agreement_score = calculate_agreement_score(J_star, y, C)\n\n        # Format floating-point numbers to six decimal places\n        msr_str = f\"{min_msr:.6f}\"\n        agreement_str = f\"{agreement_score:.6f}\"\n\n        final_results.extend([gene_count, condition_count, msr_str, agreement_str])\n\n    # Print the final output in the exact required format\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n```", "id": "2432840"}, {"introduction": "Our final practice bridges the gap between supervised and unsupervised learning by building a realistic two-stage predictive pipeline. In bioinformatics, high-dimensional feature spaces can overwhelm supervised models. This exercise [@problem_id:2432878] demonstrates a powerful solution: first, an unsupervised method learns a compact, low-dimensional latent representation, $z^{(\\mathrm{lat})}$, of the data. This learned representation then serves as a new, more informative set of features for a supervised model to predict a clinical outcome, showcasing how the two paradigms can work in synergy to create more effective and interpretable models.", "problem": "You are to write a complete, runnable program that performs a two-stage learning pipeline on simulated gene expression data for multiple test cases, combining an unsupervised representation learning stage with a supervised prediction stage. The objectives and the data-generating process are specified mathematically below, and your program must implement these definitions exactly.\n\nData-generating process. For each test case, generate a training set and an independent test set as follows. Let the number of training patients be $n_{\\mathrm{train}}$, the number of test patients be $n_{\\mathrm{test}}$, the number of genes be $p$, and the true latent dimension be $r^\\star$. Let the unsupervised latent dimension used by the learner be $k$, where $k$ is an integer in the range $0 \\le k \\le \\min\\{p,n_{\\mathrm{train}}-1\\}$. Fix a base random seed $s$ for each test case to ensure reproducibility.\n\n- Draw a gene loading matrix $W \\in \\mathbb{R}^{p \\times r^\\star}$ with independent standard normal entries, using the base seed $s$.\n- Draw a survival coefficient vector $b \\in \\mathbb{R}^{r^\\star}$ with independent standard normal entries, using the same base seed $s$ and continuing the sequence of generated values.\n- Draw training latent factors $Z_{\\mathrm{train}} \\in \\mathbb{R}^{n_{\\mathrm{train}} \\times r^\\star}$ with independent standard normal entries, using the same base seed $s$ and continuing the sequence of generated values.\n- Draw training gene expression noise $E^{(X)}_{\\mathrm{train}} \\in \\mathbb{R}^{n_{\\mathrm{train}} \\times p}$ with independent entries distributed as $\\mathcal{N}(0,\\sigma_X^2)$, using the same base seed $s$ and continuing the sequence of generated values.\n- Draw training survival noise $e^{(y)}_{\\mathrm{train}} \\in \\mathbb{R}^{n_{\\mathrm{train}}}$ with independent entries distributed as $\\mathcal{N}(0,\\sigma_y^2)$, using the same base seed $s$ and continuing the sequence of generated values.\n- Define training gene expression as $X_{\\mathrm{train}} = Z_{\\mathrm{train}} W^\\top + E^{(X)}_{\\mathrm{train}} \\in \\mathbb{R}^{n_{\\mathrm{train}} \\times p}$.\n- Define training survival times as $y_{\\mathrm{train}} = Z_{\\mathrm{train}} b + e^{(y)}_{\\mathrm{train}} \\in \\mathbb{R}^{n_{\\mathrm{train}}}$.\n\nFor the test set, use the same $W$ and $b$ as above but independent latent factors and noise:\n- Use the test seed $s+1$ to draw $Z_{\\mathrm{test}} \\in \\mathbb{R}^{n_{\\mathrm{test}} \\times r^\\star}$ with independent standard normal entries, $E^{(X)}_{\\mathrm{test}} \\in \\mathbb{R}^{n_{\\mathrm{test}} \\times p}$ with independent entries distributed as $\\mathcal{N}(0,\\sigma_X^2)$, and $e^{(y)}_{\\mathrm{test}} \\in \\mathbb{R}^{n_{\\mathrm{test}}}$ with independent entries distributed as $\\mathcal{N}(0,\\sigma_y^2)$, respectively.\n- Define test gene expression and survival as $X_{\\mathrm{test}} = Z_{\\mathrm{test}} W^\\top + E^{(X)}_{\\mathrm{test}}$ and $y_{\\mathrm{test}} = Z_{\\mathrm{test}} b + e^{(y)}_{\\mathrm{test}}$.\n\nTwo-stage learning pipeline. Your program must perform the following two optimization problems on the training data and then evaluate on the test data.\n\n1. Unsupervised representation learning. Learn a linear encoder $E: \\mathbb{R}^p \\to \\mathbb{R}^k$ and a linear decoder $D: \\mathbb{R}^k \\to \\mathbb{R}^p$, together with a centering vector $\\mu \\in \\mathbb{R}^p$, by solving\n$$\n\\min_{E,D,\\mu} \\ \\frac{1}{n_{\\mathrm{train}}} \\sum_{i=1}^{n_{\\mathrm{train}}} \\left\\| x_i - D\\!\\left(E\\!\\left(x_i - \\mu\\right)\\right) \\right\\|_2^2,\n$$\nwhere $x_i \\in \\mathbb{R}^p$ denotes the $i$-th row of $X_{\\mathrm{train}}$. The learned encoder $E$ must be linear and the learned decoder $D$ must be linear. Use the learned encoder and centering vector to produce latent representations $z_i^{(\\mathrm{lat})} = E(x_i - \\mu) \\in \\mathbb{R}^k$ for each training sample $i \\in \\{1,\\dots,n_{\\mathrm{train}}\\}$, and for each test sample $j \\in \\{1,\\dots,n_{\\mathrm{test}}\\}$ produce $z_j^{(\\mathrm{lat,test})} = E(x^{(\\mathrm{test})}_j - \\mu)$ using the same $E$ and $\\mu$ estimated from the training data.\n\n2. Supervised prediction. Learn an affine predictor $g: \\mathbb{R}^k \\to \\mathbb{R}$ of the form $g(z) = c^\\top z + d$ by solving\n$$\n\\min_{c \\in \\mathbb{R}^k, \\ d \\in \\mathbb{R}} \\ \\frac{1}{n_{\\mathrm{train}}} \\sum_{i=1}^{n_{\\mathrm{train}}} \\left( y_i - \\left(c^\\top z_i^{(\\mathrm{lat})} + d\\right) \\right)^2,\n$$\nwhere $y_i$ denotes the $i$-th entry of $y_{\\mathrm{train}}$. Using the learned $(c,d)$, compute predictions $\\hat{y}_j = c^\\top z_j^{(\\mathrm{lat,test})} + d$ for each test sample $j$, and evaluate the test mean squared error\n$$\n\\mathrm{MSE}_{\\mathrm{test}} = \\frac{1}{n_{\\mathrm{test}}} \\sum_{j=1}^{n_{\\mathrm{test}}} \\left( \\hat{y}_j - y^{(\\mathrm{test})}_j \\right)^2.\n$$\n\nTest suite. Your program must implement the above process and output the test mean squared error for each of the following parameter settings, using the exact generation and training procedures described. Each test case is a tuple $(s, n_{\\mathrm{train}}, n_{\\mathrm{test}}, p, r^\\star, k, \\sigma_X, \\sigma_y)$:\n\n- Test case $1$: $(7, 120, 80, 60, 5, 5, 0.3, 0.5)$.\n- Test case $2$: $(13, 150, 150, 80, 6, 6, 2.0, 2.0)$.\n- Test case $3$: $(21, 50, 50, 200, 3, 3, 0.5, 0.5)$.\n- Test case $4$: $(1, 100, 100, 70, 8, 2, 0.4, 0.4)$.\n- Test case $5$: $(99, 100, 100, 50, 4, 0, 0.5, 0.5)$.\n\nAnswer specification and output format. Your program must produce a single line of output containing the list of test mean squared errors for the test cases in the order listed above. Each value must be rounded to exactly $6$ decimal places. The output must be a comma-separated list enclosed in square brackets, for example, $[\\mathrm{mse}_1,\\mathrm{mse}_2,\\dots]$, with no extra spaces or text.", "solution": "The problem statement is valid. It is scientifically grounded, well-posed, objective, and provides a complete and consistent specification for a computational experiment. The task involves a standard two-stage machine learning pipeline applied to simulated data, a common practice in bioinformatics research for evaluating methods. The data-generating process is a linear factor model, and the learning algorithms are principal component analysis and ordinary least squares regression, both of which are fundamental and well-understood. The parameters for all test cases are within valid ranges. We may therefore proceed with the solution.\n\nThe problem requires the implementation of a two-stage learning pipeline. The first stage is unsupervised, learning a low-dimensional representation of the gene expression data. The second is supervised, using this learned representation to predict a clinical outcome (survival time).\n\n**Stage 1: Unsupervised Representation Learning**\n\nThe first optimization problem is:\n$$\n\\min_{E,D,\\mu} \\ \\frac{1}{n_{\\mathrm{train}}} \\sum_{i=1}^{n_{\\mathrm{train}}} \\left\\| x_i - D\\!\\left(E\\!\\left(x_i - \\mu\\right)\\right) \\right\\|_2^2\n$$\nwhere $E: \\mathbb{R}^p \\to \\mathbb{R}^k$ and $D: \\mathbb{R}^k \\to \\mathbb{R}^p$ are linear transformations and $\\mu \\in \\mathbb{R}^p$ is a centering vector. This objective function seeks to minimize the reconstruction error of a linear autoencoder. The solution to this problem is given by Principal Component Analysis (PCA).\n\nFirst, the optimal centering vector $\\mu$ is the empirical mean of the training data vectors $x_i$:\n$$\n\\mu = \\frac{1}{n_{\\mathrm{train}}} \\sum_{i=1}^{n_{\\mathrm{train}}} x_i = \\frac{1}{n_{\\mathrm{train}}} X_{\\mathrm{train}}^\\top \\mathbf{1}_{n_{\\mathrm{train}}}\n$$\nLet $\\bar{X}_{\\mathrm{train}} = X_{\\mathrm{train}} - \\mathbf{1}_{n_{\\mathrm{train}}} \\mu^\\top$ be the centered training data matrix. The optimization problem reduces to finding the best rank-$k$ approximation of $\\bar{X}_{\\mathrm{train}}$. By the Eckart-Young-Mirsky theorem, this is achieved using the Singular Value Decomposition (SVD) of $\\bar{X}_{\\mathrm{train}}$.\n\nLet the SVD of the centered data matrix be $\\bar{X}_{\\mathrm{train}} = U S V^\\top$, where the columns of $V \\in \\mathbb{R}^{p \\times p}$ are the principal components (eigenvectors of the sample covariance matrix). Let $V_k \\in \\mathbb{R}^{p \\times k}$ be the matrix whose columns are the first $k$ principal components, corresponding to the $k$ largest singular values.\n\nThe linear encoder $E$ projects the centered data onto the basis defined by these $k$ components. Its matrix representation is $V_k^\\top$. The learned latent representations for the training samples are:\n$$\nZ^{(\\mathrm{lat})} = \\bar{X}_{\\mathrm{train}} V_k \\in \\mathbb{R}^{n_{\\mathrm{train}} \\times k}\n$$\nFor the test data $X_{\\mathrm{test}}$, the latent representations are computed by first centering the data using the mean $\\mu$ from the training set, and then applying the same projection:\n$$\nZ^{(\\mathrm{lat,test})} = (X_{\\mathrm{test}} - \\mathbf{1}_{n_{\\mathrm{test}}} \\mu^\\top) V_k\n$$\n\n**Stage 2: Supervised Prediction**\n\nThe second stage involves learning an affine predictor $g(z) = c^\\top z + d$ by solving the following ordinary least squares (OLS) problem on the training data:\n$$\n\\min_{c \\in \\mathbb{R}^k, \\ d \\in \\mathbb{R}} \\ \\frac{1}{n_{\\mathrm{train}}} \\sum_{i=1}^{n_{\\mathrm{train}}} \\left( y_i - \\left(c^\\top z_i^{(\\mathrm{lat})} + d\\right) \\right)^2\n$$\nTo solve this, we can augment the latent representation matrix $Z^{(\\mathrm{lat})}$ with a column of ones to account for the intercept term $d$. Let $\\tilde{Z}^{(\\mathrm{lat})} = [Z^{(\\mathrm{lat})} \\ \\mathbf{1}_{n_{\\mathrm{train}}}] \\in \\mathbb{R}^{n_{\\mathrm{train}} \\times (k+1)}$ and $\\tilde{c} = [c^\\top, d]^\\top \\in \\mathbb{R}^{k+1}$. The objective becomes minimizing $\\|\\mathbf{y}_{\\mathrm{train}} - \\tilde{Z}^{(\\mathrm{lat})} \\tilde{c}\\|_2^2$.\n\nThe solution is found by solving the normal equations, yielding:\n$$\n\\tilde{c}_{\\mathrm{opt}} = (\\tilde{Z}^{(\\mathrm{lat})\\top} \\tilde{Z}^{(\\mathrm{lat})})^\\dagger \\tilde{Z}^{(\\mathrm{lat})\\top} \\mathbf{y}_{\\mathrm{train}}\n$$\nwhere $\\dagger$ denotes the Moore-Penrose pseudoinverse, which provides a stable solution even if the matrix is not invertible.\n\nA special case arises when $k=0$. Here, the latent space is trivial (dimension zero), so $Z^{(\\mathrm{lat})}$ is an empty matrix. The model simplifies to $g(z)=d$. The optimization problem becomes $\\min_d \\sum_i (y_i - d)^2$, whose solution is the mean of the training labels: $d = \\bar{y}_{\\mathrm{train}} = \\frac{1}{n_{\\mathrm{train}}} \\sum_i y_i$.\n\n**Evaluation**\n\nFinally, predictions for the test set are generated using the learned coefficients $\\tilde{c}_{\\mathrm{opt}} = [c_{\\mathrm{opt}}^\\top, d_{\\mathrm{opt}}]^\\top$ and the test latent representations $Z^{(\\mathrm{lat,test})}$.\nFor $k > 0$, we form an augmented test matrix $\\tilde{Z}^{(\\mathrm{lat,test})} = [Z^{(\\mathrm{lat,test})} \\ \\mathbf{1}_{n_{\\mathrm{test}}}]$, and the predictions are $\\hat{\\mathbf{y}}_{\\mathrm{test}} = \\tilde{Z}^{(\\mathrm{lat,test})} \\tilde{c}_{\\mathrm{opt}}$.\nFor $k=0$, the prediction for every test sample is simply the constant value $\\hat{y}_j = d_{\\mathrm{opt}} = \\bar{y}_{\\mathrm{train}}$.\n\nThe performance is evaluated using the test Mean Squared Error (MSE):\n$$\n\\mathrm{MSE}_{\\mathrm{test}} = \\frac{1}{n_{\\mathrm{test}}} \\sum_{j=1}^{n_{\\mathrm{test}}} \\left( \\hat{y}_j - y^{(\\mathrm{test})}_j \\right)^2\n$$\nThe following program implements this entire procedure for each of the specified test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve_pipeline(s, n_train, n_test, p, r_star, k, sigma_X, sigma_y):\n    \"\"\"\n    Implements the full data generation and two-stage learning pipeline for a single test case.\n    \n    Args:\n        s (int): Base random seed.\n        n_train (int): Number of training patients.\n        n_test (int): Number of test patients.\n        p (int): Number of genes.\n        r_star (int): True latent dimension.\n        k (int): Unsupervised latent dimension for the learner.\n        sigma_X (float): Standard deviation of gene expression noise.\n        sigma_y (float): Standard deviation of survival noise.\n        \n    Returns:\n        float: The test mean squared error.\n    \"\"\"\n    \n    # --- Data Generation ---\n    # Training data generation\n    rng_train = np.random.default_rng(s)\n    W = rng_train.standard_normal(size=(p, r_star))\n    b = rng_train.standard_normal(size=r_star)\n    Z_train = rng_train.standard_normal(size=(n_train, r_star))\n    E_X_train = rng_train.normal(scale=sigma_X, size=(n_train, p))\n    e_y_train = rng_train.normal(scale=sigma_y, size=n_train)\n    \n    X_train = Z_train @ W.T + E_X_train\n    y_train = Z_train @ b + e_y_train\n    \n    # Test data generation\n    rng_test = np.random.default_rng(s + 1)\n    Z_test = rng_test.standard_normal(size=(n_test, r_star))\n    E_X_test = rng_test.normal(scale=sigma_X, size=(n_test, p))\n    e_y_test = rng_test.normal(scale=sigma_y, size=n_test)\n    \n    X_test = Z_test @ W.T + E_X_test\n    y_test = Z_test @ b + e_y_test\n\n    # --- Two-Stage Learning Pipeline ---\n\n    # Stage 1: Unsupervised Representation Learning (PCA)\n    mu = np.mean(X_train, axis=0)\n    X_train_centered = X_train - mu\n    \n    if k > 0:\n        # SVD on centered training data\n        # full_matrices=False is more efficient\n        _, _, Vt = np.linalg.svd(X_train_centered, full_matrices=False)\n        \n        # Encoder is based on the top k principal components (right singular vectors)\n        # Vt is V.T, so we take the first k rows and transpose\n        Vk = Vt[:k, :].T  # Shape: (p, k)\n        \n        # Get latent representations for training and test sets\n        Z_lat_train = X_train_centered @ Vk\n        X_test_centered = X_test - mu\n        Z_lat_test = X_test_centered @ Vk\n    \n    # Stage 2: Supervised Prediction (OLS)\n    if k == 0:\n        # If k=0, the model is y = d. The best predictor is the mean of y_train.\n        d = np.mean(y_train)\n        y_hat_test = np.full(n_test, d)\n    else:\n        # Augment latent features with a column of ones for the intercept\n        Z_train_aug = np.c_[Z_lat_train, np.ones(n_train)]\n        \n        # Solve for coefficients (c and d) using least squares\n        # coeffs will contain [c_1, ..., c_k, d]\n        coeffs, _, _, _ = np.linalg.lstsq(Z_train_aug, y_train, rcond=None)\n        \n        # Predict on the test set\n        Z_test_aug = np.c_[Z_lat_test, np.ones(n_test)]\n        y_hat_test = Z_test_aug @ coeffs\n\n    # --- Evaluation ---\n    mse_test = np.mean((y_hat_test - y_test)**2)\n    return mse_test\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    # Test cases: (s, n_train, n_test, p, r_star, k, sigma_X, sigma_y)\n    test_cases = [\n        (7, 120, 80, 60, 5, 5, 0.3, 0.5),\n        (13, 150, 150, 80, 6, 6, 2.0, 2.0),\n        (21, 50, 50, 200, 3, 3, 0.5, 0.5),\n        (1, 100, 100, 70, 8, 2, 0.4, 0.4),\n        (99, 100, 100, 50, 4, 0, 0.5, 0.5),\n    ]\n\n    results = []\n    for params in test_cases:\n        mse = solve_pipeline(*params)\n        results.append(mse)\n\n    # Format the output as specified: a comma-separated list in brackets,\n    # with each value rounded to 6 decimal places.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2432878"}]}