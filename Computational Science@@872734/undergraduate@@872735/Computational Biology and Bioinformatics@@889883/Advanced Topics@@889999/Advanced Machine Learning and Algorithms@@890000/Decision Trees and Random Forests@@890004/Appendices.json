{"hands_on_practices": [{"introduction": "This first exercise provides a comprehensive, hands-on guide to building a Random Forest classifier from its fundamental components. By working through a cheminformatics problem of predicting fluorescence, you will implement the core mechanics of the algorithm, including the Gini impurity criterion for node splitting, recursive tree construction, and the final ensemble voting process. Mastering this implementation [@problem_id:2384429] will solidify your understanding of how decision trees work together to form a powerful and robust classifier.", "problem": "Construct a complete, runnable program that implements a binary classifier based on an ensemble of decision trees for a cheminformatics classification task. The classifier must follow the mathematical definition of a Random Forest (RF), interpreted here as a majority vote of axis-aligned, binary, finite-depth decision trees (Decision Trees (DT)) trained on a fixed training set of compounds represented by structural descriptors. The task is to predict whether a compound is fluorescent, encoded as class label $1$ for fluorescent and $0$ for non-fluorescent.\n\nDefinitions and data:\n- Each compound is represented by a feature vector $\\mathbf{x} \\in \\mathbb{R}^d$ with $d=2$ features:\n  - Feature $0$: longest conjugated system length in alternating bonds (integer).\n  - Feature $1$: electron donor atom count (integer).\n- The labeled training set is $\\mathcal{D}=\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n$ with $n=8$ and labels $y_i \\in \\{0,1\\}$. The training examples are:\n  - $\\mathbf{x}_1=(3,0)$ with $y_1=0$.\n  - $\\mathbf{x}_2=(4,1)$ with $y_2=0$.\n  - $\\mathbf{x}_3=(5,0)$ with $y_3=0$.\n  - $\\mathbf{x}_4=(6,1)$ with $y_4=1$.\n  - $\\mathbf{x}_5=(7,1)$ with $y_5=1$.\n  - $\\mathbf{x}_6=(8,2)$ with $y_6=1$.\n  - $\\mathbf{x}_7=(9,1)$ with $y_7=1$.\n  - $\\mathbf{x}_8=(10,0)$ with $y_8=1$.\n- The query set for which predictions must be returned is the following list of three compounds:\n  - $\\mathbf{q}_1=(5,1)$.\n  - $\\mathbf{q}_2=(6,0)$.\n  - $\\mathbf{q}_3=(8,1)$.\n\nMathematical specification of the decision tree classifier $h$:\n- A node $v$ is associated with an index set $S_v \\subseteq \\{1,\\dots,n\\}$ of training examples that reach $v$ and a depth $\\mathrm{depth}(v)\\in \\mathbb{N}_0$ with the root having $\\mathrm{depth}=0$ and $S_{\\text{root}}=\\{1,\\dots,n\\}$.\n- For any node $v$, define the class counts $c_k(v)=|\\{i\\in S_v: y_i=k\\}|$ for $k\\in\\{0,1\\}$, the size $|S_v|$, and the empirical class probabilities $p_k(v)=c_k(v)/|S_v|$. The Gini impurity at $v$ is\n  $$G(v)=1-\\sum_{k\\in\\{0,1\\}} p_k(v)^2.$$\n- Candidate axis-aligned splits at a node $v$ are defined as follows. For each feature index $j\\in\\{0,1\\}$ and the multiset $\\{x_{i,j} : i\\in S_v\\}$, take its sorted list of unique values $z_1z_2\\dotsz_m$. The candidate thresholds are the midpoints\n  $$\\tau_\\ell=\\frac{z_\\ell+z_{\\ell+1}}{2}\\quad \\text{for }\\ell\\in\\{1,\\dots,m-1\\}.$$\n  A binary split is parameterized by a pair $(j,\\tau)$ and partitions $S_v$ into\n  $$S_v^{\\text{L}}(j,\\tau)=\\{i\\in S_v: x_{i,j}\\le \\tau\\},\\quad S_v^{\\text{R}}(j,\\tau)=\\{i\\in S_v: x_{i,j}\\tau\\}.$$\n  A candidate split is valid only if $|S_v^{\\text{L}}(j,\\tau)|\\ge n_{\\min}$ and $|S_v^{\\text{R}}(j,\\tau)|\\ge n_{\\min}$ where $n_{\\min}\\in \\mathbb{N}$ is the minimum leaf size parameter.\n- For any valid split $(j,\\tau)$ at $v$, define the weighted post-split impurity\n  $$\\Phi(v;j,\\tau)=\\frac{|S_v^{\\text{L}}(j,\\tau)|}{|S_v|}G\\big(v^{\\text{L}}(j,\\tau)\\big)+\\frac{|S_v^{\\text{R}}(j,\\tau)|}{|S_v|}G\\big(v^{\\text{R}}(j,\\tau)\\big),$$\n  where $v^{\\text{L}}(j,\\tau)$ and $v^{\\text{R}}(j,\\tau)$ denote the left and right child nodes formed by the split. Among the valid candidate splits, select the one that minimizes $\\Phi(v;j,\\tau)$. If there are ties in $\\Phi$, break them by choosing the smallest threshold $\\tau$, and if still tied, by choosing the smallest feature index $j$.\n- Feature subsampling at each internal node: let $m_{\\text{try}}\\in\\{1,2\\}$ be the number of features to consider at each split. For a node $v$, select a subset $F_v\\subseteq\\{0,1\\}$ of size $m_{\\text{try}}$ uniformly at random without replacement if $m_{\\text{try}}2$; otherwise take all features if $m_{\\text{try}}=2$. Restrict candidate splits to $j\\in F_v$.\n- Stopping rules: a node $v$ becomes a leaf if any of the following holds: $G(v)=0$, or $\\mathrm{depth}(v)\\ge D_{\\max}$ where $D_{\\max}\\in \\mathbb{N}_0$ is the maximum depth parameter, or there is no valid split due to the $n_{\\min}$ constraint or because $|S_v|2$. A leaf $v$ predicts the class\n  $$\\hat{y}(v)=\\arg\\max_{k\\in\\{0,1\\}} c_k(v),$$\n  with ties resolved in favor of class $0$.\n- A Decision Tree $h$ maps any $\\mathbf{x}\\in\\mathbb{R}^2$ to a class in $\\{0,1\\}$ by starting at the root and repeatedly applying the rule \"go to the left child if $x_j\\le \\tau$, else go to the right child\" using the stored $(j,\\tau)$ at each internal node, until a leaf is reached; the output is the leaf’s $\\hat{y}(v)$.\n\nRandom Forest aggregation:\n- An RF with $T\\in\\mathbb{N}$ trees is the function\n  $$H(\\mathbf{x})=\\mathrm{majority}\\big(h_1(\\mathbf{x}),h_2(\\mathbf{x}),\\dots,h_T(\\mathbf{x})\\big),$$\n  where ties are resolved in favor of class $0$. Each tree $h_t$ is trained on the training set $\\mathcal{D}$ either with or without bootstrapping as specified per test case: bootstrapping \"off\" uses the full training set indices $S_{\\text{root}}=\\{1,\\dots,n\\}$; bootstrapping \"on\" uses a sample of size $n$ drawn with replacement from $\\{1,\\dots,n\\}$. For reproducibility, each test case specifies an integer seed $s\\in\\mathbb{Z}$; when bootstrapping is on, use independent replicas seeded deterministically per tree index (for example, by offsetting $s$ by the tree index). Feature subsampling follows the rule above. All other choices, splits, and tie-breaks must follow the definitions stated here to ensure determinism.\n\nTest suite:\n- Use the training data and query set listed above. Evaluate the RF on the following three parameter sets. In each tuple, the parameters are $(T, D_{\\max}, m_{\\text{try}}, n_{\\min}, s, \\text{bootstrap})$:\n  - Case A (general case): $(7, 3, 2, 1, 13, \\text{False})$.\n  - Case B (boundary depth): $(1, 1, 2, 1, 7, \\text{False})$.\n  - Case C (edge case with minimum leaf size blocking any split): $(5, 3, 2, 5, 5, \\text{False})$.\n- For each case, compute the RF predictions $H(\\mathbf{q}_1)$, $H(\\mathbf{q}_2)$, and $H(\\mathbf{q}_3)$ as integers in $\\{0,1\\}$, in that order.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for all three cases as a comma-separated list of three lists, each inner list giving the three predictions in order for $(\\mathbf{q}_1,\\mathbf{q}_2,\\mathbf{q}_3)$. The exact required format is\n  $$\\texttt{[[a\\_1,a\\_2,a\\_3],[b\\_1,b\\_2,b\\_3],[c\\_1,c\\_2,c\\_3]]}$$\n  where each $a_i$, $b_i$, $c_i$ is an integer in $\\{0,1\\}$ with no spaces anywhere in the line.", "solution": "The problem statement has been rigorously validated and is determined to be **valid**. It is a self-contained, scientifically grounded, and well-posed problem in computational science, providing a complete specification for constructing a Random Forest classifier. All necessary data, mathematical definitions, hyperparameters, and tie-breaking rules are provided, ensuring a unique and deterministic solution.\n\nThe task is to build a Random Forest ($H$) classifier, which is an ensemble of $T$ decision trees ($h_t$). The prediction of the forest is the majority vote over the predictions of individual trees. The problem statement provides three test cases with specific hyperparameters. A critical observation for all three test cases is that the `bootstrap` parameter is set to `False` and the feature subsampling parameter $m_{\\text{try}}$ is set to $2$, which is the total number of features ($d=2$). This has a significant consequence:\n- **No Bootstrapping**: Each tree $h_t$ in the forest is trained on the exact same full training dataset $\\mathcal{D}$.\n- **No Feature Subsampling**: At every node of every tree, all $d=2$ features are considered for splitting.\nSince the tree construction algorithm, including all tie-breaking rules, is deterministic, all $T$ trees within a single Random Forest instance will be identical. Therefore, for any given test case, the Random Forest prediction $H(\\mathbf{x})$ simplifies to the prediction of a single decision tree $h(\\mathbf{x})$ built with the corresponding hyperparameters. The parameter $T$ (number of trees) becomes irrelevant for the final prediction.\n\nOur procedure will be to construct a single decision tree for each of the three test cases using the specified parameters ($D_{\\max}, n_{\\min}$) and then use it to classify the three query vectors.\n\nA decision tree is built recursively, starting from the root node which contains all training data indices $S_{\\text{root}} = \\{1, 2, \\dots, 8\\}$. For any node $v$ with associated training indices $S_v$, we perform the following steps:\n\n1.  **Check Stopping Criteria**: The node $v$ becomes a leaf if:\n    a. The node is pure, i.e., its Gini impurity $G(v) = 1 - \\sum_{k \\in \\{0,1\\}} p_k(v)^2$ is $0$.\n    b. The node's depth reaches the maximum allowed depth, $\\mathrm{depth}(v) \\ge D_{\\max}$.\n    c. There are no valid splits possible from this node. A split is valid if it partitions the node's data of size $|S_v|$ into two children of size $|S_v^{\\text{L}}|$ and $|S_v^{\\text{R}}|$, such that $|S_v^{\\text{L}}| \\ge n_{\\min}$ and $|S_v^{\\text{R}}| \\ge n_{\\min}$.\n\n    If a node is a leaf, its prediction $\\hat{y}(v)$ is the majority class among the samples in $S_v$. Ties are broken in favor of class $0$.\n\n2.  **Find Best Split**: If no stopping criteria are met, we search for the optimal split $(j^*, \\tau^*)$ that minimizes the weighted Gini impurity:\n    $$ \\Phi(v; j, \\tau) = \\frac{|S_v^{\\text{L}}(j,\\tau)|}{|S_v|}G(v^{\\text{L}}(j,\\tau)) + \\frac{|S_v^{\\text{R}}(j,\\tau)|}{|S_v|}G(v^{\\text{R}}(j,\\tau)) $$\n    The search is performed over all features $j \\in \\{0, 1\\}$ and all valid candidate thresholds $\\tau$. Thresholds are midpoints between sorted unique feature values. The tie-breaking rule is to choose the split with the smallest threshold $\\tau$, and then the smallest feature index $j$.\n\n3.  **Recurse**: The node $v$ is made an internal node storing $(j^*, \\tau^*)$. Two children are created by partitioning $S_v$ into $S_v^{\\text{L}}(j^*,\\tau^*)$ and $S_v^{\\text{R}}(j^*,\\tau^*)$, and the tree construction process is recursively applied to them at $\\mathrm{depth}(v)+1$.\n\nThe training data consists of $n=8$ samples with labels $y \\in \\{0,1\\}$:\n$\\mathcal{D} = \\{(\\mathbf{x}_1=(3,0), y_1=0), (\\mathbf{x}_2=(4,1), y_2=0), (\\mathbf{x}_3=(5,0), y_3=0), (\\mathbf{x}_4=(6,1), y_4=1), (\\mathbf{x}_5=(7,1), y_5=1), (\\mathbf{x}_6=(8,2), y_6=1), (\\mathbf{x}_7=(9,1), y_7=1), (\\mathbf{x}_8=(10,0), y_8=1)\\}$.\nThe root node contains indices $\\{1, \\dots, 8\\}$ with labels $\\{0,0,0,1,1,1,1,1\\}$. The class counts are $c_0=3, c_1=5$. The initial Gini impurity is $G(\\text{root}) = 1 - ((3/8)^2 + (5/8)^2) = 30/64 \\approx 0.46875$.\n\nLet us analyze each case:\n\n**Case A: $(T=7, D_{\\max}=3, m_{\\text{try}}=2, n_{\\min}=1)$ and Case B: $(T=1, D_{\\max}=1, m_{\\text{try}}=2, n_{\\min}=1)$**\n\nFor both cases, $n_{\\min}=1$, so any split that creates non-empty children is valid. We seek the best split for the root node.\n- **Feature $j=0$**: Values are $\\{3,4,5,6,7,8,9,10\\}$. A candidate split is at $\\tau=5.5$.\n  - Left child ($x_0 \\le 5.5$): Indices $\\{1,2,3\\}$, labels $\\{0,0,0\\}$. This set is pure, so $G(v^{\\text{L}})=0$. Size is $3$.\n  - Right child ($x_0  5.5$): Indices $\\{4,5,6,7,8\\}$, labels $\\{1,1,1,1,1\\}$. This set is also pure, $G(v^{\\text{R}})=0$. Size is $5$.\n  - The weighted impurity is $\\Phi(v; j=0, \\tau=5.5) = (3/8) \\cdot 0 + (5/8) \\cdot 0 = 0$.\nThis is a perfect split, achieving the minimum possible impurity of $0$. No other split can do better. Thus, the root node splits on feature $j=0$ at threshold $\\tau=5.5$.\n\n- **For Case B ($D_{\\max}=1$)**: The children are at depth $1$. Since $\\mathrm{depth} \\ge D_{\\max}$, both children become leaf nodes.\n  - Left Leaf: Contains samples with labels $\\{0,0,0\\}$. The majority class is $0$. Prediction $\\hat{y}=0$.\n  - Right Leaf: Contains samples with labels $\\{1,1,1,1,1\\}$. The majority class is $1$. Prediction $\\hat{y}=1$.\n\n- **For Case A ($D_{\\max}=3$)**: The children are at depth $1$. We check stopping criteria. Both children are pure nodes ($G=0$), so they become leaves immediately, without regard to $D_{\\max}$.\n  - The resulting tree structure is identical to that of Case B.\n\nTherefore, for both Case A and Case B, the decision tree is: if $x_0 \\le 5.5$, predict $0$; otherwise, predict $1$.\nPredictions for query set $\\{\\mathbf{q}_1=(5,1), \\mathbf{q}_2=(6,0), \\mathbf{q}_3=(8,1)\\}$:\n- $\\mathbf{q}_1=(5,1)$: $x_0=5 \\le 5.5 \\implies$ predict $0$.\n- $\\mathbf{q}_2=(6,0)$: $x_0=6  5.5 \\implies$ predict $1$.\n- $\\mathbf{q}_3=(8,1)$: $x_0=8  5.5 \\implies$ predict $1$.\nThe prediction vector for both cases is $[0,1,1]$.\n\n**Case C: $(T=5, D_{\\max}=3, m_{\\text{try}}=2, n_{\\min}=5)$**\n\nHere, the minimum leaf size parameter is $n_{\\min}=5$. At the root node, which contains $|S_{\\text{root}}|=8$ samples, we search for a valid split. A split is valid only if both child nodes have at least $n_{\\min}=5$ samples. If a split produces children of sizes $|S^{\\text{L}}|$ and $|S^{\\text{R}}|$, we must satisfy $|S^{\\text{L}}| \\ge 5$ and $|S^{\\text{R}}| \\ge 5$. However, $|S^{\\text{L}}| + |S^{\\text{R}}| = 8$, so this is impossible (e.g., $5+5=108$).\nTherefore, there are no valid splits from the root node. According to the stopping rules, the root node becomes a leaf node.\n- Leaf prediction: The data at the root has counts $c_0=3$ and $c_1=5$. The majority class is $1$.\nThe tree for Case C is simply a single leaf node that always predicts $1$.\nPredictions for query set:\n- $\\mathbf{q}_1=(5,1) \\implies$ predict $1$.\n- $\\mathbf{q}_2=(6,0) \\implies$ predict $1$.\n- $\\mathbf{q}_3=(8,1) \\implies$ predict $1$.\nThe prediction vector for Case C is $[1,1,1]$.\n\n**Summary of Predictions:**\n- Case A: $[0,1,1]$\n- Case B: $[0,1,1]$\n- Case C: $[1,1,1]$\nThe final output string is constructed from these results.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\nclass Node:\n    \"\"\"Represents a node in the decision tree.\"\"\"\n    def __init__(self, depth):\n        self.depth = depth\n        self.feature_index = None\n        self.threshold = None\n        self.left = None\n        self.right = None\n        self.is_leaf = False\n        self.prediction = None\n\nclass DecisionTree:\n    \"\"\"Implements a single decision tree classifier.\"\"\"\n    def __init__(self, max_depth, min_samples_leaf):\n        self.max_depth = max_depth\n        self.min_samples_leaf = min_samples_leaf\n        self.root = None\n    \n    def _gini(self, y):\n        \"\"\"Calculates the Gini impurity for a set of labels.\"\"\"\n        n_samples = len(y)\n        if n_samples == 0:\n            return 0.0\n        _, counts = np.unique(y, return_counts=True)\n        probabilities = counts / n_samples\n        return 1.0 - np.sum(probabilities**2)\n\n    def _find_best_split(self, X, y, indices):\n        \"\"\"Finds the best split for a node.\"\"\"\n        n_samples = len(indices)\n        if n_samples  2:\n            return None, None\n            \n        # Gini of the current node before splitting\n        parent_gini = self._gini(y[indices])\n        \n        best_gini = parent_gini\n        best_split = None\n        \n        n_features = X.shape[1]\n        \n        # As per problem, m_try=2, so we iterate through all features\n        for feature_index in range(n_features):\n            feature_values = X[indices, feature_index]\n            unique_values = np.unique(feature_values)\n            \n            if len(unique_values)  2:\n                continue\n\n            thresholds = (unique_values[:-1] + unique_values[1:]) / 2.0\n            \n            for threshold in thresholds:\n                left_indices = indices[X[indices, feature_index] = threshold]\n                right_indices = indices[X[indices, feature_index]  threshold]\n                \n                # Check min_samples_leaf constraint\n                if len(left_indices)  self.min_samples_leaf or len(right_indices)  self.min_samples_leaf:\n                    continue\n\n                gini_left = self._gini(y[left_indices])\n                gini_right = self._gini(y[right_indices])\n                \n                weighted_gini = (len(left_indices) / n_samples) * gini_left + \\\n                                (len(right_indices) / n_samples) * gini_right\n                \n                # Tie-breaking rules from problem: smallest tau, then smallest feature index.\n                # The loop order naturally handles this.\n                if weighted_gini  best_gini:\n                    best_gini = weighted_gini\n                    best_split = (feature_index, threshold)\n\n        if best_split is None:\n            return None, None # No valid split found\n        \n        return best_split\n\n    def _grow_tree(self, X, y, indices, depth):\n        \"\"\"Recursively grows the decision tree.\"\"\"\n        node = Node(depth)\n        \n        # Determine leaf prediction\n        labels_at_node = y[indices]\n        count_1 = np.sum(labels_at_node == 1)\n        count_0 = len(labels_at_node) - count_1\n        # Tie-breaking: favor class 0\n        node.prediction = 1 if count_1  count_0 else 0\n\n        # Check stopping criteria\n        is_pure = self._gini(labels_at_node) == 0.0\n        if is_pure or depth = self.max_depth or len(indices)  2:\n            node.is_leaf = True\n            return node\n\n        feature_index, threshold = self._find_best_split(X, y, indices)\n        \n        if feature_index is None: # No valid split found\n            node.is_leaf = True\n            return node\n\n        node.feature_index = feature_index\n        node.threshold = threshold\n        \n        left_indices = indices[X[indices, feature_index] = threshold]\n        right_indices = indices[X[indices, feature_index]  threshold]\n        \n        node.left = self._grow_tree(X, y, left_indices, depth + 1)\n        node.right = self._grow_tree(X, y, right_indices, depth + 1)\n        \n        return node\n\n    def fit(self, X, y):\n        \"\"\"Builds the decision tree.\"\"\"\n        initial_indices = np.arange(len(y))\n        self.root = self._grow_tree(X, y, initial_indices, 0)\n        \n    def _predict_one(self, x, node):\n        \"\"\"Predicts class for a single sample.\"\"\"\n        current_node = node\n        while not current_node.is_leaf:\n            if x[current_node.feature_index] = current_node.threshold:\n                current_node = current_node.left\n            else:\n                current_node = current_node.right\n        return current_node.prediction\n\n    def predict(self, X):\n        \"\"\"Predicts classes for a set of samples.\"\"\"\n        return np.array([self._predict_one(x, self.root) for x in X])\n\nclass RandomForest:\n    \"\"\"Implements a Random Forest classifier.\"\"\"\n    def __init__(self, n_trees, max_depth, min_samples_leaf, bootstrap, seed):\n        self.n_trees = n_trees\n        self.max_depth = max_depth\n        self.min_samples_leaf = min_samples_leaf\n        self.bootstrap = bootstrap\n        self.seed = seed\n        self.trees = []\n\n    def fit(self, X, y):\n        \"\"\"Trains the Random Forest.\"\"\"\n        self.trees = []\n        n_samples = len(y)\n        \n        # The problem states m_try=2, so no feature subsampling is needed.\n        # It also sets bootstrap=False, so no data bootstrapping is needed.\n        # This means all trees will be identical for a given hyperparameter set.\n        \n        for i in range(self.n_trees):\n            tree = DecisionTree(max_depth=self.max_depth, min_samples_leaf=self.min_samples_leaf)\n            \n            if self.bootstrap:\n                # Per problem, we'd use seed + i for reproducibility\n                rng = np.random.default_rng(self.seed + i)\n                indices = rng.choice(n_samples, n_samples, replace=True)\n                tree.fit(X[indices], y[indices])\n            else:\n                tree.fit(X, y)\n                \n            self.trees.append(tree)\n            \n    def predict(self, X):\n        \"\"\"Makes predictions with the Random Forest.\"\"\"\n        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n        # tree_preds shape is (n_trees, n_samples)\n        \n        # Majority vote; ties go to class 0\n        n_ones = np.sum(tree_preds == 1, axis=0)\n        n_zeros = self.n_trees - n_ones\n        \n        return np.where(n_ones  n_zeros, 1, 0)\n\ndef solve():\n    # Define training data\n    X_train = np.array([\n        [3, 0], [4, 1], [5, 0], [6, 1], [7, 1], [8, 2], [9, 1], [10, 0]\n    ])\n    y_train = np.array([0, 0, 0, 1, 1, 1, 1, 1])\n\n    # Define query data\n    X_query = np.array([\n        [5, 1], [6, 0], [8, 1]\n    ])\n\n    # Define test cases: (T, D_max, m_try, n_min, s, bootstrap)\n    test_cases = [\n        (7, 3, 2, 1, 13, False),  # Case A\n        (1, 1, 2, 1, 7, False),   # Case B\n        (5, 3, 2, 5, 5, False)   # Case C\n    ]\n\n    all_results = []\n    for case in test_cases:\n        T, D_max, m_try, n_min, s, bootstrap = case\n        \n        # As per problem, m_try is always 2. It is not passed to the RF, as the DT handles all features.\n        rf = RandomForest(n_trees=T, max_depth=D_max, min_samples_leaf=n_min, bootstrap=bootstrap, seed=s)\n        rf.fit(X_train, y_train)\n        predictions = rf.predict(X_query)\n        all_results.append(list(predictions))\n\n    # Format the final output string exactly as required\n    inner_parts = []\n    for res_list in all_results:\n        inner_parts.append(f\"[{','.join(map(str, res_list))}]\")\n    final_output = f\"[{','.join(inner_parts)}]\"\n    \n    print(final_output)\n\nsolve()\n```", "id": "2384429"}, {"introduction": "Building on the classification framework, this next practice demonstrates the versatility of tree-based models by adapting them for regression. You will construct a Random Forest to predict a continuous biological variable: the half-life of mRNA molecules based on their sequence features. This task involves replacing the Gini impurity with a squared-error minimization objective and modifying leaf predictions from a majority vote to a sample mean, showcasing how the same core algorithm can address different types of predictive problems in bioinformatics [@problem_id:2384472].", "problem": "You are given a supervised learning task from computational biology and bioinformatics: predict the half-life of a messenger Ribonucleic Acid (mRNA) molecule, measured in minutes, from sequence-derived features. The model should be a random forest regressor composed of decision trees. Your program must construct decision trees using squared-error minimization at each split and aggregate them into a random forest via averaging of tree predictions. Each decision tree must split on a threshold of a single feature at each internal node and terminate by either reaching a maximum depth or a minimum sample constraint. The random forest must support the standard hyperparameters: number of trees, maximum depth, number of features considered per split, minimum samples required to split, and the option to enable or disable bootstrap sampling.\n\nYou will be provided a fixed training dataset of transcripts, each represented by a feature vector extracted from its sequence:\n- Feature definitions for each transcript $i$:\n    - $x_{i,1}$: $5^{\\prime}$ Untranslated Region (UTR) length in nucleotides.\n    - $x_{i,2}$: coding sequence length in codons.\n    - $x_{i,3}$: coding sequence guanine-cytosine (GC) fraction as a real number in $[0,1]$.\n    - $x_{i,4}$: fraction of codons deemed optimal (unitless real number in $[0,1]$) under a fixed reference.\n    - $x_{i,5}$: count of the AU-rich element motif $AUUUA$ in the $3^{\\prime}$ UTR (a nonnegative integer).\n    - $x_{i,6}$: uracil (U) fraction in the $3^{\\prime}$ UTR as a real number in $[0,1]$.\n- Response:\n    - $y_i$: measured mRNA half-life in minutes (a positive real number).\n\nTraining set with $n = 12$ transcripts (each line lists $(x_{i,1}, x_{i,2}, x_{i,3}, x_{i,4}, x_{i,5}, x_{i,6}; y_i)$):\n- $\\mathrm{T}_1$: $(80, 300, 0.55, 0.60, 0, 0.35; 49.85)$\n- $\\mathrm{T}_2$: $(120, 400, 0.40, 0.30, 2, 0.55; 17.85)$\n- $\\mathrm{T}_3$: $(60, 250, 0.65, 0.80, 0, 0.20; 64.90)$\n- $\\mathrm{T}_4$: $(30, 150, 0.35, 0.10, 3, 0.70; 0.60)$\n- $\\mathrm{T}_5$: $(90, 320, 0.50, 0.50, 1, 0.40; 37.00)$\n- $\\mathrm{T}_6$: $(70, 280, 0.45, 0.20, 2, 0.60; 14.50)$\n- $\\mathrm{T}_7$: $(110, 380, 0.70, 0.90, 0, 0.15; 69.15)$\n- $\\mathrm{T}_8$: $(50, 200, 0.30, 0.25, 1, 0.65; 18.15)$\n- $\\mathrm{T}_9$: $(95, 310, 0.60, 0.55, 0, 0.30; 48.60)$\n- $\\mathrm{T}_{10}$: $(85, 290, 0.42, 0.35, 1, 0.45; 28.65)$\n- $\\mathrm{T}_{11}$: $(130, 450, 0.52, 0.62, 0, 0.25; 49.95)$\n- $\\mathrm{T}_{12}$: $(40, 180, 0.38, 0.15, 2, 0.50; 11.70)$\n\nYour program must use this fixed dataset for training in every test case. The query (held-out) transcript whose half-life you must predict has features\n$$\n\\mathbf{x}^{\\ast} = (75, 270, 0.58, 0.65, 0, 0.28).\n$$\n\nFundamental base to assume:\n- A decision tree for regression recursively partitions the feature space via axis-aligned splits and predicts at a leaf with the sample mean of responses at that leaf.\n- The squared-error impurity of a set $S$ with responses $\\{y_i\\}_{i \\in S}$ is $I(S) = \\sum_{i \\in S} (y_i - \\bar{y}_S)^2$, where $\\bar{y}_S$ is the mean of $\\{y_i\\}$ in $S$.\n- A split of $S$ into disjoint $L$ and $R$ is chosen to minimize $I(L) + I(R)$.\n\nTest suite. For each parameter set below, train a random forest on the training set and output the prediction for $\\mathbf{x}^{\\ast}$ in minutes, rounded to exactly two decimal places:\n- Case A (happy path, no splits): number of trees $T = 25$, maximum depth $d_{\\max} = 0$, number of features per split $m_{\\mathrm{try}} = 6$, minimum samples to split $n_{\\min} = 2$, bootstrap disabled.\n- Case B (single stump): $T = 1$, $d_{\\max} = 1$, $m_{\\mathrm{try}} = 6$, $n_{\\min} = 2$, bootstrap disabled.\n- Case C (small forest of stumps): $T = 25$, $d_{\\max} = 1$, $m_{\\mathrm{try}} = 6$, $n_{\\min} = 2$, bootstrap disabled.\n\nAngle units are not applicable. Express the predicted half-life in minutes as a float. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order [Case A, Case B, Case C], for example, \"[12.34,56.78,90.12]\".", "solution": "The problem statement has been subjected to rigorous validation against the established criteria.\n\n### Step 1: Extract Givens\n- **Task**: Predict mRNA half-life ($y_i$, in minutes) using a Random Forest regressor.\n- **Features**: For each transcript $i$, a vector of $6$ features: $\\mathbf{x}_i = (x_{i,1}, x_{i,2}, x_{i,3}, x_{i,4}, x_{i,5}, x_{i,6})$.\n    - $x_{i,1}$: $5^{\\prime}$ UTR length.\n    - $x_{i,2}$: CDS length.\n    - $x_{i,3}$: CDS GC fraction.\n    - $x_{i,4}$: Optimal codon fraction.\n    - $x_{i,5}$: $AUUUA$ motif count.\n    - $x_{i,6}$: $3^{\\prime}$ UTR Uracil fraction.\n- **Training Data**: A fixed set of $n=12$ transcripts with their feature vectors and corresponding half-lives.\n- **Query Point**: A single held-out transcript with feature vector $\\mathbf{x}^{\\ast} = (75, 270, 0.58, 0.65, 0, 0.28)$.\n- **Decision Tree Model**:\n    - **Splitting Criterion**: Minimization of squared error. Impurity of a set $S$ is $I(S) = \\sum_{i \\in S} (y_i - \\bar{y}_S)^2$. A split of $S$ into $L$ and $R$ is chosen to minimize $I(L) + I(R)$.\n    - **Leaf Prediction**: The sample mean of response values at the leaf.\n    - **Termination**: Reaching maximum depth ($d_{\\max}$) or if a node's sample count is less than the minimum samples required to split ($n_{\\min}$).\n- **Random Forest Model**:\n    - **Aggregation**: Average of individual tree predictions.\n    - **Hyperparameters**: Number of trees ($T$), maximum depth ($d_{\\max}$), features per split ($m_{\\mathrm{try}}$), minimum samples to split ($n_{\\min}$), and bootstrap sampling.\n- **Test Suite**:\n    - Case A: $T = 25$, $d_{\\max} = 0$, $m_{\\mathrm{try}} = 6$, $n_{\\min} = 2$, bootstrap disabled.\n    - Case B: $T = 1$, $d_{\\max} = 1$, $m_{\\mathrm{try}} = 6$, $n_{\\min} = 2$, bootstrap disabled.\n    - Case C: $T = 25$, $d_{\\max} = 1$, $m_{\\mathrm{try}} = 6$, $n_{\\min} = 2$, bootstrap disabled.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientific Grounding**: The problem is scientifically sound. The features selected are known modulators of mRNA stability, and predicting half-life is a standard task in computational biology. The chosen model, Random Forest, is a well-established and powerful machine learning technique.\n- **Well-Posedness**: The problem is mathematically and algorithmically well-posed. It provides a complete training dataset, a specific query point, and a precise definition of the model to be constructed, including all necessary hyperparameters for each test case. This ensures that a unique and verifiable solution can be computed.\n- **Objectivity**: The problem is stated in precise, objective language, free of ambiguity or subjective claims.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. It is a well-defined and standard computational task grounded in scientific principles. A solution will be provided.\n\n### Principle-Based Solution\nThe task is to implement a Random Forest regressor from fundamental principles to predict mRNA half-life. The solution involves two main components: a Decision Tree regressor and the Random Forest ensemble that aggregates multiple such trees.\n\n**Decision Tree Regressor**\nA regression tree is a hierarchical model that recursively partitions the feature space into hyper-rectangles. Each internal node in the tree corresponds to a split based on a single feature and a threshold. Each terminal node, or leaf, contains a prediction value.\n\n1.  **Recursive Partitioning**: The tree is built starting from the root, which contains all training data. At each node, we seek the best possible split to partition the data into two child nodes.\n2.  **Split Criterion**: For a regression task, a common criterion for finding the best split is the minimization of the Sum of Squared Errors (SSE). Given a set of data points $S$ at a node, its impurity is the SSE relative to its mean:\n    $$I(S) = \\sum_{i \\in S} (y_i - \\bar{y}_S)^2$$\n    where $\\bar{y}_S = \\frac{1}{|S|} \\sum_{i \\in S} y_i$. A split partitions the data $S$ into a left subset $L$ and a right subset $R$. The optimal split (i.e., the best feature and threshold) is the one that minimizes the combined impurity of the children: $\\min(I(L) + I(R))$.\n3.  **Termination**: The recursive partitioning stops, and a leaf node is created, under any of the following conditions:\n    - The current depth of the tree reaches the hyperparameter $d_{\\max}$.\n    - The number of samples at the current node is less than the hyperparameter $n_{\\min}$.\n    - The node is \"pure,\" meaning all target values $y_i$ are identical, and thus the impurity $I(S)$ is $0$.\n4.  **Prediction**: For a new data point, it is passed down the tree according to the splits at each internal node. The prediction is the value stored in the leaf node it reaches, which is the mean of the training target values that fell into that leaf: $\\bar{y}_{\\text{leaf}}$.\n\n**Random Forest Regressor**\nA Random Forest is an ensemble method that improves upon a single decision tree by reducing variance and overfitting. It builds multiple decision trees and aggregates their predictions.\n\n1.  **Bagging (Bootstrap Aggregating)**: Each tree in the forest is trained on a random subsample of the training data, typically drawn with replacement (bootstrapping). The problem specifies this feature can be disabled.\n2.  **Feature Randomness**: At each split in a decision tree, only a random subset of $m_{\\mathrm{try}}$ features is considered for finding the best split. This decorrelates the trees in the forest.\n3.  **Prediction**: The final prediction of the Random Forest for a new data point is the average of the predictions from all individual trees in the forest.\n\n**Analysis of Test Cases**\n\nThe training dataset consists of $n=12$ samples. The query point is $\\mathbf{x}^{\\ast} = (75, 270, 0.58, 0.65, 0, 0.28)$. The hyperparameters for all test cases include bootstrap disabled and $m_{\\mathrm{try}}=6$. Since there are $6$ features in total, considering $6$ features per split means all features are considered, removing any randomness from feature selection. As bootstrapping is also disabled, all trees within a single forest will be identical.\n\n**Case A**: $T = 25$, $d_{\\max} = 0$, $m_{\\mathrm{try}} = 6$, $n_{\\min} = 2$, bootstrap disabled.\nHere, $d_{\\max}=0$. The tree building terminates immediately at the root node. The tree is simply a single leaf. Its prediction is the mean of all target values in the training set.\n$$ \\bar{y}_{\\text{train}} = \\frac{1}{12} \\sum_{i=1}^{12} y_i = \\frac{410.9}{12} \\approx 34.24167 $$\nEach of the $25$ trees will be this single leaf, and each will predict $\\approx 34.24167$. The forest's prediction is the average of these identical values, which is $\\approx 34.24167$. Rounded to two decimal places, the result is $34.24$.\n\n**Case B**: $T = 1$, $d_{\\max} = 1$, $m_{\\mathrm{try}} = 6$, $n_{\\min} = 2$, bootstrap disabled.\nA single decision tree with $d_{\\max}=1$ is constructed. This is a \"decision stump\". We must find the single best split over all $6$ features and all possible thresholds. The implementation systematically evaluates every possible split. The optimal split is found to be on feature index $4$ (the count of the $AUUUA$ motif, $x_5$) at a threshold of $0.5$.\n- **Data points with $x_5 \\le 0.5$ (i.e., $x_5=0$):** Transcripts $T_1, T_3, T_7, T_9, T_{11}$.\n  - The mean half-life for this group (left leaf) is $\\frac{49.85 + 64.90 + 69.15 + 48.60 + 49.95}{5} = \\frac{282.45}{5} = 56.49$.\n- **Data points with $x_5  0.5$ (i.e., $x_5 \\ge 1$):** Transcripts $T_2, T_4, T_5, T_6, T_8, T_{10}, T_{12}$.\n  - The mean half-life for this group (right leaf) is $\\frac{17.85 + 0.60 + 37.00 + 14.50 + 18.15 + 28.65 + 11.70}{7} = \\frac{128.45}{7} \\approx 18.35$.\nThe query point $\\mathbf{x}^{\\ast}$ has its fifth feature value as $0$. Since $0 \\le 0.5$, it is routed to the left leaf. The prediction is $56.49$.\n\n**Case C**: $T = 25$, $d_{\\max} = 1$, $m_{\\mathrm{try}} = 6$, $n_{\\min} = 2$, bootstrap disabled.\nThis case constructs a forest of $25$ decision stumps. As noted earlier, because bootstrap is disabled and all features are considered at each split, there is no source of randomness in the tree construction process. Therefore, all $25$ trees will be identical to the single decision stump built in Case B. Each tree will predict $56.49$ for $\\mathbf{x}^{\\ast}$. The final prediction of the forest is the average of $25$ identical values, which is $56.49$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and runs a Random Forest regressor from scratch based on the problem specification.\n    \"\"\"\n\n    class Node:\n        \"\"\"\n        Represents a node in a decision tree.\n        A node is either a leaf node (with a prediction 'value') or an\n        internal node (with split 'feature_idx', 'threshold', and child nodes).\n        \"\"\"\n        def __init__(self, value=None, feature_idx=None, threshold=None, left=None, right=None):\n            self.value = value\n            self.feature_idx = feature_idx\n            self.threshold = threshold\n            self.left = left\n            self.right = right\n\n    class DecisionTree:\n        \"\"\"\n        Decision Tree regressor implementation.\n        \"\"\"\n        def __init__(self, max_depth=100, min_samples_split=2, m_try=None):\n            self.max_depth = max_depth\n            self.min_samples_split = min_samples_split\n            self.m_try = m_try\n            self.root = None\n            self.n_features_ = 0\n\n        def fit(self, X, y):\n            \"\"\"Builds the decision tree.\"\"\"\n            self.n_features_ = X.shape[1] if X.ndim  1 else 1\n            if self.m_try is None:\n                self.m_try = self.n_features_\n            self.root = self._build_tree(X, y)\n\n        def _build_tree(self, X, y, depth=0):\n            \"\"\"Recursively builds the tree.\"\"\"\n            n_samples = len(y)\n            \n            # Termination conditions\n            is_leaf = (\n                depth = self.max_depth or\n                n_samples  self.min_samples_split or\n                len(np.unique(y)) == 1\n            )\n            if is_leaf:\n                leaf_value = np.mean(y)\n                return Node(value=leaf_value)\n\n            # In this problem, m_try equals total features, so no random selection.\n            feat_idxs = np.arange(self.n_features_)\n\n            best_split = self._find_best_split(X, y, feat_idxs)\n            \n            if best_split['score'] == np.inf:\n                leaf_value = np.mean(y)\n                return Node(value=leaf_value)\n\n            left_indices = X[:, best_split['feature_idx']] = best_split['threshold']\n            right_indices = ~left_indices\n            \n            X_left, y_left = X[left_indices], y[left_indices]\n            X_right, y_right = X[right_indices], y[right_indices]\n\n            left_child = self._build_tree(X_left, y_left, depth + 1)\n            right_child = self._build_tree(X_right, y_right, depth + 1)\n            \n            return Node(feature_idx=best_split['feature_idx'], threshold=best_split['threshold'], left=left_child, right=right_child)\n\n        def _squared_error(self, y):\n            \"\"\"Calculates the sum of squared errors for a set of values.\"\"\"\n            if len(y) == 0:\n                return 0.0\n            mean = np.mean(y)\n            return np.sum((y - mean)**2)\n\n        def _find_best_split(self, X, y, feat_idxs):\n            \"\"\"Finds the best split (feature and threshold) for a node.\"\"\"\n            best_split = {'score': np.inf, 'feature_idx': None, 'threshold': None}\n            \n            for feat_idx in feat_idxs:\n                feature_values = X[:, feat_idx]\n                unique_vals = np.unique(feature_values)\n                \n                for i in range(len(unique_vals) - 1):\n                    threshold = (unique_vals[i] + unique_vals[i+1]) / 2.0\n                    \n                    left_indices = feature_values = threshold\n                    \n                    y_left, y_right = y[left_indices], y[~left_indices]\n                    \n                    if len(y_left) == 0 or len(y_right) == 0:\n                        continue\n                        \n                    current_sse = self._squared_error(y_left) + self._squared_error(y_right)\n                    \n                    if current_sse  best_split['score']:\n                        best_split['score'] = current_sse\n                        best_split['feature_idx'] = feat_idx\n                        best_split['threshold'] = threshold\n            \n            return best_split\n\n        def predict(self, x):\n            \"\"\"Predicts the value for a single sample x.\"\"\"\n            return self._traverse_tree(x, self.root)\n\n        def _traverse_tree(self, x, node):\n            \"\"\"Traverses the tree to find the prediction for a sample x.\"\"\"\n            if node.value is not None:\n                return node.value\n            \n            if x[node.feature_idx] = node.threshold:\n                return self._traverse_tree(x, node.left)\n            else:\n                return self._traverse_tree(x, node.right)\n\n    class RandomForest:\n        \"\"\"\n        Random Forest regressor implementation.\n        \"\"\"\n        def __init__(self, n_trees=25, max_depth=10, min_samples_split=2, m_try=None, bootstrap=True):\n            self.n_trees = n_trees\n            self.max_depth = max_depth\n            self.min_samples_split = min_samples_split\n            self.m_try = m_try\n            self.bootstrap = bootstrap\n            self.trees = []\n        \n        def fit(self, X, y):\n            \"\"\"Trains the Random Forest model.\"\"\"\n            self.trees = []\n            n_samples = len(y)\n            for _ in range(self.n_trees):\n                tree = DecisionTree(\n                    max_depth=self.max_depth,\n                    min_samples_split=self.min_samples_split,\n                    m_try=self.m_try\n                )\n                \n                if self.bootstrap:\n                    idxs = np.random.choice(n_samples, n_samples, replace=True)\n                    X_sample, y_sample = X[idxs], y[idxs]\n                else:\n                    X_sample, y_sample = X, y\n\n                tree.fit(X_sample, y_sample)\n                self.trees.append(tree)\n\n        def predict(self, x):\n            \"\"\"Makes a prediction for a single sample.\"\"\"\n            tree_predictions = [tree.predict(x) for tree in self.trees]\n            return np.mean(tree_predictions)\n\n    # Define the fixed training dataset from the problem statement.\n    X_train = np.array([\n        [80.0, 300.0, 0.55, 0.60, 0.0, 0.35],\n        [120.0, 400.0, 0.40, 0.30, 2.0, 0.55],\n        [60.0, 250.0, 0.65, 0.80, 0.0, 0.20],\n        [30.0, 150.0, 0.35, 0.10, 3.0, 0.70],\n        [90.0, 320.0, 0.50, 0.50, 1.0, 0.40],\n        [70.0, 280.0, 0.45, 0.20, 2.0, 0.60],\n        [110.0, 380.0, 0.70, 0.90, 0.0, 0.15],\n        [50.0, 200.0, 0.30, 0.25, 1.0, 0.65],\n        [95.0, 310.0, 0.60, 0.55, 0.0, 0.30],\n        [85.0, 290.0, 0.42, 0.35, 1.0, 0.45],\n        [130.0, 450.0, 0.52, 0.62, 0.0, 0.25],\n        [40.0, 180.0, 0.38, 0.15, 2.0, 0.50]\n    ])\n    y_train = np.array([49.85, 17.85, 64.90, 0.60, 37.00, 14.50, 69.15, 18.15, 48.60, 28.65, 49.95, 11.70])\n\n    # Query transcript features\n    x_star = np.array([75.0, 270.0, 0.58, 0.65, 0.0, 0.28])\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A\n        {'n_trees': 25, 'max_depth': 0, 'm_try': 6, 'min_samples_split': 2, 'bootstrap': False},\n        # Case B\n        {'n_trees': 1, 'max_depth': 1, 'm_try': 6, 'min_samples_split': 2, 'bootstrap': False},\n        # Case C\n        {'n_trees': 25, 'max_depth': 1, 'm_try': 6, 'min_samples_split': 2, 'bootstrap': False},\n    ]\n\n    results = []\n    for params in test_cases:\n        rf = RandomForest(\n            n_trees=params['n_trees'],\n            max_depth=params['max_depth'],\n            min_samples_split=params['min_samples_split'],\n            m_try=params['m_try'],\n            bootstrap=params['bootstrap']\n        )\n        \n        rf.fit(X_train, y_train)\n        prediction = rf.predict(x_star)\n        \n        # Format the result to exactly two decimal places\n        results.append(f\"{prediction:.2f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n\n```", "id": "2384472"}, {"introduction": "After learning to build models for both classification and regression, this final exercise encourages deeper analytical thinking about model behavior. You will investigate the structural properties of a decision tree and see how incorporating prior domain knowledge—in this case, forcing the first split to be on a specific gene—can alter the model's architecture. This practice [@problem_id:2384430] is a crucial step toward understanding model interpretation and the important dialogue between data-driven results and existing biological hypotheses.", "problem": "You are given a binary classification task in computational biology where each instance corresponds to a sample with measured gene expression levels for multiple genes. The feature matrix is denoted by $X \\in \\mathbb{R}^{n \\times d}$ and labels by $y \\in \\{0,1\\}^n$. Consider a family of binary decision trees defined as follows.\n\nA node corresponds to a subset $S \\subseteq \\{1,\\dots,n\\}$ of sample indices. For a node $S$, define the Gini impurity by\n$$\nG(S) \\;=\\; 1 \\;-\\; \\sum_{c \\in \\{0,1\\}} \\left(\\frac{| \\{ i \\in S \\,:\\, y_i = c \\} |}{|S|}\\right)^2.\n$$\nFor any feature index $j \\in \\{0,\\dots,d-1\\}$, and threshold $t \\in \\mathbb{R}$, define the split of $S$ into\n$$\nS_L(j,t) \\;=\\; \\{ i \\in S \\,:\\, X_{i,j} \\le t \\}, \\quad S_R(j,t) \\;=\\; \\{ i \\in S \\,:\\, X_{i,j}  t \\}.\n$$\nThe set of admissible thresholds for feature $j$ at node $S$ is the set of midpoints between successive distinct values of $\\{ X_{i,j} \\,:\\, i \\in S \\}$ sorted in increasing order. A split $(j,t)$ at node $S$ has impurity decrease\n$$\n\\Delta(S;j,t) \\;=\\; G(S) \\;-\\; \\frac{|S_L(j,t)|}{|S|} G(S_L(j,t)) \\;-\\; \\frac{|S_R(j,t)|}{|S|} G(S_R(j,t)).\n$$\nA split is valid at node $S$ if both children are nonempty and both satisfy the minimum leaf size constraint $|S_L(j,t)| \\ge m_{\\min}$ and $|S_R(j,t)| \\ge m_{\\min}$, where $m_{\\min} \\in \\mathbb{N}$ is a given parameter. Among all valid splits, the split chosen at a node is the one that maximizes $\\Delta(S;j,t)$. If there are ties in $\\Delta(S;j,t)$, break ties by choosing the smallest feature index $j$, and if still tied, the smallest threshold $t$. If no valid split yields a strictly positive impurity decrease, or if the node depth equals the maximum depth $D_{\\max}$, or if $|S|  2$, then the node is a leaf. A leaf predicts the majority class in $S$, and in the case of a tie, predicts class $0$.\n\nDefine two trees on the same data and with the same $(D_{\\max}, m_{\\min})$:\n- An unconstrained tree, constructed by recursively applying the above rule starting at the root with $S = \\{1,\\dots,n\\}$.\n- A constrained-root tree, in which the root node is forced to split on a specified feature index $j^\\star \\in \\{0,\\dots,d-1\\}$. At the root only, select $t$ by maximizing $\\Delta(\\{1,\\dots,n\\}; j^\\star, t)$ among admissible thresholds. If at the root no threshold for $j^\\star$ satisfies the minimum leaf size constraint, choose $t$ by maximizing $\\Delta(\\{1,\\dots,n\\}; j^\\star, t)$ among admissible thresholds that produce nonempty children, ignoring the minimum leaf size constraint at the root only. If no admissible threshold exists (i.e., all $X_{i,j^\\star}$ are equal), the root becomes a leaf. Below the root, all nodes obey the same rules as the unconstrained tree with the given $(D_{\\max}, m_{\\min})$.\n\nFor each tree, define the tree depth as the maximum number of edges on any path from the root to a leaf. Define the training misclassification rate as\n$$\n\\mathrm{err} \\;=\\; \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}\\{ \\hat{y}_i \\ne y_i \\},\n$$\nwhere $\\hat{y}_i$ is the prediction of the tree for sample $i$. Define the preorder internal-structure sequence of a tree as the list of pairs $(j,t)$ obtained by a preorder traversal that records, for each internal node visited, its chosen feature index $j$ and threshold $t$; leaves do not contribute entries. Declare that the constrained-root tree \"structurally changes\" relative to the unconstrained tree if and only if the two preorder internal-structure sequences differ in length or in at least one corresponding pair $(j,t)$, where thresholds are compared as real numbers.\n\nYour task is to write a complete program that, for each test case below, constructs both trees exactly as defined and outputs, for each case, a list containing:\n- the unconstrained tree depth (an integer),\n- the constrained-root tree depth (an integer),\n- the unconstrained training misclassification rate (a float rounded to six decimal places),\n- the constrained-root training misclassification rate (a float rounded to six decimal places),\n- the forced root threshold actually used in the constrained-root tree (a float rounded to six decimal places; if the constrained-root tree does not split at the root, output the single number $-1.0$ here),\n- a boolean indicating whether the preorder internal-structure sequence changed (True if it changed, False otherwise).\n\nThe program must aggregate the results for all test cases into a single line of output containing the results as a comma-separated list enclosed in square brackets, with no spaces.\n\nTest suite. For each case, you are given $(X, y, D_{\\max}, m_{\\min}, j^\\star)$:\n1) Case A:\n- $X =$ [\n[$0.1$, $0.3$, $0.2$],\n[$0.2$, $0.1$, $0.5$],\n[$0.4$, $0.6$, $0.7$],\n[$0.6$, $0.2$, $0.4$],\n[$0.7$, $0.8$, $0.9$],\n[$0.8$, $0.1$, $0.3$],\n[$0.9$, $0.5$, $0.2$],\n[$0.55$, $0.4$, $0.6$]\n]\n- $y = [\\,0,\\,0,\\,0,\\,1,\\,1,\\,1,\\,1,\\,1\\,]$\n- $D_{\\max} = 3$\n- $m_{\\min} = 2$\n- $j^\\star = 0$\n\n2) Case B:\n- $X =$ [\n[$0.2$, $0.3$, $0.1$],\n[$0.1$, $0.2$, $0.8$],\n[$0.4$, $0.6$, $0.5$],\n[$0.35$, $0.7$, $0.4$],\n[$0.5$, $0.9$, $0.6$],\n[$0.45$, $0.55$, $0.2$],\n[$0.3$, $0.4$, $0.9$],\n[$0.25$, $0.8$, $0.3$],\n[$0.6$, $0.1$, $0.7$],\n[$0.7$, $0.85$, $0.2$]\n]\n- $y = [\\,0,\\,0,\\,1,\\,1,\\,1,\\,1,\\,0,\\,1,\\,0,\\,1\\,]$\n- $D_{\\max} = 3$\n- $m_{\\min} = 2$\n- $j^\\star = 2$\n\n3) Case C:\n- $X =$ [\n[$0.0$, $0.0$],\n[$0.0$, $1.0$],\n[$1.0$, $0.0$],\n[$1.0$, $1.0$],\n[$0.5$, $0.5$],\n[$0.5$, $0.5$]\n]\n- $y = [\\,0,\\,0,\\,1,\\,1,\\,0,\\,1\\,]$\n- $D_{\\max} = 3$\n- $m_{\\min} = 1$\n- $j^\\star = 1$\n\nAll numeric values are unitless. All angles, if any appear in intermediate computations, are irrelevant. Floats in the final output must be rounded to six decimal places as specified. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, \"[$[a_1,a_2,\\dots],\\,[b_1,b_2,\\dots],\\,[c_1,c_2,\\dots]$]\"), with no spaces.", "solution": "The user requests a program to construct and analyze two types of binary decision trees for classification tasks in computational biology: an unconstrained tree and a constrained-root tree. The problem is a well-defined algorithmic task based on standard principles of machine learning, specifically decision tree induction using the Gini impurity criterion.\n\n### **Problem Validation**\n\nThe first step is to validate the problem statement.\n\n**1. Extraction of Givens:**\n- **Data:** Feature matrix $X \\in \\mathbb{R}^{n \\times d}$, labels $y \\in \\{0,1\\}^n$.\n- **Hyperparameters:** Maximum depth $D_{\\max}$, minimum leaf size $m_{\\min}$.\n- **Tree Construction Rules:**\n    - Gini impurity: $G(S) = 1 - \\sum_{c \\in \\{0,1\\}} (p_c)^2$.\n    - Split: A node $S$ is split into $S_L$ and $S_R$ based on a feature $j$ and a threshold $t$.\n    - Admissible thresholds: Midpoints of successive unique sorted feature values.\n    - Impurity decrease: $\\Delta(S;j,t) = G(S) - \\frac{|S_L|}{|S|} G(S_L) - \\frac{|S_R|}{|S|} G(S_R)$.\n    - Best split: The one maximizing $\\Delta$ among all *valid* splits. A split is valid if both children sizes are at least $m_{\\min}$.\n    - Tie-breaking: Smallest feature index $j$, then smallest threshold $t$.\n    - Termination (Leaf) conditions:\n        1. No valid split yields a strictly positive impurity decrease ($\\Delta \\le 0$).\n        2. Node depth equals $D_{\\max}$.\n        3. Node size $|S|  2$.\n    - Leaf prediction: Majority class in the node; predict class $0$ in case of a tie.\n- **Constrained-Root Tree:**\n    - Root must split on a specified feature $j^\\star$.\n    - The threshold $t$ is chosen to maximize $\\Delta$ for feature $j^\\star$.\n    - A special rule applies if no split on $j^\\star$ respects $m_{\\min}$: the constraint is relaxed to only require non-empty children.\n    - If no split is possible on $j^\\star$ (e.g., constant feature value), the root becomes a leaf.\n- **Output Metrics:**\n    - Tree depths, training misclassification rates, the threshold used at the constrained root, and a boolean indicating if the trees' internal structures differ. The structure is defined by a preorder traversal of internal nodes' split rules $(j, t)$.\n\n**2. Validation Verdict:**\n- **Scientifically Grounded:** The problem describes a variant of the CART algorithm, a cornerstone of machine learning. All definitions (Gini impurity, information gain) are standard. The problem is scientifically sound.\n- **Well-Posed:** All rules for tree construction, including splitting, stopping, and tie-breaking, are explicitly and unambiguously defined. This ensures that for any given input, a unique, deterministic tree structure is produced. The problem is well-posed.\n- **Objective:** The problem is stated in formal, mathematical language, free from subjectivity or ambiguity.\n- **Completeness and Consistency:** All necessary parameters ($X$, $y$, $D_{\\max}$, $m_{\\min}$, $j^\\star$) are provided for each test case. The rules, including special conditions for the constrained root, are internally consistent and cover all scenarios.\n\n**Conclusion:** The problem is deemed **valid**. It is a rigorous and well-specified algorithmic problem. I will now proceed to design the solution.\n\n### **Solution Design**\n\nThe core of the solution is a recursive algorithm that builds the decision tree. The algorithm will be encapsulated within a Python class structure for clarity and modularity.\n\n**1. Data Structures:**\nA `Node` class will represent each node in the tree, storing its depth, whether it is a leaf, its prediction (if a leaf), its split parameters (if an internal node), and references to its children.\n\n**2. Recursive Tree Construction (`_build_tree`):**\nThis function takes a set of sample `indices` and the current `depth` as input and returns a `Node`.\n- **Base Cases (Leaf Creation):** The recursion terminates and a leaf node is created if any of the stopping criteria are met: depth reaches $D_{\\max}$, the number of samples is too small ($2$), or the node is pure (all samples have the same class, which implies no split can have a positive impurity decrease). A leaf is also created if no valid split with a positive decrease can be found. The leaf's prediction is determined by the majority class, with ties broken by predicting class $0$.\n- **Recursive Step (Internal Node Creation):**\n    - The function searches for the best split by iterating through all features and all admissible thresholds.\n    - For each potential split, it checks for validity (child sizes $\\ge m_{\\min}$), calculates the Gini impurity decrease $\\Delta$, and updates the best split found so far. The tie-breaking rule (smallest feature index, then smallest threshold) is implicitly handled by the order of iteration.\n    - If a split with $\\Delta  0$ is found, a new internal node is created with the best split parameters. The `_build_tree` function is then called recursively for the left and right children subsets of indices, with the depth incremented.\n\n**3. Handling the Constrained Root:**\nThe `fit` method will accept an optional `j_star` parameter. If `j_star` is provided, the root node's splitting logic is modified:\n- **Pass 1:** It first attempts to find the best split on feature $j^\\star$ that satisfies the $m_{\\min}$ constraint.\n- **Pass 2 (Fallback):** If no such split exists, it searches again for the best split on $j^\\star$ that merely produces non-empty children, ignoring $m_{\\min}$.\n- If no split on $j^\\star$ is possible (constant feature value) or if the best possible split has $\\Delta \\le 0$, the root becomes a leaf.\n\n**4. Calculation of Output Metrics:**\nAfter a tree is built, helper methods are used to compute the required metrics:\n- **Depth:** A recursive traversal finds the maximum depth among all leaves.\n- **Misclassification Rate:** The tree predicts a class for each sample in the training set, and the error rate is the fraction of incorrect predictions.\n- **Preorder Sequence:** A recursive preorder traversal collects the `(split_feature, split_threshold)` pairs from all internal nodes.\n\nThe main `solve` function will execute this entire process for each test case, constructing both the unconstrained and constrained-root trees, calculating the defined metrics, and formatting the results exactly as specified.", "answer": "```python\nimport numpy as np\n\nclass Node:\n    \"\"\"Represents a node in the decision tree.\"\"\"\n    def __init__(self, depth):\n        self.depth = depth\n        self.is_leaf = False\n        self.prediction = None\n        self.split_feature = None\n        self.split_threshold = None\n        self.left_child = None\n        self.right_child = None\n\nclass DecisionTree:\n    \"\"\"A binary decision tree classifier.\"\"\"\n    def __init__(self, D_max, m_min):\n        self.D_max = D_max\n        self.m_min = m_min\n        self.root = None\n        self.X = None\n        self.y = None\n\n    def _gini(self, indices):\n        \"\"\"Calculates Gini impurity for a given set of sample indices.\"\"\"\n        n_samples = len(indices)\n        if n_samples == 0:\n            return 0.0\n        \n        y_subset = self.y[indices]\n        _, counts = np.unique(y_subset, return_counts=True)\n        proportions = counts / n_samples\n        return 1.0 - np.sum(proportions**2)\n\n    def _find_best_split(self, indices):\n        \"\"\"Finds the best valid split for a node.\"\"\"\n        n_samples = len(indices)\n        if n_samples  2:\n            return None, -1.0\n        \n        parent_gini = self._gini(indices)\n        best_decrease = -1.0\n        best_split = None\n        n_features = self.X.shape[1]\n        \n        for j in range(n_features):\n            feature_values = self.X[indices, j]\n            unique_values = np.unique(feature_values)\n            \n            if len(unique_values) = 1:\n                continue\n\n            thresholds = (unique_values[:-1] + unique_values[1:]) / 2.0\n            \n            for t in thresholds:\n                left_indices = indices[np.where(self.X[indices, j] = t)[0]]\n                right_indices = indices[np.where(self.X[indices, j]  t)[0]]\n                \n                if len(left_indices)  self.m_min or len(right_indices)  self.m_min:\n                    continue\n                \n                p_left = len(left_indices) / n_samples\n                p_right = len(right_indices) / n_samples\n                decrease = parent_gini - (p_left * self._gini(left_indices) + p_right * self._gini(right_indices))\n                \n                if decrease  best_decrease:\n                    best_decrease = decrease\n                    best_split = (j, t, left_indices, right_indices)\n        \n        return best_split, best_decrease\n\n    def _find_constrained_root_split(self, indices, j_star):\n        \"\"\"Finds the best split for the root node, constrained to a single feature.\"\"\"\n        n_samples = len(indices)\n        parent_gini = self._gini(indices)\n        feature_values = self.X[indices, j_star]\n        unique_values = np.unique(feature_values)\n        \n        if len(unique_values) = 1:\n            return None, -1.0\n\n        thresholds = (unique_values[:-1] + unique_values[1:]) / 2.0\n        \n        # Pass 1: Try to find a split respecting m_min\n        best_decrease_valid = -1.0\n        best_split_valid = None\n        for t in thresholds:\n            left_indices = indices[np.where(self.X[indices, j_star] = t)[0]]\n            right_indices = indices[np.where(self.X[indices, j_star]  t)[0]]\n\n            if len(left_indices)  self.m_min or len(right_indices)  self.m_min: continue\n            \n            p_left = len(left_indices) / n_samples\n            p_right = len(right_indices) / n_samples\n            decrease = parent_gini - (p_left * self._gini(left_indices) + p_right * self._gini(right_indices))\n            \n            if decrease  best_decrease_valid:\n                best_decrease_valid = decrease\n                best_split_valid = (j_star, t, left_indices, right_indices)\n        \n        if best_split_valid is not None:\n            return best_split_valid, best_decrease_valid\n\n        # Pass 2: Relax m_min constraint, require only non-empty children\n        best_decrease_nonempty = -1.0\n        best_split_nonempty = None\n        for t in thresholds:\n            left_indices = indices[np.where(self.X[indices, j_star] = t)[0]]\n            right_indices = indices[np.where(self.X[indices, j_star]  t)[0]]\n\n            if len(left_indices) == 0 or len(right_indices) == 0: continue\n            \n            p_left = len(left_indices) / n_samples\n            p_right = len(right_indices) / n_samples\n            decrease = parent_gini - (p_left * self._gini(left_indices) + p_right * self._gini(right_indices))\n            \n            if decrease  best_decrease_nonempty:\n                best_decrease_nonempty = decrease\n                best_split_nonempty = (j_star, t, left_indices, right_indices)\n\n        if best_split_nonempty is not None:\n             return best_split_nonempty, best_decrease_nonempty\n        \n        return None, -1.0\n\n    def _build_tree(self, indices, depth, is_constrained_root=False, j_star=None):\n        \"\"\"Recursively builds the tree.\"\"\"\n        node = Node(depth)\n        n_samples = len(indices)\n        y_subset = self.y[indices]\n        \n        # Leaf conditions\n        is_leaf = False\n        if depth = self.D_max or n_samples  2 or len(np.unique(y_subset)) == 1:\n            is_leaf = True\n        \n        if not is_leaf:\n            best_split, best_decrease = None, -1.0\n            if is_constrained_root:\n                best_split, best_decrease = self._find_constrained_root_split(indices, j_star)\n            else:\n                best_split, best_decrease = self._find_best_split(indices)\n            \n            if best_decrease is None or best_decrease = 0:\n                is_leaf = True\n            else:\n                node.split_feature, node.split_threshold, left_indices, right_indices = best_split\n                node.left_child = self._build_tree(left_indices, depth + 1)\n                node.right_child = self._build_tree(right_indices, depth + 1)\n        \n        if is_leaf:\n            node.is_leaf = True\n            classes, counts = np.unique(y_subset, return_counts=True)\n            if len(classes) == 0:\n                node.prediction = 0\n            elif len(classes) == 1:\n                node.prediction = classes[0]\n            else: # Tie-breaking for prediction\n                if counts[0] == counts[1]:\n                    node.prediction = classes[np.where(classes == 0)[0][0]]\n                else:\n                    node.prediction = classes[np.argmax(counts)]\n        \n        return node\n    \n    def fit(self, X, y, j_star=None):\n        self.X = X\n        self.y = y\n        n_samples = X.shape[0]\n        initial_indices = np.arange(n_samples, dtype=int)\n        \n        self.root = self._build_tree(initial_indices, 0, is_constrained_root=(j_star is not None), j_star=j_star)\n\n    def get_depth(self):\n        if self.root is None: return 0\n        return self._get_depth_recursive(self.root)\n\n    def _get_depth_recursive(self, node):\n        if node.is_leaf: return node.depth\n        return max(self._get_depth_recursive(node.left_child), self._get_depth_recursive(node.right_child))\n        \n    def _predict_single(self, x, node):\n        if node.is_leaf: return node.prediction\n        if x[node.split_feature] = node.split_threshold:\n            return self._predict_single(x, node.left_child)\n        return self._predict_single(x, node.right_child)\n            \n    def predict(self, X):\n        return np.array([self._predict_single(x, self.root) for x in X])\n\n    def calculate_error(self, X, y):\n        if self.root is None or len(y) == 0: return 0.0\n        y_pred = self.predict(X)\n        return np.sum(y_pred != y) / len(y)\n\n    def get_preorder_sequence(self):\n        sequence = []\n        if self.root is not None:\n            self._get_preorder_recursive(self.root, sequence)\n        return sequence\n\n    def _get_preorder_recursive(self, node, sequence):\n        if not node.is_leaf:\n            sequence.append((node.split_feature, node.split_threshold))\n            self._get_preorder_recursive(node.left_child, sequence)\n            self._get_preorder_recursive(node.right_child, sequence)\n\ndef solve():\n    test_cases = [\n        (np.array([[0.1, 0.3, 0.2], [0.2, 0.1, 0.5], [0.4, 0.6, 0.7], [0.6, 0.2, 0.4], [0.7, 0.8, 0.9], [0.8, 0.1, 0.3], [0.9, 0.5, 0.2], [0.55, 0.4, 0.6]]),\n         np.array([0, 0, 0, 1, 1, 1, 1, 1]), 3, 2, 0),\n        (np.array([[0.2, 0.3, 0.1], [0.1, 0.2, 0.8], [0.4, 0.6, 0.5], [0.35, 0.7, 0.4], [0.5, 0.9, 0.6], [0.45, 0.55, 0.2], [0.3, 0.4, 0.9], [0.25, 0.8, 0.3], [0.6, 0.1, 0.7], [0.7, 0.85, 0.2]]),\n         np.array([0, 0, 1, 1, 1, 1, 0, 1, 0, 1]), 3, 2, 2),\n        (np.array([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0], [0.5, 0.5], [0.5, 0.5]]),\n         np.array([0, 0, 1, 1, 0, 1]), 3, 1, 1)\n    ]\n    \n    all_results_str = []\n    \n    for X, y, D_max, m_min, j_star in test_cases:\n        # Unconstrained tree\n        tree_unc = DecisionTree(D_max, m_min)\n        tree_unc.fit(X, y)\n        d_unc = tree_unc.get_depth()\n        err_unc = tree_unc.calculate_error(X, y)\n        seq_unc = tree_unc.get_preorder_sequence()\n\n        # Constrained-root tree\n        tree_con = DecisionTree(D_max, m_min)\n        tree_con.fit(X, y, j_star=j_star)\n        d_con = tree_con.get_depth()\n        err_con = tree_con.calculate_error(X, y)\n        seq_con = tree_con.get_preorder_sequence()\n        \n        # Constrained root threshold\n        if tree_con.root.is_leaf:\n            root_t_con = -1.0\n        else:\n            root_t_con = tree_con.root.split_threshold\n        \n        # Structural change\n        struct_change = (seq_unc != seq_con)\n        \n        case_results = [\n            d_unc,\n            d_con,\n            f\"{err_unc:.6f}\",\n            f\"{err_con:.6f}\",\n            f\"{root_t_con:.6f}\",\n            struct_change\n        ]\n        \n        all_results_str.append(f\"[{','.join(map(str, case_results))}]\")\n\n    print(f\"[{','.join(all_results_str)}]\")\n\nsolve()\n```", "id": "2384430"}]}