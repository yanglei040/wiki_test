{"hands_on_practices": [{"introduction": "Understanding how a Convolutional Neural Network (CNN) processes biological sequences begins with tracing the data's journey through the network. This first exercise provides a hands-on walk-through of a complete forward pass, from one-hot encoding a DNA sequence to calculating the final prediction probability. By manually applying the convolution, activation, and pooling operations with a pre-defined model, you will build a concrete intuition for how these networks learn to recognize patterns like transcription factor binding sites [@problem_id:2382334].", "problem": "Consider a binary classification model for Deoxyribonucleic Acid (DNA) sequences that first computes a latent feature vector using a single one-dimensional convolutional layer with a rectified linear unit activation and global max pooling, and then applies logistic regression on the latent vector. Let the input be a DNA sequence over the alphabet $\\{A,C,G,T\\}$ of length $L$, which is one-hot encoded into a matrix $X \\in \\{0,1\\}^{L \\times 4}$ using the fixed channel order $(A,C,G,T)$, so that for each position $i \\in \\{0,\\dots,L-1\\}$, the row $X_{i,:}$ is the one-hot vector corresponding to the base at position $i$.\n\nThe convolutional layer has $K=2$ filters, each of kernel width $F=3$, with valid convolution (no padding) and stride $1$. Denote the filter weights by $W \\in \\mathbb{R}^{K \\times F \\times 4}$ and biases by $b \\in \\mathbb{R}^{K}$. For each position $i \\in \\{0,\\dots,L-F\\}$ and filter index $k \\in \\{0,1\\}$, the pre-activation is\n$$\nH_{i,k} \\;=\\; \\sum_{j=0}^{F-1} \\sum_{c=0}^{3} W_{k,j,c}\\, X_{i+j,c} \\;+\\; b_k.\n$$\nThe activation is the rectified linear unit $R_{i,k} = \\max(0, H_{i,k})$. The global max pooling produces the latent vector $z \\in \\mathbb{R}^{K}$ with components\n$$\nz_k \\;=\\; \\max_{0 \\le i \\le L-F} R_{i,k}.\n$$\nThe logistic regression computes the probability of the positive class as\n$$\n\\hat{y} \\;=\\; \\sigma\\!\\left(\\sum_{k=0}^{K-1} u_k z_k + a \\right), \\quad \\text{where} \\quad \\sigma(t) = \\frac{1}{1+e^{-t}},\n$$\nwith weights $u \\in \\mathbb{R}^{K}$ and bias $a \\in \\mathbb{R}$.\n\nUse the following fixed parameters for all test cases:\n- Filter definitions via explicit weights and biases (channel order $(A,C,G,T)$):\n  - For filter $k=0$ (pattern matching $A$ at offset $0$, $C$ at offset $1$, $G$ at offset $2$):\n    - $W_{0,0,:} = [\\,1,\\,-1,\\,-1,\\,-1\\,]$,\n    - $W_{0,1,:} = [\\,-1,\\,1,\\,-1,\\,-1\\,]$,\n    - $W_{0,2,:} = [\\,-1,\\,-1,\\,1,\\,-1\\,]$,\n    - $b_0 = -1$.\n  - For filter $k=1$ (pattern matching $T$ at offsets $0,1,2$):\n    - $W_{1,0,:} = [\\,-1,\\,-1,\\,-1,\\,1\\,]$,\n    - $W_{1,1,:} = [\\,-1,\\,-1,\\,-1,\\,1\\,]$,\n    - $W_{1,2,:} = [\\,-1,\\,-1,\\,-1,\\,1\\,]$,\n    - $b_1 = -1$.\n- Logistic regression parameters:\n  - $u = [\\,1.0,\\,-0.5\\,]$,\n  - $a = -1.0$.\n\nAll sequences in the test suite satisfy $L \\ge F$. For each sequence, compute the one-hot encoding $X$, then compute $H$, $R$, $z$, and finally $\\hat{y}$ as defined above.\n\nTest suite:\n- Case $1$: sequence $S_1 = \\text{\"ACGAC\"}$.\n- Case $2$: sequence $S_2 = \\text{\"TTT\"}$.\n- Case $3$: sequence $S_3 = \\text{\"ACGTTT\"}$.\n- Case $4$: sequence $S_4 = \\text{\"GCA\"}$.\n\nYour program must output a single line containing the results for the four cases as a comma-separated list of the four probabilities $[\\hat{y}_1,\\hat{y}_2,\\hat{y}_3,\\hat{y}_4]$, each rounded to exactly $6$ decimal places, with no spaces, enclosed in square brackets. For example, an output should have the form $[\\text{float},\\text{float},\\text{float},\\text{float}]$ such as $[0.123456,0.234567,0.345678,0.456789]$.", "solution": "The problem statement is scientifically sound, well-posed, and complete. It describes a standard, albeit simplified, forward pass of a convolutional neural network for sequence classification. All necessary parameters and test cases are provided, and there are no ambiguities or contradictions. We shall proceed with the calculation.\n\nThe task is to compute the output probability $\\hat{y}$ of a small convolutional neural network for four given DNA sequences. The network architecture consists of a one-dimensional convolutional layer, a Rectified Linear Unit (ReLU) activation, a global max pooling layer, and a logistic regression output layer. We will meticulously follow the defined mathematical operations for each sequence.\n\nFirst, we define the fixed model parameters:\n- The alphabet is $\\{\\text{A, C, G, T}\\}$. The one-hot encoding channel order is $(\\text{A, C, G, T})$.\n- Convolutional layer: $K=2$ filters, kernel width $F=3$, stride $1$, no padding.\n- Filter $k=0$ is designed to detect the motif \"ACG\". Its weights $W_0$ and bias $b_0$ are given as:\n$$W_0 = \\begin{pmatrix} [\\,1,-1,-1,-1\\,] \\\\ [-1,\\,1,-1,-1\\,] \\\\ [-1,-1,\\,1,-1\\,] \\end{pmatrix}, \\quad b_0 = -1$$\nA perfect match \"ACG\" yields a pre-activation of $(1 \\times 1 + 1 \\times 1 + 1 \\times 1) + b_0 = 3 - 1 = 2$.\n- Filter $k=1$ is designed to detect \"TTT\". Its weights $W_1$ and bias $b_1$ are:\n$$W_1 = \\begin{pmatrix} [-1,-1,-1,\\,1\\,] \\\\ [-1,-1,-1,\\,1\\,] \\\\ [-1,-1,-1,\\,1\\,] \\end{pmatrix}, \\quad b_1 = -1$$\nA perfect match \"TTT\" yields a pre-activation of $(1 \\times 1 + 1 \\times 1 + 1 \\times 1) + b_1 = 3 - 1 = 2$.\n- Logistic regression parameters are weights $u = [1.0, -0.5]$ and bias $a = -1.0$.\n\nThe computation proceeds in five steps for each sequence:\n1.  **One-Hot Encoding**: The input sequence $S$ of length $L$ is transformed into a binary matrix $X \\in \\{0, 1\\}^{L \\times 4}$.\n2.  **Convolution**: The pre-activation matrix $H \\in \\mathbb{R}^{(L-F+1) \\times K}$ is computed. For each filter $k$ and each possible position $i$ of the kernel, $H_{i,k} = \\sum_{j=0}^{F-1} \\sum_{c=0}^{3} W_{k,j,c} X_{i+j,c} + b_k$.\n3.  **ReLU Activation**: The non-linear activation matrix is $R_{i,k} = \\max(0, H_{i,k})$.\n4.  **Global Max Pooling**: The latent feature vector $z \\in \\mathbb{R}^K$ is formed by taking the maximum activation value for each filter across all positions: $z_k = \\max_i R_{i,k}$.\n5.  **Logistic Regression**: The final probability is computed as $\\hat{y} = \\sigma(u \\cdot z + a)$, where $\\sigma(t) = 1/(1+e^{-t})$ is the sigmoid function.\n\nWe now apply this procedure to each test case.\n\n**Case 1: Sequence $S_1 = \\text{\"ACGAC\"}$**\n- Length $L=5$. Number of $F=3$ windows is $5-3+1=3$. The windows are: \"ACG\", \"CGA\", \"GAC\".\n- For filter $k=0$ (ACG detector):\n  - Window \"ACG\" ($i=0$): a perfect match. $H_{0,0} = 2$.\n  - Window \"CGA\" ($i=1$): non-match. $H_{1,0} = (-1-1-1) - 1 = -4$.\n  - Window \"GAC\" ($i=2$): non-match. $H_{2,0} = (-1-1+1) - 1 = -2$.\n  - Pre-activations $H_{:,0} = [2, -4, -2]$. After ReLU, $R_{:,0} = [2, 0, 0]$.\n  - $z_0 = \\max(2, 0, 0) = 2$.\n- For filter $k=1$ (TTT detector): All windows are non-matches, resulting in pre-activations $H_{0,1}=-4$, $H_{1,1}=-4$, $H_{2,1}=-4$. After ReLU, all are $0$.\n  - $z_1 = 0$.\n- Latent vector $z = [2, 0]$.\n- Logistic regression input: $t_1 = u \\cdot z + a = (1.0 \\times 2) + (-0.5 \\times 0) - 1.0 = 1.0$.\n- Final probability: $\\hat{y}_1 = \\sigma(1.0) = \\frac{1}{1+e^{-1}} \\approx 0.73105858$.\n\n**Case 2: Sequence $S_2 = \\text{\"TTT\"}$**\n- Length $L=3$. Number of windows is $3-3+1=1$. The window is \"TTT\".\n- For filter $k=0$ (ACG detector): Window \"TTT\" is a non-match. $H_{0,0} = (-1-1-1)-1=-4$. ReLU gives $0$.\n  - $z_0 = 0$.\n- For filter $k=1$ (TTT detector): Window \"TTT\" is a perfect match. $H_{0,1} = 2$. ReLU gives $2$.\n  - $z_1 = 2$.\n- Latent vector $z = [0, 2]$.\n- Logistic regression input: $t_2 = u \\cdot z + a = (1.0 \\times 0) + (-0.5 \\times 2) - 1.0 = -2.0$.\n- Final probability: $\\hat{y}_2 = \\sigma(-2.0) = \\frac{1}{1+e^{2}} \\approx 0.11920292$.\n\n**Case 3: Sequence $S_3 = \\text{\"ACGTTT\"}$**\n- Length $L=6$. Number of windows is $6-3+1=4$. The windows are: \"ACG\", \"CGT\", \"GTT\", \"TTT\".\n- For filter $k=0$ (ACG detector):\n  - Window \"ACG\" ($i=0$) is a perfect match ($H_{0,0}=2$). Other windows are non-matches, yielding negative pre-activations.\n  - After ReLU, the activation vector is $[2, 0, 0, 0]$.\n  - $z_0 = \\max(2, 0, 0, 0) = 2$.\n- For filter $k=1$ (TTT detector):\n  - Window \"TTT\" ($i=3$) is a perfect match ($H_{3,1}=2$). The other windows yield $H_{0,1}=-4, H_{1,1}=-2, H_{2,1}=0$.\n  - After ReLU, the activation vector is $[0, 0, 0, 2]$.\n  - $z_1 = \\max(0, 0, 0, 2) = 2$.\n- Latent vector $z = [2, 2]$.\n- Logistic regression input: $t_3 = u \\cdot z + a = (1.0 \\times 2) + (-0.5 \\times 2) - 1.0 = 2 - 1 - 1 = 0$.\n- Final probability: $\\hat{y}_3 = \\sigma(0) = \\frac{1}{1+e^{0}} = 0.5$.\n\n**Case 4: Sequence $S_4 = \\text{\"GCA\"}$**\n- Length $L=3$. Number of windows is $3-3+1=1$. The window is \"GCA\".\n- For filter $k=0$ (ACG detector): Window \"GCA\" is a non-match. $H_{0,0} = (-1+1-1)-1=-2$. ReLU gives $0$.\n  - $z_0 = 0$.\n- For filter $k=1$ (TTT detector): Window \"GCA\" is a non-match. $H_{0,1} = (-1-1-1)-1=-4$. ReLU gives $0$.\n  - $z_1 = 0$.\n- Latent vector $z = [0, 0]$.\n- Logistic regression input: $t_4 = u \\cdot z + a = (1.0 \\times 0) + (-0.5 \\times 0) - 1.0 = -1.0$.\n- Final probability: $\\hat{y}_4 = \\sigma(-1.0) = \\frac{1}{1+e^{1}} \\approx 0.26894142$.\n\nThe final results, rounded to $6$ decimal places, are:\n- $\\hat{y}_1 \\approx 0.731059$\n- $\\hat{y}_2 \\approx 0.119203$\n- $\\hat{y}_3 = 0.500000$\n- $\\hat{y}_4 \\approx 0.268941$", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the output of a simple CNN for DNA sequence classification.\n    The solution rigorously follows the forward pass defined in the problem statement.\n    \"\"\"\n    \n    # Define fixed model parameters as specified in the problem statement.\n    # Channel order is (A, C, G, T)\n    W = np.array([\n        [  # Filter k=0 for \"ACG\"\n            [1, -1, -1, -1],\n            [-1, 1, -1, -1],\n            [-1, -1, 1, -1]\n        ],\n        [  # Filter k=1 for \"TTT\"\n            [-1, -1, -1, 1],\n            [-1, -1, -1, 1],\n            [-1, -1, -1, 1]\n        ]\n    ], dtype=np.float64) # Shape: K x F x 4 = 2 x 3 x 4\n    \n    b = np.array([-1.0, -1.0], dtype=np.float64) # Shape: K = 2\n    \n    u = np.array([1.0, -0.5], dtype=np.float64) # Shape: K = 2\n    a = -1.0\n    \n    F = 3 # Kernel width\n    \n    # Mapping from DNA base to channel index\n    base_to_idx = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n    \n    test_cases = [\n        \"ACGAC\",    # Case 1\n        \"TTT\",      # Case 2\n        \"ACGTTT\",   # Case 3\n        \"GCA\"       # Case 4\n    ]\n    \n    results = []\n    \n    for seq in test_cases:\n        L = len(seq)\n        \n        # Step 1: One-hot encode the sequence\n        X = np.zeros((L, 4), dtype=np.float64)\n        for i, base in enumerate(seq):\n            X[i, base_to_idx[base]] = 1.0\n            \n        # The number of positions for the convolutional kernel\n        num_windows = L - F + 1\n        \n        # Step 2: Convolution to get pre-activations H\n        K = W.shape[0]\n        H = np.zeros((num_windows, K), dtype=np.float64)\n        \n        for i in range(num_windows):\n            window = X[i : i + F, :] # Shape F x 4\n            for k in range(K):\n                # Element-wise product and sum, plus bias\n                pre_activation = np.sum(W[k] * window) + b[k]\n                H[i, k] = pre_activation\n        \n        # Step 3: ReLU Activation\n        R = np.maximum(0, H)\n        \n        # Step 4: Global Max Pooling\n        # If there are no windows (L < F), max would fail. Problem states L >= F.\n        # axis=0 takes the max over all window positions for each filter.\n        z = np.max(R, axis=0)\n        \n        # Step 5: Logistic Regression\n        logit = np.dot(u, z) + a\n        y_hat = 1.0 / (1.0 + np.exp(-logit))\n        \n        results.append(y_hat)\n        \n    # Format the final output string exactly as required.\n    # Each probability is rounded to exactly 6 decimal places.\n    output_str = f\"[{','.join(f'{r:.6f}' for r in results)}]\"\n    print(output_str)\n\nsolve()\n```", "id": "2382334"}, {"introduction": "Once a model is trained, it can be used as a powerful *in silico* laboratory to predict the effects of genetic changes. This practice explores how to quantify the impact of a single nucleotide polymorphism (SNP) on a CNN's prediction by comparing the output for a reference sequence against a mutated one. This technique, a form of sensitivity analysis, is fundamental for interpreting what features the model has learned and for prioritizing variants for experimental validation [@problem_id:2382374].", "problem": "You are given a small, deterministic convolutional neural network (CNN) for deoxyribonucleic acid (DNA) sequence analysis, along with a protocol for simulating a single nucleotide polymorphism (SNP) by altering one position in the one-hot encoded input. Your task is to implement the exact mathematical pipeline and compute the signed change in the model prediction caused by each SNP for a provided test suite of sequences and mutations. Your program must produce the final results as a single line in a strictly specified format.\n\nThe input domain is the set of DNA sequences over the alphabet $\\{A,C,G,T\\}$. Each sequence of length $L$ is represented by one-hot encoding into a matrix $X \\in \\mathbb{R}^{L \\times 4}$ according to the mapping $A \\mapsto (1,0,0,0)$, $C \\mapsto (0,1,0,0)$, $G \\mapsto (0,0,1,0)$, $T \\mapsto (0,0,0,1)$. A single nucleotide polymorphism is simulated by changing exactly one position $p$ ($0$-based index) in the sequence from its original base to a different target base, and re-encoding to obtain the mutated matrix $X' \\in \\mathbb{R}^{L \\times 4}$.\n\nThe convolutional layer performs a one-dimensional cross-correlation (the conventional operation used in convolutional neural networks) with $F$ filters, stride $1$, and “valid” borders (no padding). Let $K$ be the kernel width and $C=4$ the number of input channels. For filter $f \\in \\{0,1,\\dots,F-1\\}$ with weights $W^{(f)} \\in \\mathbb{R}^{K \\times C}$ and bias $b^{(f)} \\in \\mathbb{R}$, and for output position $i \\in \\{0,1,\\dots,L-K\\}$, the pre-activation is\n$$\nz^{(f)}[i] \\;=\\; \\sum_{k=0}^{K-1} \\sum_{c=0}^{C-1} W^{(f)}[k,c] \\, X[i+k,c] \\;+\\; b^{(f)}.\n$$\nThe nonlinearity is the Rectified Linear Unit (ReLU), defined by\n$$\n\\mathrm{ReLU}(u) \\;=\\; \\max(0,u).\n$$\nThis yields activations\n$$\na^{(f)}[i] \\;=\\; \\mathrm{ReLU}\\!\\left(z^{(f)}[i]\\right).\n$$\nA global max pooling is then applied independently to each filter,\n$$\nm^{(f)} \\;=\\; \\max_{i} \\, a^{(f)}[i] \\, ,\n$$\nproducing a vector $m \\in \\mathbb{R}^{F}$. Finally, a fully connected linear readout produces the scalar prediction\n$$\ny \\;=\\; \\sum_{f=0}^{F-1} w^{\\mathrm{fc}}[f] \\, m^{(f)} \\;+\\; b^{\\mathrm{fc}} \\, .\n$$\n\nYou are provided with a specific network of $F=2$ filters and kernel width $K=3$, with the following weights and biases. All omitted entries are zero.\n- Filter $f=0$ (detects the pattern $A\\!C\\!G$):\n  $$\n  W^{(0)} \\;=\\; \\begin{bmatrix}\n  1 & 0 & 0 & 0 \\\\\n  0 & 1 & 0 & 0 \\\\\n  0 & 0 & 1 & 0\n  \\end{bmatrix}, \\quad b^{(0)} \\;=\\; -2.5 \\, .\n  $$\n- Filter $f=1$ (detects the pattern $T\\!T\\!T$):\n  $$\n  W^{(1)} \\;=\\; \\begin{bmatrix}\n  0 & 0 & 0 & 1 \\\\\n  0 & 0 & 0 & 1 \\\\\n  0 & 0 & 0 & 1\n  \\end{bmatrix}, \\quad b^{(1)} \\;=\\; -2.5 \\, .\n  $$\n- Fully connected readout:\n  $$\n  w^{\\mathrm{fc}} \\;=\\; \\begin{bmatrix} 1.0 \\\\ -0.5 \\end{bmatrix}, \\quad b^{\\mathrm{fc}} \\;=\\; 0.1 \\, .\n  $$\n\nFor each test case, compute the signed change in prediction\n$$\n\\Delta \\;=\\; y_{\\mathrm{mut}} \\;-\\; y_{\\mathrm{ref}} \\, ,\n$$\nwhere $y_{\\mathrm{ref}}$ is the prediction for the original sequence and $y_{\\mathrm{mut}}$ is the prediction after applying the SNP. Use $0$-based indexing for positions. Sequences contain only the characters $A$, $C$, $G$, $T$. Every SNP changes a base to a different base.\n\nTest suite to implement and evaluate:\n- Case $1$: sequence $S = \\text{\"AACGTTGA\"}$, position $p = 2$, new base $B' = \\text{\"T\"}$.\n- Case $2$: sequence $S = \\text{\"TTTACGTT\"}$, position $p = 0$, new base $B' = \\text{\"G\"}$.\n- Case $3$: sequence $S = \\text{\"ACG\"}$, position $p = 1$, new base $B' = \\text{\"G\"}$.\n- Case $4$: sequence $S = \\text{\"ACGACG\"}$, position $p = 1$, new base $B' = \\text{\"T\"}$.\n\nYour program must:\n- Implement the one-hot encoding, the convolutional cross-correlation, the Rectified Linear Unit, the global max pooling, and the fully connected readout exactly as specified above.\n- For each test case, compute $\\Delta$ as defined.\n- Round each $\\Delta$ to $6$ decimal places.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases, for example $[\\Delta_1,\\Delta_2,\\Delta_3,\\Delta_4]$. Each value must be rounded to $6$ decimal places with standard rounding (for example, $0.5$ becomes $\\text{\"0.500000\"}$). No other text should be printed.", "solution": "The problem statement is subjected to validation and is found to be valid. It is scientifically grounded in the principles of computational biology and deep learning, presents a well-posed mathematical task with all necessary parameters, and is formulated in objective, unambiguous language. All components of the described convolutional neural network—one-hot encoding, convolution, ReLU activation, max pooling, and a fully connected layer—are standard elements in this domain. The task of computing the effect of a single nucleotide polymorphism (SNP) on the model's output is a standard *in silico* sensitivity analysis technique. Therefore, we proceed to a complete solution.\n\nThe problem requires the computation of the change in a deterministic neural network's output, $\\Delta = y_{\\mathrm{mut}} - y_{\\mathrm{ref}}$, resulting from a single point mutation in an input DNA sequence. This is achieved by implementing the forward pass of the network for both the reference sequence ($S_{\\mathrm{ref}}$) and the mutated sequence ($S_{\\mathrm{mut}}$) to obtain their respective scalar predictions, $y_{\\mathrm{ref}}$ and $y_{\\mathrm{mut}}$.\n\nThe computational pipeline is structured as follows:\n\n**1. Input Encoding**\nA DNA sequence of length $L$ is transformed into a numerical matrix $X \\in \\mathbb{R}^{L \\times 4}$ via one-hot encoding. Each nucleotide is mapped to a unique 4-dimensional binary vector, establishing a canonical basis: $A \\mapsto [1, 0, 0, 0]$, $C \\mapsto [0, 1, 0, 0]$, $G \\mapsto [0, 0, 1, 0]$, and $T \\mapsto [0, 0, 0, 1]$.\n\n**2. Convolutional Layer**\nThis layer detects local patterns in the sequence. It consists of $F=2$ filters, each with a kernel of width $K=3$ and weights $W^{(f)} \\in \\mathbb{R}^{3 \\times 4}$. The operation is a 1D cross-correlation with a stride of $1$ and no padding (\"valid\" convolution). For each filter $f$ and each possible starting position $i$ in the input sequence (where $i \\in \\{0, 1, \\dots, L-K\\}$), a pre-activation value $z^{(f)}[i]$ is computed. This value is the sum of the element-wise product of the filter's weights $W^{(f)}$ and the corresponding $K \\times 4$ slice of the input matrix $X[i:i+K, :]$, plus a filter-specific bias $b^{(f)}$. The formula is:\n$$\nz^{(f)}[i] = \\left( \\sum_{k=0}^{K-1} \\sum_{c=0}^{C-1} W^{(f)}[k,c] \\cdot X[i+k,c] \\right) + b^{(f)}\n$$\nThe provided filter weights are designed to detect the motifs \"ACG\" ($f=0$) and \"TTT\" ($f=1$). A perfect match between a sequence window and the motif encoded in the filter weights results in a dot product sum of $3$, to which the bias $b^{(f)} = -2.5$ is added, yielding a pre-activation of $0.5$.\n\n**3. Activation Function**\nThe pre-activations are passed through a non-linear activation function, the Rectified Linear Unit (ReLU), defined as $\\mathrm{ReLU}(u) = \\max(0, u)$. This function introduces non-linearity, allowing the model to learn more complex patterns. It effectively sets any negative pre-activation values to zero, meaning a filter only \"fires\" if the input motif provides a sufficiently strong match to overcome the negative bias. The activations are given by:\n$$\na^{(f)}[i] = \\mathrm{ReLU}(z^{(f)}[i])\n$$\n\n**4. Global Max Pooling Layer**\nFor each filter's activation map $a^{(f)}$, a single feature value $m^{(f)}$ is extracted by taking the maximum value across all positions $i$. This is known as global max pooling.\n$$\nm^{(f)} = \\max_{i} a^{(f)}[i]\n$$\nThis operation makes the model's prediction invariant to the position of the detected motif and reduces the dimensionality of the representation to a fixed-size vector $m \\in \\mathbb{R}^{F}$, regardless of the input sequence length $L$. If no position yields a positive activation, the maximum is $0$.\n\n**5. Fully Connected Readout Layer**\nThe final prediction, a scalar value $y$, is computed as a linear combination of the pooled feature values $m^{(f)}$, weighted by the fully connected layer's weights $w^{\\mathrm{fc}}$, plus a final bias term $b^{\\mathrm{fc}}$.\n$$\ny = \\sum_{f=0}^{F-1} w^{\\mathrm{fc}}[f] \\cdot m^{(f)} + b^{\\mathrm{fc}}\n$$\n\n**Example Calculation: Case 1**\nLet us walk through the calculation for Case 1: $S_{\\mathrm{ref}} = \\text{\"AACGTTGA\"}$, position $p=2$, new base $B'=\\text{\"T\"}$. The mutated sequence is $S_{\\mathrm{mut}} = \\text{\"AATGTTGA\"}$.\n\n**Reference Prediction ($y_{\\mathrm{ref}}$):**\n- Sequence: $S_{\\mathrm{ref}} = \\text{\"AACGTTGA\"}$ ($L=8$).\n- Convolutional windows ($K=3$): \"AAC\", \"ACG\", \"CGT\", \"GTT\", \"TTG\", \"TGA\".\n- **Filter 0 (\"ACG\"):** The window \"ACG\" at position $i=1$ gives a perfect match. The dot product is $3$. $z^{(0)}[1] = 3 - 2.5 = 0.5$. All other windows result in a dot product less than $2.5$, so their pre-activations are negative. The activation map is $a^{(0)}_{\\mathrm{ref}} = [0, 0.5, 0, 0, 0, 0]$.\n- **Filter 1 (\"TTT\"):** No window perfectly matches \"TTT\". The highest dot products come from \"GTT\" and \"TTG\" (score of $2$), yielding pre-activations of $2 - 2.5 = -0.5$. Thus, all activations for this filter are $0$. The activation map is $a^{(1)}_{\\mathrm{ref}} = [0, 0, 0, 0, 0, 0]$.\n- **Max Pooling:** $m^{(0)}_{\\mathrm{ref}} = \\max(a^{(0)}_{\\mathrm{ref}}) = 0.5$. $m^{(1)}_{\\mathrm{ref}} = \\max(a^{(1)}_{\\mathrm{ref}}) = 0$. The pooled feature vector is $m_{\\mathrm{ref}} = [0.5, 0]$.\n- **Readout:** $y_{\\mathrm{ref}} = (1.0 \\cdot 0.5) + (-0.5 \\cdot 0) + 0.1 = 0.5 + 0 + 0.1 = 0.6$.\n\n**Mutated Prediction ($y_{\\mathrm{mut}}$):**\n- Sequence: $S_{\\mathrm{mut}} = \\text{\"AATGTTGA\"}$. The mutation at $p=2$ changes \"C\" to \"T\".\n- The original \"ACG\" motif is destroyed. The new windows affected are \"AAT\" ($i=0$), \"ATG\" ($i=1$), and \"TGT\" ($i=2$).\n- **Filter 0 (\"ACG\"):** The highest dot product from any window is now $2$ (from \"ATG\"), yielding a pre-activation of $2 - 2.5 = -0.5$. All activations are $0$. $a^{(0)}_{\\mathrm{mut}} = [0, 0, 0, 0, 0, 0]$.\n- **Filter 1 (\"TTT\"):** The mutation does not create a \"TTT\" motif. The highest dot product is $2$ (from \"TGT\", \"GTT\", \"TTG\"), yielding pre-activations of $-0.5$. All activations are $0$. $a^{(1)}_{\\mathrm{mut}} = [0, 0, 0, 0, 0, 0]$.\n- **Max Pooling:** $m^{(0)}_{\\mathrm{mut}} = 0$. $m^{(1)}_{\\mathrm{mut}} = 0$. The pooled feature vector is $m_{\\mathrm{mut}} = [0, 0]$.\n- **Readout:** $y_{\\mathrm{mut}} = (1.0 \\cdot 0) + (-0.5 \\cdot 0) + 0.1 = 0.1$.\n\n**Final Calculation for Case 1:**\nThe change in prediction is $\\Delta_1 = y_{\\mathrm{mut}} - y_{\\mathrm{ref}} = 0.1 - 0.6 = -0.5$.\n\nThis exact procedure is applied to all test cases, with the results rounded to $6$ decimal places. The provided program implements this logic.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the signed change in a CNN's prediction due to single nucleotide polymorphisms.\n    \"\"\"\n    # Define the fixed network parameters\n    params = {\n        'W0': np.array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0]], dtype=float),\n        'b0': -2.5,\n        'W1': np.array([[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1]], dtype=float),\n        'b1': -2.5,\n        'w_fc': np.array([1.0, -0.5], dtype=float),\n        'b_fc': 0.1\n    }\n    \n    # One-hot encoding mapping\n    one_hot_map = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n    num_channels = 4\n    kernel_width = 3\n\n    def get_prediction(sequence: str) -> float:\n        \"\"\"\n        Performs a full forward pass of the CNN for a given DNA sequence.\n        \"\"\"\n        seq_len = len(sequence)\n        \n        # Handle sequences shorter than the kernel\n        if seq_len < kernel_width:\n             # No convolution is possible, so all activations are zero by default\n             m = np.array([0.0, 0.0])\n             y = np.sum(m * params['w_fc']) + params['b_fc']\n             return y\n        \n        # 1. One-hot encode the sequence\n        X = np.zeros((seq_len, num_channels), dtype=float)\n        for i, base in enumerate(sequence):\n            if base in one_hot_map:\n                X[i, one_hot_map[base]] = 1.0\n\n        # Define filters and biases\n        filters = [(params['W0'], params['b0']), (params['W1'], params['b1'])]\n        pooled_features = []\n\n        for W_f, b_f in filters:\n            # 2. Convolutional Layer (pre-activations)\n            conv_len = seq_len - kernel_width + 1\n            pre_activations = np.zeros(conv_len, dtype=float)\n            for i in range(conv_len):\n                window = X[i : i + kernel_width, :]\n                pre_activations[i] = np.sum(window * W_f) + b_f\n            \n            # 3. ReLU Activation\n            activations = np.maximum(0, pre_activations)\n            \n            # 4. Global Max Pooling\n            # np.max on an empty array raises error. If activations is empty, this\n            # means conv_len was 0. In this case max is 0.\n            if activations.size > 0:\n                max_activation = np.max(activations)\n            else:\n                max_activation = 0.0\n            \n            pooled_features.append(max_activation)\n        \n        m = np.array(pooled_features)\n        \n        # 5. Fully Connected Readout\n        y = np.sum(m * params['w_fc']) + params['b_fc']\n        \n        return y\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (\"AACGTTGA\", 2, \"T\"),\n        (\"TTTACGTT\", 0, \"G\"),\n        (\"ACG\", 1, \"G\"),\n        (\"ACGACG\", 1, \"T\"),\n    ]\n\n    results = []\n    for seq_ref, p, new_base in test_cases:\n        # Get prediction for reference sequence\n        y_ref = get_prediction(seq_ref)\n        \n        # Create mutated sequence\n        seq_list = list(seq_ref)\n        seq_list[p] = new_base\n        seq_mut = \"\".join(seq_list)\n        \n        # Get prediction for mutated sequence\n        y_mut = get_prediction(seq_mut)\n        \n        # Compute the change and round\n        delta = y_mut - y_ref\n        results.append(f\"{delta:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2382374"}, {"introduction": "The performance and behavior of any neural network are profoundly influenced by how the input data is represented. This exercise moves from DNA to protein sequences and contrasts the standard one-hot encoding with a feature-based encoding using physicochemical properties. By analyzing the architectural and learning implications of this choice, you will explore the crucial concept of inductive bias and how it can help a model generalize better to unseen sequence variants [@problem_id:2382354].", "problem": "You are designing a protein sequence classifier using a convolutional neural network (CNN). Each protein is a sequence over the standard set of amino acids. You consider two input encodings per residue: (i) a one-hot encoding over the $20$ canonical amino acids, and (ii) a vector of three physicochemical properties, hydrophobicity $h$, net charge $q$, and polarity $p$, so each position is represented by a vector in $\\mathbb{R}^{3}$. The CNN begins with a single $1$-dimensional convolutional layer with kernel width $k=5$, stride $1$, no padding, and $F=64$ filters, followed by a pointwise nonlinearity. Assume the input sequence length is $L$ (a positive integer), and ignore any downstream layers. Unless otherwise stated, treat convolutional kernels as including only their weights and consider bias terms separately.\n\nSelect all options that are correct.\n\nA. Excluding biases, the total number of trainable weights in the first convolutional layer is $k \\times 20 \\times F$ for the one-hot input and $k \\times 3 \\times F$ for the property input; therefore, with $k=5$ and $F=64$, the property input uses exactly $960$ kernel weights while the one-hot input uses $6400$.\n\nB. If the three property channels $(h,q,p)$ are fed as raw numerical values on potentially different scales, the channel with the largest variance can dominate learning dynamics in the first convolutional layer; standardizing each channel to zero mean and unit variance helps mitigate this effect.\n\nC. Because the property encoding aggregates amino acids by physicochemical similarity, a width-$5$ convolution cannot detect position-specific motifs under this encoding, whereas it can under a one-hot encoding.\n\nD. For a fixed set of learned convolution weights, permuting the order of the three property channels (for example, swapping $h$ and $p$) leaves the convolution outputs unchanged.\n\nE. Compared to a one-hot encoding, the property encoding can allow a single learned filter to respond similarly to unseen amino acid substitutions that preserve physicochemical similarity, potentially improving generalization to variants not seen during training.", "solution": "The problem statement is a valid description of a standard setup in computational biology for applying convolutional neural networks (CNNs) to protein sequence analysis. The problem is scientifically grounded, well-posed, objective, and self-contained, providing sufficient information to evaluate the provided options. The two encoding methods, one-hot and physicochemical properties, are standard representations, and the CNN architecture described is typical for sequence processing tasks.\n\nLet us analyze the structure of the input and the convolutional layer. The input is a sequence of length $L$. Each element of the sequence is a vector representing an amino acid.\nFor the one-hot encoding, each amino acid is represented by a vector of dimension $C_1 = 20$. The input tensor thus has a shape of $(L, 20)$, if we consider the format (length, channels).\nFor the physicochemical property encoding, each amino acid is represented by a vector of dimension $C_2 = 3$. The input tensor thus has a shape of $(L, 3)$.\n\nThe first layer is a $1$-dimensional convolution with the following parameters:\n- Kernel width, $k=5$\n- Stride, $S=1$\n- Number of filters (output channels), $F=64$\n- Padding, $P=0$\n\nThe weights of a single filter in a $1$D convolution have dimensions (kernel width, input channels). Since there are $F$ such filters, the total weight tensor for the layer has dimensions $(F, k, C)$, where $C$ is the number of input channels. The total number of trainable weights in the kernels (excluding biases, as specified) is the product of these dimensions: $F \\times k \\times C$.\n\nNow, let us evaluate each option.\n\n**A. Excluding biases, the total number of trainable weights in the first convolutional layer is $k \\times 20 \\times F$ for the one-hot input and $k \\times 3 \\times F$ for the property input; therefore, with $k=5$ and $F=64$, the property input uses exactly $960$ kernel weights while the one-hot input uses $6400$.**\n\nThe number of trainable weights is given by the formula $F \\times k \\times C$.\n- For the one-hot encoding, the number of input channels is $C_1 = 20$. The total number of weights is $F \\times k \\times C_1 = 64 \\times 5 \\times 20 = 320 \\times 20 = 6400$. The formula presented, $k \\times 20 \\times F$, is equivalent due to the commutativity of multiplication.\n- For the physicochemical property encoding, the number of input channels is $C_2 = 3$. The total number of weights is $F \\times k \\times C_2 = 64 \\times 5 \\times 3 = 320 \\times 3 = 960$. The formula presented, $k \\times 3 \\times F$, is also equivalent.\n\nThe statement provides the correct formulae and the correct numerical results based on the given parameters $k=5$ and $F=64$.\n**Verdict: Correct.**\n\n**B. If the three property channels $(h,q,p)$ are fed as raw numerical values on potentially different scales, the channel with the largest variance can dominate learning dynamics in the first convolutional layer; standardizing each channel to zero mean and unit variance helps mitigate this effect.**\n\nThis statement addresses a fundamental principle of training machine learning models, including neural networks. The convolution operation is a linear combination of inputs with learned weights. If the input features (here, the channels $h$, $q$, and $p$) have vastly different numerical ranges or variances, the features with larger values will have a disproportionately large effect on the output of the convolutional layer and, consequently, on the gradients calculated during backpropagation. This can cause training instability and may lead the model to focus primarily on learning from the high-variance channel, while ignoring potentially useful information in other channels. Standardizing the input features (in this case, per channel across the entire dataset) to have a mean of $0$ and a standard deviation of $1$ is a standard and highly recommended preprocessing step. This ensures all channels are on a comparable scale, which typically leads to more stable and efficient training.\n**Verdict: Correct.**\n\n**C. Because the property encoding aggregates amino acids by physicochemical similarity, a width-$5$ convolution cannot detect position-specific motifs under this encoding, whereas it can under a one-hot encoding.**\n\nA convolution operation is inherently position-specific within its receptive field. A filter of width $k=5$ applies a set of $5$ weight vectors to the $5$ consecutive input positions it covers. For the property encoding, each of the $5$ weight vectors has dimension $3$ (one weight for each of $h, q, p$). The filter learns a specific pattern of physicochemical properties across these $5$ positions. For example, it might learn to detect a region of high hydrophobicity at position $1$ of the window, followed by a positive charge at position $3$. This is a \"position-specific motif\" of physicochemical properties.\n\nThe claim that it \"cannot detect\" such motifs is false. What is true is that this encoding makes it impossible to distinguish between two different amino acids if they happen to have identical or very similar physicochemical vectors. A one-hot encoding, being a unique identifier for each amino acid, does not have this ambiguity and allows a filter to learn motifs based on specific amino acid identities (e.g., 'Alanine' at position $1$, not just 'a small hydrophobic amino acid'). However, the inability to distinguish certain amino acids is not equivalent to an inability to detect position-specific patterns. The convolution operator, by its very nature, detects local, position-specific patterns in its input.\n**Verdict: Incorrect.**\n\n**D. For a fixed set of learned convolution weights, permuting the order of the three property channels (for example, swapping $h$ and $p$) leaves the convolution outputs unchanged.**\n\nLet the input vector at sequence position $j$ be $\\mathbf{x}_j = [x_{j,h}, x_{j,q}, x_{j,p}]^T$, representing the $(h,q,p)$ values. Let a single filter's weights for a position $m$ within its kernel ($m \\in \\{0, 1, ..., 4\\}$) be $\\mathbf{w}_m = [w_{m,h}, w_{m,q}, w_{m,p}]^T$. The contribution to the convolution output from position $j=i+m$ in the sequence is the dot product $\\mathbf{w}_m \\cdot \\mathbf{x}_j = w_{m,h}x_{j,h} + w_{m,q}x_{j,q} + w_{m,p}x_{j,p}$. The total output for one filter at position $i$ is the sum of these dot products over the kernel window: $y_i = \\sum_{m=0}^{4} \\mathbf{w}_m \\cdot \\mathbf{x}_{i+m}$.\n\nIf we permute the input channels, for example swapping $h$ and $p$, the new input vector is $\\mathbf{x}'_j = [x_{j,p}, x_{j,q}, x_{j,h}]^T$. The new output, using the same fixed weights $\\mathbf{w}_m$, will have contributions of the form $\\mathbf{w}_m \\cdot \\mathbf{x}'_j = w_{m,h}x_{j,p} + w_{m,q}x_{j,q} + w_{m,p}x_{j,h}$.\nIn general, $w_{m,h}x_{j,h} + w_{m,p}x_{j,p} \\neq w_{m,h}x_{j,p} + w_{m,p}x_{j,h}$.\nThe output will remain unchanged only under the trivial conditions that $x_{j,h} = x_{j,p}$ for all relevant $j$, or if the learned weights have a specific symmetry $w_{m,h} = w_{m,p}$ for all $m$. Neither of these can be assumed. The convolution operation is not invariant to the permutation of its input channels because each channel is processed by a distinct set of weights.\n**Verdict: Incorrect.**\n\n**E. Compared to a one-hot encoding, the property encoding can allow a single learned filter to respond similarly to unseen amino acid substitutions that preserve physicochemical similarity, potentially improving generalization to variants not seen during training.**\n\nThis statement describes the concept of inductive bias introduced by the choice of encoding. A one-hot encoding represents each amino acid as an orthogonal vector. The model has no a priori information about which amino acids are similar. For example, the biochemically similar amino acids Leucine (L) and Isoleucine (I) are as distinct as Leucine (L) and Aspartic Acid (D) in this representation. A filter trained to recognize a motif containing L will not automatically respond to the same motif with I substituted for L.\n\nIn contrast, the physicochemical property encoding is a dense representation where similar amino acids are mapped to nearby points in the $\\mathbb{R}^3$ space. Leucine and Isoleucine would have very similar $(h,q,p)$ vectors. A filter that learns to recognize a pattern of properties (e.g., \"highly hydrophobic residue\") will respond similarly to both L and I in that position. This enables the model to generalize. If the training data contains a functional site with Leucine, the model can learn its properties and correctly predict that a variant with Isoleucine, which was not seen in training, is also a functional site. This is a key motivation for using feature-based encodings and can improve a model's performance on new, unseen data.\n**Verdict: Correct.**", "answer": "$$\\boxed{ABE}$$", "id": "2382354"}]}