{"hands_on_practices": [{"introduction": "Building a reliable scaffold requires synthesizing information from multiple, sometimes conflicting, data sources. A powerful way to formally weigh this evidence is through a Bayesian framework. This exercise guides you through creating such a model to calculate the posterior probability that a proposed link between two contigs is correct, integrating evidence from diverse experimental methods like mate-pairs, optical mapping, and Hi-C. [@problem_id:2427655]", "problem": "You are given a formal probabilistic model for evaluating whether a proposed scaffold link between two contigs is correct based on multiple, independent experimental evidence types commonly used in genome assembly: mate-pair libraries, optical mapping, and High-throughput Chromosome Conformation Capture (Hi-C). Let the binary event $L \\in \\{0,1\\}$ denote whether the proposed link is correct ($L=1$) or incorrect ($L=0$). For each data type $d$ in the set $\\mathcal{D} = \\{\\text{MP}, \\text{OM}, \\text{HC}\\}$, an observation $x_d \\in \\{+1,-1,0\\}$ is recorded, where $+1$ denotes supportive evidence (consistent with the link), $-1$ denotes contradictory evidence (inconsistent with the link), and $0$ denotes missing or uninformative evidence. Assume conditional independence of observations given $L$.\n\nFor each data type $d \\in \\mathcal{D}$, you are given the conditional probabilities $P(x_d=+1 \\mid L=1)$ and $P(x_d=+1 \\mid L=0)$ as model parameters. When $x_d=-1$, use $P(x_d=-1 \\mid L=1)=1-P(x_d=+1 \\mid L=1)$ and $P(x_d=-1 \\mid L=0)=1-P(x_d=+1 \\mid L=0)$. When $x_d=0$, the observation should not change the posterior.\n\nYour task is to compute the posterior probability $P(L=1 \\mid \\mathbf{x})$ from first principles, where $\\mathbf{x}=(x_{\\text{MP}},x_{\\text{OM}},x_{\\text{HC}})$, a prior $P(L=1)=p_0$ is specified, and conditional independence implies\n$$\nP(\\mathbf{x}\\mid L=\\ell) \\;=\\; \\prod_{d\\in\\mathcal{D}} P(x_d\\mid L=\\ell), \\quad \\ell\\in\\{0,1\\}.\n$$\nUse Bayes' rule:\n$$\nP(L=1\\mid \\mathbf{x}) \\;=\\; \\frac{p_0 \\cdot \\prod_{d\\in\\mathcal{D}} P(x_d\\mid L=1)}{p_0 \\cdot \\prod_{d\\in\\mathcal{D}} P(x_d\\mid L=1) + (1-p_0) \\cdot \\prod_{d\\in\\mathcal{D}} P(x_d\\mid L=0)}.\n$$\n\nAll probabilities must be treated as decimals in $[0,1]$.\n\nUse the following fixed model parameters for all test cases:\n- Mate-pair (MP): $P(x_{\\text{MP}}=+1 \\mid L=1)=0.9$, $P(x_{\\text{MP}}=+1 \\mid L=0)=0.1$.\n- Optical map (OM): $P(x_{\\text{OM}}=+1 \\mid L=1)=0.95$, $P(x_{\\text{OM}}=+1 \\mid L=0)=0.05$.\n- High-throughput Chromosome Conformation Capture (Hi-C) (HC): $P(x_{\\text{HC}}=+1 \\mid L=1)=0.85$, $P(x_{\\text{HC}}=+1 \\mid L=0)=0.2$.\n\nImplement a program that computes $P(L=1\\mid \\mathbf{x})$ for each of the following test cases, rounding each result to $6$ decimal places:\n- Case $1$: $p_0=0.5$, $x_{\\text{MP}}=+1$, $x_{\\text{OM}}=+1$, $x_{\\text{HC}}=+1$.\n- Case $2$: $p_0=0.5$, $x_{\\text{MP}}=+1$, $x_{\\text{OM}}=-1$, $x_{\\text{HC}}=+1$.\n- Case $3$: $p_0=0.01$, $x_{\\text{MP}}=+1$, $x_{\\text{OM}}=+1$, $x_{\\text{HC}}=+1$.\n- Case $4$: $p_0=0.9$, $x_{\\text{MP}}=-1$, $x_{\\text{OM}}=-1$, $x_{\\text{HC}}=-1$.\n- Case $5$: $p_0=0.2$, $x_{\\text{MP}}=+1$, $x_{\\text{OM}}=0$, $x_{\\text{HC}}=-1$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each value rounded to $6$ decimal places (e.g., $[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4,\\text{result}_5]$). No physical units are involved, and no angles are involved.", "solution": "The problem as presented is scientifically sound and computationally well-posed. It requires the application of fundamental principles of probability theory, specifically Bayes' rule, to a problem in bioinformatics. We shall proceed with a solution derived from first principles.\n\nThe objective is to compute the posterior probability $P(L=1 \\mid \\mathbf{x})$ that a scaffold link is correct ($L=1$) given a vector of observations $\\mathbf{x} = (x_{\\text{MP}}, x_{\\text{OM}}, x_{\\text{HC}})$. The prior probability of a correct link is given as $P(L=1) = p_0$.\n\nAccording to Bayes' rule, the posterior probability is given by:\n$$\nP(L=1 \\mid \\mathbf{x}) = \\frac{P(\\mathbf{x} \\mid L=1) P(L=1)}{P(\\mathbf{x})}\n$$\nThe term in the denominator, $P(\\mathbf{x})$, is the marginal probability of the evidence, which is calculated using the law of total probability:\n$$\nP(\\mathbf{x}) = P(\\mathbf{x} \\mid L=1) P(L=1) + P(\\mathbf{x} \\mid L=0) P(L=0)\n$$\nGiven that $P(L=0) = 1 - P(L=1) = 1 - p_0$, we arrive at the formula provided in the problem statement:\n$$\nP(L=1\\mid \\mathbf{x}) = \\frac{p_0 \\cdot P(\\mathbf{x}\\mid L=1)}{p_0 \\cdot P(\\mathbf{x}\\mid L=1) + (1-p_0) \\cdot P(\\mathbf{x}\\mid L=0)}\n$$\nThe problem states that the observations are conditionally independent given the true state of the link, $L$. Therefore, the likelihood function $P(\\mathbf{x}\\mid L=\\ell)$ for $\\ell \\in \\{0, 1\\}$ is the product of the individual conditional probabilities:\n$$\nP(\\mathbf{x}\\mid L=\\ell) = \\prod_{d\\in\\mathcal{D}} P(x_d\\mid L=\\ell)\n$$\nwhere $\\mathcal{D} = \\{\\text{MP}, \\text{OM}, \\text{HC}\\}$.\n\nThe conditional probabilities for each data type $d \\in \\mathcal{D}$ and observation $x_d \\in \\{+1, -1, 0\\}$ are defined as follows. The parameters for supportive evidence ($x_d=+1$) are given:\n- Mate-pair (MP): $P(x_{\\text{MP}}=+1 \\mid L=1) = 0.9$, $P(x_{\\text{MP}}=+1 \\mid L=0) = 0.1$.\n- Optical map (OM): $P(x_{\\text{OM}}=+1 \\mid L=1) = 0.95$, $P(x_{\\text{OM}}=+1 \\mid L=0) = 0.05$.\n- Hi-C (HC): $P(x_{\\text{HC}}=+1 \\mid L=1) = 0.85$, $P(x_{\\text{HC}}=+1 \\mid L=0) = 0.2$.\n\nFor contradictory evidence ($x_d=-1$), the probabilities are derived as:\n$$\nP(x_d = -1 \\mid L=\\ell) = 1 - P(x_d = +1 \\mid L=\\ell)\n$$\nThis gives:\n- MP: $P(x_{\\text{MP}}=-1 \\mid L=1) = 0.1$, $P(x_{\\text{MP}}=-1 \\mid L=0) = 0.9$.\n- OM: $P(x_{\\text{OM}}=-1 \\mid L=1) = 0.05$, $P(x_{\\text{OM}}=-1 \\mid L=0) = 0.95$.\n- HC: $P(x_{\\text{HC}}=-1 \\mid L=1) = 0.15$, $P(x_{\\text{HC}}=-1 \\mid L=0) = 0.8$.\n\nFor missing evidence ($x_d=0$), the observation must not change the posterior distribution. This is achieved by setting the likelihood ratio for this observation to $1$. In our calculation, this is equivalent to setting $P(x_d=0 \\mid L=\\ell) = 1$ for both $\\ell=1$ and $\\ell=0$, effectively removing the term from the product.\n\nWe now compute the posterior probability for each test case.\n\n**Case 1:** $p_0=0.5$, $\\mathbf{x}=(x_{\\text{MP}}=+1, x_{\\text{OM}}=+1, x_{\\text{HC}}=+1)$.\n- Likelihood for $L=1$: $P(\\mathbf{x} \\mid L=1) = 0.9 \\times 0.95 \\times 0.85 = 0.72675$.\n- Likelihood for $L=0$: $P(\\mathbf{x} \\mid L=0) = 0.1 \\times 0.05 \\times 0.2 = 0.001$.\n- Posterior: $P(L=1 \\mid \\mathbf{x}) = \\frac{0.5 \\times 0.72675}{0.5 \\times 0.72675 + (1-0.5) \\times 0.001} = \\frac{0.363375}{0.363375 + 0.0005} = \\frac{0.363375}{0.363875} \\approx 0.998626$.\n\n**Case 2:** $p_0=0.5$, $\\mathbf{x}=(x_{\\text{MP}}=+1, x_{\\text{OM}}=-1, x_{\\text{HC}}=+1)$.\n- Likelihood for $L=1$: $P(\\mathbf{x} \\mid L=1) = 0.9 \\times (1-0.95) \\times 0.85 = 0.9 \\times 0.05 \\times 0.85 = 0.03825$.\n- Likelihood for $L=0$: $P(\\mathbf{x} \\mid L=0) = 0.1 \\times (1-0.05) \\times 0.2 = 0.1 \\times 0.95 \\times 0.2 = 0.019$.\n- Posterior: $P(L=1 \\mid \\mathbf{x}) = \\frac{0.5 \\times 0.03825}{0.5 \\times 0.03825 + (1-0.5) \\times 0.019} = \\frac{0.019125}{0.019125 + 0.0095} = \\frac{0.019125}{0.028625} \\approx 0.668122$.\n\n**Case 3:** $p_0=0.01$, $\\mathbf{x}=(x_{\\text{MP}}=+1, x_{\\text{OM}}=+1, x_{\\text{HC}}=+1)$.\n- Likelihoods are as in Case $1$: $P(\\mathbf{x} \\mid L=1) = 0.72675$, $P(\\mathbf{x} \\mid L=0) = 0.001$.\n- Posterior: $P(L=1 \\mid \\mathbf{x}) = \\frac{0.01 \\times 0.72675}{0.01 \\times 0.72675 + (1-0.01) \\times 0.001} = \\frac{0.0072675}{0.0072675 + 0.00099} = \\frac{0.0072675}{0.0082575} \\approx 0.880115$.\n\n**Case 4:** $p_0=0.9$, $\\mathbf{x}=(x_{\\text{MP}}=-1, x_{\\text{OM}}=-1, x_{\\text{HC}}=-1)$.\n- Likelihood for $L=1$: $P(\\mathbf{x} \\mid L=1) = (1-0.9) \\times (1-0.95) \\times (1-0.85) = 0.1 \\times 0.05 \\times 0.15 = 0.00075$.\n- Likelihood for $L=0$: $P(\\mathbf{x} \\mid L=0) = (1-0.1) \\times (1-0.05) \\times (1-0.2) = 0.9 \\times 0.95 \\times 0.8 = 0.684$.\n- Posterior: $P(L=1 \\mid \\mathbf{x}) = \\frac{0.9 \\times 0.00075}{0.9 \\times 0.00075 + (1-0.9) \\times 0.684} = \\frac{0.000675}{0.000675 + 0.0684} = \\frac{0.000675}{0.069075} \\approx 0.009772$.\n\n**Case 5:** $p_0=0.2$, $\\mathbf{x}=(x_{\\text{MP}}=+1, x_{\\text{OM}}=0, x_{\\text{HC}}=-1)$.\n- Likelihood for $L=1$: $P(\\mathbf{x} \\mid L=1) = 0.9 \\times 1 \\times (1-0.85) = 0.9 \\times 0.15 = 0.135$.\n- Likelihood for $L=0$: $P(\\mathbf{x} \\mid L=0) = 0.1 \\times 1 \\times (1-0.2) = 0.1 \\times 0.8 = 0.08$.\n- Posterior: $P(L=1 \\mid \\mathbf{x}) = \\frac{0.2 \\times 0.135}{0.2 \\times 0.135 + (1-0.2) \\times 0.08} = \\frac{0.027}{0.027 + 0.064} = \\frac{0.027}{0.091} \\approx 0.296703$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the posterior probability for scaffold link correctness\n    based on a Bayesian model with multiple evidence types.\n    \"\"\"\n\n    # Fixed model parameters\n    # For each data type d, we store (P(x_d=+1|L=1), P(x_d=+1|L=0))\n    model_params = {\n        \"MP\": (0.9, 0.1),\n        \"OM\": (0.95, 0.05),\n        \"HC\": (0.85, 0.2)\n    }\n\n    # Test cases from the problem statement\n    # Each case is a tuple: (p0, (x_mp, x_om, x_hc))\n    test_cases = [\n        (0.5, (1, 1, 1)),   # Case 1\n        (0.5, (1, -1, 1)),  # Case 2\n        (0.01, (1, 1, 1)),  # Case 3\n        (0.9, (-1, -1, -1)),# Case 4\n        (0.2, (1, 0, -1))   # Case 5\n    ]\n    \n    data_types = [\"MP\", \"OM\", \"HC\"]\n    results = []\n\n    for case in test_cases:\n        p0, observations = case\n        \n        # Initialize likelihoods for L=1 and L=0\n        lik_L1 = 1.0\n        lik_L0 = 1.0\n\n        for i, obs in enumerate(observations):\n            data_type = data_types[i]\n            p_plus_1_given_L1, p_plus_1_given_L0 = model_params[data_type]\n            \n            if obs == 1:\n                # Supportive evidence\n                lik_L1 *= p_plus_1_given_L1\n                lik_L0 *= p_plus_1_given_L0\n            elif obs == -1:\n                # Contradictory evidence\n                # P(x_d=-1|L) = 1 - P(x_d=+1|L)\n                lik_L1 *= (1.0 - p_plus_1_given_L1)\n                lik_L0 *= (1.0 - p_plus_1_given_L0)\n            elif obs == 0:\n                # Missing or uninformative evidence\n                # Likelihood ratio is 1, so we multiply by 1 (i.e., do nothing)\n                pass\n\n        # Calculate numerator and denominator of Bayes' rule\n        numerator = p0 * lik_L1\n        denominator = numerator + (1.0 - p0) * lik_L0\n        \n        if denominator == 0:\n            # This case is not expected with the given parameters but is good practice\n            posterior = 0.0\n        else:\n            posterior = numerator / denominator\n        \n        results.append(posterior)\n\n    # Format the results as a comma-separated string, rounded to 6 decimal places\n    output_str = f\"[{','.join([f'{r:.6f}' for r in results])}]\"\n    print(output_str)\n\nsolve()\n```", "id": "2427655"}, {"introduction": "Once contigs are ordered into a scaffold, the next challenge is to fill the sequence gaps between them. This gap-filling process can be elegantly modeled as a pathfinding problem on a specialized graph built from the reads that span the gap. In this practice, you will tackle this by finding the most confident path through a de Bruijn graph, where confidence is defined probabilistically based on the read evidence for each connection. [@problem_id:2427629]", "problem": "You are given a finite set of unmapped sequencing reads over the nucleotide alphabet and a target gap defined by a start node and an end node in a de Bruijn graph of order $k$. Model the confidence of a path that bridges the gap as the product of smoothed transition probabilities along the path. Formally, let $\\Sigma=\\{A,C,G,T\\}$ and let the de Bruijn graph $G_k$ be constructed from the given reads as follows. For each read $r$ and each contiguous substring of length $k$ within $r$, denoted $x_1 x_2 \\dots x_k$, add a directed edge from node $u = x_1 x_2 \\dots x_{k-1}$ to node $v = x_2 x_3 \\dots x_k$. The count $c(u \\to v)$ is the number of times this $k$-mer occurs across all reads. For a given smoothing parameter $\\alpha > 0$, define the outdegree $d(u)$ as the number of distinct outgoing edges from $u$ and define the smoothed transition probability from $u$ to $v$ by\n$$\nP(u \\to v) = \\frac{c(u \\to v) + \\alpha}{\\sum_{v'} c(u \\to v') + \\alpha \\cdot d(u)} \\quad \\text{over observed outgoing neighbors } v'.\n$$\nA path $u_0 \\to u_1 \\to \\dots \\to u_m$ has confidence\n$$\n\\mathrm{Conf} = \\prod_{i=0}^{m-1} P(u_i \\to u_{i+1}).\n$$\nLet the start node be $s$ and the end node be $t$, both strings of length $k-1$. Let the maximum allowed number of edges be $L \\in \\mathbb{N}$. Define the optimal confidence for the gap fill as the maximum of $\\mathrm{Conf}$ over all directed paths from $s$ to $t$ that use at most $L$ edges; if $m=0$ and $s=t$, define $\\mathrm{Conf}=1$. If no such path exists (with $m \\ge 1$ when $s \\ne t$), define the optimal confidence to be $0$.\n\nYour program must, for each specified test case, compute the natural logarithm (base $e$) of the optimal confidence, that is,\n$$\n\\log \\mathrm{Conf}^* = \\max_{\\substack{s \\to t \\text{ paths} \\\\ \\text{with } m \\le L}} \\sum_{i=0}^{m-1} \\log P(u_i \\to u_{i+1}),\n$$\nwith the convention that if no path exists and $s \\ne t$, output $-\\infty$, and if $s=t$ and $L \\ge 0$, the empty path is permissible with $\\log \\mathrm{Conf}^* = 0$. All logarithms must be natural logarithms. The required numeric outputs must be rounded to $6$ decimal places (if finite). There are no physical units involved. Angles are not used. Do not use percentages.\n\nTest Suite. For each test case, the parameters are $(\\text{reads}, k, \\alpha, s, t, L)$, where reads is a finite list of strings over $\\Sigma$.\n\n- Test case $1$ (happy path, unique chain):\n  - reads $= [\\text{ATG}, \\text{TGA}, \\text{TGA}, \\text{GAC}]$\n  - $k = 3$\n  - $\\alpha = 0.5$\n  - $s = \\text{AT}$\n  - $t = \\text{AC}$\n  - $L = 3$\n\n- Test case $2$ (branching with unequal support):\n  - reads $= [\\text{ATG}, \\text{ATG}, \\text{ATA}, \\text{TGA}, \\text{TGC}, \\text{GCA}]$\n  - $k = 3$\n  - $\\alpha = 0.5$\n  - $s = \\text{AT}$\n  - $t = \\text{GA}$\n  - $L = 2$\n\n- Test case $3$ (no path exists):\n  - reads $= [\\text{AAA}, \\text{AAT}, \\text{ATC}]$\n  - $k = 3$\n  - $\\alpha = 0.5$\n  - $s = \\text{GG}$\n  - $t = \\text{TT}$\n  - $L = 5$\n\n- Test case $4$ (boundary condition $L=0$ and $s=t$):\n  - reads $= [\\text{AAA}, \\text{AAT}, \\text{ATC}]$\n  - $k = 3$\n  - $\\alpha = 0.5$\n  - $s = \\text{AT}$\n  - $t = \\text{AT}$\n  - $L = 0$\n\nFinal Output Format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the same order as the test cases. Each entry must be a floating-point number equal to $\\log \\mathrm{Conf}^*$ for that test case, rounded to $6$ decimal places if finite, or $-\\infty$ if no path exists. For example, an output with four results must look like $[\\text{r}_1,\\text{r}_2,\\text{r}_3,\\text{r}_4]$.", "solution": "The problem presented is a well-defined exercise in optimization on a weighted, directed graph. It models a simplified, yet conceptually valid, approach to a common problem in computational genomics: the filling of gaps in a draft genome assembly. The confidence model, based on smoothed transition probabilities within a de Bruijn graph, is statistically sound. The objective is to find a path of maximum confidence between two specified nodes, subject to a constraint on the path length. This is equivalent to finding a longest path where edge weights are defined as the logarithm of transition probabilities. The problem contains no logical contradictions, scientific fallacies, or ambiguities. Therefore, we shall proceed with its formal solution.\n\nThe core of the problem is to find the path with the maximum cumulative log-probability. Let the graph be $G = (V, E)$, where $V$ is the set of $(k-1)$-mers derived from the reads and an edge $(u, v) \\in E$ exists if the $k$-mer corresponding to the transition from $u$ to $v$ is observed. The weight of an edge $(u, v)$ is given by $w(u, v) = \\log P(u \\to v)$. The problem is then to find:\n$$\n\\log \\mathrm{Conf}^* = \\max_{\\substack{\\pi = u_0 \\to \\dots \\to u_m \\\\ u_0 = s, u_m = t \\\\ m \\le L}} \\sum_{i=0}^{m-1} w(u_i, u_{i+1})\n$$\n\nThis is a variant of the longest path problem, constrained by the number of edges. Such problems are correctly solved using dynamic programming. A method analogous to the Bellman-Ford algorithm is suitable.\n\nThe solution is constructed via the following steps:\n\n1.  **De Bruijn Graph Construction**\n    First, we must construct the de Bruijn graph from the input reads. The nodes of the graph are unique substrings of length $k-1$ (termed $(k-1)$-mers). An edge exists from node $u$ to node $v$ if a $k$-mer exists in the reads whose first $k-1$ characters form $u$ and whose last $k-1$ characters form $v$. We process all reads to find all constituent $k$-mers and their frequencies. Let $c(u \\to v)$ be the count of the $k$-mer that corresponds to the transition from node $u$ to node $v$. These counts are stored, for instance, in a hash map data structure mapping a pair of nodes $(u, v)$ to its count.\n\n2.  **Edge Weight Calculation**\n    With the graph structure and edge counts established, we calculate the weight for each edge. For each node $u$ that is a source of at least one edge, we compute two quantities:\n    - The out-degree, $d(u)$, which is the number of distinct nodes $v'$ reachable from $u$ in one step.\n    - The total outgoing count, $\\sum_{v'} c(u \\to v')$, summed over all observed neighbors $v'$ of $u$.\n\n    The smoothed transition probability $P(u \\to v)$ is then calculated for each edge $(u, v)$ according to the provided formula:\n    $$\n    P(u \\to v) = \\frac{c(u \\to v) + \\alpha}{\\sum_{v'} c(u \\to v') + \\alpha \\cdot d(u)}\n    $$\n    The weight of the edge $(u, v)$ is the natural logarithm of this probability, $w(u, v) = \\log P(u \\to v)$. Since $P(u \\to v)$ may be less than $1$, its logarithm will be non-positive. This guarantees that there are no positive cycles, which simplifies the longest path problem.\n\n3.  **Dynamic Programming for Longest Path**\n    We define a dynamic programming state, $D(l, v)$, as the maximum log-confidence of a path from the start node $s$ to an arbitrary node $v$ using *exactly* $l$ edges.\n    The state space is indexed by path length $l \\in [0, L]$ and node $v \\in V$. We can map the string representation of each node to a unique integer index for efficient access in an array-based implementation.\n\n    The DP formulation is as follows:\n    - **Base Case:** For paths of length $l=0$, the only node that can be \"reached\" is the start node $s$ itself (via an empty path).\n      $$ D(0, v) = \\begin{cases} 0 & \\text{if } v = s \\\\ -\\infty & \\text{if } v \\neq s \\end{cases} $$\n    - **Recurrence Relation:** For $l$ from $1$ to $L$, we update the log-confidence for reaching each node $v$ by considering all incoming edges $(u, v)$.\n      $$\n      D(l, v) = \\max_{u \\text{ such that } (u,v) \\in E} \\{ D(l-1, u) + w(u, v) \\}\n      $$\n      If no path of length $l-1$ reaches any predecessor $u$ of $v$, or if $v$ has no predecessors, $D(l, v)$ remains $-\\infty$.\n\n4.  **Final Answer Extraction**\n    After populating the DP table up to $l=L$, the optimal log-confidence for a path from $s$ to the target node $t$ using *at most* $L$ edges is the maximum value found in the entries for node $t$ across all possible path lengths.\n    $$\n    \\max_{0 \\le l \\le L} D(l, t)\n    $$\n    We must adhere to the specific problem conventions:\n    - If $s = t$, a zero-length path is permitted, with a log-confidence of $0$. Therefore, the answer must be at least $0$. The final result is $\\max(0, \\max_{1 \\le l \\le L} D(l, t))$.\n    - If no path from $s$ to $t$ exists within $L$ edges (i.e., all $D(l, t)$ for $l \\geq 1$ remain $-\\infty$), and $s \\neq t$, the result is $-\\infty$.\n    - If either $s$ or $t$ are not present in the graph constructed from the reads, no path can exist. The result is $0$ if $s=t$ and $-\\infty$ otherwise.\n\nThis completes the logical design of the algorithm. The implementation will systematically follow these steps.", "answer": "```python\nimport numpy as np\nfrom collections import defaultdict\n\ndef format_result(val):\n    \"\"\"Formats a number for the final output string.\"\"\"\n    if val == float('-inf'):\n        return '-inf'\n    return f\"{val:.6f}\"\n\ndef calculate_best_log_confidence(reads, k, alpha, s, t, L):\n    \"\"\"\n    Computes the optimal log confidence for a gap fill.\n    \"\"\"\n    # Step 1: Graph Construction\n    kmer_counts = defaultdict(int)\n    for read in reads:\n        if len(read) >= k:\n            for i in range(len(read) - k + 1):\n                kmer = read[i:i+k]\n                kmer_counts[kmer] += 1\n\n    # Adjacency list representation: graph[u] = {v1: count, v2: count}\n    graph = defaultdict(lambda: defaultdict(int))\n    nodes = set()\n    if k <= 1: # Nodes must have length k-1 >= 1\n        return 0.0 if s == t else float('-inf')\n\n    for kmer, count in kmer_counts.items():\n        u, v = kmer[:-1], kmer[1:]\n        graph[u][v] += count\n        nodes.add(u)\n        nodes.add(v)\n    \n    if not nodes:\n        return 0.0 if s == t else float('-inf')\n\n    node_list = sorted(list(nodes))\n    node_to_idx = {node: i for i, node in enumerate(node_list)}\n    num_nodes = len(node_list)\n\n    # Step 2: Edge Weight Calculation\n    weights = {}\n    for u, neighbors in graph.items():\n        if not neighbors:\n            continue\n        d_u = len(neighbors)\n        sum_c_u = sum(neighbors.values())\n        denominator = sum_c_u + alpha * d_u\n        for v, c_uv in neighbors.items():\n            numerator = c_uv + alpha\n            prob = numerator / denominator\n            weights[(u, v)] = np.log(prob)\n\n    # Step 3: Dynamic Programming\n    dp = np.full((L + 1, num_nodes), float('-inf'))\n\n    if s in node_to_idx:\n        s_idx = node_to_idx[s]\n        dp[0][s_idx] = 0.0\n\n    for l in range(1, L + 1):\n        for u_str, u_neighbors in graph.items():\n            u_idx = node_to_idx[u_str]\n            if dp[l-1][u_idx] == float('-inf'):\n                continue\n            for v_str in u_neighbors:\n                v_idx = node_to_idx[v_str]\n                weight = weights[(u_str, v_str)]\n                new_log_conf = dp[l-1][u_idx] + weight\n                dp[l][v_idx] = max(dp[l][v_idx], new_log_conf)\n\n    # Step 4: Final Answer Extraction\n    if t not in node_to_idx:\n        return 0.0 if s == t else float('-inf')\n        \n    t_idx = node_to_idx[t]\n    \n    # max log-confidence over paths of length 1 to L\n    max_log_conf = np.max(dp[1:, t_idx]) if L > 0 else float('-inf')\n\n    if s == t:\n        # A path of length 0 (log-conf 0) is also an option.\n        return max(0.0, max_log_conf)\n    else:\n        return max_log_conf\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and print results.\n    \"\"\"\n    test_cases = [\n        # Test case 1 (happy path, unique chain)\n        {'reads': ['ATG', 'TGA', 'TGA', 'GAC'], 'k': 3, 'alpha': 0.5, 's': 'AT', 't': 'AC', 'L': 3},\n        # Test case 2 (branching with unequal support)\n        {'reads': ['ATG', 'ATG', 'ATA', 'TGA', 'TGC', 'GCA'], 'k': 3, 'alpha': 0.5, 's': 'AT', 't': 'GA', 'L': 2},\n        # Test case 3 (no path exists)\n        {'reads': ['AAA', 'AAT', 'ATC'], 'k': 3, 'alpha': 0.5, 's': 'GG', 't': 'TT', 'L': 5},\n        # Test case 4 (boundary condition L=0 and s=t)\n        {'reads': ['AAA', 'AAT', 'ATC'], 'k': 3, 'alpha': 0.5, 's': 'AT', 't': 'AT', 'L': 0},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = calculate_best_log_confidence(\n            case['reads'], case['k'], case['alpha'], case['s'], case['t'], case['L']\n        )\n        results.append(result)\n\n    formatted_results = [format_result(r) for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2427629"}, {"introduction": "The final phase of genome assembly, known as finishing, is dedicated to ensuring the highest possible accuracy and contiguity. A crucial part of this is identifying and correcting structural mis-assemblies, where parts of the genome are incorrectly joined. This hands-on problem demonstrates a statistical method for detecting such errors by analyzing read alignment patterns, specifically by identifying regions with an abnormally high number of 'soft-clipped' reads. [@problem_id:2427648]", "problem": "You are given a simplified abstraction of read-to-assembly alignments for a finished genome assembly. For each assembly position indexed by an integer $i \\in \\{0,1,\\dots,L-1\\}$, you are provided two nonnegative integers: $n_i$, the total number of reads that overlap position $i$, and $c_i$, the number of those reads that require a soft-clip at position $i$ in order to align. Assume that, in correctly assembled regions, individual reads independently require soft-clipping at a specific position with probability $p_0$, so that $C_i \\sim \\mathrm{Binomial}(n_i,p_0)$ under a null model of correctness. For a one-sided test at significance level $\\alpha$, position $i$ is called abnormal if the upper-tail probability under the null,\n$$\n\\mathbb{P}\\left(X \\ge c_i \\mid X \\sim \\mathrm{Binomial}(n_i,p_0)\\right),\n$$\nis strictly less than $\\alpha$. A detected region is defined to be a maximal contiguous interval of abnormal positions $[a,b]$ in $0$-based indexing (inclusive), and only intervals with length at least $L_{\\min}$, that is $b-a+1 \\ge L_{\\min}$, are to be reported.\n\nYour task is to write a complete program that, for each test case provided below, determines and outputs the list of detected intervals $[a,b]$ using the definitions above. The program must treat each test case independently. Express all interval endpoints as integers, with no physical units. Angles are not involved. All probabilities must be treated as real numbers in $[0,1]$.\n\nTest suite (each test case consists of $(\\{n_i\\},\\{c_i\\},p_0,\\alpha,L_{\\min})$):\n- Test case $1$:\n  - $\\{n_i\\}_{i=0}^{19} = [100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100]$\n  - $\\{c_i\\}_{i=0}^{19} = [2,2,2,2,2,2,2,2,15,15,15,2,2,2,2,2,2,2,2,2]$\n  - $p_0 = 0.02$, $\\alpha = 10^{-6}$, $L_{\\min} = 2$\n- Test case $2$:\n  - $\\{n_i\\}_{i=0}^{6} = [20,20,100,100,100,100,100]$\n  - $\\{c_i\\}_{i=0}^{6} = [8,8,2,2,2,2,2]$\n  - $p_0 = 0.02$, $\\alpha = 10^{-6}$, $L_{\\min} = 2$\n- Test case $3$:\n  - $\\{n_i\\}_{i=0}^{7} = [5,5,5,5,5,100,100,100]$\n  - $\\{c_i\\}_{i=0}^{7} = [2,0,2,0,2,4,4,4]$\n  - $p_0 = 0.02$, $\\alpha = 10^{-3}$, $L_{\\min} = 2$\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case, and each test case is represented by a list of detected intervals, with each interval written as a two-element list $[a,b]$ using $0$-based indexing and inclusive endpoints. For example, the overall format is $[R_1,R_2,R_3]$ where each $R_j$ is a list such as $[[a_1,b_1],[a_2,b_2],\\dots]$. For instance, if the three test cases yield intervals $[[8,10]]$, $[[0,1]]$, and $[]$, then the program must print the single line $[[[8,10]],[[0,1]],[]]$.", "solution": "The solution follows a multi-step procedure for each test case, grounded in statistical hypothesis testing. The objective is to identify genomic regions where the observed number of soft-clipped reads is improbably high, suggesting a deviation from the null model of a correct assembly.\n\nFor each test case characterized by the parameters $(\\{n_i\\}_{i=0}^{L-1}, \\{c_i\\}_{i=0}^{L-1}, p_0, \\alpha, L_{\\min})$:\n\n**1. Identification of Abnormal Positions**\n\nFor each position $i$ from $0$ to $L-1$, we perform a one-sided hypothesis test. The null hypothesis, $H_0$, is that the position is correctly assembled, and the number of soft-clipped reads is a random variable $X \\sim \\mathrm{Binomial}(n_i, p_0)$. The alternative hypothesis, $H_1$, is that the number of soft-clipped reads is significantly higher than expected under $H_0$.\n\nWe calculate the p-value, which is the probability of observing a result at least as extreme as the observed count $c_i$, assuming $H_0$ is true. This corresponds to the upper-tail probability, or survival function (SF), of the binomial distribution:\n$$\nP_i = \\mathbb{P}(X \\ge c_i) = \\sum_{k=c_i}^{n_i} \\binom{n_i}{k} p_0^k (1 - p_0)^{n_i-k}\n$$\nA position $i$ is classified as \"abnormal\" if its p-value $P_i$ is strictly less than the predetermined significance level $\\alpha$. This procedure is applied to all positions, resulting in a boolean vector indicating which positions are abnormal.\n\n**2. Grouping into Contiguous Regions**\n\nThe next step is to identify all maximal contiguous intervals of abnormal positions. We iterate through the boolean vector of abnormal positions and group consecutive `True` values. For example, if positions $i, i+1, \\dots, j$ are all abnormal, but $i-1$ and $j+1$ are not (or are out of bounds), then $[i, j]$ constitutes a maximal contiguous region.\n\n**3. Filtering by Minimum Length**\n\nFinally, each identified region $[a, b]$ is subjected to a length filter. The length of the interval is calculated as $b - a + 1$. Only regions for which the length is greater than or equal to the specified minimum length, $L_{\\min}$, are retained and reported as detected regions.\n\nThis complete algorithm is applied independently to each test case. The use of a computational library with robust implementations of statistical functions, such as `scipy.stats`, is essential for accurate calculation of the binomial survival function, especially for large $n_i$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import binom\n\ndef solve():\n    \"\"\"\n    Solves the problem for all test cases as specified.\n    \"\"\"\n\n    test_cases = [\n        # Test Case 1:\n        (\n            np.array([100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100], dtype=np.int64),\n            np.array([2, 2, 2, 2, 2, 2, 2, 2, 15, 15, 15, 2, 2, 2, 2, 2, 2, 2, 2, 2], dtype=np.int64),\n            0.02,\n            1e-6,\n            2\n        ),\n        # Test Case 2:\n        (\n            np.array([20, 20, 100, 100, 100, 100, 100], dtype=np.int64),\n            np.array([8, 8, 2, 2, 2, 2, 2], dtype=np.int64),\n            0.02,\n            1e-6,\n            2\n        ),\n        # Test Case 3:\n        (\n            np.array([5, 5, 5, 5, 5, 100, 100, 100], dtype=np.int64),\n            np.array([2, 0, 2, 0, 2, 4, 4, 4], dtype=np.int64),\n            0.02,\n            1e-3,\n            2\n        ),\n    ]\n\n    all_results = []\n    for case in test_cases:\n        n_i, c_i, p_0, alpha, L_min = case\n        \n        # Step 1: Identify abnormal positions\n        # Calculate P(X >= c_i) for each position.\n        # This is the survival function P(X > k) where k = c_i - 1.\n        p_values = binom.sf(c_i - 1, n_i, p_0)\n        is_abnormal = p_values < alpha\n        \n        # Step 2: Group into contiguous regions\n        candidate_regions = []\n        in_region = False\n        start_index = -1\n        \n        for i, abnormal_flag in enumerate(is_abnormal):\n            if abnormal_flag and not in_region:\n                # Start of a new potential region\n                in_region = True\n                start_index = i\n            elif not abnormal_flag and in_region:\n                # End of the current region\n                in_region = False\n                end_index = i - 1\n                candidate_regions.append([start_index, end_index])\n        \n        # Handle the case where the sequence ends with an abnormal region\n        if in_region:\n            candidate_regions.append([start_index, len(is_abnormal) - 1])\n            \n        # Step 3: Filter regions by minimum length\n        detected_regions = []\n        for region in candidate_regions:\n            start, end = region\n            length = end - start + 1\n            if length >= L_min:\n                detected_regions.append(region)\n                \n        all_results.append(detected_regions)\n\n    # Convert results to the required string format\n    # Example: [[[8,10]],[[0,1]],[]]\n    result_str = str(all_results).replace(\" \", \"\")\n    print(result_str)\n\nsolve()\n```", "id": "2427648"}]}