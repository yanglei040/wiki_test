{"hands_on_practices": [{"introduction": "Assessing the quality of a genome assembly begins with measuring its contiguity. The $N50$ statistic is one of the most widely used metrics for this purpose, providing a single number to summarize how broken or continuous an assembly is. This first practice provides a foundational exercise in calculating the $N50$ from a set of contigs, a core skill for any bioinformatician, and then challenges you to think critically about what this number truly represents [@problem_id:2841042].", "problem": "You are evaluating a de novo genome assembly derived from Next-Generation Sequencing (NGS) reads. The assembler output a set of contigs with the following lengths in base pairs (bp): $80000$, $150000$, $30000$, $95000$, $25000$, $10000$, $160000$, $40000$, $35000$, $85000$, $185000$, $70000$, $20000$, $15000$, $55000$. Assume contigs are non-overlapping and the total assembly length is the sum of the contig lengths. The N50 statistic is defined as the contig length $L$ such that the set of contigs of length at least $L$ collectively covers at least a fraction $0.5$ of the total assembly length.\n\nCompute the N50 for this assembly. Then, based on principles of genome assembly and sequence data properties, critique whether N50 alone is sufficient to assess assembly quality, identifying at least one fundamental limitation tied to how N50 is constructed. Express your numerical answer in base pairs (bp). If your computation yields an exact integer, do not round; otherwise, round to an appropriate number of significant figures justified by the given data.", "solution": "The problem requires the computation of the N50 statistic for a given set of de novo assembly contigs and a critical evaluation of this metric. I will first validate the problem statement. The problem is scientifically grounded in the field of bioinformatics, well-posed with all necessary information provided, and stated objectively. It contains no logical contradictions or factual errors. Therefore, it is a valid problem, and I shall proceed with a rigorous solution.\n\nFirst, we must calculate the total length of the assembly. The set of contig lengths, denoted by $C$, is given as:\n$C = \\{80000, 150000, 30000, 95000, 25000, 10000, 160000, 40000, 35000, 85000, 185000, 70000, 20000, 15000, 55000\\}$.\nThe total assembly length, $S_{total}$, is the sum of the lengths of all contigs.\n$$ S_{total} = \\sum_{c_i \\in C} c_i $$\n$$ S_{total} = 80000 + 150000 + 30000 + 95000 + 25000 + 10000 + 160000 + 40000 + 35000 + 85000 + 185000 + 70000 + 20000 + 15000 + 55000 $$\nPerforming the summation:\n$$ S_{total} = 1055000 \\text{ bp} $$\n\nThe N50 statistic is defined as the length $L$ of the contig such that at least $50\\%$ of the total assembly length is contained in contigs of length $L$ or greater. We must therefore find the threshold value, which is $50\\%$ of the total length:\n$$ S_{threshold} = 0.5 \\times S_{total} = 0.5 \\times 1055000 = 527500 \\text{ bp} $$\n\nTo find the N50 value, we must sort the contig lengths in descending order and calculate their cumulative sum. The N50 is the length of the contig that, when added, causes the cumulative sum to meet or exceed $S_{threshold}$.\n\nThe sorted list of contig lengths is:\n$185000, 160000, 150000, 95000, 85000, 80000, 70000, 55000, 40000, 35000, 30000, 25000, 20000, 15000, 10000$.\n\nNow, we compute the cumulative sum:\n1. Longest contig: $185000$. The cumulative sum is $185000$. This is less than $527500$.\n2. Next contig: $160000$. The cumulative sum is $185000 + 160000 = 345000$. This is less than $527500$.\n3. Next contig: $150000$. The cumulative sum is $345000 + 150000 = 495000$. This is less than $527500$.\n4. Next contig: $95000$. The cumulative sum is $495000 + 95000 = 590000$. This value exceeds the threshold of $527500$.\n\nThe contig whose length is $95000$ bp is the one that causes the cumulative length of the largest contigs to surpass the $50\\%$ mark. Therefore, the N50 for this assembly is $95000$ bp.\n\nThe second part of the problem requires a critique of the N50 statistic. The N50 statistic, while widely used, is fundamentally insufficient as a sole measure of genome assembly quality. Its primary limitation is directly tied to its construction: N50 is a metric of **contiguity**, not **correctness**.\n\nThe calculation of N50, as demonstrated above, depends exclusively on the lengths of the contigs. It is entirely agnostic to the nucleotide sequence content or the structural accuracy of the assembly. An assembler can produce long contigs by incorrectly joining sequences from disparate parts of the genome (creating so-called chimeric contigs) or by misrepresenting structural features like inversions or translocations. Such errors would lead to an artificially inflated N50 value, suggesting a high-quality assembly, when in fact the assembly is structurally flawed. The N50 metric rewards an assembler for producing long contigs, irrespective of whether those contigs represent a biologically accurate reconstruction of the genome. A higher N50 does not guarantee a better assembly; it only guarantees a more contiguous one.\n\nTherefore, to properly assess assembly quality, N50 must be complemented by metrics that evaluate correctness and completeness. Examples include:\n- **Structural Correctness**: Aligning the raw NGS reads back to the assembly to identify regions with inconsistent read-pair mappings, which indicate potential misassemblies. For a known species, one can also compare the assembly against a high-quality reference genome to detect large-scale structural differences.\n- **Completeness**: Assessing the presence and integrity of a set of conserved, single-copy orthologous genes (e.g., using BUSCO - Benchmarking Universal Single-Copy Orthologs). A high percentage of complete genes indicates a more complete gene space representation.\n\nIn summary, N50 provides a simple, single-value summary of contig length distribution, but its inherent blindness to sequence accuracy makes it a poor standalone indicator of overall assembly quality. Its value lies in being a part of a larger suite of quality assessment tools.", "answer": "$$\\boxed{95000}$$", "id": "2841042"}, {"introduction": "While the $N50$ statistic is a useful measure of contiguity, it can sometimes be misleading. Because $N50$ is defined relative to the total length of the assembly, certain data filtering steps can artificially inflate it without any real improvement in assembly correctness. This exercise presents a carefully constructed scenario to demonstrate this statistical artifact, sharpening your critical eye for interpreting assembly quality reports [@problem_id:2373731].", "problem": "A genome assembly is represented by a multiset of contig lengths (in base pairs), listed here in non-increasing order: $1200$, $800$, $600$, $500$, and $100$ additional contigs each of length $10$. The $N_{50}$ statistic is defined as follows: sort contigs by non-increasing length, let the total assembly length be $L_{\\text{tot}}$, and find the smallest index $k$ such that the cumulative length of the first $k$ contigs is at least $0.5\\,L_{\\text{tot}}$; the $N_{50}$ is the length of the $k$-th contig. Consider filtering the assembly by removing every contig shorter than $100$ base pairs (bp), and recomputing $N_{50}$ on the filtered set. Which of the following statements are correct? Select all that apply.\n\nA. After filtering, $N_{50}$ increases from $600$ to $800$ because the reduction in total assembly length lowers the $50\\%$ cumulative threshold, causing it to be reached at an earlier (longer) contig.\n\nB. After filtering, $N_{50}$ decreases from $600$ to $500$ because removing contigs necessarily reduces assembly continuity.\n\nC. After filtering, $N_{50}$ remains $600$ because removing small contigs does not change the set or order of the largest contigs.\n\nD. After filtering, $N_{50}$ equals the mean of the two longest contigs due to halving the assembly length.\n\nE. This scenario demonstrates that $N_{50}$ can be inflated by discarding many short contigs without improving correctness: the artifact arises because $N_{50}$ is defined relative to the total assembly length.", "solution": "We begin with a critical validation of the problem statement.\n\n### Step 1: Extract Givens\nThe problem provides the following information:\n1.  **Initial Assembly:** A multiset of contig lengths consisting of:\n    *   Four large contigs with lengths $1200$, $800$, $600$, and $500$ base pairs (bp).\n    *   $100$ additional contigs, each with a length of $10$ bp.\n2.  **Definition of $N_{50}$:**\n    *   Contigs are sorted by non-increasing length.\n    *   The total assembly length is $L_{\\text{tot}}$.\n    *   The $N_{50}$ value is the length of the $k$-th contig, where $k$ is the smallest integer index such that the sum of the lengths of the first $k$ contigs is at least $0.5 \\cdot L_{\\text{tot}}$.\n3.  **Action:** The assembly is filtered by removing all contigs shorter than $100$ bp.\n4.  **Question:** Recompute $N_{50}$ on the filtered set and determine which of the provided statements are correct.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded:** The problem is based on standard concepts in bioinformatics and computational biology. Contig assembly, filtering, and the $N_{50}$ metric are fundamental topics in genome sequence analysis. The definition of $N_{50}$ is correct. The values are numerically plausible.\n2.  **Well-Posed:** The problem is self-contained and provides all necessary data and definitions for a unique solution. The filtering rule is unambiguous. The question is precise.\n3.  **Objective:** The language is formal and quantitative. It is free from subjective or non-scientific claims.\n\n### Step 3: Verdict and Action\nThe problem statement is scientifically sound, well-posed, and objective. It is **valid**. We may proceed to the solution.\n\n### Derivation of Solution\nWe will first calculate the $N_{50}$ for the initial assembly and then for the filtered assembly.\n\n**I. Analysis of the Initial Assembly (Before Filtering)**\n\nFirst, we list the contig lengths in non-increasing order:\n$1200, 800, 600, 500, \\underbrace{10, 10, \\dots, 10}_{100 \\text{ times}}$.\n\nThe total assembly length, $L_{\\text{tot, initial}}$, is the sum of all contig lengths:\n$$L_{\\text{tot, initial}} = 1200 + 800 + 600 + 500 + (100 \\times 10) = 2600 + 500 + 1000 = 4100 \\text{ bp}$$\n\nThe $N_{50}$ threshold is $50\\%$ of this total length:\n$$0.5 \\cdot L_{\\text{tot, initial}} = 0.5 \\times 4100 = 2050 \\text{ bp}$$\n\nNow, we find the smallest number of contigs, $k$, whose cumulative length meets or exceeds this threshold.\n*   Cumulative length for $k=1$: $1200$ bp (Does not meet threshold $2050$)\n*   Cumulative length for $k=2$: $1200 + 800 = 2000$ bp (Does not meet threshold $2050$)\n*   Cumulative length for $k=3$: $1200 + 800 + 600 = 2600$ bp (Meets threshold $2050$)\n\nThe smallest index $k$ is $3$. The $N_{50}$ is the length of the $k=3$-rd contig.\n$$N_{50, \\text{initial}} = 600 \\text{ bp}$$\n\n**II. Analysis of the Filtered Assembly (After Filtering)**\n\nThe filtering rule is to remove all contigs shorter than $100$ bp. In this set, the $100$ contigs of length $10$ bp are removed, as $10 < 100$. The remaining contigs have lengths $1200, 800, 600, 500$.\n\nThe new total assembly length, $L_{\\text{tot, filtered}}$, is:\n$$L_{\\text{tot, filtered}} = 1200 + 800 + 600 + 500 = 3100 \\text{ bp}$$\n\nThe new $N_{50}$ threshold is $50\\%$ of this new total length:\n$$0.5 \\cdot L_{\\text{tot, filtered}} = 0.5 \\times 3100 = 1550 \\text{ bp}$$\n\nNow, we find the smallest number of contigs, $k$, in the filtered set whose cumulative length meets or exceeds this new threshold.\n*   Cumulative length for $k=1$: $1200$ bp (Does not meet threshold $1550$)\n*   Cumulative length for $k=2$: $1200 + 800 = 2000$ bp (Meets threshold $1550$)\n\nThe smallest index $k$ is $2$. The $N_{50}$ is the length of the $k=2$-nd contig in the filtered list.\n$$N_{50, \\text{filtered}} = 800 \\text{ bp}$$\n\n**Summary of Results:**\n*   Initial $N_{50}$: $600$ bp\n*   Filtered $N_{50}$: $800$ bp\n*   The $N_{50}$ value increased after filtering.\n\n### Option-by-Option Analysis\n\n**A. After filtering, $N_{50}$ increases from $600$ to $800$ because the reduction in total assembly length lowers the $50\\%$ cumulative threshold, causing it to be reached at an earlier (longer) contig.**\nOur calculation confirms that $N_{50}$ increases from $600$ bp to $800$ bp. The reasoning provided is precisely correct. The total length $L_{\\text{tot}}$ decreased from $4100$ to $3100$, which lowered the $50\\%$ threshold from $2050$ to $1550$. This lower threshold was met by the cumulative sum of the first two contigs ($2000$), whereas the original threshold required three contigs ($2600$). The second contig is longer than the third, leading to the increase in $N_{50}$.\n**Verdict: Correct**\n\n**B. After filtering, $N_{50}$ decreases from $600$ to $500$ because removing contigs necessarily reduces assembly continuity.**\nOur calculation shows that $N_{50}$ increases, not decreases. The premise of the statement is factually incorrect based on the derived results. The reasoning that removing contigs \"necessarily reduces assembly continuity\" is not a formal argument for why $N_{50}$ must decrease; indeed, as demonstrated, $N_{50}$ can increase.\n**Verdict: Incorrect**\n\n**C. After filtering, $N_{50}$ remains $600$ because removing small contigs does not change the set or order of the largest contigs.**\nOur calculation shows that $N_{50}$ changes from $600$ to $800$. The statement is factually incorrect. The reasoning is flawed because the $N_{50}$ calculation depends on the total assembly length $L_{\\text{tot}}$. While the largest contigs remain, the change in $L_{\\text{tot}}$ alters the cumulative sum threshold, which can, and in this case does, change the $N_{50}$ value.\n**Verdict: Incorrect**\n\n**D. After filtering, $N_{50}$ equals the mean of the two longest contigs due to halving the assembly length.**\nThe mean of the two longest contigs is $(1200 + 800)/2 = 1000$ bp. The filtered $N_{50}$ is $800$ bp, not $1000$ bp. Furthermore, the total assembly length was reduced from $4100$ to $3100$, which is not a halving. The statement is factually incorrect on multiple counts.\n**Verdict: Incorrect**\n\n**E. This scenario demonstrates that $N_{50}$ can be inflated by discarding many short contigs without improving correctness: the artifact arises because $N_{50}$ is defined relative to the total assembly length.**\nThe scenario shows an increase in $N_{50}$ from $600$ to $800$ bp, which can be described as an \"inflation\" of the statistic. This inflation was achieved by simply removing short contigs. This action does not guarantee any improvement in the actual correctness of the assembly; those short contigs might be biologically real. The statement correctly identifies the cause of this artifact: the $N_{50}$ metric's dependence on the total assembly length $L_{\\text{tot}}$. By reducing $L_{\\text{tot}}$, the threshold for reaching the $50\\%$ mark is lowered, making it easier to achieve with fewer, longer contigs. This is a well-known characteristic and limitation of the $N_{50}$ metric. The statement accurately describes the phenomenon illustrated by this problem.\n**Verdict: Correct**", "answer": "$$\\boxed{AE}$$", "id": "2373731"}, {"introduction": "A truly high-quality genome assembly is not just contiguous, but also complete, meaning it contains the full set of expected genes. To assess this, we use metrics like BUSCO (Benchmarking Universal Single-Copy Orthologs) alongside $N50$. This final practice explores a common and challenging scenario where an action taken to improve contiguity inadvertently harms gene completeness, forcing you to weigh the trade-offs between different facets of assembly quality [@problem_id:2373720].", "problem": "A diploid eukaryotic genome is assembled twice from the same set of long-read sequencing data. Assembly X is built from reads after an aggressive correction and filtering step; Assembly Y is built from the uncorrected reads with minimal filtering. When assessed, Assembly X shows a higher $N50$ and fewer contigs than Assembly Y, but a lower Benchmarking Universal Single-Copy Orthologs (BUSCO) completeness score. The $N50$ is defined as the contig length $L$ such that at least $50\\%$ of the total assembly length is contained in contigs of length $\\geq L$. BUSCO completeness estimates the recovery of expected universal single-copy genes by classifying them as Complete (which includes both single-copy and duplicated), Fragmented, or Missing.\n\nWhich of the following hypotheses best explains why Assembly X has a higher $N50$ but lower BUSCO completeness than Assembly Y?\n\nA. The read correction and filtering collapsed allelic variation and some repeat-derived sequences, removing or fragmenting genuine loci; this reduced the number of contigs and increased $N50$, but also eliminated or broke some universal single-copy orthologs, lowering BUSCO completeness.\n\nB. By definition, $N50$ and BUSCO completeness are inversely related, so any increase in $N50$ must cause a decrease in BUSCO completeness regardless of biological content.\n\nC. BUSCO completeness is computed as the fraction of assembled bases mapping to orthologs; increasing $N50$ lowers this fraction even if the same orthologs are present, explaining the drop in BUSCO completeness.\n\nD. BUSCO completeness only counts single-copy genes and penalizes duplicated genes; the correction step reduced duplicated genes, so BUSCO completeness fell even though all orthologs remained present.", "solution": "The problem statement will first be validated for scientific and logical integrity.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n*   A diploid eukaryotic genome is the subject.\n*   Two assemblies, Assembly X and Assembly Y, are generated from the same set of long-read sequencing data.\n*   Assembly X is created from reads that underwent an \"aggressive correction and filtering step\".\n*   Assembly Y is created from \"uncorrected reads with minimal filtering\".\n*   Comparison of metrics:\n    *   Assembly X has a higher $N50$ than Assembly Y.\n    *   Assembly X has fewer contigs than Assembly Y.\n    *   Assembly X has a lower Benchmarking Universal Single-Copy Orthologs (BUSCO) completeness score than Assembly Y.\n*   Definition of $N50$: \"the contig length $L$ such that at least $50\\%$ of the total assembly length is contained in contigs of length $\\geq L$.\"\n*   Definition of BUSCO completeness: \"estimates the recovery of expected universal single-copy genes by classifying them as Complete (which includes both single-copy and duplicated), Fragmented, or Missing.\" The completeness score is the percentage of genes classified as 'Complete'.\n\n**Step 2: Validate Using Extracted Givens**\n\n1.  **Scientifically Grounded**: The problem describes a realistic and common scenario in the field of bioinformatics, specifically *de novo* genome assembly. The use of long-read sequencing for diploid genomes and the analysis of trade-offs between assembly contiguity ($N50$) and completeness (BUSCO) due to pre-assembly read processing are standard topics. The described outcome is a known phenomenon.\n2.  **Well-Posed**: The problem is well-posed. It presents a set of empirical observations and asks for the most plausible explanatory hypothesis. The definitions provided for key terms ($N50$, BUSCO) are correct and sufficient for reasoning.\n3.  **Objective**: The language is technical and objective. The relationships are stated as \"higher\" or \"lower,\" which are objective comparisons.\n\nThe problem statement does not violate any of the invalidity criteria. It is scientifically sound, self-contained, and presents a non-trivial question based on established principles of computational biology.\n\n**Step 3: Verdict and Action**\n\nThe problem is **valid**. A solution will be derived.\n\n### Derivation of Solution\n\nThe problem asks for an explanation of a set of observations: an \"aggressive\" pre-processing of reads for Assembly X leads to higher contiguity (higher $N50$, fewer contigs) but lower gene completeness (lower BUSCO score) compared to Assembly Y, which used minimally processed reads.\n\nThe core of the problem lies in the nature of a \"diploid eukaryotic genome\" and the effect of an \"aggressive correction and filtering step.\"\n\n1.  **Diploid Genome Assembly**: A diploid organism has two sets of chromosomes, one from each parent. These sets are largely identical but differ at heterozygous sites. A genome assembler processing reads from such an organism may treat the two different alleles as distinct sequences. This can lead to a \"bubble\" in the assembly graph, where the two allelic paths diverge and then reconverge. An assembler might stop at these bubbles, breaking the assembly into smaller contigs. A more \"haplotype-resolved\" assembly, like Assembly Y from minimally filtered reads, might correctly represent both alleles as separate contigs. This would result in a higher contig count, a larger total assembly size (close to twice the haploid genome size), and consequently, a lower $N50$.\n\n2.  **Effect of Aggressive Correction and Filtering**: An aggressive correction algorithm may not distinguish between sequencing errors and genuine allelic variation. It might \"correct\" reads corresponding to one allele to match the sequence of the other, more prevalent allele. Furthermore, aggressive filtering might discard reads that appear to be outliers, which could include reads from highly divergent allelic regions or repetitive elements. This process, known as **allelic collapse** or **haplotype collapse**, effectively forces the assembler to create a single, chimeric sequence representing both haplotypes in heterozygous regions.\n\n3.  **Consequences of Allelic Collapse (Assembly X)**:\n    *   **Higher $N50$ and Fewer Contigs**: By collapsing the bubbles in the assembly graph caused by heterozygosity, the assembler can generate much longer, continuous contigs. This directly leads to a higher $N50$ and a reduced number of total contigs.\n    *   **Lower BUSCO Completeness**: BUSCO genes are expected to be conserved. If a BUSCO gene lies in a heterozygous region, the aggressive filtering might discard reads from one of the alleles, or the correction process might damage the sequence evidence. This can lead to several negative outcomes for the BUSCO assessment:\n        *   The locus might not be assembled at all, resulting in a **Missing** BUSCO.\n        *   The locus might be partially assembled, resulting in a **Fragmented** BUSCO.\n        *   The collapsed, chimeric sequence might be divergent enough from the reference that it is not recognized as a complete gene, also leading to a **Missing** or **Fragmented** classification.\n    Any of these outcomes reduces the 'Complete' BUSCO count, thus lowering the completeness score.\n\nThis chain of reasoning—aggressive correction leads to allelic collapse, which increases contiguity at the cost of gene content integrity—provides a complete explanation for all the observations.\n\n### Evaluation of Options\n\n*   **A. The read correction and filtering collapsed allelic variation and some repeat-derived sequences, removing or fragmenting genuine loci; this reduced the number of contigs and increased $N50$, but also eliminated or broke some universal single-copy orthologs, lowering BUSCO completeness.**\n    This hypothesis is perfectly aligned with the derivation above. It correctly identifies \"collapsed allelic variation\" as the mechanism. It correctly links this mechanism to the observed increase in contiguity (\"reduced the number of contigs and increased $N50$\"). It also correctly links the mechanism to the observed decrease in gene content (\"eliminated or broke some universal single-copy orthologs, lowering BUSCO completeness\").\n    **Verdict: Correct.**\n\n*   **B. By definition, $N50$ and BUSCO completeness are inversely related, so any increase in $N50$ must cause a decrease in BUSCO completeness regardless of biological content.**\n    This statement is fundamentally false. $N50$ and BUSCO completeness are independent metrics assessing different qualities of an assembly (contiguity and gene content completeness, respectively). It is the goal of assembly projects to maximize both. For instance, closing a gap in an assembly by adding a new scaffold could increase $N50$ while simultaneously completing a previously fragmented BUSCO gene, thereby increasing both metrics. There is no definitional inverse relationship.\n    **Verdict: Incorrect.**\n\n*   **C. BUSCO completeness is computed as the fraction of assembled bases mapping to orthologs; increasing $N50$ lowers this fraction even if the same orthologs are present, explaining the drop in BUSCO completeness.**\n    This misrepresents the BUSCO metric. BUSCO completeness is a gene-count based metric, not a base-fraction metric. It reports the number and percentage of expected genes found in the assembly. The total assembly size, which is the denominator for the $N50$ calculation, is not the denominator for the BUSCO completeness percentage. An increase in $N50$ does not computationally necessitate a decrease in the BUSCO score.\n    **Verdict: Incorrect.**\n\n*   **D. BUSCO completeness only counts single-copy genes and penalizes duplicated genes; the correction step reduced duplicated genes, so BUSCO completeness fell even though all orthologs remained present.**\n    This statement contradicts the definition of BUSCO completeness provided directly in the problem statement: \"...classifying them as Complete (which includes both single-copy and duplicated)...\". The 'Complete' score is the sum of single-copy (S) and duplicated (D) genes. Finding a gene as 'Duplicated' does not lower the completeness score. In fact, collapsing two allelic copies of a BUSCO gene from 'Duplicated' into one 'Single-copy' gene would leave the overall 'Complete' score unchanged. Therefore, this hypothesis fails to explain the observed drop in BUSCO completeness.\n    **Verdict: Incorrect.**", "answer": "$$\\boxed{A}$$", "id": "2373720"}]}