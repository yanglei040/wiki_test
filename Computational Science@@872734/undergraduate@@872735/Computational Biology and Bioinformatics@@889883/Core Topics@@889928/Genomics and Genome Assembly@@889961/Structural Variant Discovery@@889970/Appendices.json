{"hands_on_practices": [{"introduction": "Before diving into complex algorithms, it's crucial to understand the scale of the challenge in structural variant discovery. This exercise asks you to perform a theoretical calculation to quantify the computational cost of a brute-force search for genomic inversions [@problem_id:2431948]. By analyzing the complexity of this exhaustive approach, you will gain an appreciation for why modern SV discovery relies on efficient heuristics rather than simple enumeration.", "problem": "A linear haploid reference genome has length $N$ nucleotides. A candidate inversion is defined by choosing a pair of distinct breakpoint positions $(i,j)$ with $1 \\le i < j \\le N$. Consider an exhaustive split-read alignment approach to identify all possible inversions, operating as follows under the stated computational model.\n\n- Reads have fixed length $\\ell \\ge 2$, independent of $N$. For each candidate inversion $(i,j)$, the algorithm considers every possible split of a read into two nonempty fragments of lengths $s$ and $\\ell - s$, where $s \\in \\{1,2,\\dots,\\ell - 1\\}$.\n- For a given split $s$, the algorithm performs two local alignments: the left fragment of length $s$ is aligned to a window of fixed length $w$ centered at breakpoint $i$ in forward orientation, and the right fragment of length $\\ell - s$ is aligned to a window of fixed length $w$ centered at breakpoint $j$ in reverse-complement orientation. The window length $w$ is a fixed constant independent of $N$ and $\\ell$.\n- Each local alignment is performed by the Smith–Waterman dynamic programming algorithm (Dynamic Programming (DP)), and the computational cost is measured as the exact number of DP cell updates. Aligning a sequence of length $a$ to a sequence of length $b$ incurs exactly $a \\times b$ DP cell updates. Treat each DP cell update as one unit of work. Ignore edge effects at the ends of the genome so that every window is of length $w$.\n\nUnder this model, determine a closed-form expression $T(N,\\ell,w)$ for the total number of DP cell updates required to exhaustively evaluate all candidate inversions and all splits as described. Express your final answer as a single simplified analytic expression in terms of $N$, $\\ell$, and $w$. No rounding is required.", "solution": "The problem statement has been validated and found to be scientifically grounded, well-posed, and self-contained. It presents a simplified but coherent model for analyzing the computational cost of an exhaustive split-read alignment strategy for inversion discovery. The model is based on established principles in bioinformatics, namely the use of Smith-Waterman dynamic programming for local alignment and the split-read paradigm for detecting structural variants. All parameters are clearly defined, and the objective is unambiguous. We may therefore proceed with a formal derivation of the solution.\n\nThe goal is to find the total computational cost $T(N, \\ell, w)$, measured in the number of dynamic programming (DP) cell updates, required to exhaustively evaluate all possible candidate inversions. We can decompose this total cost by considering the hierarchical structure of the search procedure. The total cost is the product of the number of candidate inversions and the computational work required to evaluate each one.\n\nFirst, we must determine the number of candidate inversions. A candidate inversion is defined by a pair of distinct breakpoint positions $(i,j)$ on a genome of length $N$, with the constraint that $1 \\le i < j \\le N$. The number of such pairs is equivalent to the number of ways to choose $2$ distinct positions from a set of $N$ positions. This is given by the binomial coefficient $\\binom{N}{2}$.\n$$\n\\text{Number of candidate inversions} = \\binom{N}{2} = \\frac{N(N-1)}{2}\n$$\nThe problem states that due to the assumption of ignoring edge effects, the computational work is identical for every candidate inversion.\n\nNext, we calculate the computational cost for a single candidate inversion. For each inversion, the algorithm considers every possible split of a read of length $\\ell$ into two non-empty fragments. A split is defined by an integer $s$, where $s$ is the length of the left fragment. The remaining right fragment has length $\\ell - s$. The condition that fragments are non-empty implies $s \\ge 1$ and $\\ell - s \\ge 1$, which means $1 \\le s \\le \\ell - 1$. Thus, for a single read of length $\\ell$, there are $\\ell - 1$ possible splits.\n\nFor a given split $s$, two local alignments are performed:\n$1$. The left fragment of length $s$ is aligned against a genomic window of length $w$. According to the problem's cost model, the number of DP cell updates for this alignment is $s \\times w$.\n$2$. The right fragment of length $\\ell - s$ is aligned against another genomic window of length $w$. The cost for this alignment is $(\\ell - s) \\times w$.\n\nThe total computational cost for a single split $s$, let us denote it $C_s$, is the sum of the costs of these two alignments:\n$$\nC_s = (s \\cdot w) + ((\\ell - s) \\cdot w) = s \\cdot w + \\ell \\cdot w - s \\cdot w = \\ell \\cdot w\n$$\nInterestingly, the cost per split is constant and independent of the split position $s$.\n\nThe total cost to evaluate one candidate inversion, let us call it $C_{\\text{inv}}$, is the sum of costs for all possible splits of a read.\n$$\nC_{\\text{inv}} = \\sum_{s=1}^{\\ell-1} C_s = \\sum_{s=1}^{\\ell-1} (\\ell \\cdot w)\n$$\nSince $\\ell \\cdot w$ is a constant with respect to the summation index $s$, we can factor it out of the sum. The sum runs over $\\ell - 1$ terms.\n$$\nC_{\\text{inv}} = (\\ell - 1) \\cdot (\\ell \\cdot w) = w \\ell (\\ell - 1)\n$$\n\nFinally, the total computational cost $T(N, \\ell, w)$ for the exhaustive evaluation is the product of the number of candidate inversions and the cost per inversion.\n$$\nT(N, \\ell, w) = (\\text{Number of candidate inversions}) \\times C_{\\text{inv}}\n$$\nSubstituting the expressions derived above:\n$$\nT(N, \\ell, w) = \\left( \\frac{N(N-1)}{2} \\right) \\cdot \\left( w \\ell (\\ell - 1) \\right)\n$$\nThis expression can be simplified and rearranged to provide the final closed-form expression.\n$$\nT(N, \\ell, w) = \\frac{1}{2} w \\ell (\\ell - 1) N (N - 1)\n$$\nThis is the total number of DP cell updates required under the specified computational model.", "answer": "$$\n\\boxed{\\frac{1}{2} w \\ell (\\ell-1) N (N-1)}\n$$", "id": "2431948"}, {"introduction": "Having appreciated the computational hurdles, we now turn to a practical method for identifying SVs using paired-end sequencing data. This practice problem challenges you to implement a decision algorithm that uses characteristic data signatures—specifically, read pair orientation and insert size—to distinguish between a large genomic deletion and a novel retrotransposon insertion [@problem_id:2431909]. This task provides direct experience in translating conceptual models of SV signatures into a working classifier.", "problem": "You are given a decision problem grounded in paired-end sequencing for structural variant discovery. Consider a DNA sequencing library in which the distribution of physical fragment lengths (insert sizes) for properly paired reads is approximately Gaussian with mean $\\mu$ and standard deviation $\\sigma$ in base pairs (bp). At a genomic locus, you observe paired-end mappings characterized by the following two observables: (i) the orientation of each mapped read pair relative to the reference genome (categories: FR, RF, FF, RR), and (ii) the mapped distance on the reference between the two reads of a pair (the mapped insert size) for those pairs in the FR orientation. Your task is to decide, for each locus, whether the evidence is more consistent with a large deletion in the sample relative to the reference (output integer $0$) or with a novel retrotransposon insertion in the sample relative to the reference (output integer $1$), using only these two observables.\n\nDefinitions and assumptions:\n- Paired-End (PE) sequencing produces read pairs. The FR orientation denotes an inward-facing configuration that is the expected proper-pair orientation under standard Illumina protocols; RF, FF, and RR denote outward-facing or same-strand configurations regarded as discordant with respect to the library’s expected orientation.\n- A large deletion in the sample relative to the reference tends to produce an enrichment of FR pairs whose mapped insert sizes on the reference are stochastically larger than the library distribution characterized by $\\mu$ and $\\sigma$, with FR remaining the predominant orientation locally.\n- A novel retrotransposon insertion in the sample relative to the reference tends to produce an enrichment of non-FR orientations (RF, FF, RR) and FR pairs whose mapped insert sizes on the reference are stochastically smaller than the library distribution characterized by $\\mu$ and $\\sigma$.\n\nInput specification for each test case:\n- Library parameters $\\mu$ and $\\sigma$ (both in bp).\n- Orientation counts $c_{\\mathrm{FR}}$, $c_{\\mathrm{RF}}$, $c_{\\mathrm{FF}}$, $c_{\\mathrm{RR}}$ measured in a fixed window around the locus.\n- A list of mapped insert sizes (in bp) for the subset of read pairs observed in FR orientation at that locus.\n\nOutput specification:\n- For each test case, output a single integer: $0$ if the locus is classified as a deletion and $1$ if it is classified as a novel retrotransposon insertion.\n- Your program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets with no spaces, for example, $\"[0,1,1,0]\"$.\n\nTest suite:\n- Test case $1$:\n  - $\\mu = 350$, $\\sigma = 50$.\n  - $c_{\\mathrm{FR}} = 40$, $c_{\\mathrm{RF}} = 1$, $c_{\\mathrm{FF}} = 0$, $c_{\\mathrm{RR}} = 0$.\n  - FR insert sizes: $\\{520,540,560,590,610\\}$.\n- Test case $2$:\n  - $\\mu = 350$, $\\sigma = 50$.\n  - $c_{\\mathrm{FR}} = 10$, $c_{\\mathrm{RF}} = 25$, $c_{\\mathrm{FF}} = 5$, $c_{\\mathrm{RR}} = 3$.\n  - FR insert sizes: $\\{150,160,170,180,190,200,210\\}$.\n- Test case $3$:\n  - $\\mu = 350$, $\\sigma = 50$.\n  - $c_{\\mathrm{FR}} = 22$, $c_{\\mathrm{RF}} = 2$, $c_{\\mathrm{FF}} = 1$, $c_{\\mathrm{RR}} = 1$.\n  - FR insert sizes: $\\{410,430,420,380,370\\}$.\n- Test case $4$:\n  - $\\mu = 400$, $\\sigma = 40$.\n  - $c_{\\mathrm{FR}} = 8$, $c_{\\mathrm{RF}} = 30$, $c_{\\mathrm{FF}} = 4$, $c_{\\mathrm{RR}} = 2$.\n  - FR insert sizes: $\\{260,280,300,310\\}$.\n- Test case $5$:\n  - $\\mu = 300$, $\\sigma = 30$.\n  - $c_{\\mathrm{FR}} = 15$, $c_{\\mathrm{RF}} = 14$, $c_{\\mathrm{FF}} = 3$, $c_{\\mathrm{RR}} = 3$.\n  - FR insert sizes: $\\{200,210,190,205,195\\}$.\n\nFinal output format:\n- Your program must compute a classification for each of the above test cases and print a single line containing the results as a comma-separated list of integers enclosed in square brackets with no spaces, for example, $\"[0,1,0,1,1]\"$.", "solution": "The problem has been validated and is determined to be scientifically grounded, well-posed, and objective. It poses a standard binary classification task within the domain of computational biology, specifically the analysis of structural variation from paired-end DNA sequencing data.\n\nThe task is to classify a genomic locus as either containing a large deletion (output $0$) or a novel retrotransposon insertion (output $1$) relative to a reference genome. We are provided with two categories of evidence for this decision:\n$1$. The counts of read pairs in different orientations relative to the reference: inward-facing ($c_{\\mathrm{FR}}$) and discordant orientations ($c_{\\mathrm{RF}}$, $c_{\\mathrm{FF}}$, $c_{\\mathrm{RR}}$).\n$2$. A sample of mapped insert sizes, $\\{x_1, x_2, \\ldots, x_n\\}$, for the subset of $n$ read pairs observed in the FR orientation.\nThese observations are to be interpreted in the context of a sequencing library where properly paired reads exhibit a physical fragment length distribution that is Gaussian with mean $\\mu$ and standard deviation $\\sigma$.\n\nThe decision algorithm is derived directly from the formalization of the two mutually exclusive genomic signatures described in the problem statement.\n\n**Hypothesis $H_0$: Large Deletion**\nThe signature for a large deletion is described by two concurrent phenomena:\n- **Orientation evidence:** An enrichment of FR-oriented pairs, with FR remaining the predominant orientation. This is formalized as the count of FR pairs exceeding the total count of all non-FR (discordant) pairs.\n$$c_{\\mathrm{FR}} > c_{\\mathrm{RF}} + c_{\\mathrm{FF}} + c_{\\mathrm{RR}}$$\n- **Insert size evidence:** The mapped insert sizes for FR pairs are stochastically larger than the library's physical fragment length distribution. This is because reads spanning the deletion in the sample genome map to distant points on the reference, creating an artificially large mapped insert size. This is formalized by comparing the sample mean of the observed FR insert sizes, $\\bar{x}_{\\mathrm{FR}}$, to the library mean, $\\mu$.\n$$ \\bar{x}_{\\mathrm{FR}} = \\frac{1}{n} \\sum_{i=1}^{n} x_i > \\mu $$\n\n**Hypothesis $H_1$: Novel Retrotransposon Insertion**\nThe signature for a novel retrotransposon insertion is described by phenomena that are characteristically distinct from a deletion:\n- **Orientation evidence:** An enrichment of non-FR orientations. This occurs because read pairs spanning the insertion site have one read mapping to the flanking genomic sequence and the other mapping to a different copy of the retrotransposon elsewhere in the genome, leading to discordant orientations. This is formalized as the count of FR pairs not exceeding the count of non-FR pairs.\n$$c_{\\mathrm{FR}} \\le c_{\\mathrm{RF}} + c_{\\mathrm{FF}} + c_{\\mathrm{RR}}$$\n- **Insert size evidence:** The mapped insert sizes for FR pairs are stochastically smaller than the library's distribution. This applies to fragment pairs that are entirely contained within the novel insertion, which, when mapped back to the single-copy reference, appear to have originated from a smaller fragment than they did. This is formalized as:\n$$ \\bar{x}_{\\mathrm{FR}} < \\mu $$\n\n**Decision Rule**\nThe problem asks for a decisive classification. The description of the deletion signature implies a logical conjunction of its two evidentiary components. A locus is classified as a deletion if, and only if, both the orientation evidence and the insert size evidence are consistent with a deletion. Given that this is a binary classification task between two well-defined alternatives, any case not meeting the strict criteria for a deletion is consequently classified as an insertion. This leads to the following robust and complete decision rule:\n\nA locus is classified as a **deletion (output $0$)** if both of the following conditions are true:\n$1$. $c_{\\mathrm{FR}} > (c_{\\mathrm{RF}} + c_{\\mathrm{FF}} + c_{\\mathrm{RR}})$\n$2$. $\\bar{x}_{\\mathrm{FR}} > \\mu$\n\nOtherwise, the locus is classified as a **novel retrotransposon insertion (output $1$)**.\n\nThis logical framework is sufficient for all provided test cases, as none exhibit conflicting evidence (e.g., FR predominance with smaller-than-average inserts). The use of a more complex model incorporating the standard deviation $\\sigma$, for instance, to calculate Z-scores or likelihoods, would require additional assumptions about the relative weighting of evidence types, which are not provided in the problem statement. The chosen algorithm represents the most direct and scientifically sound interpretation of the problem as stated.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the structural variant classification problem based on paired-end sequencing data.\n    \"\"\"\n    \n    # Test cases defined in the problem statement.\n    test_cases = [\n        # (mu, sigma, c_fr, c_rf, c_ff, c_rr, fr_sizes)\n        (350, 50, 40, 1, 0, 0, [520, 540, 560, 590, 610]),\n        (350, 50, 10, 25, 5, 3, [150, 160, 170, 180, 190, 200, 210]),\n        (350, 50, 22, 2, 1, 1, [410, 430, 420, 380, 370]),\n        (400, 40, 8, 30, 4, 2, [260, 280, 300, 310]),\n        (300, 30, 15, 14, 3, 3, [200, 210, 190, 205, 195]),\n    ]\n\n    results = []\n    for case in test_cases:\n        mu, _, c_fr, c_rf, c_ff, c_rr, fr_sizes = case\n        \n        # Calculate the total count of non-FR (discordant) read pairs.\n        c_non_fr = c_rf + c_ff + c_rr\n        \n        # Calculate the sample mean of the FR insert sizes.\n        # Handle the edge case of an empty list of FR sizes, though not present in tests.\n        if not fr_sizes:\n            # If no FR sizes are available, the insert size criterion cannot be tested.\n            # The decision rule defaults to 'insertion' as the strict 'deletion' criteria cannot be met.\n            x_bar_fr = -np.inf # Ensures the comparison 'x_bar_fr > mu' is false.\n        else:\n            x_bar_fr = np.mean(fr_sizes)\n        \n        # A deletion signature requires FR orientation to be predominant AND\n        # the mean FR insert size to be larger than the library mean.\n        is_deletion_signature = (c_fr > c_non_fr) and (x_bar_fr > mu)\n        \n        if is_deletion_signature:\n            # Output 0 for a deletion.\n            results.append(0)\n        else:\n            # Otherwise, classify as an insertion (output 1).\n            # This covers cases where orientation or insert size (or both)\n            # are consistent with an insertion signature.\n            results.append(1)\n\n    # Print the final results in the specified format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2431909"}, {"introduction": "A key task in bioinformatics is not just detecting signals, but also filtering out noise. This exercise focuses on a common source of false-positive SV calls: mapping artifacts in repetitive regions of the genome [@problem_id:2431938]. You will develop and apply a quantitative scoring function that combines sequence complexity, measured by Shannon entropy, with mapping quality to differentiate high-confidence split-read alignments from unreliable ones.", "problem": "You are given a formal definition of a score that quantifies the reliability of a split-read alignment purported to support a genomic inversion, using the deoxyribonucleic acid (DNA) sequence complexity of the two aligned read segments and their mapping qualities. The DNA alphabet is restricted to the set of symbols {A, C, G, T}. For each split-read alignment, there are two flanks (left and right) with corresponding sequences and Phred-scaled mapping qualities.\n\nDefine, for a DNA sequence $s$ of length $n$, with mononucleotide counts $c_A$, $c_C$, $c_G$, $c_T$ satisfying $c_A + c_C + c_G + c_T = n$, the empirical base frequencies $p_b = c_b / n$ for $b \\in \\{A,C,G,T\\}$. Define the Shannon entropy in bits as\n$$\nH(s) = - \\sum_{b \\in \\{A,C,G,T\\}} p_b \\log_2 p_b,\n$$\nwith the convention that terms with $p_b = 0$ contribute $0$. Define the normalized entropy\n$$\nh(s) = \\frac{H(s)}{\\log_2 4} = \\frac{H(s)}{2},\n$$\nso that $h(s) \\in [0,1]$.\n\nLet the left and right flanks have sequences $s_L$ and $s_R$ and Phred-scale mapping qualities $Q_L$ and $Q_R$ respectively. Convert each mapping quality $Q$ to a probability of correct mapping using\n$$\nP(Q) = 1 - 10^{-Q/10}.\n$$\n\nDefine the composite reliability score\n$$\nS(s_L,s_R,Q_L,Q_R) = \\min\\{ h(s_L), h(s_R) \\} \\times \\min\\{ P(Q_L), P(Q_R) \\}.\n$$\n\nGiven a decision threshold $\\tau$, classify a split-read alignment as a true inversion-supporting signal if and only if $S \\ge \\tau$, and otherwise as a mapping artifact, yielding a boolean result.\n\nImplement a program that, for the following test suite, computes the boolean classification for each case using the threshold $\\tau = 0.7$:\n\n- Case $1$: $s_L=$ \"ACGTACGTACGT\", $s_R=$ \"TGCATGCATGCA\", $Q_L = 40$, $Q_R = 42$.\n- Case $2$: $s_L=$ \"ATATATATATAT\", $s_R=$ \"ATATATATATAT\", $Q_L = 10$, $Q_R = 10$.\n- Case $3$: $s_L=$ \"GATTACAGATTACA\", $s_R=$ \"TCGATCGATCGA\", $Q_L = 35$, $Q_R = 5$.\n- Case $4$: $s_L=$ \"AAAAAAAAAAAA\", $s_R=$ \"ACGTACGTACGT\", $Q_L = 60$, $Q_R = 60$.\n- Case $5$: $s_L=$ \"CG\", $s_R=$ \"CG\", $Q_L = 20$, $Q_R = 20$.\n- Case $6$: $s_L=$ \"ACGTACGT\", $s_R=$ \"GTACGTAC\", $Q_L = 20$, $Q_R = 20$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the same order as the cases above. For example, a valid output format is \"[True,False,True]\". The output must contain exactly six boolean values corresponding to Cases $1$ through $6$.", "solution": "The problem statement is first subjected to rigorous validation.\n\nStep 1: Extracted Givens\nThe problem provides the following definitions and data:\n- DNA alphabet: $\\Sigma = \\{A, C, G, T\\}$.\n- For a sequence $s$ of length $n$ with mononucleotide counts $c_A, c_C, c_G, c_T$ such that $\\sum_{b \\in \\Sigma} c_b = n$.\n- Empirical base frequency: $p_b = c_b / n$ for $b \\in \\Sigma$.\n- Shannon entropy in bits: $H(s) = - \\sum_{b \\in \\Sigma} p_b \\log_2 p_b$, with $0 \\log_2 0 = 0$.\n- Normalized entropy: $h(s) = H(s) / \\log_2 4 = H(s) / 2$, where $h(s) \\in [0,1]$.\n- Left and right flank sequences: $s_L, s_R$.\n- Phred-scale mapping qualities: $Q_L, Q_R$.\n- Probability of correct mapping: $P(Q) = 1 - 10^{-Q/10}$.\n- Composite reliability score: $S(s_L, s_R, Q_L, Q_R) = \\min\\{ h(s_L), h(s_R) \\} \\times \\min\\{ P(Q_L), P(Q_R) \\}$.\n- Decision threshold: $\\tau = 0.7$.\n- Classification rule: True if $S \\ge \\tau$, False otherwise.\n- Test Cases:\n    1. $s_L=$ \"ACGTACGTACGT\", $s_R=$ \"TGCATGCATGCA\", $Q_L = 40$, $Q_R = 42$.\n    2. $s_L=$ \"ATATATATATAT\", $s_R=$ \"ATATATATATAT\", $Q_L = 10$, $Q_R = 10$.\n    3. $s_L=$ \"GATTACAGATTACA\", $s_R=$ \"TCGATCGATCGA\", $Q_L = 35$, $Q_R = 5$.\n    4. $s_L=$ \"AAAAAAAAAAAA\", $s_R=$ \"ACGTACGTACGT\", $Q_L = 60$, $Q_R = 60$.\n    5. $s_L=$ \"CG\", $s_R=$ \"CG\", $Q_L = 20$, $Q_R = 20$.\n    6. $s_L=$ \"ACGTACGT\", $s_R=$ \"GTACGTAC\", $Q_L = 20$, $Q_R = 20$.\n\nStep 2: Validation Using Extracted Givens\nThe problem is evaluated against the specified criteria.\n- **Scientifically Grounded**: The problem is well-grounded. It employs standard definitions from information theory (Shannon entropy) and bioinformatics (Phred quality scores, sequence complexity). The proposed reliability score is a logical, albeit simplified, heuristic for evaluating evidence for structural variants. It is free of pseudoscience.\n- **Well-Posed**: The problem is well-posed. All mathematical relations are explicitly defined. All constants and input data for the test suite are provided. A unique boolean output is expected for each case.\n- **Objective**: The problem is stated with objective, mathematical precision. There are no subjective or ambiguous terms.\n\nThe problem exhibits no flaws such as scientific unsoundness, non-formalizability, incompleteness, contradiction, infeasibility, or ill-posed structure.\n\nStep 3: Verdict and Action\nThe problem is deemed **valid**. A solution will be furnished.\n\nThe solution requires the implementation of a classification procedure based on a composite reliability score $S$. This score must be calculated for each of the six test cases and compared against the decision threshold $\\tau = 0.7$.\n\nThe calculation of the score $S$ is composed of two primary components: the sequence complexity, measured by normalized entropy $h(s)$, and the mapping reliability, measured by the probability of correct mapping $P(Q)$.\n\nFirst, we define the calculation of the normalized entropy $h(s)$. For a given sequence $s$ of length $n$, we count the occurrences of each base $c_b$ for $b \\in \\{A, C, G, T\\}$. The frequencies are $p_b = c_b/n$. The Shannon entropy is then $H(s) = - \\sum p_b \\log_2 p_b$. The maximum entropy for a DNA sequence occurs when all four bases are equally likely ($p_b=0.25$), yielding $H_{max} = \\log_2 4 = 2$ bits. Normalizing by this maximum value gives $h(s) = H(s)/2$, which maps the complexity to the interval $[0,1]$. A value of $h(s)=0$ corresponds to a homopolymer (e.g., \"AAAA\"), representing minimum complexity. A value of $h(s)=1$ corresponds to a sequence with uniform base distribution, representing maximum complexity.\n\nSecond, we define the calculation of the mapping probability $P(Q)$. A Phred-scaled quality score $Q$ is logarithmically related to the error probability $P_{error}$. Specifically, $Q = -10 \\log_{10} P_{error}$, which implies $P_{error} = 10^{-Q/10}$. The probability of a correct mapping is thus $P(Q) = 1 - P_{error} = 1 - 10^{-Q/10}$.\n\nThe composite score $S$ combines these two metrics: $S = \\min\\{h(s_L), h(s_R)\\} \\times \\min\\{P(Q_L), P(Q_R)\\}$. This formulation is conservative; the final score is limited by the flank with the lower sequence complexity and the flank with the lower mapping quality. An alignment is classified as a true signal if $S \\ge \\tau$.\n\nWe now apply this procedure to each case with $\\tau = 0.7$.\n\nCase 1: $s_L=$ \"ACGTACGTACGT\", $s_R=$ \"TGCATGCATGCA\", $Q_L = 40$, $Q_R = 42$.\n- For both $s_L$ and $s_R$, $n=12$ and $c_A=c_C=c_G=c_T=3$. Thus, $p_b = 3/12 = 0.25$ for all bases.\n- $H(s_L) = H(s_R) = -4 \\times (0.25 \\log_2 0.25) = -4 \\times (0.25 \\times -2) = 2$.\n- $h(s_L) = h(s_R) = 2/2 = 1$. $\\min\\{h\\} = 1$.\n- $P(Q_L) = 1 - 10^{-40/10} = 1 - 10^{-4} = 0.9999$.\n- $P(Q_R) = 1 - 10^{-42/10} = 1 - 10^{-4.2} \\approx 0.999937$.\n- $\\min\\{P\\} = 0.9999$.\n- $S = 1 \\times 0.9999 = 0.9999$.\n- $0.9999 \\ge 0.7$. Classification: **True**.\n\nCase 2: $s_L=$ \"ATATATATATAT\", $s_R=$ \"ATATATATATAT\", $Q_L = 10$, $Q_R = 10$.\n- For both sequences, $n=12$, $c_A=6, c_T=6$. $p_A=p_T=0.5$.\n- $H(s_L) = H(s_R) = -2 \\times (0.5 \\log_2 0.5) = -2 \\times (0.5 \\times -1) = 1$.\n- $h(s_L) = h(s_R) = 1/2 = 0.5$. $\\min\\{h\\} = 0.5$.\n- $P(Q_L) = P(Q_R) = 1 - 10^{-10/10} = 1 - 0.1 = 0.9$. $\\min\\{P\\} = 0.9$.\n- $S = 0.5 \\times 0.9 = 0.45$.\n- $0.45 < 0.7$. Classification: **False**.\n\nCase 3: $s_L=$ \"GATTACAGATTACA\", $s_R=$ \"TCGATCGATCGA\", $Q_L = 35$, $Q_R = 5$.\n- For $s_L$, $n=14$, $c_A=6, c_C=2, c_G=2, c_T=4$. Frequencies: $p_A=6/14, p_C=2/14, p_G=2/14, p_T=4/14$.\n- $H(s_L) \\approx -(\\frac{6}{14}\\log_2\\frac{6}{14} + \\frac{2}{14}\\log_2\\frac{2}{14} + \\frac{2}{14}\\log_2\\frac{2}{14} + \\frac{4}{14}\\log_2\\frac{4}{14}) \\approx 1.8423$.\n- $h(s_L) \\approx 1.8423 / 2 \\approx 0.9212$.\n- For $s_R$, $n=12$, $p_b = 0.25$ for all bases, so $h(s_R)=1$.\n- $\\min\\{h\\} \\approx 0.9212$.\n- $P(Q_L) = 1 - 10^{-35/10} = 1 - 10^{-3.5} \\approx 0.9997$.\n- $P(Q_R) = 1 - 10^{-5/10} = 1 - 10^{-0.5} \\approx 0.6838$.\n- $\\min\\{P\\} \\approx 0.6838$.\n- $S \\approx 0.9212 \\times 0.6838 \\approx 0.6299$.\n- $0.6299 < 0.7$. Classification: **False**.\n\nCase 4: $s_L=$ \"AAAAAAAAAAAA\", $s_R=$ \"ACGTACGTACGT\", $Q_L = 60$, $Q_R = 60$.\n- For $s_L$, a homopolymer, $p_A=1$ and other $p_b=0$.\n- $H(s_L) = -(1 \\log_2 1) = 0$. $h(s_L) = 0$.\n- For $s_R$, $p_b=0.25$, so $h(s_R)=1$.\n- $\\min\\{h\\} = 0$.\n- $S = 0 \\times \\min\\{P(60), P(60)\\} = 0$.\n- $0 < 0.7$. Classification: **False**.\n\nCase 5: $s_L=$ \"CG\", $s_R=$ \"CG\", $Q_L = 20$, $Q_R = 20$.\n- For both sequences, $n=2$, $c_C=1, c_G=1$. $p_C=p_G=0.5$.\n- This is analogous to Case 2 in terms of frequencies: $H(s_{L,R})=1$, $h(s_{L,R})=0.5$.\n- $\\min\\{h\\} = 0.5$.\n- $P(Q_{L,R}) = 1 - 10^{-20/10} = 1 - 10^{-2} = 0.99$. $\\min\\{P\\}=0.99$.\n- $S = 0.5 \\times 0.99 = 0.495$.\n- $0.495 < 0.7$. Classification: **False**.\n\nCase 6: $s_L=$ \"ACGTACGT\", $s_R=$ \"GTACGTAC\", $Q_L = 20$, $Q_R = 20$.\n- For both sequences, $n=8$, $c_A=c_C=c_G=c_T=2$. $p_b=2/8=0.25$.\n- This is analogous to Case 1 in terms of frequencies: $H(s_{L,R})=2$, $h(s_{L,R})=1$.\n- $\\min\\{h\\} = 1$.\n- $P(Q_{L,R}) = 1 - 10^{-20/10} = 0.99$. $\\min\\{P\\}=0.99$.\n- $S = 1 \\times 0.99 = 0.99$.\n- $0.99 \\ge 0.7$. Classification: **True**.\n\nThe final results are: [True, False, False, False, False, True].", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom collections import Counter\n\ndef calculate_normalized_entropy(s: str) -> float:\n    \"\"\"\n    Calculates the normalized Shannon entropy for a DNA sequence.\n    The entropy is normalized by log2(4) = 2.\n    \"\"\"\n    n = len(s)\n    if n == 0:\n        return 0.0\n\n    counts = Counter(s)\n    dna_bases = {'A', 'C', 'G', 'T'}\n    entropy = 0.0\n    \n    for base in dna_bases:\n        count = counts.get(base, 0)\n        if count > 0:\n            p_b = count / n\n            entropy -= p_b * np.log2(p_b)\n    \n    # Normalize by log2(4), which is 2.\n    h_s = entropy / 2.0\n    return h_s\n\ndef calculate_mapping_probability(Q: int) -> float:\n    \"\"\"\n    Converts a Phred-scaled mapping quality score to a probability of correct mapping.\n    \"\"\"\n    # The problem specifies Q as an integer (from the test cases), so we can treat it as such.\n    return 1.0 - 10**(-float(Q) / 10.0)\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test suite and prints the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each tuple is (s_L, s_R, Q_L, Q_R).\n    test_cases = [\n        (\"ACGTACGTACGT\", \"TGCATGCATGCA\", 40, 42),\n        (\"ATATATATATAT\", \"ATATATATATAT\", 10, 10),\n        (\"GATTACAGATTACA\", \"TCGATCGATCGA\", 35, 5),\n        (\"AAAAAAAAAAAA\", \"ACGTACGTACGT\", 60, 60),\n        (\"CG\", \"CG\", 20, 20),\n        (\"ACGTACGT\", \"GTACGTAC\", 20, 20),\n    ]\n\n    # The decision threshold tau.\n    tau = 0.7\n    \n    results = []\n    \n    for s_L, s_R, Q_L, Q_R in test_cases:\n        # Calculate normalized entropies for left and right flanks.\n        h_L = calculate_normalized_entropy(s_L)\n        h_R = calculate_normalized_entropy(s_R)\n        \n        # Calculate mapping probabilities for left and right flanks.\n        P_L = calculate_mapping_probability(Q_L)\n        P_R = calculate_mapping_probability(Q_R)\n        \n        # Determine the minimum of each pair of metrics.\n        min_h = min(h_L, h_R)\n        min_P = min(P_L, P_R)\n        \n        # Calculate the composite reliability score S.\n        S = min_h * min_P\n        \n        # Classify the alignment based on the threshold tau.\n        classification = S >= tau\n        results.append(classification)\n\n    # Final print statement in the exact required format.\n    # The str() of a bool in Python is 'True' or 'False'.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "2431938"}]}