{"hands_on_practices": [{"introduction": "The effectiveness of a compiler's optimization pipeline often depends critically on the sequence in which passes are executed. Some passes create new optimization opportunities for subsequent passes (an enabling relationship), while others might eliminate code that another pass could have used. This exercise provides a concrete model to explore these dependencies and quantify their impact. By analyzing a small set of interacting passes—Global Value Numbering (GVN), Loop-Invariant Code Motion (LICM), and Dead Code Elimination (DCE)—you will calculate the total optimization benefit for every possible ordering, revealing just how much performance can be gained or lost based on scheduling decisions. [@problem_id:3629247]", "problem": "A compiler pipeline consists of three optimization passes applied to a single procedure’s intermediate representation, executed exactly once each and in some order: Global Value Numbering (GVN), Loop-Invariant Code Motion (LICM), and Dead Code Elimination (DCE). The following foundational facts apply to these passes:\n\n- Global Value Numbering (GVN) identifies computations with identical value semantics and eliminates later redundant computations when an earlier computation’s value dominates the later one.\n- Loop-Invariant Code Motion (LICM) hoists computations whose operands do not change across iterations from loop bodies to a location that dominates the loop (such as a loop preheader).\n- Dead Code Elimination (DCE) removes instructions whose results are unused and which have no side effects, as determined by dataflow after previous transformations.\n\nConsider that the procedure contains independent code idioms partitioned into disjoint groups, each group contributing potential eliminated instructions subject to enabling relationships among passes. Let the group counts be:\n- Type $1$: $t_{1} = 7$ groups. Each group contains duplicate loop-invariant computations located in separate blocks that do not dominate one another initially. Only after $LICM$ hoists these computations to a common dominator can $GVN$ eliminate one redundant computation per group. Thus, elimination in these groups occurs if and only if $LICM$ precedes $GVN$ in the pipeline, and in that case exactly $1$ instruction per group is eliminated by $GVN$; otherwise $0$.\n- Type $2$: $t_{2} = 5$ groups. Each group contains a redundancy that $GVN$ can always eliminate, independent of $LICM$. Furthermore, the $GVN$ transformation exposes exactly one additional dead temporary per group, which $DCE$ can eliminate if and only if $DCE$ runs after $GVN$. Thus, each such group yields exactly $1$ instruction eliminated by $GVN$ in all orders, and yields an additional $1$ instruction eliminated by $DCE$ only when $DCE$ occurs after $GVN$.\n- Type $3$: $t_{3} = 4$ groups. Each group contains a loop-internal temporary whose uses are confined to the loop. After $LICM$ hoists the invariant producer out of the loop and rewrites uses, the temporary becomes unused. $DCE$ will eliminate this temporary only if $DCE$ runs after $LICM$. $GVN$ plays no role for these groups. Thus, elimination occurs if and only if $LICM$ precedes $DCE$, and in that case exactly $1$ instruction per group is eliminated by $DCE$; otherwise $0$.\n- Type $4$: $t_{4} = 6$ groups. Each group requires an enabling chain: $LICM$ must first hoist to establish dominance among duplicate computations; then $GVN$ eliminates one redundant computation; this in turn exposes a dead temporary that $DCE$ can eliminate only if $DCE$ runs after $GVN$. In these groups, if $LICM$ precedes $GVN$ and $DCE$ runs after $GVN$, exactly $2$ instructions per group are eliminated (one by $GVN$ and one by $DCE$). If $LICM$ precedes $GVN$ but $DCE$ runs before $GVN$, then only $1$ instruction per group is eliminated (by $GVN$). If $LICM$ does not precede $GVN$, then $0$ instructions are eliminated in the group.\n\nAssume all groups are disjoint and independent (no instruction belongs to more than one group), no side effects or undefined behavior interfere with elimination, and no pass creates new opportunities beyond the described enabling relationships. Consider all $6$ permutations of the three passes: $GVN \\rightarrow LICM \\rightarrow DCE$, $GVN \\rightarrow DCE \\rightarrow LICM$, $LICM \\rightarrow GVN \\rightarrow DCE$, $LICM \\rightarrow DCE \\rightarrow GVN$, $DCE \\rightarrow GVN \\rightarrow LICM$, and $DCE \\rightarrow LICM \\rightarrow GVN$.\n\nCompute the arithmetic mean of the total number of eliminated instructions over these $6$ permutations. Express your final answer as an exact number with no units. No rounding is required.", "solution": "The problem statement has been validated and is deemed sound. It is scientifically grounded in the principles of compiler optimization, well-posed with a clear objective and sufficient data, and free of contradictions or ambiguities. We may proceed with the solution.\n\nThe objective is to compute the arithmetic mean of the total number of eliminated instructions over all possible orderings of the three optimization passes: Global Value Numbering ($GVN$), Loop-Invariant Code Motion ($LICM$), and Dead Code Elimination ($DCE$). Let us denote the passes by $G$, $L$, and $D$ respectively. There are $3! = 6$ unique permutations of these three passes.\n\nThe counts of the disjoint instruction groups are given as:\n- Type $1$: $t_{1} = 7$\n- Type $2$: $t_{2} = 5$\n- Type $3$: $t_{3} = 4$\n- Type $4$: $t_{4} = 6$\n\nLet $pos(X)$ be the position ($1$, $2$, or $3$) of a pass $X$ in a given permutation. The conditions for instruction elimination for each group type can be formalized as follows:\n\n- **Type 1 ($t_1$ groups):** $1$ instruction is eliminated per group if and only if $pos(L) < pos(G)$. This yields $t_{1} \\times 1 = 7$ instructions. Otherwise, $0$ instructions are eliminated.\n- **Type 2 ($t_2$ groups):** $1$ instruction is always eliminated by $GVN$. An additional $1$ instruction is eliminated by $DCE$ if and only if $pos(D) > pos(G)$. This yields a total of $t_{2} \\times 1 = 5$ instructions if $pos(D) < pos(G)$, and $t_{2} \\times (1+1) = 10$ instructions if $pos(D) > pos(G)$.\n- **Type 3 ($t_3$ groups):** $1$ instruction is eliminated per group if and only if $pos(D) > pos(L)$. This yields $t_{3} \\times 1 = 4$ instructions. Otherwise, $0$ instructions are eliminated.\n- **Type 4 ($t_4$ groups):**\n    - If $pos(L) < pos(G)$ and $pos(G) < pos(D)$, a total of $2$ instructions are eliminated per group. This yields $t_{4} \\times 2 = 12$ instructions.\n    - If $pos(L) < pos(G)$ and $pos(D) < pos(G)$, a total of $1$ instruction is eliminated per group. This yields $t_{4} \\times 1 = 6$ instructions.\n    - If $pos(L) > pos(G)$, $0$ instructions are eliminated.\n\nWe will now systematically calculate the total number of eliminated instructions, $E_i$, for each of the $6$ permutations.\n\n1.  **Permutation $P_1: G \\rightarrow L \\rightarrow D$** ($pos(G)=1, pos(L)=2, pos(D)=3$)\n    - Type 1: $pos(L) > pos(G)$. Eliminations: $0$.\n    - Type 2: $pos(D) > pos(G)$. Eliminations: $t_2 \\times (1+1) = 5 \\times 2 = 10$.\n    - Type 3: $pos(D) > pos(L)$. Eliminations: $t_3 \\times 1 = 4$.\n    - Type 4: $pos(L) > pos(G)$. Eliminations: $0$.\n    - Total $E_1 = 0 + 10 + 4 + 0 = 14$.\n\n2.  **Permutation $P_2: G \\rightarrow D \\rightarrow L$** ($pos(G)=1, pos(D)=2, pos(L)=3$)\n    - Type 1: $pos(L) > pos(G)$. Eliminations: $0$.\n    - Type 2: $pos(D) > pos(G)$. Eliminations: $t_2 \\times (1+1) = 10$.\n    - Type 3: $pos(D) < pos(L)$. Eliminations: $0$.\n    - Type 4: $pos(L) > pos(G)$. Eliminations: $0$.\n    - Total $E_2 = 0 + 10 + 0 + 0 = 10$.\n\n3.  **Permutation $P_3: D \\rightarrow G \\rightarrow L$** ($pos(D)=1, pos(G)=2, pos(L)=3$)\n    - Type 1: $pos(L) > pos(G)$. Eliminations: $0$.\n    - Type 2: $pos(D) < pos(G)$. Eliminations: $t_2 \\times 1 = 5$.\n    - Type 3: $pos(D) < pos(L)$. Eliminations: $0$.\n    - Type 4: $pos(L) > pos(G)$. Eliminations: $0$.\n    - Total $E_3 = 0 + 5 + 0 + 0 = 5$.\n\n4.  **Permutation $P_4: L \\rightarrow G \\rightarrow D$** ($pos(L)=1, pos(G)=2, pos(D)=3$)\n    - Type 1: $pos(L) < pos(G)$. Eliminations: $t_1 \\times 1 = 7$.\n    - Type 2: $pos(D) > pos(G)$. Eliminations: $t_2 \\times (1+1) = 10$.\n    - Type 3: $pos(D) > pos(L)$. Eliminations: $t_3 \\times 1 = 4$.\n    - Type 4: $pos(L) < pos(G)$ and $pos(G) < pos(D)$. Eliminations: $t_4 \\times 2 = 6 \\times 2 = 12$.\n    - Total $E_4 = 7 + 10 + 4 + 12 = 33$.\n\n5.  **Permutation $P_5: L \\rightarrow D \\rightarrow G$** ($pos(L)=1, pos(D)=2, pos(G)=3$)\n    - Type 1: $pos(L) < pos(G)$. Eliminations: $t_1 \\times 1 = 7$.\n    - Type 2: $pos(D) < pos(G)$. Eliminations: $t_2 \\times 1 = 5$.\n    - Type 3: $pos(D) > pos(L)$. Eliminations: $t_3 \\times 1 = 4$.\n    - Type 4: $pos(L) < pos(G)$ and $pos(D) < pos(G)$. Eliminations: $t_4 \\times 1 = 6$.\n    - Total $E_5 = 7 + 5 + 4 + 6 = 22$.\n\n6.  **Permutation $P_6: D \\rightarrow L \\rightarrow G$** ($pos(D)=1, pos(L)=2, pos(G)=3$)\n    - Type 1: $pos(L) < pos(G)$. Eliminations: $t_1 \\times 1 = 7$.\n    - Type 2: $pos(D) < pos(G)$. Eliminations: $t_2 \\times 1 = 5$.\n    - Type 3: $pos(D) < pos(L)$. Eliminations: $0$.\n    - Type 4: $pos(L) < pos(G)$ and $pos(D) < pos(G)$. Eliminations: $t_4 \\times 1 = 6$.\n    - Total $E_6 = 7 + 5 + 0 + 6 = 18$.\n\nThe total number of eliminated instructions over all $6$ permutations is the sum of the totals for each permutation:\n$$ E_{\\text{total}} = E_1 + E_2 + E_3 + E_4 + E_5 + E_6 $$\n$$ E_{\\text{total}} = 14 + 10 + 5 + 33 + 22 + 18 = 102 $$\nThe arithmetic mean, $\\mu$, is this total sum divided by the number of permutations, which is $6$:\n$$ \\mu = \\frac{E_{\\text{total}}}{6} = \\frac{102}{6} $$\n$$ \\mu = 17 $$\nThe arithmetic mean of the total number of eliminated instructions is $17$.", "answer": "$$\\boxed{17}$$", "id": "3629247"}, {"introduction": "While the previous exercise demonstrated how sensitive compilers are to pass ordering, this raises an important question: are there passes that can be reordered freely without changing the program's final semantics? This property, known as commutativity, is highly desirable because it simplifies scheduling and can even enable parallel execution of passes. This practice challenges you to reason about the fundamental behavior of several common compiler passes, analyzing their interactions to determine which pairs commute. [@problem_id:3629191] Successfully identifying these pairs is a key skill in designing flexible and efficient compiler pipelines.", "problem": "A compiler organizes its pipeline as a sequence of passes, each modeled as a pure function on a program’s Intermediate Representation (IR). Consider an IR that is a directed Control Flow Graph (CFG), where each node is a basic block containing a list of instructions, and uses Static Single Assignment (SSA) form for virtual registers. Control Flow Graph (CFG) and Static Single Assignment (SSA) are defined as follows: the CFG is a graph of basic blocks connected by control-flow edges, and SSA form assigns each variable exactly once with explicit $\\phi$-nodes for merges. A pass is a total function $p$ that maps IR graphs to IR graphs. Two passes $p$ and $q$ commute if for all IR inputs $I$,\n$$(p \\circ q)(I) \\equiv (q \\circ p)(I),$$\nwhere $\\equiv$ denotes semantic equivalence modulo alpha-renaming of SSA names and relabeling of basic blocks (that is, graph isomorphism preserving control-flow and instruction multiset).\n\nAssume deterministic, well-defined program semantics and the following pass specifications. Use these as the fundamental base:\n\n- Variable Renaming (VR): capture-avoiding alpha-renaming of SSA virtual registers by a bijection on names. VR does not change the CFG, instruction opcodes, or def-use edges except for renaming identifiers. It preserves all dataflow facts up to renaming.\n- Block Relabeling and Layout (BRL): permutation of basic block identifiers and linear order. BRL does not change CFG edges, instruction sequences within a block, or any instruction content; it changes only labels and layout metadata.\n- Local Constant Folding (LCF): for any instruction whose operands are all literal constants, replace the instruction with the computed literal. LCF does not add or remove blocks or control-flow edges; it replaces certain instruction nodes with literals but does not propagate constants across instructions.\n- Unreachable Code Elimination (UCE): removes basic blocks not reachable from the entry block in the current CFG. UCE does not rewrite instructions in reachable blocks; it deletes unreachable blocks and any edges incident to them.\n- Constant Propagation (CP): propagates compile-time constants along def-use chains using dominance, replacing uses with literal constants when definitions are literal and intervening redefinitions do not occur. CP does not change the CFG.\n\nA scheduler wants to run commuting pass pairs in parallel without changing semantics. If $p$ and $q$ commute, any parallel execution that is equivalent to committing either $p$ then $q$ or $q$ then $p$ is considered semantics-preserving. Under the pass definitions above, which option(s) list only commuting pass pairs that can be safely scheduled in parallel in this sense?\n\nChoose all that apply.\n\n- A. The pairs $\\{$(LCF, UCE), (BRL, VR)$\\}$\n\n- B. The pair $\\{$(CP, Dead Branch Elimination)$\\}$, where Dead Branch Elimination replaces a conditional branch with a direct jump when the condition is a literal boolean and deletes the now-unreachable successor and its descendants.\n\n- C. The pairs $\\{$(LCF, CP), (UCE, Dead Branch Elimination)$\\}$\n\n- D. The pairs $\\{$(UCE, BRL), (LCF, BRL)$\\}$\n\n- E. The pairs $\\{$(VR, LCF), (VR, UCE)$\\}$\n\nYour reasoning should start from the formal definitions above and not assume undocumented behaviors. For incorrect choices, it suffices to exhibit a single counterexample IR $I$ where $(p \\circ q)(I) \\not\\equiv (q \\circ p)(I)$, or to identify a violated independence principle. For correct choices, argue from first principles (read/write footprints, invariances, and graph isomorphism) that commutativity holds for all $I$ and thus a parallel schedule that serializes to either order preserves semantics.", "solution": "### Step 1: Extract Givens\n\nThe problem statement provides the following definitions and conditions:\n- **Intermediate Representation (IR)**: A directed Control Flow Graph (CFG) where nodes are basic blocks and virtual registers are in Static Single Assignment (SSA) form.\n- **CFG**: A graph of basic blocks connected by control-flow edges.\n- **SSA**: Each variable is assigned exactly once, with explicit $\\phi$-nodes for merges.\n- **Pass**: A total function $p$ mapping an IR graph to another IR graph.\n- **Commutativity**: Two passes $p$ and $q$ commute if for all IR inputs $I$, $(p \\circ q)(I) \\equiv (q \\circ p)(I)$.\n- **Equivalence ($\\equiv$)**: Semantic equivalence defined as graph isomorphism of the CFG that preserves control-flow and the multiset of instructions in each block, modulo alpha-renaming of SSA names and relabeling of basic blocks.\n- **Pass Specifications**:\n    - **Variable Renaming (VR)**: Alpha-renames SSA virtual registers. Does not change the CFG, opcodes, or def-use edges (except for renaming).\n    - **Block Relabeling and Layout (BRL)**: Permutes basic block identifiers and their linear order. Does not change CFG edges or instructions.\n    - **Local Constant Folding (LCF)**: For an instruction with all literal constant operands, it is replaced by the computed literal result. Does not change the CFG. Does not propagate constants.\n    - **Unreachable Code Elimination (UCE)**: Removes basic blocks not reachable from the entry block, along with incident edges. Does not rewrite instructions in reachable blocks.\n    - **Constant Propagation (CP)**: Replaces uses of a variable with a literal constant if its definition is that literal constant and no redefinitions occur on the path. Does not change the CFG.\n- **Scheduling Goal**: To run commuting pass pairs in parallel.\n\n### Step 2: Validate Using Extracted Givens\n\n- **Scientifically Grounded**: The problem is well-grounded in the principles of compiler construction. The concepts of IR, CFG, SSA, and compiler passes like constant folding and dead code elimination are standard material in this domain.\n- **Well-Posed**: The problem is well-posed. It provides formal definitions for passes and a precise, albeit strict, definition of equivalence ($\\equiv$). The question asks for an evaluation of specific pairs against this formal definition of commutativity. A unique set of correct options can be determined.\n- **Objective**: The problem is stated using formal, objective language without ambiguity or subjective claims.\n\nThe problem statement is self-contained and internally consistent. The definitions, while simplified, are sufficient for rigorous analysis. A new pass, Dead Branch Elimination, is introduced within the options, but its definition is provided, so it can be analyzed in that context. The validation applies to the core setup, which is sound.\n\n### Step 3: Verdict and Action\n\nThe problem statement is **valid**. I will proceed with deriving the solution.\n\n### Solution Derivation\n\nTo determine if two passes $p$ and $q$ commute, we must check if for any IR $I$, the application $(p \\circ q)(I)$ results in an IR that is equivalent to $(q \\circ p)(I)$ under the given definition of $\\equiv$. This equivalence requires that the resulting CFGs are isomorphic and that the multisets of instructions in corresponding basic blocks are identical, allowing for different SSA variable names (alpha-renaming) and basic block labels.\n\nLet's analyze the pairs in each option.\n\n**A. The pairs $\\{$(LCF, UCE), (BRL, VR)$\\}$**\n\n- **Pair ($LCF$, $UCE$)**:\n    - $LCF$ modifies instructions within basic blocks but does not alter the CFG structure (edges or blocks).\n    - $UCE$ removes blocks and edges from the CFG but does not alter instructions within the reachable blocks.\n    - Let's analyze the compositions. Let $I_r$ be the set of reachable blocks and instructions in $I$, and $I_u$ be the unreachable part.\n    - $(UCE \\circ LCF)(I)$: $LCF$ acts on $I$, producing $LCF(I)$. Since $LCF$ does not change the CFG, the set of reachable blocks in $LCF(I)$ is the same as in $I$. $UCE$ then removes the unreachable portion, resulting in an IR containing only the reachable blocks, with instructions modified by $LCF$. The result is effectively $LCF(I_r)$.\n    - $(LCF \\circ UCE)(I)$: $UCE$ acts on $I$, removing $I_u$ to produce $I_r$. $LCF$ then acts on $I_r$, producing $LCF(I_r)$.\n    - Since $(UCE \\circ LCF)(I)$ and $(LCF \\circ UCE)(I)$ both produce $LCF(I_r)$, the resulting IRs are not just equivalent but identical. Thus, $LCF$ and $UCE$ commute.\n\n- **Pair ($BRL$, $VR$)**:\n    - $BRL$ modifies basic block identifiers and layout.\n    - $VR$ modifies SSA virtual register names.\n    - These passes operate on completely disjoint aspects of the IR. $BRL$ affects CFG node metadata, while $VR$ affects instruction-level operand/definition names. The action of one does not change the set of items the other acts upon.\n    - $(VR \\circ BRL)(I)$: First, relabel blocks. Second, rename registers.\n    - $(BRL \\circ VR)(I)$: First, rename registers. Second, relabel blocks.\n    - The final IR in both cases has the same CFG structure, the same instructions, but with relabeled blocks and renamed registers. The final result is the same regardless of order. Thus, $BRL$ and $VR$ commute.\n\n- **Verdict on A**: Both pairs commute. **Correct**.\n\n\n**B. The pair $\\{$(CP, Dead Branch Elimination)$\\}$**\n\n- **Pair ($CP$, $DBE$)**:\n    - $CP$ propagates constants, which can change a variable in a conditional branch's condition to a literal constant.\n    - $DBE$ is defined as simplifying a conditional branch whose condition is a literal, and then removing the newly unreachable code. This means $CP$ can create opportunities for $DBE$.\n    - Let's construct a counterexample. Consider the IR $I$:\n    $$\n    B_1: x = \\text{true} \\\\\n    \\quad \\text{goto } B_2 \\\\\n    B_2: \\text{if } (x)\\text{ goto } B_3 \\text{ else goto } B_4\n    $$\n    - $(DBE \\circ CP)(I)$:\n        1. $CP(I)$: Propagates $x = \\text{true}$. The branch in $B_2$ becomes `if (true) goto $B_3$ else goto $B_4$`.\n        2. $DBE$ acts on this modified IR. It simplifies the branch to an unconditional `goto $B_3$` and deletes the now-unreachable block $B_4$ and its associated edge. The final CFG does not contain $B_4$.\n    - $(CP \\circ DBE)(I)$:\n        1. $DBE(I)$: The condition $x$ in the branch `if (x)` is not a literal constant. $DBE$ does nothing. The IR is unchanged.\n        2. $CP$ acts on the original IR. It propagates $x = \\text{true}$, so the branch becomes `if (true) goto $B_3$ else goto $B_4$`. Block $B_4$ and the edge to it remain in the CFG.\n    - The resulting CFGs are not isomorphic, as one contains block $B_4$ and the other does not. Therefore, $(DBE \\circ CP)(I) \\not\\equiv (CP \\circ DBE)(I)$. They do not commute.\n\n- **Verdict on B**: The pair does not commute. **Incorrect**.\n\n\n**C. The pairs $\\{$(LCF, CP), (UCE, Dead Branch Elimination)$\\}$**\n\n- **Pair ($LCF$, $CP$)**:\n    - $LCF$ can create new constant definitions, which $CP$ can then propagate. This suggests a dependency.\n    - Consider the IR $I$ in a single basic block:\n    $$\n    x = 2 + 3 \\\\\n    y = x\n    $$\n    - $(CP \\circ LCF)(I)$:\n        1. $LCF(I)$: The instruction `x = 2 + 3` is folded into `x = 5`. The IR becomes `x = 5; y = x`.\n        2. $CP$ acts on this. The definition of $x$ is the literal $5$, so it propagates this to the use in `y = x`. The instruction becomes `y = 5`. The final instruction multiset is $\\{ `x = 5`, `y = 5` \\}$.\n    - $(LCF \\circ CP)(I)$:\n        1. $CP(I)$: The definition of $x$ is an expression, not a literal. $CP$ does nothing. The IR is unchanged.\n        2. $LCF$ acts on this. It folds `x = 2 + 3` into `x = 5`. The instruction `y = x` is not affected. The final instruction multiset is $\\{ `x = 5`, `y = x` \\}$.\n    - The instruction multisets are different: $\\{ `x=5`, `y=5` \\}$ versus $\\{ `x=5`, `y=x` \\}$. According to the strict definition of equivalence $\\equiv$, which requires preserving the instruction multiset, the results are not equivalent. Thus, $LCF$ and $CP$ do not commute.\n\n- **Verdict on C**: Since the first pair does not commute, the option is incorrect. We need not analyze the second pair. **Incorrect**.\n\n\n**D. The pairs $\\{$(UCE, BRL), (LCF, BRL)$\\}$**\n\n- **Pair ($UCE$, $BRL$)**:\n    - $UCE$ depends on CFG connectivity. $BRL$ changes block labels but explicitly \"does not change CFG edges\".\n    - $(BRL \\circ UCE)(I)$: $UCE$ removes unreachable blocks, resulting in a subgraph $I_r$. $BRL$ then relabels the blocks in $I_r$.\n    - $(UCE \\circ BRL)(I)$: $BRL$ first relabels all blocks in $I$, creating an isomorphic graph $I'$. $UCE$ then operates on $I'$. Since reachability is a graph-structural property, $UCE$ will remove the blocks in $I'$ that correspond to the unreachable blocks in $I$. The resulting graph is isomorphic to $I_r$, just with a different labeling.\n    - Since equivalence $\\equiv$ is defined modulo relabeling of basic blocks, the two results are equivalent. Thus, $UCE$ and $BRL$ commute.\n\n- **Pair ($LCF$, $BRL$)**:\n    - $LCF$ modifies instructions inside blocks. $BRL$ modifies the labels of blocks. These are orthogonal operations. $LCF$'s operation does not depend on block labels, and $BRL$'s operation does not depend on the content of instructions.\n    - $(BRL \\circ LCF)(I)$: Fold constants, then relabel blocks.\n    - $(LCF \\circ BRL)(I)$: Relabel blocks, then fold constants.\n    - The final instruction multisets within corresponding blocks will be identical. The final CFG structures will be identical. The only difference could be the block labels, which is permitted by the equivalence relation $\\equiv$. Thus, $LCF$ and $BRL$ commute.\n\n- **Verdict on D**: Both pairs commute. **Correct**.\n\n\n**E. The pairs $\\{$(VR, LCF), (VR, UCE)$\\}$**\n\n- **Pair ($VR$, $LCF$)**:\n    - $VR$ renames SSA registers.\n    - $LCF$ acts on instructions whose operands are \"all literal constants\". SSA registers are not literal constants, even if they hold a constant value. Therefore, the operations of $VR$ and $LCF$ are disjoint. $VR$ reads/writes register names, while $LCF$ reads/writes instructions defined on literals.\n    - $(LCF \\circ VR)(I)$: Rename registers first. $LCF$ then runs and is unaffected by the renaming.\n    - $(VR \\circ LCF)(I)$: $LCF$ runs first. $VR$ then renames registers, which is unaffected by the folding LCF may have performed.\n    - The resulting IRs are equivalent modulo alpha-renaming. They commute.\n\n- **Pair ($VR$, $UCE$)**:\n    - $VR$ renames registers. It does not change the CFG.\n    - $UCE$ removes unreachable blocks.\n    - $(UCE \\circ VR)(I)$: $VR$ renames registers throughout the program $I$, producing $I_v$. $UCE$ then runs on $I_v$. As the CFG was not changed by $VR$, $UCE$ removes the same set of blocks as it would from $I$.\n    - $(VR \\circ UCE)(I)$: $UCE$ first removes unreachable blocks from $I$, producing $I_r$. $VR$ then renames the registers in the remaining reachable code $I_r$.\n    - The result of $(UCE \\circ VR)(I)$ is the reachable subgraph with renamed registers. The result of $(VR \\circ UCE)(I)$ is the same reachable subgraph, with its registers also renamed. The two final IRs are equivalent under alpha-renaming. Thus, $VR$ and $UCE$ commute.\n\n- **Verdict on E**: Both pairs commute. **Correct**.", "answer": "$$\\boxed{ADE}$$", "id": "3629191"}, {"introduction": "Moving from theoretical analysis to practical implementation, we now address the challenge of building a robust system to manage and apply sequences of passes. A real-world compiler must ensure that the Intermediate Representation (IR) remains valid and correct after each transformation. This coding exercise guides you through implementing a transactional pass manager, a mechanism that applies a batch of passes atomically. By using a verifier to check the result before committing it, this approach guarantees that complex transformations either succeed completely or are rolled back, preserving the integrity of the IR and preventing compilation errors. [@problem_id:3629198]", "problem": "You are asked to formalize and implement a transactional compilation pass manager. The Intermediate Representation (IR) is modeled as a finite sequence $S = [s_0, s_1, \\dots, s_{n-1}]$ of integers. A compilation pass is defined as a deterministic total function $p: \\mathbb{Z}^n \\rightarrow \\mathbb{Z}^n$ on fixed-length sequences, and a batch is the composition $P = p_k \\circ p_{k-1} \\circ \\cdots \\circ p_1$ of $k$ passes. The verifier is a predicate $V: \\mathbb{Z}^n \\rightarrow \\{\\text{true}, \\text{false}\\}$ that checks IR invariants. The transactional semantics require that a batch is applied atomically to $S$: compute $S' = P(S)$ on a scratch copy, evaluate $V(S')$, and if $V(S') = \\text{true}$, commit $S := S'$, otherwise discard $S'$ and leave $S$ unchanged.\n\nYou must derive from foundational definitions the algorithmic organization of such a pass manager and implement it in the C programming language following these constraints:\n- Model the IR as a sequence of signed $32$-bit integers.\n- Implement the following pass primitives, each as a deterministic function that preserves sequence length:\n  - Add-constant: $p_{\\text{add}}(S; k)$ produces $[s_0 + k, s_1 + k, \\dots, s_{n-1} + k]$, where $k \\in \\mathbb{Z}$.\n  - Multiply-constant: $p_{\\text{mul}}(S; m)$ produces $[m \\cdot s_0, m \\cdot s_1, \\dots, m \\cdot s_{n-1}]$, where $m \\in \\mathbb{Z}$.\n  - Clamp-to-range: $p_{\\text{clamp}}(S; a,b)$ produces $[\\min(\\max(s_0, a), b), \\dots, \\min(\\max(s_{n-1}, a), b)]$ with $a \\le b$, $a,b \\in \\mathbb{Z}$.\n  - Sort-ascending: $p_{\\text{sort}}(S)$ produces the non-decreasing reordering of $S$.\n- Implement a verifier $V$ parameterized by four integers $(\\text{min\\_elem}, \\text{max\\_elem}, \\text{max\\_sum}, \\text{require\\_sorted})$ where $\\text{require\\_sorted} \\in \\{0,1\\}$. The verifier returns $\\text{true}$ if and only if:\n  - For all indices $i \\in \\{0,1,\\dots,n-1\\}$, $s_i \\in [\\text{min\\_elem}, \\text{max\\_elem}]$, i.e., $\\text{min\\_elem} \\le s_i \\le \\text{max\\_elem}$.\n  - $\\sum_{i=0}^{n-1} s_i \\le \\text{max\\_sum}$.\n  - If $\\text{require\\_sorted} = 1$, then $s_i \\le s_{i+1}$ holds for all $i \\in \\{0,1,\\dots,n-2\\}$.\n\nStarting from these definitions, implement the transactional pass manager that applies batches to the IR as atomic transactions. No external inputs are allowed; the program must instantiate and run the following test suite of $5$ cases:\n\n- Test case $1$ (general happy path):\n  - Initial IR $S_1 = [1,2,3,4]$.\n  - Batch $B_1 = [p_{\\text{add}}(\\cdot; 1), p_{\\text{mul}}(\\cdot; 2), p_{\\text{clamp}}(\\cdot; 0, 9), p_{\\text{sort}}(\\cdot)]$ applied in this order.\n  - Verifier parameters $V_1$: $\\text{min\\_elem} = 0$, $\\text{max\\_elem} = 9$, $\\text{max\\_sum} = 30$, $\\text{require\\_sorted} = 1$.\n\n- Test case $2$ (rollback due to sum overflow):\n  - Initial IR $S_2 = [5,5,5]$.\n  - Batch $B_2 = [p_{\\text{mul}}(\\cdot; 3)]$.\n  - Verifier parameters $V_2$: $\\text{min\\_elem} = 0$, $\\text{max\\_elem} = 100$, $\\text{max\\_sum} = 40$, $\\text{require\\_sorted} = 0$.\n\n- Test case $3$ (edge case: empty IR):\n  - Initial IR $S_3 = []$.\n  - Batch $B_3 = [p_{\\text{add}}(\\cdot; 10), p_{\\text{sort}}(\\cdot)]$.\n  - Verifier parameters $V_3$: $\\text{min\\_elem} = 0$, $\\text{max\\_elem} = 100$, $\\text{max\\_sum} = 0$, $\\text{require\\_sorted} = 1$.\n\n- Test case $4$ (boundary commit at exact limits):\n  - Initial IR $S_4 = [9]$.\n  - Batch $B_4 = [p_{\\text{add}}(\\cdot; 1), p_{\\text{clamp}}(\\cdot; 0, 10), p_{\\text{sort}}(\\cdot)]$.\n  - Verifier parameters $V_4$: $\\text{min\\_elem} = 0$, $\\text{max\\_elem} = 10$, $\\text{max\\_sum} = 10$, $\\text{require\\_sorted} = 1$.\n\n- Test case $5$ (rollback due to unsorted requirement):\n  - Initial IR $S_5 = [0,5,3]$.\n  - Batch $B_5 = [p_{\\text{add}}(\\cdot; 0)]$.\n  - Verifier parameters $V_5$: $\\text{min\\_elem} = 0$, $\\text{max\\_elem} = 100$, $\\text{max\\_sum} = 100$, $\\text{require\\_sorted} = 1$.\n\nYour program must apply each batch transactionally to its corresponding initial IR under the given verifier, computing the final IR $S_i^{\\text{final}}$ for each test case $i \\in \\{1,2,3,4,5\\}$. The final output must be a single line containing the list of results as a comma-separated list of lists of integers, enclosed in square brackets, with no spaces. For the above suite, your program should produce exactly one line in the following format:\n\"[[4,6,8,9],[5,5,5],[],[10],[0,5,3]]\".", "solution": "The problem requires the formalization and implementation of a transactional compilation pass manager. Before proceeding to a solution, a rigorous validation of the problem statement is necessary.\n\n### Step 1: Extract Givens\n\n- **Intermediate Representation (IR)**: A finite sequence $S = [s_0, s_1, \\dots, s_{n-1}]$ of integers. In the implementation, these are signed $32$-bit integers.\n- **Compilation Pass**: A deterministic total function $p: \\mathbb{Z}^n \\rightarrow \\mathbb{Z}^n$ that preserves sequence length $n$.\n- **Pass Batch**: The composition of $k$ passes, $P = p_k \\circ p_{k-1} \\circ \\cdots \\circ p_1$.\n- **Verifier**: A predicate $V: \\mathbb{Z}^n \\rightarrow \\{\\text{true}, \\text{false}\\}$.\n- **Transactional Semantics**: A batch $P$ is applied atomically. A scratch copy $S'$ is created, $S' = P(S)$. If $V(S')$ is true, the original IR is updated: $S := S'$. Otherwise, $S'$ is discarded and $S$ remains unchanged.\n- **Pass Primitives**:\n    1.  $p_{\\text{add}}(S; k)$: Produces $[s_0 + k, \\dots, s_{n-1} + k]$.\n    2.  $p_{\\text{mul}}(S; m)$: Produces $[m \\cdot s_0, \\dots, m \\cdot s_{n-1}]$.\n    3.  $p_{\\text{clamp}}(S; a,b)$: Produces $[\\min(\\max(s_i, a), b), \\dots]$ for $a \\le b$.\n    4.  $p_{\\text{sort}}(S)$: Produces a non-decreasing reordering of $S$.\n- **Verifier Specification**: $V(\\cdot; \\text{min\\_elem}, \\text{max\\_elem}, \\text{max\\_sum}, \\text{require\\_sorted})$ is true if and only if:\n    1.  $\\forall i \\in \\{0, \\dots, n-1\\}, s_i \\in [\\text{min\\_elem}, \\text{max\\_elem}]$.\n    2.  $\\sum_{i=0}^{n-1} s_i \\le \\text{max\\_sum}$.\n    3.  If $\\text{require\\_sorted} = 1$, then $\\forall i \\in \\{0, \\dots, n-2\\}, s_i \\le s_{i+1}$.\n- **Test Suite**: Five test cases are provided, each with an initial IR, a batch of passes, and verifier parameters.\n    - **Case 1**: $S_1 = [1,2,3,4]$, $B_1 = [p_{\\text{add}}(;1), p_{\\text{mul}}(;2), p_{\\text{clamp}}(;0,9), p_{\\text{sort}}]$, $V_1(\\cdot; 0, 9, 30, 1)$.\n    - **Case 2**: $S_2 = [5,5,5]$, $B_2 = [p_{\\text{mul}}(;3)]$, $V_2(\\cdot; 0, 100, 40, 0)$.\n    - **Case 3**: $S_3 = []$, $B_3 = [p_{\\text{add}}(;10), p_{\\text{sort}}]$, $V_3(\\cdot; 0, 100, 0, 1)$.\n    - **Case 4**: $S_4 = [9]$, $B_4 = [p_{\\text{add}}(;1), p_{\\text{clamp}}(;0,10), p_{\\text{sort}}]$, $V_4(\\cdot; 0, 10, 10, 1)$.\n    - **Case 5**: $S_5 = [0,5,3]$, $B_5 = [p_{\\text{add}}(;0)]$, $V_5(\\cdot; 0, 100, 100, 1)$.\n- **Output Format**: A single line containing a comma-separated list of the final IRs for each test case, e.g., `\"[[...],[...]]\"`.\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem statement is evaluated against the validation criteria.\n\n- **Scientifically Grounded**: The problem is an exercise in algorithmic design, using concepts from compiler construction (IR, passes) as a well-defined mathematical abstraction. The operations are based on standard integer arithmetic and sequence manipulation. All definitions are formal and logically sound.\n- **Well-Posed**: The problem is exceptionally well-posed. For each test case, the initial state, the sequence of deterministic transformations, and the verification predicate are all explicitly and unambiguously defined. This guarantees the existence of a unique and computable solution for each case.\n- **Objective**: The problem uses precise mathematical language (e.g., \"deterministic total function\", \"predicate\", \"composition\") and provides concrete numerical values for all parameters and initial conditions. There are no subjective or ambiguous statements.\n- **Completeness and Consistency**: The problem provides all necessary information. The definitions for passes, the verifier, and the transactional semantics are self-contained and consistent. The properties of the empty sequence ($S_3$) are handled correctly and consistently by the mathematical definitions (e.g., the sum of elements is $0$, and it is vacuously sorted).\n\n### Step 3: Verdict and Action\n\nThe problem is **valid**. It is a well-defined, self-contained, and logically sound computer science problem that can be solved algorithmically. We will now proceed with deriving and implementing the solution.\n\n### Algorithmic Design and Implementation\n\nThe solution will be structured based on a direct translation of the formal definitions into C language constructs.\n\n**1. Data Structures**\n\n-   **Intermediate Representation (IR)**: The sequence $S = [s_0, s_1, \\dots, s_{n-1}]$ of signed $32$-bit integers is modeled by a `struct IR` containing a pointer to a dynamically allocated array and its length. Per the problem constraints, the C `int` type will be used, assuming it is $32$-bit, as `<stdint.h>` is disallowed.\n    ```c\n    typedef struct {\n        int* data;\n        size_t len;\n    } IR;\n    ```\n-   **Pass Primitives**: To represent a heterogeneous collection of passes, each with different parameters, we define an enumeration for the pass type and a structure to hold an instance of a pass.\n    ```c\n    typedef enum { PASS_ADD, PASS_MUL, PASS_CLAMP, PASS_SORT } PassType;\n    typedef struct {\n        PassType type;\n        int param1; // For k, m, or a\n        int param2; // For b\n    } Pass;\n    ```\n-   **Pass Batch and Verifier**: A batch is a sequence of passes, and verifier parameters are a set of integers. These are modeled with simple structures.\n    ```c\n    typedef struct {\n        const Pass* passes;\n        size_t count;\n    } PassBatch;\n    typedef struct {\n        int min_elem;\n        int max_elem;\n        int max_sum;\n        int require_sorted; // 0 or 1\n    } VerifierParams;\n    ```\n\n**2. Transactional Pass Manager Logic**\n\nThe core of the problem is the atomic application of a pass batch. This transactional behavior is implemented as follows for a given initial IR $S$, batch $P$, and verifier parameters $V$:\n1.  **Allocate Scratch Space**: Create a scratch IR, $S'$, by allocating new memory and making a deep copy of the data from $S$. The `malloc` and `memcpy` functions are suitable for this.\n2.  **Apply Batch**: Iterate through the passes in the batch $P = p_k \\circ \\cdots \\circ p_1$ and apply each function $p_j$ to the scratch IR $S'$ in sequence. Each pass modifies $S'$ in-place.\n3.  **Verify**: Invoke the `verify` function on the transformed scratch IR, $S'$, using the parameters from $V$. This function returns true ($1$) or false ($0$).\n4.  **Commit or Rollback**:\n    -   If verification succeeds (returns $1$), the changes are committed. This is achieved by copying the data from $S'$ back into the original IR's data buffer $S$.\n    -   If verification fails (returns $0$), the changes are rolled back. This is a null operation, as the original IR $S$ was never modified. The scratch copy $S'$ is simply discarded.\n5.  **Cleanup**: Deallocate the memory used by the scratch IR $S'$.\n\n**3. Pass Primitive Implementation**\n\nEach pass is a C function that modifies an `IR` struct's data in-place.\n-   $p_{\\text{add}}(S; k)$: A loop iterates from $i=0$ to $n-1$, performing `S.data[i] += k;`.\n-   $p_{\\text{mul}}(S; m)$: A loop iterates from $i=0$ to $n-1$, performing `S.data[i] *= m;`.\n-   $p_{\\text{clamp}}(S; a,b)$: A loop iterates from $i=0$ to $n-1$. For each element $s_i$, it applies `s_i = (s_i < a) ? a : s_i;` followed by `s_i = (s_i > b) ? b : s_i;`. This implements the $\\min(\\max(s_i, a), b)$ logic.\n-   $p_{\\text{sort}}(S)$: This function uses the standard library's `qsort`. A helper comparison function `int compare_ints(const void* a, const void* b)` is required, which returns a negative, zero, or positive value based on the relative order of the two integers being compared.\n\n**4. Verifier Implementation**\n\nThe verifier is a predicate function `int verify(const IR* ir, const VerifierParams* params)` that checks three conditions.\n1.  **Element Range**: It iterates from $i=0$ to $n-1$, returning $0$ immediately if any element $s_i$ is found such that $s_i < \\text{min\\_elem}$ or $s_i > \\text{max\\_elem}$.\n2.  **Sum Constraint**: It computes the sum of all elements. A `long long` accumulator is used to prevent integer overflow during summation. If the final sum exceeds `max_sum`, it returns $0$.\n3.  **Sortedness**: If `require_sorted` is $1$, it iterates from $i=0$ to $n-2$, returning $0$ if it finds any adjacent pair where $s_i > s_{i+1}$.\n\nFor an empty IR ($n=0$), all loops are vacuous and thus their conditions are satisfied. The sum is $0$. The predicate will evaluate correctly based on the `max_sum` parameter.\n\n**5. Main Program Execution**\n\nThe `main` function serves as the test harness.\n1.  It defines the initial data arrays, pass batches, and verifier parameters for all five test cases.\n2.  It groups these into an array of `IR` structs and corresponding arrays for batches and verifiers.\n3.  It iterates through each test case, applying the transactional logic described above. The original `IR` data arrays are modified in-place upon a successful commit.\n4.  After processing all test cases, a final loop prints the resulting IRs in the precise, specified format: `[[elem1,elem2,...],[...],...]`. This requires careful control of `printf` calls to place brackets `[]` and commas `,` correctly, and to handle the empty list `[]` case gracefully.", "answer": "```c\n// The complete and compilable C program goes here.\n// Headers must adhere to the specified restrictions.\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n\n// Note: As per problem constraints, this implementation uses `int` for signed 32-bit integers,\n// which is standard on most modern platforms. The <stdint.h> header is not permitted.\n\n// Represents the Intermediate Representation (IR) as a sequence of integers.\ntypedef struct {\n    int* data;\n    size_t len;\n} IR;\n\n// Defines the types of compilation passes available.\ntypedef enum {\n    PASS_ADD,\n    PASS_MUL,\n    PASS_CLAMP,\n    PASS_SORT\n} PassType;\n\n// Represents a single compilation pass with its parameters.\ntypedef struct {\n    PassType type;\n    int param1; // Used for k, m, or a\n    int param2; // Used for b\n} Pass;\n\n// Represents a batch of passes to be applied sequentially.\ntypedef struct {\n    const Pass* passes;\n    size_t count;\n} PassBatch;\n\n// Holds the parameters for the verifier predicate.\ntypedef struct {\n    int min_elem;\n    int max_elem;\n    int max_sum;\n    int require_sorted; // Boolean flag: 1 for true, 0 for false\n} VerifierParams;\n\n// Comparison function for qsort.\nint compare_ints(const void* a, const void* b) {\n    int val1 = *(const int*)a;\n    int val2 = *(const int*)b;\n    if (val1 < val2) return -1;\n    if (val1 > val2) return 1;\n    return 0;\n}\n\n// Pass primitive: p_add(S; k)\nvoid apply_pass_add(IR* ir, int k) {\n    for (size_t i = 0; i < ir->len; ++i) {\n        ir->data[i] += k;\n    }\n}\n\n// Pass primitive: p_mul(S; m)\nvoid apply_pass_mul(IR* ir, int m) {\n    for (size_t i = 0; i < ir->len; ++i) {\n        ir->data[i] *= m;\n    }\n}\n\n// Pass primitive: p_clamp(S; a, b)\nvoid apply_pass_clamp(IR* ir, int a, int b) {\n    for (size_t i = 0; i < ir->len; ++i) {\n        if (ir->data[i] < a) {\n            ir->data[i] = a;\n        }\n        if (ir->data[i] > b) {\n            ir->data[i] = b;\n        }\n    }\n}\n\n// Pass primitive: p_sort(S)\nvoid apply_pass_sort(IR* ir) {\n    if (ir->len > 1) {\n        qsort(ir->data, ir->len, sizeof(int), compare_ints);\n    }\n}\n\n// Verifier predicate: V(S; params)\nint verify(const IR* ir, const VerifierParams* params) {\n    long long sum = 0;\n    for (size_t i = 0; i < ir->len; ++i) {\n        // Condition 1: Element range check\n        if (ir->data[i] < params->min_elem || ir->data[i] > params->max_elem) {\n            return 0; // false\n        }\n        sum += ir->data[i];\n    }\n    \n    // Condition 2: Max sum check\n    if (sum > params->max_sum) {\n        return 0; // false\n    }\n\n    // Condition 3: Sortedness check\n    if (params->require_sorted) {\n        for (size_t i = 0; i + 1 < ir->len; ++i) {\n            if (ir->data[i] > ir->data[i + 1]) {\n                return 0; // false\n            }\n        }\n    }\n\n    return 1; // true\n}\n\n// Applies a batch of passes transactionally to an IR.\nvoid run_transaction(IR* original_ir, const PassBatch* batch, const VerifierParams* verifier) {\n    // 1. Create a scratch copy of the IR.\n    IR scratch_ir;\n    scratch_ir.len = original_ir->len;\n    // Handle zero-length IR\n    if (scratch_ir.len == 0) {\n        scratch_ir.data = NULL;\n    } else {\n        scratch_ir.data = (int*)malloc(scratch_ir.len * sizeof(int));\n        if (scratch_ir.data == NULL) {\n            perror(\"Failed to allocate memory for scratch IR\");\n            exit(EXIT_FAILURE);\n        }\n        memcpy(scratch_ir.data, original_ir->data, scratch_ir.len * sizeof(int));\n    }\n\n    // 2. Apply the batch of passes to the scratch copy.\n    for (size_t i = 0; i < batch->count; ++i) {\n        const Pass* p = &batch->passes[i];\n        switch (p->type) {\n            case PASS_ADD:   apply_pass_add(&scratch_ir, p->param1); break;\n            case PASS_MUL:   apply_pass_mul(&scratch_ir, p->param1); break;\n            case PASS_CLAMP: apply_pass_clamp(&scratch_ir, p->param1, p->param2); break;\n            case PASS_SORT:  apply_pass_sort(&scratch_ir); break;\n        }\n    }\n\n    // 3. Verify the transformed scratch IR.\n    if (verify(&scratch_ir, verifier)) {\n        // 4a. Commit: Copy the valid result back to the original IR.\n        if (original_ir->len > 0) {\n            memcpy(original_ir->data, scratch_ir.data, original_ir->len * sizeof(int));\n        }\n    }\n    // 4b. Rollback: Do nothing, discarding the scratch copy.\n\n    // 5. Clean up the scratch copy.\n    free(scratch_ir.data);\n}\n\nint main(void) {\n    // Test Case 1 Data\n    int data1[] = {1, 2, 3, 4};\n    Pass passes1[] = {\n        {PASS_ADD, 1, 0}, {PASS_MUL, 2, 0}, {PASS_CLAMP, 0, 9}, {PASS_SORT, 0, 0}\n    };\n\n    // Test Case 2 Data\n    int data2[] = {5, 5, 5};\n    Pass passes2[] = { {PASS_MUL, 3, 0} };\n\n    // Test Case 3 Data (Empty IR)\n    int data3[] = {};\n    Pass passes3[] = { {PASS_ADD, 10, 0}, {PASS_SORT, 0, 0} };\n    \n    // Test Case 4 Data\n    int data9[] = {9}; // Renamed to avoid conflict with standard library symbols\n    Pass passes4[] = { {PASS_ADD, 1, 0}, {PASS_CLAMP, 0, 10}, {PASS_SORT, 0, 0} };\n\n    // Test Case 5 Data\n    int data5[] = {0, 5, 3};\n    Pass passes5[] = { {PASS_ADD, 0, 0} };\n\n    // Group all test case components together\n    IR test_irs[] = {\n        {data1, sizeof(data1) / sizeof(data1[0])},\n        {data2, sizeof(data2) / sizeof(data2[0])},\n        {data3, sizeof(data3) / sizeof(data3[0])},\n        {data9, sizeof(data9) / sizeof(data9[0])},\n        {data5, sizeof(data5) / sizeof(data5[0])},\n    };\n\n    PassBatch test_batches[] = {\n        {passes1, sizeof(passes1) / sizeof(passes1[0])},\n        {passes2, sizeof(passes2) / sizeof(passes2[0])},\n        {passes3, sizeof(passes3) / sizeof(passes3[0])},\n        {passes4, sizeof(passes4) / sizeof(passes4[0])},\n        {passes5, sizeof(passes5) / sizeof(passes5[0])},\n    };\n\n    VerifierParams test_verifiers[] = {\n        {0, 9, 30, 1},    // Verifier 1\n        {0, 100, 40, 0},  // Verifier 2\n        {0, 100, 0, 1},   // Verifier 3\n        {0, 10, 10, 1},   // Verifier 4\n        {0, 100, 100, 1}  // Verifier 5\n    };\n    \n    int num_cases = sizeof(test_irs) / sizeof(test_irs[0]);\n\n    // Execute the transactional pass manager for each test case.\n    for (int i = 0; i < num_cases; ++i) {\n        run_transaction(&test_irs[i], &test_batches[i], &test_verifiers[i]);\n    }\n\n    // Print the results in the EXACT REQUIRED format.\n    printf(\"[\");\n    for (int i = 0; i < num_cases; ++i) {\n        printf(\"[\");\n        for (size_t j = 0; j < test_irs[i].len; ++j) {\n            printf(\"%d\", test_irs[i].data[j]);\n            if (j < test_irs[i].len - 1) {\n                printf(\",\");\n            }\n        }\n        printf(\"]\");\n        if (i < num_cases - 1) {\n            printf(\",\");\n        }\n    }\n    printf(\"]\");\n\n    return EXIT_SUCCESS;\n}\n```", "id": "3629198"}]}