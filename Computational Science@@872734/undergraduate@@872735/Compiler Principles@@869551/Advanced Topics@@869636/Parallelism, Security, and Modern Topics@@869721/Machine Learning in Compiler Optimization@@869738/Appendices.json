{"hands_on_practices": [{"introduction": "A key application of machine learning in compilers is to guide speculative optimizations, where the compiler makes an aggressive transformation that is beneficial under common-case assumptions but requires a fallback for correctness. This practice [@problem_id:3656426] places you in the role of a compiler designer deciding whether to speculatively hoist a memory load out of a loop. You will use a logistic regression model to predict the likelihood of correctness and perform a rigorous expected-cost analysis to determine the precise point at which the optimization pays off.", "problem": "A just-in-time optimizing compiler considers speculative code motion to hoist a loop-invariant memory load out of a loop when aliasing is uncertain. To maintain correctness, the compiler inserts a runtime guard in the loop preheader that checks non-aliasing; if the guard passes, the load is hoisted and executed once before the loop; if the guard fails, execution falls back to the original loop. The compiler uses Machine Learning (ML) to predict the probability that the guard will pass based on code features. Assume the following scientifically realistic cost model, expressed in processor cycles:\n- Without speculation, each loop iteration executes the load at cost $c_{l} = 4$.\n- The runtime guard has a one-time cost $c_{g} = 20$.\n- If the guard passes, the hoisted load performed once outside the loop costs $c_{o} = 4$.\n- If the guard fails, there is fallback overhead $c_{f} = 12$ (e.g., due to misprediction recovery and control transfer), in addition to executing the original loop.\n\nLet the loop have $N$ iterations. A logistic regression model predicts the guard pass probability $p$ from a feature vector $\\mathbf{x}$, a weight vector $\\mathbf{w}$, and a bias $b$ via the sigmoid function $\\sigma(u) = \\frac{1}{1+\\exp(-u)}$ with $u = \\mathbf{w}^{\\top}\\mathbf{x} + b$. For this program, let\n$\\mathbf{w} = \\langle 0.8,\\,-0.5,\\,1.2 \\rangle$, $\\mathbf{x} = \\langle 0.4,\\,0.6,\\,0.3 \\rangle$, and $b = -0.1$.\nUsing a principled expected-cost analysis grounded in basic definitions of expectation and the logistic function, determine the real-valued threshold $N^{\\star}$ such that, for $N > N^{\\star}$, the expected total cycles of the guarded-hoisting strategy are strictly less than the baseline (non-speculative) strategy. Compute $N^{\\star}$ from the given numbers and express your final answer as a single real number. Round your answer to four significant figures.", "solution": "The problem asks for the threshold number of loop iterations, $N^{\\star}$, beyond which a speculative code hoisting strategy becomes more efficient than a baseline non-speculative strategy. This requires an expected-cost analysis.\n\nFirst, we define the total cycle costs for both strategies. Let $N$ be the number of loop iterations. The provided costs are:\n-   Cost of the load per iteration in the original loop: $c_{l} = 4$.\n-   One-time cost of the runtime guard: $c_{g} = 20$.\n-   One-time cost of the hoisted load if the guard passes: $c_{o} = 4$.\n-   One-time fallback overhead if the guard fails: $c_{f} = 12$.\n\nThe baseline strategy executes the load inside the loop for all $N$ iterations. Its total cost, $C_{base}$, is:\n$$C_{base} = N c_{l}$$\n\nThe guarded-hoisting strategy is speculative. Its cost depends on the outcome of the runtime guard. Let $p$ be the probability that the guard passes (non-aliasing). The probability that the guard fails is therefore $1-p$.\n\n-   If the guard passes (with probability $p$), the total cost consists of the guard cost and the single hoisted load cost. The load is not executed inside the loop.\n    $$C_{pass} = c_{g} + c_{o}$$\n-   If the guard fails (with probability $1-p$), the total cost consists of the guard cost, the fallback overhead, and the cost of executing the original loop where the load is still performed in each of the $N$ iterations.\n    $$C_{fail} = c_{g} + c_{f} + N c_{l}$$\n\nThe expected total cost of the speculative strategy, $E[C_{spec}]$, is the weighted average of these two outcomes:\n$$E[C_{spec}] = p \\cdot C_{pass} + (1-p) \\cdot C_{fail}$$\n$$E[C_{spec}] = p(c_{g} + c_{o}) + (1-p)(c_{g} + c_{f} + N c_{l})$$\n\nThe problem requires finding the threshold $N^{\\star}$ such that for $N > N^{\\star}$, the speculative strategy is strictly better, meaning its expected cost is less than the baseline cost:\n$$E[C_{spec}]  C_{base}$$\n$$p(c_{g} + c_{o}) + (1-p)(c_{g} + c_{f} + N c_{l})  N c_{l}$$\n\nWe now solve this inequality for $N$. Let's expand the left side:\n$$p c_{g} + p c_{o} + c_{g} + c_{f} + N c_{l} - p c_{g} - p c_{f} - p N c_{l}  N c_{l}$$\nCombining terms, we get:\n$$c_{g} + p c_{o} + c_{f} - p c_{f} + (1-p) N c_{l}  N c_{l}$$\nTo isolate $N$, we move all terms containing $N$ to one side:\n$$c_{g} + c_{f} + p(c_{o} - c_{f})  N c_{l} - (1-p) N c_{l}$$\n$$c_{g} + c_{f} + p(c_{o} - c_{f})  N c_{l} (1 - (1-p))$$\n$$c_{g} + c_{f} + p(c_{o} - c_{f})  p N c_{l}$$\nSince $p  0$ and $c_{l}  0$, we can divide by $p c_{l}$ without changing the inequality's direction:\n$$N  \\frac{c_{g} + c_{f} + p(c_{o} - c_{f})}{p c_{l}}$$\nThis gives us the expression for the threshold $N^{\\star}$:\n$$N^{\\star} = \\frac{c_{g} + c_{f} + p(c_{o} - c_{f})}{p c_{l}}$$\n\nNext, we must calculate the probability $p$ using the given logistic regression model. The input to the sigmoid function is $u = \\mathbf{w}^{\\top}\\mathbf{x} + b$. The vectors are given as $\\mathbf{w} = \\langle 0.8,\\,-0.5,\\,1.2 \\rangle$ and $\\mathbf{x} = \\langle 0.4,\\,0.6,\\,0.3 \\rangle$. Formally, these are column vectors:\n$$\\mathbf{w} = \\begin{pmatrix} 0.8 \\\\ -0.5 \\\\ 1.2 \\end{pmatrix}, \\quad \\mathbf{x} = \\begin{pmatrix} 0.4 \\\\ 0.6 \\\\ 0.3 \\end{pmatrix}$$\nThe bias is $b = -0.1$. We compute $u$:\n$$u = \\mathbf{w}^{\\top}\\mathbf{x} + b = \\begin{pmatrix} 0.8  -0.5  1.2 \\end{pmatrix} \\begin{pmatrix} 0.4 \\\\ 0.6 \\\\ 0.3 \\end{pmatrix} - 0.1$$\n$$u = (0.8)(0.4) + (-0.5)(0.6) + (1.2)(0.3) - 0.1$$\n$$u = 0.32 - 0.30 + 0.36 - 0.1 = 0.02 + 0.36 - 0.1 = 0.28$$\nThe probability $p$ is the output of the sigmoid function $\\sigma(u)$:\n$$p = \\sigma(u) = \\frac{1}{1 + \\exp(-u)} = \\frac{1}{1 + \\exp(-0.28)}$$\n\nNow we substitute the given numerical values into the expression for $N^{\\star}$:\n$c_{l} = 4$, $c_{g} = 20$, $c_{o} = 4$, $c_{f} = 12$.\n$$N^{\\star} = \\frac{20 + 12 + p(4 - 12)}{p \\cdot 4} = \\frac{32 - 8p}{4p} = \\frac{8 - 2p}{p} = \\frac{8}{p} - 2$$\nSubstituting the expression for $p$:\n$$N^{\\star} = \\frac{8}{\\frac{1}{1 + \\exp(-0.28)}} - 2 = 8(1 + \\exp(-0.28)) - 2$$\n$$N^{\\star} = 8 + 8\\exp(-0.28) - 2 = 6 + 8\\exp(-0.28)$$\nNow we compute the numerical value:\n$$\\exp(-0.28) \\approx 0.75578374$$\n$$N^{\\star} \\approx 6 + 8(0.75578374) = 6 + 6.0462699$$\n$$N^{\\star} \\approx 12.0462699$$\nRounding to four significant figures, we get:\n$$N^{\\star} \\approx 12.05$$\n\nThus, for any number of loop iterations $N  12.05$, the expected cost of the guarded-hoisting strategy is strictly less than the baseline cost.", "answer": "$$\\boxed{12.05}$$", "id": "3656426"}, {"introduction": "While ML models can guide powerful optimizations, their predictions are not infallible. A robust compiler design must account for the possibility of error and understand its consequences. This exercise [@problem_id:3656416] moves from making a single decision to analyzing the risk associated with it, asking you to quantify the \"misprediction penalty\" for several different optimization passes. By analyzing which decision is most sensitive to model error, you will gain insight into how to build more reliable ML-driven systems.", "problem": "Consider a compiler that tunes optimization passes using Machine Learning (ML). The Quality of Result (QoR) is defined as expected runtime. Let the baseline expected runtime be $Q_0 = 1.00$ seconds. For each pass $i$, enabling it changes the runtime according to the model $Q_{\\text{enable},i} = Q_0 (1 - s_i) + h_i$, where $s_i$ is the true fractional speedup ($0 \\le s_i  1$) and $h_i$ is the overhead in seconds (for example, due to increased code size or register pressure). The ML policy enables pass $i$ if its predicted net runtime $Q_0 (1 - \\hat{s}_i) + h_i$ is strictly less than the baseline $Q_0$, which is equivalent to $\\hat{s}_i  h_i / Q_0$. Since $Q_0 = 1.00$, the decision rule reduces to $\\hat{s}_i  h_i$.\n\nSuppose the ML model’s prediction for each pass $i$ has an absolute error of magnitude $\\Delta = 0.02$ in $s_i$, meaning $\\hat{s}_i = s_i \\pm \\Delta$. For each pass, define its misprediction penalty at error magnitude $\\Delta$ as the maximum possible increase in expected runtime caused by the wrong decision for that pass (enable when the optimal choice is disable, or disable when the optimal choice is enable), compared to the optimal decision for that pass, assuming all other passes are held constant. The optimal decision for pass $i$ is to enable if $Q_{\\text{enable},i}  Q_0$ and disable otherwise.\n\nFour passes have the following true parameters:\n- Loop Unrolling: $s_A = 0.06$, $h_A = 0.04$.\n- Instruction Scheduling: $s_B = 0.04$, $h_B = 0.01$.\n- Function Inlining: $s_C = 0.09$, $h_C = 0.08$.\n- Register Pressure Reduction: $s_D = 0.025$, $h_D = 0.03$.\n\nWhich single pass incurs the largest misprediction penalty in expected runtime when the model is wrong by $\\Delta = 0.02$?\n\nOptions:\n- A. Loop Unrolling\n\n- B. Instruction Scheduling\n\n- C. Function Inlining\n\n- D. Register Pressure Reduction", "solution": "The user wants me to validate the problem and, if valid, solve for the optimization pass with the largest misprediction penalty.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n-   Baseline expected runtime: $Q_0 = 1.00$ seconds.\n-   Runtime model for enabling pass $i$: $Q_{\\text{enable},i} = Q_0 (1 - s_i) + h_i$.\n-   $s_i$: true fractional speedup, $0 \\le s_i  1$.\n-   $h_i$: overhead in seconds.\n-   ML policy decision rule: Enable pass $i$ if $\\hat{s}_i  h_i$.\n-   ML model prediction error: $\\hat{s}_i = s_i \\pm \\Delta$ with absolute error magnitude $\\Delta = 0.02$. This is interpreted as the predicted value $\\hat{s}_i$ lies in the interval $[s_i - \\Delta, s_i + \\Delta]$.\n-   Optimal decision rule: Enable pass $i$ if $Q_{\\text{enable},i}  Q_0$, which simplifies to $s_i  h_i$. Otherwise, disable.\n-   Misprediction penalty definition: The maximum possible increase in expected runtime from a wrong decision compared to the optimal decision.\n-   Pass A (Loop Unrolling): $s_A = 0.06$, $h_A = 0.04$.\n-   Pass B (Instruction Scheduling): $s_B = 0.04$, $h_B = 0.01$.\n-   Pass C (Function Inlining): $s_C = 0.09$, $h_C = 0.08$.\n-   Pass D (Register Pressure Reduction): $s_D = 0.025$, $h_D = 0.03$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, modeling compiler optimization performance trade-offs with a simplified but plausible linear model. The use of machine learning for this purpose is a valid concept in computer science. The problem is well-posed, providing all necessary definitions, data, and a clear question. All terms like \"misprediction penalty\" are explicitly defined. The language is objective and formal. The data is consistent and does not violate any given constraints. The problem requires a careful application of the definitions rather than being trivial or ill-structured.\n\n**Step 3: Verdict and Action**\nThe problem is valid. I will proceed with the solution.\n\n### Solution Derivation\n\nThe analysis aims to find which pass has the largest misprediction penalty. This penalty is the increase in runtime incurred when the ML-based decision is wrong, compared to the optimal decision.\n\nFirst, let's formalize the runtime and decisions. The baseline runtime, with pass $i$ disabled, is $Q_{\\text{disable},i} = Q_0 = 1.00$ s. The runtime with pass $i$ enabled is $Q_{\\text{enable},i} = Q_0(1-s_i) + h_i = 1.00(1-s_i) + h_i = 1 - s_i + h_i$.\n\nThe optimal decision is to enable the pass if it reduces runtime, i.e., $Q_{\\text{enable},i}  Q_{\\text{disable},i}$.\n$$\n1 - s_i + h_i  1\n$$\n$$\ns_i  h_i\n$$\nSo, the optimal policy is: enable if $s_i  h_i$, and disable if $s_i \\le h_i$.\n\nThe ML-based policy makes its decision based on the predicted speedup, $\\hat{s}_i$. It enables the pass if $\\hat{s}_i  h_i$. A misprediction occurs if the ML policy's decision is different from the optimal decision.\n\nThe misprediction penalty is the difference between the runtime of the wrongful action and the runtime of the optimal action. There are two cases for a wrong decision:\n\n1.  **Mistaken Enable:** The optimal decision is to disable ($s_i \\le h_i$), but the ML model enables ($\\hat{s}_i  h_i$).\n    -   Runtime of optimal decision (disable): $Q_{opt} = Q_0 = 1.00$.\n    -   Runtime of wrong decision (enable): $Q_{wrong} = 1 - s_i + h_i$.\n    -   Penalty = $Q_{wrong} - Q_{opt} = (1 - s_i + h_i) - 1 = h_i - s_i$. Since $s_i \\le h_i$, this value is non-negative.\n\n2.  **Mistaken Disable:** The optimal decision is to enable ($s_i  h_i$), but the ML model disables ($\\hat{s}_i \\le h_i$).\n    -   Runtime of optimal decision (enable): $Q_{opt} = 1 - s_i + h_i$.\n    -   Runtime of wrong decision (disable): $Q_{wrong} = Q_0 = 1.00$.\n    -   Penalty = $Q_{wrong} - Q_{opt} = 1 - (1 - s_i + h_i) = s_i - h_i$. Since $s_i  h_i$, this value is positive.\n\nIn both cases, the magnitude of the penalty, if a misprediction occurs, is $|s_i - h_i|$.\n\nA misprediction is only possible if the ML model's prediction $\\hat{s}_i$ can cross the decision boundary $h_i$. The model's prediction $\\hat{s}_i$ is known to be in the interval $[s_i - \\Delta, s_i + \\Delta]$, where $\\Delta = 0.02$. A misprediction can happen if and only if this interval of possible predictions contains the threshold $h_i$.\n-   Case 1 (Mistaken Enable, $s_i \\le h_i$): A prediction $\\hat{s}_i  h_i$ is possible only if $s_i + \\Delta  h_i$.\n-   Case 2 (Mistaken Disable, $s_i  h_i$): A prediction $\\hat{s}_i \\le h_i$ is possible only if $s_i - \\Delta \\le h_i$.\n\nCombining these, a misprediction is possible if and only if $s_i - \\Delta \\le h_i \\le s_i + \\Delta$, which is equivalent to $|s_i - h_i| \\le \\Delta$.\n\nIf $|s_i - h_i|  \\Delta$, the interval $[s_i - \\Delta, s_i + \\Delta]$ does not contain $h_i$. All possible predictions $\\hat{s}_i$ are on the same side of the threshold $h_i$ as the true value $s_i$. Thus, the ML policy will always make the correct decision, and the misprediction penalty is $0$.\n\nThe misprediction penalty for pass $i$, denoted $P_i$, is therefore:\n$$\nP_i = \\begin{cases} |s_i - h_i|  \\text{if } |s_i - h_i| \\le \\Delta \\\\ 0  \\text{if } |s_i - h_i|  \\Delta \\end{cases}\n$$\nWe are given $\\Delta = 0.02$. We now calculate $P_i$ for each pass.\n\n**A. Loop Unrolling**\n-   $s_A = 0.06$, $h_A = 0.04$.\n-   Difference: $|s_A - h_A| = |0.06 - 0.04| = 0.02$.\n-   Condition: Is $|s_A - h_A| \\le \\Delta$? Yes, $0.02 \\le 0.02$.\n-   A misprediction is possible. The optimal choice is to enable ($s_A  h_A$). A prediction of $\\hat{s}_A = s_A - \\Delta = 0.04$ would lead to a \"disable\" decision (since $0.04 \\ngtr 0.04$).\n-   Penalty: $P_A = |s_A - h_A| = 0.02$ s.\n\n**B. Instruction Scheduling**\n-   $s_B = 0.04$, $h_B = 0.01$.\n-   Difference: $|s_B - h_B| = |0.04 - 0.01| = 0.03$.\n-   Condition: Is $|s_B - h_B| \\le \\Delta$? No, $0.03  0.02$.\n-   A misprediction is not possible. The prediction interval is $[0.04-0.02, 0.04+0.02] = [0.02, 0.06]$. All possible $\\hat{s}_B$ are greater than $h_B = 0.01$, so the ML model will always correctly enable the pass.\n-   Penalty: $P_B = 0$ s.\n\n**C. Function Inlining**\n-   $s_C = 0.09$, $h_C = 0.08$.\n-   Difference: $|s_C - h_C| = |0.09 - 0.08| = 0.01$.\n-   Condition: Is $|s_C - h_C| \\le \\Delta$? Yes, $0.01 \\le 0.02$.\n-   A misprediction is possible. The optimal choice is to enable ($s_C  h_C$). A prediction in the range $[0.07, 0.08]$ would lead to a wrong \"disable\" decision.\n-   Penalty: $P_C = |s_C - h_C| = 0.01$ s.\n\n**D. Register Pressure Reduction**\n-   $s_D = 0.025$, $h_D = 0.03$.\n-   Difference: $|s_D - h_D| = |0.025 - 0.03| = |-0.005| = 0.005$.\n-   Condition: Is $|s_D - h_D| \\le \\Delta$? Yes, $0.005 \\le 0.02$.\n-   A misprediction is possible. The optimal choice is to disable ($s_D  h_D$). A prediction in the range $(0.03, 0.045]$ would lead to a wrong \"enable\" decision.\n-   Penalty: $P_D = |s_D - h_D| = 0.005$ s.\n\n**Conclusion**\nComparing the penalties:\n-   $P_A = 0.02$\n-   $P_B = 0$\n-   $P_C = 0.01$\n-   $P_D = 0.005$\nThe largest misprediction penalty is $0.02$ s, which corresponds to the Loop Unrolling pass.\n\n### Option-by-Option Analysis\n\n-   **A. Loop Unrolling**: The calculated misprediction penalty is $P_A = 0.02$ s. This is the maximum value among the four passes ($0.02  0$, $0.02  0.01$, $0.02  0.005$). Therefore, this pass incurs the largest misprediction penalty. **Correct**.\n\n-   **B. Instruction Scheduling**: For this pass, $|s_B - h_B| = 0.03  \\Delta = 0.02$. A misprediction is impossible given the model's error magnitude. The penalty is $P_B = 0$. This is not the largest penalty. **Incorrect**.\n\n-   **C. Function Inlining**: The calculated misprediction penalty is $P_C = 0.01$ s. This is smaller than the penalty for Loop Unrolling ($0.01  0.02$). **Incorrect**.\n\n-   **D. Register Pressure Reduction**: The calculated misprediction penalty is $P_D = 0.005$ s. This is smaller than the penalties for both Loop Unrolling and Function Inlining. **Incorrect**.\n\nThe single pass that incurs the largest misprediction penalty is Loop Unrolling.", "answer": "$$\\boxed{A}$$", "id": "3656416"}, {"introduction": "Integrating machine learning into a production compiler requires more than just an accurate model; it demands an efficient and safe system. Since recomputing ML predictions for every function can be prohibitively slow, caching is essential. This advanced practice [@problem_id:3656494] tackles the real-world engineering problem of designing a safe prediction cache. You will develop and implement an invalidation rule that guarantees correctness even as the compiler's intermediate representation changes, balancing the need for speed with the requirement for safety by using hashing and mathematical bounds.", "problem": "You are building a cache for machine learning predictions that a compiler uses to guide optimizations on a per-function basis. Each function is represented by an intermediate representation (IR) from which a fixed-length feature vector is extracted. Early IR passes can change the IR before the point when the cached prediction is reused, so your cache must use a code-hash derived from an IR signature and a principled invalidation rule to ensure safety. Your task is to formalize the reuse condition from first principles and implement a program that applies it to a small test suite.\n\nFundamental base:\n- A feature extractor maps a function’s IR to a numeric feature vector $\\mathbf{f} \\in \\mathbb{R}^{d}$ that is consistent across compilations for the same IR state.\n- A deterministic machine learning model maps features to a scalar prediction $y \\in \\mathbb{R}$ via a function $M:\\mathbb{R}^{d} \\to \\mathbb{R}$.\n- A deterministic hash function $H$ maps a string and a tuple of integers to a machine word and is used to derive a cache key. Collisions are possible but rare and are ignored in this exercise.\n- A subset of feature dimensions is designated “stable” under a set of early IR passes. Let $\\mathbf{s} \\in \\{0,1\\}^{d}$ be a binary mask with $s_{i}=1$ if the $i$-th feature is included in the hash signature and treated as stability-critical.\n- The invalidation rule must be safe, meaning that if the cache is reused, the deviation between the reused prediction and the hypothetical recomputed prediction after early passes is bounded in a way derivable from the model structure and the observed IR differences.\n\nModel and representation used in this problem:\n- The model is linear: $M(\\mathbf{f}) = \\mathbf{w}^{\\top}\\mathbf{f} + b$, with a weight vector $\\mathbf{w} \\in \\mathbb{R}^{d}$ and bias $b \\in \\mathbb{R}$.\n- A per-function cache key is constructed by hashing the function identifier string together with the subvector of features selected by $\\mathbf{s}$.\n- Let $\\mathbf{f}^{(0)}$ be the features before early passes and $\\mathbf{f}^{(1)}$ after early passes. The hash signature is built from $\\mathbf{f}^{(0)}$ at insertion time and from $\\mathbf{f}^{(1)}$ at lookup time.\n- The invalidation rule must depend only on: the equality of the two hash signatures and a bound on the potential change in $M(\\cdot)$ attributable to differences in the non-hashed feature dimensions (those with $s_{i}=0$). The bound should follow from the model structure and elementary inequalities without assuming any oracle.\n\nDesign requirements:\n- Derive, justify, and implement a conservative reuse condition that uses:\n  - Hash signature equality for the hashed subvector.\n  - A tolerance parameter $\\tau \\in \\mathbb{R}_{\\ge 0}$ that upper-bounds permissible change in the model output due to differences in the non-hashed features. The tolerance is provided as part of the configuration.\n- If the condition indicates safe reuse, return a reuse flag of $1$ for that test case; otherwise return $0$.\n\nConfiguration for all test cases:\n- Dimension $d = 4$.\n- Weight vector $\\mathbf{w} = \\langle 0.4, -0.1, 0.2, 0.05 \\rangle$.\n- Bias $b = 0.0$.\n- Stability mask $\\mathbf{s} = \\langle 1, 0, 1, 0 \\rangle$.\n- Tolerance $\\tau = 0.15$.\n\nTest suite:\nFor each test case, you are given a function identifier, an initial feature vector $\\mathbf{f}^{(0)} \\in \\mathbb{Z}^{d}$, and a post-pass feature vector $\\mathbf{f}^{(1)} \\in \\mathbb{Z}^{d}$.\n\n- Case A: identifier “foo”, $\\mathbf{f}^{(0)} = \\langle 10, 3, 2, 5 \\rangle$, $\\mathbf{f}^{(1)} = \\langle 10, 4, 2, 4 \\rangle$.\n- Case B: identifier “bar”, $\\mathbf{f}^{(0)} = \\langle 0, 0, 0, 0 \\rangle$, $\\mathbf{f}^{(1)} = \\langle 0, 1, 0, 0 \\rangle$.\n- Case C: identifier “baz”, $\\mathbf{f}^{(0)} = \\langle 7, 2, 3, 1 \\rangle$, $\\mathbf{f}^{(1)} = \\langle 8, 2, 3, 1 \\rangle$.\n- Case D: identifier “qux”, $\\mathbf{f}^{(0)} = \\langle 9, 9, 9, 9 \\rangle$, $\\mathbf{f}^{(1)} = \\langle 9, 13, 9, 3 \\rangle$.\n- Case E: identifier “edge”, $\\mathbf{f}^{(0)} = \\langle 1, 1, 1, 1 \\rangle$, $\\mathbf{f}^{(1)} = \\langle 1, 1, 1, 1 \\rangle$.\n\nProgram requirements:\n- Implement a complete program that:\n  - Encodes the model and configuration above.\n  - Computes the cache signature from the function identifier and the masked feature subvector.\n  - Decides reuse for each test case using your derived safe invalidation rule.\n- Final output format:\n  - Your program should produce a single line of output containing the results as a comma-separated list of integers enclosed in square brackets (e.g., “[$1,0,1$]”), with one integer per test case in the order A, B, C, D, E, where $1$ indicates “reuse cached prediction” and $0$ indicates “invalidate and recompute”.", "solution": "We formalize the caching and invalidation problem using a linear model and a decomposition of features into hashed (stability-critical) and non-hashed (tolerated-change) components.\n\nFoundational definitions:\n- Let the feature dimension be $d \\in \\mathbb{N}$. Here $d = 4$.\n- Let the linear model be $M(\\mathbf{f}) = \\mathbf{w}^{\\top}\\mathbf{f} + b$, where $\\mathbf{w} \\in \\mathbb{R}^{d}$ and $b \\in \\mathbb{R}$ are fixed constants set by prior training. For the test, $\\mathbf{w} = \\langle 0.4, -0.1, 0.2, 0.05 \\rangle$ and $b = 0.0$.\n- Let the stability mask be $\\mathbf{s} \\in \\{0,1\\}^{d}$ with $\\mathbf{s} = \\langle 1, 0, 1, 0 \\rangle$. Indices $i$ with $s_{i}=1$ are included in the code-hash signature.\n- Let a function identifier be a string $\\text{id}$. Let $H(\\text{id}, \\text{proj}_{\\mathbf{s}}(\\mathbf{f}))$ denote a deterministic $64$-bit hash of the identifier and the subvector $\\text{proj}_{\\mathbf{s}}(\\mathbf{f}) = \\{ f_{i} \\mid s_{i}=1 \\}$.\n- For each function, we observe a pair $(\\mathbf{f}^{(0)}, \\mathbf{f}^{(1)})$ where $\\mathbf{f}^{(0)}$ precedes early IR passes and $\\mathbf{f}^{(1)}$ follows them.\n\nPrinciple-based derivation of a safe invalidation rule:\n- The cache should be reused only if two conditions hold:\n  1. The equivalence of hashed stability-critical content is preserved. This is captured by $H(\\text{id}, \\text{proj}_{\\mathbf{s}}(\\mathbf{f}^{(0)})) = H(\\text{id}, \\text{proj}_{\\mathbf{s}}(\\mathbf{f}^{(1)}))$. This relies on the determinism of $H$ and the assumption that the mask $\\mathbf{s}$ characterizes the subspace whose change must force invalidation.\n  2. The change in the model’s output due to differences in non-hashed features is provably small. For a linear model, for any $\\mathbf{u}, \\mathbf{v} \\in \\mathbb{R}^{d}$,\n     $$\n     \\left|M(\\mathbf{u}) - M(\\mathbf{v})\\right| = \\left|\\mathbf{w}^{\\top}(\\mathbf{u}-\\mathbf{v})\\right|.\n     $$\n     Decompose the difference into hashed and non-hashed coordinates using the mask $\\mathbf{s}$. If the hashed coordinates are unchanged (as enforced by the hash equality), only the non-hashed coordinates contribute. Let $\\Delta \\mathbf{f} = \\mathbf{f}^{(1)} - \\mathbf{f}^{(0)}$. Consider the index set $U = \\{ i \\mid s_{i} = 0 \\}$. Then\n     $$\n     \\left|M(\\mathbf{f}^{(1)}) - M(\\mathbf{f}^{(0)})\\right| = \\left|\\sum_{i \\in U} w_{i} \\Delta f_{i}\\right| \\le \\sum_{i \\in U} \\left|w_{i}\\right| \\cdot \\left|\\Delta f_{i}\\right|.\n     $$\n     The inequality follows from the triangle inequality for real numbers. This provides a worst-case bound on the model output deviation using only coordinate-wise absolute changes and absolute weights. If this bound is less than or equal to a tolerance $\\tau \\in \\mathbb{R}_{\\ge 0}$ (set by the system designer), then reusing the cached prediction is safe under the specified tolerance; otherwise, invalidate.\n\nTherefore, the conservative reuse rule is:\n- Reuse if and only if both:\n  - $H(\\text{id}, \\text{proj}_{\\mathbf{s}}(\\mathbf{f}^{(0)})) = H(\\text{id}, \\text{proj}_{\\mathbf{s}}(\\mathbf{f}^{(1)}))$, and\n  - $\\sum_{i: s_{i}=0} \\left|w_{i}\\right| \\cdot \\left|f^{(1)}_{i} - f^{(0)}_{i}\\right| \\le \\tau$.\n\nAlgorithmic design:\n- Compute $h_{0} = H(\\text{id}, \\text{proj}_{\\mathbf{s}}(\\mathbf{f}^{(0)}))$ and $h_{1} = H(\\text{id}, \\text{proj}_{\\mathbf{s}}(\\mathbf{f}^{(1)}))$ using a fixed $64$-bit rolling hash that mixes the function identifier with the masked integer features. Determinism ensures equal inputs yield equal outputs.\n- If $h_{0} \\ne h_{1}$, return reuse flag $0$.\n- Else, compute the non-hashed deviation bound $B = \\sum_{i: s_{i}=0} \\left|w_{i}\\right| \\cdot \\left|f^{(1)}_{i} - f^{(0)}_{i}\\right|$ and compare to $\\tau$; return reuse flag $1$ if $B \\le \\tau$, else $0$.\n\nManual reasoning on the provided test suite with $\\mathbf{w} = \\langle 0.4, -0.1, 0.2, 0.05 \\rangle$, $b = 0.0$, $\\mathbf{s} = \\langle 1, 0, 1, 0 \\rangle$, and $\\tau = 0.15$:\n- Case A: “foo”, $\\mathbf{f}^{(0)} = \\langle 10, 3, 2, 5 \\rangle$, $\\mathbf{f}^{(1)} = \\langle 10, 4, 2, 4 \\rangle$. Hashed coordinates (indices 0 and 2) are unchanged. Non-hashed differences (indices 1 and 3): $\\left|\\Delta f_{1}\\right| = 1$, $\\left|\\Delta f_{3}\\right| = 1$. Bound $B = \\left|{-0.1}\\right|\\cdot 1 + \\left|0.05\\right|\\cdot 1 = 0.1 + 0.05 = 0.15 \\le 0.15$. Reuse flag $1$.\n- Case B: “bar”, $\\mathbf{f}^{(0)} = \\langle 0, 0, 0, 0 \\rangle$, $\\mathbf{f}^{(1)} = \\langle 0, 1, 0, 0 \\rangle$. Hashed coordinates unchanged. Non-hashed differences: $\\left|\\Delta f_{1}\\right| = 1$, $\\left|\\Delta f_{3}\\right| = 0$. Bound $B = 0.1 \\cdot 1 + 0.05 \\cdot 0 = 0.1 \\le 0.15$. Reuse flag $1$.\n- Case C: “baz”, $\\mathbf{f}^{(0)} = \\langle 7, 2, 3, 1 \\rangle$, $\\mathbf{f}^{(1)} = \\langle 8, 2, 3, 1 \\rangle$. Hashed coordinate at index 0 changes, thus $h_{0} \\ne h_{1}$, invalidate. Reuse flag $0$.\n- Case D: “qux”, $\\mathbf{f}^{(0)} = \\langle 9, 9, 9, 9 \\rangle$, $\\mathbf{f}^{(1)} = \\langle 9, 13, 9, 3 \\rangle$. Hashed coordinates unchanged. Non-hashed differences: $\\left|\\Delta f_{1}\\right| = 4$, $\\left|\\Delta f_{3}\\right| = 6$. Bound $B = 0.1 \\cdot 4 + 0.05 \\cdot 6 = 0.4 + 0.3 = 0.7  0.15$. Invalidate. Reuse flag $0$.\n- Case E: “edge”, $\\mathbf{f}^{(0)} = \\langle 1, 1, 1, 1 \\rangle$, $\\mathbf{f}^{(1)} = \\langle 1, 1, 1, 1 \\rangle$. Identical features, so signatures equal and bound $B=0$. Reuse flag $1$.\n\nThus the expected sequence of reuse flags is $\\langle 1, 1, 0, 0, 1 \\rangle$.\n\nThe implementation computes the hash signatures deterministically using a $64$-bit FNV-style mixing for the identifier and masked features, evaluates the bound over non-hashed coordinates via absolute values, compares with $\\tau$, and prints the flags in the specified format.", "answer": "```c\n// The complete and compilable C program goes here.\n// Headers must adhere to the specified restrictions.\n#include stdio.h\n#include stdlib.h\n#include string.h\n#include math.h\n// #include complex.h\n// #include threads.h\n// #include stdatomic.h\n\n// A struct to hold the parameters for a single test case.\ntypedef struct {\n    const char *func_id;\n    int f0[4];\n    int f1[4];\n} TestCase;\n\n// 64-bit FNV-1a style hasher for strings\nstatic unsigned long long fnv1a64(const char *s) {\n    unsigned long long h = 1469598103934665603ULL; // offset basis\n    while (*s) {\n        unsigned char c = (unsigned char)(*s++);\n        h ^= (unsigned long long)c;\n        h *= 1099511628211ULL; // FNV prime\n    }\n    return h;\n}\n\n// Hash the function id together with masked features\nstatic unsigned long long hash_signature(const char *func_id, const int *f, const int *mask, int d) {\n    unsigned long long h = fnv1a64(func_id);\n    for (int i = 0; i  d; ++i) {\n        if (mask[i]) {\n            // Mix in the masked feature value deterministically\n            unsigned long long v = (unsigned long long)(f[i]);\n            // Simple mix: xor with a shifted/multiplied variant to diffuse bits\n            h ^= (v + 0x9e3779b97f4a7c15ULL + (h  6) + (h  2));\n            h *= 1099511628211ULL;\n        }\n    }\n    return h;\n}\n\nint main(void) {\n    // Global configuration\n    const int d = 4;\n    // Linear model parameters\n    const double w[4] = {0.4, -0.1, 0.2, 0.05};\n    const double b = 0.0;\n    // Stability mask: 1 means included in hash (stability-critical)\n    const int s_mask[4] = {1, 0, 1, 0};\n    // Tolerance for non-hashed change\n    const double tau = 0.15;\n\n    // Define the test cases from the problem statement.\n    TestCase test_cases[] = {\n        { \"foo\",  {10, 3, 2, 5}, {10, 4, 2, 4} },\n        { \"bar\",  { 0, 0, 0, 0}, { 0, 1, 0, 0} },\n        { \"baz\",  { 7, 2, 3, 1}, { 8, 2, 3, 1} },\n        { \"qux\",  { 9, 9, 9, 9}, { 9,13, 9, 3} },\n        { \"edge\", { 1, 1, 1, 1}, { 1, 1, 1, 1} }\n    };\n\n    // Calculate the number of test cases.\n    int num_cases = (int)(sizeof(test_cases) / sizeof(test_cases[0]));\n    int results[num_cases]; // 1 for reuse, 0 for invalidate\n\n    // Calculate the result for each test case.\n    for (int i = 0; i  num_cases; ++i) {\n        TestCase *tc = test_cases[i];\n\n        // Compute initial and lookup signatures\n        unsigned long long h0 = hash_signature(tc-func_id, tc-f0, s_mask, d);\n        unsigned long long h1 = hash_signature(tc-func_id, tc-f1, s_mask, d);\n\n        int reuse = 0;\n\n        if (h0 == h1) {\n            // Compute conservative bound over non-hashed coords\n            double bound = 0.0;\n            for (int k = 0; k  d; ++k) {\n                if (!s_mask[k]) {\n                    double delta = (double)(tc-f1[k] - tc-f0[k]);\n                    bound += fabs(w[k]) * fabs(delta);\n                }\n            }\n            if (bound = tau) {\n                reuse = 1;\n            } else {\n                reuse = 0;\n            }\n        } else {\n            reuse = 0;\n        }\n\n        // Optionally compute predictions (not printed), to reflect realistic workflow\n        // double y0 = b;\n        // double y1 = b;\n        // for (int k = 0; k  d; ++k) {\n        //     y0 += w[k] * (double)tc-f0[k];\n        //     y1 += w[k] * (double)tc-f1[k];\n        // }\n        results[i] = reuse;\n    }\n\n    // Print the results in the EXACT REQUIRED format before the final return statement\n    printf(\"[\");\n    for (int i = 0; i  num_cases; ++i) {\n        if (i) printf(\",\");\n        printf(\"%d\", results[i]);\n    }\n    printf(\"]\");\n\n    return EXIT_SUCCESS;\n}\n```", "id": "3656494"}]}