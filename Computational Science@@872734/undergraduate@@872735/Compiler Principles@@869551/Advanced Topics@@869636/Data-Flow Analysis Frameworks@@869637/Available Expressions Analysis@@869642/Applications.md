## Applications and Interdisciplinary Connections

Having established the principles and mechanisms of [available expressions](@entry_id:746600) analysis, we now turn our attention to its role in practice. This chapter explores how this fundamental [data-flow analysis](@entry_id:638006) serves as a cornerstone for numerous [compiler optimizations](@entry_id:747548) and connects to broader concepts in [program analysis](@entry_id:263641) and software engineering. Our exploration will begin with the most direct applications in [code optimization](@entry_id:747441) and gradually expand to more advanced techniques, interprocedural contexts, and even connections to the field of [formal verification](@entry_id:149180). The objective is not to reiterate the mechanics of the analysis but to illuminate its utility and versatility in solving real-world problems.

### Core Application: Redundancy Elimination

The primary motivation for [available expressions](@entry_id:746600) analysis is to identify and eliminate redundant computations. A computation is redundant if its value has already been computed and is still valid. The "must-analysis" nature of [available expressions](@entry_id:746600) provides the rigorous guarantee needed to safely perform this optimization.

#### Common Subexpression Elimination (CSE)

Common Subexpression Elimination (CSE) is the most direct application of [available expressions](@entry_id:746600) analysis. After the analysis reaches a fixed point, the set of [available expressions](@entry_id:746600) is known for every program point. Before executing a statement that computes an expression, such as `t := x + y`, the compiler can check if `x + y` is a member of the set of [available expressions](@entry_id:746600) at that point. If it is, the computation is redundant. The compiler can then replace the computation with a simple copy from the temporary variable that holds the previously computed value, thereby saving computational resources.

This principle extends from local CSE (within a single basic block) to global CSE (across an entire procedure). For global CSE to be sound, an expression must be available at a program point, which means it must have been computed along *all* paths leading to that point without its operands being redefined. Consider a control-flow structure where two distinct paths merge at a block $B$. If an expression $e$ is computed on both paths, and its operands are not modified before the merge, $e$ will be in the `IN` set of block $B$. Consequently, a re-computation of $e$ at the beginning of $B$ can be safely eliminated. [@problem_id:3622879]

For this transformation to be valid, certain conditions beyond just availability must hold. The operator itself must be pure, meaning it has no side effects and does not raise exceptions that alter control flow. Furthermore, all uses of the expression's value must be dominated by the new, unified computation point. If a use occurred before the merge point on one of the branches, eliminating the computation on that branch would be incorrect. [@problem_id:3643987]

#### Beyond Syntactic Redundancy

Classical [available expressions](@entry_id:746600) analysis operates on the syntactic form of expressions. It would not, for example, recognize that `x + 0` is equivalent to `x`. More advanced optimization frameworks integrate [available expressions](@entry_id:746600) with other techniques to detect semantic equivalences. For instance, a canonicalization pass using [constant folding](@entry_id:747743) can transform all occurrences of `x + 0` into `x` before the analysis runs, allowing a subsequent copy propagation pass to eliminate the redundancy. Alternatively, more powerful analysis techniques like Global Value Numbering (GVN) assign numbers to computed values rather than syntactic expressions. A GVN-based approach can determine that `x` and `x + 0` correspond to the same value number and eliminate the recomputation without relying on a separate canonicalization pass. These examples illustrate that while [available expressions](@entry_id:746600) analysis is powerful, it is most effective as part of a larger suite of cooperating analyses and transformations. [@problem_id:3644039]

Pure, side-effect-free functions, such as a hash computation `hash(x, y)`, can also be treated as expressions within this framework. The computation `hash(x, y)` is considered "generated" when it is called. Crucially, any assignment to its operands, `x` or `y`, is modeled as "killing" the expression's availability. This correctly mirrors the semantics of [memoization](@entry_id:634518): a call populates a conceptual cache, and a mutation of an input invalidates the cached result, forcing a re-computation. [@problem_id:3622858]

### Application in Loop Optimizations

Loops are a primary source of optimization opportunities in most programs. Available expressions analysis is instrumental in enabling several critical loop optimizations.

#### Loop-Invariant Code Motion (LICM)

An expression is [loop-invariant](@entry_id:751464) if its value does not change across iterations of the loop. This is true if all of its operands are either constants, defined outside the loop, or are themselves [loop-invariant](@entry_id:751464). Such computations can be safely "hoisted" out of the loop and executed only once before the loop begins, with the result stored in a temporary variable that is used within the loop.

Available expressions analysis provides a direct way to identify candidate expressions for LICM. An expression that is available at the loop header is guaranteed to have been computed on all paths leading to the loop, including the [back edge](@entry_id:260589) from the end of the loop to the start. If an expression's operands are not modified within the loop body, its availability will be preserved through the loop. After iterating to a fixed point, the set of [available expressions](@entry_id:746600) at the loop header will contain expressions whose values persist across iterations.

Conversely, if an expression contains an operand that is modified within the loop (e.g., an [induction variable](@entry_id:750618)), the analysis will correctly determine that it is not [loop-invariant](@entry_id:751464). For example, if a variable `y` is incremented in block $B_3$ at the end of a loop, this assignment will kill the availability of any expression involving `y`, such as `x + y`. Even if `x + y` is computed within the loop body, the kill action on the [back edge](@entry_id:260589) prevents it from being in the `IN` set of the loop header. The meet operation (intersection) at the header ensures that only expressions whose availability survives every iteration are identified as invariant. [@problem_id:3622936]

This detection mechanism can be formalized as a forward, must-analysis. The lattice is the powerset of candidate expressions, initialized to the optimistic assumption that all expressions are invariant. The transfer function for each block simply removes (kills) any expression whose operands are defined within that block. The confluence at the loop header, which meets the flow from the preheader and the [back edge](@entry_id:260589), ensures that any expression killed on any path through the loop is correctly removed from the set of invariants. [@problem_id:3635688]

### Advanced Optimizations Building on Availability

While CSE and LICM are direct applications, [available expressions](@entry_id:746600) analysis is also a building block for more sophisticated optimizations that handle more complex patterns of redundancy.

#### Partial Redundancy Elimination (PRE)

An expression is partially redundant if it is computed on some, but not all, paths leading to a merge point. For example, consider a conditional where `x + y` is computed in the `then` branch but not the `else` branch, followed by a computation of `x + y` after the branches join. The latter computation is redundant only if the `then` branch was taken.

Partial Redundancy Elimination (PRE) is an optimization that eliminates partial redundancies by transforming them into full redundancies. It does this by inserting the computation on the paths where it is missing. In the example above, PRE would insert a computation of `x + y` into the `else` branch. This makes the expression fully available at the merge point, allowing the subsequent computation to be safely eliminated. To perform this transformation safely and optimally, PRE relies on a combination of data-flow analyses, including not only **[available expressions](@entry_id:746600)** (a [forward analysis](@entry_id:749527)) but also **anticipability** (a backward analysis that determines if an expression is guaranteed to be used in the future). An expression is a candidate for insertion on a path if it is anticipatable but not yet available. [@problem_id:3622952]

Implementing PRE requires careful handling of the [control-flow graph](@entry_id:747825). Inserting code on an edge $(B_p, B_s)$ can be problematic if the source block $B_p$ has multiple successors or the destination block $B_s$ has multiple predecessors. An edge is called a **[critical edge](@entry_id:748053)** if both conditions are true. Inserting code into $B_p$ would affect other paths, and inserting into $B_s$ is too late. The correct solution is to split the [critical edge](@entry_id:748053) by creating a new, empty basic block that becomes the sole target of $B_p$ on that path and the sole predecessor of $B_s$ on that path. The new computation can then be safely placed in this new block. [@problem_id:3644037]

Modern compilers often implement PRE using algorithms like **Lazy Code Motion (LCM)**, especially on intermediate representations like Static Single Assignment (SSA) form. In SSA, where variables are defined by `phi`-functions at merge points, the interaction with availability becomes more nuanced. If an operand of an expression is redefined on one branch before a merge, it kills the availability of any simple, hoisted version of the expression. LCM handles this by effectively pushing different versions of the computation into the predecessor blocks, allowing the original computation to be replaced by a `phi`-function that merges the results. The core decision-making process still relies on determining where an expression's availability is killed by redefinitions. [@problem_id:3649366]

### Interdisciplinary and Inter-Component Connections

The utility of [available expressions](@entry_id:746600) analysis extends beyond [code optimization](@entry_id:747441), connecting to other compiler components and even other fields of computer science.

#### Register Allocation and Rematerialization

During [register allocation](@entry_id:754199), high [register pressure](@entry_id:754204) may force the compiler to "spill" the value of a temporary variable to memory. To use that value later, it must be "reloaded" from memory. However, if the spilled value was the result of a simple, cheap computation (e.g., `a + b`), it may be more efficient to recompute it (a process called **rematerialization**) than to perform a costly memory load.

The decision to rematerialize is only semantically correct if the recomputed value is identical to the original spilled value. Available expressions analysis is the key to ensuring this. An expression can be safely rematerialized if it is still available at the point of use. If, between the original computation and the use, an operand of the expression has been modified, the expression is no longer available. In such a case, rematerialization would produce an incorrect value, and the compiler must perform a reload from the spill slot. This creates a path-sensitive optimization problem where the compiler might choose to rematerialize on paths where the expression is available and reload on paths where it is not, balancing correctness and performance. [@problem_id:3668320]

#### Interaction with Function Inlining

Function inlining, which replaces a call with the body of the callee, can significantly improve the precision of [available expressions](@entry_id:746600) analysis. Without inlining, an intraprocedural analysis must make conservative assumptions about a function call, typically assuming it kills all expressions involving variables it might modify. This loss of information prevents the elimination of redundancies that cross call boundaries. When a function is inlined, its code becomes part of the caller's [control-flow graph](@entry_id:747825). The analysis can then "see" the computations performed inside the inlined body, potentially making new expressions available in the caller. This increased precision, however, comes at the cost of increased analysis time due to the larger code size. [@problem_id:3622913]

#### Interprocedural and Whole-Program Analysis

To analyze programs with many functions without inlining everything, [available expressions](@entry_id:746600) analysis can be extended to an **interprocedural** setting. This is typically done using function summaries. A simple, conservative summary for a function `g` might be its *may-modify* set—the set of global variables or parameters it might change. At a call to `g`, the analysis in the caller would use this summary to kill any [available expressions](@entry_id:746600) whose operands are in `g`'s may-modify set. [@problem_id:3622864]

More precise summaries can be constructed. For side-effect-free code, a function's summary can capture the set of expressions (over its formal parameters) that are guaranteed to be available at its exit. When analyzing a call, the caller can instantiate this summary with the actual arguments, propagating availability information across the call boundary. This allows for transitive reasoning through the [call graph](@entry_id:747097). [@problem_id:3635630]

In its most advanced form, this concept evolves into **[whole-program analysis](@entry_id:756727)**, which is essential for optimizing computations on complex data structures on the heap. For example, to eliminate redundant hash computations on a graph of objects, the analysis must prove that the object's "structural fingerprint" (the set of all relevant field values) has not changed. This requires integrating [available expressions](@entry_id:746600) analysis with a **[points-to analysis](@entry_id:753542)** to resolve [aliasing](@entry_id:146322) and an **effect analysis** to create summaries of which functions may write to which parts of the heap. A fact like `AvailH(o)` (available hash for object `o`) is generated by a hash computation and killed by any potential mutation to the object's fingerprint, as determined by the pointer and effect analyses. This sophisticated, multi-faceted analysis is crucial for optimizing modern object-oriented and data-intensive applications. [@problem_id:3682756]

#### Connection to Formal Verification

Finally, [available expressions](@entry_id:746600) analysis has a deep connection to the field of [formal verification](@entry_id:149180). The property "expression `e` is available" is a classic example of a program **invariant**—a property that holds true at a specific program point every time it is reached. Data-flow analysis can be viewed as an algorithm for automatically discovering a specific class of invariants.

This connection can be made explicit. One can model the availability of an expression `x + y` using a "ghost" boolean variable, say `A_{x+y}`. The program is augmented with rules: `A_{x+y}` is set to `true` whenever `x + y` is computed and to `false` whenever `x` or `y` is modified. The claim that `x + y` is available at a program point `p` is then equivalent to the claim that "on all executions, whenever the program reaches `p`, the invariant `A_{x+y} = true` holds." This formulation allows the use of general-purpose [formal verification](@entry_id:149180) tools, such as model checkers, to prove or disprove claims about availability. This demonstrates that the concepts developed for [compiler optimization](@entry_id:636184) are instances of more general principles of program reasoning and correctness. [@problem_id:3622872]