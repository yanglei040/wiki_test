{"hands_on_practices": [{"introduction": "A key difference between baseline and tracing Just-In-Time (JIT) compilers lies in how they handle data types. Baseline JITs often use generic representations, such as NaN-boxing, which impose a per-operation overhead for tasks like tagging and masking. This exercise asks you to quantify the performance trade-off by comparing this overhead to the tracing JIT's strategy of specializing code within a hot loop, which pays a fixed cost at the trace boundaries but enjoys faster execution inside. By modeling this scenario, you will calculate the expected performance gain, a core analysis in modern compiler design [@problem_id:3623717].", "problem": "A dynamic language runtime uses Not a Number (NaN) boxing on a $64$ bit architecture to represent all values in a single machine word. In this scheme, integers are stored in the lower $32$ bits of the word and tagged using a quiet NaN pattern in the upper bits. A mask operation is defined as a single bitwise mask application that selects the lower $32$ bits of a word before or after arithmetic, implemented as one logical operation with a constant mask. A baseline Just-In-Time (JIT) compiler generates generic code for addition of two dynamically typed values as follows: for each operand it performs one mask to extract the integer payload, performs the integer addition, and then applies one mask to ensure the result is truncated to $32$ bits before retagging. Thus, when both operands are integers, the baseline JIT performs exactly $3$ mask operations per dynamic addition.\n\nA tracing JIT records a hot loop and specializes it to integers along a trace. Inside the trace, the values are kept unboxed, and no masks are performed for the internal arithmetic operations. However, at the trace boundary, the tracing JIT must apply masks to convert from boxed to unboxed on entry and from unboxed to boxed on exit. Assume the loop has $n$ arithmetic additions per iteration, and that $v$ integer live values cross the trace boundary each time it is entered and exited, incurring $v$ masks on entry and $v$ masks on exit. Assume that the trace executes for an independent geometrically distributed number of iterations with parameter $s$ (the per-iteration probability of a guard failure that exits the trace), and then re-enters; i.e., in steady state, each iteration independently ends the current trace run with probability $s$.\n\nGiven the following concrete parameters: $n = 30$, $v = 3$, and $s = 0.05$, derive from first principles the expected number of mask operations eliminated per arithmetic addition when using the specialized integer trace instead of the baseline JIT, in steady state across many loop iterations. Round your answer to four significant figures.", "solution": "The problem requires us to calculate the expected number of mask operations eliminated per arithmetic addition when a specialized tracing Just-In-Time (JIT) compiler is used for a hot loop, compared to a baseline JIT. The comparison is to be made in steady state.\n\nLet $C_{base}$ be the number of mask operations per addition for the baseline JIT. From the problem statement, this is $C_{base} = 3$.\nLet $n$ be the number of arithmetic additions per loop iteration, given as $n = 30$.\nLet $v$ be the number of live integer values crossing the trace boundary, given as $v = 3$.\nLet $s$ be the per-iteration probability of exiting the trace, given as $s = 0.05$.\n\nThe quantity to be determined is the expected number of masks eliminated per addition in steady state. This is best calculated as the ratio of the total expected number of masks eliminated to the total expected number of additions over a representative cycle of the system's execution. A natural cycle is one complete run of a trace, from entry to exit.\n\nLet $K$ be the random variable for the number of iterations executed in a single trace run. The problem states that each iteration independently ends the trace run with probability $s$. This describes a geometric distribution. The probability of executing exactly $k$ iterations is the probability of successfully completing $k-1$ iterations (each with probability $1-s$) and then failing to continue on the $k$-th iteration (with probability $s$).\nThus, the probability mass function for $K$ is:\n$$ P(K=k) = (1-s)^{k-1}s, \\quad \\text{for } k \\in \\{1, 2, 3, \\dots\\} $$\nThis is a standard geometric distribution. The expected value of $K$ is derived as follows:\n$$ E[K] = \\sum_{k=1}^{\\infty} k P(K=k) = \\sum_{k=1}^{\\infty} k (1-s)^{k-1}s $$\nFactoring out $s$ and using the known sum for an arithmetico-geometric series, $\\sum_{k=1}^{\\infty} k x^{k-1} = \\frac{1}{(1-x)^2}$ with $x = 1-s$:\n$$ E[K] = s \\sum_{k=1}^{\\infty} k (1-s)^{k-1} = s \\left( \\frac{1}{(1-(1-s))^2} \\right) = s \\left( \\frac{1}{s^2} \\right) = \\frac{1}{s} $$\nThe expected number of iterations per trace run is $E[K] = \\frac{1}{s}$.\n\nNow, we calculate the expected number of additions and mask operations over one full trace cycle.\n\n1.  **Expected number of additions per cycle ($E[A]$):**\n    Each iteration performs $n$ additions. A trace run of $K$ iterations performs $nK$ additions. The expected number of additions is:\n    $$ E[A] = E[nK] = n E[K] = \\frac{n}{s} $$\n\n2.  **Expected number of masks for the baseline JIT per cycle ($E[M_{base}]$):**\n    The baseline JIT performs $3$ masks per addition. For $n$ additions in an iteration, it performs $3n$ masks. Over a trace run of $K$ iterations, the total number of masks would be $3nK$. The expected number of masks is:\n    $$ E[M_{base}] = E[3nK] = 3n E[K] = \\frac{3n}{s} $$\n\n3.  **Expected number of masks for the tracing JIT per cycle ($E[M_{trace}]$):**\n    The tracing JIT incurs a fixed number of masks per trace run, regardless of its length $K$. There are $v$ masks on entry and $v$ masks on exit. No masks are used for the arithmetic operations within the trace. The total number of masks for any trace run is constant:\n    $$ M_{trace} = v + v = 2v $$\n    The expected number of masks is therefore:\n    $$ E[M_{trace}] = E[2v] = 2v $$\n\nThe expected number of masks eliminated per cycle is the difference between the baseline and tracing JIT costs:\n$$ E[M_{elim}] = E[M_{base}] - E[M_{trace}] = \\frac{3n}{s} - 2v $$\n\nThe final metric is the expected number of masks eliminated per arithmetic addition. This is the ratio of the total expected masks eliminated to the total expected additions over one cycle:\n$$ E_{elim/add} = \\frac{E[M_{elim}]}{E[A]} = \\frac{\\frac{3n}{s} - 2v}{\\frac{n}{s}} $$\nSimplifying this expression:\n$$ E_{elim/add} = \\left(\\frac{3n}{s} - 2v\\right) \\frac{s}{n} = \\frac{3n}{s} \\frac{s}{n} - 2v \\frac{s}{n} = 3 - \\frac{2vs}{n} $$\nThis is the derived formula from first principles.\n\nNow, we substitute the given numerical values: $n = 30$, $v = 3$, and $s = 0.05$.\n$$ E_{elim/add} = 3 - \\frac{2 \\times 3 \\times 0.05}{30} $$\n$$ E_{elim/add} = 3 - \\frac{6 \\times 0.05}{30} $$\n$$ E_{elim/add} = 3 - \\frac{0.3}{30} $$\n$$ E_{elim/add} = 3 - 0.01 $$\n$$ E_{elim/add} = 2.99 $$\nThe problem requires the answer to be rounded to four significant figures. The value $2.99$ has three significant figures. To express it with four, we append a zero.\n$$ E_{elim/add} \\approx 2.990 $$\nThis represents the average number of masks saved for each addition performed within the hot loop when using the tracing JIT instead of the baseline JIT.", "answer": "$$\\boxed{2.990}$$", "id": "3623717"}, {"introduction": "Tracing JITs derive much of their speed from speculation: making an optimistic assumption, such as \"this addition will not overflow,\" and generating highly efficient code based on that guess. This gamble is backed by a guard that triggers a slow \"deoptimization\" path if the assumption proves false. This practice challenges you to model the economic trade-off of such a speculation. By deriving the break-even probability for an overflow event, you will develop a formal framework to determine when a speculative optimization is profitable, a fundamental skill in performance engineering [@problem_id:3623726].", "problem": "A dynamic language runtime implements integer addition for $64$-bit signed integers within a tight loop that performs many dynamic additions of the form $a+b$. Two Just-In-Time (JIT) compilation strategies are compared:\n\n1. A baseline Just-In-Time (JIT) compiler that always emits an inline overflow-checked addition. When no overflow occurs, the dynamic addition costs an average of $B_{n}$ cycles. When an overflow occurs, it costs an average of $B_{o}$ cycles. The overflow event for any given dynamic addition occurs independently with probability $\\rho$, where $0  \\rho  1$.\n\n2. A tracing Just-In-Time (JIT) compiler that speculates that $a+b$ does not overflow and emits an unchecked addition guarded by a deoptimization guard. When no overflow occurs, the dynamic addition costs an average of $T_{n}$ cycles. When an overflow occurs, the guard fails and triggers deoptimization; the dynamic addition costs an average of $T_{o}$ cycles. The overflow event for any given dynamic addition is the same as above, occurs with probability $\\rho$, and is independent across additions.\n\nAssume steady-state execution with no warm-up effects, and model expected cycles per dynamic addition using the law of total expectation. Treat $B_{n}$, $B_{o}$, $T_{n}$, and $T_{o}$ as fixed positive constants reflecting microarchitectural effects including branch prediction, pipeline behavior, and slow-path handling, with $B_{o}  B_{n}$ and $T_{o}  T_{n}$.\n\nStarting from the definition of expected value and without invoking any specialized performance formulas, derive a closed-form analytical expression for the break-even overflow probability $\\rho^{*}$ at which the expected cycles per dynamic addition are equal between the baseline JIT and the tracing JIT. Your final answer must be a single simplified analytic expression in terms of $B_{n}$, $B_{o}$, $T_{n}$, and $T_{o}$. No numerical evaluation is required. Express the final answer as a single expression without units.", "solution": "The objective is to find the break-even overflow probability, $\\rho^{*}$, at which the expected number of cycles per dynamic addition is the same for both the baseline JIT and the tracing JIT. We will begin by defining the expected cost for each strategy based on the law of total expectation.\n\nLet $C_B$ be the random variable representing the cycle cost of a single dynamic addition executed by the baseline JIT. There are two mutually exclusive outcomes for each addition:\n1.  No overflow occurs. This happens with probability $1 - \\rho$. The cost is $B_{n}$ cycles.\n2.  An overflow occurs. This happens with probability $\\rho$. The cost is $B_{o}$ cycles.\n\nAccording to the law of total expectation, the expected cost for the baseline JIT, $E[C_B]$, is the sum of the costs of each outcome weighted by their respective probabilities:\n$$E[C_B] = B_{n} \\cdot P(\\text{no overflow}) + B_{o} \\cdot P(\\text{overflow})$$\n$$E[C_B] = B_{n}(1 - \\rho) + B_{o}\\rho$$\n\nSimilarly, let $C_T$ be the random variable for the cycle cost of an addition executed by the tracing JIT. The two outcomes are:\n1.  No overflow occurs, and the speculation succeeds. This happens with probability $1 - \\rho$. The cost is $T_{n}$ cycles.\n2.  An overflow occurs, the guard fails, and deoptimization is triggered. This happens with probability $\\rho$. The cost is $T_{o}$ cycles.\n\nThe expected cost for the tracing JIT, $E[C_T]$, is:\n$$E[C_T] = T_{n} \\cdot P(\\text{no overflow}) + T_{o} \\cdot P(\\text{overflow})$$\n$$E[C_T] = T_{n}(1 - \\rho) + T_{o}\\rho$$\n\nThe break-even point occurs at the probability $\\rho^{*}$ where the expected costs are equal:\n$$E[C_B] = E[C_T]$$\nSubstituting the expressions for the expected values, we get:\n$$B_{n}(1 - \\rho^{*}) + B_{o}\\rho^{*} = T_{n}(1 - \\rho^{*}) + T_{o}\\rho^{*}$$\n\nWe now solve this equation for $\\rho^{*}$. First, expand the terms:\n$$B_{n} - B_{n}\\rho^{*} + B_{o}\\rho^{*} = T_{n} - T_{n}\\rho^{*} + T_{o}\\rho^{*}$$\n\nNext, gather all terms containing $\\rho^{*}$ on one side of the equation and constant terms on the other:\n$$B_{o}\\rho^{*} - B_{n}\\rho^{*} + T_{n}\\rho^{*} - T_{o}\\rho^{*} = T_{n} - B_{n}$$\n\nFactor out $\\rho^{*}$ from the left-hand side:\n$$\\rho^{*}(B_{o} - B_{n} + T_{n} - T_{o}) = T_{n} - B_{n}$$\n\nFinally, isolate $\\rho^{*}$ by dividing by its coefficient, assuming a non-zero denominator:\n$$\\rho^{*} = \\frac{T_{n} - B_{n}}{B_{o} - B_{n} + T_{n} - T_{o}}$$\nThis can be rearranged to a more physically intuitive form by multiplying the numerator and denominator by $-1$ and regrouping:\n$$\\rho^{*} = \\frac{B_{n} - T_{n}}{(T_{o} - T_{n}) - (B_{o} - B_{n})}$$\nThis final form is physically intuitive. The numerator, $B_{n} - T_{n}$, represents the performance gain (cycle savings) of the tracing JIT in the common case (no overflow). The denominator, $(T_{o} - T_{n}) - (B_{o} - B_{n})$, represents the additional penalty incurred by the tracing JIT during an overflow event compared to the baseline JIT's overflow penalty. The break-even probability is thus the ratio of this fast-path gain to the extra slow-path cost.", "answer": "$$ \\boxed{\\frac{B_n - T_n}{(T_o - T_n) - (B_o - B_n)}} $$", "id": "3623726"}, {"introduction": "Performance optimizations like speculation are only viable if they are correct. The safety net for a tracing JIT is its ability to deoptimize—perfectly reconstruct the program's state and resume execution in the interpreter when a guard fails. This exercise shifts our focus from performance to correctness, asking you to think like a compiler test engineer. You will reason about the formal execution model of a virtual machine to identify the critical invariants that a deoptimization snapshot must preserve, learning to distinguish the essential, abstract state of the program from the transient implementation details of the JIT compiler [@problem_id:3623731].", "problem": "A virtual machine for a dynamically typed language implements both a baseline just-in-time compiler (baseline JIT) and a tracing just-in-time compiler (tracing JIT). Both compilers maintain deoptimization snapshots: the baseline JIT records snapshots at safe points in compiled methods; the tracing JIT attaches snapshots to guard sites in a trace so that a failing guard can reconstruct an interpreter frame (or a stack of frames) and resume in the interpreter.\n\nYou are tasked with designing a snapshot fidelity test that mutates local variables and operand stack entries immediately prior to a forced deoptimization, then checks that the deoptimized execution continues exactly as if it were executed by the interpreter from the same program point. The test framework can run the same program fragment under the interpreter and under the JIT with deoptimization triggered at a chosen point, and can read out the values of locals and the operand stack at that point.\n\nUse the following formal base:\n\n- Let the small-step operational semantics of the interpreter be a relation $s \\Rightarrow s'$ on states. A state $s$ is a tuple $s = \\langle P, \\kappa, \\rho, S, H, M \\rangle$ where $P$ is the program, $\\kappa$ is a call stack of frames with program counters, $\\rho$ is the environment mapping local variable identifiers to values, $S$ is the operand stack, $H$ is the heap, and $M$ is the multiset of held monitors (locks) with reentrancy counts. The interpreter-visible state at a program point is the projection $\\pi(s) = \\langle \\kappa, \\rho, S, H, M \\rangle$.\n- Let $\\mathcal{L}_i$ be the set of live locals and stack slots at bytecode index $i$ according to a standard liveness analysis on the interpreter’s control-flow graph. Equality of primitive values is the usual value equality, while equality of references is by object identity in $H$.\n- Let $T(s)$ denote the (possibly infinite) sequence of externally observable events produced by continuing execution from state $s$ under the interpreter’s semantics (e.g., I/O, final store results to $H$ observable by the program).\n\nA deoptimization snapshot associated with a JIT execution state $s_{\\mathrm{jit}}$ at a program point $i$ is correct if it reconstructs an interpreter state $s_{\\mathrm{int}}$ such that $\\pi(s_{\\mathrm{jit}})$ and $\\pi(s_{\\mathrm{int}})$ agree on the interpreter-visible state needed to continue execution and $T(s_{\\mathrm{int}})$ equals the trace produced by the interpreter had it reached the same point without JIT.\n\nYour test mutates locals and operand stack entries prior to deoptimization, then asserts invariants at the moment of deopt and also by continuing execution after deopt and comparing to a pure-interpreter run paused and resumed at the corresponding bytecode index.\n\nWhich of the following invariants must the snapshot fidelity test assert to soundly validate that deoptimization reconstructs exactly the interpreter-visible state, for both baseline JIT and tracing JIT? Select all that apply.\n\nA. For each element of the live set $\\mathcal{L}_i$, the deoptimized interpreter frame’s environment $\\rho$ and operand stack $S$ contain values that are equal to those in the pure-interpreter execution at bytecode index $i$, with primitive values equal by value and reference values equal by object identity in $H$.\n\nB. The contents of all machine registers and spill slots in the JIT-compiled frame must match a hypothetical interpreter register allocation at the deopt point; the test should fail if any register differs.\n\nC. The deoptimized frame stack $\\kappa$ (including any frames reconstructed from inlining in a trace) has the same call sequence and bytecode indices as the pure-interpreter call stack at the corresponding point, and the held monitors $M$ (including reentrancy counts) match by object identity and counts.\n\nD. Values of locals and stack slots that are not in the live set $\\mathcal{L}_i$ must also match the pure-interpreter execution at bytecode index $i$, to detect latent miscompilations that might later become visible.\n\nE. If execution continues after deoptimization, the observable event trace $T(s_{\\mathrm{int}})$ from the deoptimized state equals the trace from the pure-interpreter state paused at bytecode index $i$ and resumed, i.e., the two executions are observationally indistinguishable to the program.\n\nF. The set of garbage-collector roots at the deopt point must be exactly equal to the interpreter’s set of roots; any extra roots retained by the JIT during deopt indicate a snapshot mismatch.\n\nProvide the best answer by selecting all and only the invariants that are necessary to validate snapshot fidelity as defined above.", "solution": "The problem requires identifying the necessary invariants that a snapshot fidelity test must assert to validate that deoptimization from a Just-In-Time (JIT) compiled state to an interpreter state is correct. The problem provides a formal definition of the interpreter state and the correctness of a deoptimization snapshot.\n\nA deoptimization is correct if the program's execution can continue from the reconstructed interpreter state as if it had been running in the interpreter all along. This means the reconstructed state must be semantically equivalent to the state the interpreter would have had at the same program point, and consequently, the future observable behavior of the program must be identical.\n\nThe problem defines the interpreter state as $s = \\langle P, \\kappa, \\rho, S, H, M \\rangle$, where $P$ is the program, $\\kappa$ is the call stack, $\\rho$ is the environment of local variables, $S$ is the operand stack, $H$ is the heap, and $M$ is the set of held monitors. The \"interpreter-visible state\" is defined as the projection $\\pi(s) = \\langle \\kappa, \\rho, S, H, M \\rangle$. The correctness of a deoptimization snapshot from a JIT state $s_{\\mathrm{jit}}$ to an interpreter state $s_{\\mathrm{int}}$ is defined by two conditions:\n1.  **State Equivalence:** The reconstructed state $s_{\\mathrm{int}}$ must agree with the interpreter-visible state needed to continue execution. The question clarifies this as reconstructing \"exactly the interpreter-visible state\". This means the components of $\\pi(s_{\\mathrm{int}})$ must be equivalent to those of the pure-interpreter state $s_{\\mathrm{pure-int}}$ at the same program point.\n2.  **Behavioral Equivalence:** The sequence of externally observable events $T(s_{\\mathrm{int}})$ produced by continuing execution from the reconstructed state must equal the trace that would have been produced by the interpreter, $T(s_{\\mathrm{pure-int}})$.\n\nA sound validation test must, therefore, assert invariants that are sufficient to establish these two conditions. The test procedure is described as asserting invariants \"at the moment of deopt and also by continuing execution after deopt\". This implies checking both the state and the subsequent behavior.\n\nLet's evaluate each option:\n\n**A. For each element of the live set $\\mathcal{L}_i$, the deoptimized interpreter frame’s environment $\\rho$ and operand stack $S$ contain values that are equal to those in the pure-interpreter execution at bytecode index $i$, with primitive values equal by value and reference values equal by object identity in $H$.**\n\n-   **Analysis:** This invariant directly tests parts of the interpreter-visible state: the local variable environment $\\rho$ and the operand stack $S$. The correctness of future execution fundamentally depends on the values of variables and temporary operands that will be used. The restriction to the *live set* $\\mathcal{L}_i$ is crucial. By definition, variables not in the live set will not be read before their next write, so their values are irrelevant to the program's future semantics. A JIT compiler is free to disregard or repurpose storage for dead variables. The specified method of comparison—by value for primitives and by object identity for references—is the standard for semantic equivalence of program states. This is a check \"at the moment of deopt\" and is necessary for validating the data-state reconstruction.\n-   **Verdict:** Correct.\n\n**B. The contents of all machine registers and spill slots in the JIT-compiled frame must match a hypothetical interpreter register allocation at the deopt point; the test should fail if any register differs.**\n\n-   **Analysis:** This invariant concerns the implementation-specific details of the JIT-compiled code (machine registers, stack layout for spills). Deoptimization is a translation from this low-level, machine-specific representation to the high-level, abstract representation of the interpreter. The JIT's register allocation and memory layout are its own concern; all that matters is its ability to correctly reconstruct the abstract interpreter state. There is no \"hypothetical interpreter register allocation\" to compare against, as the interpreter operates on an abstract machine model. This check incorrectly conflates implementation with specification.\n-   **Verdict:** Incorrect.\n\n**C. The deoptimized frame stack $\\kappa$ (including any frames reconstructed from inlining in a trace) has the same call sequence and bytecode indices as the pure-interpreter call stack at the corresponding point, and the held monitors $M$ (including reentrancy counts) match by object identity and counts.**\n\n-   **Analysis:** This invariant tests two other critical components of the interpreter-visible state: the call stack $\\kappa$ and the held monitors $M$. The call stack dictates control flow (e.g., where `return` statements transfer control) and provides the context for each active function. JIT compilers, particularly tracing JITs, often perform inlining, where multiple logical interpreter frames are compiled into a single-machine code function. Correct deoptimization requires reconstructing the original sequence of interpreter frames. The set of held monitors $M$ is essential for correct concurrent execution, ensuring that synchronization primitives behave as expected. This is another necessary check \"at the moment of deopt\" to validate the control and synchronization state.\n-   **Verdict:** Correct.\n\n**D. Values of locals and stack slots that are not in the live set $\\mathcal{L}_i$ must also match the pure-interpreter execution at bytecode index $i$, to detect latent miscompilations that might later become visible.**\n\n-   **Analysis:** This is an overly strict condition. As explained for option A, the values of dead variables do not affect the future execution of the program. Requiring them to match the interpreter's state is not necessary for semantic correctness. A JIT compiler may legitimately use dead variable slots for its own purposes (e.g., to store intermediate results of a different type) and is not obligated to preserve their original values. Forcing this match would penalize correct and efficient compiler implementations.\n-   **Verdict:** Incorrect.\n\n**E. If execution continues after deoptimization, the observable event trace $T(s_{\\mathrm{int}})$ from the deoptimized state equals the trace from the pure-interpreter state paused at bytecode index $i$ and resumed, i.e., the two executions are observationally indistinguishable to the program.**\n\n-   **Analysis:** This invariant corresponds directly to the second part of the problem's definition of correctness: behavioral equivalence, $T(s_{\\mathrm{int}}) = T(s_{\\mathrm{pure-int}})$. This is the ultimate test of fidelity. While correct state reconstruction (checked by A and C) should imply correct future behavior, this check is necessary for a *sound* validation. It can detect subtle bugs that are not immediately apparent from the stack state, such as incorrect modifications to the heap $H$ or other system state that is part of the full state $s$ but not directly inspected by options A and C. The problem states that the test framework \"continues execution after deopt and comparing,\" indicating this is an intended part of the validation. Therefore, it is a necessary invariant to assert.\n-   **Verdict:** Correct.\n\n**F. The set of garbage-collector roots at the deopt point must be exactly equal to the interpreter’s set of roots; any extra roots retained by the JIT during deopt indicate a snapshot mismatch.**\n\n-   **Analysis:** The set of garbage collection (GC) roots consists of all references from which live objects are reachable. This set is derived from the live variables in $\\rho$, $S$, and any global/static variables. Option A already ensures that all references that are live from the interpreter's viewpoint are correctly restored. A JIT may, for implementation reasons, retain references in registers or memory for longer than they are strictly live from the program's perspective. This is known as having conservative roots. As long as the JIT's root set is a superset of the interpreter's required root set, no live object will be incorrectly collected. Demanding exact equality is too strict and would forbid common, correct compiler optimizations. It would lead to test failures for correct programs.\n-   **Verdict:** Incorrect.\n\nIn summary, a sound and comprehensive snapshot fidelity test as described must validate the core components of the interpreter-visible state at the moment of deoptimization (A and C) and also validate that this reconstructed state leads to the correct future behavior (E), which serves as a holistic check and is part of the formal definition of correctness.", "answer": "$$\\boxed{ACE}$$", "id": "3623731"}]}