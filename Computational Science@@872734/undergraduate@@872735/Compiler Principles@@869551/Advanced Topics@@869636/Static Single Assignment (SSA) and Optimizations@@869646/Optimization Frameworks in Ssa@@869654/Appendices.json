{"hands_on_practices": [{"introduction": "This first exercise dives into the mechanics of Sparse Conditional Constant Propagation (SCCP), a powerful optimization that leverages the SSA form. Unlike traditional constant propagation, SCCP simultaneously determines which parts of the program are executable, allowing it to find more optimization opportunities. By tracing the analysis on a small program, you will see firsthand how SCCP evaluates $\\phi$-functions and prunes un-executable code paths, demonstrating the interplay between data-flow values and control-flow information. [@problem_id:3660120]", "problem": "You are given a small program in Static Single Assignment (SSA) form that illustrates how constant folding across a $\\phi$-function behaves under Sparse Conditional Constant Propagation (SCCP). The environment uses the standard three-point value lattice $\\{\\bot,\\text{constant},\\top\\}$ for each SSA value and maintains a set of executable edges to model control-flow feasibility. A $\\phi$-function in a join block selects the operand corresponding to the control-flow predecessor actually taken at runtime, and under SCCP its value is computed as the meet over the value lattice of the operands coming from executable predecessors only. Assume integer booleans with $0$ for false and $1$ for true, and integer arithmetic. The control-flow graph (CFG) and SSA code are:\n\n- Block $\\mathrm{B0}$ (entry):\n  - $q \\leftarrow 0$\n  - if $q \\neq 0$ goto $\\mathrm{B4}$ else goto $\\mathrm{B00}$\n- Block $\\mathrm{B4}$:\n  - $a_4 \\leftarrow 3$\n  - goto $\\mathrm{J}$\n- Block $\\mathrm{B00}$:\n  - $p \\leftarrow 1$\n  - if $p \\neq 0$ goto $\\mathrm{B1}$ else goto $\\mathrm{B2}$\n- Block $\\mathrm{B1}$:\n  - $a_1 \\leftarrow 3$\n  - $b_1 \\leftarrow 3$\n  - goto $\\mathrm{J}$\n- Block $\\mathrm{B2}$:\n  - $a_2 \\leftarrow 3$\n  - $b_2 \\leftarrow 4$\n  - goto $\\mathrm{J}$\n- Block $\\mathrm{J}$ (join of predecessors $\\mathrm{B1}$, $\\mathrm{B2}$, $\\mathrm{B4}$):\n  - $a \\leftarrow \\phi(a_1,a_2,a_4)$\n  - $b \\leftarrow \\phi(b_1,b_2)$\n  - $\\mathrm{out} \\leftarrow 10 \\cdot a + b$\n  - return $\\mathrm{out}$\n\nYour task is to determine the numeric value returned by this program after applying Sparse Conditional Constant Propagation to a fixed point, followed by standard algebraic simplification and constant folding. You must base your reasoning on the following fundamental definitions and facts:\n\n- Static Single Assignment (SSA) form assigns each variable exactly once and uses $\\phi$-functions to merge values from multiple predecessors at join points in the control-flow graph. The semantics of a $\\phi$-function is to yield the operand corresponding to the control-flow path actually taken.\n- Sparse Conditional Constant Propagation (SCCP) simultaneously reasons about value information using the lattice $\\{\\bot,\\text{constant},\\top\\}$ and about control-flow executability. The transfer functions are monotone, and the analysis iterates to a fixed point. A $\\phi$-function’s value is the meet, restricted to executable predecessors, of its incoming operand values in the value lattice. If multiple executable incoming operands are distinct constants, the $\\phi$-function becomes $\\top$; if they are equal constants, it becomes that constant; if there is exactly one executable predecessor with a constant and all other executable predecessors are $\\bot$, it becomes that constant.\n\nExplain, from first principles and using the above framework, how SCCP discovers and exploits:\n- Constant folding across a $\\phi$-function when all incoming operands are the same constant, as in $a \\leftarrow \\phi(3,3,3)$.\n- Convergence when a $\\phi$-function has distinct constants on different predecessors, as in $b \\leftarrow \\phi(3,4)$, but one predecessor is discovered to be infeasible.\n\nCompute the final returned value $\\mathrm{out}$ as an exact integer. Do not round, and do not include any units.", "solution": "The problem is first validated against the specified criteria.\n\n### Step 1: Extract Givens\n- **Program in SSA form and its Control-Flow Graph (CFG):**\n  - Block $\\mathrm{B0}$ (entry):\n    - $q \\leftarrow 0$\n    - if $q \\neq 0$ goto $\\mathrm{B4}$ else goto $\\mathrm{B00}$\n  - Block $\\mathrm{B4}$:\n    - $a_4 \\leftarrow 3$\n    - goto $\\mathrm{J}$\n  - Block $\\mathrm{B00}$:\n    - $p \\leftarrow 1$\n    - if $p \\neq 0$ goto $\\mathrm{B1}$ else goto $\\mathrm{B2}$\n  - Block $\\mathrm{B1}$:\n    - $a_1 \\leftarrow 3$\n    - $b_1 \\leftarrow 3$\n    - goto $\\mathrm{J}$\n  - Block $\\mathrm{B2}$:\n    - $a_2 \\leftarrow 3$\n    - $b_2 \\leftarrow 4$\n    - goto $\\mathrm{J}$\n  - Block $\\mathrm{J}$ (join of predecessors $\\mathrm{B1}$, $\\mathrm{B2}$, $\\mathrm{B4}$):\n    - $a \\leftarrow \\phi(a_1, a_2, a_4)$\n    - $b \\leftarrow \\phi(b_1, b_2)$\n    - $\\mathrm{out} \\leftarrow 10 \\cdot a + b$\n    - return $\\mathrm{out}$\n- **Sparse Conditional Constant Propagation (SCCP) Framework:**\n  - Value Lattice: $\\{\\bot,\\text{constant},\\top\\}$, where $\\bot$ means undefined, a constant is a specific value (e.g., $0, 1, 3, 4$), and $\\top$ means overdefined (not a constant).\n  - Control-Flow: A set of executable edges is maintained.\n  - $\\phi$-function evaluation: The value is the meet of its operand values, restricted to those coming from executable predecessors.\n- **Integer Semantics:** Booleans are represented as integers, with $0$ for false and $1$ for true.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded within the domain of compiler theory, a subfield of computer science. The concepts of Static Single Assignment (SSA), Control-Flow Graphs (CFGs), and Sparse Conditional Constant Propagation (SCCP) are standard and well-defined. The provided lattice and meet rules are the canonical formulation for SCCP. The problem is well-posed, providing all necessary code and semantic rules to uniquely determine the outcome of the analysis. The language is objective and precise. The problem is self-contained and internally consistent.\n\n### Step 3: Verdict and Action\nThe problem is valid. A detailed solution follows.\n\nThe solution proceeds by simulating the Sparse Conditional Constant Propagation (SCCP) algorithm on the given program until a fixed point is reached. The algorithm maintains a lattice value for each variable and a set of executable CFG edges. We use worklists for variables whose values have changed and for newly discovered executable edges.\n\n**Initialization:**\n1.  All variables are initialized to $\\bot$: $v(q) = \\bot$, $v(p) = \\bot$, $v(a_1) = \\bot$, etc.\n2.  All CFG edges are initially considered not executable.\n3.  The entry block $\\mathrm{B0}$ is always reachable. We add the (conceptual) edge from the program entry to $\\mathrm{B0}$ to a worklist of executable edges.\n\n**Execution Trace:**\n\n1.  **Block $\\mathrm{B0}$:** The algorithm begins by visiting the entry block $\\mathrm{B0}$.\n    - The instruction $q \\leftarrow 0$ is evaluated. The lattice value of $q$ changes from $\\bot$ to the constant $0$.\n    - The conditional branch `if $q \\neq 0$` is evaluated. Since $v(q) = 0$, the condition $0 \\neq 0$ is statically determined to be false.\n    - Consequently, the control-flow edge $\\mathrm{B0} \\rightarrow \\mathrm{B4}$ is determined to be non-executable.\n    - The edge $\\mathrm{B0} \\rightarrow \\mathrm{B00}$ is determined to be executable and is processed.\n\n2.  **Block $\\mathrm{B00}$:** The algorithm proceeds to block $\\mathrm{B00}$.\n    - The instruction $p \\leftarrow 1$ sets the lattice value of $p$ to the constant $1$.\n    - The conditional branch `if $p \\neq 0$` is evaluated. Since $v(p) = 1$, the condition $1 \\neq 0$ is statically determined to be true.\n    - The edge $\\mathrm{B00} \\rightarrow \\mathrm{B2}$ is non-executable.\n    - The edge $\\mathrm{B00} \\rightarrow \\mathrm{B1}$ is executable and is processed.\n\n3.  **Dead Code Elimination:** At this stage, SCCP has proven that blocks $\\mathrm{B4}$ and $\\mathrm{B2}$ are unreachable. Any instructions within these blocks will not be evaluated, and their defined variables will remain at their initial lattice value of $\\bot$. The control-flow paths that lead to them ($\\mathrm{B0} \\rightarrow \\mathrm{B4}$ and $\\mathrm{B00} \\rightarrow \\mathrm{B2}$) are pruned from the analysis of the join block $\\mathrm{J}$.\n\n4.  **Block $\\mathrm{B1}$:** The algorithm proceeds to the only reachable successor, block $\\mathrm{B1}$.\n    - The instruction $a_1 \\leftarrow 3$ sets $v(a_1)$ to the constant $3$.\n    - The instruction $b_1 \\leftarrow 3$ sets $v(b_1)$ to the constant $3$.\n    - The unconditional jump `goto $\\mathrm{J}$` makes the edge $\\mathrm{B1} \\rightarrow \\mathrm{J}$ executable.\n\n5.  **Block $\\mathrm{J}$ (Join):** The algorithm reaches the join point $\\mathrm{J}$. The $\\phi$-functions are now evaluated. The SCCP rule states that the meet operation for a $\\phi$-function is performed only over operands corresponding to executable predecessor edges.\n    - **Evaluation of $a \\leftarrow \\phi(a_1, a_2, a_4)$:**\n      The predecessors of $\\mathrm{J}$ are $\\mathrm{B1}$, $\\mathrm{B2}$, and $\\mathrm{B4}$. However, the analysis has proven that only the edge $\\mathrm{B1} \\rightarrow \\mathrm{J}$ is executable. Therefore, the value of $a$ is determined solely by the operand from $\\mathrm{B1}$.\n      $$v(a) = v(a_1) = 3$$\n      The variable $a$ is now known to be the constant $3$.\n\n    - **Explanation for constant folding across $\\phi$ (as requested):**\n      The problem asks to explain folding for $a \\leftarrow \\phi(3,3,3)$. The assignments are $a_1 \\leftarrow 3$, $a_2 \\leftarrow 3$, and $a_4 \\leftarrow 3$. In our specific case, since only the path through $\\mathrm{B1}$ is live, the values of $a_2$ and $a_4$ (defined in dead blocks) are irrelevant. The result is determined by $a_1$ alone.\n      However, in a hypothetical scenario where all three paths to $\\mathrm{J}$ were found to be executable, SCCP would evaluate all three assignments, yielding $v(a_1)=3$, $v(a_2)=3$, and $v(a_4)=3$. The $\\phi$-function's value would be the meet of these constants:\n      $$v(a) = \\text{meet}(v(a_1), v(a_2), v(a_4)) = \\text{meet}(3, 3, 3) = 3$$\n      Thus, SCCP folds the $\\phi$-function to a constant because the meet of identical constants is that constant itself.\n\n    - **Evaluation of $b \\leftarrow \\phi(b_1, b_2)$:**\n      The text of the $\\phi$-function for $b$ indicates it merges values from predecessors $\\mathrm{B1}$ and $\\mathrm{B2}$. As established, only the path through $\\mathrm{B1}$ is executable.\n      $$v(b) = v(b_1) = 3$$\n      The variable $b$ is determined to be the constant $3$.\n\n    - **Explanation for convergence with infeasible paths (as requested):**\n      The problem asks to explain the case of $b \\leftarrow \\phi(3,4)$, where $b_1 \\leftarrow 3$ and $b_2 \\leftarrow 4$. This is the core demonstration of SCCP's power. If both paths through $\\mathrm{B1}$ and $\\mathrm{B2}$ were executable, the evaluation would be:\n      $$v(b) = \\text{meet}(v(b_1), v(b_2)) = \\text{meet}(3, 4) = \\top$$\n      This would mean $b$ is not a constant. However, SCCP's simultaneous analysis of values and control flow proves that the path through $\\mathrm{B2}$ is unreachable. Therefore, the operand $b_2$ from this dead path is excluded from the meet calculation. The algorithm considers only the value from the live path, resulting in $v(b) = v(b_1) = 3$. This prevents the value from conservatively becoming $\\top$ and allows for more precise constant propagation.\n\n6.  **Final Calculation:** With the fixed-point values $v(a)=3$ and $v(b)=3$, the final instruction in block $\\mathrm{J}$ is evaluated:\n    - $\\mathrm{out} \\leftarrow 10 \\cdot a + b$\n    - This is computed using the discovered constants: $\\mathrm{out} \\leftarrow 10 \\cdot 3 + 3$.\n    - The expression is folded to a single constant: $\\mathrm{out} \\leftarrow 30 + 3 = 33$.\n    - The lattice value of $\\mathrm{out}$ becomes the constant $33$.\n\nThe program returns the value of $\\mathrm{out}$, which the SCCP analysis has determined to be $33$.", "answer": "$$\\boxed{33}$$", "id": "3660120"}, {"introduction": "Building upon the fundamentals of data flow on SSA, this practice moves from analysis to transformation. Here, you will tackle Partial Redundancy Elimination (PRE), an optimization that aims to remove computations that are redundant on some, but not all, execution paths. You will first apply your knowledge of dominance frontiers to correctly place $\\phi$-functions, and then use the resulting SSA graph to devise a minimal plan for hoisting and eliminating a partially redundant expression. [@problem_id:3660116]", "problem": "A compiler for a simple imperative language uses Static Single Assignment (SSA) form and drives optimizations with dominance frontiers. Consider the following acyclic Control Flow Graph (CFG) with basic blocks labeled $B_0$ through $B_7$. All arithmetic is over mathematical integers, operations have no side effects, and no instruction can raise an exception. Variables $x$ and $y$ are initially defined in $B_0$ by reading incoming parameters $x_0$ and $y_0$, respectively. The program is:\n\n- $B_0$: entry; $x := x_0$; $y := y_0$; if predicate $p$ then goto $B_1$ else goto $B_2$.\n- $B_1$: $x := 10$; goto $B_3$.\n- $B_2$: $y := 20$; goto $B_3$.\n- $B_3$: if predicate $q$ then goto $B_4$ else goto $B_5$.\n- $B_4$: $y := 30$; $u := x + y$; goto $B_6$.\n- $B_5$: $x := 40$; goto $B_6$.\n- $B_6$: $v := x + y$; goto $B_7$.\n- $B_7$: return $v$.\n\nYour tasks are as follows, starting from core definitions used in compiler theory:\n\n1. Using the definition of dominance and dominance frontier, determine the minimal placement of $\\phi$-functions for variables $x$ and $y$ to construct SSA form. Assume pruned SSA, but note that $x$ and $y$ are both live at the point of the computation $v := x + y$ in $B_6$. Give your reasoning based only on the structure of the CFG and the definition sites shown above.\n\n2. Using the principles of SSA-based Partial Redundancy Elimination (PRE), reason about the expression $x + y$ in the blocks above. Identify whether $x + y$ is partially redundant at $B_6$ relative to computations earlier in the CFG and propose a plan that preserves semantics and minimizes the number of inserted computations of $x + y$. Your plan may insert computations in predecessors and remove redundant computations, and may rely on $\\phi$-functions to merge values, but only explicit evaluations of $x + y$ count as insertions of computations.\n\n3. Let $N_{\\phi,x}$ be the number of $\\phi$-functions inserted for $x$, $N_{\\phi,y}$ be the number of $\\phi$-functions inserted for $y$, and $N_{\\mathrm{ins}}$ be the minimal number of newly inserted explicit evaluations of $x + y$ required by your PRE plan. Compute the total\n$$\nT \\;=\\; N_{\\phi,x} \\;+\\; N_{\\phi,y} \\;+\\; N_{\\mathrm{ins}}.\n$$\n\nProvide only the value of $T$ as your final answer. No rounding is needed, and no units are required.", "solution": "The user-provided problem is first validated against the specified criteria.\n\n### Problem Validation\n\n#### Step 1: Extract Givens\n- **Control Flow Graph (CFG)**:\n    - $B_0$: entry; successors are $B_1$, $B_2$.\n    - $B_1$: successor is $B_3$.\n    - $B_2$: successor is $B_3$.\n    - $B_3$: successors are $B_4$, $B_5$.\n    - $B_4$: successor is $B_6$.\n    - $B_5$: successor is $B_6$.\n    - $B_6$: successor is $B_7$.\n    - $B_7$: return block.\n- **Variable Definitions**:\n    - $B_0$: $x := x_0$; $y := y_0$.\n    - $B_1$: $x := 10$.\n    - $B_2$: $y := 20$.\n    - $B_4$: $y := 30$.\n    - $B_5$: $x := 40$.\n- **Computations**:\n    - $B_4$: $u := x + y$.\n    - $B_6$: $v := x + y$.\n- **Liveness Information**:\n    - Variables $x$ and $y$ are live at the point of computation in $B_6$.\n- **Tasks**:\n    1. Determine minimal placement of $\\phi$-functions for variables $x$ and $y$.\n    2. Analyze partial redundancy of the expression $x+y$ at $B_6$ and propose a minimal insertion plan.\n    3. Compute $T = N_{\\phi,x} + N_{\\phi,y} + N_{\\mathrm{ins}}$.\n\n#### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is based on standard, well-established concepts in compiler theory, namely Static Single Assignment (SSA), dominance, dominance frontiers, and Partial Redundancy Elimination (PRE). These are core topics in compiler design and optimization. The problem is sound.\n- **Well-Posed**: The CFG is fully specified, the initial definitions are clear, and the optimization goals (minimal placement, minimal insertions) are standard. The problem is structured to have a unique, determinable solution based on these compiler algorithms.\n- **Objective**: The problem uses formal, unambiguous terminology from computer science. There are no subjective or opinion-based statements.\n\nThe problem does not exhibit any of the invalidity flaws. It is self-contained, consistent, and formalizable.\n\n#### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Solution\n\nThe solution is provided in three parts corresponding to the tasks.\n\n#### 1. Minimal $\\phi$-Function Placement\n\nTo determine the minimal placement of $\\phi$-functions, we first need to compute the dominator tree and then the dominance frontiers for the given Control Flow Graph (CFG).\n\n**Control Flow Graph Successors:**\n- $\\text{succ}(B_0) = \\{B_1, B_2\\}$\n- $\\text{succ}(B_1) = \\{B_3\\}$\n- $\\text{succ}(B_2) = \\{B_3\\}$\n- $\\text{succ}(B_3) = \\{B_4, B_5\\}$\n- $\\text{succ}(B_4) = \\{B_6\\}$\n- $\\text{succ}(B_5) = \\{B_6\\}$\n- $\\text{succ}(B_6) = \\{B_7\\}$\n\n**Dominators and Dominator Tree:**\nA block $A$ dominates a block $B$ if every path from the entry block ($B_0$) to $B$ must pass through $A$. The immediate dominator, $\\text{idom}(B)$, is the unique dominator of $B$ that is dominated by all other dominators of $B$.\n- $\\text{dom}(B_0) = \\{B_0\\}$\n- $\\text{dom}(B_1) = \\{B_0, B_1\\}$, so $\\text{idom}(B_1) = B_0$.\n- $\\text{dom}(B_2) = \\{B_0, B_2\\}$, so $\\text{idom}(B_2) = B_0$.\n- $\\text{dom}(B_3) = \\{B_0, B_3\\}$, as all paths to $B_3$ must pass through $B_0$. $\\text{idom}(B_3) = B_0$.\n- $\\text{dom}(B_4) = \\{B_0, B_3, B_4\\}$, so $\\text{idom}(B_4) = B_3$.\n- $\\text{dom}(B_5) = \\{B_0, B_3, B_5\\}$, so $\\text{idom}(B_5) = B_3$.\n- $\\text{dom}(B_6) = \\{B_0, B_3, B_6\\}$, as all paths to $B_6$ must pass through $B_3$. $\\text{idom}(B_6) = B_3$.\n- $\\text{dom}(B_7) = \\{B_0, B_3, B_6, B_7\\}$, so $\\text{idom}(B_7) = B_6$.\n\nThe dominator tree is: $B_0$ is the root. $B_1, B_2, B_3$ are children of $B_0$. $B_4, B_5, B_6$ are children of $B_3$. $B_7$ is the child of $B_6$.\n\n**Dominance Frontiers (DF):**\nThe dominance frontier of a node $N$, $DF(N)$, is the set of all nodes $M$ such that $N$ dominates an immediate predecessor of $M$, but $N$ does not strictly dominate $M$.\n- $DF(B_0) = \\emptyset$.\n- $DF(B_1)$: $B_1$ dominates a predecessor of $B_3$ (i.e., $B_1$ itself), but $B_1$ does not strictly dominate $B_3$. So, $DF(B_1) = \\{B_3\\}$.\n- $DF(B_2)$: Similarly, $B_2$ dominates a predecessor of $B_3$, but not $B_3$ itself. So, $DF(B_2) = \\{B_3\\}$.\n- $DF(B_3) = \\emptyset$. $B_3$ strictly dominates all its successors in the dominator tree.\n- $DF(B_4)$: $B_4$ dominates a predecessor of $B_6$, but not $B_6$ itself. So, $DF(B_4) = \\{B_6\\}$.\n- $DF(B_5)$: Similarly, $B_5$ dominates a predecessor of $B_6$, but not $B_6$ itself. So, $DF(B_5) = \\{B_6\\}$.\n- $DF(B_6) = \\emptyset$.\n- $DF(B_7) = \\emptyset$.\n\n**$\\phi$-Function Placement for variable $x$:**\nThe set of blocks containing definitions of $x$ is $A_x = \\{B_0, B_1, B_5\\}$. We compute the iterated dominance frontier, $DF^+(A_x)$.\n1.  Start with the dominance frontiers of the defining blocks: $S_0 = DF(B_0) \\cup DF(B_1) \\cup DF(B_5) = \\emptyset \\cup \\{B_3\\} \\cup \\{B_6\\} = \\{B_3, B_6\\}$.\n2.  Iterate: Now compute the dominance frontiers of the blocks in $S_0$: $S_1 = S_0 \\cup DF(B_3) \\cup DF(B_6) = \\{B_3, B_6\\} \\cup \\emptyset \\cup \\emptyset = \\{B_3, B_6\\}$.\nThe set has converged. The potential placement sites for $\\phi$-functions for $x$ are $B_3$ and $B_6$.\nFor pruned SSA, we only place a $\\phi$-function if the variable is live at that point.\n- At $B_3$, variable $x$ is live on exit towards $B_4$ because it is used in the expression $u := x + y$. Thus, a $\\phi$-function for $x$ is needed in $B_3$.\n- At $B_6$, the problem states $x$ is live for the computation $v := x + y$. Thus, a $\\phi$-function for $x$ is needed in $B_6$.\nThe number of $\\phi$-functions for $x$ is $N_{\\phi,x} = 2$.\n\n**$\\phi$-Function Placement for variable $y$:**\nThe set of blocks containing definitions of $y$ is $A_y = \\{B_0, B_2, B_4\\}$. We compute $DF^+(A_y)$.\n1.  Start with the dominance frontiers of the defining blocks: $S_0 = DF(B_0) \\cup DF(B_2) \\cup DF(B_4) = \\emptyset \\cup \\{B_3\\} \\cup \\{B_6\\} = \\{B_3, B_6\\}$.\n2.  Iterate: $S_1 = S_0 \\cup DF(B_3) \\cup DF(B_6) = \\{B_3, B_6\\} \\cup \\emptyset \\cup \\emptyset = \\{B_3, B_6\\}$.\nThe set has converged. The potential placement sites for $\\phi$-functions for $y$ are $B_3$ and $B_6$.\n- At $B_3$, variable $y$ is live on exit towards $B_4$ for the computation $u := x + y$. A $\\phi$-function for $y$ is needed.\n- At $B_6$, the problem states $y$ is live for $v := x + y$. A $\\phi$-function for $y$ is needed.\nThe number of $\\phi$-functions for $y$ is $N_{\\phi,y} = 2$.\n\n#### 2. Partial Redundancy Elimination (PRE)\n\nThe expression of interest is $E = x + y$. It is computed in $B_4$ as $u := x + y$ and in $B_6$ as $v := x + y$.\nWe analyze the computation at $B_6$. A computation is partially redundant if it is computed on at least one, but not all, paths leading to that point. The predecessors of $B_6$ are $B_4$ and $B_5$.\n- Path through $B_4$: The path from $B_3$ is $B_3 \\to B_4 \\to B_6$. The expression $x+y$ is computed in $B_4$.\n- Path through $B_5$: The path from $B_3$ is $B_3 \\to B_5 \\to B_6$. The expression $x+y$ is not computed in $B_5$.\n\nSince $E$ is computed on the path through $B_4$ but not on the path through $B_5$, the computation $v := x + y$ in $B_6$ is partially redundant.\n\nTo eliminate this partial redundancy, we must make the computation fully available at the entry of $B_6$. This means the value of $x+y$ must be computed on all paths leading to $B_6$. The path through $B_5$ lacks this computation. Therefore, we must insert a computation of $x+y$ on this path. The latest possible point for this insertion is in block $B_5$ itself.\n\nLet's consider the SSA-renamed variables to ensure semantic preservation.\n- After SSA renaming (as derived in Part 1), at the entry to the $B_3 \\to \\{B_4, B_5\\}$ split, we have variables $x_3$ and $y_3$.\n- Path $B_3 \\to B_4$: In $B_4$, $y$ is redefined as $y_4 := 30$. The existing computation is $u := x_3 + y_4$.\n- Path $B_3 \\to B_5$: In $B_5$, $x$ is redefined as $x_4 := 40$. The available variables are $x_4$ and $y_3$. To make $x+y$ available, we must insert a computation: $\\text{temp} := x_4 + y_3$. This constitutes a new insertion.\n- At $B_6$, we can now eliminate the local computation. We introduce a new variable, say $h$, to hold the value of $x+y$. We add a $\\phi$-function for $h$: $h_{\\text{new}} := \\phi(u, \\text{temp})$. Then, the computation in $B_6$ becomes $v := h_{\\text{new}}$.\n\nThis plan preserves program semantics and makes the computation in $B_6$ fully redundant, allowing its removal. The plan requires inserting exactly one new computation of the form $x+y$. Any attempt to hoist the computation further up (e.g., to $B_3$) would be incorrect due to the redefinitions of $x$ and $y$ in blocks $B_4$ and $B_5$. Therefore, the minimal number of newly inserted explicit evaluations is one.\n$N_{\\mathrm{ins}} = 1$.\n\n#### 3. Total Calculation\n\nWe sum the number of $\\phi$-functions for $x$ and $y$ and the number of newly inserted computations for the PRE plan.\n- $N_{\\phi,x} = 2$\n- $N_{\\phi,y} = 2$\n- $N_{\\mathrm{ins}} = 1$\n\nThe total is given by:\n$$\nT = N_{\\phi,x} + N_{\\phi,y} + N_{\\mathrm{ins}} = 2 + 2 + 1 = 5\n$$", "answer": "$$\\boxed{5}$$", "id": "3660116"}, {"introduction": "The final practice addresses a critical limitation of the basic SSA form: its focus on scalar variables. Real-world programs are rich with memory operations and side effects, which can invalidate optimizations that appear safe in a purely scalar context. This problem challenges you to analyze a scenario where a naive Common Subexpression Elimination (CSE) would be incorrect due to memory side effects, forcing you to consider the advanced techniques like memory SSA and effect summaries that compilers use to ensure correctness. [@problem_id:3660131]", "problem": "Consider a language with a single global integer location $H$ and a function $g(x)$ that both reads and writes $H$. The function $g(x)$ is defined as: it increments $H$ by $1$ and then returns $x + H$. A program fragment is:\n- $H := 0$\n- $t_1 := g(a)$\n- if $(c)$ then $H := 0$ end\n- $t_2 := g(a)$\nAssume the compiler builds Static Single Assignment (SSA) form for all scalar variables (e.g., $a$, $t_1$, $t_2$, $c$), but does not initially place memory into SSA form. An optimizer attempts global Common Subexpression Elimination (CSE) using value numbering that keys an expression by its operator and the value numbers of its operands, treating a call as a black-box operation identified by its callee and argument value numbers only, unless otherwise specified. The optimizer has access to optional effect summaries that, when present, conservatively describe which memory locations a function may read or write. Starting from first principles, use the following facts as the fundamental base:\n- In SSA, each scalar variable is assigned exactly once, and dominance captures that a definition reaches all its uses along every path.\n- CSE is correct when two computations are referentially transparent and have equal inputs in the same abstract state, implying equal results without altering observable behavior.\n- Side effects break referential transparency, and memory is an implicit operand to operations that read from or write to memory.\n- Effect summaries conservatively approximate may-read and may-write sets, enabling optimizations to treat memory as an explicit operand.\nConstruct and analyze the case above to determine which statements are correct regarding the safety of eliminating the second call to $g(a)$ or reusing $t_1$ for $t_2$ in the presence of $g$’s side effects. Select all that apply.\nA. Because $a$ is unchanged and $t_1$ dominates $t_2$, SSA-based value numbering that ignores memory may safely replace $t_2$ by $t_1$ as a valid CSE.\n\nB. With an effect summary that $g$ may read and write $H$, value numbering must include a memory version $M$ for $H$ in the key of $g(a)$; since $M$ differs across the two calls along some paths, the calls cannot be merged by CSE.\n\nC. If profiling shows that the branch with $H := 0$ is rarely taken, the optimizer may ignore it for correctness and still safely replace $t_2$ by $t_1$.\n\nD. Modeling memory in SSA (Memory SSA) or using alias analysis to attach a memory operand representing $H$ to $g$ prevents classifying the two calls as common subexpressions, thereby avoiding an incorrect elimination.\n\nE. It is safe, due to SSA renaming alone, to reorder the first call $g(a)$ above the initialization $H := 0$, because $a$ and $H$ have distinct names in SSA and thus cannot interfere.", "solution": "The validity of the provided problem statement is affirmed. It is scientifically grounded in the principles of compiler optimization, well-posed with a clear and analyzable program fragment, and objective in its terminology. We may proceed with a formal analysis.\n\nThe core of the problem is to determine the safety of applying Common Subexpression Elimination (CSE) to two calls to a function `$g(x)$` which has side effects on a global memory location `$H$`. The function `$g(x)$` is defined to perform two actions: first, it increments `$H$` (`$H := H + 1$`), and second, it returns the value `$x + H$`.\n\nLet us trace the execution of the program fragment and the state of the global variable `$H$`.\n\n1.  `$H := 0$`\n    The integer value at memory location `$H$` is set to `$0$`.\n\n2.  `$t_1 := g(a)$`\n    The function `$g(a)$` is called.\n    - Inside `$g$`, `$H$` is first incremented. Its value changes from `$0$` to `$0 + 1 = 1$`.\n    - Then, `$g$` returns `$a + H$`, which evaluates to `$a + 1$`.\n    - The variable `$t_1$` is assigned the value `$a + 1$`.\n    - After this statement, the global state is that `$H$` holds the value `$1$`.\n\n3.  `if (c) then H := 0 end`\n    This statement introduces a conditional branch, resulting in two possible control flow paths to the next statement.\n    - **Path 1 (if `$c$` is true):** The statement `$H := 0$` is executed. The value of `$H$` is reset to `$0$`.\n    - **Path 2 (if `$c$` is false):** The `then` block is skipped. The value of `$H$` remains `$1$`.\n\n4.  `$t_2 := g(a)$`\n    The function `$g(a)$` is called again. The result depends on which path was taken through the `if` statement.\n    - **On Path 1:** `$H$` is `$0$` before this call.\n        - Inside `$g$`, `$H$` is incremented to `$0 + 1 = 1$`.\n        - `$g$` returns `$a + H$`, which evaluates to `$a + 1$`.\n        - `$t_2$` is assigned the value `$a + 1$`.\n    - **On Path 2:** `$H$` is `$1$` before this call.\n        - Inside `$g$`, `$H$` is incremented to `$1 + 1 = 2$`.\n        - `$g$` returns `$a + H$`, which evaluates to `$a + 2$`.\n        - `$t_2$` is assigned the value `$a + 2$`.\n\nIn summary, the value of `$t_1$` is consistently `$a + 1$`. However, the value of `$t_2$` is `$a + 1$` if `$c$` is true, but `$a + 2$` if `$c$` is false. Since `$t_1 \\ne t_2$` on at least one execution path (the path where `$c$` is false), the expressions `$g(a)$` at the two call sites are not semantically equivalent. Therefore, eliminating the second call and replacing `$t_2$` with `$t_1$` is an unsafe optimization that would alter the program's observable behavior.\n\nBased on this analysis, we evaluate each of the provided statements.\n\n**A. Because `$a$` is unchanged and `$t_1$` dominates `$t_2$`, SSA-based value numbering that ignores memory may safely replace `$t_2$` by `$t_1$` as a valid CSE.**\nThis statement describes the reasoning of a naive optimizer. The variable `$a$` is indeed loop-invariant between the calls, and the definition of `$t_1$` dominates the definition of `$t_2$`. An optimizer that \"ignores memory\" would key both function calls by the callee and the value number of the argument `$a$`, i.e., `$(g, \\text{value\\_number}(a))$`. Since these keys are identical, it would erroneously identify the two calls as a common subexpression. The central flaw in the statement is the claim that this replacement is \"safe\". As our trace proves, it is unsafe because it fails to account for the side effect on `$H$`, which is an implicit input to `$g(a)$`. The principle that side effects break referential transparency is violated.\n**Verdict: Incorrect.**\n\n**B. With an effect summary that `$g$` may read and write `$H$`, value numbering must include a memory version `$M$` for `$H$` in the key of `$g(a)$`; since `$M$` differs across the two calls along some paths, the calls cannot be merged by CSE.**\nThis statement describes a correct, more sophisticated optimization strategy. An effect summary would inform the optimizer that `$g(a)$` reads and writes `$H$`. To apply CSE correctly, the optimizer must treat memory as an explicit operand. This is often modeled using memory versions or tokens.\n- Let `$M_0$` represent the state of memory where `$H=0$` after the initial assignment. The first call to `$g(a)$` depends on `$M_0$`.\n- This call produces a new memory state, `$M_1$`, where `$H=1$`.\n- The `if` statement conditionally produces another state, `$M_2$`, where `$H=0$`.\n- The memory state before the second call, `$M_3$`, is a merge of the states from the two paths (`$M_1$` and `$M_2$`). This merged state is distinct from the initial state `$M_0$`.\nBecause the input memory state for the first call (`$M_0$`) is different from the input memory state for the second call (`$M_3$`), the value numbering keys for the two calls will differ. Consequently, the optimizer will correctly conclude they are not common subexpressions.\n**Verdict: Correct.**\n\n**C. If profiling shows that the branch with `$H := 0$` is rarely taken, the optimizer may ignore it for correctness and still safely replace `$t_2$` by `$t_1$`.**\nThis statement proposes compromising soundness for performance based on profiling data. Compiler correctness requires that transformations preserve program semantics on all possible execution paths, not just the most frequent ones. Even if the path where `$c$` is true is rare, an optimizer claiming correctness must handle it. Furthermore, even on the \"common\" path where `$c$` is false, our analysis shows `$t_1 = a + 1$` while `$t_2 = a + 2$`. Thus, the replacement is incorrect on that path as well. Profiling can guide speculative optimizations (which must have recovery mechanisms) but cannot justify a semantically incorrect transformation under the banner of \"safety\" and \"correctness\".\n**Verdict: Incorrect.**\n\n**D. Modeling memory in SSA (Memory SSA) or using alias analysis to attach a memory operand representing `$H$` to `$g$` prevents classifying the two calls as common subexpressions, thereby avoiding an incorrect elimination.**\nThis statement describes two standard and sound compiler frameworks for handling memory dependencies.\n- **Memory SSA** is a formal extension of SSA to memory. It gives versions to memory states and uses `$\\phi$`-functions at join points. As analyzed for option B, the memory state version before the first call would be different from the `$\\phi$`-version before the second call, thus distinguishing them.\n- **Alias analysis with memory operands** makes the dependency on memory explicit. Alias analysis identifies that `$g$` accesses the global `$H$`. The compiler can then model the call as if it took an additional, invisible argument representing the state of `$H$`, e.g., `$t_1 := g(a, H_{\\text{in}})$`. Since the state of `$H$` is different before the two calls, the calls are not identical and thus not common subexpressions.\nBoth methods correctly prevent the unsafe CSE.\n**Verdict: Correct.**\n\n**E. It is safe, due to SSA renaming alone, to reorder the first call `$g(a)$` above the initialization `$H := 0$`, because `$a$` and `$H$` have distinct names in SSA and thus cannot interfere.**\nThis statement is incorrect on multiple grounds. First, the reordering is unsafe. The original sequence is `$H := 0; t_1 := g(a);$`. The call `$g(a)$` reads the value of `$H$` written by the preceding statement. This constitutes a true dependency (Read-After-Write, or RAW). Reordering statements with a true dependency is illegal as it changes program semantics. Second, the justification is invalid. The problem specifies that memory is *not* in SSA form, so `$H$` does not have an SSA name. Even if it did, SSA renaming is used to break false dependencies (Write-After-Read and Write-After-Write), not true dependencies (Read-After-Write).\n**Verdict: Incorrect.**", "answer": "$$\\boxed{BD}$$", "id": "3660131"}]}