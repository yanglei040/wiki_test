{"hands_on_practices": [{"introduction": "The best way to appreciate the power of quadratic convergence is to see it in action. This first exercise provides a concrete, hands-on opportunity to apply one iteration of Newton's method. By calculating the error before and after the step, you can directly observe the dramatic reduction in error that characterizes this powerful technique [@problem_id:2195730].", "problem": "In numerical analysis, Newton's method is an iterative procedure for finding successively better approximations to the roots (or zeroes) of a real-valued function. For a function $f(x)$ with a simple root $r$ (i.e., $f(r)=0$ and $f'(r) \\neq 0$), the error at iteration $n$, defined as $e_n = |x_n - r|$, often satisfies the relationship $e_{n+1} \\approx C e_n^2$ for some constant $C$, a property known as quadratic convergence. This implies that the ratio $K_n = \\frac{e_{n+1}}{e_n^2}$ should approach a constant value as the number of iterations increases.\n\nConsider the task of finding the real root of the function $f(x) = x^3 - 8$. The exact real root of this function is $r=2$. We will analyze the convergence behavior of Newton's method starting from an initial guess $x_0 = 3$.\n\nCalculate the specific value of the ratio $K_0 = \\frac{e_1}{e_0^2}$, where $e_0$ is the initial error and $e_1$ is the error after the first complete iteration of Newton's method. Report your answer as a decimal rounded to four significant figures.", "solution": "The goal is to calculate the ratio $K_0 = \\frac{e_1}{e_0^2}$. To do this, we need to find the initial error $e_0$ and the error after the first iteration, $e_1$.\n\nFirst, we define the necessary components for Newton's method. The iterative formula is:\n$$x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}$$\nThe given function is $f(x) = x^3 - 8$.\nThe derivative of the function is $f'(x) = 3x^2$.\n\nNext, we calculate the initial error, $e_0$.\nThe problem states the initial guess is $x_0 = 3$ and the true root is $r = 2$.\nThe initial error $e_0$ is defined as the absolute difference between the initial guess and the true root:\n$$e_0 = |x_0 - r| = |3 - 2| = 1$$\n\nNow, we must perform one iteration of Newton's method to find $x_1$. We use the iterative formula with $n=0$:\n$$x_1 = x_0 - \\frac{f(x_0)}{f'(x_0)}$$\nWe evaluate the function and its derivative at the initial guess $x_0 = 3$:\n$$f(x_0) = f(3) = (3)^3 - 8 = 27 - 8 = 19$$\n$$f'(x_0) = f'(3) = 3(3)^2 = 3(9) = 27$$\nSubstituting these values into the formula for $x_1$:\n$$x_1 = 3 - \\frac{19}{27} = \\frac{3 \\times 27}{27} - \\frac{19}{27} = \\frac{81 - 19}{27} = \\frac{62}{27}$$\n\nWith the value of $x_1$, we can now calculate the error after the first iteration, $e_1$.\nThe error $e_1$ is defined as the absolute difference between the first iterate and the true root:\n$$e_1 = |x_1 - r| = \\left|\\frac{62}{27} - 2\\right| = \\left|\\frac{62}{27} - \\frac{2 \\times 27}{27}\\right| = \\left|\\frac{62 - 54}{27}\\right| = \\frac{8}{27}$$\n\nFinally, we calculate the desired ratio $K_0 = \\frac{e_1}{e_0^2}$.\nSubstituting the values we found for $e_0$ and $e_1$:\n$$K_0 = \\frac{e_1}{e_0^2} = \\frac{8/27}{(1)^2} = \\frac{8}{27}$$\n\nThe problem asks for the answer as a decimal rounded to four significant figures. We convert the fraction to a decimal:\n$$K_0 = \\frac{8}{27} \\approx 0.296296296...$$\nRounding this to four significant figures gives:\n$$K_0 \\approx 0.2963$$", "answer": "$$\\boxed{0.2963}$$", "id": "2195730"}, {"introduction": "While Newton's method is remarkably fast, its quadratic convergence is not guaranteed in all situations. This practice explores the crucial theoretical condition that governs the speed of convergence: the multiplicity of the root. By analyzing the behavior of Newton's method for different functions at a root, you will develop a deeper understanding of why the method works so well for simple roots and why its performance degrades for multiple roots [@problem_id:2195658].", "problem": "Newton's method is an iterative procedure used to find successively better approximations to the roots (or zeros) of a real-valued function. The convergence rate of this method is sensitive to the nature of the root. A key performance metric is quadratic convergence, where the number of correct digits roughly doubles with each iteration, provided the initial guess is sufficiently close to the root.\n\nConsider the following three functions, each of which has a root at $x=0$:\n1.  $f_1(x) = x^{3}$\n2.  $f_2(x) = \\cos(x) - 1$\n3.  $f_3(x) = \\exp(x) - 1$\n\nAssuming an initial guess is chosen appropriately close to the root at $x=0$, for which of these functions will Newton's method exhibit quadratic convergence?\n\nA. Function $f_1(x)$ only\n\nB. Function $f_2(x)$ only\n\nC. Function $f_3(x)$ only\n\nD. Functions $f_1(x)$ and $f_2(x)$\n\nE. All three functions", "solution": "Newton’s method for solving $f(x)=0$ is defined by the iteration\n$$\nx_{k+1}=x_{k}-\\frac{f(x_{k})}{f'(x_{k})}.\n$$\nA standard local convergence result states: if $f$ is twice continuously differentiable near a root $\\alpha$ and $f'(\\alpha)\\neq 0$, then for initial $x_{0}$ sufficiently close to $\\alpha$, Newton’s method converges quadratically to $\\alpha$. If instead $f'(\\alpha)=0$ (i.e., the root has multiplicity greater than $1$), then Newton’s method does not have quadratic convergence; for a root of multiplicity $m1$, the convergence is typically linear with asymptotic error reduction factor $(m-1)/m$.\n\nWe analyze each function at the root $\\alpha=0$.\n\n1) For $f_{1}(x)=x^{3}$, we have $f_{1}'(x)=3x^{2}$. The Newton iterate is\n$$\nx_{k+1}=x_{k}-\\frac{x_{k}^{3}}{3x_{k}^{2}}=x_{k}-\\frac{x_{k}}{3}=\\frac{2}{3}x_{k}.\n$$\nLet $e_{k}=x_{k}-0=x_{k}$. Then\n$$\ne_{k+1}=\\frac{2}{3}e_{k},\n$$\nwhich is linear convergence (error is reduced by a constant factor), not quadratic. This aligns with the fact that $f_{1}'(0)=0$ and the root has multiplicity $3$.\n\n2) For $f_{2}(x)=\\cos(x)-1$, we have $f_{2}'(x)=-\\sin(x)$. The Newton iterate is\n$$\nx_{k+1}=x_{k}-\\frac{\\cos(x_{k})-1}{-\\sin(x_{k})}=x_{k}+\\frac{\\cos(x_{k})-1}{\\sin(x_{k})}.\n$$\nUsing trigonometric identities,\n$$\n\\cos(x)-1=-2\\sin^{2}\\!\\left(\\frac{x}{2}\\right),\\quad \\sin(x)=2\\sin\\!\\left(\\frac{x}{2}\\right)\\cos\\!\\left(\\frac{x}{2}\\right),\n$$\nso\n$$\n\\frac{\\cos(x)-1}{\\sin(x)}=-\\tan\\!\\left(\\frac{x}{2}\\right).\n$$\nTherefore,\n$$\nx_{k+1}=x_{k}-\\tan\\!\\left(\\frac{x_{k}}{2}\\right).\n$$\nFor small $x_{k}$, the Taylor expansion $\\tan(y)=y+\\frac{y^{3}}{3}+O(y^{5})$ yields\n$$\n\\tan\\!\\left(\\frac{x_{k}}{2}\\right)=\\frac{x_{k}}{2}+\\frac{x_{k}^{3}}{24}+O(x_{k}^{5}),\n$$\nhence\n$$\nx_{k+1}=x_{k}-\\left(\\frac{x_{k}}{2}+\\frac{x_{k}^{3}}{24}+O(x_{k}^{5})\\right)=\\frac{1}{2}x_{k}-\\frac{1}{24}x_{k}^{3}+O(x_{k}^{5}).\n$$\nWith $e_{k}=x_{k}$, this gives\n$$\ne_{k+1}=\\frac{1}{2}e_{k}+O(e_{k}^{3}),\n$$\nwhich is linear convergence, not quadratic. This is consistent with $f_{2}'(0)=0$ and the root’s effective multiplicity $2$ (since $f_{2}''(0)=-1\\neq 0$).\n\n3) For $f_{3}(x)=\\exp(x)-1$, we have $f_{3}'(x)=\\exp(x)$, so $f_{3}'(0)=1\\neq 0$. The Newton iterate is\n$$\nx_{k+1}=x_{k}-\\frac{\\exp(x_{k})-1}{\\exp(x_{k})}=x_{k}-\\left(1-\\exp(-x_{k})\\right)=x_{k}-1+\\exp(-x_{k}).\n$$\nUsing the Taylor expansion $\\exp(-x)=1-x+\\frac{x^{2}}{2}-\\frac{x^{3}}{6}+O(x^{4})$, we obtain\n$$\nx_{k+1}=x_{k}-1+\\left(1-x_{k}+\\frac{x_{k}^{2}}{2}-\\frac{x_{k}^{3}}{6}+O(x_{k}^{4})\\right)=\\frac{x_{k}^{2}}{2}-\\frac{x_{k}^{3}}{6}+O(x_{k}^{4}).\n$$\nLetting $e_{k}=x_{k}$, we have\n$$\ne_{k+1}=\\frac{1}{2}e_{k}^{2}+O(e_{k}^{3}),\n$$\nwhich demonstrates quadratic convergence. This matches the general criterion since $f_{3}'(0)\\neq 0$ and $f_{3}$ is smooth.\n\nTherefore, among the given functions, only $f_{3}(x)=\\exp(x)-1$ yields quadratic convergence of Newton’s method near $x=0$.", "answer": "$$\\boxed{C}$$", "id": "2195658"}, {"introduction": "What is the secret ingredient behind Newton's method's quadratic convergence? This problem investigates a thoughtful, yet ultimately less efficient, modification of the algorithm where the derivative is held constant. By analyzing the convergence of this simplified method, you can appreciate why the seemingly costly step of re-evaluating the derivative at each iteration is precisely what unlocks the method's celebrated speed [@problem_id:2195669].", "problem": "A student is implementing an algorithm to compute the square root of a positive number $A$. They start with the function $f(x) = x^2 - A$, for which the positive root is $\\alpha = \\sqrt{A}$. To find this root, they decide to use an iterative method. Recalling Newton's method, $x_{n+1} = x_n - f(x_n)/f'(x_n)$, they realize that computing the derivative $f'(x_n) = 2x_n$ at every step might be computationally expensive.\n\nTo optimize their code, they propose a modified method where the derivative is calculated only once at the beginning of the iteration, using their initial guess $x_0$, and then treated as a constant throughout. The resulting iterative scheme is:\n$$x_{n+1} = x_n - \\frac{x_n^2 - A}{f'(x_0)}$$\nwhere $x_0$ is the initial guess, and it is assumed that $x_0  0$ and $x_0 \\neq \\sqrt{A}$.\n\nThis defines a fixed-point iteration $x_{n+1} = g(x_n)$. Assuming the sequence of iterates $x_n$ converges to the root $\\alpha = \\sqrt{A}$, determine the order of convergence, $p$, and the asymptotic error constant, $\\lambda$. Present your answer as a row matrix containing the pair $(p, \\lambda)$.", "solution": "We are given $f(x) = x^{2} - A$ with the positive root $\\alpha = \\sqrt{A}$ and the modified iteration\n$$\nx_{n+1} = x_{n} - \\frac{x_{n}^{2} - A}{f'(x_{0})} = x_{n} - \\frac{x_{n}^{2} - A}{2 x_{0}},\n$$\nwhich defines a fixed-point iteration $x_{n+1} = g(x_{n})$ with\n$$\ng(x) = x - \\frac{x^{2} - A}{2 x_{0}}.\n$$\nFirst, verify that $\\alpha$ is a fixed point:\n$$\ng(\\alpha) = \\alpha - \\frac{\\alpha^{2} - A}{2 x_{0}} = \\alpha.\n$$\nLet $e_{n} = x_{n} - \\alpha$ denote the error. Using the algebraic identity $x_{n}^{2} - \\alpha^{2} = (x_{n} - \\alpha)(x_{n} + \\alpha)$, we obtain an exact error recursion:\n$$\ne_{n+1} = x_{n+1} - \\alpha = x_{n} - \\frac{x_{n}^{2} - A}{2 x_{0}} - \\alpha = e_{n} - \\frac{(x_{n} - \\alpha)(x_{n} + \\alpha)}{2 x_{0}} = e_{n}\\left(1 - \\frac{x_{n} + \\alpha}{2 x_{0}}\\right).\n$$\nSince $x_{n} \\to \\alpha$ by assumption, we write $x_{n} = \\alpha + e_{n}$ and substitute:\n$$\ne_{n+1} = e_{n}\\left(1 - \\frac{2\\alpha + e_{n}}{2 x_{0}}\\right) = \\left(1 - \\frac{\\alpha}{x_{0}}\\right)e_{n} - \\frac{1}{2 x_{0}} e_{n}^{2}.\n$$\nAs $n \\to \\infty$, the linear term dominates because $x_{0} \\neq \\alpha$ implies $1 - \\alpha/x_{0} \\neq 0$. Therefore, the convergence is linear with order $p = 1$, and the asymptotic error constant is\n$$\n\\lambda = \\lim_{n \\to \\infty} \\frac{|e_{n+1}|}{|e_{n}|} = \\left|1 - \\frac{\\alpha}{x_{0}}\\right| = \\left|1 - \\frac{\\sqrt{A}}{x_{0}}\\right|.\n$$\nEquivalently, this conclusion follows from the fixed-point derivative $g'(x) = 1 - x/x_{0}$, so $g'(\\alpha) = 1 - \\alpha/x_{0} \\neq 0$, which yields linear convergence with $\\lambda = |g'(\\alpha)|$.", "answer": "$$\\boxed{\\begin{pmatrix} 1  \\left|1 - \\frac{\\sqrt{A}}{x_{0}}\\right| \\end{pmatrix}}$$", "id": "2195669"}]}