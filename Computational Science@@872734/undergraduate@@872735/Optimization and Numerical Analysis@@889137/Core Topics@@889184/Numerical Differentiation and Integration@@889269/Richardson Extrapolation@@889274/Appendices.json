{"hands_on_practices": [{"introduction": "This first exercise provides a direct application of Richardson extrapolation in a practical engineering context. You will use two numerical estimates from a first-order method, each with a different step size, to compute a single, more accurate result. This practice will help you master the core formula for eliminating the leading error term and see firsthand how extrapolation can improve a solution's accuracy [@problem_id:2197905].", "problem": "A team of aerospace engineers is simulating the atmospheric entry of a new space probe. They are particularly interested in estimating the maximum deceleration experienced by the probe, a critical parameter for structural design. The numerical algorithm they use to solve the equations of motion has a first-order global truncation error, meaning the error in the estimate is of order $O(h)$, where $h$ is the time step size used in the simulation.\n\nLet $D(h)$ be the estimated maximum deceleration when the simulation is run with a time step of $h$.\nThe team performs two simulations:\n1.  With a time step of $h_1 = 0.4$ seconds, they find the maximum deceleration to be $D(0.4) = 58.6$ m/s$^2$.\n2.  With a reduced time step of $h_2 = 0.2$ seconds, they find the maximum deceleration to be $D(0.2) = 59.3$ m/s$^2$.\n\nTo obtain a more accurate result without performing another lengthy simulation with an even smaller time step, the engineers decide to apply Richardson extrapolation to these two estimates. Calculate the improved estimate for the maximum deceleration. Express your answer in $m/s^2$, rounded to three significant figures.", "solution": "Because the numerical method has first-order global truncation error, the estimated quantity satisfies the asymptotic error model\n$$\nD(h) = D^{\\ast} + C h + O(h^{2}),\n$$\nwhere $D^{\\ast}$ is the true maximum deceleration and $C$ is an $h$-independent constant.\n\nLet $h_{1} = 0.4$, $h_{2} = 0.2$, and define the refinement ratio $r = \\frac{h_{1}}{h_{2}} = 2$. Then\n$$\nD(h_{1}) = D^{\\ast} + C h_{1} + O(h_{1}^{2}), \\quad D(h_{2}) = D^{\\ast} + C \\frac{h_{1}}{r} + O\\!\\left(\\frac{h_{1}^{2}}{r^{2}}\\right).\n$$\nEliminating the $O(h)$ term via Richardson extrapolation gives\n$$\nD_{\\text{RE}} = \\frac{r D(h_{2}) - D(h_{1})}{r - 1} = D(h_{2}) + \\frac{D(h_{2}) - D(h_{1})}{r - 1} = D^{\\ast} + O(h_{1}^{2}).\n$$\nSubstituting $r=2$, $D(0.4)=58.6$, and $D(0.2)=59.3$,\n$$\nD_{\\text{RE}} = \\frac{2 \\cdot 59.3 - 58.6}{2 - 1} = 118.6 - 58.6 = 60.0.\n$$\nRounded to three significant figures, the improved estimate is $60.0$ in $m/s^2$.", "answer": "$$\\boxed{60.0}$$", "id": "2197905"}, {"introduction": "Moving beyond direct calculation, this problem explores the theoretical power of Richardson extrapolation. By comparing two different numerical methods, you will investigate how the structure of a method's truncation error impacts the degree of accuracy improvement gained from extrapolation. This thought experiment reveals why methods with symmetric error terms, which contain only even powers of the step size $h$, benefit exceptionally from this technique [@problem_id:2197912].", "problem": "Two engineers, Anya and Ben, are tasked with numerically estimating the value of a physical constant, denoted by the exact value $L$. They use different numerical methods.\n\nAnya uses Method A, which produces an approximation $A(h)$ for a given step size $h$. The error of this method is known to have the form of a power series in $h$:\n$A(h) = L + c_1 h + c_2 h^2 + c_3 h^3 + O(h^4)$\nwhere $c_1, c_2, c_3$ are non-zero constants independent of $h$. The order of accuracy of this method is the lowest power of $h$ in the error term $A(h) - L$.\n\nBen uses Method B, which produces an approximation $B(h)$. This method is more sophisticated, and its error series contains only even powers of $h$:\n$B(h) = L + d_2 h^2 + d_4 h^4 + d_6 h^6 + O(h^8)$\nwhere $d_2, d_4, d_6$ are non-zero constants independent of $h$.\n\nTo improve their initial results, both engineers decide to apply a single step of Richardson extrapolation. They each compute two approximations: one with a step size $h$ and another with a step size $h/2$. They then combine these two approximations to create a new, more accurate estimate.\n\nLet the original order of accuracy for Method A be $p_{A, \\text{orig}}$ and for Method B be $p_{B, \\text{orig}}$. After applying Richardson extrapolation, their new estimates have orders of accuracy $p_{A, \\text{new}}$ and $p_{B, \\text{new}}$, respectively. The \"improvement\" in the order of accuracy is defined as the difference between the new and original orders, i.e., $\\Delta p_A = p_{A, \\text{new}} - p_{A, \\text{orig}}$ for Anya and $\\Delta p_B = p_{B, \\text{new}} - p_{B, \\text{orig}}$ for Ben.\n\nWhich of the following statements correctly compares the improvements $\\Delta p_A$ and $\\Delta p_B$?\n\nA. $\\Delta p_A > \\Delta p_B$\n\nB. $\\Delta p_A = \\Delta p_B$\n\nC. $\\Delta p_A < \\Delta p_B$\n\nD. The comparison is impossible without knowing the values of $h$, $c_i$, and $d_i$.\n\nE. Both methods fail to improve, so $\\Delta p_A = \\Delta p_B = 0$.", "solution": "The order of accuracy of a method with error expansion $S(h)=L+\\sum_{k} a_{k} h^{p_{k}}$ is the smallest exponent $p_{k}$ with nonzero coefficient. A single Richardson extrapolation step for refinement ratio $r=2$ and leading order $p$ uses\n$$\nR(h)=\\frac{2^{p} S\\!\\left(\\frac{h}{2}\\right)-S(h)}{2^{p}-1},\n$$\nwhich cancels the $h^{p}$ term and promotes the order to the next nonvanishing power in the error series.\n\nFor Method A,\n$$\nA(h)=L+c_{1} h+c_{2} h^{2}+c_{3} h^{3}+O(h^{4}),\n$$\nso $p_{A,\\text{orig}}=1$. Expand at $h/2$:\n$$\nA\\!\\left(\\frac{h}{2}\\right)=L+\\frac{c_{1}}{2} h+\\frac{c_{2}}{4} h^{2}+\\frac{c_{3}}{8} h^{3}+O(h^{4}).\n$$\nApply Richardson with $p=1$:\n$$\n2 A\\!\\left(\\frac{h}{2}\\right)-A(h)=\\left(2L+c_{1} h+\\frac{c_{2}}{2} h^{2}+\\frac{c_{3}}{4} h^{3}+\\cdots\\right)-\\left(L+c_{1} h+c_{2} h^{2}+c_{3} h^{3}+\\cdots\\right)\n$$\n$$\n= L-\\frac{c_{2}}{2} h^{2}-\\frac{3 c_{3}}{4} h^{3}+O(h^{4}).\n$$\nThus\n$$\nR_{A}(h)=\\frac{2 A\\!\\left(\\frac{h}{2}\\right)-A(h)}{1}=L-\\frac{c_{2}}{2} h^{2}-\\frac{3 c_{3}}{4} h^{3}+O(h^{4}),\n$$\nso the new leading error is $O(h^{2})$, i.e., $p_{A,\\text{new}}=2$ and\n$$\n\\Delta p_{A}=p_{A,\\text{new}}-p_{A,\\text{orig}}=2-1=1.\n$$\n\nFor Method B,\n$$\nB(h)=L+d_{2} h^{2}+d_{4} h^{4}+d_{6} h^{6}+O(h^{8}),\n$$\nso $p_{B,\\text{orig}}=2$. Expand at $h/2$:\n$$\nB\\!\\left(\\frac{h}{2}\\right)=L+\\frac{d_{2}}{4} h^{2}+\\frac{d_{4}}{16} h^{4}+\\frac{d_{6}}{64} h^{6}+O(h^{8}).\n$$\nApply Richardson with $p=2$:\n$$\n4 B\\!\\left(\\frac{h}{2}\\right)-B(h)=\\left(4L+d_{2} h^{2}+\\frac{d_{4}}{4} h^{4}+\\frac{d_{6}}{16} h^{6}+\\cdots\\right)-\\left(L+d_{2} h^{2}+d_{4} h^{4}+d_{6} h^{6}+\\cdots\\right)\n$$\n$$\n=3L-\\frac{3}{4} d_{4} h^{4}-\\frac{15}{16} d_{6} h^{6}+O(h^{8}).\n$$\nDivide by $4-1=3$:\n$$\nR_{B}(h)=L-\\frac{1}{4} d_{4} h^{4}-\\frac{5}{16} d_{6} h^{6}+O(h^{8}),\n$$\nso the new leading error is $O(h^{4})$, i.e., $p_{B,\\text{new}}=4$ and\n$$\n\\Delta p_{B}=p_{B,\\text{new}}-p_{B,\\text{orig}}=4-2=2.\n$$\n\nTherefore, $\\Delta p_{A}=1$ and $\\Delta p_{B}=2$, giving $\\Delta p_{A}<\\Delta p_{B}$. The correct choice is C.", "answer": "$$\\boxed{C}$$", "id": "2197912"}, {"introduction": "This final practice bridges theory and implementation by tasking you with building and analyzing Richardson extrapolation algorithms. You will apply the technique to the central difference formula for differentiation, a method known for its efficient $O(h^2)$ error structure, and compare the computational cost and numerical stability of different implementations. This advanced exercise highlights the practical trade-offs in algorithm design and the limitations imposed by floating-point arithmetic when striving for high precision [@problem_id:2435030].", "problem": "Implement two mathematically equivalent estimators for the first derivative based on Richardson extrapolation for a real-valued function. The objective is to compare their computational cost and numerical stability when applied to a small test suite. Let a differentiable function be denoted by $f(x)$, and let the derivative at a point $x=x_{0}$ be $f^{\\prime}(x_{0})$. Consider the symmetric finite-difference base approximation defined for step size $h>0$ by\n$$\nD(h) \\equiv \\frac{f(x_{0}+h)-f(x_{0}-h)}{2h},\n$$\nwhich has a truncation error that begins at order $h^{2}$ for functions $f$ that are sufficiently smooth. Implement the following two estimators that use Richardson extrapolation with refinement ratio $r$ to improve $D(h)$:\n- A recursive Richardson extrapolation estimator that computes the extrapolated values by calling itself on smaller subproblems without caching intermediate results.\n- An iterative tableau-based Richardson extrapolation estimator that builds the triangular tableau of extrapolated values bottom-up.\n\nFor both estimators, use the same refinement ratio $r=2$, and interpret angles in radians. For every test case below, both estimators must produce the sequence of extrapolated estimates corresponding to successive extrapolation levels $k=0,1,\\dots,L$, where $k=0$ denotes the base approximation and $k=L$ denotes the highest extrapolation level. For each method and test case, report:\n- the final extrapolated estimate at level $k=L$,\n- the absolute error of this final estimate with respect to the exact derivative,\n- the total number of calls to the function $f(x)$ needed to generate the entire sequence of estimates up to level $k=L$,\n- a boolean indicating whether the absolute error across levels $k=0,1,\\dots,L$ is monotonically nonincreasing.\n\nYour program must implement the above for the following test suite, where every test specifies the function $f(x)$, the evaluation point $x_{0}$, the initial step $h_{0}$, and the number of extrapolation levels $L$:\n- Test A (happy path, smooth, moderate step): $f(x)=e^{x}$, $x_{0}=1.0$, $h_{0}=0.1$, $L=4$, $r=2$. The exact derivative is $f^{\\prime}(x)=e^{x}$ evaluated at $x=1.0$.\n- Test B (smooth trigonometric, moderate step): $f(x)=\\sin(x)$, $x_{0}=1.3$, $h_{0}=0.2$, $L=5$, $r=2$. The exact derivative is $f^{\\prime}(x)=\\cos(x)$ evaluated at $x=1.3$.\n- Test C (oscillatory, higher local frequency): $f(x)=\\sin(50\\,x)$, $x_{0}=0.2$, $h_{0}=0.05$, $L=4$, $r=2$. The exact derivative is $f^{\\prime}(x)=50\\cos(50\\,x)$ evaluated at $x=0.2$.\n- Test D (round-off sensitive, very small step): $f(x)=e^{x}$, $x_{0}=1.0$, $h_{0}=10^{-8}$, $L=3$, $r=2$. The exact derivative is $f^{\\prime}(x)=e^{x}$ evaluated at $x=1.0$.\n\nFor each test case, aggregate the results into a list with the following eight entries in order:\n$[$recursive final estimate at level $L$, iterative final estimate at level $L$, absolute error of recursive final estimate, absolute error of iterative final estimate, total function evaluations used by the recursive method to produce all $L+1$ levels, total function evaluations used by the iterative method to produce all $L+1$ levels, recursive errors monotone nonincreasing (boolean), iterative errors monotone nonincreasing (boolean)$]$.\n\nYour program should produce a single line of output containing the results for all test cases as a comma-separated list of these per-test lists enclosed in square brackets (for example, $[ [\\dots],[\\dots],\\dots ]$). All quantities are unitless, and angles must be interpreted in radians.", "solution": "The core of the problem lies in approximating the first derivative $f'(x_0)$ of a function $f(x)$ at a point $x_0$. The suggested base approximation is the symmetric finite-difference formula:\n$$\nD(h) = \\frac{f(x_{0}+h)-f(x_{0}-h)}{2h}\n$$\nFor a sufficiently smooth function $f(x)$, a Taylor series expansion around $x_0$ reveals that the error of this approximation has a specific structure:\n$$\nD(h) = f'(x_0) + c_1 h^2 + c_2 h^4 + c_3 h^6 + \\dots\n$$\nThe error is an infinite series in even powers of the step size $h$. Richardson extrapolation is a technique that systematically eliminates these error terms to produce a sequence of increasingly accurate estimates.\n\nGiven a base approximation $A(h)$ with an error series in powers of $h^2$, we can combine approximations computed with different step sizes to cancel the leading error term. Using a step size $h$ and a smaller step size $h/r$ (where $r$ is the refinement ratio, given as $r=2$), we have:\n$$\nA(h) \\approx I + c_1 h^2 \\\\\nA(h/r) \\approx I + c_1 (h/r)^2\n$$\nSolving for $I$ yields a more accurate estimate $A^{(1)}(h)$:\n$$\nA^{(1)}(h) = \\frac{r^2 A(h/r) - A(h)}{r^2 - 1}\n$$\nThis new estimate has an error that starts with a term proportional to $h^4$. This process can be repeated. After $k$ levels of extrapolation, the error term $c_k h^{2k}$ is eliminated. This can be computed using a triangular tableau, where $r=2$:\nLet $T_{i,0} = D(h_0 / 2^i)$. The tableau entries are then computed as:\n$$\nT_{i,k} = T_{i, k-1} + \\frac{T_{i, k-1} - T_{i-1, k-1}}{4^k - 1} \\quad \\text{for } k \\geq 1\n$$\nThe sequence of estimates with increasing accuracy, corresponding to extrapolation levels $k=0, 1, \\dots, L$, is given by the diagonal of this tableau: $\\{T_{0,0}, T_{1,1}, \\dots, T_{L,L}\\}$.\n\nThe problem asks for the implementation of two methods to generate this sequence:\n\n1.  **Iterative Tableau-Based Estimator**: This method directly implements the bottom-up construction of the Richardson tableau. First, it computes the initial column of approximations, $T_{i,0} = D(h_0/2^i)$, for $i=0, \\dots, L$. This requires $L+1$ evaluations of the $D(h)$ formula. Then, it iteratively computes subsequent columns of the tableau until the desired diagonal elements $T_{k,k}$ are found. This approach is computationally efficient because each intermediate value $T_{i,k}$ is computed only once and stored for future use. The total number of evaluations of the function $f(x)$ is $2(L+1)$.\n\n2.  **Recursive Estimator**: This method computes each diagonal element $T_{k,k}$ via a recursive function that embodies the extrapolation formula, without any memoization or caching of intermediate results as stipulated. A call to compute $T_{k,k}$ will recursively call itself to compute $T_{k, k-1}$ and $T_{k-1, k-1}$. Since there is no caching, shared subproblems are recomputed. The number of calls to the base function $D(h)$ to compute $T_{k,k}$ is $2^k$. To generate the full sequence $\\{T_{0,0}, T_{1,1}, \\dots, T_{L,L}\\}$, we sum the costs for each element, leading to a total of $\\sum_{k=0}^{L} 2^k = 2^{L+1}-1$ calls to $D(h)$, and thus $2(2^{L+1}-1)$ evaluations of $f(x)$.\n\nAlthough mathematically equivalent (and thus expected to yield identical numerical results), the two methods have vastly different computational costs. The recursive method is exponential in $L$, while the iterative method is linear in $L$. This comparison highlights the importance of algorithmic design, particularly the principle of dynamic programming (or memoization), which the iterative method implicitly uses.\n\nThe test suite includes a case (Test D) with a very small initial step size $h_0 = 10^{-8}$. For such small $h$, the numerical calculation of $D(h)$ is dominated by subtractive cancellation and floating-point round-off error, which typically scales as $\\epsilon/h$ (where $\\epsilon$ is machine precision). Since Richardson extrapolation involves taking differences of approximate values, it tends to amplify round-off error. We expect this to manifest as a non-monotonic decrease in the absolute error for this test case.\n\nThe implementation will proceed by creating both estimators, applying them to the test suite, and collecting the specified metrics: final estimate, absolute error, total function calls, and error monotonicity.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares recursive and iterative Richardson extrapolation\n    for numerical differentiation on a given test suite.\n    \"\"\"\n\n    test_cases = [\n        # Test A: happy path, smooth, moderate step\n        {\n            \"f\": lambda x: np.exp(x),\n            \"f_prime\": lambda x: np.exp(x),\n            \"x0\": 1.0, \"h0\": 0.1, \"L\": 4,\n        },\n        # Test B: smooth trigonometric, moderate step\n        {\n            \"f\": lambda x: np.sin(x),\n            \"f_prime\": lambda x: np.cos(x),\n            \"x0\": 1.3, \"h0\": 0.2, \"L\": 5,\n        },\n        # Test C: oscillatory, higher local frequency\n        {\n            \"f\": lambda x: np.sin(50 * x),\n            \"f_prime\": lambda x: 50 * np.cos(50 * x),\n            \"x0\": 0.2, \"h0\": 0.05, \"L\": 4,\n        },\n        # Test D: round-off sensitive, very small step\n        {\n            \"f\": lambda x: np.exp(x),\n            \"f_prime\": lambda x: np.exp(x),\n            \"x0\": 1.0, \"h0\": 1e-8, \"L\": 3,\n        }\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        f, f_prime, x0, h0, L = case[\"f\"], case[\"f_prime\"], case[\"x0\"], case[\"h0\"], case[\"L\"]\n        r = 2.0  # Refinement ratio\n\n        # --- Recursive Estimator ---\n        class FuncCounter:\n            def __init__(self, func):\n                self.func = func\n                self.count = 0\n            def __call__(self, x):\n                self.count += 1\n                return self.func(x)\n            def reset(self):\n                self.count = 0\n\n        f_wrapped_rec = FuncCounter(f)\n\n        def recursive_T(i, k):\n            if k == 0:\n                h = h0 / (r**i)\n                return (f_wrapped_rec(x0 + h) - f_wrapped_rec(x0 - h)) / (2.0 * h)\n            \n            term1 = recursive_T(i, k - 1)\n            term2 = recursive_T(i - 1, k - 1)\n            \n            # Using r=2, the power for eliminating error h^(2k) is 4^k\n            return term1 + (term1 - term2) / (np.power(r, 2*k) - 1.0)\n\n        f_wrapped_rec.reset()\n        estimates_rec = [recursive_T(k, k) for k in range(L + 1)]\n        total_f_calls_rec = f_wrapped_rec.count\n\n        # --- Iterative Tableau-Based Estimator ---\n        f_wrapped_iter = FuncCounter(f)\n\n        def D(h_val):\n            return (f_wrapped_iter(x0 + h_val) - f_wrapped_iter(x0 - h_val)) / (2.0 * h_val)\n        \n        T = np.zeros((L + 1, L + 1))\n        \n        f_wrapped_iter.reset()\n        for i in range(L + 1):\n            h_i = h0 / (r**i)\n            T[i, 0] = D(h_i)\n        \n        total_f_calls_iter = f_wrapped_iter.count\n        \n        for k in range(1, L + 1):\n            for i in range(k, L + 1):\n                T[i, k] = T[i, k - 1] + (T[i, k - 1] - T[i - 1, k - 1]) / (np.power(r, 2*k) - 1.0)\n        \n        estimates_iter = T.diagonal().tolist()\n\n        # --- Collect and report results ---\n        exact_val = f_prime(x0)\n\n        final_est_rec = estimates_rec[L]\n        final_est_iter = estimates_iter[L]\n\n        abs_err_rec = np.abs(final_est_rec - exact_val)\n        abs_err_iter = np.abs(final_est_iter - exact_val)\n\n        errors_rec = [np.abs(est - exact_val) for est in estimates_rec]\n        errors_iter = [np.abs(est - exact_val) for est in estimates_iter]\n        \n        # Check if absolute error is monotonically nonincreasing\n        is_mono_rec = all(errors_rec[i] = errors_rec[i-1] + 1e-18 for i in range(1, L + 1))\n        is_mono_iter = all(errors_iter[i] = errors_iter[i-1] + 1e-18 for i in range(1, L + 1))\n\n        case_results = [\n            final_est_rec,\n            final_est_iter,\n            abs_err_rec,\n            abs_err_iter,\n            total_f_calls_rec,\n            total_f_calls_iter,\n            is_mono_rec,\n            is_mono_iter\n        ]\n        all_results.append(case_results)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\n# Due to a potential ambiguity in the problem's Python execution environment,\n# the check for monotonicity includes a small tolerance to account for\n# floating point inaccuracies where mathematically identical operations\n# might yield slightly different results, potentially breaking strict\n# monotonicity. The recursive and iterative methods should be identical,\n# but their error monotonicity is checked independently as requested.\n# The `solve` function is wrapped to allow for this explanatory comment.\nsolve()\n```", "id": "2435030"}]}