{"hands_on_practices": [{"introduction": "The conditioning of the normal equations matrix $A^T A$ is fundamentally linked to the linear independence of the columns of matrix $A$. This exercise provides a direct, geometric illustration of this principle. By considering the columns of $A$ as two vectors separated by a small angle $\\theta$, you can explore how their near-collinearity impacts the numerical stability of the system [@problem_id:2162074]. Analyzing the condition number as this angle approaches zero builds a foundational intuition for why nearly parallel vectors are problematic in numerical computations.", "problem": "In numerical linear algebra, the conditioning of a matrix is a critical measure of its sensitivity to errors in input data. For a linear least-squares problem $A\\mathbf{x} \\approx \\mathbf{b}$, the sensitivity is governed by the condition number of the normal equations matrix, $A^T A$. Poor conditioning often arises when the columns of the matrix $A$ are nearly linearly dependent.\n\nConsider a simple $2 \\times 2$ matrix $A$ whose columns represent two basis vectors in a plane. The first column is the vector $\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ and the second column is the vector $\\begin{pmatrix} \\cos\\theta \\\\ \\sin\\theta \\end{pmatrix}$, where $\\theta$, in radians, is the angle between the two vectors. Assume $\\theta$ is a small positive angle. The matrix $A$ is thus given by $A = \\begin{pmatrix} 1  \\cos\\theta \\\\ 0  \\sin\\theta \\end{pmatrix}$.\n\nDetermine the leading-order asymptotic expression for the 2-norm condition number of the associated normal equations matrix, $A^T A$, in the limit as $\\theta \\to 0^+$. Your answer should be a simple expression in terms of $\\theta$ that captures the dominant behavior for small angles.", "solution": "We are asked for the 2-norm condition number of the normal equations matrix $A^T A$ for $A=\\begin{pmatrix} 1  \\cos\\theta \\\\ 0  \\sin\\theta \\end{pmatrix}$, in the limit $\\theta \\to 0^{+}$. For a symmetric positive definite matrix $M$, the 2-norm condition number is $\\kappa_{2}(M)=\\frac{\\lambda_{\\max}(M)}{\\lambda_{\\min}(M)}$, where $\\lambda_{\\max}$ and $\\lambda_{\\min}$ are the largest and smallest eigenvalues of $M$.\n\nCompute $A^T A$:\n$$\nA^T=\\begin{pmatrix} 1  0 \\\\ \\cos\\theta  \\sin\\theta \\end{pmatrix}, \\quad\nA^T A=\\begin{pmatrix} 1  \\cos\\theta \\\\ \\cos\\theta  1 \\end{pmatrix}.\n$$\nThe matrix $\\begin{pmatrix} 1  \\cos\\theta \\\\ \\cos\\theta  1 \\end{pmatrix}$ has eigenvalues\n$$\n\\lambda_{\\pm}=1 \\pm \\cos\\theta.\n$$\nTherefore,\n$$\n\\kappa_{2}(A^T A)=\\frac{\\lambda_{\\max}}{\\lambda_{\\min}}=\\frac{1+\\cos\\theta}{1-\\cos\\theta}.\n$$\nFor small $\\theta$, use the Taylor expansion $\\cos\\theta=1-\\frac{\\theta^{2}}{2}+O(\\theta^{4})$. Hence\n$$\n1-\\cos\\theta=\\frac{\\theta^{2}}{2}+O(\\theta^{4}), \\quad 1+\\cos\\theta=2-\\frac{\\theta^{2}}{2}+O(\\theta^{4}).\n$$\nThus\n$$\n\\kappa_{2}(A^T A)=\\frac{2-\\frac{\\theta^{2}}{2}+O(\\theta^{4})}{\\frac{\\theta^{2}}{2}+O(\\theta^{4})}\n=\\frac{4}{\\theta^{2}}+O(1),\n$$\nso the leading-order asymptotic behavior as $\\theta \\to 0^{+}$ is\n$$\n\\kappa_{2}(A^T A)\\sim \\frac{4}{\\theta^{2}}.\n$$", "answer": "$$\\boxed{\\frac{4}{\\theta^{2}}}$$", "id": "2162074"}, {"introduction": "Building on the geometric intuition of near-collinearity, this practice demonstrates how ill-conditioning arises in the common application of polynomial regression. When fitting a model to data points that are clustered closely together, the columns of the resulting design matrix (a Vandermonde matrix) can become nearly linearly dependent. This exercise asks you to quantify this effect by calculating how the condition number of the normal equations matrix behaves as two data points become infinitesimally close [@problem_id:2162115], reinforcing the crucial link between data point distribution and numerical stability.", "problem": "Consider a linear least squares problem for fitting a model $y(x) = c_1 + c_2 x$ to two data points. The x-coordinates of these data points are given by $x_1 = 1$ and $x_2 = 1 + \\epsilon$, where $\\epsilon$ is a small, positive real number. The design matrix $A$ for this problem has elements $A_{ij} = x_i^{j-1}$ for $i,j \\in \\{1,2\\}$. The stability of solving the associated normal equations, $A^T A \\mathbf{c} = A^T \\mathbf{y}$, is related to the condition number of the matrix $B(\\epsilon) = A^T A(\\epsilon)$.\n\nThe 2-norm condition number of the symmetric positive-definite matrix $B(\\epsilon)$ is defined as the ratio of its largest to smallest eigenvalue, $\\kappa_2(B(\\epsilon)) = \\frac{\\lambda_{\\max}(B(\\epsilon))}{\\lambda_{\\min}(B(\\epsilon))}$. For small $\\epsilon$, this condition number can be approximated by an asymptotic formula of the form:\n$$ \\kappa_2(B(\\epsilon)) \\approx \\frac{C}{\\epsilon^n} $$\nwhere $C$ is a positive constant and $n$ is a positive integer.\n\nDetermine the values of the constant $C$ and the integer $n$. Present your answer as a 1-by-2 row matrix with elements $C$ and $n$ in that order.", "solution": "We form the Vandermonde-type design matrix for two points with $x_{1}=1$ and $x_{2}=1+\\epsilon$:\n$$\nA=\\begin{pmatrix}\n1  x_{1} \\\\\n1  x_{2}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1  1 \\\\\n1  1+\\epsilon\n\\end{pmatrix}.\n$$\nThe normal matrix is $B(\\epsilon)=A^{T}A$, which is symmetric positive-definite for $\\epsilon0$ (since the two rows of $A$ are linearly independent when $x_{1}\\neq x_{2}$). Compute $B(\\epsilon)$ explicitly:\n$$\nB(\\epsilon)=A^{T}A=\n\\begin{pmatrix}\n\\sum_{i=1}^{2}1^{2}  \\sum_{i=1}^{2}1\\cdot x_{i} \\\\\n\\sum_{i=1}^{2}x_{i}\\cdot 1  \\sum_{i=1}^{2}x_{i}^{2}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n2  2+\\epsilon \\\\\n2+\\epsilon  2+2\\epsilon+\\epsilon^{2}\n\\end{pmatrix}.\n$$\nFor a $2\\times 2$ symmetric matrix $\\begin{pmatrix}a  b \\\\ b  c\\end{pmatrix}$, the eigenvalues satisfy $\\lambda_{\\max}+\\lambda_{\\min}=\\operatorname{tr}=a+c$ and $\\lambda_{\\max}\\lambda_{\\min}=\\det=ac-b^{2}$. Here,\n$$\na=2,\\quad b=2+\\epsilon,\\quad c=2+2\\epsilon+\\epsilon^{2}.\n$$\nHence the determinant is\n$$\n\\det B(\\epsilon)=ac-b^{2}=2(2+2\\epsilon+\\epsilon^{2})-(2+\\epsilon)^{2}=(4+4\\epsilon+2\\epsilon^{2})-(4+4\\epsilon+\\epsilon^{2})=\\epsilon^{2}.\n$$\nAt $\\epsilon=0$, we have\n$$\nB(0)=\\begin{pmatrix}2  2 \\\\ 2  2\\end{pmatrix},\n$$\nwhose eigenvalues are $4$ and $0$. Therefore, for small $\\epsilon0$, by continuity,\n$$\n\\lambda_{\\max}(B(\\epsilon))=4+O(\\epsilon).\n$$\nUsing the identity $\\lambda_{\\min}=\\det/\\lambda_{\\max}$,\n$$\n\\lambda_{\\min}(B(\\epsilon))=\\frac{\\det B(\\epsilon)}{\\lambda_{\\max}(B(\\epsilon))}=\\frac{\\epsilon^{2}}{4+O(\\epsilon)}=\\frac{1}{4}\\epsilon^{2}+O(\\epsilon^{3}).\n$$\nTherefore, the $2$-norm condition number is\n$$\n\\kappa_{2}(B(\\epsilon))=\\frac{\\lambda_{\\max}(B(\\epsilon))}{\\lambda_{\\min}(B(\\epsilon))}=\\frac{4+O(\\epsilon)}{\\frac{1}{4}\\epsilon^{2}+O(\\epsilon^{3})}=\\frac{16}{\\epsilon^{2}}+O\\!\\left(\\frac{1}{\\epsilon}\\right).\n$$\nThus the asymptotic form $\\kappa_{2}(B(\\epsilon))\\approx \\frac{C}{\\epsilon^{n}}$ has\n$$\nC=16,\\quad n=2.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}16  2\\end{pmatrix}}$$", "id": "2162115"}, {"introduction": "Ill-conditioning is not only caused by geometrically close vectors but can also result from poor data scaling, a frequent pitfall in real-world data analysis. This problem presents a practical scenario of fitting a linear model to time-series data where the time values are large but not necessarily close together. By computing the condition number for this realistic setup, you will see how a seemingly benign choice of data representation leads to a dramatically ill-conditioned system [@problem_id:2218032]. This highlights the critical importance of data preprocessing steps, such as centering and scaling, to ensure numerical stability.", "problem": "In scientific computing, the normal equations method is a common approach to solving linear least squares problems of the form $A\\mathbf{x} \\approx \\mathbf{b}$. This method transforms the problem into solving a square linear system $(A^T A)\\mathbf{x} = A^T\\mathbf{b}$. However, this approach can suffer from numerical instability if the matrix $M = A^T A$ is ill-conditioned. The ill-conditioning is often exacerbated when the columns of the matrix $A$ are nearly linearly dependent.\n\nConsider an experiment where a physical quantity $y$ is modeled as a linear function of time $t$, such that $y(t) = c_0 + c_1 t$. To determine the coefficients $c_0$ and $c_1$, three measurements of $y$ are taken at times $t_1 = 100.0$, $t_2 = 101.0$, and $t_3 = 102.0$. The corresponding least squares matrix $A$ for this problem is given by:\n$$A = \\begin{pmatrix} 1  t_1 \\\\ 1  t_2 \\\\ 1  t_3 \\end{pmatrix}$$\nThe numerical stability of solving the normal equations is related to the condition number of the matrix $M = A^T A$. For a symmetric positive definite matrix such as $M$, the spectral condition number, denoted $\\kappa_2(M)$, is defined as the ratio of its largest eigenvalue to its smallest eigenvalue:\n$$\\kappa_2(M) = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}}$$\nA large condition number indicates that the matrix is ill-conditioned.\n\nYour task is to compute the spectral condition number $\\kappa_2(A^T A)$ for the matrix $A$ defined by the measurement times above. Round your final answer to four significant figures.", "solution": "We are given $A=\\begin{pmatrix}1  t_{1}\\\\ 1  t_{2}\\\\ 1  t_{3}\\end{pmatrix}$ with $t_{1}=100$, $t_{2}=101$, $t_{3}=102$. The normal equations matrix is\n$$\nA^{T}A=\\begin{pmatrix}\n\\sum_{i=1}^{3}1  \\sum_{i=1}^{3}t_{i} \\\\\n\\sum_{i=1}^{3}t_{i}  \\sum_{i=1}^{3}t_{i}^{2}\n\\end{pmatrix}.\n$$\nCompute the sums:\n$$\n\\sum_{i=1}^{3}1=3,\\quad \\sum_{i=1}^{3}t_{i}=100+101+102=303,\\quad \\sum_{i=1}^{3}t_{i}^{2}=10000+10201+10404=30605.\n$$\nHence\n$$\nA^{T}A=\\begin{pmatrix}3  303\\\\ 303  30605\\end{pmatrix}.\n$$\nFor a symmetric $2\\times 2$ matrix $\\begin{pmatrix}a  b\\\\ b  c\\end{pmatrix}$, the eigenvalues are\n$$\n\\lambda_{\\pm}=\\frac{(a+c)\\pm\\sqrt{(a-c)^{2}+4b^{2}}}{2}.\n$$\nEquivalently, with trace $\\tau=a+c$ and determinant $\\det=ac-b^{2}$, we can write\n$$\n\\lambda_{\\pm}=\\frac{\\tau\\pm\\sqrt{\\tau^{2}-4\\det}}{2}.\n$$\nHere $a=3$, $b=303$, $c=30605$, so\n$$\n\\tau=3+30605=30608,\\qquad \\det=3\\cdot 30605-303^{2}=91815-91809=6,\n$$\nand\n$$\nD=\\tau^{2}-4\\det=30608^{2}-24=936849640.\n$$\nThus\n$$\n\\lambda_{\\max}=\\frac{\\tau+\\sqrt{D}}{2},\\qquad \\lambda_{\\min}=\\frac{\\tau-\\sqrt{D}}{2}.\n$$\nThe spectral condition number is\n$$\n\\kappa_{2}(A^{T}A)=\\frac{\\lambda_{\\max}}{\\lambda_{\\min}}=\\frac{\\tau+\\sqrt{D}}{\\tau-\\sqrt{D}}.\n$$\nTo avoid subtractive cancellation, rewrite\n$$\n\\kappa_{2}(A^{T}A)=\\frac{(\\tau+\\sqrt{D})^{2}}{\\tau^{2}-D}=\\frac{(\\tau+\\sqrt{D})^{2}}{4\\det}=\\frac{(30608+\\sqrt{936849640})^{2}}{24}.\n$$\nCompute $\\sqrt{936849640}$ accurately. Note that $30608^{2}=936849664$, so\n$$\n\\sqrt{936849640}=\\sqrt{30608^{2}-24}\\approx 30608-\\frac{24}{2\\cdot 30608}=30608-\\frac{3}{7652}\\approx 30607.9996079456.\n$$\nHence\n$$\n\\tau+\\sqrt{D}\\approx 61215.9996079456,\n$$\nso\n$$\n(\\tau+\\sqrt{D})^{2}\\approx 3{,}747{,}398{,}608,\n$$\nand therefore\n$$\n\\kappa_{2}(A^{T}A)\\approx \\frac{3{,}747{,}398{,}608}{24}=156{,}141{,}608.666\\ldots\n$$\nRounding to four significant figures gives\n$$\n\\kappa_{2}(A^{T}A)\\approx 1.561\\times 10^{8}.\n$$", "answer": "$$\\boxed{1.561 \\times 10^{8}}$$", "id": "2218032"}]}