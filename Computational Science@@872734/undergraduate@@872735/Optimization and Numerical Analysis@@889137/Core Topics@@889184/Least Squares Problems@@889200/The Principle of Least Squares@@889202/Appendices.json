{"hands_on_practices": [{"introduction": "To begin our exploration of the least squares principle, we will start with a common scenario: modeling a proportional relationship between two variables. This practice demonstrates how to find the optimal slope for a line passing through the origin that best fits a set of data points [@problem_id:2219020]. By applying basic calculus to minimize the sum of squared errors, you will derive the fundamental formula for the slope, building a concrete understanding of the method from the ground up.", "problem": "In many physical systems, a proportional relationship is expected between two quantities, which can be modeled by an equation of the form $y = mx$. Consider an experiment that yields a set of $N$ data points $(x_1, y_1), (x_2, y_2), \\dots, (x_N, y_N)$. We wish to find the \"best-fit\" line that passes through the origin for this dataset.\n\nAccording to the principle of least squares, the best fit is the one that minimizes the sum of the squared vertical distances between the observed data points and the model line. Let this sum of squared errors be denoted by $S$.\n\nDetermine the analytic expression for the slope $m$ that minimizes $S$. Your answer should be expressed in terms of the given data coordinates $x_i$ and $y_i$.", "solution": "We assume the model is $y=m x$ (a line constrained to pass through the origin). For data $\\{(x_{i},y_{i})\\}_{i=1}^{N}$, the least-squares objective (sum of squared vertical residuals) as a function of $m$ is\n$$\nS(m)=\\sum_{i=1}^{N}\\left(y_{i}-m x_{i}\\right)^{2}.\n$$\nTo find the minimizing slope, differentiate $S(m)$ with respect to $m$ and set the derivative to zero (first-order optimality condition):\n$$\n\\frac{dS}{dm}=\\sum_{i=1}^{N}2\\left(y_{i}-m x_{i}\\right)\\left(-x_{i}\\right)=-2\\sum_{i=1}^{N}x_{i}y_{i}+2m\\sum_{i=1}^{N}x_{i}^{2}.\n$$\nSet $\\frac{dS}{dm}=0$:\n$$\n-2\\sum_{i=1}^{N}x_{i}y_{i}+2m\\sum_{i=1}^{N}x_{i}^{2}=0.\n$$\nSolve for $m$:\n$$\nm\\sum_{i=1}^{N}x_{i}^{2}=\\sum_{i=1}^{N}x_{i}y_{i}\\quad\\Rightarrow\\quad m=\\frac{\\sum_{i=1}^{N}x_{i}y_{i}}{\\sum_{i=1}^{N}x_{i}^{2}}.\n$$\nTo confirm this is a minimum, compute the second derivative:\n$$\n\\frac{d^{2}S}{dm^{2}}=2\\sum_{i=1}^{N}x_{i}^{2}\\ge 0,\n$$\nwith strict positivity whenever at least one $x_{i}\\neq 0$, ensuring a unique minimizer. If $\\sum_{i=1}^{N}x_{i}^{2}=0$ (i.e., all $x_{i}=0$), then $S(m)=\\sum_{i=1}^{N}y_{i}^{2}$ is independent of $m$, so the slope is not identifiable. Otherwise, the minimizing slope is the expression above.", "answer": "$$\\boxed{\\frac{\\sum_{i=1}^{N} x_{i} y_{i}}{\\sum_{i=1}^{N} x_{i}^{2}}}$$", "id": "2219020"}, {"introduction": "While calculus is effective for simple models, most real-world problems are best formulated using linear algebra. This exercise introduces the powerful method of normal equations, which provides a general recipe for solving any linear least squares problem of the form $A\\mathbf{x} = \\mathbf{b}$ [@problem_id:2218998]. Working through this example will solidify your ability to set up and solve for the least squares solution vector $\\hat{\\mathbf{x}}$ in a matrix context.", "problem": "In many data analysis applications, we encounter overdetermined linear systems of the form $A\\mathbf{x} = \\mathbf{b}$, where the matrix $A$ has more rows than columns. Such systems typically have no exact solution. The principle of least squares provides a way to find an approximate solution, $\\hat{\\mathbf{x}}$, by minimizing the squared Euclidean norm of the residual vector, $r = A\\mathbf{x} - \\mathbf{b}$.\n\nConsider the specific overdetermined system $A\\mathbf{x} = \\mathbf{b}$ where the matrix $A$ and the vector $\\mathbf{b}$ are given by:\n$$\nA = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{pmatrix}, \\quad \\mathbf{b} = \\begin{pmatrix} 2 \\\\ 3 \\\\ 5 \\end{pmatrix}\n$$\nFind the vector $\\hat{\\mathbf{x}} = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$ that is the least squares solution to this system. Your answer should be the vector $\\hat{\\mathbf{x}}$.", "solution": "We seek the least squares solution $\\hat{\\mathbf{x}}$ that minimizes $\\|A\\mathbf{x}-\\mathbf{b}\\|^{2}$. The necessary condition for a minimizer is given by the normal equations\n$$\nA^{\\top}A\\,\\hat{\\mathbf{x}}=A^{\\top}\\mathbf{b}.\n$$\nWith\n$$\nA=\\begin{pmatrix}1&0\\\\0&1\\\\1&1\\end{pmatrix},\\quad \\mathbf{b}=\\begin{pmatrix}2\\\\3\\\\5\\end{pmatrix},\n$$\ncompute\n$$\nA^{\\top}=\\begin{pmatrix}1&0&1\\\\0&1&1\\end{pmatrix},\n$$\nso\n$$\nA^{\\top}A=\\begin{pmatrix}1&0&1\\\\0&1&1\\end{pmatrix}\\begin{pmatrix}1&0\\\\0&1\\\\1&1\\end{pmatrix}\n=\\begin{pmatrix}2&1\\\\1&2\\end{pmatrix},\n$$\nand\n$$\nA^{\\top}\\mathbf{b}=\\begin{pmatrix}1&0&1\\\\0&1&1\\end{pmatrix}\\begin{pmatrix}2\\\\3\\\\5\\end{pmatrix}\n=\\begin{pmatrix}7\\\\8\\end{pmatrix}.\n$$\nThus $\\hat{\\mathbf{x}}$ satisfies\n$$\n\\begin{pmatrix}2&1\\\\1&2\\end{pmatrix}\\begin{pmatrix}x_{1}\\\\x_{2}\\end{pmatrix}=\\begin{pmatrix}7\\\\8\\end{pmatrix}.\n$$\nSolving,\n$$\n\\begin{cases}\n2x_{1}+x_{2}=7,\\\\\nx_{1}+2x_{2}=8,\n\\end{cases}\n\\quad\\Rightarrow\\quad x_{2}=7-2x_{1},\n$$\nsubstitute into the second equation:\n$$\nx_{1}+2(7-2x_{1})=8\\;\\Rightarrow\\;-3x_{1}=-6\\;\\Rightarrow\\;x_{1}=2,\n$$\nand then\n$$\nx_{2}=7-2\\cdot 2=3.\n$$\nTherefore,\n$$\n\\hat{\\mathbf{x}}=\\begin{pmatrix}2\\\\3\\end{pmatrix}.\n$$\nNotably, $A\\hat{\\mathbf{x}}=\\begin{pmatrix}2\\\\3\\\\5\\end{pmatrix}=\\mathbf{b}$, so the residual is zero and the system is consistent.", "answer": "$$\\boxed{\\begin{pmatrix}2\\\\3\\end{pmatrix}}$$", "id": "2218998"}, {"introduction": "While the normal equations provide a direct theoretical solution, their practical application can sometimes suffer from numerical instability. This practice introduces a more robust and efficient computational technique: QR factorization [@problem_id:2218978]. By using the given $Q$ and $R$ matrices to solve the equivalent, but better-behaved, system $R\\hat{\\mathbf{x}} = Q^T\\mathbf{b}$, you will gain insight into the modern numerical methods used to tackle least squares problems reliably.", "problem": "In a data analysis problem, one needs to find the vector $\\mathbf{x} \\in \\mathbb{R}^2$ that best approximates the solution to the overdetermined linear system $A\\mathbf{x} = \\mathbf{b}$, in the least squares sense.\nThe matrix $A$ has been decomposed using a reduced QR factorization, $A = QR$, where $Q$ is a matrix with orthonormal columns and $R$ is an upper triangular matrix.\nYou are given the matrices $Q$ and $R$, and the vector $\\mathbf{b}$ as follows:\n$$Q = \\frac{1}{3}\\begin{pmatrix} 2 & 2 \\\\ 2 & -1 \\\\ -1 & 2 \\end{pmatrix}, \\quad R = \\begin{pmatrix} 3 & 6 \\\\ 0 & 3 \\end{pmatrix}, \\quad \\mathbf{b} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix}$$\nDetermine the least squares solution vector $\\hat{\\mathbf{x}} = (\\hat{x}_1, \\hat{x}_2)^T$.\nExpress your answer as a row matrix containing the components $\\hat{x}_1$ and $\\hat{x}_2$ in that order. Express any non-integer values as fractions.", "solution": "We are given a reduced QR factorization $A = QR$, with $Q \\in \\mathbb{R}^{3 \\times 2}$ having orthonormal columns and $R \\in \\mathbb{R}^{2 \\times 2}$ upper triangular. The least squares solution $\\hat{\\mathbf{x}}$ to $A\\mathbf{x} = \\mathbf{b}$ satisfies the normal equations, which under QR factorization reduce to solving the triangular system\n$$\nR\\hat{\\mathbf{x}} = Q^{T}\\mathbf{b}.\n$$\nCompute $Q^{T}\\mathbf{b}$. With $Q = \\frac{1}{3}\\begin{pmatrix} 2 & 2 \\\\ 2 & -1 \\\\ -1 & 2 \\end{pmatrix}$ and $\\mathbf{b} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix}$, we have\n$$\nQ^{T} = \\frac{1}{3}\\begin{pmatrix} 2 & 2 & -1 \\\\ 2 & -1 & 2 \\end{pmatrix},\n$$\nso\n$$\nQ^{T}\\mathbf{b} = \\frac{1}{3}\\begin{pmatrix} 2\\cdot 1 + 2\\cdot 2 + (-1)\\cdot 3 \\\\ 2\\cdot 1 + (-1)\\cdot 2 + 2\\cdot 3 \\end{pmatrix}\n= \\frac{1}{3}\\begin{pmatrix} 3 \\\\ 6 \\end{pmatrix}\n= \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}.\n$$\nNow solve $R\\hat{\\mathbf{x}} = Q^{T}\\mathbf{b}$ with $R = \\begin{pmatrix} 3 & 6 \\\\ 0 & 3 \\end{pmatrix}$:\n$$\n\\begin{pmatrix} 3 & 6 \\\\ 0 & 3 \\end{pmatrix}\\begin{pmatrix} \\hat{x}_{1} \\\\ \\hat{x}_{2} \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}.\n$$\nFrom the second row, $3\\hat{x}_{2} = 2$, hence $\\hat{x}_{2} = \\frac{2}{3}$. Substituting into the first row gives $3\\hat{x}_{1} + 6\\hat{x}_{2} = 1$, i.e.,\n$$\n3\\hat{x}_{1} + 6\\left(\\frac{2}{3}\\right) = 1 \\quad \\Rightarrow \\quad 3\\hat{x}_{1} + 4 = 1 \\quad \\Rightarrow \\quad 3\\hat{x}_{1} = -3 \\quad \\Rightarrow \\quad \\hat{x}_{1} = -1.\n$$\nTherefore, the least squares solution is the row vector with components $\\hat{x}_{1}$ and $\\hat{x}_{2}$ in that order.", "answer": "$$\\boxed{\\begin{pmatrix}-1 & \\frac{2}{3}\\end{pmatrix}}$$", "id": "2218978"}]}