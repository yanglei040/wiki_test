{"hands_on_practices": [{"introduction": "The first step in leveraging the power of the Hessian matrix is to become proficient in its calculation. This exercise provides foundational practice by asking you to compute the Hessian for the squared Euclidean norm, a function that is fundamental in many areas of science and engineering, including machine learning as a Tikhonov regularization term. By working through this problem, you will see how the second derivatives of even a non-linear function can result in a constant and elegantly simple Hessian, providing a concrete starting point for understanding local curvature. [@problem_id:2198466]", "problem": "In the context of numerical optimization and machine learning, the analysis of a function's local curvature is crucial for designing efficient algorithms like Newton's method. This curvature is captured by the Hessian matrix.\n\nConsider a function $f: \\mathbb{R}^n \\to \\mathbb{R}$ which represents a simplified regularization term in a model. This function is defined as the squared Euclidean norm of a vector $x \\in \\mathbb{R}^n$, scaled by a constant factor:\n$$f(x) = \\frac{1}{2}\\|x\\|^2_2$$\nwhere $\\|x\\|_2 = \\sqrt{x_1^2 + x_2^2 + \\dots + x_n^2}$ is the standard Euclidean norm.\n\nThe Hessian matrix of $f$, denoted by $\\nabla^2 f(x)$, is an $n \\times n$ matrix whose entry at the $j$-th row and $k$-th column is given by the second-order partial derivative $\\frac{\\partial^2 f}{\\partial x_j \\partial x_k}$.\n\nCalculate the Hessian matrix of the function $f(x)$ for an arbitrary vector $x \\in \\mathbb{R}^n$. Express your final answer in terms of the $n \\times n$ identity matrix, denoted by $I_n$.", "solution": "We start from the definition of the function as a sum of squares:\n$$\nf(x) = \\frac{1}{2}\\|x\\|_{2}^{2} = \\frac{1}{2}\\sum_{i=1}^{n} x_{i}^{2}.\n$$\nTo compute the Hessian, we first find the gradient. For each coordinate $x_{j}$, the partial derivative is\n$$\n\\frac{\\partial f}{\\partial x_{j}} = \\frac{1}{2}\\cdot 2 x_{j} = x_{j}.\n$$\nCollecting these components, the gradient is\n$$\n\\nabla f(x) = \\begin{pmatrix} x_{1} \\\\ \\vdots \\\\ x_{n} \\end{pmatrix} = x.\n$$\nNext, we compute the second-order partial derivatives. For the Hessian entry at row $j$ and column $k$,\n$$\n\\frac{\\partial^{2} f}{\\partial x_{j}\\,\\partial x_{k}} = \\frac{\\partial}{\\partial x_{k}}\\left(\\frac{\\partial f}{\\partial x_{j}}\\right) = \\frac{\\partial}{\\partial x_{k}}(x_{j}) = \n\\begin{cases}\n1, & \\text{if } j = k, \\\\\n0, & \\text{if } j \\neq k,\n\\end{cases}\n$$\nwhich can be written as $\\delta_{jk}$, the Kronecker delta. Therefore, the Hessian matrix has $1$ on the diagonal and $0$ off-diagonal, which is exactly the $n \\times n$ identity matrix. Hence,\n$$\n\\nabla^{2} f(x) = I_{n}.\n$$\nThis matrix is constant and does not depend on $x$.", "answer": "$$\\boxed{I_{n}}$$", "id": "2198466"}, {"introduction": "Beyond mere calculation, the true utility of the Hessian lies in what it tells us about a function's geometry. This practice builds on the previous exercise by exploring the deep connection between the Hessian and the concept of convexity. You will determine the conditions under which a given matrix can serve as the Hessian for a strictly convex function, a property that guarantees any found local minimum is also the unique global minimum. This exercise solidifies the understanding of positive definiteness as the key algebraic test for the desirable \"bowl-shaped\" curvature in optimization problems. [@problem_id:2198477]", "problem": "In the field of numerical optimization, a common task is to find the minimum of a function. For a multivariate quadratic function $f(\\mathbf{x}) = \\frac{1}{2}\\mathbf{x}^T H \\mathbf{x} + \\mathbf{c}^T \\mathbf{x} + d$, where $\\mathbf{x}$ is a column vector of variables, the function is strictly convex if and only if its Hessian matrix, $H$, is positive definite. This property is highly desirable as it guarantees that any local minimum found is also the unique global minimum.\n\nConsider a two-dimensional quadratic cost function, $f(x_1, x_2)$, whose Hessian matrix is given by:\n$$\nH = \\begin{pmatrix} 2 & k \\\\ k & 8 \\end{pmatrix}\n$$\nwhere $k$ is a real-valued parameter that can be adjusted during the design of the cost function.\n\nDetermine the range of values for the parameter $k$ such that the function $f(x_1, x_2)$ is strictly convex.\n\nA. $k < 4$\n\nB. $k > -4$\n\nC. $-4 < k < 4$\n\nD. $-4 \\leq k \\leq 4$\n\nE. $k > 4$ or $k < -4$\n\nF. $k = \\pm 4$", "solution": "For a quadratic function $f(\\mathbf{x})=\\frac{1}{2}\\mathbf{x}^{T}H\\mathbf{x}+\\mathbf{c}^{T}\\mathbf{x}+d$ with constant Hessian $H$, strict convexity is equivalent to $H$ being positive definite. Since $H$ is symmetric, by Sylvester's criterion it is positive definite if and only if all leading principal minors are positive.\n\nGiven\n$$\nH=\\begin{pmatrix}2 & k \\\\ k & 8\\end{pmatrix},\n$$\nthe first leading principal minor is\n$$\n\\Delta_{1}=2>0,\n$$\nwhich holds for all real $k$. The second leading principal minor is the determinant\n$$\n\\Delta_{2}=\\det(H)=2\\cdot 8 - k^{2}=16 - k^{2}.\n$$\nPositive definiteness requires\n$$\n16 - k^{2} > 0 \\quad \\Longleftrightarrow \\quad k^{2} < 16 \\quad \\Longleftrightarrow \\quad -4 < k < 4.\n$$\nTherefore, $H$ is positive definite, and hence $f$ is strictly convex, precisely for $-4<k<4$. This corresponds to option C.", "answer": "$$\\boxed{C}$$", "id": "2198477"}, {"introduction": "Understanding a function's curvature is not just an abstract exercise; it has profound, practical consequences for the performance of optimization algorithms. This advanced practice demonstrates how the Hessian's properties directly govern the efficiency of the gradient descent method. By analyzing a classic quadratic optimization problem, you will derive the algorithm's convergence rate and see how it is dictated by the Hessian's condition number, $\\kappa$. This exercise powerfully illustrates why functions with ill-conditioned Hessians (large $\\kappa$) are notoriously difficult to minimize and motivates the need for more sophisticated second-order methods. [@problem_id:2198464]", "problem": "Consider the problem of minimizing the strongly convex quadratic function $f(x) = \\frac{1}{2}x^T Q x - b^T x$, where $x, b \\in \\mathbb{R}^n$ and $Q$ is an $n \\times n$ symmetric positive definite matrix. The minimum of this function is achieved at a unique point $x^*$.\n\nWe apply the gradient descent method to find this minimum, using the iterative update rule $x_{k+1} = x_k - \\alpha \\nabla f(x_k)$, where $k$ is the iteration number and $\\alpha$ is a constant step size. Let $\\lambda_{\\min}$ and $\\lambda_{\\max}$ be the smallest and largest eigenvalues of the matrix $Q$, respectively. For this problem, we use the specific fixed step size $\\alpha = \\frac{2}{\\lambda_{\\min} + \\lambda_{\\max}}$.\n\nThe Euclidean norm of the error, $\\|x_k - x^*\\|_2$, is known to decrease at each step. The convergence is linear, meaning there exists a constant $\\rho \\in [0, 1)$ such that $\\|x_{k+1} - x^*\\|_2 \\le \\rho \\|x_k - x^*\\|_2$ for all $k \\ge 0$.\n\nYour task is to determine the tightest possible value for this convergence factor $\\rho$. Express your final answer solely in terms of the condition number $\\kappa = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}}$ of the matrix $Q$.", "solution": "The objective function is $f(x) = \\frac{1}{2}x^T Q x - b^T x$. Its gradient is given by $\\nabla f(x) = Qx - b$.\nThe optimal solution $x^*$ is found by setting the gradient to zero: $\\nabla f(x^*) = Qx^* - b = 0$, which implies $b = Qx^*$.\n\nThe gradient descent update rule is $x_{k+1} = x_k - \\alpha \\nabla f(x_k)$.\nSubstituting the expression for the gradient, we get:\n$x_{k+1} = x_k - \\alpha (Qx_k - b)$.\n\nLet the error at iteration $k$ be defined as $e_k = x_k - x^*$. We want to find a recursive relation for the error.\n$e_{k+1} = x_{k+1} - x^* = (x_k - \\alpha (Qx_k - b)) - x^*$.\nSubstitute $b = Qx^*$:\n$e_{k+1} = x_k - \\alpha (Qx_k - Qx^*) - x^* = (x_k - x^*) - \\alpha Q(x_k - x^*)$.\nThis gives the error update rule:\n$e_{k+1} = (I - \\alpha Q)e_k$.\n\nTo analyze the convergence in the Euclidean norm, we take the 2-norm of both sides:\n$\\|e_{k+1}\\|_2 = \\|(I - \\alpha Q)e_k\\|_2$.\nUsing the property of matrix norms, $\\|Ax\\|_2 \\le \\|A\\|_2 \\|x\\|_2$, where $\\|A\\|_2$ is the spectral norm of $A$, we get:\n$\\|e_{k+1}\\|_2 \\le \\|I - \\alpha Q\\|_2 \\|e_k\\|_2$.\nThe convergence factor $\\rho$ is therefore the spectral norm of the iteration matrix $G = I - \\alpha Q$.\n$\\rho = \\|I - \\alpha Q\\|_2$.\n\nSince $Q$ is a symmetric matrix, the matrix $G = I - \\alpha Q$ is also symmetric. The spectral norm of a symmetric matrix is the maximum of the absolute values of its eigenvalues.\nLet $\\lambda_i$ for $i=1, \\dots, n$ be the eigenvalues of $Q$. Since $Q$ is positive definite, $0 < \\lambda_{\\min} \\le \\lambda_i \\le \\lambda_{\\max}$ for all $i$.\nThe eigenvalues of $G = I - \\alpha Q$ are $\\mu_i = 1 - \\alpha \\lambda_i$.\nSo, the convergence factor is:\n$\\rho = \\|G\\|_2 = \\max_{i} |\\mu_i| = \\max_{i} |1 - \\alpha \\lambda_i|$.\n\nThe function $g(\\lambda) = |1 - \\alpha \\lambda|$ is maximized over the interval $[\\lambda_{\\min}, \\lambda_{\\max}]$ at one of the endpoints. Thus,\n$\\rho = \\max(|1 - \\alpha \\lambda_{\\min}|, |1 - \\alpha \\lambda_{\\max}|)$.\n\nThe problem specifies the step size $\\alpha = \\frac{2}{\\lambda_{\\min} + \\lambda_{\\max}}$. We substitute this value into the expression for $\\rho$.\nLet's evaluate the two terms inside the maximum function.\nFirst term:\n$1 - \\alpha \\lambda_{\\min} = 1 - \\frac{2\\lambda_{\\min}}{\\lambda_{\\min} + \\lambda_{\\max}} = \\frac{(\\lambda_{\\min} + \\lambda_{\\max}) - 2\\lambda_{\\min}}{\\lambda_{\\min} + \\lambda_{\\max}} = \\frac{\\lambda_{\\max} - \\lambda_{\\min}}{\\lambda_{\\max} + \\lambda_{\\min}}$.\nSecond term:\n$1 - \\alpha \\lambda_{\\max} = 1 - \\frac{2\\lambda_{\\max}}{\\lambda_{\\min} + \\lambda_{\\max}} = \\frac{(\\lambda_{\\min} + \\lambda_{\\max}) - 2\\lambda_{\\max}}{\\lambda_{\\min} + \\lambda_{\\max}} = \\frac{\\lambda_{\\min} - \\lambda_{\\max}}{\\lambda_{\\min} + \\lambda_{\\max}} = - \\frac{\\lambda_{\\max} - \\lambda_{\\min}}{\\lambda_{\\max} + \\lambda_{\\min}}$.\n\nThe absolute values of these two terms are equal:\n$|1 - \\alpha \\lambda_{\\min}| = |1 - \\alpha \\lambda_{\\max}| = \\frac{\\lambda_{\\max} - \\lambda_{\\min}}{\\lambda_{\\max} + \\lambda_{\\min}}$.\nTherefore, the convergence factor is:\n$\\rho = \\frac{\\lambda_{\\max} - \\lambda_{\\min}}{\\lambda_{\\max} + \\lambda_{\\min}}$.\n\nFinally, we need to express this in terms of the condition number $\\kappa = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}}$. To do this, we divide both the numerator and the denominator of the expression for $\\rho$ by $\\lambda_{\\min}$ (which is strictly positive):\n$\\rho = \\frac{\\frac{\\lambda_{\\max}}{\\lambda_{\\min}} - \\frac{\\lambda_{\\min}}{\\lambda_{\\min}}}{\\frac{\\lambda_{\\max}}{\\lambda_{\\min}} + \\frac{\\lambda_{\\min}}{\\lambda_{\\min}}} = \\frac{\\kappa - 1}{\\kappa + 1}$.\n\nThis is the tightest possible value for the convergence factor $\\rho$ for the given step size.", "answer": "$$\\boxed{\\frac{\\kappa - 1}{\\kappa + 1}}$$", "id": "2198464"}]}