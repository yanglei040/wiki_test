{"hands_on_practices": [{"introduction": "To begin, let's walk through a fundamental application of exact line search within the steepest descent method. This first exercise [@problem_id:2170891] provides a complete, step-by-step calculation for a simple quadratic function. This practice will establish the core procedure of first finding a search direction and then calculating the precise step size that minimizes the function along that line.", "problem": "Consider the optimization problem of minimizing the quadratic objective function $f(x_1, x_2) = (x_1 + x_2 - 2)^2$. We will perform one step of the steepest descent algorithm, beginning from the initial point $\\mathbf{x}_0 = (0, 0)$. The search direction for this step is the steepest descent direction, defined as $\\mathbf{p}_0 = -\\nabla f(\\mathbf{x}_0)$, where $\\nabla f$ is the gradient of the function.\n\nTo find the optimal distance to move along this direction, an exact line search is performed. This procedure involves solving a one-dimensional minimization problem to find the step size $\\alpha > 0$ that minimizes the function $g(\\alpha) = f(\\mathbf{x}_0 + \\alpha\\mathbf{p}_0)$.\n\nDetermine the exact optimal step size $\\alpha^*$ that results from this procedure.", "solution": "We are given the quadratic function $f(x_1, x_2) = (x_1 + x_2 - 2)^2$ and the initial point $\\mathbf{x}_0 = (0, 0)$. The steepest descent direction is defined as $\\mathbf{p}_0 = -\\nabla f(\\mathbf{x}_0)$, where the gradient is computed by differentiating:\n$$\n\\nabla f(x_1, x_2) = \\left(\\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}\\right).\n$$\nSince $f(x_1, x_2) = (x_1 + x_2 - 2)^2$, by the chain rule,\n$$\n\\frac{\\partial f}{\\partial x_1} = 2(x_1 + x_2 - 2)\\cdot \\frac{\\partial}{\\partial x_1}(x_1 + x_2 - 2) = 2(x_1 + x_2 - 2),\n$$\nand similarly,\n$$\n\\frac{\\partial f}{\\partial x_2} = 2(x_1 + x_2 - 2).\n$$\nTherefore,\n$$\n\\nabla f(x_1, x_2) = 2(x_1 + x_2 - 2)\\,(1, 1).\n$$\nEvaluating at $\\mathbf{x}_0 = (0, 0)$ gives\n$$\n\\nabla f(\\mathbf{x}_0) = 2(0 + 0 - 2)\\,(1, 1) = -4\\,(1, 1) = (-4, -4),\n$$\nso the steepest descent direction is\n$$\n\\mathbf{p}_0 = -\\nabla f(\\mathbf{x}_0) = (4, 4).\n$$\nFor the exact line search, define the univariate function\n$$\ng(\\alpha) = f(\\mathbf{x}_0 + \\alpha \\mathbf{p}_0) = f\\big((0, 0) + \\alpha(4, 4)\\big) = f(4\\alpha, 4\\alpha).\n$$\nSubstituting into $f$,\n$$\ng(\\alpha) = (4\\alpha + 4\\alpha - 2)^2 = (8\\alpha - 2)^2.\n$$\nTo minimize $g(\\alpha)$ over $\\alpha > 0$, set its derivative to zero:\n$$\ng'(\\alpha) = 2(8\\alpha - 2)\\cdot 8 = 16(8\\alpha - 2).\n$$\nSolve $g'(\\alpha) = 0$:\n$$\n16(8\\alpha - 2) = 0 \\;\\;\\Longrightarrow\\;\\; 8\\alpha - 2 = 0 \\;\\;\\Longrightarrow\\;\\; \\alpha^* = \\frac{1}{4}.\n$$\nThis satisfies $\\alpha^* > 0$ and yields the exact minimizer along the search direction.", "answer": "$$\\boxed{\\frac{1}{4}}$$", "id": "2170891"}, {"introduction": "Optimization problems in the real world are rarely simple quadratics, and the line search subproblem can take many forms. This next practice [@problem_id:2170899] challenges you to apply the exact line search principle to a trigonometric function, which introduces the possibility of multiple local minima along the search direction. Solving it will require you to not only find a valid step size but to identify the smallest positive one that achieves the minimum, a common and important task in iterative methods.", "problem": "Consider the process of minimizing a single-variable objective function $f(x)$ using an iterative optimization algorithm. A key step in such algorithms is the line search, where, starting from a point $x_k$, we find a step size $\\alpha$ that minimizes the function along a given search direction $p_k$. The next point in the sequence is then determined by $x_{k+1} = x_k + \\alpha p_k$.\n\nLet the objective function be $f(x) = \\sin(x) + \\cos(x)$, where the argument $x$ is in radians. Suppose we are at the initial point $x_0 = 0$. The search direction is chosen to be the direction of steepest descent, which for a single-variable function is given by $p_0 = -f'(x_0)$, where $f'(x_0)$ is the derivative of $f(x)$ evaluated at $x_0$.\n\nDetermine the smallest positive step size, $\\alpha > 0$, that minimizes the function along this direction. That is, find the smallest positive $\\alpha$ that minimizes the new function $\\phi(\\alpha) = f(x_0 + \\alpha p_0)$.\n\nProvide the exact value of $\\alpha$ as a closed-form analytic expression.", "solution": "We are given $f(x) = \\sin(x) + \\cos(x)$ and $x_0 = 0$. The steepest descent direction is $p_0 = -f'(x_0)$. Compute the derivative:\n$$\nf'(x) = \\cos(x) - \\sin(x).\n$$\nEvaluate at $x_0 = 0$:\n$$\nf'(0) = \\cos(0) - \\sin(0) = 1 - 0 = 1,\n$$\nso\n$$\np_0 = -f'(0) = -1.\n$$\nDefine the line-search objective $\\phi(\\alpha) = f(x_0 + \\alpha p_0)$:\n$$\n\\phi(\\alpha) = f(0 + \\alpha(-1)) = f(-\\alpha) = \\sin(-\\alpha) + \\cos(-\\alpha) = -\\sin(\\alpha) + \\cos(\\alpha).\n$$\nTo minimize $\\phi(\\alpha)$ over $\\alpha > 0$, differentiate and set to zero:\n$$\n\\phi'(\\alpha) = -\\cos(\\alpha) - \\sin(\\alpha) = 0 \\quad \\Longleftrightarrow \\quad \\cos(\\alpha) + \\sin(\\alpha) = 0.\n$$\nThis gives\n$$\n\\tan(\\alpha) = -1 \\quad \\text{with} \\quad \\cos(\\alpha) \\neq 0 \\quad \\Longrightarrow \\quad \\alpha = -\\frac{\\pi}{4} + n\\pi, \\quad n \\in \\mathbb{Z}.\n$$\nAmong positive solutions, the smallest is obtained with $n=1$:\n$$\n\\alpha = -\\frac{\\pi}{4} + \\pi = \\frac{3\\pi}{4}.\n$$\nVerify it is a minimum via the second derivative:\n$$\n\\phi''(\\alpha) = \\sin(\\alpha) - \\cos(\\alpha).\n$$\nAt $\\alpha = \\frac{3\\pi}{4}$, $\\sin\\left(\\frac{3\\pi}{4}\\right) > 0$ and $\\cos\\left(\\frac{3\\pi}{4}\\right) < 0$, hence $\\phi''\\left(\\frac{3\\pi}{4}\\right) > 0$, confirming a local minimum. Since $\\phi$ is periodic with period $2\\pi$ and all minima occur at $\\alpha = \\frac{3\\pi}{4} + 2\\pi k$, $k \\in \\mathbb{Z}$, the smallest positive minimizer is $\\alpha = \\frac{3\\pi}{4}$.", "answer": "$$\\boxed{\\frac{3\\pi}{4}}$$", "id": "2170899"}, {"introduction": "A deep understanding of an algorithm comes from being able to predict its behavior and, conversely, to deduce properties of a problem from the algorithm's output. This final exercise [@problem_id:2170940] presents an insightful 'inverse problem' where you will use an observed behavior of the steepest descent algorithm to uncover a key characteristic of the objective function itself. This practice powerfully connects the algebraic mechanics of the line search to the underlying geometry of the optimization landscape, particularly the shape of the function's level sets.", "problem": "Consider a quadratic objective function $f(x_1, x_2) = \\frac{A}{2} x_1^2 + \\frac{B}{2} x_2^2 - C x_2$, where $A, B, C$ are positive real constants. The method of steepest descent with exact line search is used to find the minimum of this function. It is observed that for any starting point $\\mathbf{x}_0 = (x_{0,1}, 0)$ with $x_{0,1} \\neq 0$, the first iterate of the algorithm, $\\mathbf{x}_1$, always lies on the $x_2$-axis. Based on this observation, determine the value of the ratio $B/A$.", "solution": "We minimize the quadratic function $f(x_1, x_2) = \\frac{A}{2} x_1^2 + \\frac{B}{2} x_2^2 - C x_2$ with $A>0$, $B>0$, $C>0$ by steepest descent with exact line search. Its gradient and Hessian are\n$$\n\\nabla f(x_1, x_2) = \\begin{pmatrix} A x_1 \\\\ B x_2 - C \\end{pmatrix}, \\qquad H = \\begin{pmatrix} A & 0 \\\\ 0 & B \\end{pmatrix}.\n$$\nLet the starting point be $\\mathbf{x}_0 = (x_{0,1}, 0)$ with $x_{0,1} \\neq 0$. Then the initial gradient is\n$$\n\\mathbf{g}_0 = \\nabla f(\\mathbf{x}_0) = \\begin{pmatrix} A x_{0,1} \\\\ -C \\end{pmatrix}.\n$$\nThe steepest descent direction is $\\mathbf{d}_0 = -\\mathbf{g}_0$, and with exact line search the step size $\\alpha_0$ minimizes $\\phi(\\alpha) = f(\\mathbf{x}_0 + \\alpha \\mathbf{d}_0)$. For a quadratic, using $\\nabla f(\\mathbf{x}_0 + \\alpha \\mathbf{d}_0) = \\nabla f(\\mathbf{x}_0) + \\alpha H \\mathbf{d}_0$, we have\n$$\n\\phi'(\\alpha) = \\nabla f(\\mathbf{x}_0 + \\alpha \\mathbf{d}_0)^T \\mathbf{d}_0 = (\\mathbf{g}_0 + \\alpha H \\mathbf{d}_0)^T \\mathbf{d}_0 = \\mathbf{g}_0^T \\mathbf{d}_0 + \\alpha \\mathbf{d}_0^T H \\mathbf{d}_0.\n$$\nSetting $\\phi'(\\alpha_0) = 0$ yields\n$$\n\\alpha_0 = \\frac{\\mathbf{g}_0^T \\mathbf{g}_0}{\\mathbf{g}_0^T H \\mathbf{g}_0}.\n$$\nCompute the needed quantities:\n$$\n\\mathbf{g}_0^T \\mathbf{g}_0 = (A x_{0,1})^2 + C^2 = A^2 x_{0,1}^2 + C^2,\n$$\n$$\nH \\mathbf{g}_0 = \\begin{pmatrix} A & 0 \\\\ 0 & B \\end{pmatrix} \\begin{pmatrix} A x_{0,1} \\\\ -C \\end{pmatrix} = \\begin{pmatrix} A^2 x_{0,1} \\\\ -BC \\end{pmatrix},\n$$\n$$\n\\mathbf{g}_0^T H \\mathbf{g}_0 = (A x_{0,1})(A^2 x_{0,1}) + (-C)(-BC) = A^3 x_{0,1}^2 + BC^2.\n$$\nHence\n$$\n\\alpha_0 = \\frac{A^2 x_{0,1}^2 + C^2}{A^3 x_{0,1}^2 + BC^2}.\n$$\nThe first iterate is\n$$\n\\mathbf{x}_1 = \\mathbf{x}_0 - \\alpha_0 \\mathbf{g}_0 = \\begin{pmatrix} x_{0,1} - \\alpha_0 A x_{0,1} \\\\ 0 - \\alpha_0(-C) \\end{pmatrix},\n$$\nso its first component is\n$$\nx_{1,1} = x_{0,1}(1 - \\alpha_0 A).\n$$\nThe observation states $\\mathbf{x}_1$ always lies on the $x_2$-axis for any $x_{0,1} \\neq 0$, thus $x_{1,1}=0$ for all such $x_{0,1}$. Therefore $1 - \\alpha_0 A = 0$ must hold for all $x_{0,1}$, i.e.,\n$$\n\\alpha_0 = \\frac{1}{A}.\n$$\nEquating this with the exact line-search expression gives\n$$\n\\frac{A^2 x_{0,1}^2 + C^2}{A^3 x_{0,1}^2 + BC^2} = \\frac{1}{A}.\n$$\nCross-multiplying and simplifying,\n$$\nA(A^2 x_{0,1}^2 + C^2) = A^3 x_{0,1}^2 + BC^2\n\\;\\;\\Longrightarrow\\;\\;\nA^3 x_{0,1}^2 + AC^2 = A^3 x_{0,1}^2 + BC^2\n\\;\\;\\Longrightarrow\\;\\;\n(A-B)C^2 = 0.\n$$\nSince $C>0$, it follows that $A=B$, hence\n$$\n\\frac{B}{A}=1.\n$$\nThis condition is also sufficient, because if $A=B$ then $\\alpha_0 = (A^2 x_{0,1}^2 + C^2) / (A^3 x_{0,1}^2 + AC^2) = 1/A$ for every $x_{0,1}$, yielding $x_{1,1} = x_{0,1}(1 - \\alpha_0 A) = 0$.", "answer": "$$\\boxed{1}$$", "id": "2170940"}]}