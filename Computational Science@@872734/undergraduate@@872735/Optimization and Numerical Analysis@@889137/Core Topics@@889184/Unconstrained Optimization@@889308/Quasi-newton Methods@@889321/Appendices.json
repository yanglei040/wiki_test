{"hands_on_practices": [{"introduction": "Quasi-Newton methods begin an optimization journey from an initial point, and the very first task is to determine the most promising direction to move in. This exercise [@problem_id:2195903] demystifies this crucial first step. By using the simplest possible approximation for the Hessian matrix—the identity matrix—you will see how the initial search direction is computed, which effectively makes the first step equivalent to a steepest descent step. This practice builds a foundational understanding of how these powerful iterative methods are initiated.", "problem": "An iterative optimization algorithm is employed to find a local minimum of the two-variable function $f(x_1, x_2) = \\sin(x_1) + \\cosh(x_2)$. The algorithm is initialized at the point $x_0 = \\begin{pmatrix} 0 \\\\ \\ln(2) \\end{pmatrix}$. At the first step (iteration $k=0$), a search direction $p_0$ is computed by solving the linear system of equations $B_0 p_0 = -\\nabla f(x_0)$, where $\\nabla f(x_0)$ is the gradient of $f$ evaluated at $x_0$, and $B_0$ is an approximation of the Hessian matrix. For this procedure, the initial approximation is chosen as the $2 \\times 2$ identity matrix, $I$. Determine the components of the initial search direction vector $p_0$.", "solution": "The problem asks for the initial search direction vector $p_0$, which is the solution to the linear system $B_0 p_0 = -\\nabla f(x_0)$. We can solve this problem by following three main steps: first, compute the gradient of the function $f(x_1, x_2)$; second, evaluate this gradient at the initial point $x_0$; and third, solve the given linear system for $p_0$.\n\nStep 1: Compute the gradient of the function.\nThe function is given by $f(x_1, x_2) = \\sin(x_1) + \\cosh(x_2)$.\nThe gradient of $f$, denoted by $\\nabla f$, is a vector of its partial derivatives:\n$$\n\\nabla f(x_1, x_2) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\end{pmatrix}\n$$\nThe partial derivative with respect to $x_1$ is:\n$$\n\\frac{\\partial f}{\\partial x_1} = \\frac{\\partial}{\\partial x_1} (\\sin(x_1) + \\cosh(x_2)) = \\cos(x_1)\n$$\nThe partial derivative with respect to $x_2$ is:\n$$\n\\frac{\\partial f}{\\partial x_2} = \\frac{\\partial}{\\partial x_2} (\\sin(x_1) + \\cosh(x_2)) = \\sinh(x_2)\n$$\nTherefore, the gradient vector is:\n$$\n\\nabla f(x_1, x_2) = \\begin{pmatrix} \\cos(x_1) \\\\ \\sinh(x_2) \\end{pmatrix}\n$$\n\nStep 2: Evaluate the gradient at the initial point $x_0$.\nThe initial point is given as $x_0 = \\begin{pmatrix} 0 \\\\ \\ln(2) \\end{pmatrix}$. We substitute these values into the gradient expression:\n$$\n\\nabla f(x_0) = \\nabla f(0, \\ln(2)) = \\begin{pmatrix} \\cos(0) \\\\ \\sinh(\\ln(2)) \\end{pmatrix}\n$$\nWe evaluate each component. The cosine of 0 is:\n$$\n\\cos(0) = 1\n$$\nThe hyperbolic sine function is defined as $\\sinh(y) = \\frac{\\exp(y) - \\exp(-y)}{2}$. For $y = \\ln(2)$:\n$$\n\\sinh(\\ln(2)) = \\frac{\\exp(\\ln(2)) - \\exp(-\\ln(2))}{2} = \\frac{2 - \\exp(\\ln(2^{-1}))}{2} = \\frac{2 - \\frac{1}{2}}{2} = \\frac{\\frac{3}{2}}{2} = \\frac{3}{4}\n$$\nSo, the gradient vector at the initial point is:\n$$\n\\nabla f(x_0) = \\begin{pmatrix} 1 \\\\ \\frac{3}{4} \\end{pmatrix}\n$$\n\nStep 3: Solve the linear system for $p_0$.\nThe system to solve is $B_0 p_0 = -\\nabla f(x_0)$. We are given that $B_0$ is the $2 \\times 2$ identity matrix, $I = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}$.\nThe system becomes:\n$$\nI p_0 = -\\nabla f(x_0)\n$$\nMultiplying any vector by the identity matrix leaves the vector unchanged, so $p_0 = -\\nabla f(x_0)$.\nSubstituting the value of the gradient we found:\n$$\np_0 = - \\begin{pmatrix} 1 \\\\ \\frac{3}{4} \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ -\\frac{3}{4} \\end{pmatrix}\n$$\nThus, the components of the initial search direction vector $p_0$ are $-1$ and $-\\frac{3}{4}$.", "answer": "$$\\boxed{\\begin{pmatrix} -1 \\\\ -\\frac{3}{4} \\end{pmatrix}}$$", "id": "2195903"}, {"introduction": "After taking the initial step, a quasi-Newton method's real power comes from its ability to learn about the function's curvature from the gradient's behavior. This practice [@problem_id:2195901] guides you through one complete cycle of the celebrated Broyden–Fletcher–Goldfarb–Shanno (BFGS) algorithm. You will use the information gathered from the first step to update the Hessian approximation, illustrating how the method refines its quadratic model of the objective function to generate a more sophisticated search direction for the subsequent iteration.", "problem": "An optimization routine is used to find the minimum of a quadratic function $f(x)$ defined for $x \\in \\mathbb{R}^2$ as:\n$$f(x) = \\frac{1}{2} x^T A x - b^T x$$\nwhere the matrix $A$ and vector $b$ are given by:\n$$A = \\begin{pmatrix} 3  1 \\\\ 1  2 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$$\n\nThe routine employs a quasi-Newton method, which iteratively refines an approximation of the Hessian matrix. The method starts at an initial point $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ with an initial Hessian approximation $B_0 = I$, where $I$ is the $2 \\times 2$ identity matrix.\n\nAt each iteration $k$, the search direction $p_k$ is computed by solving the linear system $B_k p_k = -\\nabla f(x_k)$. The next point is then found using a fixed unit step length, i.e., $x_{k+1} = x_k + p_k$.\n\nAfter determining the new point $x_{k+1}$, the Hessian approximation is updated from $B_k$ to $B_{k+1}$ using the Broyden-Fletcher-Goldfarb-Shanno (BFGS) update formula:\n$$B_{k+1} = B_k - \\frac{B_k s_k s_k^T B_k}{s_k^T B_k s_k} + \\frac{y_k y_k^T}{y_k^T s_k}$$\nwhere $s_k = x_{k+1} - x_k$ and $y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$.\n\nYour task is to perform the first full iteration of this algorithm to determine the search direction for the second step, $p_1$. Calculate the value of the first component of the vector $p_1$. Round your final answer to four significant figures.", "solution": "We are given the quadratic function $f(x) = \\frac{1}{2} x^{T} A x - b^{T} x$ with $A = \\begin{pmatrix} 3  1 \\\\ 1  2 \\end{pmatrix}$ and $b = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$. Its gradient is $\\nabla f(x) = A x - b$.\n\nAt the initial point $x_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ and with $B_{0} = I$, compute the first search direction $p_{0}$ by solving\n$$\nB_{0} p_{0} = -\\nabla f(x_{0}).\n$$\nSince $\\nabla f(x_{0}) = A x_{0} - b = -b = \\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix}$ and $B_{0} = I$, we have\n$$\np_{0} = -\\nabla f(x_{0}) = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}.\n$$\nWith unit step, the next point is\n$$\nx_{1} = x_{0} + p_{0} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}.\n$$\nDefine $s_{0} = x_{1} - x_{0} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$. Compute the gradients:\n$$\n\\nabla f(x_{1}) = A x_{1} - b = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}, \\quad \\nabla f(x_{0}) = \\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix}.\n$$\nThus\n$$\ny_{0} = \\nabla f(x_{1}) - \\nabla f(x_{0}) = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}.\n$$\nUpdate the Hessian approximation using the BFGS formula\n$$\nB_{1} = B_{0} - \\frac{B_{0} s_{0} s_{0}^{T} B_{0}}{s_{0}^{T} B_{0} s_{0}} + \\frac{y_{0} y_{0}^{T}}{y_{0}^{T} s_{0}}.\n$$\nWith $B_{0} = I$, $s_{0} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$, $y_{0} = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}$, we have\n$$\ns_{0}^{T} s_{0} = 1, \\quad s_{0} s_{0}^{T} = \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix}, \\quad y_{0} y_{0}^{T} = \\begin{pmatrix} 9  3 \\\\ 3  1 \\end{pmatrix}, \\quad y_{0}^{T} s_{0} = 3,\n$$\nso\n$$\nB_{1} = I - \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix} + \\frac{1}{3} \\begin{pmatrix} 9  3 \\\\ 3  1 \\end{pmatrix} = \\begin{pmatrix} 3  1 \\\\ 1  \\frac{4}{3} \\end{pmatrix}.\n$$\nThe search direction for the second step $p_{1}$ solves\n$$\nB_{1} p_{1} = -\\nabla f(x_{1}) = -\\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}.\n$$\nLet $p_{1} = \\begin{pmatrix} p \\\\ q \\end{pmatrix}$. Then\n$$\n\\begin{cases}\n3p + q = -2, \\\\\np + \\frac{4}{3} q = -1.\n\\end{cases}\n$$\nFrom the first equation $q = -2 - 3p$. Substitute into the second:\n$$\np + \\frac{4}{3}(-2 - 3p) = -1 \\;\\Rightarrow\\; p - \\frac{8}{3} - 4p = -1 \\;\\Rightarrow\\; -3p - \\frac{8}{3} = -1 \\;\\Rightarrow\\; -3p = \\frac{5}{3} \\;\\Rightarrow\\; p = -\\frac{5}{9}.\n$$\nTherefore, the first component of $p_{1}$ is $-\\frac{5}{9}$, which to four significant figures is $-0.5556$.", "answer": "$$\\boxed{-0.5556}$$", "id": "2195901"}, {"introduction": "The BFGS method has a remarkable feature: if you start with a positive definite Hessian approximation, all subsequent approximations can also be kept positive definite, which guarantees that each computed search direction is a descent direction. This stability, however, hinges on a critical requirement known as the curvature condition, $y_k^T s_k > 0$. This conceptual problem [@problem_id:2195904] invites you to explore the theoretical underpinnings of the algorithm by investigating what happens when this condition is violated, revealing why it is so essential for the robustness and success of the BFGS method.", "problem": "In the context of unconstrained optimization, quasi-Newton methods are used to find the minimum of a function $f(x)$ by iteratively updating an approximation to the Hessian matrix. The Broyden–Fletcher–Goldfarb–Shanno (BFGS) method is a popular quasi-Newton algorithm. Starting with an initial guess $x_0$ and an initial symmetric positive definite Hessian approximation $B_0$ (often the identity matrix), the method proceeds iteratively. At step $k$, a search direction $p_k = -B_k^{-1} \\nabla f(x_k)$ is computed. A suitable step length $\\alpha_k  0$ is then found via a line search, and the position is updated as $x_{k+1} = x_k + \\alpha_k p_k$.\n\nThe Hessian approximation is then updated using the BFGS formula. Let $s_k = x_{k+1} - x_k$ be the step vector and $y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$ be the change in the gradient. The BFGS update is given by:\n$$B_{k+1} = B_k - \\frac{B_k s_k s_k^T B_k}{s_k^T B_k s_k} + \\frac{y_k y_k^T}{y_k^T s_k}$$\nA crucial property of the BFGS method is that if $B_k$ is symmetric and positive definite, then $B_{k+1}$ will also be symmetric and positive definite, provided that the \"curvature condition\" $y_k^T s_k  0$ is satisfied. This condition is typically enforced by choosing the step length $\\alpha_k$ to satisfy the Wolfe conditions.\n\nSuppose that at a particular iteration $k$, the line search procedure fails to find a step length that satisfies the curvature condition. Instead, the resulting step yields $y_k^T s_k \\le 0$. Assuming $B_k$ is a symmetric positive definite matrix and $s_k \\neq 0$, what is the most direct and certain mathematical consequence for the updated Hessian approximation $B_{k+1}$ derived from the BFGS formula?\n\nA. The updated Hessian approximation $B_{k+1}$ is not guaranteed to be positive definite.\n\nB. The algorithm immediately terminates because the update formula for $B_{k+1}$ will always involve division by zero.\n\nC. The updated Hessian approximation $B_{k+1}$ is guaranteed to remain positive definite, though the algorithm's convergence may be impaired.\n\nD. The updated Hessian approximation $B_{k+1}$ will become singular.\n\nE. The updated Hessian approximation $B_{k+1}$ will always be negative definite.", "solution": "To determine the consequence of violating the curvature condition, we analyze the BFGS update formula under the given assumptions. The formula is:\n$$B_{k+1} = B_k - \\frac{B_k s_k s_k^T B_k}{s_k^T B_k s_k} + \\frac{y_k y_k^T}{y_k^T s_k}$$\nWe are given that $B_k$ is symmetric and positive definite. The goal is to determine the properties of $B_{k+1}$ when the curvature condition is violated, i.e., when $y_k^T s_k \\le 0$.\n\nLet's examine the terms of the update. The update consists of two rank-one updates to $B_k$.\n\nThe first update term is $-\\frac{B_k s_k s_k^T B_k}{s_k^T B_k s_k}$. Since $B_k$ is positive definite and $s_k \\neq 0$, the denominator $s_k^T B_k s_k$ is strictly positive. The numerator is the outer product of the vector $B_k s_k$ with itself, which results in a positive semi-definite matrix. The matrix $B_k - \\frac{B_k s_k s_k^T B_k}{s_k^T B_k s_k}$ can be shown to be positive semi-definite and preserves a significant part of the positive definite character of $B_k$.\n\nThe second update term is $\\frac{y_k y_k^T}{y_k^T s_k}$. The numerator $y_k y_k^T$ is the outer product of the vector $y_k$ with itself. For any vector $z$, $z^T(y_k y_k^T)z = (z^T y_k)^2 \\ge 0$, so $y_k y_k^T$ is a positive semi-definite matrix. The nature of this second update term is critically dependent on the sign of the scalar denominator, $y_k^T s_k$.\n\nStandard Case (Curvature condition holds): If $y_k^T s_k  0$, then the second term is a positive scalar multiple of a positive semi-definite matrix. Adding this to the first part of the update guarantees that $B_{k+1}$ is positive definite.\n\nViolated Case (Curvature condition fails): We are given that $y_k^T s_k \\le 0$. We must consider two sub-cases.\nCase 1: $y_k^T s_k = 0$. In this scenario, the denominator of the second update term is zero. The BFGS formula is undefined due to division by zero. This means the update cannot be computed as written. This invalidates option B, which claims this *always* happens; it only happens in this specific sub-case, not for $y_k^T s_k  0$.\n\nCase 2: $y_k^T s_k  0$. In this scenario, the denominator is negative. The second update term becomes a negative scalar multiple of the positive semi-definite matrix $y_k y_k^T$. This means we are adding a negative semi-definite matrix to $B_k$. Subtracting a positive semi-definite matrix from a positive definite matrix does not guarantee that the result remains positive definite. In fact, it is very likely to destroy the positive definite property.\n\nLet's construct a simple counterexample to demonstrate that positive definiteness can be lost. Let the dimension be 2. Let the current Hessian approximation be the identity matrix, $B_k = I = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}$, which is positive definite. Let the step be $s_k = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$. Let the change in gradient be $y_k = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}$.\n\nFirst, check the curvature condition:\n$y_k^T s_k = \\begin{pmatrix} -1  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = -1  0$. The condition is violated.\n\nNow, compute the terms for the BFGS update:\n$s_k^T B_k s_k = s_k^T I s_k = s_k^T s_k = 1^2 + 0^2 = 1$.\n$B_k s_k = I s_k = s_k = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\nThe first update term (the subtraction):\n$\\frac{B_k s_k s_k^T B_k}{s_k^T B_k s_k} = \\frac{\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\begin{pmatrix} 1  0 \\end{pmatrix}}{1} = \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix}$.\nThe second update term (the addition):\n$\\frac{y_k y_k^T}{y_k^T s_k} = \\frac{\\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix} \\begin{pmatrix} -1  1 \\end{pmatrix}}{-1} = -\\begin{pmatrix} 1  -1 \\\\ -1  1 \\end{pmatrix} = \\begin{pmatrix} -1  1 \\\\ 1  -1 \\end{pmatrix}$.\n\nNow, assemble $B_{k+1}$:\n$B_{k+1} = B_k - \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix} + \\begin{pmatrix} -1  1 \\\\ 1  -1 \\end{pmatrix}$\n$B_{k+1} = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} - \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix} + \\begin{pmatrix} -1  1 \\\\ 1  -1 \\end{pmatrix} = \\begin{pmatrix} 0  0 \\\\ 0  1 \\end{pmatrix} + \\begin{pmatrix} -1  1 \\\\ 1  -1 \\end{pmatrix} = \\begin{pmatrix} -1  1 \\\\ 1  0 \\end{pmatrix}$.\n\nTo check if $B_{k+1}$ is positive definite, we can check its eigenvalues or the determinants of its leading principal minors.\nThe first leading principal minor has determinant $\\det([-1]) = -1$. Since this is not positive, the matrix $B_{k+1}$ is not positive definite.\nAlso, the determinant of the full matrix is $\\det(B_{k+1}) = (-1)(0) - (1)(1) = -1$, which also shows it is not positive definite. It is an indefinite matrix. This example also shows it is not necessarily singular (since det $\\neq 0$) or negative definite.\n\nBased on this analysis, the failure of the curvature condition ($y_k^T s_k \\le 0$) means that the resulting matrix $B_{k+1}$ is not guaranteed to be positive definite. It can be indefinite, negative definite, or even singular, depending on the specific values of $B_k$, $s_k$, and $y_k$. Therefore, the primary and most certain consequence is the loss of the guarantee of positive definiteness.\n\nLet's review the options:\nA. The updated Hessian approximation $B_{k+1}$ is not guaranteed to be positive definite. - This is correct, as shown by our analysis and counterexample.\nB. The algorithm immediately terminates because the update formula for $B_{k+1}$ will always involve division by zero. - Incorrect. This only happens if $y_k^T s_k = 0$, not if $y_k^T s_k  0$.\nC. The updated Hessian approximation $B_{k+1}$ is guaranteed to remain positive definite... - Incorrect. The core mechanism for preserving positive definiteness is broken.\nD. The updated Hessian approximation $B_{k+1}$ will become singular. - Incorrect. Our counterexample resulted in a non-singular matrix. Singularity is possible but not guaranteed.\nE. The updated Hessian approximation $B_{k+1}$ will always be negative definite. - Incorrect. Our counterexample resulted in an indefinite matrix.\n\nThus, the correct choice is A.", "answer": "$$\\boxed{A}$$", "id": "2195904"}]}