{"hands_on_practices": [{"introduction": "The very first step in any steepest descent iteration is to determine the most promising direction for minimization. This exercise grounds you in that fundamental concept by asking you to calculate the direction of the most rapid decrease from a starting point. By working with the famous Rosenbrock function, a standard benchmark in optimization known for its challenging, narrow valley, you will practice the essential skill of computing the gradient to find this initial search direction [@problem_id:2221567].", "problem": "In the field of numerical optimization, the performance of new algorithms is often benchmarked using a set of standard test functions. One such function is the Rosenbrock function, which is challenging to minimize due to its narrow, parabolic valley.\n\nConsider a two-dimensional version of the Rosenbrock function given by\n$$f(x,y) = (a-x)^2 + b(y-x^2)^2$$\nwhere $a$ and $b$ are positive real constants.\n\nAn iterative minimization algorithm is initiated at the point $(x_0, y_0) = (0, 0)$. The first step of this algorithm involves determining the initial search direction. This direction is defined as the vector along which the function's value decreases most rapidly from the starting point.\n\nDetermine this initial search direction vector, $\\mathbf{d}_0$. Express your answer as a column vector in terms of the constants $a$ and $b$.", "solution": "The direction along which $f$ decreases most rapidly at a point is the negative gradient, so the initial search direction from $(x_{0},y_{0})=(0,0)$ is $\\mathbf{d}_{0}=-\\nabla f(0,0)$.\n\nCompute the gradient of $f(x,y)=(a-x)^{2}+b(y-x^{2})^{2}$:\n- The partial derivative with respect to $x$ is\n$$\n\\frac{\\partial f}{\\partial x}=2(x-a)-4bx(y-x^{2}).\n$$\n- The partial derivative with respect to $y$ is\n$$\n\\frac{\\partial f}{\\partial y}=2b(y-x^{2}).\n$$\n\nEvaluate these at $(0,0)$:\n$$\n\\frac{\\partial f}{\\partial x}(0,0)=-2a,\\qquad \\frac{\\partial f}{\\partial y}(0,0)=0.\n$$\nHence,\n$$\n\\nabla f(0,0)=\\begin{pmatrix}-2a\\\\0\\end{pmatrix},\n\\quad\n\\mathbf{d}_{0}=-\\nabla f(0,0)=\\begin{pmatrix}2a\\\\0\\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}2a\\\\0\\end{pmatrix}}$$", "id": "2221567"}, {"introduction": "Once a direction of descent is established, the next critical question is how far to travel along it. An 'exact line search' provides the answer by finding the precise step size, $\\alpha$, that minimizes the function along that specific path. This practice isolates this crucial sub-problem, transforming a multi-dimensional optimization into a more familiar single-variable calculus problem of finding the minimum of a function $\\phi(\\alpha)$ [@problem_id:2170908].", "problem": "In the field of numerical optimization, the steepest descent method is a fundamental algorithm for finding a local minimum of a function. A crucial part of this method is the line search, which determines how far to move along the search direction in each iteration.\n\nConsider the task of minimizing the quadratic function $f(x_1, x_2) = 2x_1^2 + x_2^2 + x_1 x_2$. The first step of the steepest descent algorithm starts at an initial point $\\mathbf{x}_0 = (3, 0)$ and moves along the direction of the negative gradient evaluated at that point, which we denote as $\\mathbf{p}_0 = -\\nabla f(\\mathbf{x}_0)$.\n\nAn exact line search is performed to find the optimal step size $\\alpha > 0$ that minimizes the function along this direction. This involves finding the value of $\\alpha$ that minimizes the one-dimensional function $\\phi(\\alpha) = f(\\mathbf{x}_0 + \\alpha \\mathbf{p}_0)$.\n\nYour task is to determine this optimal step size $\\alpha$. Express your answer as an exact fraction in its simplest form.", "solution": "We minimize $f(x_{1},x_{2})=2x_{1}^{2}+x_{2}^{2}+x_{1}x_{2}$ using steepest descent with exact line search starting at $\\mathbf{x}_{0}=(3,0)$. The gradient is\n$$\n\\nabla f(x_{1},x_{2})=\\begin{pmatrix}4x_{1}+x_{2}\\\\ x_{1}+2x_{2}\\end{pmatrix}.\n$$\nEvaluated at $\\mathbf{x}_{0}$, this gives\n$$\n\\mathbf{g}_{0}=\\nabla f(\\mathbf{x}_{0})=\\begin{pmatrix}4\\cdot 3+0\\\\ 3+2\\cdot 0\\end{pmatrix}=\\begin{pmatrix}12\\\\ 3\\end{pmatrix},\n$$\nso the steepest descent direction is $\\mathbf{p}_{0}=-\\mathbf{g}_{0}=\\begin{pmatrix}-12\\\\ -3\\end{pmatrix}$.\n\nDefine the line $x(\\alpha)=\\mathbf{x}_{0}+\\alpha \\mathbf{p}_{0}=(3-12\\alpha,-3\\alpha)$ and the one-dimensional function $\\phi(\\alpha)=f(x(\\alpha))$. Then\n$$\n\\phi(\\alpha)=2(3-12\\alpha)^{2}+(-3\\alpha)^{2}+(3-12\\alpha)(-3\\alpha).\n$$\nExpand each term:\n$$\n(3-12\\alpha)^{2}=9-72\\alpha+144\\alpha^{2},\\quad 2(3-12\\alpha)^{2}=18-144\\alpha+288\\alpha^{2},\n$$\n$$\n(-3\\alpha)^{2}=9\\alpha^{2},\\quad (3-12\\alpha)(-3\\alpha)=-9\\alpha+36\\alpha^{2}.\n$$\nSumming,\n$$\n\\phi(\\alpha)=18-144\\alpha+288\\alpha^{2}+9\\alpha^{2}-9\\alpha+36\\alpha^{2}\n=18-153\\alpha+333\\alpha^{2}.\n$$\nFor exact line search, set the derivative to zero:\n$$\n\\phi'(\\alpha)=-153+666\\alpha=0 \\quad \\Rightarrow \\quad \\alpha=\\frac{153}{666}.\n$$\nSimplify the fraction:\n$$\n\\frac{153}{666}=\\frac{51}{222}=\\frac{17}{74}.\n$$\nSince $\\phi''(\\alpha)=666>0$, this $\\alpha$ indeed minimizes $\\phi$.", "answer": "$$\\boxed{\\frac{17}{74}}$$", "id": "2170908"}, {"introduction": "Now, let's combine the concepts of finding the direction and determining the step size to execute full iterations of the steepest descent algorithm. By calculating the first two steps for a quadratic function with elliptical level sets, you will not only practice the complete procedure but also observe the algorithm's characteristic 'zigzagging' convergence pattern. This behavior, which you will compute directly, provides key insight into the performance and convergence properties of the steepest descent method [@problem_id:2221576].", "problem": "Consider the unconstrained optimization problem of minimizing the function $f(x, y) = 10x^2 + y^2$. The optimization process is initiated at the point $\\mathbf{x}_0 = (x_0, y_0) = (1, 1)$.\n\nThe steepest descent algorithm is to be used. For each iteration $k$, the step size, denoted by $\\alpha_k > 0$, is determined via an exact line search. This means that for a given point $\\mathbf{x}_k$ and descent direction $\\mathbf{p}_k$, the step size $\\alpha_k$ is chosen to find the global minimum of the single-variable function $g(\\alpha) = f(\\mathbf{x}_k + \\alpha \\mathbf{p}_k)$.\n\nCalculate the coordinates of the point $\\mathbf{x}_2 = (x_2, y_2)$ after two complete iterations of this method. The coordinates in your final answer should be expressed as exact fractions in their simplest form.", "solution": "We minimize $f(x,y)=10x^{2}+y^{2}$ starting at $\\mathbf{x}_{0}=(1,1)$ using steepest descent with exact line search. The gradient and Hessian are\n$$\n\\nabla f(x,y)=\\begin{pmatrix}20x\\\\2y\\end{pmatrix},\\qquad H=\\begin{pmatrix}20&0\\\\0&2\\end{pmatrix}.\n$$\nAt iteration $k$, with $\\mathbf{g}_{k}=\\nabla f(\\mathbf{x}_{k})$ and direction $\\mathbf{p}_{k}=-\\mathbf{g}_{k}$, the exact line search minimizes $g(\\alpha)=f(\\mathbf{x}_{k}+\\alpha\\mathbf{p}_{k})$. Since $H$ is constant (quadratic function), the derivative is\n$$\ng'(\\alpha)=\\nabla f(\\mathbf{x}_{k}-\\alpha \\mathbf{g}_{k})^{\\mathsf{T}}(-\\mathbf{g}_{k})=\\left(\\mathbf{g}_{k}-\\alpha H\\mathbf{g}_{k}\\right)^{\\mathsf{T}}(-\\mathbf{g}_{k})=-(\\mathbf{g}_{k}^{\\mathsf{T}}\\mathbf{g}_{k})+\\alpha\\,\\mathbf{g}_{k}^{\\mathsf{T}}H\\mathbf{g}_{k}.\n$$\nSetting $g'(\\alpha)=0$ yields the exact step size\n$$\n\\alpha_{k}=\\frac{\\mathbf{g}_{k}^{\\mathsf{T}}\\mathbf{g}_{k}}{\\mathbf{g}_{k}^{\\mathsf{T}}H\\mathbf{g}_{k}}.\n$$\n\nIteration $0$: $\\mathbf{x}_{0}=(1,1)$ gives\n$$\n\\mathbf{g}_{0}=\\begin{pmatrix}20\\\\2\\end{pmatrix},\\quad \\mathbf{g}_{0}^{\\mathsf{T}}\\mathbf{g}_{0}=404,\\quad H\\mathbf{g}_{0}=\\begin{pmatrix}400\\\\4\\end{pmatrix},\\quad \\mathbf{g}_{0}^{\\mathsf{T}}H\\mathbf{g}_{0}=8008,\n$$\nso\n$$\n\\alpha_{0}=\\frac{404}{8008}=\\frac{101}{2002}.\n$$\nThen\n$$\n\\mathbf{x}_{1}=\\mathbf{x}_{0}-\\alpha_{0}\\mathbf{g}_{0}=\\begin{pmatrix}1-20\\alpha_{0}\\\\1-2\\alpha_{0}\\end{pmatrix}=\\begin{pmatrix}1-\\frac{1010}{1001}\\\\1-\\frac{101}{1001}\\end{pmatrix}=\\begin{pmatrix}-\\frac{9}{1001}\\\\\\frac{900}{1001}\\end{pmatrix}.\n$$\n\nIteration $1$: $\\mathbf{x}_{1}=\\left(-\\frac{9}{1001},\\,\\frac{900}{1001}\\right)$ gives\n$$\n\\mathbf{g}_{1}=\\begin{pmatrix}20x_{1}\\\\2y_{1}\\end{pmatrix}=\\begin{pmatrix}-\\frac{180}{1001}\\\\\\frac{1800}{1001}\\end{pmatrix}.\n$$\nCompute\n$$\n\\mathbf{g}_{1}^{\\mathsf{T}}\\mathbf{g}_{1}=\\frac{180^{2}+1800^{2}}{1001^{2}}=\\frac{180^{2}\\cdot 101}{1001^{2}},\\qquad\nH\\mathbf{g}_{1}=\\begin{pmatrix}-\\frac{3600}{1001}\\\\\\frac{3600}{1001}\\end{pmatrix},\\qquad\n\\mathbf{g}_{1}^{\\mathsf{T}}H\\mathbf{g}_{1}=\\frac{180\\cdot 3600}{1001^{2}}(1+10)=\\frac{7{,}128{,}000}{1001^{2}}.\n$$\nThus\n$$\n\\alpha_{1}=\\frac{\\mathbf{g}_{1}^{\\mathsf{T}}\\mathbf{g}_{1}}{\\mathbf{g}_{1}^{\\mathsf{T}}H\\mathbf{g}_{1}}=\\frac{3{,}272{,}400}{7{,}128{,}000}=\\frac{101}{220}.\n$$\nUpdate\n$$\n\\mathbf{x}_{2}=\\mathbf{x}_{1}-\\alpha_{1}\\mathbf{g}_{1}=\\begin{pmatrix}-\\frac{9}{1001}-\\frac{101}{220}\\left(-\\frac{180}{1001}\\right)\\\\\\frac{900}{1001}-\\frac{101}{220}\\left(\\frac{1800}{1001}\\right)\\end{pmatrix}=\\begin{pmatrix}-\\frac{9}{1001}+\\frac{909}{11011}\\\\\\frac{900}{1001}-\\frac{9090}{11011}\\end{pmatrix}=\\begin{pmatrix}\\frac{810}{11011}\\\\\\frac{810}{11011}\\end{pmatrix}.\n$$\nThe fractions are already in simplest terms because $\\gcd(810,11011)=1$.", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{810}{11011} \\\\ \\frac{810}{11011} \\end{pmatrix}}$$", "id": "2221576"}]}