{"hands_on_practices": [{"introduction": "Before applying the full DFP update, it is essential to master the calculation of its core components. This exercise focuses on computing the step vector, $s_k$, which represents the change in our position, and the gradient difference vector, $y_k$, which represents the change in the function's slope. Mastering the construction of these two vectors from basic iterative data is the foundational first step in implementing any quasi-Newton method [@problem_id:2212535].", "problem": "In the context of quasi-Newton optimization methods, such as the Davidon-Fletcher-Powell (DFP) algorithm, the approximation of the Hessian matrix is updated at each iteration. This update relies on two key vectors, denoted as $s_k$ and $y_k$. The vector $s_k$ represents the step taken in the variable space, and the vector $y_k$ represents the corresponding change in the gradient.\n\nConsider the unconstrained optimization of the function $f(x_1, x_2) = 2x_1^2 + x_2^2 + x_1 x_2$.\nSuppose we are at the $k$-th iteration, with the current point being $x_k = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$. The next point in the sequence, found by some line search procedure, is $x_{k+1} = \\begin{pmatrix} 0.5 \\\\ 1 \\end{pmatrix}$.\n\nThe vectors required for the DFP update are defined as $s_k = x_{k+1} - x_k$ and $y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$.\n\nWhich of the following options correctly represents the vectors $s_k$ and $y_k$ for this iteration?\n\nA. $s_k = \\begin{pmatrix} -0.5 \\\\ 1 \\end{pmatrix}$, $y_k = \\begin{pmatrix} -1 \\\\ 1.5 \\end{pmatrix}$\n\nB. $s_k = \\begin{pmatrix} 0.5 \\\\ -1 \\end{pmatrix}$, $y_k = \\begin{pmatrix} 1 \\\\ -1.5 \\end{pmatrix}$\n\nC. $s_k = \\begin{pmatrix} 1.5 \\\\ 1 \\end{pmatrix}$, $y_k = \\begin{pmatrix} 7 \\\\ 3.5 \\end{pmatrix}$\n\nD. $s_k = \\begin{pmatrix} -1 \\\\ 1.5 \\end{pmatrix}$, $y_k = \\begin{pmatrix} -0.5 \\\\ 1 \\end{pmatrix}$\n\nE. $s_k = \\begin{pmatrix} -0.5 \\\\ 1 \\end{pmatrix}$, $y_k = \\begin{pmatrix} 7 \\\\ 3.5 \\end{pmatrix}$", "solution": "We are given the function $f(x_{1},x_{2})=2x_{1}^{2}+x_{2}^{2}+x_{1}x_{2}$. The gradient is obtained by differentiating componentwise:\n$$\n\\nabla f(x_{1},x_{2})=\\begin{pmatrix}\\frac{\\partial f}{\\partial x_{1}}\\\\[4pt]\\frac{\\partial f}{\\partial x_{2}}\\end{pmatrix}\n=\\begin{pmatrix}4x_{1}+x_{2}\\\\[4pt]2x_{2}+x_{1}\\end{pmatrix}.\n$$\nBy definition, the displacement vector is $s_{k}=x_{k+1}-x_{k}$. With $x_{k}=\\begin{pmatrix}1\\\\[2pt]0\\end{pmatrix}$ and $x_{k+1}=\\begin{pmatrix}\\tfrac{1}{2}\\\\[2pt]1\\end{pmatrix}$, we get\n$$\ns_{k}=\\begin{pmatrix}\\tfrac{1}{2}-1\\\\[4pt]1-0\\end{pmatrix}=\\begin{pmatrix}-\\tfrac{1}{2}\\\\[4pt]1\\end{pmatrix}.\n$$\nNext, the gradient change is $y_{k}=\\nabla f(x_{k+1})-\\nabla f(x_{k})$. Evaluate the gradient at each point:\n$$\n\\nabla f(x_{k})=\\nabla f(1,0)=\\begin{pmatrix}4\\cdot 1+0\\\\[4pt]2\\cdot 0+1\\end{pmatrix}=\\begin{pmatrix}4\\\\[4pt]1\\end{pmatrix},\n$$\n$$\n\\nabla f(x_{k+1})=\\nabla f\\!\\left(\\tfrac{1}{2},1\\right)=\\begin{pmatrix}4\\cdot \\tfrac{1}{2}+1\\\\[4pt]2\\cdot 1+\\tfrac{1}{2}\\end{pmatrix}=\\begin{pmatrix}3\\\\[4pt]\\tfrac{5}{2}\\end{pmatrix}.\n$$\nTherefore,\n$$\ny_{k}=\\begin{pmatrix}3-4\\\\[4pt]\\tfrac{5}{2}-1\\end{pmatrix}=\\begin{pmatrix}-1\\\\[4pt]\\tfrac{3}{2}\\end{pmatrix}.\n$$\nComparing with the options, this corresponds to $s_{k}=\\begin{pmatrix}-\\tfrac{1}{2}\\\\[2pt]1\\end{pmatrix}$ and $y_{k}=\\begin{pmatrix}-1\\\\[2pt]\\tfrac{3}{2}\\end{pmatrix}$, i.e., option A.", "answer": "$$\\boxed{A}$$", "id": "2212535"}, {"introduction": "With the fundamental vectors $s_k$ and $y_k$ in hand, we can now execute a full DFP update to refine our approximation of the inverse Hessian. This practice provides a concrete numerical example, starting with the identity matrix $I$ as the initial guess $H_0$â€”a standard choice in practice. By working through the matrix and vector operations, you will see exactly how the DFP formula combines current information to build a more accurate model of the function's curvature [@problem_id:2212500].", "problem": "In the field of numerical optimization, quasi-Newton methods are iterative algorithms used to find local minima or maxima of functions. A key feature of these methods is the approximation of the Hessian matrix or its inverse. One of the earliest and most famous quasi-Newton algorithms is the Davidon-Fletcher-Powell (DFP) method.\n\nThe DFP update formula for approximating the inverse of the Hessian matrix, denoted by $H$, is given by:\n$$H_{k+1} = H_k + \\frac{s_k s_k^T}{s_k^T y_k} - \\frac{H_k y_k y_k^T H_k}{y_k^T H_k y_k}$$\nwhere $k$ is the iteration index, $s_k = x_{k+1} - x_k$ is the step taken in the parameter space, and $y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$ is the change in the gradient of the objective function $f(x)$.\n\nConsider the first step ($k=0$) of an optimization process. The initial approximation of the inverse Hessian matrix is set to the identity matrix, $H_0 = I$. After the first line search, the step vector $s_0$ and the change-in-gradient vector $y_0$ are computed to be:\n$$s_0 = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}, \\quad y_0 = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}$$\n\nYour task is to calculate the updated inverse Hessian approximation, $H_1$, after this first step. Express your answer as a matrix with fractional entries.", "solution": "We use the DFP update for the inverse Hessian approximation:\n$$H_{k+1} = H_{k} + \\frac{s_{k}s_{k}^{T}}{s_{k}^{T}y_{k}} - \\frac{H_{k}y_{k}y_{k}^{T}H_{k}}{y_{k}^{T}H_{k}y_{k}}.$$\nAt the first step with $k=0$, $H_{0}=I$, $s_{0}=\\begin{pmatrix}1 \\\\ -2\\end{pmatrix}$, and $y_{0}=\\begin{pmatrix}2 \\\\ -1\\end{pmatrix}$. Since $H_{0}=I$, we have $H_{0}y_{0}=y_{0}$ and $y_{0}^{T}H_{0}y_{0}=y_{0}^{T}y_{0}$. Therefore,\n$$H_{1} = I + \\frac{s_{0}s_{0}^{T}}{s_{0}^{T}y_{0}} - \\frac{y_{0}y_{0}^{T}}{y_{0}^{T}y_{0}}.$$\nCompute the required inner products:\n$$s_{0}^{T}y_{0} = 1\\cdot 2 + (-2)\\cdot(-1) = 4,$$\n$$y_{0}^{T}y_{0} = 2^{2} + (-1)^{2} = 5.$$\nCompute the outer products:\n$$s_{0}s_{0}^{T} = \\begin{pmatrix}1  -2 \\\\ -2  4\\end{pmatrix}, \\quad y_{0}y_{0}^{T} = \\begin{pmatrix}4  -2 \\\\ -2  1\\end{pmatrix}.$$\nThus,\n$$\\frac{s_{0}s_{0}^{T}}{s_{0}^{T}y_{0}} = \\frac{1}{4}\\begin{pmatrix}1  -2 \\\\ -2  4\\end{pmatrix} = \\begin{pmatrix}\\frac{1}{4}  -\\frac{1}{2} \\\\ -\\frac{1}{2}  1\\end{pmatrix}, \\quad \\frac{y_{0}y_{0}^{T}}{y_{0}^{T}y_{0}} = \\frac{1}{5}\\begin{pmatrix}4  -2 \\\\ -2  1\\end{pmatrix} = \\begin{pmatrix}\\frac{4}{5}  -\\frac{2}{5} \\\\ -\\frac{2}{5}  \\frac{1}{5}\\end{pmatrix}.$$\nCombine terms:\n$$H_{1} = I + \\begin{pmatrix}\\frac{1}{4}  -\\frac{1}{2} \\\\ -\\frac{1}{2}  1\\end{pmatrix} - \\begin{pmatrix}\\frac{4}{5}  -\\frac{2}{5} \\\\ -\\frac{2}{5}  \\frac{1}{5}\\end{pmatrix}.$$\nCompute each entry:\n$$(1,1): 1 + \\frac{1}{4} - \\frac{4}{5} = \\frac{5}{4} - \\frac{4}{5} = \\frac{25 - 16}{20} = \\frac{9}{20},$$\n$$(1,2) \\text{ and } (2,1): 0 - \\frac{1}{2} - \\left(-\\frac{2}{5}\\right) = -\\frac{1}{2} + \\frac{2}{5} = -\\frac{1}{10},$$\n$$(2,2): 1 + 1 - \\frac{1}{5} = 2 - \\frac{1}{5} = \\frac{9}{5}.$$\nTherefore,\n$$H_{1} = \\begin{pmatrix}\\frac{9}{20}  -\\frac{1}{10} \\\\ -\\frac{1}{10}  \\frac{9}{5}\\end{pmatrix}.$$\nAs a check, the secant condition $H_{1}y_{0}=s_{0}$ holds:\n$$\\begin{pmatrix}\\frac{9}{20}  -\\frac{1}{10} \\\\ -\\frac{1}{10}  \\frac{9}{5}\\end{pmatrix}\\begin{pmatrix}2 \\\\ -1\\end{pmatrix} = \\begin{pmatrix}1 \\\\ -2\\end{pmatrix} = s_{0}.$$", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{9}{20}  -\\frac{1}{10} \\\\ -\\frac{1}{10}  \\frac{9}{5}\\end{pmatrix}}$$", "id": "2212500"}, {"introduction": "To build deeper intuition, it's often helpful to see how a complex formula behaves in a simple case. This exercise explores the DFP update for a function of a single variable, where all matrices and vectors become scalars. You will discover that the sophisticated DFP formula elegantly simplifies to the update rule for the classic secant method, revealing the DFP method's core identity as a multi-dimensional generalization of this fundamental root-finding algorithm [@problem_id:2212502].", "problem": "In numerical optimization, quasi-Newton methods are an important class of algorithms for finding a local minimum of a function. These methods iteratively build an approximation to the Hessian matrix of the function. The Davidon-Fletcher-Powell (DFP) method provides one such update rule. While often expressed as an update for the inverse of the Hessian, the DFP update can also be formulated directly for the Hessian approximation matrix, $B_k$, at iteration $k$. This formula is:\n$$B_{k+1} = \\left(I - \\frac{y_k s_k^T}{y_k^T s_k}\\right) B_k \\left(I - \\frac{s_k y_k^T}{y_k^T s_k}\\right) + \\frac{y_k y_k^T}{y_k^T s_k}$$\nHere, $I$ is the identity matrix. The vector $s_k = x_{k+1} - x_k$ is the step taken in the variable space, and the vector $y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$ is the corresponding change in the gradient of the function $f$ being minimized.\n\nConsider the application of the DFP method to a function of a single variable, $f(x)$, where $x$ is a scalar. In this one-dimensional case, all vectors become scalars, and matrix operations like multiplication and transpose become simple scalar multiplication. The Hessian approximation $B_k$ becomes a scalar value, which we shall denote $b_k$.\n\nYour task is to determine the simplified expression for the updated scalar Hessian approximation, $b_{k+1}$. Your result should be a single, closed-form analytic expression in terms of the scalar quantities $s_k$ and $y_k$.", "solution": "We begin with the provided matrix DFP-style update written for the Hessian approximation:\n$$\nB_{k+1} = \\left(I - \\frac{y_k s_k^{T}}{y_k^{T} s_k}\\right) B_k \\left(I - \\frac{s_k y_k^{T}}{y_k^{T} s_k}\\right) + \\frac{y_k y_k^{T}}{y_k^{T} s_k}.\n$$\nIn one dimension, all quantities are scalars. Thus $I$ becomes $1$, $B_k$ becomes the scalar $b_k$, and transposes have no effect. The step $s_k$ and gradient change $y_k$ are scalars, and $y_k^{T} s_k = y_k s_k$. Therefore, the update reduces to\n$$\nb_{k+1} = \\left(1 - \\frac{y_k s_k}{y_k s_k}\\right) b_k \\left(1 - \\frac{s_k y_k}{y_k s_k}\\right) + \\frac{y_k y_k}{y_k s_k}.\n$$\nAssuming the standard curvature condition $y_k s_k \\neq 0$, we simplify the scalar fractions:\n$$\n\\frac{y_k s_k}{y_k s_k} = 1, \\quad \\frac{s_k y_k}{y_k s_k} = 1,\n$$\nso the first term vanishes:\n$$\n\\left(1 - 1\\right) b_k \\left(1 - 1\\right) = 0.\n$$\nFor the remaining term,\n$$\n\\frac{y_k y_k}{y_k s_k} = \\frac{y_k^{2}}{y_k s_k} = \\frac{y_k}{s_k}.\n$$\nHence the updated scalar Hessian approximation is\n$$\nb_{k+1} = \\frac{y_k}{s_k}.\n$$\nThis also coincides with the one-dimensional secant condition $b_{k+1} s_k = y_k$.", "answer": "$$\\boxed{\\frac{y_{k}}{s_{k}}}$$", "id": "2212502"}]}