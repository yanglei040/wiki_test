{"hands_on_practices": [{"introduction": "This first practice exercise focuses on the fundamental mechanics of Newton's method. By providing the gradient and Hessian matrix directly, we isolate the core computational step of calculating the Newton update. This allows you to master the update rule, $\\mathbf{x}_{k+1} = \\mathbf{x}_k - [H(\\mathbf{x}_k)]^{-1} \\nabla f(\\mathbf{x}_k)$, which forms the heart of the algorithm [@problem_id:2190699].", "problem": "An optimization algorithm is being used to find the minimum of a function $f(\\mathbf{x})$, where $\\mathbf{x}$ is a two-dimensional vector. The algorithm is currently at the point $\\mathbf{x}_k = [3, -4]^T$. At this point, the gradient of the function has been computed as $\\nabla f(\\mathbf{x}_k) = [2, -1]^T$, and the Hessian matrix is $H(\\mathbf{x}_k) = \\begin{pmatrix} 5  1 \\\\ 1  1 \\end{pmatrix}$. Determine the next point, $\\mathbf{x}_{k+1}$, by applying a single full step of Newton's method. Express your final answer as a row vector $[x, y]$ where the components are given as exact fractions.", "solution": "The core of Newton's method for unconstrained optimization is to iteratively find the minimum of a function by creating a quadratic approximation at the current point and then moving to the minimum of that approximation. The update rule for a full step of Newton's method is given by:\n$$\n\\mathbf{x}_{k+1} = \\mathbf{x}_k - [H(\\mathbf{x}_k)]^{-1} \\nabla f(\\mathbf{x}_k)\n$$\nwhere $\\mathbf{x}_k$ is the current point, $\\nabla f(\\mathbf{x}_k)$ is the gradient of the function at that point, and $H(\\mathbf{x}_k)$ is the Hessian matrix at that point.\n\nThe problem provides the following information:\n- The current iterate: $\\mathbf{x}_k = \\begin{pmatrix} 3 \\\\ -4 \\end{pmatrix}$\n- The gradient at $\\mathbf{x}_k$: $\\nabla f(\\mathbf{x}_k) = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}$\n- The Hessian at $\\mathbf{x}_k$: $H(\\mathbf{x}_k) = \\begin{pmatrix} 5  1 \\\\ 1  1 \\end{pmatrix}$\n\nFirst, we need to compute the inverse of the Hessian matrix, $H(\\mathbf{x}_k)^{-1}$. For a general 2x2 matrix $A = \\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix}$, the inverse is given by the formula:\n$$\nA^{-1} = \\frac{1}{ad-bc} \\begin{pmatrix} d  -b \\\\ -c  a \\end{pmatrix}\n$$\nThe term $ad-bc$ is the determinant of the matrix. Let's calculate the determinant of $H(\\mathbf{x}_k)$:\n$$\n\\det(H(\\mathbf{x}_k)) = (5)(1) - (1)(1) = 5 - 1 = 4\n$$\nNow, we can find the inverse of the Hessian:\n$$\nH(\\mathbf{x}_k)^{-1} = \\frac{1}{4} \\begin{pmatrix} 1  -1 \\\\ -1  5 \\end{pmatrix} = \\begin{pmatrix} 1/4  -1/4 \\\\ -1/4  5/4 \\end{pmatrix}\n$$\nNext, we compute the product of the inverse Hessian and the gradient, which is often called the Newton step direction, $\\mathbf{p}_k = -H(\\mathbf{x}_k)^{-1} \\nabla f(\\mathbf{x}_k)$.\n$$\nH(\\mathbf{x}_k)^{-1} \\nabla f(\\mathbf{x}_k) = \\begin{pmatrix} 1/4  -1/4 \\\\ -1/4  5/4 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}\n$$\nPerforming the matrix-vector multiplication:\n$$\nH(\\mathbf{x}_k)^{-1} \\nabla f(\\mathbf{x}_k) = \\begin{pmatrix} (1/4)(2) + (-1/4)(-1) \\\\ (-1/4)(2) + (5/4)(-1) \\end{pmatrix} = \\begin{pmatrix} 2/4 + 1/4 \\\\ -2/4 - 5/4 \\end{pmatrix} = \\begin{pmatrix} 3/4 \\\\ -7/4 \\end{pmatrix}\n$$\nNow we can update the point $\\mathbf{x}_k$ to find $\\mathbf{x}_{k+1}$:\n$$\n\\mathbf{x}_{k+1} = \\mathbf{x}_k - H(\\mathbf{x}_k)^{-1} \\nabla f(\\mathbf{x}_k) = \\begin{pmatrix} 3 \\\\ -4 \\end{pmatrix} - \\begin{pmatrix} 3/4 \\\\ -7/4 \\end{pmatrix}\n$$\nPerforming the vector subtraction component-wise:\n$$\n\\mathbf{x}_{k+1} = \\begin{pmatrix} 3 - 3/4 \\\\ -4 - (-7/4) \\end{pmatrix} = \\begin{pmatrix} 12/4 - 3/4 \\\\ -16/4 + 7/4 \\end{pmatrix} = \\begin{pmatrix} 9/4 \\\\ -9/4 \\end{pmatrix}\n$$\nThe problem asks for the answer to be expressed as a row vector $[x, y]$ with components as exact fractions.\nThus, the next point is $\\mathbf{x}_{k+1} = [9/4, -9/4]$.", "answer": "$$\\boxed{[9/4, -9/4]}$$", "id": "2190699"}, {"introduction": "Moving beyond the basic mechanics, this problem requires you to perform a complete iteration of Newton's method from the beginning. You will start with a non-trivial objective function, calculate its gradient vector and Hessian matrix, and then apply the update rule you practiced previously. This exercise mirrors a real-world scenario where you must apply the full algorithm to a given cost function [@problem_id:2190727].", "problem": "A team of engineers is working to minimize the operational cost of a new robotic arm. The cost is modeled by a function $f(x, y)$ of two dimensionless design parameters $x$ and $y$. The function is given by:\n$$f(x, y) = (y - x^2)^2 + (1-x)^2$$\nTo find the optimal parameters that minimize this cost, the team decides to use Newton's method for unconstrained optimization. They start from an initial design guess of $(x_0, y_0) = (2, 3)$.\n\nCalculate the coordinates of the next iterate, $(x_1, y_1)$, that results from applying one step of Newton's method. Express your answer as an ordered pair of exact fractions.", "solution": "For unconstrained optimization in two variables, one step of Newton's method updates the iterate $\\mathbf{z} = (x, y)$ by\n$$\n\\mathbf{z}_{k+1} = \\mathbf{z}_{k} - H(\\mathbf{z}_{k})^{-1} \\nabla f(\\mathbf{z}_{k}),\n$$\nwhere $\\nabla f$ is the gradient and $H$ is the Hessian of $f$.\n\nGiven $f(x, y) = (y - x^{2})^{2} + (1 - x)^{2}$, compute the gradient:\n$$\n\\frac{\\partial f}{\\partial x} = 2(y - x^{2})(-2x) + 2(1 - x)(-1) = -4xy + 4x^{3} - 2 + 2x,\n$$\n$$\n\\frac{\\partial f}{\\partial y} = 2(y - x^{2}) = 2y - 2x^{2}.\n$$\nThus,\n$$\n\\nabla f(x, y) = \\begin{pmatrix} -4xy + 4x^{3} - 2 + 2x \\\\ 2y - 2x^{2} \\end{pmatrix}.\n$$\n\nCompute the Hessian:\n$$\n\\frac{\\partial^{2} f}{\\partial x^{2}} = \\frac{\\partial}{\\partial x}\\left(-4xy + 4x^{3} - 2 + 2x\\right) = -4y + 12x^{2} + 2,\n$$\n$$\n\\frac{\\partial^{2} f}{\\partial x \\partial y} = \\frac{\\partial}{\\partial y}\\left(-4xy + 4x^{3} - 2 + 2x\\right) = -4x,\n$$\n$$\n\\frac{\\partial^{2} f}{\\partial y^{2}} = \\frac{\\partial}{\\partial y}\\left(2y - 2x^{2}\\right) = 2.\n$$\nTherefore,\n$$\nH(x, y) = \\begin{pmatrix} 12x^{2} - 4y + 2  -4x \\\\ -4x  2 \\end{pmatrix}.\n$$\n\nEvaluate at $(x_{0}, y_{0}) = (2, 3)$:\n$$\n\\nabla f(2, 3) = \\begin{pmatrix} -4\\cdot 2 \\cdot 3 + 4\\cdot 2^{3} - 2 + 2\\cdot 2 \\\\ 2\\cdot 3 - 2\\cdot 2^{2} \\end{pmatrix} = \\begin{pmatrix} 10 \\\\ -2 \\end{pmatrix},\n$$\n$$\nH(2, 3) = \\begin{pmatrix} 12\\cdot 2^{2} - 4\\cdot 3 + 2  -4\\cdot 2 \\\\ -4\\cdot 2  2 \\end{pmatrix} = \\begin{pmatrix} 38  -8 \\\\ -8  2 \\end{pmatrix}.\n$$\n\nThe Newton step $\\mathbf{s}$ solves $H(2, 3)\\,\\mathbf{s} = -\\nabla f(2, 3)$:\n$$\n\\begin{pmatrix} 38  -8 \\\\ -8  2 \\end{pmatrix} \\begin{pmatrix} s_{1} \\\\ s_{2} \\end{pmatrix} = \\begin{pmatrix} -10 \\\\ 2 \\end{pmatrix}.\n$$\nFrom the system\n$$\n38 s_{1} - 8 s_{2} = -10, \\quad -8 s_{1} + 2 s_{2} = 2,\n$$\nmultiply the second equation by $4$ to get $-32 s_{1} + 8 s_{2} = 8$ and add to the first to obtain $6 s_{1} = -2$, hence $s_{1} = -\\frac{1}{3}$. Substituting into $-8 s_{1} + 2 s_{2} = 2$ gives $8/3 + 2 s_{2} = 2$, so $2 s_{2} = -2/3$ and $s_{2} = -\\frac{1}{3}$.\n\nThus,\n$$\n(x_{1}, y_{1}) = (x_{0}, y_{0}) + \\mathbf{s} = \\left(2 - \\frac{1}{3},\\, 3 - \\frac{1}{3}\\right) = \\left(\\frac{5}{3},\\, \\frac{8}{3}\\right).\n$$", "answer": "$$\\boxed{(5/3, 8/3)}$$", "id": "2190727"}, {"introduction": "An essential part of using any numerical algorithm is understanding its limitations. This exercise explores a critical failure condition of Newton's method, which occurs when the second derivative (or the Hessian matrix in higher dimensions) becomes zero (or singular). Such a scenario makes the Newton step undefined, and this practice will guide you to analytically determine the specific starting points where this failure occurs [@problem_id:2190705].", "problem": "A classic algorithm for unconstrained optimization is Newton's method. Consider the problem of minimizing the one-dimensional objective function $f(x) = (x^2 - A)^2$, where $A$ is a given positive real constant. The iterative scheme of Newton's method starts from an initial guess $x_0$ and generates a sequence of points intended to converge to a local minimum. However, for certain choices of $x_0$, the algorithm can fail at the very first step because the update cannot be computed. Determine the positive value of the initial guess, $x_0$, for which this failure occurs. Your answer should be a symbolic expression in terms of $A$.", "solution": "Newton's method for unconstrained minimization in one dimension updates according to\n$$\nx_{k+1} = x_{k} - \\frac{f'(x_{k})}{f''(x_{k})}.\n$$\nThe update cannot be computed at the first step if and only if the denominator is zero, i.e., when $f''(x_{0}) = 0$.\n\nGiven $f(x) = (x^{2} - A)^{2}$ with $A  0$, compute the first and second derivatives:\n$$\nf'(x) = 2(x^{2} - A)\\cdot 2x = 4x(x^{2} - A),\n$$\n$$\nf''(x) = 4(x^{2} - A) + 8x^{2} = 12x^{2} - 4A = 4(3x^{2} - A).\n$$\nSet $f''(x_{0}) = 0$ to find the problematic initial guesses:\n$$\n4(3x_{0}^{2} - A) = 0 \\;\\Longrightarrow\\; 3x_{0}^{2} - A = 0 \\;\\Longrightarrow\\; x_{0}^{2} = \\frac{A}{3}.\n$$\nThus the positive initial guess that makes the Newton step undefined is\n$$\nx_{0} = \\sqrt{\\frac{A}{3}}.\n$$", "answer": "$$\\boxed{\\sqrt{\\frac{A}{3}}}$$", "id": "2190705"}]}