{"hands_on_practices": [{"introduction": "Understanding how computational cost scales with problem size is a cornerstone of numerical analysis. This first practice provides a direct and practical scenario to apply the concept of polynomial complexity. By estimating the runtime for a larger problem based on data from a smaller one, you will develop an intuition for the $O(N^3)$ cost that characterizes many direct solvers for dense linear systems, a crucial consideration in engineering and scientific computing. [@problem_id:2160731]", "problem": "An engineering student is performing a structural analysis using a custom-built Finite Element Method (FEM) software. The core of the simulation involves solving a large, dense system of linear equations, $A\\mathbf{x} = \\mathbf{b}$, where the size of the square matrix $A$ is $N \\times N$. The software uses a direct solver, and for such methods, the computational time required is known to be proportional to the cube of the matrix dimension, $N$.\n\nFor a preliminary analysis, the student runs a coarse model with $N_1 = 8,000$ degrees of freedom. The solver takes $T_1 = 10.0$ seconds to complete on her workstation. To obtain more accurate results for her final report, she must run a refined model with $N_2 = 12,000$ degrees of freedom.\n\nAssuming the computational performance is exclusively determined by this relationship and all other factors are negligible, estimate the time the solver will take for the refined model. Express your answer in seconds, rounded to three significant figures.", "solution": "The problem states that the computational time, $T$, is proportional to the cube of the matrix dimension, $N$. We can express this relationship mathematically as:\n$$T = C N^{3}$$\nwhere $C$ is a constant of proportionality that depends on the specific hardware and software implementation.\n\nWe are given two scenarios: a coarse model and a refined model. Let's denote the parameters for the coarse model with subscript 1 and for the refined model with subscript 2.\n\nFor the coarse model, we have:\n$$T_1 = C N_1^{3}$$\n\nFor the refined model, we have:\n$$T_2 = C N_2^{3}$$\n\nWe are given $T_1 = 10.0$ s, $N_1 = 8,000$, and $N_2 = 12,000$. We need to find $T_2$. To eliminate the unknown constant $C$, we can take the ratio of the two equations:\n$$\\frac{T_2}{T_1} = \\frac{C N_2^{3}}{C N_1^{3}} = \\left(\\frac{N_2}{N_1}\\right)^{3}$$\n\nNow, we can solve for $T_2$:\n$$T_2 = T_1 \\left(\\frac{N_2}{N_1}\\right)^{3}$$\n\nLet's substitute the given values into this equation. First, we compute the ratio of the dimensions:\n$$\\frac{N_2}{N_1} = \\frac{12,000}{8,000} = \\frac{12}{8} = \\frac{3}{2} = 1.5$$\n\nNow, we substitute this ratio and the value of $T_1$ into the equation for $T_2$:\n$$T_2 = 10.0 \\times (1.5)^{3}$$\n\nWe calculate the value of $(1.5)^{3}$:\n$$(1.5)^{3} = 1.5 \\times 1.5 \\times 1.5 = 2.25 \\times 1.5 = 3.375$$\n\nFinally, we compute $T_2$:\n$$T_2 = 10.0 \\times 3.375 = 33.75$$\n\nThe problem asks for the answer to be rounded to three significant figures. The calculated value is $33.75$. The first three significant figures are 3, 3, and 7. The next digit is 5, so we round up the last significant digit.\n$$T_2 \\approx 33.8$$\nThe estimated time for the refined model is 33.8 seconds.", "answer": "$$\\boxed{33.8}$$", "id": "2160731"}, {"introduction": "While the $O(N^3)$ cost for dense matrices can be daunting, many real-world applications yield matrices with special, sparse structures. This exercise demonstrates the dramatic efficiency gains possible when an algorithm is tailored to such a structure. By performing a detailed operation count for solving a tridiagonal system, you will see how we can reduce the computational complexity from cubic to linear, illustrating one of the most powerful principles in scientific computing: always exploit structure. [@problem_id:2160762]", "problem": "Consider the problem of solving a linear system of equations $A\\mathbf{x} = \\mathbf{d}$, where $A$ is a non-singular $n \\times n$ tridiagonal matrix and $\\mathbf{x}$ and $\\mathbf{d}$ are column vectors of size $n$. A tridiagonal matrix is one where the only non-zero elements are on the main diagonal, the first subdiagonal (the one directly below the main diagonal), and the first superdiagonal (the one directly above the main diagonal).\n\nA highly efficient algorithm for solving such a system consists of two stages:\n\n1.  **Forward Elimination:** This stage transforms the matrix $A$ into an upper bidiagonal matrix (non-zero elements only on the main diagonal and the superdiagonal). This is achieved by iterating from the second row ($i=2$) up to the last row ($i=n$). For each row $i$, the algorithm performs a row operation to eliminate the subdiagonal element $A_{i, i-1}$. This is done by calculating a multiplier based on the element $A_{i, i-1}$ and the (potentially modified) main diagonal element of the previous row, $A_{i-1, i-1}$, and then subtracting that multiple of row $i-1$ from row $i$. This operation modifies both the main diagonal element $A_{i,i}$ and the corresponding element $d_i$ in the right-hand side vector. The superdiagonal elements are not modified in this process.\n\n2.  **Backward Substitution:** The resulting upper bidiagonal system is then solved for the vector $\\mathbf{x}$. This is accomplished by first calculating the last component, $x_n$, and then iterating backwards from $i = n-1$ down to $1$ to find the remaining components of $\\mathbf{x}$.\n\nYour task is to determine the exact total number of fundamental arithmetic operations (i.e., additions, subtractions, multiplications, and divisions) required to solve the system using this specific algorithm. Assume that no divisions by zero are encountered.\n\nProvide your answer as a single closed-form analytic expression in terms of $n$.", "solution": "Let the tridiagonal matrix have subdiagonal entries $\\{a_{i}\\}_{i=2}^{n}$, main diagonal entries $\\{b_{i}\\}_{i=1}^{n}$, and superdiagonal entries $\\{c_{i}\\}_{i=1}^{n-1}$, and let the right-hand side be $\\{d_{i}\\}_{i=1}^{n}$.\n\nThe forward elimination phase proceeds for $i=2,3,\\dots,n$. In each step, we first compute the multiplier:\n$$\nm_{i}=\\frac{a_{i}}{b'_{i-1}}\n$$\nwhere $b'_{i-1}$ is the modified diagonal entry from the previous step. This is one division. Then, we update the main diagonal entry and the right-hand side vector:\n$$\nb'_{i} = b_{i}-m_{i}c_{i-1} \\\\\nd'_{i} = d_{i}-m_{i}d'_{i-1}\n$$\nEach of these updates involves one multiplication and one subtraction. Therefore, each step of the forward elimination requires 1 division, 2 multiplications, and 2 subtractions, for a total of 5 operations. Since this loop runs for $i=2$ to $n$, there are $n-1$ steps. The total cost for the forward elimination is:\n$$ \\text{Cost}_{\\text{forward}} = 5(n-1) $$\n\nThe backward substitution phase solves the resulting upper bidiagonal system. First, we compute the last component:\n$$\nx_{n}=\\frac{d'_{n}}{b'_{n}}\n$$\nThis is one division. Then, for $i=n-1,n-2,\\dots,1$, we compute:\n$$\nx_{i}=\\frac{d'_{i}-c_{i}x_{i+1}}{b'_{i}}\n$$\nEach step requires one multiplication ($c_{i}x_{i+1}$), one subtraction, and one division, for a total of 3 operations. This backward loop runs for $n-1$ steps. The total cost for the backward substitution is:\n$$ \\text{Cost}_{\\text{backward}} = 1 + 3(n-1) = 3n - 2 $$\n\nAdding the costs of the two stages yields the exact total number of fundamental arithmetic operations:\n$$ \\text{Total Cost} = \\text{Cost}_{\\text{forward}} + \\text{Cost}_{\\text{backward}} = 5(n-1) + (3n-2) = 5n - 5 + 3n - 2 = 8n-7. $$\n\nThis count is valid for all $n\\geq 1$. For $n=1$, the cost is $8(1)-7=1$, which corresponds to the single division $x_1=d_1/b_1$.", "answer": "$$\\boxed{8n-7}$$", "id": "2160762"}, {"introduction": "This final practice synthesizes the preceding concepts into a comprehensive analysis of a realistic, multi-step workflow from the field of machine learning. Solving problems like ridge regression involves not just one, but a sequence of numerical operations. This exercise challenges you to deconstruct the entire computational pipeline, from forming the normal equations to solving the final system with a Cholesky factorization, and to sum the costs of each stage to determine the total effort. [@problem_id:2160711]", "problem": "In the field of machine learning, ridge regression is a common technique used to address overfitting in linear models. It is mathematically equivalent to solving a Tikhonov-regularized least squares problem. For a given overdetermined system of linear equations $Ax=b$, where $A$ is a dense $m \\times n$ matrix of input features (with $m$ samples and $n$ features, where $m > n$), and $b$ is an $m \\times 1$ vector of observations, the regularized solution for the parameter vector $x$ can be found by solving the normal equations:\n$$\n(A^T A + \\lambda I)x = A^T b\n$$\nHere, $\\lambda$ is a positive scalar regularization parameter and $I$ is the $n \\times n$ identity matrix.\n\nConsider a direct method for solving this system that proceeds in the following sequence:\n1.  Compute the matrix $M = A^T A$.\n2.  Compute the vector $c = A^T b$.\n3.  Form the matrix $B = M + \\lambda I$.\n4.  Solve the linear system $Bx = c$ by first computing the Cholesky factorization of $B$ (i.e., $B = L L^T$), and then solving for $x$ via one forward and one backward substitution.\n\nYour task is to determine the total computational cost of this entire procedure. For your calculation, you are given the following standard costs for basic operations, where a \"flop\" is defined as a single floating-point addition, subtraction, multiplication, or division.\n\n-   The cost of multiplying a $p \\times q$ matrix by a $q \\times r$ matrix is $2pqr$ flops. However, if the resulting $p \\times r$ matrix is known to be symmetric and $p=r$, the cost can be reduced to $pqr$ flops by computing only the unique elements.\n-   The cost of computing the Cholesky factorization of a symmetric positive-definite $k \\times k$ matrix is approximately $\\frac{1}{3}k^3$ flops.\n-   The cost of solving a $k \\times k$ triangular system (i.e., forward or backward substitution) is approximately $k^2$ flops.\n-   You may neglect the computational cost of matrix addition, such as forming $B = M + \\lambda I$.\n\nExpress the total number of flops as a polynomial expression in terms of $m$ and $n$.", "solution": "We compute the total flop count by summing the costs of each step, using the provided cost model and exploiting symmetry where applicable.\n\nFirst, compute $M = A^{T}A$. Here, $A^{T}$ is an $n \\times m$ matrix and $A$ is an $m \\times n$ matrix. The resulting matrix $M$ is $n \\times n$ and symmetric. Using the provided rule for symmetric matrix products, the cost is $mn^2$ flops.\n\nSecond, compute $c = A^{T} b$. This is the product of an $n \\times m$ matrix and an $m \\times 1$ vector. Using the standard matrix-vector multiplication cost rule ($p=n, q=m, r=1$), the cost is $2nm = 2mn$ flops.\n\nThird, form $B = M + \\lambda I$. The problem statement allows us to neglect the cost of this matrix addition and scalar multiplication, so we count 0 flops for this step.\n\nFourth, solve the system $Bx=c$. This is done in two sub-steps:\na. Compute the Cholesky factorization $B = L L^{T}$. Since $B$ is an $n \\times n$ symmetric positive-definite matrix, the cost is approximately $\\frac{1}{3} n^{3}$ flops.\nb. Solve the two triangular systems $L y = c$ (forward substitution) and $L^{T} x = y$ (backward substitution). Each triangular solve on an $n \\times n$ system costs approximately $n^{2}$ flops, for a total of $2 n^{2}$ flops for both.\n\nSumming all contributions gives the total computational cost:\n$$\n\\text{Total Cost} = (\\text{Cost for } A^TA) + (\\text{Cost for } A^Tb) + (\\text{Cost for Cholesky}) + (\\text{Cost for substitutions}) \\\\\n= mn^2 + 2mn + \\frac{1}{3}n^3 + 2n^2\n$$\nRearranging the terms to group by powers of $n$ gives the final polynomial expression:\n$$ \\frac{1}{3}n^{3}+mn^{2}+2n^{2}+2mn $$", "answer": "$$\\boxed{\\frac{1}{3}n^{3}+mn^{2}+2n^{2}+2mn}$$", "id": "2160711"}]}