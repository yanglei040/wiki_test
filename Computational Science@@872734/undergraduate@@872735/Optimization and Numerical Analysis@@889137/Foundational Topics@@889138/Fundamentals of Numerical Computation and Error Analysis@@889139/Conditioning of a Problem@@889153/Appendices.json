{"hands_on_practices": [{"introduction": "The concept of a condition number quantifies how sensitive a function's output is to small changes in its input. To build an intuition for this fundamental idea, we will start with a familiar statistical measure: the arithmetic mean. This practice [@problem_id:2161810] challenges you to apply the formal definition of the relative condition number to find out how much a single data point, particularly an outlier, can influence the final average.", "problem": "In numerical analysis, the conditioning of a problem refers to the sensitivity of its solution to small relative perturbations in the input data. Consider the problem of computing the arithmetic mean of a set of $n$ positive data points, $\\{x_1, x_2, \\dots, x_n\\}$. The mean is given by the function $M(x_1, \\dots, x_n) = \\frac{1}{n}\\sum_{i=1}^n x_i$.\n\nThe relative condition number, $\\kappa_k$, of this computation with respect to a specific data point $x_k$, measures how a relative error in $x_k$ is amplified in the computed mean. For a differentiable function $f$ of multiple variables $x_1, \\dots, x_n$, this condition number is defined as:\n$$ \\kappa_k = \\left| \\frac{\\partial f}{\\partial x_k} \\frac{x_k}{f} \\right| $$\nSuppose a physicist has collected the following five measurements for a certain quantity: $\\{12.5, 15.2, 11.8, 13.5, 90.0\\}$. The physicist is concerned that the last measurement might have a larger uncertainty and wants to quantify the sensitivity of the arithmetic mean to this specific value.\n\nCalculate the relative condition number of the arithmetic mean computation with respect to the fifth data point, $x_5 = 90.0$. Round your final answer to three significant figures.", "solution": "The arithmetic mean of $n$ positive data points is the function $M(x_{1},\\dots,x_{n})=\\frac{1}{n}\\sum_{i=1}^{n}x_{i}$. For a differentiable function $f$, the relative condition number with respect to $x_{k}$ is defined as\n$$\n\\kappa_{k}=\\left|\\frac{\\partial f}{\\partial x_{k}}\\frac{x_{k}}{f}\\right|.\n$$\nFor $f=M$, the partial derivative with respect to $x_{k}$ is\n$$\n\\frac{\\partial M}{\\partial x_{k}}=\\frac{1}{n}.\n$$\nTherefore,\n$$\n\\kappa_{k}=\\left|\\frac{1}{n}\\cdot\\frac{x_{k}}{M}\\right|=\\left|\\frac{x_{k}}{nM}\\right|.\n$$\nSince $nM=\\sum_{i=1}^{n}x_{i}$, this simplifies to\n$$\n\\kappa_{k}=\\left|\\frac{x_{k}}{\\sum_{i=1}^{n}x_{i}}\\right|.\n$$\nFor the given data $\\{12.5,15.2,11.8,13.5,90.0\\}$ with $n=5$, the sum is\n$$\n\\sum_{i=1}^{5}x_{i}=12.5+15.2+11.8+13.5+90.0=143.0,\n$$\nand the mean is\n$$\nM=\\frac{143.0}{5}=28.6.\n$$\nFor $x_{5}=90.0$,\n$$\n\\kappa_{5}=\\left|\\frac{1}{5}\\cdot\\frac{90.0}{28.6}\\right|=\\frac{90}{5\\cdot 28.6}=\\frac{90}{143}.\n$$\nNumerically,\n$$\n\\frac{90}{143}\\approx 0.629370\\ldots\n$$\nRounding to three significant figures gives $0.629$.", "answer": "$$\\boxed{0.629}$$", "id": "2161810"}, {"introduction": "While condition numbers apply to any function, they are critically important in linear algebra for understanding the stability of solving systems of equations of the form $Ax=b$. A large condition number for the matrix $A$ signals an ill-conditioned problem, where the solution $x$ is highly sensitive to perturbations. This exercise [@problem_id:2428542] provides an analytical deep dive, asking you to examine how a matrix's condition number behaves as it approaches a point of singularity, revealing the mathematical roots of numerical instability.", "problem": "In computational engineering, the sensitivity of a linear system $A(\\epsilon)\\,x=b$ to perturbations in $b$ (or in $A(\\epsilon)$) is quantified by the condition number. Consider the parameterized $2 \\times 2$ symmetric matrix\n$$\nA(\\epsilon)=\\begin{pmatrix}\n1 & 1 \\\\\n1 & 1+\\epsilon\n\\end{pmatrix},\n$$\nwhere $\\epsilon>0$ is a small parameter. Using the spectral norm (the matrix norm induced by the Euclidean vector norm), determine the leading-order asymptotic expression for the spectral condition number $\\kappa_{2}(A(\\epsilon))$ as $\\epsilon \\to 0^{+}$. Provide a single simplified analytic expression in terms of $\\epsilon$. Do not approximate numerically.", "solution": "The spectral condition number, $\\kappa_{2}(A)$, of an invertible matrix $A$ is defined as the product of the spectral norm of the matrix and its inverse:\n$$\n\\kappa_{2}(A) = \\|A\\|_{2} \\|A^{-1}\\|_{2}\n$$\nThe problem specifies the matrix $A(\\epsilon)$ as:\n$$\nA(\\epsilon) = \\begin{pmatrix} 1 & 1 \\\\ 1 & 1+\\epsilon \\end{pmatrix}\n$$\nThis matrix is symmetric. For a symmetric matrix, the spectral norm, $\\| \\cdot \\|_{2}$, is equal to its spectral radius, which is the maximum of the absolute values of its eigenvalues. Let the eigenvalues of $A(\\epsilon)$ be denoted by $\\lambda$. The eigenvalues are the roots of the characteristic equation $\\det(A(\\epsilon) - \\lambda I) = 0$, where $I$ is the $2 \\times 2$ identity matrix.\n\nThe characteristic equation is:\n$$\n\\det \\begin{pmatrix} 1-\\lambda & 1 \\\\ 1 & 1+\\epsilon-\\lambda \\end{pmatrix} = 0\n$$\nExpanding the determinant gives:\n$$\n(1-\\lambda)(1+\\epsilon-\\lambda) - 1^{2} = 0\n$$\n$$\n1 + \\epsilon - \\lambda - \\lambda - \\epsilon\\lambda + \\lambda^{2} - 1 = 0\n$$\n$$\n\\lambda^{2} - (2+\\epsilon)\\lambda + \\epsilon = 0\n$$\nThis is a quadratic equation for $\\lambda$. We solve for $\\lambda$ using the quadratic formula:\n$$\n\\lambda = \\frac{-(-(2+\\epsilon)) \\pm \\sqrt{(-(2+\\epsilon))^{2} - 4(1)(\\epsilon)}}{2(1)}\n$$\n$$\n\\lambda = \\frac{2+\\epsilon \\pm \\sqrt{4 + 4\\epsilon + \\epsilon^{2} - 4\\epsilon}}{2}\n$$\n$$\n\\lambda = \\frac{2+\\epsilon \\pm \\sqrt{4 + \\epsilon^{2}}}{2}\n$$\nThis gives two eigenvalues, which we denote as $\\lambda_{max}$ and $\\lambda_{min}$:\n$$\n\\lambda_{max} = \\frac{2+\\epsilon + \\sqrt{4 + \\epsilon^{2}}}{2}\n$$\n$$\n\\lambda_{min} = \\frac{2+\\epsilon - \\sqrt{4 + \\epsilon^{2}}}{2}\n$$\nSince $\\epsilon > 0$, we have $\\sqrt{4+\\epsilon^{2}} > \\sqrt{4} = 2$. Also, $(2+\\epsilon)^2 = 4+4\\epsilon+\\epsilon^2 > 4+\\epsilon^2$, which implies $2+\\epsilon > \\sqrt{4+\\epsilon^2}$. Therefore, it follows that $\\lambda_{max} > \\lambda_{min} > 0$. Both eigenvalues are positive.\n\nThe spectral norm of $A(\\epsilon)$ is the largest absolute eigenvalue:\n$$\n\\|A(\\epsilon)\\|_{2} = |\\lambda_{max}| = \\lambda_{max} = \\frac{2+\\epsilon + \\sqrt{4 + \\epsilon^{2}}}{2}\n$$\nThe matrix $A(\\epsilon)$ is invertible because its determinant is $\\det(A(\\epsilon)) = (1)(1+\\epsilon) - (1)(1) = \\epsilon \\neq 0$. The eigenvalues of the inverse matrix $A(\\epsilon)^{-1}$ are the reciprocals of the eigenvalues of $A(\\epsilon)$, i.e., $1/\\lambda_{max}$ and $1/\\lambda_{min}$. Since $A(\\epsilon)^{-1}$ is also symmetric, its spectral norm is its largest absolute eigenvalue. As $0 < \\lambda_{min} < \\lambda_{max}$, it follows that $0 < 1/\\lambda_{max} < 1/\\lambda_{min}$. Thus:\n$$\n\\|A(\\epsilon)^{-1}\\|_{2} = \\frac{1}{\\lambda_{min}} = \\frac{2}{2+\\epsilon - \\sqrt{4 + \\epsilon^{2}}}\n$$\nThe spectral condition number is the ratio of the largest to the smallest eigenvalue magnitude:\n$$\n\\kappa_{2}(A(\\epsilon)) = \\frac{\\lambda_{max}}{\\lambda_{min}} = \\frac{\\frac{2+\\epsilon + \\sqrt{4 + \\epsilon^{2}}}{2}}{\\frac{2+\\epsilon - \\sqrt{4 + \\epsilon^{2}}}{2}} = \\frac{2+\\epsilon + \\sqrt{4 + \\epsilon^{2}}}{2+\\epsilon - \\sqrt{4 + \\epsilon^{2}}}\n$$\nTo determine the leading-order asymptotic expression as $\\epsilon \\to 0^{+}$, we analyze the behavior of the numerator and denominator for small $\\epsilon$. The term $\\sqrt{4+\\epsilon^2}$ is expanded using the binomial series $(1+x)^{\\alpha} \\approx 1+\\alpha x$ for small $x$:\n$$\n\\sqrt{4+\\epsilon^2} = 2\\sqrt{1+\\frac{\\epsilon^2}{4}}\n$$\nWith $x = \\epsilon^{2}/4$ and $\\alpha = 1/2$, the expansion is:\n$$\n2\\sqrt{1+\\frac{\\epsilon^2}{4}} \\approx 2\\left(1 + \\frac{1}{2}\\frac{\\epsilon^2}{4}\\right) = 2 + \\frac{\\epsilon^2}{4}\n$$\nSo, for $\\epsilon \\to 0^{+}$, we have $\\sqrt{4+\\epsilon^2} = 2 + \\frac{\\epsilon^2}{4} + O(\\epsilon^4)$.\n\nNow we analyze the leading-order behavior of $\\lambda_{max}$ and $\\lambda_{min}$.\nFor the numerator of $\\kappa_2(A(\\epsilon))$:\n$$\n2+\\epsilon + \\sqrt{4 + \\epsilon^{2}} = 2+\\epsilon + \\left(2 + \\frac{\\epsilon^2}{4} + O(\\epsilon^4)\\right) = 4 + \\epsilon + O(\\epsilon^2)\n$$\nThe leading term as $\\epsilon \\to 0^{+}$ is $4$.\n\nFor the denominator of $\\kappa_2(A(\\epsilon))$:\n$$\n2+\\epsilon - \\sqrt{4 + \\epsilon^{2}} = 2+\\epsilon - \\left(2 + \\frac{\\epsilon^2}{4} + O(\\epsilon^4)\\right) = \\epsilon - \\frac{\\epsilon^2}{4} + O(\\epsilon^4)\n$$\nThe leading term as $\\epsilon \\to 0^{+}$ is $\\epsilon$.\n\nTherefore, the leading-order asymptotic expression for the condition number is the ratio of these leading-order terms:\n$$\n\\kappa_2(A(\\epsilon)) \\approx \\frac{4+\\epsilon}{\\epsilon - \\frac{\\epsilon^2}{4}} = \\frac{4(1+\\frac{\\epsilon}{4})}{\\epsilon(1-\\frac{\\epsilon}{4})} \\approx \\frac{4}{\\epsilon} \\quad \\text{as } \\epsilon \\to 0^{+}\n$$\nMore formally, we can write:\n$$\n\\kappa_2(A(\\epsilon)) = \\frac{4 + O(\\epsilon)}{\\epsilon + O(\\epsilon^2)} = \\frac{1}{\\epsilon} \\frac{4 + O(\\epsilon)}{1 + O(\\epsilon)} = \\frac{1}{\\epsilon} (4 + O(\\epsilon)) = \\frac{4}{\\epsilon} + O(1)\n$$\nThe leading-order asymptotic expression for $\\kappa_{2}(A(\\epsilon))$ as $\\epsilon \\to 0^{+}$ is $\\frac{4}{\\epsilon}$. This indicates severe ill-conditioning as the matrix approaches singularity for $\\epsilon \\to 0$.", "answer": "$$\\boxed{\\frac{4}{\\epsilon}}$$", "id": "2428542"}, {"introduction": "Theoretical analysis of ill-conditioning comes to life when we confront the limitations of computer arithmetic. This final practice [@problem_id:2428600] is a computational experiment that uses the notoriously ill-conditioned Hilbert matrix to demonstrate the practical consequences of a large condition number. By solving a linear system and comparing the numerical solution to the true solution, you will empirically verify the relationship between the condition number, forward error, and backward error, and witness how numerical accuracy can be completely lost.", "problem": "Write a complete program that empirically demonstrates the effect of problem conditioning on the accuracy of solving linear systems in standard floating-point arithmetic. Consider the linear system $A x = b$ where $A$ is the $n \\times n$ Hilbert matrix with entries $A_{i j} = \\dfrac{1}{i + j - 1}$ for $1 \\le i,j \\le n$. For each test case, define the exact solution vector $x_{\\text{true}} \\in \\mathbb{R}^n$ by $x_{\\text{true}} = \\mathbf{1}$ (all ones), and construct the right-hand side $b = A x_{\\text{true}}$. Then compute the numerical solution $\\hat{x}$ using standard floating-point arithmetic conforming to Institute of Electrical and Electronics Engineers (IEEE) $754$ double precision (binary$64$) semantics. Using $\\hat{x}$, compute the following quantities:\n- The $2$-norm condition number $\\kappa_2(A) = \\|A\\|_2 \\,\\|A^{-1}\\|_2$.\n- The relative forward error in the $2$-norm, $\\dfrac{\\|\\hat{x} - x_{\\text{true}}\\|_2}{\\|x_{\\text{true}}\\|_2}$.\n- The scaled residual (a normalized backward error proxy), $\\dfrac{\\|b - A \\hat{x}\\|_2}{\\|A\\|_2 \\,\\|\\hat{x}\\|_2 + \\|b\\|_2}$.\n- The estimated number of correct decimal digits in the solution, defined as $\\max\\!\\bigl(0,\\,-\\log_{10}(\\max(\\text{relative forward error}, \\varepsilon)))$, where $\\varepsilon$ denotes the smallest positive normalized IEEE $754$ double-precision number.\n\nTest Suite:\nEvaluate the above for the following problem sizes $n$:\n- $n = 3$,\n- $n = 6$,\n- $n = 10$,\n- $n = 12$.\n\nAll vector and matrix norms are the spectral norm (that is, the matrix $2$-norm and the vector $2$-norm). Angles are not involved. No physical units are involved.\n\nYour program must produce a single line of output containing all test results as a comma-separated list enclosed in square brackets, where each test result is itself a list of the form $[n,\\;\\kappa_2(A),\\;\\text{relative forward error},\\;\\text{scaled residual},\\;\\text{estimated digits}]$. The final output must therefore be a single line representing a list of lists, with no additional text. For example, the structure must be similar to $[[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],\\ldots]$ but with the actual computed numbers in place of the placeholders.", "solution": "This problem requires an empirical investigation into the effects of ill-conditioning on the solution of a linear system of equations, $A x = b$. The matrix $A$ is chosen to be the $n \\times n$ Hilbert matrix, a classic example of a severely ill-conditioned matrix. Its entries are given by $A_{i j} = \\frac{1}{i + j - 1}$ for $i, j$ from $1$ to $n$.\n\nThe core of numerical analysis is not only to compute a solution but also to understand its accuracy. The accuracy of the computed solution, which we denote $\\hat{x}$, is affected by two main factors: the stability of the algorithm used and the intrinsic sensitivity of the problem itself. This sensitivity is quantified by the condition number.\n\nFor a linear system $A x = b$, the $2$-norm condition number of the matrix $A$ is defined as:\n$$ \\kappa_2(A) = \\|A\\|_2 \\|A^{-1}\\|_2 $$\nwhere $\\| \\cdot \\|_2$ is the spectral norm (the largest singular value). A large condition number, $\\kappa_2(A) \\gg 1$, signifies an ill-conditioned problem, where small relative perturbations in the input data ($A$ or $b$) can lead to large relative changes in the solution $x$.\n\nWhen we solve $A x = b$ using floating-point arithmetic, round-off errors are inevitably introduced. A backward stable algorithm, such as the LU decomposition employed by standard solvers, produces a computed solution $\\hat{x}$ that is the exact solution to a slightly perturbed problem:\n$$ (A + \\delta A) \\hat{x} = b + \\delta b $$\nThe \"smallness\" of these perturbations $\\delta A$ and $\\delta b$ is a measure of the algorithm's backward stability. A key result in numerical analysis establishes the following bound on the relative forward error:\n$$ \\frac{\\|\\hat{x} - x_{\\text{true}}\\|_2}{\\|x_{\\text{true}}\\|_2} \\le \\kappa_2(A) \\left( \\frac{\\|\\delta A\\|_2}{\\|A\\|_2} + \\frac{\\|\\delta b\\|_2}{\\|b\\|_2} \\right) $$\nThe right-hand side parenthetical term is the relative backward error. For a backward stable algorithm operating with machine precision $\\varepsilon_{\\text{mach}}$, the backward error is typically of order $\\mathcal{O}(\\varepsilon_{\\text{mach}})$. Machine precision for IEEE $754$ double-precision is approximately $2.22 \\times 10^{-16}$. Therefore, we expect the relative forward error to be bounded by approximately $\\kappa_2(A) \\cdot \\varepsilon_{\\text{mach}}$. This demonstrates that a large condition number amplifies the unavoidable round-off errors, potentially destroying the accuracy of the solution.\n\nThe residual vector is defined as $r = b - A \\hat{x}$. The norm of the residual, $\\|r\\|_2$, is related to the backward error. The problem asks for a specific scaled residual:\n$$ \\frac{\\|b - A \\hat{x}\\|_2}{\\|A\\|_2 \\,\\|\\hat{x}\\|_2 + \\|b\\|_2} $$\nThis quantity serves as a normalized proxy for the backward error. Due to the backward stability of the solver, we expect this value to remain small, on the order of $\\varepsilon_{\\text{mach}}$, even as the condition number grows and the forward error explodes. This is a crucial distinction: a small residual does not guarantee a small forward error.\n\nFinally, we estimate the number of correct decimal digits in the solution. This is directly related to the relative forward error. If the relative error is $10^{-k}$, the solution is accurate to roughly $k$ decimal digits. The given formula, $\\max\\!\\bigl(0,\\,-\\log_{10}(\\max(\\text{relative forward error}, \\varepsilon)))$, formalizes this. The term $\\varepsilon$ is the smallest positive normalized double-precision number, approximately $2.225 \\times 10^{-308}$, which prevents a logarithm of zero and handles cases where the error is smaller than representable precision allows.\n\nWe will now perform these calculations for the specified test suite of matrix sizes $n \\in \\{3, 6, 10, 12\\}$. For each $n$:\n1.  Construct the $n \\times n$ Hilbert matrix $A$.\n2.  Define the true solution $x_{\\text{true}}$ as a vector of $n$ ones.\n3.  Compute the right-hand side $b = A x_{\\text{true}}$.\n4.  Solve for the numerical solution $\\hat{x}$ using a standard linear solver.\n5.  Compute the four specified quantities: $\\kappa_2(A)$, relative forward error, scaled residual, and estimated digits.\n\nThe results will empirically validate the theory. The condition number of the Hilbert matrix grows extremely rapidly with $n$. For small $n$ (e.g., $n=3$), $\\kappa_2(A)$ is moderate, and we expect a reasonably accurate solution. As $n$ increases to $10$ and $12$, $\\kappa_2(A)$ will become enormous ($> 10^{13}$), leading to a relative forward error of order $1$ or greater, signifying a complete loss of accuracy. Throughout this process, the scaled residual should remain small, demonstrating the backward stability of the algorithm in contrast to the poor forward accuracy for an ill-conditioned problem.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import hilbert\n\ndef solve():\n    \"\"\"\n    Empirically demonstrates the effect of problem conditioning on the accuracy\n    of solving linear systems Ax = b using the Hilbert matrix.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [3, 6, 10, 12]\n\n    results = []\n    \n    # Epsilon as defined in the problem: the smallest positive normalized\n    # IEEE 754 double-precision number.\n    smallest_norm_val = np.finfo(np.float64).smallest_normal\n\n    for n in test_cases:\n        # Step 1: Construct the n x n Hilbert matrix A.\n        # The problem statement indices are 1-based, but `scipy.linalg.hilbert` is 0-based,\n        # which results in the same matrix A_ij = 1 / ((i+1) + (j+1) - 1) for 0<=i,j<n.\n        A = hilbert(n)\n\n        # Step 2: Define the exact solution vector x_true (all ones).\n        x_true = np.ones(n)\n\n        # Step 3: Construct the right-hand side b = A * x_true.\n        # This ensures that b is consistent with A and x_true.\n        b = A @ x_true\n\n        # Step 4: Compute the numerical solution x_hat using a standard solver.\n        # numpy.linalg.solve uses LAPACK routines which are backward stable and\n        # operate in IEEE 754 double precision.\n        x_hat = np.linalg.solve(A, b)\n\n        # --- Calculate the required quantities ---\n\n        # The 2-norm condition number kappa_2(A).\n        kappa_2_A = np.linalg.cond(A, 2)\n\n        # The relative forward error in the 2-norm.\n        norm_x_true = np.linalg.norm(x_true, 2)\n        if norm_x_true == 0:\n            # Avoid division by zero, though not possible for x_true = 1.\n            rel_fwd_err = np.linalg.norm(x_hat - x_true, 2)\n        else:\n            rel_fwd_err = np.linalg.norm(x_hat - x_true, 2) / norm_x_true\n        \n        # The scaled residual (a normalized backward error proxy).\n        # residual = b - A @ x_hat\n        # norm_residual = ||b - A*x_hat||_2\n        # scaled_residual = norm_residual / (||A||_2 * ||x_hat||_2 + ||b||_2)\n        norm_A = np.linalg.norm(A, 2)\n        norm_x_hat = np.linalg.norm(x_hat, 2)\n        norm_b = np.linalg.norm(b, 2)\n        norm_residual = np.linalg.norm(b - A @ x_hat, 2)\n        \n        denominator = norm_A * norm_x_hat + norm_b\n        if denominator == 0:\n            # Handle potential division by zero.\n            scaled_res = norm_residual\n        else:\n            scaled_res = norm_residual / denominator\n\n        # The estimated number of correct decimal digits.\n        # est_digits = max(0, -log10(max(relative forward error, epsilon)))\n        log_val = max(rel_fwd_err, smallest_norm_val)\n        est_digits = max(0.0, -np.log10(log_val))\n        \n        # Store results for this test case.\n        results.append([n, kappa_2_A, rel_fwd_err, scaled_res, est_digits])\n\n    # Final print statement in the exact required format.\n    # The output format is a string representation of a list of lists.\n    # Example: [[3, 5.2e+02, ...], [6, 1.5e+07, ...], ...]\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2428600"}]}