{"hands_on_practices": [{"introduction": "The conjugate gradient method elegantly builds upon a more intuitive algorithm: steepest descent. The first iteration is, in fact, a pure steepest descent step. This exercise provides foundational practice in calculating the two key components of this first move: the initial search direction, which points opposite to the gradient, and the optimal step size, which ensures we travel the perfect distance along that direction to minimize the function [@problem_id:2211287].", "problem": "Consider the problem of minimizing the quadratic objective function $f(\\mathbf{x}) = f(x_1, x_2)$ given by:\n$$f(x_1, x_2) = \\frac{3}{2}x_1^2 + x_1x_2 + x_2^2 - x_1 - 4x_2$$\nwhere $\\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$.\n\nWe wish to apply the conjugate gradient method, starting from the initial point $\\mathbf{x}_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$. The first iteration of the algorithm moves from $\\mathbf{x}_0$ to a new point $\\mathbf{x}_1 = \\mathbf{x}_0 + \\alpha_0 \\mathbf{d}_0$, where $\\mathbf{d}_0$ is the initial search direction and $\\alpha_0$ is the optimal step size.\n\nDetermine the components of the initial search direction vector, $\\mathbf{d}_0 = \\begin{pmatrix} d_{0,1} \\\\ d_{0,2} \\end{pmatrix}$, and the exact value of the optimal step size $\\alpha_0$.\n\nPresent your final answer as a $1 \\times 3$ row matrix containing the values of $d_{0,1}$, $d_{0,2}$, and $\\alpha_0$, in that specific order. Express all values as exact fractions where necessary.", "solution": "We rewrite the quadratic objective in the standard form $f(\\mathbf{x})=\\frac{1}{2}\\mathbf{x}^{T}Q\\mathbf{x}-\\mathbf{c}^{T}\\mathbf{x}$ by identifying the symmetric matrix $Q$ and vector $\\mathbf{c}$. From\n$$f(x_{1},x_{2})=\\frac{3}{2}x_{1}^{2}+x_{1}x_{2}+x_{2}^{2}-x_{1}-4x_{2},$$\nwe have\n$$Q=\\begin{pmatrix}3 & 1 \\\\ 1 & 2\\end{pmatrix},\\qquad \\mathbf{c}=\\begin{pmatrix}1 \\\\ 4\\end{pmatrix}.$$\nThe gradient is $\\nabla f(\\mathbf{x})=Q\\mathbf{x}-\\mathbf{c}$. At the initial point $\\mathbf{x}_{0}=\\begin{pmatrix}0 \\\\ 0\\end{pmatrix}$, the gradient is\n$$\\mathbf{g}_{0}=\\nabla f(\\mathbf{x}_{0})=Q\\mathbf{x}_{0}-\\mathbf{c}=\\begin{pmatrix}0 \\\\ 0\\end{pmatrix}-\\begin{pmatrix}1 \\\\ 4\\end{pmatrix}=\\begin{pmatrix}-1 \\\\ -4\\end{pmatrix}.$$\nIn the conjugate gradient method, the initial search direction is the negative gradient:\n$$\\mathbf{d}_{0}=-\\mathbf{g}_{0}=\\begin{pmatrix}1 \\\\ 4\\end{pmatrix}.$$\nThe exact line-search step size along $\\mathbf{d}_{0}$ is obtained by minimizing $\\phi(\\alpha)=f(\\mathbf{x}_{0}+\\alpha\\mathbf{d}_{0})$, which yields\n$$\\alpha_{0}=-\\frac{\\mathbf{d}_{0}^{T}\\mathbf{g}_{0}}{\\mathbf{d}_{0}^{T}Q\\mathbf{d}_{0}}=\\frac{\\mathbf{g}_{0}^{T}\\mathbf{g}_{0}}{\\mathbf{d}_{0}^{T}Q\\mathbf{d}_{0}},$$\nsince $\\mathbf{d}_{0}=-\\mathbf{g}_{0}$. Compute the numerator:\n$$\\mathbf{g}_{0}^{T}\\mathbf{g}_{0}=(-1)^{2}+(-4)^{2}=1+16=17.$$\nCompute the denominator:\n$$Q\\mathbf{d}_{0}=\\begin{pmatrix}3 & 1 \\\\ 1 & 2\\end{pmatrix}\\begin{pmatrix}1 \\\\ 4\\end{pmatrix}=\\begin{pmatrix}7 \\\\ 9\\end{pmatrix},\\qquad \\mathbf{d}_{0}^{T}Q\\mathbf{d}_{0}=\\begin{pmatrix}1 & 4\\end{pmatrix}\\begin{pmatrix}7 \\\\ 9\\end{pmatrix}=7+36=43.$$\nTherefore,\n$$\\alpha_{0}=\\frac{17}{43}.$$\nThus, $d_{0,1}=1$, $d_{0,2}=4$, and $\\alpha_{0}=\\frac{17}{43}$, as exact fractions where necessary.", "answer": "$$\\boxed{\\begin{pmatrix} 1 & 4 & \\frac{17}{43} \\end{pmatrix}}$$", "id": "2211287"}, {"introduction": "What truly distinguishes the conjugate gradient method is its \"memory\" of previous directions, ensuring that new search directions are \"A-conjugate\" to the old ones. This property guarantees that for an $n$-dimensional quadratic problem, we don't undo progress made in previous steps and can find the minimum in at most $n$ iterations. This practice demonstrates this core principle by guiding you through the calculation of the second search direction for a 2D problem and verifying the A-conjugacy property firsthand [@problem_id:2211315].", "problem": "Consider the unconstrained optimization problem of minimizing a two-dimensional quadratic function $f(\\mathbf{x})$, where $\\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$. The function is defined as $f(\\mathbf{x}) = \\frac{1}{2}\\mathbf{x}^T A \\mathbf{x} - \\mathbf{b}^T \\mathbf{x}$, with the symmetric positive-definite matrix $A$ and vector $\\mathbf{b}$ given by:\n$$\nA = \\begin{pmatrix} 5 & 2 \\\\ 2 & 1 \\end{pmatrix}, \\quad \\mathbf{b} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n$$\nWe will use the Fletcher-Reeves variant of the Conjugate Gradient (CG) method to find the minimum of this function. The gradient of the function is $\\nabla f(\\mathbf{x}) = A\\mathbf{x} - \\mathbf{b}$. The iterative steps of the CG method, starting from a point $\\mathbf{x}_k$, are as follows:\n\n1.  Let $g_k = \\nabla f(\\mathbf{x}_k)$ be the gradient at the current point.\n2.  The search direction $\\mathbf{p}_k$ is determined by the rule:\n    -   For the first iteration ($k=0$): $\\mathbf{p}_0 = -g_0$.\n    -   For subsequent iterations ($k > 0$): $\\mathbf{p}_k = -g_k + \\beta_k \\mathbf{p}_{k-1}$, where $\\beta_k = \\frac{g_k^T g_k}{g_{k-1}^T g_{k-1}}$.\n3.  The step size $\\alpha_k$ is calculated as $\\alpha_k = \\frac{g_k^T g_k}{\\mathbf{p}_k^T A \\mathbf{p}_k}$.\n4.  The position is updated as $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{p}_k$.\n5.  The gradient for the next iteration is updated using the efficient formula $g_{k+1} = g_k + \\alpha_k A \\mathbf{p}_k$.\n\nTwo vectors $\\mathbf{u}$ and $\\mathbf{v}$ are said to be conjugate with respect to the matrix $A$ if the quantity $\\mathbf{u}^T A \\mathbf{v}$ is equal to zero. A key property of the CG method is that it generates a sequence of mutually conjugate search directions.\n\nYour task is to apply the CG method starting from the point $\\mathbf{x}_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ to generate the first two search directions, $\\mathbf{p}_0$ and $\\mathbf{p}_1$. Then, compute the value of the expression $\\mathbf{p}_0^T A \\mathbf{p}_1$.", "solution": "We minimize $f(\\mathbf{x})=\\frac{1}{2}\\mathbf{x}^{T}A\\mathbf{x}-\\mathbf{b}^{T}\\mathbf{x}$ with $A=\\begin{pmatrix}5&2\\\\2&1\\end{pmatrix}$, $\\mathbf{b}=\\begin{pmatrix}1\\\\1\\end{pmatrix}$, starting at $\\mathbf{x}_{0}=\\begin{pmatrix}0\\\\0\\end{pmatrix}$. The gradient is $\\nabla f(\\mathbf{x})=A\\mathbf{x}-\\mathbf{b}$.\n\nCompute the initial gradient:\n$$\n\\mathbf{g}_{0}=\\nabla f(\\mathbf{x}_{0})=A\\mathbf{x}_{0}-\\mathbf{b}=\\begin{pmatrix}0\\\\0\\end{pmatrix}-\\begin{pmatrix}1\\\\1\\end{pmatrix}=\\begin{pmatrix}-1\\\\-1\\end{pmatrix}.\n$$\nFirst search direction:\n$$\n\\mathbf{p}_{0}=-\\mathbf{g}_{0}=\\begin{pmatrix}1\\\\1\\end{pmatrix}.\n$$\nStep size $\\alpha_{0}$:\n$$\n\\alpha_{0}=\\frac{\\mathbf{g}_{0}^{T}\\mathbf{g}_{0}}{\\mathbf{p}_{0}^{T}A\\mathbf{p}_{0}},\\quad \\mathbf{g}_{0}^{T}\\mathbf{g}_{0}=(-1)^{2}+(-1)^{2}=2,\n$$\n$$\nA\\mathbf{p}_{0}=\\begin{pmatrix}5&2\\\\2&1\\end{pmatrix}\\begin{pmatrix}1\\\\1\\end{pmatrix}=\\begin{pmatrix}7\\\\3\\end{pmatrix},\\quad \\mathbf{p}_{0}^{T}A\\mathbf{p}_{0}=\\begin{pmatrix}1&1\\end{pmatrix}\\begin{pmatrix}7\\\\3\\end{pmatrix}=10,\n$$\n$$\n\\alpha_{0}=\\frac{2}{10}=\\frac{1}{5}.\n$$\nUpdate the position and gradient:\n$$\n\\mathbf{x}_{1}=\\mathbf{x}_{0}+\\alpha_{0}\\mathbf{p}_{0}=\\begin{pmatrix}0\\\\0\\end{pmatrix}+\\frac{1}{5}\\begin{pmatrix}1\\\\1\\end{pmatrix}=\\begin{pmatrix}\\frac{1}{5}\\\\\\frac{1}{5}\\end{pmatrix},\n$$\n$$\n\\mathbf{g}_{1}=\\mathbf{g}_{0}+\\alpha_{0}A\\mathbf{p}_{0}=\\begin{pmatrix}-1\\\\-1\\end{pmatrix}+\\frac{1}{5}\\begin{pmatrix}7\\\\3\\end{pmatrix}=\\begin{pmatrix}\\frac{2}{5}\\\\-\\frac{2}{5}\\end{pmatrix}.\n$$\nCompute $\\beta_{1}$ and the second search direction $\\mathbf{p}_{1}$:\n$$\n\\beta_{1}=\\frac{\\mathbf{g}_{1}^{T}\\mathbf{g}_{1}}{\\mathbf{g}_{0}^{T}\\mathbf{g}_{0}}=\\frac{\\left(\\frac{2}{5}\\right)^{2}+\\left(-\\frac{2}{5}\\right)^{2}}{2}=\\frac{\\frac{8}{25}}{2}=\\frac{4}{25},\n$$\n$$\n\\mathbf{p}_{1}=-\\mathbf{g}_{1}+\\beta_{1}\\mathbf{p}_{0}=-\\begin{pmatrix}\\frac{2}{5}\\\\-\\frac{2}{5}\\end{pmatrix}+\\frac{4}{25}\\begin{pmatrix}1\\\\1\\end{pmatrix}=\\begin{pmatrix}-\\frac{6}{25}\\\\\\frac{14}{25}\\end{pmatrix}.\n$$\nFinally, compute $\\mathbf{p}_{0}^{T}A\\mathbf{p}_{1}$:\n$$\nA\\mathbf{p}_{1}=\\begin{pmatrix}5&2\\\\2&1\\end{pmatrix}\\begin{pmatrix}-\\frac{6}{25}\\\\\\frac{14}{25}\\end{pmatrix}=\\begin{pmatrix}-\\frac{2}{25}\\\\\\frac{2}{25}\\end{pmatrix},\n$$\n$$\n\\mathbf{p}_{0}^{T}A\\mathbf{p}_{1}=\\begin{pmatrix}1&1\\end{pmatrix}\\begin{pmatrix}-\\frac{2}{25}\\\\\\frac{2}{25}\\end{pmatrix}=0.\n$$\nThis confirms that the first two CG search directions are $A$-conjugate.", "answer": "$$\\boxed{0}$$", "id": "2211315"}, {"introduction": "To truly grasp the efficiency of the conjugate gradient method, it is instructive to consider an ideal scenario. This exercise explores the behavior of the algorithm on a special quadratic function whose level sets are perfect spheres, a case where the Hessian matrix is a multiple of the identity matrix. By working through this hypothetical problem, you will discover why the method converges in a single step, providing deep insight into the connection between the problem's geometry and the algorithm's performance [@problem_id:2211314].", "problem": "Consider the minimization of the multivariate quadratic function $f(\\mathbf{x})$ defined as:\n$$f(\\mathbf{x}) = \\frac{1}{2} c \\mathbf{x}^T \\mathbf{x} - \\mathbf{b}^T \\mathbf{x}$$\nwhere $\\mathbf{x}$ and $\\mathbf{b}$ are column vectors in $\\mathbb{R}^n$ for some integer $n \\ge 1$, and $c$ is a positive real-valued constant.\n\nStarting from an arbitrary initial point $\\mathbf{x}_0$, for which the gradient of $f$ is non-zero, a single iteration of the conjugate gradient (CG) method is performed to find the next point, $\\mathbf{x}_1$.\n\nDetermine the expression for $\\mathbf{x}_1$. Express your answer in terms of the constant $c$ and the vectors $\\mathbf{b}$ and $\\mathbf{x}_0$.", "solution": "We consider the quadratic function $f(\\mathbf{x})=\\frac{1}{2}c\\,\\mathbf{x}^{T}\\mathbf{x}-\\mathbf{b}^{T}\\mathbf{x}$ with $c>0$. Its gradient and Hessian are\n$$\n\\nabla f(\\mathbf{x})=c\\,\\mathbf{x}-\\mathbf{b}, \\qquad \\nabla^{2}f(\\mathbf{x})=c\\,\\mathbf{I},\n$$\nso minimizing $f$ is equivalent to solving the linear system\n$$\n\\mathbf{A}\\mathbf{x}=\\mathbf{b}, \\quad \\text{with} \\quad \\mathbf{A}=c\\,\\mathbf{I},\n$$\nwhich is symmetric positive definite.\n\nIn the conjugate gradient method, starting from $\\mathbf{x}_{0}$ with nonzero gradient, define the residual and search direction at the first iteration as\n$$\n\\mathbf{r}_{0}=\\mathbf{b}-\\mathbf{A}\\mathbf{x}_{0}=\\mathbf{b}-c\\,\\mathbf{x}_{0}, \\qquad \\mathbf{p}_{0}=\\mathbf{r}_{0}.\n$$\nThe step size $\\alpha_{0}$ is given by\n$$\n\\alpha_{0}=\\frac{\\mathbf{r}_{0}^{T}\\mathbf{r}_{0}}{\\mathbf{p}_{0}^{T}\\mathbf{A}\\mathbf{p}_{0}}=\\frac{\\mathbf{r}_{0}^{T}\\mathbf{r}_{0}}{\\mathbf{r}_{0}^{T}(c\\,\\mathbf{I})\\mathbf{r}_{0}}=\\frac{\\mathbf{r}_{0}^{T}\\mathbf{r}_{0}}{c\\,\\mathbf{r}_{0}^{T}\\mathbf{r}_{0}}=\\frac{1}{c}.\n$$\nThus, the updated point after one CG iteration is\n$$\n\\mathbf{x}_{1}=\\mathbf{x}_{0}+\\alpha_{0}\\mathbf{p}_{0}=\\mathbf{x}_{0}+\\frac{1}{c}\\left(\\mathbf{b}-c\\,\\mathbf{x}_{0}\\right)=\\frac{1}{c}\\,\\mathbf{b}.\n$$\nTherefore, a single CG step reaches the exact minimizer since $\\mathbf{A}=c\\,\\mathbf{I}$ has a single eigenvalue.", "answer": "$$\\boxed{\\frac{1}{c}\\,\\mathbf{b}}$$", "id": "2211314"}]}