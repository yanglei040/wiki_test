{"hands_on_practices": [{"introduction": "Particle Swarm Optimization (PSO) is a powerful metaheuristic inspired by the collective behavior of animal swarms. Its effectiveness hinges on a delicate balance between individual exploration and group exploitation. This practice challenges you to analyze how tuning the cognitive ($c_1$) and social ($c_2$) coefficients directly controls this balance, leading to distinct search behaviors such as premature convergence or swarm stagnation [@problem_id:2176755]. Understanding this relationship is fundamental to successfully applying PSO to complex, multi-modal optimization problems.", "problem": "An engineer is using a Particle Swarm Optimization (PSO) algorithm to find the minimum of a complex, high-dimensional, non-convex function that is known to have many local minima. The motion of each particle in the swarm is governed by the following standard update rules for its position $x_i$ and velocity $v_i$ at discrete time step $t$:\n\n$$v_{i}(t+1) = w v_{i}(t) + c_1 r_1 (p_{i} - x_i(t)) + c_2 r_2 (p_{g} - x_i(t))$$\n$$x_{i}(t+1) = x_{i}(t) + v_{i}(t+1)$$\n\nHere, $w$ is the inertia weight, $p_i$ is the best position found so far by particle $i$ (its personal best), and $p_g$ is the best position found so far by any particle in the entire swarm (the global best). The parameters $c_1$ and $c_2$ are positive constants known as the cognitive and social coefficients, respectively, which control the influence of the personal and global best positions. The terms $r_1$ and $r_2$ are random numbers independently drawn from a uniform distribution on $[0, 1]$ at each update step for each particle.\n\nThe engineer experiments with two different parameter configurations, keeping all other parameters ($w$, swarm size, number of iterations) constant.\n\n*   **Parameter Set A:** $c_1 = 2.5$ and $c_2 = 0.1$.\n*   **Parameter Set B:** $c_1 = 0.1$ and $c_2 = 2.5$.\n\nBased on the principles of the PSO algorithm, what is the most likely search behavior to be observed for Parameter Set A and Parameter Set B, respectively?\n\nA. Premature convergence to a single suboptimal local minimum; Stagnation, with particles clustering at multiple distinct local optima.\n\nB. Stagnation, with particles clustering at multiple distinct local optima; Premature convergence to a single suboptimal local minimum.\n\nC. Uncontrolled divergence where particles leave the bounds of the search space; Rapid and efficient convergence to the true global minimum.\n\nD. Aimless wandering of particles without any clear convergence pattern; Stagnation, with particles clustering at multiple distinct local optima.\n\nE. Rapid and efficient convergence to the true global minimum; Premature convergence to a single suboptimal local minimum.", "solution": "We analyze the standard PSO updates\n$$v_{i}(t+1)=w\\,v_{i}(t)+c_{1}r_{1}\\big(p_{i}-x_{i}(t)\\big)+c_{2}r_{2}\\big(p_{g}-x_{i}(t)\\big),$$\n$$x_{i}(t+1)=x_{i}(t)+v_{i}(t+1).$$\nAt each update, $r_{1},r_{2}\\in[0,1]$. Taking norms and using the triangle inequality gives\n$$\\|v_{i}(t+1)\\|\\leq w\\|v_{i}(t)\\|+c_{1}r_{1}\\|p_{i}-x_{i}(t)\\|+c_{2}r_{2}\\|p_{g}-x_{i}(t)\\|.$$\nSince $r_{1},r_{2}\\in[0,1]$, the cognitive and social contributions are bounded above by $c_{1}\\|p_{i}-x_{i}(t)\\|$ and $c_{2}\\|p_{g}-x_{i}(t)\\|$, respectively. Thus, the relative influence of the personal-best attraction versus the global-best attraction is controlled directly by the magnitudes of $c_{1}$ and $c_{2}$.\n\nFor Parameter Set A, $c_{1}\\gg c_{2}$. Therefore, for each particle $i$, the dominant pull in the velocity update is toward its own best $p_{i}$, since\n$$c_{1}r_{1}\\|p_{i}-x_{i}(t)\\|\\gg c_{2}r_{2}\\|p_{g}-x_{i}(t)\\|$$\nwhenever the distances are comparable, due to the coefficient disparity. Consequently, particles tend to exploit around their personal bests with relatively weak coupling to the swarm’s global best. This promotes formation of multiple clusters, each around different $p_{i}$, and limits information sharing, a known cause of stagnation where particles settle near various local optima without coordinated convergence.\n\nFor Parameter Set B, $c_{2}\\gg c_{1}$. Then the social term dominates, and for each particle\n$$c_{2}r_{2}\\|p_{g}-x_{i}(t)\\|\\gg c_{1}r_{1}\\|p_{i}-x_{i}(t)\\|.$$\nThe strong attraction toward $p_{g}$ causes rapid contraction of the swarm toward the current global best. In non-convex landscapes with many local minima, this strong social pull frequently leads to the entire swarm collapsing prematurely around a single suboptimal local minimum (premature convergence), since exploration driven by the cognitive term is weak.\n\nTherefore, under Set A (high $c_{1}$, low $c_{2}$) the likely behavior is stagnation with clustering at multiple distinct local optima, and under Set B (low $c_{1}$, high $c_{2}$) the likely behavior is premature convergence to a single suboptimal local minimum. Among the options, this corresponds to choice B.\n\nNo other listed behavior aligns with these parameter regimes: uncontrolled divergence (option C) is not implied by increasing either $c_{1}$ or $c_{2}$ alone under fixed $w$, aimless wandering (option D) contradicts the strong attraction terms, and rapid efficient convergence to the true global minimum (option E) is not generally guaranteed in high-dimensional, multimodal landscapes with such imbalanced coefficients.", "answer": "$$\\boxed{B}$$", "id": "2176755"}, {"introduction": "Differential Evolution (DE) stands out for its simplicity and power, but its performance is critically tied to the chosen mutation strategy. This exercise provides a concrete, hands-on comparison between two canonical DE strategies: the explorative `DE/rand/1` and the greedy `DE/best/1`. By calculating the next position of an agent under each strategy, you will gain direct insight into how the mathematical formulation of the mutation step dictates the algorithm's tendency to either broadly explore the search space or rapidly exploit the best-known solution [@problem_id:2176774].", "problem": "An engineer is using a Differential Evolution (DE) algorithm to find the minimum of a one-dimensional objective function. DE is a population-based metaheuristic that evolves a population of candidate solutions over generations to find an optimal solution. The behavior of the algorithm is determined by its strategy, commonly denoted as `DE/x/y/z`.\n\n-   `x` defines the base vector for mutation (e.g., `rand` for a random agent, `best` for the best agent in the current population).\n-   `y` is the number of difference vectors used.\n-   `z` specifies the crossover scheme (e.g., `bin` for binomial crossover).\n\nA single generation of the DE algorithm for a target agent $x_i$ involves three steps:\n1.  **Mutation**: A mutant vector $v_i$ is generated.\n    -   For the `DE/rand/1/bin` strategy: $v_i = x_{r1} + F \\cdot (x_{r2} - x_{r3})$, where $x_{r1}, x_{r2}, x_{r3}$ are distinct agents chosen randomly from the population, and are also different from the target agent $x_i$.\n    -   For the `DE/best/1/bin` strategy: $v_i = x_{best} + F \\cdot (x_{r1} - x_{r2})$, where $x_{best}$ is the agent with the lowest objective function value in the current population, and $x_{r1}, x_{r2}$ are distinct agents chosen randomly, different from $x_i$ and each other.\n    -   $F$ is a constant scaling factor.\n\n2.  **Crossover**: A trial vector $u_i$ is created. For the one-dimensional case with binomial crossover, the trial vector $u_i$ is formed as follows:\n    $$\n    u_i = \n    \\begin{cases} \n    v_i & \\text{if } p \\leq CR \\\\\n    x_i & \\text{if } p > CR \n    \\end{cases}\n    $$\n    where $p$ is a randomly generated number in $[0,1]$ and $CR$ is the crossover rate.\n\n3.  **Selection**: The trial vector $u_i$ replaces the target vector $x_i$ in the next generation if and only if it has a better (i.e., lower) objective function value. If $f(u_i) < f(x_i)$, the new position is $u_i$; otherwise, the agent remains at its original position $x_i$.\n\nThe engineer is minimizing the one-dimensional Ackley function, given by:\n$f(x) = -20 \\exp(-0.2 |x|) - \\exp(\\cos(2\\pi x)) + 20 + \\exp(1)$.\nAssume all arguments for trigonometric functions are in radians.\n\nThe initial population consists of four agents ($N_P=4$) at the following positions:\n$x_1 = 2.5$, $x_2 = -2.9$, $x_3 = 0.1$, $x_4 = 1.2$.\n\nThe DE parameters are set to $F=0.8$ and $CR=0.9$.\n\nConsider the agent initially at position $x_4 = 1.2$. Calculate its new position after one generation under two different DE strategies. For deterministic calculations, the \"random\" choices are fixed as follows:\n-   **Strategy 1 (`DE/rand/1/bin`)**: The random indices for mutation are $r1=1, r2=2, r3=3$. The number for the crossover check is $p=0.75$.\n-   **Strategy 2 (`DE/best/1/bin`)**: The random indices for mutation are $r1=1, r2=2$. The number for the crossover check is $p=0.75$.\n\nProvide the final position for agent $x_4$ resulting from Strategy 1 and Strategy 2, respectively. Present your answer as a row matrix containing the two numerical values. Round your final answers to three significant figures.", "solution": "We minimize the one-dimensional Ackley function\n$$\nf(x)=-20\\exp(-0.2|x|)-\\exp(\\cos(2\\pi x))+20+\\exp(1),\n$$\nwith initial agents $x_{1}=2.5$, $x_{2}=-2.9$, $x_{3}=0.1$, $x_{4}=1.2$, and parameters $F=0.8$, $CR=0.9$. For both strategies the crossover uses $p=0.75$, so in one dimension\n$$\nu_{4}=\n\\begin{cases}\nv_{4}, & p\\leq CR,\\\\\nx_{4}, & p>CR,\n\\end{cases}\n$$\nand here $p\\leq CR$, hence $u_{4}=v_{4}$ in both cases. Selection then uses $x_{4}^{\\text{new}}=u_{4}$ if and only if $f(u_{4})<f(x_{4})$.\n\nStrategy 1 (DE/rand/1/bin): Mutation with $r1=1$, $r2=2$, $r3=3$ gives\n$$\nv_{4}=x_{1}+F(x_{2}-x_{3})=2.5+0.8(-2.9-0.1)=2.5+0.8(-3.0)=0.1.\n$$\nCrossover yields $u_{4}=v_{4}=0.1$. For selection, compute\n$$\nf(1.2)=-20\\exp(-0.2\\cdot 1.2)-\\exp(\\cos(2\\pi\\cdot 1.2))+20+\\exp(1).\n$$\nUsing $\\exp(-0.24)\\approx 0.7866279$ and $\\cos(2.4\\pi)=\\cos(0.4\\pi)\\approx 0.3090169$,\n$$\nf(1.2)\\approx -20(0.7866279)-\\exp(0.3090169)+20+\\exp(1)\\approx -15.7326-1.3621+22.7183\\approx 5.6236.\n$$\nSimilarly,\n$$\nf(0.1)=-20\\exp(-0.2\\cdot 0.1)-\\exp(\\cos(2\\pi\\cdot 0.1))+20+\\exp(1)\n=-20\\exp(-0.02)-\\exp(\\cos(0.2\\pi))+20+\\exp(1).\n$$\nWith $\\exp(-0.02)\\approx 0.9801987$ and $\\cos(0.2\\pi)\\approx 0.8090169$,\n$$\nf(0.1)\\approx -20(0.9801987)-\\exp(0.8090169)+20+\\exp(1)\\approx -19.6040-2.2457+22.7183\\approx 0.8686.\n$$\nSince $f(0.1)<f(1.2)$, selection accepts $u_{4}$ and $x_{4}^{\\text{new}}=0.1$.\n\nStrategy 2 (DE/best/1/bin): First determine $x_{\\text{best}}$ in the initial population by evaluating $f$ at all agents. We already have $f(0.1)\\approx 0.8686$ and $f(1.2)\\approx 5.6236$. For completeness,\n$$\nf(2.5)=-20\\exp(-0.5)-\\exp(\\cos(5\\pi))+20+\\exp(1)\\approx -12.1306-\\exp(-1)+22.7183\\approx 10.2198,\n$$\nsince $\\exp(-0.5)\\approx 0.6065307$ and $\\cos(5\\pi)=-1$. Also,\n$$\nf(-2.9)=-20\\exp(-0.58)-\\exp(\\cos(-5.8\\pi))+20+\\exp(1)\n\\approx -20(0.559898)-\\exp(\\cos(0.2\\pi))+22.7183\\approx 9.2746.\n$$\nThus $x_{\\text{best}}=x_3=0.1$. Mutation with $r1=1$, $r2=2$ gives\n$$\nv_{4}=x_{\\text{best}}+F(x_{1}-x_{2})=0.1+0.8(2.5-(-2.9))=0.1+0.8\\cdot 5.4=4.42.\n$$\nCrossover yields $u_{4}=v_{4}=4.42$. For selection, compute\n$$\nf(4.42)=-20\\exp(-0.2\\cdot 4.42)-\\exp(\\cos(2\\pi\\cdot 4.42))+20+\\exp(1)\n=-20\\exp(-0.884)-\\exp(\\cos(0.84\\pi))+20+\\exp(1).\n$$\nUsing $\\exp(-0.884)\\approx 0.413133$ and $\\cos(0.84\\pi)=-\\cos(0.16\\pi)\\approx -0.8763075$, hence $\\exp(\\cos(0.84\\pi))\\approx \\exp(-0.8763075)\\approx 0.416147$, we obtain\n$$\nf(4.42)\\approx -20(0.413133)-0.416147+22.7183\\approx 14.0395.\n$$\nSince $f(4.42)>f(1.2)$, selection rejects $u_{4}$ and $x_{4}^{\\text{new}}=1.2$.\n\nTherefore, after one generation, the new positions of agent $x_{4}$ are $0.100$ for Strategy 1 and $1.20$ for Strategy 2, rounded to three significant figures.", "answer": "$$\\boxed{\\begin{pmatrix}0.100 & 1.20\\end{pmatrix}}$$", "id": "2176774"}, {"introduction": "In practical optimization, a \"one-size-fits-all\" approach rarely succeeds. This practice introduces the powerful concept of hybrid algorithms, which combine the strengths of different methods. You will implement a two-stage strategy: first, a coarse grid search to globally identify a promising basin of attraction, followed by a precise local refinement using the Nelder-Mead algorithm [@problem_id:2176757]. This exercise demonstrates how to synergistically couple a global search method with a local one to create a more robust and efficient optimizer.", "problem": "An engineer is developing a hybrid optimization algorithm to locate the minimum of a complex two-dimensional energy landscape. The landscape is described by the objective function $f(x, y)$, which needs to be minimized:\n$$f(x,y) = -20 \\cos(x) \\cos(y) \\exp(-((x-2.2)^2 + (y-3.3)^2))$$\nAssume all trigonometric function inputs are in radians.\n\nThe optimization process consists of two stages:\n\n**Stage 1: Global Grid Search**\nFirst, a coarse grid search is performed to identify a promising region. Evaluate the function $f(x, y)$ on a grid of points $(x_i, y_j)$ where $x_i = i$ and $y_j = j$ for all integers $i, j$ in the range $0 \\le i \\le 5$ and $0 \\le j \\le 5$. Identify the grid point $(x^*, y^*)$ that yields the lowest function value among all evaluated grid points.\n\n**Stage 2: Local Refinement with Nelder-Mead**\nNext, a local search using the Nelder-Mead algorithm is initiated from the point $(x^*, y^*)$ found in Stage 1.\nThe initial simplex for the Nelder-Mead algorithm consists of three vertices: $V_0 = (x^*, y^*)$, $V_1 = (x^*+h, y^*)$, and $V_2 = (x^*, y^*+h)$, with a step size of $h=0.1$.\nThe algorithm parameters are fixed as: reflection coefficient $\\alpha=1$, expansion coefficient $\\gamma=2$, contraction coefficient $\\rho=0.5$, and shrink coefficient $\\sigma=0.5$.\n\nPerform exactly **two** full iterations of the Nelder-Mead algorithm. An iteration consists of ordering the vertices by function value, calculating a reflection point, and then choosing one of the following moves based on the standard algorithm rules: accept reflection, expand, contract (outside or inside), or shrink.\n\nAfter completing two iterations, determine the coordinates $(x_f, y_f)$ of the vertex in the final simplex that has the lowest function value. Report the coordinate pair $(x_f, y_f)$, with each coordinate value rounded to four significant figures.", "solution": "We minimize the function\n$$f(x,y)=-20\\cos(x)\\cos(y)\\,\\exp\\!\\left(-\\left((x-2.2)^{2}+(y-3.3)^{2}\\right)\\right).$$\nStage 1 (grid search). Evaluate on integer grid points. The exponential factor is maximized near $(2.2,3.3)$, so the most relevant integer points are $(2,3)$, $(2,4)$, $(3,3)$, $(3,4)$, with squared offsets\n$$(2,3):0.13,\\quad (2,4):0.53,\\quad (3,3):0.73,\\quad (3,4):1.13.$$\nCompute representative values:\n- At $(2,3)$: $\\cos(2)\\cos(3)>0$, $\\exp(-0.13)$ large, giving $f(2,3)\\approx -20\\cdot\\cos(2)\\cos(3)\\cdot \\exp(-0.13)\\approx -7.24$.\n- At $(2,4)$: $\\cos(2)\\cos(4)>0$, $\\exp(-0.53)$, so $f(2,4)\\approx -3.20$.\n- At $(3,4)$: $f(3,4)\\approx -4.18$.\n- At $(3,3)$: both cosines are near $-1$, product $\\cos^{2}(3)\\approx 0.9800666$, and $\\exp(-0.73)\\approx 0.4819$, hence\n$$f(3,3)\\approx -20\\cdot 0.9800666\\cdot 0.4819\\approx -9.446.$$\nPoints farther from $(2.2,3.3)$ have smaller $\\exp(-r^{2})$; to beat $-9.446$ their $\\cos(x)\\cos(y)$ would need to be extremely close to $1$, which on the integer grid in $[0,5]$ only happens near $x=y=3$ (already considered). Also points with one coordinate $0$ and the other $3$ yield $\\cos(x)\\cos(y)<0$ and thus positive $f$. Therefore the grid minimum is at\n$$(x^{*},y^{*})=(3,3).$$\n\nStage 2 (Nelder–Mead with $\\alpha=1$, $\\gamma=2$, $\\rho=0.5$, $\\sigma=0.5$, $h=0.1$). Initial simplex:\n$$V_{0}=(3,3),\\quad V_{1}=(3.1,3),\\quad V_{2}=(3,3.1).$$\nEvaluate:\n- $f(V_{0})=f(3,3)\\approx -9.446$ as above.\n- $f(V_{1})=-20\\cos(3.1)\\cos(3)\\exp(-0.90)\\approx -8.045$ (using $\\cos(3.1)\\approx -0.999134$, $\\cos(3)\\approx -0.989992$, $\\exp(-0.90)\\approx 0.40657$).\n- $f(V_{2})=-20\\cos(3)\\cos(3.1)\\exp(-0.68)\\approx -10.022$ (using $\\exp(-0.68)\\approx 0.50662$).\nOrdering for minimization: best $x_{l}=V_{2}$, next $x_{g}=V_{0}$, worst $x_{h}=V_{1}$.\n\nIteration 1.\n- Centroid of best two (excluding worst): \n$$\\bar{x}=\\frac{V_{2}+V_{0}}{2}=(3,3.05).$$\n- Reflection: \n$$x_{r}=\\bar{x}+\\alpha(\\bar{x}-x_{h})=(3,3.05)+((3,3.05)-(3.1,3))=(2.9,3.10).$$\nEvaluate\n$$f(x_{r})=-20\\cos(2.9)\\cos(3.1)\\exp(-0.53)\\approx -11.431,$$\n(using $\\cos(2.9)\\approx -0.970959$, $\\cos(3.1)\\approx -0.999134$, $\\exp(-0.53)\\approx 0.58861$). Since $f(x_{r})<f(x_{l})$, attempt expansion:\n- Expansion:\n$$x_{e}=\\bar{x}+\\gamma(x_{r}-\\bar{x})=(3,3.05)+2\\cdot(-0.1,0.05)=(2.8,3.15).$$\nEvaluate\n$$f(x_{e})=-20\\cos(2.8)\\cos(3.15)\\exp(-0.3825)\\approx -12.848,$$\n(using $\\cos(2.8)\\approx -0.942222$, $\\cos(3.15)\\approx -0.999965$, $\\exp(-0.3825)\\approx 0.6823$). Since $f(x_{e})<f(x_{r})$, accept expansion and replace $x_{h}$ by $x_{e}$. New simplex: $(3,3.1)$, $(3,3)$, $(2.8,3.15)$.\n\nIteration 2.\nEvaluate and order: $f(2.8,3.15)\\approx -12.848$ (best), $f(3,3.1)\\approx -10.022$ (next), $f(3,3)\\approx -9.446$ (worst).\n- Centroid of best two:\n$$\\bar{x}=\\frac{(2.8,3.15)+(3,3.1)}{2}=(2.9,3.125).$$\n- Reflection of worst $(3,3)$:\n$$x_{r}=\\bar{x}+(\\bar{x}-x_{h})=(2.9,3.125)+(-0.1,0.125)=(2.8,3.25).$$\nEvaluate\n$$f(x_{r})=-20\\cos(2.8)\\cos(3.25)\\exp(-0.3625)\\approx -13.020,$$\n(using $\\cos(3.25)\\approx -0.994130$, $\\exp(-0.3625)\\approx 0.69594$). Since $f(x_{r})<f(x_{l})$, attempt expansion:\n- Expansion:\n$$x_{e}=\\bar{x}+\\gamma(x_{r}-\\bar{x})=(2.9,3.125)+2\\cdot(-0.1,0.125)=(2.7,3.375).$$\nEvaluate\n$$f(x_{e})=-20\\cos(2.7)\\cos(3.375)\\exp(-0.255625)\\approx -13.621,$$\n(using $\\cos(2.7)\\approx -0.904073$, $\\cos(3.375)\\approx -0.972884$, $\\exp(-0.255625)\\approx 0.7743$). Since $f(x_{e})<f(x_{r})$, accept expansion and replace the worst vertex with $x_{e}$.\n\nAfter two iterations, the final simplex contains $(3,3.1)$, $(2.8,3.15)$, and $(2.7,3.375)$. The lowest function value is at $(2.7,3.375)$. Rounding each coordinate to four significant figures yields $(2.700,\\,3.375)$.", "answer": "$$\\boxed{\\begin{pmatrix}2.700 & 3.375\\end{pmatrix}}$$", "id": "2176757"}]}