{"hands_on_practices": [{"introduction": "Before applying the full proximal gradient method, it is crucial to understand its core component: the proximal operator. This first exercise guides you through the process of deriving the operator for a fundamental function, the squared $L_2$-norm [@problem_id:2195122]. By applying basic calculus, you will see how this operator elegantly boils down to a simple scaling operation, providing concrete intuition for how proximal operators work.", "problem": "In the field of convex optimization, the proximal operator of a function $g: \\mathbb{R}^n \\to \\mathbb{R}$ is a fundamental tool used in algorithms designed to solve complex optimization problems. For a given function $g$, its proximal operator, denoted $\\text{prox}_{g}$, is a mapping from $\\mathbb{R}^n$ to $\\mathbb{R}^n$ defined for any vector $v \\in \\mathbb{R}^n$ as the solution to a small optimization problem:\n$$\n\\text{prox}_{g}(v) = \\arg\\min_{x \\in \\mathbb{R}^n} \\left( g(x) + \\frac{1}{2} \\|x-v\\|_2^2 \\right)\n$$\nHere, $\\| \\cdot \\|_2$ represents the standard Euclidean norm (also known as the L2-norm), where for a vector $u \\in \\mathbb{R}^n$, $\\|u\\|_2^2 = u^T u = \\sum_{i=1}^n u_i^2$.\n\nConsider the specific function $g(x) = \\frac{\\lambda}{2} \\|x\\|_2^2$, which represents a scaled squared Euclidean norm. The vector $x$ is in $\\mathbb{R}^n$ and $\\lambda$ is a strictly positive real constant ($\\lambda > 0$).\n\nBy directly applying the definition of the proximal operator, determine the closed-form expression for $\\text{prox}_{g}(v)$. Your final answer should be an expression in terms of the vector $v$ and the scalar constant $\\lambda$.", "solution": "We seek $\\text{prox}_{g}(v)$ for $g(x) = \\frac{\\lambda}{2}\\|x\\|_{2}^{2}$, defined by\n$$\n\\text{prox}_{g}(v) = \\arg\\min_{x \\in \\mathbb{R}^{n}} \\left( \\frac{\\lambda}{2}\\|x\\|_{2}^{2} + \\frac{1}{2}\\|x - v\\|_{2}^{2} \\right).\n$$\nDefine the objective\n$$\nf(x) = \\frac{\\lambda}{2} x^{T} x + \\frac{1}{2}(x - v)^{T}(x - v).\n$$\nUsing the standard gradients $\\nabla_{x}\\left(\\frac{1}{2}x^{T}x\\right) = x$ and $\\nabla_{x}\\left(\\frac{1}{2}\\|x - v\\|_{2}^{2}\\right) = x - v$, we obtain the gradient\n$$\n\\nabla f(x) = \\lambda x + (x - v) = (\\lambda + 1)x - v.\n$$\nSetting the first-order optimality condition $\\nabla f(x) = 0$ gives\n$$\n(\\lambda + 1)x - v = 0 \\quad \\Rightarrow \\quad x = \\frac{1}{\\lambda + 1} v.\n$$\nThe Hessian is\n$$\n\\nabla^{2} f(x) = (\\lambda + 1) I,\n$$\nwhich is positive definite since $\\lambda > 0$, ensuring the minimizer is unique. Therefore,\n$$\n\\text{prox}_{g}(v) = \\frac{1}{1 + \\lambda} \\, v.\n$$", "answer": "$$\\boxed{\\frac{1}{1+\\lambda}\\,v}$$", "id": "2195122"}, {"introduction": "With a grasp of the proximal operator itself, the next step is to see it in action within the proximal gradient algorithm. This problem asks you to perform a single iteration to solve a constrained optimization problem [@problem_id:2195110]. You will practice the two-part \"gradient-then-prox\" update, where a standard gradient step is followed by a proximal step that projects the result onto the feasible set, thereby enforcing the constraints.", "problem": "Consider the optimization problem of finding a point $x = (x_1, x_2) \\in \\mathbb{R}^2$ that minimizes a function $F(x)$ subject to non-negativity constraints on its components, i.e., $x_1 \\ge 0$ and $x_2 \\ge 0$. The function to be minimized is the squared Euclidean distance from $x$ to a target point $a$, given by $F(x) = \\frac{1}{2}\\|x - a\\|_2^2$.\n\nThis problem can be cast into the standard form for proximal algorithms, $\\min_{x} f(x) + g(x)$, by defining the smooth part as $f(x) = \\frac{1}{2}\\|x - a\\|_2^2$ and the non-smooth part $g(x)$ as the indicator function for the non-negative orthant. The indicator function $g(x)$ is zero if $x_1 \\ge 0$ and $x_2 \\ge 0$, and infinity otherwise.\n\nYou are tasked with applying the proximal gradient method to solve this problem. The iterative update rule for the proximal gradient method is given by:\n$$ x_{k+1} = \\text{prox}_{\\gamma g}(x_k - \\gamma \\nabla f(x_k)) $$\nwhere $\\gamma$ is the step size and $\\text{prox}_{\\gamma g}$ is the proximal operator associated with the function $g$.\n\nGiven the target point $a = (5, -4)$, the initial point $x_0 = (1, 1)$, and a step size of $\\gamma = 0.2$, compute the next iterate, $x_1$. Express your answer as a row vector $(x_{1,1}, x_{1,2})$, where $x_{1,1}$ and $x_{1,2}$ are the components of the vector $x_1$.", "solution": "We are minimizing $F(x)=\\frac{1}{2}\\|x-a\\|_{2}^{2}$ over the non-negative orthant. In the proximal gradient decomposition, set $f(x)=\\frac{1}{2}\\|x-a\\|_{2}^{2}$ and $g(x)=\\iota_{\\mathbb{R}_{+}^{2}}(x)$, the indicator of the feasible set $\\mathbb{R}_{+}^{2}=\\{x\\in\\mathbb{R}^{2}:x_{1}\\ge 0,\\ x_{2}\\ge 0\\}$.\n\nThe gradient of $f$ is given by\n$$\n\\nabla f(x)=x-a.\n$$\nThe proximal operator of $\\gamma g$ at a point $z$ is the Euclidean projection onto $\\mathbb{R}_{+}^{2}$:\n$$\n\\text{prox}_{\\gamma g}(z)=\\operatorname*{arg\\,min}_{y\\in\\mathbb{R}^{2}}\\left\\{\\iota_{\\mathbb{R}_{+}^{2}}(y)+\\frac{1}{2\\gamma}\\|y-z\\|_{2}^{2}\\right\\}=P_{\\mathbb{R}_{+}^{2}}(z),\n$$\nwhich is the componentwise truncation at zero:\n$$\nP_{\\mathbb{R}_{+}^{2}}(z)=\\big(\\max\\{z_{1},0\\},\\ \\max\\{z_{2},0\\}\\big).\n$$\n\nWith $a=(5,-4)$, $x_{0}=(1,1)$, and $\\gamma=0.2$, compute the gradient at $x_{0}$:\n$$\n\\nabla f(x_{0})=x_{0}-a=(1,1)-(5,-4)=(-4,5).\n$$\nPerform the gradient step:\n$$\nx_{0}-\\gamma \\nabla f(x_{0})=(1,1)-0.2\\,(-4,5)=(1+0.8,\\ 1-1)=\\left(\\frac{9}{5},\\ 0\\right).\n$$\nApply the proximal map, i.e., the projection onto $\\mathbb{R}_{+}^{2}$:\n$$\nx_{1}=\\text{prox}_{\\gamma g}\\big(x_{0}-\\gamma \\nabla f(x_{0})\\big)=P_{\\mathbb{R}_{+}^{2}}\\left(\\frac{9}{5},0\\right)=\\left(\\frac{9}{5},0\\right),\n$$\nsince both components are already non-negative.", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{9}{5}  0 \\end{pmatrix}}$$", "id": "2195110"}, {"introduction": "We now combine these concepts to tackle a famous and powerful application: the LASSO problem, used for finding sparse solutions in machine learning and statistics. This exercise involves running the proximal gradient method for several iterations, introducing the essential soft-thresholding operator for the $L_1$-norm [@problem_id:2195138]. By tracking the iterates until they converge, you will gain a practical understanding of how the algorithm proceeds from an initial guess to a final solution.", "problem": "Consider the optimization problem known as LASSO (Least Absolute Shrinkage and Selection Operator), which aims to find a vector $x \\in \\mathbb{R}^n$ that minimizes the objective function $F(x) = f(x) + g(x)$. The function is composed of a smooth part, $f(x) = \\frac{1}{2} \\|Ax - b\\|_2^2$, and a non-smooth regularization part, $g(x) = \\lambda \\|x\\|_1$, where $\\lambda > 0$ is a regularization parameter.\n\nThis problem can be solved using the proximal gradient method, which generates a sequence of iterates $\\{x_k\\}$ according to the update rule:\n$$x_{k+1} = \\text{prox}_{\\alpha g}(x_k - \\alpha \\nabla f(x_k))$$\nwhere $\\alpha > 0$ is a constant step size. The proximal operator for $g(x) = \\lambda \\|x\\|_1$ is the soft-thresholding operator, $\\text{prox}_{\\alpha g}(v) = S_{\\alpha\\lambda}(v)$, which acts component-wise on a vector $v$ as:\n$$[S_{c}(v)]_i = \\text{sign}(v_i) \\max(|v_i| - c, 0)$$\nfor a positive constant $c$.\n\nYou are tasked to simulate this algorithm for a specific configuration. Let the vector be $x \\in \\mathbb{R}^2$. The system parameters are given by:\n$$A = \\begin{pmatrix} 1  1 \\\\ 1  -1 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 6 \\\\ 2 \\end{pmatrix}, \\quad \\lambda = 4$$\nThe algorithm starts from the initial point $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ and uses a step size of $\\alpha = 0.25$.\n\nA common practical stopping criterion is to terminate the algorithm when the change between successive iterates is sufficiently small. The algorithm is said to have converged and terminates after completing iteration $k+1$ if the condition $\\|x_{k+1} - x_k\\|_2  \\epsilon$ is met for the first time, where $\\epsilon$ is a predefined tolerance. For this problem, use a tolerance of $\\epsilon = 1.0$.\n\nDetermine the total number of iterations performed before the algorithm terminates.", "solution": "The problem asks for the number of iterations the proximal gradient method takes to converge based on a given stopping criterion. The objective function is $F(x) = \\frac{1}{2} \\|Ax - b\\|_2^2 + \\lambda \\|x\\|_1$.\n\nFirst, we need to find the gradient of the smooth part, $f(x) = \\frac{1}{2} \\|Ax - b\\|_2^2$.\nThe function $f(x)$ can be written as $f(x) = \\frac{1}{2} (Ax - b)^T (Ax - b) = \\frac{1}{2} (x^T A^T - b^T)(Ax - b) = \\frac{1}{2} (x^T A^T A x - x^T A^T b - b^T A x + b^T b)$.\nThe gradient with respect to $x$ is $\\nabla f(x) = \\frac{1}{2} (2 A^T A x - A^T b - (b^T A)^T) = A^T A x - A^T b = A^T(Ax - b)$.\n\nLet's compute the matrix $A^T A$:\n$$A^T A = \\begin{pmatrix} 1  1 \\\\ 1  -1 \\end{pmatrix} \\begin{pmatrix} 1  1 \\\\ 1  -1 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 1 + 1 \\cdot 1  1 \\cdot 1 + 1 \\cdot (-1) \\\\ 1 \\cdot 1 + (-1) \\cdot 1  1 \\cdot 1 + (-1) \\cdot (-1) \\end{pmatrix} = \\begin{pmatrix} 2  0 \\\\ 0  2 \\end{pmatrix} = 2I$$\nwhere $I$ is the $2 \\times 2$ identity matrix.\nLet's also compute the vector $A^T b$:\n$$A^T b = \\begin{pmatrix} 1  1 \\\\ 1  -1 \\end{pmatrix} \\begin{pmatrix} 6 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 6 + 1 \\cdot 2 \\\\ 1 \\cdot 6 - 1 \\cdot 2 \\end{pmatrix} = \\begin{pmatrix} 8 \\\\ 4 \\end{pmatrix}$$\nSo, the gradient is $\\nabla f(x) = 2Ix - A^T b = 2x - \\begin{pmatrix} 8 \\\\ 4 \\end{pmatrix}$.\n\nThe proximal gradient iteration is $x_{k+1} = \\text{prox}_{\\alpha g}(x_k - \\alpha \\nabla f(x_k))$.\nThe argument of the proximal operator is $v_k = x_k - \\alpha \\nabla f(x_k) = x_k - \\alpha \\left(2x_k - \\begin{pmatrix} 8 \\\\ 4 \\end{pmatrix}\\right) = (1 - 2\\alpha)x_k + \\alpha \\begin{pmatrix} 8 \\\\ 4 \\end{pmatrix}$.\nThe update is $x_{k+1} = S_{\\alpha\\lambda}(v_k)$.\n\nWe are given the parameters $\\alpha = 0.25$ and $\\lambda = 4$.\nThe constant for the soft-thresholding operator is $c = \\alpha \\lambda = 0.25 \\times 4 = 1$.\nThe expression for $v_k$ becomes:\n$v_k = (1 - 2(0.25))x_k + 0.25 \\begin{pmatrix} 8 \\\\ 4 \\end{pmatrix} = 0.5 x_k + \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$.\nThe iteration rule is thus:\n$x_{k+1} = S_1\\left(0.5 x_k + \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}\\right)$.\n\nWe start with $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ and the stopping tolerance is $\\epsilon = 1.0$.\n\n**Iteration 1 (k=0):**\nWe start with $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\nFirst, calculate $v_0$:\n$$v_0 = 0.5 x_0 + \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = 0.5 \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$$\nNext, apply the soft-thresholding operator to find $x_1$:\n$$x_1 = S_1(v_0) = \\begin{pmatrix} S_1(2) \\\\ S_1(1) \\end{pmatrix} = \\begin{pmatrix} \\text{sign}(2)\\max(|2|-1,0) \\\\ \\text{sign}(1)\\max(|1|-1,0) \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot \\max(1,0) \\\\ 1 \\cdot \\max(0,0) \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$$\nNow, check the stopping criterion: $\\|x_1 - x_0\\|_2  \\epsilon$.\n$$\\|x_1 - x_0\\|_2 = \\left\\| \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} \\right\\|_2 = \\left\\| \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\right\\|_2 = \\sqrt{1^2 + 0^2} = 1$$\nThe condition is $1  1.0$, which is false. The algorithm does not terminate. We proceed to the next iteration.\n\n**Iteration 2 (k=1):**\nWe start with $x_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\nFirst, calculate $v_1$:\n$$v_1 = 0.5 x_1 + \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = 0.5 \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0.5 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2.5 \\\\ 1 \\end{pmatrix}$$\nNext, apply the soft-thresholding operator to find $x_2$:\n$$x_2 = S_1(v_1) = \\begin{pmatrix} S_1(2.5) \\\\ S_1(1) \\end{pmatrix} = \\begin{pmatrix} \\text{sign}(2.5)\\max(|2.5|-1,0) \\\\ \\text{sign}(1)\\max(|1|-1,0) \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot \\max(1.5,0) \\\\ 1 \\cdot \\max(0,0) \\end{pmatrix} = \\begin{pmatrix} 1.5 \\\\ 0 \\end{pmatrix}$$\nNow, check the stopping criterion: $\\|x_2 - x_1\\|_2  \\epsilon$.\n$$\\|x_2 - x_1\\|_2 = \\left\\| \\begin{pmatrix} 1.5 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\right\\|_2 = \\left\\| \\begin{pmatrix} 0.5 \\\\ 0 \\end{pmatrix} \\right\\|_2 = \\sqrt{0.5^2 + 0^2} = 0.5$$\nThe condition is $0.5  1.0$, which is true. The algorithm terminates.\n\nThe first iteration (k=0) produced $x_1$. The second iteration (k=1) produced $x_2$. The stopping condition was met after the second iteration was completed. Therefore, the total number of iterations performed is 2.", "answer": "$$\\boxed{2}$$", "id": "2195138"}]}