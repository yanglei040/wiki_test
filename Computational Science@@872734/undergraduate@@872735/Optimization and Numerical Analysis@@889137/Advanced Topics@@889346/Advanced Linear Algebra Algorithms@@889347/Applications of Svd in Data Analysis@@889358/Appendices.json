{"hands_on_practices": [{"introduction": "A crucial first step in data analysis is understanding the relationships between different features in a dataset. The Singular Value Decomposition (SVD) provides a powerful diagnostic tool for this purpose by revealing the data's intrinsic dimensionality. This exercise demonstrates how the singular values of a centered data matrix can be used to definitively identify collinearity, a situation where two or more features are linearly dependent. Mastering this connection is fundamental to understanding dimensionality reduction and why techniques like Principal Component Analysis (PCA) work [@problem_id:2154133].", "problem": "A data scientist is analyzing a dataset containing measurements of two properties, Feature A and Feature B, for four different samples. The dataset is provided below:\n\n- Sample 1: Feature A = 3, Feature B = 3\n- Sample 2: Feature A = 5, Feature B = 7\n- Sample 3: Feature A = 2, Feature B = 1\n- Sample 4: Feature A = 6, Feature B = 9\n\nTo analyze the structure of the data, the scientist decides to investigate its singular values. First, create a centered data matrix $X$ of size $4 \\times 2$, where each column corresponds to a feature and each row corresponds to a sample. Centering is performed by subtracting the mean of each feature (column) from all values in that feature's column.\n\nThe singular values of a matrix, denoted by $\\sigma_i$, are a measure of the data's variance along its principal axes and are defined as the non-negative square roots of the eigenvalues of the matrix $X^T X$. A singular value of zero has a specific implication for the relationship between the matrix's columns.\n\nAfter calculating the singular values of the centered data matrix $X$, determine which of the following statements accurately describes the relationship between Feature A and Feature B.\n\nA. The features are linearly independent.\n\nB. The features are perfectly collinear.\n\nC. The features exhibit a strong but not perfect positive correlation.\n\nD. The features exhibit a strong but not perfect negative correlation.\n\nE. The data is insufficient to determine the relationship.", "solution": "Let the uncentered data matrix with rows as samples and columns as features be\n$$\nX_{\\text{raw}}=\\begin{pmatrix}\n3 & 3\\\\\n5 & 7\\\\\n2 & 1\\\\\n6 & 9\n\\end{pmatrix}.\n$$\nCompute the column means\n$$\n\\mu_{A}=\\frac{3+5+2+6}{4}=4,\\qquad \\mu_{B}=\\frac{3+7+1+9}{4}=5.\n$$\nCenter each column by subtracting its mean to obtain the centered columns\n$$\na=\\begin{pmatrix}3-4\\\\5-4\\\\2-4\\\\6-4\\end{pmatrix}=\\begin{pmatrix}-1\\\\1\\\\-2\\\\2\\end{pmatrix},\\qquad\nb=\\begin{pmatrix}3-5\\\\7-5\\\\1-5\\\\9-5\\end{pmatrix}=\\begin{pmatrix}-2\\\\2\\\\-4\\\\4\\end{pmatrix}.\n$$\nObserve that\n$$\nb=2a,\n$$\nso the centered columns are exactly collinear. The centered data matrix is\n$$\nX=\\begin{pmatrix}\n-1 & -2\\\\\n\\phantom{-}1 & \\phantom{-}2\\\\\n-2 & -4\\\\\n\\phantom{-}2 & \\phantom{-}4\n\\end{pmatrix}.\n$$\nCompute\n$$\nX^{T}X=\\begin{pmatrix}\na^{T}a & a^{T}b\\\\\nb^{T}a & b^{T}b\n\\end{pmatrix}\n=\\begin{pmatrix}\n10 & 20\\\\\n20 & 40\n\\end{pmatrix},\n$$\nsince\n$$\na^{T}a=(-1)^{2}+1^{2}+(-2)^{2}+2^{2}=10,\\quad b^{T}b=(-2)^{2}+2^{2}+(-4)^{2}+4^{2}=40,\\quad a^{T}b=(-1)(-2)+1\\cdot 2+(-2)(-4)+2\\cdot 4=20.\n$$\nThe eigenvalues of $X^{T}X$ solve\n$$\n\\det\\begin{pmatrix}10-\\lambda & 20\\\\\n20 & 40-\\lambda\\end{pmatrix}=(10-\\lambda)(40-\\lambda)-20^{2}=0,\n$$\nwhich simplifies to\n$$\n\\lambda^{2}-50\\lambda=0 \\quad\\Rightarrow\\quad \\lambda\\in\\{0,50\\}.\n$$\nHence the singular values are\n$$\n\\sigma_{1}=\\sqrt{50},\\qquad \\sigma_{2}=0.\n$$\nA zero singular value implies the columns of $X$ are linearly dependent, i.e., the features are perfectly collinear. Therefore, the correct statement is option B.", "answer": "$$\\boxed{B}$$", "id": "2154133"}, {"introduction": "Beyond diagnosing data structure, SVD is instrumental in solving practical problems like model fitting. When we fit a linear model to data, we often have more data points than model parameters, resulting in an overdetermined system of equations that has no exact solution. This practice guides you through using SVD to compute the Moore-Penrose pseudoinverse, which gives the optimal least-squares solution to such systems [@problem_id:2154101]. This technique is the foundation of linear regression and many other core machine learning algorithms.", "problem": "A data scientist is attempting to model a relationship between a feature $x$ and a target variable $y$ using a simple linear model of the form $y = mx + c$, where $m$ is the slope and $c$ is the y-intercept. To determine the optimal values for $m$ and $c$, the scientist has collected the following four data points $(x, y)$: $(-2, -1)$, $(-1, 1)$, $(1, 2)$, and $(2, 4)$.\n\nEach data point provides a linear equation relating $m$ and $c$. Since there are more data points (equations) than unknown parameters, this forms an overdetermined system of linear equations of the form $A\\mathbf{x} = \\mathbf{b}$, where the solution vector is $\\mathbf{x} = \\begin{pmatrix} m \\\\ c \\end{pmatrix}$. The goal is to find the least-squares solution for $\\mathbf{x}$, which minimizes the sum of squared differences between the model's predictions and the actual $y$ values.\n\nYour task is to find this least-squares solution. You must follow a specific procedure: first, construct the matrix $A$ and the vector $\\mathbf{b}$ from the given data. Then, compute the Singular Value Decomposition (SVD) of $A$. Using the SVD, find the Moore-Penrose pseudoinverse $A^\\dagger$. Finally, calculate the least-squares solution vector $\\mathbf{x} = A^\\dagger\\mathbf{b}$.\n\nProvide the values for the slope $m$ and the y-intercept $c$ in the form of a single closed-form analytic expression.", "solution": "We model $y$ as $y = mx + c$. For each data point $(x_{i},y_{i})$, this gives $m x_{i} + c = y_{i}$. With the given four points, the matrix $A$ and vector $\\mathbf{b}$ are\n$$\nA = \\begin{pmatrix}\n-2 & 1 \\\\\n-1 & 1 \\\\\n1 & 1 \\\\\n2 & 1\n\\end{pmatrix},\\quad\n\\mathbf{b} = \\begin{pmatrix}\n-1 \\\\\n1 \\\\\n2 \\\\\n4\n\\end{pmatrix}.\n$$\nWe compute the singular value decomposition $A = U \\Sigma V^{T}$. Start from $A^{T}A$:\n$$\nA^{T}A = \\begin{pmatrix}\n\\sum x_{i}^{2} & \\sum x_{i} \\\\\n\\sum x_{i} & \\sum 1\n\\end{pmatrix}\n= \\begin{pmatrix}\n10 & 0 \\\\\n0 & 4\n\\end{pmatrix}.\n$$\nThus the eigenvalues of $A^{T}A$ are $10$ and $4$ with eigenvectors given by the standard basis, so we can take\n$$\nV = \\begin{pmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{pmatrix},\\qquad\n\\Sigma_{r} = \\begin{pmatrix}\n\\sqrt{10} & 0 \\\\\n0 & 2\n\\end{pmatrix}.\n$$\nWith the thin SVD, $U_{r}$ has columns $u_{1}$ and $u_{2}$ given by normalizing the columns of $A$:\n$$\nu_{1} = \\frac{1}{\\sqrt{10}}\\begin{pmatrix}-2\\\\-1\\\\1\\\\2\\end{pmatrix},\\qquad\nu_{2} = \\frac{1}{2}\\begin{pmatrix}1\\\\1\\\\1\\\\1\\end{pmatrix},\n$$\nso $U_{r} = \\begin{pmatrix}u_{1} & u_{2}\\end{pmatrix}$. Therefore, the Moore-Penrose pseudoinverse is\n$$\nA^{\\dagger} = V \\Sigma_{r}^{-1} U_{r}^{T} = \\begin{pmatrix}\n\\frac{1}{\\sqrt{10}} & 0 \\\\\n0 & \\frac{1}{2}\n\\end{pmatrix}\n\\begin{pmatrix}\nu_{1}^{T} \\\\\nu_{2}^{T}\n\\end{pmatrix}.\n$$\nThe least-squares solution is $\\mathbf{x} = A^{\\dagger}\\mathbf{b} = V \\Sigma_{r}^{-1} U_{r}^{T}\\mathbf{b} = \\Sigma_{r}^{-1} U_{r}^{T}\\mathbf{b}$. Compute the projections:\n$$\nu_{1}^{T}\\mathbf{b} = \\frac{1}{\\sqrt{10}}\\big((-2)(-1)+(-1)(1)+(1)(2)+(2)(4)\\big) = \\frac{11}{\\sqrt{10}},\n$$\n$$\nu_{2}^{T}\\mathbf{b} = \\frac{1}{2}\\big((-1)+1+2+4\\big) = 3.\n$$\nMultiply by $\\Sigma_{r}^{-1}$:\n$$\n\\mathbf{x} = \\begin{pmatrix}\n\\frac{1}{\\sqrt{10}} & 0 \\\\\n0 & \\frac{1}{2}\n\\end{pmatrix}\n\\begin{pmatrix}\n\\frac{11}{\\sqrt{10}} \\\\\n3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\frac{11}{10} \\\\\n\\frac{3}{2}\n\\end{pmatrix}.\n$$\nThus the least-squares slope and intercept are $m = \\frac{11}{10}$ and $c = \\frac{3}{2}$.", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{11}{10} \\\\ \\frac{3}{2}\\end{pmatrix}}$$", "id": "2154101"}, {"introduction": "Modern data is often messy, containing not just small measurement noise but also large, sparse errors or outliers. Advanced techniques like Robust Principal Component Analysis (RPCA) are designed to handle such corruptions by separating a data matrix into its underlying low-rank structure and a sparse error component. This exercise introduces a key step in this process: the Singular Value Thresholding (SVT) operator, which intelligently shrinks singular values to filter out noise [@problem_id:2154141]. This practice provides insight into how SVD serves as an active component in sophisticated, iterative algorithms for data cleaning and recovery.", "problem": "An essential technique in modern data analysis and machine learning is Robust Principal Component Analysis (RPCA), which aims to decompose a data matrix $M$ into a low-rank component $L$ (representing the principal structure) and a sparse component $S$ (representing outliers or noise). A key step in many RPCA algorithms involves the application of the Singular Value Thresholding (SVT) operator.\n\nThe SVT operator, denoted $D_{\\tau}(X)$ for a given matrix $X$ and a threshold $\\tau > 0$, is defined as follows:\n1. Compute the Singular Value Decomposition (SVD) of $X$: $X = U \\Sigma V^T$, where $\\Sigma$ is the diagonal matrix of singular values $\\sigma_i$.\n2. Form a new diagonal matrix $\\Sigma_{\\tau}$ by applying a soft-thresholding function to each singular value: $(\\Sigma_{\\tau})_{ii} = \\max(0, \\sigma_i - \\tau)$.\n3. Reconstruct the matrix: $D_{\\tau}(X) = U \\Sigma_{\\tau} V^T$.\n\nConsider a data matrix $M$ that has been corrupted by sparse noise. We wish to perform one step of a recovery algorithm to obtain an estimate of the low-rank component, $L$, by computing $L = D_{\\tau}(M)$.\n\nYou are given the following data matrix $M$:\n$$\nM = \\begin{pmatrix} 6.0 & 1.0 & 7.0 \\\\ 2.0 & 8.0 & 0.0 \\\\ 7.0 & 0.0 & 7.0 \\end{pmatrix}\n$$\nThe SVD of this matrix, $M = U \\Sigma V^T$, has singular values $\\sigma_1 = 12.0$, $\\sigma_2 = 6.0$, and $\\sigma_3 = 5.0$. The corresponding orthogonal matrices $U$ and $V^T$ are given by:\n$$\nU = \\begin{pmatrix} -0.6667 & -0.3333 & -0.6667 \\\\ -0.3333 & 0.6667 & 0.6667 \\\\ -0.6667 & 0.6667 & -0.3333 \\end{pmatrix}\n$$\n$$\nV^T = \\begin{pmatrix} -0.6667 & -0.3333 & -0.6667 \\\\ -0.6667 & 0.6667 & -0.3333 \\\\ -0.3333 & 0.6667 & 0.6667 \\end{pmatrix}\n$$\nUsing a threshold value of $\\tau = 8.0$, compute the estimated low-rank matrix $L = D_{\\tau}(M)$. Express your answer as a matrix, with each element rounded to three significant figures.", "solution": "We use the Singular Value Thresholding operator definition: for $M=U\\Sigma V^{T}$ and threshold $\\tau>0$,\n$$\nD_{\\tau}(M)=U\\,\\Sigma_{\\tau}\\,V^{T},\\quad \\text{with}\\quad \\Sigma_{\\tau}=\\operatorname{diag}\\big(\\max(0,\\sigma_{i}-\\tau)\\big).\n$$\nGiven singular values $\\sigma_{1}=12.0$, $\\sigma_{2}=6.0$, $\\sigma_{3}=5.0$ and $\\tau=8.0$, we obtain\n$$\n\\Sigma_{\\tau}=\\operatorname{diag}(12-8,\\max(0,6-8),\\max(0,5-8))=\\operatorname{diag}(4,0,0).\n$$\nTherefore,\n$$\nL=D_{\\tau}(M)=U\\,\\Sigma_{\\tau}\\,V^{T}=4\\,u_{1}v_{1}^{T},\n$$\nwhere $u_{1}$ and $v_{1}$ are the first left and right singular vectors (the first column of $U$ and the first column of $V$, equivalently the first row of $V^{T}$ transposed). From the data,\n$$\nu_{1}=\\begin{pmatrix}-0.6667\\\\ -0.3333\\\\ -0.6667\\end{pmatrix},\\qquad v_{1}=\\begin{pmatrix}-0.6667\\\\ -0.3333\\\\ -0.6667\\end{pmatrix}.\n$$\nCompute the outer product $u_{1}v_{1}^{T}$ entrywise:\n$$\nu_{1}v_{1}^{T}=\\begin{pmatrix}\n0.6667^{2} & 0.6667\\cdot 0.3333 & 0.6667^{2}\\\\\n0.3333\\cdot 0.6667 & 0.3333^{2} & 0.3333\\cdot 0.6667\\\\\n0.6667^{2} & 0.6667\\cdot 0.3333 & 0.6667^{2}\n\\end{pmatrix}\n=\\begin{pmatrix}\n0.44448889 & 0.22221111 & 0.44448889\\\\\n0.22221111 & 0.11108889 & 0.22221111\\\\\n0.44448889 & 0.22221111 & 0.44448889\n\\end{pmatrix}.\n$$\nMultiplying by $4$ gives\n$$\nL=4\\,u_{1}v_{1}^{T}=\\begin{pmatrix}\n1.77795556 & 0.88884444 & 1.77795556\\\\\n0.88884444 & 0.44435556 & 0.88884444\\\\\n1.77795556 & 0.88884444 & 1.77795556\n\\end{pmatrix}.\n$$\nRounding each entry to three significant figures yields\n$$\nL\\approx\\begin{pmatrix}\n1.78 & 0.889 & 1.78\\\\\n0.889 & 0.444 & 0.889\\\\\n1.78 & 0.889 & 1.78\n\\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}1.78 & 0.889 & 1.78 \\\\ 0.889 & 0.444 & 0.889 \\\\ 1.78 & 0.889 & 1.78\\end{pmatrix}}$$", "id": "2154141"}]}