{"hands_on_practices": [{"introduction": "To effectively use an algorithm, we must first understand its internal mechanics. This exercise provides a blueprint of the Randomized SVD process, asking you to trace the dimensions of the matrices at each stage. By following the flow from the full-sized matrix $A$ to the compact projected matrix $B$, you'll build a clear mental model of how rSVD achieves its dramatic data compression. [@problem_id:2196156]", "problem": "The Randomized Singular Value Decomposition (rSVD) is a powerful algorithm for computing a low-rank approximation of a large matrix. Consider a matrix $A \\in \\mathbb{R}^{m \\times n}$ with $m=1000$ and $n=500$. We wish to find a rank-$k$ approximation of $A$ using rSVD with a target rank of $k=10$ and an oversampling parameter of $p=5$.\n\nThe main steps of this variant of the rSVD algorithm are as follows:\n1. Generate a random Gaussian matrix $\\Omega$.\n2. Form a sketched matrix $Y = A\\Omega$.\n3. Compute an orthonormal basis $Q$ for the column space of $Y$ (e.g., using a QR decomposition).\n4. Form the smaller projected matrix $B = Q^T A$.\n5. Compute the economy-size Singular Value Decomposition (SVD) of the matrix $B$, which is given by $B = \\tilde{U}\\Sigma V^T$.\n\nBased on the parameters provided, determine the dimensions of the following matrices in the order they are listed: $\\Omega$, $Y$, $B$, $\\tilde{U}$, and $V$. The dimensions should be represented as a pair (rows, columns).\n\nSelect the option that correctly lists the sequence of dimensions for $(\\Omega, Y, B, \\tilde{U}, V)$.\n\nA. $(500, 15), (1000, 15), (15, 500), (15, 15), (500, 15)$\n\nB. $(500, 10), (1000, 10), (10, 500), (10, 10), (500, 10)$\n\nC. $(500, 15), (1000, 15), (15, 500), (15, 15), (500, 500)$\n\nD. $(500, 15), (1000, 15), (500, 15), (500, 15), (15, 15)$\n\nE. $(1000, 15), (500, 15), (15, 1000), (15, 15), (1000, 15)$", "solution": "Let $A \\in \\mathbb{R}^{m \\times n}$ with $m=1000$ and $n=500$. The target rank is $k=10$ and the oversampling parameter is $p=5$. Define the sketch size $l$ by $l=k+p$, so $l=15$.\n\nStep 1 (random test matrix): To form a column sketch $Y=A\\Omega$, the Gaussian matrix must have size $\\Omega \\in \\mathbb{R}^{n \\times l}$ so that the product is defined. Therefore, $\\Omega$ has dimensions $(500, 15)$.\n\nStep 2 (sketched matrix): Compute\n$$\nY = A \\Omega,\n$$\nwhere $A \\in \\mathbb{R}^{1000 \\times 500}$ and $\\Omega \\in \\mathbb{R}^{500 \\times 15}$. Hence, $Y \\in \\mathbb{R}^{1000 \\times 15}$, i.e., $(1000, 15)$.\n\nStep 3 (orthonormal basis): An orthonormal basis $Q$ for the columns of $Y$ has the same number of rows as $A$ and $l$ columns, so $Q \\in \\mathbb{R}^{1000 \\times 15}$.\n\nStep 4 (projected matrix): Form\n$$\nB = Q^{T} A.\n$$\nHere, $Q^{T} \\in \\mathbb{R}^{15 \\times 1000}$ and $A \\in \\mathbb{R}^{1000 \\times 500}$, so $B \\in \\mathbb{R}^{15 \\times 500}$, i.e., $(15, 500)$.\n\nStep 5 (economy-size SVD of $B$): For $B \\in \\mathbb{R}^{l \\times n}$ with $l \\leq n$, the economy SVD\n$$\nB = \\tilde{U} \\Sigma V^{T}\n$$\nhas $\\tilde{U} \\in \\mathbb{R}^{l \\times l}$ and $V \\in \\mathbb{R}^{n \\times l}$. Therefore, $\\tilde{U}$ has dimensions $(15, 15)$ and $V$ has dimensions $(500, 15)$.\n\nCollecting these, the sequence $(\\Omega, Y, B, \\tilde{U}, V)$ is $(500, 15), (1000, 15), (15, 500), (15, 15), (500, 15)$, which matches option A.", "answer": "$$\\boxed{A}$$", "id": "2196156"}, {"introduction": "The primary motivation for using randomized algorithms is speed, and smart application is key to maximizing this advantage. This practice explores a crucial strategic choice: is it more efficient to apply rSVD to a matrix $A$ directly, or to its transpose $A^T$? By analyzing the computational costs for \"tall-and-skinny\" versus \"short-and-fat\" matrices, you will learn how to tailor your approach to the specific shape of your data for optimal performance. [@problem_id:2196144]", "problem": "In numerical linear algebra, Randomized Singular Value Decomposition (rSVD) provides an efficient way to compute a low-rank approximation of a large matrix. A key step in many rSVD algorithms involves finding an orthonormal basis $Q$ that approximates the range (column space) of the matrix.\n\nConsider a matrix $A \\in \\mathbb{R}^{m \\times n}$. We want to find a rank-$k$ approximation where $k$ is much smaller than both $m$ and $n$. We can approach this in two ways:\n\n**Strategy 1:** Directly apply a randomized procedure to $A$.\n**Strategy 2:** Apply the same randomized procedure to the transpose $A^T$, and use the fact that the left singular vectors of $A^T$ are the right singular vectors of $A$.\n\nLet's analyze the computational cost of the core \"range finding\" part of the algorithm for both strategies. The procedure is as follows:\n1. Given an input matrix $M$, generate a random Gaussian matrix $\\Omega$ of appropriate size. Form the \"sketch\" matrix $Y = M \\Omega$. The number of columns in $\\Omega$ is $k$.\n2. Compute an orthonormal basis $Q$ for the columns of $Y$ by performing a QR factorization on $Y$.\n\nFor your analysis, use the following standard estimates for floating-point operations (flops):\n- The multiplication of a $p \\times q$ matrix with a $q \\times r$ matrix costs approximately $2pqr$ flops.\n- The QR factorization of a $p \\times q$ matrix (with $p \\ge q$) costs approximately $2pq^2$ flops.\n\nThe total cost for each strategy is the sum of the costs of Step 1 and Step 2. Compare the leading-order costs of these two strategies for two distinct types of matrices:\n- A \"tall-and-skinny\" matrix, where $m \\gg n$.\n- A \"short-and-fat\" matrix, where $n \\gg m$.\n\nIn both cases, assume that $m \\gg k$ and $n \\gg k$. Based on your comparison of the leading-order computational costs, which of the following statements is correct?\n\nA. Strategy 1 is computationally cheaper for a \"tall-and-skinny\" matrix, while Strategy 2 is cheaper for a \"short-and-fat\" matrix.\n\nB. Strategy 2 is computationally cheaper for a \"tall-and-skinny\" matrix, while Strategy 1 is cheaper for a \"short-and-fat\" matrix.\n\nC. Strategy 1 is always computationally cheaper, regardless of the matrix shape.\n\nD. Strategy 2 is always computationally cheaper, regardless of the matrix shape.\n\nE. Both strategies have the same leading-order computational cost in both scenarios.", "solution": "We denote the target rank by $k$ with $m \\gg k$ and $n \\gg k$. The core range-finding procedure consists of:\n1) Multiply an input matrix by a Gaussian test matrix with $k$ columns.\n2) Compute a QR factorization of the resulting sketch.\n\nFlop models:\n- Matrix product of size $p \\times q$ by $q \\times r$: $2pqr$ flops.\n- QR of $p \\times q$ with $p \\ge q$: $2pq^{2}$ flops.\n\nStrategy 1 (apply to $A \\in \\mathbb{R}^{m \\times n}$):\n- Step 1: $\\Omega \\in \\mathbb{R}^{n \\times k}$, $Y = A\\Omega \\in \\mathbb{R}^{m \\times k}$. Cost: $2mnk$.\n- Step 2: QR of $Y$ ($m \\times k$ with $m \\ge k$). Cost: $2mk^{2}$.\nTotal:\n$$\nC_{1}=2mnk+2mk^{2}.\n$$\n\nStrategy 2 (apply to $A^{T} \\in \\mathbb{R}^{n \\times m}$):\n- Step 1: $\\Omega \\in \\mathbb{R}^{m \\times k}$, $Y' = A^{T}\\Omega \\in \\mathbb{R}^{n \\times k}$. Cost: $2nmk=2mnk$.\n- Step 2: QR of $Y'$ ($n \\times k$ with $n \\ge k$). Cost: $2nk^{2}$.\nTotal:\n$$\nC_{2}=2mnk+2nk^{2}.\n$$\n\nComparison by matrix shape:\n- Tall-and-skinny ($m \\gg n$ with $n \\gg k$): Both $C_{1}$ and $C_{2}$ share the same leading term $2mnk$. The next term differs: $2mk^{2}$ vs $2nk^{2}$. Since $m \\gg n$, we have $2nk^{2} \\ll 2mk^{2}$, hence $C_{2}<C_{1}$; Strategy 2 is cheaper.\n- Short-and-fat ($n \\gg m$ with $m \\gg k$): Again the leading term $2mnk$ is shared. Now $2mk^{2} \\ll 2nk^{2}$, hence $C_{1}<C_{2}$; Strategy 1 is cheaper.\n\nTherefore, Strategy 2 is cheaper for tall-and-skinny matrices, and Strategy 1 is cheaper for short-and-fat matrices.", "answer": "$$\\boxed{B}$$", "id": "2196144"}, {"introduction": "The magic of rSVD lies in the ability of a random matrix to effectively capture the most important features of a much larger data matrix. But what happens when this core assumption is violated? This exercise challenges you to think critically about the algorithm's limits by constructing a specific, non-random, scenario where the method fails catastrophically. Exploring such a failure case provides a deeper insight into why randomness is a fundamental pillar of the algorithm's success. [@problem_id:2196157]", "problem": "The first step in a common Randomized Singular Value Decomposition (rSVD) algorithm for approximating an $m \\times n$ matrix $A$ with a rank-$k$ matrix involves generating a test matrix $\\Omega$ of size $n \\times \\ell$ (where $\\ell = k+p$ for some oversampling parameter $p$) and forming the sample matrix $Y = A\\Omega$. The columns of $Y$ are then used to form an orthonormal basis $Q$ that approximates the range of $A$. This method can fail catastrophically if the choice of $\\Omega$ is such that the resulting sample matrix $Y$ does not contain any useful information about the range of $A$.\n\nConsider the matrix $A$ given by:\n$$A = \\begin{pmatrix} 1 & 2 & -3 \\\\ -2 & 1 & 1 \\\\ -1 & 3 & -2 \\end{pmatrix}$$\nWe intend to find a rank-1 approximation ($k=1$) using a test matrix $\\Omega$ of size $3 \\times 2$ (implying an oversampling parameter $p=1$). Your task is to construct a specific, non-random, non-zero $3 \\times 2$ matrix $\\Omega$ that causes the algorithm to fail completely by yielding a sample matrix $Y=A\\Omega$ that is the zero matrix. To make the answer unique, provide the matrix $\\Omega$ whose entries are all the smallest possible positive integer.", "solution": "We want the sample matrix $Y=A\\Omega$ to be the zero matrix. Since $Y=A\\Omega=[A\\omega_{1}\\;A\\omega_{2}]$, where $\\omega_{1}$ and $\\omega_{2}$ are the columns of $\\Omega$, this occurs if and only if each column of $\\Omega$ lies in the right null space of $A$, i.e., $A\\omega_{j}=0$ for $j=1,2$.\n\nCompute the right null space of $A$ by solving $Ax=0$ with $x=(x_{1},x_{2},x_{3})^{\\top}$:\n$$\n\\begin{cases}\nx_{1}+2x_{2}-3x_{3}=0,\\\\\n-2x_{1}+x_{2}+x_{3}=0,\\\\\n-x_{1}+3x_{2}-2x_{3}=0.\n\\end{cases}\n$$\nFrom the second equation, $x_{2}=2x_{1}-x_{3}$. Substitute into the first:\n$$\nx_{1}+2(2x_{1}-x_{3})-3x_{3}=0 \\;\\Rightarrow\\; x_{1}+4x_{1}-2x_{3}-3x_{3}=0 \\;\\Rightarrow\\; 5x_{1}-5x_{3}=0 \\;\\Rightarrow\\; x_{1}=x_{3}.\n$$\nThen $x_{2}=2x_{1}-x_{3}=x_{1}$. The third equation is redundant because the rows satisfy $r_{3}=r_{1}+r_{2}$, so $\\operatorname{rank}(A)=2$ and the null space is one-dimensional:\n$ \\ker(A)=\\operatorname{span}\\{(1,1,1)^{\\top}\\} $.\n\nThus any nonzero vector in the null space is a scalar multiple of $v=(1,1,1)^{\\top}$. To ensure $Y=0$, choose both columns of $\\Omega$ to be $v$. To make the answer unique and to have all entries be the smallest possible positive integer, take the scalar to be $1$. Therefore,\n$$\n\\Omega=\\begin{pmatrix}\n1 & 1\\\\\n1 & 1\\\\\n1 & 1\n\\end{pmatrix},\n$$\nand indeed $Y=A\\Omega=[Av\\;Av]=[0\\;0]=0$, causing the algorithm to fail completely.", "answer": "$$\\boxed{\\begin{pmatrix}1 & 1 \\\\ 1 & 1 \\\\ 1 & 1\\end{pmatrix}}$$", "id": "2196157"}]}