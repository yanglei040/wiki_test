{"hands_on_practices": [{"introduction": "To truly understand an algorithm, we must start with its fundamental operations. This first practice focuses on executing a single, complete step of the Arnoldi iteration. By working through the normalization of the starting vector, the projection, and the generation of the next orthonormal vector, you will build a concrete foundation for the entire process [@problem_id:2154392].", "problem": "In numerical linear algebra, the Arnoldi iteration is an algorithm for building an orthonormal basis of the Krylov subspace generated by a matrix $A$ and a vector $b$. Consider the square matrix $A$ and the initial vector $b$ given by:\n$$ A = \\begin{pmatrix} 1  1  0 \\\\ 1  1  0 \\\\ 0  0  1 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 1 \\\\ 2 \\\\ 2 \\end{pmatrix} $$\nThe first step in the Arnoldi process is to normalize the initial vector to obtain the first basis vector, $v_1 = b / \\|b\\|_2$, where $\\|\\cdot\\|_2$ denotes the standard Euclidean norm. The iteration then proceeds to generate subsequent vectors and the entries of an upper Hessenberg matrix $H$.\n\nPerform one complete step of the Arnoldi iteration to compute the Hessenberg matrix entry $h_{1,1}$ and the second Arnoldi basis vector $v_2$.\n\nYour final answer should be given as a single row matrix containing four exact, symbolic values in the following order: the value of $h_{1,1}$, followed by the first, second, and third components of the vector $v_2$.", "solution": "We apply one step of the Arnoldi iteration.\n\nFirst, normalize $b$ to obtain $v_{1}$. The Euclidean norm is\n$$\n\\|b\\|_{2}=\\sqrt{1^{2}+2^{2}+2^{2}}=\\sqrt{9}=3,\n$$\nso\n$$\nv_{1}=\\frac{b}{\\|b\\|_{2}}=\\begin{pmatrix}\\frac{1}{3}\\\n$$4pt]\\frac{2}{3}\\\n$$4pt]\\frac{2}{3}\\end{pmatrix}.\n$$\nCompute $w=A v_{1}$:\n$$\nw=\\begin{pmatrix}1  1  0\\\\ 1  1  0\\\\ 0  0  1\\end{pmatrix}\\begin{pmatrix}\\frac{1}{3}\\\n$$4pt]\\frac{2}{3}\\\n$$4pt]\\frac{2}{3}\\end{pmatrix}=\\begin{pmatrix}1\\\n$$4pt]1\\\n$$4pt]\\frac{2}{3}\\end{pmatrix}.\n$$\nThe Hessenberg entry is\n$$\nh_{1,1}=v_{1}^{\\top}w=\\frac{1}{3}\\cdot 1+\\frac{2}{3}\\cdot 1+\\frac{2}{3}\\cdot\\frac{2}{3}= \\frac{13}{9}.\n$$\nOrthogonalize and normalize to get $v_{2}$. Define\n$$\nr=w-h_{1,1}v_{1}=\\begin{pmatrix}1\\\n$$4pt]1\\\n$$4pt]\\frac{2}{3}\\end{pmatrix}-\\frac{13}{9}\\begin{pmatrix}\\frac{1}{3}\\\n$$4pt]\\frac{2}{3}\\\n$$4pt]\\frac{2}{3}\\end{pmatrix}=\\begin{pmatrix}\\frac{14}{27}\\\n$$4pt]\\frac{1}{27}\\\n$$4pt]-\\frac{8}{27}\\end{pmatrix}.\n$$\nIts norm is\n$$\n\\|r\\|_{2}=\\sqrt{\\left(\\frac{14}{27}\\right)^{2}+\\left(\\frac{1}{27}\\right)^{2}+\\left(-\\frac{8}{27}\\right)^{2}}=\\frac{1}{27}\\sqrt{261}=\\frac{\\sqrt{29}}{9}.\n$$\nThus\n$$\nv_{2}=\\frac{r}{\\|r\\|_{2}}=\\frac{9}{\\sqrt{29}}\\begin{pmatrix}\\frac{14}{27}\\\n$$4pt]\\frac{1}{27}\\\n$$4pt]-\\frac{8}{27}\\end{pmatrix}=\\begin{pmatrix}\\frac{14}{3\\sqrt{29}}\\\n$$4pt]\\frac{1}{3\\sqrt{29}}\\\n$$4pt]-\\frac{8}{3\\sqrt{29}}\\end{pmatrix}.\n$$\nTherefore, $h_{1,1}=\\frac{13}{9}$ and $v_{2}=\\left(\\frac{14}{3\\sqrt{29}},\\,\\frac{1}{3\\sqrt{29}},\\,-\\frac{8}{3\\sqrt{29}}\\right)^{\\top}$.", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{13}{9}  \\frac{14}{3\\sqrt{29}}  \\frac{1}{3\\sqrt{29}}  -\\frac{8}{3\\sqrt{29}}\\end{pmatrix}}$$", "id": "2154392"}, {"introduction": "The Arnoldi iteration sometimes terminates much earlier than expected, a situation known as a \"lucky breakdown.\" This exercise moves beyond simple mechanics to explore the underlying reason for such an event. You will discover that this phenomenon is directly linked to the fundamental concepts of eigenvalues and eigenvectors, revealing a deep connection between the iterative process and the matrix's invariant subspaces [@problem_id:2154381].", "problem": "The Arnoldi iteration is a numerical algorithm for approximating eigenvalues. For a given square matrix $A$ and a starting vector $v_1$, the first step involves normalizing the vector to $q_1 = v_1 / \\|v_1\\|_2$, computing $w = A q_1$, and then calculating a residual vector $r_1 = w - (q_1^T w) q_1$. The algorithm terminates after this first step if the residual vector $r_1$ is the zero vector.\n\nConsider a $3 \\times 3$ matrix $A$ with elements defined as $A_{11}=1$, $A_{12}=0$, $A_{13}=1$, $A_{21}=1$, $A_{22}=1$, $A_{23}=1$, $A_{31}=1$, $A_{32}=1$, and $A_{33}=-1$. Which of the following column vectors, when chosen as the starting vector $v_1$, will cause the Arnoldi iteration to terminate after just one step?\n\nA. A vector with components (1, 1, 1).\n\nB. A vector with components (1, 2, 1).\n\nC. A vector with components (1, -1, 0).\n\nD. A vector with components (0, 1, 1).\n\nE. A vector with components (1, 0, -1).", "solution": "The problem asks to identify which starting vector $v_1$ causes the Arnoldi iteration to terminate after the first step.\nAccording to the problem description, the first step of the Arnoldi iteration terminates if the residual vector $r_1$ is the zero vector.\nThe residual vector is defined as $r_1 = w - h_{11} q_1$, where $q_1 = v_1 / \\|v_1\\|_2$, $w = A q_1$, and $h_{11} = q_1^T w = q_1^T A q_1$.\n\nSetting $r_1 = 0$, we get:\n$$\nw - h_{11} q_1 = 0 \\implies w = h_{11} q_1\n$$\nSubstituting the expression for $w$:\n$$\nA q_1 = h_{11} q_1\n$$\nThis is the definition of an eigenvector and eigenvalue. The equation shows that the vector $q_1$ must be an eigenvector of the matrix $A$, and the scalar $h_{11}$ must be the corresponding eigenvalue.\n\nSince $q_1$ is just a normalized version of the starting vector $v_1$ (i.e., $q_1$ is $v_1$ scaled by $1/\\|v_1\\|_2$), if $q_1$ is an eigenvector, then $v_1$ must also be an eigenvector of $A$. Specifically, if $A q_1 = \\lambda q_1$, then:\n$$\nA \\left(\\frac{v_1}{\\|v_1\\|_2}\\right) = \\lambda \\left(\\frac{v_1}{\\|v_1\\|_2}\\right)\n$$\nMultiplying both sides by the scalar $\\|v_1\\|_2$ gives:\n$$\nA v_1 = \\lambda v_1\n$$\nThus, the problem reduces to finding which of the given vectors is an eigenvector of the matrix $A$.\n\nThe matrix $A$ is given by its components:\n$$\nA = \\begin{pmatrix} 1  0  1 \\\\ 1  1  1 \\\\ 1  1  -1 \\end{pmatrix}\n$$\nWe now test each option by multiplying the matrix $A$ by the vector $v_1$ from that option and checking if the resulting vector is a scalar multiple of the original vector.\n\nA. For $v_1 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$:\n$$\nA v_1 = \\begin{pmatrix} 1  0  1 \\\\ 1  1  1 \\\\ 1  1  -1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1(1) + 0(1) + 1(1) \\\\ 1(1) + 1(1) + 1(1) \\\\ 1(1) + 1(1) - 1(1) \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 3 \\\\ 1 \\end{pmatrix}\n$$\nIs $\\begin{pmatrix} 2 \\\\ 3 \\\\ 1 \\end{pmatrix} = \\lambda \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$ for some scalar $\\lambda$? No, because $2/1 \\neq 3/1$.\n\nB. For $v_1 = \\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix}$:\n$$\nA v_1 = \\begin{pmatrix} 1  0  1 \\\\ 1  1  1 \\\\ 1  1  -1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1(1) + 0(2) + 1(1) \\\\ 1(1) + 1(2) + 1(1) \\\\ 1(1) + 1(2) - 1(1) \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 4 \\\\ 2 \\end{pmatrix}\n$$\nIs $\\begin{pmatrix} 2 \\\\ 4 \\\\ 2 \\end{pmatrix} = \\lambda \\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix}$ for some scalar $\\lambda$? Yes, if we let $\\lambda=2$, we have $2 \\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 4 \\\\ 2 \\end{pmatrix}$. This vector is an eigenvector of $A$.\n\nC. For $v_1 = \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix}$:\n$$\nA v_1 = \\begin{pmatrix} 1  0  1 \\\\ 1  1  1 \\\\ 1  1  -1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1(1) + 0(-1) + 1(0) \\\\ 1(1) + 1(-1) + 1(0) \\\\ 1(1) + 1(-1) - 1(0) \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nIs $\\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\lambda \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix}$ for some scalar $\\lambda$? No. For example, the second component is $0 = \\lambda(-1)$, which means $\\lambda=0$, but the first component is $1 = \\lambda(1)$, which would mean $\\lambda=1$. This is a contradiction.\n\nD. For $v_1 = \\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\end{pmatrix}$:\n$$\nA v_1 = \\begin{pmatrix} 1  0  1 \\\\ 1  1  1 \\\\ 1  1  -1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1(0) + 0(1) + 1(1) \\\\ 1(0) + 1(1) + 1(1) \\\\ 1(0) + 1(1) - 1(1) \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 0 \\end{pmatrix}\n$$\nIs $\\begin{pmatrix} 1 \\\\ 2 \\\\ 0 \\end{pmatrix} = \\lambda \\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\end{pmatrix}$ for some scalar $\\lambda$? No. The first component requires $1 = \\lambda(0)$, which is impossible.\n\nE. For $v_1 = \\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\end{pmatrix}$:\n$$\nA v_1 = \\begin{pmatrix} 1  0  1 \\\\ 1  1  1 \\\\ 1  1  -1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 1(1) + 0(0) + 1(-1) \\\\ 1(1) + 1(0) + 1(-1) \\\\ 1(1) + 1(0) - 1(-1) \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 2 \\end{pmatrix}\n$$\nIs $\\begin{pmatrix} 0 \\\\ 0 \\\\ 2 \\end{pmatrix} = \\lambda \\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\end{pmatrix}$ for some scalar $\\lambda$? No. The first component requires $0 = \\lambda(1)$, which means $\\lambda=0$. But the third component requires $2 = \\lambda(-1)$, which would mean $\\lambda=-2$. This is a contradiction.\n\nOnly the vector in option B is an eigenvector of $A$. Therefore, choosing this vector as the starting vector $v_1$ will cause the Arnoldi iteration to terminate after the first step.", "answer": "$$\\boxed{B}$$", "id": "2154381"}, {"introduction": "How do the intrinsic properties of a matrix affect the Arnoldi iteration? This problem challenges you to connect the algebraic structure of a matrix—specifically, its rank—to the behavior of the algorithm over multiple steps. By analyzing a low-rank matrix, you will see why the dimension of the Krylov subspace is bounded, which guarantees that the iteration must terminate and reveals the theoretical limits of the process [@problem_id:2154383].", "problem": "In the analysis of large-scale linear dynamical systems, model order reduction is often achieved by projecting the system dynamics onto a lower-dimensional Krylov subspace. Consider a discrete-time linear system whose state evolution is governed by the matrix $A \\in \\mathbb{R}^{n \\times n}$. The matrix $A$ is constructed as the sum of two outer products:\n$$A = u_1 v_1^T + u_2 v_2^T$$\nwhere $u_1, u_2, v_1, v_2$ are non-zero column vectors in $\\mathbb{R}^n$. The pairs $\\{u_1, u_2\\}$ and $\\{v_1, v_2\\}$ are each linearly independent.\n\nThe Arnoldi iteration is a numerical algorithm used to generate an orthonormal basis $\\{q_1, q_2, \\dots, q_m\\}$ for the Krylov subspace $\\mathcal{K}_m(A, b) = \\text{span}\\{b, Ab, A^2b, \\dots, A^{m-1}b\\}$, starting from an initial vector $b$. The algorithm proceeds as follows:\n1. Initialize: $q_1 = b / \\|b\\|_2$.\n2. Iterate for $j = 1, 2, 3, \\dots, m-1$:\n    a. Compute the next Krylov vector: $w = A q_j$.\n    b. Orthogonalize $w$ against the previous basis vectors (Gram-Schmidt):\n        i. For $i=1, \\dots, j$, compute the projection coefficient: $h_{i,j} = q_i^T w$.\n        ii. Subtract the projection: $w_{\\perp} = w - \\sum_{i=1}^{j} h_{i,j} q_i$.\n    c. Compute the norm of the new orthogonal vector: $h_{j+1,j} = \\|w_{\\perp}\\|_2$.\n    d. If $h_{j+1,j} = 0$, the iteration terminates. Otherwise, normalize to get the next basis vector: $q_{j+1} = w_{\\perp} / h_{j+1,j}$.\n\nThe coefficients $h_{i,j}$ form an upper Hessenberg matrix $H_m$.\n\nSuppose we apply the Arnoldi iteration with a starting vector $b \\in \\mathbb{R}^n$ that is chosen such that the dimensions of the first three Krylov subspaces are $\\dim(\\mathcal{K}_1(A,b))=1$, $\\dim(\\mathcal{K}_2(A,b))=2$, and $\\dim(\\mathcal{K}_3(A,b))=3$.\n\nWhat is the value of the entry $h_{4,3}$?\n\nA. 1\n\nB. 0\n\nC. A non-zero value that depends on the norms of $u_1, u_2, v_1, v_2$.\n\nD. A non-zero value that depends on the inner products between $u_i, v_i$, and $b$.\n\nE. The value cannot be determined without the explicit numerical values of the vectors.", "solution": "Write $A$ as $A = u_{1} v_{1}^{T} + u_{2} v_{2}^{T}$. For any $x \\in \\mathbb{R}^{n}$,\n$$\nA x = u_{1}\\big(v_{1}^{T} x\\big) + u_{2}\\big(v_{2}^{T} x\\big) \\in \\operatorname{span}\\{u_{1},u_{2}\\}.\n$$\nHence $\\operatorname{Im}(A) \\subseteq \\operatorname{span}\\{u_{1},u_{2}\\}$ and, for all $k \\geq 1$, $A^{k} b \\in \\operatorname{span}\\{u_{1},u_{2}\\}$.\n\nTherefore, for any $m \\geq 1$,\n$$\n\\mathcal{K}_{m}(A,b) = \\operatorname{span}\\{b,Ab,\\dots,A^{m-1}b\\} \\subseteq \\operatorname{span}\\{b\\} + \\operatorname{span}\\{u_{1},u_{2}\\}.\n$$\nIn particular, $\\dim\\big(\\mathcal{K}_{m}(A,b)\\big) \\leq 3$. The hypothesis gives $\\dim\\big(\\mathcal{K}_{1}(A,b)\\big)=1$, $\\dim\\big(\\mathcal{K}_{2}(A,b)\\big)=2$, and $\\dim\\big(\\mathcal{K}_{3}(A,b)\\big)=3$, which forces\n$$\n\\mathcal{K}_{3}(A,b) = \\operatorname{span}\\{b,Ab,A^{2}b\\} = \\operatorname{span}\\{b\\} \\oplus \\operatorname{span}\\{u_{1},u_{2}\\}.\n$$\nSince $A b \\in \\operatorname{span}\\{u_{1},u_{2}\\}$ and $A$ maps $\\operatorname{span}\\{u_{1},u_{2}\\}$ into itself, it follows that\n$$\nA\\big(\\mathcal{K}_{3}(A,b)\\big) \\subseteq \\operatorname{span}\\{u_{1},u_{2}\\} \\subseteq \\mathcal{K}_{3}(A,b).\n$$\nThus $\\mathcal{K}_{3}(A,b)$ is $A$-invariant, and $A q_{3} \\in \\operatorname{span}\\{q_{1},q_{2},q_{3}\\}$ for any Arnoldi orthonormal basis $\\{q_{1},q_{2},q_{3}\\}$ of $\\mathcal{K}_{3}(A,b)$.\n\nIn the Arnoldi step $j=3$, one computes $w = A q_{3}$ and orthogonalizes against $q_{1},q_{2},q_{3}$, obtaining $w_{\\perp} = w - \\sum_{i=1}^{3} h_{i,3} q_{i}$. Because $w \\in \\operatorname{span}\\{q_{1},q_{2},q_{3}\\}$, it follows that $w_{\\perp} = 0$. Hence\n$$\nh_{4,3} = \\|w_{\\perp}\\|_{2} = 0.\n$$\nTherefore, the correct choice is B.", "answer": "$$\\boxed{B}$$", "id": "2154383"}]}