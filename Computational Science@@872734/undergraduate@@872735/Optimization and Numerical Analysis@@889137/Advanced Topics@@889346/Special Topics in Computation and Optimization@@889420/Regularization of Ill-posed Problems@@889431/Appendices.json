{"hands_on_practices": [{"introduction": "Many inverse problems, when discretized, result in underdetermined linear systems with infinite possible solutions. Tikhonov regularization resolves this ambiguity by selecting the unique solution with the minimum Euclidean norm, $\\|x\\|_2$. This practice provides a concrete geometric interpretation of this principle, tasking you with finding the point on a line of solutions that is closest to the origin, thereby making the abstract concept of norm minimization tangible. [@problem_id:2197177]", "problem": "A simplified model for a resource allocation system involves two control variables, $x_1$ and $x_2$. To achieve a target output, these variables must satisfy the linear constraint equation $3x_1 + 2x_2 = 10$. The system is designed to operate with minimal energy expenditure. The cost of operation, denoted by $L$, is directly proportional to the sum of the squares of the control variables, which means minimizing $L = x_1^2 + x_2^2$.\n\nFrom the infinite number of pairs $(x_1, x_2)$ that satisfy the constraint, determine the unique pair that also minimizes the cost function $L$. Present your answer as a pair of values $(x_1, x_2)$.", "solution": "The problem requires us to find the values of $x_1$ and $x_2$ that minimize the objective function $L(x_1, x_2) = x_1^2 + x_2^2$ subject to the linear constraint $3x_1 + 2x_2 = 10$. This is a constrained optimization problem.\n\nFirst, we can use the constraint equation to express one variable in terms of the other. Let's solve the constraint for $x_2$:\n$$3x_1 + 2x_2 = 10$$\n$$2x_2 = 10 - 3x_1$$\n$$x_2 = 5 - \\frac{3}{2}x_1$$\n\nNow, we can substitute this expression for $x_2$ into the objective function $L$. This transforms $L$ into a function of a single variable, $x_1$.\n$$L(x_1) = x_1^2 + \\left(5 - \\frac{3}{2}x_1\\right)^2$$\n\nTo find the value of $x_1$ that minimizes $L$, we first expand the expression for $L(x_1)$:\n$$L(x_1) = x_1^2 + \\left(5^2 - 2(5)\\left(\\frac{3}{2}x_1\\right) + \\left(\\frac{3}{2}x_1\\right)^2\\right)$$\n$$L(x_1) = x_1^2 + 25 - 15x_1 + \\frac{9}{4}x_1^2$$\n\nCombine the terms involving $x_1^2$:\n$$L(x_1) = \\left(1 + \\frac{9}{4}\\right)x_1^2 - 15x_1 + 25$$\n$$L(x_1) = \\frac{13}{4}x_1^2 - 15x_1 + 25$$\n\nThis is a quadratic function of $x_1$ that opens upwards (since the coefficient $\\frac{13}{4}$ is positive), so it has a unique minimum. To find the minimum, we take the derivative of $L(x_1)$ with respect to $x_1$ and set it to zero.\n$$\\frac{dL}{dx_1} = \\frac{d}{dx_1}\\left(\\frac{13}{4}x_1^2 - 15x_1 + 25\\right)$$\n$$\\frac{dL}{dx_1} = 2 \\cdot \\frac{13}{4}x_1 - 15$$\n$$\\frac{dL}{dx_1} = \\frac{13}{2}x_1 - 15$$\n\nSet the derivative to zero to find the critical point:\n$$\\frac{13}{2}x_1 - 15 = 0$$\n$$\\frac{13}{2}x_1 = 15$$\n$$x_1 = 15 \\cdot \\frac{2}{13}$$\n$$x_1 = \\frac{30}{13}$$\n\nNow that we have the value for $x_1$, we can substitute it back into the expression for $x_2$ derived from the constraint:\n$$x_2 = 5 - \\frac{3}{2}x_1$$\n$$x_2 = 5 - \\frac{3}{2}\\left(\\frac{30}{13}\\right)$$\n$$x_2 = 5 - \\frac{90}{26}$$\n$$x_2 = 5 - \\frac{45}{13}$$\n\nTo subtract the fraction, we find a common denominator:\n$$x_2 = \\frac{5 \\cdot 13}{13} - \\frac{45}{13}$$\n$$x_2 = \\frac{65 - 45}{13}$$\n$$x_2 = \\frac{20}{13}$$\n\nThus, the pair of control variables that satisfies the constraint and minimizes the cost function is $(x_1, x_2) = \\left(\\frac{30}{13}, \\frac{20}{13}\\right)$.", "answer": "$$\\boxed{\\left(\\frac{30}{13}, \\frac{20}{13}\\right)}$$", "id": "2197177"}, {"introduction": "While Tikhonov regularization is powerful, it is just one of several approaches. A crucial alternative is $L_1$ regularization, also known as LASSO, which penalizes the sum of the absolute values of the coefficients, $\\|x\\|_1$. This exercise directly contrasts the dense solutions produced by Tikhonov regularization with the sparse solutions favored by LASSO, allowing you to observe how LASSO can perform \"feature selection\" by driving some components of the solution vector to exactly zero. [@problem_id:2197169]", "problem": "Consider an underdetermined linear system $Ax = b$, where the solution vector is $x = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} \\in \\mathbb{R}^2$, the matrix is $A = \\begin{pmatrix} 2 & 1 \\end{pmatrix}$, and the right-hand side is the scalar $b=4$. This system, representing the single equation $2x_1 + x_2 = 4$, has an infinite number of solutions. To select a unique and desirable solution, we can employ regularization techniques.\n\nThis problem explores two common regularization methods:\n\n1.  **Tikhonov Regularization ($L_2$ norm):** The Tikhonov-regularized solution, denoted by $x_T$, is the vector $x$ that minimizes the objective function $F(x) = \\|Ax-b\\|_2^2 + \\lambda \\|x\\|_2^2$. Here, $\\|v\\|_2 = \\sqrt{\\sum_i v_i^2}$ is the standard Euclidean norm (or $L_2$ norm) of a vector $v$.\n\n2.  **LASSO Regularization ($L_1$ norm):** The LASSO solution, denoted by $x_L$, is the vector $x$ that minimizes the objective function $G(x) = \\|Ax-b\\|_2^2 + \\lambda \\|x\\|_1$. Here, $\\|v\\|_1 = \\sum_i |v_i|$ is the taxicab norm (or $L_1$ norm) of a vector $v$. The acronym LASSO stands for Least Absolute Shrinkage and Selection Operator.\n\nUsing a regularization parameter of $\\lambda = 4$ for both methods, calculate the ratio of the L1 norms of the two resulting solution vectors, $\\frac{\\|x_T\\|_1}{\\|x_L\\|_1}$.\n\nExpress your answer as an exact fraction in simplest form.", "solution": "We have $A=\\begin{pmatrix}2 & 1\\end{pmatrix}$, $b=4$, and $x=\\begin{pmatrix}x_{1} \\\\ x_{2}\\end{pmatrix}$.\n\nFor Tikhonov regularization with parameter $\\lambda=4$, we minimize $F(x)=\\|Ax-b\\|_{2}^{2}+4\\|x\\|_{2}^{2}$. The first-order optimality condition for ridge regression is\n$$(A^{T}A+\\lambda I)x=A^{T}b.$$\nCompute\n$$A^{T}A=\\begin{pmatrix}4 & 2 \\\\ 2 & 1\\end{pmatrix},\\quad \\lambda I=\\begin{pmatrix}4 & 0 \\\\ 0 & 4\\end{pmatrix},\\quad A^{T}b=\\begin{pmatrix}8 \\\\ 4\\end{pmatrix}.$$\nHence\n$$\\begin{pmatrix}8 & 2 \\\\ 2 & 5\\end{pmatrix}x=\\begin{pmatrix}8 \\\\ 4\\end{pmatrix}.$$\nSolving,\n$$\\det=8\\cdot 5-2\\cdot 2=36,\\quad x_{T}=\\frac{1}{36}\\begin{pmatrix}5 & -2 \\\\ -2 & 8\\end{pmatrix}\\begin{pmatrix}8 \\\\ 4\\end{pmatrix}=\\frac{1}{36}\\begin{pmatrix}32 \\\\ 16\\end{pmatrix}=\\begin{pmatrix}\\frac{8}{9} \\\\ \\frac{4}{9}\\end{pmatrix}.$$\nThus\n$$\\|x_{T}\\|_{1}=\\left|\\frac{8}{9}\\right|+\\left|\\frac{4}{9}\\right|=\\frac{12}{9}=\\frac{4}{3}.$$\n\nFor LASSO with $\\lambda=4$, we minimize $G(x)=\\|Ax-b\\|_{2}^{2}+4\\|x\\|_{1}$. Let $r=Ax-b=2x_{1}+x_{2}-4$. The subgradient optimality condition is\n$$2A^{T}(Ax-b)+\\lambda z=0,\\quad z\\in\\partial\\|x\\|_{1}.$$\nSince $A^{T}(Ax-b)=\\begin{pmatrix}2r \\\\ r\\end{pmatrix}$, we obtain\n$$\\begin{pmatrix}4r \\\\ 2r\\end{pmatrix}+4\\begin{pmatrix}z_{1} \\\\ z_{2}\\end{pmatrix}=0\\quad\\Rightarrow\\quad z_{1}=-r,\\; z_{2}=-\\frac{r}{2}.$$\nFeasibility requires $z_{i}\\in\\partial|x_{i}|$: if $x_{i}\\neq 0$, then $z_{i}=\\operatorname{sign}(x_{i})\\in\\{\\pm 1\\}$; if $x_{i}=0$, then $z_{i}\\in[-1,1]$.\n\n- If both $x_{1}\\neq 0$ and $x_{2}\\neq 0$, then $z_{1},z_{2}\\in\\{\\pm 1\\}$, which would require simultaneously $-r\\in\\{\\pm 1\\}$ and $-r/2\\in\\{\\pm 1\\}$; there is no $r$ satisfying both equalities, so this is impossible.\n\n- If $x_{1}=0$ and $x_{2}\\neq 0$, then $z_{1}=-r\\in[-1,1]$ implies $r\\in[-1,1]$, but $z_{2}=-r/2=\\operatorname{sign}(x_{2})\\in\\{\\pm 1\\}$ forces $r\\in\\{-2,2\\}$, a contradiction.\n\n- If $x_{2}=0$ and $x_{1}\\neq 0$, then $z_{1}=-r=\\operatorname{sign}(x_{1})\\in\\{\\pm 1\\}$ implies $r\\in\\{-1,1\\}$, and $z_{2}=-r/2\\in[-1,1]$ holds automatically for both values. Check consistency with $r=2x_{1}-4$:\n  - If $r=-1$, then $2x_{1}-4=-1$ gives $x_{1}=\\frac{3}{2}$, which matches $\\operatorname{sign}(x_{1})=+1$ so $z_{1}=-r=1$.\n  - If $r=1$, then $2x_{1}-4=1$ gives $x_{1}=\\frac{5}{2}$, which would require $z_{1}=-1$, contradicting $\\operatorname{sign}(x_{1})=+1$.\n\nTherefore the LASSO minimizer is\n$$x_{L}=\\begin{pmatrix}\\frac{3}{2} \\\\ 0\\end{pmatrix},\\quad \\|x_{L}\\|_{1}=\\frac{3}{2}.$$\n\nFinally, the requested ratio is\n$$\\frac{\\|x_{T}\\|_{1}}{\\|x_{L}\\|_{1}}=\\frac{\\frac{4}{3}}{\\frac{3}{2}}=\\frac{4}{3}\\cdot\\frac{2}{3}=\\frac{8}{9}.$$", "answer": "$$\\boxed{\\frac{8}{9}}$$", "id": "2197169"}, {"introduction": "A critical aspect of applying regularization is choosing the regularization parameter, $\\lambda$, which balances the trade-off between fitting the data and constraining the solution. This choice is not arbitrary; a poor $\\lambda$ can lead to either an overly noisy or an overly smooth solution. This practice introduces Generalized Cross-Validation (GCV), a powerful data-driven method for selecting a suitable $\\lambda$ automatically, demonstrating how to find the optimal parameter from a set of candidates in a realistic signal processing scenario. [@problem_id:2197162]", "problem": "An engineer is working to restore a signal that has been blurred and corrupted by noise. The problem is formulated as a one-dimensional (1D) discrete deconvolution problem described by the linear system $\\mathbf{y} = K\\mathbf{x} + \\boldsymbol{\\epsilon}$, where $\\mathbf{y} \\in \\mathbb{R}^3$ is the observed data vector, $K \\in \\mathbb{R}^{3 \\times 3}$ is the blurring matrix, $\\mathbf{x} \\in \\mathbb{R}^3$ is the unknown true signal, and $\\boldsymbol{\\epsilon}$ is an additive noise vector.\n\nTo find a stable solution, Tikhonov regularization is employed. The optimal regularization parameter $\\lambda$ is determined using the Generalized Cross-Validation (GCV) method, which minimizes the GCV function $V(\\lambda)$ without prior knowledge of the noise level.\n\nThe analysis is performed using the Singular Value Decomposition (SVD) of the matrix $K$. In this basis, the GCV function for a system of size $N$ can be written as:\n$$ V(\\lambda) = N \\frac{\\sum_{i=1}^{N} \\left( \\frac{\\lambda^2 y'_i}{\\sigma_i^2 + \\lambda^2} \\right)^2}{\\left( \\sum_{i=1}^{N} \\frac{\\lambda^2}{\\sigma_i^2 + \\lambda^2} \\right)^2} $$\nwhere $\\{\\sigma_i\\}$ are the singular values of $K$, and $\\{y'_i\\}$ are the components of the data vector $y$ projected onto the basis of the left singular vectors of $K$.\n\nFor this specific problem, the system size is $N=3$. The singular values of the matrix $K$ have been computed as:\n$$ \\sigma_1 = 10, \\quad \\sigma_2 = 1, \\quad \\sigma_3 = 0.1 $$\nThe components of the transformed data vector are:\n$$ y'_1 = 10, \\quad y'_2 = 2, \\quad y'_3 = 1 $$\n\nYour task is to determine the optimal regularization parameter $\\lambda$ from the following set of candidate values by finding which one minimizes the GCV function $V(\\lambda)$.\n\nA. $0.01$\n\nB. $0.1$\n\nC. $1$\n\nD. $10$", "solution": "We are given the GCV function in the SVD basis:\n$$\nV(\\lambda) = N \\frac{\\sum_{i=1}^{N} \\left( \\frac{\\lambda^{2} y'_{i}}{\\sigma_{i}^{2} + \\lambda^{2}} \\right)^{2}}{\\left( \\sum_{i=1}^{N} \\frac{\\lambda^{2}}{\\sigma_{i}^{2} + \\lambda^{2}} \\right)^{2}},\n$$\nwith $N=3$, singular values $\\sigma_{1}=10$, $\\sigma_{2}=1$, $\\sigma_{3}= \\frac{1}{10}$, and transformed data $y'_{1}=10$, $y'_{2}=2$, $y'_{3}=1$.\n\nDefine for each $\\lambda$:\n$$\na_{i}(\\lambda) := \\frac{\\lambda^{2}}{\\sigma_{i}^{2}+\\lambda^{2}}, \\quad S(\\lambda) := \\sum_{i=1}^{3} (y_{i}^{\\prime} a_{i}(\\lambda))^{2}, \\quad T(\\lambda) := \\left(\\sum_{i=1}^{3} a_{i}(\\lambda)\\right)^{2}.\n$$\nThen\n$$\nV(\\lambda) = 3 \\frac{S(\\lambda)}{T(\\lambda)}.\n$$\nWe evaluate $V(\\lambda)$ for the candidate values.\n\nCase A: $\\lambda = \\frac{1}{100}$, so $\\lambda^{2}=\\frac{1}{10000}$ and $\\sigma_{1}^{2}=100$, $\\sigma_{2}^{2}=1$, $\\sigma_{3}^{2}=\\frac{1}{100}$.\nCompute\n$$\na_{1}=\\frac{\\frac{1}{10000}}{100+\\frac{1}{10000}}=\\frac{1}{1000001}, \\quad\na_{2}=\\frac{\\frac{1}{10000}}{1+\\frac{1}{10000}}=\\frac{1}{10001}, \\quad\na_{3}=\\frac{\\frac{1}{10000}}{\\frac{1}{100}+\\frac{1}{10000}}=\\frac{1}{101}.\n$$\nThus\n$$\nS= (10 a_1)^2 + (2 a_2)^2 + (1 a_3)^2 = \\frac{100}{(1000001)^{2}}+\\frac{4}{(10001)^{2}}+\\frac{1}{101^{2}}, \\quad\nT=\\left(\\frac{1}{1000001}+\\frac{1}{10001}+\\frac{1}{101}\\right)^{2}.\n$$\nNumerically,\n$$\nS \\approx 9.8069366091 \\times 10^{-5}, \\quad T \\approx 1.00039604 \\times 10^{-4},\n$$\nso\n$$\nV\\left(\\tfrac{1}{100}\\right) \\approx 3 \\times \\frac{9.8069 \\times 10^{-5}}{1.0004 \\times 10^{-4}} \\approx 2.941.\n$$\n\nCase B: $\\lambda = \\frac{1}{10}$, so $\\lambda^{2}=\\frac{1}{100}$.\nCompute\n$$\na_{1}=\\frac{\\frac{1}{100}}{100+\\frac{1}{100}}=\\frac{1}{10001}, \\quad\na_{2}=\\frac{\\frac{1}{100}}{1+\\frac{1}{100}}=\\frac{1}{101}, \\quad\na_{3}=\\frac{\\frac{1}{100}}{\\frac{1}{100}+\\frac{1}{100}}=\\frac{1}{2}.\n$$\nThus\n$$\nS = \\frac{100}{(10001)^{2}}+\\frac{4}{101^{2}}+\\frac{1}{4}, \\quad\nT=\\left(\\frac{1}{10001}+\\frac{1}{101}+\\frac{1}{2}\\right)^{2}.\n$$\nNumerically,\n$$\nS \\approx 0.2503931169, \\quad T \\approx 0.2601020197,\n$$\nso\n$$\nV\\left(\\tfrac{1}{10}\\right) \\approx 3 \\times \\frac{0.2503931169}{0.2601020197} \\approx 2.888.\n$$\n\nCase C: $\\lambda = 1$, so $\\lambda^{2}=1$.\nCompute\n$$\na_{1}=\\frac{1}{100+1}=\\frac{1}{101}, \\quad\na_{2}=\\frac{1}{1+1}=\\frac{1}{2}, \\quad\na_{3}=\\frac{1}{\\frac{1}{100}+1}=\\frac{100}{101}.\n$$\nThen\n$$\nS = 10^2\\left(\\frac{1}{101}\\right)^{2} + 2^2\\left(\\frac{1}{2}\\right)^{2} + 1^2\\left(\\frac{100}{101}\\right)^{2}\n= \\frac{100}{10201} + 1 + \\frac{10000}{10201}.\n$$\nCombine the fractional terms:\n$$\n\\frac{100}{10201} + \\frac{10000}{10201} = \\frac{10100}{10201}.\n$$\nHence\n$$\nS = 1 + \\frac{10100}{10201} = \\frac{10201+10100}{10201} = \\frac{20301}{10201} = \\frac{201}{101}.\n$$\nAlso\n$$\n\\sum_{i=1}^{3} a_{i} = \\frac{1}{101} + \\frac{1}{2} + \\frac{100}{101} = 1 + \\frac{1}{2} = \\frac{3}{2},\n$$\nso\n$$\nT = \\left(\\frac{3}{2}\\right)^{2} = \\frac{9}{4}.\n$$\nTherefore\n$$\nV(1) = 3 \\cdot \\frac{\\frac{201}{101}}{\\frac{9}{4}} = 3 \\cdot \\frac{201}{101} \\cdot \\frac{4}{9} = \\frac{268}{101} \\approx 2.653465.\n$$\n\nCase D: $\\lambda = 10$, so $\\lambda^{2}=100$.\nCompute\n$$\na_{1}=\\frac{100}{100+100}=\\frac{1}{2}, \\quad\na_{2}=\\frac{100}{1+100}=\\frac{100}{101}, \\quad\na_{3}=\\frac{100}{\\frac{1}{100}+100}=\\frac{10000}{10001}.\n$$\nThus\n$$\nS = 10^2\\left(\\frac{1}{2}\\right)^{2} + 2^2\\left(\\frac{100}{101}\\right)^{2} + 1^2\\left(\\frac{10000}{10001}\\right)^{2}\n= 25 + \\frac{40000}{10201} + \\frac{100000000}{100020001},\n$$\nand\n$$\nT = \\left(\\frac{1}{2} + \\frac{100}{101} + \\frac{10000}{10001}\\right)^{2}.\n$$\nNumerically,\n$$\nS \\approx 25 + 3.92 + 0.999 \\approx 29.92, \\quad T \\approx (0.5+0.99+0.999)^2 \\approx (2.489)^2 \\approx 6.20,\n$$\nso\n$$\nV(10) \\approx 3 \\times \\frac{29.92}{6.20} \\approx 14.5.\n$$\n\nComparing the four values, we have approximately $V\\left(\\tfrac{1}{100}\\right) \\approx 2.941$, $V\\left(\\tfrac{1}{10}\\right) \\approx 2.888$, $V(1) = \\frac{268}{101} \\approx 2.653$, and $V(10) \\approx 14.5$. The minimum is attained at $\\lambda=1$.", "answer": "$$\\boxed{C}$$", "id": "2197162"}]}