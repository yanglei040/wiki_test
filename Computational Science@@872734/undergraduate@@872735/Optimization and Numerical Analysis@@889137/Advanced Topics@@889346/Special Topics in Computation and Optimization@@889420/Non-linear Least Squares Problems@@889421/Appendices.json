{"hands_on_practices": [{"introduction": "Before we can minimize the discrepancy between a model and our data, we first need a way to quantify it. The sum of squared errors, $S(\\beta)$, provides this crucial measure, and calculating its value for an initial parameter guess is the essential first step in any non-linear least squares optimization routine [@problem_id:2214255]. This foundational exercise grounds the abstract goal of \"fitting a model\" in a concrete, computable value.", "problem": "In the field of data analysis and numerical optimization, non-linear least squares is a fundamental technique used to fit a model to a set of observed data points. The goal is to find the model parameters that minimize the sum of the squared differences between the observed data and the values predicted by the model.\n\nConsider a model intended to describe a saturation process, given by the function:\n$$f(x, \\beta) = \\frac{\\beta_1}{1 + x^{\\beta_2}}$$\nwhere $\\beta = (\\beta_1, \\beta_2)$ is a vector of the model's parameters.\n\nSuppose an experiment yields two data points: $(x_1, y_1) = (1, 1)$ and $(x_2, y_2) = (2, 0.5)$.\n\nThe quality of the fit for a particular choice of parameters is quantified by the sum-of-squared-errors objective function, $S(\\beta)$, defined as the sum of the squared residuals:\n$$S(\\beta) = \\sum_{i=1}^{2} (y_i - f(x_i, \\beta))^2$$\n\nAs a first step in an optimization procedure like the Gauss-Newton method, one must evaluate the objective function at an initial guess for the parameters. Calculate the value of $S(\\beta)$ for the specific parameter vector $\\beta = (2, 1)$.\n\nExpress your final answer as an exact fraction in its simplest form.", "solution": "We are given $f(x,\\beta)=\\dfrac{\\beta_{1}}{1+x^{\\beta_{2}}}$ and $S(\\beta)=\\sum_{i=1}^{2}\\left(y_{i}-f(x_{i},\\beta)\\right)^{2}$ with data $(x_{1},y_{1})=(1,1)$ and $(x_{2},y_{2})=(2,\\dfrac{1}{2})$. For $\\beta=(2,1)$, the model evaluates to\n$$\nf(x,(2,1))=\\frac{2}{1+x^{1}}=\\frac{2}{1+x}.\n$$\nAt $x_{1}=1$, we have\n$$\nf(1,(2,1))=\\frac{2}{1+1}=\\frac{2}{2}=1,\n$$\nso the residual is $r_{1}=y_{1}-f(1,(2,1))=1-1=0$, giving $r_{1}^{2}=0$.\nAt $x_{2}=2$, we have\n$$\nf(2,(2,1))=\\frac{2}{1+2}=\\frac{2}{3},\n$$\nso the residual is\n$$\nr_{2}=y_{2}-f(2,(2,1))=\\frac{1}{2}-\\frac{2}{3}=\\frac{3-4}{6}=-\\frac{1}{6},\n$$\ngiving $r_{2}^{2}=\\left(-\\frac{1}{6}\\right)^{2}=\\frac{1}{36}$.\nTherefore,\n$$\nS(\\beta)=r_{1}^{2}+r_{2}^{2}=0+\\frac{1}{36}=\\frac{1}{36}.\n$$", "answer": "$$\\boxed{\\frac{1}{36}}$$", "id": "2214255"}, {"introduction": "The power of the Gauss-Newton algorithm lies in its clever simplification of the optimization landscape's curvature. This exercise provides a direct comparison between the true Hessian of the error function and the Gauss-Newton approximation, revealing the mathematical assumption at the heart of the method's efficiency [@problem_id:2214286]. Understanding this difference is key to grasping both the algorithm's strengths and its limitations, particularly in cases with large final residuals.", "problem": "Consider a single-parameter nonlinear model used to describe a growth process, given by the function $f(x; \\beta) = \\exp(\\beta x)$, where $\\beta$ is the model parameter. We want to fit this model to a single experimental data point $(x_1, y_1) = (1, 3)$.\n\nThe quality of the fit for a given $\\beta$ is measured by the least-squares objective function, which for a single data point is the squared residual:\n$$ S(\\beta) = (y_1 - f(x_1; \\beta))^2 $$\n\nIn optimization, the second derivative (Hessian) of the objective function provides information about the curvature of the error surface. The true Hessian of $S(\\beta)$ is the scalar $H = \\frac{d^2S}{d\\beta^2}$. The Gauss-Newton algorithm, a popular method for solving nonlinear least squares problems, uses an approximation for the Hessian given by $H_{GN} = J^T J$, where $J$ is the Jacobian of the residual function $r(\\beta) = y_1 - f(x_1; \\beta)$. Since there is only one parameter and one data point, the Jacobian $J$ is a $1 \\times 1$ matrix.\n\nCalculate the exact value of the discrepancy between the true Hessian and the Gauss-Newton approximation, $\\Delta H = H - H_{GN}$, evaluated at the specific parameter value $\\beta=1$.", "solution": "We have a single-parameter model $f(x;\\beta)=\\exp(\\beta x)$ and one data point $(x_{1},y_{1})=(1,3)$. The residual is defined by $r(\\beta)=y_{1}-f(x_{1};\\beta)=3-\\exp(\\beta)$. The least-squares objective for one data point is $S(\\beta)=r(\\beta)^{2}$.\n\nTo compute the true Hessian, differentiate $S(\\beta)=r(\\beta)^{2}$. Using the chain rule,\n$$\n\\frac{dS}{d\\beta}=2\\,r(\\beta)\\,r'(\\beta),\n$$\nand differentiating again gives\n$$\nH=\\frac{d^{2}S}{d\\beta^{2}}=2\\left[(r'(\\beta))^{2}+r(\\beta)\\,r''(\\beta)\\right].\n$$\nFor $r(\\beta)=3-\\exp(\\beta)$, we have $r'(\\beta)=-\\exp(\\beta)$ and $r''(\\beta)=-\\exp(\\beta)$. Substituting,\n$$\nH=2\\left[(-\\exp(\\beta))^{2}+(3-\\exp(\\beta))(-\\exp(\\beta))\\right]\n=2\\left[\\exp(2\\beta)-3\\exp(\\beta)+\\exp(2\\beta)\\right]\n=4\\exp(2\\beta)-6\\exp(\\beta).\n$$\n\nThe Gauss-Newton approximation uses $H_{GN}=J^{T}J$, where $J$ is the Jacobian of $r(\\beta)$ with respect to $\\beta$. Since $J=r'(\\beta)=-\\exp(\\beta)$ in this scalar case, we have\n$$\nH_{GN}=J^{T}J=(-\\exp(\\beta))^{2}=\\exp(2\\beta).\n$$\n\nThe discrepancy is\n$$\n\\Delta H=H-H_{GN}=\\left(4\\exp(2\\beta)-6\\exp(\\beta)\\right)-\\exp(2\\beta)\n=3\\exp(2\\beta)-6\\exp(\\beta).\n$$\nEvaluating at $\\beta=1$ yields\n$$\n\\Delta H=3\\exp(2)-6\\exp(1)=3\\exp(1)\\left(\\exp(1)-2\\right).\n$$", "answer": "$$\\boxed{3\\exp(2)-6\\exp(1)}$$", "id": "2214286"}, {"introduction": "Now, let's put theory into practice by applying a full iteration of the Gauss-Newton algorithm to a realistic problem: locating a sound source using time-difference-of-arrival data. This practice will guide you through calculating the residual vector and the Jacobian matrix to solve for the parameter update step, demonstrating the complete mechanics of the algorithm in a multi-parameter setting [@problem_id:2191258]. It is an excellent example of how this numerical method is used to solve complex positioning and navigation problems.", "problem": "An acoustic localization system is designed to find the position of a sound source in a two-dimensional plane. The system consists of three microphones located at positions $\\mathbf{m}_1 = (0, 0)$, $\\mathbf{m}_2 = (L, 0)$, and $\\mathbf{m}_3 = (0, L)$. The source is at an unknown position $\\mathbf{p} = (x, y)$.\n\nThe system operates by measuring the Time-Difference-of-Arrival (TDOA) of a sound signal at different microphones. The time differences are measured with respect to the signal's arrival at the reference microphone $\\mathbf{m}_1$. The theoretical model for the time difference between microphone $\\mathbf{m}_i$ and $\\mathbf{m}_1$ is given by $\\Delta t_{i1}(\\mathbf{p}) = \\frac{\\|\\mathbf{p} - \\mathbf{m}_i\\| - \\|\\mathbf{p} - \\mathbf{m}_1\\|}{v_s}$, where $\\|\\cdot\\|$ denotes the Euclidean distance and $v_s$ is the speed of sound.\n\nThe position $\\mathbf{p}$ is estimated by finding the coordinates $(x, y)$ that minimize the sum of squared differences between the measured TDOAs and the model's predictions. This non-linear least squares problem is to be solved using the Gauss-Newton algorithm. The algorithm iteratively refines an estimate $\\mathbf{p}^{(k)}$ to a new estimate $\\mathbf{p}^{(k+1)}$ using the update rule:\n$$ \\mathbf{p}^{(k+1)} = \\mathbf{p}^{(k)} + \\Delta\\mathbf{p} $$\nwhere the update step $\\Delta\\mathbf{p}$ is the solution to the linear system $(J^T J) \\Delta\\mathbf{p} = -J^T \\mathbf{r}$. Here, $\\mathbf{r}$ is the vector of residuals (measured values minus model predictions) and $J$ is the Jacobian matrix of the model functions, both evaluated at the current estimate $\\mathbf{p}^{(k)}$.\n\nYou are given the following parameters:\n- Microphone configuration constant: $L = 100 \\text{ m}$\n- Speed of sound: $v_s = 340 \\text{ m/s}$\n- Measured TDOAs: $\\tau_{21} = \\frac{5(1-\\sqrt{2})}{17} \\text{ s}$ and $\\tau_{31} = \\frac{5(1-\\sqrt{2})}{17} \\text{ s}$\n- Initial guess for the source position: $\\mathbf{p}^{(0)} = (L/2, L/2)$\n\nYour task is to calculate the next position estimate $\\mathbf{p}^{(1)} = (x_1, y_1)$ by performing a single step of the Gauss-Newton algorithm. Provide the coordinates $(x_1, y_1)$ as an analytic expression.", "solution": "We model the TDOA for microphones $\\mathbf{m}_{2}$ and $\\mathbf{m}_{3}$ relative to $\\mathbf{m}_{1}$ by\n$$\nh_{i}(\\mathbf{p})=\\Delta t_{i1}(\\mathbf{p})=\\frac{\\|\\mathbf{p}-\\mathbf{m}_{i}\\|-\\|\\mathbf{p}-\\mathbf{m}_{1}\\|}{v_{s}},\\quad i\\in\\{2,3\\}.\n$$\nGiven measured TDOAs $\\tau_{21}$ and $\\tau_{31}$, the residual vector at a current estimate $\\mathbf{p}^{(k)}$ is\n$$\n\\mathbf{r}=\\begin{pmatrix}\\tau_{21}-h_{2}(\\mathbf{p}^{(k)})\\\\ \\tau_{31}-h_{3}(\\mathbf{p}^{(k)})\\end{pmatrix}.\n$$\nThe Gauss-Newton step solves $(J^{T}J)\\,\\Delta\\mathbf{p}=-J^{T}\\mathbf{r}$, where $J$ is the Jacobian of $(h_{2},h_{3})$ at $\\mathbf{p}^{(k)}$. Using $\\nabla_{\\mathbf{p}}\\|\\mathbf{p}-\\mathbf{a}\\|=\\frac{\\mathbf{p}-\\mathbf{a}}{\\|\\mathbf{p}-\\mathbf{a}\\|}$ for $\\mathbf{p}\\neq\\mathbf{a}$, we obtain\n$$\n\\nabla h_{i}(\\mathbf{p})=\\frac{1}{v_{s}}\\left(\\frac{\\mathbf{p}-\\mathbf{m}_{i}}{\\|\\mathbf{p}-\\mathbf{m}_{i}\\|}-\\frac{\\mathbf{p}-\\mathbf{m}_{1}}{\\|\\mathbf{p}-\\mathbf{m}_{1}\\|}\\right),\\quad i\\in\\{2,3\\}.\n$$\n\nEvaluate at the initial guess $\\mathbf{p}^{(0)}=\\left(\\frac{L}{2},\\frac{L}{2}\\right)$ with $\\mathbf{m}_{1}=(0,0)$, $\\mathbf{m}_{2}=(L,0)$, $\\mathbf{m}_{3}=(0,L)$. Then\n$$\n\\mathbf{p}^{(0)}-\\mathbf{m}_{1}=\\left(\\frac{L}{2},\\frac{L}{2}\\right),\\quad\n\\mathbf{p}^{(0)}-\\mathbf{m}_{2}=\\left(-\\frac{L}{2},\\frac{L}{2}\\right),\\quad\n\\mathbf{p}^{(0)}-\\mathbf{m}_{3}=\\left(\\frac{L}{2},-\\frac{L}{2}\\right),\n$$\nand their norms are all equal:\n$$\n\\left\\|\\mathbf{p}^{(0)}-\\mathbf{m}_{1}\\right\\|=\\left\\|\\mathbf{p}^{(0)}-\\mathbf{m}_{2}\\right\\|=\\left\\|\\mathbf{p}^{(0)}-\\mathbf{m}_{3}\\right\\|=\\frac{L}{\\sqrt{2}}.\n$$\nTherefore the unit vectors are\n$$\n\\frac{\\mathbf{p}^{(0)}-\\mathbf{m}_{1}}{\\|\\mathbf{p}^{(0)}-\\mathbf{m}_{1}\\|}=\\left(\\frac{1}{\\sqrt{2}},\\frac{1}{\\sqrt{2}}\\right),\\quad\n\\frac{\\mathbf{p}^{(0)}-\\mathbf{m}_{2}}{\\|\\mathbf{p}^{(0)}-\\mathbf{m}_{2}\\|}=\\left(-\\frac{1}{\\sqrt{2}},\\frac{1}{\\sqrt{2}}\\right),\\quad\n\\frac{\\mathbf{p}^{(0)}-\\mathbf{m}_{3}}{\\|\\mathbf{p}^{(0)}-\\mathbf{m}_{3}\\|}=\\left(\\frac{1}{\\sqrt{2}},-\\frac{1}{\\sqrt{2}}\\right).\n$$\nThe Jacobian of the model functions $(h_2, h_3)$ has rows $\\nabla h_i$. The Jacobian of the residuals is $J_{res} = -J_h$. Let's use the main text definition where $J$ is the jacobian of the residuals. $J_{ij} = -\\frac{\\partial h_i}{\\partial p_j}$.\n$$\n\\text{row 1 of J} = -\\nabla h_{2}(\\mathbf{p}^{(0)})=-\\frac{1}{v_{s}}\\left(\\left(-\\frac{1}{\\sqrt{2}},\\frac{1}{\\sqrt{2}}\\right)-\\left(\\frac{1}{\\sqrt{2}},\\frac{1}{\\sqrt{2}}\\right)\\right)=\\left(\\frac{\\sqrt{2}}{v_{s}},\\,0\\right),\n$$\n$$\n\\text{row 2 of J} = -\\nabla h_{3}(\\mathbf{p}^{(0)})=-\\frac{1}{v_{s}}\\left(\\left(\\frac{1}{\\sqrt{2}},-\\frac{1}{\\sqrt{2}}\\right)-\\left(\\frac{1}{\\sqrt{2}},\\frac{1}{\\sqrt{2}}\\right)\\right)=\\left(0,\\,\\frac{\\sqrt{2}}{v_{s}}\\right).\n$$\nThus\n$$\nJ=\\begin{pmatrix}\\frac{\\sqrt{2}}{v_{s}} & 0\\\\ 0 & \\frac{\\sqrt{2}}{v_{s}}\\end{pmatrix},\\qquad\nJ^{T}J=\\begin{pmatrix}\\frac{2}{v_{s}^{2}} & 0\\\\ 0 & \\frac{2}{v_{s}^{2}}\\end{pmatrix},\\qquad\n(J^{T}J)^{-1}=\\begin{pmatrix}\\frac{v_{s}^{2}}{2} & 0\\\\ 0 & \\frac{v_{s}^{2}}{2}\\end{pmatrix}.\n$$\n\nAt $\\mathbf{p}^{(0)}$, the model predictions are $h_{2}(\\mathbf{p}^{(0)})=h_{3}(\\mathbf{p}^{(0)})=0$ (equal distances), so with $\\tau_{21}=\\tau_{31}=\\tau$,\n$$\n\\mathbf{r}=\\begin{pmatrix}\\tau\\\\ \\tau\\end{pmatrix},\\qquad\nJ^{T}\\mathbf{r}=\\begin{pmatrix}\\frac{\\sqrt{2}}{v_{s}}\\,\\tau\\\\ \\frac{\\sqrt{2}}{v_{s}}\\,\\tau\\end{pmatrix}.\n$$\nTherefore the Gauss-Newton step is\n$$\n\\Delta\\mathbf{p}=-(J^{T}J)^{-1}J^{T}\\mathbf{r}=-\\begin{pmatrix}\\frac{v_{s}^{2}}{2} & 0\\\\ 0 & \\frac{v_{s}^{2}}{2}\\end{pmatrix}\\begin{pmatrix}\\frac{\\sqrt{2}}{v_{s}}\\,\\tau\\\\ \\frac{\\sqrt{2}}{v_{s}}\\,\\tau\\end{pmatrix} = -\\begin{pmatrix}\\frac{v_{s}\\,\\tau}{\\sqrt{2}}\\\\ \\frac{v_{s}\\,\\tau}{\\sqrt{2}}\\end{pmatrix}.\n$$\nThe update yields\n$$\n\\mathbf{p}^{(1)}=\\mathbf{p}^{(0)}+\\Delta\\mathbf{p}=\\left(\\frac{L}{2}-\\frac{v_{s}\\,\\tau}{\\sqrt{2}},\\,\\frac{L}{2}-\\frac{v_{s}\\,\\tau}{\\sqrt{2}}\\right).\n$$\n\nWith the given values $L=100$, $v_{s}=340$, and $\\tau=\\frac{5(1-\\sqrt{2})}{17}$, the step components are:\n$$\n-\\frac{v_{s}\\,\\tau}{\\sqrt{2}}=-\\frac{340\\cdot \\frac{5(1-\\sqrt{2})}{17}}{\\sqrt{2}}=-\\frac{100(1-\\sqrt{2})}{\\sqrt{2}}=100\\left(\\frac{1}{\\sqrt{2}}-1\\right) = 50\\sqrt{2}-100.\n$$\nso\n$$\n\\mathbf{p}^{(1)}=\\left(\\frac{100}{2} + 50\\sqrt{2}-100,\\,\\frac{100}{2} + 50\\sqrt{2}-100\\right)=\\left(50\\sqrt{2}-50,\\,50\\sqrt{2}-50\\right).\n$$", "answer": "$$\\boxed{(50\\sqrt{2} - 50, 50\\sqrt{2} - 50)}$$", "id": "2191258"}]}