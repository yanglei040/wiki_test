{"hands_on_practices": [{"introduction": "The concept of positive semidefiniteness is not just an abstract matrix property; it is the cornerstone of convex optimization. This practice will guide you through verifying the convexity of a function by examining its Hessian matrix, a fundamental skill for understanding and formulating optimization problems. By working through this example [@problem_id:2201476], you will solidify your understanding of how matrix properties directly translate into the geometric properties of functions.", "problem": "In the field of convex optimization, a crucial property of a cost function is its convexity, as this guarantees that any local minimum found is also a global minimum. For a twice-differentiable function $f(\\mathbf{x})$ defined on a convex set, it is convex if and only if its Hessian matrix, $H(\\mathbf{x})$, is positive semidefinite for all $\\mathbf{x}$ in its domain.\n\nConsider a quadratic cost function used to model a two-product manufacturing process, given by:\n$$f(x_1, x_2) = x_1^2 + \\alpha x_1 x_2 + \\frac{3}{2}x_2^2$$\nHere, $x_1$ and $x_2$ represent non-negative production quantities, and $\\alpha$ is a real-valued coupling constant that models the economic interaction between the two production lines. A key objective for process stability and predictability is to ensure the cost function is convex over its entire domain.\n\nDetermine the complete range of values for the coupling constant $\\alpha$ that ensures the cost function $f(x_1, x_2)$ is convex.\n\nA) $[-\\sqrt{5}, \\sqrt{5}]$\n\nB) $[-\\sqrt{6}, \\sqrt{6}]$\n\nC) $[-2, 2]$\n\nD) $[-3, 3]$\n\nE) The function is convex for all real $\\alpha$.\n\nF) The function is never convex for any real $\\alpha$.", "solution": "We use the Hessian criterion for convexity of a twice-differentiable function on a convex set: $f$ is convex if and only if its Hessian is positive semidefinite at every point in the domain.\n\nGiven\n$$f(x_{1}, x_{2}) = x_{1}^{2} + \\alpha x_{1}x_{2} + \\frac{3}{2}x_{2}^{2},$$\nits gradient is\n$$\\nabla f(x_{1}, x_{2}) = \\begin{pmatrix} 2x_{1} + \\alpha x_{2} \\\\ \\alpha x_{1} + 3x_{2} \\end{pmatrix},$$\nand the Hessian matrix, which is constant, is\n$$H = \\nabla^{2} f = \\begin{pmatrix} 2 & \\alpha \\\\ \\alpha & 3 \\end{pmatrix}.$$\n\nSince $H$ is a symmetric $2 \\times 2$ matrix, it is positive semidefinite if and only if all its leading principal minors are nonnegative:\n- The first leading principal minor is $2 \\ge 0$, which holds.\n- The determinant must satisfy $\\det(H) = 2 \\cdot 3 - \\alpha^{2} = 6 - \\alpha^{2} \\ge 0.$\nThis yields\n$$\\alpha^{2} \\le 6 \\quad \\Longleftrightarrow \\quad |\\alpha| \\le \\sqrt{6}.$$\n\nTherefore, $f$ is convex if and only if $\\alpha \\in [-\\sqrt{6}, \\sqrt{6}]$, which corresponds to option B.", "answer": "$$\\boxed{B}$$", "id": "2201476"}, {"introduction": "Having established the link between positive semidefinite (PSD) matrices and convexity, we now turn to a canonical semidefinite program. This exercise [@problem_id:2201478] presents a core SDP task: maximizing a linear function of a matrix variable, $\\text{tr}(CX)$, subject to the constraint that the matrix $X$ is PSD. Solving this problem reveals a profound connection between the solution of the SDP and the spectral properties of the cost matrix $C$, offering a glimpse into the elegant structure of these problems.", "problem": "Consider a $2 \\times 2$ real symmetric matrix $X$ defined as\n$$\nX = \\begin{pmatrix} x_{11} & x_{12} \\\\ x_{12} & x_{22} \\end{pmatrix}.\n$$\nThis matrix is subject to two conditions:\n1. It is positive semidefinite, meaning all its eigenvalues are non-negative.\n2. The sum of its diagonal entries is equal to one, i.e., $x_{11} + x_{22} = 1$.\n\nNow, consider another constant $2 \\times 2$ real symmetric matrix $C$ given by\n$$\nC = \\begin{pmatrix} 3 & -1 \\\\ -1 & 5 \\end{pmatrix}.\n$$\nYour task is to find the maximum possible value of the trace of the product of these two matrices, $\\text{tr}(CX)$. The trace of a square matrix is the sum of the elements on its main diagonal. Present your final answer as a single closed-form analytic expression.", "solution": "We need to maximize $\\text{tr}(CX)$ over real symmetric $X$ such that $X \\succeq 0$ and $\\text{tr}(X) = 1$. The feasible set $\\{X \\succeq 0, \\text{tr}(X)=1\\}$ is convex and compact, and the objective $\\text{tr}(CX)$ is linear in $X$, so the maximum is attained at an extreme point of the feasible set.\n\nEvery $X \\succeq 0$ with $\\text{tr}(X)=1$ has a spectral decomposition $X = \\sum_{i} \\lambda_{i} u_{i} u_{i}^{\\top}$ where $\\lambda_{i} \\ge 0$, $\\sum_{i} \\lambda_{i} = 1$, and $\\{u_{i}\\}$ are orthonormal vectors. Thus,\n$$\n\\text{tr}(CX) = \\text{tr}\\!\\left(C \\sum_{i} \\lambda_{i} u_{i} u_{i}^{\\top}\\right) = \\sum_{i} \\lambda_{i} \\text{tr}(C u_{i} u_{i}^{\\top}) = \\sum_{i} \\lambda_{i} u_{i}^{\\top} C u_{i}.\n$$\nSince $\\lambda_{i}$ form a convex combination, we have\n$$\n\\text{tr}(CX) \\le \\sum_{i} \\lambda_{i} \\lambda_{\\max}(C) = \\lambda_{\\max}(C),\n$$\nwith equality achieved by choosing $X = vv^{\\top}$ where $v$ is a unit eigenvector of $C$ corresponding to its largest eigenvalue. Therefore, the maximum value is $\\lambda_{\\max}(C)$.\n\nWe compute the eigenvalues of $C = \\begin{pmatrix} 3 & -1 \\\\ -1 & 5 \\end{pmatrix}$ from its characteristic polynomial:\n$$\n\\det(C - \\lambda I) = (3 - \\lambda)(5 - \\lambda) - (-1)(-1) = (3 - \\lambda)(5 - \\lambda) - 1.\n$$\nExpanding,\n$$\n(3 - \\lambda)(5 - \\lambda) - 1 = 15 - 3\\lambda - 5\\lambda + \\lambda^{2} - 1 = \\lambda^{2} - 8\\lambda + 14.\n$$\nSolving $\\lambda^{2} - 8\\lambda + 14 = 0$ gives\n$$\n\\lambda = \\frac{8 \\pm \\sqrt{64 - 56}}{2} = \\frac{8 \\pm \\sqrt{8}}{2} = 4 \\pm \\sqrt{2}.\n$$\nThus $\\lambda_{\\max}(C) = 4 + \\sqrt{2}$, which is the maximum value of $\\text{tr}(CX)$ under the given constraints.", "answer": "$$\\boxed{4+\\sqrt{2}}$$", "id": "2201478"}, {"introduction": "Semidefinite programming truly shines when applied to problems that are computationally \"hard\" in their original form. This practice explores the celebrated Goemans-Williamson relaxation for the Max-Cut problem, a classic example of an NP-hard challenge. You will see how replacing discrete variables with vectors allows us to \"relax\" the problem into a solvable SDP [@problem_id:2201518], yielding a powerful approximation algorithm and demonstrating SDP's utility in tackling complex combinatorial optimization tasks.", "problem": "The Maximum Cut (Max-Cut) problem in graph theory seeks to partition the vertices of a graph into two disjoint sets, say $S_1$ and $S_2$, such that the sum of the weights of the edges connecting a vertex in $S_1$ to a vertex in $S_2$ is maximized. While this problem is computationally difficult for general graphs, approximation methods exist.\n\nOne approach involves formulating the problem algebraically. For a graph with vertices $V = \\{1, 2, ..., n\\}$ and weighted edges $(i,j)$ with weight $w_{ij}$, we can assign a variable $y_i \\in \\{-1, 1\\}$ to each vertex $i$. A partition is then defined by which vertices are assigned $+1$ and which are assigned $-1$. The weight of the cut can be expressed as the objective function to be maximized:\n$$\n\\text{maximize} \\quad \\frac{1}{2} \\sum_{(i,j) \\in E} w_{ij}(1 - y_i y_j)\n$$\nwhere $E$ is the set of edges.\n\nA powerful approximation technique involves \"relaxing\" this problem. Instead of using discrete scalar variables $y_i \\in \\{-1, 1\\}$, we associate each vertex $i$ with a continuous vector variable $v_i$ in a higher-dimensional space. The only constraint on an individual vector is that it must be a unit vector, i.e., $v_i \\cdot v_i = 1$. The product of scalars $y_i y_j$ is replaced by the dot product of vectors $v_i \\cdot v_j$. The relaxed optimization problem becomes:\n$$\n\\text{maximize} \\quad \\frac{1}{2} \\sum_{(i,j) \\in E} w_{ij}(1 - v_i \\cdot v_j)\n$$\n$$\n\\text{subject to} \\quad v_i \\cdot v_i = 1 \\quad \\text{for all } i \\in V.\n$$\n\nConsider a simple triangular graph with three vertices, labeled 1, 2, and 3. Every vertex is connected to every other vertex, forming the edges (1,2), (2,3), and (3,1). All edges have a weight of 1.\n\nDetermine the exact maximum value of the objective function for this relaxed problem. Express your final answer as a decimal rounded to three significant figures.", "solution": "For the triangle with vertices $\\{1,2,3\\}$ and unit edge weights, the relaxed objective is\n$$\n\\max \\; \\frac{1}{2}\\sum_{(i,j)\\in E}\\left(1 - v_{i}\\cdot v_{j}\\right),\n$$\nwith constraints $v_{i}\\cdot v_{i}=1$ for all $i$. Since there are three edges, we define\n$$\nS \\equiv v_{1}\\cdot v_{2} + v_{2}\\cdot v_{3} + v_{3}\\cdot v_{1}.\n$$\nThen the objective becomes\n$$\n\\frac{1}{2}\\left(3 - S\\right).\n$$\nMaximizing the objective is equivalent to minimizing $S$ subject to the unit-vector constraints.\n\nIntroduce the Gram matrix $G$ with entries $G_{ij} = v_{i}\\cdot v_{j}$. Feasibility of vectors $\\{v_{i}\\}$ is equivalent to $G\\succeq 0$ and $G_{ii}=1$ for all $i$. The objective depends on $G$ only through the sum of its off-diagonal entries $S = G_{12}+G_{23}+G_{31}$.\n\nBy symmetry of the problem (constraints and objective are invariant under permutations of $\\{1,2,3\\}$), there is an optimal Gram matrix with equal off-diagonal entries. Averaging any feasible $G$ over all $6$ permutations yields a feasible matrix with the same value of $S$ and off-diagonals equal. Hence, without loss of generality, we set\n$$\nG(t) = \\begin{pmatrix}\n1 & t & t\\\\\nt & 1 & t\\\\\nt & t & 1\n\\end{pmatrix},\n\\qquad S = 3t.\n$$\nThe eigenvalues of $G(t)$ are $1- t$ (with multiplicity $2$) and $1+2t$ (with multiplicity $1$). Positive semidefiniteness requires\n$$\n1 - t \\ge 0 \\quad \\text{and} \\quad 1 + 2t \\ge 0,\n$$\nthat is,\n$$\n- \\frac{1}{2} \\le t \\le 1.\n$$\nSince $S=3t$, minimizing $S$ over the feasible interval yields $t^{\\star} = -\\frac{1}{2}$ and\n$$\nS^{\\star} = 3\\left(-\\frac{1}{2}\\right) = -\\frac{3}{2}.\n$$\nTherefore, the maximum objective value is\n$$\n\\frac{1}{2}\\left(3 - S^{\\star}\\right)\n= \\frac{1}{2}\\left(3 + \\frac{3}{2}\\right)\n= \\frac{1}{2}\\cdot \\frac{9}{2}\n= \\frac{9}{4}.\n$$\nThis value is attainable, for example, by three unit vectors in the plane separated by angles $2\\pi/3$, since then $v_{i}\\cdot v_{j} = \\cos(2\\pi/3) = -1/2$ for $i\\neq j$. Converting $\\frac{9}{4}$ to a decimal and rounding to three significant figures gives $2.25$.", "answer": "$$\\boxed{2.25}$$", "id": "2201518"}]}