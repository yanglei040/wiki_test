{"hands_on_practices": [{"introduction": "Claude Shannon's foundational insight was that information could be quantified as the reduction of uncertainty. This first practice invites you to compute the Shannon entropy for a simple system, which measures the average uncertainty, or \"surprise,\" inherent in a random variable's possible outcomes. By working through the provided hypothetical weather model, you will gain a concrete understanding of how the probabilities of different events determine the amount of information a source produces. [@problem_id:1659073]", "problem": "A simplified meteorological model describes the weather for the next day as a discrete random variable $W$ with four possible outcomes: Sunny, Cloudy, Rainy, or Snowy. The model is treated as a discrete memoryless source. Based on extensive historical data for a particular city, the probabilities of these outcomes are determined to be:\n- $P(W=\\text{Sunny}) = 0.60$\n- $P(W=\\text{Cloudy}) = 0.25$\n- $P(W=\\text{Rainy}) = 0.10$\n- $P(W=\\text{Snowy}) = 0.05$\nCalculate the Shannon entropy of this weather source. Express your answer in bits, rounded to four significant figures.", "solution": "For a discrete memoryless source with outcomes having probabilities $\\{p_{i}\\}$, the Shannon entropy in bits is defined by\n$$\nH(W)=-\\sum_{i} p_{i}\\log_{2}(p_{i}).\n$$\nSubstituting the given probabilities,\n$$\nH(W)=-\\big(0.60\\log_{2}(0.60)+0.25\\log_{2}(0.25)+0.10\\log_{2}(0.10)+0.05\\log_{2}(0.05)\\big).\n$$\nUsing the change-of-base formula $\\log_{2}(x)=\\frac{\\ln(x)}{\\ln(2)}$, evaluate each term:\n$$\n\\log_{2}(0.60)\\approx -0.7369655942 \\;\\Rightarrow\\; -0.60\\log_{2}(0.60)\\approx 0.4421793565,\n$$\n$$\n\\log_{2}(0.25)=-2 \\;\\Rightarrow\\; -0.25\\log_{2}(0.25)=0.5000000000,\n$$\n$$\n\\log_{2}(0.10)\\approx -3.3219280949 \\;\\Rightarrow\\; -0.10\\log_{2}(0.10)\\approx 0.3321928095,\n$$\n$$\n\\log_{2}(0.05)\\approx -4.3219280949 \\;\\Rightarrow\\; -0.05\\log_{2}(0.05)\\approx 0.2160964047.\n$$\nSumming these contributions gives\n$$\nH(W)\\approx 0.4421793565+0.5000000000+0.3321928095+0.2160964047=1.4904685707 \\text{ bits}.\n$$\nRounding to four significant figures yields $1.490$ bits.", "answer": "$$\\boxed{1.490}$$", "id": "1659073"}, {"introduction": "After quantifying information, Shannon's next great challenge was to define the limits of transmitting it reliably through a noisy medium. This exercise explores the concept of channel capacity, a cornerstone of his noisy-channel coding theorem, which establishes the maximum rate for error-free communication. By modeling a volatile memory cell as a Binary Symmetric Channel, you will calculate the ultimate limit on how much information can be reliably stored and retrieved over time despite random errors. [@problem_id:1610553]", "problem": "Consider a simplified model for a volatile computational memory system, structured as a one-dimensional tape of cells. Each cell can store a single bit, either a 0 or a 1. The memory is subject to noise. At each discrete time step of a computational clock, the bit stored in any given cell has a constant probability $p$ of spontaneously flipping to its opposite state (i.e., 0 becomes 1, or 1 becomes 0), where $0 \\le p \\le 1$. This flipping event is independent for each cell and for each time step.\n\nWe can analyze this memory system from an information-theoretic perspective by modeling a single cell as a communication channel through time. The input to this channel is the state of the cell at a time $t$, and the output is the state of that same cell at the next time step, $t+1$.\n\nDetermine the Shannon capacity of this single-cell memory channel. Your answer should be an analytic expression in terms of the flip probability $p$. All logarithms used must be base 2. Provide the capacity in units of bits per cell per time step.", "solution": "Let $X \\in \\{0,1\\}$ denote the state of a given cell at time $t$ and $Y \\in \\{0,1\\}$ the state of that same cell at time $t+1$. By the problemâ€™s dynamics, the transition law is\n$$\n\\Pr(Y=X)=1-p,\\quad \\Pr(Y\\neq X)=p,\n$$\nso this is a binary symmetric channel (BSC) with crossover probability $p$ used once per time step.\n\nLet the input distribution be $\\Pr(X=1)=q$ and $\\Pr(X=0)=1-q$. The output distribution is\n$$\n\\Pr(Y=1)=\\Pr(Y=1|X=1)\\Pr(X=1)+\\Pr(Y=1|X=0)\\Pr(X=0)\n= (1-p)q + p(1-q) = p + q(1-2p).\n$$\nDefine $r=\\Pr(Y=1)=p+q(1-2p)$. The mutual information between $X$ and $Y$ for a fixed input distribution is\n$$\nI(X;Y)=H(Y)-H(Y|X).\n$$\nBecause the channel is a BSC with crossover probability $p$, the conditional entropy is\n$$\nH(Y|X)=H_{2}(p)=-p\\log_{2}p-(1-p)\\log_{2}(1-p).\n$$\nThe output entropy is\n$$\nH(Y)=H_{2}(r)=-r\\log_{2}r-(1-r)\\log_{2}(1-r).\n$$\nTherefore,\n$$\nI(X;Y)=H_{2}(r)-H_{2}(p)=H_{2}\\big(p+q(1-2p)\\big)-H_{2}(p).\n$$\nThe Shannon capacity per channel use (per cell per time step) is the maximum of $I(X;Y)$ over the input distribution $q \\in [0,1]$. Since the second term $H_{2}(p)$ is independent of $q$, we must maximize $H_{2}(r)$ with respect to $q$ (equivalently with respect to $r$). The binary entropy $H_{2}(r)$ is concave in $r$ and is maximized at $r=\\tfrac{1}{2}$. To verify, compute\n$$\n\\frac{d}{dr}H_{2}(r)=-\\log_{2}r - \\frac{1}{\\ln 2} + \\log_{2}(1-r) + \\frac{1}{\\ln 2}\n=\\log_{2}\\!\\left(\\frac{1-r}{r}\\right),\n$$\nwhich vanishes if and only if $r=\\tfrac{1}{2}$; the second derivative is negative, so this is a maximum. Imposing $r=\\tfrac{1}{2}$ yields\n$$\np+q(1-2p)=\\frac{1}{2}\\quad\\Longrightarrow\\quad q^{\\star}=\\frac{\\frac{1}{2}-p}{1-2p}.\n$$\nFor $p\\neq \\tfrac{1}{2}$ this simplifies to $q^{\\star}=\\tfrac{1}{2}$; for $p=\\tfrac{1}{2}$ one has $r=\\tfrac{1}{2}$ for any $q$, and the capacity will be zero as shown below.\n\nAt the maximizing input, $H(Y)=H_{2}(\\tfrac{1}{2})=1$, so the channel capacity is\n$$\nC=\\max_{q} I(X;Y)=1 - H_{2}(p).\n$$\nWriting $H_{2}(p)$ explicitly with base-2 logarithms gives\n$$\nC = 1 + p\\log_{2}p + (1-p)\\log_{2}(1-p),\n$$\nwith the usual convention that $0\\log_{2}0=0$. This capacity is in bits per cell per time step, equals $1$ at $p=0$ or $p=1$, and equals $0$ at $p=\\tfrac{1}{2}$.", "answer": "$$\\boxed{1 + p\\log_{2}p + (1-p)\\log_{2}(1-p)}$$", "id": "1610553"}, {"introduction": "Building upon the concept of channel capacity, this final practice moves from discrete models to the continuous domain, tackling the ubiquitous Additive White Gaussian Noise (AWGN) channel. The problem introduces a realistic complication where the noise level itself depends on the power of the transmitted signal, a scenario common in advanced communication systems. Solving this requires you to not just apply a formula, but to use optimization to find the best transmission strategy, demonstrating the practical power and flexibility of Shannon's framework in real-world engineering design. [@problem_id:1610570]", "problem": "A deep-space communication system is designed to transmit data over vast interstellar distances. The communication channel is modeled as an Additive White Gaussian Noise (AWGN) channel with a fixed bandwidth $W$. A unique characteristic of this channel is that the high-power radio signals transmitted by the probe interact with the tenuous interstellar plasma, generating a secondary noise component. This results in a total noise power, $\\sigma_N^2$, that depends on the transmitted signal power, $P$. The relationship is well-approximated by the model:\n\n$$ \\sigma_N^2(P) = N_0 + \\beta P^2 $$\n\nHere, $N_0$ is the constant background noise power (from cosmic microwave background and receiver electronics), and $\\beta$ is a positive dimensionless coefficient that quantifies the strength of the signal-plasma interaction. The transmitter on the space probe is subject to an average power constraint, such that the utilized signal power $P$ cannot exceed a maximum value $P_{max}$.\n\nAssuming the use of an optimal encoding scheme that approaches the Shannon limit, determine a single, closed-form analytic expression for the capacity $C$ of this communication channel in terms of $W$, $N_0$, $\\beta$, and $P_{max}$.", "solution": "The continuous-time AWGN channel of bandwidth $W$ with total (in-band) noise power $N$ and average signal power $P$ has Shannon capacity\n$$\nC(P) = W \\log_{2}\\!\\left(1 + \\frac{P}{N}\\right),\n$$\nassuming optimal coding. In this problem, the total noise power depends on the chosen transmit power $P$ as\n$$\n\\sigma_{N}^{2}(P) = N_{0} + \\beta P^{2},\n$$\nwith $P$ constrained by $0 \\leq P \\leq P_{max}$. Substituting $N = \\sigma_{N}^{2}(P)$ gives the capacity as a function of $P$:\n$$\nC(P) = W \\log_{2}\\!\\left(1 + \\frac{P}{N_{0} + \\beta P^{2}}\\right).\n$$\n\nTo satisfy the power constraint and maximize capacity, we optimize over $P \\in [0, P_{max}]$. Define the signal-to-noise ratio\n$$\n\\mathrm{SNR}(P) = \\frac{P}{N_{0} + \\beta P^{2}}.\n$$\nThen\n$$\nC(P) = \\frac{W}{\\ln 2}\\,\\ln\\!\\bigl(1 + \\mathrm{SNR}(P)\\bigr).\n$$\nSince $\\ln(1+x)$ is strictly increasing, maximizing $C(P)$ is equivalent to maximizing $\\mathrm{SNR}(P)$ over $P \\in [0, P_{max}]$. Differentiate $\\mathrm{SNR}(P)$:\n$$\n\\frac{d}{dP}\\mathrm{SNR}(P) = \\frac{N_{0} - \\beta P^{2}}{\\bigl(N_{0} + \\beta P^{2}\\bigr)^{2}}.\n$$\nSetting the derivative to zero yields the unique interior critical point\n$$\nN_{0} - \\beta P^{2} = 0 \\quad \\Rightarrow \\quad P_{0} = \\sqrt{\\frac{N_{0}}{\\beta}}.\n$$\nFor $P  P_{0}$, the derivative is positive; for $P  P_{0}$, it is negative. Hence $P_{0}$ is the global maximizer of $\\mathrm{SNR}(P)$ over $P \\geq 0$. Enforcing the constraint $P \\leq P_{max}$ gives the optimal transmit power\n$$\nP^{\\star} = \\min\\!\\left(P_{max}, \\sqrt{\\frac{N_{0}}{\\beta}}\\right).\n$$\nSubstituting $P^{\\star}$ back into the capacity expression gives the channel capacity under the power-dependent noise model:\n$$\nC = W \\log_{2}\\!\\left(1 + \\frac{P^{\\star}}{N_{0} + \\beta \\bigl(P^{\\star}\\bigr)^{2}}\\right), \\quad P^{\\star} = \\min\\!\\left(P_{max}, \\sqrt{\\frac{N_{0}}{\\beta}}\\right).\n$$\nThis is a single, closed-form analytic expression in terms of $W$, $N_{0}$, $\\beta$, and $P_{max}$.", "answer": "$$\\boxed{W \\log_{2}\\!\\left(1 + \\frac{\\min\\!\\left(P_{max},\\, \\sqrt{N_{0}/\\beta}\\right)}{\\,N_{0} + \\beta\\,\\min\\!\\left(P_{max},\\, \\sqrt{N_{0}/\\beta}\\right)^{2}}\\right)}$$", "id": "1610570"}]}