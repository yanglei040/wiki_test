{"hands_on_practices": [{"introduction": "The journey into information theory begins with a simple but profound question: how much information is in a message? This practice takes us back to a time before formal definitions, using the familiar context of a card game to build intuition. By calculating the information gained when learning about a drawn card, we will see how the mathematical concept of self-information formalizes our intuitive sense of \"surprise\"—rare events convey more information than common ones. This exercise [@problem_id:1629826] lays the groundwork for quantifying information as a physical and mathematical entity.", "problem": "Imagine yourself in the late 19th century, decades before the formalization of information theory. You are observing a card game where a single card is drawn from a well-shuffled, standard 52-card deck. The deck consists of four suits (Clubs, Diamonds, Hearts, Spades), and each suit has 13 ranks (2, 3, 4, 5, 6, 7, 8, 9, 10, Jack, Queen, King, Ace). The cards Jack, Queen, and King are known as \"face cards\".\n\nAn observer, who cannot see the card, receives two pieces of information in sequence. First, they are told, \"The card is a face card.\" After absorbing this fact, they are then told, \"The card is a spade.\"\n\nCalculate the total quantity of new information the observer has gained from both of these announcements combined. Express your answer in bits as a single, exact analytical expression.", "solution": "Let's denote the initial sample space of all possible outcomes as $\\Omega$, where $|\\Omega| = 52$. The problem asks for the total information gained from two successive announcements. We can calculate this by summing the information gained at each step.\n\nThe information content, or self-information, of an event $E$ is given by the formula $I(E) = -\\log_2(P(E))$, where $P(E)$ is the probability of the event. The base-2 logarithm ensures the result is in bits.\n\n**Step 1: Information from the first announcement**\n\nLet $A$ be the event that the drawn card is a face card.\nIn a standard 52-card deck, there are 3 face cards (Jack, Queen, King) in each of the 4 suits.\nThe total number of face cards is $N_A = 3 \\times 4 = 12$.\nThe probability of drawing a face card is:\n$$P(A) = \\frac{\\text{Number of face cards}}{\\text{Total number of cards}} = \\frac{12}{52} = \\frac{3}{13}$$\nThe information gained from learning that event $A$ has occurred is:\n$$I(A) = -\\log_2(P(A)) = -\\log_2\\left(\\frac{3}{13}\\right) = \\log_2\\left(\\frac{13}{3}\\right)$$\n\n**Step 2: Information from the second announcement**\n\nLet $B$ be the event that the drawn card is a spade.\nThe second announcement is made after it is already known that the card is a face card. Therefore, we need to calculate the conditional information of event $B$ given that event $A$ has already occurred. This is given by $I(B|A) = -\\log_2(P(B|A))$.\n\nThe conditional probability $P(B|A)$ is the probability that the card is a spade, given that it is a face card. Our sample space is now reduced to the 12 face cards.\nAmong these 12 face cards, there are 3 that are spades: the Jack of Spades, the Queen of Spades, and the King of Spades.\nSo, the number of outcomes corresponding to \"spade and face card\" is 3.\n\nThe conditional probability is:\n$$P(B|A) = \\frac{\\text{Number of face cards that are spades}}{\\text{Total number of face cards}} = \\frac{3}{12} = \\frac{1}{4}$$\nThe information gained from the second announcement, given the first, is:\n$$I(B|A) = -\\log_2(P(B|A)) = -\\log_2\\left(\\frac{1}{4}\\right) = \\log_2(4) = 2 \\text{ bits}$$\n\n**Step 3: Total Information**\n\nThe total information gained is the sum of the information from each successive announcement:\n$$I_{\\text{total}} = I(A) + I(B|A)$$\n$$I_{\\text{total}} = \\log_2\\left(\\frac{13}{3}\\right) + \\log_2(4)$$\nUsing the logarithm property $\\log(x) + \\log(y) = \\log(xy)$:\n$$I_{\\text{total}} = \\log_2\\left(\\frac{13}{3} \\times 4\\right) = \\log_2\\left(\\frac{52}{3}\\right)$$\n\n**Alternative Method (Verification)**\n\nThe total information gained is equivalent to the information content of the final event, which is that the card is both a face card and a spade. Let this intersection event be $C = A \\cap B$.\nThe outcomes for event $C$ are {Jack of Spades, Queen of Spades, King of Spades}. There are 3 such cards in the original 52-card deck.\nThe probability of this event is:\n$$P(C) = P(A \\cap B) = \\frac{3}{52}$$\nThe total information is:\n$$I(C) = -\\log_2(P(C)) = -\\log_2\\left(\\frac{3}{52}\\right) = \\log_2\\left(\\frac{52}{3}\\right)$$\nBoth methods yield the same result, confirming the solution.", "answer": "$$\\boxed{\\log_{2}\\left(\\frac{52}{3}\\right)}$$", "id": "1629826"}, {"introduction": "Having established how to measure information in a single event, we now consider a source that produces a stream of messages. This exercise [@problem_id:1629773] places you in the role of an early communications engineer designing a railway signaling system, a classic problem that motivated the development of efficient codes. You will compare a practical, fixed-length code with the theoretical minimum average length, given by the source's entropy. In doing so, you will calculate the code's redundancy, a key metric that quantifies inefficiency and highlights the fundamental trade-offs in source coding.", "problem": "An engineer in the early days of digital communication is designing a signaling system for a remote railway line. The system is simplified to transmit only three distinct messages: `NORMAL`, `CAUTION`, and `HALT`. Based on operational analysis, the engineer determines the long-term frequency of these messages: `NORMAL` signals constitute 50% of all transmissions, while `CAUTION` and `HALT` signals each constitute 25% of transmissions.\n\nFor simplicity of design and hardware, the engineer implements a fixed-length binary encoding scheme. In this scheme, each of the three messages is represented by a unique sequence of exactly 2 binary digits.\n\nThe redundancy of a code is defined as the average number of excess bits transmitted per message when compared to the theoretical minimum average number of bits required for an ideal encoding of the information source. Calculate the redundancy of the engineer's encoding scheme. Express your answer as a single numerical value in units of bits per symbol.", "solution": "The problem asks for the redundancy of a specific encoding scheme. Redundancy, as defined in the problem, is the difference between the actual average code length per symbol and the theoretical minimum average code length per symbol. The theoretical minimum is given by the entropy of the source.\n\nLet the set of messages be $S = \\{\\text{NORMAL}, \\text{CAUTION}, \\text{HALT}\\}$. We are given their probabilities and the lengths of their corresponding codewords.\n\nStep 1: Identify the probabilities and code lengths.\nThe probabilities of the symbols are:\n$p(\\text{NORMAL}) = 0.5$\n$p(\\text{CAUTION}) = 0.25$\n$p(\\text{HALT}) = 0.25$\n\nThe problem states that a fixed-length binary encoding of length 2 is used for all three messages. Thus, the code lengths are:\n$l(\\text{NORMAL}) = 2$\n$l(\\text{CAUTION}) = 2$\n$l(\\text{HALT}) = 2$\n\nStep 2: Calculate the entropy of the source.\nThe entropy, $H(S)$, gives the theoretical minimum average number of bits per symbol. The formula for entropy is:\n$$H(S) = -\\sum_{i \\in S} p(i) \\log_{2}(p(i))$$\n\nSubstituting the given probabilities:\n$$H(S) = - [p(\\text{NORMAL})\\log_{2}(p(\\text{NORMAL})) + p(\\text{CAUTION})\\log_{2}(p(\\text{CAUTION})) + p(\\text{HALT})\\log_{2}(p(\\text{HALT}))]$$\n$$H(S) = - [0.5 \\log_{2}(0.5) + 0.25 \\log_{2}(0.25) + 0.25 \\log_{2}(0.25)]$$\n\nWe evaluate the logarithm terms:\n$\\log_{2}(0.5) = \\log_{2}(2^{-1}) = -1$\n$\\log_{2}(0.25) = \\log_{2}(2^{-2}) = -2$\n\nNow substitute these values back into the entropy equation:\n$$H(S) = - [0.5(-1) + 0.25(-2) + 0.25(-2)]$$\n$$H(S) = - [-0.5 - 0.5 - 0.5]$$\n$$H(S) = - [-1.5] = 1.5 \\text{ bits/symbol}$$\n\nThis is the theoretical minimum average length for any code representing this source.\n\nStep 3: Calculate the average code length of the engineer's code.\nThe average code length, $L$, is the weighted average of the lengths of the codewords, where the weights are the probabilities of the symbols.\n$$L = \\sum_{i \\in S} p(i) l(i)$$\n\nSubstituting the given probabilities and code lengths:\n$$L = p(\\text{NORMAL})l(\\text{NORMAL}) + p(\\text{CAUTION})l(\\text{CAUTION}) + p(\\text{HALT})l(\\text{HALT})$$\n$$L = (0.5)(2) + (0.25)(2) + (0.25)(2)$$\n$$L = 1.0 + 0.5 + 0.5 = 2.0 \\text{ bits/symbol}$$\n\nThis is the actual average number of bits per symbol used by the engineer's fixed-length code.\n\nStep 4: Calculate the redundancy.\nThe redundancy, $R$, is the difference between the average code length and the entropy.\n$$R = L - H(S)$$\n\nSubstituting the values calculated in the previous steps:\n$$R = 2.0 \\text{ bits/symbol} - 1.5 \\text{ bits/symbol}$$\n$$R = 0.5 \\text{ bits/symbol}$$", "answer": "$$\\boxed{0.5}$$", "id": "1629773"}, {"introduction": "Efficiently encoding information is only half the battle; ensuring it arrives intact is the other. This final practice delves into the crucial topic of channel coding by examining an early error-detection scheme, a precursor to the more sophisticated codes developed by Richard Hamming and others. By analyzing a two-dimensional parity check system [@problem_id:1629782], your task is to discover its breaking point—the minimum number of errors that could fool the system. This hands-on analysis reveals the inherent challenges of reliable communication over noisy channels and demonstrates why more advanced error-correcting codes became essential.", "problem": "In the early days of digital communication, before the development of more advanced techniques like Hamming codes, simple error detection schemes were employed. One such method involved arranging data bits into a two-dimensional grid and adding parity bits for each row and column.\n\nConsider a scheme where 9 data bits, denoted as $d_{ij}$ with $i, j \\in \\{1, 2, 3\\}$, are arranged in a $3 \\times 3$ grid. For each of the three rows, an additional parity bit $p_{i,c}$ is appended. For each of the three columns, an additional parity bit $p_{r,j}$ is appended. This creates a $4 \\times 3$ block of data and row-parity bits, and a $1 \\times 4$ block of column-parity bits (including a parity bit for the column of row-parity bits). The complete transmitted block is a $4 \\times 4$ grid of bits.\n\nAll parity bits are calculated using an even parity scheme, meaning the sum of bits in any complete row (3 data bits + 1 parity bit) or any complete column (3 data bits + 1 parity bit) must be an even number. An error occurs when one or more bits in the original 9-bit data block are flipped (0 becomes 1, or 1 becomes 0) during transmission. An error pattern is considered \"undetected\" if, after the bits are flipped, all row and column parity checks on the received $4 \\times 4$ block still pass.\n\nAssuming errors only occur in the 9 data bits, what is the minimum number of bit-flip errors that can result in an undetected error?", "solution": "Let $d_{ij}$, with $i,j \\in \\{1,2,3\\}$, denote the original data bits and let $e_{ij} \\in \\{0,1\\}$ indicate whether $d_{ij}$ is flipped during transmission ($e_{ij}=1$ means a flip at position $(i,j)$). Errors occur only in the data bits, so all parity bits remain unchanged. For the even-parity checks on the received $4 \\times 4$ block to pass, the parity of each data row and each data column must be unchanged. Since the parity bits are fixed, this is equivalent to requiring that in each data row and each data column the number of flips is even.\n\nFormally, the undetected error condition is\n$$\n\\sum_{j=1}^{3} e_{ij} \\equiv 0 \\pmod{2} \\quad \\text{for } i=1,2,3,\n\\qquad\n\\sum_{i=1}^{3} e_{ij} \\equiv 0 \\pmod{2} \\quad \\text{for } j=1,2,3.\n$$\nLet the total number of flips be $w=\\sum_{i=1}^{3}\\sum_{j=1}^{3} e_{ij}$. First, $w$ must be even, because\n$$\nw=\\sum_{i=1}^{3}\\sum_{j=1}^{3} e_{ij} \\equiv \\sum_{i=1}^{3} \\left( \\sum_{j=1}^{3} e_{ij} \\right) \\equiv 0 \\pmod{2},\n$$\ngiven that each row sum is even. Hence $w$ cannot be $1$ or $3$.\n\nConsider $w=2$. There are three possibilities for the locations of the two flips:\n(i) same row, different columns: the affected row has sum $2 \\equiv 0 \\pmod{2}$, but the two affected columns each have sum $1 \\equiv 1 \\pmod{2}$, violating column parity;\n(ii) same column, different rows: the affected column has sum $2 \\equiv 0 \\pmod{2}$, but the two affected rows each have sum $1 \\equiv 1 \\pmod{2}$, violating row parity;\n(iii) different rows and different columns: two rows have sum $1$ and two columns have sum $1$, violating both row and column parities. Therefore $w=2$ is impossible.\n\nNow consider $w=4$. Choose two distinct rows $i_{1}\\neq i_{2}$ and two distinct columns $j_{1}\\neq j_{2}$, and set $e_{i_{1}j_{1}}=e_{i_{1}j_{2}}=e_{i_{2}j_{1}}=e_{i_{2}j_{2}}=1$ with all other $e_{ij}=0$. Then each of the rows $i_{1},i_{2}$ has exactly two flips (even), the remaining row has zero flips (even), and similarly each of the columns $j_{1},j_{2}$ has exactly two flips (even), the remaining column has zero flips (even). Thus all row and column parity checks pass. The parity checks involving the parity-only row and parity-only column also pass automatically since those bits are not flipped.\n\nTherefore, the minimal nonzero number of data-bit flips that yields an undetected error is $w=4$.", "answer": "$$\\boxed{4}$$", "id": "1629782"}]}