{"hands_on_practices": [{"introduction": "Let's begin with a foundational exercise. This practice grounds the abstract concept of binary entropy in a tangible calculation, connecting it directly to its most famous application: the fundamental limit of data compression. By working through this problem, you will compute the Shannon entropy for a given binary source and understand its physical meaning as the minimum average number of bits needed per symbol. [@problem_id:1604155]", "problem": "A digital communication system transmits data as a stream of binary symbols, either '0' or '1'. Due to the nature of the information being encoded, the symbols are not equally likely. A statistical analysis of a very long transmission reveals that the probability of a '1' appearing, which we denote as $p$, is $p = \\frac{1}{8}$. An engineer is tasked with designing an optimal lossless compression scheme for this data source. The theoretical limit for such compression is given by the average information content per symbol. Calculate this fundamental lower bound on the average number of bits required to represent each symbol from this source.\n\nExpress your answer in bits per symbol, rounded to three significant figures.", "solution": "The theoretical lower bound on the average number of bits per source symbol for optimal lossless compression is the Shannon entropy of the binary source. For a binary symbol $X$ with $P(X=1)=p$ and $P(X=0)=1-p$, the entropy in bits per symbol is\n$$\nH(X)=-p\\log_{2}(p)-(1-p)\\log_{2}(1-p).\n$$\nGiven $p=\\frac{1}{8}$ and $1-p=\\frac{7}{8}$, we compute\n$$\nH(X)=-\\frac{1}{8}\\log_{2}\\left(\\frac{1}{8}\\right)-\\frac{7}{8}\\log_{2}\\left(\\frac{7}{8}\\right).\n$$\nUse $\\log_{2}\\left(\\frac{1}{8}\\right)=\\log_{2}\\left(2^{-3}\\right)=-3$ and $\\log_{2}\\left(\\frac{7}{8}\\right)=\\log_{2}(7)-3$ to simplify:\n$$\nH(X)=-\\frac{1}{8}(-3)-\\frac{7}{8}\\left(\\log_{2}(7)-3\\right)=\\frac{3}{8}-\\frac{7}{8}\\log_{2}(7)+\\frac{21}{8}.\n$$\nCombine constants:\n$$\nH(X)=3-\\frac{7}{8}\\log_{2}(7).\n$$\nFor a numerical value, use the change-of-base formula $\\log_{2}(7)=\\frac{\\ln(7)}{\\ln(2)}$:\n$$\n\\log_{2}(7)\\approx 2.807354922,\\quad \\Rightarrow\\quad H(X)\\approx 3-\\frac{7}{8}\\times 2.807354922\\approx 0.543564443.\n$$\nRounding to three significant figures gives $0.544$ bits per symbol.", "answer": "$$\\boxed{0.544}$$", "id": "1604155"}, {"introduction": "Beyond simple calculation, a true understanding of the binary entropy function, $H(p)$, comes from grasping its qualitative behavior. This exercise challenges you to rank the entropies of several binary sources without a calculator, relying instead on your knowledge of the function's key properties. Mastering this will give you an intuitive feel for how uncertainty is symmetric around $p=0.5$ and diminishes as the outcome becomes more predictable. [@problem_id:1604183]", "problem": "An information theorist is analyzing the fundamental limits of data compression for four different, independent binary sources. Each source produces a stream of '0's and '1's. The behavior of each source is characterized by the probability of it emitting a '1'. The Shannon entropy of a source determines the theoretical minimum average number of bits required to represent each symbol from that source.\n\nFor a binary source where the probability of emitting a '1' is $p$ and the probability of emitting a '0' is $1-p$, the entropy $H(p)$ in bits per symbol is given by the binary entropy function:\n$$H(p) = -p \\log_{2}(p) - (1-p) \\log_{2}(1-p)$$\nA lower entropy implies the source is more structured and predictable, allowing for better compression.\n\nThe probabilities of emitting a '1' for the four sources are as follows:\n- Source A: $p_A = 0.02$\n- Source B: $p_B = 0.48$\n- Source C: $p_C = 0.52$\n- Source D: $p_D = 0.99$\n\nLet $S_A, S_B, S_C,$ and $S_D$ be the Shannon entropies for sources A, B, C, and D, respectively. Your task is to rank these entropies. Which of the following options correctly orders the entropies from smallest to largest?\n\nA. $S_D < S_A < S_B < S_C$\n\nB. $S_A < S_D < S_B < S_C$\n\nC. $S_D < S_A < S_B = S_C$\n\nD. $S_A = S_D < S_B = S_C$\n\nE. $S_A < S_D < S_C < S_B$", "solution": "We are given the binary entropy function in bits per symbol for a Bernoulli source:\n$$\nH(p) = -p \\log_{2}(p) - (1-p) \\log_{2}(1-p).\n$$\nStep 1: Symmetry. Observe that\n$$\nH(1-p) = -(1-p)\\log_{2}(1-p) - p \\log_{2}(p) = H(p),\n$$\nso $H$ is symmetric about $p=\\frac{1}{2}$.\n\nStep 2: Monotonicity. Rewrite using natural logarithms:\n$$\nH(p) = -\\frac{p \\ln p + (1-p)\\ln(1-p)}{\\ln 2}.\n$$\nDifferentiate with respect to $p$:\n$$\nH'(p) = -\\frac{\\ln p - \\ln(1-p)}{\\ln 2} = -\\frac{\\ln\\!\\left(\\frac{p}{1-p}\\right)}{\\ln 2}.\n$$\nHence $H'(p)>0$ when $0<p<\\frac{1}{2}$ and $H'(p)<0$ when $\\frac{1}{2}<p<1$. Therefore, $H$ increases on $(0,\\frac{1}{2}]$, decreases on $[\\frac{1}{2},1)$, and attains its maximum at $p=\\frac{1}{2}$.\n\nStep 3: Apply to the given probabilities.\n- For $p_{B}=0.48$ and $p_{C}=0.52$, by symmetry $H(0.48)=H(0.52)$, so $S_{B}=S_{C}$, and both are near the maximum.\n- For $p_{D}=0.99$, by symmetry $H(0.99)=H(0.01)$.\n- Compare $S_{A}=H(0.02)$ and $S_{D}=H(0.99)=H(0.01)$. Since $H$ is strictly increasing on $(0,\\frac{1}{2})$, we have $H(0.01)<H(0.02)$, hence $S_{D}<S_{A}$.\n\nCombining, we obtain the ordering\n$$\nS_{D} < S_{A} < S_{B} = S_{C}.\n$$\nThis corresponds to option C.", "answer": "$$\\boxed{C}$$", "id": "1604183"}, {"introduction": "In many practical situations, we face the inverse problem: we can measure the entropy of a system and need to infer the underlying probabilities of its components. This problem guides you through deriving a powerful numerical tool, the Newton-Raphson method, to solve for the source probability $p$ given a specific entropy value $c$. This hands-on derivation bridges the gap between information theory, calculus, and computational science, showing how to build an algorithm to solve an equation that has no simple algebraic solution. [@problem_id:1604135]", "problem": "In the field of information theory, the binary entropy function is a fundamental measure of the uncertainty associated with a binary random variable. Consider a discrete memoryless source that generates binary symbols, where the probability of generating a '1' is $p$ and a '0' is $1-p$. The entropy of this source, measured in nats, is given by the function:\n$$H(p) = -p \\ln(p) - (1-p) \\ln(1-p)$$\nfor $p \\in (0, 1)$.\n\nIn a practical scenario, we might measure the entropy of a source to be a constant value $c$ (where $0 < c \\le \\ln(2)$) and wish to determine the underlying source probability $p$. Since the equation $H(p) = c$ cannot be solved for $p$ analytically using elementary functions, numerical methods are required.\n\nYour task is to derive the iterative update rule for finding $p$ by applying the Newton-Raphson method. Let $p_n$ be the estimate of $p$ at the $n$-th iteration. Find the expression for the next estimate, $p_{n+1}$, as a function of $p_n$ and the target entropy $c$.", "solution": "We aim to solve $H(p)=c$ for $p \\in (0,1)$, where $H(p)=-p \\ln(p)-(1-p)\\ln(1-p)$ and $0<c \\le \\ln(2)$. Define the root-finding function\n$$\nf(p)=H(p)-c=-p \\ln(p)-(1-p)\\ln(1-p)-c.\n$$\nTo apply the Newton-Raphson method, we need $f'(p)$. Differentiate $H(p)$ with respect to $p$:\n- For the first term, using the product rule and derivative of $\\ln(p)$, we have\n$$\n\\frac{d}{dp}\\bigl[-p \\ln(p)\\bigr]=-(\\ln(p)+1).\n$$\n- For the second term, let $g(p)=(1-p)\\ln(1-p)$. Then\n$$\ng'(p)=-\\ln(1-p)-1,\n$$\nso\n$$\n\\frac{d}{dp}\\bigl[-(1-p)\\ln(1-p)\\bigr]=-\\bigl(-\\ln(1-p)-1\\bigr)=\\ln(1-p)+1.\n$$\nTherefore,\n$$\nH'(p)=-(\\ln(p)+1)+\\bigl(\\ln(1-p)+1\\bigr)=\\ln(1-p)-\\ln(p)=\\ln\\!\\left(\\frac{1-p}{p}\\right).\n$$\nHence,\n$$\nf'(p)=H'(p)=\\ln\\!\\left(\\frac{1-p}{p}\\right).\n$$\nThe Newton-Raphson update for solving $f(p)=0$ is\n$$\np_{n+1}=p_{n}-\\frac{f(p_{n})}{f'(p_{n})}\n= p_{n}-\\frac{-p_{n}\\ln(p_{n})-(1-p_{n})\\ln(1-p_{n})-c}{\\ln\\!\\left(\\frac{1-p_{n}}{p_{n}}\\right)}.\n$$\nEquivalently, this can be written as\n$$\np_{n+1}=p_{n}+\\frac{c+p_{n}\\ln(p_{n})+(1-p_{n})\\ln(1-p_{n})}{\\ln\\!\\left(\\frac{1-p_{n}}{p_{n}}\\right)}.\n$$\nBoth forms express the next iterate $p_{n+1}$ purely in terms of $p_{n}$ and $c$.", "answer": "$$\\boxed{p_{n+1}=p_{n}-\\frac{-p_{n}\\ln(p_{n})-(1-p_{n})\\ln(1-p_{n})-c}{\\ln\\!\\left(\\frac{1-p_{n}}{p_{n}}\\right)}}$$", "id": "1604135"}]}