{"hands_on_practices": [{"introduction": "To begin our hands-on exploration, we'll examine one of the most direct relationships between two variables. This practice explores a scenario where one variable, $Y$, is completely determined by another, $X$. By calculating the mutual information $I(X;Y)$ in this context, we will demonstrate a fundamental principle: the information that $X$ provides about $Y$ is equivalent to the total uncertainty, or entropy, of $Y$ itself [@problem_id:1653506].", "problem": "A digital signal processing unit handles symbols from a source alphabet $\\mathcal{X} = \\{s_1, s_2, s_3, s_4\\}$. The symbols are generated independently with the following probabilities, where $X$ is the random variable for the symbol being processed:\n$$ P(X=s_1) = \\frac{1}{2} $$\n$$ P(X=s_2) = \\frac{1}{4} $$\n$$ P(X=s_3) = \\frac{1}{8} $$\n$$ P(X=s_4) = \\frac{1}{8} $$\nA feature extractor then analyzes each symbol $X$ and outputs a binary classification label $Y$. The classification is deterministic and follows this rule:\n- If the symbol is from the set $\\{s_1, s_2\\}$, the label is $Y=0$.\n- If the symbol is from the set $\\{s_3, s_4\\}$, the label is $Y=1$.\n\nCalculate the mutual information $I(X;Y)$ between the symbol $X$ and its classification label $Y$. Provide your answer as a single closed-form analytic expression in units of bits.", "solution": "We denote the source symbol by the random variable $X$ with alphabet $\\mathcal{X}=\\{s_{1},s_{2},s_{3},s_{4}\\}$ and probabilities\n$$\nP(X=s_{1})=\\frac{1}{2},\\quad P(X=s_{2})=\\frac{1}{4},\\quad P(X=s_{3})=\\frac{1}{8},\\quad P(X=s_{4})=\\frac{1}{8}.\n$$\nThe label $Y$ is a deterministic function of $X$ defined by $Y=0$ if $X\\in\\{s_{1},s_{2}\\}$ and $Y=1$ if $X\\in\\{s_{3},s_{4}\\}$. Therefore,\n$$\nP(Y=0)=P(X=s_{1})+P(X=s_{2})=\\frac{1}{2}+\\frac{1}{4}=\\frac{3}{4},\\qquad\nP(Y=1)=P(X=s_{3})+P(X=s_{4})=\\frac{1}{8}+\\frac{1}{8}=\\frac{1}{4}.\n$$\nSince $Y$ is a deterministic function of $X$, the conditional entropy satisfies\n$$\nH(Y\\mid X)=0.\n$$\nThe mutual information is\n$$\nI(X;Y)=H(Y)-H(Y\\mid X)=H(Y).\n$$\nUsing the definition of binary entropy in bits (logarithm base $2$),\n$$\nH(Y)=-\\sum_{y\\in\\{0,1\\}}P(Y=y)\\log_{2}P(Y=y)\n= -\\left(\\frac{3}{4}\\log_{2}\\frac{3}{4}+\\frac{1}{4}\\log_{2}\\frac{1}{4}\\right).\n$$\nThus,\n$$\nI(X;Y)=-\\left(\\frac{3}{4}\\log_{2}\\frac{3}{4}+\\frac{1}{4}\\log_{2}\\frac{1}{4}\\right)\\ \\text{bits}.\n$$", "answer": "$$\\boxed{-\\left(\\frac{3}{4}\\log_{2}\\frac{3}{4}+\\frac{1}{4}\\log_{2}\\frac{1}{4}\\right)}$$", "id": "1653506"}, {"introduction": "Having explored a deterministic system, we now move to a more general and practical scenario where the relationship between variables is probabilistic. This exercise uses a hypothetical weather forecasting model to guide you through the core calculation of mutual information from a joint probability distribution [@problem_id:1654581]. Mastering this process is essential for quantifying the statistical dependence between any two random variables, a cornerstone of information theory.", "problem": "Consider a simplified model for a weather forecasting system. Let the random variable $X$ represent the daily forecast, which can take one of three states: $x_1$ ('Forecast: Sunny'), $x_2$ ('Forecast: Cloudy'), or $x_3$ ('Forecast: Rainy'). Let the random variable $Y$ represent the actual weather outcome for that day, which can take one of two states: $y_1$ ('Actual: Sunny') or $y_2$ ('Actual: Not Sunny').\n\nAfter observing the system for a long time, the joint probability distribution $p(x, y)$ for the forecast and the actual outcome has been determined as follows:\n- $p(X=x_1, Y=y_1) = 5/16$\n- $p(X=x_1, Y=y_2) = 1/16$\n- $p(X=x_2, Y=y_1) = 1/8$\n- $p(X=x_2, Y=y_2) = 3/16$\n- $p(X=x_3, Y=y_1) = 1/16$\n- $p(X=x_3, Y=y_2) = 1/4$\n\nCalculate the mutual information $I(X; Y)$ between the forecast $X$ and the actual weather $Y$. Express your answer in units of bits, rounded to four significant figures. All logarithms should be interpreted as base 2.", "solution": "We are given the joint distribution of $(X,Y)$ over $\\{x_{1},x_{2},x_{3}\\}\\times\\{y_{1},y_{2}\\}$:\n$$\np(x_{1},y_{1})=\\frac{5}{16},\\quad p(x_{1},y_{2})=\\frac{1}{16},\\quad\np(x_{2},y_{1})=\\frac{1}{8},\\quad p(x_{2},y_{2})=\\frac{3}{16},\\quad\np(x_{3},y_{1})=\\frac{1}{16},\\quad p(x_{3},y_{2})=\\frac{1}{4}.\n$$\nFirst compute the marginals by summing the joint probabilities over the other variable:\n$$\np(x_{1})=\\frac{5}{16}+\\frac{1}{16}=\\frac{3}{8},\\quad\np(x_{2})=\\frac{1}{8}+\\frac{3}{16}=\\frac{5}{16},\\quad\np(x_{3})=\\frac{1}{16}+\\frac{1}{4}=\\frac{5}{16},\n$$\n$$\np(y_{1})=\\frac{5}{16}+\\frac{1}{8}+\\frac{1}{16}=\\frac{8}{16}=\\frac{1}{2},\\quad\np(y_{2})=\\frac{1}{16}+\\frac{3}{16}+\\frac{1}{4}=\\frac{8}{16}=\\frac{1}{2}.\n$$\nThe mutual information is\n$$\nI(X;Y)=\\sum_{x}\\sum_{y}p(x,y)\\,\\log_{2}\\!\\left(\\frac{p(x,y)}{p(x)\\,p(y)}\\right).\n$$\nSince $p(y_{1})=p(y_{2})=\\frac{1}{2}$, the ratio simplifies to $\\frac{p(x,y)}{p(x)\\,p(y)}=\\frac{2\\,p(x,y)}{p(x)}$. Compute each term:\n$$\n\\begin{aligned}\n(x_{1},y_{1}):\\quad \\frac{2\\,p}{p(x_{1})}=\\frac{2\\cdot\\frac{5}{16}}{\\frac{3}{8}}=\\frac{5}{3},\\text{contribution } \\frac{5}{16}\\log_{2}\\!\\left(\\frac{5}{3}\\right),\\\\\n(x_{1},y_{2}):\\quad \\frac{2\\,p}{p(x_{1})}=\\frac{2\\cdot\\frac{1}{16}}{\\frac{3}{8}}=\\frac{1}{3},\\text{contribution } \\frac{1}{16}\\log_{2}\\!\\left(\\frac{1}{3}\\right),\\\\\n(x_{2},y_{1}):\\quad \\frac{2\\,p}{p(x_{2})}=\\frac{2\\cdot\\frac{1}{8}}{\\frac{5}{16}}=\\frac{4}{5},\\text{contribution } \\frac{1}{8}\\log_{2}\\!\\left(\\frac{4}{5}\\right),\\\\\n(x_{2},y_{2}):\\quad \\frac{2\\,p}{p(x_{2})}=\\frac{2\\cdot\\frac{3}{16}}{\\frac{5}{16}}=\\frac{6}{5},\\text{contribution } \\frac{3}{16}\\log_{2}\\!\\left(\\frac{6}{5}\\right),\\\\\n(x_{3},y_{1}):\\quad \\frac{2\\,p}{p(x_{3})}=\\frac{2\\cdot\\frac{1}{16}}{\\frac{5}{16}}=\\frac{2}{5},\\text{contribution } \\frac{1}{16}\\log_{2}\\!\\left(\\frac{2}{5}\\right),\\\\\n(x_{3},y_{2}):\\quad \\frac{2\\,p}{p(x_{3})}=\\frac{2\\cdot\\frac{1}{4}}{\\frac{5}{16}}=\\frac{8}{5},\\text{contribution } \\frac{1}{4}\\log_{2}\\!\\left(\\frac{8}{5}\\right).\n\\end{aligned}\n$$\nThus\n$$\n\\begin{aligned}\nI(X;Y)\n=\\frac{5}{16}\\log_{2}\\!\\left(\\frac{5}{3}\\right)+\\frac{1}{16}\\log_{2}\\!\\left(\\frac{1}{3}\\right)+\\frac{1}{8}\\log_{2}\\!\\left(\\frac{4}{5}\\right)+\\frac{3}{16}\\log_{2}\\!\\left(\\frac{6}{5}\\right)+\\frac{1}{16}\\log_{2}\\!\\left(\\frac{2}{5}\\right)+\\frac{1}{4}\\log_{2}\\!\\left(\\frac{8}{5}\\right)\\\\\n=\\frac{5}{16}(\\log_{2}5-\\log_{2}3)+\\frac{1}{16}(-\\log_{2}3)+\\frac{1}{8}(2-\\log_{2}5)+\\frac{3}{16}(\\log_{2}3+1-\\log_{2}5)+\\frac{1}{16}(1-\\log_{2}5)+\\frac{1}{4}(3-\\log_{2}5)\\\\\n=\\left(\\frac{1}{4}+\\frac{3}{16}+\\frac{1}{16}+\\frac{3}{4}\\right)+\\left(-\\frac{5}{16}-\\frac{1}{16}+\\frac{3}{16}\\right)\\log_{2}3+\\left(\\frac{5}{16}-\\frac{1}{8}-\\frac{3}{16}-\\frac{1}{16}-\\frac{1}{4}\\right)\\log_{2}5\\\\\n=\\frac{5}{4}-\\frac{3}{16}\\log_{2}3-\\frac{5}{16}\\log_{2}5.\n\\end{aligned}\n$$\nEvaluating numerically with base-2 logarithms and rounding to four significant figures gives\n$$\nI(X;Y)\\approx 0.2272\\ \\text{bits}.\n$$", "answer": "$$\\boxed{0.2272}$$", "id": "1654581"}, {"introduction": "A common point of confusion is equating the entropy of a channel's output with the information it transmits. This practice is designed to clarify that distinction by comparing two different communication channels [@problem_id:1653502]. You will discover that a channel can have a high-entropy output while conveying very little information, highlighting that mutual information, $I(X;Y)$, is the true measure of information transfer, not simply the output entropy, $H(Y)$.", "problem": "An information theorist is analyzing two different communication systems designed to transmit a signal from a binary source. The source, represented by the random variable $X$, produces symbols from the alphabet $\\{0, 1\\}$ with equal probability, i.e., $P(X=0) = P(X=1) = 1/2$. All logarithms in this problem are base 2.\n\nThe two communication systems are modeled as information channels:\n\n1.  **Channel 1 (Erasure Channel):** This channel takes an input $x \\in \\{0, 1\\}$ and produces an output symbol $Y$ from the alphabet $\\{0, 1, e\\}$, where '$e$' represents an erasure. The channel operates as follows: with a probability $p = 3/4$, the channel erases the input, and the output is $Y=e$. With probability $1-p$, the transmission is successful, and the output is $Y=x$.\n\n2.  **Channel 2 (Ideal Channel):** This channel is a perfect, noiseless transmitter. It takes an input $x \\in \\{0, 1\\}$ and produces an output $Z=x$ with no possibility of error.\n\nYour task is to compare the entropic properties of these two channels. Calculate the output entropy ratio, defined as $R_H = \\frac{H(Y)}{H(Z)}$, and the mutual information ratio, defined as $R_I = \\frac{I(X;Y)}{I(X;Z)}$.\n\nProvide your final answer as a pair of closed-form analytic expressions for $R_H$ and $R_I$, in that order.", "solution": "The source $X$ is uniform on $\\{0,1\\}$, so its entropy with base-2 logarithms is\n$$\nH(X) = - \\sum_{x \\in \\{0,1\\}} \\frac{1}{2} \\log_{2}\\left(\\frac{1}{2}\\right) = 1.\n$$\nChannel 2 is ideal with $Z=X$. Therefore,\n$$\nH(Z) = H(X) = 1,\\quad H(Z \\mid X) = 0,\\quad I(X;Z) = H(Z) - H(Z \\mid X) = 1.\n$$\n\nFor Channel 1 (erasure with probability $p=\\frac{3}{4}$), the output $Y$ takes values in $\\{0,1,e\\}$. With $X$ uniform,\n$$\nP(Y=e) = p = \\frac{3}{4},\\quad P(Y=0) = P(X=0)(1-p) = \\frac{1}{2}\\cdot\\frac{1}{4} = \\frac{1}{8},\\quad P(Y=1) = \\frac{1}{8}.\n$$\nHence the output entropy is\n$$\nH(Y) = -\\left[\\frac{3}{4}\\log_{2}\\left(\\frac{3}{4}\\right) + \\frac{1}{8}\\log_{2}\\left(\\frac{1}{8}\\right) + \\frac{1}{8}\\log_{2}\\left(\\frac{1}{8}\\right)\\right].\n$$\nUsing $\\log_{2}\\left(\\frac{3}{4}\\right) = \\log_{2}(3) - 2$ and $\\log_{2}\\left(\\frac{1}{8}\\right) = -3$, this simplifies to\n$$\nH(Y) = -\\frac{3}{4}\\left(\\log_{2}(3) - 2\\right) - \\frac{1}{4}(-3) = -\\frac{3}{4}\\log_{2}(3) + \\frac{9}{4}.\n$$\n\nThe mutual information for the erasure channel satisfies\n$$\nI(X;Y) = H(X) - H(X \\mid Y).\n$$\nConditioning on $Y$, if $Y=e$ then $X$ retains its prior entropy $H(X)$, while if $Y \\in \\{0,1\\}$ then $X$ is determined and the conditional entropy is $0$. Thus\n$$\nH(X \\mid Y) = P(Y=e) H(X \\mid Y=e) + P(Y\\in\\{0,1\\}) H(X \\mid Y \\in \\{0,1\\}) = p\\,H(X) + (1-p)\\cdot 0 = p\\,H(X),\n$$\nso\n$$\nI(X;Y) = H(X) - p\\,H(X) = (1-p)H(X) = \\frac{1}{4}.\n$$\n\nTherefore, the requested ratios are\n$$\nR_{H} = \\frac{H(Y)}{H(Z)} = \\frac{-\\frac{3}{4}\\log_{2}(3) + \\frac{9}{4}}{1} = -\\frac{3}{4}\\log_{2}(3) + \\frac{9}{4},\n$$\nand\n$$\nR_{I} = \\frac{I(X;Y)}{I(X;Z)} = \\frac{\\frac{1}{4}}{1} = \\frac{1}{4}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}-\\frac{3}{4}\\log_{2}(3)+\\frac{9}{4}  \\frac{1}{4}\\end{pmatrix}}$$", "id": "1653502"}]}