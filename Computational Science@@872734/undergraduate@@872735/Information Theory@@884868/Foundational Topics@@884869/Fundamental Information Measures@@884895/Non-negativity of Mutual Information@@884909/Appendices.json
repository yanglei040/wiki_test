{"hands_on_practices": [{"introduction": "The most direct way to understand mutual information is to calculate it from a given joint probability distribution. This first exercise [@problem_id:1643384] provides a concrete scenario involving two correlated sensors in an industrial reactor. By working through the calculation step-by-step, you will practice applying the fundamental formula for $I(X;Y)$ and see how statistical dependencies translate into a positive, quantifiable amount of information.", "problem": "Consider a simplified model of a monitoring system for an industrial chemical reactor. Two binary sensors are used: one for temperature and one for pressure. Let $X$ be the random variable representing the state of the temperature sensor, and $Y$ be the random variable representing the state of the pressure sensor. Both sensors output a value of $0$ for a \"normal\" reading and $1$ for an \"alert\" reading. Over a long period of observation, the system's behavior is characterized by the following joint probability mass function, $p(x,y) = P(X=x, Y=y)$:\n\n$p(0,0) = 0.10$\n$p(0,1) = 0.20$\n$p(1,0) = 0.40$\n$p(1,1) = 0.30$\n\nCalculate the mutual information, $I(X;Y)$, between the two sensor readings. All calculations should use the base-2 logarithm. Express your answer in bits, rounded to three significant figures.", "solution": "We are asked to compute the mutual information between two binary random variables $X$ and $Y$ with joint pmf $p(x,y)$. By definition, using base-2 logarithms, the mutual information is\n$$\nI(X;Y)=\\sum_{x\\in\\{0,1\\}}\\sum_{y\\in\\{0,1\\}} p(x,y)\\,\\log_{2}\\!\\left(\\frac{p(x,y)}{p_{X}(x)\\,p_{Y}(y)}\\right).\n$$\nFirst compute the marginal distributions:\n$$\np_{X}(0)=p(0,0)+p(0,1)=0.10+0.20=0.30,\\quad p_{X}(1)=p(1,0)+p(1,1)=0.40+0.30=0.70,\n$$\n$$\np_{Y}(0)=p(0,0)+p(1,0)=0.10+0.40=0.50,\\quad p_{Y}(1)=p(0,1)+p(1,1)=0.20+0.30=0.50.\n$$\nNow evaluate each term in the sum:\n- For $(x,y)=(0,0)$:\n$$\n\\frac{p(0,0)}{p_{X}(0)p_{Y}(0)}=\\frac{0.10}{0.30\\times 0.50}=\\frac{2}{3},\\quad \\text{contribution}=0.10\\,\\log_{2}\\!\\left(\\frac{2}{3}\\right)\\approx -0.0584963.\n$$\n- For $(x,y)=(0,1)$:\n$$\n\\frac{p(0,1)}{p_{X}(0)p_{Y}(1)}=\\frac{0.20}{0.30\\times 0.50}=\\frac{4}{3},\\quad \\text{contribution}=0.20\\,\\log_{2}\\!\\left(\\frac{4}{3}\\right)\\approx 0.0830075.\n$$\n- For $(x,y)=(1,0)$:\n$$\n\\frac{p(1,0)}{p_{X}(1)p_{Y}(0)}=\\frac{0.40}{0.70\\times 0.50}=\\frac{8}{7},\\quad \\text{contribution}=0.40\\,\\log_{2}\\!\\left(\\frac{8}{7}\\right)\\approx 0.0770580.\n$$\n- For $(x,y)=(1,1)$:\n$$\n\\frac{p(1,1)}{p_{X}(1)p_{Y}(1)}=\\frac{0.30}{0.70\\times 0.50}=\\frac{6}{7},\\quad \\text{contribution}=0.30\\,\\log_{2}\\!\\left(\\frac{6}{7}\\right)\\approx -0.0667177.\n$$\nSumming the four contributions gives\n$$\nI(X;Y)\\approx -0.0584963+0.0830075+0.0770580-0.0667177=0.0348516\\ \\text{bits}.\n$$\nRounding to three significant figures yields $0.0349$ bits.", "answer": "$$\\boxed{0.0349}$$", "id": "1643384"}, {"introduction": "While total mutual information $I(X;Y)$ is always non-negative, this property applies to the *average* information gain across all possible outcomes. This practice [@problem_id:1643401] explores the more nuanced concept of pointwise mutual information, $i(x;y)$, which can be negative for specific outcomes. This exercise is crucial for resolving the apparent paradox of how learning an outcome $y$ can make an outcome $x$ seem less likely, yet the overall information shared between the variables remains non-negative.", "problem": "Consider a simple noisy communication channel with input $X$ and output $Y$. The set of possible inputs (the input alphabet) is $\\mathcal{X} = \\{A_1, A_2\\}$, and the set of possible outputs (the output alphabet) is $\\mathcal{Y} = \\{B_1, B_2\\}$. The behavior of the channel is completely characterized by the joint probability distribution $p(x, y)$ of the input-output pairs, which is given as follows:\n- $p(X=A_1, Y=B_1) = \\frac{3}{8}$\n- $p(X=A_1, Y=B_2) = \\frac{1}{8}$\n- $p(X=A_2, Y=B_1) = \\frac{1}{8}$\n- $p(X=A_2, Y=B_2) = \\frac{3}{8}$\n\nThe pointwise mutual information, $i(x;y)$, quantifies the information that observing outcome $y$ provides about outcome $x$. It can be negative for specific outcomes $(x,y)$, indicating that their joint occurrence is less likely than if they were independent. This happens even when the total average mutual information, $I(X;Y)$, remains non-negative.\n\nFor the system defined above, which of the following options correctly identifies an outcome pair $(x,y)$ with negative pointwise mutual information and gives the correct value for the total mutual information $I(X;Y)$? Express the mutual information in bits, as a symbolic expression involving base-2 logarithms.\n\nA. Pair: $(A_1, B_1)$; $I(X;Y) = \\frac{3}{4}\\log_{2}(3) - 1$\n\nB. Pair: $(A_2, B_1)$; $I(X;Y) = 2 - \\frac{3}{4}\\log_{2}(3)$\n\nC. Pair: $(A_1, B_1)$; $I(X;Y) = 0$\n\nD. Pair: $(A_2, B_1)$; $I(X;Y) = \\frac{3}{4}\\log_{2}(3) - 1$\n\nE. Pair: $(A_1, B_2)$; $I(X;Y) = 1$", "solution": "We are given the joint pmf:\n$$\np(A_{1},B_{1})=\\frac{3}{8},\\quad p(A_{1},B_{2})=\\frac{1}{8},\\quad p(A_{2},B_{1})=\\frac{1}{8},\\quad p(A_{2},B_{2})=\\frac{3}{8}.\n$$\nFirst compute marginals:\n$$\np_{X}(A_{1})=\\frac{3}{8}+\\frac{1}{8}=\\frac{1}{2},\\quad p_{X}(A_{2})=\\frac{1}{2},\\quad\np_{Y}(B_{1})=\\frac{3}{8}+\\frac{1}{8}=\\frac{1}{2},\\quad p_{Y}(B_{2})=\\frac{1}{2}.\n$$\nFor any pair, $p_{X}(x)p_{Y}(y)=\\frac{1}{4}$. The pointwise mutual information is\n$$\ni(x;y)=\\log_{2}\\!\\left(\\frac{p(x,y)}{p_{X}(x)p_{Y}(y)}\\right).\n$$\nEvaluate for each pair:\n- For $(A_{1},B_{1})$ and $(A_{2},B_{2})$:\n$$\ni=\\log_{2}\\!\\left(\\frac{3/8}{1/4}\\right)=\\log_{2}\\!\\left(\\frac{3}{2}\\right)>0.\n$$\n- For $(A_{1},B_{2})$ and $(A_{2},B_{1})$:\n$$\ni=\\log_{2}\\!\\left(\\frac{1/8}{1/4}\\right)=\\log_{2}\\!\\left(\\frac{1}{2}\\right)=-1<0.\n$$\nThus any of $(A_{1},B_{2})$ or $(A_{2},B_{1})$ has negative pointwise mutual information.\n\nNow compute the total mutual information:\n$$\nI(X;Y)=\\sum_{x,y}p(x,y)\\log_{2}\\!\\left(\\frac{p(x,y)}{p_{X}(x)p_{Y}(y)}\\right)\n=2\\cdot\\frac{3}{8}\\log_{2}\\!\\left(\\frac{3}{2}\\right)+2\\cdot\\frac{1}{8}\\log_{2}\\!\\left(\\frac{1}{2}\\right).\n$$\nThis simplifies to\n$$\nI(X;Y)=\\frac{3}{4}\\log_{2}\\!\\left(\\frac{3}{2}\\right)-\\frac{1}{4}\n=\\frac{3}{4}\\bigl(\\log_{2}(3)-1\\bigr)-\\frac{1}{4}\n=\\frac{3}{4}\\log_{2}(3)-1.\n$$\nTherefore, a correct option is the one that lists a negative pair such as $(A_{2},B_{1})$ and gives $I(X;Y)=\\frac{3}{4}\\log_{2}(3)-1$, which corresponds to option D.", "answer": "$$\\boxed{D}$$", "id": "1643401"}, {"introduction": "Moving from static probabilities to dynamic systems, this final practice [@problem_id:1643353] challenges you to analyze a communication channel with imperfections. You will first model the system by deriving the channel's characteristics from a description of its behavior, which includes both normal operation and a \"glitch\" mode. Calculating the mutual information in this context demonstrates its power in quantifying the information successfully transmitted despite the presence of noise and component failure.", "problem": "In the design of a robust communication system, understanding the impact of channel imperfections is crucial. Consider a binary information source that produces a random variable $X$, which can take values in $\\{0, 1\\}$. The source is biased, with the probability of producing a '1' being $P(X=1) = p = \\frac{1}{3}$. This signal is transmitted through a faulty channel to produce an output $Y$, which also takes values in $\\{0, 1\\}$.\n\nThe channel's behavior depends on a random operational mode. With a small probability $\\epsilon = \\frac{1}{4}$, the channel is in \"Normal Mode\" and functions perfectly, meaning its output is identical to its input ($Y=X$). However, with probability $1-\\epsilon$, the channel enters a \"Glitch Mode\" where it ignores the input signal and always outputs a fixed value of $0$ (i.e., $Y=0$).\n\nCalculate the mutual information $I(X;Y)$ between the input $X$ and the output $Y$. Provide your answer in units of nats, which implies the use of the natural logarithm ($\\ln$) for all calculations. Your final answer should be an exact analytical expression involving fractions and logarithms.", "solution": "We are given a binary source $X \\in \\{0,1\\}$ with $P(X=1)=p=\\frac{1}{3}$ and $P(X=0)=1-p=\\frac{2}{3}$. The channel operates as follows: with probability $\\epsilon=\\frac{1}{4}$ (Normal Mode), $Y=X$; with probability $1-\\epsilon$ (Glitch Mode), $Y=0$ irrespective of $X$.\n\nFrom this, the conditional distribution $P(Y|X)$ is:\n- If $X=0$, then $Y=0$ deterministically in both modes, so $P(Y=1|X=0)=0$ and $P(Y=0|X=0)=1$.\n- If $X=1$, then in Normal Mode $Y=1$ and in Glitch Mode $Y=0$, hence $P(Y=1|X=1)=\\epsilon$ and $P(Y=0|X=1)=1-\\epsilon$.\n\nThus, the marginal of $Y$ is\n$$\nP(Y=1)=P(X=1)P(Y=1|X=1)=p\\epsilon,\\qquad P(Y=0)=1-p\\epsilon.\n$$\nWith $p=\\frac{1}{3}$ and $\\epsilon=\\frac{1}{4}$, this gives\n$$\nP(Y=1)=\\frac{1}{12},\\qquad P(Y=0)=\\frac{11}{12}.\n$$\n\nUsing natural logarithms, the entropy of $Y$ is\n$$\nH(Y)=-\\sum_{y\\in\\{0,1\\}}P(Y=y)\\ln P(Y=y)\n= -\\left(\\frac{1}{12}\\ln\\frac{1}{12}+\\frac{11}{12}\\ln\\frac{11}{12}\\right).\n$$\n\nThe conditional entropy is\n$$\nH(Y|X)=P(X=0)H(Y|X=0)+P(X=1)H(Y|X=1).\n$$\nHere $H(Y|X=0)=0$ (deterministic), and\n$$\nH(Y|X=1)= -\\left(\\epsilon\\ln\\epsilon+(1-\\epsilon)\\ln(1-\\epsilon)\\right).\n$$\nTherefore,\n$$\nH(Y|X)=p\\left[-\\epsilon\\ln\\epsilon-(1-\\epsilon)\\ln(1-\\epsilon)\\right].\n$$\nSubstituting $p=\\frac{1}{3}$ and $\\epsilon=\\frac{1}{4}$ yields\n$$\nH(Y|X)=\\frac{1}{3}\\left[-\\frac{1}{4}\\ln\\frac{1}{4}-\\frac{3}{4}\\ln\\frac{3}{4}\\right].\n$$\n\nThe mutual information is $I(X;Y)=H(Y)-H(Y|X)$, hence\n$$\nI(X;Y)\n= -\\left(\\frac{1}{12}\\ln\\frac{1}{12}+\\frac{11}{12}\\ln\\frac{11}{12}\\right)\n+\\frac{1}{3}\\left(\\frac{1}{4}\\ln\\frac{1}{4}+\\frac{3}{4}\\ln\\frac{3}{4}\\right).\n$$\nA convenient simplification is obtained by combining terms:\n$$\nI(X;Y)\n= \\frac{1}{12}\\ln 3+\\frac{1}{4}\\ln\\frac{3}{4}-\\frac{11}{12}\\ln\\frac{11}{12}.\n$$\nThis is an exact expression in nats, as required.", "answer": "$$\\boxed{\\frac{1}{12}\\ln 3+\\frac{1}{4}\\ln\\frac{3}{4}-\\frac{11}{12}\\ln\\frac{11}{12}}$$", "id": "1643353"}]}