{"hands_on_practices": [{"introduction": "This first exercise provides a concrete starting point by asking you to calculate the relative entropy between two binomial distributions. This scenario is fundamental, as binomial distributions model the outcomes of a fixed number of independent binary trials, a common occurrence in many scientific and engineering contexts. By working through this problem [@problem_id:1654970], you will see how the Kullback-Leibler (KL) divergence for a sequence of trials can be elegantly expressed in terms of the divergence of a single trial, revealing the additive nature of information divergence for independent events.", "problem": "In the field of statistical modeling, it is often necessary to quantify the difference between two probability distributions. One common measure is the Kullback-Leibler (KL) divergence, also known as relative entropy. For two discrete probability distributions $P(x)$ and $Q(x)$ defined over the same sample space $\\mathcal{X}$, the KL divergence of $Q$ from $P$ is given by:\n$$D_{KL}(P || Q) = \\sum_{x \\in \\mathcal{X}} P(x) \\ln\\left(\\frac{P(x)}{Q(x)}\\right)$$\nwhere $\\ln$ denotes the natural logarithm.\n\nConsider two independent manufacturing processes creating a specific type of semiconductor.\n- Process 1 is modeled by a random variable $K_1$ that follows a binomial distribution with parameters $(n, p_1)$. This represents the number of non-defective semiconductors in a batch of size $n$, where $p_1$ is the probability of any single semiconductor being non-defective.\n- Process 2 is modeled by a random variable $K_2$ that also follows a binomial distribution, but with parameters $(n, p_2)$. Here, $p_2$ is the probability of a single semiconductor being non-defective.\n\nThe probability mass function (PMF) for a binomial distribution $B(n, p)$ is given by $P(K=k) = \\binom{n}{k} p^k (1-p)^{n-k}$ for $k \\in \\{0, 1, \\dots, n\\}$.\n\nLet $P_1$ denote the binomial distribution for Process 1 and $P_2$ denote the distribution for Process 2. Assuming $p_1, p_2 \\in (0,1)$, find a closed-form analytic expression for the Kullback-Leibler divergence $D_{KL}(P_1 || P_2)$ in terms of $n$, $p_1$, and $p_2$.", "solution": "We denote the PMFs of $K_{1} \\sim \\mathrm{Bin}(n,p_{1})$ and $K_{2} \\sim \\mathrm{Bin}(n,p_{2})$ by\n$$\nP_{1}(k)=\\binom{n}{k}p_{1}^{k}(1-p_{1})^{n-k}, \\quad\nP_{2}(k)=\\binom{n}{k}p_{2}^{k}(1-p_{2})^{n-k},\n$$\nfor $k \\in \\{0,1,\\dots,n\\}$. By definition, the Kullback-Leibler divergence of $P_{2}$ from $P_{1}$ is\n$$\nD_{KL}(P_{1}\\,\\|\\,P_{2})=\\sum_{k=0}^{n}P_{1}(k)\\,\\ln\\!\\left(\\frac{P_{1}(k)}{P_{2}(k)}\\right).\n$$\nCompute the likelihood ratio inside the logarithm:\n$$\n\\frac{P_{1}(k)}{P_{2}(k)}=\\frac{\\binom{n}{k}p_{1}^{k}(1-p_{1})^{n-k}}{\\binom{n}{k}p_{2}^{k}(1-p_{2})^{n-k}}\n=\\left(\\frac{p_{1}}{p_{2}}\\right)^{k}\\left(\\frac{1-p_{1}}{1-p_{2}}\\right)^{n-k}.\n$$\nTaking the logarithm yields\n$$\n\\ln\\!\\left(\\frac{P_{1}(k)}{P_{2}(k)}\\right)\n= k\\,\\ln\\!\\left(\\frac{p_{1}}{p_{2}}\\right) + (n-k)\\,\\ln\\!\\left(\\frac{1-p_{1}}{1-p_{2}}\\right).\n$$\nSubstitute back into the definition of $D_{KL}$:\n$$\nD_{KL}(P_{1}\\,\\|\\,P_{2})\n=\\sum_{k=0}^{n}P_{1}(k)\\left[ k\\,\\ln\\!\\left(\\frac{p_{1}}{p_{2}}\\right) + (n-k)\\,\\ln\\!\\left(\\frac{1-p_{1}}{1-p_{2}}\\right)\\right].\n$$\nSince the logarithmic factors do not depend on $k$, we can factor them out and recognize the sums as expectations under $K \\sim \\mathrm{Bin}(n,p_{1})$:\n$$\nD_{KL}(P_{1}\\,\\|\\,P_{2})\n=\\ln\\!\\left(\\frac{p_{1}}{p_{2}}\\right)\\sum_{k=0}^{n}k\\,P_{1}(k)\n+\\ln\\!\\left(\\frac{1-p_{1}}{1-p_{2}}\\right)\\sum_{k=0}^{n}(n-k)\\,P_{1}(k).\n$$\nUsing $\\sum_{k=0}^{n}k\\,P_{1}(k)=\\mathbb{E}_{P_{1}}[K]=n p_{1}$ and $\\sum_{k=0}^{n}(n-k)\\,P_{1}(k)=n-\\mathbb{E}_{P_{1}}[K]=n(1-p_{1})$, we obtain\n$$\nD_{KL}(P_{1}\\,\\|\\,P_{2})\n= n p_{1}\\,\\ln\\!\\left(\\frac{p_{1}}{p_{2}}\\right)\n+ n(1-p_{1})\\,\\ln\\!\\left(\\frac{1-p_{1}}{1-p_{2}}\\right).\n$$\nEquivalently, this is $n$ times the KL divergence between $\\mathrm{Bernoulli}(p_{1})$ and $\\mathrm{Bernoulli}(p_{2})$, and it is finite for $p_{1},p_{2}\\in(0,1)$.", "answer": "$$\\boxed{n\\left[p_{1}\\ln\\!\\left(\\frac{p_{1}}{p_{2}}\\right)+(1-p_{1})\\ln\\!\\left(\\frac{1-p_{1}}{1-p_{2}}\\right)\\right]}$$", "id": "1654970"}, {"introduction": "Building upon the previous example, this practice explores the KL divergence between two Poisson distributions, which are crucial for modeling rates and occurrences of events in fields ranging from network traffic analysis to particle physics. This exercise [@problem_id:1654998] reinforces the computational steps for discrete distributions while applying them to a different, infinitely supported probability model. It illustrates the versatility of relative entropy as a universal measure of statistical distance between probabilistic models.", "problem": "In the field of network engineering, the arrival of data packets at a server is often modeled as a Poisson process. An old model, which we will call $P_{old}$, assumes that the number of packets $k$ arriving in a given time interval follows a Poisson distribution with an average rate of $\\lambda_{old}$. After a system hardware upgrade, a network analyst proposes a new model, $P_{new}$, which is also a Poisson distribution but with a new average rate of $\\lambda_{new}$.\n\nTo quantify the \"information gain\" when updating one's belief from the old model to the new one (assuming the new model represents the true state of the network), an information theorist decides to compute the Kullback-Leibler (KL) divergence, also known as relative entropy. The KL divergence $D_{KL}(P || Q)$ measures the information lost when an approximate distribution $Q$ is used to model a true distribution $P$.\n\nThe probability mass function for a Poisson distribution with mean $\\lambda$ is given by $P(k; \\lambda) = \\frac{\\exp(-\\lambda) \\lambda^k}{k!}$ for non-negative integer values of $k$.\n\nAssuming the new model $P_{new}$ is the true distribution and $P_{old}$ is the model being replaced, derive a closed-form analytic expression for the KL divergence $D_{KL}(P_{new} || P_{old})$ in terms of the rate parameters $\\lambda_{new}$ and $\\lambda_{old}$. Assume both $\\lambda_{new}$ and $\\lambda_{old}$ are positive real numbers.", "solution": "We model the packet count in a fixed interval as a Poisson random variable. The KL divergence from $P_{new}$ to $P_{old}$ is defined for discrete distributions as\n$$\nD_{KL}(P_{new} \\,\\|\\, P_{old})=\\sum_{k=0}^{\\infty} P_{new}(k)\\,\\ln\\!\\left(\\frac{P_{new}(k)}{P_{old}(k)}\\right).\n$$\nFor a Poisson distribution with rate $\\lambda$, the probability mass function is\n$$\nP(k;\\lambda)=\\frac{\\exp(-\\lambda)\\,\\lambda^{k}}{k!},\\quad k\\in\\{0,1,2,\\ldots\\}.\n$$\nSubstituting $P_{new}(k)=\\exp(-\\lambda_{new})\\lambda_{new}^{k}/k!$ and $P_{old}(k)=\\exp(-\\lambda_{old})\\lambda_{old}^{k}/k!$ into the KL divergence gives\n$$\n\\ln\\!\\left(\\frac{P_{new}(k)}{P_{old}(k)}\\right)\n=\\ln\\!\\left(\\frac{\\exp(-\\lambda_{new})\\,\\lambda_{new}^{k}/k!}{\\exp(-\\lambda_{old})\\,\\lambda_{old}^{k}/k!}\\right)\n=(-\\lambda_{new}+k\\ln\\lambda_{new}-\\ln k!) - (-\\lambda_{old}+k\\ln\\lambda_{old}-\\ln k!)\n$$\n$$\n=-\\lambda_{new}+\\lambda_{old}+k\\bigl(\\ln\\lambda_{new}-\\ln\\lambda_{old}\\bigr).\n$$\nTherefore,\n$$\nD_{KL}(P_{new}\\,\\|\\,P_{old})\n=\\sum_{k=0}^{\\infty}P_{new}(k)\\left[-\\lambda_{new}+\\lambda_{old}+k\\bigl(\\ln\\lambda_{new}-\\ln\\lambda_{old}\\bigr)\\right].\n$$\nUsing normalization $\\sum_{k=0}^{\\infty}P_{new}(k)=1$ and the mean of a Poisson distribution $\\sum_{k=0}^{\\infty}k\\,P_{new}(k)=\\mathbb{E}_{P_{new}}[K]=\\lambda_{new}$, we obtain\n$$\nD_{KL}(P_{new}\\,\\|\\,P_{old})\n=-\\lambda_{new}+\\lambda_{old}+\\lambda_{new}\\bigl(\\ln\\lambda_{new}-\\ln\\lambda_{old}\\bigr)\n=\\lambda_{new}\\ln\\!\\left(\\frac{\\lambda_{new}}{\\lambda_{old}}\\right)+\\lambda_{old}-\\lambda_{new}.\n$$\nThis expression is well-defined for positive $\\lambda_{new}$ and $\\lambda_{old}$.", "answer": "$$\\boxed{\\lambda_{new}\\ln\\!\\left(\\frac{\\lambda_{new}}{\\lambda_{old}}\\right)+\\lambda_{old}-\\lambda_{new}}$$", "id": "1654998"}, {"introduction": "This final practice transitions from simply measuring divergence to actively using it as a principle for optimization. You will solve for a probability distribution that satisfies a specific constraint while being as \"close\" as possible to a given prior distribution, a task known as minimizing relative entropy. This powerful technique [@problem_id:1655009], central to fields like machine learning and statistical physics, demonstrates how information theory provides robust tools for inference and model building under incomplete information.", "problem": "A cloud computing platform allocates its resources among four distinct classes of computational tasks, labeled 1, 2, 3, and 4. A historical analysis has established a baseline resource allocation strategy represented by a prior probability distribution $Q = (q_1, q_2, q_3, q_4)$, where $q_i$ is the fraction of resources allocated to task class $i$. The prior distribution is given by $Q = (0.1, 0.2, 0.3, 0.4)$.\n\nEach task class has an associated average power consumption, given by a function $f(i)$. The values are $f(1) = 1.0$, $f(2) = 2.0$, $f(3) = 4.0$, and $f(4) = 5.0$, in normalized power units.\n\nTo meet new energy efficiency targets, the platform needs to adopt a new allocation strategy $P = (p_1, p_2, p_3, p_4)$ that adjusts the average power consumption to a new target value of $C=3.9$ power units. To ensure a smooth transition and minimize disruption, the new strategy $P$ must be as close as possible to the prior strategy $Q$. The \"closeness\" is measured by the Kullback-Leibler (KL) divergence, also known as relative entropy, defined as $D_{KL}(P||Q) = \\sum_{i=1}^4 p_i \\ln\\left(\\frac{p_i}{q_i}\\right)$.\n\nYour task is to find the new allocation strategy $P$ that minimizes $D_{KL}(P||Q)$ subject to the constraints that it is a valid probability distribution and that the expected power consumption is equal to the target value $C$.\n\nCalculate the probability $p_4$ for the fourth task class under this new optimal allocation strategy. Round your final answer to three significant figures.", "solution": "We minimize the relative entropy $D_{KL}(P||Q) = \\sum_{i=1}^{4} p_{i} \\ln\\left(\\frac{p_{i}}{q_{i}}\\right)$ subject to $\\sum_{i=1}^{4} p_{i} = 1$ and $\\sum_{i=1}^{4} p_{i} f(i) = C$ with $C=3.9$. Introduce Lagrange multipliers $\\alpha$ and $\\beta$ and form the Lagrangian\n$$\n\\mathcal{L} = \\sum_{i=1}^{4} p_{i} \\ln\\left(\\frac{p_{i}}{q_{i}}\\right) + \\alpha \\left(\\sum_{i=1}^{4} p_{i} - 1\\right) + \\beta \\left(\\sum_{i=1}^{4} p_{i} f(i) - C\\right).\n$$\nStationarity with respect to $p_{i}$ gives\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial p_{i}} = \\ln\\left(\\frac{p_{i}}{q_{i}}\\right) + 1 + \\alpha + \\beta f(i) = 0,\n$$\nso\n$$\np_{i} = q_{i} \\exp\\left(-1 - \\alpha - \\beta f(i)\\right).\n$$\nLet $\\eta = -\\beta$ and define the normalizer\n$$\nZ(\\eta) = \\sum_{j=1}^{4} q_{j} \\exp\\left(\\eta f(j)\\right).\n$$\nThen\n$$\np_{i} = \\frac{q_{i} \\exp\\left(\\eta f(i)\\right)}{Z(\\eta)}, \\quad Z(\\eta) = \\sum_{j=1}^{4} q_{j} \\exp\\left(\\eta f(j)\\right).\n$$\nThe expectation constraint becomes\n$$\n\\sum_{i=1}^{4} p_{i} f(i) = \\frac{1}{Z(\\eta)} \\sum_{i=1}^{4} q_{i} f(i) \\exp\\left(\\eta f(i)\\right) = C.\n$$\nWith $Q=(0.1,0.2,0.3,0.4)$ and $f(1)=1$, $f(2)=2$, $f(3)=4$, $f(4)=5$, we have\n$$\nZ(\\eta) = 0.1 \\exp(\\eta) + 0.2 \\exp(2\\eta) + 0.3 \\exp(4\\eta) + 0.4 \\exp(5\\eta),\n$$\n$$\n\\sum_{i=1}^{4} q_{i} f(i) \\exp\\left(\\eta f(i)\\right) = 0.1 \\cdot 1 \\cdot \\exp(\\eta) + 0.2 \\cdot 2 \\cdot \\exp(2\\eta) + 0.3 \\cdot 4 \\cdot \\exp(4\\eta) + 0.4 \\cdot 5 \\cdot \\exp(5\\eta).\n$$\nSet this ratio equal to $C=3.9$:\n$$\n\\frac{0.1 \\exp(\\eta) + 0.4 \\exp(2\\eta) + 1.2 \\exp(4\\eta) + 2.0 \\exp(5\\eta)}{0.1 \\exp(\\eta) + 0.2 \\exp(2\\eta) + 0.3 \\exp(4\\eta) + 0.4 \\exp(5\\eta)} = 3.9.\n$$\nLet $t = \\exp(\\eta) > 0$. Then the equation becomes\n$$\n\\frac{0.1 t + 0.4 t^{2} + 1.2 t^{4} + 2.0 t^{5}}{0.1 t + 0.2 t^{2} + 0.3 t^{4} + 0.4 t^{5}} = 3.9.\n$$\nCross-multiplying and simplifying yields\n$$\n-0.29 t - 0.38 t^{2} + 0.03 t^{4} + 0.44 t^{5} = 0,\n$$\nequivalently\n$$\n0.44 t^{4} + 0.03 t^{3} - 0.38 t - 0.29 = 0.\n$$\nSolving numerically for $t>1$ (since the target $C=3.9$ exceeds the prior mean $3.7$) gives $t \\approx 1.11132$.\n\nWith this $t$, compute the normalizer and $p_{4}$. First compute powers:\n$$\nt \\approx 1.11132,\\quad t^{2} \\approx 1.2350321424,\\quad t^{4} \\approx 1.5253043928,\\quad t^{5} \\approx 1.6951012778.\n$$\nThen\n$$\nZ = 0.1 t + 0.2 t^{2} + 0.3 t^{4} + 0.4 t^{5} \\approx 0.111132 + 0.24700642848 + 0.45759131784 + 0.67804051112 \\approx 1.49377025741.\n$$\nTherefore\n$$\np_{4} = \\frac{0.4 t^{5}}{Z} \\approx \\frac{0.67804051112}{1.49377025741} \\approx 0.453912,\n$$\nwhich, rounded to three significant figures, is $0.454$.", "answer": "$$\\boxed{0.454}$$", "id": "1655009"}]}