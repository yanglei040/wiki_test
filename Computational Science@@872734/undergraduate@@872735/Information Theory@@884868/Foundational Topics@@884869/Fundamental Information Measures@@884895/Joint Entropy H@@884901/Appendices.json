{"hands_on_practices": [{"introduction": "To begin our hands-on exploration of joint entropy, let's start with a foundational calculation. This first exercise [@problem_id:1634897] provides a simple system with a constrained set of outcomes, challenging you to apply the Shannon entropy formula directly. By working through this problem, you'll gain a concrete understanding of how joint entropy quantifies the total uncertainty in a system described by multiple random variables.", "problem": "Consider a simple logic circuit that has two binary inputs, represented by the random variables $X_1$ and $X_2$. Each input can be in either state 0 or state 1. A design flaw in the circuit makes it impossible for the system to be in the state where $X_1=1$ and $X_2=0$. All other three possible joint states, $(X_1=0, X_2=0)$, $(X_1=0, X_2=1)$, and $(X_1=1, X_2=1)$, are observed to be equally likely.\n\nCalculate the joint entropy $H(X_1, X_2)$ of this system. Your answer should be a closed-form analytic expression, derived using the base-2 logarithm.", "solution": "We are given a joint distribution over the four possible states of the binary pair $(X_{1},X_{2})$, with the state $(1,0)$ impossible and the other three states equally likely. Therefore, the joint probabilities are:\n$$(X_{1},X_{2}) \\in \\{(0,0),(0,1),(1,1)\\} \\text{ with probability } \\frac{1}{3} \\text{ each, and } (1,0) \\text{ with probability } 0.$$\nThe joint entropy is defined by the Shannon formula using base-2 logarithms:\n$$H(X_{1},X_{2})=-\\sum_{x_{1},x_{2}} p(x_{1},x_{2}) \\log_{2}\\big(p(x_{1},x_{2})\\big).$$\nBy the continuity convention, terms with $p=0$ contribute $0$ because $\\lim_{p \\to 0^{+}} p \\log_{2}(p)=0$. Hence only the three nonzero-probability states contribute:\n$$H(X_{1},X_{2})=-3 \\cdot \\frac{1}{3} \\log_{2}\\!\\left(\\frac{1}{3}\\right)=-\\log_{2}\\!\\left(\\frac{1}{3}\\right)=\\log_{2}(3).$$\nThus the joint entropy is $\\log_{2}(3)$ bits.", "answer": "$$\\boxed{\\log_{2}(3)}$$", "id": "1634897"}, {"introduction": "Now let's explore a special but important case: what happens to joint entropy when one variable is completely determined by another? This problem [@problem_id:1634881] illustrates the concept of functional dependence, where knowing the value of variable $X$ automatically tells you the value of variable $Y$. By solving this, you will discover a fundamental property of joint entropy and see why no new uncertainty is added when information is redundant.", "problem": "Consider a system described by two discrete random variables, $X$ and $Y$. The random variable $X$ is generated by a process that draws an integer uniformly at random from the set $\\{1, 2, 3, 4, 5, 6, 7, 8\\}$. The random variable $Y$ is functionally dependent on $X$ and is defined by the relation $Y = X \\pmod 3$. The result of the modulo operation for $Y$ is taken to be in the set $\\{0, 1, 2\\}$.\n\nCalculate the joint entropy $H(X, Y)$ of these two random variables. The answer should be given in units of bits.", "solution": "Let $X$ be uniformly distributed on the set $\\{1,2,3,4,5,6,7,8\\}$, so $P_{X}(x)=\\frac{1}{8}$ for each $x$ in this set. Define $Y=f(X)$ with $f(x)=x \\bmod 3$, taking values in $\\{0,1,2\\}$. Since $Y$ is a deterministic function of $X$, the conditional entropy satisfies\n$$\nH(Y \\mid X)=0.\n$$\nBy the chain rule for entropy,\n$$\nH(X,Y)=H(X)+H(Y \\mid X)=H(X).\n$$\nFor a uniform distribution on $8$ outcomes,\n$$\nH(X)=-\\sum_{x=1}^{8} P_{X}(x)\\log_{2} P_{X}(x)=-8 \\cdot \\frac{1}{8}\\log_{2}\\left(\\frac{1}{8}\\right)=\\log_{2}(8).\n$$\nSince $\\log_{2}(8)=3$, it follows that\n$$\nH(X,Y)=3.\n$$\nThis value is in bits because the logarithm is taken base $2$.", "answer": "$$\\boxed{3}$$", "id": "1634881"}, {"introduction": "Our final practice problem tackles the more general and common scenario where two variables are related probabilistically, but not deterministically. This exercise [@problem_id:1634887] requires you to use the full power of the chain rule for entropy, $H(X,Y) = H(X) + H(Y|X)$, to find the total uncertainty of a system. You will calculate the conditional entropy $H(Y|X)$, which represents the average uncertainty about one variable that remains even after the other is known, a key concept in information theory.", "problem": "A simple autonomous robot operates on a square grid of $3 \\times 3$ equally spaced docking stations. Due to a fault in its navigation system, the robot's true position, represented by the random variable $X$, is uniformly distributed among the 9 stations on the grid.\n\nThe robot is equipped with a short-range communication beacon. From any given station, the robot can establish a direct communication link with all adjacent stations, including those that are horizontally, vertically, and diagonally adjacent.\n\nFor a diagnostic check, a remote system randomly selects one of the stations the robot can communicate with from its current position. This selected station is represented by the random variable $Y$. The selection is uniform over the set of all stations reachable from the robot's current position $X$.\n\nCalculate the joint entropy $H(X,Y)$ of the robot's position and the selected communication station's position. Express your answer as a single, simplified analytical expression involving base-2 logarithms.", "solution": "Let $X$ be the robotâ€™s position, uniformly distributed over the $3 \\times 3$ grid, so $P(X=x)=\\frac{1}{9}$ for all $x$ and hence $H(X)=\\log_{2}(9)$. From a given station $x$, the set of directly communicable stations consists of all adjacent (including diagonal) stations, excluding $x$ itself. Denote by $d(x)$ the number of such adjacent stations. Then $Y \\mid X=x$ is uniform over $d(x)$ choices, so\n$$\nH(Y \\mid X=x)=\\log_{2}\\big(d(x)\\big).\n$$\nBy the chain rule of entropy,\n$$\nH(X,Y)=H(X)+H(Y \\mid X)=\\log_{2}(9)+\\sum_{x} P(X=x)\\, \\log_{2}\\big(d(x)\\big).\n$$\nOn a $3 \\times 3$ grid with 8-neighborhood adjacency, the neighbor counts are:\n- Corners: $d(x)=3$ for $4$ stations,\n- Edges (non-corner): $d(x)=5$ for $4$ stations,\n- Center: $d(x)=8$ for $1$ station.\nTherefore,\n$$\nH(Y \\mid X)=\\frac{1}{9}\\left(4 \\log_{2}(3)+4 \\log_{2}(5)+\\log_{2}(8)\\right).\n$$\nThus,\n$$\nH(X,Y)=\\log_{2}(9)+\\frac{1}{9}\\left(4 \\log_{2}(3)+4 \\log_{2}(5)+\\log_{2}(8)\\right).\n$$\nEquivalently, since $\\log_{2}(8)=3$, one may write\n$$\nH(X,Y)=\\log_{2}(9)+\\frac{4}{9}\\log_{2}(3)+\\frac{4}{9}\\log_{2}(5)+\\frac{1}{3}.\n$$\nEither form is a single, simplified analytical expression involving base-2 logarithms.", "answer": "$$\\boxed{\\log_{2}(9)+\\frac{1}{9}\\left(4 \\log_{2}(3)+4 \\log_{2}(5)+\\log_{2}(8)\\right)}$$", "id": "1634887"}]}