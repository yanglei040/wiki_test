{"hands_on_practices": [{"introduction": "To begin, we'll ground our understanding of mutual information with a direct calculation. This first exercise [@problem_id:1649998] models a simple signal processing scenario where a sensor's output is transformed by a deterministic function, $Y=X^2$. By calculating the mutual information $I(X;Y)$ from first principles, you will gain a concrete feel for how it quantifies the reduction in uncertainty about the input signal after observing the processed output.", "problem": "A simple digital sensor produces a raw output signal, represented by the discrete random variable $X$. The signal $X$ can take one of three values, $\\{-1, 0, 1\\}$, with equal probability. This signal is then fed into a signal processing unit that computes a new variable $Y$ according to the function $Y = X^2$. This transformation effectively makes the system insensitive to the sign of the raw signal.\n\nYour task is to calculate the mutual information $I(X;Y)$ between the raw signal $X$ and the processed signal $Y$. This quantity measures the information (in an information-theoretic sense) that the processed signal $Y$ provides about the original signal $X$.\n\nExpress your final answer in bits as a closed-form analytic expression. Your calculations and final answer should use base 2 logarithms (e.g., $\\log_{2}(...)$).", "solution": "The mutual information $I(X;Y)$ can be calculated using the formula $I(X;Y) = H(X) - H(X|Y)$, where $H(X)$ is the entropy of the random variable $X$ and $H(X|Y)$ is the conditional entropy of $X$ given $Y$. We will calculate each term separately. All logarithms are base 2, as required for an answer in bits.\n\nFirst, let's calculate the entropy of the source signal $X$, denoted by $H(X)$. The random variable $X$ is uniformly distributed over the set $\\{-1, 0, 1\\}$. Therefore, the probabilities of its outcomes are:\n$P(X=-1) = \\frac{1}{3}$\n$P(X=0) = \\frac{1}{3}$\n$P(X=1) = \\frac{1}{3}$\n\nThe entropy $H(X)$ is given by the formula $H(X) = -\\sum_{x} P(x) \\log_{2}(P(x))$.\n$$H(X) = - \\left( P(X=-1)\\log_{2}(P(X=-1)) + P(X=0)\\log_{2}(P(X=0)) + P(X=1)\\log_{2}(P(X=1)) \\right)$$\n$$H(X) = - \\left( \\frac{1}{3}\\log_{2}\\left(\\frac{1}{3}\\right) + \\frac{1}{3}\\log_{2}\\left(\\frac{1}{3}\\right) + \\frac{1}{3}\\log_{2}\\left(\\frac{1}{3}\\right) \\right)$$\n$$H(X) = -3 \\times \\frac{1}{3} \\log_{2}\\left(\\frac{1}{3}\\right) = -\\log_{2}\\left(\\frac{1}{3}\\right) = \\log_{2}(3)$$\n\nNext, we calculate the conditional entropy $H(X|Y)$. This is defined as $H(X|Y) = \\sum_{y} P(y)H(X|Y=y)$. To do this, we first need to find the probability distribution of $Y$. The possible values for $Y=X^2$ are:\nIf $X=-1$, then $Y = (-1)^2 = 1$.\nIf $X=0$, then $Y = (0)^2 = 0$.\nIf $X=1$, then $Y = (1)^2 = 1$.\n\nSo, the set of possible values for $Y$ is $\\{0, 1\\}$. Now we find their probabilities:\nThe event $Y=0$ occurs only if $X=0$. So, $P(Y=0) = P(X=0) = \\frac{1}{3}$.\nThe event $Y=1$ occurs if $X=-1$ or $X=1$. So, $P(Y=1) = P(X=-1) + P(X=1) = \\frac{1}{3} + \\frac{1}{3} = \\frac{2}{3}$.\n\nNow we can calculate the conditional entropies for each value of $y$:\nCase 1: $Y=0$.\nGiven $Y=0$, we know for certain that $X$ must be 0. There is no uncertainty left. The conditional probability distribution $P(X|Y=0)$ is $P(X=0|Y=0)=1$ and $P(X=x|Y=0)=0$ for $x \\neq 0$.\nThe conditional entropy is $H(X|Y=0) = - \\sum_x P(x|Y=0) \\log_{2}(P(x|Y=0)) = - (1 \\cdot \\log_{2}(1)) = 0$.\n\nCase 2: $Y=1$.\nGiven $Y=1$, we know that $X$ could be either $-1$ or $1$. We need the conditional probabilities:\n$P(X=-1|Y=1) = \\frac{P(X=-1, Y=1)}{P(Y=1)} = \\frac{P(X=-1)}{P(Y=1)} = \\frac{1/3}{2/3} = \\frac{1}{2}$.\n$P(X=1|Y=1) = \\frac{P(X=1, Y=1)}{P(Y=1)} = \\frac{P(X=1)}{P(Y=1)} = \\frac{1/3}{2/3} = \\frac{1}{2}$.\nThe conditional entropy is:\n$$H(X|Y=1) = - \\left( P(X=-1|Y=1)\\log_{2}(P(X=-1|Y=1)) + P(X=1|Y=1)\\log_{2}(P(X=1|Y=1)) \\right)$$\n$$H(X|Y=1) = - \\left( \\frac{1}{2}\\log_{2}\\left(\\frac{1}{2}\\right) + \\frac{1}{2}\\log_{2}\\left(\\frac{1}{2}\\right) \\right) = - \\log_{2}\\left(\\frac{1}{2}\\right) = \\log_{2}(2) = 1$$\n\nNow we can compute the total conditional entropy $H(X|Y)$:\n$$H(X|Y) = P(Y=0)H(X|Y=0) + P(Y=1)H(X|Y=1)$$\n$$H(X|Y) = \\left(\\frac{1}{3}\\right) \\cdot (0) + \\left(\\frac{2}{3}\\right) \\cdot (1) = \\frac{2}{3}$$\n\nFinally, we calculate the mutual information $I(X;Y)$:\n$$I(X;Y) = H(X) - H(X|Y) = \\log_{2}(3) - \\frac{2}{3}$$\nThis expression represents the amount of information, in bits, that observing $Y$ provides about $X$.", "answer": "$$\\boxed{\\log_{2}(3) - \\frac{2}{3}}$$", "id": "1649998"}, {"introduction": "Beyond direct calculation, much of the power of information theory lies in the relationships between different entropic quantities. This practice problem [@problem_id:1650000] illustrates the utility of the chain rule for entropy, which states $H(X,Y|Z)=H(X|Z)+H(Y|X,Z)=H(Y|Z)+H(X|Y,Z)$. Set within a hypothetical weather forecasting model, you will see how this fundamental identity allows you to deduce the uncertainty of one variable given others, purely by manipulating known entropy values.", "problem": "An information theorist is analyzing a simplified model for weather forecasting. The model involves three discrete random variables:\n- $Z$: Represents the atmospheric pressure category, which can be 'High' or 'Low'.\n- $Y$: Represents the temperature category, which can be 'Warm' or 'Cool'.\n- $X$: Represents the model's forecast for the next day's weather, which can be 'Sunny' or 'Rainy'.\n\nThe theorist quantifies the relationships between these variables using Shannon entropy, which measures the average uncertainty of a random variable. All entropy values are calculated in units of bits, corresponding to the use of base-2 logarithms in their definition.\n\nAfter analyzing a large dataset, the theorist has determined the following conditional entropies:\n1. The uncertainty in the forecast $X$ given only the pressure $Z$ is $H(X|Z) = 0.875$ bits.\n2. The uncertainty in the temperature $Y$ given only the pressure $Z$ is $H(Y|Z) = 0.941$ bits.\n3. The uncertainty in the temperature $Y$ given both the forecast $X$ and the pressure $Z$ is $H(Y|X,Z) = 0.726$ bits.\n\nUsing this information, calculate the uncertainty in the forecast $X$ given both the temperature $Y$ and the pressure $Z$, which is denoted by $H(X|Y,Z)$. Express your answer in bits, rounded to three significant figures.", "solution": "We use the conditional chain rule for entropy. For any random variables $X$, $Y$, and $Z$, the chain rule gives\n$$\nH(X,Y|Z)=H(X|Z)+H(Y|X,Z)=H(Y|Z)+H(X|Y,Z).\n$$\nEquating the two expansions and solving for $H(X|Y,Z)$ yields\n$$\nH(X|Y,Z)=H(X|Z)+H(Y|X,Z)-H(Y|Z).\n$$\nSubstituting the given values,\n$$\nH(X|Y,Z)=0.875+0.726-0.941=0.660 \\text{ bits}.\n$$\nRounded to three significant figures, this is $0.660$ bits.", "answer": "$$\\boxed{0.660}$$", "id": "1650000"}, {"introduction": "Our final exercise [@problem_id:1650010] moves from discrete variables to the continuous domain, tackling a common scenario in communications and signal processing. By analyzing a noisy two-stage pipeline where signals are corrupted by additive Gaussian noise, you will apply the concepts of mutual information to continuous random variables. This problem highlights a practical consequence of the Data Processing Inequality, showing how information about a source signal degrades through successive processing stages and allowing you to quantify this loss.", "problem": "Consider a simplified model for a two-stage signal processing pipeline. An input signal, represented by a random variable $X$, is transmitted through the first stage. During this process, it is corrupted by additive noise, modeled by a random variable $N_1$. The output of the first stage is $Y = X + N_1$. This intermediate signal $Y$ is then fed into a second stage, where it is further corrupted by another independent additive noise source, $N_2$, resulting in the final output signal $Z = Y + N_2$.\n\nThe random variables $X$, $N_1$, and $N_2$ are all mutually independent, zero-mean Gaussian random variables with variances $\\sigma_X^2$, $\\sigma_{N_1}^2$, and $\\sigma_{N_2}^2$, respectively. To ensure a meaningful analysis, assume the input signal is not deterministic ($\\sigma_X^2 > 0$) and the first stage is noisy ($\\sigma_{N_1}^2 > 0$).\n\nThe quality of the signal at the end of the first stage is characterized by the Signal-to-Noise Ratio (SNR), defined as $S_{in} = \\sigma_X^2 / \\sigma_{N_1}^2$. For this particular system, it is measured that $S_{in} = 15$.\n\nAccording to the Data Processing Inequality, information about the original signal $X$ can only be lost as the signal propagates through the cascade, meaning the mutual information $I(X;Z)$ is less than or equal to $I(X;Y)$. Your task is to find the specific condition under which the information retained at the final output is exactly 75% of the information available at the intermediate stage.\n\nCalculate the required ratio of the noise variances, $R = \\sigma_{N_2}^2 / \\sigma_{N_1}^2$, for which the relation $I(X; Z) = 0.75 \\cdot I(X; Y)$ holds.", "solution": "Because $X$, $N_{1}$, and $N_{2}$ are mutually independent zero-mean Gaussians, $Y=X+N_{1}$ and $Z=X+N_{1}+N_{2}$ are Gaussian. The mutual information for an additive Gaussian channel with Gaussian input is obtained from differential entropies:\n$$\nI(X;Y)=h(Y)-h(N_{1}).\n$$\nSince $Y\\sim\\mathcal{N}\\!\\left(0,\\sigma_{X}^{2}+\\sigma_{N_{1}}^{2}\\right)$ and $N_{1}\\sim\\mathcal{N}\\!\\left(0,\\sigma_{N_{1}}^{2}\\right)$, using $h(\\mathcal{N}(0,\\sigma^{2}))=\\tfrac{1}{2}\\ln\\!\\left(2\\pi e\\,\\sigma^{2}\\right)$ gives\n$$\nI(X;Y)=\\frac{1}{2}\\ln\\!\\left(\\frac{\\sigma_{X}^{2}+\\sigma_{N_{1}}^{2}}{\\sigma_{N_{1}}^{2}}\\right)=\\frac{1}{2}\\ln\\!\\left(1+\\frac{\\sigma_{X}^{2}}{\\sigma_{N_{1}}^{2}}\\right).\n$$\nDefine $S_{in}=\\sigma_{X}^{2}/\\sigma_{N_{1}}^{2}$. Then\n$$\nI(X;Y)=\\frac{1}{2}\\ln(1+S_{in}).\n$$\n\nFor the cascade output $Z=X+N_{1}+N_{2}$, the effective noise is $N_{1}+N_{2}$ with variance $\\sigma_{N_{1}}^{2}+\\sigma_{N_{2}}^{2}$, independent of $X$. Thus\n$$\nI(X;Z)=\\frac{1}{2}\\ln\\!\\left(1+\\frac{\\sigma_{X}^{2}}{\\sigma_{N_{1}}^{2}+\\sigma_{N_{2}}^{2}}\\right).\n$$\nIntroduce $R=\\sigma_{N_{2}}^{2}/\\sigma_{N_{1}}^{2}$ so that $\\sigma_{N_{1}}^{2}+\\sigma_{N_{2}}^{2}=\\sigma_{N_{1}}^{2}(1+R)$ and\n$$\nI(X;Z)=\\frac{1}{2}\\ln\\!\\left(1+\\frac{S_{in}}{1+R}\\right).\n$$\n\nImpose the condition $I(X;Z)=0.75\\,I(X;Y)$:\n$$\n\\frac{1}{2}\\ln\\!\\left(1+\\frac{S_{in}}{1+R}\\right)=0.75\\cdot\\frac{1}{2}\\ln(1+S_{in}).\n$$\nMultiply by $2$ and exponentiate:\n$$\n\\ln\\!\\left(1+\\frac{S_{in}}{1+R}\\right)=0.75\\,\\ln(1+S_{in})\n\\quad\\Longrightarrow\\quad\n1+\\frac{S_{in}}{1+R}=(1+S_{in})^{0.75}.\n$$\nSolve for $R$:\n$$\n\\frac{S_{in}}{1+R}=(1+S_{in})^{0.75}-1\n\\;\\Longrightarrow\\;\n1+R=\\frac{S_{in}}{(1+S_{in})^{0.75}-1}\n\\;\\Longrightarrow\\;\nR=\\frac{S_{in}}{(1+S_{in})^{0.75}-1}-1.\n$$\n\nWith the given $S_{in}=15$, compute\n$$\nR=\\frac{15}{16^{0.75}-1}-1.\n$$\nNote that $16^{0.75}=16^{3/4}=(2^{4})^{3/4}=2^{3}=8$, so\n$$\nR=\\frac{15}{8-1}-1=\\frac{15}{7}-1=\\frac{8}{7}.\n$$", "answer": "$$\\boxed{\\frac{8}{7}}$$", "id": "1650010"}]}