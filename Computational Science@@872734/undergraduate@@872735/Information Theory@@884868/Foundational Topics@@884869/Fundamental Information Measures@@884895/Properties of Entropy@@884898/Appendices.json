{"hands_on_practices": [{"introduction": "A common pitfall when first learning about conditional entropy is to assume it is symmetric, like mutual information. This practice directly confronts that misconception by asking you to calculate both $H(Y|X)$ and $H(X|Y)$ for a specific joint distribution [@problem_id:1649381]. By working through this counterexample, you will see firsthand that knowing $X$ can reduce the uncertainty of $Y$ by a different amount than knowing $Y$ reduces the uncertainty of $X$.", "problem": "In a simplified model of a cellular signaling pathway, a signaling protein $X$ can exist in one of two states: 'active' ($A$) or 'inactive' ($B$). The state of this protein influences a downstream cellular response $Y$, which can result in one of three distinct outcomes: 'growth' (1), 'stasis' (2), or 'apoptosis' (3).\n\nThe joint probability distribution, $p(x, y)$, for the state of protein $X$ and the cellular response $Y$ has been determined experimentally and is given by the following table:\n\n|            | $Y=1$ (growth) | $Y=2$ (stasis) | $Y=3$ (apoptosis) |\n| :--------: | :------------: | :------------: | :---------------: |\n| $X=A$ (active) |     $1/2$      |       0        |         0         |\n| $X=B$ (inactive) |       0        |     $1/4$      |       $1/4$       |\n\nThe uncertainty of a random variable $Z$ is quantified by the Shannon entropy, $H(Z) = -\\sum_{z} p(z) \\log_2 p(z)$, where the logarithm is base 2 and the resulting unit is bits. The conditional entropy of a variable $U$ given another variable $V$, denoted $H(U|V)$, measures the remaining uncertainty in $U$ when $V$ is known.\n\nCalculate the conditional entropy of the cellular response given the protein state, $H(Y|X)$, and the conditional entropy of the protein state given the cellular response, $H(X|Y)$. Express your answer as the ordered pair $(H(Y|X), H(X|Y))$, with both values in units of bits.", "solution": "We use the definitions of Shannon entropy and conditional entropy. For any random variables $U$ and $V$, the conditional entropy can be written as\n$$\nH(U|V)=\\sum_{v} p(v)\\,H(U|V=v), \\quad \\text{where} \\quad H(U|V=v)=-\\sum_{u} p(u|v)\\log_{2} p(u|v).\n$$\nFrom the joint distribution, the marginals of $X$ are\n$$\np(X=A)=\\frac{1}{2}+0+0=\\frac{1}{2}, \\quad p(X=B)=0+\\frac{1}{4}+\\frac{1}{4}=\\frac{1}{2}.\n$$\nThe conditional distributions $p(Y|X)$ are:\n- For $X=A$: $p(Y=1|X=A)=\\frac{(1/2)}{(1/2)}=1$, $p(Y=2|X=A)=0$, $p(Y=3|X=A)=0$, hence\n$$\nH(Y|X=A)=-\\left[1\\cdot \\log_{2} 1+0+0\\right]=0.\n$$\n- For $X=B$: $p(Y=1|X=B)=0$, $p(Y=2|X=B)=\\frac{(1/4)}{(1/2)}=\\frac{1}{2}$, $p(Y=3|X=B)=\\frac{(1/4)}{(1/2)}=\\frac{1}{2}$, hence\n$$\nH(Y|X=B)=-\\left[\\frac{1}{2}\\log_{2}\\frac{1}{2}+\\frac{1}{2}\\log_{2}\\frac{1}{2}\\right]\n=-\\left[\\frac{1}{2}(-1)+\\frac{1}{2}(-1)\\right]=1.\n$$\nTherefore,\n$$\nH(Y|X)=p(X=A)H(Y|X=A)+p(X=B)H(Y|X=B)=\\frac{1}{2}\\cdot 0+\\frac{1}{2}\\cdot 1=\\frac{1}{2}.\n$$\n\nNext, compute $H(X|Y)$. The marginals of $Y$ are\n$$\np(Y=1)=\\frac{1}{2}, \\quad p(Y=2)=\\frac{1}{4}, \\quad p(Y=3)=\\frac{1}{4}.\n$$\nThe conditional distributions $p(X|Y)$ are:\n- For $Y=1$: only $(X=A,Y=1)$ has nonzero probability, so $p(X=A|Y=1)=1$, $p(X=B|Y=1)=0$, hence $H(X|Y=1)=0$.\n- For $Y=2$: only $(X=B,Y=2)$ has nonzero probability, so $p(X=B|Y=2)=1$, hence $H(X|Y=2)=0$.\n- For $Y=3$: only $(X=B,Y=3)$ has nonzero probability, so $p(X=B|Y=3)=1$, hence $H(X|Y=3)=0$.\n\nThus,\n$$\nH(X|Y)=\\sum_{y} p(y)H(X|Y=y)=\\frac{1}{2}\\cdot 0+\\frac{1}{4}\\cdot 0+\\frac{1}{4}\\cdot 0=0.\n$$\n\nTherefore, the ordered pair is $\\left(H(Y|X),\\,H(X|Y)\\right)=\\left(\\frac{1}{2},\\,0\\right)$ in bits.", "answer": "$$\\boxed{(\\frac{1}{2}, 0)}$$", "id": "1649381"}, {"introduction": "What does it mean for conditional entropy to be zero? This exercise challenges you to translate abstract information-theoretic conditions, namely $H(A)=1$ bit and $H(A|B)=0$, into a concrete joint probability distribution [@problem_id:1649405]. This practice is fundamental to understanding that zero conditional entropy implies a deterministic relationship, where knowing the value of one variable completely removes all uncertainty about the other.", "problem": "Consider a simple stochastic system described by two binary random variables, $A$ and $B$, which can each take values in the set $\\{0, 1\\}$. The behavior of this system is characterized by the joint probability mass function $P(A=i, B=j)$, where $i, j \\in \\{0, 1\\}$.\n\nThe Shannon entropy of a discrete random variable $X$ with probability mass function $P(x)$ is defined as $H(X) = -\\sum_{x} P(x) \\log_2 P(x)$, measured in bits. The conditional entropy of $A$ given $B$ is defined as $H(A|B) = \\sum_{j} P(B=j) H(A|B=j) = -\\sum_{j} P(B=j) \\sum_{i} P(A=i|B=j) \\log_2 P(A=i|B=j)$.\n\nIt is observed that the entropy of variable $A$ is exactly one bit, $H(A) = 1$, and the conditional entropy of $A$ given $B$ is zero, $H(A|B) = 0$.\n\nDetermine a possible joint probability mass function for these two variables that is consistent with the given observations. Present your answer as the four numerical values for $P(A=0, B=0)$, $P(A=0, B=1)$, $P(A=1, B=0)$, and $P(A=1, B=1)$, in this specific order. Express your answers as exact fractions.", "solution": "Let the joint probability mass function be denoted by $p_{ij} = P(A=i, B=j)$ for $i,j \\in \\{0, 1\\}$. The joint probability distribution can be represented by a 2x2 table:\n\n|       | B=0       | B=1       |\n|-------|-----------|-----------|\n| A=0   | $p_{00}$  | $p_{01}$  |\n| A=1   | $p_{10}$  | $p_{11}$  |\n\nThe sum of all joint probabilities must be 1: $p_{00} + p_{01} + p_{10} + p_{11} = 1$.\n\nFirst, we analyze the condition $H(A) = 1$ bit. The entropy of the binary random variable $A$ is given by $H(A) = -P(A=0)\\log_2 P(A=0) - P(A=1)\\log_2 P(A=1)$. The maximum possible entropy for a binary variable is 1 bit, which occurs if and only if its two outcomes are equally likely. Therefore, the condition $H(A)=1$ implies:\n$P(A=0) = 1/2$\n$P(A=1) = 1/2$\n\nFrom the joint probability table, the marginal probabilities for $A$ are:\n$P(A=0) = p_{00} + p_{01}$\n$P(A=1) = p_{10} + p_{11}$\nThis gives us two linear equations:\n1. $p_{00} + p_{01} = 1/2$\n2. $p_{10} + p_{11} = 1/2$\n\nNext, we analyze the condition $H(A|B) = 0$ bits. The conditional entropy is given by $H(A|B) = P(B=0)H(A|B=0) + P(B=1)H(A|B=1)$. Since probabilities and entropies are non-negative, for their sum to be zero, each term must be zero. This means that if $P(B=j) > 0$, then we must have $H(A|B=j) = 0$.\n\nThe entropy $H(A|B=j)$ being zero implies that there is no uncertainty about the value of $A$ once the value of $B$ is known. For a given $B=j$, the conditional probability distribution $P(A=i|B=j)$ must be deterministic. This means that for each $j \\in \\{0, 1\\}$, one of the conditional probabilities $P(A=i|B=j)$ must be 1 and the other must be 0.\n\nThis leads to two possible scenarios for the functional dependence of $A$ on $B$:\nScenario I: For a given value of $B$, $A$ takes the same value.\n- If $B=0$, then $A$ must be $0$. This means $P(A=0|B=0)=1$ and $P(A=1|B=0)=0$.\n- If $B=1$, then $A$ must be $1$. This means $P(A=1|B=1)=1$ and $P(A=0|B=1)=0$.\n\nScenario II: For a given value of $B$, $A$ takes the opposite value.\n- If $B=0$, then $A$ must be $1$. This means $P(A=1|B=0)=1$ and $P(A=0|B=0)=0$.\n- If $B=1$, then $A$ must be $0$. This means $P(A=0|B=1)=1$ and $P(A=1|B=1)=0$.\n\nLet's use the definition of conditional probability $P(A=i|B=j) = \\frac{p_{ij}}{P(B=j)}$ to translate these conditions to the joint probabilities $p_{ij}$. A conditional probability of 0 implies the numerator (the joint probability) must be 0, assuming the denominator is non-zero.\n\nLet's develop Scenario I:\nFrom $P(A=1|B=0)=0$, we get $p_{10}=0$.\nFrom $P(A=0|B=1)=0$, we get $p_{01}=0$.\n\nNow, we substitute these into the equations derived from $H(A)=1$:\n1. $p_{00} + 0 = 1/2 \\implies p_{00} = 1/2$\n2. $0 + p_{11} = 1/2 \\implies p_{11} = 1/2$\n\nThis gives us one possible joint distribution:\n$P(A=0, B=0) = 1/2$\n$P(A=0, B=1) = 0$\n$P(A=1, B=0) = 0$\n$P(A=1, B=1) = 1/2$\nLet's check if this is valid. The sum is $1/2+0+0+1/2=1$. The marginals for A are correct. The conditional entropies are zero because for $B=0$, only $A=0$ happens, and for $B=1$, only $A=1$ happens. This solution is valid.\n\nNow let's develop Scenario II for completeness, although the problem only asks for one solution.\nFrom $P(A=0|B=0)=0$, we get $p_{00}=0$.\nFrom $P(A=1|B=1)=0$, we get $p_{11}=0$.\n\nSubstitute these into the equations for the marginals of A:\n1. $0 + p_{01} = 1/2 \\implies p_{01} = 1/2$\n2. $p_{10} + 0 = 1/2 \\implies p_{10} = 1/2$\n\nThis gives a second possible joint distribution:\n$P(A=0, B=0) = 0$\n$P(A=0, B=1) = 1/2$\n$P(A=1, B=0) = 1/2$\n$P(A=1, B=1) = 0$\nThis solution is also valid.\n\nThe problem asks for one possible joint probability mass function. We can provide the result from Scenario I. The required values are $P(A=0, B=0)$, $P(A=0, B=1)$, $P(A=1, B=0)$, and $P(A=1, B=1)$, which are $1/2$, $0$, $0$, and $1/2$, respectively.", "answer": "$$\\boxed{(\\frac{1}{2}, 0, 0, \\frac{1}{2})}$$", "id": "1649405"}, {"introduction": "Information theory provides powerful tools not just for measuring uncertainty, but also for designing systems that control it. This final practice places you in the role of a system designer tasked with minimizing the overall uncertainty, or joint entropy $H(X,Y)$, of a coupled process [@problem_id:1649373]. By solving this optimization problem, you will discover the direct link between minimizing entropy and maximizing the correlation between variables, a key principle in creating predictable systems.", "problem": "In the design of a cryptographic hardware module, two coupled processes generate a stream of binary random variables, denoted as $X$ and $Y$. These variables can take values in $\\{0, 1\\}$. For the system to function correctly, the marginal distributions of these variables must be uniform, meaning that the probability of observing a '1' is the same as observing a '0' for each variable individually. Specifically, the design enforces $P(X=1) = 1/2$ and $P(Y=1) = 1/2$.\n\nThe lead designer has one degree of freedom to tune the system: the conditional probability $p = P(Y=1|X=1)$. This parameter controls the correlation between the two binary streams. The goal is to configure the system to be as predictable as possible, which in the language of information theory means minimizing the total uncertainty, or joint entropy, $H(X,Y)$.\n\nAssuming entropy is calculated in bits (using logarithm base 2), determine all possible values of the parameter $p$ that will minimize the joint entropy $H(X,Y)$.", "solution": "Let $X,Y\\in\\{0,1\\}$ with $P(X=1)=\\frac{1}{2}$ and $P(Y=1)=\\frac{1}{2}$. Let $p=P(Y=1\\mid X=1)$. Denote $q=P(Y=1\\mid X=0)$. The marginal constraint on $Y$ gives, by the law of total probability,\n$$\nP(Y=1)=P(Y=1\\mid X=1)P(X=1)+P(Y=1\\mid X=0)P(X=0)=\\tfrac{1}{2}p+\\tfrac{1}{2}q=\\tfrac{1}{2},\n$$\nhence $p+q=1$ and thus $q=1-p$. The joint probabilities are\n$$\nP(X=1,Y=1)=\\tfrac{1}{2}p,\\quad P(X=1,Y=0)=\\tfrac{1}{2}(1-p),\\quad P(X=0,Y=1)=\\tfrac{1}{2}(1-p),\\quad P(X=0,Y=0)=\\tfrac{1}{2}p,\n$$\nso $0\\leq p\\leq 1$.\n\nThe joint entropy in bits is\n$$\nH(X,Y)=-\\sum_{x,y}P(x,y)\\log_{2}P(x,y)\n= -2\\cdot\\tfrac{1}{2}p\\log_{2}\\!\\big(\\tfrac{1}{2}p\\big)\\;-\\;2\\cdot\\tfrac{1}{2}(1-p)\\log_{2}\\!\\big(\\tfrac{1}{2}(1-p)\\big).\n$$\nUsing $\\log_{2}\\!\\big(\\tfrac{a}{2}\\big)=\\log_{2}a-1$, this simplifies to\n$$\nH(X,Y)=-p\\big(\\log_{2}p-1\\big)-(1-p)\\big(\\log_{2}(1-p)-1\\big)\n= -p\\log_{2}p-(1-p)\\log_{2}(1-p)+1.\n$$\nDefine the binary entropy function $h_{2}(p)=-p\\log_{2}p-(1-p)\\log_{2}(1-p)$. Then\n$$\nH(X,Y)=1+h_{2}(p).\n$$\nTo minimize $H(X,Y)$ over $p\\in[0,1]$, it suffices to minimize $h_{2}(p)$. Differentiating,\n$$\nh_{2}'(p)=-\\log_{2}p+\\log_{2}(1-p),\\qquad\nh_{2}''(p)=-\\frac{1}{\\ln 2}\\Big(\\frac{1}{p}+\\frac{1}{1-p}\\Big)0\\quad\\text{for }p\\in(0,1),\n$$\nso $h_{2}$ is concave with its unique stationary point at $h_{2}'(p)=0\\Rightarrow p=1/2$, which is the maximum. Therefore the minimum of $h_{2}(p)$ on $[0,1]$ occurs at the endpoints $p=0$ and $p=1$, where $h_{2}(0)=h_{2}(1)=0$. Consequently,\n$$\n\\min_{p\\in[0,1]}H(X,Y)=1+h_{2}(p)\\quad\\text{is attained at }p\\in\\{0,1\\}.\n$$\nAt $p=1$, we have $Y=X$ almost surely; at $p=0$, we have $Y=1-X$ almost surely. Both satisfy the marginal constraints and yield the minimum joint entropy.", "answer": "$$\\boxed{\\{0, 1\\}}$$", "id": "1649373"}]}