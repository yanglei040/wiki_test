{"hands_on_practices": [{"introduction": "We begin with a foundational scenario that establishes a crucial baseline for understanding conditional mutual information. When two variables are both direct results of a single, known cause, how much new information can one provide about the other? This exercise [@problem_id:1612854] explores the case where two outputs, $Y$ and $Z$, are deterministic functions of a single input, $X$, and reveals a core principle of information flow when the common cause is observed.", "problem": "In an autonomous vehicle's safety system, a primary Light Detection and Ranging (LIDAR) sensor measures the distance to a forward obstacle. Let this distance be represented by a discrete random variable $X$. This measurement is fed to two independent onboard processing units.\n\n1.  The Threat Assessment Module (TAM) processes the distance $X$ to determine a \"threat level,\" which we represent by another random variable $Y$. The value of $Y$ is completely determined by the value of $X$ through a fixed, deterministic function.\n2.  The Active Safety Protocol (ASP) unit also processes the distance $X$ to make a binary decision on whether to pre-charge the hydraulic brakes. We represent this decision by a random variable $Z$. The value of $Z$ is also completely determined by the value of $X$ through a separate, fixed, deterministic function.\n\nGiven this setup, where both $Y$ and $Z$ are deterministic functions of $X$, calculate the conditional mutual information $I(Y; Z | X)$ between the threat level $Y$ and the braking decision $Z$, given the LIDAR measurement $X$.\n\nExpress your answer as a single numerical value in units of bits.", "solution": "Let $X$ be a discrete random variable representing the LIDAR measurement, and let $Y=f(X)$ and $Z=g(X)$ be deterministic functions of $X$. We are asked to compute the conditional mutual information $I(Y; Z \\mid X)$.\n\nBy definition, the conditional mutual information can be written as\n$$\nI(Y; Z \\mid X) = H(Y \\mid X) - H(Y \\mid Z, X).\n$$\nWe evaluate each term.\n\nSince $Y$ is a deterministic function of $X$, for any $x$ we have $p_{Y \\mid X}(y \\mid x) = 1$ if $y=f(x)$ and $0$ otherwise. Therefore,\n$$\nH(Y \\mid X=x) = -\\sum_{y} p_{Y \\mid X}(y \\mid x) \\log_{2} p_{Y \\mid X}(y \\mid x) = -1 \\cdot \\log_{2}(1) = 0.\n$$\nAveraging over $X$ gives\n$$\nH(Y \\mid X) = \\sum_{x} p_{X}(x) H(Y \\mid X=x) = 0.\n$$\n\nNext, consider $H(Y \\mid Z, X)$. Because $Y=f(X)$ depends only on $X$, conditioning further on $Z$ does not introduce any uncertainty about $Y$ once $X$ is known. For any $(x,z)$, $Y$ is still $f(x)$ with probability $1$, hence\n$$\nH(Y \\mid Z=z, X=x) = 0 \\quad \\text{and thus} \\quad H(Y \\mid Z, X) = 0.\n$$\n\nSubstituting into the definition,\n$$\nI(Y; Z \\mid X) = H(Y \\mid X) - H(Y \\mid Z, X) = 0 - 0 = 0.\n$$\n\nEquivalently, one can use the identity\n$$\nI(Y; Z \\mid X) = H(Y \\mid X) + H(Z \\mid X) - H(Y, Z \\mid X),\n$$\nand note that each conditional entropy is zero given that $(Y,Z)$ are deterministic functions of $X$, yielding the same result.\n\nTherefore, the conditional mutual information between $Y$ and $Z$ given $X$ is zero bits.", "answer": "$$\\boxed{0}$$", "id": "1612854"}, {"introduction": "One of the most surprising properties of conditional mutual information is its ability to create statistical dependence where none existed before. This practice [@problem_id:1612850] presents a classic example involving two independent random variables, which are initially unrelated. By conditioning on a variable that combines them, we will uncover a hidden dependency, a phenomenon that underscores why conditioning is such a powerful and subtle concept in information theory.", "problem": "Consider a simple digital communication system involving two independent binary sources, Source 1 and Source 2. Each source generates a single bit, represented by the random variables $X_1$ and $X_2$, respectively. For each source, the bit is a '0' with probability 0.5 and a '1' with probability 0.5.\n\nA monitoring station receives these two bits and computes their sum, defined by the random variable $Z = X_1 + X_2$. The value of this sum $Z$ is then observed. We are interested in quantifying how much information the bit from Source 1 gives about the bit from Source 2, once their sum $Z$ is known. This is a measure of the remaining statistical dependence between the sources after observing their sum.\n\nCalculate the conditional mutual information $I(X_1; X_2 | Z)$. All calculations involving logarithms must use base 2. Express your final answer in bits as a decimal value.", "solution": "Let $X_{1}$ and $X_{2}$ be independent $\\text{Bernoulli}\\left(\\frac{1}{2}\\right)$ random variables. Define $Z=X_{1}+X_{2}$, which takes values in $\\{0,1,2\\}$ with\n$$\nP(Z=0)=\\frac{1}{4},\\quad P(Z=1)=\\frac{1}{2},\\quad P(Z=2)=\\frac{1}{4}.\n$$\nThe conditional mutual information is\n$$\nI(X_{1};X_{2}\\mid Z)=\\sum_{z}P(z)\\,I(X_{1};X_{2}\\mid Z=z),\n$$\nwith all entropies and logarithms in base $2$ (bits).\n\nCompute $I(X_{1};X_{2}\\mid Z=z)$ for each $z$ using $I(X_{1};X_{2}\\mid Z=z)=H(X_{1}\\mid Z=z)-H(X_{1}\\mid X_{2},Z=z)$.\n\n1) For $z=0$: only $(X_{1},X_{2})=(0,0)$ is possible, so $H(X_{1}\\mid Z=0)=0$ and $H(X_{1}\\mid X_{2},Z=0)=0$. Hence $I(X_{1};X_{2}\\mid Z=0)=0$.\n\n2) For $z=2$: only $(X_{1},X_{2})=(1,1)$ is possible, so $H(X_{1}\\mid Z=2)=0$ and $H(X_{1}\\mid X_{2},Z=2)=0$. Hence $I(X_{1};X_{2}\\mid Z=2)=0$.\n\n3) For $z=1$: the pairs $(0,1)$ and $(1,0)$ occur with equal probability $\\frac{1}{2}$. Thus $X_{1}\\mid Z=1$ is $\\text{Bernoulli}\\left(\\frac{1}{2}\\right)$, giving\n$$\nH(X_{1}\\mid Z=1)=-\\left(\\tfrac{1}{2}\\log_{2}\\tfrac{1}{2}+\\tfrac{1}{2}\\log_{2}\\tfrac{1}{2}\\right)=1.\n$$\nGiven $Z=1$ and $X_{2}$, $X_{1}$ is determined by $X_{1}=1-X_{2}$, so $H(X_{1}\\mid X_{2},Z=1)=0$. Hence $I(X_{1};X_{2}\\mid Z=1)=1$.\n\nAverage over $Z$:\n$$\nI(X_{1};X_{2}\\mid Z)=\\frac{1}{4}\\cdot 0+\\frac{1}{2}\\cdot 1+\\frac{1}{4}\\cdot 0=\\frac{1}{2}.\n$$\nTherefore, the conditional mutual information is $0.5$ bits.", "answer": "$$\\boxed{0.5}$$", "id": "1612850"}, {"introduction": "Real-world systems often involve a mixture of informational relationships. This problem [@problem_id:1612830] models such a scenario, where the effect of conditioning is not uniform. Here, we analyze how the relationship between two variables, $X$ and $Y$, changes drastically depending on the state of a third variable, $Z$. This exercise will reinforce your calculation skills and deepen your intuition for how conditional mutual information is a weighted average of information over different contexts.", "problem": "An electronic game generates a random integer outcome, $\\omega$, from the set $\\{1, 2, 3, 4, 5, 6\\}$. The process simulates a single roll of a fair six-sided die, so each outcome $\\omega$ has a probability of $1/6$. Based on the outcome, three binary random variables, $X$, $Y$, and $Z$, are determined as follows:\n\n-   $X=1$ if the outcome $\\omega$ is an odd number; otherwise, $X=0$.\n-   $Y=1$ if the outcome $\\omega$ is a prime number; otherwise, $Y=0$. For the purpose of this problem, the prime numbers in the set of outcomes are $\\{2, 3, 5\\}$.\n-   $Z=1$ if the outcome $\\omega$ is strictly greater than 4; otherwise, $Z=0$.\n\nCalculate the conditional mutual information $I(X; Y | Z)$. Express your answer in bits as a closed-form analytic expression. Note that information measured in bits corresponds to the use of base-2 logarithms in entropy calculations.", "solution": "Let $\\Omega=\\{1,2,3,4,5,6\\}$ with $P(\\omega)=\\frac{1}{6}$ for each outcome. Define\n- $X=1$ for $\\{1,3,5\\}$ and $X=0$ for $\\{2,4,6\\}$,\n- $Y=1$ for $\\{2,3,5\\}$ and $Y=0$ for $\\{1,4,6\\}$,\n- $Z=1$ for $\\{5,6\\}$ and $Z=0$ for $\\{1,2,3,4\\}$.\n\nWe seek $I(X;Y|Z)$, which can be written as\n$$\nI(X;Y|Z)=\\sum_{z}P(z)\\,I(X;Y\\,|\\,Z=z)\n=\\sum_{z}P(z)\\sum_{x,y}P(x,y\\,|\\,z)\\,\\log_{2}\\!\\left(\\frac{P(x,y\\,|\\,z)}{P(x\\,|\\,z)P(y\\,|\\,z)}\\right).\n$$\n\nFirst, $P(Z=1)=\\frac{2}{6}=\\frac{1}{3}$ and $P(Z=0)=\\frac{4}{6}=\\frac{2}{3}$.\n\nCase $Z=1$: The outcomes are $\\{5,6\\}$, each with conditional probability $\\frac{1}{2}$. For $\\omega=5$ we have $(X,Y)=(1,1)$; for $\\omega=6$ we have $(X,Y)=(0,0)$. Hence $P(X=1\\,|\\,Z=1)=P(Y=1\\,|\\,Z=1)=\\frac{1}{2}$ and $P(X=Y\\,|\\,Z=1)=1$. Therefore $Y$ is a deterministic function of $X$ (and vice versa) given $Z=1$, so\n$$\nI(X;Y\\,|\\,Z=1)=H(X\\,|\\,Z=1)-H(X\\,|\\,Y,Z=1)=H(X\\,|\\,Z=1).\n$$\nSince $P(X=1\\,|\\,Z=1)=\\frac{1}{2}$,\n$$\nH(X\\,|\\,Z=1)=-\\sum_{x\\in\\{0,1\\}}P(x\\,|\\,Z=1)\\log_{2}P(x\\,|\\,Z=1)=1,\n$$\nso $I(X;Y\\,|\\,Z=1)=1$.\n\nCase $Z=0$: The outcomes are $\\{1,2,3,4\\}$, each with conditional probability $\\frac{1}{4}$. The pairs $(X,Y)$ realized are $(1,0)$ from $\\omega=1$, $(0,1)$ from $\\omega=2$, $(1,1)$ from $\\omega=3$, and $(0,0)$ from $\\omega=4$, each with probability $\\frac{1}{4}$. Thus\n$$\nP(X=1\\,|\\,Z=0)=\\frac{1}{2},\\quad P(Y=1\\,|\\,Z=0)=\\frac{1}{2},\\quad P(X=x,Y=y\\,|\\,Z=0)=\\frac{1}{4}=P(X=x\\,|\\,Z=0)P(Y=y\\,|\\,Z=0),\n$$\nso $X$ and $Y$ are independent given $Z=0$, and hence\n$$\nI(X;Y\\,|\\,Z=0)=0.\n$$\n\nTherefore,\n$$\nI(X;Y\\,|\\,Z)=P(Z=1)\\,I(X;Y\\,|\\,Z=1)+P(Z=0)\\,I(X;Y\\,|\\,Z=0)=\\frac{1}{3}\\cdot 1+\\frac{2}{3}\\cdot 0=\\frac{1}{3}.\n$$\n\nFor completeness, this agrees with the identity $I(X;Y|Z)=H(X|Z)-H(X|Y,Z)$. Indeed, $H(X|Z=1)=1$ and $H(X|Z=0)=1$, so $H(X|Z)=1$. Also $H(X|Y,Z=1)=0$ and $H(X|Y,Z=0)=1$, so $H(X|Y,Z)=\\frac{1}{3}\\cdot 0+\\frac{2}{3}\\cdot 1=\\frac{2}{3}$. Thus $I(X;Y|Z)=1-\\frac{2}{3}=\\frac{1}{3}$ bits.", "answer": "$$\\boxed{\\frac{1}{3}}$$", "id": "1612830"}]}