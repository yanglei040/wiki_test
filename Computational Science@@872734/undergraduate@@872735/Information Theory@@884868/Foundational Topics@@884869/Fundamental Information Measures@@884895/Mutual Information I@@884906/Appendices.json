{"hands_on_practices": [{"introduction": "We begin with the most straightforward case: a deterministic relationship where one variable is a direct function of another. This exercise explores how to calculate the mutual information between a number and one of its propertiesâ€”its last digit. This practice provides a foundational understanding of how the mutual information $I(X;Y)$ elegantly simplifies to the entropy $H(Y)$ when variable $Y$ is fully determined by variable $X$ [@problem_id:1642332].", "problem": "An integer $X$ is selected uniformly at random from the set $\\{1, 2, \\dots, 100\\}$. A second random variable $Y$ is defined to be the last digit of the integer $X$. For example, if the selected integer is $X=57$, then the value of $Y$ is 7. If the selected integer is $X=90$, the value of $Y$ is 0.\n\nDetermine the mutual information $I(X;Y)$ between these two random variables. Express your answer as a single closed-form analytic expression in units of bits, which implies the use of the logarithm to the base 2.", "solution": "Let $X$ be uniformly distributed on $\\{1,2,\\dots,100\\}$, so $P(X=x)=\\frac{1}{100}$ for each $x$. Define $Y$ as the last digit of $X$. Then $Y$ is a deterministic function of $X$, which implies the conditional entropy satisfies $H(Y|X)=0$. By the definition of mutual information,\n$$\nI(X;Y)=H(Y)-H(Y|X)=H(Y).\n$$\nTo compute $H(Y)$, determine the distribution of $Y$. For each $y\\in\\{0,1,\\dots,9\\}$, the integers in $\\{1,\\dots,100\\}$ with last digit $y$ are exactly $10$ values, hence\n$$\nP(Y=y)=\\frac{10}{100}=\\frac{1}{10},\\quad y\\in\\{0,1,\\dots,9\\}.\n$$\nThus $Y$ is uniform on $10$ outcomes, and its entropy in bits is\n$$\nH(Y)=-\\sum_{y=0}^{9}\\frac{1}{10}\\log_{2}\\!\\left(\\frac{1}{10}\\right)=\\log_{2}(10).\n$$\nTherefore,\n$$\nI(X;Y)=\\log_{2}(10).\n$$\nAs a cross-check, using $I(X;Y)=H(X)-H(X|Y)$, we have $H(X)=\\log_{2}(100)$ and, given any $Y=y$, the $10$ possible $X$ values are equally likely, so $H(X|Y=y)=\\log_{2}(10)$ for all $y$, yielding $H(X|Y)=\\log_{2}(10)$ and hence $I(X;Y)=\\log_{2}(100)-\\log_{2}(10)=\\log_{2}(10)$.", "answer": "$$\\boxed{\\log_{2}(10)}$$", "id": "1642332"}, {"introduction": "Now we move from a deterministic relationship to a probabilistic one, where one variable influences, but does not fully determine, another. Using an intuitive model of a packet moving through a simple network, this problem asks you to quantify the information shared between its starting and ending nodes. This practice will solidify your grasp of mutual information as a measure of the reduction in uncertainty about one variable, given knowledge of another [@problem_id:1642319].", "problem": "Consider a simplified model of a small, decentralized communication network. The network consists of three nodes, which we can label as $N_1$, $N_2$, and $N_3$. Each node is connected by a direct communication link to the other two nodes. A single data packet is created at one of these nodes. Let the random variable $X$ represent the starting node of the packet, which is chosen uniformly at random from the set $\\{N_1, N_2, N_3\\}$.\n\nOnce created, the packet is immediately forwarded to one of its neighboring nodes. The destination node is chosen with equal probability from the two available neighbors. Let the random variable $Y$ represent this destination node.\n\nCalculate the mutual information $I(X; Y)$ between the starting node and the destination node. Express your answer as a closed-form analytic expression using logarithm base 2 ($\\log_2$).", "solution": "Let $X$ be the starting node, uniformly distributed over $\\{N_{1},N_{2},N_{3}\\}$, so $P(X=x)=\\frac{1}{3}$ for each $x$. Given $X=x$, the destination $Y$ is chosen uniformly among the two neighbors, hence\n$$\nP(Y=y \\mid X=x)=\\begin{cases}\n\\frac{1}{2},  y\\neq x,\\\\\n0,  y=x.\n\\end{cases}\n$$\nThe marginal of $Y$ is obtained by summing over $x$:\n$$\nP(Y=y)=\\sum_{x} P(Y=y \\mid X=x)P(X=x)=\\left(\\frac{1}{2}\\cdot\\frac{1}{3}\\right)+\\left(\\frac{1}{2}\\cdot\\frac{1}{3}\\right)=\\frac{1}{3},\n$$\nso $Y$ is also uniform on $\\{N_{1},N_{2},N_{3}\\}$.\n\nUsing base-2 logarithms, the entropy of $Y$ is\n$$\nH(Y)=-\\sum_{y} P(Y=y)\\log_{2}\\big(P(Y=y)\\big)=-3\\cdot\\frac{1}{3}\\log_{2}\\!\\left(\\frac{1}{3}\\right)=\\log_{2}(3).\n$$\nFor the conditional entropy, for any fixed $x$ the distribution of $Y$ given $X=x$ is supported on two outcomes with probabilities $\\frac{1}{2}$ each, hence\n$$\nH(Y\\mid X=x)=-\\left(\\frac{1}{2}\\log_{2}\\!\\left(\\frac{1}{2}\\right)+\\frac{1}{2}\\log_{2}\\!\\left(\\frac{1}{2}\\right)\\right)=1,\n$$\nand averaging over $x$ yields $H(Y\\mid X)=1$.\n\nTherefore, the mutual information is\n$$\nI(X;Y)=H(Y)-H(Y\\mid X)=\\log_{2}(3)-1.\n$$", "answer": "$$\\boxed{\\log_{2}(3)-1}$$", "id": "1642319"}, {"introduction": "Finally, we tackle a more subtle scenario where two variables share information indirectly through a common underlying cause. This situation is frequent in fields like signal processing and complex systems. By calculating the mutual information between two signals constructed from a shared random bit, you will learn how to analyze these indirect dependencies and appreciate the power of mutual information in uncovering hidden correlations [@problem_id:1642335].", "problem": "Consider a simple signal processing model where new signals are generated from sums of input bits. Let $X_1$, $X_2$, and $X_3$ be three mutually independent random variables, each representing a bit that is equally likely to be 0 or 1. That is, for each $i \\in \\{1, 2, 3\\}$, $P(X_i=0) = P(X_i=1) = \\frac{1}{2}$.\n\nTwo new random variables, $Y$ and $Z$, are formed by taking sums of these bits as follows:\n$$Y = X_1 + X_2$$\n$$Z = X_2 + X_3$$\n\nCalculate the mutual information $I(Y; Z)$. Express your answer in bits, using the base-2 logarithm for all calculations. The final answer should be a single real number, which can be provided as a fraction or a decimal.", "solution": "We are given independent bits $X_{1},X_{2},X_{3}$ with $P(X_{i}=0)=P(X_{i}=1)=\\frac{1}{2}$ for each $i$. Define $Y=X_{1}+X_{2}$ and $Z=X_{2}+X_{3}$. The mutual information in bits is\n$$\nI(Y;Z)=H(Y)+H(Z)-H(Y,Z),\n$$\nwhere all entropies use $\\log_{2}$.\n\nFirst compute the marginals. Since $Y=X_{1}+X_{2}$ is the sum of two independent Bernoulli random variables with parameter $\\frac{1}{2}$,\n$$\nP(Y=0)=\\frac{1}{4},\\quad P(Y=1)=\\frac{1}{2},\\quad P(Y=2)=\\frac{1}{4}.\n$$\nThus\n$$\nH(Y)=-\\sum_{y\\in\\{0,1,2\\}}P(Y=y)\\log_{2}P(Y=y)\n= -\\left(\\frac{1}{4}\\log_{2}\\frac{1}{4}+\\frac{1}{2}\\log_{2}\\frac{1}{2}+\\frac{1}{4}\\log_{2}\\frac{1}{4}\\right)\n=\\frac{3}{2}\\ \\text{bits}.\n$$\nBy symmetry, $H(Z)=\\frac{3}{2}$ bits.\n\nNext compute the joint distribution of $(Y,Z)$. Condition on $X_{2}$. Given $X_{2}=0$, we have $Y=X_{1}$ and $Z=X_{3}$, which are independent and each Bernoulli with parameter $\\frac{1}{2}$. Therefore,\n$$\nP\\big((Y,Z)=(a,b)\\mid X_{2}=0\\big)=\\frac{1}{4}\\quad\\text{for }(a,b)\\in\\{0,1\\}\\times\\{0,1\\}.\n$$\nGiven $X_{2}=1$, we have $Y=1+X_{1}$ and $Z=1+X_{3}$, again independent with each supported on $\\{1,2\\}$ equally likely. Therefore,\n$$\nP\\big((Y,Z)=(a,b)\\mid X_{2}=1\\big)=\\frac{1}{4}\\quad\\text{for }(a,b)\\in\\{1,2\\}\\times\\{1,2\\}.\n$$\nUsing the law of total probability with $P(X_{2}=0)=P(X_{2}=1)=\\frac{1}{2}$, the nonzero joint probabilities are:\n$$\n\\begin{aligned}\nP(Y,Z)\\text{ at }(0,0),(0,1),(1,0),(1,2),(2,1),(2,2)\\text{ equals }\\frac{1}{8}\\ \\text{each},\\\\\nP(Y,Z)\\text{ at }(1,1)\\text{ equals }\\frac{1}{8}+\\frac{1}{8}=\\frac{1}{4}.\n\\end{aligned}\n$$\nHence\n$$\nH(Y,Z)=-\\sum_{y,z}P(y,z)\\log_{2}P(y,z)\n=-\\left(6\\cdot\\frac{1}{8}\\log_{2}\\frac{1}{8}+\\frac{1}{4}\\log_{2}\\frac{1}{4}\\right)\n=\\frac{11}{4}\\ \\text{bits}.\n$$\nFinally,\n$$\nI(Y;Z)=H(Y)+H(Z)-H(Y,Z)=\\frac{3}{2}+\\frac{3}{2}-\\frac{11}{4}=\\frac{1}{4}\\ \\text{bits}.\n$$", "answer": "$$\\boxed{\\frac{1}{4}}$$", "id": "1642335"}]}