{"hands_on_practices": [{"introduction": "A core skill in information theory is the ability to quantify the relationship between two variables directly from observational data. This practice provides a foundational exercise in calculating mutual information from a given joint probability distribution, which represents the frequencies of co-occurring events. By applying the definition of mutual information as the Kullback-Leibler (KL) divergence, $I(X;Y) = D_{KL}(p(x,y) || p(x)p(y))$, you will step through the essential mechanics of computing marginal distributions and then quantifying the 'distance' from statistical independence [@problem_id:1654581]. This hands-on calculation solidifies the connection between the joint distribution's structure and the amount of shared information.", "problem": "Consider a simplified model for a weather forecasting system. Let the random variable $X$ represent the daily forecast, which can take one of three states: $x_1$ ('Forecast: Sunny'), $x_2$ ('Forecast: Cloudy'), or $x_3$ ('Forecast: Rainy'). Let the random variable $Y$ represent the actual weather outcome for that day, which can take one of two states: $y_1$ ('Actual: Sunny') or $y_2$ ('Actual: Not Sunny').\n\nAfter observing the system for a long time, the joint probability distribution $p(x, y)$ for the forecast and the actual outcome has been determined as follows:\n- $p(X=x_1, Y=y_1) = 5/16$\n- $p(X=x_1, Y=y_2) = 1/16$\n- $p(X=x_2, Y=y_1) = 1/8$\n- $p(X=x_2, Y=y_2) = 3/16$\n- $p(X=x_3, Y=y_1) = 1/16$\n- $p(X=x_3, Y=y_2) = 1/4$\n\nCalculate the mutual information $I(X; Y)$ between the forecast $X$ and the actual weather $Y$. Express your answer in units of bits, rounded to four significant figures. All logarithms should be interpreted as base 2.", "solution": "We are given the joint distribution of $(X,Y)$ over $\\{x_{1},x_{2},x_{3}\\}\\times\\{y_{1},y_{2}\\}$:\n$$\np(x_{1},y_{1})=\\frac{5}{16},\\quad p(x_{1},y_{2})=\\frac{1}{16},\\quad\np(x_{2},y_{1})=\\frac{1}{8},\\quad p(x_{2},y_{2})=\\frac{3}{16},\\quad\np(x_{3},y_{1})=\\frac{1}{16},\\quad p(x_{3},y_{2})=\\frac{1}{4}.\n$$\nFirst compute the marginals by summing the joint probabilities over the other variable:\n$$\np(x_{1})=\\frac{5}{16}+\\frac{1}{16}=\\frac{3}{8},\\quad\np(x_{2})=\\frac{1}{8}+\\frac{3}{16}=\\frac{5}{16},\\quad\np(x_{3})=\\frac{1}{16}+\\frac{1}{4}=\\frac{5}{16},\n$$\n$$\np(y_{1})=\\frac{5}{16}+\\frac{1}{8}+\\frac{1}{16}=\\frac{8}{16}=\\frac{1}{2},\\quad\np(y_{2})=\\frac{1}{16}+\\frac{3}{16}+\\frac{1}{4}=\\frac{8}{16}=\\frac{1}{2}.\n$$\nThe mutual information is\n$$\nI(X;Y)=\\sum_{x}\\sum_{y}p(x,y)\\,\\log_{2}\\!\\left(\\frac{p(x,y)}{p(x)\\,p(y)}\\right).\n$$\nSince $p(y_{1})=p(y_{2})=\\frac{1}{2}$, the ratio simplifies to $\\frac{p(x,y)}{p(x)\\,p(y)}=\\frac{2\\,p(x,y)}{p(x)}$. Compute each term:\n$$\n\\begin{aligned}\n(x_{1},y_{1}):\\quad \\frac{2\\,p}{p(x_{1})}=\\frac{2\\cdot\\frac{5}{16}}{\\frac{3}{8}}=\\frac{5}{3},\\text{contribution } \\frac{5}{16}\\log_{2}\\!\\left(\\frac{5}{3}\\right),\\\\\n(x_{1},y_{2}):\\quad \\frac{2\\,p}{p(x_{1})}=\\frac{2\\cdot\\frac{1}{16}}{\\frac{3}{8}}=\\frac{1}{3},\\text{contribution } \\frac{1}{16}\\log_{2}\\!\\left(\\frac{1}{3}\\right),\\\\\n(x_{2},y_{1}):\\quad \\frac{2\\,p}{p(x_{2})}=\\frac{2\\cdot\\frac{1}{8}}{\\frac{5}{16}}=\\frac{4}{5},\\text{contribution } \\frac{1}{8}\\log_{2}\\!\\left(\\frac{4}{5}\\right),\\\\\n(x_{2},y_{2}):\\quad \\frac{2\\,p}{p(x_{2})}=\\frac{2\\cdot\\frac{3}{16}}{\\frac{5}{16}}=\\frac{6}{5},\\text{contribution } \\frac{3}{16}\\log_{2}\\!\\left(\\frac{6}{5}\\right),\\\\\n(x_{3},y_{1}):\\quad \\frac{2\\,p}{p(x_{3})}=\\frac{2\\cdot\\frac{1}{16}}{\\frac{5}{16}}=\\frac{2}{5},\\text{contribution } \\frac{1}{16}\\log_{2}\\!\\left(\\frac{2}{5}\\right),\\\\\n(x_{3},y_{2}):\\quad \\frac{2\\,p}{p(x_{3})}=\\frac{2\\cdot\\frac{1}{4}}{\\frac{5}{16}}=\\frac{8}{5},\\text{contribution } \\frac{1}{4}\\log_{2}\\!\\left(\\frac{8}{5}\\right).\n\\end{aligned}\n$$\nThus\n$$\n\\begin{aligned}\nI(X;Y)\n=\\frac{5}{16}\\log_{2}\\!\\left(\\frac{5}{3}\\right)+\\frac{1}{16}\\log_{2}\\!\\left(\\frac{1}{3}\\right)+\\frac{1}{8}\\log_{2}\\!\\left(\\frac{4}{5}\\right)+\\frac{3}{16}\\log_{2}\\!\\left(\\frac{6}{5}\\right)+\\frac{1}{16}\\log_{2}\\!\\left(\\frac{2}{5}\\right)+\\frac{1}{4}\\log_{2}\\!\\left(\\frac{8}{5}\\right)\\\\\n=\\frac{5}{16}(\\log_{2}5-\\log_{2}3)+\\frac{1}{16}(-\\log_{2}3)+\\frac{1}{8}(2-\\log_{2}5)+\\frac{3}{16}(\\log_{2}3+1-\\log_{2}5)+\\frac{1}{16}(1-\\log_{2}5)+\\frac{1}{4}(3-\\log_{2}5)\\\\\n=\\left(\\frac{1}{4}+\\frac{3}{16}+\\frac{1}{16}+\\frac{3}{4}\\right)+\\left(-\\frac{5}{16}-\\frac{1}{16}+\\frac{3}{16}\\right)\\log_{2}3+\\left(\\frac{5}{16}-\\frac{1}{8}-\\frac{3}{16}-\\frac{1}{16}-\\frac{1}{4}\\right)\\log_{2}5\\\\\n=\\frac{5}{4}-\\frac{3}{16}\\log_{2}3-\\frac{5}{16}\\log_{2}5.\n\\end{aligned}\n$$\nEvaluating numerically with base-2 logarithms and rounding to four significant figures gives\n$$\nI(X;Y)\\approx 0.2272\\ \\text{bits}.\n$$", "answer": "$$\\boxed{0.2272}$$", "id": "1654581"}, {"introduction": "Moving from numerical computation to conceptual proof, this exercise explores a fundamental property of information flow in sequential systems. Many real-world processes, from signal processing to genetic inheritance, can be modeled as Markov chains, where the future state is independent of the past given the present. This problem asks you to demonstrate how this probabilistic structure, denoted $X \\to Y \\to Z$, has a profound and direct consequence in information theory [@problem_id:1654632]. By using the KL divergence definition of conditional mutual information, $I(X; Z | Y)$, you will prove that no new information can be gained about $X$ from $Z$ if $Y$ is already known, a result that underpins the famous Data Processing Inequality.", "problem": "Consider a sequence of three discrete random variables $X, Y, Z$ that form a Markov chain, denoted as $X \\to Y \\to Z$. This relationship implies that given the present state $Y$, the future state $Z$ is conditionally independent of the past state $X$. Mathematically, this condition is defined as $p(z | x, y) = p(z | y)$ for all values $x, y, z$ for which the joint probability $p(x, y) > 0$.\n\nIn information theory, the conditional mutual information $I(X; Z | Y)$ quantifies the amount of information that $X$ and $Z$ share, given that $Y$ is known. It can be defined in terms of the Kullback-Leibler (KL) divergence between the true conditional joint distribution $p(x, z | y)$ and the product of the conditional marginal distributions $p(x|y)p(z|y)$. The formula is given by:\n\n$$I(X; Z | Y) = \\sum_{x, y, z} p(x, y, z) \\log_{2} \\left( \\frac{p(x, z | y)}{p(x|y)p(z|y)} \\right)$$\n\nwhere the sums are over all possible values of the random variables.\n\nGiven these definitions, calculate the value of the conditional mutual information $I(X; Z | Y)$ for any set of variables forming a Markov chain $X \\to Y \\to Z$. Express your answer as a single number in units of bits.", "solution": "The problem asks for the calculation of the conditional mutual information $I(X; Z | Y)$ for a set of random variables $X, Y, Z$ that form a Markov chain $X \\to Y \\to Z$. We are given the definition of $I(X; Z | Y)$ in terms of the Kullback-Leibler divergence:\n$$I(X; Z | Y) = \\sum_{x, y, z} p(x, y, z) \\log_{2} \\left( \\frac{p(x, z | y)}{p(x|y)p(z|y)} \\right)$$\nOur goal is to simplify the term inside the logarithm using the properties of the Markov chain.\n\nThe key to solving this is to show that for a Markov chain, the numerator inside the logarithm, $p(x, z | y)$, is equal to the denominator, $p(x|y)p(z|y)$. Let's prove this factorization.\n\nBy the definition of conditional probability, the term $p(x, z | y)$ can be written as:\n$$p(x, z | y) = \\frac{p(x, y, z)}{p(y)}$$\nNow, we apply the chain rule of probability to the joint distribution $p(x, y, z)$:\n$$p(x, y, z) = p(z | x, y) p(x, y)$$\nSubstituting this back into the expression for $p(x, z | y)$:\n$$p(x, z | y) = \\frac{p(z | x, y) p(x, y)}{p(y)}$$\nWe are given that $X, Y, Z$ form a Markov chain $X \\to Y \\to Z$. The defining property of this chain is that $p(z | x, y) = p(z | y)$. We can substitute this property into our equation:\n$$p(x, z | y) = \\frac{p(z | y) p(x, y)}{p(y)}$$\nNext, we can rearrange the terms on the right-hand side:\n$$p(x, z | y) = p(z | y) \\left( \\frac{p(x, y)}{p(y)} \\right)$$\nWe recognize the term in the parentheses as the definition of the conditional probability $p(x|y)$:\n$$p(x|y) = \\frac{p(x, y)}{p(y)}$$\nSubstituting this back, we get the desired factorization:\n$$p(x, z | y) = p(z | y) p(x | y)$$\nThis result shows that for a Markov chain, the variables $X$ and $Z$ are conditionally independent given $Y$.\n\nNow we can substitute this result back into the original formula for conditional mutual information:\n$$I(X; Z | Y) = \\sum_{x, y, z} p(x, y, z) \\log_{2} \\left( \\frac{p(x|y)p(z|y)}{p(x|y)p(z|y)} \\right)$$\nThe fraction inside the logarithm simplifies to 1:\n$$I(X; Z | Y) = \\sum_{x, y, z} p(x, y, z) \\log_{2}(1)$$\nSince $\\log_{2}(1) = 0$, the expression becomes:\n$$I(X; Z | Y) = \\sum_{x, y, z} p(x, y, z) \\cdot 0$$\nThis simplifies to:\n$$I(X; Z | Y) = 0$$\nThus, the conditional mutual information between $X$ and $Z$ given $Y$ is 0 for any Markov chain $X \\to Y \\to Z$. This is an important result known as the Data Processing Inequality, of which this is a specific case. The value is 0 bits.", "answer": "$$\\boxed{0}$$", "id": "1654632"}, {"introduction": "The relationship between mutual information and the input signal distribution is not just a matter of calculation; it has deep implications for optimizing system design. This problem investigates a key mathematical property: the concavity of mutual information with respect to the input distribution $p(x)$. By analyzing a hypothetical scenario involving mixed transmission strategies in a communication channel, you will numerically verify this property [@problem_id:1654631]. Calculating the \"synergy gain\" provides tangible evidence that the information obtained from a mixed strategy can be greater than the weighted average of the individual strategies, a non-intuitive result that is crucial for finding the channel capacity.", "problem": "A communications engineer is analyzing the performance of a binary communication channel. The channel takes an input from the alphabet $\\mathcal{X} = \\{0, 1\\}$ and produces an output in the alphabet $\\mathcal{Y} = \\{0, 1\\}$. The channel is characterized by the following conditional probabilities, which describe the crossover errors:\n- The probability of receiving a '1' given a '0' was sent is $p(y=1|x=0) = 0.1$.\n- The probability of receiving a '0' given a '1' was sent is $p(y=0|x=1) = 0.3$.\n\nThe engineer is considering two different transmission strategies, defined by their respective input probability distributions:\n1.  **Strategy 1:** The input distribution is $p_1(x)$, with $p_1(x=0) = 0.2$.\n2.  **Strategy 2:** The input distribution is $p_2(x)$, with $p_2(x=0) = 0.8$.\n\nThe engineer then devises a \"mixed\" strategy. This new strategy is formed by randomly choosing to use Strategy 1 with probability $\\lambda=0.5$ or Strategy 2 with probability $1-\\lambda=0.5$. The resulting input distribution is therefore a mixture, $p_{\\text{mix}}(x) = \\lambda p_1(x) + (1-\\lambda) p_2(x)$.\n\nThe performance of each strategy is measured by the mutual information between the input and the output, denoted as $I(p)$, where $p$ is the input distribution. To quantify the benefit of mixing strategies, the engineer defines a quantity called the \"synergy gain,\" $\\Delta I$, as the difference between the mutual information of the mixed strategy and the weighted average of the mutual informations of the individual strategies:\n$$ \\Delta I = I(p_{\\text{mix}}) - \\left[ \\lambda I(p_1) + (1-\\lambda) I(p_2) \\right] $$\nCalculate the value of this synergy gain, $\\Delta I$. Use the natural logarithm (base $e$) for all calculations. Express your final answer in units of nats, rounded to four significant figures.", "solution": "We have a binary-input, binary-output discrete memoryless channel with crossover probabilities:\n$$\np(y=1\\mid x=0)=0.1,\\quad p(y=0\\mid x=1)=0.3,\n$$\nhence\n$$\np(y=0\\mid x=0)=0.9,\\quad p(y=1\\mid x=1)=0.7.\n$$\nLet $q\\equiv p(x=0)$. The output distribution is\n$$\np(y=1)=p(y=1\\mid x=0)q+p(y=1\\mid x=1)(1-q)=0.1q+0.7(1-q)=0.7-0.6q,\n$$\n$$\np(y=0)=1-p(y=1)=0.3+0.6q.\n$$\nUsing $I(X;Y)=H(Y)-H(Y\\mid X)$ with natural logarithms, we write\n$$\nH(Y)=-\\sum_{y\\in\\{0,1\\}}p(y)\\ln p(y)=h(0.7-0.6q),\n$$\n$$\nH(Y\\mid X)=q\\,h(0.1)+(1-q)\\,h(0.3),\n$$\nwhere $h(p)\\equiv -p\\ln p-(1-p)\\ln(1-p)$ is the binary entropy in nats. Thus\n$$\nI(q)=h(0.7-0.6q)-\\big[q\\,h(0.1)+(1-q)\\,h(0.3)\\big].\n$$\nThe two strategies use $q_{1}=0.2$ and $q_{2}=0.8$. The mixed strategy with $\\lambda=0.5$ yields\n$$\nq_{\\text{mix}}=\\lambda q_{1}+(1-\\lambda)q_{2}=0.5.\n$$\n\nFirst compute the conditional entropy terms:\n$$\nh(0.1)=-0.1\\ln 0.1-0.9\\ln 0.9\\approx 0.325082973391448,\n$$\n$$\nh(0.3)=-0.3\\ln 0.3-0.7\\ln 0.7\\approx 0.610864302054893.\n$$\n\nStrategy 1 ($q=0.2$): $p(y=1)=0.7-0.6(0.2)=0.58$, $p(y=0)=0.42$, hence\n$$\nH(Y)=h(0.58)=-0.58\\ln 0.58-0.42\\ln 0.42\\approx 0.6802920001925,\n$$\n$$\nH(Y\\mid X)=0.2\\,h(0.1)+0.8\\,h(0.3)\\approx 0.5537080363222,\n$$\n$$\nI(p_{1})\\approx 0.6802920001925-0.5537080363222=0.1265839638708.\n$$\n\nStrategy 2 ($q=0.8$): $p(y=1)=0.22$, $p(y=0)=0.78$, hence\n$$\nH(Y)=h(0.22)=-0.22\\ln 0.22-0.78\\ln 0.78\\approx 0.5269079614337,\n$$\n$$\nH(Y\\mid X)=0.8\\,h(0.1)+0.2\\,h(0.3)\\approx 0.3822392391241,\n$$\n$$\nI(p_{2})\\approx 0.5269079614337-0.3822392391241=0.1446687223095.\n$$\n\nMixed strategy ($q=0.5$): $p(y=1)=0.4$, $p(y=0)=0.6$, hence\n$$\nH(Y)=h(0.4)=-0.4\\ln 0.4-0.6\\ln 0.6\\approx 0.6730116670093,\n$$\n$$\nH(Y\\mid X)=0.5\\,h(0.1)+0.5\\,h(0.3)\\approx 0.4679736377232,\n$$\n$$\nI(p_{\\text{mix}})\\approx 0.6730116670093-0.4679736377232=0.2050380292861.\n$$\n\nThe synergy gain is\n$$\n\\Delta I=I(p_{\\text{mix}})-\\big[\\lambda I(p_{1})+(1-\\lambda)I(p_{2})\\big]\n=I(p_{\\text{mix}})-\\frac{I(p_{1})+I(p_{2})}{2}.\n$$\nNumerically,\n$$\n\\frac{I(p_{1})+I(p_{2})}{2}\\approx \\frac{0.1265839638708+0.1446687223095}{2}\\approx 0.1356263430902,\n$$\n$$\n\\Delta I\\approx 0.2050380292861-0.1356263430902=0.0694116861959.\n$$\nRounded to four significant figures (in nats), this is $0.06941$.", "answer": "$$\\boxed{0.06941}$$", "id": "1654631"}]}