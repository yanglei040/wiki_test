{"hands_on_practices": [{"introduction": "The best way to understand a new mathematical tool is to apply it directly. This first exercise provides a straightforward application of the log sum inequality to a simple two-element scenario, helping you become comfortable with its components and the lower bound it establishes. By working through this fundamental case, you will build a solid foundation for tackling more complex applications. [@problem_id:1637885]", "problem": "In digital signal processing, a common task is to compare feature vectors extracted from signals. Consider two feature vectors, $\\mathbf{a} = \\langle a_0, a_1 \\rangle$ and $\\mathbf{b} = \\langle b_0, b_1 \\rangle$, where the components are positive real numbers representing metrics associated with two distinct states, '0' and '1', of a binary signal.\n\nA fundamental tool for comparing such quantities is the log sum inequality. It states that for any two sets of positive real numbers, $\\{x_1, x_2, \\dots, x_n\\}$ and $\\{y_1, y_2, \\dots, y_n\\}$, the following relationship holds:\n$$\n\\sum_{i=1}^{n} x_i \\ln \\left( \\frac{x_i}{y_i} \\right) \\ge \\left( \\sum_{i=1}^{n} x_i \\right) \\ln \\left( \\frac{\\sum_{i=1}^{n} x_i}{\\sum_{i=1}^{n} y_i} \\right)\n$$\nHere, $\\ln$ denotes the natural logarithm.\n\nThe expression $\\sum_{i=0}^{1} a_i \\ln \\left(\\frac{a_i}{b_i}\\right)$ can be interpreted as a generalized divergence measure between the feature vectors $\\mathbf{a}$ and $\\mathbf{b}$. According to the log sum inequality, this divergence measure has a guaranteed lower bound that can be expressed in terms of the components of $\\mathbf{a}$ and $\\mathbf{b}$.\n\nWhat is this lower bound? Provide your answer as a single closed-form analytic expression in terms of $a_0, a_1, b_0,$ and $b_1$.", "solution": "The objective is to find the lower bound for the expression $\\sum_{i=0}^{1} a_i \\ln \\left(\\frac{a_i}{b_i}\\right)$ by applying the provided log sum inequality.\n\nThe log sum inequality is given as:\n$$\n\\sum_{i=1}^{n} x_i \\ln \\left( \\frac{x_i}{y_i} \\right) \\ge \\left( \\sum_{i=1}^{n} x_i \\right) \\ln \\left( \\frac{\\sum_{i=1}^{n} x_i}{\\sum_{i=1}^{n} y_i} \\right)\n$$\nThis inequality states that the term on the left-hand side (LHS) is always greater than or equal to the term on the right-hand side (RHS). Therefore, the RHS of the inequality represents the lower bound for the LHS.\n\nIn our specific problem, we are dealing with two-component vectors, so the number of terms in the sum is $n=2$. The indices for summation are given as $i \\in \\{0, 1\\}$. We can match the terms in our problem to the general form of the inequality.\n\nLet the set $\\{x_1, x_2, \\dots, x_n\\}$ correspond to the components of vector $\\mathbf{a}$. Since our indices are 0 and 1, we can set:\n$x_0 = a_0$\n$x_1 = a_1$\n\nSimilarly, let the set $\\{y_1, y_2, \\dots, y_n\\}$ correspond to the components of vector $\\mathbf{b}$:\n$y_0 = b_0$\n$y_1 = b_1$\n\nThe divergence expression given in the problem is $\\sum_{i=0}^{1} a_i \\ln \\left(\\frac{a_i}{b_i}\\right)$. This expression is precisely the LHS of the log sum inequality for our specific sets of numbers.\n$$\n\\text{LHS} = \\sum_{i=0}^{1} a_i \\ln \\left(\\frac{a_i}{b_i}\\right) = a_0 \\ln \\left(\\frac{a_0}{b_0}\\right) + a_1 \\ln \\left(\\frac{a_1}{b_1}\\right)\n$$\n\nTo find the lower bound for this expression, we need to evaluate the RHS of the log sum inequality using our specific values. The general form of the RHS is:\n$$\n\\text{RHS} = \\left( \\sum_{i=0}^{1} x_i \\right) \\ln \\left( \\frac{\\sum_{i=0}^{1} x_i}{\\sum_{i=0}^{1} y_i} \\right)\n$$\n\nFirst, we calculate the sums:\nThe sum of the $x_i$ terms is $\\sum_{i=0}^{1} x_i = x_0 + x_1 = a_0 + a_1$.\nThe sum of the $y_i$ terms is $\\sum_{i=0}^{1} y_i = y_0 + y_1 = b_0 + b_1$.\n\nNow, we substitute these sums into the expression for the RHS:\n$$\n\\text{RHS} = (a_0 + a_1) \\ln \\left( \\frac{a_0 + a_1}{b_0 + b_1} \\right)\n$$\n\nAccording to the log sum inequality, $\\text{LHS} \\ge \\text{RHS}$. Thus, the expression $(a_0 + a_1) \\ln \\left( \\frac{a_0 + a_1}{b_0 + b_1} \\right)$ is the lower bound for the given divergence measure. This is the desired closed-form analytic expression.", "answer": "$$\\boxed{(a_0 + a_1) \\ln\\left(\\frac{a_0 + a_1}{b_0 + b_1}\\right)}$$", "id": "1637885"}, {"introduction": "Beyond being a mathematical curiosity, the log sum inequality is the foundation for key results in information theory. This practice demonstrates its power by proving that the Kullback-Leibler (KL) divergence is non-negative, a cornerstone property known as Gibbs' inequality. This result is what allows KL divergence to be interpreted as a measure of \"distance\" or \"dissimilarity\" between probability distributions. [@problem_id:1637863]", "problem": "Consider a system with three mutually exclusive outcomes, labeled 1, 2, and 3. Let $P = (p_1, p_2, p_3)$ be an arbitrary probability distribution for these outcomes, where $p_i \\ge 0$ for all $i$ and $\\sum_{i=1}^{3} p_i = 1$. Let $U = (u_1, u_2, u_3)$ be the uniform probability distribution over the same outcomes, meaning $u_1 = u_2 = u_3 = 1/3$.\n\nThe Kullback-Leibler (KL) divergence, a measure of how one probability distribution is different from a second, reference probability distribution, is defined for $P$ and $U$ as:\n$$ D_{KL}(P || U) = \\sum_{i=1}^{3} p_i \\log_2\\left(\\frac{p_i}{u_i}\\right) $$\nBy convention, if $p_i = 0$, the corresponding term $0 \\log_2(0/u_i)$ is taken to be 0.\n\nYou are given the log sum inequality, which states that for any two sets of non-negative numbers $\\{a_1, a_2, \\dots, a_n\\}$ and $\\{b_1, b_2, \\dots, b_n\\}$:\n$$ \\sum_{i=1}^n a_i \\log_2 \\left( \\frac{a_i}{b_i} \\right) \\ge \\left( \\sum_{i=1}^n a_i \\right) \\log_2 \\left( \\frac{\\sum_{i=1}^n a_i}{\\sum_{i=1}^n b_i} \\right) $$\nEquality holds if and only if $\\frac{a_i}{b_i} = c$ for some constant $c$ for all $i$ where $a_i > 0$.\n\nUsing this inequality, determine the minimum possible value of $D_{KL}(P || U)$ over all possible probability distributions $P$.", "solution": "We apply the log-sum inequality to the KL divergence. Set $n=3$, $a_{i}=p_{i}$, and $b_{i}=u_{i}$ for $i \\in \\{1,2,3\\}$. Then $a_{i} \\ge 0$, $b_{i} \\ge 0$, and\n$$\n\\sum_{i=1}^{3} a_{i} = \\sum_{i=1}^{3} p_{i} = 1, \\qquad \\sum_{i=1}^{3} b_{i} = \\sum_{i=1}^{3} u_{i} = 1.\n$$\nThe log-sum inequality with base $2$ gives\n$$\n\\sum_{i=1}^{3} p_{i} \\log_{2}\\!\\left(\\frac{p_{i}}{u_{i}}\\right) \\ge \\left(\\sum_{i=1}^{3} p_{i}\\right) \\log_{2}\\!\\left(\\frac{\\sum_{i=1}^{3} p_{i}}{\\sum_{i=1}^{3} u_{i}}\\right) = 1 \\cdot \\log_{2}(1) = 0.\n$$\nBy the definition of KL divergence,\n$$\nD_{KL}(P || U) = \\sum_{i=1}^{3} p_{i} \\log_{2}\\!\\left(\\frac{p_{i}}{u_{i}}\\right) \\ge 0.\n$$\nTo see this lower bound is attainable, evaluate at $P=U$:\n$$\nD_{KL}(U || U) = \\sum_{i=1}^{3} u_{i} \\log_{2}\\!\\left(\\frac{u_{i}}{u_{i}}\\right) = \\sum_{i=1}^{3} u_{i} \\log_{2}(1) = 0.\n$$\nTherefore, the minimum possible value of $D_{KL}(P || U)$ over all admissible $P$ is $0$, achieved at $P=U$.", "answer": "$$\\boxed{0}$$", "id": "1637863"}, {"introduction": "A deep understanding of any inequality requires knowing not just the bound, but also the specific conditions under which that bound is met. This final exercise focuses on the equality condition of the log sum inequality, which occurs if and only if the sequences being compared are proportional. By exploring this case, you will gain a more nuanced appreciation for when the divergence between two sets of values is minimized. [@problem_id:1637900]", "problem": "In information theory, the log sum inequality relates two sequences of positive numbers, $a=(a_1, \\dots, a_n)$ and $b=(b_1, \\dots, b_n)$. A key aspect of this inequality is its condition for equality: the inequality becomes an exact equation if and only if the two sequences are proportional. This means the ratio $\\frac{a_i}{b_i}$ must be a constant for all corresponding pairs of elements $(a_i, b_i)$.\n\nConsider the specific two-element sequences $a=(2, 8)$ and $b=(x, y)$, where $x$ and $y$ are positive real numbers. It is given that these two sequences satisfy the equality condition of the log sum inequality. Furthermore, the product of the terms in the second sequence is constrained such that $xy=32$.\n\nYour task is to find the value of the sum $x+y$.", "solution": "The problem provides two conditions that the positive real numbers $x$ and $y$ must satisfy. We will use these two conditions to form a system of equations and solve for $x$ and $y$.\n\nFirst, we use the condition for equality in the log sum inequality. The problem states that this occurs when the sequences $a=(a_1, a_2)$ and $b=(b_1, b_2)$ are proportional. This means the ratio of corresponding elements is constant:\n$$ \\frac{a_1}{b_1} = \\frac{a_2}{b_2} $$\nWe are given the sequences $a=(2, 8)$ and $b=(x, y)$. Substituting these values into the proportionality condition gives:\n$$ \\frac{2}{x} = \\frac{8}{y} $$\nWe can rearrange this equation to find a relationship between $x$ and $y$. Cross-multiplying yields:\n$$ 2y = 8x $$\nDividing both sides by 2, we get a simpler linear relationship:\n$$ y = 4x $$\n\nSecond, we use the additional constraint given in the problem statement:\n$$ xy = 32 $$\n\nNow we have a system of two equations with two unknowns:\n1. $y = 4x$\n2. $xy = 32$\n\nWe can solve this system by substituting the expression for $y$ from the first equation into the second equation:\n$$ x(4x) = 32 $$\nThis simplifies to:\n$$ 4x^2 = 32 $$\nDivide both sides by 4:\n$$ x^2 = 8 $$\nSince the problem states that $x$ is a positive real number, we take the positive square root of 8:\n$$ x = \\sqrt{8} = \\sqrt{4 \\times 2} = 2\\sqrt{2} $$\n\nNow that we have the value for $x$, we can find the value for $y$ using the relationship $y=4x$:\n$$ y = 4(2\\sqrt{2}) = 8\\sqrt{2} $$\n\nThe problem asks for the value of the sum $x+y$. We add the values we found for $x$ and $y$:\n$$ x+y = 2\\sqrt{2} + 8\\sqrt{2} $$\nCombining the terms, we get:\n$$ x+y = (2+8)\\sqrt{2} = 10\\sqrt{2} $$\nThus, the value of the sum $x+y$ is $10\\sqrt{2}$.", "answer": "$$\\boxed{10\\sqrt{2}}$$", "id": "1637900"}]}