{"hands_on_practices": [{"introduction": "Bayes' rule is fundamentally about updating our beliefs in light of new evidence. This first exercise provides a classic scenario from information theory—the binary erasure channel—where an output is ambiguous rather than definitively wrong. You will apply Bayes' rule in its most direct form to determine the probability of an original input bit, given that the communication channel produced an erasure [@problem_id:1603705].", "problem": "A digital memory device stores data as a sequence of binary bits. Let the source bit, denoted by the random variable $X$, be either $0$ or $1$. From extensive analysis of the data patterns, it is known that the prior probability of a bit being $0$ is $P(X=0) = \\alpha$.\n\nWhen a bit is read from the device, one of three outcomes can occur for the received symbol, denoted by $Y$: the bit is read correctly as $0$, correctly as $1$, or the read operation fails, resulting in an 'erasure' symbol, which we denote as '?'. The device is designed such that it never flips a bit; a stored $0$ is never read as a $1$, and a stored $1$ is never read as a $0$.\n\nThe reliability of the read operation, however, depends on the stored value. The probability of an erasure occurring when the stored bit is $0$ is $P(Y='?'|X=0) = p_0$. The probability of an erasure occurring when the stored bit is $1$ is $P(Y='?'|X=1) = p_1$.\n\nSuppose a single bit is read from the device and the outcome is an erasure, '?'. Determine the posterior probability that the bit originally stored in the device was a $0$. Provide your answer as a single closed-form analytic expression in terms of $\\alpha$, $p_0$, and $p_1$.", "solution": "We are asked to find the posterior probability that the transmitted bit was a $0$ given that the received symbol was an erasure. This can be written as $P(X=0 | Y='?')$.\n\nTo solve this, we apply Bayes' rule, which states:\n$$ P(A|B) = \\frac{P(B|A)P(A)}{P(B)} $$\n\nIn the context of our problem, let event $A$ be $X=0$ (the stored bit is 0) and event $B$ be $Y='?'$ (an erasure is observed). Substituting these into Bayes' rule, we get:\n$$ P(X=0 | Y='?') = \\frac{P(Y='?' | X=0) \\cdot P(X=0)}{P(Y='?')} $$\n\nThe problem provides the following values:\n- $P(X=0) = \\alpha$\n- $P(Y='?' | X=0) = p_0$\n\nThe only term we need to calculate is the total probability of observing an erasure, $P(Y='?')$. We can find this using the law of total probability, summing over all possible inputs for $X$:\n$$ P(Y='?') = P(Y='?' | X=0) \\cdot P(X=0) + P(Y='?' | X=1) \\cdot P(X=1) $$\n\nWe are given $P(X=0) = \\alpha$, which implies that the probability of the stored bit being $1$ is $P(X=1) = 1 - P(X=0) = 1 - \\alpha$. We are also given the conditional probability $P(Y='?' | X=1) = p_1$.\n\nNow, we can substitute all the known probabilities into the expression for $P(Y='?')$:\n$$ P(Y='?') = (p_0) \\cdot (\\alpha) + (p_1) \\cdot (1 - \\alpha) $$\n$$ P(Y='?') = \\alpha p_0 + p_1(1-\\alpha) $$\n\nWith the expression for $P(Y='?')$, we can now complete the calculation for our target posterior probability using Bayes' rule:\n$$ P(X=0 | Y='?') = \\frac{P(Y='?' | X=0) \\cdot P(X=0)}{P(Y='?')} $$\n$$ P(X=0 | Y='?') = \\frac{p_0 \\cdot \\alpha}{\\alpha p_0 + p_1(1-\\alpha)} $$\n\nThis is the final expression for the posterior probability that the stored bit was a $0$ given that an erasure was observed.", "answer": "$$\\boxed{\\frac{\\alpha p_{0}}{\\alpha p_{0} + p_{1}(1-\\alpha)}}$$", "id": "1603705"}, {"introduction": "How does our certainty change as we gather more evidence? This practice explores this crucial question by modeling a system that transmits a signal twice to improve reliability. By applying Bayes' rule to multiple, independent observations, you will see how accumulating data can dramatically increase our confidence in an inference, a principle that underpins many error-correction and signal-processing techniques [@problem_id:1603696].", "problem": "A deep-space probe monitors a critical subsystem. It summarizes the subsystem's status as a single binary digit, $X$, where $X=0$ signifies 'nominal operation' and $X=1$ signifies an 'alert condition'. Based on extensive testing and operational history, the prior probability of the subsystem being in the nominal state is known to be $p_0 = 0.95$.\n\nTo ensure reliable communication over the vast distance to Earth, the probe transmits the status bit $X$ twice in succession over the same communication channel. This channel is subject to noise and can be accurately modeled as a Binary Symmetric Channel (BSC), a channel in which a transmitted bit is flipped (i.e., a 0 is received as a 1, or a 1 is received as a 0) with a constant probability $\\epsilon = 0.10$. Each transmission is an independent event.\n\nAn analyst at mission control on Earth receives the sequence of two bits $(0, 0)$. Given this observation, calculate the updated probability that the probe's subsystem is in the 'nominal operation' state.\n\nProvide your answer as a decimal rounded to four significant figures.", "solution": "Let $X \\in \\{0,1\\}$ denote the subsystem state, with prior $P(X=0)=p_{0}$ and $P(X=1)=1-p_{0}$. Let $Y_{1},Y_{2} \\in \\{0,1\\}$ be the two received bits, transmitted independently through a Binary Symmetric Channel with crossover probability $\\epsilon$. For a BSC, $P(Y_{i}=y \\mid X=x)=1-\\epsilon$ if $y=x$, and $P(Y_{i}=y \\mid X=x)=\\epsilon$ if $y \\neq x$. Conditional independence of transmissions gives $P(Y_{1}=y_{1},Y_{2}=y_{2} \\mid X=x)=\\prod_{i=1}^{2}P(Y_{i}=y_{i} \\mid X=x)$.\n\nGiven the observation $(Y_{1},Y_{2})=(0,0)$, the likelihoods are\n$$\nP(Y_{1}=0,Y_{2}=0 \\mid X=0)=(1-\\epsilon)^{2}, \\quad P(Y_{1}=0,Y_{2}=0 \\mid X=1)=\\epsilon^{2}.\n$$\nBy Bayes’ rule,\n$$\nP(X=0 \\mid Y_{1}=0,Y_{2}=0)=\\frac{p_{0}(1-\\epsilon)^{2}}{p_{0}(1-\\epsilon)^{2}+(1-p_{0})\\epsilon^{2}}.\n$$\nSubstituting $p_{0}=0.95$ and $\\epsilon=0.10$,\n$$\nP(X=0 \\mid Y_{1}=0,Y_{2}=0)=\\frac{0.95 \\times 0.90^{2}}{0.95 \\times 0.90^{2}+0.05 \\times 0.10^{2}}=\\frac{0.7695}{0.7695+0.0005}=\\frac{0.7695}{0.7700}.\n$$\nThus,\n$$\nP(X=0 \\mid Y_{1}=0,Y_{2}=0)=0.999350649\\ldots \\approx 0.9994 \\text{ (to four significant figures)}.\n$$", "answer": "$$\\boxed{0.9994}$$", "id": "1603696"}, {"introduction": "Real-world processes often occur in stages, where the output of one step becomes the input to the next. This exercise challenges you to reason backward through such a multi-stage system, modeled as a Markov chain. You will use Bayes' rule to infer the original state of a signal after it has passed through two consecutive noisy relays, demonstrating how to untangle a sequence of probabilistic events to find the most likely cause [@problem_id:1603695].", "problem": "A digital communication system involves sending a signal through a two-stage relay process. At the origin, a source generates one of three possible signal types, let's call them Type 1, Type 2, and Type 3. The initial signal, represented by the random variable $X \\in \\{1, 2, 3\\}$, is generated with the following prior probabilities:\n$P(X=1) = 0.5$, $P(X=2) = 0.3$, and $P(X=3) = 0.2$.\n\nThe signal then passes through a first noisy relay. The output of this relay is a signal $Y \\in \\{1, 2, 3\\}$. The probabilistic nature of the noise in this stage is captured by the conditional probability matrix $T_{Y|X}$, where the entry $(T_{Y|X})_{ij} = P(Y=i|X=j)$ is the probability of observing signal type $i$ at the first relay given that the original signal was type $j$. The matrix is:\n$$ T_{Y|X} = \\begin{pmatrix} 0.8 & 0.1 & 0.2 \\\\ 0.1 & 0.7 & 0.3 \\\\ 0.1 & 0.2 & 0.5 \\end{pmatrix} $$\n\nThe signal $Y$ is then sent through a second noisy relay, producing the final signal $Z \\in \\{1, 2, 3\\}$. This second transmission step is independent of the original signal $X$ given the intermediate signal $Y$, forming a Markov chain $X \\to Y \\to Z$. The noise characteristics of the second relay are given by the matrix $T_{Z|Y}$, where $(T_{Z|Y})_{ij} = P(Z=i|Y=j)$:\n$$ T_{Z|Y} = \\begin{pmatrix} 0.9 & 0.2 & 0.1 \\\\ 0.05 & 0.6 & 0.2 \\\\ 0.05 & 0.2 & 0.7 \\end{pmatrix} $$\n\nAn engineer at the receiving end observes that the final signal is of Type 3, i.e., $Z=3$. Based on this observation, what is the posterior probability distribution for the original signal type $X$? Calculate the probabilities $P(X=1|Z=3)$, $P(X=2|Z=3)$, and $P(X=3|Z=3)$.\n\nExpress your answer as a row matrix containing the three probabilities in the order $[P(X=1|Z=3), P(X=2|Z=3), P(X=3|Z=3)]$. Round each probability in your final answer to four significant figures.", "solution": "We apply Bayes' theorem with the Markov property $X \\to Y \\to Z$. For $j \\in \\{1,2,3\\}$,\n$$\nP(X=j \\mid Z=3) = \\frac{P(Z=3 \\mid X=j) P(X=j)}{P(Z=3)}.\n$$\nBy the law of total probability and conditional independence,\n$$\nP(Z=3 \\mid X=j) = \\sum_{y=1}^{3} P(Z=3 \\mid Y=y) P(Y=y \\mid X=j).\n$$\nUsing the given matrices, the row for $Z=3$ in $T_{Z \\mid Y}$ is $[0.05, 0.2, 0.7]$, and the columns of $T_{Y \\mid X}$ for $X=1,2,3$ are $[0.8, 0.1, 0.1]^{\\top}$, $[0.1, 0.7, 0.2]^{\\top}$, and $[0.2, 0.3, 0.5]^{\\top}$, respectively. Therefore,\n$$\n\\begin{aligned}\nP(Z=3 \\mid X=1) &= 0.05 \\cdot 0.8 + 0.2 \\cdot 0.1 + 0.7 \\cdot 0.1 = 0.13, \\\\\nP(Z=3 \\mid X=2) &= 0.05 \\cdot 0.1 + 0.2 \\cdot 0.7 + 0.7 \\cdot 0.2 = 0.285, \\\\\nP(Z=3 \\mid X=3) &= 0.05 \\cdot 0.2 + 0.2 \\cdot 0.3 + 0.7 \\cdot 0.5 = 0.42.\n\\end{aligned}\n$$\nThe evidence is\n$$\nP(Z=3) = \\sum_{j=1}^{3} P(Z=3 \\mid X=j) P(X=j) = 0.13 \\cdot 0.5 + 0.285 \\cdot 0.3 + 0.42 \\cdot 0.2 = 0.2345.\n$$\nThus,\n$$\n\\begin{aligned}\nP(X=1 \\mid Z=3) &= \\frac{0.13 \\cdot 0.5}{0.2345} = \\frac{0.065}{0.2345} \\approx 0.2771855, \\\\\nP(X=2 \\mid Z=3) &= \\frac{0.285 \\cdot 0.3}{0.2345} = \\frac{0.0855}{0.2345} \\approx 0.3646055, \\\\\nP(X=3 \\mid Z=3) &= \\frac{0.42 \\cdot 0.2}{0.2345} = \\frac{0.084}{0.2345} \\approx 0.3582089.\n\\end{aligned}\n$$\nRounding each to four significant figures gives\n$$\n\\begin{pmatrix}\n0.2772 & 0.3646 & 0.3582\n\\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}0.2772 & 0.3646 & 0.3582\\end{pmatrix}}$$", "id": "1603695"}]}