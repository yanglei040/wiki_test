{"hands_on_practices": [{"introduction": "The Bernoulli distribution is the fundamental model for a single binary event, such as the successful or failed transmission of a data bit. Calculating its skewness provides a concrete example of how higher-order moments quantify the asymmetry of a distribution, offering insights beyond just its average and variability. This exercise builds foundational skills in applying the definition of skewness to a simple, discrete random variable. [@problem_id:1629524]", "problem": "In the analysis of digital communication systems, a simple model for the transmission of a single data bit over a noisy channel is the Bernoulli trial. Let the random variable $X$ represent the outcome of the transmission, where $X=1$ indicates a successful transmission and $X=0$ indicates a transmission error. The behavior of this random variable is characterized by a single parameter, $p$, which is the probability of a successful transmission, i.e., $P(X=1) = p$ and $P(X=0) = 1-p$.\n\nThe skewness of a distribution is a measure of its asymmetry. For a random variable $X$ with mean $\\mu = E[X]$ and standard deviation $\\sigma = \\sqrt{E[(X-\\mu)^2]}$, the skewness, denoted by $\\gamma_1$, is defined as the third standardized moment:\n$$ \\gamma_1 = \\frac{E[(X-\\mu)^3]}{\\sigma^3} $$\n\nConsider a specific communication channel where the probability of successful bit transmission is found to be $p = 1/4$. Calculate the skewness, $\\gamma_1$, of the random variable $X$ that models this transmission.\n\nReport your answer as a real number rounded to four significant figures.", "solution": "For a Bernoulli random variable $X \\in \\{0,1\\}$ with $P(X=1)=p$ and $P(X=0)=1-p$, the mean and variance are\n$$\n\\mu = E[X] = p, \\qquad \\sigma^{2} = \\operatorname{Var}(X) = p(1-p), \\qquad \\sigma = \\sqrt{p(1-p)}.\n$$\nThe third central moment is\n$$\nE[(X-\\mu)^{3}] = (1-p)^{3}\\cdot p + (-p)^{3}\\cdot (1-p) = p(1-p)^{3} - p^{3}(1-p).\n$$\nFactor $p(1-p)$ to obtain\n$$\nE[(X-\\mu)^{3}] = p(1-p)\\big[(1-p)^{2} - p^{2}\\big] = p(1-p)(1 - 2p).\n$$\nTherefore, the skewness is\n$$\n\\gamma_{1} = \\frac{E[(X-\\mu)^{3}]}{\\sigma^{3}} = \\frac{p(1-p)(1-2p)}{\\big(p(1-p)\\big)^{3/2}} = \\frac{1 - 2p}{\\sqrt{p(1-p)}}.\n$$\nFor $p = \\frac{1}{4}$,\n$$\n\\gamma_{1} = \\frac{1 - 2\\cdot \\frac{1}{4}}{\\sqrt{\\frac{1}{4}\\left(1 - \\frac{1}{4}\\right)}} = \\frac{\\frac{1}{2}}{\\sqrt{\\frac{3}{16}}} = \\frac{\\frac{1}{2}}{\\frac{\\sqrt{3}}{4}} = \\frac{2}{\\sqrt{3}}.\n$$\nAs a decimal rounded to four significant figures,\n$$\n\\gamma_{1} \\approx 1.155\n$$", "answer": "$$\\boxed{1.155}$$", "id": "1629524"}, {"introduction": "In communication systems, random noise is often modeled by a normal distribution, which is perfectly symmetric and has a mean of zero. However, the *power* of the noise, proportional to the square of its amplitude, is a non-negative and skewed quantity. This practice demonstrates how to use the higher-order moments of a standard normal variable to find the variance of its corresponding power, a crucial step in noise analysis. [@problem_id:1629549]", "problem": "In a simplified model of a communication system, the amplitude of a random noise signal at any given instant is described by a standard normal random variable, $Z$. A standard normal random variable has a probability density function $p(z) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{z^2}{2}\\right)$ for $-\\infty  z  \\infty$. The instantaneous power associated with this noise is often related to the square of its amplitude. We define a new random variable, $Y = Z^2$, which represents the normalized instantaneous power of this noise. All quantities are dimensionless.\n\nCalculate the variance of the normalized instantaneous power, $Y$.", "solution": "The problem asks for the variance of the random variable $Y$, defined as $Y = Z^2$, where $Z$ is a standard normal random variable.\n\nThe variance of a random variable $Y$ is given by the formula:\n$$ \\text{Var}(Y) = E[Y^2] - (E[Y])^2 $$\nwhere $E[\\cdot]$ denotes the expectation operator.\n\nFirst, we express the variance of $Y$ in terms of the random variable $Z$. Substituting $Y = Z^2$:\n$$ \\text{Var}(Y) = E[(Z^2)^2] - (E[Z^2])^2 = E[Z^4] - (E[Z^2])^2 $$\nThis shows that we need to compute the second and fourth moments of the standard normal random variable $Z$.\n\nThe $n$-th moment of a continuous random variable $Z$ with probability density function (PDF) $p(z)$ is defined as:\n$$ E[Z^n] = \\int_{-\\infty}^{\\infty} z^n p(z) \\,dz $$\nFor the standard normal distribution, the PDF is $p(z) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{z^2}{2}\\right)$.\n\nLet's calculate the second moment, $E[Z^2]$.\n$$ E[Z^2] = \\int_{-\\infty}^{\\infty} z^2 \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{z^2}{2}\\right) \\,dz $$\nFor a standard normal distribution, the mean $E[Z]$ is 0 and the variance $\\text{Var}(Z)$ is 1. The variance can also be expressed as $\\text{Var}(Z) = E[Z^2] - (E[Z])^2$.\nTherefore, $1 = E[Z^2] - 0^2$, which directly gives $E[Z^2] = 1$.\nSo, the expectation of $Y$ is $E[Y] = E[Z^2] = 1$.\n\nNext, we calculate the fourth moment, $E[Z^4]$.\n$$ E[Z^4] = \\int_{-\\infty}^{\\infty} z^4 \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{z^2}{2}\\right) \\,dz $$\nWe can solve this integral using integration by parts, where $\\int u \\,dv = uv - \\int v \\,du$.\nLet's choose $u = z^3$ and $dv = z \\exp\\left(-\\frac{z^2}{2}\\right) dz$.\nThen $du = 3z^2 dz$ and $v = \\int z \\exp\\left(-\\frac{z^2}{2}\\right) dz = -\\exp\\left(-\\frac{z^2}{2}\\right)$.\n\nThe integral for $E[Z^4]$ becomes:\n$$ E[Z^4] = \\frac{1}{\\sqrt{2\\pi}} \\left[ \\left( z^3 \\right) \\left( -\\exp\\left(-\\frac{z^2}{2}\\right) \\right) \\right]_{-\\infty}^{\\infty} - \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} \\left( -\\exp\\left(-\\frac{z^2}{2}\\right) \\right) (3z^2) \\,dz $$\nThe first term (the boundary term) is evaluated at $\\pm \\infty$. As $z \\to \\pm\\infty$, the exponential term $\\exp(-z^2/2)$ goes to zero much faster than any polynomial term $z^3$ goes to infinity. Thus, the boundary term is 0.\n$$ \\left[ -z^3 \\exp\\left(-\\frac{z^2}{2}\\right) \\right]_{-\\infty}^{\\infty} = 0 - 0 = 0 $$\nSo, we are left with the second term:\n$$ E[Z^4] = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} 3z^2 \\exp\\left(-\\frac{z^2}{2}\\right) \\,dz $$\n$$ E[Z^4] = 3 \\int_{-\\infty}^{\\infty} z^2 \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{z^2}{2}\\right) \\,dz $$\nThe integral in this expression is exactly the definition of the second moment, $E[Z^2]$.\n$$ E[Z^4] = 3 \\cdot E[Z^2] $$\nSince we already found that $E[Z^2] = 1$, we have:\n$$ E[Z^4] = 3 \\cdot 1 = 3 $$\n\nNow we have both moments needed to calculate the variance of $Y$.\n$$ \\text{Var}(Y) = E[Z^4] - (E[Z^2])^2 $$\nSubstituting the values we found:\n$$ \\text{Var}(Y) = 3 - (1)^2 = 3 - 1 = 2 $$\n\nThus, the variance of the normalized instantaneous power $Y$ is 2.", "answer": "$$\\boxed{2}$$", "id": "1629549"}, {"introduction": "A realistic communication channel involves a transmitted signal being corrupted by additive noise. This exercise synthesizes the concepts of signal and noise by modeling the received signal as the sum of two independent random variables. You will learn how to compute the third moment of this combined signal, a skill essential for understanding how channel characteristics and noise properties influence the final received data. [@problem_id:1629531]", "problem": "In the analysis of a digital communication channel, a simplified model is often used to study the effects of noise. Consider such a model where a transmitter sends a signal that can take one of two levels. This transmitted signal is represented by a random variable $S$, which can take a value of $+A$ with probability $p$, or a value of $-A$ with a probability of $1-p$. Here, $A$ is a positive constant representing the signal amplitude.\n\nThe signal travels through a channel that adds noise, which is represented by another random variable $N$. The noise $N$ is statistically independent of the signal $S$. The key characteristics of the noise are:\n1.  It has a mean of zero, so its expected value is $E[N] = 0$.\n2.  Its variance is given as $\\sigma^2$. Since the mean is zero, the second moment is equal to the variance, i.e., $E[N^2] = \\sigma^2$.\n3.  The probability distribution of the noise is symmetric about its mean.\n\nThe received signal at the other end of the channel is the sum of the transmitted signal and the noise, given by the random variable $Y = S + N$.\n\nYour task is to determine the third moment of the received signal, $E[Y^3]$. Express your answer as a closed-form analytic expression in terms of the parameters $A$, $p$, and $\\sigma$.", "solution": "We are given $Y = S + N$, with $S \\in \\{+A,-A\\}$ such that $\\Pr(S=+A)=p$ and $\\Pr(S=-A)=1-p$, and $N$ independent of $S$, with $E[N]=0$, $E[N^{2}]=\\sigma^{2}$, and a distribution symmetric about zero. We are asked to find $E[Y^{3}]$.\n\nBy the binomial expansion,\n$$\nY^{3}=(S+N)^{3}=S^{3}+3S^{2}N+3SN^{2}+N^{3}.\n$$\nTaking expectations and using linearity,\n$$\nE[Y^{3}]=E[S^{3}]+3E[S^{2}N]+3E[SN^{2}]+E[N^{3}].\n$$\nUsing independence of $S$ and $N$, we factor the mixed moments:\n$$\nE[S^{2}N]=E[S^{2}]\\,E[N], \\quad E[SN^{2}]=E[S]\\,E[N^{2}].\n$$\nGiven $E[N]=0$, $E[N^{2}]=\\sigma^{2}$, and symmetry of $N$ about zero implies $E[N^{3}]=0$, we obtain\n$$\nE[Y^{3}]=E[S^{3}]+3E[S]\\sigma^{2}.\n$$\nNow compute the required moments of $S$. Since $S=\\pm A$,\n$$\nE[S]=pA+(1-p)(-A)=A(2p-1),\n$$\n$$\nE[S^{3}]=pA^{3}+(1-p)(-A^{3})=A^{3}(2p-1).\n$$\nSubstituting these into the expression for $E[Y^{3}]$,\n$$\nE[Y^{3}]=A^{3}(2p-1)+3\\sigma^{2}A(2p-1)=(2p-1)A\\left(A^{2}+3\\sigma^{2}\\right).\n$$\nThis is the desired closed-form expression.", "answer": "$$\\boxed{(2p-1)A\\left(A^{2}+3\\sigma^{2}\\right)}$$", "id": "1629531"}]}