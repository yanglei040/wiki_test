{"hands_on_practices": [{"introduction": "Calculating the expectation of a complex event can often be simplified by breaking it down into a sum of simpler events. This practice introduces the powerful technique of using indicator variables, combined with the linearity of expectation, to solve problems that might otherwise seem intractable [@problem_id:1301081]. This method allows us to calculate the expected outcome without needing to know the full probability distribution of the variable, which is a cornerstone of probabilistic analysis.", "problem": "In a televised game show, a contestant is presented with a wall containing $N$ identical, sealed boxes. It is known that exactly $S$ of these boxes contain a special prize, while the remaining $N-S$ boxes are empty. The contestant is allowed to choose and open $k$ distinct boxes, where $1 \\le k < S < N$.\n\nUnbeknownst to the contestant, the show's producers have added a twist. Before the game begins, they secretly select one of the $S$ prize-winning boxes at random and designate it as the \"Booby-Trapped Box\". A prize from a non-Booby-Trapped box is considered a \"safe prize\". The contestant's strategy is to simply choose $k$ boxes at random from the $N$ available boxes.\n\nLet $Y$ be the random variable representing the number of safe prizes the contestant finds among their $k$ chosen boxes. Determine the expectation of $Y$. Express your answer as a single closed-form analytic expression in terms of $N$, $S$, and $k$.", "solution": "Let the producers first select the booby-trapped box uniformly at random among the $S$ prize boxes. After this selection, exactly $S-1$ boxes contain safe prizes and the remaining $N-(S-1)$ boxes are either empty or the single booby-trapped prize.\n\nCondition on the realized set $A$ of safe prize boxes. Then $|A|=S-1$ deterministically. The contestant selects $k$ distinct boxes uniformly without replacement from the $N$ boxes. For each box $i \\in A$, define the indicator variable $I_{i}$ that equals $1$ if box $i$ is chosen and $0$ otherwise. Then\n$$\nY=\\sum_{i \\in A} I_{i}.\n$$\nGiven $A$, each specific box has selection probability $k/N$ under uniform sampling without replacement, so for each $i \\in A$,\n$$\n\\mathbb{E}[I_{i}\\mid A]=\\frac{k}{N}.\n$$\nBy linearity of expectation,\n$$\n\\mathbb{E}[Y\\mid A]=\\sum_{i \\in A}\\mathbb{E}[I_{i}\\mid A]=(S-1)\\frac{k}{N}.\n$$\nThis conditional expectation depends only on $|A|=S-1$, not on the particular identity of $A$. Applying the law of total expectation,\n$$\n\\mathbb{E}[Y]=\\mathbb{E}\\big[\\mathbb{E}[Y\\mid A]\\big]=(S-1)\\frac{k}{N}.\n$$\nEquivalently, $Y$ is hypergeometric with population size $N$, number of successes $S-1$, and draws $k$, whose mean is $k(S-1)/N$.", "answer": "$$\\boxed{\\frac{k\\left(S-1\\right)}{N}}$$", "id": "1301081"}, {"introduction": "Moving from discrete to continuous random variables, the fundamental idea of expectation as a weighted average remains, but our tool shifts from summation to integration. This problem provides a hands-on application in the context of geometric probability, requiring you to first determine the probability distribution of a variable of interest before applying the definition of expectation [@problem_id:1301055]. Mastering this process is essential for analyzing variables that can take on a continuum of values, such as physical measurements.", "problem": "A particle is deposited onto a circular silicon wafer of radius $R$. The position of the particle on the wafer is random, with its location following a uniform probability distribution over the entire area of the circular wafer. Let the random variable $D$ represent the distance of the particle from the center of the wafer. Determine the expected value of $D$. Express your answer as an analytic expression in terms of $R$.", "solution": "Let the wafer be a disk of radius $R$ in the plane, and let the particleâ€™s position be uniformly distributed over its area. The uniformity over area implies the joint probability density over the disk is constant and equal to $1/(\\pi R^{2})$ on $\\{(x,y): x^{2}+y^{2} \\leq R^{2}\\}$ and zero otherwise.\n\nDefine $D$ as the distance from the center, so $D = \\sqrt{X^{2}+Y^{2}}$. The cumulative distribution function of $D$ for $0 \\leq d \\leq R$ is the probability that the point lies within a concentric disk of radius $d$, which equals the ratio of areas:\n$$\nF_{D}(d) = \\mathbb{P}(D \\leq d) = \\frac{\\pi d^{2}}{\\pi R^{2}} = \\frac{d^{2}}{R^{2}}, \\quad 0 \\leq d \\leq R.\n$$\nDifferentiating gives the probability density function of $D$:\n$$\nf_{D}(d) = \\frac{\\mathrm{d}}{\\mathrm{d}d}F_{D}(d) = \\frac{2d}{R^{2}}, \\quad 0 \\leq d \\leq R.\n$$\nThe expected value is computed by the definition of expectation for a continuous random variable:\n$$\n\\mathbb{E}[D] = \\int_{0}^{R} d \\, f_{D}(d) \\, \\mathrm{d}d = \\int_{0}^{R} d \\cdot \\frac{2d}{R^{2}} \\, \\mathrm{d}d = \\frac{2}{R^{2}} \\int_{0}^{R} d^{2} \\, \\mathrm{d}d.\n$$\nEvaluating the integral,\n$$\n\\int_{0}^{R} d^{2} \\, \\mathrm{d}d = \\left. \\frac{d^{3}}{3} \\right|_{0}^{R} = \\frac{R^{3}}{3},\n$$\nso\n$$\n\\mathbb{E}[D] = \\frac{2}{R^{2}} \\cdot \\frac{R^{3}}{3} = \\frac{2R}{3}.\n$$\nTherefore, the expected distance from the center is $\\frac{2R}{3}$.", "answer": "$$\\boxed{\\frac{2R}{3}}$$", "id": "1301055"}, {"introduction": "This final practice synthesizes our understanding of expectation with another key statistical measure: variance. In many applications, particularly in signal processing and information theory, we are interested in properties derived from combinations of random variables [@problem_id:1622947]. This exercise demonstrates how to use the properties of linearity, independence, and the relationship between expectation and variance to analyze a signal's \"roughness,\" a concept crucial for tasks like data compression.", "problem": "In the design of a digital communication system, a signal is modeled as a sequence of measurements, $\\{X_i\\}_{i=1}^{\\infty}$. These measurements are treated as independent and identically distributed (i.i.d.) random variables. Each random variable $X_i$ in the sequence has a mean $E[X_i] = \\mu$ and a variance $\\text{Var}(X_i) = \\sigma^2$.\n\nA common technique to analyze the high-frequency content or \"roughness\" of such a signal is to examine the statistical properties of the difference between successive samples. This analysis is fundamental in areas like data compression and predictive coding.\n\nDetermine the expected value of the squared difference between any two adjacent samples, $E[(X_{i+1} - X_i)^2]$. Express your answer as a closed-form expression in terms of $\\mu$ and $\\sigma^2$.", "solution": "We seek $E[(X_{i+1} - X_{i})^{2}]$ for i.i.d. random variables $\\{X_{i}\\}$ with $E[X_{i}] = \\mu$ and $\\text{Var}(X_{i}) = \\sigma^{2}$. Expand the square and take expectation:\n$$\nE[(X_{i+1} - X_{i})^{2}] = E[X_{i+1}^{2}] + E[X_{i}^{2}] - 2E[X_{i+1}X_{i}].\n$$\nUse the identity $E[X_{i}^{2}] = \\text{Var}(X_{i}) + (E[X_{i}])^{2} = \\sigma^{2} + \\mu^{2}$, which also holds for $X_{i+1}$ since the variables are identically distributed:\n$$\nE[X_{i+1}^{2}] = \\sigma^{2} + \\mu^{2}, \\quad E[X_{i}^{2}] = \\sigma^{2} + \\mu^{2}.\n$$\nBecause $X_{i+1}$ and $X_{i}$ are independent, $E[X_{i+1}X_{i}] = E[X_{i+1}]E[X_{i}] = \\mu^{2}$. Substitute these into the expansion:\n$$\nE[(X_{i+1} - X_{i})^{2}] = (\\sigma^{2} + \\mu^{2}) + (\\sigma^{2} + \\mu^{2}) - 2\\mu^{2} = 2\\sigma^{2}.\n$$\nAlternatively, note that $E[X_{i+1} - X_{i}] = \\mu - \\mu = 0$ and $\\text{Var}(X_{i+1} - X_{i}) = \\text{Var}(X_{i+1}) + \\text{Var}(X_{i}) - 2\\text{Cov}(X_{i+1}, X_{i}) = \\sigma^{2} + \\sigma^{2} - 0 = 2\\sigma^{2}$ by independence, and thus $E[(X_{i+1} - X_{i})^{2}] = \\text{Var}(X_{i+1} - X_{i}) + (E[X_{i+1} - X_{i}])^{2} = 2\\sigma^{2}$.", "answer": "$$\\boxed{2\\sigma^{2}}$$", "id": "1622947"}]}