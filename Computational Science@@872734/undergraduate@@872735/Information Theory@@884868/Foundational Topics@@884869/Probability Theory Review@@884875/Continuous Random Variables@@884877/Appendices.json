{"hands_on_practices": [{"introduction": "A common task in both signal processing and system modeling is to understand the uncertainty that arises when independent processes are combined. This introductory exercise provides practice with a fundamental skill: calculating the differential entropy of a sum of two random variables. The process involves first determining the probability density function of the sum via convolution, and then applying the definition of differential entropy to quantify the total uncertainty [@problem_id:1617737].", "problem": "Consider two independent and identically distributed continuous random variables, $X$ and $Y$. Both variables follow a uniform distribution on the interval $[0, L]$, where $L=e$, the base of the natural logarithm. A new random variable, $Z$, is formed by their sum, $Z = X+Y$.\n\nCalculate the differential entropy, $h(Z)$, of the random variable $Z$. Provide the exact numerical value for your answer.", "solution": "Let $X$ and $Y$ be independent and identically distributed with $X,Y \\sim \\mathrm{Unif}(0,L)$. The sum $Z=X+Y$ has the triangular density given by the convolution of two uniforms:\n$$\nf_{Z}(z)=\\begin{cases}\n\\frac{z}{L^{2}},  0 \\leq z \\leq L,\\\\\n\\frac{2L - z}{L^{2}},  L \\leq z \\leq 2L,\\\\\n0,  \\text{otherwise}.\n\\end{cases}\n$$\nThe differential entropy is\n$$\nh(Z)=-\\int_{0}^{L}\\frac{z}{L^{2}}\\ln\\!\\left(\\frac{z}{L^{2}}\\right)\\,dz-\\int_{L}^{2L}\\frac{2L-z}{L^{2}}\\ln\\!\\left(\\frac{2L-z}{L^{2}}\\right)\\,dz.\n$$\nBy the substitution $u=2L-z$ in the second integral, both integrals are equal. Defining\n$$\nI=\\int_{0}^{L}\\frac{z}{L^{2}}\\ln\\!\\left(\\frac{z}{L^{2}}\\right)\\,dz,\n$$\nwe have $h(Z)=-2I$. Compute $I$ via $z=Lt$, $dz=L\\,dt$:\n$$\nI=\\int_{0}^{1}t\\left[\\ln t-\\ln L\\right]\\,dt=\\int_{0}^{1}t\\ln t\\,dt-(\\ln L)\\int_{0}^{1}t\\,dt.\n$$\nUsing $\\int_{0}^{1}t\\,dt=\\frac{1}{2}$ and $\\int_{0}^{1}t\\ln t\\,dt=-\\frac{1}{4}$, we obtain\n$$\nI=-\\frac{1}{4}-\\frac{1}{2}\\ln L,\n$$\nhence\n$$\nh(Z)=-2I=\\frac{1}{2}+\\ln L.\n$$\nWith $L=\\exp(1)$, we have $\\ln L=\\ln(\\exp(1))=1$, so\n$$\nh(Z)=\\frac{1}{2}+1=\\frac{3}{2}.\n$$", "answer": "$$\\boxed{\\frac{3}{2}}$$", "id": "1617737"}, {"introduction": "In engineering and science, we often use mathematical models to approximate reality. But how do our modeling choices affect our conclusions about uncertainty? This practice delves into the principle of maximum entropy by comparing the ubiquitous Gaussian distribution with a plausible alternative, the Laplace distribution [@problem_id:1613629]. By constraining both noise models to have the same variance, or power, we can directly compare their inherent randomness and understand why the Gaussian distribution holds a special place in information theory. This exercise reveals that for a fixed variance, Gaussian noise represents the maximum possible uncertainty, a crucial insight for designing robust systems.", "problem": "A systems engineer is designing a communication channel and models the additive noise in the system. The engineer makes a standard assumption that the noise, denoted by the random variable $N_A$, follows a Gaussian distribution with a mean of zero and a variance of $\\sigma^2$.\n\nHowever, subsequent high-precision measurements reveal that the true noise, denoted by the random variable $N_T$, is not Gaussian. Instead, it is better described by a Laplace distribution, which also has a mean of zero. Crucially, the variance of this true Laplace noise is measured to be identical to the variance $\\sigma^2$ of the engineer's assumed Gaussian model.\n\nFor your reference:\n- The Probability Density Function (PDF) of a Laplace distribution with mean $\\mu$ and scale parameter $b  0$ is given by $f(x) = \\frac{1}{2b} \\exp\\left(-\\frac{|x-\\mu|}{b}\\right)$. The variance of this distribution is $2b^2$.\n- The differential entropy of a zero-mean Gaussian random variable with variance $\\sigma^2$ is given by $h_{Gauss} = \\frac{1}{2}\\ln(2\\pi e \\sigma^2)$.\n- The differential entropy of a zero-mean Laplace random variable with scale parameter $b$ is given by $h_{Laplace} = \\ln(2be)$.\n\nCalculate the difference $\\Delta h = h_{true} - h_{assumed}$, where $h_{true}$ is the differential entropy of the true noise $N_T$ and $h_{assumed}$ is the differential entropy calculated by the engineer for the assumed noise $N_A$. Express your answer as a closed-form analytic expression.", "solution": "The assumed noise $N_{A}$ is Gaussian with mean $0$ and variance $\\sigma^{2}$, so its differential entropy is $h_{assumed}=\\frac{1}{2}\\ln(2\\pi e \\sigma^{2})$. The true noise $N_{T}$ is Laplace with mean $0$ and variance equal to $\\sigma^{2}$. For a Laplace distribution with scale $b$, the variance is $2b^{2}$, so matching the variances gives\n$$\n2b^{2}=\\sigma^{2}\\quad\\Rightarrow\\quad b=\\frac{\\sigma}{\\sqrt{2}}.\n$$\nThe differential entropy of a zero-mean Laplace random variable with scale $b$ is $h_{true}=\\ln(2be)$. Substituting $b=\\sigma/\\sqrt{2}$ yields\n$$\nh_{true}=\\ln\\!\\left(2e\\cdot\\frac{\\sigma}{\\sqrt{2}}\\right)=\\ln\\!\\left(e\\sigma\\sqrt{2}\\right)=\\ln(e)+\\ln(\\sigma)+\\frac{1}{2}\\ln(2).\n$$\nFor the Gaussian model,\n$$\nh_{assumed}=\\frac{1}{2}\\ln(2\\pi e\\sigma^{2})=\\frac{1}{2}\\left[\\ln(2)+\\ln(\\pi)+\\ln(e)+\\ln(\\sigma^{2})\\right]\n=\\frac{1}{2}\\ln(2)+\\frac{1}{2}\\ln(\\pi)+\\frac{1}{2}+ \\ln(\\sigma),\n$$\nwhere we used $\\ln(e)=1$ and $\\ln(\\sigma^{2})=2\\ln(\\sigma)$. Therefore, the difference is\n$$\n\\Delta h=h_{true}-h_{assumed}=\\left[1+\\ln(\\sigma)+\\frac{1}{2}\\ln(2)\\right]-\\left[\\frac{1}{2}\\ln(2)+\\frac{1}{2}\\ln(\\pi)+\\frac{1}{2}+\\ln(\\sigma)\\right]\n=\\frac{1}{2}-\\frac{1}{2}\\ln(\\pi).\n$$\nThus,\n$$\n\\Delta h=\\frac{1-\\ln(\\pi)}{2}.\n$$", "answer": "$$\\boxed{\\frac{1-\\ln(\\pi)}{2}}$$", "id": "1613629"}, {"introduction": "Beyond calculating the uncertainty of a single variable, information theory gives us tools to measure the relationship between variables. In communications, a signal's Cartesian coordinates $(X, Y)$ are often converted to polar coordinates $(R, \\Theta)$, and it is not immediately obvious if this transformation creates a statistical dependence between the magnitude and phase. This problem introduces mutual information, a key metric for dependence, and uses the skill of transforming random variable densities to find an answer [@problem_id:1613667]. The result is a cornerstone of communication theory, demonstrating a profound property of Gaussian noise that greatly simplifies the analysis of advanced modulation schemes.", "problem": "In digital communications, a signal is often represented by its in-phase ($I$) and quadrature ($Q$) components. Consider a simplified model where, due to noise, the measured values of these components at a particular instant are random variables. Let these components be denoted by $X$ and $Y$. We model $X$ and $Y$ as two independent random variables, each following a standard normal distribution (mean of 0 and variance of 1).\n\nThe system electronics then convert this Cartesian representation $(X, Y)$ into a polar representation consisting of a magnitude $R$ and a phase $\\Theta$. The magnitude is given by $R = \\sqrt{X^2 + Y^2}$, and the phase is given by $\\Theta = \\text{atan2}(Y, X)$, where the angle $\\Theta$ is defined in the range $[0, 2\\pi)$ radians.\n\nCalculate the mutual information $I(R; \\Theta)$ between the magnitude $R$ and the phase $\\Theta$. The mutual information quantifies the statistical dependence between two random variables, or equivalently, the amount of information one variable contains about the other. Express your answer in nats.", "solution": "We begin with two independent standard normal random variables $X$ and $Y$ with joint density\n$$\np_{X,Y}(x,y)=\\frac{1}{2\\pi}\\exp\\!\\left(-\\frac{x^{2}+y^{2}}{2}\\right),\n$$\nby the product rule for independent densities and the standard normal density formula.\n\nDefine the transformation to polar coordinates by\n$$\nR=\\sqrt{X^{2}+Y^{2}},\\qquad \\Theta=\\arctan\\!\\left(\\frac{Y}{X}\\right)\\ \\text{with quadrant correction so that}\\ \\Theta\\in[0,2\\pi),\n$$\nequivalently the inverse mapping\n$$\nX=R\\cos\\Theta,\\qquad Y=R\\sin\\Theta,\n$$\nwith domain $r\\in[0,\\infty)$ and $\\theta\\in[0,2\\pi)$. The Jacobian determinant of the inverse transformation $(r,\\theta)\\mapsto(x,y)$ is\n$$\nJ=\\det\\begin{pmatrix}\n\\frac{\\partial x}{\\partial r}  \\frac{\\partial x}{\\partial \\theta} \\\\\n\\frac{\\partial y}{\\partial r}  \\frac{\\partial y}{\\partial \\theta}\n\\end{pmatrix}\n=\\det\\begin{pmatrix}\n\\cos\\theta  -r\\sin\\theta \\\\\n\\sin\\theta  r\\cos\\theta\n\\end{pmatrix}\n=r(\\cos^{2}\\theta+\\sin^{2}\\theta)=r,\n$$\nso $|J|=r$. By the change-of-variables formula for densities,\n$$\np_{R,\\Theta}(r,\\theta)=p_{X,Y}(r\\cos\\theta,\\,r\\sin\\theta)\\,|J|\n=\\frac{1}{2\\pi}\\exp\\!\\left(-\\frac{r^{2}}{2}\\right)\\,r,\n$$\nfor $r\\geq 0$ and $\\theta\\in[0,2\\pi)$.\n\nWe compute the marginals. First,\n$$\np_{R}(r)=\\int_{0}^{2\\pi}p_{R,\\Theta}(r,\\theta)\\,d\\theta\n=\\int_{0}^{2\\pi}\\frac{1}{2\\pi}\\exp\\!\\left(-\\frac{r^{2}}{2}\\right)\\,r\\,d\\theta\n=r\\,\\exp\\!\\left(-\\frac{r^{2}}{2}\\right),\n$$\nfor $r\\geq 0$. Second,\n$$\np_{\\Theta}(\\theta)=\\int_{0}^{\\infty}p_{R,\\Theta}(r,\\theta)\\,dr\n=\\int_{0}^{\\infty}\\frac{1}{2\\pi}\\exp\\!\\left(-\\frac{r^{2}}{2}\\right)\\,r\\,dr\n=\\frac{1}{2\\pi}\\int_{0}^{\\infty}\\exp\\!\\left(-\\frac{r^{2}}{2}\\right)\\,r\\,dr.\n$$\nWith the substitution $u=\\frac{r^{2}}{2}$, $du=r\\,dr$, this becomes\n$$\np_{\\Theta}(\\theta)=\\frac{1}{2\\pi}\\int_{0}^{\\infty}\\exp(-u)\\,du=\\frac{1}{2\\pi},\n$$\nfor $\\theta\\in[0,2\\pi)$, showing $\\Theta$ is uniform on $[0,2\\pi)$.\n\nNow observe the factorization\n$$\np_{R,\\Theta}(r,\\theta)=\\frac{1}{2\\pi}\\exp\\!\\left(-\\frac{r^{2}}{2}\\right)\\,r\n=\\bigg[r\\,\\exp\\!\\left(-\\frac{r^{2}}{2}\\right)\\bigg]\\cdot\\bigg[\\frac{1}{2\\pi}\\bigg]\n=p_{R}(r)\\,p_{\\Theta}(\\theta).\n$$\nTherefore $R$ and $\\Theta$ are independent. By the definition of mutual information,\n$$\nI(R;\\Theta)=\\iint p_{R,\\Theta}(r,\\theta)\\,\\ln\\!\\left(\\frac{p_{R,\\Theta}(r,\\theta)}{p_{R}(r)\\,p_{\\Theta}(\\theta)}\\right)\\,dr\\,d\\theta,\n$$\nand since the ratio inside the logarithm is identically $1$, the integrand is identically $0$, giving\n$$\nI(R;\\Theta)=0\\ \\text{nats}.\n$$\nThe measure-zero singularity at $r=0$ does not affect the densities or the mutual information.", "answer": "$$\\boxed{0}$$", "id": "1613667"}]}