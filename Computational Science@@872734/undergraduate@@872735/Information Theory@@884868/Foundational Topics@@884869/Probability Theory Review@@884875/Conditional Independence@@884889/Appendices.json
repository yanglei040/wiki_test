{"hands_on_practices": [{"introduction": "This first exercise explores one of the most fundamental structures giving rise to conditional independence: the common cause. In this scenario [@problem_id:1612649], a coin flip acts as a common cause ($Z$) that determines which of two subsequent events occurs. By calculating the conditional mutual information, you will formally verify the intuition that once the outcome of the cause is known, its effects are no longer correlated.", "problem": "In a game of chance, the choice of which die to roll is determined by a single flip of a fair coin. Let the random variable $Z$ represent the outcome of the coin flip, where $Z=1$ for heads and $Z=0$ for tails. The probability of heads is $P(Z=1) = 0.5$, and the probability of tails is $P(Z=0) = 0.5$.\n\nIf the coin shows heads ($Z=1$), a fair six-sided red die is rolled. The outcome of this die is represented by the random variable $X$, which can take any integer value from 1 to 6 with equal probability.\nIf the coin shows tails ($Z=0$), a fair four-sided blue die is rolled. The outcome of this die is represented by the random variable $Y$, which can take any integer value from 1 to 4 with equal probability.\n\nTo ensure that the random variables $X$ and $Y$ are well-defined for every outcome of the coin flip, we adopt the following convention: the die that is not rolled is considered to have an outcome of 0. That is:\n- If $Z=1$, an outcome for $X \\in \\{1, 2, 3, 4, 5, 6\\}$ is observed, and $Y$ is assigned the value 0.\n- If $Z=0$, an outcome for $Y \\in \\{1, 2, 3, 4\\}$ is observed, and $X$ is assigned the value 0.\n\nBased on this setup, calculate the conditional mutual information between the outcomes of the red and blue dice, given the outcome of the coin flip, denoted as $I(X;Y|Z)$. Express your answer in bits, which implies using the logarithm of base 2 for all entropy calculations.", "solution": "We use the definition of conditional mutual information with logarithms base 2:\n$$\nI(X;Y|Z) \\equiv \\sum_{z} P(Z=z)\\, I(X;Y \\mid Z=z).\n$$\nIt suffices to compute $I(X;Y \\mid Z=1)$ and $I(X;Y \\mid Z=0)$ and then take the average with weights $P(Z=1)=\\frac{1}{2}$ and $P(Z=0)=\\frac{1}{2}$.\n\nFor $Z=1$:\n- The conditional distributions are $P(Y=0 \\mid Z=1)=1$ (so $Y$ is deterministic) and $P(X=i \\mid Z=1)=\\frac{1}{6}$ for $i \\in \\{1,2,3,4,5,6\\}$.\n- Using $I(X;Y \\mid Z=1)=H(Y \\mid Z=1)-H(Y \\mid X,Z=1)$, we have $H(Y \\mid Z=1)=0$ because $Y$ is constant given $Z=1$, and $H(Y \\mid X,Z=1)=0$ for the same reason. Therefore,\n$$\nI(X;Y \\mid Z=1)=0-0=0.\n$$\n\nFor $Z=0$:\n- The conditional distributions are $P(X=0 \\mid Z=0)=1$ (so $X$ is deterministic) and $P(Y=j \\mid Z=0)=\\frac{1}{4}$ for $j \\in \\{1,2,3,4\\}$.\n- Using $I(X;Y \\mid Z=0)=H(X \\mid Z=0)-H(X \\mid Y,Z=0)$, we have $H(X \\mid Z=0)=0$ and $H(X \\mid Y,Z=0)=0$. Therefore,\n$$\nI(X;Y \\mid Z=0)=0-0=0.\n$$\n\nAveraging over $Z$,\n$$\nI(X;Y \\mid Z)=\\sum_{z \\in \\{0,1\\}} P(Z=z)\\, I(X;Y \\mid Z=z)=\\tfrac{1}{2}\\cdot 0+\\tfrac{1}{2}\\cdot 0=0.\n$$\nThus the conditional mutual information in bits is zero.", "answer": "$$\\boxed{0}$$", "id": "1612649"}, {"introduction": "In contrast to the common cause scenario, this practice investigates the 'collider' or 'explaining away' effect, where conditioning can induce dependence between previously independent variables. Here [@problem_id:1612630], two independent data streams are combined via an XOR operation. You will demonstrate the counter-intuitive result that while the streams are initially independent, observing their combined output makes them informationally linked.", "problem": "In many modern data systems, information is processed in parallel streams. Consider a simplified model of such a system with two independent binary data streams, represented by random variables $X$ and $Y$. Both streams are designed to be statistically unbiased, meaning the probability of a bit being 0 or 1 is equal. Thus, $P(X=0) = P(X=1) = 1/2$, and $P(Y=0) = P(Y=1) = 1/2$. A monitoring process does not observe $X$ and $Y$ directly but instead observes a third binary variable $Z$, which is generated by taking the exclusive OR (XOR) of the corresponding bits from the two streams: $Z = X \\oplus Y$.\n\nAn engineer is analyzing the information flow in this system. A key question is how much information the two initial streams share, conditioned on the observed output. Specifically, calculate the conditional mutual information $I(X;Y \\mid Z)$ between the two streams $X$ and $Y$, given the observation of the combined stream $Z$.\n\nExpress your answer in bits.", "solution": "We are given two independent unbiased binary random variables $X$ and $Y$ with $P(X=0)=P(X=1)=\\frac{1}{2}$ and $P(Y=0)=P(Y=1)=\\frac{1}{2}$, and $Z=X\\oplus Y$. We are to compute the conditional mutual information $I(X;Y\\mid Z)$ in bits.\n\nBy definition,\n$$\nI(X;Y\\mid Z)=H(X\\mid Z)-H(X\\mid Y,Z).\n$$\nFirst compute $H(X\\mid Z)$. We have\n$$\nP(Z=0)=P(X=0,Y=0)+P(X=1,Y=1)=\\frac{1}{4}+\\frac{1}{4}=\\frac{1}{2},\n$$\n$$\nP(Z=1)=P(X=0,Y=1)+P(X=1,Y=0)=\\frac{1}{4}+\\frac{1}{4}=\\frac{1}{2}.\n$$\nFor $z=0$,\n$$\nP(X=0\\mid Z=0)=\\frac{P(X=0,Z=0)}{P(Z=0)}=\\frac{P(X=0,Y=0)}{\\frac{1}{2}}=\\frac{\\frac{1}{4}}{\\frac{1}{2}}=\\frac{1}{2},\n$$\nand similarly $P(X=1\\mid Z=0)=\\frac{1}{2}$. Thus $H(X\\mid Z=0)=1$ bit.\n\nFor $z=1$,\n$$\nP(X=0\\mid Z=1)=\\frac{P(X=0,Z=1)}{P(Z=1)}=\\frac{P(X=0,Y=1)}{\\frac{1}{2}}=\\frac{\\frac{1}{4}}{\\frac{1}{2}}=\\frac{1}{2},\n$$\nand similarly $P(X=1\\mid Z=1)=\\frac{1}{2}$. Thus $H(X\\mid Z=1)=1$ bit.\n\nTherefore,\n$$\nH(X\\mid Z)=\\sum_{z\\in\\{0,1\\}}P(Z=z)\\,H(X\\mid Z=z)=\\frac{1}{2}\\cdot 1+\\frac{1}{2}\\cdot 1=1.\n$$\n\nNext compute $H(X\\mid Y,Z)$. Since $Z=X\\oplus Y$, the value of $X$ is determined by $Y$ and $Z$ via $X=Y\\oplus Z$. Hence\n$$\nH(X\\mid Y,Z)=0.\n$$\n\nPutting these together,\n$$\nI(X;Y\\mid Z)=H(X\\mid Z)-H(X\\mid Y,Z)=1-0=1.\n$$\nThus the conditional mutual information is $1$ bit.", "answer": "$$\\boxed{1}$$", "id": "1612630"}, {"introduction": "This final practice applies the concept of conditional independence to a classic signal processing problem involving noisy channels. We model a scenario [@problem_id:1612658] where two receivers get a noisy version of the same transmitted signal. This exercise illustrates how the observations are dependent due to their shared origin, but become independent once we condition on the original source signal, allowing us to disentangle the true signal from the channel noise.", "problem": "Consider a simple communication system where a source's state $Z$ can be either 'LOW' (represented by the binary value 0) or 'HIGH' (represented by 1). The source selects a state with equal probability, i.e., $p(Z=0) = p(Z=1) = 0.5$. The state $Z$ is transmitted simultaneously to two physically separate and independent receivers, A and B.\n\nThe transmission channels to both receivers are noisy. The observation at receiver A is denoted by the binary variable $X$, and at receiver B by $Y$. For each receiver, there is a constant probability of a bit-flip error, $\\alpha = 1/4$. That is, the probability that a receiver records a state different from the one transmitted is $p(X \\neq Z) = \\alpha$ and $p(Y \\neq Z) = \\alpha$. The errors in the two channels are independent of each other, conditioned on the source state $Z$.\n\nCalculate the mutual information $I(X;Y)$ between the observations of the two receivers. Express your final answer as a closed-form analytic expression in bits.", "solution": "Let $Z \\in \\{0,1\\}$ with $p(Z=0)=p(Z=1)=\\frac{1}{2}$. The observations $X$ and $Y$ are produced by independent binary symmetric channels (BSC) with crossover probability $\\alpha=\\frac{1}{4}$, so $p(X \\neq Z \\mid Z)=\\alpha$ and $p(Y \\neq Z \\mid Z)=\\alpha$, and $X$ and $Y$ are conditionally independent given $Z$.\n\nFirst, compute the marginals of $X$ and $Y$. For $X$,\n$$\np(X=1) = \\sum_{z \\in \\{0,1\\}} p(Z=z)\\,p(X=1 \\mid Z=z) = \\frac{1}{2}\\big[(1-\\alpha)+\\alpha\\big] = \\frac{1}{2},\n$$\nand similarly $p(X=0)=\\frac{1}{2}$, so $H(X)=1$ bit; likewise $H(Y)=1$ bit.\n\nNext, compute the joint distribution of $(X,Y)$. The probability that the two observations are equal, $p(X=Y)$, is found by summing over the cases where both channels are correct or both are incorrect, giving:\n$$\np(X=Y) = (1-\\alpha)^{2} + \\alpha^{2}\n$$\nThe probability of them being different is:\n$$\np(X \\neq Y) = 2\\alpha(1-\\alpha).\n$$\nDefine $r \\equiv p(X \\neq Y) = 2\\alpha(1-\\alpha)$ and $q \\equiv p(X=Y) = 1-r = (1-\\alpha)^{2} + \\alpha^{2}$. By symmetry, the four joint probabilities are\n$$\np(0,0) = p(1,1) = \\frac{q}{2}, \\quad p(0,1) = p(1,0) = \\frac{r}{2}.\n$$\n\nThe joint entropy is\n$$\nH(X,Y) = -2 \\cdot \\frac{q}{2} \\log_{2}\\!\\left(\\frac{q}{2}\\right) - 2 \\cdot \\frac{r}{2} \\log_{2}\\!\\left(\\frac{r}{2}\\right)\n= - q \\log_{2}\\!\\left(\\frac{q}{2}\\right) - r \\log_{2}\\!\\left(\\frac{r}{2}\\right).\n$$\nUsing $\\log_{2}\\!\\left(\\frac{q}{2}\\right) = \\log_{2} q - 1$ and $\\log_{2}\\!\\left(\\frac{r}{2}\\right) = \\log_{2} r - 1$, and $q+r=1$, this simplifies to\n$$\nH(X,Y) = 1 - \\big[q \\log_{2} q + r \\log_{2} r\\big].\n$$\nTherefore, the mutual information is\n$$\nI(X;Y) = H(X) + H(Y) - H(X,Y) = 2 - \\Big[1 - \\big(q \\log_{2} q + r \\log_{2} r\\big)\\Big]\n= 1 + q \\log_{2} q + r \\log_{2} r.\n$$\n\nSubstitute $\\alpha=\\frac{1}{4}$. Then\n$$\nr = 2\\alpha(1-\\alpha) = 2 \\cdot \\frac{1}{4} \\cdot \\frac{3}{4} = \\frac{3}{8}, \n\\quad\nq = 1 - r = \\frac{5}{8}.\n$$\nHence\n$$\nI(X;Y) = 1 + \\frac{5}{8} \\log_{2}\\!\\left(\\frac{5}{8}\\right) + \\frac{3}{8} \\log_{2}\\!\\left(\\frac{3}{8}\\right),\n$$\nwhich is an exact closed-form expression in bits. Equivalently, $I(X;Y) = 1 - h_{2}\\!\\left(\\frac{3}{8}\\right)$, where $h_{2}$ denotes the binary entropy function in bits.", "answer": "$$\\boxed{1+\\frac{5}{8}\\log_{2}\\left(\\frac{5}{8}\\right)+\\frac{3}{8}\\log_{2}\\left(\\frac{3}{8}\\right)}$$", "id": "1612658"}]}