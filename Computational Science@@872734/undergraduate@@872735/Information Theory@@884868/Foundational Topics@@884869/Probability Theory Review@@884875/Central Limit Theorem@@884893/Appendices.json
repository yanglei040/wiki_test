{"hands_on_practices": [{"introduction": "The Central Limit Theorem provides a powerful tool for approximating the distribution of a sum of many independent random variables. This exercise offers a foundational application of the CLT, where we approximate the probability of a sum of Poisson random variables falling within a certain range [@problem_id:480135]. Mastering this technique is essential for situations where calculating an exact probability is computationally difficult or impossible.", "problem": "Let $Y_1, Y_2, \\dots$ be a sequence of independent and identically distributed random variables, each following a Poisson distribution with parameter $\\lambda = 1$. Define the partial sum $S_n = \\sum_{k=1}^n Y_k$ for $n \\in \\mathbb{N}$. Using the central limit theorem, approximate the probability $P(90 \\leq S_{100} \\leq 110)$. Provide the numerical value of the approximation rounded to 10 significant digits.", "solution": "1. For i.i.d. $Y_k\\sim\\mathrm{Poisson}(1)$, the sum $S_{100}=\\sum_{k=1}^{100}Y_k$ has mean and variance\n$$\\mu=\\mathbb{E}[S_{100}]=100\\cdot1=100,\\quad\\sigma^2=\\mathrm{Var}(S_{100})=100\\cdot1=100.$$\n2. By the central limit theorem, for $Z\\sim N(0,1)$,\n$$P(90\\le S_{100}\\le110)\\approx P\\Bigl(\\frac{90-\\mu}{\\sigma}\\le Z\\le\\frac{110-\\mu}{\\sigma}\\Bigr).$$\n3. Compute the standardized bounds:\n$$\\frac{90-100}{10}=-1,\\quad\\frac{110-100}{10}=1.$$\n4. Thus\n$$P(90\\le S_{100}\\le110)\\approx \\Phi(1)-\\Phi(-1)=2\\Phi(1)-1.$$\n5. Using $\\Phi(1)\\approx0.8413447461$ gives\n$$2\\cdot0.8413447461-1=0.6826894921370859\\approx0.6826894921\\quad(\\text{10 significant digits}).$$", "answer": "$$\\boxed{0.6826894921}$$", "id": "480135"}, {"introduction": "When using a continuous distribution like the normal to approximate a discrete one, a small adjustment can greatly improve accuracy. This practice introduces the continuity correction, a vital refinement to the standard Central Limit Theorem approximation [@problem_id:852436]. You will apply this technique to find the probability associated with a sum of custom discrete random variables, a common task in statistical modeling.", "problem": "A discrete random variable $X$ can take values from the set $S = \\{-2, -1, 0, 1, 3\\}$. Its probability mass function (PMF) is given by $P(X=k) = C(k^2+1)$ for $k \\in S$, where $C$ is a normalization constant.\n\nLet $X_1, X_2, \\dots, X_n$ be a sequence of $n$ independent and identically distributed (i.i.d.) random variables, each with the same distribution as $X$. Consider their sum, $S_n = \\sum_{i=1}^n X_i$.\n\nFor $n=470$, using the Central Limit Theorem with continuity correction, calculate the probability $P(S_{470}  540)$.\n\nYou are given that for a standard normal random variable $Z \\sim N(0,1)$, the value of its cumulative distribution function (CDF) at $z=1.5$ is $\\Phi(1.5) \\approx 0.9331927987$.", "solution": "1. Normalization constant:\n$$\\sum_{k\\in\\{-2,-1,0,1,3\\}}(k^2+1)=5+2+1+2+10=20,\\quad C=\\frac1{20}.$$\n2. Mean:\n$$\\mu=E[X]=C\\sum k\\,(k^2+1)\n=\\frac1{20}(-2\\cdot5-1\\cdot2+0\\cdot1+1\\cdot2+3\\cdot10)\n=\\frac{20}{20}=1.$$\n3. Second moment:\n$$E[X^2]=C\\sum k^2\\,(k^2+1)\n=\\frac1{20}(20+2+0+2+90)=\\frac{114}{20}=5.7.$$\nVariance:\n$$\\sigma^2=E[X^2]-\\mu^2=5.7-1^2=4.7.$$\n4. Sum $S_{470}$: mean $470\\mu=470$, variance $470\\sigma^2=470\\cdot4.7=2209$, so $\\sigma_{S}=47$.\n5. Continuity correction:\n$$P(S_{470}540)\\approx P\\bigl(S_{470}540.5\\bigr)\n= P\\!\\Bigl(Z\\frac{540.5-470}{47}\\Bigr)\n= P(Z1.5)=1-\\Phi(1.5)\\approx1-0.9331927987=0.0668072013.$$", "answer": "$$\\boxed{0.0668072013}$$", "id": "852436"}, {"introduction": "The influence of the Central Limit Theorem extends beyond simple sums and means through a powerful tool called the Delta Method. This advanced problem challenges you to derive the asymptotic variance for a log-odds ratio, a critical measure in fields like epidemiology and clinical trials [@problem_id:852421]. This demonstrates how the CLT underpins the statistical properties of more complex estimators.", "problem": "Consider two independent random samples, of sizes $n_1$ and $n_2$ respectively, drawn from two distinct Bernoulli populations. The first population has a success probability of $p_1$, and the second has a success probability of $p_2$. Let $X_1$ and $X_2$ be the number of successes observed in the first and second samples, respectively. The sample proportions are then given by $\\hat{p}_1 = \\frac{X_1}{n_1}$ and $\\hat{p}_2 = \\frac{X_2}{n_2}$.\n\nIn many statistical analyses, particularly in epidemiology and clinical trials, the log-odds ratio is a quantity of significant interest. The true log-odds ratio is defined as $\\theta = \\log\\left(\\frac{p_1/(1-p_1)}{p_2/(1-p_2)}\\right)$. An estimator for this quantity, based on the sample proportions, is the sample log-odds ratio:\n$$\n\\hat{\\theta} = \\log\\left(\\frac{\\hat{p}_1/(1-\\hat{p}_1)}{\\hat{p}_2/(1-\\hat{p}_2)}\\right)\n$$\nAssuming that the sample sizes $n_1$ and $n_2$ are sufficiently large, the Central Limit Theorem can be extended via the Delta Method to find the approximate distribution of $\\hat{\\theta}$.\n\nDerive the asymptotic variance of the log-odds ratio estimator, $\\text{Var}(\\hat{\\theta})$.", "solution": "The problem asks for the asymptotic variance of the log-odds ratio estimator $\\hat{\\theta}$. We can find this using the multivariate Delta Method.\n\nFirst, by the Central Limit Theorem, for large sample sizes $n_1$ and $n_2$, the sample proportions $\\hat{p}_1$ and $\\hat{p}_2$ are approximately normally distributed:\n$$\n\\hat{p}_1 \\approx N\\left(p_1, \\frac{p_1(1-p_1)}{n_1}\\right)\n$$\n$$\n\\hat{p}_2 \\approx N\\left(p_2, \\frac{p_2(1-p_2)}{n_2}\\right)\n$$\nSince the two samples are independent, the random variables $\\hat{p}_1$ and $\\hat{p}_2$ are also independent. Therefore, the vector of sample proportions $\\hat{\\mathbf{p}} = (\\hat{p}_1, \\hat{p}_2)^T$ has an asymptotic bivariate normal distribution with mean vector $\\boldsymbol{\\mu}$ and covariance matrix $\\boldsymbol{\\Sigma}$:\n$$\n\\boldsymbol{\\mu} = E[\\hat{\\mathbf{p}}] = \\begin{pmatrix} p_1 \\\\ p_2 \\end{pmatrix}\n$$\n$$\n\\boldsymbol{\\Sigma} = \\text{Cov}(\\hat{\\mathbf{p}}) = \\begin{pmatrix} \\text{Var}(\\hat{p}_1)  \\text{Cov}(\\hat{p}_1, \\hat{p}_2) \\\\ \\text{Cov}(\\hat{p}_1, \\hat{p}_2)  \\text{Var}(\\hat{p}_2) \\end{pmatrix} = \\begin{pmatrix} \\frac{p_1(1-p_1)}{n_1}  0 \\\\ 0  \\frac{p_2(1-p_2)}{n_2} \\end{pmatrix}\n$$\nThe estimator $\\hat{\\theta}$ is a function of $\\hat{p}_1$ and $\\hat{p}_2$. Let this function be $g(x, y)$:\n$$\ng(x, y) = \\log\\left(\\frac{x/(1-x)}{y/(1-y)}\\right) = \\log(x) - \\log(1-x) - \\log(y) + \\log(1-y)\n$$\nThe multivariate Delta Method states that the asymptotic variance of a function $g(\\hat{\\mathbf{p}})$ is given by:\n$$\n\\text{Var}(g(\\hat{\\mathbf{p}})) \\approx (\\nabla g(\\boldsymbol{\\mu}))^T \\boldsymbol{\\Sigma} (\\nabla g(\\boldsymbol{\\mu}))\n$$\nwhere $\\nabla g(\\boldsymbol{\\mu})$ is the gradient of $g$ evaluated at the mean vector $\\boldsymbol{\\mu} = (p_1, p_2)^T$.\n\nFirst, we compute the gradient of $g(x, y)$:\n$$\n\\frac{\\partial g}{\\partial x} = \\frac{1}{x} - \\frac{1}{1-x}(-1) = \\frac{1}{x} + \\frac{1}{1-x} = \\frac{1-x+x}{x(1-x)} = \\frac{1}{x(1-x)}\n$$\n$$\n\\frac{\\partial g}{\\partial y} = -\\frac{1}{y} + \\frac{1}{1-y}(-1) = -\\frac{1}{y} - \\frac{1}{1-y} = -\\frac{1-y+y}{y(1-y)} = -\\frac{1}{y(1-y)}\n$$\nSo the gradient vector is $\\nabla g(x,y) = \\left(\\frac{1}{x(1-x)}, -\\frac{1}{y(1-y)}\\right)^T$.\n\nNext, we evaluate the gradient at the mean $\\boldsymbol{\\mu} = (p_1, p_2)^T$:\n$$\n\\nabla g(\\boldsymbol{\\mu}) = \\begin{pmatrix} \\frac{1}{p_1(1-p_1)} \\\\ -\\frac{1}{p_2(1-p_2)} \\end{pmatrix}\n$$\nNow, we can substitute the gradient and the covariance matrix into the Delta Method formula for the variance:\n$$\n\\text{Var}(\\hat{\\theta}) \\approx \n\\begin{pmatrix} \\frac{1}{p_1(1-p_1)}  -\\frac{1}{p_2(1-p_2)} \\end{pmatrix}\n\\begin{pmatrix} \\frac{p_1(1-p_1)}{n_1}  0 \\\\ 0  \\frac{p_2(1-p_2)}{n_2} \\end{pmatrix}\n\\begin{pmatrix} \\frac{1}{p_1(1-p_1)} \\\\ -\\frac{1}{p_2(1-p_2)} \\end{pmatrix}\n$$\nWe perform the matrix multiplication from left to right. First, multiply the row vector (the transpose of the gradient) by the covariance matrix:\n$$\n\\begin{pmatrix} \\frac{1}{p_1(1-p_1)} \\cdot \\frac{p_1(1-p_1)}{n_1} + (-\\frac{1}{p_2(1-p_2)}) \\cdot 0  \\frac{1}{p_1(1-p_1)} \\cdot 0 + (-\\frac{1}{p_2(1-p_2)}) \\cdot \\frac{p_2(1-p_2)}{n_2} \\end{pmatrix}\n$$\n$$\n= \\begin{pmatrix} \\frac{1}{n_1}  -\\frac{1}{n_2} \\end{pmatrix}\n$$\nFinally, multiply this resulting row vector by the column vector (the gradient):\n$$\n\\text{Var}(\\hat{\\theta}) \\approx \\begin{pmatrix} \\frac{1}{n_1}  -\\frac{1}{n_2} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{p_1(1-p_1)} \\\\ -\\frac{1}{p_2(1-p_2)} \\end{pmatrix}\n$$\n$$\n= \\left(\\frac{1}{n_1}\\right) \\left(\\frac{1}{p_1(1-p_1)}\\right) + \\left(-\\frac{1}{n_2}\\right) \\left(-\\frac{1}{p_2(1-p_2)}\\right)\n$$\n$$\n= \\frac{1}{n_1 p_1 (1-p_1)} + \\frac{1}{n_2 p_2 (1-p_2)}\n$$\nThis is the asymptotic variance of the log-odds ratio estimator.", "answer": "$$\n\\boxed{\\frac{1}{n_1 p_1 (1-p_1)} + \\frac{1}{n_2 p_2 (1-p_2)}}\n$$", "id": "852421"}]}