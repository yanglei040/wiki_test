{"hands_on_practices": [{"introduction": "To begin our exploration of entropy rate, we'll start with a foundational case. This first practice examines a binary source with a simple but powerful constraint: a '1' is always followed by a '0'. This scenario provides an intuitive entry point for understanding how deterministic rules within a stochastic process reduce its overall uncertainty and, consequently, its entropy rate. By working through this problem [@problem_id:1621587], you will directly see how predictability impacts the amount of information generated per symbol.", "problem": "A specialized digital information source generates a binary sequence of symbols, '0' and '1'. The generation process is governed by two rules:\n1. Any time a '1' is generated, the next symbol must be a '0'.\n2. After a '0' is generated, the probability that the next symbol will be a '1' is a constant value $p$, where $0  p  1$.\n\nAssuming the process has been running for a long time and has reached a stationary state, determine the entropy rate of this source. Express your answer as an analytic function of $p$, in units of bits per symbol.", "solution": "The source generates a binary sequence under a first-order Markov rule:\n- From state 0: the next symbol is 1 with probability $p$ and 0 with probability $1-p$.\n- From state 1: the next symbol is deterministically 0.\n\nThus the transition matrix $P$ on states $\\{0,1\\}$ is\n$$\nP=\\begin{pmatrix}\n1-p  p\\\\\n1  0\n\\end{pmatrix}.\n$$\nLet $(\\mu_0, \\mu_1)$ be the stationary distribution, satisfying $\\mu P = \\mu$ and $\\mu_0 + \\mu_1 = 1$. Writing the stationarity equations,\n$$\n\\mu_0=\\mu_0(1-p)+\\mu_1\\cdot 1,\\qquad \\mu_1=\\mu_0 p+\\mu_1\\cdot 0.\n$$\nFrom the second equation, $\\mu_1=\\mu_0 p$. Using normalization,\n$$\n\\mu_0+\\mu_1=\\mu_0+\\mu_0 p=\\mu_0(1+p)=1 \\;\\Rightarrow\\; \\mu_0=\\frac{1}{1+p},\\quad \\mu_1=\\frac{p}{1+p}.\n$$\n\nFor a stationary first-order Markov source, the entropy rate is the stationary average of the next-symbol conditional entropy:\n$$\nH=-\\sum_{i\\in\\{0,1\\}}\\mu_i\\sum_{j\\in\\{0,1\\}}P_{ij}\\log_{2}(P_{ij})\n=\\sum_{i\\in\\{0,1\\}}\\mu_i H(X_{n+1}\\mid X_{n}=i).\n$$\nGiven $X_n=0$, the next symbol has distribution $\\{1-p,p\\}$, so\n$$\nH(X_{n+1}\\mid X_n=0)=-(1-p)\\log_2(1-p)-p\\log_2(p).\n$$\nGiven $X_n=1$, the next symbol is deterministically $0$, so $H(X_{n+1}\\mid X_n=1)=0$. Therefore,\n$$\nH=\\mu_0\\left[-(1-p)\\log_2(1-p)-p\\log_2(p)\\right]\n=\\frac{1}{1+p}\\left[-(1-p)\\log_2(1-p)-p\\log_2(p)\\right].\n$$\nThis is the entropy rate in bits per symbol.", "answer": "$$\\boxed{\\frac{1}{1+p}\\left[-p\\log_{2}(p)-(1-p)\\log_{2}(1-p)\\right]}$$", "id": "1621587"}, {"introduction": "Having seen how constraints affect entropy, we now move to a more general and widely applicable model. Many real-world processes, from digital communication to financial markets, exhibit memory where the probability of future events depends on the current state. This exercise [@problem_id:1621600] models a communication channel where transmission errors are not independent but depend on the success of the previous transmission. Calculating the entropy rate here requires you to master the essential technique of finding the stationary distribution of a Markov chain and using it to average the uncertainty across all possible states.", "problem": "Consider a simplified model for a digital communication channel that transmits a long sequence of binary symbols. This channel exhibits memory, meaning the probability of an error in transmission for a given symbol depends on whether the previous symbol was transmitted correctly or not.\n\nLet the state of the channel for each symbol transmission be described by a stochastic process $\\{X_n\\}_{n=1}^{\\infty}$, where $X_n=0$ if the $n$-th symbol is transmitted correctly, and $X_n=1$ if it is transmitted with an error (i.e., the bit is flipped).\n\nThe behavior of the channel is governed by the following conditional probabilities:\n- If the $(n-1)$-th symbol was transmitted correctly ($X_{n-1}=0$), the probability that the $n$-th symbol is transmitted with an error ($X_n=1$) is $\\alpha$.\n- If the $(n-1)$-th symbol was transmitted with an error ($X_{n-1}=1$), the probability that the $n$-th symbol is transmitted correctly ($X_n=0$) is $\\beta$.\n\nAssume that the process has been running for a long time, such that it has reached a stationary state. The parameters $\\alpha$ and $\\beta$ are constants in the open interval $(0, 1)$.\n\nUsing base-2 logarithms for entropy calculations, determine the entropy rate of the error process $\\{X_n\\}$. The entropy rate quantifies the average uncertainty per symbol in the stationary state. Express your final answer as a symbolic expression in terms of $\\alpha$ and $\\beta$.", "solution": "The error process $\\{X_n\\}$ can be modeled as a discrete-time Markov chain with two states: state 0 (correct transmission) and state 1 (error). The problem provides the conditional probabilities that define the transitions between these states.\n\nFirst, we construct the transition probability matrix $P$, where $P_{ij} = P(X_n = j | X_{n-1} = i)$.\nFrom the problem statement:\n- $P(X_n=1 | X_{n-1}=0) = P_{01} = \\alpha$. Since there are only two outcomes, $P(X_n=0 | X_{n-1}=0) = P_{00} = 1 - \\alpha$.\n- $P(X_n=0 | X_{n-1}=1) = P_{10} = \\beta$. Similarly, $P(X_n=1 | X_{n-1}=1) = P_{11} = 1 - \\beta$.\n\nSo, the transition probability matrix is:\n$$ P = \\begin{pmatrix} 1-\\alpha  \\alpha \\\\ \\beta  1-\\beta \\end{pmatrix} $$\n\nThe entropy rate $H(\\mathcal{X})$ of a stationary Markov chain is given by the conditional entropy of the next state given the current state:\n$$ H(\\mathcal{X}) = H(X_n | X_{n-1}) = \\sum_{i \\in \\{0, 1\\}} \\pi_i H(X_n | X_{n-1}=i) $$\nwhere $\\pi = (\\pi_0, \\pi_1)$ is the stationary distribution of the chain, with $\\pi_i = P(X_{n-1}=i)$.\n\nTo find the stationary distribution, we solve the system of equations $\\pi P = \\pi$ and $\\pi_0 + \\pi_1 = 1$.\nThe equation $\\pi P = \\pi$ expands to:\n$$ \\begin{pmatrix} \\pi_0  \\pi_1 \\end{pmatrix} \\begin{pmatrix} 1-\\alpha  \\alpha \\\\ \\beta  1-\\beta \\end{pmatrix} = \\begin{pmatrix} \\pi_0  \\pi_1 \\end{pmatrix} $$\nThis gives two linear equations:\n1. $\\pi_0(1-\\alpha) + \\pi_1\\beta = \\pi_0$\n2. $\\pi_0\\alpha + \\pi_1(1-\\beta) = \\pi_1$\n\nFrom the first equation:\n$$ \\pi_0 - \\pi_0\\alpha + \\pi_1\\beta = \\pi_0 \\implies \\pi_1\\beta = \\pi_0\\alpha $$\n(The second equation gives the same relationship).\nNow, we use the normalization condition $\\pi_1 = 1 - \\pi_0$:\n$$ (1-\\pi_0)\\beta = \\pi_0\\alpha $$\n$$ \\beta - \\pi_0\\beta = \\pi_0\\alpha $$\n$$ \\beta = \\pi_0(\\alpha + \\beta) $$\nSolving for $\\pi_0$, we get:\n$$ \\pi_0 = \\frac{\\beta}{\\alpha + \\beta} $$\nAnd for $\\pi_1$:\n$$ \\pi_1 = 1 - \\pi_0 = 1 - \\frac{\\beta}{\\alpha + \\beta} = \\frac{\\alpha}{\\alpha + \\beta} $$\nSince $\\alpha, \\beta \\in (0,1)$, the denominator $\\alpha+\\beta$ is non-zero.\n\nNext, we calculate the conditional entropies $H(X_n | X_{n-1}=i)$. These are the entropies of the rows of the transition matrix $P$.\nFor $i=0$, the conditional probability distribution is $(P_{00}, P_{01}) = (1-\\alpha, \\alpha)$. The conditional entropy is:\n$$ H(X_n | X_{n-1}=0) = -\\sum_{j \\in \\{0,1\\}} P_{0j} \\log_2(P_{0j}) = -(1-\\alpha)\\log_2(1-\\alpha) - \\alpha\\log_2(\\alpha) $$\nThis is the binary entropy function, sometimes denoted $H_b(\\alpha)$.\n\nFor $i=1$, the conditional probability distribution is $(P_{10}, P_{11}) = (\\beta, 1-\\beta)$. The conditional entropy is:\n$$ H(X_n | X_{n-1}=1) = -\\sum_{j \\in \\{0,1\\}} P_{1j} \\log_2(P_{1j}) = -\\beta\\log_2(\\beta) - (1-\\beta)\\log_2(1-\\beta) $$\nThis is the binary entropy function $H_b(\\beta)$.\n\nFinally, we substitute the stationary probabilities and conditional entropies into the formula for the entropy rate:\n$$ H(\\mathcal{X}) = \\pi_0 H(X_n | X_{n-1}=0) + \\pi_1 H(X_n | X_{n-1}=1) $$\n$$ H(\\mathcal{X}) = \\frac{\\beta}{\\alpha + \\beta} \\left( -(1-\\alpha)\\log_2(1-\\alpha) - \\alpha\\log_2(\\alpha) \\right) + \\frac{\\alpha}{\\alpha + \\beta} \\left( -\\beta\\log_2(\\beta) - (1-\\beta)\\log_2(1-\\beta) \\right) $$\nThis is the final expression for the entropy rate of the error process.", "answer": "$$\\boxed{\\frac{\\beta}{\\alpha + \\beta} \\left( -(1-\\alpha)\\log_{2}(1-\\alpha) - \\alpha\\log_{2}(\\alpha) \\right) + \\frac{\\alpha}{\\alpha + \\beta} \\left( -\\beta\\log_{2}(\\beta) - (1-\\beta)\\log_{2}(1-\\beta) \\right)}$$", "id": "1621600"}, {"introduction": "Our final practice challenges you to apply your skills to a more complex scenario that mirrors real-world engineering and scientific modeling. Instead of being given a simple transition matrix, you are presented with a physical description of a noisy data storage device attempting to generate a repeating pattern. The core task in this problem [@problem_id:1621635] is to first construct the appropriate state-space model from this description before you can begin to calculate the entropy rate. This exercise emphasizes the crucial skill of abstraction—translating a system's operational rules into a formal stochastic process—and represents a key step towards applying information theory to practical problems.", "problem": "A specialized data storage device is designed to repeatedly output the binary sequence \"0110\". However, due to physical imperfections, the read-out process is noisy. The device's behavior can be modeled as a stationary stochastic process.\n\nThe process has four states, $S_0, S_1, S_2, S_3$, corresponding to the length of the prefix of \"0110\" that has been correctly generated in sequence.\n- State $S_0$: The empty prefix has been matched (initial state or after an error that breaks the pattern). The next target bit is '0'.\n- State $S_1$: The prefix \"0\" has been matched. The next target bit is '1'.\n- State $S_2$: The prefix \"01\" has been matched. The next target bit is '1'.\n- State $S_3$: The prefix \"011\" has been matched. The next target bit is '0'.\n\nFrom any state $S_k$, the device attempts to output the next correct bit in the \"0110\" sequence. The probability of outputting the correct bit is given by a state-dependent parameter $p_k$. The probability of outputting the incorrect bit is $1-p_k$. After a bit is output, the system transitions to a new state. The new state is determined by finding the length of the longest prefix of \"0110\" that is also a suffix of the sequence generated so far.\n\nThe probabilities of generating the correct bit for each state are given as:\n- $p_0 = 1/2$\n- $p_1 = 1/2$\n- $p_2 = 1/2$\n- $p_3 = 1/4$\n\nCalculate the entropy rate of the output binary sequence, in bits per symbol. Express your answer as a single closed-form analytic expression. Your expression may include logarithms and fractions but should be fully simplified.", "solution": "The output process is generated by a unifilar hidden Markov model: given the current state and the emitted symbol, the next state is uniquely determined by the longest-prefix-suffix rule. For such unifilar presentations, the entropy rate equals the expected uncertainty of the emitted symbol conditioned on the current state under the stationary state distribution. Therefore, the entropy rate $h$ is given by $h=\\sum_{k=0}^{3}\\pi_{k}\\,H_{2}(p_{k})$, where $\\pi_k$ is the stationary probability of state $S_k$ and $H_{2}(p)=-p\\log_{2}(p)-(1-p)\\log_{2}(1-p)$ is the binary entropy in bits.\n\nFirst, determine the state transition structure. Let the emitted bit be $x\\in\\{0,1\\}$.\n- From $S_0$ (target $0$): with probability $p_0$ emit $0$ and move to $S_1$; with probability $1-p_0$ emit $1$ and move to $S_0$.\n- From $S_1$ (target $1$): with probability $p_1$ emit $1$ and move to $S_2$; with probability $1-p_1$ emit $0$ and move to $S_1$.\n- From $S_2$ (target $1$): with probability $p_2$ emit $1$ and move to $S_3$; with probability $1-p_2$ emit $0$ and move to $S_1$.\n- From $S_3$ (target $0$): with probability $p_3$ emit $0$ and, since the matched word is now \"0110\" whose longest proper prefix that is also a suffix is \"0\", move to $S_1$; with probability $1-p_3$ emit $1$ and move to $S_0$.\n\nWith the given values $p_0=p_1=p_2=\\frac{1}{2}$ and $p_3=\\frac{1}{4}$, the transition probabilities between states are\n$$\n\\begin{aligned}\n\\text{From }S_{0}:\\quad S_{0}\\xleftarrow{\\frac{1}{2}},\\; S_{1}\\xleftarrow{\\frac{1}{2}},\\\\\n\\text{From }S_{1}:\\quad S_{1}\\xleftarrow{\\frac{1}{2}},\\; S_{2}\\xleftarrow{\\frac{1}{2}},\\\\\n\\text{From }S_{2}:\\quad S_{1}\\xleftarrow{\\frac{1}{2}},\\; S_{3}\\xleftarrow{\\frac{1}{2}},\\\\\n\\text{From }S_{3}:\\quad S_{0}\\xleftarrow{\\frac{3}{4}},\\; S_{1}\\xleftarrow{\\frac{1}{4}}.\n\\end{aligned}\n$$\nLet the stationary distribution be $\\pi=(\\pi_0,\\pi_1,\\pi_2,\\pi_3)=(a,b,c,d)$. Solving $\\pi P = \\pi$ and $a+b+c+d=1$ gives\n$$\n\\begin{aligned}\nc=\\frac{1}{2}b,\\qquad d=\\frac{1}{2}c=\\frac{1}{4}b,\\qquad a=\\frac{1}{2}a+\\frac{3}{4}d\\;\\Rightarrow\\;a=\\frac{3}{8}b,\\\\\na+b+c+d=\\frac{3}{8}b+b+\\frac{1}{2}b+\\frac{1}{4}b=\\frac{17}{8}b=1\\;\\Rightarrow\\;b=\\frac{8}{17}.\n\\end{aligned}\n$$\nHence\n$$\n\\pi_0=a=\\frac{3}{17},\\quad \\pi_1=b=\\frac{8}{17},\\quad \\pi_2=c=\\frac{4}{17},\\quad \\pi_3=d=\\frac{2}{17}.\n$$\n\nNext compute the per-state symbol entropies. For $p=\\frac{1}{2}$, $H_{2}\\!\\left(\\frac{1}{2}\\right)=1$. For $p=\\frac{1}{4}$,\n$$\nH_{2}\\!\\left(\\frac{1}{4}\\right)=-\\frac{1}{4}\\log_{2}\\!\\left(\\frac{1}{4}\\right)-\\frac{3}{4}\\log_{2}\\!\\left(\\frac{3}{4}\\right)\n=2-\\frac{3}{4}\\log_{2}(3).\n$$\nTherefore,\n$$\nh=\\left(\\pi_0+\\pi_1+\\pi_2\\right)\\cdot 1+\\pi_3\\cdot H_{2}\\!\\left(\\frac{1}{4}\\right)\n=\\frac{15}{17}+\\frac{2}{17}\\left(2-\\frac{3}{4}\\log_{2}(3)\\right)\n=\\frac{19}{17}-\\frac{3}{34}\\log_{2}(3).\n$$\nThis is the entropy rate in bits per symbol as a closed-form expression.", "answer": "$$\\boxed{\\frac{19}{17}-\\frac{3}{34}\\log_{2}(3)}$$", "id": "1621635"}]}