{"hands_on_practices": [{"introduction": "To begin, we will tackle a foundational problem that illustrates the core procedure for calculating the entropy rate of a Markov source. Using a simple and intuitive model of daily weather patterns, this exercise will guide you step-by-step through defining a transition matrix, finding the steady-state distribution, and computing the conditional entropy to arrive at the final rate. Mastering this example is the first crucial step toward understanding the information content of more complex time-dependent processes [@problem_id:1621329].", "problem": "A simplified climatological model for a remote island describes the weather on any given day as being either \"Sunny\" or \"Rainy\". The model is a stationary first-order Markov process. The probability that a Sunny day is followed by a Rainy day is $p$. Similarly, the probability that a Rainy day is followed by a Sunny day is also $p$. The value of $p$ is a constant such that $0 < p < 1$.\n\nAssuming this model has reached its steady state, determine the entropy rate of the daily weather sequence. Express your answer as a symbolic expression in terms of $p$. For all calculations, use logarithms in base 2. The final answer should be in units of bits per day.", "solution": "Let the two states of the Markov process be $S$ for a Sunny day and $R$ for a Rainy day. The problem statement provides the transition probabilities between these states.\n\nThe probability of transitioning from Sunny to Rainy is given as $P(R|S) = p$.\nSince the weather on the next day can only be Sunny or Rainy, the sum of probabilities of transitioning from a Sunny day must be 1. Therefore, the probability of a Sunny day being followed by another Sunny day is:\n$P(S|S) = 1 - P(R|S) = 1 - p$.\n\nSimilarly, the probability of transitioning from Rainy to Sunny is given as $P(S|R) = p$.\nThe probability of a Rainy day being followed by another Rainy day is:\n$P(R|R) = 1 - P(S|R) = 1 - p$.\n\nWe can represent these transition probabilities in a transition matrix $P$, where the rows correspond to the current state and columns to the next state (in the order S, R):\n$$\nP = \\begin{pmatrix} P(S|S) & P(R|S) \\\\ P(S|R) & P(R|R) \\end{pmatrix} = \\begin{pmatrix} 1-p & p \\\\ p & 1-p \\end{pmatrix}\n$$\n\nThe problem asks for the entropy rate of the process in its steady state. For a stationary Markov chain, the entropy rate $H(\\mathcal{X})$ is given by the conditional entropy of the next state given the current state:\n$$\nH(\\mathcal{X}) = H(X_{n+1} | X_n) = \\sum_{i \\in \\{S, R\\}} \\pi_i H(X_{n+1} | X_n=i)\n$$\nwhere $\\pi = (\\pi_S, \\pi_R)$ is the stationary distribution of the Markov chain.\n\nFirst, we need to find the stationary distribution $\\pi$, which satisfies the equation $\\pi P = \\pi$. This, along with the normalization condition $\\pi_S + \\pi_R = 1$, allows us to solve for $\\pi_S$ and $\\pi_R$.\nThe equation $\\pi P = \\pi$ gives:\n$$\n(\\pi_S, \\pi_R) \\begin{pmatrix} 1-p & p \\\\ p & 1-p \\end{pmatrix} = (\\pi_S, \\pi_R)\n$$\nThis results in the system of linear equations:\n1. $\\pi_S(1-p) + \\pi_R p = \\pi_S$\n2. $\\pi_S p + \\pi_R (1-p) = \\pi_R$\n\nFrom equation 1:\n$-\\pi_S p + \\pi_R p = 0 \\implies p(\\pi_R - \\pi_S) = 0$.\nSince $0 < p < 1$, we must have $\\pi_R = \\pi_S$.\n\nUsing the normalization condition $\\pi_S + \\pi_R = 1$:\n$\\pi_S + \\pi_S = 1 \\implies 2\\pi_S = 1 \\implies \\pi_S = \\frac{1}{2}$.\nTherefore, $\\pi_R = \\frac{1}{2}$. The stationary distribution is $\\pi = (\\frac{1}{2}, \\frac{1}{2})$. This is expected for a symmetric transition matrix.\n\nNext, we calculate the conditional entropies for each state. The problem specifies using base-2 logarithms.\nThe conditional entropy given that the current state is Sunny is:\n$$\nH(X_{n+1} | X_n=S) = -\\sum_{j \\in \\{S,R\\}} P(j|S) \\log_2 P(j|S)\n$$\n$$\nH(X_{n+1} | X_n=S) = -[P(S|S)\\log_2 P(S|S) + P(R|S)\\log_2 P(R|S)]\n$$\n$$\nH(X_{n+1} | X_n=S) = -[(1-p)\\log_2(1-p) + p\\log_2(p)]\n$$\n\nThe conditional entropy given that the current state is Rainy is:\n$$\nH(X_{n+1} | X_n=R) = -\\sum_{j \\in \\{S,R\\}} P(j|R) \\log_2 P(j|R)\n$$\n$$\nH(X_{n+1} | X_n=R) = -[P(S|R)\\log_2 P(S|R) + P(R|R)\\log_2 P(R|R)]\n$$\n$$\nH(X_{n+1} | X_n=R) = -[p\\log_2(p) + (1-p)\\log_2(1-p)]\n$$\nBoth conditional entropies are equal to the binary entropy function $H_b(p)$.\n\nFinally, we calculate the entropy rate using the stationary distribution:\n$$\nH(\\mathcal{X}) = \\pi_S H(X_{n+1} | X_n=S) + \\pi_R H(X_{n+1} | X_n=R)\n$$\n$$\nH(\\mathcal{X}) = \\frac{1}{2} [-p\\log_2(p) - (1-p)\\log_2(1-p)] + \\frac{1}{2} [-p\\log_2(p) - (1-p)\\log_2(1-p)]\n$$\n$$\nH(\\mathcal{X}) = (\\frac{1}{2} + \\frac{1}{2}) [-p\\log_2(p) - (1-p)\\log_2(1-p)]\n$$\n$$\nH(\\mathcal{X}) = -p\\log_2(p) - (1-p)\\log_2(1-p)\n$$\nThis function is often denoted as the binary entropy function $H_b(p)$. The units are bits per symbol, or in this context, bits per day.", "answer": "$$\\boxed{-p\\log_{2}(p) - (1-p)\\log_{2}(1-p)}$$", "id": "1621329"}, {"introduction": "This next practice explores a special but conceptually vital case: a Markov process designed to have no memory. By analyzing a source where the next state is independent of the current one, we can see how the general formula for the entropy rate of a Markov source simplifies to that of a memoryless, or IID (Independent and Identically Distributed), source. This exercise [@problem_id:1621359] is key to understanding the role of dependency in determining a system's information content and demonstrates the consistency of information-theoretic principles.", "problem": "Consider a simplified model for a random text generator that produces a sequence of characters from the alphabet $\\{X, Y, Z\\}$. The generator is designed as a first-order stationary Markov source. However, due to a particular design choice, the probability of generating the next character is actually independent of the current character in the sequence.\n\nSpecifically, the probability of generating an 'X' is $p_X$, the probability of generating a 'Y' is $p_Y$, and the probability of generating a 'Z' is $p_Z$, regardless of the preceding character. The values $p_X$, $p_Y$, and $p_Z$ are positive constants that sum to one: $p_X + p_Y + p_Z = 1$.\n\nAssuming this process has started long ago and has reached a steady state, determine the entropy rate of this source. Express your answer as an analytic expression in terms of $p_X$, $p_Y$, and $p_Z$, using the base-2 logarithm.", "solution": "The problem asks for the entropy rate of a stationary first-order Markov source. Let the set of states be $S = \\{X, Y, Z\\}$.\n\nFirst, we need to define the transition probability matrix $P$ for this Markov source. The entry $P_{ij}$ represents the probability of transitioning from state $i$ to state $j$. Based on the problem description, the probability of the next character is independent of the current character. This means that each row of the transition matrix is identical.\n\nThe rows of the matrix represent the current state (X, Y, or Z), and the columns represent the next state (X, Y, or Z). The transition matrix $P$ is therefore:\n$$\nP = \\begin{pmatrix}\nP(X_{n+1}=X | X_n=X) & P(X_{n+1}=Y | X_n=X) & P(X_{n+1}=Z | X_n=X) \\\\\nP(X_{n+1}=X | X_n=Y) & P(X_{n+1}=Y | X_n=Y) & P(X_{n+1}=Z | X_n=Y) \\\\\nP(X_{n+1}=X | X_n=Z) & P(X_{n+1}=Y | X_n=Z) & P(X_{n+1}=Z | X_n=Z)\n\\end{pmatrix}\n$$\nGiven the description, these probabilities are $p_X, p_Y, p_Z$ respectively, regardless of the current state. So, the matrix becomes:\n$$\nP = \\begin{pmatrix}\np_X & p_Y & p_Z \\\\\np_X & p_Y & p_Z \\\\\np_X & p_Y & p_Z\n\\end{pmatrix}\n$$\n\nThe entropy rate $H(\\mathcal{X})$ of a stationary Markov source is given by the formula:\n$$\nH(\\mathcal{X}) = \\sum_{i \\in S} \\pi_i H(P_{i,\\cdot})\n$$\nwhere $\\boldsymbol{\\pi} = (\\pi_X, \\pi_Y, \\pi_Z)$ is the stationary distribution of the Markov chain, and $H(P_{i,\\cdot})$ is the entropy of the probability distribution given by the $i$-th row of the transition matrix $P$.\n\nFirst, we find the stationary distribution $\\boldsymbol{\\pi}$ by solving the equation $\\boldsymbol{\\pi} P = \\boldsymbol{\\pi}$, subject to the constraint $\\pi_X + \\pi_Y + \\pi_Z = 1$.\nThe equation $\\boldsymbol{\\pi} P = \\boldsymbol{\\pi}$ expands to:\n$$\n(\\pi_X, \\pi_Y, \\pi_Z)\n\\begin{pmatrix}\np_X & p_Y & p_Z \\\\\np_X & p_Y & p_Z \\\\\np_X & p_Y & p_Z\n\\end{pmatrix}\n= (\\pi_X, \\pi_Y, \\pi_Z)\n$$\nThis gives us a system of linear equations:\n1.  $\\pi_X p_X + \\pi_Y p_X + \\pi_Z p_X = \\pi_X$\n2.  $\\pi_X p_Y + \\pi_Y p_Y + \\pi_Z p_Y = \\pi_Y$\n3.  $\\pi_X p_Z + \\pi_Y p_Z + \\pi_Z p_Z = \\pi_Z$\n\nLet's analyze the first equation:\n$(\\pi_X + \\pi_Y + \\pi_Z) p_X = \\pi_X$\nSince $\\pi_X + \\pi_Y + \\pi_Z = 1$, this simplifies to $p_X = \\pi_X$.\nSimilarly, from the second and third equations, we get $p_Y = \\pi_Y$ and $p_Z = \\pi_Z$.\nThus, the stationary distribution is $\\boldsymbol{\\pi} = (p_X, p_Y, p_Z)$. This is consistent with the constraint $\\sum \\pi_i = p_X + p_Y + p_Z = 1$.\n\nNext, we calculate the entropy of each row of the transition matrix. Let $H_i = H(P_{i,\\cdot}) = -\\sum_{j \\in S} P_{ij} \\log_2(P_{ij})$.\nSince all rows of $P$ are identical, the entropy of each row is the same. Let's calculate the entropy for the first row (state X):\n$$\nH_X = -(p_X \\log_2(p_X) + p_Y \\log_2(p_Y) + p_Z \\log_2(p_Z))\n$$\nSimilarly, for the second and third rows:\n$$\nH_Y = -(p_X \\log_2(p_X) + p_Y \\log_2(p_Y) + p_Z \\log_2(p_Z)) = H_X\n$$\n$$\nH_Z = -(p_X \\log_2(p_X) + p_Y \\log_2(p_Y) + p_Z \\log_2(p_Z)) = H_X\n$$\n\nFinally, we calculate the entropy rate $H(\\mathcal{X})$:\n$$\nH(\\mathcal{X}) = \\pi_X H_X + \\pi_Y H_Y + \\pi_Z H_Z\n$$\nSubstituting the values we found for $\\boldsymbol{\\pi}$ and the row entropies:\n$$\nH(\\mathcal{X}) = p_X H_X + p_Y H_X + p_Z H_X = (p_X + p_Y + p_Z) H_X\n$$\nSince $p_X + p_Y + p_Z = 1$, we have:\n$$\nH(\\mathcal{X}) = 1 \\cdot H_X = H_X\n$$\nSo, the entropy rate of the source is simply the entropy of the common row vector.\n$$\nH(\\mathcal{X}) = -(p_X \\log_2(p_X) + p_Y \\log_2(p_Y) + p_Z \\log_2(p_Z))\n$$\nThis result is expected, as a Markov source whose next state is independent of the current state is equivalent to a memoryless or Independent and Identically Distributed (IID) source. The entropy rate of an IID source is simply the entropy of a single symbol's probability distribution.", "answer": "$$\\boxed{-(p_X \\log_{2}(p_X) + p_Y \\log_{2}(p_Y) + p_Z \\log_{2}(p_Z))}$$", "id": "1621359"}, {"introduction": "Finally, we advance to a more challenging problem that moves beyond straightforward calculation into the realm of analysis and optimization. Here, you will not only compute the entropy rate for a non-symmetrical system but also determine the specific conditions under which this rate is maximized. This exercise [@problem_id:1621362] integrates your knowledge of Markov processes with calculus, providing a deeper insight into how system parameters influence unpredictability and information generation.", "problem": "Consider a discrete-time stochastic process, modeled as a Markov chain, evolving on a set of three states $\\{S_1, S_2, S_3\\}$. The states are arranged in a cycle, and transitions are only possible between adjacent states. The transition probabilities are state-dependent and are defined as follows, where \"clockwise\" refers to the sequence of transitions $S_1 \\to S_2 \\to S_3 \\to S_1$:\n\n- From state $S_1$, the probability of moving clockwise to state $S_2$ is given by a parameter $p$.\n- From state $S_2$, the probability of moving clockwise to state $S_3$ is $1/2$.\n- From state $S_3$, the probability of moving clockwise to state $S_1$ is $1-p$.\n\nFor any given state, the probability of moving counter-clockwise is one minus the probability of moving clockwise. The parameter $p$ is a real number within the open interval $(0, 1)$.\n\nThe entropy of a discrete random variable $Y$ with probability mass function $p(y)$ is defined as $H(Y) = -\\sum_y p(y) \\log_2(p(y))$. The entropy rate of a stationary Markov chain is defined as the conditional entropy of the next state given the current state, $H(X_{n+1}|X_n)$. This can be calculated using the stationary distribution $\\pi = (\\pi_1, \\pi_2, \\pi_3)$ as $H(X_{n+1}|X_n) = \\sum_{i=1}^{3} \\pi_i H(X_{n+1}|X_n=S_i)$.\n\nAssuming the process has reached its stationary distribution, determine the value of the parameter $p$ that maximizes the entropy rate of this Markov chain. The final answer must be a single real number.", "solution": "Let the states be indexed as $1,2,3$ in the clockwise order. The transition probabilities are:\n- From state $1$: to $2$ with probability $p$ and to $3$ with probability $1-p$.\n- From state $2$: to $3$ with probability $\\frac{1}{2}$ and to $1$ with probability $\\frac{1}{2}$.\n- From state $3$: to $1$ with probability $1-p$ and to $2$ with probability $p$.\n\nThus the transition matrix $P$ (rows: current state, columns: next state) is\n$$\nP=\\begin{pmatrix}\n0 & p & 1-p \\\\\n\\frac{1}{2} & 0 & \\frac{1}{2} \\\\\n1-p & p & 0\n\\end{pmatrix}\n$$.\nThe chain is irreducible and aperiodic for $p\\in(0,1)$, so it has a unique stationary distribution $\\pi=(\\pi_{1},\\pi_{2},\\pi_{3})$ satisfying detailed balance on each edge:\n$$\n\\pi_{1}\\,p=\\pi_{2}\\,\\frac{1}{2},\\qquad \\pi_{2}\\,\\frac{1}{2}=\\pi_{3}\\,p,\\qquad \\pi_{3}\\,(1-p)=\\pi_{1}\\,(1-p).\n$$\nSince $1-p>0$, the third condition gives $\\pi_{3}=\\pi_{1}$. Let $\\pi_{1}=\\pi_{3}=x$. From $\\pi_{1}\\,p=\\pi_{2}\\,\\frac{1}{2}$, we get $\\pi_{2}=2px$. The normalization $2x+\\pi_{2}=1$ then yields\n$$\n2x+2px=1\\;\\Rightarrow\\;x=\\frac{1}{2(1+p)},\\qquad \\pi_{2}=\\frac{p}{1+p}.\n$$\nTherefore,\n$$\n\\pi_{1}=\\pi_{3}=\\frac{1}{2(1+p)},\\qquad \\pi_{2}=\\frac{p}{1+p}.\n$$\n\nThe conditional entropy of the next state given the current state $i$ equals the binary entropy of the outgoing transition probabilities from state $i$. Denote the binary entropy by $H_{b}(q)=-q\\log_{2}q-(1-q)\\log_{2}(1-q)$. Then\n$$\nH(X_{n+1}\\mid X_{n}=S_{1})=H_{b}(p),\\quad H(X_{n+1}\\mid X_{n}=S_{2})=H_{b}\\!\\left(\\frac{1}{2}\\right)=1,\\quad H(X_{n+1}\\mid X_{n}=S_{3})=H_{b}(1-p)=H_{b}(p).\n$$\nThe entropy rate is\n$$\nH(p)=\\sum_{i=1}^{3}\\pi_{i}\\,H(X_{n+1}\\mid X_{n}=S_{i})=(\\pi_{1}+\\pi_{3})H_{b}(p)+\\pi_{2}\\cdot 1\n=\\frac{1}{1+p}H_{b}(p)+\\frac{p}{1+p}=\\frac{H_{b}(p)+p}{1+p}.\n$$\n\nTo maximize $H(p)$ over $p\\in(0,1)$, compute the derivative. First,\n$$\nH_{b}'(p)=\\frac{\\mathrm{d}}{\\mathrm{d}p}\\Big(-p\\log_{2}p-(1-p)\\log_{2}(1-p)\\Big)\n=\\log_{2}\\!\\left(\\frac{1-p}{p}\\right),\n$$\nusing $\\frac{\\mathrm{d}}{\\mathrm{d}p}\\log_{2}p=\\frac{1}{p\\ln 2}$. By the quotient rule,\n$$\nH'(p)=\\frac{\\big(H_{b}'(p)+1\\big)(1+p)-\\big(H_{b}(p)+p\\big)}{(1+p)^{2}}\n=\\frac{(1+p)H_{b}'(p)+1-H_{b}(p)}{(1+p)^{2}}.\n$$\nSetting $H'(p)=0$ gives the critical point condition\n$$\n(1+p)\\log_{2}\\!\\left(\\frac{1-p}{p}\\right)+1-H_{b}(p)=0.\n$$\nAt $p=\\frac{1}{2}$, we have $\\log_{2}\\!\\left(\\frac{1-p}{p}\\right)=\\log_{2}(1)=0$ and $H_{b}\\!\\left(\\frac{1}{2}\\right)=1$, so the condition holds.\n\nTo establish that this critical point is the unique maximizer, differentiate the numerator\n$$\nN(p)=(1+p)H_{b}'(p)+1-H_{b}(p),\n$$\nto find\n$$\nN'(p)=H_{b}'(p)+(1+p)H_{b}''(p)-H_{b}'(p)=(1+p)H_{b}''(p).\n$$\nSince $H_{b}''(p)=\\frac{\\mathrm{d}}{\\mathrm{d}p}\\log_{2}\\!\\left(\\frac{1-p}{p}\\right)=-\\frac{1}{\\ln 2}\\cdot\\frac{1}{p(1-p)}<0$ for $p\\in(0,1)$, it follows that $N'(p)<0$ on $(0,1)$. Hence $N(p)$ is strictly decreasing, with $N(p)\\to +\\infty$ as $p\\to 0^{+}$ and $N(p)\\to -\\infty$ as $p\\to 1^{-}$. Therefore $N(p)$ has exactly one zero in $(0,1)$, namely at $p=\\frac{1}{2}$, where $H'(p)=0$. Since $H'(p)>0$ for small $p$ and $H'(p)<0$ for $p$ near $1$, this critical point is the unique global maximizer.\n\nThus the entropy rate is maximized at $p=\\frac{1}{2}$.", "answer": "$$\\boxed{\\frac{1}{2}}$$", "id": "1621362"}]}