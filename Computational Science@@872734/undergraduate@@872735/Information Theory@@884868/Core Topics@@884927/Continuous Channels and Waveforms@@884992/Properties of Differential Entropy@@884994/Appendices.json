{"hands_on_practices": [{"introduction": "This first exercise provides practice in applying the fundamental definition of differential entropy. We will calculate the entropy for a Laplace distribution, which is often used in signal processing and statistics to model phenomena with sharper peaks and heavier tails than the normal distribution. This practice is essential for building a concrete understanding of how a distribution's specific functional form determines its measure of uncertainty. [@problem_id:1649134]", "problem": "In the field of signal processing and robust statistics, the Laplace distribution is often used to model phenomena where data exhibits heavier tails than the normal distribution, meaning outliers are more probable. Consider a noise signal in a communication channel, represented by a random variable $X$. This noise follows a Laplace distribution with a mean location parameter $\\mu$ and a positive scale parameter $b$. The probability density function (PDF) of $X$ is given by:\n$$ f(x; \\mu, b) = \\frac{1}{2b} \\exp\\left(-\\frac{|x-\\mu|}{b}\\right) $$\nfor all real numbers $x$.\n\nYour task is to calculate the differential entropy, $h(X)$, of this random variable. Differential entropy is a measure of the average uncertainty of a continuous random variable.\n\nExpress your final answer as a symbolic expression in terms of the scale parameter $b$ and the mathematical constant $e$.", "solution": "The differential entropy $h(X)$ of a continuous random variable $X$ with probability density function (PDF) $f(x)$ is defined as:\n$$ h(X) = -\\int_{-\\infty}^{\\infty} f(x) \\ln(f(x)) \\,dx $$\nFirst, let's find the expression for $\\ln(f(x))$ for the given Laplace PDF.\n$$ f(x) = \\frac{1}{2b} \\exp\\left(-\\frac{|x-\\mu|}{b}\\right) $$\nTaking the natural logarithm of both sides, we get:\n$$ \\ln(f(x)) = \\ln\\left(\\frac{1}{2b} \\exp\\left(-\\frac{|x-\\mu|}{b}\\right)\\right) $$\nUsing the properties of logarithms, $\\ln(AB) = \\ln(A) + \\ln(B)$ and $\\ln(\\exp(C)) = C$, we have:\n$$ \\ln(f(x)) = \\ln\\left(\\frac{1}{2b}\\right) - \\frac{|x-\\mu|}{b} = -\\ln(2b) - \\frac{|x-\\mu|}{b} $$\nNow, we substitute this expression into the integral for differential entropy:\n$$ h(X) = -\\int_{-\\infty}^{\\infty} f(x) \\left(-\\ln(2b) - \\frac{|x-\\mu|}{b}\\right) \\,dx $$\nWe can split this into two separate integrals:\n$$ h(X) = -\\int_{-\\infty}^{\\infty} f(x)(-\\ln(2b)) \\,dx - \\int_{-\\infty}^{\\infty} f(x)\\left(-\\frac{|x-\\mu|}{b}\\right) \\,dx $$\n$$ h(X) = \\ln(2b) \\int_{-\\infty}^{\\infty} f(x) \\,dx + \\frac{1}{b} \\int_{-\\infty}^{\\infty} |x-\\mu| f(x) \\,dx $$\nLet's evaluate each term.\n\nFor the first term, the integral $\\int_{-\\infty}^{\\infty} f(x) \\,dx$ is the integral of a probability density function over its entire domain. By definition, this must equal 1.\n$$ \\int_{-\\infty}^{\\infty} f(x) \\,dx = 1 $$\nSo, the first term simplifies to $\\ln(2b)$.\n\nThe second term involves the integral $\\int_{-\\infty}^{\\infty} |x-\\mu| f(x) \\,dx$. This is the definition of the expectation of the random variable $|X-\\mu|$, which is the mean absolute deviation from the mean.\n$$ \\int_{-\\infty}^{\\infty} |x-\\mu| f(x) \\,dx = E[|X-\\mu|] $$\nLet's compute this expectation:\n$$ E[|X-\\mu|] = \\int_{-\\infty}^{\\infty} |x-\\mu| \\frac{1}{2b} \\exp\\left(-\\frac{|x-\\mu|}{b}\\right) \\,dx $$\nTo simplify this integral, we make a substitution $u = x - \\mu$, which means $du = dx$. The limits of integration remain from $-\\infty$ to $\\infty$.\n$$ E[|X-\\mu|] = \\int_{-\\infty}^{\\infty} |u| \\frac{1}{2b} \\exp\\left(-\\frac{|u|}{b}\\right) \\,du $$\nThe integrand, $|u| \\exp(-|u|/b)$, is an even function of $u$. Therefore, we can change the integration interval from $(-\\infty, \\infty)$ to $(0, \\infty)$ and multiply by 2:\n$$ E[|X-\\mu|] = 2 \\int_{0}^{\\infty} u \\frac{1}{2b} \\exp\\left(-\\frac{u}{b}\\right) \\,du = \\frac{1}{b} \\int_{0}^{\\infty} u \\exp\\left(-\\frac{u}{b}\\right) \\,du $$\nThis integral can be solved using integration by parts, $\\int U dV = UV - \\int V dU$.\nLet $U = u$ and $dV = \\exp(-u/b) \\,du$.\nThen $dU = du$ and $V = \\int \\exp(-u/b) \\,du = -b \\exp(-u/b)$.\n$$ \\int_{0}^{\\infty} u \\exp\\left(-\\frac{u}{b}\\right) \\,du = \\left[u \\left(-b \\exp\\left(-\\frac{u}{b}\\right)\\right)\\right]_{0}^{\\infty} - \\int_{0}^{\\infty} \\left(-b \\exp\\left(-\\frac{u}{b}\\right)\\right) \\,du $$\nThe first term evaluates to 0 at both limits:\n- As $u \\to \\infty$, the term $\\lim_{u\\to\\infty} -bu \\exp(-u/b) = 0$ (exponential decay dominates polynomial growth).\n- At $u=0$, the term is $-b(0)\\exp(0) = 0$.\nSo, we are left with the second term:\n$$ b \\int_{0}^{\\infty} \\exp\\left(-\\frac{u}{b}\\right) \\,du = b \\left[-b \\exp\\left(-\\frac{u}{b}\\right)\\right]_{0}^{\\infty} $$\n$$ = b \\left( \\lim_{u\\to\\infty} -b \\exp\\left(-\\frac{u}{b}\\right) - \\left(-b \\exp(0)\\right) \\right) = b(0 - (-b)) = b^2 $$\nTherefore, $E[|X-\\mu|] = \\frac{1}{b} (b^2) = b$. Note that the result is independent of $\\mu$, which is expected as differential entropy is invariant to shifts in location.\n\nNow, we substitute this result back into our expression for $h(X)$:\n$$ h(X) = \\ln(2b) + \\frac{1}{b} E[|X-\\mu|] = \\ln(2b) + \\frac{1}{b}(b) = \\ln(2b) + 1 $$\nFinally, to express the answer in terms of $b$ and $e$, we use the property that $1 = \\ln(e)$:\n$$ h(X) = \\ln(2b) + \\ln(e) = \\ln(2be) $$", "answer": "$$\\boxed{\\ln(2be)}$$", "id": "1649134"}, {"introduction": "Building on direct calculation, this practice explores the relative uncertainty of different probability distributions. By comparing the entropy of a uniform distribution—which represents maximum uncertainty for a fixed interval—with that of a triangular distribution over the same support, we can develop an intuition for how the shape of a probability density function relates to its entropy. This exercise highlights how concentrating probability mass affects the overall uncertainty. [@problem_id:1649112]", "problem": "Consider two continuous random variables, $X$ and $Y$, both supported on the same symmetric interval $[-a, a]$, where $a$ is a positive real constant. The random variable $X$ follows a uniform probability distribution over this interval. The random variable $Y$ follows a symmetric triangular probability distribution over the same interval, with its probability density function peaking at the center of the interval, $x=0$.\n\nThe differential entropy, $h(Z)$, of a continuous random variable $Z$ with probability density function (PDF) $p(z)$ is defined as:\n$$h(Z) = -\\int_{-\\infty}^{\\infty} p(z) \\ln(p(z)) dz$$\nwhere $\\ln$ denotes the natural logarithm. The unit of entropy when using the natural logarithm is nats.\n\nCalculate the difference in the differential entropies of these two random variables, $h(X) - h(Y)$. Express your answer as a closed-form analytic expression.", "solution": "Let $X \\sim \\mathrm{Unif}([-a,a])$. Its probability density function is $p_{X}(x) = \\frac{1}{2a}$ for $x \\in [-a,a]$ and $0$ otherwise. The differential entropy is\n$$\nh(X) = -\\int_{-\\infty}^{\\infty} p_{X}(x)\\ln(p_{X}(x))\\,dx = -\\int_{-a}^{a} \\frac{1}{2a}\\ln\\!\\left(\\frac{1}{2a}\\right)\\,dx = -\\ln\\!\\left(\\frac{1}{2a}\\right)\\int_{-a}^{a}\\frac{1}{2a}\\,dx.\n$$\nSince $\\int_{-a}^{a}\\frac{1}{2a}\\,dx=\\frac{1}{2a}\\cdot 2a=1$, it follows that\n$$\nh(X)= -\\ln\\!\\left(\\frac{1}{2a}\\right)=\\ln(2a).\n$$\n\nLet $Y$ have the symmetric triangular density on $[-a,a]$ peaking at $0$. Write $p_{Y}(y)=c\\left(1-\\frac{|y|}{a}\\right)$ for $|y|\\le a$ and $0$ otherwise. Normalization gives\n$$\n1=\\int_{-a}^{a} c\\left(1-\\frac{|y|}{a}\\right)dy=2c\\int_{0}^{a}\\left(1-\\frac{y}{a}\\right)dy=2c\\left[a-\\frac{a}{2}\\right]=ca,\n$$\nhence $c=\\frac{1}{a}$ and thus $p_{Y}(y)=\\frac{a-|y|}{a^{2}}$ for $|y|\\le a$.\n\nUsing symmetry,\n$$\nh(Y)=-\\int_{-a}^{a} p_{Y}(y)\\ln p_{Y}(y)\\,dy=-2\\int_{0}^{a} \\frac{a-y}{a^{2}}\\ln\\!\\left(\\frac{a-y}{a^{2}}\\right)\\,dy.\n$$\nWith the substitution $t=a-y$, $dt=-dy$, the limits $y=0\\to t=a$, $y=a\\to t=0$, we obtain\n$$\nh(Y)=-2\\int_{0}^{a}\\frac{t}{a^{2}}\\ln\\!\\left(\\frac{t}{a^{2}}\\right)\\,dt.\n$$\nSplit the logarithm: $\\ln\\!\\left(\\frac{t}{a^{2}}\\right)=\\ln t - 2\\ln a$, to get\n$$\nh(Y)=-\\frac{2}{a^{2}}\\left[\\int_{0}^{a} t\\ln t\\,dt - 2\\ln a \\int_{0}^{a} t\\,dt\\right].\n$$\nCompute the needed integrals. By integration by parts with $u=\\ln t$, $dv=t\\,dt$ (so $du=\\frac{1}{t}dt$, $v=\\frac{t^{2}}{2}$),\n$$\n\\int t\\ln t\\,dt=\\frac{t^{2}}{2}\\ln t - \\frac{t^{2}}{4}+C,\n$$\nhence\n$$\n\\int_{0}^{a} t\\ln t\\,dt=\\left[\\frac{t^{2}}{2}\\ln t - \\frac{t^{2}}{4}\\right]_{0}^{a}=\\frac{a^{2}}{2}\\ln a - \\frac{a^{2}}{4},\n$$\nwhere the lower limit uses $\\lim_{t\\to 0^{+}} t^{2}\\ln t=0$. Also,\n$$\n\\int_{0}^{a} t\\,dt=\\frac{a^{2}}{2}.\n$$\nTherefore,\n$$\nh(Y)=-\\frac{2}{a^{2}}\\left[\\left(\\frac{a^{2}}{2}\\ln a - \\frac{a^{2}}{4}\\right) - 2\\ln a \\cdot \\frac{a^{2}}{2}\\right]\n=-\\frac{2}{a^{2}}\\left[\\frac{a^{2}}{2}\\ln a - \\frac{a^{2}}{4} - a^{2}\\ln a\\right]\n=-2\\left[-\\frac{1}{2}\\ln a - \\frac{1}{4}\\right]\n=\\ln a + \\frac{1}{2}.\n$$\n\nFinally, the difference is\n$$\nh(X)-h(Y)=\\ln(2a)-\\left(\\ln a + \\frac{1}{2}\\right)=\\ln 2 - \\frac{1}{2}.\n$$\nThis difference is independent of $a$, as expected from the common scaling of the two distributions.", "answer": "$$\\boxed{\\ln 2 - \\frac{1}{2}}$$", "id": "1649112"}, {"introduction": "Many real-world systems involve transforming random signals, and it is crucial to understand how such transformations affect their information content. This problem challenges you to analyze a non-linear, periodic transformation of a uniformly distributed random variable ($Y = \\cos(X)$). This practice demonstrates the application of the change-of-variables formula for a many-to-one mapping and reveals how a simple initial distribution can lead to a more complex one with its own characteristic entropy. [@problem_id:1649118]", "problem": "A continuous random variable $X$ is uniformly distributed over the interval $[0, L]$, where its Probability Density Function (PDF) is constant. The length of the interval is given by $L = 2N\\pi$, where $N$ is a positive integer. A new random variable $Y$ is generated through the transformation $Y = \\cos(X)$.\n\nYour task is to determine the differential entropy, $h(Y)$, of the random variable $Y$. The differential entropy $h(Z)$ of a continuous random variable $Z$ with PDF $p_Z(z)$ is defined as $h(Z) = - \\int_{-\\infty}^{\\infty} p_Z(z) \\ln(p_Z(z)) dz$.\n\nExpress your answer as a closed-form analytic expression.", "solution": "Let $X$ be uniformly distributed on $[0,L]$ with $L=2N\\pi$, so its PDF is $p_{X}(x)=\\frac{1}{L}$ for $x\\in[0,L]$ and $0$ otherwise. Define $Y=\\cos(X)$. To find $p_{Y}(y)$ for $y\\in(-1,1)$, use the change-of-variables formula for many-to-one transformations: if $Y=g(X)$ with $g(x)=\\cos x$, then\n$$\np_{Y}(y)=\\sum_{x_{k}: \\cos x_{k}=y}\\frac{p_{X}(x_{k})}{|g'(x_{k})|}, \\quad g'(x)=-\\sin x.\n$$\nFor any fixed $y\\in(-1,1)$, the equation $\\cos x=y$ has exactly $2N$ solutions $x_{k}$ in $[0,2N\\pi)$, one pair per period, and for each such solution $|\\sin x_{k}|=\\sqrt{1-y^{2}}$. Therefore,\n$$\np_{Y}(y)=\\frac{2N}{L}\\cdot\\frac{1}{\\sqrt{1-y^{2}}}=\\frac{1}{\\pi\\sqrt{1-y^{2}}}, \\quad y\\in(-1,1),\n$$\nand $p_{Y}(y)=0$ otherwise. This is the arcsine distribution on $[-1,1]$, independent of $N$.\n\nThe differential entropy of $Y$ is\n$$\nh(Y)=-\\int_{-1}^{1}p_{Y}(y)\\,\\ln\\big(p_{Y}(y)\\big)\\,dy\n=-\\int_{-1}^{1}\\frac{1}{\\pi\\sqrt{1-y^{2}}}\\,\\ln\\!\\left(\\frac{1}{\\pi\\sqrt{1-y^{2}}}\\right)dy.\n$$\nUse $\\ln\\!\\left(\\frac{1}{\\pi\\sqrt{1-y^{2}}}\\right)=-\\ln\\pi-\\frac{1}{2}\\ln(1-y^{2})$ to obtain\n$$\nh(Y)=\\int_{-1}^{1}\\frac{1}{\\pi\\sqrt{1-y^{2}}}\\,\\ln\\pi\\,dy+\\frac{1}{2}\\int_{-1}^{1}\\frac{1}{\\pi\\sqrt{1-y^{2}}}\\,\\ln(1-y^{2})\\,dy.\n$$\nSince $\\int_{-1}^{1}\\frac{1}{\\pi\\sqrt{1-y^{2}}}dy=1$, the first term equals $\\ln\\pi$. For the second term, substitute $y=\\cos\\theta$, so $dy=-\\sin\\theta\\,d\\theta$ and $\\sqrt{1-y^{2}}=\\sin\\theta$, which yields\n$$\n\\int_{-1}^{1}\\frac{1}{\\pi\\sqrt{1-y^{2}}}\\,\\ln(1-y^{2})\\,dy\n=\\frac{1}{\\pi}\\int_{0}^{\\pi}\\ln\\big(1-\\cos^{2}\\theta\\big)\\,d\\theta\n=\\frac{1}{\\pi}\\int_{0}^{\\pi}\\ln\\big(\\sin^{2}\\theta\\big)\\,d\\theta.\n$$\nUsing the standard integral $\\int_{0}^{\\pi}\\ln(\\sin\\theta)\\,d\\theta=-\\pi\\ln 2$, we have\n$$\n\\int_{0}^{\\pi}\\ln(\\sin^{2}\\theta)\\,d\\theta=2\\int_{0}^{\\pi}\\ln(\\sin\\theta)\\,d\\theta=-2\\pi\\ln 2.\n$$\nTherefore,\n$$\n\\int_{-1}^{1}\\frac{1}{\\pi\\sqrt{1-y^{2}}}\\,\\ln(1-y^{2})\\,dy=-2\\ln 2.\n$$\nPutting this into the entropy expression,\n$$\nh(Y)=\\ln\\pi+\\frac{1}{2}\\big(-2\\ln 2\\big)=\\ln\\pi-\\ln 2=\\ln\\!\\left(\\frac{\\pi}{2}\\right).\n$$\nThus, the differential entropy $h(Y)$ is $\\ln(\\pi/2)$, independent of $N$.", "answer": "$$\\boxed{\\ln\\!\\left(\\frac{\\pi}{2}\\right)}$$", "id": "1649118"}]}