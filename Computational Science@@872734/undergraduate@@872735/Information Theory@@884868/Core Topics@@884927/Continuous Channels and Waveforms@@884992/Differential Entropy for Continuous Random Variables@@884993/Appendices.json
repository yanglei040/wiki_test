{"hands_on_practices": [{"introduction": "The foundational skill in working with differential entropy is the ability to compute it directly from a given probability density function (PDF). This exercise provides practice with the fundamental definition, $h(X) = -\\int p(x) \\ln p(x) dx$. By working through the calculation for a symmetric triangular distribution, you will reinforce the core integration techniques required to quantify the uncertainty of a continuous random variable [@problem_id:1617715].", "problem": "Consider a continuous random variable $X$ whose Probability Density Function (PDF) is a symmetric triangular distribution. The distribution is centered at $x=0$ and is non-zero only over the interval $[-w, w]$, where $w$ is a positive constant. The PDF is given by:\n$$p(x) = \\begin{cases} \\frac{1}{w} \\left(1 - \\frac{|x|}{w}\\right) & \\text{for } x \\in [-w, w] \\\\ 0 & \\text{otherwise} \\end{cases}$$\nFor a system where this parameter is $w = 5$, calculate the differential entropy $h(X)$ of the random variable. Express your answer as a single closed-form analytic expression.", "solution": "The differential entropy of a continuous random variable with PDF $p(x)$ is defined as\n$$\nh(X) = -\\int_{-\\infty}^{\\infty} p(x)\\,\\ln p(x)\\,dx.\n$$\nFor the symmetric triangular PDF\n$$\np(x) = \\frac{1}{w}\\left(1 - \\frac{|x|}{w}\\right), \\quad x \\in [-w,w],\n$$\nand $p(x)=0$ otherwise, we use symmetry to write\n$$\nh(X) = -2 \\int_{0}^{w} \\frac{1}{w}\\left(1 - \\frac{x}{w}\\right) \\ln\\!\\left[\\frac{1}{w}\\left(1 - \\frac{x}{w}\\right)\\right] dx.\n$$\nSplit the logarithm and integrate term by term:\n$$\nh(X) = -2 \\int_{0}^{w} \\frac{1}{w}\\left(1 - \\frac{x}{w}\\right) \\left[\\ln\\!\\left(\\frac{1}{w}\\right) + \\ln\\!\\left(1 - \\frac{x}{w}\\right)\\right] dx\n$$\n$$\n= -2 \\ln\\!\\left(\\frac{1}{w}\\right) \\int_{0}^{w} \\frac{1}{w}\\left(1 - \\frac{x}{w}\\right) dx \\;-\\; 2 \\int_{0}^{w} \\frac{1}{w}\\left(1 - \\frac{x}{w}\\right) \\ln\\!\\left(1 - \\frac{x}{w}\\right) dx.\n$$\nEvaluate the first integral:\n$$\n\\int_{0}^{w} \\frac{1}{w}\\left(1 - \\frac{x}{w}\\right) dx = \\frac{1}{w}\\left[\\int_{0}^{w} 1\\,dx - \\frac{1}{w}\\int_{0}^{w} x\\,dx\\right] = \\frac{1}{w}\\left[w - \\frac{w^{2}}{2w}\\right] = \\frac{1}{2}.\n$$\nThus the first term equals\n$$\n-2 \\ln\\!\\left(\\frac{1}{w}\\right) \\cdot \\frac{1}{2} = -\\ln\\!\\left(\\frac{1}{w}\\right) = \\ln w.\n$$\nFor the second term, use the substitution $t = 1 - \\frac{x}{w}$, so $dx = -w\\,dt$, and as $x$ goes from $0$ to $w$, $t$ goes from $1$ to $0$. Then\n$$\n-2 \\int_{0}^{w} \\frac{1}{w}\\left(1 - \\frac{x}{w}\\right) \\ln\\!\\left(1 - \\frac{x}{w}\\right) dx\n= -2 \\int_{1}^{0} \\frac{1}{w}\\,t \\ln t\\,(-w\\,dt)\n= -2 \\int_{0}^{1} t \\ln t\\,dt.\n$$\nUse the standard integral $\\int_{0}^{1} t \\ln t\\,dt = -\\frac{1}{4}$ to obtain\n$$\n-2 \\int_{0}^{1} t \\ln t\\,dt = -2 \\left(-\\frac{1}{4}\\right) = \\frac{1}{2}.\n$$\nCombining both parts gives\n$$\nh(X) = \\ln w + \\frac{1}{2}.\n$$\nFor $w = 5$, the differential entropy is\n$$\nh(X) = \\ln 5 + \\frac{1}{2}.\n$$", "answer": "$$\\boxed{\\ln 5 + \\frac{1}{2}}$$", "id": "1617715"}, {"introduction": "In many applications, a random signal or variable is processed through a system, resulting in a transformation. It is crucial to understand how such transformations affect the variable's information content. This practice explores the change in differential entropy under a non-linear mapping ($Y=X^2$), highlighting the essential two-step method: first, derive the PDF of the transformed variable, and second, compute its entropy [@problem_id:1617710].", "problem": "Let $X$ be a continuous random variable that is uniformly distributed over the interval $[-1, 1]$. Its Probability Density Function (PDF), $f_X(x)$, is given by:\n$$ f_X(x) = \\begin{cases} \\frac{1}{2} & \\text{for } -1 \\le x \\le 1 \\\\ 0 & \\text{otherwise} \\end{cases} $$\nA new random variable $Y$ is created through the transformation $Y = X^2$.\n\nThe differential entropy $h(Y)$ of a continuous random variable $Y$ with a PDF $f_Y(y)$ is defined as:\n$$ h(Y) = -\\int_{S} f_Y(y) \\ln(f_Y(y)) \\, dy $$\nwhere $S$ is the support of the random variable $Y$ (the set of all possible values of $Y$), and $\\ln$ denotes the natural logarithm.\n\nCalculate the differential entropy $h(Y)$. Express your answer as a single, closed-form analytic expression.", "solution": "We start from the given $X \\sim \\text{Uniform}([-1,1])$ with\n$$\nf_{X}(x)=\\begin{cases}\n\\frac{1}{2}, & -1 \\le x \\le 1,\\\\\n0, & \\text{otherwise}.\n\\end{cases}\n$$\nDefine $Y=X^{2}$. The support of $Y$ is $S=[0,1]$.\n\nTo find $f_{Y}(y)$, use the change-of-variables formula for transformations with multiple preimages: if $Y=g(X)$ with differentiable $g$ and, for a given $y$, the preimages are $\\{x_{i}(y)\\}$ satisfying $g(x_{i})=y$, then\n$$\nf_{Y}(y)=\\sum_{i}\\frac{f_{X}(x_{i}(y))}{|g'(x_{i}(y))|}.\n$$\nHere $g(x)=x^{2}$, so $g'(x)=2x$. For $y \\in (0,1]$, the roots are $x_{1}(y)=\\sqrt{y}$ and $x_{2}(y)=-\\sqrt{y}$, both within $[-1,1]$. Therefore,\n$$\nf_{Y}(y)=\\frac{f_{X}(\\sqrt{y})}{|2\\sqrt{y}|}+\\frac{f_{X}(-\\sqrt{y})}{|2(-\\sqrt{y})|}\n=\\frac{\\frac{1}{2}}{2\\sqrt{y}}+\\frac{\\frac{1}{2}}{2\\sqrt{y}}\n=\\frac{1}{2\\sqrt{y}}, \\quad 0<y\\le 1,\n$$\nand $f_{Y}(y)=0$ otherwise. This density is properly normalized since\n$$\n\\int_{0}^{1}\\frac{1}{2\\sqrt{y}}\\,dy=\\left[\\sqrt{y}\\right]_{0}^{1}=1.\n$$\n\nThe differential entropy is\n$$\nh(Y)=-\\int_{0}^{1} f_{Y}(y)\\,\\ln\\bigl(f_{Y}(y)\\bigr)\\,dy\n=-\\int_{0}^{1}\\frac{1}{2\\sqrt{y}}\\ln\\!\\left(\\frac{1}{2\\sqrt{y}}\\right)dy.\n$$\nUse the substitution $y=t^{2}$ with $t\\in[0,1]$, so $dy=2t\\,dt$ and $\\frac{1}{2\\sqrt{y}}\\,dy=dt$. Then\n$$\nh(Y)=-\\int_{0}^{1}\\ln\\!\\left(\\frac{1}{2t}\\right)dt\n=\\int_{0}^{1}\\ln(2t)\\,dt.\n$$\nAn antiderivative is $t\\ln(2t)-t$, since $\\frac{d}{dt}\\bigl(t\\ln(2t)-t\\bigr)=\\ln(2t)$. Evaluating,\n$$\nh(Y)=\\left[t\\ln(2t)-t\\right]_{0}^{1}\n=\\left(1\\cdot\\ln 2-1\\right)-\\lim_{t\\to 0^{+}}\\bigl(t\\ln(2t)-t\\bigr).\n$$\nBecause $\\lim_{t\\to 0^{+}}t\\ln(2t)=0$ and $\\lim_{t\\to 0^{+}}t=0$, the lower-limit contribution is $0$. Hence\n$$\nh(Y)=\\ln 2 - 1.\n$$", "answer": "$$\\boxed{\\ln 2 - 1}$$", "id": "1617710"}, {"introduction": "Real-world phenomena rarely involve isolated variables; instead, they are often interconnected. Conditional differential entropy, $h(Y|X)$, is a key tool for analyzing these relationships, quantifying the uncertainty that remains in one variable ($Y$) after the value of another ($X$) is known. This exercise demonstrates how to compute $h(Y|X)$ from a joint PDF defined over a simple geometric region, providing a bridge from single-variable analysis to the study of information in multi-variable systems [@problem_id:1617717].", "problem": "Consider a physical system where the position of a particle is described by two continuous random variables, $X$ and $Y$. The joint probability density function (PDF), denoted by $f(x,y)$, is uniform over a two-dimensional region and zero elsewhere. This region is a right-angled triangle in the $xy$-plane, defined by the vertices at coordinates $(0,0)$, $(1,0)$, and $(1,1)$.\n\nYour task is to quantify the uncertainty in the particle's $Y$ position, given that the $X$ position is known. Specifically, calculate the conditional differential entropy $h(Y|X)$.\n\nPresent your final answer as an analytic expression.", "solution": "The triangular support is $R=\\{(x,y): 0 \\leq x \\leq 1,\\; 0 \\leq y \\leq x\\}$, whose area is $\\frac{1}{2}$. Since the joint PDF is uniform on $R$, its value is the constant\n$$\nf_{X,Y}(x,y)=\\frac{1}{\\text{Area}(R)}=2,\\quad (x,y)\\in R,\n$$\nand $f_{X,Y}(x,y)=0$ otherwise.\n\nThe marginal density of $X$ is\n$$\nf_{X}(x)=\\int_{0}^{x} 2\\,dy=2x,\\quad 0 \\leq x \\leq 1.\n$$\nHence the conditional density of $Y$ given $X=x$ is\n$$\nf_{Y|X}(y|x)=\\frac{f_{X,Y}(x,y)}{f_{X}(x)}=\\frac{2}{2x}=\\frac{1}{x},\\quad 0 \\leq y \\leq x,\n$$\nso $Y|X=x$ is uniform on $[0,x]$.\n\nFor a uniform density on $[0,x]$, the conditional differential entropy is\n$$\nh(Y|X=x)=-\\int_{0}^{x}\\frac{1}{x}\\ln\\!\\left(\\frac{1}{x}\\right)dy=\\ln x.\n$$\nTherefore,\n$$\nh(Y|X)=\\mathbb{E}[\\ln X]=\\int_{0}^{1} 2x\\,\\ln x\\,dx.\n$$\nCompute the integral by parts:\n$$\n\\int x\\ln x\\,dx=\\frac{x^{2}}{2}\\ln x-\\int \\frac{x^{2}}{2}\\cdot\\frac{1}{x}\\,dx=\\frac{x^{2}}{2}\\ln x-\\frac{x^{2}}{4}+C,\n$$\nso\n$$\n\\int_{0}^{1} 2x\\,\\ln x\\,dx=2\\left[\\frac{x^{2}}{2}\\ln x-\\frac{x^{2}}{4}\\right]_{0}^{1}=2\\left(0-\\frac{1}{4}-0\\right)=-\\frac{1}{2}.\n$$\nHence,\n$$\nh(Y|X)=-\\frac{1}{2}.\n$$", "answer": "$$\\boxed{-\\frac{1}{2}}$$", "id": "1617717"}]}