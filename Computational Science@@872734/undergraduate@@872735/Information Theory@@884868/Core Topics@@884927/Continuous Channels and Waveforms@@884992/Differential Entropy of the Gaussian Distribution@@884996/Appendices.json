{"hands_on_practices": [{"introduction": "Comparing different types of probability distributions is fundamental to understanding their respective information content. This practice explores the concept of differential entropy by contrasting the ubiquitous Gaussian distribution with the simple uniform distribution. By finding the specific variance that makes their entropies equal, you will gain a deeper intuition for how a distribution's spread, or variance, directly relates to its information-theoretic uncertainty and discover the conditions under which differential entropy can be zero [@problem_id:1617971].", "problem": "In the field of information theory, the differential entropy of a continuous random variable provides a measure of its average uncertainty. Consider two distinct random variables, $X$ and $Y$. The variable $X$ is governed by a Gaussian distribution with a mean of $\\mu = 0$ and an unknown positive variance $\\sigma^2$. The variable $Y$ is described by a continuous uniform distribution over the interval $[0, 1]$.\n\nYour task is to determine the exact value of the variance $\\sigma^2$ such that the differential entropy of the Gaussian random variable $X$ is identical to the differential entropy of the uniform random variable $Y$. Express your answer as a symbolic, closed-form expression.", "solution": "We work in nats, using the natural logarithm. The differential entropy of a Gaussian $X \\sim \\mathcal{N}(0,\\sigma^{2})$ is given by the standard formula\n$$\nh(X)=\\frac{1}{2}\\ln\\!\\left(2\\pi e\\,\\sigma^{2}\\right).\n$$\nFor a continuous uniform random variable $Y$ on $[a,b]$, the differential entropy is\n$$\nh(Y)=\\ln(b-a).\n$$\nWith $Y \\sim \\text{Unif}[0,1]$, we have $b-a=1$, hence\n$$\nh(Y)=\\ln(1)=0.\n$$\nSetting the entropies equal yields\n$$\n\\frac{1}{2}\\ln\\!\\left(2\\pi e\\,\\sigma^{2}\\right)=0.\n$$\nMultiplying both sides by $2$ gives\n$$\n\\ln\\!\\left(2\\pi e\\,\\sigma^{2}\\right)=0.\n$$\nUsing the fact that $\\ln(u)=0$ if and only if $u=\\exp(0)=1$, we obtain\n$$\n2\\pi e\\,\\sigma^{2}=1.\n$$\nSolving for $\\sigma^{2}$,\n$$\n\\sigma^{2}=\\frac{1}{2\\pi e}.\n$$\nThis value is positive, consistent with the requirement that the variance be positive.", "answer": "$$\\boxed{\\frac{1}{2\\pi e}}$$", "id": "1617971"}, {"introduction": "In experimental science and signal processing, averaging multiple measurements is a standard technique to reduce noise and increase precision. This exercise demonstrates the information-theoretic consequence of this practice, showing how the differential entropy of a sample mean changes as more data points are included. You will see firsthand how increasing the sample size, $n$, systematically reduces the uncertainty of an estimate, a cornerstone of both statistics and information theory [@problem_id:1618009].", "problem": "In digital signal processing and data analysis, a common technique to improve measurement precision is to average multiple independent readings. This process tends to reduce the effect of random noise.\n\nConsider a scenario where a scientist collects $n$ measurements, which are modeled as independent and identically distributed (i.i.d.) random variables $X_1, X_2, \\dots, X_n$. Each measurement $X_i$ is known to follow a standard normal distribution, which is a Gaussian distribution with a mean of 0 and a variance of 1.\n\nThe scientist computes an aggregated measurement, $\\bar{X}_n$, by taking the sample mean of these $n$ values:\n$$\n\\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^{n} X_i\n$$\nThe uncertainty or \"surprise\" associated with this new random variable $\\bar{X}_n$ can be quantified by its differential entropy. For a general Gaussian random variable $Y$ with variance $\\sigma^2$, its differential entropy $h(Y)$ is given by the formula:\n$$\nh(Y) = \\frac{1}{2} \\ln(2\\pi e \\sigma^2)\n$$\nwhere $\\ln$ denotes the natural logarithm.\n\nDetermine the differential entropy of the sample mean, $h(\\bar{X}_n)$, as a function of the number of measurements, $n$. Your answer should be a symbolic expression.", "solution": "We are given independent identically distributed $X_{1},\\dots,X_{n}$ with $X_{i}\\sim \\mathcal{N}(0,1)$ and the sample mean $\\bar{X}_{n}=\\frac{1}{n}\\sum_{i=1}^{n}X_{i}$. A linear combination of independent Gaussian random variables is Gaussian, so $\\bar{X}_{n}$ is Gaussian. Its mean is\n$$\n\\mathbb{E}[\\bar{X}_{n}]=\\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}[X_{i}]=\\frac{1}{n}\\cdot n\\cdot 0=0.\n$$\nUsing independence and the variance scaling property $\\operatorname{Var}(cZ)=c^{2}\\operatorname{Var}(Z)$, its variance is\n$$\n\\operatorname{Var}(\\bar{X}_{n})=\\operatorname{Var}\\!\\left(\\frac{1}{n}\\sum_{i=1}^{n}X_{i}\\right)=\\frac{1}{n^{2}}\\sum_{i=1}^{n}\\operatorname{Var}(X_{i})=\\frac{1}{n^{2}}\\cdot n\\cdot 1=\\frac{1}{n}.\n$$\nFor a Gaussian random variable $Y$ with variance $\\sigma^{2}$, the differential entropy is\n$$\nh(Y)=\\frac{1}{2}\\ln\\!\\left(2\\pi e\\,\\sigma^{2}\\right).\n$$\nSubstituting $\\sigma^{2}=\\frac{1}{n}$ for $Y=\\bar{X}_{n}$ gives\n$$\nh(\\bar{X}_{n})=\\frac{1}{2}\\ln\\!\\left(2\\pi e\\,\\frac{1}{n}\\right)=\\frac{1}{2}\\ln\\!\\left(\\frac{2\\pi e}{n}\\right).\n$$\nThis expresses the differential entropy of the sample mean as a function of $n$.", "answer": "$$\\boxed{\\frac{1}{2}\\ln\\!\\left(\\frac{2\\pi e}{n}\\right)}$$", "id": "1618009"}, {"introduction": "Real-world systems, from wireless communication to data storage, always operate under resource limitations like power or bandwidth. This practice moves from merely calculating entropy to using it as a tool for optimization, a task central to engineering design. You will solve for the optimal allocation of power between two independent channels to maximize the total information throughput, illustrating a key principle in communication theory and providing a glimpse into resource allocation problems [@problem_id:1617989].", "problem": "Consider a dual-channel communication system designed to transmit two independent signals. These signals are modeled as zero-mean Gaussian random variables, $X_1$ and $X_2$, with powers corresponding to their variances, $\\sigma_1^2$ and $\\sigma_2^2$, respectively.\n\nThe system operates under a resource constraint. The total weighted power consumption must not exceed a total budget $P$. This relationship is expressed as $a_1 \\sigma_1^2 + a_2 \\sigma_2^2 = P$, where $a_1 > 0$ and $a_2 > 0$ are given positive constants representing the resource cost per unit of power for each channel.\n\nTo maximize the total information throughput, one must maximize the joint differential entropy of the two signals, $h(X_1, X_2)$. For reference, the differential entropy of a single Gaussian random variable $X$ with variance $\\sigma^2$ is given by the formula $h(X) = \\frac{1}{2} \\ln(2\\pi e \\sigma^2)$.\n\nDetermine the values of the individual variances, $\\sigma_1^2$ and $\\sigma_2^2$, that maximize the joint differential entropy subject to the power budget constraint. Present your answers for $\\sigma_1^2$ and then $\\sigma_2^2$.", "solution": "We are given two independent, zero-mean Gaussian random variables $X_{1}$ and $X_{2}$ with variances $\\sigma_{1}^{2}$ and $\\sigma_{2}^{2}$. For independent variables, the joint differential entropy equals the sum of individual entropies:\n$$\nh(X_{1},X_{2})=h(X_{1})+h(X_{2}).\n$$\nFor a Gaussian random variable with variance $\\sigma^{2}$, $h(X)=\\frac{1}{2}\\ln\\!\\big(2\\pi e\\,\\sigma^{2}\\big)$. Therefore,\n$$\nh(X_{1},X_{2})=\\frac{1}{2}\\ln\\!\\big(2\\pi e\\,\\sigma_{1}^{2}\\big)+\\frac{1}{2}\\ln\\!\\big(2\\pi e\\,\\sigma_{2}^{2}\\big)\n=\\ln(2\\pi e)+\\frac{1}{2}\\ln\\!\\big(\\sigma_{1}^{2}\\sigma_{2}^{2}\\big).\n$$\nMaximizing $h(X_{1},X_{2})$ over $\\sigma_{1}^{2},\\sigma_{2}^{2}>0$ subject to the resource constraint $a_{1}\\sigma_{1}^{2}+a_{2}\\sigma_{2}^{2}=P$ is equivalent to maximizing $\\ln(\\sigma_{1}^{2})+\\ln(\\sigma_{2}^{2})$ under the same constraint, since constants do not affect the optimizer. This is a concave maximization with a linear equality constraint; we use Lagrange multipliers.\n\nDefine\n$$\n\\mathcal{L}=\\frac{1}{2}\\ln(\\sigma_{1}^{2})+\\frac{1}{2}\\ln(\\sigma_{2}^{2})-\\lambda\\big(a_{1}\\sigma_{1}^{2}+a_{2}\\sigma_{2}^{2}-P\\big).\n$$\nStationarity conditions are\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\sigma_{1}^{2}}=\\frac{1}{2}\\frac{1}{\\sigma_{1}^{2}}-\\lambda a_{1}=0\n\\quad\\Rightarrow\\quad \\sigma_{1}^{2}=\\frac{1}{2\\lambda a_{1}},\n$$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\sigma_{2}^{2}}=\\frac{1}{2}\\frac{1}{\\sigma_{2}^{2}}-\\lambda a_{2}=0\n\\quad\\Rightarrow\\quad \\sigma_{2}^{2}=\\frac{1}{2\\lambda a_{2}}.\n$$\nImposing the constraint $a_{1}\\sigma_{1}^{2}+a_{2}\\sigma_{2}^{2}=P$ gives\n$$\na_{1}\\left(\\frac{1}{2\\lambda a_{1}}\\right)+a_{2}\\left(\\frac{1}{2\\lambda a_{2}}\\right)=P\n\\;\\Rightarrow\\;\n\\frac{1}{2\\lambda}+\\frac{1}{2\\lambda}=P\n\\;\\Rightarrow\\;\n\\frac{1}{\\lambda}=P\n\\;\\Rightarrow\\;\n\\lambda=\\frac{1}{P}.\n$$\nSubstituting back yields\n$$\n\\sigma_{1}^{2}=\\frac{P}{2a_{1}},\\qquad \\sigma_{2}^{2}=\\frac{P}{2a_{2}}.\n$$\nBecause the objective is strictly concave in $(\\sigma_{1}^{2},\\sigma_{2}^{2})$ and the feasible set is affine with $a_{1},a_{2}>0$, this stationary point is the unique global maximizer, and it uses the full budget.", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{P}{2 a_{1}} & \\frac{P}{2 a_{2}}\\end{pmatrix}}$$", "id": "1617989"}]}