{"hands_on_practices": [{"introduction": "At the heart of any lossy compression scheme is the concept of distortion—a measure of the error introduced by the approximation. This first exercise grounds your understanding by tasking you with calculating the squared Euclidean distortion, a standard metric used to quantify the difference between an original vector and its quantized counterpart [@problem_id:1667391]. Mastering this calculation is the first step toward evaluating and optimizing quantizer performance.", "problem": "In digital signal processing, Vector Quantization (VQ) is a technique used for data compression. It works by mapping a set of high-dimensional vectors to a smaller, finite set of representative vectors called a 'codebook'. When a specific input vector is to be encoded, it is replaced by the 'closest' vector from the codebook.\n\nConsider a simple 2-dimensional sensor system that measures two related physical quantities, producing a data vector $\\mathbf{x}$. A specific measurement yields the vector $\\mathbf{x} = [1.2, 3.8]$. For compression, this vector is quantized using a codebook. The closest codebook vector assigned to $\\mathbf{x}$ is found to be $\\mathbf{c} = [1.0, 4.0]$.\n\nThe error or 'distortion' introduced by this quantization process can be measured in several ways. One common metric is the squared Euclidean distortion, which is the square of the standard Euclidean distance between the original vector and its quantized representation.\n\nCalculate the squared Euclidean distortion between the original data vector $\\mathbf{x}$ and its corresponding codebook vector $\\mathbf{c}$. Express your answer as a numerical value.", "solution": "The squared Euclidean distortion between a data vector $\\mathbf{x}$ and its quantized codebook vector $\\mathbf{c}$ is defined as the squared norm of their difference:\n$$\nd^{2} = \\|\\mathbf{x} - \\mathbf{c}\\|^{2} = \\sum_{i=1}^{n} (x_{i} - c_{i})^{2}.\n$$\nFor $\\mathbf{x} = [1.2, 3.8]$ and $\\mathbf{c} = [1.0, 4.0]$, compute the component-wise differences:\n$$\nx_{1} - c_{1} = 1.2 - 1.0 = 0.2, \\quad x_{2} - c_{2} = 3.8 - 4.0 = -0.2.\n$$\nSquare and sum:\n$$\nd^{2} = (0.2)^{2} + (-0.2)^{2} = 0.04 + 0.04 = 0.08.\n$$\nTherefore, the squared Euclidean distortion is $0.08$.", "answer": "$$\\boxed{0.08}$$", "id": "1667391"}, {"introduction": "Vector quantization encodes data by replacing an input vector with the 'closest' vector from a finite codebook. This practice puts you in the driver's seat of the encoding process, where you will apply the nearest neighbor rule to determine the appropriate codevector for a given data point [@problem_id:1667384]. This partitioning of space into regions, known as Voronoi cells, is the fundamental mechanism by which VQ operates.", "problem": "In a simplified model for lossy data compression, a 2-dimensional vector quantizer is used to represent input data points. The quantizer has a fixed codebook containing four representative vectors (codewords). The process involves mapping any given input vector to the closest codeword in the codebook, where \"closeness\" is determined by the standard Euclidean distance. The region of space containing all input vectors that are closer to a particular codeword than to any other is known as that codeword's Voronoi cell.\n\nThe codebook for this system is defined by the following four vectors:\n$\\mathbf{c}_1 = (-2.0, -3.0)$\n$\\mathbf{c}_2 = (4.0, 1.0)$\n$\\mathbf{c}_3 = (-1.0, 5.0)$\n$\\mathbf{c}_4 = (3.0, -4.0)$\n\nAn input data vector $\\mathbf{x} = (1.5, 2.0)$ is received. To which codeword's Voronoi cell does this input vector belong?\n\nA. $\\mathbf{c}_1 = (-2.0, -3.0)$\n\nB. $\\mathbf{c}_2 = (4.0, 1.0)$\n\nC. $\\mathbf{c}_3 = (-1.0, 5.0)$\n\nD. $\\mathbf{c}_4 = (3.0, -4.0)$", "solution": "We determine the nearest codeword using the Euclidean distance. For a codeword $\\mathbf{c}_{i} = (c_{i1}, c_{i2})$ and input $\\mathbf{x} = (x_{1}, x_{2})$, the Euclidean distance is\n$$\nd(\\mathbf{x}, \\mathbf{c}_{i}) = \\sqrt{(x_{1} - c_{i1})^{2} + (x_{2} - c_{i2})^{2}}.\n$$\nSince the square root is strictly increasing, we can compare squared distances:\n$$\nd^{2}(\\mathbf{x}, \\mathbf{c}_{i}) = (x_{1} - c_{i1})^{2} + (x_{2} - c_{i2})^{2}.\n$$\nCompute for each codeword with $\\mathbf{x} = (1.5, 2.0)$:\n- For $\\mathbf{c}_{1} = (-2.0, -3.0)$:\n$$\nd^{2}(\\mathbf{x}, \\mathbf{c}_{1}) = (1.5 - (-2.0))^{2} + (2.0 - (-3.0))^{2} = 3.5^{2} + 5.0^{2} = 12.25 + 25 = 37.25.\n$$\n- For $\\mathbf{c}_{2} = (4.0, 1.0)$:\n$$\nd^{2}(\\mathbf{x}, \\mathbf{c}_{2}) = (1.5 - 4.0)^{2} + (2.0 - 1.0)^{2} = (-2.5)^{2} + 1.0^{2} = 6.25 + 1 = 7.25.\n$$\n- For $\\mathbf{c}_{3} = (-1.0, 5.0)$:\n$$\nd^{2}(\\mathbf{x}, \\mathbf{c}_{3}) = (1.5 - (-1.0))^{2} + (2.0 - 5.0)^{2} = 2.5^{2} + (-3.0)^{2} = 6.25 + 9 = 15.25.\n$$\n- For $\\mathbf{c}_{4} = (3.0, -4.0)$:\n$$\nd^{2}(\\mathbf{x}, \\mathbf{c}_{4}) = (1.5 - 3.0)^{2} + (2.0 - (-4.0))^{2} = (-1.5)^{2} + 6.0^{2} = 2.25 + 36 = 38.25.\n$$\nThe smallest squared distance is $7.25$, corresponding to $\\mathbf{c}_{2} = (4.0, 1.0)$. Therefore, $\\mathbf{x}$ lies in the Voronoi cell of $\\mathbf{c}_{2}$, which is option B.", "answer": "$$\\boxed{B}$$", "id": "1667384"}, {"introduction": "A quantizer is only as good as its codebook, but where do these codebooks come from? This exercise provides hands-on experience with the renowned Linde-Buzo-Gray (LBG) algorithm, an iterative method for generating an optimal codebook from a training dataset [@problem_id:1667388]. By performing a full iteration—partitioning the data and updating the centroids—you will gain practical insight into how quantizers are designed and refined.", "problem": "In the field of data compression and clustering, the Linde-Buzo-Gray (LBG) algorithm is a classic method for designing a vector quantizer. It iteratively refines a set of representative points, called a codebook, to better represent a dataset.\n\nConsider a simplified two-dimensional dataset consisting of three points: $\\mathbf{x}_1 = (2, 2)$, $\\mathbf{x}_2 = (3, 1)$, and $\\mathbf{x}_3 = (8, 9)$. We want to quantize these points using a codebook of size two. The initial codebook consists of two centroids: $\\mathbf{c}_1 = (1, 1)$ and $\\mathbf{c}_2 = (10, 10)$.\n\nPerform one full iteration of the LBG algorithm, which consists of two steps:\n1.  **Partitioning:** Assign each data point $\\mathbf{x}_i$ to the closest centroid $\\mathbf{c}_j$ in the current codebook. The distance is measured using the squared Euclidean distance, defined as $d^2(\\mathbf{x}, \\mathbf{c}) = ||\\mathbf{x} - \\mathbf{c}||^2$. This creates two disjoint sets of data points, one for each initial centroid.\n2.  **Centroid Update:** For each partition, calculate a new centroid by finding the arithmetic mean (the center of mass) of all the data points assigned to it.\n\nWhich of the following pairs represents the new centroids after this single iteration? The pair of new centroids is presented as $(\\mathbf{c}'_a, \\mathbf{c}'_b)$, ordered such that the x-coordinate of $\\mathbf{c}'_a$ is less than the x-coordinate of $\\mathbf{c}'_b$.\n\nA. $((2.5, 1.5), (8, 9))$\n\nB. $((2, 2), (5.5, 5))$\n\nC. $((3, 1), (5, 5.5))$\n\nD. $((2, 2.5), (9, 8))$\n\nE. $((5, 3), (8, 9))$", "solution": "The problem asks for the new set of centroids after one full iteration of the Linde-Buzo-Gray (LBG) algorithm. This iteration consists of a partitioning step followed by a centroid update step.\n\nThe initial centroids are $\\mathbf{c}_1 = (1, 1)$ and $\\mathbf{c}_2 = (10, 10)$.\nThe data points are $\\mathbf{x}_1 = (2, 2)$, $\\mathbf{x}_2 = (3, 1)$, and $\\mathbf{x}_3 = (8, 9)$.\n\n**Step 1: Partitioning**\n\nWe must assign each data point to the nearest centroid using the squared Euclidean distance, $d^2(\\mathbf{x}, \\mathbf{c}) = (x_x - c_x)^2 + (x_y - c_y)^2$. Let's create two partitions, $S_1$ and $S_2$, corresponding to the initial centroids $\\mathbf{c}_1$ and $\\mathbf{c}_2$.\n\nFor data point $\\mathbf{x}_1 = (2, 2)$:\n- Distance to $\\mathbf{c}_1$: $d^2(\\mathbf{x}_1, \\mathbf{c}_1) = (2 - 1)^2 + (2 - 1)^2 = 1^2 + 1^2 = 2$.\n- Distance to $\\mathbf{c}_2$: $d^2(\\mathbf{x}_1, \\mathbf{c}_2) = (2 - 10)^2 + (2 - 10)^2 = (-8)^2 + (-8)^2 = 64 + 64 = 128$.\nSince $2 < 128$, $\\mathbf{x}_1$ is assigned to the partition of $\\mathbf{c}_1$. So, $\\mathbf{x}_1 \\in S_1$.\n\nFor data point $\\mathbf{x}_2 = (3, 1)$:\n- Distance to $\\mathbf{c}_1$: $d^2(\\mathbf{x}_2, \\mathbf{c}_1) = (3 - 1)^2 + (1 - 1)^2 = 2^2 + 0^2 = 4$.\n- Distance to $\\mathbf{c}_2$: $d^2(\\mathbf{x}_2, \\mathbf{c}_2) = (3 - 10)^2 + (1 - 10)^2 = (-7)^2 + (-9)^2 = 49 + 81 = 130$.\nSince $4 < 130$, $\\mathbf{x}_2$ is also assigned to the partition of $\\mathbf{c}_1$. So, $\\mathbf{x}_2 \\in S_1$.\n\nFor data point $\\mathbf{x}_3 = (8, 9)$:\n- Distance to $\\mathbf{c}_1$: $d^2(\\mathbf{x}_3, \\mathbf{c}_1) = (8 - 1)^2 + (9 - 1)^2 = 7^2 + 8^2 = 49 + 64 = 113$.\n- Distance to $\\mathbf{c}_2$: $d^2(\\mathbf{x}_3, \\mathbf{c}_2) = (8 - 10)^2 + (9 - 10)^2 = (-2)^2 + (-1)^2 = 4 + 1 = 5$.\nSince $5 < 113$, $\\mathbf{x}_3$ is assigned to the partition of $\\mathbf{c}_2$. So, $\\mathbf{x}_3 \\in S_2$.\n\nAfter the partitioning step, the two partitions are:\n$S_1 = \\{\\mathbf{x}_1, \\mathbf{x}_2\\} = \\{(2, 2), (3, 1)\\}$\n$S_2 = \\{\\mathbf{x}_3\\} = \\{(8, 9)\\}$\n\n**Step 2: Centroid Update**\n\nNow, we compute the new centroids, $\\mathbf{c}'_1$ and $\\mathbf{c}'_2$, by finding the arithmetic mean of the points in each partition.\n\nFor partition $S_1$:\nThe new centroid $\\mathbf{c}'_1$ is the average of the points in $S_1$.\n$$ \\mathbf{c}'_1 = \\frac{\\mathbf{x}_1 + \\mathbf{x}_2}{2} = \\frac{(2, 2) + (3, 1)}{2} = \\frac{(2+3, 2+1)}{2} = \\frac{(5, 3)}{2} = (2.5, 1.5) $$\n\nFor partition $S_2$:\nThe new centroid $\\mathbf{c}'_2$ is the average of the points in $S_2$. Since there is only one point, the centroid is the point itself.\n$$ \\mathbf{c}'_2 = \\frac{\\mathbf{x}_3}{1} = (8, 9) $$\n\nThe new centroids are $(2.5, 1.5)$ and $(8, 9)$.\n\nThe problem asks for the pair of new centroids $(\\mathbf{c}'_a, \\mathbf{c}'_b)$ ordered by their x-coordinate. The x-coordinate of $(2.5, 1.5)$ is $2.5$, and the x-coordinate of $(8, 9)$ is $8$. Since $2.5 < 8$, the ordered pair is $((2.5, 1.5), (8, 9))$.\n\nComparing this result with the given options:\nA. $((2.5, 1.5), (8, 9))$ - This matches our result.\nB. $((2, 2), (5.5, 5))$\nC. $((3, 1), (5, 5.5))$\nD. $((2, 2.5), (9, 8))$\nE. $((5, 3), (8, 9))$\n\nTherefore, the correct option is A.", "answer": "$$\\boxed{A}$$", "id": "1667388"}]}