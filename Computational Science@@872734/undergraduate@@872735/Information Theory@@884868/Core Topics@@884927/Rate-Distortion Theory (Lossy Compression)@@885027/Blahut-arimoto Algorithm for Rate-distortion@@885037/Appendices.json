{"hands_on_practices": [{"introduction": "To truly understand an algorithm, there's no substitute for walking through its steps yourself. This first exercise guides you through a single, complete iteration of the Blahut-Arimoto algorithm, starting from a simple uniform distribution [@problem_id:1605365]. By manually applying the update rules, you will gain a concrete feel for how the channel probabilities and output distributions evolve to balance rate and distortion.", "problem": "Consider a communication problem involving a binary source, where the source alphabet is $\\mathcal{X} = \\{0, 1\\}$. The source is asymmetric, generating symbols with probabilities $p(x=0) = 0.75$ and $p(x=1) = 0.25$. We wish to represent the source output using a reproduction alphabet $\\mathcal{\\hat{X}} = \\{0, 1\\}$. The quality of the reproduction is measured by the Hamming distortion, defined as $d(x, \\hat{x}) = 0$ if $x = \\hat{x}$ and $d(x, \\hat{x}) = 1$ if $x \\neq \\hat{x}$.\n\nThe rate-distortion function for this source can be computed numerically using the Blahut-Arimoto algorithm. This algorithm iteratively refines a conditional distribution $q(\\hat{x}|x)$ and a reproduction distribution $q(\\hat{x})$ to find a point on the rate-distortion curve. The iterative updates depend on a trade-off parameter $\\beta > 0$. For an iteration step $k$, the update rules are:\n\n1.  Update the conditional distribution:\n    $$ q_{k+1}(\\hat{x}|x) = \\frac{q_k(\\hat{x}) \\exp(-\\beta d(x, \\hat{x}))}{\\sum_{\\hat{x}' \\in \\mathcal{\\hat{X}}} q_k(\\hat{x}') \\exp(-\\beta d(x, \\hat{x}'))} $$\n\n2.  Update the reproduction distribution:\n    $$ q_{k+1}(\\hat{x}) = \\sum_{x \\in \\mathcal{X}} p(x) q_{k+1}(\\hat{x}|x) $$\n\nSuppose we initialize the algorithm (at step $k=0$) with a uniform reproduction distribution, $q_0(\\hat{x}=0) = q_0(\\hat{x}=1) = 0.5$. Using a parameter value of $\\beta = 2.0$, perform one full iteration of the algorithm to find the updated reproduction distribution $q_1(\\hat{x})$.\n\nCalculate the value of $q_1(\\hat{x}=0)$. Express your answer as a decimal rounded to four significant figures.", "solution": "We are given a binary source with $p(0)=0.75$ and $p(1)=0.25$, Hamming distortion $d(x,\\hat{x})$, an initial reproduction distribution $q_{0}(\\hat{x}=0)=q_{0}(\\hat{x}=1)=0.5$, and $\\beta=2.0$. The Blahut-Arimoto conditional update is\n$$\nq_{1}(\\hat{x}\\mid x)=\\frac{q_{0}(\\hat{x})\\exp(-\\beta d(x,\\hat{x}))}{\\sum_{\\hat{x}'\\in\\{0,1\\}}q_{0}(\\hat{x}')\\exp(-\\beta d(x,\\hat{x}'))}.\n$$\nLet $E'=\\exp(-2)$. For $x=0$, $d(0,0)=0$ and $d(0,1)=1$, so\n$$\nq_{1}(0\\mid 0)=\\frac{0.5\\cdot \\exp(0)}{0.5\\cdot \\exp(0)+0.5\\cdot \\exp(-2)}=\\frac{1}{1+E'},\\quad\nq_{1}(1\\mid 0)=\\frac{0.5\\cdot \\exp(-2)}{0.5\\cdot \\exp(0)+0.5\\cdot \\exp(-2)}=\\frac{E'}{1+E'}.\n$$\nFor $x=1$, $d(1,1)=0$ and $d(1,0)=1$, yielding\n$$\nq_{1}(0\\mid 1)=\\frac{E'}{1+E'},\\quad q_{1}(1\\mid 1)=\\frac{1}{1+E'}.\n$$\nThe reproduction update is\n$$\nq_{1}(\\hat{x})=\\sum_{x\\in\\{0,1\\}}p(x)\\,q_{1}(\\hat{x}\\mid x).\n$$\nThus\n$$\nq_{1}(0)=p(0)\\,q_{1}(0\\mid 0)+p(1)\\,q_{1}(0\\mid 1)=0.75 \\cdot \\frac{1}{1+E'} + 0.25 \\cdot \\frac{E'}{1+E'}=\\frac{0.75+0.25E'}{1+E'}.\n$$\nWith $E'=\\exp(-2)\\approx 0.135335$, we obtain\n$$\nq_{1}(0)=\\frac{0.75+0.25(\\exp(-2))}{1+\\exp(-2)}\\approx 0.6903985,\n$$\nwhich rounded to four significant figures is $0.6904$.", "answer": "$$\\boxed{0.6904}$$", "id": "1605365"}, {"introduction": "The power of the Blahut-Arimoto algorithm lies in its iterative nature, which allows it to converge towards an optimal solution. This practice takes you a step further by having you perform multiple iterations, allowing you to witness this convergence firsthand for a symmetric source [@problem_id:1605389]. You will then calculate the mutual information, connecting the final channel distribution to the achievable compression rate, $R(D)$.", "problem": "Consider a binary memoryless source that generates symbols from the alphabet $\\mathcal{X} = \\{0, 1\\}$. The symbols are produced with equal probability, such that $p(X=0) = 1/2$ and $p(X=1) = 1/2$. We wish to compress this source and reconstruct it using the alphabet $\\hat{\\mathcal{X}} = \\{0, 1\\}$. The quality of the reconstruction is measured by the Hamming distortion, defined as $d(x, \\hat{x}) = 0$ if $x = \\hat{x}$ and $d(x, \\hat{x}) = 1$ if $x \\neq \\hat{x}$.\n\nThe Blahut-Arimoto algorithm can be used to iteratively find the optimal conditional probability distribution $q(\\hat{x}|x)$ that minimizes the mutual information $I(X; \\hat{X})$ for a given distortion level. For a fixed trade-off parameter $\\beta > 0$, the algorithm updates a distribution $q_k(\\hat{x}|x)$ to $q_{k+1}(\\hat{x}|x)$ via the following two steps:\n\n1.  First, compute the marginal output distribution based on the current channel $q_k(\\hat{x}|x)$:\n    $$ q_k(\\hat{x}) = \\sum_{x \\in \\mathcal{X}} p(x) q_k(\\hat{x}|x) $$\n2.  Then, update the channel distribution using the marginal from the previous step:\n    $$ q_{k+1}(\\hat{x}|x) = \\frac{q_k(\\hat{x}) \\exp(-\\beta d(x, \\hat{x}))}{\\sum_{\\hat{x}' \\in \\hat{\\mathcal{X}}} q_k(\\hat{x}') \\exp(-\\beta d(x, \\hat{x}'))} $$\n\nYour task is to analyze this process for a specific scenario. Let the parameter be $\\beta = \\ln(9)$. Start with the initial distribution $q_0(\\hat{x}|x) = 1/2$ for all combinations of $x \\in \\mathcal{X}$ and $\\hat{x} \\in \\hat{\\mathcal{X}}$.\n\nPerform two full iterations of the Blahut-Arimoto algorithm to determine the distribution $q_2(\\hat{x}|x)$. Subsequently, calculate the mutual information $I_2(X; \\hat{X})$ corresponding to this distribution. The mutual information, which represents the achievable compression rate, is given by the formula:\n$$ I(X; \\hat{X}) = \\sum_{x \\in \\mathcal{X}} \\sum_{\\hat{x} \\in \\hat{\\mathcal{X}}} p(x) q(\\hat{x}|x) \\log_2\\left(\\frac{q(\\hat{x}|x)}{q(\\hat{x})}\\right) $$\nProvide the value of this rate. Express your final answer in bits, rounded to four significant figures.", "solution": "The source is binary with $p(0)=p(1)=\\frac{1}{2}$ and the reconstruction alphabet is also binary. The distortion is Hamming: $d(x,\\hat{x})=0$ if $x=\\hat{x}$ and $d(x,\\hat{x})=1$ otherwise. The Blahut-Arimoto update for a fixed $\\beta>0$ is\n$$\nq_{k}(\\hat{x})=\\sum_{x\\in\\mathcal{X}}p(x)\\,q_{k}(\\hat{x}\\mid x),\n\\qquad\nq_{k+1}(\\hat{x}\\mid x)=\\frac{q_{k}(\\hat{x})\\exp(-\\beta d(x,\\hat{x}))}{\\sum_{\\hat{x}'\\in\\hat{\\mathcal{X}}}q_{k}(\\hat{x}')\\exp(-\\beta d(x,\\hat{x}'))}.\n$$\nWe are given $\\beta=\\ln(9)$ and the initialization $q_{0}(\\hat{x}\\mid x)=\\frac{1}{2}$ for all $x,\\hat{x}$.\n\nIteration 0 to 1:\nFirst compute the marginal output distribution using $q_{0}(\\hat{x}\\mid x)$:\n$$\nq_{0}(\\hat{x})=\\sum_{x\\in\\mathcal{X}}p(x)\\,q_{0}(\\hat{x}\\mid x)=\\frac{1}{2}\\cdot\\frac{1}{2}+\\frac{1}{2}\\cdot\\frac{1}{2}=\\frac{1}{2},\\quad\\text{for }\\hat{x}\\in\\{0,1\\}.\n$$\nUsing $\\beta=\\ln(9)$, we have $\\exp(-\\beta)=\\exp(-\\ln(9))=\\frac{1}{9}$. The update then gives, for each $x$:\n- If $\\hat{x}=x$ (so $d(x,\\hat{x})=0$), the numerator is $q_{0}(\\hat{x})\\exp(0)=\\frac{1}{2}$.\n- If $\\hat{x}\\neq x$ (so $d(x,\\hat{x})=1$), the numerator is $q_{0}(\\hat{x})\\exp(-\\beta)=\\frac{1}{2}\\cdot\\frac{1}{9}$.\nThe denominator is the sum of these two terms:\n$$\n\\frac{1}{2}+\\frac{1}{2}\\cdot\\frac{1}{9}=\\frac{1}{2}\\left(1+\\frac{1}{9}\\right)=\\frac{1}{2}\\cdot\\frac{10}{9}=\\frac{5}{9}.\n$$\nTherefore,\n$$\nq_{1}(\\hat{x}=x\\mid x)=\\frac{\\frac{1}{2}}{\\frac{5}{9}}=\\frac{9}{10},\\qquad\nq_{1}(\\hat{x}\\neq x\\mid x)=\\frac{\\frac{1}{2}\\cdot\\frac{1}{9}}{\\frac{5}{9}}=\\frac{1}{10}.\n$$\nThus $q_{1}(\\hat{x}\\mid x)$ is a binary symmetric channel with crossover probability $\\frac{1}{10}$.\n\nIteration 1 to 2:\nCompute the new marginal using $q_{1}(\\hat{x}\\mid x)$:\n$$\nq_{1}(0)=\\frac{1}{2}\\,q_{1}(0\\mid 0)+\\frac{1}{2}\\,q_{1}(0\\mid 1)=\\frac{1}{2}\\cdot\\frac{9}{10}+\\frac{1}{2}\\cdot\\frac{1}{10}=\\frac{1}{2},\n$$\nand similarly $q_{1}(1)=\\frac{1}{2}$. Using these marginals in the update with the same $\\beta$ yields the same calculation as above, hence\n$$\nq_{2}(\\hat{x}=x\\mid x)=\\frac{9}{10},\\qquad q_{2}(\\hat{x}\\neq x\\mid x)=\\frac{1}{10}.\n$$\nTherefore $q_{2}(\\hat{x}\\mid x)=q_{1}(\\hat{x}\\mid x)$.\n\nMutual information for $q_{2}$:\nUsing\n$$\nI(X;\\hat{X})=\\sum_{x\\in\\mathcal{X}}\\sum_{\\hat{x}\\in\\hat{\\mathcal{X}}}p(x)\\,q(\\hat{x}\\mid x)\\,\\log_{2}\\!\\left(\\frac{q(\\hat{x}\\mid x)}{q(\\hat{x})}\\right),\n$$\nand $q_{2}(\\hat{x})=\\frac{1}{2}$ for both $\\hat{x}$, together with $q_{2}(x\\mid x)=\\frac{9}{10}$ and $q_{2}(\\hat{x}\\neq x\\mid x)=\\frac{1}{10}$, we obtain\n$$\nI_{2}(X;\\hat{X})=\\frac{1}{2}\\sum_{x}\\left[\\frac{9}{10}\\log_{2}\\!\\left(\\frac{\\frac{9}{10}}{\\frac{1}{2}}\\right)+\\frac{1}{10}\\log_{2}\\!\\left(\\frac{\\frac{1}{10}}{\\frac{1}{2}}\\right)\\right]\n=\\left(\\frac{9}{10}\\right)\\log_{2}\\!\\left(\\frac{9}{5}\\right)+\\left(\\frac{1}{10}\\right)\\log_{2}\\!\\left(\\frac{1}{5}\\right).\n$$\nEquivalently, $I_{2}(X;\\hat{X})=1-\\left[-\\frac{1}{10}\\log_{2}\\!\\left(\\frac{1}{10}\\right)-\\frac{9}{10}\\log_{2}\\!\\left(\\frac{9}{10}\\right)\\right]=1-H_{2}\\!\\left(\\frac{1}{10}\\right)$. Numerically,\n$$\nI_{2}(X;\\hat{X})\\approx 0.5310044064\\ \\text{bits}.\n$$\nRounded to four significant figures, this is $0.5310$ bits.", "answer": "$$\\boxed{0.5310}$$", "id": "1605389"}, {"introduction": "While powerful, iterative algorithms can have pitfalls, and understanding them is key to successful application. This thought experiment explores a crucial aspect of the Blahut-Arimoto algorithm: the choice of the initial distribution [@problem_id:1605398]. By analyzing the consequences of starting with a zero-probability symbol, you will uncover why this can trap the algorithm and prevent it from finding the true optimal solution.", "problem": "An information theory student is using the Blahut-Arimoto algorithm to numerically approximate a point on the rate-distortion function for a discrete memoryless source. The goal is to find a channel (a conditional probability distribution $q(\\hat{x}|x)$) that minimizes the functional $J = I(X;\\hat{X}) + \\beta D$, where $I(X;\\hat{X})$ is the mutual information, $D$ is the expected distortion, and $\\beta > 0$ is a fixed, positive constant.\n\nThe source is defined by an alphabet $\\mathcal{X}$, with a probability mass function $p(x) > 0$ for all $x \\in \\mathcal{X}$. The reproduction alphabet is $\\mathcal{\\hat{X}}$, and the distortion between any source symbol $x$ and reproduction symbol $\\hat{x}$ is given by a finite value $d(x, \\hat{x})$.\n\nThe iterative algorithm proceeds as follows, starting from an initial guess for the marginal reproduction distribution $q_0(\\hat{x})$:\n\n1.  **For iteration $k$, update the conditional distribution:**\n    $$ q_{k+1}(\\hat{x}|x) = \\frac{q_k(\\hat{x}) \\exp(-\\beta d(x, \\hat{x}))}{\\sum_{\\hat{x}' \\in \\mathcal{\\hat{X}}} q_k(\\hat{x}') \\exp(-\\beta d(x, \\hat{x}'))} $$\n2.  **Update the marginal reproduction distribution:**\n    $$ q_{k+1}(\\hat{x}) = \\sum_{x \\in \\mathcal{X}} p(x) q_{k+1}(\\hat{x}|x) $$\n\nThe student makes a specific initialization choice for the distribution on the reproduction alphabet. They select a particular symbol $\\hat{x}_0 \\in \\mathcal{\\hat{X}}$ and set its initial probability to zero, $q_0(\\hat{x}_0) = 0$. For all other reproduction symbols $\\hat{x} \\in \\mathcal{\\hat{X}}$ where $\\hat{x} \\neq \\hat{x}_0$, the initial probabilities are strictly positive, $q_0(\\hat{x}) > 0$.\n\nWhich of the following statements is a guaranteed consequence of this specific initialization strategy for all subsequent iterations of the algorithm (i.e., for all $k \\ge 1$)?\n\nA. The expected distortion $D_k = \\sum_{x, \\hat{x}} p(x)q_k(\\hat{x}|x)d(x,\\hat{x})$ will be infinite.\n\nB. The algorithm will necessarily converge to the globally optimal channel that minimizes the functional $J$.\n\nC. The mutual information $I_k(X;\\hat{X})$ will be zero.\n\nD. For the specific symbol $\\hat{x}_0$, its marginal probability will remain zero, i.e., $q_k(\\hat{x}_0) = 0$.\n\nE. The conditional probability $q_1(\\hat{x}_0|x)$ will become strictly positive for at least one source symbol $x$, as the algorithm attempts to correct the poor initialization.", "solution": "The Blahut-Arimoto updates for iteration $k$ are\n$$\nq_{k+1}(\\hat{x}\\mid x)=\\frac{q_{k}(\\hat{x})\\exp(-\\beta d(x,\\hat{x}))}{\\sum_{\\hat{x}'\\in\\mathcal{\\hat{X}}}q_{k}(\\hat{x}')\\exp(-\\beta d(x,\\hat{x}'))},\\qquad\nq_{k+1}(\\hat{x})=\\sum_{x\\in\\mathcal{X}}p(x)q_{k+1}(\\hat{x}\\mid x).\n$$\nGiven $\\beta>0$ and finite $d(x,\\hat{x})$ for all $(x,\\hat{x})$, we have $\\exp(-\\beta d(x,\\hat{x}))>0$ for all $(x,\\hat{x})$.\n\nAssume the initialization satisfies $q_{0}(\\hat{x}_{0})=0$ and $q_{0}(\\hat{x})>0$ for all $\\hat{x}\\neq\\hat{x}_{0}$. For any $x\\in\\mathcal{X}$, the update for $\\hat{x}_{0}$ at $k=0$ yields\n$$\nq_{1}(\\hat{x}_{0}\\mid x)=\\frac{q_{0}(\\hat{x}_{0})\\exp(-\\beta d(x,\\hat{x}_{0}))}{\\sum_{\\hat{x}'\\in\\mathcal{\\hat{X}}}q_{0}(\\hat{x}')\\exp(-\\beta d(x,\\hat{x}'))}\n=\\frac{0}{\\sum_{\\hat{x}'\\neq\\hat{x}_{0}}q_{0}(\\hat{x}')\\exp(-\\beta d(x,\\hat{x}'))}=0,\n$$\nsince the denominator is strictly positive due to $q_{0}(\\hat{x}')>0$ for at least one $\\hat{x}'\\neq\\hat{x}_{0}$ and strictly positive exponentials. Therefore,\n$$\nq_{1}(\\hat{x}_{0})=\\sum_{x\\in\\mathcal{X}}p(x)q_{1}(\\hat{x}_{0}\\mid x)=0.\n$$\nBy induction, suppose $q_{k}(\\hat{x}_{0})=0$ for some $k\\geq 0$. Then for all $x$,\n$$\nq_{k+1}(\\hat{x}_{0}\\mid x)=\\frac{q_{k}(\\hat{x}_{0})\\exp(-\\beta d(x,\\hat{x}_{0}))}{\\sum_{\\hat{x}'\\in\\mathcal{\\hat{X}}}q_{k}(\\hat{x}')\\exp(-\\beta d(x,\\hat{x}'))}=0,\n$$\nimplying\n$$\nq_{k+1}(\\hat{x}_{0})=\\sum_{x\\in\\mathcal{X}}p(x)q_{k+1}(\\hat{x}_{0}\\mid x)=0.\n$$\nHence $q_{k}(\\hat{x}_{0})=0$ for all $k\\geq 0$, and $q_{k+1}(\\hat{x}_{0}\\mid x)=0$ for all $x$ and all $k\\geq 0$.\n\nWith this established, we evaluate the options:\n- A is false because the expected distortion $D_{k}=\\sum_{x,\\hat{x}}p(x)q_{k}(\\hat{x}\\mid x)d(x,\\hat{x})$ is a finite weighted sum of finite $d(x,\\hat{x})$ with nonnegative weights summing to one; thus $D_{k}$ is finite.\n- B is not guaranteed: by fixing $q_{k}(\\hat{x}_{0})=0$ for all $k$, the algorithm is restricted to channels supported on $\\mathcal{\\hat{X}}\\setminus\\{\\hat{x}_{0}\\}$; if the global minimizer assigns positive mass to $\\hat{x}_{0}$, it is unreachable.\n- C is not guaranteed: $I_{k}(X;\\hat{X})=\\sum_{x,\\hat{x}}p(x)q_{k}(\\hat{x}\\mid x)\\ln\\!\\left(\\frac{q_{k}(\\hat{x}\\mid x)}{q_{k}(\\hat{x})}\\right)$ need not be zero unless $q_{k}(\\hat{x}\\mid x)=q_{k}(\\hat{x})$ for all $x$, which is not enforced by the updates.\n- D is guaranteed by the induction above: $q_{k}(\\hat{x}_{0})=0$ for all $k\\geq 0$.\n- E is false because $q_{1}(\\hat{x}_{0}\\mid x)=0$ for all $x$, as shown.\n\nTherefore, the guaranteed consequence is that the marginal probability of $\\hat{x}_{0}$ remains zero at all iterations.", "answer": "$$\\boxed{D}$$", "id": "1605398"}]}