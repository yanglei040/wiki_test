{"hands_on_practices": [{"introduction": "We begin our hands-on exploration with the canonical example in rate-distortion theory: a binary source with Hamming distortion. This scenario models the fundamental problem of compressing a sequence of bits while allowing for a certain fraction of bit-flip errors. By deriving the rate-distortion function for this source, you will gain direct insight into the essential trade-off between compression rate and data fidelity, culminating in the celebrated formula $R(D) = H(p) - H(D)$ [@problem_id:1652137].", "problem": "Consider a discrete memoryless source that generates binary symbols, $X$, from the alphabet $\\mathcal{X} = \\{0, 1\\}$. The probability of generating a '1' is $P(X=1) = p$, and the probability of generating a '0' is $P(X=0) = 1-p$. For simplicity, assume $0  p \\le 1/2$. We wish to compress the output of this source and represent it with symbols, $\\hat{X}$, from the same alphabet $\\hat{\\mathcal{X}} = \\{0, 1\\}$.\n\nThe quality of the compression is measured by the single-letter Hamming distortion, defined as $d(x, \\hat{x}) = 0$ if $x = \\hat{x}$ and $d(x, \\hat{x}) = 1$ if $x \\neq \\hat{x}$. The average distortion is denoted by $D = E[d(X, \\hat{X})]$.\n\nThe rate-distortion function, $R(D)$, gives the minimum achievable rate (in bits per symbol) for a given maximum average distortion $D$. Find the expression for the rate-distortion function $R(D)$ for this source in the region where the rate is non-zero. Your final answer should be expressed in terms of the binary entropy function, $H(y) = -y \\log_2(y) - (1-y)\\log_2(1-y)$, and may involve the parameters $p$ and $D$.", "solution": "The rate-distortion function $R(D)$ is defined as the minimum possible mutual information $I(X; \\hat{X})$ between the source $X$ and the reconstruction $\\hat{X}$ over all joint distributions $p(x, \\hat{x})$ such that the average distortion $E[d(X, \\hat{X})]$ does not exceed $D$.\n$$R(D) = \\min_{p(\\hat{x}|x) : E[d(X,\\hat{X})] \\le D} I(X; \\hat{X})$$\nThe mutual information can be expressed in terms of entropy as $I(X; \\hat{X}) = H(X) - H(X|\\hat{X})$. For the given Bernoulli source, the entropy $H(X)$ is constant and equal to $H(p)$. Therefore, minimizing $I(X; \\hat{X})$ is equivalent to maximizing the conditional entropy $H(X|\\hat{X})$.\n\n$$R(D) = H(p) - \\max_{p(\\hat{x}|x) : E[d(X,\\hat{X})] \\le D} H(X|\\hat{X})$$\n\nWe can find an upper bound for $H(X|\\hat{X})$ using Fano's inequality. The Hamming distortion $d(x, \\hat{x})$ is 1 if $x \\neq \\hat{x}$ and 0 otherwise. Thus, the average distortion is the probability of error: $D = E[d(X, \\hat{X})] = P(X \\neq \\hat{X})$.\n\nFano's inequality states that for any two random variables $X$ and $\\hat{X}$ with the same alphabet $\\mathcal{X}$,\n$$H(X|\\hat{X}) \\le H(P(X \\neq \\hat{X})) + P(X \\neq \\hat{X}) \\log_2(|\\mathcal{X}|-1)$$\nFor our binary source, $|\\mathcal{X}|=2$, so $\\log_2(|\\mathcal{X}|-1) = \\log_2(1) = 0$. Using $P(X \\neq \\hat{X}) \\le D$, we get:\n$$H(X|\\hat{X}) \\le H(D)$$\nThis provides a lower bound on the rate-distortion function:\n$$R(D) \\ge H(p) - H(D)$$\nThis bound is achievable if we can find a test channel (i.e., a conditional distribution $p(\\hat{x}|x)$) that simultaneously satisfies the distortion constraint $E[d(X, \\hat{X})] = D$ and achieves the entropy bound $H(X|\\hat{X}) = H(D)$.\n\nLet's construct such a channel. The condition $H(X|\\hat{X}) = H(D)$ is met if the conditional distribution of $X$ given $\\hat{X}$ is a simple binary distribution with probability $D$. Let's define a *reverse* test channel where the crossover probability is $D$:\n$$p(X=1|\\hat{X}=0) = D \\quad \\text{and} \\quad p(X=0|\\hat{X}=1) = D$$\nThis implies $p(X=0|\\hat{X}=0) = 1-D$ and $p(X=1|\\hat{X}=1) = 1-D$.\nThe conditional entropy $H(X|\\hat{X}=\\hat{x})$ is thus $H(D)$ for both $\\hat{x}=0$ and $\\hat{x}=1$. Consequently, the total conditional entropy is $H(X|\\hat{X}) = \\sum_{\\hat{x}} p(\\hat{x}) H(X|\\hat{X}=\\hat{x}) = H(D)$.\n\nNow, we need to define the output distribution $p(\\hat{x})$ such that the source distribution $p(x)$ is recovered and the distortion constraint is met. Let $p(\\hat{X}=1) = q$. Then $p(\\hat{X}=0) = 1-q$. The marginal probability $P(X=1)$ can be calculated as:\n$$P(X=1) = \\sum_{\\hat{x}} P(\\hat{X}=\\hat{x}) P(X=1|\\hat{X}=\\hat{x})$$\n$$p = (1-q) P(X=1|\\hat{X}=0) + q P(X=1|\\hat{X}=1)$$\n$$p = (1-q)D + q(1-D) = D - qD + q - qD = q(1-2D) + D$$\nSolving for $q$:\n$$q = \\frac{p-D}{1-2D}$$\nFor $q$ to be a valid probability, we need $0 \\le q \\le 1$.\nGiven $p \\le 1/2$, the condition $D \\le p  1/2$ ensures that $1-2D  0$.\nThe requirement $q \\ge 0$ implies $p-D \\ge 0$, so $D \\le p$.\nThe requirement $q \\le 1$ implies $p-D \\le 1-2D$, so $D \\le 1-p$.\nSince we are given $p \\le 1/2$, we have $p \\le 1-p$. Therefore, the stricter condition is $D \\le p$. So, this construction is valid for $0 \\le D \\le p$.\n\nFinally, let's verify the average distortion for this channel:\n$$E[d(X, \\hat{X})] = P(X \\neq \\hat{X}) = \\sum_{\\hat{x}} P(\\hat{X}=\\hat{x}) P(X \\neq \\hat{x} | \\hat{X}=\\hat{x})$$\n$$E[d(X, \\hat{X})] = P(\\hat{X}=0) P(X=1|\\hat{X}=0) + P(\\hat{X}=1) P(X=0|\\hat{X}=1)$$\n$$E[d(X, \\hat{X})] = (1-q)D + qD = D$$\nThe distortion constraint is satisfied with equality.\n\nSince we have constructed a channel that produces distortion $D$ and has a rate $I(X;\\hat{X}) = H(p) - H(D)$ for any $D$ in the range $0 \\le D \\le p$, the lower bound is achievable. For $D  p$, we can achieve zero rate. For example, by always setting $\\hat{X}=0$ (since 0 is the more probable symbol), the distortion is $D = P(X=1) = p$ and the rate is $I(X; \\text{const}) = 0$. Since $R(D)$ is a non-increasing function, $R(D)=0$ for all $D \\ge p$.\n\nThe expression for the rate-distortion function in the non-trivial region ($0 \\le D \\le p$) is therefore:\n$$R(D) = H(p) - H(D)$$", "answer": "$$\\boxed{H(p) - H(D)}$$", "id": "1652137"}, {"introduction": "Moving beyond simple binary data, this practice explores a source with multiple discrete levels, which is more representative of digitized analog signals. Here, we replace the simple error-counting of Hamming distortion with the squared-error distortion, a measure ubiquitous in signal processing and statistics. This problem will introduce you to a powerful and practical technique for finding the rate-distortion function: identifying optimal deterministic quantizers and then using the principle of time-sharing to trace out the complete, convex $R(D)$ curve [@problem_id:1652148].", "problem": "A discrete memoryless source emits symbols from the alphabet $\\mathcal{X} = \\{1, 2, 3, 4\\}$. Each symbol is generated independently and with equal probability. We wish to compress this source and represent it with a reproduction alphabet $\\hat{\\mathcal{X}}$, which can consist of any real numbers. The quality of the reproduction is measured by the squared-error distortion, defined as $d(x, \\hat{x}) = (x - \\hat{x})^2$ for a source symbol $x \\in \\mathcal{X}$ and its reproduction $\\hat{x} \\in \\hat{\\mathcal{X}}$.\n\nDetermine the rate-distortion function $R(D)$ for this source, which gives the minimum achievable rate in bits per symbol for a given maximum average distortion $D$. Express your answer as a piecewise function of $D$.", "solution": "Let $X$ be uniform on $\\mathcal{X}=\\{1,2,3,4\\}$, with $p(x)=\\frac{1}{4}$. The rate-distortion function under squared-error distortion is\n$$\nR(D)=\\inf_{p(\\hat{x}|x):\\,\\mathbb{E}[(X-\\hat{X})^{2}]\\le D} I(X;\\hat{X}),\n$$\nmeasured in bits per symbol (so all logarithms are base $2$).\n\nKey facts used:\n- For squared-error distortion with unconstrained reproduction alphabet, for any deterministic partition (quantizer) of the source alphabet into clusters, the optimal reproduction value for a cluster is its conditional mean, and the clusterâ€™s contribution to the total mean-squared error is its within-cluster variance.\n- The set of achievable $(D,R)$ pairs is convex by time-sharing. Therefore, $R(D)$ is the lower convex envelope of the achievable points from deterministic mappings.\n\nCompute the canonical deterministic mappings (clusterings), their distortions, and rates.\n\n1) One-cluster mapping (constant reproduction).\n- Reproduction $\\hat{x}=\\mathbb{E}[X]=\\frac{1+2+3+4}{4}=\\frac{5}{2}$.\n- Distortion $D=\\mathrm{Var}(X)=\\mathbb{E}[X^{2}]-(\\mathbb{E}[X])^{2}=\\frac{30}{4}-\\left(\\frac{5}{2}\\right)^{2}=\\frac{15}{2}-\\frac{25}{4}=\\frac{5}{4}$.\n- Since $\\hat{X}$ is constant, $I(X;\\hat{X})=0$. Hence point $\\left(D,R\\right)=\\left(\\frac{5}{4},0\\right)$.\n\n2) Two-cluster mapping.\n- The optimal $2$-means partition groups adjacent points: $\\{1,2\\}$ and $\\{3,4\\}$, with centroids $\\frac{3}{2}$ and $\\frac{7}{2}$ respectively.\n- For a two-point adjacent cluster $\\{x,x+1\\}$, the centroid is $x+\\frac{1}{2}$ and each point has squared deviation $\\left(\\frac{1}{2}\\right)^{2}=\\frac{1}{4}$. Thus cluster distortion per occurrence is $\\frac{1}{4}$. With cluster probability $\\frac{2}{4}$, the contribution to overall $D$ is $\\frac{2}{4}\\cdot\\frac{1}{4}=\\frac{1}{8}$ per cluster. With two such clusters, total\n$$\nD=\\frac{1}{8}+\\frac{1}{8}=\\frac{1}{4}.\n$$\n- The cluster probabilities are $\\frac{1}{2}$ and $\\frac{1}{2}$, so $H(\\hat{X})=1$ and since the mapping is deterministic, $I(X;\\hat{X})=H(\\hat{X})=1$. Hence point $\\left(D,R\\right)=\\left(\\frac{1}{4},1\\right)$.\n- Any other two-cluster partition (e.g., $\\{1,2,3\\}$ vs. $\\{4\\}$ or nonadjacent pairings) yields larger distortion, so $\\left(\\frac{1}{4},1\\right)$ is the optimal two-cluster point.\n\n3) Three-cluster mapping.\n- The optimal partition merges one adjacent pair and leaves the other two as singletons, e.g., $\\{1,2\\}$, $\\{3\\}$, $\\{4\\}$ (or any symmetric variant). The merged pair contributes $\\frac{1}{8}$ to $D$ as above; singletons contribute $0$. Hence\n$$\nD=\\frac{1}{8}.\n$$\n- Cluster probabilities are $\\frac{1}{2},\\frac{1}{4},\\frac{1}{4}$, so\n$$\nI(X;\\hat{X})=H(\\hat{X})=-\\frac{1}{2}\\log_{2}\\left(\\frac{1}{2}\\right)-2\\cdot\\frac{1}{4}\\log_{2}\\left(\\frac{1}{4}\\right)=\\frac{1}{2}+1=\\frac{3}{2}.\n$$\nHence point $\\left(D,R\\right)=\\left(\\frac{1}{8},\\frac{3}{2}\\right)$.\n\n4) Four-cluster mapping (lossless reproduction).\n- Identity mapping with centroids at the source points gives $D=0$ and $R=H(X)=\\log_{2}4=2$. Hence point $\\left(D,R\\right)=\\left(0,2\\right)$.\n\nConvexification by time-sharing:\n- The achievable points include $\\left(\\frac{5}{4},0\\right)$, $\\left(\\frac{1}{4},1\\right)$, $\\left(\\frac{1}{8},\\frac{3}{2}\\right)$, and $\\left(0,2\\right)$, and by time-sharing, all points on line segments joining them.\n- Compute slopes between these points:\n  - Between $\\left(0,2\\right)$ and $\\left(\\frac{1}{4},1\\right)$: slope is $\\frac{1-2}{\\frac{1}{4}-0}=-4$; the intermediate point $\\left(\\frac{1}{8},\\frac{3}{2}\\right)$ lies exactly on this line, so it does not create a new segment.\n  - Between $\\left(\\frac{1}{4},1\\right)$ and $\\left(\\frac{5}{4},0\\right)$: slope is $\\frac{0-1}{\\frac{5}{4}-\\frac{1}{4}}=-1$.\n\nThus the lower convex envelope is piecewise linear with two segments:\n- For $0\\le D\\le\\frac{1}{4}$, the line through $\\left(0,2\\right)$ and $\\left(\\frac{1}{4},1\\right)$:\n$$\nR(D)=2-4D.\n$$\n- For $\\frac{1}{4}\\le D\\le\\frac{5}{4}$, the line through $\\left(\\frac{1}{4},1\\right)$ and $\\left(\\frac{5}{4},0\\right)$:\n$$\nR(D)=\\frac{5}{4}-D.\n$$\n- For $D\\ge\\frac{5}{4}$, one can use a constant-reproduction code and achieve $R(D)=0$.\n\nTherefore, in bits per symbol,\n$$\nR(D)=\n\\begin{cases}\n2-4D,  0\\le D\\le \\frac{1}{4},\\\\\n\\frac{5}{4}-D,  \\frac{1}{4}\\le D\\le \\frac{5}{4},\\\\\n0,  D\\ge \\frac{5}{4}.\n\\end{cases}\n$$", "answer": "$$\\boxed{R(D)=\\begin{cases}\n2-4D,  0\\le D\\le \\frac{1}{4},\\\\\n\\frac{5}{4}-D,  \\frac{1}{4}\\le D\\le \\frac{5}{4},\\\\\n0,  D\\ge \\frac{5}{4}.\n\\end{cases}}$$", "id": "1652148"}, {"introduction": "Modern data, from images to feature sets in machine learning, is often structured in vectors or blocks of bits, not just single symbols. This advanced practice tackles such a scenario by considering a source of $k$-dimensional binary vectors. The key to solving this seemingly complex problem lies in recognizing and exploiting its inherent symmetry, a powerful strategy that allows us to decompose the high-dimensional problem into a set of simpler, parallel channels. This exercise demonstrates how principles from the basic binary case can be elegantly scaled to analyze more complex, structured data sources [@problem_id:1652150].", "problem": "Consider a digital system that generates feature vectors for a machine learning application. Each vector is a $k$-bit binary string, and the system produces each of the $2^k$ possible vectors with equal probability. For efficient storage, these vectors are compressed and then decompressed, a process which may introduce errors. The distortion between an original vector $X$ and its reconstructed version $\\hat{X}$ is measured by their Hamming distance, $d(X, \\hat{X})$, defined as the number of bit positions in which they differ.\n\nThe fundamental limit on this compression process is described by the rate-distortion function, $R(D)$, which specifies the minimum number of bits per vector required to represent the source such that the expected distortion does not exceed a value $D$.\n\nDetermine the rate-distortion function $R(D)$ for this source. Provide the analytical expression for $R(D)$ valid for the distortion range $0 \\le D \\le k/2$. The rate should be expressed in bits per vector. Your final expression should be in terms of $k$, $D$, and base-2 logarithms.", "solution": "The rate-distortion function $R(D)$ is defined as the minimum of the mutual information $I(X; \\hat{X})$ over all conditional distributions (test channels) $p(\\hat{x}|x)$ such that the expected distortion is less than or equal to $D$.\n$$R(D) = \\min_{p(\\hat{x}|x) : \\mathbb{E}[d(X, \\hat{X})] \\le D} I(X; \\hat{X})$$\nThe mutual information can be expressed as $I(X; \\hat{X}) = H(X) - H(X|\\hat{X})$.\n\nThe source $X$ is uniformly distributed over the $2^k$ vertices of the $k$-dimensional hypercube, which are represented by binary strings of length $k$. The entropy of the source is therefore:\n$$H(X) = \\log_2(|\\mathcal{X}|) = \\log_2(2^k) = k \\text{ bits}$$\nSo, minimizing $I(X; \\hat{X})$ is equivalent to maximizing the conditional entropy $H(X|\\hat{X})$.\n$$R(D) = k - \\max_{p(\\hat{x}|x) : \\mathbb{E}[d(X, \\hat{X})] \\le D} H(X|\\hat{X})$$\n\nDue to the symmetry of the source distribution (uniform) and the distortion measure (Hamming distance, which is invariant to bit-flipping across all vectors), the optimal test channel $p(\\hat{x}|x)$ will also be symmetric. This means that $p(\\hat{x}|x)$ depends only on the error vector $E = X \\oplus \\hat{X}$ (where $\\oplus$ is the bitwise XOR operation), and its distribution is invariant to permutations of the bit positions.\n\nLet us model this symmetric channel by assuming that each bit of the source vector $X$ is independently flipped with a probability $p$ to produce the corresponding bit of the reconstructed vector $\\hat{X}$. This is equivalent to applying $k$ parallel binary symmetric channels (BSCs) to the bits of $X$. The error vector $E$ will have components $E_i$ that are independent and identically distributed Bernoulli random variables with $P(E_i=1) = p$ and $P(E_i=0) = 1-p$.\n\nThe distortion for a given pair $(x, \\hat{x})$ is $d(x, \\hat{x}) = w_H(x \\oplus \\hat{x})$, where $w_H$ is the Hamming weight (number of ones). The expected distortion is the expected Hamming weight of the error vector $E$. Since the number of flipped bits follows a binomial distribution $B(k, p)$, the expected distortion is:\n$$D(p) = \\mathbb{E}[w_H(E)] = k p$$\nWe are interested in reconstructed vectors, so we should choose $p$ to minimize errors. A flip probability $p  0.5$ is suboptimal, as one could achieve a lower distortion $k(1-p)$ with the same rate by flipping the logic. Thus, we consider the range $p \\in [0, 0.5]$. This corresponds to a distortion range of $D \\in [0, k/2]$. From $D=kp$, we can express the parameter $p$ as $p = D/k$.\n\nNow we calculate the rate for this channel. We need to maximize $H(X|\\hat{X})$.\n$$H(X|\\hat{X}) = H(X \\oplus \\hat{X} | \\hat{X}) = H(E | \\hat{X})$$\nThe error vector $E$ is generated independently of the source vector $X$ in this channel model. Since $\\hat{X}$ is a function of $X$ and $E$, the error $E$ is independent of $\\hat{X}$.\nTherefore, $H(E|\\hat{X}) = H(E)$.\n\nThe entropy of the error vector $E$ is the sum of the entropies of its independent components:\n$$H(E) = \\sum_{i=1}^k H(E_i) = k H_b(p)$$\nwhere $H_b(p)$ is the binary entropy function:\n$$H_b(p) = -p \\log_2(p) - (1-p) \\log_2(1-p)$$\nThe mutual information is thus:\n$$I(X; \\hat{X}) = H(X) - H(X|\\hat{X}) = k - k H_b(p) = k(1 - H_b(p))$$\nThis expression gives a point $(D(p), R(p))$ on the rate-distortion curve. To find $R(D)$, we substitute $p=D/k$ into the rate expression:\n$$R(D) = k \\left(1 - H_b\\left(\\frac{D}{k}\\right)\\right)$$\n$$R(D) = k \\left(1 - \\left(-\\frac{D}{k}\\log_2\\left(\\frac{D}{k}\\right) - \\left(1-\\frac{D}{k}\\right)\\log_2\\left(1-\\frac{D}{k}\\right)\\right)\\right)$$\nThis is valid for $p \\in [0, 0.5]$, which corresponds to $D \\in [0, k/2]$. Let's simplify the expression:\n$$R(D) = k + k\\left(\\frac{D}{k}\\log_2\\left(\\frac{D}{k}\\right)\\right) + k\\left(\\frac{k-D}{k}\\log_2\\left(\\frac{k-D}{k}\\right)\\right)$$\n$$R(D) = k + D\\log_2\\left(\\frac{D}{k}\\right) + (k-D)\\log_2\\left(\\frac{k-D}{k}\\right)$$\nThis can also be written as:\n$$R(D) = k - (D + k - D)\\log_2(k) + D\\log_2(D) + (k-D)\\log_2(k-D)$$\n$$R(D) = k - k\\log_2(k) + D\\log_2(D) + (k-D)\\log_2(k-D)$$\nThe problem asks for the expression in terms of $k$, $D$, and base-2 logarithms. The form derived first is cleaner and sufficient.\n\nFor $D > k/2$, the rate $R(D)$ is 0. This is because a rate of 0 can be achieved by sending a constant vector, for instance $\\hat{x}_0 = (0, ..., 0)$, regardless of the input. The expected distortion in this case is the average Hamming weight of a vector in $\\{0,1\\}^k$, which is $\\frac{1}{2^k} \\sum_{x} w_H(x) = \\frac{1}{2^k} (k 2^{k-1}) = k/2$. Since this strategy achieves distortion $D=k/2$ with rate $R=0$, any distortion $D > k/2$ can also be achieved with zero rate.\n\nThe question specifically asks for the expression in the range $0 \\le D \\le k/2$.\nFinal expression:\n$$R(D) = k + D\\log_2\\left(\\frac{D}{k}\\right) + (k-D)\\log_2\\left(1-\\frac{D}{k}\\right)$$\nNote that $\\log_2\\left(1-\\frac{D}{k}\\right) = \\log_2\\left(\\frac{k-D}{k}\\right)$.\nSo, $R(D) = k + D\\log_2\\left(\\frac{D}{k}\\right) + (k-D)\\log_2\\left(\\frac{k-D}{k}\\right)$.", "answer": "$$\\boxed{k + D \\log_{2}\\left(\\frac{D}{k}\\right) + (k-D) \\log_{2}\\left(1-\\frac{D}{k}\\right)}$$", "id": "1652150"}]}