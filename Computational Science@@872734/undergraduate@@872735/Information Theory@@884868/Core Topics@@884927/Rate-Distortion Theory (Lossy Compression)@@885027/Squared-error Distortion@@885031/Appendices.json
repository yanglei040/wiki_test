{"hands_on_practices": [{"introduction": "The first step in minimizing distortion is understanding how to best represent a group of signals with a single value. This practice gets to the heart of the matter by asking you to find the optimal representation levels for pre-defined sets of source symbols. The solution reveals a fundamental rule in quantization: the best reconstruction point to minimize squared error is the conditional mean, or \"centroid,\" of the signals it represents [@problem_id:1659863].", "problem": "A discrete memoryless source $X$ produces symbols from the alphabet $\\mathcal{X} = \\{1, 2, 4, 8\\}$. Each symbol is generated with equal probability. We wish to design a simple data compression scheme using a two-level quantizer. The source alphabet is partitioned into two disjoint sets: $R_1 = \\{1, 2\\}$ and $R_2 = \\{4, 8\\}$. All symbols in $R_1$ are mapped to a single representation level $\\hat{x}_1$, and all symbols in $R_2$ are mapped to a second representation level $\\hat{x}_2$.\n\nThe quality of the quantization is measured by the average squared-error distortion, defined as the expected value of the square of the difference between the source symbol and its representation.\n\nDetermine the optimal representation levels, $\\hat{x}_1^*$ and $\\hat{x}_2^*$, that minimize this average squared-error distortion for the given partitions. Present your answer as the exact values of $\\hat{x}_1^*$ and $\\hat{x}_2^*$ in that order.", "solution": "The problem asks for the optimal representation levels $\\hat{x}_1^*$ and $\\hat{x}_2^*$ that minimize the average squared-error distortion for a given source and partition.\n\nThe source alphabet is $\\mathcal{X} = \\{1, 2, 4, 8\\}$. Since each of the four symbols is generated with equal probability, the probability mass function (PMF) is $p(x) = \\frac{1}{4}$ for each $x \\in \\mathcal{X}$.\n\nThe distortion measure is the squared error, $d(x, \\hat{x}) = (x - \\hat{x})^2$.\n\nThe average distortion, $D$, is the expected value of the distortion, which can be written as a sum over all source symbols:\n$$D = E[d(X, \\hat{X})] = \\sum_{x \\in \\mathcal{X}} p(x) d(x, \\hat{x}(x))$$\nwhere $\\hat{x}(x)$ is the representation level for the source symbol $x$.\n\nThe source alphabet is partitioned into $R_1 = \\{1, 2\\}$ and $R_2 = \\{4, 8\\}$. All symbols in $R_1$ are represented by $\\hat{x}_1$, and all symbols in $R_2$ are represented by $\\hat{x}_2$. We can thus write the average distortion as a function of $\\hat{x}_1$ and $\\hat{x}_2$:\n$$D(\\hat{x}_1, \\hat{x}_2) = \\sum_{x \\in R_1} p(x) (x - \\hat{x}_1)^2 + \\sum_{x \\in R_2} p(x) (x - \\hat{x}_2)^2$$\nThe two terms in the sum are independent. The first term depends only on $\\hat{x}_1$, and the second term depends only on $\\hat{x}_2$. Therefore, we can minimize $D$ by minimizing each term separately.\n\nTo find the optimal representation level $\\hat{x}_1^*$ for the region $R_1$, we minimize the corresponding term by taking its derivative with respect to $\\hat{x}_1$ and setting it to zero:\n$$\\frac{\\partial}{\\partial \\hat{x}_1} \\left[ \\sum_{x \\in R_1} p(x) (x - \\hat{x}_1)^2 \\right] = 0$$\n$$\\sum_{x \\in R_1} p(x) \\cdot 2(x - \\hat{x}_1) \\cdot (-1) = 0$$\n$$-2 \\left( \\sum_{x \\in R_1} x p(x) - \\hat{x}_1 \\sum_{x \\in R_1} p(x) \\right) = 0$$\nSolving for $\\hat{x}_1$, we find the optimal value $\\hat{x}_1^*$:\n$$\\hat{x}_1^* = \\frac{\\sum_{x \\in R_1} x p(x)}{\\sum_{x \\in R_1} p(x)}$$\nThis is the conditional expectation of $X$ given that $X \\in R_1$, also known as the centroid of the region.\n\nNow we can substitute the values for the region $R_1 = \\{1, 2\\}$:\n$$p(1) = \\frac{1}{4}, \\quad p(2) = \\frac{1}{4}$$\n$$\\hat{x}_1^* = \\frac{1 \\cdot p(1) + 2 \\cdot p(2)}{p(1) + p(2)} = \\frac{1 \\cdot \\frac{1}{4} + 2 \\cdot \\frac{1}{4}}{\\frac{1}{4} + \\frac{1}{4}} = \\frac{\\frac{3}{4}}{\\frac{2}{4}} = \\frac{3}{2}$$\n\nSimilarly, for the region $R_2 = \\{4, 8\\}$, the optimal representation level $\\hat{x}_2^*$ is the centroid of $R_2$:\n$$\\hat{x}_2^* = \\frac{\\sum_{x \\in R_2} x p(x)}{\\sum_{x \\in R_2} p(x)}$$\nSubstituting the values for the region $R_2 = \\{4, 8\\}$:\n$$p(4) = \\frac{1}{4}, \\quad p(8) = \\frac{1}{4}$$\n$$\\hat{x}_2^* = \\frac{4 \\cdot p(4) + 8 \\cdot p(8)}{p(4) + p(8)} = \\frac{4 \\cdot \\frac{1}{4} + 8 \\cdot \\frac{1}{4}}{\\frac{1}{4} + \\frac{1}{4}} = \\frac{\\frac{12}{4}}{\\frac{2}{4}} = \\frac{12}{2} = 6$$\n\nThus, the optimal representation levels that minimize the average squared-error distortion are $\\hat{x}_1^* = 3/2$ and $\\hat{x}_2^* = 6$.", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{3}{2}  6 \\end{pmatrix}}$$", "id": "1659863"}, {"introduction": "Real-world engineering is often about making the best use of limited resources. This problem models a classic scenario in data compression where you have just one bit to improve a system's fidelity. By deciding which of two independent sources to quantize, you will explore an essential principle of bit allocation: to achieve the greatest reduction in overall distortion, it is most effective to apply quantization resources to the source with the highest variance [@problem_id:1659835].", "problem": "A data acquisition system measures two independent physical quantities, which can be modeled as two independent random variables, $X$ and $Y$. Both are normally distributed with a mean of zero. The variance of $X$ is $\\sigma_X^2 = 9$, and the variance of $Y$ is $\\sigma_Y^2 = 1$.\n\nDue to bandwidth limitations, you can only use a single bit to improve the representation of one of these quantities. You have two options:\n1.  Use the bit to create a one-bit quantizer for $X$. In this case, the representation of $Y$, denoted $\\hat{Y}$, will be its expected value.\n2.  Use the bit to create a one-bit quantizer for $Y$. In this case, the representation of $X$, denoted $\\hat{X}$, will be its expected value.\n\nAssume that for whichever variable is quantized, an optimal one-bit quantizer is used. An optimal quantizer is one that minimizes the mean squared-error distortion. The total distortion of the system is the sum of the individual distortions: $D_{total} = E[(X-\\hat{X})^2] + E[(Y-\\hat{Y})^2]$.\n\nWhich of the following strategies results in a lower total mean squared-error distortion?\n\nA. Quantizing $X$ with one bit results in a lower total distortion.\n\nB. Quantizing $Y$ with one bit results in a lower total distortion.\n\nC. Both strategies result in the exact same total distortion.\n\nD. It is not possible to determine which strategy is better without more information about the quantizer design.", "solution": "Let $X \\sim \\mathcal{N}(0,\\sigma_{X}^{2})$ with $\\sigma_{X}^{2}=9$ and $Y \\sim \\mathcal{N}(0,\\sigma_{Y}^{2})$ with $\\sigma_{Y}^{2}=1$, independent. If a variable is not quantized and is represented by its mean, which is zero, its mean squared error equals its variance:\n$$\nE[(V-0)^{2}] = \\operatorname{Var}(V).\n$$\n\nFor the variable that is quantized with one bit using an optimal MSE quantizer, consider first the standard normal case $Z \\sim \\mathcal{N}(0,1)$. By symmetry and the Lloyd-Max optimality conditions, the optimal 1-bit quantizer has a zero threshold and symmetric reconstruction levels $\\pm a$. Define $Q(Z)=a$ if $Z \\geq 0$ and $Q(Z)=-a$ if $Z0$. The distortion as a function of $a$ is\n$$\nD(a)=E[(Z-Q(Z))^{2}]=E[Z^{2}] - 2a\\,E[Z\\,\\operatorname{sign}(Z)] + a^{2}.\n$$\nSince $Z\\,\\operatorname{sign}(Z)=|Z|$ and $E[Z^{2}]=1$, we have\n$$\nD(a)=1 - 2a\\,E[|Z|] + a^{2}.\n$$\nMinimizing over $a$ gives\n$$\n\\frac{dD}{da}=-2E[|Z|]+2a=0 \\quad \\Rightarrow \\quad a^{\\star}=E[|Z|].\n$$\nTherefore the minimum distortion is\n$$\nD^{\\star}=1-(E[|Z|])^{2}.\n$$\nFor $Z \\sim \\mathcal{N}(0,1)$, using $\\phi(z)=\\frac{1}{\\sqrt{2\\pi}}\\exp(-z^{2}/2)$,\n$$\nE[|Z|]=2\\int_{0}^{\\infty} z \\phi(z)\\,dz=2\\left[-\\phi(z)\\right]_{0}^{\\infty}=2\\phi(0)=\\sqrt{\\frac{2}{\\pi}},\n$$\nso\n$$\nD^{\\star}=1-\\frac{2}{\\pi}.\n$$\n\nBy scale invariance, if $V=\\sigma Z$, applying the scaled optimal 1-bit quantizer yields\n$$\nE[(V-\\hat{V})^{2}]=\\sigma^{2} D^{\\star}=\\sigma^{2}\\left(1-\\frac{2}{\\pi}\\right).\n$$\n\nNow evaluate the two strategies:\n\n1) Quantize $X$, represent $Y$ by its mean:\n$$\nD_{\\text{total},X}=E[(X-\\hat{X})^{2}] + E[(Y-\\hat{Y})^{2}]\n= \\sigma_{X}^{2}\\left(1-\\frac{2}{\\pi}\\right) + \\sigma_{Y}^{2}\n= 9\\left(1-\\frac{2}{\\pi}\\right) + 1.\n$$\n\n2) Quantize $Y$, represent $X$ by its mean:\n$$\nD_{\\text{total},Y}=E[(X-\\hat{X})^{2}] + E[(Y-\\hat{Y})^{2}]\n= \\sigma_{X}^{2} + \\sigma_{Y}^{2}\\left(1-\\frac{2}{\\pi}\\right)\n= 9 + \\left(1-\\frac{2}{\\pi}\\right).\n$$\n\nCompare:\n$$\nD_{\\text{total},X} - D_{\\text{total},Y}\n= \\left[9\\left(1-\\frac{2}{\\pi}\\right)+1\\right] - \\left[9 + \\left(1-\\frac{2}{\\pi}\\right)\\right]\n= -\\frac{16}{\\pi}  0.\n$$\nHence $D_{\\text{total},X}  D_{\\text{total},Y}$, so quantizing $X$ yields lower total distortion.\n\nTherefore, the correct choice is A.", "answer": "$$\\boxed{A}$$", "id": "1659835"}, {"introduction": "Building on the idea of resource allocation, this advanced exercise tackles a more complex and realistic scenario involving signal processing. A two-dimensional signal is transformed into a new coordinate system before quantization, a common practice in fields like image and audio processing. This problem demonstrates how to analyze error propagation through such transformations and how to optimally distribute a large budget of bits to minimize error in the original space, a key concept in high-rate quantization theory [@problem_id:1659814].", "problem": "A sensor network is used to track the position of a mobile agent. The agent's position is modeled as a 2D random vector $(X, Y)$ that is uniformly distributed over an annular region defined by an inner radius $r_{in}$ and an outer radius $r_{out}$, where $0  r_{in}  r_{out}$. To transmit this position data, the system first converts the Cartesian coordinates $(X, Y)$ into polar coordinates $(R, \\Theta)$, where $R = \\sqrt{X^2 + Y^2}$ is the radius and $\\Theta = \\arctan(Y/X)$ is the angle.\n\nThe radial component $R$ and the angular component $\\Theta$ are then quantized independently using two separate scalar quantizers. A total bit rate of $R_{total}$ bits per sample is available, which is allocated between the two quantizers such that $R_{total} = R_r + R_\\theta$, where $R_r$ and $R_\\theta$ are the number of bits used for the radial and angular quantizers, respectively. The goal is to minimize the total expected distortion, defined as the Mean Squared Error (MSE) between the original Cartesian coordinates $(X,Y)$ and the reconstructed Cartesian coordinates $(\\hat{X}, \\hat{Y})$ obtained from the quantized polar coordinates $(\\hat{R}, \\hat{\\Theta})$.\n\nAssuming a very high total bit rate $R_{total}$, determine the optimal fraction of bits, $f = R_r / R_{total}$, that should be allocated to quantize the radial coordinate $R$ to minimize this Cartesian MSE. Your answer should be a numerical value.", "solution": "Let $(X,Y)$ be uniformly distributed over the annulus $\\{(x,y): r_{in} \\leq \\sqrt{x^{2}+y^{2}} \\leq r_{out}\\}$. Converting to polar coordinates, $R=\\sqrt{X^{2}+Y^{2}}$ and $\\Theta$ is uniform on $[0,2\\pi)$ and independent of $R$, while $R$ has density $f_{R}(r)=\\frac{2r}{r_{out}^{2}-r_{in}^{2}}$ for $r\\in[r_{in},r_{out}]$. In particular,\n$$\n\\mathbb{E}[R^{2}]=\\frac{\\int_{r_{in}}^{r_{out}} r^{2} r \\, dr}{\\int_{r_{in}}^{r_{out}} r \\, dr}=\\frac{1}{2}\\left(r_{out}^{2}+r_{in}^{2}\\right).\n$$\n\nLet the quantization errors be $e_{r}=\\hat{R}-R$ and $e_{\\theta}=\\hat{\\Theta}-\\Theta$. For small errors (high-rate regime), linearizing the Cartesian reconstruction gives\n$$\n\\begin{aligned}\nX=R\\cos\\Theta,\\quad Y=R\\sin\\Theta,\\\\\n\\hat{X}\\approx (R+e_{r})\\cos(\\Theta+e_{\\theta})\\approx R\\cos\\Theta+e_{r}\\cos\\Theta-R e_{\\theta}\\sin\\Theta,\\\\\n\\hat{Y}\\approx (R+e_{r})\\sin(\\Theta+e_{\\theta})\\approx R\\sin\\Theta+e_{r}\\sin\\Theta+R e_{\\theta}\\cos\\Theta.\n\\end{aligned}\n$$\nThus the squared Cartesian error is\n$$\n(\\hat{X}-X)^{2}+(\\hat{Y}-Y)^{2}=(e_{r}\\cos\\Theta-R e_{\\theta}\\sin\\Theta)^{2}+(e_{r}\\sin\\Theta+R e_{\\theta}\\cos\\Theta)^{2}=e_{r}^{2}+R^{2} e_{\\theta}^{2}.\n$$\nUnder high-rate scalar quantization, the errors are approximately uncorrelated with the source and with each other, and have variances $D_{r}=\\mathbb{E}[e_{r}^{2}]$ and $D_{\\theta}=\\mathbb{E}[e_{\\theta}^{2}]$. Therefore the Cartesian mean-squared error is\n$$\nD=\\mathbb{E}\\big[(\\hat{X}-X)^{2}+(\\hat{Y}-Y)^{2}\\big]=D_{r}+\\mathbb{E}[R^{2}]\\,D_{\\theta}.\n$$\n\nFor high-rate optimal scalar quantizers (fixed-rate or entropy-coded), each distortion obeys an exponential rate law\n$$\nD_{r}\\approx \\alpha\\,2^{-2 R_{r}},\\qquad D_{\\theta}\\approx \\beta\\,2^{-2 R_{\\theta}},\n$$\nwhere $\\alpha$ and $\\beta$ are positive constants depending on the source marginals and the quantizer design. With $R_{r}=f R_{total}$ and $R_{\\theta}=(1-f)R_{total}$, the total distortion is\n$$\nD(f)\\approx \\alpha\\,2^{-2 f R_{total}}+\\gamma\\,2^{-2(1-f)R_{total}},\\quad \\gamma\\triangleq \\mathbb{E}[R^{2}]\\,\\beta.\n$$\nDifferentiate and set to zero to minimize:\n$$\n\\frac{dD}{df}=-2 R_{total}\\ln 2\\,\\alpha\\,2^{-2 f R_{total}}+2 R_{total}\\ln 2\\,\\gamma\\,2^{-2(1-f)R_{total}}=0,\n$$\nwhich yields\n$$\n\\alpha\\,2^{-2 f R_{total}}=\\gamma\\,2^{-2(1-f)R_{total}}.\n$$\nTaking $\\log_{2}$,\n$$\n\\log_{2}\\alpha-2 f R_{total}=\\log_{2}\\gamma-2(1-f)R_{total},\n$$\nso\n$$\n2 R_{total}(1-2f)=\\log_{2}\\!\\left(\\frac{\\gamma}{\\alpha}\\right)\\quad\\Rightarrow\\quad f=\\frac{1}{2}-\\frac{1}{4 R_{total}}\\log_{2}\\!\\left(\\frac{\\gamma}{\\alpha}\\right).\n$$\nIn the assumed very high-rate regime (large $R_{total}$), the offset term is negligible, and the optimal allocation converges to\n$$\nf\\to \\frac{1}{2}.\n$$\nTherefore, the optimal fraction of bits allocated to the radial coordinate is $f=0.5$.", "answer": "$$\\boxed{0.5}$$", "id": "1659814"}]}