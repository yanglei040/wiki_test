{"hands_on_practices": [{"introduction": "To build a strong intuition for the concept of a typical set, it is often helpful to start by examining extreme cases. This first practice explores a source with a uniform probability distribution, where every symbol is equally likely. By analyzing this scenario of maximum entropy, you will see how the definition of typicality applies when there is no single preferred outcome, providing a clear baseline for understanding more complex sources. [@problem_id:1650559]", "problem": "Consider two distinct discrete memoryless sources, designated Source 1 and Source 2. Both sources generate sequences of a fixed length $n$.\n\nSource 1 operates on an alphabet of size $K_1 = 4$ and follows a uniform probability distribution, meaning every symbol in its alphabet has an equal probability of being emitted.\n\nSource 2 operates on an alphabet of size $K_2 = 8$, also with a uniform probability distribution over its symbols.\n\nFor a source that generates sequences of length $n$, the typical set, denoted as $A_{\\epsilon}^{(n)}$, contains all sequences $\\mathbf{x} = (x_1, \\dots, x_n)$ which satisfy the condition $|-\\frac{1}{n} \\log_b P(\\mathbf{x}) - H_b(X)| \\le \\epsilon$. Here, $P(\\mathbf{x})$ is the probability of the sequence $\\mathbf{x}$, $H_b(X)$ is the entropy of the source, $\\epsilon$ is a small positive constant, and all logarithms are calculated in base $b$. For this problem, we will use base $b=2$. The size of a set $S$ is its cardinality, denoted $|S|$.\n\nLet $A_{1,\\epsilon}^{(n)}$ be the typical set for Source 1 and $A_{2,\\epsilon}^{(n)}$ be the typical set for Source 2. Determine the ratio of the sizes of these two typical sets, $\\frac{|A_{2,\\epsilon}^{(n)}|}{|A_{1,\\epsilon}^{(n)}|}$. Express your answer as a symbolic expression in terms of $n$.", "solution": "For a discrete memoryless source with uniform distribution over an alphabet of size $K$, each symbol has probability $1/K$. For a sequence $\\mathbf{x} = (x_{1},\\dots,x_{n})$ generated i.i.d., the sequence probability is\n$$\nP(\\mathbf{x})=\\prod_{i=1}^{n}\\frac{1}{K}=K^{-n}.\n$$\nThe entropy in base $2$ is\n$$\nH_{2}(X)=\\log_{2}K.\n$$\nThe per-symbol self-information of any length-$n$ sequence is\n$$\n-\\frac{1}{n}\\log_{2}P(\\mathbf{x})=-\\frac{1}{n}\\log_{2}\\left(K^{-n}\\right)=\\log_{2}K=H_{2}(X).\n$$\nTherefore, for any $\\epsilon>0$,\n$$\n\\left|-\\frac{1}{n}\\log_{2}P(\\mathbf{x})-H_{2}(X)\\right|=0\\le \\epsilon,\n$$\nso every length-$n$ sequence is typical. Hence, the typical set equals the entire set of sequences of length $n$, whose cardinality is $K^{n}$.\n\nApplying this to the two sources:\n- Source 1 has $K_{1}=4$, so $|A_{1,\\epsilon}^{(n)}|=4^{n}$.\n- Source 2 has $K_{2}=8$, so $|A_{2,\\epsilon}^{(n)}|=8^{n}$.\n\nThe ratio of the sizes is\n$$\n\\frac{|A_{2,\\epsilon}^{(n)}|}{|A_{1,\\epsilon}^{(n)}|}=\\frac{8^{n}}{4^{n}}=\\left(\\frac{8}{4}\\right)^{n}=2^{n}.\n$$", "answer": "$$\\boxed{2^{n}}$$", "id": "1650559"}, {"introduction": "A common point of confusion when first learning about typical sets is whether the single most probable sequence is, by definition, a \"typical\" one. This exercise directly addresses this question by examining a biased source. You will discover that typicality is a statement about a sequence's statistical resemblance to the source, not its individual rank in probability, leading to the crucial insight that the most likely outcome may not be typical at all. [@problem_id:1650620]", "problem": "Consider a binary Discrete Memoryless Source (DMS) that generates sequences of symbols from the alphabet $\\mathcal{X} = \\{0, 1\\}$. The probability of emitting symbol '0' is $P(0) = 0.9$, and the probability of emitting symbol '1' is $P(1) = 0.1$. The entropy of this source is denoted by $H(X)$.\n\nFor a given $\\epsilon > 0$ and a sequence length $n$, the typical set, $A_{\\epsilon}^{(n)}$, is defined as the set of all sequences $x^n = (x_1, x_2, \\dots, x_n)$ that satisfy the following condition:\n$$ \\left| -\\frac{1}{n} \\log_{2} P(x^n) - H(X) \\right| \\le \\epsilon $$\nwhere $P(x^n)$ is the probability of the sequence $x^n$.\n\nLet's set the parameter $\\epsilon = 0.1$. Based on the source properties and definitions provided, which of the following statements is correct regarding the single most probable sequence of length $n$ generated by this source?\n\nA. The most probable sequence of any length $n$ is a member of the typical set $A_{0.1}^{(n)}$.\n\nB. The most probable sequence of any length $n$ is not a member of the typical set $A_{0.1}^{(n)}$.\n\nC. Whether the most probable sequence is a member of $A_{0.1}^{(n)}$ depends on the specific value of the sequence length $n$.\n\nD. The concept of a typical set and its defining condition cannot be applied to the single most probable sequence.\n\nE. The most probable sequence is only a member of the typical set $A_{0.1}^{(n)}$ if the source is unbiased, i.e., if $P(0)=P(1)=0.5$.", "solution": "A binary DMS produces i.i.d. symbols, so for any length-$n$ sequence $x^{n}=(x_{1},\\dots,x_{n})$ the probability factors as\n$$\nP(x^{n})=\\prod_{i=1}^{n}P(x_{i}).\n$$\nWith $P(0)>P(1)$, the single most probable sequence is the one that uses the most probable symbol in every position, namely $0^{n}=(0,0,\\dots,0)$. Its probability is\n$$\nP(0^{n})=P(0)^{n}.\n$$\nHence its per-symbol self-information is\n$$\n-\\frac{1}{n}\\log_{2}P(0^{n})=-\\frac{1}{n}\\log_{2}\\big(P(0)^{n}\\big)=-\\log_{2}P(0).\n$$\nThe source entropy is\n$$\nH(X)=-P(0)\\log_{2}P(0)-P(1)\\log_{2}P(1).\n$$\nThe deviation that defines typicality for $0^{n}$ is therefore\n$$\n\\left|-\\frac{1}{n}\\log_{2}P(0^{n})-H(X)\\right|=\\left|-\\log_{2}P(0)-H(X)\\right|.\n$$\nSubstituting $H(X)$ and simplifying,\n$$\n\\begin{aligned}\nH(X)-\\big(-\\log_{2}P(0)\\big)\n=\\big[-P(0)\\log_{2}P(0)-P(1)\\log_{2}P(1)\\big]+\\log_{2}P(0)\\\\\n=(1-P(0))\\log_{2}P(0)-P(1)\\log_{2}P(1)\\\\\n=P(1)\\big[\\log_{2}P(0)-\\log_{2}P(1)\\big]\\\\\n=P(1)\\log_{2}\\!\\left(\\frac{P(0)}{P(1)}\\right).\n\\end{aligned}\n$$\nSince $P(0)=0.9$ and $P(1)=0.1$,\n$$\n\\left|-\\log_{2}P(0)-H(X)\\right|=H(X)+\\log_{2}P(0)=0.1\\,\\log_{2}(9).\n$$\nTo compare with $\\epsilon=0.1$, note that $9>8$ implies $\\log_{2}(9)>\\log_{2}(8)=3$, hence\n$$\n0.1\\,\\log_{2}(9)>0.1\\times 3=0.3>0.1.\n$$\nTherefore,\n$$\n\\left|-\\frac{1}{n}\\log_{2}P(0^{n})-H(X)\\right|0.1=\\epsilon,\n$$\nso $0^{n}$ is not in $A_{0.1}^{(n)}$. This deviation is independent of $n$, hence the conclusion holds for any sequence length $n$.\n\nThus, the single most probable sequence of any length $n$ is not a member of the typical set $A_{0.1}^{(n)}$ for this source, which corresponds to option B.", "answer": "$$\\boxed{B}$$", "id": "1650620"}, {"introduction": "The Asymptotic Equipartition Property (AEP) beautifully connects the probabilistic nature of information with combinatorics. This final practice guides you through a foundational derivation that reveals entropy's deep meaning by approximating the size of the typical set. By using Stirling's approximation to count the number of sequences that have the expected statistical composition, you will derive the very formula for Shannon entropy, cementing the idea that entropy quantifies the effective number of high-probability sequences. [@problem_id:1650580]", "problem": "Consider a communication system that transmits data from a binary source. The source independently generates '0's and '1's, with the probability of generating a '1' being $p$, and '0' being $1-p$.\n\nFor a large block length $n$, the overwhelming majority of sequences that are likely to be observed (i.e., the typical sequences) contain a number of '1's, denoted by $k$, that is very close to the statistical expectation, $np$.\n\nLet's model the set of typical sequences as the collection of all binary sequences of length $n$ that contain exactly $k = np$ ones. The number of such sequences is given by the binomial coefficient $\\binom{n}{k}$. A fundamental result from information theory, related to the Asymptotic Equipartition Property (AEP), states that for large $n$, the size of this set can be approximated by an expression of the form:\n$$ \\binom{n}{np} \\approx 2^{n \\cdot H_{\\text{comb}}(p)} $$\nwhere $H_{\\text{comb}}(p)$ can be interpreted as a combinatorially-derived entropy for the source.\n\nYour task is to derive the analytical expression for $H_{\\text{comb}}(p)$. To do this, use Stirling's approximation for the factorial, which for large $m$ is given by $\\ln(m!) \\approx m \\ln m - m$. You should assume that $n$, $np$, and $n(1-p)$ are all sufficiently large for this approximation to be accurate. The final expression for $H_{\\text{comb}}(p)$ should be given in terms of $p$ and base-2 logarithms.", "solution": "We want an explicit expression for $H_{\\text{comb}}(p)$ in the approximation\n$$\n\\binom{n}{np} \\approx 2^{n \\cdot H_{\\text{comb}}(p)}.\n$$\nAssume $n$, $np$, and $n(1-p)$ are large and, for convenience, that $np$ is an integer. Start by taking natural logarithms and applying Stirlingâ€™s approximation $\\ln(m!) \\approx m \\ln m - m$:\n$$\n\\ln \\binom{n}{np} = \\ln(n!) - \\ln((np)!) - \\ln((n(1-p))!) \\approx \\left(n \\ln n - n\\right) - \\left(np \\ln(np) - np\\right) - \\left(n(1-p)\\ln(n(1-p)) - n(1-p)\\right).\n$$\nExpand the logarithms $\\ln(np) = \\ln n + \\ln p$ and $\\ln(n(1-p)) = \\ln n + \\ln(1-p)$:\n$$\n\\ln \\binom{n}{np} \\approx n \\ln n - n - \\left[np(\\ln n + \\ln p) - np\\right] - \\left[n(1-p)(\\ln n + \\ln(1-p)) - n(1-p)\\right].\n$$\nDistribute and collect terms:\n$$\n\\ln \\binom{n}{np} \\approx n \\ln n - n - np \\ln n - np \\ln p + np - n(1-p) \\ln n - n(1-p) \\ln(1-p) + n(1-p).\n$$\nGroup the $\\ln n$ terms and the constant terms:\n- The $\\ln n$ terms cancel:\n$$\nn \\ln n - np \\ln n - n(1-p) \\ln n = n \\ln n - n \\ln n = 0.\n$$\n- The constants cancel:\n$$\n-n + np + n(1-p) = -n + n = 0.\n$$\nThe remaining terms are\n$$\n\\ln \\binom{n}{np} \\approx -np \\ln p - n(1-p) \\ln(1-p) = n\\left[-p \\ln p - (1-p)\\ln(1-p)\\right].\n$$\nExponentiating and converting from natural logarithms to base-$2$ using $\\exp(x) = 2^{x / \\ln 2}$ yields\n$$\n\\binom{n}{np} \\approx \\exp\\!\\left(n\\left[-p \\ln p - (1-p)\\ln(1-p)\\right]\\right)\n= 2^{\\,n\\left[-p \\log_{2} p - (1-p)\\log_{2}(1-p)\\right]}.\n$$\nComparing with $\\binom{n}{np} \\approx 2^{n \\cdot H_{\\text{comb}}(p)}$, we identify\n$$\nH_{\\text{comb}}(p) = -p \\log_{2} p - (1-p)\\log_{2}(1-p).\n$$\nThis expression extends continuously to $p \\in \\{0,1\\}$ by the convention $0 \\log_{2} 0 = 0$.", "answer": "$$\\boxed{-p \\log_{2} p - (1-p)\\log_{2}(1-p)}$$", "id": "1650580"}]}