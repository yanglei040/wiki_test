{"hands_on_practices": [{"introduction": "To build a strong intuition for perplexity, it is best to start with the simplest case: a uniform probability distribution. In this scenario, perplexity reveals its most direct meaning as the effective number of choices, which is simply the total count of all possible outcomes. This exercise grounds the concept in a practical counting problem related to password security, challenging you to first determine the size of the outcome space before calculating its perplexity. [@problem_id:1646104]", "problem": "A security analyst is studying the patterns in 4-digit Personal Identification Numbers (PINs). A PIN is an ordered sequence of four digits, where each digit is an integer from 0 to 9, inclusive. The analyst models a common user behavior wherein individuals avoid repeating a digit immediately after it appears. For example, a PIN like '3435' is considered valid under this model, but '3345' and '3445' are not.\n\nAssuming that every 4-digit PIN that satisfies this non-consecutive-repetition rule is equally likely to be chosen, what is the perplexity of the resulting probability distribution over the set of possible PINs?\n\nProvide the answer as a single integer.", "solution": "A 4-digit PIN is an ordered 4-tuple of digits from the set $\\{0,1,2,\\dots,9\\}$. The non-consecutive-repetition rule requires that for each position $i \\in \\{2,3,4\\}$, the digit at position $i$ must not equal the digit at position $i-1$.\n\nCount the number of valid PINs:\n- For the first digit, there are $10$ choices.\n- For each subsequent digit, there are $9$ choices (any digit except the immediately preceding one).\nThus, the total number of valid PINs is\n$$\nN = 10 \\times 9^{3}.\n$$\nCompute this explicitly:\n$$\nN = 10 \\times 729 = 7290.\n$$\n\nPerplexity of a distribution with entropy $H$ (in bits) is defined as $2^{H}$. For the uniform distribution over $N$ outcomes,\n$$\nH = \\log_{2}(N) \\quad \\Rightarrow \\quad \\text{perplexity} = 2^{H} = 2^{\\log_{2}(N)} = N.\n$$\nTherefore, the perplexity equals the number of valid PINs, which is $7290$.", "answer": "$$\\boxed{7290}$$", "id": "1646104"}, {"introduction": "Real-world systems rarely exhibit uniform probabilities; some outcomes are more likely than others. This practice problem transitions to the more general case of a non-uniform distribution, a scenario central to fields like natural language processing. You will calculate perplexity for a simplified predictive text model, learning how this metric quantifies the model's uncertainty about its next prediction. [@problem_id:1646124]", "problem": "A computer scientist is analyzing a simplified character-level predictive model. This model's purpose is to predict the next character in a sequence based on statistical data from a training corpus. After observing a specific sequence of characters, the model consults its training data and finds the following frequencies for the characters that have previously appeared in this context:\n- The character 'a' appeared 125 times.\n- The character 'e' appeared 125 times.\n- The character 'i' appeared 60 times.\n- The character 'o' appeared 60 times.\n- The character 'u' appeared 30 times.\n\nThe model defines the probability of the next character being a specific character as being directly proportional to its observed frequency. Any character not in the list above has a zero probability of appearing. Based on this probability distribution, calculate the perplexity of the model's prediction.\n\nUse the natural logarithm for any logarithmic calculations. Express your final answer as a numerical value rounded to three significant figures.", "solution": "The model assigns probabilities proportional to observed frequencies. Let the counts be $f_{a}=125$, $f_{e}=125$, $f_{i}=60$, $f_{o}=60$, $f_{u}=30$. The total count is\n$$\nF=\\sum f=125+125+60+60+30=400.\n$$\nThus the probabilities are\n$$\np(a)=\\frac{125}{400}=\\frac{5}{16},\\quad p(e)=\\frac{5}{16},\\quad p(i)=\\frac{60}{400}=\\frac{3}{20},\\quad p(o)=\\frac{3}{20},\\quad p(u)=\\frac{30}{400}=\\frac{3}{40}.\n$$\nPerplexity for a single-step prediction distribution using the natural logarithm is defined as\n$$\n\\mathrm{PP}=\\exp\\!\\left(H\\right),\\quad H=-\\sum_{x}p(x)\\ln p(x).\n$$\nCompute the entropy:\n$$\nH=-\\Bigg[2\\cdot\\frac{5}{16}\\ln\\!\\left(\\frac{5}{16}\\right)+2\\cdot\\frac{3}{20}\\ln\\!\\left(\\frac{3}{20}\\right)+\\frac{3}{40}\\ln\\!\\left(\\frac{3}{40}\\right)\\Bigg].\n$$\nEvaluating the logarithms and products numerically (using $\\ln$):\n$$\nH\\approx -\\Big[2\\cdot 0.3125\\cdot(-1.1631508098)+2\\cdot 0.15\\cdot(-1.8971199849)+0.075\\cdot(-2.5902671654)\\Big]\\approx 1.4903752890.\n$$\nHence the perplexity is\n$$\n\\mathrm{PP}=\\exp(H)\\approx \\exp(1.4903752890)\\approx 4.4387610695.\n$$\nRounded to three significant figures, the perplexity is $4.44$.", "answer": "$$\\boxed{4.44}$$", "id": "1646124"}, {"introduction": "Beyond simply calculating perplexity for a given distribution, a deeper understanding comes from exploring how it behaves across a range of possible distributions. This advanced problem challenges you to find the specific probability distribution that *minimizes* perplexity while satisfying an external constraint on its mean value. Solving this will reveal the fundamental principle that minimum uncertainty (and thus minimum perplexity) is achieved by making the distribution as \"spiky\" or deterministic as the constraints allow. [@problem_id:1646139]", "problem": "A data stream produces integer values from the set $S = \\{1, 2, 3\\}$. The long-term average value of the stream is known to be a constant, $\\mu$. We can model this stream as a discrete random variable $X$ with outcomes in $S$ and an unknown probability distribution $P = (p_1, p_2, p_3)$, where $p_i = \\text{Prob}(X=i)$. The uncertainty of this model is quantified by its perplexity, defined as $PP(X) = 2^{H(X)}$, where $H(X)$ is the Shannon entropy of the distribution $P$ calculated in bits (i.e., using the base-2 logarithm).\n\nGiven that the mean value of the outcomes is fixed at $\\mu = 2.5$, determine the probability distribution $(p_1, p_2, p_3)$ that minimizes the perplexity of the stream. Express the probabilities $p_1, p_2$, and $p_3$ as exact fractions or terminating decimals.", "solution": "We seek to minimize the perplexity $PP(X) = 2^{H(X)}$ subject to the constraints of a probability distribution on $S=\\{1,2,3\\}$ with fixed mean $\\mu=2.5$. Since $2^{x}$ is strictly increasing, minimizing $PP(X)$ is equivalent to minimizing the Shannon entropy\n$$\nH(X) = -\\sum_{i=1}^{3} p_{i} \\log_{2}(p_{i})\n$$\nsubject to $p_{i} \\geq 0$, $\\sum_{i=1}^{3} p_{i} = 1$, and the mean constraint\n$$\n1 \\cdot p_{1} + 2 \\cdot p_{2} + 3 \\cdot p_{3} = 2.5.\n$$\n\nUsing $p_{1} = 1 - p_{2} - p_{3}$ in the mean constraint gives\n$$\n1 + p_{2} + 2 p_{3} = 2.5 \\quad \\Longrightarrow \\quad p_{2} + 2 p_{3} = 1.5.\n$$\nThus\n$$\np_{2} = 1.5 - 2 p_{3}, \\quad p_{1} = 1 - p_{2} - p_{3} = -0.5 + p_{3}.\n$$\nNonnegativity requires $p_{2} \\geq 0 \\Rightarrow p_{3} \\leq \\frac{3}{4}$ and $p_{1} \\geq 0 \\Rightarrow p_{3} \\geq \\frac{1}{2}$. Therefore the feasible set is the line segment\n$$\np_{3} \\in \\left[\\frac{1}{2}, \\frac{3}{4}\\right], \\quad p_{2} = 1.5 - 2 p_{3}, \\quad p_{1} = -0.5 + p_{3}.\n$$\n\nThe Shannon entropy is a concave function of $(p_{1},p_{2},p_{3})$, so its minimum over this convex feasible set occurs at an extreme point. The endpoints are:\n- At $p_{3} = \\frac{1}{2}$: $(p_{1},p_{2},p_{3}) = \\left(0, \\frac{1}{2}, \\frac{1}{2}\\right)$.\n- At $p_{3} = \\frac{3}{4}$: $(p_{1},p_{2},p_{3}) = \\left(\\frac{1}{4}, 0, \\frac{3}{4}\\right)$.\n\nCompute the entropies at these endpoints (with the convention $0 \\log_{2} 0 = 0$ by continuity):\n$$\nH\\left(0, \\frac{1}{2}, \\frac{1}{2}\\right) = -\\frac{1}{2}\\log_{2}\\!\\left(\\frac{1}{2}\\right) - \\frac{1}{2}\\log_{2}\\!\\left(\\frac{1}{2}\\right) = 1,\n$$\n$$\nH\\left(\\frac{1}{4}, 0, \\frac{3}{4}\\right) = -\\frac{1}{4}\\log_{2}\\!\\left(\\frac{1}{4}\\right) - \\frac{3}{4}\\log_{2}\\!\\left(\\frac{3}{4}\\right)\n= \\frac{1}{2} - \\frac{3}{4}\\left(\\log_{2} 3 - 2\\right) = 2 - \\frac{3}{4}\\log_{2} 3.\n$$\nTo compare, note that $3^{3} = 27 > 16 = 2^{4}$ implies $\\log_{2} 3 > \\frac{4}{3}$, hence\n$$\n2 - \\frac{3}{4}\\log_{2} 3 < 2 - \\frac{3}{4}\\cdot \\frac{4}{3} = 1.\n$$\nTherefore $H\\left(\\frac{1}{4}, 0, \\frac{3}{4}\\right) < H\\left(0, \\frac{1}{2}, \\frac{1}{2}\\right)$, so the entropy (and thus the perplexity) is minimized at $(p_{1},p_{2},p_{3}) = \\left(\\frac{1}{4}, 0, \\frac{3}{4}\\right)$.\n\nThus, the distribution minimizing perplexity under the mean constraint $\\mu=2.5$ is $p_{1} = \\frac{1}{4}$, $p_{2} = 0$, $p_{3} = \\frac{3}{4}$.", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{4} & 0 & \\frac{3}{4}\\end{pmatrix}}$$", "id": "1646139"}]}