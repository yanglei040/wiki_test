{"hands_on_practices": [{"introduction": "This first practice problem lays the foundation for understanding MAP decoding. By working through a classic scenario involving a binary source and a Binary Symmetric Channel (BSC), you will learn how to formulate the MAP decision rule and calculate the resulting probability of error, demonstrating how prior probabilities give MAP an edge over simpler methods [@problem_id:1639810].", "problem": "A remote environmental monitoring system uses a binary sensor to transmit its findings. The sensor's state is represented by a binary random variable $X$, where $X=0$ indicates a \"Normal\" condition and $X=1$ indicates an \"Alert\" condition. Long-term data shows that the \"Normal\" state is more common, with prior probabilities given by $P(X=0) = 0.7$ and $P(X=1) = 0.3$.\n\nThe sensor's state is transmitted to a base station over a noisy communication link that behaves as a Binary Symmetric Channel (BSC). A BSC is characterized by a single parameter, the crossover probability $p$, which is the probability that a transmitted bit is incorrectly received (i.e., a '0' is received as a '1' or a '1' is received as a '0'). For this particular channel, the crossover probability is $p = 0.1$.\n\nAt the base station, a decoder receives a symbol $Y \\in \\{0, 1\\}$ and must make an estimate, $\\hat{X}$, of the originally transmitted symbol $X$. To achieve the lowest possible rate of incorrect decisions, a Maximum a Posteriori (MAP) decision rule is employed.\n\nYour task is to determine the specific MAP decision rule and the resulting average probability of error, $P_e = P(\\hat{X} \\neq X)$. The decision rule is fully described by the pair of outputs $(\\hat{X}(Y=0), \\hat{X}(Y=1))$, which specifies the estimate for each possible received symbol.\n\nWhich of the following options correctly states the MAP decision rule and the corresponding average probability of error?\n\nA. The rule is $(\\hat{X}(Y=0), \\hat{X}(Y=1)) = (0, 1)$, and $P_e = 0.10$.\n\nB. The rule is $(\\hat{X}(Y=0), \\hat{X}(Y=1)) = (0, 0)$, and $P_e = 0.30$.\n\nC. The rule is $(\\hat{X}(Y=0), \\hat{X}(Y=1)) = (0, 1), and $P_e = 0.07$.\n\nD. The rule is $(\\hat{X}(Y=0), \\hat{X}(Y=1)) = (0, 0)$, and $P_e = 0.10$.\n\nE. The rule is $(\\hat{X}(Y=0), \\hat{X}(Y=1)) = (1, 0)$, and $P_e = 0.90$.", "solution": "We are given a binary source with priors $P(X=0)=0.7$ and $P(X=1)=0.3$, and a Binary Symmetric Channel (BSC) with crossover probability $p=0.1$. The received symbol is $Y \\in \\{0,1\\}$. The MAP rule selects, for each received $y$, the hypothesis $x \\in \\{0,1\\}$ that maximizes the posterior $P(X=x \\mid Y=y)$, which is equivalent to maximizing the product $P(Y=y \\mid X=x)P(X=x)$.\n\nFor a BSC, the likelihoods are\n$$\nP(Y=y \\mid X=x)=\\begin{cases}\n1-p,  y=x,\\\\\np,  y \\neq x.\n\\end{cases}\n$$\n\nDecision for $Y=0$: compare $(1-p)P(X=0)$ to $p P(X=1)$.\n$$\n(1-p)P(X=0) \\overset{?}{\\ge} p P(X=1).\n$$\nSubstituting the given values,\n$$\n(1-0.1)\\cdot 0.7 = 0.63 \\quad \\text{and} \\quad 0.1 \\cdot 0.3 = 0.03,\n$$\nso $(1-p)P(X=0)  p P(X=1)$ and the MAP decision is $\\hat{X}(Y=0)=0$.\n\nDecision for $Y=1$: compare $p P(X=0)$ to $(1-p)P(X=1)$.\n$$\np P(X=0) \\overset{?}{\\ge} (1-p) P(X=1).\n$$\nSubstituting,\n$$\n0.1 \\cdot 0.7 = 0.07 \\quad \\text{and} \\quad (1-0.1)\\cdot 0.3 = 0.27,\n$$\nso $p P(X=0)  (1-p)P(X=1)$ and the MAP decision is $\\hat{X}(Y=1)=1$.\n\nTherefore, the MAP rule is $(\\hat{X}(Y=0), \\hat{X}(Y=1))=(0,1)$.\n\nNext, compute the average probability of error under this rule:\n$$\nP_{e} = P(X=0) P(\\hat{X} \\neq X \\mid X=0) + P(X=1) P(\\hat{X} \\neq X \\mid X=1).\n$$\nGiven the rule, an error occurs if and only if the channel flips the bit:\n$$\nP(\\hat{X} \\neq X \\mid X=0) = P(Y=1 \\mid X=0)=p, \\quad P(\\hat{X} \\neq X \\mid X=1) = P(Y=0 \\mid X=1)=p.\n$$\nHence,\n$$\nP_{e} = P(X=0)\\,p + P(X=1)\\,p = p\\big(P(X=0)+P(X=1)\\big) = p = 0.1.\n$$\nEquivalently, using $P_{e} = 1 - \\sum_{y} \\max_{x} P(X=x, Y=y)$,\n$$\n\\sum_{y} \\max_{x} P(X=x, Y=y) = P(X=0, Y=0) + P(X=1, Y=1) = (1-p)P(X=0) + (1-p)P(X=1) = (1-p),\n$$\nso $P_{e} = 1 - (1-p) = p = 0.1$, confirming the result.\n\nThus the correct option is the rule $(0,1)$ with $P_{e}=0.10$.", "answer": "$$\\boxed{A}$$", "id": "1639810"}, {"introduction": "Building on the basics, this exercise introduces more realistic complexities. You will apply the MAP principle to a system with a non-binary source, a simple encoding scheme, and independent but non-identical error channels, a situation common in data storage systems. This problem highlights how MAP decoding elegantly combines information from multiple sources—priors, encoding rules, and channel characteristics—to make an optimal decision, even when the received data is corrupted into a form that was not originally transmitted [@problem_id:1639840].", "problem": "A specialized data storage system handles three types of data packets, labeled A, B, and C. The prior probabilities of encountering each packet type are given by $P(X=A) = \\frac{1}{2}$, $P(X=B) = \\frac{1}{3}$, and $P(X=C) = \\frac{1}{6}$, where $X$ is the random variable representing the packet type. Before storage, each packet is encoded into a 2-bit codeword as follows:\n- Packet A is encoded as `00`.\n- Packet B is encoded as `01`.\n- Packet C is encoded as `11`.\n\nThe two bits of the codeword are stored in two separate, independent memory modules. The first bit is stored in Module 1, and the second bit in Module 2. Both modules are subject to bit-flip errors and can be modeled as communication channels. Module 1 behaves as a Binary Symmetric Channel (BSC) with a crossover probability (the probability of a bit flip) of $p_1 = 0.1$. Module 2 is less reliable and is modeled as a BSC with a crossover probability of $p_2 = 0.2$.\n\nSuppose that upon retrieval, the 2-bit sequence read from the memory modules is $Y = (1,0)$. Given this observation, which of the original packet types is the most likely to have been stored? Your task is to determine the Maximum A Posteriori (MAP) estimate for the original packet $X$.\n\nWhich of the following represents the MAP estimate for the original packet?\n\nA. A\n\nB. B\n\nC. C\n\nD. B and C are equally likely and more probable than A.\n\nE. It is impossible to determine from the given information.", "solution": "We want the Maximum A Posteriori (MAP) estimate, defined by\n$$\n\\hat{x}_{\\text{MAP}}=\\arg\\max_{x\\in\\{A,B,C\\}} P(X=x\\mid Y=(1,0)),\n$$\nwhich is equivalent to maximizing the posterior proportional to prior times likelihood:\n$$\nP(X=x\\mid Y=(1,0))\\propto P(Y=(1,0)\\mid X=x)P(X=x).\n$$\nThe encoding is deterministic: $A\\mapsto 00$, $B\\mapsto 01$, $C\\mapsto 11$. The two memory modules are independent Binary Symmetric Channels with crossover probabilities $p_{1}$ for the first bit and $p_{2}$ for the second bit. For a BSC, the probability of no flip is $1-p$ and of a flip is $p$. Because the modules act independently on the two bits, the likelihood factors as\n$$\nP(Y=(1,0)\\mid X=x)=P(Y_{1}=1\\mid \\text{bit}_{1}(x))\\cdot P(Y_{2}=0\\mid \\text{bit}_{2}(x)).\n$$\nCompute each likelihood:\n- For $X=A$ (codeword $00$): the first bit flips ($0\\to 1$) with probability $p_{1}$ and the second bit does not flip ($0\\to 0$) with probability $1-p_{2}$, so\n$$\nP(Y=(1,0)\\mid X=A)=p_{1}(1-p_{2}).\n$$\n- For $X=B$ (codeword $01$): the first bit flips ($0\\to 1$) with probability $p_{1}$ and the second bit flips ($1\\to 0$) with probability $p_{2}$, so\n$$\nP(Y=(1,0)\\mid X=B)=p_{1}p_{2}.\n$$\n- For $X=C$ (codeword $11$): the first bit does not flip ($1\\to 1$) with probability $1-p_{1}$ and the second bit flips ($1\\to 0$) with probability $p_{2}$, so\n$$\nP(Y=(1,0)\\mid X=C)=(1-p_{1})p_{2}.\n$$\nMultiply by priors to form unnormalized posteriors:\n$$\nS_{A}=P(X=A)P(Y=(1,0)\\mid X=A)=\\frac{1}{2}\\,p_{1}(1-p_{2}),\n$$\n$$\nS_{B}=P(X=B)P(Y=(1,0)\\mid X=B)=\\frac{1}{3}\\,p_{1}p_{2},\n$$\n$$\nS_{C}=P(X=C)P(Y=(1,0)\\mid X=C)=\\frac{1}{6}\\,(1-p_{1})p_{2}.\n$$\nSubstitute $p_{1}=\\frac{1}{10}$ and $p_{2}=\\frac{1}{5}$:\n$$\nS_{A}=\\frac{1}{2}\\cdot \\frac{1}{10}\\cdot \\left(1-\\frac{1}{5}\\right)=\\frac{1}{2}\\cdot \\frac{1}{10}\\cdot \\frac{4}{5}=\\frac{1}{25},\n$$\n$$\nS_{B}=\\frac{1}{3}\\cdot \\frac{1}{10}\\cdot \\frac{1}{5}=\\frac{1}{150},\n$$\n$$\nS_{C}=\\frac{1}{6}\\cdot \\left(1-\\frac{1}{10}\\right)\\cdot \\frac{1}{5}=\\frac{1}{6}\\cdot \\frac{9}{10}\\cdot \\frac{1}{5}=\\frac{3}{100}.\n$$\nCompare:\n$$\n\\frac{1}{25} > \\frac{3}{100} > \\frac{1}{150}.\n$$\nTherefore the MAP estimate is $X=A$, which corresponds to option A.", "answer": "$$\\boxed{A}$$", "id": "1639840"}, {"introduction": "This final practice problem takes a significant step from single-symbol estimation to decoding sequences with memory. Many real-world data sources, from natural language to biological sequences, exhibit dependencies where the current symbol is influenced by the previous one. By modeling the source as a Markov chain, you will see how MAP estimation can be applied recursively to find the most likely transmitted bit, providing a foundational understanding of the algorithms used in Hidden Markov Models [@problem_id:1639803].", "problem": "Consider a simplified model for a data storage system where binary bits, represented by a sequence of random variables $\\{X_k\\}_{k=1,2,3,...}$ with $X_k \\in \\{0, 1\\}$, are stored sequentially. Due to physical interactions between adjacent storage cells, the system behaves as a first-order binary Markov source. The transition probabilities are given by $P(X_k = 1 | X_{k-1} = 0) = 0.2$ and $P(X_k = 0 | X_{k-1} = 1) = 0.3$ for $k \\ge 2$. For the very first bit, the probabilities are equal, i.e., $P(X_1=0) = P(X_1=1) = 0.5$.\n\nWhen the data is read, the process is corrupted by noise, which is modeled as a memoryless Binary Symmetric Channel (BSC). The crossover probability of the BSC is $\\epsilon = 0.1$, meaning the probability of a bit being flipped during the read-out process is $0.1$. Let $\\{Y_k\\}_{k=1,2,3,...}$ be the sequence of bits read from the storage system.\n\nAn operator reads the first three bits from the device and obtains the sequence $Y_1=1, Y_2=0, Y_3=1$. To perform an optimal bit-by-bit Maximum a Posteriori (MAP) decision, the operator needs to compute the posterior probability of each transmitted bit given the observed sequence up to that point.\n\nCalculate the posterior probability $P(X_3=1 | Y_1=1, Y_2=0, Y_3=1)$. Round your final answer to four significant figures.", "solution": "We model the hidden bits as a first-order binary Markov chain with transition probabilities\n$$\nP(X_{k}=1\\mid X_{k-1}=0)=0.2,\\quad P(X_{k}=0\\mid X_{k-1}=1)=0.3,\n$$\nso\n$$\nP(X_{k}=0\\mid X_{k-1}=0)=0.8,\\quad P(X_{k}=1\\mid X_{k-1}=1)=0.7,\n$$\nand initial distribution\n$$\nP(X_{1}=0)=P(X_{1}=1)=\\frac{1}{2}.\n$$\nThe observations come through a memoryless Binary Symmetric Channel with crossover probability $\\epsilon=0.1$, so for each $k$,\n$$\nP(Y_{k}=y\\mid X_{k}=x)=\\begin{cases}\n0.9, y=x,\\\\\n0.1, y\\neq x.\n\\end{cases}\n$$\nDefine the forward variables $\\alpha_{k}(x)=P(X_{k}=x,Y_{1}=y_{1},\\ldots,Y_{k}=y_{k})$. Then\n$$\n\\alpha_{1}(x_{1})=P(Y_{1}=y_{1}\\mid X_{1}=x_{1})P(X_{1}=x_{1}),\n$$\nand for $k\\geq 2$,\n$$\n\\alpha_{k}(x_{k})=P(Y_{k}=y_{k}\\mid X_{k}=x_{k})\\sum_{x_{k-1}\\in\\{0,1\\}}P(X_{k}=x_{k}\\mid X_{k-1}=x_{k-1})\\,\\alpha_{k-1}(x_{k-1}).\n$$\n\nGiven $y_{1}=1$, we have\n$$\n\\alpha_{1}(1)=0.9\\cdot\\frac{1}{2}=\\frac{9}{20},\\qquad \\alpha_{1}(0)=0.1\\cdot\\frac{1}{2}=\\frac{1}{20}.\n$$\n\nGiven $y_{2}=0$, compute the prediction to time $2$:\n$$\n\\sum_{x_{1}}P(X_{2}=0\\mid X_{1}=x_{1})\\alpha_{1}(x_{1})=0.8\\cdot\\frac{1}{20}+0.3\\cdot\\frac{9}{20}=\\frac{7}{40},\n$$\n$$\n\\sum_{x_{1}}P(X_{2}=1\\mid X_{1}=x_{1})\\alpha_{1}(x_{1})=0.2\\cdot\\frac{1}{20}+0.7\\cdot\\frac{9}{20}=\\frac{13}{40}.\n$$\nThen\n$$\n\\alpha_{2}(0)=0.9\\cdot\\frac{7}{40}=\\frac{63}{400},\\qquad \\alpha_{2}(1)=0.1\\cdot\\frac{13}{40}=\\frac{13}{400}.\n$$\n\nGiven $y_{3}=1$, compute the prediction to time $3$:\n$$\n\\sum_{x_{2}}P(X_{3}=0\\mid X_{2}=x_{2})\\alpha_{2}(x_{2})=0.8\\cdot\\frac{63}{400}+0.3\\cdot\\frac{13}{400}=\\frac{543}{4000},\n$$\n$$\n\\sum_{x_{2}}P(X_{3}=1\\mid X_{2}=x_{2})\\alpha_{2}(x_{2})=0.2\\cdot\\frac{63}{400}+0.7\\cdot\\frac{13}{400}=\\frac{217}{4000}.\n$$\nThus\n$$\n\\alpha_{3}(1)=0.9\\cdot\\frac{217}{4000}=\\frac{1953}{40000},\\qquad \\alpha_{3}(0)=0.1\\cdot\\frac{543}{4000}=\\frac{543}{40000}.\n$$\n\nThe desired posterior is\n$$\nP(X_{3}=1\\mid Y_{1}=1,Y_{2}=0,Y_{3}=1)=\\frac{\\alpha_{3}(1)}{\\alpha_{3}(0)+\\alpha_{3}(1)}=\\frac{1953/40000}{(1953+543)/40000}=\\frac{1953}{2496}=\\frac{651}{832}.\n$$\nNumerically,\n$$\n\\frac{651}{832}\\approx 0.782451923\\ldots,\n$$\nwhich rounded to four significant figures is $0.7825$.", "answer": "$$\\boxed{0.7825}$$", "id": "1639803"}]}