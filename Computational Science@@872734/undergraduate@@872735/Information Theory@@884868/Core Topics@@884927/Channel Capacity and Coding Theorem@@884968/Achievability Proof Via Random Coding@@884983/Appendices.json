{"hands_on_practices": [{"introduction": "The foundation of Shannon's achievability proof is the analysis of a random ensemble of codes, rather than a single specific code. This practice [@problem_id:1601638] introduces this probabilistic approach by having you calculate a basic property of a randomly generated codebook: the expected number of all-zero sequences. Mastering this use of linearity of expectation is the first step toward understanding the more complex arguments for channel capacity.", "problem": "In the analysis of channel coding, the random coding argument is a powerful tool used to prove the existence of good codes. Consider the construction of a random codebook, $\\mathcal{C}$, for a binary channel. The codebook contains $M = 2^{nR}$ unique codewords, where $n$ is the blocklength (the length of each codeword in bits) and $R$ is the code rate in bits per channel use.\n\nEach of the $M$ codewords in the codebook is generated independently. Specifically, for each codeword, its $n$ bits are generated as a sequence of independent and identically distributed (i.i.d.) random variables. Each bit is drawn from a Bernoulli distribution where the probability of the bit being a '1' is $p$ and the probability of it being a '0' is $1-p$.\n\nCalculate the expected number of codewords that are the all-zero sequence (i.e., a sequence of $n$ zeros) in this randomly generated codebook. Your answer should be a closed-form analytic expression in terms of $n$, $R$, and $p$.", "solution": "Let $M=2^{nR}$ be the number of codewords. For each codeword, define the indicator random variable $I_{m}$ that equals $1$ if the $m$th codeword is the all-zero sequence and $0$ otherwise, for $m \\in \\{1,\\dots,M\\}$. The total number of all-zero codewords in the codebook is\n$$\nN_{0}=\\sum_{m=1}^{M} I_{m}.\n$$\nWe compute $\\mathbb{E}[N_{0}]$ using linearity of expectation:\n$$\n\\mathbb{E}[N_{0}]=\\sum_{m=1}^{M} \\mathbb{E}[I_{m}]=\\sum_{m=1}^{M} \\Pr\\{I_{m}=1\\}.\n$$\nEach codeword consists of $n$ i.i.d. bits, with each bit equal to $0$ with probability $1-p$. By independence across the $n$ bits, the probability that a given codeword is the all-zero sequence is\n$$\n\\Pr\\{I_{m}=1\\}=(1-p)^{n}.\n$$\nTherefore,\n$$\n\\mathbb{E}[N_{0}]=M(1-p)^{n}=2^{nR}(1-p)^{n}.\n$$\nThis is a closed-form expression in terms of $n$, $R$, and $p$.", "answer": "$$\\boxed{2^{nR}(1-p)^{n}}$$", "id": "1601638"}, {"introduction": "The core of the achievability proof involves showing that the probability of decoding incorrectly is small, which happens when the received sequence is \"confusable\" with a codeword that was not sent. This practice [@problem_id:1601672] simplifies this analysis for the Binary Erasure Channel (BEC), asking you to calculate the expected number of these confusable codewords. This quantity is the key metric that must be driven toward zero to ensure reliable communication.", "problem": "Consider a random codebook $\\mathcal{C}$ designed for communication over a Binary Erasure Channel (BEC). The codebook consists of $M$ codewords, say $\\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_M\\}$, each of length $n$. Each bit of each codeword is generated independently from a Bernoulli(1/2) distribution, meaning the probability of a bit being 0 is $1/2$, and the probability of it being 1 is also $1/2$. The generation of each codeword is independent of all other codewords.\n\nSuppose codeword $\\mathbf{x}_1$ is transmitted, and the received sequence is $\\mathbf{y}$. Due to the nature of the BEC, this sequence $\\mathbf{y}$ has exactly $k$ components that are erasures and $n-k$ components that are identical to the corresponding bits in $\\mathbf{x}_1$.\n\nAn incorrect codeword, $\\mathbf{x}_i$ where $i \\neq 1$, is said to be 'consistent' with the received sequence $\\mathbf{y}$ if its bits match the non-erased bits of $\\mathbf{y}$. That is, for any position $j$ where the received bit $y_j$ is not an erasure, the $j$-th bit of $\\mathbf{x}_i$ must be equal to $y_j$.\n\nLet $N$ be the random variable for the number of incorrect codewords in the codebook that are consistent with $\\mathbf{y}$. Calculate the expected value of $N$, denoted $E[N]$, over the ensemble of all possible randomly generated codebooks.\n\nExpress your final answer as a closed-form analytic expression in terms of $M$, $n$, and $k$.", "solution": "Define the indicator variable for each incorrect codeword $\\mathbf{x}_{i}$, $i \\in \\{2,3,\\ldots,M\\}$, as\n$$\nI_{i}=\\begin{cases}\n1,  \\text{if }\\mathbf{x}_{i}\\text{ is consistent with }\\mathbf{y},\\\\\n0,  \\text{otherwise.}\n\\end{cases}\n$$\nBy definition, the total number of incorrect consistent codewords is\n$$\nN=\\sum_{i=2}^{M} I_{i}.\n$$\nUsing linearity of expectation,\n$$\nE[N]=\\sum_{i=2}^{M} E[I_{i}]=\\sum_{i=2}^{M} \\Pr\\{I_{i}=1\\}.\n$$\nCondition on the received sequence $\\mathbf{y}$ that has exactly $k$ erasures and $n-k$ non-erased positions that match $\\mathbf{x}_{1}$. Let $S$ be the set of non-erased positions, so $|S|=n-k$, and for each $j \\in S$, $y_{j}=x_{1j}$ is fixed. Each codeword $\\mathbf{x}_{i}$ is generated independently with i.i.d. Bernoulli$\\left(\\tfrac{1}{2}\\right)$ bits, independent of $\\mathbf{y}$ and $\\mathbf{x}_{1}$. Therefore, for a fixed $i \\neq 1$,\n$$\n\\Pr\\{I_{i}=1\\}=\\Pr\\{\\mathbf{x}_{i}\\text{ matches }\\mathbf{y}\\text{ on all }j \\in S\\}\n=\\prod_{j \\in S} \\Pr\\{x_{ij}=y_{j}\\}\n=\\left(\\frac{1}{2}\\right)^{|S|}=2^{-(n-k)}.\n$$\nThis probability is the same for each $i \\in \\{2,\\ldots,M\\}$, hence\n$$\nE[N]=\\sum_{i=2}^{M} 2^{-(n-k)}=(M-1)\\,2^{-(n-k)}.\n$$", "answer": "$$\\boxed{(M-1)\\,2^{-(n-k)}}$$", "id": "1601672"}, {"introduction": "The basic random coding argument guarantees a code with good *average* performance, but some individual codewords may still be poor. This practice [@problem_id:1601652] introduces the powerful idea of expurgation, a method for improving a code by discarding its worst-performing members. You will use a fundamental tool, Markov's inequality, to find the minimum expected size of the improved codebook, demonstrating that we can construct codes with even stronger guarantees of reliability.", "problem": "In a digital communication system, a random coding scheme is proposed to ensure reliable data transmission. A codebook $\\mathcal{C}$ is created by generating $M$ distinct codewords, where each codeword is a sequence of $n$ bits. The generation process for each codeword involves choosing each of its $n$ bits independently and uniformly at random (i.e., a '0' or a '1' with a probability of 0.5 each).\n\nFor a specific, randomly generated codebook $\\mathcal{C}$, let $\\lambda_i(\\mathcal{C})$ denote the conditional probability of a decoding error, given that the $i$-th codeword was transmitted. A key metric for the overall performance of the random coding ensemble is the average probability of error, $P_{avg}$, which is the average of $\\lambda_i(\\mathcal{C})$ taken over all $M$ messages and all possible random codebooks. It has been determined for this system that $P_{avg} = 0.01$.\n\nTo improve performance, a process called \"expurgation\" is applied. From a given randomly generated codebook $\\mathcal{C}$, any codeword $c_i$ is discarded if its individual error probability $\\lambda_i(\\mathcal{C})$ is more than double the ensemble average error probability. That is, $c_i$ is removed if $\\lambda_i(\\mathcal{C})  2 P_{avg}$. The remaining codewords form a new, smaller, \"expurgated\" codebook, $\\mathcal{C'}$.\n\nGiven an initial codebook size of $M=1024$, what is the minimum expected number of codewords that will remain in the codebook after this expurgation procedure is performed? Provide your answer as a single integer.", "solution": "Let the random codebook be denoted by $\\mathcal{C}$, and define the random variable $\\Lambda_{i}=\\lambda_{i}(\\mathcal{C})$ for a fixed message index $i \\in \\{1,\\ldots,M\\}$. By construction $\\Lambda_{i} \\ge 0$. The ensemble average error probability is\n$$\nP_{avg}=\\frac{1}{M}\\sum_{i=1}^{M}\\mathbb{E}_{\\mathcal{C}}[\\Lambda_{i}],\n$$\nand by symmetry of the random coding ensemble (all messages are generated identically), we have\n$$\n\\mathbb{E}_{\\mathcal{C}}[\\Lambda_{i}]=P_{avg}\\quad\\text{for every }i.\n$$\n\nA codeword $i$ is expurgated if $\\Lambda_{i}2P_{avg}$. By Markov's inequality, for each $i$,\n$$\n\\mathbb{P}\\big(\\Lambda_{i}2P_{avg}\\big)\\le \\frac{\\mathbb{E}[\\Lambda_{i}]}{2P_{avg}}=\\frac{P_{avg}}{2P_{avg}}=\\frac{1}{2}.\n$$\n\nDefine the indicator $X_{i}=\\mathbf{1}\\{\\Lambda_{i}2P_{avg}\\}$. Then $\\mathbb{E}[X_{i}]=\\mathbb{P}(\\Lambda_{i}2P_{avg})\\le \\frac{1}{2}$. The total number expurgated is $X=\\sum_{i=1}^{M}X_{i}$, so by linearity of expectation,\n$$\n\\mathbb{E}[X]=\\sum_{i=1}^{M}\\mathbb{E}[X_{i}]\\le \\sum_{i=1}^{M}\\frac{1}{2}=\\frac{M}{2}.\n$$\nHence, the expected number remaining after expurgation is\n$$\n\\mathbb{E}[\\text{remaining}]=M-\\mathbb{E}[X]\\ge M-\\frac{M}{2}=\\frac{M}{2}.\n$$\n\nWith $M=1024$, the minimum expected number of codewords that remain is $\\frac{1024}{2}=512$.", "answer": "$$\\boxed{512}$$", "id": "1601652"}]}