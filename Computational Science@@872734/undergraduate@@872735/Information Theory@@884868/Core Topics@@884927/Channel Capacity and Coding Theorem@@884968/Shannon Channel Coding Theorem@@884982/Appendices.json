{"hands_on_practices": [{"introduction": "The Shannon Channel Coding Theorem provides a powerful link between a channel's physical properties, summarized by its capacity $C$, and our ability to communicate reliably. This first exercise offers a direct application of this principle. By knowing the channel capacity and the block length of our code, we can determine the theoretical maximum number of unique messages we can transmit without error, providing a tangible sense of the theorem's promise. [@problem_id:1657433]", "problem": "A deep-space probe, \"Aether-Scout 7,\" is transmitting scientific data back to Earth through a noisy communication channel influenced by interstellar plasma. After extensive analysis, mission control engineers have determined the Shannon capacity of this channel to be $C = 0.5$ bits per symbol. The probe's communication system is designed to group data into codewords of a fixed block length of $n = 1000$ symbols.\n\nAccording to the noisy-channel coding theorem, it is possible to design a code that allows for communication with an arbitrarily low probability of error, provided the transmission rate does not exceed the channel capacity. Based on this principle, what is the theoretical maximum number of distinct messages (e.g., unique scientific measurements or status reports) that can be encoded into a single block for reliable transmission?\n\nExpress your answer as a power of 2, in the form $2^k$ for some integer $k$.", "solution": "By the noisy-channel coding theorem, reliable communication (arbitrarily low probability of error) is achievable if the transmission rate per symbol $R$ does not exceed the channel capacity $C$. For a block code of length $n$ symbols that encodes $M$ distinct messages, the code rate in bits per symbol is defined by\n$$\nR=\\frac{1}{n}\\log_{2} M.\n$$\nFor reliable transmission, we require $R \\leq C$. The maximum number of messages is achieved when $R=C$, giving\n$$\n\\log_{2} M_{\\max}=nC \\quad \\Rightarrow \\quad M_{\\max}=2^{nC}.\n$$\nWith $C=\\frac{1}{2}$ bits per symbol and $n=1000$, we obtain\n$$\nM_{\\max}=2^{1000 \\cdot \\frac{1}{2}}=2^{500}.\n$$\nThus, the theoretical maximum number of distinct messages encodable per block for reliable transmission is $2^{500}$.", "answer": "$$\\boxed{2^{500}}$$", "id": "1657433"}, {"introduction": "Not all noise is created equal; some types of channel interference are more detrimental to communication than others. This practice asks you to compare two fundamental channel models: the Binary Symmetric Channel (BSC), where bits are flipped, and the Binary Erasure Channel (BEC), where bits are lost but their positions are known. By calculating and comparing their capacities, you will develop a deeper intuition for how channel capacity quantifies the impact of different kinds of noise on reliable data transmission. [@problem_id:1657419]", "problem": "An aerospace engineering team is finalizing the design for a communication system on a long-range exploratory probe. The probe will transmit data back to Earth as a stream of binary bits. The team is evaluating two competing prototype technologies, designated System A and System B, and must select the one that supports the highest theoretical rate of error-free communication.\n\nThe communication channels for the two systems have been characterized as follows:\n\n*   **System A** operates as a Binary Symmetric Channel (BSC). In a BSC, each transmitted bit (0 or 1) has a fixed probability $p$ of being flipped to the opposite value upon reception.\n*   **System B** operates as a Binary Erasure Channel (BEC). In a BEC, each transmitted bit is either received correctly, or it is lost and replaced by an 'erasure' symbol. The receiver knows that an erasure has occurred, but does not know the original bit's value. The probability of a bit being erased is $\\epsilon$.\n\nThrough rigorous testing, the team has determined the characteristic parameters for the two systems under operational conditions to be $p = 0.1$ for System A, and $\\epsilon = 0.2$ for System B.\n\nAccording to information theory, the maximum rate at which information can be transmitted with arbitrarily low error probability is known as the channel capacity. Given the parameters, which of the following statements correctly identifies the superior system?\n\nA. System A is superior because its capacity is greater than that of System B.\n\nB. System B is superior because its capacity is greater than that of System A.\n\nC. Both systems are equally capable, as their capacities are identical for the given parameters.\n\nD. The comparison is inconclusive without specifying the probabilities of transmitting 0s and 1s.", "solution": "The channel capacity is defined as the maximum mutual information over all input distributions: for a given discrete memoryless channel, $C=\\max_{P_{X}} I(X;Y)$.\n\nFor a Binary Symmetric Channel with crossover probability $p$, symmetry implies the maximizing input is uniform, and the capacity in bits per channel use is\n$$\nC_{A}=1-H_{2}(p),\n$$\nwhere the binary entropy function is\n$$\nH_{2}(p)=-p \\log_{2}(p)-(1-p)\\log_{2}(1-p).\n$$\nSubstituting $p=0.1$,\n$$\nH_{2}(0.1)=-0.1\\log_{2}(0.1)-0.9\\log_{2}(0.9).\n$$\nUsing the change of base $\\log_{2}(x)=\\frac{\\ln x}{\\ln 2}$ and numerical evaluation,\n$$\n\\log_{2}(0.1)\\approx -3.321928,\\quad \\log_{2}(0.9)\\approx -0.152003,\n$$\nso\n$$\nH_{2}(0.1)\\approx 0.1\\times 3.321928+0.9\\times 0.152003\\approx 0.332193+0.136803\\approx 0.468996,\n$$\nand therefore\n$$\nC_{A}\\approx 1-0.468996\\approx 0.531004.\n$$\n\nFor a Binary Erasure Channel with erasure probability $\\epsilon$, the capacity in bits per channel use is\n$$\nC_{B}=1-\\epsilon.\n$$\nSubstituting $\\epsilon=0.2$ gives\n$$\nC_{B}=1-0.2=0.8.\n$$\n\nComparison:\n$$\nC_{A}\\approx 0.5310040.8=C_{B}.\n$$\nHence, System B has the greater capacity. Note that option D is incorrect because capacity already accounts for optimization over input distributions; for both BSC and BEC the uniform input achieves capacity, so no additional specification of input probabilities is needed for comparison.", "answer": "$$\\boxed{B}$$", "id": "1657419"}, {"introduction": "A key aspect of understanding any theoretical limit is to examine its boundary conditions. This exercise explores the extreme case of a completely broken channel, where the output is statistically independent of the input. Calculating the capacity in this scenario reveals the fundamental meaning of zero capacity and solidifies the concept that information transfer is only possible when there is a statistical dependence between what is sent and what is received. [@problem_id:1657440]", "problem": "A deep-space probe communicates with a ground station on Earth by transmitting binary data. Initially, the communication channel is accurately modeled as a Binary Symmetric Channel (BSC), where each transmitted bit (0 or 1) has a probability $p = 0.11$ of being flipped to the opposite bit upon reception.\n\nDuring a solar storm, the probe's transmitter is permanently damaged. The new behavior of the transmitter is as follows: regardless of the input bit it is instructed to send, it transmits a '0' with a fixed probability of $\\gamma = 0.45$ and a '1' with a probability of $1-\\gamma = 0.55$. The transmission path itself from the probe to Earth remains error-free after this damage.\n\nYour task is to analyze the impact of this damage on the fundamental limit of reliable communication.\n\n1.  Calculate the capacity of the communication channel *before* the solar storm damage occurred.\n2.  Calculate the capacity of the communication channel *after* the solar storm damage occurred.\n\nExpress both capacities in units of bits per channel use. Round your final numerical answers to three significant figures. If a capacity is exactly zero, state it as 0.", "solution": "Channel capacity for a discrete memoryless channel is defined as\n$$\nC=\\max_{P_{X}} I(X;Y),\n$$\nwhere $I(X;Y)=H(Y)-H(Y|X)$ and the maximization is over all input distributions $P_{X}$.\n\n1) Before the solar storm, the channel is a Binary Symmetric Channel (BSC) with crossover probability $p=0.11$. A BSC is a symmetric channel; hence the capacity is achieved by the uniform input distribution $P_{X}(0)=P_{X}(1)=\\frac{1}{2}$. Under this input,\n$$\nH(Y)=1 \\text{ bit}, \\quad H(Y|X)=H_{2}(p),\n$$\nwhere the binary entropy function in bits is\n$$\nH_{2}(p)=-p \\log_{2}(p)-(1-p)\\log_{2}(1-p).\n$$\nTherefore,\n$$\nC_{\\text{before}}=1-H_{2}(p).\n$$\nSubstituting $p=0.11$ and evaluating numerically using $\\log_{2}(x)=\\frac{\\ln x}{\\ln 2}$ gives\n$$\nH_{2}(0.11)=-0.11 \\log_{2}(0.11)-0.89 \\log_{2}(0.89)\n\\approx 0.499904,\n$$\nso\n$$\nC_{\\text{before}}\\approx 1-0.499904=0.500096 \\text{ bits/channel use}.\n$$\nRounded to three significant figures, $C_{\\text{before}}=0.500$ bits/channel use.\n\n2) After the solar storm, the transmitter outputs $Y$ as $P(Y=0|X=x)=\\gamma=0.45$ and $P(Y=1|X=x)=1-\\gamma=0.55$ for both $x\\in\\{0,1\\}$, and the path is error-free. Thus the output distribution does not depend on the input: $P_{Y|X=x}$ is identical for all $x$. Hence $X$ and $Y$ are independent for any input distribution $P_{X}$, which implies\n$$\nI(X;Y)=H(Y)-H(Y|X)=H(Y)-H(Y)=0\n$$\nfor every $P_{X}$, and therefore\n$$\nC_{\\text{after}}=0 \\text{ bits/channel use}.\n$$\nRounded to three significant figures, this remains $0$.\n\nThus the capacities are $0.500$ (before) and $0$ (after), in bits per channel use.", "answer": "$$\\boxed{\\begin{pmatrix}0.500  0\\end{pmatrix}}$$", "id": "1657440"}]}