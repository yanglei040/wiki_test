{"hands_on_practices": [{"introduction": "The Binary Erasure Channel (BEC) is a foundational model in information theory that captures a different type of error from the more common bit-flip. Instead of corrupting data, this channel either transmits a bit perfectly or explicitly reports that the data was lost, an \"erasure.\" This exercise will guide you through calculating the mutual information for a BEC, providing a clear quantitative measure of how much information successfully transits a channel susceptible to erasures.", "problem": "A specialized digital communication system is designed to transmit binary data. The system can be modeled as a discrete memoryless channel. The input to the channel is a single bit, represented by a random variable $X$ with the alphabet $\\mathcal{X} = \\{0, 1\\}$. The output of the channel is represented by a random variable $Y$. Due to imperfections, the channel sometimes fails to resolve the transmitted bit, outputting a special \"erasure\" symbol, $\\lambda$. Thus, the output alphabet is $\\mathcal{Y} = \\{0, 1, \\lambda\\}$.\n\nThe behavior of the channel is characterized by the following properties:\n1.  The channel never flips a bit. This means that if the output is $0$, the input must have been $0$, and if the output is $1$, the input must have been $1$.\n2.  The probability of an erasure occurring is $\\alpha$, where $0 < \\alpha < 1$. This probability is independent of whether a $0$ or a $1$ was sent.\n\nThe source transmitting the data produces bits with the following probabilities: $P(X=0) = p$ and $P(X=1) = 1-p$, where $0 < p < 1$.\n\nDerive a closed-form analytic expression for the mutual information $I(X;Y)$ between the input $X$ and the output $Y$ for this channel. Your final expression should be in terms of the parameters $p$ and $\\alpha$. In your calculations and final answer, all logarithms must be base 2.", "solution": "The mutual information $I(X;Y)$ can be calculated using the formula $I(X;Y) = H(X) - H(X|Y)$, where $H(X)$ is the entropy of the input and $H(X|Y)$ is the conditional entropy of the input given the output. All logarithms are base 2.\n\nFirst, let's calculate the entropy of the input source, $H(X)$.\nThe random variable $X$ follows a Bernoulli distribution with parameter $p$. Its entropy is given by the binary entropy function:\n$$H(X) = -\\sum_{x \\in \\mathcal{X}} P(X=x) \\log_{2}(P(X=x))$$\n$$H(X) = -[P(X=0) \\log_{2}(P(X=0)) + P(X=1) \\log_{2}(P(X=1))]$$\nSubstituting the given probabilities $P(X=0) = p$ and $P(X=1) = 1-p$:\n$$H(X) = -[p \\log_{2}(p) + (1-p) \\log_{2}(1-p)]$$\n\nNext, we calculate the conditional entropy $H(X|Y)$. The formula is:\n$$H(X|Y) = \\sum_{y \\in \\mathcal{Y}} P(Y=y) H(X|Y=y)$$\nwhere $H(X|Y=y) = -\\sum_{x \\in \\mathcal{X}} P(X=x|Y=y) \\log_{2}(P(X=x|Y=y))$.\n\nTo do this, we need to determine the channel's transition probabilities $P(Y=y|X=x)$, the output probabilities $P(Y=y)$, and the posterior probabilities $P(X=x|Y=y)$.\n\nThe transition probabilities are given by the problem description:\n-   $P(Y=0|X=0) = 1-\\alpha$ (successful transmission of 0)\n-   $P(Y=\\lambda|X=0) = \\alpha$ (erasure of 0)\n-   $P(Y=1|X=0) = 0$ (no bit flips)\n-   $P(Y=1|X=1) = 1-\\alpha$ (successful transmission of 1)\n-   $P(Y=\\lambda|X=1) = \\alpha$ (erasure of 1)\n-   $P(Y=0|X=1) = 0$ (no bit flips)\n\nNow, we find the output probabilities $P(Y=y)$ using the law of total probability, $P(Y=y) = \\sum_{x} P(Y=y|X=x)P(X=x)$:\n-   $P(Y=0) = P(Y=0|X=0)P(X=0) + P(Y=0|X=1)P(X=1) = (1-\\alpha)p + 0 \\cdot (1-p) = p(1-\\alpha)$\n-   $P(Y=1) = P(Y=1|X=0)P(X=0) + P(Y=1|X=1)P(X=1) = 0 \\cdot p + (1-\\alpha)(1-p) = (1-p)(1-\\alpha)$\n-   $P(Y=\\lambda) = P(Y=\\lambda|X=0)P(X=0) + P(Y=\\lambda|X=1)P(X=1) = \\alpha p + \\alpha(1-p) = \\alpha(p+1-p) = \\alpha$\n\nNext, we determine the posterior probabilities $P(X=x|Y=y)$ using Bayes' theorem, $P(X=x|Y=y) = \\frac{P(Y=y|X=x)P(X=x)}{P(Y=y)}$:\n-   If $Y=0$:\n    $P(X=0|Y=0) = \\frac{P(Y=0|X=0)P(X=0)}{P(Y=0)} = \\frac{(1-\\alpha)p}{p(1-\\alpha)} = 1$.\n    $P(X=1|Y=0) = 0$.\n    The uncertainty about X is zero when Y=0 is received. Thus, $H(X|Y=0) = 0$.\n-   If $Y=1$:\n    $P(X=1|Y=1) = \\frac{P(Y=1|X=1)P(X=1)}{P(Y=1)} = \\frac{(1-\\alpha)(1-p)}{(1-p)(1-\\alpha)} = 1$.\n    $P(X=0|Y=1) = 0$.\n    The uncertainty about X is zero when Y=1 is received. Thus, $H(X|Y=1) = 0$.\n-   If $Y=\\lambda$:\n    $P(X=0|Y=\\lambda) = \\frac{P(Y=\\lambda|X=0)P(X=0)}{P(Y=\\lambda)} = \\frac{\\alpha p}{\\alpha} = p$.\n    $P(X=1|Y=\\lambda) = \\frac{P(Y=\\lambda|X=1)P(X=1)}{P(Y=\\lambda)} = \\frac{\\alpha (1-p)}{\\alpha} = 1-p$.\n    The conditional distribution of $X$ given $Y=\\lambda$ is the same as the original distribution of $X$. The conditional entropy is therefore $H(X|Y=\\lambda) = -[p \\log_{2}(p) + (1-p) \\log_{2}(1-p)] = H(X)$.\n\nNow we can calculate the total conditional entropy $H(X|Y)$:\n$$H(X|Y) = P(Y=0)H(X|Y=0) + P(Y=1)H(X|Y=1) + P(Y=\\lambda)H(X|Y=\\lambda)$$\n$$H(X|Y) = [p(1-\\alpha)] \\cdot 0 + [(1-p)(1-\\alpha)] \\cdot 0 + \\alpha \\cdot H(X)$$\n$$H(X|Y) = \\alpha H(X)$$\n\nFinally, we compute the mutual information:\n$$I(X;Y) = H(X) - H(X|Y) = H(X) - \\alpha H(X) = (1-\\alpha)H(X)$$\nSubstituting the expression for $H(X)$:\n$$I(X;Y) = (1-\\alpha)[-p \\log_{2}(p) - (1-p) \\log_{2}(1-p)]$$\nThis expression represents the mutual information between the input and output of the binary erasure channel for the given source distribution.", "answer": "$$\\boxed{(1-\\alpha)(-p \\log_{2}(p) - (1-p) \\log_{2}(1-p))}$$", "id": "1622685"}, {"introduction": "Real-world communication systems can be complex, often exhibiting different behaviors under various conditions. This problem presents a \"Forgetful Binary Channel,\" a hybrid model that sometimes operates as a standard Binary Symmetric Channel (BSC) and at other times outputs a random bit, irrespective of the input. Your task is to analyze this composite behavior, model it as a single equivalent channel, and determine its ultimate information-carrying limitâ€”its capacity.", "problem": "Consider a \"Forgetful Binary Channel\", a discrete memoryless communication channel with a binary input alphabet $X \\in \\{0, 1\\}$ and a binary output alphabet $Y \\in \\{0, 1\\}$. The channel's behavior is governed by two probabilistic modes. With a probability $q$, the channel enters a \"forgetful\" state where it completely ignores the input bit and transmits a random bit, meaning the output $Y$ is $0$ with probability $1/2$ and $1$ with probability $1/2$, regardless of the input $X$. With the remaining probability $1-q$, the channel acts as a standard Binary Symmetric Channel (BSC), which correctly transmits the input bit with probability $1-p$ and flips the input bit (i.e., transmits a $0$ as a $1$ or a $1$ as a $0$) with a crossover probability $p$. The parameters $p$ and $q$ are real numbers in the interval $[0, 1]$.\n\nDetermine the capacity of this Forgetful Binary Channel. Provide your answer as a closed-form analytic expression in terms of the parameters $p$ and $q$. Use base-2 logarithms in your final expression.", "solution": "Let $X \\in \\{0,1\\}$ be the input and $Y \\in \\{0,1\\}$ the output. The channel operates in two modes selected independently for each use:\n- With probability $q$ (forgetful mode), $Y$ is independent of $X$ and is $0$ or $1$ with probability $\\frac{1}{2}$ each.\n- With probability $1-q$ (BSC mode), the channel behaves as a Binary Symmetric Channel with crossover probability $p$.\n\nCondition on the input $X$ and average over the two modes to obtain the overall transition probabilities. For any $x \\in \\{0,1\\}$,\n$$\n\\Pr(Y \\neq X \\mid X=x) \\;=\\; q \\cdot \\frac{1}{2} \\;+\\; (1-q)\\,p,\n$$\nand\n$$\n\\Pr(Y = X \\mid X=x) \\;=\\; q \\cdot \\frac{1}{2} \\;+\\; (1-q)\\,(1-p).\n$$\nDefine the effective crossover probability\n$$\nr \\;\\triangleq\\; (1-q)\\,p \\;+\\; \\frac{q}{2}.\n$$\nThen, for $X=0$, $\\Pr(Y=1 \\mid X=0)=r$ and $\\Pr(Y=0 \\mid X=0)=1-r$, and for $X=1$, $\\Pr(Y=0 \\mid X=1)=r$ and $\\Pr(Y=1 \\mid X=1)=1-r$. Therefore, the overall channel is exactly a Binary Symmetric Channel with crossover probability $r$.\n\nThe capacity of a Binary Symmetric Channel with crossover probability $r$ (in bits per channel use, with base-2 logarithms) is achieved by the uniform input distribution $\\Pr(X=0)=\\Pr(X=1)=\\frac{1}{2}$ and equals\n$$\nC \\;=\\; I(X;Y) \\;=\\; H(Y) - H(Y \\mid X).\n$$\nUnder the uniform input, $H(Y)=1$ for a BSC, and $H(Y \\mid X) = h_{2}(r)$, where the binary entropy function is $h_{2}(r) = - r \\log_{2} r - (1-r) \\log_{2} (1-r)$. Hence,\n$$\nC \\;=\\; 1 - h_{2}(r) \\;=\\; 1 + r \\log_{2} r + (1-r) \\log_{2} (1-r),\n$$\nwith $r = (1-q)p + \\frac{q}{2}$.\n\nSubstituting $r$ in terms of $p$ and $q$ gives the capacity as\n$$\nC(p,q) \\;=\\; 1 + \\left((1-q)p + \\frac{q}{2}\\right)\\log_{2}\\left((1-q)p + \\frac{q}{2}\\right) + \\left(1 - (1-q)p - \\frac{q}{2}\\right)\\log_{2}\\left(1 - (1-q)p - \\frac{q}{2}\\right).\n$$\nThis expression reduces to the standard BSC capacity $1 - h_{2}(p)$ when $q=0$, and to $0$ when $q=1$ (since then $r=\\frac{1}{2}$), as expected.", "answer": "$$\\boxed{1 + \\left((1-q)p + \\frac{q}{2}\\right)\\log_{2}\\left((1-q)p + \\frac{q}{2}\\right) + \\left(1 - (1-q)p - \\frac{q}{2}\\right)\\log_{2}\\left(1 - (1-q)p - \\frac{q}{2}\\right)}$$", "id": "1622700"}, {"introduction": "Information loss is not always a result of random noise; it can also be a deterministic outcome of a channel's design. The \"Pair-Set Channel\" provides a perfect, noise-free illustration of this concept, where the channel faithfully transmits the elements of an ordered pair but discards the information about their original sequence. By calculating the capacity of this channel, you will uncover the fundamental relationship between the number of distinguishable outcomes and a channel's maximum achievable information rate.", "problem": "Consider a discrete memoryless channel called the \"Pair-Set Channel\". The input alphabet $\\mathcal{X}$ consists of all possible ordered pairs $(i, j)$ of distinct integers drawn from the set $S = \\{1, 2, \\ldots, N\\}$, where $N \\ge 2$ is an integer. The output alphabet $\\mathcal{Y}$ consists of all possible two-element subsets $\\{i, j\\}$ of the set $S$.\n\nThe channel operates deterministically as follows: for any input $(i, j) \\in \\mathcal{X}$, the channel produces the output $\\{i, j\\} \\in \\mathcal{Y}$. This means the channel preserves the identities of the two numbers but discards the information about their original order.\n\nDetermine the capacity of this channel as a function of $N$. Express the capacity in bits as a closed-form analytic expression in terms of $N$.", "solution": "Let the input alphabet be $\\mathcal{X}=\\{(i,j):i,j\\in S,\\,i\\neq j\\}$, so $|\\mathcal{X}|=N(N-1)$, and the output alphabet be $\\mathcal{Y}=\\{\\{i,j\\}:i,j\\in S,\\,i<j\\}$, so $|\\mathcal{Y}|=\\binom{N}{2}$. The channel is deterministic with mapping $f:\\mathcal{X}\\to\\mathcal{Y}$ given by $f((i,j))=\\{i,j\\}$.\n\nFor any input distribution $P_{X}$, since $Y=f(X)$ is a deterministic function of $X$, we have\n$$\nH(Y|X)=0,\n$$\nso the mutual information is\n$$\nI(X;Y)=H(Y)-H(Y|X)=H(Y).\n$$\nTherefore, the channel capacity is\n$$\nC=\\max_{P_{X}}I(X;Y)=\\max_{P_{X}}H(Y)=\\max_{P_{Y}\\,\\text{achievable}}H(Y).\n$$\nBy the entropy bound, for any $P_{Y}$ supported on $\\mathcal{Y}$,\n$$\nH(Y)\\leq \\log_{2}|\\mathcal{Y}|=\\log_{2}\\binom{N}{2},\n$$\nwith equality if and only if $Y$ is uniform on $\\mathcal{Y}$.\n\nWe now show that a uniform $P_{Y}$ is achievable. Choose $P_{X}$ uniform over all ordered pairs:\n$$\nP_{X}(i,j)=\\frac{1}{N(N-1)}\\quad\\text{for all }i\\neq j.\n$$\nFor any output $y=\\{i,j\\}$, there are exactly two preimages $(i,j)$ and $(j,i)$, hence\n$$\nP_{Y}(\\{i,j\\})=P_{X}(i,j)+P_{X}(j,i)=\\frac{2}{N(N-1)}=\\frac{1}{\\binom{N}{2}},\n$$\nso $Y$ is uniform over $\\mathcal{Y}$ and\n$$\nH(Y)=\\log_{2}\\binom{N}{2}.\n$$\nTherefore, the capacity is\n$$\nC=\\log_{2}\\binom{N}{2}.\n$$", "answer": "$$\\boxed{\\log_{2}\\binom{N}{2}}$$", "id": "1622705"}]}