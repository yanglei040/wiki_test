{"hands_on_practices": [{"introduction": "Before we can measure how much information a channel can carry, we must first ask a more fundamental question: what is required for information to be transmitted at all? This first exercise explores a crucial corner case by modeling a system with a deterministic, unchanging input. Its purpose is to solidify the foundational concept that information transfer requires initial uncertainty at the source.\n\nBy analyzing this scenario [@problem_id:1648908], you will discover that without any variation in the input, no information can be conveyed, regardless of the channel's characteristics. This practice helps clarify the role of source entropy $H(X)$ as an absolute upper bound on the mutual information $I(X;Y)$.", "problem": "Consider a prototype for a single-bit digital memory cell. Due to a manufacturing defect, the \"write\" mechanism is permanently stuck, causing the cell to always store the logical value '0'. The \"read\" operation, however, is subject to random noise.\n\nThis entire system can be modeled as a communication channel where the intended stored bit is the input, $X \\in \\{0, 1\\}$, and the bit value obtained during the read operation is the output, $Y \\in \\{0, 1\\}$. The noise is characterized by a Binary Symmetric Channel (BSC) model. A BSC is defined by a single parameter, the crossover probability $p$, which is the probability that a bit is flipped during transmission (or, in this case, during the read operation). Specifically, $P(Y=1|X=0) = P(Y=0|X=1) = p$. Assume that $0 < p < 1$.\n\nGiven the fault in the memory cell, the input is deterministic, meaning the distribution of the input random variable $X$ is $P(X=0) = 1$ and $P(X=1) = 0$.\n\nCalculate the mutual information $I(X;Y)$ between the intended stored bit $X$ and the read bit $Y$. Express your answer as a symbolic expression in terms of the crossover probability $p$. All logarithms are base 2.", "solution": "We model the system as a channel with input $X \\in \\{0,1\\}$ and output $Y \\in \\{0,1\\}$, where $P(X=0)=1$ and $P(X=1)=0$. The read process is a BSC with crossover probability $p$, so $P(Y=1 \\mid X=0)=p$ and $P(Y=0 \\mid X=0)=1-p$.\n\nBy definition, the mutual information is\n$$\nI(X;Y)=H(Y)-H(Y \\mid X).\n$$\nFirst, compute the marginal distribution of $Y$. Since $X=0$ with probability $1$,\n$$\nP(Y=1)=P(Y=1 \\mid X=0)P(X=0)=p \\cdot 1 = p, \\quad P(Y=0)=1-p.\n$$\nThus,\n$$\nH(Y)=-p \\log_{2}(p) - (1-p)\\log_{2}(1-p).\n$$\nNext, compute the conditional entropy:\n$$\nH(Y \\mid X)=\\sum_{x \\in \\{0,1\\}} P(X=x)\\,H(Y \\mid X=x)=P(X=0)H(Y \\mid X=0)+P(X=1)H(Y \\mid X=1).\n$$\nGiven $P(X=0)=1$ and $P(X=1)=0$, and that $Y \\mid X=0$ is Bernoulli with parameter $p$,\n$$\nH(Y \\mid X)=1 \\cdot \\big[-p \\log_{2}(p) - (1-p)\\log_{2}(1-p)\\big] + 0 \\cdot H(Y \\mid X=1)=-p \\log_{2}(p) - (1-p)\\log_{2}(1-p).\n$$\nTherefore,\n$$\nI(X;Y)=H(Y)-H(Y \\mid X)=\\big[-p \\log_{2}(p) - (1-p)\\log_{2}(1-p)\\big]-\\big[-p \\log_{2}(p) - (1-p)\\log_{2}(1-p)\\big]=0.\n$$\nEquivalently, using $I(X;Y)=H(X)-H(X \\mid Y)$ and $H(X)=0$ (since $P(X=0)=1$), and $H(X \\mid Y)=0$ (since $P(X=0 \\mid Y=y)=1$ for all $y$), also yields $I(X;Y)=0$.", "answer": "$$\\boxed{0}$$", "id": "1648908"}, {"introduction": "Having established the necessity of input uncertainty, we can now proceed to calculate a channel's maximum information rate, or its capacity. This problem presents a simple, deterministic channel where the relationship between input and output is perfectly predictable, but with a twist: multiple distinct inputs are mapped to the same output. This \"confusability\" is a key factor that limits a channel's performance.\n\nThis exercise [@problem_id:1648942] provides hands-on practice in calculating the capacity of a deterministic channel, where the formula simplifies to $C = \\max H(Y)$. You will learn to identify how the channel's structure constrains the possible output distributions and then find the specific input probabilities that achieve the maximum possible output entropy.", "problem": "A simple digital processing unit is designed to operate on a limited set of integer inputs. The unit accepts one of four possible input signals, represented by the integers in the set $\\mathcal{X} = \\{1, 2, 3, 4\\}$. It performs a fixed computation and produces an output integer from the set $\\mathcal{Y} = \\{0, 1, 2\\}$. The relationship between any input $x \\in \\mathcal{X}$ and the corresponding output $y \\in \\mathcal{Y}$ is governed by the deterministic function $y = x \\pmod 3$, where the result of the modulo operation is the non-negative remainder of the division of $x$ by 3.\n\nAssuming each use of this processing unit represents a single use of a communication channel, determine the capacity of this channel. Express your answer as a single analytic expression in units of bits per channel use. Use the base-2 logarithm in your calculations.", "solution": "Let $X \\in \\mathcal{X}=\\{1,2,3,4\\}$ and $Y \\in \\mathcal{Y}=\\{0,1,2\\}$ with the deterministic mapping $Y=f(X)=X \\bmod 3$. For a deterministic channel, the conditional entropy satisfies $H(Y|X)=0$. Therefore, the mutual information for an input distribution $P_{X}$ is\n$$\nI(X;Y)=H(Y)-H(Y|X)=H(Y).\n$$\nThe channel capacity is the maximum mutual information over all input distributions, hence\n$$\nC=\\max_{P_{X}} I(X;Y)=\\max_{P_{X}} H(Y).\n$$\nThe mapping groups inputs by their remainders: $1 \\mapsto 1$, $2 \\mapsto 2$, $3 \\mapsto 0$, $4 \\mapsto 1$. Thus, for any input distribution $P_{X}$, the induced output distribution satisfies\n$$\nP_{Y}(0)=P_{X}(3), \\quad P_{Y}(1)=P_{X}(1)+P_{X}(4), \\quad P_{Y}(2)=P_{X}(2).\n$$\nGiven any desired output distribution $(p_{0},p_{1},p_{2})$ with $p_{0}+p_{1}+p_{2}=1$ and $p_{i}\\geq 0$, one can choose, for example,\n$$\nP_{X}(3)=p_{0}, \\quad P_{X}(2)=p_{2}, \\quad P_{X}(1)=p_{1}, \\quad P_{X}(4)=0,\n$$\nwhich induces $P_{Y}=p$. Hence every distribution on $\\{0,1,2\\}$ is achievable, and the capacity reduces to the maximum entropy over a 3-point alphabet. The maximum of $H(Y)$ occurs at the uniform distribution, giving\n$$\n\\max_{P_{Y}} H(Y)=\\log_{2}(3).\n$$\nTherefore, the channel capacity is\n$$\nC=\\log_{2}(3) \\text{ bits per channel use.}\n$$", "answer": "$$\\boxed{\\log_{2}(3)}$$", "id": "1648942"}, {"introduction": "In practical communication systems, capacity is not just an abstract measure of \"bits per symbol\" but is often constrained by physical resources like time, energy, or bandwidth. This final practice moves from the abstract to the physical by introducing a channel where different symbols have different transmission durations. The goal is to determine the true information rate in bits per unit time.\n\nSolving this problem [@problem_id:1648953] requires a more advanced approach than simply maximizing entropy. It guides you through the process of optimizing the ratio of information generated to the time consumed, a common challenge in engineering. This exercise demonstrates how to find the capacity of a channel under a cost constraint, revealing a fundamental principle that connects information theory to resource optimization.", "problem": "A digital communication system utilizes a noiseless binary channel to transmit a stream of symbols from the input alphabet $\\{0, 1\\}$. The channel operates perfectly, such that the received output symbol is always identical to the transmitted input symbol. However, the physical transmitter imposes a temporal cost on the symbols. The time required to transmit a '0' symbol is $\\tau$, while the time required to transmit a '1' symbol is $2\\tau$.\n\nDetermine the capacity of this channel in bits per unit time. Your final answer should be a closed-form analytic expression in terms of the time parameter $\\tau$.", "solution": "Because the channel is noiseless, the information delivered per symbol equals the source entropy, but symbols have unequal durations. For an input distribution $p_{0}=\\Pr\\{0\\}$ and $p_{1}=\\Pr\\{1\\}=1-p_{0}$, the entropy per symbol is\n$$\nH(X)=-p_{0}\\log_{2}p_{0}-p_{1}\\log_{2}p_{1},\n$$\nand the average transmission time per symbol is\n$$\n\\mathbb{E}[T]=p_{0}\\tau+p_{1}(2\\tau)=\\tau(2-p_{0}).\n$$\nThe information rate (bits per unit time) for this input is $R=\\frac{H(X)}{\\mathbb{E}[T]}$, and the channel capacity is\n$$\nC=\\sup_{p_{0}\\in[0,1]}\\frac{H(X)}{\\mathbb{E}[T]}.\n$$\n\nRather than optimizing directly in $p_{0}$, we use a standard variational method for maximizing entropy rate under a cost (time) constraint. Define, for a general finite alphabet with symbol times $\\{t_{i}\\}$ and probabilities $\\{p_{i}\\}$,\n$$\nH=-\\sum_{i}p_{i}\\log_{2}p_{i},\\qquad \\mathbb{E}[T]=\\sum_{i}p_{i}t_{i}.\n$$\nFor a given nonnegative parameter $s$, maximize $H-s\\mathbb{E}[T]$ subject to $\\sum_{i}p_{i}=1$. Using $\\ln$ for natural logarithm, write $H=-\\frac{1}{\\ln 2}\\sum_{i}p_{i}\\ln p_{i}$ and form the Lagrangian\n$$\n\\mathcal{L}=-\\frac{1}{\\ln 2}\\sum_{i}p_{i}\\ln p_{i}-s\\sum_{i}p_{i}t_{i}-\\mu\\left(\\sum_{i}p_{i}-1\\right).\n$$\nStationarity $\\frac{\\partial \\mathcal{L}}{\\partial p_{i}}=0$ gives\n$$\n-\\frac{1}{\\ln 2}(\\ln p_{i}+1)-s t_{i}-\\mu=0 \\quad \\Longrightarrow \\quad \\ln p_{i}=-1-(\\ln 2)(s t_{i}+\\mu),\n$$\nhence\n$$\np_{i}=a\\,2^{-s t_{i}},\\qquad a=\\exp(-1)\\exp\\bigl(-(\\ln 2)\\mu\\bigr),\n$$\nwith $a$ determined by normalization:\n$$\na=\\frac{1}{\\sum_{j}2^{-s t_{j}}}.\n$$\nFor this maximizing distribution,\n$$\nH=-\\sum_{i}p_{i}\\log_{2}(a\\,2^{-s t_{i}})=-\\log_{2}a+s\\sum_{i}p_{i}t_{i}=-\\log_{2}a+s\\,\\mathbb{E}[T].\n$$\nBecause $a^{-1}=\\sum_{j}2^{-s t_{j}}$, we have\n$$\nH-s\\,\\mathbb{E}[T]=-\\log_{2}a=\\log_{2}\\left(\\sum_{j}2^{-s t_{j}}\\right).\n$$\nFor any $s$ with $\\sum_{j}2^{-s t_{j}}\\leq 1$, this yields $H-s\\,\\mathbb{E}[T]\\leq 0$, i.e., $\\frac{H}{\\mathbb{E}[T]}\\leq s$. The tightest bound is obtained at the unique $s=C$ satisfying\n$$\n\\sum_{j}2^{-C t_{j}}=1,\n$$\nfor which $a=1$ and $H=C\\,\\mathbb{E}[T]$, so the supremum of $\\frac{H}{\\mathbb{E}[T]}$ equals $C$.\n\nIn the given channel, there are two symbols with times $t_{0}=\\tau$ and $t_{1}=2\\tau$. Therefore $C$ is the unique positive solution of\n$$\n2^{-C\\tau}+2^{-2C\\tau}=1.\n$$\nLet $x=2^{-C\\tau}$. Then\n$$\nx+x^{2}=1 \\quad \\Longrightarrow \\quad x^{2}+x-1=0,\n$$\nwhose positive root is\n$$\nx=\\frac{\\sqrt{5}-1}{2}.\n$$\nThus\n$$\n2^{-C\\tau}=\\frac{\\sqrt{5}-1}{2}\\quad \\Longrightarrow \\quad -C\\tau=\\log_{2}\\!\\left(\\frac{\\sqrt{5}-1}{2}\\right),\n$$\nand hence\n$$\nC=-\\frac{1}{\\tau}\\log_{2}\\!\\left(\\frac{\\sqrt{5}-1}{2}\\right)=\\frac{1}{\\tau}\\log_{2}\\!\\left(\\frac{1+\\sqrt{5}}{2}\\right).\n$$\nThis is the channel capacity in bits per unit time, expressed in closed form in terms of $\\tau$.", "answer": "$$\\boxed{\\frac{1}{\\tau}\\log_{2}\\left(\\frac{1+\\sqrt{5}}{2}\\right)}$$", "id": "1648953"}]}