{"hands_on_practices": [{"introduction": "The goal of the Blahut-Arimoto algorithm is to find the input distribution that maximizes mutual information, $I(X;Y)$. Before tackling this optimization problem, it is essential to master the foundational skill of calculating $I(X;Y)$ for a *given* channel and input distribution. This practice problem [@problem_id:1605107] will guide you through the necessary steps, reinforcing the core definitions of entropy and conditional entropy.", "problem": "Consider a discrete memoryless communication channel with an input alphabet $\\mathcal{X} = \\{x_1, x_2\\}$ and an output alphabet $\\mathcal{Y} = \\{y_1, y_2, y_3\\}$. The behavior of the channel is characterized by the conditional probability transition matrix $P(y_j|x_i)$, where the element in the $i$-th row and $j$-th column represents the probability of receiving symbol $y_j$ given that symbol $x_i$ was sent. The matrix is given by:\n$$\nP_{Y|X} = \\begin{pmatrix} P(y_1|x_1) & P(y_2|x_1) & P(y_3|x_1) \\\\ P(y_1|x_2) & P(y_2|x_2) & P(y_3|x_2) \\end{pmatrix} = \\begin{pmatrix} 1/2 & 1/2 & 0 \\\\ 1/4 & 1/2 & 1/4 \\end{pmatrix}\n$$\nThe input symbols are chosen according to a non-uniform probability distribution $p(x)$, where $p(x_1) = 1/3$ and $p(x_2) = 2/3$.\n\nCalculate the mutual information $I(X;Y)$ between the input and the output for this channel and input distribution. Provide your answer as a single closed-form analytic expression in bits.", "solution": "The mutual information between input $X$ and output $Y$ is defined as $I(X;Y)=H(Y)-H(Y|X)$, measured in bits when logarithms are taken base $2$.\n\nFirst compute the output distribution $p(y_{j})$ using $p(y_{j})=\\sum_{i}p(x_{i})P(y_{j}\\mid x_{i})$ with $p(x_{1})=\\frac{1}{3}$ and $p(x_{2})=\\frac{2}{3}$:\n$$\np(y_{1})=\\frac{1}{3}\\cdot\\frac{1}{2}+\\frac{2}{3}\\cdot\\frac{1}{4}=\\frac{1}{3},\\quad\np(y_{2})=\\frac{1}{3}\\cdot\\frac{1}{2}+\\frac{2}{3}\\cdot\\frac{1}{2}=\\frac{1}{2},\\quad\np(y_{3})=\\frac{1}{3}\\cdot 0+\\frac{2}{3}\\cdot\\frac{1}{4}=\\frac{1}{6}.\n$$\nThus $p_{Y}=\\left(\\frac{1}{3},\\frac{1}{2},\\frac{1}{6}\\right)$ and\n$$\nH(Y)=-\\sum_{j}p(y_{j})\\log_{2}p(y_{j})\n= -\\left[\\frac{1}{3}\\log_{2}\\!\\left(\\frac{1}{3}\\right)+\\frac{1}{2}\\log_{2}\\!\\left(\\frac{1}{2}\\right)+\\frac{1}{6}\\log_{2}\\!\\left(\\frac{1}{6}\\right)\\right].\n$$\nUsing $\\log_{2}\\!\\left(\\frac{1}{3}\\right)=-\\log_{2}(3)$, $\\log_{2}\\!\\left(\\frac{1}{2}\\right)=-1$, and $\\log_{2}\\!\\left(\\frac{1}{6}\\right)=-\\log_{2}(6)$ with $\\log_{2}(6)=1+\\log_{2}(3)$, we obtain\n$$\nH(Y)=\\frac{1}{3}\\log_{2}(3)+\\frac{1}{2}+\\frac{1}{6}\\log_{2}(6)\n=\\frac{1}{3}\\log_{2}(3)+\\frac{1}{2}+\\frac{1}{6}\\bigl(1+\\log_{2}(3)\\bigr)\n=\\frac{1}{2}\\log_{2}(3)+\\frac{2}{3}.\n$$\n\nNext compute the conditional entropy $H(Y|X)=\\sum_{i}p(x_{i})H(Y|X=x_{i})$. For $x_{1}$, the conditional distribution is $(\\frac{1}{2},\\frac{1}{2},0)$, so\n$$\nH(Y|X=x_{1})=-\\left[\\tfrac{1}{2}\\log_{2}\\!\\left(\\tfrac{1}{2}\\right)+\\tfrac{1}{2}\\log_{2}\\!\\left(\\tfrac{1}{2}\\right)+0\\right]=1.\n$$\nFor $x_{2}$, the conditional distribution is $\\left(\\frac{1}{4},\\frac{1}{2},\\frac{1}{4}\\right)$, hence\n$$\nH(Y|X=x_{2})=-\\left[\\tfrac{1}{4}\\log_{2}\\!\\left(\\tfrac{1}{4}\\right)+\\tfrac{1}{2}\\log_{2}\\!\\left(\\tfrac{1}{2}\\right)+\\tfrac{1}{4}\\log_{2}\\!\\left(\\tfrac{1}{4}\\right)\\right]\n=-\\left[\\tfrac{1}{4}(-2)+\\tfrac{1}{2}(-1)+\\tfrac{1}{4}(-2)\\right]=\\tfrac{3}{2}.\n$$\nTherefore,\n$$\nH(Y|X)=\\frac{1}{3}\\cdot 1+\\frac{2}{3}\\cdot\\frac{3}{2}=\\frac{1}{3}+1=\\frac{4}{3}.\n$$\n\nFinally,\n$$\nI(X;Y)=H(Y)-H(Y|X)=\\left(\\frac{1}{2}\\log_{2}(3)+\\frac{2}{3}\\right)-\\frac{4}{3}\n=\\frac{1}{2}\\log_{2}(3)-\\frac{2}{3}\\ \\text{bits}.\n$$", "answer": "$$\\boxed{\\frac{1}{2}\\log_{2}(3)-\\frac{2}{3}}$$", "id": "1605107"}, {"introduction": "With the fundamentals in place, we can now engage directly with the mechanics of the Blahut-Arimoto algorithm. The power of this method lies in its iterative update rule, which progressively refines an input distribution to increase mutual information. This exercise [@problem_id:1605109] provides a concrete, step-by-step application of one full iteration, allowing you to see exactly how the probabilities are adjusted.", "problem": "Consider a discrete memoryless channel with a binary input alphabet $\\mathcal{X} = \\{x_1, x_2\\}$ and a binary output alphabet $\\mathcal{Y} = \\{y_1, y_2\\}$. The behavior of the channel is described by the matrix of conditional probabilities $P(y_j|x_i)$, where the rows correspond to the inputs $x_i$ and the columns to the outputs $y_j$. The specific channel matrix is given by:\n$$\nP(y|x) = \\begin{pmatrix} P(y_1|x_1) & P(y_2|x_1) \\\\ P(y_1|x_2) & P(y_2|x_2) \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ \\frac{1}{2} & \\frac{1}{2} \\end{pmatrix}\n$$\nThe Blahut-Arimoto algorithm is an iterative procedure for computing the channel capacity and the input probability distribution $p(x)$ that achieves it. A single iteration of this algorithm updates an initial probability distribution $p_k(x)$ to a new distribution $p_{k+1}(x)$. The update consists of the following steps:\n1.  Compute the reverse conditional probability distribution $q_k(x|y)$ from the current input distribution $p_k(x)$ and the channel matrix $P(y|x)$:\n    $$\n    q_k(x|y) = \\frac{p_k(x) P(y|x)}{\\sum_{x' \\in \\mathcal{X}} p_k(x') P(y|x')}\n    $$\n2.  Compute the new, un-normalized weights $c_k(x)$ for each input symbol:\n    $$\n    c_k(x) = \\prod_{y \\in \\mathcal{Y}} [q_k(x|y)]^{P(y|x)}\n    $$\n    For this calculation, use the convention that $0^0 = 1$.\n3.  Normalize the weights to obtain the updated input distribution $p_{k+1}(x)$:\n    $$\n    p_{k+1}(x) = \\frac{c_k(x)}{\\sum_{x' \\in \\mathcal{X}} c_k(x')}\n    $$\nYour task is to perform one full iteration of this algorithm. Starting with a uniform input distribution $p_0(x) = (p_0(x_1), p_0(x_2)) = (\\frac{1}{2}, \\frac{1}{2})$, calculate the updated distribution $p_1(x) = (p_1(x_1), p_1(x_2))$.\n\nProvide your final answer as an exact analytic expression in the form of a row matrix.", "solution": "The goal is to compute the probability distribution $p_1(x) = (p_1(x_1), p_1(x_2))$ after one iteration of the Blahut-Arimoto algorithm, starting from a uniform distribution $p_0(x_1) = 1/2$, $p_0(x_2) = 1/2$. The channel matrix is given by:\n$$\nP(y|x) = \\begin{pmatrix} 1 & 0 \\\\ \\frac{1}{2} & \\frac{1}{2} \\end{pmatrix}\n$$\n\nThe first step is to calculate the output probability distribution $p_0(y)$ corresponding to the initial input distribution $p_0(x)$.\n$$\np_0(y_1) = \\sum_{i=1}^{2} p_0(x_i) P(y_1|x_i) = p_0(x_1)P(y_1|x_1) + p_0(x_2)P(y_1|x_2)\n$$\n$$\np_0(y_1) = \\left(\\frac{1}{2}\\right)(1) + \\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right) = \\frac{1}{2} + \\frac{1}{4} = \\frac{3}{4}\n$$\n$$\np_0(y_2) = \\sum_{i=1}^{2} p_0(x_i) P(y_2|x_i) = p_0(x_1)P(y_2|x_1) + p_0(x_2)P(y_2|x_2)\n$$\n$$\np_0(y_2) = \\left(\\frac{1}{2}\\right)(0) + \\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right) = 0 + \\frac{1}{4} = \\frac{1}{4}\n$$\nAs a check, $p_0(y_1) + p_0(y_2) = 3/4 + 1/4 = 1$.\n\nNext, we compute the reverse conditional probabilities $q_0(x|y)$.\nFor $y=y_1$:\n$$\nq_0(x_1|y_1) = \\frac{p_0(x_1)P(y_1|x_1)}{p_0(y_1)} = \\frac{(1/2)(1)}{3/4} = \\frac{1/2}{3/4} = \\frac{2}{3}\n$$\n$$\nq_0(x_2|y_1) = \\frac{p_0(x_2)P(y_1|x_2)}{p_0(y_1)} = \\frac{(1/2)(1/2)}{3/4} = \\frac{1/4}{3/4} = \\frac{1}{3}\n$$\nCheck: $q_0(x_1|y_1) + q_0(x_2|y_1) = 2/3 + 1/3 = 1$.\n\nFor $y=y_2$:\n$$\nq_0(x_1|y_2) = \\frac{p_0(x_1)P(y_2|x_1)}{p_0(y_2)} = \\frac{(1/2)(0)}{1/4} = 0\n$$\n$$\nq_0(x_2|y_2) = \\frac{p_0(x_2)P(y_2|x_2)}{p_0(y_2)} = \\frac{(1/2)(1/2)}{1/4} = \\frac{1/4}{1/4} = 1\n$$\nCheck: $q_0(x_1|y_2) + q_0(x_2|y_2) = 0 + 1 = 1$.\n\nNow, we compute the un-normalized weights $c_0(x)$.\nFor $x=x_1$:\n$$\nc_0(x_1) = [q_0(x_1|y_1)]^{P(y_1|x_1)} \\cdot [q_0(x_1|y_2)]^{P(y_2|x_1)} = \\left(\\frac{2}{3}\\right)^1 \\cdot (0)^0\n$$\nUsing the specified convention $0^0=1$:\n$$\nc_0(x_1) = \\frac{2}{3} \\cdot 1 = \\frac{2}{3}\n$$\n\nFor $x=x_2$:\n$$\nc_0(x_2) = [q_0(x_2|y_1)]^{P(y_1|x_2)} \\cdot [q_0(x_2|y_2)]^{P(y_2|x_2)} = \\left(\\frac{1}{3}\\right)^{1/2} \\cdot (1)^{1/2}\n$$\n$$\nc_0(x_2) = \\sqrt{\\frac{1}{3}} \\cdot 1 = \\frac{1}{\\sqrt{3}}\n$$\n\nFinally, we find the new distribution $p_1(x)$ by normalizing these weights. The normalization constant is the sum of the weights:\n$$\nZ = c_0(x_1) + c_0(x_2) = \\frac{2}{3} + \\frac{1}{\\sqrt{3}} = \\frac{2}{3} + \\frac{\\sqrt{3}}{3} = \\frac{2+\\sqrt{3}}{3}\n$$\nNow we find each component of $p_1(x)$:\n$$\np_1(x_1) = \\frac{c_0(x_1)}{Z} = \\frac{2/3}{(2+\\sqrt{3})/3} = \\frac{2}{2+\\sqrt{3}}\n$$\nTo simplify, we rationalize the denominator:\n$$\np_1(x_1) = \\frac{2}{2+\\sqrt{3}} \\cdot \\frac{2-\\sqrt{3}}{2-\\sqrt{3}} = \\frac{2(2-\\sqrt{3})}{2^2 - (\\sqrt{3})^2} = \\frac{4-2\\sqrt{3}}{4-3} = 4 - 2\\sqrt{3}\n$$\n$$\np_1(x_2) = \\frac{c_0(x_2)}{Z} = \\frac{1/\\sqrt{3}}{(2+\\sqrt{3})/3} = \\frac{1}{\\sqrt{3}} \\cdot \\frac{3}{2+\\sqrt{3}} = \\frac{\\sqrt{3}}{2+\\sqrt{3}}\n$$\nRationalizing the denominator:\n$$\np_1(x_2) = \\frac{\\sqrt{3}}{2+\\sqrt{3}} \\cdot \\frac{2-\\sqrt{3}}{2-\\sqrt{3}} = \\frac{\\sqrt{3}(2-\\sqrt{3})}{4-3} = \\frac{2\\sqrt{3}-3}{1} = 2\\sqrt{3} - 3\n$$\nAs a final check, let's sum the probabilities:\n$$\np_1(x_1) + p_1(x_2) = (4 - 2\\sqrt{3}) + (2\\sqrt{3} - 3) = 4 - 3 = 1\n$$\nThe updated distribution is $p_1(x) = (4 - 2\\sqrt{3}, 2\\sqrt{3} - 3)$.\nIn the required row matrix format, this is $\\begin{pmatrix}4 - 2\\sqrt{3} & 2\\sqrt{3} - 3\\end{pmatrix}$.", "answer": "$$\\boxed{\\begin{pmatrix} 4 - 2\\sqrt{3} & 2\\sqrt{3} - 3 \\end{pmatrix}}$$", "id": "1605109"}, {"introduction": "The practical application of any algorithm requires understanding its properties and potential pitfalls. This exercise [@problem_id:1605115] explores a crucial aspect of the Blahut-Arimoto algorithm's behavior by examining the consequences of initializing an input symbol's probability to zero. By analyzing the multiplicative nature of the update rule, you will gain a deeper insight into how the algorithm explores the space of possible solutions.", "problem": "Consider a Discrete Memoryless Channel (DMC) with an input alphabet $\\mathcal{X} = \\{x_1, x_2, x_3\\}$ and an output alphabet $\\mathcal{Y} = \\{y_1, y_2\\}$. The channel is characterized by the following conditional probability matrix $P(Y|X)$, where the entry in row $i$ and column $j$ is $p(y_j|x_i)$:\n$$\nP(Y|X) = \n\\begin{pmatrix} \n0.8 & 0.2 \\\\\n0.5 & 0.5 \\\\\n0.1 & 0.9 \n\\end{pmatrix}\n$$\nThe Blahut-Arimoto algorithm is an iterative procedure used to find the input probability distribution $p(x)$ that achieves channel capacity. Let $p_k(x_i)$ denote the probability of input symbol $x_i$ at iteration $k$. The algorithm updates the probabilities according to the following rule:\n$$\np_{k+1}(x_i) = \\frac{p_k(x_i) \\exp\\left( \\sum_{j=1}^{2} p(y_j|x_i) \\ln \\frac{p(y_j|x_i)}{p_k(y_j)} \\right)}{Z_k}\n$$\nwhere $p_k(y_j) = \\sum_{i=1}^{3} p(y_j|x_i) p_k(x_i)$ is the probability of output symbol $y_j$ at iteration $k$, and $Z_k$ is a normalization factor that ensures $\\sum_{i=1}^{3} p_{k+1}(x_i) = 1$.\n\nSuppose the algorithm is initialized at $k=0$ with the input distribution $p_0(x) = (p_0(x_1), p_0(x_2), p_0(x_3)) = (0, 0.5, 0.5)$. Which of the following statements accurately describes the value of the probability mass $p_k(x_1)$ for any iteration $k \\geq 1$?\n\nA. $p_k(x_1)$ will remain at 0 for all $k \\geq 1$.\n\nB. $p_k(x_1)$ will become non-zero after the first iteration and will converge towards the optimal probability for $x_1$.\n\nC. The value of $p_k(x_1)$ depends on the specific non-zero values in the channel matrix and cannot be determined without running the full algorithm.\n\nD. The algorithm will encounter a division-by-zero error on the first iteration because $p_k(y_j)$ will be zero for some $j$.\n\nE. $p_k(x_1)$ will increase to a small positive value in the first iteration and then decrease back to zero in the second iteration.", "solution": "The Blahut-Arimoto update for the input distribution is\n$$\np_{k+1}(x_{i})=\\frac{p_{k}(x_{i})\\exp\\left(\\sum_{j=1}^{2}p(y_{j}\\mid x_{i})\\ln\\frac{p(y_{j}\\mid x_{i})}{p_{k}(y_{j})}\\right)}{Z_{k}},\n$$\nwith\n$$\np_{k}(y_{j})=\\sum_{i=1}^{3}p(y_{j}\\mid x_{i})p_{k}(x_{i}),\\quad Z_{k}=\\sum_{i=1}^{3}p_{k}(x_{i})\\exp\\left(\\sum_{j=1}^{2}p(y_{j}\\mid x_{i})\\ln\\frac{p(y_{j}\\mid x_{i})}{p_{k}(y_{j})}\\right).\n$$\nThe initialization is $p_{0}(x_{1})=0$, $p_{0}(x_{2})=\\frac{1}{2}$, $p_{0}(x_{3})=\\frac{1}{2}$, and the channel matrix entries are $p(y_{1}\\mid x_{1})=\\frac{4}{5}$, $p(y_{2}\\mid x_{1})=\\frac{1}{5}$, $p(y_{1}\\mid x_{2})=\\frac{1}{2}$, $p(y_{2}\\mid x_{2})=\\frac{1}{2}$, $p(y_{1}\\mid x_{3})=\\frac{1}{10}$, $p(y_{2}\\mid x_{3})=\\frac{9}{10}$. First compute the output distribution at $k=0$:\n$$\np_{0}(y_{1})=\\sum_{i=1}^{3}p(y_{1}\\mid x_{i})p_{0}(x_{i})=\\frac{1}{2}\\cdot\\frac{1}{2}+\\frac{1}{2}\\cdot\\frac{1}{10}=\\frac{1}{4}+\\frac{1}{20}=\\frac{3}{10},\n$$\n$$\np_{0}(y_{2})=\\sum_{i=1}^{3}p(y_{2}\\mid x_{i})p_{0}(x_{i})=\\frac{1}{2}\\cdot\\frac{1}{2}+\\frac{1}{2}\\cdot\\frac{9}{10}=\\frac{1}{4}+\\frac{9}{20}=\\frac{7}{10}.\n$$\nBoth $p_{0}(y_{1})$ and $p_{0}(y_{2})$ are strictly positive, so all logarithms in the update are finite. Then, using the update for $i=1$ at $k=0$,\n$$\np_{1}(x_{1})=\\frac{p_{0}(x_{1})\\exp\\left(\\sum_{j=1}^{2}p(y_{j}\\mid x_{1})\\ln\\frac{p(y_{j}\\mid x_{1})}{p_{0}(y_{j})}\\right)}{Z_{0}}=\\frac{0\\cdot\\exp(\\cdot)}{Z_{0}}=0,\n$$\nwhere $Z_{0}>0$ because at least $i=2,3$ contribute strictly positive terms. This shows $p_{1}(x_{1})=0$.\n\nWe now prove by induction that $p_{k}(x_{1})=0$ for all $k\\geq 1$. The base case $k=1$ holds as shown. Assume $p_{k}(x_{1})=0$ and $p_{k}(x_{2})>0$, $p_{k}(x_{3})>0$ (the latter two hold because the update multiplies positive $p_{k-1}(x_{i})$ by positive exponentials and renormalizes). Then for each output symbol,\n$$\np_{k}(y_{j})=\\sum_{i=1}^{3}p(y_{j}\\mid x_{i})p_{k}(x_{i})=\\sum_{i=2}^{3}p(y_{j}\\mid x_{i})p_{k}(x_{i})>0,\n$$\nbecause $p(y_{j}\\mid x_{2})$ and $p(y_{j}\\mid x_{3})$ are strictly positive and $p_{k}(x_{2}),p_{k}(x_{3})>0$. Therefore the exponent for $i=1$ is finite and\n$$\np_{k+1}(x_{1})=\\frac{p_{k}(x_{1})\\exp\\left(\\sum_{j=1}^{2}p(y_{j}\\mid x_{1})\\ln\\frac{p(y_{j}\\mid x_{1})}{p_{k}(y_{j})}\\right)}{Z_{k}}=\\frac{0\\cdot\\exp(\\cdot)}{Z_{k}}=0,\n$$\nwith $Z_{k}>0$ for the same reason as before. Hence, by induction, $p_{k}(x_{1})=0$ for all $k\\geq 1$.\n\nThis establishes that the zero probability on $x_{1}$ is preserved by the Blahut-Arimoto multiplicative update. There is no division-by-zero at the first iteration because $p_{0}(y_{1})=\\frac{3}{10}$ and $p_{0}(y_{2})=\\frac{7}{10}$ are strictly positive. Therefore, among the options, the accurate statement is that $p_{k}(x_{1})$ remains zero for all iterations.", "answer": "$$\\boxed{A}$$", "id": "1605115"}]}