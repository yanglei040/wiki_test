{"hands_on_practices": [{"introduction": "To truly grasp the implications of the converse to the channel coding theorem, it is often best to start with an extreme case. This first exercise explores a hypothetical \"Collapse Channel,\" where all distinct inputs are mapped to a single output. By analyzing this scenario [@problem_id:1613884], you will calculate a channel capacity of zero and see in the clearest possible terms how the converse theorem establishes a hard limit, proving that no information can be reliably transmitted when the channel fundamentally destroys all distinctions between input signals.", "problem": "Consider a hypothetical discrete memoryless channel called the \"Collapse Channel\". This channel has an input alphabet $\\mathcal{X}$ containing $m$ distinct symbols, where $m \\ge 2$, and an output alphabet $\\mathcal{Y}$ consisting of a single symbol, $\\{c\\}$. The channel is characterized by its transition probabilities: for any input symbol $x \\in \\mathcal{X}$, the output is always $c$. That is, the conditional probability $p(y=c|x) = 1$ for all $x \\in \\mathcal{X}$.\n\nLet the capacity of this channel be $C$, measured in bits per channel use. Based on this definition, determine the value of $C$ and identify the correct implication of the converse to the channel coding theorem for this specific channel.\n\nWhich of the following statements is correct?\n\nA. The capacity is $C = \\log_2(m)$ bits/use. This implies that information can be transmitted reliably up to a rate of $\\log_2(m)$ bits/use.\n\nB. The capacity is $C=0$ bits/use. This implies that for any transmission rate $R > 0$, it is impossible to achieve an arbitrarily low probability of error.\n\nC. The capacity is $C=1$ bit/use. This implies that one bit of information can be reliably transmitted per channel use, regardless of the size of the input alphabet.\n\nD. The capacity is $C=0$ bits/use. This implies that reliable communication is still possible, but only if the block length of the code used is made infinitely long.\n\nE. The capacity is $C=0$ bits/use. This implies that information can be transmitted without error, but only at an infinitesimally small, non-zero rate $R \\to 0^{+}$.", "solution": "By definition, the capacity of a discrete memoryless channel is\n$$\nC=\\max_{p(x)} I(X;Y),\n$$\nwhere $I(X;Y)$ is the mutual information between the input $X$ and output $Y$ induced by the input distribution $p(x)$ and the channel transition probabilities.\n\nFor the Collapse Channel, the transition law is $p(y=c \\mid x)=1$ for all $x \\in \\mathcal{X}$. Hence the output $Y$ is a constant random variable equal to $c$, regardless of the input. Using the identity\n$$\nI(X;Y)=H(Y)-H(Y \\mid X),\n$$\nwe evaluate each term:\n- Since $Y$ is constant, its entropy is\n$$\nH(Y)=0.\n$$\n- Because $Y$ is a deterministic function of $X$ (specifically, $Y=c$ for all $X$), the conditional entropy is\n$$\nH(Y \\mid X)=0.\n$$\nTherefore,\n$$\nI(X;Y)=H(Y)-H(Y \\mid X)=0-0=0\n$$\nfor every input distribution $p(x)$. Maximizing over $p(x)$ gives\n$$\nC=\\max_{p(x)} I(X;Y)=0 \\text{ bits/use}.\n$$\n\nThe converse to the channel coding theorem states that for any rate $R>C$, the average probability of error of any sequence of codes cannot be made arbitrarily small; specifically, it is bounded away from zero uniformly in block length. Since $C=0$ for this channel, this means that for any $R>0$, it is impossible to achieve an arbitrarily low probability of error. This matches statement B and contradicts statements A, C, D, and E.", "answer": "$$\\boxed{B}$$", "id": "1613884"}, {"introduction": "Moving from a channel with zero capacity, we now consider a more practical scenario involving a noisy but not entirely broken communication link. This problem [@problem_id:1613898] connects the channel's capacity directly to the concept of conditional entropy, $H(X|Y)$, which quantifies the uncertainty about the input $X$ that remains even after observing the output $Y$. By working through this exercise, you will learn how to calculate the maximum rate for reliable communication based on a measured property of the channel, reinforcing that capacity is precisely the amount of information that survives the transmission process.", "problem": "An engineering team is analyzing a deep-space communication link. The signal travels through a dense nebula, causing significant noise. The team models this link as a Binary Symmetric Channel (BSC), which has a binary input alphabet $\\mathcal{X}=\\{0, 1\\}$ and a binary output alphabet $\\mathcal{Y}=\\{0, 1\\}$. In a BSC, an input bit is flipped (i.e., a 0 becomes a 1, or a 1 becomes a 0) with a certain crossover probability $p$, and is transmitted correctly with probability $1-p$.\n\nTo characterize the channel's performance, the team sends a test signal where the input bits are chosen with equal probability, i.e., $P(X=0) = P(X=1) = 0.5$. From the received data, they perform a statistical analysis and determine that the conditional entropy of the input given the output is $H(X|Y) = 0.9985$ bits. All entropy calculations use the base-2 logarithm.\n\nAccording to Shannon's channel coding theorem and its converse, reliable communication (i.e., with an arbitrarily small probability of error) is only possible for transmission rates $R$ that do not exceed the channel capacity $C$. What is the maximum theoretical rate for reliable communication, in bits per channel use, for this specific channel? Round your final answer to four significant figures.", "solution": "We model the link as a Binary Symmetric Channel (BSC) with crossover probability $p$, binary input $\\mathcal{X}=\\{0,1\\}$, and binary output $\\mathcal{Y}=\\{0,1\\}$. For a BSC, the channel capacity is given by\n$$\nC=\\max_{P_{X}} I(X;Y).\n$$\nIt is known that the capacity-achieving input distribution for a BSC is uniform, that is, $P(X=0)=P(X=1)=\\frac{1}{2}$. Under this distribution, we have $H(X)=1$ bit. The mutual information can be written as\n$$\nI(X;Y)=H(X)-H(X|Y).\n$$\nThe problem states that, for the uniform input used in the test, $H(X|Y)=0.9985$ bits. Therefore, for this input,\n$$\nI(X;Y)=1-0.9985=0.0015 \\text{ bits}.\n$$\nFor the BSC, the uniform input achieves capacity, hence\n$$\nC=I(X;Y)=1-H(X|Y)=0.0015 \\text{ bits per channel use}.\n$$\nBy Shannonâ€™s channel coding theorem and its converse, the maximum theoretical rate for reliable communication equals the channel capacity. Rounding to four significant figures yields\n$$\nC=1.500 \\times 10^{-3}.\n$$", "answer": "$$\\boxed{1.500 \\times 10^{-3}}$$", "id": "1613898"}, {"introduction": "The channel coding theorem and its converse are powerful, but their conclusions are asymptotic, meaning they formally hold in the limit of infinitely long codes. This final practice problem addresses a common and subtle point of confusion: what happens with finite-length codes? Here, you will analyze a situation where a code's rate $R$ is demonstrably greater than the channel capacity $C$, yet it achieves a small, non-zero probability of error [@problem_id:1613859]. This exercise is crucial for understanding that the converse does not forbid such occurrences for finite block lengths, but rather guarantees that this apparent success cannot be sustained to achieve arbitrarily high reliability.", "problem": "An engineer is designing a communication system to transmit data over a noisy channel, which is accurately modeled as a Binary Symmetric Channel (BSC). In a BSC, each transmitted bit is flipped (from 0 to 1 or 1 to 0) with a fixed crossover probability $p$. For this particular channel, the crossover probability is $p = 0.1$.\n\nThe engineer devises a block code with a block length of $n=8$. The code is designed to transmit one of $M=64$ distinct messages in each block. After implementing and testing this code, the engineer reports an average probability of error, $P_e^{(n)}$, of approximately $0.05$.\n\nA student reviewing the design notes that the code's rate $R = \\frac{\\log_2(M)}{n}$ appears to be greater than the channel capacity $C = 1 - H_2(p)$, where $H_2(p) = -p \\log_2(p) - (1-p) \\log_2(1-p)$ is the binary entropy function. The student is concerned that this result violates the converse to the channel coding theorem.\n\nWhich of the following statements provides the correct explanation for this situation?\n\nA. The converse to the channel coding theorem is an asymptotic result. It states that for any sequence of codes with a rate $R > C$, the probability of error must approach 1 as the block length $n$ approaches infinity. It does not forbid the existence of a specific code with a finite block length $n$ and a small, non-zero error probability.\n\nB. The converse theorem is violated. Any code with a rate greater than the channel capacity must have a probability of error equal to 1, regardless of the block length. The engineer's reported error probability of $0.05$ must be the result of a measurement error.\n\nC. The engineer has miscalculated the capacity. For a BSC with $p=0.1$, the capacity is actually greater than the code's rate, which is why a low probability of error is achievable.\n\nD. This result is only possible because the engineer used a highly optimized non-linear code. Shannon's converse theorem only provides a bound for the average performance of randomly constructed linear codes, not for all possible codes.\n\nE. The direct part of the channel coding theorem is more relevant here. It implies that even for rates $R > C$, it is sometimes possible to achieve a low probability of error, although it is not guaranteed.", "solution": "Compute the code rate. By definition,\n$$\nR=\\frac{\\log_{2}(M)}{n}.\n$$\nWith $M=64$ and $n=8$, use $\\log_{2}(64)=6$ to get\n$$\nR=\\frac{6}{8}=\\frac{3}{4}=0.75.\n$$\n\nCompute the BSC capacity. For a BSC with crossover probability $p$, the capacity is\n$$\nC=1-H_{2}(p),\\quad H_{2}(p)=-p\\log_{2}(p)-(1-p)\\log_{2}(1-p).\n$$\nSubstitute $p=0.1$:\n$$\nH_{2}(0.1)=-0.1\\log_{2}(0.1)-0.9\\log_{2}(0.9).\n$$\nUsing $\\log_{2}(0.1)=\\frac{\\ln(0.1)}{\\ln(2)}=-\\frac{\\ln(10)}{\\ln(2)}$ and $\\log_{2}(0.9)=\\frac{\\ln(0.9)}{\\ln(2)}$, one finds numerically that $H_{2}(0.1)\\approx 0.469$, hence\n$$\nC=1-H_{2}(0.1)\\approx 0.531.\n$$\nTherefore,\n$$\nR=0.75C\\approx 0.531.\n$$\n\nInterpretation relative to the converse. The (strong) converse to the channel coding theorem for discrete memoryless channels, including the BSC, is an asymptotic statement: for any sequence of codes with rate $RC$, the probability of error $P_{e}^{(n)}$ tends to $1$ as $n\\to\\infty$. It does not preclude the existence of a particular finite-length code with nonzero but small error probability. Hence observing $P_{e}^{(n)}\\approx 0.05$ at $n=8$ and $RC$ does not violate the converse; it merely cannot persist as $n$ grows while keeping $RC$.\n\nTherefore, the correct explanation is that the converse is asymptotic and does not forbid isolated finite-$n$ codes of small error probability at rates above capacity, which corresponds to option A. Options B, C, D, and E are incorrect for the following reasons: B falsely applies the converse at finite $n$; C contradicts the computation showing $RC$; D misstates the scope of the converse, which holds for all codes; E misstates the direct theorem, which guarantees reliability only for $RC$.", "answer": "$$\\boxed{A}$$", "id": "1613859"}]}