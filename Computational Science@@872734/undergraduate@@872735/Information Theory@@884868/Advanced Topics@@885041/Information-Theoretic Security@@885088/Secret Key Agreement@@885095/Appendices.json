{"hands_on_practices": [{"introduction": "To begin our hands-on exploration, we first establish the foundational principle of secret key agreement. This exercise provides a clear, discrete model to directly illustrate the central idea that a secret key is possible only when there is an information advantage. By calculating the mutual information for both Bob and Eve, you will see how the maximum secret key rate emerges from the difference between the information Alice successfully shares with Bob and the information that is inevitably leaked to Eve [@problem_id:1657000].", "problem": "In a communication scenario designed for secret key generation, Alice transmits information to Bob, while an eavesdropper, Eve, listens in. The protocol proceeds as follows for each \"channel use\":\n\n1.  Alice generates a pair of independent and uniformly random bits, $X_1$ and $X_2$. This means for each bit, the probability of it being 0 is equal to the probability of it being 1, which is $1/2$. The pair $(X_1, X_2)$ constitutes the total information Alice sends in one channel use.\n\n2.  Bob receives a corresponding pair of bits, $(Y_1, Y_2)$. Bob's channel is partially noisy. The first bit is received perfectly, such that $Y_1 = X_1$. The second bit is subject to noise, modeled by a Binary Symmetric Channel (BSC) with a crossover probability $p$, where $0 < p < 1/2$. This implies that the probability of Bob receiving the wrong second bit is $P(Y_2 \\neq X_2) = p$.\n\n3.  Eve is unable to tap into Bob's channel directly. Instead, her equipment perfectly intercepts the parity of the bits Alice sent. Eve observes the variable $Z = X_1 \\oplus X_2$, where $\\oplus$ denotes the addition modulo 2 (XOR operation). This observation is noiseless.\n\nAlice and Bob can use a public, authenticated channel for post-processing (reconciliation and privacy amplification) to agree on a shared secret key. The maximum achievable rate of this secret key is the difference between the information Bob receives about Alice's transmission and the information Eve receives.\n\nCalculate the maximum achievable secret key rate in bits per channel use. Express your answer as a function of the crossover probability $p$. In your final expression, use the notation $h_2(\\alpha) = -\\alpha \\log_2(\\alpha) - (1-\\alpha) \\log_2(1-\\alpha)$ for the binary entropy function. All logarithms are base 2.", "solution": "Let $X \\triangleq (X_{1},X_{2})$, $Y \\triangleq (Y_{1},Y_{2})$, and $Z \\triangleq X_{1} \\oplus X_{2}$. The problem states that the maximum achievable secret key rate is the difference\n$$\nR = I(X;Y) - I(X;Z).\n$$\nWe compute each term.\n\nFirst, compute $I(X;Y)$. Since $Y_{1}=X_{1}$ (noiseless) and $Y_{2}=X_{2}\\oplus N$ with $N \\sim \\mathrm{Bern}(p)$ independent of $(X_{1},X_{2})$, we have\n$$\nH(Y\\mid X)=H(Y_{1},Y_{2}\\mid X_{1},X_{2})=H(Y_{1}\\mid X_{1},X_{2})+H(Y_{2}\\mid Y_{1}, X_{1},X_{2})=0+H(N)=h_{2}(p).\n$$\nBecause $X_{1}$ and $X_{2}$ are independent and uniform, $Y_{1}$ is uniform, and $Y_{2}$ is also uniform (a BSC output with uniform input). Moreover, $Y_{1}$ depends only on $X_{1}$, and $Y_{2}$ depends only on $(X_{2},N)$, which are independent of $X_{1}$; hence $Y_{1}$ and $Y_{2}$ are independent. Therefore,\n$$\nH(Y)=H(Y_{1})+H(Y_{2})=1+1=2,\n$$\nand thus\n$$\nI(X;Y)=H(Y)-H(Y\\mid X)=2-h_{2}(p).\n$$\n\nNext, compute $I(X;Z)$. Since $Z$ is a deterministic function of $X$, we have $H(Z\\mid X)=0$ and hence\n$$\nI(X;Z)=H(Z).\n$$\nWith $X_{1}$ and $X_{2}$ independent and uniform, the parity $Z=X_{1}\\oplus X_{2}$ is uniform, so $H(Z)=1$. Hence\n$$\nI(X;Z)=1.\n$$\n\nCombining the two,\n$$\nR=I(X;Y)-I(X;Z)=(2-h_{2}(p)) - 1 = 1 - h_{2}(p).\n$$\nThis is nonnegative for $0<p<\\frac{1}{2}$ since $h_{2}(p)\\in(0,1)$, and it matches the intuition that as $p \\to 0$ the rate approaches $1$, and as $p \\to \\frac{1}{2}$ the rate approaches $0$.", "answer": "$$\\boxed{1 - h_{2}(p)}$$", "id": "1657000"}, {"introduction": "Building upon the basic framework, this next practice introduces a more subtle and powerful concept: the use of a public channel. It explores the seemingly paradoxical scenario where a public announcement, visible to everyone including the eavesdropper, can be used to generate a private key. This exercise demonstrates how Alice and Bob can leverage public feedback for \"information reconciliation,\" allowing Bob to correct errors in his received data without revealing the underlying secret bits to Eve [@problem_id:1656995].", "problem": "Alice wishes to establish a secret key with Bob using a noisy communication channel. She generates a sequence of independent and identically distributed binary random variables $X_i$, where each $X_i$ follows a Bernoulli distribution with $P(X_i=1) = q$ and $P(X_i=0) = 1-q$. She transmits this sequence to Bob through a Binary Symmetric Channel (BSC), defined by a crossover probability $p = P(Y_i \\neq X_i | X_i)$, where $0 < p < 1$. Bob receives the sequence $Y_i$.\n\nFor each transmission, a feedback mechanism, whose output is visible to Alice, Bob, and an eavesdropper Eve, announces whether an error occurred. This public feedback is represented by a binary variable $Z_i$, where $Z_i=1$ if $Y_i = X_i$ (no error) and $Z_i=0$ if $Y_i \\neq X_i$ (error).\n\nAssuming Alice, Bob, and Eve can all use this public information $Z_i$ to coordinate their actions, the secret key capacity of this system is defined as the maximum rate (in bits per channel use) at which Alice and Bob can generate a shared key that remains perfectly secret from Eve.\n\nProvide the secret key capacity as a symbolic expression in terms of the source parameter $q$. Your answer should be expressed using the binary entropy function $H_b(x) = -x \\log_2(x) - (1-x) \\log_2(1-x)$.", "solution": "The secret key capacity, in a scenario where side information $Z$ is publicly available to all parties (Alice, Bob, and Eve), is given by the conditional mutual information between Alice's variable $X$ and Bob's variable $Y$, given $Z$. We drop the subscript $i$ as the process is independent and identically distributed. The capacity $K$ is:\n$$K = I(X;Y|Z)$$\nWe can expand this using the chain rule for mutual information in two ways. Let's use the expansion involving conditional entropy of $X$:\n$$I(X;Y|Z) = H(X|Z) - H(X|Y,Z)$$\nWe will compute each term separately.\n\nFirst, let's analyze the term $H(X|Y,Z)$. This represents the remaining uncertainty about Alice's transmitted bit $X$ once Bob's received bit $Y$ and the public error flag $Z$ are both known.\nLet's consider the two possible values of $Z$:\n1.  If $Z=1$, this means by definition that $X=Y$. If we are given the value of $Y$, we know the value of $X$ exactly.\n2.  If $Z=0$, this means by definition that $X \\neq Y$. Since $X$ and $Y$ are binary, this implies that $X = 1-Y$. If we are given the value of $Y$, we again know the value of $X$ exactly.\n\nIn both cases, knowledge of $Y$ and $Z$ completely determines $X$. Therefore, the uncertainty of $X$ given $Y$ and $Z$ is zero.\n$$H(X|Y,Z) = 0$$\n\nNext, we analyze the term $H(X|Z)$. This is the uncertainty about $X$ given the public error flag $Z$. We can write this as $H(X|Z) = H(X) - I(X;Z)$. To find the mutual information $I(X;Z)$, we first must determine if $X$ and $Z$ are statistically independent. They are independent if and only if $P(Z|X) = P(Z)$. Let's compute the conditional probability of an error, $P(Z=0|X=x)$.\nThe probability of an error event ($Z=0$) given that Alice sent $X=x$ is, by definition of the BSC, the crossover probability $p$.\n$$P(Z=0|X=0) = P(Y \\neq 0|X=0) = p$$\n$$P(Z=0|X=1) = P(Y \\neq 1|X=1) = p$$\nSince $P(Z=0|X=x)$ is the same for all values of $x$ (it is always $p$), the conditional probability is equal to the marginal probability $P(Z=0)$. We can verify this:\n$$P(Z=0) = \\sum_{x \\in \\{0,1\\}} P(Z=0|X=x)P(X=x) = p \\cdot (1-q) + p \\cdot q = p(1-q+q) = p$$\nSince $P(Z=0|X=x) = P(Z=0)$, the random variables $X$ and $Z$ are statistically independent.\n\nBecause $X$ and $Z$ are independent, the knowledge of $Z$ provides no information about $X$. Therefore, the conditional entropy $H(X|Z)$ is equal to the unconditional entropy $H(X)$.\n$$H(X|Z) = H(X)$$\nThe source $X$ is a Bernoulli random variable with $P(X=1) = q$. Its entropy is given by the binary entropy function:\n$$H(X) = H_b(q) = -q \\log_2(q) - (1-q) \\log_2(1-q)$$\n\nNow we can substitute our results back into the expression for the capacity $K$:\n$$K = H(X|Z) - H(X|Y,Z) = H(X) - 0 = H_b(q)$$\nThe secret key capacity is equal to the entropy of the source. This is because the public announcement of the error flag $Z$ allows Alice and Bob to perfectly reconcile their bit sequences (Alice knows $X$, and Bob can deduce $X$ from his received $Y$ and the flag $Z$), while for Eve, the error flag $Z$ provides no information about the content of $X$. Thus, the only limit on the rate of the secret key is the amount of randomness available from the source itself, which is its entropy $H_b(q)$. The channel's noise level, $p$, does not affect the final capacity.", "answer": "$$\\boxed{H_b(q)}$$", "id": "1656995"}, {"introduction": "Our final practice challenges you to apply these principles to a more complex and realistic scenario. Here, the communication channels to Bob and Eve are not just different in their noise levels but also in their fundamental behavior for certain symbols. By analyzing this hybrid channel model, you will learn to quantify the information advantage in an asymmetric environment and see how Alice can strategically use different types of symbols to secure her communication [@problem_id:1656931].", "problem": "Alice wishes to establish a secret key with Bob in the presence of an eavesdropper, Eve. To do this, she uses a source that generates symbols from the alphabet $\\mathcal{X} = \\{0, 1, S\\}$ with probabilities $P(X=0)=3/8$, $P(X=1)=3/8$, and $P(X=S)=1/4$.\n\nAlice transmits these symbols to Bob over a primary channel. For inputs $x \\in \\{0, 1\\}$, this channel acts as a Binary Symmetric Channel (BSC) with a crossover probability of $p_B = 0.1$. When the input is $x=S$, the channel output is a distinct erasure symbol 'E', which Bob receives.\n\nSimultaneously, Eve intercepts Alice's transmission via a wiretap channel. For inputs $x \\in \\{0, 1\\}$, the wiretap channel also behaves as a BSC, but with a crossover probability of $p_E = 0.2$. When Alice sends the symbol $S$, Eve receives it perfectly as the symbol 'S' without error.\n\nThe maximum rate at which a secret key can be generated, assuming access to a public discussion channel for reconciliation, is the secret key capacity of the system. Calculate this capacity. For your calculation, all logarithms are base-2, and you may use the binary entropy function, defined as $H_b(p) = -p \\log_2(p) - (1-p) \\log_2(1-p)$ for $p \\in [0, 1]$.\n\nExpress your answer in bits per source symbol, rounded to four significant figures.", "solution": "The secret key capacity $K$ for this wiretap channel model is given by the difference between the mutual information of Alice's and Bob's channel, $I(X;Y)$, and the mutual information of Alice's and Eve's channel, $I(X;Z)$. Let $X$ be the random variable for Alice's source, $Y$ for Bob's received symbol, and $Z$ for Eve's received symbol.\n\n$$K = I(X;Y) - I(X;Z)$$\n\nWe will calculate each mutual information term separately.\n\n**Part 1: Calculation of $I(X;Y)$**\n\nThe mutual information is defined as $I(X;Y) = H(Y) - H(Y|X)$.\n\nFirst, we calculate the conditional entropy $H(Y|X)$:\n$$H(Y|X) = \\sum_{x \\in \\{0,1,S\\}} P(X=x) H(Y|X=x)$$\n- For $X=0$, Bob's channel is a BSC with crossover $p_B=0.1$. The output is either 0 or 1. So, $H(Y|X=0) = H_b(p_B)$.\n- For $X=1$, Bob's channel is also a BSC with crossover $p_B=0.1$. So, $H(Y|X=1) = H_b(p_B)$.\n- For $X=S$, Bob always receives 'E'. The outcome is certain, so the entropy is zero. $H(Y|X=S) = H(1) = 0$.\n\nLet $P_{bin} = P(X=0) + P(X=1) = 3/8 + 3/8 = 6/8 = 3/4$.\n$$H(Y|X) = P(X=0)H_b(p_B) + P(X=1)H_b(p_B) + P(X=S) \\cdot 0$$\n$$H(Y|X) = (P(X=0) + P(X=1))H_b(p_B) = P_{bin} H_b(p_B)$$\n\nNext, we calculate the output entropy $H(Y)$. The output alphabet for Bob is $\\mathcal{Y} = \\{0, 1, E\\}$. We need the probabilities of these symbols.\n- $P(Y=E) = P(Y=E|X=S)P(X=S) = 1 \\cdot P(X=S) = 1/4$.\n- $P(Y=0) = P(Y=0|X=0)P(X=0) + P(Y=0|X=1)P(X=1) = (1-p_B)P(X=0) + p_B P(X=1)$.\nSince $P(X=0)=P(X=1)=3/8$, this simplifies to:\n$P(Y=0) = (1-p_B)(3/8) + p_B(3/8) = 3/8$.\n- $P(Y=1) = P(Y=1|X=0)P(X=0) + P(Y=1|X=1)P(X=1) = p_B P(X=0) + (1-p_B)P(X=1)$.\n$P(Y=1) = p_B(3/8) + (1-p_B)(3/8) = 3/8$.\n\nSo, the output distribution for Bob is $P(Y=0)=3/8$, $P(Y=1)=3/8$, $P(Y=E)=1/4$.\nThe output entropy is:\n$$H(Y) = -\\sum_{y \\in \\mathcal{Y}} P(Y=y) \\log_2(P(Y=y))$$\n$$H(Y) = - \\left( \\frac{3}{8}\\log_2\\left(\\frac{3}{8}\\right) + \\frac{3}{8}\\log_2\\left(\\frac{3}{8}\\right) + \\frac{1}{4}\\log_2\\left(\\frac{1}{4}\\right) \\right)$$\n$$H(Y) = - \\left( 2 \\cdot \\frac{3}{8}\\log_2\\left(\\frac{3}{8}\\right) + \\frac{1}{4}\\log_2\\left(\\frac{1}{4}\\right) \\right) = - \\left( \\frac{3}{4}\\log_2\\left(\\frac{3}{8}\\right) + \\frac{1}{4}(-2) \\right)$$\n$H(Y) = - \\frac{3}{4}(\\log_2(3) - 3) + \\frac{1}{2} = \\frac{9}{4} - \\frac{3}{4}\\log_2(3)$.\n\nNow, we can find $I(X;Y)$:\n$$I(X;Y) = H(Y) - H(Y|X) = H(Y) - P_{bin}H_b(p_B)$$\n\n**Part 2: Calculation of $I(X;Z)$**\n\nThe mutual information for Eve is $I(X;Z) = H(Z) - H(Z|X)$.\n\nFirst, we calculate the conditional entropy $H(Z|X)$:\n$$H(Z|X) = \\sum_{x \\in \\{0,1,S\\}} P(X=x) H(Z|X=x)$$\n- For $X=0$ and $X=1$, Eve's channel is a BSC with crossover $p_E$. Thus, $H(Z|X=0) = H(Z|X=1) = H_b(p_E)$.\n- For $X=S$, Eve receives 'S' with certainty. So, $H(Z|X=S) = H(1) = 0$.\n\n$$H(Z|X) = (P(X=0) + P(X=1))H_b(p_E) = P_{bin} H_b(p_E)$$\n\nNext, we calculate the output entropy $H(Z)$. Eve's output alphabet is $\\mathcal{Z} = \\{0, 1, S\\}$.\n- $P(Z=S) = P(Z=S|X=S)P(X=S) = 1 \\cdot P(X=S) = 1/4$.\n- $P(Z=0) = P(Z=0|X=0)P(X=0) + P(Z=0|X=1)P(X=1) = (1-p_E)P(X=0) + p_E P(X=1)$.\nSince $P(X=0)=P(X=1)=3/8$, this simplifies to $P(Z=0) = (1-p_E)(3/8) + p_E(3/8) = 3/8$.\n- $P(Z=1) = P(Z=1|X=0)P(X=0) + P(Z=1|X=1)P(X=1) = p_E P(X=0) + (1-p_E)P(X=1) = 3/8$.\n\nThe output distribution for Eve, $P(Z)$, is identical to the output distribution for Bob, $P(Y)$. Thus, $H(Z) = H(Y)$.\n\nNow, we can find $I(X;Z)$:\n$$I(X;Z) = H(Z) - H(Z|X) = H(Y) - P_{bin}H_b(p_E)$$\n\n**Part 3: Final Calculation of Secret Key Capacity $K$**\n\n$$K = I(X;Y) - I(X;Z) = (H(Y) - P_{bin}H_b(p_B)) - (H(Y) - P_{bin}H_b(p_E))$$\nThe $H(Y)$ terms cancel out, leaving a simplified expression:\n$$K = P_{bin} H_b(p_E) - P_{bin} H_b(p_B) = P_{bin} (H_b(p_E) - H_b(p_B))$$\n\nNow we substitute the numerical values:\n$P_{bin} = 3/4 = 0.75$\n$p_B = 0.1$\n$p_E = 0.2$\n\nFirst, calculate the binary entropies:\n$H_b(p_B) = H_b(0.1) = -0.1 \\log_2(0.1) - 0.9 \\log_2(0.9) \\approx 0.468996$ bits.\n$H_b(p_E) = H_b(0.2) = -0.2 \\log_2(0.2) - 0.8 \\log_2(0.8) \\approx 0.721928$ bits.\n\nNow, substitute these into the expression for $K$:\n$$K = 0.75 \\times (0.721928 - 0.468996)$$\n$$K = 0.75 \\times 0.252932$$\n$$K \\approx 0.189699$$\n\nRounding to four significant figures, we get $K \\approx 0.1897$.\nThe secret key capacity is approximately 0.1897 bits per source symbol.", "answer": "$$\\boxed{0.1897}$$", "id": "1656931"}]}