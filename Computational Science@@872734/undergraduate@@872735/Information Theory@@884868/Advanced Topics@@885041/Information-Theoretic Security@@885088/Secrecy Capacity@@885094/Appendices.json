{"hands_on_practices": [{"introduction": "We begin with a foundational exercise that illustrates the core principle of secrecy capacity. In this scenario [@problem_id:1656645], we model a classic wiretap channel where the legitimate receiver has a perfect connection, while the eavesdropper receives a noisy version of the signal. By calculating the maximum rate of secret communication, you will directly apply the fundamental formula for secrecy capacity, $C_S = \\max_{p(X)} [I(X;Y) - I(X;Z)]$, and see how a noise advantage for the main channel translates into a secure data rate.", "problem": "A secure communication system is designed to transmit confidential information. The transmitter, Alice, sends a symbol $X$ from a ternary alphabet $\\mathcal{X} = \\{0, 1, 2\\}$ to a legitimate receiver, Bob, and an eavesdropper, Eve.\n\nThe main channel between Alice and Bob is perfect; any symbol transmitted by Alice is received by Bob without error. That is, if Alice sends symbol $X$, Bob receives symbol $Y = X$.\n\nThe eavesdropper's channel between Alice and Eve is a noisy, symmetric channel. If Alice sends symbol $X$, Eve observes a symbol $Z$ from the same alphabet $\\{0, 1, 2\\}$. The probability that Eve receives the correct symbol is $P(Z=X|X) = 1-\\alpha$. If an error occurs, the two possible incorrect symbols are equally likely. The channel parameter is given as $\\alpha = 0.3$.\n\nAssuming information is measured in bits, a single use of the channel corresponds to the transmission of one symbol. Calculate the secrecy capacity of this wiretap channel configuration. Express your answer in bits per channel use, rounded to four significant figures.", "solution": "The secrecy capacity $C_S$ of a wiretap channel is defined as the maximum achievable rate of secret communication. For a discrete memoryless wiretap channel, it is given by the optimization problem:\n$$C_S = \\max_{p(X)} [I(X;Y) - I(X;Z)]$$\nwhere $X$ is the random variable for the transmitted symbol, $Y$ is for Bob's received symbol, and $Z$ is for Eve's received symbol. The maximization is performed over all possible probability distributions $p(X)$ for the input symbol $X$.\n\nFirst, let's analyze the mutual information $I(X;Y)$ for the main channel (Alice to Bob). The mutual information is defined as:\n$$I(X;Y) = H(Y) - H(Y|X)$$\nwhere $H(Y)$ is the entropy of the output at Bob and $H(Y|X)$ is the conditional entropy. Since the channel is perfect, $Y=X$. This means there is no uncertainty about $Y$ given $X$, so $H(Y|X) = 0$. Consequently, $Y$ has the same distribution as $X$, which means $H(Y) = H(X)$. Therefore, the mutual information for the main channel is:\n$$I(X;Y) = H(X)$$\n\nNext, let's analyze the mutual information $I(X;Z)$ for the eavesdropper's channel (Alice to Eve). This is given by:\n$$I(X;Z) = H(Z) - H(Z|X)$$\nLet's first compute the conditional entropy $H(Z|X)$.\n$$H(Z|X) = \\sum_{x \\in \\{0,1,2\\}} p(X=x) H(Z|X=x)$$\nwhere $H(Z|X=x)$ is the entropy of the eavesdropper's output given that Alice transmitted symbol $x$.\nThe problem states that the probability of correct reception by Eve is $P(Z=x|X=x) = 1-\\alpha$, and the two incorrect symbols are equally likely. This means for any $x \\in \\{0,1,2\\}$ and $z \\neq x$, the error probability is $P(Z=z|X=x) = \\frac{\\alpha}{2}$.\nLet's calculate $H(Z|X=x)$ for any specific $x$, for example $x=0$:\n$$H(Z|X=0) = -\\sum_{z \\in \\{0,1,2\\}} P(Z=z|X=0) \\log_2 P(Z=z|X=0)$$\n$$H(Z|X=0) = - \\left[ P(0|0)\\log_2 P(0|0) + P(1|0)\\log_2 P(1|0) + P(2|0)\\log_2 P(2|0) \\right]$$\n$$H(Z|X=0) = - \\left[ (1-\\alpha)\\log_2(1-\\alpha) + \\frac{\\alpha}{2}\\log_2\\left(\\frac{\\alpha}{2}\\right) + \\frac{\\alpha}{2}\\log_2\\left(\\frac{\\alpha}{2}\\right) \\right]$$\n$$H(Z|X=0) = - \\left[ (1-\\alpha)\\log_2(1-\\alpha) + \\alpha\\log_2\\left(\\frac{\\alpha}{2}\\right) \\right]$$\nUsing the logarithm property $\\log(a/b) = \\log(a) - \\log(b)$:\n$$H(Z|X=0) = - \\left[ (1-\\alpha)\\log_2(1-\\alpha) + \\alpha\\log_2(\\alpha) - \\alpha\\log_2(2) \\right]$$\n$$H(Z|X=0) = -((1-\\alpha)\\log_2(1-\\alpha) + \\alpha\\log_2(\\alpha)) + \\alpha$$\nThe term in the parentheses is the definition of the binary entropy function, $H_2(\\alpha)$. So, $H(Z|X=x) = H_2(\\alpha) + \\alpha$. Because the channel is symmetric, this value is the same for all input symbols $x$.\nTherefore, the conditional entropy $H(Z|X)$ is:\n$$H(Z|X) = \\sum_{x} p(X=x) (H_2(\\alpha) + \\alpha) = (H_2(\\alpha) + \\alpha) \\sum_{x} p(X=x) = H_2(\\alpha) + \\alpha$$\nThis quantity is independent of the input distribution $p(X)$.\n\nNow we can write the expression for the secrecy capacity as:\n$$C_S = \\max_{p(X)} [H(X) - I(X;Z)]$$\nA crucial observation is that both terms in the original expression, $I(X;Y) = H(X)$ and $I(X;Z)$, are maximized by the same input distribution. $H(X)$ is maximized when the input distribution is uniform, i.e., $p(X=x) = 1/3$ for $x \\in \\{0, 1, 2\\}$. The eavesdropper's channel is a symmetric channel, and the capacity of a symmetric channel is achieved when the input distribution is uniform.\nTherefore, the maximization of the difference is the difference of the maxima:\n$$C_S = \\left(\\max_{p(X)} I(X;Y)\\right) - \\left(\\max_{p(X)} I(X;Z)\\right) = C_B - C_E$$\nwhere $C_B$ and $C_E$ are the capacities of the main and eavesdropper channels, respectively.\n\nThe capacity of the main channel is $C_B = \\max_{p(X)} H(X) = \\log_2(3)$, achieved with a uniform input.\n\nThe capacity of the symmetric eavesdropper channel is $C_E = \\log_2(|\\mathcal{Z}|) - H(\\text{row of transition matrix})$, where $|\\mathcal{Z}|=3$ is the size of the output alphabet and $H(\\text{row})$ is the entropy of any row of the transition matrix, which we found to be $H_2(\\alpha) + \\alpha$.\n$$C_E = \\log_2(3) - (H_2(\\alpha) + \\alpha)$$\n\nSubstituting these capacities back into the expression for $C_S$:\n$$C_S = C_B - C_E = \\log_2(3) - [\\log_2(3) - (H_2(\\alpha) + \\alpha)]$$\n$$C_S = H_2(\\alpha) + \\alpha$$\n\nNow, we substitute the given value $\\alpha=0.3$:\n$$C_S = H_2(0.3) + 0.3 = -[0.3\\log_2(0.3) + (1-0.3)\\log_2(1-0.3)] + 0.3$$\n$$C_S = -[0.3\\log_2(0.3) + 0.7\\log_2(0.7)] + 0.3$$\nWe use the base change formula $\\log_2(x) = \\frac{\\ln(x)}{\\ln(2)}$.\n$\\ln(2) \\approx 0.693147$\n$\\ln(0.3) \\approx -1.20397$\n$\\ln(0.7) \\approx -0.356675$\n\n$\\log_2(0.3) = \\frac{-1.20397}{0.693147} \\approx -1.73697$\n$\\log_2(0.7) = \\frac{-0.356675}{0.693147} \\approx -0.514573$\n\nNow, we calculate $H_2(0.3)$:\n$$H_2(0.3) \\approx -[0.3 \\times (-1.73697) + 0.7 \\times (-0.514573)]$$\n$$H_2(0.3) \\approx -[-0.521091 - 0.360201] = -[-0.881292] = 0.881292$$\n\nFinally, we calculate the secrecy capacity:\n$$C_S = H_2(0.3) + 0.3 \\approx 0.881292 + 0.3 = 1.181292$$\nThe problem asks to round the final answer to four significant figures.\n$$C_S \\approx 1.181$$\nThe secrecy capacity is 1.181 bits per channel use.", "answer": "$$\\boxed{1.181}$$", "id": "1656645"}, {"introduction": "Secrecy can be achieved not only when the eavesdropper's channel is noisier, but also when it is structurally different. This practice problem [@problem_id:1656667] explores a scenario where the eavesdropper's observation is a deterministic but non-invertible function of the transmitted signal. This exercise demonstrates a key strategy in physical layer security: designing signals that create ambiguity for the eavesdropper, quantified by the conditional entropy $H(X|Z)$, thereby opening a channel for secure communication.", "problem": "Consider a wiretap channel scenario where a transmitter, Alice, sends a message to a legitimate receiver, Bob, over a main channel. An eavesdropper, Eve, intercepts the transmission through a separate eavesdropper's channel.\n\nAlice selects symbols from a ternary input alphabet $\\mathcal{X} = \\{0, 1, 2\\}$. The probability distribution of her symbol choices is denoted by $P(X=0)=p_0$, $P(X=1)=p_1$, and $P(X=2)=p_2$, where $p_0+p_1+p_2=1$.\n\nThe main channel from Alice to Bob is a noiseless channel. This means Bob's received symbol, $Y$, is identical to Alice's transmitted symbol, $X$.\n\nThe eavesdropper's channel is a deterministic, non-invertible channel described by the function $Z = X \\pmod 2$, where $Z$ is the symbol observed by Eve.\n\nThe secrecy capacity, $C_s$, is defined as the maximum value of the difference between the mutual information of the main channel, $I(X;Y)$, and the mutual information of the eavesdropper's channel, $I(X;Z)$, where the maximization is performed over all possible input probability distributions $(p_0, p_1, p_2)$.\n\nCalculate the secrecy capacity $C_s$ for this system. All information-theoretic quantities are to be calculated using logarithms to the base 2. Express your final answer in bits per channel use, rounded to three significant figures.", "solution": "The main channel is noiseless with $Y=X$, so the mutual information between input and output is\n$$\nI(X;Y)=H(X)-H(X|Y)=H(X),\n$$\nsince $H(X|Y)=0$ when $Y=X$.\n\nThe eavesdropperâ€™s channel is deterministic with $Z=f(X)=X \\bmod 2$, hence $H(Z|X)=0$ and\n$I(X;Z)=H(Z)-H(Z|X)=H(Z)$.\nTherefore, the secrecy rate for an input distribution $(p_{0},p_{1},p_{2})$ is\n$I(X;Y)-I(X;Z)=H(X)-H(Z)$.\nBecause $Z$ is a function of $X$, the chain rule gives $H(X)=H(Z)+H(X|Z)$, so\n$I(X;Y)-I(X;Z)=H(X|Z)$.\nWe thus need to maximize $H(X|Z)$ over all $(p_{0},p_{1},p_{2})$ with $p_{0}+p_{1}+p_{2}=1$ and $p_{i}\\geq 0$.\n\nLet $Z=1$ correspond to $X=1$ and $Z=0$ correspond to $X\\in\\{0,2\\}$. Then\n$$\nP(Z=1)=p_{1},\\qquad P(Z=0)=p_{0}+p_{2}\\equiv s.\n$$\nGiven $Z=1$, $X=1$ deterministically, so $H(X|Z=1)=0$. Given $Z=0$, $X\\in\\{0,2\\}$ with conditional probabilities $P(X=0|Z=0)=p_{0}/s$ and $P(X=2|Z=0)=p_{2}/s$. Hence\n$$\nH(X|Z)=P(Z=0)\\,H(X|Z=0)+P(Z=1)\\,H(X|Z=1)=s\\,H_{2}\\!\\left(\\frac{p_{0}}{s}\\right),\n$$\nwhere $H_{2}(q)=-q\\log_{2}q-(1-q)\\log_{2}(1-q)$ is the binary entropy function.\n\nFor fixed $s\\in[0,1]$, the term $H_{2}(q)$ is maximized at $q=\\tfrac{1}{2}$. To see this, differentiate:\n$$\n\\frac{d}{dq}H_{2}(q)=\\log_{2}\\!\\left(\\frac{1-q}{q}\\right),\n$$\nwhich vanishes at $q=\\tfrac{1}{2}$, and\n$$\n\\frac{d^{2}}{dq^{2}}H_{2}(q)=-\\frac{1}{\\ln 2}\\left(\\frac{1}{q}+\\frac{1}{1-q}\\right)<0,\n$$\nso $q=\\tfrac{1}{2}$ is the unique maximizer with $H_{2}(\\tfrac{1}{2})=1$. Therefore, for fixed $s$,\n$$\n\\max_{p_{0},p_{2}\\ge 0,\\,p_{0}+p_{2}=s} H(X|Z)=s.\n$$\nNow maximize over $s\\in[0,1]$:\n$$\n\\max_{s\\in[0,1]} s = 1,\n$$\nachieved at $s=1$, i.e., $p_{1}=0$ and, to attain the inner maximum, $p_{0}=p_{2}=\\tfrac{1}{2}$. For this distribution, $H(X|Z)=1$ bit, and hence the secrecy capacity is\n$$\nC_{s}=\\max_{(p_{0},p_{1},p_{2})}\\bigl[I(X;Y)-I(X;Z)\\bigr]=\\max H(X|Z)=1 \\text{ bit per channel use.}\n$$\nRounded to three significant figures, this is $1.00$.", "answer": "$$\\boxed{1.00}$$", "id": "1656667"}, {"introduction": "Moving from discrete alphabets to continuous signals, this problem [@problem_id:1656703] delves into the practical implications of secrecy capacity for Gaussian channels, a common model for wireless communication. It presents a fascinating and counter-intuitive result in the low signal-to-noise ratio (SNR) regime, comparing ideal Gaussian inputs with practical BPSK signals. This exercise challenges the assumption that what's best for regular communication is always best for secure communication, emphasizing the unique optimization required in secrecy engineering.", "problem": "A secure communication system is designed to operate over a Gaussian wiretap channel. A transmitter sends a signal with a fixed average power $P$ to a legitimate receiver, Bob. An eavesdropper, Eve, also intercepts the signal. The channel to Bob is an Additive White Gaussian Noise (AWGN) channel with noise power $\\sigma_B^2$, and the channel to Eve is an independent AWGN channel with noise power $\\sigma_E^2$.\n\nThe secrecy capacity, $C_s$, for a given input signal distribution is defined as the difference between the mutual information of the main channel (transmitter-to-Bob) and the mutual information of the eavesdropper's channel (transmitter-to-Eve), i.e., $C_s = I_B - I_E$. We assume the main channel is superior to the eavesdropper's channel.\n\nThe system operates in a low-power, wideband regime where the Signal-to-Noise Ratio (SNR), defined as $\\text{SNR} = P/\\sigma^2$, is much less than 1. In this regime, information theory provides the following highly accurate approximations for the mutual information, $I(\\text{SNR})$, in units of bits per transmission:\n-   For an ideal Gaussian input signal: $I_{\\text{Gauss}}(\\text{SNR}) \\approx \\frac{1}{\\ln 2} \\left( \\frac{1}{2} \\text{SNR} - \\frac{1}{4} \\text{SNR}^2 \\right)$\n-   For a Binary Phase-Shift Keying (BPSK) input signal, where the signal takes values from $\\{-\\sqrt{P}, +\\sqrt{P}\\}$ with equal probability: $I_{\\text{BPSK}}(\\text{SNR}) \\approx \\frac{1}{\\ln 2} \\left( \\frac{1}{2} \\text{SNR} - \\frac{1}{8} \\text{SNR}^2 \\right)$\n\nGiven the system parameters $P = 0.015$, $\\sigma_B^2 = 0.50$, and $\\sigma_E^2 = 0.75$, calculate the difference in secrecy capacity between using a Gaussian input and a BPSK input, $\\Delta C_s = C_{s, \\text{Gauss}} - C_{s, \\text{BPSK}}$.\n\nExpress your answer in bits per transmission, rounded to three significant figures.", "solution": "We define the secrecy capacity for a given input as $C_{s}=I_{B}-I_{E}$, where the mutual informations are approximated in the low-SNR regime as\n$$\nI_{\\text{Gauss}}(\\text{SNR}) \\approx \\frac{1}{\\ln 2}\\left(\\frac{1}{2}\\text{SNR}-\\frac{1}{4}\\text{SNR}^{2}\\right),\\quad\nI_{\\text{BPSK}}(\\text{SNR}) \\approx \\frac{1}{\\ln 2}\\left(\\frac{1}{2}\\text{SNR}-\\frac{1}{8}\\text{SNR}^{2}\\right).\n$$\nThe difference in secrecy capacity between Gaussian and BPSK inputs is\n$$\n\\Delta C_{s}=C_{s,\\text{Gauss}}-C_{s,\\text{BPSK}}=\\big(I_{\\text{Gauss}}(\\text{SNR}_{B})-I_{\\text{BPSK}}(\\text{SNR}_{B})\\big)-\\big(I_{\\text{Gauss}}(\\text{SNR}_{E})-I_{\\text{BPSK}}(\\text{SNR}_{E})\\big).\n$$\nFor any SNR,\n$$\nI_{\\text{Gauss}}(\\text{SNR})-I_{\\text{BPSK}}(\\text{SNR})=\\frac{1}{\\ln 2}\\left(-\\frac{1}{4}\\text{SNR}^{2}+\\frac{1}{8}\\text{SNR}^{2}\\right)=-\\frac{1}{8\\ln 2}\\text{SNR}^{2}.\n$$\nTherefore,\n$$\n\\Delta C_{s}=-\\frac{1}{8\\ln 2}\\left(\\text{SNR}_{B}^{2}-\\text{SNR}_{E}^{2}\\right).\n$$\nWith $P=0.015$, $\\sigma_{B}^{2}=0.50$, and $\\sigma_{E}^{2}=0.75$, the SNRs are\n$$\n\\text{SNR}_{B}=\\frac{P}{\\sigma_{B}^{2}}=\\frac{0.015}{0.50}=0.03,\\qquad\n\\text{SNR}_{E}=\\frac{P}{\\sigma_{E}^{2}}=\\frac{0.015}{0.75}=0.02.\n$$\nHence,\n$$\n\\text{SNR}_{B}^{2}-\\text{SNR}_{E}^{2}=0.03^{2}-0.02^{2}=0.0009-0.0004=0.0005.\n$$\nThus,\n$$\n\\Delta C_{s}=-\\frac{1}{8\\ln 2}\\times 0.0005.\n$$\nEvaluating numerically and rounding to three significant figures,\n$$\n\\Delta C_{s}\\approx -9.02\\times 10^{-5}\\ \\text{bits per transmission}.\n$$", "answer": "$$\\boxed{-9.02 \\times 10^{-5}}$$", "id": "1656703"}]}