{"hands_on_practices": [{"introduction": "A cornerstone of the One-time Pad's perfect secrecy is its effect on the ciphertext's statistical properties. When a truly random key is used, the resulting ciphertext is also uniformly random, completely masking the statistical patterns of the original message. This exercise [@problem_id:1644135] provides a concrete way to explore this principle by calculating the probability of obtaining a specific ciphertext, demonstrating that all possible ciphertexts are equally likely.", "problem": "A basic secure communication system uses an encryption scheme known as the One-Time Pad (OTP). In this system, a binary message $M$ of length $n$ bits is encrypted using a binary key $K$, also of length $n$ bits. The resulting ciphertext $C$ is produced by taking the bitwise Exclusive OR (XOR) of the message and the key, which can be written as $C = M \\oplus K$.\n\nAssume that the message $M$ is chosen uniformly at random from the set of all possible $n$-bit strings, and independently, the key $K$ is also chosen uniformly at random from the set of all possible $n$-bit strings.\n\nCalculate the probability that the resulting ciphertext $C$ is the specific $n$-bit string consisting of all zeros. Express your answer as a function of $n$.", "solution": "Let $M \\in \\{0,1\\}^{n}$ and $K \\in \\{0,1\\}^{n}$ be independent and uniformly distributed, and let $C = M \\oplus K$ be the ciphertext. We seek $\\Pr(C = 0^{n})$, where $0^{n}$ denotes the $n$-bit all-zero string.\n\nFor bits $a,b \\in \\{0,1\\}$, the property of XOR is $a \\oplus b = 0$ if and only if $a = b$. Applying this bitwise to $n$-bit strings, we have that $C = 0^{n}$ if and only if $M = K$.\n\nThus,\n$$\n\\Pr(C = 0^{n}) = \\Pr(M = K) = \\sum_{m \\in \\{0,1\\}^{n}} \\Pr(M = m, K = m).\n$$\nBy independence of $M$ and $K$,\n$$\n\\Pr(M = m, K = m) = \\Pr(M = m)\\Pr(K = m).\n$$\nSince both are uniform on $\\{0,1\\}^{n}$, for every $m$,\n$$\n\\Pr(M = m) = \\frac{1}{2^{n}}, \\quad \\Pr(K = m) = \\frac{1}{2^{n}}.\n$$\nTherefore,\n$$\n\\Pr(C = 0^{n}) = \\sum_{m \\in \\{0,1\\}^{n}} \\frac{1}{2^{n}} \\cdot \\frac{1}{2^{n}} = 2^{n} \\cdot \\frac{1}{2^{n}} \\cdot \\frac{1}{2^{n}} = \\frac{1}{2^{n}}.\n$$\nEquivalently, conditioning on any fixed $M$, there is exactly one key $K$ (namely $K = M$) among $2^{n}$ equally likely keys that yields $C = 0^{n}$, giving the same probability $\\frac{1}{2^{n}}$.", "answer": "$$\\boxed{\\frac{1}{2^{n}}}$$", "id": "1644135"}, {"introduction": "The \"perfect secrecy\" of the One-time Pad hinges on several strict conditions, with the randomness of the key being paramount. This next problem [@problem_id:1644119] moves from the ideal to a more realistic scenario where a design flaw could compromise the key generation process. By calculating the information leakage that results from using a key from a restricted set, you will quantify exactly how much security is lost when this crucial requirement is violated.", "problem": "A cryptography startup is designing a system based on the one-time pad principle. In their system, a secret message $M$, represented as an $n$-bit binary string, is encrypted by performing a bitwise XOR operation with an $n$-bit key $K$. The resulting $n$-bit ciphertext is $C = M \\oplus K$.\n\nDue to a flaw in the key generation process, the key $K$ is not chosen uniformly at random from the entire set of $2^n$ possible keys. Instead, it is chosen uniformly at random from a specific, publicly known subset of keys, denoted by $\\mathcal{S}$. The size of this subset is known to be $|\\mathcal{S}| = 2^{n-k}$, where $n$ and $k$ are positive integers with $k < n$.\n\nAn adversary, Eve, intercepts a single ciphertext $C$. Eve knows the encryption algorithm ($C = M \\oplus K$) and the exact composition of the restricted key set $\\mathcal{S}$. The amount of information that Eve can learn about the message $M$ from observing the ciphertext $C$ is quantified by the mutual information, $I(M; C)$. This value may depend on the statistical distribution of the original message $M$.\n\nAssuming the message $M$ and the key $K$ are chosen independently, determine the maximum possible value of the mutual information $I(M; C)$ in bits. This maximum is to be taken over all possible probability distributions for the message $M$. Your answer should be an expression in terms of $n$ and/or $k$.", "solution": "Let $H(\\cdot)$ denote Shannon entropy in bits. The mutual information satisfies\n$$\nI(M;C)=H(C)-H(C|M).\n$$\nSince $C=M\\oplus K$ and, for any fixed $m$, the map $k\\mapsto c=m\\oplus k$ is a bijection on $\\{0,1\\}^{n}$, we have\n$$\nH(C|M=m)=H(K) \\quad \\text{for all } m,\n$$\nand because $M$ and $K$ are independent, this yields\n$$\nH(C|M)=H(K).\n$$\nGiven that $K$ is uniform on $\\mathcal{S}$ with $|\\mathcal{S}|=2^{n-k}$, we obtain\n$$\nH(K)=\\log_{2}|\\mathcal{S}|=\\log_{2}\\left(2^{n-k}\\right)=n-k.\n$$\nTherefore\n$$\nI(M;C)=H(C)-(n-k).\n$$\nSince $C$ takes values in $\\{0,1\\}^{n}$, we have the bound $H(C)\\leq n$, which implies\n$$\nI(M;C)\\leq n-(n-k)=k.\n$$\nTo show this upper bound is achievable, choose $M$ to be uniform over $\\{0,1\\}^{n}$. Then for any $c$,\n$$\nP_{C}(c)=\\sum_{m}P_{M}(m)P_{K}(c\\oplus m)=\\frac{1}{2^{n}}\\sum_{m}P_{K}(c\\oplus m)=\\frac{1}{2^{n}}\\sum_{k}P_{K}(k)=\\frac{1}{2^{n}},\n$$\nso $C$ is uniform and $H(C)=n$. Substituting gives\n$$\nI(M;C)=n-(n-k)=k.\n$$\nHence, the maximum over all message distributions is $k$ bits.", "answer": "$$\\boxed{k}$$", "id": "1644119"}, {"introduction": "Our final exercise examines a different type of security challenge: the aftermath of a partial information leak. In cryptography, this is often related to a \"known-plaintext attack,\" where an adversary gains knowledge of some plaintext-ciphertext pairs. This practice [@problem_id:1644090] models such a hypothetical scenario to help you determine precisely how much uncertainty about the secret key remains, illustrating the OTP's resilience and the localized nature of security compromises.", "problem": "A secure military communication system uses a One-Time Pad (OTP) for encrypting messages. In this system, messages, keys, and ciphertexts are treated as sequences of bits.\n\nThe message $M$ is a sequence of $L$ bits, $M = (M_1, M_2, \\dots, M_L)$. The key $K$, also a sequence of $L$ bits, $K = (K_1, K_2, \\dots, K_L)$, is generated by a true random number generator where each key bit $K_i$ is chosen independently and uniformly at random from $\\{0, 1\\}$. Furthermore, the key $K$ is statistically independent of the message $M$. The ciphertext $C = (C_1, C_2, \\dots, C_L)$ is produced by applying the bitwise exclusive-OR (XOR) operation: $C_i = M_i \\oplus K_i$ for each bit $i$ from $1$ to $L$.\n\nAn intelligence agency intercepts a specific ciphertext $C$. Sometime later, a partial security breach reveals the first $b$ bits of the original plaintext message, where $0 < b < L$. Let this known portion of the message be denoted as $M_{1..b} = (M_1, M_2, \\dots, M_b)$. The agency has no information about the remaining $L-b$ bits of the plaintext.\n\nThe agency wants to quantify its remaining uncertainty about the entire key $K$ used to generate the intercepted ciphertext. Calculate the conditional entropy of the key $K$ given the full ciphertext $C$ and the known portion of the plaintext $M_{1..b}$. Express your answer as a symbolic expression in terms of $L$ and $b$. The unit of entropy in this problem is bits, which implies the use of the base-2 logarithm in all entropy calculations.", "solution": "Let $M=(M_{1},\\dots,M_{L})$, $K=(K_{1},\\dots,K_{L})$, and $C=(C_{1},\\dots,C_{L})$ with $C_{i}=M_{i}\\oplus K_{i}$. Partition all vectors into the first $b$ components and the remaining $L-b$ components:\n$$\nK=\\bigl(K_{1..b},\\,K_{b+1..L}\\bigr),\\quad M=\\bigl(M_{1..b},\\,M_{b+1..L}\\bigr),\\quad C=\\bigl(C_{1..b},\\,C_{b+1..L}\\bigr).\n$$\nWe are to compute $H\\bigl(K\\,\\big|\\,C,M_{1..b}\\bigr)$ in bits.\n\nBy the chain rule for entropy,\n$$\nH\\bigl(K\\,\\big|\\,C,M_{1..b}\\bigr)\n=H\\bigl(K_{1..b},K_{b+1..L}\\,\\big|\\,C_{1..b},C_{b+1..L},M_{1..b}\\bigr)\n$$\n$$\n=H\\bigl(K_{1..b}\\,\\big|\\,C_{1..b},C_{b+1..L},M_{1..b}\\bigr)+H\\bigl(K_{b+1..L}\\,\\big|\\,K_{1..b},C_{1..b},C_{b+1..L},M_{1..b}\\bigr).\n$$\n\nFor the first term, since $C_{i}=M_{i}\\oplus K_{i}$, knowing $C_{i}$ and $M_{i}$ determines $K_{i}$ for each $i\\leq b$, hence\n$$\nH\\bigl(K_{1..b}\\,\\big|\\,C_{1..b},C_{b+1..L},M_{1..b}\\bigr)=0.\n$$\n\nFor the second term, use independence across positions and between $K$ and $M$. The variables $\\bigl(K_{b+1..L},C_{b+1..L}\\bigr)$ are independent of $\\bigl(K_{1..b},C_{1..b},M_{1..b}\\bigr)$ given the one-time pad model with independent bits across positions. Therefore,\n$$\nH\\bigl(K_{b+1..L}\\,\\big|\\,K_{1..b},C_{1..b},C_{b+1..L},M_{1..b}\\bigr)=H\\bigl(K_{b+1..L}\\,\\big|\\,C_{b+1..L}\\bigr).\n$$\n\nWe now evaluate $H\\bigl(K_{b+1..L}\\,\\big|\\,C_{b+1..L}\\bigr)$. Given the statement that the agency has no information about $M_{b+1..L}$, we model each unknown plaintext bit $M_{i}$ for $i>b$ as independent and uniformly distributed on $\\{0,1\\}$, independent of $K$. For any $i>b$ and any $c,k\\in\\{0,1\\}$,\n$$\n\\Pr\\bigl(K_{i}=k,C_{i}=c\\bigr)=\\sum_{m\\in\\{0,1\\}}\\Pr\\bigl(K_{i}=k,M_{i}=m\\bigr)\\,\\mathbf{1}\\{c=m\\oplus k\\}\n=\\Pr(K_{i}=k)\\Pr\\bigl(M_{i}=c\\oplus k\\bigr)=\\frac{1}{2}\\cdot\\frac{1}{2}=\\frac{1}{4}.\n$$\nHence $K_{i}$ and $C_{i}$ are independent and $K_{i}$ remains uniform given $C_{i}$:\n$$\n\\Pr\\bigl(K_{i}=k\\,\\big|\\,C_{i}=c\\bigr)=\\frac{\\Pr(K_{i}=k,C_{i}=c)}{\\Pr(C_{i}=c)}=\\frac{1/4}{1/2}=\\frac{1}{2},\n$$\nso\n$$\nH\\bigl(K_{i}\\,\\big|\\,C_{i}\\bigr)=H(K_{i})=1\\quad\\text{bit}.\n$$\nIndependence across positions implies\n$$\nH\\bigl(K_{b+1..L}\\,\\big|\\,C_{b+1..L}\\bigr)=\\sum_{i=b+1}^{L}H\\bigl(K_{i}\\,\\big|\\,C_{i}\\bigr)=\\sum_{i=b+1}^{L}1=L-b.\n$$\n\nCombining both parts,\n$$\nH\\bigl(K\\,\\big|\\,C,M_{1..b}\\bigr)=0+(L-b)=L-b\\quad\\text{bits}.\n$$", "answer": "$$\\boxed{L-b}$$", "id": "1644090"}]}