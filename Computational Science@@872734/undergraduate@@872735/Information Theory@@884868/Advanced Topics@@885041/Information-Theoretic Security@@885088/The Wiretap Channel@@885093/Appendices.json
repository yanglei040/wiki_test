{"hands_on_practices": [{"introduction": "To begin, we'll analyze the most fundamental wiretap channel model, a concept pioneered by Aaron D. Wyner. This practice explores a scenario where the intended recipient, Bob, has a perfect, error-free channel, while the eavesdropper, Eve, receives the signal through a noisy Binary Symmetric Channel (BSC). Calculating the secrecy capacity here [@problem_id:1664575] provides a clear baseline for understanding how the eavesdropper's channel quality directly limits the rate of secure communication.", "problem": "In the context of physical layer security, we consider a simple wiretap channel model. Alice wants to send a binary message to a legitimate receiver, Bob, over a communication channel. An eavesdropper, Eve, is also listening in on a separate channel.\n\nThe communication process is modeled as follows:\n- Alice's transmitted signal is a binary random variable $X$ from the alphabet $\\{0, 1\\}$.\n- The channel from Alice to Bob is perfect, meaning Bob receives a signal $Y$ that is identical to Alice's transmission, i.e., $Y = X$.\n- The channel from Alice to Eve is a Binary Symmetric Channel (BSC). When Alice transmits a bit, there is a fixed probability $p_E$ that the bit is flipped before it reaches Eve. Eve receives a signal $Z$ from the alphabet $\\{0, 1\\}$.\n\nThe goal is to determine the secrecy capacity, $C_S$, of this system, which represents the maximum rate at which Alice can transmit information to Bob such that Eve learns absolutely nothing about the message.\n\nThe binary entropy function is defined as $H_2(p) = -p \\log_2(p) - (1-p) \\log_2(1-p)$ for $p \\in [0, 1]$.\n\nDetermine the secrecy capacity $C_S$ as a function of the crossover probability $p_E$. Express your answer in terms of $p_E$ and the binary entropy function $H_2(\\cdot)$. The capacity is measured in bits per channel use.", "solution": "The secrecy capacity $C_S$ of a wiretap channel is defined as the maximum achievable secrecy rate over all possible input distributions. The secrecy rate for a given input distribution $p(X)$ is the difference between the mutual information of the main channel (Alice-to-Bob) and the wiretapper's channel (Alice-to-Eve).\n$$C_S = \\max_{p(X)} [I(X;Y) - I(X;Z)]$$\nHere, $X$ is the input from Alice, $Y$ is the output to Bob, and $Z$ is the output to Eve. We will calculate the two mutual information terms separately and then find the maximum of their difference.\n\nFirst, we analyze the main channel (Alice to Bob).\nThe channel is noiseless, so $Y=X$. The mutual information is given by $I(X;Y) = H(Y) - H(Y|X)$.\nSince $Y$ is completely determined by $X$, the conditional entropy $H(Y|X)=0$.\nThis simplifies the mutual information to $I(X;Y) = H(Y)$. Since $Y=X$, we have $H(Y)=H(X)$.\nSo, for the main channel, $I(X;Y) = H(X)$.\nThe capacity of the main channel, $C_B$, is the maximum of this mutual information over all input distributions: $C_B = \\max_{p(X)} H(X)$. For a binary random variable $X$, the entropy $H(X)$ is maximized when the distribution is uniform, i.e., $P(X=0)=P(X=1)=1/2$. In this case, $H(X) = H_2(1/2) = 1$ bit. So, $C_B = 1$ bit per channel use.\n\nSecond, we analyze the wiretapper's channel (Alice to Eve).\nThis channel is a Binary Symmetric Channel (BSC) with crossover probability $p_E$. The mutual information is $I(X;Z) = H(Z) - H(Z|X)$.\nFor a BSC, the conditional entropy $H(Z|X)$ is independent of the input distribution and is equal to the binary entropy of the crossover probability: $H(Z|X) = H_2(p_E)$.\nThus, $I(X;Z) = H(Z) - H_2(p_E)$.\nThe capacity of Eve's channel, $C_E$, is the maximum of this mutual information: $C_E = \\max_{p(X)} [H(Z) - H_2(p_E)]$.\nTo maximize this expression, we need to maximize $H(Z)$, the entropy of the output. The output entropy $H(Z)$ is maximized when the output distribution $p(Z)$ is uniform.\nLet's see if a uniform input $P(X=0)=P(X=1)=1/2$ leads to a uniform output. The probability of Eve receiving a 0 is:\n$P(Z=0) = P(Z=0|X=0)P(X=0) + P(Z=0|X=1)P(X=1)$\n$P(Z=0) = (1-p_E)(\\frac{1}{2}) + (p_E)(\\frac{1}{2}) = \\frac{1-p_E+p_E}{2} = \\frac{1}{2}$.\nSince $P(Z=0) + P(Z=1) = 1$, we must have $P(Z=1)=1/2$ as well. So, a uniform input on a BSC results in a uniform output. This maximizes $H(Z)$ to be $H_2(1/2)=1$.\nTherefore, the capacity of Eve's channel is $C_E = 1 - H_2(p_E)$, and this is achieved with a uniform input distribution.\n\nFinally, we calculate the secrecy capacity $C_S$.\nThis wiretap channel is a \"degraded\" channel, because the main channel is a special case of the wiretapper's channel (a BSC with crossover probability 0), making it less noisy. More formally, the Markov chain $X \\to Y \\to Z$ holds. Since $Y=X$, this means we need to show that a channel $Y \\to Z$ exists such that the overall transition $X \\to Z$ is recovered. Setting the transition $P(Z|Y)$ to be a BSC with probability $p_E$ fulfills this, as $P(Z|X) = \\sum_y P(Z|y)P(y|X) = \\sum_y P(Z|y)\\delta_{y,X} = P(Z|X)$.\n\nFor degraded wiretap channels where the capacity of the main channel $C_B$ and the capacity of the wiretapper's channel $C_E$ are both achieved by the same input distribution, the secrecy capacity is simply the difference of the individual capacities: $C_S = C_B - C_E$.\nIn our case, both $C_B$ and $C_E$ are achieved with a uniform input distribution ($P(X=0)=P(X=1)=1/2$).\nTherefore, we can use this simplified formula.\nSubstituting the values we found:\n$C_S = C_B - C_E = 1 - (1 - H_2(p_E))$\n$C_S = H_2(p_E)$\n\nSo, the secrecy capacity of this wiretap channel is given by the binary entropy function of Eve's crossover probability.", "answer": "$$\\boxed{H_2(p_E)}$$", "id": "1664575"}, {"introduction": "Building on the basic model, this next practice introduces more realistic complexities by considering noise on both the main and wiretap channels. Through a comparative analysis of three distinct scenarios, you will investigate how secrecy rates change when Bob's channel is also imperfect. This exercise [@problem_id:1664565] crucially highlights the profound difference between channels with independent noise sources and those affected by identical noise, revealing a fundamental condition under which information-theoretic security becomes impossible.", "problem": "An engineer is tasked with evaluating the security of three different designs for a binary communication system intended to transmit a secret message. In all scenarios, the input from the transmitter (Alice) is a random bit $X$ following a uniform distribution, such that $P(X=0)=P(X=1)=1/2$. The legitimate receiver is Bob, and a potential eavesdropper is Eve. All noise processes are modeled as sequences of independent and identically distributed Bernoulli random variables, which are also independent of the input $X$. The performance metric is the secrecy rate, defined as $R_s = I(X;Y) - I(X;Z)$, where $Y$ and $Z$ are the bits received by Bob and Eve, respectively, and $I(\\cdot;\\cdot)$ denotes the mutual information. All logarithms are base 2.\n\n*   **Scenario A:** Bob has a perfect, noiseless channel, so he receives $Y=X$. Eve's channel is a Binary Symmetric Channel (BSC) with a crossover probability of $q_E = 1/4$. Her received bit is $Z = X \\oplus N_E$, where $N_E$ is a Bernoulli random variable with $P(N_E=1) = q_E$.\n\n*   **Scenario B:** Bob's channel is a BSC with crossover probability $q_B = 1/8$, such that $Y = X \\oplus N_B$ where $N_B \\sim \\text{Bernoulli}(q_B)$. Eve's channel is also a BSC, but with a crossover probability of $q_E = 1/4$, such that $Z = X \\oplus N_E$ where $N_E \\sim \\text{Bernoulli}(q_E)$. The noise variables $N_B$ and $N_E$ are independent.\n\n*   **Scenario C:** A hardware implementation flaw causes both Bob's and Eve's receivers to be affected by the exact same noise source $N$, which is a Bernoulli random variable with $P(N=1) = 1/8$. Their received bits are $Y = X \\oplus N$ and $Z = X \\oplus N$.\n\nCalculate the secrecy rate in bits per channel use for each of the three scenarios. Provide your answer as a set of three numerical values corresponding to $(R_{s,A}, R_{s,B}, R_{s,C})$. Round each value to three significant figures.", "solution": "The secrecy rate is defined as $R_{s}=I(X;Y)-I(X;Z)$ with logarithms base $2$. For a Binary Symmetric Channel (BSC) with crossover probability $q$ and a uniform binary input $X$, the mutual information is\n$$\nI(X;Y)=H(Y)-H(Y|X).\n$$\nWith $X$ uniform and a BSC, $Y$ is uniform, so $H(Y)=1$, and $H(Y|X)=H_{2}(q)$, where the binary entropy function is\n$$\nH_{2}(q)=-q \\log_{2}(q)-(1-q)\\log_{2}(1-q).\n$$\nHence, for a BSC, $I(X;Y)=1-H_{2}(q)$.\n\nScenario A: Bob has a perfect channel ($Y=X$), so $I(X;Y)=H(X)=1$. Eve has a BSC with $q_{E}=\\frac{1}{4}$, so $I(X;Z)=1-H_{2}\\!\\left(\\frac{1}{4}\\right)$. Therefore,\n$$\nR_{s,A}=1-\\bigl(1-H_{2}\\!\\left(\\tfrac{1}{4}\\right)\\bigr)=H_{2}\\!\\left(\\tfrac{1}{4}\\right).\n$$\nCompute $H_{2}\\!\\left(\\tfrac{1}{4}\\right)$:\n$$\nH_{2}\\!\\left(\\tfrac{1}{4}\\right)=-\\tfrac{1}{4}\\log_{2}\\!\\left(\\tfrac{1}{4}\\right)-\\tfrac{3}{4}\\log_{2}\\!\\left(\\tfrac{3}{4}\\right)\n=\\tfrac{1}{2}-\\tfrac{3}{4}\\log_{2}\\!\\left(\\tfrac{3}{4}\\right).\n$$\nUsing $\\log_{2}\\!\\left(\\tfrac{3}{4}\\right)=\\log_{2}(3)-2\\approx-0.415037$, we get\n$$\nH_{2}\\!\\left(\\tfrac{1}{4}\\right)\\approx 0.5-0.75\\times(-0.415037)\\approx 0.811278,\n$$\nso $R_{s,A}\\approx 0.811278$, which rounds to $0.811$.\n\nScenario B: Bob has a BSC with $q_{B}=\\frac{1}{8}$ and Eve has a BSC with $q_{E}=\\frac{1}{4}$. Thus\n$$\nI(X;Y)=1-H_{2}\\!\\left(\\tfrac{1}{8}\\right),\\quad I(X;Z)=1-H_{2}\\!\\left(\\tfrac{1}{4}\\right),\n$$\nand\n$$\nR_{s,B}=H_{2}\\!\\left(\\tfrac{1}{4}\\right)-H_{2}\\!\\left(\\tfrac{1}{8}\\right).\n$$\nCompute $H_{2}\\!\\left(\\tfrac{1}{8}\\right)$:\n$$\nH_{2}\\!\\left(\\tfrac{1}{8}\\right)=-\\tfrac{1}{8}\\log_{2}\\!\\left(\\tfrac{1}{8}\\right)-\\tfrac{7}{8}\\log_{2}\\!\\left(\\tfrac{7}{8}\\right)\n=\\tfrac{3}{8}-\\tfrac{7}{8}\\log_{2}\\!\\left(\\tfrac{7}{8}\\right).\n$$\nUsing $\\log_{2}\\!\\left(\\tfrac{7}{8}\\right)=\\log_{2}(7)-3\\approx-0.192645$, we get\n$$\nH_{2}\\!\\left(\\tfrac{1}{8}\\right)\\approx 0.375-0.875\\times(-0.192645)\\approx 0.543564.\n$$\nHence,\n$$\nR_{s,B}\\approx 0.811278-0.543564\\approx 0.267714,\n$$\nwhich rounds to $0.268$.\n\nScenario C: Both receivers observe the same corrupted bit with the same noise $N$:\n$$\nY=X\\oplus N,\\quad Z=X\\oplus N,\n$$\nso $Z=Y$ deterministically. Therefore $I(X;Z)=I(X;Y)$, which implies\n$$\nR_{s,C}=I(X;Y)-I(X;Z)=0.\n$$\nNumerically, with $q=\\frac{1}{8}$, $I(X;Y)=1-H_{2}\\!\\left(\\tfrac{1}{8}\\right)\\approx 0.456436$ and $I(X;Z)$ is identical, so the difference is $0$. Rounding each value to three significant figures yields\n$$\n(R_{s,A},R_{s,B},R_{s,C})\\approx (0.811,\\,0.268,\\,0.000).\n$$", "answer": "$$\\boxed{\\begin{pmatrix} 0.811  0.268  0.000 \\end{pmatrix}}$$", "id": "1664565"}, {"introduction": "To solidify our understanding, let's examine an important limiting case: what happens when the eavesdropper's channel is completely ineffective? This practice considers a scenario where Eve's channel is \"stuck,\" always producing the same output regardless of the transmitted signal. By working through this problem [@problem_id:1664534], you will see how the secrecy capacity is maximized when the eavesdropper's mutual information drops to zero, making the main channel's quality the only limiting factor for secure communication.", "problem": "Alice is sending a secret message to Bob using a binary communication scheme, where the input alphabet is $\\mathcal{X} = \\{0, 1\\}$. The communication channel between Alice and Bob is a Binary Symmetric Channel (BSC), characterized by a crossover probability $p_B$, where $0  p_B \\le 1/2$. Simultaneously, an eavesdropper, Eve, is monitoring the transmission. Eve's wiretap channel is defective in a peculiar way: for any bit transmitted by Alice, Eve's receiver deterministically outputs a '0'. Let the input from Alice be the random variable $X$, the output at Bob's receiver be $Y$, and the output at Eve's receiver be $Z$.\n\nCalculate the secrecy capacity, $C_s$, for this system. Express your answer as a closed-form analytic expression in terms of the crossover probability $p_B$. All logarithms in your final expression should be base-2.", "solution": "The secrecy capacity, $C_s$, of a wiretap channel is defined as the maximum of the difference between the mutual information of the main channel (Alice-Bob) and the wiretap channel (Alice-Eve) over all possible input distributions $p(x)$.\n$$C_s = \\max_{p(x)} [I(X;Y) - I(X;Z)]$$\n\nFirst, we analyze the wiretap channel from Alice to Eve. The channel is described as deterministic: for any input $X \\in \\{0, 1\\}$, the output is always $Z=0$. This means the conditional probabilities are $P(Z=0|X=0) = 1$ and $P(Z=0|X=1) = 1$.\n\nWe calculate the mutual information $I(X;Z) = H(Z) - H(Z|X)$.\n\nTo calculate $H(Z|X)$, we use the definition:\n$$H(Z|X) = -\\sum_{x \\in \\mathcal{X}} p(x) \\sum_{z \\in \\mathcal{Z}} P(Z=z|X=x) \\log_2(P(Z=z|X=x))$$\nFor $X=0$, $P(Z=0|X=0)=1$. The entropy contribution is $-1 \\log_2(1) = 0$.\nFor $X=1$, $P(Z=0|X=1)=1$. The entropy contribution is $-1 \\log_2(1) = 0$.\nThus, $H(Z|X=x) = 0$ for all $x$, which means $H(Z|X) = 0$.\n\nNext, we calculate the entropy of the output, $H(Z)$. Since the output $Z$ is always 0, regardless of the input distribution $p(x)$, the probability distribution of $Z$ is $P(Z=0) = 1$ and $P(Z=z \\neq 0) = 0$. The entropy of a deterministic variable is zero.\n$$H(Z) = -P(Z=0) \\log_2(P(Z=0)) = -1 \\log_2(1) = 0$$\nTherefore, the mutual information for the wiretap channel is:\n$$I(X;Z) = H(Z) - H(Z|X) = 0 - 0 = 0$$\nThis makes intuitive sense: since Eve always observes a '0', she learns nothing about Alice's input bit $X$.\n\nNow, we can substitute this result back into the formula for secrecy capacity:\n$$C_s = \\max_{p(x)} [I(X;Y) - 0] = \\max_{p(x)} I(X;Y)$$\nThis expression is simply the capacity of the main channel from Alice to Bob, which is a BSC with crossover probability $p_B$. Let's call this capacity $C_B$.\n$$C_s = C_B = \\max_{p(x)} I(X;Y)$$\nThe mutual information for the main channel is $I(X;Y) = H(Y) - H(Y|X)$.\nFor a BSC, the conditional entropy $H(Y|X)$ is constant regardless of the input distribution and is equal to the binary entropy of the crossover probability:\n$$H(Y|X) = \\sum_{x \\in \\mathcal{X}} p(x) H(Y|X=x) = H_2(p_B) = -p_B \\log_2(p_B) - (1-p_B) \\log_2(1-p_B)$$\nSo, to maximize $I(X;Y) = H(Y) - H_2(p_B)$, we need to maximize $H(Y)$. The entropy of the binary output $Y$ is maximized when its distribution is uniform, i.e., $p(Y=0)=p(Y=1)=1/2$. This is achieved by choosing a uniform input distribution, $p(X=0)=p(X=1)=1/2$.\nWith a uniform input, the output probabilities are:\n$$P(Y=0) = P(Y=0|X=0)P(X=0) + P(Y=0|X=1)P(X=1) = (1-p_B)\\frac{1}{2} + p_B\\frac{1}{2} = \\frac{1}{2}$$\n$$P(Y=1) = P(Y=1|X=0)P(X=0) + P(Y=1|X=1)P(X=1) = p_B\\frac{1}{2} + (1-p_B)\\frac{1}{2} = \\frac{1}{2}$$\nWith a uniform output distribution, the output entropy is maximal:\n$$H(Y)_{\\max} = H_2(1/2) = -\\frac{1}{2}\\log_2\\left(\\frac{1}{2}\\right) - \\frac{1}{2}\\log_2\\left(\\frac{1}{2}\\right) = 1$$\nTherefore, the capacity of the BSC is:\n$$C_B = H(Y)_{\\max} - H_2(p_B) = 1 - H_2(p_B)$$\nSubstituting the full expression for the binary entropy function:\n$$C_s = C_B = 1 - [-p_B \\log_2(p_B) - (1-p_B) \\log_2(1-p_B)]$$\n$$C_s = 1 + p_B \\log_2(p_B) + (1-p_B) \\log_2(1-p_B)$$\nThis is the final expression for the secrecy capacity.", "answer": "$$\\boxed{1 + p_B \\log_{2}(p_B) + (1-p_B) \\log_{2}(1-p_B)}$$", "id": "1664534"}]}