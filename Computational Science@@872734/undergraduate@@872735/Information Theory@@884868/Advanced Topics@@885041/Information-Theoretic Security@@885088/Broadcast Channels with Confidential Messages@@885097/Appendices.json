{"hands_on_practices": [{"introduction": "To build a foundation for understanding secure communications, we must first learn how to quantify the flow of information. This exercise provides direct, hands-on practice in calculating the \"information advantage\" — the difference in mutual information between a legitimate user and an eavesdropper. By working through this concrete example [@problem_id:1606121], you will solidify your skills in applying the core formulas of information theory to assess the basic security posture of a communication system.", "problem": "A technology firm uses a digital communication system to broadcast one of three possible strategic directives to its team. The set of directives is $\\mathcal{X} = \\{\\text{PROJECT ALPHA}, \\text{PROJECT BETA}, \\text{PROJECT GAMMA}\\}$. These are encoded as symbols $x \\in \\{0, 1, 2\\}$, respectively. Based on historical data, each directive is chosen with equal probability. The transmission is sent over a broadcast channel, reaching both an intended employee (receiver 1) and a corporate competitor who is eavesdropping (receiver 2).\n\nLet $X$ be the random variable for the chosen directive, with outputs $Y_1$ for the employee and $Y_2$ for the competitor. Both output alphabets are $\\mathcal{Y}_1 = \\mathcal{Y}_2 = \\{0, 1, 2\\}$. The communication channels are characterized by the following transition probability matrices, where $W_{ij} = P(Y=j | X=i)$:\n\nThe channel to the employee (receiver 1) is described by matrix $W_1$:\n$$\nW_1 = \\begin{pmatrix} 0.8  0.1  0.1 \\\\ 0.1  0.8  0.1 \\\\ 0.1  0.1  0.8 \\end{pmatrix}\n$$\n\nThe channel to the competitor (receiver 2) is described by matrix $W_2$:\n$$\nW_2 = \\begin{pmatrix} 0.50  0.25  0.25 \\\\ 0.25  0.50  0.25 \\\\ 0.40  0.30  0.30 \\end{pmatrix}\n$$\n\nCalculate the firm's \"information advantage,\" which is defined as the difference between the mutual information of the employee's channel and the competitor's channel, $I(X;Y_1) - I(X;Y_2)$. All information-theoretic quantities are to be calculated using the base-2 logarithm.\n\nExpress your answer in bits, rounded to four significant figures.", "solution": "Let $P(X=i)=\\frac{1}{3}$ for $i\\in\\{0,1,2\\}$. For any discrete memoryless channel, $I(X;Y)=H(Y)-H(Y|X)$ with base-2 logarithms.\n\nEmployee’s channel $W_{1}$:\nThe output distribution is\n$$\nP_{Y_{1}}(j)=\\sum_{i=0}^{2}P(X=i)W_{1,ij}=\\frac{1}{3}\\sum_{i=0}^{2}W_{1,ij}=\\frac{1}{3},\\quad j\\in\\{0,1,2\\},\n$$\nso $H(Y_{1})=\\log_{2}(3)$. The conditional entropy per row is\n$$\nH\\big((0.8,0.1,0.1)\\big)=-\\big(0.8\\log_{2}0.8+0.1\\log_{2}0.1+0.1\\log_{2}0.1\\big).\n$$\nUsing $\\log_{2}(0.8)=-0.321928094887362$ and $\\log_{2}(0.1)=-3.321928094887362$,\n$$\nH(Y_{1}|X)=0.921928094887362,\n$$\nand hence\n$$\nI(X;Y_{1})=\\log_{2}(3)-0.921928094887362=0.663034405833794\\ \\text{bits}.\n$$\n\nCompetitor’s channel $W_{2}$:\nThe output distribution is\n$$\nP_{Y_{2}}=\\left(\\frac{1.15}{3},\\frac{1.05}{3},\\frac{0.80}{3}\\right)=\\left(\\frac{23}{60},\\frac{7}{20},\\frac{4}{15}\\right).\n$$\nThus\n$$\nH(Y_{2})=-\\sum_{j}P_{Y_{2}}(j)\\log_{2}P_{Y_{2}}(j)\n= -\\left(\\frac{23}{60}\\log_{2}\\frac{23}{60}+\\frac{7}{20}\\log_{2}\\frac{7}{20}+\\frac{4}{15}\\log_{2}\\frac{4}{15}\\right)\n$$\n$$\n=0.530275978494744+0.530100610490415+0.508504158828938=1.568880747814097\\ \\text{bits}.\n$$\nThe conditional entropy is the average of the row entropies:\n$$\nH(Y_{2}|X)=\\frac{1}{3}\\left[H(0.5,0.25,0.25)+H(0.5,0.25,0.25)+H(0.4,0.3,0.3)\\right]\n=\\frac{2}{3}\\cdot 1.5+\\frac{1}{3}\\cdot H(0.4,0.3,0.3).\n$$\nWith $\\log_{2}(0.4)=-1.321928094887362$ and $\\log_{2}(0.3)=-1.736965594166206$,\n$$\nH(0.4,0.3,0.3)=-\\left(0.4\\log_{2}0.4+0.3\\log_{2}0.3+0.3\\log_{2}0.3\\right)=1.570950594454668,\n$$\nso\n$$\nH(Y_{2}|X)=\\frac{2}{3}\\cdot 1.5+\\frac{1}{3}\\cdot 1.570950594454668=1.523650198151556,\n$$\nand\n$$\nI(X;Y_{2})=1.568880747814097-1.523650198151556=0.045230549662541\\ \\text{bits}.\n$$\n\nTherefore, the information advantage is\n$$\nI(X;Y_{1})-I(X;Y_{2})=0.663034405833794-0.045230549662541=0.617803856171253\\ \\text{bits},\n$$\nwhich rounded to four significant figures is $0.6178$ bits.", "answer": "$$\\boxed{0.6178}$$", "id": "1606121"}, {"introduction": "Moving from a single calculation to a general principle, this next practice explores the fundamental prerequisite for secure communication. Is it always possible to send a secret message, or are there specific conditions that must be met? By analyzing a broadcast channel composed of two Z-channels [@problem_id:1606176], you will discover the intuitive yet powerful condition that the legitimate receiver's channel must be superior to the eavesdropper's, providing a clear rule for when a positive secrecy capacity, $C_s  0$, can be achieved.", "problem": "An intelligence agency is transmitting a secret message, encoded as a binary random variable $X \\in \\{0, 1\\}$, to a friendly operative (Bob). Unfortunately, an adversary (Eve) is also intercepting the transmission. This scenario is modeled as a binary broadcast channel.\n\nThe communication link to Bob is modeled as a binary Z-channel. In this channel, if the input bit is '1', it is always received correctly as '1'. However, if the input bit is '0', it is correctly received as '0' with probability $1-p_1$ and is incorrectly flipped to a '1' with probability $p_1$.\n\nThe communication link to Eve is modeled as a second, independent binary Z-channel. Similarly, an input bit '1' is always received by Eve as '1'. An input bit '0' is received as '0' with probability $1-p_2$ and is flipped to '1' with probability $p_2$.\n\nThe parameters $p_1$ and $p_2$ are channel error probabilities, and they lie in the range $0 \\le p_1  1$ and $0 \\le p_2  1$.\n\nThe secrecy capacity, $C_s$, of this broadcast channel determines if secure communication is possible. It is defined as the maximum possible rate of secret information transmission and is given by the formula $C_s = \\max_{P(X)} [I(X; Y_1) - I(X; Y_2)]$, where $Y_1$ is the output received by Bob, $Y_2$ is the output received by Eve, $P(X)$ is the probability distribution of the input message $X$, and $I(\\cdot;\\cdot)$ denotes the mutual information. A positive secrecy capacity ($C_s  0$) is the necessary and sufficient condition for secure communication to be possible at a non-zero rate.\n\nWhich of the following conditions on the error probabilities $p_1$ and $p_2$ guarantees that secure communication is possible (i.e., $C_s  0$)?\n\nA. $p_1  p_2$\n\nB. $p_1  p_2$\n\nC. $p_1 = p_2$\n\nD. $p_1 + p_2  1$\n\nE. $p_1 + p_2  1$\n\nF. $|p_1 - p_2|  0.5$", "solution": "The secrecy capacity is defined as $C_{s}=\\max_{P(X)}\\left[I(X;Y_{1})-I(X;Y_{2})\\right]$, where $Y_{1}$ is Bob’s Z-channel output with parameter $p_{1}$ and $Y_{2}$ is Eve’s Z-channel output with parameter $p_{2}$. Denote $q=P(X=0)$ and $1-q=P(X=1)$. For a binary Z-channel with parameter $p\\in[0,1)$, the transition law is: if $X=1$ then $Y=1$ deterministically; if $X=0$ then $Y=0$ with probability $1-p$ and $Y=1$ with probability $p$.\n\nFor fixed $p$ and $q$, the output distribution is $P(Y=1)=1-q(1-p)$ and $P(Y=0)=q(1-p)$. The conditional entropy is $H(Y|X)=q\\,H_{b}(p)$ because $H(Y|X=0)=H_{b}(p)$ and $H(Y|X=1)=0$. The output entropy is $H(Y)=H_{b}\\!\\left(q(1-p)\\right)$, where the binary entropy function is $H_{b}(t)=-t\\log_2 t-(1-t)\\log_2(1-t)$ for $t\\in[0,1]$. Hence the mutual information for a Z-channel with parameter $p$ under input distribution $q$ is\n$$\nI_{q}(p)\\equiv I(X;Y)=H_{b}\\!\\left(q(1-p)\\right)-q\\,H_{b}(p).\n$$\nTherefore,\n$$\nI(X;Y_{1})-I(X;Y_{2})=I_{q}(p_{1})-I_{q}(p_{2})=H_{b}\\!\\left(q(1-p_{1})\\right)-H_{b}\\!\\left(q(1-p_{2})\\right)-q\\left[H_{b}(p_{1})-H_{b}(p_{2})\\right].\n$$\n\nTo determine when $C_{s}0$, we analyze the monotonicity of $I_{q}(p)$ with respect to $p$. Differentiating $I_{q}(p)$ with respect to $p$ yields\n$$\n\\frac{\\partial I_{q}}{\\partial p}(p)=\\frac{\\partial}{\\partial p}\\left[H_{b}\\!\\left(q(1-p)\\right)-q\\,H_{b}(p)\\right]\n=H_{b}'\\!\\left(q(1-p)\\right)\\cdot(-q)-q\\,H_{b}'(p),\n$$\nand since $H_{b}'(t)=\\log_2\\!\\left(\\frac{1-t}{t}\\right)$, we get\n$$\n\\frac{\\partial I_{q}}{\\partial p}(p)=-q\\log_2\\!\\left(\\frac{1-q(1-p)}{q(1-p)}\\right)-q\\log_2\\!\\left(\\frac{1-p}{p}\\right)\n=-q\\log_2\\!\\left(\\frac{1-q(1-p)}{q(1-p)}\\cdot\\frac{1-p}{p}\\right)\n=-q\\log_2\\!\\left(\\frac{1-q(1-p)}{q\\,p}\\right).\n$$\nFor $q\\in(0,1)$ and $p\\in(0,1)$, observe that $1-q(1-p)=(1-q)+q pq p$, so the ratio $\\frac{1-q(1-p)}{q\\,p}1$ and thus $\\log_2\\!\\left(\\frac{1-q(1-p)}{q\\,p}\\right)0$. Hence\n$$\n\\frac{\\partial I_{q}}{\\partial p}(p)0 \\quad \\text{for all } q\\in(0,1),\\;p\\in(0,1).\n$$\nTherefore, for any fixed $q\\in(0,1)$, $I_{q}(p)$ is strictly decreasing in $p$. It follows that:\n- If $p_{1}p_{2}$, then for any $q\\in(0,1)$, $I_{q}(p_{1})I_{q}(p_{2})$, so there exists such a $q$ with $I_{q}(p_{1})-I_{q}(p_{2})0$, and consequently $C_{s}=\\max_{q\\in[0,1]}[I_{q}(p_{1})-I_{q}(p_{2})]0$.\n- If $p_{1}=p_{2}$, then $I_{q}(p_{1})-I_{q}(p_{2})=0$ for all $q$, so $C_{s}=0$.\n- If $p_{1}p_{2}$, then for any $q\\in(0,1)$, $I_{q}(p_{1})I_{q}(p_{2})$, hence $I_{q}(p_{1})-I_{q}(p_{2})0$, and the maximum over $q$ is attained at a trivial input (e.g., $q=0$ or $q=1$) with value $0$, so $C_{s}\\le 0$ and cannot be positive.\n\nThus the necessary and sufficient condition for $C_{s}0$ is $p_{1}p_{2}$. Among the listed options, this corresponds exactly to option B. All other listed conditions are neither necessary nor sufficient for $C_{s}0$ in general.", "answer": "$$\\boxed{B}$$", "id": "1606176"}, {"introduction": "Finally, we explore how security can be proactively designed through clever coding, even in a seemingly transparent, noiseless environment. This problem demonstrates that an information advantage can be created not just by chance or better channel physics, but by strategic signal design. By intelligently selecting which input symbols to use from a given set [@problem_id:1606180], you can obscure information from an eavesdropper while ensuring perfect clarity for the intended recipient, illustrating a powerful method for achieving confidentiality.", "problem": "A transmitter sends a symbol $X$ chosen from an input alphabet $\\mathcal{X} = \\{0, 1, 2, 3\\}$ over a deterministic, discrete, memoryless broadcast channel. The channel has two outputs, $Y_1$ and $Y_2$. The output $Y_1$ is received by a legitimate user, Bob, while the output $Y_2$ is intercepted by an eavesdropper, Eve.\n\nThe channel is defined by the following deterministic functions:\n- Bob's received symbol is $Y_1 = X \\pmod 2$.\n- Eve's received symbol is $Y_2 = X \\pmod 3$.\n\nThe goal is to send a message to Bob with the highest possible rate, while keeping it perfectly secret from Eve. This maximum achievable secret rate is known as the secrecy capacity, $C_s$.\n\nDetermine the secrecy capacity of this channel. Provide your final answer as an exact analytic expression in units of bits per channel use.", "solution": "We model the system as a discrete memoryless wiretap channel with input $X \\in \\{0,1,2,3\\}$ and deterministic outputs $Y_{1} = X \\bmod 2$ (to Bob) and $Y_{2} = X \\bmod 3$ (to Eve). The secrecy capacity for a general wiretap channel is\n$$\nC_{s} = \\max_{P_{U,X}} \\left[I(U;Y_{1}) - I(U;Y_{2})\\right], \\quad \\text{with } U \\to X \\to (Y_{1},Y_{2}).\n$$\nSince Bob’s channel is deterministic, we can upper bound any such rate by\n$$\nI(U;Y_{1}) - I(U;Y_{2}) = I(U;Y_{1}\\mid Y_{2}) - I(U;Y_{2}\\mid Y_{1}) \\leq I(U;Y_{1}\\mid Y_{2}) \\leq I(X;Y_{1}\\mid Y_{2}),\n$$\nwhere the last inequality follows from data processing given $U \\to X \\to (Y_{1},Y_{2})$. Because $Y_{1}$ is a deterministic function of $X$, we have\n$$\nI(X;Y_{1}\\mid Y_{2}) = H(Y_{1}\\mid Y_{2}).\n$$\nThus\n$$\nC_{s} \\leq \\max_{P_{X}} H(Y_{1}\\mid Y_{2}).\n$$\nThis upper bound is achievable by choosing $U = Y_{1}$ (which satisfies $U \\to X \\to (Y_{1},Y_{2})$ since $Y_{1}$ is a function of $X$), yielding\n$$\nI(U;Y_{1}) - I(U;Y_{2}) = H(Y_{1}) - I(Y_{1};Y_{2}) = H(Y_{1}\\mid Y_{2}).\n$$\nTherefore,\n$$\nC_{s} = \\max_{P_{X}} H(Y_{1}\\mid Y_{2}).\n$$\n\nWe now compute and maximize $H(Y_{1}\\mid Y_{2})$ over $P_{X}$. The deterministic mapping is\n$$\nX=0 \\mapsto (Y_{1},Y_{2})=(0,0),\\quad X=1 \\mapsto (1,1),\\quad X=2 \\mapsto (0,2),\\quad X=3 \\mapsto (1,0).\n$$\nThese four pairs are distinct, so $(Y_{1},Y_{2})$ is in one-to-one correspondence with $X$, implying\n$$\nH(Y_{1},Y_{2}) = H(X) \\quad \\text{and} \\quad H(Y_{1}\\mid Y_{2}) = H(Y_{1},Y_{2}) - H(Y_{2}) = H(X) - H(Y_{2}).\n$$\nLet $p_{i} = P(X=i)$ for $i \\in \\{0,1,2,3\\}$, and define $q_{0} = P(Y_{2}=0) = p_{0}+p_{3}$, $q_{1} = P(Y_{2}=1) = p_{1}$, $q_{2} = P(Y_{2}=2) = p_{2}$. Since $Y_{2}$ deterministically partitions the inputs as $\\{0,3\\}$, $\\{1\\}$, and $\\{2\\}$, we have\n$$\nH(Y_{1}\\mid Y_{2}) = H(X\\mid Y_{2}) = q_{0}\\, H_{b}\\!\\left(\\frac{p_{0}}{p_{0}+p_{3}}\\right) + q_{1} \\cdot 0 + q_{2} \\cdot 0 = (p_{0}+p_{3})\\, H_{b}\\!\\left(\\frac{p_{0}}{p_{0}+p_{3}}\\right),\n$$\nwhere $H_b(t) = -t \\log_{2} t - (1-t)\\log_{2}(1-t)$ is the binary entropy function. Let $s = p_{0}+p_{3} \\in [0,1]$ and $t = \\frac{p_{0}}{p_{0}+p_{3}} \\in [0,1]$ (for $s0$). Then\n$$\nH(Y_{1}\\mid Y_{2}) = s\\, H_b(t).\n$$\nFor fixed $s$, $H_b(t)$ is maximized at $t=\\frac{1}{2}$ with value $1$, so the expression becomes $s$. Maximizing over $s \\in [0,1]$ gives the maximum $s=1$, achieved by choosing $p_{1}=p_{2}=0$ and $p_{0}=p_{3}=\\frac{1}{2}$. Therefore,\n$$\n\\max_{P_{X}} H(Y_{1}\\mid Y_{2}) = 1 \\text{ bit}.\n$$\nThis is also intuitive since $Y_{1} \\in \\{0,1\\}$, so $H(Y_{1}) \\leq 1$, and equality is achieved by the choice $P(X=0)=P(X=3)=\\frac{1}{2}$, for which $Y_{1}$ is a fair bit while $Y_{2}$ is constant ($Y_{2}=0$), yielding perfect secrecy.\n\nHence the secrecy capacity is $1$ bit per channel use.", "answer": "$$\\boxed{1}$$", "id": "1606180"}]}