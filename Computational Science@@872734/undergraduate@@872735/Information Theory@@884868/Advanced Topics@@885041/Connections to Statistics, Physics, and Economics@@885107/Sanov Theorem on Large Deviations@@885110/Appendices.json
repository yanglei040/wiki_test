{"hands_on_practices": [{"introduction": "To build our intuition, we begin with a scenario where the large deviation rate can be understood through fundamental probability principles. This practice explores the rare event of a specific symbol from an alphabet failing to appear in a very long sequence. By first approaching this with a direct calculation, we can appreciate how the more general framework of Sanov's theorem [@problem_id:1655867] arrives at the same conclusion, connecting abstract theory to a concrete and tangible result.", "problem": "Consider a source that generates a sequence of symbols $X_1, X_2, \\ldots, X_n$ which are independent and identically distributed (i.i.d.). The symbols are drawn from a finite alphabet $\\mathcal{X} = \\{x_1, x_2, \\ldots, x_K\\}$ where $K \\ge 2$. The probability of generating symbol $x_k$ is given by $p_k = \\text{Pr}(X_i = x_k)$ for any $i$. We are given that $p_k > 0$ for all $k \\in \\{1, 2, \\ldots, K\\}$.\n\nFor a very long sequence of length $n$, the empirical frequency of the symbol $x_k$ is the fraction of times it appears in the sequence. The probability that the empirical frequency of the specific symbol $x_K$ is exactly zero is a rare event. For large $n$, this probability can be well-approximated by the form:\n$$ \\text{Pr}(\\text{empirical frequency of } x_K \\text{ is } 0) \\approx \\exp(-nC) $$\nwhere $C$ is a positive constant that depends on the source probabilities.\n\nDetermine the constant $C$ as an analytic expression in terms of the probability $p_K$.", "solution": "Let $X_{1},\\ldots,X_{n}$ be i.i.d. over $\\mathcal{X}$ with $\\text{Pr}(X_{i}=x_{K})=p_{K}$ and $0<p_{K}<1$. The event that the empirical frequency of $x_{K}$ is zero is exactly the event that $x_{K}$ never appears in the $n$ independent draws. By independence,\n$$\n\\text{Pr}(\\text{no }x_{K}\\text{ in }n\\text{ draws})=(1-p_{K})^{n}.\n$$\nRewrite this probability in exponential form using the logarithm:\n$$\n(1-p_{K})^{n}=\\exp\\!\\big(n\\ln(1-p_{K})\\big)=\\exp\\!\\big(-n\\big[-\\ln(1-p_{K})\\big]\\big).\n$$\nComparing with the asymptotic form $\\exp(-nC)$, we identify\n$$\nC=-\\ln(1-p_{K}).\n$$\nSince $0<p_{K}<1$, we have $1-p_{K}\\in(0,1)$, hence $-\\ln(1-p_{K})>0$, so $C$ is positive as required.\n\nFor completeness, the same constant arises from large deviations (Sanovâ€™s theorem): minimizing the relative entropy $D(q\\|p)$ over distributions $q$ with $q_{K}=0$ gives the optimizer $q_{i}=p_{i}/(1-p_{K})$ for $i<K$, and\n$$\nD(q\\|p)=\\sum_{i<K}q_{i}\\ln\\!\\left(\\frac{q_{i}}{p_{i}}\\right)=\\sum_{i<K}q_{i}\\ln\\!\\left(\\frac{1}{1-p_{K}}\\right)=\\ln\\!\\left(\\frac{1}{1-p_{K}}\\right)=-\\ln(1-p_{K}),\n$$\nmatching the constant above.", "answer": "$$\\boxed{-\\ln(1-p_{K})}$$", "id": "1655867"}, {"introduction": "This next practice moves to a core application of Sanov's theorem: determining the likelihood of an empirical distribution falling within a continuous *set* of possible outcomes. We will investigate a hypothetical biological scenario where a deviation from an expected uniform distribution of traits is observed. This exercise [@problem_id:1655881] will guide you through the essential technique of identifying the specific distribution within the rare-event set that is \"closest\" to the true distribution, as measured by the Kullback-Leibler divergence.", "problem": "A simplified model for the coloration of a species of tropical fish assumes three pigment types: Red (R), Green (G), and Blue (B). In a large, randomly mating population, these three color types are expected to appear with equal frequency. A biologist studies a specific, isolated lagoon and suspects environmental factors might be altering the natural distribution. The biologist collects data from a very large sample of $N$ fish from this lagoon.\n\nAn anomaly is declared if the empirical frequency of Red fish in the sample is at least 50%. According to the theory of large deviations, the probability of observing such an anomalous sample is approximated by $P(\\text{anomaly}) \\approx \\exp(-N \\cdot I)$ for large $N$, where $I$ is a positive constant known as the rate function.\n\nCalculate the value of this large deviation rate $I$. Express your final answer as a numerical value, rounded to four significant figures.", "solution": "The problem asks for the large deviation rate $I$ for the event that the empirical frequency of Red fish, $q_R$, is at least $0.5$.\n\nAccording to Sanov's theorem, for a large number of independent and identically distributed samples $N$ drawn from a true distribution $P$, the probability that the empirical distribution $Q$ falls into a set $\\mathcal{E}$ of probability distributions is given by\n$$P(Q \\in \\mathcal{E}) \\approx \\exp(-N \\inf_{Q \\in \\mathcal{E}} D_{KL}(Q || P))$$\nThe rate $I$ is therefore given by the infimum of the Kullback-Leibler (KL) divergence between any distribution $Q$ in the set $\\mathcal{E}$ and the true distribution $P$.\n$$I = \\inf_{Q \\in \\mathcal{E}} D_{KL}(Q || P)$$\n\nFirst, we define the distributions and the set $\\mathcal{E}$.\nThe true distribution $P$ is uniform over the three colors {R, G, B}. Let the probabilities for R, G, and B be $p_R, p_G, p_B$.\n$$P = (p_R, p_G, p_B) = \\left(\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3}\\right)$$\nAn empirical distribution $Q$ is defined by the observed frequencies $Q = (q_R, q_G, q_B)$, where $\\sum q_i = 1$ and $q_i \\ge 0$.\nThe set $\\mathcal{E}$ corresponds to the anomalous event where the frequency of Red fish is at least 50%.\n$$\\mathcal{E} = \\{ Q = (q_R, q_G, q_B) \\mid q_R + q_G + q_B = 1, q_i \\ge 0, \\text{ and } q_R \\ge 0.5 \\}$$\nThe KL-divergence is given by the formula:\n$$D_{KL}(Q || P) = \\sum_{i \\in \\{R,G,B\\}} q_i \\ln\\left(\\frac{q_i}{p_i}\\right)$$\nWe need to find the minimum of this function for $Q \\in \\mathcal{E}$. Since the KL-divergence is a continuous and convex function, and the set $\\mathcal{E}$ is closed and convex, the minimum will be achieved on the boundary of the set $\\mathcal{E}$ that is \"closest\" to $P$. This boundary corresponds to $q_R = 0.5$.\n\nSo, our problem reduces to a constrained optimization problem:\nMinimize $D_{KL}(Q || P)$ subject to the constraints:\n1. $q_R = 0.5$\n2. $q_R + q_G + q_B = 1$\n3. $q_G \\ge 0, q_B \\ge 0$\n\nFrom constraints 1 and 2, we get $0.5 + q_G + q_B = 1$, which simplifies to $q_G + q_B = 0.5$.\n\nNow we substitute these into the KL-divergence formula:\n$$I = \\min_{q_G+q_B=0.5} \\left( q_R \\ln\\left(\\frac{q_R}{p_R}\\right) + q_G \\ln\\left(\\frac{q_G}{p_G}\\right) + q_B \\ln\\left(\\frac{q_B}{p_B}\\right) \\right)$$\n$$I = \\min_{q_G+q_B=0.5} \\left( 0.5 \\ln\\left(\\frac{0.5}{1/3}\\right) + q_G \\ln\\left(\\frac{q_G}{1/3}\\right) + q_B \\ln\\left(\\frac{q_B}{1/3}\\right) \\right)$$\nThe first term is constant. Let's focus on minimizing the part involving $q_G$ and $q_B$. Let $q_B = 0.5 - q_G$. We want to minimize the function:\n$$f(q_G) = q_G \\ln(3q_G) + (0.5 - q_G) \\ln(3(0.5 - q_G))$$\nfor $q_G \\in [0, 0.5]$.\nWe can expand the logarithms:\n$$f(q_G) = q_G(\\ln 3 + \\ln q_G) + (0.5-q_G)(\\ln 3 + \\ln(0.5-q_G))$$\n$$f(q_G) = q_G\\ln 3 + (0.5-q_G)\\ln 3 + q_G\\ln q_G + (0.5-q_G)\\ln(0.5-q_G)$$\n$$f(q_G) = 0.5\\ln 3 + q_G\\ln q_G + (0.5-q_G)\\ln(0.5-q_G)$$\nTo find the minimum, we can take the derivative with respect to $q_G$ and set it to zero. Let $g(q_G) = q_G\\ln q_G + (0.5-q_G)\\ln(0.5-q_G)$.\n$$g'(q_G) = \\frac{d}{dq_G} (q_G\\ln q_G) + \\frac{d}{dq_G} ((0.5-q_G)\\ln(0.5-q_G))$$\n$$g'(q_G) = (\\ln q_G + 1) + (-1 \\cdot \\ln(0.5-q_G) + (0.5-q_G) \\cdot \\frac{-1}{0.5-q_G})$$\n$$g'(q_G) = \\ln q_G + 1 - \\ln(0.5-q_G) - 1 = \\ln q_G - \\ln(0.5-q_G)$$\nSetting the derivative to zero:\n$$g'(q_G) = 0 \\implies \\ln q_G = \\ln(0.5-q_G) \\implies q_G = 0.5 - q_G$$\nThis gives $2q_G = 0.5$, so $q_G = 0.25$.\nThen, $q_B = 0.5 - q_G = 0.5 - 0.25 = 0.25$.\nThis result could also be deduced from the symmetry of the problem with respect to Green and Blue colors.\n\nThe empirical distribution $Q^*$ that minimizes the KL-divergence is:\n$$Q^* = (q_R, q_G, q_B) = (0.5, 0.25, 0.25) = \\left(\\frac{1}{2}, \\frac{1}{4}, \\frac{1}{4}\\right)$$\nNow we calculate the rate $I$ by evaluating $D_{KL}(Q^* || P)$:\n$$I = D_{KL}(Q^* || P) = \\frac{1}{2} \\ln\\left(\\frac{1/2}{1/3}\\right) + \\frac{1}{4} \\ln\\left(\\frac{1/4}{1/3}\\right) + \\frac{1}{4} \\ln\\left(\\frac{1/4}{1/3}\\right)$$\n$$I = \\frac{1}{2} \\ln\\left(\\frac{3}{2}\\right) + \\frac{1}{4} \\ln\\left(\\frac{3}{4}\\right) + \\frac{1}{4} \\ln\\left(\\frac{3}{4}\\right)$$\n$$I = \\frac{1}{2} \\ln\\left(\\frac{3}{2}\\right) + \\frac{1}{2} \\ln\\left(\\frac{3}{4}\\right)$$\nUsing the logarithm property $\\ln a + \\ln b = \\ln(ab)$:\n$$I = \\frac{1}{2} \\left[ \\ln\\left(\\frac{3}{2}\\right) + \\ln\\left(\\frac{3}{4}\\right) \\right] = \\frac{1}{2} \\ln\\left(\\frac{3}{2} \\cdot \\frac{3}{4}\\right)$$\n$$I = \\frac{1}{2} \\ln\\left(\\frac{9}{8}\\right)$$\nTo get the numerical value, we calculate:\n$$I = \\frac{1}{2} \\ln(1.125) \\approx \\frac{1}{2} (0.117783035) \\approx 0.058891517$$\nRounding to four significant figures, we get $0.05889$.", "answer": "$$\\boxed{0.05889}$$", "id": "1655881"}, {"introduction": "Finally, we demonstrate the power and flexibility of large deviation theory by applying it to an abstract property of a sequence: its empirical entropy. This problem [@problem_id:1655900] requires using the contraction principle, a key concept that allows us to analyze rare events defined by complex functions of the empirical distribution. This practice is an excellent showcase of how to translate a condition on a derived quantity like entropy back into a constraint on the underlying frequencies to find the large deviation rate.", "problem": "Consider a source that generates a sequence of binary digits (bits) in an independent and identically distributed (i.i.d.) manner. The probability of generating a '1' is $p = 1/4$, and the probability of generating a '0' is $1-p = 3/4$. For a long sequence of $N$ bits, let $\\hat{p}$ denote the empirical frequency of '1's (i.e., the number of '1's in the sequence divided by $N$). The empirical entropy of the sequence is given by $H(\\hat{p}) = -\\hat{p} \\log_2(\\hat{p}) - (1-\\hat{p})\\log_2(1-\\hat{p})$.\n\nWe are interested in the probability of a specific large deviation event: the event that the empirical entropy $H(\\hat{p})$ of the sequence is greater than or equal to the entropy of a different binary source that produces '1's with a probability of $1/3$.\n\nAccording to the theory of large deviations, for very large $N$, the probability of this event, $P_N$, can be approximated as $P_N \\approx \\exp(-N \\cdot I)$, where $I$ is a positive constant known as the rate function or large deviation exponent.\n\nDetermine the exact value of this exponent $I$. Your answer should be a closed-form analytic expression involving natural logarithms ($\\ln$).", "solution": "The source is i.i.d. Bernoulli with true parameter $p=1/4$. Let $q$ denote a generic empirical frequency. The large deviations principle for empirical distributions (Sanov's theorem) states that, for a set $A$ of empirical measures,\n$$\n\\Pr(\\hat{p} \\in A) \\approx \\exp\\!\\left(-N \\inf_{q \\in A} D(q\\|p)\\right),\n$$\nwhere for Bernoulli laws\n$$\nD(q\\|p) = q \\ln\\!\\left(\\frac{q}{p}\\right) + (1-q)\\ln\\!\\left(\\frac{1-q}{1-p}\\right).\n$$\n\nThe event of interest is $H(\\hat{p}) \\geq H(1/3)$ where $H$ is defined with base-$2$ logarithms. Since changing the logarithm base multiplies entropy by the positive constant $1/\\ln 2$, the inequality $H(\\hat{p}) \\geq H(1/3)$ is equivalent to $H_{e}(\\hat{p}) \\geq H_{e}(1/3)$, where $H_{e}(q) = -q \\ln q - (1-q)\\ln(1-q)$. Because $H_{e}(q)$ is symmetric around $q=1/2$ and strictly increasing on $[0,1/2]$, the constraint $H_{e}(q) \\geq H_{e}(1/3)$ is equivalent to $q \\in [1/3,\\,2/3]$.\n\nHence, by Sanov's theorem and the contraction principle, the large deviation exponent is\n$$\nI = \\inf_{q \\in [1/3,\\,2/3]} D(q\\|1/4).\n$$\nThe function $D(q\\|1/4)$ is convex in $q$ with a unique global minimum at $q=1/4$, which lies outside $[1/3,\\,2/3]$. Therefore, the infimum over the interval is attained at the boundary point closest to $1/4$, namely $q^{\\star}=1/3$. Thus\n$$\nI = D\\!\\left(\\frac{1}{3}\\,\\Big\\|\\,\\frac{1}{4}\\right)\n= \\frac{1}{3}\\ln\\!\\left(\\frac{1/3}{1/4}\\right) + \\frac{2}{3}\\ln\\!\\left(\\frac{1-1/3}{1-1/4}\\right)\n= \\frac{1}{3}\\ln\\!\\left(\\frac{4}{3}\\right) + \\frac{2}{3}\\ln\\!\\left(\\frac{8}{9}\\right).\n$$\nCombining terms,\n$$\nI = \\frac{1}{3}\\big(2\\ln 2 - \\ln 3\\big) + \\frac{2}{3}\\big(3\\ln 2 - 2\\ln 3\\big)\n= \\frac{8}{3}\\ln 2 - \\frac{5}{3}\\ln 3\n= \\frac{1}{3}\\ln\\!\\left(\\frac{256}{243}\\right).\n$$\nThis is the exact large deviation exponent.", "answer": "$$\\boxed{\\frac{1}{3}\\ln\\!\\left(\\frac{256}{243}\\right)}$$", "id": "1655900"}]}