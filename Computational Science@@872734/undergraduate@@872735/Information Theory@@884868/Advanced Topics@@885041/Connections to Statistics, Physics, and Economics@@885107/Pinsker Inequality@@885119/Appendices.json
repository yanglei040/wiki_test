{"hands_on_practices": [{"introduction": "The best way to develop an intuition for a fundamental result like Pinsker's inequality is to test it with a simple, concrete example. In this exercise [@problem_id:1646398], we will consider two Bernoulli distributions, which are the building blocks for modeling binary events. By explicitly calculating both the Kullback-Leibler divergence and the Total Variation distance, you will verify that the inequality holds and see how these two important measures of statistical difference relate to one another in a tangible scenario.", "problem": "In a digital manufacturing process, a component is designed to be in one of two states, labeled 0 and 1. The ideal manufacturing specification dictates that the probability of a component being in state 1 is exactly $p$. This can be modeled by a Bernoulli distribution, let's call it $P$, with a probability mass function $P(1) = p$ and $P(0) = 1-p$.\n\nAfter a period of operation, a quality control check reveals that the process has drifted. The components now follow a different Bernoulli distribution, $Q$, with a probability mass function $Q(1) = q$ and $Q(0) = 1-q$.\n\nTo quantify the statistical difference between the ideal distribution $P$ and the actual distribution $Q$, two important metrics from information theory are used:\n\n1.  The Total Variation Distance, defined as $\\delta(P, Q) = \\frac{1}{2} \\sum_{x \\in \\{0, 1\\}} |P(x) - Q(x)|$.\n2.  The Kullback-Leibler (KL) Divergence, defined as $D_{KL}(P \\| Q) = \\sum_{x \\in \\{0, 1\\}} P(x) \\ln\\left(\\frac{P(x)}{Q(x)}\\right)$.\n\nThese two measures are related by Pinsker's inequality, which states that $D_{KL}(P \\| Q) \\ge 2[\\delta(P, Q)]^2$. The term $2[\\delta(P, Q)]^2$ acts as a theoretical lower bound for the KL divergence.\n\nSuppose the ideal specification is $p=0.5$, but the actual observed process has drifted to $q=0.8$. Calculate the ratio of the actual KL divergence to the theoretical lower bound given by Pinsker's inequality. That is, compute the value of the ratio $R = \\frac{D_{KL}(P \\| Q)}{2[\\delta(P, Q)]^2}$.\n\nRound your final answer to three significant figures.", "solution": "The total variation distance between two Bernoulli distributions with parameters $p$ and $q$ is\n$$\n\\delta(P,Q)=\\frac{1}{2}\\left(|p-q|+|(1-p)-(1-q)|\\right)=\\frac{1}{2}\\left(|p-q|+|q-p|\\right)=|p-q|.\n$$\nFor $p=\\frac{1}{2}$ and $q=\\frac{4}{5}$,\n$$\n\\delta(P,Q)=\\left|\\frac{1}{2}-\\frac{4}{5}\\right|=\\left|\\frac{5-8}{10}\\right|=\\frac{3}{10}.\n$$\nPinsker's lower bound is\n$$\n2[\\delta(P,Q)]^{2}=2\\left(\\frac{3}{10}\\right)^{2}=2\\cdot\\frac{9}{100}=\\frac{9}{50}.\n$$\nThe Kullback-Leibler divergence for Bernoulli distributions is\n$$\nD_{KL}(P\\|Q)=p\\ln\\!\\left(\\frac{p}{q}\\right)+(1-p)\\ln\\!\\left(\\frac{1-p}{1-q}\\right).\n$$\nSubstituting $p=\\frac{1}{2}$ and $q=\\frac{4}{5}$ gives\n$$\nD_{KL}(P\\|Q)=\\frac{1}{2}\\ln\\!\\left(\\frac{\\frac{1}{2}}{\\frac{4}{5}}\\right)+\\frac{1}{2}\\ln\\!\\left(\\frac{\\frac{1}{2}}{\\frac{1}{5}}\\right)=\\frac{1}{2}\\left[\\ln\\!\\left(\\frac{5}{8}\\right)+\\ln\\!\\left(\\frac{5}{2}\\right)\\right]=\\frac{1}{2}\\ln\\!\\left(\\frac{25}{16}\\right)=\\ln\\!\\left(\\frac{5}{4}\\right).\n$$\nTherefore, the ratio is\n$$\nR=\\frac{D_{KL}(P\\|Q)}{2[\\delta(P,Q)]^{2}}=\\frac{\\ln\\!\\left(\\frac{5}{4}\\right)}{\\frac{9}{50}}=\\frac{50}{9}\\ln\\!\\left(\\frac{5}{4}\\right)\\approx 1.239686\\ldots\n$$\nRounded to three significant figures, $R=1.24$.", "answer": "$$\\boxed{1.24}$$", "id": "1646398"}, {"introduction": "Having confirmed the inequality for a binary case, we now generalize to a scenario with more than two outcomes. This practice [@problem_id:1646410] explores two distinct probability distributions over a three-element set, a common situation in areas like machine learning classification or statistical modeling. You will apply the general formulas for KL divergence and Total Variation distance to calculate an upper bound on the distance between the distributions, demonstrating how Pinsker's inequality serves as a practical tool for bounding one metric with another.", "problem": "A discrete random variable $X$ can take one of three outcomes from the set $\\mathcal{X} = \\{o_1, o_2, o_3\\}$. We have two different probability distributions, $P$ and $Q$, that model this variable:\n-   Distribution $P$: $P(X=o_1) = 0.6$, $P(X=o_2) = 0.3$, $P(X=o_3) = 0.1$.\n-   Distribution $Q$: $Q(X=o_1) = 0.1$, $Q(X=o_2) = 0.3$, $Q(X=o_3) = 0.6$.\n\nIn information theory, two important measures are used to compare probability distributions. The first is the total variation distance (TVD), defined for discrete distributions as $d_{TV}(P, Q) = \\frac{1}{2} \\sum_{x \\in \\mathcal{X}} |P(x) - Q(x)|$. The second is the Kullback-Leibler (KL) divergence, defined as $D_{KL}(P \\| Q) = \\sum_{x \\in \\mathcal{X}} P(x) \\ln\\left(\\frac{P(x)}{Q(x)}\\right)$, where $\\ln$ denotes the natural logarithm.\n\nThese two measures are related by a fundamental result, often known as Pinsker's inequality, which provides an upper bound on the TVD in terms of the KL divergence:\n$$d_{TV}(P, Q) \\le \\sqrt{\\frac{1}{2} D_{KL}(P \\| Q)}$$\n\nYour task is to calculate the numerical value of this upper bound for the given distributions $P$ and $Q$. Report your result as a dimensionless number, rounded to four significant figures.", "solution": "We are given discrete distributions $P$ and $Q$ on $\\mathcal{X} = \\{o_{1}, o_{2}, o_{3}\\}$ with\n$P(o_{1}) = 0.6$, $P(o_{2}) = 0.3$, $P(o_{3}) = 0.1$ and $Q(o_{1}) = 0.1$, $Q(o_{2}) = 0.3$, $Q(o_{3}) = 0.6$.\nPinsker's inequality states\n$$\nd_{TV}(P,Q) \\le \\sqrt{\\frac{1}{2} D_{KL}(P \\| Q)}.\n$$\nFirst compute the Kullback-Leibler divergence using its definition\n$$\nD_{KL}(P \\| Q) = \\sum_{x \\in \\mathcal{X}} P(x)\\,\\ln\\!\\left(\\frac{P(x)}{Q(x)}\\right).\n$$\nTerm by term,\n$$\n\\begin{aligned}\nx=o_{1}:&\\quad 0.6\\,\\ln\\!\\left(\\frac{0.6}{0.1}\\right) = 0.6\\,\\ln(6),\\\\\nx=o_{2}:&\\quad 0.3\\,\\ln\\!\\left(\\frac{0.3}{0.3}\\right) = 0.3\\,\\ln(1) = 0,\\\\\nx=o_{3}:&\\quad 0.1\\,\\ln\\!\\left(\\frac{0.1}{0.6}\\right) = 0.1\\,\\ln\\!\\left(\\frac{1}{6}\\right) = -0.1\\,\\ln(6).\n\\end{aligned}\n$$\nSumming these gives\n$$\nD_{KL}(P \\| Q) = \\left(0.6 - 0.1\\right)\\ln(6) = \\frac{1}{2}\\ln(6).\n$$\nTherefore the Pinsker upper bound evaluates to\n$$\n\\sqrt{\\frac{1}{2} D_{KL}(P \\| Q)} = \\sqrt{\\frac{1}{2}\\cdot \\frac{1}{2}\\ln(6)} = \\sqrt{\\frac{1}{4}\\ln(6)} = \\frac{1}{2}\\sqrt{\\ln(6)}.\n$$\nNumerically,\n$$\n\\ln(6) \\approx 1.791759469,\\quad \\sqrt{\\ln(6)} \\approx 1.338566287,\\quad \\frac{1}{2}\\sqrt{\\ln(6)} \\approx 0.6692831435.\n$$\nRounded to four significant figures, the upper bound is $0.6693$, which is dimensionless.", "answer": "$$\\boxed{0.6693}$$", "id": "1646410"}, {"introduction": "A crucial question for any mathematical inequality is understanding its \"tightness\"â€”how close is the bound to the actual value? This advanced practice [@problem_id:1646397] delves into this question by examining two continuous Gaussian distributions that are nearly identical. By calculating the limit of the ratio of the squared TV distance to the KL divergence as the separation between the distributions vanishes, you will uncover a precise, fundamental constant that characterizes their local relationship. This exercise reveals that Pinsker's inequality is not just a loose bound but describes an asymptotically exact relationship for one of the most important distribution families in all of science.", "problem": "Consider two univariate Gaussian probability distributions, $P$ and $Q$. The probability density function corresponding to $P$ is $p(x)$, for a random variable $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$. The probability density function for $Q$ is $q(x)$, for a random variable $X \\sim \\mathcal{N}(\\mu+\\delta, \\sigma^2)$. Here, $\\mu$ and $\\sigma > 0$ are the mean and standard deviation, and $\\delta$ is a parameter representing a small shift in the mean.\n\nThe relationship between these two distributions can be quantified using various metrics. Two of the most important are the Kullback-Leibler (KL) divergence and the Total Variation (TV) distance.\nThe KL divergence is defined as:\n$$D_{KL}(P\\|Q) = \\int_{-\\infty}^{\\infty} p(x) \\ln\\left(\\frac{p(x)}{q(x)}\\right) dx$$\nThe TV distance is defined as:\n$$TV(P,Q) = \\frac{1}{2} \\int_{-\\infty}^{\\infty} |p(x) - q(x)| dx$$\nA celebrated result in information theory, Pinsker's inequality, provides a lower bound for the KL divergence in terms of the TV distance: $D_{KL}(P\\|Q) \\ge 2[TV(P,Q)]^2$.\n\nTo investigate the behavior of this relationship for nearly identical Gaussian distributions, you are tasked to evaluate the limiting value of the ratio that appears in Pinsker's inequality. Calculate the exact value of the following limit:\n$$L = \\lim_{\\delta \\to 0} \\frac{2 [TV(P,Q)]^2}{D_{KL}(P\\|Q)}$$\nYour final answer should be a single, closed-form analytic expression.", "solution": "Let $P$ and $Q$ be $\\mathcal{N}(\\mu,\\sigma^{2})$ and $\\mathcal{N}(\\mu+\\delta,\\sigma^{2})$, respectively, with densities\n$$\np(x)=\\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\!\\left(-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}\\right),\\quad\nq(x)=\\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\!\\left(-\\frac{(x-\\mu-\\delta)^{2}}{2\\sigma^{2}}\\right).\n$$\n\nFirst, compute the Kullback-Leibler divergence using its definition $D_{KL}(P\\|Q)=\\int p(x)\\ln\\!\\left(\\frac{p(x)}{q(x)}\\right)\\,dx$. Since\n$$\n\\ln p(x)=-\\frac{1}{2}\\ln(2\\pi\\sigma^{2})-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}},\\quad\n\\ln q(x)=-\\frac{1}{2}\\ln(2\\pi\\sigma^{2})-\\frac{(x-\\mu-\\delta)^{2}}{2\\sigma^{2}},\n$$\nwe have\n$$\n\\ln\\!\\left(\\frac{p(x)}{q(x)}\\right)=\\frac{(x-\\mu-\\delta)^{2}-(x-\\mu)^{2}}{2\\sigma^{2}}\n=\\frac{-2\\delta(x-\\mu)+\\delta^{2}}{2\\sigma^{2}}\n=-\\frac{\\delta}{\\sigma^{2}}(x-\\mu)+\\frac{\\delta^{2}}{2\\sigma^{2}}.\n$$\nTaking expectation under $p$, and using $\\mathbb{E}_{p}[X-\\mu]=0$, yields\n$$\nD_{KL}(P\\|Q)=\\mathbb{E}_{p}\\!\\left[\\ln\\!\\left(\\frac{p(X)}{q(X)}\\right)\\right]\n=-\\frac{\\delta}{\\sigma^{2}}\\mathbb{E}_{p}[X-\\mu]+\\frac{\\delta^{2}}{2\\sigma^{2}}\n=\\frac{\\delta^{2}}{2\\sigma^{2}}.\n$$\n\nNext, compute the total variation distance, defined by $TV(P,Q)=\\frac{1}{2}\\int_{-\\infty}^{\\infty}|p(x)-q(x)|\\,dx$. The densities $p$ and $q$ cross at the unique point $x^{\\ast}$ where $p(x^{\\ast})=q(x^{\\ast})$. Solving\n$$\n\\exp\\!\\left(-\\frac{(x^{\\ast}-\\mu)^{2}}{2\\sigma^{2}}\\right)=\\exp\\!\\left(-\\frac{(x^{\\ast}-\\mu-\\delta)^{2}}{2\\sigma^{2}}\\right)\n$$\ngives $(x^{\\ast}-\\mu)^{2}=(x^{\\ast}-\\mu-\\delta)^{2}$, hence $x^{\\ast}=\\mu+\\frac{\\delta}{2}$. Splitting the integral at $x^{\\ast}$ and using $TV(P,Q)=\\int_{\\{p\\ge q\\}}(p-q)\\,dx$ yields\n$$\nTV(P,Q)=\\int_{-\\infty}^{x^{\\ast}}(p-q)\\,dx=\\left[\\int_{-\\infty}^{x^{\\ast}}p(x)\\,dx\\right]-\\left[\\int_{-\\infty}^{x^{\\ast}}q(x)\\,dx\\right].\n$$\nLet $\\Phi$ denote the standard normal cumulative distribution function. Then\n$$\n\\int_{-\\infty}^{x^{\\ast}}p(x)\\,dx=\\Phi\\!\\left(\\frac{x^{\\ast}-\\mu}{\\sigma}\\right)=\\Phi\\!\\left(\\frac{\\delta}{2\\sigma}\\right),\\quad\n\\int_{-\\infty}^{x^{\\ast}}q(x)\\,dx=\\Phi\\!\\left(\\frac{x^{\\ast}-\\mu-\\delta}{\\sigma}\\right)=\\Phi\\!\\left(-\\frac{\\delta}{2\\sigma}\\right).\n$$\nUsing $\\Phi(-z)=1-\\Phi(z)$, we obtain the exact expression\n$$\nTV(P,Q)=\\Phi\\!\\left(\\frac{|\\delta|}{2\\sigma}\\right)-\\left[1-\\Phi\\!\\left(\\frac{|\\delta|}{2\\sigma}\\right)\\right]\n=2\\Phi\\!\\left(\\frac{|\\delta|}{2\\sigma}\\right)-1.\n$$\n\nTo evaluate the limit, set $y=\\frac{|\\delta|}{2\\sigma}$ and use the Taylor expansion of $\\Phi$ at $0$. Since $\\Phi'(y)=\\varphi(y)$ with $\\varphi(y)=\\frac{1}{\\sqrt{2\\pi}}\\exp(-y^{2}/2)$ and $\\varphi(0)=\\frac{1}{\\sqrt{2\\pi}}$, we have\n$$\n\\Phi(y)=\\frac{1}{2}+\\frac{1}{\\sqrt{2\\pi}}\\,y+o(y)\\quad\\text{as }y\\to 0.\n$$\nTherefore,\n$$\n2\\,TV(P,Q)=2\\left[2\\Phi(y)-1\\right]=4\\left(\\frac{1}{\\sqrt{2\\pi}}\\,y\\right)+o(y)=\\frac{4}{\\sqrt{2\\pi}}\\,y+o(y),\n$$\nand hence\n$$\n2\\,[TV(P,Q)]^{2}=2\\left[2\\Phi(y)-1\\right]^{2}\n=2\\left(\\frac{2}{\\sqrt{2\\pi}}\\,y+o(y)\\right)^{2}\n=\\frac{4}{\\pi}\\,y^{2}+o(y^{2}).\n$$\nSince $y^{2}=\\frac{\\delta^{2}}{4\\sigma^{2}}$, it follows that\n$$\n2\\,[TV(P,Q)]^{2}=\\frac{\\delta^{2}}{\\pi\\sigma^{2}}+o(\\delta^{2}).\n$$\nRecalling that $D_{KL}(P\\|Q)=\\frac{\\delta^{2}}{2\\sigma^{2}}$, we obtain\n$$\n\\frac{2\\,[TV(P,Q)]^{2}}{D_{KL}(P\\|Q)}\n=\\frac{\\frac{\\delta^{2}}{\\pi\\sigma^{2}}+o(\\delta^{2})}{\\frac{\\delta^{2}}{2\\sigma^{2}}}\n=\\frac{2}{\\pi}+o(1),\n$$\nso the desired limit is\n$$\nL=\\lim_{\\delta\\to 0}\\frac{2\\,[TV(P,Q)]^{2}}{D_{KL}(P\\|Q)}=\\frac{2}{\\pi}.\n$$", "answer": "$$\\boxed{\\frac{2}{\\pi}}$$", "id": "1646397"}]}