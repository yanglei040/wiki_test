{"hands_on_practices": [{"introduction": "Understanding the behavior of a communication system at its limits is key to robust design. This first practice explores the scenario of extremely low Signal-to-Noise Ratio (SNR), where the noise completely dominates the signal. By calculating the Minimum Mean Square Error (MMSE) in this limit, you will discover its fundamental connection to the variance of the input signal itself, providing an intuitive anchor for the I-MMSE relationship.", "problem": "In digital communication, understanding channel performance at very low Signal-to-Noise Ratios (SNRs) is crucial for designing robust systems. For an Additive White Gaussian Noise (AWGN) channel, the slope of the mutual information curve with respect to SNR near zero is determined by a fundamental quantity: the Minimum Mean Square Error (MMSE) evaluated at zero SNR.\n\nConsider a communication system that transmits a signal $X$. The signal is a random variable that takes values from the set $\\{-A, 0, A\\}$, where $A=5.0$. The probability distribution for $X$ is given by $P(X = A) = 0.25$, $P(X = -A) = 0.25$, and $P(X=0)=0.5$.\n\nThe signal is transmitted over an AWGN channel, and the received signal is $Y = X + Z$. Here, $Z$ is a zero-mean Gaussian noise variable with variance $\\sigma^2$, independent of $X$. The SNR is defined as the ratio of the average signal power to the noise power, $\\text{SNR} = \\frac{E[X^2]}{\\sigma^2}$.\n\nThe MMSE is the minimum possible average squared error of any estimator of $X$ based on the observation $Y$, and is given by $\\text{mmse} = E[(X - E[X|Y])^2]$.\n\nCalculate the value of the MMSE in the limit as the SNR approaches zero.", "solution": "The problem requires us to compute the Minimum Mean Square Error (MMSE) in the limit of zero Signal-to-Noise Ratio (SNR). The MMSE in estimating a random variable $X$ from an observation $Y$ is given by the expression $\\text{mmse} = E[(X - \\hat{X})^2]$, where the optimal estimator is the conditional mean, $\\hat{X} = E[X|Y]$.\n\nThe SNR is defined as $\\text{SNR} = \\frac{E[X^2]}{\\sigma^2}$, where $E[X^2]$ is the average power of the signal $X$ and $\\sigma^2$ is the variance (power) of the noise $Z$.\n\nFirst, we compute the average power of the signal $X$. The signal $X$ can take values from $\\{-5.0, 0, 5.0\\}$ with probabilities $P(X=-5.0) = 0.25$, $P(X=0) = 0.5$, and $P(X=5.0) = 0.25$.\nThe average signal power, $E[X^2]$, is calculated as:\n$$ E[X^2] = \\sum_{x} x^2 P(X=x) $$\n$$ E[X^2] = (-5.0)^2 P(X=-5.0) + (0)^2 P(X=0) + (5.0)^2 P(X=5.0) $$\n$$ E[X^2] = (25.0)(0.25) + (0)(0.5) + (25.0)(0.25) = 6.25 + 0 + 6.25 = 12.5 $$\nThe average signal power $E[X^2]$ is a finite, positive constant.\n\nThe problem asks for the MMSE in the limit as $\\text{SNR} \\to 0$. We examine the implication of this limit on the noise power $\\sigma^2$:\n$$ \\lim_{\\text{SNR} \\to 0} \\frac{E[X^2]}{\\sigma^2} = \\lim_{\\text{SNR} \\to 0} \\frac{12.5}{\\sigma^2} = 0 $$\nFor this limit to hold, the denominator must approach infinity, so $\\sigma^2 \\to \\infty$. A zero-SNR condition corresponds to infinite noise power.\n\nThe received signal is $Y = X + Z$. In the limit of infinite noise power ($\\sigma^2 \\to \\infty$), the noise term $Z$ completely overwhelms the finite signal term $X$. Consequently, the observed signal $Y$ contains no information about the transmitted signal $X$. In this limit, $X$ and $Y$ become statistically independent.\n\nThe MMSE estimator is the conditional expectation $\\hat{X} = E[X|Y]$. When the observation $Y$ is independent of the signal $X$, the conditional expectation provides no more information than the unconditional expectation. Therefore, the estimator becomes:\n$$ \\lim_{\\sigma^2 \\to \\infty} E[X|Y] = E[X] $$\n\nWe now calculate the unconditional mean of the signal, $E[X]$:\n$$ E[X] = \\sum_{x} x P(X=x) $$\n$$ E[X] = (-5.0) P(X=-5.0) + (0) P(X=0) + (5.0) P(X=5.0) $$\n$$ E[X] = (-5.0)(0.25) + (0)(0.5) + (5.0)(0.25) = -1.25 + 0 + 1.25 = 0 $$\nThus, in the zero-SNR limit, the best estimate of $X$ is its mean value, which is $0$, irrespective of the value of $Y$.\n\nNow, we can find the MMSE in this limit by substituting the limiting estimator into the definition of MMSE:\n$$ \\text{mmse}(\\text{SNR}=0) = \\lim_{\\text{SNR} \\to 0} E[(X - E[X|Y])^2] $$\nBy moving the limit inside the expectation (which is permissible here), we get:\n$$ \\text{mmse}(\\text{SNR}=0) = E\\left[\\left(X - \\lim_{\\text{SNR} \\to 0} E[X|Y]\\right)^2\\right] = E[(X - E[X])^2] $$\nThis final expression is the definition of the variance of $X$, denoted as $\\text{Var}(X)$.\n\nThe variance is calculated using the formula $\\text{Var}(X) = E[X^2] - (E[X])^2$. Using the values we have already computed:\n$$ E[X^2] = 12.5 $$\n$$ E[X] = 0 $$\nThe variance is:\n$$ \\text{Var}(X) = 12.5 - (0)^2 = 12.5 $$\n\nTherefore, the MMSE in the limit as the SNR approaches zero is equal to the variance of the signal $X$, which is 12.5.", "answer": "$$\\boxed{12.5}$$", "id": "1654355"}, {"introduction": "The I-MMSE relationship provides a powerful bridge between estimation error and information-theoretic measures. This exercise guides you through the process of constructing this bridge for a simple binary signal, which is the cornerstone of digital communications. You will derive the MMSE estimator as a function of the received signal and then formulate the complete integral expression for mutual information, giving you a concrete, step-by-step understanding of how these concepts are mechanically linked.", "problem": "Consider a simple digital communication system where a binary signal $X$ is drawn from the set $\\{-1, 1\\}$, with $P(X=1) = P(X=-1) = 1/2$. The signal is transmitted through an Additive White Gaussian Noise (AWGN) channel, resulting in a received signal $Y = X + Z$, where the noise $Z$ is a random variable following a normal distribution with mean 0 and variance $\\sigma^2$, i.e., $Z \\sim \\mathcal{N}(0, \\sigma^2)$. The noise $Z$ is independent of the signal $X$.\n\nA fundamental result in information theory, known as the I-MMSE relation, connects the mutual information between the input and output of this channel to the Minimum Mean Square Error (MMSE) of estimating the input. For this channel model, the relationship can be established by considering an auxiliary channel $Y_t = X + \\sqrt{t}W$, where $W \\sim \\mathcal{N}(0, 1)$ is standard Gaussian noise and $t \\ge 0$ is a parameter representing noise variance. The original channel corresponds to the case where $t = \\sigma^2$. The mutual information for the original channel is then given by the integral:\n$$ I(X; Y) = \\frac{1}{2} \\int_{\\sigma^2}^{\\infty} \\frac{\\text{mmse}(X|Y_t)}{t^2} dt $$\nHere, $\\text{mmse}(X|Y_t) = E[(X - E[X|Y_t])^2]$ is the MMSE associated with estimating the signal $X$ from the observation $Y_t$.\n\nYour task is to derive a complete and explicit integral expression for the mutual information $I(X; Y)$ as a function of the noise variance $\\sigma^2$. The final expression will be in the form of a nested integral.", "solution": "The problem asks for an explicit nested integral expression for the mutual information $I(X;Y)$ for a channel $Y=X+Z$ with $Z\\sim\\mathcal{N}(0,\\sigma^2)$, where $X\\in\\{-1,1\\}$ with equal probability. We are given the I-MMSE relation in terms of an integral over noise variance:\n$$\nI(X;Y)=\\frac{1}{2}\\int_{\\sigma^{2}}^{\\infty}\\frac{\\text{mmse}(X|Y_{t})}{t^2}\\,dt,\n$$\nwhere $Y_t = X + \\sqrt{t}W$ with $W\\sim\\mathcal{N}(0,1)$, and $\\text{mmse}(X|Y_{t})=E\\big[(X-E[X|Y_{t}])^{2}\\big]$.\n\nWe first compute the posterior mean $m_{t}(y)\\triangleq E[X|Y_{t}=y]$ using Bayesâ€™ rule. The conditional probability density functions are\n$$\nf_{Y_{t}|X}(y|x)=\\frac{1}{\\sqrt{2\\pi t}}\\exp\\!\\left(-\\frac{(y-x)^{2}}{2t}\\right),\\quad x\\in\\{-1,1\\}.\n$$\nWith $P(X=\\pm 1)=\\frac{1}{2}$, the posterior probabilities are\n$$\nP(X=1|y)=\\frac{f_{Y_{t}|X}(y|1)}{f_{Y_{t}|X}(y|1)+f_{Y_{t}|X}(y|-1)},\\quad\nP(X=-1|y)=\\frac{f_{Y_{t}|X}(y|-1)}{f_{Y_{t}|X}(y|1)+f_{Y_{t}|X}(y|-1)}.\n$$\nThe posterior mean is the difference of these probabilities:\n$$\nm_{t}(y)=P(X=1|y)-P(X=-1|y)=\\frac{f_{Y_{t}|X}(y|1)-f_{Y_{t}|X}(y|-1)}{f_{Y_{t}|X}(y|1)+f_{Y_{t}|X}(y|-1)}.\n$$\nSubstituting the densities and factoring out common terms $\\exp\\!\\left(-\\frac{y^{2}+1}{2t}\\right)$ from the numerator and denominator, we get:\n$$\nm_{t}(y)=\\frac{\\exp\\!\\left(\\frac{y}{t}\\right)-\\exp\\!\\left(-\\frac{y}{t}\\right)}{\\exp\\!\\left(\\frac{y}{t}\\right)+\\exp\\!\\left(-\\frac{y}{t}\\right)}=\\tanh\\!\\left(\\frac{y}{t}\\right).\n$$\n\nNext, we find the MMSE. Since $X^2=1$, the MMSE can be expressed as:\n$$\n\\text{mmse}(X|Y_{t})=E[X^2] - E[(E[X|Y_t])^2] = 1-E\\big[m_{t}(Y_{t})^{2}\\big]=1-E\\!\\left[\\tanh^{2}\\!\\left(\\frac{Y_{t}}{t}\\right)\\right].\n$$\nTo evaluate the expectation, we need the marginal probability density of $Y_{t}$, which is an equally weighted Gaussian mixture:\n$$\np_{Y_{t}}(y)=\\frac{1}{2}f_{Y_{t}|X}(y|1) + \\frac{1}{2}f_{Y_{t}|X}(y|-1) = \\frac{1}{2\\sqrt{2\\pi t}}\\left[\\exp\\!\\left(-\\frac{(y-1)^{2}}{2t}\\right)+\\exp\\!\\left(-\\frac{(y+1)^{2}}{2t}\\right)\\right].\n$$\nThe expectation is therefore an integral over this density:\n$$\nE\\!\\left[\\tanh^{2}\\!\\left(\\frac{Y_{t}}{t}\\right)\\right]=\\int_{-\\infty}^{\\infty}\\tanh^{2}\\!\\left(\\frac{y}{t}\\right)\\,p_{Y_{t}}(y)\\,dy\n$$\nThis gives the MMSE as a function of noise variance $t$:\n$$\n\\text{mmse}(X|Y_{t})=1-\\int_{-\\infty}^{\\infty}\\tanh^{2}\\!\\left(\\frac{y}{t}\\right)\\,\\frac{1}{2\\sqrt{2\\pi t}}\\left[\\exp\\!\\left(-\\frac{(y-1)^{2}}{2t}\\right)+\\exp\\!\\left(-\\frac{(y+1)^{2}}{2t}\\right)\\right]dy.\n$$\n\nFinally, we substitute this expression for $\\text{mmse}(X|Y_{t})$ into the given integral formula for mutual information to obtain the desired explicit nested integral as a function of $\\sigma^{2}$:\n$$\nI(X;Y)=\\frac{1}{2}\\int_{\\sigma^{2}}^{\\infty}\\frac{1}{t^2}\\left[1-\\int_{-\\infty}^{\\infty}\\tanh^{2}\\!\\left(\\frac{y}{t}\\right)\\,\\frac{1}{2\\sqrt{2\\pi t}}\\left(\\exp\\!\\left(-\\frac{(y-1)^{2}}{2t}\\right)+\\exp\\!\\left(-\\frac{(y+1)^{2}}{2t}\\right)\\right)dy\\right]dt.\n$$\nThis is the complete expression for the mutual information in nats.", "answer": "$$\\boxed{\\frac{1}{2}\\int_{\\sigma^{2}}^{\\infty}\\frac{1}{t^2}\\left[1-\\int_{-\\infty}^{\\infty}\\tanh^{2}\\!\\left(\\frac{y}{t}\\right)\\,\\frac{1}{2\\sqrt{2\\pi t}}\\left(\\exp\\!\\left(-\\frac{(y-1)^{2}}{2t}\\right)+\\exp\\!\\left(-\\frac{(y+1)^{2}}{2t}\\right)\\right)dy\\right]dt}$$", "id": "1654322"}, {"introduction": "Beyond calculating mutual information for a specific channel, the I-MMSE relationship can reveal deeper, more fundamental connections within information theory. In this final practice, you will use the theorem to evaluate a definite integral of a function involving the MMSE over all possible noise levels. The result of this calculation is surprisingly elegant, connecting the worlds of channel estimation and source coding by showing that this integral is directly proportional to the entropy of the source signal, $H(X)$.", "problem": "A digital communication system employs a 4-level Pulse Amplitude Modulation (4-PAM) scheme. The set of possible transmitted symbols is $\\mathcal{S} = \\{-3\\alpha, -\\alpha, \\alpha, 3\\alpha\\}$, where $\\alpha$ is a positive real constant scaling factor. The symbols are generated with the following probabilities:\n$$P(X=-3\\alpha) = P(X=3\\alpha) = \\frac{1}{8}$$\n$$P(X=-\\alpha) = P(X=\\alpha) = \\frac{3}{8}$$\n\nThe signal $X$ is transmitted over an Additive White Gaussian Noise (AWGN) channel. The received signal $Y$ is given by $Y = X + Z$, where the noise $Z$ is a Gaussian random variable with zero mean and variance $\\sigma^2 > 0$. The noise $Z$ is statistically independent of the signal $X$.\n\nAt the receiver, an optimal estimator is used to approximate $X$ from the observation $Y$. The performance of this estimator is measured by the Minimum Mean-Square Error (MMSE), which is a function of the noise variance: $\\text{mmse}(\\sigma^2) = E\\left[(X - E[X|Y])^2\\right]$.\n\nA fundamental result in information theory relates the mutual information between $X$ and $Y$ to the MMSE. For a normalized signal $X_0 = X/\\sqrt{P}$ (where $P = E[X^2]$ is the average power of $X$), this relationship is given by:\n$$I(X;Y) = \\frac{1}{2} \\int_0^{\\gamma} \\text{mmse}_0(\\rho) \\, d\\rho$$\nHere, $\\gamma = P/\\sigma^2$ is the Signal-to-Noise Ratio (SNR), $\\rho$ is the integration variable representing SNR, and $\\text{mmse}_0(\\rho)$ is the MMSE associated with estimating the normalized signal $X_0$ in a correspondingly scaled channel model.\n\nYour task is to calculate the exact value of the following integral:\n$$\\mathcal{J} = \\int_{0}^{\\infty} \\frac{\\text{mmse}(\\sigma^2)}{\\sigma^4} \\, d\\sigma^2$$\nExpress your answer as a symbolic expression.", "solution": "We are asked to evaluate the integral $\\mathcal{J} = \\int_{0}^{\\infty} \\frac{\\text{mmse}(\\sigma^2)}{\\sigma^4} \\, d\\sigma^2$. The key is to relate the integrand to the derivative of the mutual information $I(X;Y)$ with respect to the noise variance $\\sigma^2$.\n\nThe I-MMSE relation is typically given with respect to the SNR, $\\gamma = P/\\sigma^2$, for a normalized channel. Let's start from there:\n$$\n\\frac{d}{d\\gamma}I(X;Y)=\\frac{1}{2}\\,\\text{mmse}_{0}(\\gamma),\n$$\nwhere $\\text{mmse}_0(\\gamma)$ is the MMSE for a normalized signal in a channel with SNR $\\gamma$. The MMSE for the original signal $X$ is related by $\\text{mmse}(\\sigma^2) = P \\cdot \\text{mmse}_0(\\gamma)$, where $P=E[X^2]$.\n\nWe can use the chain rule to find the derivative of mutual information with respect to the noise variance $\\sigma^2$:\n$$\n\\frac{d}{d\\sigma^{2}}I(X;Y)=\\frac{dI}{d\\gamma}\\cdot\\frac{d\\gamma}{d\\sigma^{2}}\n$$\nWe have the two components:\n1.  $\\frac{dI}{d\\gamma} = \\frac{1}{2}\\text{mmse}_0(\\gamma)$\n2.  $\\frac{d\\gamma}{d\\sigma^{2}} = \\frac{d}{d\\sigma^{2}}\\left(\\frac{P}{\\sigma^2}\\right) = -\\frac{P}{(\\sigma^2)^2} = -\\frac{P}{\\sigma^4}$\n\nCombining these, we get:\n$$\n\\frac{d}{d\\sigma^{2}}I(X;Y) = \\left(\\frac{1}{2}\\text{mmse}_0(\\gamma)\\right) \\cdot \\left(-\\frac{P}{\\sigma^{4}}\\right) = -\\frac{1}{2} \\frac{P \\cdot \\text{mmse}_0(\\gamma)}{\\sigma^4}\n$$\nSince $P \\cdot \\text{mmse}_0(\\gamma) = \\text{mmse}(\\sigma^2)$, this simplifies to:\n$$\n\\frac{d}{d\\sigma^{2}}I(X;Y) = -\\frac{1}{2}\\frac{\\text{mmse}(\\sigma^{2})}{\\sigma^{4}}\n$$\nRearranging this equation gives us an expression for the integrand of $\\mathcal{J}$:\n$$\n\\frac{\\text{mmse}(\\sigma^{2})}{\\sigma^{4}}=-2\\,\\frac{d}{d\\sigma^{2}}I(X;Y)\n$$\nNow we can evaluate the integral $\\mathcal{J}$:\n$$\n\\mathcal{J}=\\int_{0}^{\\infty}\\frac{\\text{mmse}(\\sigma^{2})}{\\sigma^{4}}\\,d\\sigma^{2}\n= \\int_{0}^{\\infty} -2\\,\\frac{d}{d\\sigma^{2}}I(X;Y)\\,d\\sigma^{2}\n$$\nUsing the fundamental theorem of calculus:\n$$\n\\mathcal{J} = -2\\big[I(X;Y)\\big]_{\\sigma^{2}=0}^{\\sigma^{2}=\\infty} = -2\\left( \\lim_{\\sigma^2 \\to \\infty} I(X;Y) - \\lim_{\\sigma^2 \\to 0} I(X;Y) \\right)\n$$\nWe evaluate the limits:\n*   As $\\sigma^2 \\to \\infty$, the noise power is infinite (SNR $\\to 0$). The received signal $Y$ is independent of $X$, so the mutual information is zero: $I(X;Y) = 0$.\n*   As $\\sigma^2 \\to 0$, the channel is noiseless (SNR $\\to \\infty$). The received signal $Y$ perfectly reveals $X$, so the mutual information equals the entropy of the source: $I(X;Y) = H(X)$.\n\nSubstituting these limits back into the expression for $\\mathcal{J}$:\n$$\n\\mathcal{J} = -2(0 - H(X)) = 2H(X)\n$$\nThe value of the integral is twice the entropy of the source signal $X$, measured in nats.\n\nNow, we compute the entropy $H(X)$ for the given distribution:\n$$P(X) = \\left\\{\\frac{1}{8}, \\frac{3}{8}, \\frac{3}{8}, \\frac{1}{8}\\right\\}$$\nThe entropy formula is $H(X) = -\\sum_i p_i \\ln(p_i)$:\n$$\nH(X) = - \\left( 2 \\cdot \\frac{1}{8}\\ln\\frac{1}{8} + 2 \\cdot \\frac{3}{8}\\ln\\frac{3}{8} \\right)\n= -\\frac{1}{4}\\ln\\frac{1}{8} - \\frac{3}{4}\\ln\\frac{3}{8}\n$$\nUsing logarithm properties $\\ln(a/b) = \\ln a - \\ln b$ and $\\ln(1)=0$:\n$$\nH(X) = -\\frac{1}{4}(-\\ln 8) - \\frac{3}{4}(\\ln 3 - \\ln 8)\n= \\frac{1}{4}\\ln 8 - \\frac{3}{4}\\ln 3 + \\frac{3}{4}\\ln 8\n= \\ln 8 - \\frac{3}{4}\\ln 3\n$$\nSince $\\ln 8 = \\ln(2^3) = 3\\ln 2$:\n$$\nH(X) = 3\\ln 2 - \\frac{3}{4}\\ln 3\n$$\nFinally, we calculate $\\mathcal{J}$:\n$$\n\\mathcal{J}=2H(X)=2\\left(3\\ln 2 - \\frac{3}{4}\\ln 3\\right) = 6\\ln 2 - \\frac{3}{2}\\ln 3\n$$\nThis result is independent of the scaling factor $\\alpha$, as expected.", "answer": "$$\\boxed{6\\ln 2-\\frac{3}{2}\\ln 3}$$", "id": "1654320"}]}