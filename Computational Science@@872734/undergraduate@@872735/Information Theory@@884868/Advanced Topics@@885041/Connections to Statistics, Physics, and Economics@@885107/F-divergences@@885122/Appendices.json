{"hands_on_practices": [{"introduction": "The first step in mastering f-divergences is to become comfortable with the fundamental calculation. This exercise provides a direct, hands-on opportunity to apply the definition of $D_f(P || Q)$ to a tangible scenario involving two Bernoulli distributions. By working through this problem [@problem_id:1623955], you will solidify your understanding of how the generator function $f$ and the underlying probability distributions combine to produce a measure of their difference.", "problem": "In information theory, f-divergences provide a generalized framework for measuring the difference between two probability distributions. For two discrete probability distributions, $P$ and $Q$, defined on the same finite set of outcomes $\\mathcal{X}$, the f-divergence from $Q$ to $P$ is given by the formula:\n$$D_f(P || Q) = \\sum_{x \\in \\mathcal{X}} Q(x) f\\left(\\frac{P(x)}{Q(x)}\\right)$$\nwhere $f$ is a convex function such that $f(1) = 0$.\n\nConsider two independent random experiments, each of which can result in either \"success\" (outcome 1) or \"failure\" (outcome 0). The first experiment is described by a Bernoulli distribution $P_1$ with parameter $p_1$, and the second experiment is described by a Bernoulli distribution $P_2$ with parameter $p_2$. The probability mass function for a Bernoulli distribution with parameter $p$ is given by $P(x=1) = p$ and $P(x=0) = 1-p$. Assume that both $p_1$ and $p_2$ are strictly between 0 and 1, i.e., $p_1, p_2 \\in (0, 1)$.\n\nCalculate the specific f-divergence, $D_f(P_1 || P_2)$, between these two Bernoulli distributions using the function $f(u) = (u-1)^2$. Express your answer as a single, simplified algebraic expression in terms of $p_1$ and $p_2$.", "solution": "We use the definition of the f-divergence for discrete distributions:\n$$D_{f}(P || Q)=\\sum_{x\\in\\mathcal{X}}Q(x)\\,f\\!\\left(\\frac{P(x)}{Q(x)}\\right).$$\nHere, $P=P_{1}$ and $Q=P_{2}$ are Bernoulli distributions on $\\mathcal{X}=\\{0,1\\}$ with $P_{1}(1)=p_{1}$, $P_{1}(0)=1-p_{1}$, $P_{2}(1)=p_{2}$, and $P_{2}(0)=1-p_{2}$. The chosen function is $f(u)=(u-1)^{2}$, which is convex and satisfies $f(1)=0$.\n\nSubstituting into the definition gives\n$$D_{f}(P_{1} || P_{2})=P_{2}(0)\\,f\\!\\left(\\frac{P_{1}(0)}{P_{2}(0)}\\right)+P_{2}(1)\\,f\\!\\left(\\frac{P_{1}(1)}{P_{2}(1)}\\right).$$\nCompute each term:\n$$P_{2}(0)\\,f\\!\\left(\\frac{P_{1}(0)}{P_{2}(0)}\\right)=(1-p_{2})\\left(\\frac{1-p_{1}}{1-p_{2}}-1\\right)^{2}\n=(1-p_{2})\\left(\\frac{-p_{1}+p_{2}}{1-p_{2}}\\right)^{2}\n=\\frac{(p_{1}-p_{2})^{2}}{1-p_{2}},$$\n$$P_{2}(1)\\,f\\!\\left(\\frac{P_{1}(1)}{P_{2}(1)}\\right)=p_{2}\\left(\\frac{p_{1}}{p_{2}}-1\\right)^{2}\n=p_{2}\\left(\\frac{p_{1}-p_{2}}{p_{2}}\\right)^{2}\n=\\frac{(p_{1}-p_{2})^{2}}{p_{2}}.$$\nSumming these contributions yields\n$$D_{f}(P_{1} || P_{2})=\\frac{(p_{1}-p_{2})^{2}}{1-p_{2}}+\\frac{(p_{1}-p_{2})^{2}}{p_{2}}\n=(p_{1}-p_{2})^{2}\\left(\\frac{1}{p_{2}}+\\frac{1}{1-p_{2}}\\right)\n=\\frac{(p_{1}-p_{2})^{2}}{p_{2}(1-p_{2})}.$$\nSince $p_{2}\\in(0,1)$, the denominator is nonzero, and the expression is valid.", "answer": "$$\\boxed{\\frac{(p_{1}-p_{2})^{2}}{p_{2}(1-p_{2})}}$$", "id": "1623955"}, {"introduction": "Not all divergence measures are created equal; some possess important properties like symmetry, where $D_f(P || Q) = D_f(Q || P)$. This symmetry is not arbitrary but is entirely determined by a specific mathematical property of the generator function $f$. This practice [@problem_id:1623941] challenges you to move beyond simple calculation and test whether a given generator function satisfies the condition for symmetry, providing insight into the structural design of these measures.", "problem": "In information theory, the f-divergence is a family of measures that quantify the difference between two probability distributions. For a pair of probability distributions $P$ and $Q$ with respective probability density functions $p(x)$ and $q(x)$, the f-divergence of $P$ from $Q$ is defined as:\n$$ D_f(P || Q) = \\int q(x) f\\left(\\frac{p(x)}{q(x)}\\right) dx $$\nHere, $f: (0, \\infty) \\to \\mathbb{R}$ is a convex function satisfying the condition $f(1) = 0$.\n\nA divergence measure is said to be symmetric if $D_f(P || Q) = D_f(Q || P)$ for all possible choices of distributions $P$ and $Q$. This symmetry property holds if and only if the generator function $f$ satisfies the relation $f(u) = u f\\left(\\frac{1}{u}\\right)$ for all $u > 0$.\n\nConsider the function $f(u) = u^2 - u$. This function is convex and satisfies $f(1)=0$, making it a valid generator for an f-divergence. Your task is to determine whether the f-divergence generated by this specific function is symmetric.\n\nA. Yes, the divergence is symmetric because the symmetry condition is satisfied for all $u > 0$.\n\nB. No, the divergence is not symmetric because the symmetry condition is not satisfied.\n\nC. The symmetry of the divergence depends on the specific choice of distributions $P$ and $Q$.\n\nD. The divergence is symmetric only for the special case where $P=Q$.\n\nE. No, because the function $f(u) = u^2-u$ is not a valid generator function.", "solution": "The symmetry condition for an f-divergence is that $D_{f}(P || Q) = D_{f}(Q || P)$ for all distributions $P,Q$ if and only if the generator $f$ satisfies\n$$\nf(u) = u f\\left(\\frac{1}{u}\\right) \\quad \\text{for all } u>0.\n$$\nGiven $f(u) = u^{2} - u$, compute\n$$\nf\\left(\\frac{1}{u}\\right) = \\left(\\frac{1}{u}\\right)^{2} - \\left(\\frac{1}{u}\\right) = u^{-2} - u^{-1},\n$$\nso\n$$\nu f\\left(\\frac{1}{u}\\right) = u\\left(u^{-2} - u^{-1}\\right) = u^{-1} - 1.\n$$\nTo satisfy the symmetry condition, we would need\n$$\nu^{2} - u = u^{-1} - 1 \\quad \\text{for all } u>0.\n$$\nMultiply both sides by $u$ to clear the denominator:\n$$\nu^{3} - u^{2} = 1 - u,\n$$\nwhich rearranges to\n$$\nu^{3} - u^{2} + u - 1 = 0.\n$$\nFactor by grouping:\n$$\nu^{2}(u - 1) + (u - 1) = (u - 1)(u^{2} + 1) = 0.\n$$\nFor $u>0$, the only real solution is $u=1$. Therefore, the equality $f(u) = u f(1/u)$ holds only at $u=1$ and not for all $u>0$, so the symmetry condition fails. Hence the divergence generated by $f(u)=u^{2}-u$ is not symmetric in general. Note that $f$ is a valid generator since it is convex (with $f''(u)=2>0$) and $f(1)=0$, so the lack of symmetry is not due to invalidity of $f$.", "answer": "$$\\boxed{B}$$", "id": "1623941"}, {"introduction": "The Kullback-Leibler divergence is famous for its elegant chain rule, which allows for the decomposition of a joint divergence into marginal and conditional components. A natural question arises: is this a universal property of all f-divergences? This advanced exercise [@problem_id:1623987] guides you through constructing a concrete counterexample to investigate whether the chain rule holds for the Pearson $\\chi^2$-divergence, revealing important subtleties about the structure of information measures.", "problem": "In information theory, an f-divergence is a measure of the difference between two probability distributions. For discrete distributions $P$ and $Q$ over the same alphabet $\\mathcal{X}$, the f-divergence is defined as $D_f(P || Q) = \\sum_{x \\in \\mathcal{X}} Q(x) f\\left(\\frac{P(x)}{Q(x)}\\right)$, where $f$ is a convex function with $f(1)=0$.\n\nA special case is the Pearson $\\chi^2$-divergence, which corresponds to the choice of function $f(u) = (u-1)^2$. Thus, its formula is:\n$$ D_{\\chi^2}(P || Q) = \\sum_{x \\in \\mathcal{X}} \\frac{(P(x) - Q(x))^2}{Q(x)} $$\n\nFor the well-known Kullback-Leibler (KL) divergence, a chain rule for joint distributions holds:\n$$ D_{KL}(P(X,Y) || Q(X,Y)) = D_{KL}(P(X) || Q(X)) + \\mathbb{E}_{P(X)}[D_{KL}(P(Y|X) || Q(Y|X))] $$\nwhere $\\mathbb{E}_{P(X)}[\\cdot]$ denotes the expectation with respect to the marginal distribution $P(X)$.\n\nThis problem explores whether a similar chain rule applies to the Pearson $\\chi^2$-divergence. Consider two random variables, $X$ and $Y$, each taking values in the set $\\{0, 1\\}$. Two joint probability distributions, $P(X,Y)$ and $Q(X,Y)$, are defined as follows:\n\nFor distribution $P(X,Y)$:\n$P(X=0, Y=0) = \\frac{1}{2}$\n$P(X=0, Y=1) = \\frac{1}{4}$\n$P(X=1, Y=0) = \\frac{1}{8}$\n$P(X=1, Y=1) = \\frac{1}{8}$\n\nFor distribution $Q(X,Y)$:\n$Q(X=x, Y=y) = \\frac{1}{4}$ for all four possible outcomes $(x,y)$.\n\nLet us define the following three quantities:\n1. The joint divergence: $A = D_{\\chi^2}(P(X,Y) || Q(X,Y))$\n2. The marginal divergence: $B = D_{\\chi^2}(P(X) || Q(X))$\n3. The expected conditional divergence: $C = \\sum_{x \\in \\{0,1\\}} P(X=x) \\cdot D_{\\chi^2}(P(Y|X=x) || Q(Y|X=x))$\n\nCalculate the value of the quantity $A - (B + C)$. Express your answer as a fraction in simplest form.", "solution": "The goal is to calculate the value of $A - (B + C)$, which tests the validity of a chain rule for the Pearson $\\chi^2$-divergence. We will compute the quantities $A$, $B$, and $C$ in separate steps.\n\nFirst, we need to determine the marginal and conditional distributions from the given joint distributions $P(X,Y)$ and $Q(X,Y)$.\n\n**1. Derivation of Marginal and Conditional Distributions**\n\nFor distribution $P(X,Y)$:\nThe marginal distribution $P(X)$ is found by summing over $Y$:\n$P(X=0) = P(X=0, Y=0) + P(X=0, Y=1) = \\frac{1}{2} + \\frac{1}{4} = \\frac{3}{4}$\n$P(X=1) = P(X=1, Y=0) + P(X=1, Y=1) = \\frac{1}{8} + \\frac{1}{8} = \\frac{2}{8} = \\frac{1}{4}$\n\nThe conditional distributions $P(Y|X)$ are found using $P(Y|X) = P(X,Y)/P(X)$:\nFor $X=0$:\n$P(Y=0|X=0) = \\frac{P(X=0, Y=0)}{P(X=0)} = \\frac{1/2}{3/4} = \\frac{2}{3}$\n$P(Y=1|X=0) = \\frac{P(X=0, Y=1)}{P(X=0)} = \\frac{1/4}{3/4} = \\frac{1}{3}$\n\nFor $X=1$:\n$P(Y=0|X=1) = \\frac{P(X=1, Y=0)}{P(X=1)} = \\frac{1/8}{1/4} = \\frac{1}{2}$\n$P(Y=1|X=1) = \\frac{P(X=1, Y=1)}{P(X=1)} = \\frac{1/8}{1/4} = \\frac{1}{2}$\n\nFor distribution $Q(X,Y)$:\nThe marginal distribution $Q(X)$ is:\n$Q(X=0) = Q(X=0, Y=0) + Q(X=0, Y=1) = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2}$\n$Q(X=1) = Q(X=1, Y=0) + Q(X=1, Y=1) = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2}$\n\nThe conditional distributions $Q(Y|X)$ are:\nFor $X=0$:\n$Q(Y=0|X=0) = \\frac{Q(X=0, Y=0)}{Q(X=0)} = \\frac{1/4}{1/2} = \\frac{1}{2}$\n$Q(Y=1|X=0) = \\frac{Q(X=0, Y=1)}{Q(X=0)} = \\frac{1/4}{1/2} = \\frac{1}{2}$\n\nFor $X=1$:\n$Q(Y=0|X=1) = \\frac{Q(X=1, Y=0)}{Q(X=1)} = \\frac{1/4}{1/2} = \\frac{1}{2}$\n$Q(Y=1|X=1) = \\frac{Q(X=1, Y=1)}{Q(X=1)} = \\frac{1/4}{1/2} = \\frac{1}{2}$\n\n**2. Calculation of A: The Joint Divergence**\n$A = D_{\\chi^2}(P(X,Y) || Q(X,Y)) = \\sum_{x,y} \\frac{(P(x,y) - Q(x,y))^2}{Q(x,y)}$\n$A = \\frac{(P(0,0)-Q(0,0))^2}{Q(0,0)} + \\frac{(P(0,1)-Q(0,1))^2}{Q(0,1)} + \\frac{(P(1,0)-Q(1,0))^2}{Q(1,0)} + \\frac{(P(1,1)-Q(1,1))^2}{Q(1,1)}$\n$A = \\frac{(\\frac{1}{2} - \\frac{1}{4})^2}{\\frac{1}{4}} + \\frac{(\\frac{1}{4} - \\frac{1}{4})^2}{\\frac{1}{4}} + \\frac{(\\frac{1}{8} - \\frac{1}{4})^2}{\\frac{1}{4}} + \\frac{(\\frac{1}{8} - \\frac{1}{4})^2}{\\frac{1}{4}}$\n$A = \\frac{(\\frac{1}{4})^2}{\\frac{1}{4}} + 0 + \\frac{(-\\frac{1}{8})^2}{\\frac{1}{4}} + \\frac{(-\\frac{1}{8})^2}{\\frac{1}{4}}$\n$A = \\frac{1}{4} + \\frac{1/64}{1/4} + \\frac{1/64}{1/4} = \\frac{1}{4} + \\frac{1}{16} + \\frac{1}{16} = \\frac{1}{4} + \\frac{2}{16} = \\frac{1}{4} + \\frac{1}{8} = \\frac{3}{8}$\n\n**3. Calculation of B: The Marginal Divergence**\n$B = D_{\\chi^2}(P(X) || Q(X)) = \\sum_{x} \\frac{(P(x) - Q(x))^2}{Q(x)}$\n$B = \\frac{(P(X=0) - Q(X=0))^2}{Q(X=0)} + \\frac{(P(X=1) - Q(X=1))^2}{Q(X=1)}$\n$B = \\frac{(\\frac{3}{4} - \\frac{1}{2})^2}{\\frac{1}{2}} + \\frac{(\\frac{1}{4} - \\frac{1}{2})^2}{\\frac{1}{2}}$\n$B = \\frac{(\\frac{1}{4})^2}{\\frac{1}{2}} + \\frac{(-\\frac{1}{4})^2}{\\frac{1}{2}} = \\frac{1/16}{1/2} + \\frac{1/16}{1/2} = \\frac{1}{8} + \\frac{1}{8} = \\frac{2}{8} = \\frac{1}{4}$\n\n**4. Calculation of C: The Expected Conditional Divergence**\n$C = P(X=0) D_{\\chi^2}(P(Y|X=0) || Q(Y|X=0)) + P(X=1) D_{\\chi^2}(P(Y|X=1) || Q(Y|X=1))$\n\nFirst, calculate the conditional divergences:\nFor $X=0$:\n$D_{\\chi^2}(P(Y|X=0) || Q(Y|X=0)) = \\frac{(P(Y=0|X=0) - Q(Y=0|X=0))^2}{Q(Y=0|X=0)} + \\frac{(P(Y=1|X=0) - Q(Y=1|X=0))^2}{Q(Y=1|X=0)}$\n$= \\frac{(\\frac{2}{3} - \\frac{1}{2})^2}{\\frac{1}{2}} + \\frac{(\\frac{1}{3} - \\frac{1}{2})^2}{\\frac{1}{2}} = \\frac{(\\frac{1}{6})^2}{\\frac{1}{2}} + \\frac{(-\\frac{1}{6})^2}{\\frac{1}{2}}$\n$= \\frac{1/36}{1/2} + \\frac{1/36}{1/2} = \\frac{1}{18} + \\frac{1}{18} = \\frac{2}{18} = \\frac{1}{9}$\n\nFor $X=1$:\n$P(Y|X=1)$ and $Q(Y|X=1)$ are both the uniform distribution $\\{\\frac{1}{2}, \\frac{1}{2}\\}$. Since the distributions are identical, the divergence is 0.\n$D_{\\chi^2}(P(Y|X=1) || Q(Y|X=1)) = 0$\n\nNow, compute the expectation $C$:\n$C = P(X=0) \\cdot (\\frac{1}{9}) + P(X=1) \\cdot (0) = \\frac{3}{4} \\cdot \\frac{1}{9} + \\frac{1}{4} \\cdot 0 = \\frac{3}{36} = \\frac{1}{12}$\n\n**5. Final Calculation**\nWe need to compute $A - (B + C)$.\n$B + C = \\frac{1}{4} + \\frac{1}{12} = \\frac{3}{12} + \\frac{1}{12} = \\frac{4}{12} = \\frac{1}{3}$\n$A - (B + C) = \\frac{3}{8} - \\frac{1}{3} = \\frac{9}{24} - \\frac{8}{24} = \\frac{1}{24}$\nThe non-zero result demonstrates that the simple additive chain rule does not hold for the Pearson $\\chi^2$-divergence in this case.", "answer": "$$\\boxed{\\frac{1}{24}}$$", "id": "1623987"}]}