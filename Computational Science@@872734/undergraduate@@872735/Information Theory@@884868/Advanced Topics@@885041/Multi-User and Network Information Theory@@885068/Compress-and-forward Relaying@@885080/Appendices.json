{"hands_on_practices": [{"introduction": "The compress-and-forward strategy begins with the fundamental act of compression. To understand the value of the relay's contribution, we must first quantify the amount of information its compressed message contains about the original source signal. This introductory problem [@problem_id:1611908] models the simplest possible compression—quantizing a noisy observation into a single bit—and asks you to calculate the mutual information, revealing the tangible benefit of even this coarse representation.", "problem": "In a wireless communication system, a source node transmits a single bit $X$ to a destination node. The bit $X$ is chosen from the set $\\{-1, +1\\}$ with equal probability. An intermediate relay node assists the communication. The relay receives a noisy version of the source's signal, which it quantizes into a single bit $B$ using a simple sign function: $B=+1$ if the received signal is positive, and $B=-1$ if it is negative. This bit $B$ is then transmitted to the destination over an error-free link.\n\nDue to noise in the link between the source and the relay, the quantization process is not perfect. The probability that the relay's bit $B$ is the same as the original source bit $X$ is $0.9$. That is, $P(B=X) = 0.9$.\n\nAssuming this model, calculate the mutual information $I(X; B)$ between the source bit $X$ and the relay's bit $B$. Express your final answer in bits, rounded to three significant figures.", "solution": "Let $X \\in \\{-1,+1\\}$ with $P(X=+1)=P(X=-1)=\\frac{1}{2}$. The relay produces $B \\in \\{-1,+1\\}$ such that $P(B=X)=0.9$ and $P(B \\neq X)=0.1$. This defines a binary symmetric channel (BSC) with crossover probability $q=0.1$.\n\nThe mutual information between $X$ and $B$ is\n$$\nI(X;B)=H(B)-H(B|X).\n$$\nFirst, compute $H(B)$. The output $B$ is symmetric because the input is uniform and the channel is symmetric:\n$$\nP(B=+1)=P(B=+1|X=+1)P(X=+1)+P(B=+1|X=-1)P(X=-1)=0.9 \\cdot \\frac{1}{2}+0.1 \\cdot \\frac{1}{2}=\\frac{1}{2},\n$$\nand similarly $P(B=-1)=\\frac{1}{2}$. Hence\n$$\nH(B)=-\\sum_{b \\in \\{-1,+1\\}} P(B=b)\\log_{2} P(B=b)=-2 \\cdot \\frac{1}{2}\\log_{2}\\left(\\frac{1}{2}\\right)=1.\n$$\nNext, compute the conditional entropy. For a BSC with crossover probability $q=0.1$,\n$$\nH(B|X)=-q \\log_{2} q-(1-q)\\log_{2}(1-q)=-0.1\\log_{2}(0.1)-0.9\\log_{2}(0.9).\n$$\nTherefore,\n$$\nI(X;B)=1-\\left[-0.1\\log_{2}(0.1)-0.9\\log_{2}(0.9)\\right].\n$$\nEvaluating the logarithms,\n$$\n-0.1\\log_{2}(0.1)\\approx 0.3321928095,\\quad -0.9\\log_{2}(0.9)\\approx 0.1368027841,\n$$\nso\n$$\nH(B|X)\\approx 0.4689955936,\\quad I(X;B)\\approx 1-0.4689955936=0.5310044064.\n$$\nRounding to three significant figures yields $I(X;B)\\approx 0.531$ bits.", "answer": "$$\\boxed{0.531}$$", "id": "1611908"}, {"introduction": "A key element of compress-and-forward relaying is the intelligent fusion of information at the destination. This practice problem [@problem_id:1611890] moves beyond simple quantization to a more sophisticated \"soft-decision\" forwarding scheme, where the relay transmits a summary of its confidence about the source symbol. By calculating the combined Log-Likelihood Ratio (LLR), you will see firsthand how the destination leverages both its direct observation and the relay's compressed message to make a more reliable decision.", "problem": "Consider a wireless communication system comprising a source (S), a relay (R), and a destination (D). The source transmits a symbol $X$, chosen with equal probability from the set $\\{-1, +1\\}$. This transmission is received by both the relay and the destination.\n\nThe communication links are modeled as Additive White Gaussian Noise (AWGN) channels. The signal received at the relay is $Y_R = X + N_R$, and the signal received at the destination is $Y_D = X + N_D$. The noise terms $N_R$ and $N_D$ are statistically independent Gaussian random variables with zero mean and variances $\\sigma_R^2$ and $\\sigma_D^2$, respectively.\n\nThe relay implements a \"soft-decision forward\" strategy. It first computes the Log-Likelihood Ratio (LLR) of the source symbol $X$ based on its observation $Y_R$. It then performs a one-bit quantization of this LLR to generate a message $M \\in \\{-1, +1\\}$. The message is set to $M=+1$ if the relay's computed LLR is positive, and $M=-1$ if it is negative. This message $M$ is then transmitted to the destination over an error-free, dedicated channel.\n\nThe destination's goal is to make an optimal decision about the original symbol $X$ by combining its direct observation $Y_D$ with the message $M$ received from the relay. The optimal combination under the Maximum A Posteriori (MAP) criterion is based on the total LLR, defined as $L_{total} = \\log \\frac{P(Y_D=y_D, M | X=+1)}{P(Y_D=y_D, M | X=-1)}$, where $\\log$ denotes the natural logarithm.\n\nSuppose the noise variances are $\\sigma_R^2 = 2.0$ and $\\sigma_D^2 = 3.0$. The destination observes the value $y_D = -0.75$ and receives the message $M=+1$ from the relay.\n\nCalculate the numerical value of the total LLR, $L_{total}$, at the destination.\n\nFor your calculation, you may need the complementary cumulative distribution function (often called the Q-function) of the standard normal distribution, $Q(x) = P(Z > x)$ for $Z \\sim \\mathcal{N}(0,1)$. Round your final answer to four significant figures.", "solution": "We work with equiprobable $X \\in \\{-1,+1\\}$ and independent AWGN observations $Y_{R}=X+N_{R}$ and $Y_{D}=X+N_{D}$ with $N_{R} \\sim \\mathcal{N}(0,\\sigma_{R}^{2})$, $N_{D} \\sim \\mathcal{N}(0,\\sigma_{D}^{2})$. The relay computes the LLR of $X$ from $Y_{R}$:\n$$\nL_{R}(y_{R})=\\ln \\frac{p(y_{R}\\,|\\,X=+1)}{p(y_{R}\\,|\\,X=-1)}\n=\\frac{-(y_{R}-1)^{2}+(y_{R}+1)^{2}}{2\\sigma_{R}^{2}}=\\frac{2y_{R}}{\\sigma_{R}^{2}}.\n$$\nThus $\\operatorname{sign}(L_{R})=\\operatorname{sign}(y_{R})$, and the relay’s one-bit message is $M=+1$ if and only if $Y_{R}0$.\n\nAt the destination, by conditional independence of $Y_{D}$ and $M$ given $X$,\n$$\nL_{\\text{total}}=\\ln \\frac{p(Y_{D}=y_{D}\\,|\\,X=+1)}{p(Y_{D}=y_{D}\\,|\\,X=-1)}+\\ln \\frac{\\Pr(M\\,|\\,X=+1)}{\\Pr(M\\,|\\,X=-1)}.\n$$\nThe first term is the standard AWGN LLR:\n$$\n\\ln \\frac{p(Y_{D}=y_{D}\\,|\\,X=+1)}{p(Y_{D}=y_{D}\\,|\\,X=-1)}\n=\\frac{-(y_{D}-1)^{2}+(y_{D}+1)^{2}}{2\\sigma_{D}^{2}}=\\frac{2y_{D}}{\\sigma_{D}^{2}}.\n$$\nFor the second term with the observed $M=+1$,\n$$\n\\Pr(M=+1\\,|\\,X=+1)=\\Pr(Y_{R}0\\,|\\,X=+1)=\\Pr\\!\\left(Z \\frac{0-1}{\\sigma_{R}}\\right)=\\Phi\\!\\left(\\frac{1}{\\sigma_{R}}\\right)=1-Q\\!\\left(\\frac{1}{\\sigma_{R}}\\right),\n$$\n$$\n\\Pr(M=+1\\,|\\,X=-1)=\\Pr(Y_{R}0\\,|\\,X=-1)=\\Pr\\!\\left(Z \\frac{0-(-1)}{\\sigma_{R}}\\right)=Q\\!\\left(\\frac{1}{\\sigma_{R}}\\right),\n$$\nwhere $Z \\sim \\mathcal{N}(0,1)$, $\\Phi$ is its CDF, and $Q(x)=\\Pr(Zx)$. Therefore,\n$$\nL_{\\text{total}}=\\frac{2y_{D}}{\\sigma_{D}^{2}}+\\ln \\left(\\frac{1-Q\\!\\left(\\frac{1}{\\sigma_{R}}\\right)}{Q\\!\\left(\\frac{1}{\\sigma_{R}}\\right)}\\right).\n$$\n\nNow substitute the given values $\\sigma_{R}^{2}=2$, $\\sigma_{D}^{2}=3$, $y_{D}=-0.75$, and $M=+1$:\n$$\n\\frac{2y_{D}}{\\sigma_{D}^{2}}=\\frac{2(-0.75)}{3}=-0.5,\n\\qquad\na=\\frac{1}{\\sigma_{R}}=\\frac{1}{\\sqrt{2}}.\n$$\nUsing $Q(a)=Q\\!\\left(\\frac{1}{\\sqrt{2}}\\right)\\approx 0.239758$ (so $1-Q(a)\\approx 0.760242$),\n$$\n\\ln \\left(\\frac{1-Q(a)}{Q(a)}\\right)=\\ln \\left(\\frac{0.760242}{0.239758}\\right)\\approx \\ln(3.17083)\\approx 1.15395.\n$$\nHence,\n$$\nL_{\\text{total}}\\approx -0.5+1.15395=0.65395,\n$$\nwhich to four significant figures is $0.6540$.", "answer": "$$\\boxed{0.6540}$$", "id": "1611890"}, {"introduction": "Designing an effective compress-and-forward system involves optimizing the relay's compression strategy to minimize end-to-end distortion, often under constraints and uncertainty. This advanced exercise [@problem_id:1611914] places you in the role of a system designer, tasked with finding the optimal compression parameters for a relay that is unaware of the channel state at the destination. By working through this rate-distortion framework, you will gain insight into the theoretical principles that govern the performance of sophisticated relaying protocols.", "problem": "A relay node in a communication system observes a zero-mean Gaussian process $Y_R$ with variance $\\sigma_Y^2$. The relay's function is to compress this observation and transmit it to a destination over a digital link with a capacity of $C$ bits per sample.\n\nThe system's overall performance is judged at the destination based on a state-dependent distortion metric given by $d(y_R, \\hat{y}_R, S) = (y_R - k(S)\\hat{y}_R)^2$, where $\\hat{y}_R$ is the reconstructed signal value. The state $S$ is a discrete random variable, unknown to the relay, which can be in one of two conditions: $S=s_1$ with probability $p$, or $S=s_2$ with probability $1-p$. The corresponding scaling factors in the distortion function are $k(s_1) = k_1$ and $k(s_2) = k_2$.\n\nThe relay must implement a single compression strategy (independent of $S$) that minimizes the expected distortion, where the expectation is taken over the process $Y_R$ and the state $S$. For this class of problems, it is known that the optimal compression can be modeled by an effective Gaussian test channel of the form $\\hat{Y}_R = g Y_R + N_q$, where $g$ is a gain parameter and $N_q$ is a zero-mean Gaussian quantization noise with variance $\\sigma_q^2$, independent of $Y_R$. To ensure the most faithful representation for the given structure, the relay's encoder is designed to operate at the channel capacity limit, i.e., the rate of information transmitted, $I(Y_R; \\hat{Y}_R)$, is exactly equal to $C$.\n\nYour task is to determine the optimal gain parameter, $g_{opt}$, that the relay must use. Express your answer as an analytic expression in terms of $p$, $k_1$, $k_2$, and $C$.", "solution": "Let $Y \\triangleq Y_{R} \\sim \\mathcal{N}(0,\\sigma_{Y}^{2})$ and consider the Gaussian test channel $\\hat{Y} = g Y + N_{q}$ with $N_{q} \\sim \\mathcal{N}(0,\\sigma_{q}^{2})$ independent of $Y$. For a given state $S=s_{i}$ with scaling $k_{i}$, the mean squared distortion is\n$$\n\\mathbb{E}\\big[(Y - k_{i}\\hat{Y})^{2}\\big]\n= \\mathbb{E}\\big[((1 - k_{i}g)Y - k_{i}N_{q})^{2}\\big]\n= (1 - k_{i}g)^{2}\\sigma_{Y}^{2} + k_{i}^{2}\\sigma_{q}^{2}.\n$$\nAveraging over the state $S$ with $\\mathbb{P}[S=s_{1}]=p$ and $\\mathbb{P}[S=s_{2}]=1-p$, the expected distortion becomes\n$$\nD(g) = p\\big[(1 - k_{1}g)^{2}\\sigma_{Y}^{2} + k_{1}^{2}\\sigma_{q}^{2}\\big] + (1-p)\\big[(1 - k_{2}g)^{2}\\sigma_{Y}^{2} + k_{2}^{2}\\sigma_{q}^{2}\\big].\n$$\nDefine $K \\triangleq p k_{1} + (1-p)k_{2}$ and $B \\triangleq p k_{1}^{2} + (1-p)k_{2}^{2}$. Then\n$$\nD(g) = \\sigma_{Y}^{2}\\big[1 - 2Kg + Bg^{2}\\big] + B\\sigma_{q}^{2}.\n$$\nThe rate constraint at capacity for the Gaussian test channel is\n$$\nI(Y;\\hat{Y}) = \\frac{1}{2}\\log_{2}\\!\\left(1 + \\frac{g^{2}\\sigma_{Y}^{2}}{\\sigma_{q}^{2}}\\right) = C,\n$$\nwhich implies\n$$\n1 + \\frac{g^{2}\\sigma_{Y}^{2}}{\\sigma_{q}^{2}} = 2^{2C}\n\\quad\\Longrightarrow\\quad\n\\sigma_{q}^{2} = \\frac{g^{2}\\sigma_{Y}^{2}}{2^{2C} - 1}.\n$$\nSubstituting $\\sigma_{q}^{2}$ into $D(g)$ gives\n$$\nD(g) = \\sigma_{Y}^{2}\\left[1 - 2Kg + Bg^{2} + \\frac{B}{2^{2C} - 1}g^{2}\\right]\n= \\sigma_{Y}^{2}\\left[1 - 2Kg + \\frac{2^{2C}}{2^{2C} - 1}Bg^{2}\\right].\n$$\nThis is a convex quadratic in $g$, so the minimizer satisfies\n$$\n\\frac{\\mathrm{d}D}{\\mathrm{d}g} = \\sigma_{Y}^{2}\\left[-2K + 2\\frac{2^{2C}}{2^{2C} - 1}Bg\\right] = 0,\n$$\nwhich yields\n$$\ng_{\\mathrm{opt}} = \\frac{K}{B}\\cdot\\frac{2^{2C} - 1}{2^{2C}} = \\frac{K}{B}\\left(1 - 2^{-2C}\\right).\n$$\nRestoring $K$ and $B$ in terms of $p,k_{1},k_{2}$,\n$$\ng_{\\mathrm{opt}} = \\frac{p k_{1} + (1-p)k_{2}}{p k_{1}^{2} + (1-p)k_{2}^{2}}\\left(1 - 2^{-2C}\\right).\n$$\nThis expression is valid whenever $p k_{1}^{2} + (1-p)k_{2}^{2}  0$. In the degenerate case $k_{1}=k_{2}=0$, the distortion is independent of $g$.", "answer": "$$\\boxed{\\frac{p k_{1} + (1 - p) k_{2}}{p k_{1}^{2} + (1 - p) k_{2}^{2}}\\left(1 - 2^{-2C}\\right)}$$", "id": "1611914"}]}