{"hands_on_practices": [{"introduction": "The Multiple-Access Channel (MAC), where several transmitters communicate with a single receiver, is a cornerstone of network information theory. This first exercise [@problem_id:1642871] introduces a practical complication: a channel whose availability fluctuates over time. By analyzing a MAC that randomly switches \"on\" and \"off,\" you will explore how prior knowledge of the channel's state allows transmitters to adapt their strategies and learn how the overall capacity is intuitively linked to the channel's uptime.", "problem": "Consider a communication system involving two users, User 1 and User 2, transmitting information to a single receiver. This can be modeled as a two-user Multiple Access Channel (MAC). The inputs from the users are binary, represented by random variables $X_1 \\in \\{0, 1\\}$ and $X_2 \\in \\{0, 1\\}$.\n\nThe transmission is affected by a stateful environment. The channel's state is described by a random variable $S \\in \\{0, 1\\}$, which follows an independent and identically distributed (i.i.d.) Bernoulli process. The probability that the channel is in state $S=1$ is $p$, i.e., $P(S=1)=p$. The state $S$ for each channel use is known non-causally to both transmitters and the receiver.\n\nThe output of the channel, $Y$, depends on the inputs and the state $S$ as follows:\n- If $S=1$, the channel behaves as a binary adder, where the output is the sum modulo 2 (XOR operation) of the inputs: $Y = X_1 \\oplus X_2$.\n- If $S=0$, the channel is inactive, and the output is always zero, i.e., $Y=0$, regardless of the inputs $X_1$ and $X_2$.\n\nYour task is to calculate the sum-rate capacity of this MAC, which is the maximum achievable value of the sum of the individual rates $(R_1 + R_2)$. Express your answer as an analytic expression in terms of the parameter $p$. All rates are measured in bits per channel use.", "solution": "Because the state $S$ is known non-causally to both transmitters and the receiver, the capacity region of the state-dependent two-user MAC is given by all rate pairs $(R_{1},R_{2})$ such that, for some conditional input distributions $p(x_{1}\\mid s)$ and $p(x_{2}\\mid s)$ with independent encoders conditioned on $S$,\n$$\nR_{1} \\leq I(X_{1};Y \\mid X_{2}, S),\\quad\nR_{2} \\leq I(X_{2};Y \\mid X_{1}, S),\\quad\nR_{1}+R_{2} \\leq I(X_{1},X_{2};Y \\mid S).\n$$\nTherefore, the sum-rate capacity is\n$$\nC_{\\text{sum}}=\\max_{p(x_{1}\\mid s)p(x_{2}\\mid s)} I(X_{1},X_{2};Y \\mid S).\n$$\n\nFor the given channel, conditioned on $S$, the channel is deterministic:\n- If $S=1$, then $Y=X_{1}\\oplus X_{2}$ deterministically.\n- If $S=0$, then $Y=0$ deterministically.\n\nHence, for any $p(x_{1}\\mid s)p(x_{2}\\mid s)$,\n$$\nI(X_{1},X_{2};Y \\mid S)=H(Y \\mid S)-H(Y \\mid X_{1},X_{2},S).\n$$\nBecause $Y$ is a deterministic function of $(X_{1},X_{2},S)$, we have\n$$\nH(Y \\mid X_{1},X_{2},S)=0,\n$$\nso\n$$\nI(X_{1},X_{2};Y \\mid S)=H(Y \\mid S).\n$$\nAveraging over $S$,\n$$\nH(Y \\mid S)=P(S=1)\\,H(Y \\mid S=1)+P(S=0)\\,H(Y \\mid S=0)=p\\,H(Y \\mid S=1)+(1-p)\\cdot 0.\n$$\nWhen $S=1$, $Y=X_{1}\\oplus X_{2}$. If $X_{1}$ and $X_{2}$ are chosen independently given $S=1$ with $X_{1}\\sim \\text{Bernoulli}\\left(\\frac{1}{2}\\right)$ (and any choice for $X_{2}$, e.g., $X_{2}\\sim \\text{Bernoulli}\\left(\\frac{1}{2}\\right)$), then $Y$ is $\\text{Bernoulli}\\left(\\frac{1}{2}\\right)$, which maximizes the entropy:\n$$\nH(Y \\mid S=1)=1.\n$$\nTherefore, the maximal conditional entropy is\n$$\nH(Y \\mid S)=p \\cdot 1=p,\n$$\nand hence\n$$\nC_{\\text{sum}}=\\max_{p(x_{1}\\mid s)p(x_{2}\\mid s)} I(X_{1},X_{2};Y \\mid S)=p.\n$$\nThis value is achievable by using only the channel uses with $S=1$ and choosing $X_{1}\\mid S=1 \\sim \\text{Bernoulli}\\left(\\frac{1}{2}\\right)$ (and, for instance, $X_{2}\\mid S=1 \\sim \\text{Bernoulli}\\left(\\frac{1}{2}\\right)$), which makes $Y$ uniform when active. The inactive times $S=0$ carry no information since $Y$ is constant, so they do not contribute to the sum rate.\n\nHence the sum-rate capacity, in bits per channel use, equals $p$.", "answer": "$$\\boxed{p}$$", "id": "1642871"}, {"introduction": "Moving from the many-to-one MAC, we now consider its dual: the one-to-many Broadcast Channel (BC), where a single transmitter sends information to multiple receivers. This problem [@problem_id:1642881] models this scenario using the intuitive Binary Erasure Channel to investigate the fundamental limits of sending a common message to all users. This practice will help you understand how the system's performance is dictated by the need for universal reliability, revealing that the achievable rate is constrained by the receiver with the weakest link.", "problem": "A deep-space satellite is tasked with broadcasting a critical software patch to two identical, independent ground stations, designated Station A and Station B. The transmission occurs as a sequence of binary digits (bits). Due to unpredictable atmospheric interference, each bit transmitted from the satellite to a ground station is subject to potential loss.\n\nThe communication channel between the satellite and each ground station can be modeled as follows:\n1.  With a probability of $1-\\epsilon$, a transmitted bit is received correctly by the station.\n2.  With a probability of $\\epsilon$, the bit is erased during transmission. In this case, the station's receiver detects that a bit was sent but has no information about its value (0 or 1).\n\nThis process occurs independently for each bit and for each station. The channel described is sometimes referred to as a Binary Erasure Channel (BEC). The primary mission objective is to ensure that the entire software patch can be reconstructed with an arbitrarily low probability of error at *both* Station A and Station B, using the same transmission from the satellite.\n\nDetermine the maximum possible rate of transmission (in bits per channel use) for this common message. Your answer should be an expression in terms of the erasure probability $\\epsilon$.", "solution": "Let $M$ be the common message, uniformly distributed over $\\{1,\\ldots,2^{nR}\\}$, and let $X^{n}(M)$ be the codeword of length $n$ sent by the satellite. Each station $i \\in \\{A,B\\}$ observes $Y_{i}^{n}$ through an independent memoryless binary erasure channel (BEC) with erasure probability $\\epsilon$, used $n$ times. The reliability requirement is that the probability of error at both stations tends to zero, which implies the individual error probabilities at each station tend to zero as $n \\to \\infty$.\n\nConverse (outer bound on the rate):\nFor each receiver $i \\in \\{A,B\\}$, by Fano's inequality,\n$$\nnR \\leq I(M;Y_{i}^{n}) + n\\delta_{n},\n$$\nwhere $\\delta_{n} \\to 0$ as $n \\to \\infty$. By the data-processing inequality $M \\to X^{n} \\to Y_{i}^{n}$,\n$$\nI(M;Y_{i}^{n}) \\leq I(X^{n};Y_{i}^{n}).\n$$\nUsing the chain rule and memorylessness,\n$$\nI(X^{n};Y_{i}^{n}) \\leq \\sum_{t=1}^{n} I(X_{t};Y_{i,t}).\n$$\nCombining and dividing by $n$,\n$$\nR \\leq \\frac{1}{n} \\sum_{t=1}^{n} I(X_{t};Y_{i,t}) + \\delta_{n}.\n$$\nTaking the minimum over $i \\in \\{A,B\\}$,\n$$\nR \\leq \\min_{i \\in \\{A,B\\}} \\frac{1}{n} \\sum_{t=1}^{n} I(X_{t};Y_{i,t}) + \\delta_{n}.\n$$\nUsing $\\min_{i} \\frac{1}{n} \\sum_{t} a_{i,t} \\geq \\frac{1}{n} \\sum_{t} \\min_{i} a_{i,t}$,\n$$\nR \\leq \\frac{1}{n} \\sum_{t=1}^{n} \\min_{i \\in \\{A,B\\}} I(X_{t};Y_{i,t}) + \\delta_{n}.\n$$\nEach term satisfies\n$$\n\\min_{i \\in \\{A,B\\}} I(X_{t};Y_{i,t}) \\leq \\max_{p(x)} \\min_{i \\in \\{A,B\\}} I(X;Y_{i}),\n$$\nso\n$$\nR \\leq \\max_{p(x)} \\min_{i \\in \\{A,B\\}} I(X;Y_{i}) + \\delta_{n}.\n$$\nLetting $n \\to \\infty$ yields the common-message capacity\n$$\nC_{\\text{common}} = \\max_{p(x)} \\min\\{I(X;Y_{A}), I(X;Y_{B})\\}.\n$$\n\nSince both channels are identical BECs with erasure probability $\\epsilon$, $I(X;Y_{A}) = I(X;Y_{B}) = I(X;Y)$, so\n$$\nC_{\\text{common}} = \\max_{p(x)} I(X;Y).\n$$\n\nSingle-receiver mutual information for the BEC:\nLet $X \\in \\{0,1\\}$ with distribution $p(x)$ and $Y \\in \\{0,1,?\\}$ be the BEC output with erasure probability $\\epsilon$:\n$$\n\\Pr(Y=? \\mid X) = \\epsilon, \\quad \\Pr(Y=X \\mid X) = 1-\\epsilon.\n$$\nThen\n$$\nI(X;Y) = H(X) - H(X \\mid Y).\n$$\nCompute\n$$\nH(X \\mid Y) = \\Pr(Y=?) H(X \\mid Y=?) + \\Pr(Y \\neq ?) H(X \\mid Y \\neq ?).\n$$\nGiven $Y \\neq ?$, we observe $Y=X$, so $H(X \\mid Y \\neq ?) = 0$. Given $Y=?$, $X$ is unrevealed, so $H(X \\mid Y=?) = H(X)$. Hence\n$$\nH(X \\mid Y) = \\epsilon H(X) + (1-\\epsilon)\\cdot 0 = \\epsilon H(X),\n$$\nand thus\n$$\nI(X;Y) = H(X) - \\epsilon H(X) = (1-\\epsilon) H(X).\n$$\nMaximizing over $p(x)$ gives $H(X) \\leq 1$ with equality at the uniform input, so\n$$\n\\max_{p(x)} I(X;Y) = 1 - \\epsilon.\n$$\n\nTherefore,\n$$\nC_{\\text{common}} = 1 - \\epsilon.\n$$\n\nAchievability:\nChoose the uniform input distribution and a sequence of codes achieving the BEC capacity $1-\\epsilon$ for a single receiver. Each station then decodes with probability of error tending to zero. By the union bound, the probability that at least one station errs also tends to zero. Hence the rate $1-\\epsilon$ is achievable for the common message, matching the converse.\n\nThus, the maximum possible transmission rate to ensure reliable reconstruction at both stations is $1 - \\epsilon$ bits per channel use.", "answer": "$$\\boxed{1-\\epsilon}$$", "id": "1642881"}, {"introduction": "Our final practice problem moves beyond the basic MAC and BC models to explore a larger, structured network with a more sophisticated goal. Instead of simply relaying data, the nodes in this network must cooperate to compute a specific function of their collective observations, a core task in sensor networks and distributed systems. This challenging exercise [@problem_id:1642885] will guide you in applying the powerful cut-set bound to find the minimum required link capacity and in designing an elegant, efficient coding scheme to achieve this fundamental limit.", "problem": "A large-scale environmental monitoring system is architected as a perfect binary tree. At each of the $N=2^d$ leaves of the tree, for an integer depth $d  2$, there is a sensor $S_k$ for $k \\in \\{1, 2, \\dots, N\\}$. In each observation cycle, sensor $S_k$ measures a binary environmental state $X_k \\in \\{0, 1\\}$. The states $\\{X_k\\}_{k=1}^N$ are modeled as independent and identically distributed random variables, with $P(X_k=1) = P(X_k=0) = 1/2$.\n\nThe sensors are connected upwards through a hierarchy of nodes, culminating in a single data fusion center at the root of the tree. Communication between any child node and its parent node occurs over a dedicated noiseless digital link. All links in the network have the same uniform capacity of $C$ bits per observation cycle. Intermediate nodes in the tree can perform arbitrary computations on the messages they receive from their children before sending a new message to their own parent.\n\nThe fusion center must be able to losslessly recover two distinct pieces of information at the end of each cycle:\n1. The state of the first sensor, $X_1$.\n2. The global parity of all sensor states, defined as $S_{global} = \\bigoplus_{k=1}^N X_k = (X_1 + X_2 + \\dots + X_N) \\pmod 2$.\n\nDetermine the minimum integer value of the link capacity $C$ (in bits per observation cycle) that is required for the fusion center to accomplish this task.", "solution": "We model the required outputs as the function $F = (X_{1}, S_{global})$, where $S_{global} = \\bigoplus_{k=1}^{N} X_{k}$. For any edge $e$ of the tree, removing it partitions the leaves into disjoint sets $A$ and $A^{c}$. Since the only way information from $A$ can reach the root is across $e$, an information cut-set bound implies that the rate across $e$ must satisfy\n$$\nC \\geq H\\big(F \\mid X_{A^{c}}\\big).\n$$\nWe evaluate $H\\big(F \\mid X_{A^{c}}\\big)$ by cases.\n\nIf $1 \\notin A$, then $X_{1}$ is determined by $X_{A^{c}}$, and\n$$\nH\\big(F \\mid X_{A^{c}}\\big) = H\\big(S_{global} \\mid X_{A^{c}}\\big) = H\\Big(\\bigoplus_{k \\in A} X_{k} \\,\\Big|\\, X_{A^{c}}\\Big) = 1,\n$$\nbecause the parity of any nonempty subset of independent fair bits is a single fair bit independent of $X_{A^{c}}$.\n\nIf $1 \\in A$, write $Y = \\bigoplus_{k \\in A \\setminus \\{1\\}} X_{k}$. Given $X_{A^{c}}$, the pair $(X_{1}, S_{global})$ is equivalent to $(X_{1}, X_{1} \\oplus Y)$, so\n$$\nH\\big(F \\mid X_{A^{c}}\\big) = H\\big(X_{1}, X_{1} \\oplus Y\\big).\n$$\nIf $|A| = 1$, then $Y = 0$ and $(X_{1}, X_{1} \\oplus Y) = (X_{1}, X_{1})$, hence $H\\big(F \\mid X_{A^{c}}\\big) = H(X_{1}) = 1$. If $|A| \\geq 2$, then $Y$ is a fair bit independent of $X_{1}$, so the mapping $(X_{1}, Y) \\mapsto (X_{1}, X_{1} \\oplus Y)$ is bijective and\n$$\nH\\big(F \\mid X_{A^{c}}\\big) = H(X_{1}, Y) = 2.\n$$\nIn the perfect binary tree with depth $d  2$, there exists at least one edge for which $A$ contains $1$ and $|A| \\geq 2$ (for example, the edge immediately above the parent of leaf $1$ and all higher edges on that path). Therefore, the cut-set lower bound gives $C \\geq 2$.\n\nTo show achievability with $C = 2$, define for each subtree $T$ the two-bit summary\n$$\nB_{1}(T) = \\bigoplus_{k \\in T} X_{k}, \\quad B_{2}(T) = \\begin{cases}\nX_{1},  \\text{if } 1 \\in T,\\\\\n0,  \\text{if } 1 \\notin T.\n\\end{cases}\n$$\nAt a leaf $k$, send $(B_{1}, B_{2}) = (X_{k}, 0)$ if $k \\neq 1$ and $(B_{1}, B_{2}) = (X_{1}, X_{1})$ if $k = 1$. Each internal node with children subtrees $L$ and $R$ computes\n$$\nB_{1}(T) = B_{1}(L) \\oplus B_{1}(R), \\quad B_{2}(T) = B_{2}(L) \\oplus B_{2}(R),\n$$\nand forwards the two-bit vector $(B_{1}(T), B_{2}(T))$ to its parent. At the root, $(B_{1}, B_{2}) = \\big(\\bigoplus_{k=1}^{N} X_{k}, X_{1}\\big) = (S_{global}, X_{1})$, exactly the required outputs. Every edge carries exactly two bits, so $C = 2$ suffices.\n\nCombining the converse and achievability, the minimum integer capacity is $C = 2$.", "answer": "$$\\boxed{2}$$", "id": "1642885"}]}