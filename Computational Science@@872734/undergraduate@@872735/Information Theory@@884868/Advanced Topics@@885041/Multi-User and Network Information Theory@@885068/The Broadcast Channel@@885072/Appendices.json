{"hands_on_practices": [{"introduction": "Understanding the fundamental limits of a communication system is a central goal of information theory. For a broadcast channel, this limit is not a single number but a whole region of achievable rate pairs. This first exercise provides a gentle introduction to this concept by asking you to determine the capacity region for a simple, deterministic broadcast channel, helping you build intuition before we tackle more complex, noisy scenarios. [@problem_id:1662906]", "problem": "Consider a deterministic Broadcast Channel (BC) where a transmitter sends a symbol $x$ from the input alphabet $\\mathcal{X} = \\{1, 2, 3, 4\\}$ to two separate receivers.\n\nThe first receiver observes an output $y_1$ given by the function $y_1 = f_1(x) = x \\pmod 2$. The output alphabet for this receiver is $\\mathcal{Y}_1 = \\{0, 1\\}$.\n\nThe second receiver observes an output $y_2$ given by the function $y_2 = f_2(x) = \\lfloor x/3 \\rfloor$. The output alphabet for this receiver is $\\mathcal{Y}_2 = \\{0, 1\\}$.\n\nThe set of all achievable communication rate pairs $(R_1, R_2)$, where $R_1$ is the rate to the first receiver and $R_2$ is the rate to the second, forms a convex region in the $R_1-R_2$ plane. This is known as the capacity region of the broadcast channel. All rates are measured in bits per channel use.\n\nWhich of the following options correctly describes the capacity region for this channel?\n\nA. The triangular region defined by $R_1 \\ge 0, R_2 \\ge 0$, and $R_1 + R_2 \\le 1$.\n\nB. The square region defined by $0 \\le R_1 \\le 1$ and $0 \\le R_2 \\le 1$.\n\nC. The region defined by $R_1 \\le 1, R_2 \\le 1$, and $R_1 + R_2 \\le \\log_2{3}$.\n\nD. The triangular region defined by $R_1 \\ge 0, R_2 \\ge 0$, and $R_1 + R_2 \\le 2$.\n\nE. A line segment connecting the point $(1, 0)$ to $(0, 1)$.", "solution": "We enumerate the channel mappings. For $x \\in \\{1,2,3,4\\}$,\n$$\ny_{1}=f_{1}(x)=x \\bmod 2,\\quad y_{2}=f_{2}(x)=\\lfloor x/3 \\rfloor.\n$$\nThus\n$$\nx=1 \\Rightarrow (y_{1},y_{2})=(1,0),\\quad\nx=2 \\Rightarrow (y_{1},y_{2})=(0,0),\\quad\nx=3 \\Rightarrow (y_{1},y_{2})=(1,1),\\quad\nx=4 \\Rightarrow (y_{1},y_{2})=(0,1).\n$$\nTherefore, the mapping $x \\mapsto (y_{1},y_{2})$ is one-to-one onto $\\{0,1\\}\\times\\{0,1\\}$.\n\nOuter bounds: For a deterministic channel $Y_{i}=f_{i}(X)$, we have $H(Y_{i}\\mid X)=0$, hence for any input distribution $P_{X}$, $I(X;Y_{i})=H(Y_{i})-H(Y_{i}\\mid X)=H(Y_{i}) \\le \\log_{2}|\\mathcal{Y}_{i}|=1$.\nThus any achievable rate must satisfy $R_{1} \\le 1$ and $R_{2} \\le 1$.\n\nSum-rate consideration: Since $(Y_{1},Y_{2})$ is a deterministic function of $X$ and the mapping is bijective, for any $P_{X}$, $H(Y_{1},Y_{2})=H(X)$.\nChoosing $P_{X}$ uniform on $\\{1,2,3,4\\}$ gives $H(X)=\\log_{2}4=2$, hence the usual deterministic-BC sum-rate bound $R_{1}+R_{2} \\le H(Y_{1},Y_{2})$ becomes $R_{1}+R_{2} \\le 2$, which is redundant given $R_{1} \\le 1$ and $R_{2} \\le 1$.\n\nAchievability: Because for each pair $(b_{1},b_{2}) \\in \\{0,1\\}^{2}$ there exists a unique $x$ with $f_{1}(x)=b_{1}$ and $f_{2}(x)=b_{2}$, the transmitter can, at each channel use, map the two intended bits $(b_{1},b_{2})$ to the corresponding $x$. Then receiver $1$ observes $y_{1}=b_{1}$ and receiver $2$ observes $y_{2}=b_{2}$ with zero error. Over $n$ channel uses this yields $R_{1}=1$ and $R_{2}=1$, and by standard time-sharing any pair with $0 \\le R_{1} \\le 1$ and $0 \\le R_{2} \\le 1$ is achievable.\n\nTherefore, the capacity region is exactly the square\n$$\n0 \\le R_{1} \\le 1,\\quad 0 \\le R_{2} \\le 1,\n$$\nwhich corresponds to option B.", "answer": "$$\\boxed{B}$$", "id": "1662906"}, {"introduction": "In the real world, not all receivers are created equal; some might have a clearer signal than others. This leads to the important concept of a 'degraded' broadcast channel, a model that captures this hierarchy in signal quality. This problem asks you to verify the mathematical condition—a Markov chain—that formally defines a physically degraded channel, a crucial step in understanding the specific coding strategies that work best for these common scenarios. [@problem_id:1662955]", "problem": "In a simplified model of a two-stage signal processing cascade, an initial discrete signal, represented by the random variable $X$, is transmitted. The signal passes through the first processing stage, which adds noise. The output of this stage is the random variable $Y_1$, defined by the relation $Y_1 = X + Z_1$. This intermediate signal $Y_1$ is then fed into a second processing stage, which introduces additional noise. The final output signal is the random variable $Y_2$, defined by the relation $Y_2 = Y_1 + Z_2$. The initial signal $X$ and the noise terms $Z_1$ and $Z_2$ are all mutually independent random variables.\n\nWhich of the following statements accurately describes the statistical relationship among the random variables $X$, $Y_1$, and $Y_2$?\n\nA. $X \\to Y_2 \\to Y_1$ forms a Markov chain.\n\nB. $X, Y_1$, and $Y_2$ are mutually independent.\n\nC. The variables are related by the Markov chain $Y_1 \\to X \\to Y_2$.\n\nD. $X$ and $Y_2$ are independent.\n\nE. $X \\to Y_1 \\to Y_2$ forms a Markov chain.", "solution": "Let $X$, $Z_{1}$, and $Z_{2}$ be mutually independent random variables, and define $Y_{1} = X + Z_{1}$ and $Y_{2} = Y_{1} + Z_{2} = X + Z_{1} + Z_{2}$. We analyze the conditional distributions implied by these relations to determine the correct statistical structure.\n\nBy definition, a Markov chain $U \\to V \\to W$ holds if and only if $W$ is conditionally independent of $U$ given $V$, i.e., $p_{W|U,V}(w|u,v) = p_{W|V}(w|v)$ for all values.\n\nTo test $X \\to Y_{1} \\to Y_{2}$, compute the conditional distribution:\n$$\np_{Y_{2}|Y_{1},X}(y_{2}|y_{1},x) = p_{Z_{2}}(y_{2} - y_{1}),\n$$\nsince $Y_{2} = y_{1} + Z_{2}$ and $Z_{2}$ is independent of $(X, Z_{1})$ and hence independent of $Y_{1}$. Therefore,\n$$\np_{Y_{2}|Y_{1},X}(y_{2}|y_{1},x) = p_{Z_{2}}(y_{2} - y_{1}) = p_{Y_{2}|Y_{1}}(y_{2}|y_{1}),\n$$\nwhich verifies the Markov property $X \\to Y_{1} \\to Y_{2}$. Thus option E is true.\n\nWe now show that the other options are false.\n\nFor A: $X \\to Y_{2} \\to Y_{1}$ would require $p_{Y_{1}|X,Y_{2}}(y_{1}|x,y_{2}) = p_{Y_{1}|Y_{2}}(y_{1}|y_{2})$. Using the independence structure and Bayes’ rule,\n$$\np_{Y_{1}|X,Y_{2}}(y_{1}|x,y_{2}) \\propto p_{Z_{1}}(y_{1} - x)\\,p_{Z_{2}}(y_{2} - y_{1}),\n$$\nwhereas\n$$\np_{Y_{1}|Y_{2}}(y_{1}|y_{2}) \\propto \\sum_{x'} p_{X}(x')\\,p_{Z_{1}}(y_{1} - x')\\,p_{Z_{2}}(y_{2} - y_{1})\n$$\nfor discrete $X$ (or the analogous integral for continuous $X$). These cannot be equal for all $x$ unless $X$ is degenerate, so A is false in general.\n\nFor C: $Y_{1} \\to X \\to Y_{2}$ would require $p_{Y_{2}|X,Y_{1}}(y_{2}|x,y_{1}) = p_{Y_{2}|X}(y_{2}|x)$. However,\n$$\np_{Y_{2}|X,Y_{1}}(y_{2}|x,y_{1}) = p_{Z_{2}}(y_{2} - y_{1}),\n$$\nwhile\n$$\np_{Y_{2}|X}(y_{2}|x) = p_{Z_{1}+Z_{2}}(y_{2} - x),\n$$\nwhich are generally different (a single-noise shift versus a convolution), so C is false unless $Z_{1}$ is almost surely zero, which is not assumed.\n\nFor D: $X$ and $Y_{2}$ are independent would require $p_{Y_{2}|X}(y_{2}|x) = p_{Y_{2}}(y_{2})$ for all $x$. But\n$$\np_{Y_{2}|X}(y_{2}|x) = p_{Z_{1}+Z_{2}}(y_{2} - x),\n$$\nwhich depends on $x$ unless $X$ is degenerate. Hence D is false in general.\n\nFor B: Mutual independence fails already because $Y_{1} = X + Z_{1}$ implies\n$$\np_{Y_{1}|X}(y_{1}|x) = p_{Z_{1}}(y_{1} - x),\n$$\nwhich depends on $x$, so $X$ and $Y_{1}$ are not independent. Therefore B is false.\n\nThe only correct statement is $X \\to Y_{1} \\to Y_{2}$.", "answer": "$$\\boxed{E}$$", "id": "1662955"}, {"introduction": "Once we know a channel is degraded, how do we best communicate over it? This problem puts theory into practice by exploring superposition coding, an elegant strategy for such channels. You will calculate the maximum achievable rate for the 'disadvantaged' user when the 'advantaged' user is already receiving information at a specific rate, revealing the fundamental trade-off at the heart of broadcast communication. [@problem_id:1662908]", "problem": "Consider a two-user discrete memoryless Broadcast Channel (BC) where a single transmitter sends information to two separate receivers. The channel input, denoted by $X$, is a binary symbol from the alphabet $\\{0, 1\\}$.\n\nThe channel to the first user is perfectly noiseless. Its output $Y_1$ is identical to the input, i.e., $Y_1 = X$.\n\nThe channel to the second user is a Binary Symmetric Channel (BSC). Its output $Y_2$ is a flipped version of the input $X$ with a crossover probability $p$, where $0 < p < 1/2$. The conditional probability is given by $P(Y_2 \\ne X | X) = p$.\n\nLet $(R_1, R_2)$ be a pair of achievable communication rates in bits per channel use for user 1 and user 2, respectively. If the rate for user 1 is fixed at $R_1 = H_b(p)$, where $H_b(z) = -z \\log_2(z) - (1-z)\\log_2(1-z)$ is the binary entropy function, what is the maximum possible achievable rate $R_2$ for user 2?\n\nProvide your answer as a closed-form analytic expression in terms of the crossover probability $p$ and the binary entropy function $H_b(\\cdot)$.", "solution": "Because $Y_{1}=X$ and $Y_{2}$ is obtained by passing $X$ through a BSC with crossover $p$, the broadcast channel is physically degraded with $U \\to X \\to Y_{2}$ and $Y_{1}$ being the better output. The capacity region for a physically degraded broadcast channel is the union over all $p(u)p(x|u)$ of rate pairs satisfying\n$$\nR_{1} \\leq I(X;Y_{1}\\mid U), \\qquad R_{2} \\leq I(U;Y_{2}).\n$$\nSince $Y_{1}=X$ deterministically, $I(X;Y_{1}\\mid U) = H(X\\mid U)$.\nThus any achievable pair must satisfy\n$$\nR_{1} \\leq H(X\\mid U), \\qquad R_{2} \\leq I(U;Y_{2}).\n$$\nWe are given $R_{1}=H_{b}(p)$. To maximize $R_{2}$ under the constraint $H(X\\mid U) \\geq H_{b}(p)$, it is optimal to use exactly $H(X\\mid U)=H_{b}(p)$; adding extra randomness in $X$ given $U$ would only further degrade $Y_{2}$ and decrease $I(U;Y_{2})$.\n\nBy symmetry of the BSC and standard extremal arguments for degraded broadcast channels, the optimal choice for fixed $H(X\\mid U)$ is to take $U$ binary and uniform, and let $X$ be obtained from $U$ through a binary symmetric “test channel”: $X=U \\oplus S$ with $S \\sim \\mathrm{Bern}(r)$ independent of $U$, so that $H(X\\mid U)=H(S)=H_{b}(r)$.\nEnforcing $H(X\\mid U)=H_{b}(p)$ gives $H_{b}(r)=H_{b}(p)$, and with $0<p<\\frac{1}{2}$ we may choose $r=p$.\n\nThe effective channel from $U$ to $Y_{2}$ is then the cascade of two independent BSCs: $U \\to X$ with crossover $r=p$ and $X \\to Y_{2}$ with crossover $p$. The cascade is a BSC with crossover\n$$\np \\star r = r(1-p) + (1-r)p = p(1-p) + (1-p)p = 2p(1-p).\n$$\nWith $U$ uniform, this yields\n$$\nI(U;Y_{2}) = 1 - H_{b}(p \\star r) = 1 - H_{b}\\bigl(2p(1-p)\\bigr).\n$$\nThis value is achievable by the above superposition construction, and by the degraded BC capacity characterization and channel symmetry it is also the maximum $R_{2}$ compatible with $R_{1}=H_{b}(p)$.\n\nTherefore, the maximum achievable $R_{2}$ is\n$$\nR_{2}^{\\max} = 1 - H_{b}\\bigl(2p(1-p)\\bigr).\n$$", "answer": "$$\\boxed{1 - H_{b}\\!\\left(2p(1-p)\\right)}$$", "id": "1662908"}]}