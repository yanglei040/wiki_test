{"hands_on_practices": [{"introduction": "To begin our exploration of the Multiple-Access Channel (MAC), we start with the simplest possible case: a channel that completely ignores one of the users [@problem_id:1608075]. By applying the formal definition of the capacity region to this trivial scenario, you will verify that the mathematics aligns with our intuition. This exercise serves as a crucial sanity check and builds confidence in the core formulas before we tackle more complex forms of interference.", "problem": "A two-user Multiple Access Channel (MAC) is defined by two binary inputs, $X_1 \\in \\{0, 1\\}$ from User 1 and $X_2 \\in \\{0, 1\\}$ from User 2, and a single output $Y$. The inputs $X_1$ and $X_2$ are transmitted independently. The channel is deterministic and is characterized by the relationship $Y = X_1$, meaning the output is always identical to the input from User 1 and completely unaffected by the input from User 2. The communication rates for User 1 and User 2 are denoted by $R_1$ and $R_2$ respectively, with units of bits per channel use.\n\nWhich of the following options correctly describes the capacity region $\\mathcal{C}$, which is the set of all achievable rate pairs $(R_1, R_2)$ for this channel?\n\nA. A square region with corners at (0,0), (1,0), (1,1), and (0,1).\n\nB. A triangular region with corners at (0,0), (1,0), and (0,1).\n\nC. A line segment on the $R_1$-axis, connecting the points (0,0) and (1,0).\n\nD. A line segment on the $R_2$-axis, connecting the points (0,0) and (0,1).\n\nE. A single point at the origin (0,0).", "solution": "We use the standard capacity region of a two-user memoryless MAC. For any product input distribution $p(x_{1})p(x_{2})$, the achievable rate pairs $(R_{1},R_{2})$ satisfy\n$$\n\\begin{aligned}\nR_{1} &\\leq I(X_{1};Y\\mid X_{2}),\\\\\nR_{2} &\\leq I(X_{2};Y\\mid X_{1}),\\\\\nR_{1}+R_{2} &\\leq I(X_{1},X_{2};Y),\n\\end{aligned}\n$$\nand the capacity region $\\mathcal{C}$ is the convex closure (e.g., via time-sharing) of the union of these regions over all such input distributions.\n\nGiven the deterministic channel $Y=X_{1}$, we compute each mutual information term.\n\nFirst, for $I(X_{2};Y\\mid X_{1})$:\n$$\n\\begin{aligned}\nI(X_{2};Y\\mid X_{1}) &= H(Y\\mid X_{1}) - H(Y\\mid X_{1},X_{2})\\\\\n&= H(X_{1}\\mid X_{1}) - H(X_{1}\\mid X_{1},X_{2})\\\\\n&= 0 - 0\\\\\n&= 0.\n\\end{aligned}\n$$\nThus $R_{2} \\leq 0$, which forces $R_{2}=0$.\n\nNext, for $I(X_{1};Y\\mid X_{2})$:\n$$\n\\begin{aligned}\nI(X_{1};Y\\mid X_{2}) &= H(Y\\mid X_{2}) - H(Y\\mid X_{1},X_{2})\\\\\n&= H(X_{1}\\mid X_{2}) - 0\\\\\n&= H(X_{1}),\n\\end{aligned}\n$$\nwhere we used that $Y=X_{1}$ and $X_{1}$ and $X_{2}$ are independent, so $H(X_{1}\\mid X_{2})=H(X_{1})$. Therefore $R_{1} \\leq H(X_{1})$.\n\nFor the sum-rate:\n$$\n\\begin{aligned}\nI(X_{1},X_{2};Y) &= H(Y) - H(Y\\mid X_{1},X_{2})\\\\\n&= H(X_{1}) - 0\\\\\n&= H(X_{1}),\n\\end{aligned}\n$$\nso $R_{1}+R_{2} \\leq H(X_{1})$, which is redundant given $R_{2}=0$ and $R_{1} \\leq H(X_{1})$.\n\nFor binary $X_{1}$ with logs in base $2$, $H(X_{1}) \\leq 1$, with equality when $X_{1}$ is Bernoulli with parameter $\\frac{1}{2}$. Taking the union over all product input distributions $p(x_{1})p(x_{2})$ yields $0 \\leq R_{1} \\leq 1$ and $R_{2}=0$. Convexification via time-sharing does not introduce any $R_{2}>0$ points, so the capacity region is exactly the line segment on the $R_{1}$-axis from $(0,0)$ to $(1,0)$.\n\nHence the correct option is the line segment on the $R_{1}$-axis, i.e., option C.", "answer": "$$\\boxed{C}$$", "id": "1608075"}, {"introduction": "Having established a baseline, we now consider a channel where both users' signals interact in a simple, deterministic way [@problem_id:1608091]. This practice explores the classic noiseless binary sum channel, a foundational model for understanding user interference. Here, you will calculate the maximum symmetric rate and see firsthand how the sum-rate constraint, $R_1 + R_2 \\le I(X_1, X_2; Y)$, limits the total throughput, a central concept in multi-user information theory.", "problem": "Two independent, minimalistic sensors are deployed to monitor a specific environmental condition. Sensor 1 transmits a binary value $X_1$, and Sensor 2 transmits a binary value $X_2$, where $X_1, X_2 \\in \\{0, 1\\}$. These two sensors transmit their data simultaneously to a central processing unit over a shared, noiseless communication channel.\n\nThe communication hardware is designed such that the signal received by the processing unit, $Y$, is the arithmetic sum of the two transmitted bits: $Y = X_1 + X_2$. Consequently, the alphabet for the received signal is $Y \\in \\{0, 1, 2\\}$.\n\nThe sensors must be configured to operate at the same transmission rate, i.e., $R_1 = R_2 = R$. Determine the maximum possible value for this symmetric rate $R$. Express your final answer as a single real number representing the rate in units of bits per channel use.", "solution": "We model the uplink as a discrete memoryless multiple-access channel with two independent binary inputs $X_{1},X_{2}\\in\\{0,1\\}$ and deterministic output $Y=X_{1}+X_{2}\\in\\{0,1,2\\}$. For such a MAC, the capacity region is given by the union over all product input distributions $P_{X_{1}}P_{X_{2}}$ of all rate pairs $(R_{1},R_{2})$ satisfying\n$$\nR_{1}\\leq I(X_{1};Y\\mid X_{2}),\\quad R_{2}\\leq I(X_{2};Y\\mid X_{1}),\\quad R_{1}+R_{2}\\leq I(X_{1},X_{2};Y),\n$$\nwhere all entropies and mutual informations are measured in bits, i.e., with $\\log_{2}$.\n\nBecause the channel is deterministic, $H(Y\\mid X_{1},X_{2})=0$, so\n$$\nI(X_{1},X_{2};Y)=H(Y).\n$$\nMoreover,\n$$\nI(X_{1};Y\\mid X_{2})=H(Y\\mid X_{2})-H(Y\\mid X_{1},X_{2})=H(Y\\mid X_{2}),\n$$\nand similarly $I(X_{2};Y\\mid X_{1})=H(Y\\mid X_{1})$. For the adder $Y=X_{1}+X_{2}$, conditioning on $X_{2}=0$ yields $Y=X_{1}$, and conditioning on $X_{2}=1$ yields $Y=1+X_{1}$, which is a bijection of $X_{1}$. Hence $H(Y\\mid X_{2})=H(X_{1})$ and, by symmetry, $H(Y\\mid X_{1})=H(X_{2})$. Therefore, for any independent inputs,\n$$\nR_{1}\\leq H(X_{1}),\\quad R_{2}\\leq H(X_{2}),\\quad R_{1}+R_{2}\\leq H(Y).\n$$\n\nLet $X_{1}\\sim\\mathrm{Bern}(p_{1})$ and $X_{2}\\sim\\mathrm{Bern}(p_{2})$ be independent. Then\n$$\nH(X_{1})=h_{2}(p_{1}),\\quad H(X_{2})=h_{2}(p_{2}),\n$$\nwhere $h_{2}(p)=-p\\log_{2}(p)-(1-p)\\log_{2}(1-p)$ is the binary entropy function, and the distribution of $Y$ is\n$$\n\\Pr(Y=0)=(1-p_{1})(1-p_{2}),\\quad \\Pr(Y=1)=p_{1}+p_{2}-2p_{1}p_{2},\\quad \\Pr(Y=2)=p_{1}p_{2},\n$$\nso that\n$$\nH(Y)=H\\big((1-p_{1})(1-p_{2}),\\, p_{1}+p_{2}-2p_{1}p_{2},\\, p_{1}p_{2}\\big).\n$$\n\nFor symmetric operation $R_{1}=R_{2}=R$, the constraints reduce to\n$$\nR\\leq h_{2}(p_{1}),\\quad R\\leq h_{2}(p_{2}),\\quad 2R\\leq H(Y).\n$$\nMaximizing the symmetric rate requires maximizing the minimum of these three bounds over $p_{1},p_{2}$. By symmetry, it is optimal to set $p_{1}=p_{2}=p$. Then $H(X_{1})=H(X_{2})=h_{2}(p)$ and\n$$\n\\Pr(Y=0)=(1-p)^{2},\\quad \\Pr(Y=1)=2p(1-p),\\quad \\Pr(Y=2)=p^{2}.\n$$\nThe entropy $H(Y)$ is maximized at $p=\\frac{1}{2}$ (by symmetry and concavity of entropy), yielding\n$$\nH(X_{1})=H(X_{2})=h_{2}\\!\\left(\\tfrac{1}{2}\\right)=1,\\quad H(Y)=H\\!\\left(\\tfrac{1}{4},\\tfrac{1}{2},\\tfrac{1}{4}\\right)=\\tfrac{3}{2}.\n$$\nThus, at $p=\\frac{1}{2}$, the symmetric constraints are\n$$\nR\\leq 1,\\quad 2R\\leq \\tfrac{3}{2}\\ \\Rightarrow\\ R\\leq \\tfrac{3}{4}.\n$$\nThe sum-rate bound is the active constraint, so the maximum symmetric rate is\n$$\nR^{\\ast}=\\tfrac{3}{4}\\ \\text{bits per channel use}.\n$$\nThis rate is achievable with independent equiprobable inputs, and is optimal since any $(R,R)$ must satisfy $2R\\leq H(Y)\\leq \\tfrac{3}{2}$ for this channel.", "answer": "$$\\boxed{\\frac{3}{4}}$$", "id": "1608091"}, {"introduction": "Our final practice presents a channel that appears significantly more complex, featuring two distinct outputs and the presence of noise [@problem_id:1608093]. This exercise is designed to test your ability to look beyond surface complexity and rigorously analyze the flow of information. By carefully applying the definitions of mutual information, you will uncover a profound insight about redundant channel outputs and solidify your understanding of what truly defines the limits of communication in a multi-user system.", "problem": "Consider a two-user binary-input Multiple Access Channel (MAC). User 1 and User 2 select inputs $X_1 \\in \\{0, 1\\}$ and $X_2 \\in \\{0, 1\\}$ respectively, which are assumed to be independent. The receiver observes a pair of outputs $Y = (Y_1, Y_2)$ for each use of the channel.\n\nThe first output, $Y_1$, is the noiseless arithmetic sum of the inputs:\n$$Y_1 = X_1 + X_2$$\nThe second output, $Y_2$, is obtained by passing the modulo-2 sum (XOR) of the inputs through a a Binary Symmetric Channel (BSC). Specifically:\n$$Y_2 = (X_1 \\oplus X_2) \\oplus Z$$\nwhere $\\oplus$ denotes addition modulo 2, and $Z$ is a Bernoulli random variable with $P(Z=1) = p$ and $P(Z=0) = 1-p$, for a fixed crossover probability $0 < p < 1/2$. The noise variable $Z$ is independent of the inputs $X_1$ and $X_2$.\n\nThe capacity region $\\mathcal{C}$ is the set of all achievable rate pairs $(R_1, R_2)$, where rates are measured in bits per channel use. Determine the area of the capacity region $\\mathcal{C}$. Express your answer as a single real number.", "solution": "For a two-user discrete memoryless MAC with independent inputs $X_{1}$ and $X_{2}$, the capacity region (with time-sharing) is the union over input distributions $P_{X_{1}}P_{X_{2}}$ of the pentagons defined by\n$$\nR_{1} \\leq I(X_{1};Y \\mid X_{2}), \\quad\nR_{2} \\leq I(X_{2};Y \\mid X_{1}), \\quad\nR_{1}+R_{2} \\leq I(X_{1},X_{2};Y),\n$$\nwhere $Y=(Y_{1},Y_{2})$ is the channel output.\n\nLet $X_{1} \\sim \\mathrm{Bern}(\\alpha)$ and $X_{2} \\sim \\mathrm{Bern}(\\beta)$, independent. All entropies and mutual informations are measured in bits, i.e., with base-$2$ logarithms. Write the binary entropy as $H_{2}(u) = -u \\log_{2} u - (1-u)\\log_{2}(1-u)$.\n\nFirst compute $I(X_{1};Y \\mid X_{2})$. Given $X_{2}$, $Y_{1}=X_{1}+X_{2}$ deterministically reveals $X_{1}$, hence $H(X_{1} \\mid Y,X_{2})=0$. Since $X_{1}$ is independent of $X_{2}$, $H(X_{1} \\mid X_{2})=H(X_{1})=H_{2}(\\alpha)$. Therefore\n$$\nI(X_{1};Y \\mid X_{2}) = H(X_{1} \\mid X_{2}) - H(X_{1} \\mid Y,X_{2}) = H_{2}(\\alpha).\n$$\nBy symmetry,\n$$\nI(X_{2};Y \\mid X_{1}) = H_{2}(\\beta).\n$$\n\nNext compute $I(X_{1},X_{2};Y)$. Since $Y=(Y_{1},Y_{2})$ with $Y_{1}=X_{1}+X_{2}$ deterministic and $Y_{2}=(X_{1}\\oplus X_{2}) \\oplus Z$ where $Z \\sim \\mathrm{Bern}(p)$ is independent of $(X_{1},X_{2})$, we have\n$$\nH(Y \\mid X_{1},X_{2}) = H(Y_{1},Y_{2} \\mid X_{1},X_{2}) = H(Y_{2} \\mid X_{1},X_{2}) = H_{2}(p).\n$$\nMoreover, $S = X_{1} \\oplus X_{2}$ is a deterministic function of $Y_{1}$ (indeed, $S=1$ iff $Y_{1}=1$ and $S=0$ otherwise), so\n$$\nH(Y) = H(Y_{1},Y_{2}) = H(Y_{1}) + H(Y_{2} \\mid Y_{1}) = H(Y_{1}) + H_{2}(p),\n$$\nwhich yields\n$$\nI(X_{1},X_{2};Y) = H(Y) - H(Y \\mid X_{1},X_{2}) = H(Y_{1}).\n$$\nThus the three defining quantities for the pentagon corresponding to $(\\alpha,\\beta)$ are\n$$\na := I(X_{1};Y \\mid X_{2}) = H_{2}(\\alpha), \\quad\nb := I(X_{2};Y \\mid X_{1}) = H_{2}(\\beta), \\quad\nc := I(X_{1},X_{2};Y) = H(Y_{1}),\n$$\nwith $Y_{1} \\in \\{0,1,2\\}$ distributed as\n$$\n\\begin{aligned}\nP(Y_{1}=0) &= (1-\\alpha)(1-\\beta), \\\\\nP(Y_{1}=1) &= \\alpha(1-\\beta) + (1-\\alpha)\\beta, \\\\\nP(Y_{1}=2) &= \\alpha \\beta,\n\\end{aligned}\n$$\nso that $H(Y_{1})$ is the ternary entropy of these masses.\n\nFor a fixed $(a,b,c)$ satisfying $c \\leq a+b$, the corresponding capacity pentagon has area\n$$\n\\mathsf{Area}(a,b,c) = a b - \\frac{1}{2}\\bigl(a + b - c\\bigr)^{2},\n$$\nsince it is the area of the rectangle $[0,a] \\times [0,b]$ minus the right triangle cut off by $R_{1}+R_{2} \\leq c$ with leg lengths $a+b-c$.\n\nWe now maximize these bounds over input distributions. Clearly $a \\leq 1$ with equality at $\\alpha=\\tfrac{1}{2}$, and $b \\leq 1$ with equality at $\\beta=\\tfrac{1}{2}$. The entropy $H(Y_{1})$ is maximized at $\\alpha=\\beta=\\tfrac{1}{2}$, where\n$$\nP(Y_{1}=0)=\\tfrac{1}{4}, \\quad P(Y_{1}=1)=\\tfrac{1}{2}, \\quad P(Y_{1}=2)=\\tfrac{1}{4},\n$$\nso\n$$\nH(Y_{1}) = -\\tfrac{1}{4}\\log_{2}\\tfrac{1}{4} - \\tfrac{1}{2}\\log_{2}\\tfrac{1}{2} - \\tfrac{1}{4}\\log_{2}\\tfrac{1}{4} = \\tfrac{3}{2}.\n$$\nTherefore, at $\\alpha=\\beta=\\tfrac{1}{2}$ we have simultaneously\n$$\na^{\\star}=1, \\quad b^{\\star}=1, \\quad c^{\\star}=\\tfrac{3}{2}.\n$$\nFor any other $(\\alpha,\\beta)$, $a \\leq a^{\\star}$, $b \\leq b^{\\star}$, and $c \\leq c^{\\star}$. Hence every pentagon for any input distribution is contained in the pentagon with $(a^{\\star},b^{\\star},c^{\\star})$, and allowing time-sharing cannot enlarge beyond this maximal pentagon. Consequently, the capacity region equals the pentagon with $a=1$, $b=1$, $c=\\tfrac{3}{2}$, independent of $p$.\n\nIts area is\n$$\n\\mathsf{Area}(\\mathcal{C}) = 1 \\cdot 1 - \\frac{1}{2}\\bigl(1 + 1 - \\tfrac{3}{2}\\bigr)^{2}\n= 1 - \\frac{1}{2}\\bigl(\\tfrac{1}{2}\\bigr)^{2}\n= 1 - \\frac{1}{8}\n= \\frac{7}{8}.\n$$\nThus, the area of the capacity region is $\\tfrac{7}{8}$.", "answer": "$$\\boxed{\\frac{7}{8}}$$", "id": "1608093"}]}