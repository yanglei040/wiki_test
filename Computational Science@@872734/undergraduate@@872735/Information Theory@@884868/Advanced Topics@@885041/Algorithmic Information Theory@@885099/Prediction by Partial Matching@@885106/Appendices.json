{"hands_on_practices": [{"introduction": "Before we can make predictions, we must first build the model. This exercise walks you through the fundamental process of constructing the context frequency tables that form the backbone of any Prediction by Partial Matching (PPM) model. By manually tracing a sequence and tabulating the symbol frequencies for different context orders, you will gain a concrete understanding of how PPM captures statistical patterns from data [@problem_id:1647197].", "problem": "In the field of data compression, statistical models are used to predict upcoming symbols in a data stream. One such class of models is built using the Prediction by Partial Matching (PPM) algorithm. A PPM model maintains frequency counts of symbols that follow specific preceding sequences, known as contexts.\n\nFor a given context order `k`, the model builds a frequency table. A `k`-order context is a sequence of `k` symbols. The table for order `k` records, for each distinct `k`-order context `C` found in a training sequence, which symbols immediately followed `C` and how many times each one appeared. The order `k=0` model is a special case with an empty context, and its table simply records the total frequency of each symbol in the entire sequence.\n\nConsider a PPM model that is being trained on the text sequence `S = \"AGADADAGA\"` from left to right. Which of the following options correctly represents the complete set of context frequency tables for orders `k=2`, `k=1`, and `k=0` after the model has processed the entire sequence?\n\nThe tables are represented using the notation: `k=N: {Context1: {Symbol1: Count1, ...}, Context2: {...}, ...}`.\n\nA.\n`k=2`: {`AG`: {`A`: 1}, `GA`: {`D`: 1}, `AD`: {`A`: 1}, `DA`: {`D`: 1, `G`: 1}}\n`k=1`: {`A`: {`G`: 1, `D`: 1}, `G`: {`A`: 1}, `D`: {`A`: 1}}\n`k=0`: {`A`: 5, `G`: 2, `D`: 2}\n\nB.\n`k=2`: {`AG`: {`A`: 2}, `GA`: {`D`: 1}, `AD`: {`A`: 2}, `DA`: {`D`: 1, `G`: 1}}\n`k=1`: {`A`: {`G`: 2, `D`: 2}, `G`: {`A`: 2}, `D`: {`A`: 2}}\n`k=0`: {`A`: 4, `G`: 3, `D`: 2}\n\nC.\n`k=2`: {`AG`: {`A`: 2}, `GA`: {`D`: 1}, `AD`: {`A`: 2}, `DA`: {`D`: 1, `G`: 1}}\n`k=1`: {`A`: {`G`: 2, `D`: 2}, `G`: {`A`: 2}, `D`: {`A`: 2}}\n`k=0`: {`A`: 5, `G`: 2, `D`: 2}\n\nD.\n`k=2`: {`GA`: {`A`: 2}, `AG`: {`A`: 2}, `DA`: {`D`: 1, `G`: 1}}\n`k=1`: {`A`: {`G`: 2, `D`: 2}, `G`: {`A`: 2}, `D`: {`A`: 1, `G`: 1}}\n`k=0`: {`A`: 5, `G`: 2, `D`: 2}", "solution": "We are given the sequence S = \"AGADADAGA\" of length $n=9$, processed left to right. For a PPM model of order $k$, each occurrence is derived from positions $i=k+1$ to $n$, where the context is the length-$k$ substring $S_{i-k}\\dots S_{i-1}$ and the symbol counted is $S_{i}$. The order $k=0$ model simply records total symbol frequencies in the entire sequence.\n\nCompute the $k=2$ table using $i=3$ to $9$:\n- $i=3$: context \"AG\", next \"A\" gives \"AG\": {\"A\": 1}.\n- $i=4$: context \"GA\", next \"D\" gives \"GA\": {\"D\": 1}.\n- $i=5$: context \"AD\", next \"A\" gives \"AD\": {\"A\": 1}.\n- $i=6$: context \"DA\", next \"D\" gives \"DA\": {\"D\": 1}.\n- $i=7$: context \"AD\", next \"A\" updates \"AD\": {\"A\": 2}.\n- $i=8$: context \"DA\", next \"G\" updates \"DA\": {\"D\": 1, \"G\": 1}.\n- $i=9$: context \"AG\", next \"A\" updates \"AG\": {\"A\": 2}.\nThus $k=2$: {\"AG\": {\"A\": 2}, \"GA\": {\"D\": 1}, \"AD\": {\"A\": 2}, \"DA\": {\"D\": 1, \"G\": 1}}.\n\nCompute the $k=1$ table using $i=2$ to $9$:\n- $i=2$: context \"A\", next \"G\" gives \"A\": {\"G\": 1}.\n- $i=3$: context \"G\", next \"A\" gives \"G\": {\"A\": 1}.\n- $i=4$: context \"A\", next \"D\" updates \"A\": {\"G\": 1, \"D\": 1}.\n- $i=5$: context \"D\", next \"A\" gives \"D\": {\"A\": 1}.\n- $i=6$: context \"A\", next \"D\" updates \"A\": {\"G\": 1, \"D\": 2}.\n- $i=7$: context \"D\", next \"A\" updates \"D\": {\"A\": 2}.\n- $i=8$: context \"A\", next \"G\" updates \"A\": {\"G\": 2, \"D\": 2}.\n- $i=9$: context \"G\", next \"A\" updates \"G\": {\"A\": 2}.\nThus $k=1$: {\"A\": {\"G\": 2, \"D\": 2}, \"G\": {\"A\": 2}, \"D\": {\"A\": 2}}.\n\nCompute the $k=0$ table by total frequencies in S:\n- \"A\" occurs at positions $1,3,5,7,9$: total $5$.\n- \"G\" occurs at positions $2,8$: total $2$.\n- \"D\" occurs at positions $4,6$: total $2$.\nThus $k=0$: {\"A\": 5, \"G\": 2, \"D\": 2}.\n\nComparing with the options, the set of tables matches exactly option C.", "answer": "$$\\boxed{C}$$", "id": "1647197"}, {"introduction": "Once a PPM model's frequency tables are built, its primary function is to predict upcoming symbols. This practice problem demonstrates how to calculate the probability of a specific symbol appearing next, introducing the critical concepts of hierarchical prediction and the escape mechanism [@problem_id:1647202]. You will apply a simplified but illustrative set of rules to see how the model gracefully transitions from higher-order (more specific) to lower-order (more general) contexts when it lacks information.", "problem": "Prediction by Partial Matching (PPM) is a statistical data compression algorithm that predicts the next symbol in a sequence based on the preceding symbols, known as the \"context\". This problem explores a specific variant of this algorithm.\n\nConsider a \"Simplified PPM\" model defined by the following rules, which is used to predict the symbol following a given sequence.\n\n**Algorithm Definition: Simplified PPM**\n\n1.  **Initial Context**: To predict the probability of a symbol following a sequence, the model starts by considering the context of the maximum possible order. For a given maximum order $k_{\\text{max}}$, the initial context is the last $k_{\\text{max}}$ symbols of the sequence.\n\n2.  **Model Statistics (for an order-$k$ context, $c_k$)**:\n    *   The model's statistics are derived from a training sequence.\n    *   Let $N(c_k)$ be the total number of times the context $c_k$ appears in the training sequence, where each appearance is immediately followed by at least one symbol.\n    *   Let $N(s|c_k)$ be the number of times a specific symbol $s$ is observed immediately following the context $c_k$.\n    *   Let $U(c_k)$ be the set of unique symbols that have been observed to follow $c_k$.\n\n3.  **Probability Calculation at Order $k$**:\n    *   **Case 1 (Symbol Seen)**: If the symbol $s$ to be predicted is in the set $U(c_k)$, its probability is calculated directly:\n        $$P_k(s | c_k) = \\frac{N(s|c_k)}{N(c_k) + |U(c_k)|}$$\n    *   **Case 2 (Symbol Not Seen - Escape)**: If the symbol $s$ is *not* in the set $U(c_k)$, the model \"escapes\" to a lower-order model. The probability is the product of an escape probability and the probability from the next lower-order model:\n        $$P_k(s | c_k) = P_{\\text{esc},k} \\times P_{k-1}(s | c_{k-1})$$\n        where the escape probability is $P_{\\text{esc},k} = \\frac{|U(c_k)|}{N(c_k) + |U(c_k)|}$ and $c_{k-1}$ is the suffix of context $c_k$ of length $k-1$.\n\n4.  **Base Models**:\n    *   **Order-0 Model**: The context is the empty string, $c_0 = \\text{\"\"}$. For this model, $N(c_0)$ is the total length of the training sequence, and $U(c_0)$ is the set of all unique symbols in the entire sequence.\n    *   **Order-(-1) Model**: This is a uniform prior. If an escape occurs from the Order-0 model (which happens when predicting a symbol that never appears in the training sequence), this model assigns a probability of $1/M$ to any symbol, where $M$ is the size of the alphabet.\n\n**Problem:**\n\nA Simplified PPM model is built using the training sequence `AABABAA`. The alphabet of possible symbols consists of all unique symbols present in this sequence. The maximum model order is set to $k_{\\text{max}}=2$.\n\nUsing this specific model, what is the probability that the symbol immediately following the sequence `AABABAA` is 'A'? Your answer should be a fraction in its simplest form.", "solution": "The training sequence is AABABAA, so the alphabet is $\\{A,B\\}$. The maximum order is $k_{\\text{max}}=2$, so the initial context for predicting the next symbol after AABABAA is the last two symbols, $c_{2}=\\text{\"AA\"}$.\n\nFor an order-$k$ context $c_{k}$, the model uses:\n$$P_{k}(s\\mid c_{k})=\\begin{cases}\n\\frac{N(s\\mid c_{k})}{N(c_{k})+|U(c_{k})|}, & \\text{if } s\\in U(c_{k}),\\\\[6pt]\nP_{\\text{esc},k}\\,P_{k-1}(s\\mid c_{k-1}), & \\text{if } s\\notin U(c_{k}),\n\\end{cases}$$\nwith escape probability\n$$P_{\\text{esc},k}=\\frac{|U(c_{k})|}{N(c_{k})+|U(c_{k})|}.$$\n\nOrder-2 statistics from the training sequence are computed by counting contexts of length $2$ that are immediately followed by a symbol. The occurrences are:\n- $\\text{\"AA\"}\\to\\text{\"B\"}$ (from positions $1$–$3$),\n- $\\text{\"AB\"}\\to\\text{\"A\"}$ (from positions $2$–$4$ and $4$–$6$),\n- $\\text{\"BA\"}\\to\\text{\"B\"}$ (from positions $3$–$5$) and $\\text{\"BA\"}\\to\\text{\"A\"}$ (from positions $5$–$7$).\nThe terminal $\\text{\"AA\"}$ at positions $6$–$7$ has no following symbol and is not counted.\n\nThus, for $c_{2}=\\text{\"AA\"}$,\n$$N(\\text{\"AA\"})=1,\\quad U(\\text{\"AA\"})=\\{\\text{\"B\"}\\},\\quad N(A\\mid \\text{\"AA\"})=0.$$\nSince $A\\notin U(\\text{\"AA\"})$, we escape:\n$$P_{2}(A\\mid \\text{\"AA\"})=P_{\\text{esc},2}\\,P_{1}(A\\mid \\text{\"A\"}),\\quad P_{\\text{esc},2}=\\frac{|U(\\text{\"AA\"})|}{N(\\text{\"AA\"})+|U(\\text{\"AA\"})|}=\\frac{1}{1+1}=\\frac{1}{2}.$$\n\nNow compute the order-1 probability for $c_{1}=\\text{\"A\"}$ by counting contexts of length $1$ that are immediately followed by a symbol. The occurrences are:\n- $\\text{\"A\"}\\to\\text{\"A\"}$ (positions $1$–$2$ and $6$–$7$),\n- $\\text{\"A\"}\\to\\text{\"B\"}$ (positions $2$–$3$ and $4$–$5$).\nHence,\n$$N(\\text{\"A\"})=4,\\quad U(\\text{\"A\"})=\\{\\text{\"A\"},\\text{\"B\"}\\},\\quad N(A\\mid \\text{\"A\"})=2,$$\nso\n$$P_{1}(A\\mid \\text{\"A\"})=\\frac{N(A\\mid \\text{\"A\"})}{N(\\text{\"A\"})+|U(\\text{\"A\"})|}=\\frac{2}{4+2}=\\frac{1}{3}.$$\n\nCombine the factors:\n$$P_{2}(A\\mid \\text{\"AA\"})=\\frac{1}{2}\\cdot \\frac{1}{3}=\\frac{1}{6}.$$\nNo further escape is needed because $A\\in U(\\text{\"A\"})$, so lower orders are not used.\n\nTherefore, the probability that the symbol following AABABAA is $A$ under this Simplified PPM model is $\\frac{1}{6}$.", "answer": "$$\\boxed{\\frac{1}{6}}$$", "id": "1647202"}, {"introduction": "Prediction by Partial Matching is not a static process; the model's internal state evolves as it processes a sequence symbol by symbol. This exercise simulates the dynamic encoding of a string, focusing on tracking the number of \"escape events\" that occur [@problem_id:1647198]. Understanding when and why the model must \"escape\" to a lower-order context is key to grasping how PPM adapts to new patterns and achieves efficient data compression.", "problem": "A Prediction by Partial Matching (PPM) compression model is used to encode a sequence of symbols. The model predicts the next symbol based on its preceding symbols, which form a \"context\". The number of preceding symbols used is the \"order\" of the context.\n\nConsider a PPM model with a maximum context order of $k=2$. When encoding a symbol, the model first attempts to use the order-2 context (the two preceding symbols). If the current symbol has never been seen in this order-2 context before, the model generates an **escape event** and tries the order-1 context (the single preceding symbol). This process continues downwards through order 0 (which has no context and considers the overall frequency of symbols in the sequence so far) and finally to order -1 (which lists all unique symbols seen anywhere in the sequence so far).\n\nAn escape event is defined as a single step down in the hierarchy of contexts. For example:\n- A failure at order 2, leading to a check at order 1, is one escape event.\n- A subsequent failure at order 1, leading to a check at order 0, is a second escape event.\n- A failure at order 0, leading to a check at order -1, is a third escape event.\n- If the symbol is not even in the order -1 model (i.e., it is a completely new symbol), a final escape event occurs to signal a novelty.\n\nThe model's internal tables of symbol counts are updated *after* each symbol is processed.\n\nCalculate the total number of escape events that occur when encoding the sequence `XYYXYYXXY`.", "solution": "We model a PPM encoder with maximum context order $k=2$, descending through orders $2 \\to 1 \\to 0 \\to -1$. The model updates its counts after processing each symbol. An escape event occurs each time the current symbol is not present in the current context model and the encoder steps down one order; if the symbol is not even present in the order $-1$ model (new globally), a final novelty escape occurs. When fewer than $j$ preceding symbols exist, the highest available starting order is $\\min\\{2,i-1\\}$ at position $i$.\n\nWe process the sequence $XYYXYYXXY$, tracking for each position $i$ the number of escapes $e_{i}$, using prior occurrences only.\n\nInitialize all models empty.\n\nPosition $1$ ($X$), start at order $0$:\n- Order $0$: $X$ not seen globally $\\Rightarrow$ escape to order $-1$.\n- Order $-1$: $X$ not in unique list $\\Rightarrow$ final novelty escape.\nThus $e_{1}=2$. Update: global seen $\\{X\\}$.\n\nPosition $2$ ($Y$), start at order $1$ with context $X$:\n- Order $1$ (after $X$): $Y$ not seen $\\Rightarrow$ escape.\n- Order $0$: $Y$ not seen globally (only $X$ seen) $\\Rightarrow$ escape.\n- Order $-1$: $Y$ not in unique list $\\Rightarrow$ final novelty escape.\nThus $e_{2}=3$. Update: global seen $\\{X,Y\\}$; $C_{1}[X]$ contains $Y$.\n\nPosition $3$ ($Y$), start at order $2$ with context $XY$:\n- Order $2$ ($XY$): $Y$ not seen $\\Rightarrow$ escape.\n- Order $1$ ($Y$): $Y$ not seen after $Y$ yet $\\Rightarrow$ escape.\n- Order $0$: $Y$ seen globally $\\Rightarrow$ success.\nThus $e_{3}=2$. Update: $C_{1}[Y]$ contains $Y$; $C_{2}[XY]$ contains $Y$.\n\nPosition $4$ ($X$), start at order $2$ with context $YY$:\n- Order $2$ ($YY$): $X$ not seen $\\Rightarrow$ escape.\n- Order $1$ ($Y$): $X$ not yet seen after $Y$ $\\Rightarrow$ escape.\n- Order $0$: $X$ seen globally $\\Rightarrow$ success.\nThus $e_{4}=2$. Update: $C_{1}[Y]$ contains $\\{Y,X\\}$; $C_{2}[YY]$ contains $X$.\n\nPosition $5$ ($Y$), start at order $2$ with context $YX$:\n- Order $2$ ($YX$): $Y$ not seen $\\Rightarrow$ escape.\n- Order $1$ ($X$): $Y$ seen after $X$ $\\Rightarrow$ success.\nThus $e_{5}=1$. Update: $C_{2}[YX]$ contains $Y$.\n\nPosition $6$ ($Y$), start at order $2$ with context $XY$:\n- Order $2$ ($XY$): $Y$ seen $\\Rightarrow$ success.\nThus $e_{6}=0$. Update: no new symbols in sets.\n\nPosition $7$ ($X$), start at order $2$ with context $YY$:\n- Order $2$ ($YY$): $X$ seen $\\Rightarrow$ success.\nThus $e_{7}=0$.\n\nPosition $8$ ($X$), start at order $2$ with context $YX$:\n- Order $2$ ($YX$): $X$ not seen ($C_{2}[YX]=\\{Y\\}$) $\\Rightarrow$ escape.\n- Order $1$ ($X$): $X$ not yet seen after $X$ ($C_{1}[X]=\\{Y\\}$) $\\Rightarrow$ escape.\n- Order $0$: $X$ seen globally $\\Rightarrow$ success.\nThus $e_{8}=2$. Update: $C_{1}[X]$ contains $\\{Y,X\\}$; $C_{2}[YX]$ contains $\\{Y,X\\}$.\n\nPosition $9$ ($Y$), start at order $2$ with context $XX$:\n- Order $2$ ($XX$): $Y$ not seen $\\Rightarrow$ escape.\n- Order $1$ ($X$): $Y$ seen after $X$ $\\Rightarrow$ success.\nThus $e_{9}=1$.\n\nSumming the escapes:\n$$E_{\\text{total}}=\\sum_{i=1}^{9} e_{i}=2+3+2+2+1+0+0+2+1=13.$$", "answer": "$$\\boxed{13}$$", "id": "1647198"}]}