{"hands_on_practices": [{"introduction": "To effectively use the Entropy Power Inequality, we must first understand its fundamental quantity: the entropy power, $N(X)$. This initial exercise provides a concrete starting point by asking you to calculate the entropy power for a very common non-Gaussian distribution, the uniform distribution. This practice will solidify your understanding of the relationship between a variable's probability density function, its differential entropy $h(X)$, and its corresponding entropy power.", "problem": "A sensor is designed to measure a physical quantity. Due to inherent limitations, its reading, represented by a continuous random variable $X$, is known to be uniformly distributed over an interval of length $L$. We can model this by saying that the probability density function (PDF) of $X$ is constant over an interval, which for simplicity we take to be $[0, L]$, and zero elsewhere.\n\nThe uncertainty of this measurement can be quantified using differential entropy. The differential entropy $h(X)$ for a random variable $X$ with PDF $f(x)$ is defined as:\n$$h(X) = -\\int_{-\\infty}^{\\infty} f(x) \\ln(f(x)) \\,dx$$\n\nA related concept is the entropy power, $N(X)$, which represents the variance of a Gaussian random variable having the same differential entropy as $X$. The relationship between differential entropy and entropy power is given by:\n$$h(X) = \\frac{1}{2}\\ln(2\\pi e N(X))$$\n\nGiven the uniform distribution of the sensor's reading over the interval $[0, L]$, determine its entropy power, $N(X)$. Express your answer as an analytic expression in terms of $L$ and any necessary mathematical constants.", "solution": "For a uniform distribution on $[0, L]$, the probability density function is\n$$\nf(x)=\\begin{cases}\n\\frac{1}{L}, & 0 \\leq x \\leq L,\\\\\n0, & \\text{otherwise}.\n\\end{cases}\n$$\nThe differential entropy is\n$$\nh(X)=-\\int_{-\\infty}^{\\infty} f(x)\\ln(f(x))\\,dx=-\\int_{0}^{L}\\frac{1}{L}\\ln\\!\\left(\\frac{1}{L}\\right)\\,dx.\n$$\nSince the integrand is constant over $[0,L]$,\n$$\nh(X)=-\\frac{1}{L}\\ln\\!\\left(\\frac{1}{L}\\right)\\int_{0}^{L}dx=-\\frac{1}{L}\\ln\\!\\left(\\frac{1}{L}\\right)\\,L=-\\ln\\!\\left(\\frac{1}{L}\\right)=\\ln L.\n$$\n\nThe entropy power $N(X)$ satisfies\n$$\nh(X)=\\frac{1}{2}\\ln\\!\\big(2\\pi e\\,N(X)\\big).\n$$\nSolving for $N(X)$, first multiply by $2$ and exponentiate:\n$$\n2h(X)=\\ln\\!\\big(2\\pi e\\,N(X)\\big)\\;\\;\\Longrightarrow\\;\\;\\exp\\!\\big(2h(X)\\big)=2\\pi e\\,N(X).\n$$\nThus\n$$\nN(X)=\\frac{\\exp\\!\\big(2h(X)\\big)}{2\\pi e}.\n$$\nSubstituting $h(X)=\\ln L$ gives\n$$\nN(X)=\\frac{\\exp\\!\\big(2\\ln L\\big)}{2\\pi e}=\\frac{L^{2}}{2\\pi e}.\n$$", "answer": "$$\\boxed{\\frac{L^{2}}{2\\pi e}}$$", "id": "1620999"}, {"introduction": "The Entropy Power Inequality has a special case: equality holds if and only if the independent random variables are Gaussian. This next practice explores this fundamental aspect by modeling two independent noise sources as Gaussian random variables. By calculating the entropy powers of the individual sources and their sum, you will directly verify that $N(X+Y) = N(X) + N(Y)$, and in doing so, reveal the important fact that for a Gaussian variable, its entropy power is simply its variance.", "problem": "A crucial component in a deep space communication receiver is designed to operate at extremely low temperatures. Its performance is limited by two primary sources of internal thermal noise. These noise sources can be accurately modeled as independent, zero-mean Gaussian random variables representing noise voltages. Let these random variables be $X$ and $Y$.\n\nFrom laboratory measurements, the variance of the first noise source is $\\sigma_X^2 = 3.5 \\text{ V}^2$, and the variance of the second noise source is $\\sigma_Y^2 = 5.2 \\text{ V}^2$. The total noise voltage affecting the signal is the sum of these two, $Z = X+Y$.\n\nIn information theory, the \"informativeness\" of a continuous random signal can be quantified by its entropy power. The entropy power, $N(V)$, of a random variable $V$ with differential entropy $h(V)$ is defined by the expression:\n$$N(V) = \\frac{1}{2\\pi e} \\exp(2h(V))$$\nwhere $e$ is the base of the natural logarithm and $\\pi$ is the mathematical constant.\n\nCalculate the entropy power of the total noise voltage, $N(Z)$. Express your answer in units of $\\text{V}^2$.", "solution": "Let $X$ and $Y$ be independent, zero-mean Gaussian random variables with variances $\\sigma_{X}^{2}=3.5$ and $\\sigma_{Y}^{2}=5.2$ (in units of $\\text{V}^{2}$). The sum $Z=X+Y$ is Gaussian by the closure of the Gaussian family under addition, and independence implies variance additivity:\n$$\n\\sigma_{Z}^{2}=\\operatorname{Var}(Z)=\\operatorname{Var}(X)+\\operatorname{Var}(Y)=\\sigma_{X}^{2}+\\sigma_{Y}^{2}.\n$$\n\nThe differential entropy of a univariate Gaussian random variable with variance $\\sigma^{2}$ is\n$$\nh(Z)=\\frac{1}{2}\\ln\\!\\big(2\\pi e\\,\\sigma_{Z}^{2}\\big).\n$$\nBy the definition of entropy power,\n$$\nN(Z)=\\frac{1}{2\\pi e}\\exp\\!\\big(2h(Z)\\big)=\\frac{1}{2\\pi e}\\exp\\!\\left(2\\cdot\\frac{1}{2}\\ln\\!\\big(2\\pi e\\,\\sigma_{Z}^{2}\\big)\\right)=\\frac{1}{2\\pi e}\\exp\\!\\left(\\ln\\!\\big(2\\pi e\\,\\sigma_{Z}^{2}\\big)\\right)=\\sigma_{Z}^{2}.\n$$\nTherefore, for Gaussian $Z$, the entropy power equals its variance. Substituting the given variances,\n$$\n\\sigma_{Z}^{2}=3.5+5.2=8.7,\n$$\nso $N(Z)=8.7$ in units of $\\text{V}^{2}$.", "answer": "$$\\boxed{8.7}$$", "id": "1620997"}, {"introduction": "While equality in the EPI is reserved for Gaussian variables, the strict inequality $N(X+Y) \\gt N(X) + N(Y)$ for non-Gaussian variables is where the theorem reveals a deeper principle about uncertainty. This final exercise challenges you to quantify this effect by examining the sum of two independent uniform random variables. By calculating the \"entropy power gap,\" you will demonstrate that the uncertainty of the sum is greater than the sum of its parts, a key insight in information theory.", "problem": "In information theory, the concept of entropy provides a measure of uncertainty for a random variable. For a continuous random variable $A$ with a probability density function (PDF) $f_A(a)$, the differential entropy is defined as $h(A) = -\\int_{-\\infty}^{\\infty} f_A(a) \\ln(f_A(a)) da$.\n\nFrom this, one can define the entropy power of the random variable, denoted $N(A)$, which represents the variance of a Gaussian random variable having the same differential entropy as $A$. The formula for entropy power is $N(A) = \\frac{1}{2\\pi e} \\exp(2h(A))$.\n\nA fundamental result connecting the entropies of sums of independent random variables is the Entropy Power Inequality (EPI). For two independent random variables $X$ and $Y$, the EPI states that the entropy power of their sum is at least the sum of their individual entropy powers: $N(X+Y) \\ge N(X) + N(Y)$.\n\nThe equality holds if and only if $X$ and $Y$ are Gaussian random variables. For non-Gaussian variables, there is a strict inequality, leading to an \"entropy power gap\". This gap, $\\Delta N$, is the amount by which the entropy power of the sum exceeds the sum of the individual entropy powers: $\\Delta N = N(X+Y) - (N(X) + N(Y))$.\n\nConsider two independent and identically distributed (i.i.d.) random variables, $X$ and $Y$, where each follows a uniform distribution on the interval $[0, 1]$. Let their sum be $Z=X+Y$.\n\nCalculate the exact value of the entropy power gap, $\\Delta N$, for this specific case. Your final answer should be a closed-form analytic expression in terms of mathematical constants like $\\pi$ and $e$.", "solution": "We are given i.i.d. random variables $X$ and $Y$, each uniform on $[0,1]$, and we define $Z=X+Y$. The entropy power of a continuous random variable $A$ with differential entropy $h(A)$ is\n$$\nN(A)=\\frac{1}{2\\pi e}\\exp\\!\\left(2h(A)\\right).\n$$\nThe entropy power gap is\n$$\n\\Delta N = N(X+Y) - \\left(N(X)+N(Y)\\right).\n$$\n\nFirst, compute the differential entropy of $X$ and $Y$. Since $X\\sim\\mathrm{Unif}[0,1]$, its PDF is $f_{X}(x)=1$ for $x\\in[0,1]$ and $0$ otherwise. Therefore,\n$$\nh(X) = -\\int_{-\\infty}^{\\infty} f_{X}(x)\\ln f_{X}(x)\\,dx = -\\int_{0}^{1} 1\\cdot \\ln(1)\\,dx = 0.\n$$\nBy identical distribution, $h(Y)=0$. Hence,\n$$\nN(X) = N(Y) = \\frac{1}{2\\pi e}\\exp\\!\\left(2\\cdot 0\\right)=\\frac{1}{2\\pi e}.\n$$\n\nNext, compute $h(Z)$. The PDF of $Z=X+Y$ is the triangular density obtained by convolution:\n$$\nf_{Z}(z) = \\begin{cases}\nz, & 0\\le z \\le 1,\\\\\n2-z, & 1< z \\le 2,\\\\\n0, & \\text{otherwise}.\n\\end{cases}\n$$\nThus,\n$$\nh(Z) = -\\int_{-\\infty}^{\\infty} f_{Z}(z)\\ln f_{Z}(z)\\,dz\n= -\\left(\\int_{0}^{1} z\\ln z\\,dz + \\int_{1}^{2} (2-z)\\ln(2-z)\\,dz\\right).\n$$\nIn the second integral, substitute $u=2-z$, so $du=-dz$ and the limits map as $z=1\\mapsto u=1$, $z=2\\mapsto u=0$. Then\n$$\n\\int_{1}^{2} (2-z)\\ln(2-z)\\,dz = \\int_{1}^{0} u\\ln u\\,(-du) = \\int_{0}^{1} u\\ln u\\,du.\n$$\nTherefore,\n$$\nh(Z) = -2\\int_{0}^{1} t\\ln t\\,dt.\n$$\nUsing the standard integral $\\int_{0}^{1} t^{\\alpha}\\ln t\\,dt = -\\frac{1}{(\\alpha+1)^{2}}$ for $\\alpha>-1$, with $\\alpha=1$, we get\n$$\n\\int_{0}^{1} t\\ln t\\,dt = -\\frac{1}{4},\n$$\nso\n$$\nh(Z) = -2\\left(-\\frac{1}{4}\\right)=\\frac{1}{2}.\n$$\nThen the entropy power of $Z$ is\n$$\nN(Z) = \\frac{1}{2\\pi e}\\exp\\!\\left(2h(Z)\\right) = \\frac{1}{2\\pi e}\\exp(1) = \\frac{1}{2\\pi}.\n$$\n\nFinally, the entropy power gap is\n$$\n\\Delta N = N(Z) - \\left(N(X)+N(Y)\\right) = \\frac{1}{2\\pi} - 2\\cdot\\frac{1}{2\\pi e}\n= \\frac{1}{2\\pi} - \\frac{1}{\\pi e}\n= \\frac{e-2}{2\\pi e}.\n$$\nThis is strictly positive since $e>2$, consistent with the strict EPI inequality for non-Gaussian $X$ and $Y$.", "answer": "$$\\boxed{\\frac{e-2}{2\\pi e}}$$", "id": "1620980"}]}