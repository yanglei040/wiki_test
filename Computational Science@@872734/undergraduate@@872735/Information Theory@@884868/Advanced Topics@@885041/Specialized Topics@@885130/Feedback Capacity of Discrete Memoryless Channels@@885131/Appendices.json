{"hands_on_practices": [{"introduction": "A cornerstone of information theory is the principle that for discrete memoryless channels (DMCs), adding a feedback path from the receiver to the transmitter does not increase the channel's fundamental capacity. This exercise provides a direct application of this theorem. By calculating the capacity for a Z-channel with an idealized feedback loop [@problem_id:1669160], you will reinforce the understanding that the problem reduces to the standard method of finding capacity without feedback.", "problem": "A digital communication system is modeled by a binary asymmetric channel known as a Z-channel. The input alphabet is $\\mathcal{X} = \\{0, 1\\}$ and the output alphabet is $\\mathcal{Y} = \\{0, 1\\}$. The channel's behavior is defined by the following conditional probabilities:\n- A transmitted '0' is always received correctly as a '0'.\n- A transmitted '1' is received correctly as a '1' with probability $1-p$, but is erroneously flipped to a '0' with probability $p$, where $0 < p < 1$.\n\nAn enhancement is proposed for this system: after each symbol is received, a perfect, instantaneous, and noiseless feedback path from the receiver to the transmitter is established. This feedback informs the transmitter of the exact symbol ($0$ or $1$) that was just received at the output. The transmitter is free to use this information to adapt its transmission strategy.\n\nYour task is to determine the fundamental capacity of this Z-channel with the described feedback mechanism. The capacity is the maximum rate, in bits per channel use, at which information can be transmitted with arbitrarily low error probability.\n\nExpress your answer as a closed-form analytic expression in terms of the crossover probability $p$. For all logarithms in your final answer, use base 2.", "solution": "Let the channel input be $X \\in \\{0,1\\}$ with $P(X=1)=q$ and $P(X=0)=1-q$, and the output be $Y \\in \\{0,1\\}$. The Z-channel transition probabilities are $P(Y=0 \\mid X=0)=1$, $P(Y=1 \\mid X=0)=0$, $P(Y=1 \\mid X=1)=1-p$, and $P(Y=0 \\mid X=1)=p$, with $0<p<1$.\n\nFirst, we address the effect of feedback. For a discrete memoryless channel, the feedback capacity is\n$$\nC_{\\text{FB}}=\\sup \\lim_{n \\to \\infty} \\frac{1}{n} I(X^{n} \\to Y^{n}),\n$$\nwhere $I(X^{n} \\to Y^{n})=\\sum_{i=1}^{n} I(X_{i};Y_{i} \\mid Y^{i-1})$ is the directed information. By memorylessness and the fact that $Y_{i}$ depends on $X_{i}$ only through the one-step transition law, we have for each $i$\n$$\nI(X_{i};Y_{i} \\mid Y^{i-1}) \\leq \\max_{P_{X}} I(X;Y),\n$$\nbecause conditioning on $Y^{i-1}$ induces some input distribution $P_{X_{i}\\mid Y^{i-1}}$, and $I(X_{i};Y_{i}\\mid Y^{i-1}=y^{i-1}) \\leq \\max_{P_{X}} I(X;Y)$ for every $y^{i-1}$. Therefore,\n$$\nI(X^{n} \\to Y^{n}) \\leq n \\max_{P_{X}} I(X;Y),\n$$\nwhich gives $C_{\\text{FB}} \\leq C$, where $C=\\max_{P_{X}} I(X;Y)$ is the usual (no-feedback) capacity. Achievability of $C$ with feedback is immediate by using the same i.i.d. capacity-achieving input distribution and ignoring the feedback, so $C_{\\text{FB}}=C$. Hence, it suffices to compute $C=\\max_{q \\in [0,1]} I(X;Y)$.\n\nFor the Z-channel, compute\n$$\nP(Y=1)=P(Y=1 \\mid X=1)P(X=1)+P(Y=1 \\mid X=0)P(X=0)=(1-p)q,\n$$\nand hence $P(Y=0)=1-(1-p)q$. The output entropy is\n$$\nH(Y)=H_{b}((1-p)q),\n$$\nwhere $H_{b}(u)=-u \\log_{2} u-(1-u)\\log_{2}(1-u)$ is the binary entropy function (with base-2 logarithms). The conditional entropy is\n$$\nH(Y \\mid X)=P(X=0) H(Y \\mid X=0)+P(X=1) H(Y \\mid X=1)=0 \\cdot (1-q)+q \\, H_{b}(p)=q \\, H_{b}(p).\n$$\nTherefore,\n$$\nI(X;Y)=H(Y)-H(Y \\mid X)=H_{b}((1-p)q)-q \\, H_{b}(p).\n$$\n\nWe maximize $f(q)=H_{b}((1-p)q)-q \\, H_{b}(p)$ over $q \\in [0,1]$. Since $H_{b}$ is concave and the composition with an affine map preserves concavity, $f(q)$ is concave and has a unique maximizer in $[0,1]$. Differentiate using $H_{b}'(u)=\\log_{2}\\!\\left(\\frac{1-u}{u}\\right)$:\n$$\nf'(q)=(1-p)\\, H_{b}'((1-p)q)-H_{b}(p)=(1-p)\\log_{2}\\!\\left(\\frac{1-(1-p)q}{(1-p)q}\\right)-H_{b}(p).\n$$\nSet $f'(q)=0$ to obtain\n$$\n\\log_{2}\\!\\left(\\frac{1-(1-p)q}{(1-p)q}\\right)=\\frac{H_{b}(p)}{1-p}.\n$$\nDefine\n$$\nA \\triangleq 2^{\\frac{H_{b}(p)}{1-p}}.\n$$\nThen\n$$\n\\frac{1-(1-p)q}{(1-p)q}=A \\quad \\Longrightarrow \\quad (1-p)q=\\frac{1}{1+A}.\n$$\nLet $r \\triangleq (1-p)q=\\frac{1}{1+A}$. The maximizing $q^{\\ast}$ is $q^{\\ast}=\\frac{r}{1-p}=\\frac{1}{(1-p)(1+A)}$. The corresponding maximum mutual information is\n$$\nC=f(q^{\\ast})=H_{b}(r)-\\frac{r}{1-p} H_{b}(p).\n$$\nUse $r=\\frac{1}{1+A}$ and the identity $H_{b}(r)=\\log_{2}(1+A)-\\frac{A}{1+A}\\log_{2} A$ (obtained by substituting $r=\\frac{1}{1+A}$ and $1-r=\\frac{A}{1+A}$) to get\n$$\nC=\\log_{2}(1+A)-\\frac{A}{1+A}\\log_{2} A-\\frac{1}{1+A}\\log_{2} A=\\log_{2}(1+A)-\\log_{2} A=\\log_{2}\\!\\left(1+\\frac{1}{A}\\right).\n$$\nNow express $A$ in terms of $p$ alone. Since\n$$\n\\frac{H_{b}(p)}{1-p}=\\frac{-p \\log_{2} p-(1-p)\\log_{2}(1-p)}{1-p}=-\\frac{p}{1-p}\\log_{2} p-\\log_{2}(1-p),\n$$\nwe have\n$$\nA=2^{\\frac{H_{b}(p)}{1-p}}=2^{-\\left(\\frac{p}{1-p}\\log_{2} p+\\log_{2}(1-p)\\right)}=\\frac{1}{(1-p)\\, p^{\\frac{p}{1-p}}}.\n$$\nTherefore\n$$\n\\frac{1}{A}=(1-p)\\, p^{\\frac{p}{1-p}},\n$$\nand the capacity is\n$$\nC=\\log_{2}\\!\\left(1+(1-p)\\, p^{\\frac{p}{1-p}}\\right).\n$$\nBecause feedback does not increase the capacity of a discrete memoryless channel, this is also the capacity with the described perfect, instantaneous, noiseless feedback.", "answer": "$$\\boxed{\\log_{2}\\!\\left(1+(1-p)\\,p^{\\frac{p}{1-p}}\\right)}$$", "id": "1669160"}, {"introduction": "Correct conclusions can sometimes be reached through flawed reasoning. This practice problem transitions from calculation to critical thinking, asking you to analyze an argument about the capacity of a composite channel [@problem_id:1624728]. Your task is to identify the logical error in an engineer's reasoning, which will sharpen your understanding of why the capacity of a channel mixture is not simply a weighted average of individual capacities.", "problem": "A communications engineer is designing a system for a channel that exhibits complex noisy behavior. For each bit transmitted, the channel behaves in one of two ways, with the choice being random and independent for each transmission.\n- With probability $\\alpha$, the channel acts as a Binary Symmetric Channel, or BSC($p$), which has a binary input $\\{0, 1\\}$ and binary output $\\{0, 1\\}$. An input bit is flipped to the opposite value with a crossover probability $p$. The capacity of a BSC($p$) is $C_{BSC} = 1 - H_2(p)$, where $H_2(p) = -p \\log_2(p) - (1-p) \\log_2(1-p)$ is the binary entropy function.\n- With probability $1-\\alpha$, the channel acts as a Binary Erasure Channel, or BEC($\\epsilon$), which has a binary input $\\{0, 1\\}$ and a ternary output $\\{0, 1, e\\}$. An input bit is erased (output is $e$) with probability $\\epsilon$, and transmitted correctly otherwise. The capacity of a BEC($\\epsilon$) is $C_{BEC} = 1-\\epsilon$.\n\nA junior engineer on the team analyzes the potential benefit of adding a perfect, delay-free feedback link from the receiver to the transmitter. The engineer presents the following argument to conclude that feedback will not increase the capacity of this composite channel:\n\n1.  For the BSC($p$) sub-channel, it is a known result that its capacity with feedback, $C_{BSC, fb}$, is equal to its capacity without feedback, $C_{BSC}$.\n2.  For the BEC($\\epsilon$) sub-channel, it is also known that its capacity with feedback, $C_{BEC, fb}$, is equal to its capacity without feedback, $C_{BEC}$.\n3.  The overall channel is a probabilistic mixture of the two sub-channels. Therefore, its capacity without feedback, $C$, must be the weighted average of the individual capacities: $C = \\alpha C_{BSC} + (1-\\alpha) C_{BEC}$.\n4.  Similarly, the capacity with feedback, $C_{fb}$, must be the weighted average of the individual feedback capacities: $C_{fb} = \\alpha C_{BSC, fb} + (1-\\alpha) C_{BEC, fb}$.\n5.  Combining these points, it is clear that $C_{fb} = \\alpha C_{BSC} + (1-\\alpha) C_{BEC} = C$. Thus, feedback does not increase capacity.\n\nThe senior engineer confirms that the final conclusion is correct (i.e., $C_{fb} = C$), but states that the junior engineer's argument is fundamentally flawed. Which of the following statements correctly identifies the flaw in the junior engineer's reasoning?\n\nA. The junior engineer is wrong; feedback *can* increase the capacity of this composite channel because the transmitter can use the feedback to learn which sub-channel (BSC or BEC) was used for a transmission and adapt its strategy for future transmissions.\n\nB. The junior engineer's argument is flawed because the capacity of a probabilistic mixture of channels is, in general, not the weighted average of their individual capacities.\n\nC. The junior engineer's argument is flawed because while feedback does not increase the capacity of a BSC, it *does* increase the capacity of a BEC, as it allows for perfect transmission through re-transmitting erased bits.\n\nD. The junior engineer's argument is perfectly correct in all its steps and provides the right justification for why the capacity is unchanged.\n\nE. The junior engineer's argument is flawed because the composite channel is not memoryless, and the theorem that feedback does not increase capacity only applies to memoryless channels.", "solution": "Let $X \\in \\{0,1\\}$ denote the channel input, $Y \\in \\{0,1,e\\}$ the channel output, and let $S \\in \\{\\text{BSC},\\text{BEC}\\}$ be the random sub-channel selector, independent across uses, with $\\Pr[S=\\text{BSC}]=\\alpha$ and $\\Pr[S=\\text{BEC}]=1-\\alpha$. Conditioned on $S$, the channel is memoryless; since $S$ is i.i.d. across uses, the overall composite channel is a single discrete memoryless channel (DMC) with transition law\n$$\n\\Pr[Y=e \\mid X=x] = (1-\\alpha)\\epsilon,\\quad\n\\Pr[Y=1-x \\mid X=x] = \\alpha p,\\quad\n\\Pr[Y=x \\mid X=x] = \\alpha(1-p) + (1-\\alpha)(1-\\epsilon).\n$$\nTherefore, the capacity without feedback is\n$$\nC \\;=\\; \\max_{P_{X}} I(X;Y).\n$$\nBy Shannon’s feedback theorem for DMCs, perfect, delay-free feedback does not increase capacity; hence\n$$\nC_{\\text{fb}} \\;=\\; C.\n$$\nThus the senior engineer’s acceptance of the conclusion $C_{\\text{fb}}=C$ is justified on general grounds.\n\nThe flaw in the junior engineer’s reasoning lies in Steps 3 and 4, which assert that the capacity of the probabilistic mixture equals the weighted average of the individual capacities. In general, capacity does not commute with probabilistic mixing. To see the correct relation, introduce the selector $S$; for any input distribution $P_{X}$, apply the chain rule:\n$$\nI(X;Y) \\;=\\; I(X;Y,S) - I(X;S \\mid Y)\n\\;=\\; I(X;S) + I(X;Y \\mid S) - I(X;S \\mid Y).\n$$\nSince $X$ and $S$ are independent, $I(X;S)=0$, so\n$$\nI(X;Y) \\;=\\; \\alpha\\, I_{\\text{BSC}}(X;Y) + (1-\\alpha)\\, I_{\\text{BEC}}(X;Y) - I(X;S \\mid Y),\n$$\nwhere $I_{\\text{BSC}}(X;Y)$ and $I_{\\text{BEC}}(X;Y)$ denote the mutual informations when the channel is BSC or BEC, respectively, evaluated under the same $P_{X}$. Because $I(X;S \\mid Y) \\geq 0$, we obtain\n$$\nI(X;Y) \\;\\leq\\; \\alpha\\, I_{\\text{BSC}}(X;Y) + (1-\\alpha)\\, I_{\\text{BEC}}(X;Y).\n$$\nMaximizing both sides over $P_{X}$ yields\n$$\nC \\;=\\; \\max_{P_{X}} I(X;Y)\n\\;\\leq\\; \\max_{P_{X}} \\big[ \\alpha\\, I_{\\text{BSC}}(X;Y) + (1-\\alpha)\\, I_{\\text{BEC}}(X;Y) \\big]\n\\;\\leq\\; \\alpha\\, C_{\\text{BSC}} + (1-\\alpha)\\, C_{\\text{BEC}}.\n$$\nThus the weighted average $\\alpha C_{\\text{BSC}} + (1-\\alpha) C_{\\text{BEC}}$ is, in general, only an upper bound on the composite channel capacity, not its value. Consequently, the junior engineer’s Step 3 (and by the same logic, Step 4) is fundamentally incorrect: the capacity of a probabilistic mixture of channels is not, in general, the weighted average of the individual capacities. This identifies the flaw while remaining consistent with the correct final conclusion $C_{\\text{fb}}=C$ based on the DMC feedback theorem.\n\nTherefore, among the proposed choices, the correct identification of the flaw is that the capacity of a probabilistic mixture is not the weighted average of the individual capacities.", "answer": "$$\\boxed{B}$$", "id": "1624728"}, {"introduction": "The concept of channel capacity has profound consequences for the limits of reliable communication. This final exercise explores what happens when we try to push information through a channel faster than its capacity allows, even with the help of a feedback mechanism [@problem_id:1624717]. By connecting the source entropy, channel capacity, and Fano's inequality, you will derive a fundamental lower bound on the achievable bit error rate, illustrating the hard limit imposed by capacity.", "problem": "An information theorist is analyzing the fundamental limits of a communication system designed to transmit data from a source modeled as an independent and identically distributed (i.i.d.) Bernoulli process with parameter $p$. The transmission occurs over a Binary Symmetric Channel (BSC), an acronym for a channel that flips each transmitted bit with a crossover probability of $\\epsilon$. The system is operated at a rate of one source symbol per channel use.\n\nThe theorist considers a powerful coding scheme that performs joint source-channel coding over very long blocks of data. This scheme also has access to an ideal (zero-delay, error-free) feedback channel, allowing the transmitter's encoding strategy for a given symbol to depend on all previously received channel outputs. The primary performance metric is the average bit error probability, $p_b$, defined as the long-term average probability that a decoded bit differs from its original source bit.\n\nAssume a scenario where the source generates information at a rate fundamentally faster than the channel can reliably handle; specifically, the entropy of the source is greater than the capacity of the channel.\n\nLet $H(x) = -x \\log_2(x) - (1-x) \\log_2(1-x)$ be the binary entropy function for $x \\in (0,1)$. Let $H^{-1}(y)$ be the inverse of this function defined for a domain of $y \\in [0, 1]$ that maps to a range of $x \\in [0, 1/2]$.\n\nWhich of the following expressions represents the tightest possible lower bound on the average bit error probability $p_b$ that *any* such communication system can achieve under these conditions?\n\nA. $p_b \\ge \\epsilon$\n\nB. Communication with an arbitrarily small $p_b > 0$ is possible with a sufficiently complex coding scheme.\n\nC. $p_b \\ge H^{-1}(H(p))$\n\nD. $p_b \\ge H^{-1}(H(p) + H(\\epsilon) - 1)$\n\nE. $p_b \\ge H^{-1}(1 - H(\\epsilon))$", "solution": "The problem asks for a lower bound on the average bit error probability $p_b$ when transmitting a Bernoulli($p$) source over a BSC($\\epsilon$) at a rate of one source symbol per channel use, under the condition that the source entropy $H(p)$ is greater than the channel capacity $C$.\n\nStep 1: Relate source entropy, information transmission, and channel capacity.\nLet $X^n = (X_1, ..., X_n)$ be a sequence of $n$ source symbols, and $\\hat{X}^n = (\\hat{X}_1, ..., \\hat{X}_n)$ be the corresponding sequence of decoded symbols. Since the source is i.i.d. Bernoulli($p$), the entropy of the source sequence is $H(X^n) = nH(p)$.\n\nThe relationship between the entropy of the source, the mutual information between the source and its estimate, and the conditional entropy (equivocation) is given by:\n$$H(X^n) = I(X^n; \\hat{X}^n) + H(X^n | \\hat{X}^n)$$\nwhere $I(X^n; \\hat{X}^n)$ is the mutual information.\n\nThe data processing inequality states that for any processing of the channel output $Y^n$ to get $\\hat{X}^n$, the mutual information cannot increase. The encoding and channel transmission form a Markov chain $X^n \\to U^n \\to Y^n \\to \\hat{X}^n$, where $U^n$ is the channel input sequence. Thus,\n$$I(X^n; \\hat{X}^n) \\le I(U^n; Y^n)$$\nEven with feedback, the channel input $U_i$ at time $i$ can depend on $X^n$ and previous channel outputs $Y^{i-1}$. However, a fundamental result of information theory is that for a discrete memoryless channel (DMC), feedback does not increase the channel capacity. The mutual information across $n$ uses of a DMC is bounded by $n$ times the capacity $C$:\n$$I(U^n; Y^n) \\le nC$$\nThe capacity of a BSC with crossover probability $\\epsilon$ is given by $C = 1 - H(\\epsilon)$.\n\nCombining these inequalities, we have:\n$$I(X^n; \\hat{X}^n) \\le nC$$\nSubstituting this back into the entropy equation:\n$$n H(p) = I(X^n; \\hat{X}^n) + H(X^n | \\hat{X}^n) \\le nC + H(X^n | \\hat{X}^n)$$\nRearranging this gives a lower bound on the equivocation:\n$$H(X^n | \\hat{X}^n) \\ge n H(p) - nC = n(H(p) - C)$$\n\nStep 2: Relate the equivocation to the bit error probability using Fano's Inequality.\nWe need to find an upper bound on $H(X^n | \\hat{X}^n)$ in terms of the average bit error probability $p_b$.\nUsing the chain rule for entropy and the fact that conditioning reduces entropy:\n$$H(X^n | \\hat{X}^n) = H(X_1, ..., X_n | \\hat{X}_1, ..., \\hat{X}_n) = \\sum_{i=1}^{n} H(X_i | X_1, ..., X_{i-1}, \\hat{X}^n) \\le \\sum_{i=1}^{n} H(X_i | \\hat{X}_i)$$\nFor a single bit, the conditional entropy $H(X_i | \\hat{X}_i)$ is related to the probability of error for that bit, $p_{e,i} = P(X_i \\ne \\hat{X}_i)$, by Fano's inequality for a single variable:\n$$H(X_i | \\hat{X}_i) \\le H(p_{e,i})$$\nTherefore, we have:\n$$H(X^n | \\hat{X}^n) \\le \\sum_{i=1}^{n} H(p_{e,i})$$\nThe binary entropy function $H(x)$ is concave. By Jensen's inequality:\n$$\\frac{1}{n} \\sum_{i=1}^{n} H(p_{e,i}) \\le H\\left(\\frac{1}{n} \\sum_{i=1}^{n} p_{e,i}\\right)$$\nThe term inside the entropy function on the right is the definition of the average bit error probability, $p_b$. So,\n$$\\sum_{i=1}^{n} H(p_{e,i}) \\le n H(p_b)$$\nThis gives the upper bound on equivocation:\n$$H(X^n | \\hat{X}^n) \\le n H(p_b)$$\n\nStep 3: Combine bounds and solve for $p_b$.\nWe have two bounds on $H(X^n | \\hat{X}^n)$:\n$$n(H(p) - C) \\le H(X^n | \\hat{X}^n) \\le n H(p_b)$$\nThis implies:\n$$n(H(p) - C) \\le n H(p_b)$$\n$$H(p) - C \\le H(p_b)$$\nNow substitute the capacity of the BSC, $C = 1 - H(\\epsilon)$:\n$$H(p) - (1 - H(\\epsilon)) \\le H(p_b)$$\n$$H(p) + H(\\epsilon) - 1 \\le H(p_b)$$\nThe problem assumes we are in a regime where $H(p) > C$, which means $H(p) + H(\\epsilon) - 1 > 0$. By convention, the bit error probability $p_b$ is assumed to be less than or equal to $1/2$ (if it were greater, one could flip all bits to get a better error rate). The binary entropy function $H(x)$ is monotonically increasing on the interval $[0, 1/2]$. Therefore, we can apply its inverse, $H^{-1}(y)$, to both sides of the inequality without changing the direction of the inequality:\n$$H^{-1}(H(p) + H(\\epsilon) - 1) \\le H^{-1}(H(p_b))$$\nGiven that $H^{-1}$ is the inverse of $H$ on the range $[0, 1/2]$, this simplifies to:\n$$p_b \\ge H^{-1}(H(p) + H(\\epsilon) - 1)$$\nThis provides the tightest possible lower bound on $p_b$ based on these information-theoretic arguments.\n\nStep 4: Evaluate the options.\nThe derived expression matches option D. Let's analyze the other options:\nA. $p_b \\ge \\epsilon$: This is a plausible distractor. A naive transmission without coding would yield $p_b=\\epsilon$. Coding can sometimes reduce the error rate below $\\epsilon$, but this bound is not universally tight. Our derived bound can be smaller or larger than $\\epsilon$ depending on parameters.\nB. Arbitrarily small $p_b$ is possible: This is a statement of the channel coding theorem, which holds only if the information rate is less than capacity. Here the information rate is $H(p)$ (since we send 1 source bit/channel use), and the problem states $H(p)>C$. Thus, this statement is false.\nC. $p_b \\ge H^{-1}(H(p)) = p$: This corresponds to the source statistics, not the channel limitations. It represents an incorrect application of the principles.\nE. $p_b \\ge H^{-1}(1 - H(\\epsilon)) = H^{-1}(C)$: This bound ignores the rate of information generation from the source, $H(p)$, and only considers the channel capacity. It would be the result of an incomplete analysis.\n\nTherefore, the only correct and tightest bound among the choices is D.", "answer": "$$\\boxed{D}$$", "id": "1624717"}]}