{"hands_on_practices": [{"introduction": "To fully grasp the Information Bottleneck trade-off, it is instructive to first examine the boundary cases. This practice explores two extreme compression schemes: one with no compression at all, and another where the compression is so severe that it erases all information about the original data. By calculating the mutual information values for these scenarios [@problem_id:1631245], you will map the theoretical limits of the compression-relevance landscape and establish a clear intuition for the problem space.", "problem": "In the design of a remote medical monitoring system, a sensor measures a discrete physiological state $X$, which can take one of three values: `low`, `medium`, or `high`. This state is known to be correlated with a patient's future health outcome $Y$, which can be either `healthy` or `sick`. The joint probability distribution $p(X, Y)$ describing this relationship is given by the table below:\n\n| $p(X,Y)$ | $Y = \\text{healthy}$ | $Y = \\text{sick}$ |\n| :--- | :---: | :---: |\n| $X = \\text{low}$ | $1/4$ | $0$ |\n| $X = \\text{medium}$ | $1/4$ | $1/4$ |\n| $X = \\text{high}$ | $0$ | $1/4$ |\n\nTo conserve transmission bandwidth, the sensor's reading $X$ is mapped to a compressed representation $T$ before being sent to a central server. The effectiveness of this compression is evaluated using the principles of the Information Bottleneck (IB) method. This method considers two key quantities: the mutual information $I(X; T)$, which quantifies the \"bottleneck\" size (a smaller value means more compression), and the mutual information $I(T; Y)$, which measures how much information the compressed signal retains about the relevant outcome $Y$ (a larger value is better).\n\nYou are asked to analyze two extreme compression schemes:\n\n1.  **Scheme 1 (No Compression):** The transmitted signal, denoted $T_1$, is a perfect and uncompressed copy of the sensor's measurement.\n2.  **Scheme 2 (Maximum Useless Compression):** The transmitted signal, denoted $T_2$, is generated in such a way that it is statistically independent of the sensor's measurement $X$.\n\nFor each of these two schemes, determine the values of the pair $(I(X; T), I(T; Y))$. Use base 2 for all logarithm calculations, which means that all information-theoretic quantities should be in units of bits.\n\nProvide your answer as a 2x2 matrix of numerical values. The first row should contain the pair of values for Scheme 1, and the second row should contain the pair for Scheme 2. In both rows, the first column must be the value of $I(X;T)$ and the second column must be the value of $I(T;Y)$.", "solution": "We are given the joint distribution of $(X,Y)$. First compute the marginals:\n$$\np_{X}(\\text{low})=\\frac{1}{4},\\quad p_{X}(\\text{medium})=\\frac{1}{2},\\quad p_{X}(\\text{high})=\\frac{1}{4},\n$$\n$$\np_{Y}(\\text{healthy})=\\frac{1}{2},\\quad p_{Y}(\\text{sick})=\\frac{1}{2}.\n$$\nUsing base-2 logarithms, the entropy of $X$ is\n$$\nH(X)=-\\sum_{x}p_{X}(x)\\log_{2}p_{X}(x)=-\\left[\\frac{1}{4}\\log_{2}\\frac{1}{4}+\\frac{1}{2}\\log_{2}\\frac{1}{2}+\\frac{1}{4}\\log_{2}\\frac{1}{4}\\right]=\\frac{3}{2}.\n$$\nThe entropy of $Y$ is\n$$\nH(Y)=-\\sum_{y}p_{Y}(y)\\log_{2}p_{Y}(y)=-\\left[\\frac{1}{2}\\log_{2}\\frac{1}{2}+\\frac{1}{2}\\log_{2}\\frac{1}{2}\\right]=1.\n$$\nThe conditional distribution of $Y$ given $X$ yields\n$$\nH(Y|X=\\text{low})=0,\\quad H(Y|X=\\text{medium})=1,\\quad H(Y|X=\\text{high})=0,\n$$\nso\n$$\nH(Y|X)=\\sum_{x}p_{X}(x)H(Y|X=x)=\\frac{1}{2}.\n$$\nTherefore the mutual information between $X$ and $Y$ is\n$$\nI(X;Y)=H(Y)-H(Y|X)=1-\\frac{1}{2}=\\frac{1}{2}.\n$$\n\nScheme 1 (No compression): $T_{1}=X$, so by definition\n$$\nI(X;T_{1})=I(X;X)=H(X)=\\frac{3}{2},\n$$\nand by data processing with identity mapping $T_{1}=X$,\n$$\nI(T_{1};Y)=I(X;Y)=\\frac{1}{2}.\n$$\n\nScheme 2 (Maximum useless compression): $T_{2}$ is independent of $X$, i.e., $p(t|x)=p(t)$ for all $x$. Then\n$$\nI(X;T_{2})=0.\n$$\nMoreover,\n$$\np(t,y)=\\sum_{x}p(t|x)p(x,y)=\\sum_{x}p(t)p(x,y)=p(t)\\sum_{x}p(x,y)=p(t)p(y),\n$$\nso $T_{2}$ is also independent of $Y$ and\n$$\nI(T_{2};Y)=0.\n$$\n\nThus, the requested pairs $(I(X;T),I(T;Y))$ are\n- Scheme 1: $\\left(\\frac{3}{2},\\frac{1}{2}\\right)$,\n- Scheme 2: $(0,0)$,\nassembled into the required $2\\times 2$ matrix.", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{3}{2} & \\frac{1}{2} \\\\ 0 & 0 \\end{pmatrix}}$$", "id": "1631245"}, {"introduction": "Most real-world applications of the Information Bottleneck method operate between the extremes of perfect retention and total information loss. This exercise places you in the role of a designer tasked with finding an optimal balance for a given trade-off parameter $\\beta$. You will systematically evaluate different ways to compress data into a single bit and use the IB objective function to determine which strategy best preserves relevant information while adhering to a complexity constraint [@problem_id:1631238].", "problem": "In the field of machine learning, it is often desirable to create a compressed, low-dimensional representation of a high-dimensional input variable, while retaining as much relevant information as possible about a target variable. This problem explores a simplified version of this trade-off.\n\nConsider a system where an input signal $X$ is drawn uniformly at random from the set of four symbols $\\{a, b, c, d\\}$. A related binary output signal $Y$ can take values in $\\{0, 1\\}$. The relationship between $X$ and $Y$ is described by the following conditional probabilities:\n- $P(Y=1|X=a) = 0.1$\n- $P(Y=1|X=b) = 0.2$\n- $P(Y=1|X=c) = 0.8$\n- $P(Y=1|X=d) = 0.9$\n\nWe want to design a one-bit deterministic encoder for $X$. This means we partition the set of four input symbols $\\{a, b, c, d\\}$ into two non-empty groups. All symbols in the first group are mapped to a compressed representation $T=0$, and all symbols in the second group are mapped to $T=1$.\n\nThe quality of a given encoding (i.e., a partition) is measured by a score $S$, defined as:\n$$S = I(T; Y) - \\beta I(X; T)$$\nHere, $I(A; B)$ denotes the mutual information between random variables $A$ and $B$, all logarithms are base-2, and information is measured in bits. The parameter $\\beta$ balances the trade-off between how much information the representation $T$ has about the target $Y$ (relevance, $I(T;Y)$), and how much information $T$ retains about the original signal $X$ (complexity, $I(X;T)$).\n\nFor a trade-off parameter of $\\beta = 0.2$, determine the maximum possible score $S$ over all possible one-bit deterministic encodings. Round your final answer to three significant figures.", "solution": "Let $X$ be uniform on $\\{a,b,c,d\\}$ and $Y \\in \\{0,1\\}$ with $P(Y=1|X=a)=0.1$, $P(Y=1|X=b)=0.2$, $P(Y=1|X=c)=0.8$, $P(Y=1|X=d)=0.9$. A one-bit deterministic encoder $T=f(X)$ partitions $\\{a,b,c,d\\}$ into two non-empty groups. For such a deterministic $T$, we have $I(X;T)=H(T)$, since $H(T|X)=0$. The relevance is $I(T;Y)=H(Y)-H(Y|T)$. With $X$ uniform, the marginal of $Y$ is\n$$\nP(Y=1)=\\frac{0.1+0.2+0.8+0.9}{4}=\\frac{2}{4}=0.5 \\;\\Rightarrow\\; H(Y)=1.\n$$\nThus, for any partition with group sizes $n_{0}$ and $n_{1}$ (where $n_{0}+n_{1}=4$), letting $q_{t}=P(Y=1|T=t)$ be the average of the corresponding $P(Y=1|X=x)$ over the symbols in group $t$, we have\n$$\nI(T;Y)=1-\\sum_{t\\in\\{0,1\\}} \\frac{n_{t}}{4}\\,h_{2}(q_{t}),\\qquad I(X;T)=H(T)=h_{2}\\!\\left(\\frac{n_{0}}{4}\\right),\n$$\nso the score is\n$$\nS=1-\\sum_{t} \\frac{n_{t}}{4}\\,h_{2}(q_{t})-\\beta\\,h_{2}\\!\\left(\\frac{n_{0}}{4}\\right),\\qquad \\beta=0.2,\n$$\nwhere $h_{2}(p)=-p\\log_{2}p-(1-p)\\log_{2}(1-p)$.\n\nEnumerate all distinct partitions up to swapping the labels $T=0,1$.\n\n1) Two-two splits:\n- $\\{a,b\\}\\mid\\{c,d\\}$: $q_{\\text{low}}=\\frac{0.1+0.2}{2}=0.15$, $q_{\\text{high}}=\\frac{0.8+0.9}{2}=0.85$, hence $h_{2}(0.15)=h_{2}(0.85)$. Then\n$$\nH(Y|T)=\\frac{1}{2}h_{2}(0.15)+\\frac{1}{2}h_{2}(0.85)=h_{2}(0.15),\\quad H(T)=h_{2}\\!\\left(\\frac{1}{2}\\right)=1,\n$$\nso\n$$\nS=1-h_{2}(0.15)-0.2\\times 1=0.8-h_{2}(0.15).\n$$\nCompute $h_{2}(0.15)=-0.15\\log_{2}(0.15)-0.85\\log_{2}(0.85)\\approx 0.609840304716,$ hence\n$$\nS\\approx 0.8-0.609840304716=0.190159695284.\n$$\n- $\\{a,c\\}\\mid\\{b,d\\}$: $q=\\frac{0.1+0.8}{2}=0.45$ and $\\frac{0.2+0.9}{2}=0.55$, so $H(Y|T)=h_{2}(0.45)$ and $H(T)=1$. Then\n$$\nS=1-h_{2}(0.45)-0.2,\\quad h_{2}(0.45)\\approx 0.992454708 \\;\\Rightarrow\\; S\\approx -0.192454708.\n$$\n- $\\{a,d\\}\\mid\\{b,c\\}$: both groups have $q=0.5$, so $I(T;Y)=0$ and $S=-0.2$.\n\nThe best two-two split is $\\{a,b\\}\\mid\\{c,d\\}$ with $S\\approx 0.190159695284$.\n\n2) One-three splits (here $H(T)=h_{2}(1/4)=0.811278124459$, so the penalty term is $0.2\\times 0.811278124459\\approx 0.162255624892$):\n- Singleton $\\{a\\}$: $h_{2}(0.1)\\approx 0.468995593$, other group mean $q=\\frac{0.2+0.8+0.9}{3}=\\frac{19}{30}\\approx 0.633333333$, $h_{2}(q)\\approx 0.947161163$. Then\n$$\nH(Y|T)=\\frac{1}{4}h_{2}(0.1)+\\frac{3}{4}h_{2}\\!\\left(\\tfrac{19}{30}\\right)\\approx 0.827619771,\n$$\nso $I(T;Y)\\approx 0.172380229$ and\n$$\nS\\approx 0.172380229-0.162255625\\approx 0.010124604.\n$$\n- Singleton $\\{b\\}$: $h_{2}(0.2)=0.721928095$, other group $q=\\frac{0.1+0.8+0.9}{3}=0.6$, $h_{2}(0.6)\\approx 0.970950594$. Then $I(T;Y)\\approx 0.091305031$ and $S\\approx -0.070950594$.\n- Singleton $\\{c\\}$ is symmetric to $\\{b\\}$: $S\\approx -0.070950594$.\n- Singleton $\\{d\\}$ is symmetric to $\\{a\\}$: $S\\approx 0.010124604$.\n\nThus the maximal score over all one-bit deterministic encodings is achieved by the partition $\\{a,b\\}\\mid\\{c,d\\}$, yielding\n$$\nS_{\\max}=0.8-h_{2}(0.15)\\approx 0.190159695284.\n$$\nRounded to three significant figures, this is $0.190$.", "answer": "$$\\boxed{0.190}$$", "id": "1631238"}, {"introduction": "The optimal compression strategy is not a one-size-fits-all solution; it fundamentally depends on how much we value relevance over compression, as controlled by the parameter $\\beta$. This practice explores the dynamic nature of this trade-off by asking you to find a \"critical value\" of $\\beta$. At this critical point, the optimal data representation transitions from a highly compressed, simple form to a more complex one that retains more relevant information, illustrating how the solution evolves across the trade-off spectrum [@problem_id:132061].", "problem": "The Information Bottleneck (IB) method provides a framework for finding an optimal trade-off between compression and preservation of relevant information. Given a joint probability distribution $p(x,y)$ for a source variable $X$ and a relevance variable $Y$, the goal is to find a compressed representation, or \"bottleneck\" variable, $T$ that is as informative as possible about $Y$ while being as simple as possible with respect to $X$. The variables are assumed to form a Markov chain $Y \\leftrightarrow X \\leftrightarrow T$.\n\nThe trade-off is formalized by minimizing the Lagrangian:\n$$\n\\mathcal{L}[p(t|x)] = I(X;T) - \\beta I(T;Y)\n$$\nwhere $I(X;T)$ is the mutual information between $X$ and $T$ (a measure of compression cost), $I(T;Y)$ is the mutual information between $T$ and $Y$ (a measure of relevance), and $\\beta \\ge 0$ is a Lagrange multiplier that controls the trade-off. A small $\\beta$ emphasizes compression (low $I(X;T)$), while a large $\\beta$ emphasizes relevance (high $I(T;Y)$). All logarithms are taken to base 2.\n\nConsider a system where the source variable $X$ is drawn from the set $\\{1, 2, 3, 4\\}$ with a uniform probability distribution, $p(x) = 1/4$ for each $x$. The relevance variable $Y$ is determined by a deterministic function of $X$:\n$$\nY = f(X) = X^2 \\pmod 5\n$$\nFor this system, the optimal encoder $p(t|x)$ that minimizes $\\mathcal{L}$ corresponds to a deterministic partitioning of the values of $X$ into clusters, where each cluster is a value of $T$. The structure of this optimal partition depends on the value of $\\beta$.\n\nFind the critical value of the trade-off parameter, $\\beta_c$, at which the optimal representation of $X$ transitions from a trivial single-cluster representation (maximum compression) to a non-trivial multi-cluster representation.", "solution": "We have $X\\in\\{1,2,3,4\\}$ uniformly, and $Y=f(X)=X^2\\pmod5$.  Compute the mapping:\n$$\nf(1)=1,\\quad f(2)=4,\\quad f(3)=4,\\quad f(4)=1.\n$$\nThus $Y\\in\\{1,4\\}$ with $p(1)=p(4)=1/2$.\n\n1. Trivial encoder: all $x$ map to a single $t_0$.  Then\n$$\nI(X;T)=0,\\quad I(T;Y)=0,\\quad \\mathcal L_{\\rm trivial}=0-\\beta\\cdot0=0.\n$$\n\n2. Two-cluster encoder: group $\\{1,4\\}\\to t_1$, $\\{2,3\\}\\to t_2$.  This coincides with $T=Y$, so\n$$\nI(X;T)=H(T)-H(T|X)=H(Y)=1,\\qquad I(T;Y)=H(Y)-H(Y|T)=1.\n$$\nHence\n$$\n\\mathcal L_{2\\text{-cluster}}=I(X;T)-\\beta\\,I(T;Y)=1-\\beta.\n$$\n\n3. Compare:\n- For $\\beta<1$, $\\mathcal L_{2\\text{-cluster}}=1-\\beta>0=\\mathcal L_{\\rm trivial}$, so trivial encoding is optimal.\n- For $\\beta>1$, $\\mathcal L_{2\\text{-cluster}}<0=\\mathcal L_{\\rm trivial}$, so two-cluster encoding is optimal.\n\nThus the critical trade-off is\n$$\n\\beta_c=1.\n$$", "answer": "$$\\boxed{1}$$", "id": "132061"}]}