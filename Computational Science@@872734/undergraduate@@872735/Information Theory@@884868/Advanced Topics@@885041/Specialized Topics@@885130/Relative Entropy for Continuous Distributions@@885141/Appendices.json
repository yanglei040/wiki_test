{"hands_on_practices": [{"introduction": "We begin with a foundational exercise to build intuition for the Kullback-Leibler (KL) divergence. By calculating the divergence between two simple uniform distributions, we can clearly see how this measure quantifies the information gained when we refine our probabilistic model from a broader range of possibilities to a more specific one. This practice highlights the core mechanism of the KL divergence formula in a straightforward context.", "problem": "Suppose an initial probabilistic model, denoted by $P$, for a continuous random variable $X$ assumes that $X$ is uniformly distributed on the interval $[a, b]$. Subsequently, a revised and more accurate model, denoted by $Q$, proposes that $X$ is instead uniformly distributed on a larger interval $[c, d]$. It is given that the initial interval is fully contained within the revised interval, i.e., $c \\le a < b \\le d$.\n\nThe Kullback-Leibler (KL) divergence, $D_{KL}(P || Q)$, is a measure of the information gain when one revises one's beliefs from a prior probability distribution $P$ to a posterior probability distribution $Q$. Calculate the KL divergence $D_{KL}(P || Q)$ from the initial model $P$ to the revised model $Q$.\n\nExpress your answer as a symbolic expression in terms of $a, b, c,$ and $d$.", "solution": "Let $P$ be $\\operatorname{Unif}[a,b]$ and $Q$ be $\\operatorname{Unif}[c,d]$ with $c \\le a < b \\le d$. The corresponding densities are\n$$\np(x)=\\begin{cases}\n\\frac{1}{b-a}, & x\\in[a,b],\\\\\n0, & \\text{otherwise},\n\\end{cases}\n\\quad\nq(x)=\\begin{cases}\n\\frac{1}{d-c}, & x\\in[c,d],\\\\\n0, & \\text{otherwise}.\n\\end{cases}\n$$\nThe Kullback-Leibler divergence for continuous distributions is\n$$\nD_{KL}(P\\|Q)=\\int_{-\\infty}^{\\infty} p(x)\\,\\ln\\!\\left(\\frac{p(x)}{q(x)}\\right)\\,dx.\n$$\nSince $[a,b]\\subseteq[c,d]$, we have $q(x)=\\frac{1}{d-c}$ for all $x\\in[a,b]$, and $p(x)=0$ outside $[a,b]$. Therefore,\n$$\nD_{KL}(P\\|Q)=\\int_{a}^{b}\\frac{1}{b-a}\\,\\ln\\!\\left(\\frac{\\frac{1}{b-a}}{\\frac{1}{d-c}}\\right)\\,dx\n=\\int_{a}^{b}\\frac{1}{b-a}\\,\\ln\\!\\left(\\frac{d-c}{b-a}\\right)\\,dx.\n$$\nThe integrand is constant, so\n$$\nD_{KL}(P\\|Q)=\\ln\\!\\left(\\frac{d-c}{b-a}\\right)\\int_{a}^{b}\\frac{1}{b-a}\\,dx\n=\\ln\\!\\left(\\frac{d-c}{b-a}\\right)\\cdot\\frac{b-a}{b-a}\n=\\ln\\!\\left(\\frac{d-c}{b-a}\\right).\n$$", "answer": "$$\\boxed{\\ln\\!\\left(\\frac{d-c}{b-a}\\right)}$$", "id": "1655218"}, {"introduction": "In practical data modeling, we often approximate a true distribution with a simpler, more tractable one. This exercise provides a concrete example by calculating the KL divergence from a standard normal distribution to a standard Laplace distribution. The result quantifies the information lost, or the \"cost,\" of using the Laplace model as a substitute for the normal model, a common task in statistical inference and machine learning.", "problem": "In information theory, the Kullback-Leibler (KL) divergence, also known as relative entropy, measures how one probability distribution diverges from a second, reference probability distribution. For two continuous probability distributions $P$ and $Q$ with corresponding probability density functions (PDFs) $p(x)$ and $q(x)$ defined over the real numbers, the KL divergence of $Q$ from $P$ is defined as:\n$$D_{\\text{KL}}(P \\| Q) = \\int_{-\\infty}^{\\infty} p(x) \\ln\\left(\\frac{p(x)}{q(x)}\\right) dx$$\nConsider a random variable whose true distribution is a standard normal distribution, which we will denote as $P$. Its PDF is given by:\n$$p(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2}\\right)$$\nSuppose we approximate this distribution using a standard Laplace distribution, which we will denote as $Q$. Its PDF is given by:\n$$q(x) = \\frac{1}{2} \\exp(-|x|)$$\nCalculate the KL divergence $D_{\\text{KL}}(P \\| Q)$, which quantifies the information lost when using $Q$ to approximate $P$. Express your answer as a single closed-form analytic expression.", "solution": "We start from the definition of the KL divergence for continuous distributions:\n$$\nD_{\\text{KL}}(P \\| Q) = \\int_{-\\infty}^{\\infty} p(x)\\,\\ln\\!\\left(\\frac{p(x)}{q(x)}\\right)\\,dx\n= \\mathbb{E}_{P}[\\ln p(X)] - \\mathbb{E}_{P}[\\ln q(X)].\n$$\nFor the standard normal $P$ with $p(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\!\\left(-\\frac{x^{2}}{2}\\right)$, compute\n$$\n\\ln p(x) = -\\frac{1}{2}\\ln(2\\pi) - \\frac{x^{2}}{2},\n$$\nso\n$$\n\\mathbb{E}_{P}[\\ln p(X)] = -\\frac{1}{2}\\ln(2\\pi) - \\frac{1}{2}\\mathbb{E}_{P}[X^{2}] = -\\frac{1}{2}\\ln(2\\pi) - \\frac{1}{2},\n$$\nsince $\\mathbb{E}_{P}[X^{2}] = 1$ for a standard normal.\n\nFor the standard Laplace $Q$ with $q(x) = \\frac{1}{2}\\exp(-|x|)$, we have\n$$\n\\ln q(x) = -\\ln 2 - |x|,\n$$\nthus\n$$\n\\mathbb{E}_{P}[\\ln q(X)] = -\\ln 2 - \\mathbb{E}_{P}[|X|].\n$$\nTo evaluate $\\mathbb{E}_{P}[|X|]$ with $X \\sim \\mathcal{N}(0,1)$, use symmetry:\n$$\n\\mathbb{E}_{P}[|X|] = 2\\int_{0}^{\\infty} x\\,\\frac{1}{\\sqrt{2\\pi}}\\exp\\!\\left(-\\frac{x^{2}}{2}\\right)\\,dx.\n$$\nLet $u = \\frac{x^{2}}{2}$, so $du = x\\,dx$. Then\n$$\n\\mathbb{E}_{P}[|X|] = \\frac{2}{\\sqrt{2\\pi}}\\int_{0}^{\\infty} \\exp(-u)\\,du = \\frac{2}{\\sqrt{2\\pi}}\\cdot 1 = \\sqrt{\\frac{2}{\\pi}}.\n$$\nTherefore,\n$$\nD_{\\text{KL}}(P \\| Q) = \\left(-\\frac{1}{2}\\ln(2\\pi) - \\frac{1}{2}\\right) - \\left(-\\ln 2 - \\sqrt{\\frac{2}{\\pi}}\\right)\n= \\sqrt{\\frac{2}{\\pi}} + \\ln 2 - \\frac{1}{2}\\ln(2\\pi) - \\frac{1}{2}.\n$$\nSimplifying the logarithms,\n$$\n\\ln 2 - \\frac{1}{2}\\ln(2\\pi) = \\frac{1}{2}\\ln 4 - \\frac{1}{2}\\ln(2\\pi) = -\\frac{1}{2}\\ln\\!\\left(\\frac{2\\pi}{4}\\right) = -\\frac{1}{2}\\ln\\!\\left(\\frac{\\pi}{2}\\right),\n$$\nwhich yields the closed-form expression\n$$\nD_{\\text{KL}}(P \\| Q) = \\sqrt{\\frac{2}{\\pi}} - \\frac{1}{2}\\ln\\!\\left(\\frac{\\pi}{2}\\right) - \\frac{1}{2}.\n$$", "answer": "$$\\boxed{\\sqrt{\\frac{2}{\\pi}}-\\frac{1}{2}\\ln\\!\\left(\\frac{\\pi}{2}\\right)-\\frac{1}{2}}$$", "id": "1655225"}, {"introduction": "A key property of KL divergence is its asymmetry, meaning $D_{KL}(p||q)$ is generally not equal to $D_{KL}(q||p)$. This exercise introduces a symmetrized alternative, the Jeffreys divergence, and applies it to one of the most important families of distributions: the normal distribution. Calculating this for two Gaussians with different means reveals a beautifully simple relationship between information-theoretic divergence and the statistical distance between the distributions' parameters.", "problem": "In information theory, one way to measure the \"distance\" or dissimilarity between two probability distributions is through divergence measures. Consider two continuous probability density functions, $p(x)$ and $q(x)$, defined for a random variable $x$ over the real line. The Kullback-Leibler (KL) divergence, or relative entropy, from $q$ to $p$ is defined as:\n$$D_{KL}(p||q) = \\int_{-\\infty}^{\\infty} p(x) \\ln\\left(\\frac{p(x)}{q(x)}\\right) dx$$\nThe KL divergence is not symmetric, meaning $D_{KL}(p||q) \\neq D_{KL}(q||p)$ in general. To create a symmetric measure, the Jeffreys divergence, $J(p, q)$, is defined as the sum of the two KL divergences:\n$$J(p, q) = D_{KL}(p||q) + D_{KL}(q||p)$$\nLet $p(x)$ be the probability density function for a univariate normal distribution with mean $\\mu_A$ and variance $\\sigma^2$. Let $q(x)$ be the probability density function for another univariate normal distribution with a different mean $\\mu_B$ but the same variance $\\sigma^2$.\n\nDetermine the Jeffreys divergence $J(p, q)$ for these two normal distributions. Express your answer as a symbolic expression in terms of $\\mu_A$, $\\mu_B$, and $\\sigma$.", "solution": "We begin with the definition of the Kullback-Leibler divergence:\n$$\nD_{KL}(p\\|q)=\\int_{-\\infty}^{\\infty} p(x)\\,\\ln\\!\\left(\\frac{p(x)}{q(x)}\\right)\\,dx=\\mathbb{E}_{p}\\!\\left[\\ln p(X)-\\ln q(X)\\right].\n$$\nLet $p(x)$ and $q(x)$ be the densities of univariate normal distributions with the same variance $\\sigma^{2}$ and means $\\mu_{A}$ and $\\mu_{B}$, respectively:\n$$\np(x)=\\frac{1}{\\sqrt{2\\pi}\\,\\sigma}\\,\\exp\\!\\left(-\\frac{(x-\\mu_{A})^{2}}{2\\sigma^{2}}\\right),\\quad\nq(x)=\\frac{1}{\\sqrt{2\\pi}\\,\\sigma}\\,\\exp\\!\\left(-\\frac{(x-\\mu_{B})^{2}}{2\\sigma^{2}}\\right).\n$$\nTaking logarithms,\n$$\n\\ln p(x)=-\\ln(\\sqrt{2\\pi}\\,\\sigma)-\\frac{(x-\\mu_{A})^{2}}{2\\sigma^{2}},\\quad\n\\ln q(x)=-\\ln(\\sqrt{2\\pi}\\,\\sigma)-\\frac{(x-\\mu_{B})^{2}}{2\\sigma^{2}}.\n$$\nTherefore,\n$$\n\\ln p(x)-\\ln q(x)=-\\frac{(x-\\mu_{A})^{2}-(x-\\mu_{B})^{2}}{2\\sigma^{2}}=\\frac{(x-\\mu_{B})^{2}-(x-\\mu_{A})^{2}}{2\\sigma^{2}}.\n$$\nTaking expectation under $p$,\n$$\nD_{KL}(p\\|q)=\\mathbb{E}_{p}\\!\\left[\\frac{(X-\\mu_{B})^{2}-(X-\\mu_{A})^{2}}{2\\sigma^{2}}\\right]\n=\\frac{1}{2\\sigma^{2}}\\left(\\mathbb{E}_{p}[(X-\\mu_{B})^{2}]-\\mathbb{E}_{p}[(X-\\mu_{A})^{2}]\\right).\n$$\nUsing $\\mathbb{E}_{p}[X]=\\mu_{A}$ and $\\operatorname{Var}_{p}(X)=\\sigma^{2}$, and the identity $\\mathbb{E}[(X-a)^{2}]=\\operatorname{Var}(X)+(\\mathbb{E}[X]-a)^{2}$, we obtain\n$$\n\\mathbb{E}_{p}[(X-\\mu_{A})^{2}]=\\sigma^{2},\\quad\n\\mathbb{E}_{p}[(X-\\mu_{B})^{2}]=\\sigma^{2}+(\\mu_{A}-\\mu_{B})^{2}.\n$$\nHence,\n$$\nD_{KL}(p\\|q)=\\frac{1}{2\\sigma^{2}}\\left(\\sigma^{2}+(\\mu_{A}-\\mu_{B})^{2}-\\sigma^{2}\\right)\n=\\frac{(\\mu_{A}-\\mu_{B})^{2}}{2\\sigma^{2}}.\n$$\nBy symmetry of the roles of $p$ and $q$ in the above derivation (interchanging $\\mu_{A}$ and $\\mu_{B}$ leaves the expression unchanged), we also have\n$$\nD_{KL}(q\\|p)=\\frac{(\\mu_{B}-\\mu_{A})^{2}}{2\\sigma^{2}}=\\frac{(\\mu_{A}-\\mu_{B})^{2}}{2\\sigma^{2}}.\n$$\nTherefore, the Jeffreys divergence, defined as $J(p,q)=D_{KL}(p\\|q)+D_{KL}(q\\|p)$, is\n$$\nJ(p,q)=\\frac{(\\mu_{A}-\\mu_{B})^{2}}{2\\sigma^{2}}+\\frac{(\\mu_{A}-\\mu_{B})^{2}}{2\\sigma^{2}}=\\frac{(\\mu_{A}-\\mu_{B})^{2}}{\\sigma^{2}}.\n$$", "answer": "$$\\boxed{\\frac{(\\mu_{A}-\\mu_{B})^{2}}{\\sigma^{2}}}$$", "id": "1655241"}]}