{"hands_on_practices": [{"introduction": "The foundation of information geometry is the Fisher information metric, which quantifies how much information an observable variable provides about an unknown parameter. Our first practice involves calculating this fundamental quantity for a geometric distribution, a common model for processes like waiting for a specific event to occur. This exercise will provide a concrete understanding of how the metric is derived from the log-likelihood function [@problem_id:1631455].", "problem": "A quality control engineer is testing a batch of newly manufactured microchips. The probability that any given chip is defective (a 'success' in this statistical context) is $p$, where $0  p  1$. The chips are tested one by one. Let the random variable $K$ be the number of chips tested until the first defective one is found. The process is modeled by a geometric distribution, where the probability mass function for $K$ is given by:\n$$P(K=k; p) = (1-p)^{k-1}p$$\nfor $k = 1, 2, 3, \\ldots$.\n\nThe Fisher information, $I(p)$, quantifies the amount of information that the observable random variable $K$ carries about the unknown parameter $p$. It is a fundamental concept in statistical inference and information geometry, and is crucial for determining the best possible precision for estimating the parameter $p$.\n\nDetermine the Fisher information $I(p)$ for the parameter $p$ of this geometric distribution. Express your answer as a function of $p$.", "solution": "We have a single observation $K$ from the geometric distribution with pmf $P(K=k;p)=(1-p)^{k-1}p$ for $k\\in\\{1,2,\\ldots\\}$. The log-likelihood for parameter $p$ is\n$$\n\\ell(p;K)=\\ln P(K; p)=\\ln p+(K-1)\\ln(1-p).\n$$\nDifferentiate with respect to $p$ to obtain the score:\n$$\n\\frac{\\partial \\ell}{\\partial p}=\\frac{1}{p}+(K-1)\\frac{\\partial}{\\partial p}\\ln(1-p)=\\frac{1}{p}-\\frac{K-1}{1-p}.\n$$\nDifferentiate again to get the observed information (negative of the second derivative of the log-likelihood):\n$$\n\\frac{\\partial^{2} \\ell}{\\partial p^{2}}=-\\frac{1}{p^{2}}-(K-1)\\frac{\\partial}{\\partial p}\\left(\\frac{1}{1-p}\\right)=-\\frac{1}{p^{2}}-\\frac{K-1}{(1-p)^{2}}.\n$$\nThe Fisher information is the negative expectation of this second derivative:\n$$\nI(p)=-\\operatorname{E}\\!\\left[\\frac{\\partial^{2} \\ell}{\\partial p^{2}}\\right]=\\frac{1}{p^{2}}+\\frac{\\operatorname{E}[K-1]}{(1-p)^{2}}.\n$$\nUsing the known mean of the geometric distribution with support starting at $1$, $\\operatorname{E}[K]=\\frac{1}{p}$, we have\n$$\n\\operatorname{E}[K-1]=\\frac{1}{p}-1=\\frac{1-p}{p}.\n$$\nSubstitute this into the expression for $I(p)$:\n$$\nI(p)=\\frac{1}{p^{2}}+\\frac{(1-p)/p}{(1-p)^{2}}=\\frac{1}{p^{2}}+\\frac{1}{p(1-p)}.\n$$\nCombine over the common denominator $p^{2}(1-p)$:\n$$\nI(p)=\\frac{1-p+p}{p^{2}(1-p)}=\\frac{1}{p^{2}(1-p)}.\n$$\nTherefore, the Fisher information for $p$ is $I(p)=\\frac{1}{p^{2}(1-p)}$.", "answer": "$$\\boxed{\\frac{1}{p^{2}(1-p)}}$$", "id": "1631455"}, {"introduction": "Not all probability distributions are well-behaved; some, like the Cauchy distribution, lack a defined mean or variance. This next exercise challenges us to explore whether information can still be meaningfully quantified in such cases. By calculating the Fisher information for the Cauchy distribution's location parameter, we can investigate the robustness of this measure and see that it can exist even when other statistical moments do not [@problem_id:1631488].", "problem": "In statistical physics and information theory, the Cauchy distribution is a continuous probability distribution often used to model resonance phenomena and is notable for its heavy tails. A random variable $X$ follows a Cauchy distribution with location parameter $\\theta$ and a fixed scale parameter of 1 if its Probability Density Function (PDF) is given by:\n$$f(x; \\theta) = \\frac{1}{\\pi(1 + (x-\\theta)^2)}$$\nfor $x \\in (-\\infty, \\infty)$. This distribution has the peculiar property that its expected value and variance are undefined.\n\nThe Fisher information, $I(\\theta)$, quantifies the amount of information that an observable random variable $X$ carries about an unknown parameter $\\theta$ upon which the probability of $X$ depends. For a single observation, it is defined by the expectation of the squared score:\n$$I(\\theta) = E\\left[ \\left( \\frac{\\partial}{\\partial \\theta} \\ln f(X; \\theta) \\right)^2 \\right]$$\nwhere the expectation is taken with respect to the distribution $f(x; \\theta)$.\n\nGiven the unusual properties of the Cauchy distribution, your task is to determine the Fisher information $I(\\theta)$ for its location parameter $\\theta$. Which of the following statements is correct?\n\nA. $I(\\theta) = 1/2$\n\nB. $I(\\theta) = 1$\n\nC. $I(\\theta) = 2$\n\nD. The calculation involves an integral that does not converge, hence the Fisher information is undefined.\n\nE. $I(\\theta) = 0$", "solution": "The problem asks for the Fisher information $I(\\theta)$ for the location parameter $\\theta$ of a Cauchy distribution with PDF $f(x; \\theta) = \\frac{1}{\\pi(1 + (x-\\theta)^2)}$.\n\nThe definition of Fisher information is given as:\n$$I(\\theta) = E\\left[ \\left( \\frac{\\partial}{\\partial \\theta} \\ln f(X; \\theta) \\right)^2 \\right]$$\nThis can be written as an integral:\n$$I(\\theta) = \\int_{-\\infty}^{\\infty} \\left( \\frac{\\partial}{\\partial \\theta} \\ln f(x; \\theta) \\right)^2 f(x; \\theta) \\, dx$$\n\nFirst, we find the log-likelihood, $\\ln f(x; \\theta)$:\n$$\\ln f(x; \\theta) = \\ln\\left( \\frac{1}{\\pi(1 + (x-\\theta)^2)} \\right) = -\\ln(\\pi) - \\ln(1 + (x-\\theta)^2)$$\n\nNext, we compute the score, which is the partial derivative of the log-likelihood with respect to $\\theta$:\n$$\\frac{\\partial}{\\partial \\theta} \\ln f(x; \\theta) = \\frac{\\partial}{\\partial \\theta} \\left( -\\ln(\\pi) - \\ln(1 + (x-\\theta)^2) \\right)$$\n$$= 0 - \\frac{1}{1 + (x-\\theta)^2} \\cdot \\frac{\\partial}{\\partial \\theta} (1 + (x-\\theta)^2)$$\n$$= - \\frac{1}{1 + (x-\\theta)^2} \\cdot (2(x-\\theta) \\cdot (-1))$$\n$$= \\frac{2(x-\\theta)}{1 + (x-\\theta)^2}$$\n\nNow, we square the score:\n$$\\left( \\frac{\\partial}{\\partial \\theta} \\ln f(x; \\theta) \\right)^2 = \\left( \\frac{2(x-\\theta)}{1 + (x-\\theta)^2} \\right)^2 = \\frac{4(x-\\theta)^2}{(1 + (x-\\theta)^2)^2}$$\n\nWe can now substitute this expression and the PDF into the integral for the Fisher information:\n$$I(\\theta) = \\int_{-\\infty}^{\\infty} \\left( \\frac{4(x-\\theta)^2}{(1 + (x-\\theta)^2)^2} \\right) \\cdot \\left( \\frac{1}{\\pi(1 + (x-\\theta)^2)} \\right) \\, dx$$\n$$I(\\theta) = \\frac{4}{\\pi} \\int_{-\\infty}^{\\infty} \\frac{(x-\\theta)^2}{(1 + (x-\\theta)^2)^3} \\, dx$$\n\nTo evaluate this integral, we perform a substitution. Let $u = x - \\theta$. Then $du = dx$. The limits of integration from $-\\infty$ to $\\infty$ for $x$ remain the same for $u$.\n$$I(\\theta) = \\frac{4}{\\pi} \\int_{-\\infty}^{\\infty} \\frac{u^2}{(1 + u^2)^3} \\, du$$\n\nBefore proceeding, it is crucial to check if this integral converges. For large values of $|u|$, the integrand behaves as $\\frac{u^2}{(u^2)^3} = \\frac{u^2}{u^6} = \\frac{1}{u^4}$. The integral $\\int_a^{\\infty} \\frac{1}{u^p} \\, du$ converges if and only if $p  1$. In our case, $p=4$, so the integral converges. This rules out option D.\n\nTo compute the value of the integral, we can use a standard result derived from integration by parts. Let $J_n = \\int_{-\\infty}^{\\infty} \\frac{1}{(1 + u^2)^n} \\, du$. A recurrence relation can be found: $J_{n+1} = \\frac{2n-1}{2n} J_n$.\nThe base case is $J_1 = \\int_{-\\infty}^{\\infty} \\frac{1}{1 + u^2} \\, du = [\\arctan(u)]_{-\\infty}^{\\infty} = \\frac{\\pi}{2} - (-\\frac{\\pi}{2}) = \\pi$.\n\nUsing the recurrence relation:\n$J_2 = \\frac{2(1)-1}{2(1)} J_1 = \\frac{1}{2} \\pi$.\n$J_3 = \\frac{2(2)-1}{2(2)} J_2 = \\frac{3}{4} \\left(\\frac{\\pi}{2}\\right) = \\frac{3\\pi}{8}$.\n\nThe integral we need to evaluate is $\\int_{-\\infty}^{\\infty} \\frac{u^2}{(1 + u^2)^3} \\, du$. We can rewrite the numerator:\n$$\\int_{-\\infty}^{\\infty} \\frac{u^2}{(1 + u^2)^3} \\, du = \\int_{-\\infty}^{\\infty} \\frac{(1+u^2) - 1}{(1 + u^2)^3} \\, du$$\n$$= \\int_{-\\infty}^{\\infty} \\left( \\frac{1}{(1 + u^2)^2} - \\frac{1}{(1 + u^2)^3} \\right) \\, du$$\n$$= \\int_{-\\infty}^{\\infty} \\frac{1}{(1 + u^2)^2} \\, du - \\int_{-\\infty}^{\\infty} \\frac{1}{(1 + u^2)^3} \\, du$$\n$$= J_2 - J_3 = \\frac{\\pi}{2} - \\frac{3\\pi}{8} = \\frac{4\\pi - 3\\pi}{8} = \\frac{\\pi}{8}$$\n\nFinally, we substitute this result back into our expression for $I(\\theta)$:\n$$I(\\theta) = \\frac{4}{\\pi} \\left( \\frac{\\pi}{8} \\right) = \\frac{4}{8} = \\frac{1}{2}$$\n\nThe Fisher information is constant and does not depend on $\\theta$. The value is $1/2$. Therefore, the correct statement is A.", "answer": "$$\\boxed{A}$$", "id": "1631488"}, {"introduction": "With the Fisher metric established, we can explore the geometry it induces on the space of probability distributions. This practice provides a hands-on verification of the information-geometric Pythagorean theorem, a profound result that relates a probability distribution to its 'projection' onto a constrained family of distributions. By calculating Kullback-Leibler divergences, you will see how an optimization problem reveals a beautiful geometric structure analogous to the classical theorem of Pythagoras [@problem_id:1631519].", "problem": "In the field of information geometry, the set of all probability distributions of a given form can be viewed as a smooth manifold. For a discrete random variable with three possible outcomes, the set of all possible probability distributions forms a 2-dimensional statistical manifold, which can be thought of as a simplex in $\\mathbb{R}^3$.\n\nConsider three specific probability distributions, $P$ and $R$, for a random variable that can take one of three states $\\{1, 2, 3\\}$. The distributions are given by the probability vectors:\n- $P = (p_1, p_2, p_3) = (1/2, 1/4, 1/4)$\n- $R = (r_1, r_2, r_3) = (1/3, 1/3, 1/3)$\n\nLet $S$ be a 1-dimensional submanifold within this space, defined by the linear constraint that the probabilities of the first two outcomes are equal. That is, $S = \\{Q = (q_1, q_2, q_3) | q_1 = q_2 \\text{ and } \\sum q_i = 1, q_i  0\\}$.\n\nThe information-geometric Pythagorean theorem relates a point $P$ to its projection on a submanifold $S$. The \"e-projection\" of $P$ onto $S$, which we will denote as $P^*$, is the unique point in $S$ that minimizes the Kullback-Leibler (KL) divergence from $P$ to points in $S$. The KL divergence is defined as $D_{KL}(A||B) = \\sum_{i} a_i \\ln(a_i/b_i)$ for two distributions $A=(a_i)$ and $B=(b_i)$. The theorem states that for the setup described, $D_{KL}(P||R) = D_{KL}(P||P^*) + D_{KL}(P^*||R)$.\n\nYour task is to verify this theorem for the given distributions. First, find the e-projection $P^*$ of $P$ onto the submanifold $S$. Then, compute the values of the three divergences: $A = D_{KL}(P||R)$, $B = D_{KL}(P||P^*)$, and $C = D_{KL}(P^*||R)$.\n\nCalculate the numerical value of the ratio $\\frac{A}{B+C}$. The final result should be an exact value, not a decimal approximation.", "solution": "We are given $P=(p_{1},p_{2},p_{3})=(\\frac{1}{2},\\frac{1}{4},\\frac{1}{4})$, $R=(r_{1},r_{2},r_{3})=(\\frac{1}{3},\\frac{1}{3},\\frac{1}{3})$, and the submanifold $S=\\{Q=(q_{1},q_{2},q_{3}):q_{1}=q_{2},\\,q_{1}+q_{2}+q_{3}=1,\\,q_{i}0\\}$.\n\nTo find the e-projection $P^{*}$ of $P$ onto $S$, we minimize $D_{KL}(P\\|Q)=\\sum_{i=1}^{3}p_{i}\\ln\\!\\left(\\frac{p_{i}}{q_{i}}\\right)$ over $Q\\in S$. Since $\\sum_{i}p_{i}\\ln p_{i}$ is constant with respect to $Q$, this is equivalent to minimizing\n$$\nf(Q)=-\\sum_{i=1}^{3}p_{i}\\ln q_{i}\n$$\nsubject to $q_{1}=q_{2}$ and $q_{1}+q_{2}+q_{3}=1$. Let $q_{1}=q_{2}=t$ and $q_{3}=1-2t$ with $0t\\frac{1}{2}$. Then\n$$\nf(t)=-(p_{1}+p_{2})\\ln t-p_{3}\\ln(1-2t).\n$$\nDifferentiate and set to zero:\n$$\n\\frac{df}{dt}=-\\frac{p_{1}+p_{2}}{t}+\\frac{2p_{3}}{1-2t}=0\n\\quad\\Longrightarrow\\quad\n\\frac{2p_{3}}{1-2t}=\\frac{p_{1}+p_{2}}{t}.\n$$\nCross-multiplying gives\n$$\n2p_{3}t=(p_{1}+p_{2})(1-2t)\n\\;\\Longrightarrow\\;\n2\\big(p_{3}+p_{1}+p_{2}\\big)t=p_{1}+p_{2}\n\\;\\Longrightarrow\\;\n2t=p_{1}+p_{2}\n\\;\\Longrightarrow\\;\nt=\\frac{p_{1}+p_{2}}{2}.\n$$\nSince $p_{1}+p_{2}+p_{3}=1$, it follows that $q_{3}=1-2t=p_{3}$. Therefore\n$$\nP^{*}=\\left(\\frac{p_{1}+p_{2}}{2},\\,\\frac{p_{1}+p_{2}}{2},\\,p_{3}\\right)=\\left(\\frac{3}{8},\\,\\frac{3}{8},\\,\\frac{1}{4}\\right).\n$$\nThe second derivative is\n$$\n\\frac{d^{2}f}{dt^{2}}=\\frac{p_{1}+p_{2}}{t^{2}}+\\frac{4p_{3}}{(1-2t)^{2}}0,\n$$\nso this critical point is the unique minimizer, as required for the e-projection.\n\nNext, compute the divergences $A=D_{KL}(P\\|R)$, $B=D_{KL}(P\\|P^{*})$, and $C=D_{KL}(P^{*}\\|R)$.\n\nFor $A$:\n$$\nA=\\sum_{i=1}^{3}p_{i}\\ln\\!\\left(\\frac{p_{i}}{r_{i}}\\right)\n=\\sum_{i=1}^{3}p_{i}\\ln(3p_{i})\n=\\frac{1}{2}\\ln\\!\\left(\\frac{3}{2}\\right)+\\frac{1}{4}\\ln\\!\\left(\\frac{3}{4}\\right)+\\frac{1}{4}\\ln\\!\\left(\\frac{3}{4}\\right)\n=\\frac{1}{2}\\ln\\!\\left(\\frac{9}{8}\\right).\n$$\n\nFor $B$:\n$$\nB=\\sum_{i=1}^{3}p_{i}\\ln\\!\\left(\\frac{p_{i}}{q_{i}^{*}}\\right)\n=\\frac{1}{2}\\ln\\!\\left(\\frac{\\frac{1}{2}}{\\frac{3}{8}}\\right)+\\frac{1}{4}\\ln\\!\\left(\\frac{\\frac{1}{4}}{\\frac{3}{8}}\\right)+\\frac{1}{4}\\ln\\!\\left(\\frac{\\frac{1}{4}}{\\frac{1}{4}}\\right)\n=\\frac{1}{2}\\ln\\!\\left(\\frac{4}{3}\\right)+\\frac{1}{4}\\ln\\!\\left(\\frac{2}{3}\\right)+0.\n$$\nUsing $\\ln\\!\\left(\\frac{4}{3}\\right)=-\\ln\\!\\left(\\frac{3}{4}\\right)$ and $\\ln\\!\\left(\\frac{2}{3}\\right)=-\\ln\\!\\left(\\frac{3}{2}\\right)$, we get\n$$\nB=-\\frac{1}{2}\\ln\\!\\left(\\frac{3}{4}\\right)-\\frac{1}{4}\\ln\\!\\left(\\frac{3}{2}\\right).\n$$\n\nFor $C$:\n$$\nC=\\sum_{i=1}^{3}q_{i}^{*}\\ln\\!\\left(\\frac{q_{i}^{*}}{r_{i}}\\right)\n=\\sum_{i=1}^{3}q_{i}^{*}\\ln(3q_{i}^{*})\n=\\frac{3}{8}\\ln\\!\\left(\\frac{9}{8}\\right)+\\frac{3}{8}\\ln\\!\\left(\\frac{9}{8}\\right)+\\frac{1}{4}\\ln\\!\\left(\\frac{3}{4}\\right)\n=\\frac{3}{4}\\ln\\!\\left(\\frac{9}{8}\\right)+\\frac{1}{4}\\ln\\!\\left(\\frac{3}{4}\\right).\n$$\nSince $\\ln\\!\\left(\\frac{9}{8}\\right)=\\ln\\!\\left(\\frac{3}{2}\\right)+\\ln\\!\\left(\\frac{3}{4}\\right)$, we can write\n$$\nC=\\frac{3}{4}\\ln\\!\\left(\\frac{3}{2}\\right)+\\ln\\!\\left(\\frac{3}{4}\\right).\n$$\n\nNow sum $B$ and $C$:\n$$\nB+C=\\left[-\\frac{1}{2}\\ln\\!\\left(\\frac{3}{4}\\right)-\\frac{1}{4}\\ln\\!\\left(\\frac{3}{2}\\right)\\right]+\\left[\\frac{3}{4}\\ln\\!\\left(\\frac{3}{2}\\right)+\\ln\\!\\left(\\frac{3}{4}\\right)\\right]\n=\\frac{1}{2}\\ln\\!\\left(\\frac{3}{2}\\right)+\\frac{1}{2}\\ln\\!\\left(\\frac{3}{4}\\right)\n=\\frac{1}{2}\\ln\\!\\left(\\frac{9}{8}\\right).\n$$\nTherefore $A=B+C$, and the requested ratio is\n$$\n\\frac{A}{B+C}=1.\n$$\nThis verifies the information-geometric Pythagorean theorem for the given distributions and submanifold.", "answer": "$$\\boxed{1}$$", "id": "1631519"}]}