{"hands_on_practices": [{"introduction": "The first step in applying the Slepian-Wolf theorem is to master the calculation of the achievable rate region from a given joint probability distribution. This foundational exercise guides you through computing the necessary entropy bounds—$H(X|Y)$, $H(Y|X)$, and $H(X,Y)$—for a pair of correlated binary sources. By working through this problem [@problem_id:1658808], you will build the core computational skills needed to determine the limits of lossless distributed compression.", "problem": "Consider a distributed source coding scenario involving two correlated binary random variables, $X$ and $Y$, each taking values in the set $\\{0, 1\\}$. Their joint Probability Mass Function (PMF), denoted as $p(x, y) = P(X=x, Y=y)$, is given as follows:\n$p(0, 0) = \\frac{1}{3}$\n$p(0, 1) = \\frac{1}{6}$\n$p(1, 0) = \\frac{1}{6}$\n$p(1, 1) = \\frac{1}{3}$\nTwo separate encoders observe sequences of these variables, one for $X$ and one for $Y$. They compress their respective sequences at rates $R_X$ and $R_Y$. The compressed streams are sent to a joint decoder that aims to reconstruct both sequences losslessly (i.e., with arbitrarily low probability of error). The achievable rate region is the set of all pairs $(R_X, R_Y)$ for which this is possible.\n\nWhich of the following sets of inequalities, with rates in bits per symbol and logarithms to base 2, correctly defines this achievable rate region?\n\nA. $R_X \\geq 1$, $R_Y \\geq 1$\n\nB. $R_X \\geq \\log_{2}3 - \\frac{2}{3}$, $R_Y \\geq \\log_{2}3 - \\frac{2}{3}$\n\nC. $R_X + R_Y \\geq \\log_{2}3 + \\frac{1}{3}$\n\nD. $R_X \\geq \\log_{2}3 - \\frac{2}{3}$, $R_Y \\geq \\log_{2}3 - \\frac{2}{3}$, and $R_X + R_Y \\geq \\log_{2}3 + \\frac{1}{3}$\n\nE. $R_X \\geq 1$, $R_Y \\geq 1$, and $R_X + R_Y \\geq \\log_{2}3 + \\frac{1}{3}$", "solution": "The Slepian-Wolf theorem for two correlated discrete memoryless sources states that lossless distributed compression is achievable if and only if the rates satisfy\n$$\nR_{X} \\geq H(X|Y), \\quad R_{Y} \\geq H(Y|X), \\quad R_{X}+R_{Y} \\geq H(X,Y).\n$$\nWe compute the relevant entropies from the given joint PMF.\n\nFirst, compute the marginals:\n$$\nP(X=0) = p(0,0) + p(0,1) = \\frac{1}{3} + \\frac{1}{6} = \\frac{1}{2}, \\quad P(X=1) = \\frac{1}{2},\n$$\nand similarly\n$$\nP(Y=0) = p(0,0) + p(1,0) = \\frac{1}{3} + \\frac{1}{6} = \\frac{1}{2}, \\quad P(Y=1) = \\frac{1}{2}.\n$$\nHence\n$$\nH(X) = -\\sum_{x} P(X=x)\\log_{2} P(X=x) = -2 \\cdot \\frac{1}{2} \\log_{2}\\left(\\frac{1}{2}\\right) = 1,\n$$\nand $H(Y)=1$ by symmetry.\n\nNext, compute the joint entropy:\n$$\nH(X,Y) = -\\sum_{x,y} p(x,y)\\log_{2} p(x,y)\n= -\\left[2 \\cdot \\frac{1}{3}\\log_{2}\\!\\left(\\frac{1}{3}\\right) + 2 \\cdot \\frac{1}{6}\\log_{2}\\!\\left(\\frac{1}{6}\\right)\\right].\n$$\nUse $\\log_{2}\\!\\left(\\frac{1}{3}\\right) = -\\log_{2} 3$ and $\\log_{2}\\!\\left(\\frac{1}{6}\\right) = -\\log_{2} 6 = -\\left(\\log_{2} 3 + 1\\right)$ to get\n$$\nH(X,Y) = \\frac{2}{3}\\log_{2} 3 + \\frac{1}{3}\\log_{2} 6\n= \\frac{2}{3}\\log_{2} 3 + \\frac{1}{3}\\left(\\log_{2} 3 + 1\\right)\n= \\log_{2} 3 + \\frac{1}{3}.\n$$\nTherefore, the conditional entropies are\n$$\nH(X|Y) = H(X,Y) - H(Y) = \\left(\\log_{2} 3 + \\frac{1}{3}\\right) - 1 = \\log_{2} 3 - \\frac{2}{3},\n$$\nand by symmetry\n$$\nH(Y|X) = \\log_{2} 3 - \\frac{2}{3}.\n$$\nSubstituting into the Slepian-Wolf bounds gives the achievable rate region:\n$$\nR_{X} \\geq \\log_{2} 3 - \\frac{2}{3}, \\quad\nR_{Y} \\geq \\log_{2} 3 - \\frac{2}{3}, \\quad\nR_{X} + R_{Y} \\geq \\log_{2} 3 + \\frac{1}{3},\n$$\nwhich corresponds to option D.", "answer": "$$\\boxed{D}$$", "id": "1658808"}, {"introduction": "Theoretical concepts often become clearest when we examine their boundary cases. This practice explores a scenario where one source, $Y$, is a deterministic function of another source, $X$, representing a form of perfect correlation [@problem_id:1658801]. Analyzing this setup reveals the profound implications of side information, showing how the required rate for one encoder can approach zero when the other source provides all necessary information about it at the decoder.", "problem": "Two correlated information sources, denoted by the random variables $X$ and $Y$, are being monitored. The source $X$ can produce one of four possible values from the set $\\mathcal{X} = \\{-3, -1, 1, 3\\}$, with each value occurring with equal probability. The source $Y$ is perfectly correlated with $X$ through the deterministic relationship $Y = X^2$. A data compression system is designed to encode the outputs of these two sources separately, at rates $R_X$ and $R_Y$ respectively, for transmission to a central station where they will be jointly decoded. The goal is to ensure that the original sequences from both sources can be reconstructed without any loss of information.\n\nAccording to the Slepian-Wolf theorem, there is an achievable region of rate pairs $(R_X, R_Y)$ that allows for such lossless reconstruction. If the encoder for source $Y$ is designed to operate at a fixed rate of $R_Y = 0.5$ bits per symbol, what is the absolute minimum rate $R_X$ that must be used for encoding source $X$? Express your answer in bits per symbol, rounded to two significant figures. All logarithms are to be taken in base 2.", "solution": "Let $X$ be uniform over $\\{-3,-1,1,3\\}$, so $P(X=x)=\\frac{1}{4}$ for each of the four values. The mapping $Y=X^{2}$ yields $Y\\in\\{1,9\\}$ with $P(Y=1)=P(X\\in\\{-1,1\\})=\\frac{1}{2}$ and $P(Y=9)=P(X\\in\\{-3,3\\})=\\frac{1}{2}$.\n\nCompute the relevant entropies (all logarithms are base $2$):\n$$\nH(X)=-\\sum_{x}P(x)\\log_{2}P(x)=-4\\cdot \\frac{1}{4}\\log_{2}\\frac{1}{4}=2,\n$$\n$$\nH(Y)=-\\sum_{y}P(y)\\log_{2}P(y)=-2\\cdot \\frac{1}{2}\\log_{2}\\frac{1}{2}=1.\n$$\nSince $Y$ is a deterministic function of $X$, $H(Y|X)=0$. Given $Y$, $X$ is equally likely to be one of two values (the two signs), hence\n$$\nH(X|Y)=1.\n$$\nThe joint entropy is\n$$\nH(X,Y)=H(X)+H(Y|X)=2+0=2.\n$$\n\nThe Slepian-Wolf achievable region for lossless reconstruction requires\n$$\nR_{X}\\geq H(X|Y),\\quad R_{Y}\\geq H(Y|X),\\quad R_{X}+R_{Y}\\geq H(X,Y).\n$$\nSubstituting the values and the fixed $R_{Y}=0.5$ gives\n$$\nR_{X}\\geq 1,\\quad R_{X}\\geq H(X,Y)-R_{Y}=2-0.5=1.5.\n$$\nTherefore the absolute minimum rate for $X$ is\n$$\nR_{X,\\min}=\\max\\{H(X|Y),\\,H(X,Y)-R_{Y}\\}=\\max\\{1,\\,1.5\\}=1.5,\n$$\nwhich in bits per symbol, rounded to two significant figures, is $1.5$.", "answer": "$$\\boxed{1.5}$$", "id": "1658801"}, {"introduction": "In data analysis, 'uncorrelated' is often mistakenly used as a synonym for 'independent'. This practice tackles this critical misconception head-on by presenting a case where two sources are uncorrelated yet statistically dependent [@problem_id:1658814]. By calculating the Slepian-Wolf bounds, you will discover that compression gains are still possible, proving that the benefit of distributed coding hinges on statistical dependence (measured by mutual information), not just the absence of linear correlation.", "problem": "Consider two discrete random sources, $X$ and $Y$, which generate sequences of symbols. These sources are known to be correlated. An experiment establishes their joint probability mass function, $p(x,y)$, for pairs of symbols $(X,Y)$ as follows:\n$p(-1, 1) = \\frac{1}{3}$\n$p(0, 0) = \\frac{1}{3}$\n$p(1, 1) = \\frac{1}{3}$\nand $p(x,y) = 0$ for all other pairs of $(x,y)$.\n\nThe output of the sources consists of long sequences $(X_1, X_2, \\dots, X_n)$ and $(Y_1, Y_2, \\dots, Y_n)$, where each pair $(X_i, Y_i)$ is an independent and identically distributed (i.i.d.) draw from the joint distribution $p(x,y)$ described above. It is a known fact for this distribution that the random variables $X$ and $Y$ are uncorrelated, but they are not statistically independent.\n\nTwo separate encoders are used to compress the sequences from source $X$ and source $Y$ at rates $R_X$ and $R_Y$ respectively. A single joint decoder receives both compressed streams and must be able to losslessly reconstruct both original sequences.\n\nSuppose one desires to encode the sequence from source $Y$ at a rate $R_Y$ exactly equal to its standalone Shannon entropy, $H(Y)$. To guarantee that both sequences can be losslessly jointly decoded, what is the absolute minimum achievable code rate $R_X$ for the sequence from source $X$?\n\nExpress your answer as a single closed-form analytic expression in bits per symbol. Logarithms are to be taken in base 2.", "solution": "We use the Slepian–Wolf lossless distributed source coding region for two correlated discrete memoryless sources $(X,Y)$. The achievable rate region is characterized by\n$$\nR_{X} \\geq H(X|Y), \\quad R_{Y} \\geq H(Y|X), \\quad R_{X} + R_{Y} \\geq H(X,Y),\n$$\nwith all entropies in bits (logarithms base 2).\n\nHere $R_{Y}$ is fixed to $H(Y)$. The absolute minimum $R_{X}$ that still satisfies the region is\n$$\nR_{X,\\min}=\\max\\!\\big\\{H(X|Y),\\, H(X,Y)-R_{Y}\\big\\}=\\max\\!\\big\\{H(X|Y),\\, H(X,Y)-H(Y)\\big\\}.\n$$\nSince $H(X,Y)=H(Y)+H(X|Y)$, this reduces to\n$$\nR_{X,\\min}=H(X|Y).\n$$\n\nWe now compute $H(X|Y)$ from the given joint pmf:\n$$\np(-1,1)=\\frac{1}{3},\\quad p(0,0)=\\frac{1}{3},\\quad p(1,1)=\\frac{1}{3},\\quad \\text{else }0.\n$$\nFirst compute the marginal of $Y$:\n$$\nP(Y=1)=\\frac{1}{3}+\\frac{1}{3}=\\frac{2}{3}, \\quad P(Y=0)=\\frac{1}{3}.\n$$\nNext, the conditional distributions of $X$ given $Y$:\n- If $Y=0$, then the only possible pair is $(0,0)$, so $P(X=0\\mid Y=0)=1$ and $H(X\\mid Y=0)=0$.\n- If $Y=1$, then the possible pairs are $(-1,1)$ and $(1,1)$ with equal probability after conditioning, so $P(X=-1\\mid Y=1)=\\frac{1}{2}$ and $P(X=1\\mid Y=1)=\\frac{1}{2}$, hence $H(X\\mid Y=1)=1$.\n\nTherefore,\n$$\nH(X\\mid Y)=P(Y=0)\\,H(X\\mid Y=0)+P(Y=1)\\,H(X\\mid Y=1)\n=\\frac{1}{3}\\cdot 0+\\frac{2}{3}\\cdot 1=\\frac{2}{3}.\n$$\nSince $R_{Y}=H(Y)$ by stipulation, the minimal feasible $R_{X}$ that guarantees lossless joint decoding is $H(X\\mid Y)=\\frac{2}{3}$ bits per symbol.", "answer": "$$\\boxed{\\frac{2}{3}}$$", "id": "1658814"}]}