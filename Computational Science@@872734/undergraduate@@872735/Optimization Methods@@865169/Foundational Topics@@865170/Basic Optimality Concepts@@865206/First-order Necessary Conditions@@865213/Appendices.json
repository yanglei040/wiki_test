{"hands_on_practices": [{"introduction": "A fundamental application of first-order necessary conditions is in finding the shortest distance from a point to a surface. This practice explores this concept by asking you to find the closest point on an ellipse to a given external point. You will see firsthand how the abstract Lagrange multiplier condition beautifully translates into a clear geometric requirement: the displacement vector must be normal to the constraint surface at the optimal point [@problem_id:3129605].", "problem": "Consider the equality-constrained optimization problem of minimizing the squared Euclidean distance from a fixed point to an ellipse. Let the objective function be $f(x_{1},x_{2}) = (x_{1}-y_{1})^{2} + (x_{2}-y_{2})^{2}$, and the constraint be the ellipse $g(x_{1},x_{2}) = \\frac{x_{1}^{2}}{a^{2}} + \\frac{x_{2}^{2}}{b^{2}} - 1 = 0$, where $a>0$ and $b>0$. A physically meaningful necessary condition for a local constrained minimizer is that the first-order variation of the objective, when restricted to feasible directions, vanishes. Starting from this foundational statement and the differentiability of $f$ and $g$, derive the first-order necessary conditions (Lagrange conditions) for a candidate minimizer $(x_{1}^{\\star},x_{2}^{\\star})$ with a Lagrange multiplier $\\mu^{\\star}$, and explain why these conditions imply that the displacement vector from $y$ to the optimizer is aligned with the normal vector of the ellipse at $(x_{1}^{\\star},x_{2}^{\\star})$.\n\nThen, apply this to the specific instance with parameters $a=4$, $b=3$, and point $y=(9,0)$, that is,\n$$\n\\min_{(x_{1},x_{2})\\in\\mathbb{R}^{2}} \\;\\; (x_{1}-9)^{2} + (x_{2}-0)^{2} \\quad \\text{subject to} \\quad \\frac{x_{1}^{2}}{16} + \\frac{x_{2}^{2}}{9} = 1.\n$$\nUsing only the first-order necessary conditions you derived, find the minimum value of the squared distance. Provide the final answer as a single exact number. No rounding is required.", "solution": "We begin from the foundational statement of equality-constrained optimization: If $f:\\mathbb{R}^{n}\\to\\mathbb{R}$ and $g:\\mathbb{R}^{n}\\to\\mathbb{R}$ are differentiable, and $x^{\\star}$ is a local minimizer of $f$ subject to $g(x)=0$ with $\\nabla g(x^{\\star})\\neq 0$, then the first-order variation of $f$ along any feasible direction must vanish at $x^{\\star}$. Feasible directions $d$ at $x^{\\star}$ satisfy the linearized constraint $\\nabla g(x^{\\star})^{\\top} d = 0$. Therefore, for every such $d$, the directional derivative of $f$ at $x^{\\star}$ along $d$ must satisfy $\\nabla f(x^{\\star})^{\\top} d = 0$. This implies that $\\nabla f(x^{\\star})$ is orthogonal to every vector $d$ orthogonal to $\\nabla g(x^{\\star})$, which yields the classical Lagrange condition\n$$\n\\nabla f(x^{\\star}) + \\mu^{\\star} \\nabla g(x^{\\star}) = 0\n$$\nfor some scalar $\\mu^{\\star}\\in\\mathbb{R}$, together with the feasibility condition $g(x^{\\star})=0$. These are the first-order necessary conditions (Lagrange conditions) for equality-constrained problems.\n\nFor our specific problem, $f(x_{1},x_{2}) = (x_{1}-y_{1})^{2} + (x_{2}-y_{2})^{2}$ with $y=(9,0)$, so $f(x_{1},x_{2})=(x_{1}-9)^{2}+x_{2}^{2}$. The constraint is $g(x_{1},x_{2})=\\frac{x_{1}^{2}}{16}+\\frac{x_{2}^{2}}{9}-1=0$. The gradients are\n$$\n\\nabla f(x_{1},x_{2}) = \\begin{pmatrix} 2(x_{1}-9) \\\\ 2 x_{2} \\end{pmatrix}, \\quad\n\\nabla g(x_{1},x_{2}) = \\begin{pmatrix} \\frac{2 x_{1}}{16} \\\\ \\frac{2 x_{2}}{9} \\end{pmatrix} = \\begin{pmatrix} \\frac{x_{1}}{8} \\\\ \\frac{2 x_{2}}{9} \\end{pmatrix}.\n$$\nThe first-order necessary (Lagrange) conditions read:\n$$\n\\begin{cases}\n2(x_{1}-9) + \\mu \\cdot \\frac{x_{1}}{8} = 0, \\\\\n2 x_{2} + \\mu \\cdot \\frac{2 x_{2}}{9} = 0, \\\\\n\\frac{x_{1}^{2}}{16} + \\frac{x_{2}^{2}}{9} = 1.\n\\end{cases}\n$$\nThe second stationarity equation can be written as\n$$\n2 x_{2} \\left( 1 + \\frac{\\mu}{9} \\right) = 0,\n$$\nwhich implies either $x_{2}=0$ or $\\mu = -9$. If $\\mu=-9$, the first stationarity equation becomes\n$$\n2(x_{1}-9) - 9 \\cdot \\frac{x_{1}}{8} = 0 \\;\\;\\Longleftrightarrow\\;\\; 16(x_{1}-9) - 9 x_{1} = 0 \\;\\;\\Longleftrightarrow\\;\\; 7 x_{1} = 144 \\;\\;\\Longleftrightarrow\\;\\; x_{1} = \\frac{144}{7}.\n$$\nThis candidate does not satisfy the constraint, because\n$$\n\\frac{x_{1}^{2}}{16} \\;=\\; \\frac{(144/7)^{2}}{16} \\;=\\; \\frac{20736}{49 \\cdot 16} \\;>\\; 1,\n$$\nso no feasible point with $\\mu=-9$ exists. Therefore, we must have $x_{2}=0$.\n\nSubstituting $x_{2}=0$ into the constraint gives\n$$\n\\frac{x_{1}^{2}}{16} = 1 \\;\\;\\Longrightarrow\\;\\; x_{1} = \\pm 4.\n$$\nWe now use the first stationarity equation to determine the corresponding Lagrange multipliers and to verify feasibility with respect to optimality. For $x_{1}=4$,\n$$\n2(4-9) + \\mu \\cdot \\frac{4}{8} = 0 \\;\\;\\Longleftrightarrow\\;\\; -10 + \\frac{\\mu}{2} = 0 \\;\\;\\Longleftrightarrow\\;\\; \\mu = 20.\n$$\nFor $x_{1}=-4$,\n$$\n2(-4-9) + \\mu \\cdot \\frac{-4}{8} = 0 \\;\\;\\Longleftrightarrow\\;\\; -26 - \\frac{\\mu}{2} = 0 \\;\\;\\Longleftrightarrow\\;\\; \\mu = -52.\n$$\nBoth candidates satisfy the stationarity and feasibility conditions; to select the minimizer, evaluate the objective:\n$$\nf(4,0) = (4-9)^{2} + 0^{2} = 25, \\quad f(-4,0) = (-4-9)^{2} + 0^{2} = 169.\n$$\nThus, the minimizing point is $(x_{1}^{\\star},x_{2}^{\\star})=(4,0)$, and the minimum squared distance is $25$.\n\nGeometric interpretation (normal alignment): The stationarity condition $\\nabla f(x^{\\star}) + \\mu^{\\star} \\nabla g(x^{\\star})=0$ can be rewritten as\n$$\n2\\big(x^{\\star} - y\\big) = - \\mu^{\\star} \\nabla g(x^{\\star}),\n$$\nwhich shows that the displacement vector from $y$ to the optimal point $x^{\\star}$ is colinear with the constraint normal $\\nabla g(x^{\\star})$. This expresses that the line segment from $y$ to the closest point is normal to the ellipse at the point of tangency.\n\nTherefore, the minimum value of the squared distance is $25$.", "answer": "$$\\boxed{25}$$", "id": "3129605"}, {"introduction": "While equality constraints define a specific surface, many optimization problems involve feasible regions bounded by inequalities. This exercise challenges you to find the closest point in the probability simplex to a given vector, a classic problem in machine learning and statistics. By deriving and applying the full Karush-Kuhn-Tucker (KKT) conditions, you will discover the power of complementary slackness in determining which constraints become active at the optimal solution [@problem_id:3129540].", "problem": "Consider the minimization of a convex differentiable objective over the probability simplex. Let $\\Delta^{5} \\subset \\mathbb{R}^{5}$ denote the set $\\Delta^{5} = \\{ p \\in \\mathbb{R}^{5} \\,:\\, \\mathbf{1}^{\\top} p = 1,\\; p \\ge 0 \\}$, where the inequality is interpreted componentwise. Define the function $f: \\mathbb{R}^{5} \\to \\mathbb{R}$ by\n$$\nf(p) = \\frac{1}{2}\\sum_{i=1}^{5} \\big(p_{i} - v_{i}\\big)^{2},\n$$\nwith a given data vector $v \\in \\mathbb{R}^{5}$. The optimization problem is\n$$\n\\min_{p \\in \\mathbb{R}^{5}} \\; f(p) \\quad \\text{subject to} \\quad \\mathbf{1}^{\\top} p = 1,\\;\\; p \\ge 0,\n$$\nthat is, minimize the squared Euclidean distance to $v$ over the simplex $\\Delta^{5}$.\n\nTasks:\n1) Starting only from the definitions of the Lagrangian for equality and inequality constraints and the concept of first-order necessary conditions, derive the full set of Karush-Kuhn-Tucker (KKT) conditions for this problem. Write down explicitly the stationarity, primal feasibility, dual feasibility, and complementary slackness conditions.\n\n2) Explain, using complementary slackness, how to determine which coordinates of the optimizer become zero. Provide a logical argument that connects the signs of the dual variables with the zero pattern of the primal variables.\n\n3) For the specific vector $v = (0.7,\\; 0.6,\\; 0.4,\\; 0.1,\\; -0.2)$, solve the KKT system exactly to obtain the optimizer $p^{\\star} \\in \\Delta^{5}$. Express your final answer as exact rational numbers. No rounding is required.", "solution": "The problem asks for the minimization of a convex, differentiable function over a convex set defined by linear equality and inequality constraints. The Karush-Kuhn-Tucker (KKT) conditions provide first-order necessary conditions for a solution to be optimal. Since the problem is convex, these necessary conditions are also sufficient for optimality.\n\n### Task 1: Derivation of the KKT Conditions\n\nThe optimization problem can be written in standard form as:\n$$\n\\min_{p \\in \\mathbb{R}^{5}} f(p) = \\frac{1}{2}\\sum_{i=1}^{5} (p_{i} - v_{i})^{2}\n$$\nsubject to:\n$$\nh(p) = \\sum_{i=1}^{5} p_i - 1 = 0 \\\\\ng_i(p) = -p_i \\le 0 \\quad \\text{for } i = 1, \\dots, 5\n$$\nThe Lagrangian function $L(p, \\lambda, \\mu)$ is formed by adding the objective function to a weighted sum of the constraints. We associate a Lagrange multiplier $\\lambda \\in \\mathbb{R}$ with the equality constraint and a vector of Lagrange multipliers $\\mu \\in \\mathbb{R}^5$ with the inequality constraints, where $\\mu = (\\mu_1, \\dots, \\mu_5)$.\n$$\nL(p, \\lambda, \\mu) = f(p) + \\lambda h(p) + \\sum_{i=1}^{5} \\mu_i g_i(p)\n$$\nSubstituting the specific functions, we get:\n$$\nL(p, \\lambda, \\mu) = \\frac{1}{2}\\sum_{i=1}^{5} (p_{i} - v_{i})^{2} + \\lambda \\left(\\sum_{i=1}^{5} p_i - 1\\right) + \\sum_{i=1}^{5} \\mu_i (-p_i)\n$$\nThe KKT conditions are a set of four conditions that must hold at an optimal point $p^{\\star}$ with corresponding multipliers $\\lambda^{\\star}$ and $\\mu^{\\star}$.\n\n1.  **Stationarity**: The gradient of the Lagrangian with respect to the primal variables $p$ must be zero. For each component $p_i$, we have:\n    $$\n    \\frac{\\partial L}{\\partial p_i} = (p_i^{\\star} - v_i) + \\lambda^{\\star} - \\mu_i^{\\star} = 0, \\quad \\text{for } i = 1, \\dots, 5\n    $$\n2.  **Primal Feasibility**: The point $p^{\\star}$ must satisfy all constraints.\n    $$\n    \\sum_{i=1}^{5} p_i^{\\star} = 1 \\\\\n    p_i^{\\star} \\ge 0, \\quad \\text{for } i = 1, \\dots, 5\n    $$\n3.  **Dual Feasibility**: The Lagrange multipliers associated with the inequality constraints must be non-negative.\n    $$\n    \\mu_i^{\\star} \\ge 0, \\quad \\text{for } i = 1, \\dots, 5\n    $$\n4.  **Complementary Slackness**: The product of each inequality multiplier and its corresponding constraint function must be zero.\n    $$\n    \\mu_i^{\\star} g_i(p^{\\star}) = \\mu_i^{\\star} (-p_i^{\\star}) = 0, \\quad \\text{for } i = 1, \\dots, 5\n    $$\n    This is equivalent to $\\mu_i^{\\star} p_i^{\\star} = 0$.\n\n### Task 2: Role of Complementary Slackness\n\nThe complementary slackness condition, $\\mu_i p_i = 0$ for each $i \\in \\{1, \\dots, 5\\}$, is key to determining the structure of the solution. Since both $p_i \\ge 0$ (primal feasibility) and $\\mu_i \\ge 0$ (dual feasibility), the condition $\\mu_i p_i = 0$ implies that for each index $i$, at least one of $p_i$ or $\\mu_i$ must be zero.\n\nWe analyze the implications by combining complementary slackness with the stationarity condition, $p_i = v_i - \\lambda + \\mu_i$.\n\n- **Case 1: The $i$-th coordinate of the optimizer is strictly positive, i.e., $p_i > 0$.**\n  By complementary slackness ($\\mu_i p_i = 0$), we must have $\\mu_i = 0$. Substituting $\\mu_i = 0$ into the stationarity equation gives $p_i = v_i - \\lambda$. Since we assumed $p_i > 0$, it must be that $v_i - \\lambda > 0$, which implies $v_i > \\lambda$.\n  So, if a component $p_i$ of the optimal solution is positive, its corresponding Lagrange multiplier $\\mu_i$ is zero, and its value in the data vector $v_i$ must be strictly greater than the multiplier $\\lambda$.\n\n- **Case 2: The $i$-th coordinate of the optimizer is zero, i.e., $p_i = 0$.**\n  In this case, complementary slackness ($\\mu_i p_i = 0$) is satisfied for any $\\mu_i \\ge 0$. The stationarity equation becomes $0 = v_i - \\lambda + \\mu_i$, which implies $\\mu_i = \\lambda - v_i$. The dual feasibility condition requires $\\mu_i \\ge 0$, so we must have $\\lambda - v_i \\ge 0$, which implies $v_i \\le \\lambda$.\n  So, if a component $p_i$ of the optimal solution is zero, the corresponding value $v_i$ must be less than or equal to the multiplier $\\lambda$.\n\nIn summary, the Lagrange multiplier $\\lambda$ acts as a threshold. For any given component $i$:\n- If $v_i > \\lambda$, then $p_i > 0$ and $\\mu_i=0$.\n- If $v_i \\le \\lambda$, then $p_i = 0$ and $\\mu_i \\ge 0$.\n\nThis provides a clear procedure to find the solution: find the value of $\\lambda$ that partitions the indices $i$ correctly and also satisfies the summation constraint $\\sum p_i = 1$.\n\n### Task 3: Solving for the Optimizer $p^{\\star}$\n\nWe are given the vector $v = (0.7, 0.6, 0.4, 0.1, -0.2)$.\nFrom the analysis in Task 2, the optimal solution $p^{\\star}$ must satisfy $p_i^{\\star} = v_i - \\lambda^{\\star}$ if $p_i^{\\star} > 0$, and $p_i^{\\star} = 0$ if $v_i \\le \\lambda^{\\star}$. This can be compactly written as:\n$$\np_i^{\\star} = \\max(0, v_i - \\lambda^{\\star})\n$$\nThe value of $\\lambda^{\\star}$ is determined by the primal feasibility constraint $\\sum_{i=1}^{5} p_i^{\\star} = 1$:\n$$\n\\sum_{i=1}^{5} \\max(0, v_i - \\lambda^{\\star}) = 1\n$$\nWe need to solve this equation for $\\lambda^{\\star}$. The function $g(\\lambda) = \\sum_{i=1}^{5} \\max(0, v_i - \\lambda)$ is a continuous, piecewise linear, and monotonically decreasing function of $\\lambda$. There is a unique solution for $\\lambda$. A systematic approach is to sort the components of $v$ in descending order (which they already are) and hypothesize which components of $p^{\\star}$ are non-zero. Let $I_{+} = \\{ i \\mid p_i^{\\star}>0 \\}$. For an assumed set $I_{+}$, we have $p_i^{\\star}=0$ for $i \\notin I_{+}$, and for $i \\in I_{+}$, $p_i^{\\star} = v_i - \\lambda^{\\star}$. The sum constraint becomes:\n$$\n\\sum_{i \\in I_{+}} (v_i - \\lambda^{\\star}) = 1 \\implies \\sum_{i \\in I_{+}} v_i - |I_{+}| \\lambda^{\\star} = 1\n$$\nThis gives a formula for $\\lambda^{\\star}$ based on the assumed active set $I_{+}$:\n$$\n\\lambda^{\\star} = \\frac{1}{|I_{+}|} \\left( \\sum_{i \\in I_{+}} v_i - 1 \\right)\n$$\nWe then must check if this value of $\\lambda^{\\star}$ is consistent with our set $I_{+}$, i.e., if $v_i > \\lambda^{\\star}$ for all $i \\in I_{+}$ and $v_i \\le \\lambda^{\\star}$ for all $i \\notin I_{+}$.\n\nLet's start by hypothesizing that only the components of $v$ with the largest values will correspond to non-zero $p_i$. Let's try $I_{+} = \\{1, 2, 3\\}$, meaning we assume $p_1^{\\star}, p_2^{\\star}, p_3^{\\star} > 0$ and $p_4^{\\star}, p_5^{\\star}=0$.\nFirst, we convert $v$ to rational numbers: $v = (\\frac{7}{10}, \\frac{6}{10}, \\frac{4}{10}, \\frac{1}{10}, -\\frac{2}{10})$.\nUnder this hypothesis, $|I_{+}| = 3$. The sum of the corresponding $v_i$ is:\n$$\n\\sum_{i \\in I_{+}} v_i = \\frac{7}{10} + \\frac{6}{10} + \\frac{4}{10} = \\frac{17}{10}\n$$\nNow, we calculate $\\lambda^{\\star}$:\n$$\n\\lambda^{\\star} = \\frac{1}{3} \\left( \\frac{17}{10} - 1 \\right) = \\frac{1}{3} \\left( \\frac{17}{10} - \\frac{10}{10} \\right) = \\frac{1}{3} \\left( \\frac{7}{10} \\right) = \\frac{7}{30}\n$$\nNext, we check for consistency.\nFor $i \\in I_{+} = \\{1, 2, 3\\}$, we must have $v_i > \\lambda^{\\star} = \\frac{7}{30}$.\n- $v_1 = \\frac{7}{10} = \\frac{21}{30}$. Is $\\frac{21}{30} > \\frac{7}{30}$? Yes.\n- $v_2 = \\frac{6}{10} = \\frac{18}{30}$. Is $\\frac{18}{30} > \\frac{7}{30}$? Yes.\n- $v_3 = \\frac{4}{10} = \\frac{12}{30}$. Is $\\frac{12}{30} > \\frac{7}{30}$? Yes.\n\nFor $i \\notin I_{+} = \\{4, 5\\}$, we must have $v_i \\le \\lambda^{\\star} = \\frac{7}{30}$.\n- $v_4 = \\frac{1}{10} = \\frac{3}{30}$. Is $\\frac{3}{30} \\le \\frac{7}{30}$? Yes.\n- $v_5 = -\\frac{2}{10} = -\\frac{6}{30}$. Is $-\\frac{6}{30} \\le \\frac{7}{30}$? Yes.\n\nAll conditions are satisfied. Our hypothesis for $I_{+}$ is correct. Now we can compute the components of the optimizer $p^{\\star}$.\n\n- For $i \\in \\{1, 2, 3\\}$, $p_i^{\\star} = v_i - \\lambda^{\\star}$:\n  $$\n  p_1^{\\star} = \\frac{7}{10} - \\frac{7}{30} = \\frac{21 - 7}{30} = \\frac{14}{30} = \\frac{7}{15} \\\\\n  p_2^{\\star} = \\frac{6}{10} - \\frac{7}{30} = \\frac{18 - 7}{30} = \\frac{11}{30} \\\\\n  p_3^{\\star} = \\frac{4}{10} - \\frac{7}{30} = \\frac{12 - 7}{30} = \\frac{5}{30} = \\frac{1}{6}\n  $$\n- For $i \\in \\{4, 5\\}$, $p_i^{\\star} = 0$:\n  $$\n  p_4^{\\star} = 0 \\\\\n  p_5^{\\star} = 0\n  $$\nAs a final check, we confirm that $\\sum p_i^{\\star} = 1$:\n$$\n\\frac{7}{15} + \\frac{11}{30} + \\frac{1}{6} + 0 + 0 = \\frac{14}{30} + \\frac{11}{30} + \\frac{5}{30} = \\frac{14 + 11 + 5}{30} = \\frac{30}{30} = 1\n$$\nThe condition holds. The optimizer is $p^{\\star} = (\\frac{7}{15}, \\frac{11}{30}, \\frac{1}{6}, 0, 0)$.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{7}{15} & \\frac{11}{30} & \\frac{1}{6} & 0 & 0 \\end{pmatrix}}\n$$", "id": "3129540"}, {"introduction": "The geometric principles of optimization are not just theoretical constructs; they form the basis of numerical algorithms. This hands-on coding practice challenges you to implement a numerical test for stationarity based on its fundamental geometric meaning. You will verify if a candidate point satisfies the first-order necessary conditions by checking if the gradient of the objective function is orthogonal to the tangent space of the constraint manifold, a task you will accomplish through vector projection [@problem_id:3129549].", "problem": "You are given equality-constrained minimization problems of the form: minimize a differentiable scalar function $f(\\mathbf{x})$ subject to a differentiable constraint $g(\\mathbf{x}) = 0$, where $\\mathbf{x}$ lives in a Euclidean space of appropriate dimension. From the fundamental definition of constrained minimization and the concept of directional derivatives, the first-order necessary condition for stationarity at a regular feasible point requires that the directional derivative of $f$ along every feasible direction (tangent to the manifold defined by $g(\\mathbf{x})=0$) vanishes. Equivalently, at such a point, the gradient of the objective $ \\nabla f(\\mathbf{x}) $ is orthogonal to the tangent space of the manifold, meaning it lies in the normal space generated by the constraint.\n\nYour task is to implement a numerical test of this stationarity condition via projection: for each specified problem and candidate point $ \\mathbf{x}^\\star $, compute whether the component of $ \\nabla f(\\mathbf{x}^\\star) $ along the tangent space to the manifold $ g(\\mathbf{x}) = 0 $ at $ \\mathbf{x}^\\star $ is negligible, within a specified tolerance. In addition to testing stationarity, ensure the point is feasible and the manifold is regular at $ \\mathbf{x}^\\star $, meaning $ \\lVert \\nabla g(\\mathbf{x}^\\star) \\rVert_2 > 0 $.\n\nUse the following tolerances: feasibility tolerance $ \\varepsilon_{\\text{feas}} = 10^{-9} $ and stationarity tolerance $ \\varepsilon_{\\text{stat}} = 10^{-9} $. A candidate point is considered feasible if $ \\lvert g(\\mathbf{x}^\\star) \\rvert \\le \\varepsilon_{\\text{feas}} $. The stationarity test should return the Boolean value $ \\text{True} $ when the norm of the projected component of $ \\nabla f(\\mathbf{x}^\\star) $ onto the tangent space has Euclidean norm at most $ \\varepsilon_{\\text{stat}} $; otherwise return $ \\text{False} $. If $ \\lVert \\nabla g(\\mathbf{x}^\\star) \\rVert_2 = 0 $ (a non-regular point), return $ \\text{False} $.\n\nImplement the program to evaluate the following test suite. For each case, $ f $, $ g $, and $ \\mathbf{x}^\\star $ are provided. Gradients must be computed analytically.\n\n- Test case $1$ (two-dimensional line constraint, expected to be stationary):\n  $f_1(x,y) = x^2 + y^2$, $g_1(x,y) = x + y - 1$, $\\mathbf{x}_1^\\star = \\left(\\frac{1}{2}, \\frac{1}{2}\\right)$.\n\n- Test case $2$ (two-dimensional line constraint, feasible but not stationary):\n  $f_1(x,y) = x^2 + y^2$, $g_1(x,y) = x + y - 1$, $\\mathbf{x}_2^\\star = (1, 0)$.\n\n- Test case $3$ (two-dimensional circle constraint with anisotropic objective, stationary):\n  $f_2(x,y) = x^2 + 4y^2$, $g_2(x,y) = x^2 + y^2 - 1$, $\\mathbf{x}_3^\\star = (1, 0)$.\n\n- Test case $4$ (three-dimensional plane constraint, stationary):\n  $f_3(x,y,z) = x^2 + y^2 + z^2$, $g_3(x,y,z) = x + 2y + 3z - 6$, $\\mathbf{x}_4^\\star = \\left(\\frac{3}{7}, \\frac{6}{7}, \\frac{9}{7}\\right)$.\n\n- Test case $5$ (two-dimensional degenerate constraint at a feasible point, non-regular, should be rejected):\n  $f_4(x,y) = x^2 + y^2$, $g_4(x,y) = x^3$, $\\mathbf{x}_5^\\star = (0, 1)$.\n\n- Test case $6$ (two-dimensional circle constraint with anisotropic objective, infeasible point):\n  $f_2(x,y) = x^2 + 4y^2$, $g_2(x,y) = x^2 + y^2 - 1$, $\\mathbf{x}_6^\\star = \\left(\\frac{1}{2}, \\frac{1}{2}\\right)$.\n\nYour program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases $1$ through $6$, for example, $[\\text{True},\\text{False},\\dots]$.", "solution": "The problem requires the numerical validation of the first-order necessary conditions for stationarity at a candidate point $\\mathbf{x}^\\star$ for an equality-constrained optimization problem of the form:\n$$ \\text{minimize } f(\\mathbf{x}) \\quad \\text{subject to } g(\\mathbf{x}) = 0 $$\nwhere $f: \\mathbb{R}^n \\to \\mathbb{R}$ and $g: \\mathbb{R}^n \\to \\mathbb{R}$ are differentiable functions.\n\nThe validation process involves three main checks: feasibility of the point, regularity of the constraint manifold at that point, and the stationarity condition itself.\n\n**1. Theoretical Foundation: First-Order Necessary Conditions**\n\nA point $\\mathbf{x}^\\star$ is a candidate for a local minimum only if it satisfies the first-order necessary conditions, often known as the Karush-Kuhn-Tucker (KKT) conditions. For a problem with a single equality constraint, these conditions have a clear geometric interpretation.\n\nThe constraint $g(\\mathbf{x}) = 0$ defines a manifold (a surface, curve, etc.) in $\\mathbb{R}^n$. A point $\\mathbf{x}^\\star$ is feasible if it lies on this manifold, i.e., $g(\\mathbf{x}^\\star) = 0$.\n\nAt a feasible point $\\mathbf{x}^\\star$, the gradient of the constraint function, $\\nabla g(\\mathbf{x}^\\star)$, is a vector that is orthogonal (normal) to the tangent space of the manifold at that point. The point is called *regular* if this gradient is non-zero, i.e., $\\nabla g(\\mathbf{x}^\\star) \\neq \\mathbf{0}$. If a point is regular, $\\nabla g(\\mathbf{x}^\\star)$ spans the one-dimensional *normal space*. The set of all vectors orthogonal to $\\nabla g(\\mathbf{x}^\\star)$ forms the $(n-1)$-dimensional *tangent space*, which contains all feasible directions of movement from $\\mathbf{x}^\\star$.\n\nThe core principle of constrained optimization is that at a local minimum $\\mathbf{x}^\\star$, the objective function $f$ cannot decrease in any feasible direction. This implies that the directional derivative of $f$ in any direction tangent to the constraint manifold must be zero. The directional derivative of $f$ in a direction $\\mathbf{d}$ is given by $\\nabla f(\\mathbf{x}^\\star) \\cdot \\mathbf{d}$. For this to be zero for all vectors $\\mathbf{d}$ in the tangent space, the gradient of the objective function, $\\nabla f(\\mathbf{x}^\\star)$, must be orthogonal to the tangent space.\n\nIf $\\nabla f(\\mathbf{x}^\\star)$ is orthogonal to the tangent space, it must lie in the normal space. Since the normal space is spanned by $\\nabla g(\\mathbf{x}^\\star)$ at a regular point, the stationarity condition is equivalent to stating that $\\nabla f(\\mathbf{x}^\\star)$ and $\\nabla g(\\mathbf{x}^\\star)$ must be collinear. That is, there must exist a scalar $\\lambda$ (the Lagrange multiplier) such that:\n$$ \\nabla f(\\mathbf{x}^\\star) + \\lambda \\nabla g(\\mathbf{x}^\\star) = \\mathbf{0} $$\nThis is equivalent to $\\nabla f(\\mathbf{x}^\\star) = -\\lambda \\nabla g(\\mathbf{x}^\\star)$.\n\n**2. Numerical Verification via Projection**\n\nThe problem asks to verify this stationarity condition by projecting the objective gradient $\\nabla f(\\mathbf{x}^\\star)$ onto the tangent space and checking if the result is a near-zero vector.\n\nLet $\\mathbf{v} = \\nabla f(\\mathbf{x}^\\star)$ and $\\mathbf{n} = \\nabla g(\\mathbf{x}^\\star)$.\nAssuming $\\mathbf{n} \\neq \\mathbf{0}$ (regularity), any vector $\\mathbf{v}$ can be uniquely decomposed into a component parallel to $\\mathbf{n}$ (in the normal space) and a component orthogonal to $\\mathbf{n}$ (in the tangent space).\n\nThe component of $\\mathbf{v}$ in the normal space is its projection onto $\\mathbf{n}$:\n$$ \\mathbf{v}_{\\text{norm}} = \\text{proj}_{\\mathbf{n}}(\\mathbf{v}) = \\frac{\\mathbf{v} \\cdot \\mathbf{n}}{\\mathbf{n} \\cdot \\mathbf{n}} \\mathbf{n} = \\frac{\\langle \\mathbf{v}, \\mathbf{n} \\rangle}{\\lVert \\mathbf{n} \\rVert_2^2} \\mathbf{n} $$\n\nThe component of $\\mathbf{v}$ in the tangent space, $\\mathbf{v}_{\\text{tan}}$, is what remains after subtracting the normal component:\n$$ \\mathbf{v}_{\\text{tan}} = \\mathbf{v} - \\mathbf{v}_{\\text{norm}} = \\mathbf{v} - \\frac{\\langle \\mathbf{v}, \\mathbf{n} \\rangle}{\\lVert \\mathbf{n} \\rVert_2^2} \\mathbf{n} $$\n\nThe stationarity condition $\\nabla f(\\mathbf{x}^\\star) \\in \\text{span}\\{\\nabla g(\\mathbf{x}^\\star)\\}$ is satisfied if and only if the tangential component of $\\nabla f(\\mathbf{x}^\\star)$ is the zero vector. Numerically, we test if its Euclidean norm is below a given tolerance, $\\varepsilon_{\\text{stat}}$:\n$$ \\lVert \\mathbf{v}_{\\text{tan}} \\rVert_2 \\le \\varepsilon_{\\text{stat}} $$\n\n**3. The Complete Validation Algorithm**\n\nFor each test case (defined by functions $f$, $g$, and a point $\\mathbf{x}^\\star$), we perform the following checks in order with specified tolerances $\\varepsilon_{\\text{feas}} = 10^{-9}$ and $\\varepsilon_{\\text{stat}} = 10^{-9}$.\n\n1.  **Feasibility Check**: Evaluate $g(\\mathbf{x}^\\star)$. If $\\lvert g(\\mathbf{x}^\\star) \\rvert > \\varepsilon_{\\text{feas}}$, the point is not feasible. The conditions for stationarity are not applicable. The test fails, returning `False`.\n\n2.  **Regularity Check**: Compute the constraint gradient $\\mathbf{n} = \\nabla g(\\mathbf{x}^\\star)$. If $\\lVert \\mathbf{n} \\rVert_2 = 0$, the point is not regular. The geometric basis for the test is invalid. The test fails, returning `False`.\n\n3.  **Stationarity Check**: If the point is both feasible and regular, proceed to compute the tangential component of the objective gradient.\n    a. Compute the objective gradient $\\mathbf{v} = \\nabla f(\\mathbf{x}^\\star)$.\n    b. Compute the tangential component: $\\mathbf{v}_{\\text{tan}} = \\mathbf{v} - \\frac{\\langle \\mathbf{v}, \\mathbf{n} \\rangle}{\\lVert \\mathbf{n} \\rVert_2^2} \\mathbf{n}$.\n    c. Calculate its Euclidean norm: $\\lVert \\mathbf{v}_{\\text{tan}} \\rVert_2$.\n    d. If $\\lVert \\mathbf{v}_{\\text{tan}} \\rVert_2 \\le \\varepsilon_{\\text{stat}}$, the stationarity condition is met. The test passes, returning `True`. Otherwise, it fails, returning `False`.\n\n**4. Analytical Gradients for Test Cases**\n\nThe problem requires analytical computation of gradients.\n\n- For $f_1(x,y) = x^2 + y^2$ and $f_4(x,y) = x^2 + y^2$: $\\nabla f(x,y) = (2x, 2y)^T$.\n- For $g_1(x,y) = x + y - 1$: $\\nabla g_1(x,y) = (1, 1)^T$.\n- For $f_2(x,y) = x^2 + 4y^2$: $\\nabla f_2(x,y) = (2x, 8y)^T$.\n- For $g_2(x,y) = x^2 + y^2 - 1$: $\\nabla g_2(x,y) = (2x, 2y)^T$.\n- For $f_3(x,y,z) = x^2 + y^2 + z^2$: $\\nabla f_3(x,y,z) = (2x, 2y, 2z)^T$.\n- For $g_3(x,y,z) = x + 2y + 3z - 6$: $\\nabla g_3(x,y,z) = (1, 2, 3)^T$.\n- For $g_4(x,y) = x^3$: $\\nabla g_4(x,y) = (3x^2, 0)^T$.\n\nThese gradients will be used in the implementation to carry out the validation algorithm for each test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef check_stationarity(g, grad_f, grad_g, x_star, eps_feas, eps_stat):\n    \"\"\"\n    Checks if a candidate point x_star satisfies the first-order necessary\n    conditions for stationarity for an equality-constrained problem.\n    \"\"\"\n    # 1. Feasibility Check: Is the point on the constraint manifold?\n    # A point is feasible if |g(x*)| <= eps_feas.\n    if np.abs(g(x_star)) > eps_feas:\n        return False\n\n    # 2. Regularity Check: Is the constraint gradient non-zero?\n    # The normal to the manifold must be well-defined.\n    n = grad_g(x_star)\n    norm_n = np.linalg.norm(n)\n    \n    # A point is non-regular if the norm of the constraint gradient is zero.\n    # Floating point comparison to exact zero is sufficient here as per problem specs.\n    if norm_n == 0:\n        return False\n        \n    # 3. Stationarity Check: Is the objective gradient in the normal space?\n    # This is tested by checking if the component of the objective gradient\n    # in the tangent space is a zero vector.\n    v = grad_f(x_star)\n    \n    # Project v onto the tangent space.\n    # This is done by subtracting the projection of v onto the normal vector n.\n    # proj_v_on_n = (<v, n> / ||n||^2) * n\n    v_tan = v - (np.dot(v, n) / (norm_n**2)) * n\n    \n    norm_v_tan = np.linalg.norm(v_tan)\n    \n    # The point is stationary if the norm of the tangential component is negligible.\n    return norm_v_tan <= eps_stat\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and run the stationarity checks.\n    \"\"\"\n    # Define tolerances from the problem statement\n    eps_feas = 1e-9\n    eps_stat = 1e-9\n\n    # Define objective and constraint functions and their analytical gradients\n    # Case 1 & 2\n    g1 = lambda x: x[0] + x[1] - 1\n    grad_f1 = lambda x: np.array([2 * x[0], 2 * x[1]], dtype=float)\n    grad_g1 = lambda x: np.array([1.0, 1.0], dtype=float)\n\n    # Case 3 & 6\n    g2 = lambda x: x[0]**2 + x[1]**2 - 1\n    grad_f2 = lambda x: np.array([2 * x[0], 8 * x[1]], dtype=float)\n    grad_g2 = lambda x: np.array([2 * x[0], 2 * x[1]], dtype=float)\n\n    # Case 4\n    g3 = lambda x: x[0] + 2 * x[1] + 3 * x[2] - 6\n    grad_f3 = lambda x: np.array([2 * x[0], 2 * x[1], 2 * x[2]], dtype=float)\n    grad_g3 = lambda x: np.array([1.0, 2.0, 3.0], dtype=float)\n\n    # Case 5\n    g4 = lambda x: x[0]**3\n    grad_f4 = grad_f1 # f4(x,y) = x^2+y^2, same as f1\n    grad_g4 = lambda x: np.array([3 * x[0]**2, 0.0], dtype=float)\n\n    # Define the test cases as a list of tuples: (g, grad_f, grad_g, x_star)\n    test_cases = [\n        # Case 1: 2D line constraint, stationary\n        (g1, grad_f1, grad_g1, np.array([0.5, 0.5])),\n        # Case 2: 2D line constraint, feasible but not stationary\n        (g1, grad_f1, grad_g1, np.array([1.0, 0.0])),\n        # Case 3: 2D circle constraint, stationary\n        (g2, grad_f2, grad_g2, np.array([1.0, 0.0])),\n        # Case 4: 3D plane constraint, stationary\n        (g3, grad_f3, grad_g3, np.array([3/7, 6/7, 9/7])),\n        # Case 5: 2D degenerate constraint, non-regular\n        (g4, grad_f4, grad_g4, np.array([0.0, 1.0])),\n        # Case 6: 2D circle constraint, infeasible point\n        (g2, grad_f2, grad_g2, np.array([0.5, 0.5]))\n    ]\n\n    results = []\n    for g, grad_f, grad_g, x_star in test_cases:\n        is_stationary = check_stationarity(g, grad_f, grad_g, x_star, eps_feas, eps_stat)\n        results.append(is_stationary)\n\n    # Format the output as a comma-separated list of booleans in brackets\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3129549"}]}