{"hands_on_practices": [{"introduction": "We begin our exploration with a visually intuitive function, $f(x)=\\lVert x \\rVert^{4}-\\alpha\\lVert x \\rVert^{2}$, that resembles a potential energy well whose shape is controlled by the parameter $\\alpha$. By analyzing this function, you will discover the critical threshold where the system transitions from having a single, stable ground state (a unique minimum) to an entire family of equivalent ground states (non-unique minima). This exercise provides a foundational look at how local curvature, particularly at the origin, can dictate the global structure and uniqueness of a solution set. [@problem_id:3196760]", "problem": "Consider the function $f:\\mathbb{R}^{n}\\to\\mathbb{R}$ defined by $f(x)=\\lVert x \\rVert^{4}-\\alpha\\lVert x \\rVert^{2}$, where $\\lVert x \\rVert$ denotes the Euclidean norm and $\\alpha\\in\\mathbb{R}$ is a parameter. Using only core definitions from optimization methods, analyze the structure of global minimizers of $f$ on $\\mathbb{R}^{n}$ for $n\\geq 1$ and determine how the uniqueness of the global minimum depends on $\\alpha$. Specifically, identify the single critical threshold value $\\alpha^{\\ast}\\in\\mathbb{R}$ such that for $\\alpha\\leq\\alpha^{\\ast}$ the global minimizer is unique, while for $\\alpha\\alpha^{\\ast}$ the global minimizer is non-unique. Provide the exact value of $\\alpha^{\\ast}$ as your final answer. No rounding is required.", "solution": "The problem is to determine the critical threshold value $\\alpha^{\\ast}$ for the parameter $\\alpha \\in \\mathbb{R}$ that dictates the uniqueness of the global minimizer of the function $f(x) = \\lVert x \\rVert^{4} - \\alpha\\lVert x \\rVert^{2}$ for $x \\in \\mathbb{R}^{n}$ with $n \\geq 1$.\n\nThe function $f(x)$ is radially symmetric, meaning its value depends only on the magnitude of the vector $x$, which is its Euclidean norm $\\lVert x \\rVert$. This allows us to simplify the problem by studying a one-dimensional function. Let $r = \\lVert x \\rVert \\geq 0$. We define a function $g: [0, \\infty) \\to \\mathbb{R}$ such that $g(r) = f(x)$ where $\\lVert x \\rVert=r$. The function is given by:\n$$g(r) = r^{4} - \\alpha r^{2}$$\nMinimizing $f(x)$ over $\\mathbb{R}^{n}$ is equivalent to minimizing $g(r)$ over the domain $r \\in [0, \\infty)$. The location of the global minimizer(s) of $f(x)$ will be the vector(s) $x$ whose norm $\\lVert x \\rVert$ corresponds to the value(s) of $r$ that minimize $g(r)$.\n\nTo find the minimum of $g(r)$, we first find its critical points by computing its derivative with respect to $r$ and setting it to zero.\n$$g'(r) = \\frac{d}{dr}(r^{4} - \\alpha r^{2}) = 4r^{3} - 2\\alpha r = 2r(2r^{2} - \\alpha)$$\nSetting $g'(r)=0$, we find the critical points in the domain $r \\geq 0$.\n$$2r(2r^{2} - \\alpha) = 0$$\nThis equation yields two possibilities for critical points:\n1. $r = 0$. This is always a critical point, and it is the boundary of our domain.\n2. $2r^{2} - \\alpha = 0$, which implies $r^{2} = \\frac{\\alpha}{2}$. Since $r$ must be real and non-negative, a critical point exists at $r = \\sqrt{\\frac{\\alpha}{2}}$ if and only if $\\alpha  0$.\n\nWe must analyze the behavior of the function based on the value of $\\alpha$.\n\nCase 1: $\\alpha \\leq 0$\nIn this case, the term $-\\alpha$ is non-negative. The expression for the derivative is $g'(r) = 2r(2r^{2} - \\alpha)$.\nIf $\\alpha=0$, $g(r) = r^4$ and $g'(r)=4r^3$. The only critical point is $r=0$. Since $g(r)  g(0)=0$ for all $r0$, $r=0$ is the unique global minimum.\nIf $\\alpha  0$, then $-\\alpha  0$. For any $r  0$, the term $2r^{2} - \\alpha$ is strictly positive. Since $2r$ is also positive for $r0$, we have $g'(r)  0$ for all $r  0$. This means that the function $g(r)$ is strictly increasing on the interval $(0, \\infty)$.\nTherefore, for any $\\alpha \\leq 0$, the global minimum of $g(r)$ on $[0, \\infty)$ occurs at $r=0$.\nThe value of the minimum is $g(0) = 0^{4} - \\alpha(0^{2}) = 0$.\nThe global minimizer of $f(x)$ corresponds to the vector(s) $x$ such that $\\lVert x \\rVert=r=0$. The only vector in $\\mathbb{R}^{n}$ with a norm of $0$ is the zero vector, $x=0$.\nThus, for $\\alpha \\leq 0$, the global minimizer of $f(x)$ is unique and is located at $x=0$.\n\nCase 2: $\\alpha  0$\nIn this case, there are two non-negative critical points for $g(r)$: $r_{1} = 0$ and $r_{2} = \\sqrt{\\frac{\\alpha}{2}}$.\nTo determine which corresponds to the global minimum, we evaluate $g(r)$ at these points.\n$$g(r_{1}) = g(0) = 0$$\n$$g(r_{2}) = g\\left(\\sqrt{\\frac{\\alpha}{2}}\\right) = \\left(\\sqrt{\\frac{\\alpha}{2}}\\right)^{4} - \\alpha\\left(\\sqrt{\\frac{\\alpha}{2}}\\right)^{2} = \\left(\\frac{\\alpha}{2}\\right)^{2} - \\alpha\\left(\\frac{\\alpha}{2}\\right) = \\frac{\\alpha^{2}}{4} - \\frac{\\alpha^{2}}{2} = -\\frac{\\alpha^{2}}{4}$$\nSince $\\alpha  0$, we have $\\alpha^{2}  0$, which implies $-\\frac{\\alpha^{2}}{4}  0$. Thus, $g\\left(\\sqrt{\\frac{\\alpha}{2}}\\right)  g(0)$.\nThe value at $r_{2} = \\sqrt{\\frac{\\alpha}{2}}$ is lower than the value at $r_{1}=0$.\nTo confirm this is the global minimum, we examine the behavior of $g(r)$ as $r \\to \\infty$.\n$$\\lim_{r \\to \\infty} g(r) = \\lim_{r \\to \\infty} r^{2}(r^{2} - \\alpha) = \\infty$$\nSince the function value goes to infinity as $r$ increases, the global minimum must be attained at one of the critical points. As $g\\left(\\sqrt{\\frac{\\alpha}{2}}\\right) = -\\frac{\\alpha^{2}}{4}$ is the lowest value among the critical points, it is the global minimum value of $g(r)$.\nThis global minimum occurs at $r = \\sqrt{\\frac{\\alpha}{2}}$.\nThe global minimizers of the original function $f(x)$ are all vectors $x \\in \\mathbb{R}^{n}$ such that $\\lVert x \\rVert = \\sqrt{\\frac{\\alpha}{2}}$. This set is the sphere of radius $\\sqrt{\\frac{\\alpha}{2}}$ centered at the origin.\nFor $n \\geq 1$ and $\\alpha0$, this sphere contains more than one point. For instance, if $n=1$, the minimizers are $x = \\sqrt{\\frac{\\alpha}{2}}$ and $x = -\\sqrt{\\frac{\\alpha}{2}}$. If $n  1$, the set of minimizers is infinite. In all cases for $\\alpha  0$, the global minimizer is non-unique.\n\nSummary of findings:\n- If $\\alpha \\leq 0$, the global minimizer is unique ($x=0$).\n- If $\\alpha  0$, the global minimizer is non-unique (the set $\\{x \\in \\mathbb{R}^n : \\lVert x \\rVert = \\sqrt{\\frac{\\alpha}{2}}\\}$).\n\nThe problem asks for the critical threshold $\\alpha^{\\ast}$ such that for $\\alpha \\leq \\alpha^{\\ast}$ the global minimizer is unique, and for $\\alpha  \\alpha^{\\ast}$ it is non-unique. From our analysis, this transition occurs at $\\alpha=0$.\nTherefore, the critical threshold value is $\\alpha^{\\ast} = 0$.", "answer": "$$\\boxed{0}$$", "id": "3196760"}, {"introduction": "What happens when a problem inherently has multiple equivalent \"best\" solutions? This practice explores a common scenario where a function possesses a flat region of minimizers, making any point in that region an equally valid solution. You will see how adding a simple linear perturbation, which creates a new function $F_{\\epsilon}(x) = f(x) + \\epsilon x$, acts as a powerful tie-breaker, tilting the flat landscape to produce a single unique minimum. This exercise is a hands-on demonstration of regularization, a fundamental technique used to enforce uniqueness and improve the stability of solutions in machine learning and inverse problems. [@problem_id:3196742]", "problem": "Consider the one-dimensional objective function $f:\\mathbb{R}\\to\\mathbb{R}$ defined by\n$$\nf(x) =\n\\begin{cases}\n0,  |x|\\leq 1,\\\\\n(x-1)^{2},  x\\geq 1,\\\\\n(-x-1)^{2},  x\\leq -1.\n\\end{cases}\n$$\nThis $f(x)$ is piecewise quadratic, convex, and has a non-unique set of minimizers. Define the perturbed objective $F_{\\epsilon}(x) = f(x) + \\epsilon x$, where $\\epsilon\\in\\mathbb{R}$ is a nonzero parameter. Using only core definitions and facts of convex analysis and optimization (for example, the definition of convexity, the existence and use of subgradients for convex functions, and first-order optimality conditions for convex functions), determine the unique minimizer $x^{\\star}(\\epsilon)$ of $F_{\\epsilon}(x)$ for all $\\epsilon\\neq 0$. Express your final answer as a single closed-form analytic expression in $\\epsilon$. No rounding is required, and no units are involved.", "solution": "To find the minimizer $x^{\\star}$ of the convex function $F_{\\epsilon}(x) = f(x) + \\epsilon x$, we apply the first-order optimality condition. A point $x^{\\star}$ is a global minimizer of $F_{\\epsilon}(x)$ if and only if $0$ belongs to the subgradient of $F_{\\epsilon}$ at $x^{\\star}$:\n$$0 \\in \\partial F_{\\epsilon}(x^{\\star})$$\nUsing the sum rule for subgradients, $\\partial(g_1 + g_2)(x) = \\partial g_1(x) + \\partial g_2(x)$, we have:\n$$\\partial F_{\\epsilon}(x) = \\partial (f(x) + \\epsilon x) = \\partial f(x) + \\partial(\\epsilon x)$$\nThe function $\\epsilon x$ is differentiable everywhere, with a constant derivative of $\\epsilon$. Thus, its subgradient is the singleton set $\\{\\epsilon\\}$. The optimality condition becomes:\n$$0 \\in \\partial f(x^{\\star}) + \\{\\epsilon\\}$$\nwhich is equivalent to:\n$$-\\epsilon \\in \\partial f(x^{\\star})$$\n\nNext, we compute the subgradient of $f(x)$. The function $f(x)$ is defined as:\n$$\nf(x) =\n\\begin{cases}\n(x+1)^{2},  x  -1 \\\\\n0,  -1 \\le x \\le 1 \\\\\n(x-1)^{2},  x  1\n\\end{cases}\n$$\nThe function $f(x)$ is continuously differentiable on $\\mathbb{R}$. Its derivative $f'(x)$ is given by:\n$$\nf'(x) =\n\\begin{cases}\n2(x+1),  x \\le -1 \\\\\n0,  -1 \\le x \\le 1 \\\\\n2(x-1),  x \\ge 1\n\\end{cases}\n$$\nSince $f(x)$ is differentiable everywhere, its subgradient is simply $\\partial f(x) = \\{f'(x)\\}$. The optimality condition $-\\epsilon \\in \\partial f(x^{\\star})$ reduces to the equation:\n$$f'(x^{\\star}) = -\\epsilon$$\nWe solve this equation for $x^{\\star}$ by considering the sign of $\\epsilon$, given that $\\epsilon \\neq 0$.\n\nCase 1: $\\epsilon  0$.\nIn this case, $-\\epsilon  0$. We must find $x^{\\star}$ such that $f'(x^{\\star})$ is a negative value. From the definition of $f'(x)$, this can only occur if $x^{\\star}  -1$. For this domain, we have $f'(x) = 2(x+1)$.\n$$2(x^{\\star}+1) = -\\epsilon$$\n$$x^{\\star}+1 = -\\frac{\\epsilon}{2}$$\n$$x^{\\star} = -1 - \\frac{\\epsilon}{2}$$\nWe check if this solution is consistent with the condition $x^{\\star}  -1$:\n$-1 - \\frac{\\epsilon}{2}  -1 \\implies -\\frac{\\epsilon}{2}  0 \\implies \\epsilon  0$. This is consistent with our case assumption. Since $f'(x)$ is strictly increasing on $(-\\infty, -1]$, this solution is unique for any given $\\epsilon  0$.\n\nCase 2: $\\epsilon  0$.\nIn this case, $-\\epsilon  0$. We must find $x^{\\star}$ such that $f'(x^{\\star})$ is a positive value. This only occurs if $x^{\\star}  1$. For this domain, we have $f'(x) = 2(x-1)$.\n$$2(x^{\\star}-1) = -\\epsilon$$\n$$x^{\\star}-1 = -\\frac{\\epsilon}{2}$$\n$$x^{\\star} = 1 - \\frac{\\epsilon}{2}$$\nWe check for consistency: $x^{\\star}  1 \\implies 1 - \\frac{\\epsilon}{2}  1 \\implies -\\frac{\\epsilon}{2}  0 \\implies \\epsilon  0$. This matches our case assumption. Since $f'(x)$ is strictly increasing on $[1, \\infty)$, this solution is unique for any given $\\epsilon  0$.\n\nWe can combine the two cases into a single expression. The piecewise solution is:\n$$\nx^{\\star}(\\epsilon) =\n\\begin{cases}\n-1 - \\frac{\\epsilon}{2},  \\epsilon  0 \\\\\n1 - \\frac{\\epsilon}{2},  \\epsilon  0\n\\end{cases}\n$$\nThis can be written using the signum function, $\\text{sgn}(\\epsilon)$, which is $1$ for $\\epsilon  0$ and $-1$ for $\\epsilon  0$. The expression is:\n$$x^{\\star}(\\epsilon) = -\\text{sgn}(\\epsilon) - \\frac{\\epsilon}{2}$$\nFor $\\epsilon \\neq 0$, the signum function can be expressed as $\\text{sgn}(\\epsilon) = \\frac{\\epsilon}{|\\epsilon|}$. Substituting this gives the final closed-form expression:\n$$x^{\\star}(\\epsilon) = -\\frac{\\epsilon}{|\\epsilon|} - \\frac{\\epsilon}{2}$$\nThis expression uniquely defines the minimizer $x^{\\star}$ for any nonzero real number $\\epsilon$.", "answer": "$$\\boxed{-\\frac{\\epsilon}{|\\epsilon|} - \\frac{\\epsilon}{2}}$$", "id": "3196742"}, {"introduction": "We conclude by comparing two workhorses of data science: least squares ($\\ell_2$) regression, which minimizes $\\lVert Ax - b \\rVert_2^2$, and least absolute deviations ($\\ell_1$) regression, which minimizes $\\lVert Ax - b \\rVert_1$. This exercise challenges you to investigate why a simple condition on the data matrix $A$ guarantees a unique solution for the smooth $\\ell_2$ problem, while the same condition is insufficient for the non-smooth $\\ell_1$ problem. By exploring this contrast, you will build deep intuition connecting a function's geometric properties—such as smooth ellipsoidal level sets versus sharp, polyhedral ones—to the conditions that ensure a unique minimizer. [@problem_id:3196755]", "problem": "Consider the unconstrained minimization problems in $\\mathbb{R}^n$ with data matrix $A \\in \\mathbb{R}^{m \\times n}$ and vector $b \\in \\mathbb{R}^m$:\n- $f_1(x) = \\lVert Ax - b \\rVert_1$,\n- $f_2(x) = \\lVert Ax - b \\rVert_2^2$.\nYou are asked to compare conditions that ensure uniqueness of a minimizer for each problem, and to connect these conditions to geometric intuition via level sets. Rely only on fundamental facts such as definitions of convexity, strict convexity, positive definiteness, subgradients of norms, and basic linear algebra (e.g., column rank and null space). Select all statements that are correct.\n\nA. For $f_2(x) = \\lVert Ax - b \\rVert_2^2$, the minimizer is unique if and only if $A$ has full column rank.\n\nB. For $f_1(x) = \\lVert Ax - b \\rVert_1$, full column rank of $A$ is necessary for uniqueness of the minimizer, but it is not sufficient to guarantee uniqueness for every $b$.\n\nC. For $f_1(x) = \\lVert Ax - b \\rVert_1$, if there exists $x^\\star$ with $Ax^\\star = b$ and $A$ has full column rank, then $x^\\star$ is the unique minimizer.\n\nD. The level sets $\\{x : \\lVert Ax - b \\rVert_2^2 = c\\}$ are (possibly degenerate) ellipsoids in $x$; when $A$ has full column rank they are strictly convex, which geometrically supports uniqueness of the minimizer.\n\nE. The level sets $\\{x : \\lVert Ax - b \\rVert_1 = c\\}$ are strictly convex whenever $A$ has full column rank, hence uniqueness follows from full column rank in the $\\ell_1$ case as well.\n\nChoose all that apply.", "solution": "We begin from core definitions and linear algebraic facts.\n\n- A function $g:\\mathbb{R}^n \\to \\mathbb{R}$ is strictly convex if for any distinct $x,y$ and any $\\theta \\in (0,1)$, $g(\\theta x + (1-\\theta)y)  \\theta g(x) + (1-\\theta)g(y)$. A strictly convex function over a convex domain has at most one minimizer.\n- A twice differentiable function with Hessian $H(x) \\succeq 0$ everywhere is convex; if $H(x) \\succ 0$ on the entire domain, it is strictly (indeed strongly) convex, ensuring a unique minimizer.\n- If a function $g$ depends on $x$ only through $Ax$ and $\\operatorname{null}(A) \\neq \\{0\\}$, then $g(x) = g(x + d)$ for any $d \\in \\operatorname{null}(A)$; hence uniqueness is impossible.\n- The subdifferential of the $\\ell_1$ norm is given by $\\partial \\lVert r \\rVert_1 = \\{s \\in \\mathbb{R}^m : s_i = \\operatorname{sign}(r_i)$ if $r_i \\neq 0$, and $s_i \\in [-1,1]$ if $r_i = 0\\}$. For $g(x) = \\lVert Ax - b \\rVert_1$, first-order optimality is $0 \\in \\partial g(x) = A^\\top \\partial \\lVert Ax - b \\rVert_1$.\n\nNow we address each statement.\n\nAnalysis for $f_2(x) = \\lVert Ax - b \\rVert_2^2$ (Options A and D):\n- The function $f_2$ is differentiable with gradient $\\nabla f_2(x) = 2A^\\top(Ax - b)$ and Hessian $\\nabla^2 f_2(x) = 2A^\\top A$. The Hessian is positive semidefinite for any $A$, and is positive definite if and only if $A$ has full column rank (equivalently, $A^\\top A \\succ 0$). When $A$ has full column rank, $f_2$ is strictly (indeed strongly) convex, which guarantees a unique minimizer. Conversely, if $A$ is rank-deficient, there exists $d \\neq 0$ with $Ad = 0$, hence $f_2(x + d) = \\lVert A(x + d) - b \\rVert_2^2 = \\lVert Ax - b \\rVert_2^2 = f_2(x)$; therefore uniqueness cannot hold. This proves the “if and only if” in Option A.\n- The level sets of $f_2$ are $\\{x : \\lVert Ax - b \\rVert_2^2 = c\\}$, which are preimages of Euclidean spheres under the linear map $x \\mapsto Ax - b$. When $A$ has full column rank, these sets are ellipsoids (strictly convex closed surfaces); if $A$ is rank-deficient, the level sets become cylindrical (non-strictly convex) along directions in $\\operatorname{null}(A)$. This geometric picture aligns with the uniqueness characterization. Hence Option D is correct.\n\nAnalysis for $f_1(x) = \\lVert Ax - b \\rVert_1$ (Options B, C, E):\n- Necessity of full column rank for uniqueness: If $\\operatorname{null}(A) \\neq \\{0\\}$, then for any minimizer $x^\\star$ and any $d \\in \\operatorname{null}(A)$ with $d \\neq 0$, we have $f_1(x^\\star + d) = \\lVert A(x^\\star + d) - b \\rVert_1 = \\lVert Ax^\\star - b \\rVert_1 = f_1(x^\\star)$, so uniqueness is impossible. Thus full column rank is necessary. However, full column rank is not sufficient to guarantee uniqueness for every $b$. A concrete example shows this:\n  - Take $n = 1$, $m = 2$, $A = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$ (which has full column rank), and $b = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$. Then $f_1(x) = |x - 0| + |x - 1|$. The set of minimizers is the interval $[0,1]$ (any median of the two points), so the minimizer is not unique. Therefore Option B is correct.\n- Sufficient condition ensuring uniqueness in a particular case: If there exists $x^\\star$ such that $Ax^\\star = b$ and $A$ has full column rank, then for any $x \\neq x^\\star$ we have $Ax \\neq b$ (injectivity of $A$ on $\\mathbb{R}^n$), and hence $\\lVert Ax - b \\rVert_1  0 = \\lVert Ax^\\star - b \\rVert_1$. Thus $x^\\star$ is the unique global minimizer of $f_1$. Therefore Option C is correct.\n- Level sets and lack of strict convexity for the $\\ell_1$ objective: The sets $\\{r : \\lVert r \\rVert_1 = c\\}$ in residual space are cross-polytopes (polytopes with flat faces and vertices), not strictly convex. Their preimages $\\{x : \\lVert Ax - b \\rVert_1 = c\\}$ in $x$ inherit flat features induced by $A$, regardless of full column rank. Therefore full column rank of $A$ does not make $\\ell_1$ level sets strictly convex, and it does not, by itself, ensure uniqueness. Hence Option E is incorrect.\n\nVerdicts:\n- Option A: Correct.\n- Option B: Correct.\n- Option C: Correct.\n- Option D: Correct.\n- Option E: Incorrect.\n\nGeometric intuition summary tying both problems together:\n- For $f_2$, when $A$ has full column rank, ellipsoidal (strictly convex) level sets intersect in at most one point at the minimum; if $A$ is rank-deficient, cylinders along $\\operatorname{null}(A)$ create flat directions and prevent uniqueness.\n- For $f_1$, level sets are polytopal (with flat faces) irrespective of the rank; even with full column rank, the intersection of a polytope’s faces with the affine image of $\\mathbb{R}^n$ can produce non-unique minimizers, unless additional problem-specific conditions (such as exact fit $Ax = b$ or suitable subgradient strict complementarity) hold.", "answer": "$$\\boxed{ABCD}$$", "id": "3196755"}]}