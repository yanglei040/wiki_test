{"hands_on_practices": [{"introduction": "This first practice invites you to explore the fundamental dynamics of projection algorithms in a clean, geometric setting. By analyzing the repeated projection between a halfspace and a line, you will derive the precise linear convergence rate of the process [@problem_id:3137760]. This exercise sharpens your skills in applying vector projections and geometric intuition to understand the performance of iterative methods, providing a concrete foundation for how the angle between sets governs the speed of convergence.", "problem": "Consider the Euclidean space $\\mathbb{R}^{2}$ equipped with the standard Euclidean norm $\\|\\cdot\\|_{2}$ and inner product $x^{\\top}y$. Let $H$ be the closed halfspace\n$$\nH \\;=\\; \\{\\, x \\in \\mathbb{R}^{2} \\mid n^{\\top} x \\le 0 \\,\\},\n$$\nwhere $n \\in \\mathbb{R}^{2}$ is a unit normal vector satisfying $\\|n\\|_{2} = 1$. Let $S$ be an affine subspace given by the line\n$$\nS \\;=\\; \\{\\, p + t\\,u \\mid t \\in \\mathbb{R} \\,\\},\n$$\nwhere $u \\in \\mathbb{R}^{2}$ is a unit direction vector with $\\|u\\|_{2}=1$, and $p \\in \\mathbb{R}^{2}$ is a point such that $n^{\\top}p = 0$ (so $p$ lies on the boundary hyperplane of $H$). Define the alternating projection sequence $x_{k+1} = P_{H}(P_{S}(x_{k}))$, where for any nonempty closed convex set $C$, $P_{C}(x)$ denotes the Euclidean projection of $x$ onto $C$, that is, the unique minimizer of $\\|x - y\\|_{2}$ over $y \\in C$.\n\nTranslate coordinates by $p$ so that the intersection point $p$ is at the origin, and define $y_{k} = P_{H}(x_{k})$ (the point after the halfspace projection in cycle $k$). Assume the initial point is such that $y_{0}$ lies on the boundary hyperplane of $H$, i.e., $n^{\\top}y_{0}=0$, and that the sequence satisfies $n^{\\top}P_{S}(y_{k}) \\ge 0$ for all $k$ (that is, each projection onto $S$ produces a point that is on or outside $H$ so that the subsequent projection onto $H$ acts by orthogonal projection onto the boundary hyperplane). Let $\\theta \\in [0,\\pi]$ denote the angle between the unit vectors $n$ and $u$, i.e., $\\cos\\theta = n^{\\top}u$.\n\nStarting from the foundational definition of Euclidean projections, derive a closed-form expression for the iterate update $y_{k+1}$ in terms of $y_{k}$ and the geometric quantities $n$, $u$, and $\\theta$. Then, using this expression, determine the linear convergence speed per full cycle of alternating projections, quantified by the scalar $r(\\theta)$ such that for all sufficiently large $k$,\n$$\n\\|y_{k+1}\\|_{2} \\;=\\; r(\\theta)\\,\\|y_{k}\\|_{2}.\n$$\nYour final answer must be a single closed-form analytic expression for $r(\\theta)$ in terms of $\\theta$. No rounding is required, and no units are involved.", "solution": "The problem is subjected to validation before a solution is attempted.\n\n### Step 1: Extract Givens\n- The space is the Euclidean space $\\mathbb{R}^{2}$ with the standard Euclidean norm $\\|\\cdot\\|_{2}$ and inner product $x^{\\top}y$.\n- The closed halfspace is $H = \\{\\, x \\in \\mathbb{R}^{2} \\mid n^{\\top} x \\le 0 \\,\\}$, where $n \\in \\mathbb{R}^{2}$ is a unit normal vector, $\\|n\\|_{2} = 1$.\n- The affine subspace is the line $S = \\{\\, p + t\\,u \\mid t \\in \\mathbb{R} \\,\\}$, where $u \\in \\mathbb{R}^{2}$ is a unit direction vector, $\\|u\\|_{2}=1$, and $p \\in \\mathbb{R}^{2}$.\n- A condition on $p$ is given: $n^{\\top}p = 0$, which means $p$ lies on the boundary of $H$, denoted $\\partial H = \\{\\, x \\in \\mathbb{R}^{2} \\mid n^{\\top} x = 0 \\,\\}$.\n- The alternating projection sequence is defined as $x_{k+1} = P_{H}(P_{S}(x_{k}))$, where $P_{C}(x)$ is the Euclidean projection of $x$ onto a closed convex set $C$.\n- An auxiliary sequence is defined as $y_{k} = P_{H}(x_{k})$.\n- An initial condition is given for the auxiliary sequence: $y_{0}$ lies on the boundary hyperplane, $n^{\\top}y_{0}=0$.\n- A crucial assumption on the sequence is provided: $n^{\\top}P_{S}(y_{k}) \\ge 0$ for all $k \\ge 0$.\n- The angle $\\theta \\in [0,\\pi]$ between $n$ and $u$ is defined by $\\cos\\theta = n^{\\top}u$.\n- An instruction is given: \"Translate coordinates by $p$ so that the intersection point $p$ is at the origin\".\n- The objective is to derive the update rule for $y_{k+1}$ in terms of $y_k$ and then find the linear convergence speed $r(\\theta)$ defined by the relation $\\|y_{k+1}\\|_{2} = r(\\theta)\\,\\|y_{k}\\|_{2}$ for sufficiently large $k$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is a well-defined exercise in the field of optimization methods, specifically concerning the convergence of alternating projection algorithms.\n- **Scientifically Grounded (Critical)**: The problem is based on fundamental concepts of Euclidean geometry, linear algebra, and convex analysis (projections onto convex sets). It is mathematically sound.\n- **Well-Posed**: The problem is well-posed. The ambiguity regarding the relationship between the primary sequence $x_k$ and the auxiliary sequence $y_k$ is resolved by the structure of the problem. The assumption $n^{\\top}P_{S}(y_{k}) \\ge 0$ and the goal of finding a recurrence for $\\|y_k\\|_2$ imply that the problem intends for the analysis to be performed on the effective sequence $y_{k+1} = P_H(P_S(y_k))$. Under this standard interpretation, a unique solution for $r(\\theta)$ exists.\n- **Objective (Critical)**: The problem is stated using precise, objective mathematical language.\n- **All other criteria**: The problem does not violate any other validation criteria. It is not incomplete, contradictory, unrealistic, or trivial.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A full solution will be provided.\n\n### Solution Derivation\nThe problem asks for an analysis of a geometric iteration. We will follow the provided instructions and derive the result from first principles.\n\nFirst, we adopt the instructed coordinate translation. Let the new coordinate system be denoted by primes, where $x' = x - p$. The intersection of the line and the hyperplane boundary is now at the origin, $p' = p-p=0$.\n- The affine set $S$ becomes a linear subspace: $S' = S - p = \\{\\, tu \\mid t \\in \\mathbb{R} \\,\\}$.\n- The halfspace $H$ is described by $n^{\\top}x \\le 0$. In the new coordinates, $n^{\\top}(x'+p) \\le 0$, which simplifies to $n^{\\top}x' + n^{\\top}p \\le 0$. Since we are given $n^{\\top}p = 0$, the halfspace has the same form in the new system: $H' = \\{\\, x' \\in \\mathbb{R}^{2} \\mid n^{\\top} x' \\le 0 \\,\\}$.\n- The iterates are translated as $y'_k = y_k - p$. The initial condition $n^{\\top}y_0 = 0$ becomes $n^{\\top}(y'_0+p) = 0$, which implies $n^{\\top}y'_0 = 0$. So, the translated initial point $y'_0$ is on the translated boundary $\\partial H'$.\n\nFor simplicity, from now on we work exclusively in this translated coordinate system and drop the primes. The problem is now to analyze an iteration starting from $y_0$ on the hyperplane $\\partial H = \\{\\, x \\mid n^{\\top}x = 0 \\,\\}$, where $S = \\{\\, tu \\mid t \\in \\mathbb{R} \\,\\}$ is a line through the origin.\n\nThe problem requires finding a relationship between $y_{k+1}$ and $y_k$. As argued in the validation step, we analyze the effective iteration $y_{k+1} = P_{H}(P_{S}(y_{k}))$. The sequence starts with $y_0$ such that $n^\\top y_0 = 0$.\n\nThe iteration consists of two projection steps:\n1.  **Projection onto $S$**: The set $S$ is a line through the origin in the direction of the unit vector $u$. The Euclidean projection of a point $y_k$ onto $S$ is given by:\n    $$\n    P_{S}(y_k) = (y_k^{\\top}u)u\n    $$\n    This follows from minimizing $\\|y_k - tu\\|_2^2$ with respect to $t$, giving the optimal $t = y_k^{\\top}u / \\|u\\|_2^2 = y_k^{\\top}u$. Let's denote the result by $z_k = P_S(y_k)$.\n\n2.  **Projection onto $H$**: The point $z_k$ is then projected onto the halfspace $H = \\{\\, x \\mid n^{\\top}x \\le 0 \\,\\}$. The formula for this projection depends on whether $z_k$ is inside or outside $H$. The problem provides the crucial assumption that $n^{\\top}P_{S}(y_{k}) \\ge 0$ for all $k$. This means $n^{\\top}z_k \\ge 0$.\n    - If $n^{\\top}z_k = 0$, then $z_k$ is on the boundary $\\partial H$, so $z_k \\in H$, and $P_H(z_k) = z_k$.\n    - If $n^{\\top}z_k > 0$, then $z_k$ is outside $H$. The projection of $z_k$ onto the closed convex set $H$ is its orthogonal projection onto the boundary hyperplane $\\partial H = \\{\\, x \\mid n^{\\top}x = 0 \\,\\}$. This projection is given by:\n    $$\n    P_{H}(z_k) = z_k - (n^{\\top}z_k)n\n    $$\n    This same formula also covers the case $n^{\\top}z_k = 0$. Therefore, under the given assumption, the projection onto $H$ is always given by this expression.\n\nNow, we combine these two steps to find the update rule for $y_{k+1}$.\n$$\ny_{k+1} = P_{H}(z_k) = P_{H}(P_{S}(y_k)) = P_S(y_k) - (n^{\\top}P_S(y_k))n\n$$\nSubstitute the expression for $P_S(y_k)$:\n$$\ny_{k+1} = (y_k^{\\top}u)u - (n^{\\top}((y_k^{\\top}u)u))n\n$$\nThe term $y_k^{\\top}u$ is a scalar, so we can factor it out:\n$$\ny_{k+1} = (y_k^{\\top}u)u - (y_k^{\\top}u)(n^{\\top}u)n\n$$\nThis yields the closed-form expression for the iterate update:\n$$\ny_{k+1} = (y_k^{\\top}u) [u - (n^{\\top}u)n]\n$$\nAn important property of this recurrence is that if $y_k \\in \\partial H$, then $y_{k+1} \\in \\partial H$. We can verify this:\n$n^{\\top}y_{k+1} = n^{\\top}((y_k^{\\top}u)[u - (n^{\\top}u)n]) = (y_k^{\\top}u)[n^{\\top}u - (n^{\\top}u)(n^{\\top}n)]$. Since $\\|n\\|_2=1$, we have $n^{\\top}n=1$. Thus, $n^{\\top}y_{k+1} = (y_k^{\\top}u)[n^{\\top}u - n^{\\top}u] = 0$. Since we start with $y_0$ on the boundary, all subsequent iterates $y_k$ remain on the boundary $\\partial H$.\n\nNext, we determine the linear convergence speed $r(\\theta)$ by finding the ratio of the norms $\\|y_{k+1}\\|_2 / \\|y_k\\|_2$.\nTaking the Euclidean norm of the update equation:\n$$\n\\|y_{k+1}\\|_2 = \\| (y_k^{\\top}u) [u - (n^{\\top}u)n] \\|_2 = |y_k^{\\top}u| \\, \\|u - (n^{\\top}u)n\\|_2\n$$\nWe analyze each of the two terms on the right-hand side.\n\nTerm 1: $|y_k^{\\top}u|$.\nThe vector $y_k$ lies on the line $\\partial H = \\{\\, x \\mid n^{\\top}x = 0 \\,\\}$. This means $y_k$ is orthogonal to the unit vector $n$. The term $y_k^{\\top}u$ is the inner product between $y_k$ and the unit vector $u$. Let $\\phi$ be the angle between $y_k$ and $u$. Then $y_k^{\\top}u = \\|y_k\\|_2 \\|u\\|_2 \\cos\\phi = \\|y_k\\|_2 \\cos\\phi$.\nThe vector $u$ forms an angle $\\theta$ with $n$. Since $y_k$ is orthogonal to $n$, the angle between $u$ and the line containing $y_k$ is $\\frac{\\pi}{2} - \\theta$. Therefore, the angle $\\phi$ between $u$ and $y_k$ must be either $\\frac{\\pi}{2} - \\theta$ or $\\frac{\\pi}{2} + \\theta$ (depending on the direction of $y_k$ along the line $\\partial H$). In both cases, $|\\cos\\phi| = |\\cos(\\frac{\\pi}{2} \\pm \\theta)| = |\\mp\\sin\\theta| = |\\sin\\theta|$.\nSince $\\theta \\in [0, \\pi]$, $\\sin\\theta \\ge 0$, so $|\\sin\\theta| = \\sin\\theta$.\nThus, $|y_k^{\\top}u| = \\|y_k\\|_2 \\sin\\theta$.\n\nTerm 2: $\\|u - (n^{\\top}u)n\\|_2$.\nThe vector $v = u - (n^{\\top}u)n$ is the component of the unit vector $u$ that is orthogonal to $n$. This is the result of applying the Gram-Schmidt process. We can compute its norm-squared:\n$$\n\\|u - (n^{\\top}u)n\\|_2^2 = (u - (n^{\\top}u)n)^{\\top}(u - (n^{\\top}u)n)\n$$\n$$\n= u^{\\top}u - 2(n^{\\top}u)(u^{\\top}n) + (n^{\\top}u)^2(n^{\\top}n)\n$$\nSince $u$ and $n$ are unit vectors, $u^{\\top}u = 1$ and $n^{\\top}n = 1$. Also, $n^{\\top}u = \\cos\\theta$.\n$$\n= 1 - 2(\\cos\\theta)^2 + (\\cos\\theta)^2 = 1 - \\cos^2\\theta = \\sin^2\\theta\n$$\nTaking the square root gives $\\|u - (n^{\\top}u)n\\|_2 = \\sqrt{\\sin^2\\theta} = |\\sin\\theta|$. Since $\\theta \\in [0, \\pi]$, $\\sin\\theta \\ge 0$, so the norm is $\\sin\\theta$.\n\nCombining the two terms, we get the norm of $y_{k+1}$:\n$$\n\\|y_{k+1}\\|_2 = (\\|y_k\\|_2 \\sin\\theta) (\\sin\\theta) = (\\sin^2\\theta) \\|y_k\\|_2\n$$\nThe relation $\\|y_{k+1}\\|_2 = r(\\theta)\\|y_k\\|_2$ holds for all $k \\ge 0$. The linear convergence speed per cycle is therefore given by the scalar factor $r(\\theta)$.\n\n$$\nr(\\theta) = \\sin^2\\theta\n$$", "answer": "$$\\boxed{\\sin^2(\\theta)}$$", "id": "3137760"}, {"introduction": "Building upon the mechanics of single projections, this practice challenges you to solve a more complex and practical problem: finding the closest point in a polyhedron to a given external point. You will implement Dykstra's algorithm, a powerful iterative method for projecting onto an intersection of multiple convex sets—in this case, halfspaces [@problem_id:3137818]. This hands-on coding exercise demonstrates how to systematically handle multiple constraints and provides insight into the algorithm's \"memory\" through correction terms that accumulate along the normals of the halfspace boundaries.", "problem": "Consider the Euclidean projection problem in a finite-dimensional real vector space. Let $n \\in \\mathbb{N}$ and let a family of $m \\in \\mathbb{N}$ closed halfspaces in $\\mathbb{R}^n$ be given by linear inequalities of the form $C_i = \\{x \\in \\mathbb{R}^n : a_i^\\top x \\le b_i\\}$ for $i \\in \\{1,\\dots,m\\}$, where $a_i \\in \\mathbb{R}^n$ and $b_i \\in \\mathbb{R}$. Let $x_0 \\in \\mathbb{R}^n$ be a point for which we seek the Euclidean projection onto the intersection $C = \\bigcap_{i=1}^m C_i$, that is, a point $x^\\star \\in C$ minimizing the objective $x \\mapsto \\tfrac{1}{2}\\|x - x_0\\|_2^2$.\n\nStarting from the fundamental definition of Euclidean projection onto a nonempty closed convex set in $\\mathbb{R}^n$ and the well-tested fact that the intersection of closed halfspaces is a closed convex set, derive and implement an iterative method that uses cyclic projections onto the individual halfspaces and per-set correction terms to converge to the projection of $x_0$ onto $C$. The method must be designed so that the corrections accumulate along the normals $a_i$ of the halfspaces, providing a direct way to observe this accumulation numerically. The projection onto a single closed halfspace must be treated by solving the corresponding minimal-distance problem subject to one linear inequality. The iterative method must terminate when the change in the iterate after a full cycle is below a prescribed tolerance or a maximum number of cycles is reached.\n\nYour program must implement this projection procedure and, for each test case in the test suite below, produce as its result a list of real numbers containing:\n- First, the coordinates of the projected point $x^\\star$ in order.\n- Next, for each halfspace in the order given, a scalar quantifying the accumulation of the final correction term along the corresponding normal. Use, for the $i$-th halfspace, the scalar $s_i = \\dfrac{a_i^\\top y_i}{\\|a_i\\|_2^2}$, where $y_i \\in \\mathbb{R}^n$ denotes the final correction vector stored for set $i$ by the iterative method at termination. This scalar equals the signed step length taken along the normal direction $a_i$ at the last correction for that set and is a concise measure of how corrections accumulate along $a_i$.\n\nUse the stopping tolerance $10^{-10}$ and a maximum of $10000$ cycles through all halfspaces. There are no physical units involved in this problem. All angles, if any appear implicitly in vector operations, are handled in radians by the standard inner product; no explicit angle unit is needed. All answers must be expressed as real numbers.\n\nTest suite:\n- Case $1$ ($2$-dimensional, two halfspaces forming the nonpositive quadrant): $n = 2$, $m = 2$, $a_1 = [1, 0]^\\top$, $b_1 = 0$, $a_2 = [0, 1]^\\top$, $b_2 = 0$, $x_0 = [1, 2]^\\top$.\n- Case $2$ ($2$-dimensional, three halfspaces forming a wedge with a floor): $n = 2$, $m = 3$, $a_1 = [1, 1]^\\top$, $b_1 = 1$, $a_2 = [-1, 2]^\\top$, $b_2 = 2$, $a_3 = [0, -1]^\\top$, $b_3 = 0$, $x_0 = [2, 3]^\\top$.\n- Case $3$ ($3$-dimensional axis-aligned box): $n = 3$, $m = 6$, $a_1 = [1, 0, 0]^\\top$, $b_1 = 1$, $a_2 = [-1, 0, 0]^\\top$, $b_2 = 1$, $a_3 = [0, 1, 0]^\\top$, $b_3 = 1$, $a_4 = [0, -1, 0]^\\top$, $b_4 = 1$, $a_5 = [0, 0, 1]^\\top$, $b_5 = 1$, $a_6 = [0, 0, -1]^\\top$, $b_6 = 1$, $x_0 = [2, -3, 0.5]^\\top$.\n- Case $4$ ($2$-dimensional point intersection from four halfspaces): $n = 2$, $m = 4$, $a_1 = [1, 0]^\\top$, $b_1 = 1$, $a_2 = [-1, 0]^\\top$, $b_2 = -1$, $a_3 = [0, 1]^\\top$, $b_3 = -1$, $a_4 = [0, -1]^\\top$, $b_4 = 1$, $x_0 = [-2, 4]^\\top$.\n- Case $5$ ($2$-dimensional interior point in the wedge of Case $2$): $n = 2$, $m = 3$, $a_1 = [1, 1]^\\top$, $b_1 = 1$, $a_2 = [-1, 2]^\\top$, $b_2 = 2$, $a_3 = [0, -1]^\\top$, $b_3 = 0$, $x_0 = [0.2, 0.3]^\\top$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element of this list must be the Python list representation for one test case, constructed by concatenating the coordinates of $x^\\star$ followed by the scalars $s_i$ in the order of the halfspaces for that case. For example, an output for three cases would look like $[ [x^\\star\\_1\\_1, x^\\star\\_1\\_2, s\\_{1,1}, s\\_{1,2}, \\dots], [x^\\star\\_2\\_1, x^\\star\\_2\\_2, s\\_{2,1}, s\\_{2,2}, \\dots], [x^\\star\\_3\\_1, x^\\star\\_3\\_2, x^\\star\\_3\\_3, s\\_{3,1}, \\dots ] ]$, with no additional text.", "solution": "The user-provided problem is valid. It is scientifically grounded in the field of convex optimization, specifically Euclidean projection onto a convex set. The problem is well-posed, as the projection of a point onto a nonempty closed convex set is unique. The problem statement is objective, complete, and contains no contradictions. We may proceed with a solution.\n\nThe problem asks for the Euclidean projection of a point $x_0 \\in \\mathbb{R}^n$ onto a closed convex set $C$, where $C$ is the intersection of $m$ closed halfspaces $C_i = \\{x \\in \\mathbb{R}^n : a_i^\\top x \\le b_i\\}$. The projection, denoted $x^\\star$, is the unique solution to the optimization problem:\n$$\nx^\\star = \\arg\\min_{x \\in C} \\frac{1}{2} \\|x - x_0\\|_2^2\n$$\nThe problem requires the implementation of a specific iterative method known as Dykstra's projection algorithm. This algorithm finds the projection onto an intersection of convex sets by performing a cycle of projections onto the individual sets, incorporating a set of auxiliary correction vectors that account for the iterative displacement.\n\n### Projection onto a Single Halfspace\n\nThe fundamental operation in Dykstra's algorithm is the projection onto a single halfspace, $P_{C_i}(y)$, which finds the point in $C_i$ closest to a given point $y \\in \\mathbb{R}^n$. This is a convex optimization subproblem:\n$$\n\\text{minimize} \\quad \\frac{1}{2}\\|x - y\\|_2^2 \\quad \\text{subject to} \\quad a_i^\\top x \\le b_i\n$$\nThe solution can be derived using the Karush-Kuhn-Tucker (KKT) conditions. The Lagrangian is $L(x, \\mu) = \\frac{1}{2}\\|x - y\\|_2^2 + \\mu(a_i^\\top x - b_i)$, with $\\mu \\ge 0$.\nThe KKT conditions are:\n1.  **Stationarity:** $\\nabla_x L = x - y + \\mu a_i = 0 \\implies x = y - \\mu a_i$.\n2.  **Primal feasibility:** $a_i^\\top x \\le b_i$.\n3.  **Dual feasibility:** $\\mu \\ge 0$.\n4.  **Complementary slackness:** $\\mu(a_i^\\top x - b_i) = 0$.\n\nTwo cases arise:\n- If $y$ is already in the halfspace $C_i$, then $a_i^\\top y \\le b_i$. We can set $\\mu = 0$. From stationarity, $x = y$. All KKT conditions are satisfied, so $P_{C_i}(y) = y$.\n- If $y$ is outside $C_i$, then $a_i^\\top y  b_i$. This requires $\\mu  0$. By complementary slackness, the constraint must be active: $a_i^\\top x = b_i$. Substituting $x = y - \\mu a_i$ into this equality gives $a_i^\\top (y - \\mu a_i) = b_i$, which solves for $\\mu = \\frac{a_i^\\top y - b_i}{\\|a_i\\|_2^2}$. Since $a_i^\\top y  b_i$, we have $\\mu  0$ as required. The projection is $x = y - \\frac{a_i^\\top y - b_i}{\\|a_i\\|_2^2} a_i$.\n\nCombining both cases using the positive part function $(z)_+ = \\max(0, z)$, the projection operator is given by:\n$$\nP_{C_i}(y) = y - \\frac{(a_i^\\top y - b_i)_+}{\\|a_i\\|_2^2} a_i\n$$\nThis formula assumes $a_i \\neq 0$, which is true for any well-defined halfspace.\n\n### Dykstra's Projection Algorithm\n\nDykstra's algorithm is an iterative procedure to find the projection onto the intersection $C=\\bigcap_{i=1}^m C_i$. It maintains a primary iterate, which converges to the solution $x^\\star$, and a set of $m$ auxiliary correction vectors, one for each halfspace. These vectors, denoted $y_i$, 'store' the displacement component normal to the respective halfspace boundary from previous projection steps. The problem statement accurately describes the role of these corrections.\n\nThe algorithm proceeds as follows:\n\n1.  **Initialization:**\n    -   Set the initial iterate $x^{(0)} = x_0$.\n    -   Initialize $m$ correction vectors $y_i^{(0)} = \\mathbf{0} \\in \\mathbb{R}^n$ for $i=1, \\dots, m$.\n\n2.  **Iteration:** For each cycle $k = 0, 1, 2, \\dots$ up to a maximum number of cycles:\n    a.  Store the iterate at the start of the cycle: $x_{\\text{start of cycle}} = x^{(k)}$.\n    b.  Initialize the intermediate point for the cycle: $z_0 = x^{(k)}$.\n    c.  Perform a full cycle of projections through all halfspaces $i = 1, \\dots, m$:\n        i.   Define the point to be projected onto $C_i$: $v_i = z_{i-1} + y_i^{(k)}$.\n        ii.  Project this point: $z_i = P_{C_i}(v_i)$.\n        iii. Update the correction vector for the next cycle: $y_i^{(k+1)} = v_i - z_i$.\n    d.  Update the primary iterate for the next cycle: $x^{(k+1)} = z_m$.\n    e.  **Termination Check:** If $\\|x^{(k+1)} - x_{\\text{start of cycle}}\\|_2  \\text{tolerance}$, the algorithm has converged.\n\nThe update rule for the correction vector, $y_i^{(k+1)} = v_i - P_{C_i}(v_i)$, combined with the formula for the projection operator, shows that $y_i^{(k+1)}$ is always a non-negative scalar multiple of the normal vector $a_i$:\n$$\ny_i^{(k+1)} = \\frac{(a_i^\\top v_i - b_i)_+}{\\|a_i\\|_2^2} a_i\n$$\nThis confirms the problem's description that \"corrections accumulate along the normals $a_i$\".\n\n### Final Output Calculation\n\nUpon termination, the algorithm yields the projected point $x^\\star \\approx x^{(k+1)}$ and the final set of correction vectors $\\{y_i^{\\text{final}}\\}$. The problem requires calculating a scalar $s_i$ for each halfspace, defined as:\n$$\ns_i = \\frac{a_i^\\top y_i^{\\text{final}}}{\\|a_i\\|_2^2}\n$$\nSince $y_i^{\\text{final}}$ must be of the form $\\lambda_i a_i$ for some scalar $\\lambda_i \\ge 0$, we can substitute this into the formula for $s_i$:\n$$\ns_i = \\frac{a_i^\\top (\\lambda_i a_i)}{\\|a_i\\|_2^2} = \\frac{\\lambda_i (a_i^\\top a_i)}{\\|a_i\\|_2^2} = \\lambda_i\n$$\nThus, $s_i$ is precisely the final scalar multiplier that relates the correction vector $y_i^{\\text{final}}$ to the normal vector $a_i$. This value represents the cumulative correction magnitude in the direction of $a_i$.\n\nThe implementation will follow this derived algorithm, compute the final coordinates of $x^\\star$, and then use the final correction vectors $\\{y_i^{\\text{final}}\\}$ to compute the scalars $\\{s_i\\}$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Euclidean projection problem for a set of test cases using Dykstra's algorithm.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: 2D, nonpositive quadrant\n        {\n            \"x0\": np.array([1, 2], dtype=float),\n            \"a\": [np.array([1, 0], dtype=float), np.array([0, 1], dtype=float)],\n            \"b\": [0.0, 0.0]\n        },\n        # Case 2: 2D, wedge with floor\n        {\n            \"x0\": np.array([2, 3], dtype=float),\n            \"a\": [np.array([1, 1], dtype=float), np.array([-1, 2], dtype=float), np.array([0, -1], dtype=float)],\n            \"b\": [1.0, 2.0, 0.0]\n        },\n        # Case 3: 3D, axis-aligned box\n        {\n            \"x0\": np.array([2, -3, 0.5], dtype=float),\n            \"a\": [\n                np.array([1, 0, 0], dtype=float), np.array([-1, 0, 0], dtype=float),\n                np.array([0, 1, 0], dtype=float), np.array([0, -1, 0], dtype=float),\n                np.array([0, 0, 1], dtype=float), np.array([0, 0, -1], dtype=float)\n            ],\n            \"b\": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n        },\n        # Case 4: 2D, point intersection\n        {\n            \"x0\": np.array([-2, 4], dtype=float),\n            \"a\": [\n                np.array([1, 0], dtype=float), np.array([-1, 0], dtype=float),\n                np.array([0, 1], dtype=float), np.array([0, -1], dtype=float)\n            ],\n            \"b\": [1.0, -1.0, -1.0, 1.0]\n        },\n        # Case 5: 2D, interior point\n        {\n            \"x0\": np.array([0.2, 0.3], dtype=float),\n            \"a\": [np.array([1, 1], dtype=float), np.array([-1, 2], dtype=float), np.array([0, -1], dtype=float)],\n            \"b\": [1.0, 2.0, 0.0]\n        }\n    ]\n\n    # Algorithm parameters\n    TOLERANCE = 1e-10\n    MAX_CYCLES = 10000\n\n    results = []\n\n    def project_halfspace(y_proj, a_i, b_i):\n        \"\"\"\n        Projects a point y_proj onto the halfspace a_i^T x = b_i.\n        \"\"\"\n        a_norm_sq = np.dot(a_i, a_i)\n        if a_norm_sq == 0:\n            return y_proj\n\n        violation = np.dot(a_i, y_proj) - b_i\n        \n        if violation = 0:\n            return y_proj\n        \n        lambda_val = violation / a_norm_sq\n        return y_proj - lambda_val * a_i\n\n    for case in test_cases:\n        x0 = case[\"x0\"]\n        a_vectors = case[\"a\"]\n        b_scalars = case[\"b\"]\n        m = len(a_vectors)\n        n = len(x0)\n\n        # Initialization\n        x_k = np.copy(x0)\n        y_corrections = [np.zeros(n, dtype=float) for _ in range(m)]\n\n        for k in range(MAX_CYCLES):\n            x_start_of_cycle = np.copy(x_k)\n            \n            x_intermediate = x_k\n            for i in range(m):\n                a_i = a_vectors[i]\n                b_i = b_scalars[i]\n                \n                point_to_project = x_intermediate + y_corrections[i]\n                x_projected = project_halfspace(point_to_project, a_i, b_i)\n                \n                # Update correction vector for this halfspace\n                y_corrections[i] = point_to_project - x_projected\n                \n                # Update the intermediate point for the next projection in the cycle\n                x_intermediate = x_projected\n            \n            x_k = x_intermediate # Final point after one full cycle\n            \n            # Termination check\n            change = np.linalg.norm(x_k - x_start_of_cycle)\n            if change  TOLERANCE:\n                break\n        \n        x_star = x_k\n        \n        # Calculate final scalars s_i\n        s_values = []\n        for i in range(m):\n            a_i = a_vectors[i]\n            y_i = y_corrections[i]\n            a_norm_sq = np.dot(a_i, a_i)\n            if a_norm_sq > 0:\n                s_i = np.dot(a_i, y_i) / a_norm_sq\n            else:\n                s_i = 0.0 # Should not happen with valid halfspaces\n            s_values.append(s_i)\n            \n        # Combine results for this case\n        case_result = list(x_star) + s_values\n        results.append(case_result)\n\n    # Format the final output string\n    # str() on a list gives a nice '[...]' representation.\n    # We join these string representations with commas.\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\nsolve()\n```", "id": "3137818"}, {"introduction": "In our final practice, we move from the geometry of projections to the application of hyperplanes as a core tool in advanced optimization. You will step into the world of Benders decomposition, a method for solving large-scale problems, by implementing the logic for managing \"Benders cuts\" [@problem_id:3137769]. This exercise connects the abstract concept of linear programming duality to the tangible task of generating, selecting, and evaluating these cutting planes, illustrating how hyperplanes can be used to build an increasingly accurate approximation of a complex cost function.", "problem": "You are given a Benders decomposition context for a two-stage linear optimization model, where the recourse (second-stage) cost is captured by a value function. The goal is to construct hyperplanes (Benders cuts), select the strongest one at a given master solution by quantifying violation, detect dominance among candidate cuts over a bounded master domain, and quantify a convergence metric that measures how well the available cuts approximate the true recourse value at the current master solution.\n\nStart from fundamental definitions and laws:\n- Linear programming duality and weak duality: For a primal linear program and its dual, any dual feasible solution yields a valid bound on the primal objective.\n- In Benders decomposition, the recourse value function is the optimal value of a linear program parameterized by the master decision vector. Its dual feasible solutions define supporting hyperplanes that are valid lower bounds on this value function.\n- A hyperplane in this setting is an affine function in the master variables and the recourse surrogate variable; it defines a halfspace that contains the feasible set of the aggregated problem.\n- Dominance of one cut over another on a bounded polyhedral master domain can be tested by comparing the two affine right-hand sides over that domain; the worst-case comparison reduces to minimizing an affine function over a hyperrectangle, which occurs at one of its corners.\n\nTasks:\n1) From the above foundations, derive the analytic form of the Benders cut induced by a dual feasible vector. Express the cut as a hyperplane in the master variables and the recourse surrogate; identify the coefficients entirely in terms of the given data for the subproblem dual feasibility set and the affine parameterization in the master variables.\n2) For each test case, using the derived form, evaluate each candidate cut at a specified current master point, compute its violation against a given surrogate value, and select the cut with the largest violation. In case of ties, choose the smallest index.\n3) Formalize cut dominance: given two cuts, say that one dominates the other over a given bounded hyperrectangular master domain if, for every master point in the domain, its right-hand side is greater than or equal to the other’s right-hand side. Reduce the decision to a finitely checkable condition and count how many cuts are dominated by at least one other.\n4) Quantify a convergence metric at the current master point as the difference between the true recourse value (obtained by solving the subproblem dual linear program at that point) and the best available lower bound among the candidate cuts at that point.\n5) Implement a program that, for each test case, returns a list containing: the zero-based index of the strongest cut, the corresponding violation value, the number of dominated cuts, and the convergence metric. The final program must hard-code the following test suite.\n\nTest suite:\n- Test case 1:\n  - Dimensions: master variable dimension $n=2$, dual variable dimension $r=3$.\n  - Data:\n    - $E = \\begin{bmatrix} 1  0 \\\\ -1  2 \\\\ 0  1 \\end{bmatrix}$, $h = \\begin{bmatrix} 4 \\\\ 1 \\\\ 3 \\end{bmatrix}$.\n    - Dual feasibility polyhedron for the subproblem dual variables $ \\pi \\in \\mathbb{R}^{r} $: $A \\pi \\le b$, $\\pi \\ge 0$, with $A = \\begin{bmatrix} 1  1  1 \\\\ 2  0  1 \\end{bmatrix}$ and $b = \\begin{bmatrix} 5 \\\\ 6 \\end{bmatrix}$.\n    - Master domain bounds: $l = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$, $u = \\begin{bmatrix} 3 \\\\ 3 \\end{bmatrix}$.\n    - Current master point $x_0 = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}$, surrogate $\\theta_0 = 3$.\n    - Candidate dual feasible vectors:\n      - $\\pi^{(0)} = \\begin{bmatrix} 2 \\\\ 1 \\\\ 1 \\end{bmatrix}$,\n      - $\\pi^{(1)} = \\begin{bmatrix} 0 \\\\ 2 \\\\ 1 \\end{bmatrix}$,\n      - $\\pi^{(2)} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 3 \\end{bmatrix}$,\n      - $\\pi^{(3)} = \\begin{bmatrix} 1.5 \\\\ 1.5 \\\\ 0 \\end{bmatrix}$.\n- Test case 2:\n  - Dimensions: $n=2$, $r=3$.\n  - Data:\n    - $E = \\begin{bmatrix} 1  0 \\\\ -1  2 \\\\ 0  1 \\end{bmatrix}$, $h = \\begin{bmatrix} 4 \\\\ 1 \\\\ 3 \\end{bmatrix}$.\n    - Dual feasibility: same $A$ and $b$ as in test case $1$.\n    - Master domain bounds: $l = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$, $u = \\begin{bmatrix} 3 \\\\ 3 \\end{bmatrix}$.\n    - Current master point $x_0 = \\begin{bmatrix} 3 \\\\ 3 \\end{bmatrix}$, surrogate $\\theta_0 = 1$.\n    - Candidate dual feasible vectors (including duplicates to test dominance and tie-breaking):\n      - $\\pi^{(0)} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 3 \\end{bmatrix}$,\n      - $\\pi^{(1)} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 3 \\end{bmatrix}$,\n      - $\\pi^{(2)} = \\begin{bmatrix} 2 \\\\ 1 \\\\ 1 \\end{bmatrix}$,\n      - $\\pi^{(3)} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}$.\n- Test case 3:\n  - Dimensions: $n=1$, $r=2$.\n  - Data:\n    - $E = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}$, $h = \\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix}$.\n    - Dual feasibility: $A = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}$, $b = \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix}$ (componentwise upper bounds) and $\\pi \\ge 0$.\n    - Master domain bounds: $l = \\begin{bmatrix} 0 \\end{bmatrix}$, $u = \\begin{bmatrix} 5 \\end{bmatrix}$.\n    - Current master point $x_0 = \\begin{bmatrix} 5 \\end{bmatrix}$, surrogate $\\theta_0 = -1$.\n    - Candidate dual feasible vectors:\n      - $\\pi^{(0)} = \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix}$,\n      - $\\pi^{(1)} = \\begin{bmatrix} 0 \\\\ 2 \\end{bmatrix}$,\n      - $\\pi^{(2)} = \\begin{bmatrix} 3 \\\\ 0 \\end{bmatrix}$.\n\nPrecise program requirements:\n- Implement all computations directly in code with the above data; no input is read.\n- For each test case, compute:\n  - The zero-based index of the cut with the largest violation at $ (x_0,\\theta_0) $.\n  - That largest violation value as a real number.\n  - The number of cuts that are dominated by at least one other cut over the hyperrectangle $[l,u]$.\n  - The convergence metric at $x_0$, defined as the difference between the true recourse value at $x_0$ (the optimal value of the subproblem dual) and the best available lower bound from the candidate cuts at $x_0$.\n- Dominance checking over $[l,u]$ must be exact for affine functions by minimizing their difference over the hyperrectangle through corner evaluation logic that uses sign of each coefficient to choose $l$ or $u$ in each coordinate.\n- The final output must be a single line containing a list of results, one per test case, in order. Each test case result must be a list of four items: $[\\text{best\\_index}, \\text{best\\_violation}, \\text{dominated\\_count}, \\text{convergence\\_gap}]$. Use zero-based indexing for $\\text{best\\_index}$. Represent real numbers rounded to four decimal places. The program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[\\ldots]$).", "solution": "The problem is valid as it is scientifically grounded in the theory of linear programming and Benders decomposition, is well-posed with all necessary data provided, and is expressed in objective, formal language. The candidate dual vectors are verified to be feasible for their respective test cases. I will proceed with a full solution.\n\nThe solution is structured according to the four tasks enumerated in the problem description.\n\n### 1. Derivation of the Benders Cut\n\nIn the context of Benders decomposition for a two-stage linear program, the master problem seeks to minimize a first-stage cost plus the expected future cost, where the future cost is given by the value function $Q(x)$ of a second-stage (recourse) problem. The recourse problem is a linear program parameterized by the master decision vector $x \\in \\mathbb{R}^n$.\n\nThe value function is defined as the optimal value of the recourse problem:\n$$\nQ(x) = \\min_{y} \\{ c^T y \\mid Wy \\ge h - Ex, y \\ge 0 \\}\n$$\nwhere $y$ are the second-stage decision variables.\n\nBy linear programming duality, the value function $Q(x)$ is equal to the optimal value of the dual of the recourse problem:\n$$\nQ(x) = \\max_{\\pi} \\{ (h - Ex)^T \\pi \\mid W^T \\pi \\le c, \\pi \\ge 0 \\}\n$$\nThis holds assuming the primal is feasible for any feasible $x$ and is bounded, which guarantees strong duality. The vector $\\pi \\in \\mathbb{R}^r$ is the vector of dual variables. The problem provides the dual feasibility set directly as a polyhedron $D = \\{ \\pi \\in \\mathbb{R}^r \\mid A\\pi \\le b, \\pi \\ge 0 \\}$. This simplifies the dual problem to:\n$$\nQ(x) = \\max_{\\pi \\in D} \\{ (h - Ex)^T \\pi \\}\n$$\nThe value function $Q(x)$ is convex and piecewise-linear. In Benders decomposition, $Q(x)$ is approximated from below by a set of cutting planes. We introduce a surrogate scalar variable $\\theta$ in the master problem such that $\\theta \\ge Q(x)$.\n\nBy the weak duality theorem, for any dual feasible vector $\\bar{\\pi} \\in D$, the following inequality holds for all $x$:\n$$\nQ(x) \\ge (h - Ex)^T \\bar{\\pi}\n$$\nThis fundamental inequality gives rise to the Benders cuts. Combining with the condition $\\theta \\ge Q(x)$, we obtain valid constraints for the master problem:\n$$\n\\theta \\ge (h - Ex)^T \\bar{\\pi}\n$$\nThis is the analytic form of a Benders cut corresponding to a dual feasible vector $\\bar{\\pi}$. To express it as a hyperplane separating $(x, \\theta)$ from the region below the epigraph of $Q(x)$, we can expand and rearrange the terms:\n$$\n\\theta \\ge h^T\\bar{\\pi} - (E x)^T\\bar{\\pi} = h^T\\bar{\\pi} - x^T E^T \\bar{\\pi}\n$$\nRearranging to the standard affine form gives:\n$$\n(E^T \\bar{\\pi})^T x + \\theta \\ge h^T\\bar{\\pi}\n$$\nThis inequality defines a halfspace in the $(x, \\theta)$ space. The boundary of this halfspace is the hyperplane with coefficients for $x$ given by the vector $\\gamma = E^T \\bar{\\pi}$ and the constant right-hand side given by the scalar $\\delta = h^T \\bar{\\pi}$. All coefficients are defined in terms of the given problem data $E, h$ and the chosen dual feasible vector $\\bar{\\pi}$.\n\n### 2. Selection of the Strongest Cut\n\nGiven a set of candidate dual feasible vectors $\\{ \\pi^{(k)} \\}$, each generates a Benders cut $k$:\n$$\n\\theta \\ge h^T\\pi^{(k)} - (E x)^T\\pi^{(k)}\n$$\nAt a current master solution $(x_0, \\theta_0)$, this cut is satisfied if $\\theta_0 \\ge h^T\\pi^{(k)} - (E x_0)^T\\pi^{(k)}$. If the inequality is violated, the point $(x_0, \\theta_0)$ is \"cut off\". The magnitude of this violation for cut $k$ at $(x_0, \\theta_0)$ is defined as:\n$$\nv_k = \\left( h^T\\pi^{(k)} - (E x_0)^T\\pi^{(k)} \\right) - \\theta_0\n$$\nA positive violation $v_k  0$ means the current solution is infeasible with respect to this cut. The \"strongest\" cut is the one that is most violated, as it cuts deepest into the current infeasible solution space. We select the cut with the largest violation:\n$$\nk^* = \\arg\\max_{k} \\{ v_k \\}\n$$\nIn case of a tie, the problem specifies selecting the smallest index $k$.\n\n### 3. Cut Dominance\n\nA cut $i$, generated by $\\pi^{(i)}$, is said to dominate another cut $j$, generated by $\\pi^{(j)}$, over a bounded master domain $S = \\{x \\in \\mathbb{R}^n \\mid l \\le x \\le u\\}$, if its corresponding affine function provides a lower bound on $\\theta$ that is never worse than the bound from cut $j$.\nLet $f_k(x) = h^T\\pi^{(k)} - (E x)^T\\pi^{(k)}$ be the lower bound provided by cut $k$. Cut $i$ dominates cut $j$ on $S$ if:\n$$\nf_i(x) \\ge f_j(x) \\quad \\forall x \\in S\n$$\nThis is equivalent to checking if the minimum value of the difference function $\\Delta_{ij}(x) = f_i(x) - f_j(x)$ is non-negative over $S$. The difference function is affine:\n$$\n\\Delta_{ij}(x) = (h^T\\pi^{(i)} - (Ex)^T\\pi^{(i)}) - (h^T\\pi^{(j)} - (Ex)^T\\pi^{(j)}) = (h^T(\\pi^{(i)} - \\pi^{(j)})) - x^T(E^T(\\pi^{(i)} - \\pi^{(j)}))\n$$\nLet a constant scalar be $d = h^T(\\pi^{(i)} - \\pi^{(j)})$ and a constant vector be $c = -E^T(\\pi^{(i)} - \\pi^{(j)})$. Then $\\Delta_{ij}(x) = d + c^T x$. The minimum of this affine function over the hyperrectangle $S=[l,u]$ occurs at one of its corners. The minimizing corner point $x^*$ has components:\n$$\nx^*_p = \\begin{cases} l_p  \\text{if } c_p \\ge 0 \\\\ u_p  \\text{if } c_p  0 \\end{cases} \\quad \\text{for } p = 1, \\dots, n\n$$\nCut $i$ dominates cut $j$ if and only if $\\min_{x \\in S} \\Delta_{ij}(x) = d + c^T x^* \\ge 0$.\nTo find the number of dominated cuts, we iterate through all pairs of cuts $(i, j)$ with $i \\neq j$. A cut $j$ is counted as dominated if there exists at least one cut $i$ which dominates it.\n\n### 4. Convergence Metric\n\nThe convergence metric quantifies how closely the current approximation of the value function $Q(x)$ matches the true value at the point $x_0$. The best available lower bound on $Q(x_0)$ from the set of candidate cuts is:\n$$\n\\hat{Q}(x_0) = \\max_{k} \\{ f_k(x_0) \\} = \\max_{k} \\{ h^T\\pi^{(k)} - (E x_0)^T\\pi^{(k)} \\}\n$$\nThe true recourse value is $Q(x_0) = \\max_{\\pi \\in D} \\{ (h - Ex_0)^T \\pi \\}$. This is a linear program that can be solved to find the exact value.\nThe convergence metric, or gap, is the difference between the true value and the best current approximation:\n$$\nG(x_0) = Q(x_0) - \\hat{Q}(x_0)\n$$\nA value of $G(x_0) = 0$ would indicate that at least one of the candidate cuts is optimal for the subproblem at $x_0$, and the approximation is exact at that point.\nThe computation requires solving the LP for $Q(x_0)$ using numerical methods, such as the simplex or interior-point algorithms available in libraries like SciPy.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import linprog\n\ndef solve():\n    \"\"\"\n    Main function to solve the Benders decomposition analysis problem\n    for a suite of test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1\n        {\n            \"n\": 2, \"r\": 3,\n            \"E\": np.array([[1, 0], [-1, 2], [0, 1]]),\n            \"h\": np.array([4, 1, 3]),\n            \"A\": np.array([[1, 1, 1], [2, 0, 1]]),\n            \"b\": np.array([5, 6]),\n            \"l\": np.array([0, 0]),\n            \"u\": np.array([3, 3]),\n            \"x0\": np.array([1, 2]),\n            \"theta0\": 3.0,\n            \"pi_candidates\": [\n                np.array([2.0, 1.0, 1.0]),\n                np.array([0.0, 2.0, 1.0]),\n                np.array([1.0, 0.0, 3.0]),\n                np.array([1.5, 1.5, 0.0])\n            ]\n        },\n        # Test case 2\n        {\n            \"n\": 2, \"r\": 3,\n            \"E\": np.array([[1, 0], [-1, 2], [0, 1]]),\n            \"h\": np.array([4, 1, 3]),\n            \"A\": np.array([[1, 1, 1], [2, 0, 1]]),\n            \"b\": np.array([5, 6]),\n            \"l\": np.array([0, 0]),\n            \"u\": np.array([3, 3]),\n            \"x0\": np.array([3, 3]),\n            \"theta0\": 1.0,\n            \"pi_candidates\": [\n                np.array([1.0, 0.0, 3.0]),\n                np.array([1.0, 0.0, 3.0]),\n                np.array([2.0, 1.0, 1.0]),\n                np.array([0.0, 1.0, 0.0])\n            ]\n        },\n        # Test case 3\n        {\n            \"n\": 1, \"r\": 2,\n            \"E\": np.array([[2], [1]]),\n            \"h\": np.array([2, 4]),\n            \"A\": np.array([[1, 0], [0, 1]]),\n            \"b\": np.array([3, 2]),\n            \"l\": np.array([0]),\n            \"u\": np.array([5]),\n            \"x0\": np.array([5]),\n            \"theta0\": -1.0,\n            \"pi_candidates\": [\n                np.array([3.0, 2.0]),\n                np.array([0.0, 2.0]),\n                np.array([3.0, 0.0])\n            ]\n        }\n    ]\n\n    results_str = []\n    for case in test_cases:\n        E, h, A, b, l, u, x0, theta0, pi_candidates = (\n            case[\"E\"], case[\"h\"], case[\"A\"], case[\"b\"],\n            case[\"l\"], case[\"u\"], case[\"x0\"], case[\"theta0\"],\n            case[\"pi_candidates\"]\n        )\n        n, r = case[\"n\"], case[\"r\"]\n        num_cuts = len(pi_candidates)\n\n        # Task 2: Find the strongest cut and its violation\n        violations = []\n        cut_rh_sides_at_x0 = []\n        for pi_k in pi_candidates:\n            # f_k(x0) = h.T @ pi_k - x0.T @ E.T @ pi_k\n            rh_side_k = h @ pi_k - x0 @ E.T @ pi_k\n            cut_rh_sides_at_x0.append(rh_side_k)\n            violation_k = rh_side_k - theta0\n            violations.append(violation_k)\n\n        best_violation = -np.inf\n        best_index = -1\n        for i, v in enumerate(violations):\n            if v > best_violation:\n                best_violation = v\n                best_index = i\n\n        # Task 3: Count dominated cuts\n        is_dominated = [False] * num_cuts\n        ET = E.T\n        for i in range(num_cuts):\n            for j in range(num_cuts):\n                if i == j:\n                    continue\n                \n                pi_i = pi_candidates[i]\n                pi_j = pi_candidates[j]\n                \n                pi_diff = pi_i - pi_j\n                \n                # Difference function: d + c.T @ x\n                d = h @ pi_diff\n                c = -ET @ pi_diff\n                \n                # Find the corner x_star that minimizes the affine difference function\n                x_star = np.zeros(n)\n                for p in range(n):\n                    if c[p] >= 0:\n                        x_star[p] = l[p]\n                    else:\n                        x_star[p] = u[p]\n                \n                min_diff = d + c @ x_star\n                \n                # Use a small tolerance for floating point comparison\n                if min_diff >= -1e-9:\n                    is_dominated[j] = True\n        \n        dominated_count = sum(is_dominated)\n\n        # Task 4: Calculate the convergence metric\n        # Solve LP for true recourse value Q(x0)\n        # Q(x0) = max (h - E@x0).T @ pi s.t. A@pi = b, pi >= 0\n        lp_obj = h - E @ x0\n        \n        # scipy.optimize.linprog minimizes, so we minimize -lp_obj\n        bounds = [(0, None)] * r\n        res = linprog(-lp_obj, A_ub=A, b_ub=b, bounds=bounds, method='highs')\n        \n        true_recourse_value = -res.fun\n        \n        best_lower_bound = max(cut_rh_sides_at_x0)\n        \n        convergence_gap = true_recourse_value - best_lower_bound\n        \n        # Format and append result for this case\n        result_list = [best_index, best_violation, dominated_count, convergence_gap]\n        \n        # Format floats to 4 decimal places for the output string\n        formatted_result = (\n            f\"[{result_list[0]},\"\n            f\"{result_list[1]:.4f},\"\n            f\"{result_list[2]},\"\n            f\"{result_list[3]:.4f}]\"\n        )\n        results_str.append(formatted_result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results_str)}]\")\n\nsolve()\n```", "id": "3137769"}]}