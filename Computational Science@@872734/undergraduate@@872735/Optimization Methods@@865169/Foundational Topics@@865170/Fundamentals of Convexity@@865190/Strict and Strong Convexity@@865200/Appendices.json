{"hands_on_practices": [{"introduction": "The curvature of a function is not always a global, unchanging property; it can be profoundly influenced by the space over which we analyze it. This first exercise provides a clear, geometric illustration of this principle. We will examine a simple quadratic function that is convex but fails to be strongly convex on the plane due to a 'flat' direction, and then discover how restricting our view to a line within that plane can restore strong convexity, revealing the interplay between a function's form and its domain [@problem_id:3188365].", "problem": "Consider the function $f:\\mathbb{R}^2 \\to \\mathbb{R}$ defined by $f(x,y)=x^2$. Work with the Euclidean norm $\\|\\cdot\\|_2$. Use only core definitions of convexity and strong convexity, and basic multivariable calculus, as the fundamental base of your reasoning.\n\n(a) Using the definition of convexity and the definition of strong convexity on $\\mathbb{R}^2$, justify that $f$ is convex but not strongly convex on $\\mathbb{R}^2$.\n\n(b) Now constrain the domain to the affine line\n$$\n\\mathcal{C}_{\\alpha,\\beta}=\\{(x,y)\\in\\mathbb{R}^2: y=\\alpha x+\\beta\\},\n$$\nwhere $\\alpha\\in\\mathbb{R}$ and $\\beta\\in\\mathbb{R}$ are fixed. Using the definition of strong convexity on a convex set with respect to the Euclidean norm, determine the largest constant $m(\\alpha)$ such that for all $z_1,z_2\\in \\mathcal{C}_{\\alpha,\\beta}$,\n$$\nf(z_2)\\ge f(z_1)+\\nabla f(z_1)^{\\mathsf{T}}(z_2-z_1)+\\frac{m(\\alpha)}{2}\\|z_2-z_1\\|_2^2.\n$$\nYour final answer must be a single closed-form expression for $m(\\alpha)$ that depends only on $\\alpha$ (and not on $\\beta$). Do not provide an inequality or an equation as the final answer. No rounding is required.", "solution": "The problem is analyzed in two parts as specified.\n\nPart (a): Analysis of convexity and strong convexity of $f(x,y) = x^2$ on $\\mathbb{R}^2$.\n\nFirst, we assess the convexity of $f$. A twice-differentiable function is convex on a convex set if and only if its Hessian matrix is positive semidefinite over that set. The domain $\\mathbb{R}^2$ is a convex set. The function is $f(x,y) = x^2$. Its gradient vector is:\n$$\n\\nabla f(x,y) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x} \\\\ \\frac{\\partial f}{\\partial y} \\end{pmatrix} = \\begin{pmatrix} 2x \\\\ 0 \\end{pmatrix}\n$$\nThe Hessian matrix is the matrix of second-order partial derivatives:\n$$\n\\nabla^2 f(x,y) = H = \\begin{pmatrix} \\frac{\\partial^2 f}{\\partial x^2}  \\frac{\\partial^2 f}{\\partial x \\partial y} \\\\ \\frac{\\partial^2 f}{\\partial y \\partial x}  \\frac{\\partial^2 f}{\\partial y^2} \\end{pmatrix} = \\begin{pmatrix} 2  0 \\\\ 0  0 \\end{pmatrix}\n$$\nThe Hessian matrix is constant for all $(x,y) \\in \\mathbb{R}^2$. To check for positive semidefiniteness, we examine the quadratic form $v^{\\mathsf{T}}Hv$ for an arbitrary vector $v = \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix} \\in \\mathbb{R}^2$.\n$$\nv^{\\mathsf{T}}Hv = \\begin{pmatrix} v_1  v_2 \\end{pmatrix} \\begin{pmatrix} 2  0 \\\\ 0  0 \\end{pmatrix} \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix} = \\begin{pmatrix} 2v_1  0 \\end{pmatrix} \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix} = 2v_1^2\n$$\nSince $v_1^2 \\ge 0$, we have $v^{\\mathsf{T}}Hv = 2v_1^2 \\ge 0$ for all $v \\in \\mathbb{R}^2$. Therefore, the Hessian matrix is positive semidefinite, and the function $f(x,y)=x^2$ is convex on $\\mathbb{R}^2$.\n\nNext, we assess the strong convexity of $f$. A differentiable function $f$ is strongly convex on $\\mathbb{R}^2$ if there exists a constant $m  0$ such that for all $z_1, z_2 \\in \\mathbb{R}^2$, the following inequality holds:\n$$\nf(z_2) \\ge f(z_1) + \\nabla f(z_1)^{\\mathsf{T}}(z_2 - z_1) + \\frac{m}{2} \\|z_2 - z_1\\|_2^2\n$$\nLet $z_1 = (x_1, y_1)$ and $z_2 = (x_2, y_2)$. Substituting the expressions for $f$ and its gradient:\n$$\nx_2^2 \\ge x_1^2 + \\begin{pmatrix} 2x_1  0 \\end{pmatrix} \\begin{pmatrix} x_2-x_1 \\\\ y_2-y_1 \\end{pmatrix} + \\frac{m}{2} \\left( (x_2-x_1)^2 + (y_2-y_1)^2 \\right)\n$$\n$$\nx_2^2 \\ge x_1^2 + 2x_1(x_2-x_1) + \\frac{m}{2} \\left( \\|z_2 - z_1\\|_2^2 \\right)\n$$\n$$\nx_2^2 \\ge x_1^2 + 2x_1x_2 - 2x_1^2 + \\frac{m}{2} \\|z_2 - z_1\\|_2^2\n$$\n$$\nx_2^2 - 2x_1x_2 + x_1^2 \\ge \\frac{m}{2} \\|z_2 - z_1\\|_2^2\n$$\n$$\n(x_2-x_1)^2 \\ge \\frac{m}{2} \\left( (x_2-x_1)^2 + (y_2-y_1)^2 \\right)\n$$\nFor $f$ to be strongly convex, this inequality must hold for some constant $m0$ and for all pairs of points $z_1, z_2 \\in \\mathbb{R}^2$. To show it is not strongly convex, we need only find a pair of points for which the inequality is violated for any $m0$. Let us choose points with the same $x$-coordinate, for instance, $x_1=x_2=0$. Let $y_1=0$ and $y_2=1$.\nSo, $z_1=(0,0)$ and $z_2=(0,1)$.\nSubstituting these values into the inequality:\n$$\n(0-0)^2 \\ge \\frac{m}{2} \\left( (0-0)^2 + (1-0)^2 \\right)\n$$\n$$\n0 \\ge \\frac{m}{2} \\left( 0 + 1 \\right)\n$$\n$$\n0 \\ge \\frac{m}{2}\n$$\nThis inequality is false for any $m0$. Since the condition for strong convexity must hold for all points in the domain, and we have found a case where it fails for any positive $m$, we conclude that $f(x,y)=x^2$ is not strongly convex on $\\mathbb{R}^2$.\n\nPart (b): Strong convexity of $f$ on the affine line $\\mathcal{C}_{\\alpha,\\beta}$.\n\nWe are tasked with finding the largest constant $m(\\alpha)$ such that $f$ is strongly convex with this parameter when its domain is restricted to the line $\\mathcal{C}_{\\alpha,\\beta}=\\{(x,y)\\in\\mathbb{R}^2: y=\\alpha x+\\beta\\}$.\nLet $z_1 = (x_1, y_1)$ and $z_2 = (x_2, y_2)$ be two distinct points on the line $\\mathcal{C}_{\\alpha,\\beta}$. This means $y_1 = \\alpha x_1 + \\beta$ and $y_2 = \\alpha x_2 + \\beta$.\nWe use the first-order inequality for strong convexity:\n$$\nf(z_2) \\ge f(z_1) + \\nabla f(z_1)^{\\mathsf{T}}(z_2 - z_1) + \\frac{m(\\alpha)}{2} \\|z_2 - z_1\\|_2^2\n$$\nWe need to find the largest $m(\\alpha)$ for which this inequality holds for all $z_1, z_2 \\in \\mathcal{C}_{\\alpha,\\beta}$.\nLet us express each term as a function of $x_1$ and $x_2$.\nThe left-hand side is $f(z_2) = x_2^2$.\nThe terms on the right-hand side are:\n$f(z_1) = x_1^2$.\n$\\nabla f(z_1) = \\begin{pmatrix} 2x_1 \\\\ 0 \\end{pmatrix}$.\nThe vector difference $z_2-z_1$ is:\n$$\nz_2 - z_1 = \\begin{pmatrix} x_2 - x_1 \\\\ y_2 - y_1 \\end{pmatrix} = \\begin{pmatrix} x_2 - x_1 \\\\ (\\alpha x_2 + \\beta) - (\\alpha x_1 + \\beta) \\end{pmatrix} = \\begin{pmatrix} x_2 - x_1 \\\\ \\alpha(x_2 - x_1) \\end{pmatrix}\n$$\nThe inner product term is:\n$$\n\\nabla f(z_1)^{\\mathsf{T}}(z_2 - z_1) = \\begin{pmatrix} 2x_1  0 \\end{pmatrix} \\begin{pmatrix} x_2 - x_1 \\\\ \\alpha(x_2 - x_1) \\end{pmatrix} = 2x_1(x_2 - x_1)\n$$\nThe squared Euclidean norm is:\n$$\n\\|z_2 - z_1\\|_2^2 = (x_2-x_1)^2 + (y_2-y_1)^2 = (x_2-x_1)^2 + (\\alpha(x_2-x_1))^2 = (1+\\alpha^2)(x_2-x_1)^2\n$$\nSubstituting these expressions back into the strong convexity inequality:\n$$\nx_2^2 \\ge x_1^2 + 2x_1(x_2 - x_1) + \\frac{m(\\alpha)}{2} (1+\\alpha^2)(x_2-x_1)^2\n$$\nRearranging the terms to isolate the quadratic term in $(x_2-x_1)$:\n$$\nx_2^2 - (x_1^2 + 2x_1x_2 - 2x_1^2) \\ge \\frac{m(\\alpha)}{2} (1+\\alpha^2)(x_2-x_1)^2\n$$\n$$\nx_2^2 - 2x_1x_2 + x_1^2 \\ge \\frac{m(\\alpha)}{2} (1+\\alpha^2)(x_2-x_1)^2\n$$\n$$\n(x_2-x_1)^2 \\ge \\frac{m(\\alpha)}{2} (1+\\alpha^2)(x_2-x_1)^2\n$$\nThis inequality must hold for all $x_1, x_2 \\in \\mathbb{R}$. If $x_1=x_2$, the inequality becomes $0 \\ge 0$, which is true. If $x_1 \\ne x_2$, we can divide both sides by the positive quantity $(x_2-x_1)^2$:\n$$\n1 \\ge \\frac{m(\\alpha)}{2} (1+\\alpha^2)\n$$\nTo find the largest possible value for $m(\\alpha)$ that satisfies this condition for all choices of points on the line, we solve for $m(\\alpha)$:\n$$\nm(\\alpha) \\le \\frac{2}{1+\\alpha^2}\n$$\nThe inequality must hold for this $m(\\alpha)$. The largest value that $m(\\alpha)$ can take is therefore the upper bound of this inequality.\nThe largest constant $m(\\alpha)$ is thus $\\frac{2}{1+\\alpha^2}$. This expression depends only on $\\alpha$, as required. Note that $\\beta$ does not appear, as expected, because the function $f$ is independent of $y$, and $\\beta$ only shifts the line vertically.", "answer": "$$\\boxed{\\frac{2}{1+\\alpha^2}}$$", "id": "3188365"}, {"introduction": "Moving from simple geometric examples to applications in data science, we now analyze the objective function of the famous least-squares problem. This practice delves into the convexity of $f(x) = \\|Ax-b\\|$ and its squared counterpart, which are central to regression and machine learning. You will connect abstract linear algebra concepts, like the rank of matrix $A$, to the crucial optimization properties of strict and strong convexity, which determine the existence and uniqueness of solutions [@problem_id:3188362].", "problem": "Consider a matrix $A \\in \\mathbb{R}^{m \\times n}$ and a vector $b \\in \\mathbb{R}^{m}$. Define $f : \\mathbb{R}^{n} \\to \\mathbb{R}$ by $f(x) = \\|A x - b\\|$, where $\\|\\cdot\\|$ denotes the Euclidean norm, and define $g : \\mathbb{R}^{n} \\to \\mathbb{R}$ by $g(x) = \\tfrac{1}{2}\\|A x - b\\|^{2}$. In optimization methods, the notions of convexity, strict convexity, and strong convexity are used to characterize geometric and algorithmic properties of objective functions. Using only the fundamental definitions of convexity, strict convexity, and strong convexity, properties of the Euclidean norm, and the basic rules for composition of functions with affine maps, analyze how the convexity properties of $f$ and $g$ depend on algebraic properties of $A$ and the location of $b$ relative to the range of $A$.\n\nSelect all statements that are true:\n\nA. $f(x)$ is convex for all $A$ and $b$.\n\nB. If $A$ has full column rank (i.e., $A$ is injective as a linear map $\\mathbb{R}^{n} \\to \\mathbb{R}^{m}$) and $b \\notin \\mathrm{range}(A)$, then $f(x)$ is strictly convex.\n\nC. $g(x)$ is strongly convex if and only if $A$ has full column rank, and in that case its strong convexity modulus with respect to the Euclidean norm equals $\\lambda_{\\min}(A^{\\top}A)$, where $\\lambda_{\\min}(\\cdot)$ denotes the smallest eigenvalue.\n\nD. If $b \\in \\mathrm{range}(A)$ and $A$ has full column rank, then $f(x)$ is strictly convex.\n\nE. Squaring the norm, i.e., passing from $f$ to $g$, always yields a strictly convex function regardless of the rank of $A$.", "solution": "The problem statement is a well-posed inquiry within the field of convex optimization. All terms are standard and mathematically precise. The setup is self-contained and free of scientific or logical contradictions. Therefore, the problem is valid, and we may proceed to the solution.\n\nWe are given a matrix $A \\in \\mathbb{R}^{m \\times n}$ and a vector $b \\in \\mathbb{R}^{m}$. We define two functions $f, g: \\mathbb{R}^{n} \\to \\mathbb{R}$ as $f(x) = \\|A x - b\\|$ and $g(x) = \\tfrac{1}{2}\\|A x - b\\|^{2}$, where $\\|\\cdot\\|$ is the Euclidean norm. We analyze their convexity properties based on the definitions.\n\nA function $h: \\mathbb{R}^n \\to \\mathbb{R}$ is:\n1.  **Convex** if for all $x, y \\in \\mathbb{R}^n$ and $\\theta \\in [0, 1]$, $h(\\theta x + (1-\\theta) y) \\le \\theta h(x) + (1-\\theta) h(y)$.\n2.  **Strictly convex** if for all $x \\neq y$ and $\\theta \\in (0, 1)$, $h(\\theta x + (1-\\theta) y)  \\theta h(x) + (1-\\theta) h(y)$.\n3.  **$\\mu$-strongly convex** for $\\mu  0$ if the function $x \\mapsto h(x) - \\frac{\\mu}{2}\\|x\\|^2$ is convex. For a twice-differentiable function, this is equivalent to its Hessian satisfying $\\nabla^2 h(x) \\succeq \\mu I$ for all $x$, where $I$ is the identity matrix.\n\n**Analysis of $f(x) = \\|A x - b\\|$**\n\nThe function $f(x)$ can be viewed as the composition of two functions: the Euclidean norm $h(z) = \\|z\\|$ defined on $\\mathbb{R}^m$, and the affine mapping $T(x) = Ax - b$ from $\\mathbb{R}^n$ to $\\mathbb{R}^m$.\nThe Euclidean norm $h(z) = \\|z\\|$ is a convex function. This follows from the triangle inequality and homogeneity of the norm. For any $z_1, z_2 \\in \\mathbb{R}^m$ and $\\theta \\in [0, 1]$:\n$$\nh(\\theta z_1 + (1-\\theta) z_2) = \\|\\theta z_1 + (1-\\theta) z_2\\| \\le \\|\\theta z_1\\| + \\|(1-\\theta) z_2\\| = \\theta \\|z_1\\| + (1-\\theta) \\|z_2\\| = \\theta h(z_1) + (1-\\theta) h(z_2)\n$$\nThe composition of a convex function with an affine map is always convex. To see this for $f(x)$:\n$$\n\\begin{aligned}\nf(\\theta x + (1-\\theta) y) = \\|A(\\theta x + (1-\\theta) y) - b\\| \\\\\n= \\|\\theta Ax + (1-\\theta) Ay - (\\theta b + (1-\\theta) b)\\| \\\\\n= \\|\\theta(Ax - b) + (1-\\theta)(Ay - b)\\| \\\\\n\\le \\theta \\|Ax - b\\| + (1-\\theta) \\|Ay - b\\| \\quad \\text{(by convexity of the norm)} \\\\\n= \\theta f(x) + (1-\\theta) f(y)\n\\end{aligned}\n$$\nThis holds for all $A$ and $b$, so $f(x)$ is always convex.\n\nFor strict convexity of $f(x)$, the inequality above must be strict for any $x \\neq y$ and $\\theta \\in (0, 1)$. The equality case of the triangle inequality for vectors $u, v$ holds, i.e., $\\|u+v\\| = \\|u\\| + \\|v\\|$, if and only if one vector is a non-negative scalar multiple of the other ($u = c v$ or $v=cu$ for some $c \\ge 0$).\nIn our case, with $u = \\theta(Ax-b)$ and $v = (1-\\theta)(Ay-b)$, the condition for equality is that $Ax-b$ and $Ay-b$ are collinear and point in the same direction (or one is zero). That is, $Ax-b = c(Ay-b)$ for some $c \\ge 0$ or $Ay-b = c(Ax-b)$ for some $c \\ge 0$.\nFor $f(x)$ to be strictly convex, this equality condition must never be met for any $x \\neq y$. Let's analyze when this can happen. Suppose $Ax-b = c(Ay-b)$ for some $c \\ge 0$.\n- If $c = 1$, we have $Ax-b = Ay-b$, which implies $A(x-y)=0$. If $A$ has full column rank (is injective), this implies $x-y=0$, i.e., $x=y$, which contradicts our assumption $x \\neq y$.\n- If $c \\neq 1$, we can write $A(x-cy) = (1-c)b$. This implies $A\\left(\\frac{x-cy}{1-c}\\right) = b$. This means $b$ is in the range of $A$, i.e., $b \\in \\mathrm{range}(A)$.\n- We must also consider the case where $Ay-b = 0$. This implies $b=Ay$, so $b \\in \\mathrm{range}(A)$.\n\nSo, if $f(x)$ is to be strictly convex, we must avoid these scenarios for all $x \\neq y$.\n1.  We must require $A$ to have full column rank to prevent $A(x-y)=0$ for $x \\neq y$.\n2.  We must require $b \\notin \\mathrm{range}(A)$ to prevent $A(z) = b$ for some $z$, which would allow us to construct $x,y$ leading to equality.\nIf both conditions hold ($A$ has full column rank and $b \\notin \\mathrm{range}(A)$), then for any $x \\neq y$, the vectors $Ax-b$ and $Ay-b$ are never collinear in the same direction. Therefore, the inequality in the convexity definition is always strict, and $f(x)$ is strictly convex.\n\n**Analysis of $g(x) = \\tfrac{1}{2}\\|A x - b\\|^{2}$**\n\nThe function $g(x)$ is a quadratic function of $x$:\n$$\ng(x) = \\tfrac{1}{2}(Ax-b)^{\\top}(Ax-b) = \\tfrac{1}{2}(x^{\\top}A^{\\top}Ax - 2b^{\\top}Ax + b^{\\top}b)\n$$\nThis function is twice-differentiable everywhere. We can analyze its convexity by examining its Hessian matrix.\nThe gradient is $\\nabla g(x) = A^{\\top}(Ax - b) = A^{\\top}Ax - A^{\\top}b$.\nThe Hessian is $\\nabla^2 g(x) = A^{\\top}A$.\n- For convexity, the Hessian must be positive semidefinite. The matrix $A^{\\top}A$ is always positive semidefinite for any $A$, because for any vector $v \\in \\mathbb{R}^n$, $v^{\\top}(A^{\\top}A)v = (Av)^{\\top}(Av) = \\|Av\\|^2 \\ge 0$. Thus, $g(x)$ is always convex.\n- For strict convexity, the Hessian must be positive definite. $\\nabla^2 g(x) = A^{\\top}A$ is positive definite if and only if $v^{\\top}(A^{\\top}A)v  0$ for all non-zero $v \\in \\mathbb{R}^n$. This is equivalent to $\\|Av\\|^2  0$ for $v \\neq 0$, which means $Av \\neq 0$ for $v \\neq 0$. This is the definition of $A$ being an injective linear map, which for a matrix means having a trivial null space, i.e., full column rank ($\\mathrm{rank}(A)=n$). The location of $b$ is irrelevant for the strict convexity of $g(x)$.\n- For strong convexity, there must exist a $\\mu  0$ such that $\\nabla^2 g(x) \\succeq \\mu I$. Here, this requires $A^{\\top}A \\succeq \\mu I$, which is equivalent to $A^{\\top}A - \\mu I$ being positive semidefinite. This holds if and only if the minimum eigenvalue of $A^{\\top}A$, denoted $\\lambda_{\\min}(A^{\\top}A)$, is strictly positive. The eigenvalues of $A^{\\top}A$ are non-negative, and $\\lambda_{\\min}(A^{\\top}A)  0$ if and only if $A^{\\top}A$ is invertible, which again is equivalent to $A$ having full column rank. In this case, the largest possible value for $\\mu$, known as the strong convexity modulus, is precisely $\\lambda_{\\min}(A^{\\top}A)$.\n\nNow we evaluate each statement.\n\n**A. $f(x)$ is convex for all $A$ and $b$.**\nAs demonstrated above, $f(x)$ is the composition of the convex Euclidean norm and an affine transformation. Such a composition is always convex.\n**Verdict: Correct.**\n\n**B. If $A$ has full column rank (i.e., $A$ is injective as a linear map $\\mathbb{R}^{n} \\to \\mathbb{R}^{m}$) and $b \\notin \\mathrm{range}(A)$, then $f(x)$ is strictly convex.**\nAs derived in our analysis of $f(x)$, these two conditions together ensure that for any $x \\neq y$, the vectors $Ax-b$ and $Ay-b$ are not collinear in the same direction. This guarantees that the inequality in the definition of convexity is always strict.\n**Verdict: Correct.**\n\n**C. $g(x)$ is strongly convex if and only if $A$ has full column rank, and in that case its strong convexity modulus with respect to the Euclidean norm equals $\\lambda_{\\min}(A^{\\top}A)$, where $\\lambda_{\\min}(\\cdot)$ denotes the smallest eigenvalue.**\nOur Hessian analysis of $g(x)$ showed that it is strongly convex if and only if its constant Hessian, $A^{\\top}A$, has a strictly positive minimum eigenvalue. This is equivalent to $A^{\\top}A$ being positive definite, which is equivalent to $A$ having full column rank. The modulus of strong convexity is indeed the smallest eigenvalue of the Hessian, which is $\\lambda_{\\min}(A^{\\top}A)$.\n**Verdict: Correct.**\n\n**D. If $b \\in \\mathrm{range}(A)$ and $A$ has full column rank, then $f(x)$ is strictly convex.**\nIf $A$ has full column rank and $b \\in \\mathrm{range}(A)$, there exists a unique solution $x_0$ to $Ax=b$. For this $x_0$, we have $f(x_0) = 0$. Consider any other point $y \\neq x_0$ and a point on the segment between them, $z = \\theta x_0 + (1-\\theta)y$ for $\\theta \\in (0, 1)$.\n$f(z) = \\|A(\\theta x_0 + (1-\\theta)y) - b\\| = \\|\\theta Ax_0 + (1-\\theta)Ay - b\\| = \\|\\theta b + (1-\\theta)Ay - b\\| = \\|(1-\\theta)(Ay-b)\\| = (1-\\theta)f(y)$.\nThe strict convexity inequality requires $f(z)  \\theta f(x_0) + (1-\\theta)f(y) = \\theta(0) + (1-\\theta)f(y) = (1-\\theta)f(y)$.\nSince we found $f(z) = (1-\\theta)f(y)$, the strict inequality does not hold. Thus, $f(x)$ is not strictly convex.\n**Verdict: Incorrect.**\n\n**E. Squaring the norm, i.e., passing from $f$ to $g$, always yields a strictly convex function regardless of the rank of $A$.**\nThis statement claims $g(x)=\\tfrac{1}{2}\\|Ax-b\\|^2$ is always strictly convex. Our analysis showed that $g(x)$ is strictly convex if and only if its Hessian $A^{\\top}A$ is positive definite, which is true if and only if $A$ has full column rank. If $A$ does not have full column rank, it has a non-trivial null space. For any $z \\in \\ker(A)$ with $z \\neq 0$, and any $x \\in \\mathbb{R}^n$, the points $x$ and $y=x+z$ are distinct. However, $Ay=A(x+z)=Ax+Az=Ax$. Thus, $g(x) = g(y)$, and for any $\\theta \\in (0,1)$, $g(\\theta x + (1-\\theta)y) = g(x)$. This violates the definition of strict convexity. Therefore, the statement is false.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{ABC}$$", "id": "3188362"}, {"introduction": "Convexity is preserved under many common operations, but what about strict or strong convexity? This exercise explores a subtle but important case where the pointwise infimum of a family of strictly convex functions fails to be strictly convex, a phenomenon with practical implications. You will then practice a powerful and common technique in modern optimization and machine learning—adding a quadratic term ($\\ell_2$ regularization)—to enforce strong convexity, thereby improving the conditioning and solvability of the problem [@problem_id:3188379].", "problem": "Consider the family of functions on the real line defined by $f_{t}:\\mathbb{R}\\to\\mathbb{R}$ with $f_{t}(x)=\\left(x-t\\right)^{2}$ for each parameter $t\\in[-1,1]$. Let $h:\\mathbb{R}\\to\\mathbb{R}$ be the pointwise infimum $h(x)=\\inf_{t\\in[-1,1]}f_{t}(x)$.\n\nUsing only the core definitions of convexity, strict convexity, and strong convexity on $\\mathbb{R}$ (Euclidean norm is the absolute value), do the following:\n\n1. Prove that for each fixed $t\\in[-1,1]$, the function $f_{t}$ is strictly convex, and then derive an explicit expression for $h(x)$ by solving the inner minimization over $t$.\n2. Show that $h$ is convex but not strictly convex by directly analyzing its form and the behavior of its slope.\n3. For a fixed parameter $\\mu0$, define $g_{\\mu}:\\mathbb{R}\\to\\mathbb{R}$ by\n$$\ng_{\\mu}(x)=\\inf_{t\\in[-1,1]}\\Big(f_{t}(x)+\\frac{\\mu}{2}\\,x^{2}\\Big).\n$$\nDerive a fully simplified, explicit expression for $g_{\\mu}(x)$, and justify that $g_{\\mu}$ is strongly convex by identifying its strong convexity parameter from first principles.\n\nYour final answer should be the explicit expression for $g_{\\mu}(x)$ as a single closed-form formula (piecewise is allowed). No numerical rounding is required.", "solution": "We begin from the core definitions. A function $f:\\mathbb{R}\\to\\mathbb{R}$ is convex if for all $x,y\\in\\mathbb{R}$ and all $\\lambda\\in[0,1]$ one has $f(\\lambda x+(1-\\lambda)y)\\leq \\lambda f(x)+(1-\\lambda)f(y)$. It is strictly convex if the inequality is strict whenever $x\\neq y$ and $\\lambda\\in(0,1)$. A function $f$ is $\\mu$-strongly convex with parameter $\\mu0$ if $x\\mapsto f(x)-\\frac{\\mu}{2}|x|^{2}$ is convex, equivalently if for all $x,y\\in\\mathbb{R}$ and $\\lambda\\in[0,1]$,\n$$\nf(\\lambda x+(1-\\lambda)y)\\leq \\lambda f(x)+(1-\\lambda)f(y)-\\frac{\\mu}{2}\\lambda(1-\\lambda)|x-y|^{2}.\n$$\n\nStep 1: Strict convexity of $f_{t}$ and explicit form of $h$. For a fixed $t\\in[-1,1]$, the function $f_{t}(x)=\\left(x-t\\right)^{2}$ is twice differentiable with second derivative $f_{t}''(x)=2$, which is strictly positive for all $x\\in\\mathbb{R}$. A standard consequence of the second-derivative test for convexity on $\\mathbb{R}$ is that a twice differentiable function with strictly positive second derivative is strictly convex. Hence each $f_{t}$ is strictly convex.\n\nTo compute $h(x)=\\inf_{t\\in[-1,1]}(x-t)^{2}$, fix $x\\in\\mathbb{R}$ and minimize the quadratic in $t$ over the interval $[-1,1]$. The unconstrained minimizer in $t$ is obtained by setting the derivative with respect to $t$ to zero:\n$$\n\\frac{\\partial}{\\partial t}(x-t)^{2}=-2(x-t)=0\\quad\\Longrightarrow\\quad t=x.\n$$\nApplying the interval constraint $t\\in[-1,1]$, the constrained minimizer $t^{\\star}(x)$ is the projection (in the Euclidean metric on $\\mathbb{R}$) of $x$ onto $[-1,1]$:\n$$\nt^{\\star}(x)=\n\\begin{cases}\n-1, x-1,\\\\\nx, -1\\leq x\\leq 1,\\\\\n1, x1.\n\\end{cases}\n$$\nTherefore,\n$$\nh(x)=\\min_{t\\in[-1,1]}(x-t)^{2}=\n\\begin{cases}\n(x+1)^{2}, x-1,\\\\\n0, -1\\leq x\\leq 1,\\\\\n(x-1)^{2}, x1.\n\\end{cases}\n$$\nEquivalently, this can be written in a single compact formula as\n$$\nh(x)=\\big(\\max\\{|x|-1,\\,0\\}\\big)^{2}.\n$$\n\nStep 2: Convexity but not strict convexity of $h$. The explicit formula shows that $h(x)=0$ on the entire interval $[-1,1]$, so $h$ has a flat region containing more than one point. Hence $h$ cannot be strictly convex, because strict convexity would force $h(\\lambda x+(1-\\lambda)y)\\lambda h(x)+(1-\\lambda)h(y)$ for $x\\neq y$, which fails when $x,y\\in[-1,1]$ since all such values equal $0$. To see convexity directly, observe that outside $[-1,1]$, $h$ coincides with convex quadratics $(x+1)^{2}$ on $(-\\infty,-1)$ and $(x-1)^{2}$ on $(1,\\infty)$, and it is $0$ on $[-1,1]$. The derivative of $h$ is\n$$\nh'(x)=\n\\begin{cases}\n2(x+1), x-1,\\\\\n0, -1x1,\\\\\n2(x-1), x1,\n\\end{cases}\n$$\nwith one-sided limits $h'(-1^{-})=0$, $h'(-1^{+})=0$, $h'(1^{-})=0$, and $h'(1^{+})=0$. The derivative is nondecreasing on $\\mathbb{R}$, which is equivalent to convexity on $\\mathbb{R}$. Thus $h$ is convex but not strictly convex.\n\nStep 3: Adding a quadratic and strong convexity of the infimum. For $\\mu0$, define\n$$\ng_{\\mu}(x)=\\inf_{t\\in[-1,1]}\\left((x-t)^{2}+\\frac{\\mu}{2}x^{2}\\right).\n$$\nThe term $\\frac{\\mu}{2}x^{2}$ is independent of $t$, so it can be taken outside the infimum:\n$$\ng_{\\mu}(x)=\\frac{\\mu}{2}x^{2}+\\inf_{t\\in[-1,1]}(x-t)^{2}=\\frac{\\mu}{2}x^{2}+h(x).\n$$\nUsing the expression for $h$, we obtain the explicit form\n$$\ng_{\\mu}(x)=\n\\begin{cases}\n\\frac{\\mu}{2}x^{2}+(x+1)^{2}, x-1,\\\\\n\\frac{\\mu}{2}x^{2}, -1\\leq x\\leq 1,\\\\\n\\frac{\\mu}{2}x^{2}+(x-1)^{2}, x1.\n\\end{cases}\n$$\nEquivalently, in a single compact form,\n$$\ng_{\\mu}(x)=\\frac{\\mu}{2}x^{2}+\\big(\\max\\{|x|-1,\\,0\\}\\big)^{2}.\n$$\nTo justify strong convexity, observe from the definition that\n$$\ng_{\\mu}(x)-\\frac{\\mu}{2}x^{2}=h(x),\n$$\nand we have already shown $h$ is convex. By the definition of strong convexity, this implies $g_{\\mu}$ is $\\mu$-strongly convex. Therefore, adding the quadratic $\\frac{\\mu}{2}x^{2}$ enforces strong convexity of the infimum.\n\nThe requested final explicit formula for $g_{\\mu}(x)$ is the piecewise expression above (or its compact equivalent).", "answer": "$$\\boxed{g_{\\mu}(x)=\\begin{cases}\\dfrac{\\mu}{2}x^{2}+(x+1)^{2}, x-1,\\\\[6pt]\\dfrac{\\mu}{2}x^{2}, -1\\leq x\\leq 1,\\\\[6pt]\\dfrac{\\mu}{2}x^{2}+(x-1)^{2}, x1.\\end{cases}}$$", "id": "3188379"}]}