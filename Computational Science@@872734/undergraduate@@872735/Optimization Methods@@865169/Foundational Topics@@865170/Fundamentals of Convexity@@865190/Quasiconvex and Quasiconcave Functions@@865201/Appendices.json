{"hands_on_practices": [{"introduction": "While quasiconvexity extends the reach of convex optimization tools, it is crucial to understand that not all properties of convex functions carry over. This exercise explores a key difference: unlike convex functions, the sum of two quasiconvex functions is not guaranteed to be quasiconvex. By constructing a simple counterexample, you will gain a deeper, hands-on understanding of the structure of sublevel sets and why this fundamental closure property fails [@problem_id:3170821].", "problem": "Let $f:\\mathbb{R}\\to\\mathbb{R}$ and $g:\\mathbb{R}\\to\\mathbb{R}$ be defined by\n$$\nf(x)=\\begin{cases}\n0,  x\\in[-3,-2],\\\\\n1,  \\text{otherwise},\n\\end{cases}\n\\qquad\ng(x)=\\begin{cases}\n0,  x\\in[2,3],\\\\\n1,  \\text{otherwise}.\n\\end{cases}\n$$\nRecall that a function $h:\\mathbb{R}^{n}\\to\\mathbb{R}$ is quasiconvex if and only if each of its sublevel sets $S_{h}(\\alpha):=\\{x\\in\\mathbb{R}^{n}:\\ h(x)\\le \\alpha\\}$ is convex.\n\nUsing only this definition and first principles of set operations, do the following:\n- Show that $f$ and $g$ are quasiconvex on $\\mathbb{R}$.\n- Define $h:=f+g$. Exhibit a value $\\alpha\\in\\mathbb{R}$ for which the sublevel set $S_{h}(\\alpha)$ is not convex, thereby proving that $h$ is not quasiconvex.\n- Derive the identity\n$$\nS_{f+g}(t)\\;=\\;\\bigcup_{\\alpha+\\beta\\le t}\\Big(S_{f}(\\alpha)\\cap S_{g}(\\beta)\\Big),\n$$\nand explain why the right-hand side can be nonconvex even when all $S_{f}(\\alpha)$ and $S_{g}(\\beta)$ are convex. In your explanation, contrast this union-of-intersections structure with the Minkowski addition of sets that arises for horizontal slices of Minkowski sums of epigraphs, to clarify why nonconvex sublevel sets of $f+g$ can occur.\n- Let\n$$\nt_{\\star}\\;:=\\;\\inf\\{\\,t\\in\\mathbb{R}:\\ S_{h}(t)\\ \\text{is convex and nonempty}\\,\\}.\n$$\nCompute $t_{\\star}$ in exact form. Your final answer must be a single real number; no rounding is required.", "solution": "The problem is validated as self-contained, scientifically grounded, and well-posed within the domain of convex analysis.\n\nFirst, we demonstrate that the functions $f$ and $g$ are quasiconvex.\nA function is quasiconvex if all its sublevel sets are convex. The sublevel set of a function $\\phi:\\mathbb{R}\\to\\mathbb{R}$ is $S_{\\phi}(\\alpha) := \\{x \\in \\mathbb{R} : \\phi(x) \\le \\alpha\\}$.\n\nFor the function $f(x)$, its range is the set $\\{0, 1\\}$. We analyze its sublevel sets $S_f(\\alpha)$ for any $\\alpha \\in \\mathbb{R}$:\n- If $\\alpha  0$, the condition $f(x) \\le \\alpha$ is never satisfied, so $S_f(\\alpha) = \\emptyset$. The empty set is convex.\n- If $0 \\le \\alpha  1$, the condition $f(x) \\le \\alpha$ is equivalent to $f(x) = 0$. This occurs for $x \\in [-3, -2]$. Thus, $S_f(\\alpha) = [-3, -2]$. A closed interval is a convex set in $\\mathbb{R}$.\n- If $\\alpha \\ge 1$, the condition $f(x) \\le \\alpha$ is always satisfied, since $f(x)$ is either $0$ or $1$. Thus, $S_f(\\alpha) = \\mathbb{R}$. The entire real line is a convex set.\nSince $S_f(\\alpha)$ is a convex set for all $\\alpha \\in \\mathbb{R}$, the function $f$ is quasiconvex.\n\nFor the function $g(x)$, its range is also $\\{0, 1\\}$. We analyze its sublevel sets $S_g(\\beta)$:\n- If $\\beta  0$, $S_g(\\beta) = \\emptyset$, which is convex.\n- If $0 \\le \\beta  1$, the condition $g(x) \\le \\beta$ is equivalent to $g(x) = 0$. This occurs for $x \\in [2, 3]$. Thus, $S_g(\\beta) = [2, 3]$, which is a convex interval.\n- If $\\beta \\ge 1$, the condition $g(x) \\le \\beta$ is always satisfied. Thus, $S_g(\\beta) = \\mathbb{R}$, which is convex.\nSince $S_g(\\beta)$ is a convex set for all $\\beta \\in \\mathbb{R}$, the function $g$ is quasiconvex.\n\nNext, we define $h(x) := f(x) + g(x)$ and show it is not quasiconvex. We first determine the form of $h(x)$:\n- If $x \\in [-3, -2]$, then $f(x)=0$ and $g(x)=1$, so $h(x) = 0+1=1$.\n- If $x \\in [2, 3]$, then $f(x)=1$ and $g(x)=0$, so $h(x) = 1+0=1$.\n- Otherwise, $x \\notin [-3, -2]$ and $x \\notin [2, 3]$, so $f(x)=1$ and $g(x)=1$, which gives $h(x)=1+1=2$.\nThus, the function $h$ is given by\n$$\nh(x) = \\begin{cases}\n1,  x \\in [-3, -2] \\cup [2, 3] \\\\\n2,  \\text{otherwise}.\n\\end{cases}\n$$\nTo show $h$ is not quasiconvex, we must find a value $\\alpha$ for which the sublevel set $S_h(\\alpha)$ is not convex. Let us choose $\\alpha=1$. The corresponding sublevel set is\n$$\nS_h(1) = \\{x \\in \\mathbb{R} : h(x) \\le 1\\} = \\{x \\in \\mathbb{R} : h(x) = 1\\} = [-3, -2] \\cup [2, 3].\n$$\nThis set is the union of two disjoint intervals. To show it is not convex, we can pick a point from each interval, say $x_1 = -2.5 \\in [-3, -2]$ and $x_2 = 2.5 \\in [2, 3]$. A convex combination of these points, for example with $\\lambda = 0.5$, is $x_{\\lambda} = 0.5(-2.5) + 0.5(2.5) = 0$. For $x=0$, we have $h(0)=2$. Since $h(0)  1$, the point $x=0$ is not in $S_h(1)$. The sublevel set $S_h(1)$ does not contain the line segment between two of its points, so it is not a convex set. Therefore, $h$ is not a quasiconvex function.\n\nNow, we derive the identity $S_{f+g}(t)\\;=\\;\\bigcup_{\\alpha+\\beta\\le t}\\Big(S_{f}(\\alpha)\\cap S_{g}(\\beta)\\Big)$. Let $h=f+g$.\nWe prove this by double inclusion.\n($\\subseteq$): Let $x \\in S_h(t)$. By definition, $f(x)+g(x) \\le t$. Let $\\alpha_0 = f(x)$ and $\\beta_0 = g(x)$. Then $\\alpha_0 + \\beta_0 \\le t$. Furthermore, $f(x) \\le \\alpha_0$ implies $x \\in S_f(\\alpha_0)$, and $g(x) \\le \\beta_0$ implies $x \\in S_g(\\beta_0)$. Thus, $x \\in S_f(\\alpha_0) \\cap S_g(\\beta_0)$. Since there exists a pair $(\\alpha_0, \\beta_0)$ with $\\alpha_0 + \\beta_0 \\le t$ such that $x$ is in the corresponding intersection, $x$ must be in the union over all such pairs. That is, $x \\in \\bigcup_{\\alpha+\\beta\\le t}\\Big(S_{f}(\\alpha)\\cap S_{g}(\\beta)\\Big)$.\n($\\supseteq$): Let $x \\in \\bigcup_{\\alpha+\\beta\\le t}\\Big(S_{f}(\\alpha)\\cap S_{g}(\\beta)\\Big)$. By the definition of a union, there exists at least one pair $(\\alpha_1, \\beta_1)$ such that $\\alpha_1 + \\beta_1 \\le t$ and $x \\in S_f(\\alpha_1) \\cap S_g(\\beta_1)$. From $x \\in S_f(\\alpha_1)$, we have $f(x) \\le \\alpha_1$. From $x \\in S_g(\\beta_1)$, we have $g(x) \\le \\beta_1$. Adding these inequalities gives $f(x) + g(x) \\le \\alpha_1 + \\beta_1$. Since $\\alpha_1 + \\beta_1 \\le t$, we conclude that $f(x)+g(x) \\le t$, which means $x \\in S_{f+g}(t)$.\nThe identity is thus established.\n\nThis identity explains why the sum of two quasiconvex functions is not necessarily quasiconvex. While $f$ and $g$ being quasiconvex ensures that each sublevel set $S_f(\\alpha)$ and $S_g(\\beta)$ is convex, the intersection of two convex sets, $S_f(\\alpha) \\cap S_g(\\beta)$, is also always convex. However, the sublevel set of the sum, $S_{f+g}(t)$, is constructed as a *union* of these convex intersections. The union of convex sets is not, in general, a convex set. Our example for $h$ with $t=1$ exemplifies this: $S_h(1)$ is the union of the sets $[-3, -2]$ (from the pair $(\\alpha,\\beta) = (0,1)$ which has $\\alpha+\\beta=1\\le 1$) and $[2, 3]$ (from the pair $(\\alpha,\\beta)=(1,0)$ which has $\\alpha+\\beta=1\\le 1$), resulting in the non-convex set $[-3, -2] \\cup [2, 3]$. This contrasts sharply with the behavior of convex functions. For *convex* functions $\\phi_1, \\phi_2$, the sum $\\phi_1+\\phi_2$ is also convex. This property is rooted in the fact that the epigraph of the sum is the Minkowski sum of the individual epigraphs, $\\operatorname{epi}(\\phi_1+\\phi_2) = \\operatorname{epi}(\\phi_1) + \\operatorname{epi}(\\phi_2)$, and the Minkowski sum of two convex sets is always convex. Quasiconvexity is a weaker condition defined on sublevel sets, and the operation of function addition does not translate to a convexity-preserving operation like Minkowski sum on these sets. Instead, it leads to the union-of-intersections structure, which can break convexity.\n\nFinally, we compute $t_{\\star}\\;:=\\;\\inf\\{\\,t\\in\\mathbb{R}:\\ S_{h}(t)\\ \\text{is convex and nonempty}\\,\\}$. We analyze the sublevel sets of $h(x)$ for all $t \\in \\mathbb{R}$:\n- For $t  1$: $S_h(t) = \\{x \\in \\mathbb{R} : h(x) \\le t\\} = \\emptyset$. The empty set is convex but not non-empty. So, these values of $t$ are not in the set over which we take the infimum.\n- For $1 \\le t  2$: $S_h(t) = \\{x \\in \\mathbb{R} : h(x) \\le t\\} = [-3, -2] \\cup [2, 3]$. This set is non-empty, but as we have shown, it is not convex.\n- For $t \\ge 2$: $S_h(t) = \\{x \\in \\mathbb{R} : h(x) \\le t\\} = \\mathbb{R}$. This is because the maximum value of $h(x)$ is $2$. The set $\\mathbb{R}$ is both convex and non-empty.\n\nThe set of values of $t$ for which $S_h(t)$ is convex and non-empty is therefore the interval $[2, \\infty)$. The infimum of this set is requested.\n$$\nt_{\\star} = \\inf [2, \\infty) = 2.\n$$\nThe value jumps from being in a non-convex set to a convex set precisely at $t=2$.", "answer": "$$\n\\boxed{2}\n$$", "id": "3170821"}, {"introduction": "Having established some of the theoretical subtleties, we now turn to a constructive application of quasiconvexity. This practice challenges you to prove that a common family of functions, often appearing in modeling signal strength or error metrics, is indeed quasiconvex by analyzing its sublevel sets [@problem_id:3170745]. You will then use this insight to solve a stylized sensor-placement problem, directly connecting the theoretical proof to an optimization outcome.", "problem": "Let $f:\\mathbb{R}^{n}\\to\\mathbb{R}$ be defined by $f(x)=\\|Ax-b\\|_{2}^{\\alpha}+c$ where $A\\in\\mathbb{R}^{m\\times n}$, $b\\in\\mathbb{R}^{m}$, $c\\in\\mathbb{R}$, and $\\alpha\\in(0,1)$. Using only the definition of quasiconvexity in terms of convexity of lower level sets and standard facts about norms and affine maps, rigorously justify that $f$ is quasiconvex on $\\mathbb{R}^{n}$.\n\nThen, consider a sensor-placement design with two candidate sites, modeled by the decision vector $x\\in\\mathbb{R}^{2}$, where $x_{1}$ and $x_{2}$ represent the nonnegative fractions of a fixed unit budget allocated to sites $1$ and $2$, respectively. The allocation must satisfy the budget feasibility set\n$$\nX=\\{x\\in\\mathbb{R}^{2}:\\ x_{1}+x_{2}=1,\\ x_{1}\\ge 0,\\ x_{2}\\ge 0\\},\n$$\nwhich is convex. Suppose the residual mismatch vector is measured relative to a target $b\\in\\mathbb{R}^{2}$ via the identity mapping $A=I_{2}$, i.e., $f(x)=\\|x-b\\|_{2}^{\\alpha}+c$, with $b=\\begin{pmatrix}2\\\\-1\\end{pmatrix}$. Formulate the problem $\\min_{x\\in X}f(x)$ in this context and determine the exact optimal objective value, expressed as a closed-form analytic expression in $\\alpha$ and $c$. No rounding is required. Your final answer must be the single analytic expression for the optimal objective value.", "solution": "The problem consists of two parts. First, we must prove that the function $f(x)=\\|Ax-b\\|_{2}^{\\alpha}+c$ is quasiconvex. Second, we must solve a specific optimization problem involving this function.\n\n**Part 1: Justification of Quasiconvexity**\n\nA function is defined as quasiconvex if its domain is a convex set and all its lower level sets are convex. The function is $f:\\mathbb{R}^{n}\\to\\mathbb{R}$, defined as $f(x)=\\|Ax-b\\|_{2}^{\\alpha}+c$, where $A\\in\\mathbb{R}^{m\\times n}$, $b\\in\\mathbb{R}^{m}$, $c\\in\\mathbb{R}$, and $\\alpha\\in(0,1)$.\n\nThe domain of $f$ is $\\mathbb{R}^{n}$, which is a convex set. To establish that $f$ is quasiconvex, we must show that for any real number $\\gamma$, the lower level set (or sublevel set) $S_{\\gamma}$ defined as\n$$S_{\\gamma} = \\{ x \\in \\mathbb{R}^{n} \\mid f(x) \\le \\gamma \\}$$\nis a convex set.\n\nThe condition $f(x) \\le \\gamma$ can be written as:\n$$\\|Ax-b\\|_{2}^{\\alpha}+c \\le \\gamma$$\n$$\\|Ax-b\\|_{2}^{\\alpha} \\le \\gamma - c$$\n\nWe consider two cases based on the value of the right-hand side.\n\nCase 1: $\\gamma - c  0$.\nIn this case, the inequality $\\|Ax-b\\|_{2}^{\\alpha} \\le \\gamma - c$ has no solution, because the norm term $\\|Ax-b\\|_{2}^{\\alpha}$ is always non-negative. Thus, the lower level set $S_{\\gamma}$ is the empty set, $S_{\\gamma} = \\emptyset$. The empty set is convex by definition.\n\nCase 2: $\\gamma - c \\ge 0$.\nSince $\\alpha \\in (0,1)$, we have $\\alpha > 0$. The function $t \\mapsto t^{1/\\alpha}$ is well-defined and strictly increasing for $t \\ge 0$. We can therefore raise both sides of the inequality to the power of $1/\\alpha$ without changing the inequality's direction:\n$$(\\|Ax-b\\|_{2}^{\\alpha})^{1/\\alpha} \\le (\\gamma - c)^{1/\\alpha}$$\n$$\\|Ax-b\\|_{2} \\le (\\gamma - c)^{1/\\alpha}$$\nLet us define a non-negative constant $R = (\\gamma - c)^{1/\\alpha}$. The lower level set can now be characterized as:\n$$S_{\\gamma} = \\{ x \\in \\mathbb{R}^{n} \\mid \\|Ax-b\\|_{2} \\le R \\}$$\nLet's define a map $T:\\mathbb{R}^{n}\\to\\mathbb{R}^{m}$ as $T(x) = Ax-b$. This map is an affine transformation, since it is the sum of a linear transformation ($x \\mapsto Ax$) and a constant vector ($-b$).\n\nLet $C$ be a set in $\\mathbb{R}^{m}$ defined as:\n$$C = \\{ y \\in \\mathbb{R}^{m} \\mid \\|y\\|_{2} \\le R \\}$$\nThis set $C$ is a closed ball of radius $R$ centered at the origin in $\\mathbb{R}^{m}$. A closed ball is a well-known example of a convex set.\n\nThe lower level set $S_{\\gamma}$ can be expressed in terms of $T$ and $C$:\n$$S_{\\gamma} = \\{ x \\in \\mathbb{R}^{n} \\mid T(x) \\in C \\}$$\nThis shows that $S_{\\gamma}$ is the preimage of the convex set $C$ under the affine map $T$. A fundamental property of affine maps is that the preimage of a convex set is convex.\n\nTo formally verify this property, let $x_1, x_2 \\in S_{\\gamma}$ and let $\\theta \\in [0,1]$. We need to show that the convex combination $x_{\\theta} = \\theta x_1 + (1-\\theta)x_2$ is also in $S_{\\gamma}$.\nSince $x_1, x_2 \\in S_{\\gamma}$, we have $T(x_1) \\in C$ and $T(x_2) \\in C$.\nBecause $T$ is an affine map, it preserves convex combinations:\n$$T(x_{\\theta}) = T(\\theta x_1 + (1-\\theta)x_2) = \\theta T(x_1) + (1-\\theta)T(x_2)$$\nSince $C$ is a convex set and $T(x_1), T(x_2) \\in C$, their convex combination $\\theta T(x_1) + (1-\\theta)T(x_2)$ must also lie in $C$.\nTherefore, $T(x_{\\theta}) \\in C$, which implies $x_{\\theta} \\in S_{\\gamma}$.\nThis proves that $S_{\\gamma}$ is a convex set for any $\\gamma$.\n\nSince the domain of $f$ is convex and all its lower level sets are convex, the function $f$ is quasiconvex on $\\mathbb{R}^{n}$.\n\n**Part 2: Optimization Problem**\n\nThe problem is to find the minimum value of $f(x) = \\|x-b\\|_{2}^{\\alpha}+c$ over the set $X=\\{x\\in\\mathbb{R}^{2}:\\ x_{1}+x_{2}=1,\\ x_{1}\\ge 0,\\ x_{2}\\ge 0\\}$.\nThe parameters are given as $x=\\begin{pmatrix}x_1 \\\\ x_2\\end{pmatrix}$, $b=\\begin{pmatrix}2\\\\-1\\end{pmatrix}$, $\\alpha\\in(0,1)$, and $c\\in\\mathbb{R}$.\n\nThe optimization problem is:\n$$\\min_{x \\in X} \\left( \\|x-b\\|_{2}^{\\alpha}+c \\right)$$\nThe term $c$ is a constant, so it shifts the objective value but does not affect the location of the minimum. The function $t \\mapsto t^{\\alpha}$ is strictly increasing for $t \\ge 0$ since $\\alpha > 0$. Therefore, minimizing $f(x)$ is equivalent to minimizing $\\|x-b\\|_{2}$. This is a geometric problem: finding the point $x^* \\in X$ that has the minimum Euclidean distance to the point $b$. This point $x^*$ is the projection of $b$ onto the convex set $X$.\n\nThe set $X$ is the line segment in $\\mathbb{R}^{2}$ connecting the points $v_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ and $v_2 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$. Any point $x \\in X$ can be parameterized by $\\lambda \\in [0,1]$ as:\n$$x(\\lambda) = (1-\\lambda)v_2 + \\lambda v_1 = (1-\\lambda)\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} + \\lambda\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\lambda \\\\ 1-\\lambda \\end{pmatrix}$$\nWe want to minimize the distance from $x(\\lambda)$ to $b=\\begin{pmatrix}2\\\\-1\\end{pmatrix}$ for $\\lambda \\in [0,1]$. It is equivalent to minimize the squared distance, $D(\\lambda) = \\|x(\\lambda)-b\\|_{2}^{2}$.\n$$D(\\lambda) = \\left\\|\\begin{pmatrix} \\lambda \\\\ 1-\\lambda \\end{pmatrix} - \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}\\right\\|_{2}^{2} = \\left\\|\\begin{pmatrix} \\lambda-2 \\\\ 2-\\lambda \\end{pmatrix}\\right\\|_{2}^{2}$$\n$$D(\\lambda) = (\\lambda-2)^{2} + (2-\\lambda)^{2} = 2(\\lambda-2)^{2}$$\nWe need to find the minimum of $D(\\lambda)$ for $\\lambda \\in [0,1]$. Let's examine the derivative of $D(\\lambda)$ with respect to $\\lambda$:\n$$D'(\\lambda) = \\frac{d}{d\\lambda} (2(\\lambda-2)^{2}) = 4(\\lambda-2)$$\nFor any $\\lambda$ in the interval $[0,1]$, the term $(\\lambda-2)$ is negative. Thus, $D'(\\lambda)  0$ for all $\\lambda \\in [0,1]$. This means that $D(\\lambda)$ is a strictly decreasing function on the interval $[0,1]$.\nFor a strictly decreasing function on a closed interval, the minimum value is achieved at the right endpoint of the interval. Therefore, the minimum occurs at $\\lambda=1$.\n\nThe optimal parameter is $\\lambda^{*} = 1$. The corresponding optimal point $x^*$ is:\n$$x^* = x(1) = \\begin{pmatrix} 1 \\\\ 1-1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$$\nThe minimum squared distance is $D(1) = 2(1-2)^2 = 2$.\nThe minimum distance is $\\min_{x\\in X}\\|x-b\\|_{2} = \\sqrt{D(1)} = \\sqrt{2}$.\n\nThe problem asks for the optimal objective value, which is $\\min_{x\\in X}f(x) = f(x^*)$.\n$$f(x^*) = \\|x^*-b\\|_{2}^{\\alpha}+c = (\\sqrt{2})^{\\alpha} + c$$\nThis can be written as $2^{\\alpha/2} + c$.", "answer": "$$\\boxed{2^{\\frac{\\alpha}{2}}+c}$$", "id": "3170745"}, {"introduction": "The ultimate goal in recognizing a problem's structure is to leverage it for efficient computation. This practice brings theory to life by guiding you through the implementation of the bisection method, a powerful algorithm for optimizing quasiconvex and quasiconcave functions [@problem_id:3170813]. By repeatedly checking the feasibility of level sets using linear programming, you will build a solver that can efficiently tackle a class of non-convex problems that might otherwise seem intractable.", "problem": "You are to implement the level-set method for quasiconcave maximization via binary search on a scalar threshold. The objective function is defined as $f(x)=\\min_{i\\in\\{1,\\dots,m\\}} a_i^\\top x$, where each $a_i\\in\\mathbb{R}^n$ and the feasible region $X\\subset\\mathbb{R}^n$ is a polytope described by linear inequalities and bound constraints. The method relies on testing feasibility of level sets: for a given threshold $\\alpha\\in\\mathbb{R}$, check whether there exists $x\\in X$ such that $f(x)\\ge \\alpha$. Using this monotone feasibility property, implement a binary search on $\\alpha$ to approximate the maximum value of $f(x)$ over $X$.\n\nBase definitions to use:\n- A function $f:\\mathbb{R}^n\\to\\mathbb{R}$ is quasiconcave if every upper level set $\\{x\\in\\mathbb{R}^n:\\ f(x)\\ge \\alpha\\}$ is convex for all $\\alpha\\in\\mathbb{R}$.\n- A polytope $X$ is the solution set of finitely many linear inequalities and bound constraints.\n- Linear Programming (LP) is the class of optimization problems with linear objective and linear constraints.\n\nCore feasibility equivalence to use:\n- For $f(x)=\\min_i a_i^\\top x$, the upper level set $\\{x\\in X:\\ f(x)\\ge \\alpha\\}$ is equivalent to $\\{x\\in X:\\ a_i^\\top x\\ge \\alpha\\ \\text{for all}\\ i=1,\\dots,m\\}$, which is the intersection of halfspaces. Feasibility of this set can be checked by a Linear Programming (LP) solver by setting a zero objective and only constraints.\n\nImplementation requirements:\n1. Use binary search on $\\alpha$ within a bounded interval $[\\alpha_\\text{low},\\alpha_\\text{high}]$ where:\n   - $\\alpha_\\text{low}$ is obtained by evaluating $f(x_0)$ at some feasible point $x_0\\in X$. You may obtain $x_0$ by solving a Linear Programming (LP) with a zero objective and only the constraints defining $X$.\n   - $\\alpha_\\text{high}$ is obtained from the bound $f(x)\\le \\min_{i} \\max_{x\\in X} a_i^\\top x$, which you should compute by solving $m$ Linear Programming (LP) problems that maximize $a_i^\\top x$ over $X$ (equivalently minimize $-a_i^\\top x$).\n2. For a given $\\alpha$, test feasibility by solving a Linear Programming (LP) with zero objective, subject to the constraints defining $X$ and the additional constraints $a_i^\\top x\\ge \\alpha$ for all $i$. This is equivalent to $-a_i^\\top x\\le -\\alpha$ for all $i$ and can be handled by inequality constraints.\n3. Continue binary search until the interval length is less than a tolerance and return the final $\\alpha_\\text{low}$ as the approximation of the maximum of $f(x)$ over $X$.\n\nYour program must be a single, complete, runnable Python program using the specified environment. It must solve the following test suite and produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each value rounded to $6$ decimal places:\n\nTest Suite:\n- Case $1$ (happy path): $n=2$, $X=\\{x\\in\\mathbb{R}^2:\\ 0\\le x_1\\le 1,\\ 0\\le x_2\\le 1\\}$, with $a_1^\\top=[1,0]$, $a_2^\\top=[0,1]$. The expected maximizer attains the value $\\alpha^\\star=1$.\n- Case $2$ (boundary coverage): $n=3$, $X=\\{x\\in\\mathbb{R}^3:\\ x_i\\ge 0\\ \\text{for}\\ i\\in\\{1,2,3\\},\\ x_1+x_2+x_3\\le 1\\}$, with $a_1^\\top=[1,0,0]$, $a_2^\\top=[0,1,0]$, $a_3^\\top=[0,0,1]$. The expected maximizer attains the value $\\alpha^\\star=1/3$.\n- Case $3$ (edge case with negative directions): $n=2$, $X=\\{x\\in\\mathbb{R}^2:\\ 0\\le x_1\\le 1,\\ 0\\le x_2\\le 1\\}$, with $a_1^\\top=[-1,0]$, $a_2^\\top=[0,-1]$, $a_3^\\top=[1,-1]$. The expected maximizer attains the value $\\alpha^\\star=0$.\n\nTolerance and output precision:\n- Use a binary search tolerance of $10^{-7}$ on the interval for $\\alpha$.\n- Output each caseâ€™s approximated maximum of $f(x)$ as a float rounded to $6$ decimal places.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[0.123456,0.333333,1.000000]$).", "solution": "The user wants to implement a bisection algorithm to find the maximum value of a quasiconcave function over a polytope.\n\n### Step 1: Extract Givens\n\n- **Objective Function:** $f(x)=\\min_{i\\in\\{1,\\dots,m\\}} a_i^\\top x$, where $x \\in \\mathbb{R}^n$ and $a_i\\in\\mathbb{R}^n$.\n- **Feasible Region:** A polytope $X\\subset\\mathbb{R}^n$ defined by a finite set of linear inequalities and bound constraints.\n- **Algorithm:** Bisection method (binary search) on a scalar threshold $\\alpha$.\n- **Feasibility Test:** For a given $\\alpha$, check if the set $\\{x\\in X:\\ f(x)\\ge \\alpha\\}$ is non-empty. This is equivalent to checking the feasibility of the linear system: $x \\in X$ and $a_i^\\top x \\ge \\alpha$ for all $i \\in \\{1, \\dots, m\\}$. This feasibility check is performed using a Linear Programming (LP) solver with a zero objective function.\n- **Initial Interval $[\\alpha_\\text{low}, \\alpha_\\text{high}]$**:\n    - $\\alpha_\\text{low}$ is determined by finding a feasible point $x_0 \\in X$ and calculating $f(x_0)$. The point $x_0$ is found by solving an LP with a zero objective over $X$.\n    - $\\alpha_\\text{high}$ is determined by the upper bound $U = \\min_{i=1,\\dots,m} \\left( \\max_{x\\in X} a_i^\\top x \\right)$. This requires solving $m$ separate LP problems.\n- **Termination Condition:** The binary search continues until the interval length $(\\alpha_\\text{high} - \\alpha_\\text{low})$ is less than a tolerance of $10^{-7}$.\n- **Output:** The final value of $\\alpha_\\text{low}$, rounded to $6$ decimal places.\n- **Test Cases:**\n    1.  $n=2$, $X=\\{x\\in\\mathbb{R}^2 : 0\\le x_1\\le 1, 0\\le x_2\\le 1\\}$, $a_1=[1,0]^\\top$, $a_2=[0,1]^\\top$.\n    2.  $n=3$, $X=\\{x\\in\\mathbb{R}^3 : x_i\\ge 0 \\text{ for } i\\in\\{1,2,3\\}, x_1+x_2+x_3\\le 1\\}$, $a_1=[1,0,0]^\\top$, $a_2=[0,1,0]^\\top$, $a_3=[0,0,1]^\\top$.\n    3.  $n=2$, $X=\\{x\\in\\mathbb{R}^2 : 0\\le x_1\\le 1, 0\\le x_2\\le 1\\}$, $a_1=[-1,0]^\\top$, $a_2=[0,-1]^\\top$, $a_3=[1,-1]^\\top$.\n\n### Step 2: Validate Using Extracted Givens\n\n- **Scientifically Grounded:** The problem is a standard exercise in the field of convex and quasiconvex optimization. The function $f(x) = \\min_i a_i^\\top x$ is the pointwise minimum of a set of linear (and therefore concave) functions. The pointwise minimum of concave functions is a quasiconcave function. The bisection method based on checking the feasibility of level sets is a classical and correct algorithm for optimizing quasiconvex/quasiconcave functions. The method for establishing the search interval and performing the feasibility check using LPs is also standard and mathematically sound.\n- **Well-Posed:** The problem is well-posed. The objective function is continuous and the feasible set $X$ is a compact polytope for all test cases. Therefore, a maximum is guaranteed to exist. The bisection algorithm is guaranteed to converge to this maximum value.\n- **Objective:** The problem is stated in precise mathematical terms, free of ambiguity or subjective language.\n\nThe problem statement is internally consistent, scientifically sound, and well-posed. All required information is provided.\n\n### Step 3: Verdict and Action\n\nThe problem is valid. A solution will be provided.\n\n### Principle-Based Solution\n\nThe problem requires us to find the maximum of a function $f(x)$ over a set $X$, which can be formally stated as:\n$$\n\\text{maximize} \\quad f(x) = \\min_{i=1,\\dots,m} a_i^\\top x \\\\\n\\text{subject to} \\quad x \\in X\n$$\nwhere $X$ is a polytope. This is a quasiconcave maximization problem.\n\n**1. Quasiconcavity and the Bisection Method**\n\nA function $f(x)$ is quasiconcave if its upper level sets, $S_\\alpha = \\{x \\mid f(x) \\ge \\alpha\\}$, are convex for all $\\alpha \\in \\mathbb{R}$. For our function $f(x) = \\min_{i} a_i^\\top x$, the upper level set is:\n$$\nS_\\alpha = \\{x \\mid \\min_{i=1,\\dots,m} a_i^\\top x \\ge \\alpha\\} = \\{x \\mid a_i^\\top x \\ge \\alpha \\text{ for all } i=1,\\dots,m\\}\n$$\nEach condition $a_i^\\top x \\ge \\alpha$ defines a closed half-space, which is a convex set. The intersection of a finite number of convex sets is also a convex set. Therefore, $S_\\alpha$ is convex, and $f(x)$ is quasiconcave.\n\nLet $\\alpha^\\star = \\max_{x \\in X} f(x)$ be the optimal value. The key property that enables the bisection method is that the feasibility of finding a point in the intersection of the level set and the feasible region, i.e., the non-emptiness of the set $S_\\alpha \\cap X$, is monotonic with respect to $\\alpha$.\n- If there exists an $x \\in X$ such that $f(x) \\ge \\alpha$, then the optimal value $\\alpha^\\star$ must be at least $\\alpha$.\n- If there is no $x \\in X$ such that $f(x) \\ge \\alpha$, then the optimal value $\\alpha^\\star$ must be less than $\\alpha$.\n\nThis monotonic property allows us to use a binary search (bisection) on the value of $\\alpha$ to find $\\alpha^\\star$.\n\n**2. Feasibility Checking via Linear Programming**\n\nThe core of the bisection algorithm is the feasibility check: for a given $\\alpha$, is the set $\\{x \\in X \\mid f(x) \\ge \\alpha\\}$ empty? This set is described by the original constraints defining the polytope $X$ plus the new constraints $a_i^\\top x \\ge \\alpha$ for all $i$. Since all these constraints are linear inequalities, they collectively define another polytope. Determining if a polytope is empty is a standard linear feasibility problem, which can be solved using a Linear Programming (LP) solver. We formulate an LP with a zero objective function, $c = 0$, and all the combined linear constraints. If the LP solver finds a feasible solution, the set is non-empty. Otherwise, it is empty.\n\n**3. The Algorithm**\n\nThe bisection algorithm proceeds as follows:\n\n- **Step A: Initialization of the Search Interval $[\\alpha_\\text{low}, \\alpha_\\text{high}]$**\n    An initial interval that is guaranteed to contain the optimal value $\\alpha^\\star$ is required.\n    1.  **Lower Bound ($\\alpha_\\text{low}$):** We find any feasible point $x_0 \\in X$. This can be done by solving an LP with a zero objective function subject only to the constraints defining $X$. If a solution $x_0$ is found, we compute $f(x_0)$. Since $x_0 \\in X$, we know that $\\alpha^\\star \\ge f(x_0)$. We can thus set $\\alpha_\\text{low} = f(x_0)$.\n    2.  **Upper Bound ($\\alpha_\\text{high}$):** For any $x \\in X$, we have $f(x) = \\min_i a_i^\\top x \\le a_j^\\top x$ for any $j \\in \\{1,\\dots,m\\}$. This further implies $f(x) \\le \\max_{y \\in X} a_j^\\top y$. Since this holds for all $j$, it must also hold for the minimum over $j$:\n        $$\n        f(x) \\le \\min_{j=1,\\dots,m} \\left( \\max_{y \\in X} a_j^\\top y \\right)\n        $$\n        This gives a valid upper bound on $\\alpha^\\star$. We compute this bound by solving $m$ LPs, where the $j$-th LP maximizes $a_j^\\top x$ over $X$. Then we set $\\alpha_\\text{high}$ to the minimum of these $m$ maximum values.\n\n- **Step B: Bisection Iteration**\n    While the interval width $(\\alpha_\\text{high} - \\alpha_\\text{low})$ is greater than a specified tolerance $\\epsilon$:\n    1.  Compute the midpoint: $\\alpha_\\text{mid} = (\\alpha_\\text{low} + \\alpha_\\text{high}) / 2$.\n    2.  Solve the feasibility LP for $\\alpha_\\text{mid}$. The constraints are those of $X$ plus $a_i^\\top x \\ge \\alpha_\\text{mid}$ for all $i$.\n    3.  If a feasible solution exists, it means $\\alpha^\\star \\ge \\alpha_\\text{mid}$. We update the lower bound: $\\alpha_\\text{low} = \\alpha_\\text{mid}$.\n    4.  If no feasible solution exists, it means $\\alpha^\\star  \\alpha_\\text{mid}$. We update the upper bound: $\\alpha_\\text{high} = \\alpha_\\text{mid}$.\n\n- **Step C: Termination**\n    The loop terminates when $\\alpha_\\text{high} - \\alpha_\\text{low} \\le \\epsilon$. The final value of $\\alpha_\\text{low}$ is a certified lower bound on $\\alpha^\\star$ and is within $\\epsilon$ of the true optimum. This value is returned as the solution.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import linprog\n\ndef solve_quasiconcave_max(a_vectors, A_ub, b_ub, bounds, tol=1e-7):\n    \"\"\"\n    Maximizes a quasiconcave function f(x) = min(a_i^T x) over a polytope X\n    using a bisection method on the function value alpha.\n\n    Args:\n        a_vectors (np.ndarray): Matrix where rows are the vectors a_i.\n        A_ub (np.ndarray or None): Matrix A for inequality constraints A @ x = b of the polytope X.\n        b_ub (np.ndarray or None): Vector b for inequality constraints A @ x = b of the polytope X.\n        bounds (list of tuples): Bounds for each variable x_i in X.\n        tol (float): Tolerance for the binary search interval width.\n\n    Returns:\n        float: The approximate maximum value of f(x).\n    \"\"\"\n    n = a_vectors.shape[1]\n    m = a_vectors.shape[0]\n    c_zero = np.zeros(n)\n    \n    # --- Step 1: Find initial search interval [alpha_low, alpha_high] ---\n\n    # Find an initial feasible point x0 to establish a lower bound.\n    res_x0 = linprog(c_zero, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method='highs-ds')\n    if not res_x0.success:\n        # The feasible set X is empty.\n        # This case is not expected for the given problem.\n        raise ValueError(\"Feasible region X is empty.\")\n    x0 = res_x0.x\n    \n    # alpha_low is f(x0), a guaranteed achievable value.\n    alpha_low = np.min(a_vectors @ x0)\n\n    # Find the upper bound by solving m LPs: max_{x in X} (a_i^T x) for each i.\n    max_vals = []\n    for i in range(m):\n        # linprog minimizes, so to maximize a_i^T x, we minimize -a_i^T x.\n        c_i = -a_vectors[i]\n        res_max = linprog(c_i, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method='highs-ds')\n        if not res_max.success:\n            # Should not happen for bounded polytopes.\n            # Could indicate an unbounded problem if bounds were not provided.\n             raise ValueError(f\"Could not solve LP to find upper bound for a_{i}.\")\n        # The maximum value is the negative of the minimum found by linprog.\n        max_val = -res_max.fun\n        max_vals.append(max_val)\n    \n    # alpha_high is the minimum of these maximums.\n    alpha_high = np.min(max_vals)\n    \n    # --- Step 2: Binary Search ---\n    \n    while alpha_high - alpha_low  tol:\n        alpha_mid = (alpha_low + alpha_high) / 2\n        \n        # Check feasibility of {x in X | f(x) = alpha_mid}.\n        # This adds constraints a_i^T x = alpha_mid for all i.\n        # In linprog format: -a_i^T x = -alpha_mid.\n        level_set_A = -a_vectors\n        level_set_b = -np.full(m, alpha_mid)\n\n        # Combine constraints of X and the level set.\n        if A_ub is not None and b_ub is not None:\n            feas_A_ub = np.vstack((A_ub, level_set_A))\n            feas_b_ub = np.concatenate((b_ub, level_set_b))\n        else:\n            feas_A_ub = level_set_A\n            feas_b_ub = level_set_b\n\n        # Solve the feasibility LP (zero objective function).\n        res_feas = linprog(c_zero, A_ub=feas_A_ub, b_ub=feas_b_ub, bounds=bounds, method='highs-ds')\n        \n        if res_feas.success:\n            # A feasible point exists for alpha_mid, so the true max is at least alpha_mid.\n            alpha_low = alpha_mid\n        else:\n            # No feasible point exists, the true max must be less than alpha_mid.\n            alpha_high = alpha_mid\n            \n    # --- Step 3: Return result ---\n    return alpha_low\n\ndef solve():\n    \"\"\"\n    Defines and runs the test suite for the quasiconcave maximization problem.\n    \"\"\"\n    \n    # Case 1: n=2, X=[0,1]^2, a1=[1,0], a2=[0,1]\n    case1_a = np.array([[1, 0], [0, 1]])\n    case1_A_ub = None\n    case1_b_ub = None\n    case1_bounds = [(0, 1), (0, 1)]\n\n    # Case 2: n=3, X={x=0, sum(x)=1}, a_i are standard basis vectors\n    case2_a = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n    case2_A_ub = np.array([[1, 1, 1]])\n    case2_b_ub = np.array([1])\n    case2_bounds = [(0, None), (0, None), (0, None)]\n    \n    # Case 3: n=2, X=[0,1]^2, a1=[-1,0], a2=[0,-1], a3=[1,-1]\n    case3_a = np.array([[-1, 0], [0, -1], [1, -1]])\n    case3_A_ub = None\n    case3_b_ub = None\n    case3_bounds = [(0, 1), (0, 1)]\n\n    test_cases = [\n        (case1_a, case1_A_ub, case1_b_ub, case1_bounds),\n        (case2_a, case2_A_ub, case2_b_ub, case2_bounds),\n        (case3_a, case3_A_ub, case3_b_ub, case3_bounds),\n    ]\n\n    results = []\n    for case in test_cases:\n        a_vectors, A_ub, b_ub, bounds = case\n        result = solve_quasiconcave_max(a_vectors, A_ub, b_ub, bounds)\n        results.append(result)\n\n    # Format output to 6 decimal places as specified.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3170813"}]}