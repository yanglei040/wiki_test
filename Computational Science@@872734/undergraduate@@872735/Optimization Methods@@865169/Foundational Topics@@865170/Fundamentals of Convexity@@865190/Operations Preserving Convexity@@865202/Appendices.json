{"hands_on_practices": [{"introduction": "Many complex optimization problems, especially in fields like machine learning, are built by combining simpler functions. This exercise explores how the fundamental operations of function addition and composition with linear maps preserve convexity and even strong convexity, which is crucial for ensuring that these complex models remain tractable. You will analyze a common structure, $f(x) = g(Ax) + h(x)$, to understand the rules that govern these combinations [@problem_id:3160286].", "problem": "Let $A \\in \\mathbb{R}^{m \\times n}$ be a fixed matrix, let $g:\\mathbb{R}^m \\to \\mathbb{R}$ be any function, and let $h:\\mathbb{R}^n \\to \\mathbb{R}$ be any function. Consider the composite objective $f:\\mathbb{R}^n \\to \\mathbb{R}$ defined by $f(x) = g(Ax) + h(x)$. This form models a data-fit term $g$ applied to the linear features $Ax$ together with a regularizer $h$ on the decision variable $x$. Using only the definitions of convexity and strong convexity and elementary properties of linear maps, determine which of the following statements are true about operations that preserve convexity and strong convexity in this setting.\n\nA. If $g$ is convex and $h$ is convex, then $f$ is convex for any choice of the matrix $A$.\n\nB. If $g$ is convex and $h$ is $\\mu$-strongly convex for some $\\mu>0$, then $f$ is $\\mu$-strongly convex for any choice of the matrix $A$.\n\nC. If $g$ is $m$-strongly convex for some $m>0$ and $A$ has full column rank, and $h$ is convex, then $f$ is $m\\,\\lambda_{\\min}(A^\\top A)$-strongly convex, where $\\lambda_{\\min}(A^\\top A)$ denotes the smallest eigenvalue of $A^\\top A$.\n\nD. Choosing $h(x)=\\lambda\\|x\\|_1$ with $\\lambda>0$ always makes $f$ strongly convex, even if $g$ is only convex.\n\nE. If $A$ does not have full column rank, then no choice of convex $h$ can make $f$ strongly convex.\n\nSelect all correct statements.", "solution": "The problem statement is a well-posed mathematical inquiry within the field of convex optimization. It is scientifically grounded, objective, and contains sufficient information to determine the truth value of the provided statements. I will proceed with the solution.\n\nFirst, let us state the definitions of convexity and strong convexity, as per the problem's instructions.\n\nA function $F: \\mathcal{D} \\to \\mathbb{R}$ defined on a convex domain $\\mathcal{D} \\subseteq \\mathbb{R}^k$ is said to be **convex** if for all $x, y \\in \\mathcal{D}$ and for all $\\theta \\in [0, 1]$, the following inequality holds:\n$$F(\\theta x + (1-\\theta)y) \\le \\theta F(x) + (1-\\theta)F(y)$$\nA function $F: \\mathcal{D} \\to \\mathbb{R}$ is **$\\sigma$-strongly convex** for some $\\sigma > 0$ if for all $x, y \\in \\mathcal{D}$ and for all $\\theta \\in [0, 1]$, the following inequality holds:\n$$F(\\theta x + (1-\\theta)y) \\le \\theta F(x) + (1-\\theta)F(y) - \\frac{\\sigma}{2}\\theta(1-\\theta)\\|x-y\\|_2^2$$\nAn equivalent and often more useful definition for $\\sigma$-strong convexity is that the function $x \\mapsto F(x) - \\frac{\\sigma}{2}\\|x\\|_2^2$ is convex.\n\nWe are given the composite function $f:\\mathbb{R}^n \\to \\mathbb{R}$ defined as $f(x) = g(Ax) + h(x)$, where $A \\in \\mathbb{R}^{m \\times n}$, $g:\\mathbb{R}^m \\to \\mathbb{R}$, and $h:\\mathbb{R}^n \\to \\mathbb{R}$. We will now analyze each statement.\n\n### A. If $g$ is convex and $h$ is convex, then $f$ is convex for any choice of the matrix $A$.\n\nLet's analyze the two terms of $f(x)$ separately.\n1.  The term $h(x)$ is given to be convex.\n2.  Consider the term $\\phi(x) = g(Ax)$. The function $x \\mapsto Ax$ is a linear map from $\\mathbb{R}^n$ to $\\mathbb{R}^m$. For any $x, y \\in \\mathbb{R}^n$ and $\\theta \\in [0, 1]$, we have:\n    $$\\phi(\\theta x + (1-\\theta)y) = g(A(\\theta x + (1-\\theta)y))$$\n    By the linearity of the matrix-vector product, this becomes:\n    $$\\phi(\\theta x + (1-\\theta)y) = g(\\theta(Ax) + (1-\\theta)(Ay))$$\n    Since $g$ is convex, and $Ax, Ay \\in \\mathbb{R}^m$, we can apply the definition of convexity to $g$:\n    $$g(\\theta(Ax) + (1-\\theta)(Ay)) \\le \\theta g(Ax) + (1-\\theta)g(Ay)$$\n    Substituting back $\\phi(x)$ and $\\phi(y)$, we get:\n    $$\\phi(\\theta x + (1-\\theta)y) \\le \\theta \\phi(x) + (1-\\theta)\\phi(y)$$\n    This shows that $\\phi(x) = g(Ax)$ is a convex function. This property holds for any matrix $A$.\n3.  The function $f(x)$ is the sum of two convex functions, $\\phi(x) = g(Ax)$ and $h(x)$. The sum of convex functions is convex. To demonstrate this from first principles:\n    $$f(\\theta x + (1-\\theta)y) = g(A(\\theta x + (1-\\theta)y)) + h(\\theta x + (1-\\theta)y)$$\n    Since both terms are convex:\n    $$f(\\theta x + (1-\\theta)y) \\le [\\theta g(Ax) + (1-\\theta)g(Ay)] + [\\theta h(x) + (1-\\theta)h(y)]$$\n    $$f(\\theta x + (1-\\theta)y) \\le \\theta(g(Ax) + h(x)) + (1-\\theta)(g(Ay) + h(y))$$\n    $$f(\\theta x + (1-\\theta)y) \\le \\theta f(x) + (1-\\theta)f(y)$$\n    Therefore, $f(x)$ is convex.\n\nTherefore, statement (A) is **Correct**.\n\n### B. If $g$ is convex and $h$ is $\\mu$-strongly convex for some $\\mu>0$, then $f$ is $\\mu$-strongly convex for any choice of the matrix $A$.\n\nTo check if $f(x)$ is $\\mu$-strongly convex, we examine the convexity of the function $F(x) = f(x) - \\frac{\\mu}{2}\\|x\\|_2^2$.\n$$F(x) = (g(Ax) + h(x)) - \\frac{\\mu}{2}\\|x\\|_2^2$$\nWe can rearrange the terms as:\n$$F(x) = g(Ax) + \\left(h(x) - \\frac{\\mu}{2}\\|x\\|_2^2\\right)$$\n1.  From the analysis of statement A, since $g$ is convex, the function $x \\mapsto g(Ax)$ is also convex for any matrix $A$.\n2.  By the definition of $\\mu$-strong convexity for $h$, the function $x \\mapsto h(x) - \\frac{\\mu}{2}\\|x\\|_2^2$ is convex.\n3.  $F(x)$ is the sum of two convex functions. As shown in the analysis for A, the sum of two convex functions is convex.\nSince $F(x) = f(x) - \\frac{\\mu}{2}\\|x\\|_2^2$ is convex, $f(x)$ is, by definition, $\\mu$-strongly convex. This result does not depend on the choice of $A$.\n\nTherefore, statement (B) is **Correct**.\n\n### C. If $g$ is $m$-strongly convex for some $m>0$ and $A$ has full column rank, and $h$ is convex, then $f$ is $m\\,\\lambda_{\\min}(A^\\top A)$-strongly convex, where $\\lambda_{\\min}(A^\\top A)$ denotes the smallest eigenvalue of $A^\\top A$.\n\nLet's analyze the strong convexity of $\\phi(x) = g(Ax)$. Since $g$ is $m$-strongly convex, for any $u, v \\in \\mathbb{R}^m$ and $\\theta \\in [0,1]$:\n$$g(\\theta u + (1-\\theta)v) \\le \\theta g(u) + (1-\\theta)g(v) - \\frac{m}{2}\\theta(1-\\theta)\\|u-v\\|_2^2$$\nLet $u=Ax$ and $v=Ay$. Then:\n$$\\phi(\\theta x + (1-\\theta)y) = g(\\theta Ax + (1-\\theta)Ay) \\le \\theta g(Ax) + (1-\\theta)g(Ay) - \\frac{m}{2}\\theta(1-\\theta)\\|Ax-Ay\\|_2^2$$\n$$\\phi(\\theta x + (1-\\theta)y) \\le \\theta \\phi(x) + (1-\\theta)\\phi(y) - \\frac{m}{2}\\theta(1-\\theta)\\|A(x-y)\\|_2^2$$\nWe can write the squared norm as a quadratic form: $\\|A(x-y)\\|_2^2 = (x-y)^\\top A^\\top A (x-y)$. The matrix $A^\\top A \\in \\mathbb{R}^{n \\times n}$ is always positive semidefinite. The statement specifies that $A$ has full column rank, which means $\\text{rank}(A) = n$. This implies that $A^\\top A$ is positive definite, and all its eigenvalues are strictly positive. The smallest eigenvalue, $\\lambda_{\\min}(A^\\top A)$, is therefore greater than $0$.\nFor any vector $z \\in \\mathbb{R}^n$, the Rayleigh-Ritz quotient gives the bound $z^\\top(A^\\top A)z \\ge \\lambda_{\\min}(A^\\top A)\\|z\\|_2^2$. Applying this with $z=x-y$:\n$$\\|A(x-y)\\|_2^2 \\ge \\lambda_{\\min}(A^\\top A)\\|x-y\\|_2^2$$\nSubstituting this into the inequality for $\\phi$:\n$$\\phi(\\theta x + (1-\\theta)y) \\le \\theta \\phi(x) + (1-\\theta)\\phi(y) - \\frac{m\\,\\lambda_{\\min}(A^\\top A)}{2}\\theta(1-\\theta)\\|x-y\\|_2^2$$\nThis is the definition of strong convexity for $\\phi(x)$ with modulus $\\sigma_\\phi = m\\,\\lambda_{\\min}(A^\\top A)$.\nNow, $f(x) = \\phi(x) + h(x)$, where $\\phi(x)$ is $\\sigma_\\phi$-strongly convex and $h(x)$ is convex. Let's check if $f(x)$ is $\\sigma_\\phi$-strongly convex. Consider the function $f(x) - \\frac{\\sigma_\\phi}{2}\\|x\\|_2^2$:\n$$f(x) - \\frac{\\sigma_\\phi}{2}\\|x\\|_2^2 = \\left(\\phi(x) - \\frac{\\sigma_\\phi}{2}\\|x\\|_2^2\\right) + h(x)$$\nThe first term, $\\phi(x) - \\frac{\\sigma_\\phi}{2}\\|x\\|_2^2$, is convex by the definition of $\\sigma_\\phi$-strong convexity of $\\phi$. The second term, $h(x)$, is given as convex. The sum of these two convex functions is convex. Thus, $f(x)$ is $\\sigma_\\phi = m\\,\\lambda_{\\min}(A^\\top A)$-strongly convex.\n\nTherefore, statement (C) is **Correct**.\n\n### D. Choosing $h(x)=\\lambda\\|x\\|_1$ with $\\lambda>0$ always makes $f$ strongly convex, even if $g$ is only convex.\n\nLet $f(x) = g(Ax) + \\lambda\\|x\\|_1$.\nWe are given that $g$ is convex. From the analysis for statement A, $g(Ax)$ is a convex function. The function $h(x) = \\lambda\\|x\\|_1$ is also convex for $\\lambda > 0$, as it is a positive scaling of the $\\ell_1$-norm. Thus, $f(x)$ is always convex. The question is whether it is always *strongly* convex.\nFor $f$ to be strongly convex, the function $f(x) - \\frac{\\mu}{2}\\|x\\|_2^2$ must be convex for some $\\mu>0$.\nLet's choose a simple counterexample. Let $g$ be the zero function, $g(z)=0$ for all $z \\in \\mathbb{R}^m$. The zero function is convex. In this case, $f(x) = \\lambda\\|x\\|_1$. Is $h(x) = \\lambda\\|x\\|_1$ strongly convex?\nLet's check the definition. Let $n=1$, so $x \\in \\mathbb{R}$. $h(x) = \\lambda|x|$.\nLet $x=c$ and $y=-c$ for some $c>0$. Let $\\theta=1/2$. The midpoint is $z=\\theta x + (1-\\theta)y = 0$.\nThe strong convexity condition is $h(z) \\le \\theta h(x) + (1-\\theta) h(y) - \\frac{\\mu}{2}\\theta(1-\\theta)\\|x-y\\|_2^2$.\n$$h(0) \\le \\frac{1}{2}h(c) + \\frac{1}{2}h(-c) - \\frac{\\mu}{2}\\frac{1}{2}\\frac{1}{2}\\|c - (-c)\\|_2^2$$\n$$0 \\le \\frac{1}{2}(\\lambda c) + \\frac{1}{2}(\\lambda c) - \\frac{\\mu}{8}(2c)^2$$\n$$0 \\le \\lambda c - \\frac{\\mu}{8}(4c^2)$$\n$$0 \\le \\lambda c - \\frac{\\mu}{2}c^2$$\n$$\\frac{\\mu}{2}c^2 \\le \\lambda c$$\nSince $c>0$, we can divide by it: $c \\le \\frac{2\\lambda}{\\mu}$.\nThis inequality must hold for all $c>0$ for the function to be strongly convex. However, we can clearly choose a value of $c$ that violates this, for example $c = 1 + \\frac{2\\lambda}{\\mu}$. Thus, $\\lambda\\|x\\|_1$ is not a strongly convex function. Since we found a valid choice of convex $g$ for which $f$ is not strongly convex, the statement that this choice of $h$ *always* makes $f$ strongly convex is false.\n\nTherefore, statement (D) is **Incorrect**.\n\n### E. If $A$ does not have full column rank, then no choice of convex $h$ can make $f$ strongly convex.\n\nThis statement claims that if $A$ does not have full column rank, then for *any* convex function $h$, the resulting function $f(x) = g(Ax) + h(x)$ cannot be strongly convex.\nTo disprove this, we need to find a counterexample: a matrix $A$ without full column rank and a convex function $h$ such that $f$ is strongly convex.\nLet's refer back to statement B: \"If $g$ is convex and $h$ is $\\mu$-strongly convex for some $\\mu>0$, then $f$ is $\\mu$-strongly convex for any choice of the matrix $A$.\"\nA strongly convex function is also a convex function. So, we can choose $h$ to be a $\\mu$-strongly convex function. According to statement B, which we have proven to be correct, this choice of $h$ will make $f$ be $\\mu$-strongly convex, regardless of the properties of $A$. This holds even if $A$ does not have full column rank.\nLet's provide a concrete counterexample.\nLet $n=2$, $m=1$. Let $A = \\begin{pmatrix} 1 & 0 \\end{pmatrix}$. This matrix does not have full column rank since its rank is $1$ while it has $2$ columns. Its null space is spanned by the vector $\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$.\nLet $g: \\mathbb{R} \\to \\mathbb{R}$ be $g(z)=0$, which is convex.\nLet $h: \\mathbb{R}^2 \\to \\mathbb{R}$ be $h(x) = \\frac{\\mu}{2}\\|x\\|_2^2 = \\frac{\\mu}{2}(x_1^2 + x_2^2)$ for some $\\mu>0$. The function $h$ is convex (it is even $\\mu$-strongly convex).\nThen $f(x) = g(Ax) + h(x) = 0 + \\frac{\\mu}{2}(x_1^2 + x_2^2) = \\frac{\\mu}{2}\\|x\\|_2^2$.\nThe function $f(x)$ is $\\mu$-strongly convex by definition.\nWe have found a matrix $A$ without full column rank and a convex function $h$ such that $f$ is strongly convex. This contradicts the statement.\n\nTherefore, statement (E) is **Incorrect**.\n\nFinal summary: Statements A, B, and C are correct, while D and E are incorrect.", "answer": "$$\\boxed{ABC}$$", "id": "3160286"}, {"introduction": "After seeing how to build convex functions, a natural question is how robust this property is. This practice provides a hands-on way to explore the \"boundary\" of convexity by subtracting a simple quadratic term, $\\epsilon \\|x\\|^{2}$. By determining the exact threshold at which a convex function becomes non-convex, you will gain a deeper, quantitative understanding of convexity and its connection to the important concept of strong convexity [@problem_id:3160294].", "problem": "Consider the function $f : \\mathbb{R}^{2} \\to \\mathbb{R}$ defined by $f(x) = \\frac{1}{2} x^{\\top} A x + b^{\\top} x + c$, where $x \\in \\mathbb{R}^{2}$, $A = \\begin{pmatrix} 4 & 2 \\\\ 2 & 3 \\end{pmatrix}$, $b = \\begin{pmatrix} 1 \\\\ -3 \\end{pmatrix}$, and $c = 0$. Define the modified function $g_{\\epsilon}(x) = f(x) - \\epsilon \\|x\\|^{2}$, where $\\|x\\|^{2} = x^{\\top} x$ and $\\epsilon \\ge 0$ is a scalar.\n\nStart from the fundamental definition of convexity: a function $h$ is convex if for all $x, y \\in \\mathbb{R}^{2}$ and $t \\in [0,1]$, $h(tx + (1-t)y) \\le t h(x) + (1-t) h(y)$. Use the well-tested characterization that a twice continuously differentiable function is convex if and only if its Hessian is positive semidefinite (PSD) everywhere, where positive semidefinite (PSD) means all eigenvalues are nonnegative.\n\nDetermine the largest value $\\epsilon^{\\star}$ such that $g_{\\epsilon}$ is convex on $\\mathbb{R}^{2}$ if and only if $\\epsilon \\le \\epsilon^{\\star}$. Provide your final answer as a closed-form analytic expression. Do not round.", "solution": "The user wants to find the largest value of a non-negative scalar, denoted as $\\epsilon^{\\star}$, such that the function $g_{\\epsilon}(x)$ remains convex for all $\\epsilon$ in the range $0 \\le \\epsilon \\le \\epsilon^{\\star}$.\n\nThe problem begins with the definition of a quadratic function $f: \\mathbb{R}^{2} \\to \\mathbb{R}$:\n$$f(x) = \\frac{1}{2} x^{\\top} A x + b^{\\top} x + c$$\nwhere $A = \\begin{pmatrix} 4 & 2 \\\\ 2 & 3 \\end{pmatrix}$, $b = \\begin{pmatrix} 1 \\\\ -3 \\end{pmatrix}$, and $c=0$.\n\nA modified function $g_{\\epsilon}(x)$ is defined as:\n$$g_{\\epsilon}(x) = f(x) - \\epsilon \\|x\\|^{2}$$\nwhere $\\|x\\|^{2} = x^{\\top} x = x^{\\top} I x$, with $I$ being the $2 \\times 2$ identity matrix. The scalar $\\epsilon$ is given to be non-negative, i.e., $\\epsilon \\ge 0$.\n\nWe can substitute the expressions for $f(x)$ and $\\|x\\|^{2}$ into the definition of $g_{\\epsilon}(x)$ to obtain a consolidated quadratic form.\n$$g_{\\epsilon}(x) = \\left(\\frac{1}{2} x^{\\top} A x + b^{\\top} x + c\\right) - \\epsilon (x^{\\top} I x)$$\n$$g_{\\epsilon}(x) = \\frac{1}{2} x^{\\top} A x - \\epsilon x^{\\top} I x + b^{\\top} x + c$$\nBy factoring out the quadratic term, we get:\n$$g_{\\epsilon}(x) = \\frac{1}{2} x^{\\top} (A - 2\\epsilon I) x + b^{\\top} x + c$$\nThis expresses $g_{\\epsilon}(x)$ as a standard quadratic function with the associated matrix $A - 2\\epsilon I$.\n\nThe problem states that a twice continuously differentiable function is convex if and only if its Hessian matrix is positive semidefinite (PSD) everywhere. The function $g_{\\epsilon}(x)$ is a quadratic polynomial, so it is infinitely differentiable.\n\nThe Hessian of a general quadratic function $h(x) = \\frac{1}{2} x^{\\top} Q x + d^{\\top} x + k$, where $Q$ is a symmetric matrix, is given by $\\nabla^2 h(x) = Q$.\nIn our case, the matrix for $g_{\\epsilon}(x)$ is $Q_{\\epsilon} = A - 2\\epsilon I$. The matrix $A$ is symmetric, and the identity matrix $I$ is symmetric. Thus, $Q_{\\epsilon}$ is also symmetric.\nThe Hessian of $g_{\\epsilon}(x)$ is therefore constant and equal to this matrix:\n$$\\nabla^2 g_{\\epsilon}(x) = A - 2\\epsilon I$$\n\nFor $g_{\\epsilon}(x)$ to be convex, its Hessian must be positive semidefinite.\n$$\\nabla^2 g_{\\epsilon}(x) \\succeq 0 \\quad \\iff \\quad A - 2\\epsilon I \\succeq 0$$\nThe notation $\\succeq 0$ indicates that the matrix is positive semidefinite. A symmetric matrix is positive semidefinite if and only if all its eigenvalues are non-negative.\n\nLet $\\lambda$ be an eigenvalue of $A - 2\\epsilon I$ and $v$ be the corresponding eigenvector. Then:\n$$(A - 2\\epsilon I)v = \\lambda v$$\n$$Av - 2\\epsilon Iv = \\lambda v$$\n$$Av = (\\lambda + 2\\epsilon)v$$\nThis shows that $v$ is also an eigenvector of $A$, with eigenvalue $\\lambda_A = \\lambda + 2\\epsilon$.\nConversely, if $\\lambda_A$ is an eigenvalue of $A$, then the corresponding eigenvalue of $A - 2\\epsilon I$ is $\\lambda = \\lambda_A - 2\\epsilon$.\n\nThe condition that all eigenvalues of $A - 2\\epsilon I$ are non-negative translates to:\n$$\\lambda_A - 2\\epsilon \\ge 0 \\quad \\text{for all eigenvalues } \\lambda_A \\text{ of } A$$\nThis inequality can be rearranged to find a constraint on $\\epsilon$:\n$$2\\epsilon \\le \\lambda_A$$\n$$\\epsilon \\le \\frac{\\lambda_A}{2}$$\nThis must hold for all eigenvalues of $A$. Therefore, $\\epsilon$ must be less than or equal to half the minimum eigenvalue of $A$.\n$$\\epsilon \\le \\frac{1}{2} \\min(\\lambda_A)$$\nwhere $\\min(\\lambda_A)$ denotes the smallest eigenvalue of the matrix $A$.\n\nThe problem asks for the largest value $\\epsilon^{\\star}$ such that $g_{\\epsilon}$ is convex if and only if $\\epsilon \\le \\epsilon^{\\star}$. This value is precisely $\\frac{1}{2} \\min(\\lambda_A)$.\n$$\\epsilon^{\\star} = \\frac{1}{2} \\min(\\lambda_A)$$\n\nNext, we must compute the eigenvalues of the matrix $A = \\begin{pmatrix} 4 & 2 \\\\ 2 & 3 \\end{pmatrix}$. The eigenvalues are the roots of the characteristic equation $\\det(A - \\lambda I) = 0$.\n$$\\det \\begin{pmatrix} 4-\\lambda & 2 \\\\ 2 & 3-\\lambda \\end{pmatrix} = 0$$\n$$(4-\\lambda)(3-\\lambda) - (2)(2) = 0$$\n$$12 - 4\\lambda - 3\\lambda + \\lambda^2 - 4 = 0$$\n$$\\lambda^2 - 7\\lambda + 8 = 0$$\n\nWe solve this quadratic equation for $\\lambda$ using the quadratic formula, $\\lambda = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$:\n$$\\lambda = \\frac{-(-7) \\pm \\sqrt{(-7)^2 - 4(1)(8)}}{2(1)}$$\n$$\\lambda = \\frac{7 \\pm \\sqrt{49 - 32}}{2}$$\n$$\\lambda = \\frac{7 \\pm \\sqrt{17}}{2}$$\n\nThe two eigenvalues of $A$ are $\\lambda_1 = \\frac{7 + \\sqrt{17}}{2}$ and $\\lambda_2 = \\frac{7 - \\sqrt{17}}{2}$.\nThe smallest eigenvalue is:\n$$\\min(\\lambda_A) = \\frac{7 - \\sqrt{17}}{2}$$\nNote that since $\\sqrt{16}=4$ and $\\sqrt{25}=5$, $\\sqrt{17}$ is between $4$ and $5$. Thus, $7 - \\sqrt{17}$ is positive, and so is $\\min(\\lambda_A)$. This confirms that the original function $f(x)$ is strictly convex.\n\nFinally, we determine the value of $\\epsilon^{\\star}$:\n$$\\epsilon^{\\star} = \\frac{1}{2} \\min(\\lambda_A) = \\frac{1}{2} \\left( \\frac{7 - \\sqrt{17}}{2} \\right)$$\n$$\\epsilon^{\\star} = \\frac{7 - \\sqrt{17}}{4}$$\nThis is the largest value of $\\epsilon$ for which $g_{\\epsilon}(x)$ remains convex. The initial condition $\\epsilon \\ge 0$ is satisfied since $7 - \\sqrt{17} > 7 - 5 = 2 > 0$.", "answer": "$$\\boxed{\\frac{7 - \\sqrt{17}}{4}}$$", "id": "3160294"}, {"introduction": "This final practice moves from a purely algebraic view to a powerful geometric one, connecting operations on functions to operations on the sets defined by their epigraphs. You will investigate the Minkowski sum of the epigraphs of two convex functions and derive the resulting function, an operation known as infimal convolution. This exercise reveals that the geometry of convex sets provides a rich source of new convexity-preserving operations, distinct from simple pointwise addition [@problem_id:3160290].", "problem": "Consider the real-valued functions $f:\\mathbb{R}\\to\\mathbb{R}$ and $g:\\mathbb{R}\\to\\mathbb{R}$ defined by $f(x)=\\frac{1}{2}x^{2}$ and $g(x)=\\frac{1}{2}(x-1)^{2}$. Let the epigraph of a function $h:\\mathbb{R}\\to\\mathbb{R}$ be defined by $\\operatorname{epi}(h)=\\{(x,t)\\in\\mathbb{R}\\times\\mathbb{R}\\,:\\,t\\geq h(x)\\}$. The Minkowski sum of two subsets $A,B\\subseteq\\mathbb{R}^{2}$ is defined by $A\\oplus B=\\{a+b\\,:\\,a\\in A,\\ b\\in B\\}$, where the addition is componentwise. Starting from the core definitions of convex sets, convex functions, epigraphs, and Minkowski sums, reason about the set $H=\\operatorname{epi}(f)\\oplus\\operatorname{epi}(g)$: first, justify whether $H$ is convex and whether it can be represented as the epigraph of some real-valued function $h:\\mathbb{R}\\to\\mathbb{R}$. Then, derive $h(x)$ explicitly and in closed form by eliminating set quantifiers through first principles, without invoking any pre-packaged results or named constructions beyond the given definitions. Finally, compare the derived $h(x)$ to the pointwise sum $f(x)+g(x)$ to illustrate how the operation on epigraphs relates to operations preserving convexity. Provide your final answer as the simplified analytic expression for $h(x)$. No rounding is required, and no units are involved.", "solution": "The problem is valid. It is scientifically grounded in the established principles of convex analysis, well-posed with a clear and complete set of definitions, and stated objectively. All necessary information is provided, and the task is to derive a result from first principles, which is a standard and meaningful exercise in mathematics.\n\nThe problem asks to analyze the set $H = \\operatorname{epi}(f) \\oplus \\operatorname{epi}(g)$, where $f(x) = \\frac{1}{2}x^2$ and $g(x) = \\frac{1}{2}(x-1)^2$. The analysis involves three parts: justifying the convexity of $H$ and its representation as an epigraph, explicitly deriving the function $h(x)$ for which $H = \\operatorname{epi}(h)$, and comparing $h(x)$ to the pointwise sum $f(x)+g(x)$.\n\nFirst, we determine if $H$ is a convex set. The convexity of a function is equivalent to the convexity of its epigraph. A function $k:\\mathbb{R} \\to \\mathbb{R}$ is convex if its second derivative is non-negative, provided it is twice differentiable.\nFor $f(x) = \\frac{1}{2}x^2$, the derivatives are $f'(x) = x$ and $f''(x) = 1$. Since $f''(x) = 1 > 0$ for all $x \\in \\mathbb{R}$, the function $f$ is strictly convex.\nFor $g(x) = \\frac{1}{2}(x-1)^2$, the derivatives are $g'(x) = x-1$ and $g''(x) = 1$. Since $g''(x) = 1 > 0$ for all $x \\in \\mathbb{R}$, the function $g$ is also strictly convex.\nBecause both $f$ and $g$ are convex functions, their respective epigraphs, $\\operatorname{epi}(f)$ and $\\operatorname{epi}(g)$, are convex sets in $\\mathbb{R}^2$.\n\nNext, we must show that the Minkowski sum of two convex sets is itself a convex set. Let $A$ and $B$ be two convex sets in $\\mathbb{R}^2$. Their Minkowski sum is $A \\oplus B = \\{a+b \\mid a \\in A, b \\in B\\}$. Let $p_1, p_2$ be two arbitrary points in $A \\oplus B$. By definition, there exist $a_1, a_2 \\in A$ and $b_1, b_2 \\in B$ such that $p_1 = a_1+b_1$ and $p_2 = a_2+b_2$. To show that $A \\oplus B$ is convex, we must show that the line segment connecting $p_1$ and $p_2$ is contained in $A \\oplus B$. A point on this segment can be written as $p_\\lambda = \\lambda p_1 + (1-\\lambda) p_2$ for any $\\lambda \\in [0, 1]$.\nSubstituting the expressions for $p_1$ and $p_2$:\n$$p_\\lambda = \\lambda(a_1+b_1) + (1-\\lambda)(a_2+b_2)$$\nRearranging the terms by linearity of vector addition and scalar multiplication:\n$$p_\\lambda = (\\lambda a_1 + (1-\\lambda) a_2) + (\\lambda b_1 + (1-\\lambda) b_2)$$\nSince $A$ is a convex set, the convex combination $\\lambda a_1 + (1-\\lambda) a_2$ is a point in $A$. Let's call this point $a_\\lambda$.\nSince $B$ is a convex set, the convex combination $\\lambda b_1 + (1-\\lambda) b_2$ is a point in $B$. Let's call this point $b_\\lambda$.\nThus, $p_\\lambda = a_\\lambda + b_\\lambda$, where $a_\\lambda \\in A$ and $b_\\lambda \\in B$. By the definition of the Minkowski sum, this means $p_\\lambda \\in A \\oplus B$.\nSince this holds for any $p_1, p_2 \\in A \\oplus B$ and any $\\lambda \\in [0, 1]$, the set $A \\oplus B$ is convex.\nApplying this result, since $\\operatorname{epi}(f)$ and $\\operatorname{epi}(g)$ are convex sets, their Minkowski sum $H = \\operatorname{epi}(f) \\oplus \\operatorname{epi}(g)$ is also a convex set.\n\nSecond, we ascertain whether $H$ can be represented as the epigraph of some function $h:\\mathbb{R} \\to \\mathbb{R}$. A set $S \\subseteq \\mathbb{R}^2$ is the epigraph of a function $h$ if, for every $x \\in \\mathbb{R}$, the vertical slice $S_x = \\{t \\in \\mathbb{R} \\mid (x, t) \\in S\\}$ is a closed ray of the form $[h(x), \\infty)$.\nA point $(x, t) \\in H$ can be written as the sum of a point $(x_f, t_f) \\in \\operatorname{epi}(f)$ and a point $(x_g, t_g) \\in \\operatorname{epi}(g)$.\nThis means:\n$1.$ $x = x_f + x_g$\n$2.$ $t = t_f + t_g$\n$3.$ $t_f \\geq f(x_f)$\n$4.$ $t_g \\geq g(x_g)$\nFrom conditions $3$ and $4$, we can write $t = t_f + t_g \\geq f(x_f) + g(x_g)$, subject to the constraint $x = x_f + x_g$.\nFor a fixed value of $x$, to be in $H$, a point must have coordinates $(x, t)$ such that there exists a decomposition $x = x_f + x_g$ and $t \\geq f(x_f) + g(x_g)$.\nThe set of all possible values for $t$ for a given $x$ is the union of all intervals $[f(x_f)+g(x_g), \\infty)$ over all possible decompositions $x=x_f+x_g$. This union is itself a ray of the form $[v, \\infty)$, where $v$ is the infimum of all lower bounds.\nWe can therefore define a function $h(x)$ as the infimum of these lower bounds for a fixed $x$:\n$$h(x) = \\inf \\{ f(x_f) + g(x_g) \\mid x_f, x_g \\in \\mathbb{R}, x_f + x_g = x \\}$$\nLetting $x_f = u$, we have $x_g = x - u$. The variable $u$ can range over all of $\\mathbb{R}$. So, the expression for $h(x)$ becomes:\n$$h(x) = \\inf_{u \\in \\mathbb{R}} \\{ f(u) + g(x-u) \\}$$\nThe set of points $(x, t)$ in $H$ is precisely the set of pairs satisfying $t \\geq h(x)$. Therefore, $H$ is the epigraph of the function $h(x)$, i.e., $H = \\operatorname{epi}(h)$.\n\nThird, we derive the explicit closed-form expression for $h(x)$. We substitute the definitions of $f$ and $g$:\n$$h(x) = \\inf_{u \\in \\mathbb{R}} \\left\\{ \\frac{1}{2}u^2 + \\frac{1}{2}((x-u)-1)^2 \\right\\}$$\nLet $L(u, x) = \\frac{1}{2}u^2 + \\frac{1}{2}(x-u-1)^2$. To find the infimum with respect to $u$ for a fixed $x$, we can use calculus. $L(u, x)$ is a quadratic function of $u$ opening upwards, so its minimum can be found by setting its derivative with respect to $u$ to zero.\n$$\\frac{\\partial L}{\\partial u} = \\frac{\\partial}{\\partial u} \\left[ \\frac{1}{2}u^2 + \\frac{1}{2}(x-1-u)^2 \\right]$$\n$$\\frac{\\partial L}{\\partial u} = u + \\frac{1}{2} \\cdot 2(x-1-u) \\cdot (-1) = u - (x-1-u) = u - x + 1 + u = 2u - x + 1$$\nSetting the derivative to zero to find the critical point:\n$$2u - x + 1 = 0 \\implies u = \\frac{x-1}{2}$$\nThe second derivative is $\\frac{\\partial^2 L}{\\partial u^2} = 2 > 0$, confirming that this value of $u$ corresponds to a minimum. The infimum is attained at this point.\nWe substitute this optimal value of $u$ back into the expression for $L(u, x)$ to find $h(x)$:\n$$h(x) = \\frac{1}{2}\\left(\\frac{x-1}{2}\\right)^2 + \\frac{1}{2}\\left(x - \\left(\\frac{x-1}{2}\\right) - 1\\right)^2$$\nLet's simplify the argument of the second term:\n$$x - \\frac{x-1}{2} - 1 = \\frac{2x - (x-1) - 2}{2} = \\frac{2x - x + 1 - 2}{2} = \\frac{x-1}{2}$$\nSo the expression for $h(x)$ simplifies to:\n$$h(x) = \\frac{1}{2}\\left(\\frac{x-1}{2}\\right)^2 + \\frac{1}{2}\\left(\\frac{x-1}{2}\\right)^2 = \\left(\\frac{x-1}{2}\\right)^2$$\n$$h(x) = \\frac{(x-1)^2}{4} = \\frac{1}{4}(x-1)^2$$\n\nFinally, we compare $h(x)$ to the pointwise sum $s(x) = f(x) + g(x)$.\n$$s(x) = \\frac{1}{2}x^2 + \\frac{1}{2}(x-1)^2 = \\frac{1}{2}x^2 + \\frac{1}{2}(x^2 - 2x + 1) = x^2 - x + \\frac{1}{2}$$\nWe have $h(x) = \\frac{1}{4}(x-1)^2$ and $s(x) = x^2 - x + \\frac{1}{2}$.\nClearly, $h(x) \\neq s(x)$. For example, at $x=1$, $h(1) = \\frac{1}{4}(1-1)^2=0$, while $s(1) = 1^2 - 1 + \\frac{1}{2} = \\frac{1}{2}$.\nThis demonstrates that the Minkowski sum of epigraphs corresponds to the operation known as infimal convolution on the functions, which is distinct from the pointwise sum of the functions. Both operations, pointwise addition and infimal convolution, are fundamental operations that preserve convexity. The problem requires the explicit simplified expression for $h(x)$.\n\nThe simplified analytic expression for $h(x)$ is $\\frac{1}{4}(x-1)^2$.", "answer": "$$\\boxed{\\frac{1}{4}(x-1)^{2}}$$", "id": "3160290"}]}