{"hands_on_practices": [{"introduction": "Many powerful optimization algorithms rely on the function being smooth, meaning its gradient does not change too abruptly. However, many practical problems involve non-smooth functions, such as the maximum of a set of linear functions. This exercise [@problem_id:3183315] explores a cornerstone technique: approximating a non-smooth function with a smooth one. You will calculate the Lipschitz constant for the non-smooth `max` function and then derive the gradient Lipschitz constant (or smoothness parameter) for its widely used 'log-sum-exp' approximation, revealing how a smoothing parameter $\\gamma$ controls this crucial property.", "problem": "Let $m=3$ and $d=2$. Consider the convex function $f:\\mathbb{R}^{2}\\to\\mathbb{R}$ defined by\n$$\nf(x)=\\max_{i\\in\\{1,2,3\\}} a_{i}^{\\top}x,\n$$\nwhere $a_{1}=\\begin{pmatrix}1\\\\0\\end{pmatrix}$, $a_{2}=\\begin{pmatrix}0\\\\2\\end{pmatrix}$, and $a_{3}=\\begin{pmatrix}-1\\\\-1\\end{pmatrix}$. Work under the Euclidean norm $\\|\\cdot\\|_{2}$. Define the smooth approximation (log-sum-exp smoothing) $f_{\\gamma}:\\mathbb{R}^{2}\\to\\mathbb{R}$ for any $\\gamma>0$ by\n$$\nf_{\\gamma}(x)=\\gamma\\log\\!\\left(\\sum_{i=1}^{3}\\exp\\!\\left(\\frac{a_{i}^{\\top}x}{\\gamma}\\right)\\right).\n$$\nStarting from the foundational definitions of Lipschitz continuity and gradient Lipschitz continuity, determine:\n1. The smallest Lipschitz constant of $f$ with respect to $\\|\\cdot\\|_{2}$.\n2. The smallest global Lipschitz constant of the gradient $\\nabla f_{\\gamma}(x)$ with respect to $\\|\\cdot\\|_{2}$, expressed as a closed-form analytic function $L(\\gamma)$ of $\\gamma$.\n\nProvide your final answer as a row matrix whose first entry is the Lipschitz constant of $f$ and whose second entry is $L(\\gamma)$. No rounding is required.", "solution": "The user wants me to solve a two-part problem related to a specific convex function $f(x)$ and its smooth approximation $f_\\gamma(x)$.\n\n**Part 1: Lipschitz Constant of $f(x)$**\n\nA function $f:\\mathbb{R}^{d}\\to\\mathbb{R}$ is said to be $L_f$-Lipschitz continuous with respect to a norm $\\|\\cdot\\|$ if there exists a constant $L_f \\ge 0$ such that for all $x, y \\in \\mathbb{R}^{d}$, the inequality holds:\n$$|f(x) - f(y)| \\le L_f \\|x - y\\|$$\nThe smallest such constant $L_f$ is the Lipschitz constant of $f$.\n\nFor a convex function, the Lipschitz constant is given by the largest possible norm of a subgradient of the function. The norm used for the subgradient is the dual norm of the norm used for the domain. Given that we are working with the Euclidean norm $\\|\\cdot\\|_{2}$, which is self-dual, the Lipschitz constant is:\n$$L_f = \\sup_{x \\in \\mathbb{R}^{d}} \\sup_{g \\in \\partial f(x)} \\|g\\|_{2}$$\nwhere $\\partial f(x)$ is the subdifferential of $f$ at $x$.\n\nThe given function is $f(x) = \\max_{i \\in \\{1, 2, 3\\}} a_{i}^{\\top}x$. This is the maximum of a finite number of linear functions, which is a well-known convex function. The subdifferential of $f(x)$ at a point $x$ is the convex hull of the gradients of the active linear functions, i.e., those that achieve the maximum at $x$:\n$$\\partial f(x) = \\text{conv}\\{a_i \\mid a_i^\\top x = f(x)\\}$$\nThe set of all possible subgradients over the entire domain is the convex hull of all the vectors $a_i$:\n$$\\bigcup_{x \\in \\mathbb{R}^{d}} \\partial f(x) = \\text{conv}\\{a_1, a_2, ..., a_m\\}$$\nIn our case, $m=3$, so this set is $\\text{conv}\\{a_1, a_2, a_3\\}$.\n\nThe Lipschitz constant is the supremum of the norm of any element in this set:\n$$L_f = \\sup_{g \\in \\text{conv}\\{a_1, a_2, a_3\\}} \\|g\\|_2$$\nSince the norm $\\|\\cdot\\|_2$ is a convex function, its maximum value over a compact convex set (the convex hull of a finite set of points is compact and convex) is attained at one of the extreme points of the set. The extreme points of $\\text{conv}\\{a_1, a_2, a_3\\}$ are a subset of $\\{a_1, a_2, a_3\\}$. Therefore, the problem simplifies to finding the maximum norm among the given vectors $a_i$:\n$$L_f = \\max_{i \\in \\{1, 2, 3\\}} \\|a_i\\|_2$$\nThe given vectors are $a_1=\\begin{pmatrix}1\\\\0\\end{pmatrix}$, $a_2=\\begin{pmatrix}0\\\\2\\end{pmatrix}$, and $a_3=\\begin{pmatrix}-1\\\\-1\\end{pmatrix}$. We compute their Euclidean norms:\n$$\\|a_1\\|_2 = \\sqrt{1^2 + 0^2} = \\sqrt{1} = 1$$\n$$\\|a_2\\|_2 = \\sqrt{0^2 + 2^2} = \\sqrt{4} = 2$$\n$$\\|a_3\\|_2 = \\sqrt{(-1)^2 + (-1)^2} = \\sqrt{1+1} = \\sqrt{2}$$\nThe maximum of these values is:\n$$L_f = \\max\\{1, 2, \\sqrt{2}\\} = 2$$\n\n**Part 2: Gradient Lipschitz Constant of $f_\\gamma(x)$**\n\nA differentiable function $g:\\mathbb{R}^{d}\\to\\mathbb{R}$ has a Lipschitz continuous gradient with constant $L$ (also known as $L$-smoothness) if for all $x, y \\in \\mathbb{R}^{d}$:\n$$\\|\\nabla g(x) - \\nabla g(y)\\|_2 \\le L \\|x - y\\|_2$$\nFor a twice-differentiable function, the smallest such constant $L$ is the supremum of the spectral norm (the largest eigenvalue for a symmetric positive semidefinite matrix) of its Hessian matrix over the domain:\n$$L = \\sup_{x \\in \\mathbb{R}^{d}} \\|\\nabla^2 g(x)\\|_2 = \\sup_{x \\in \\mathbb{R}^{d}} \\lambda_{\\max}(\\nabla^2 g(x))$$\nThe function is $f_{\\gamma}(x)=\\gamma\\log\\left(\\sum_{i=1}^{3}\\exp\\left(\\frac{a_{i}^{\\top}x}{\\gamma}\\right)\\right)$. Let's compute its gradient and Hessian.\nLet $p_i(x) = \\frac{\\exp(a_i^\\top x / \\gamma)}{\\sum_{k=1}^3 \\exp(a_k^\\top x / \\gamma)}$. Note that $p_i(x) > 0$ and $\\sum_{i=1}^3 p_i(x) = 1$.\nThe gradient of $f_\\gamma(x)$ is:\n$$\\nabla f_\\gamma(x) = \\sum_{i=1}^3 p_i(x) a_i$$\nThe Hessian matrix $\\nabla^2 f_\\gamma(x)$ has entries $(\\nabla^2 f_\\gamma(x))_{jk} = \\frac{\\partial^2 f_\\gamma(x)}{\\partial x_j \\partial x_k}$. A standard calculation yields:\n$$\\nabla^2 f_\\gamma(x) = \\frac{1}{\\gamma} \\left( \\sum_{i=1}^3 p_i(x) a_i a_i^\\top - \\left(\\sum_{i=1}^3 p_i(x) a_i\\right) \\left(\\sum_{i=1}^3 p_i(x) a_i\\right)^\\top \\right)$$\nThis matrix represents the covariance matrix of the vector-valued random variable $A$ that takes value $a_i$ with probability $p_i(x)$, scaled by $\\frac{1}{\\gamma}$. Let $\\bar{a}(x) = \\mathbb{E}_{p(x)}[A] = \\sum_i p_i(x) a_i$. Then $\\nabla^2 f_\\gamma(x) = \\frac{1}{\\gamma} \\text{Cov}_{p(x)}(A) = \\frac{1}{\\gamma}(\\mathbb{E}_{p(x)}[AA^\\top] - \\bar{a}(x)\\bar{a}(x)^\\top)$.\n\nThe smallest global Lipschitz constant of the gradient, $L(\\gamma)$, is:\n$$L(\\gamma) = \\sup_{x \\in \\mathbb{R}^2} \\|\\nabla^2 f_\\gamma(x)\\|_2 = \\frac{1}{\\gamma} \\sup_{x \\in \\mathbb{R}^2} \\lambda_{\\max}(\\text{Cov}_{p(x)}(A))$$\nThe spectral norm is $\\lambda_{\\max}(M) = \\sup_{\\|v\\|_2=1} v^\\top M v$. For the covariance matrix, we have:\n$$v^\\top (\\text{Cov}_{p(x)}(A)) v = \\mathbb{E}_{p(x)}[(v^\\top A)^2] - (\\mathbb{E}_{p(x)}[v^\\top A])^2 = \\text{Var}_{p(x)}(a^\\top v)$$\nThe values $p(x)$ can be made arbitrarily close to any point in the probability simplex $\\Delta_3 = \\{p \\in \\mathbb{R}^3 \\mid p_i \\ge 0, \\sum p_i = 1\\}$. Thus, we need to find the supremum over $p \\in \\Delta_3$:\n$$L(\\gamma) = \\frac{1}{\\gamma} \\sup_{p \\in \\Delta_3} \\sup_{\\|v\\|_2=1} \\text{Var}_p(a^\\top v) = \\frac{1}{\\gamma} \\sup_{\\|v\\|_2=1} \\sup_{p \\in \\Delta_3} \\text{Var}_p(a^\\top v)$$\nFor a fixed vector $v$, let $c_i = a_i^\\top v$. We want to find $\\sup_{p \\in \\Delta_3} \\text{Var}_p(c)$. The variance of a random variable with finite support is bounded by $\\frac{1}{4}(\\max(c) - \\min(c))^2$. This maximum is achieved when probability mass is split as $\\frac{1}{2}$ on the maximum value and $\\frac{1}{2}$ on the minimum value. Thus:\n$$\\sup_{p \\in \\Delta_3} \\text{Var}_p(c) = \\frac{1}{4} \\max_{i,j} (c_i - c_j)^2 = \\frac{1}{4} \\max_{i,j} ((a_i - a_j)^\\top v)^2$$\nSubstituting this back into the expression for $L(\\gamma)$:\n$$L(\\gamma) = \\frac{1}{\\gamma} \\sup_{\\|v\\|_2=1} \\left( \\frac{1}{4} \\max_{i,j} ((a_i - a_j)^\\top v)^2 \\right) = \\frac{1}{4\\gamma} \\max_{i,j} \\left( \\sup_{\\|v\\|_2=1} ((a_i - a_j)^\\top v)^2 \\right)$$\nBy the definition of the dual norm and the Cauchy-Schwarz inequality, $\\sup_{\\|v\\|_2=1} ((a_i - a_j)^\\top v)^2 = \\|a_i - a_j\\|_2^2$. Therefore, the smallest global Lipschitz constant is:\n$$L(\\gamma) = \\frac{1}{4\\gamma} \\max_{i,j \\in \\{1,2,3\\}} \\|a_i - a_j\\|_2^2$$\nWe now compute the squared norms of the differences between the vectors:\n$$a_1 - a_2 = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix} - \\begin{pmatrix}0 \\\\ 2\\end{pmatrix} = \\begin{pmatrix}1 \\\\ -2\\end{pmatrix} \\implies \\|a_1 - a_2\\|_2^2 = 1^2 + (-2)^2 = 5$$\n$$a_1 - a_3 = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix} - \\begin{pmatrix}-1 \\\\ -1\\end{pmatrix} = \\begin{pmatrix}2 \\\\ 1\\end{pmatrix} \\implies \\|a_1 - a_3\\|_2^2 = 2^2 + 1^2 = 5$$\n$$a_2 - a_3 = \\begin{pmatrix}0 \\\\ 2\\end{pmatrix} - \\begin{pmatrix}-1 \\\\ -1\\end{pmatrix} = \\begin{pmatrix}1 \\\\ 3\\end{pmatrix} \\implies \\|a_2 - a_3\\|_2^2 = 1^2 + 3^2 = 10$$\nThe maximum of these values is $10$.\nSubstituting this into the formula for $L(\\gamma)$:\n$$L(\\gamma) = \\frac{10}{4\\gamma} = \\frac{5}{2\\gamma}$$\n\nThe final answer is a row matrix containing the Lipschitz constant of $f$, which is $2$, and the Lipschitz constant of $\\nabla f_\\gamma$, which is $\\frac{5}{2\\gamma}$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n2 & \\frac{5}{2\\gamma}\n\\end{pmatrix}\n}\n$$", "id": "3183315"}, {"introduction": "The speed at which optimization algorithms converge often depends on the conditioning of the problem, captured by the ratio of its smoothness ($L$) to its strong convexity ($\\mu$). This exercise [@problem_id:3183345] delves into a practical and powerful technique for improving a problem's conditioning: adding a quadratic regularizer. You will first determine how this simple addition modifies the $L$ and $\\mu$ parameters and then solve a design problem to find the optimal amount of regularization that minimizes the condition number factor subject to a practical engineering constraint.", "problem": "Let $f:\\mathbb{R}^{n}\\to\\mathbb{R}$ be a differentiable convex function whose gradient is $L$-Lipschitz continuous, meaning $|\\!|\\nabla f(x)-\\nabla f(y)|\\!|\\leq L\\,|\\!|x-y|\\!|$ for all $x,y\\in\\mathbb{R}^{n}$, and suppose $f$ is $\\mu_{0}$-strongly convex, meaning $f(y)\\geq f(x)+\\nabla f(x)^{\\top}(y-x)+\\frac{\\mu_{0}}{2}|\\!|y-x|\\!|^{2}$ for all $x,y\\in\\mathbb{R}^{n}$, with $L>\\mu_{0}\\geq 0$. Consider adding a quadratic regularizer to form\n$$\nF_{\\alpha}(x)\\;=\\;f(x)\\;+\\;\\frac{\\alpha}{2}\\,|\\!|x|\\!|^{2},\n$$\nwhere $\\alpha\\geq 0$ is a tunable parameter. Using only the fundamental definitions of Lipschitz continuity of the gradient and strong convexity, determine the smoothness constant $L_{\\alpha}$ and strong convexity modulus $\\mu_{\\alpha}$ of $F_{\\alpha}$.\n\nIn smooth strongly convex optimization, the iteration complexity of Nesterov’s Accelerated Gradient (NAG) method scales, up to multiplicative constants independent of $\\alpha$, like $\\sqrt{L_{\\alpha}/\\mu_{\\alpha}}$ times a logarithmic factor in the target accuracy. To quantify the trade-off of increasing strong convexity “without changing $L$ too much,” impose the engineering constraint $L_{\\alpha}\\leq (1+\\rho)\\,L$ for a given parameter $0<\\rho<1$. Choose $\\alpha$ to minimize the leading condition-number factor $\\sqrt{L_{\\alpha}/\\mu_{\\alpha}}$ subject to this constraint, and report the minimized factor as a single closed-form expression in terms of $L$, $\\mu_{0}$, and $\\rho$. Do not round your answer.", "solution": "The user wants me to solve a two-part problem in the context of optimization.\n\n**Part 1: Find the smoothness and strong convexity constants of a regularized function.**\n\n1.  **Analyze the function:** The new function is $F_{\\alpha}(x) = f(x) + \\frac{\\alpha}{2} ||x||^2$.\n2.  **Define the properties:**\n    *   **Smoothness (L-smooth):** A function $g$ is $L_g$-smooth if its gradient is $L_g$-Lipschitz continuous, i.e., $||\\nabla g(x) - \\nabla g(y)|| \\leq L_g ||x - y||$.\n    *   **Strong Convexity ($\\mu$-strongly convex):** A differentiable function $g$ is $\\mu_g$-strongly convex if $g(y) \\geq g(x) + \\nabla g(x)^T(y-x) + \\frac{\\mu_g}{2} ||y-x||^2$.\n3.  **Apply these definitions to $F_{\\alpha}(x)$.**\n\n    *   **Smoothness of $F_{\\alpha}$:**\n        *   First, I need the gradient of $F_{\\alpha}(x)$.\n        *   The gradient of $f(x)$ is $\\nabla f(x)$.\n        *   The regularizer is $g(x) = \\frac{\\alpha}{2} ||x||^2 = \\frac{\\alpha}{2} \\sum_{i=1}^n x_i^2$.\n        *   The gradient of the regularizer is $\\nabla g(x) = \\alpha x$.\n        *   So, $\\nabla F_{\\alpha}(x) = \\nabla f(x) + \\alpha x$.\n        *   Now, I need to check the Lipschitz continuity of $\\nabla F_{\\alpha}(x)$.\n        *   $||\\nabla F_{\\alpha}(x) - \\nabla F_{\\alpha}(y)|| = ||(\\nabla f(x) + \\alpha x) - (\\nabla f(y) + \\alpha y)||$.\n        *   $= ||(\\nabla f(x) - \\nabla f(y)) + \\alpha(x - y)||$.\n        *   Using the triangle inequality: $||\\nabla F_{\\alpha}(x) - \\nabla F_{\\alpha}(y)|| \\leq ||\\nabla f(x) - \\nabla f(y)|| + ||\\alpha(x - y)||$.\n        *   $= ||\\nabla f(x) - \\nabla f(y)|| + |\\alpha| ||x - y||$.\n        *   Since $\\alpha \\geq 0$, this is $||\\nabla f(x) - \\nabla f(y)|| + \\alpha ||x - y||$.\n        *   We are given that $\\nabla f$ is $L$-Lipschitz, so $||\\nabla f(x) - \\nabla f(y)|| \\leq L ||x - y||$.\n        *   Substituting this in, we get $||\\nabla F_{\\alpha}(x) - \\nabla F_{\\alpha}(y)|| \\leq L ||x - y|| + \\alpha ||x - y|| = (L + \\alpha) ||x - y||$.\n        *   This means $\\nabla F_{\\alpha}$ is $(L+\\alpha)$-Lipschitz continuous.\n        *   Therefore, the smoothness constant of $F_{\\alpha}$ is $L_{\\alpha} = L + \\alpha$.\n\n    *   **Strong Convexity of $F_{\\alpha}$:**\n        *   I need to use the definition: $F_{\\alpha}(y) \\geq F_{\\alpha}(x) + \\nabla F_{\\alpha}(x)^T(y-x) + \\frac{\\mu_{\\alpha}}{2} ||y-x||^2$.\n        *   Let's expand the terms. $F_{\\alpha}(y) = f(y) + \\frac{\\alpha}{2} ||y||^2$.\n        *   $F_{\\alpha}(x) = f(x) + \\frac{\\alpha}{2} ||x||^2$.\n        *   $\\nabla F_{\\alpha}(x) = \\nabla f(x) + \\alpha x$.\n        *   The inequality becomes:\n            $f(y) + \\frac{\\alpha}{2} ||y||^2 \\geq (f(x) + \\frac{\\alpha}{2} ||x||^2) + (\\nabla f(x) + \\alpha x)^T(y-x) + \\frac{\\mu_{\\alpha}}{2} ||y-x||^2$.\n        *   This looks complicated. Let's try another approach.\n        *   A function $g$ is $\\mu_g$-strongly convex if and only if the function $h(x) = g(x) - \\frac{\\mu_g}{2} ||x||^2$ is convex.\n        *   Let's use the definition directly but be more clever.\n        *   Let $g(x) = \\frac{\\alpha}{2} ||x||^2$. This is a quadratic function.\n        *   We know $f$ is $\\mu_0$-strongly convex:\n            $f(y) \\geq f(x) + \\nabla f(x)^T(y-x) + \\frac{\\mu_0}{2} ||y-x||^2$.\n        *   What about $g(x)$? $\\nabla g(x) = \\alpha x$.\n            $g(y) = \\frac{\\alpha}{2} ||y||^2$.\n            $g(x) + \\nabla g(x)^T(y-x) + \\frac{\\alpha}{2} ||y-x||^2 = \\frac{\\alpha}{2} ||x||^2 + (\\alpha x)^T(y-x) + \\frac{\\alpha}{2} (||y||^2 - 2y^T x + ||x||^2)$.\n            $= \\frac{\\alpha}{2} ||x||^2 + \\alpha y^T x - \\alpha ||x||^2 + \\frac{\\alpha}{2} ||y||^2 - \\alpha y^T x + \\frac{\\alpha}{2} ||x||^2$.\n            $= \\frac{\\alpha}{2} ||y||^2 = g(y)$.\n        *   So, for $g(x) = \\frac{\\alpha}{2} ||x||^2$, we have the equality $g(y) = g(x) + \\nabla g(x)^T(y-x) + \\frac{\\alpha}{2} ||y-x||^2$. This means $g(x)$ is exactly $\\alpha$-strongly convex.\n        *   Now consider $F_{\\alpha}(x) = f(x) + g(x)$.\n        *   The sum of a $\\mu_1$-strongly convex function and a $\\mu_2$-strongly convex function is $(\\mu_1 + \\mu_2)$-strongly convex.\n        *   Let's prove this. Let $h_1$ be $\\mu_1$-SC and $h_2$ be $\\mu_2$-SC.\n            $h_1(y) \\geq h_1(x) + \\nabla h_1(x)^T(y-x) + \\frac{\\mu_1}{2} ||y-x||^2$.\n            $h_2(y) \\geq h_2(x) + \\nabla h_2(x)^T(y-x) + \\frac{\\mu_2}{2} ||y-x||^2$.\n        *   Summing these two inequalities:\n            $(h_1(y) + h_2(y)) \\geq (h_1(x) + h_2(x)) + (\\nabla h_1(x) + \\nabla h_2(x))^T(y-x) + \\frac{(\\mu_1 + \\mu_2)}{2} ||y-x||^2$.\n        *   This is exactly the definition of strong convexity for $h_1+h_2$ with modulus $\\mu_1 + \\mu_2$.\n        *   In our case, $f(x)$ is $\\mu_0$-strongly convex and $g(x) = \\frac{\\alpha}{2} ||x||^2$ is $\\alpha$-strongly convex.\n        *   Therefore, $F_{\\alpha}(x) = f(x) + g(x)$ is $(\\mu_0 + \\alpha)$-strongly convex.\n        *   The strong convexity modulus is $\\mu_{\\alpha} = \\mu_0 + \\alpha$.\n\n    *   **Summary for Part 1:**\n        *   $L_{\\alpha} = L + \\alpha$.\n        *   $\\mu_{\\alpha} = \\mu_0 + \\alpha$.\n        *   These seem correct and were derived using the fundamental definitions as requested.\n\n**Part 2: Optimize the condition number subject to a constraint.**\n\n1.  **State the objective:** Minimize $\\sqrt{\\frac{L_{\\alpha}}{\\mu_{\\alpha}}}$ with respect to $\\alpha \\geq 0$.\n2.  **State the quantities from Part 1:**\n    *   $L_{\\alpha} = L + \\alpha$.\n    *   $\\mu_{\\alpha} = \\mu_0 + \\alpha$.\n3.  **State the constraint:**\n    *   $L_{\\alpha} \\leq (1+\\rho)L$.\n    *   Substituting $L_{\\alpha}$: $L + \\alpha \\leq (1+\\rho)L = L + \\rho L$.\n    *   This simplifies to $\\alpha \\leq \\rho L$.\n    *   We also have the implicit constraint $\\alpha \\geq 0$.\n    *   So, the feasible range for $\\alpha$ is $0 \\leq \\alpha \\leq \\rho L$.\n4.  **Formulate the optimization problem:**\n    *   Minimize $g(\\alpha) = \\sqrt{\\frac{L + \\alpha}{\\mu_0 + \\alpha}}$\n    *   Subject to $0 \\leq \\alpha \\leq \\rho L$.\n5.  **Analyze the objective function $g(\\alpha)$.**\n    *   Minimizing $\\sqrt{x}$ is equivalent to minimizing $x$ for $x \\geq 0$.\n    *   So, let's minimize $h(\\alpha) = \\frac{L + \\alpha}{\\mu_0 + \\alpha}$.\n    *   To find the behavior of $h(\\alpha)$, let's take its derivative with respect to $\\alpha$.\n    *   $h'(\\alpha) = \\frac{d}{d\\alpha} \\left( \\frac{L + \\alpha}{\\mu_0 + \\alpha} \\right)$.\n    *   Using the quotient rule:\n        $h'(\\alpha) = \\frac{(1)(\\mu_0 + \\alpha) - (L + \\alpha)(1)}{(\\mu_0 + \\alpha)^2}$.\n        $h'(\\alpha) = \\frac{\\mu_0 + \\alpha - L - \\alpha}{(\\mu_0 + \\alpha)^2} = \\frac{\\mu_0 - L}{(\\mu_0 + \\alpha)^2}$.\n6.  **Analyze the sign of the derivative.**\n    *   The denominator $(\\mu_0 + \\alpha)^2$ is always positive.\n    *   The sign of $h'(\\alpha)$ is determined by the sign of the numerator, $\\mu_0 - L$.\n    *   The problem statement says $L > \\mu_0 \\ge 0$.\n    *   Therefore, $\\mu_0 - L < 0$.\n    *   So, $h'(\\alpha) < 0$ for all $\\alpha$ in its domain.\n7.  **Interpret the result.**\n    *   Since the derivative of $h(\\alpha)$ is always negative, the function $h(\\alpha)$ is a strictly decreasing function of $\\alpha$.\n    *   The function $g(\\alpha) = \\sqrt{h(\\alpha)}$ is also strictly decreasing.\n    *   To minimize a strictly decreasing function over a closed interval $[a, b]$, we must choose the largest possible value in the interval.\n    *   The interval for $\\alpha$ is $[0, \\rho L]$.\n    *   The largest possible value is $\\alpha = \\rho L$.\n8.  **Calculate the minimized factor.**\n    *   The optimal value of $\\alpha$ is $\\alpha_{opt} = \\rho L$.\n    *   Now, substitute this value back into the expression for the condition number factor.\n    *   Minimized factor = $\\sqrt{\\frac{L + \\alpha_{opt}}{\\mu_0 + \\alpha_{opt}}}$.\n    *   $= \\sqrt{\\frac{L + \\rho L}{\\mu_0 + \\rho L}}$.\n    *   $= \\sqrt{\\frac{L(1 + \\rho)}{\\mu_0 + \\rho L}}$.\n9.  **Final check of the steps and constraints.**\n    *   Problem statement: $f$ is differentiable convex, $\\nabla f$ is $L$-Lipschitz, $f$ is $\\mu_0$-strongly convex.\n    *   $L > \\mu_0 \\geq 0$.\n    *   $F_{\\alpha}(x) = f(x) + \\frac{\\alpha}{2} ||x||^2$, with $\\alpha \\geq 0$.\n    *   Constraint: $L_{\\alpha} \\leq (1+\\rho)L$ for $0 < \\rho < 1$.\n    *   The derivations for $L_{\\alpha} = L+\\alpha$ and $\\mu_{\\alpha} = \\mu_0+\\alpha$ are sound and follow from the definitions.\n    *   The constraint $\\alpha \\leq \\rho L$ is correct.\n    *   The analysis of the objective function $\\frac{L+\\alpha}{\\mu_0+\\alpha}$ is correct. The derivative is indeed $\\frac{\\mu_0 - L}{(\\mu_0+\\alpha)^2}$.\n    *   Since $L > \\mu_0$, the derivative is negative. The function is decreasing.\n    *   The minimum value is achieved at the maximum allowed value of $\\alpha$.\n    *   The maximum allowed value is $\\alpha = \\rho L$.\n    *   The resulting minimal value is $\\sqrt{\\frac{L(1+\\rho)}{\\mu_0 + \\rho L}}$.\n    *   This expression is in terms of $L$, $\\mu_0$, and $\\rho$ as requested. It is a single closed-form expression.\n    *   All steps seem logically sound and mathematically correct.", "answer": "$$\n\\boxed{\\sqrt{\\frac{L(1+\\rho)}{\\mu_{0}+\\rho L}}}\n$$", "id": "3183345"}, {"introduction": "Theoretical analysis provides critical boundaries for algorithm parameters, but seeing these boundaries in action provides a deeper intuition. The smoothness constant $L$ dictates the maximum stable step size for gradient descent, a bound given by $\\alpha \\lt 2/L$. In this hands-on coding exercise [@problem_id:3183363], you will implement a numerical experiment to directly observe the consequences of this theory. By running gradient descent on a simple quadratic problem with step sizes above, below, and at this critical threshold, you will verify the stark transition from stable convergence to catastrophic divergence, bridging the gap between mathematical theory and computational practice.", "problem": "You are to design and implement a numerical experiment that examines the stability of gradient descent for unconstrained quadratic minimization through the lens of smoothness and Lipschitz continuity of the gradient. Work entirely in the setting of a quadratic objective of the form $$f(x) = \\tfrac{1}{2} x^{\\top} A x - b^{\\top} x,$$ where $A$ is a symmetric positive definite (SPD) matrix and $b$ is a vector. The gradient of $f$ is given by $$\\nabla f(x) = A x - b.$$ The gradient map is said to be Lipschitz continuous with constant $L$ if $$\\|\\nabla f(x) - \\nabla f(y)\\| \\le L \\|x-y\\|$$ for all $x,y$ in the domain, where $\\|\\cdot\\|$ denotes the Euclidean norm. In this quadratic setting with SPD $A$, the Lipschitz constant of the gradient equals the spectral norm of $A$, which coincides with the largest eigenvalue of $A$.\n\nStarting from the basic gradient descent iteration $$x_{k+1} = x_k - \\alpha \\nabla f(x_k),$$ where $\\alpha$ is a fixed step size, express the error $e_k = x_k - x^{\\star}$ relative to the minimizer $x^{\\star}$ (which satisfies $A x^{\\star} = b$) and analyze the linear error dynamics. The stability of the iteration can be characterized by the spectral radius of a certain matrix derived from $A$ and $\\alpha$. Based on this characterization, there is a critical step-size boundary associated with the Lipschitz constant of the gradient that separates stable behavior from oscillatory or divergent behavior. Your program must:\n\n- Compute the Lipschitz constant $L$ of the gradient as the largest eigenvalue of $A$.\n- Compute the theoretical stability/divergence classification by checking whether the spectral radius of the linear error propagation operator exceeds $1$.\n- Run gradient descent numerically on the error dynamics for a fixed number of iterations, starting from a specified initial point, and determine whether divergence is observed numerically. For this experiment, define “observed divergence” as occurring if both of the following hold:\n  1. The maximum error norm during the run exceeds the initial error norm by at least a multiplicative factor $r_{\\text{grow}}$.\n  2. The final error norm exceeds the initial error norm by at least a multiplicative factor $r_{\\text{final}}$.\n  \nUse $N$ iterations, with $N$ specified below, and fixed thresholds $r_{\\text{grow}}$ and $r_{\\text{final}}$ specified below. Your output for each test case must be a boolean indicating whether the observed divergence matches the theoretical divergence classification.\n\nFundamental base definitions to be used:\n- Lipschitz continuity of the gradient: $$\\|\\nabla f(x) - \\nabla f(y)\\| \\le L \\|x-y\\|.$$\n- Gradient descent update: $$x_{k+1} = x_k - \\alpha \\nabla f(x_k).$$\n- Error dynamics relative to the minimizer $$x^{\\star}$$ satisfying $$A x^{\\star} = b.$$\n\nImplement the following test suite. Each case specifies $A$, $b$, $x_0$, and $\\alpha$. Treat one-dimensional cases with $A$ as a $1 \\times 1$ matrix and vectors of length $1$. Use $N = 200$ iterations, $r_{\\text{grow}} = 1.5$, and $r_{\\text{final}} = 1.2$ for all cases.\n\n- Case $1$ (one-dimensional, step size below the boundary): $$A = [5],\\quad b = [5],\\quad x_0 = [10],\\quad \\alpha = 0.30.$$\n- Case $2$ (one-dimensional, step size at the boundary): $$A = [5],\\quad b = [5],\\quad x_0 = [10],\\quad \\alpha = 0.40.$$\n- Case $3$ (one-dimensional, step size above the boundary): $$A = [5],\\quad b = [5],\\quad x_0 = [10],\\quad \\alpha = 0.41.$$\n- Case $4$ (two-dimensional, step size below the boundary): $$A = \\begin{bmatrix}3 & 1 \\\\ 1 & 3\\end{bmatrix},\\quad b = \\begin{bmatrix}1 \\\\ -2\\end{bmatrix},\\quad x_0 = \\begin{bmatrix}5 \\\\ 4\\end{bmatrix},\\quad \\alpha = 0.49.$$\n- Case $5$ (two-dimensional, step size at the boundary): $$A = \\begin{bmatrix}3 & 1 \\\\ 1 & 3\\end{bmatrix},\\quad b = \\begin{bmatrix}1 \\\\ -2\\end{bmatrix},\\quad x_0 = \\begin{bmatrix}5 \\\\ 4\\end{bmatrix},\\quad \\alpha = 0.50.$$\n- Case $6$ (two-dimensional, step size above the boundary): $$A = \\begin{bmatrix}3 & 1 \\\\ 1 & 3\\end{bmatrix},\\quad b = \\begin{bmatrix}1 \\\\ -2\\end{bmatrix},\\quad x_0 = \\begin{bmatrix}5 \\\\ 4\\end{bmatrix},\\quad \\alpha = 0.51.$$\n- Case $7$ (three-dimensional, step size below the boundary): $$A = \\operatorname{diag}(1,3,7),\\quad b = \\begin{bmatrix}2 \\\\ -1 \\\\ 5\\end{bmatrix},\\quad x_0 = \\begin{bmatrix}5 \\\\ -3 \\\\ 1\\end{bmatrix},\\quad \\alpha = 0.28.$$\n- Case $8$ (three-dimensional, step size above the boundary): $$A = \\operatorname{diag}(1,3,7),\\quad b = \\begin{bmatrix}2 \\\\ -1 \\\\ 5\\end{bmatrix},\\quad x_0 = \\begin{bmatrix}5 \\\\ -3 \\\\ 1\\end{bmatrix},\\quad \\alpha = 0.29.$$\n\nYour program must:\n- For each case, compute the largest eigenvalue $L$ of $A$.\n- Compute the theoretical divergence classification by evaluating the spectral radius of the linear error-propagation operator and checking whether it exceeds $1$.\n- Run the numerical gradient descent on the error dynamics for $N = 200$ iterations with the specified $\\alpha$, and classify whether divergence is observed using $r_{\\text{grow}} = 1.5$ and $r_{\\text{final}} = 1.2$ as defined above.\n- Produce one boolean per case indicating whether the observed divergence matches the theoretical classification.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $$[result_1,result_2,\\dots,result_8]$$). Each $$result_i$$ must be a boolean.", "solution": "The problem asks for a numerical validation of the stability theory for gradient descent applied to an unconstrained quadratic minimization problem. We are given the objective function $f(x) = \\tfrac{1}{2} x^{\\top} A x - b^{\\top} x$, where $A$ is a symmetric positive definite (SPD) matrix. The gradient descent update rule is $x_{k+1} = x_k - \\alpha \\nabla f(x_k)$, with a fixed step size $\\alpha > 0$.\n\nFirst, we establish the theoretical framework for stability by analyzing the error dynamics. The gradient of the objective function is $\\nabla f(x) = A x - b$. The unique minimizer, denoted by $x^{\\star}$, is the solution to the linear system $\\nabla f(x^{\\star}) = 0$, which gives $A x^{\\star} = b$.\n\nThe error at iteration $k$ is defined as $e_k = x_k - x^{\\star}$. To derive the evolution of the error, we substitute the gradient and the relation $b = A x^{\\star}$ into the gradient descent update rule:\n$$x_{k+1} = x_k - \\alpha (A x_k - b)$$\n$$x_{k+1} = x_k - \\alpha (A x_k - A x^{\\star})$$\nSubtracting $x^{\\star}$ from both sides yields the error at iteration $k+1$:\n$$x_{k+1} - x^{\\star} = (x_k - x^{\\star}) - \\alpha A (x_k - x^{\\star})$$\n$$e_{k+1} = e_k - \\alpha A e_k$$\nThis simplifies to a linear dynamical system for the error vector:\n$$e_{k+1} = (I - \\alpha A) e_k$$\nwhere $I$ is the identity matrix. Let $B = I - \\alpha A$ be the error propagation matrix. The error at iteration $k$ is given by $e_k = B^k e_0$, where $e_0$ is the initial error.\n\nThe stability of the iteration, which corresponds to the convergence of the error $e_k$ to zero as $k \\to \\infty$ for any initial error $e_0$, is determined by the spectral radius of the propagation matrix $B$. The spectral radius, $\\rho(B)$, is the maximum of the absolute values of the eigenvalues of $B$. The iteration is stable and converges if and only if $\\rho(B) < 1$. It diverges if $\\rho(B) > 1$. If $\\rho(B) = 1$, the error does not converge to zero (unless it is already zero), but its norm might not necessarily grow to infinity.\n\nSince $A$ is symmetric, it has real eigenvalues $\\lambda_i$. The eigenvalues of $B = I - \\alpha A$ are $\\mu_i = 1 - \\alpha \\lambda_i$. The spectral radius is therefore:\n$$\\rho(B) = \\max_i |1 - \\alpha \\lambda_i|$$\nSince $A$ is positive definite, all $\\lambda_i > 0$. The condition for convergence, $\\rho(B) < 1$, becomes equivalent to $-1 < 1 - \\alpha \\lambda_i < 1$ for all $i$.\nThe right-hand inequality, $1 - \\alpha \\lambda_i < 1$, simplifies to $-\\alpha \\lambda_i < 0$, which is always true for $\\alpha > 0$ and $\\lambda_i > 0$.\nThe left-hand inequality, $-1 < 1 - \\alpha \\lambda_i$, simplifies to $\\alpha \\lambda_i < 2$, or $\\alpha < 2/\\lambda_i$. This must hold for all eigenvalues $\\lambda_i$. The most restrictive condition is imposed by the largest eigenvalue, $\\lambda_{\\max}(A)$. Thus, the step size must satisfy:\n$$\\alpha < \\frac{2}{\\lambda_{\\max}(A)}$$\nThe problem states that the Lipschitz constant of the gradient, $L$, is equal to $\\lambda_{\\max}(A)$. The stability condition is $\\alpha < 2/L$.\n\nTheoretical divergence is classified as occurring if the stability condition is violated such that $\\rho(I - \\alpha A) > 1$.\n\nThe numerical experiment is designed to verify this theoretical prediction. For each test case, we perform the following steps:\n1.  **Theoretical Classification**: We compute the eigenvalues of the matrix $A$, find the spectral radius $\\rho(I - \\alpha A)$, and determine if it is greater than $1$. This gives the `theoretical_divergence` boolean classification.\n2.  **Numerical Simulation**: We simulate the error dynamics $e_{k+1} = (I - \\alpha A)e_k$ for $N=200$ iterations.\n    - First, the minimizer $x^{\\star}$ is found by solving $A x^{\\star} = b$.\n    - The initial error $e_0 = x_0 - x^{\\star}$ and its Euclidean norm $\\|e_0\\|$ are computed.\n    - We iterate to find $e_1, e_2, \\dots, e_N$ and their norms.\n    - **Observed Divergence**: We check if the simulation meets the specified criteria for divergence:\n      - (1) The maximum error norm over the run, $\\max_{0 \\le k \\le N} \\|e_k\\|$, is at least $r_{\\text{grow}} = 1.5$ times the initial error norm $\\|e_0\\|$.\n      - (2) The final error norm, $\\|e_N\\|$, is at least $r_{\\text{final}} = 1.2$ times the initial error norm $\\|e_0\\|$.\n    - If both conditions are met, the behavior is classified as `observed_divergence`.\n3.  **Comparison**: For each case, we compare the theoretical and observed classifications. The expected result is that they match, yielding a boolean `True`. When $\\rho(B) > 1$, the error component corresponding to the dominant eigenvalue of $B$ grows exponentially as $(\\rho(B))^k$. For a sufficiently large number of iterations $N$, this growth will inevitably surpass the fixed thresholds $r_{\\text{grow}}$ and $r_{\\text{final}}$. Conversely, when $\\rho(B) \\le 1$, the error norm will not grow, so the divergence criteria will not be met.\n\nThe implementation will process each of the eight test cases according to this logic.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Analyzes the stability of gradient descent for quadratic minimization problems,\n    comparing theoretical predictions with numerical simulations.\n    \"\"\"\n    # Use float for all numerical values to ensure consistency.\n    test_cases = [\n        # Case 1 (1D, stable)\n        {'A': np.array([[5.0]]), 'b': np.array([5.0]), 'x0': np.array([10.0]), 'alpha': 0.30},\n        # Case 2 (1D, boundary)\n        {'A': np.array([[5.0]]), 'b': np.array([5.0]), 'x0': np.array([10.0]), 'alpha': 0.40},\n        # Case 3 (1D, divergent)\n        {'A': np.array([[5.0]]), 'b': np.array([5.0]), 'x0': np.array([10.0]), 'alpha': 0.41},\n        # Case 4 (2D, stable)\n        {'A': np.array([[3.0, 1.0], [1.0, 3.0]]), 'b': np.array([1.0, -2.0]), 'x0': np.array([5.0, 4.0]), 'alpha': 0.49},\n        # Case 5 (2D, boundary)\n        {'A': np.array([[3.0, 1.0], [1.0, 3.0]]), 'b': np.array([1.0, -2.0]), 'x0': np.array([5.0, 4.0]), 'alpha': 0.50},\n        # Case 6 (2D, divergent)\n        {'A': np.array([[3.0, 1.0], [1.0, 3.0]]), 'b': np.array([1.0, -2.0]), 'x0': np.array([5.0, 4.0]), 'alpha': 0.51},\n        # Case 7 (3D, stable)\n        {'A': np.diag([1.0, 3.0, 7.0]), 'b': np.array([2.0, -1.0, 5.0]), 'x0': np.array([5.0, -3.0, 1.0]), 'alpha': 0.28},\n        # Case 8 (3D, divergent)\n        {'A': np.diag([1.0, 3.0, 7.0]), 'b': np.array([2.0, -1.0, 5.0]), 'x0': np.array([5.0, -3.0, 1.0]), 'alpha': 0.29},\n    ]\n\n    results = []\n    \n    # Simulation parameters\n    N = 200\n    r_grow = 1.5\n    r_final = 1.2\n    \n    for case in test_cases:\n        A, b, x0, alpha = case['A'], case['b'], case['x0'], case['alpha']\n        \n        # 1. Theoretical Analysis\n        # Since A is symmetric, eigvalsh is appropriate and returns real eigenvalues.\n        eigenvalues_A = np.linalg.eigvalsh(A)\n        # The eigenvalues of the error propagation matrix B = I - alpha * A\n        eigenvalues_B = 1.0 - alpha * eigenvalues_A\n        # The spectral radius of B\n        rho_B = np.max(np.abs(eigenvalues_B))\n        # Theoretical divergence occurs if the spectral radius is > 1\n        theoretical_divergence = rho_B > 1.0\n        \n        # 2. Numerical Simulation\n        # Solve for the minimizer x_star from A * x_star = b\n        x_star = np.linalg.solve(A, b)\n        \n        # Initial error vector and its norm\n        e_k = x0 - x_star\n        initial_error_norm = np.linalg.norm(e_k)\n        \n        # Handle the edge case where the initial guess is the solution\n        if initial_error_norm < 1e-12:\n            observed_divergence = False\n        else:\n            max_error_norm = initial_error_norm\n            \n            # Error propagation matrix B = I - alpha * A\n            B = np.identity(A.shape[0]) - alpha * A\n            \n            # Run the simulation on the error dynamics\n            for _ in range(N):\n                e_k = B @ e_k\n                current_norm = np.linalg.norm(e_k)\n                if current_norm > max_error_norm:\n                    max_error_norm = current_norm\n            \n            final_error_norm = np.linalg.norm(e_k)\n            \n            # Check the conditions for observed divergence\n            cond1 = (max_error_norm >= r_grow * initial_error_norm)\n            cond2 = (final_error_norm >= r_final * initial_error_norm)\n            observed_divergence = cond1 and cond2\n\n        # 3. Compare theoretical prediction with numerical observation\n        # The result is True if they match, False otherwise.\n        match = (observed_divergence == theoretical_divergence)\n        results.append(match)\n\n    # Format the final output as a string representing a list of booleans\n    # The default string representation of Python booleans is 'True' or 'False'.\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\nsolve()\n```", "id": "3183363"}]}