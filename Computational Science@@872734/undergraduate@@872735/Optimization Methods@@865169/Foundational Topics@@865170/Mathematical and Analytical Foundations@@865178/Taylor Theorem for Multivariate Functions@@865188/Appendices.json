{"hands_on_practices": [{"introduction": "Before we can use Taylor's theorem to design sophisticated optimization algorithms, we must master the fundamental skill of constructing the approximation itself. This first exercise focuses on the mechanics of building a second-order Taylor polynomial for a multivariate function. By working through the partial derivatives and assembling the terms, you will solidify your understanding of how a complex function can be locally approximated by a simpler quadratic surface [@problem_id:24079].", "problem": "The second-order Taylor polynomial of a twice-differentiable function $f(x, y)$ centered at the point $(a, b)$ provides a quadratic approximation of the function near that point. It is given by the formula:\n$$\nT_2(x, y) = f(a, b) + \\frac{\\partial f}{\\partial x}(a, b)(x - a) + \\frac{\\partial f}{\\partial y}(a, b)(y - b) + \\frac{1}{2!}\\left(\\frac{\\partial^2 f}{\\partial x^2}(a, b)(x - a)^2 + 2\\frac{\\partial^2 f}{\\partial x \\partial y}(a, b)(x - a)(y - b) + \\frac{\\partial^2 f}{\\partial y^2}(a, b)(y - b)^2\\right)\n$$\nConsider the function $f(x, y) = \\frac{x}{y+1}$. Derive the second-order Taylor polynomial for this function centered at the origin, $(a, b) = (0, 0)$.", "solution": "The problem asks for the second-order Taylor polynomial of the function $f(x, y) = \\frac{x}{y+1}$ centered at the point $(a, b) = (0, 0)$.\n\nThe general formula for the second-order Taylor polynomial at $(a, b)$ is:\n$$\nT_2(x, y) = f(a, b) + f_x(a, b)(x-a) + f_y(a, b)(y-b) + \\frac{1}{2} \\left[ f_{xx}(a, b)(x-a)^2 + 2f_{xy}(a, b)(x-a)(y-b) + f_{yy}(a, b)(y-b)^2 \\right]\n$$\nwhere the subscripts denote partial differentiation (e.g., $f_x = \\frac{\\partial f}{\\partial x}$ and $f_{xy} = \\frac{\\partial^2 f}{\\partial y \\partial x}$).\n\nSince we are centering the polynomial at the origin, $(a, b) = (0, 0)$, the formula simplifies to:\n$$\nT_2(x, y) = f(0, 0) + f_x(0, 0)x + f_y(0, 0)y + \\frac{1}{2} \\left[ f_{xx}(0, 0)x^2 + 2f_{xy}(0, 0)xy + f_{yy}(0, 0)y^2 \\right]\n$$\n\nWe must compute the function's value and its partial derivatives up to the second order, and then evaluate them at the point $(0, 0)$.\n\n**Step 1: Evaluate the function at (0, 0).**\n$$\nf(x, y) = \\frac{x}{y+1}\n$$\n$$\nf(0, 0) = \\frac{0}{0+1} = 0\n$$\n\n**Step 2: Compute the first-order partial derivatives and evaluate at (0, 0).**\n$$\nf_x(x, y) = \\frac{\\partial}{\\partial x}\\left(\\frac{x}{y+1}\\right) = \\frac{1}{y+1}\n$$\n$$\nf_x(0, 0) = \\frac{1}{0+1} = 1\n$$\n$$\nf_y(x, y) = \\frac{\\partial}{\\partial y}\\left(x(y+1)^{-1}\\right) = x(-1)(y+1)^{-2} = -\\frac{x}{(y+1)^2}\n$$\n$$\nf_y(0, 0) = -\\frac{0}{(0+1)^2} = 0\n$$\n\n**Step 3: Compute the second-order partial derivatives and evaluate at (0, 0).**\n$$\nf_{xx}(x, y) = \\frac{\\partial}{\\partial x}(f_x) = \\frac{\\partial}{\\partial x}\\left(\\frac{1}{y+1}\\right) = 0\n$$\n$$\nf_{xx}(0, 0) = 0\n$$\n$$\nf_{xy}(x, y) = \\frac{\\partial}{\\partial y}(f_x) = \\frac{\\partial}{\\partial y}\\left((y+1)^{-1}\\right) = -1(y+1)^{-2} = -\\frac{1}{(y+1)^2}\n$$\n$$\nf_{xy}(0, 0) = -\\frac{1}{(0+1)^2} = -1\n$$\n$$\nf_{yy}(x, y) = \\frac{\\partial}{\\partial y}(f_y) = \\frac{\\partial}{\\partial y}\\left(-\\frac{x}{(y+1)^2}\\right) = -x \\frac{\\partial}{\\partial y}\\left((y+1)^{-2}\\right) = -x(-2)(y+1)^{-3} = \\frac{2x}{(y+1)^3}\n$$\n$$\nf_{yy}(0, 0) = \\frac{2(0)}{(0+1)^3} = 0\n$$\n\n**Step 4: Substitute these values into the Taylor polynomial formula.**\n$$\nT_2(x, y) = f(0, 0) + f_x(0, 0)x + f_y(0, 0)y + \\frac{1}{2} \\left[ f_{xx}(0, 0)x^2 + 2f_{xy}(0, 0)xy + f_{yy}(0, 0)y^2 \\right]\n$$\n$$\nT_2(x, y) = 0 + (1)x + (0)y + \\frac{1}{2} \\left[ (0)x^2 + 2(-1)xy + (0)y^2 \\right]\n$$\n$$\nT_2(x, y) = x + \\frac{1}{2} (-2xy)\n$$\n$$\nT_2(x, y) = x - xy\n$$\nThe second-order Taylor polynomial for $f(x, y) = \\frac{x}{y+1}$ centered at the origin is $x - xy$.", "answer": "$$\\boxed{x - xy}$$", "id": "24079"}, {"introduction": "A quadratic model provides a powerful lens, but what happens when it is not informative? This practice explores a scenario where the second-derivative test, which relies on the Hessian matrix, is inconclusive. By analyzing a function whose second-order Taylor approximation at a stationary point is zero, you will see the necessity of examining higher-order terms to correctly classify the point and understand the local geometry [@problem_id:3191338]. This exercise highlights the limitations of purely quadratic models and motivates the development of more robust optimization methods.", "problem": "Consider the function $f:\\mathbb{R}^{2}\\to\\mathbb{R}$ defined by $f(x,y)=x^{3}+3\\,x\\,y^{2}$. Let $x^{\\star}=(0,0)$.\n\nTasks:\n- Verify that $x^{\\star}$ is a stationary point and that the Hessian of $f$ at $x^{\\star}$ is the zero matrix, while some third-order partial derivatives at $x^{\\star}$ are nonzero.\n- Using Taylor’s theorem for multivariate functions centered at $x^{\\star}$, classify the stationary point $x^{\\star}$.\n- In the spirit of trust-region methods, design a “cubic-aware” optimization step by minimizing the third-order Taylor model of $f$ centered at $x^{\\star}$ over the sphere $\\{s\\in\\mathbb{R}^{2}:\\|s\\|_{2}=r\\}$, where $r>0$ is given. Among all minimizers, choose the one with $y>0$. Provide the explicit optimizer step $s^{\\star}(r)$ as a single row vector.\n\nYour final answer must be the explicit expression of $s^{\\star}(r)$ in terms of $r$ only, written as a row matrix. No rounding is required.", "solution": "The problem asks us to analyze the function $f(x,y)=x^{3}+3\\,x\\,y^{2}$ at the point $x^{\\star}=(0,0)$, classify this stationary point, and then find an optimal step $s^{\\star}(r)$ by minimizing the third-order Taylor model of $f$ on a sphere of radius $r$.\n\nFirst, we validate that $x^{\\star}=(0,0)$ is a stationary point and that its Hessian is the zero matrix. We compute the gradient of $f$, denoted by $\\nabla f$. The partial derivatives are:\n$$\n\\frac{\\partial f}{\\partial x} = 3x^{2}+3y^{2}\n$$\n$$\n\\frac{\\partial f}{\\partial y} = 6xy\n$$\nThe gradient is $\\nabla f(x,y) = \\begin{pmatrix} 3x^{2}+3y^{2} \\\\ 6xy \\end{pmatrix}$.\nEvaluating at $x^{\\star}=(0,0)$, we get:\n$$\n\\nabla f(0,0) = \\begin{pmatrix} 3(0)^{2}+3(0)^{2} \\\\ 6(0)(0) \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nSince the gradient at $x^{\\star}=(0,0)$ is the zero vector, $x^{\\star}$ is a stationary point of $f$.\n\nNext, we compute the Hessian matrix of $f$, denoted by $\\nabla^{2}f$. The second-order partial derivatives are:\n$$\n\\frac{\\partial^{2} f}{\\partial x^{2}} = \\frac{\\partial}{\\partial x}(3x^{2}+3y^{2}) = 6x\n$$\n$$\n\\frac{\\partial^{2} f}{\\partial y^{2}} = \\frac{\\partial}{\\partial y}(6xy) = 6x\n$$\n$$\n\\frac{\\partial^{2} f}{\\partial x \\partial y} = \\frac{\\partial}{\\partial y}(3x^{2}+3y^{2}) = 6y\n$$\nThe Hessian matrix is $\\nabla^{2}f(x,y) = \\begin{pmatrix} 6x & 6y \\\\ 6y & 6x \\end{pmatrix}$.\nEvaluating at $x^{\\star}=(0,0)$:\n$$\n\\nabla^{2}f(0,0) = \\begin{pmatrix} 6(0) & 6(0) \\\\ 6(0) & 6(0) \\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix}\n$$\nThis confirms that the Hessian of $f$ at $x^{\\star}$ is the zero matrix. The second derivative test is inconclusive for classifying the stationary point.\n\nWe now compute the third-order partial derivatives at $x^{\\star}=(0,0)$:\n$$\n\\frac{\\partial^{3} f}{\\partial x^{3}} = 6 \\implies \\frac{\\partial^{3} f}{\\partial x^{3}}(0,0) = 6\n$$\n$$\n\\frac{\\partial^{3} f}{\\partial y^{3}} = 0 \\implies \\frac{\\partial^{3} f}{\\partial y^{3}}(0,0) = 0\n$$\n$$\n\\frac{\\partial^{3} f}{\\partial x^{2} \\partial y} = 0 \\implies \\frac{\\partial^{3} f}{\\partial x^{2} \\partial y}(0,0) = 0\n$$\n$$\n\\frac{\\partial^{3} f}{\\partial x \\partial y^{2}} = 6 \\implies \\frac{\\partial^{3} f}{\\partial x \\partial y^{2}}(0,0) = 6\n$$\nWe see that some third-order partial derivatives at $x^{\\star}$ are indeed nonzero.\n\nTo classify the stationary point, we use the Taylor expansion of $f$ around $x^{\\star}=(0,0)$. For a vector $s=(s_x, s_y)$, the expansion is:\n$f(s_x, s_y) = f(0,0) + \\nabla f(0,0)^{T}s + \\frac{1}{2}s^{T}\\nabla^{2}f(0,0)s + \\dots$\nSince $f(0,0)=0$, $\\nabla f(0,0)=0$, and $\\nabla^{2}f(0,0)=0$, the lowest-order non-vanishing term is the third-order term. The function $f(x,y)=x^3+3xy^2$ is a homogeneous polynomial of degree $3$. Its Taylor expansion about the origin is the function itself.\nThus, near the origin, the behavior of $f$ is given by $f(x,y) = x^3 + 3xy^2 = x(x^2 + 3y^2)$.\nIn any arbitrarily small neighborhood of $(0,0)$:\n- If we choose a point with $x > 0$, then $x^2+3y^2 > 0$ (unless $(x,y)=(0,0)$), so $f(x,y) > 0$.\n- If we choose a point with $x < 0$, then $f(x,y) < 0$.\nSince $f(0,0)=0$ and the function takes both positive and negative values in any neighborhood of $(0,0)$, the point $x^{\\star}=(0,0)$ is a saddle point (specifically, a degenerate or higher-order saddle point).\n\nFinally, we design the \"cubic-aware\" optimization step by minimizing the third-order Taylor model of $f$ centered at $x^{\\star}$ over the sphere $\\{s\\in\\mathbb{R}^{2}:\\|s\\|_{2}=r\\}$. Let $s = (s_x, s_y)$. The third-order Taylor model $m(s)$ is:\n$$\nm(s) = f(0,0) + \\nabla f(0,0)^{T}s + \\frac{1}{2}s^{T}\\nabla^{2}f(0,0)s + \\frac{1}{3!}\\sum_{i,j,k=1}^{2} \\frac{\\partial^3 f(0,0)}{\\partial x_i \\partial x_j \\partial x_k}s_i s_j s_k\n$$\nAs previously established, the first three terms are zero, and the third-order expansion matches the function itself. So we want to minimize $m(s_x, s_y) = s_x^3 + 3s_x s_y^2$ subject to the constraint $s_x^2 + s_y^2 = r^2$, where $r>0$.\n\nWe can parameterize the constraint using polar coordinates: $s_x = r\\cos\\theta$ and $s_y = r\\sin\\theta$ for $\\theta \\in [0, 2\\pi)$. Substituting this into the objective function gives a function of $\\theta$:\n$$\nh(\\theta) = (r\\cos\\theta)^{3} + 3(r\\cos\\theta)(r\\sin\\theta)^{2}\n$$\n$$\nh(\\theta) = r^{3}\\cos^{3}\\theta + 3r^{3}\\cos\\theta\\sin^{2}\\theta = r^{3}\\cos\\theta(\\cos^{2}\\theta + 3\\sin^{2}\\theta)\n$$\nUsing the identity $\\cos^{2}\\theta = 1 - \\sin^{2}\\theta$:\n$$\nh(\\theta) = r^{3}\\cos\\theta(1 - \\sin^{2}\\theta + 3\\sin^{2}\\theta) = r^{3}\\cos\\theta(1 + 2\\sin^{2}\\theta)\n$$\nTo find the minimum, we compute the derivative with respect to $\\theta$ and set it to zero:\n$$\n\\frac{dh}{d\\theta} = r^{3}[(-\\sin\\theta)(1+2\\sin^{2}\\theta) + (\\cos\\theta)(4\\sin\\theta\\cos\\theta)]\n$$\n$$\n\\frac{dh}{d\\theta} = r^{3}[-\\sin\\theta - 2\\sin^{3}\\theta + 4\\sin\\theta\\cos^{2}\\theta] = r^{3}\\sin\\theta[-1 - 2\\sin^{2}\\theta + 4\\cos^{2}\\theta]\n$$\nSetting $\\frac{dh}{d\\theta}=0$ (and since $r>0$), we have two cases:\nCase 1: $\\sin\\theta = 0$. This implies $\\theta=0$ or $\\theta=\\pi$.\n- If $\\theta=0$, $h(0)=r^{3}\\cos(0)(1+0) = r^{3}$.\n- If $\\theta=\\pi$, $h(\\pi)=r^{3}\\cos(\\pi)(1+0) = -r^{3}$.\n\nCase 2: $-1 - 2\\sin^{2}\\theta + 4\\cos^{2}\\theta = 0$. Using $\\cos^{2}\\theta = 1-\\sin^{2}\\theta$:\n$$\n-1 - 2\\sin^{2}\\theta + 4(1-\\sin^{2}\\theta) = 0 \\implies 3 - 6\\sin^{2}\\theta = 0 \\implies \\sin^{2}\\theta = \\frac{1}{2}\n$$\nThis also implies $\\cos^{2}\\theta = 1 - \\frac{1}{2} = \\frac{1}{2}$. The value of the objective function is $h(\\theta) = r^{3}\\cos\\theta(1 + 2(\\frac{1}{2})) = 2r^{3}\\cos\\theta$.\n- If $\\cos\\theta = \\sqrt{\\frac{1}{2}} = \\frac{1}{\\sqrt{2}}$, then $h(\\theta) = 2r^{3}(\\frac{1}{\\sqrt{2}}) = \\sqrt{2}r^{3}$.\n- If $\\cos\\theta = -\\sqrt{\\frac{1}{2}} = -\\frac{1}{\\sqrt{2}}$, then $h(\\theta) = 2r^{3}(-\\frac{1}{\\sqrt{2}}) = -\\sqrt{2}r^{3}$.\n\nComparing all values $\\{r^3, -r^3, \\sqrt{2}r^3, -\\sqrt{2}r^3\\}$, the minimum value is $-\\sqrt{2}r^{3}$. This occurs when $\\cos\\theta = -1/\\sqrt{2}$ and $\\sin^2\\theta = 1/2$. The possible angles are $\\theta = \\frac{3\\pi}{4}$ and $\\theta = \\frac{5\\pi}{4}$.\nThe corresponding minimizers $s=(s_x,s_y)$ are:\n1. For $\\theta = \\frac{3\\pi}{4}$: $s_x = r\\cos(\\frac{3\\pi}{4}) = -\\frac{r}{\\sqrt{2}}$, $s_y = r\\sin(\\frac{3\\pi}{4}) = \\frac{r}{\\sqrt{2}}$. The minimizer is $(-\\frac{r}{\\sqrt{2}}, \\frac{r}{\\sqrt{2}})$.\n2. For $\\theta = \\frac{5\\pi}{4}$: $s_x = r\\cos(\\frac{5\\pi}{4}) = -\\frac{r}{\\sqrt{2}}$, $s_y = r\\sin(\\frac{5\\pi}{4}) = -\\frac{r}{\\sqrt{2}}$. The minimizer is $(-\\frac{r}{\\sqrt{2}}, -\\frac{r}{\\sqrt{2}})$.\n\nThe problem asks to choose the minimizer with $y > 0$, which corresponds to $s_y > 0$. This is the first case.\nThus, the explicit optimizer step is $s^{\\star}(r) = (-\\frac{r}{\\sqrt{2}}, \\frac{r}{\\sqrt{2}})$. Rationalizing the denominator gives $s^{\\star}(r) = (-\\frac{\\sqrt{2}}{2}r, \\frac{\\sqrt{2}}{2}r)$.\nWe express this as a row matrix.\n$$\ns^{\\star}(r) = \\begin{pmatrix} -\\frac{\\sqrt{2}}{2}r & \\frac{\\sqrt{2}}{2}r \\end{pmatrix}\n$$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n-\\frac{\\sqrt{2}}{2}r & \\frac{\\sqrt{2}}{2}r\n\\end{pmatrix}\n}\n$$", "id": "3191338"}, {"introduction": "Having seen how to construct a Taylor model and where it can fail, we now turn to a core task in modern optimization: controlling the model's error. This advanced practice challenges you to use Taylor's theorem not just for approximation, but for rigorous analysis. You will derive a bound on the remainder term, which represents the error of our quadratic model, and use this bound to determine a safe step size for an algorithm [@problem_id:3191337]. This exercise directly connects the theory of Taylor remainders to the practical design of trust-region methods, demonstrating how we ensure our algorithms are both aggressive and reliable.", "problem": "Consider the function $f:\\mathbb{R}^{n}\\to\\mathbb{R}$ defined by $f(x)=\\|x\\|^{2}+\\alpha \\sin\\!\\big(\\beta\\,u^{\\top}x\\big)$, where $\\alpha>0$ and $\\beta>0$ are fixed scalars and $u\\in\\mathbb{R}^{n}$ is a fixed unit vector satisfying $\\|u\\|=1$. Let $m_{2}(s)$ denote the second-order Taylor model of $f$ around the center $x=0$, applied to a step $s\\in\\mathbb{R}^{n}$, which is the quadratic model used in trust-region (TR) methods in optimization. Using Taylor’s theorem for multivariate functions and only fundamental definitions and well-tested facts, derive a bound on the remainder $R_{2}(s)$ that holds uniformly for all steps $s$ in the ball $\\{s\\in\\mathbb{R}^{n}:\\|s\\|\\leq \\Delta\\}$, by computing a Lipschitz constant for the Hessian of $f$ on that ball. Then, for a prescribed tolerance $\\tau>0$ measuring the maximum acceptable magnitude of the Taylor remainder, determine the largest trust-region radius $\\Delta=\\Delta(\\alpha,\\beta,\\tau)$ such that $|R_{2}(s)|\\leq\\tau$ for all steps $s$ with $\\|s\\|\\leq \\Delta$. Express your final answer as a single closed-form analytic expression in terms of $\\alpha$, $\\beta$, and $\\tau$. No numerical approximation or rounding is required.", "solution": "The problem requires us to find the largest trust-region radius $\\Delta$ for a function $f:\\mathbb{R}^{n}\\to\\mathbb{R}$, defined as $f(x)=\\|x\\|^{2}+\\alpha \\sin(\\beta u^{\\top}x)$, where $\\alpha>0$, $\\beta>0$, and $u\\in\\mathbb{R}^{n}$ is a unit vector with $\\|u\\|=1$. The condition is that the second-order Taylor remainder, $R_{2}(s)$, around the center $x=0$, must satisfy $|R_{2}(s)| \\leq \\tau$ for a given tolerance $\\tau>0$, for all steps $s$ within the trusty region, i.e., $\\|s\\|\\leq \\Delta$.\n\nAccording to Taylor's theorem for a function with a continuous third derivative, the expansion of $f$ around a point $x_c$ is given by:\n$$f(x_c+s) = f(x_c) + \\nabla f(x_c)^{\\top}s + \\frac{1}{2}s^{\\top}\\nabla^{2}f(x_c)s + R_{2}(s)$$\nwhere $s$ is the step vector, $\\nabla f$ is the gradient, and $\\nabla^{2}f$ is the Hessian matrix. The term $m_2(s) = f(x_c) + \\nabla f(x_c)^{\\top}s + \\frac{1}{2}s^{\\top}\\nabla^{2}f(x_c)s$ is the second-order Taylor model of $f$ around $x_c$. The remainder term $R_{2}(s)$ is of order $O(\\|s\\|^{3})$.\n\nA standard result from optimization theory provides a bound on this remainder. If the Hessian $\\nabla^2 f(x)$ is Lipschitz continuous with constant $L_H$ in a convex set containing $x_c$ and $x_c+s$, i.e., $\\|\\nabla^{2}f(y) - \\nabla^{2}f(z)\\| \\leq L_H\\|y-z\\|$ for all $y,z$ in the set, then the remainder is bounded by:\n$$|R_{2}(s)| = |f(x_c+s) - m_2(s)| \\leq \\frac{L_H}{6}\\|s\\|^{3}$$\nThe Lipschitz constant $L_H$ can be taken as the supremum of the norm of the third-order derivative tensor, $L_H = \\sup_x \\|\\nabla^3 f(x)\\|$.\n\nOur expansion is centered at $x_c=0$. First, we compute the necessary derivatives of $f(x) = x^{\\top}x + \\alpha \\sin(\\beta u^{\\top}x)$.\n\nThe gradient of $f(x)$ is:\n$$\\nabla f(x) = \\frac{\\partial}{\\partial x} \\left(x^{\\top}x + \\alpha \\sin(\\beta u^{\\top}x)\\right) = 2x + \\alpha \\cos(\\beta u^{\\top}x) (\\beta u) = 2x + \\alpha\\beta\\cos(\\beta u^{\\top}x)u$$\n\nThe Hessian matrix of $f(x)$ is the matrix of second partial derivatives:\n$$\\nabla^{2}f(x) = \\frac{\\partial}{\\partial x^{\\top}} \\left(2x + \\alpha\\beta\\cos(\\beta u^{\\top}x)u\\right) = 2I + \\alpha\\beta(-\\sin(\\beta u^{\\top}x) (\\beta u^{\\top}))u = 2I - \\alpha\\beta^{2}\\sin(\\beta u^{\\top}x)uu^{\\top}$$\nwhere $I$ is the $n\\times n$ identity matrix and $uu^{\\top}$ is an $n\\times n$ matrix.\n\nTo find the Lipschitz constant for the Hessian, we compute the third-order derivative of $f(x)$. This is a third-order tensor whose components are $\\frac{\\partial^3 f(x)}{\\partial x_i \\partial x_j \\partial x_k}$. Differentiating the Hessian:\n$$\\frac{\\partial}{\\partial x_k}\\left(\\nabla^{2}f(x)\\right)_{ij} = \\frac{\\partial}{\\partial x_k}\\left(2\\delta_{ij} - \\alpha\\beta^{2}\\sin(\\beta u^{\\top}x)u_{i}u_{j}\\right) = -\\alpha\\beta^{2}\\cos(\\beta u^{\\top}x)(\\beta u_{k})u_{i}u_{j} = -\\alpha\\beta^{3}\\cos(\\beta u^{\\top}x)u_{i}u_{j}u_{k}$$\nThe action of the third derivative tensor on three vectors $v_1, v_2, v_3 \\in \\mathbb{R}^n$ is:\n$$D^{3}f(x)[v_1,v_2,v_3] = \\sum_{i,j,k=1}^{n} \\frac{\\partial^3 f(x)}{\\partial x_i \\partial x_j \\partial x_k} (v_1)_i (v_2)_j (v_3)_k = -\\alpha\\beta^{3}\\cos(\\beta u^{\\top}x)\\sum_{i,j,k=1}^{n} u_{i}u_{j}u_{k}(v_1)_i (v_2)_j (v_3)_k$$\nThis simplifies to:\n$$D^{3}f(x)[v_1,v_2,v_3] = -\\alpha\\beta^{3}\\cos(\\beta u^{\\top}x)(u^{\\top}v_1)(u^{\\top}v_2)(u^{\\top}v_3)$$\n\nThe norm of this tensor at a point $x$ is defined as $\\|\\nabla^3 f(x)\\| = \\sup_{\\|v_1\\|=\\|v_2\\|=\\|v_3\\|=1} |D^{3}f(x)[v_1,v_2,v_3]|$.\n$$\\|\\nabla^3 f(x)\\| = \\sup_{\\|v_1\\|=\\|v_2\\|=\\|v_3\\|=1} |-\\alpha\\beta^{3}\\cos(\\beta u^{\\top}x)(u^{\\top}v_1)(u^{\\top}v_2)(u^{\\top}v_3)|$$\nSince $\\alpha>0$ and $\\beta>0$:\n$$\\|\\nabla^3 f(x)\\| = \\alpha\\beta^{3}|\\cos(\\beta u^{\\top}x)| \\sup_{\\|v_1\\|=\\|v_2\\|=\\|v_3\\|=1} |(u^{\\top}v_1)(u^{\\top}v_2)(u^{\\top}v_3)|$$\nBy the Cauchy-Schwarz inequality, $|u^{\\top}v| \\leq \\|u\\|\\|v\\|$. Since $\\|u\\|=1$ and we take $\\|v\\|=1$, we have $|u^{\\top}v| \\leq 1$. The supremum is achieved when $v_1, v_2, v_3$ are all chosen to be $u$ or $-u$, for which $|u^{\\top}v|=1$. Thus, the supremum is $1$.\n$$\\|\\nabla^3 f(x)\\| = \\alpha\\beta^{3}|\\cos(\\beta u^{\\top}x)|$$\n\nThe problem asks for a bound that holds uniformly for all steps $s$ in the ball $\\|s\\|\\leq \\Delta$. The remainder bound uses a Lipschitz constant $L_H$ for the Hessian valid on the ball of radius $\\Delta$ around $x=0$, which is the set $\\{x \\in \\mathbb{R}^n : \\|x\\| \\leq \\Delta\\}$.\n$$L_H = \\sup_{\\|x\\|\\leq\\Delta} \\|\\nabla^3 f(x)\\| = \\sup_{\\|x\\|\\leq\\Delta} \\alpha\\beta^{3}|\\cos(\\beta u^{\\top}x)|$$\nThe maximum value of $|\\cos(z)|$ is $1$, which occurs when $z$ is an integer multiple of $\\pi$. For any $\\Delta > 0$, the range of the argument $\\beta u^{\\top}x$ for $\\|x\\|\\le\\Delta$ is $[-\\beta \\Delta, \\beta \\Delta]$. This interval always contains $0$, and $\\cos(0)=1$. Hence:\n$$\\sup_{\\|x\\|\\leq\\Delta} |\\cos(\\beta u^{\\top}x)| = 1$$\nTherefore, the Lipschitz constant for the Hessian is $L_H = \\alpha\\beta^{3}$.\n\nWith this constant, the bound on the remainder $R_{2}(s)$ for any step $s$ is:\n$$|R_{2}(s)| \\leq \\frac{L_H}{6}\\|s\\|^{3} = \\frac{\\alpha\\beta^{3}}{6}\\|s\\|^{3}$$\n\nWe need to find the largest radius $\\Delta$ such that $|R_{2}(s)| \\leq \\tau$ for all steps $s$ with $\\|s\\| \\leq \\Delta$. This requires the upper bound on $|R_2(s)|$ to be no more than $\\tau$ throughout this region.\n$$\\frac{\\alpha\\beta^{3}}{6}\\|s\\|^{3} \\leq \\tau \\quad \\forall s \\text{ such that } \\|s\\|\\leq\\Delta$$\nThe left-hand side is a monotonically increasing function of $\\|s\\|$. Thus, the condition is satisfied if it holds for the maximum value of $\\|s\\|$, which is $\\Delta$.\n$$\\frac{\\alpha\\beta^{3}}{6}\\Delta^{3} \\leq \\tau$$\nTo find the largest $\\Delta$ that satisfies this inequality, we solve the corresponding equality:\n$$\\frac{\\alpha\\beta^{3}}{6}\\Delta^{3} = \\tau$$\n$$\\Delta^{3} = \\frac{6\\tau}{\\alpha\\beta^{3}}$$\nSolving for $\\Delta$, we obtain the largest possible radius for the trust region:\n$$\\Delta = \\left(\\frac{6\\tau}{\\alpha\\beta^{3}}\\right)^{\\frac{1}{3}}$$\nThis provides the required closed-form analytic expression for $\\Delta$ in terms of the given parameters $\\alpha$, $\\beta$, and $\\tau$.", "answer": "$$ \\boxed{\\left(\\frac{6\\tau}{\\alpha\\beta^{3}}\\right)^{\\frac{1}{3}}} $$", "id": "3191337"}]}