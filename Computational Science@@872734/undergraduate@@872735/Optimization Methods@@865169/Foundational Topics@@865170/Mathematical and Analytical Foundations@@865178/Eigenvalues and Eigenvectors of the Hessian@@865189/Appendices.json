{"hands_on_practices": [{"introduction": "Understanding the Hessian begins with exploring simple, illustrative systems. This first practice examines the function $f(x) = \\|x\\|^4$, a classic example whose rotational symmetry provides a perfect model for understanding the relationship between a function's geometry and its second-order derivatives. By deriving the Hessian's eigenvalues and eigenvectors, you will gain a concrete intuition for principal curvatures and see how they directly determine the behavior of optimization steps like the Newton direction [@problem_id:3124793].", "problem": "Consider the function $f:\\mathbb{R}^{n}\\to\\mathbb{R}$ defined by $f(x)=\\|x\\|^{4}$, where $\\|\\cdot\\|$ denotes the Euclidean norm. Starting from the core definitions of the gradient and the Hessian for a smooth scalar field on $\\mathbb{R}^{n}$, derive $\\nabla f(x)$ and $\\nabla^{2} f(x)$ for arbitrary $x$. For any $x\\neq 0$, determine the complete eigenvalue–eigenvector structure of $\\nabla^{2} f(x)$ by identifying directions in which $\\nabla^{2} f(x)$ acts as a scalar multiple of the identity, and explain why this spectrum depends only on $\\|x\\|$ and not on the direction of $x$ (isotropy under rotations). Then, using the defining linear system of the Newton step for unconstrained minimization, namely that the Newton step $p(x)$ at a point $x$ satisfies $\\nabla^{2} f(x)\\,p(x)=-\\nabla f(x)$, solve for $p(x)$ in closed form for $x\\neq 0$ and discuss its behavior as $\\|x\\|\\to 0$ in the context of the singular Hessian at the origin $x=0$. Express your final answer as the single closed-form analytic expression for $p(x)$ as a function of $x$. No rounding is required, and there are no physical units involved.", "solution": "The problem asks for a complete analysis of the function $f(x) = \\|x\\|^{4}$ on $\\mathbb{R}^{n}$, including its gradient, Hessian, the Hessian's spectral properties, and the associated Newton step for unconstrained minimization.\n\nFirst, we validate the problem statement.\n**Step 1: Extract Givens**\n- Function: $f(x)=\\|x\\|^{4}$ for $f:\\mathbb{R}^{n}\\to\\mathbb{R}$, where $\\|\\cdot\\|$ is the Euclidean norm.\n- Task 1: Derive the gradient $\\nabla f(x)$ and the Hessian $\\nabla^{2} f(x)$.\n- Task 2: For any $x\\neq 0$, determine the complete eigenvalue–eigenvector structure of $\\nabla^{2} f(x)$.\n- Task 3: Explain the isotropy of the spectrum of $\\nabla^{2} f(x)$ under rotations.\n- Task 4: Solve the Newton step equation $\\nabla^{2} f(x)\\,p(x)=-\\nabla f(x)$ for the Newton step $p(x)$ in closed form for $x\\neq 0$.\n- Task 5: Discuss the behavior of $p(x)$ as $\\|x\\|\\to 0$ in the context of the singular Hessian at the origin $x=0$.\n- Final Answer: The single closed-form analytic expression for $p(x)$ as a function of $x$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is mathematically well-defined and self-contained. The function $f(x)$ is a standard example in multivariable calculus and optimization theory. All concepts used—gradient, Hessian, eigenvalues, eigenvectors, and Newton's method—are fundamental to these fields. The problem is scientifically grounded, objective, and contains no contradictions or missing information. It is a standard, formalizable problem in the specified domain.\n\n**Step 3: Verdict and Action**\nThe problem is valid. We proceed with the solution.\n\n**Derivation of the Gradient $\\nabla f(x)$**\n\nThe function is given by $f(x) = \\|x\\|^{4}$, where $x \\in \\mathbb{R}^{n}$. We can write this in terms of the components $x_i$ of the vector $x$ as $f(x) = \\left( \\sum_{i=1}^{n} x_i^{2} \\right)^{2}$. The Euclidean norm squared is $\\|x\\|^{2} = x^{T}x$. Thus, $f(x) = (x^{T}x)^{2}$.\n\nThe gradient $\\nabla f(x)$ is a vector whose $k$-th component is the partial derivative $\\frac{\\partial f}{\\partial x_k}$. We use the chain rule:\n$$\n(\\nabla f(x))_k = \\frac{\\partial f}{\\partial x_k} = \\frac{\\partial}{\\partial x_k} \\left( \\left( \\sum_{i=1}^{n} x_i^{2} \\right)^{2} \\right)\n$$\n$$\n= 2 \\left( \\sum_{i=1}^{n} x_i^{2} \\right) \\cdot \\frac{\\partial}{\\partial x_k} \\left( \\sum_{i=1}^{n} x_i^{2} \\right)\n$$\nThe derivative of the sum is $\\frac{\\partial}{\\partial x_k} \\left( \\sum_{i=1}^{n} x_i^{2} \\right) = 2x_k$.\nSubstituting this back, we get:\n$$\n\\frac{\\partial f}{\\partial x_k} = 2 \\|x\\|^{2} \\cdot (2x_k) = 4\\|x\\|^{2} x_k\n$$\nIn vector form, this means the gradient is:\n$$\n\\nabla f(x) = 4\\|x\\|^{2} x\n$$\n\n**Derivation of the Hessian $\\nabla^{2} f(x)$**\n\nThe Hessian matrix $\\nabla^{2} f(x)$ has entries $(\\nabla^{2} f(x))_{jk} = \\frac{\\partial^{2} f}{\\partial x_j \\partial x_k}$. We differentiate the $k$-th component of the gradient with respect to $x_j$:\n$$\n\\frac{\\partial^{2} f}{\\partial x_j \\partial x_k} = \\frac{\\partial}{\\partial x_j} \\left( 4\\|x\\|^{2} x_k \\right) = \\frac{\\partial}{\\partial x_j} \\left[ 4 \\left( \\sum_{i=1}^{n} x_i^{2} \\right) x_k \\right]\n$$\nUsing the product rule for differentiation:\n$$\n\\frac{\\partial^{2} f}{\\partial x_j \\partial x_k} = 4x_k \\frac{\\partial}{\\partial x_j} \\left( \\sum_{i=1}^{n} x_i^{2} \\right) + 4 \\left( \\sum_{i=1}^{n} x_i^{2} \\right) \\frac{\\partial x_k}{\\partial x_j}\n$$\nThe terms evaluate to:\n- $\\frac{\\partial}{\\partial x_j} \\left( \\sum_{i=1}^{n} x_i^{2} \\right) = 2x_j$\n- $\\frac{\\partial x_k}{\\partial x_j} = \\delta_{jk}$ (the Kronecker delta)\n\nSubstituting these, we find the entries of the Hessian:\n$$\n(\\nabla^{2} f(x))_{jk} = 4x_k(2x_j) + 4\\|x\\|^{2}\\delta_{jk} = 8x_j x_k + 4\\|x\\|^{2}\\delta_{jk}\n$$\nIn matrix notation, the term $x_j x_k$ corresponds to the outer product matrix $xx^{T}$, and the term $\\delta_{jk}$ corresponds to the identity matrix $I$. Therefore, the Hessian matrix is:\n$$\n\\nabla^{2} f(x) = 8xx^{T} + 4\\|x\\|^{2} I\n$$\n\n**Eigenvalue and Eigenvector Structure of $\\nabla^{2} f(x)$ for $x \\neq 0$**\n\nThe Hessian is $\\nabla^{2} f(x) = 8xx^{T} + 4\\|x\\|^{2} I$. To find the eigenvalues and eigenvectors, we consider two cases for a vector $v \\in \\mathbb{R}^{n}$.\n\nCase 1: $v$ is parallel to $x$.\nLet $v = \\alpha x$ for some scalar $\\alpha \\neq 0$. Without loss of generality, we can simply test the vector $x$ itself.\n$$\n(\\nabla^{2} f(x)) x = (8xx^{T} + 4\\|x\\|^{2} I)x = 8x(x^{T}x) + 4\\|x\\|^{2}(Ix) = 8x\\|x\\|^{2} + 4\\|x\\|^{2}x = 12\\|x\\|^{2}x\n$$\nThis shows that $x$ is an eigenvector of $\\nabla^{2} f(x)$ with the corresponding eigenvalue $\\lambda_1 = 12\\|x\\|^{2}$. The eigenspace for this eigenvalue is the one-dimensional subspace spanned by $x$, i.e., $\\text{span}\\{x\\}$.\n\nCase 2: $v$ is orthogonal to $x$.\nLet $v$ be any non-zero vector such that $x^{T}v = 0$.\n$$\n(\\nabla^{2} f(x)) v = (8xx^{T} + 4\\|x\\|^{2} I)v = 8x(x^{T}v) + 4\\|x\\|^{2}(Iv) = 8x(0) + 4\\|x\\|^{2}v = 4\\|x\\|^{2}v\n$$\nThis shows that any vector $v$ orthogonal to $x$ is an eigenvector of $\\nabla^{2} f(x)$ with the corresponding eigenvalue $\\lambda_2 = 4\\|x\\|^{2}$. The space of vectors orthogonal to $x$ (the orthogonal complement of $\\text{span}\\{x\\}$) has dimension $n-1$. Thus, $\\lambda_2 = 4\\|x\\|^{2}$ is an eigenvalue with multiplicity $n-1$.\n\nIn summary, for $x \\neq 0$, the Hessian $\\nabla^{2} f(x)$ has two distinct eigenvalues:\n- $\\lambda_1 = 12\\|x\\|^{2}$ with multiplicity $1$, and its eigenspace is $\\text{span}\\{x\\}$.\n- $\\lambda_2 = 4\\|x\\|^{2}$ with multiplicity $n-1$, and its eigenspace is $\\{v \\in \\mathbb{R}^{n} \\mid x^{T}v=0\\}$.\n\n**Isotropy of the Eigenvalue Spectrum**\n\nThe eigenvalues $\\lambda_1 = 12\\|x\\|^{2}$ and $\\lambda_2 = 4\\|x\\|^{2}$ depend only on the scalar quantity $\\|x\\|$, the Euclidean norm of $x$. They do not depend on the direction of the vector $x$. Therefore, for any two vectors $x$ and $y$ with the same norm, $\\|x\\|=\\|y\\|$, the spectrum of $\\nabla^{2} f(x)$ will be identical to the spectrum of $\\nabla^{2} f(y)$.\nMore formally, consider a rotation, which is represented by an orthogonal matrix $R$ (with $R^T R = I$ and $\\det(R)=1$). The vector $Rx$ is a rotated version of $x$, and $\\|Rx\\|^{2} = (Rx)^{T}(Rx) = x^{T}R^{T}Rx = x^{T}Ix = \\|x\\|^{2}$. The Hessian at the rotated point $Rx$ is:\n$$\n\\nabla^{2} f(Rx) = 8(Rx)(Rx)^{T} + 4\\|Rx\\|^{2}I = 8Rxx^{T}R^{T} + 4\\|x\\|^{2}RR^{T} = R(8xx^{T} + 4\\|x\\|^{2}I)R^{T}\n$$\nSo, $\\nabla^{2} f(Rx) = R(\\nabla^{2} f(x))R^{T}$. This shows that the Hessian matrix at $Rx$ is related to the Hessian at $x$ by a similarity transformation. Since similarity transformations preserve eigenvalues, the spectrum of $\\nabla^{2} f(x)$ is invariant under rotations of $x$. This isotropy is a direct consequence of the rotational invariance of the original function $f(x) = \\|x\\|^{4}$, since $f(Rx) = \\|Rx\\|^{4} = \\|x\\|^{4} = f(x)$.\n\n**Solving for the Newton Step $p(x)$**\n\nThe Newton step $p(x)$ is defined by the linear system $\\nabla^{2} f(x) p(x) = -\\nabla f(x)$. For $x \\neq 0$, both the gradient and Hessian are non-zero. The eigenvalues of the Hessian ($12\\|x\\|^{2}$ and $4\\|x\\|^{2}$) are strictly positive, so the Hessian is positive definite and thus invertible. This guarantees a unique solution for $p(x)$.\nSubstituting the expressions for the gradient and Hessian:\n$$\n(8xx^{T} + 4\\|x\\|^{2} I) p(x) = -4\\|x\\|^{2} x\n$$\nGiven that the right-hand side is a multiple of $x$, we can hypothesize a solution of the form $p(x) = c x$ for some scalar $c$. Substituting this ansatz into the equation:\n$$\n(8xx^{T} + 4\\|x\\|^{2} I) (cx) = -4\\|x\\|^{2} x\n$$\n$$\nc(8xx^{T}x + 4\\|x\\|^{2}Ix) = -4\\|x\\|^{2} x\n$$\n$$\nc(8x\\|x\\|^{2} + 4\\|x\\|^{2}x) = -4\\|x\\|^{2} x\n$$\n$$\nc(12\\|x\\|^{2}x) = -4\\|x\\|^{2} x\n$$\n$$\n12c\\|x\\|^{2}x = -4\\|x\\|^{2}x\n$$\nSince this equation must hold for any $x \\neq 0$, we can equate the scalar coefficients:\n$$\n12c\\|x\\|^{2} = -4\\|x\\|^{2}\n$$\nAs $x \\neq 0$, we have $\\|x\\|^{2} \\neq 0$, so we can divide by it to find $c$:\n$$\n12c = -4 \\implies c = -\\frac{4}{12} = -\\frac{1}{3}\n$$\nThus, the Newton step is:\n$$\np(x) = -\\frac{1}{3}x\n$$\n\n**Behavior as $\\|x\\| \\to 0$ and the Singular Hessian at the Origin**\n\nLet's examine the situation at the origin, $x=0$.\nThe gradient at the origin is $\\nabla f(0) = 4\\|0\\|^{2}(0) = 0$.\nThe Hessian at the origin is $\\nabla^{2} f(0) = 8(0)(0)^{T} + 4\\|0\\|^{2}I = 0$, the zero matrix.\nThe zero matrix is singular (all its eigenvalues are $0$).\nThe Newton step equation at $x=0$ becomes:\n$$\n\\nabla^{2} f(0) p(0) = -\\nabla f(0) \\implies 0 \\cdot p(0) = 0\n$$\nThis equation is satisfied for any vector $p(0) \\in \\mathbb{R}^{n}$, meaning the Newton step is not uniquely defined at the origin.\nHowever, if we consider the limit of the Newton step as $x$ approaches $0$:\n$$\n\\lim_{x \\to 0} p(x) = \\lim_{x \\to 0} \\left(-\\frac{1}{3}x\\right) = 0\n$$\nThe limit exists and is the zero vector. So, while the Newton step is formally indeterminate at the singular point $x=0$, it can be defined by continuity. In an optimization context, this means that for any point $x_k$ close to the minimum at $0$, the next iterate would be $x_{k+1} = x_k + p(x_k) = x_k - \\frac{1}{3}x_k = \\frac{2}{3}x_k$. This implies linear convergence to the minimum with a rate of $\\frac{2}{3}$, which is slower than the quadratic convergence expected for Newton's method on functions with a non-singular Hessian at the minimum. The slow-down is due to the flatness of the function $f(x)=\\|x\\|^4$ near its minimum, which is characterized by the singular (zero) Hessian.\n\nThe problem asks for the single closed-form analytic expression for $p(x)$ as a function of $x$. This is the result derived for $x \\neq 0$, which is also continuous at $x=0$.", "answer": "$$\n\\boxed{-\\frac{1}{3}x}\n$$", "id": "3124793"}, {"introduction": "Building on the fundamentals, we now turn to a function with direct relevance to modern machine learning. This exercise analyzes the curvature of a sum of softplus functions, whose Hessian structure reveals how optimization can be challenged by varying local geometry [@problem_id:3124755]. You will calculate the eigenvalues and use them to determine the Hessian's condition number, a crucial metric that quantifies the difficulty of optimization and exposes potential numerical instability in certain regions of the parameter space.", "problem": "Consider the function $f:\\mathbb{R}^{n}\\to\\mathbb{R}$ defined by $f(\\mathbf{x})=\\sum_{i=1}^{n}\\ln\\!\\big(1+\\exp(x_{i})\\big)$, where $\\mathbf{x}=(x_{1},\\dots,x_{n})$. Starting from the definitions of the gradient (the vector of first-order partial derivatives) and the Hessian (the matrix of second-order partial derivatives), derive expressions for $\\nabla f(\\mathbf{x})$ and $\\nabla^{2} f(\\mathbf{x})$. Using only properties of partial differentiation, characterize the structure of $\\nabla^{2} f(\\mathbf{x})$, and then determine its eigenvalues and eigenvectors.\n\nNext, evaluate the spectral condition number (the ratio of the largest eigenvalue to the smallest eigenvalue) of the Hessian at the point $\\mathbf{x}^{\\ast}=\\big(\\ln(9),\\,0,\\,-\\ln(999),\\,\\ln(3),\\,-\\ln(3)\\big)\\in\\mathbb{R}^{5}$. Round your final numerical answer to $4$ significant figures.", "solution": "The problem asks for the derivation of the gradient and Hessian of a given function, the characterization of the Hessian's eigenvalues and eigenvectors, and the calculation of its spectral condition number at a specific point.\n\nThe function is given by $f:\\mathbb{R}^{n}\\to\\mathbb{R}$, where $f(\\mathbf{x})=\\sum_{i=1}^{n}\\ln(1+\\exp(x_{i}))$ for $\\mathbf{x}=(x_{1},\\dots,x_{n})$.\n\nFirst, we derive the gradient $\\nabla f(\\mathbf{x})$. The $j$-th component of the gradient is the partial derivative of $f$ with respect to $x_j$:\n$$ [\\nabla f(\\mathbf{x})]_j = \\frac{\\partial f}{\\partial x_j} = \\frac{\\partial}{\\partial x_j} \\left( \\sum_{i=1}^{n}\\ln(1+\\exp(x_{i})) \\right) $$\nBy the linearity of the differentiation operator, we can write:\n$$ \\frac{\\partial f}{\\partial x_j} = \\sum_{i=1}^{n} \\frac{\\partial}{\\partial x_j} \\ln(1+\\exp(x_{i})) $$\nThe term $\\ln(1+\\exp(x_{i}))$ only depends on the variable $x_j$ when the index $i$ is equal to $j$. For all $i \\neq j$, the partial derivative is zero. Therefore, the sum collapses to a single term corresponding to $i=j$:\n$$ \\frac{\\partial f}{\\partial x_j} = \\frac{d}{d x_j} \\ln(1+\\exp(x_j)) $$\nUsing the chain rule, $\\frac{d}{du}\\ln(u) = \\frac{1}{u}$, we get:\n$$ \\frac{\\partial f}{\\partial x_j} = \\frac{1}{1+\\exp(x_j)} \\cdot \\frac{d}{d x_j}(1+\\exp(x_j)) = \\frac{1}{1+\\exp(x_j)} \\cdot \\exp(x_j) = \\frac{\\exp(x_j)}{1+\\exp(x_j)} $$\nThis is the logistic sigmoid function, often denoted $\\sigma(x_j)$. The gradient vector is thus:\n$$ \\nabla f(\\mathbf{x}) = \\begin{pmatrix} \\frac{\\exp(x_1)}{1+\\exp(x_1)} \\\\ \\frac{\\exp(x_2)}{1+\\exp(x_2)} \\\\ \\vdots \\\\ \\frac{\\exp(x_n)}{1+\\exp(x_n)} \\end{pmatrix} $$\n\nNext, we derive the Hessian matrix $\\nabla^2 f(\\mathbf{x})$, whose elements are the second-order partial derivatives, $H_{jk} = \\frac{\\partial^2 f}{\\partial x_j \\partial x_k}$.\nWe consider two cases for the indices $j$ and $k$.\n\nCase 1: Off-diagonal elements ($j \\neq k$).\n$$ H_{jk} = \\frac{\\partial^2 f}{\\partial x_j \\partial x_k} = \\frac{\\partial}{\\partial x_j} \\left( \\frac{\\partial f}{\\partial x_k} \\right) = \\frac{\\partial}{\\partial x_j} \\left( \\frac{\\exp(x_k)}{1+\\exp(x_k)} \\right) $$\nSince the expression inside the derivative with respect to $x_j$ is a function of $x_k$ only, its partial derivative with respect to $x_j$ is zero.\n$$ H_{jk} = 0 \\quad \\text{for } j \\neq k $$\n\nCase 2: Diagonal elements ($j = k$).\n$$ H_{jj} = \\frac{\\partial^2 f}{\\partial x_j^2} = \\frac{d}{dx_j} \\left( \\frac{\\partial f}{\\partial x_j} \\right) = \\frac{d}{dx_j} \\left( \\frac{\\exp(x_j)}{1+\\exp(x_j)} \\right) $$\nWe apply the quotient rule, $\\frac{d}{dx}(\\frac{u}{v}) = \\frac{u'v - uv'}{v^2}$, with $u(x_j) = \\exp(x_j)$ and $v(x_j) = 1+\\exp(x_j)$. Their derivatives are $u'(x_j) = \\exp(x_j)$ and $v'(x_j) = \\exp(x_j)$.\n$$ H_{jj} = \\frac{\\exp(x_j)(1+\\exp(x_j)) - \\exp(x_j)\\exp(x_j)}{(1+\\exp(x_j))^2} = \\frac{\\exp(x_j) + \\exp(2x_j) - \\exp(2x_j)}{(1+\\exp(x_j))^2} = \\frac{\\exp(x_j)}{(1+\\exp(x_j))^2} $$\n\nThe Hessian matrix $\\nabla^2 f(\\mathbf{x})$ is a diagonal matrix, as all its off-diagonal elements are zero. Its structure is:\n$$ \\nabla^2 f(\\mathbf{x}) = \\text{diag}\\left(\\frac{\\exp(x_1)}{(1+\\exp(x_1))^2}, \\frac{\\exp(x_2)}{(1+\\exp(x_2))^2}, \\dots, \\frac{\\exp(x_n)}{(1+\\exp(x_n))^2}\\right) $$\n\nFor a diagonal matrix $D = \\text{diag}(d_1, d_2, \\dots, d_n)$, the eigenvalues are the diagonal entries $d_i$, and the corresponding eigenvectors are the standard basis vectors $\\mathbf{e}_i$ (where $\\mathbf{e}_i$ is the vector with a $1$ in the $i$-th position and $0$s elsewhere).\nTherefore, the eigenvalues of the Hessian $\\nabla^2 f(\\mathbf{x})$ are:\n$$ \\lambda_i = H_{ii} = \\frac{\\exp(x_i)}{(1+\\exp(x_i))^2} \\quad \\text{for } i = 1, \\dots, n $$\nThe corresponding eigenvectors are $\\mathbf{v}_i = \\mathbf{e}_i$.\n\nWe are asked to evaluate the spectral condition number of the Hessian at the point $\\mathbf{x}^{\\ast}=\\big(\\ln(9),\\,0,\\,-\\ln(999),\\,\\ln(3),\\,-\\ln(3)\\big)\\in\\mathbb{R}^{5}$. Here, $n=5$. The spectral condition number $\\kappa$ is the ratio of the largest eigenvalue to the smallest eigenvalue, $\\kappa = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}}$.\n\nLet $g(x) = \\frac{\\exp(x)}{(1+\\exp(x))^2}$. The eigenvalues are $\\lambda_i = g(x_i^*)$.\n1.  For $x_1^* = \\ln(9)$: $\\exp(x_1^*) = 9$.\n    $\\lambda_1 = g(\\ln(9)) = \\frac{9}{(1+9)^2} = \\frac{9}{100} = 0.09$.\n2.  For $x_2^* = 0$: $\\exp(x_2^*) = 1$.\n    $\\lambda_2 = g(0) = \\frac{1}{(1+1)^2} = \\frac{1}{4} = 0.25$.\n3.  For $x_3^* = -\\ln(999)$: $\\exp(x_3^*) = \\exp(-\\ln(999)) = \\frac{1}{999}$.\n    $\\lambda_3 = g(-\\ln(999)) = \\frac{1/999}{(1+1/999)^2} = \\frac{1/999}{(1000/999)^2} = \\frac{1}{999} \\frac{999^2}{1000^2} = \\frac{999}{1000^2} = 0.000999$.\n4.  For $x_4^* = \\ln(3)$: $\\exp(x_4^*) = 3$.\n    $\\lambda_4 = g(\\ln(3)) = \\frac{3}{(1+3)^2} = \\frac{3}{16} = 0.1875$.\n5.  For $x_5^* = -\\ln(3)$: $\\exp(x_5^*) = \\frac{1}{3}$.\n    $\\lambda_5 = g(-\\ln(3)) = \\frac{1/3}{(1+1/3)^2} = \\frac{1/3}{(4/3)^2} = \\frac{1}{3} \\frac{9}{16} = \\frac{3}{16} = 0.1875$.\n    Note that $g(x)$ is an even function, $g(x)=g(-x)$, which is confirmed by the fact that $\\lambda_5 = \\lambda_4$.\n\nThe calculated eigenvalues are $\\{0.09, 0.25, 0.000999, 0.1875, 0.1875\\}$.\nThe largest eigenvalue is $\\lambda_{\\max} = \\max\\{0.09, 0.25, 0.000999, 0.1875\\} = 0.25 = \\frac{1}{4}$.\nThe smallest eigenvalue is $\\lambda_{\\min} = \\min\\{0.09, 0.25, 0.000999, 0.1875\\} = 0.000999 = \\frac{999}{1000000}$.\n\nThe spectral condition number is the ratio:\n$$ \\kappa = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}} = \\frac{1/4}{999/1000000} = \\frac{1}{4} \\times \\frac{1000000}{999} = \\frac{250000}{999} $$\nTo obtain the numerical value, we perform the division:\n$$ \\kappa = 250.250250\\dots = 250.\\overline{250} $$\nThe problem requires this value to be rounded to $4$ significant figures. The first four significant figures of $250.250\\dots$ are $2, 5, 0, 2$. The fifth significant figure is $5$, which requires rounding up the fourth digit.\n$$ \\kappa \\approx 250.3 $$", "answer": "$$\\boxed{250.3}$$", "id": "3124755"}, {"introduction": "The final practice moves from analysis to design, challenging you to use the Hessian's spectral properties to build a more robust optimization algorithm. Faced with a function exhibiting a \"flat ridge\"—a region of near-zero curvature where standard Newton's method falters—you will implement a \"spectral-truncated\" Newton method [@problem_id:3124776]. This hands-on coding exercise demonstrates how to leverage eigenvalue analysis to intelligently adapt the search direction, providing a practical solution to the challenges of ill-conditioning and singular Hessians.", "problem": "Consider the unconstrained minimization of a twice continuously differentiable function $f : \\mathbb{R}^n \\to \\mathbb{R}$ using Newton-type methods. The classical Newton method selects an update direction by minimizing the local second-order Taylor approximation of $f$ around the current iterate. In the presence of directions of very small curvature (a \"flat ridge\"), the Hessian can have eigenvalues that are close to zero, making the pure Newton step unstable or ineffective.\n\nYour task is to design and implement a spectral-truncated Newton method that, at each iterate, decomposes the Hessian into its eigenvalues and eigenvectors and skips updates along eigenvectors whose eigenvalues are below a user-specified threshold $\\tau$. Then, analyze the convergence behavior of this method on a function exhibiting a flat ridge.\n\nFundamental base you should use:\n- The gradient $\\nabla f(\\mathbf{x})$ and the Hessian $\\nabla^2 f(\\mathbf{x})$ are defined by first and second derivatives, and the Newton direction is the direction that minimizes the second-order Taylor model of $f$ at the current point.\n- For a symmetric matrix $\\mathbf{H}$, spectral (eigen) decomposition yields $\\mathbf{H} = \\mathbf{Q} \\mathbf{\\Lambda} \\mathbf{Q}^\\top$ with orthonormal $\\mathbf{Q}$ and diagonal $\\mathbf{\\Lambda}$ whose entries are the eigenvalues, and any vector can be projected into the eigenbasis.\n\nProblem-specific setup:\n- Work in $\\mathbb{R}^2$ with the objective function\n$$\nf(x_1, x_2) = (x_1 - 1)^2 + x_2^4,\n$$\nwhich has a flat ridge along the $x_2$-axis near $x_2 = 0$. The unique global minimizer is at $(x_1^\\star, x_2^\\star) = (1, 0)$.\n- At each iterate $(x_1, x_2)$, form the Hessian, compute its eigenvalues and eigenvectors, and construct an update direction that uses only components associated with eigenvalues greater than or equal to a threshold $\\tau  0$. Intuitively, use the full Newton step along sufficiently curved directions and skip updates along ridges with curvature below $\\tau$.\n- Use a unit step size for the update. Terminate the iteration when either the update step has Euclidean norm less than a tolerance $\\varepsilon_{\\text{step}}$ or a maximum number of iterations $N_{\\max}$ is reached. There are no physical units in this problem; all quantities are unitless real numbers.\n\nTest suite:\nRun your method on the following $5$ test cases, each specified by $(x_1^{(0)}, x_2^{(0)}, \\tau)$, with $N_{\\max} = 100$ and $\\varepsilon_{\\text{step}} = 10^{-15}$:\n- Test $1$: $(0.0, 1.0, 10^{-12})$\n- Test $2$: $(0.0, 1.0, 0.05)$\n- Test $3$: $(0.0, 0.1, 0.5)$\n- Test $4$: $(10.0, 0.2, 3.0)$\n- Test $5$: $(-5.0, 0.0, 0.01)$\n\nFor each test case, after termination, compute the Euclidean distance from the final iterate $(x_1^{(f)}, x_2^{(f)})$ to $(1, 0)$, namely\n$$\nd = \\sqrt{(x_1^{(f)} - 1)^2 + (x_2^{(f)} - 0)^2}.\n$$\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[d_1,d_2,d_3,d_4,d_5]$), where $d_i$ is the distance for test $i$. Each $d_i$ must be a real number (float). No additional text should be printed.", "solution": "The user-provided problem is assessed as valid. It is a well-posed problem in the field of numerical optimization, based on established mathematical principles. The problem statement is self-contained, with all necessary data and conditions provided. The objective is clear and the requested output is specific.\n\nThe problem requires the implementation and analysis of a spectral-truncated Newton method for minimizing the function $f(x_1, x_2) = (x_1 - 1)^2 + x_2^4$. This function is characterized by a \"flat ridge\" along the $x_2$-axis near the minimizer, which can pose challenges for standard optimization algorithms. The proposed method adapts the update step based on the local curvature, as encoded in the eigenvalues of the Hessian matrix.\n\nFirst, we formulate the necessary mathematical components. The function to be minimized is $f : \\mathbb{R}^2 \\to \\mathbb{R}$ given by\n$$\nf(x_1, x_2) = (x_1 - 1)^2 + x_2^4\n$$\nThe unique global minimizer is at $\\mathbf{x}^\\star = (1, 0)$, where $f(1, 0) = 0$.\n\nThe gradient of $f$, denoted by $\\nabla f(\\mathbf{x})$, is a vector of its first partial derivatives:\n$$\n\\nabla f(\\mathbf{x}) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\end{pmatrix} = \\begin{pmatrix} 2(x_1 - 1) \\\\ 4x_2^3 \\end{pmatrix}\n$$\n\nThe Hessian of $f$, denoted by $\\nabla^2 f(\\mathbf{x})$ or $\\mathbf{H}(\\mathbf{x})$, is a matrix of its second partial derivatives:\n$$\n\\mathbf{H}(\\mathbf{x}) = \\begin{pmatrix} \\frac{\\partial^2 f}{\\partial x_1^2}  \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} \\\\ \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1}  \\frac{\\partial^2 f}{\\partial x_2^2} \\end{pmatrix} = \\begin{pmatrix} 2  0 \\\\ 0  12x_2^2 \\end{pmatrix}\n$$\nThe Hessian is a diagonal matrix for any point $\\mathbf{x} = (x_1, x_2)$. This simplifies the spectral analysis significantly. The eigenvalues of a diagonal matrix are its diagonal entries, and the standard basis vectors are the corresponding eigenvectors.\nThe eigenvalues are:\n$$\n\\lambda_1 = 2\n$$\n$$\n\\lambda_2 = 12x_2^2\n$$\nThe corresponding orthonormal eigenvectors are:\n$$\n\\mathbf{q}_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\quad \\text{and} \\quad \\mathbf{q}_2 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\n$$\nThe eigenvalue $\\lambda_1 = 2$ is constant, indicating a constant, positive curvature in the $x_1$ direction. The eigenvalue $\\lambda_2 = 12x_2^2$ depends on $x_2$. As $x_2$ approaches $0$, $\\lambda_2$ also approaches $0$, indicating that the curvature in the $x_2$ direction becomes extremely small. This corresponds to the \"flat ridge\" mentioned in the problem.\n\nThe classical Newton's method update direction, $\\mathbf{p}_k$, at an iterate $\\mathbf{x}_k$ is found by solving the linear system $\\mathbf{H}(\\mathbf{x}_k) \\mathbf{p}_k = -\\nabla f(\\mathbf{x}_k)$. The solution is $\\mathbf{p}_k = -\\mathbf{H}(\\mathbf{x}_k)^{-1} \\nabla f(\\mathbf{x}_k)$. Using the spectral decomposition $\\mathbf{H} = \\mathbf{Q} \\mathbf{\\Lambda} \\mathbf{Q}^\\top$, the update direction can be expressed as a sum over the contributions from each eigenvector direction:\n$$\n\\mathbf{p}_k = - \\sum_{i=1}^{2} \\frac{\\mathbf{q}_i^\\top \\nabla f(\\mathbf{x}_k)}{\\lambda_i} \\mathbf{q}_i\n$$\nThis formula shows that the update along each eigenvector $\\mathbf{q}_i$ is scaled by the inverse of the eigenvalue $\\lambda_i$. If an eigenvalue is close to zero, this scaling factor becomes very large, potentially leading to an unstable and excessively large update step.\n\nThe spectral-truncated Newton method addresses this issue by ignoring components of the update associated with eigenvalues smaller than a given threshold $\\tau  0$. The modified update direction, which we will also call $\\mathbf{p}_k$, is constructed as:\n$$\n\\mathbf{p}_k = - \\sum_{i \\text{ s.t. } \\lambda_i \\ge \\tau} \\frac{\\mathbf{q}_i^\\top \\nabla f(\\mathbf{x}_k)}{\\lambda_i} \\mathbf{q}_i\n$$\nLet us derive the specific components of $\\mathbf{p}_k$ for our problem. Let $\\mathbf{x}_k = (x_{1,k}, x_{2,k})$.\nThe gradient is $\\mathbf{g}_k = \\nabla f(\\mathbf{x}_k) = \\begin{pmatrix} 2(x_{1,k} - 1) \\\\ 4x_{2,k}^3 \\end{pmatrix}$.\nThe projections of the gradient onto the eigenvectors are:\n$$\n\\mathbf{q}_1^\\top \\mathbf{g}_k = \\begin{pmatrix} 1  0 \\end{pmatrix} \\begin{pmatrix} 2(x_{1,k} - 1) \\\\ 4x_{2,k}^3 \\end{pmatrix} = 2(x_{1,k} - 1)\n$$\n$$\n\\mathbf{q}_2^\\top \\mathbf{g}_k = \\begin{pmatrix} 0  1 \\end{pmatrix} \\begin{pmatrix} 2(x_{1,k} - 1) \\\\ 4x_{2,k}^3 \\end{pmatrix} = 4x_{2,k}^3\n$$\nThe update direction $\\mathbf{p}_k = \\begin{pmatrix} p_{1,k} \\\\ p_{2,k} \\end{pmatrix}$ is then constructed by checking the condition $\\lambda_i \\ge \\tau$ for each component.\n\nFor the first component (along $\\mathbf{q}_1 = [1, 0]^\\top$):\nThe eigenvalue is $\\lambda_1 = 2$. If $2 \\ge \\tau$, the component of the update is $p_{1,k} = -\\frac{2(x_{1,k}-1)}{2} = -(x_{1,k} - 1)$. If $2  \\tau$, this component is skipped, so $p_{1,k} = 0$.\n\nFor the second component (along $\\mathbf{q}_2 = [0, 1]^\\top$):\nThe eigenvalue is $\\lambda_2 = 12x_{2,k}^2$. If $12x_{2,k}^2 \\ge \\tau$, and assuming $x_{2,k} \\neq 0$, the component of the update is $p_{2,k} = -\\frac{4x_{2,k}^3}{12x_{2,k}^2} = -\\frac{x_{2,k}}{3}$. If $x_{2,k} = 0$, then $\\lambda_2=0$, and since $\\tau0$, the condition $0 \\ge \\tau$ is false, correctly yielding $p_{2,k}=0$. The case $x_{2,k}=0$ requires no special handling. If $12x_{2,k}^2  \\tau$, the component is skipped, so $p_{2,k} = 0$.\n\nThe iterative algorithm is as follows:\n1. Initialize $\\mathbf{x}_0 = (x_1^{(0)}, x_2^{(0)})$.\n2. For $k=0, 1, \\dots, N_{\\max}-1$:\n   a. Let $\\mathbf{x}_k = (x_{1,k}, x_{2,k})$.\n   b. Calculate the update direction components:\n      $p_{1,k} = \\begin{cases} -(x_{1,k}-1)  \\text{if } 2 \\ge \\tau \\\\ 0  \\text{if } 2  \\tau \\end{cases}$\n      $p_{2,k} = \\begin{cases} -x_{2,k}/3  \\text{if } 12x_{2,k}^2 \\ge \\tau \\\\ 0  \\text{if } 12x_{2,k}^2  \\tau \\end{cases}$\n   c. Form the update vector $\\mathbf{p}_k = \\begin{pmatrix} p_{1,k} \\\\ p_{2,k} \\end{pmatrix}$.\n   d. Check for termination: if the Euclidean norm $\\|\\mathbf{p}_k\\|_2  \\varepsilon_{\\text{step}}$, terminate the loop.\n   e. Update the iterate using a unit step size: $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\mathbf{p}_k$.\n3. Let the final iterate be $\\mathbf{x}^{(f)}$.\n4. Compute the final distance to the true minimizer $\\mathbf{x}^\\star = (1, 0)$: $d = \\|\\mathbf{x}^{(f)} - \\mathbf{x}^\\star\\|_2 = \\sqrt{(x_1^{(f)} - 1)^2 + (x_2^{(f)})^2}$.\n\nThis algorithm is implemented for each of the five test cases specified. The choice of $\\tau$ critically affects convergence. If $\\tau$ is too large, it may prematurely halt updates in certain directions, stalling the algorithm far from the optimum. If $\\tau$ is too small, the algorithm behaves like the standard Newton method, which may converge well for this specific problem but could be unstable in more complex scenarios with indefinite Hessians.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\n#\n# Execution Environment:\n# language: Python\n# version: 3.12\n# libraries:\n#   - name: numpy\n#     version: 1.23.5\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and runs the spectral-truncated Newton method for a set of\n    test cases to minimize f(x1, x2) = (x1 - 1)^2 + x2^4.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (initial_x1, initial_x2, threshold_tau)\n    test_cases = [\n        (0.0, 1.0, 10**-12),\n        (0.0, 1.0, 0.05),\n        (0.0, 0.1, 0.5),\n        (10.0, 0.2, 3.0),\n        (-5.0, 0.0, 0.01),\n    ]\n\n    # Constants for the optimization algorithm\n    N_max = 100\n    eps_step = 1e-15\n    x_star = np.array([1.0, 0.0])\n\n    results = []\n    for case in test_cases:\n        x1_0, x2_0, tau = case\n        x = np.array([x1_0, x2_0], dtype=float)\n\n        for _ in range(N_max):\n            x1, x2 = x[0], x[1]\n            \n            # The Hessian is diagonal: H = [[2, 0], [0, 12*x2^2]].\n            # Eigenvalues are the diagonal entries.\n            lambda1 = 2.0\n            lambda2 = 12.0 * x2**2\n            \n            # Initialize update direction vector p\n            p = np.zeros(2)\n\n            # Component 1 (along eigenvector q1 = [1, 0])\n            # The full Newton step component would be -(x1 - 1).\n            # We apply this step only if the eigenvalue is not too small.\n            if lambda1 = tau:\n                p[0] = -(x1 - 1)\n            \n            # Component 2 (along eigenvector q2 = [0, 1])\n            # The full Newton step component would be -x2 / 3.\n            # We apply this step only if the eigenvalue is not too small.\n            # The check `lambda2 = tau` also handles the x2=0 case, as\n            # lambda2 would be 0, and tau is specified to be  0.\n            if lambda2 = tau:\n                p[1] = -x2 / 3.0\n            \n            # Check for termination based on the norm of the update step\n            step_norm = np.linalg.norm(p)\n            if step_norm  eps_step:\n                break\n\n            # Update the iterate with a unit step size\n            x = x + p\n\n        # After the loop terminates (by convergence or max iterations),\n        # calculate the Euclidean distance to the global minimum.\n        final_distance = np.linalg.norm(x - x_star)\n        results.append(final_distance)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{d:.12f}' for d in results)}]\")\n\nsolve()\n\n```", "id": "3124776"}]}