{"hands_on_practices": [{"introduction": "The gradient vector, $\\nabla f(x)$, points in the direction of steepest ascent, but its magnitude, $\\|\\nabla f(x)\\|$, is equally important in optimization. For some functions, this magnitude can grow at an extreme rate, leading to \"exploding gradients\" that make algorithms like gradient descent unstable. This practice explores such a function, guiding you to derive its gradient and Hessian, and to see firsthand how these concepts are used to derive a principled step-size in optimization [@problem_id:3120161].", "problem": "Consider the function $f:\\mathbb{R}^{n}\\to\\mathbb{R}$ defined by $f(x)=\\exp\\!\\big(\\|x\\|_{2}^{2}\\big)$, where $\\|x\\|_{2}$ denotes the Euclidean norm (Two-Norm). Work under the standard definitions from multivariable calculus: the gradient $\\nabla f(x)$ is the unique vector satisfying $D_{u}f(x)=\\nabla f(x)\\cdot u$ for every direction $u\\in\\mathbb{R}^{n}$, where $D_{u}f(x)$ is the directional derivative of $f$ at $x$ in the direction $u$, and the Hessian $\\nabla^{2}f(x)$ is the matrix of second-order partial derivatives. Assume $x\\neq 0$ throughout.\n\n1. Starting from the definition of the gradient and the chain rule, derive $\\nabla f(x)$ for the given $f(x)$.\n\n2. Express the gradient norm $\\|\\nabla f(x)\\|_{2}$ purely as a function of $r=\\|x\\|_{2}$, and briefly explain, based on this expression, why gradient norms can become large as $r$ increases.\n\n3. In optimization methods, a common step-size damping strategy is to move along a unit descent direction rather than the raw gradient. Let $u(x)=-\\nabla f(x)/\\|\\nabla f(x)\\|_{2}$ be the unit negative gradient direction at $x$. Consider the one-dimensional function $g(\\alpha)=f\\big(x+\\alpha\\,u(x)\\big)$, where $\\alpha\\in\\mathbb{R}$ is a scalar step-size. Using the second-order Taylor expansion of $g(\\alpha)$ about $\\alpha=0$, derive the value $\\alpha^{\\star}$ that minimizes the quadratic model $g(0)+g^{\\prime}(0)\\,\\alpha+\\tfrac{1}{2}g^{\\prime\\prime}(0)\\,\\alpha^{2}$, and express $\\alpha^{\\star}$ symbolically in terms of $r=\\|x\\|_{2}$ only.\n\nReport the single expression for $\\alpha^{\\star}$ as your final answer. No rounding is required.", "solution": "The problem is validated as self-contained, scientifically grounded, and well-posed. The function $f(x) = \\exp(\\|x\\|_2^2)$ is infinitely differentiable for all $x \\in \\mathbb{R}^n$. The condition $x \\neq 0$ ensures that the gradient is non-zero, and thus the unit direction $u(x)$ is well-defined. The solution proceeds by answering the three parts in sequence.\n\nLet $r = \\|x\\|_2 = \\sqrt{\\sum_{i=1}^n x_i^2}$. The function can be written as $f(x) = \\exp(r^2)$.\n\n### 1. Derivation of the Gradient $\\nabla f(x)$\n\nTo find the gradient of $f(x) = \\exp(\\|x\\|_2^2)$, we apply the chain rule. Let the outer function be $g(y) = \\exp(y)$ and the inner function be $h(x) = \\|x\\|_2^2 = \\sum_{i=1}^n x_i^2$. Then $f(x) = g(h(x))$. The chain rule for gradients states that $\\nabla f(x) = g'(h(x)) \\nabla h(x)$.\n\nFirst, we compute the derivative of the outer function $g(y)$:\n$$\ng'(y) = \\frac{d}{dy}(\\exp(y)) = \\exp(y)\n$$\nEvaluated at $y=h(x)$, this gives $g'(h(x)) = \\exp(h(x)) = \\exp(\\|x\\|_2^2)$.\n\nNext, we compute the gradient of the inner function $h(x)$:\n$$\nh(x) = x_1^2 + x_2^2 + \\dots + x_n^2\n$$\nThe partial derivative with respect to the $i$-th component, $x_i$, is:\n$$\n\\frac{\\partial h}{\\partial x_i} = \\frac{\\partial}{\\partial x_i} \\left(\\sum_{j=1}^n x_j^2\\right) = 2x_i\n$$\nThe gradient $\\nabla h(x)$ is the vector of these partial derivatives:\n$$\n\\nabla h(x) = \\begin{pmatrix} 2x_1 \\\\ 2x_2 \\\\ \\vdots \\\\ 2x_n \\end{pmatrix} = 2x\n$$\nCombining these results, the gradient of $f(x)$ is:\n$$\n\\nabla f(x) = g'(h(x)) \\nabla h(x) = \\exp(\\|x\\|_2^2) \\cdot (2x) = 2\\exp(\\|x\\|_2^2)x\n$$\n\n### 2. Gradient Norm $\\|\\nabla f(x)\\|_2$\n\nUsing the expression for the gradient derived above, we compute its Euclidean norm.\n$$\n\\nabla f(x) = 2\\exp(\\|x\\|_2^2)x\n$$\nThe term $2\\exp(\\|x\\|_2^2)$ is a positive scalar. For any scalar $c \\in \\mathbb{R}$ and vector $v \\in \\mathbb{R}^n$, we have $\\|cv\\|_2 = |c|\\|v\\|_2$.\n$$\n\\|\\nabla f(x)\\|_2 = \\|2\\exp(\\|x\\|_2^2)x\\|_2 = 2\\exp(\\|x\\|_2^2)\\|x\\|_2\n$$\nSubstituting $r = \\|x\\|_2$, the expression for the gradient norm becomes:\n$$\n\\|\\nabla f(x)\\|_2 = 2r\\exp(r^2)\n$$\nThis expression demonstrates why gradient norms can become large as $r = \\|x\\|_2$ increases. The term $\\exp(r^2)$ grows super-exponentially with $r$. As $r \\to \\infty$, this term dominates, causing the magnitude of the gradient to increase extremely rapidly.\n\n### 3. Derivation of the Optimal Step-Size $\\alpha^\\star$\n\nWe are asked to find the value $\\alpha^\\star$ that minimizes the second-order Taylor expansion of $g(\\alpha) = f(x+\\alpha u(x))$ around $\\alpha=0$. The quadratic model is:\n$$\nq(\\alpha) = g(0) + g'(0)\\alpha + \\frac{1}{2}g''(0)\\alpha^2\n$$\nTo find the minimizer, we set the derivative of $q(\\alpha)$ with respect to $\\alpha$ to zero:\n$$\nq'(\\alpha) = g'(0) + g''(0)\\alpha = 0 \\implies \\alpha^\\star = -\\frac{g'(0)}{g''(0)}\n$$\nWe must compute $g'(0)$ and $g''(0)$.\n\nThe first derivative $g'(\\alpha)$ is found using the chain rule for directional derivatives:\n$$\ng'(\\alpha) = \\frac{d}{d\\alpha}f(x+\\alpha u) = \\nabla f(x+\\alpha u) \\cdot u\n$$\nEvaluating at $\\alpha=0$:\n$$\ng'(0) = \\nabla f(x) \\cdot u\n$$\nThe direction $u = u(x)$ is the unit negative gradient direction:\n$$\nu = -\\frac{\\nabla f(x)}{\\|\\nabla f(x)\\|_2}\n$$\nSubstituting this into the expression for $g'(0)$:\n$$\ng'(0) = \\nabla f(x) \\cdot \\left(-\\frac{\\nabla f(x)}{\\|\\nabla f(x)\\|_2}\\right) = -\\frac{\\|\\nabla f(x)\\|_2^2}{\\|\\nabla f(x)\\|_2} = -\\|\\nabla f(x)\\|_2\n$$\nFrom Part 2, $\\|\\nabla f(x)\\|_2 = 2r\\exp(r^2)$, so:\n$$\ng'(0) = -2r\\exp(r^2)\n$$\nThe second derivative $g''(\\alpha)$ is the second-order directional derivative:\n$$\ng''(\\alpha) = \\frac{d}{d\\alpha}\\left(\\nabla f(x+\\alpha u) \\cdot u\\right) = u^T \\nabla^2 f(x+\\alpha u) u\n$$\nEvaluating at $\\alpha=0$:\n$$\ng''(0) = u^T \\nabla^2 f(x) u\n$$\nWe need the Hessian matrix $\\nabla^2 f(x)$. The $(i,j)$ entry of the Hessian is $(\\nabla^2 f(x))_{ij} = \\frac{\\partial}{\\partial x_i}(\\nabla f(x))_j$.\n$$\n(\\nabla f(x))_j = 2x_j\\exp(\\|x\\|_2^2)\n$$\nUsing the product rule for differentiation:\n$$\n\\frac{\\partial}{\\partial x_i} \\big(2x_j\\exp(\\|x\\|_2^2)\\big) = 2x_j\\frac{\\partial}{\\partial x_i}\\exp(\\|x\\|_2^2) + 2\\exp(\\|x\\|_2^2)\\frac{\\partial x_j}{\\partial x_i}\n$$\n$$\n= 2x_j\\exp(\\|x\\|_2^2)(2x_i) + 2\\exp(\\|x\\|_2^2)\\delta_{ij} = 2\\exp(\\|x\\|_2^2)(2x_i x_j + \\delta_{ij})\n$$\nwhere $\\delta_{ij}$ is the Kronecker delta. The Hessian matrix is $\\nabla^2 f(x) = 2\\exp(\\|x\\|_2^2)(2xx^T + I)$, where $I$ is the identity matrix.\n\nNow we compute $g''(0)$. The direction $u$ can be simplified:\n$$\nu = -\\frac{2\\exp(\\|x\\|_2^2)x}{2\\exp(\\|x\\|_2^2)\\|x\\|_2} = -\\frac{x}{\\|x\\|_2} = -\\frac{x}{r}\n$$\nSubstituting into $g''(0) = u^T \\nabla^2 f(x) u$:\n$$\ng''(0) = \\left(-\\frac{x^T}{r}\\right) \\left[2\\exp(r^2)(2xx^T+I)\\right] \\left(-\\frac{x}{r}\\right)\n$$\n$$\n= \\frac{2\\exp(r^2)}{r^2} \\left[ x^T(2xx^T+I)x \\right] = \\frac{2\\exp(r^2)}{r^2} \\left[ 2(x^T x)(x^T x) + x^T x \\right]\n$$\nSince $x^T x = \\|x\\|_2^2 = r^2$:\n$$\ng''(0) = \\frac{2\\exp(r^2)}{r^2} \\left[ 2(r^2)(r^2) + r^2 \\right] = \\frac{2\\exp(r^2)}{r^2} \\left[ 2r^4 + r^2 \\right]\n$$\nFactoring out $r^2$ from the bracket (since $r > 0$):\n$$\ng''(0) = \\frac{2\\exp(r^2)r^2(2r^2+1)}{r^2} = 2\\exp(r^2)(2r^2+1)\n$$\nFinally, we compute $\\alpha^\\star$:\n$$\n\\alpha^\\star = -\\frac{g'(0)}{g''(0)} = -\\frac{-2r\\exp(r^2)}{2\\exp(r^2)(2r^2+1)}\n$$\nThe terms $2\\exp(r^2)$ cancel, yielding the final expression for $\\alpha^\\star$ in terms of $r$:\n$$\n\\alpha^\\star = \\frac{r}{2r^2+1}\n$$", "answer": "$$\\boxed{\\frac{r}{2r^{2}+1}}$$", "id": "3120161"}, {"introduction": "The direction of the gradient is not random; it is intimately linked to the geometry of the function's level sets. For the ubiquitous linear least squares problem, $f(x) = \\| Ax - b \\|_2^2$, this geometry is dictated by the properties of the matrix $A$. This exercise uses the powerful tool of Singular Value Decomposition (SVD) to dissect the gradient's orientation, revealing why gradient-based methods can struggle when the problem is ill-conditioned [@problem_id:3120177]. Your analysis will provide a deep, geometric reason for the characteristic slow convergence of gradient descent.", "problem": "Consider a matrix $A \\in \\mathbb{R}^{m \\times n}$ with full column rank and an observed vector $b \\in \\mathbb{R}^{m}$. Define the objective function $f:\\mathbb{R}^{n} \\to \\mathbb{R}$ by $f(x) = \\|A x - b\\|_{2}^{2}$. Let the singular value decomposition (SVD) of $A$ be $A = U \\Sigma V^{\\top}$, where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is diagonal with strictly positive diagonal entries $\\sigma_{1} \\geq \\sigma_{2} \\geq \\cdots \\geq \\sigma_{n}  0$. Denote the right singular vectors by $v_{i} \\in \\mathbb{R}^{n}$ (the columns of $V$) and the left singular vectors by $u_{i} \\in \\mathbb{R}^{m}$ (the columns of $U$). Let $x_{0} \\in \\mathbb{R}^{n}$ be a current iterate and define the residual $r = A x_{0} - b$. Let $c = U^{\\top} r \\in \\mathbb{R}^{m}$ so that $c_{i} = u_{i}^{\\top} r$ for $i = 1, \\dots, m$. Assume $A$ is ill-conditioned, meaning $\\sigma_{1} / \\sigma_{n}$ is large.\n\n1) Starting from the definition of the directional derivative,\n$$\nD_{v} f(x_{0}) = \\lim_{t \\to 0} \\frac{f(x_{0} + t v) - f(x_{0})}{t},\n$$\nderive a formula for $D_{v_{i}} f(x_{0})$ for each $i \\in \\{1, \\dots, n\\}$ in terms of $\\sigma_{i}$ and $c_{i}$.\n\n2) Using only orthogonality properties and basic rules of vector calculus, derive the cosine of the angle between the steepest descent direction at $x_{0}$, which is the direction $- \\nabla f(x_{0})$, and a right singular vector $v_{k}$ for a fixed $k \\in \\{1, \\dots, n\\}$. Express your result in terms of $\\{\\sigma_{j}\\}_{j=1}^{n}$ and $\\{c_{j}\\}_{j=1}^{n}$.\n\n3) Based on your expression in part 2, briefly explain how ill-conditioning (that is, widely varying $\\sigma_{j}$) affects which right singular vectors $v_{k}$ the initial gradient descent trajectory aligns with most strongly, assuming $c_{j} \\neq 0$ for all $j$.\n\nFor your final answer, report your closed-form expression from part 2 for the cosine as a function of $k$, $\\{\\sigma_{j}\\}_{j=1}^{n}$, and $\\{c_{j}\\}_{j=1}^{n}$. No numerical evaluation is required.", "solution": "The problem statement has been rigorously validated and is found to be self-contained, mathematically sound, and well-posed. It presents a standard exercise in the analysis of optimization algorithms for linear least squares. We may proceed with the solution.\n\nThe problem asks for three related derivations concerning the objective function $f(x) = \\|A x - b\\|_{2}^{2}$, where $A \\in \\mathbb{R}^{m \\times n}$ has SVD $A = U \\Sigma V^{\\top}$.\n\nFirst, we derive a formula for the directional derivative of $f$ at a point $x_{0}$ in the direction of a right singular vector $v_{i}$. The directional derivative $D_{v_{i}} f(x_{0})$ is defined as:\n$$\nD_{v_{i}} f(x_{0}) = \\lim_{t \\to 0} \\frac{f(x_{0} + t v_{i}) - f(x_{0})}{t}\n$$\nWe begin by expanding the term $f(x_{0} + t v_{i})$:\n$$\nf(x_{0} + t v_{i}) = \\|A(x_{0} + t v_{i}) - b\\|_{2}^{2} = \\|(A x_{0} - b) + t A v_{i}\\|_{2}^{2}\n$$\nUsing the definition of the residual, $r = A x_{0} - b$, we have:\n$$\nf(x_{0} + t v_{i}) = \\|r + t A v_{i}\\|_{2}^{2}\n$$\nExpanding the squared L2-norm, which is equivalent to the dot product of the vector with itself:\n$$\n\\|r + t A v_{i}\\|_{2}^{2} = (r + t A v_{i})^{\\top}(r + t A v_{i}) = r^{\\top}r + 2t r^{\\top}(A v_{i}) + t^{2} (A v_{i})^{\\top}(A v_{i})\n$$\nThis can be written in terms of norms as:\n$$\nf(x_{0} + t v_{i}) = \\|r\\|_{2}^{2} + 2t r^{\\top}(A v_{i}) + t^{2} \\|A v_{i}\\|_{2}^{2}\n$$\nGiven that $f(x_{0}) = \\|A x_{0} - b\\|_{2}^{2} = \\|r\\|_{2}^{2}$, we can form the difference quotient:\n$$\n\\frac{f(x_{0} + t v_{i}) - f(x_{0})}{t} = \\frac{(\\|r\\|_{2}^{2} + 2t r^{\\top}(A v_{i}) + t^{2} \\|A v_{i}\\|_{2}^{2}) - \\|r\\|_{2}^{2}}{t} = 2 r^{\\top}(A v_{i}) + t \\|A v_{i}\\|_{2}^{2}\n$$\nTaking the limit as $t \\to 0$:\n$$\nD_{v_{i}} f(x_{0}) = \\lim_{t \\to 0} (2 r^{\\top}(A v_{i}) + t \\|A v_{i}\\|_{2}^{2}) = 2 r^{\\top}(A v_{i})\n$$\nTo express this in the desired terms, we use the SVD of $A$. From $A = U \\Sigma V^{\\top}$, we can write $A V = U \\Sigma$. Since the columns of $V$ are the right singular vectors $v_{j}$, the $i$-th column of the matrix equation gives $A v_{i} = \\sigma_{i} u_{i}$, where $u_{i}$ is the $i$-th left singular vector (the $i$-th column of $U$) and $\\sigma_{i}$ is the corresponding singular value.\nSubstituting this into our expression for the directional derivative:\n$$\nD_{v_{i}} f(x_{0}) = 2 r^{\\top}(\\sigma_{i} u_{i}) = 2 \\sigma_{i} (r^{\\top} u_{i})\n$$\nBy definition, the component $c_{i}$ of the vector $c = U^{\\top} r$ is $c_{i} = u_{i}^{\\top} r$. Since the dot product is symmetric ($u_{i}^{\\top} r = r^{\\top} u_{i}$), we arrive at the final expression for the first part:\n$$\nD_{v_{i}} f(x_{0}) = 2 \\sigma_{i} c_{i}\n$$\n\nSecond, we derive the cosine of the angle between the steepest descent direction at $x_{0}$, which is $d = - \\nabla f(x_{0})$, and a right singular vector $v_{k}$. The cosine of the angle $\\theta_{k}$ between two non-zero vectors $p$ and $q$ is given by $\\cos(\\theta_{k}) = \\frac{p^{\\top}q}{\\|p\\|_{2}\\|q\\|_{2}}$. Here, $p = d = -\\nabla f(x_{0})$ and $q = v_{k}$.\n\nFirst, we find the gradient of $f(x) = \\|Ax - b\\|_{2}^{2} = (Ax-b)^{\\top}(Ax-b)$.\n$$\nf(x) = (x^{\\top}A^{\\top} - b^{\\top})(Ax-b) = x^{\\top}A^{\\top}Ax - 2b^{\\top}Ax + b^{\\top}b\n$$\nThe gradient of this quadratic form with respect to $x$ is:\n$$\n\\nabla f(x) = 2 A^{\\top} A x - 2 A^{\\top} b = 2 A^{\\top} (A x - b)\n$$\nAt the point $x_{0}$, using the residual $r=Ax_0-b$:\n$$\n\\nabla f(x_{0}) = 2 A^{\\top} r\n$$\nThe steepest descent direction is $d = - \\nabla f(x_{0}) = -2 A^{\\top} r$.\n\nNow we compute the terms for the cosine formula. The numerator is the dot product $d^{\\top}v_{k}$:\n$$\nd^{\\top}v_{k} = (-2 A^{\\top} r)^{\\top}v_{k} = -2 r^{\\top} A v_{k}\n$$\nUsing the relation $A v_{k} = \\sigma_{k} u_{k}$ and the definition $c_{k} = u_{k}^{\\top} r$:\n$$\nd^{\\top}v_{k} = -2 r^{\\top} (\\sigma_{k} u_{k}) = -2 \\sigma_{k} (r^{\\top} u_{k}) = -2 \\sigma_{k} c_{k}\n$$\nThe denominator involves the norms of the vectors. The vector $v_{k}$ is a column of the orthogonal matrix $V$, so it is a unit vector: $\\|v_{k}\\|_{2} = 1$. The norm of the steepest descent direction is:\n$$\n\\|d\\|_{2} = \\|-2 A^{\\top} r\\|_{2} = 2 \\|A^{\\top} r\\|_{2}\n$$\nWe must express $\\|A^{\\top} r\\|_{2}$ in terms of $\\{\\sigma_{j}\\}$ and $\\{c_{j}\\}$. We use the SVD for $A^{\\top} = (U \\Sigma V^{\\top})^{\\top} = V \\Sigma^{\\top} U^{\\top}$.\n$$\nA^{\\top}r = V \\Sigma^{\\top} (U^{\\top}r) = V \\Sigma^{\\top} c\n$$\nSince $V$ is an orthogonal matrix, it preserves the L2-norm, so $\\|A^{\\top}r\\|_{2} = \\|V \\Sigma^{\\top} c\\|_{2} = \\|\\Sigma^{\\top} c\\|_{2}$.\nThe matrix $\\Sigma^{\\top}$ is an $n \\times m$ diagonal matrix with entries $\\sigma_{1}, \\dots, \\sigma_{n}$ on its diagonal. The vector $c = U^{\\top}r \\in \\mathbb{R}^{m}$ has components $c_{j}$. The product $\\Sigma^{\\top}c$ is a vector in $\\mathbb{R}^{n}$ with components $(\\sigma_{j} c_{j})$ for $j = 1, \\dots, n$.\n$$\n\\Sigma^{\\top} c = \\begin{pmatrix} \\sigma_{1} c_{1} \\\\ \\sigma_{2} c_{2} \\\\ \\vdots \\\\ \\sigma_{n} c_{n} \\end{pmatrix}\n$$\nThe norm is therefore:\n$$\n\\|A^{\\top} r\\|_{2} = \\|\\Sigma^{\\top} c\\|_{2} = \\sqrt{\\sum_{j=1}^{n} (\\sigma_{j} c_{j})^{2}}\n$$\nSubstituting all parts into the cosine formula:\n$$\n\\cos(\\theta_{k}) = \\frac{d^{\\top}v_{k}}{\\|d\\|_{2} \\|v_{k}\\|_{2}} = \\frac{-2 \\sigma_{k} c_{k}}{(2 \\sqrt{\\sum_{j=1}^{n} \\sigma_{j}^{2} c_{j}^{2}})(1)} = \\frac{-\\sigma_{k} c_{k}}{\\sqrt{\\sum_{j=1}^{n} \\sigma_{j}^{2} c_{j}^{2}}}\n$$\nThis is the required expression.\n\nThird, we explain how ill-conditioning affects the alignment of the initial gradient descent trajectory with the right singular vectors $v_{k}$. The alignment is measured by $|\\cos(\\theta_{k})|$. Let us consider the squared cosine for simplicity:\n$$\n\\cos^{2}(\\theta_{k}) = \\frac{\\sigma_{k}^{2} c_{k}^{2}}{\\sum_{j=1}^{n} \\sigma_{j}^{2} c_{j}^{2}}\n$$\nThis expression shows how the squared-magnitude of the projection of the gradient direction onto $v_k$ is distributed. Ill-conditioning implies that the singular values vary over a large range, i.e., $\\sigma_{1} \\gg \\sigma_{n}$. Assuming the components $c_{j} = u_{j}^{\\top} r$ are non-zero and of comparable magnitude, the sum in the denominator, $\\sum_{j=1}^{n} \\sigma_{j}^{2} c_{j}^{2}$, will be overwhelmingly dominated by the terms corresponding to the largest singular values.\nFor instance, the term $\\sigma_{1}^{2} c_{1}^{2}$ will be much larger than $\\sigma_{n}^{2} c_{n}^{2}$.\nConsequently, for a singular vector $v_{k}$ associated with a large singular value $\\sigma_{k}$ (e.g., $k=1$), the numerator $\\sigma_{k}^{2} c_{k}^{2}$ will be large, making $\\cos^{2}(\\theta_{k})$ close to $1$. The steepest descent direction $-\\nabla f(x_{0})$ is thus strongly aligned with $v_{k}$.\nConversely, for a singular vector $v_{k}$ associated with a small singular value $\\sigma_{k}$ (e.g., $k=n$), the numerator $\\sigma_{k}^{2} c_{k}^{2}$ will be very small compared to the denominator, making $\\cos^{2}(\\theta_{k})$ close to $0$. The steepest descent direction is nearly orthogonal to these singular vectors.\nTherefore, the initial trajectory of gradient descent, which follows $-\\nabla f(x_{0})$, aligns predominantly with the right singular vectors corresponding to the largest singular values. This leads to rapid initial reduction of the error components in these directions but extremely slow progress for error components associated with small singular values, which is the characteristic slow convergence of gradient descent on ill-conditioned problems.", "answer": "$$\n\\boxed{\\frac{-\\sigma_{k} c_{k}}{\\sqrt{\\sum_{j=1}^{n} \\sigma_{j}^{2} c_{j}^{2}}}}\n$$", "id": "3120177"}, {"introduction": "Many functions in modern optimization and machine learning are not differentiable everywhere, featuring \"kinks\" or sharp corners where the gradient is undefined. In these situations, we must return to the more fundamental concept of the directional derivative to understand how the function changes. This practice examines a function defined with a `max` operator, which creates a potential non-differentiable boundary, and asks you to rigorously investigate its properties at this boundary using the definition of the directional derivative [@problem_id:3120157]. The result may challenge your initial intuition about differentiability.", "problem": "Consider the function $f:\\mathbb{R}^{n}\\to\\mathbb{R}$ defined by $f(x)=\\max\\{0, \\|x\\|-1\\}^{2}$, where $\\|x\\|$ denotes the Euclidean norm. Use only the core definitions from multivariable calculus and optimization methods: differentiability at a point $x_{0}$ means there exists a linear map $L$ such that $f(x_{0}+h)=f(x_{0})+L(h)+o(\\|h\\|)$ as $h\\to 0$, the gradient $\\nabla f(x)$ is the unique vector representing $L$ via $L(h)=\\nabla f(x)^{\\top}h$, and the directional derivative of $f$ at $x$ in direction $d$ is $D f(x;d)=\\lim_{t\\to 0}\\frac{f(x+td)-f(x)}{t}$ when this limit exists. Starting from these definitions and well-tested formulas (including the chain rule and the differentiability of the Euclidean norm at all $x\\neq 0$), investigate the differentiability of $f$ at the boundary points satisfying $\\|x\\|=1$, and derive explicit expressions for the gradient inside the unit ball ($\\|x\\|1$) and outside the unit ball ($\\|x\\|1$). Then compute the directional derivative at an arbitrary boundary point $x_{0}$ with $\\|x_{0}\\|=1$ in an arbitrary direction $d\\in\\mathbb{R}^{n}$. Provide your final results in the following order: (i) the gradient for $\\|x\\|1$, (ii) the gradient for $\\|x\\|1$, (iii) the directional derivative at a boundary point with $\\|x\\|=1$ in any direction $d$. The final answer must be an exact analytic expression; do not perform any numerical rounding.", "solution": "The problem is subjected to validation before a solution is attempted.\n\n### Step 1: Extract Givens\n- Function definition: $f:\\mathbb{R}^{n}\\to\\mathbb{R}$ is given by $f(x)=\\max\\{0, \\|x\\|-1\\}^{2}$.\n- Notation: $\\|x\\|$ represents the Euclidean norm of the vector $x \\in \\mathbb{R}^n$.\n- Core definitions:\n    - Differentiability at $x_{0}$: Existence of a linear map $L$ such that $f(x_{0}+h)=f(x_{0})+L(h)+o(\\|h\\|)$ as $h\\to 0$.\n    - Gradient $\\nabla f(x)$: The unique vector representing $L$ via the inner product, $L(h)=\\nabla f(x)^{\\top}h$.\n    - Directional derivative: $D f(x;d)=\\lim_{t\\to 0}\\frac{f(x+td)-f(x)}{t}$, when the limit exists.\n- Assumed knowledge: Chain rule and the differentiability of the Euclidean norm for all $x \\neq 0$.\n- Tasks:\n    1. Investigate the differentiability of $f$ at boundary points where $\\|x\\|=1$.\n    2. Derive the gradient for $\\|x\\|1$.\n    3. Derive the gradient for $\\|x\\|1$.\n    4. Compute the directional derivative at a point $x_0$ with $\\|x_0\\|=1$ in an arbitrary direction $d\\in\\mathbb{R}^{n}$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is a standard exercise in multivariable calculus and non-smooth optimization. The function $f(x)$ is a well-defined composition of standard functions (norm, subtraction, max, square). No scientific or factual unsoundness is present.\n- **Well-Posed:** The problem is clearly stated and asks for specific mathematical objects (gradients and a directional derivative) under well-defined conditions. A unique solution exists.\n- **Objective:** The language is formal and unambiguous. All terms are standard in mathematics.\n- **Completeness and Consistency:** The problem is self-contained. The provided definitions and assumed knowledge are sufficient for its resolution. There are no internal contradictions.\n- **No other flaws are identified.** The problem does not rely on metaphors, is relevant to the specified topic, is not trivial, and is mathematically verifiable.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Solution Derivation\nThe function is $f(x) = \\max\\{0, \\|x\\|-1\\}^2$. We analyze the function in three distinct regions of $\\mathbb{R}^n$.\n\n**Part (i): Gradient for $\\|x\\|1$**\n\nFor any $x$ in the open unit ball, i.e., $\\|x\\|1$, the term $\\|x\\|-1$ is negative. The definition of the maximum function implies $\\max\\{0, \\|x\\|-1\\} = 0$.\nThus, for all $x$ such that $\\|x\\|1$, the function simplifies to:\n$$f(x) = 0^2 = 0$$\nThe function $f(x)$ is constant on the open set $\\{x \\in \\mathbb{R}^n \\mid \\|x\\|1\\}$. The derivative of a constant function is the zero map, and its corresponding gradient is the zero vector.\nTherefore, for $\\|x\\|1$, the gradient is:\n$$\\nabla f(x) = 0$$\n\n**Part (ii): Gradient for $\\|x\\|1$**\n\nFor any $x$ outside the closed unit ball, i.e., $\\|x\\|1$, the term $\\|x\\|-1$ is positive. The definition of the maximum function implies $\\max\\{0, \\|x\\|-1\\} = \\|x\\|-1$.\nThus, for all $x$ such that $\\|x\\|1$, the function simplifies to:\n$$f(x) = (\\|x\\|-1)^2$$\nThis function is a composition $f(x) = g(h(x))$, where $g(u) = u^2$ and $h(x) = \\|x\\|-1$. Both functions are differentiable in their respective domains (for $h(x)$, we require $x \\neq 0$, which is satisfied since $\\|x\\|1$). We apply the chain rule for gradients: $\\nabla f(x) = g'(h(x)) \\nabla h(x)$.\n\nThe derivative of $g(u)$ is $g'(u) = 2u$. Evaluated at $h(x)$, this gives $g'(h(x)) = 2(\\|x\\|-1)$.\nThe gradient of $h(x)$ is $\\nabla h(x) = \\nabla(\\|x\\|-1) = \\nabla(\\|x\\|)$. The gradient of the Euclidean norm is a standard result: $\\nabla(\\|x\\|) = \\frac{x}{\\|x\\|}$ for $x \\neq 0$.\n\nCombining these results, the gradient of $f(x)$ for $\\|x\\|1$ is:\n$$\\nabla f(x) = 2(\\|x\\|-1) \\frac{x}{\\|x\\|}$$\nThis can also be written as $\\nabla f(x) = 2\\left(1 - \\frac{1}{\\|x\\|}\\right)x$.\n\n**Part (iii): Differentiability and Directional Derivative for $\\|x_0\\|=1$**\n\nLet $x_0$ be a point on the boundary of the unit ball, so $\\|x_0\\|=1$. At this point, $f(x_0) = \\max\\{0, \\|x_0\\|-1\\}^2 = \\max\\{0, 1-1\\}^2 = 0$.\n\nFirst, we investigate differentiability. A function is differentiable at a point if its partial derivatives exist and are continuous in a neighborhood of that point.\nFrom parts (i) and (ii), the gradient is piecewise defined:\n$$ \\nabla f(x) = \\begin{cases} 0  \\text{if } \\|x\\|  1 \\\\ 2\\left(1 - \\frac{1}{\\|x\\|}\\right)x  \\text{if } \\|x\\|  1 \\end{cases} $$\nLet's check the limit of the gradient as $x \\to x_0$ where $\\|x_0\\|=1$.\nFrom the interior region ($\\|x\\|1$): $\\lim_{x\\to x_0, \\|x\\|1} \\nabla f(x) = \\lim_{x\\to x_0, \\|x\\|1} 0 = 0$.\nFrom the exterior region ($\\|x\\|1$): $\\lim_{x\\to x_0, \\|x\\|1} \\nabla f(x) = \\lim_{x\\to x_0, \\|x\\|1} 2\\left(1 - \\frac{1}{\\|x\\|}\\right)x = 2\\left(1 - \\frac{1}{\\|x_0\\|}\\right)x_0 = 2\\left(1-\\frac{1}{1}\\right)x_0 = 0$.\nSince the limits from both sides agree and are equal to the zero vector, we can define $\\nabla f(x_0) = 0$. The resulting gradient function $\\nabla f(x)$ is continuous for all $x \\neq 0$. The continuity of the partial derivatives implies that the function $f$ is continuously differentiable ($C^1$) for all $x \\neq 0$. This includes all points $x_0$ on the boundary $\\|x_0\\|=1$. Therefore, $f$ is differentiable at these points, and its gradient is $\\nabla f(x_0) = 0$.\n\nNow, we compute the directional derivative $D f(x_0; d)$ for an arbitrary direction $d \\in \\mathbb{R}^n$. Since $f$ is differentiable at $x_0$, the directional derivative is given by the dot product of the gradient and the direction vector:\n$$D f(x_0; d) = (\\nabla f(x_0))^{\\top}d$$\nSubstituting the gradient we found, $\\nabla f(x_0) = 0$:\n$$D f(x_0; d) = 0^{\\top}d = 0$$\nThis holds for any direction $d \\in \\mathbb{R}^n$.\n\nAs a verification, we can compute this directly from the limit definition:\n$D f(x_0; d) = \\lim_{t\\to 0} \\frac{f(x_0+td) - f(x_0)}{t} = \\lim_{t\\to 0} \\frac{\\max\\{0, \\|x_0+td\\|-1\\}^2}{t}$.\nWe use the expansion $\\|x_0+td\\| = \\sqrt{\\|x_0\\|^2 + 2t(x_0^\\top d) + t^2\\|d\\|^2} = \\sqrt{1 + 2t(x_0^\\top d) + t^2\\|d\\|^2}$.\nUsing the Taylor expansion $\\sqrt{1+u} = 1 + \\frac{1}{2}u + O(u^2)$ for small $u$, we have:\n$\\|x_0+td\\| = 1 + \\frac{1}{2}(2t(x_0^\\top d) + t^2\\|d\\|^2) + O(t^2) = 1 + t(x_0^\\top d) + O(t^2)$.\nSo, $\\|x_0+td\\|-1 = t(x_0^\\top d) + O(t^2)$.\nThe limit becomes:\n$D f(x_0; d) = \\lim_{t\\to 0} \\frac{\\max\\{0, t(x_0^\\top d) + O(t^2)\\}^2}{t}$.\nThe numerator is of order $O(t^2)$, as $\\max\\{0, \\dots\\}^2$ will be either $0$ or $(t(x_0^\\top d) + O(t^2))^2 = O(t^2)$.\nThe expression inside the limit is therefore $\\frac{O(t^2)}{t} = O(t)$.\nAs $t \\to 0$, $O(t) \\to 0$. This confirms that the directional derivative is $0$.\n\nThe final results are:\n(i) For $\\|x\\|1$, $\\nabla f(x) = 0$.\n(ii) For $\\|x\\|1$, $\\nabla f(x) = 2(\\|x\\|-1)\\frac{x}{\\|x\\|}$.\n(iii) At $\\|x_0\\|=1$, for any direction $d$, $D f(x_0; d) = 0$.", "answer": "$$\n\\boxed{\n\\begin{aligned}\n\\text{(i) }  \\nabla f(x) = 0 \\text{ for } \\|x\\|  1 \\\\\n\\text{(ii) }  \\nabla f(x) = 2(\\|x\\|-1)\\frac{x}{\\|x\\|} \\text{ for } \\|x\\| > 1 \\\\\n\\text{(iii) }  D f(x_0; d) = 0 \\text{ for } \\|x_0\\| = 1\n\\end{aligned}\n}\n$$", "id": "3120157"}]}