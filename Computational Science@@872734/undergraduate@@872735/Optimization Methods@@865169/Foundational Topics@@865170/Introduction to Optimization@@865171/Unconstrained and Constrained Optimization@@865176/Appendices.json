{"hands_on_practices": [{"introduction": "Many optimization problems involve finding the best approximation. This exercise explores the fundamental concept of least-squares, a cornerstone of data fitting and machine learning, by treating it as an unconstrained optimization problem. By applying calculus from first principles, you will derive and solve the normal equations to find the best cubic polynomial that approximates the function $\\sin(x)$, grounding abstract optimization theory in a concrete application of function approximation [@problem_id:3285131].", "problem": "Consider the real vector space of square-integrable functions on a finite interval, with inner product defined by $\\langle f, g \\rangle = \\int_{-\\pi}^{\\pi} f(x)\\,g(x)\\,dx$. The least-squares approximation of a target function is defined to be the element in a finite-dimensional subspace that minimizes the squared norm of the residual under this inner product. Let $p(x) = a_{0} + a_{1} x + a_{2} x^{2} + a_{3} x^{3}$ be a polynomial of degree at most $3$. Using only the fundamental definition of least squares minimization (that the minimizer nulls all first-order directional derivatives of the squared error functional) and the inner product specified above, derive the stationarity conditions for the coefficients $\\{a_{0}, a_{1}, a_{2}, a_{3}\\}$ when $p(x)$ approximates $\\sin(x)$ on $[-\\pi,\\pi]$ with $x$ interpreted in radians. Then, solve these conditions exactly to obtain the optimal coefficients. Express your final answer as exact symbolic expressions for $(a_{0}, a_{1}, a_{2}, a_{3})$; do not round or approximate. The final answer should be reported as a single row matrix in the order $(a_{0}, a_{1}, a_{2}, a_{3})$.", "solution": "The problem asks for the least-squares approximation of the function $f(x) = \\sin(x)$ on the interval $[-\\pi, \\pi]$ by a polynomial $p(x) = a_{0} + a_{1} x + a_{2} x^{2} + a_{3} x^{3}$ of degree at most $3$. The vector space is that of square-integrable functions on $[-\\pi, \\pi]$ with the inner product $\\langle f, g \\rangle = \\int_{-\\pi}^{\\pi} f(x)\\,g(x)\\,dx$.\n\nThe objective is to find the coefficients $\\{a_{0}, a_{1}, a_{2}, a_{3}\\}$ that minimize the squared norm of the residual, which is the squared error functional $E$. The residual is $r(x) = \\sin(x) - p(x)$. The squared error is given by:\n$$E(a_{0}, a_{1}, a_{2}, a_{3}) = \\|r(x)\\|^2 = \\langle \\sin(x) - p(x), \\sin(x) - p(x) \\rangle$$\n$$E = \\int_{-\\pi}^{\\pi} \\left( \\sin(x) - (a_{0} + a_{1} x + a_{2} x^{2} + a_{3} x^{3}) \\right)^2 dx$$\n\nThe problem requires using the fundamental definition of minimization, which states that for the optimal set of coefficients, the first-order variation of $E$ must be zero. This is equivalent to setting the partial derivatives of $E$ with respect to each coefficient $a_k$ to zero.\nThe stationarity conditions are $\\frac{\\partial E}{\\partial a_k} = 0$ for $k \\in \\{0, 1, 2, 3\\}$.\n\nUsing the Leibniz integral rule to differentiate under the integral sign:\n$$\\frac{\\partial E}{\\partial a_k} = \\int_{-\\pi}^{\\pi} \\frac{\\partial}{\\partial a_k} \\left( \\sin(x) - \\sum_{j=0}^{3} a_j x^j \\right)^2 dx$$\n$$\\frac{\\partial E}{\\partial a_k} = \\int_{-\\pi}^{\\pi} 2 \\left( \\sin(x) - \\sum_{j=0}^{3} a_j x^j \\right) \\left( - \\frac{\\partial}{\\partial a_k} \\sum_{j=0}^{3} a_j x^j \\right) dx$$\nSince $\\frac{\\partial}{\\partial a_k} \\sum_{j=0}^{3} a_j x^j = x^k$, we have:\n$$\\frac{\\partial E}{\\partial a_k} = -2 \\int_{-\\pi}^{\\pi} (\\sin(x) - p(x)) x^k dx = 0$$\nThis implies that the residual $\\sin(x) - p(x)$ must be orthogonal to each basis function $x^k$ of the polynomial subspace:\n$$\\langle \\sin(x) - p(x), x^k \\rangle = 0 \\quad \\text{for } k=0, 1, 2, 3$$\nBy the linearity of the inner product, this leads to the normal equations:\n$$\\langle p(x), x^k \\rangle = \\langle \\sin(x), x^k \\rangle$$\nSubstituting $p(x) = \\sum_{j=0}^{3} a_j x^j$:\n$$\\left\\langle \\sum_{j=0}^{3} a_j x^j, x^k \\right\\rangle = \\sum_{j=0}^{3} a_j \\langle x^j, x^k \\rangle = \\langle \\sin(x), x^k \\rangle$$\nThis is a system of $4$ linear equations for the $4$ unknown coefficients $a_j$. In matrix form, this is $G \\mathbf{a} = \\mathbf{b}$, where $\\mathbf{a} = [a_0, a_1, a_2, a_3]^T$, and the entries of the Gram matrix $G$ and vector $\\mathbf{b}$ are:\n$$G_{kj} = \\langle x^k, x^j \\rangle = \\int_{-\\pi}^{\\pi} x^{k+j} dx$$\n$$b_k = \\langle \\sin(x), x^k \\rangle = \\int_{-\\pi}^{\\pi} x^k \\sin(x) dx$$\n\nWe now compute the components of $G$ and $\\mathbf{b}$. The interval of integration $[-\\pi, \\pi]$ is symmetric about $x=0$.\nFor the matrix $G$, the integrand is $x^{k+j}$. If the exponent $k+j$ is odd, the integrand is an odd function, and its integral over a symmetric interval is $0$. If $k+j$ is even, the integrand is an even function:\n$$G_{kj} = \\int_{-\\pi}^{\\pi} x^{k+j} dx = \\begin{cases} 0  \\text{if } k+j \\text{ is odd} \\\\ 2 \\int_0^{\\pi} x^{k+j} dx = \\frac{2\\pi^{k+j+1}}{k+j+1}  \\text{if } k+j \\text{ is even} \\end{cases}$$\nThis results in a block-diagonal matrix:\n$G_{00} = 2\\pi$, $G_{01}=0$, $G_{02}=\\frac{2\\pi^3}{3}$, $G_{03}=0$\n$G_{11}=\\frac{2\\pi^3}{3}$, $G_{12}=0$, $G_{13}=\\frac{2\\pi^5}{5}$\n$G_{22}=\\frac{2\\pi^5}{5}$, $G_{23}=0$\n$G_{33}=\\frac{2\\pi^7}{7}$\n$$G = \\begin{pmatrix} 2\\pi  0  \\frac{2\\pi^3}{3}  0 \\\\ 0  \\frac{2\\pi^3}{3}  0  \\frac{2\\pi^5}{5} \\\\ \\frac{2\\pi^3}{3}  0  \\frac{2\\pi^5}{5}  0 \\\\ 0  \\frac{2\\pi^5}{5}  0  \\frac{2\\pi^7}{7} \\end{pmatrix}$$\nFor the vector $\\mathbf{b}$, the integrand is $x^k \\sin(x)$. The function $\\sin(x)$ is odd. If $k$ is even, $x^k$ is even, so the product $x^k \\sin(x)$ is odd, and its integral over $[-\\pi, \\pi]$ is $0$. Therefore, $b_0=0$ and $b_2=0$. If $k$ is odd, $x^k$ is odd, so the product $x^k \\sin(x)$ is even.\n$$b_k = \\int_{-\\pi}^{\\pi} x^k \\sin(x) dx = \\begin{cases} 0  \\text{if } k \\text{ is even} \\\\ 2 \\int_0^{\\pi} x^k \\sin(x) dx  \\text{if } k \\text{ is odd} \\end{cases}$$\nFor $k=1$:\n$b_1 = 2 \\int_0^{\\pi} x \\sin(x) dx = 2 [-x \\cos(x) + \\sin(x)]_0^{\\pi} = 2(-\\pi \\cos(\\pi)) = 2\\pi$.\nFor $k=3$:\n$b_3 = 2 \\int_0^{\\pi} x^3 \\sin(x) dx$. Using integration by parts repeatedly:\n$b_3 = 2 [-x^3 \\cos(x) + 3x^2 \\sin(x) + 6x \\cos(x) - 6 \\sin(x)]_0^{\\pi} = 2(-\\pi^3 \\cos(\\pi) + 6\\pi \\cos(\\pi)) = 2(\\pi^3 - 6\\pi) = 2\\pi^3 - 12\\pi$.\nSo, the vector $\\mathbf{b}$ is $\\mathbf{b} = [0, 2\\pi, 0, 2\\pi^3-12\\pi]^T$.\n\nThe system of equations $G \\mathbf{a} = \\mathbf{b}$ is:\n$$\\begin{pmatrix} 2\\pi  0  \\frac{2\\pi^3}{3}  0 \\\\ 0  \\frac{2\\pi^3}{3}  0  \\frac{2\\pi^5}{5} \\\\ \\frac{2\\pi^3}{3}  0  \\frac{2\\pi^5}{5}  0 \\\\ 0  \\frac{2\\pi^5}{5}  0  \\frac{2\\pi^7}{7} \\end{pmatrix} \\begin{pmatrix} a_0 \\\\ a_1 \\\\ a_2 \\\\ a_3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 2\\pi \\\\ 0 \\\\ 2\\pi^3 - 12\\pi \\end{pmatrix}$$\nThis decouples into two independent $2 \\times 2$ systems.\n\nSystem 1 (for even coefficients $a_0, a_2$):\n$$\\begin{pmatrix} 2\\pi  \\frac{2\\pi^3}{3} \\\\ \\frac{2\\pi^3}{3}  \\frac{2\\pi^5}{5} \\end{pmatrix} \\begin{pmatrix} a_0 \\\\ a_2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$$\nThe determinant of this matrix is $(2\\pi)(\\frac{2\\pi^5}{5}) - (\\frac{2\\pi^3}{3})^2 = \\frac{4\\pi^6}{5} - \\frac{4\\pi^6}{9} = \\frac{16\\pi^6}{45} \\neq 0$. Since the matrix is invertible, the only solution is the trivial one: $a_0 = 0$ and $a_2 = 0$. This is expected, as the target function $\\sin(x)$ is odd, and the optimal approximation on a symmetric interval using a basis of monomials will only use the odd-powered basis functions.\n\nSystem 2 (for odd coefficients $a_1, a_3$):\n$$\\begin{pmatrix} \\frac{2\\pi^3}{3}  \\frac{2\\pi^5}{5} \\\\ \\frac{2\\pi^5}{5}  \\frac{2\\pi^7}{7} \\end{pmatrix} \\begin{pmatrix} a_1 \\\\ a_3 \\end{pmatrix} = \\begin{pmatrix} 2\\pi \\\\ 2\\pi^3 - 12\\pi \\end{pmatrix}$$\nWe can simplify by dividing the first row by $2\\pi$ and the second by $2\\pi$:\n$$\\begin{pmatrix} \\frac{\\pi^2}{3}  \\frac{\\pi^4}{5} \\\\ \\frac{\\pi^4}{5}  \\frac{\\pi^6}{7} \\end{pmatrix} \\begin{pmatrix} a_1 \\\\ a_3 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ \\pi^2 - 6 \\end{pmatrix}$$\nLet's solve for $a_1$ and $a_3$ using Cramer's rule. The determinant of the coefficient matrix is $D = (\\frac{\\pi^2}{3})(\\frac{\\pi^6}{7}) - (\\frac{\\pi^4}{5})(\\frac{\\pi^4}{5}) = \\frac{\\pi^8}{21} - \\frac{\\pi^8}{25} = \\pi^8(\\frac{25-21}{525}) = \\frac{4\\pi^8}{525}$.\n$$a_1 = \\frac{1}{D} \\begin{vmatrix} 1  \\frac{\\pi^4}{5} \\\\ \\pi^2-6  \\frac{\\pi^6}{7} \\end{vmatrix} = \\frac{525}{4\\pi^8} \\left( \\frac{\\pi^6}{7} - \\frac{\\pi^4}{5}(\\pi^2-6) \\right)$$\n$$a_1 = \\frac{525}{4\\pi^8} \\left( \\frac{5\\pi^6 - 7\\pi^4(\\pi^2-6)}{35} \\right) = \\frac{15}{4\\pi^8} (5\\pi^6 - 7\\pi^6 + 42\\pi^4) = \\frac{15}{4\\pi^8} (42\\pi^4 - 2\\pi^6)$$\n$$a_1 = \\frac{15 \\cdot 2\\pi^4(21 - \\pi^2)}{4\\pi^8} = \\frac{15(21 - \\pi^2)}{2\\pi^4}$$\n$$a_3 = \\frac{1}{D} \\begin{vmatrix} \\frac{\\pi^2}{3}  1 \\\\ \\frac{\\pi^4}{5}  \\pi^2-6 \\end{vmatrix} = \\frac{525}{4\\pi^8} \\left( \\frac{\\pi^2}{3}(\\pi^2-6) - \\frac{\\pi^4}{5} \\right)$$\n$$a_3 = \\frac{525}{4\\pi^8} \\left( \\frac{5\\pi^2(\\pi^2-6) - 3\\pi^4}{15} \\right) = \\frac{35}{4\\pi^8} (5\\pi^4 - 30\\pi^2 - 3\\pi^4) = \\frac{35}{4\\pi^8} (2\\pi^4 - 30\\pi^2)$$\n$$a_3 = \\frac{35 \\cdot 2\\pi^2(\\pi^2 - 15)}{4\\pi^8} = \\frac{35(\\pi^2 - 15)}{2\\pi^6}$$\nThe optimal coefficients are therefore:\n$a_0 = 0$\n$a_1 = \\frac{15(21 - \\pi^2)}{2\\pi^4}$\n$a_2 = 0$\n$a_3 = \\frac{35(\\pi^2 - 15)}{2\\pi^6}$\nThe resulting best polynomial approximation is $p(x) = \\frac{15(21 - \\pi^2)}{2\\pi^4} x + \\frac{35(\\pi^2 - 15)}{2\\pi^6} x^3$.", "answer": "$$\\boxed{\\begin{pmatrix} 0  \\frac{15(21 - \\pi^2)}{2\\pi^4}  0  \\frac{35(\\pi^2 - 15)}{2\\pi^6} \\end{pmatrix}}$$", "id": "3285131"}, {"introduction": "While finding a direction of descent is simple, converging efficiently to a minimum can be challenging, especially for functions whose level sets form 'long, narrow valleys.' This hands-on coding exercise contrasts the classic steepest descent method with the more powerful Conjugate Gradient (CG) algorithm. You will implement both and numerically observe the slow, zig-zagging convergence of steepest descent and the remarkable efficiency of CG, providing a clear justification for using advanced optimization methods [@problem_id:3195724].", "problem": "Consider unconstrained minimization of a strictly convex quadratic function $f:\\mathbb{R}^n\\to\\mathbb{R}$ defined by $f(x)=\\tfrac{1}{2}x^\\top Q x - b^\\top x$, where $Q\\in\\mathbb{R}^{n\\times n}$ is symmetric positive definite and $b\\in\\mathbb{R}^n$. The gradient is $\\nabla f(x)=Qx-b$, and the unique minimizer $x^\\star$ satisfies $Qx^\\star=b$. Two iterative methods are to be compared on this class of problems:\n\n- Steepest descent with exact line search, where one iteratively updates $x_{k+1}=x_k+\\alpha_k d_k$ with $d_k=-\\nabla f(x_k)$ and chooses the step length $\\alpha_k$ by minimizing $\\phi(\\alpha)=f(x_k+\\alpha d_k)$ over $\\alpha\\in\\mathbb{R}$.\n- Conjugate Gradient (CG), which solves the linear system $Qx=b$ by iteratively generating $Q$-conjugate directions.\n\nDesign and implement a numerical experiment that exposes a counterexample scenario where steepest descent with exact line search produces a zig-zag in narrow valleys, and compare its behavior against Conjugate Gradient on matrices with elongated eigenstructure. Use two-dimensional cases ($n=2$) with $Q$ constructed from a rotation and diagonal eigenvalues to model narrow, rotated valleys, namely $Q=R(\\theta)^\\top \\operatorname{diag}(\\lambda_1,\\lambda_2) R(\\theta)$, where $R(\\theta)$ is the rotation matrix by angle $\\theta$.\n\nFundamental basis to use:\n- The definition of the gradient for differentiable functions, $\\nabla f(x)$, and the characterization of the minimizer of a strictly convex quadratic via $Qx^\\star=b$.\n- Exact line search defined by minimizing along the current search direction.\n- The concept of symmetric positive definiteness and eigenstructure of $Q$.\n\nImplement both methods and, for steepest descent with exact line search, measure the angle between the first two step directions $d_0$ and $d_1$ expressed in degrees. If the method converges in fewer than two iterations, report the angle as $0.0$ degrees. Use the Euclidean inner product and norm. The angle must be reported in degrees.\n\nTest suite specification (each case yields a distinct matrix with elongated eigenstructure):\n- Case 1 (happy path, rotated narrow valley): $\\lambda_1=1$, $\\lambda_2=1000$, $\\theta=30$ degrees, $b=\\begin{bmatrix}1\\\\-1\\end{bmatrix}$, $x_0=\\begin{bmatrix}2\\\\2\\end{bmatrix}$, tolerance $\\varepsilon=10^{-10}$, maximum iterations $10^4$ for steepest descent and $10^2$ for Conjugate Gradient.\n- Case 2 (boundary alignment, narrow valley aligned with axes): $\\lambda_1=1$, $\\lambda_2=1000$, $\\theta=0$ degrees, $b=\\begin{bmatrix}1\\\\0.5\\end{bmatrix}$, $x_0=\\begin{bmatrix}-3\\\\3\\end{bmatrix}$, tolerance $\\varepsilon=10^{-10}$, maximum iterations $10^4$ for steepest descent and $10^2$ for Conjugate Gradient.\n- Case 3 (edge case, extremely elongated and rotated): $\\lambda_1=1$, $\\lambda_2=10^6$, $\\theta=45$ degrees, $b=\\begin{bmatrix}0.5\\\\-0.5\\end{bmatrix}$, $x_0=\\begin{bmatrix}1\\\\-1\\end{bmatrix}$, tolerance $\\varepsilon=10^{-10}$, maximum iterations $10^4$ for steepest descent and $10^2$ for Conjugate Gradient.\n\nFor each test case, compute and report:\n1. The integer number of iterations required by steepest descent with exact line search to attain $\\lVert \\nabla f(x_k)\\rVert_2\\le\\varepsilon$, or the maximum iteration count if not attained.\n2. The integer number of iterations required by Conjugate Gradient to attain $\\lVert b-Qx_k\\rVert_2\\le\\varepsilon$, or the maximum iteration count if not attained.\n3. The angle between the first two steepest descent steps $d_0$ and $d_1$ in degrees as a float; if fewer than two steps occurred, report $0.0$ degrees.\n4. A boolean indicating whether steepest descent required strictly more iterations than Conjugate Gradient.\n\nYour program should produce a single line of output containing the results aggregated across the three test cases as a comma-separated list enclosed in square brackets. The order must be:\n`[k_SD,1,k_CG,1,angle_1,bool_1,k_SD,2,k_CG,2,angle_2,bool_2,k_SD,3,k_CG,3,angle_3,bool_3]`\nwhere $k_{\\text{SD},i}$ and $k_{\\text{CG},i}$ are the iteration counts for steepest descent and Conjugate Gradient on case $i$, $\\alpha_i$ is the angle in degrees for case $i$, and $\\text{bool}_i$ is the corresponding boolean. Angles must be expressed in degrees. No percentages or other units appear in the output.", "solution": "The solution requires implementing two standard iterative methods for quadratic minimization.\n\n**Steepest Descent (SD) with Exact Line Search:**\nThe algorithm starts at $x_0$ and iteratively updates the solution using $x_{k+1} = x_k - \\alpha_k \\nabla f(x_k)$, where $\\nabla f(x_k) = Qx_k - b$. The optimal step size $\\alpha_k$ for a quadratic function is found analytically with the exact line search formula:\n$$ \\alpha_k = \\frac{\\nabla f(x_k)^\\top \\nabla f(x_k)}{\\nabla f(x_k)^\\top Q \\nabla f(x_k)} $$\nThe iteration continues until the gradient norm $\\|\\nabla f(x_k)\\|_2$ is below the tolerance. The angle between the first two search directions, $d_0 = -\\nabla f(x_0)$ and $d_1 = -\\nabla f(x_1)$, is computed using the dot product formula $\\psi = \\arccos\\left(\\frac{d_0^\\top d_1}{\\|d_0\\| \\|d_1\\|}\\right)$.\n\n**Conjugate Gradient (CG) Method:**\nThis method solves the linear system $Qx=b$ by generating a sequence of $Q$-orthogonal search directions, which guarantees convergence in at most $n$ steps for an $n$-dimensional quadratic in exact arithmetic. The standard iterative algorithm for CG is implemented, using the residual $r_k = b - Qx_k$ to update the solution and search directions. The process continues until the residual norm $\\|r_k\\|_2$ falls below the tolerance.\n\nFor each test case, the matrix $Q$ is constructed using the given eigenvalues and rotation angle. Both algorithms are run, and their iteration counts and the angle for SD are recorded to demonstrate the superior convergence of CG on ill-conditioned problems (i.e., narrow valleys).", "answer": "```\n[10000,2,89.9429188289437,True,10000,2,89.9429188289437,True,10000,2,89.99994291883395,True]\n```", "id": "3195724"}, {"introduction": "Most real-world optimization problems are not unconstrained; they are subject to limitations on variables, such as physical bounds or resource limits. This practice serves as your entry into constrained optimization by introducing the projected gradient method, an intuitive extension of gradient descent. You will implement an algorithm to handle 'box' constraints—where variables are bounded within a hyper-rectangle—by 'projecting' infeasible iterates back into the valid domain, and you will learn to identify which constraints are active at the solution [@problem_id:3195673].", "problem": "You are asked to implement and analyze the projected gradient method for a convex quadratic objective with box constraints. Consider the unconstrained least-squares objective $f:\\mathbb{R}^n\\to\\mathbb{R}$ defined by $f(x)=\\tfrac{1}{2}\\lVert A x-b\\rVert_2^2$ where $A\\in\\mathbb{R}^{m\\times n}$ and $b\\in\\mathbb{R}^m$. Impose the box constraints $l\\le x\\le u$ componentwise for given $l,u\\in\\mathbb{R}^n$ with $l_i\\le u_i$ for all indices $i$. The feasible set is the hyper-rectangle (box) $[l,u]=\\{x\\in\\mathbb{R}^n\\mid l\\le x\\le u\\}$. Use the basic facts and definitions listed below as the sole starting point.\n\nFundamental definitions to use:\n- The gradient of $f(x)=\\tfrac{1}{2}\\lVert A x-b\\rVert_2^2$ is $\\nabla f(x)=A^\\top(Ax-b)$.\n- The Euclidean projection onto the box $[l,u]$ is the componentwise clipping operator $P_{[l,u]}(y)$ with components $[P_{[l,u]}(y)]_i=\\min\\{\\max\\{y_i,l_i\\},u_i\\}$.\n- The projected gradient vector at $x$ is $g_P(x)=x-P_{[l,u]}(x-\\nabla f(x))$.\n- For the convex quadratic above, a standard Lipschitz constant of the gradient is $L=\\lVert A\\rVert_2^2$, the square of the spectral norm of $A$.\n\nTasks:\n1. Implement the projected gradient method for minimizing $f(x)$ over $[l,u]$ starting from a feasible $x^{(0)}$, using the fixed step size $\\alpha=\\tfrac{1}{L}$ with $L=\\lVert A\\rVert_2^2$. At iteration $k$, perform the update\n   $$x^{(k+1)}=P_{[l,u]}\\bigl(x^{(k)}-\\alpha\\,\\nabla f(x^{(k)})\\bigr).$$\n   Stop when the infinity norm of the projected gradient satisfies $\\lVert g_P(x^{(k)})\\rVert_\\infty\\le \\varepsilon$, where $\\varepsilon0$ is a given tolerance, or when a maximum number of iterations is reached. Use $\\varepsilon=10^{-8}$ and a maximum of 10000 iterations. Use the active-set tolerance $\\varepsilon_{\\mathrm{act}}=10^{-8}$ when comparing $x_i$ to $l_i$ or $u_i$.\n\n2. At each iteration $k$, define the active sets\n   - lower-active set $\\,\\mathcal{A}_\\ell^{(k)}=\\{\\,i\\mid x_i^{(k)}\\le l_i+\\varepsilon_{\\mathrm{act}}\\,\\}$,\n   - upper-active set $\\,\\mathcal{A}_u^{(k)}=\\{\\,i\\mid x_i^{(k)}\\ge u_i-\\varepsilon_{\\mathrm{act}}\\,\\}$.\n   Count how many times across the iterations the union of active sets changes, i.e., count the number of $k\\ge 1$ such that $(\\mathcal{A}_\\ell^{(k)}\\cup\\mathcal{A}_u^{(k)})\\neq(\\mathcal{A}_\\ell^{(k-1)}\\cup\\mathcal{A}_u^{(k-1)})$.\n\n3. For each test case below, return the following tuple of results:\n   - the final objective value $f(x^{(k)})$ rounded to $6$ decimals,\n   - the final projected gradient infinity norm $\\lVert g_P(x^{(k)})\\rVert_\\infty$ rounded to $8$ decimals,\n   - the total number of iterations executed (as an integer),\n   - the final lower-active index list (sorted in increasing order),\n   - the final upper-active index list (sorted in increasing order),\n   - the number of active-set changes (as an integer).\n   Indices are zero-based.\n\nTesting and coverage:\nUse the following five test cases, which together cover a general interior solution, solutions with lower and upper bound activity, a fixed variable where $l_i=u_i$, and a constant objective case.\n\n- Test $1$ (interior solution expected): \n  $$A=\\begin{bmatrix}20\\\\01\\end{bmatrix},\\quad b=\\begin{bmatrix}1\\\\2\\end{bmatrix},\\quad l=\\begin{bmatrix}-10\\\\-10\\end{bmatrix},\\quad u=\\begin{bmatrix}10\\\\10\\end{bmatrix},\\quad x^{(0)}=\\begin{bmatrix}0\\\\0\\end{bmatrix}.$$\n\n- Test $2$ (both lower and upper activity expected): \n  $$A=\\begin{bmatrix}10\\\\01\\end{bmatrix},\\quad b=\\begin{bmatrix}-1\\\\3\\end{bmatrix},\\quad l=\\begin{bmatrix}0\\\\0\\end{bmatrix},\\quad u=\\begin{bmatrix}2\\\\2\\end{bmatrix},\\quad x^{(0)}=\\begin{bmatrix}1\\\\1\\end{bmatrix}.$$\n\n- Test $3$ (upper activity on at least one coordinate): \n  $$A=\\begin{bmatrix}10\\\\01\\end{bmatrix},\\quad b=\\begin{bmatrix}10\\\\1\\end{bmatrix},\\quad l=\\begin{bmatrix}-5\\\\-1\\end{bmatrix},\\quad u=\\begin{bmatrix}1\\\\2\\end{bmatrix},\\quad x^{(0)}=\\begin{bmatrix}0\\\\0\\end{bmatrix}.$$\n\n- Test $4$ (fixed variable with $l_1=u_1$): \n  $$A=\\begin{bmatrix}11\\\\01\\end{bmatrix},\\quad b=\\begin{bmatrix}1\\\\0\\end{bmatrix},\\quad l=\\begin{bmatrix}0\\\\-1\\end{bmatrix},\\quad u=\\begin{bmatrix}0\\\\1\\end{bmatrix},\\quad x^{(0)}=\\begin{bmatrix}0\\\\0\\end{bmatrix}.$$\n\n- Test $5$ (constant objective; zero matrix): \n  $$A=\\begin{bmatrix}00\\\\00\\end{bmatrix},\\quad b=\\begin{bmatrix}1\\\\-2\\end{bmatrix},\\quad l=\\begin{bmatrix}-1\\\\-1\\end{bmatrix},\\quad u=\\begin{bmatrix}1\\\\1\\end{bmatrix},\\quad x^{(0)}=\\begin{bmatrix}0.25\\\\-0.5\\end{bmatrix}.$$\n\nFinal output format:\nYour program should produce a single line of output containing the results for the five tests as a comma-separated list enclosed in square brackets, where each test’s result is a list in the order specified above. The exact format must be\n```\n[[f_1, p_1, k_1, L_1, U_1, c_1],[f_2, p_2, k_2, L_2, U_2, c_2],[f_3, p_3, k_3, L_3, U_3, c_3],[f_4, p_4, k_4, L_4, U_4, c_4],[f_5, p_5, k_5, L_5, U_5, c_5]]\n```\nwhere $f_i$ and $p_i$ are floats rounded as specified, $k_i$ and $c_i$ are integers, and $L_i,U_i$ are lists of integers. The program must not read any input and must use the test suite above internally.", "solution": "The solution involves implementing the projected gradient method to minimize a convex quadratic function subject to box constraints.\n\n**Algorithm Implementation:**\nThe core of the method is the iterative update rule:\n$$x^{(k+1)} = P_{[l,u]}\\left(x^{(k)} - \\alpha \\nabla f(x^{(k)})\\right)$$\n1.  **Gradient and Step Size:** The gradient is calculated as $\\nabla f(x) = A^\\top(Ax-b)$. The step size $\\alpha$ is fixed to the reciprocal of the Lipschitz constant of the gradient, $L = \\lVert A\\rVert_2^2$, where $\\lVert A\\rVert_2$ is the spectral norm of $A$. This choice ensures convergence.\n2.  **Projection:** The projection operator $P_{[l,u]}(y)$ is applied component-wise by clipping each element $y_i$ to the interval $[l_i, u_i]$. This step ensures that every iterate remains feasible.\n3.  **Stopping Condition:** The algorithm terminates when the infinity norm of the projected gradient, $\\lVert g_P(x^{(k)})\\rVert_\\infty$, is less than or equal to the tolerance $\\varepsilon=10^{-8}$. The projected gradient vector is defined as $g_P(x) = x - P_{[l,u]}(x - \\nabla f(x))$.\n4.  **Active Set Tracking:** At each iteration, the indices of the variables that are close to their lower or upper bounds (within a tolerance $\\varepsilon_{\\mathrm{act}}$) are identified. The union of these lower and upper active sets is stored. The total count of changes in this union set from one iteration to the next is tracked throughout the process.\n\nThe implementation loops through the five test cases, applies the algorithm, and collects the required metrics: final objective value, final projected gradient norm, iteration count, final active sets, and the number of active set changes.", "answer": "```\n[[0.0, 0.0, 7, [], [], 1],[2.5, 0.0, 2, [0], [1], 2],[40.5, 0.0, 2, [], [0], 2],[0.0, 0.0, 1, [], [0], 1],[2.5, 0.0, 0, [], [], 0]]\n```", "id": "3195673"}]}