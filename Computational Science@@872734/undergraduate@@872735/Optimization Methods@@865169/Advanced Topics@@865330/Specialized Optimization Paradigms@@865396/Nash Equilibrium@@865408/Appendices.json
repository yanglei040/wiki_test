{"hands_on_practices": [{"introduction": "A Nash Equilibrium represents a state of stability where no individual player benefits from changing their strategy alone. But is this \"selfish\" equilibrium the best outcome for the group as a whole? This exercise provides a hands-on way to explore this fundamental question by comparing the Nash Equilibrium of a simple two-player game with the centralized, or \"socially optimal,\" solution that minimizes the total cost for both players. By working through the calculations [@problem_id:3154673], you will directly quantify the inefficiency that can arise from decentralized decision-making.", "problem": "Consider a two-player linear-quadratic game in which player $i \\in \\{1,2\\}$ chooses a scalar decision variable $x_i \\in \\mathbb{R}$. The cost functions are given by\n$$\nf_i(x_1,x_2) \\;=\\; \\frac{1}{2} \\, q_i \\, x_i^{2} \\;+\\; b_i \\, x_i \\, x_{-i} \\;+\\; c_i \\, x_i,\n$$\nwhere $x_{-i}$ denotes the decision of the other player. Let $q_1 = 2$, $q_2 = 4$, $b_1 = 1$, $b_2 = 1$, $c_1 = -5$, and $c_2 = 1$. Assume $q_i > 0$ for each player so that each player’s optimization problem is strictly convex in their own decision variable.\n\nStarting from the definition of Nash equilibrium (NE), which states that no player can reduce their own cost by a unilateral deviation, and using first-order optimality conditions for differentiable convex functions, derive the block linear system that characterizes the NE and solve for the equilibrium $(x_1^{\\ast}, x_2^{\\ast})$. Then form the centralized optimization problem that minimizes the social cost\n$$\nF(x_1,x_2) \\;=\\; f_1(x_1,x_2) \\;+\\; f_2(x_1,x_2),\n$$\nderive its first-order optimality conditions, and solve for the centralized optimizer $(x_1^{c}, x_2^{c})$.\n\nProvide your final result as the two solutions concatenated into a single row vector $\\big(x_1^{\\ast}, \\, x_2^{\\ast}, \\, x_1^{c}, \\, x_2^{c}\\big)$. No rounding is required.", "solution": "The problem requires finding two distinct solutions for a two-player linear-quadratic game: the Nash equilibrium (NE) and the centralized (socially optimal) solution.\n\nFirst, we determine the Nash Equilibrium, denoted by $(x_1^{\\ast}, x_2^{\\ast})$. A Nash equilibrium is a state where neither player can improve their outcome (i.e., lower their cost) by unilaterally changing their own decision, assuming the other player's decision remains fixed. For player $i \\in \\{1,2\\}$ with a cost function $f_i(x_1, x_2)$ that is differentiable and convex in their own decision variable $x_i$, this condition is mathematically expressed by the first-order optimality condition, $\\frac{\\partial f_i}{\\partial x_i} = 0$.\n\nThe cost function for player $i$ is given as:\n$$\nf_i(x_1,x_2) = \\frac{1}{2} q_i x_i^{2} + b_i x_i x_{-i} + c_i x_i\n$$\nwhere $x_{-i}$ is the decision of the other player. The problem states that $q_i  0$, ensuring that $f_i$ is strictly convex with respect to $x_i$, which guarantees that the first-order condition identifies a unique minimum for player $i$'s optimization problem.\n\nLet's compute the partial derivatives for each player.\nFor player $1$:\n$$\nf_1(x_1,x_2) = \\frac{1}{2} q_1 x_1^{2} + b_1 x_1 x_2 + c_1 x_1\n$$\nThe first-order condition is:\n$$\n\\frac{\\partial f_1}{\\partial x_1} = q_1 x_1 + b_1 x_2 + c_1 = 0\n$$\n\nFor player $2$:\n$$\nf_2(x_1,x_2) = \\frac{1}{2} q_2 x_2^{2} + b_2 x_2 x_1 + c_2 x_2\n$$\nThe first-order condition is:\n$$\n\\frac{\\partial f_2}{\\partial x_2} = q_2 x_2 + b_2 x_1 + c_2 = 0\n$$\n\nThese two equations form a linear system for the Nash equilibrium $(x_1^{\\ast}, x_2^{\\ast})$:\n\\begin{align*}\nq_1 x_1^{\\ast} + b_1 x_2^{\\ast} = -c_1 \\\\\nb_2 x_1^{\\ast} + q_2 x_2^{\\ast} = -c_2\n\\end{align*}\nIn matrix form, this is:\n$$\n\\begin{pmatrix} q_1  b_1 \\\\ b_2  q_2 \\end{pmatrix} \\begin{pmatrix} x_1^{\\ast} \\\\ x_2^{\\ast} \\end{pmatrix} = \\begin{pmatrix} -c_1 \\\\ -c_2 \\end{pmatrix}\n$$\nWe are given the parameter values: $q_1 = 2$, $q_2 = 4$, $b_1 = 1$, $b_2 = 1$, $c_1 = -5$, and $c_2 = 1$. Substituting these values into the system gives:\n\\begin{align*}\n2 x_1^{\\ast} + 1 x_2^{\\ast} = -(-5) = 5 \\\\\n1 x_1^{\\ast} + 4 x_2^{\\ast} = -1\n\\end{align*}\nFrom the first equation, we can express $x_2^{\\ast}$ in terms of $x_1^{\\ast}$:\n$$\nx_2^{\\ast} = 5 - 2 x_1^{\\ast}\n$$\nSubstituting this into the second equation:\n$$\nx_1^{\\ast} + 4(5 - 2 x_1^{\\ast}) = -1\n$$\n$$\nx_1^{\\ast} + 20 - 8 x_1^{\\ast} = -1\n$$\n$$\n-7 x_1^{\\ast} = -21\n$$\n$$\nx_1^{\\ast} = 3\n$$\nNow, we find $x_2^{\\ast}$ by substituting the value of $x_1^{\\ast}$ back:\n$$\nx_2^{\\ast} = 5 - 2(3) = 5 - 6 = -1\n$$\nThus, the Nash equilibrium is $(x_1^{\\ast}, x_2^{\\ast}) = (3, -1)$.\n\nNext, we determine the centralized optimizer, denoted by $(x_1^{c}, x_2^{c})$. This is the pair of decisions that minimizes the total or social cost, $F(x_1,x_2) = f_1(x_1,x_2) + f_2(x_1,x_2)$.\nLet's write out the social cost function $F(x_1,x_2)$:\n$$\nF(x_1,x_2) = \\left(\\frac{1}{2} q_1 x_1^{2} + b_1 x_1 x_2 + c_1 x_1\\right) + \\left(\\frac{1}{2} q_2 x_2^{2} + b_2 x_1 x_2 + c_2 x_2\\right)\n$$\n$$\nF(x_1,x_2) = \\frac{1}{2} q_1 x_1^{2} + \\frac{1}{2} q_2 x_2^{2} + (b_1 + b_2) x_1 x_2 + c_1 x_1 + c_2 x_2\n$$\nThis is a joint optimization problem in the variables $(x_1, x_2)$. To find the minimum, we compute the gradient of $F$ and set it to zero. The first-order conditions are $\\frac{\\partial F}{\\partial x_1} = 0$ and $\\frac{\\partial F}{\\partial x_2} = 0$.\n$$\n\\frac{\\partial F}{\\partial x_1} = q_1 x_1 + (b_1 + b_2) x_2 + c_1 = 0\n$$\n$$\n\\frac{\\partial F}{\\partial x_2} = q_2 x_2 + (b_1 + b_2) x_1 + c_2 = 0\n$$\nThis gives us a system of linear equations for the centralized optimizer $(x_1^{c}, x_2^{c})$:\n\\begin{align*}\nq_1 x_1^{c} + (b_1 + b_2) x_2^{c} = -c_1 \\\\\n(b_1 + b_2) x_1^{c} + q_2 x_2^{c} = -c_2\n\\end{align*}\nTo ensure this is a minimum, we can check the Hessian matrix of $F$, which is $H_F = \\begin{pmatrix} q_1  b_1+b_2 \\\\ b_1+b_2  q_2 \\end{pmatrix}$. With the given values, $H_F = \\begin{pmatrix} 2  2 \\\\ 2  4 \\end{pmatrix}$. The principal minors are $2  0$ and $\\det(H_F) = 2(4) - 2(2) = 4  0$. Since the Hessian is positive definite, $F$ is strictly convex, and the solution to the first-order conditions is the unique global minimum.\n\nSubstituting the parameter values into the system for the centralized solution:\n\\begin{align*}\n2 x_1^{c} + (1+1) x_2^{c} = -(-5) \\implies 2 x_1^{c} + 2 x_2^{c} = 5 \\\\\n(1+1) x_1^{c} + 4 x_2^{c} = -1 \\implies 2 x_1^{c} + 4 x_2^{c} = -1\n\\end{align*}\nWe now solve this system. Subtracting the first equation from the second gives:\n$$\n(2 x_1^{c} + 4 x_2^{c}) - (2 x_1^{c} + 2 x_2^{c}) = -1 - 5\n$$\n$$\n2 x_2^{c} = -6\n$$\n$$\nx_2^{c} = -3\n$$\nSubstituting $x_2^{c} = -3$ into the first equation:\n$$\n2 x_1^{c} + 2(-3) = 5\n$$\n$$\n2 x_1^{c} - 6 = 5\n$$\n$$\n2 x_1^{c} = 11\n$$\n$$\nx_1^{c} = \\frac{11}{2}\n$$\nThus, the centralized optimal solution is $(x_1^{c}, x_2^{c}) = (\\frac{11}{2}, -3)$.\n\nThe problem asks for the final result as the concatenated row vector $\\big(x_1^{\\ast}, x_2^{\\ast}, x_1^{c}, x_2^{c}\\big)$.\nThe calculated values are $x_1^{\\ast} = 3$, $x_2^{\\ast} = -1$, $x_1^{c} = \\frac{11}{2}$, and $x_2^{c} = -3$.\nThe final vector is $\\big(3, -1, \\frac{11}{2}, -3\\big)$.", "answer": "$$\n\\boxed{\\begin{pmatrix} 3  -1  \\frac{11}{2}  -3 \\end{pmatrix}}\n$$", "id": "3154673"}, {"introduction": "In many strategic interactions, the timing of decisions is crucial. This practice explores that dynamic by contrasting two classic models of duopoly: the simultaneous-move Cournot game and the sequential-move Stackelberg game [@problem_id:3154609]. By solving for the equilibrium in each scenario, you will discover the \"first-mover advantage\" and see how a leader can leverage their position by anticipating the follower's response. This exercise sharpens your skills in applying optimization and backward induction to analyze strategic environments.", "problem": "Consider a duopoly quantity-competition model with a homogeneous good. Market inverse demand is given by $p(Q) = a - bQ$ with $a = 120$ and $b = 2$, where $Q = q_{1} + q_{2}$ is total output and $q_{i} \\ge 0$ is firm $i$’s quantity for $i \\in \\{1,2\\}$. Each firm has constant marginal cost $c = 20$ and zero fixed cost, so firm $i$’s profit is $\\pi_{i}(q_{i}, q_{j}) = \\big(p(q_{1}+q_{2}) - c\\big) q_{i}$.\n\nYou will analyze two timing protocols using optimization from first principles:\n\n- Stackelberg (two-stage): Firm $1$ (leader) chooses $q_{1}$ in stage $1$. Then Firm $2$ (follower) observes $q_{1}$ and chooses $q_{2}$ in stage $2$.\n- Cournot (simultaneous-move): Firms $1$ and $2$ choose $q_{1}$ and $q_{2}$ simultaneously.\n\nUse the definition of Nash equilibrium (NE) as mutual best responses and solve each firm’s problem by taking the appropriate derivative and applying the First-Order Condition (FOC), verifying optimality via concavity where necessary. Proceed by backward induction for the Stackelberg case to derive the follower’s best response and the leader’s optimal choice, then compute the resulting equilibrium outputs and Firm $1$’s profit. Next, derive the simultaneous-move NE quantities and Firm $1$’s profit in the Cournot game by solving the system of best responses.\n\nFinally, compute the exact profit gain to Firm $1$ from being the Stackelberg leader relative to the simultaneous-move Nash equilibrium, defined as\n$\\Delta \\pi_{1} = \\pi_{1}^{\\text{Stackelberg}} - \\pi_{1}^{\\text{Cournot}}$.\nGive your final answer as an exact rational number (no rounding). Do not include any units in your final answer.", "solution": "The problem is first validated to ensure it is self-contained, scientifically grounded, well-posed, and objective.\n\n**Step 1: Extract Givens**\n- Market inverse demand: $p(Q) = a - bQ$\n- Parameter $a$: $a = 120$\n- Parameter $b$: $b = 2$\n- Total output: $Q = q_{1} + q_{2}$\n- Firm quantities: $q_{i} \\ge 0$ for $i \\in \\{1,2\\}$\n- Marginal cost: $c = 20$ for both firms\n- Fixed cost: $0$ for both firms\n- Profit function for firm $i$: $\\pi_{i}(q_{i}, q_{j}) = \\big(p(q_{1}+q_{2}) - c\\big) q_{i}$\n- Two models to analyze: Stackelberg (Firm 1 leader, Firm 2 follower) and Cournot (simultaneous move).\n- Objective: Compute the profit gain for Firm 1, $\\Delta \\pi_{1} = \\pi_{1}^{\\text{Stackelberg}} - \\pi_{1}^{\\text{Cournot}}$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem uses standard, well-established models of duopoly from microeconomic theory (Cournot and Stackelberg competition). The functional forms are canonical. No scientific principles are violated.\n- **Well-Posed:** The profit functions are quadratic and concave with respect to each firm's own quantity, ensuring that the first-order conditions for profit maximization yield unique maxima. The resulting equilibrium quantities are well-defined and unique for both models.\n- **Objective:** The problem is stated using precise mathematical definitions and objective language.\n- **Conclusion:** The problem is valid as it is a standard exercise in microeconomic optimization, free of any listed flaws.\n\n**Step 3: Proceed to Solution**\n\nFirst, we write the specific profit function for firm $i$ by substituting the given parameters. The price is $p(q_1+q_2) = 120 - 2(q_1+q_2)$. The profit for firm $i$ is:\n$$ \\pi_{i}(q_1, q_2) = (120 - 2(q_1+q_2) - 20)q_i = (100 - 2q_1 - 2q_2)q_i $$\n\n**Stackelberg Model Analysis**\n\nWe solve using backward induction, starting with the follower's problem in stage 2.\n\n**Stage 2: Follower's (Firm 2) Problem**\nFirm 2 observes $q_1$ as given and chooses $q_2$ to maximize its profit $\\pi_2$.\n$$ \\pi_2(q_1, q_2) = (100 - 2q_1 - 2q_2)q_2 = 100q_2 - 2q_1q_2 - 2q_2^2 $$\nTo find the maximum, we take the derivative of $\\pi_2$ with respect to $q_2$ and set it to zero (First-Order Condition, FOC):\n$$ \\frac{\\partial \\pi_2}{\\partial q_2} = 100 - 2q_1 - 4q_2 = 0 $$\nSolving for $q_2$ gives Firm 2's best response function, $R_2(q_1)$:\n$$ 4q_2 = 100 - 2q_1 \\implies q_2 = R_2(q_1) = 25 - \\frac{1}{2}q_1 $$\nThe second-order condition (SOC) confirms this is a maximum, as the profit function is concave in $q_2$: $\\frac{\\partial^2 \\pi_2}{\\partial q_2^2} = -4  0$.\n\n**Stage 1: Leader's (Firm 1) Problem**\nFirm 1 anticipates Firm 2's response $R_2(q_1)$ and chooses $q_1$ to maximize its own profit, $\\pi_1$. We substitute $R_2(q_1)$ into the expression for $\\pi_1$:\n$$ \\pi_1(q_1) = \\left(100 - 2q_1 - 2\\left(25 - \\frac{1}{2}q_1\\right)\\right)q_1 $$\n$$ \\pi_1(q_1) = (100 - 2q_1 - 50 + q_1)q_1 $$\n$$ \\pi_1(q_1) = (50 - q_1)q_1 = 50q_1 - q_1^2 $$\nTo find the optimal $q_1$, we apply the FOC:\n$$ \\frac{d\\pi_1}{dq_1} = 50 - 2q_1 = 0 \\implies q_1^{\\text{Stackelberg}} = 25 $$\nThe SOC is satisfied: $\\frac{d^2\\pi_1}{dq_1^2} = -2  0$.\n\nNow we find the follower's quantity by substituting the leader's quantity into the best response function:\n$$ q_2^{\\text{Stackelberg}} = 25 - \\frac{1}{2}(25) = 12.5 $$\nThe total quantity is $Q^{\\text{Stackelberg}} = 25 + 12.5 = 37.5$.\nThe market price is $p^{\\text{Stackelberg}} = 120 - 2(37.5) = 120 - 75 = 45$.\nFirm 1's profit in the Stackelberg equilibrium is:\n$$ \\pi_1^{\\text{Stackelberg}} = (p^{\\text{Stackelberg}} - c)q_1^{\\text{Stackelberg}} = (45 - 20)(25) = 25 \\times 25 = 625 $$\n\n**Cournot Model Analysis**\n\nIn the Cournot model, both firms choose their quantities simultaneously. The Nash Equilibrium is a pair of quantities $(q_1, q_2)$ where each firm's quantity is a best response to the other's.\n\n**Firm 1's Best Response**\nFirm 1 chooses $q_1$ to maximize $\\pi_1(q_1, q_2) = 100q_1 - 2q_1^2 - 2q_1q_2$, taking $q_2$ as given.\nFOC for Firm 1:\n$$ \\frac{\\partial \\pi_1}{\\partial q_1} = 100 - 4q_1 - 2q_2 = 0 $$\nSolving for $q_1$ gives Firm 1's best response function, $R_1(q_2)$:\n$$ 4q_1 = 100 - 2q_2 \\implies q_1 = R_1(q_2) = 25 - \\frac{1}{2}q_2 $$\n\n**Firm 2's Best Response**\nBy symmetry, Firm 2's best response function, $R_2(q_1)$, is identical in form:\n$$ q_2 = R_2(q_1) = 25 - \\frac{1}{2}q_1 $$\n\n**Cournot-Nash Equilibrium**\nThe equilibrium is found by solving the system of the two best response functions:\n$$ q_1 = 25 - \\frac{1}{2}q_2 $$\n$$ q_2 = 25 - \\frac{1}{2}q_1 $$\nSubstitute the second equation into the first:\n$$ q_1 = 25 - \\frac{1}{2}\\left(25 - \\frac{1}{2}q_1\\right) = 25 - \\frac{25}{2} + \\frac{1}{4}q_1 $$\n$$ q_1 - \\frac{1}{4}q_1 = \\frac{25}{2} $$\n$$ \\frac{3}{4}q_1 = \\frac{25}{2} \\implies q_1^{\\text{Cournot}} = \\frac{25}{2} \\times \\frac{4}{3} = \\frac{50}{3} $$\nDue to symmetry, $q_2^{\\text{Cournot}} = q_1^{\\text{Cournot}} = \\frac{50}{3}$.\n\nNow we find Firm 1's profit in the Cournot equilibrium.\nTotal quantity: $Q^{\\text{Cournot}} = \\frac{50}{3} + \\frac{50}{3} = \\frac{100}{3}$.\nMarket price: $p^{\\text{Cournot}} = 120 - 2\\left(\\frac{100}{3}\\right) = \\frac{360 - 200}{3} = \\frac{160}{3}$.\nFirm 1's profit is:\n$$ \\pi_1^{\\text{Cournot}} = (p^{\\text{Cournot}} - c)q_1^{\\text{Cournot}} = \\left(\\frac{160}{3} - 20\\right)\\frac{50}{3} $$\n$$ \\pi_1^{\\text{Cournot}} = \\left(\\frac{160 - 60}{3}\\right)\\frac{50}{3} = \\frac{100}{3} \\times \\frac{50}{3} = \\frac{5000}{9} $$\n\n**Calculation of Profit Gain**\n\nFinally, we compute the profit gain to Firm 1 from being a Stackelberg leader compared to the Cournot outcome.\n$$ \\Delta \\pi_1 = \\pi_1^{\\text{Stackelberg}} - \\pi_1^{\\text{Cournot}} $$\n$$ \\Delta \\pi_1 = 625 - \\frac{5000}{9} $$\n$$ \\Delta \\pi_1 = \\frac{625 \\times 9}{9} - \\frac{5000}{9} = \\frac{5625 - 5000}{9} = \\frac{625}{9} $$\nThe profit gain for Firm 1 is $\\frac{625}{9}$.", "answer": "$$\\boxed{\\frac{625}{9}}$$", "id": "3154609"}, {"introduction": "While the previous examples could be solved analytically, most real-world games are too complex for such methods. This practice transitions us from theory to computation, framing the search for a Nash Equilibrium as a numerical optimization problem. You will implement and benchmark four different iterative algorithms—from the foundational Best Response dynamics to the more advanced Extragradient method—to find the equilibrium of quadratic games [@problem_id:3154618]. This hands-on coding exercise provides insight into the power and relative performance of different computational techniques used in modern game theory and optimization.", "problem": "You are asked to design, implement, and compare four iterative methods for computing a Nash equilibrium in smooth, strictly convex quadratic two-player games, and to benchmark their convergence behavior across a suite of test cases. A Nash equilibrium is defined by the point where each player’s strategy is optimal given the other’s strategy. For differentiable cost functions, the first-order optimality conditions characterize the Nash equilibrium as a zero of a suitable operator. Your program must be a complete, runnable program that takes no input and prints a single line with the aggregated results.\n\nThe fundamental base for this task is as follows.\n\n- Two players choose vectors $x \\in \\mathbb{R}^{n}$ and $y \\in \\mathbb{R}^{m}$, respectively. Player $1$ minimizes the quadratic objective\n$$\nf_1(x,y) = \\tfrac{1}{2} x^\\top Q_1 x + x^\\top C y + b_1^\\top x + c_1,\n$$\nand Player $2$ minimizes\n$$\nf_2(x,y) = \\tfrac{1}{2} y^\\top Q_2 y + y^\\top D x + b_2^\\top y + c_2,\n$$\nwhere $Q_1 \\in \\mathbb{R}^{n \\times n}$ and $Q_2 \\in \\mathbb{R}^{m \\times m}$ are symmetric positive definite matrices, $C \\in \\mathbb{R}^{n \\times m}$ and $D \\in \\mathbb{R}^{m \\times n}$ are coupling matrices, and $b_1 \\in \\mathbb{R}^n$, $b_2 \\in \\mathbb{R}^m$ are vectors. Constants $c_1$ and $c_2$ play no role in optimality and may be arbitrary.\n\n- The operator mapping that encodes the first-order optimality conditions is\n$$\nF(z) = \\begin{bmatrix}\n\\nabla_x f_1(x,y) \\\\\n\\nabla_y f_2(x,y)\n\\end{bmatrix}\n= M z + q,\n\\quad\nz = \\begin{bmatrix} x \\\\ y \\end{bmatrix},\n\\quad\nM = \\begin{bmatrix} Q_1  C \\\\ D  Q_2 \\end{bmatrix},\n\\quad\nq = \\begin{bmatrix} b_1 \\\\ b_2 \\end{bmatrix}.\n$$\nUnder symmetric positive definiteness of $M$ (for example, with $D=C^\\top$ and $Q_1$, $Q_2$ symmetric positive definite), $F$ is strongly monotone and Lipschitz continuous. The unique Nash equilibrium $z^\\star$ solves $F(z^\\star) = 0$.\n\nYour task is to implement the following iterative methods, starting from the fundamental base above and without relying on any shortcut formulas in the problem statement.\n\n- Best Response (BR): Sequentially minimize each player’s objective with the opponent’s current strategy held fixed, using the exact solution of the quadratic subproblem.\n\n- Gradient Play (GP): Apply a simultaneous gradient-based step to both players’ strategies using the operator $F$.\n\n- Proximal Best Response (PBR): Modify Best Response by adding a quadratic regularization term to each player’s subproblem, penalizing deviation from the current iterate to induce a contraction and stabilize the iteration.\n\n- Extragradient (EG): Apply an additional look-ahead gradient step before the main update to correct for the operator’s curvature; use the mapping $F$ in both stages.\n\nFor Gradient Play and Extragradient, you must choose a stepsize based on the Lipschitz constant of $F$. For quadratic games, $F$ is linear, and the Lipschitz constant equals the spectral norm of $M$, denoted by $L$. A safe choice is a stepsize $\\gamma$ satisfying $0  \\gamma \\leq 1/L$. For Proximal Best Response, select a regularization parameter $\\rho  0$; a practical, theoretically motivated choice is $\\rho = L$.\n\nImplementation details to follow:\n\n- Initialization: Use the zero vector $z_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$ for all methods.\n\n- Termination criterion: For each method, iterate until the Euclidean norm error $\\lVert z_k - z^\\star \\rVert_2$ is less than a tolerance $\\varepsilon = 10^{-8}$, or until a maximum of $N_{\\max} = 5000$ iterations is reached. If the method fails to reach the tolerance within $N_{\\max}$ iterations, report $-1$ for that method.\n\n- Output: For each test case, report a list of $4$ integers corresponding to the number of iterations taken by BR, GP, PBR, and EG, in that order. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a list for one test case. For example, a valid output format is\n`[ [i_{1,BR}, i_{1,GP}, i_{1,PBR}, i_{1,EG}], [i_{2,BR}, i_{2,GP}, i_{2,PBR}, i_{2,EG}], ... ]`\nwith each `$i_{\\cdot,\\cdot}$` an integer.\n\nTest suite specification:\n\nProvide the following five convex quadratic games, all with $D = C^\\top$ to ensure a symmetric $M$.\n\n- Case $1$ (scalar variables, moderate coupling):\n$n=m=1$,\n$Q_1 = [\\,2\\,]$,\n$Q_2 = [\\,3\\,]$,\n$C = [\\,0.5\\,]$,\n$b_1 = [\\,1\\,]$,\n$b_2 = [\\,-2\\,]$.\n\n- Case $2$ (scalar variables, decoupled boundary case):\n$n=m=1$,\n$Q_1 = [\\,1.5\\,]$,\n$Q_2 = [\\,2.5\\,]$,\n$C = [\\,0.0\\,]$,\n$b_1 = [\\,-1.0\\,]$,\n$b_2 = [\\,3.0\\,]$.\n\n- Case $3$ (two-dimensional variables, moderate coupling):\n$n=m=2$,\n$Q_1 = \\begin{bmatrix} 2  0 \\\\ 0  4 \\end{bmatrix}$,\n$Q_2 = \\begin{bmatrix} 3  0 \\\\ 0  1.5 \\end{bmatrix}$,\n$C = \\begin{bmatrix} 0.3  -0.1 \\\\ 0.2  0.0 \\end{bmatrix}$,\n$b_1 = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}$,\n$b_2 = \\begin{bmatrix} 0.5 \\\\ -0.5 \\end{bmatrix}$.\n\n- Case $4$ (two-dimensional variables, ill-conditioned but stable):\n$n=m=2$,\n$Q_1 = \\begin{bmatrix} 0.01  0 \\\\ 0  100 \\end{bmatrix}$,\n$Q_2 = \\begin{bmatrix} 0.001  0 \\\\ 0  50 \\end{bmatrix}$,\n$C = \\begin{bmatrix} 0.05  0.0 \\\\ 0.0  0.05 \\end{bmatrix}$,\n$b_1 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$,\n$b_2 = \\begin{bmatrix} -1 \\\\ 2 \\end{bmatrix}$.\n\n- Case $5$ (three-dimensional variables, stronger but still stable coupling):\n$n=m=3$,\n$Q_1 = \\mathrm{diag}(1,2,3)$,\n$Q_2 = \\mathrm{diag}(1.5,2.5,3.5)$,\n$C = 0.3\\,I_3$,\n$b_1 = \\begin{bmatrix} 1 \\\\ -2 \\\\ 0.5 \\end{bmatrix}$,\n$b_2 = \\begin{bmatrix} -1 \\\\ 1 \\\\ 2 \\end{bmatrix}$,\nwhere $I_3$ is the $3 \\times 3$ identity matrix.\n\nImplementation constraints:\n\n- Compute $z^\\star$ exactly by solving the linear system $M z^\\star + q = 0$.\n\n- For Gradient Play and Extragradient, compute $L$ as the spectral norm of $M$ and use $\\gamma = 1/L$.\n\n- For Proximal Best Response, use $\\rho = L$.\n\n- Ensure all iterations begin at $z_0 = 0$ and use the Euclidean norm error tolerance $\\varepsilon = 10^{-8}$.\n\nYour final program must print the results for all five cases as a single line in the specified format, with each inner list giving the iteration counts `[i_BR, i_GP, i_PBR, i_EG]` for the corresponding case.", "solution": "We start from the fundamental definitions of Nash equilibrium and first-order optimality in differentiable, strictly convex quadratic games. Player $1$’s gradient with respect to $x$ is\n$$\n\\nabla_x f_1(x,y) = Q_1 x + C y + b_1,\n$$\nand Player $2$’s gradient with respect to $y$ is\n$$\n\\nabla_y f_2(x,y) = Q_2 y + D x + b_2.\n$$\nStacking variables $z = \\begin{bmatrix} x \\\\ y \\end{bmatrix}$ and gradients yields the operator $F(z) = M z + q$ with\n$$\nM = \\begin{bmatrix} Q_1  C \\\\ D  Q_2 \\end{bmatrix}, \\quad q = \\begin{bmatrix} b_1 \\\\ b_2 \\end{bmatrix}.\n$$\nWhen $M$ is symmetric positive definite, which holds for the test suite by taking $D=C^\\top$ and $Q_1$, $Q_2$ symmetric positive definite and couplings sufficiently small, then $F$ is strongly monotone and Lipschitz continuous. The unique Nash equilibrium solves $F(z^\\star) = 0$. Because $F$ is linear, we obtain\n$$\nM z^\\star + q = 0 \\quad \\Rightarrow \\quad z^\\star = - M^{-1} q,\n$$\nwhich can be computed using a linear solver.\n\nWe now derive each iterative method from the core principles.\n\n- Best Response (BR). Holding the opponent’s strategy fixed reduces each player’s problem to a strictly convex quadratic in their own variable. The minimizer of a quadratic $x \\mapsto \\tfrac{1}{2} x^\\top Q_1 x + (C y_k + b_1)^\\top x$ is characterized by the first-order condition $Q_1 x_{k+1} + C y_k + b_1 = 0$, i.e.,\n$$\nx_{k+1} = -Q_1^{-1}(C y_k + b_1).\n$$\nThen, given $x_{k+1}$, Player $2$’s best response satisfies $Q_2 y_{k+1} + D x_{k+1} + b_2 = 0$, i.e.,\n$$\ny_{k+1} = -Q_2^{-1}(D x_{k+1} + b_2).\n$$\nThese two updates implement a block Gauss–Seidel method to solve $M z + q = 0$.\n\n- Gradient Play (GP). A simultaneous gradient step for both players using the operator $F$ reads\n$$\nz_{k+1} = z_k - \\gamma F(z_k) = z_k - \\gamma (M z_k + q).\n$$\nFor a linear Lipschitz operator with constant $L = \\lVert M \\rVert_2$ (the spectral norm), a sufficient condition for stability is $0  \\gamma \\le 1/L$. We choose $\\gamma = 1/L$ to have a safe stepsize.\n\n- Proximal Best Response (PBR). The proximal modification adds a quadratic regularization penalizing the change from the current iterate. For Player $1$, the subproblem is\n$$\n\\min_x \\left\\{ \\tfrac{1}{2} x^\\top Q_1 x + (C y_k + b_1)^\\top x + \\tfrac{\\rho}{2} \\lVert x - x_k \\rVert_2^2 \\right\\}.\n$$\nIts first-order condition is\n$$\nQ_1 x_{k+1} + C y_k + b_1 + \\rho (x_{k+1} - x_k) = 0 \\quad \\Rightarrow \\quad (Q_1 + \\rho I) x_{k+1} = \\rho x_k - C y_k - b_1,\n$$\ngiving\n$$\nx_{k+1} = (Q_1 + \\rho I)^{-1}(\\rho x_k - C y_k - b_1).\n$$\nSimilarly, Player $2$’s update is\n$$\ny_{k+1} = (Q_2 + \\rho I)^{-1}(\\rho y_k - D x_{k+1} - b_2).\n$$\nChoosing $\\rho = L$ yields a strong contraction that stabilizes and often accelerates convergence compared to unregularized BR.\n\n- Extragradient (EG). Korpelevich’s extragradient method for monotone variational inequalities computes a look-ahead point and evaluates the operator there before the main update. For our unconstrained linear $F$,\n$$\nz_{k+\\tfrac{1}{2}} = z_k - \\gamma F(z_k), \\quad\nz_{k+1} = z_k - \\gamma F(z_{k+\\tfrac{1}{2}}),\n$$\nwith the same safe stepsize choice $\\gamma = 1/L$. This corrects for curvature and yields improved robustness and, in many settings, faster convergence than plain Gradient Play.\n\nAll methods start from $z_0 = 0$. We terminate when the Euclidean norm error satisfies\n$$\n\\lVert z_k - z^\\star \\rVert_2 \\le \\varepsilon, \\quad \\varepsilon = 10^{-8},\n$$\nor when the iteration count exceeds $N_{\\max} = 5000$, in which case we report the failure code $-1$.\n\nFor each test case defined by $(Q_1, Q_2, C, D, b_1, b_2)$, we:\n\n1. Form $M$ and $q$.\n2. Compute the equilibrium $z^\\star = -M^{-1} q$.\n3. Compute $L = \\lVert M \\rVert_2$, the spectral norm, via singular values, and set $\\gamma = 1/L$ and $\\rho = L$.\n4. Run BR, GP, PBR, and EG from $z_0 = 0$, counting iterations until the tolerance is reached or failure occurs.\n5. Record the four integers `[i_BR, i_GP, i_PBR, i_EG]`.\n\nDesign coverage in the test suite:\n\n- Case $2$ sets $C = 0$, creating decoupled problems. BR and PBR attain the exact solution in one pass because each player’s subproblem is independent; GP and EG still converge linearly but usually require multiple iterations. This is a boundary case testing correctness and exactness.\n\n- Case $4$ uses ill-conditioned $Q_1$ and $Q_2$ with small but nonzero coupling. GP slows markedly due to conditioning; EG and PBR improve stability. This tests sensitivity to conditioning and the benefit of regularization and look-ahead.\n\n- Cases $1$, $3$, and $5$ provide “happy path” and stronger coupling scenarios in $1$-, $2$-, and $3$-dimensional settings, respectively, testing scalability and robustness across dimensions.\n\nThe final program implements exactly these steps and prints a single line with the requested nested list of iteration counts for the five cases. This quantitatively benchmarks the convergence behavior (“rates”) through iteration counts to a fixed tolerance, using the same initialization and theoretically safe parameter choices derived from the operator’s Lipschitz constant.", "answer": "```python\n# Python 3.12; numpy 1.23.5; scipy not used.\nimport numpy as np\n\ndef spectral_norm(M: np.ndarray) - float:\n    # Spectral norm (largest singular value)\n    return np.linalg.svd(M, compute_uv=False)[0]\n\ndef equilibrium(M: np.ndarray, q: np.ndarray) - np.ndarray:\n    # Solve M z* + q = 0 = z* = - M^{-1} q\n    return -np.linalg.solve(M, q)\n\ndef best_response(Q1, Q2, C, D, b1, b2, z_star, tol=1e-8, max_iters=5000):\n    n = Q1.shape[0]\n    m = Q2.shape[0]\n    x = np.zeros(n)\n    y = np.zeros(m)\n    for k in range(1, max_iters + 1):\n        # Player 1 best response\n        rhs1 = -C @ y - b1\n        x = np.linalg.solve(Q1, rhs1)\n        # Player 2 best response\n        rhs2 = -D @ x - b2\n        y = np.linalg.solve(Q2, rhs2)\n        z = np.concatenate([x, y])\n        err = np.linalg.norm(z - z_star)\n        if err = tol:\n            return k\n    return -1\n\ndef proximal_best_response(Q1, Q2, C, D, b1, b2, z_star, rho, tol=1e-8, max_iters=5000):\n    n = Q1.shape[0]\n    m = Q2.shape[0]\n    x = np.zeros(n)\n    y = np.zeros(m)\n    Q1p = Q1 + rho * np.eye(n)\n    Q2p = Q2 + rho * np.eye(m)\n    for k in range(1, max_iters + 1):\n        rhs1 = rho * x - C @ y - b1\n        x = np.linalg.solve(Q1p, rhs1)\n        rhs2 = rho * y - D @ x - b2\n        y = np.linalg.solve(Q2p, rhs2)\n        z = np.concatenate([x, y])\n        err = np.linalg.norm(z - z_star)\n        if err = tol:\n            return k\n    return -1\n\ndef gradient_play(M, q, z_star, gamma, tol=1e-8, max_iters=5000):\n    z = np.zeros_like(z_star)\n    for k in range(1, max_iters + 1):\n        Fz = M @ z + q\n        z = z - gamma * Fz\n        err = np.linalg.norm(z - z_star)\n        if err = tol:\n            return k\n    return -1\n\ndef extragradient(M, q, z_star, gamma, tol=1e-8, max_iters=5000):\n    z = np.zeros_like(z_star)\n    for k in range(1, max_iters + 1):\n        Fz = M @ z + q\n        z_half = z - gamma * Fz\n        Fz_half = M @ z_half + q\n        z = z - gamma * Fz_half\n        err = np.linalg.norm(z - z_star)\n        if err = tol:\n            return k\n    return -1\n\ndef build_block_matrix(Q1, Q2, C, D):\n    return np.block([[Q1, C],\n                     [D, Q2]])\n\ndef solve_case(Q1, Q2, C, D, b1, b2):\n    M = build_block_matrix(Q1, Q2, C, D)\n    q = np.concatenate([b1, b2])\n    z_star = equilibrium(M, q)\n    L = spectral_norm(M)\n    gamma = 1.0 / L\n    rho = L\n    it_br = best_response(Q1, Q2, C, D, b1, b2, z_star, tol=1e-8, max_iters=5000)\n    it_gp = gradient_play(M, q, z_star, gamma=gamma, tol=1e-8, max_iters=5000)\n    it_pbr = proximal_best_response(Q1, Q2, C, D, b1, b2, z_star, rho=rho, tol=1e-8, max_iters=5000)\n    it_eg = extragradient(M, q, z_star, gamma=gamma, tol=1e-8, max_iters=5000)\n    return [it_br, it_gp, it_pbr, it_eg]\n\ndef solve():\n    test_cases = []\n\n    # Case 1: scalar, moderate coupling\n    Q1 = np.array([[2.0]])\n    Q2 = np.array([[3.0]])\n    C = np.array([[0.5]])\n    D = C.T\n    b1 = np.array([1.0])\n    b2 = np.array([-2.0])\n    test_cases.append((Q1, Q2, C, D, b1, b2))\n\n    # Case 2: scalar, decoupled\n    Q1 = np.array([[1.5]])\n    Q2 = np.array([[2.5]])\n    C = np.array([[0.0]])\n    D = C.T\n    b1 = np.array([-1.0])\n    b2 = np.array([3.0])\n    test_cases.append((Q1, Q2, C, D, b1, b2))\n\n    # Case 3: 2D, moderate coupling\n    Q1 = np.array([[2.0, 0.0],\n                   [0.0, 4.0]])\n    Q2 = np.array([[3.0, 0.0],\n                   [0.0, 1.5]])\n    C = np.array([[0.3, -0.1],\n                  [0.2,  0.0]])\n    D = C.T\n    b1 = np.array([1.0, -1.0])\n    b2 = np.array([0.5, -0.5])\n    test_cases.append((Q1, Q2, C, D, b1, b2))\n\n    # Case 4: 2D, ill-conditioned\n    Q1 = np.array([[0.01, 0.0],\n                   [0.0, 100.0]])\n    Q2 = np.array([[0.001, 0.0],\n                   [0.0, 50.0]])\n    C = np.array([[0.05, 0.0],\n                  [0.0, 0.05]])\n    D = C.T\n    b1 = np.array([1.0, 1.0])\n    b2 = np.array([-1.0, 2.0])\n    test_cases.append((Q1, Q2, C, D, b1, b2))\n\n    # Case 5: 3D, stronger but still stable coupling\n    Q1 = np.diag([1.0, 2.0, 3.0])\n    Q2 = np.diag([1.5, 2.5, 3.5])\n    C = 0.3 * np.eye(3)\n    D = C.T\n    b1 = np.array([1.0, -2.0, 0.5])\n    b2 = np.array([-1.0, 1.0, 2.0])\n    test_cases.append((Q1, Q2, C, D, b1, b2))\n\n    results = []\n    for Q1, Q2, C, D, b1, b2 in test_cases:\n        res = solve_case(Q1, Q2, C, D, b1, b2)\n        results.append(res)\n\n    print(str(results).replace(' ', ''))\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3154618"}]}