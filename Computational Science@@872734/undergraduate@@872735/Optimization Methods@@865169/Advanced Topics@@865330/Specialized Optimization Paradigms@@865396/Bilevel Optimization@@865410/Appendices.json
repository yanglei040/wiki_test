{"hands_on_practices": [{"introduction": "The most direct way to solve a bilevel optimization problem is to find an explicit expression for the follower's optimal reaction and substitute it into the leader's objective. This exercise walks you through this fundamental technique for a case with a unique lower-level solution. You will then compare this exact solution to a common single-level approximation using a penalty method, providing insight into the nature and impact of such reformulations [@problem_id:3102923].", "problem": "Consider the bilevel optimization problem where the upper-level decision variable is $x \\in \\mathbb{R}$ and the lower-level decision variable is $y \\in \\mathbb{R}$. The upper-level objective is $F(x,y) = (x - 1)^{2} + (y - 2)^{2}$. The lower-level problem, parameterized by $x$, is to minimize the strongly convex function $g(y; x) = (y - x)^{2} + y$ with respect to $y$ over $\\mathbb{R}$. The lower-level first-order Karush–Kuhn–Tucker (KKT) stationarity condition (which, for an unconstrained problem, reduces to the condition that the gradient with respect to $y$ vanishes at the minimizer) is given by $\\partial g/\\partial y = 2(y - x) + 1 = 0$.\n\nTasks:\n1. Using only the fundamental definition of a bilevel problem and the first-order optimality condition for unconstrained convex minimization, eliminate the lower level by characterizing the lower-level solution $y^{\\star}(x)$ as a function of $x$, and then solve for the exact bilevel optimizer $(x^{\\star}, y^{\\star})$ and the corresponding optimal upper-level value $F^{\\star} = F(x^{\\star}, y^{\\star})$.\n2. Now consider a single-level penalty formulation in which the upper-level objective penalizes violation of the lower-level KKT residual. Define the KKT residual as $r(x,y) = \\partial g/\\partial y = 2(y - x) + 1$. For a given penalty parameter $\\rho = 1$, solve the penalized problem\n$\n\\min_{x \\in \\mathbb{R},\\, y \\in \\mathbb{R}} \\;\\; \\Phi_{\\rho}(x,y) \\equiv F(x,y) + \\rho \\, r(x,y)^{2}.\n$\nCompute the minimizer $(x_{\\rho}, y_{\\rho})$ and evaluate the upper-level objective $F(x_{\\rho}, y_{\\rho})$ at this penalized solution.\n\nFinally, compute the scalar difference \n$\n\\Delta \\equiv F(x_{\\rho}, y_{\\rho}) - F^{\\star},\n$\nand report $\\Delta$ as a single exact value. No rounding is required and no units are involved.", "solution": "The solution proceeds in two parts as requested by the problem statement.\n\nFirst, we solve the bilevel problem. The bilevel optimization problem is given by:\n$$ \\min_{x \\in \\mathbb{R}, y \\in \\mathbb{R}} F(x,y) = (x - 1)^{2} + (y - 2)^{2} $$\n$$ \\text{subject to } y \\in \\arg\\min_{z \\in \\mathbb{R}} g(z; x) = (z - x)^{2} + z $$\nThe lower-level problem is unconstrained and its objective function $g(z; x)$ is strictly convex with respect to $z$ (since $\\frac{\\partial^2 g}{\\partial z^2} = 2 > 0$). Therefore, the unique minimizer $y^{\\star}(x)$ is found by setting the first derivative with respect to the lower-level variable to zero. The problem provides this first-order optimality condition:\n$$ \\frac{\\partial g}{\\partial y} = 2(y - x) + 1 = 0 $$\nSolving for $y$ as a function of $x$ gives the lower-level optimal response, or rational reaction function, $y^{\\star}(x)$:\n$$ 2(y - x) = -1 \\implies y - x = -\\frac{1}{2} \\implies y^{\\star}(x) = x - \\frac{1}{2} $$\nTo solve the upper-level problem, we substitute this expression for $y$ into the upper-level objective function $F(x,y)$, reducing the problem to a single-level optimization problem in terms of $x$ only:\n$$ \\hat{F}(x) = F(x, y^{\\star}(x)) = (x - 1)^{2} + \\left( \\left(x - \\frac{1}{2}\\right) - 2 \\right)^{2} = (x - 1)^{2} + \\left(x - \\frac{5}{2}\\right)^{2} $$\nTo find the optimal value $x^{\\star}$, we minimize $\\hat{F}(x)$ by finding its stationary point. We compute the derivative with respect to $x$ and set it to zero:\n$$ \\frac{d\\hat{F}}{dx} = \\frac{d}{dx} \\left[ (x - 1)^{2} + \\left(x - \\frac{5}{2}\\right)^{2} \\right] = 2(x - 1) + 2\\left(x - \\frac{5}{2}\\right) = 2x - 2 + 2x - 5 = 4x - 7 $$\nSetting the derivative to zero yields:\n$$ 4x - 7 = 0 \\implies x^{\\star} = \\frac{7}{4} $$\nThe second derivative is $\\frac{d^2\\hat{F}}{dx^2} = 4 > 0$, which confirms that $x^{\\star}$ is a minimizer. Now we find the corresponding optimal lower-level variable $y^{\\star}$:\n$$ y^{\\star} = y^{\\star}(x^{\\star}) = \\frac{7}{4} - \\frac{1}{2} = \\frac{7}{4} - \\frac{2}{4} = \\frac{5}{4} $$\nThe bilevel optimizer is $(x^{\\star}, y^{\\star}) = \\left(\\frac{7}{4}, \\frac{5}{4}\\right)$. The corresponding optimal upper-level value is $F^{\\star}$:\n$$ F^{\\star} = F(x^{\\star}, y^{\\star}) = \\left(\\frac{7}{4} - 1\\right)^{2} + \\left(\\frac{5}{4} - 2\\right)^{2} = \\left(\\frac{3}{4}\\right)^{2} + \\left(-\\frac{3}{4}\\right)^{2} = \\frac{9}{16} + \\frac{9}{16} = \\frac{18}{16} = \\frac{9}{8} $$\n\nSecond, we solve the single-level penalized problem. The penalized objective function $\\Phi_{\\rho}(x,y)$ with penalty parameter $\\rho = 1$ is:\n$$ \\Phi_{1}(x,y) = F(x,y) + 1 \\cdot r(x,y)^{2} = (x - 1)^{2} + (y - 2)^{2} + (2(y - x) + 1)^{2} $$\nWe minimize this function with respect to both $x$ and $y$. We find the stationary point by setting the gradient of $\\Phi_{1}(x,y)$ to zero. The partial derivatives are:\n$$ \\frac{\\partial \\Phi_{1}}{\\partial x} = \\frac{\\partial}{\\partial x} \\left[ (x - 1)^{2} + (y - 2)^{2} + (2y - 2x + 1)^{2} \\right] = 2(x - 1) + 2(2y - 2x + 1)(-2) = 2x - 2 - 8y + 8x - 4 = 10x - 8y - 6 $$\n$$ \\frac{\\partial \\Phi_{1}}{\\partial y} = \\frac{\\partial}{\\partial y} \\left[ (x - 1)^{2} + (y - 2)^{2} + (2y - 2x + 1)^{2} \\right] = 2(y - 2) + 2(2y - 2x + 1)(2) = 2y - 4 + 8y - 8x + 4 = 10y - 8x $$\nSetting the partial derivatives to zero gives a system of linear equations for the minimizer $(x_{\\rho}, y_{\\rho})$:\n1. $10x - 8y - 6 = 0 \\implies 5x - 4y = 3$\n2. $10y - 8x = 0 \\implies 10y = 8x \\implies y = \\frac{4}{5}x$\nSubstitute the expression for $y$ from the second equation into the first:\n$$ 5x - 4\\left(\\frac{4}{5}x\\right) = 3 \\implies 5x - \\frac{16}{5}x = 3 \\implies \\frac{25x - 16x}{5} = 3 \\implies \\frac{9}{5}x = 3 $$\nSolving for $x$, we find the penalized solution $x_{\\rho}$:\n$$ x_{\\rho} = 3 \\cdot \\frac{5}{9} = \\frac{15}{9} = \\frac{5}{3} $$\nNow we find the corresponding $y_{\\rho}$:\n$$ y_{\\rho} = \\frac{4}{5} x_{\\rho} = \\frac{4}{5} \\cdot \\frac{5}{3} = \\frac{4}{3} $$\nThe minimizer of the penalized problem is $(x_{\\rho}, y_{\\rho}) = \\left(\\frac{5}{3}, \\frac{4}{3}\\right)$. Next, we evaluate the upper-level objective $F(x,y)$ at this point:\n$$ F(x_{\\rho}, y_{\\rho}) = \\left(\\frac{5}{3} - 1\\right)^{2} + \\left(\\frac{4}{3} - 2\\right)^{2} = \\left(\\frac{2}{3}\\right)^{2} + \\left(-\\frac{2}{3}\\right)^{2} = \\frac{4}{9} + \\frac{4}{9} = \\frac{8}{9} $$\n\nFinally, we compute the required scalar difference $\\Delta$:\n$$ \\Delta = F(x_{\\rho}, y_{\\rho}) - F^{\\star} = \\frac{8}{9} - \\frac{9}{8} $$\nTo compute this, we find a common denominator, which is $72$:\n$$ \\Delta = \\frac{8 \\cdot 8}{9 \\cdot 8} - \\frac{9 \\cdot 9}{8 \\cdot 9} = \\frac{64 - 81}{72} = -\\frac{17}{72} $$\nThe difference is an exact rational number.", "answer": "$$\\boxed{-\\frac{17}{72}}$$", "id": "3102923"}, {"introduction": "A key complexity in bilevel optimization arises when the follower's problem has multiple optimal solutions for a given leader action. This ambiguity requires the leader to consider best-case (optimistic) and worst-case (pessimistic) scenarios. This practice challenges you to explore this very situation, quantifying the gap between the optimistic and pessimistic outcomes and thus deepening your understanding of the problem's inherent structure [@problem_id:3102843].", "problem": "Consider a bilevel optimization problem with an upper-level decision variable $x \\in [0,2]$ and a lower-level decision vector $y = (y_{1}, y_{2}) \\in \\mathbb{R}^{2}$. The upper-level objective is to minimize the real-valued function $F(x,y) = (x - 1)^{2} + y_{1} - y_{2}$. The lower-level problem, parameterized by the upper-level decision $x$, is a Linear Programming (LP) problem defined as follows: minimize $y_{1} + y_{2}$ subject to $y_{1} + y_{2} = x$, $0 \\leq y_{1} \\leq 1$, and $0 \\leq y_{2} \\leq 1$. Definitions: In the optimistic bilevel convention, the lower level can select any of its optimal solutions so as to minimize the upper-level objective. In the pessimistic bilevel convention, the lower level can select any of its optimal solutions so as to maximize the upper-level objective. Starting from standard definitions of bilevel optimization and properties of linear programming solutions, determine the optimistic upper-level optimal objective value $F_{\\text{opt}}^{\\star}$ and the pessimistic upper-level optimal objective value $F_{\\text{pess}}^{\\star}$, and then compute the numerical gap $G = F_{\\text{pess}}^{\\star} - F_{\\text{opt}}^{\\star}$. Express the final answer for $G$ as an exact value (no rounding).", "solution": "The bilevel problem is given by:\nUpper Level:\n$$ \\min_{x, y} F(x, y) = (x - 1)^{2} + y_{1} - y_{2} $$\nsubject to $x \\in [0, 2]$ and $y$ being an optimal solution to the lower-level problem.\n\nLower Level (for a fixed $x$):\n$$ \\min_{y_1, y_2} f(y_1, y_2) = y_{1} + y_{2} $$\nsubject to:\n$$ y_{1} + y_{2} = x $$\n$$ 0 \\leq y_{1} \\leq 1 $$\n$$ 0 \\leq y_{2} \\leq 1 $$\n\nFirst, we analyze the lower-level problem (LLP). For any feasible point $(y_1, y_2)$, the constraint $y_1 + y_2 = x$ dictates that the lower-level objective function $f(y_1, y_2)$ is always equal to $x$. This means that any point $(y_1, y_2)$ that is feasible for the LLP is also an optimal solution to the LLP.\n\nLet $S(x)$ denote the set of optimal solutions to the LLP for a given $x$. This set is identical to the feasible set of the LLP. We can characterize $S(x)$ by expressing the constraints in terms of $y_1$ alone. From $y_1 + y_2 = x$, we have $y_2 = x - y_1$. The box constraints $0 \\leq y_1 \\leq 1$ and $0 \\leq y_2 \\leq 1$ become:\n$1.$ $0 \\leq y_1 \\leq 1$\n$2.$ $0 \\leq x - y_1 \\leq 1 \\implies x - 1 \\leq y_1 \\leq x$\n\nCombining these gives the range for $y_1$:\n$$ \\max(0, x - 1) \\leq y_1 \\leq \\min(1, x) $$\nThe set of optimal lower-level solutions is therefore:\n$$ S(x) = \\{ (y_1, y_2) \\in \\mathbb{R}^2 \\mid y_2 = x - y_1, \\text{ and } \\max(0, x-1) \\leq y_1 \\leq \\min(1, x) \\} $$\nThe domain $x \\in [0, 2]$ ensures this interval for $y_1$ is non-empty. For $x=0$, $y_1=0$, and for $x=2$, $y_1=1$.\n\nThe upper-level objective is $F(x, y) = (x - 1)^{2} + y_{1} - y_{2}$. Substituting $y_2 = x - y_1$, we get:\n$$ F(x, y_1) = (x - 1)^{2} + y_{1} - (x - y_{1}) = (x - 1)^{2} + 2y_{1} - x $$\nThe bilevel problem can now be rewritten by replacing the LLP with the choice of $y_1$ from its feasible range.\n\n**Optimistic Case**\nIn the optimistic convention, for each $x$, the lower level chooses a solution $y \\in S(x)$ that minimizes the upper-level objective $F(x,y)$. The upper-level problem is:\n$$ F_{\\text{opt}}^{\\star} = \\min_{x \\in [0, 2]} \\left( \\min_{y \\in S(x)} F(x, y) \\right) $$\nThe inner minimization is:\n$$ \\min_{y \\in S(x)} F(x, y) = \\min_{y_1} \\left( (x - 1)^{2} + 2y_{1} - x \\right) \\quad \\text{s.t. } y_1 \\in [\\max(0, x-1), \\min(1, x)] $$\nTo minimize this expression with respect to $y_1$, we must choose the smallest possible value of $y_1$, which is $y_1 = \\max(0, x-1)$.\nLet $F_{\\text{opt}}(x)$ be the resulting upper-level objective function:\n$$ F_{\\text{opt}}(x) = (x - 1)^{2} + 2\\max(0, x-1) - x $$\nWe minimize $F_{\\text{opt}}(x)$ over $x \\in [0, 2]$ by cases:\nCase 1: $x \\in [0, 1]$. Here, $x-1 \\leq 0$, so $\\max(0, x-1) = 0$.\n$$ F_{\\text{opt}}(x) = (x - 1)^{2} - x = x^2 - 2x + 1 - x = x^2 - 3x + 1 $$\nThe derivative is $F'_{\\text{opt}}(x) = 2x - 3$. The vertex of this parabola is at $x=3/2$, which is outside $[0, 1]$. Since the parabola opens upward, the minimum on $[0, 1]$ occurs at an endpoint.\n$F_{\\text{opt}}(0) = 1$.\n$F_{\\text{opt}}(1) = 1^2 - 3(1) + 1 = -1$.\nThe minimum over $[0, 1]$ is $-1$.\n\nCase 2: $x \\in (1, 2]$. Here, $x-1 > 0$, so $\\max(0, x-1) = x-1$.\n$$ F_{\\text{opt}}(x) = (x - 1)^{2} + 2(x - 1) - x = x^2 - 2x + 1 + 2x - 2 - x = x^2 - x - 1 $$\nThe derivative is $F'_{\\text{opt}}(x) = 2x - 1$. The vertex is at $x=1/2$, which is outside $(1, 2]$. Since the parabola opens upward, the minimum over $(1, 2]$ is at endpoint $x \\to 1^+$ or $x=2$.\n$\\lim_{x \\to 1^+} F_{\\text{opt}}(x) = 1^2 - 1 - 1 = -1$.\n$F_{\\text{opt}}(2) = 2^2 - 2 - 1 = 1$.\nThe minimum over $(1, 2]$ is $-1$.\n\nCombining both cases, the global minimum of $F_{\\text{opt}}(x)$ on $[0, 2]$ is $-1$, occurring at $x=1$. Thus, $F_{\\text{opt}}^{\\star} = -1$.\n\n**Pessimistic Case**\nIn the pessimistic convention, for each $x$, the lower level chooses a solution $y \\in S(x)$ that maximizes the upper-level objective $F(x,y)$. The upper-level problem is:\n$$ F_{\\text{pess}}^{\\star} = \\min_{x \\in [0, 2]} \\left( \\max_{y \\in S(x)} F(x, y) \\right) $$\nThe inner maximization is:\n$$ \\max_{y \\in S(x)} F(x, y) = \\max_{y_1} \\left( (x - 1)^{2} + 2y_{1} - x \\right) \\quad \\text{s.t. } y_1 \\in [\\max(0, x-1), \\min(1, x)] $$\nTo maximize this expression with respect to $y_1$, we must choose the largest possible value of $y_1$, which is $y_1 = \\min(1, x)$.\nLet $F_{\\text{pess}}(x)$ be the resulting upper-level objective function:\n$$ F_{\\text{pess}}(x) = (x - 1)^{2} + 2\\min(1, x) - x $$\nWe minimize $F_{\\text{pess}}(x)$ over $x \\in [0, 2]$ by cases:\nCase 1: $x \\in [0, 1]$. Here, $\\min(1, x) = x$.\n$$ F_{\\text{pess}}(x) = (x - 1)^{2} + 2x - x = x^2 - 2x + 1 + x = x^2 - x + 1 $$\nThe derivative is $F'_{\\text{pess}}(x) = 2x - 1$. The vertex is at $x=1/2$, which is in $[0, 1]$.\nThe minimum value is $F_{\\text{pess}}(1/2) = (1/2)^2 - 1/2 + 1 = 1/4 - 1/2 + 1 = 3/4$.\n\nCase 2: $x \\in (1, 2]$. Here, $\\min(1, x) = 1$.\n$$ F_{\\text{pess}}(x) = (x - 1)^{2} + 2(1) - x = x^2 - 2x + 1 + 2 - x = x^2 - 3x + 3 $$\nThe derivative is $F'_{\\text{pess}}(x) = 2x - 3$. The vertex is at $x=3/2$, which is in $(1, 2]$.\nThe minimum value is $F_{\\text{pess}}(3/2) = (3/2)^2 - 3(3/2) + 3 = 9/4 - 9/2 + 3 = 9/4 - 18/4 + 12/4 = 3/4$.\n\nCombining both cases, the global minimum of $F_{\\text{pess}}(x)$ on $[0, 2]$ is $3/4$. Thus, $F_{\\text{pess}}^{\\star} = 3/4$.\n\n**Compute the Gap**\nThe numerical gap $G$ is the difference between the pessimistic and optimistic optimal objective values:\n$$ G = F_{\\text{pess}}^{\\star} - F_{\\text{opt}}^{\\star} $$\n$$ G = \\frac{3}{4} - (-1) = \\frac{3}{4} + 1 = \\frac{7}{4} $$\nThe final answer is $7/4$.", "answer": "$$ \\boxed{\\frac{7}{4}} $$", "id": "3102843"}, {"introduction": "For complex bilevel problems where direct substitution is not feasible, gradient-based methods are essential. These methods require computing the sensitivity of the lower-level solution with respect to the upper-level variables. This hands-on exercise guides you through deriving this sensitivity, the Jacobian matrix, using the powerful technique of implicit differentiation and verifying your analytical result numerically [@problem_id:3102819].", "problem": "You are given a bilevel optimization setup in which the lower-level problem defines the mapping $y^{*}(x)$ as the unique minimizer of the function $f(y;x) = \\frac{1}{2}\\lVert B y - x \\rVert^{2}$, where $B \\in \\mathbb{R}^{m \\times n}$ has full column rank ($n \\leq m$). The upper-level objective depends on $x \\in \\mathbb{R}^{m}$, and one needs to compute the sensitivity of the lower-level minimizer with respect to the upper-level variable, namely the Jacobian $\\frac{\\partial y^{*}}{\\partial x} \\in \\mathbb{R}^{n \\times m}$. Starting from fundamental principles in optimization methods—specifically, the gradient-based first-order optimality condition for unconstrained convex problems (equivalently, the Karush-Kuhn-Tucker (KKT) conditions specialized to the unconstrained case)—derive an expression for $\\frac{\\partial y^{*}}{\\partial x}$ via implicit differentiation. Then implement a program that:\n- Computes the analytic Jacobian $\\frac{\\partial y^{*}}{\\partial x}$ using your derived expression.\n- Approximates the same Jacobian using a central finite-difference scheme by perturbing each coordinate of $x$ by $\\pm \\epsilon$ and recomputing $y^{*}$.\n- Returns, for each test case, the Frobenius norm of the difference between the analytic Jacobian and the finite-difference approximation as a floating-point number.\n\nThe base facts you may use include:\n- For a differentiable, strictly convex function in $y$, the unique minimizer $y^{*}(x)$ satisfies the first-order optimality condition $\\nabla_{y} f(y^{*}(x);x) = 0$.\n- For the quadratic function $f(y;x) = \\frac{1}{2}\\lVert B y - x \\rVert^{2}$, the gradient with respect to $y$ is $\\nabla_{y} f(y;x) = B^{\\top}(B y - x)$.\n- If $B$ has full column rank, then $B^{\\top} B$ is positive definite and invertible.\n\nYour program must implement the following steps for each test case:\n1. Compute the analytic Jacobian $\\frac{\\partial y^{*}}{\\partial x}$ using your implicit differentiation result.\n2. Compute $y^{*}(x)$ by solving the normal equations $(B^{\\top} B) y^{*}(x) = B^{\\top} x$ using a linear solver (do not explicitly invert matrices).\n3. Approximate $\\frac{\\partial y^{*}}{\\partial x}$ by central finite differences with step size $\\epsilon = 10^{-6}$ as follows. For each coordinate $i \\in \\{1,\\dots,m\\}$, form $x^{+} = x + \\epsilon e_{i}$ and $x^{-} = x - \\epsilon e_{i}$, compute $y^{*}(x^{+})$ and $y^{*}(x^{-})$, and set the $i$-th column of the finite-difference Jacobian to $\\frac{y^{*}(x^{+}) - y^{*}(x^{-})}{2 \\epsilon}$, where $e_{i}$ is the $i$-th standard basis vector in $\\mathbb{R}^{m}$.\n4. Compute the Frobenius norm of the difference between the analytic Jacobian and the finite-difference Jacobian.\n\nTest suite:\n- Case $1$ (well-conditioned, orthonormal columns): $m = 4$, $n = 3$,\n  $$B = \\begin{bmatrix}\n  1.0 & 0.0 & 0.0 \\\\\n  0.0 & 1.0 & 0.0 \\\\\n  0.0 & 0.0 & 1.0 \\\\\n  0.0 & 0.0 & 0.0\n  \\end{bmatrix}, \\quad\n  x = \\begin{bmatrix} 1.0 \\\\ -2.0 \\\\ 0.5 \\\\ 3.0 \\end{bmatrix}.$$\n- Case $2$ (square, invertible): $m = 2$, $n = 2$,\n  $$B = \\begin{bmatrix}\n  2.0 & -1.0 \\\\\n  1.0 & 1.0\n  \\end{bmatrix}, \\quad\n  x = \\begin{bmatrix} 0.3 \\\\ -1.2 \\end{bmatrix}.$$\n- Case $3$ (tall, ill-conditioned yet full rank): $m = 5$, $n = 3$,\n  $$B = \\begin{bmatrix}\n  10.0 & 0.0 & 0.0 \\\\\n  0.0 & 10^{-3} & 0.0 \\\\\n  0.0 & 0.0 & 5.0 \\\\\n  0.1 & 0.2 & 0.3 \\\\\n  1.0 & -1.0 & 2.0\n  \\end{bmatrix}, \\quad\n  x = \\begin{bmatrix} 1.0 \\\\ 2.0 \\\\ 3.0 \\\\ 4.0 \\\\ 5.0 \\end{bmatrix}.$$\n\nAngle units do not apply. No physical units are involved. Percentages are not involved.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_{1},r_{2},r_{3}]$), where $r_{k}$ is the Frobenius norm (a floating-point number) of the Jacobian difference for test case $k$.", "solution": "The lower-level problem is defined as finding the minimizer $y^{*}(x)$ of the objective function $f(y;x)$:\n$$ y^{*}(x) = \\arg\\min_{y \\in \\mathbb{R}^n} f(y;x) = \\arg\\min_{y \\in \\mathbb{R}^n} \\frac{1}{2}\\lVert B y - x \\rVert^{2} $$\nwhere $B \\in \\mathbb{R}^{m \\times n}$ is a matrix with full column rank ($n \\le m$), $x \\in \\mathbb{R}^m$, and $y \\in \\mathbb{R}^n$.\n\n**Derivation of the Analytic Jacobian**\n\nTo derive an expression for the Jacobian $\\frac{\\partial y^{*}}{\\partial x}$, we leverage the first-order optimality condition for unconstrained convex optimization. The objective function $f(y;x)$ can be written as $f(y;x) = \\frac{1}{2}(By-x)^\\top(By-x)$. This function is quadratic in $y$. Its Hessian with respect to $y$ is $\\nabla_{yy}^2 f(y;x) = B^\\top B$. Since the problem states that $B$ has full column rank, the matrix $B^\\top B \\in \\mathbb{R}^{n \\times n}$ is symmetric and positive definite. This guarantees that $f(y;x)$ is a strictly convex function of $y$, and therefore possesses a unique minimizer $y^*(x)$ for any given $x$.\n\nThe unique minimizer $y^*(x)$ is characterized by the first-order necessary and sufficient condition that the gradient of $f$ with respect to $y$, evaluated at $y^*(x)$, is zero:\n$$ \\nabla_y f(y^*(x); x) = 0 $$\nThe gradient of $f(y;x)$ with respect to $y$ is given by:\n$$ \\nabla_y f(y;x) = B^\\top (B y - x) $$\nSubstituting this into the optimality condition, we obtain an equation that implicitly defines $y^*$ as a function of $x$:\n$$ B^\\top (B y^*(x) - x) = 0 $$\nThis can be rearranged into the well-known normal equations for linear least squares:\n$$ B^\\top B y^*(x) = B^\\top x $$\nThis equation holds for any $x$ for which $y^*(x)$ is defined. To find the sensitivity of the solution $y^*$ to changes in $x$, we can differentiate this entire identity with respect to $x$. Let $J_y = \\frac{\\partial y^*}{\\partial x} \\in \\mathbb{R}^{n \\times m}$ be the Jacobian matrix we seek. Applying the chain rule to the left side and the standard rules of matrix-vector differentiation to the right side, we get:\n$$ \\frac{d}{dx} [B^\\top B y^*(x)] = \\frac{d}{dx} [B^\\top x] $$\n$$ (B^\\top B) \\frac{\\partial y^*}{\\partial x} = B^\\top $$\nSince $B^\\top B$ is invertible, we can solve for the Jacobian $J_y$:\n$$ J_y = \\frac{\\partial y^*}{\\partial x} = (B^\\top B)^{-1} B^\\top $$\nThis expression provides the analytic form of the Jacobian. It is noteworthy that this is the definition of the Moore-Penrose pseudoinverse of $B$, often denoted $B^\\dagger$.\n\n**Numerical Implementation and Verification**\n\nThe problem requires a program to compute this analytic Jacobian and compare it against a numerical approximation obtained via finite differences.\n\n1.  **Analytic Jacobian Calculation**: For a given test case with matrices $B$ and $x$, the analytic Jacobian $J_{\\text{analytic}} = (B^\\top B)^{-1} B^\\top$ is computed. To ensure numerical stability, we avoid explicit matrix inversion. We first compute the matrices $A = B^\\top B \\in \\mathbb{R}^{n \\times n}$ and $C = B^\\top \\in \\mathbb{R}^{n \\times m}$. We then solve the linear matrix equation $A X = C$ for the unknown matrix $X = J_{\\text{analytic}}$.\n\n2.  **Finite-Difference Jacobian Approximation**: The Jacobian is approximated column-by-column. The $i$-th column of the Jacobian, $\\frac{\\partial y^*}{\\partial x_i}$, is approximated using the second-order accurate central finite-difference formula:\n    $$ \\left(\\frac{\\partial y^*}{\\partial x}\\right)_{:,i} \\approx \\frac{y^*(x + \\epsilon e_i) - y^*(x - \\epsilon e_i)}{2\\epsilon} $$\n    Here, $e_i \\in \\mathbb{R}^m$ is the $i$-th standard basis vector (a vector of zeros with a $1$ in the $i$-th position), and $\\epsilon$ is a small step size, given as $\\epsilon = 10^{-6}$.\n    For each perturbation vector, e.g., $x' = x + \\epsilon e_i$, the corresponding optimal solution $y^*(x')$ is found by solving the normal equations:\n    $$ (B^\\top B) y^*(x') = B^\\top x' $$\n    This linear system is solved for $y^*(x^+)$ and $y^*(x^-)$ for each $i \\in \\{1, \\dots, m\\}$ to construct the columns of the approximate Jacobian, $J_{\\text{fd}}$.\n\n3.  **Error Calculation**: The accuracy of the finite-difference approximation is assessed by calculating the Frobenius norm of the difference between the analytic and approximate Jacobians:\n    $$ \\text{Error} = \\| J_{\\text{analytic}} - J_{\\text{fd}} \\|_F = \\sqrt{\\sum_{i=1}^n \\sum_{j=1}^m ( (J_{\\text{analytic}})_{ij} - (J_{\\text{fd}})_{ij} )^2} $$\nThis procedure is carried out for each specified test case, and the resulting error norms are reported.", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import solve\n\ndef solve_problem():\n    \"\"\"\n    Solves the bilevel optimization sensitivity problem for a suite of test cases.\n    For each case, it computes the analytic Jacobian, approximates it using finite\n    differences, and returns the Frobenius norm of their difference.\n    \"\"\"\n    test_cases = [\n        {\n            \"B\": np.array([\n                [1.0, 0.0, 0.0],\n                [0.0, 1.0, 0.0],\n                [0.0, 0.0, 1.0],\n                [0.0, 0.0, 0.0]\n            ]),\n            \"x\": np.array([1.0, -2.0, 0.5, 3.0])\n        },\n        {\n            \"B\": np.array([\n                [2.0, -1.0],\n                [1.0, 1.0]\n            ]),\n            \"x\": np.array([0.3, -1.2])\n        },\n        {\n            \"B\": np.array([\n                [10.0, 0.0, 0.0],\n                [0.0, 1e-3, 0.0],\n                [0.0, 0.0, 5.0],\n                [0.1, 0.2, 0.3],\n                [1.0, -1.0, 2.0]\n            ]),\n            \"x\": np.array([1.0, 2.0, 3.0, 4.0, 5.0])\n        }\n    ]\n\n    epsilon = 1e-6\n    results = []\n\n    for case in test_cases:\n        B = case[\"B\"]\n        x = case[\"x\"]\n        m, n = B.shape\n\n        # Step 1: Compute the analytic Jacobian\n        # J_analytic = (B^T B)^-1 B^T\n        # We solve (B^T B) * J_analytic = B^T to avoid explicit inversion.\n        A = B.T @ B\n        C = B.T\n        J_analytic = solve(A, C, assume_a='pos') # B^T B is positive definite\n\n        # Step 2: Compute y*(x) by solving normal equations\n        # This is not directly needed for the base x, but the function to do so\n        # will be used repeatedly in the finite difference approximation.\n        def get_y_star(x_vec):\n            # Solves (B^T B) y* = B^T x_vec\n            rhs = B.T @ x_vec\n            return solve(A, rhs, assume_a='pos')\n\n        # Step 3: Approximate the Jacobian using central finite differences\n        J_fd = np.zeros((n, m))\n        for i in range(m):\n            e_i = np.zeros(m)\n            e_i[i] = 1.0\n            \n            x_plus = x + epsilon * e_i\n            x_minus = x - epsilon * e_i\n            \n            y_plus = get_y_star(x_plus)\n            y_minus = get_y_star(x_minus)\n            \n            # i-th column of the finite-difference Jacobian\n            J_fd[:, i] = (y_plus - y_minus) / (2 * epsilon)\n\n        # Step 4: Compute the Frobenius norm of the difference\n        diff_norm = np.linalg.norm(J_analytic - J_fd, 'fro')\n        results.append(diff_norm)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve_problem()\n```", "id": "3102819"}]}