{"hands_on_practices": [{"introduction": "The greedy algorithm provides a powerful and efficient baseline for maximizing monotone submodular functions, but it is not always optimal. This exercise [@problem_id:3189733] invites you to implement the standard greedy algorithm for the classic facility location problem and then explore a common heuristic for improving its solution: a single-swap local search. By implementing both and analyzing the trade-off between the improvement in solution quality and the additional computational cost, you will gain practical insight into designing hybrid optimization strategies.", "problem": "You are given a family of set functions arising from the facility location objective in submodular optimization. Let a finite ground set be denoted by $\\mathcal{V} = \\{0,1,\\dots,n-1\\}$, and let a nonnegative weight matrix be denoted by $W \\in \\mathbb{R}_{\\ge 0}^{n \\times m}$. For any subset $S \\subseteq \\mathcal{V}$, the facility location objective is defined by\n$$\nf(S) \\;=\\; \\sum_{j=0}^{m-1} \\max_{i \\in S} W_{i,j},\n$$\nwith the convention $f(\\varnothing) = 0$. The task is to select a subset $S$ of fixed cardinality $k$ that makes $f(S)$ large.\n\nFoundations:\n- A set function $f: 2^{\\mathcal{V}} \\to \\mathbb{R}$ is called monotone if for all $A \\subseteq B \\subseteq \\mathcal{V}$ we have $f(A) \\le f(B)$, and submodular if for all $A \\subseteq B \\subseteq \\mathcal{V}$ and all $x \\in \\mathcal{V} \\setminus B$ we have $f(A \\cup \\{x\\}) - f(A) \\ge f(B \\cup \\{x\\}) - f(B)$.\n- The facility location objective $f$ defined above is a well-known monotone submodular function.\n\nAlgorithms to implement:\n- Greedy selection under a cardinality constraint: start from the empty set $S_0 = \\varnothing$ and for $t = 1,2,\\dots,k$ add one element $x_t \\in \\mathcal{V} \\setminus S_{t-1}$ that maximizes the marginal gain $f(S_{t-1} \\cup \\{x\\}) - f(S_{t-1})$. Use the following deterministic tie-breaking: among maximizers, pick the one with the smallest index. Let $S_{\\mathrm{greedy}}$ denote the resulting set.\n- Single best $1$-swap local improvement applied once: given $S_{\\mathrm{greedy}}$, consider all pairs $(a,b)$ with $a \\in S_{\\mathrm{greedy}}$ and $b \\in \\mathcal{V} \\setminus S_{\\mathrm{greedy}}$. For each pair, evaluate $f(S_{\\mathrm{greedy}} \\setminus \\{a\\} \\cup \\{b\\})$. Let $(a^\\star,b^\\star)$ be the pair that attains the largest value; break ties by choosing the lexicographically smallest pair $(a,b)$, that is, by the smallest $a$, and if tied, the smallest $b$. If this largest value is strictly greater than $f(S_{\\mathrm{greedy}})$, perform the swap once to obtain $S_{\\mathrm{swap}} = S_{\\mathrm{greedy}} \\setminus \\{a^\\star\\} \\cup \\{b^\\star\\}$. Otherwise, set $S_{\\mathrm{swap}} = S_{\\mathrm{greedy}}$.\n\nCost model:\n- Treat evaluation of $f$ as a unit-cost oracle call. Count an oracle call each time $f(\\cdot)$ is computed on a concrete set.\n- In the greedy algorithm, do not count $f(\\varnothing)$; for each candidate set $S \\cup \\{x\\}$ that is evaluated, count a single oracle call. Do not make redundant oracle calls; carry forward the selected candidate’s objective value to be the current set’s value for the next iteration without an extra oracle call.\n- In the single best $1$-swap stage, count exactly one oracle call per candidate swap set evaluated. Do not iterate beyond this single best swap attempt; perform at most one improving swap.\n\nEmpirical evidence task:\n- For each test instance, compute the improvement $\\Delta = f(S_{\\mathrm{swap}}) - f(S_{\\mathrm{greedy}})$ and the multiplicative oracle-cost factor\n$$\n\\varrho \\;=\\; \\frac{E_{\\mathrm{greedy}} + E_{\\mathrm{swap}}}{E_{\\mathrm{greedy}}},\n$$\nwhere $E_{\\mathrm{greedy}}$ is the number of oracle calls made by the greedy stage and $E_{\\mathrm{swap}}$ is the number of oracle calls made in the single best $1$-swap stage.\n- Report $\\Delta$ and $\\varrho$ for each test case.\n\nTest suite:\n- Case $1$: $n = 4$, $m = 6$, $k = 3$, with\n$$\nW^{(1)} \\;=\\; \\begin{bmatrix}\n5  5  5  5  5  5 \\\\\n10  10  0  0  0  0 \\\\\n0  0  9  9  0  0 \\\\\n0  0  0  0  9  9\n\\end{bmatrix}.\n$$\n- Case $2$: $n = 4$, $m = 4$, $k = 2$, with all rows identical,\n$$\nW^{(2)} \\;=\\; \\begin{bmatrix}\n3  3  3  3 \\\\\n3  3  3  3 \\\\\n3  3  3  3 \\\\\n3  3  3  3\n\\end{bmatrix}.\n$$\n- Case $3$: $n = 4$, $m = 5$, $k = 2$, with\n$$\nW^{(3)} \\;=\\; \\begin{bmatrix}\n5  5  5  5  5 \\\\\n10  10  0  0  0 \\\\\n0  0  9  9  0 \\\\\n0  0  0  0  9\n\\end{bmatrix}.\n$$\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain, in order, for Case $1$ then Case $2$ then Case $3$, the pairs $[\\Delta,\\varrho]$ flattened into a single list. Concretely, output\n$$\n[\\Delta^{(1)},\\varrho^{(1)},\\Delta^{(2)},\\varrho^{(2)},\\Delta^{(3)},\\varrho^{(3)}].\n$$\n- All reported values must be real numbers (floating-point). No other text may be printed.", "solution": "The user wants to analyze the performance of a greedy algorithm and a subsequent local search improvement step for the facility location problem, which is a classic example of maximizing a monotone submodular function under a cardinality constraint.\n\n### Step 1: Problem Validation\n\n**1. Extraction of Givens**\n\n- **Ground Set**: A finite set $\\mathcal{V} = \\{0, 1, \\dots, n-1\\}$.\n- **Weight Matrix**: A non-negative matrix $W \\in \\mathbb{R}_{\\ge 0}^{n \\times m}$.\n- **Objective Function**: For any subset $S \\subseteq \\mathcal{V}$, the facility location objective is $f(S) = \\sum_{j=0}^{m-1} \\max_{i \\in S} W_{i,j}$, with $f(\\varnothing) = 0$.\n- **Constraint**: The chosen subset $S$ must have a fixed cardinality, $|S|=k$.\n- **Foundations**: The function $f$ is stated to be monotone and submodular.\n- **Greedy Algorithm**:\n    - Start with $S_0 = \\varnothing$.\n    - For $t=1, \\dots, k$, select $x_t \\in \\mathcal{V} \\setminus S_{t-1}$ that maximizes the marginal gain $f(S_{t-1} \\cup \\{x\\}) - f(S_{t-1})$.\n    - Tie-breaking: Among maximizers, pick the one with the smallest index. The resulting set is $S_{\\mathrm{greedy}}$.\n- **Single Best 1-Swap Algorithm**:\n    - Given $S_{\\mathrm{greedy}}$, consider all pairs $(a,b)$ with $a \\in S_{\\mathrm{greedy}}$ and $b \\in \\mathcal{V} \\setminus S_{\\mathrm{greedy}}$.\n    - Find the pair $(a^\\star, b^\\star)$ that maximizes $f(S_{\\mathrm{greedy}} \\setminus \\{a\\} \\cup \\{b\\})$.\n    - Tie-breaking: Choose the lexicographically smallest pair $(a,b)$.\n    - If $f(S_{\\mathrm{greedy}} \\setminus \\{a^\\star\\} \\cup \\{b^\\star\\})  f(S_{\\mathrm{greedy}})$, set $S_{\\mathrm{swap}} = S_{\\mathrm{greedy}} \\setminus \\{a^\\star\\} \\cup \\{b^\\star\\}$.\n    - Otherwise, $S_{\\mathrm{swap}} = S_{\\mathrm{greedy}}$.\n- **Cost Model**:\n    - Evaluation of $f(\\cdot)$ is one unit-cost oracle call.\n    - Greedy stage: $f(\\varnothing)$ is not counted. Each candidate set $S \\cup \\{x\\}$ evaluation counts as one call. The value $f(S_t)$ is carried over from the evaluation of the best candidate in step $t$. $E_{\\mathrm{greedy}}$ is the total number of calls.\n    - Swap stage: Each candidate swap set evaluation counts as one call. $E_{\\mathrm{swap}}$ is the total number of calls.\n- **Empirical Evidence Task**:\n    - Compute the improvement $\\Delta = f(S_{\\mathrm{swap}}) - f(S_{\\mathrm{greedy}})$.\n    - Compute the oracle-cost factor $\\varrho = (E_{\\mathrm{greedy}} + E_{\\mathrm{swap}}) / E_{\\mathrm{greedy}}$.\n- **Test Suite**:\n    - Case 1: $n=4, m=6, k=3$, $W^{(1)} = \\begin{bmatrix} 555555 \\\\ 10100000 \\\\ 009900 \\\\ 000099 \\end{bmatrix}$.\n    - Case 2: $n=4, m=4, k=2$, $W^{(2)} = \\begin{bmatrix} 3333 \\\\ 3333 \\\\ 3333 \\\\ 3333 \\end{bmatrix}$.\n    - Case 3: $n=4, m=5, k=2$, $W^{(3)} = \\begin{bmatrix} 55555 \\\\ 1010000 \\\\ 00990 \\\\ 00009 \\end{bmatrix}$.\n- **Output Format**: A single line `[Δ(1),ϱ(1),Δ(2),ϱ(2),Δ(3),ϱ(3)]`.\n\n**2. Validation using Extracted Givens**\n\nThe problem statement is a well-defined exercise in algorithm implementation and analysis within the standard framework of submodular optimization.\n- **Scientifically Grounded**: The problem is based on the facility location function, a canonical example of a monotone submodular function. The algorithms (greedy and local search) are standard, widely studied methods for this class of problems.\n- **Well-Posed**: The problem is specified with complete information. The objective function, algorithms, constraints, and tie-breaking rules are all defined precisely, ensuring a unique, deterministic outcome for each test case.\n- **Objective**: The problem is stated using formal mathematical language and is devoid of any subjective or ambiguous terminology.\n\n**3. Verdict and Action**\n\nThe problem is **valid**. It is self-contained, scientifically sound, and well-posed. I will proceed with the full solution.\n\n### Step 2: Solution Derivation\n\nThe solution requires implementing the facility location objective function, the greedy selection algorithm, and the single best $1$-swap local search procedure. We will apply these to each test case and compute the required metrics, $\\Delta$ and $\\varrho$.\n\n**Objective Function Oracle**\n\nThe function $f(S)$ for a given set $S \\subseteq \\mathcal{V}$ and weight matrix $W$ is $f(S) = \\sum_{j=0}^{m-1} \\max_{i \\in S} W_{i,j}$. An oracle for this function will be implemented to take a set of row indices $S$ and the matrix $W$ as input. If $S$ is empty, it returns $0$. Otherwise, it selects the rows of $W$ corresponding to indices in $S$, finds the maximum value in each column of this submatrix, and returns the sum of these maximums. Each call to this oracle will be counted.\n\n**Greedy Algorithm**\n\nThe greedy algorithm constructs a set $S_{\\mathrm{greedy}}$ of size $k$ iteratively.\n1.  Initialize $S_0 = \\varnothing$ and $f(S_0)=0$.\n2.  For each iteration $t$ from $1$ to $k$:\n    a. Let $S_{t-1}$ be the set from the previous iteration. The set of available elements is $\\mathcal{V} \\setminus S_{t-1}$.\n    b. For each element $x \\in \\mathcal{V} \\setminus S_{t-1}$, calculate the marginal gain $g(x) = f(S_{t-1} \\cup \\{x\\}) - f(S_{t-1})$. This requires $|\\mathcal{V} \\setminus S_{t-1}| = n - (t-1)$ oracle calls.\n    c. Find the maximum gain, $g_{\\max} = \\max_{x \\in \\mathcal{V} \\setminus S_{t-1}} g(x)$.\n    d. Identify the set of maximizers, $X_{\\max} = \\{x \\in \\mathcal{V} \\setminus S_{t-1} | g(x) = g_{\\max}\\}$.\n    e. Apply the tie-breaking rule: select the element $x_t = \\min(X_{\\max})$.\n    f. Update the set $S_t = S_{t-1} \\cup \\{x_t\\}$. The value $f(S_t)$ is known from the evaluation in step 2b and is carried forward.\n\nThe total number of oracle calls for the greedy stage is $E_{\\mathrm{greedy}} = \\sum_{t=1}^{k} (n - (t-1)) = \\sum_{i=0}^{k-1} (n - i) = nk - \\frac{k(k-1)}{2}$.\n\n**Single Best 1-Swap Local Search**\n\nAfter obtaining $S_{\\mathrm{greedy}}$ and $f(S_{\\mathrm{greedy}})$, we perform a single local search improvement step.\n1.  Identify the set of elements to swap in, $B = \\mathcal{V} \\setminus S_{\\mathrm{greedy}}$, and the set of elements to swap out, $A = S_{\\mathrm{greedy}}$.\n2.  For every pair $(a, b)$ where $a \\in A$ and $b \\in B$, form a candidate set $S'_{a,b} = (S_{\\mathrm{greedy}} \\setminus \\{a\\}) \\cup \\{b\\}$.\n3.  Evaluate $f(S'_{a,b})$ for each pair. This requires $|A| \\times |B| = k(n-k)$ oracle calls. This is the value of $E_{\\mathrm{swap}}$.\n4.  Find the maximum objective value achieved by any swap: $f_{\\max} = \\max_{a \\in A, b \\in B} f(S'_{a,b})$.\n5.  If $f_{\\max} \\le f(S_{\\mathrm{greedy}})$, no improvement is found. Set $S_{\\mathrm{swap}} = S_{\\mathrm{greedy}}$ and $f(S_{\\mathrm{swap}}) = f(S_{\\mathrm{greedy}})$.\n6.  If $f_{\\max}  f(S_{\\mathrm{greedy}})$, an improvement exists.\n    a. Identify the set of best pairs: $P_{\\max} = \\{(a,b) | a \\in A, b \\in B, f(S'_{a,b}) = f_{\\max}\\}$.\n    b. Apply the tie-breaking rule: choose the lexicographically smallest pair $(a^\\star, b^\\star)$ from $P_{\\max}$.\n    c. Perform the swap: $S_{\\mathrm{swap}} = (S_{\\mathrm{greedy}} \\setminus \\{a^\\star\\}) \\cup \\{b^\\star\\}$, and set $f(S_{\\mathrm{swap}}) = f_{\\max}$.\n\n**Calculations for Test Cases**\n\nLet's apply this procedure to each case.\n\n**Case 1:** $n=4, k=3$.\n- **Greedy Stage**:\n    - $S_0=\\varnothing, f(S_0)=0$.\n    - Iteration 1: Candidates $\\{0,1,2,3\\}$. $f(\\{0\\})=30, f(\\{1\\})=20, f(\\{2\\})=18, f(\\{3\\})=18$. Max gain from $x_1=0$. $S_1=\\{0\\}, f(S_1)=30$. Calls: $4$.\n    - Iteration 2: Candidates $\\{1,2,3\\}$. $f(\\{0,1\\})=40, f(\\{0,2\\})=38, f(\\{0,3\\})=38$. Max gain from $x_2=1$. $S_2=\\{0,1\\}, f(S_2)=40$. Calls: $3$.\n    - Iteration 3: Candidates $\\{2,3\\}$. $f(\\{0,1,2\\})=48, f(\\{0,1,3\\})=48$. Max gain is a tie. Smallest index is $2$. $x_3=2$. $S_3=\\{0,1,2\\}, f(S_3)=48$. Calls: $2$.\n    - Result: $S_{\\mathrm{greedy}} = \\{0,1,2\\}$, $f(S_{\\mathrm{greedy}}) = 48$. $E_{\\mathrm{greedy}} = 4+3+2=9$.\n- **Swap Stage**:\n    - Swaps $(a,b)$ for $a \\in \\{0,1,2\\}, b \\in \\{3\\}$.\n    - Evaluate $f(\\{1,2,3\\})$ for $(a,b)=(0,3)$: $\\sum \\max(W_{1j},W_{2j},W_{3j}) = 10+10+9+9+9+9=56$.\n    - Evaluate $f(\\{0,2,3\\})$ for $(a,b)=(1,3)$: $\\sum \\max(W_{0j},W_{2j},W_{3j}) = 5+5+9+9+9+9=46$.\n    - Evaluate $f(\\{0,1,3\\})$ for $(a,b)=(2,3)$: $\\sum \\max(W_{0j},W_{1j},W_{3j}) = 10+10+5+5+9+9=48$.\n    - Max value is $56$, for pair $(0,3)$. This is $ 48$.\n    - Result: $S_{\\mathrm{swap}} = \\{1,2,3\\}$, $f(S_{\\mathrm{swap}}) = 56$. $E_{\\mathrm{swap}}=3$.\n- **Metrics**:\n    - $\\Delta^{(1)} = 56 - 48 = 8$.\n    - $\\varrho^{(1)} = (9+3)/9 = 12/9 \\approx 1.3333$.\n\n**Case 2:** $n=4, k=2$. $W$ has all entries equal to $3$. For any $S \\neq \\varnothing, f(S) = \\sum_{j=0}^3 3 = 12$.\n- **Greedy Stage**:\n    - $S_0=\\varnothing, f(S_0)=0$.\n    - Iteration 1: Candidates $\\{0,1,2,3\\}$. $f(\\{0\\})=f(\\{1\\})=f(\\{2\\})=f(\\{3\\})=12$. All have max gain. Tie-break: $x_1=0$. $S_1=\\{0\\}, f(S_1)=12$. Calls: $4$.\n    - Iteration 2: Candidates $\\{1,2,3\\}$. $f(\\{0,1\\})=f(\\{0,2\\})=f(\\{0,3\\})=12$. Gains are all $0$. Tie-break: $x_2=1$. $S_2=\\{0,1\\}, f(S_2)=12$. Calls: $3$.\n    - Result: $S_{\\mathrm{greedy}} = \\{0,1\\}$, $f(S_{\\mathrm{greedy}}) = 12$. $E_{\\mathrm{greedy}} = 4+3=7$.\n- **Swap Stage**:\n    - Swaps $(a,b)$ for $a \\in \\{0,1\\}, b \\in \\{2,3\\}$. Any swap results in a set of size $2$, so its function value is $12$.\n    - Max value is $12$, which is not strictly greater than $f(S_{\\mathrm{greedy}})=12$. No swap is performed.\n    - Result: $S_{\\mathrm{swap}} = S_{\\mathrm{greedy}} = \\{0,1\\}$, $f(S_{\\mathrm{swap}}) = 12$. $E_{\\mathrm{swap}}=k(n-k)=2(4-2)=4$.\n- **Metrics**:\n    - $\\Delta^{(2)} = 12 - 12 = 0$.\n    - $\\varrho^{(2)} = (7+4)/7 = 11/7 \\approx 1.5714$.\n\n**Case 3:** $n=4, k=2$.\n- **Greedy Stage**:\n    - $S_0=\\varnothing, f(S_0)=0$.\n    - Iteration 1: Candidates $\\{0,1,2,3\\}$. $f(\\{0\\})=25, f(\\{1\\})=20, f(\\{2\\})=18, f(\\{3\\})=9$. Max gain from $x_1=0$. $S_1=\\{0\\}, f(S_1)=25$. Calls: $4$.\n    - Iteration 2: Candidates $\\{1,2,3\\}$. $f(\\{0,1\\})=35$ (gain 10), $f(\\{0,2\\})=33$ (gain 8), $f(\\{0,3\\})=29$ (gain 4). Max gain from $x_2=1$. $S_2=\\{0,1\\}, f(S_2)=35$. Calls: $3$.\n    - Result: $S_{\\mathrm{greedy}} = \\{0,1\\}$, $f(S_{\\mathrm{greedy}}) = 35$. $E_{\\mathrm{greedy}} = 4+3=7$.\n- **Swap Stage**:\n    - Swaps $(a,b)$ for $a \\in \\{0,1\\}, b \\in \\{2,3\\}$.\n    - Evaluate $f(\\{1,2\\})$ for $(a,b)=(0,2)$: $10+10+9+9+0 = 38$.\n    - Evaluate $f(\\{1,3\\})$ for $(a,b)=(0,3)$: $10+10+0+0+9=29$.\n    - Evaluate $f(\\{0,2\\})$ for $(a,b)=(1,2)$: $5+5+9+9+5 = 33$.\n    - Evaluate $f(\\{0,3\\})$ for $(a,b)=(1,3)$: $5+5+5+5+9=29$.\n    - Max value is $38$, for pair $(0,2)$. This is $ 35$.\n    - Result: $S_{\\mathrm{swap}} = \\{1,2\\}$, $f(S_{\\mathrm{swap}}) = 38$. $E_{\\mathrm{swap}}=k(n-k)=2(4-2)=4$.\n- **Metrics**:\n    - $\\Delta^{(3)} = 38 - 35 = 3$.\n    - $\\varrho^{(3)} = (7+4)/7 = 11/7 \\approx 1.5714$.\n\nFinal results to be reported:\n$[\\Delta^{(1)}, \\varrho^{(1)}, \\Delta^{(2)}, \\varrho^{(2)}, \\Delta^{(3)}, \\varrho^{(3)}] = [8.0, 1.3333..., 0.0, 1.5714..., 3.0, 1.5714...]$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    It orchestrates the execution of algorithms on each test case and prints the final result.\n    \"\"\"\n\n    def f_oracle(S, W):\n        \"\"\"\n        Computes the facility location objective function value f(S).\n        This is the core oracle function.\n        \n        Args:\n            S (set): A subset of the ground set {0, ..., n-1}.\n            W (np.ndarray): The weight matrix.\n            \n        Returns:\n            float: The value of f(S).\n        \"\"\"\n        # The convention is f(emptyset) = 0\n        if not S:\n            return 0.0\n        \n        # Select rows corresponding to indices in S\n        s_list = list(S)\n        \n        # Compute max over selected rows (axis=0) for each column\n        # and sum the results.\n        return np.sum(np.max(W[s_list, :], axis=0))\n\n    def run_case(W, k):\n        \"\"\"\n        Runs the greedy and 1-swap algorithms for a single test case.\n        \n        Args:\n            W (np.ndarray): The n x m weight matrix.\n            k (int): The cardinality constraint.\n            \n        Returns:\n            tuple: A pair (delta, rho) containing the improvement and oracle cost factor.\n        \"\"\"\n        n, _ = W.shape\n        V = set(range(n))\n\n        # ---- Greedy Algorithm ----\n        S_current = set()\n        f_current = 0.0\n        E_greedy = 0\n\n        for _ in range(k):\n            candidates = V - S_current\n            \n            # Store gains and resulting f-values for all candidates\n            candidate_evals = {}\n            for x in candidates:\n                S_candidate = S_current | {x}\n                f_val = f_oracle(S_candidate, W)\n                E_greedy += 1\n                gain = f_val - f_current\n                candidate_evals[x] = {'gain': gain, 'f_val': f_val}\n\n            # Find the maximum gain\n            max_gain = -np.inf\n            if candidate_evals:\n                max_gain = max(v['gain'] for v in candidate_evals.values())\n\n            # Find all elements that achieve the maximum gain\n            maximizers = [x for x, v in candidate_evals.items() if v['gain'] == max_gain]\n            \n            # Tie-break by choosing the smallest index\n            best_x = min(maximizers)\n            \n            # Update the set and its function value\n            S_current.add(best_x)\n            f_current = candidate_evals[best_x]['f_val']\n        \n        S_greedy = S_current\n        f_greedy = f_current\n\n        # ---- Single Best 1-Swap Local Improvement ----\n        E_swap = 0\n        S_outside = V - S_greedy\n        \n        # Store results of all possible 1-swaps\n        swap_evals = {}\n        for a in S_greedy:\n            for b in S_outside:\n                S_candidate = (S_greedy - {a}) | {b}\n                f_val = f_oracle(S_candidate, W)\n                E_swap += 1\n                swap_evals[(a, b)] = f_val\n        \n        # Find the maximum function value among all swaps\n        max_f_swap = -np.inf\n        if swap_evals:\n            max_f_swap = max(swap_evals.values())\n\n        S_swap = S_greedy\n        f_swap = f_greedy\n\n        # Check for strict improvement\n        if max_f_swap  f_greedy:\n            # Find all pairs that achieve the maximum value\n            best_pairs = [pair for pair, val in swap_evals.items() if val == max_f_swap]\n            \n            # Tie-break by lexicographically smallest pair (a,b)\n            best_pairs.sort()\n            a_star, b_star = best_pairs[0]\n            \n            S_swap = (S_greedy - {a_star}) | {b_star}\n            f_swap = max_f_swap\n\n        # ---- Compute Metrics ----\n        delta = f_swap - f_greedy\n        \n        if E_greedy == 0:\n             # handle k=0 case, although not in test suite\n            rho = 1.0 if E_swap == 0 else np.inf\n        else:\n            rho = (E_greedy + E_swap) / E_greedy\n\n        return delta, rho\n\n    # --- Test Suite ---\n    test_cases = [\n        # Case 1\n        (\n            np.array([\n                [5, 5, 5, 5, 5, 5],\n                [10, 10, 0, 0, 0, 0],\n                [0, 0, 9, 9, 0, 0],\n                [0, 0, 0, 0, 9, 9]\n            ]),\n            3\n        ),\n        # Case 2\n        (\n            np.array([\n                [3, 3, 3, 3],\n                [3, 3, 3, 3],\n                [3, 3, 3, 3],\n                [3, 3, 3, 3]\n            ]),\n            2\n        ),\n        # Case 3\n        (\n            np.array([\n                [5, 5, 5, 5, 5],\n                [10, 10, 0, 0, 0],\n                [0, 0, 9, 9, 0],\n                [0, 0, 0, 0, 9]\n            ]),\n            2\n        )\n    ]\n    \n    # --- Run and Collect Results ---\n    results_flat = []\n    for W, k in test_cases:\n        delta, rho = run_case(W, k)\n        results_flat.extend([float(delta), float(rho)])\n\n    # --- Print Final Output ---\n    # Format: [d1,r1,d2,r2,d3,r3]\n    result_str = \",\".join(map(str, results_flat))\n    print(f\"[{result_str}]\")\n\nsolve()\n```", "id": "3189733"}, {"introduction": "Many real-world optimization problems, like the $k$-median clustering problem, are not submodular and are computationally very difficult to solve exactly. A powerful technique in submodular optimization is to approximate such problems using a submodular proxy function. This practice [@problem_id:3189779] guides you through this process by first asking you to prove that the facility location function is indeed submodular, and then to empirically test how well it serves as a proxy for finding high-quality solutions to the $k$-median problem.", "problem": "Consider a finite ground set $V = \\{1,2,\\dots,n\\}$, and a distance function $d: V \\times V \\to \\mathbb{R}_{\\ge 0}$ that satisfies the axioms of a metric: for all $i,j,k \\in V$, $d(i,j) = 0$ if and only if $i=j$, $d(i,j) = d(j,i)$, and $d(i,k) \\le d(i,j) + d(j,k)$. For any selection $S \\subseteq V$ with $|S| = k$, the $k$-median cost is defined by the set function $C(S) = \\sum_{j \\in V} \\min_{i \\in S} d(i,j)$.\n\nDefine a nonnegative similarity matrix $W \\in \\mathbb{R}_{\\ge 0}^{n \\times n}$ with entries $w_{ij}$ obtained from distances $d(i,j)$ by any fixed, nonincreasing transformation that maps larger distances to smaller or equal similarities, for instance $w_{ij} = \\max(0, r - d(i,j))$ for a chosen radius $r  0$. The facility location function is defined as $f(S) = \\sum_{j \\in V} \\max_{i \\in S} w_{ij}$.\n\nTask 1 (Theory): Starting only from the core definitions of submodularity and the definitions above, show that $f(S)$ is a monotone submodular function on $2^V$ for any nonnegative similarity matrix $W$. Use the diminishing returns characterization of submodularity: a set function $f: 2^V \\to \\mathbb{R}$ is submodular if for all $A \\subseteq B \\subseteq V$ and $x \\in V \\setminus B$, it holds that $f(A \\cup \\{x\\}) - f(A) \\ge f(B \\cup \\{x\\}) - f(B)$. Also show monotonicity: for $A \\subseteq B$, $f(A) \\le f(B)$.\n\nTask 2 (Algorithm and Empirics): Implement the greedy algorithm for maximizing the facility location function under a cardinality constraint $|S| = k$. The algorithm should iteratively add the element $x \\in V \\setminus S$ with the largest marginal gain $f(S \\cup \\{x\\}) - f(S)$ until $|S| = k$. Using synthetic metrics generated from points in the Euclidean plane, compare the $k$-median cost $C(S)$ of the set $S$ returned by the greedy facility location algorithm to the optimal $k$-median solution obtained by exhaustive search over all $\\binom{n}{k}$ subsets. Quantify the approximation quality empirically as the ratio $R = C(S_{\\mathrm{FL}})/C(S_{\\mathrm{OPT}})$, where $S_{\\mathrm{FL}}$ is the greedy facility location solution and $S_{\\mathrm{OPT}}$ is the exact $k$-median minimizer. No physical units are involved, and angles do not appear, so no unit specification is required.\n\nTest Suite and Parameters:\n- Use Euclidean distances in $\\mathbb{R}^2$, i.e., for points with coordinates $(x_i, y_i)$, define $d(i,j) = \\sqrt{(x_i - x_j)^2 + (y_i - y_j)^2}$.\n- For the similarity transformation, use $w_{ij} = \\max(0, r - d(i,j))$ with the specified $r$ in each case.\n- Provide three test cases that cover the happy path, boundary, and edge situations:\n  1. Case A (happy path): $n = 12$, two well-separated clusters centered near $(0,0)$ and $(5,5)$, $k = 2$, $r = 3.0$.\n  2. Case B (boundary case): $n = 12$, a single compact cluster near $(0,0)$, $k = 1$, $r = 1.5$.\n  3. Case C (edge case): $n = 9$, three moderately separated clusters near $(0,0)$, $(4,0)$, and $(0,4)$, $k = 3$, $r = 2.0$.\n\nProgram Requirements:\n- The program must construct the specified synthetic point sets, compute the pairwise distance matrix $d(i,j)$, build the similarity matrix $W$ via $w_{ij} = \\max(0, r - d(i,j))$, run the greedy facility location algorithm to obtain $S_{\\mathrm{FL}}$, compute $C(S_{\\mathrm{FL}})$, compute the exact optimal $k$-median solution $S_{\\mathrm{OPT}}$ by exhaustive search along with $C(S_{\\mathrm{OPT}})$, and then compute the ratio $R = C(S_{\\mathrm{FL}})/C(S_{\\mathrm{OPT}})$ for each test case.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\"), where each result corresponds to the ratio $R$ for the respective test case. Each result must be a float.\n\nThe implementation must be self-contained and deterministic, requiring no user input, external files, or network access. The only allowed libraries are the Python standard library and the specified numerical libraries. Ensure the synthetic datasets are constructed with fixed coordinates so that results are reproducible across runs.", "solution": "The problem statement has been critically validated and is deemed to be scientifically grounded, well-posed, objective, and complete. It presents a standard theoretical exercise and a constrained empirical task within the established field of submodular optimization. The tasks are computationally feasible and the parameters are specified sufficiently for a deterministic and reproducible implementation.\n\n### **Task 1: Theoretical Analysis**\n\nThe task is to demonstrate that the facility location function $f(S)$ is monotone and submodular. The function is defined over the subsets of a ground set $V = \\{1, 2, \\dots, n\\}$ as $f(S) = \\sum_{j \\in V} \\max_{i \\in S} w_{ij}$, where $W$ is a matrix of non-negative similarities, $w_{ij} \\ge 0$.\n\nWe can express the function $f(S)$ as a sum of simpler functions. For each element $j \\in V$, let us define a function $g_j(S): 2^V \\to \\mathbb{R}$ as:\n$$\ng_j(S) = \\max_{i \\in S} w_{ij}\n$$\nWith this definition, the facility location function can be written as:\n$$\nf(S) = \\sum_{j \\in V} g_j(S)\n$$\nThe properties of monotonicity and submodularity are closed under non-negative linear combinations. Therefore, if we can show that each function $g_j(S)$ is both monotone and submodular, it will follow that $f(S)$ is also monotone and submodular, as it is a sum of such functions. By convention, the maximum over an empty set of non-negative numbers is taken to be $0$, so $g_j(\\emptyset) = 0$ for all $j \\in V$, which implies $f(\\emptyset) = 0$.\n\n**1. Proof of Monotonicity**\n\nA function $f$ is monotone if for any subsets $A \\subseteq B \\subseteq V$, it holds that $f(A) \\le f(B)$. We will show this for each $g_j(S)$.\n\nLet $A \\subseteq B \\subseteq V$. For any fixed $j \\in V$, the set of similarities $\\{w_{ij} \\mid i \\in A\\}$ is a subset of $\\{w_{ij} \\mid i \\in B\\}$. The maximum of a set of numbers is always less than or equal to the maximum of any of its supersets. Thus,\n$$\n\\max_{i \\in A} w_{ij} \\le \\max_{i \\in B} w_{ij}\n$$\nThis is equivalent to $g_j(A) \\le g_j(B)$. Since this inequality holds for all $j \\in V$, summing over all $j$ preserves the inequality:\n$$\n\\sum_{j \\in V} g_j(A) \\le \\sum_{j \\in V} g_j(B)\n$$\nThis directly yields $f(A) \\le f(B)$, which proves that the facility location function $f(S)$ is monotone.\n\n**2. Proof of Submodularity**\n\nA function $f$ is submodular if it exhibits the property of diminishing returns. That is, for any subsets $A \\subseteq B \\subseteq V$ and any element $x \\in V \\setminus B$, the marginal gain of adding $x$ to the smaller set $A$ is greater than or equal to the marginal gain of adding it to the larger set $B$:\n$$\nf(A \\cup \\{x\\}) - f(A) \\ge f(B \\cup \\{x\\}) - f(B)\n$$\nAgain, we first prove this property for each component function $g_j(S)$. The marginal gain of adding an element $x \\notin S$ to a set $S$ for the function $g_j$ is:\n$$\n\\Delta_{g_j}(x | S) = g_j(S \\cup \\{x\\}) - g_j(S) = \\max_{i \\in S \\cup \\{x\\}} w_{ij} - \\max_{i \\in S} w_{ij}\n$$\nUsing the property $\\max(a, b) - b = \\max(0, a-b)$, we can rewrite the marginal gain as:\n$$\n\\Delta_{g_j}(x | S) = \\max(w_{xj}, \\max_{i \\in S} w_{ij}) - \\max_{i \\in S} w_{ij} = \\max(0, w_{xj} - \\max_{i \\in S} w_{ij})\n$$\nNow, let $A \\subseteq B \\subseteq V$ and $x \\in V \\setminus B$. From the monotonicity of $g_j$ that we have just proven, we know that $g_j(A) \\le g_j(B)$, which means:\n$$\n\\max_{i \\in A} w_{ij} \\le \\max_{i \\in B} w_{ij}\n$$\nLet's analyze the function $h(z) = \\max(0, c - z)$ for some constant $c$. This function is non-increasing in its argument $z$. Let $c = w_{xj}$. Then the marginal gains for sets $A$ and $B$ can be written as:\n$$\n\\Delta_{g_j}(x | A) = \\max(0, w_{xj} - \\max_{i \\in A} w_{ij})\n$$\n$$\n\\Delta_{g_j}(x | B) = \\max(0, w_{xj} - \\max_{i \\in B} w_{ij})\n$$\nSince $\\max_{i \\in A} w_{ij} \\le \\max_{i \\in B} w_{ij}$ and the function $h(z)$ is non-increasing, it follows that:\n$$\n\\max(0, w_{xj} - \\max_{i \\in A} w_{ij}) \\ge \\max(0, w_{xj} - \\max_{i \\in B} w_{ij})\n$$\nThis establishes that $\\Delta_{g_j}(x | A) \\ge \\Delta_{g_j}(x | B)$. As this holds for each $j \\in V$, summing the marginal gains over all $j$ gives:\n$$\n\\sum_{j \\in V} \\Delta_{g_j}(x | A) \\ge \\sum_{j \\in V} \\Delta_{g_j}(x | B)\n$$\nRecognizing that $\\Delta_f(x | S) = \\sum_{j \\in V} \\Delta_{g_j}(x | S)$, we have $\\Delta_f(x | A) \\ge \\Delta_f(x | B)$, which is precisely the submodularity condition. This completes the proof that $f(S)$ is a monotone submodular function for any non-negative similarity matrix $W$.\n\n### **Task 2: Algorithm and Empirical Analysis**\n\nThe empirical task requires comparing the solution from a greedy maximization of the facility location function with the optimal solution for the $k$-median problem.\n\n**Algorithm Design**\n\n1.  **Greedy Facility Location (FL) Algorithm**: To find the set $S_{\\mathrm{FL}}$ that provides a good solution for maximizing $f(S)$ under a cardinality constraint $|S|=k$, we use the standard greedy algorithm. This algorithm is known to provide a $(1 - 1/e)$-approximation for maximizing a monotone submodular function.\n    - Initialize the solution set $S$ to be empty.\n    - Repeat $k$ times:\n        - For each element $x$ not already in $S$, calculate the marginal gain $\\Delta_f(x|S) = f(S \\cup \\{x\\}) - f(S)$.\n        - Select the element $x^*$ that provides the maximum marginal gain.\n        - Add $x^*$ to $S$.\n    - The final set $S$ is our solution, $S_{\\mathrm{FL}}$.\n\n2.  **Optimal k-Median Algorithm**: To find the true optimal set of centers $S_{\\mathrm{OPT}}$ that minimizes the $k$-median cost $C(S) = \\sum_{j \\in V} \\min_{i \\in S} d(i,j)$, we perform an exhaustive search.\n    - Generate all possible subsets of $V$ of size $k$. The number of such subsets is $\\binom{n}{k}$.\n    - For each subset, calculate its $k$-median cost $C(S)$.\n    - The subset with the minimum cost is the optimal solution, $S_{\\mathrm{OPT}}$, and its cost is $C(S_{\\mathrm{OPT}})$.\n\n**Implementation Plan**\n\nThe implementation follows the problem specification. Synthetic datasets are constructed using hardcoded coordinates to ensure reproducibility. For each test case:\n1.  A set of $n$ points in $\\mathbb{R}^2$ is defined.\n2.  The pairwise Euclidean distance matrix $D$ is computed, where $d_{ij} = d(i,j)$.\n3.  The similarity matrix $W$ is computed using the transformation $w_{ij} = \\max(0, r - d_{ij})$.\n4.  The greedy algorithm for facility location is run to produce the set $S_{\\mathrm{FL}}$.\n5.  The $k$-median cost of this set, $C(S_{\\mathrm{FL}})$, is calculated using the distance matrix $D$.\n6.  The exhaustive search is performed to find the optimal $k$-median cost, $C(S_{\\mathrm{OPT}})$.\n7.  The ratio $R = C(S_{\\mathrm{FL}}) / C(S_{\\mathrm{OPT}})$ is computed and stored. This ratio quantifies how well the facility location proxy objective performs in finding a good solution for the $k$-median problem. A ratio close to $1.0$ indicates high quality.\n\nThe final program implements this logic for the three specified test cases and outputs the resulting ratios.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.spatial.distance import cdist\nfrom itertools import combinations\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and print the results.\n    \"\"\"\n    # Define the test cases with fixed coordinates for reproducibility.\n    test_cases = [\n        {\n            \"name\": \"Case A (happy path)\",\n            \"n\": 12, \"k\": 2, \"r\": 3.0,\n            \"points\": np.array([\n                # Cluster 1 (near (0,0))\n                [0.1, 0.2], [-0.3, 0.1], [0.0, -0.4], [0.5, 0.3], [-0.2, -0.2], [0.4, -0.1],\n                # Cluster 2 (near (5,5))\n                [5.1, 5.2], [4.8, 5.3], [5.0, 4.7], [5.5, 5.1], [4.9, 4.8], [5.2, 5.5]\n            ])\n        },\n        {\n            \"name\": \"Case B (boundary case)\",\n            \"n\": 12, \"k\": 1, \"r\": 1.5,\n            \"points\": np.array([\n                # Single compact cluster near (0,0)\n                [0.1, 0.2], [-0.3, 0.1], [0.0, -0.4], [0.5, 0.3], [-0.2, -0.2], [0.4, -0.1],\n                [-0.5, 0.5], [0.6, -0.6], [0.2, 0.7], [-0.1, -0.5], [0.8, 0.0], [-0.6, 0.0]\n            ])\n        },\n        {\n            \"name\": \"Case C (edge case)\",\n            \"n\": 9, \"k\": 3, \"r\": 2.0,\n            \"points\": np.array([\n                # Cluster 1 (near (0,0))\n                [0.1, 0.1], [-0.2, 0.0], [0.0, -0.3],\n                # Cluster 2 (near (4,0))\n                [4.1, 0.2], [3.8, -0.1], [4.0, 0.0],\n                # Cluster 3 (near (0,4))\n                [0.1, 4.1], [-0.1, 3.9], [0.2, 4.3]\n            ])\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        ratio = run_case(case)\n        results.append(ratio)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef calculate_k_median_cost(S, D):\n    \"\"\"Calculates the k-median cost C(S) for a given set S and distance matrix D.\"\"\"\n    if not S:\n        return np.inf\n    # For each point j, find the minimum distance to a center in S.\n    min_distances = np.min(D[S, :], axis=0)\n    return np.sum(min_distances)\n\ndef run_case(case_params):\n    \"\"\"\n    Runs a single test case: computes S_FL via greedy algorithm, S_OPT via\n    exhaustive search, and returns the ratio of their k-median costs.\n    \"\"\"\n    points = case_params[\"points\"]\n    n = case_params[\"n\"]\n    k = case_params[\"k\"]\n    r = case_params[\"r\"]\n\n    # Compute pairwise Euclidean distance matrix D\n    D = cdist(points, points, 'euclidean')\n\n    # Compute similarity matrix W\n    W = np.maximum(0, r - D)\n\n    # --- Greedy Algorithm for Facility Location ---\n    S_fl = []\n    candidates = list(range(n))\n    # Keep track of the maximum similarity from S_fl to each point j\n    current_max_sim = np.zeros(n)\n\n    for _ in range(k):\n        best_gain = -1.0\n        best_candidate = -1\n        \n        for x in candidates:\n            # Efficiently calculate marginal gain: sum(max(0, w_xj - max_sim_j))\n            gains_vector = np.maximum(0, W[x, :] - current_max_sim)\n            gain = np.sum(gains_vector)\n\n            if gain  best_gain:\n                best_gain = gain\n                best_candidate = x\n\n        if best_candidate != -1:\n            S_fl.append(best_candidate)\n            candidates.remove(best_candidate)\n            # Update the vector of maximum similarities\n            current_max_sim = np.maximum(current_max_sim, W[best_candidate, :])\n\n    # Calculate the k-median cost of the greedy facility location solution\n    C_fl = calculate_k_median_cost(S_fl, D)\n\n    # --- Exhaustive Search for Optimal k-Median ---\n    min_C_opt = np.inf\n    S_opt = None\n\n    for s_candidate_tuple in combinations(range(n), k):\n        s_candidate = list(s_candidate_tuple)\n        c_val = calculate_k_median_cost(s_candidate, D)\n        if c_val  min_C_opt:\n            min_C_opt = c_val\n            S_opt = s_candidate\n    \n    C_opt = min_C_opt\n    \n    # --- Compute the approximation ratio ---\n    if C_opt == 0:\n        # This case should not happen with the given test data,\n        # but as a safeguard, if the optimal cost is 0, the greedy\n        # solution should also be optimal.\n        return 1.0 if C_fl == 0 else np.inf\n        \n    ratio = C_fl / C_opt\n    return ratio\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3189779"}, {"introduction": "While many optimization problems involve objectives that only increase as we add elements, some scenarios include penalties or negative interactions, leading to non-monotone submodular functions. This exercise [@problem_id:3189759] moves beyond monotone functions to explore this more complex landscape, where the myopic nature of a greedy algorithm can be a significant drawback. You will implement and compare a greedy approach against a local search algorithm, constructing specific instances to reveal the distinct strengths and failure modes of each method when faced with non-monotone objectives and general matroid constraints.", "problem": "You are given a family of finite combinatorial optimization instances that combine a non-monotone submodular objective with a matroid constraint. Your task is to implement and compare a greedy algorithm and a local search algorithm, and to construct specific instances where each algorithm outperforms the other. All mathematical symbols, variables, constants, and numbers must be interpreted in the standard algebraic sense.\n\nFundamental base:\n- A set function $f: 2^{V} \\to \\mathbb{R}$ is called submodular if for all $A \\subseteq B \\subseteq V$ and all $e \\in V \\setminus B$, it satisfies the diminishing returns property $f(A \\cup \\{e\\}) - f(A) \\ge f(B \\cup \\{e\\}) - f(B)$.\n- A matroid is a pair $(V, \\mathcal{I})$ where $V$ is a finite ground set and $\\mathcal{I} \\subseteq 2^{V}$ is a nonempty collection of independent sets satisfying heredity and the exchange property. A partition matroid arises by partitioning $V$ into disjoint parts and setting an upper bound on the number of elements that can be selected from each part.\n\nProblem setting:\n- Ground set: $V = \\{0,1,2,3\\}$.\n- Partition matroid: Elements are assigned to parts by a map $g: V \\to \\{0,1\\}$ with capacities $b(0) = 1$ and $b(1) = 1$. Thus, a feasible set $S$ must satisfy $\\lvert \\{i \\in S: g(i) = p\\} \\rvert \\le b(p)$ for each part $p \\in \\{0,1\\}$.\n- Non-monotone submodular objective: For a subset $S \\subseteq V$, define\n$$\nf(S) = \\sum_{i \\in S} w_i \\;-\\; \\sum_{\\{i,j\\} \\subseteq S} c_{ij},\n$$\nwhere $w_i \\in \\mathbb{R}$ are item weights and $c_{ij} \\in \\mathbb{R}_{\\ge 0}$ are symmetric pairwise penalties with $c_{ij} = c_{ji}$ and $c_{ii} = 0$. Note that the second sum is over unordered pairs $\\{i,j\\}$ with $i \\ne j$. Because all $c_{ij} \\ge 0$, the function $f$ is submodular and generally non-monotone.\n\nAlgorithms to implement:\n- Greedy (max-positive-marginal): Initialize $S \\gets \\emptyset$. Repeatedly select an element $e \\in V \\setminus S$ that is feasible to add (i.e., $S \\cup \\{e\\} \\in \\mathcal{I}$) and maximizes the marginal gain $\\Delta(e \\mid S) = f(S \\cup \\{e\\}) - f(S)$. Break ties by choosing the minimum index. If the maximum marginal gain is strictly positive, add that element to $S$ and continue; otherwise, stop and return $S$.\n- Local search (first-improvement with add/delete/swap): Initialize $S \\gets \\emptyset$. Repeat the following loop until no improvement is found:\n  1. Add move: scan $e \\in V \\setminus S$ in increasing index order. If $S \\cup \\{e\\} \\in \\mathcal{I}$ and $\\Delta(e \\mid S)  0$, set $S \\gets S \\cup \\{e\\}$ and restart the loop.\n  2. Delete move: scan $e \\in S$ in increasing index order. If $f(S \\setminus \\{e\\}) - f(S)  0$, set $S \\gets S \\setminus \\{e\\}$ and restart the loop.\n  3. Swap move: scan pairs $(o,i)$ with $o \\in S$ and $i \\in V \\setminus S$ in lexicographic order by indices. If $(S \\setminus \\{o\\}) \\cup \\{i\\} \\in \\mathcal{I}$ and $f((S \\setminus \\{o\\}) \\cup \\{i\\}) - f(S)  0$, perform the swap $S \\gets (S \\setminus \\{o\\}) \\cup \\{i\\}$ and restart the loop.\nReturn $S$ when the loop terminates.\n\nTest suite:\nImplement the two algorithms and evaluate them on the following three instances. In all cases, use the same index order $0,1,2,3$ for tie-breaking and scanning as specified above.\n\n- Test case $1$ (local search outperforms greedy): \n  - Group assignments: $g(0) = 0$, $g(1) = 0$, $g(2) = 1$, $g(3) = 1$ with capacities $b(0) = 1$, $b(1) = 1$.\n  - Weights: $(w_0, w_1, w_2, w_3) = (6, 10, 5, 7)$.\n  - Nonzero penalties: $c_{1,2} = 6$, $c_{1,3} = 8$, $c_{0,2} = 2$, $c_{0,3} = 1$. All other $c_{ij} = 0$ for $i \\ne j$.\n  - Expected qualitative behavior: the greedy algorithm stops with a single element due to non-positive marginals for any addition, whereas local search can use swaps to reach a strictly better two-element solution.\n\n- Test case $2$ (greedy outperforms local search due to first-improvement getting stuck): \n  - Group assignments: $g(0) = 0$, $g(1) = 0$, $g(2) = 1$, $g(3) = 1$ with capacities $b(0) = 1$, $b(1) = 1$.\n  - Weights: $(w_0, w_1, w_2, w_3) = (7, 9, 7, 9)$.\n  - Nonzero penalties: $c_{1,3} = 4$, $c_{0,2} = 4$, $c_{1,2} = 8$, $c_{0,3} = 8$. All other $c_{ij} = 0$ for $i \\ne j$.\n  - Expected qualitative behavior: the greedy algorithm builds a high-value compatible pair, while the local search with first-improvement locks into a lower-value compatible pair and cannot perform a non-improving intermediate swap, hence it gets stuck.\n\n- Test case $3$ (boundary case with non-positive singleton values): \n  - Group assignments: $g(0) = 0$, $g(1) = 0$, $g(2) = 1$, $g(3) = 1$ with capacities $b(0) = 1$, $b(1) = 1$.\n  - Weights: $(w_0, w_1, w_2, w_3) = (-1, -2, -3, -1)$.\n  - All penalties $c_{ij} = 0$ for $i \\ne j$.\n  - Expected qualitative behavior: both algorithms return the empty set with value $0$.\n\nRequired computations and output:\n- For each test case $t \\in \\{1,2,3\\}$, compute $S_{\\mathrm{greedy}}^{(t)}$ and $S_{\\mathrm{LS}}^{(t)}$, then the values $f(S_{\\mathrm{greedy}}^{(t)})$ and $f(S_{\\mathrm{LS}}^{(t)})$, and finally the integer difference\n$$\nd^{(t)} = f(S_{\\mathrm{LS}}^{(t)}) - f(S_{\\mathrm{greedy}}^{(t)}).\n$$\n- Your program should produce a single line of output containing the three results as a comma-separated list enclosed in square brackets (for example, $\\;[d^{(1)},d^{(2)},d^{(3)}]\\;$). The outputs must be integers with no additional whitespace or text.\n\nNo physical units or angles are involved. All outputs are pure integers. Ensure that all algorithmic steps respect the matroid independence constraints exactly as specified, and use strictly positive-improvement moves to proceed in both algorithms.", "solution": "The problem requires the implementation and comparison of a greedy algorithm and a local search algorithm for a non-monotone submodular maximization problem subject to a partition matroid constraint. We must evaluate these algorithms on three specific instances and calculate the difference in the objective values they achieve.\n\nThe ground set is $V = \\{0, 1, 2, 3\\}$. The objective function is $f(S) = \\sum_{i \\in S} w_i - \\sum_{\\{i,j\\} \\subseteq S, i \\ne j} c_{ij}$ for a given set $S \\subseteq V$, where $w_i$ are weights and $c_{ij} \\ge 0$ are symmetric penalties. The function $f$ is submodular.\n\nThe constraint is a partition matroid $(V, \\mathcal{I})$ defined by the partition of $V$ into two parts, $P_0$ and $P_1$, with capacities $b(0)=1$ and $b(1)=1$. A set $S$ is independent, i.e., $S \\in \\mathcal{I}$, if it contains at most one element from each part. For all test cases, the partition is $P_0 = \\{i \\in V \\mid g(i)=0\\} = \\{0, 1\\}$ and $P_1 = \\{i \\in V \\mid g(i)=1\\} = \\{2, 3\\}$.\n\nWe will now analyze each test case by tracing the execution of both algorithms.\n\n### Test Case 1\n\n**Instance Parameters:**\n- Weights: $(w_0, w_1, w_2, w_3) = (6, 10, 5, 7)$.\n- Nonzero penalties: $c_{1,2} = 6$, $c_{1,3} = 8$, $c_{0,2} = 2$, $c_{0,3} = 1$.\n\n**Greedy Algorithm Trace:**\n1.  Initialize $S = \\emptyset$, so $f(S) = 0$.\n2.  **Iteration 1:** We evaluate the marginal gain $\\Delta(e \\mid \\emptyset) = f(\\{e\\}) - f(\\emptyset) = w_e$ for each $e \\in V$:\n    - $\\Delta(0 \\mid \\emptyset) = w_0 = 6$.\n    - $\\Delta(1 \\mid \\emptyset) = w_1 = 10$.\n    - $\\Delta(2 \\mid \\emptyset) = w_2 = 5$.\n    - $\\Delta(3 \\mid \\emptyset) = w_3 = 7$.\n    The maximum gain is $10$ for element $e=1$. Since $10  0$, we update $S \\gets \\{1\\}$. The current value is $f(\\{1\\}) = 10$.\n3.  **Iteration 2:** The current solution is $S = \\{1\\}$. We evaluate marginal gains for feasible additions. Since $g(1)=0$, we can only add an element from part $P_1 = \\{2, 3\\}$.\n    - $\\Delta(2 \\mid \\{1\\}) = f(\\{1,2\\}) - f(\\{1\\}) = (w_1+w_2-c_{1,2}) - w_1 = w_2 - c_{1,2} = 5 - 6 = -1$.\n    - $\\Delta(3 \\mid \\{1\\}) = f(\\{1,3\\}) - f(\\{1\\}) = (w_1+w_3-c_{1,3}) - w_1 = w_3 - c_{1,3} = 7 - 8 = -1$.\n    The maximum marginal gain is $-1$, which is not strictly positive. The algorithm terminates.\n\nThe final greedy solution is $S_{\\mathrm{greedy}}^{(1)} = \\{1\\}$, with value $f(S_{\\mathrm{greedy}}^{(1)}) = 10$.\n\n**Local Search Algorithm Trace:**\n1.  Initialize $S = \\emptyset$. Start loop.\n2.  **Iteration 1:** $S = \\emptyset$, $f(S)=0$.\n    - **Add move:** Scan $e \\in \\{0,1,2,3\\}$. For $e=0$, $f(\\{0\\}) = 6  f(\\emptyset)$. This is a valid first improvement.\n    - Update $S \\gets \\{0\\}$. Restart loop.\n3.  **Iteration 2:** $S = \\{0\\}$, $f(S)=6$.\n    - **Add move:** Scan $e \\in \\{1,2,3\\}$. $e=1$ is not feasible ($g(0)=g(1)=0$). For $e=2$, $S \\cup \\{2\\} = \\{0,2\\}$ is feasible. $f(\\{0,2\\}) = w_0+w_2-c_{0,2} = 6+5-2=9$. Gain is $9-6=30$. This is a valid first improvement.\n    - Update $S \\gets \\{0,2\\}$. Restart loop.\n4.  **Iteration 3:** $S = \\{0,2\\}$, $f(S)=9$.\n    - **Add move:** No element can be added.\n    - **Delete move:** Deleting $0$ yields $f(\\{2\\})=5  9$. Deleting $2$ yields $f(\\{0\\})=6  9$. No improvement.\n    - **Swap move:** Scan pairs $(o, i)$ with $o \\in \\{0,2\\}$ and $i \\in \\{1,3\\}$ lexicographically.\n        - $(0,1)$: Swap to $\\{1,2\\}$. Infeasible (elements from same partitions as initial $S=\\{0,2\\}$ swapped, but here $g(1)=0, g(2)=1$, so it is feasible). Let's check $g$: $g(0)=0, g(1)=0, g(2)=1, g(3)=1$. Swapping $0$ with $1$ gives $\\{1,2\\}$, which is feasible. $f(\\{1,2\\}) = w_1+w_2-c_{1,2} = 10+5-6=9$. Change is $9-9=0$. No improvement.\n        - $(0,3)$: Swap to $\\{2,3\\}$. Feasible. $f(\\{2,3\\}) = w_2+w_3-c_{2,3} = 5+7-0=12$. Gain is $12-9=3  0$. This is a valid first improvement.\n    - Update $S \\gets \\{2,3\\}$. Restart loop.\n5.  **Iteration 4:** $S = \\{2,3\\}$, $f(S)=12$.\n    - **Add/Delete moves:** No improvements found. (Deleting $2$ gives $f(\\{3\\})=712$; deleting $3$ gives $f(\\{2\\})=512$).\n    - **Swap move:** Scan pairs $(o,i)$ with $o \\in \\{2,3\\}, i \\in \\{0,1\\}$.\n        - $(2,0)$: Swap to $\\{0,3\\}$, feasible. $f(\\{0,3\\}) = w_0+w_3-c_{0,3} = 6+7-1=12$. Change is $0$.\n        - $(2,1)$: Swap to $\\{1,3\\}$, feasible. $f(\\{1,3\\}) = w_1+w_3-c_{1,3} = 10+7-8=9$. Change is $9-12  0$.\n        - $(3,0)$: Swap to $\\{0,2\\}$, feasible. $f(\\{0,2\\}) = w_0+w_2-c_{0,2} = 6+5-2=9$. Change is $9-12  0$.\n        - $(3,1)$: Swap to $\\{1,2\\}$, feasible. $f(\\{1,2\\}) = w_1+w_2-c_{1,2} = 10+5-6=9$. Change is $9-12  0$.\n    - No improving moves are found. The algorithm terminates.\n\nThe final local search solution is $S_{\\mathrm{LS}}^{(1)} = \\{2,3\\}$, with value $f(S_{\\mathrm{LS}}^{(1)}) = 12$.\nThe difference is $d^{(1)} = f(S_{\\mathrm{LS}}^{(1)}) - f(S_{\\mathrm{greedy}}^{(1)}) = 12 - 10 = 2$.\n\n### Test Case 2\n\n**Instance Parameters:**\n- Weights: $(w_0, w_1, w_2, w_3) = (7, 9, 7, 9)$.\n- Nonzero penalties: $c_{1,3} = 4$, $c_{0,2} = 4$, $c_{1,2} = 8$, $c_{0,3} = 8$.\n\n**Greedy Algorithm Trace:**\n1.  Initialize $S = \\emptyset$, $f(S) = 0$.\n2.  **Iteration 1:** Evaluate marginal gains $\\Delta(e \\mid \\emptyset) = w_e$:\n    - $\\Delta(0 \\mid \\emptyset) = 7$.\n    - $\\Delta(1 \\mid \\emptyset) = 9$.\n    - $\\Delta(2 \\mid \\emptyset) = 7$.\n    - $\\Delta(3 \\mid \\emptyset) = 9$.\n    The maximum gain is $9$, achieved by elements $1$ and $3$. Tie-breaking selects the minimum index, $e=1$. Update $S \\gets \\{1\\}$, $f(\\{1\\}) = 9$.\n3.  **Iteration 2:** Current solution $S = \\{1\\}$. Feasible additions are from $P_1 = \\{2,3\\}$.\n    - $\\Delta(2 \\mid \\{1\\}) = w_2 - c_{1,2} = 7 - 8 = -1$.\n    - $\\Delta(3 \\mid \\{1\\}) = w_3 - c_{1,3} = 9 - 4 = 5$.\n    The maximum gain is $5$ for element $e=3$. Since $50$, update $S \\gets \\{1,3\\}$. Value is $f(\\{1,3\\}) = w_1+w_3-c_{1,3} = 9+9-4=14$.\n4.  **Iteration 3:** Current solution $S=\\{1,3\\}$. No more elements can be added. The algorithm terminates.\n\nThe final greedy solution is $S_{\\mathrm{greedy}}^{(2)} = \\{1,3\\}$, with value $f(S_{\\mathrm{greedy}}^{(2)}) = 14$.\n\n**Local Search Algorithm Trace:**\n1.  Initialize $S = \\emptyset$. Start loop.\n2.  **Iteration 1:** $S = \\emptyset$, $f(S)=0$.\n    - **Add move:** Scan $e \\in \\{0,1,2,3\\}$. For $e=0$, $f(\\{0\\})=7  0$. First improvement.\n    - Update $S \\gets \\{0\\}$. Restart loop.\n3.  **Iteration 2:** $S = \\{0\\}$, $f(S)=7$.\n    - **Add move:** Scan $e \\in \\{1,2,3\\}$. $e=1$ is infeasible. For $e=2$, $S \\cup \\{2\\}=\\{0,2\\}$ is feasible. $f(\\{0,2\\}) = w_0+w_2-c_{0,2} = 7+7-4=10$. Gain is $10-7=30$. First improvement.\n    - Update $S \\gets \\{0,2\\}$. Restart loop.\n4.  **Iteration 3:** $S = \\{0,2\\}$, $f(S)=10$.\n    - **Add/Delete moves:** No improvements found. (Deleting $0$ gives $f(\\{2\\})=710$; deleting $2$ gives $f(\\{0\\})=710$).\n    - **Swap move:** Scan pairs $(o,i)$ lexicographically. $S=\\{0,2\\}$, $V \\setminus S = \\{1,3\\}$.\n        - $(0,1)$: Swap to $\\{1,2\\}$. Feasible. $f(\\{1,2\\}) = w_1+w_2-c_{1,2} = 9+7-8=8$. Change is $8-100$.\n        - $(0,3)$: Swap to $\\{2,3\\}$. Infeasible ($g(2)=1, g(3)=1$).\n        - $(2,1)$: Swap to $\\{0,1\\}$. Infeasible ($g(0)=0, g(1)=0$).\n        - $(2,3)$: Swap to $\\{0,3\\}$. Feasible. $f(\\{0,3\\}) = w_0+w_3-c_{0,3} = 7+9-8=8$. Change is $8-100$.\n    - No improving moves are found. The algorithm terminates.\n\nThe final local search solution is $S_{\\mathrm{LS}}^{(2)} = \\{0,2\\}$, with value $f(S_{\\mathrm{LS}}^{(2)}) = 10$.\nThe difference is $d^{(2)} = f(S_{\\mathrm{LS}}^{(2)}) - f(S_{\\mathrm{greedy}}^{(2)}) = 10 - 14 = -4$.\n\n### Test Case 3\n\n**Instance Parameters:**\n- Weights: $(w_0, w_1, w_2, w_3) = (-1, -2, -3, -1)$.\n- Nonzero penalties: All $c_{ij}=0$.\n\n**Greedy Algorithm Trace:**\n1.  Initialize $S = \\emptyset$, $f(S)=0$.\n2.  **Iteration 1:** Evaluate marginal gains $\\Delta(e \\mid \\emptyset) = w_e$:\n    - $\\Delta(0 \\mid \\emptyset) = -1$.\n    - $\\Delta(1 \\mid \\emptyset) = -2$.\n    - $\\Delta(2 \\mid \\emptyset) = -3$.\n    - $\\Delta(3 \\mid \\emptyset) = -1$.\n    The maximum gain is $-1$, which is not strictly positive. The algorithm terminates.\n\nThe final greedy solution is $S_{\\mathrm{greedy}}^{(3)} = \\emptyset$, with value $f(S_{\\mathrm{greedy}}^{(3)}) = 0$.\n\n**Local Search Algorithm Trace:**\n1.  Initialize $S = \\emptyset$. Start loop.\n2.  **Iteration 1:** $S = \\emptyset$, $f(S)=0$.\n    - **Add move:** All singleton sets have negative values, so $f(\\{e\\})  f(\\emptyset)$ for all $e$. No improving add moves.\n    - **Delete/Swap moves:** Not applicable as $S$ is empty.\n    - No improving moves are found. The algorithm terminates.\n\nThe final local search solution is $S_{\\mathrm{LS}}^{(3)} = \\emptyset$, with value $f(S_{\\mathrm{LS}}^{(3)}) = 0$.\nThe difference is $d^{(3)} = f(S_{\\mathrm{LS}}^{(3)}) - f(S_{\\mathrm{greedy}}^{(3)}) = 0 - 0 = 0$.\n\n### Summary of Differences\n\n- $d^{(1)} = 2$\n- $d^{(2)} = -4$\n- $d^{(3)} = 0$", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares greedy and local search algorithms for a submodular\n    maximization problem under a partition matroid constraint.\n    \"\"\"\n\n    test_cases = [\n        # Test case 1 (local search  greedy)\n        {\n            \"g\": [0, 0, 1, 1],\n            \"b\": [1, 1],\n            \"w\": [6, 10, 5, 7],\n            \"c_raw\": {(1, 2): 6, (1, 3): 8, (0, 2): 2, (0, 3): 1},\n        },\n        # Test case 2 (greedy  local search)\n        {\n            \"g\": [0, 0, 1, 1],\n            \"b\": [1, 1],\n            \"w\": [7, 9, 7, 9],\n            \"c_raw\": {(1, 3): 4, (0, 2): 4, (1, 2): 8, (0, 3): 8},\n        },\n        # Test case 3 (boundary case)\n        {\n            \"g\": [0, 0, 1, 1],\n            \"b\": [1, 1],\n            \"w\": [-1, -2, -3, -1],\n            \"c_raw\": {},\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        V = set(range(4))\n        w = np.array(case[\"w\"])\n        g = case[\"g\"]\n        b = case[\"b\"]\n        C = np.zeros((4, 4))\n        for (i, j), val in case[\"c_raw\"].items():\n            C[i, j] = C[j, i] = val\n\n        def f(S, w_vec, C_mat):\n            \"\"\"Calculates the objective function f(S).\"\"\"\n            if not S:\n                return 0\n            s_list = list(S)\n            val = np.sum(w_vec[s_list])\n            penalty = 0\n            for i in range(len(s_list)):\n                for j in range(i + 1, len(s_list)):\n                    penalty += C_mat[s_list[i], s_list[j]]\n            return val - penalty\n\n        def is_independent(S, g_map, b_caps):\n            \"\"\"Checks if a set S is independent under the partition matroid.\"\"\"\n            part_counts = [0] * len(b_caps)\n            for e in S:\n                part_idx = g_map[e]\n                part_counts[part_idx] += 1\n            for part_idx, count in enumerate(part_counts):\n                if count  b_caps[part_idx]:\n                    return False\n            return True\n\n        def greedy_algorithm(V_set, w_vec, C_mat, g_map, b_caps):\n            S = set()\n            while True:\n                best_e = -1\n                max_gain = -np.inf\n                \n                # Iterate through candidates sorted by index for tie-breaking\n                candidates = sorted(list(V_set - S))\n                for e in candidates:\n                    S_prime = S | {e}\n                    if is_independent(S_prime, g_map, b_caps):\n                        gain = f(S_prime, w_vec, C_mat) - f(S, w_vec, C_mat)\n                        if gain  max_gain:\n                            max_gain = gain\n                            best_e = e\n                \n                if best_e != -1 and max_gain  0:\n                    S.add(best_e)\n                else:\n                    break\n            return S\n\n        def local_search_algorithm(V_set, w_vec, C_mat, g_map, b_caps):\n            S = set()\n            while True:\n                current_f_S = f(S, w_vec, C_mat)\n                improved = False\n\n                # 1. Add moves\n                add_candidates = sorted(list(V_set - S))\n                for e in add_candidates:\n                    S_prime = S | {e}\n                    if is_independent(S_prime, g_map, b_caps):\n                        if f(S_prime, w_vec, C_mat) - current_f_S  0:\n                            S = S_prime\n                            improved = True\n                            break\n                if improved:\n                    continue\n\n                # 2. Delete moves\n                delete_candidates = sorted(list(S))\n                for e in delete_candidates:\n                    S_prime = S - {e}\n                    if f(S_prime, w_vec, C_mat) - current_f_S  0:\n                        S = S_prime\n                        improved = True\n                        break\n                if improved:\n                    continue\n\n                # 3. Swap moves\n                o_candidates = sorted(list(S))\n                i_candidates = sorted(list(V_set - S))\n                swap_pairs = []\n                for o in o_candidates:\n                    for i in i_candidates:\n                        swap_pairs.append((o, i))\n                \n                for o, i in swap_pairs:\n                    S_prime = (S - {o}) | {i}\n                    if is_independent(S_prime, g_map, b_caps):\n                        if f(S_prime, w_vec, C_mat) - current_f_S  0:\n                            S = S_prime\n                            improved = True\n                            break\n                if improved:\n                    continue\n\n                # No improvement found in a full pass\n                break\n\n            return S\n\n        S_greedy = greedy_algorithm(V, w, C, g, b)\n        f_greedy = f(S_greedy, w, C)\n        \n        S_ls = local_search_algorithm(V, w, C, g, b)\n        f_ls = f(S_ls, w, C)\n\n        diff = f_ls - f_greedy\n        results.append(int(diff))\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3189759"}]}