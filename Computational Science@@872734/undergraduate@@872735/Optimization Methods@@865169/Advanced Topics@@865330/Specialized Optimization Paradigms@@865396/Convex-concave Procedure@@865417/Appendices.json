{"hands_on_practices": [{"introduction": "To build a solid understanding of the Convex-Concave Procedure (CCP), we begin with a foundational exercise. This practice [@problem_id:3114687] uses a simple, one-dimensional 'toy' function, $f(x) = \\sin(x) + \\alpha x^2$, which, despite its simplicity, possesses multiple local minima, making it a great laboratory for exploring optimization algorithms. By deriving the DC decomposition and implementing the CCP update rule, you will gain first-hand experience with the core mechanics of the algorithm and observe how its convergence can depend on the starting point and the local curvature of the objective function.", "problem": "You are asked to implement the convex-concave procedure (CCP) for a univariate difference-of-convex (DC) minimization of the toy objective $f(x) = \\sin(x) + \\alpha x^2$, where $\\alpha \\ge 0$ is a parameter. The assignment has three parts: construct a valid DC splitting, derive the CCP update, and run the algorithm from different initializations to study convergence behavior in relation to the initial curvature. All angles used in trigonometric functions must be in radians.\n\nStart from the following foundational base:\n- The second derivative test: a twice-differentiable function $q(x)$ is convex if and only if $q''(x) \\ge 0$ for all $x$ in its domain.\n- The gradient of $\\sin(x)$ is $\\cos(x)$, and the second derivative is $-\\sin(x)$.\n- If a differentiable function $r(x)$ has a gradient that is $L$-Lipschitz continuous, i.e., $\\|\\nabla r(x) - \\nabla r(y)\\| \\le L \\|x-y\\|$ for all $x,y$, then the function $x \\mapsto \\frac{L}{2}x^2 - r(x)$ is convex, because its second derivative is bounded below by $0$.\n\nTasks to be completed by your program:\n1) Construct a DC splitting $f(x) = g(x) - h(x)$ with $g(x)$ convex and $h(x)$ convex. Use only the above foundational facts to justify your construction.\n2) Using only the definition of CCP for minimizing a DC function—namely, at iteration $k$, minimize the convex surrogate obtained by linearizing the concave part—derive the explicit iterative update for the univariate case and implement it. Your implementation must be numerically stable with a stopping criterion based on a tolerance in the decision variable, and must cap the number of iterations by a maximum iteration count to avoid non-termination.\n3) For each test case described below, run CCP and output a summary that relates the observed convergence to the initial curvature of $f$ at the starting point. Specifically, for each case, compute:\n   - The computed stationary point $x^\\star$ found by CCP.\n   - The objective value $f(x^\\star)$.\n   - A boolean indicating whether the second derivative $f''(x^\\star)  0$ (interpreted here as a local minimum certificate).\n   - A boolean indicating whether the algorithm converged under the stopping criterion.\n   - The initial curvature $f''(x_0)$ at the starting point $x_0$.\n   - A boolean indicating whether the initial curvature $f''(x_0)  0$.\n   - The number of iterations used.\n\nDefinition of quantities your program must use:\n- The function is $f(x) = \\sin(x) + \\alpha x^2$.\n- The initial curvature is $f''(x_0) = -\\sin(x_0) + 2\\alpha$.\n- The local minimum certificate is the boolean value of $f''(x^\\star)  0$.\n- The CCP iterate is obtained by solving the convex subproblem produced by the linearization of the concave part at each iteration.\n\nAngle unit requirement:\n- All trigonometric computations must use radians.\n\nTest suite:\nUse the following test cases, each specified by $(\\alpha, x_0)$, with the curvature-related quantities and CCP run computed as described.\n- Case $1$: $\\alpha = 0.1$, $x_0 = 0.1$.\n- Case $2$: $\\alpha = 0.1$, $x_0 = 2.5$.\n- Case $3$: $\\alpha = 0.6$, $x_0 = -10.0$.\n- Case $4$: $\\alpha = 0.3$, $x_0 = 4.0$.\n- Case $5$: $\\alpha = 0.5$, $x_0 = \\frac{\\pi}{2}$.\n\nAlgorithmic details to implement:\n- Use the smallest valid curvature-shifting constant $L$ that makes your DC splitting valid for all $x \\in \\mathbb{R}$.\n- Use a stopping tolerance $\\varepsilon = 10^{-10}$ on the absolute change in $x$ between iterations.\n- Use a maximum iteration cap of $10000$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case in the order listed above. Each element must itself be a list containing the $7$ quantities in the order: $x^\\star$, $f(x^\\star)$, local minimum certificate, convergence boolean, $f''(x_0)$, initial curvature positive boolean, number of iterations. For example, the overall output must have the form\n$[ [x^\\star_1,f(x^\\star_1),\\text{min}_1,\\text{conv}_1,f''(x_0^{(1)}),\\text{icurvpos}_1,\\text{iters}_1], \\ldots, [x^\\star_5,f(x^\\star_5),\\text{min}_5,\\text{conv}_5,f''(x_0^{(5)}),\\text{icurvpos}_5,\\text{iters}_5] ]$.", "solution": "The user-provided problem is valid as it is scientifically grounded in the principles of optimization theory, well-posed with a clear objective and constraints, and free of any contradictions or ambiguities. We will proceed with a full solution.\n\nThe problem asks for the implementation and analysis of the Convex-Concave Procedure (CCP) to minimize the univariate objective function $f(x) = \\sin(x) + \\alpha x^2$ for a given parameter $\\alpha \\ge 0$. The process involves three main tasks: constructing a valid Difference-of-Convex (DC) decomposition, deriving the CCP iterative update, and implementing the algorithm to study its convergence behavior on specified test cases.\n\n### Part 1: Construction of a Valid DC Splitting\n\nThe objective function is $f(x) = \\sin(x) + \\alpha x^2$. A DC splitting represents $f(x)$ as a difference of two convex functions, $f(x) = g(x) - h(x)$.\n\nThe term $\\alpha x^2$ is convex for $\\alpha \\ge 0$, as its second derivative is $2\\alpha \\ge 0$. The term $\\sin(x)$ is not convex, as its second derivative, $-\\sin(x)$, can be negative. We must decompose the function to handle this non-convex part.\n\nA standard method for creating a DC decomposition is to add and subtract a sufficiently large quadratic term, $\\frac{L}{2}x^2$, to make the components convex. We can decompose $f(x)$ as:\n$$f(x) = \\left( \\alpha x^2 + \\frac{L}{2}x^2 \\right) - \\left( \\frac{L}{2}x^2 - \\sin(x) \\right)$$\nLet us define:\n$g(x) = (\\alpha + \\frac{L}{2})x^2$\n$h(x) = \\frac{L}{2}x^2 - \\sin(x)$\n\nFor this to be a valid DC splitting, both $g(x)$ and $h(x)$ must be convex.\n\n1.  **Convexity of $g(x)$:**\n    The second derivative of $g(x)$ is $g''(x) = 2(\\alpha + \\frac{L}{2}) = 2\\alpha + L$. For $g(x)$ to be convex, we need $g''(x) \\ge 0$. Since $\\alpha \\ge 0$ and we will choose a non-negative constant $L$, this condition will be satisfied.\n\n2.  **Convexity of $h(x)$:**\n    The second derivative of $h(x)$ is $h''(x) = L + \\sin(x)$. According to the second derivative test, $h(x)$ is convex if and only if $h''(x) \\ge 0$ for all $x \\in \\mathbb{R}$. This implies we must have $L + \\sin(x) \\ge 0$ for all $x$. Since the minimum value of $\\sin(x)$ is $-1$, the condition becomes $L - 1 \\ge 0$, which means $L \\ge 1$.\n\nThe problem requires using the smallest valid constant $L$. Therefore, we choose $L = 1$.\n\nWith $L=1$:\n- $g(x) = (\\alpha + \\frac{1}{2})x^2$ is convex, since its second derivative $2\\alpha + 1 \\ge 1  0$ for $\\alpha \\ge 0$.\n- $h(x) = \\frac{1}{2}x^2 - \\sin(x)$ is convex, since its second derivative $1 + \\sin(x) \\ge 1 - 1 = 0$ for all $x$.\n\nThus, a valid DC splitting is $f(x) = g(x) - h(x)$ with the functions defined above and $L=1$.\n\n### Part 2: Derivation of the CCP Update Rule\n\nThe CCP algorithm solves the non-convex problem $\\min_x \\{g(x) - h(x)\\}$ by iteratively solving a sequence of convex subproblems. At each iteration $k$, given the current iterate $x_k$, the concave part of the objective, $-h(x)$, is replaced by its first-order Taylor approximation around $x_k$. This yields a convex surrogate function to be minimized.\n\nThe linearization of $h(x)$ at $x_k$ is $h(x_k) + \\nabla h(x_k)(x - x_k)$.\nThe CCP subproblem at iteration $k+1$ is:\n$$x_{k+1} = \\arg\\min_x \\left\\{ g(x) - \\left[ h(x_k) + \\nabla h(x_k)(x - x_k) \\right] \\right\\}$$\nThe terms $h(x_k)$ and $-\\nabla h(x_k)(-x_k)$ are constant with respect to the optimization variable $x$, so we can simplify the problem to:\n$$x_{k+1} = \\arg\\min_x \\left\\{ g(x) - x \\nabla h(x_k) \\right\\}$$\nWe need the gradient of $h(x)$:\n$\\nabla h(x) = \\frac{d}{dx} \\left(\\frac{1}{2}x^2 - \\sin(x)\\right) = x - \\cos(x)$.\nSo, $\\nabla h(x_k) = x_k - \\cos(x_k)$.\n\nThe objective for the subproblem is:\n$$J(x) = g(x) - x \\nabla h(x_k) = \\left(\\alpha + \\frac{1}{2}\\right)x^2 - (x_k - \\cos(x_k))x$$\nThis is a convex quadratic function of $x$. To find its minimizer, we set its derivative with respect to $x$ to zero:\n$$\\nabla J(x) = 2\\left(\\alpha + \\frac{1}{2}\\right)x - (x_k - \\cos(x_k)) = 0$$\n$$(2\\alpha + 1)x = x_k - \\cos(x_k)$$\nSolving for $x$ gives the explicit iterative update rule for $x_{k+1}$:\n$$x_{k+1} = \\frac{x_k - \\cos(x_k)}{2\\alpha + 1}$$\nSince $\\alpha \\ge 0$, the denominator $2\\alpha+1$ is always greater than or equal to $1$, so the update is well-defined.\n\n### Part 3: Implementation and Analysis\n\nThe derived update rule is implemented as an iterative algorithm. Starting from an initial point $x_0$, the sequence $\\{x_k\\}$ is generated until the absolute difference between successive iterates, $|x_{k+1} - x_k|$, falls below a specified tolerance $\\varepsilon = 10^{-10}$, or a maximum of $10000$ iterations is reached.\n\nFor each test case defined by a pair $(\\alpha, x_0)$, the algorithm is run to find a stationary point $x^\\star$. A stationary point of the CCP iteration is a fixed point, which must satisfy $x = \\frac{x - \\cos(x)}{2\\alpha+1}$. This simplifies to $(2\\alpha+1)x = x-\\cos(x)$, or $2\\alpha x + \\cos(x) = 0$. This condition is identical to setting the gradient of the original function, $f'(x) = 2\\alpha x + \\cos(x)$, to zero. Thus, the CCP algorithm correctly converges to the stationary points of $f(x)$.\n\nAfter convergence, the following quantities are calculated and reported for each test case:\n- The computed stationary point, $x^\\star$.\n- The objective value at the stationary point, $f(x^\\star) = \\sin(x^\\star) + \\alpha(x^\\star)^2$.\n- A boolean certificate for being a local minimum, evaluated as $f''(x^\\star)  0$. The second derivative is $f''(x) = -\\sin(x) + 2\\alpha$.\n- A boolean flag indicating if the algorithm converged within the maximum number of iterations.\n- The initial curvature at the starting point, $f''(x_0) = -\\sin(x_0) + 2\\alpha$.\n- A boolean flag indicating if the initial curvature at $x_0$ is positive.\n- The number of iterations performed.\n\nThe implementation will apply this procedure to all specified test cases and format the results as requested.", "answer": "```python\nimport numpy as np\n\ndef run_ccp(alpha, x0, tol=1e-10, max_iter=10000):\n    \"\"\"\n    Implements the Convex-Concave Procedure (CCP) for the given objective.\n    \n    Args:\n        alpha (float): The parameter for the objective function.\n        x0 (float): The initial starting point.\n        tol (float): The tolerance for the stopping criterion.\n        max_iter (int): The maximum number of iterations.\n        \n    Returns:\n        list: A list containing the 7 required output quantities.\n    \"\"\"\n    x_k = x0\n    converged = False\n    iters = 0\n\n    for i in range(max_iter):\n        x_k_plus_1 = (x_k - np.cos(x_k)) / (2 * alpha + 1)\n        iters = i + 1\n\n        if np.abs(x_k_plus_1 - x_k)  tol:\n            converged = True\n            x_k = x_k_plus_1\n            break\n        \n        x_k = x_k_plus_1\n    \n    # In case of non-convergence, x_k is the last computed value.\n    if not converged:\n        iters = max_iter\n\n    x_star = x_k\n\n    # Calculate required metrics\n    f_x_star = np.sin(x_star) + alpha * x_star**2\n    f_double_prime_x_star = -np.sin(x_star) + 2 * alpha\n    is_local_min = bool(f_double_prime_x_star  0)\n\n    f_double_prime_x0 = -np.sin(x0) + 2 * alpha\n    is_initial_curvature_pos = bool(f_double_prime_x0  0)\n\n    return [\n        x_star,\n        f_x_star,\n        is_local_min,\n        converged,\n        f_double_prime_x0,\n        is_initial_curvature_pos,\n        iters\n    ]\n\ndef solve():\n    \"\"\"\n    Runs the CCP solver for all test cases and prints the formatted output.\n    \"\"\"\n    test_cases = [\n        (0.1, 0.1),\n        (0.1, 2.5),\n        (0.6, -10.0),\n        (0.3, 4.0),\n        (0.5, np.pi / 2.0)\n    ]\n\n    results = []\n    for alpha, x0 in test_cases:\n        result = run_ccp(alpha, x0)\n        # Ensure booleans are lowercase 'true'/'false' for perfect format matching if needed,\n        # but Python's default `str(bool)` is 'True'/'False' and is acceptable.\n        # Example: list_str = f\"[{','.join(str(item).lower() if isinstance(item, bool) else str(item) for item in result)}]\"\n        results.append(result)\n\n    # Format the final output string according to the strict specification.\n    # The format requires a string representation of a list of lists with no spaces.\n    formatted_results = []\n    for case_result in results:\n        # Convert each item in the sublist to a string\n        str_items = [str(item) for item in case_result]\n        # Join them with commas\n        list_str = ','.join(str_items)\n        # Enclose in brackets\n        formatted_results.append(f\"[{list_str}]\")\n    \n    # Join the formatted sublists with commas\n    main_content = ','.join(formatted_results)\n    \n    # Enclose the entire string in brackets\n    final_output = f\"[{main_content}]\"\n    \n    print(final_output)\n\n# Run the solver\nsolve()\n\n```", "id": "3114687"}, {"introduction": "Moving from foundational concepts to practical applications, this exercise [@problem_id:3114735] explores how CCP can solve problems in machine learning, specifically for model quantization. The goal is to encourage the weights of a model to take on discrete values, like $-1$ or $1$, which can drastically reduce model size and computational cost. You will learn to formulate a clever difference-of-convex (DC) decomposition for a penalty function that promotes this behavior and implement the CCP algorithm to observe how the weights \"snap\" to the desired values during optimization.", "problem": "Consider the following penalized least-squares problem in the context of optimization methods. You are given a matrix $A \\in \\mathbb{R}^{m \\times n}$, a vector $b \\in \\mathbb{R}^{m}$, and a regularization parameter $\\lambda \\in \\mathbb{R}_{+}$. Define the objective function\n$$\nF(w) \\triangleq \\frac{1}{2}\\|A w - b\\|_{2}^{2} + \\lambda \\sum_{i=1}^{n} \\min\\{(w_i - 1)^2,\\,(w_i + 1)^2\\},\n$$\nwhere $w \\in \\mathbb{R}^{n}$ is the decision variable. The scalar regularization encourages each coordinate $w_i$ to align with the quantization set $\\{-1,\\,1\\}$ by penalizing the squared distance to the closest of the two points $-1$ and $1$.\n\nYou must solve this problem using the Convex-Concave Procedure (CCP), which is a standard method for minimizing a difference-of-convex (DC) function. The fundamental bases required are: the definition of convexity and subgradients, the concept of DC decomposition (writing a nonconvex function as the difference of two convex functions), and the first-order optimality conditions for convex functions. Explicit formulas specialized to this problem are not provided and must be derived by you from first principles.\n\nTasks:\n- Starting from the definition of convexity and the absolute value function, derive a valid difference-of-convex decomposition of each term $\\min\\{(w_i - 1)^2,\\,(w_i + 1)^2\\}$ and use it to construct a DC decomposition of the full objective $F(w)$.\n- Using first-order optimality conditions and the subgradient of the convex component in the DC decomposition, derive the convex subproblem that is solved at each CCP iteration. Express the iteration update rule for $w$ in terms of the subgradient evaluated at the current iterate.\n- Implement the CCP algorithm that starts from a specified initial $w^{(0)}$, chooses a subgradient consistent with the absolute value function (assigning the subgradient $0$ at coordinates equal to $0$), and solves the convex subproblem exactly at each iteration. Use the stopping rule: terminate when the relative change $\\|w^{(t+1)} - w^{(t)}\\|_{2} / \\max\\{\\|w^{(t)}\\|_{2},\\,1\\}$ is strictly less than $10^{-6}$, or when the number of iterations reaches $100$, whichever occurs first.\n- Define that a coordinate $w_i$ is considered “snapped” to the quantization set if $| |w_i| - 1 | \\le 10^{-3}$.\n- For each test case, produce three outputs: the integer count of snapped coordinates, the final objective value $F(w)$ rounded to $6$ decimal places, and a boolean indicating whether the algorithm met the stopping criterion before reaching the iteration limit.\n\nYou must implement a complete, standalone program that performs this computation for the test suite specified below and produces a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case’s result is itself a list of the form $[k,f,\\text{converged}]$ with $k$ an integer, $f$ a float rounded to $6$ decimals, and $\\text{converged}$ a boolean.\n\nTest Suite:\n- Case $1$ (happy path): $m=8$, $n=4$, \n  $A=\\begin{bmatrix}\n  0.5  -0.2  0.1  0.0 \\\\\n  1.0  0.3  -0.4  0.2 \\\\\n  -0.7  0.5  0.0  -0.1 \\\\\n  0.0  -0.6  0.8  0.3 \\\\\n  0.2  0.1  -0.5  -0.9 \\\\\n  0.3  0.7  0.9  -0.4 \\\\\n  -0.4  -0.2  0.6  0.5 \\\\\n  0.6  -0.1  -0.3  0.7\n  \\end{bmatrix}$,\n  $b=\\begin{bmatrix}0.1 \\\\ -0.5 \\\\ 0.3 \\\\ -0.2 \\\\ 0.8 \\\\ -0.9 \\\\ 0.0 \\\\ 0.4\\end{bmatrix}$, $\\lambda=1.0$, $w^{(0)}=\\begin{bmatrix}0.0 \\\\ 0.0 \\\\ 0.0 \\\\ 0.0\\end{bmatrix}$.\n- Case $2$ (strong quantization pressure): same $A$ and $b$ as Case $1$, $\\lambda=100.0$, $w^{(0)}=\\begin{bmatrix}0.0 \\\\ 0.0 \\\\ 0.0 \\\\ 0.0\\end{bmatrix}$.\n- Case $3$ (zero data term, immediate snapping behavior): $m=6$, $n=3$, $A$ is the $6 \\times 3$ zero matrix, $b=\\begin{bmatrix}0.0 \\\\ 0.0 \\\\ 0.0 \\\\ 0.0 \\\\ 0.0 \\\\ 0.0\\end{bmatrix}$, $\\lambda=0.5$, $w^{(0)}=\\begin{bmatrix}0.2 \\\\ -0.3 \\\\ 0.8\\end{bmatrix}$.\n- Case $4$ (weak quantization pressure): same $A$ and $b$ as Case $1$, $\\lambda=0.01$, $w^{(0)}=\\begin{bmatrix}0.0 \\\\ 0.0 \\\\ 0.0 \\\\ 0.0\\end{bmatrix}$.\n- Case $5$ (one-dimensional boundary case): $m=5$, $n=1$, $A=\\begin{bmatrix}1.0 \\\\ -0.5 \\\\ 0.3 \\\\ 0.7 \\\\ -0.2\\end{bmatrix}$, $b=\\begin{bmatrix}0.2 \\\\ -0.1 \\\\ 0.0 \\\\ 1.0 \\\\ -0.5\\end{bmatrix}$, $\\lambda=0.5$, $w^{(0)}=\\begin{bmatrix}0.0\\end{bmatrix}$.\n\nAngle units are not applicable. No physical units are involved. The final program output must be a single line string of the form\n$[\\,[k_1,f_1,\\text{converged}_1],\\,[k_2,f_2,\\text{converged}_2],\\,\\ldots\\,]$,\nwith no spaces inside the inner lists. The float $f_i$ must be rounded to exactly $6$ decimal places.", "solution": "The problem requires the minimization of a nonconvex objective function using the Convex-Concave Procedure (CCP). The derivation and implementation steps are outlined below.\n\n### 1. Difference-of-Convex (DC) Decomposition\n\nThe objective function is given by\n$$ F(w) \\triangleq \\frac{1}{2}\\|A w - b\\|_{2}^{2} + \\lambda \\sum_{i=1}^{n} \\min\\{(w_i - 1)^2,\\,(w_i + 1)^2\\} $$\nTo apply CCP, we first need to express $F(w)$ as a difference of two convex functions, $F(w) = G(w) - H(w)$.\n\nLet's analyze the penalty term $P(w_i) = \\min\\{(w_i - 1)^2, (w_i + 1)^2\\}$. We determine which of the two squared terms is smaller by examining the sign of their difference:\n$$ (w_i - 1)^2 - (w_i + 1)^2 = (w_i^2 - 2w_i + 1) - (w_i^2 + 2w_i + 1) = -4w_i $$\n- If $w_i \\ge 0$, then $-4w_i \\le 0$, which implies $(w_i - 1)^2 \\le (w_i + 1)^2$.\n- If $w_i  0$, then $-4w_i  0$, which implies $(w_i - 1)^2  (w_i + 1)^2$.\n\nTherefore, the penalty term can be written piecewise as:\n$$ P(w_i) = \\begin{cases} (w_i - 1)^2  \\text{if } w_i \\ge 0 \\\\ (w_i + 1)^2  \\text{if } w_i  0 \\end{cases} $$\nThis piecewise function can be represented by a single expression using the absolute value function, $|w_i|$. Consider the expression $w_i^2 - 2|w_i| + 1$:\n- If $w_i \\ge 0$, it becomes $w_i^2 - 2w_i + 1 = (w_i - 1)^2$.\n- If $w_i  0$, it becomes $w_i^2 - 2(-w_i) + 1 = w_i^2 + 2w_i + 1 = (w_i + 1)^2$.\nThis matches the piecewise definition of $P(w_i)$. Thus, we have the identity $P(w_i) = w_i^2 - 2|w_i| + 1$.\n\nSubstituting this back into the objective function $F(w)$:\n$$ F(w) = \\frac{1}{2}\\|A w - b\\|_{2}^{2} + \\lambda \\sum_{i=1}^{n} (w_i^2 - 2|w_i| + 1) $$\nWe can rearrange and group terms to separate the convex and concave parts:\n$$ F(w) = \\left( \\frac{1}{2}\\|A w - b\\|_{2}^{2} + \\lambda \\sum_{i=1}^{n} (w_i^2 + 1) \\right) - \\left( 2\\lambda \\sum_{i=1}^{n} |w_i| \\right) $$\nThis provides a DC decomposition $F(w) = G(w) - H(w)$, where:\n- $G(w) = \\frac{1}{2}\\|A w - b\\|_{2}^{2} + \\lambda \\|w\\|_{2}^{2} + n\\lambda$.\n- $H(w) = 2\\lambda \\|w\\|_{1}$.\n\nThe function $G(w)$ is the sum of a convex quadratic term ($\\frac{1}{2}\\|A w - b\\|_{2}^{2}$), another convex quadratic term ($\\lambda \\|w\\|_{2}^{2}$), and a constant. The sum of convex functions is convex. The Hessian matrix of $G(w)$ is $\\nabla^2 G(w) = A^T A + 2\\lambda I$. Since $A^T A$ is positive semi-definite and, for $\\lambda  0$, $2\\lambda I$ is positive definite, their sum is positive definite. Hence, $G(w)$ is strictly convex.\nThe function $H(w)$ is a positively scaled L1-norm of $w$, which is a standard convex function.\n\n### 2. CCP Iteration Update Rule\n\nThe CCP algorithm minimizes $F(w) = G(w) - H(w)$ by iteratively solving a convex majorization of the objective. At iteration $t+1$, the function $H(w)$ is replaced by its first-order Taylor approximation around the current iterate $w^{(t)}$. Since $H(w)$ is non-differentiable at points where some $w_i=0$, we use a subgradient $v^{(t)} \\in \\partial H(w^{(t)})$. The convex majorization of $F(w)$ at $w^{(t)}$ is:\n$$ \\hat{F}(w; w^{(t)}) = G(w) - \\left( H(w^{(t)}) + \\langle v^{(t)}, w - w^{(t)} \\rangle \\right) $$\nThe next iterate $w^{(t+1)}$ is found by minimizing this convex function:\n$$ w^{(t+1)} = \\arg\\min_w \\hat{F}(w; w^{(t)}) = \\arg\\min_w \\{ G(w) - \\langle v^{(t)}, w \\rangle \\} $$\nThe subgradient of $H(w) = 2\\lambda \\sum_{i=1}^n |w_i|$ is a vector $v$ with components $v_i \\in \\partial(2\\lambda|w_i|) = 2\\lambda \\cdot \\partial|w_i|$. The subdifferential of the absolute value function is $\\partial|x| = \\{\\text{sgn}(x)\\}$ if $x \\neq 0$ and the interval $[-1, 1]$ if $x=0$. As per the problem statement, we must choose the subgradient value of $0$ at coordinates equal to $0$, which corresponds to defining $\\text{sgn}(0)=0$. Thus, the specific subgradient vector $v^{(t)}$ has components $v_i^{(t)} = 2\\lambda \\cdot \\text{sgn}(w^{(t)}_i)$.\n\nThe minimization subproblem at each iteration is:\n$$ \\min_w \\left\\{ \\frac{1}{2}\\|A w - b\\|_{2}^{2} + \\lambda\\|w\\|_2^2 - (v^{(t)})^T w \\right\\} $$\nThis is an unconstrained convex optimization problem. We find the minimum by setting the gradient of the objective with respect to $w$ to zero. Let the objective of the subproblem be $J(w)$.\n$$ J(w) = \\frac{1}{2}w^T A^T A w - b^T A w + \\frac{1}{2}b^T b + \\lambda w^T w - (v^{(t)})^T w $$\nThe gradient is:\n$$ \\nabla_w J(w) = A^T A w - A^T b + 2\\lambda w - v^{(t)} $$\nSetting the gradient to zero yields the linear system:\n$$ (A^T A + 2\\lambda I) w = A^T b + v^{(t)} $$\nSince $A^T A + 2\\lambda I$ is positive definite for $\\lambda  0$, it is invertible. The update rule for $w^{(t+1)}$ is the solution to this system:\n$$ w^{(t+1)} = (A^T A + 2\\lambda I)^{-1} (A^T b + v^{(t)}) $$\nwhere $v^{(t)}$ is the vector with components $v_i^{(t)} = 2\\lambda \\cdot \\text{sgn}(w_i^{(t)})$. This equation forms the core of the iterative algorithm.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves a series of penalized least-squares problems using the\n    Convex-Concave Procedure (CCP).\n    \"\"\"\n\n    test_cases = [\n        # Case 1 (happy path)\n        {\n            \"A\": np.array([\n                [0.5, -0.2, 0.1, 0.0],\n                [1.0, 0.3, -0.4, 0.2],\n                [-0.7, 0.5, 0.0, -0.1],\n                [0.0, -0.6, 0.8, 0.3],\n                [0.2, 0.1, -0.5, -0.9],\n                [0.3, 0.7, 0.9, -0.4],\n                [-0.4, -0.2, 0.6, 0.5],\n                [0.6, -0.1, -0.3, 0.7]\n            ]),\n            \"b\": np.array([0.1, -0.5, 0.3, -0.2, 0.8, -0.9, 0.0, 0.4]),\n            \"lambda\": 1.0,\n            \"w0\": np.array([0.0, 0.0, 0.0, 0.0])\n        },\n        # Case 2 (strong quantization pressure)\n        {\n            \"A\": np.array([\n                [0.5, -0.2, 0.1, 0.0],\n                [1.0, 0.3, -0.4, 0.2],\n                [-0.7, 0.5, 0.0, -0.1],\n                [0.0, -0.6, 0.8, 0.3],\n                [0.2, 0.1, -0.5, -0.9],\n                [0.3, 0.7, 0.9, -0.4],\n                [-0.4, -0.2, 0.6, 0.5],\n                [0.6, -0.1, -0.3, 0.7]\n            ]),\n            \"b\": np.array([0.1, -0.5, 0.3, -0.2, 0.8, -0.9, 0.0, 0.4]),\n            \"lambda\": 100.0,\n            \"w0\": np.array([0.0, 0.0, 0.0, 0.0])\n        },\n        # Case 3 (zero data term)\n        {\n            \"A\": np.zeros((6, 3)),\n            \"b\": np.zeros(6),\n            \"lambda\": 0.5,\n            \"w0\": np.array([0.2, -0.3, 0.8])\n        },\n        # Case 4 (weak quantization pressure)\n        {\n            \"A\": np.array([\n                [0.5, -0.2, 0.1, 0.0],\n                [1.0, 0.3, -0.4, 0.2],\n                [-0.7, 0.5, 0.0, -0.1],\n                [0.0, -0.6, 0.8, 0.3],\n                [0.2, 0.1, -0.5, -0.9],\n                [0.3, 0.7, 0.9, -0.4],\n                [-0.4, -0.2, 0.6, 0.5],\n                [0.6, -0.1, -0.3, 0.7]\n            ]),\n            \"b\": np.array([0.1, -0.5, 0.3, -0.2, 0.8, -0.9, 0.0, 0.4]),\n            \"lambda\": 0.01,\n            \"w0\": np.array([0.0, 0.0, 0.0, 0.0])\n        },\n        # Case 5 (one-dimensional)\n        {\n            \"A\": np.array([1.0, -0.5, 0.3, 0.7, -0.2]).reshape(-1, 1),\n            \"b\": np.array([0.2, -0.1, 0.0, 1.0, -0.5]),\n            \"lambda\": 0.5,\n            \"w0\": np.array([0.0])\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        A = case[\"A\"]\n        b = case[\"b\"]\n        lmbda = case[\"lambda\"]\n        w = case[\"w0\"].copy()\n        m, n = A.shape\n        \n        # Algorithm parameters\n        max_iter = 100\n        tolerance = 1e-6\n        converged = False\n\n        # Pre-compute matrices for the linear system in the update rule\n        # M_inv = (A^T A + 2*lambda*I)^(-1)\n        AtA = A.T @ A\n        I = np.eye(n)\n        M_inv = np.linalg.inv(AtA + 2 * lmbda * I)\n        \n        # A^T b\n        Atb = A.T @ b\n\n        for t in range(max_iter):\n            w_old = w.copy()\n            \n            # Compute subgradient v of H(w) = 2*lambda*||w||_1 at w_old\n            # Using sgn(0)=0 as specified\n            v = 2 * lmbda * np.sign(w_old)\n            \n            # Solve the linear system for the next iterate\n            # w = M_inv * (A^T*b + v)\n            rhs = Atb + v\n            w = M_inv @ rhs\n            \n            # Check for convergence\n            # Use max(||w_old||_2, 1) as the denominator for numerical stability\n            w_old_norm = np.linalg.norm(w_old)\n            rel_change = np.linalg.norm(w - w_old) / max(w_old_norm, 1.0)\n            \n            if rel_change  tolerance:\n                converged = True\n                break\n        \n        # --- Post-processing and output calculation ---\n\n        # 1. Count snapped coordinates\n        # A coordinate w_i is \"snapped\" if ||w_i| - 1| = 1e-3\n        num_snapped = np.sum(np.abs(np.abs(w) - 1.0) = 1e-3)\n        \n        # 2. Calculate final objective value F(w)\n        # F(w) = 0.5 * ||Aw - b||^2 + lambda * sum(min((w_i - 1)^2, (w_i + 1)^2))\n        least_squares_term = 0.5 * np.linalg.norm(A @ w - b)**2\n        penalty_term_per_coord = np.minimum((w - 1)**2, (w + 1)**2)\n        penalty_term = lmbda * np.sum(penalty_term_per_coord)\n        final_objective_value = least_squares_term + penalty_term\n        \n        results.append(\n            f\"[{num_snapped},{final_objective_value:.6f},{str(converged).lower()}]\"\n        )\n        \n    print(f\"[{','.join(results)}]\")\n\nsolve()\n\n```", "id": "3114735"}, {"introduction": "A powerful algorithm is only as good as its implementation, and efficiency is paramount in large-scale optimization. This advanced practice [@problem_id:3114701] shifts our focus from the 'what' of CCP to the 'how' of implementing it efficiently for more complex, constrained problems. By tackling a sequence of quadratic programs (QPs) within the CCP loop, you will investigate critical computational techniques like warm-starting and the reuse of matrix factorizations, which are essential for developing fast and scalable optimization solvers.", "problem": "Consider the difference-of-convex (DC) optimization problem where the decision variable is $x \\in \\mathbb{R}^n$, the objective is $f(x) = g(x) + h(x)$, and the feasible set is the box defined by lower and upper bounds. The convex part $g(x)$ is quadratic and the concave part $h(x)$ is the negative of a convex, smooth function. Specifically, consider\n$$\ng(x) = \\tfrac{1}{2} x^\\top H x + r^\\top x,\n$$\nwhere $H \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite and $r \\in \\mathbb{R}^n$, and\n$$\nh(x) = -\\mu \\sum_{j=1}^m \\log\\left(1 + e^{b_j^\\top x}\\right),\n$$\nwhere each $b_j \\in \\mathbb{R}^n$ is a given row vector and $\\mu  0$ is a scalar. The constraints are componentwise bounds $l \\le x \\le u$, where $l, u \\in \\mathbb{R}^n$.\n\nThe convex-concave procedure (CCP) produces a sequence $\\{x^{(k)}\\}$ by linearizing the concave part $h(x)$ at the current iterate $x^{(k)}$ and solving the resulting convex subproblem. At iteration $k$, the subproblem is\n$$\n\\min_{x \\in \\mathbb{R}^n} \\ \\tfrac{1}{2} x^\\top H x + \\left(r + \\nabla h\\left(x^{(k)}\\right)\\right)^\\top x \\quad \\text{subject to} \\quad l \\le x \\le u,\n$$\nwhich is a convex quadratic program (QP) with a constant Hessian $H$ and a linear term that changes with $k$ through $\\nabla h\\left(x^{(k)}\\right)$.\n\nFocus on the following computational investigation:\n- At each CCP iteration, solve the box-constrained QP using an active-set method that warm-starts from the previous iterate’s active set (indices fixed at either their lower or upper bounds).\n- Show how to reuse matrix factorizations when the subproblems are QPs whose Hessian $H$ and constraints remain unchanged, and only the linear term changes between iterations. Concretely, for a given active set, the reduced system matrix for the free variables is $H_{FF}$, and the corresponding Cholesky factorization can be reused across CCP iterations as long as the active set $F$ does not change.\n\nYou must implement a program that:\n1. Constructs three test cases with specified data.\n2. Runs CCP with a box-constrained active-set QP subsolver.\n3. Uses warm-starts based on the previous iterate’s active set.\n4. Reuses Cholesky factorizations for the reduced Hessian $H_{FF}$ when the active set remains unchanged across subproblems.\n5. Terminates CCP when the infinity norm of the iterate difference satisfies $\\|\\left\\|x^{(k+1)} - x^{(k)}\\right\\|_{\\infty}  \\varepsilon$ with $\\varepsilon = 10^{-6}$, or when the number of iterations reaches $30$.\n\nFor each test case, the output must be a list of three integers $[I, F, R]$ where:\n- $I$ is the number of CCP iterations executed until termination,\n- $F$ is the total number of fresh Cholesky factorizations computed for reduced Hessians $H_{FF}$ across all QP subproblems within CCP,\n- $R$ is the total number of reused Cholesky factorizations (i.e., solves performed using a cached factorization for an unchanged active set).\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for the three test cases, in order. For example, the format must be exactly\n$$\n\\text{[[}I_1\\text{,}F_1\\text{,}R_1\\text{],[}I_2\\text{,}F_2\\text{,}R_2\\text{],[}I_3\\text{,}F_3\\text{,}R_3\\text{]]}\n$$\n\nUse the following test suite. In each case, all numeric entries are real numbers, and matrices are symmetric as given.\n\nTest Case $1$ ($n = 5$, $m = 3$):\n- Hessian\n$$\nH_1 =\n\\begin{bmatrix}\n4  1  0  0  0 \\\\\n1  3  0.5  0  0 \\\\\n0  0.5  2  0.3  0 \\\\\n0  0  0.3  2.5  0.2 \\\\\n0  0  0  0.2  1.8\n\\end{bmatrix}.\n$$\n- Linear term vector\n$$\nr_1 = \\begin{bmatrix} 0.1 \\\\ -0.2 \\\\ 0.05 \\\\ -0.1 \\\\ 0.0 \\end{bmatrix}.\n$$\n- Rows of $B_1 \\in \\mathbb{R}^{3 \\times 5}$:\n$$\nb_1^\\top = \\begin{bmatrix} 1  0  0  0  0 \\end{bmatrix}, \\quad\nb_2^\\top = \\begin{bmatrix} 0  1  -1  0  0 \\end{bmatrix}, \\quad\nb_3^\\top = \\begin{bmatrix} 0  0  0  1  -1 \\end{bmatrix}.\n$$\n- Scalar\n$$\n\\mu_1 = 0.3.\n$$\n- Bounds\n$$\nl_1 = \\begin{bmatrix} -0.5 \\\\ -0.5 \\\\ -0.5 \\\\ -0.5 \\\\ -0.5 \\end{bmatrix}, \\quad\nu_1 = \\begin{bmatrix} 0.5 \\\\ 0.5 \\\\ 0.5 \\\\ 0.5 \\\\ 0.5 \\end{bmatrix}.\n$$\n\nTest Case $2$ ($n = 6$, $m = 4$):\n- Hessian\n$$\nH_2 =\n\\begin{bmatrix}\n3.5  0.8  0  0  0  0 \\\\\n0.8  4.2  0.6  0  0  0 \\\\\n0  0.6  2.8  0.4  0  0 \\\\\n0  0  0.4  3.1  0.5  0 \\\\\n0  0  0  0.5  2.2  0.7 \\\\\n0  0  0  0  0.7  1.9\n\\end{bmatrix}.\n$$\n- Linear term vector\n$$\nr_2 = \\begin{bmatrix} 1.0 \\\\ -1.2 \\\\ 0.9 \\\\ -0.7 \\\\ 0.5 \\\\ -0.6 \\end{bmatrix}.\n$$\n- Rows of $B_2 \\in \\mathbb{R}^{4 \\times 6}$:\n$$\nb_1^\\top = \\begin{bmatrix} 1  -1  0  0  0  0 \\end{bmatrix}, \\quad\nb_2^\\top = \\begin{bmatrix} 0  1  0  -1  0  0 \\end{bmatrix}, \\quad\nb_3^\\top = \\begin{bmatrix} 0  0  1  0  -1  0 \\end{bmatrix}, \\quad\nb_4^\\top = \\begin{bmatrix} 0  0  0  1  0  -1 \\end{bmatrix}.\n$$\n- Scalar\n$$\n\\mu_2 = 0.5.\n$$\n- Bounds\n$$\nl_2 = \\begin{bmatrix} -0.3 \\\\ -0.3 \\\\ -0.3 \\\\ -0.3 \\\\ -0.3 \\\\ -0.3 \\end{bmatrix}, \\quad\nu_2 = \\begin{bmatrix} 0.3 \\\\ 0.3 \\\\ 0.3 \\\\ 0.3 \\\\ 0.3 \\\\ 0.3 \\end{bmatrix}.\n$$\n\nTest Case $3$ ($n = 4$, $m = 2$):\n- Hessian\n$$\nH_3 =\n\\begin{bmatrix}\n1.0  0  0  0 \\\\\n0  2.0  0  0 \\\\\n0  0  3.0  0 \\\\\n0  0  0  4.0\n\\end{bmatrix}.\n$$\n- Linear term vector\n$$\nr_3 = \\begin{bmatrix} 0.2 \\\\ -0.1 \\\\ 0.05 \\\\ -0.05 \\end{bmatrix}.\n$$\n- Rows of $B_3 \\in \\mathbb{R}^{2 \\times 4}$:\n$$\nb_1^\\top = \\begin{bmatrix} 1  0.5  0  0 \\end{bmatrix}, \\quad\nb_2^\\top = \\begin{bmatrix} 0  0  1  -0.5 \\end{bmatrix}.\n$$\n- Scalar\n$$\n\\mu_3 = 0.2.\n$$\n- Bounds\n$$\nl_3 = \\begin{bmatrix} -0.4 \\\\ -0.4 \\\\ -0.4 \\\\ -0.4 \\end{bmatrix}, \\quad\nu_3 = \\begin{bmatrix} 0.4 \\\\ 0.4 \\\\ 0.4 \\\\ 0.4 \\end{bmatrix}.\n$$\n\nImplement the program to perform the CCP using the active-set QP solver with warm-starts and factorization reuse. For each test case, start from $x^{(0)} = 0$ within the bounds. Use the infinity norm termination criterion $\\|\\left\\|x^{(k+1)} - x^{(k)}\\right\\|_{\\infty}  \\varepsilon$ with $\\varepsilon = 10^{-6}$ or stop after $30$ iterations. The final output must be a single line in the exact format\n$$\n\\text{[[}I_1\\text{,}F_1\\text{,}R_1\\text{],[}I_2\\text{,}F_2\\text{,}R_2\\text{],[}I_3\\text{,}F_3\\text{,}R_3\\text{]]},\n$$\ncontaining the triplets for the three test cases in order.", "solution": "The user wants to solve a difference-of-convex (DC) optimization problem using the convex-concave procedure (CCP). I must first validate the problem statement.\n\n### Step 1: Extract Givens\nThe problem is to minimize $f(x) = g(x) + h(x)$ for $x \\in \\mathbb{R}^n$ subject to box constraints $l \\le x \\le u$.\n\n- **Convex part $g(x)$**: $g(x) = \\frac{1}{2} x^\\top H x + r^\\top x$, where $H \\in \\mathbb{R}^{n \\times n}$ is a symmetric positive definite matrix and $r \\in \\mathbb{R}^n$.\n- **Concave part $h(x)$**: $h(x) = -\\mu \\sum_{j=1}^m \\log(1 + e^{b_j^\\top x})$, where $\\mu  0$ is a scalar and $b_j \\in \\mathbb{R}^n$ are given vectors.\n- **Constraints**: $l \\le x \\le u$, where $l, u \\in \\mathbb{R}^n$.\n- **Algorithm**: The convex-concave procedure (CCP) is to be used. At iteration $k$, the concave part $h(x)$ is linearized around the current iterate $x^{(k)}$, leading to the convex subproblem:\n$$\n\\min_{x \\in \\mathbb{R}^n} \\ \\tfrac{1}{2} x^\\top H x + \\left(r + \\nabla h\\left(x^{(k)}\\right)\\right)^\\top x \\quad \\text{subject to} \\quad l \\le x \\le u\n$$\n- **Subproblem Solver**: The box-constrained quadratic program (QP) must be solved using an active-set method.\n- **Computational Requirements**:\n    1.  The active-set method must be warm-started using the active set from the solution of the previous subproblem.\n    2.  Cholesky factorizations of the reduced Hessian matrix $H_{FF}$ must be reused if the set of free variables $F$ is unchanged between QP solver steps.\n- **Termination Criteria**: The CCP terminates when $\\|x^{(k+1)} - x^{(k)}\\|_\\infty  \\varepsilon$ with $\\varepsilon = 10^{-6}$, or after a maximum of $30$ iterations.\n- **Initial Condition**: The CCP starts from $x^{(0)} = 0$.\n- **Output Metrics**: For each test case, the program must report a list $[I, F, R]$, where:\n    - $I$: Number of CCP iterations.\n    - $F$: Total number of fresh Cholesky factorizations.\n    - $R$: Total number of reused Cholesky factorizations.\n- **Test Data**: Three specific test cases are provided with all necessary data ($H, r, B, \\mu, l, u$).\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded**: The problem is well-grounded in the field of mathematical optimization. Difference-of-convex programming, the convex-concave procedure, and active-set methods for quadratic programming are all standard and well-established topics. The functions are correctly identified as convex or concave. The gradient of $h(x)$ is derivable from standard calculus rules. The problem is scientifically and mathematically sound.\n2.  **Well-Posed**: The objective function is defined over a compact set (a box). Since $H$ is positive definite, the quadratic part $g(x)$ is strictly convex, which ensures that each CCP subproblem is a strictly convex QP. A strictly convex QP over a non-empty convex set has a unique solution. The CCP algorithm is a standard procedure, though its convergence to a global minimum is not guaranteed (it typically converges to a critical point). The problem is well-posed in the sense that the requested algorithm can be executed to produce a unique sequence of iterates.\n3.  **Objective**: The problem is stated using precise mathematical language and provides all necessary numerical data. There are no subjective or ambiguous terms.\n4.  **Self-Contained and Consistent**: The problem provides all matrices, vectors, scalars, and constraints required to implement the algorithm. The dimensions are consistent. For example, in Test Case $1$, $n=5$, $m=3$, $H_1$ is $5 \\times 5$, $r_1$ is $5 \\times 1$, $B_1$ is $3 \\times 5$, and $l_1, u_1$ are $5 \\times 1$, which are all dimensionally consistent.\n5.  **Non-Trivial**: The problem requires the implementation of two nested iterative algorithms (CCP and an active-set QP solver) and specific logic for caching and reusing matrix factorizations. This is a non-trivial implementation task that tests understanding of these optimization methods.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. It is a well-defined computational problem in numerical optimization. I will proceed with providing a solution.\n\n### Algorithmic Design\nThe solution will be implemented by creating a hierarchy of functions.\n\n1.  **Main CCP Loop (`_run_ccp_for_case`)**: This function orchestrates the convex-concave procedure for a given test case.\n    - It initializes the iterate $x^{(0)} = 0$, a cache for Cholesky factorizations, and the performance counters ($I, F, R$).\n    - In each iteration $k=0, 1, \\dots$, it computes the gradient of the concave part, $\\nabla h(x^{(k)})$, to form the linear term $c_k = r + \\nabla h(x^{(k)})$ for the QP subproblem.\n    - It calls the QP solver to find the next iterate $x^{(k+1)}$, passing the current iterate $x^{(k)}$ to enable a warm-start.\n    - It updates the counters for factorizations based on the statistics returned by the QP solver.\n    - It checks the termination condition $\\|x^{(k+1)} - x^{(k)}\\|_\\infty  10^{-6}$ or the maximum iteration count.\n    - The gradient of $h(x)$ is $\\nabla h(x) = -\\mu \\sum_{j=1}^m \\sigma(b_j^\\top x) b_j = -\\mu B^\\top \\sigma(Bx)$, where $\\sigma(z) = 1/(1+e^{-z})$ is the sigmoid function, applied element-wise. A numerically stable implementation of the sigmoid function is required to avoid overflow.\n\n2.  **Active-Set QP Solver (`_solve_qp_active_set`)**: This function solves the box-constrained QP subproblem for a given linear term $c_k$.\n    - It implements a primal active-set method. The state is defined by the current point $x$ and a `working set` of indices corresponding to constraints that are actively held as equalities.\n    - The solver is warm-started: the initial point is taken from the previous CCP iterate, and the initial working set is determined by the bounds active at that point.\n    - In each iteration of the active-set method:\n        a. **Optimality Check**: It first checks if the current point is optimal for the current working set (i.e., the gradient's projection onto the free variables' subspace is zero). If so, it computes the Lagrange multipliers for the active constraints. If all multipliers are non-negative, the solution is optimal for the QP, and the solver terminates. If a multiplier is negative, the corresponding constraint is removed from the working set, and the process repeats.\n        b. **Search Direction**: If not at a subproblem optimum, it computes a search direction $p$ by solving a linear system involving the reduced Hessian $H_{FF}$ for the free variables $F$. The system is $H_{FF}p_F = -g_F$, where $g_F$ is the gradient with respect to the free variables.\n        c. **Factorization and Caching**: To solve this system, it computes the Cholesky factorization of $H_{FF}$. Before computing, it checks a cache (a dictionary passed from the main CCP loop) using the tuple of free indices as a key. If a factorization exists, it is reused (incrementing counter $R$); otherwise, a new one is computed and stored (incrementing counter $F$). `scipy.linalg.cholesky` and `scipy.linalg.cho_solve` are used for this.\n        d. **Step Length**: It computes the maximum step length $\\alpha \\in [0, 1]$ that can be taken along $p$ before a new variable hits its bound.\n        e. **Update**: It updates the position $x \\leftarrow x + \\alpha p$. If $\\alpha  1$, a new constraint became active; this constraint is added to the working set for the next iteration.\n\nThis design directly implements the requirements of the problem, including the nested loops, warm-starting, and factorization reuse. The final program structure will define the test cases and iterate through them, collecting and formatting the results as specified.", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import cholesky, cho_solve\n\ndef _stable_sigmoid(z):\n    \"\"\"\n    Computes the sigmoid function in a numerically stable way.\n    It handles vector inputs.\n    \"\"\"\n    z_arr = np.asarray(z)\n    out = np.empty_like(z_arr, dtype=float)\n    \n    # For z = 0, 1 / (1 + exp(-z)) is stable.\n    pos_mask = z_arr = 0\n    out[pos_mask] = 1.0 / (1.0 + np.exp(-z_arr[pos_mask]))\n    \n    # For z  0, exp(z) / (1 + exp(z)) avoids overflow of exp(-z).\n    neg_mask = ~pos_mask\n    exp_z_neg = np.exp(z_arr[neg_mask])\n    out[neg_mask] = exp_z_neg / (1.0 + exp_z_neg)\n    \n    return out\n\ndef _nabla_h(x, B, mu):\n    \"\"\"\n    Computes the gradient of the concave part h(x).\n    h(x) = -mu * sum(log(1 + exp(b_j.T * x)))\n    grad_h(x) = -mu * B.T * sigmoid(B*x)\n    \"\"\"\n    Bx = B @ x\n    sigma_vals = _stable_sigmoid(Bx)\n    grad = -mu * (B.T @ sigma_vals)\n    return grad\n\ndef _solve_qp_active_set(H, c, l, u, x_init, cholesky_cache):\n    \"\"\"\n    Solves a box-constrained QP using a primal active-set method.\n    min 0.5 * x.T * H * x + c.T * x\n    s.t. l = x = u\n    This function is warm-started with x_init.\n    \"\"\"\n    n = len(x_init)\n    x = np.clip(x_init, l, u)\n    \n    fresh_factorizations = 0\n    reused_factorizations = 0\n    \n    working_set = {i for i in range(n) if np.isclose(x[i], l[i]) or np.isclose(x[i], u[i])}\n    \n    max_qp_iter = 3 * n  # Safeguard against cycling\n    for _ in range(max_qp_iter):\n        free_indices = sorted(list(set(range(n)) - working_set))\n        \n        grad = H @ x + c\n        \n        # Check for optimality at subproblem\n        if not free_indices or np.linalg.norm(grad[free_indices])  1e-12:\n            lambda_min = 0.0\n            j_to_remove = -1\n            \n            for i in working_set:\n                if np.isclose(x[i], l[i]):\n                    lambda_i = grad[i]\n                    if lambda_i  lambda_min:\n                        lambda_min, j_to_remove = lambda_i, i\n                elif np.isclose(x[i], u[i]):\n                    lambda_i = -grad[i]\n                    if lambda_i  lambda_min:\n                        lambda_min, j_to_remove = lambda_i, i\n\n            if lambda_min = -1e-12:\n                break\n            else:\n                working_set.remove(j_to_remove)\n                continue\n\n        # Solve for search direction p_F\n        H_FF = H[np.ix_(free_indices, free_indices)]\n        grad_F = grad[free_indices]\n        \n        free_tuple = tuple(free_indices)\n        \n        # This check is needed in case all variables are fixed\n        if free_tuple in cholesky_cache:\n            L = cholesky_cache[free_tuple]\n            reused_factorizations += 1\n        else:\n            try:\n                L = cholesky(H_FF, lower=True)\n                cholesky_cache[free_tuple] = L\n                fresh_factorizations += 1\n            except np.linalg.LinAlgError:\n                p_F = -grad_F # Fallback for non-PD H_FF\n                L = None\n        \n        if L is not None:\n             p_F = cho_solve((L, True), -grad_F)\n\n        p = np.zeros(n)\n        p[free_indices] = p_F\n        \n        # Line search for step length alpha\n        alpha = 1.0\n        blocking_j = -1\n        \n        for idx in free_indices:\n            if p[idx]  1e-12:\n                dist = (u[idx] - x[idx]) / p[idx]\n                if dist  alpha:\n                    alpha, blocking_j = dist, idx\n            elif p[idx]  -1e-12:\n                dist = (l[idx] - x[idx]) / p[idx]\n                if dist  alpha:\n                    alpha, blocking_j = dist, idx\n        \n        x += alpha * p\n        \n        if blocking_j != -1:\n            if p[blocking_j]  0:\n                x[blocking_j] = u[blocking_j]\n            else:\n                x[blocking_j] = l[blocking_j]\n            working_set.add(blocking_j)\n\n    stats = {\n        'fresh_factorizations': fresh_factorizations,\n        'reused_factorizations': reused_factorizations\n    }\n    return x, stats\n\ndef _run_ccp_for_case(H, r, B, mu, l, u):\n    \"\"\"Runs the Convex-Concave Procedure for a single test case.\"\"\"\n    n = H.shape[0]\n    x_k = np.zeros(n) \n    \n    epsilon = 1e-6\n    max_ccp_iter = 30\n    \n    total_fresh_factorizations = 0\n    total_reused_factorizations = 0\n    \n    cholesky_cache = {}\n    \n    iterations = 0\n    for k in range(max_ccp_iter):\n        iterations = k + 1\n        \n        grad_h_k = _nabla_h(x_k, B, mu)\n        c_k = r + grad_h_k\n        \n        x_kp1, qp_stats = _solve_qp_active_set(\n            H, c_k, l, u, x_k, cholesky_cache\n        )\n        \n        total_fresh_factorizations += qp_stats['fresh_factorizations']\n        total_reused_factorizations += qp_stats['reused_factorizations']\n        \n        diff_norm = np.linalg.norm(x_kp1 - x_k, ord=np.inf)\n        \n        x_k = x_kp1\n        \n        if diff_norm  epsilon:\n            break\n            \n    return [iterations, total_fresh_factorizations, total_reused_factorizations]\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, run the CCP algorithm, and print results.\n    \"\"\"\n    test_cases = [\n        # Test Case 1\n        {\n            'H': np.array([\n                [4, 1, 0, 0, 0],\n                [1, 3, 0.5, 0, 0],\n                [0, 0.5, 2, 0.3, 0],\n                [0, 0, 0.3, 2.5, 0.2],\n                [0, 0, 0, 0.2, 1.8]\n            ]),\n            'r': np.array([0.1, -0.2, 0.05, -0.1, 0.0]),\n            'B': np.array([\n                [1, 0, 0, 0, 0],\n                [0, 1, -1, 0, 0],\n                [0, 0, 0, 1, -1]\n            ]),\n            'mu': 0.3,\n            'l': np.full(5, -0.5),\n            'u': np.full(5, 0.5)\n        },\n        # Test Case 2\n        {\n            'H': np.array([\n                [3.5, 0.8, 0, 0, 0, 0],\n                [0.8, 4.2, 0.6, 0, 0, 0],\n                [0, 0.6, 2.8, 0.4, 0, 0],\n                [0, 0, 0.4, 3.1, 0.5, 0],\n                [0, 0, 0, 0.5, 2.2, 0.7],\n                [0, 0, 0, 0, 0.7, 1.9]\n            ]),\n            'r': np.array([1.0, -1.2, 0.9, -0.7, 0.5, -0.6]),\n            'B': np.array([\n                [1, -1, 0, 0, 0, 0],\n                [0, 1, 0, -1, 0, 0],\n                [0, 0, 1, 0, -1, 0],\n                [0, 0, 0, 1, 0, -1]\n            ]),\n            'mu': 0.5,\n            'l': np.full(6, -0.3),\n            'u': np.full(6, 0.3)\n        },\n        # Test Case 3\n        {\n            'H': np.array([\n                [1.0, 0, 0, 0],\n                [0, 2.0, 0, 0],\n                [0, 0, 3.0, 0],\n                [0, 0, 0, 4.0]\n            ]),\n            'r': np.array([0.2, -0.1, 0.05, -0.05]),\n            'B': np.array([\n                [1, 0.5, 0, 0],\n                [0, 0, 1, -0.5]\n            ]),\n            'mu': 0.2,\n            'l': np.full(4, -0.4),\n            'u': np.full(4, 0.4)\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = _run_ccp_for_case(\n            case['H'], case['r'], case['B'], case['mu'], case['l'], case['u']\n        )\n        results.append(result)\n\n    # Format the output string exactly as required\n    output_str = f\"[{','.join([f'[{i},{f},{r}]' for i, f, r in results])}]\"\n    print(output_str)\n\nsolve()\n```", "id": "3114701"}]}