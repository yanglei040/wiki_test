{"hands_on_practices": [{"introduction": "We begin by applying the seminal Goemans-Williamson (GW) semidefinite relaxation to the Maximum Cut problem on two elementary graphs: a triangle ($K_3$) and a square ($C_4$). This exercise provides a concrete walk-through of the entire process, from formulating the SDP to analyzing the performance of the rounding algorithm [@problem_id:3177813]. By contrasting a non-bipartite graph with a bipartite one, we can directly observe how the relaxation behaves and why an \"integrality gap\" arises in certain structures.", "problem": "Consider the Maximum Cut problem on two simple undirected unit-weight graphs: the triangle with $3$ vertices (denoted $K_3$) and the square cycle with $4$ vertices (denoted $C_4$). The Maximum Cut objective can be written using sign variables $s_i \\in \\{-1, +1\\}$ assigned to vertices $i \\in V$ and edge weights $w_{ij} \\ge 0$ as the sum over edges of an indicator that the endpoints receive opposite signs. The Goemans–Williamson (GW) semidefinite programming (SDP) relaxation replaces these signs by unit vectors in a Euclidean space.\n\nYour tasks are:\n1. Formulate the GW SDP relaxation for Maximum Cut in terms of unit vectors $\\{v_i\\}_{i \\in V}$ with $\\|v_i\\| = 1$, and express the relaxed objective in terms of pairwise inner products $v_i^{\\top} v_j$ over edges.\n2. For $K_3$ and for $C_4$, compute the exact optimal SDP objective value by explicit construction or by a first-principles argument.\n3. Apply the GW random hyperplane rounding to an optimal SDP vector solution in each case, and compute the exact expected cut value produced by this rounding (expressed in terms of the edge angles determined by the inner products).\n4. For each graph, define the integrality gap bound as the ratio $\\rho = \\dfrac{\\text{expected rounded cut value}}{\\text{optimal SDP value}}$ and compute this ratio exactly.\n\nProvide your final answer as a single row matrix containing the two ratios for $K_3$ and $C_4$, in that order, in exact fractional form. No rounding is required, and no units are to be used.", "solution": "The analysis proceeds in four parts as required by the problem statement. First, we formulate the Goemans-Williamson (GW) Semidefinite Programming (SDP) relaxation for the Maximum Cut problem. Second and third, we apply this framework to the graphs $K_3$ and $C_4$ to compute their optimal SDP values and the expected values from the GW rounding procedure. Fourth, we compute the integrality gap bound for each case.\n\n**1. Formulation of the Goemans-Williamson SDP Relaxation**\n\nThe Maximum Cut (Max-Cut) problem on an undirected graph $G=(V, E)$ with non-negative edge weights $w_{ij}$ seeks a partition of the vertex set $V$ into two disjoint sets, say $S$ and $V \\setminus S$, such that the sum of weights of the edges crossing the partition is maximized.\n\nThis can be formulated as an integer quadratic program. Let a variable $s_i \\in \\{-1, +1\\}$ be associated with each vertex $i \\in V$, where $s_i = +1$ if $i \\in S$ and $s_i = -1$ if $i \\in V \\setminus S$. An edge $(i,j)$ is in the cut if and only if $s_i$ and $s_j$ have opposite signs, i.e., $s_i s_j = -1$. The term $\\frac{1}{2}(1 - s_i s_j)$ is $1$ if the edge is cut and $0$ otherwise. The Max-Cut objective is to maximize the total weight of the cut edges:\n$$ \\text{MAX-CUT} = \\max_{s_i \\in \\{-1, +1\\}} \\sum_{(i,j) \\in E} \\frac{w_{ij}}{2} (1 - s_i s_j) $$\nFor the unit-weight graphs specified, $w_{ij}=1$ for all $(i,j) \\in E$. The problem is equivalent to:\n$$ \\max \\sum_{(i,j) \\in E} \\frac{1}{2} (1 - s_i s_j) \\quad \\text{subject to} \\quad s_i^2 = 1, \\quad \\forall i \\in V $$\nThe Goemans-Williamson SDP relaxation lifts this problem from scalars to vectors. Each scalar variable $s_i$ is replaced by a a unit vector $v_i$ in a higher-dimensional Euclidean space, typically $\\mathbb{R}^{|V|}$. The quadratic constraint $s_i^2=1$ becomes the vector-norm constraint $\\|v_i\\| = 1$. The product $s_i s_j$ is replaced by the inner product $v_i^\\top v_j$.\n\nThe GW SDP relaxation is thus formulated as:\n$$ Z_{SDP} = \\max \\sum_{(i,j) \\in E} \\frac{1}{2} (1 - v_i^\\top v_j) $$\n$$ \\text{subject to} \\quad \\|v_i\\| = 1, \\quad \\forall i \\in V $$\nHere, the vectors $v_i$ are optimization variables. This completes the first task.\n\n**2. Analysis for the Triangle Graph ($K_3$)**\n\nThe graph $K_3$ has $V = \\{1, 2, 3\\}$ and $E = \\{(1,2), (2,3), (3,1)\\}$.\n\n**Optimal SDP Value for $K_3$:**\nThe objective function for $K_3$ is:\n$$ Z(K_3) = \\frac{1}{2}(1 - v_1^\\top v_2) + \\frac{1}{2}(1 - v_2^\\top v_3) + \\frac{1}{2}(1 - v_3^\\top v_1) = \\frac{3}{2} - \\frac{1}{2}(v_1^\\top v_2 + v_2^\\top v_3 + v_3^\\top v_1) $$\nTo maximize $Z(K_3)$, we must minimize the sum of the inner products $v_1^\\top v_2 + v_2^\\top v_3 + v_3^\\top v_1$. A fundamental inequality for three unit vectors is derived from the non-negativity of the squared norm of their sum:\n$$ \\|v_1 + v_2 + v_3\\|^2 \\ge 0 $$\n$$ (v_1 + v_2 + v_3)^\\top(v_1 + v_2 + v_3) = \\sum_{i=1}^3 \\|v_i\\|^2 + 2\\sum_{1 \\le i  j \\le 3} v_i^\\top v_j \\ge 0 $$\nSince $\\|v_i\\|=1$ for $i=1,2,3$, this becomes:\n$$ 3 + 2(v_1^\\top v_2 + v_2^\\top v_3 + v_3^\\top v_1) \\ge 0 $$\n$$ v_1^\\top v_2 + v_2^\\top v_3 + v_3^\\top v_1 \\ge -\\frac{3}{2} $$\nThis lower bound is achievable. Let the vectors $v_1, v_2, v_3$ be coplanar in $\\mathbb{R}^2$, separated by angles of $2\\pi/3$ radians ($120^\\circ$). For instance:\n$v_1 = (1, 0)$, $v_2 = (\\cos(2\\pi/3), \\sin(2\\pi/3)) = (-1/2, \\sqrt{3}/2)$, $v_3 = (\\cos(4\\pi/3), \\sin(4\\pi/3)) = (-1/2, -\\sqrt{3}/2)$.\nFor this configuration, $v_1+v_2+v_3 = 0$, and the inner products are $v_i^\\top v_j = \\cos(2\\pi/3) = -1/2$ for all $i \\ne j$.\nSubstituting this minimal sum into the objective function gives the optimal SDP value:\n$$ Z_{SDP}(K_3) = \\frac{3}{2} - \\frac{1}{2}\\left(-\\frac{3}{2}\\right) = \\frac{3}{2} + \\frac{3}{4} = \\frac{9}{4} $$\n\n**Expected Rounded Cut Value for $K_3$:**\nThe GW rounding algorithm selects a random hyperplane and partitions vertices based on which side their corresponding vectors lie. The probability that an edge $(i, j)$ is cut is given by $\\frac{\\theta_{ij}}{\\pi}$, where $\\theta_{ij} = \\arccos(v_i^\\top v_j)$ is the angle between vectors $v_i$ and $v_j$. The expected value of the cut is the sum of these probabilities over all edges (since weights are $1$).\nFor the optimal SDP solution for $K_3$, $v_i^\\top v_j = -1/2$ for all edges. The angle is:\n$$ \\theta_{ij} = \\arccos(-1/2) = \\frac{2\\pi}{3} $$\nThe expected cut value is the sum over the three edges:\n$$ E[\\text{cut}(K_3)] = \\sum_{(i,j) \\in E} \\frac{\\arccos(v_i^\\top v_j)}{\\pi} = 3 \\times \\frac{2\\pi/3}{\\pi} = 2 $$\n\n**Integrality Gap Bound for $K_3$:**\nThe ratio $\\rho$ is:\n$$ \\rho(K_3) = \\frac{E[\\text{cut}(K_3)]}{Z_{SDP}(K_3)} = \\frac{2}{9/4} = \\frac{8}{9} $$\n\n**3. Analysis for the Square Cycle Graph ($C_4$)**\n\nThe graph $C_4$ has $V = \\{1, 2, 3, 4\\}$ and $E = \\{(1,2), (2,3), (3,4), (4,1)\\}$.\n\n**Optimal SDP Value for $C_4$:**\nThe objective function for $C_4$ is:\n$$ Z(C_4) = \\frac{1}{2}(1 - v_1^\\top v_2) + \\frac{1}{2}(1 - v_2^\\top v_3) + \\frac{1}{2}(1 - v_3^\\top v_4) + \\frac{1}{2}(1 - v_4^\\top v_1) $$\nThe graph $C_4$ is bipartite, with partitions $S_1 = \\{1, 3\\}$ and $S_2 = \\{2, 4\\}$. The maximum cut places $S_1$ and $S_2$ on opposite sides, cutting all $4$ edges. The optimal integer value is $4$.\nSince the SDP is a relaxation, its optimal value must be at least the integer optimum: $Z_{SDP}(C_4) \\ge 4$.\nWe can construct a feasible SDP solution that achieves this value. Let $v_1 = v_3$ and $v_2 = v_4$, with $v_1 = -v_2$. For instance, in $\\mathbb{R}^1$, let $v_1=v_3=1$ and $v_2=v_4=-1$. These are unit vectors.\nThe inner products for the edges are:\n$v_1^\\top v_2 = (1)(-1) = -1$.\n$v_2^\\top v_3 = (-1)(1) = -1$.\n$v_3^\\top v_4 = (1)(-1) = -1$.\n$v_4^\\top v_1 = (-1)(1) = -1$.\nSubstituting these values:\n$$ Z_{SDP}(C_4) = \\frac{1}{2}(1 - (-1)) + \\frac{1}{2}(1 - (-1)) + \\frac{1}{2}(1 - (-1)) + \\frac{1}{2}(1 - (-1)) = 4 \\times \\frac{2}{2} = 4 $$\nSince we found a feasible solution with value $4$ and we know $Z_{SDP}(C_4) \\ge 4$, the optimal SDP value is exactly $4$.\n\n**Expected Rounded Cut Value for $C_4$:**\nFor the optimal SDP solution found above, the inner product for every edge is $v_i^\\top v_j = -1$.\nThe angle for each edge is:\n$$ \\theta_{ij} = \\arccos(-1) = \\pi $$\nThe expected cut value is the sum over the four edges:\n$$ E[\\text{cut}(C_4)] = \\sum_{(i,j) \\in E} \\frac{\\arccos(v_i^\\top v_j)}{\\pi} = 4 \\times \\frac{\\pi}{\\pi} = 4 $$\n\n**Integrality Gap Bound for $C_4$:**\nThe ratio $\\rho$ is:\n$$ \\rho(C_4) = \\frac{E[\\text{cut}(C_4)]}{Z_{SDP}(C_4)} = \\frac{4}{4} = 1 $$\n\n**4. Final Answer**\n\nThe ratios for $K_3$ and $C_4$ are $\\rho(K_3) = 8/9$ and $\\rho(C_4) = 1$. The required final answer is the row matrix containing these two values in order.", "answer": "$$ \\boxed{\n\\begin{pmatrix}\n\\frac{8}{9}  1\n\\end{pmatrix}\n} $$", "id": "3177813"}, {"introduction": "After analyzing given instances, we now shift our perspective from analysis to design. This practice challenges you to determine which graph structure, from a specified class of odd cycles, maximizes the integrality gap for the Max-Cut SDP [@problem_id:3177822]. By deriving general formulas for the relaxation's performance, you will uncover the geometric properties that create the largest discrepancy between the relaxed solution and the true combinatorial optimum.", "problem": "Consider the maximum cut problem on an undirected graph with $n=8$ labeled vertices and nonnegative edge weights $\\{w_{ij}\\}_{1 \\leq i  j \\leq 8}$. The maximum cut objective is\n$$\n\\max_{s \\in \\{-1,1\\}^{8}} \\sum_{1 \\leq i  j \\leq 8} w_{ij} \\,\\frac{1 - s_i s_j}{2},\n$$\nwhere $s_i$ encodes the side of the cut containing vertex $i$. A widely used semidefinite relaxation replaces the binary variables by unit vectors $\\{v_i\\}_{i=1}^{8} \\subset \\mathbb{R}^{8}$ and optimizes\n$$\n\\max_{\\{v_i\\}} \\sum_{1 \\leq i  j \\leq 8} w_{ij} \\,\\frac{1 - v_i^{\\top} v_j}{2}\n\\quad \\text{subject to} \\quad \\|v_i\\|_2 = 1 \\ \\text{for all } i,\n$$\nwhich is equivalent to optimizing a correlation matrix $X \\in \\mathbb{R}^{8 \\times 8}$ with $X \\succeq 0$ and $X_{ii}=1$ via $\\sum_{ij} w_{ij} \\,(1 - X_{ij})/2$. Define the integrality gap of the relaxation on a given weighted instance as the ratio of the optimal semidefinite relaxation objective to the optimal cut objective.\n\nYou are to design the weights $\\{w_{ij}\\}$ on $n=8$ nodes to maximize the integrality gap subject to the normalization $\\sum_{1 \\leq i  j \\leq 8} w_{ij} = 1$ and $w_{ij} \\geq 0$ for all $ij$. Restrict attention to designs in which all nonzero weights lie on the edges of a single odd cycle $C_k$ embedded on a subset of the $8$ vertices, where $k \\in \\{3,5,7\\}$ and all $k$ cycle edges receive the same weight. Within this class:\n\n1. Derive, from first principles, formulas for the optimal cut value and the optimal semidefinite relaxation value as functions of $k$ under the given normalization.\n2. Determine the choice of $k \\in \\{3,5,7\\}$ that maximizes the integrality gap ratio and compute the exact value of this maximal ratio as a closed-form expression.\n3. Explain qualitatively, using geometric reasoning about the relaxation, why your maximizing design induces near-parallel unit vectors $v_i$ for the $8-k$ vertices not belonging to the cycle.\n\nYour final answer must be the single exact value of the maximal integrality gap ratio as a closed-form analytic expression. Do not round. No units are required.", "solution": "The problem is valid as it is mathematically well-posed, scientifically grounded in the theory of combinatorial optimization and semidefinite programming, and internally consistent. We proceed with the solution.\n\nThe problem asks for the design of nonnegative edge weights $\\{w_{ij}\\}$ on a graph with $n=8$ vertices that maximizes the integrality gap, subject to specific constraints. The constraints are that the total weight $\\sum_{1 \\leq i  j \\leq 8} w_{ij} = 1$, and the non-zero weights are confined to the edges of a $k$-cycle $C_k$ for $k \\in \\{3,5,7\\}$. Furthermore, all $k$ edges of the cycle must have the same weight. This implies that for an edge $(i,j)$ in the cycle $C_k$, its weight is $w_{ij} = \\frac{1}{k}$, and for any pair $(i,j)$ not forming an edge of the cycle, $w_{ij}=0$.\n\nThe integrality gap is defined as the ratio of the optimal value of the semidefinite programming (SDP) relaxation to the optimal value of the integer maximum cut problem.\nMax-Cut objective: $\\text{Cut}_{opt} = \\max_{s \\in \\{-1,1\\}^8} \\sum_{1 \\leq i  j \\leq 8} w_{ij} \\frac{1 - s_i s_j}{2}$.\nSDP relaxation objective: $\\text{SDP}_{opt} = \\max_{\\{v_i\\}} \\sum_{1 \\leq i  j \\leq 8} w_{ij} \\frac{1 - v_i^\\top v_j}{2}$ subject to $\\|v_i\\|_2 = 1$.\n\nSince weights are non-zero only on the cycle edges, the objectives simplify to sums over the edges of $C_k$.\n$\\text{Cut}_{opt} = \\max_{s \\in \\{-1,1\\}^k} \\frac{1}{k} \\sum_{(i,j) \\in E(C_k)} \\frac{1 - s_i s_j}{2}$.\n$\\text{SDP}_{opt} = \\max_{\\{v_i\\}} \\frac{1}{k} \\sum_{(i,j) \\in E(C_k)} \\frac{1 - v_i^\\top v_j}{2}$ subject to $\\|v_i\\|_2 = 1$.\nThe vertices not in the cycle (labeled $\\{k+1, \\dots, 8\\}$) do not affect the objective values.\n\n### 1. Optimal Cut and SDP Values\n\nWe derive the optimal cut value, $\\text{Cut}_{opt}$, and the optimal SDP relaxation value, $\\text{SDP}_{opt}$, as functions of the cycle length $k$.\n\n**Optimal Cut Value ($\\text{Cut}_{opt}$)**\n\nThe term $\\frac{1 - s_i s_j}{2}$ is $1$ if vertices $i$ and $j$ are in different partitions of the cut ($s_i \\neq s_j$), and $0$ if they are in the same partition ($s_i = s_j$). The value of the cut is the sum of weights of the edges crossing the partition. With uniform weights $\\frac{1}{k}$, the cut value is $\\frac{1}{k}$ times the number of edges crossing the cut. Maximizing the cut value is equivalent to maximizing the number of edges in the cut partition.\n\nA graph is bipartite if and only if it contains no odd-length cycles. The cycles $C_k$ considered here have odd length ($k \\in \\{3,5,7\\}$), so they are not bipartite. This means it is impossible to partition the vertices into two sets such that all edges connect vertices from different sets. Therefore, the maximum number of cut edges must be less than $k$.\n\nConsider an attempt to place all edges in the cut. Let the vertices of $C_k$ be labeled $1, 2, \\dots, k$ in cyclic order. If we assign vertex $1$ to one side ($s_1=1$), we must assign vertex $2$ to the other side ($s_2=-1$) to cut edge $(1,2)$, then $s_3=1$ to cut edge $(2,3)$, and so on. The assignment must alternate: $s_i = (-1)^{i-1}$. For the last edge $(k,1)$, we have $s_k = (-1)^{k-1}$ and $s_1 = (-1)^0 = 1$. Since $k$ is odd, $k-1$ is even, so $s_k = (-1)^{\\text{even}} = 1$. Thus, $s_k=s_1=1$, and the edge $(k,1)$ is not cut. This partitioning cuts exactly $k-1$ edges. It is not possible to cut more, as that would require the graph to be bipartite. Thus, the maximum number of edges in the cut of an odd cycle $C_k$ is $k-1$.\n\nThe optimal cut value is then:\n$$ \\text{Cut}_{opt}(k) = \\frac{1}{k} \\times (k-1) = 1 - \\frac{1}{k} $$\n\n**Optimal SDP Relaxation Value ($\\text{SDP}_{opt}$)**\n\nThe SDP objective is to maximize $\\frac{1}{k} \\sum_{(i,j) \\in E(C_k)} \\frac{1 - v_i^\\top v_j}{2} = \\frac{1}{2k} (k - \\sum_{(i,j) \\in E(C_k)} v_i^\\top v_j)$. This is equivalent to minimizing the sum of dot products $\\sum_{(i,j) \\in E(C_k)} v_i^\\top v_j$ over all unit vectors $\\{v_i\\}$.\n\nFor an odd cycle $C_k$, a well-known optimal configuration for the vectors $\\{v_i\\}_{i=1}^k$ places them in $\\mathbb{R}^2$. Let the vectors be $v_i = (\\cos \\theta_i, \\sin \\theta_i)^\\top$. To minimize the sum of dot products between adjacent vectors, we should make the angle between them as close to $\\pi$ as possible. A symmetric arrangement is optimal. Let's define the vectors $v_j$ for $j \\in \\{1,\\dots,k\\}$ as:\n$$ v_j = \\left( \\cos\\left((j-1)\\left(\\pi - \\frac{\\pi}{k}\\right)\\right), \\sin\\left((j-1)\\left(\\pi - \\frac{\\pi}{k}\\right)\\right) \\right)^\\top $$\nThe dot product for an edge $(j, j+1)$ for $j \\in \\{1, \\dots, k-1\\}$ is:\n$$ v_j^\\top v_{j+1} = \\cos\\left( j\\left(\\pi - \\frac{\\pi}{k}\\right) - (j-1)\\left(\\pi - \\frac{\\pi}{k}\\right) \\right) = \\cos\\left(\\pi - \\frac{\\pi}{k}\\right) = -\\cos\\left(\\frac{\\pi}{k}\\right) $$\nFor the closing edge $(k,1)$, the dot product is:\n$$ v_k^\\top v_1 = \\cos\\left((k-1)\\left(\\pi - \\frac{\\pi}{k}\\right)\\right) = \\cos\\left((k-1)\\pi - \\frac{(k-1)\\pi}{k}\\right) = \\cos\\left((k-1)\\pi - \\pi + \\frac{\\pi}{k}\\right) $$\nSince $k$ is odd, $k-1$ is even, so $(k-2)$ is odd.\n$$ v_k^\\top v_1 = \\cos\\left((k-2)\\pi + \\frac{\\pi}{k}\\right) = \\cos\\left(\\pi + \\frac{\\pi}{k}\\right) = -\\cos\\left(\\frac{\\pi}{k}\\right) $$\nAll $k$ edges contribute the same minimal dot product value of $-\\cos(\\frac{\\pi}{k})$. The sum is $k \\cdot (-\\cos(\\frac{\\pi}{k}))$. This construction is known to be optimal.\n\nThe optimal SDP value is:\n$$ \\text{SDP}_{opt}(k) = \\frac{1}{2k} \\left( k - k\\left(-\\cos\\left(\\frac{\\pi}{k}\\right)\\right) \\right) = \\frac{1}{2}\\left(1 + \\cos\\left(\\frac{\\pi}{k}\\right)\\right) $$\nUsing the half-angle identity $1+\\cos(2\\theta) = 2\\cos^2\\theta$, this simplifies to:\n$$ \\text{SDP}_{opt}(k) = \\cos^2\\left(\\frac{\\pi}{2k}\\right) $$\n\n### 2. Maximizing the Integrality Gap\n\nThe integrality gap, $\\alpha(k)$, is the ratio of the SDP optimum to the cut optimum:\n$$ \\alpha(k) = \\frac{\\text{SDP}_{opt}(k)}{\\text{Cut}_{opt}(k)} = \\frac{\\cos^2\\left(\\frac{\\pi}{2k}\\right)}{1 - \\frac{1}{k}} = \\frac{k \\cos^2\\left(\\frac{\\pi}{2k}\\right)}{k-1} $$\nWe need to evaluate this expression for $k \\in \\{3,5,7\\}$ and find the maximum.\n\nFor $k=3$:\n$$ \\alpha(3) = \\frac{3 \\cos^2\\left(\\frac{\\pi}{6}\\right)}{3-1} = \\frac{3 \\left(\\frac{\\sqrt{3}}{2}\\right)^2}{2} = \\frac{3 \\cdot \\frac{3}{4}}{2} = \\frac{9}{8} $$\n\nFor $k=5$:\n$$ \\alpha(5) = \\frac{5 \\cos^2\\left(\\frac{\\pi}{10}\\right)}{5-1} = \\frac{5}{4} \\cos^2\\left(\\frac{\\pi}{10}\\right) $$\nTo evaluate $\\cos^2(\\frac{\\pi}{10})$, we use the value $\\cos(\\frac{\\pi}{5}) = \\frac{1+\\sqrt{5}}{4}$. Using the half-angle identity $\\cos^2(\\theta) = \\frac{1+\\cos(2\\theta)}{2}$:\n$$ \\cos^2\\left(\\frac{\\pi}{10}\\right) = \\frac{1+\\cos\\left(\\frac{\\pi}{5}\\right)}{2} = \\frac{1+\\frac{1+\\sqrt{5}}{4}}{2} = \\frac{\\frac{4+1+\\sqrt{5}}{4}}{2} = \\frac{5+\\sqrt{5}}{8} $$\nSubstituting this into the expression for $\\alpha(5)$:\n$$ \\alpha(5) = \\frac{5}{4} \\cdot \\frac{5+\\sqrt{5}}{8} = \\frac{25+5\\sqrt{5}}{32} $$\n\nFor $k=7$:\n$$ \\alpha(7) = \\frac{7 \\cos^2\\left(\\frac{\\pi}{14}\\right)}{7-1} = \\frac{7}{6} \\cos^2\\left(\\frac{\\pi}{14}\\right) $$\nThe value of $\\cos(\\frac{\\pi}{14})$ cannot be expressed in terms of elementary radicals.\n\nTo compare the values, we can use numerical approximations:\n$\\alpha(3) = \\frac{9}{8} = 1.125$.\n$\\alpha(5) = \\frac{25+5\\sqrt{5}}{32} \\approx \\frac{25+5(2.23607)}{32} \\approx \\frac{36.18035}{32} \\approx 1.130636$.\n$\\alpha(7) = \\frac{7}{6}\\cos^2(\\frac{\\pi}{14}) \\approx \\frac{7}{6} (0.974927...)^2 \\approx 1.1666... \\times 0.95048... \\approx 1.1089$.\n\nThe numerical comparison shows that $\\alpha(5)$ is the largest of the three values. It is a known result that the integrality gap for odd cycles $C_k$ is maximized at $k=5$. Thus, the maximum integrality gap for the specified class of weight designs is $\\alpha(5)$.\n\n### 3. Geometric Reasoning for Non-Cycle Vertices\n\nLet $V_C = \\{1, \\dots, k\\}$ be the set of vertices in the cycle and $V_N = \\{k+1, \\dots, 8\\}$ be the remaining $8-k$ vertices. The SDP objective function depends only on the vectors $\\{v_i\\}_{i \\in V_C}$. The vectors $\\{v_j\\}_{j \\in V_N}$ do not appear in the objective. Their configuration is constrained only by the requirements that $\\|v_j\\|_2=1$ and that the full $8 \\times 8$ Gram matrix $X$, with entries $X_{ij}=v_i^\\top v_j$, must be positive semidefinite ($X \\succeq 0$).\n\nLet an optimal configuration of vectors for the cycle, $\\{v_i^*\\}_{i \\in V_C}$, be found. As shown, these can be chosen to lie in a 2-dimensional subspace. The objective value is fixed by this choice. Now we must choose the vectors $\\{v_j\\}_{j \\in V_N}$ to satisfy the constraints.\n\nThe positive semidefinite condition on $X$ imposes a complex set of inequalities on the dot products involving vectors from $V_N$. For any principal submatrix of $X$ to be positive semidefinite, its determinant must be non-negative.\nA simple way to satisfy all these constraints is to eliminate any new geometric complexity arising from the vectors in $V_N$. This can be achieved by making them identical. Let us choose $v_j = u$ for all $j \\in V_N$, where $u$ is any unit vector.\n\nWith this choice, for any $j, l \\in V_N$, the dot product is $v_j^\\top v_l = u^\\top u = 1$. The submatrix of $X$ corresponding to $V_N$ is the all-ones matrix, which is positive semidefinite.\nThe off-diagonal block entries are $X_{ij} = v_i^{*\\top} u$ for $i \\in V_C, j \\in V_N$.\nLet's check the positive semidefiniteness of the full matrix $X$. If we choose $u$ to be one of the cycle vectors, say $u = v_p^*$ for some $p \\in V_C$, then the vectors are $\\{v_1^*, \\dots, v_k^*, v_p^*, \\dots, v_p^*\\}$. Since all these vectors lie in a 2-dimensional space, their $8 \\times 8$ Gram matrix has rank at most $2$ and is therefore positive semidefinite. This is a valid optimal solution.\nAnother valid choice is to pick $u$ to be a unit vector orthogonal to the span of $\\{v_i^*\\}_{i \\in V_C}$.\nThe key insight is that since the configuration of vectors in $V_N$ does not affect the objective value, any choice that satisfies the constraints is permitted. The choice $v_{k+1}=\\dots=v_8=u$ is the simplest one that guarantees the positive semidefiniteness of the overall Gram matrix without any further conditions on $u$. In this solution, the vectors for the non-cycle vertices are identical, meaning they are perfectly parallel. The term \"near-parallel\" in the problem statement can be interpreted as encompassing this case of being identical.", "answer": "$$ \\boxed{\\frac{25+5\\sqrt{5}}{32}} $$", "id": "3177822"}, {"introduction": "Theory and pen-and-paper calculations provide crucial insights, but many real-world optimization problems are large and involve noisy data. This final practice moves from idealized models to a computational setting, tasking you with implementing an SDP solver and evaluating its performance on graphs with a known, but hidden, community structure [@problem_id:3177869]. This experiment demonstrates the power of SDP relaxations in practical data analysis scenarios and bridges the gap between theoretical guarantees and empirical performance.", "problem": "You are tasked to design and implement a self-contained computational experiment to examine how Semidefinite Programming (SDP) relaxations behave on a combinatorial partitioning problem with a block-structured weight matrix. The experiment must be reproducible, fully specified, and must report quantitative outcomes for a small test suite.\n\nLet there be a weighted, undirected graph with $n$ vertices, represented by a symmetric weight matrix $W \\in \\mathbb{R}^{n \\times n}$ with zero diagonal. Consider a ground-truth binary partition encoded by a label vector $s \\in \\{-1, +1\\}^n$, where $s_i = +1$ indicates membership in block $A$ and $s_i = -1$ indicates membership in block $B$. The graph weights are generated from a signed block model:\n- For pairs $(i, j)$ within the same block, the weight $W_{ij}$ is drawn independently from a normal distribution with mean $-\\alpha$ and standard deviation $\\sigma$.\n- For pairs $(i, j)$ in different blocks, the weight $W_{ij}$ is drawn independently from a normal distribution with mean $+\\beta$ and standard deviation $\\sigma$.\n- The diagonal entries satisfy $W_{ii} = 0$, and the matrix is made symmetric by setting $W_{ji} = W_{ij}$ for $i  j$.\n\nThe combinatorial objective we consider uses the $\\{-1, +1\\}$ encoding for a cut: Given a partition $s \\in \\{-1, +1\\}^n$, define the signed cut objective\n$$\nJ(s) \\triangleq \\frac{1}{4} \\sum_{i=1}^n \\sum_{j=1}^n W_{ij} \\left(1 - s_i s_j\\right).\n$$\nThis favors separating vertices connected by positive weights and aligning vertices connected by negative weights. Directly optimizing over $s \\in \\{-1, +1\\}^n$ is combinatorial. A Semidefinite Programming (SDP) relaxation replaces the rank-one matrix $s s^\\top$ with a positive semidefinite matrix $X \\in \\mathbb{S}_+^n$ subject to $\\operatorname{diag}(X) = \\mathbf{1}$, and optimizes a linear functional in $X$. In this experiment, use a low-rank factorization $X = Y Y^\\top$ with $Y \\in \\mathbb{R}^{n \\times r}$ and enforce the constraint that each row of $Y$ has unit Euclidean norm. Optimize the linear functional consistent with the relaxation by performing projected gradient descent on $Y$ for a fixed number of steps, using a step size scaled by the spectral norm of $W$. Concretely:\n- Initialize $Y$ with independent, standard normal entries and project each row to have unit norm.\n- At each iteration, update $Y$ by a gradient step on the factorized objective followed by projection of each row back to unit norm.\n- Use a step size of the form $\\eta = c / (\\lambda_{\\max} + 10^{-8})$, where $\\lambda_{\\max}$ is the largest absolute eigenvalue of $W$, and $c$ is a dimensionless coefficient specified per test case.\n\nAfter optimization, obtain a randomized rounding of the SDP solution using a random hyperplane: draw a random vector $g \\in \\mathbb{R}^r$ with independent, standard normal entries and set\n$$\n\\hat{s}_i \\triangleq \\operatorname{sign}\\left(\\langle y_i, g \\rangle\\right),\n$$\nwhere $y_i$ denotes the $i$-th row of $Y$ and $\\operatorname{sign}(0)$ is defined to be $+1$.\n\nMeasure and report:\n1. An alignment score quantifying how much the optimized vectors align with the ground-truth blocks:\n   - Compute the matrix $S \\triangleq Y Y^\\top$.\n   - Compute the mean of $S_{ij}$ over all pairs $(i, j)$ with $i \\neq j$ that satisfy $s_i = s_j$ (same block), and the mean of $S_{ij}$ over all pairs with $s_i \\neq s_j$ (different blocks).\n   - Define the alignment score as the difference between these two means (same-block mean minus different-block mean).\n2. The rounding classification accuracy as a decimal in $[0,1]$:\n   - Compute the fraction of indices $i$ for which $\\hat{s}_i = s_i$ and the fraction for which $-\\hat{s}_i = s_i$; report the maximum of these two fractions.\n3. The ratio of cut objective values $J(\\hat{s}) / J(s)$, using the same $J(\\cdot)$ defined above.\n\nImplement the above using the following test suite of parameter sets, each specifying $(n, \\text{block sizes}, \\alpha, \\beta, \\sigma, r, T, c)$:\n- Test case $1$ (happy path): $(n = 40, \\text{block sizes} = (20, 20), \\alpha = 1.0, \\beta = 1.0, \\sigma = 0.2, r = 5, T = 600, c = 0.9)$.\n- Test case $2$ (low-noise boundary): $(n = 30, \\text{block sizes} = (15, 15), \\alpha = 1.5, \\beta = 1.0, \\sigma = 0.05, r = 5, T = 400, c = 0.9)$.\n- Test case $3$ (high-noise edge): $(n = 30, \\text{block sizes} = (15, 15), \\alpha = 0.8, \\beta = 1.0, \\sigma = 0.7, r = 5, T = 800, c = 0.7)$.\n\nYour program must:\n- Construct $W$ for each test case as specified.\n- Run the projected gradient factorization-based SDP relaxation for $T$ iterations with step size $\\eta = c / (\\lambda_{\\max} + 10^{-8})$, where $\\lambda_{\\max}$ is the largest absolute eigenvalue of $W$.\n- Compute the alignment score, rounding classification accuracy, and the ratio $J(\\hat{s}) / J(s)$.\n- Produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, where each test case’s result is a sublist of three floats, ordered as $[\\text{alignment score}, \\text{accuracy}, \\text{ratio}]$. For example: $[[a_1,b_1,c_1],[a_2,b_2,c_2],[a_3,b_3,c_3]]$.\n\nNo physical units or angle units are involved. All reported quantities must be pure numerical values. Percentages must be expressed as decimals in $[0,1]$.", "solution": "The problem statement is assessed to be **valid**. It presents a self-contained, scientifically grounded, and well-posed computational experiment in the domain of optimization and combinatorial problems. The problem provides a complete specification of the model, the algorithm, the parameters for test cases, and the required output metrics. The methodology is based on established techniques, namely the signed block model for graph generation, semidefinite programming (SDP) relaxation for a MAX-CUT-like problem, the Burer-Monteiro factorization approach for solving the SDP using projected gradient descent, and randomized hyperplane rounding. The task is to implement this computational experiment and report the results for a given set of test cases.\n\nThe core task is to find a binary partition of a graph, represented by a vector $s \\in \\{-1, +1\\}^n$, that maximizes the signed cut objective:\n$$\nJ(s) = \\frac{1}{4} \\sum_{i=1}^n \\sum_{j=1}^n W_{ij} (1 - s_i s_j)\n$$\nMaximizing $J(s)$ is equivalent to minimizing the quadratic form $s^\\top W s = \\sum_{i,j} W_{ij} s_i s_j$, since $\\sum_{i,j} W_{ij}$ is a constant. We can write $s^\\top W s = \\operatorname{Tr}(W s s^\\top)$. The combinatorial constraint is $s_i \\in \\{-1, +1\\}$, which is equivalent to $s_i^2 = 1$. The SDP relaxation lifts this problem by replacing the rank-$1$ matrix $X = ss^\\top$ with a general positive semidefinite matrix $X \\in \\mathbb{S}_+^n$. The constraint $s_i^2 = 1$ becomes $\\operatorname{diag}(X) = \\mathbf{1}$. The relaxed optimization problem is:\n$$\n\\min_{X \\in \\mathbb{S}_+^n} \\operatorname{Tr}(WX) \\quad \\text{subject to} \\quad \\operatorname{diag}(X) = \\mathbf{1}\n$$\nThe problem specifies solving this using a low-rank factorization approach. We set $X = YY^\\top$ where $Y \\in \\mathbb{R}^{n \\times r}$. The constraint $\\operatorname{diag}(X)=\\mathbf{1}$ translates to requiring each row $y_i$ of $Y$ to be a unit vector, i.e., $\\|y_i\\|_2 = 1$. The objective becomes minimizing $F(Y) = \\operatorname{Tr}(WYY^\\top)$ over the manifold of such matrices $Y$. This non-convex problem is solved using projected gradient descent.\n\nThe methodology for the computational experiment is as follows:\n\n**1. Data Generation**\nFor each test case with parameters $(n, \\text{block sizes}, \\alpha, \\beta, \\sigma, r, T, c)$:\n- A ground-truth partition vector $s \\in \\{-1, +1\\}^n$ is constructed based on the specified block sizes. For example, for sizes $(k, n-k)$, $s_i = +1$ for $i \\in \\{1, \\dots, k\\}$ and $s_i = -1$ for $i \\in \\{k+1, \\dots, n\\}$.\n- A symmetric weight matrix $W \\in \\mathbb{R}^{n \\times n}$ with a zero diagonal is generated. For each pair of indices $(i, j)$ with $i  j$:\n    - If $s_i = s_j$ (intra-block), $W_{ij}$ is drawn from a normal distribution $\\mathcal{N}(-\\alpha, \\sigma^2)$.\n    - If $s_i \\neq s_j$ (inter-block), $W_{ij}$ is drawn from $\\mathcal{N}(+\\beta, \\sigma^2)$.\n- Symmetry is enforced by setting $W_{ji} = W_{ij}$. To ensure reproducibility of the experiment, all random number generation processes are seeded with a fixed value.\n\n**2. Optimization via Projected Gradient Descent**\nThe objective function to be minimized is $F(Y) = \\operatorname{Tr}(WYY^\\top)$.\n- **Initialization**: An initial matrix $Y_0 \\in \\mathbb{R}^{n \\times r}$ is generated with entries drawn from the standard normal distribution $\\mathcal{N}(0, 1)$. Its rows are then normalized to have unit Euclidean norm.\n- **Gradient Calculation**: The gradient of $F(Y)$ with respect to $Y$ is $\\nabla_Y F(Y) = 2WY$.\n- **Step Size**: The step size $\\eta$ is set to $\\eta = c / (\\lambda_{\\max} + 10^{-8})$, where $\\lambda_{\\max} = \\max_i |\\lambda_i(W)|$ is the largest absolute eigenvalue (spectral norm) of the symmetric matrix $W$.\n- **Iteration**: For $k=0, 1, \\dots, T-1$, the following update is performed:\n    1.  **Gradient Step**: A temporary matrix $Y'$ is computed by taking a step in the negative gradient direction: $Y' = Y_k - \\eta (2 W Y_k)$.\n    2.  **Projection**: Each row $y'_i$ of $Y'$ is projected back onto the unit sphere: $y_{i, k+1} = y'_i / \\|y'_i\\|_2$. The resulting matrix is $Y_{k+1}$.\n\n**3. Randomized Rounding**\nAfter $T$ iterations, the final matrix $Y$ provides an approximate solution to the SDP. A discrete partition $\\hat{s} \\in \\{-1, +1\\}^n$ is recovered using randomized hyperplane rounding:\n- A random vector $g \\in \\mathbb{R}^r$ is drawn, with each component sampled independently from $\\mathcal{N}(0, 1)$.\n- The estimated partition is computed as $\\hat{s}_i = \\operatorname{sign}(\\langle y_i, g \\rangle)$, where $y_i$ is the $i$-th row of $Y$. The convention $\\operatorname{sign}(0) = +1$ is used.\n\n**4. Performance Evaluation**\nThree quantitative metrics are computed to evaluate the outcome:\n- **Alignment Score**: This measures how well the geometry of the solution vectors $\\{y_i\\}$ reflects the ground-truth partition. Let $S = YY^\\top$. The score is the difference between the average inner product of vectors in the same block and the average inner product of vectors in different blocks:\n  $$ \\text{score} = \\frac{\\sum_{ij, s_i=s_j} S_{ij}}{|\\{(i,j) \\mid ij, s_i=s_j\\}|} - \\frac{\\sum_{ij, s_i \\neq s_j} S_{ij}}{|\\{(i,j) \\mid ij, s_i \\neq s_j\\}|} $$\n- **Rounding Classification Accuracy**: Since the labels $\\{+1, -1\\}$ are arbitrary, the accuracy of $\\hat{s}$ is measured against both $s$ and $-s$, and the better of the two is reported:\n  $$ \\text{accuracy} = \\max\\left( \\frac{1}{n}\\sum_{i=1}^n \\mathbb{I}(\\hat{s}_i = s_i), \\frac{1}{n}\\sum_{i=1}^n \\mathbb{I}(\\hat{s}_i = -s_i) \\right) $$\n  where $\\mathbb{I}(\\cdot)$ is the indicator function.\n- **Ratio of Cut Objectives**: This compares the quality of the recovered partition $\\hat{s}$ to the ground-truth partition $s$ in terms of the original objective function $J(\\cdot)$. The ratio $J(\\hat{s}) / J(s)$ is computed. Based on the problem formulation, $J(s)$ is expected to be positive.\n\nThe implementation will follow these steps precisely for each of the three test cases provided.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ... # Scipy is not required for this implementation.\n\ndef solve():\n    \"\"\"\n    Main function to run the computational experiment for all test cases.\n    \"\"\"\n    \n    # Test cases defined as (n, block_sizes, alpha, beta, sigma, r, T, c)\n    test_cases = [\n        # Test case 1 (happy path)\n        (40, (20, 20), 1.0, 1.0, 0.2, 5, 600, 0.9),\n        # Test case 2 (low-noise boundary)\n        (30, (15, 15), 1.5, 1.0, 0.05, 5, 400, 0.9),\n        # Test case 3 (high-noise edge)\n        (30, (15, 15), 0.8, 1.0, 0.7, 5, 800, 0.7),\n    ]\n\n    # Use a fixed seed for reproducibility of the entire experiment.\n    rng = np.random.default_rng(seed=42)\n    \n    results = []\n    for params in test_cases:\n        results.append(run_experiment(params, rng))\n\n    # Format the final output string as specified.\n    # The str() representation of a list is '[item1, item2, ...]'\n    # Joining these with ',' and enclosing in '[]' gives '[[...],[...]]'\n    final_output = f\"[{','.join(map(str, results))}]\"\n    print(final_output)\n\ndef run_experiment(params, rng):\n    \"\"\"\n    Executes a single instance of the experiment for a given parameter set.\n    \"\"\"\n    n, block_sizes, alpha, beta, sigma, r, T, c = params\n    \n    # 1. Generate Ground Truth and Weight Matrix\n    s = np.ones(n, dtype=int)\n    s[block_sizes[0]:] = -1\n    \n    W = np.zeros((n, n))\n    for i in range(n):\n        for j in range(i + 1, n):\n            if s[i] == s[j]: # Same block\n                weight = rng.normal(loc=-alpha, scale=sigma)\n            else: # Different blocks\n                weight = rng.normal(loc=beta, scale=sigma)\n            W[i, j] = weight\n            W[j, i] = weight\n            \n    # 2. Optimization via Projected Gradient Descent\n    # Calculate step size\n    # np.linalg.eigvalsh is used for real symmetric matrices.\n    eigenvalues = np.linalg.eigvalsh(W)\n    lambda_max_abs = np.max(np.abs(eigenvalues))\n    eta = c / (lambda_max_abs + 1e-8)\n    \n    # Initialize Y\n    Y = rng.normal(loc=0.0, scale=1.0, size=(n, r))\n    # Project rows to unit norm\n    row_norms = np.linalg.norm(Y, axis=1, keepdims=True)\n    Y /= row_norms\n    \n    # Run gradient descent\n    for _ in range(T):\n        # Gradient step\n        grad_Y = 2 * (W @ Y)\n        Y_temp = Y - eta * grad_Y\n        \n        # Projection step\n        row_norms = np.linalg.norm(Y_temp, axis=1, keepdims=True)\n        # Avoid division by zero for potential zero-norm rows\n        row_norms[row_norms == 0] = 1.0\n        Y = Y_temp / row_norms\n        \n    # 3. Randomized Rounding\n    g = rng.normal(loc=0.0, scale=1.0, size=r)\n    s_hat = np.sign(Y @ g)\n    s_hat[s_hat == 0] = 1 # As per problem spec: sign(0) = +1\n    \n    # 4. Performance Evaluation\n    # Metric 1: Alignment Score\n    S = Y @ Y.T\n    s_outer = np.outer(s, s)\n    off_diagonal_mask = ~np.eye(n, dtype=bool)\n    \n    same_block_mask = (s_outer == 1)  off_diagonal_mask\n    diff_block_mask = (s_outer == -1)  off_diagonal_mask\n    \n    mean_same = np.mean(S[same_block_mask]) if np.any(same_block_mask) else 0.0\n    mean_diff = np.mean(S[diff_block_mask]) if np.any(diff_block_mask) else 0.0\n    \n    alignment_score = mean_same - mean_diff\n    \n    # Metric 2: Rounding Classification Accuracy\n    acc1 = np.mean(s_hat == s)\n    acc2 = np.mean(-s_hat == s)\n    accuracy = max(acc1, acc2)\n    \n    # Metric 3: Ratio of Cut Objectives\n    def compute_J(W_mat, s_vec):\n        s_outer_mat = np.outer(s_vec, s_vec)\n        return 0.25 * np.sum(W_mat * (1 - s_outer_mat))\n    \n    J_s = compute_J(W, s)\n    J_s_hat = compute_J(W, s_hat)\n    \n    # Handle case where J(s) might be zero or negative, although unlikely.\n    ratio = J_s_hat / J_s if J_s != 0 else 0.0\n\n    # Ensure results are standard Python floats for consistent printing\n    return [float(alignment_score), float(accuracy), float(ratio)]\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3177869"}]}