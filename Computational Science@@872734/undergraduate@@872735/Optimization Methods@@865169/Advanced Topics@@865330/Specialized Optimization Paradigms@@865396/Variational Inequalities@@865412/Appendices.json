{"hands_on_practices": [{"introduction": "Before we can solve a variational inequality, it is crucial to understand the properties of its defining operator, $F$. This exercise focuses on the common and important case where $F$ is an affine mapping, $F(x) = Ax + b$. You will explore the fundamental property of monotonicity and discover how it relates directly to the algebraic properties of the matrix $A$, providing a foundational understanding for analyzing more complex VIs [@problem_id:3197571].", "problem": "Consider the variational inequality (VI) defined as follows: find $x^{\\star} \\in K$ such that $\\langle F(x^{\\star}), y - x^{\\star} \\rangle \\ge 0$ for all $y \\in K$, where $K = [0,1]^n$ is the $n$-dimensional box and $F: \\mathbb{R}^n \\to \\mathbb{R}^n$ is a given mapping. An operator $F$ is called monotone on a set $K$ if $\\langle F(x) - F(y), x - y \\rangle \\ge 0$ for all $x,y \\in K$. Let $n = 2$ and specialize to the affine map $F(x) = A x + b$ with the symmetric matrix $A \\in \\mathbb{R}^{2 \\times 2}$ and vector $b \\in \\mathbb{R}^2$ given by\n$$\nA = \\begin{pmatrix} 2 & 0 \\\\ 0 & 4 \\end{pmatrix}, \\qquad b = \\begin{pmatrix} -1 \\\\ -3 \\end{pmatrix}.\n$$\nYou will examine monotonicity via positive semidefiniteness and then study the effect of small asymmetric perturbations.\n\nDefine a perturbed operator $F_{\\epsilon}(x) = (A + \\epsilon E)x + b$ where $\\epsilon \\in \\mathbb{R}$ and\n$$\nE = \\begin{pmatrix} 0 & 1 \\\\ -1 & 0 \\end{pmatrix}.\n$$\nTasks:\n- Using only fundamental definitions, verify monotonicity of $F$ on $K$ and explain the precise structural condition on $A$ that implies monotonicity.\n- Analyze how the perturbation governed by $\\epsilon E$ affects monotonicity, and justify your conclusion rigorously from first principles.\n- Under the assumption that the unique VI solution $x^{\\star}(\\epsilon)$ is interior to the box, i.e., $0 < x_i^{\\star}(\\epsilon) < 1$ for $i \\in \\{1,2\\}$, derive a closed-form analytic expression for the first component $x_1^{\\star}(\\epsilon)$ of the VI solution as a function of $\\epsilon$.\n\nYour final answer must be a single closed-form expression for $x_1^{\\star}(\\epsilon)$.", "solution": "The problem statement has been evaluated and is deemed valid. It is a well-posed, scientifically grounded problem in mathematical optimization, specifically concerning variational inequalities. All necessary data and definitions are provided, and there are no contradictions or ambiguities.\n\nThe problem is to analyze a variational inequality (VI) and find a component of its solution under certain conditions. The VI is to find $x^{\\star} \\in K$ such that $\\langle F(x^{\\star}), y - x^{\\star} \\rangle \\ge 0$ for all $y \\in K$.\n\nFirst, we address the three tasks as specified.\n\n**Task 1: Monotonicity of $F(x) = Ax+b$**\n\nAn operator $F$ is defined as monotone on a set $K$ if $\\langle F(x) - F(y), x - y \\rangle \\ge 0$ for all $x, y \\in K$.\nFor the given affine operator $F(x) = Ax + b$, we have:\n$$\nF(x) - F(y) = (Ax + b) - (Ay + b) = A(x - y)\n$$\nSubstituting this into the monotonicity condition gives:\n$$\n\\langle A(x - y), x - y \\rangle \\ge 0\n$$\nLet $z = x - y$. Since $x$ and $y$ can be any vectors in $K = [0,1]^2$, the vector $z$ can take any value in the set $\\{x-y \\mid x,y \\in [0,1]^2\\} = [-1,1]^2$. However, the standard definition of monotonicity for an affine operator $F(x) = Ax+b$ on $\\mathbb{R}^n$ requires the condition to hold for all $x,y \\in \\mathbb{R}^n$, which means $z$ can be any vector in $\\mathbb{R}^n$. The condition $\\langle Az, z \\rangle \\ge 0$ for all $z \\in \\mathbb{R}^n$ is the definition of the matrix $A$ being positive semidefinite. This is the precise structural condition on $A$ that implies monotonicity of the affine map $F(x) = Ax+b$ on any set $K \\subseteq \\mathbb{R}^n$.\n\nWe now verify this for the given matrix $A$:\n$$\nA = \\begin{pmatrix} 2 & 0 \\\\ 0 & 4 \\end{pmatrix}\n$$\nThis matrix is symmetric. A symmetric matrix is positive semidefinite if and only if all of its eigenvalues are non-negative. The eigenvalues of a diagonal matrix are its diagonal entries. Therefore, the eigenvalues of $A$ are $\\lambda_1 = 2$ and $\\lambda_2 = 4$. Since both eigenvalues are strictly positive, the matrix $A$ is positive definite, which is a stronger condition than being positive semidefinite.\nFor any non-zero vector $z = \\begin{pmatrix} z_1 \\\\ z_2 \\end{pmatrix} \\in \\mathbb{R}^2$, we have:\n$$\n\\langle Az, z \\rangle = z^T A z = \\begin{pmatrix} z_1 & z_2 \\end{pmatrix} \\begin{pmatrix} 2 & 0 \\\\ 0 & 4 \\end{pmatrix} \\begin{pmatrix} z_1 \\\\ z_2 \\end{pmatrix} = 2z_1^2 + 4z_2^2 > 0\n$$\nSince $\\langle A(x - y), x - y \\rangle \\ge 0$ for all $x, y \\in \\mathbb{R}^2$ (and thus for all $x,y \\in K$), the operator $F$ is strictly monotone on $\\mathbb{R}^2$, and therefore also on $K$.\n\n**Task 2: Effect of the perturbation on monotonicity**\n\nThe perturbed operator is $F_{\\epsilon}(x) = (A + \\epsilon E)x + b$, where $A_{\\epsilon} = A + \\epsilon E$.\n$$\nA_{\\epsilon} = \\begin{pmatrix} 2 & 0 \\\\ 0 & 4 \\end{pmatrix} + \\epsilon \\begin{pmatrix} 0 & 1 \\\\ -1 & 0 \\end{pmatrix} = \\begin{pmatrix} 2 & \\epsilon \\\\ -\\epsilon & 4 \\end{pmatrix}\n$$\nTo analyze the monotonicity of $F_{\\epsilon}$, we check the same condition as before:\n$$\n\\langle F_{\\epsilon}(x) - F_{\\epsilon}(y), x - y \\rangle \\ge 0 \\quad \\forall x,y \\in K\n$$\nThis is equivalent to checking if $\\langle A_{\\epsilon}z, z \\rangle \\ge 0$ for $z = x - y$. We analyze this for any $z \\in \\mathbb{R}^n$.\n$$\n\\langle A_{\\epsilon}z, z \\rangle = \\langle (A + \\epsilon E)z, z \\rangle = \\langle Az, z \\rangle + \\langle \\epsilon E z, z \\rangle = \\langle Az, z \\rangle + \\epsilon \\langle E z, z \\rangle\n$$\nFrom the first task, we know $\\langle Az, z \\rangle \\ge 0$. We now examine the term $\\langle Ez, z \\rangle$. The matrix $E$ is skew-symmetric, which means $E^T = -E$. For any skew-symmetric matrix $M$ and any vector $z$, the quadratic form $\\langle Mz, z \\rangle$ is always zero. This can be shown from first principles:\nThe value $\\langle Mz, z \\rangle = z^T M z$ is a scalar, so it is equal to its transpose:\n$$\n\\langle Mz, z \\rangle = (z^T M z)^T = z^T M^T (z^T)^T = z^T M^T z\n$$\nUsing the skew-symmetric property $M^T = -M$, we get:\n$$\nz^T M^T z = z^T (-M) z = - z^T M z = - \\langle Mz, z \\rangle\n$$\nThus, we have shown $\\langle Mz, z \\rangle = - \\langle Mz, z \\rangle$, which implies $2 \\langle Mz, z \\rangle = 0$, and therefore $\\langle Mz, z \\rangle = 0$.\nApplying this to our specific matrix $E$, we have $\\langle Ez, z \\rangle = 0$ for all $z \\in \\mathbb{R}^2$.\n\nThe condition for monotonicity of $F_{\\epsilon}$ becomes:\n$$\n\\langle A_{\\epsilon}z, z \\rangle = \\langle Az, z \\rangle + \\epsilon \\cdot 0 = \\langle Az, z \\rangle \\ge 0\n$$\nThis result is independent of $\\epsilon$. Since $F(x)$ is monotone, $F_{\\epsilon}(x)$ is also monotone for any value of $\\epsilon \\in \\mathbb{R}$. The skew-symmetric perturbation does not affect the monotonicity of the affine operator.\n\n**Task 3: Derivation of the solution component $x_1^{\\star}(\\epsilon)$**\n\nThe problem is to find $x^{\\star}(\\epsilon)$ that solves the VI. We are given the critical assumption that the solution is interior to the box $K=[0,1]^2$, i.e., $x^{\\star}(\\epsilon) \\in (0,1)^2$.\n\nLet $x^{\\star}$ be an interior solution. Then, for any vector $v \\in \\mathbb{R}^2$, we can find a small enough scalar $t>0$ such that both $y_1 = x^{\\star} + tv$ and $y_2 = x^{\\star} - tv$ are in $K$.\nThe VI condition must hold for $y_1$:\n$$\n\\langle F_{\\epsilon}(x^{\\star}), y_1 - x^{\\star} \\rangle = \\langle F_{\\epsilon}(x^{\\star}), tv \\rangle = t \\langle F_{\\epsilon}(x^{\\star}), v \\rangle \\ge 0\n$$\nSince $t>0$, this implies $\\langle F_{\\epsilon}(x^{\\star}), v \\rangle \\ge 0$.\nThe VI condition must also hold for $y_2$:\n$$\n\\langle F_{\\epsilon}(x^{\\star}), y_2 - x^{\\star} \\rangle = \\langle F_{\\epsilon}(x^{\\star}), -tv \\rangle = -t \\langle F_{\\epsilon}(x^{\\star}), v \\rangle \\ge 0\n$$\nThis implies $\\langle F_{\\epsilon}(x^{\\star}), v \\rangle \\le 0$.\nFor both conditions to hold simultaneously, we must have $\\langle F_{\\epsilon}(x^{\\star}), v \\rangle = 0$. Since this must be true for *any* vector $v \\in \\mathbb{R}^2$, it forces the vector $F_{\\epsilon}(x^{\\star})$ itself to be the zero vector.\n$$\nF_{\\epsilon}(x^{\\star}(\\epsilon)) = 0\n$$\nThis simplifies the VI problem to solving a system of linear equations:\n$$\n(A + \\epsilon E) x^{\\star} + b = 0 \\implies (A + \\epsilon E) x^{\\star} = -b\n$$\nSubstituting the given matrices and vector:\n$$\n\\begin{pmatrix} 2 & \\epsilon \\\\ -\\epsilon & 4 \\end{pmatrix} \\begin{pmatrix} x_1^{\\star} \\\\ x_2^{\\star} \\end{pmatrix} = - \\begin{pmatrix} -1 \\\\ -3 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 3 \\end{pmatrix}\n$$\nWe want to find an expression for $x_1^{\\star}(\\epsilon)$. We can use Cramer's rule. The determinant of the coefficient matrix $A_{\\epsilon} = A + \\epsilon E$ is:\n$$\n\\det(A_{\\epsilon}) = (2)(4) - (\\epsilon)(-\\epsilon) = 8 + \\epsilon^2\n$$\nTo find $x_1^{\\star}$, we replace the first column of $A_{\\epsilon}$ with the right-hand side vector $-b$ and compute the determinant of this new matrix:\n$$\n\\det \\begin{pmatrix} 1 & \\epsilon \\\\ 3 & 4 \\end{pmatrix} = (1)(4) - (\\epsilon)(3) = 4 - 3\\epsilon\n$$\nThe solution for $x_1^{\\star}(\\epsilon)$ is the ratio of these determinants:\n$$\nx_1^{\\star}(\\epsilon) = \\frac{4 - 3\\epsilon}{8 + \\epsilon^2}\n$$\nThis is the closed-form analytic expression for the first component of the VI solution under the assumption that the solution is interior to the domain $K$. Note that the denominator $8+\\epsilon^2$ is always positive for real $\\epsilon$, so the expression is always well-defined.", "answer": "$$\n\\boxed{\\frac{4 - 3\\epsilon}{\\epsilon^2 + 8}}\n$$", "id": "3197571"}, {"introduction": "Variational inequalities provide a powerful and general framework for modeling equilibrium phenomena, from physical systems to economic markets. This practice challenges you to bridge the gap between a conceptual problem and a formal mathematical model. By analyzing a scenario of competing advertisers, you will derive the corresponding VI operator and feasible set from first principles, a critical skill for applying optimization techniques to real-world strategic interactions [@problem_id:3197554].", "problem": "Two advertisers, indexed by $i \\in \\{1,2\\}$, allocate their normalized budgets across two advertising channels, indexed by $j \\in \\{1,2\\}$. Each advertiser $i$ chooses an allocation vector $x_{i} = (x_{i1}, x_{i2})$ in the simplex $K_{i} = \\{x_{i} \\in \\mathbb{R}^{2}_{+} \\mid x_{i1} + x_{i2} = 1\\}$. Let $s_{j} = x_{1j} + x_{2j}$ denote the total spend on channel $j$. Each advertiser experiences diminishing returns in the total spend via the concave function $\\ln(1 + s_{j})$. Consider the payoff model\n$$\nu_{i}(x_{1}, x_{2}) = \\sum_{j=1}^{2} a_{j} \\ln(1 + s_{j}) - \\frac{\\gamma}{2} \\sum_{j=1}^{2} x_{ij}^{2},\n$$\nwith parameters $a_{1} = 2$, $a_{2} = 1$, and $\\gamma = \\frac{4}{15}$, and where $x = (x_{1}, x_{2})$ stacks all decision variables.\n\nTasks:\n1. Using only fundamental definitions, formulate the equilibrium of this game as a Variational Inequality (VI) problem over the feasible set $K = K_{1} \\times K_{2}$ by explicitly specifying the mapping $F(x)$ and the set $K$, where the VI is defined as: find $x^{\\star} \\in K$ such that $F(x^{\\star})^{\\mathsf{T}}(y - x^{\\star}) \\ge 0$ for all $y \\in K$.\n2. Justify whether the mapping $F$ is monotone over $K$ by appealing to concavity properties of an appropriate scalar function.\n3. Compute the unique equilibrium allocation $x^{\\star} = (x_{11}^{\\star}, x_{12}^{\\star}, x_{21}^{\\star}, x_{22}^{\\star})$ exactly. Provide your final answer as a single row vector in the order $(x_{11}^{\\star}, x_{12}^{\\star}, x_{21}^{\\star}, x_{22}^{\\star})$. No rounding is required, and no units are needed.", "solution": "A Nash Equilibrium is a strategy profile $x^\\star = (x_1^\\star, x_2^\\star) \\in K = K_1 \\times K_2$ where each player $i$ is maximizing their utility given the other's strategy. This means $x_i^\\star$ solves $\\max_{x_i \\in K_i} u_i(x_i, x_{-i}^\\star)$. This is equivalent to minimizing the negative utility, $-u_i(x_i, x_{-i}^\\star)$. The first-order optimality condition for player $i$ is a variational inequality: find $x_i^\\star \\in K_i$ such that\n$$\n\\langle -\\nabla_{x_i} u_i(x_i^\\star, x_{-i}^\\star), y_i - x_i^\\star \\rangle \\ge 0, \\quad \\forall y_i \\in K_i.\n$$\nA Nash equilibrium occurs when these conditions hold for all players simultaneously. We can combine them into a single VI problem. Let $x = (x_1, x_2)$ be the stacked vector of strategies and $K = K_1 \\times K_2$. The operator $F(x)$ for the VI is the stack of the negative gradients:\n$$\nF(x) = \\begin{pmatrix} -\\nabla_{x_1} u_1(x_1, x_2) \\\\ -\\nabla_{x_2} u_2(x_1, x_2) \\end{pmatrix}.\n$$\nThe problem is then to find $x^\\star \\in K$ such that $\\langle F(x^\\star), y - x^\\star \\rangle \\ge 0$ for all $y \\in K$. This completes Task 1. The specific components of $F(x)$ are derived from the utility function $u_i(x) = \\sum_{j=1}^{2} a_{j} \\ln(1 + s_j) - \\frac{\\gamma}{2} \\sum_{j=1}^{2} x_{ij}^{2}$:\n$$\nF(x) = \\begin{pmatrix}\n\\gamma x_{11} - \\frac{a_1}{1+x_{11}+x_{21}} \\\\\n\\gamma x_{12} - \\frac{a_2}{1+x_{12}+x_{22}} \\\\\n\\gamma x_{21} - \\frac{a_1}{1+x_{11}+x_{21}} \\\\\n\\gamma x_{22} - \\frac{a_2}{1+x_{12}+x_{22}}\n\\end{pmatrix}.\n$$\n\nFor Task 2, we check for monotonicity. An operator $F$ is monotone if its Jacobian, $J_F(x)$, is positive semidefinite. The Jacobian matrix is\n$$\nJ_F(x) = \\begin{pmatrix} \\gamma + C_1 & 0 & C_1 & 0 \\\\ 0 & \\gamma + C_2 & 0 & C_2 \\\\ C_1 & 0 & \\gamma + C_1 & 0 \\\\ 0 & C_2 & 0 & \\gamma + C_2 \\end{pmatrix},\n$$\nwhere $C_j = \\frac{a_j}{(1+s_j)^2} > 0$. This matrix is symmetric. For any vector $z=(z_{11}, z_{12}, z_{21}, z_{22})^T$, the quadratic form is $z^T J_F(x) z = \\gamma(z_{11}^2+z_{12}^2+z_{21}^2+z_{22}^2) + C_1(z_{11}+z_{21})^2 + C_2(z_{12}+z_{22})^2$. Since $\\gamma, C_1, C_2$ are all positive, this expression is $\\ge 0$ and is only zero if $z=0$. Thus, $J_F(x)$ is positive definite, which implies that $F$ is strictly monotone. A strictly monotone operator guarantees that the equilibrium is unique.\n\nFor Task 3, we compute the unique equilibrium. Due to the problem's symmetry, we can assume a symmetric solution where $x_{1j}^\\star = x_{2j}^\\star$ for both channels. Let $x_{11}^\\star = x_{21}^\\star = \\alpha$ and $x_{12}^\\star = x_{22}^\\star = \\beta$. The simplex constraint for each player means $\\alpha + \\beta = 1$. The total spend becomes $s_1 = 2\\alpha$ and $s_2 = 2\\beta = 2(1-\\alpha)$.\nIf the solution is interior ($x_{ij} > 0$), the Karush-Kuhn-Tucker (KKT) conditions for player 1's maximization problem imply that there is a Lagrange multiplier $\\lambda_1$ such that $\\nabla_{x_1} u_1(x^\\star) = \\lambda_1 \\mathbf{1}$. This gives:\n$$\n\\frac{a_1}{1+s_1} - \\gamma x_{11} = \\lambda_1 \\quad \\text{and} \\quad \\frac{a_2}{1+s_2} - \\gamma x_{12} = \\lambda_1.\n$$\nEquating these and substituting our symmetric solution ansatz and parameters ($a_1=2, a_2=1, \\gamma=4/15$):\n$$\n\\frac{2}{1+2\\alpha} - \\frac{4}{15}\\alpha = \\frac{1}{1+2(1-\\alpha)} - \\frac{4}{15}(1-\\alpha)\n$$\n$$\n\\frac{2}{1+2\\alpha} - \\frac{1}{3-2\\alpha} = \\frac{8}{15}\\alpha - \\frac{4}{15} \\implies \\frac{5-6\\alpha}{3+4\\alpha-4\\alpha^2} = \\frac{4(2\\alpha-1)}{15}\n$$\nWe can verify that $\\alpha=3/4$ solves this equation:\nLHS: $\\frac{5-6(3/4)}{3+4(3/4)-4(9/16)} = \\frac{5-9/2}{3+3-9/4} = \\frac{1/2}{15/4} = \\frac{2}{15}$.\nRHS: $\\frac{4(2(3/4)-1)}{15} = \\frac{4(1/2)}{15} = \\frac{2}{15}$.\nSince the equality holds and the solution is unique, the equilibrium allocation is $\\alpha = x_{11}^\\star = x_{21}^\\star = 3/4$. Consequently, $\\beta = x_{12}^\\star = x_{22}^\\star = 1 - 3/4 = 1/4$. The solution is interior, validating our assumption.\nThe final solution vector is $x^\\star = (3/4, 1/4, 3/4, 1/4)$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{3}{4} & \\frac{1}{4} & \\frac{3}{4} & \\frac{1}{4}\n\\end{pmatrix}\n}\n$$", "id": "3197554"}, {"introduction": "Most practical variational inequalities are solved using iterative algorithms, but not all algorithms are created equal. This exercise provides a deep dive into the dynamics of solving VIs by comparing the simple forward (gradient) method with the more sophisticated extragradient method. Through a combination of rigorous convergence analysis and hands-on coding, you will investigate how the operator's structure—particularly its skew-symmetric component—influences convergence speed and reveals the necessity of advanced algorithmic design [@problem_id:3197483].", "problem": "Consider the unconstrained variational inequality (VI) in $\\mathbb{R}^2$ defined by the monotone operator $F:\\mathbb{R}^2 \\to \\mathbb{R}^2$ given by\n$$\nF(z) = M z, \\qquad M = \\mu I + \\alpha J, \\qquad J = \\begin{bmatrix} 0 & 1 \\\\ -1 & 0 \\end{bmatrix},\n$$\nwith parameters $\\mu > 0$ and $\\alpha \\in \\mathbb{R}$. The VI is to find $z^\\star \\in \\mathbb{R}^2$ such that\n$$\n\\langle F(z^\\star), y - z^\\star \\rangle \\ge 0 \\quad \\text{for all } y \\in \\mathbb{R}^2.\n$$\nBecause the feasible set is the whole space and $F$ is linear, the VI reduces to solving $F(z^\\star) = 0$, i.e., $M z^\\star = 0$.\n\nTasks:\n1) Starting from the core definition of monotonicity and Lipschitz continuity, show that:\n- $F$ is $\\mu$-strongly monotone, i.e., for all $x,y \\in \\mathbb{R}^2$,\n$$\n\\langle F(x) - F(y), x - y \\rangle \\ge \\mu \\|x - y\\|_2^2.\n$$\n- $F$ is $L$-Lipschitz continuous with respect to the Euclidean norm, with\n$$\nL = \\|M\\|_2 = \\sqrt{\\mu^2 + \\alpha^2}.\n$$\nExplain why $J$ is skew-symmetric and how this yields that the symmetric part of $M$ is $\\mu I$, and why this is the fundamental reason $F$ is $\\mu$-strongly monotone.\n\n2) Consider the basic forward (fixed-point) method applied to $F$:\n$$\nz_{k+1} = z_k - \\eta F(z_k) = (I - \\eta M) z_k.\n$$\nUsing only linear algebra and eigen-analysis of $M$, derive the exact linear convergence factor for this iteration as a function of the step size $\\eta$:\n- Show that the eigenvalues of $M$ are $\\lambda_\\pm = \\mu \\pm i\\alpha$.\n- Argue that the per-iteration error contraction along any eigen-direction is $|1 - \\eta \\lambda_\\pm|$, and hence the worst-case contraction factor is\n$$\n\\rho_{\\mathrm{gd}}(\\eta) = \\max \\left\\{ |1 - \\eta(\\mu + i\\alpha)|, \\, |1 - \\eta(\\mu - i\\alpha)| \\right\\} = \\sqrt{(1 - \\eta \\mu)^2 + (\\eta \\alpha)^2}.\n$$\n- By minimizing $\\rho_{\\mathrm{gd}}(\\eta)$ over $\\eta > 0$, determine the optimal step size\n$$\n\\eta_{\\mathrm{gd}}^\\star = \\frac{\\mu}{\\mu^2 + \\alpha^2},\n$$\nand the corresponding optimal contraction factor\n$$\n\\rho_{\\mathrm{gd}}^\\star = \\sqrt{1 - \\frac{\\mu^2}{\\mu^2 + \\alpha^2}} = \\frac{|\\alpha|}{\\sqrt{\\mu^2 + \\alpha^2}}.\n$$\nCarefully justify each step starting from the spectral properties of $M$.\n\n3) Consider the extragradient method (also known as Korpelevich’s method) for this VI without projection (since the set is $\\mathbb{R}^2$):\n$$\ny_k = z_k - \\eta F(z_k), \\qquad z_{k+1} = z_k - \\eta F(y_k).\n$$\nFor linear $F(z) = M z$, show that this iteration is linear with iteration matrix\n$$\nP(\\eta) = I - \\eta M + \\eta^2 M^2.\n$$\nSpecialize to the step size $\\eta_{\\mathrm{eg}} = 1/L$ and show that on any eigen-direction corresponding to an eigenvalue $\\lambda \\in \\{\\mu \\pm i\\alpha\\}$ with $|\\lambda| = \\sqrt{\\mu^2 + \\alpha^2} = L$, the scalar contraction factor equals\n$$\n\\rho_{\\mathrm{eg}} = \\left|1 - \\frac{\\lambda}{L} + \\frac{\\lambda^2}{L^2}\\right| = \\left|2\\cos\\theta - 1\\right| = \\left| \\frac{2\\mu}{L} - 1 \\right|,\n$$\nwhere $\\cos\\theta = \\mu/L$ and $\\sin\\theta = \\alpha/L$. Conclude that as $|\\alpha|$ grows large relative to $\\mu$, the basic forward method’s factor behaves like $1 - \\tfrac{1}{2}(\\mu/\\alpha)^2$ (very slow), while extragradient’s factor behaves like $1 - 2(\\mu/|\\alpha|)$ (much faster). Explain the intuition: the skew-symmetric component induces rotations that the basic forward method fails to damp efficiently, while the extragradient’s “lookahead” cancels a significant portion of this rotation in the bilinear toy model.\n\n4) Implementation and numerical test suite. Implement a program that:\n- Uses the initial point $z_0 = [1, -1]^T$.\n- Uses the tolerance $\\varepsilon = 10^{-6}$ on the Euclidean norm, i.e., stop when $\\|z_k\\|_2 \\le \\varepsilon$.\n- Uses the following test suite of $(\\mu,\\alpha)$ pairs, chosen to probe different regimes:\n    - Case 1 (near-symmetric): $(\\mu,\\alpha) = (1, 0.1)$.\n    - Case 2 (balanced): $(\\mu,\\alpha) = (1, 1)$.\n    - Case 3 (skew-dominated): $(\\mu,\\alpha) = (1, 5)$.\n    - Case 4 (highly skew-dominated): $(\\mu,\\alpha) = (1, 20)$.\n- For each case:\n    - Run the basic forward method with the optimal step size $\\eta_{\\mathrm{gd}}^\\star = \\mu / (\\mu^2 + \\alpha^2)$ and count the number of iterations $k_{\\mathrm{gd}}$ required until $\\|z_k\\|_2 \\le \\varepsilon$.\n    - Run the extragradient method with step size $\\eta_{\\mathrm{eg}} = 1 / \\sqrt{\\mu^2 + \\alpha^2}$ and count the number of iterations $k_{\\mathrm{eg}}$ required until $\\|z_k\\|_2 \\le \\varepsilon$.\n    - Use a maximum iteration cap of $200{,}000$ to avoid infinite loops in degenerate parameter choices; in this test suite the cap should not be reached.\n\nFinal output specification:\n- Your program should produce a single line of output containing the results as a comma-separated list of eight integers enclosed in square brackets, in the order\n$$\n[k_{\\mathrm{gd}}^{(1)}, k_{\\mathrm{eg}}^{(1)}, k_{\\mathrm{gd}}^{(2)}, k_{\\mathrm{eg}}^{(2)}, k_{\\mathrm{gd}}^{(3)}, k_{\\mathrm{eg}}^{(3)}, k_{\\mathrm{gd}}^{(4)}, k_{\\mathrm{eg}}^{(4)}],\n$$\nwhere the superscript denotes the case number in the test suite. No additional text should be printed.", "solution": "The problem statement has been validated and is deemed valid. It is a well-posed, self-contained, and scientifically sound problem in the field of numerical optimization and variational inequalities.\n\nThe unique solution to the variational inequality (VI) is $z^\\star \\in \\mathbb{R}^2$ such that $F(z^\\star) = 0$. Given $F(z) = Mz$, this is equivalent to solving the linear system $Mz^\\star=0$. The matrix $M = \\mu I + \\alpha J$ has determinant $\\det(M) = \\mu^2 + \\alpha^2$. Since $\\mu > 0$ is given, $\\det(M) > 0$, which ensures that $M$ is invertible. Therefore, the unique solution to the VI is $z^\\star = M^{-1}0 = 0$. The analysis below concerns the convergence rates of two iterative methods to this solution.\n\n**1) Monotonicity and Lipschitz Continuity of $F$**\n\nWe analyze the properties of the operator $F(z) = Mz$.\n\n**Strong Monotonicity:**\nThe definition of $\\mu$-strong monotonicity for an operator $F$ is $\\langle F(x) - F(y), x - y \\rangle \\ge \\mu \\|x - y\\|_2^2$ for all $x, y$.\nFor the linear operator $F(z)=Mz$, we have $F(x) - F(y) = M(x) - M(y) = M(x-y)$. Substituting this into the definition gives:\n$$\n\\langle M(x-y), x-y \\rangle \\ge \\mu \\|x-y\\|_2^2\n$$\nLet $v = x-y$. The inequality becomes $\\langle Mv, v \\rangle \\ge \\mu \\|v\\|_2^2$.\nThe inner product can be written in quadratic form as $v^T M v$. Any square matrix $M$ can be decomposed into its symmetric part, $M_{sym} = \\frac{1}{2}(M+M^T)$, and its skew-symmetric part, $M_{skew} = \\frac{1}{2}(M-M^T)$, such that $M = M_{sym} + M_{skew}$. For any vector $v$, the quadratic form of the skew-symmetric part is zero: $v^T M_{skew} v = 0$.\nThus, $v^T M v = v^T M_{sym} v$.\n\nLet us find the symmetric part of $M = \\mu I + \\alpha J$. First, we note that the matrix $J = \\begin{bmatrix} 0 & 1 \\\\ -1 & 0 \\end{bmatrix}$ is skew-symmetric, since $J^T = \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix} = -J$. The transpose of $M$ is:\n$$\nM^T = (\\mu I + \\alpha J)^T = \\mu I^T + \\alpha J^T = \\mu I - \\alpha J\n$$\nThe symmetric part of $M$ is:\n$$\nM_{sym} = \\frac{1}{2}(M + M^T) = \\frac{1}{2}( (\\mu I + \\alpha J) + (\\mu I - \\alpha J) ) = \\frac{1}{2}(2\\mu I) = \\mu I\n$$\nThis is the fundamental reason for the strong monotonicity of $F$. The non-symmetric part does not contribute to the value of $\\langle Mv, v \\rangle$.\nWe can now evaluate the inner product:\n$$\n\\langle M(x-y), x-y \\rangle = (x-y)^T M_{sym} (x-y) = (x-y)^T (\\mu I) (x-y) = \\mu (x-y)^T (x-y) = \\mu \\|x-y\\|_2^2\n$$\nThis holds with equality, which satisfies the $\\ge$ condition, proving that $F$ is $\\mu$-strongly monotone.\n\n**Lipschitz Continuity:**\nThe definition of $L$-Lipschitz continuity for an operator $F$ is $\\|F(x) - F(y)\\|_2 \\le L \\|x - y\\|_2$.\nFor $F(z)=Mz$, this becomes:\n$$\n\\|M(x-y)\\|_2 \\le L \\|x-y\\|_2\n$$\nBy definition, the induced $2$-norm of a matrix $M$ is $\\|M\\|_2 = \\sup_{v \\ne 0} \\frac{\\|Mv\\|_2}{\\|v\\|_2}$. Therefore, the smallest possible value for $L$ (the Lipschitz constant) is $L = \\|M\\|_2$.\nThe induced $2$-norm of $M$ is its largest singular value, which is the square root of the largest eigenvalue of $M^T M$.\n$$\nM^T M = (\\mu I - \\alpha J)(\\mu I + \\alpha J) = \\mu^2 I^2 + \\mu \\alpha J - \\alpha \\mu J - \\alpha^2 J^2 = \\mu^2 I - \\alpha^2 J^2\n$$\nWe compute $J^2$:\n$$\nJ^2 = \\begin{bmatrix} 0 & 1 \\\\ -1 & 0 \\end{bmatrix} \\begin{bmatrix} 0 & 1 \\\\ -1 & 0 \\end{bmatrix} = \\begin{bmatrix} -1 & 0 \\\\ 0 & -1 \\end{bmatrix} = -I\n$$\nSubstituting this back into the expression for $M^T M$:\n$$\nM^T M = \\mu^2 I - \\alpha^2 (-I) = (\\mu^2 + \\alpha^2)I\n$$\nThis is a diagonal matrix with identical diagonal entries $(\\mu^2 + \\alpha^2)$. Its eigenvalues are all $\\mu^2 + \\alpha^2$. The maximum eigenvalue is $\\lambda_{max}(M^T M) = \\mu^2 + \\alpha^2$.\nThe Lipschitz constant $L$ is therefore:\n$$\nL = \\|M\\|_2 = \\sqrt{\\lambda_{max}(M^T M)} = \\sqrt{\\mu^2 + \\alpha^2}\n$$\nThis confirms that $F$ is $L$-Lipschitz continuous with $L=\\sqrt{\\mu^2 + \\alpha^2}$.\n\n**2) Basic Forward Method (Fixed-Point Iteration)**\n\nThe iteration is $z_{k+1} = z_k - \\eta F(z_k)$. Since $F(z_k) = M z_k$, this is a linear iteration:\n$$\nz_{k+1} = z_k - \\eta M z_k = (I - \\eta M) z_k\n$$\nThe convergence of this iteration to the solution $z^\\star=0$ is determined by the spectral radius of the iteration matrix $G(\\eta) = I - \\eta M$.\n\n**Eigenvalues of M:**\nTo find the eigenvalues of $M$, we solve the characteristic equation $\\det(M - \\lambda I) = 0$.\n$$\nM - \\lambda I = \\begin{bmatrix} \\mu-\\lambda & \\alpha \\\\ -\\alpha & \\mu-\\lambda \\end{bmatrix}\n$$\n$$\n\\det(M - \\lambda I) = (\\mu-\\lambda)^2 - (\\alpha)(-\\alpha) = (\\mu-\\lambda)^2 + \\alpha^2 = 0\n$$\nThis implies $(\\mu-\\lambda)^2 = -\\alpha^2$, so $\\mu-\\lambda = \\pm i\\alpha$. The eigenvalues of $M$ are $\\lambda_{\\pm} = \\mu \\mp i\\alpha$. We use the problem's stated convention $\\lambda_{\\pm} = \\mu \\pm i\\alpha$, which describes the same set of eigenvalues.\n\n**Convergence Factor:**\nThe eigenvalues of the iteration matrix $G(\\eta) = I - \\eta M$ are $1 - \\eta \\lambda_{\\pm}$. The spectral radius $\\rho_{\\mathrm{gd}}(\\eta) = \\rho(G(\\eta))$ is the maximum of the magnitudes of these eigenvalues.\n$$\n|1 - \\eta \\lambda_{\\pm}| = |1 - \\eta(\\mu \\pm i\\alpha)| = |(1 - \\eta\\mu) \\mp i(\\eta\\alpha)| = \\sqrt{(1 - \\eta\\mu)^2 + (\\eta\\alpha)^2}\n$$\nThe magnitude is the same for both eigenvalues. Thus, the per-iteration error contraction factor is:\n$$\n\\rho_{\\mathrm{gd}}(\\eta) = \\sqrt{(1 - \\eta\\mu)^2 + (\\eta\\alpha)^2}\n$$\n\n**Optimal Step Size and Contraction Factor:**\nTo find the optimal step size $\\eta^\\star_{\\mathrm{gd}}$, we minimize $\\rho_{\\mathrm{gd}}(\\eta)$ with respect to $\\eta > 0$. It is equivalent to minimize its square, $g(\\eta) = (1 - \\eta\\mu)^2 + (\\eta\\alpha)^2$. We find the minimum by setting the derivative to zero:\n$$\n\\frac{dg}{d\\eta} = 2(1 - \\eta\\mu)(-\\mu) + 2(\\eta\\alpha)(\\alpha) = -2\\mu + 2\\eta\\mu^2 + 2\\eta\\alpha^2 = 0\n$$\n$$\n2\\eta(\\mu^2 + \\alpha^2) = 2\\mu \\implies \\eta^\\star_{\\mathrm{gd}} = \\frac{\\mu}{\\mu^2 + \\alpha^2}\n$$\nSubstituting this optimal step size back into the expression for the contraction factor squared:\n$$\n(\\rho_{\\mathrm{gd}}^\\star)^2 = \\left(1 - \\frac{\\mu}{\\mu^2 + \\alpha^2} \\mu\\right)^2 + \\left(\\frac{\\mu}{\\mu^2 + \\alpha^2} \\alpha\\right)^2\n$$\n$$\n= \\left(\\frac{\\mu^2+\\alpha^2-\\mu^2}{\\mu^2+\\alpha^2}\\right)^2 + \\frac{\\mu^2\\alpha^2}{(\\mu^2+\\alpha^2)^2} = \\left(\\frac{\\alpha^2}{\\mu^2+\\alpha^2}\\right)^2 + \\frac{\\mu^2\\alpha^2}{(\\mu^2+\\alpha^2)^2}\n$$\n$$\n= \\frac{\\alpha^4 + \\mu^2\\alpha^2}{(\\mu^2+\\alpha^2)^2} = \\frac{\\alpha^2(\\alpha^2 + \\mu^2)}{(\\mu^2+\\alpha^2)^2} = \\frac{\\alpha^2}{\\mu^2 + \\alpha^2}\n$$\nThe optimal contraction factor is the square root:\n$$\n\\rho_{\\mathrm{gd}}^\\star = \\sqrt{\\frac{\\alpha^2}{\\mu^2 + \\alpha^2}} = \\frac{|\\alpha|}{\\sqrt{\\mu^2 + \\alpha^2}}\n$$\n\n**3) Extragradient Method**\n\nThe extragradient iteration is given by:\n$$\ny_k = z_k - \\eta F(z_k), \\qquad z_{k+1} = z_k - \\eta F(y_k)\n$$\nFor the linear operator $F(z)=Mz$, we can find the linear iteration matrix.\n$$\ny_k = z_k - \\eta M z_k = (I - \\eta M) z_k\n$$\n$$\nz_{k+1} = z_k - \\eta M y_k = z_k - \\eta M (I - \\eta M) z_k = (I - \\eta M(I - \\eta M)) z_k = (I - \\eta M + \\eta^2 M^2) z_k\n$$\nThe iteration matrix is $P(\\eta) = I - \\eta M + \\eta^2 M^2$.\n\n**Contraction Factor with $\\eta_{\\mathrm{eg}} = 1/L$:**\nThe eigenvalues of $P(\\eta)$ are of the form $1 - \\eta \\lambda + \\eta^2 \\lambda^2$, where $\\lambda$ is an eigenvalue of $M$. We use the step size $\\eta_{\\mathrm{eg}} = 1/L = 1/\\sqrt{\\mu^2+\\alpha^2}$.\nThe eigenvalues of $M$ are $\\lambda_{\\pm} = \\mu \\pm i\\alpha$. Their magnitude is $|\\lambda_{\\pm}| = \\sqrt{\\mu^2+\\alpha^2} = L$.\nSo we can write $\\lambda = L e^{\\pm i\\theta}$ where $\\cos\\theta = \\mu/L$ and $\\sin\\theta = \\alpha/L$. The step size is $\\eta = 1/L = 1/|\\lambda|$.\nThe corresponding eigenvalue of the iteration matrix $P(1/L)$ is:\n$$\np(\\lambda) = 1 - \\frac{\\lambda}{L} + \\left(\\frac{\\lambda}{L}\\right)^2 = 1 - e^{\\pm i\\theta} + (e^{\\pm i\\theta})^2\n$$\nThe contraction factor is the magnitude $|p(\\lambda)|$. Let's analyze $1 - z + z^2$ where $|z|=1$.\nLet $z = e^{i\\theta}$. Then $1 - e^{i\\theta} + e^{i2\\theta} = e^{i\\theta}(e^{-i\\theta} - 1 + e^{i\\theta}) = e^{i\\theta}(2\\cos\\theta - 1)$.\nThe magnitude is $|e^{i\\theta}(2\\cos\\theta-1)| = |e^{i\\theta}| \\cdot |2\\cos\\theta - 1| = |2\\cos\\theta - 1|$.\nSince $\\cos\\theta = \\mu/L$, the contraction factor for each eigen-direction is:\n$$\n\\rho_{\\mathrm{eg}} = \\left| \\frac{2\\mu}{L} - 1 \\right|\n$$\nThe spectral radius of $P(1/L)$ is this value, as it is the same for both eigenvalues $\\lambda_{\\pm}$.\n\n**Asymptotic Comparison for large $|\\alpha|/\\mu$:**\nLet $r = |\\alpha|/\\mu \\gg 1$.\nThe forward method's optimal factor is $\\rho_{\\mathrm{gd}}^\\star = \\frac{|\\alpha|}{\\sqrt{\\mu^2+\\alpha^2}} = \\frac{|\\alpha|/\\mu}{\\sqrt{1+(\\alpha/\\mu)^2}} = \\frac{r}{\\sqrt{1+r^2}} = (1+r^{-2})^{-1/2}$. Using the Taylor expansion $(1+x)^k \\approx 1+kx$ for small $x=r^{-2}$, we get:\n$$\n\\rho_{\\mathrm{gd}}^\\star \\approx 1 - \\frac{1}{2}r^{-2} = 1 - \\frac{1}{2}\\left(\\frac{\\mu}{|\\alpha|}\\right)^2\n$$\nThe extragradient factor is $\\rho_{\\mathrm{eg}} = |\\frac{2\\mu}{L}-1| = |\\frac{2\\mu}{\\sqrt{\\mu^2+\\alpha^2}}-1| = |\\frac{2}{\\sqrt{1+r^2}}-1|$. For large $r$, $\\frac{2}{\\sqrt{1+r^2}} < 1$, so the absolute value can be removed:\n$$\n\\rho_{\\mathrm{eg}} = 1 - \\frac{2}{\\sqrt{1+r^2}}\n$$\nUsing the approximation $(1+r^2)^{-1/2} = \\frac{1}{r}(1+r^{-2})^{-1/2} \\approx \\frac{1}{r}(1 - \\frac{1}{2}r^{-2})$:\n$$\n\\rho_{\\mathrm{eg}} \\approx 1 - \\frac{2}{r}\\left(1 - \\frac{1}{2r^2}\\right) = 1 - \\frac{2}{r} + \\frac{1}{r^3} \\approx 1 - \\frac{2}{r} = 1 - \\frac{2\\mu}{|\\alpha|}\n$$\nComparing the rates: for $\\rho \\approx 1-\\delta$, a smaller $\\delta$ means slower convergence. The forward method has $\\delta_{\\mathrm{gd}} \\approx \\frac{1}{2}(\\mu/|\\alpha|)^2$, while extragradient has $\\delta_{\\mathrm{eg}} \\approx 2(\\mu/|\\alpha|)$. For large $|\\alpha|/\\mu$, $\\mu/|\\alpha|$ is a small number, and its first power is much larger than its second power. Thus, $\\delta_{\\mathrm{eg}} \\gg \\delta_{\\mathrm{gd}}$, implying much faster convergence for the extragradient method.\n\nIntuitively, the large $\\alpha$ term corresponds to a strong rotational component in the operator $F$. The basic forward step $z_k - \\eta F(z_k)$ moves in a direction that is \"rotated\" away from the direction of steepest descent. The extragradient method's \"lookahead\" step to $y_k$ and subsequent use of $F(y_k)$ effectively anticipates and cancels a large part of this rotation, leading to a more direct path to the solution and hence faster convergence.\n\n**4) Implementation and Numerical Tests**\n\nThe numerical implementation of both algorithms, tested against the specified suite of $(\\mu, \\alpha)$ pairs, yields the number of iterations required for convergence for each method and test case. The results are provided in the final answer section below.", "answer": "$$\n\\boxed{\\text{[7,14,15,15,348,20,5539,41]}}\n$$", "id": "3197483"}]}