{"hands_on_practices": [{"introduction": "The choice of step size is paramount in the subgradient method. This first exercise explores a fundamental behavior of the algorithm by using a constant, oversized step size, which can cause the iterates to overshoot the minimum. Understanding this potential for oscillation [@problem_id:2207140] is a crucial first step in appreciating why more sophisticated step-size rules are necessary for guaranteed convergence.", "problem": "Consider the problem of minimizing the nonsmooth convex function $f(x) = |x|$ using the subgradient method. The iterative update rule for the subgradient method is given by $x_{k+1} = x_k - \\alpha_k g_k$, where $x_k$ is the iterate at step $k$, $\\alpha_k$ is the step size, and $g_k$ is a subgradient of the function $f$ at the point $x_k$. For the function $f(x)=|x|$, a valid subgradient at any point $x \\neq 0$ is given by $g = \\text{sgn}(x)$, where $\\text{sgn}(x)$ is the sign function which returns $1$ for $x0$ and $-1$ for $x0$.\n\nSuppose we start the iteration at the point $x_0 = 5$ and use a constant step size $\\alpha_k = \\alpha = 10$ for all steps.\n\nCalculate the value of the next iterate, $x_1$.", "solution": "We are minimizing the convex nonsmooth function $f(x)=|x|$ using the subgradient method with update rule\n$$\nx_{k+1} = x_{k} - \\alpha_{k} g_{k},\n$$\nwhere $g_{k} \\in \\partial f(x_{k})$ is a subgradient of $f$ at $x_{k}$. For $f(x)=|x|$ and any $x \\neq 0$, a valid subgradient is $g=\\text{sgn}(x)$, where $\\text{sgn}(x)=1$ if $x0$ and $\\text{sgn}(x)=-1$ if $x0$.\n\nGiven $x_{0}=5$ and constant step size $\\alpha_{k}=\\alpha=10$, we compute the subgradient at $x_{0}$:\n$$\ng_{0}=\\text{sgn}(x_{0})=\\text{sgn}(5)=1.\n$$\nApplying the update rule,\n$$\nx_{1}=x_{0}-\\alpha g_{0}=5-10\\cdot 1=-5.\n$$\nThus, the next iterate is $x_{1}=-5$.", "answer": "$$\\boxed{-5}$$", "id": "2207140"}, {"introduction": "Building upon the basic update rule, we now move to a more practical scenario in two dimensions. This problem models a robotic agent minimizing its distance to a target, providing hands-on practice with vector subgradients and the widely used diminishing step-size rule. This method [@problem_id:2207185] offers a reliable path to convergence without requiring prior knowledge of the optimal solution's value.", "problem": "A robotic agent is being trained to navigate to a target coordinate in a 2D plane. The cost function to be minimized is the Manhattan distance (also known as the L1 distance) to the target, defined as $C(\\mathbf{x}) = \\|\\mathbf{x} - \\mathbf{p}\\|_1$, where $\\mathbf{x}=(x,y)$ is the agent's current position and $\\mathbf{p}=(p_x, p_y)$ is the target position. For a vector $\\mathbf{v}=(v_x, v_y)$, the L1 norm is $\\|\\mathbf{v}\\|_1 = |v_x| + |v_y|$.\n\nThe agent uses the subgradient method to update its position. The update rule for the $k$-th iteration is given by $\\mathbf{x}_{k} = \\mathbf{x}_{k-1} - \\alpha_k \\mathbf{g}_{k-1}$, where $\\mathbf{g}_{k-1}$ is any vector from the subdifferential of the cost function $C$ evaluated at position $\\mathbf{x}_{k-1}$.\n\nThe agent starts at an initial position $\\mathbf{x}_0 = (9.5, -2.5)$ and the target is at $\\mathbf{p} = (1.5, 4.5)$. The step size for the $k$-th iteration (for $k=1, 2, \\dots$) is given by the rule $\\alpha_k = \\frac{2}{k}$.\n\nCalculate the agent's position $\\mathbf{x}_2 = (x_2, y_2)$ after two full iterations.", "solution": "We minimize $C(\\mathbf{x})=\\|\\mathbf{x}-\\mathbf{p}\\|_{1}=|x-p_{x}|+|y-p_{y}|$. For $f(t)=|t|$, the subdifferential is $\\partial f(t)=\\{1\\}$ if $t0$, $\\{-1\\}$ if $t0$, and $[-1,1]$ if $t=0$. Therefore, a subgradient of $C$ at $\\mathbf{x}$ is any $\\mathbf{g}=(g_{x},g_{y})$ with $g_{x}\\in\\partial|x-p_{x}|$ and $g_{y}\\in\\partial|y-p_{y}|$.\n\nThe update rule is $\\mathbf{x}_{k}=\\mathbf{x}_{k-1}-\\alpha_{k}\\mathbf{g}_{k-1}$ with $\\alpha_{k}=\\frac{2}{k}$.\n\nIteration $k=1$: With $\\mathbf{x}_{0}=(9.5,-2.5)$ and $\\mathbf{p}=(1.5,4.5)$, we have\n$$\nx_{0}-p_{x}=9.5-1.5=80,\\qquad y_{0}-p_{y}=-2.5-4.5=-70.\n$$\nThus we can choose\n$$\n\\mathbf{g}_{0}=(1,-1).\n$$\nSince $\\alpha_{1}=\\frac{2}{1}=2$, the update is\n$$\n\\mathbf{x}_{1}=\\mathbf{x}_{0}-\\alpha_{1}\\mathbf{g}_{0}\n=(9.5,-2.5)-2(1,-1)=(9.5-2,\\,-2.5+2)=(7.5,-0.5).\n$$\n\nIteration $k=2$: Now with $\\mathbf{x}_{1}=(7.5,-0.5)$,\n$$\nx_{1}-p_{x}=7.5-1.5=60,\\qquad y_{1}-p_{y}=-0.5-4.5=-50,\n$$\nso we may take again\n$$\n\\mathbf{g}_{1}=(1,-1).\n$$\nWith $\\alpha_{2}=\\frac{2}{2}=1$, the update is\n$$\n\\mathbf{x}_{2}=\\mathbf{x}_{1}-\\alpha_{2}\\mathbf{g}_{1}\n=(7.5,-0.5)-1(1,-1)=(7.5-1,\\,-0.5+1)=(6.5,0.5).\n$$\n\nTherefore, after two iterations, the agentâ€™s position is $\\mathbf{x}_{2}=(6.5,0.5)$.", "answer": "$$\\boxed{(6.5, 0.5)}$$", "id": "2207185"}, {"introduction": "While diminishing step sizes guarantee convergence, they can often be slow. This exercise introduces the powerful Polyak step-size rule, a method that leverages knowledge of the optimal function value, $f^*$, to compute a more aggressive and often much faster step. Applying this rule [@problem_id:2207151] demonstrates how incorporating additional information about the problem can dramatically improve the algorithm's performance.", "problem": "Consider the task of minimizing the nonsmooth convex function $f(x) = |4x - 7|$, where $x$ is a scalar variable. It is known that the minimum value of this function is $f^* = 0$. We will use the subgradient method to find a better estimate for the minimizer.\n\nThe update rule for the subgradient method is given by:\n$$x_{k+1} = x_k - \\alpha_k g_k$$\nwhere $x_k$ is the current point, $g_k$ is a subgradient of the function $f$ at the point $x_k$, and $\\alpha_k$ is the step size for that iteration.\n\nStarting from an initial point $x_0 = 1$, perform a single iteration. The step size $\\alpha_0$ for this first iteration is determined by the Polyak step size rule, defined as:\n$$\\alpha_0 = \\frac{f(x_0) - f^*}{\\|g_0\\|^2}$$\n\nCalculate the value of the next iterate, $x_1$. Express your answer as a fraction in simplest form.", "solution": "We have the convex function $f(x)=|4x-7|$. For $h(t)=|t|$, a subgradient is $s\\in\\partial h(t)$ with $s=\\operatorname{sign}(t)$ when $t\\neq 0$ and $s\\in[-1,1]$ when $t=0$. By the subgradient chain rule for an affine argument, a subgradient of $f$ at $x$ is $g=4s$ with $s\\in\\partial|4x-7|$.\n\nAt the initial point $x_{0}=1$, we have $4x_{0}-7=4\\cdot 1-7=-30$, hence $s_{0}=-1$ and thus\n$$\ng_{0}=4(-1)=-4.\n$$\nEvaluate the function and the optimal value: $f(x_{0})=|4\\cdot 1-7|=3$ and $f^{*}=0$. Since $x$ is scalar, $\\|g_{0}\\|^{2}=(-4)^{2}=16$. The Polyak step size is\n$$\n\\alpha_{0}=\\frac{f(x_{0})-f^{*}}{\\|g_{0}\\|^{2}}=\\frac{3}{16}.\n$$\nApply the subgradient update $x_{1}=x_{0}-\\alpha_{0}g_{0}$:\n$$\nx_{1}=1-\\frac{3}{16}\\cdot(-4)=1+\\frac{12}{16}=1+\\frac{3}{4}=\\frac{7}{4}.\n$$\nThus the next iterate is $\\frac{7}{4}$.", "answer": "$$\\boxed{\\frac{7}{4}}$$", "id": "2207151"}]}