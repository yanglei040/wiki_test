{"hands_on_practices": [{"introduction": "The elegance of Lagrangian relaxation lies in its \"divide and conquer\" strategy. By moving complex constraints into the objective function, we are often left with a simpler, \"relaxed\" subproblem. This first practice focuses on that core step by asking you to solve the Lagrangian subproblem for a linear program with simple box constraints. Mastering this derivation [@problem_id:3141448] will build your intuition for how the Lagrange multipliers, often interpreted as \"prices,\" guide the solution of the relaxed problem.", "problem": "Consider the linear optimization problem with box constraints: minimize the linear objective $\\;c^{\\top}x\\;$ subject to the equality constraint $\\;Ax=b\\;$ and bounds $\\;0 \\le x \\le u\\;$, where $A \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^{m}$, $c \\in \\mathbb{R}^{n}$, $u \\in \\mathbb{R}^{n}$ with $u_{i} > 0$ for all indices $i \\in \\{1,\\dots,n\\}$, and inequalities are understood componentwise. Perform Lagrangian relaxation of the equality constraint $\\;Ax=b\\;$ by introducing the vector of Lagrange multipliers $\\;\\lambda \\in \\mathbb{R}^{m}\\;$, and consider the resulting Lagrangian function $\\;L(x,\\lambda)\\;$ and dual function $\\;g(\\lambda) = \\inf_{0 \\le x \\le u} L(x,\\lambda)\\;$.\n\nStarting only from the definition of the Lagrangian for equality-constrained problems and the basic properties of minimizing a linear function over a closed interval, derive the componentwise minimizer $x(\\lambda)$ that attains the infimum in the dual function. In particular, for each index $i$, determine the form of\n$$\nx_{i}(\\lambda) \\in \\arg\\min_{x_{i} \\in [0,u_{i}]} \\left( c_{i} + a_{i}^{\\top}\\lambda \\right) x_{i},\n$$\nwhere $a_{i} \\in \\mathbb{R}^{m}$ denotes the $i$-th column of the matrix $A$. Show and explain the thresholding behavior with respect to the sign of the scalar $\\;c_{i} + a_{i}^{\\top}\\lambda\\;$.\n\nFor definiteness and to ensure a single-valued answer, when the objective is flat over the interval (that is, when $c_{i} + a_{i}^{\\top}\\lambda = 0$), select the smallest minimizer in the set, namely $x_{i}(\\lambda) = 0$. Provide the final closed-form analytic expression for $x_{i}(\\lambda)$ as a function of $\\lambda$, $c_{i}$, $a_{i}$, and $u_{i}$. The final answer must be a single analytic expression and must not include any units. No numerical rounding is required.", "solution": "The user wants to solve the problem statement.\n\n**Problem Validation**\n\n**Step 1: Extract Givens**\n\n- **Problem Type**: Linear optimization with box constraints.\n- **Objective Function**: Minimize $\\;c^{\\top}x\\;$.\n- **Constraints**:\n  1. Equality constraint: $\\;Ax=b\\;$.\n  2. Box constraints: $\\;0 \\le x \\le u\\;$.\n- **Variable and Constant Definitions**:\n  - $A \\in \\mathbb{R}^{m \\times n}$ is a matrix.\n  - $b \\in \\mathbb{R}^{m}$ is a vector.\n  - $c \\in \\mathbb{R}^{n}$ is a vector.\n  - $u \\in \\mathbb{R}^{n}$ is a vector of upper bounds with $u_{i} > 0$ for all indices $i \\in \\{1,\\dots,n\\}$.\n  - $x \\in \\mathbb{R}^{n}$ is the optimization variable.\n  - Inequalities are componentwise.\n- **Methodology**:\n  - Lagrangian relaxation of the equality constraint $\\;Ax=b\\;$.\n  - Lagrange multipliers: $\\;\\lambda \\in \\mathbb{R}^{m}\\;$.\n  - Lagrangian function: $\\;L(x,\\lambda)\\;$.\n  - Dual function: $\\;g(\\lambda) = \\inf_{0 \\le x \\le u} L(x,\\lambda)\\;$.\n- **Task**:\n  - Derive the componentwise minimizer $x_{i}(\\lambda)$ that attains the infimum in the dual function.\n  - Specifically, for each index $i$, determine the form of $x_{i}(\\lambda) \\in \\arg\\min_{x_{i} \\in [0,u_{i}]} \\left( c_{i} + a_{i}^{\\top}\\lambda \\right) x_{i}$.\n  - $a_{i} \\in \\mathbb{R}^{m}$ denotes the $i$-th column of the matrix $A$.\n  - Explain the thresholding behavior with respect to the sign of the scalar $\\;c_{i} + a_{i}^{\\top}\\lambda\\;$.\n- **Tie-Breaking Rule**:\n  - If $c_{i} + a_{i}^{\\top}\\lambda = 0$, select the smallest minimizer, $x_{i}(\\lambda) = 0$.\n- **Required Output**: A single closed-form analytic expression for $x_{i}(\\lambda)$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded**: The problem is a standard application of Lagrangian duality theory to a linear program, a core topic in optimization methods. All concepts and formulations are well-established.\n- **Well-Posed**: The problem is clearly stated. It provides all necessary definitions ($\\;A, b, c, u\\;$) and constraints. The objective is unambiguous. The inclusion of a specific tie-breaking rule ensures that the minimizer $x(\\lambda)$ is uniquely defined, making the problem well-posed.\n- **Objective**: The language is formal, mathematical, and free of bias or subjectivity.\n\nThe problem statement is self-contained, consistent, and adheres to the principles of mathematical optimization. No flaws are identified.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be provided.\n\n**Derivation of the Solution**\n\nThe primal optimization problem is given by:\n$$\n\\begin{aligned}\n\\text{minimize} \\quad & c^{\\top}x \\\\\n\\text{subject to} \\quad & Ax = b \\\\\n& 0 \\le x \\le u\n\\end{aligned}\n$$\nwhere $0$, $x$, and $u$ are vectors in $\\mathbb{R}^{n}$ and the inequalities are componentwise.\n\nWe perform Lagrangian relaxation on the equality constraint $Ax=b$ by introducing a vector of Lagrange multipliers $\\lambda \\in \\mathbb{R}^{m}$. The Lagrangian function $L(x, \\lambda)$ is defined as the sum of the original objective function and the dot product of the multipliers with the constraint function:\n$$\nL(x, \\lambda) = c^{\\top}x + \\lambda^{\\top}(Ax - b)\n$$\nThe dual function $g(\\lambda)$ is defined as the infimum of the Lagrangian over the remaining feasible set for $x$, which consists of the box constraints $0 \\le x \\le u$:\n$$\ng(\\lambda) = \\inf_{0 \\le x \\le u} L(x, \\lambda) = \\inf_{0 \\le x \\le u} \\left( c^{\\top}x + \\lambda^{\\top}(Ax - b) \\right)\n$$\nTo find this infimum, we can rearrange the terms in the Lagrangian that depend on $x$:\n$$\nc^{\\top}x + \\lambda^{\\top}(Ax - b) = c^{\\top}x + \\lambda^{\\top}Ax - \\lambda^{\\top}b = (c^{\\top} + \\lambda^{\\top}A)x - \\lambda^{\\top}b\n$$\nThe term $\\lambda^{\\top}b$ is constant with respect to $x$, so it does not affect the minimizer $x(\\lambda)$. The minimization problem becomes:\n$$\nx(\\lambda) = \\arg\\min_{0 \\le x \\le u} \\left( (c^{\\top} + \\lambda^{\\top}A)x \\right)\n$$\nLet's analyze the objective term $(c^{\\top} + \\lambda^{\\top}A)x$. This can be written as a sum over the components of $x$:\n$$\n(c^{\\top} + \\lambda^{\\top}A)x = \\sum_{i=1}^{n} (c_i + (\\lambda^{\\top}A)_i) x_i\n$$\nThe $i$-th component of the row vector $\\lambda^{\\top}A$ is the dot product of $\\lambda^{\\top}$ with the $i$-th column of $A$. The problem defines the $i$-th column of $A$ as $a_i$. Therefore, $(\\lambda^{\\top}A)_i = \\lambda^{\\top}a_i = a_i^{\\top}\\lambda$.\nSubstituting this back, the objective becomes:\n$$\n\\sum_{i=1}^{n} (c_i + a_i^{\\top}\\lambda) x_i\n$$\nThe box constraint $0 \\le x \\le u$ is equivalent to $n$ independent constraints $0 \\le x_i \\le u_i$ for each $i \\in \\{1, \\dots, n\\}$. Because the objective function is a sum of terms each involving only one component $x_i$, and the constraints are also separable by component, the $n$-dimensional minimization problem decomposes into $n$ independent one-dimensional minimization problems:\n$$\n\\min_{0 \\le x \\le u} \\sum_{i=1}^{n} (c_i + a_i^{\\top}\\lambda) x_i = \\sum_{i=1}^{n} \\min_{0 \\le x_i \\le u_i} \\left[ (c_i + a_i^{\\top}\\lambda) x_i \\right]\n$$\nThe minimizer $x(\\lambda) = (x_1(\\lambda), \\dots, x_n(\\lambda))^{\\top}$ is found by solving each one-dimensional problem for $x_i(\\lambda)$ independently. For each index $i \\in \\{1, \\dots, n\\}$, we must find:\n$$\nx_i(\\lambda) \\in \\arg\\min_{x_i \\in [0, u_i]} \\left( c_i + a_i^{\\top}\\lambda \\right) x_i\n$$\nLet the scalar coefficient of $x_i$ be denoted by $s_i = c_i + a_i^{\\top}\\lambda$. The problem for each component is to minimize a linear function $s_i x_i$ over the closed interval $[0, u_i]$. The solution depends entirely on the sign of the coefficient $s_i$. We analyze the three possible cases.\n\n**Case 1: $s_i > 0$**\nIf $c_i + a_i^{\\top}\\lambda > 0$, the objective $s_i x_i$ is a strictly increasing linear function of $x_i$. To minimize this function over the interval $[0, u_i]$, we must choose the smallest possible value for $x_i$. Therefore, the unique minimizer is $x_i(\\lambda) = 0$.\n\n**Case 2: $s_i < 0$**\nIf $c_i + a_i^{\\top}\\lambda < 0$, the objective $s_i x_i$ is a strictly decreasing linear function of $x_i$. To minimize this function over the interval $[0, u_i]$, we must choose the largest possible value for $x_i$. Therefore, the unique minimizer is $x_i(\\lambda) = u_i$.\n\n**Case 3: $s_i = 0$**\nIf $c_i + a_i^{\\top}\\lambda = 0$, the objective becomes $0 \\cdot x_i = 0$ for all values of $x_i$. In this case, the objective function is flat, and any value of $x_i$ in the interval $[0, u_i]$ is a minimizer. The problem provides a specific tie-breaking rule to ensure a unique solution: \"select the smallest minimizer in the set, namely $x_{i}(\\lambda) = 0$\".\n\nThis demonstrates the thresholding behavior. The value of $x_i(\\lambda)$ switches from one end of its feasible interval to the other based on the sign of the reduced cost $s_i = c_i + a_i^{\\top}\\lambda$. The threshold is $s_i=0$.\n\nCombining these three cases yields a complete definition for $x_i(\\lambda)$:\n- If $c_i + a_i^{\\top}\\lambda > 0$, then $x_i(\\lambda) = 0$.\n- If $c_i + a_i^{\\top}\\lambda = 0$, then $x_i(\\lambda) = 0$.\n- If $c_i + a_i^{\\top}\\lambda < 0$, then $x_i(\\lambda) = u_i$.\n\nThe first two conditions can be merged into one. Thus, we arrive at the final closed-form expression for $x_i(\\lambda)$ as a piecewise function:\n$$\nx_i(\\lambda) =\n\\begin{cases}\n0 & \\text{if } c_i + a_i^{\\top}\\lambda \\ge 0 \\\\\nu_i & \\text{if } c_i + a_i^{\\top}\\lambda < 0\n\\end{cases}\n$$\nThis is the required single analytic expression for the componentwise minimizer.", "answer": "$$\n\\boxed{\nx_{i}(\\lambda) = \\begin{cases} 0 & \\text{if } c_{i} + a_{i}^{\\top}\\lambda \\ge 0 \\\\ u_{i} & \\text{if } c_{i} + a_{i}^{\\top}\\lambda < 0 \\end{cases}\n}\n$$", "id": "3141448"}, {"introduction": "Moving from theory to practice, this exercise challenges you to implement Lagrangian relaxation to find lower bounds for the classic NP-hard Set Cover problem. You will write a program to compare the bound obtained from Lagrangian relaxation with the bound from the more standard Linear Programming (LP) relaxation. This hands-on comparison [@problem_id:3141444] is designed to reveal a deep insight: the quality of the Lagrangian bound is not fixed, but depends critically on which structural constraints are preserved in the subproblem, a key strategic choice when applying this powerful technique.", "problem": "Consider the Minimum Cost Set Cover problem defined as follows. Let there be a universe of $m$ elements indexed by $i \\in \\{1,\\dots,m\\}$ and a collection of $n$ sets indexed by $j \\in \\{1,\\dots,n\\}$. Let the binary decision variable $x_j \\in \\{0,1\\}$ indicate whether set $j$ is chosen. Let the coverage matrix $A \\in \\{0,1\\}^{m \\times n}$ have entries $A_{ij} = 1$ if and only if element $i$ is in set $j$, and $A_{ij} = 0$ otherwise. Let the cost vector be $c \\in \\mathbb{R}^n_{\\ge 0}$. The integer program is\n$$\n\\min_{x \\in \\{0,1\\}^n} \\; c^\\top x \\quad \\text{subject to} \\quad A x \\ge \\mathbf{1},\n$$\nwhere $\\mathbf{1} \\in \\mathbb{R}^m$ is the all-ones vector.\n\nA common lower bound is obtained by the Linear Programming relaxation, which replaces $x_j \\in \\{0,1\\}$ with $x_j \\in [0,1]$:\n$$\n\\min_{x \\in [0,1]^n} \\; c^\\top x \\quad \\text{subject to} \\quad A x \\ge \\mathbf{1}.\n$$\nAnother lower bound arises from Lagrangian relaxation, which dualizes one or more complicating constraints using nonnegative Lagrange multipliers while keeping some integrality or structural constraints intact. In general, for multipliers $u \\in \\mathbb{R}^m_{\\ge 0}$ associated with coverage constraints $A x \\ge \\mathbf{1}$, the Lagrangian function for a given structure-preserving subproblem is\n$$\n\\mathcal{L}(u) \\;=\\; \\min_{x \\in \\mathcal{X}} \\left\\{ c^\\top x + u^\\top (\\mathbf{1} - A x) \\right\\}\n\\;=\\; u^\\top \\mathbf{1} + \\min_{x \\in \\mathcal{X}} \\left\\{ \\sum_{j=1}^n \\big(c_j - (A^\\top u)_j\\big) x_j \\right\\},\n$$\nwhere $\\mathcal{X}$ encodes the constraints we keep (for example, integrality, cardinality, or knapsack constraints). The Lagrangian dual maximizes $\\mathcal{L}(u)$ over $u \\ge 0$:\n$$\n\\max_{u \\in \\mathbb{R}^m_{\\ge 0}} \\; \\mathcal{L}(u).\n$$\nWhen $\\mathcal{X} = \\{ x \\in \\{0,1\\}^n \\}$ and all coverage constraints are relaxed, the resulting Lagrangian dual is equivalent to the dual of the Linear Programming relaxation, yielding the same lower bound. When $\\mathcal{X}$ retains additional structure such as a cardinality constraint $\\sum_{j=1}^n x_j \\le B$ or a knapsack constraint $\\sum_{j=1}^n w_j x_j \\le W$ with weights $w \\in \\mathbb{R}^n_{> 0}$ and capacity $W \\in \\mathbb{R}_{> 0}$, the Lagrangian relaxation can provide strictly tighter bounds by exploiting integrality or combinatorial structure in the subproblem.\n\nYour task is to write a program that, for each specified test instance, computes:\n- the Linear Programming relaxation lower bound, and\n- the Lagrangian relaxation lower bound by dualizing coverage constraints with multipliers $u \\ge 0$ and solving the remaining structure-preserving subproblem via a principled method. Use subgradient ascent on $u$ with a diminishing step size to approximately maximize the Lagrangian dual. For the subgradient $g(u)$, use the standard fact that a subgradient of $u^\\top \\mathbf{1} - u^\\top A x^\\star(u)$ at $u$ is $g(u) = \\mathbf{1} - A x^\\star(u)$, where $x^\\star(u)$ is an optimal solution of the current subproblem (with reduced costs $r_j(u) = c_j - (A^\\top u)_j$).\n\nThe subproblem to evaluate $\\mathcal{L}(u)$ differs by instance:\n- For the base Set Cover relaxation (no additional constraints beyond integrality), $\\mathcal{X} = \\{ x \\in \\{0,1\\}^n \\}$, the subproblem minimizes $\\sum_j r_j x_j$, and the optimal $x^\\star(u)$ simply chooses all sets with $r_j(u) < 0$; the corresponding Lagrangian dual bound equals the Linear Programming relaxation bound and can be computed exactly by solving the Linear Programming dual: maximize $u^\\top \\mathbf{1}$ subject to $A^\\top u \\le c$, $u \\ge 0$.\n- For the Cardinality-constrained Set Cover, $\\mathcal{X} = \\{ x \\in \\{0,1\\}^n : \\sum_j x_j \\le B \\}$ with given integer $B$, the subproblem chooses up to $B$ sets with the most negative reduced costs $r_j(u)$. The subgradient ascent updates $u$ using $g(u) = \\mathbf{1} - A x^\\star(u)$.\n- For the Knapsack-constrained Set Cover, $\\mathcal{X} = \\{ x \\in \\{0,1\\}^n : \\sum_j w_j x_j \\le W \\}$ with given weights $w_j$ and capacity $W$, the subproblem selects sets with negative reduced costs to minimize $\\sum_j r_j x_j$ subject to the knapsack constraint; equivalently, it is a $0$-$1$ knapsack problem with item profits $p_j(u) = \\max\\{ -r_j(u), 0 \\}$ and weights $w_j$ that maximizes total profit. Use dynamic programming to solve the knapsack subproblem exactly for the given small instances.\n\nFundamental base to use:\n- The definition of Linear Programming relaxation for integer programs, and its duality relating the primal relaxation to a dual problem.\n- The definition of Lagrangian relaxation and subgradient ascent for maximizing a concave dual function, where a subgradient is derived from the relaxed constraints’ residual, i.e., $g(u) = \\mathbf{1} - A x^\\star(u)$.\n- The standard $0$-$1$ knapsack dynamic programming recurrence for small instances.\n\nTest suite:\nProvide computations for the following three instances (all costs are unitless nonnegative reals, and there are no physical units involved):\n- Instance $1$ (Base Set Cover):\n    - $m = 3$, $n = 4$,\n    - Coverage matrix rows indexed by elements $1,2,3$ and columns indexed by sets $1,2,3,4$:\n      $A = \\begin{bmatrix}\n      1 & 0 & 1 & 0 \\\\\n      1 & 1 & 0 & 0 \\\\\n      0 & 1 & 0 & 1\n      \\end{bmatrix}$,\n    - Costs $c = [4, 4, 2, 2]$.\n- Instance $2$ (Cardinality-constrained Set Cover):\n    - Same $A$ and $c$ as Instance $1$,\n    - Cardinality bound $B = 2$.\n- Instance $3$ (Knapsack-constrained Set Cover):\n    - Same $A$ and $c$ as Instance $1$,\n    - Weights $w = [2, 2, 1, 1]$, capacity $W = 3$.\n\nOutput specification:\n- For each instance, compute the Linear Programming relaxation optimal cost and the Lagrangian relaxation lower bound using the prescribed method.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each entry is a two-element list $[v_{\\text{LP}}, v_{\\text{LR}}]$ of floats for the corresponding instance in order. For example: $[[v_{\\text{LP},1}, v_{\\text{LR},1}], [v_{\\text{LP},2}, v_{\\text{LR},2}], [v_{\\text{LP},3}, v_{\\text{LR},3}]]$.\n\nDesign for coverage:\n- Instance $1$ tests the equivalence between Linear Programming relaxation and Lagrangian relaxation when relaxing all coverage constraints without additional structure.\n- Instance $2$ tests a cardinality constraint, illustrating a structure where integrality interacts with Lagrangian relaxation; the Lagrangian lower bound is computed via subgradient ascent, and results can differ from the Linear Programming relaxation bound.\n- Instance $3$ tests a knapsack constraint, requiring dynamic programming in the subproblem to properly exploit structure; the Lagrangian relaxation can be tighter due to integrality in the subproblem.\n\nAnswer types:\n- For each instance, the pair $[v_{\\text{LP}}, v_{\\text{LR}}]$ must be floats.", "solution": "The problem requires computing and comparing two types of lower bounds for the Minimum Cost Set Cover problem and its variants: the Linear Programming (LP) relaxation bound, $v_{\\text{LP}}$, and the Lagrangian relaxation bound, $v_{\\text{LR}}$. This will be done for three specific instances designed to illustrate different aspects of these relaxation techniques.\n\nThe core problem is the Set Cover Integer Program (IP):\n$$\n\\min_{x \\in \\{0,1\\}^n} \\; c^\\top x \\quad \\text{subject to} \\quad A x \\ge \\mathbf{1}\n$$\nwhere $x_j$ is a binary variable indicating if set $j$ is chosen, $c_j$ is its cost, and the matrix $A$ ensures that each element in the universe is covered.\n\n**1. Linear Programming (LP) Relaxation**\n\nThe LP relaxation is formed by relaxing the integrality constraint $x_j \\in \\{0,1\\}$ to a continuous domain $x_j \\in [0,1]$. For the base Set Cover problem, the LP relaxation is:\n$$\nv_{\\text{LP}} = \\min_{x \\in [0,1]^n} \\; c^\\top x \\quad \\text{subject to} \\quad A x \\ge \\mathbf{1}\n$$\nAny additional constraints in the original IP, such as cardinality or knapsack constraints, are retained in their linear form in the corresponding LP relaxation. The optimal value $v_{\\text{LP}}$ provides a lower bound on the optimal IP value, $v_{\\text{IP}}$, because the feasible region of the LP contains all feasible integer solutions. These LPs are solved using the `scipy.optimize.linprog` function.\n\n**2. Lagrangian Relaxation**\n\nLagrangian relaxation provides an alternative way to find lower bounds. It works by moving \"complicating\" constraints into the objective function with associated penalties, the Lagrange multipliers $u$. In this problem, we relax the coverage constraints $A x \\ge \\mathbf{1}$ (which can be written as $\\mathbf{1} - Ax \\le \\mathbf{0}$). For a vector of non-negative multipliers $u \\in \\mathbb{R}^m_{\\ge 0}$, the Lagrangian function is:\n$$\n\\mathcal{L}(u) = \\min_{x \\in \\mathcal{X}} \\left\\{ c^\\top x + u^\\top (\\mathbf{1} - A x) \\right\\}\n$$\nwhere $\\mathcal{X}$ is the set of feasible solutions defined by the retained constraints (e.g., integrality $x \\in \\{0,1\\}^n$, and possibly cardinality or knapsack constraints). This can be rewritten as:\n$$\n\\mathcal{L}(u) = u^\\top \\mathbf{1} + \\min_{x \\in \\mathcal{X}} \\left\\{ \\sum_{j=1}^n \\left(c_j - (A^\\top u)_j\\right) x_j \\right\\}\n$$\nThe term $r_j(u) = c_j - (A^\\top u)_j$ is the reduced cost of set $j$. The minimization subproblem over $x \\in \\mathcal{X}$ must be solved to evaluate $\\mathcal{L}(u)$. For any $u \\ge 0$, $\\mathcal{L}(u)$ is a lower bound on $v_{\\text{IP}}$. The tightest such bound is found by solving the Lagrangian dual problem:\n$$\nv_{\\text{LR}} = \\max_{u \\ge 0} \\mathcal{L}(u)\n$$\nSince $\\mathcal{L}(u)$ is a concave function, we can use the subgradient ascent method to find the maximum. A subgradient of $\\mathcal{L}(u)$ is given by $g(u) = \\mathbf{1} - A x^\\star(u)$, where $x^\\star(u)$ is an optimal solution to the subproblem for the given $u$. The iterative update for the multipliers is:\n$$\nu_{k+1} = \\max(0, u_k + \\alpha_k g(u_k))\n$$\nwhere $\\alpha_k$ is a step size. A diminishing step size rule, such as $\\alpha_k = \\frac{1}{k+1}$, is chosen for the iterations. The value $v_{\\text{LR}}$ is the maximum value of $\\mathcal{L}(u)$ encountered during the ascent. Theory establishes the relationship $v_{\\text{LP}} \\le v_{\\text{LR}} \\le v_{\\text{IP}}$.\n\nWe now analyze each instance. The common data are the cost vector $c = [4, 4, 2, 2]$ and the coverage matrix $A = \\begin{bmatrix} 1 & 0 & 1 & 0 \\\\ 1 & 1 & 0 & 0 \\\\ 0 & 1 & 0 & 1 \\end{bmatrix}$ for a universe of $m=3$ elements and a collection of $n=4$ sets.\n\n**Instance 1: Base Set Cover**\nThe retained constraints are simply the integrality constraints, $\\mathcal{X} = \\{ x \\in \\{0,1\\}^n \\}$.\n-   **LP Relaxation**: The LP is $\\min_{x \\in [0,1]^4} c^\\top x$ subject to $Ax \\ge \\mathbf{1}$.\n-   **Lagrangian Relaxation**: The subproblem is $\\min_{x \\in \\{0,1\\}^4} \\sum_j r_j(u) x_j$. The optimal solution is to set $x_j^\\star(u) = 1$ if $r_j(u) < 0$ and $x_j^\\star(u) = 0$ otherwise. As stated in the problem, the bound obtained from this Lagrangian dual is equal to the LP relaxation bound. We compute both using their respective standard methods to verify this.\n\n**Instance 2: Cardinality-constrained Set Cover**\nAn additional constraint $\\sum_{j=1}^4 x_j \\le B$ with $B=2$ is added.\n-   **LP Relaxation**: The LP is $\\min_{x \\in [0,1]^4} c^\\top x$ subject to $Ax \\ge \\mathbf{1}$ and $\\sum_j x_j \\le 2$.\n-   **Lagrangian Relaxation**: The subproblem's feasible set is $\\mathcal{X} = \\{ x \\in \\{0,1\\}^4 : \\sum_j x_j \\le 2 \\}$. To solve $\\min_{x \\in \\mathcal{X}} \\sum_j r_j(u) x_j$, we compute the reduced costs $r_j(u)$, sort them, and set $x_j^\\star=1$ for the up to $B=2$ sets with the most negative reduced costs. The integrality and cardinality constraints are handled together in the subproblem, which can lead to a tighter bound ($v_{\\text{LR}} > v_{\\text{LP}}$).\n\n**Instance 3: Knapsack-constrained Set Cover**\nAn additional constraint $\\sum_{j=1}^4 w_j x_j \\le W$ with weights $w = [2, 2, 1, 1]$ and capacity $W=3$ is added.\n-   **LP Relaxation**: The LP is $\\min_{x \\in [0,1]^4} c^\\top x$ subject to $Ax \\ge \\mathbf{1}$ and $\\sum_j w_j x_j \\le 3$.\n-   **Lagrangian Relaxation**: The subproblem's feasible set is $\\mathcal{X} = \\{ x \\in \\{0,1\\}^4 : \\sum_j w_j x_j \\le 3 \\}$. The subproblem $\\min_{x \\in \\mathcal{X}} \\sum_j r_j(u) x_j$ is equivalent to the 0-1 knapsack problem, where we seek to maximize total \"profit\" $\\sum_j p_j x_j$ with $p_j(u) = -r_j(u)$. This is solved exactly using a standard dynamic programming algorithm. The integer nature of the knapsack subproblem allows this relaxation to potentially yield a strictly better bound than the LP relaxation.\n\nThe implementation will now proceed to calculate $[v_{\\text{LP}}, v_{\\text{LR}}]$ for each of these three instances.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import linprog\n\ndef solve():\n    \"\"\"\n    Computes LP and Lagrangian relaxation bounds for three Set Cover instances.\n    \"\"\"\n    \n    # Common parameters for all instances\n    A = np.array([[1, 0, 1, 0], [1, 1, 0, 0], [0, 1, 0, 1]], dtype=float)\n    c = np.array([4, 4, 2, 2], dtype=float)\n    m, n = A.shape\n    bounds = [(0, 1)] * n\n    N_ITER = 3000\n\n    def solve_subgradient_ascent(subproblem_solver, A_mat, c_vec, m_dim, n_dim):\n        \"\"\"Generic subgradient ascent solver.\"\"\"\n        u = np.zeros(m_dim)\n        best_L = -np.inf\n        for k in range(N_ITER):\n            r = c_vec - A_mat.T @ u\n            x_star = subproblem_solver(r)\n            \n            L_k = u.T @ np.ones(m_dim) + r.T @ x_star\n            if L_k > best_L:\n                best_L = L_k\n            \n            g = np.ones(m_dim) - A_mat @ x_star\n            if np.linalg.norm(g) < 1e-9:\n                break\n            \n            alpha = 1.0 / (k + 1.0)\n            u = np.maximum(0, u + alpha * g)\n        return best_L\n\n    results = []\n\n    # --- Instance 1: Base Set Cover ---\n    # LP Relaxation\n    A_ub_1 = -A\n    b_ub_1 = -np.ones(m)\n    res_lp1 = linprog(c, A_ub=A_ub_1, b_ub=b_ub_1, bounds=bounds, method='highs')\n    v_lp1 = res_lp1.fun if res_lp1.success else np.nan\n\n    # Lagrangian Relaxation\n    def subproblem1(r):\n        return (r < 0).astype(float)\n    v_lr1 = solve_subgradient_ascent(subproblem1, A, c, m, n)\n    results.append([v_lp1, v_lr1])\n\n    # --- Instance 2: Cardinality-constrained Set Cover ---\n    B = 2\n    # LP Relaxation\n    A_ub_2 = np.vstack([A_ub_1, np.ones(n)])\n    b_ub_2 = np.hstack([b_ub_1, B])\n    res_lp2 = linprog(c, A_ub=A_ub_2, b_ub=b_ub_2, bounds=bounds, method='highs')\n    v_lp2 = res_lp2.fun if res_lp2.success else np.nan\n\n    # Lagrangian Relaxation\n    def subproblem2(r):\n        x = np.zeros(n)\n        sorted_indices = np.argsort(r)\n        count = 0\n        for idx in sorted_indices:\n            if r[idx] < 0 and count < B:\n                x[idx] = 1\n                count += 1\n            else:\n                break\n        return x\n    v_lr2 = solve_subgradient_ascent(subproblem2, A, c, m, n)\n    results.append([v_lp2, v_lr2])\n\n    # --- Instance 3: Knapsack-constrained Set Cover ---\n    w = np.array([2, 2, 1, 1], dtype=int)\n    W = 3\n    # LP Relaxation\n    A_ub_3 = np.vstack([A_ub_1, w])\n    b_ub_3 = np.hstack([b_ub_1, W])\n    res_lp3 = linprog(c, A_ub=A_ub_3, b_ub=b_ub_3, bounds=bounds, method='highs')\n    v_lp3 = res_lp3.fun if res_lp3.success else np.nan\n\n    # Lagrangian Relaxation (Knapsack subproblem)\n    def solve_knapsack_dp(profits, weights, capacity):\n        num_items = len(profits)\n        dp = np.zeros((num_items + 1, capacity + 1))\n\n        for i in range(1, num_items + 1):\n            profit = profits[i-1]\n            weight = weights[i-1]\n            for w_cap in range(capacity + 1):\n                dp[i, w_cap] = dp[i-1, w_cap]\n                if w_cap >= weight:\n                    dp[i, w_cap] = max(dp[i, w_cap], dp[i-1, w_cap - weight] + profit)\n\n        x_res = np.zeros(num_items)\n        w_rem = capacity\n        for i in range(num_items, 0, -1):\n            if not np.isclose(dp[i, w_rem], dp[i-1, w_rem]):\n                x_res[i-1] = 1\n                w_rem -= weights[i-1]\n        return x_res\n        \n    def subproblem3(r):\n        profits = -r\n        return solve_knapsack_dp(profits, w, W)\n        \n    v_lr3 = solve_subgradient_ascent(subproblem3, A, c, m, n)\n    results.append([v_lp3, v_lr3])\n    \n    # Format the final output\n    print(str(results))\n\nsolve()\n```", "id": "3141444"}, {"introduction": "The path to the optimal dual solution is not always smooth. The dual function $g(\\lambda)$ is often non-differentiable, which can cause instability in the subgradient ascent method. This exercise [@problem_id:3141471] lets you explore this phenomenon by investigating a scenario where the Lagrangian subproblem has multiple optimal solutions, leading to an ambiguous choice of subgradient. Through a coding experiment, you will observe how different tie-breaking rules affect the algorithm's trajectory and how adding a small quadratic \"smoothing\" term can stabilize the process by ensuring a unique solution to the subproblem.", "problem": "You are asked to study how the dual function changes when the minimizer of a Lagrangian is nonunique and how adding a small strictly convex perturbation can restore uniqueness. Consider the following Lagrangian relaxation of a simple linear program. Let $x \\in \\mathbb{R}^2$ with box constraints $x \\in [0,1]^2$, a single linear constraint defined by $A = [1,1]$ and a scalar right-hand side $b \\in \\mathbb{R}$. Let $c \\in \\mathbb{R}^2$ be the cost vector. The Lagrangian function is\n$$\nL(x,\\lambda) = c^\\top x + \\lambda \\left( b - (1,1)^\\top x \\right),\n$$\nwith dual variable $\\lambda \\ge 0$. The dual function is\n$$\ng(\\lambda) = \\min_{x \\in [0,1]^2} L(x,\\lambda).\n$$\nThis function is the pointwise infimum of affine functions in $\\lambda$, hence it is concave. However, when $x^\\star(\\lambda) \\in \\operatorname{argmin}_{x \\in [0,1]^2} L(x,\\lambda)$ is not unique, the choice of $x^\\star(\\lambda)$ affects which subgradient you obtain at $\\lambda$ and leads to different subgradient ascent trajectories. To remove ambiguity in $\\operatorname{argmin}$, consider a smoothed Lagrangian with a strictly convex perturbation:\n$$\nL_\\varepsilon(x,\\lambda) = c^\\top x + \\lambda \\left( b - (1,1)^\\top x \\right) + \\varepsilon \\|x\\|^2,\n$$\nwhere $\\varepsilon > 0$ and $\\|x\\|^2 = x_1^2 + x_2^2$. This perturbation ensures a unique minimizer $x_\\varepsilon^\\star(\\lambda)$ for each $\\lambda$.\n\nYour task is to implement a projected subgradient ascent on the dual variable $\\lambda$ for both the unsmoothed and smoothed cases and to compare trajectories under different tie-breaking rules when the unsmoothed minimizer is nonunique.\n\nThe algorithmic setup is as follows:\n- Start from $\\lambda_0 = 0$.\n- At iteration $t$, compute a subgradient $s_t \\in \\partial g(\\lambda_t)$ as $s_t = b - (1,1)^\\top x_t$, where $x_t \\in \\operatorname{argmin}_{x \\in [0,1]^2} L(x,\\lambda_t)$ in the unsmoothed case, or $x_t = x_\\varepsilon^\\star(\\lambda_t)$ in the smoothed case.\n- Update $\\lambda_{t+1} = \\max\\{0, \\lambda_t + \\alpha s_t\\}$, where $\\alpha > 0$ is a fixed step size, and the maximum enforces the projection onto the feasible set $\\lambda \\ge 0$.\n\nIn the unsmoothed case, since the objective $(c - \\lambda (1,1))^\\top x$ is linear in $x$ over the box $[0,1]^2$, the minimizer is obtained componentwise by selecting $x_i \\in \\{0,1\\}$ according to the sign of the coefficient. Specifically, if $c_i - \\lambda > 0$ then $x_i = 0$, if $c_i - \\lambda < 0$ then $x_i = 1$, and when $c_i - \\lambda = 0$, any $x_i \\in [0,1]$ minimizes the objective, creating a tie. You must implement two deterministic tie-breaking rules for the unsmoothed case:\n- Rule $\\mathsf{low}$: whenever $c_i - \\lambda = 0$, set $x_i = 0$.\n- Rule $\\mathsf{high}$: whenever $c_i - \\lambda = 0$, set $x_i = 1$.\n\nIn the smoothed case, use the perturbation parameter $\\varepsilon > 0$ to ensure a unique minimizer $x_\\varepsilon^\\star(\\lambda)$ within the box $[0,1]^2$.\n\nImplement the projected subgradient ascent for a fixed number of iterations $N$, a constant step size $\\alpha$, and initial dual variable $\\lambda_0 = 0$. For each test case, run three trajectories:\n- Unsmooth with tie-breaking rule $\\mathsf{low}$.\n- Unsmooth with tie-breaking rule $\\mathsf{high}$.\n- Smoothed with perturbation parameter $\\varepsilon$.\n\nTest suite:\n- Case $1$: $c = [0.5, 0.5]$, $b = 0.5$, $\\alpha = 0.25$, $N = 12$, $\\varepsilon = 10^{-3}$.\n- Case $2$: $c = [0.0, 0.0]$, $b = 1.0$, $\\alpha = 0.25$, $N = 12$, $\\varepsilon = 10^{-3}$.\n- Case $3$: $c = [0.4, 0.6]$, $b = 0.0$, $\\alpha = 0.25$, $N = 12$, $\\varepsilon = 10^{-3}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case’s result should be a list of three floats in the order $[\\lambda_{\\text{final}}^{\\text{unsm-}\\mathsf{low}}, \\lambda_{\\text{final}}^{\\text{unsm-}\\mathsf{high}}, \\lambda_{\\text{final}}^{\\text{smooth}}]$, so the final output format is\n$$\n[[a_1,b_1,c_1],[a_2,b_2,c_2],[a_3,b_3,c_3]],\n$$\nwhere each $a_i$, $b_i$, and $c_i$ are the final $\\lambda$ values after $N$ iterations for the three trajectories of case $i$.", "solution": "The user asks for an implementation of a projected subgradient ascent algorithm to solve the dual of a simple linear program. The task involves comparing the trajectories of the dual variable $\\lambda$ under three different schemes for choosing the minimizer of the Lagrangian: two deterministic tie-breaking rules for the non-smooth case and one regularized scheme for a smoothed version of the problem.\n\nThe problem formulation is as follows. The primal optimization problem involves a variable $x \\in \\mathbb{R}^2$ with box constraints $x \\in [0,1]^2$, and a linear constraint $(1,1)^\\top x \\ge b$. The objective is to minimize $c^\\top x$. The Lagrangian function associated with relaxing the constraint $(1,1)^\\top x \\ge b$ (written as $b - (1,1)^\\top x \\le 0$) is:\n$$\nL(x,\\lambda) = c^\\top x + \\lambda(b - (1,1)^\\top x) \\quad \\text{with } \\lambda \\ge 0\n$$\nThe dual function is the infimum of the Lagrangian over the primal variables:\n$$\ng(\\lambda) = \\min_{x \\in [0,1]^2} L(x,\\lambda)\n$$\nThe dual problem is to maximize $g(\\lambda)$ subject to $\\lambda \\ge 0$. This is solved using the projected subgradient ascent method. The update rule for $\\lambda$ at each iteration $t$ is:\n$$\n\\lambda_{t+1} = \\max\\{0, \\lambda_t + \\alpha s_t\\}\n$$\nwhere $\\alpha > 0$ is a fixed step size and $s_t$ is a subgradient of $g$ at $\\lambda_t$. A valid subgradient is given by $s_t = b - (1,1)^\\top x_t$, where $x_t$ is any minimizer of $L(x, \\lambda_t)$ over $x \\in [0,1]^2$.\n\nThe core of the problem lies in finding the minimizer $x_t$. The Lagrangian can be rewritten as:\n$$\nL(x,\\lambda) = (c_1 - \\lambda)x_1 + (c_2 - \\lambda)x_2 + \\lambda b\n$$\nSince this expression is separable in $x_1$ and $x_2$, we can find the optimal $x_i$ for each component $i \\in \\{1, 2\\}$ by solving $\\min_{x_i \\in [0,1]} (c_i - \\lambda)x_i$. The solution depends on the sign of the coefficient $(c_i - \\lambda)$.\n\n**1. Unsmoothed Lagrangian Minimizer**\nFor the standard Lagrangian $L(x,\\lambda)$, the optimal $x_i$ is determined as:\n- If $c_i - \\lambda > 0$, the coefficient of $x_i$ is positive, so minimizing requires setting $x_i = 0$.\n- If $c_i - \\lambda < 0$, the coefficient is negative, so minimizing requires setting $x_i = 1$.\n- If $c_i - \\lambda = 0$, any $x_i \\in [0,1]$ is a minimizer. This ambiguity is addressed by two rules:\n    - **Rule $\\mathsf{low}$**: If a tie occurs ($c_i - \\lambda = 0$), set $x_i = 0$. This is equivalent to setting $x_i = 1$ if $\\lambda > c_i$, and $x_i = 0$ otherwise.\n    - **Rule $\\mathsf{high}$**: If a tie occurs ($c_i - \\lambda = 0$), set $x_i = 1$. This is equivalent to setting $x_i = 0$ if $\\lambda < c_i$, and $x_i = 1$ otherwise.\n\n**2. Smoothed Lagrangian Minimizer**\nThe smoothed Lagrangian $L_\\varepsilon(x,\\lambda)$ adds a strictly convex quadratic term $\\varepsilon \\|x\\|^2 = \\varepsilon(x_1^2 + x_2^2)$ to ensure a unique minimizer:\n$$\nL_\\varepsilon(x,\\lambda) = \\sum_{i=1}^2 \\left( (c_i - \\lambda)x_i + \\varepsilon x_i^2 \\right) + \\lambda b\n$$\nThis is also separable. For each component $i$, we must solve $\\min_{x_i \\in [0,1]} \\left( \\varepsilon x_i^2 + (c_i - \\lambda)x_i \\right)$. The function is a parabola opening upwards. Its unconstrained minimum is found by setting the derivative to zero:\n$$\n\\frac{d}{dx_i} \\left( \\varepsilon x_i^2 + (c_i - \\lambda)x_i \\right) = 2\\varepsilon x_i + c_i - \\lambda = 0 \\implies x_i = \\frac{\\lambda - c_i}{2\\varepsilon}\n$$\nThe constrained minimizer $x_{\\varepsilon,i}^\\star(\\lambda)$ is the projection of this value onto the feasible interval $[0,1]$:\n$$\nx_{\\varepsilon,i}^\\star(\\lambda) = \\max\\left(0, \\min\\left(1, \\frac{\\lambda - c_i}{2\\varepsilon}\\right)\\right)\n$$\nThis provides a unique minimizer for any given $\\lambda$ and $\\varepsilon > 0$.\n\nThe implementation will simulate the subgradient ascent for $N$ iterations starting from $\\lambda_0 = 0$ for each of the three scenarios (unsmooth-$\\mathsf{low}$, unsmooth-$\\mathsf{high}$, smooth) and for each test case provided, recording the final value of $\\lambda$.", "answer": "```python\nimport numpy as np\n\ndef run_simulation(params, mode):\n    \"\"\"\n    Runs a projected subgradient ascent simulation for one scenario.\n\n    Args:\n        params (tuple): A tuple containing (c, b, alpha, N, epsilon).\n        mode (str): The minimizer selection mode ('low', 'high', or 'smooth').\n\n    Returns:\n        float: The final value of the dual variable lambda after N iterations.\n    \"\"\"\n    c, b, alpha, N, epsilon = params\n    c = np.array(c)\n    lam = 0.0\n\n    for _ in range(N):\n        x = np.zeros(2)\n        if mode == 'low':\n            # Rule low: if c_i - lam = 0, set x_i = 0.\n            # This is equivalent to x_i = 0 if lam <= c_i, and x_i = 1 if lam > c_i.\n            x = (lam > c).astype(float)\n        elif mode == 'high':\n            # Rule high: if c_i - lam = 0, set x_i = 1.\n            # This is equivalent to x_i = 0 if lam < c_i, and x_i = 1 if lam >= c_i.\n            x = (lam >= c).astype(float)\n        elif mode == 'smooth':\n            # Unique minimizer for the smoothed Lagrangian.\n            # proj_[0,1]((lam - c_i) / (2 * epsilon))\n            unconstrained_x = (lam - c) / (2 * epsilon)\n            x = np.clip(unconstrained_x, 0, 1)\n\n        # Compute the subgradient\n        s = b - np.sum(x)\n        # Update lambda with projection onto non-negative reals\n        lam = np.maximum(0, lam + alpha * s)\n\n    return lam\n\ndef solve():\n    \"\"\"\n    Solves the problem by running simulations for all test cases and scenarios.\n    \"\"\"\n    test_cases = [\n        # Case 1: c = [0.5, 0.5], b = 0.5, alpha = 0.25, N = 12, epsilon = 1e-3\n        ([0.5, 0.5], 0.5, 0.25, 12, 1e-3),\n        # Case 2: c = [0.0, 0.0], b = 1.0, alpha = 0.25, N = 12, epsilon = 1e-3\n        ([0.0, 0.0], 1.0, 0.25, 12, 1e-3),\n        # Case 3: c = [0.4, 0.6], b = 0.0, alpha = 0.25, N = 12, epsilon = 1e-3\n        ([0.4, 0.6], 0.0, 0.25, 12, 1e-3),\n    ]\n\n    all_results = []\n    for params in test_cases:\n        lam_low = run_simulation(params, 'low')\n        lam_high = run_simulation(params, 'high')\n        lam_smooth = run_simulation(params, 'smooth')\n        all_results.append([lam_low, lam_high, lam_smooth])\n\n    # Format the output string as [[a1,b1,c1],[a2,b2,c2],...]\n    case_strings = []\n    for res_list in all_results:\n        case_strings.append(f\"[{','.join(map(str, res_list))}]\")\n    \n    print(f\"[{','.join(case_strings)}]\")\n\nsolve()\n\n```", "id": "3141471"}]}