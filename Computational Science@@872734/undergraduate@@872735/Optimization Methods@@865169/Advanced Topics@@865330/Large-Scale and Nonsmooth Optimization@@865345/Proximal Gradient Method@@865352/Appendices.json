{"hands_on_practices": [{"introduction": "Before applying the full proximal gradient method, it is crucial to understand its fundamental building block: the proximal operator. This exercise provides a foundational workout by having you derive this operator for a simple quadratic function. By applying basic calculus to solve the minimization problem that defines the proximal operator, you will gain a concrete understanding of what this operator represents and how it is computed from first principles [@problem_id:2195112].", "problem": "In the field of numerical optimization, the proximal operator is a fundamental tool used in algorithms designed to solve non-differentiable or constrained problems. For a given scalar function $g(x)$ and a positive scaling parameter $\\lambda > 0$, the proximal operator of $\\lambda g$ applied to a point $v$ is defined as the value of $x$ that minimizes a composite objective function.\n\nThe definition is formally given by:\n$$\n\\text{prox}_{\\lambda g}(v) = \\arg\\min_{x \\in \\mathbb{R}} \\left( g(x) + \\frac{1}{2\\lambda} (x-v)^2 \\right)\n$$\nYour task is to find the proximal operator for a general quadratic function. Consider the function $g(x) = \\frac{1}{2}ax^2 + bx$, where $a$ and $b$ are real-valued constants and $a > 0$.\n\nDerive a closed-form analytic expression for $\\text{prox}_{\\lambda g}(v)$ in terms of the parameters $a$, $b$, $v$, and $\\lambda$.", "solution": "We are asked to compute $\\text{prox}_{\\lambda g}(v)$ for $g(x)=\\frac{1}{2}a x^{2}+b x$ with $a0$ and $\\lambda0$. By definition,\n$$\n\\text{prox}_{\\lambda g}(v)=\\arg\\min_{x\\in\\mathbb{R}}\\left(\\frac{1}{2}a x^{2}+b x+\\frac{1}{2\\lambda}(x-v)^{2}\\right).\n$$\nDefine the objective\n$$\nJ(x)=\\frac{1}{2}a x^{2}+b x+\\frac{1}{2\\lambda}(x-v)^{2}.\n$$\nSince $a0$ and $\\lambda0$, the function $J$ is strictly convex because its second derivative is\n$$\nJ''(x)=a+\\frac{1}{\\lambda}0,\n$$\nso it has a unique minimizer characterized by the first-order optimality condition $J'(x)=0$. Compute the derivative:\n$$\nJ'(x)=a x+b+\\frac{1}{\\lambda}(x-v)=(a+\\frac{1}{\\lambda})x+b-\\frac{v}{\\lambda}.\n$$\nSet $J'(x)=0$ and solve for $x$:\n$$\n(a+\\frac{1}{\\lambda})x+b-\\frac{v}{\\lambda}=0\n\\;\\;\\Longrightarrow\\;\\;\nx=\\frac{\\frac{v}{\\lambda}-b}{a+\\frac{1}{\\lambda}}.\n$$\nMultiplying numerator and denominator by $\\lambda$ gives\n$$\nx=\\frac{v-b\\lambda}{1+a\\lambda}.\n$$\nTherefore,\n$$\n\\text{prox}_{\\lambda g}(v)=\\frac{v-b\\lambda}{1+a\\lambda}.\n$$", "answer": "$$\\boxed{\\frac{v-b\\lambda}{1+a\\lambda}}$$", "id": "2195112"}, {"introduction": "With a grasp of the proximal operator, we can now assemble the full proximal gradient method. This practice demystifies the algorithm's abstract update rule by walking you through a single, concrete iteration. You will apply the method to a constrained optimization problem, clearly separating the process into a gradient step for the smooth part and a proximal step for the non-smooth constraint, which in this case is a simple projection [@problem_id:2195110].", "problem": "Consider the optimization problem of finding a point $x = (x_1, x_2) \\in \\mathbb{R}^2$ that minimizes a function $F(x)$ subject to non-negativity constraints on its components, i.e., $x_1 \\ge 0$ and $x_2 \\ge 0$. The function to be minimized is the squared Euclidean distance from $x$ to a target point $a$, given by $F(x) = \\frac{1}{2}\\|x - a\\|_2^2$.\n\nThis problem can be cast into the standard form for proximal algorithms, $\\min_{x} f(x) + g(x)$, by defining the smooth part as $f(x) = \\frac{1}{2}\\|x - a\\|_2^2$ and the non-smooth part $g(x)$ as the indicator function for the non-negative orthant. The indicator function $g(x)$ is zero if $x_1 \\ge 0$ and $x_2 \\ge 0$, and infinity otherwise.\n\nYou are tasked with applying the proximal gradient method to solve this problem. The iterative update rule for the proximal gradient method is given by:\n$$ x_{k+1} = \\text{prox}_{\\gamma g}(x_k - \\gamma \\nabla f(x_k)) $$\nwhere $\\gamma$ is the step size and $\\text{prox}_{\\gamma g}$ is the proximal operator associated with the function $g$.\n\nGiven the target point $a = (5, -4)$, the initial point $x_0 = (1, 1)$, and a step size of $\\gamma = 0.2$, compute the next iterate, $x_1$. Express your answer as a row vector $(x_{1,1}, x_{1,2})$, where $x_{1,1}$ and $x_{1,2}$ are the components of the vector $x_1$.", "solution": "We are minimizing $F(x)=\\frac{1}{2}\\|x-a\\|_{2}^{2}$ over the non-negative orthant. In the proximal gradient decomposition, set $f(x)=\\frac{1}{2}\\|x-a\\|_{2}^{2}$ and $g(x)=\\iota_{\\mathbb{R}_{+}^{2}}(x)$, the indicator of the feasible set $\\mathbb{R}_{+}^{2}=\\{x\\in\\mathbb{R}^{2}:x_{1}\\ge 0,\\ x_{2}\\ge 0\\}$.\n\nThe gradient of $f$ is given by\n$$\n\\nabla f(x)=x-a.\n$$\nThe proximal operator of $\\gamma g$ at a point $z$ is the Euclidean projection onto $\\mathbb{R}_{+}^{2}$:\n$$\n\\text{prox}_{\\gamma g}(z)=\\operatorname*{arg\\,min}_{y\\in\\mathbb{R}^{2}}\\left\\{\\iota_{\\mathbb{R}_{+}^{2}}(y)+\\frac{1}{2\\gamma}\\|y-z\\|_{2}^{2}\\right\\}=P_{\\mathbb{R}_{+}^{2}}(z),\n$$\nwhich is the componentwise truncation at zero:\n$$\nP_{\\mathbb{R}_{+}^{2}}(z)=\\big(\\max\\{z_{1},0\\},\\ \\max\\{z_{2},0\\}\\big).\n$$\n\nWith $a=(5,-4)$, $x_{0}=(1,1)$, and $\\gamma=0.2$, compute the gradient at $x_{0}$:\n$$\n\\nabla f(x_{0})=x_{0}-a=(1,1)-(5,-4)=(-4,5).\n$$\nPerform the gradient step:\n$$\nx_{0}-\\gamma \\nabla f(x_{0})=(1,1)-0.2\\,(-4,5)=(1+0.8,\\ 1-1)=\\left(\\frac{9}{5},\\ 0\\right).\n$$\nApply the proximal map, i.e., the projection onto $\\mathbb{R}_{+}^{2}$:\n$$\nx_{1}=\\text{prox}_{\\gamma g}\\big(x_{0}-\\gamma \\nabla f(x_{0})\\big)=P_{\\mathbb{R}_{+}^{2}}\\left(\\frac{9}{5},0\\right)=\\left(\\frac{9}{5},0\\right),\n$$\nsince both components are already non-negative.", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{9}{5}  0 \\end{pmatrix}}$$", "id": "2195110"}, {"introduction": "Understanding how an algorithm works is one thing; knowing why it performs well or poorly is another. This exercise bridges theory and practice by investigating how the structure of your data—specifically, the correlation between data features—directly impacts the algorithm's convergence speed. By analyzing how correlation affects the Lipschitz constant $L$ of the gradient, you will uncover a vital principle for any practitioner: the properties of the problem itself determine the efficiency of the solution method [@problem_id:2195111].", "problem": "In machine learning, the Least Absolute Shrinkage and Selection Operator (LASSO) is a regression analysis method that performs both variable selection and regularization. The LASSO optimization problem is formulated as finding a vector of coefficients $x \\in \\mathbb{R}^n$ that minimizes the objective function:\n$$ F(x) = f(x) + g(x) = \\frac{1}{2} \\|Ax - b\\|_2^2 + \\lambda \\|x\\|_1 $$\nwhere $A \\in \\mathbb{R}^{m \\times n}$ is the data matrix, $b \\in \\mathbb{R}^m$ is the observation vector, and $\\lambda  0$ is a regularization parameter.\n\nThis problem is often solved using the proximal gradient method, an iterative algorithm with the update rule:\n$$ x_{k+1} = \\text{prox}_{\\gamma g}(x_k - \\gamma \\nabla f(x_k)) $$\nwhere $\\gamma  0$ is the step size and $\\text{prox}_{\\gamma g}$ is the proximal operator of the function $g$.\n\nThe theoretical convergence rate of the proximal gradient method depends critically on the Lipschitz constant, $L$, of the gradient of the smooth component, $\\nabla f(x)$. For guaranteed convergence, the step size must satisfy $\\gamma  2/L$. A conservative and common choice is to set the step size proportional to $1/L$. A larger value of $L$ therefore forces the use of a smaller step size, which generally leads to slower convergence.\n\nConsider a simplified scenario where the data matrix $A \\in \\mathbb{R}^{m \\times 2}$ (for $m \\ge 2$) consists of two column vectors, $a_1$ and $a_2$. The columns are normalized such that $\\|a_1\\|_2 = 1$ and $\\|a_2\\|_2 = 1$. The Pearson correlation coefficient between the columns is given by $\\rho = a_1^T a_2$, where we consider $\\rho \\in [0, 1)$.\n\nYour task is to quantify how the column correlation $\\rho$ affects the Lipschitz constant $L$. Specifically, calculate the ratio $L_{0.8} / L_{0.2}$, where $L_{\\rho}$ denotes the Lipschitz constant of $\\nabla f(x)$ when the column correlation is $\\rho$. This ratio represents the factor by which the maximum allowable step size must be decreased when the column correlation increases from $0.2$ to $0.8$.\n\nExpress your answer as an exact fraction in simplest form.", "solution": "We want the Lipschitz constant of the gradient of the smooth part $f(x) = \\frac{1}{2}\\|Ax - b\\|_{2}^{2}$. The gradient is\n$$\n\\nabla f(x) = A^{T}(Ax - b) = A^{T}A\\,x - A^{T}b.\n$$\nFor a quadratic $f$ with Hessian $H = A^{T}A$, the gradient is $L$-Lipschitz with\n$$\nL = \\|A^{T}A\\|_{2},\n$$\nwhich, since $A^{T}A$ is symmetric positive semidefinite, equals its largest eigenvalue.\n\nWith $A = [a_{1}\\; a_{2}]$ where $\\|a_{1}\\|_{2} = \\|a_{2}\\|_{2} = 1$ and $\\rho = a_{1}^{T}a_{2} \\in [0,1)$, the Gram matrix is\n$$\nA^{T}A = \\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix}.\n$$\nThe eigenvalues solve\n$$\n\\det\\!\\left(\\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix} - \\lambda I\\right) = (1 - \\lambda)^{2} - \\rho^{2} = 0,\n$$\nso\n$$\n\\lambda_{\\pm} = 1 \\pm \\rho.\n$$\nTherefore, the Lipschitz constant is\n$$\nL_{\\rho} = \\lambda_{\\max}(A^{T}A) = 1 + \\rho.\n$$\nThe requested ratio is\n$$\n\\frac{L_{0.8}}{L_{0.2}} = \\frac{1 + 0.8}{1 + 0.2} = \\frac{1 + \\frac{4}{5}}{1 + \\frac{1}{5}} = \\frac{\\frac{9}{5}}{\\frac{6}{5}} = \\frac{9}{6} = \\frac{3}{2}.\n$$\nThus the maximum allowable step size (proportional to $1/L$) must be decreased by a factor of $\\frac{3}{2}$ when the correlation increases from $0.2$ to $0.8$.", "answer": "$$\\boxed{\\frac{3}{2}}$$", "id": "2195111"}]}