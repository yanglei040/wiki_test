{"hands_on_practices": [{"introduction": "The Douglas-Rachford algorithm can be best understood through its geometric interpretation as a sequence of projections and reflections. This first exercise [@problem_id:3122413] provides a hands-on opportunity to build this intuition by manually computing the algorithm's iterates for a simple feasibility problem in $\\mathbb{R}^2$. By working through the steps analytically, you will gain a concrete feel for how the iterates move and converge, and you will observe the important property of finite convergence for polyhedral sets.", "problem": "Consider the problem of finding a point in the intersection of two closed convex sets $C$ and $D$ in $\\mathbb{R}^{2}$, where $C$ and $D$ are the halfspaces\n$$C := \\{(x_{1},x_{2}) \\in \\mathbb{R}^{2} : x_{2} \\le 0\\}, \\quad D := \\{(x_{1},x_{2}) \\in \\mathbb{R}^{2} : x_{1} + x_{2} \\ge 0\\}.$$\nStarting from $x^{0} = (1,-3)$, apply the standard Douglas–Rachford splitting (DRS) for the feasibility problem formulated as minimizing the sum of the indicator functions of $C$ and $D$ with unit parameter. Using only fundamental definitions (proximity operators, projections onto halfspaces, and reflections defined from projections), compute the first three Douglas–Rachford iterates $x^{1}$, $x^{2}$, and $x^{3}$ analytically. Explain why the sequence exhibits finite convergence under this geometric alignment of $C$ and $D$ and this initialization. Provide as your final answer the value of $x^{3}$ as a vector. No rounding is required, and no units are involved.", "solution": "The problem is to find a point in the intersection of two closed convex sets, $C$ and $D$, using the Douglas-Rachford splitting (DRS) algorithm. The problem is formulated as minimizing the sum of the indicator functions of the two sets, $\\iota_C(x) + \\iota_D(x)$. The sets are a priori validated as sound for this application. There are no contradictions or missing pieces of information. The problem is therefore valid.\n\nThe sets are given by:\n$$C := \\{(x_{1},x_{2}) \\in \\mathbb{R}^{2} : x_{2} \\le 0\\}$$\n$$D := \\{(x_{1},x_{2}) \\in \\mathbb{R}^{2} : x_{1} + x_{2} \\ge 0\\}$$\n\nThe Douglas-Rachford splitting algorithm for finding a point in the intersection of two sets $C$ and $D$ can be expressed as the iteration:\n$$x^{k+1} = T(x^k)$$\nwhere the operator $T$ is defined, for a step-size parameter $\\lambda$, as:\n$$T(x) = x + \\lambda (P_C(2P_D(x) - x) - P_D(x))$$\nThe problem specifies a unit parameter, so we set $\\lambda=1$. The iteration is:\n$$x^{k+1} = x^k + P_C(2P_D(x^k) - x^k) - P_D(x^k)$$\nWe are given the starting point $x^{0} = (1, -3)$. To compute the iterates, we first need the analytical expressions for the projection operators $P_C$ and $P_D$.\n\n**1. Projection Operators**\n\nThe projection $P_S(x)$ gives the unique point in a closed convex set $S$ that is closest to $x$ in the Euclidean norm.\n\n- **Projection onto $C$:** The set $C$ is the lower half-plane, including the $x_1$-axis. For a point $x = (x_1, x_2)$, if $x_2 \\le 0$, the point is already in $C$, so $P_C(x) = x$. If $x_2 > 0$, the closest point in $C$ is found by setting the second component to $0$.\n$$P_C(x_1, x_2) = (x_1, \\min(x_2, 0))$$\n\n- **Projection onto $D$:** The set $D$ is the half-space defined by the inequality $x_1 + x_2 \\ge 0$. The boundary is the line $x_1 + x_2 = 0$. For a point $x = (x_1, x_2)$, if $x_1 + x_2 \\ge 0$, then $x \\in D$ and $P_D(x) = x$. If $x_1 + x_2 < 0$, the projection is found by moving from $x$ along the direction of the normal vector $n = (1, 1)$ to the boundary. The formula for projection onto a half-space $a^T z \\ge b$ is $P(x) = x - \\frac{a^T x - b}{\\|a\\|^2}a$ if $a^T x < b$. For $D$, we have $a=(1,1)^T$ and $b=0$.\n$$P_D(x_1, x_2) = \\begin{cases} (x_1, x_2) & \\text{if } x_1 + x_2 \\ge 0 \\\\ (x_1, x_2) - \\frac{x_1+x_2}{1^2+1^2}(1,1) = \\left(\\frac{x_1-x_2}{2}, \\frac{x_2-x_1}{2}\\right) & \\text{if } x_1 + x_2 < 0 \\end{cases}$$\n\n**2. Computation of Iterates**\n\nWe start with $x^0 = (1, -3)$. We will denote the intermediate projection $P_D(x^k)$ as $y^k$. The iteration is $x^{k+1} = x^k + P_C(2y^k - x^k) - y^k$.\n\n**Iteration $k=0$:**\n- **Initial point:** $x^0 = (1, -3)$.\n- **Compute $y^0 = P_D(x^0)$:**\nThe sum of the components of $x^0$ is $1 + (-3) = -2$, which is less than $0$. We use the projection formula for a point outside $D$:\n$$y^0 = P_D(1, -3) = \\left(\\frac{1 - (-3)}{2}, \\frac{-3 - 1}{2}\\right) = \\left(\\frac{4}{2}, \\frac{-4}{2}\\right) = (2, -2)$$\n- **Compute the argument for $P_C$:**\nThe argument is $2y^0 - x^0$. This is equivalent to reflecting $x^0$ across the set $D$, i.e., $R_D(x^0) = 2P_D(x^0) - x^0$.\n$$2y^0 - x^0 = 2(2, -2) - (1, -3) = (4, -4) - (1, -3) = (3, -1)$$\n- **Apply $P_C$:**\nLet $v^0 = (3, -1)$. The second component of $v^0$ is $-1$, which is less than or equal to $0$. Thus, $v^0 \\in C$, and its projection is itself:\n$$P_C(3, -1) = (3, -1)$$\n- **Compute $x^1$:**\n$$x^1 = x^0 + P_C(2y^0 - x^0) - y^0 = (1, -3) + (3, -1) - (2, -2)$$\n$$x^1 = (1+3-2, -3-1-(-2)) = (2, -4+2) = (2, -2)$$\n\nSo, the first iterate is $x^1 = (2, -2)$.\n\n**Iteration $k=1$:**\n- **Start with $x^1 = (2, -2)$.**\n- **Compute $y^1 = P_D(x^1)$:**\nThe sum of components is $2 + (-2) = 0$. Since $0 \\ge 0$, the point $x^1$ is in $D$. Thus, the projection is the point itself:\n$$y^1 = P_D(2, -2) = (2, -2)$$\n- **Compute the argument for $P_C$:**\n$$2y^1 - x^1 = 2(2, -2) - (2, -2) = (4, -4) - (2, -2) = (2, -2)$$\n- **Apply $P_C$:**\nLet $v^1 = (2, -2)$. The second component is $-2 \\le 0$, so $v^1 \\in C$.\n$$P_C(2, -2) = (2, -2)$$\n- **Compute $x^2$:**\n$$x^2 = x^1 + P_C(2y^1 - x^1) - y^1 = (2, -2) + (2, -2) - (2, -2) = (2, -2)$$\n\nThe second iterate is $x^2 = (2, -2)$.\n\n**Iteration $k=2$:**\nSince $x^2 = x^1$, the point $(2, -2)$ is a fixed point of the DRS operator $T$. All subsequent iterates will be the same.\n$$x^3 = T(x^2) = T(x^1) = x^1 = (2, -2)$$\nThus, the first three iterates are $x^1 = (2, -2)$, $x^2 = (2, -2)$, and $x^3 = (2, -2)$.\n\n**3. Explanation of Finite Convergence**\n\nThe sequence $(x^k)$ converges to the fixed point $x^*=(2,-2)$ in a single step. This finite convergence is expected in this context because the sets $C$ and $D$ are polyhedral (specifically, half-spaces). For such sets, the Douglas-Rachford algorithm is known to converge in a finite number of iterations.\n\nThe specific mechanism for this problem instance and initialization is as follows:\n1.  The first step of the algorithm calculates $y^0 = P_D(x^0) = P_D(1, -3) = (2, -2)$.\n2.  This point $y^0$ happens to lie in the intersection $C \\cap D$:\n    - For $C$: The second component of $y^0$ is $-2$, which satisfies $-2 \\le 0$. So, $y^0 \\in C$.\n    - For $D$: $y^0$ is a projection onto $D$, so it must be in $D$. Indeed, $2+(-2)=0 \\ge 0$.\n    Therefore, $y^0 \\in C \\cap D$. The projection onto one set landed on a feasible solution.\n3.  The algorithm becomes $x^1 = x^0 + P_C(2y^0 - x^0) - y^0$. Convergence to $y^0$ in this step depends on the term $P_C(2y^0-x^0)$. Let's analyze it:\n    $$2y^0 - x^0 = (3, -1)$$\n    Since the second component is $-1 \\le 0$, this point is already in $C$. This means $P_C(2y^0-x^0) = 2y^0-x^0$.\n4.  Substituting this into the update rule for $x^1$:\n    $$x^1 = x^0 + (2y^0 - x^0) - y^0 = y^0$$\n    So, $x^1 = y^0 = (2, -2)$.\n5.  Since $x^1 = (2,-2)$ is a point in the intersection $C \\cap D$, it is a valid solution. As shown in the calculation for $k=1$, any point $x \\in C \\cap D$ is a fixed point of the DR iteration with $\\lambda = 1$. The sequence becomes stationary from this point on: $x^k = (2, -2)$ for all $k \\ge 1$.\n\nIn summary, the finite convergence for this initialization is due to the geometric alignment where the initial projection $P_D(x^0)$ is already a solution, and the reflection $R_D(x^0)=2P_D(x^0)-x^0$ lies within the second set $C$. This combination algebraically forces the next iterate to be that very solution, which is then a fixed point.", "answer": "$$\\boxed{(2, -2)}$$", "id": "3122413"}, {"introduction": "Moving from geometric intuition to practical application, this exercise demonstrates how to solve a common constrained optimization problem using Douglas-Rachford splitting. A key skill in applying splitting methods is reformulating a problem into a suitable structure; here, you will transform a constrained least-residual problem by introducing an auxiliary variable [@problem_id:3122375]. This practice will guide you through deriving the necessary proximal operators and implementing the full algorithm, highlighting its superiority over more naive strategies like simple solution clipping.", "problem": "You are asked to design and implement a Douglas–Rachford splitting method to solve a constrained least-residual problem and compare it against a naive clipping strategy. Let $A \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^{m}$, and $\\tau > 0$. Consider the convex optimization problem\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\ \\|A x - b\\|_{2} \\quad \\text{subject to} \\quad \\|x\\|_{\\infty} \\le \\tau.\n$$\nFormulate this problem in the product space using an auxiliary variable $y \\in \\mathbb{R}^{m}$ as\n$$\n\\min_{x \\in \\mathbb{R}^{n},\\, y \\in \\mathbb{R}^{m}} \\ \\|y\\|_{2} \\quad \\text{subject to} \\quad y = A x - b,\\ \\|x\\|_{\\infty} \\le \\tau.\n$$\nLet $C := \\{x \\in \\mathbb{R}^{n} : \\|x\\|_{\\infty} \\le \\tau\\}$ and $M := \\{(x,y) \\in \\mathbb{R}^{n} \\times \\mathbb{R}^{m} : y = A x - b\\}$. Define the functions\n$$\nf(x,y) := \\iota_{M}(x,y), \\qquad g(x,y) := \\iota_{C}(x) + \\|y\\|_{2},\n$$\nwhere $\\iota_{S}$ denotes the indicator function of a set $S$. Your tasks are:\n\n1) Using fundamental definitions of the proximal operator and Euclidean projections, derive from first principles the following proximal mappings needed for Douglas–Rachford splitting.\n\n- The proximal mapping of $g$ at step size $\\gamma > 0$, that is, compute\n$$\n\\operatorname{prox}_{\\gamma g}(x_{0},y_{0}) = \\arg\\min_{x,y} \\ \\iota_{C}(x) + \\|y\\|_{2} + \\frac{1}{2\\gamma}\\|x-x_{0}\\|_{2}^{2} + \\frac{1}{2\\gamma}\\|y-y_{0}\\|_{2}^{2}.\n$$\nShow that this separates into the projection of $x_{0}$ onto $C$ and the proximal mapping of the Euclidean norm for $y$, explicitly characterizing each component.\n\n- The proximal mapping of $f$ at step size $\\gamma > 0$, that is, compute\n$$\n\\operatorname{prox}_{\\gamma f}(x_{0},y_{0}) = \\arg\\min_{x,y} \\ \\iota_{M}(x,y) + \\frac{1}{2\\gamma}\\|x-x_{0}\\|_{2}^{2} + \\frac{1}{2\\gamma}\\|y-y_{0}\\|_{2}^{2}.\n$$\nShow that this is the Euclidean projection of $(x_{0},y_{0})$ onto the affine set $M$, and derive a closed-form expression for the projection using only basic linear algebra: gradients, normal equations, and the characterization of projections onto affine subspaces.\n\n2) Implement the Douglas–Rachford iteration for the sum $f+g$ in the product space $\\mathbb{R}^{n} \\times \\mathbb{R}^{m}$. For a given $(x,y)$ iterate, with step size $\\gamma > 0$, perform\n- Compute $u = \\operatorname{prox}_{\\gamma g}(z)$ where $z = (x,y)$,\n- Compute $r = 2u - z$,\n- Compute $v = \\operatorname{prox}_{\\gamma f}(r)$,\n- Update $z \\leftarrow z + v - u$.\nStop when both the Douglas–Rachford gap $\\|v-u\\|_{2}$ and the feasibility gap $\\|A u_{x} - b - u_{y}\\|_{2}$ are below a small tolerance, where $u_{x}$ and $u_{y}$ denote the $x$ and $y$ blocks of $u$. Return the candidate solution $x_{\\mathrm{DR}} := u_{x}$ and report the objective value $\\|A x_{\\mathrm{DR}} - b\\|_{2}$.\n\n3) As a baseline, compute the unconstrained least-squares solution $x_{\\mathrm{LS}} := \\arg\\min_{x}\\|A x - b\\|_{2}$, and the clipped solution $x_{\\mathrm{clip}} := \\operatorname{proj}_{C}(x_{\\mathrm{LS}})$, where the projection is onto the $\\ell_{\\infty}$-ball. Report the objective value $\\|A x_{\\mathrm{clip}} - b\\|_{2}$.\n\n4) Test Suite. Use the following four test cases; in each case, $A$ is given by its rows, $b$ by its entries, and $\\tau$ as a positive scalar. All entries are real numbers.\n\n- Case $1$: $m = 4$, $n = 3$, $\\tau = 0.5$,\n  $$\n  A = \\begin{bmatrix}\n  1.0 & -0.5 & 0.3 \\\\\n  0.0 & 1.2 & -0.4 \\\\\n  0.8 & 0.0 & 1.0 \\\\\n  -0.3 & 0.6 & 0.5\n  \\end{bmatrix}, \\quad\n  b = \\begin{bmatrix} 0.5 \\\\ -1.0 \\\\ 0.2 \\\\ 1.5 \\end{bmatrix}, \\quad\n  \\tau = 0.5.\n  $$\n\n- Case $2$: $m = 5$, $n = 3$, $\\tau = 10.0$,\n  $$\n  A = \\begin{bmatrix}\n  2.0 & -1.0 & 0.0 \\\\\n  0.0 & 3.0 & -1.0 \\\\\n  1.0 & 0.0 & 1.0 \\\\\n  0.0 & -2.0 & 1.0 \\\\\n  1.5 & 0.5 & -0.5\n  \\end{bmatrix}, \\quad\n  b = \\begin{bmatrix} 1.0 \\\\ -2.0 \\\\ 0.5 \\\\ -1.0 \\\\ 0.0 \\end{bmatrix}, \\quad\n  \\tau = 10.0.\n  $$\n\n- Case $3$: $m = 3$, $n = 4$, $\\tau = 0.8$,\n  $$\n  A = \\begin{bmatrix}\n  1.0 & 0.0 & 0.5 & -0.2 \\\\\n  0.0 & 1.0 & -0.3 & 0.7 \\\\\n  0.2 & -0.5 & 0.0 & 1.0\n  \\end{bmatrix}, \\quad\n  b = \\begin{bmatrix} 0.1 \\\\ -0.2 \\\\ 0.3 \\end{bmatrix}, \\quad\n  \\tau = 0.8.\n  $$\n\n- Case $4$: $m = 4$, $n = 4$, $\\tau = 0.3$, with rank-deficient columns (the third column equals the sum of the first two),\n  $$\n  A = \\begin{bmatrix}\n  1.0 & 0.0 & 1.0 & 0.5 \\\\\n  0.5 & 1.0 & 1.5 & -0.3 \\\\\n  -0.2 & 0.3 & 0.1 & 0.0 \\\\\n  0.0 & -0.7 & -0.7 & 1.0\n  \\end{bmatrix}, \\quad\n  b = \\begin{bmatrix} 0.0 \\\\ 1.0 \\\\ -0.5 \\\\ 0.2 \\end{bmatrix}, \\quad\n  \\tau = 0.3.\n  $$\n\n5) Final Output Format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case in order, append three values:\n- the Douglas–Rachford objective value $\\|A x_{\\mathrm{DR}} - b\\|_{2}$ rounded to six decimal places,\n- the clipped baseline objective value $\\|A x_{\\mathrm{clip}} - b\\|_{2}$ rounded to six decimal places,\n- an integer improvement flag equal to $1$ if $\\|A x_{\\mathrm{DR}} - b\\|_{2} \\le \\|A x_{\\mathrm{clip}} - b\\|_{2} - 10^{-6}$ and $0$ otherwise.\n\nTherefore, the final printed line must look like\n$$\n[\\text{dr}_{1},\\text{clip}_{1},\\text{imp}_{1},\\text{dr}_{2},\\text{clip}_{2},\\text{imp}_{2},\\text{dr}_{3},\\text{clip}_{3},\\text{imp}_{3},\\text{dr}_{4},\\text{clip}_{4},\\text{imp}_{4}],\n$$\nwith each $\\text{dr}_{i}$ and $\\text{clip}_{i}$ formatted to six decimal places and each $\\text{imp}_{i}$ an integer $0$ or $1$. No other text should be printed. Angles are not involved; no physical units are required. Use $\\gamma = 1$ and a stopping tolerance of $10^{-9}$ for both gaps, and cap the maximum number of iterations at $20000$.", "solution": "The user has provided a valid, well-posed problem in convex optimization. The task is to solve a constrained least-residual problem using the Douglas-Rachford splitting method and compare its performance against a simple baseline method. The problem is stated as:\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\ \\|A x - b\\|_{2} \\quad \\text{subject to} \\quad \\|x\\|_{\\infty} \\le \\tau\n$$\nThis is reformulated in a product space $\\mathbb{R}^n \\times \\mathbb{R}^m$ by introducing an auxiliary variable $y \\in \\mathbb{R}^m$, such that $y=Ax-b$. The problem then becomes:\n$$\n\\min_{x \\in \\mathbb{R}^{n},\\, y \\in \\mathbb{R}^{m}} \\ \\|y\\|_{2} \\quad \\text{subject to} \\quad y = A x - b,\\ \\|x\\|_{\\infty} \\le \\tau\n$$\nThis structure is amenable to splitting methods by defining two functions, $f$ and $g$, whose sum we wish to minimize:\n$$\nf(x,y) := \\iota_{M}(x,y) \\quad \\text{and} \\quad g(x,y) := \\iota_{C}(x) + \\|y\\|_{2}\n$$\nHere, $\\iota_{S}$ is the indicator function of a set $S$. The set $C := \\{x \\in \\mathbb{R}^{n} : \\|x\\|_{\\infty} \\le \\tau\\}$ is the $\\ell_{\\infty}$-ball constraint on $x$, and $M := \\{(x,y) \\in \\mathbb{R}^{n} \\times \\mathbb{R}^{m} : y = A x - b\\}$ represents the affine relationship between $x$ and $y$. The Douglas-Rachford algorithm requires the proximal operators of these two functions.\n\n**1. Derivation of the Proximal Operator of $g$ ($\\operatorname{prox}_{\\gamma g}$)**\n\nThe proximal operator of $g$ with step size $\\gamma > 0$ at a point $(x_0, y_0)$ is defined by the minimization problem:\n$$\n\\operatorname{prox}_{\\gamma g}(x_{0},y_{0}) = \\arg\\min_{x,y} \\left\\{ g(x,y) + \\frac{1}{2\\gamma}\\left(\\|x-x_{0}\\|_{2}^{2} + \\|y-y_{0}\\|_{2}^{2}\\right) \\right\\}\n$$\nSubstituting the definition of $g(x,y) = \\iota_{C}(x) + \\|y\\|_{2}$, the problem becomes:\n$$\n\\arg\\min_{x,y} \\left\\{ \\iota_{C}(x) + \\|y\\|_{2} + \\frac{1}{2\\gamma}\\|x-x_{0}\\|_{2}^{2} + \\frac{1}{2\\gamma}\\|y-y_{0}\\|_{2}^{2} \\right\\}\n$$\nThe objective function is additively separable in $x$ and $y$, allowing us to solve for each variable independently.\n\nFor the $x$ component, the minimization is:\n$$\nx^{*} = \\arg\\min_{x} \\left\\{ \\iota_{C}(x) + \\frac{1}{2\\gamma}\\|x-x_{0}\\|_{2}^{2} \\right\\}\n$$\nThe indicator function $\\iota_C(x)$ is $0$ for $x \\in C$ and $+\\infty$ otherwise. This problem is therefore equivalent to finding the point in $C$ that is closest to $x_0$ in the Euclidean distance sense. This is, by definition, the Euclidean projection of $x_0$ onto $C$:\n$$\nx^{*} = \\operatorname{proj}_{C}(x_0)\n$$\nThe set $C = \\{x \\in \\mathbb{R}^{n} : \\|x\\|_{\\infty} \\le \\tau\\}$ is a hypercube defined by $-\\tau \\le x_i \\le \\tau$ for each component $i$. The projection is thus a component-wise clipping operation:\n$$\n(x^{*})_i = \\max(-\\tau, \\min((x_0)_i, \\tau))\n$$\n\nFor the $y$ component, the minimization is:\n$$\ny^{*} = \\arg\\min_{y} \\left\\{ \\|y\\|_{2} + \\frac{1}{2\\gamma}\\|y-y_{0}\\|_{2}^{2} \\right\\}\n$$\nThis is the definition of the proximal operator of the Euclidean norm, $\\operatorname{prox}_{\\gamma \\|\\cdot\\|_2}(y_0)$. The first-order optimality condition is $0 \\in \\partial(\\|y^*\\|_2) + \\frac{1}{\\gamma}(y^* - y_0)$, where $\\partial$ is the subdifferential. This implies $y_0 - y^* \\in \\gamma \\partial(\\|y^*\\|_2)$. Solving this yields the block soft-thresholding operator:\n$$\ny^{*} = \\max\\left(0, 1 - \\frac{\\gamma}{\\|y_0\\|_2}\\right) y_0\n$$\nThis can be written more robustly to handle the case $y_0=0$ as:\n$$\ny^{*} = \\left(1 - \\frac{\\gamma}{\\max(\\|y_0\\|_2, \\gamma)}\\right) y_0\n$$\nCombining the results, the proximal operator for $g$ separates into two independent operations:\n$$\n\\operatorname{prox}_{\\gamma g}(x_{0},y_{0}) = (\\operatorname{proj}_{C}(x_0), \\operatorname{prox}_{\\gamma \\|\\cdot\\|_2}(y_0))\n$$\n\n**2. Derivation of the Proximal Operator of $f$ ($\\operatorname{prox}_{\\gamma f}$)**\n\nThe proximal operator of $f$ is defined as:\n$$\n\\operatorname{prox}_{\\gamma f}(x_{0},y_{0}) = \\arg\\min_{x,y} \\left\\{ \\iota_{M}(x,y) + \\frac{1}{2\\gamma}\\left(\\|x-x_{0}\\|_{2}^{2} + \\|y-y_{0}\\|_{2}^{2}\\right) \\right\\}\n$$\nThe indicator function $\\iota_M(x,y)$ restricts the minimization to the affine subspace $M$. The scaling factor $1/(2\\gamma)$ does not change the minimizer, so this problem is equivalent to finding the Euclidean projection of $(x_0, y_0)$ onto $M$:\n$$\n(x^*, y^*) = \\operatorname{proj}_{M}(x_0, y_0) = \\arg\\min_{(x,y) \\in M} \\left\\{\\|x-x_{0}\\|_{2}^{2} + \\|y-y_{0}\\|_{2}^{2}\\right\\}\n$$\nWe use the constraint $y = Ax - b$ to eliminate $y$ from the objective function:\n$$\nx^* = \\arg\\min_{x} \\left\\{ \\|x-x_{0}\\|_{2}^{2} + \\|(Ax - b) - y_{0}\\|_{2}^{2} \\right\\} = \\arg\\min_{x} \\left\\{ \\|x-x_{0}\\|_{2}^{2} + \\|Ax - (b+y_{0})\\|_{2}^{2} \\right\\}\n$$\nThis is an unconstrained quadratic minimization problem. We find the solution by setting the gradient of the objective function, $L(x)$, to zero:\n$$\n\\nabla_x L(x) = 2(x - x_0) + 2A^T(Ax - (b+y_0)) = 0\n$$\nRearranging the terms, we obtain the normal equations for this least-squares problem:\n$$\nx - x_0 + A^T A x - A^T b - A^T y_0 = 0 \\implies (I + A^T A)x = x_0 + A^T(b+y_0)\n$$\nThe matrix $I + A^T A$ is symmetric and positive definite, and therefore invertible. This holds even if $A$ is rank-deficient. The solution for $x^*$ is:\n$$\nx^* = (I + A^T A)^{-1} (x_0 + A^T(b+y_0))\n$$\nThe corresponding $y^*$ is then determined by the subspace constraint:\n$$\ny^* = A x^* - b\n$$\nThis provides a closed-form expression for $\\operatorname{prox}_{\\gamma f}(x_0, y_0)$.\n\n**3. Algorithmic Implementation**\n\nThe Douglas-Rachford iteration is implemented as specified. Starting with an initial $z_0 = (x_0, y_0)$, the sequence of iterates is generated by:\n$$\n\\begin{align*}\nu_{k+1} &= \\operatorname{prox}_{\\gamma g}(z_k) \\\\\nr_{k+1} &= 2u_{k+1} - z_k \\\\\nv_{k+1} &= \\operatorname{prox}_{\\gamma f}(r_{k+1}) \\\\\nz_{k+1} &= z_k + (v_{k+1} - u_{k+1})\n\\end{align*}\n$$\nThe algorithm terminates when the Douglas-Rachford gap $\\|v_{k+1}-u_{k+1}\\|_{2}$ and the feasibility gap $\\|A (u_{k+1})_x - b - (u_{k+1})_y\\|_{2}$ are both below a specified tolerance. The solution is taken as $x_{\\mathrm{DR}} = (u_{k+1})_x$. For the baseline, the unconstrained least-squares solution $x_{\\mathrm{LS}} = \\arg\\min_x \\|Ax-b\\|_2$ is found using a standard solver and then clipped to the feasible set $C$, yielding $x_{\\mathrm{clip}} = \\operatorname{proj}_C(x_{\\mathrm{LS}})$.", "answer": "```python\nimport numpy as np\n\ndef douglas_rachford_solver(A, b, tau, gamma, tol, max_iter):\n    \"\"\"\n    Solves the constrained least-residual problem using Douglas-Rachford splitting.\n    min ||Ax - b||_2  s.t. ||x||_inf <= tau\n    \"\"\"\n    m, n = A.shape\n    \n    # Initialize iterates for the product space (x, y)\n    x = np.zeros(n)\n    y = np.zeros(m)\n    z = np.concatenate((x, y))\n\n    # Precompute the inverse matrix needed for prox_f\n    # (I + A^T A) is always invertible and well-conditioned\n    I_n = np.eye(n)\n    M_inv = np.linalg.inv(I_n + A.T @ A)\n\n    # Main iteration loop\n    for _ in range(max_iter):\n        xk, yk = z[:n], z[n:]\n\n        # --- Step 1: Compute prox_g(z) ---\n        # Part 1: prox for x is projection onto C (clipping)\n        ux = np.clip(xk, -tau, tau)\n        \n        # Part 2: prox for y is L2-norm block soft-thresholding\n        norm_yk = np.linalg.norm(yk)\n        # This formula is robust for yk = 0\n        uy = (1.0 - gamma / max(norm_yk, gamma)) * yk\n        u = np.concatenate((ux, uy))\n        \n        # --- Step 2: Reflection ---\n        r = 2 * u - z\n        rx, ry = r[:n], r[n:]\n\n        # --- Step 3: Compute prox_f(r) ---\n        # This is the projection onto the affine subspace M: y = Ax - b\n        rhs = rx + A.T @ (b + ry)\n        vx = M_inv @ rhs\n        vy = A @ vx - b\n        v = np.concatenate((vx, vy))\n\n        # --- Check stopping criteria ---\n        dr_gap = np.linalg.norm(v - u)\n        feasibility_gap = np.linalg.norm(A @ ux - b - uy)\n        \n        if dr_gap < tol and feasibility_gap < tol:\n            break\n\n        # --- Step 4: Update z ---\n        z = z + v - u\n\n    # The solution x is the x-part of the u iterate\n    x_dr = u[:n]\n    obj_dr = np.linalg.norm(A @ x_dr - b)\n    \n    return obj_dr\n\ndef baseline_clipping_solver(A, b, tau):\n    \"\"\"\n    Solves the unconstrained least-squares problem and clips the solution.\n    \"\"\"\n    # Find the unconstrained least-squares solution x_LS\n    x_ls = np.linalg.lstsq(A, b, rcond=None)[0]\n    \n    # Clip the solution to the feasible set C (l_infinity ball)\n    x_clip = np.clip(x_ls, -tau, tau)\n    \n    # Calculate the objective value for the clipped solution\n    obj_clip = np.linalg.norm(A @ x_clip - b)\n    \n    return obj_clip\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and produce the final output.\n    \"\"\"\n    test_cases = [\n        {\n            \"A\": np.array([\n                [1.0, -0.5, 0.3],\n                [0.0, 1.2, -0.4],\n                [0.8, 0.0, 1.0],\n                [-0.3, 0.6, 0.5]\n            ]),\n            \"b\": np.array([0.5, -1.0, 0.2, 1.5]),\n            \"tau\": 0.5\n        },\n        {\n            \"A\": np.array([\n                [2.0, -1.0, 0.0],\n                [0.0, 3.0, -1.0],\n                [1.0, 0.0, 1.0],\n                [0.0, -2.0, 1.0],\n                [1.5, 0.5, -0.5]\n            ]),\n            \"b\": np.array([1.0, -2.0, 0.5, -1.0, 0.0]),\n            \"tau\": 10.0\n        },\n        {\n            \"A\": np.array([\n                [1.0, 0.0, 0.5, -0.2],\n                [0.0, 1.0, -0.3, 0.7],\n                [0.2, -0.5, 0.0, 1.0]\n            ]),\n            \"b\": np.array([0.1, -0.2, 0.3]),\n            \"tau\": 0.8\n        },\n        {\n            \"A\": np.array([\n                [1.0, 0.0, 1.0, 0.5],\n                [0.5, 1.0, 1.5, -0.3],\n                [-0.2, 0.3, 0.1, 0.0],\n                [0.0, -0.7, -0.7, 1.0]\n            ]),\n            \"b\": np.array([0.0, 1.0, -0.5, 0.2]),\n            \"tau\": 0.3\n        }\n    ]\n\n    # Algorithm parameters\n    gamma = 1.0\n    tolerance = 1e-9\n    max_iterations = 20000\n\n    results = []\n    \n    for case in test_cases:\n        A, b, tau = case[\"A\"], case[\"b\"], case[\"tau\"]\n        \n        # Run Douglas-Rachford solver\n        dr_obj = douglas_rachford_solver(A, b, tau, gamma, tolerance, max_iterations)\n        \n        # Run baseline clipping solver\n        clip_obj = baseline_clipping_solver(A, b, tau)\n        \n        # Check for improvement\n        improvement = 1 if dr_obj <= clip_obj - 1e-6 else 0\n        \n        # Append results in specified format\n        results.append(f\"{dr_obj:.6f}\")\n        results.append(f\"{clip_obj:.6f}\")\n        results.append(str(improvement))\n\n    # Print the final output string\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3122375"}, {"introduction": "A critical aspect of any iterative algorithm is knowing when to stop. This advanced exercise [@problem_id:3122381] explores how to design a theoretically-grounded stopping criterion based on the primal-dual gap. You will delve into the deep connection between the fixed points of the Douglas-Rachford operator and the primal-dual optimality conditions for a convex problem. By deriving and implementing this gap, you will learn to certify the quality of a solution, a crucial skill for building reliable optimization software.", "problem": "You are to study the Douglas–Rachford splitting method for minimizing the sum of two proper, closed, convex functions and to design a stopping test based on the primal–dual gap that leverages the mapping between fixed points of the Douglas–Rachford operator and primal solutions. Your task has three parts: derive the stopping test from first principles, implement it for a specific family of convex functions, and evaluate the practical reliability of the test across a set of cases.\n\nConsider the convex optimization problem\n$$\n\\min_{x \\in \\mathbb{R}^n} \\; F(x) \\equiv f(x) + g(x),\n$$\nwhere the functions are defined by\n$$\nf(x) = \\tfrac{1}{2}\\|x - c\\|_2^2, \\qquad g(x) = \\lambda \\|x\\|_1,\n$$\nwith given vector $c \\in \\mathbb{R}^n$ and scalar $\\lambda \\ge 0$. The Douglas–Rachford splitting iterates are built from the proximal operator, defined for any proper, closed, convex function $h$ and parameter $\\gamma > 0$ by\n$$\n\\operatorname{prox}_{\\gamma h}(z) \\in \\arg\\min_{x \\in \\mathbb{R}^n} \\left\\{ h(x) + \\tfrac{1}{2\\gamma}\\|x - z\\|_2^2 \\right\\}.\n$$\n\nTasks:\n\n- Starting from the definition of the proximal operator and the convex subdifferential, derive the Douglas–Rachford fixed-point mapping that associates a fixed point $z^\\star$ of the Douglas–Rachford operator to a primal solution $x^\\star$ of the optimization problem. Design a stopping test based on the primal–dual gap that can be evaluated using only quantities computable from the current iterate and the given data.\n- Use Fenchel conjugates and Fenchel–Young inequality to express a computable primal–dual gap for the pair $(x,s)$, where $x \\in \\mathbb{R}^n$ is a primal candidate and $s \\in \\mathbb{R}^n$ is a dual candidate constrained by the dual feasibility condition. Do not assume any closed-form shortcut beyond definitions; derive what must be computed directly from these principles.\n- Demonstrate the reliability of your stopping test by relating the primal–dual gap tolerance to a bound on the distance to the true minimizer, using strong convexity when applicable.\n\nImplementation specifics:\n\n- Implement the Douglas–Rachford splitting with relaxation parameter equal to $1$ using only the definitions above. For the specific $f$ and $g$ given, derive and implement the proximal operators from first principles.\n- At each iteration, map the current point to a primal candidate $x_k$ via the fixed-point mapping you derived, and build a dual candidate $s_k$ that satisfies the dual feasibility condition. Compute the primal–dual gap at each iteration and stop when it is less than or equal to a prescribed tolerance.\n- For this specific problem, the exact primal minimizer is uniquely determined by strong convexity; use this to assess the reliability of your stopping test quantified by whether the returned $x$ satisfies the theoretical bound implied by the gap tolerance.\n\nTest suite:\n\nRun your program on the following test cases. Each case is a tuple $(c, \\lambda, \\gamma, \\text{tol}, \\text{max\\_iters})$ where $c \\in \\mathbb{R}^n$, $\\lambda \\ge 0$, $\\gamma > 0$, and the stopping tolerance and iteration cap are given.\n\n- Case $1$ (general case): $c = (3.0, -1.0, 0.5, -0.2, 0.0)$, $\\lambda = 0.8$, $\\gamma = 1.0$, $\\text{tol} = 10^{-4}$, $\\text{max\\_iters} = 10000$.\n- Case $2$ (boundary $\\lambda = 0$): $c = (1.2, -0.7, 0.0, 3.5)$, $\\lambda = 0.0$, $\\gamma = 1.0$, $\\text{tol} = 10^{-6}$, $\\text{max\\_iters} = 10000$.\n- Case $3$ (strong regularization driving to zero): $c = (1.0, -1.0, 2.0)$, $\\lambda = 5.0$, $\\gamma = 1.0$, $\\text{tol} = 10^{-6}$, $\\text{max\\_iters} = 10000$.\n- Case $4$ (small step size): $c = (0.1, -0.05, 0.0)$, $\\lambda = 0.01$, $\\gamma = 0.1$, $\\text{tol} = 10^{-8}$, $\\text{max\\_iters} = 20000$.\n- Case $5$ (mixed magnitudes and signs): $c = (-2.0, 0.5, 4.0, -3.0)$, $\\lambda = 1.5$, $\\gamma = 0.7$, $\\text{tol} = 10^{-5}$, $\\text{max\\_iters} = 15000$.\n\nOutput specification:\n\n- For each test case, determine whether the stopping decision based on the primal–dual gap is practically reliable in the following precise sense: if the algorithm stops with a primal–dual gap less than or equal to the tolerance $\\text{tol}$, then the returned $x$ must satisfy $\\|x - x^\\star\\|_2 \\le \\sqrt{2\\,\\text{tol}}$, where $x^\\star$ is the exact minimizer of $F$ for the given $(c,\\lambda)$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each entry is the Boolean truth value of the reliability condition for the corresponding test case, in the same order as listed above. For example, the output format must be exactly like $[\\text{True},\\text{False},\\text{True}]$ for three cases.", "solution": "The user-provided problem is a valid and well-posed problem in the field of convex optimization. It requests the derivation, implementation, and evaluation of a Douglas-Rachford splitting algorithm for a specific instance of LASSO-type optimization. All components of the problem are scientifically grounded, mathematically consistent, and formalizable.\n\nThe problem is to solve:\n$$\n\\min_{x \\in \\mathbb{R}^n} \\; F(x) \\equiv f(x) + g(x)\n$$\nwhere $f(x) = \\tfrac{1}{2}\\|x - c\\|_2^2$ and $g(x) = \\lambda \\|x\\|_1$ for a given vector $c \\in \\mathbb{R}^n$ and scalar $\\lambda \\ge 0$.\n\n### 1. Douglas-Rachford Iteration and Fixed-Point Mapping\n\nThe Douglas-Rachford (DR) splitting method is an iterative algorithm for minimizing the sum of two proper, closed, convex functions. For the problem $\\min_x (f(x) + g(x))$, one form of the DR iteration with relaxation parameter equal to $1$ is given by:\n$$\nz_{k+1} = z_k + \\operatorname{prox}_{\\gamma f}\\left(2\\operatorname{prox}_{\\gamma g}(z_k) - z_k\\right) - \\operatorname{prox}_{\\gamma g}(z_k)\n$$\nwhere $\\gamma > 0$ is a step-size parameter and $\\operatorname{prox}_{\\gamma h}(\\cdot)$ is the proximal operator of a function $h$.\n\nA solution to the optimization problem can be found from a fixed point $z^\\star$ of this iteration. A fixed point $z^\\star$ satisfies $z_{k+1} = z_k = z^\\star$, which implies:\n$$\n\\operatorname{prox}_{\\gamma f}\\left(2\\operatorname{prox}_{\\gamma g}(z^\\star) - z^\\star\\right) - \\operatorname{prox}_{\\gamma g}(z^\\star) = 0\n$$\nLet us define the primal solution candidate as $x^\\star = \\operatorname{prox}_{\\gamma g}(z^\\star)$. Substituting this into the fixed-point equation yields:\n$$\nx^\\star = \\operatorname{prox}_{\\gamma f}(2x^\\star - z^\\star)\n$$\nThe definition of the proximal operator states that $p = \\operatorname{prox}_{\\gamma h}(q)$ is equivalent to the variational inclusion $q \\in p + \\gamma \\partial h(p)$, where $\\partial h$ is the convex subdifferential of $h$. Applying this to our two conditions, we get:\n1. From $x^\\star = \\operatorname{prox}_{\\gamma g}(z^\\star)$: $z^\\star \\in x^\\star + \\gamma \\partial g(x^\\star) \\implies \\frac{z^\\star - x^\\star}{\\gamma} \\in \\partial g(x^\\star)$.\n2. From $x^\\star = \\operatorname{prox}_{\\gamma f}(2x^\\star - z^\\star)$: $2x^\\star - z^\\star \\in x^\\star + \\gamma \\partial f(x^\\star) \\implies \\frac{x^\\star - z^\\star}{\\gamma} \\in \\partial f(x^\\star)$.\n\nLet $v = \\frac{x^\\star - z^\\star}{\\gamma}$. Then the two inclusions become $-v \\in \\partial g(x^\\star)$ and $v \\in \\partial f(x^\\star)$. Summing these gives $0 \\in \\partial f(x^\\star) + \\partial g(x^\\star)$, which is the first-order optimality condition for the primal problem $\\min_x F(x)$. Thus, if $z^\\star$ is a fixed point of the DR operator, then $x^\\star = \\operatorname{prox}_{\\gamma g}(z^\\star)$ is a primal minimizer. This establishes the required mapping from a fixed point to the primal solution.\n\n### 2. Primal-Dual Gap and Stopping Test\n\nTo design a stopping test, we formulate the primal-dual gap. The primal problem is $P = \\inf_x \\{f(x) + g(x)\\}$. The Fenchel dual problem is $D = \\sup_s \\{-f^*(s) - g^*(-s)\\}$, where $h^*(s) = \\sup_x \\{\\langle s, x \\rangle - h(x)\\}$ is the Fenchel conjugate.\nThe primal-dual gap for a primal candidate $x$ and a dual candidate $s$ is:\n$$\n\\text{Gap}(x, s) = (f(x) + g(x)) - (-f^*(s) - g^*(-s)) = f(x) + g(x) + f^*(s) + g^*(-s)\n$$\nBy the Fenchel-Young inequality, this gap is always non-negative.\n\nAt each iteration $k$ of the DR algorithm, we generate a primal candidate $x_k = \\operatorname{prox}_{\\gamma g}(z_k)$. From the derivation above, we have the inclusion $\\frac{z_k - x_k}{\\gamma} \\in \\partial g(x_k)$. We can use this to construct a dual candidate. Let's define $u_k = \\frac{z_k - x_k}{\\gamma}$. The optimality condition requires finding a single $s^\\star$ such that $-s^\\star \\in \\partial f(x^\\star)$ and $s^\\star \\in \\partial g(x^\\star)$. This suggests setting our dual candidate $s_k = -u_k = \\frac{x_k-z_k}{\\gamma}$, so that $-s_k \\in \\partial g(x_k)$.\n\nBy the Fenchel-Young inequality, the condition $-s_k \\in \\partial g(x_k)$ implies that $g(x_k) + g^*(-s_k) = \\langle -s_k, x_k \\rangle$. This simplifies the gap expression:\n$$\n\\text{Gap}(x_k, s_k) = f(x_k) + g(x_k) + f^*(s_k) + g^*(-s_k) = f(x_k) + f^*(s_k) + \\langle-s_k, x_k\\rangle\n$$\nWe need to compute the conjugates for our specific functions:\n- $f(x) = \\frac{1}{2}\\|x - c\\|_2^2$: The conjugate is $f^*(s) = \\frac{1}{2}\\|s\\|_2^2 + \\langle s, c \\rangle$.\n- $g(x) = \\lambda \\|x\\|_1$: The conjugate is $g^*(s) = 0$ if $\\|s\\|_\\infty \\le \\lambda$ and $\\infty$ otherwise.\n\nThe choice of dual candidate $s_k$ has another favorable property. Since $-s_k \\in \\partial g(x_k) = \\lambda \\partial \\|x_k\\|_1$, its components are bounded by $\\lambda$, i.e., $\\|-s_k\\|_\\infty \\le \\lambda$. This means $g^*(-s_k) = 0$, which ensures the dual objective is finite. So, the gap can be calculated as:\n$$\n\\text{Gap}_k = f(x_k) + g(x_k) + f^*(s_k)\n$$\nSubstituting the expressions for $f$, $g$, and $f^*$:\n$$\n\\text{Gap}_k = \\left(\\frac{1}{2}\\|x_k-c\\|_2^2\\right) + \\left(\\lambda\\|x_k\\|_1\\right) + \\left(\\frac{1}{2}\\|s_k\\|_2^2 + \\langle s_k, c \\rangle\\right)\n$$\nWith $s_k = \\frac{x_k-z_k}{\\gamma}$, this becomes:\n$$\n\\text{Gap}_k = \\frac{1}{2}\\|x_k-c\\|_2^2 + \\lambda\\|x_k\\|_1 + \\frac{1}{2\\gamma^2}\\|x_k-z_k\\|_2^2 + \\frac{1}{\\gamma}\\langle x_k-z_k, c \\rangle\n$$\nThis expression is computable at each iteration using only the iterate $z_k$, the derived $x_k$, and problem data. The algorithm stops when $\\text{Gap}_k \\le \\text{tol}$.\n\n### 3. Reliability of the Stopping Test\n\nThe objective function $F(x) = f(x) + g(x)$ is the sum of a $1$-strongly convex function ($f$) and a convex function ($g$). Therefore, $F(x)$ is $1$-strongly convex. For a $\\mu$-strongly convex function, the following inequality holds:\n$$\nF(x) - F(x^\\star) \\ge \\frac{\\mu}{2}\\|x - x^\\star\\|_2^2\n$$\nIn our case, $\\mu = 1$. Furthermore, for any primal-dual pair $(x, s)$, the gap provides an upper bound on the suboptimality of the primal objective: $\\text{Gap}(x, s) \\ge F(x) - F(x^\\star)$.\nCombining these inequalities, we have:\n$$\n\\text{Gap}_k \\ge F(x_k) - F(x^\\star) \\ge \\frac{1}{2}\\|x_k - x^\\star\\|_2^2\n$$\nThis gives a bound on the distance to the true minimizer $x^\\star$:\n$$\n\\|x_k - x^\\star\\|_2 \\le \\sqrt{2 \\cdot \\text{Gap}_k}\n$$\nIf the algorithm terminates because $\\text{Gap}_k \\le \\text{tol}$, the returned solution $x_k$ is guaranteed to satisfy:\n$$\n\\|x_k - x^\\star\\|_2 \\le \\sqrt{2 \\cdot \\text{tol}}\n$$\nThis provides the theoretical basis for a reliable stopping test.\n\n### 4. Implementation Details: Proximal Operators and Exact Solution\n\nTo implement the algorithm, we need the closed-form expressions for the proximal operators.\n- For $f(x) = \\frac{1}{2}\\|x-c\\|_2^2$:\n$$ \\operatorname{prox}_{\\gamma f}(z) = \\arg\\min_x \\left\\{\\frac{1}{2}\\|x-c\\|_2^2 + \\frac{1}{2\\gamma}\\|x-z\\|_2^2\\right\\} = \\frac{\\gamma c + z}{\\gamma+1} $$\n- For $g(x) = \\lambda\\|x\\|_1$:\n$$ \\operatorname{prox}_{\\gamma g}(z) = \\arg\\min_x \\left\\{\\lambda\\|x\\|_1 + \\frac{1}{2\\gamma}\\|x-z\\|_2^2\\right\\} = S_{\\gamma\\lambda}(z) $$\nwhere $S_\\alpha(z)$ is the component-wise soft-thresholding operator: $(S_\\alpha(z))_i = \\operatorname{sign}(z_i)\\max(|z_i|-\\alpha, 0)$.\n\nThe exact solution $x^\\star$ can be found from the optimality condition $0 \\in x^\\star - c + \\lambda\\partial\\|x^\\star\\|_1$, which is equivalent to $x^\\star = \\operatorname{prox}_{\\lambda g}(c) = S_\\lambda(c)$. This allows for direct computation of the true minimizer to verify the reliability of our stopping criterion.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the Douglas-Rachford splitting method for a specific convex\n    optimization problem, evaluates a primal-dual gap-based stopping test,\n    and checks its reliability against a theoretical bound.\n    \"\"\"\n\n    def soft_threshold(z, threshold):\n        \"\"\"Component-wise soft-thresholding operator.\"\"\"\n        return np.sign(z) * np.maximum(np.abs(z) - threshold, 0)\n\n    def prox_f(z_in, c_vec, gamma_val):\n        \"\"\"Proximal operator of f(x) = 1/2*||x-c||_2^2.\"\"\"\n        return (gamma_val * c_vec + z_in) / (gamma_val + 1.0)\n\n    def prox_g(z_in, lambda_val, gamma_val):\n        \"\"\"Proximal operator of g(x) = lambda*||x||_1.\"\"\"\n        return soft_threshold(z_in, gamma_val * lambda_val)\n\n    def run_douglas_rachford(c, lam, gam, tol, max_iters):\n        \"\"\"\n        Executes the Douglas-Rachford algorithm for a given test case.\n        \"\"\"\n        n = c.shape[0]\n        z = np.zeros(n)\n        \n        final_x = np.zeros(n)\n        final_gap = np.inf\n\n        for _ in range(max_iters):\n            # Step 1: Compute primal candidate x_k\n            x = prox_g(z, lam, gam)\n            \n            # Step 2: Compute dual candidate s_k and the primal-dual gap\n            s = (x - z) / gam\n            \n            f_x = 0.5 * np.linalg.norm(x - c)**2\n            g_x = lam * np.linalg.norm(x, 1)\n            f_star_s = 0.5 * np.linalg.norm(s)**2 + np.dot(s, c)\n            \n            gap = f_x + g_x + f_star_s\n            \n            # Step 3: Check stopping condition\n            if gap <= tol:\n                final_x = x\n                final_gap = gap\n                return final_x, final_gap\n\n            # Step 4: Perform the Douglas-Rachford update for z\n            y_prime = 2 * x - z\n            x_prime = prox_f(y_prime, c, gam)\n            z = z + x_prime - x\n        \n        # If max_iters is reached, return the last computed values\n        final_x = x\n        final_gap = gap\n        return final_x, final_gap\n\n    # Test suite from the problem statement\n    test_cases_data = [\n        (np.array([3.0, -1.0, 0.5, -0.2, 0.0]), 0.8, 1.0, 1e-4, 10000),\n        (np.array([1.2, -0.7, 0.0, 3.5]), 0.0, 1.0, 1e-6, 10000),\n        (np.array([1.0, -1.0, 2.0]), 5.0, 1.0, 1e-6, 10000),\n        (np.array([0.1, -0.05, 0.0]), 0.01, 0.1, 1e-8, 20000),\n        (np.array([-2.0, 0.5, 4.0, -3.0]), 1.5, 0.7, 1e-5, 15000),\n    ]\n\n    results = []\n    for c, lam, gam, tol, max_iters in test_cases_data:\n        # Run the algorithm\n        x_final, gap_final = run_douglas_rachford(c, lam, gam, tol, max_iters)\n\n        # Compute the exact solution\n        x_star = soft_threshold(c, lam)\n\n        # Check the reliability condition\n        # The test is reliable unless the algorithm stops due to tolerance\n        # AND the error bound is violated.\n        stopped_by_tol = (gap_final <= tol)\n        \n        # Note: If gap is negative due to floating point error, it should be treated as 0 for the bound.\n        # But the primal-dual gap is theoretically non-negative.\n        bound_violated = (np.linalg.norm(x_final - x_star) > np.sqrt(2 * tol))\n\n        is_reliable = not (stopped_by_tol and bound_violated)\n        results.append(is_reliable)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3122381"}]}