{"hands_on_practices": [{"introduction": "To begin our hands-on exploration, we first need to understand how optimization landscapes become challenging enough to trap local search methods. This exercise [@problem_id:3145555] guides you through a \"constructive\" approach, showing how composing a simple convex function with a periodic \"domain-warping\" function creates a landscape riddled with numerous local minima. By analyzing this structure, you will develop a fundamental intuition for the origin of non-convexity and the critical role of basins of attraction.", "problem": "Consider a one-dimensional objective constructed by composing a convex outer function with a domain-warping inner function. Let the outer function be $g(y) = \\tfrac{1}{2}(y - a)^{2}$, where $a \\in (-1, 1)$, and let the inner function be $h(x) = \\cos(kx)$, where $k \\in \\mathbb{N}$ is a positive integer. Define the composed objective $f(x) = g(h(x)) = \\tfrac{1}{2}(\\cos(kx) - a)^{2}$ on the domain $x \\in \\mathbb{R}$. The inner function $h$ folds the domain periodically into the range $[-1, 1]$, and the composition replicates structures in $g$ across the folds. Consider local search via gradient descent with a fixed step size $\\alpha > 0$ that is sufficiently small to ensure local convergence around strict local minima. A basin of attraction is defined as the set of all initial points $x_{0}$ whose gradient descent iterates converge to the same local minimum.\n\nWhich of the following statements about the landscape $f(x)$ and the basins of attraction created by this domain warping are correct? Select all that apply.\n\nA. For any integer $k \\ge 1$ and any $a$ with $|a| < 1$, the function $f(x)$ has exactly $2$ local minima per interval of length $2\\pi/k$, located at points where $\\cos(kx) = a$.\n\nB. The set of stationary points of $f(x)$ consists only of solutions to $\\cos(kx) = a$; when $a \\neq \\pm 1$, there are no stationary points arising from $h'(x) = 0$.\n\nC. Under gradient descent with sufficiently small constant step size $\\alpha > 0$, almost every initial point $x_{0}$ converges to a local minimum of $f(x)$; the basins of attraction are open intervals bounded by points where $\\sin(kx) = 0$.\n\nD. Increasing $k$ while keeping $\\alpha$ fixed makes the basins of attraction wider, improving global convergence because the gradient magnitude scales down by $k$.\n\nE. The total number of distinct local minima of $f$ on the interval $[0, 2\\pi]$ is equal to $2k$ when $|a| < 1$.", "solution": "The problem statement has been validated and is found to be mathematically sound, well-posed, and unambiguous. It provides a clear setup for analyzing the properties of a composed objective function, which is a standard topic in optimization theory. All necessary definitions and constraints are provided. We may therefore proceed with the solution.\n\nThe objective function is given by the composition $f(x) = g(h(x))$, where the outer function is $g(y) = \\tfrac{1}{2}(y - a)^2$ for $a \\in (-1, 1)$ and the inner function is $h(x) = \\cos(kx)$ for a positive integer $k \\in \\mathbb{N}$. The function $f(x)$ is therefore $f(x) = \\tfrac{1}{2}(\\cos(kx) - a)^2$. The domain is $x \\in \\mathbb{R}$.\n\nTo analyze the landscape of $f(x)$, we first find its stationary points by setting its first derivative with respect to $x$ to zero. Using the chain rule, the derivative is:\n$$ f'(x) = \\frac{d}{dx} \\left[ \\tfrac{1}{2}(\\cos(kx) - a)^2 \\right] = (\\cos(kx) - a) \\cdot \\frac{d}{dx}(\\cos(kx)) $$\n$$ f'(x) = (\\cos(kx) - a) \\cdot (-k \\sin(kx)) = -k \\sin(kx) (\\cos(kx) - a) $$\nFor a stationary point, we must have $f'(x) = 0$. This equation holds if and only if one of the following two conditions is met:\n1. $\\sin(kx) = 0$\n2. $\\cos(kx) - a = 0$, which is $\\cos(kx) = a$\n\nNext, we classify these stationary points using the second derivative test. The second derivative, $f''(x)$, is found by differentiating $f'(x)$ using the product rule:\n$$ f''(x) = \\frac{d}{dx} [-k \\sin(kx) (\\cos(kx) - a)] $$\n$$ f''(x) = -k \\left[ (\\frac{d}{dx}\\sin(kx))(\\cos(kx) - a) + \\sin(kx)(\\frac{d}{dx}(\\cos(kx) - a)) \\right] $$\n$$ f''(x) = -k \\left[ (k \\cos(kx))(\\cos(kx) - a) + \\sin(kx)(-k \\sin(kx)) \\right] $$\n$$ f''(x) = -k^2 \\left[ \\cos^2(kx) - a\\cos(kx) - \\sin^2(kx) \\right] $$\nUsing the double-angle identity $\\cos(2\\theta) = \\cos^2(\\theta) - \\sin^2(\\theta)$, this simplifies to:\n$$ f''(x) = -k^2 [\\cos(2kx) - a\\cos(kx)] = k^2 [a\\cos(kx) - \\cos(2kx)] $$\n\nNow we can classify the two types of stationary points:\n\nCase 1: $\\cos(kx) = a$.\nSince a is given to be in the interval $(-1, 1)$, this equation always has solutions for $x$.\nAt these points, we evaluate the second derivative:\n$$ f''(x) = k^2 [a(a) - \\cos(2kx)] $$\nUsing the identity $\\cos(2\\theta) = 2\\cos^2(\\theta) - 1$, we have:\n$$ f''(x) = k^2 [a^2 - (2\\cos^2(kx) - 1)] = k^2 [a^2 - (2a^2 - 1)] = k^2 (1 - a^2) $$\nSince $k$ is a positive integer, $k^2 > 0$. Since $a \\in (-1, 1)$, we have $a^2 < 1$, which means $1 - a^2 > 0$.\nThus, $f''(x) = k^2(1 - a^2) > 0$. By the second derivative test, all points where $\\cos(kx) = a$ are strict local minima. At these points, the function value is $f(x) = \\tfrac{1}{2}(a - a)^2 = 0$, which is the global minimum value.\n\nCase 2: $\\sin(kx) = 0$.\nThis condition implies $kx = n\\pi$ for some integer $n \\in \\mathbb{Z}$.\nAt these points, $\\cos(kx) = \\cos(n\\pi) = (-1)^n$. Also, $\\cos(2kx) = \\cos(2n\\pi) = 1$.\nWe evaluate the second derivative:\n$$ f''(x) = k^2 [a\\cos(kx) - \\cos(2kx)] = k^2 [a(-1)^n - 1] $$\nSince $a \\in (-1, 1)$, we have $-1 < a < 1$. It follows that $-1 < a(-1)^n < 1$. Therefore, $a(-1)^n - 1$ is a number between $-2$ and $0$ (exclusive of endpoints).\nSpecifically, $a(-1)^n - 1 < 0$. Since $k^2 > 0$, we have $f''(x) < 0$.\nBy the second derivative test, all points where $\\sin(kx) = 0$ are strict local maxima.\n\nWith this complete classification of stationary points, we can evaluate each option.\n\nA. For any integer $k \\ge 1$ and any $a$ with $|a| < 1$, the function $f(x)$ has exactly $2$ local minima per interval of length $2\\pi/k$, located at points where $\\cos(kx) = a$.\nThe function $f(x) = \\tfrac{1}{2}(\\cos(kx) - a)^2$ is periodic with a fundamental period of $P = 2\\pi/k$. To find the number of local minima in one period, we can consider the interval $x \\in [0, 2\\pi/k)$. In this interval, the argument $u = kx$ ranges over $[0, 2\\pi)$. We need to count the number of solutions to $\\cos(u) = a$ for $u \\in [0, 2\\pi)$.\nSince $|a| < 1$, the equation $\\cos(u) = a$ has exactly two solutions in the interval $[0, 2\\pi)$: one is $u_1 = \\arccos(a) \\in (0, \\pi)$, and the other is $u_2 = 2\\pi - \\arccos(a) \\in (\\pi, 2\\pi)$. These correspond to two distinct points $x_1 = u_1/k$ and $x_2 = u_2/k$ in the interval $[0, 2\\pi/k)$. As established, these points are local minima.\nTherefore, this statement is **Correct**.\n\nB. The set of stationary points of $f(x)$ consists only of solutions to $\\cos(kx) = a$; when $a \\neq \\pm 1$, there are no stationary points arising from $h'(x) = 0$.\nOur derivation of stationary points shows that they occur where $f'(x) = -k \\sin(kx) (\\cos(kx) - a) = 0$. This leads to two sets of solutions: $\\cos(kx) = a$ and $\\sin(kx) = 0$. The inner function is $h(x) = \\cos(kx)$, so its derivative is $h'(x) = -k\\sin(kx)$. The condition $h'(x) = 0$ is exactly $\\sin(kx) = 0$ (since $k \\neq 0$). We have shown that points satisfying this condition are indeed stationary points (specifically, local maxima). The statement claims there are no stationary points arising from $h'(x)=0$, which is false.\nTherefore, this statement is **Incorrect**.\n\nC. Under gradient descent with sufficiently small constant step size $\\alpha > 0$, almost every initial point $x_{0}$ converges to a local minimum of $f(x)$; the basins of attraction are open intervals bounded by points where $\\sin(kx) = 0$.\nThe function $f(x)$ is a one-dimensional, continuously differentiable function. Its stationary points are either local minima or local maxima. For gradient descent, local minima are stable fixed points, while local maxima are unstable fixed points. The set of initial points that converge to a local maximum consists only of the maxima themselves. This set is countable and thus has Lebesgue measure zero. Therefore, \"almost every\" initial point (all points except the maxima) will converge to a local minimum.\nThe boundaries between the basins of attraction of adjacent local minima in a one-dimensional landscape are the local maxima that lie between them. These maxima act as \"dividing points\" or \"watersheds\". We have shown that the local maxima of $f(x)$ occur at points where $\\sin(kx) = 0$. Thus, the basins of attraction for the local minima are the open intervals bounded by these maxima.\nTherefore, this statement is **Correct**.\n\nD. Increasing $k$ while keeping $\\alpha$ fixed makes the basins of attraction wider, improving global convergence because the gradient magnitude scales down by $k$.\nFrom the analysis of option C, the basins of attraction are bounded by the local maxima, which are located at $x$ values such that $kx = n\\pi$, or $x = n\\pi/k$. The width of a typical basin of attraction is the distance between two consecutive maxima, which is $|(n+1)\\pi/k - n\\pi/k| = \\pi/k$. As $k$ increases, the width $\\pi/k$ decreases, meaning the basins become narrower, not wider.\nThe second part of the claim is about the gradient magnitude. The gradient is $f'(x) = -k \\sin(kx) (\\cos(kx) - a)$. The magnitude is $|f'(x)| = k |\\sin(kx) (\\cos(kx) - a)|$. Holding all else constant, the magnitude of the gradient scales proportionally with $k$, i.e., it scales *up* with $k$. Both claims in the statement are false.\nTherefore, this statement is **Incorrect**.\n\nE. The total number of distinct local minima of $f$ on the interval $[0, 2\\pi]$ is equal to $2k$ when $|a| < 1$.\nWe need to count the number of distinct solutions to $\\cos(kx) = a$ for $x \\in [0, 2\\pi]$.\nLet $u = kx$. As $x$ ranges over $[0, 2\\pi]$, $u$ ranges over $[0, 2\\pi k]$.\nWe are counting the solutions to $\\cos(u) = a$ in the interval $u \\in [0, 2\\pi k]$. The interval $[0, 2\\pi k]$ consists of $k$ complete cycles of the cosine function, specifically the intervals $[0, 2\\pi), [2\\pi, 4\\pi), \\dots, [2\\pi(k-1), 2\\pi k)$. In each of these $k$ intervals, the equation $\\cos(u) = a$ has exactly $2$ solutions because $|a| < 1$.\nThe total number of solutions is therefore $k \\times 2 = 2k$. These $2k$ solutions for $u$ are all distinct and lie within $[0, 2\\pi k)$. Since $x=u/k$, they correspond to $2k$ distinct solutions for $x$ in the interval $[0, 2\\pi)$. The endpoint $x=2\\pi$ corresponds to $u=2\\pi k$, and $\\cos(2\\pi k) = 1 \\ne a$, so no minimum is located at the endpoint.\nTherefore, this statement is **Correct**.", "answer": "$$\\boxed{ACE}$$", "id": "3145555"}, {"introduction": "Having established how complex landscapes can emerge, we now turn to a foundational strategy for global search: the Multi-Start algorithm. This practice [@problem_id:3145569] moves beyond simple execution and asks you to theoretically analyze its performance on the well-known Rastrigin benchmark function. Through a probabilistic derivation, you will discover how the required number of restarts scales with the problem's dimension, providing a stark, quantitative illustration of the \"curse of dimensionality\" that plagues many simple global search methods.", "problem": "Consider the Rastrigin function defined for a dimension $n \\in \\mathbb{N}$ by\n$$\nf(\\mathbf{x}) \\;=\\; 10 n \\;+\\; \\sum_{i=1}^{n} \\left(x_{i}^{2} \\;-\\; 10 \\cos\\!\\left(2 \\pi x_{i}\\right)\\right),\n$$\non the box domain\n$$\n\\mathcal{D} \\;=\\; [-5.12,\\, 5.12]^{n}.\n$$\nYou will analyze Multi-Start (MS) gradient descent, defined as the following procedure: each run initializes $\\mathbf{x}_{0}$ by drawing independently and uniformly from $\\mathcal{D}$, then applies gradient descent with a sufficiently small fixed step size so that the iterates converge to a local minimizer of $f$. For this study, adopt the following scientifically reasonable idealization grounded in symmetry and the periodicity of the cosine term: the basins of attraction of the local minimizers coincide with the Voronoi cells of the integer lattice $\\mathbb{Z}^{n}$ restricted to $\\mathcal{D}$, so that each local minimizer located at an integer lattice point $\\mathbf{k} \\in \\mathbb{Z}^{n} \\cap \\mathcal{D}$ attracts all points in the hypercube centered at $\\mathbf{k}$ with side length $1$, and the global minimizer is at $\\mathbf{0}$.\n\nStarting from fundamental definitions in optimization and probability, and without invoking any pre-derived shortcut formulas, do the following:\n- Using the geometric description of basins of attraction and the uniform initialization over $\\mathcal{D}$, derive the single-run success probability $p(n)$ that MS gradient descent converges to the global minimizer $\\mathbf{0}$.\n- Using independence of restarts and the definition of probability of at least one success in $M$ independent Bernoulli trials, derive the minimal number of restarts $M(n,\\delta)$ required so that the probability of having reached the global minimizer at least once across $M$ runs is at least $1 - \\delta$, where $\\delta \\in (0,1)$ is a prescribed failure probability.\n\nProvide your final answer as one closed-form analytic expression for $M(n,\\delta)$ in terms of $n$ and $\\delta$. Do not provide an inequality or a bound; provide the exact expression implied by your derivation. No rounding is required, and no physical units are involved.", "solution": "The problem is first subjected to a rigorous validation process.\n\n### Step 1: Extract Givens\n- **Function**: The Rastrigin function is defined as $f(\\mathbf{x}) = 10n + \\sum_{i=1}^{n} \\left(x_{i}^{2} - 10 \\cos(2 \\pi x_{i})\\right)$.\n- **Dimension**: $n \\in \\mathbb{N}$.\n- **Domain**: $\\mathcal{D} = [-5.12, 5.12]^{n}$.\n- **Algorithm**: Multi-Start (MS) gradient descent.\n- **Initialization**: For each run, the starting point $\\mathbf{x}_{0}$ is drawn independently and uniformly from the domain $\\mathcal{D}$.\n- **Convergence Assumption**: Gradient descent is assumed to converge to a local minimizer.\n- **Basin of Attraction Idealization**: The basin of attraction for a local minimizer at an integer lattice point $\\mathbf{k} \\in \\mathbb{Z}^{n} \\cap \\mathcal{D}$ is the hypercube of side length $1$ centered at $\\mathbf{k}$.\n- **Global Minimizer**: The global minimizer is located at $\\mathbf{0}$.\n- **Objective 1**: Derive the single-run success probability, $p(n)$, of converging to the global minimizer.\n- **Objective 2**: Derive the minimal number of restarts, $M(n,\\delta)$, required to achieve a probability of at least $1 - \\delta$ of finding the global minimizer at least once, where $\\delta \\in (0,1)$.\n- **Output Requirement**: The final answer must be a single closed-form analytic expression for $M(n,\\delta)$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the validation criteria.\n\n- **Scientifically Grounded**: The problem is well-grounded in the field of numerical optimization. The Rastrigin function is a standard, non-convex benchmark for testing optimization algorithms. The concepts of gradient descent, basins of attraction, and the Multi-Start strategy are fundamental. The crucial element is the \"scientifically reasonable idealization\" regarding the basins of attraction. For the Rastrigin function, local minima are located near integer points. The simplification that the basins of attraction are the Voronoi cells of the integer lattice (which are hypercubes) is a common and appropriate idealization for analytical tractability. It captures the essential multi-modal nature of the function without delving into the complex fractal boundaries of the true basins. The identification of $\\mathbf{x}=\\mathbf{0}$ as the global minimizer is correct. The problem is scientifically sound.\n- **Well-Posed**: The problem is clearly stated with all necessary information provided. The objectives are specific and lead to a unique, derivable solution under the given assumptions.\n- **Objective**: The language is precise and formal. Assumptions are explicitly stated as \"idealizations,\" which is a mark of objective scientific communication. There are no subjective or ambiguous terms.\n\nThe problem does not exhibit any of the listed flaws (e.g., factual unsoundness, incompleteness, contradiction, etc.). It represents a standard theoretical exercise in the analysis of a global optimization algorithm.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A full, reasoned solution will be provided.\n\n### Solution Derivation\n\nThe solution is derived in two parts as requested by the problem statement.\n\n**Part 1: Derivation of the single-run success probability $p(n)$**\n\nThe success of a single run of gradient descent is defined as converging to the global minimizer at $\\mathbf{x} = \\mathbf{0}$. According to the problem's idealization, this occurs if and only if the starting point $\\mathbf{x}_{0}$ is initialized within the basin of attraction of the global minimizer.\n\nLet $\\mathcal{B}_{\\mathbf{0}}$ denote the basin of attraction of the global minimizer at $\\mathbf{0}$. The problem states that this basin is the hypercube of side length $1$ centered at $\\mathbf{0}$. Mathematically, this is:\n$$\n\\mathcal{B}_{\\mathbf{0}} = \\left[-\\frac{1}{2}, \\frac{1}{2}\\right]^n\n$$\nThe starting point $\\mathbf{x}_{0}$ is drawn uniformly from the domain $\\mathcal{D} = [-5.12, 5.12]^{n}$. For a uniform probability distribution over a geometric region, the probability of an event (i.e., drawing a point from a specific subregion) is the ratio of the volume of the subregion to the volume of the total sample space.\n\nThe single-run success probability, $p(n)$, is therefore given by:\n$$\np(n) = \\frac{\\text{Vol}(\\mathcal{B}_{\\mathbf{0}})}{\\text{Vol}(\\mathcal{D})}\n$$\nThe volume of a hypercube in $n$ dimensions with side length $L$ is $L^n$.\nThe volume of the domain $\\mathcal{D}$ is calculated with a side length of $L_{\\mathcal{D}} = 5.12 - (-5.12) = 10.24$.\n$$\n\\text{Vol}(\\mathcal{D}) = (10.24)^n\n$$\nThe volume of the basin of attraction $\\mathcal{B}_{\\mathbf{0}}$ is calculated with a side length of $L_{\\mathcal{B}_{\\mathbf{0}}} = \\frac{1}{2} - \\left(-\\frac{1}{2}\\right) = 1$.\n$$\n\\text{Vol}(\\mathcal{B}_{\\mathbf{0}}) = (1)^n = 1\n$$\nNote that $\\mathcal{B}_{\\mathbf{0}}$ is fully contained within $\\mathcal{D}$, as required.\nSubstituting these volumes into the probability formula, we obtain the single-run success probability:\n$$\np(n) = \\frac{1}{(10.24)^n} = (10.24)^{-n}\n$$\n\n**Part 2: Derivation of the minimal number of restarts $M(n,\\delta)$**\n\nThe Multi-Start procedure consists of $M$ independent runs. Each run is a Bernoulli trial with two outcomes:\n- Success (finding the global minimum), with probability $p(n)$.\n- Failure (converging to a different local minimum), with probability $1 - p(n)$.\n\nWe require that the probability of finding the global minimum at least once in $M$ runs is at least $1 - \\delta$. Let $S_M$ be the event of at least one success in $M$ trials. It is more convenient to analyze the complement event, $S_M^c$, which is the event of having no successes in $M$ trials (i.e., all $M$ trials are failures).\nThe probability of $S_M^c$ is:\n$$\nP(S_M^c) = P(\\text{failure in trial 1 AND failure in trial 2 AND ... AND failure in trial } M)\n$$\nSince the trials are independent, the probability of the joint event is the product of the individual probabilities:\n$$\nP(S_M^c) = \\prod_{i=1}^{M} P(\\text{failure in trial } i) = (1 - p(n))^M\n$$\nThe probability of at least one success is then:\n$$\nP(S_M) = 1 - P(S_M^c) = 1 - (1 - p(n))^M\n$$\nThe problem requires this probability to be at least $1 - \\delta$:\n$$\n1 - (1 - p(n))^M \\ge 1 - \\delta\n$$\nWe now solve this inequality for $M$.\n$$\n- (1 - p(n))^M \\ge - \\delta\n$$\nMultiplying by $-1$ reverses the inequality sign:\n$$\n(1 - p(n))^M \\le \\delta\n$$\nTo isolate the exponent $M$, we take the natural logarithm of both sides. Since both $1-p(n)$ and $\\delta$ are in the interval $(0,1)$, their logarithms are well-defined and negative.\n$$\n\\ln\\left((1 - p(n))^M\\right) \\le \\ln(\\delta)\n$$\n$$\nM \\ln(1 - p(n)) \\le \\ln(\\delta)\n$$\nTo solve for $M$, we divide by $\\ln(1 - p(n))$. Since $p(n) \\in (0,1)$, we have $1-p(n) \\in (0,1)$, which implies $\\ln(1 - p(n)) < 0$. Dividing by a negative number reverses the inequality sign again:\n$$\nM \\ge \\frac{\\ln(\\delta)}{\\ln(1 - p(n))}\n$$\nThe number of restarts, $M$, must be an integer. The minimal integer $M$ that satisfies this inequality is the ceiling of the right-hand side. The ceiling function $\\lceil z \\rceil$ gives the smallest integer greater than or equal to $z$.\nTherefore, the minimal number of restarts $M(n,\\delta)$ is:\n$$\nM(n,\\delta) = \\left\\lceil \\frac{\\ln(\\delta)}{\\ln(1 - p(n))} \\right\\rceil\n$$\nFinally, we substitute the expression for $p(n) = (10.24)^{-n}$ derived in the first part:\n$$\nM(n,\\delta) = \\left\\lceil \\frac{\\ln(\\delta)}{\\ln(1 - (10.24)^{-n})} \\right\\rceil\n$$\nThis is the closed-form analytic expression for the minimal number of restarts required.", "answer": "$$\\boxed{\\left\\lceil \\frac{\\ln(\\delta)}{\\ln(1 - (10.24)^{-n})} \\right\\rceil}$$", "id": "3145569"}, {"introduction": "Simple global search can be prohibitively expensive, motivating the development of hybrid strategies that intelligently blend exploration and exploitation. This final practice [@problem_id:3145524] challenges you to implement a sophisticated optimizer that uses local curvature information to decide its next move. You will create a method that switches between a fast local Newton step in convex regions and a global \"escape\" step along directions of negative curvature, demonstrating a core principle of modern non-convex optimization.", "problem": "Consider unconstrained minimization of a twice continuously differentiable objective function $f:\\mathbb{R}^n \\to \\mathbb{R}$. The method of local search typically relies on local curvature information, while global search emphasizes exploration to avoid getting trapped at non-minimizing stationary points. Starting from the fundamental definitions of the gradient $\\nabla f(x)$ and the Hessian $\\nabla^2 f(x)$, propose a principled global–local switching criterion that uses the smallest eigenvalue of the Hessian, denoted $\\lambda_{\\min}(\\nabla^2 f(x))$, together with the gradient norm $\\lVert \\nabla f(x) \\rVert$. Then implement this criterion in a hybrid global–local search for a toy nonconvex example.\n\nThe toy nonconvex objective is two-dimensional and defined by\n$$\nf(x,y) = (x^2 - 1)^2 + (y^2 - 1)^2 + \\tfrac{1}{2} x y .\n$$\nThis objective has multiple stationary points including non-minimizers. The hybrid method must use the following scientifically motivated switching criterion:\n- Enter a local Newton-type refinement regime if $\\lVert \\nabla f(x) \\rVert \\le g_{\\text{tol}}$ and $\\lambda_{\\min}(\\nabla^2 f(x)) \\ge \\kappa_{\\text{pos}}$ (indicating small first-order term and sufficiently positive curvature).\n- Enter a global escape regime whenever $\\lambda_{\\min}(\\nabla^2 f(x)) < -\\kappa_{\\text{neg}}$ (indicating the presence of a direction of negative curvature) or whenever $\\lVert \\nabla f(x) \\rVert \\le g_{\\text{tol}}$ and $\\lambda_{\\min}(\\nabla^2 f(x)) \\le \\kappa_{\\text{pos}}$ (indicating near-stationarity without sufficient positive curvature, as in a saddle or flat region).\n\nIn the local regime, use a damped Newton step defined by solving $\\nabla^2 f(x)\\,p = -\\nabla f(x)$ followed by backtracking based on the Armijo decrease condition. In the global regime, move along the unit eigenvector $v_{\\min}$ corresponding to $\\lambda_{\\min}(\\nabla^2 f(x))$ with a trust-region radius and decrease-only acceptance. Outside these regimes, use a gradient descent step with Armijo backtracking. Use fixed thresholds $g_{\\text{tol}} = 10^{-3}$, $\\kappa_{\\text{pos}} = 10^{-3}$, $\\kappa_{\\text{neg}} = 10^{-8}$, a trust-region radius $r = 5\\times 10^{-1}$, and a maximum of $N = 200$ iterations per run.\n\nYour program must implement the above hybrid method on the specified objective and produce results for the following test suite of initial points:\n- $x_0 = (0,0)$,\n- $x_0 = (1,1)$,\n- $x_0 = (0,1)$,\n- $x_0 = (3,-3)$,\n- $x_0 = (0.57735,\\,0.57735)$.\n\nFor each initial point, run the optimizer for $N$ iterations and return the final objective value $f(x_N)$ as a floating-point number. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, i.e., $[r_1,r_2,r_3,r_4,r_5]$, where each $r_i$ is the final objective value for the corresponding test case, in the order listed above. No physical units or angles are involved, and the outputs must be pure real numbers without additional text.", "solution": "The hybrid criterion follows from the second-order Taylor model around a point $x \\in \\mathbb{R}^n$:\n$$\nf(x+p) \\approx f(x) + \\nabla f(x)^\\top p + \\tfrac{1}{2} p^\\top \\nabla^2 f(x)\\, p .\n$$\nThe fundamental facts are as follows. A stationary point satisfies $\\nabla f(x) = 0$. If additionally $\\nabla^2 f(x)$ is positive definite, then the quadratic term is strictly positive for any nonzero $p$, indicating that $x$ is a strict local minimizer. If $\\nabla^2 f(x)$ has at least one negative eigenvalue, then there exists a direction of negative curvature along which the quadratic term is strictly negative, implying that $x$ cannot be a local minimum and that a descent step can be realized by moving along that direction. Therefore, it is principled to use both the gradient norm $\\lVert \\nabla f(x) \\rVert$ and the smallest eigenvalue $\\lambda_{\\min}(\\nabla^2 f(x))$ to decide between local refinement and global escape.\n\nWe now define the switching criterion. Let $g_{\\text{tol}}$, $\\kappa_{\\text{pos}}$, and $\\kappa_{\\text{neg}}$ be small positive thresholds. If $\\lVert \\nabla f(x) \\rVert \\le g_{\\text{tol}}$ and $\\lambda_{\\min}(\\nabla^2 f(x)) \\ge \\kappa_{\\text{pos}}$, the second-order model indicates that $x$ is in a locally convex region near a candidate minimizer; this justifies entering a local Newton-type regime. Conversely, if $\\lambda_{\\min}(\\nabla^2 f(x)) < -\\kappa_{\\text{neg}}$, or if $\\lVert \\nabla f(x) \\rVert \\le g_{\\text{tol}}$ and $\\lambda_{\\min}(\\nabla^2 f(x)) \\le \\kappa_{\\text{pos}}$, then either a direction of negative curvature exists or the point is near-stationary without sufficient positive curvature; this justifies entering a global escape regime oriented by the negative curvature direction. Outside these cases, using a gradient descent step is reasonable, since large gradients dominate the model and a simple descent direction $p = -\\nabla f(x)$ effectively reduces the first-order term.\n\nThe local step is based on Newton's method: solve $\\nabla^2 f(x)\\, p = -\\nabla f(x)$ for $p$, and then apply backtracking line search to ensure sufficient decrease. The Armijo condition requires the existence of a step size $\\alpha$ such that\n$$\nf(x + \\alpha p) \\le f(x) + c_1 \\alpha \\nabla f(x)^\\top p,\n$$\nwith a small constant $c_1 \\in (0,1)$, commonly $c_1 = 10^{-4}$. This condition is grounded in first principles: it guarantees a decrease at least proportional to the predicted linear reduction. If the linear system is ill-conditioned, fallback to gradient descent is used.\n\nThe global escape step exploits negative curvature. Let $v_{\\min}$ be a unit eigenvector associated with $\\lambda_{\\min}(\\nabla^2 f(x))$. For any nonzero scalar $t$, the second-order term along $p = t v_{\\min}$ is $\\tfrac{1}{2} t^2 \\lambda_{\\min}(\\nabla^2 f(x))$, which is strictly negative if $\\lambda_{\\min}(\\nabla^2 f(x)) < 0$. This ensures that sufficiently small steps along $\\pm v_{\\min}$ reduce the quadratic model. To bias the decrease with respect to the linear term, choose the direction $d = -\\operatorname{sign}(\\nabla f(x)^\\top v_{\\min})\\, v_{\\min}$ and perform a backtracking search on the step length $s$ within a trust-region radius $r$ to guarantee actual decrease, i.e., accept $x_{\\text{new}} = x + s d$ only if $f(x_{\\text{new}}) < f(x)$. This is a safe decrease-only acceptance rule consistent with the model and the nonconvexity.\n\nWe now specify the toy example. The objective is\n$$\nf(x,y) = (x^2 - 1)^2 + (y^2 - 1)^2 + \\tfrac{1}{2} x y ,\n$$\nwith gradient\n$$\n\\nabla f(x,y) = \\begin{bmatrix} 4x(x^2 - 1) + \\tfrac{1}{2} y \\\\ 4y(y^2 - 1) + \\tfrac{1}{2} x \\end{bmatrix}\n$$\nand Hessian\n$$\n\\nabla^2 f(x,y) = \\begin{bmatrix} 12x^2 - 4 & \\tfrac{1}{2} \\\\ \\tfrac{1}{2} & 12y^2 - 4 \\end{bmatrix} .\n$$\nThe eigenstructure of $\\nabla^2 f(x,y)$ varies across the domain, producing regions of positive and negative curvature. The hybrid algorithm is parameterized by $g_{\\text{tol}} = 10^{-3}$, $\\kappa_{\\text{pos}} = 10^{-3}$, $\\kappa_{\\text{neg}} = 10^{-8}$, trust-region radius $r = 5 \\times 10^{-1}$, Armijo parameter $c_1 = 10^{-4}$, backtracking factor $\\beta = 1/2$, and maximum iterations $N = 200$.\n\nAlgorithmic design:\n- Evaluate $\\nabla f(x)$ and $\\nabla^2 f(x)$.\n- Compute $\\lambda_{\\min}(\\nabla^2 f(x))$ via symmetric eigendecomposition and obtain $v_{\\min}$.\n- If $\\lVert \\nabla f(x) \\rVert \\le g_{\\text{tol}}$ and $\\lambda_{\\min}(\\nabla^2 f(x)) \\ge \\kappa_{\\text{pos}}$, take a damped Newton step with Armijo backtracking.\n- Else if $\\lambda_{\\min}(\\nabla^2 f(x)) < -\\kappa_{\\text{neg}}$ or $\\lVert \\nabla f(x) \\rVert \\le g_{\\text{tol}}$ with $\\lambda_{\\min}(\\nabla^2 f(x)) \\le \\kappa_{\\text{pos}}$, take a negative-curvature escape step along $v_{\\min}$ with backtracking decrease-only acceptance within radius $r$.\n- Else, take a gradient descent step with Armijo backtracking.\n\nThis principled switching is grounded in the second-order Taylor model and the classification of stationary points by the Hessian. The test suite consists of initial points chosen to exercise different regimes: $(0,0)$ is a stationary point with negative curvature; $(1,1)$ lies near a local minimizer with positive curvature but nonzero gradient; $(0,1)$ is near a saddle-like region with mixed curvature; $(3,-3)$ is far from a minimizer with large gradients; $(0.57735,\\,0.57735)$ is near a boundary of curvature sign change. For each case, after $N$ iterations the algorithm returns the final objective value $f(x_N)$ as a real number. The program outputs these five results in the specified single-line list format $[r_1,r_2,r_3,r_4,r_5]$, in the order of the test cases.", "answer": "```python\nimport numpy as np\n\n# Hybrid global–local optimizer implementing curvature- and gradient-based switching\n# for the toy nonconvex function:\n#     f(x,y) = (x^2 - 1)^2 + (y^2 - 1)^2 + 0.5 * x * y\n\ndef f_val(xy: np.ndarray) -> float:\n    x, y = xy\n    return (x**2 - 1)**2 + (y**2 - 1)**2 + 0.5 * x * y\n\ndef grad_f(xy: np.ndarray) -> np.ndarray:\n    x, y = xy\n    gx = 4.0 * x * (x**2 - 1.0) + 0.5 * y\n    gy = 4.0 * y * (y**2 - 1.0) + 0.5 * x\n    return np.array([gx, gy])\n\ndef hess_f(xy: np.ndarray) -> np.ndarray:\n    x, y = xy\n    hxx = 12.0 * x**2 - 4.0\n    hyy = 12.0 * y**2 - 4.0\n    hxy = 0.5\n    return np.array([[hxx, hxy],\n                     [hxy, hyy]])\n\ndef armijo_line_search(x: np.ndarray, f_x: float, g_x: np.ndarray, p: np.ndarray,\n                       c1: float = 1e-4, alpha0: float = 1.0, beta: float = 0.5,\n                       max_backtracks: int = 50):\n    \"\"\"Armijo backtracking line search. Returns (x_new, f_new, alpha).\"\"\"\n    alpha = alpha0\n    gTp = float(np.dot(g_x, p))\n    for _ in range(max_backtracks):\n        x_new = x + alpha * p\n        f_new = f_val(x_new)\n        if f_new <= f_x + c1 * alpha * gTp:\n            return x_new, f_new, alpha\n        alpha *= beta\n    # If no acceptable step found, return original point\n    return x, f_x, 0.0\n\ndef neg_curvature_escape(x: np.ndarray, f_x: float, g_x: np.ndarray, H_x: np.ndarray,\n                         tr_radius: float = 0.5, beta: float = 0.5, max_backtracks: int = 50):\n    \"\"\"Step along the most negative curvature direction within a trust region radius,\n    accepting only if f decreases. Returns (x_new, f_new, step_len).\"\"\"\n    # Symmetric eigendecomposition\n    evals, evecs = np.linalg.eigh(H_x)\n    idx_min = int(np.argmin(evals))\n    vmin = evecs[:, idx_min]\n    # Bias direction by linear term to favor decrease\n    sign = -1.0 if float(np.dot(g_x, vmin)) >= 0.0 else 1.0\n    d = sign * vmin  # unit direction (since evecs are normalized)\n    s = tr_radius\n    for _ in range(max_backtracks):\n        x_new = x + s * d\n        f_new = f_val(x_new)\n        if f_new < f_x:\n            return x_new, f_new, s\n        s *= beta\n    return x, f_x, 0.0\n\ndef hybrid_optimize(x0: np.ndarray,\n                    max_iters: int = 200,\n                    g_tol: float = 1e-3,\n                    kappa_pos: float = 1e-3,\n                    kappa_neg: float = 1e-8,\n                    tr_radius: float = 0.5,\n                    c1: float = 1e-4,\n                    beta: float = 0.5) -> float:\n    \"\"\"Run the hybrid global–local optimizer from x0 and return final objective value.\"\"\"\n    x = x0.copy().astype(float)\n    for _ in range(max_iters):\n        f_x = f_val(x)\n        g_x = grad_f(x)\n        H_x = hess_f(x)\n        grad_norm = float(np.linalg.norm(g_x))\n        # Compute smallest eigenvalue of Hessian\n        evals = np.linalg.eigvalsh(H_x)\n        lam_min = float(evals[0])\n\n        # Decide regime and take a step\n        if grad_norm <= g_tol and lam_min >= kappa_pos:\n            # Local Newton-type refinement\n            try:\n                # Newton direction\n                p = np.linalg.solve(H_x, -g_x)\n            except np.linalg.LinAlgError:\n                # Fallback to gradient descent if Hessian is singular\n                p = -g_x\n            # Armijo backtracking\n            x_new, f_new, alpha = armijo_line_search(x, f_x, g_x, p, c1=c1, alpha0=1.0, beta=beta)\n            if alpha == 0.0:\n                # If no progress, break early\n                break\n            x = x_new\n        elif (lam_min < -kappa_neg) or (grad_norm <= g_tol and lam_min <= kappa_pos):\n            # Global escape via negative curvature direction\n            x_new, f_new, s = neg_curvature_escape(x, f_x, g_x, H_x, tr_radius=tr_radius, beta=beta)\n            if s == 0.0:\n                # If escape step fails to decrease, fallback to gradient descent\n                p = -g_x\n                x_new, f_new, alpha = armijo_line_search(x, f_x, g_x, p, c1=c1, alpha0=1.0, beta=beta)\n                if alpha == 0.0:\n                    break\n            x = x_new\n        else:\n            # Default local descent\n            p = -g_x\n            x_new, f_new, alpha = armijo_line_search(x, f_x, g_x, p, c1=c1, alpha0=1.0, beta=beta)\n            if alpha == 0.0:\n                break\n            x = x_new\n    return f_val(x)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        np.array([0.0, 0.0]),\n        np.array([1.0, 1.0]),\n        np.array([0.0, 1.0]),\n        np.array([3.0, -3.0]),\n        np.array([0.57735, 0.57735]),\n    ]\n\n    results = []\n    for x0 in test_cases:\n        final_f = hybrid_optimize(x0,\n                                  max_iters=200,\n                                  g_tol=1e-3,\n                                  kappa_pos=1e-3,\n                                  kappa_neg=1e-8,\n                                  tr_radius=0.5,\n                                  c1=1e-4,\n                                  beta=0.5)\n        results.append(final_f)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3145524"}]}