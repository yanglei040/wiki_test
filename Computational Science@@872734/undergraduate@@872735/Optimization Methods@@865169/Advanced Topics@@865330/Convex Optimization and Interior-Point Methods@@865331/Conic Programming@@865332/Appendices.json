{"hands_on_practices": [{"introduction": "The trust-region subproblem is a cornerstone of many powerful optimization algorithms. This exercise challenges you to translate this classic quadratically constrained quadratic program into the language of conic programming. By reformulating the objective and constraints into a standard Second-Order Cone Program (SOCP), you will practice fundamental techniques like using epigraph variables and rotated cones, gaining a deeper appreciation for the expressive power of conic forms [@problem_id:3111118].", "problem": "Consider the quadratic trust-region subproblem\n$$\n\\min_{x \\in \\mathbb{R}^{2}} \\;\\; \\frac{1}{2} x^{\\top} Q x + c^{\\top} x \\quad \\text{subject to} \\quad \\|x\\|_{2} \\leq \\delta,\n$$\nwhere $Q \\in \\mathbb{R}^{2 \\times 2}$ is symmetric positive definite, $c \\in \\mathbb{R}^{2}$, and $\\delta > 0$. Let\n$$\nQ = \\begin{pmatrix} 2 & 0 \\\\ 0 & 1 \\end{pmatrix}, \\quad c = \\begin{pmatrix} -3 \\\\ -4 \\end{pmatrix}, \\quad \\delta = \\sqrt{5}.\n$$\nYour tasks are:\n\n1) Starting only from core definitions of convexity, the definition of a second-order cone, and epigraph transformations, recast the problem as an equivalent Second-Order Cone Program (SOCP). Introduce an epigraph variable to linearize the quadratic objective, and express all constraints using the Euclidean second-order cone and, where appropriate, the rotated second-order cone. Present the SOCP in a canonical conic form with linear objective and conic constraints only.\n\n2) Independently derive the optimizer using the eigenvalue-based closed-form approach that follows from the Karush–Kuhn–Tucker (KKT) conditions for the trust-region problem. Use the stationarity condition to relate the optimizer and the Lagrange multiplier associated with the trust-region constraint, determine whether the trust-region is active, and solve for the multiplier. Then determine the corresponding optimizer.\n\n3) Compare the canonical conic formulation from part 1) with the eigenvalue-based optimizer from part 2) by verifying they are consistent. Finally, compute the optimal objective value\n$$\nf^{\\star} \\triangleq \\min_{\\|x\\|_{2} \\leq \\delta} \\left\\{ \\frac{1}{2} x^{\\top} Q x + c^{\\top} x \\right\\}.\n$$\nReport $f^{\\star}$ as a single real number. No rounding is required.", "solution": "The problem is first validated to ensure it is well-posed and scientifically sound.\n\n### Step 1: Extract Givens\nThe problem is a quadratic trust-region subproblem:\n$$\n\\min_{x \\in \\mathbb{R}^{2}} \\;\\; f(x) = \\frac{1}{2} x^{\\top} Q x + c^{\\top} x \\quad \\text{subject to} \\quad \\|x\\|_{2} \\leq \\delta\n$$\nThe given data are:\n-   The matrix $Q = \\begin{pmatrix} 2 & 0 \\\\ 0 & 1 \\end{pmatrix}$. This matrix is symmetric and its eigenvalues are $2$ and $1$, so it is positive definite.\n-   The vector $c = \\begin{pmatrix} -3 \\\\ -4 \\end{pmatrix}$.\n-   The trust-region radius $\\delta = \\sqrt{5}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is a standard trust-region subproblem, a fundamental topic in optimization. The objective function is a strictly convex quadratic function (since $Q$ is positive definite), and the feasible set is a closed and bounded (compact) convex set (a Euclidean ball). Therefore, a unique minimizer is guaranteed to exist. All parameters are well-defined, and the problem statement is self-contained and unambiguous. The problem is scientifically grounded, well-posed, and objective.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n---\n\nThe solution proceeds in three parts as requested.\n\n### Part 1: Recasting as a Second-Order Cone Program (SOCP)\n\nThe original problem is:\n$$\n\\min_{x \\in \\mathbb{R}^{2}} \\;\\; \\frac{1}{2} x^{\\top} Q x + c^{\\top} x \\quad \\text{s.t.} \\quad \\|x\\|_{2} \\leq \\delta\n$$\nWe introduce an epigraph variable $t \\in \\mathbb{R}$ to represent the value of the objective function. The problem is equivalent to:\n$$\n\\min_{x \\in \\mathbb{R}^{2}, t \\in \\mathbb{R}} \\;\\; t \\quad \\text{s.t.} \\quad \\frac{1}{2} x^{\\top} Q x + c^{\\top} x \\leq t \\quad \\text{and} \\quad \\|x\\|_{2} \\leq \\delta\n$$\nThe objective is now linear. We must express the constraints in conic form.\n\nThe trust-region constraint $\\|x\\|_{2} \\leq \\delta$ is a standard second-order cone constraint. The second-order cone of dimension $n+1$ is defined as $\\mathcal{K}_s^{n+1} = \\{ (y_0, y_1, \\dots, y_n) \\in \\mathbb{R}^{n+1} \\mid y_0 \\ge \\|(y_1, \\dots, y_n)\\|_2 \\}$. For $x \\in \\mathbb{R}^2$, this constraint can be written as:\n$$ \\begin{pmatrix} \\delta \\\\ x \\end{pmatrix} \\in \\mathcal{K}_s^3 $$\nwhere $x=(x_1, x_2)^{\\top}$ and $\\delta = \\sqrt{5}$.\n\nThe quadratic constraint is $\\frac{1}{2} x^{\\top} Q x + c^{\\top} x \\leq t$. Since $Q$ is symmetric positive definite, we can find a matrix $A$ such that $Q = A^{\\top}A$. Here, $Q$ is diagonal, so we can choose $A = Q^{1/2}$.\n$$\nA = Q^{1/2} = \\begin{pmatrix} \\sqrt{2} & 0 \\\\ 0 & 1 \\end{pmatrix}\n$$\nThe constraint becomes $\\frac{1}{2}\\|Ax\\|_2^2 + c^{\\top}x \\leq t$, which can be rearranged to:\n$$\n\\|Ax\\|_2^2 \\leq 2(t - c^{\\top}x)\n$$\nThis inequality has the structure of a rotated second-order cone constraint. A rotated second-order cone of dimension $k+2$ is defined as $\\mathcal{K}_r^{k+2} = \\{ (u, v, y_1, \\dots, y_k) \\in \\mathbb{R}^{k+2} \\mid 2uv \\ge \\|y\\|_2^2, u \\ge 0, v \\ge 0 \\}$.\nTo put our inequality into this form, we can let $y = Ax$, $u=1$, and introduce a new variable $v = t - c^{\\top}x$. The inequality becomes $\\|y\\|_2^2 \\leq 2v$, with $u=1 \\ge 0$. The condition $v \\ge 0$ is implicitly enforced by the cone definition.\nThis gives the conic constraint:\n$$\n\\begin{pmatrix} 1 \\\\ v \\\\ Ax \\end{pmatrix} \\in \\mathcal{K}_r^4\n$$\nWith the substitution $v = t - c^{\\top}x$, we have $t = v + c^{\\top}x$. We can substitute this into the objective function. The problem becomes:\n$$\n\\min_{x \\in \\mathbb{R}^2, v \\in \\mathbb{R}} \\;\\; c^{\\top}x + v\n$$\nThe decision variables for the SOCP are $x_1, x_2,$ and $v$. The final SOCP formulation in canonical form is:\n$$\n\\min_{x_1, x_2, v} \\;\\; -3x_1 - 4x_2 + v\n$$\nsubject to the two conic constraints:\n$$\n\\begin{pmatrix} \\sqrt{5} \\\\ x_1 \\\\ x_2 \\end{pmatrix} \\in \\mathcal{K}_s^3\n$$\n$$\n\\begin{pmatrix} 1 \\\\ v \\\\ \\sqrt{2}x_1 \\\\ x_2 \\end{pmatrix} \\in \\mathcal{K}_r^4\n$$\n\n### Part 2: Eigenvalue-based Solution via KKT Conditions\n\nThe Lagrangian for the trust-region subproblem is:\n$$ L(x, \\lambda) = \\frac{1}{2} x^{\\top} Q x + c^{\\top} x + \\frac{\\lambda}{2} (x^{\\top}x - \\delta^2) $$\nThe Karush-Kuhn-Tucker (KKT) conditions for an optimal solution $x^\\star$ and Lagrange multiplier $\\lambda^\\star$ are:\n1.  **Stationarity:** $\\nabla_x L(x^\\star, \\lambda^\\star) = Qx^\\star + c + \\lambda^\\star x^\\star = (Q + \\lambda^\\star I)x^\\star + c = 0$.\n2.  **Primal Feasibility:** $\\|x^\\star\\|_2^2 \\leq \\delta^2$.\n3.  **Dual Feasibility:** $\\lambda^\\star \\ge 0$.\n4.  **Complementary Slackness:** $\\lambda^\\star (\\|x^\\star\\|_2^2 - \\delta^2) = 0$.\n\nFirst, consider the case where the solution is in the interior of the trust region, i.e., $\\|x^\\star\\|_2 < \\delta$. By complementary slackness, this implies $\\lambda^\\star = 0$. The stationarity condition simplifies to $Qx^\\star = -c$. The unconstrained minimizer is $x_{unc} = -Q^{-1}c$.\nGiven $Q=\\begin{pmatrix} 2 & 0 \\\\ 0 & 1 \\end{pmatrix}$, its inverse is $Q^{-1} = \\begin{pmatrix} 1/2 & 0 \\\\ 0 & 1 \\end{pmatrix}$.\n$$ x_{unc} = -\\begin{pmatrix} 1/2 & 0 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} -3 \\\\ -4 \\end{pmatrix} = \\begin{pmatrix} 3/2 \\\\ 4 \\end{pmatrix} $$\nWe check if this solution is feasible:\n$$ \\|x_{unc}\\|_2^2 = (3/2)^2 + 4^2 = \\frac{9}{4} + 16 = \\frac{73}{4} = 18.25 $$\nThe squared radius is $\\delta^2 = (\\sqrt{5})^2 = 5$. Since $18.25 > 5$, the unconstrained solution is not feasible. This means the optimal solution must lie on the boundary of the trust region.\n\nThus, the constraint is active: $\\|x^\\star\\|_2^2 = \\delta^2$. By complementary slackness, this requires $\\lambda^\\star > 0$ (since if $\\lambda^\\star=0$, we get the infeasible $x_{unc}$).\nFrom the stationarity condition, we have $(Q + \\lambda I)x = -c$. The eigenvalues of $Q$ are $2$ and $1$. Since $\\lambda > 0$, the matrix $Q+\\lambda I$ is positive definite and thus invertible.\n$$ x(\\lambda) = -(Q+\\lambda I)^{-1}c $$\n$$ Q+\\lambda I = \\begin{pmatrix} 2+\\lambda & 0 \\\\ 0 & 1+\\lambda \\end{pmatrix} \\implies (Q+\\lambda I)^{-1} = \\begin{pmatrix} \\frac{1}{2+\\lambda} & 0 \\\\ 0 & \\frac{1}{1+\\lambda} \\end{pmatrix} $$\n$$ x(\\lambda) = -\\begin{pmatrix} \\frac{1}{2+\\lambda} & 0 \\\\ 0 & \\frac{1}{1+\\lambda} \\end{pmatrix} \\begin{pmatrix} -3 \\\\ -4 \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{2+\\lambda} \\\\ \\frac{4}{1+\\lambda} \\end{pmatrix} $$\nWe enforce the active constraint condition $\\|x(\\lambda)\\|_2^2 = \\delta^2$:\n$$ \\left(\\frac{3}{2+\\lambda}\\right)^2 + \\left(\\frac{4}{1+\\lambda}\\right)^2 = 5 $$\n$$ \\frac{9}{(2+\\lambda)^2} + \\frac{16}{(1+\\lambda)^2} = 5 $$\nBy inspection, we can test integer values for $\\lambda$. For $\\lambda=1$:\n$$ \\frac{9}{(2+1)^2} + \\frac{16}{(1+1)^2} = \\frac{9}{3^2} + \\frac{16}{2^2} = \\frac{9}{9} + \\frac{16}{4} = 1 + 4 = 5 $$\nSo, $\\lambda^\\star = 1$ is the unique positive solution to this secular equation.\nThe optimal solution $x^\\star$ is found by substituting $\\lambda^\\star=1$ back into the expression for $x(\\lambda)$:\n$$ x^\\star = \\begin{pmatrix} \\frac{3}{2+1} \\\\ \\frac{4}{1+1} \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} $$\n\n### Part 3: Comparison and Optimal Value Calculation\n\nWe verify that the optimizer $x^\\star = (1, 2)^{\\top}$ from Part 2 is consistent with the SOCP formulation from Part 1.\nFor $x^\\star = (1, 2)^{\\top}$, the first SOCP constraint is:\n$$ \\begin{pmatrix} \\sqrt{5} \\\\ 1 \\\\ 2 \\end{pmatrix} \\in \\mathcal{K}_s^3 \\iff \\sqrt{5} \\ge \\sqrt{1^2+2^2} = \\sqrt{5} $$\nThis constraint is satisfied (and active).\nThe second SOCP constraint involves the variable $v$:\n$$ \\begin{pmatrix} 1 \\\\ v \\\\ \\sqrt{2}(1) \\\\ 2 \\end{pmatrix} \\in \\mathcal{K}_r^4 \\iff 2(1)v \\ge (\\sqrt{2})^2 + 2^2 \\implies 2v \\ge 2+4=6 \\implies v \\ge 3 $$\nThe SOCP seeks to minimize the objective function $c^{\\top}x + v = -3x_1 - 4x_2 + v$. For the fixed optimal $x^\\star=(1,2)^{\\top}$, this becomes:\n$$ \\min_{v \\ge 3} \\;\\; -3(1) - 4(2) + v = \\min_{v \\ge 3} \\;\\; -11 + v $$\nThe minimum is achieved at the smallest possible value of $v$, which is $v=3$.\nThe optimal value of the SOCP objective is $-11+3 = -8$.\n\nFinally, we compute the optimal objective value $f^\\star$ of the original problem using the optimizer $x^\\star = (1, 2)^{\\top}$:\n$$ f^\\star = f(x^\\star) = \\frac{1}{2} x^{\\star\\top} Q x^\\star + c^{\\top} x^\\star $$\n$$ c^{\\top}x^\\star = \\begin{pmatrix} -3 & -4 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = -3 - 8 = -11 $$\n$$ x^{\\star\\top} Q x^\\star = \\begin{pmatrix} 1 & 2 \\end{pmatrix} \\begin{pmatrix} 2 & 0 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 1 & 2 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix} = 1(2) + 2(2) = 6 $$\n$$ f^\\star = \\frac{1}{2}(6) + (-11) = 3 - 11 = -8 $$\nThe optimal value $f^\\star = -8$ matches the optimal value of the SOCP objective. The two approaches yield consistent results. The optimal objective value is $-8$.", "answer": "$$\n\\boxed{-8}\n$$", "id": "3111118"}, {"introduction": "Minimizing a norm subject to constraints is a frequent task in fields like machine learning and signal processing, often related to finding a simple model that fits data well. This practice moves beyond basic formulation and into the powerful world of duality. You will not only model the problem as an SOCP but also derive its Lagrange dual, revealing a beautiful connection between the dual variables and the geometric concept of a support function [@problem_id:3111130].", "problem": "Consider the norm minimization problem with an additional $\\ell_{1}$-type constraint\n$$\\min_{x \\in \\mathbb{R}^{2}} \\|F x - g\\|_{2} \\quad \\text{subject to} \\quad \\|x\\|_{1} \\leq \\tau,$$\nwhere $F \\in \\mathbb{R}^{2 \\times 2}$, $g \\in \\mathbb{R}^{2}$, and $\\tau \\in \\mathbb{R}_{+}$. Take the specific instance\n$$F = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix}, \\quad g = \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix}, \\quad \\tau = 1.$$\nStarting from the definitions of the Euclidean norm, the $\\ell_{1}$-norm, the $\\ell_{\\infty}$-norm, the second-order cone, the support function of a convex set, and convex conjugates, carry out the following:\n\n$1.$ Reformulate the problem as a Second-Order Cone Program (SOCP), where Second-Order Cone Program (SOCP) refers to a convex optimization problem with linear objectives and constraints consisting of linear equalities/inequalities and second-order (quadratic) cone memberships. Explicitly introduce all auxiliary variables and write the conic and linear constraints for the given instance.\n\n$2.$ Derive the Lagrange dual, in general form for the problem $\\min_{x} \\|F x - g\\|_{2}$ subject to $\\|x\\|_{1} \\leq \\tau$, using convex conjugates and support functions. Interpret the dual objective in terms of support functions of norm balls and linear images.\n\n$3.$ For the given data $F$, $g$, and $\\tau$, compute the optimal value of the primal problem exactly. Express the final answer as a single closed-form expression. No rounding is required.\n\nYour final answer must be a single real-valued number or a single closed-form analytic expression without any inequalities or units.", "solution": "The problem as stated constitutes a valid, well-posed convex optimization problem. It is mathematically and logically sound, with all necessary information provided. We may therefore proceed with a full solution. The solution is presented in three parts as requested.\n\nThe definitions central to this problem are:\n- The Euclidean or $\\ell_2$-norm of a vector $z \\in \\mathbb{R}^m$ is $\\|z\\|_2 = \\sqrt{\\sum_{i=1}^m z_i^2}$.\n- The $\\ell_1$-norm of a vector $x \\in \\mathbb{R}^n$ is $\\|x\\|_1 = \\sum_{i=1}^n |x_i|$.\n- The $\\ell_\\infty$-norm of a vector $v \\in \\mathbb{R}^n$ is $\\|v\\|_\\infty = \\max_{1 \\le i \\le n} |v_i|$.\n- The second-order cone in $\\mathbb{R}^{m+1}$ is $\\mathcal{K}_m = \\{(y, t) \\in \\mathbb{R}^m \\times \\mathbb{R} \\mid \\|y\\|_2 \\le t\\}$.\n- The support function of a convex set $C$ is $\\sigma_C(y) = \\sup_{z \\in C} y^T z$.\n- The convex conjugate of a function $f(x)$ is $f^*(y) = \\sup_x (y^T x - f(x))$.\n\n**Part 1: Reformulation as a Second-Order Cone Program (SOCP)**\n\nThe primal problem for the given instance is:\n$$\n\\min_{x \\in \\mathbb{R}^{2}} \\|F x - g\\|_{2} \\quad \\text{subject to} \\quad \\|x\\|_{1} \\leq 1,\n$$\nwith $F = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix}$ and $g = \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix}$. Let $x = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$.\n\nThe objective is to minimize $\\|Fx - g\\|_2$. We introduce a new scalar variable $t$ and reformulate the problem as minimizing $t$ subject to the constraint $\\|Fx - g\\|_2 \\leq t$. This is a second-order cone constraint. For the given data, $Fx - g = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix}\\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} - \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix} = \\begin{pmatrix} x_1 - 3 \\\\ -4 \\end{pmatrix}$.\nThe constraint becomes $\\sqrt{(x_1-3)^2 + (-4)^2} \\le t$, which is equivalent to the conic constraint $\\left( \\begin{pmatrix} x_1 - 3 \\\\ -4 \\end{pmatrix}, t \\right) \\in \\mathcal{K}_2$.\n\nThe constraint is $\\|x\\|_1 \\leq 1$, which is $|x_1| + |x_2| \\leq 1$. To express this constraint using linear inequalities, we introduce two auxiliary variables $u_1$ and $u_2$. We impose the constraints $|x_1| \\leq u_1$ and $|x_2| \\leq u_2$. Then the original constraint becomes $u_1 + u_2 \\leq 1$. The absolute value inequalities are equivalent to pairs of linear inequalities:\n$|x_1| \\leq u_1 \\iff -u_1 \\leq x_1 \\leq u_1$.\n$|x_2| \\leq u_2 \\iff -u_2 \\leq x_2 \\leq u_2$.\nWe also require $u_1 \\ge 0$ and $u_2 \\ge 0$.\n\nCombining these, we formulate the problem as an SOCP. The optimization variables are $x_1, x_2, t, u_1, u_2$.\n\nObjective function:\n$$ \\min t $$\n\nConstraints:\n$1$. **Conic Constraint:**\nThe constraint $\\|Fx - g\\|_2 \\leq t$ is written as:\n$$ \\left( \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} - \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix}, t \\right) \\in \\mathcal{K}_2 $$\nwhich explicitly is $\\sqrt{(x_1-3)^2 + 16} \\leq t$.\n\n$2$. **Linear Constraints:**\nThe constraints for the $\\ell_1$-norm are:\n$$\n\\begin{cases}\nu_1 + u_2 \\leq 1 \\\\\nx_1 - u_1 \\leq 0 \\\\\n-x_1 - u_1 \\leq 0 \\\\\nx_2 - u_2 \\leq 0 \\\\\n-x_2 - u_2 \\leq 0 \\\\\nu_1 \\geq 0 \\\\\nu_2 \\geq 0\n\\end{cases}\n$$\n\nThe auxiliary variables introduced are $t \\in \\mathbb{R}$, $u_1 \\in \\mathbb{R}$, and $u_2 \\in \\mathbb{R}$.\n\n**Part 2: Derivation of the Lagrange Dual**\n\nWe derive the dual of the general problem $\\min_{x} \\|F x - g\\|_{2}$ subject to $\\|x\\|_{1} \\leq \\tau$. We introduce an auxiliary variable $y=Fx-g$ to write the problem as:\n$$ \\min_{x,y} \\|y\\|_2 \\quad \\text{subject to} \\quad y = Fx-g, \\quad \\|x\\|_1 \\leq \\tau $$\nThe Lagrangian $L(x, y; \\nu, \\lambda)$ is constructed with dual variables $\\nu$ for the equality constraint and $\\lambda \\ge 0$ for the inequality constraint:\n$$ L(x, y; \\nu, \\lambda) = \\|y\\|_2 + \\nu^T(Fx-g-y) + \\lambda(\\|x\\|_1 - \\tau) $$\nThe Lagrange dual function $q(\\nu, \\lambda)$ is the infimum of the Lagrangian over the primal variables $x$ and $y$:\n$$ q(\\nu, \\lambda) = \\inf_{x,y} L(x, y; \\nu, \\lambda) = \\inf_{y}(\\|y\\|_2 - \\nu^T y) + \\inf_{x}(\\nu^T Fx + \\lambda\\|x\\|_1) - \\nu^T g - \\lambda\\tau $$\nWe evaluate the two infima separately using the concept of convex conjugates.\n\nThe first term is $\\inf_y (\\|y\\|_2 - \\nu^T y) = - \\sup_y(\\nu^T y - \\|y\\|_2)$. The supremum is the convex conjugate of the $\\ell_2$-norm, denoted $f_2^*( \\nu)$ where $f_2(y) = \\|y\\|_2$. The conjugate of a norm is the indicator function of the unit ball of the dual norm. The $\\ell_2$-norm is self-dual. Therefore,\n$$ \\sup_y(\\nu^T y - \\|y\\|_2) = \\begin{cases} 0 & \\text{if } \\|\\nu\\|_2 \\le 1 \\\\ \\infty & \\text{otherwise} \\end{cases} $$\nSo, $\\inf_y (\\|y\\|_2 - \\nu^T y)$ is $0$ if $\\|\\nu\\|_2 \\le 1$ and $-\\infty$ otherwise. This implies the constraint $\\|\\nu\\|_2 \\le 1$ in the dual problem.\n\nThe second term is $\\inf_x(\\nu^T Fx + \\lambda\\|x\\|_1) = \\inf_x((F^T\\nu)^T x + \\lambda\\|x\\|_1)$. Assuming $\\lambda > 0$, this is $-\\lambda \\sup_x(- (F^T\\nu/\\lambda)^T x - \\|x\\|_1)$, which is not standard. Instead, we write it as $\\inf_x(\\lambda\\|x\\|_1 - (-F^T\\nu)^T x)$. This is $-h^*(-F^T\\nu)$, where $h(x)=\\lambda\\|x\\|_1$. The conjugate $h^*(z)=\\sup_x(z^Tx-\\lambda\\|x\\|_1)$ is $0$ if $\\|z\\|_\\infty \\le \\lambda$ and $\\infty$ otherwise. The dual of the $\\ell_1$-norm is the $\\ell_\\infty$-norm.\nSo, $\\inf_x(\\nu^T Fx + \\lambda\\|x\\|_1)$ is $0$ if $\\|-F^T\\nu\\|_\\infty \\le \\lambda$, which simplifies to $\\|F^T\\nu\\|_\\infty \\le \\lambda$, and $-\\infty$ otherwise. This condition also holds for $\\lambda=0$.\n\nThe dual function is non-infinite only when $\\|\\nu\\|_2 \\le 1$ and $\\|F^T\\nu\\|_\\infty \\le \\lambda$. In this case,\n$$ q(\\nu, \\lambda) = 0 + 0 - \\nu^T g - \\lambda\\tau = -g^T\\nu - \\lambda\\tau $$\nThe dual problem is to maximize $q(\\nu, \\lambda)$ subject to the derived constraints:\n$$ \\max_{\\nu, \\lambda} \\quad -g^T\\nu - \\lambda\\tau \\quad \\text{subject to} \\quad \\|\\nu\\|_2 \\le 1, \\quad \\|F^T\\nu\\|_\\infty \\le \\lambda, \\quad \\lambda \\ge 0 $$\nFor any fixed $\\nu$, the objective is maximized by choosing the smallest possible value for $\\lambda$, since $\\tau > 0$. The constraints require $\\lambda \\ge \\|F^T\\nu\\|_\\infty$ and $\\lambda \\ge 0$. As the norm is non-negative, the smallest valid choice is $\\lambda = \\|F^T\\nu\\|_\\infty$. Substituting this into the dual problem eliminates $\\lambda$:\n$$ d^* = \\max_{\\nu} \\quad -g^T\\nu - \\tau\\|F^T\\nu\\|_\\infty \\quad \\text{subject to} \\quad \\|\\nu\\|_2 \\le 1 $$\nTo interpret this in terms of support functions, we analyze the term $\\tau\\|F^T\\nu\\|_\\infty$. By definition of the dual norm, $\\|v\\|_\\infty = \\sup_{\\|u\\|_1 \\le 1} v^T u$.\n$$ \\tau\\|F^T\\nu\\|_\\infty = \\tau \\sup_{\\|u\\|_1 \\le 1} u^T F^T\\nu = \\sup_{\\|x\\|_1 \\le \\tau} x^T F^T\\nu = \\sup_{z \\in F(B_1(\\tau))} z^T \\nu $$\nwhere $B_1(\\tau)$ is the $\\ell_1$-ball of radius $\\tau$, i.e., $\\{x \\in \\mathbb{R}^n \\mid \\|x\\|_1 \\le \\tau\\}$. The expression $\\sup_{z \\in C} z^T \\nu$ is the definition of the support function of the set $C$, denoted $\\sigma_C(\\nu)$. Here, the set is $C_\\tau = F(B_1(\\tau))$, the image of the $\\ell_1$-ball under the linear map $F$.\nThus, the dual objective function can be written as $-g^T\\nu - \\sigma_{C_\\tau}(\\nu)$. The dual problem is\n$$ d^* = \\max_{\\|\\nu\\|_2 \\leq 1} \\left( -g^T\\nu - \\sigma_{F(B_1(\\tau))}(\\nu) \\right) $$\nThis has a geometric interpretation. The primal problem $\\min_{\\|x\\|_1 \\le \\tau} \\|Fx - g\\|_2$ is equivalent to finding the minimum distance between the point $g$ and the convex set $C_\\tau = F(B_1(\\tau))$, i.e., $\\min_{z \\in C_\\tau} \\|z-g\\|_2$. The derived dual problem is precisely the standard dual of this minimum distance problem.\n\n**Part 3: Computation of the Optimal Value**\n\nWe solve the specific primal problem:\n$$ \\min_{x_1, x_2} \\sqrt{(x_1 - 3)^2 + 16} \\quad \\text{subject to} \\quad |x_1| + |x_2| \\leq 1 $$\nThe objective function $J(x_1, x_2) = \\sqrt{(x_1 - 3)^2 + 16}$ depends only on the variable $x_1$. The optimization problem can be decoupled. We need to find the range of feasible values for $x_1$ and then minimize $J$ with respect to $x_1$ over that range.\n\nThe constraint is $|x_1| + |x_2| \\leq 1$. Since $|x_2| \\geq 0$, it must be that $|x_1| \\leq 1$. This implies that the feasible range for $x_1$ is the interval $[-1, 1]$. For any $x_1 \\in [-1, 1]$, we can choose a suitable $x_2$ (for example, $x_2=0$) such that the constraint $|x_1| + |x_2| \\leq 1$ is satisfied.\n\nTherefore, the problem reduces to a one-dimensional minimization:\n$$ \\min_{x_1 \\in [-1, 1]} \\sqrt{(x_1 - 3)^2 + 16} $$\nSince the square root function is strictly increasing, minimizing the expression is equivalent to minimizing its argument:\n$$ \\min_{x_1 \\in [-1, 1]} (x_1 - 3)^2 + 16 $$\nLet $h(x_1) = (x_1 - 3)^2$. This is a parabola with its vertex (global minimum) at $x_1 = 3$. The interval of interest is $[-1, 1]$. Since $x_1 = 3$ is outside this interval, the minimum of $h(x_1)$ over $[-1, 1]$ must occur at one of the endpoints. The function $h(x_1)$ is decreasing for $x_1 < 3$, which covers the entire interval $[-1, 1]$.\nThus, the minimum value of $h(x_1)$ on $[-1, 1]$ is achieved at the right endpoint, $x_1 = 1$.\n\nThe optimal value for $x_1$ is $x_1^* = 1$. Substituting this value back into the original objective function gives the optimal value of the problem:\n$$ p^* = \\sqrt{(1 - 3)^2 + 16} = \\sqrt{(-2)^2 + 16} = \\sqrt{4 + 16} = \\sqrt{20} $$\nThe optimal value can be simplified to $2\\sqrt{5}$.", "answer": "$$\\boxed{2\\sqrt{5}}$$", "id": "3111130"}, {"introduction": "How do we make optimal decisions when some parameters are uncertain? This is the central question of robust optimization. This exercise tackles a \"max-min\" problem, where we seek a decision that performs best in the worst-case scenario—a structure that is typically difficult to solve directly. You will see how the elegant machinery of conic duality allows us to transform this complex, nested problem into a single, solvable SOCP, showcasing a powerful technique for managing uncertainty [@problem_id:3111113].", "problem": "Consider the following robust optimization problem in the setting of conic programming. Let the uncertainty set for a scenario vector be ellipsoidal, given by $\\{\\bar{\\theta} + P z : \\|z\\|_{2} \\leq 1\\}$, where $\\bar{\\theta} \\in \\mathbb{R}^{2}$ and $P \\in \\mathbb{R}^{2 \\times 2}$. A decision-maker chooses $x \\in \\mathbb{R}^{2}$ subject to the trust-region constraint $\\|x\\|_{2} \\leq 1$. The robust objective is the worst-case linear payoff against the uncertainty:\n$$\n\\max_{\\|x\\|_{2} \\leq 1} \\ \\min_{\\|z\\|_{2} \\leq 1} \\ x^{\\top}(\\bar{\\theta} + P z).\n$$\nYou are given $\\bar{\\theta} = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}$ and $P = 2 I_{2}$, where $I_{2}$ denotes the $2 \\times 2$ identity matrix.\n\nUsing only the fundamental definitions of:\n- the Second-Order Cone (SOC), defined as $\\mathcal{Q}^{n+1} = \\{(t, w) \\in \\mathbb{R} \\times \\mathbb{R}^{n} : \\|w\\|_{2} \\leq t\\}$,\n- the conic primal-dual pair in standard form, where a primal is written as $\\min_{z} \\ c^{\\top}z$ subject to $A z + b \\in \\mathcal{K}$ with $\\mathcal{K}$ a closed convex cone, and its dual is $\\max_{y} \\ -b^{\\top} y$ subject to $A^{\\top} y + c = 0$, $y \\in \\mathcal{K}^{*}$,\n- Slater’s condition (strict feasibility) implying strong duality for conic programs,\n\nperform the following tasks:\n1. For a fixed $x$, formulate the inner minimization over $z$ as a conic program over the Second-Order Cone, derive its conic dual, and use strong duality to express the inner minimization value as a function of $x$ only.\n2. By swapping the $\\min$ and $\\max$ via strong duality, rewrite the original max-min robust objective as a single-level maximization involving $x$ and any dual variables necessary.\n3. Verify that Slater’s condition holds for both the inner primal and dual problems under the given data, thereby justifying the use of strong duality.\n4. Solve the resulting problem to determine the exact optimal robust value of the objective. Express your final answer as a single exact real number or a closed-form analytic expression. No rounding is required.", "solution": "The problem is a robust optimization problem given by\n$$\n\\max_{\\|x\\|_{2} \\leq 1} \\ \\min_{\\|z\\|_{2} \\leq 1} \\ x^{\\top}(\\bar{\\theta} + P z)\n$$\nwith parameters $\\bar{\\theta} = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}$ and $P = 2 I_{2}$, where $I_{2}$ is the $2 \\times 2$ identity matrix. We will address the tasks sequentially.\n\n**1. Inner Minimization as a Conic Program**\n\nFor a fixed decision vector $x \\in \\mathbb{R}^{2}$, the inner problem is\n$$\n\\min_{\\|z\\|_{2} \\leq 1} \\ x^{\\top}(\\bar{\\theta} + P z) = x^{\\top}\\bar{\\theta} + \\min_{\\|z\\|_{2} \\leq 1} x^{\\top} P z\n$$\nLet's focus on the minimization part: $\\min_{\\|z\\|_{2} \\leq 1} (P^{\\top}x)^{\\top}z$. This is an optimization problem over the variable $z \\in \\mathbb{R}^2$. The constraint $\\|z\\|_{2} \\leq 1$ can be expressed using the Second-Order Cone (SOC) $\\mathcal{Q}^3 = \\{(t, w) \\in \\mathbb{R} \\times \\mathbb{R}^{2} : \\|w\\|_{2} \\leq t\\}$ by setting $t=1$ and $w=z$, so $(1, z) \\in \\mathcal{Q}^3$.\n\nWe formulate this as a conic program in the specified primal form: $\\min_{\\tilde{z}} c^{\\top}\\tilde{z}$ subject to $A \\tilde{z} + b \\in \\mathcal{K}$.\nLet the optimization variable be $\\tilde{z} = z \\in \\mathbb{R}^2$.\nThe objective function is $(P^{\\top}x)^{\\top}z$, so we set the cost vector $c = P^{\\top}x$.\nThe constraint $(1, z) \\in \\mathcal{Q}^3$ must be written as $A z + b \\in \\mathcal{Q}^3$. We can define $A$ and $b$ as follows:\n$$\nA = \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\\\ 0 & 1 \\end{pmatrix} \\in \\mathbb{R}^{3 \\times 2}, \\quad b = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} \\in \\mathbb{R}^3\n$$\nWith these choices, $A z + b = \\begin{pmatrix} 0 \\\\ z_1 \\\\ z_2 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ z \\end{pmatrix}$.\nThe cone is $\\mathcal{K} = \\mathcal{Q}^3$. The primal conic program is:\n$$\n\\text{(Primal)} \\quad \\min_{z \\in \\mathbb{R}^2} \\ (P^{\\top}x)^{\\top}z \\quad \\text{s.t.} \\quad Az + b \\in \\mathcal{Q}^3\n$$\nThe dual problem is given by $\\max_{y} -b^{\\top} y$ subject to $A^{\\top} y + c = 0$ and $y \\in \\mathcal{K}^{*}$. The second-order cone is self-dual, so $\\mathcal{K}^{*} = \\mathcal{Q}^3$. The dual variable is $y = (y_0, y_w) \\in \\mathbb{R} \\times \\mathbb{R}^2$.\nThe dual constraints are:\n1. $y \\in \\mathcal{Q}^3 \\implies \\|y_w\\|_2 \\leq y_0$.\n2. $A^{\\top}y + c = 0$. Let's compute $A^{\\top}y$:\n$$\nA^{\\top}y = \\begin{pmatrix} 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} y_0 \\\\ y_{w1} \\\\ y_{w2} \\end{pmatrix} = \\begin{pmatrix} y_{w1} \\\\ y_{w2} \\end{pmatrix} = y_w\n$$\nSo the constraint becomes $y_w + P^{\\top}x = 0$, which implies $y_w = -P^{\\top}x$.\n\nThe dual objective is $-b^{\\top}y = -\\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} y_0 \\\\ y_w \\end{pmatrix} = -y_0$.\nThe dual problem is thus:\n$$\n\\text{(Dual)} \\quad \\max_{y_0, y_w} \\ -y_0 \\quad \\text{s.t.} \\quad y_w = -P^{\\top}x, \\quad \\|y_w\\|_2 \\leq y_0\n$$\nTo maximize $-y_0$, we must minimize $y_0$. The constraint $\\|y_w\\|_2 \\leq y_0$ implies that the minimum possible value for $y_0$ is $\\|y_w\\|_2$. Substituting $y_w = -P^{\\top}x$, the optimal $y_0$ is $y_0^* = \\|-P^{\\top}x\\|_2 = \\|P^{\\top}x\\|_2$.\nThe optimal value of the dual problem is $-y_0^* = -\\|P^{\\top}x\\|_2$.\n\nAssuming strong duality (which we will justify in Task 3), the optimal value of the primal equals the optimal value of the dual. The value of the inner minimization is:\n$$\n\\min_{\\|z\\|_{2} \\leq 1} x^{\\top}(\\bar{\\theta} + P z) = x^{\\top}\\bar{\\theta} - \\|P^{\\top}x\\|_2\n$$\n\n**2. Rewriting the Max-Min Problem**\n\nUsing the strong duality result from Task 1, we can replace the inner minimization problem with its dual maximization problem.\nThe original problem $\\max_{\\|x\\|_{2} \\leq 1} \\left( \\min_{\\|z\\|_{2} \\leq 1} x^{\\top}(\\bar{\\theta} + P z) \\right)$ becomes:\n$$\n\\max_{\\|x\\|_{2} \\leq 1} \\left( x^{\\top}\\bar{\\theta} + \\max_{y_0, y_w: \\|y_w\\|_2 \\leq y_0, y_w = -P^{\\top}x} (-y_0) \\right)\n$$\nThis can be rewritten as a single-level maximization problem over variables $x$, $y_0$, and $y_w$:\n$$\n\\begin{aligned}\n\\max_{x, y_0, y_w} \\quad & x^{\\top}\\bar{\\theta} - y_0 \\\\\n\\text{s.t.} \\quad & \\|x\\|_{2} \\leq 1 \\\\\n& \\|y_w\\|_{2} \\leq y_0 \\\\\n& P^{\\top}x + y_w = 0\n\\end{aligned}\n$$\nThis is a single-level optimization problem, which is also a Second-Order Cone Program (SOCP).\n\n**3. Verification of Slater's Condition**\n\nSlater's condition for conic programs guarantees strong duality. We need to check it for both the inner primal and dual problems.\n\nFor the primal problem: $\\min_{z} (P^{\\top}x)^{\\top}z$ s.t. $Az + b \\in \\mathcal{Q}^3$.\nSlater's condition requires the existence of a feasible point $z_0$ such that $Az_0+b$ is in the interior of the cone $\\mathcal{K}=\\mathcal{Q}^3$. The interior of $\\mathcal{Q}^3$ is $\\text{int}(\\mathcal{Q}^3) = \\{(t,w) \\in \\mathbb{R} \\times \\mathbb{R}^2 : \\|w\\|_2 < t\\}$.\nWe need to find $z_0 \\in \\mathbb{R}^2$ such that $(1, z_0) \\in \\text{int}(\\mathcal{Q}^3)$, which means $\\|z_0\\|_2 < 1$. We can simply choose $z_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$. Then $\\|z_0\\|_2 = 0 < 1$. Thus, a strictly feasible point exists, and Slater's condition holds for the primal problem for any fixed $x$.\n\nFor the dual problem: $\\max_{y} -b^{\\top}y$ s.t. $A^{\\top}y+c=0, y \\in \\mathcal{Q}^3$.\nSlater's condition requires the existence of a feasible point $y^*$ that lies in the interior of the cone, i.e., $y^* \\in \\text{int}(\\mathcal{Q}^3)$ and satisfies the equality constraint $A^{\\top}y^*+c=0$.\nWe need to find $y^*=(y_0^*, y_w^*) \\in \\mathbb{R} \\times \\mathbb{R}^2$ such that $\\|y_w^*\\|_2 < y_0^*$ and $y_w^* + P^{\\top}x = 0$.\nLet's choose $y_w^* = -P^{\\top}x$. To satisfy the strict inequality, we need to find a $y_0^*$ such that $\\| -P^{\\top}x \\|_2 < y_0^*$, which is $\\|P^{\\top}x\\|_2 < y_0^*$.\nWe can always find such a $y_0^*$, for example, by choosing $y_0^* = \\|P^{\\top}x\\|_2 + 1$.\nThe point $y^* = (\\|P^{\\top}x\\|_2 + 1, -P^{\\top}x)$ is strictly feasible for the dual problem. Thus, Slater's condition holds for the dual as well.\n\nSince Slater's condition holds for both the primal and dual problems, strong duality is justified.\n\n**4. Solving the Resulting Problem**\n\nFrom Task 1 and 2, the problem reduces to solving:\n$$\nV = \\max_{\\|x\\|_{2} \\leq 1} \\left( x^{\\top}\\bar{\\theta} - \\|P^{\\top}x\\|_2 \\right)\n$$\nWe are given $\\bar{\\theta} = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}$ and $P = 2 I_{2}$. Therefore, $P^{\\top} = (2 I_{2})^{\\top} = 2 I_{2}$.\nThis gives $\\|P^{\\top}x\\|_2 = \\|2 I_{2} x\\|_2 = \\|2x\\|_2 = 2\\|x\\|_2$.\nThe objective function becomes $f(x) = x^{\\top}\\bar{\\theta} - 2\\|x\\|_2$. The optimization problem is:\n$$\nV = \\max_{\\|x\\|_{2} \\leq 1} f(x) = \\max_{\\|x\\|_{2} \\leq 1} \\left( 3x_1 + x_2 - 2\\sqrt{x_1^2 + x_2^2} \\right)\n$$\nThe domain is the closed unit disk in $\\mathbb{R}^2$, which is a compact set. The function $f(x)$ is continuous on this domain, so a maximum exists. We check for the maximum in the interior and on the boundary of the disk.\n\nThe function is not differentiable at $x=0$. At this point, $f(0) = 0$.\n\nFor the interior of the disk ($0 < \\|x\\|_2 < 1$), we seek critical points by setting the gradient of $f(x)$ to zero.\n$$\n\\nabla f(x) = \\bar{\\theta} - 2 \\frac{x}{\\|x\\|_2}\n$$\nSetting $\\nabla f(x) = 0$ gives $x = \\frac{\\|x\\|_2}{2}\\bar{\\theta}$. Taking the Euclidean norm of both sides:\n$$\n\\|x\\|_2 = \\left\\| \\frac{\\|x\\|_2}{2}\\bar{\\theta} \\right\\| = \\frac{\\|x\\|_2}{2}\\|\\bar{\\theta}\\|_2\n$$\nSince we are in the interior, $\\|x\\|_2 > 0$, so we can divide by it to get $1 = \\frac{1}{2}\\|\\bar{\\theta}\\|_2$, which implies $\\|\\bar{\\theta}\\|_2 = 2$.\nLet's calculate $\\|\\bar{\\theta}\\|_2$:\n$$\n\\|\\bar{\\theta}\\|_2 = \\sqrt{3^2 + 1^2} = \\sqrt{9+1} = \\sqrt{10}\n$$\nSince $\\|\\bar{\\theta}\\|_2 = \\sqrt{10} \\neq 2$, there are no critical points in the interior of the disk.\n\nThe maximum must therefore lie on the boundary, where $\\|x\\|_2 = 1$. The problem on the boundary is:\n$$\n\\max_{\\|x\\|_{2} = 1} \\left( x^{\\top}\\bar{\\theta} - 2(1) \\right) = \\left( \\max_{\\|x\\|_{2} = 1} x^{\\top}\\bar{\\theta} \\right) - 2\n$$\nBy the Cauchy-Schwarz inequality, $x^{\\top}\\bar{\\theta} \\leq \\|x\\|_2 \\|\\bar{\\theta}\\|_2$. The maximum value of $x^{\\top}\\bar{\\theta}$ for $\\|x\\|_2=1$ is $\\|\\bar{\\theta}\\|_2$, achieved when $x$ is aligned with $\\bar{\\theta}$, i.e., $x = \\frac{\\bar{\\theta}}{\\|\\bar{\\theta}\\|_2}$.\nThe maximum value of $f(x)$ on the boundary is $\\|\\bar{\\theta}\\|_2 - 2 = \\sqrt{10} - 2$.\n\nComparing the values, we have $f(0) = 0$ and the boundary maximum $\\sqrt{10}-2$. Since $\\sqrt{10} > \\sqrt{4} = 2$, the value $\\sqrt{10}-2$ is positive.\nThus, the global maximum value is $V = \\sqrt{10}-2$.\nThis is the optimal robust value of the objective function.", "answer": "$$\\boxed{\\sqrt{10}-2}$$", "id": "3111113"}]}