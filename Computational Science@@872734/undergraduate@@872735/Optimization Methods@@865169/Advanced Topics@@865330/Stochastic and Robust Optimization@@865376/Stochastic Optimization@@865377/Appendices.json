{"hands_on_practices": [{"introduction": "Real-world optimization often involves uncertainty, like fluctuating traffic when planning a delivery route. The Sample Average Approximation (SAA) method provides a foundational strategy for tackling such problems. By averaging outcomes over a set of simulated scenarios, SAA transforms a complex stochastic problem into a deterministic one that can be solved with standard algorithms [@problem_id:2182114].", "problem": "A logistics company is using simulations to optimize delivery routes in a small, congested city district. The district's road network can be modeled as a set of four intersections, labeled 1, 2, 3, and 4, connected by five two-way road segments. The segments are (1,2), (1,3), (2,3), (2,4), and (3,4).\n\nTo account for traffic variability, the company has run five simulations, each representing a different plausible traffic scenario. The resulting travel time in minutes for traversing each road segment under each scenario is given in the table below.\n\n| Road Segment | Scenario 1 | Scenario 2 | Scenario 3 | Scenario 4 | Scenario 5 |\n|--------------|------------|------------|------------|------------|------------|\n| (1,2)        | 15         | 25         | 18         | 22         | 20         |\n| (1,3)        | 30         | 12         | 15         | 18         | 25         |\n| (2,3)        | 10         | 8          | 12         | 15         | 5          |\n| (2,4)        | 35         | 40         | 28         | 30         | 32         |\n| (3,4)        | 20         | 25         | 35         | 30         | 20         |\n\nYour task is to find the best route from intersection 1 to intersection 4. You are to use the Sample Average Approximation (SAA) method. In the SAA method, the true (but unknown) expected travel time for each road segment is approximated by its sample average, calculated from the simulation data. The problem then becomes finding the path with the minimum total approximated travel time.\n\nWhat is the estimated minimum average travel time for a trip from intersection 1 to intersection 4 based on this SAA model? Express your answer in minutes, rounded to three significant figures.", "solution": "We apply the Sample Average Approximation (SAA): for each road segment $e$, approximate its expected travel time by the sample mean over the $5$ scenarios,\n$$\n\\hat{t}_{e}=\\frac{1}{5}\\sum_{s=1}^{5}t_{e}^{(s)}.\n$$\nCompute the sample means for all segments:\n- For $(1,2)$:\n$$\n\\hat{t}_{12}=\\frac{1}{5}(15+25+18+22+20)=\\frac{100}{5}=20.\n$$\n- For $(1,3)$:\n$$\n\\hat{t}_{13}=\\frac{1}{5}(30+12+15+18+25)=\\frac{100}{5}=20.\n$$\n- For $(2,3)$:\n$$\n\\hat{t}_{23}=\\frac{1}{5}(10+8+12+15+5)=\\frac{50}{5}=10.\n$$\n- For $(2,4)$:\n$$\n\\hat{t}_{24}=\\frac{1}{5}(35+40+28+30+32)=\\frac{165}{5}=33.\n$$\n- For $(3,4)$:\n$$\n\\hat{t}_{34}=\\frac{1}{5}(20+25+35+30+20)=\\frac{130}{5}=26.\n$$\n\nWith these edge weights, compute total times for simple paths from $1$ to $4$:\n- Path $1\\to 2\\to 4$: $20+33=53$.\n- Path $1\\to 3\\to 4$: $20+26=46$.\n- Path $1\\to 2\\to 3\\to 4$: $20+10+26=56$.\n- Path $1\\to 3\\to 2\\to 4$: $20+10+33=63$.\n\nThe minimum total is $46$ via path $1\\to 3\\to 4$. Rounding to three significant figures gives $46.0$.", "answer": "$$\\boxed{46.0}$$", "id": "2182114"}, {"introduction": "In modern machine learning, the efficiency of Stochastic Gradient Descent (SGD) is paramount, and the choice of mini-batch size is a critical tuning parameter. This exercise moves beyond simply reducing gradient variance and asks a more practical question: how do we maximize progress per unit of *time*? You will derive the relationship between batch size, computational cost, and learning progress to find the optimal balance in a realistic trade-off scenario [@problem_id:3187488].", "problem": "Consider single-parameter Stochastic Gradient Descent (SGD) on a strongly convex quadratic objective defined by $f(x) = \\frac{\\lambda}{2} x^{2}$, where $\\lambda  0$ is the curvature. At iteration $k$, the true gradient is $\\nabla f(x_{k}) = \\lambda x_{k}$. Assume an unbiased stochastic gradient estimator with additive noise so that a single-sample gradient observation has the form $g(x_{k}) = \\lambda x_{k} + \\varepsilon$, where $\\mathbb{E}[\\varepsilon] = 0$ and $\\mathrm{Var}(\\varepsilon) = \\sigma^{2}$. For a mini-batch of size $b$, the averaged noise has variance $\\sigma^{2} / b$ due to independence, and the mini-batch estimator is $g_{b}(x_{k}) = \\lambda x_{k} + \\bar{\\varepsilon}_{b}$ with $\\mathrm{Var}(\\bar{\\varepsilon}_{b}) = \\sigma^{2} / b$.\n\nAn SGD update with fixed step-size $\\alpha  0$ is $x_{k+1} = x_{k} - \\alpha g_{b}(x_{k})$. Define the computational time per update as $T(b) = t_{0} + c b$, where $t_{0}  0$ is a fixed overhead time per update and $c  0$ is the compute cost per sample. Both $t_{0}$ and $c$ have units of seconds (s), while the mini-batch size $b$ is unitless.\n\nDefine the expected one-step progress as the expected decrease in objective value,\n$$\n\\Delta(b) = \\mathbb{E}\\left[f(x_{k}) - f(x_{k+1})\\right],\n$$\nand the expected progress per unit time as\n$$\nR(b) = \\frac{\\Delta(b)}{T(b)}.\n$$\n\nStarting from the core definitions above—convex quadratic objective $f(x)$, unbiased gradient estimator with independent noise, mini-batch averaging, and the SGD update rule—derive $R(b)$ in terms of the parameters $(\\lambda, \\alpha, x_{k}, \\sigma^{2}, t_{0}, c, b)$ without using any shortcut formulas or external hints. Then, design a program that, for each test case listed below, searches over integer mini-batch sizes $b \\in \\{1,2,\\dots,B_{\\max}\\}$ and returns the integer value of $b$ that maximizes $R(b)$. Use the fixed constants $\\lambda = 1$, $\\alpha = 0.5$, $x_{k} = 1$, $t_{0} = 0.02$ (s), and $B_{\\max} = 512$ across all test cases. In every test case, the unit for $c$ must be seconds per sample (s/sample). No angle units are involved, and no percentages are required.\n\nTest Suite (each test case is a pair $(\\sigma^{2}, c)$ with $\\sigma^{2}$ unitless and $c$ in seconds per sample):\n- Case $1$: $(\\sigma^{2}, c) = (1.0, 0.002)$\n- Case $2$: $(\\sigma^{2}, c) = (10.0, 0.002)$\n- Case $3$: $(\\sigma^{2}, c) = (0.01, 0.002)$\n- Case $4$: $(\\sigma^{2}, c) = (1.0, 0.02)$\n- Case $5$: $(\\sigma^{2}, c) = (5.0, 0.0005)$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[b_{1},b_{2},b_{3},b_{4},b_{5}]$), where $b_{i}$ is the optimal mini-batch size for Case $i$. The output values must be integers.", "solution": "The problem is valid as it is scientifically grounded in the principles of stochastic optimization, is well-posed, objective, and internally consistent. All data and constants required for a unique solution are provided. We will now proceed with the derivation and solution.\n\nThe primary goal is to find the integer mini-batch size $b$ that maximizes the expected progress per unit time, $R(b)$. This quantity is defined as the ratio of the expected one-step progress, $\\Delta(b)$, to the computational time per update, $T(b)$. We must first derive an explicit expression for $R(b)$ from the given definitions.\n\nThe objective function in question is a strongly convex quadratic:\n$$f(x) = \\frac{\\lambda}{2} x^{2}$$\nwhere the curvature $\\lambda  0$. The true gradient at a point $x_k$ is $\\nabla f(x_k) = \\lambda x_k$.\n\nThe mini-batch stochastic gradient estimator of size $b$ is given as:\n$$g_{b}(x_{k}) = \\lambda x_{k} + \\bar{\\varepsilon}_{b}$$\nwhere the averaged noise term $\\bar{\\varepsilon}_{b}$ has mean $\\mathbb{E}[\\bar{\\varepsilon}_{b}] = 0$ and variance $\\mathrm{Var}(\\bar{\\varepsilon}_{b}) = \\frac{\\sigma^{2}}{b}$. Here, $\\sigma^2$ is the variance of the noise from a single sample.\n\nThe Stochastic Gradient Descent (SGD) update rule with a fixed step-size $\\alpha  0$ is:\n$$x_{k+1} = x_{k} - \\alpha g_{b}(x_{k})$$\n\nThe expected one-step progress is defined as $\\Delta(b) = \\mathbb{E}\\left[f(x_{k}) - f(x_{k+1})\\right]$. We begin by expressing $f(x_{k+1})$ in terms of $x_k$ and the stochastic gradient:\n$$f(x_{k+1}) = \\frac{\\lambda}{2} x_{k+1}^{2} = \\frac{\\lambda}{2} \\left(x_{k} - \\alpha g_{b}(x_{k})\\right)^{2}$$\nExpanding the squared term gives:\n$$f(x_{k+1}) = \\frac{\\lambda}{2} \\left( x_{k}^{2} - 2\\alpha x_{k} g_{b}(x_{k}) + \\alpha^{2} g_{b}(x_{k})^{2} \\right)$$\nTo find the expected value of $f(x_{k+1})$, we take the expectation with respect to the noise in the gradient estimator. The value of $x_k$ is considered fixed at iteration $k$:\n$$\\mathbb{E}[f(x_{k+1})] = \\frac{\\lambda}{2} \\mathbb{E}\\left[ x_{k}^{2} - 2\\alpha x_{k} g_{b}(x_{k}) + \\alpha^{2} g_{b}(x_{k})^{2} \\right]$$\nBy linearity of expectation:\n$$\\mathbb{E}[f(x_{k+1})] = \\frac{\\lambda}{2} \\left( x_{k}^{2} - 2\\alpha x_{k} \\mathbb{E}[g_{b}(x_{k})] + \\alpha^{2} \\mathbb{E}[g_{b}(x_{k})^{2}] \\right)$$\nWe now need to compute the first and second moments of $g_{b}(x_{k})$.\nThe first moment (expectation) is:\n$$\\mathbb{E}[g_{b}(x_{k})] = \\mathbb{E}[\\lambda x_{k} + \\bar{\\varepsilon}_{b}] = \\lambda x_{k} + \\mathbb{E}[\\bar{\\varepsilon}_{b}] = \\lambda x_{k}$$\nThe second moment is found using the relation $\\mathbb{E}[X^2] = \\mathrm{Var}(X) + (\\mathbb{E}[X])^2$:\n$$\\mathbb{E}[g_{b}(x_{k})^{2}] = \\mathrm{Var}(g_{b}(x_{k})) + (\\mathbb{E}[g_{b}(x_{k})])^{2}$$\nThe variance of the estimator is:\n$$\\mathrm{Var}(g_{b}(x_{k})) = \\mathrm{Var}(\\lambda x_{k} + \\bar{\\varepsilon}_{b}) = \\mathrm{Var}(\\bar{\\varepsilon}_{b}) = \\frac{\\sigma^{2}}{b}$$\nThus, the second moment is:\n$$\\mathbb{E}[g_{b}(x_{k})^{2}] = \\frac{\\sigma^{2}}{b} + (\\lambda x_{k})^{2} = \\lambda^{2} x_{k}^{2} + \\frac{\\sigma^{2}}{b}$$\nSubstituting these moments back into the expression for $\\mathbb{E}[f(x_{k+1})]$:\n$$\\mathbb{E}[f(x_{k+1})] = \\frac{\\lambda}{2} \\left( x_{k}^{2} - 2\\alpha x_{k} (\\lambda x_{k}) + \\alpha^{2} \\left(\\lambda^{2} x_{k}^{2} + \\frac{\\sigma^{2}}{b}\\right) \\right)$$\n$$\\mathbb{E}[f(x_{k+1})] = \\frac{\\lambda}{2} \\left( x_{k}^{2} - 2\\alpha\\lambda x_{k}^{2} + \\alpha^{2}\\lambda^{2} x_{k}^{2} + \\frac{\\alpha^{2}\\sigma^{2}}{b} \\right)$$\nGrouping terms with $x_k^2$:\n$$\\mathbb{E}[f(x_{k+1})] = \\frac{\\lambda}{2} x_{k}^{2} (1 - 2\\alpha\\lambda + \\alpha^{2}\\lambda^{2}) + \\frac{\\lambda\\alpha^{2}\\sigma^{2}}{2b}$$\nThe term in the parenthesis is a perfect square, $(1 - \\alpha\\lambda)^{2}$:\n$$\\mathbb{E}[f(x_{k+1})] = \\frac{\\lambda}{2} x_{k}^{2} (1 - \\alpha\\lambda)^{2} + \\frac{\\lambda\\alpha^{2}\\sigma^{2}}{2b}$$\nNow we can compute the expected progress $\\Delta(b) = \\mathbb{E}[f(x_k)] - \\mathbb{E}[f(x_{k+1})]$. Since $x_k$ is fixed, $\\mathbb{E}[f(x_k)]=f(x_k)=\\frac{\\lambda}{2}x_k^2$.\n$$\\Delta(b) = \\frac{\\lambda}{2}x_k^2 - \\left( \\frac{\\lambda}{2} x_{k}^{2} (1 - \\alpha\\lambda)^{2} + \\frac{\\lambda\\alpha^{2}\\sigma^{2}}{2b} \\right)$$\n$$\\Delta(b) = \\frac{\\lambda}{2}x_k^2 \\left[ 1 - (1 - \\alpha\\lambda)^{2} \\right] - \\frac{\\lambda\\alpha^{2}\\sigma^{2}}{2b}$$\nExpanding the term $1 - (1 - \\alpha\\lambda)^{2} = 1 - (1 - 2\\alpha\\lambda + \\alpha^2\\lambda^2) = 2\\alpha\\lambda - \\alpha^2\\lambda^2 = \\alpha\\lambda(2 - \\alpha\\lambda)$.\n$$\\Delta(b) = \\frac{\\lambda}{2}x_k^2 \\left[ \\alpha\\lambda(2 - \\alpha\\lambda) \\right] - \\frac{\\lambda\\alpha^{2}\\sigma^{2}}{2b}$$\nSimplifying gives the final expression for the expected one-step progress:\n$$\\Delta(b) = \\alpha\\lambda^2 x_k^2 \\left(1 - \\frac{\\alpha\\lambda}{2}\\right) - \\frac{\\lambda\\alpha^{2}\\sigma^{2}}{2b}$$\nThis expression clearly shows two components: a positive progress term independent of the batch size (corresponding to a deterministic gradient step) and a negative term due to gradient noise, which diminishes as the batch size $b$ increases.\n\nThe computational time per update is given by the linear model $T(b) = t_{0} + c b$.\n\nThe rate of progress $R(b)$ is the ratio of these two quantities:\n$$R(b) = \\frac{\\Delta(b)}{T(b)} = \\frac{\\alpha\\lambda^2 x_k^2 \\left(1 - \\frac{\\alpha\\lambda}{2}\\right) - \\frac{\\lambda\\alpha^{2}\\sigma^{2}}{2b}}{t_{0} + c b}$$\nThe problem requires finding the integer $b \\in \\{1, 2, \\dots, B_{\\max}\\}$ that maximizes this function $R(b)$ for given sets of parameters. The constants are $\\lambda = 1$, $\\alpha = 0.5$, $x_{k} = 1$, $t_{0} = 0.02$, and $B_{\\max} = 512$. The parameters $(\\sigma^2, c)$ vary per test case.\n\nLet's substitute the fixed constants into the expression for $R(b)$.\nThe term $\\alpha\\lambda^2 x_k^2 \\left(1 - \\frac{\\alpha\\lambda}{2}\\right)$ becomes:\n$$C_1 = (0.5)(1)^2(1)^2 \\left(1 - \\frac{(0.5)(1)}{2}\\right) = 0.5 \\left(1 - 0.25\\right) = 0.5(0.75) = 0.375$$\nThe coefficient of the noise term $\\frac{\\lambda\\alpha^{2}}{2}$ becomes:\n$$\\frac{(1)(0.5)^2}{2} = \\frac{0.25}{2} = 0.125$$\nSo, the full expression for $R(b)$ for the given parameters is:\n$$R(b) = \\frac{0.375 - \\frac{0.125 \\sigma^2}{b}}{0.02 + c b}$$\nThe algorithmic approach to find the optimal integer $b$ is to perform a direct search. We will iterate through all possible integer values of $b$ from $1$ to $B_{\\max} = 512$, calculate $R(b)$ for each, and identify the value of $b$ that yields the maximum $R(b)$. This is a simple and robust method for a small search space. For a given test case $(\\sigma^2, c)$, the program will compute $R(b)$ for $b=1, 2, ..., 512$ and record the $b$ that results in the highest value.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for the optimal mini-batch size b for a series of test cases.\n    The optimization goal is to maximize the expected progress per unit time, R(b).\n    \"\"\"\n\n    # Define the fixed constants from the problem statement.\n    LAMBDA = 1.0  # Curvature\n    ALPHA = 0.5   # Step-size\n    XK = 1.0      # Current position\n    T0 = 0.02     # Fixed overhead time per update (s)\n    B_MAX = 512   # Maximum mini-batch size to search\n\n    # Test suite with pairs of (sigma^2, c)\n    # sigma^2 is the single-sample noise variance (unitless)\n    # c is the compute cost per sample (s/sample)\n    test_cases = [\n        (1.0, 0.002),\n        (10.0, 0.002),\n        (0.01, 0.002),\n        (1.0, 0.02),\n        (5.0, 0.0005),\n    ]\n\n    results = []\n\n    # Pre-calculate the constant part of the numerator of R(b), which is independent of sigma^2 and c.\n    # This term corresponds to alpha * lambda^2 * x_k^2 * (1 - (alpha * lambda) / 2)\n    progress_term_constant = ALPHA * LAMBDA**2 * XK**2 * (1 - (ALPHA * LAMBDA) / 2.)\n\n    for sigma_sq, c in test_cases:\n        best_b = -1\n        max_R = -np.inf\n\n        # Pre-calculate the coefficient of the noise term in the numerator.\n        # This term corresponds to (lambda * alpha^2 * sigma^2) / 2\n        noise_term_coeff = (LAMBDA * ALPHA**2 * sigma_sq) / 2.\n\n        # Search over all allowed integer mini-batch sizes\n        for b in range(1, B_MAX + 1):\n            # Calculate the expected one-step progress, Delta(b)\n            # Delta(b) = progress_term_constant - noise_term_coeff / b\n            delta_b = progress_term_constant - noise_term_coeff / b\n            \n            # The progress rate R(b) can only be maximal if progress Delta(b) is positive.\n            # While we could skip b values where delta_b = 0, the max search handles this naturally.\n\n            # Calculate the computational time per update, T(b)\n            T_b = T0 + c * b\n\n            # Calculate the expected progress per unit time, R(b)\n            # Avoid division by zero, although T_b  0 is guaranteed by problem constraints.\n            if T_b  0:\n                R_b = delta_b / T_b\n            else:\n                R_b = -np.inf\n\n            # Update the best batch size if the current one is better\n            if R_b  max_R:\n                max_R = R_b\n                best_b = b\n        \n        results.append(best_b)\n\n    # Format the final output as a comma-separated list in brackets\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3187488"}, {"introduction": "Adaptive optimizers like Adam are widely used for their fast convergence, but their theoretical guarantees can be subtle. This practice challenges you to construct a scenario that reveals a known instability in the Adam algorithm, caused by its adaptive learning rate mechanism. By implementing both Adam and its more robust successor, AMSGrad, you will gain a deep, hands-on understanding of what makes these advanced algorithms tick and why algorithmic modifications are sometimes necessary for reliable performance [@problem_id:3187493].", "problem": "You are to design a deterministic stochastic optimization experiment that contrasts the behavior of the Adaptive Moment Estimation (Adam) method and the Adaptive Moment Estimation with Maximum (AMSGrad) method under different hyperparameter choices. The focus is on constructing a controlled gradient stream in which Adam diverges due to improper choices of the numerical stability term and exponential average parameters, while AMSGrad remains stable on the same data.\n\nThe fundamental base must be the following, without shortcuts: stochastic optimization uses gradient-based updates computed from a sequence of noisy gradients. Adaptive moment methods are built from exponential moving averages of the first moment and the second moment of the observed gradients, combined with bias correction arising from initialization at zero. Stability is impacted by the interplay of the decay parameters for the exponential averages, the learning rate, and an additive stabilizer in the denominator.\n\nConstruct the experiment entirely in one dimension. Use a time horizon of $T$ iterations, generated by $C$ identical cycles of length $L$ so that $T = C L$. Within each cycle, define a gradient stream $\\{g_t\\}_{t=1}^T$ using a single positive burst followed by many small negatives:\n- Let the burst magnitude be $S  0$.\n- For each cycle, set $g_{t} = S$ on the first step of the cycle.\n- For the remaining $L-1$ steps of that cycle, set $g_{t} = -s$, where $s = \\dfrac{S}{L-1}$, so that the sum of gradients over each cycle is exactly $0$.\n\nThis construction produces a nonstationary second moment of the gradients with intermittent spikes and long quiet periods of small magnitude that test the sensitivity of adaptive methods. Initialize the parameter at $x_0 = 0$ and perform parameter updates with the chosen adaptive method. Both adaptive methods must be implemented from the definitions of exponential moving averages and bias correction, with the following hyperparameters:\n- Learning rate $\\alpha > 0$.\n- First moment decay $\\beta_1 \\in (0,1)$.\n- Second moment decay $\\beta_2 \\in (0,1)$.\n- Additive stabilizer $\\varepsilon > 0$.\n\nA run is deemed to have diverged if either the final iterate magnitude exceeds a fixed bound or becomes nonfinite. Specifically, define the divergence threshold $D$ and say the method diverges if $|x_T| > D$ or $x_T$ is not a finite real number.\n\nYour program must:\n- Implement both Adam and AMSGrad from first principles of exponential moving averages and bias correction.\n- Generate the gradient stream as described, with $L = 200$, $S = 1$, and $C = 8$, so that $T = 1600$.\n- Run both methods on the exact same gradient stream for each test case.\n- Use an initial iterate $x_0 = 0$ and divergence threshold $D = 10$.\n\nUse the following test suite of hyperparameter sets $(\\alpha, \\beta_1, \\beta_2, \\varepsilon)$:\n1. $\\left(10^{-2}, 0.99, 0.1, 10^{-20}\\right)$, a case with very small $\\varepsilon$ and relatively small $\\beta_2$ that should induce instability in Adam when second moment estimates drop quickly.\n2. $\\left(10^{-2}, 0.9, 0.999, 10^{-8}\\right)$, a typical choice expected to be stable for both Adam and AMSGrad.\n3. $\\left(10^{-2}, 0.9, 10^{-6}, 10^{-32}\\right)$, a case with extremely small $\\beta_2$ and $\\varepsilon$ aimed to demonstrate Adam’s sensitivity.\n4. $\\left(2 \\times 10^{-4}, 0.99, 0.1, 10^{-20}\\right)$, a boundary case with a very small learning rate expected to avoid divergence even with adverse $\\beta_2$ and $\\varepsilon$.\n\nFor each test case, return two boolean results in a fixed order:\n- First boolean: whether Adam diverged on that case.\n- Second boolean: whether AMSGrad diverged on that case.\n\nFinal output format: Your program should produce a single line of output containing all eight booleans as a comma-separated list enclosed in square brackets, ordered by test case as\n$[ \\text{Adam case 1}, \\text{AMSGrad case 1}, \\text{Adam case 2}, \\text{AMSGrad case 2}, \\text{Adam case 3}, \\text{AMSGrad case 3}, \\text{Adam case 4}, \\text{AMSGrad case 4} ]$.", "solution": "The problem statement is evaluated as valid. It presents a well-posed, scientifically grounded, and objective computational experiment in the field of stochastic optimization. All necessary parameters and definitions are provided, and the task is to implement and compare two standard algorithms, Adam and AMSGrad, under specific conditions designed to highlight a known potential failure mode of Adam.\n\nThe objective of this experiment is to demonstrate a scenario where the Adaptive Moment Estimation (Adam) optimizer diverges, while its variant, AMSGrad, remains stable. This is achieved by constructing a specific nonstationary gradient stream and selecting hyperparameters that expose Adam's sensitivity to rapid changes in the second moment estimate of the gradients.\n\nFirst, we define the gradient stream $\\{g_t\\}_{t=1}^T$. The stream is constructed over a total of $T$ time steps, composed of $C$ identical cycles, each of length $L$. The problem specifies $C=8$ cycles and a cycle length of $L=200$, for a total time horizon of $T = C \\times L = 8 \\times 200 = 1600$ steps. Within each cycle, the gradient is a large positive burst, $g_t = S$, followed by $L-1$ small negative gradients. The problem sets the burst magnitude to $S=1$. To ensure the sum of gradients over a single cycle is zero, the small negative gradient value, $s$, must be $s = \\frac{S}{L-1} = \\frac{1}{200-1} = \\frac{1}{199}$.\nThe gradient at any time step $t \\in \\{1, 2, \\dots, 1600\\}$ is thus given by:\n$$\ng_t =\n\\begin{cases}\n    S = 1  \\text{if } (t-1) \\pmod{L} = 0 \\\\\n    -s = -\\frac{1}{199}  \\text{if } (t-1) \\pmod{L} \\neq 0\n\\end{cases}\n$$\nThis structure ensures that the second moment of the gradients, $g_t^2$, experiences large spikes ($S^2=1$) followed by long periods of very small values ($s^2 \\approx 2.5 \\times 10^{-5}$).\n\nThe optimization process starts with an initial parameter value of $x_0 = 0$. We will implement and compare two optimization algorithms: Adam and AMSGrad. Both rely on exponential moving averages of the first and second moments of the gradient. The hyperparameters are the learning rate $\\alpha$, the first moment decay rate $\\beta_1$, the second moment decay rate $\\beta_2$, and the numerical stability term $\\varepsilon$.\n\nThe Adam algorithm updates are as follows. We initialize the first and second moment vectors as $m_0 = 0$ and $v_0 = 0$. For each time step $t = 1, \\dots, T$:\n$1$. Update the biased first moment estimate: $m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t$.\n$2$. Update the biased second moment estimate: $v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2$.\n$3$. Compute the bias-corrected first moment estimate: $\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}$.\n$4$. Compute the bias-corrected second moment estimate: $\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}$.\n$5$. Update the parameter: $x_t = x_{t-1} - \\alpha \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\varepsilon}$.\n\nThe AMSGrad algorithm modifies the Adam update to ensure a non-increasing effective learning rate. We initialize $m_0 = 0$, $v_0 = 0$, and additionally $v_{\\max, 0} = 0$. For each time step $t = 1, \\dots, T$:\n$1$. Update the biased first moment estimate: $m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t$.\n$2$. Update the biased second moment estimate: $v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2$.\n$3$. Compute the bias-corrected first moment estimate: $\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}$.\n$4$. Maintain the maximum of the second moment estimates seen so far: $v_{\\max, t} = \\max(v_{\\max, t-1}, v_t)$.\n$5$. Update the parameter using this maximum: $x_t = x_{t-1} - \\alpha \\frac{\\hat{m}_t}{\\sqrt{v_{\\max, t}} + \\varepsilon}$.\n\nThe critical difference lies in the denominator of the update rule. In Adam, the term $\\hat{v}_t$ can decrease if the recent gradients are small, especially if $\\beta_2$ is small (i.e., short memory). If $\\hat{v}_t$ becomes very small, the effective learning rate $\\alpha / (\\sqrt{\\hat{v}_t} + \\varepsilon)$ can explode, leading to divergence. AMSGrad prevents this by using $v_{\\max, t}$, which is non-decreasing and thus keeps the effective learning rate from growing uncontrollably.\n\nA run is considered to have diverged if the final parameter value $x_T$ has a magnitude greater than the threshold $D=10$, or if $x_T$ is a non-finite number (e.g., infinity or NaN).\n\nThe experiment is run for four distinct hyperparameter sets $(\\alpha, \\beta_1, \\beta_2, \\varepsilon)$:\n$1$. $(10^{-2}, 0.99, 0.1, 10^{-20})$: A small $\\beta_2$ and extremely small $\\varepsilon$ are expected to cause Adam to diverge, as the memory of the large gradient spike fades quickly, and the denominator shrinks dramatically. AMSGrad should remain stable.\n$2$. $(10^{-2}, 0.9, 0.999, 10^{-8})$: A large $\\beta_2$ gives the second moment estimate a long memory, which should keep Adam stable. Both methods are expected to be stable.\n$3$. $(10^{-2}, 0.9, 10^{-6}, 10^{-32})$: An extremely small $\\beta_2$ and $\\varepsilon$ provide an even more severe test case, strongly favoring AMSGrad's stability mechanism. Adam is highly likely to diverge.\n$4$. $(2 \\times 10^{-4}, 0.99, 0.1, 10^{-20})$: A very small learning rate $\\alpha$ should dampen the update step sufficiently to prevent divergence in Adam, even with otherwise problematic values for $\\beta_2$ and $\\varepsilon$.\n\nFor each case, we will simulate both Adam and AMSGrad on the identical gradient stream and report a boolean value indicating divergence for each.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Designs and runs a deterministic stochastic optimization experiment\n    to contrast Adam and AMSGrad, implementing both from first principles.\n    \"\"\"\n    \n    # Define problem constants\n    L = 200  # Cycle length\n    S = 1.0  # Burst magnitude\n    C = 8    # Number of cycles\n    T = L * C  # Total iterations\n    D = 10.0 # Divergence threshold\n    \n    # Generate the gradient stream\n    s = S / (L - 1)\n    gradients = np.full(T, -s)\n    cycle_starts = np.arange(0, T, L)\n    gradients[cycle_starts] = S\n\n    # Define the test suite of hyperparameters (alpha, beta1, beta2, epsilon)\n    test_cases = [\n        (1e-2, 0.99, 0.1, 1e-20),\n        (1e-2, 0.9, 0.999, 1e-8),\n        (1e-2, 0.9, 1e-6, 1e-32),\n        (2e-4, 0.99, 0.1, 1e-20),\n    ]\n\n    results = []\n    \n    for params in test_cases:\n        alpha, beta1, beta2, epsilon = params\n\n        # --- Adam Simulation ---\n        x_adam = 0.0\n        m_adam = 0.0\n        v_adam = 0.0\n        beta1_power_t = 1.0\n        beta2_power_t = 1.0\n        \n        for i in range(T):\n            grad = gradients[i]\n            \n            # Update powers for bias correction\n            beta1_power_t *= beta1\n            beta2_power_t *= beta2\n            \n            # Update biased moment estimates\n            m_adam = beta1 * m_adam + (1.0 - beta1) * grad\n            v_adam = beta2 * v_adam + (1.0 - beta2) * (grad**2)\n            \n            # Compute bias-corrected estimates\n            m_hat = m_adam / (1.0 - beta1_power_t)\n            v_hat = v_adam / (1.0 - beta2_power_t)\n            \n            # Parameter update\n            # Precaution: ensure v_hat is non-negative before sqrt\n            if v_hat  0: v_hat = 0\n            x_adam -= alpha * m_hat / (np.sqrt(v_hat) + epsilon)\n            \n            # Early exit on non-finite value\n            if not np.isfinite(x_adam):\n                break\n\n        adam_diverged = np.abs(x_adam)  D or not np.isfinite(x_adam)\n        results.append(adam_diverged)\n\n        # --- AMSGrad Simulation ---\n        x_ams = 0.0\n        m_ams = 0.0\n        v_ams = 0.0\n        v_max_ams = 0.0\n        beta1_power_t_ams = 1.0\n        \n        for i in range(T):\n            grad = gradients[i]\n            \n            # Update powers for bias correction\n            beta1_power_t_ams *= beta1\n\n            # Update biased moment estimates\n            m_ams = beta1 * m_ams + (1.0 - beta1) * grad\n            v_ams = beta2 * v_ams + (1.0 - beta2) * (grad**2)\n            \n            # Maintain the maximum of the second moment estimate\n            v_max_ams = max(v_max_ams, v_ams)\n\n            # Compute bias-corrected first moment estimate\n            m_hat = m_ams / (1.0 - beta1_power_t_ams)\n            \n            # Parameter update\n            # No need to check for negative v_max_ams as it's non-decreasing from 0.\n            x_ams -= alpha * m_hat / (np.sqrt(v_max_ams) + epsilon)\n            \n            # Early exit on non-finite value\n            if not np.isfinite(x_ams):\n                break\n\n        amsgrad_diverged = np.abs(x_ams)  D or not np.isfinite(x_ams)\n        results.append(amsgrad_diverged)\n\n    # Format the final output as specified\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3187493"}]}