{"hands_on_practices": [{"introduction": "The first step in mastering robust optimization is learning how to convert an uncertain problem, filled with infinitely many possibilities, into a single, solvable deterministic problem. This process, known as creating a robust counterpart, is the cornerstone of the field. This exercise [@problem_id:3173984] guides you through this fundamental procedure for a system with an uncertain right-hand side, introducing the powerful concept of dual norms to tame the uncertainty and reveal a clear, tractable constraint.", "problem": "Consider an uncertain linear inequality system with an $m \\times n$ matrix $A$, a vector $b \\in \\mathbb{R}^m$, and a decision vector $x \\in \\mathbb{R}^n$. The uncertainty is confined to the right-hand side and is modeled as $A x \\le b + \\Delta b$ where the perturbation vector $\\Delta b \\in \\mathbb{R}^m$ belongs to the set $\\{\\Delta b : \\|\\Delta b\\|_{1} \\le \\rho\\}$ for a given budget $\\rho \\ge 0$. The robust feasibility requirement is that all $m$ inequalities hold for every $\\Delta b$ in this set.\n\nStarting from the definition of robust feasibility and the definition of the dual norm of the $\\ell_{1}$-norm, derive a deterministic equivalent (robust counterpart) that is free of $\\Delta b$ and that must hold on $x$ to guarantee robust feasibility. Prove that there exists a single scalar safety margin $\\tau(\\rho) \\ge 0$, depending only on $\\rho$, such that the robust counterpart can be expressed as $A x \\le b - \\tau(\\rho)\\,\\mathbf{1}_{m}$, where $\\mathbf{1}_{m} \\in \\mathbb{R}^{m}$ is the vector of ones. Determine the exact closed-form expression of $\\tau(\\rho)$.\n\nBriefly explain, using your derivation, how the shared budget $\\|\\Delta b\\|_{1} \\le \\rho$ induces redundancy across constraints in the robust counterpart.\n\nYour final answer must be the single closed-form expression for $\\tau(\\rho)$, with no units and no rounding.", "solution": "The problem is to find the deterministic robust counterpart of an uncertain linear inequality system and to determine a specific safety margin function $\\tau(\\rho)$.\n\n### Step 1: Problem Validation\n\nThe problem statement is a standard exercise in robust optimization theory.\n**Givens:**\n- An uncertain linear inequality system $A x \\le b + \\Delta b$, where $A$ is an $m \\times n$ matrix, $x \\in \\mathbb{R}^n$ is the decision vector, and $b \\in \\mathbb{R}^m$.\n- The uncertainty vector $\\Delta b \\in \\mathbb{R}^m$ belongs to the uncertainty set $U = \\{\\Delta b \\in \\mathbb{R}^m : \\|\\Delta b\\|_{1} \\le \\rho\\}$, where $\\rho \\ge 0$ is a given budget of uncertainty. The norm $\\|\\cdot\\|_1$ is the $\\ell_1$-norm, defined as $\\|\\Delta b\\|_1 = \\sum_{i=1}^m |(\\Delta b)_i|$.\n- Robust feasibility requirement: The inequality $A x \\le b + \\Delta b$ must hold for every $\\Delta b \\in U$.\n- The target form for the robust counterpart is $A x \\le b - \\tau(\\rho)\\,\\mathbf{1}_{m}$, where $\\mathbf{1}_{m}$ is the $m$-dimensional vector of ones.\n\n**Validation:**\n- **Scientifically Grounded:** The problem is firmly rooted in the mathematical field of robust optimization, a sub-discipline of optimization methods. The concepts of $\\ell_p$-norms, dual norms, and robust counterparts are well-established.\n- **Well-Posed:** The problem is clearly stated and asks for a derivation of a specific result. It is self-contained and has a unique, well-defined solution.\n- **Objective:** The language is formal, mathematical, and free of any subjectivity or ambiguity.\n\nThe problem is valid as it satisfies all criteria for a well-posed scientific problem. It is not speculative, incomplete, contradictory, or trivial.\n\n### Step 2: Derivation of the Robust Counterpart\n\nThe robust feasibility requirement states that the inequality system $A x \\le b + \\Delta b$ must hold for all possible realizations of the uncertainty vector $\\Delta b$ within its defined set $U$.\nLet $a_i^T$ be the $i$-th row of the matrix $A$, so that $(Ax)_i = a_i^T x$. The robust feasibility condition can be written as:\n$$ a_i^T x \\le b_i + (\\Delta b)_i, \\quad \\forall i \\in \\{1, \\dots, m\\}, \\quad \\forall \\Delta b \\in U = \\{\\Delta b : \\|\\Delta b\\|_1 \\le \\rho\\} $$\nThe universal quantifier $\\forall \\Delta b \\in U$ applies to the entire set of $m$ inequalities. We can swap the order of the universal quantifiers $\\forall i$ and $\\forall \\Delta b$ without changing the logical statement. This allows us to decouple the problem and consider each constraint individually. For each constraint $i \\in \\{1, \\dots, m\\}$, the inequality $a_i^T x \\le b_i + (\\Delta b)_i$ must hold for all $\\Delta b$ in the set $U$.\n\nFor a fixed decision vector $x$ and a fixed constraint index $i$, the term $a_i^T x - b_i$ is a constant. The inequality must hold even for the \"worst-case\" choice of $\\Delta b \\in U$, which is the one that minimizes the right-hand side, $b_i + (\\Delta b)_i$. Therefore, for each $i$, we must satisfy:\n$$ a_i^T x \\le b_i + \\min_{\\Delta b \\in U} \\{(\\Delta b)_i\\} $$\nThe problem reduces to evaluating the minimum of the $i$-th component of $\\Delta b$ over the uncertainty set. Let $e_i \\in \\mathbb{R}^m$ be the $i$-th standard basis vector (a vector of zeros with a $1$ in the $i$-th position). Then $(\\Delta b)_i$ can be written as the inner product $e_i^T \\Delta b$. The optimization problem is:\n$$ \\min_{\\|\\Delta b\\|_1 \\le \\rho} e_i^T \\Delta b $$\nThis is equivalent to:\n$$ - \\max_{\\|\\Delta b\\|_1 \\le \\rho} (-e_i^T \\Delta b) = - \\max_{\\|\\Delta b\\|_1 \\le \\rho} \\Delta b^T (-e_i) $$\nWe now use the definition of a dual norm. The dual of the $\\ell_1$-norm is the $\\ell_\\infty$-norm, denoted $\\|\\cdot\\|_\\infty$. The duality relationship for a generic vector $v$ is given by:\n$$ \\sup_{\\|\\Delta b\\|_1 \\le \\rho} \\Delta b^T v = \\rho \\|v\\|_\\infty $$\nIn our case, the vector $v$ is $-e_i$. We compute its $\\ell_\\infty$-norm:\n$$ \\|-e_i\\|_\\infty = \\max_{j=1, \\dots, m} |(-e_i)_j| = 1 $$\nSubstituting this result back into our expression, we find the maximum:\n$$ \\max_{\\|\\Delta b\\|_1 \\le \\rho} \\Delta b^T (-e_i) = \\rho \\|-e_i\\|_\\infty = \\rho \\cdot 1 = \\rho $$\nTherefore, the minimum value we sought is:\n$$ \\min_{\\|\\Delta b\\|_1 \\le \\rho} (\\Delta b)_i = -\\rho $$\nThis minimum value is achieved by the specific perturbation $\\Delta b^* = -\\rho e_i$, which satisfies $\\|\\Delta b^*\\|_1 = |-\\rho| = \\rho$ and is thus an element of the uncertainty set $U$.\n\nSubstituting this minimum value back into the inequality for the $i$-th constraint gives its deterministic robust counterpart:\n$$ a_i^T x \\le b_i - \\rho $$\nSince this must hold for all $i \\in \\{1, \\dots, m\\}$, we can write the complete robust counterpart in vector form:\n$$ A x \\le b - \\rho\\,\\mathbf{1}_{m} $$\nwhere $\\mathbf{1}_{m} \\in \\mathbb{R}^{m}$ is the vector of all ones.\n\n### Step 3: Determination of $\\tau(\\rho)$\n\nThe problem asks for the robust counterpart to be expressed in the form $A x \\le b - \\tau(\\rho)\\,\\mathbf{1}_{m}$. By comparing this with our derived robust counterpart $A x \\le b - \\rho\\,\\mathbf{1}_{m}$, we can directly identify the closed-form expression for the safety margin $\\tau(\\rho)$.\n$$ \\tau(\\rho) = \\rho $$\n\n### Step 4: Explanation of Redundancy\n\nThe derived robust counterpart $A x \\le b - \\rho\\,\\mathbf{1}_{m}$ is the exact, non-conservative deterministic equivalent of the original uncertain problem. The \"redundancy\" mentioned in the problem statement refers to the interpretation of the structure of this counterpart in light of the shared uncertainty budget.\n\nOur derivation shows that the robust constraint for each row $i$ is $a_i^T x \\le b_i - \\rho$. This hardening of the constraint by an amount $\\rho$ is necessary to protect against the specific worst-case perturbation for that row, which is $\\Delta b = -\\rho e_i$. This perturbation directs the entire uncertainty budget $\\rho$ to adversely affect the $i$-th constraint while leaving all other constraints $j \\neq i$ unperturbed (since $(\\Delta b)_j=0$).\n\nThe shared budget constraint $\\|\\Delta b\\|_1 \\le \\rho$ creates a coupling between the perturbations affecting different inequalities. For instance, if the perturbation acting on constraint $i=1$ is large, say $|(\\Delta b)_1| = \\rho$, then it must be that $(\\Delta b)_j = 0$ for all $j > 1$. It is impossible for two constraints, say $i=1$ and $i=2$, to simultaneously experience their individual worst-case perturbations ($\\Delta b = -\\rho e_1$ and $\\Delta b = -\\rho e_2$, respectively), because both of these scenarios exhaust the entire shared budget.\n\nThe robust counterpart, being a set of $m$ inequalities $\\{a_i^T x \\le b_i - \\rho\\}_{i=1}^m$, effectively ensures that the solution $x$ is feasible against each of the $m$ mutually exclusive worst-case scenarios. The formulation is thus structurally redundant in the sense that it prepares for a set of $m$ extreme events, only one of which can possibly occur at any given time. The total \"cost\" of robustness, measured by the sum of the subtractions from the right-hand sides, is $m\\rho$, even though the total uncertainty \"supply\" is limited to $\\rho$. This illustrates how the shared budget induces a form of redundancy in the system of constraints that constitutes the robust counterpart.", "answer": "$$ \\boxed{\\rho} $$", "id": "3173984"}, {"introduction": "Simply acknowledging uncertainty is not enough; how we model it has profound consequences on the quality of our decisions. A naive model that ignores correlations between uncertain parameters can lead to overly conservative solutions, sacrificing performance for an unrealistic level of protection. This practice [@problem_id:3173992] provides a concrete counterexample, asking you to compare the optimal solution from a sophisticated ellipsoidal uncertainty set with one from a simple \"box\" model, quantifying the practical cost of ignoring structure in uncertainty.", "problem": "Consider the following Robust Optimization (RO) model with correlated coefficient uncertainty. You are given a two-variable linear program with an uncertain linear constraint. Let the decision vector be $x = (x_1, x_2)^\\top$ with $x_1 \\ge 0$ and $x_2 \\ge 0$. The nominal coefficient vector in the constraint is $a_0 = (1, 1)^\\top$, and the uncertain perturbation is a vector $z \\in \\mathbb{R}^2$. The objective is to maximize the sum of the decision variables. The uncertain constraint must hold for all perturbations $z$ in a specified uncertainty set:\n- True correlated uncertainty set (ellipsoidal): $U_{\\text{ellip}} = \\{ z \\in \\mathbb{R}^2 : z^\\top \\Sigma^{-1} z \\le 1 \\}$ with $\\Sigma = \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix}$ and $\\rho = -0.8$.\n- Naive interval-inflated uncertainty set (axis-aligned box): $U_{\\text{box}} = \\{ z \\in \\mathbb{R}^2 : |z_1| \\le 1, |z_2| \\le 1 \\}$, obtained by inflating to independent marginal intervals that bound $U_{\\text{ellip}}$.\n\nThe robust optimization problem is\nmaximize $x_1 + x_2$ subject to $(a_0 + z)^\\top x \\le b$ for all $z \\in U$ and $x_1 \\ge 0$, $x_2 \\ge 0$, with $b = 4$ and $U$ chosen as either $U_{\\text{ellip}}$ (true correlated robust counterpart) or $U_{\\text{box}}$ (naive interval inflation).\n\nStarting from the definition of robust feasibility, the properties of supremum over sets, and the Cauchy–Schwarz inequality, derive the two robust counterparts (one for $U_{\\text{ellip}}$ and one for $U_{\\text{box}}$). Then solve both robust optimization problems exactly and compute the ratio $R$ of the optimal objective value under naive interval inflation to the optimal objective value under the true correlated robust counterpart,\n$$\nR \\equiv \\frac{V_{\\text{box}}}{V_{\\text{ellip}}}.\n$$\nThis ratio exhibits a concrete counterexample where naive interval inflation is dominated by the true robust counterpart under correlated uncertainty.\n\nRound your final numerical answer for $R$ to four significant figures. No units are required.", "solution": "The problem asks us to analyze a two-variable linear program under uncertainty, derive its robust counterparts for two different uncertainty sets, solve these counterparts, and compute the ratio of their optimal objective values.\n\nThe robust optimization problem is given by:\n$$\n\\begin{aligned}\n\\text{maximize} \\quad & x_1 + x_2 \\\\\n\\text{subject to} \\quad & (a_0 + z)^\\top x \\le b \\quad \\forall z \\in U \\\\\n& x_1 \\ge 0, x_2 \\ge 0\n\\end{aligned}\n$$\nwhere the decision vector is $x = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$, the nominal coefficient vector is $a_0 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$, and the right-hand side is $b=4$.\n\nThe core of the problem lies in reformulating the uncertain constraint into a deterministic equivalent. The constraint must hold for all $z$ in the uncertainty set $U$:\n$$\n(a_0 + z)^\\top x \\le b \\quad \\forall z \\in U\n$$\nThis can be rewritten as:\n$$\na_0^\\top x + z^\\top x \\le b \\quad \\forall z \\in U\n$$\nThis is equivalent to ensuring that the nominal part plus the worst-case perturbation does not violate the bound:\n$$\na_0^\\top x + \\sup_{z \\in U} (z^\\top x) \\le b\n$$\nWe will now derive the robust counterpart for each specified uncertainty set $U$.\n\n**Part 1: Naive Interval Inflation ($U = U_{\\text{box}}$)**\n\nThe first uncertainty set is the axis-aligned box defined as:\n$$\nU_{\\text{box}} = \\{ z \\in \\mathbb{R}^2 : |z_1| \\le 1, |z_2| \\le 1 \\}\n$$\nWe need to compute the supremum term $\\sup_{z \\in U_{\\text{box}}} (z^\\top x)$.\n$$\n\\sup_{z \\in U_{\\text{box}}} (z^\\top x) = \\sup_{|z_1| \\le 1, |z_2| \\le 1} (z_1 x_1 + z_2 x_2)\n$$\nThe terms in the summation are separable:\n$$\n\\sup_{|z_1| \\le 1, |z_2| \\le 1} (z_1 x_1 + z_2 x_2) = \\sup_{|z_1| \\le 1} (z_1 x_1) + \\sup_{|z_2| \\le 1} (z_2 x_2)\n$$\nTo maximize $z_i x_i$ over $|z_i| \\le 1$, we should choose $z_i$ to have the same sign as $x_i$ and the maximum possible magnitude, which is $1$. Thus, the optimal choice for $z_i$ is $\\text{sgn}(x_i)$. The supremum of $z_i x_i$ is therefore $|\\text{sgn}(x_i) x_i| = |x_i|$.\nSo, we have:\n$$\n\\sup_{z \\in U_{\\text{box}}} (z^\\top x) = |x_1| + |x_2|\n$$\nSince the problem includes the constraints $x_1 \\ge 0$ and $x_2 \\ge 0$, this simplifies to:\n$$\n\\sup_{z \\in U_{\\text{box}}} (z^\\top x) = x_1 + x_2\n$$\nSubstituting this into the robust constraint formulation gives:\n$$\na_0^\\top x + (x_1 + x_2) \\le b\n$$\nWith $a_0 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ and $b=4$, the constraint becomes:\n$$\n(x_1 + x_2) + (x_1 + x_2) \\le 4\n$$\n$$\n2x_1 + 2x_2 \\le 4 \\implies x_1 + x_2 \\le 2\n$$\nThe robust optimization problem for the box uncertainty set is therefore a simple linear program:\n$$\n\\begin{aligned}\nV_{\\text{box}} = \\text{maximize} \\quad & x_1 + x_2 \\\\\n\\text{subject to} \\quad & x_1 + x_2 \\le 2 \\\\\n& x_1 \\ge 0, x_2 \\ge 0\n\\end{aligned}\n$$\nThe objective function is identical to the left-hand side of the main constraint. By inspection, the maximum value of $x_1 + x_2$ is $2$.\n$$\nV_{\\text{box}} = 2\n$$\n\n**Part 2: True Correlated Uncertainty ($U = U_{\\text{ellip}}$)**\n\nThe second uncertainty set is the ellipsoid defined by:\n$$\nU_{\\text{ellip}} = \\{ z \\in \\mathbb{R}^2 : z^\\top \\Sigma^{-1} z \\le 1 \\}\n$$\nwith $\\Sigma = \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix}$ and $\\rho = -0.8$. The matrix $\\Sigma$ is positive definite as its determinant is $1-\\rho^2 = 1 - (-0.8)^2 = 0.36 > 0$.\n\nWe need to compute the supremum term $\\sup_{z \\in U_{\\text{ellip}}} (z^\\top x)$. This is a standard result in robust optimization. The supremum of a linear function over an ellipsoid is given by the dual norm.\n$$\n\\sup_{z^\\top \\Sigma^{-1} z \\le 1} (z^\\top x) = \\sqrt{x^\\top \\Sigma x}\n$$\nTo show this, we can use the Cauchy-Schwarz inequality. Let $y = \\Sigma^{-1/2} z$ and $w = \\Sigma^{1/2} x$. Then $z = \\Sigma^{1/2} y$. The constraint $z^\\top \\Sigma^{-1} z \\le 1$ becomes $(\\Sigma^{1/2} y)^\\top \\Sigma^{-1} (\\Sigma^{1/2} y) = y^\\top \\Sigma^{1/2} \\Sigma^{-1} \\Sigma^{1/2} y = y^\\top y = \\|y\\|_2^2 \\le 1$. The expression $z^\\top x$ becomes $(\\Sigma^{1/2} y)^\\top x = y^\\top (\\Sigma^{1/2} x) = y^\\top w$.\nThe problem is now to find $\\sup_{\\|y\\|_2 \\le 1} (y^\\top w)$. By the Cauchy-Schwarz inequality, $y^\\top w \\le \\|y\\|_2 \\|w\\|_2$. Since $\\|y\\|_2 \\le 1$, we have $y^\\top w \\le \\|w\\|_2$. The supremum is achieved when $y$ is aligned with $w$ and has norm $1$, so $\\sup_{\\|y\\|_2 \\le 1} (y^\\top w) = \\|w\\|_2$.\nSubstituting back $w = \\Sigma^{1/2} x$:\n$$\n\\sup_{z \\in U_{\\text{ellip}}} (z^\\top x) = \\|\\Sigma^{1/2} x\\|_2 = \\sqrt{(\\Sigma^{1/2} x)^\\top (\\Sigma^{1/2} x)} = \\sqrt{x^\\top \\Sigma x}\n$$\nNow we compute the quadratic form $x^\\top \\Sigma x$:\n$$\nx^\\top \\Sigma x = \\begin{pmatrix} x_1 & x_2 \\end{pmatrix} \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = x_1^2 + 2\\rho x_1 x_2 + x_2^2\n$$\nThe robust constraint for the ellipsoidal set is:\n$$\na_0^\\top x + \\sqrt{x_1^2 + 2\\rho x_1 x_2 + x_2^2} \\le b\n$$\nWith $a_0 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ and $b=4$, we get:\n$$\nx_1 + x_2 + \\sqrt{x_1^2 + 2\\rho x_1 x_2 + x_2^2} \\le 4\n$$\nThe robust optimization problem is a second-order cone program (SOCP):\n$$\n\\begin{aligned}\nV_{\\text{ellip}} = \\text{maximize} \\quad & x_1 + x_2 \\\\\n\\text{subject to} \\quad & x_1 + x_2 + \\sqrt{x_1^2 + 2\\rho x_1 x_2 + x_2^2} \\le 4 \\\\\n& x_1 \\ge 0, x_2 \\ge 0\n\\end{aligned}\n$$\nThe objective function and the constraint function are symmetric with respect to $x_1$ and $x_2$. The feasible set is convex. Therefore, an optimal solution must exist on the line $x_1 = x_2$. Let us set $x_1 = x_2 = x^*$, with $x^* \\ge 0$.\nThe objective function becomes $2x^*$. The constraint becomes:\n$$\nx^* + x^* + \\sqrt{(x^*)^2 + 2\\rho(x^*)^2 + (x^*)^2} \\le 4\n$$\n$$\n2x^* + \\sqrt{2(x^*)^2 + 2\\rho(x^*)^2} \\le 4\n$$\nSince $x^* \\ge 0$, we can factor it out of the square root:\n$$\n2x^* + x^*\\sqrt{2 + 2\\rho} \\le 4\n$$\n$$\nx^* (2 + \\sqrt{2(1+\\rho)}) \\le 4\n$$\nTo maximize the objective $2x^*$, we must choose the largest possible $x^*$, so we take the equality:\n$$\nx^* = \\frac{4}{2 + \\sqrt{2(1+\\rho)}}\n$$\nThe optimal objective value is $V_{\\text{ellip}} = 2x^*$:\n$$\nV_{\\text{ellip}} = \\frac{8}{2 + \\sqrt{2(1+\\rho)}}\n$$\nWe are given $\\rho = -0.8$. Substituting this value:\n$$\n1 + \\rho = 1 - 0.8 = 0.2\n$$\n$$\n2(1 + \\rho) = 2(0.2) = 0.4\n$$\n$$\nV_{\\text{ellip}} = \\frac{8}{2 + \\sqrt{0.4}} = \\frac{8}{2 + \\sqrt{4/10}} = \\frac{8}{2 + 2/\\sqrt{10}} = \\frac{4}{1 + 1/\\sqrt{10}} = \\frac{4\\sqrt{10}}{1+\\sqrt{10}}\n$$\n\n**Part 3: Compute the Ratio $R$**\n\nWe are asked to compute the ratio $R = \\frac{V_{\\text{box}}}{V_{\\text{ellip}}}$.\nWe found $V_{\\text{box}} = 2$ and $V_{\\text{ellip}} = \\frac{8}{2 + \\sqrt{0.4}}$.\n$$\nR = \\frac{2}{\\frac{8}{2 + \\sqrt{0.4}}} = \\frac{2(2 + \\sqrt{0.4})}{8} = \\frac{2 + \\sqrt{0.4}}{4}\n$$\nThis can be written as:\n$$\nR = \\frac{1}{2} + \\frac{\\sqrt{0.4}}{4} = \\frac{1}{2} + \\frac{\\sqrt{4/10}}{4} = \\frac{1}{2} + \\frac{2/\\sqrt{10}}{4} = \\frac{1}{2} + \\frac{1}{2\\sqrt{10}} = \\frac{1}{2} \\left( 1 + \\frac{1}{\\sqrt{10}} \\right)\n$$\nNow we compute the numerical value and round to four significant figures:\n$$\n\\sqrt{10} \\approx 3.16227766...\n$$\n$$\n\\frac{1}{\\sqrt{10}} \\approx 0.316227766...\n$$\n$$\nR \\approx \\frac{1}{2} (1 + 0.316227766) = \\frac{1}{2} (1.316227766) \\approx 0.658113883...\n$$\nRounding to four significant figures gives $R \\approx 0.6581$.\n\nThe ratio being less than $1$ indicates that the naive interval inflation model is overly conservative compared to the true correlated model. By ignoring the negative correlation between the uncertain parameters, the box uncertainty set includes unrealistic scenarios (like both parameters taking their maximum unfavorable values simultaneously), leading to a much more constrained problem and a lower optimal objective value.", "answer": "$$\n\\boxed{0.6581}\n$$", "id": "3173992"}, {"introduction": "After learning to make decisions that are robust to a given level of uncertainty, a natural and critical question arises: what is the limit of our system's resilience? This exercise [@problem_id:3174001] challenges you to find this breaking point by calculating the maximum \"radius of robustness\"—the largest amount of uncertainty the system can tolerate before no feasible solution exists at all. Solving this problem provides deep insight into the fundamental trade-off between performance and robustness.", "problem": "Consider an uncertain linear system with constraints of the form $A x \\le b$, where the matrix $A$ is not known exactly but is known to lie in an uncertainty set centered at a nominal matrix $\\bar{A}$. Let the uncertainty set be defined entrywise as\n$$\n\\mathcal{U}(\\rho) = \\{A \\in \\mathbb{R}^{2 \\times 2} : \\|A - \\bar{A}\\|_{\\infty} \\le \\rho\\},\n$$\nwhere $\\|\\cdot\\|_{\\infty}$ denotes the elementwise maximum norm, that is, $\\|M\\|_{\\infty} = \\max_{i,j} |M_{ij}|$. You are given the nominal matrix\n$$\n\\bar{A} = \\begin{pmatrix} 2 & 5 \\\\ 4 & 1 \\end{pmatrix},\n$$\nand the right-hand side vector\n$$\nb = \\begin{pmatrix} 7 \\\\ 6 \\end{pmatrix}.\n$$\nTo rule out trivial solutions and to ensure scientific realism, suppose the decision vector $x \\in \\mathbb{R}^{2}$ is constrained to lie in the probability simplex\n$$\n\\mathcal{X} = \\{x \\in \\mathbb{R}^{2} : x \\ge 0, \\ \\mathbf{1}^{\\top} x = 1\\},\n$$\nwhere $\\mathbf{1}$ denotes the all-ones vector and inequalities are understood componentwise.\n\nUsing only fundamental definitions of norms and worst-case analysis (without invoking pre-derived robust counterparts), determine the largest uncertainty radius $\\rho \\ge 0$ such that the robust feasible set\n$$\n\\{ x \\in \\mathcal{X} : A x \\le b \\text{ for all } A \\in \\mathcal{U}(\\rho)\\}\n$$\nis nonempty. Express your final answer as a single real number. No rounding is required, and no physical units apply. Do not provide an inequality or an equation as your final answer; provide only the number.", "solution": "The problem requires finding the largest uncertainty radius $\\rho \\ge 0$ for which the robust feasible set is nonempty. The robust feasible set is defined as the set of all $x$ in the probability simplex $\\mathcal{X}$ that satisfy the constraint $A x \\le b$ for all possible matrices $A$ within the specified uncertainty set $\\mathcal{U}(\\rho)$.\n\nFirst, we must formalize the condition that $A x \\le b$ holds for all $A \\in \\mathcal{U}(\\rho)$. This is a set of infinitely many constraints, which must be converted into a tractable, finite set of constraints. This is known as deriving the robust counterpart of the uncertain constraints.\n\nThe inequality $A x \\le b$ represents a system of two linear inequalities, as $A$ is a $2 \\times 2$ matrix. Let us consider the $i$-th inequality, where $i \\in \\{1, 2\\}$. Let $a_i^\\top$ be the $i$-th row of the matrix $A$. The $i$-th inequality is $a_i^\\top x \\le b_i$. For this to hold for all possible choices of $a_i^\\top$ from its uncertainty set, the maximum possible value of the left-hand side must be less than or equal to $b_i$.\n$$\n\\max_{A \\in \\mathcal{U}(\\rho)} (a_i^\\top x) \\le b_i\n$$\nThe uncertainty set $\\mathcal{U}(\\rho)$ is defined by $\\|A - \\bar{A}\\|_{\\infty} \\le \\rho$, which means $|A_{ij} - \\bar{A}_{ij}| \\le \\rho$ for all $i,j \\in \\{1, 2\\}$. Let $\\bar{a}_i^\\top$ be the $i$-th row of the nominal matrix $\\bar{A}$. We can express any matrix $A \\in \\mathcal{U}(\\rho)$ as $A = \\bar{A} + \\Delta$, where $\\|\\Delta\\|_{\\infty} = \\max_{i,j} |\\Delta_{ij}| \\le \\rho$. Thus, the $i$-th row $a_i^\\top$ can be written as $a_i^\\top = \\bar{a}_i^\\top + \\delta_i^\\top$, where $\\delta_i^\\top$ is the $i$-th row of $\\Delta$, and its components $\\delta_{ij}$ satisfy $|\\delta_{ij}| \\le \\rho$.\n\nThe maximization problem for the $i$-th constraint becomes:\n$$\n\\max_{\\|\\Delta\\|_{\\infty} \\le \\rho} (\\bar{a}_i^\\top + \\delta_i^\\top) x = \\bar{a}_i^\\top x + \\max_{|\\delta_{ij}| \\le \\rho \\text{ for } j=1,2} (\\delta_i^\\top x)\n$$\nThe term to maximize is $\\delta_i^\\top x = \\sum_{j=1}^{2} \\delta_{ij} x_j$. To maximize this sum subject to the independent constraints $|\\delta_{ij}| \\le \\rho$, we should choose each $\\delta_{ij}$ to have the maximum possible magnitude, $\\rho$, and the same sign as the corresponding $x_j$. So, the optimal choice is $\\delta_{ij} = \\rho \\cdot \\text{sgn}(x_j)$.\nThe maximum value is therefore:\n$$\n\\sum_{j=1}^{2} (\\rho \\cdot \\text{sgn}(x_j)) x_j = \\rho \\sum_{j=1}^{2} \\text{sgn}(x_j) x_j = \\rho \\sum_{j=1}^{2} |x_j| = \\rho \\|x\\|_1\n$$\nThe problem specifies that the decision vector $x$ must lie in the probability simplex $\\mathcal{X} = \\{x \\in \\mathbb{R}^{2} : x \\ge 0, \\ \\mathbf{1}^{\\top} x = 1\\}$. For any $x$ in this set, its components $x_1$ and $x_2$ are non-negative. Therefore, $|x_1| = x_1$ and $|x_2| = x_2$. The $L_1$-norm of $x$ is then:\n$$\n\\|x\\|_1 = |x_1| + |x_2| = x_1 + x_2\n$$\nFrom the definition of $\\mathcal{X}$, we have $x_1 + x_2 = 1$. Thus, for any $x \\in \\mathcal{X}$, $\\|x\\|_1 = 1$.\n\nSubstituting this result back into the robust counterpart derivation, the $i$-th robust constraint becomes:\n$$\n\\bar{a}_i^\\top x + \\rho \\|x\\|_1 \\le b_i \\implies \\bar{a}_i^\\top x + \\rho \\le b_i\n$$\nNow, we apply this to the specific data given in the problem:\n$\\bar{A} = \\begin{pmatrix} 2 & 5 \\\\ 4 & 1 \\end{pmatrix}$ and $b = \\begin{pmatrix} 7 \\\\ 6 \\end{pmatrix}$.\n\nFor $i=1$:\n$2x_1 + 5x_2 + \\rho \\le 7$\n\nFor $i=2$:\n$4x_1 + 1x_2 + \\rho \\le 6$\n\nThe problem is to find the largest $\\rho \\ge 0$ for which there exists a vector $x = (x_1, x_2)^\\top$ satisfying:\n1. $x_1 \\ge 0, x_2 \\ge 0$\n2. $x_1 + x_2 = 1$\n3. $2x_1 + 5x_2 + \\rho \\le 7$\n4. $4x_1 + x_2 + \\rho \\le 6$\n\nThese last two inequalities can be rewritten as:\n$\\rho \\le 7 - (2x_1 + 5x_2)$\n$\\rho \\le 6 - (4x_1 + x_2)$\n\nFor a feasible solution $(x, \\rho)$ to exist, $\\rho$ must be less than or equal to both right-hand side expressions. We are looking for the maximum $\\rho$, which means we should seek to maximize the minimum of these two expressions over all feasible $x$.\n$$\n\\rho_{\\text{max}} = \\max_{x \\in \\mathcal{X}} \\min(7 - (2x_1 + 5x_2), 6 - (4x_1 + x_2))\n$$\nWe can eliminate $x_2$ using the constraint $x_2 = 1 - x_1$. The constraint $x \\in \\mathcal{X}$ becomes $x_1 \\in [0, 1]$.\nLet's substitute $x_2 = 1-x_1$ into the two expressions:\n$f_1(x_1) = 7 - (2x_1 + 5(1-x_1)) = 7 - (2x_1 + 5 - 5x_1) = 7 - (5 - 3x_1) = 2 + 3x_1$.\n$f_2(x_1) = 6 - (4x_1 + (1-x_1)) = 6 - (4x_1 + 1 - x_1) = 6 - (1 + 3x_1) = 5 - 3x_1$.\n\nThe problem is now reduced to:\n$$\n\\rho_{\\text{max}} = \\max_{x_1 \\in [0, 1]} \\min(2 + 3x_1, 5 - 3x_1)\n$$\nThe function $g(x_1) = \\min(2 + 3x_1, 5 - 3x_1)$ is maximized when the two arguments of the $\\min$ function are equal. This is because $2+3x_1$ is an increasing function of $x_1$ and $5-3x_1$ is a decreasing function of $x_1$. The maximum of their minimum will occur at their intersection.\nSet the two expressions equal to find the value of $x_1$ that maximizes the minimum:\n$$\n2 + 3x_1 = 5 - 3x_1\n$$\n$$\n6x_1 = 3\n$$\n$$\nx_1 = \\frac{3}{6} = \\frac{1}{2} = 0.5\n$$\nThis value $x_1 = 0.5$ lies in the interval $[0, 1]$, so it is a valid candidate. At this point, the corresponding maximum value of $\\rho$ is:\n$$\n\\rho = 2 + 3(0.5) = 2 + 1.5 = 3.5\n$$\nChecking with the other expression:\n$$\n\\rho = 5 - 3(0.5) = 5 - 1.5 = 3.5\n$$\nThe values match. Thus, the largest uncertainty radius for which the robust feasible set is nonempty is $3.5$. For any $\\rho > 3.5$, the set of feasible $x_1$ would be empty. For instance, if $\\rho=4$, the constraints would be $3x_1 \\ge 4-2=2$ and $3x_1 \\le 5-4=1$, i.e., $x_1 \\ge 2/3$ and $x_1 \\le 1/3$, which is impossible.\nThe maximum value of $\\rho$ is $3.5$.", "answer": "$$\\boxed{3.5}$$", "id": "3174001"}]}