{"hands_on_practices": [{"introduction": "Theoretical analyses of Online Gradient Descent (OGD) often rely on a convenient assumption: that the gradients of the loss functions are uniformly bounded. This practice problem [@problem_id:3159395] confronts the practical reality where this may not hold, introducing the widely-used technique of gradient clipping. By deriving the regret bound for an OGD algorithm that incorporates clipping, you will gain a deeper understanding of the trade-off between enforcing theoretical assumptions and the bias introduced by altering the true gradients.", "problem": "Consider Online Gradient Descent (OGD) for Online Convex Optimization over a closed convex set $\\mathcal{K} \\subset \\mathbb{R}^{d}$ with Euclidean diameter $D$, meaning that for all $\\mathbf{x}, \\mathbf{y} \\in \\mathcal{K}$, $\\|\\mathbf{x} - \\mathbf{y}\\| \\le D$. At each round $t \\in \\{1,2,\\dots,T\\}$, a convex loss $f_{t} : \\mathcal{K} \\to \\mathbb{R}$ is revealed after choosing $\\mathbf{x}_{t} \\in \\mathcal{K}$, and we define the regret against a fixed comparator $\\mathbf{x}^{\\star} \\in \\mathcal{K}$ by\n$$\nR_{T} \\triangleq \\sum_{t=1}^{T} \\big( f_{t}(\\mathbf{x}_{t}) - f_{t}(\\mathbf{x}^{\\star}) \\big).\n$$\nAssume access to gradients $\\mathbf{g}_{t} \\triangleq \\nabla f_{t}(\\mathbf{x}_{t})$, but to enforce a bounded-gradient assumption used in regret proofs, you apply gradient clipping at threshold $C > 0$ by defining the clipped gradient\n$$\n\\mathbf{u}_{t} \\triangleq \\begin{cases}\n\\mathbf{g}_{t}, & \\text{if } \\|\\mathbf{g}_{t}\\| \\le C, \\\\\nC \\, \\dfrac{\\mathbf{g}_{t}}{\\|\\mathbf{g}_{t}\\|}, & \\text{if } \\|\\mathbf{g}_{t}\\| > C.\n\\end{cases}\n$$\nThe OGD update uses the clipped gradient:\n$$\n\\mathbf{x}_{t+1} \\triangleq \\Pi_{\\mathcal{K}}\\big( \\mathbf{x}_{t} - \\eta \\mathbf{u}_{t} \\big),\n$$\nwhere $\\Pi_{\\mathcal{K}}$ denotes Euclidean projection onto $\\mathcal{K}$ and $\\eta > 0$ is the step size.\n\nStarting from fundamental definitions and properties (convexity: $f_{t}(\\mathbf{x}_{t}) - f_{t}(\\mathbf{x}^{\\star}) \\le \\langle \\nabla f_{t}(\\mathbf{x}_{t}), \\mathbf{x}_{t} - \\mathbf{x}^{\\star} \\rangle$, the non-expansiveness of Euclidean projection, and the Cauchy–Schwarz inequality), derive an upper bound on $R_{T}$ that explicitly captures the trade-off between the bounded-gradient term enforced by clipping and the clipping-induced bias. Assume the following data model for the gradients to quantify the bias: there are exactly $k$ rounds on which $\\|\\mathbf{g}_{t}\\| = G$ with $G > C$, and on all other rounds $\\|\\mathbf{g}_{t}\\| \\le C$. For those $k$ rounds, the clipping induces a difference $\\|\\mathbf{g}_{t} - \\mathbf{u}_{t}\\| = G - C$, and you may upper bound the inner-product contribution of clipping bias by $D (G - C)$ per such round using the diameter of $\\mathcal{K}$.\n\nChoose $\\eta$ to minimize your derived upper bound with respect to $\\eta$, and report the minimized upper bound for $R_{T}$ as a single closed-form analytic expression in $D$, $C$, $T$, $k$, and $G$. No numerical evaluation is required, and no units are involved. The final answer must be the single expression only.", "solution": "The problem asks for an upper bound on the regret $R_{T}$ for Online Gradient Descent (OGD) with gradient clipping. The derivation must start from fundamental principles and account for the bias introduced by clipping. The final bound must be minimized with respect to the step size $\\eta$.\n\nThe regret is defined as $R_{T} \\triangleq \\sum_{t=1}^{T} \\big( f_{t}(\\mathbf{x}_{t}) - f_{t}(\\mathbf{x}^{\\star}) \\big)$.\nSince each loss function $f_t$ is convex, we can apply the first-order condition for convexity, which states that for any $\\mathbf{x}, \\mathbf{y}$ in the domain, $f_t(\\mathbf{x}) - f_t(\\mathbf{y}) \\le \\langle \\nabla f_t(\\mathbf{x}), \\mathbf{x} - \\mathbf{y} \\rangle$. Applying this to our context with $\\mathbf{g}_{t} \\triangleq \\nabla f_{t}(\\mathbf{x}_{t})$:\n$$\nf_{t}(\\mathbf{x}_{t}) - f_{t}(\\mathbf{x}^{\\star}) \\le \\langle \\mathbf{g}_{t}, \\mathbf{x}_{t} - \\mathbf{x}^{\\star} \\rangle.\n$$\nSumming over all rounds from $t=1$ to $T$:\n$$\nR_{T} \\le \\sum_{t=1}^{T} \\langle \\mathbf{g}_{t}, \\mathbf{x}_{t} - \\mathbf{x}^{\\star} \\rangle.\n$$\nThe OGD update rule uses the clipped gradient $\\mathbf{u}_t$, not the true gradient $\\mathbf{g}_t$. To analyze the algorithm's performance, we must introduce $\\mathbf{u}_t$ into the expression. We decompose the inner product as follows:\n$$\n\\langle \\mathbf{g}_{t}, \\mathbf{x}_{t} - \\mathbf{x}^{\\star} \\rangle = \\langle \\mathbf{u}_{t} + (\\mathbf{g}_{t} - \\mathbf{u}_{t}), \\mathbf{x}_{t} - \\mathbf{x}^{\\star} \\rangle = \\langle \\mathbf{u}_{t}, \\mathbf{x}_{t} - \\mathbf{x}^{\\star} \\rangle + \\langle \\mathbf{g}_{t} - \\mathbf{u}_{t}, \\mathbf{x}_{t} - \\mathbf{x}^{\\star} \\rangle.\n$$\nThe first term relates to the algorithm's update, while the second term is the bias introduced by clipping. Thus, the regret is bounded by the sum of two components:\n$$\nR_{T} \\le \\sum_{t=1}^{T} \\langle \\mathbf{u}_{t}, \\mathbf{x}_{t} - \\mathbf{x}^{\\star} \\rangle + \\sum_{t=1}^{T} \\langle \\mathbf{g}_{t} - \\mathbf{u}_{t}, \\mathbf{x}_{t} - \\mathbf{x}^{\\star} \\rangle.\n$$\nLet's analyze each component separately.\n\nFirst, consider the term involving the bias, $\\sum_{t=1}^{T} \\langle \\mathbf{g}_{t} - \\mathbf{u}_{t}, \\mathbf{x}_{t} - \\mathbf{x}^{\\star} \\rangle$.\nAccording to the problem's data model, there are two types of rounds. For the $T-k$ rounds where $\\|\\mathbf{g}_{t}\\| \\le C$, the clipping is not active, so $\\mathbf{u}_{t} = \\mathbf{g}_{t}$, which implies $\\mathbf{g}_{t} - \\mathbf{u}_{t} = \\mathbf{0}$. The contribution from these rounds is zero.\nFor the $k$ rounds where $\\|\\mathbf{g}_{t}\\| = G > C$, the problem provides an explicit upper bound on the inner product: $\\langle \\mathbf{g}_{t} - \\mathbf{u}_{t}, \\mathbf{x}_{t} - \\mathbf{x}^{\\star} \\rangle \\le D (G - C)$.\nSumming over all $T$ rounds, the total contribution from the bias term is bounded by:\n$$\n\\sum_{t=1}^{T} \\langle \\mathbf{g}_{t} - \\mathbf{u}_{t}, \\mathbf{x}_{t} - \\mathbf{x}^{\\star} \\rangle \\le k D (G - C).\n$$\n\nNext, we analyze the standard OGD term, $\\sum_{t=1}^{T} \\langle \\mathbf{u}_{t}, \\mathbf{x}_{t} - \\mathbf{x}^{\\star} \\rangle$. We use a potential function argument based on the squared Euclidean distance to the comparator, $\\|\\mathbf{x}_{t} - \\mathbf{x}^{\\star}\\|^{2}$. The update rule is $\\mathbf{x}_{t+1} = \\Pi_{\\mathcal{K}}(\\mathbf{x}_{t} - \\eta \\mathbf{u}_{t})$.\nLet's analyze the distance at step $t+1$:\n$$\n\\|\\mathbf{x}_{t+1} - \\mathbf{x}^{\\star}\\|^{2} = \\|\\Pi_{\\mathcal{K}}(\\mathbf{x}_{t} - \\eta \\mathbf{u}_{t}) - \\mathbf{x}^{\\star}\\|^{2}.\n$$\nSince $\\mathbf{x}^{\\star} \\in \\mathcal{K}$, we can use the non-expansiveness property of Euclidean projection $\\Pi_{\\mathcal{K}}$, which states that for any vector $\\mathbf{z}$ and any point $\\mathbf{y} \\in \\mathcal{K}$, $\\|\\Pi_{\\mathcal{K}}(\\mathbf{z}) - \\mathbf{y}\\| \\le \\|\\mathbf{z} - \\mathbf{y}\\|$. Applying this with $\\mathbf{z} = \\mathbf{x}_{t} - \\eta \\mathbf{u}_{t}$ and $\\mathbf{y} = \\mathbf{x}^{\\star}$:\n$$\n\\|\\mathbf{x}_{t+1} - \\mathbf{x}^{\\star}\\|^{2} \\le \\|\\mathbf{x}_{t} - \\eta \\mathbf{u}_{t} - \\mathbf{x}^{\\star}\\|^{2} = \\|(\\mathbf{x}_{t} - \\mathbf{x}^{\\star}) - \\eta \\mathbf{u}_{t}\\|^{2}.\n$$\nExpanding the squared norm:\n$$\n\\|\\mathbf{x}_{t+1} - \\mathbf{x}^{\\star}\\|^{2} \\le \\|\\mathbf{x}_{t} - \\mathbf{x}^{\\star}\\|^{2} - 2\\eta \\langle \\mathbf{u}_{t}, \\mathbf{x}_{t} - \\mathbf{x}^{\\star} \\rangle + \\eta^{2} \\|\\mathbf{u}_{t}\\|^{2}.\n$$\nRearranging this inequality to isolate the inner product term, we get:\n$$\n\\langle \\mathbf{u}_{t}, \\mathbf{x}_{t} - \\mathbf{x}^{\\star} \\rangle \\le \\frac{1}{2\\eta} \\left( \\|\\mathbf{x}_{t} - \\mathbf{x}^{\\star}\\|^{2} - \\|\\mathbf{x}_{t+1} - \\mathbf{x}^{\\star}\\|^{2} \\right) + \\frac{\\eta}{2} \\|\\mathbf{u}_{t}\\|^{2}.\n$$\nSumming this inequality from $t=1$ to $T$:\n$$\n\\sum_{t=1}^{T} \\langle \\mathbf{u}_{t}, \\mathbf{x}_{t} - \\mathbf{x}^{\\star} \\rangle \\le \\frac{1}{2\\eta} \\sum_{t=1}^{T} \\left( \\|\\mathbf{x}_{t} - \\mathbf{x}^{\\star}\\|^{2} - \\|\\mathbf{x}_{t+1} - \\mathbf{x}^{\\star}\\|^{2} \\right) + \\frac{\\eta}{2} \\sum_{t=1}^{T} \\|\\mathbf{u}_{t}\\|^{2}.\n$$\nThe first sum on the right-hand side is a telescoping series:\n$$\n\\sum_{t=1}^{T} \\left( \\|\\mathbf{x}_{t} - \\mathbf{x}^{\\star}\\|^{2} - \\|\\mathbf{x}_{t+1} - \\mathbf{x}^{\\star}\\|^{2} \\right) = \\|\\mathbf{x}_{1} - \\mathbf{x}^{\\star}\\|^{2} - \\|\\mathbf{x}_{T+1} - \\mathbf{x}^{\\star}\\|^{2}.\n$$\nSince $\\mathbf{x}_{1}, \\mathbf{x}^{\\star} \\in \\mathcal{K}$, the distance $\\|\\mathbf{x}_{1} - \\mathbf{x}^{\\star}\\|$ is bounded by the diameter $D$ of the set $\\mathcal{K}$, so $\\|\\mathbf{x}_{1} - \\mathbf{x}^{\\star}\\|^{2} \\le D^{2}$. Also, $\\|\\mathbf{x}_{T+1} - \\mathbf{x}^{\\star}\\|^{2} \\ge 0$. Therefore, the telescoping sum is bounded above by $D^{2}$.\nFor the second sum, we use the property of the clipped gradient: by construction, $\\|\\mathbf{u}_{t}\\| \\le C$ for all $t$. Thus, $\\|\\mathbf{u}_{t}\\|^{2} \\le C^{2}$, and\n$$\n\\sum_{t=1}^{T} \\|\\mathbf{u}_{t}\\|^{2} \\le \\sum_{t=1}^{T} C^{2} = T C^{2}.\n$$\nSubstituting these bounds, we get:\n$$\n\\sum_{t=1}^{T} \\langle \\mathbf{u}_{t}, \\mathbf{x}_{t} - \\mathbf{x}^{\\star} \\rangle \\le \\frac{D^{2}}{2\\eta} + \\frac{\\eta T C^{2}}{2}.\n$$\nCombining the bounds for the two components of the regret:\n$$\nR_{T} \\le \\left( \\frac{D^{2}}{2\\eta} + \\frac{\\eta T C^{2}}{2} \\right) + k D (G - C).\n$$\nThis expression is our upper bound on the regret, which is a function of the step size $\\eta$. To find the tightest bound, we must choose $\\eta$ to minimize this expression. The term $k D (G-C)$ is constant with respect to $\\eta$. We need to minimize $f(\\eta) = \\frac{D^{2}}{2\\eta} + \\frac{\\eta T C^{2}}{2}$.\nWe take the derivative with respect to $\\eta$ and set it to $0$:\n$$\n\\frac{d f}{d \\eta} = -\\frac{D^{2}}{2\\eta^{2}} + \\frac{T C^{2}}{2} = 0.\n$$\nSolving for $\\eta$:\n$$\n\\frac{T C^{2}}{2} = \\frac{D^{2}}{2\\eta^{2}} \\implies \\eta^{2} = \\frac{D^{2}}{T C^{2}} \\implies \\eta^{\\star} = \\frac{D}{C\\sqrt{T}}.\n$$\nWe choose the positive root since $\\eta > 0$. The second derivative $\\frac{d^{2}f}{d\\eta^{2}} = \\frac{D^{2}}{\\eta^{3}}$ is positive for $\\eta>0$, confirming this is a minimum.\nFinally, we substitute the optimal step size $\\eta^{\\star}$ back into the regret bound:\n$$\nR_{T} \\le \\frac{D^{2}}{2 \\left(\\frac{D}{C\\sqrt{T}}\\right)} + \\frac{\\left(\\frac{D}{C\\sqrt{T}}\\right) T C^{2}}{2} + k D (G - C)\n$$\n$$\nR_{T} \\le \\frac{D^{2} C \\sqrt{T}}{2D} + \\frac{D T C^{2}}{2 C \\sqrt{T}} + k D (G - C)\n$$\n$$\nR_{T} \\le \\frac{D C \\sqrt{T}}{2} + \\frac{D C \\sqrt{T}}{2} + k D (G - C)\n$$\n$$\nR_{T} \\le D C \\sqrt{T} + k D (G - C).\n$$\nThis is the minimized upper bound for the regret $R_T$.", "answer": "$$\\boxed{D C \\sqrt{T} + k D (G - C)}$$", "id": "3159395"}, {"introduction": "While Online Gradient Descent is powerful, its updates are inherently Euclidean. This practice problem [@problem_id:3159427] challenges you to step into a non-Euclidean world by working with the probability simplex, a cornerstone of many machine learning models. You will explore the Follow-the-Regularized-Leader (FTRL) algorithm with entropic regularization, deriving the classic multiplicative weights update and seeing firsthand how choosing the right algorithm-geometry pairing leads to more natural and effective learning rules.", "problem": "Consider an Online Convex Optimization (OCO) problem over the $3$-dimensional probability simplex $\\Delta_{3}=\\{x\\in\\mathbb{R}^{3}: x_{i}\\geq 0,\\ \\sum_{i=1}^{3}x_{i}=1\\}$. At each round $t\\in\\{1,2,3,4,5\\}$, a learner chooses a decision $x_{t}\\in\\Delta_{3}$, and an adversary reveals a linear loss function $f_{t}(x)=\\ell_{t}\\cdot x$ where $\\ell_{t}\\in\\mathbb{R}^{3}$. The learner uses Follow-The-Regularized-Leader (FTRL) with entropic regularization, specifically the negative Shannon entropy regularizer $R(x)=\\sum_{i=1}^{3} x_{i}\\ln x_{i}$, and learning-rate parameter $\\eta>0$. The FTRL decision at round $t+1$ is the solution to the convex program\n$$\nx_{t+1}\\in\\arg\\min_{x\\in\\Delta_{3}}\\left\\{\\eta\\sum_{s=1}^{t}(\\ell_{s}\\cdot x)+R(x)\\right\\}.\n$$\nSuppose an adversary chooses a sequence of losses that align with the corners of the domain in the following way: for a fixed magnitude $M>0$,\n$$\n\\ell_{1}=(M,0,0),\\quad \\ell_{2}=(M,0,0),\\quad \\ell_{3}=(0,M,0),\\quad \\ell_{4}=(0,M,0),\\quad \\ell_{5}=(0,0,M).\n$$\nStarting from no prior losses, use the OCO and FTRL definitions above and principles of convex optimization to derive the explicit analytic expression for the learner’s decision $x_{6}$ produced by the entropic FTRL update after these five rounds. Your final answer must be a single closed-form expression for $x_{6}$ as a point in $\\Delta_{3}$ written in terms of $\\eta$ and $M$. No rounding is required.", "solution": "The user wants to find the learner's decision vector $x_{6}$ after $5$ rounds of an Online Convex Optimization problem. The learner uses the Follow-The-Regularized-Leader (FTRL) algorithm with a negative Shannon entropy regularizer.\n\nThe decision $x_{t+1}$ is determined by solving the following optimization problem:\n$$\nx_{t+1} = \\arg\\min_{x\\in\\Delta_{3}}\\left\\{\\eta\\sum_{s=1}^{t}(\\ell_{s}\\cdot x)+R(x)\\right\\}\n$$\nwhere $\\Delta_{3}=\\{x\\in\\mathbb{R}^{3}: x_{i}\\geq 0,\\ \\sum_{i=1}^{3}x_{i}=1\\}$ is the $3$-dimensional probability simplex, $R(x)=\\sum_{i=1}^{3} x_{i}\\ln x_{i}$ is the regularizer, and $\\eta > 0$ is the learning rate.\n\nWe need to compute $x_{6}$, which means we sum the losses from $t=1$ to $t=5$. Let $L_{5}$ be the cumulative loss vector, $L_{5} = \\sum_{s=1}^{5} \\ell_{s}$. The given loss vectors are:\n$$\n\\ell_{1}=(M,0,0) \\\\\n\\ell_{2}=(M,0,0) \\\\\n\\ell_{3}=(0,M,0) \\\\\n\\ell_{4}=(0,M,0) \\\\\n\\ell_{5}=(0,0,M)\n$$\nSumming these vectors component-wise, we get:\n$$\nL_{5} = (M+M, M+M, M) = (2M, 2M, M)\n$$\nThe optimization problem for $x_{6}$ is:\n$$\nx_{6} = \\arg\\min_{x\\in\\Delta_{3}}\\left\\{\\eta (L_{5} \\cdot x) + R(x)\\right\\}\n$$\nLet's write out the objective function, which we denote as $F(x)$:\n$$\nF(x) = \\eta (2Mx_{1} + 2Mx_{2} + Mx_{3}) + x_{1}\\ln x_{1} + x_{2}\\ln x_{2} + x_{3}\\ln x_{3}\n$$\nThis is a convex optimization problem with constraints $x_{1}+x_{2}+x_{3}=1$ and $x_{i} \\geq 0$ for $i \\in \\{1, 2, 3\\}$. We use the method of Lagrange multipliers to find the solution. The Lagrangian function $\\mathcal{L}$ is:\n$$\n\\mathcal{L}(x, \\nu, \\lambda_{1}, \\lambda_{2}, \\lambda_{3}) = F(x) + \\nu\\left(\\sum_{i=1}^{3}x_{i} - 1\\right) - \\sum_{i=1}^{3}\\lambda_{i}x_{i}\n$$\nwhere $\\nu$ is the Lagrange multiplier for the equality constraint and $\\lambda_i$ are the multipliers for the non-negativity constraints $x_i \\geq 0$.\n\nThe Karush-Kuhn-Tucker (KKT) conditions must be satisfied at the minimum. The stationarity condition requires setting the gradient of the Lagrangian with respect to $x$ to zero. For each component $x_{i}$:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial x_{i}} = \\eta (L_{5})_{i} + (\\ln x_{i} + 1) + \\nu - \\lambda_{i} = 0\n$$\nwhere $(L_{5})_{i}$ is the $i$-th component of the vector $L_{5}$.\n\nThe regularizer $R(x)$ is strictly convex and its value approaches $+\\infty$ as any $x_i \\to 0^+$ (since $\\lim_{z\\to 0^+} z\\ln z = 0$ but its derivative $\\ln z + 1 \\to -\\infty$). This acts as a barrier, ensuring that for finite loss values, the optimal solution will be in the interior of the simplex, i.e., $x_{i} > 0$ for all $i$.\nAccording to the KKT complementary slackness condition, $\\lambda_{i}x_{i} = 0$. Since $x_{i} > 0$, we must have $\\lambda_{i} = 0$ for $i=1, 2, 3$.\n\nThe stationarity conditions thus simplify to:\n$$\n\\eta (L_{5})_{i} + \\ln x_{i} + 1 + \\nu = 0\n$$\nSolving for $x_{i}$:\n$$\n\\ln x_{i} = -\\eta (L_{5})_{i} - 1 - \\nu\n$$\n$$\nx_{i} = \\exp(-\\eta (L_{5})_{i} - 1 - \\nu) = \\exp(-\\eta (L_{5})_{i}) \\exp(-1-\\nu)\n$$\nThe term $\\exp(-1-\\nu)$ is a constant for all $i$. We can find its value using the simplex constraint $\\sum_{i=1}^{3}x_{i} = 1$:\n$$\n\\sum_{i=1}^{3} \\exp(-\\eta (L_{5})_{i}) \\exp(-1-\\nu) = 1\n$$\n$$\n\\exp(-1-\\nu) \\left( \\sum_{j=1}^{3} \\exp(-\\eta (L_{5})_{j}) \\right) = 1\n$$\n$$\n\\exp(-1-\\nu) = \\frac{1}{\\sum_{j=1}^{3} \\exp(-\\eta (L_{5})_{j})}\n$$\nSubstituting this back into the expression for $x_{i}$, we obtain the general solution, which corresponds to a softmax function applied to the negative cumulative losses:\n$$\nx_{i} = \\frac{\\exp(-\\eta (L_{5})_{i})}{\\sum_{j=1}^{3} \\exp(-\\eta (L_{5})_{j})}\n$$\nNow, we substitute the specific components of our cumulative loss vector $L_{5} = (2M, 2M, M)$ to find the components of $x_{6} = (x_{6,1}, x_{6,2}, x_{6,3})$. The denominator is:\n$$\nD = \\sum_{j=1}^{3} \\exp(-\\eta (L_{5})_{j}) = \\exp(-\\eta (2M)) + \\exp(-\\eta (2M)) + \\exp(-\\eta M) = 2\\exp(-2\\eta M) + \\exp(-\\eta M)\n$$\nThe components of $x_{6}$ are:\n$$\nx_{6,1} = \\frac{\\exp(-\\eta (L_{5})_{1})}{D} = \\frac{\\exp(-2\\eta M)}{2\\exp(-2\\eta M) + \\exp(-\\eta M)}\n$$\n$$\nx_{6,2} = \\frac{\\exp(-\\eta (L_{5})_{2})}{D} = \\frac{\\exp(-2\\eta M)}{2\\exp(-2\\eta M) + \\exp(-\\eta M)}\n$$\n$$\nx_{6,3} = \\frac{\\exp(-\\eta (L_{5})_{3})}{D} = \\frac{\\exp(-\\eta M)}{2\\exp(-2\\eta M) + \\exp(-\\eta M)}\n$$\nTo simplify these expressions, we can multiply the numerator and denominator of each fraction by $\\exp(2\\eta M)$:\n$$\nx_{6,1} = \\frac{\\exp(-2\\eta M) \\cdot \\exp(2\\eta M)}{(2\\exp(-2\\eta M) + \\exp(-\\eta M)) \\cdot \\exp(2\\eta M)} = \\frac{1}{2 + \\exp(\\eta M)}\n$$\n$$\nx_{6,2} = \\frac{\\exp(-2\\eta M) \\cdot \\exp(2\\eta M)}{(2\\exp(-2\\eta M) + \\exp(-\\eta M)) \\cdot \\exp(2\\eta M)} = \\frac{1}{2 + \\exp(\\eta M)}\n$$\n$$\nx_{6,3} = \\frac{\\exp(-\\eta M) \\cdot \\exp(2\\eta M)}{(2\\exp(-2\\eta M) + \\exp(-\\eta M)) \\cdot \\exp(2\\eta M)} = \\frac{\\exp(\\eta M)}{2 + \\exp(\\eta M)}\n$$\nThus, the learner's decision vector $x_{6}$ is:\n$$\nx_{6} = \\left( \\frac{1}{2 + \\exp(\\eta M)}, \\frac{1}{2 + \\exp(\\eta M)}, \\frac{\\exp(\\eta M)}{2 + \\exp(\\eta M)} \\right)\n$$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1}{2 + \\exp(\\eta M)} & \\frac{1}{2 + \\exp(\\eta M)} & \\frac{\\exp(\\eta M)}{2 + \\exp(\\eta M)}\n\\end{pmatrix}\n}\n$$", "id": "3159427"}, {"introduction": "First-order methods like OGD use only gradient information, but what if we could also leverage the curvature of the loss functions? This practice problem [@problem_id:3159405] investigates the power of second-order optimization in the online setting. By analyzing the Online Newton Step (ONS) algorithm on a class of well-behaved quadratic losses, you will quantify the profound performance gain possible from harnessing curvature, contrasting the logarithmic regret of ONS with the square-root regret of OGD.", "problem": "Consider an online convex optimization problem with decision set $\\mathcal{X} = \\{x \\in \\mathbb{R}^{d} : \\|x\\|_{2} \\leq B\\}$, where $d \\in \\mathbb{N}$ and $B > 0$ are given constants. At each round $t = 1, 2, \\ldots, T$, the learner picks $x_t \\in \\mathcal{X}$, then observes a convex loss function of the form $f_t(x) = \\frac{1}{2}\\|A_t x - b_t\\|_{2}^{2}$, where the data satisfy the uniform bounds $\\|A_t\\|_{2} \\leq L$ and $\\|b_t\\|_{2} \\leq M$ for known constants $L > 0$ and $M \\geq 0$. The performance is measured by the cumulative regret\n$$\nR_T \\triangleq \\sum_{t=1}^{T} f_t(x_t) - \\min_{x \\in \\mathcal{X}} \\sum_{t=1}^{T} f_t(x).\n$$\nAssume the learner either uses Online Newton Step (ONS) with initialization matrix $\\lambda_0 I_d$ for some $\\lambda_0 > 0$, or Online Gradient Descent (OGD) with the standard choice of constant step size that yields the optimal order of regret for convex Lipschitz losses on a bounded domain. Using only fundamental definitions (convexity, regret) and well-tested facts (norm inequalities, Cauchy–Schwarz inequality, spectral-norm submultiplicativity, matrix-inequality characterization of exp-concavity, and the standard regret guarantees for ONS on exp-concave losses and for OGD on convex Lipschitz losses), do the following:\n- Propose the explicit bound on the residual magnitude $\\|A_t x - b_t\\|_{2}$ over $\\mathcal{X}$, derive a uniform bound on the gradient norm $\\|\\nabla f_t(x)\\|_{2}$ over $\\mathcal{X}$, and identify the exp-concavity parameter $\\eta$ of $f_t$ on $\\mathcal{X}$.\n- Using these derived quantities, specialize the standard ONS regret guarantee to this setting to obtain an explicit upper-bound expression for $R_T$ in terms of $d$, $B$, $L$, $M$, $\\lambda_0$, and $T$ (no hidden constants).\n- Specialize the standard OGD regret guarantee for convex Lipschitz functions on a set of diameter $D$ to this setting, obtaining an explicit upper-bound expression for $R_T$ in terms of $B$, $L$, $M$, and $T$ (no hidden constants).\nProvide your final answer as a single row matrix containing these two expressions in the order: first the ONS regret upper bound and then the OGD regret upper bound. No inequalities or equations should appear in the final answer; write only the two simplified closed-form analytic expressions. No rounding is required.", "solution": "The user wants me to solve the problem by providing the required information in the expected format.\n\n### Problem Validation\n\nFirst, a critical validation of the problem statement is performed.\n\n**Step 1: Extract Givens**\n- Decision set: $\\mathcal{X} = \\{x \\in \\mathbb{R}^{d} : \\|x\\|_{2} \\leq B\\}$.\n- Parameters: $d \\in \\mathbb{N}$, $B > 0$, $L > 0$, $M \\geq 0$, $\\lambda_0 > 0$.\n- Time horizon: $t = 1, 2, \\ldots, T$.\n- Loss functions: $f_t(x) = \\frac{1}{2}\\|A_t x - b_t\\|_{2}^{2}$.\n- Data bounds: $\\|A_t\\|_{2} \\leq L$ and $\\|b_t\\|_{2} \\leq M$.\n- Regret definition: $R_T = \\sum_{t=1}^{T} f_t(x_t) - \\min_{x \\in \\mathcal{X}} \\sum_{t=1}^{T} f_t(x)$.\n- Algorithms to consider: Online Newton Step (ONS) with initialization $\\lambda_0 I_d$ and Online Gradient Descent (OGD) with an optimally chosen constant step size.\n- Permitted assumptions: Use of fundamental definitions and standard, well-tested results from online optimization theory.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Grounding:** The problem is a standard exercise in the field of online convex optimization. The setting, involving quadratic loss functions (least squares) on a norm ball, is canonical and extensively studied. All concepts and tools mentioned (regret, ONS, OGD, exp-concavity) are fundamental to the field. The problem is scientifically and mathematically sound.\n- **Well-Posed:** The problem requests the derivation of explicit upper bounds on regret for two standard algorithms. Given the standard guarantees for these algorithms, the task is well-defined and leads to unique analytical expressions based on the provided parameters.\n- **Objective:** The problem is stated in precise, formal mathematical language, devoid of ambiguity or subjectivity.\n- **Completeness and Consistency:** All necessary parameters ($d$, $B$, $L$, $M$, $\\lambda_0$, $T$) are provided. The problem explicitly allows the use of standard results, making it self-contained for deriving the required bounds. There are no contradictions in the setup.\n\n**Step 3: Verdict and Action**\nThe problem is valid. We may proceed to the solution.\n\n### Derivation of Required Quantities\n\nThe solution requires specializing standard regret bounds for ONS and OGD. To do so, we must first derive key properties of the loss functions $f_t(x)$ over the domain $\\mathcal{X}$.\n\n**1. Uniform Bound on the Gradient Norm**\n\nThe loss function at round $t$ is $f_t(x) = \\frac{1}{2}\\|A_t x - b_t\\|_{2}^{2}$. Its gradient with respect to $x$ is:\n$$ \\nabla f_t(x) = A_t^T (A_t x - b_t) $$\nWe seek a uniform upper bound on its Euclidean norm, $\\|\\nabla f_t(x)\\|_{2}$, for any $x \\in \\mathcal{X}$ and any $t \\in \\{1, \\ldots, T\\}$. Using the submultiplicativity of the spectral norm ($L_2$ operator norm) and the triangle inequality:\n$$ \\|\\nabla f_t(x)\\|_{2} = \\|A_t^T (A_t x - b_t)\\|_{2} \\leq \\|A_t^T\\|_{2} \\|A_t x - b_t\\|_{2} $$\nSince $\\|A_t^T\\|_{2} = \\|A_t\\|_{2} \\leq L$, this becomes:\n$$ \\|\\nabla f_t(x)\\|_{2} \\leq L \\left( \\|A_t x\\|_{2} + \\|b_t\\|_{2} \\right) $$\nAgain using submultiplicativity and the fact that for $x \\in \\mathcal{X}$, we have $\\|x\\|_{2} \\leq B$:\n$$ \\|\\nabla f_t(x)\\|_{2} \\leq L \\left( \\|A_t\\|_{2} \\|x\\|_{2} + \\|b_t\\|_{2} \\right) \\leq L (L B + M) $$\nThis provides a uniform Lipschitz constant, which we denote as $G$:\n$$ G = L(LB+M) $$\n\n**2. Exp-concavity Parameter**\n\nA differentiable function $f$ is $\\eta$-exp-concave if its Hessian satisfies the matrix inequality $\\nabla^2 f(x) \\succeq \\eta (\\nabla f(x))(\\nabla f(x))^T$. The Hessian of $f_t(x)$ is:\n$$ \\nabla^2 f_t(x) = A_t^T A_t $$\nThe condition for exp-concavity is thus $A_t^T A_t \\succeq \\eta (\\nabla f_t(x))(\\nabla f_t(x))^T$. For any vector $z \\in \\mathbb{R}^d$, this means $z^T (A_t^T A_t) z \\geq \\eta (z^T \\nabla f_t(x))^2$. The left side is $\\|A_t z\\|_2^2$. The right side involves $(z^T \\nabla f_t(x))^2 = ((A_t z)^T(A_t x - b_t))^2$. By the Cauchy-Schwarz inequality, this is bounded by $\\|A_t z\\|_2^2 \\|A_t x - b_t\\|_2^2$.\nTherefore, the condition $A_t^T A_t \\succeq \\eta (\\nabla f_t(x))(\\nabla f_t(x))^T$ is satisfied if $\\|A_t z\\|_2^2 \\geq \\eta \\|A_t z\\|_2^2 \\|A_t x - b_t\\|_2^2$, which simplifies to $1 \\geq \\eta \\|A_t x - b_t\\|_2^2$.\nTo find a uniform parameter $\\eta$ valid for all $t$ and all $x \\in \\mathcal{X}$, we need an upper bound on $\\|A_t x - b_t\\|_2$. As shown in the gradient bound derivation, $\\|A_t x - b_t\\|_{2} \\leq LB + M$.\nThus, the condition holds if $1 \\geq \\eta (LB+M)^2$. We can choose the largest such $\\eta$, which is:\n$$ \\eta = \\frac{1}{(LB+M)^2} $$\n\n### Online Newton Step (ONS) Regret Bound\n\nThe standard regret guarantee for ONS on a sequence of $\\eta$-exp-concave functions with gradients uniformly bounded by $G$, initialized with $\\Lambda_0 = \\lambda_0 I_d$, is:\n$$ R_T \\leq \\frac{d}{2\\eta} \\ln\\left(1 + \\frac{T G^2}{d \\lambda_0}\\right) $$\nSubstituting the derived quantities $G = L(LB+M)$ and $\\eta = \\frac{1}{(LB+M)^2}$:\n$$ R_T \\leq \\frac{d}{2 \\left(\\frac{1}{(LB+M)^2}\\right)} \\ln\\left(1 + \\frac{T (L(LB+M))^2}{d \\lambda_0}\\right) $$\nSimplifying this expression gives the explicit upper bound for ONS:\n$$ R_T \\leq \\frac{d(LB+M)^2}{2} \\ln\\left(1 + \\frac{TL^2(LB+M)^2}{d\\lambda_0}\\right) $$\n\n### Online Gradient Descent (OGD) Regret Bound\n\nThe standard regret guarantee for OGD with an optimal constant step size, applied to a sequence of convex $G$-Lipschitz functions over a convex set $\\mathcal{X}$ of diameter $D$, is:\n$$ R_T \\leq D G \\sqrt{T} $$\nWe need to find the diameter $D$ of the set $\\mathcal{X} = \\{x \\in \\mathbb{R}^{d} : \\|x\\|_{2} \\leq B\\}$. The diameter is defined as $D = \\sup_{x,y \\in \\mathcal{X}} \\|x-y\\|_2$. By the triangle inequality, $\\|x-y\\|_2 \\leq \\|x\\|_2 + \\|y\\|_2 \\leq B + B = 2B$. This maximum is achieved by choosing any $x$ with $\\|x\\|_2=B$ and setting $y=-x$, which yields $\\|x-(-x)\\|_2 = \\|2x\\|_2 = 2\\|x\\|_2 = 2B$. Thus, the diameter is:\n$$ D = 2B $$\nSubstituting $D = 2B$ and the gradient bound $G = L(LB+M)$ into the standard OGD regret guarantee:\n$$ R_T \\leq (2B) (L(LB+M)) \\sqrt{T} $$\nSimplifying this expression gives the explicit upper bound for OGD:\n$$ R_T \\leq 2BL(LB+M)\\sqrt{T} $$\n\n### Final Answer Formulation\n\nThe final answer requires the two derived regret bounds—for ONS and OGD, in that order—presented as a single row matrix.\n- ONS bound: $\\frac{d(LB+M)^2}{2} \\ln\\left(1 + \\frac{TL^2(LB+M)^2}{d\\lambda_0}\\right)$\n- OGD bound: $2BL(LB+M)\\sqrt{T}$", "answer": "$$\n\\boxed{\\pmatrix{ \\frac{d(LB+M)^2}{2} \\ln\\left(1 + \\frac{TL^2(LB+M)^2}{d\\lambda_0}\\right) & 2BL(LB+M)\\sqrt{T} }}\n$$", "id": "3159405"}]}