{"hands_on_practices": [{"introduction": "The core idea behind many variance reduction techniques is to introduce negative correlation. By pairing a standard sample with an \"antithetic\" counterpart designed to have opposite noise, we can cancel out a significant portion of the stochasticity in the gradient estimate. This exercise provides a clean, theoretical demonstration of how antithetic variates work in an idealized linear regression setting, showing that perfect negative correlation can completely eliminate gradient variance [@problem_id:3197150].", "problem": "Consider one-dimensional linear regression with squared loss used in Stochastic Gradient Descent (SGD). For a single sample $(x,y)$, define the residual $r$ and the per-sample loss and gradient at parameter $\\theta$ by\n$$\nr \\equiv \\theta x - y,\\quad \\ell(\\theta;x,y) \\equiv \\tfrac{1}{2} r^{2},\\quad g(\\theta;x,y) \\equiv \\frac{\\partial \\ell}{\\partial \\theta} = x r.\n$$\nAssume the data are generated by the model\n$$\nx \\sim \\mathcal{N}(0,\\sigma_{x}^{2}),\\quad \\varepsilon \\sim \\mathcal{N}(0,\\sigma_{\\varepsilon}^{2}),\\quad y = \\theta^{\\ast} x + \\varepsilon,\n$$\nwith $x$ and $\\varepsilon$ independent, and suppose the current iterate is at the true parameter $\\theta = \\theta^{\\ast}$. You estimate the gradient using a mini-batch of size $2$.\n\nTwo mini-batch constructions are considered:\n- Independent and identically distributed (i.i.d.) sampling: draw two independent samples $(x_{1},y_{1})$ and $(x_{2},y_{2})$, and use the average gradient estimator\n$$\n\\widehat{g}_{\\mathrm{iid}} \\equiv \\tfrac{1}{2}\\big(g(\\theta;x_{1},y_{1}) + g(\\theta;x_{2},y_{2})\\big).\n$$\n- Antithetic pairing: for a given draw of $(x,\\varepsilon)$, form two samples that share the same $x$ but have opposite residual signs by taking $y_{1} \\equiv \\theta^{\\ast} x + \\varepsilon$ and $y_{2} \\equiv \\theta^{\\ast} x - \\varepsilon$, and use the average gradient estimator\n$$\n\\widehat{g}_{\\mathrm{anti}} \\equiv \\tfrac{1}{2}\\big(g(\\theta;x,y_{1}) + g(\\theta;x,y_{2})\\big).\n$$\n\nStarting from the above definitions and the stated data model, derive the variances $\\mathrm{Var}(\\widehat{g}_{\\mathrm{iid}})$ and $\\mathrm{Var}(\\widehat{g}_{\\mathrm{anti}})$ at $\\theta = \\theta^{\\ast}$, and compute the expected gradient variance reduction defined as\n$$\n\\Delta \\equiv \\mathrm{Var}(\\widehat{g}_{\\mathrm{iid}}) - \\mathrm{Var}(\\widehat{g}_{\\mathrm{anti}}).\n$$\nExpress your final answer as a closed-form analytic expression in terms of $\\sigma_{x}$ and $\\sigma_{\\varepsilon}$. No rounding is required.", "solution": "The problem asks for the variance of two different mini-batch gradient estimators for one-dimensional linear regression and the difference between these variances. The analysis is to be conducted at the true parameter $\\theta = \\theta^{\\ast}$.\n\nFirst, let us analyze the stochastic gradient for a single sample $(x, y)$ when the parameter is at the true value $\\theta = \\theta^{\\ast}$. The problem states the data generating process is $y = \\theta^{\\ast} x + \\varepsilon$, where $x \\sim \\mathcal{N}(0, \\sigma_{x}^{2})$ and $\\varepsilon \\sim \\mathcal{N}(0, \\sigma_{\\varepsilon}^{2})$ are independent.\n\nThe residual at $\\theta = \\theta^{\\ast}$ is given by:\n$$\nr = \\theta^{\\ast} x - y = \\theta^{\\ast} x - (\\theta^{\\ast} x + \\varepsilon) = -\\varepsilon\n$$\nThe per-sample gradient $g(\\theta; x, y) = x r$ evaluated at $\\theta = \\theta^{\\ast}$ is therefore:\n$$\ng(\\theta^{\\ast}; x, y) = x(-\\varepsilon) = -x\\varepsilon\n$$\nLet us denote this stochastic gradient as $g^{\\ast} \\equiv -x\\varepsilon$. We will need its statistical properties. The expectation of $g^{\\ast}$ is:\n$$\n\\mathbb{E}[g^{\\ast}] = \\mathbb{E}[-x\\varepsilon] = -\\mathbb{E}[x]\\mathbb{E}[\\varepsilon]\n$$\nThis factorization is possible because $x$ and $\\varepsilon$ are independent. Given that $x \\sim \\mathcal{N}(0, \\sigma_{x}^{2})$ and $\\varepsilon \\sim \\mathcal{N}(0, \\sigma_{\\varepsilon}^{2})$, their means are $\\mathbb{E}[x] = 0$ and $\\mathbb{E}[\\varepsilon] = 0$. Thus,\n$$\n\\mathbb{E}[g^{\\ast}] = -(0)(0) = 0\n$$\nThe variance of $g^{\\ast}$ is $\\mathrm{Var}(g^{\\ast}) = \\mathbb{E}[(g^{\\ast})^{2}] - (\\mathbb{E}[g^{\\ast}])^{2}$. Since the mean is $0$, this simplifies to:\n$$\n\\mathrm{Var}(g^{\\ast}) = \\mathbb{E}[(g^{\\ast})^{2}] = \\mathbb{E}[(-x\\varepsilon)^{2}] = \\mathbb{E}[x^{2}\\varepsilon^{2}]\n$$\nDue to the independence of $x$ and $\\varepsilon$, $x^{2}$ and $\\varepsilon^{2}$ are also independent. Therefore, we can write:\n$$\n\\mathrm{Var}(g^{\\ast}) = \\mathbb{E}[x^{2}]\\mathbb{E}[\\varepsilon^{2}]\n$$\nFor a random variable $Z$ with mean $\\mu_{Z}$ and variance $\\sigma_{Z}^{2}$, we know $\\mathrm{Var}(Z) = \\mathbb{E}[Z^{2}] - \\mu_{Z}^{2}$, which implies $\\mathbb{E}[Z^{2}] = \\mathrm{Var}(Z) + \\mu_{Z}^{2}$.\nFor $x$, we have $\\mathbb{E}[x] = 0$ and $\\mathrm{Var}(x) = \\sigma_{x}^{2}$, so $\\mathbb{E}[x^{2}] = \\sigma_{x}^{2} + 0^{2} = \\sigma_{x}^{2}$.\nFor $\\varepsilon$, we have $\\mathbb{E}[\\varepsilon] = 0$ and $\\mathrm{Var}(\\varepsilon) = \\sigma_{\\varepsilon}^{2}$, so $\\mathbb{E}[\\varepsilon^{2}] = \\sigma_{\\varepsilon}^{2} + 0^{2} = \\sigma_{\\varepsilon}^{2}$.\nSubstituting these into the expression for the variance of $g^{\\ast}$:\n$$\n\\mathrm{Var}(g^{\\ast}) = \\sigma_{x}^{2} \\sigma_{\\varepsilon}^{2}\n$$\n\nNow we can compute the variance for the two mini-batch estimators.\n\n1.  **I.I.D. Sampling Estimator, $\\widehat{g}_{\\mathrm{iid}}$**\n\nThe i.i.d. estimator is defined as $\\widehat{g}_{\\mathrm{iid}} = \\frac{1}{2}(g_{1} + g_{2})$, where $g_{1} = g(\\theta^{\\ast}; x_{1}, y_{1})$ and $g_{2} = g(\\theta^{\\ast}; x_{2}, y_{2})$ are gradients from two independent and identically distributed samples. This means $(x_{1}, \\varepsilon_{1})$ and $(x_{2}, \\varepsilon_{2})$ are independent draws.\nThus, $g_{1} = -x_{1}\\varepsilon_{1}$ and $g_{2} = -x_{2}\\varepsilon_{2}$ are i.i.d. random variables. Each has the same distribution as $g^{\\ast}$.\nThe variance of the average of two independent random variables is:\n$$\n\\mathrm{Var}(\\widehat{g}_{\\mathrm{iid}}) = \\mathrm{Var}\\left(\\frac{1}{2}(g_{1} + g_{2})\\right) = \\left(\\frac{1}{2}\\right)^{2} \\mathrm{Var}(g_{1} + g_{2})\n$$\nSince $g_{1}$ and $g_{2}$ are independent, $\\mathrm{Var}(g_{1} + g_{2}) = \\mathrm{Var}(g_{1}) + \\mathrm{Var}(g_{2})$.\n$$\n\\mathrm{Var}(\\widehat{g}_{\\mathrm{iid}}) = \\frac{1}{4} (\\mathrm{Var}(g_{1}) + \\mathrm{Var}(g_{2}))\n$$\nAs $g_{1}$ and $g_{2}$ are identically distributed, $\\mathrm{Var}(g_{1}) = \\mathrm{Var}(g_{2}) = \\mathrm{Var}(g^{\\ast}) = \\sigma_{x}^{2}\\sigma_{\\varepsilon}^{2}$.\n$$\n\\mathrm{Var}(\\widehat{g}_{\\mathrm{iid}}) = \\frac{1}{4} (\\sigma_{x}^{2}\\sigma_{\\varepsilon}^{2} + \\sigma_{x}^{2}\\sigma_{\\varepsilon}^{2}) = \\frac{1}{4} (2 \\sigma_{x}^{2}\\sigma_{\\varepsilon}^{2}) = \\frac{1}{2} \\sigma_{x}^{2}\\sigma_{\\varepsilon}^{2}\n$$\n\n2.  **Antithetic Pairing Estimator, $\\widehat{g}_{\\mathrm{anti}}$**\n\nThe antithetic estimator is formed using a single draw of $(x, \\varepsilon)$ to construct two samples.\nSample 1: $(x, y_{1})$ with $y_{1} = \\theta^{\\ast} x + \\varepsilon$.\nSample 2: $(x, y_{2})$ with $y_{2} = \\theta^{\\ast} x - \\varepsilon$.\n\nThe gradient for the first sample, which we denote as $g'_{1}$, is:\n$$\ng'_{1} = g(\\theta^{\\ast}; x, y_{1}) = x(\\theta^{\\ast} x - y_{1}) = x(\\theta^{\\ast} x - (\\theta^{\\ast} x + \\varepsilon)) = -x\\varepsilon\n$$\nThe gradient for the second sample, which we denote as $g'_{2}$, is:\n$$\ng'_{2} = g(\\theta^{\\ast}; x, y_{2}) = x(\\theta^{\\ast} x - y_{2}) = x(\\theta^{\\ast} x - (\\theta^{\\ast} x - \\varepsilon)) = x\\varepsilon\n$$\nThe antithetic gradient estimator is the average of these two gradients:\n$$\n\\widehat{g}_{\\mathrm{anti}} = \\frac{1}{2}(g'_{1} + g'_{2}) = \\frac{1}{2}(-x\\varepsilon + x\\varepsilon) = \\frac{1}{2}(0) = 0\n$$\nSince the estimator $\\widehat{g}_{\\mathrm{anti}}$ is identically zero for any realization of the random variables $x$ and $\\varepsilon$, it is a constant. The variance of a constant is zero.\n$$\n\\mathrm{Var}(\\widehat{g}_{\\mathrm{anti}}) = \\mathrm{Var}(0) = 0\n$$\n\n3.  **Expected Gradient Variance Reduction, $\\Delta$**\n\nFinally, we compute the variance reduction $\\Delta$ by subtracting the variance of the antithetic estimator from the variance of the i.i.d. estimator.\n$$\n\\Delta = \\mathrm{Var}(\\widehat{g}_{\\mathrm{iid}}) - \\mathrm{Var}(\\widehat{g}_{\\mathrm{anti}})\n$$\nSubstituting the derived variances:\n$$\n\\Delta = \\frac{1}{2} \\sigma_{x}^{2}\\sigma_{\\varepsilon}^{2} - 0 = \\frac{1}{2} \\sigma_{x}^{2}\\sigma_{\\varepsilon}^{2}\n$$\nThis result quantifies the reduction in gradient variance achieved by using antithetic sampling instead of standard i.i.d. sampling for a mini-batch of size $2$ at the optimal parameter $\\theta^{\\ast}$. In this specific idealized setting, the antithetic estimator completely eliminates the variance.", "answer": "$$\n\\boxed{\\frac{1}{2}\\sigma_{x}^{2}\\sigma_{\\varepsilon}^{2}}\n$$", "id": "3197150"}, {"introduction": "Moving from synthetic sample pairing, we can apply variance reduction principles to the structure of the data itself. In practice, datasets for tasks like classification are often imbalanced, leading to high variance when sampling uniformly. This exercise challenges you to design a stratified sampling scheme that intelligently oversamples rare classes, derive the optimal sampling probabilities that minimize variance, and implement your solution to see the benefits firsthand [@problem_id:3197205].", "problem": "You are given a binary classification dataset with imbalanced labels and asked to analyze the variance of stochastic gradient estimators under different sampling schemes in Stochastic Gradient Descent (SGD). Work strictly from the foundational definitions of empirical risk minimization and unbiased gradient estimation, and avoid using any shortcut formulas not derived from first principles.\n\nConsider the empirical risk of logistic regression for binary labels, defined as follows. Let there be $n$ samples $\\{(x_i,y_i)\\}_{i=1}^n$ with feature vectors $x_i \\in \\mathbb{R}^d$ and labels $y_i \\in \\{0,1\\}$. The logistic loss for a single sample is\n$$\n\\ell(w; x_i, y_i) = \\log\\big(1 + e^{\\langle w, x_i \\rangle}\\big) - y_i \\langle w, x_i \\rangle,\n$$\nand the empirical risk is\n$$\nL(w) = \\frac{1}{n}\\sum_{i=1}^n \\ell(w; x_i, y_i).\n$$\nLet the sigmoid function be $\\sigma(z) = \\frac{1}{1 + e^{-z}}$. The per-sample gradient is\n$$\ng_i(w) = \\nabla_w \\ell(w; x_i, y_i) = \\big(\\sigma(\\langle w, x_i\\rangle) - y_i\\big)\\, x_i,\n$$\nand the full gradient is\n$$\ng(w) = \\nabla_w L(w) = \\frac{1}{n}\\sum_{i=1}^n g_i(w).\n$$\n\nA single-sample unbiased stochastic gradient estimator is constructed by sampling an index $i$ from $\\{1,\\dots,n\\}$ according to a probability mass function $p_i$ and returning\n$$\n\\widehat{g}(w) = \\frac{1}{n p_i} g_i(w),\n$$\nwhich satisfies $\\mathbb{E}[\\widehat{g}(w)] = g(w)$ by linearity of expectation when $p_i > 0$ for all $i$.\n\nYou must devise a stratified sampling scheme for a binary classification dataset with imbalanced labels as follows:\n- Partition the indices into two strata according to the label: $S_0 = \\{i \\mid y_i = 0\\}$ and $S_1 = \\{i \\mid y_i = 1\\}$, with sizes $n_0 = |S_0|$ and $n_1 = |S_1|$ satisfying $n_0 + n_1 = n$.\n- First sample a stratum $c \\in \\{0,1\\}$ with probability $q_c$.\n- Then, within the chosen stratum $c$, sample uniformly an index $i \\in S_c$.\n- Use the importance-weighted estimator $\\widehat{g}(w)$ with $p_i = q_c / n_c$ and the weight $\\frac{1}{n p_i}$ to preserve unbiasedness.\n\nFrom the definition of variance as the expected squared deviation from the mean,\n$$\n\\mathrm{Var}(\\widehat{g}(w)) = \\mathbb{E}\\big[\\|\\widehat{g}(w) - g(w)\\|_2^2\\big] = \\mathbb{E}\\big[\\|\\widehat{g}(w)\\|_2^2\\big] - \\|g(w)\\|_2^2,\n$$\nderive from first principles an explicit expression for $\\mathbb{E}\\big[\\|\\widehat{g}(w)\\|_2^2\\big]$ under the above stratified scheme. Then, choose the stratum probabilities $q_0$ and $q_1$ to minimize the variance subject to the constraints $q_0 + q_1 = 1$ and $q_0, q_1 > 0$, using only valid mathematical reasoning (e.g., Lagrange multipliers and properties of expectation). Do not introduce any unjustified formulas.\n\nImplement a complete program that:\n- Constructs the following test suite of datasets and parameter vectors.\n- For each test case, computes the uniform-sampling variance (where $p_i = 1/n$ and the estimator is $\\widehat{g}(w) = g_i(w)$) and the variance under your derived stratified scheme.\n- Outputs, for each test case, a single float equal to the ratio of stratified variance to uniform variance.\n- If the uniform variance is zero for a test case, define the ratio to be $1.0$ for that case.\n- Prints a single line containing the results as a comma-separated list enclosed in square brackets (e.g., \"[$r_1$,$r_2$,$r_3$]\"), with each float rounded to six decimal places.\n\nUse the following test suite. Each test case specifies $(X,y,w)$ explicitly.\n\n- Test case $1$ (moderate imbalance):\n  - $n = 8$, $d = 2$.\n  - Positive-label indices $S_1 = \\{1,2,3\\}$ with $y_1 = 1$, $y_2 = 1$, $y_3 = 1$ and feature vectors\n    $x_1 = [\\,2.0,\\,0.0\\,]$, $x_2 = [\\,1.5,\\,-0.5\\,]$, $x_3 = [\\,1.0,\\,0.5\\,]$.\n  - Negative-label indices $S_0 = \\{4,5,6,7,8\\}$ with $y_4 = 0$, $y_5 = 0$, $y_6 = 0$, $y_7 = 0$, $y_8 = 0$ and feature vectors\n    $x_4 = [\\,-1.0,\\,0.5\\,]$, $x_5 = [\\,-2.0,\\,1.0\\,]$, $x_6 = [\\,-1.5,\\,-0.5\\,]$, $x_7 = [\\,-0.5,\\,0.2\\,]$, $x_8 = [\\,-1.0,\\,-1.0\\,]$.\n  - Parameter vector $w = [\\,0.2,\\,-0.3\\,]$.\n\n- Test case $2$ (extreme imbalance):\n  - $n = 10$, $d = 2$.\n  - Positive-label indices $S_1 = \\{10\\}$ with $y_{10} = 1$ and $x_{10} = [\\,3.0,\\,3.0\\,]$.\n  - Negative-label indices $S_0 = \\{1,2,3,4,5,6,7,8,9\\}$ with $y_i = 0$ for $i \\in S_0$ and feature vectors\n    $x_1 = [\\,-0.1,\\,0.2\\,]$, $x_2 = [\\,-0.2,\\,0.1\\,]$, $x_3 = [\\,-0.3,\\,-0.1\\,]$, $x_4 = [\\,-0.1,\\,-0.2\\,]$, $x_5 = [\\,-0.2,\\,-0.2\\,]$, $x_6 = [\\,-0.05,\\,0.05\\,]$, $x_7 = [\\,-0.15,\\,0.0\\,]$, $x_8 = [\\,-0.1,\\,0.0\\,]$, $x_9 = [\\,-0.25,\\,0.1\\,]$.\n  - Parameter vector $w = [\\,0.1,\\,0.1\\,]$.\n\n- Test case $3$ (balanced and symmetric):\n  - $n = 4$, $d = 2$.\n  - Negative-label indices $S_0 = \\{1,2\\}$ with $y_1 = 0$, $y_2 = 0$ and feature vectors $x_1 = [\\,1.0,\\,0.0\\,]$, $x_2 = [\\,0.0,\\,1.0\\,]$.\n  - Positive-label indices $S_1 = \\{3,4\\}$ with $y_3 = 1$, $y_4 = 1$ and feature vectors $x_3 = [\\,-1.0,\\,0.0\\,]$, $x_4 = [\\,0.0,\\,-1.0\\,]$.\n  - Parameter vector $w = [\\,0.0,\\,0.0\\,]$.\n\n- Test case $4$ (degenerate zero-gradient case):\n  - $n = 6$, $d = 2$.\n  - Feature vectors $x_i = [\\,0.0,\\,0.0\\,]$ for all $i \\in \\{1,2,3,4,5,6\\}$.\n  - Labels $y = [\\,0,\\,0,\\,0,\\,1,\\,1,\\,0\\,]$.\n  - Parameter vector $w = [\\,1.0,\\,-1.0\\,]$.\n\nAngle units and physical units do not apply. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each float rounded to six decimal places, for example, \"[0.732145,0.512340,1.000000,1.000000]\".", "solution": "The problem requires the derivation of an optimal stratified sampling scheme for variance reduction in Stochastic Gradient Descent (SGD) and a comparison with uniform sampling for specific test cases. The solution proceeds in two parts: first, a formal derivation from first principles, and second, the implementation of the derived formulas to compute the required variance ratios.\n\n### Part 1: Derivation of the Optimal Stratified Sampling Scheme\n\nThe variance of a single-sample stochastic gradient estimator $\\widehat{g}(w)$ is given by:\n$$\n\\mathrm{Var}(\\widehat{g}(w)) = \\mathbb{E}\\big[\\|\\widehat{g}(w)\\|_2^2\\big] - \\|g(w)\\|_2^2\n$$\nSince $\\|g(w)\\|_2^2$ is a constant with respect to the sampling process, minimizing the variance is equivalent to minimizing the expected squared L2-norm of the estimator, $\\mathbb{E}\\big[\\|\\widehat{g}(w)\\|_2^2\\big]$.\n\n**1. Expression for $\\mathbb{E}\\big[\\|\\widehat{g}(w)\\|_2^2\\big]$ under Stratified Sampling**\n\nThe stratified sampling scheme is defined as a two-stage process:\n1.  A stratum $c \\in \\{0, 1\\}$ is selected with probability $q_c$.\n2.  An index $i$ is sampled uniformly from the chosen stratum $S_c$.\n\nThe probability of sampling a specific index $i \\in S_c$ is the product of the probability of choosing stratum $c$ and the probability of choosing $i$ within $S_c$:\n$$\np_i = P(\\text{sample } i) = P(\\text{sample } i \\mid \\text{choose } S_c) P(\\text{choose } S_c) = \\frac{1}{|S_c|} \\cdot q_c = \\frac{q_c}{n_c}\n$$\nThe unbiased stochastic gradient estimator for a sampled index $i$ is:\n$$\n\\widehat{g}(w) = \\frac{1}{n p_i} g_i(w) = \\frac{n_c}{n q_c} g_i(w), \\quad \\text{for } i \\in S_c\n$$\nThe expectation $\\mathbb{E}\\big[\\|\\widehat{g}(w)\\|_2^2\\big]$ is taken over this two-stage sampling process. We can compute it using the law of total expectation, conditioning on the chosen stratum $c$:\n$$\n\\mathbb{E}\\big[\\|\\widehat{g}(w)\\|_2^2\\big] = \\sum_{c \\in \\{0,1\\}} P(\\text{stratum } c \\text{ chosen}) \\cdot \\mathbb{E}\\big[\\|\\widehat{g}(w)\\|_2^2 \\mid \\text{stratum } c \\text{ chosen}\\big]\n$$\n$$\n\\mathbb{E}\\big[\\|\\widehat{g}(w)\\|_2^2\\big] = q_0 \\cdot \\mathbb{E}\\big[\\|\\widehat{g}(w)\\|_2^2 \\mid c=0\\big] + q_1 \\cdot \\mathbb{E}\\big[\\|\\widehat{g}(w)\\|_2^2 \\mid c=1\\big]\n$$\nGiven that stratum $c$ is chosen, the index $i$ is sampled uniformly from $S_c$. The estimator is $\\widehat{g}(w) = \\frac{n_c}{n q_c} g_i(w)$. The conditional expectation is:\n$$\n\\mathbb{E}\\big[\\|\\widehat{g}(w)\\|_2^2 \\mid c\\big] = \\mathbb{E}_{i \\sim \\text{Unif}(S_c)}\\left[\\left\\|\\frac{n_c}{n q_c} g_i(w)\\right\\|_2^2\\right] = \\left(\\frac{n_c}{n q_c}\\right)^2 \\mathbb{E}_{i \\sim \\text{Unif}(S_c)}\\left[\\|g_i(w)\\|_2^2\\right]\n$$\nThe expectation over a uniform sample from $S_c$ is simply the average over the elements of $S_c$:\n$$\n\\mathbb{E}_{i \\sim \\text{Unif}(S_c)}\\left[\\|g_i(w)\\|_2^2\\right] = \\frac{1}{n_c} \\sum_{i \\in S_c} \\|g_i(w)\\|_2^2\n$$\nSubstituting this back, we get:\n$$\n\\mathbb{E}\\big[\\|\\widehat{g}(w)\\|_2^2 \\mid c\\big] = \\left(\\frac{n_c}{n q_c}\\right)^2 \\left(\\frac{1}{n_c} \\sum_{i \\in S_c} \\|g_i(w)\\|_2^2\\right) = \\frac{n_c}{n^2 q_c^2} \\sum_{i \\in S_c} \\|g_i(w)\\|_2^2\n$$\nLet us define $V_c = \\sum_{i \\in S_c} \\|g_i(w)\\|_2^2$ as the sum of squared L2-norms of the gradients within stratum $c$. The conditional expectation becomes $\\frac{n_c V_c}{n^2 q_c^2}$.\n\nPlugging this into the law of total expectation formula gives:\n$$\n\\mathbb{E}\\big[\\|\\widehat{g}(w)\\|_2^2\\big] = q_0 \\left(\\frac{n_0 V_0}{n^2 q_0^2}\\right) + q_1 \\left(\\frac{n_1 V_1}{n^2 q_1^2}\\right) = \\frac{1}{n^2} \\left( \\frac{n_0 V_0}{q_0} + \\frac{n_1 V_1}{q_1} \\right)\n$$\nThis is the explicit expression for the expected squared norm of the estimator.\n\n**2. Minimizing the Variance**\n\nWe need to minimize $f(q_0, q_1) = \\frac{n_0 V_0}{q_0} + \\frac{n_1 V_1}{q_1}$ subject to $q_0 + q_1 = 1$ and $q_0, q_1 > 0$.\nSubstitute $q_1 = 1 - q_0$ to get a function of a single variable $q_0 \\in (0,1)$:\n$$\nh(q_0) = \\frac{n_0 V_0}{q_0} + \\frac{n_1 V_1}{1 - q_0}\n$$\nTo find the minimum, we take the derivative with respect to $q_0$ and set it to zero. This is valid if $V_0 > 0$ and $V_1 > 0$, ensuring the critical point is in $(0, 1)$.\n$$\n\\frac{dh}{dq_0} = -\\frac{n_0 V_0}{q_0^2} + \\frac{n_1 V_1}{(1 - q_0)^2} = 0\n$$\n$$\n\\frac{n_1 V_1}{(1 - q_0)^2} = \\frac{n_0 V_0}{q_0^2} \\implies \\frac{q_0^2}{(1 - q_0)^2} = \\frac{n_0 V_0}{n_1 V_1}\n$$\nTaking the square root of both sides (and since $q_0, 1-q_0 > 0$):\n$$\n\\frac{q_0}{1 - q_0} = \\frac{\\sqrt{n_0 V_0}}{\\sqrt{n_1 V_1}}\n$$\nSolving for $q_0$:\n$$\nq_0 \\sqrt{n_1 V_1} = (1 - q_0)\\sqrt{n_0 V_0} = \\sqrt{n_0 V_0} - q_0 \\sqrt{n_0 V_0}\n$$\n$$\nq_0 (\\sqrt{n_0 V_0} + \\sqrt{n_1 V_1}) = \\sqrt{n_0 V_0}\n$$\nThe optimal stratum probabilities are:\n$$\nq_0^* = \\frac{\\sqrt{n_0 V_0}}{\\sqrt{n_0 V_0} + \\sqrt{n_1 V_1}} \\quad \\text{and} \\quad q_1^* = \\frac{\\sqrt{n_1 V_1}}{\\sqrt{n_0 V_0} + \\sqrt{n_1 V_1}}\n$$\n\n**3. Minimal Expected Squared Norm and Variance**\n\nSubstituting these optimal probabilities back into the expression for $\\mathbb{E}\\big[\\|\\widehat{g}(w)\\|_2^2\\big]$:\n$$\n\\mathbb{E}_{\\text{strat}}^*[\\|\\widehat{g}(w)\\|_2^2] = \\frac{1}{n^2} \\left( \\frac{n_0 V_0}{q_0^*} + \\frac{n_1 V_1}{q_1^*} \\right)\n$$\nThe first term is $\\frac{n_0 V_0}{q_0^*} = n_0 V_0 \\cdot \\frac{\\sqrt{n_0 V_0} + \\sqrt{n_1 V_1}}{\\sqrt{n_0 V_0}} = \\sqrt{n_0 V_0} (\\sqrt{n_0 V_0} + \\sqrt{n_1 V_1})$.\nThe second term is $\\frac{n_1 V_1}{q_1^*} = n_1 V_1 \\cdot \\frac{\\sqrt{n_0 V_0} + \\sqrt{n_1 V_1}}{\\sqrt{n_1 V_1}} = \\sqrt{n_1 V_1} (\\sqrt{n_0 V_0} + \\sqrt{n_1 V_1})$.\nSumming them gives $(\\sqrt{n_0 V_0} + \\sqrt{n_1 V_1})^2$. Therefore, the minimal expected squared norm is:\n$$\n\\mathbb{E}_{\\text{strat}}^*[\\|\\widehat{g}(w)\\|_2^2] = \\frac{1}{n^2} (\\sqrt{n_0 V_0} + \\sqrt{n_1 V_1})^2\n$$\nThe minimized variance under optimal stratified sampling is:\n$$\n\\mathrm{Var}_{\\text{strat}}(\\widehat{g}(w)) = \\frac{1}{n^2} (\\sqrt{n_0 V_0} + \\sqrt{n_1 V_1})^2 - \\|g(w)\\|_2^2\n$$\nIf $V_0=V_1=0$, all gradients are zero, this variance is zero.\n\n**4. Comparison with Uniform Sampling**\n\nFor uniform sampling, $p_i = 1/n$ for all $i$. The estimator simplifies to $\\widehat{g}_{\\text{unif}}(w) = g_i(w)$. The expected squared norm is:\n$$\n\\mathbb{E}_{\\text{unif}}[\\|\\widehat{g}(w)\\|_2^2] = \\mathbb{E}_{i \\sim \\text{Unif}}\\left[\\|g_i(w)\\|_2^2\\right] = \\frac{1}{n} \\sum_{i=1}^n \\|g_i(w)\\|_2^2 = \\frac{1}{n} (V_0 + V_1)\n$$\nThe variance under uniform sampling is:\n$$\n\\mathrm{Var}_{\\text{unif}}(\\widehat{g}(w)) = \\frac{1}{n} (V_0 + V_1) - \\|g(w)\\|_2^2\n$$\n\nThe required ratio is $\\mathrm{Var}_{\\text{strat}} / \\mathrm{Var}_{\\text{unif}}$.\n\n### Part 2: Implementation for Test Cases\n\nThe following Python code implements these derived formulas to calculate the variance ratio for each test case. The logic computes the per-sample gradients, the stratum-wise sums of squared norms ($V_0, V_1$), the full gradient norm, and finally the two variances to find their ratio. Special cases, such as zero uniform variance, are handled as specified.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the variance reduction problem for the given test suite.\n    \"\"\"\n    test_cases = [\n        # Test case 1: Moderate imbalance\n        {\n            \"X\": np.array([\n                [2.0, 0.0], [1.5, -0.5], [1.0, 0.5],  # y=1\n                [-1.0, 0.5], [-2.0, 1.0], [-1.5, -0.5], [-0.5, 0.2], [-1.0, -1.0]  # y=0\n            ]),\n            \"y\": np.array([1, 1, 1, 0, 0, 0, 0, 0]),\n            \"w\": np.array([0.2, -0.3])\n        },\n        # Test case 2: Extreme imbalance\n        {\n            \"X\": np.array([\n                [-0.1, 0.2], [-0.2, 0.1], [-0.3, -0.1], [-0.1, -0.2], [-0.2, -0.2],\n                [-0.05, 0.05], [-0.15, 0.0], [-0.1, 0.0], [-0.25, 0.1], # y=0\n                [3.0, 3.0] # y=1\n            ]),\n            \"y\": np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1]),\n            \"w\": np.array([0.1, 0.1])\n        },\n        # Test case 3: Balanced and symmetric\n        {\n            \"X\": np.array([\n                [1.0, 0.0], [0.0, 1.0],  # y=0\n                [-1.0, 0.0], [0.0, -1.0] # y=1\n            ]),\n            \"y\": np.array([0, 0, 1, 1]),\n            \"w\": np.array([0.0, 0.0])\n        },\n        # Test case 4: Degenerate zero-gradient case\n        {\n            \"X\": np.array([\n                [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], \n                [0.0, 0.0], [0.0, 0.0], [0.0, 0.0]\n            ]),\n            \"y\": np.array([0, 0, 0, 1, 1, 0]),\n            \"w\": np.array([1.0, -1.0])\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        X, y, w = case[\"X\"], case[\"y\"], case[\"w\"]\n        \n        n, d = X.shape\n\n        # Sigmoid function\n        def sigmoid(z):\n            return 1.0 / (1.0 + np.exp(-z))\n\n        # 1. Compute per-sample gradients g_i(w)\n        w_dot_x = X @ w\n        sigmas = sigmoid(w_dot_x)\n        # Reshape for broadcasting: (n,) -> (n, 1) to multiply with X (n, d)\n        coeff = (sigmas - y)[:, np.newaxis]\n        per_sample_grads = coeff * X\n\n        # 2. Compute full gradient g(w) and its squared norm\n        full_grad = np.mean(per_sample_grads, axis=0)\n        full_grad_norm_sq = np.dot(full_grad, full_grad)\n\n        # 3. Compute sum of squared L2-norms of gradients per stratum (V_0, V_1)\n        per_sample_grad_norms_sq = np.sum(per_sample_grads**2, axis=1)\n        \n        indices_S0 = np.where(y == 0)[0]\n        indices_S1 = np.where(y == 1)[0]\n        \n        n0 = len(indices_S0)\n        n1 = len(indices_S1)\n\n        V0 = np.sum(per_sample_grad_norms_sq[indices_S0])\n        V1 = np.sum(per_sample_grad_norms_sq[indices_S1])\n\n        # 4. Compute variance for uniform sampling\n        # E[||g_hat||^2] for uniform sampling\n        E_unif_norm_sq = np.sum(per_sample_grad_norms_sq) / n\n        var_unif = E_unif_norm_sq - full_grad_norm_sq\n\n        # Handle the case where uniform variance is zero\n        if np.isclose(var_unif, 0.0):\n            results.append(1.0)\n            continue\n\n        # 5. Compute variance for optimal stratified sampling\n        # E[||g_hat||^2] for stratified sampling\n        # This formula is numerically stable even if V0 or V1 is 0\n        term_0 = np.sqrt(n0 * V0) if n0 > 0 else 0.0\n        term_1 = np.sqrt(n1 * V1) if n1 > 0 else 0.0\n        \n        E_strat_norm_sq = (term_0 + term_1)**2 / n**2\n        var_strat = E_strat_norm_sq - full_grad_norm_sq\n        \n        # 6. Compute and store the ratio\n        ratio = var_strat / var_unif\n        results.append(ratio)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```", "id": "3197205"}, {"introduction": "While powerful, variance reduction techniques are not infallible and must be used with care. A control variate's effectiveness hinges on its correlation with the gradient; a poorly chosen one can be counterproductive. This final practice explores a critical failure mode where a misspecified control variate actually *increases* variance, forcing you to analyze the conditions for this negative outcome and develop a safeguard to ensure the method is always beneficial [@problem_id:3197169].", "problem": "Consider Stochastic Gradient Descent (SGD) with a scalar stochastic gradient component modeled as a random variable $G$ at a fixed iterate. Assume $G$ is unbiased for the true gradient component, so $\\mathbb{E}[G]$ equals the true gradient component at that iterate. A Variance Reduction (VR) technique uses a control variate $M$ constructed from a predictive model of the gradient, and forms the adjusted estimator $\\tilde{G}(c) = G - c M$. Suppose the control variate $M$ is zero-mean, i.e., $\\mathbb{E}[M] = 0$, ensuring that $\\tilde{G}(c)$ remains unbiased for any scalar coefficient $c \\in \\mathbb{R}$. You are told the second-order statistics\n$$\n\\operatorname{Var}(G) = 4,\\quad \\operatorname{Var}(M) = 9,\\quad \\operatorname{Cov}(G,M) = \\rho\\,\\sigma_G\\,\\sigma_M,\n$$\nwith $\\rho = -0.2$, $\\sigma_G = 2$, and $\\sigma_M = 3$. A practitioner, using a misspecified predictive model for $M$, sets the coefficient $c_0 = -1$.\n\nStarting from the fundamental definitions of variance $\\operatorname{Var}(X) = \\mathbb{E}\\!\\left[(X - \\mathbb{E}[X])^2\\right]$ and covariance $\\operatorname{Cov}(X,Y) = \\mathbb{E}\\!\\left[(X - \\mathbb{E}[X])(Y - \\mathbb{E}[Y])\\right]$, and the well-tested identity for linear combinations of random variables, analyze the impact of using a potentially wrong control variate on the variance of $\\tilde{G}(c)$:\n\n- Derive an expression for $\\operatorname{Var}(\\tilde{G}(c))$ in terms of $\\operatorname{Var}(G)$, $\\operatorname{Var}(M)$, and $\\operatorname{Cov}(G,M)$, and evaluate the variance ratio $R = \\operatorname{Var}(\\tilde{G}(c_0))/\\operatorname{Var}(G)$ using the provided numerical values.\n\n- Identify the set of coefficients $c$ for which the variance does not increase, i.e., $\\operatorname{Var}(\\tilde{G}(c)) \\le \\operatorname{Var}(G)$, and propose a safeguard that clips any proposed coefficient to this set to prevent variance increase. Compute the safeguarded coefficient $c_{\\mathrm{safe}}$ obtained by projecting $c_0$ onto the set you identified.\n\nRound all numerical results to $4$ significant figures. Provide your final answer as the two numbers $R$ and $c_{\\mathrm{safe}}$.", "solution": "The analysis proceeds in two parts as requested.\n\nFirst, we derive an expression for the variance of the adjusted estimator $\\tilde{G}(c) = G - cM$ and evaluate the variance ratio $R$. The estimator $\\tilde{G}(c)$ is unbiased because the control variate $M$ is zero-mean:\n$$\n\\mathbb{E}[\\tilde{G}(c)] = \\mathbb{E}[G - cM] = \\mathbb{E}[G] - c\\mathbb{E}[M] = \\mathbb{E}[G] - c(0) = \\mathbb{E}[G]\n$$\nThe variance of a linear combination of two random variables, $X$ and $Y$, is given by the identity:\n$$\n\\operatorname{Var}(aX + bY) = a^2\\operatorname{Var}(X) + b^2\\operatorname{Var}(Y) + 2ab\\operatorname{Cov}(X,Y)\n$$\nApplying this identity to $\\tilde{G}(c) = G - cM$, we identify $a=1$, $X=G$, $b=-c$, and $Y=M$. The variance of $\\tilde{G}(c)$ is therefore:\n$$\n\\operatorname{Var}(\\tilde{G}(c)) = \\operatorname{Var}(G - cM) = (1)^2\\operatorname{Var}(G) + (-c)^2\\operatorname{Var}(M) + 2(1)(-c)\\operatorname{Cov}(G,M)\n$$\n$$\n\\operatorname{Var}(\\tilde{G}(c)) = \\operatorname{Var}(G) + c^2\\operatorname{Var}(M) - 2c\\operatorname{Cov}(G,M)\n$$\nThis is the general expression for the variance of the control variate estimator.\n\nNext, we substitute the given numerical values. The variances are $\\operatorname{Var}(G) = 4$ and $\\operatorname{Var}(M) = 9$. The covariance is calculated from the provided correlation coefficient $\\rho = -0.2$ and standard deviations $\\sigma_G = \\sqrt{\\operatorname{Var}(G)} = 2$ and $\\sigma_M = \\sqrt{\\operatorname{Var}(M)} = 3$:\n$$\n\\operatorname{Cov}(G,M) = \\rho\\,\\sigma_G\\,\\sigma_M = (-0.2)(2)(3) = -1.2\n$$\nSubstituting these values into the variance expression:\n$$\n\\operatorname{Var}(\\tilde{G}(c)) = 4 + c^2(9) - 2c(-1.2) = 9c^2 + 2.4c + 4\n$$\nThe practitioner uses a misspecified coefficient $c_0 = -1$. The variance for this choice is:\n$$\n\\operatorname{Var}(\\tilde{G}(c_0 = -1)) = 9(-1)^2 + 2.4(-1) + 4 = 9 - 2.4 + 4 = 10.6\n$$\nThe variance ratio $R$ is the ratio of this new variance to the original variance of $G$:\n$$\nR = \\frac{\\operatorname{Var}(\\tilde{G}(c_0))}{\\operatorname{Var}(G)} = \\frac{10.6}{4} = 2.65\n$$\nRounding to $4$ significant figures, we get $R = 2.650$. This indicates that the practitioner's choice of $c_0=-1$ more than doubles the variance, which is counterproductive.\n\nSecond, we identify the set of coefficients $c$ for which the variance does not increase. This condition is expressed as $\\operatorname{Var}(\\tilde{G}(c)) \\le \\operatorname{Var}(G)$.\n$$\n\\operatorname{Var}(G) + c^2\\operatorname{Var}(M) - 2c\\operatorname{Cov}(G,M) \\le \\operatorname{Var}(G)\n$$\nSubtracting $\\operatorname{Var}(G)$ from both sides yields a quadratic inequality in $c$:\n$$\nc^2\\operatorname{Var}(M) - 2c\\operatorname{Cov}(G,M) \\le 0\n$$\n$$\nc(c\\operatorname{Var}(M) - 2\\operatorname{Cov}(G,M)) \\le 0\n$$\nSubstituting the numerical values $\\operatorname{Var}(M) = 9$ and $\\operatorname{Cov}(G,M) = -1.2$:\n$$\nc(9c - 2(-1.2)) \\le 0 \\implies c(9c + 2.4) \\le 0\n$$\nThe quadratic function $f(c) = c(9c + 2.4)$ is a parabola opening upwards. It is less than or equal to zero between its roots. The roots are $c=0$ and $9c + 2.4 = 0 \\implies c = -2.4/9 = -4/15$. Therefore, the set of coefficients for which variance does not increase is the closed interval:\n$$\nc \\in \\left[-\\frac{4}{15}, 0\\right]\n$$\nThe proposed safeguard is to clip any proposed coefficient to this set. This is equivalent to finding the point in the interval $[-\\frac{4}{15}, 0]$ that is closest to the proposed coefficient $c_0 = -1$. This operation is a projection of $c_0$ onto the interval.\n$$\nc_{\\mathrm{safe}} = \\operatorname{proj}_{[-\\frac{4}{15}, 0]}(c_0) = \\max\\left(-\\frac{4}{15}, \\min(0, c_0)\\right)\n$$\nSubstituting $c_0 = -1$:\n$$\nc_{\\mathrm{safe}} = \\max\\left(-\\frac{4}{15}, \\min(0, -1)\\right) = \\max\\left(-\\frac{4}{15}, -1\\right)\n$$\nSince $-4/15 \\approx -0.2667$, which is greater than $-1$, the maximum is $-4/15$.\n$$\nc_{\\mathrm{safe}} = -\\frac{4}{15}\n$$\nRounding to $4$ significant figures, $c_{\\mathrm{safe}} \\approx -0.2667$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n2.650 & -0.2667\n\\end{pmatrix}\n}\n$$", "id": "3197169"}]}