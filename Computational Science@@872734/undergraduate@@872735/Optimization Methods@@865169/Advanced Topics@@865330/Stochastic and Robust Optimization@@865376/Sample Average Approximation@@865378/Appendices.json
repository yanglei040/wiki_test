{"hands_on_practices": [{"introduction": "Often in practice, our goal is not to find a single optimal value, but to compare the performance of two or more different decisions under uncertainty. In this context, how we generate our random samples can have a profound impact on the quality of our comparison. This exercise explores the powerful variance reduction technique known as Common Random Numbers (CRN), where synchronized randomness is used to sharpen the distinction between alternatives. You will quantify the significant efficiency gains of CRN over independent sampling, demonstrating a fundamental principle in the design of simulation experiments.[@problem_id:3174809]", "problem": "Consider estimating the difference in expected losses between two decisions using Sample Average Approximation (SAA). Let the expected loss be defined by $F(x) = \\mathbb{E}[g(x,\\xi)]$, and the SAA estimator by\n$$\n\\hat{F}(x) = \\frac{1}{N} \\sum_{i=1}^{N} g(x,\\xi_i),\n$$\nwhere $\\xi_1,\\dots,\\xi_N$ are independent and identically distributed realizations of the random input $\\xi$. For decision $x_1$, suppose the loss is $g(x_1,\\xi) = 2\\xi + \\varepsilon_1$, and for decision $x_2$, suppose the loss is $g(x_2,\\xi) = 3\\xi + \\varepsilon_2$, where $\\xi \\sim \\mathcal{N}(0,1)$, $\\varepsilon_1 \\sim \\mathcal{N}(0,2)$, and $\\varepsilon_2 \\sim \\mathcal{N}(0,3)$. Assume $\\xi$, $\\varepsilon_1$, and $\\varepsilon_2$ are mutually independent, and that for each $i=1,\\dots,N$, the triplets $(\\xi_i,\\varepsilon_{1,i},\\varepsilon_{2,i})$ are independent and identically distributed copies of $(\\xi,\\varepsilon_1,\\varepsilon_2)$.\n\nCompare two sampling schemes for estimating the difference $\\hat{F}(x_1) - \\hat{F}(x_2)$:\n1. Use common random numbers (CRN), meaning the same $\\xi_i$ is used in both $g(x_1,\\xi_i)$ and $g(x_2,\\xi_i)$ for each $i$, while $\\varepsilon_{1,i}$ and $\\varepsilon_{2,i}$ remain independent across $i$ and mutually independent.\n2. Use independent sampling, meaning independent sequences $\\{\\xi_i^{(1)}\\}_{i=1}^{N}$ and $\\{\\xi_i^{(2)}\\}_{i=1}^{N}$ are used for $x_1$ and $x_2$, respectively, with each sequence identically distributed as $\\xi$, and all noise terms independent across decisions and samples.\n\nStarting from the definitions of expectation, variance, and covariance, and the properties of sums of independent random variables, derive expressions for $\\operatorname{Var}\\!\\left(\\hat{F}(x_1) - \\hat{F}(x_2)\\right)$ under both schemes as functions of $N$. Then, for $N=100$, compute the exact ratio\n$$\n\\rho = \\frac{\\operatorname{Var}_{\\text{indep}}\\!\\left(\\hat{F}(x_1) - \\hat{F}(x_2)\\right)}{\\operatorname{Var}_{\\text{CRN}}\\!\\left(\\hat{F}(x_1) - \\hat{F}(x_2)\\right)}.\n$$\nExpress your final answer as a single exact number.", "solution": "### Step 1: Extract Givens\n- Expected loss: $F(x) = \\mathbb{E}[g(x,\\xi)]$\n- SAA estimator: $\\hat{F}(x) = \\frac{1}{N} \\sum_{i=1}^{N} g(x,\\xi_i)$\n- Loss function for decision $x_1$: $g(x_1,\\xi) = 2\\xi + \\varepsilon_1$\n- Loss function for decision $x_2$: $g(x_2,\\xi) = 3\\xi + \\varepsilon_2$\n- Random variable distributions:\n    - $\\xi \\sim \\mathcal{N}(0,1)$, implying $\\mathbb{E}[\\xi] = 0$ and $\\operatorname{Var}(\\xi) = 1$.\n    - $\\varepsilon_1 \\sim \\mathcal{N}(0,2)$, implying $\\mathbb{E}[\\varepsilon_1] = 0$ and $\\operatorname{Var}(\\varepsilon_1) = 2$.\n    - $\\varepsilon_2 \\sim \\mathcal{N}(0,3)$, implying $\\mathbb{E}[\\varepsilon_2] = 0$ and $\\operatorname{Var}(\\varepsilon_2) = 3$.\n- Independence assumptions:\n    - $\\xi$, $\\varepsilon_1$, and $\\varepsilon_2$ are mutually independent.\n    - For each $i=1,\\dots,N$, the triplets $(\\xi_i, \\varepsilon_{1,i}, \\varepsilon_{2,i})$ are independent and identically distributed (i.i.d.) copies.\n- Sampling schemes to compare for estimating $\\hat{F}(x_1) - \\hat{F}(x_2)$:\n    1.  Common Random Numbers (CRN): Same $\\xi_i$ for both $g(x_1,\\cdot)$ and $g(x_2,\\cdot)$.\n    2.  Independent Sampling: Independent sequences $\\{\\xi_i^{(1)}\\}_{i=1}^{N}$ and $\\{\\xi_i^{(2)}\\}_{i=1}^{N}$.\n- Objective: Derive $\\operatorname{Var}\\!\\left(\\hat{F}(x_1) - \\hat{F}(x_2)\\right)$ for both schemes, and then compute the ratio $\\rho = \\frac{\\operatorname{Var}_{\\text{indep}}}{\\operatorname{Var}_{\\text{CRN}}}$ for $N=100$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, rooted in the principles of probability theory, statistics, and simulation, specifically within the context of Sample Average Approximation (SAA) and variance reduction techniques. It is well-posed, with all necessary information provided to derive a unique, deterministic solution. The definitions of the loss functions, random variables, and their distributions are clear and unambiguous. The problem setup is internally consistent and does not contain any contradictory statements or scientifically implausible conditions. The objective is clearly stated and formalizable. The problem does not exhibit any of the flaws listed in the validation checklist.\n\n### Step 3: Verdict and Action\nThe problem is valid. The solution process will now proceed.\n\nThe objective is to compute and compare the variance of the estimator for the difference in expected losses, $\\hat{F}(x_1) - \\hat{F}(x_2)$, under two different sampling schemes. The quantity of interest is $\\operatorname{Var}(\\hat{F}(x_1) - \\hat{F}(x_2))$. Using the properties of variance, we have:\n$$\n\\operatorname{Var}(\\hat{F}(x_1) - \\hat{F}(x_2)) = \\operatorname{Var}(\\hat{F}(x_1)) + \\operatorname{Var}(\\hat{F}(x_2)) - 2\\operatorname{Cov}(\\hat{F}(x_1), \\hat{F}(x_2))\n$$\nFirst, let's compute the variances of the individual SAA estimators, $\\operatorname{Var}(\\hat{F}(x_1))$ and $\\operatorname{Var}(\\hat{F}(x_2))$. These are independent of the sampling scheme for the difference.\n\nThe SAA estimator is an average of $N$ i.i.d. random variables. Thus, its variance is $\\frac{1}{N}$ times the variance of a single term.\n$$\n\\operatorname{Var}(\\hat{F}(x)) = \\operatorname{Var}\\left(\\frac{1}{N} \\sum_{i=1}^{N} g(x,\\xi_i)\\right) = \\frac{1}{N^2} \\sum_{i=1}^{N} \\operatorname{Var}(g(x,\\xi_i)) = \\frac{N \\operatorname{Var}(g(x,\\xi))}{N^2} = \\frac{\\operatorname{Var}(g(x,\\xi))}{N}\n$$\nWe need to calculate the variance of each loss function.\nFor decision $x_1$, the loss function is $g(x_1,\\xi,\\varepsilon_1) = 2\\xi + \\varepsilon_1$. Since $\\xi$ and $\\varepsilon_1$ are independent, the variance is:\n$$\n\\operatorname{Var}(g(x_1,\\xi,\\varepsilon_1)) = \\operatorname{Var}(2\\xi + \\varepsilon_1) = \\operatorname{Var}(2\\xi) + \\operatorname{Var}(\\varepsilon_1) = 2^2 \\operatorname{Var}(\\xi) + \\operatorname{Var}(\\varepsilon_1) = 4(1) + 2 = 6\n$$\nFor decision $x_2$, the loss function is $g(x_2,\\xi,\\varepsilon_2) = 3\\xi + \\varepsilon_2$. Since $\\xi$ and $\\varepsilon_2$ are independent, the variance is:\n$$\n\\operatorname{Var}(g(x_2,\\xi,\\varepsilon_2)) = \\operatorname{Var}(3\\xi + \\varepsilon_2) = \\operatorname{Var}(3\\xi) + \\operatorname{Var}(\\varepsilon_2) = 3^2 \\operatorname{Var}(\\xi) + \\operatorname{Var}(\\varepsilon_2) = 9(1) + 3 = 12\n$$\nTherefore, the variances of the individual SAA estimators are:\n$$\n\\operatorname{Var}(\\hat{F}(x_1)) = \\frac{\\operatorname{Var}(g(x_1,\\xi,\\varepsilon_1))}{N} = \\frac{6}{N}\n$$\n$$\n\\operatorname{Var}(\\hat{F}(x_2)) = \\frac{\\operatorname{Var}(g(x_2,\\xi,\\varepsilon_2))}{N} = \\frac{12}{N}\n$$\n\nNow we will analyze the two sampling schemes.\n\n**Scheme 1: Independent Sampling**\nUnder this scheme, independent sets of random numbers are used to evaluate $\\hat{F}(x_1)$ and $\\hat{F}(x_2)$. Specifically, $\\hat{F}(x_1) = \\frac{1}{N}\\sum_{i=1}^{N} (2\\xi_i^{(1)} + \\varepsilon_{1,i})$ and $\\hat{F}(x_2) = \\frac{1}{N}\\sum_{i=1}^{N} (3\\xi_i^{(2)} + \\varepsilon_{2,i})$. All the random variables $\\{\\xi_i^{(1)}\\}$, $\\{\\xi_i^{(2)}\\}$, $\\{\\varepsilon_{1,i}\\}$, $\\{\\varepsilon_{2,i}\\}$ are mutually independent. Consequently, the two estimators $\\hat{F}(x_1)$ and $\\hat{F}(x_2)$ are independent random variables. For independent variables, the covariance is zero:\n$$\n\\operatorname{Cov}_{\\text{indep}}(\\hat{F}(x_1), \\hat{F}(x_2)) = 0\n$$\nThe variance of the difference is the sum of the variances:\n$$\n\\operatorname{Var}_{\\text{indep}}(\\hat{F}(x_1) - \\hat{F}(x_2)) = \\operatorname{Var}(\\hat{F}(x_1)) + \\operatorname{Var}(\\hat{F}(x_2)) = \\frac{6}{N} + \\frac{12}{N} = \\frac{18}{N}\n$$\n\n**Scheme 2: Common Random Numbers (CRN)**\nUnder CRN, the same stream of random numbers $\\xi_i$ is used for both estimators. Let $g_{1,i} = 2\\xi_i + \\varepsilon_{1,i}$ and $g_{2,i} = 3\\xi_i + \\varepsilon_{2,i}$. The difference of the estimators is:\n$$\n\\hat{F}(x_1) - \\hat{F}(x_2) = \\frac{1}{N}\\sum_{i=1}^{N} g_{1,i} - \\frac{1}{N}\\sum_{i=1}^{N} g_{2,i} = \\frac{1}{N}\\sum_{i=1}^{N} (g_{1,i} - g_{2,i})\n$$\nThe terms $(g_{1,i} - g_{2,i})$ are i.i.d. for $i=1, \\dots, N$. Therefore, the variance of their sum is:\n$$\n\\operatorname{Var}_{\\text{CRN}}(\\hat{F}(x_1) - \\hat{F}(x_2)) = \\operatorname{Var}\\left(\\frac{1}{N}\\sum_{i=1}^{N} (g_{1,i} - g_{2,i})\\right) = \\frac{1}{N}\\operatorname{Var}(g_1 - g_2)\n$$\nwhere $g_1 = 2\\xi + \\varepsilon_1$ and $g_2 = 3\\xi + \\varepsilon_2$. Let's compute the variance of the difference $g_1 - g_2$:\n$$\ng_1 - g_2 = (2\\xi + \\varepsilon_1) - (3\\xi + \\varepsilon_2) = -\\xi + \\varepsilon_1 - \\varepsilon_2\n$$\nSince $\\xi$, $\\varepsilon_1$, and $\\varepsilon_2$ are mutually independent, the variance of this sum is the sum of the individual variances:\n$$\n\\operatorname{Var}(g_1 - g_2) = \\operatorname{Var}(-\\xi + \\varepsilon_1 - \\varepsilon_2) = \\operatorname{Var}(-\\xi) + \\operatorname{Var}(\\varepsilon_1) + \\operatorname{Var}(-\\varepsilon_2)\n$$\nUsing the property $\\operatorname{Var}(aX) = a^2\\operatorname{Var}(X)$:\n$$\n\\operatorname{Var}(g_1 - g_2) = (-1)^2\\operatorname{Var}(\\xi) + \\operatorname{Var}(\\varepsilon_1) + (-1)^2\\operatorname{Var}(\\varepsilon_2) = \\operatorname{Var}(\\xi) + \\operatorname{Var}(\\varepsilon_1) + \\operatorname{Var}(\\varepsilon_2)\n$$\nSubstituting the given variances:\n$$\n\\operatorname{Var}(g_1 - g_2) = 1 + 2 + 3 = 6\n$$\nThus, the variance of the difference estimator under CRN is:\n$$\n\\operatorname{Var}_{\\text{CRN}}(\\hat{F}(x_1) - \\hat{F}(x_2)) = \\frac{\\operatorname{Var}(g_1 - g_2)}{N} = \\frac{6}{N}\n$$\nThis result is positive, which is consistent with the property that variance is non-negative. The positive covariance between $g_1$ and $g_2$ induced by the common random number $\\xi$ has successfully reduced the variance of the difference.\n\nFinally, we compute the required ratio $\\rho$. The problem specifies $N=100$, but as we can see, the ratio is independent of $N$.\n$$\n\\rho = \\frac{\\operatorname{Var}_{\\text{indep}}\\!\\left(\\hat{F}(x_1) - \\hat{F}(x_2)\\right)}{\\operatorname{Var}_{\\text{CRN}}\\!\\left(\\hat{F}(x_1) - \\hat{F}(x_2)\\right)} = \\frac{18/N}{6/N} = \\frac{18}{6} = 3\n$$\nThe value of $N=100$ does not affect the final ratio. The ratio is an exact number.", "answer": "$$\\boxed{3}$$", "id": "3174778"}, {"introduction": "While powerful, the solution derived from SAA can be highly sensitive to the specific random sample used, a phenomenon often called \"overfitting,\" especially when data is scarce. This practice introduces ridge regularization as a remedy to stabilize the SAA solution and improve its out-of-sample performance. You will dissect the classic bias-variance tradeoff by analytically deriving how regularization influences the solution's accuracy and variability, and ultimately find the asymptotically optimal level of regularization to minimize the mean squared error.[@problem_id:3174756]", "problem": "Consider the stochastic optimization problem of minimizing the expected squared deviation, where the loss for decision $x \\in \\mathbb{R}$ under random shock $\\xi$ is defined by $f(x,\\xi) = \\frac{1}{2}(x - \\xi)^{2}$. Assume $\\xi$ is normally distributed with mean $\\mu \\neq 0$ and variance $\\sigma^{2}$, and let $\\xi_{1},\\ldots,\\xi_{n}$ be an independent and identically distributed sample drawn from this distribution.\n\nBased on the Sample Average Approximation (SAA), define the empirical objective $\\hat{f}_{n}(x) = \\frac{1}{2n}\\sum_{i=1}^{n}(x - \\xi_{i})^{2}$ and consider the ridge-regularized SAA problem\n$$\n\\min_{x \\in \\mathbb{R}} \\hat{f}_{n}(x) + \\frac{\\lambda}{2}\\|x\\|^{2},\n$$\nwhere $\\lambda \\geq 0$ is the regularization parameter and $\\|x\\|$ denotes the Euclidean norm.\n\nStarting from the basic definitions of expectation and variance and the properties of the normal distribution, proceed as follows:\n\n1. Solve the ridge-regularized SAA problem to obtain the optimizer $x_{n}^{\\lambda}$ as a function of the sample mean $\\bar{\\xi} = \\frac{1}{n}\\sum_{i=1}^{n}\\xi_{i}$ and $\\lambda$.\n\n2. Let $F(x) = \\mathbb{E}[f(x,\\xi)]$, and denote the true optimal value by $v^{\\star} = \\inf_{x \\in \\mathbb{R}}F(x)$. Consider the plug-in estimator $\\tilde{v}_{n}^{\\lambda} = F(x_{n}^{\\lambda})$ for $v^{\\star}$. Derive explicit formulas for the bias $\\mathbb{E}[\\tilde{v}_{n}^{\\lambda}] - v^{\\star}$ and the variance $\\operatorname{Var}(\\tilde{v}_{n}^{\\lambda})$ in terms of $\\mu$, $\\sigma^{2}$, $n$, and $\\lambda$.\n\n3. Using a leading-order asymptotic analysis for large $n$, determine the regularization parameter $\\lambda$ (as a function of $n$, $\\mu$, and $\\sigma^{2}$) that minimizes the mean squared error $\\mathrm{MSE}(\\tilde{v}_{n}^{\\lambda}) = \\left(\\mathbb{E}[\\tilde{v}_{n}^{\\lambda}] - v^{\\star}\\right)^{2} + \\operatorname{Var}(\\tilde{v}_{n}^{\\lambda})$ to first order in $1/n$. Your derivation must start from the fundamental definitions and use only the properties of expectations and variances of normal random variables.\n\nExpress your final answer as a single closed-form analytic expression for the asymptotically optimal $\\lambda$ in terms of $n$, $\\mu$, and $\\sigma^{2}$. No rounding is required.", "solution": "The user wants me to solve a three-part problem related to Sample Average Approximation (SAA) with ridge regularization.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n-   Loss function: $f(x,\\xi) = \\frac{1}{2}(x - \\xi)^{2}$, where $x \\in \\mathbb{R}$.\n-   Random variable: $\\xi$ is normally distributed with mean $\\mu \\neq 0$ and variance $\\sigma^{2}$. So, $\\xi \\sim N(\\mu, \\sigma^2)$.\n-   Sample: $\\xi_{1},\\ldots,\\xi_{n}$ is an independent and identically distributed (i.i.d.) sample from the distribution of $\\xi$.\n-   SAA objective: $\\hat{f}_{n}(x) = \\frac{1}{2n}\\sum_{i=1}^{n}(x - \\xi_{i})^{2}$.\n-   Ridge-regularized SAA problem: $\\min_{x \\in \\mathbb{R}} \\ \\hat{f}_{n}(x) + \\frac{\\lambda}{2}\\|x\\|^{2}$, where $\\lambda \\geq 0$ and $\\|x\\|$ is the Euclidean norm.\n-   Sample mean: $\\bar{\\xi} = \\frac{1}{n}\\sum_{i=1}^{n}\\xi_{i}$.\n-   True objective: $F(x) = \\mathbb{E}[f(x,\\xi)]$.\n-   True optimal value: $v^{\\star} = \\inf_{x \\in \\mathbb{R}}F(x)$.\n-   Plug-in estimator: $\\tilde{v}_{n}^{\\lambda} = F(x_{n}^{\\lambda})$, where $x_{n}^{\\lambda}$ is the solution to the regularized SAA problem.\n-   Mean Squared Error (MSE): $\\mathrm{MSE}(\\tilde{v}_{n}^{\\lambda}) = \\left(\\mathbb{E}[\\tilde{v}_{n}^{\\lambda}] - v^{\\star}\\right)^{2} + \\operatorname{Var}(\\tilde{v}_{n}^{\\lambda})$.\n\n**Step 2: Validate Using Extracted Givens**\n\n-   **Scientifically Grounded**: The problem is well-established in the field of statistical learning theory and stochastic optimization. It deals with standard concepts like SAA, ridge regularization, bias-variance tradeoff, and MSE analysis. The use of a quadratic loss function and a normal distribution is common and scientifically sound.\n-   **Well-Posed**: The problem is clearly defined. The objective functions are convex, ensuring unique minimizers exist. The tasks are specific and lead to a unique analytical solution.\n-   **Objective**: The problem is stated using precise mathematical language, free from any subjective or ambiguous terms.\n\n**Step 3: Verdict and Action**\n\nThe problem is valid as it is self-contained, scientifically sound, and well-posed. I will proceed with a full solution.\n\n### Part 1: Solve the Ridge-Regularized SAA Problem\n\nThe regularized objective function is $L(x) = \\hat{f}_{n}(x) + \\frac{\\lambda}{2}\\|x\\|^{2}$. Since $x \\in \\mathbb{R}$, $\\|x\\|^2 = x^2$.\n$$\nL(x) = \\frac{1}{2n}\\sum_{i=1}^{n}(x - \\xi_{i})^{2} + \\frac{\\lambda}{2}x^{2}\n$$\nThis function is strictly convex for $\\lambda \\ge 0$ (since the first term is convex and the second is strictly convex for $\\lambda  0$, and the sum is convex for $\\lambda=0$). The unique minimizer $x_{n}^{\\lambda}$ is found by setting the first derivative with respect to $x$ to zero.\n$$\n\\frac{d L(x)}{dx} = \\frac{d}{dx} \\left( \\frac{1}{2n}\\sum_{i=1}^{n}(x^{2} - 2x\\xi_{i} + \\xi_{i}^{2}) + \\frac{\\lambda}{2}x^{2} \\right) = \\frac{1}{2n}\\sum_{i=1}^{n}(2x - 2\\xi_{i}) + \\lambda x = 0\n$$\n$$\n\\frac{1}{n} \\left( \\sum_{i=1}^{n}x - \\sum_{i=1}^{n}\\xi_{i} \\right) + \\lambda x = 0\n$$\n$$\n\\frac{1}{n} (nx - n\\bar{\\xi}) + \\lambda x = 0\n$$\n$$\nx - \\bar{\\xi} + \\lambda x = 0\n$$\n$$\n(1+\\lambda)x = \\bar{\\xi}\n$$\nSolving for $x$, we obtain the optimizer:\n$$\nx_{n}^{\\lambda} = \\frac{\\bar{\\xi}}{1+\\lambda}\n$$\n\n### Part 2: Bias and Variance of the Plug-in Estimator\n\nFirst, we determine the true objective function $F(x)$ and the true optimal value $v^{\\star}$.\n$$\nF(x) = \\mathbb{E}[f(x,\\xi)] = \\mathbb{E}\\left[\\frac{1}{2}(x - \\xi)^{2}\\right] = \\frac{1}{2}\\mathbb{E}[x^2 - 2x\\xi + \\xi^2]\n$$\nUsing the linearity of expectation,\n$$\nF(x) = \\frac{1}{2}(x^2 - 2x\\mathbb{E}[\\xi] + \\mathbb{E}[\\xi^2])\n$$\nWe know $\\mathbb{E}[\\xi] = \\mu$ and $\\operatorname{Var}(\\xi) = \\mathbb{E}[\\xi^2] - (\\mathbb{E}[\\xi])^2 = \\sigma^2$, so $\\mathbb{E}[\\xi^2] = \\sigma^2 + \\mu^2$. Substituting these into $F(x)$:\n$$\nF(x) = \\frac{1}{2}(x^2 - 2x\\mu + \\sigma^2 + \\mu^2) = \\frac{1}{2}((x-\\mu)^2 + \\sigma^2)\n$$\nThe true optimal value $v^{\\star}$ is the minimum of $F(x)$. This occurs at $x^{\\star} = \\mu$, so:\n$$\nv^{\\star} = F(\\mu) = \\frac{1}{2}((\\mu-\\mu)^2 + \\sigma^2) = \\frac{\\sigma^2}{2}\n$$\nThe plug-in estimator is $\\tilde{v}_{n}^{\\lambda} = F(x_{n}^{\\lambda})$.\n$$\n\\tilde{v}_{n}^{\\lambda} = \\frac{1}{2}\\left(\\left(x_{n}^{\\lambda} - \\mu\\right)^2 + \\sigma^2\\right) = \\frac{1}{2}\\left(\\left(\\frac{\\bar{\\xi}}{1+\\lambda} - \\mu\\right)^2\\right) + \\frac{\\sigma^2}{2}\n$$\nThe bias is $\\mathbb{E}[\\tilde{v}_{n}^{\\lambda}] - v^{\\star}$.\n$$\n\\mathbb{E}[\\tilde{v}_{n}^{\\lambda}] - v^{\\star} = \\mathbb{E}\\left[\\frac{1}{2}\\left(\\frac{\\bar{\\xi}}{1+\\lambda} - \\mu\\right)^2\\right] + \\frac{\\sigma^2}{2} - \\frac{\\sigma^2}{2} = \\frac{1}{2}\\mathbb{E}\\left[\\left(\\frac{\\bar{\\xi}}{1+\\lambda} - \\mu\\right)^2\\right]\n$$\nLet $Y = \\frac{\\bar{\\xi}}{1+\\lambda} - \\mu$. The bias is $\\frac{1}{2}\\mathbb{E}[Y^2]$. We use the property $\\mathbb{E}[Y^2] = \\operatorname{Var}(Y) + (\\mathbb{E}[Y])^2$.\nThe sample mean $\\bar{\\xi}$ has mean $\\mathbb{E}[\\bar{\\xi}] = \\mu$ and variance $\\operatorname{Var}(\\bar{\\xi}) = \\frac{\\sigma^2}{n}$.\nThe mean of $Y$ is:\n$$\n\\mathbb{E}[Y] = \\mathbb{E}\\left[\\frac{\\bar{\\xi}}{1+\\lambda} - \\mu\\right] = \\frac{\\mathbb{E}[\\bar{\\xi}]}{1+\\lambda} - \\mu = \\frac{\\mu}{1+\\lambda} - \\mu = -\\frac{\\lambda\\mu}{1+\\lambda}\n$$\nThe variance of $Y$ is:\n$$\n\\operatorname{Var}(Y) = \\operatorname{Var}\\left(\\frac{\\bar{\\xi}}{1+\\lambda} - \\mu\\right) = \\frac{1}{(1+\\lambda)^2}\\operatorname{Var}(\\bar{\\xi}) = \\frac{\\sigma^2}{n(1+\\lambda)^2}\n$$\nTherefore, the bias of $\\tilde{v}_{n}^{\\lambda}$ is:\n$$\n\\text{Bias}(\\tilde{v}_{n}^{\\lambda}) = \\frac{1}{2}\\left(\\frac{\\sigma^2}{n(1+\\lambda)^2} + \\left(-\\frac{\\lambda\\mu}{1+\\lambda}\\right)^2\\right) = \\frac{1}{2(1+\\lambda)^2}\\left(\\frac{\\sigma^2}{n} + \\lambda^2\\mu^2\\right)\n$$\nNext, we compute the variance $\\operatorname{Var}(\\tilde{v}_{n}^{\\lambda})$.\n$$\n\\operatorname{Var}(\\tilde{v}_{n}^{\\lambda}) = \\operatorname{Var}\\left(\\frac{1}{2}Y^2 + \\frac{\\sigma^2}{2}\\right) = \\frac{1}{4}\\operatorname{Var}(Y^2)\n$$\nSince $\\bar{\\xi} \\sim N(\\mu, \\sigma^2/n)$, the variable $Y = \\frac{\\bar{\\xi}}{1+\\lambda} - \\mu$ is also normally distributed. From its mean and variance calculated above, $Y \\sim N(-\\frac{\\lambda\\mu}{1+\\lambda}, \\frac{\\sigma^2}{n(1+\\lambda)^2})$.\nLet $Y = a + bZ$ where $Z \\sim N(0,1)$, $a = \\mathbb{E}[Y] = -\\frac{\\lambda\\mu}{1+\\lambda}$, and $b^2 = \\operatorname{Var}(Y) = \\frac{\\sigma^2}{n(1+\\lambda)^2}$.\nWe need $\\operatorname{Var}(Y^2) = \\operatorname{Var}((a+bZ)^2)$. Using moments of a standard normal random variable ($\\mathbb{E}[Z]=0, \\mathbb{E}[Z^2]=1, \\mathbb{E}[Z^3]=0, \\mathbb{E}[Z^4]=3$):\n$$\n\\mathbb{E}[Y^2] = a^2+b^2\\mathbb{E}[Z^2] = a^2+b^2\n$$\n$$\n\\mathbb{E}[Y^4] = \\mathbb{E}[(a+bZ)^4] = \\mathbb{E}[a^4 + 4a^3bZ + 6a^2b^2Z^2 + 4ab^3Z^3 + b^4Z^4] = a^4 + 6a^2b^2 + 3b^4\n$$\n$$\n\\operatorname{Var}(Y^2) = \\mathbb{E}[Y^4] - (\\mathbb{E}[Y^2])^2 = (a^4 + 6a^2b^2 + 3b^4) - (a^2+b^2)^2 = 4a^2b^2 + 2b^4\n$$\nSo, the variance of $\\tilde{v}_{n}^{\\lambda}$ is:\n$$\n\\operatorname{Var}(\\tilde{v}_{n}^{\\lambda}) = \\frac{1}{4}(4a^2b^2 + 2b^4) = a^2b^2 + \\frac{1}{2}b^4\n$$\nSubstituting the expressions for $a^2$ and $b^2$:\n$$\n\\operatorname{Var}(\\tilde{v}_{n}^{\\lambda}) = \\frac{\\lambda^2\\mu^2}{(1+\\lambda)^2} \\frac{\\sigma^2}{n(1+\\lambda)^2} + \\frac{1}{2}\\left(\\frac{\\sigma^2}{n(1+\\lambda)^2}\\right)^2 = \\frac{\\lambda^2\\mu^2\\sigma^2}{n(1+\\lambda)^4} + \\frac{\\sigma^4}{2n^2(1+\\lambda)^4}\n$$\n$$\n\\operatorname{Var}(\\tilde{v}_{n}^{\\lambda}) = \\frac{1}{(1+\\lambda)^4}\\left(\\frac{\\lambda^2\\mu^2\\sigma^2}{n} + \\frac{\\sigma^4}{2n^2}\\right)\n$$\n\n### Part 3: Asymptotically Optimal Regularization Parameter\n\nThe Mean Squared Error is $\\mathrm{MSE}(\\tilde{v}_{n}^{\\lambda}) = \\left(\\text{Bias}(\\tilde{v}_{n}^{\\lambda})\\right)^2 + \\operatorname{Var}(\\tilde{v}_{n}^{\\lambda})$.\n$$\n\\mathrm{MSE}(\\tilde{v}_{n}^{\\lambda}) = \\left(\\frac{1}{2(1+\\lambda)^2}\\left(\\frac{\\sigma^2}{n} + \\lambda^2\\mu^2\\right)\\right)^2 + \\frac{1}{(1+\\lambda)^4}\\left(\\frac{\\lambda^2\\mu^2\\sigma^2}{n} + \\frac{\\sigma^4}{2n^2}\\right)\n$$\nExpanding the squared bias term:\n$$\n(\\text{Bias})^2 = \\frac{1}{4(1+\\lambda)^4}\\left(\\frac{\\sigma^4}{n^2} + \\frac{2\\lambda^2\\mu^2\\sigma^2}{n} + \\lambda^4\\mu^4\\right)\n$$\nAdding the variance:\n$$\n\\mathrm{MSE}(\\tilde{v}_{n}^{\\lambda}) = \\frac{1}{4(1+\\lambda)^4}\\left(\\frac{\\sigma^4}{n^2} + \\frac{2\\lambda^2\\mu^2\\sigma^2}{n} + \\lambda^4\\mu^4\\right) + \\frac{1}{(1+\\lambda)^4}\\left(\\frac{\\lambda^2\\mu^2\\sigma^2}{n} + \\frac{\\sigma^4}{2n^2}\\right)\n$$\nCombining terms inside a common denominator:\n$$\n\\mathrm{MSE}(\\tilde{v}_{n}^{\\lambda}) = \\frac{1}{(1+\\lambda)^4} \\left[ \\frac{\\sigma^4}{4n^2} + \\frac{2\\lambda^2\\mu^2\\sigma^2}{4n} + \\frac{\\lambda^4\\mu^4}{4} + \\frac{4\\lambda^2\\mu^2\\sigma^2}{4n} + \\frac{2\\sigma^4}{4n^2} \\right]\n$$\n$$\n\\mathrm{MSE}(\\tilde{v}_{n}^{\\lambda}) = \\frac{1}{4(1+\\lambda)^4} \\left[ \\lambda^4\\mu^4 + \\frac{6\\lambda^2\\mu^2\\sigma^2}{n} + \\frac{3\\sigma^4}{n^2} \\right]\n$$\nTo find the optimal $\\lambda$ that minimizes this MSE, we differentiate with respect to $\\lambda$ and set the result to zero.\nLet $f(\\lambda) = (1+\\lambda)^{-4}$ and $g(\\lambda) = \\lambda^4\\mu^4 + \\frac{6\\lambda^2\\mu^2\\sigma^2}{n} + \\frac{3\\sigma^4}{n^2}$.\nThe derivative of $\\mathrm{MSE} = \\frac{1}{4}f(\\lambda)g(\\lambda)$ is $\\frac{1}{4}(f'g + fg')=0$.\n$ f'(\\lambda) = -4(1+\\lambda)^{-5} $, $ g'(\\lambda) = 4\\lambda^3\\mu^4 + \\frac{12\\lambda\\mu^2\\sigma^2}{n} $.\nSetting the derivative to zero yields $-4(1+\\lambda)^{-5}g(\\lambda) + (1+\\lambda)^{-4}g'(\\lambda) = 0$.\nMultiplying by $(1+\\lambda)^5$:\n$$\n-4g(\\lambda) + (1+\\lambda)g'(\\lambda) = 0\n$$\n$$\n-4\\left(\\lambda^4\\mu^4 + \\frac{6\\lambda^2\\mu^2\\sigma^2}{n} + \\frac{12\\sigma^4}{n^2}\\right) + (1+\\lambda)\\left(4\\lambda^3\\mu^4 + \\frac{12\\lambda\\mu^2\\sigma^2}{n}\\right) = 0\n$$\nExpanding and collecting terms by powers of $\\lambda$:\n$$\n(-4\\lambda^4\\mu^4 - \\frac{24\\lambda^2\\mu^2\\sigma^2}{n} - \\frac{12\\sigma^4}{n^2}) + (4\\lambda^3\\mu^4 + \\frac{12\\lambda\\mu^2\\sigma^2}{n} + 4\\lambda^4\\mu^4 + \\frac{12\\lambda^2\\mu^2\\sigma^2}{n}) = 0\n$$\n$$\n4\\lambda^3\\mu^4 - \\frac{12\\lambda^2\\mu^2\\sigma^2}{n} + \\frac{12\\lambda\\mu^2\\sigma^2}{n} - \\frac{12\\sigma^4}{n^2} = 0\n$$\n$$\n\\lambda^3\\mu^4 - \\frac{3\\lambda^2\\mu^2\\sigma^2}{n} + \\frac{3\\lambda\\mu^2\\sigma^2}{n} - \\frac{3\\sigma^4}{n^2} = 0\n$$\nFor large $n$, we expect $\\lambda$ to be small. We perform an asymptotic analysis by positing that $\\lambda$ scales as $c/n$ for some constant $c$. Let $\\lambda = c/n$.\n$$\n\\frac{c^3\\mu^4}{n^3} - \\frac{3(c^2/n^2)\\mu^2\\sigma^2}{n} + \\frac{3(c/n)\\mu^2\\sigma^2}{n} - \\frac{3\\sigma^4}{n^2} = 0\n$$\n$$\n\\frac{c^3\\mu^4}{n^3} - \\frac{3c^2\\mu^2\\sigma^2}{n^3} + \\frac{3c\\mu^2\\sigma^2}{n^2} - \\frac{3\\sigma^4}{n^2} = 0\n$$\nTo find the leading-order behavior, we multiply by $n^2$:\n$$\n\\frac{c^3\\mu^4}{n} - \\frac{3c^2\\mu^2\\sigma^2}{n} + 3c\\mu^2\\sigma^2 - 3\\sigma^4 = 0\n$$\nTaking the limit as $n \\to \\infty$, the terms with $1/n$ vanish, leaving the dominant balance equation:\n$$\n3c\\mu^2\\sigma^2 - 3\\sigma^4 = 0\n$$\nSince $\\mu \\neq 0$ and $\\sigma^2$ is a variance (assumed positive), we can solve for $c$:\n$$\nc = \\frac{3\\sigma^4}{3\\mu^2\\sigma^2} = \\frac{\\sigma^2}{\\mu^2}\n$$\nTherefore, the asymptotically optimal regularization parameter is $\\lambda = c/n$.\n$$\n\\lambda = \\frac{\\sigma^2}{n\\mu^2}\n$$", "answer": "$$\\boxed{\\frac{\\sigma^2}{n\\mu^2}}$$", "id": "3174756"}, {"introduction": "The Sample Average Approximation method forms the bedrock of solving many stochastic optimization problems, but it comes with a practical challenge: how many samples are enough? This exercise delves into the principle of diminishing returns, where each additional sample provides progressively less new information. By combining theoretical analysis with empirical simulation, you will learn to model the relationship between sample size and solution quality, enabling you to make a cost-effective decision on when to stop sampling.[@problem_id:3174778]", "problem": "Consider the stochastic optimization problem where a decision variable $x \\in \\mathbb{R}$ is chosen to minimize the expected objective $F(x) = \\mathbb{E}\\left[ \\phi(x,\\xi) \\right]$ with $\\phi(x,\\xi) = (x - \\xi)^2$ and $\\xi$ a real-valued random variable with a finite second moment. The Sample Average Approximation (SAA) method constructs the estimator $\\hat{x}_n$ as the minimizer of the sample average $\\frac{1}{n} \\sum_{i=1}^{n} \\phi(x, \\xi_i)$ based on $n$ independent and identically distributed samples $\\xi_1, \\dots, \\xi_n$. The goal is to characterize, both empirically and theoretically, the diminishing returns in the function $n \\mapsto \\mathbb{E}\\left[ F(\\hat{x}_n) \\right]$ and to use this characterization to determine when adding an additional sample is not cost-effective.\n\nFundamental base and definitions to be used:\n- The Sample Average Approximation (SAA) estimator $\\hat{x}_n$ is defined as any minimizer of the random function $x \\mapsto \\frac{1}{n}\\sum_{i=1}^n \\phi(x, \\xi_i)$.\n- The expectation operator satisfies linearity, and the variance of a random variable $\\xi$ is $\\operatorname{Var}(\\xi) = \\mathbb{E}\\left[(\\xi - \\mathbb{E}[\\xi])^2\\right]$.\n- For independent and identically distributed samples, the sample mean $\\bar{\\xi}_n = \\frac{1}{n}\\sum_{i=1}^n \\xi_i$ satisfies $\\mathbb{E}[\\bar{\\xi}_n] = \\mathbb{E}[\\xi]$ and $\\operatorname{Var}(\\bar{\\xi}_n) = \\operatorname{Var}(\\xi)/n$.\n\nTasks:\n1. Empirical characterization. For each test case below, construct empirical estimates of the function $n \\mapsto \\mathbb{E}\\left[ F(\\hat{x}_n) \\right]$ by Monte Carlo simulation. Use a fixed grid of sample sizes $\\mathcal{N} = \\{1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144\\}$ and, for each $n \\in \\mathcal{N}$, perform a fixed number of independent Monte Carlo replicates to estimate $\\mathbb{E}\\left[ F(\\hat{x}_n) \\right]$ by averaging $F(\\hat{x}_n)$ computed per replicate. In each replicate, draw $n$ independent samples to construct $\\hat{x}_n$ and evaluate $F(\\hat{x}_n)$ using an independent realization of $\\xi$ or an equivalent unbiased computation based on the definitions above. Use a fixed number of replicates across all test cases to ensure comparability.\n2. Theoretical characterization. Based on the fundamental base, derive a closed-form expression for $\\mathbb{E}\\left[ F(\\hat{x}_n) \\right]$ as a function of $n$ and $\\operatorname{Var}(\\xi)$ and analytically characterize the slope of $n \\mapsto \\mathbb{E}\\left[ F(\\hat{x}_n) \\right]$ as $n$ increases.\n3. Model fitting and decision rule. Fit the parametric model $m(n) = a + \\frac{b}{n}$ to the empirically estimated curve for $n \\in \\mathcal{N}$ by least squares to obtain estimates $(\\hat{a}, \\hat{b})$. Then, for a given nonnegative sample cost $c$ (in the same units as the objective value), consider the total expected loss $L(n) = \\mathbb{E}\\left[ F(\\hat{x}_n) \\right] + c \\, n$, and determine a cost-effective sample size $\\hat{n}$ by minimizing the fitted model $m(n) + c \\, n$ over integers $n \\geq 1$ using an analytic approximation derived from the fitted parameters.\n4. Theoretical benchmark. For each test case, compute the theoretical optimal integer sample size $n^\\star$ using the analytic characterization from Task $2$ combined with the decision rule $L(n)$ above.\n5. Output specification. For each test case, return the pair $(\\hat{n}, n^\\star)$ in the order listed below. Aggregate the outputs across all test cases into a single list in the specified order: $\\left[\\hat{n}_1, n^\\star_1, \\hat{n}_2, n^\\star_2, \\hat{n}_3, n^\\star_3, \\hat{n}_4, n^\\star_4\\right]$, where the index denotes the test case. Your program should produce a single line of output containing this list, as a comma-separated list enclosed in square brackets (for example, $\\left[\\text{result}_1, \\text{result}_2, \\text{result}_3\\right]$).\n\nTest suite:\n- Test case $1$: $\\xi \\sim \\mathcal{N}(\\mu, \\sigma^2)$ with $\\mu = 5$ and $\\sigma = 2$, and sample cost $c = 0.05$.\n- Test case $2$: $\\xi \\sim \\text{Uniform}[a,b]$ with $a = 0$ and $b = 10$, and sample cost $c = 0.1$.\n- Test case $3$: $\\xi \\sim \\text{Laplace}(\\mu, s)$ with $\\mu = 0$ and scale $s = 1$, and sample cost $c = 3$.\n- Test case $4$: $\\xi \\sim \\mathcal{N}(\\mu, \\sigma^2)$ with $\\mu = 0$ and $\\sigma = 5$, and sample cost $c = 0.001$.\n\nAll numerical answers should be unitless real numbers or integers as appropriate, and angles are not involved. The final answers must be integers; the model-fit based decision $\\hat{n}$ and the theoretical benchmark $n^\\star$ should be rounded to the nearest integer and constrained to be at least $1$.", "solution": "The problem asks to analyze the Sample Average Approximation (SAA) method for a stochastic optimization problem, and to determine a cost-effective sample size based on both theoretical derivation and empirical simulation.\n\nThe problem is stated as minimizing the expected objective function $F(x) = \\mathbb{E}\\left[ \\phi(x,\\xi) \\right]$, where $\\phi(x,\\xi) = (x - \\xi)^2$, $x \\in \\mathbb{R}$ is the decision variable, and $\\xi$ is a real-valued random variable with a finite second moment. The SAA estimator $\\hat{x}_n$ is the minimizer of the sample average objective $\\frac{1}{n} \\sum_{i=1}^{n} \\phi(x, \\xi_i)$ for a sample of size $n$.\n\nThe solution proceeds by first deriving the theoretical properties of the estimator and the expected objective value, and then detailing the empirical simulation and fitting procedure.\n\n### 1. Theoretical Characterization\n\nFirst, we derive a closed-form expression for $\\mathbb{E}\\left[ F(\\hat{x}_n) \\right]$ as a function of the sample size $n$. This corresponds to Task 2.\n\nThe true objective function is $F(x) = \\mathbb{E}[(x-\\xi)^2]$. Let $\\mu = \\mathbb{E}[\\xi]$ and $\\sigma^2 = \\operatorname{Var}(\\xi)$. Expanding the expectation gives:\n$$F(x) = \\mathbb{E}[x^2 - 2x\\xi + \\xi^2] = x^2 - 2x\\mathbb{E}[\\xi] + \\mathbb{E}[\\xi^2] = x^2 - 2x\\mu + (\\mu^2 + \\sigma^2)$$\nThis is a convex quadratic function of $x$. The minimizer $x^\\star$ is found by setting the first derivative to zero:\n$$\\frac{dF(x)}{dx} = 2x - 2\\mu = 0 \\implies x^\\star = \\mu$$\nThe minimum value of the true objective is $F(x^\\star) = F(\\mu) = (\\mu-\\mu)^2 + \\sigma^2 = \\sigma^2 = \\operatorname{Var}(\\xi)$.\n\nThe SAA estimator $\\hat{x}_n$ minimizes the sample-averaged objective function $\\hat{F}_n(x) = \\frac{1}{n}\\sum_{i=1}^n (x - \\xi_i)^2$. Setting its derivative to zero gives:\n$$\\frac{d\\hat{F}_n(x)}{dx} = \\frac{1}{n}\\sum_{i=1}^n 2(x - \\xi_i) = 2x - \\frac{2}{n}\\sum_{i=1}^n \\xi_i = 0$$\nThis yields the SAA estimator $\\hat{x}_n = \\frac{1}{n}\\sum_{i=1}^n \\xi_i$, which is the sample mean, denoted as $\\bar{\\xi}_n$.\n\nWe are interested in the expected performance of this estimator, $\\mathbb{E}[F(\\hat{x}_n)]$. The expectation here is taken with respect to the distribution of the sample $\\{\\xi_1, \\dots, \\xi_n\\}$ which determines $\\hat{x}_n$.\nThe function $F(\\cdot)$ evaluated at the random point $\\hat{x}_n$ is:\n$$F(\\hat{x}_n) = (\\hat{x}_n - \\mu)^2 + \\sigma^2$$\nThis expression itself is a random variable, because $\\hat{x}_n$ depends on the sample. To find its expected value, we take the expectation over all possible samples:\n$$\\mathbb{E}[F(\\hat{x}_n)] = \\mathbb{E}\\left[ (\\hat{x}_n - \\mu)^2 + \\sigma^2 \\right]$$\nUsing the linearity of expectation, this becomes:\n$$\\mathbb{E}[F(\\hat{x}_n)] = \\mathbb{E}\\left[ (\\hat{x}_n - \\mu)^2 \\right] + \\sigma^2$$\nThe term $\\mathbb{E}\\left[ (\\hat{x}_n - \\mu)^2 \\right]$ is the definition of the variance of $\\hat{x}_n$, since $\\mathbb{E}[\\hat{x}_n] = \\mathbb{E}[\\bar{\\xi}_n] = \\mu$. For independent and identically distributed samples, the variance of the sample mean is:\n$$\\operatorname{Var}(\\hat{x}_n) = \\operatorname{Var}(\\bar{\\xi}_n) = \\frac{\\operatorname{Var}(\\xi)}{n} = \\frac{\\sigma^2}{n}$$\nSubstituting this back, we obtain the theoretical characterization:\n$$\\mathbb{E}[F(\\hat{x}_n)] = \\sigma^2 + \\frac{\\sigma^2}{n} = \\operatorname{Var}(\\xi)\\left(1 + \\frac{1}{n}\\right)$$\nThis expression has the form $a+b/n$ with $a = b = \\operatorname{Var}(\\xi)$. The slope with respect to $n$ is $-\\operatorname{Var}(\\xi)/n^2$, which shows diminishing returns as $n$ increases.\n\n### 2. Optimal Sample Size Determination\n\nThe problem defines a total expected loss $L(n) = \\mathbb{E}\\left[ F(\\hat{x}_n) \\right] + c \\, n$, where $c$ is the cost per sample. The goal is to find the sample size $n$ that minimizes this total loss.\n\n**Theoretical Benchmark ($n^\\star$)**\nUsing the derived closed-form expression for $\\mathbb{E}[F(\\hat{x}_n)]$ (Task 4):\n$$L(n) = \\operatorname{Var}(\\xi) + \\frac{\\operatorname{Var}(\\xi)}{n} + cn$$\nTo find the minimum, we treat $n$ as a continuous variable for $n  0$ and set the derivative to zero:\n$$\\frac{dL(n)}{dn} = -\\frac{\\operatorname{Var}(\\xi)}{n^2} + c = 0$$\nThis gives $n^2 = \\frac{\\operatorname{Var}(\\xi)}{c}$, so the optimal continuous sample size is $n = \\sqrt{\\frac{\\operatorname{Var}(\\xi)}{c}}$. Since the sample size must be a positive integer, the theoretical optimal sample size $n^\\star$ is found by rounding this value to the nearest integer and ensuring it is at least $1$:\n$$n^\\star = \\max\\left(1, \\left\\lfloor \\sqrt{\\frac{\\operatorname{Var}(\\xi)}{c}} + 0.5 \\right\\rfloor\\right)$$\n\n**Empirical Estimate ($\\hat{n}$)**\nThe empirical procedure (Tasks 1 and 3) involves three steps:\n1.  **Monte Carlo Simulation**: For each sample size $n_j \\in \\mathcal{N}$, we estimate $\\mathbb{E}[F(\\hat{x}_n)]$ by averaging over a large number of independent replications. In each replication, we draw a sample $\\{\\xi_i\\}_{i=1}^n$, compute $\\hat{x}_n = \\bar{\\xi}_n$, and then calculate $F(\\hat{x}_n) = (\\hat{x}_n - \\mu)^2 + \\sigma^2$. The average of these values across all replications provides an empirical estimate $y_j \\approx \\mathbb{E}[F(\\hat{x}_n)]$. This generates a set of data points $(n_j, y_j)$ for $n_j \\in \\mathcal{N}$.\n2.  **Model Fitting**: We fit the parametric model $m(n) = a + b/n$ to the empirical data $(n_j, y_j)_{j=1}^{|\\mathcal{N}|}$. This is a linear regression problem if we let $z_j = 1/n_j$. We seek parameters $(\\hat{a}, \\hat{b})$ that minimize the sum of squared errors $\\sum (y_j - (a + b/n_j))^2$.\n3.  **Decision Rule**: Using the fitted model, we define an approximate total loss function $\\hat{L}(n) = m(n) + cn = \\hat{a} + \\frac{\\hat{b}}{n} + cn$. Minimizing this function with respect to $n$ (assuming $\\hat{b}  0$) yields an optimal continuous sample size $n = \\sqrt{\\frac{\\hat{b}}{c}}$. The integer-valued estimate $\\hat{n}$ is obtained by rounding:\n$$\\hat{n} = \\max\\left(1, \\left\\lfloor \\sqrt{\\frac{\\hat{b}}{c}} + 0.5 \\right\\rfloor\\right)$$\n\n### 3. Application to Test Cases\n\nThe procedure is applied to each test case.\n-   **Test Case 1**: $\\xi \\sim \\mathcal{N}(\\mu=5, \\sigma=2)$. $\\operatorname{Var}(\\xi) = 2^2=4$. Cost $c=0.05$.\n    $n^\\star = \\max(1, \\text{round}(\\sqrt{4/0.05})) = \\max(1, \\text{round}(\\sqrt{80})) = \\max(1, \\text{round}(8.944)) = 9$.\n-   **Test Case 2**: $\\xi \\sim \\text{Uniform}[0, 10]$. $\\operatorname{Var}(\\xi) = (10-0)^2/12 = 100/12$. Cost $c=0.1$.\n    $n^\\star = \\max(1, \\text{round}(\\sqrt{(100/12)/0.1})) = \\max(1, \\text{round}(\\sqrt{83.333})) = \\max(1, \\text{round}(9.129)) = 9$.\n-   **Test Case 3**: $\\xi \\sim \\text{Laplace}(\\mu=0, s=1)$. $\\operatorname{Var}(\\xi) = 2s^2=2$. Cost $c=3$.\n    $n^\\star = \\max(1, \\text{round}(\\sqrt{2/3})) = \\max(1, \\text{round}(\\sqrt{0.667})) = \\max(1, \\text{round}(0.816)) = 1$.\n-   **Test Case 4**: $\\xi \\sim \\mathcal{N}(\\mu=0, \\sigma=5)$. $\\operatorname{Var}(\\xi) = 5^2=25$. Cost $c=0.001$.\n    $n^\\star = \\max(1, \\text{round}(\\sqrt{25/0.001})) = \\max(1, \\text{round}(\\sqrt{25000})) = \\max(1, \\text{round}(158.114)) = 158$.\n\nThe values for $\\hat{n}$ depend on the outcome of the Monte Carlo simulation and are computed in the accompanying code.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the SAA problem by performing theoretical calculations and empirical simulations\n    to find the optimal sample sizes.\n    \"\"\"\n    \n    # Set a seed for reproducibility of random number generation.\n    np.random.seed(42)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'name': 'Case 1', 'dist': 'normal', 'params': {'mu': 5.0, 'sigma': 2.0}, 'c': 0.05},\n        {'name': 'Case 2', 'dist': 'uniform', 'params': {'a': 0.0, 'b': 10.0}, 'c': 0.1},\n        {'name': 'Case 3', 'dist': 'laplace', 'params': {'mu': 0.0, 's': 1.0}, 'c': 3.0},\n        {'name': 'Case 4', 'dist': 'normal', 'params': {'mu': 0.0, 'sigma': 5.0}, 'c': 0.001},\n    ]\n\n    # Fixed parameters for the Monte Carlo simulation.\n    N_GRID = np.array([1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144])\n    N_REPS = 50000  # Number of Monte Carlo replicates.\n\n    results = []\n    for case in test_cases:\n        c = case['c']\n        \n        # Determine distribution parameters and variance\n        if case['dist'] == 'normal':\n            mu = case['params']['mu']\n            sigma = case['params']['sigma']\n            var_xi = sigma**2\n            def sample_generator(size):\n                return np.random.normal(loc=mu, scale=sigma, size=size)\n        elif case['dist'] == 'uniform':\n            a, b = case['params']['a'], case['params']['b']\n            mu = (a + b) / 2.0\n            var_xi = (b - a)**2 / 12.0\n            def sample_generator(size):\n                return np.random.uniform(low=a, high=b, size=size)\n        elif case['dist'] == 'laplace':\n            mu = case['params']['mu']\n            s = case['params']['s']\n            var_xi = 2.0 * s**2\n            def sample_generator(size):\n                return np.random.laplace(loc=mu, scale=s, size=size)\n\n        # Task 1: Empirical characterization\n        empirical_y = []\n        for n in N_GRID:\n            # Vectorized computation for efficiency\n            # Generate all samples for all replicates at once\n            samples = sample_generator(size=(N_REPS, n))\n            # Compute SAA estimator (x_hat_n) for each replicate\n            x_hat_n_reps = np.mean(samples, axis=1)\n            # Compute F(x_hat_n) for each replicate using the analytical formula\n            F_x_hat_n_reps = (x_hat_n_reps - mu)**2 + var_xi\n            # Estimate E[F(x_hat_n)] by averaging over replicates\n            estimated_E_F_xn = np.mean(F_x_hat_n_reps)\n            empirical_y.append(estimated_E_F_xn)\n        \n        empirical_y = np.array(empirical_y)\n\n        # Task 3: Model fitting and decision rule\n        # Fit model m(n) = a + b/n, which is linear in 1/n\n        z = 1.0 / N_GRID\n        # Design matrix for least squares: y = a*1 + b*z\n        A = np.vstack([np.ones(len(z)), z]).T\n        \n        # Solve for [a, b] using least squares\n        # The result gives [intercept, slope], which corresponds to [a, b]\n        a_hat, b_hat = np.linalg.lstsq(A, empirical_y, rcond=None)[0]\n\n        # Determine empirically-derived optimal sample size n_hat\n        # Minimize L(n) = a_hat + b_hat/n + c*n\n        if b_hat = 0:\n            # If b_hat is non-positive, the cost function L(n) is monotonically increasing for n  0.\n            # The minimum occurs at the smallest possible integer n.\n            n_hat = 1\n        else:\n            # From dL/dn = 0, we get n = sqrt(b_hat/c)\n            n_hat_continuous = np.sqrt(b_hat / c)\n            # Round to the nearest integer, with a minimum of 1.\n            n_hat = max(1, int(np.round(n_hat_continuous)))\n        \n        results.append(n_hat)\n\n        # Task 4: Theoretical benchmark\n        # Optimal n is derived from minimizing L(n) = var_xi + var_xi/n + c*n\n        # This gives n = sqrt(var_xi / c)\n        n_star_continuous = np.sqrt(var_xi / c)\n        # Round to the nearest integer, with a minimum of 1.\n        n_star = max(1, int(np.round(n_star_continuous)))\n        \n        results.append(n_star)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3174809"}]}