{"hands_on_practices": [{"introduction": "A core challenge in Stochastic Gradient Descent (SGD) is balancing convergence speed with the variance introduced by noisy gradients. This practice provides a rigorous analysis of this trade-off by examining a polynomial decay schedule, $\\eta_t = \\eta_0 t^{-\\alpha}$. By deriving the asymptotic behavior of the error, you will construct a clear counterexample demonstrating how an insufficiently fast decay rate ($\\alpha \\lt 1/2$) leads to a persistent \"noise floor,\" preventing the iterates from fully converging [@problem_id:3185967].", "problem": "Consider Stochastic Gradient Descent (SGD), defined by the iterative update $x_{t+1} = x_t - \\eta_t g_t$, on the one-dimensional objective $f(x) = \\frac{1}{2}x^2$, where the true gradient is $\\nabla f(x_t) = x_t$. Assume the stochastic gradient is $g_t = x_t + \\xi_t$, where $(\\xi_t)_{t \\geq 0}$ are independent, identically distributed random variables satisfying $\\mathbb{E}[\\xi_t] = 0$ and $\\mathrm{Var}(\\xi_t) = \\sigma^2$, independent of the past. Let the step size schedule be $\\eta_t = \\eta_0 t^{-\\alpha}$ with fixed constants $\\eta_0  0$ and $\\alpha \\in (0, \\frac{1}{2})$, and let the initial condition $x_0$ be deterministic and finite.\n\nStarting only from the stated definitions of SGD, the objective and its gradient, and basic properties of variance and expectation, perform the following:\n\n1. Derive a closed recursion for $\\mathbb{E}[x_{t+1}^2]$ in terms of $\\eta_t$, $\\mathbb{E}[x_t^2]$, and $\\sigma^2$.\n2. Using an asymptotic analysis justified by the slowly varying step size schedule $\\eta_t = \\eta_0 t^{-\\alpha}$, determine the leading-order behavior of $\\mathbb{E}[x_t^2]$ as $t \\to \\infty$, clarifying how it relates to the instantaneous “noise floor” induced by the stochastic gradient.\n3. Compute the exact value of the scaled limit\n$$\nL \\;=\\; \\lim_{t \\to \\infty} \\frac{t^{\\alpha}\\,\\mathbb{E}[x_t^2]}{\\eta_0 \\sigma^2}.\n$$\n\nYou must present $L$ as a single closed-form constant. No numerical rounding is required. This construction serves as a counterexample showing that for $\\alpha  \\frac{1}{2}$, the sequence exhibits a non-vanishing normalized error floor of order $\\mathcal{O}(\\eta_t \\sigma^2)$ in the sense captured by the limit $L$ above.", "solution": "The user wants to analyze the asymptotic behavior of Stochastic Gradient Descent (SGD) on a simple one-dimensional quadratic objective. The analysis will proceed in three parts as requested: deriving a recursion for the second moment of the state, determining its asymptotic behavior, and computing a specific scaled limit.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n-   SGD update rule: $x_{t+1} = x_t - \\eta_t g_t$\n-   Objective function: $f(x) = \\frac{1}{2}x^2$\n-   True gradient: $\\nabla f(x_t) = x_t$\n-   Stochastic gradient: $g_t = x_t + \\xi_t$\n-   Noise process $(\\xi_t)_{t \\geq 0}$: independent, identically distributed (i.i.d.) random variables.\n-   Noise properties: $\\mathbb{E}[\\xi_t] = 0$ and $\\mathrm{Var}(\\xi_t) = \\mathbb{E}[\\xi_t^2] - (\\mathbb{E}[\\xi_t])^2 = \\sigma^2$. Since $\\mathbb{E}[\\xi_t]=0$, this implies $\\mathbb{E}[\\xi_t^2] = \\sigma^2$. The noise is independent of the past, i.e., of $x_0, x_1, \\dots, x_t$.\n-   Step size schedule: $\\eta_t = \\eta_0 t^{-\\alpha}$ for constants $\\eta_0  0$ and $\\alpha \\in (0, \\frac{1}{2})$.\n-   Initial condition: $x_0$ is a deterministic and finite value.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientifically Grounded:** The problem is a standard, fundamental exercise in the analysis of stochastic optimization algorithms, specifically SGD. The setup is a canonical model used in textbooks and research papers to understand the convergence properties of SGD.\n-   **Well-Posed:** The problem is well-posed. All variables and parameters are clearly defined, and the constraints on them (e.g., $\\alpha \\in (0, \\frac{1}{2})$) are specified. The tasks are precise mathematical objectives leading to a unique answer.\n-   **Objective:** The problem is stated in precise, formal mathematical language, free of ambiguity or subjective claims.\n\n**Step 3: Verdict and Action**\nThe problem is valid as it is scientifically grounded, well-posed, and objective. I will proceed with the solution.\n\n### Part 1: Derivation of the Recursion for $\\mathbb{E}[x_{t+1}^2]$\n\nWe begin with the SGD update rule:\n$$\nx_{t+1} = x_t - \\eta_t g_t\n$$\nSubstitute the expression for the stochastic gradient $g_t = x_t + \\xi_t$:\n$$\nx_{t+1} = x_t - \\eta_t (x_t + \\xi_t) = (1 - \\eta_t)x_t - \\eta_t \\xi_t\n$$\nNext, we square both sides of the equation:\n$$\nx_{t+1}^2 = \\left( (1 - \\eta_t)x_t - \\eta_t \\xi_t \\right)^2 = (1 - \\eta_t)^2 x_t^2 - 2(1 - \\eta_t)\\eta_t x_t \\xi_t + \\eta_t^2 \\xi_t^2\n$$\nNow, we take the expectation of both sides. Let $\\mathcal{F}_t$ be the sigma-algebra generated by the history of the process up to time $t$, i.e., $\\sigma(x_0, \\xi_0, \\dots, \\xi_{t-1})$. The iterate $x_t$ is measurable with respect to $\\mathcal{F}_t$. We use the law of total expectation, $\\mathbb{E}[Y] = \\mathbb{E}[\\mathbb{E}[Y | \\mathcal{F}_t]]$.\n$$\n\\mathbb{E}[x_{t+1}^2] = \\mathbb{E}\\left[ (1 - \\eta_t)^2 x_t^2 - 2(1 - \\eta_t)\\eta_t x_t \\xi_t + \\eta_t^2 \\xi_t^2 \\right]\n$$\nBy linearity of expectation:\n$$\n\\mathbb{E}[x_{t+1}^2] = (1 - \\eta_t)^2 \\mathbb{E}[x_t^2] - 2(1 - \\eta_t)\\eta_t \\mathbb{E}[x_t \\xi_t] + \\eta_t^2 \\mathbb{E}[\\xi_t^2]\n$$\nWe evaluate the expectation terms involving the noise $\\xi_t$.\nFor the cross-term $\\mathbb{E}[x_t \\xi_t]$, we condition on the past:\n$$\n\\mathbb{E}[x_t \\xi_t] = \\mathbb{E}[\\mathbb{E}[x_t \\xi_t | \\mathcal{F}_t]] = \\mathbb{E}[x_t \\mathbb{E}[\\xi_t | \\mathcal{F}_t]]\n$$\nSince $\\xi_t$ is independent of the past, $\\mathbb{E}[\\xi_t | \\mathcal{F}_t] = \\mathbb{E}[\\xi_t] = 0$. Thus:\n$$\n\\mathbb{E}[x_t \\xi_t] = \\mathbb{E}[x_t \\cdot 0] = 0\n$$\nFor the noise variance term, we are given $\\mathbb{E}[\\xi_t]=0$ and $\\mathrm{Var}(\\xi_t)=\\sigma^2$, so:\n$$\n\\mathbb{E}[\\xi_t^2] = \\mathrm{Var}(\\xi_t) + (\\mathbb{E}[\\xi_t])^2 = \\sigma^2 + 0^2 = \\sigma^2\n$$\nSubstituting these results back into the equation for $\\mathbb{E}[x_{t+1}^2]$:\n$$\n\\mathbb{E}[x_{t+1}^2] = (1 - \\eta_t)^2 \\mathbb{E}[x_t^2] + \\eta_t^2 \\sigma^2\n$$\nExpanding the squared term $(1-\\eta_t)^2 = 1 - 2\\eta_t + \\eta_t^2$, we obtain the final closed recursion:\n$$\n\\mathbb{E}[x_{t+1}^2] = (1 - 2\\eta_t + \\eta_t^2) \\mathbb{E}[x_t^2] + \\eta_t^2 \\sigma^2\n$$\n\n### Part 2: Asymptotic Analysis of $\\mathbb{E}[x_t^2]$\n\nLet $w_t = \\mathbb{E}[x_t^2]$. The recursion is:\n$$\nw_{t+1} = (1 - 2\\eta_t + \\eta_t^2) w_t + \\eta_t^2 \\sigma^2\n$$\nThis can be rewritten as a difference equation:\n$$\nw_{t+1} - w_t = (-2\\eta_t + \\eta_t^2) w_t + \\eta_t^2 \\sigma^2\n$$\nThe problem states that the step size schedule $\\eta_t = \\eta_0 t^{-\\alpha}$ is \"slowly varying\". This justifies a quasi-stationary analysis. In this regime, the system's state $w_t$ adapts quickly to the slow changes in the parameter $\\eta_t$. We can therefore approximate the asymptotic behavior by finding the \"moving fixed point\" of the dynamics, which corresponds to setting the change $w_{t+1} - w_t$ to zero.\n$$\n0 \\approx (-2\\eta_t + \\eta_t^2) w_t + \\eta_t^2 \\sigma^2\n$$\nSolving for $w_t$:\n$$\n(2\\eta_t - \\eta_t^2) w_t \\approx \\eta_t^2 \\sigma^2\n$$\n$$\nw_t \\approx \\frac{\\eta_t^2 \\sigma^2}{2\\eta_t - \\eta_t^2} = \\frac{\\eta_t \\sigma^2}{2 - \\eta_t}\n$$\nThis expression describes the \"instantaneous noise floor\" mentioned in the problem. It represents the equilibrium level where the drift towards the origin (due to the term $-2\\eta_t w_t$) is balanced by the variance introduced by the stochastic noise (the term $\\eta_t^2 \\sigma^2$) and the higher-order self-correction term $\\eta_t^2 w_t$.\n\nAs $t \\to \\infty$, we have $\\eta_t = \\eta_0 t^{-\\alpha} \\to 0$ since $\\alpha  0$. Therefore, the denominator $2 - \\eta_t \\to 2$. The leading-order asymptotic behavior of $\\mathbb{E}[x_t^2]$ is:\n$$\nw_t = \\mathbb{E}[x_t^2] \\sim \\frac{\\eta_t \\sigma^2}{2} = \\frac{\\eta_0 \\sigma^2}{2} t^{-\\alpha}\n$$\nThe chosen schedule $\\eta_t = \\eta_0 t^{-\\alpha}$ with $\\alpha \\in (0, 1/2)$ violates one of the Robbins-Monro conditions for full convergence. While the sum of step sizes diverges ($\\sum \\eta_t = \\infty$), which allows the iterates to travel infinitely, the sum of squared step sizes also diverges ($\\sum_t \\eta_t^2 = \\infty$, since $2\\alpha  1$). This second condition's violation prevents the accumulated noise from being averaged away. Instead, the error enters a quasi-stationary state where it tracks the step size. Our asymptotic analysis precisely captures the magnitude of this floor.\n\n### Part 3: Computation of the Scaled Limit $L$\n\nWe are asked to compute the exact value of the scaled limit:\n$$\nL = \\lim_{t \\to \\infty} \\frac{t^{\\alpha}\\,\\mathbb{E}[x_t^2]}{\\eta_0 \\sigma^2}\n$$\nUsing the asymptotic result for $\\mathbb{E}[x_t^2] = w_t$ from Part 2, we substitute the more precise expression $w_t \\approx \\frac{\\eta_t \\sigma^2}{2 - \\eta_t}$ into the limit.\n$$\nL = \\lim_{t \\to \\infty} \\frac{t^{\\alpha} \\left( \\frac{\\eta_t \\sigma^2}{2 - \\eta_t} \\right)}{\\eta_0 \\sigma^2}\n$$\nNow, substitute the definition of the step size, $\\eta_t = \\eta_0 t^{-\\alpha}$:\n$$\nL = \\lim_{t \\to \\infty} \\frac{t^{\\alpha} \\left( \\frac{(\\eta_0 t^{-\\alpha}) \\sigma^2}{2 - \\eta_0 t^{-\\alpha}} \\right)}{\\eta_0 \\sigma^2}\n$$\nThe terms $t^{\\alpha}$ and $t^{-\\alpha}$ in the numerator cancel out:\n$$\nL = \\lim_{t \\to \\infty} \\frac{\\frac{\\eta_0 \\sigma^2}{2 - \\eta_0 t^{-\\alpha}}}{\\eta_0 \\sigma^2}\n$$\nWe can cancel the common factor $\\eta_0 \\sigma^2$:\n$$\nL = \\lim_{t \\to \\infty} \\frac{1}{2 - \\eta_0 t^{-\\alpha}}\n$$\nSince $\\alpha  0$, as $t \\to \\infty$, the term $t^{-\\alpha} \\to 0$. Therefore, the limit is:\n$$\nL = \\frac{1}{2 - 0} = \\frac{1}{2}\n$$\nThe value of the limit is a constant, independent of $\\eta_0$, $\\sigma^2$, and $\\alpha$ (within the specified range). This confirms that for $\\alpha  1/2$, the normalized error $\\frac{\\mathbb{E}[x_t^2]}{\\eta_t \\sigma^2}$ converges to a non-zero constant.", "answer": "$$\n\\boxed{\\frac{1}{2}}\n$$", "id": "3185967"}, {"introduction": "While simple decay schedules are common, modern deep learning practice often favors more structured approaches like a warmup-plateau-decay schedule. This exercise guides you through optimizing such a schedule by explicitly modeling the final error as a trade-off between bias (insufficient convergence) and variance (noise accumulation) [@problem_id:3185881]. By minimizing this error, you will determine the optimal peak step-size, $\\eta_{\\max}$, gaining insight into how to design schedules that are both aggressive in the beginning and stable at the end.", "problem": "Consider one-dimensional Stochastic Gradient Descent (SGD) applied to the quadratic objective $f(x) = \\frac{1}{2} x^{2}$ with additive gradient noise. The discrete-time update is\n$$\nx_{t+1} = x_{t} - \\eta_{t} \\bigl( x_{t} + \\xi_{t} \\bigr),\n$$\nwhere $\\{\\xi_{t}\\}$ is an independent sequence with $\\mathbb{E}[\\xi_{t}] = 0$ and $\\operatorname{Var}(\\xi_{t}) = \\sigma^{2}$, and where $\\{\\eta_{t}\\}$ is a nonnegative step-size schedule. Let the time horizon be $T = w + p + d$, with $w  0$, $p  0$, and $d  0$. Consider the following piecewise-linear step-size schedule in continuous time $\\eta(t)$:\n- Warmup on $t \\in [0,w]$: $\\eta(t) = \\eta_{\\max} \\frac{t}{w}$.\n- Plateau on $t \\in (w, w + p]$: $\\eta(t) = \\eta_{\\max}$.\n- Linear decay on $t \\in (w + p, w + p + d]$: $\\eta(t) = \\eta_{\\max} \\left( 1 - \\frac{t - (w + p)}{d} \\right)$.\n\nAssume the small-step, continuous-time approximation for the second moment of $x_{t}$ is valid, and model the expected final squared error by the simplified bias-variance expression\n$$\n\\mathbb{E}\\bigl[x(T)^{2}\\bigr] \\approx x_{0}^{2} \\exp\\!\\left( - 2 \\int_{0}^{T} \\eta(t) \\, dt \\right) + \\sigma^{2} \\int_{0}^{T} \\eta(t)^{2} \\, dt,\n$$\nwhere $x_{0}$ is the deterministic initial condition.\n\nUsing only fundamental definitions of SGD and the stated noise model, derive the expression above from first principles and then minimize it with respect to the plateau level $\\eta_{\\max}$ while keeping $w$, $p$, $d$, $x_{0}$, and $\\sigma$ fixed. Express your final answer as a single closed-form analytical expression for the optimal $\\eta_{\\max}$ in terms of $w$, $p$, $d$, $x_{0}$, and $\\sigma$. No rounding is required.", "solution": "The problem asks for two main tasks: first, to derive a given simplified expression for the expected final squared error of a one-dimensional Stochastic Gradient Descent (SGD) process; and second, to minimize this error with respect to the peak step-size $\\eta_{\\max}$ for a specific piecewise-linear schedule.\n\n### Part 1: Derivation of the Error Model\n\nWe are asked to derive the following expression for the expected final squared error $\\mathbb{E}[x(T)^2]$:\n$$\n\\mathbb{E}\\bigl[x(T)^{2}\\bigr] \\approx x_{0}^{2} \\exp\\!\\left( - 2 \\int_{0}^{T} \\eta(t) \\, dt \\right) + \\sigma^{2} \\int_{0}^{T} \\eta(t)^{2} \\, dt\n$$\nThe derivation starts from the discrete-time SGD update rule for the objective function $f(x) = \\frac{1}{2}x^2$, which has a gradient $\\nabla f(x) = x$. The stochastic gradient is $g(x_t, \\xi_t) = x_t + \\xi_t$.\nThe update rule is:\n$$\nx_{t+1} = x_{t} - \\eta_{t} (x_{t} + \\xi_{t}) = (1 - \\eta_{t})x_{t} - \\eta_{t}\\xi_{t}\n$$\nwhere $\\xi_t$ is a noise term with $\\mathbb{E}[\\xi_t] = 0$ and $\\operatorname{Var}(\\xi_t) = \\sigma^2$.\n\nWe decompose the mean squared error into its bias and variance components:\n$$\n\\mathbb{E}[x_T^2] = \\left(\\mathbb{E}[x_T]\\right)^2 + \\operatorname{Var}(x_T)\n$$\n\n**1. Bias Term Analysis:**\nFirst, we find the evolution of the mean value of $x_t$. Let $y_t = \\mathbb{E}[x_t]$. Taking the expectation of the update rule:\n$$\n\\mathbb{E}[x_{t+1}] = \\mathbb{E}[(1 - \\eta_{t})x_{t} - \\eta_{t}\\xi_{t}]\n$$\nSince $\\xi_t$ is independent of $x_t$ and has zero mean, $\\mathbb{E}[\\eta_t \\xi_t] = \\eta_t \\mathbb{E}[\\xi_t] = 0$.\n$$\ny_{t+1} = (1 - \\eta_{t}) y_{t}\n$$\nIn the small-step, continuous-time limit ($\\eta_t \\to 0$, $t$ becomes a continuous variable), this recurrence relation can be approximated by a differential equation. Let $y(t)$ be the continuous-time counterpart of $y_t$. The change $y_{t+1} - y_t$ corresponds to the derivative $\\frac{dy}{dt}$.\n$$\ny_{t+1} - y_t = -\\eta_t y_t \\implies \\frac{dy}{dt} = -\\eta(t) y(t)\n$$\nThis is a first-order linear ordinary differential equation. With the initial condition $y(0) = \\mathbb{E}[x_0] = x_0$ (since $x_0$ is deterministic), the solution is:\n$$\ny(T) = x_0 \\exp\\left(-\\int_0^T \\eta(t) dt\\right)\n$$\nThe squared bias at time $T$ is therefore:\n$$\n\\left(\\mathbb{E}[x_T]\\right)^2 = x_0^2 \\exp\\left(-2\\int_0^T \\eta(t) dt\\right)\n$$\nThis matches the first term in the given expression.\n\n**2. Variance Term Analysis:**\nThe second term represents the accumulated variance. The term $\\sigma^2 \\int_0^T \\eta(t)^2 dt$ is a simplified model for the variance. A rigorous derivation yields a more complex expression, $\\sigma^2 \\int_0^T \\eta(t)^2 \\exp(-2\\int_t^T \\eta(s)ds) dt$. The expression given in the problem is a common simplification, justified as follows:\nAt each discrete step $t$, the update adds a noise term $-\\eta_t \\xi_t$. The variance of this term is:\n$$\n\\operatorname{Var}(-\\eta_t \\xi_t) = \\eta_t^2 \\operatorname{Var}(-\\xi_t) = \\eta_t^2 \\operatorname{Var}(\\xi_t) = \\eta_t^2 \\sigma^2\n$$\nThe simplified model for the total variance at time $T$ assumes that the final variance is the simple sum of the variances introduced at each step, ignoring the contraction effect of the dynamics on previously accumulated variance. This approximation is most accurate when the step-sizes $\\eta_t$ are small, such that the system state does not change drastically.\nUnder this simplifying assumption, the total variance is the sum of the incremental contributions:\n$$\n\\operatorname{Var}(x_T) \\approx \\sum_{t=0}^{T-1} \\eta_t^2 \\sigma^2\n$$\nIn the continuous-time limit, this sum becomes an integral:\n$$\n\\operatorname{Var}(x_T) \\approx \\sigma^2 \\int_0^T \\eta(t)^2 dt\n$$\nThis matches the second term in the given expression.\n\n**3. Combining Terms:**\nCombining the derived squared bias and the simplified variance gives the desired expression for the expected final squared error:\n$$\n\\mathbb{E}[x(T)^2] \\approx x_0^2 \\exp\\left(-2\\int_0^T \\eta(t) dt\\right) + \\sigma^2 \\int_0^T \\eta(t)^2 dt\n$$\nThis completes the derivation of the model.\n\n### Part 2: Minimization of the Error\n\nWe now minimize this expression for $\\mathbb{E}[x(T)^2]$ with respect to $\\eta_{\\max}$. Let $E(\\eta_{\\max}) = \\mathbb{E}[x(T)^2]$. The expression is:\n$$\nE(\\eta_{\\max}) = x_0^2 \\exp(-2 I_1) + \\sigma^2 I_2\n$$\nwhere $I_1 = \\int_0^T \\eta(t) dt$ and $I_2 = \\int_0^T \\eta(t)^2 dt$. We first compute these integrals for the given piecewise-linear schedule $\\eta(t)$.\n\n**Calculation of $I_1$:**\n$I_1 = \\int_0^w \\eta_{\\max} \\frac{t}{w} dt + \\int_w^{w+p} \\eta_{\\max} dt + \\int_{w+p}^{w+p+d} \\eta_{\\max} \\left( 1 - \\frac{t - (w+p)}{d} \\right) dt$.\n1.  Warmup: $\\int_0^w \\eta_{\\max} \\frac{t}{w} dt = \\frac{\\eta_{\\max}}{w} \\left[\\frac{t^2}{2}\\right]_0^w = \\frac{1}{2} w \\eta_{\\max}$.\n2.  Plateau: $\\int_w^{w+p} \\eta_{\\max} dt = \\eta_{\\max} [t]_w^{w+p} = p \\eta_{\\max}$.\n3.  Decay: $\\int_{w+p}^{w+p+d} \\eta_{\\max} \\left( 1 - \\frac{t - (w+p)}{d} \\right) dt = \\eta_{\\max} \\left[ t - \\frac{(t-(w+p))^2}{2d} \\right]_{w+p}^{w+p+d} = \\eta_{\\max} \\left( (w+p+d) - \\frac{d^2}{2d} - (w+p) \\right) = \\frac{1}{2} d \\eta_{\\max}$.\nSumming the parts:\n$$\nI_1 = \\left(\\frac{w}{2} + p + \\frac{d}{2}\\right) \\eta_{\\max} = \\left(\\frac{w+d+2p}{2}\\right) \\eta_{\\max}\n$$\n\n**Calculation of $I_2$:**\n$I_2 = \\int_0^w \\left(\\eta_{\\max} \\frac{t}{w}\\right)^2 dt + \\int_w^{w+p} \\eta_{\\max}^2 dt + \\int_{w+p}^{w+p+d} \\left(\\eta_{\\max} \\left( 1 - \\frac{t - (w+p)}{d} \\right)\\right)^2 dt$.\n1.  Warmup: $\\int_0^w \\frac{\\eta_{\\max}^2}{w^2} t^2 dt = \\frac{\\eta_{\\max}^2}{w^2} \\left[\\frac{t^3}{3}\\right]_0^w = \\frac{1}{3} w \\eta_{\\max}^2$.\n2.  Plateau: $\\int_w^{w+p} \\eta_{\\max}^2 dt = \\eta_{\\max}^2 [t]_w^{w+p} = p \\eta_{\\max}^2$.\n3.  Decay: Let $u=t-(w+p)$. $\\int_0^d \\eta_{\\max}^2 (1-\\frac{u}{d})^2 du = \\eta_{\\max}^2 \\left[u - \\frac{u^2}{d} + \\frac{u^3}{3d^2}\\right]_0^d = \\eta_{\\max}^2 \\left(d - d + \\frac{d}{3}\\right) = \\frac{1}{3} d \\eta_{\\max}^2$.\nSumming the parts:\n$$\nI_2 = \\left(\\frac{w}{3} + p + \\frac{d}{3}\\right) \\eta_{\\max}^2 = \\left(\\frac{w+d+3p}{3}\\right) \\eta_{\\max}^2\n$$\n\nLet $C_1 = \\frac{w+d+2p}{2}$ and $C_2 = \\frac{w+d+3p}{3}$. The objective function becomes:\n$$\nE(\\eta_{\\max}) = x_0^2 \\exp(-2 C_1 \\eta_{\\max}) + \\sigma^2 C_2 \\eta_{\\max}^2\n$$\nTo find the minimum, we differentiate with respect to $\\eta_{\\max}$ and set the derivative to zero. Let $y = \\eta_{\\max}$ for simplicity.\n$$\n\\frac{dE}{dy} = x_0^2 (-2C_1) \\exp(-2C_1 y) + \\sigma^2 C_2 (2y) = 0\n$$\n$$\n2 \\sigma^2 C_2 y = 2 C_1 x_0^2 \\exp(-2C_1 y)\n$$\n$$\ny \\exp(2C_1 y) = \\frac{C_1 x_0^2}{\\sigma^2 C_2}\n$$\nThis is a transcendental equation that can be solved using the Lambert W function, which is defined by $z = W(z) \\exp(W(z))$. To match this form, let $z = 2C_1 y$. Then $y = z/(2C_1)$.\n$$\n\\frac{z}{2C_1} \\exp(z) = \\frac{C_1 x_0^2}{\\sigma^2 C_2}\n$$\n$$\nz \\exp(z) = \\frac{2 C_1^2 x_0^2}{\\sigma^2 C_2}\n$$\nThe solution for $z$ is $z = W\\left(\\frac{2 C_1^2 x_0^2}{\\sigma^2 C_2}\\right)$, where $W$ is the principal branch of the Lambert W function since the argument is non-negative ($w, p, d  0$ implies $C_1, C_2  0$).\n\nThe second derivative, $\\frac{d^2E}{dy^2} = 4C_1^2 x_0^2 \\exp(-2C_1y) + 2\\sigma^2 C_2$, is always positive for $y0$, confirming that this is a minimum.\n\nSubstituting back $y = \\eta_{\\max}$ and $z=2C_1 \\eta_{\\max}$:\n$$\n\\eta_{\\max} = \\frac{z}{2C_1} = \\frac{1}{2C_1} W\\left(\\frac{2 C_1^2 x_0^2}{\\sigma^2 C_2}\\right)\n$$\nFinally, we substitute the expressions for $C_1$ and $C_2$:\n$$\n\\eta_{\\max} = \\frac{1}{2\\left(\\frac{w+d+2p}{2}\\right)} W\\left(\\frac{2 \\left(\\frac{w+d+2p}{2}\\right)^2 x_0^2}{\\sigma^2 \\left(\\frac{w+d+3p}{3}\\right)}\\right)\n$$\n$$\n\\eta_{\\max} = \\frac{1}{w+d+2p} W\\left(\\frac{2 \\frac{(w+d+2p)^2}{4} x_0^2}{\\frac{\\sigma^2(w+d+3p)}{3}}\\right)\n$$\n$$\n\\eta_{\\max} = \\frac{1}{w+d+2p} W\\left(\\frac{\\frac{(w+d+2p)^2}{2} x_0^2}{\\frac{\\sigma^2(w+d+3p)}{3}}\\right)\n$$\nSimplifying the argument of the Lambert W function gives the final result:\n$$\n\\eta_{\\max} = \\frac{1}{w+d+2p} W\\left(\\frac{3 x_0^2 (w+d+2p)^2}{2 \\sigma^2 (w+d+3p)}\\right)\n$$\nThis is the closed-form analytical expression for the optimal $\\eta_{\\max}$.", "answer": "$$\n\\boxed{\\frac{1}{w+d+2p} W\\left(\\frac{3 x_0^2 (w+d+2p)^2}{2 \\sigma^2 (w+d+3p)}\\right)}\n$$", "id": "3185881"}, {"introduction": "Moving beyond manually-specified schedules, this advanced practice explores the concept of meta-optimization: learning the optimal schedule parameters automatically. You will implement a reverse-mode differentiation algorithm to compute the \"hypergradient\" of a final loss with respect to the parameters of the step-size schedule itself [@problem_id:3185953]. This exercise offers a hands-on introduction to the powerful technique of unrolled optimization, revealing both its potential and its computational cost.", "problem": "You are given a deterministic, finite-horizon unrolled optimization using Stochastic Gradient Descent (SGD), where the goal is to learn a step-size schedule by optimizing schedule parameters through hypergradients. Consider the convex quadratic objective in $\\mathbb{R}^3$ defined by\n$$\nf(x) = \\frac{1}{2} x^\\top A x - b^\\top x,\n$$\nwhere $A \\in \\mathbb{R}^{3 \\times 3}$ is symmetric positive definite and $b \\in \\mathbb{R}^3$. The state is updated for $T$ iterations according to the stochastic gradient descent rule\n$$\nx_{t+1} = x_t - \\eta_t(\\theta)\\big(\\nabla f(x_t) + \\xi_t\\big),\n$$\nwhere $\\nabla f(x_t) = A x_t - b$, $\\xi_t \\in \\mathbb{R}^3$ is a fixed stochastic perturbation independent of $\\theta$, and the step-size schedule is parameterized by\n$$\n\\eta_t(\\theta) = \\frac{\\exp(\\theta_0)}{1 + \\exp(\\theta_1)\\, t}, \\quad \\theta = (\\theta_0,\\theta_1) \\in \\mathbb{R}^2.\n$$\nThe schedule learning objective is to adjust $\\theta$ to minimize $f(x_T(\\theta))$ by using the hypergradient $\\nabla_\\theta f(x_T(\\theta))$ that results from differentiating through the unrolled $T$ updates.\n\nStarting only from (i) the definition of Stochastic Gradient Descent (SGD), (ii) the chain rule for differentiation, and (iii) the symmetry and positive definiteness of $A$, perform the following:\n\n1. Derive, from first principles, a reverse-mode computation for the hypergradient $\\nabla_\\theta f(x_T(\\theta))$ without using any automatic differentiation software. Your derivation must explicitly show how to propagate adjoints for the state variables backward in time and how to accumulate the gradient contributions with respect to the schedule parameters $\\theta$.\n\n2. Implement a program that:\n   - Simulates the $T$ updates from an initial condition $x_0 = 0$ using the given schedule $\\eta_t(\\theta)$, a fixed matrix $A$, and vector $b$.\n   - Computes $\\nabla_\\theta f(x_T(\\theta))$ via the derived reverse-mode hypergradient.\n   - Verifies correctness by computing a central finite-difference approximation to $\\nabla_\\theta f(x_T(\\theta))$ using the same fixed noise sequence $\\{\\xi_t\\}_{t=0}^{T-1}$, and returns the Euclidean norm of the difference between the reverse-mode hypergradient and its finite-difference approximation.\n   - Quantifies computational overhead as the ratio of the number of dense matrix-vector multiplications required to compute the hypergradient versus the number required to compute only $f(x_T(\\theta))$ (i.e., forward simulation and final objective evaluation). Assume $A$ is used as a dense matrix, count each multiplication by $A$ as one unit of cost, and justify the counts you use.\n\n3. Use the following fixed data for all simulations:\n   - Matrix $A$:\n     $$\n     A = \\begin{bmatrix}\n     3  1  0 \\\\\n     1  2  0 \\\\\n     0  0  1.5\n     \\end{bmatrix}\n     $$\n   - Vector $b$:\n     $$\n     b = \\begin{bmatrix}\n     1 \\\\\n     -2 \\\\\n     0.5\n     \\end{bmatrix}\n     $$\n   - Initial condition $x_0 = \\begin{bmatrix}0 \\\\ 0 \\\\ 0\\end{bmatrix}$.\n   - Noise vectors $\\xi_t$ are independent draws from a normal distribution with zero mean and covariance $\\sigma^2 I_3$. The randomness must be held fixed within each test case by using a fixed seed to generate the entire sequence $\\{\\xi_t\\}$ for that case. The noise is independent of $\\theta$.\n\n4. Implement the reverse-mode hypergradient using the backward recurrence derived in part 1. Do not use any automatic differentiation tools.\n\n5. Test Suite. Run your program on the following three parameter sets:\n   - Case 1 (boundary case): $T = 1$, $\\theta = (-1.5, -10.0)$, $\\sigma = 0.0$, noise seed $= 0$.\n   - Case 2 (general case): $T = 50$, $\\theta = (-1.0, 0.5)$, $\\sigma = 0.1$, noise seed $= 1$.\n   - Case 3 (long-horizon case): $T = 200$, $\\theta = (-2.0, -0.2)$, $\\sigma = 0.05$, noise seed $= 2$.\n\n6. For each case, your program must output two floating-point numbers:\n   - The Euclidean norm of the difference between the reverse-mode hypergradient and the central finite-difference approximation.\n   - The computational overhead ratio as defined above.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets in the order\n$$\n[\\Delta_1, R_1, \\Delta_2, R_2, \\Delta_3, R_3],\n$$\nwhere $\\Delta_i$ is the Euclidean norm difference and $R_i$ is the overhead ratio for case $i$.", "solution": "The user-provided problem has been validated and is determined to be a well-posed, scientifically grounded problem in the field of optimization. The task requires the derivation and implementation of a reverse-mode automatic differentiation algorithm to compute a hypergradient for a step-size schedule in Stochastic Gradient Descent (SGD).\n\nThe solution is presented in two parts: first, a principled derivation of the reverse-mode hypergradient computation, and second, an analysis of the computational overhead.\n\n### Part 1: Derivation of the Reverse-Mode Hypergradient\n\nThe objective is to compute the gradient of the loss function $L(\\theta) = f(x_T(\\theta))$ with respect to the schedule parameters $\\theta = (\\theta_0, \\theta_1)$. The loss function is defined as $f(x) = \\frac{1}{2} x^\\top A x - b^\\top x$, and the final state $x_T$ is the result of $T$ SGD iterations. The gradient of $f$ with respect to its input $x$ is $\\nabla_x f(x) = Ax - b$.\n\nThe SGD update rule is given by\n$$x_{t+1} = x_t - \\eta_t(\\theta) g_t,$$\nfor $t = 0, 1, \\dots, T-1$, where $x_0=0$, $g_t = \\nabla f(x_t) + \\xi_t = Ax_t - b + \\xi_t$, and the step-size $\\eta_t(\\theta)$ is a function of $\\theta$.\n\nBy the chain rule, a change in $\\theta$ induces a change in the final loss $L$ as follows:\n$$\\nabla_\\theta L(\\theta) = \\left(\\frac{\\partial x_T}{\\partial \\theta}\\right)^\\top \\nabla_{x_T} f(x_T).$$\nHere, $\\frac{\\partial x_T}{\\partial \\theta}$ is the Jacobian matrix of the final state $x_T$ with respect to the parameters $\\theta$. Computing this Jacobian explicitly (forward-mode differentiation) can be inefficient. Reverse-mode differentiation avoids this by propagating gradients backward in time.\n\nLet's define the adjoint state vector $p_t \\in \\mathbb{R}^3$ as the sensitivity of the final loss $L$ with respect to the state $x_t$:\n$$p_t = \\left(\\frac{\\partial x_T}{\\partial x_t}\\right)^\\top \\nabla_{x_T} f(x_T).$$\nBy definition, the adjoint at the final time $T$ is the gradient of the loss with respect to the final state:\n$$p_T = \\left(\\frac{\\partial x_T}{\\partial x_T}\\right)^\\top \\nabla_{x_T} f(x_T) = I^\\top \\nabla_{x_T} f(x_T) = A x_T - b.$$\n\nWe can derive a backward recurrence for the adjoints. From the state update rule, we find the Jacobian of the state transition:\n$$\\frac{\\partial x_{t+1}}{\\partial x_t} = \\frac{\\partial}{\\partial x_t} \\left(x_t - \\eta_t(\\theta)(Ax_t - b + \\xi_t)\\right) = I - \\eta_t(\\theta) A.$$\nUsing the chain rule for the adjoints:\n$$p_t = \\left(\\frac{\\partial x_T}{\\partial x_t}\\right)^\\top p_T = \\left(\\frac{\\partial x_{t+1}}{\\partial x_t} \\cdot \\frac{\\partial x_T}{\\partial x_{t+1}}\\right)^\\top p_T = \\left(\\frac{\\partial x_{t+1}}{\\partial x_t}\\right)^\\top \\left(\\frac{\\partial x_T}{\\partial x_{t+1}}\\right)^\\top p_T = \\left(\\frac{\\partial x_{t+1}}{\\partial x_t}\\right)^\\top p_{t+1}.$$\nSubstituting the Jacobian and using the symmetry of matrix $A$ ($A=A^\\top$):\n$$p_t = (I - \\eta_t A)^\\top p_{t+1} = (I - \\eta_t A) p_{t+1} = p_{t+1} - \\eta_t A p_{t+1}.$$\nThis recurrence allows us to compute $p_t$ for $t = T-1, \\dots, 0$, starting from $p_T$.\n\nNow, we can compute the gradient of the loss with respect to the parameters $\\theta$. The total derivative of $L$ is a sum of contributions from each time step where $\\theta$ has a direct influence. In this problem, $\\theta$ influences each state update through the step size $\\eta_t$.\nThe gradient of $L$ with respect to a component $\\theta_j$ is given by\n$$\\frac{\\partial L}{\\partial \\theta_j} = \\sum_{t=0}^{T-1} \\left(\\frac{\\partial L}{\\partial x_{t+1}}\\right)^\\top \\frac{\\partial x_{t+1}}{\\partial \\theta_j},$$\nwhere $\\frac{\\partial L}{\\partial x_{t+1}}$ represents the partial derivative holding later states fixed, which is precisely our adjoint $p_{t+1}^\\top$. The term $\\frac{\\partial x_{t+1}}{\\partial \\theta_j}$ is the partial derivative of the update rule with respect to $\\theta_j$, holding $x_t$ fixed:\n$$\\frac{\\partial x_{t+1}}{\\partial \\theta_j} = \\frac{\\partial}{\\partial \\theta_j} \\left(x_t - \\eta_t(\\theta) g_t\\right) = -g_t \\frac{\\partial \\eta_t(\\theta)}{\\partial \\theta_j}.$$\nCombining these, we get the expression for the hypergradient components:\n$$\\frac{\\partial L}{\\partial \\theta_j} = -\\sum_{t=0}^{T-1} p_{t+1}^\\top \\left(-g_t \\frac{\\partial \\eta_t}{\\partial \\theta_j}\\right) = -\\sum_{t=0}^{T-1} (p_{t+1}^\\top g_t) \\frac{\\partial \\eta_t}{\\partial \\theta_j}.$$\n\nWe need the partial derivatives of the step-size function $\\eta_t(\\theta) = \\frac{\\exp(\\theta_0)}{1 + t \\exp(\\theta_1)}$:\n1.  For $\\theta_0$:\n    $$\\frac{\\partial \\eta_t}{\\partial \\theta_0} = \\frac{\\exp(\\theta_0)}{1 + t \\exp(\\theta_1)} = \\eta_t.$$\n2.  For $\\theta_1$:\n    $$\\frac{\\partial \\eta_t}{\\partial \\theta_1} = \\exp(\\theta_0) \\frac{-1}{(1 + t \\exp(\\theta_1))^2} (t \\exp(\\theta_1)) = -\\eta_t \\frac{t \\exp(\\theta_1)}{1 + t \\exp(\\theta_1)}.$$\n\nSubstituting these into the sum for each component of $\\nabla_\\theta L$:\n$$\\frac{\\partial L}{\\partial \\theta_0} = -\\sum_{t=0}^{T-1} (p_{t+1}^\\top g_t) \\eta_t.$$\n$$\\frac{\\partial L}{\\partial \\theta_1} = -\\sum_{t=0}^{T-1} (p_{t+1}^\\top g_t) \\left(-\\eta_t \\frac{t \\exp(\\theta_1)}{1 + t \\exp(\\theta_1)}\\right) = \\sum_{t=0}^{T-1} (p_{t+1}^\\top g_t) \\eta_t \\frac{t \\exp(\\theta_1)}{1 + t \\exp(\\theta_1)}.$$\n\nThe complete algorithm for computing the hypergradient $\\nabla_\\theta L = \\left(\\frac{\\partial L}{\\partial \\theta_0}, \\frac{\\partial L}{\\partial \\theta_1}\\right)$ is:\n\n1.  **Forward Pass**:\n    *   Initialize $x_0 = 0$.\n    *   For $t = 0, \\dots, T-1$:\n        *   Compute $\\eta_t = \\frac{\\exp(\\theta_0)}{1 + t \\exp(\\theta_1)}$.\n        *   Compute $g_t = Ax_t - b + \\xi_t$.\n        *   Update $x_{t+1} = x_t - \\eta_t g_t$.\n    *   Store the trajectories $\\{x_t\\}_{t=0}^T$ and $\\{g_t\\}_{t=0}^{T-1}$. (Alternatively, store only $\\{x_t\\}$ and recompute $g_t$ in the backward pass).\n\n2.  **Backward Pass**:\n    *   Initialize hypergradient components: $\\nabla_{\\theta_0} L = 0$, $\\nabla_{\\theta_1} L = 0$.\n    *   Initialize the adjoint state: $p_T = Ax_T - b$.\n    *   For $t = T-1, \\dots, 0$:\n        *   Retrieve $g_t$ from storage.\n        *   Compute the scalar term $\\alpha_t = p_{t+1}^\\top g_t$.\n        *   Compute $\\eta_t$ and its derivative-related term.\n        *   Accumulate gradient for $\\theta_0$: $\\nabla_{\\theta_0} L = \\nabla_{\\theta_0} L - \\alpha_t \\eta_t$.\n        *   Accumulate gradient for $\\theta_1$: $\\nabla_{\\theta_1} L = \\nabla_{\\theta_1} L + \\alpha_t \\eta_t \\frac{t \\exp(\\theta_1)}{1 + t \\exp(\\theta_1)}$.\n        *   Propagate the adjoint backward: $p_t = p_{t+1} - \\eta_t A p_{t+1}$.\n\n### Part 2: Computational Overhead Justification\n\nThe computational cost is measured in units of dense matrix-vector multiplications involving the matrix $A$.\n\n**Cost to compute $f(x_T(\\theta))$ (Denominator):** This involves the full forward simulation to obtain $x_T$ and the final evaluation of the objective function.\n1.  **Forward Simulation**: The loop runs $T$ times (for $t=0, \\dots, T-1$). In each iteration, we compute $g_t = Ax_t - b + \\xi_t$, which requires one matrix-vector multiplication ($Ax_t$). Total for simulation: $T$ multiplications.\n2.  **Final Objective Evaluation**: The objective is $f(x_T) = \\frac{1}{2} x_T^\\top A x_T - b^\\top x_T$. This requires computing the vector $A x_T$. Total for evaluation: $1$ multiplication.\nThe total cost for the forward evaluation is therefore $T + 1$ matrix-vector multiplications.\n\n**Cost to compute $\\nabla_\\theta f(x_T(\\theta))$ (Numerator):** This involves the forward pass, initialization of the backward pass, and the backward pass itself.\n1.  **Forward Pass**: Same as above, requires storing intermediate states. Cost: $T$ multiplications.\n2.  **Backward Pass Initialization**: We must compute $p_T = Ax_T - b$. This requires one matrix-vector multiplication ($Ax_T$). Cost: $1$ multiplication.\n3.  **Backward Pass Propagation**: The loop runs $T$ times (for $t=T-1, \\dots, 0$). In each iteration, we compute $p_t = p_{t+1} - \\eta_t A p_{t+1}$, which requires one matrix-vector multiplication ($A p_{t+1}$). Total for propagation: $T$ multiplications.\nThe total cost for the hypergradient computation is $T$ (forward) $+ 1$ (init) $+ T$ (backward) $= 2T + 1$ matrix-vector multiplications.\n\n**Overhead Ratio:** The ratio $R$ of the hypergradient cost to the forward evaluation cost is:\n$$R = \\frac{2T+1}{T+1}.$$\nAs $T \\to \\infty$, this ratio approaches $2$, indicating that the gradient computation is only about twice as expensive as the function evaluation, a key advantage of reverse-mode differentiation.\nFor the specific cases:\n-   Case 1 ($T=1$): $R = (2(1)+1)/(1+1) = 3/2 = 1.5$.\n-   Case 2 ($T=50$): $R = (2(50)+1)/(50+1) = 101/51 \\approx 1.9804$.\n-   Case 3 ($T=200$): $R = (2(200)+1)/(200+1) = 401/201 \\approx 1.9950$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for hypergradient computation.\n    \"\"\"\n    # Fixed data from the problem statement\n    A = np.array([\n        [3.0, 1.0, 0.0],\n        [1.0, 2.0, 0.0],\n        [0.0, 0.0, 1.5]\n    ])\n    b = np.array([1.0, -2.0, 0.5])\n    \n    # Test suite from the problem statement\n    test_cases = [\n        # (T, theta, sigma, seed)\n        (1, np.array([-1.5, -10.0]), 0.0, 0),\n        (50, np.array([-1.0, 0.5]), 0.1, 1),\n        (200, np.array([-2.0, -0.2]), 0.05, 2),\n    ]\n\n    results = []\n\n    for T, theta, sigma, seed in test_cases:\n        # Generate the fixed noise sequence for this case\n        np_rng = np.random.default_rng(seed)\n        xi_sequence = np_rng.normal(0, sigma, size=(T, 3))\n\n        # Compute hypergradient using the derived reverse-mode algorithm\n        grad_rm = compute_hypergradient_reverse_mode(T, theta, A, b, xi_sequence)\n\n        # Compute hypergradient using central finite differences for verification\n        grad_fd = compute_hypergradient_finite_diff(T, theta, A, b, xi_sequence)\n        \n        # Calculate the Euclidean norm of the difference\n        delta = np.linalg.norm(grad_rm - grad_fd)\n\n        # Calculate the computational overhead ratio\n        # Numerator (Hypergradient cost): T (forward) + 1 (p_T) + T (backward) = 2T + 1\n        # Denominator (Forward eval cost): T (simulation) + 1 (f(x_T)) = T + 1\n        ratio = (2 * T + 1) / (T + 1)\n\n        results.extend([delta, ratio])\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef step_size_schedule(t, theta):\n    \"\"\"Computes the step size eta_t(theta).\"\"\"\n    return np.exp(theta[0]) / (1.0 + t * np.exp(theta[1]))\n\ndef objective_function(x, A, b):\n    \"\"\"Computes the objective function f(x).\"\"\"\n    return 0.5 * x.T @ A @ x - b.T @ x\n\ndef simulate_sgd(T, theta, A, b, xi_sequence):\n    \"\"\"Performs the forward simulation of SGD.\"\"\"\n    dim = A.shape[0]\n    x_traj = np.zeros((T + 1, dim))\n    g_traj = np.zeros((T, dim))\n    \n    x = np.zeros(dim) # x_0 = 0\n    x_traj[0] = x\n    \n    for t in range(T):\n        eta_t = step_size_schedule(t, theta)\n        g_t = A @ x - b + xi_sequence[t]\n        \n        x = x - eta_t * g_t\n        \n        x_traj[t + 1] = x\n        g_traj[t] = g_t\n        \n    return x_traj, g_traj\n\ndef compute_hypergradient_reverse_mode(T, theta, A, b, xi_sequence):\n    \"\"\"Computes the hypergradient via the derived reverse-mode algorithm.\"\"\"\n    # 1. Forward Pass\n    x_traj, g_traj = simulate_sgd(T, theta, A, b, xi_sequence)\n    x_T = x_traj[-1]\n    \n    # 2. Backward Pass\n    grad_theta = np.zeros(2)\n    \n    # Initialize adjoint state\n    p = A @ x_T - b  # p_T\n    \n    for t in range(T - 1, -1, -1):\n        # Retrieve needed values\n        g_t = g_traj[t]\n        eta_t = step_size_schedule(t, theta)\n        \n        # Compute scalar term\n        alpha_t = p.T @ g_t\n        \n        # Accumulate gradients for theta_0 and theta_1\n        grad_theta[0] -= alpha_t * eta_t\n        if t > 0: # The term for theta_1 is zero when t=0\n            grad_theta[1] += alpha_t * eta_t * (t * np.exp(theta[1])) / (1.0 + t * np.exp(theta[1]))\n        \n        # Propagate adjoint\n        p = p - eta_t * (A.T @ p) # A.T = A since A is symmetric\n        \n    return grad_theta\n\ndef compute_hypergradient_finite_diff(T, theta, A, b, xi_sequence, h=1e-7):\n    \"\"\"Computes the hypergradient via central finite differences for verification.\"\"\"\n    grad_theta_fd = np.zeros(2)\n    \n    for i in range(2):\n        theta_plus = np.copy(theta)\n        theta_minus = np.copy(theta)\n        \n        theta_plus[i] += h\n        theta_minus[i] -= h\n        \n        # Run forward simulation with perturbed theta, using the *same* noise\n        x_traj_plus, _ = simulate_sgd(T, theta_plus, A, b, xi_sequence)\n        x_T_plus = x_traj_plus[-1]\n        \n        x_traj_minus, _ = simulate_sgd(T, theta_minus, A, b, xi_sequence)\n        x_T_minus = x_traj_minus[-1]\n\n        # Compute objective values\n        f_plus = objective_function(x_T_plus, A, b)\n        f_minus = objective_function(x_T_minus, A, b)\n        \n        # Central difference formula\n        grad_theta_fd[i] = (f_plus - f_minus) / (2 * h)\n        \n    return grad_theta_fd\n\nsolve()\n```", "id": "3185953"}]}