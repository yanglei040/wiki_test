{"hands_on_practices": [{"introduction": "Many powerful algorithms in sparse optimization rely on a key subroutine: projecting a point onto a convex set. This first practice focuses on a fundamental building block, the Euclidean projection onto the $\\ell_1$ ball, which is the set of all vectors whose components' absolute values sum to less than a given radius. By working through this problem, you will see how the Karush-Kuhn-Tucker (KKT) optimality conditions translate directly into an elegant and efficient computational algorithm, and you will implement a practical solver that is a cornerstone of many advanced methods. [@problem_id:3183719]", "problem": "You are given a vector $v \\in \\mathbb{R}^n$ and a radius $\\tau \\geq 0$. Consider the Euclidean projection of $v$ onto the $\\ell_1$ ball $\\{x \\in \\mathbb{R}^n : \\lVert x \\rVert_1 \\leq \\tau\\}$ defined as the unique solution $x^\\star$ of the convex optimization problem\n$$\n\\min_{x \\in \\mathbb{R}^n} \\ \\frac{1}{2}\\lVert x - v \\rVert_2^2 \\quad \\text{subject to} \\quad \\lVert x \\rVert_1 \\leq \\tau .\n$$\nYour task is to implement a program that computes this projection using a sorting-and-thresholding method, and that also takes advantage of a special-case linear-time method when applicable. The work must be grounded in fundamental definitions: norm definitions, Euclidean projection onto a closed convex set, and Karush–Kuhn–Tucker optimality conditions for convex problems.\n\nAlgorithmic requirements:\n- If $\\lVert v \\rVert_1 \\leq \\tau$, the projection is $v$ itself; this must be detected in $O(n)$ time with a single pass.\n- Otherwise, implement the standard sorting-and-thresholding approach that computes a threshold and applies componentwise shrinkage. Your implementation must use sorting to obtain the threshold and must run in $O(n \\log n)$ time in the general case.\n- Propose and implement a linear-time variant for a special case: when the absolute values of the components of $v$ are already in nonincreasing order, i.e., $\\lvert v_1 \\rvert \\geq \\lvert v_2 \\rvert \\geq \\cdots \\geq \\lvert v_n \\rvert$, you must avoid sorting and compute the projection in $O(n)$ time with a single scan using prefix sums to locate the shrinkage threshold.\n\nNumerical output specification:\n- For each test case, your program must output a pair consisting of:\n  1. The projected vector $x^\\star$ with each component rounded to $6$ decimal places.\n  2. An integer flag $f \\in \\{0,1\\}$ indicating whether the special-case linear-time branch was used ($f = 1$) or not ($f = 0$). If $\\lVert v \\rVert_1 \\leq \\tau$ and you return $v$ directly, set $f = 0$.\n- Your program must aggregate the results of all test cases into a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[ \\text{case1}, \\text{case2}, \\ldots ]$). Each case must appear as $[\\text{projected\\_list}, f]$.\n\nTest suite:\n- Use the following test cases, where each pair is $(v,\\tau)$:\n  1. $([3.0, -1.0, 2.0, -4.0], 5.0)$, a general case that requires shrinkage.\n  2. $([0.2, -0.1, 0.3], 0.7)$, a boundary case where $\\lVert v \\rVert_1 \\leq \\tau$ and the projection should be $v$.\n  3. $([-5.0, 4.0, -3.0], 0.0)$, an edge case where $\\tau = 0$.\n  4. $([4.0, -3.0, 2.0, -1.0, 0.5], 5.5)$, a special case where $\\lvert v_i \\rvert$ are already in nonincreasing order and the linear-time path should be used.\n\nFinal output format:\n- Your program must print exactly one line: a single list with four entries, one per test case, and each entry must be $[\\text{projected\\_list}, f]$. Each projected\\_list must contain real numbers rounded to $6$ decimal places. The integer flag must be $0$ or $1$. No additional text may be printed.", "solution": "The problem is to compute the Euclidean projection of a vector $v \\in \\mathbb{R}^n$ onto the $\\ell_1$ ball of radius $\\tau \\geq 0$. This is a standard problem in convex optimization and is formally stated as:\n$$\nx^\\star = \\arg\\min_{x \\in \\mathbb{R}^n} \\ \\frac{1}{2}\\lVert x - v \\rVert_2^2 \\quad \\text{subject to} \\quad \\lVert x \\rVert_1 \\leq \\tau .\n$$\nThe objective function $f(x) = \\frac{1}{2}\\lVert x - v \\rVert_2^2$ is strictly convex, and the constraint set $\\mathcal{C} = \\{x \\in \\mathbb{R}^n : \\lVert x \\rVert_1 \\leq \\tau\\}$ is a closed convex set. Therefore, a unique solution $x^\\star$ is guaranteed to exist.\n\nWe can solve this problem by analyzing the Karush-Kuhn-Tucker (KKT) conditions. The Lagrangian for this problem is:\n$$\n\\mathcal{L}(x, \\lambda) = \\frac{1}{2}\\lVert x - v \\rVert_2^2 + \\lambda(\\lVert x \\rVert_1 - \\tau),\n$$\nwhere $\\lambda \\geq 0$ is the Lagrange multiplier associated with the inequality constraint. The KKT conditions for an optimal solution $(x^\\star, \\lambda^\\star)$ are:\n1.  **Stationarity**: $\\nabla_x \\mathcal{L}(x^\\star, \\lambda^\\star) = 0$.\n2.  **Primal Feasibility**: $\\lVert x^\\star \\rVert_1 \\leq \\tau$.\n3.  **Dual Feasibility**: $\\lambda^\\star \\geq 0$.\n4.  **Complementary Slackness**: $\\lambda^\\star(\\lVert x^\\star \\rVert_1 - \\tau) = 0$.\n\nThe stationarity condition involves the subgradient of the $\\ell_1$ norm. The gradient of the Lagrangian with respect to $x$ is given by $\\nabla_x \\mathcal{L}(x, \\lambda) = x - v + \\lambda \\partial \\lVert x \\rVert_1$, where $\\partial \\lVert x \\rVert_1$ is the subgradient of the $\\ell_1$ norm. The stationarity condition $0 \\in x^\\star - v + \\lambda^\\star \\partial \\lVert x^\\star \\rVert_1$ can be analyzed component-wise:\n$$\nv_i - x_i^\\star \\in \\lambda^\\star \\partial \\lvert x_i^\\star \\rvert \\quad \\text{for } i=1, \\dots, n.\n$$\nThe subgradient of the absolute value function is $\\partial \\lvert z \\rvert = \\text{sign}(z)$ for $z \\neq 0$ and $\\partial \\lvert 0 \\rvert = [-1, 1]$. Analyzing the cases for $x_i^\\star$:\n-   If $x_i^\\star  0$, then $v_i - x_i^\\star = \\lambda^\\star$, so $x_i^\\star = v_i - \\lambda^\\star$. This implies $v_i  \\lambda^\\star$.\n-   If $x_i^\\star  0$, then $v_i - x_i^\\star = -\\lambda^\\star$, so $x_i^\\star = v_i + \\lambda^\\star$. This implies $v_i  -\\lambda^\\star$.\n-   If $x_i^\\star = 0$, then $v_i \\in [-\\lambda^\\star, \\lambda^\\star]$, so $\\lvert v_i \\rvert \\leq \\lambda^\\star$.\n\nThese three cases can be unified into a single expression, known as the soft-thresholding operator:\n$$\nx_i^\\star = \\text{sign}(v_i) \\max(0, \\lvert v_i \\rvert - \\lambda^\\star).\n$$\nThe problem is now reduced to finding the correct value of the Lagrange multiplier $\\lambda^\\star \\geq 0$. We use the complementary slackness condition to determine $\\lambda^\\star$.\n\n**Case 1: $v$ is inside the $\\ell_1$ ball**\nIf $\\lVert v \\rVert_1 \\leq \\tau$, we can test if $\\lambda^\\star = 0$ is a valid choice. Setting $\\lambda^\\star = 0$ in the soft-thresholding formula yields $x_i^\\star = \\text{sign}(v_i)\\max(0, \\lvert v_i \\rvert) = v_i$. Thus, $x^\\star = v$. This solution is primally feasible since $\\lVert x^\\star \\rVert_1 = \\lVert v \\rVert_1 \\leq \\tau$. The dual feasibility $\\lambda^\\star=0 \\geq 0$ is satisfied. The complementary slackness condition $\\lambda^\\star(\\lVert v \\rVert_1 - \\tau) = 0 \\cdot (\\lVert v \\rVert_1 - \\tau) = 0$ is also satisfied. Thus, if $\\lVert v \\rVert_1 \\leq \\tau$, the projection is $v$ itself. This check can be performed in $O(n)$ time.\n\n**Case 2: $v$ is outside the $\\ell_1$ ball**\nIf $\\lVert v \\rVert_1  \\tau$, then $x^\\star \\neq v$, which requires that $\\lambda^\\star  0$. By complementary slackness, the constraint must be active, i.e., $\\lVert x^\\star \\rVert_1 = \\tau$. Substituting the soft-thresholding expression, we get:\n$$\n\\sum_{i=1}^n \\lvert x_i^\\star \\rvert = \\sum_{i=1}^n \\max(0, \\lvert v_i \\rvert - \\lambda^\\star) = \\tau.\n$$\nWe must find the root $\\lambda^\\star  0$ of the equation $h(\\lambda) = \\sum_{i=1}^n \\max(0, \\lvert v_i \\rvert - \\lambda) = \\tau$. The function $h(\\lambda)$ is continuous, piecewise-linear, and monotonically decreasing in $\\lambda$. Its breakpoints are at the values $\\lvert v_i \\rvert$. This structure enables an efficient search for $\\lambda^\\star$.\n\n**General Algorithm ($O(n \\log n)$)**\nLet $u_i = \\lvert v_i \\rvert$. We sort these values in descending order: $u_{(1)} \\geq u_{(2)} \\geq \\cdots \\geq u_{(n)}$. Let's assume the correct threshold $\\lambda^\\star$ lies in the interval $(u_{(\\rho+1)}, u_{(\\rho)}]$ for some $\\rho \\in \\{1, \\ldots, n\\}$ (with $u_{(n+1)}:=0$). Then exactly $\\rho$ components of the projected vector $x^\\star$ will be non-zero. The equation becomes:\n$$\n\\sum_{i=1}^{\\rho} (u_{(i)} - \\lambda^\\star) = \\tau \\implies \\lambda^\\star = \\frac{1}{\\rho}\\left(\\sum_{i=1}^{\\rho} u_{(i)} - \\tau\\right).\n$$\nWe need to find the correct $\\rho$. The correct $\\rho$ is the largest index $j \\in \\{1, \\ldots, n\\}$ such that the resulting threshold is smaller than $u_{(j)}$, ensuring that $x_{(j)}^\\star$ is positive. The condition is $u_{(j)}  \\frac{1}{j}\\left(\\sum_{i=1}^{j} u_{(i)} - \\tau\\right)$, which simplifies to $j \\cdot u_{(j)}  \\sum_{i=1}^{j} u_{(i)} - \\tau$.\n\nThe algorithm is as follows:\n1.  Compute $u_i = \\lvert v_i \\rvert$ for $i=1, \\dots, n$.\n2.  If $\\sum_{i=1}^n u_i \\leq \\tau$, return $v$. Additionally, for the special case $\\tau = 0$, the only feasible point is $x=0$, so we return the zero vector.\n3.  Sort $u$ in descending order to get $u_{(1)}, \\ldots, u_{(n)}$. This takes $O(n \\log n)$ time.\n4.  Compute the cumulative sums $S_j = \\sum_{i=1}^j u_{(i)}$.\n5.  Find the largest index $\\rho \\in \\{1, \\ldots, n\\}$ such that $j \\cdot u_{(j)}  S_j - \\tau$. This can be found in $O(n)$ time after sorting. Since we assumed $\\lVert v \\rVert_1  \\tau  0$, such a $\\rho$ is guaranteed to exist.\n6.  Calculate the threshold $\\lambda^\\star = \\frac{1}{\\rho}(S_{\\rho} - \\tau)$.\n7.  Compute the projection $x^\\star$ using $x_i^\\star = \\text{sign}(v_i) \\max(0, \\lvert v_i \\rvert - \\lambda^\\star)$. This takes $O(n)$ time.\n\nThe dominant step is sorting, leading to an overall complexity of $O(n \\log n)$.\n\n**Special-Case Linear-Time Algorithm ($O(n)$)**\nIf the absolute values of the components of $v$ are already in nonincreasing order, i.e., $\\lvert v_1 \\rvert \\geq \\lvert v_2 \\rvert \\geq \\cdots \\geq \\lvert v_n \\rvert$, the sorting step (step 3) can be skipped. The vector $u$ is already $u_{(i)} = \\lvert v_i \\rvert$. We can find $\\rho$ in $O(n)$ time with a single pass by computing prefix sums and checking the condition at each index. The rest of the algorithm remains the same, yielding a total time complexity of $O(n)$. This special case is detected with an $O(n)$ scan of the input vector.\nThe flag $f=1$ is used to indicate when this linear-time path is taken; otherwise, $f=0$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef project_l1_ball(v, tau):\n    \"\"\"\n    Computes the Euclidean projection of a vector v onto the L1 ball of radius tau.\n    \n    The projection is the solution to the optimization problem:\n        min_x ||x - v||_2^2\n        s.t.  ||x||_1 = tau\n\n    Args:\n        v (list or np.ndarray): The vector to project.\n        tau (float): The radius of the L1 ball. Must be non-negative.\n\n    Returns:\n        tuple: A tuple containing:\n            - list: The projected vector, with components rounded to 6 decimal places.\n            - int: A flag (1 if the special linear-time case was used, 0 otherwise).\n    \"\"\"\n    v_np = np.asarray(v, dtype=float)\n    n = len(v_np)\n    flag = 0\n\n    # Handle the edge case where tau = 0. The projection must be the zero vector.\n    if tau == 0.0:\n        return [0.0] * n, 0\n\n    # Handle the trivial case where v is already in the L1 ball.\n    l1_norm_v = np.sum(np.abs(v_np))\n    if l1_norm_v = tau:\n        return [round(c, 6) for c in v_np], 0\n\n    # The projection algorithm requires operating on absolute values.\n    v_abs = np.abs(v_np)\n    \n    # Check if a special-case linear-time algorithm can be used.\n    # This is applicable if the absolute values of v's components are already sorted.\n    is_sorted_desc = np.all(v_abs[:-1] = v_abs[1:])\n    \n    theta = 0.0  # The shrinkage threshold\n\n    if is_sorted_desc:\n        flag = 1\n        # O(n) path: No sorting needed.\n        # Compute cumulative sums of the already sorted absolute values.\n        cssv = np.cumsum(v_abs)\n        # Find the number of non-zero elements in the solution, rho.\n        # This is the largest index j such that u_j  (cumsum(u)_j - tau) / j.\n        # Note: Using 1-based indexing for rho, so j = index + 1\n        j_vals = np.arange(1, n + 1)\n        # Since ||v||_1  tau  0, the set of indices satisfying the condition is non-empty.\n        rho_idx = np.where(v_abs  (cssv - tau) / j_vals)[0][-1]\n        rho = rho_idx + 1\n        theta = (cssv[rho_idx] - tau) / rho\n    else:\n        flag = 0\n        # O(n log n) path: General case requires sorting.\n        # Get indices that would sort v_abs in descending order.\n        desc_indices = np.argsort(v_abs)[::-1]\n        v_abs_sorted = v_abs[desc_indices]\n        \n        cssv = np.cumsum(v_abs_sorted)\n        j_vals = np.arange(1, n + 1)\n        rho_idx = np.where(v_abs_sorted  (cssv - tau) / j_vals)[0][-1]\n        rho = rho_idx + 1\n        theta = (cssv[rho_idx] - tau) / rho\n    \n    # Apply the soft-thresholding operator with the calculated threshold theta.\n    projection = np.sign(v_np) * np.maximum(v_abs - theta, 0)\n    \n    # Round components to 6 decimal places as specified.\n    rounded_projection = [round(c, 6) for c in projection]\n\n    return rounded_projection, flag\n\ndef solve():\n    \"\"\"\n    Defines and runs the test suite for the L1 ball projection problem.\n    \"\"\"\n    test_cases = [\n        # Case 1: General case requiring shrinkage.\n        ([3.0, -1.0, 2.0, -4.0], 5.0),\n        # Case 2: Trivial case, point is already in the ball.\n        ([0.2, -0.1, 0.3], 0.7),\n        # Case 3: Edge case with tau = 0.\n        ([-5.0, 4.0, -3.0], 0.0),\n        # Case 4: Special case where |v_i| are pre-sorted for linear-time solution.\n        ([4.0, -3.0, 2.0, -1.0, 0.5], 5.5),\n    ]\n\n    results = []\n    for v, tau in test_cases:\n        projection, flag = project_l1_ball(v, tau)\n        \n        # Format the projected list into a string without spaces\n        proj_str = '[' + ','.join(map(str, projection)) + ']'\n        # Format the full result for one case, e.g., \"[[1.0,2.0],0]\"\n        case_str = f\"[{proj_str},{flag}]\"\n        results.append(case_str)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3183719"}, {"introduction": "The Least Absolute Shrinkage and Selection Operator (LASSO) is a cornerstone of modern statistics and machine learning, prized for its ability to perform regression and feature selection simultaneously. This practice guides you through the implementation of the proximal gradient method, also known as the Iterative Shrinkage-Thresholding Algorithm (ISTA), which is one of the most common and intuitive algorithms for solving the LASSO problem. Beyond just finding a solution, you will use your implementation as an analytical tool to explore a critical, and sometimes surprising, property of sparse models: the sensitivity of the selected features to tiny perturbations in the input data. [@problem_id:3183732]", "problem": "Consider the sparsity-inducing least-squares regression problem known as the least absolute shrinkage and selection operator, where the decision variable is the coefficient vector $x \\in \\mathbb{R}^n$ and the data matrix-vector pair $(A, b)$ is given with $A \\in \\mathbb{R}^{m \\times n}$ and $b \\in \\mathbb{R}^m$. The objective is to minimize the convex function\n$$\nF(x) = \\frac{1}{2}\\|A x - b\\|_2^2 + \\lambda \\|x\\|_1,\n$$\nwhere $\\lambda  0$ is a regularization parameter and $\\|\\cdot\\|_1$ denotes the one norm. This model induces sparsity in the optimal solution $x^\\star$, and its support $\\mathrm{supp}(x^\\star) = \\{ i \\in \\{1,\\dots,n\\} : x^\\star_i \\neq 0 \\}$ can be highly sensitive to small perturbations in $A$, especially when $A$ is ill-conditioned.\n\nStarting from the fundamental definitions of convex optimization, the subgradient of the one norm, and the Karush-Kuhn-Tucker (KKT) conditions, derive an algorithm that computes an approximate minimizer $x^\\star$ by iterating a proximal gradient step with a properly chosen step size based on the Lipschitz constant of the gradient of the smooth part of $F(x)$. Then, quantify the sensitivity of the support under small perturbations of $A$ and relate it to the condition number and to changes in the KKT sign vector.\n\nDefinitions and requirements to use:\n- The gradient of the smooth term $f(x) = \\frac{1}{2}\\|A x - b\\|_2^2$ is $\\nabla f(x) = A^\\top (A x - b)$.\n- The subgradient of $\\|x\\|_1$ at $x$ is the set of vectors $s \\in \\mathbb{R}^n$ with $s_i = \\mathrm{sign}(x_i)$ if $x_i \\neq 0$ and $s_i \\in [-1,1]$ if $x_i = 0$.\n- The KKT optimality condition for the minimizer $x^\\star$ is $A^\\top (A x^\\star - b) + \\lambda s^\\star = 0$, where $s^\\star$ belongs to the subgradient of $\\|x^\\star\\|_1$. Equivalently, the KKT sign vector can be defined pointwise by $s^\\star = -\\frac{1}{\\lambda} A^\\top (A x^\\star - b)$, which must satisfy $|s^\\star_i| \\leq 1$ and equals $\\mathrm{sign}(x^\\star_i)$ when $x^\\star_i \\neq 0$.\n- The proximal operator for the one norm is the soft-thresholding map $S_\\alpha(u)_i = \\mathrm{sign}(u_i)\\max\\{|u_i| - \\alpha, 0\\}$.\n\nImplement a program that, for each test case defined below, performs the following steps:\n1. Compute an approximate minimizer $x^\\star$ of $F(x)$ using the iterative update $x^{k+1} = S_{\\lambda t}\\left(x^k - t A^\\top(A x^k - b)\\right)$, where $t \\in (0, 1/L]$ and $L$ is the Lipschitz constant of $\\nabla f(x)$ given by $L = \\|A\\|_2^2$, the square of the spectral norm (largest singular value) of $A$. Use the same procedure to compute $x^\\star$ for the perturbed matrix $A + \\Delta$, keeping $b$ and $\\lambda$ unchanged.\n2. Determine the support sets $\\mathrm{supp}(x^\\star)$ before and after perturbation using the threshold $10^{-6}$, that is, an index $i$ belongs to the support if $|x^\\star_i|  10^{-6}$.\n3. Compute the following quantitative metrics:\n   - The condition number of $A$, defined as $\\kappa(A) = \\sigma_{\\max}(A) / \\sigma_{\\min}(A)$, where $\\sigma_{\\max}(A)$ and $\\sigma_{\\min}(A)$ are the largest and the smallest strictly positive singular values of $A$. If no strictly positive singular value exists or if numerical underflow makes $\\sigma_{\\min}(A)$ effectively zero, set $\\kappa(A) = 10^{16}$ as a large proxy value. Compute the same for $A + \\Delta$, denoted $\\kappa(A+\\Delta)$.\n   - The Frobenius norm of the perturbation, $\\|\\Delta\\|_F$.\n   - A flip indicator, equal to $1$ if the support sets before and after perturbation differ and $0$ otherwise.\n   - The Hamming support difference, equal to the size of the symmetric difference of the supports before and after perturbation, i.e., the number of indices that are in exactly one of the two supports.\n   - The number of KKT sign changes, computed by comparing the signs of the KKT sign vectors $s^\\star = -\\frac{1}{\\lambda}A^\\top(Ax^\\star - b)$ and $s^\\star_\\Delta = -\\frac{1}{\\lambda}(A+\\Delta)^\\top((A+\\Delta)x^\\star_\\Delta - b)$, counting indices where the sign classification changes. Use the classification rule: sign is $+1$ if the component is greater than $10^{-8}$, $-1$ if it is less than $-10^{-8}$, and $0$ otherwise.\n\nTest suite:\n- Case 1 (adversarial near-collinearity that can flip support):\n  - $A = \\begin{bmatrix} 1  1 + 10^{-3}  0 \\\\ 0  0  1 \\\\ 0  0  0.5 \\end{bmatrix}$,\n  - $\\Delta = \\begin{bmatrix} 0  -2 \\cdot 10^{-3}  0 \\\\ 0  0  0 \\\\ 0  0  0 \\end{bmatrix}$,\n  - $b = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}$,\n  - $\\lambda = 0.05$.\n- Case 2 (well-conditioned baseline expected to be stable):\n  - $A = \\begin{bmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  1 \\end{bmatrix}$,\n  - $\\Delta = \\begin{bmatrix} 10^{-4}  -10^{-4}  0 \\\\ 0  10^{-4}  -10^{-4} \\\\ 0  0  10^{-4} \\end{bmatrix}$,\n  - $b = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$,\n  - $\\lambda = 0.05$.\n- Case 3 (extremely ill-conditioned near-degenerate columns):\n  - $A = \\begin{bmatrix} 1  1 + 10^{-6}  0 \\\\ 0  10^{-6}  0 \\\\ 0  0  1 \\end{bmatrix}$,\n  - $\\Delta = \\begin{bmatrix} 0  -2 \\cdot 10^{-6}  0 \\\\ 0  0  0 \\\\ 0  0  0 \\end{bmatrix}$,\n  - $b = \\begin{bmatrix} 1 \\\\ 10^{-6} \\\\ 0 \\end{bmatrix}$,\n  - $\\lambda = 0.02$.\n\nOutput specification:\n- For each case, output a list with six entries: $[\\kappa(A), \\kappa(A+\\Delta), \\|\\Delta\\|_F, \\text{flip}, \\text{hamming}, \\text{kkt\\_sign\\_changes}]$, where the first three are floating-point values and the last three are integers.\n- Your program should produce a single line of output containing the results for all cases as a comma-separated list enclosed in square brackets, where each case’s result is itself a list as above. For example, the format should look like $[[a_1,a_2,a_3,a_4,a_5,a_6],[b_1,\\dots,b_6],[c_1,\\dots,c_6]]$ with numeric entries. Round the floating-point values to six decimal places in the final output.", "solution": "The problem requires the implementation and analysis of a solution to the LASSO (Least Absolute Shrinkage and Selection Operator) optimization problem. The objective is to minimize the function $F(x) = f(x) + g(x)$, where $f(x) = \\frac{1}{2}\\|A x - b\\|_2^2$ is a smooth, convex, differentiable loss term, and $g(x) = \\lambda \\|x\\|_1$ is a convex, non-differentiable regularization term that promotes sparsity in the solution vector $x$.\n\nThe chosen algorithm to find an approximate minimizer $x^\\star$ is the Proximal Gradient Method, also known as the Iterative Shrinkage-Thresholding Algorithm (ISTA). This method is well-suited for composite objective functions of the form $f(x) + g(x)$. The core idea is to iteratively perform a gradient descent step on the smooth part $f(x)$ and then apply the proximal operator of the non-smooth part $g(x)$ to correct the step.\n\nThe iterative update rule for the Proximal Gradient Method is given by:\n$$\nx^{k+1} = \\mathrm{prox}_{t g}(x^k - t \\nabla f(x^k))\n$$\nwhere $t  0$ is the step size, $\\nabla f(x)$ is the gradient of the smooth term, and $\\mathrm{prox}_{t g}$ is the proximal operator of the function $t g(x)$.\n\nFirst, we identify the components for our specific problem:\n1.  The smooth term is $f(x) = \\frac{1}{2}\\|A x - b\\|_2^2$. Its gradient is correctly given as $\\nabla f(x) = A^\\top (A x - b)$.\n2.  The non-smooth term is $g(x) = \\lambda \\|x\\|_1$. The proximal operator of $u \\mapsto \\alpha \\|u\\|_1$ is the element-wise soft-thresholding operator, defined as $S_\\alpha(u)_i = \\mathrm{sign}(u_i)\\max\\{|u_i| - \\alpha, 0\\}$. Therefore, the proximal operator for our problem is $\\mathrm{prox}_{t g}(u) = S_{\\lambda t}(u)$.\n\nSubstituting these components into the general update rule, we obtain the specific iterative algorithm for LASSO:\n$$\nx^{k+1} = S_{\\lambda t}\\left(x^k - t A^\\top(A x^k - b)\\right)\n$$\nThis matches the update rule provided in the problem statement.\n\nThe convergence of this algorithm is guaranteed if the step size $t$ satisfies the condition $0  t \\leq 1/L$, where $L$ is the Lipschitz constant of the gradient $\\nabla f(x)$. The Lipschitz constant is the smallest scalar $L$ such that $\\|\\nabla f(x) - \\nabla f(y)\\|_2 \\leq L \\|x - y\\|_2$ for all $x, y$. We can derive $L$ as follows:\n$$\n\\|\\nabla f(x) - \\nabla f(y)\\|_2 = \\|A^\\top(A x - b) - A^\\top(A y - b)\\|_2 = \\|A^\\top A (x - y)\\|_2\n$$\nBy the definition of an induced matrix norm, we have:\n$$\n\\|A^\\top A (x - y)\\|_2 \\leq \\|A^\\top A\\|_2 \\|x - y\\|_2\n$$\nThus, the Lipschitz constant is $L = \\|A^\\top A\\|_2$. For any matrix $M$, $\\|M^\\top M\\|_2$ is the largest eigenvalue of $M^\\top M$, which is equal to the square of the largest singular value of $M$, $\\sigma_{\\max}(M)^2$. The spectral norm of $M$, $\\|M\\|_2$, is by definition $\\sigma_{\\max}(M)$. Therefore, $L = \\sigma_{\\max}(A)^2 = \\|A\\|_2^2$, confirming the value given in the problem statement. A safe and standard choice for the step size is $t = 1/L$.\n\nThe implementation proceeds by initializing $x^0$ (e.g., as a zero vector) and iterating the update rule until the change $\\|x^{k+1} - x^k\\|_2$ falls below a small tolerance, or a maximum number of iterations is reached. This procedure is performed for both the original matrix $A$ and the perturbed matrix $A+\\Delta$ to obtain the respective solutions, $x^\\star$ and $x^\\star_\\Delta$.\n\nOnce the solutions are computed, the following metrics are calculated to quantify the sensitivity of the solution's support to the perturbation $\\Delta$:\n1.  **Support Sets**: The supports of $x^\\star$ and $x^\\star_\\Delta$ are determined by identifying indices $i$ where the absolute value of the coefficient $|x_i^\\star|$ exceeds a threshold of $10^{-6}$.\n2.  **Condition Numbers**: $\\kappa(A)$ and $\\kappa(A+\\Delta)$ are computed as the ratio of the largest to the smallest strictly positive singular values. A large proxy value of $10^{16}$ is used if the matrix is singular or numerically close to singular. This metric indicates how close the matrix is to being rank-deficient, a key factor in the stability of the solution.\n3.  **Perturbation Norm**: The magnitude of the perturbation is measured by the Frobenius norm $\\|\\Delta\\|_F = \\sqrt{\\sum_{i,j} |\\Delta_{ij}|^2}$.\n4.  **Flip Indicator**: A binary value indicating whether the support sets of the original and perturbed solutions are identical. A value of $1$ signifies a change in the set of active predictors.\n5.  **Hamming Support Difference**: The size of the symmetric difference between the two support sets, which counts the number of features that are either added to or removed from the model due to the perturbation.\n6.  **KKT Sign Changes**: The Karush-Kuhn-Tucker (KKT) optimality conditions for LASSO state that at a minimizer $x^\\star$, there must exist a subgradient vector $s^\\star \\in \\partial \\|x^\\star\\|_1$ such that $\\nabla f(x^\\star) + \\lambda s^\\star = 0$. This implies $s^\\star = -\\frac{1}{\\lambda} \\nabla f(x^\\star) = -\\frac{1}{\\lambda} A^\\top (A x^\\star - b)$. The vector $s^\\star$ must satisfy $|s^\\star_i| \\le 1$ for all $i$, and $s^\\star_i = \\mathrm{sign}(x^\\star_i)$ for $i \\in \\mathrm{supp}(x^\\star)$. Changes in the sign pattern of this KKT vector between the original and perturbed problems indicate a fundamental shift in the optimality conditions, often preceding or coinciding with a support change. The signs are determined using a numerical tolerance of $10^{-8}$.\n\nThe program implements this entire pipeline, applying it to each test case to demonstrate the relationship between matrix conditioning, perturbations, and the stability of sparse solutions.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        {\n            \"A\": np.array([[1.0, 1.0 + 1e-3, 0.0], [0.0, 0.0, 1.0], [0.0, 0.0, 0.5]]),\n            \"Delta\": np.array([[0.0, -2e-3, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]),\n            \"b\": np.array([1.0, 0.0, 0.0]),\n            \"lambda_reg\": 0.05\n        },\n        {\n            \"A\": np.array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]]),\n            \"Delta\": np.array([[1e-4, -1e-4, 0.0], [0.0, 1e-4, -1e-4], [0.0, 0.0, 1e-4]]),\n            \"b\": np.array([1.0, 1.0, 1.0]),\n            \"lambda_reg\": 0.05\n        },\n        {\n            \"A\": np.array([[1.0, 1.0 + 1e-6, 0.0], [0.0, 1e-6, 0.0], [0.0, 0.0, 1.0]]),\n            \"Delta\": np.array([[0.0, -2e-6, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]),\n            \"b\": np.array([1.0, 1e-6, 0.0]),\n            \"lambda_reg\": 0.02\n        }\n    ]\n\n    def solve_lasso(A, b, lambda_reg, max_iter=20000, tol=1e-9):\n        \"\"\"\n        Solves the LASSO problem using the Proximal Gradient Method (ISTA).\n        \"\"\"\n        m, n = A.shape\n        x = np.zeros(n)\n\n        s = np.linalg.svd(A, compute_uv=False)\n        L = s[0]**2 if s.size  0 else 0.0\n\n        if L == 0:\n            return x # A is zero matrix, x=0 is a solution for b=0\n\n        t = 1.0 / L\n        alpha = lambda_reg * t\n\n        for _ in range(max_iter):\n            x_old = x.copy()\n            \n            grad = A.T @ (A @ x - b)\n            u = x - t * grad\n            \n            x = np.sign(u) * np.maximum(np.abs(u) - alpha, 0.0)\n            \n            if np.linalg.norm(x - x_old)  tol:\n                break\n                \n        return x\n\n    def process_case(A, Delta, b, lambda_reg):\n        \"\"\"\n        Computes all required metrics for a single test case.\n        \"\"\"\n        A_orig = A\n        A_pert = A + Delta\n\n        # 1. Compute solutions\n        x_star_orig = solve_lasso(A_orig, b, lambda_reg)\n        x_star_pert = solve_lasso(A_pert, b, lambda_reg)\n\n        # 2. Determine supports\n        support_threshold = 1e-6\n        support_orig = {i for i, val in enumerate(x_star_orig) if abs(val)  support_threshold}\n        support_pert = {i for i, val in enumerate(x_star_pert) if abs(val)  support_threshold}\n\n        # 3. Compute metrics\n        \n        # Condition numbers\n        def get_kappa(M):\n            s = np.linalg.svd(M, compute_uv=False)\n            s_pos = s[s  1e-12]\n            if len(s_pos) == 0:\n                return 1e16\n            kappa = s_pos[0] / s_pos[-1]\n            return kappa if np.isfinite(kappa) and kappa  1e16 else 1e16\n\n        kappa_A = get_kappa(A_orig)\n        kappa_A_delta = get_kappa(A_pert)\n\n        # Frobenius norm of perturbation\n        norm_delta_F = np.linalg.norm(Delta, 'fro')\n\n        # Flip indicator\n        flip = 1 if support_orig != support_pert else 0\n\n        # Hamming support difference\n        hamming = len(support_orig.symmetric_difference(support_pert))\n\n        # KKT sign changes\n        s_star_orig = -(1.0/lambda_reg) * A_orig.T @ (A_orig @ x_star_orig - b)\n        s_star_pert = -(1.0/lambda_reg) * A_pert.T @ (A_pert @ x_star_pert - b)\n\n        def get_sign_vector(v, tol=1e-8):\n            return np.where(v  tol, 1, np.where(v  -tol, -1, 0))\n\n        signs_orig = get_sign_vector(s_star_orig)\n        signs_pert = get_sign_vector(s_star_pert)\n\n        kkt_sign_changes = int(np.sum(signs_orig != signs_pert))\n        \n        return [kappa_A, kappa_A_delta, norm_delta_F, flip, hamming, kkt_sign_changes]\n\n    all_results = []\n    for case in test_cases:\n        result = process_case(case[\"A\"], case[\"Delta\"], case[\"b\"], case[\"lambda_reg\"])\n        all_results.append(result)\n    \n    # Format the final output string\n    output_parts = []\n    for res in all_results:\n        # Round float values to 6 decimal places\n        r_kappa_A = f\"{res[0]:.6f}\"\n        r_kappa_Ad = f\"{res[1]:.6f}\"\n        r_norm_d = f\"{res[2]:.6f}\"\n        \n        # Integers are formatted as is\n        flip = res[3]\n        hamming = res[4]\n        kkt_changes = res[5]\n        \n        case_str = f\"[{r_kappa_A},{r_kappa_Ad},{r_norm_d},{flip},{hamming},{kkt_changes}]\"\n        output_parts.append(case_str)\n        \n    final_output = f\"[{','.join(output_parts)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3183732"}, {"introduction": "For problems with complex constraints or objectives that can be split into simpler parts, the Alternating Direction Method of Multipliers (ADMM) is an exceptionally powerful and versatile tool. This exercise introduces a classic sparse recovery problem and shows how ADMM can solve it by decomposing it into a sequence of more manageable subproblems: a standard least-squares regression and a simple soft-thresholding step. Implementing this will not only give you hands-on experience with ADMM but also allow you to investigate how the method's key penalty parameter, $\\rho$, creates a fascinating trade-off between convergence speed and the sparsity of the final solution. [@problem_id:3183678]", "problem": "You are tasked with implementing a complete, runnable program that uses the Alternating Direction Method of Multipliers (ADMM) to solve the constrained convex optimization problem defined by minimizing the one-norm of a residual variable subject to a linear equality constraint. The problem is specified as: minimize $\\,\\|z\\|_{1}\\,$ with respect to $\\,x \\in \\mathbb{R}^{n}\\,$ and $\\,z \\in \\mathbb{R}^{m}\\,$ subject to the constraint $\\,Ax - z = b\\,$, where $\\,A \\in \\mathbb{R}^{m \\times n}\\,$ and $\\,b \\in \\mathbb{R}^{m}\\,$. Your program must implement a solver derived from first principles using the augmented Lagrangian and dual ascent framework, and must empirically explore how the penalty parameter $\\,\\rho\\,$ affects convergence behavior and the sparsity of the resulting $\\,z\\,$.\n\nBegin from the following fundamental base that is appropriate for optimization methods and sparsity-inducing optimization:\n- The problem is convex because the objective $\\,\\|z\\|_{1}\\,$ is convex and the constraint $\\,Ax - z = b\\,$ is affine.\n- The augmented Lagrangian defines a decomposition-based approach where a separable structure enables alternating optimization over blocks of variables.\n- The proximity operator of the one-norm encourages sparse solutions.\n\nDerive the ADMM (Alternating Direction Method of Multipliers) algorithm for this problem starting from the augmented Lagrangian and the definition of the proximity operator, without relying on any pre-specified update formulas. Design stopping criteria using primal and dual residuals derived from the optimality conditions and feasibility of the constraint. Quantify convergence by the number of iterations required to reach the stopping criteria, and quantify sparsity by counting the number of entries of $\\,z\\,$ whose magnitude exceeds a small threshold.\n\nImplementation requirements:\n- Use a scaled dual variable approach and design stopping tolerances using an absolute tolerance $\\,\\varepsilon_{\\mathrm{abs}} = 10^{-5}\\,$ and a relative tolerance $\\,\\varepsilon_{\\mathrm{rel}} = 10^{-4}\\,$. Use a maximum number of iterations $\\,2000\\,$.\n- For counting sparsity, define an entry of $\\,z\\,$ to be nonzero if its absolute value is strictly greater than $\\,10^{-6}\\,$.\n- Your solver must be deterministic and should use the specified random seeds and data generation described below.\n- No physical units or angle units apply in this problem.\n\nTest suite:\n- Use the following five test cases, each defined by a matrix $\\,A\\,$, a vector $\\,b\\,$, and a penalty parameter $\\,\\rho\\,$. All random quantities must be generated using the stated seeds and distributions to ensure reproducibility.\n1. Happy path, moderate dimensions and moderate penalty:\n   - Generate with seed $\\,42\\,$.\n   - Let $\\,m = 40\\,$ and $\\,n = 20\\,$.\n   - Draw $\\,A \\in \\mathbb{R}^{40 \\times 20}\\,$ with independent standard normal entries.\n   - Draw a sparse $\\,x_{\\mathrm{true}} \\in \\mathbb{R}^{20}\\,$ with exactly $\\,5\\,$ nonzero entries selected uniformly at random and values drawn from a standard normal distribution.\n   - Let $\\,b = A x_{\\mathrm{true}} + \\eta\\,$ where $\\,\\eta \\in \\mathbb{R}^{40}\\,$ has independent entries drawn from $\\,\\mathcal{N}(0,\\,0.01^{2})\\,$.\n   - Use $\\,\\rho = 0.5\\,$.\n2. Same data as Case $\\,1\\,$ but a larger penalty:\n   - Use the same $\\,A\\,$ and $\\,b\\,$ as Case $\\,1\\,$.\n   - Use $\\,\\rho = 5.0\\,$.\n3. Same data as Case $\\,1\\,$ but a smaller penalty:\n   - Use the same $\\,A\\,$ and $\\,b\\,$ as Case $\\,1\\,$.\n   - Use $\\,\\rho = 0.1\\,$.\n4. Poorly conditioned matrix:\n   - Generate with seed $\\,123\\,$.\n   - Let $\\,m = 30\\,$ and $\\,n = 15\\,$.\n   - Draw a base matrix $\\,A \\in \\mathbb{R}^{30 \\times 15}\\,$ with independent standard normal entries. Modify columns to induce near-collinearity: set column $\\,6\\,$ and column $\\,7\\,$ to be perturbations of column $\\,5\\,$ by adding $\\,0.001\\,$ times independent standard normal noise.\n   - Draw $\\,b \\in \\mathbb{R}^{30}\\,$ with independent standard normal entries.\n   - Use $\\,\\rho = 1.0\\,$.\n5. Zero target vector:\n   - Generate with seed $\\,7\\,$.\n   - Let $\\,m = 30\\,$ and $\\,n = 20\\,$.\n   - Draw $\\,A \\in \\mathbb{R}^{30 \\times 20}\\,$ with independent standard normal entries.\n   - Let $\\,b \\in \\mathbb{R}^{30}\\,$ be the zero vector.\n   - Use $\\,\\rho = 1.0\\,$.\n\nOutput specification:\n- For each test case, run the ADMM solver until convergence or until the maximum number of iterations is reached.\n- For each test case, compute and return a list of four values:\n  1. The number of iterations used (an integer).\n  2. The number of nonzero entries in $\\,z\\,$ using the threshold $\\,10^{-6}\\,$ (an integer).\n  3. The final primal residual norm $\\,\\|Ax - z - b\\|_{2}\\,$ (a float).\n  4. The final dual residual norm defined by the ADMM dual feasibility condition (a float).\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a list of the four quantities described above, for example $\\,[[\\text{case1}], [\\text{case2}], \\dots]\\,$. No other text should be printed.", "solution": "We address the problem of minimizing the one-norm of a residual variable $\\,z\\,$ under an affine constraint linking $\\,x\\,$ and $\\,z\\,$. The optimization problem is\n$$\n\\min_{x \\in \\mathbb{R}^{n},\\, z \\in \\mathbb{R}^{m}} \\ \\|z\\|_{1} \\quad \\text{subject to} \\quad Ax - z = b,\n$$\nwhere $\\,A \\in \\mathbb{R}^{m \\times n}\\,$ and $\\,b \\in \\mathbb{R}^{m}\\,$ are given. This problem is convex because the function $\\,\\|z\\|_{1}\\,$ is convex and the constraint set $\\,\\{(x,z) : Ax - z = b\\}\\,$ is affine.\n\nTo design an algorithm from first principles, we start from the augmented Lagrangian framework. The unscaled augmented Lagrangian introduces a dual variable $\\,u \\in \\mathbb{R}^{m}\\,$ associated with the equality constraints and a penalty parameter $\\,\\rho  0\\,$ to form\n$$\n\\mathcal{L}_{\\rho}(x,z,u) = \\|z\\|_{1} + u^{\\top}(Ax - z - b) + \\frac{\\rho}{2}\\,\\|Ax - z - b\\|_{2}^{2}.\n$$\nThis construction follows from the method of multipliers, where the quadratic penalty term enforces approximate feasibility while dual ascent in $\\,u\\,$ corrects for constraint violation. The Alternating Direction Method of Multipliers (ADMM) alternates minimization with respect to blocks of variables while performing dual ascent updates. It is beneficial to use the scaled dual variable $\\,y = \\frac{1}{\\rho}u\\,$, which yields cleaner subproblems. In scaled form, one can show that the primal subproblems separate into a quadratic problem in $\\,x\\,$ and a proximal problem in $\\,z\\,$, and the dual update becomes a simple residual accumulation.\n\nWe now derive each update step.\n\nFirst, the $\\,x\\,$-update minimizes a strictly convex quadratic objective over $\\,x\\,$ that depends on the residual $\\,Ax - z - b\\,$ and the current scaled dual variable $\\,y\\,$. The optimality condition with respect to $\\,x\\,$ is the normal equation corresponding to a linear least squares problem. Specifically, the $\\,x\\,$-subproblem is equivalent to a least squares fit\n$$\nx^{k+1} = \\arg\\min_{x} \\ \\|Ax - c^{k}\\|_{2}^{2},\n$$\nwhere\n$$\nc^{k} = z^{k} + b - y^{k}.\n$$\nThis follows by collecting the quadratic terms in $\\,Ax\\,$ while $\\,z^{k}\\,$ and $\\,y^{k}\\,$ are fixed. The solution is computed by a least squares solver, which is numerically stable even if $\\,A^{\\top}A\\,$ is ill-conditioned:\n$$\nx^{k+1} = \\operatorname*{argmin}_{x} \\|Ax - c^{k}\\|_{2}^{2}.\n$$\n\nSecond, the $\\,z\\,$-update minimizes the sum of the one-norm and a quadratic that penalizes deviation from the current affine combination involving $\\,Ax^{k+1}\\,$, $\\,b\\,$, and $\\,y^{k}\\,$. The $\\,z\\,$-subproblem has the form\n$$\nz^{k+1} = \\arg\\min_{z} \\ \\|z\\|_{1} + \\frac{\\rho}{2}\\,\\left\\|z - d^{k}\\right\\|_{2}^{2},\n$$\nwhere\n$$\nd^{k} = Ax^{k+1} - b + y^{k}.\n$$\nBy definition of the proximity operator, this is the proximal map of the one-norm evaluated at $\\,d^{k}\\,$ with parameter $\\,1/\\rho\\,$. The proximity operator of the one-norm is the soft-thresholding (also called the shrinkage) operator, which acts componentwise. Thus, the $\\,z\\,$-update is the soft-threshold of $\\,d^{k}\\,$ with threshold $\\,1/\\rho\\,$, which promotes sparsity in $\\,z\\,$ by setting small-magnitude components to zero:\n$$\nz^{k+1} = \\operatorname{soft}(d^{k},\\,1/\\rho),\n$$\nwhere, for each component $\\,i\\,$,\n$$\n\\operatorname{soft}(d_{i},\\,\\lambda) = \\begin{cases}\nd_{i} - \\lambda,  d_{i}  \\lambda, \\\\\n0,  |d_{i}| \\le \\lambda, \\\\\nd_{i} + \\lambda,  d_{i}  -\\lambda.\n\\end{cases}\n$$\n\nThird, the scaled dual variable update accumulates the residual of the equality constraint,\n$$\ny^{k+1} = y^{k} + \\left(Ax^{k+1} - z^{k+1} - b\\right).\n$$\nThis is a gradient ascent step in the dual variable for the Lagrangian saddle point condition, scaled by the penalty parameter due to our variable definition.\n\nConvergence monitoring uses the primal residual and the dual residual from ADMM theory. The primal residual measures feasibility of the constraint:\n$$\nr^{k+1} = Ax^{k+1} - z^{k+1} - b,\n$$\nwith norm $\\,\\|r^{k+1}\\|_{2}\\,$. The dual residual captures the stationarity in the $\\,x\\,$-block via the change in $\\,z\\,$ mapped through the constraint:\n$$\ns^{k+1} = \\rho A^{\\top}\\left(z^{k+1} - z^{k}\\right),\n$$\nwith norm $\\,\\|s^{k+1}\\|_{2}\\,$. Stopping thresholds combine absolute and relative tolerances to avoid oversensitivity near zero and scale appropriately with the iterate magnitudes. Let $\\,m\\,$ be the number of rows of $\\,A\\,$ and $\\,n\\,$ the number of columns. Define\n$$\n\\varepsilon_{\\mathrm{pri}} = \\sqrt{m}\\,\\varepsilon_{\\mathrm{abs}} + \\varepsilon_{\\mathrm{rel}} \\cdot \\max\\left\\{\\|Ax^{k+1}\\|_{2},\\,\\|z^{k+1}\\|_{2},\\,\\|b\\|_{2}\\right\\},\n$$\nand\n$$\n\\varepsilon_{\\mathrm{dual}} = \\sqrt{n}\\,\\varepsilon_{\\mathrm{abs}} + \\varepsilon_{\\mathrm{rel}} \\cdot \\|A^{\\top}y^{k+1}\\|_{2}.\n$$\nWe declare convergence if $\\,\\|r^{k+1}\\|_{2} \\le \\varepsilon_{\\mathrm{pri}}\\,$ and $\\,\\|s^{k+1}\\|_{2} \\le \\varepsilon_{\\mathrm{dual}}\\,$, or if the iteration count reaches the maximum allowed.\n\nEffect of the penalty parameter $\\,\\rho\\,$:\n- The $\\,z\\,$-update uses a threshold $\\,1/\\rho\\,$. Larger $\\,\\rho\\,$ implies a smaller threshold, which tends to yield less sparsity (more nonzero entries in $\\,z\\,$), while smaller $\\,\\rho\\,$ increases the threshold, encouraging more entries of $\\,z\\,$ to be exactly zero.\n- However, extremely small $\\,\\rho\\,$ can slow down reduction of the primal residual because the quadratic penalty on infeasibility becomes weak, while extremely large $\\,\\rho\\,$ can slow dual progress and lead to ill-conditioned subproblems. Thus, there is a trade-off: $\\,\\rho\\,$ controls both sparsity and convergence speed.\n\nImplementation details:\n- Initialize $\\,x^{0} = 0\\,$, $\\,z^{0} = 0\\,$, and $\\,y^{0} = 0\\,$.\n- Compute the $\\,x\\,$-update using a numerically stable least squares solver for $\\,Ax \\approx z + b - y\\,$ at each iteration.\n- Compute the $\\,z\\,$-update via the soft-thresholding operator applied to $\\,Ax - b + y\\,$ with threshold $\\,1/\\rho\\,$.\n- Update the scaled dual variable $\\,y\\,$ by adding the current primal residual.\n- After termination, count the number of nonzero entries in $\\,z\\,$ as those with absolute value strictly greater than $\\,10^{-6}\\,$. Report the number of iterations used, the sparsity count, and the final norms $\\,\\|r\\|_{2}\\,$ and $\\,\\|s\\|_{2}\\,$.\n\nThe test suite consists of five cases that collectively probe typical behavior (“happy path”), the influence of increasing and decreasing $\\,\\rho\\,$ on the same data, a poorly conditioned $\\,A\\,$ scenario challenging the least squares step, and a boundary case with $\\,b = 0\\,$ where the optimal solution can collapse to a trivial residual. The single-line program output must be a list of lists, each inner list containing an integer for iterations, an integer for the sparsity count, and two floats for the final residual norms, in the specified order.", "answer": "```python\n# Python 3.12\n# Numpy 1.23.5; SciPy is allowed but not used here.\nimport numpy as np\n\ndef soft_threshold(v, lam):\n    # Component-wise soft-thresholding: sign(v) * max(|v| - lam, 0)\n    return np.sign(v) * np.maximum(np.abs(v) - lam, 0.0)\n\ndef admm_solve(A, b, rho, max_iters=2000, eps_abs=1e-5, eps_rel=1e-4, nnz_thresh=1e-6):\n    \"\"\"\n    Solve min_{x,z} ||z||_1 s.t. A x - z = b using scaled ADMM.\n    Returns: (iters_used, z, x, final_primal_norm, final_dual_norm)\n    \"\"\"\n    m, n = A.shape\n    # Initialize variables\n    x = np.zeros(n)\n    z = np.zeros(m)\n    y = np.zeros(m)  # scaled dual variable\n\n    # Precompute norms used in tolerance calculation\n    b_norm = np.linalg.norm(b)\n\n    iters = 0\n    r_norm = None\n    s_norm = None\n\n    for k in range(1, max_iters + 1):\n        # x-update: least squares solve for Ax ≈ z + b - y\n        rhs = z + b - y\n        # Use lstsq for numerical stability; returns minimum-norm solution\n        x = np.linalg.lstsq(A, rhs, rcond=None)[0]\n\n        # z-update: soft-thresholding of (A x - b + y) with threshold 1/rho\n        Ax = A @ x\n        d = Ax - b + y\n        z_prev = z.copy()\n        z = soft_threshold(d, 1.0 / rho)\n\n        # y-update: scaled dual ascent\n        r = Ax - z - b\n        y = y + r\n\n        # Residual norms\n        r_norm = np.linalg.norm(r)\n        s = rho * (A.T @ (z - z_prev))\n        s_norm = np.linalg.norm(s)\n\n        # Tolerances\n        eps_pri = np.sqrt(m) * eps_abs + eps_rel * max(np.linalg.norm(Ax), np.linalg.norm(z), b_norm)\n        eps_dual = np.sqrt(n) * eps_abs + eps_rel * np.linalg.norm(A.T @ y)\n\n        iters = k\n        if r_norm = eps_pri and s_norm = eps_dual:\n            break\n\n    return iters, z, x, float(r_norm), float(s_norm)\n\ndef count_nnz(z, thresh=1e-6):\n    return int(np.sum(np.abs(z)  thresh))\n\ndef generate_case_1_2_3():\n    # Seed 42; m=40, n=20\n    rng = np.random.default_rng(42)\n    m, n = 40, 20\n    A = rng.standard_normal((m, n))\n    # Sparse x_true with exactly 5 nonzeros\n    idx = rng.choice(n, size=5, replace=False)\n    x_true = np.zeros(n)\n    x_true[idx] = rng.standard_normal(5)\n    noise = rng.normal(0.0, 0.01, size=m)\n    b = A @ x_true + noise\n    return A, b\n\ndef generate_case_4():\n    # Seed 123; m=30, n=15; induce near-collinearity\n    rng = np.random.default_rng(123)\n    m, n = 30, 15\n    A = rng.standard_normal((m, n))\n    # Make columns 5,6 similar to column 4 (0-based indexing)\n    col_base = A[:, 4].copy()\n    A[:, 5] = col_base + 0.001 * rng.standard_normal(m)\n    A[:, 6] = col_base + 0.001 * rng.standard_normal(m)\n    b = rng.standard_normal(m)\n    return A, b\n\ndef generate_case_5():\n    # Seed 7; m=30, n=20; b = 0\n    rng = np.random.default_rng(7)\n    m, n = 30, 20\n    A = rng.standard_normal((m, n))\n    b = np.zeros(m)\n    return A, b\n\ndef solve():\n    # Define the test cases from the problem statement.\n    A1, b1 = generate_case_1_2_3()\n    A4, b4 = generate_case_4()\n    A5, b5 = generate_case_5()\n\n    test_cases = [\n        (A1, b1, 0.5),  # Case 1\n        (A1, b1, 5.0),  # Case 2\n        (A1, b1, 0.1),  # Case 3\n        (A4, b4, 1.0),  # Case 4\n        (A5, b5, 1.0),  # Case 5\n    ]\n\n    results = []\n    for A, b, rho in test_cases:\n        iters, z, x, r_norm, s_norm = admm_solve(A, b, rho, max_iters=2000, eps_abs=1e-5, eps_rel=1e-4, nnz_thresh=1e-6)\n        nnz = count_nnz(z, thresh=1e-6)\n        # Prepare the result as [iters, nnz, r_norm, s_norm]\n        results.append([int(iters), int(nnz), float(r_norm), float(s_norm)])\n\n    # Format output exactly: a single line \"[[...],[...],...]\"\n    def format_inner(lst):\n        # ensure no spaces in the printed list for strict formatting\n        return \"[\" + \",\".join(f\"{v}\" for v in lst) + \"]\"\n\n    out = \"[\" + \",\".join(format_inner(r) for r in results) + \"]\"\n    print(out)\n\nsolve()\n```", "id": "3183678"}]}