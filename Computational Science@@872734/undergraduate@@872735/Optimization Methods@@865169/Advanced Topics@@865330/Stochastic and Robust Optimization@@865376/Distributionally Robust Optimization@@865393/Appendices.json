{"hands_on_practices": [{"introduction": "Distributionally Robust Optimization often involves optimizing against a \"worst-case\" probability distribution within a defined ambiguity set. This first exercise provides a direct and visual intuition for what this means within a Wasserstein ball, by asking you to calculate how an adversary would physically shift data points to maximize a given loss function [@problem_id:3121643]. This practice makes the abstract concept of an adversarial distribution tangible and helps build a foundational understanding of the mechanics of Wasserstein DRO.", "problem": "You are asked to implement a small, self-contained program that computes worst-case sample locations for a distributionally robust optimization problem defined over two-dimensional data under a Wasserstein ball. Consider the following setup. Let there be a finite set of points $\\{x_i\\}_{i=1}^n \\subset \\mathbb{R}^2$ with an empirical distribution that places mass $1/n$ on each $x_i$. Let the loss be a linear functional $f_w(x) = w^\\top x$ with a given parameter vector $w \\in \\mathbb{R}^2$. Consider an uncertainty set of distributions defined by a Wasserstein ball of order infinity with ground metric given by the Euclidean norm, namely the set of all distributions $Q$ such that $W_\\infty(Q, P_n) \\le \\epsilon$, where $P_n$ is the empirical distribution on $\\{x_i\\}$. In this setting, the worst-case expected loss is defined as the supremum of the expected loss over all distributions in the Wasserstein ball of radius $\\epsilon$, subject to the Wasserstein distance constraint. Using only fundamental definitions and convex analysis principles, derive the optimizer for the worst-case perturbed sample locations $\\{x_i^\\star\\}$ that maximize the average loss when each original sample $x_i$ is allowed to move within an Euclidean ball of radius $\\epsilon$ around $x_i$. Your program must compute these worst-case sample locations for the specified test suite and produce the exact required output format.\n\nFundamental base and definitions you must use:\n- The Wasserstein distance of order infinity $W_\\infty$ between two distributions on $\\mathbb{R}^2$ with Euclidean cost is defined using couplings and the essential supremum of the Euclidean distance; it implies that every mass point can be moved by at most $\\epsilon$.\n- The Euclidean norm is defined by $\\|v\\|_2 = \\sqrt{v_1^2 + v_2^2}$ for $v \\in \\mathbb{R}^2$.\n- For a linear functional $f_w(x) = w^\\top x$, the maximizer over a closed Euclidean ball is attained on the boundary in the direction of the gradient whenever the gradient is nonzero.\n- The Cauchy–Schwarz inequality: for any $a, b \\in \\mathbb{R}^2$, $a^\\top b \\le \\|a\\|_2 \\|b\\|_2$, with equality if $a$ and $b$ are positively colinear.\n\nYour tasks:\n1. From the fundamental base, derive the per-sample maximizer $x_i^\\star$ of $f_w(x)$ under the constraint $\\|x - x_i\\|_2 \\le \\epsilon$ for a given $w$, including the edge case when $\\|w\\|_2 = 0$.\n2. Implement a program that, given a set of points $\\{x_i\\}$, a vector $w$, and a radius $\\epsilon$, computes all worst-case perturbed sample locations $\\{x_i^\\star\\}$.\n3. Apply your implementation to the following test suite. For numerical stability in output, report coordinates rounded to six decimal places.\n\nTest suite (each case is of the form $(X, w, \\epsilon)$, where $X$ is a list of two-dimensional points):\n- Case 1: $X = \\{[1.0, 0.0], [0.0, 1.0]\\}$, $w = [2.0, 1.0]$, $\\epsilon = 0.0$.\n- Case 2: $X = \\{[1.0, 0.0], [0.0, 1.0]\\}$, $w = [2.0, 1.0]$, $\\epsilon = 0.5$.\n- Case 3: $X = \\{[1.0, 0.0], [0.0, 1.0]\\}$, $w = [2.0, 1.0]$, $\\epsilon = 1.0$.\n- Case 4: $X = \\{[-1.0, -1.0], [2.0, 0.0], [0.0, -2.0]\\}$, $w = [0.0, 0.0]$, $\\epsilon = 0.75$.\n- Case 5: $X = \\{[0.5, -0.5], [1.5, 2.0]\\}$, $w = [-3.0, 4.0]$, $\\epsilon = 0.25$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is a list of the computed worst-case locations for that case. Each location must be a two-element list $[x_1^\\star, x_2^\\star]$ with both entries rounded to six decimals. For example, the format should look like:\n  - For two cases with two points each: $[[[x_{11}^\\star, x_{12}^\\star], [x_{21}^\\star, x_{22}^\\star]], [[y_{11}^\\star, y_{12}^\\star], [y_{21}^\\star, y_{22}^\\star]]]$.\n- There are no physical units involved in this problem.", "solution": "The user wants to solve a worst-case optimization problem under distributional uncertainty.\n\n### **Problem Statement Validation**\n\n**Step 1: Extract Givens**\n- **Data**: A set of $n$ points $\\{x_i\\}_{i=1}^n \\subset \\mathbb{R}^2$.\n- **Empirical Distribution**: $P_n = \\frac{1}{n} \\sum_{i=1}^n \\delta_{x_i}$, where $\\delta_{x_i}$ is a Dirac mass at point $x_i$.\n- **Loss Function**: A linear functional $f_w(x) = w^\\top x$ for a given vector $w \\in \\mathbb{R}^2$.\n- **Uncertainty Set**: A Wasserstein ball of order infinity, defined as the set of all distributions $Q$ on $\\mathbb{R}^2$ such that the Wasserstein distance $W_\\infty(Q, P_n) \\le \\epsilon$. The ground metric is the Euclidean norm, $\\| \\cdot \\|_2$.\n- **Objective**: Find the worst-case sample locations $\\{x_i^\\star\\}_{i=1}^n$ that achieve the supremum of the expected loss over the uncertainty set. This is mathematically expressed as finding the locations that solve $\\sup_{Q: W_\\infty(Q, P_n) \\le \\epsilon} \\mathbb{E}_Q[w^\\top x]$.\n- **Fundamental Definitions**:\n    - The $W_\\infty$ distance constraint implies that each mass point $x_i$ can be moved to a new location $x'$ within a Euclidean ball of radius $\\epsilon$, i.e., $\\|x' - x_i\\|_2 \\le \\epsilon$.\n    - The Euclidean norm is $\\|v\\|_2 = \\sqrt{v_1^2 + v_2^2}$.\n    - The maximizer of a linear functional $w^\\top x$ over a closed Euclidean ball lies on the boundary in the direction of the gradient $w$, provided $w \\neq 0$.\n    - The Cauchy-Schwarz inequality: $a^\\top b \\le \\|a\\|_2 \\|b\\|_2$ for $a, b \\in \\mathbb{R}^2$.\n- **Tasks**:\n    1. Derive the per-sample maximizer $x_i^\\star$ for $f_w(x)$ under the constraint $\\|x - x_i\\|_2 \\le \\epsilon$, handling the case $\\|w\\|_2 = 0$.\n    2. Implement a program to compute $\\{x_i^\\star\\}$ for given $\\{x_i\\}$, $w$, and $\\epsilon$.\n    3. Run on a given test suite and format the output.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is evaluated against the validation criteria.\n- **Scientifically Grounded**: Yes. The problem is a standard formulation in distributionally robust optimization (DRO), based on established mathematical concepts like Wasserstein distance, convex optimization, and duality theory.\n- **Well-Posed**: Yes. The objective is to maximize a continuous (linear) function over a compact set (a collection of closed balls). By the extreme value theorem, a maximum exists. The derivation will show it is unique when $w \\ne 0$.\n- **Objective**: Yes. The problem is stated using precise, unambiguous mathematical terminology.\n- **Flaw Check**:\n    1.  **Scientific Unsoundness**: None. The formulation is a valid and widely studied problem in DRO.\n    2.  **Non-Formalizable/Irrelevant**: None. The problem is formally stated and is central to the topic of distributionally robust optimization.\n    3.  **Incomplete/Contradictory**: None. All necessary data ($X, w, \\epsilon$) for each test case is provided.\n    4.  **Unrealistic/Infeasible**: Not applicable. The problem is purely mathematical; no physical constraints are involved.\n    5.  **Ill-Posed**: None. As argued above, a well-defined solution exists.\n    6.  **Pseudo-Profound/Trivial**: None. The problem requires a correct derivation from first principles and a precise implementation, which constitutes a valid challenge.\n    7.  **Outside Scientific Verifiability**: None. The solution is mathematically derivable and computationally verifiable.\n\n**Step 3: Verdict and Action**\nThe problem is **valid**. Proceeding with the solution.\n\n### **Derivation of the Worst-Case Sample Locations**\n\nThe objective is to compute the worst-case expected loss:\n$$ \\sup_{Q: W_\\infty(Q, P_n) \\le \\epsilon} \\mathbb{E}_Q[w^\\top x] $$\nA fundamental result in distributionally robust optimization, often derived from Kantorovich duality, states that for a Wasserstein ball of order $\\infty$, the worst-case expectation of a function can be found by taking the supremum over the worst-case realizations of each sample independently. Given that the empirical distribution $P_n$ has mass $1/n$ at each point $x_i$, the problem decouples into $n$ independent subproblems:\n$$ \\sup_{Q: W_\\infty(Q, P_n) \\le \\epsilon} \\mathbb{E}_Q[w^\\top x] = \\frac{1}{n} \\sum_{i=1}^n \\sup_{x'} \\{ w^\\top x' \\mid \\|x' - x_i\\|_2 \\le \\epsilon \\} $$\nOur task is to find the optimizer $x_i^\\star$ for each of these $n$ identical maximization problems. Let us analyze a single subproblem for a generic sample $x_i$:\n$$ \\max_{x'} w^\\top x' \\quad \\text{subject to} \\quad \\|x' - x_i\\|_2 \\le \\epsilon $$\nLet the variable transformation be $x' = x_i + \\delta$, where $\\delta \\in \\mathbb{R}^2$. The constraint on $x'$ translates to a constraint on the perturbation $\\delta$:\n$$ \\|(x_i + \\delta) - x_i\\|_2 \\le \\epsilon \\implies \\|\\delta\\|_2 \\le \\epsilon $$\nThe objective function becomes:\n$$ w^\\top x' = w^\\top (x_i + \\delta) = w^\\top x_i + w^\\top \\delta $$\nSince $w^\\top x_i$ is a constant with respect to the optimization variable $\\delta$, maximizing $w^\\top x'$ is equivalent to maximizing $w^\\top \\delta$:\n$$ \\max_{\\delta} w^\\top \\delta \\quad \\text{subject to} \\quad \\|\\delta\\|_2 \\le \\epsilon $$\nWe can determine the optimal perturbation $\\delta^\\star$ using the Cauchy-Schwarz inequality, which states $a^\\top b \\le \\|a\\|_2 \\|b\\|_2$. Applying this, we get:\n$$ w^\\top \\delta \\le \\|w\\|_2 \\|\\delta\\|_2 $$\nGiven the constraint $\\|\\delta\\|_2 \\le \\epsilon$, we can further bound the expression:\n$$ w^\\top \\delta \\le \\|w\\|_2 \\epsilon $$\nEquality (and thus the maximum value) is achieved when $\\delta$ is positively colinear with $w$ and has the maximum possible magnitude, which is $\\epsilon$.\n\nWe now analyze two cases for the vector $w$:\n\n**Case 1: $w \\ne 0$**\nIf $w$ is not the zero vector, then $\\|w\\|_2 > 0$. The unique optimal perturbation $\\delta^\\star$ that maximizes $w^\\top \\delta$ is the vector of length $\\epsilon$ in the direction of $w$:\n$$ \\delta^\\star = \\epsilon \\frac{w}{\\|w\\|_2} $$\nThe worst-case sample location $x_i^\\star$ is then found by adding this optimal perturbation to the original sample location $x_i$:\n$$ x_i^\\star = x_i + \\delta^\\star = x_i + \\epsilon \\frac{w}{\\|w\\|_2} $$\nThis shows that to maximize a linear loss, each data point should be shifted in the direction of the loss vector's gradient, $w$, to the furthest extent allowed by the uncertainty set, which is a distance of $\\epsilon$. This holds for every sample $i=1, \\dots, n$.\n\n**Case 2: $w = 0$**\nIf $w$ is the zero vector, then $\\|w\\|_2 = 0$. The objective function becomes:\n$$ w^\\top \\delta = 0^\\top \\delta = 0 $$\nIn this case, the objective value is $0$ regardless of the choice of $\\delta$, as long as it satisfies the constraint $\\|\\delta\\|_2 \\le \\epsilon$. Any point in the ball of radius $\\epsilon$ around $x_i$ is an optimizer. A natural and standard choice in such situations is to select the solution with the minimum norm, which corresponds to making the smallest change. This leads to choosing $\\delta^\\star = 0$. Consequently, the worst-case location is the original location itself:\n$$ x_i^\\star = x_i $$\nNote that the formula from Case 1 is undefined if $\\|w\\|_2=0$, so this case must be handled separately.\n\nCombining both cases, the formula for the worst-case location $x_i^\\star$ is:\n$$ x_i^\\star = \\begin{cases} x_i + \\epsilon \\frac{w}{\\|w\\|_2} & \\text{if } \\|w\\|_2 > 0 \\\\ x_i & \\text{if } \\|w\\|_2 = 0 \\end{cases} $$\nThis formula will be implemented to solve the problem for the given test suite.", "answer": "```python\nimport numpy as np\n\ndef compute_worst_case_locations(X, w, epsilon):\n    \"\"\"\n    Computes the worst-case sample locations for a linear loss.\n\n    Args:\n        X (list of lists): The original sample locations, [[x1, y1], [x2, y2], ...].\n        w (list): The parameter vector [w1, w2] of the linear loss.\n        epsilon (float): The radius of the Wasserstein ball (and per-sample uncertainty sets).\n\n    Returns:\n        list of lists: The worst-case sample locations.\n    \"\"\"\n    X_np = np.array(X, dtype=float)\n    w_np = np.array(w, dtype=float)\n\n    # Handle the edge case where epsilon is zero. No perturbation is possible.\n    if epsilon == 0.0:\n        return X_np.tolist()\n\n    # Calculate the Euclidean norm of the vector w.\n    norm_w = np.linalg.norm(w_np)\n\n    # If the norm of w is zero (or numerically close to it), the loss is constant.\n    # The optimal perturbation is zero (no change to the points).\n    if norm_w < 1e-9:\n        return X_np.tolist()\n    \n    # Calculate the unit vector in the direction of w.\n    u_w = w_np / norm_w\n    \n    # The optimal perturbation is epsilon times the unit vector u_w.\n    # This vector is the same for all points.\n    delta_star = epsilon * u_w\n    \n    # Add the perturbation to all original sample locations.\n    # Numpy's broadcasting handles this efficiently.\n    X_star_np = X_np + delta_star\n    \n    return X_star_np.tolist()\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        ({\"X\": [[1.0, 0.0], [0.0, 1.0]], \"w\": [2.0, 1.0], \"epsilon\": 0.0}),\n        # Case 2\n        ({\"X\": [[1.0, 0.0], [0.0, 1.0]], \"w\": [2.0, 1.0], \"epsilon\": 0.5}),\n        # Case 3\n        ({\"X\": [[1.0, 0.0], [0.0, 1.0]], \"w\": [2.0, 1.0], \"epsilon\": 1.0}),\n        # Case 4\n        ({\"X\": [[-1.0, -1.0], [2.0, 0.0], [0.0, -2.0]], \"w\": [0.0, 0.0], \"epsilon\": 0.75}),\n        # Case 5\n        ({\"X\": [[0.5, -0.5], [1.5, 2.0]], \"w\": [-3.0, 4.0], \"epsilon\": 0.25}),\n    ]\n\n    # Store formatted string results for each test case\n    results_str_list = []\n\n    for case in test_cases:\n        worst_case_locs = compute_worst_case_locations(case[\"X\"], case[\"w\"], case[\"epsilon\"])\n        \n        # Format the result for a single case according to the required output format.\n        # Each point is formatted as [x,y] with 6 decimal places.\n        points_str = [f\"[{p[0]:.6f},{p[1]:.6f}]\" for p in worst_case_locs]\n        case_result_str = f\"[{','.join(points_str)}]\"\n        results_str_list.append(case_result_str)\n\n    # Join the results of all cases into a single string.\n    final_output = f\"[{','.join(results_str_list)}]\"\n    \n    # Final print statement must produce only the specified output format.\n    print(final_output)\n\n# Execute the solver.\nsolve()\n\n```", "id": "3121643"}, {"introduction": "Having seen how an adversary perturbs data, we now explore the practical benefit of optimizing against such perturbations: improved generalization. This exercise contrasts a standard data-driven approach, Sample Average Approximation (SAA), with a Wasserstein DRO model on a dataset containing outliers [@problem_id:3121634]. By implementing both and evaluating their performance on the true underlying data distribution, you will see how DRO can prevent overfitting and lead to more reliable decisions.", "problem": "You are given a toy instance to study the phenomenon of overfitting in Sample Average Approximation (SAA) and the generalization improvement obtained by Wasserstein Distributionally Robust Optimization (DRO). Consider a scalar decision variable $x \\in \\mathbb{R}$ and a scalar uncertain parameter $\\xi \\in \\mathbb{R}$. The loss is defined as $f(x,\\xi) = \\tfrac{1}{2} x^2 - x \\xi$, which is convex in $x$ for all $\\xi$. The true data-generating distribution for $\\xi$ is a mixture distribution: with probability $0.9$ draw $\\xi$ from a Normal distribution $\\mathcal{N}(0,1)$, and with probability $0.1$ set $\\xi$ equal to the constant $10$. The training dataset is constructed to contain $20$ samples: $16$ samples approximately from the $\\mathcal{N}(0,1)$ component and $4$ outliers at $10$. For reproducibility, use the fixed list\n$$\n\\Xi_{\\text{train}} = [-1.5, 1.5, -1.0, 1.0, -0.7, 0.7, -0.3, 0.3, -2.0, 2.0, -0.2, 0.2, -1.2, 1.2, -0.4, 0.4, 10, 10, 10, 10].\n$$\nAdopt the $1$-Wasserstein distance with ground metric $d(\\xi,\\xi') = |\\xi - \\xi'|$ to define the ambiguity set $\\mathcal{W}_{\\epsilon}$ as the ball of radius $\\epsilon \\ge 0$ centered at the empirical distribution $\\hat{P}_N$ built from the training data. Starting from fundamental definitions of expected risk and ambiguity sets, and without assuming pre-derived robust formulas, derive the SAA estimator $x_{\\text{SAA}}$ and the Wasserstein-DRO estimator $x_{\\text{DRO}}(\\epsilon)$ that minimize, respectively, the empirical expected loss and the worst-case expected loss over $\\mathcal{W}_{\\epsilon}$. Then, evaluate their out-of-sample risks under the true distribution. In this setting, the out-of-sample risk of a decision $x$ is the true expectation $\\mathbb{E}[f(x,\\xi)]$ taken with respect to the mixture distribution described above.\n\nYour program must:\n- Construct $\\Xi_{\\text{train}}$ exactly as provided.\n- Compute the empirical mean $\\bar{\\xi}$ over $\\Xi_{\\text{train}}$.\n- Compute the SAA decision $x_{\\text{SAA}}$ by minimizing $\\tfrac{1}{2} x^2 - x \\bar{\\xi}$.\n- Using the $1$-Wasserstein ball $\\mathcal{W}_{\\epsilon}$ and the ground metric $|\\cdot|$, derive the Wasserstein-DRO decision $x_{\\text{DRO}}(\\epsilon)$ for each given $\\epsilon$ based on principled reasoning, and implement it.\n- Evaluate the out-of-sample risk $\\mathbb{E}\\left[\\tfrac{1}{2} x^2 - x \\xi\\right]$ under the true mixture distribution for each decision $x_{\\text{DRO}}(\\epsilon)$.\n\nUse the following test suite of radius values:\n$$\n[0.0, 0.2, 1.0, 2.0, 2.5].\n$$\nThese cover a general \"happy path\" case ($0.2$), the baseline SAA case ($0.0$), a moderate robustification ($1.0$), the boundary threshold case where $\\epsilon$ equals $|\\bar{\\xi}|$ ($2.0$), and an extreme robustification ($2.5$). For each $\\epsilon$ in the list, output the corresponding out-of-sample risk of $x_{\\text{DRO}}(\\epsilon)$ evaluated under the true mixture distribution. The true distribution is specified completely by the mixture weights and component parameters; you must compute the exact mean of the mixture and use it consistently.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example $[\\text{result}_1,\\text{result}_2,\\dots]$. All answers must be real numbers (floats) without any physical units. Smaller values indicate better generalization.", "solution": "The problem requires the derivation and comparison of the Sample Average Approximation (SAA) estimator and the Wasserstein Distributionally Robust Optimization (DRO) estimator for a specific quadratic loss function. The performance of these estimators is to be evaluated based on their out-of-sample risk, calculated with respect to a true, known data-generating distribution.\n\nFirst, we define the core components of the problem. The decision variable is a scalar $x \\in \\mathbb{R}$, and the uncertain parameter is a scalar $\\xi \\in \\mathbb{R}$. The loss function is given by $f(x, \\xi) = \\frac{1}{2} x^2 - x \\xi$. The training data consists of $N=20$ samples provided as a fixed list $\\Xi_{\\text{train}}$. The empirical distribution $\\hat{P}_N$ is the discrete uniform distribution over these $N$ samples.\n\nThe SAA estimator, denoted $x_{\\text{SAA}}$, is the solution to the empirical risk minimization problem:\n$$\nx_{\\text{SAA}} = \\arg\\min_{x \\in \\mathbb{R}} \\mathbb{E}_{\\hat{P}_N}[f(x, \\xi)]\n$$\nThe empirical risk is calculated as the average loss over the training samples:\n$$\n\\mathbb{E}_{\\hat{P}_N}[f(x, \\xi)] = \\frac{1}{N} \\sum_{i=1}^{N} f(x, \\xi_i) = \\frac{1}{N} \\sum_{i=1}^{N} \\left(\\frac{1}{2} x^2 - x \\xi_i\\right) = \\frac{1}{2} x^2 - x \\left(\\frac{1}{N} \\sum_{i=1}^{N} \\xi_i\\right)\n$$\nLet $\\bar{\\xi} = \\frac{1}{N} \\sum_{i=1}^{N} \\xi_i$ be the empirical mean of the training samples. The SAA objective simplifies to $\\frac{1}{2} x^2 - x \\bar{\\xi}$. This is a strictly convex quadratic function of $x$. To find the minimizer, we set the first derivative with respect to $x$ to zero:\n$$\n\\frac{d}{dx} \\left(\\frac{1}{2} x^2 - x \\bar{\\xi}\\right) = x - \\bar{\\xi} = 0\n$$\nThis yields the SAA estimator:\n$$\nx_{\\text{SAA}} = \\bar{\\xi}\n$$\n\nNext, we address the Wasserstein-DRO estimator, $x_{\\text{DRO}}(\\epsilon)$. It is the solution to the minimax problem:\n$$\nx_{\\text{DRO}}(\\epsilon) = \\arg\\min_{x \\in \\mathbb{R}} \\sup_{P \\in \\mathcal{W}_{\\epsilon}} \\mathbb{E}_{P}[f(x, \\xi)]\n$$\nHere, $\\mathcal{W}_{\\epsilon}$ is the $1$-Wasserstein ball of radius $\\epsilon \\geq 0$ around the empirical distribution $\\hat{P}_N$, defined as $\\mathcal{W}_{\\epsilon} = \\{P \\mid W_1(P, \\hat{P}_N) \\le \\epsilon \\}$, with the ground metric being the absolute difference $d(\\xi, \\xi') = |\\xi - \\xi'|$.\n\nTo solve the minimax problem, we first analyze the inner maximization problem for a fixed $x$:\n$$\n\\sup_{P \\in \\mathcal{W}_{\\epsilon}} \\mathbb{E}_{P}[f(x, \\xi)] = \\sup_{P \\in \\mathcal{W}_{\\epsilon}} \\mathbb{E}_{P}\\left[\\frac{1}{2} x^2 - x \\xi\\right] = \\frac{1}{2} x^2 + \\sup_{P \\in \\mathcal{W}_{\\epsilon}} \\mathbb{E}_{P}[-x \\xi]\n$$\nThe Kantorovich-Rubinstein duality theorem states that the $1$-Wasserstein distance can be expressed as:\n$$\nW_1(P, \\hat{P}_N) = \\sup_{\\phi: \\|\\phi\\|_{\\text{Lip}} \\le 1} \\left( \\mathbb{E}_P[\\phi(\\xi)] - \\mathbb{E}_{\\hat{P}_N}[\\phi(\\xi)] \\right)\n$$\nwhere the supremum is over all $1$-Lipschitz functions $\\phi$. From this, for any function $g(\\xi)$, we have the inequality $\\mathbb{E}_P[g(\\xi)] - \\mathbb{E}_{\\hat{P}_N}[g(\\xi)] \\le W_1(P, \\hat{P}_N) \\cdot L(g)$, where $L(g)$ is the Lipschitz constant of $g$. For any $P \\in \\mathcal{W}_{\\epsilon}$, this implies $\\mathbb{E}_P[g(\\xi)] \\le \\mathbb{E}_{\\hat{P}_N}[g(\\xi)] + \\epsilon L(g)$. This upper bound is known to be tight.\nIn our case, the function of interest is $g(\\xi) = -x\\xi$. Its Lipschitz constant is $L(g) = \\sup_{\\xi_1\\neq\\xi_2}\\frac{|-x\\xi_1 - (-x\\xi_2)|}{|\\xi_1-\\xi_2|} = |-x| = |x|$.\nTherefore, the worst-case expectation is:\n$$\n\\sup_{P \\in \\mathcal{W}_{\\epsilon}} \\mathbb{E}_{P}[-x \\xi] = \\mathbb{E}_{\\hat{P}_N}[-x \\xi] + \\epsilon |x| = -x \\bar{\\xi} + \\epsilon |x|\n$$\nSubstituting this back into the DRO objective function, we need to solve:\n$$\nx_{\\text{DRO}}(\\epsilon) = \\arg\\min_{x \\in \\mathbb{R}} \\left( \\frac{1}{2} x^2 - x \\bar{\\xi} + \\epsilon |x| \\right)\n$$\nThe objective function $J(x) = \\frac{1}{2} x^2 - x \\bar{\\xi} + \\epsilon |x|$ is convex but non-differentiable at $x=0$. We can use subgradient calculus. The minimizer $x^*$ must satisfy $0 \\in \\partial J(x^*)$. The subgradient of $|x|$ is $\\text{sgn}(x)$ for $x \\neq 0$ and $[-1, 1]$ for $x=0$.\nThe subgradient of $J(x)$ is $\\partial J(x) = x - \\bar{\\xi} + \\epsilon \\cdot \\partial |x|$.\nIf $x > 0$, we need $x - \\bar{\\xi} + \\epsilon = 0 \\implies x = \\bar{\\xi} - \\epsilon$. This is valid only if $\\bar{\\xi} - \\epsilon > 0$, i.e., $\\bar{\\xi} > \\epsilon$.\nIf $x  0$, we need $x - \\bar{\\xi} - \\epsilon = 0 \\implies x = \\bar{\\xi} + \\epsilon$. This is valid only if $\\bar{\\xi} + \\epsilon  0$, i.e., $\\bar{\\xi}  -\\epsilon$.\nIf $x=0$, we need $0 \\in 0 - \\bar{\\xi} + \\epsilon[-1, 1] = [-\\bar{\\xi}-\\epsilon, -\\bar{\\xi}+\\epsilon]$. This is true if $-\\bar{\\xi}-\\epsilon \\le 0 \\le -\\bar{\\xi}+\\epsilon$, which simplifies to $|\\bar{\\xi}| \\le \\epsilon$.\n\nCombining these conditions, the solution is the soft-thresholding operator applied to $\\bar{\\xi}$ with threshold $\\epsilon$:\n$$\nx_{\\text{DRO}}(\\epsilon) = \\text{sign}(\\bar{\\xi}) \\max(0, |\\bar{\\xi}| - \\epsilon)\n$$\nNote that for $\\epsilon=0$, we have $x_{\\text{DRO}}(0) = \\text{sign}(\\bar{\\xi})\\max(0, |\\bar{\\xi}|) = \\bar{\\xi}$, which correctly recovers the SAA estimator $x_{\\text{SAA}}$.\n\nFinally, we compute the out-of-sample risk $R(x)$ for a given decision $x$. This is the expectation of the loss function under the true data-generating distribution $P_{\\text{true}}$:\n$$\nR(x) = \\mathbb{E}_{P_{\\text{true}}}[f(x, \\xi)] = \\mathbb{E}_{P_{\\text{true}}}\\left[\\frac{1}{2} x^2 - x \\xi\\right] = \\frac{1}{2} x^2 - x \\mathbb{E}_{P_{\\text{true}}}[\\xi]\n$$\nThe true distribution of $\\xi$ is a mixture: with probability $0.9$, $\\xi \\sim \\mathcal{N}(0,1)$, and with probability $0.1$, $\\xi=10$. The true mean $\\mu_{\\text{true}} = \\mathbb{E}_{P_{\\text{true}}}[\\xi]$ is:\n$$\n\\mu_{\\text{true}} = 0.9 \\cdot \\mathbb{E}[\\mathcal{N}(0,1)] + 0.1 \\cdot 10 = 0.9 \\cdot 0 + 0.1 \\cdot 10 = 1.0\n$$\nThus, the out-of-sample risk is $R(x) = \\frac{1}{2} x^2 - x$.\n\nWe now perform the numerical calculations.\nThe training data is $\\Xi_{\\text{train}} = [-1.5, 1.5, -1.0, 1.0, -0.7, 0.7, -0.3, 0.3, -2.0, 2.0, -0.2, 0.2, -1.2, 1.2, -0.4, 0.4, 10, 10, 10, 10]$.\nThe sum of the first $16$ symmetric samples is $0$. The sum of the four outliers is $4 \\times 10 = 40$. The total number of samples is $N=20$.\nThe empirical mean is $\\bar{\\xi} = \\frac{0 + 40}{20} = 2.0$.\n\nWe evaluate $x_{\\text{DRO}}(\\epsilon) = \\text{soft}(2.0, \\epsilon)$ and its risk $R(x_{\\text{DRO}}(\\epsilon))$ for each given $\\epsilon$:\n1.  $\\epsilon=0.0$: $x_{\\text{DRO}}(0.0) = \\text{soft}(2.0, 0.0) = 2.0$. Risk $R(2.0) = \\frac{1}{2}(2.0)^2 - 2.0 = 2.0 - 2.0 = 0.0$.\n2.  $\\epsilon=0.2$: $x_{\\text{DRO}}(0.2) = \\text{soft}(2.0, 0.2) = 1.8$. Risk $R(1.8) = \\frac{1}{2}(1.8)^2 - 1.8 = 1.62 - 1.8 = -0.18$.\n3.  $\\epsilon=1.0$: $x_{\\text{DRO}}(1.0) = \\text{soft}(2.0, 1.0) = 1.0$. Risk $R(1.0) = \\frac{1}{2}(1.0)^2 - 1.0 = 0.5 - 1.0 = -0.5$.\n4.  $\\epsilon=2.0$: $x_{\\text{DRO}}(2.0) = \\text{soft}(2.0, 2.0) = 0.0$. Risk $R(0.0) = \\frac{1}{2}(0.0)^2 - 0.0 = 0.0$.\n5.  $\\epsilon=2.5$: $x_{\\text{DRO}}(2.5) = \\text{soft}(2.0, 2.5) = 0.0$. Risk $R(0.0) = \\frac{1}{2}(0.0)^2 - 0.0 = 0.0$.\n\nThe SAA solution $x_{\\text{SAA}}=2.0$ overfits to the outliers in the training data, as it is much larger than the true optimal decision $x^* = \\arg\\min_x R(x) = \\arg\\min_x(\\frac{1}{2}x^2-x) = 1.0$. The DRO solution with $\\epsilon=1.0$ corrects the empirical mean perfectly, yielding the optimal out-of-sample performance. Smaller $\\epsilon$ values provide insufficient correction, while larger $\\epsilon$ values over-regularize and shrink the solution too much, degrading performance.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes SAA and Wasserstein-DRO solutions and their out-of-sample risks.\n    \"\"\"\n    \n    # Define the training dataset as specified in the problem statement.\n    Xi_train = np.array([\n        -1.5, 1.5, -1.0, 1.0, -0.7, 0.7, -0.3, 0.3, -2.0, 2.0, \n        -0.2, 0.2, -1.2, 1.2, -0.4, 0.4, 10.0, 10.0, 10.0, 10.0\n    ])\n\n    # Define the test suite of Wasserstein radii.\n    epsilon_values = [0.0, 0.2, 1.0, 2.0, 2.5]\n\n    # Compute the empirical mean of the training data.\n    # This corresponds to the SAA decision x_SAA.\n    xi_bar = np.mean(Xi_train)\n\n    # The true mean of the data-generating distribution for xi.\n    # P(xi) = 0.9 * N(0, 1) + 0.1 * delta_10\n    # E[xi] = 0.9 * E[N(0,1)] + 0.1 * E[delta_10] = 0.9 * 0 + 0.1 * 10 = 1.0\n    mu_true = 1.0\n\n    def soft_threshold(y, T):\n        \"\"\"\n        The soft-thresholding operator, which gives the Wasserstein-DRO solution.\n        soft(y, T) = sign(y) * max(0, |y| - T)\n        \"\"\"\n        if T  0:\n            raise ValueError(\"Threshold T must be non-negative.\")\n        return np.sign(y) * np.maximum(0, np.abs(y) - T)\n\n    def out_of_sample_risk(x):\n        \"\"\"\n        Computes the out-of-sample risk R(x) = E_true[0.5*x^2 - x*xi].\n        R(x) = 0.5*x^2 - x * E_true[xi]\n        \"\"\"\n        return 0.5 * x**2 - x * mu_true\n\n    results = []\n    for epsilon in epsilon_values:\n        # Compute the Wasserstein-DRO decision x_DRO(epsilon).\n        # For epsilon = 0, this is the SAA decision x_SAA.\n        x_dro = soft_threshold(xi_bar, epsilon)\n        \n        # Evaluate the out-of-sample risk for this decision.\n        risk = out_of_sample_risk(x_dro)\n        results.append(risk)\n\n    # Format the output as a comma-separated list in brackets.\n    # The problem specifies that smaller values indicate better generalization.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3121634"}, {"introduction": "The final step is to learn how to solve DRO problems algorithmically to find the optimal robust decision. This advanced practice guides you through the process of deriving a subgradient for a Wasserstein-robust objective function and implementing a projected subgradient method, a cornerstone of modern convex optimization [@problem_id:3121614]. This exercise bridges the gap between the theoretical formulation of DRO and its practical numerical implementation.", "problem": "You are given a distributionally robust optimization problem in which the ambiguity set is a Wasserstein-$1$ ball around an empirical distribution. Consider decision vectors $x \\in \\mathbb{R}^d$, a loss function $\\ell:\\mathbb{R}^d \\times \\mathbb{R}^d \\to \\mathbb{R}$, and a data-generating random vector $\\xi \\in \\mathbb{R}^d$. The distributionally robust objective is\n$$\nF(x) \\;=\\; \\sup_{P \\in \\mathcal{W}_\\epsilon(\\widehat P)} \\;\\mathbb{E}_P\\big[\\ell(x,\\xi)\\big],\n$$\nwhere $\\widehat P$ is the empirical distribution supported on $n$ samples $\\{\\xi_i\\}_{i=1}^n$, and $\\mathcal{W}_\\epsilon(\\widehat P)$ denotes the set of probability distributions within Wasserstein-$1$ distance at most $\\epsilon$ of $\\widehat P$ under the Euclidean ground cost. The loss function is\n$$\n\\ell(x,\\xi) \\;=\\; \\tfrac{1}{2}\\,\\|x\\|_2^2 \\;+\\; \\xi^\\top x,\n$$\nwhich is convex in $x$. The feasible set is the $\\ell_\\infty$-box\n$$\n\\mathcal{X} \\;=\\; \\{x\\in\\mathbb{R}^d : \\|x\\|_\\infty \\le B\\}.\n$$\n\nTasks:\n- Derive a subgradient of $F(x)$ for any $x \\in \\mathcal{X}$, using only the following foundational bases:\n  - The definition of subgradient: for a convex function $f:\\mathbb{R}^d\\to\\mathbb{R}$, a vector $g$ is a subgradient at $x$ if $f(y)\\ge f(x)+g^\\top(y-x)$ for all $y$.\n  - The fact that the pointwise supremum over a family of convex functions is convex, and that a subgradient of the supremum at a point $x$ can be obtained from a subgradient of any active function at $x$.\n  - The Kantorovich–Rubinstein dual representation for the Wasserstein-$1$ distance with Euclidean ground cost and its implication for Lipschitz functions: if $f$ has finite Lipschitz constant $L$ with respect to the Euclidean norm, then $\\sup_{P:W_1(P,\\widehat P)\\le \\epsilon}\\mathbb{E}_P[f(\\xi)]=\\mathbb{E}_{\\widehat P}[f(\\xi)] + \\epsilon L$.\n  - The subdifferential of the Euclidean norm: $\\partial \\|x\\|_2 = \\{x/\\|x\\|_2\\}$ if $x\\neq 0$, and $\\partial \\|0\\|_2 = \\{s\\in\\mathbb{R}^d : \\|s\\|_2 \\le 1\\}$.\n- Design a projected subgradient method for minimizing $F(x)$ over $\\mathcal{X}$ with diminishing step sizes $\\alpha_t = \\alpha_0/\\sqrt{t}$, where $t$ is the iteration index and $\\alpha_00$ is an initial step size. The projection onto $\\mathcal{X}$ is the coordinate-wise clipping operator onto $[-B,B]$.\n- Implement this method for the toy setting above and report the final value of $F(x)$ after $T$ iterations for each of the test cases below. In your implementation, take a single subgradient at the current iterate $x_t$ and update $x_{t+1}$ by projecting the point $x_t - \\alpha_t g_t$ onto $\\mathcal{X}$, where $g_t$ is any valid subgradient of $F$ at $x_t$. When $x_t=0$, select any subgradient of the Euclidean norm of magnitude at most $1$ (e.g., choose the zero vector).\n\nDefinitions and data:\n- The empirical distribution is $\\widehat P = \\frac{1}{n}\\sum_{i=1}^n \\delta_{\\xi_i}$ with samples $\\xi_i \\in \\mathbb{R}^d$.\n- The dimension, sample set, ambiguity radius, and algorithmic parameters are specified per test case below.\n- The robust objective $F$ must be evaluated exactly using its derived closed-form expression from your subgradient derivation, not by Monte Carlo.\n\nTest suite (each case specifies $d$, $n$, samples $\\{\\xi_i\\}_{i=1}^n$, radius $\\epsilon$, box bound $B$, initial step size $\\alpha_0$, and iteration count $T$):\n- Case 1: $d=2$, $n=3$, samples $\\xi_1=\\begin{bmatrix}1\\\\-1\\end{bmatrix}$, $\\xi_2=\\begin{bmatrix}0\\\\2\\end{bmatrix}$, $\\xi_3=\\begin{bmatrix}-2\\\\1\\end{bmatrix}$, $\\epsilon=0.4$, $B=10$, $\\alpha_0=1.0$, $T=2000$.\n- Case 2: $d=2$, $n=2$, samples $\\xi_1=\\begin{bmatrix}2\\\\2\\end{bmatrix}$, $\\xi_2=\\begin{bmatrix}2\\\\2\\end{bmatrix}$, $\\epsilon=0.0$, $B=10$, $\\alpha_0=1.0$, $T=2000$.\n- Case 3: $d=2$, $n=2$, samples $\\xi_1=\\begin{bmatrix}1\\\\0\\end{bmatrix}$, $\\xi_2=\\begin{bmatrix}-1\\\\0\\end{bmatrix}$, $\\epsilon=1.5$, $B=10$, $\\alpha_0=1.0$, $T=2000$.\n- Case 4: $d=2$, $n=1$, samples $\\xi_1=\\begin{bmatrix}5\\\\-5\\end{bmatrix}$, $\\epsilon=1.0$, $B=0.1$, $\\alpha_0=1.0$, $T=2000$.\n\nImplementation requirements:\n- Use the exact closed-form of $F(x)$ implied by your derivation to numerically evaluate $F(x)$ at any $x$.\n- Implement the projected subgradient method with the subgradient you derived.\n- Initialize $x_1$ at the zero vector.\n- Use diminishing step sizes $\\alpha_t=\\alpha_0/\\sqrt{t}$.\n- The projection onto $\\mathcal{X}$ must be the coordinate-wise clipping operator onto $[-B,B]$.\n- For $x_t=0$, select the zero vector as a valid element of $\\partial\\|x_t\\|_2$.\n\nFinal output format:\n- Your program should produce a single line of output containing the robust objective values for the $4$ cases, each rounded to $6$ decimal places, as a comma-separated list enclosed in square brackets, in the order Case $1$, Case $2$, Case $3$, Case $4$ (e.g., `[v_1,v_2,v_3,v_4]`).", "solution": "The user-provided problem is assessed as valid. It is scientifically grounded in the field of distributionally robust optimization, well-posed for minimization, and described using objective, formal mathematical language. All necessary data and parameters for a unique solution are provided, and there are no internal contradictions.\n\nThe problem is to minimize the distributionally robust objective function $F(x)$ over a feasible set $\\mathcal{X}$, where\n$$\nF(x) = \\sup_{P \\in \\mathcal{W}_\\epsilon(\\widehat P)} \\mathbb{E}_P[\\ell(x,\\xi)]\n$$\nThe loss function is $\\ell(x,\\xi) = \\frac{1}{2}\\|x\\|_2^2 + \\xi^\\top x$, and the feasible set is the $\\ell_\\infty$-box $\\mathcal{X} = \\{x \\in \\mathbb{R}^d : \\|x\\|_\\infty \\le B\\}$.\n\nFirst, we derive a closed-form expression for the objective function $F(x)$. For a fixed decision vector $x \\in \\mathbb{R}^d$, the loss $\\ell(x, \\xi)$ is a function of the random vector $\\xi \\in \\mathbb{R}^d$. We determine the Lipschitz constant of this function with respect to The Euclidean norm on $\\xi$. The gradient of $\\ell(x, \\xi)$ with respect to $\\xi$ is:\n$$\n\\nabla_\\xi \\ell(x, \\xi) = \\nabla_\\xi \\left( \\tfrac{1}{2}\\|x\\|_2^2 + \\xi^\\top x \\right) = x\n$$\nThe Lipschitz constant of $\\ell(x,\\xi)$ with respect to $\\xi$, denoted $L_x$, is the Euclidean norm of this gradient:\n$$\nL_x = \\|\\nabla_\\xi \\ell(x, \\xi)\\|_2 = \\|x\\|_2\n$$\nThe problem provides the Kantorovich-Rubinstein dual representation's implication for a function $f$ with Lipschitz constant $L$:\n$$\n\\sup_{P:W_1(P,\\widehat P)\\le \\epsilon}\\mathbb{E}_P[f(\\xi)] = \\mathbb{E}_{\\widehat P}[f(\\xi)] + \\epsilon L\n$$\nApplying this result to our problem by setting $f(\\xi) = \\ell(x,\\xi)$ and $L = L_x = \\|x\\|_2$, we obtain the robust objective $F(x)$ a closed form:\n$$\nF(x) = \\mathbb{E}_{\\widehat P}[\\ell(x,\\xi)] + \\epsilon \\|x\\|_2\n$$\nThe expectation over the empirical distribution $\\widehat P = \\frac{1}{n}\\sum_{i=1}^n \\delta_{\\xi_i}$ is calculated as:\n$$\n\\mathbb{E}_{\\widehat P}[\\ell(x,\\xi)] = \\frac{1}{n} \\sum_{i=1}^n \\left( \\tfrac{1}{2}\\|x\\|_2^2 + \\xi_i^\\top x \\right) = \\tfrac{1}{2}\\|x\\|_2^2 + \\left(\\frac{1}{n} \\sum_{i=1}^n \\xi_i\\right)^\\top x\n$$\nLet $\\bar{\\xi} = \\frac{1}{n}\\sum_{i=1}^n \\xi_i$ be the empirical mean of the data. The objective function simplifies to:\n$$\nF(x) = \\tfrac{1}{2}\\|x\\|_2^2 + \\bar{\\xi}^\\top x + \\epsilon \\|x\\|_2\n$$\nThis expression will be used to evaluate the objective function value at the final iterate.\n\nNext, we derive a subgradient of $F(x)$. The function $F(x)$ is convex, being a sum of three convex functions: $f_1(x) = \\frac{1}{2}\\|x\\|_2^2$, $f_2(x) = \\bar{\\xi}^\\top x$, and $f_3(x) = \\epsilon \\|x\\|_2$. The subdifferential of a sum is the sum of subdifferentials.\n$$\n\\partial F(x) = \\nabla f_1(x) + \\nabla f_2(x) + \\partial f_3(x)\n$$\nThe gradients of the differentiable terms are $\\nabla f_1(x) = x$ and $\\nabla f_2(x) = \\bar{\\xi}$. The subdifferential of the non-differentiable term is $\\partial f_3(x) = \\epsilon \\cdot \\partial \\|x\\|_2$. The subdifferential of the Euclidean norm, as given, is $\\partial \\|x\\|_2 = \\{x/\\|x\\|_2\\}$ for $x \\neq 0$ and $\\partial \\|0\\|_2 = \\{s \\in \\mathbb{R}^d : \\|s\\|_2 \\le 1\\}$ for $x=0$.\nCombining these, the subdifferential of $F(x)$ is:\n$$\n\\partial F(x) = \\{ x + \\bar{\\xi} + \\epsilon s \\mid s \\in \\partial \\|x\\|_2 \\}\n$$\nFor the projected subgradient algorithm, we need to select a single subgradient $g_t$ at each iterate $x_t$. Following the problem's instructions:\n\\begin{itemize}\n    \\item If $x_t \\neq 0$, we choose the unique subgradient $s = x_t/\\|x_t\\|_2$, which yields:\n    $$\n    g_t = x_t + \\bar{\\xi} + \\epsilon \\frac{x_t}{\\|x_t\\|_2}\n    $$\n    \\item If $x_t = 0$, we are instructed to choose the zero vector as the subgradient for the norm part. This corresponds to selecting $s=0$ from the subdifferential $\\partial \\|0\\|_2$, which yields:\n    $$\n    g_t = 0 + \\bar{\\xi} + \\epsilon \\cdot 0 = \\bar{\\xi}\n    $$\n\\end{itemize}\n\nFinally, we design the projected subgradient method to solve $\\min_{x \\in \\mathcal{X}} F(x)$. The algorithm proceeds as follows:\n\\begin{enumerate}\n    \\item \\textbf{Initialization}: Set the iteration counter $t=1$ and initialize the decision vector $x_1 = 0 \\in \\mathbb{R}^d$.\n    \\item \\textbf{Iteration}: For $t = 1, 2, \\ldots, T$:\n    \\begin{enumerate}\n        \\item Compute a subgradient $g_t \\in \\partial F(x_t)$ using the rules derived above.\n        \\item Compute the step size $\\alpha_t = \\alpha_0 / \\sqrt{t}$.\n        \\item Perform the update step: $y_{t+1} = x_t - \\alpha_t g_t$.\n        \\item Project the result onto the feasible set $\\mathcal{X}$: $x_{t+1} = \\Pi_{\\mathcal{X}}(y_{t+1})$. The projection onto the $\\ell_\\infty$-box is a coordinate-wise clipping operation: $(x_{t+1})_j = \\max(-B, \\min(B, (y_{t+1})_j))$ for each component $j$.\n    \\end{enumerate}\n    \\item \\textbf{Termination}: After $T$ iterations, the algorithm terminates. The final result is the value of the objective function $F(x_{T+1})$ evaluated using its closed form.\n\\end{enumerate}\nThis procedure is implemented to find the solution for the given test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_projected_subgradient_method(d, samples, epsilon, B, alpha0, T):\n    \"\"\"\n    Implements the projected subgradient method for the given problem.\n\n    Args:\n        d (int): Dimension of the space.\n        samples (np.ndarray): Data samples, shape (n, d).\n        epsilon (float): Radius of the Wasserstein ambiguity set.\n        B (float): Bound for the l-infinity box constraint.\n        alpha0 (float): Initial step size.\n        T (int): Number of iterations.\n\n    Returns:\n        float: The final objective value F(x) after T iterations.\n    \"\"\"\n    # Pre-computation of the empirical mean\n    xi_bar = np.mean(samples, axis=0)\n\n    # Initialization\n    x = np.zeros(d)\n    \n    # Projected subgradient method iterations\n    for t in range(1, T + 1):\n        # 1. Compute subgradient g_t at x_t\n        norm_x = np.linalg.norm(x)\n        \n        # Following the problem's rule for computing the subgradient\n        if norm_x  1e-12: # Use a small tolerance for numerical stability\n            # For x = 0, the subgradient of the norm is taken as the zero vector.\n            # g = x + xi_bar + epsilon * 0 = xi_bar\n            g = xi_bar\n        else:\n            # For x != 0, g = x + xi_bar + epsilon * (x / ||x||_2)\n            g = x + xi_bar + epsilon * x / norm_x\n\n        # 2. Compute step size alpha_t\n        alpha = alpha0 / np.sqrt(t)\n\n        # 3. Update step\n        x_unprojected = x - alpha * g\n\n        # 4. Projection step\n        x = np.clip(x_unprojected, -B, B)\n\n    # After T iterations, calculate the final objective value F(x)\n    # F(x) = 0.5 * ||x||_2^2 + xi_bar^T * x + epsilon * ||x||_2\n    final_norm_x = np.linalg.norm(x)\n    final_objective_value = 0.5 * final_norm_x**2 + np.dot(xi_bar, x) + epsilon * final_norm_x\n    \n    return final_objective_value\n\ndef solve():\n    \"\"\"\n    Defines test cases and runs the solver for each case.\n    \"\"\"\n    test_cases = [\n        # Case 1: d, n, samples, epsilon, B, alpha0, T\n        {'d': 2, 'n': 3, 'samples': np.array([[1.0, -1.0], [0.0, 2.0], [-2.0, 1.0]]), 'epsilon': 0.4, 'B': 10.0, 'alpha0': 1.0, 'T': 2000},\n        # Case 2\n        {'d': 2, 'n': 2, 'samples': np.array([[2.0, 2.0], [2.0, 2.0]]), 'epsilon': 0.0, 'B': 10.0, 'alpha0': 1.0, 'T': 2000},\n        # Case 3\n        {'d': 2, 'n': 2, 'samples': np.array([[1.0, 0.0], [-1.0, 0.0]]), 'epsilon': 1.5, 'B': 10.0, 'alpha0': 1.0, 'T': 2000},\n        # Case 4\n        {'d': 2, 'n': 1, 'samples': np.array([[5.0, -5.0]]), 'epsilon': 1.0, 'B': 0.1, 'alpha0': 1.0, 'T': 2000},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_projected_subgradient_method(\n            d=case['d'],\n            samples=case['samples'],\n            epsilon=case['epsilon'],\n            B=case['B'],\n            alpha0=case['alpha0'],\n            T=case['T']\n        )\n        results.append(f\"{result:.6f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3121614"}]}