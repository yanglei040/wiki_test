{"hands_on_practices": [{"introduction": "When an optimization model is in use, external conditions can change, imposing new limitations. This exercise [@problem_id:3095270] simulates such a scenario, where a new constraint is added that makes the current optimal solution infeasible. You will practice using the dual simplex method to efficiently find the new optimal solution and see how the change in objective value relates to the economic interpretation of shadow prices.", "problem": "Consider the linear program in inequality form with slack variables:\n- Maximize the objective $z = 3 x_1 + 2 x_2$.\n- Subject to the constraints\n  - $x_1 + x_2 \\leq 4$,\n  - $x_1 \\leq 2$,\n  - $x_2 \\leq 3$,\n  - $x_1 \\geq 0$, $x_2 \\geq 0$.\nIntroduce slack variables $s_1$, $s_2$, and $s_3$ to write the constraints as $x_1 + x_2 + s_1 = 4$, $x_1 + s_2 = 2$, $x_2 + s_3 = 3$, with $s_1, s_2, s_3 \\geq 0$.\n\nYou are told that the current optimal basis is known and consists of the basic variables $\\{x_1, x_2, s_3\\}$ with basic feasible solution $(x_1, x_2, s_1, s_2, s_3) = (2, 2, 0, 0, 1)$ and objective value $z^{\\star} = 10$.\n\nNow an additional linear constraint is proposed that cuts off the current optimal vertex:\n- New constraint $\\tilde{c}$: $x_1 + x_2 \\leq 3.9$,\nwith an added slack variable $s_4 \\geq 0$ so that $x_1 + x_2 + s_4 = 3.9$.\n\nStarting only from the core definitions of linear programming feasibility and optimality, and the primalâ€“dual relationship together with complementary slackness (no pre-given shortcuts), do the following:\n- Justify that the new constraint $\\tilde{c}$ renders the current basic feasible solution infeasible.\n- Design a minimal simplex pivot path from the known optimal basis to a new optimal basic feasible solution that restores feasibility and optimality after adding $\\tilde{c}$.\n- Analyze the local sensitivity of the optimal objective to decreasing the right-hand side of constraints of the form $x_1 + x_2 \\leq b$ in a neighborhood of $b = 4$, and use this to verify the change in the optimal objective value caused by adding $\\tilde{c}$.\n\nWhat is the new optimal objective value after adding the constraint $\\tilde{c}$? Round your final numerical answer to four significant figures.", "solution": "The user has provided a linear programming (LP) problem and has asked for an analysis of the effect of adding a new constraint. The tasks include justifying the infeasibility of the current optimal solution under the new constraint, finding the new optimal solution using a simplex pivot, and verifying the result using sensitivity analysis derived from the primal-dual relationship.\n\n### Step 1: Problem Validation\n\n**1.1. Extract Givens**\n- **Objective Function (Maximize):** $z = 3 x_1 + 2 x_2$\n- **Original Constraints:**\n  - $x_1 + x_2 \\leq 4$\n  - $x_1 \\leq 2$\n  - $x_2 \\leq 3$\n  - $x_1 \\geq 0$, $x_2 \\geq 0$\n- **Standard Form with Slack Variables ($s_1, s_2, s_3 \\geq 0$):**\n  - $x_1 + x_2 + s_1 = 4$\n  - $x_1 + s_2 = 2$\n  - $x_2 + s_3 = 3$\n- **Current Optimal Solution Information:**\n  - **Basic Variables:** $\\{x_1, x_2, s_3\\}$\n  - **Basic Feasible Solution (BFS):** $(x_1, x_2, s_1, s_2, s_3) = (2, 2, 0, 0, 1)$\n  - **Optimal Objective Value:** $z^{\\star} = 10$\n- **New Proposed Constraint ($\\tilde{c}$):**\n  - $x_1 + x_2 \\leq 3.9$\n  - **Standard Form with Slack Variable ($s_4 \\geq 0$):** $x_1 + x_2 + s_4 = 3.9$\n\n**1.2. Validate Using Extracted Givens**\nFirst, we verify the provided optimal solution for the original problem.\n- **Feasibility:**\n  - $x_1 + x_2 = 2 + 2 = 4 \\leq 4$ (Constraint 1 holds, $s_1=0$)\n  - $x_1 = 2 \\leq 2$ (Constraint 2 holds, $s_2=0$)\n  - $x_2 = 2 \\leq 3$ (Constraint 3 holds, $s_3=1$)\n  - $x_1=2 \\geq 0$, $x_2=2 \\geq 0$ (Non-negativity holds)\nThe solution is feasible.\n- **Objective Value:**\n  - $z = 3x_1 + 2x_2 = 3(2) + 2(2) = 6 + 4 = 10$. This matches the given $z^\\star$.\n- **Basis:**\n  - The basic variables are $\\{x_1, x_2, s_3\\}$ with values $\\{2, 2, 1\\}$, all non-negative. The non-basic variables are $\\{s_1, s_2\\}$ with values $\\{0, 0\\}$. This is a valid basic feasible solution.\n\nThe problem statement is self-contained, scientifically grounded in the theory of linear programming, and internally consistent.\n\n**1.3. Verdict**\nThe problem is valid.\n\n### Step 2: Solution Derivation\n\n**2.1. Infeasibility of the Current Solution with the New Constraint**\nThe current optimal solution is $(x_1, x_2) = (2, 2)$. The new constraint is $\\tilde{c}: x_1 + x_2 \\leq 3.9$.\nSubstituting the values of $x_1$ and $x_2$ into the new constraint:\n$$2 + 2 = 4$$\nThe value $4$ is not less than or equal to $3.9$. Thus, the current optimal solution $(x_1, x_2) = (2, 2)$ violates the new constraint and is no longer feasible for the modified problem.\n\n**2.2. Finding the New Optimum via the Dual Simplex Method**\nWe begin by constructing the final simplex tableau for the original problem. The system of equations is:\n$z - 3x_1 - 2x_2 = 0$\n$x_1 + x_2 + s_1 = 4$\n$x_1 + s_2 = 2$\n$x_2 + s_3 = 3$\n\nThe optimal basis is $B=\\{x_1, x_2, s_3\\}$ and the non-basic variables are $N=\\{s_1, s_2\\}$. We need to express the basic variables and the objective function in terms of the non-basic variables. From the second and first constraints (with $s_1=s_2=0$):\n$x_1 = 2 - s_2$\n$x_2 = 4 - x_1 - s_1 = 4 - (2 - s_2) - s_1 = 2 - s_1 + s_2$\nAnd from the third constraint:\n$s_3 = 3 - x_2 = 3 - (2 - s_1 + s_2) = 1 + s_1 - s_2$\n\nSubstitute $x_1$ and $x_2$ into the objective function:\n$z = 3(2-s_2) + 2(2-s_1+s_2) = 6 - 3s_2 + 4 - 2s_1 + 2s_2 = 10 - 2s_1 - s_2$\nThis can be rewritten as $z + 2s_1 + s_2 = 10$.\n\nThe final simplex tableau for the original problem represents this dictionary:\n$$\n\\begin{array}{c|c|ccccc|c}\n\\text{Basis}  z  x_1  x_2  s_1  s_2  s_3  \\text{RHS} \\\\\n\\hline\nz  1  0  0  2  1  0  10 \\\\\n\\hline\nx_1  0  1  0  0  1  0  2 \\\\\nx_2  0  0  1  1  -1  0  2 \\\\\ns_3  0  0  0  -1  1  1  1\n\\end{array}\n$$\nThe z-row coefficients for non-basic variables are non-negative ($2$ and $1$), so this is indeed an optimal tableau for a maximization problem.\n\nNow, we add the new constraint $x_1 + x_2 + s_4 = 3.9$. We must express this constraint in terms of the non-basic variables $s_1, s_2$:\n$$(2 - s_2) + (2 - s_1 + s_2) + s_4 = 3.9$$\n$$4 - s_1 + s_4 = 3.9$$\n$$s_4 = -0.1 + s_1$$\nThis gives the new constraint row: $-s_1 + s_4 = -0.1$. We add this to the tableau, with $s_4$ as the new basic variable for this row.\n\n$$\n\\begin{array}{c|c|cccccc|c}\n\\text{Basis}  z  x_1  x_2  s_1  s_2  s_3  s_4  \\text{RHS} \\\\\n\\hline\nz  1  0  0  2  1  0  0  10 \\\\\n\\hline\nx_1  0  1  0  0  1  0  0  2 \\\\\nx_2  0  0  1  1  -1  0  0  2 \\\\\ns_3  0  0  0  -1  1  1  0  1 \\\\\ns_4  0  0  0  -1  0  0  1  -0.1\n\\end{array}\n$$\nThe z-row is still optimal (all coefficients $\\ge 0$). However, the solution is not primal feasible because $s_4 = -0.1$, which violates the non-negativity constraint. This is the correct setup for the dual simplex method.\n\n**Dual Simplex Pivot:**\n1.  **Select Pivot Row:** Choose the row with the most negative value in the RHS column. This is the $s_4$ row. The variable $s_4$ will leave the basis.\n2.  **Select Pivot Column:** Find the minimum ratio $|\\frac{z_j-c_j}{a_{ij}}|$ for all $j$ where the pivot row entry $a_{ij}$ is negative. The pivot row ($i=s_4$) has only one negative entry: $a_{s_4, s_1} = -1$.\n    The ratio is $|\\frac{2}{-1}|=2$. Thus, the variable $s_1$ will enter the basis. The pivot element is $-1$.\n3.  **Perform Pivot:**\n    - Divide the pivot row (the $s_4$ row) by the pivot element ($-1$) to make the pivot entry $1$. The new row becomes the $s_1$ row in the next tableau:\n    Row $s_1' = (0, 0, 0, 1, 0, 0, -1 | 0.1)$.\n    - Use this new row to make all other entries in the pivot column ($s_1$ column) zero.\n        - New z-row: Row $z' = \\text{Row } z - 2 \\cdot \\text{Row } s_1' = (1, 0, 0, 0, 1, 0, 2 | 9.8)$.\n        - New $x_1$-row: The entry is already $0$, so no change. Row $x_1'=$ Row $x_1$.\n        - New $x_2$-row: Row $x_2' = \\text{Row } x_2 - 1 \\cdot \\text{Row } s_1' = (0, 0, 1, 0, -1, 0, 1 | 1.9)$.\n        - New $s_3$-row: Row $s_3' = \\text{Row } s_3 - (-1) \\cdot \\text{Row } s_1' = \\text{Row } s_3 + \\text{Row } s_1' = (0, 0, 0, 0, 1, 1, -1 | 1.1)$.\n\nThe new tableau is:\n$$\n\\begin{array}{c|c|cccccc|c}\n\\text{Basis}  z  x_1  x_2  s_1  s_2  s_3  s_4  \\text{RHS} \\\\\n\\hline\nz  1  0  0  0  1  0  2  9.8 \\\\\n\\hline\nx_1  0  1  0  0  1  0  0  2 \\\\\nx_2  0  0  1  0  -1  0  1  1.9 \\\\\ns_3  0  0  0  0  1  1  -1  1.1 \\\\\ns_1  0  0  0  1  0  0  -1  0.1\n\\end{array}\n$$\nThe RHS column is now all non-negative, so the solution is primal feasible. The z-row coefficients are all non-negative, so it remains dual feasible (optimal).\nThe new optimal solution is: a BFS with basic variables $\\{x_1, x_2, s_3, s_1\\}$ and non-basic variables $\\{s_2, s_4\\}$.\n- $x_1 = 2$\n- $x_2 = 1.9$\n- $s_3 = 1.1$\n- $s_1 = 0.1$\n- $s_2=0$, $s_4=0$\nThe new optimal objective value is $z^{**} = 9.8$.\n\n**2.3. Verification using Sensitivity Analysis and Primal-Dual Theory**\nThe original primal LP (P) and its dual (D) are:\n(P) Maximize $z = 3x_1 + 2x_2$ s.t.\n$x_1 + x_2 \\leq 4$\n$x_1 \\leq 2$\n$x_2 \\leq 3$\n$x_1, x_2 \\geq 0$\n\n(D) Minimize $w = 4y_1 + 2y_2 + 3y_3$ s.t.\n$y_1 + y_2 \\geq 3$\n$y_1 + y_3 \\geq 2$\n$y_1, y_2, y_3 \\geq 0$\n\nThe original optimal primal solution is $(x_1, x_2) = (2, 2)$ with $z^\\star=10$.\nBy complementary slackness:\n- $x_1=2  0 \\implies y_1 + y_2 = 3$.\n- $x_2=2  0 \\implies y_1 + y_3 = 2$.\n- The constraint $x_2 \\leq 3$ is not binding ($2  3$, so slack $s_3=1  0$) $\\implies y_3 = 0$.\n- The constraints $x_1+x_2 \\leq 4$ and $x_1 \\leq 2$ are binding (slacks $s_1=0, s_2=0$), so their dual variables $y_1, y_2$ can be non-zero.\n\nSolving for the dual variables:\n$y_3=0 \\implies y_1+0=2 \\implies y_1=2$.\nThen $2+y_2=3 \\implies y_2=1$.\nThe optimal dual solution is $(y_1, y_2, y_3) = (2, 1, 0)$. This solution is dual feasible.\nThe dual variable $y_1=2$ is the shadow price for the first constraint, $x_1 + x_2 \\leq 4$. It represents the rate of change of the optimal objective value with respect to a change in the right-hand side (RHS) of this constraint, $\\frac{\\partial z^\\star}{\\partial b_1} = y_1=2$.\n\nThe introduction of the new constraint $x_1 + x_2 \\leq 3.9$ makes the original constraint $x_1 + x_2 \\leq 4$ redundant. This is equivalent to changing the RHS of the first constraint from $b_1=4$ to $b_1'=3.9$. The change is $\\Delta b_1 = 3.9 - 4 = -0.1$.\nThis change is small, so we can predict the change in the objective value as:\n$$\\Delta z^\\star \\approx y_1 \\Delta b_1 = 2 \\times (-0.1) = -0.2$$\nThe predicted new optimal value is $z^\\star_{new} = z^\\star + \\Delta z^\\star = 10 - 0.2 = 9.8$.\n\nThis prediction is exact as long as the change $\\Delta b_1$ is within the range where the optimal basis $\\{x_1, x_2, s_3\\}$ does not change. This range is determined by keeping the basic solution $x_B' = A_B^{-1}b'$ non-negative, where $b' = [4+\\Delta b_1, 2, 3]^T$. We found the dictionary expressions to be $x_1=2$, $x_2=2+\\Delta b_1$, $s_3=1-\\Delta b_1$. Feasibility requires $2 \\ge 0$, $2+\\Delta b_1 \\ge 0 \\implies \\Delta b_1 \\ge -2$, and $1-\\Delta b_1 \\ge 0 \\implies \\Delta b_1 \\le 1$. The change $\\Delta b_1 = -0.1$ is within the interval $[-2, 1]$, so the basis does not change and the shadow price calculation is exact.\n\nThe new optimal value of $9.8$ calculated by the dual simplex method is confirmed by the sensitivity analysis.\n\nThe final question asks for the new optimal objective value after adding the constraint $\\tilde{c}$, rounded to four significant figures. The calculated value is exactly $9.8$. To express this with four significant figures, we write $9.800$.", "answer": "$$\\boxed{9.800}$$", "id": "3095270"}, {"introduction": "Sensitivity analysis is not just about changes to existing parameters; it's also about evaluating entirely new opportunities. This practice [@problem_id:3095291] introduces a new decision variable into an existing optimal model, representing a new product or process. You will use the concept of reduced cost to find the critical parameter threshold at which this new option becomes economically attractive enough to enter the optimal solution.", "problem": "Consider a Linear Programming (LP) problem in primal standard form: maximize $c^{\\top} x$ subject to $A x = b$, $x \\ge 0$. The current optimal basis uses the $2 \\times 2$ basis matrix $B$ and basic cost vector $c_B$ given by\n$$\nB = \\begin{pmatrix} 2  1 \\\\ 1  1 \\end{pmatrix}, \\quad c_B = \\begin{pmatrix} 4 \\\\ 1 \\end{pmatrix}.\n$$\nA new decision variable $x_{+}$ is proposed to be added to the model. Its constraint column is scaled by a nonnegative parameter $\\alpha$, namely $A_{+}(\\alpha) = \\alpha\\, a$ with\n$$\na = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix},\n$$\nand its objective coefficient is $c_{+} = 5$. Using the definition of the simplex multipliers $y^{\\top} = c_B^{\\top} B^{-1}$ and the reduced cost $r_{+}(\\alpha) = c_{+} - y^{\\top} A_{+}(\\alpha)$ for a maximization problem, determine the critical value $\\alpha^{\\star}$ at which the reduced cost $r_{+}(\\alpha)$ crosses zero, thereby making the new variable $x_{+}$ marginally attractive to enter the basis.\n\nProvide your final answer as a single real number in simplest exact form. No rounding is required.", "solution": "We begin from the primal standard form of Linear Programming (LP): maximize $c^{\\top} x$ subject to $A x = b$, $x \\ge 0$. For a given basis $B$ with basic costs $c_B$, the simplex multipliers (also called dual prices) are defined by\n$$\ny^{\\top} = c_B^{\\top} B^{-1}.\n$$\nFor any nonbasic column $A_j$, the reduced cost in a maximization problem is defined by\n$$\nr_j = c_j - y^{\\top} A_j.\n$$\nA nonbasic variable is attractive to enter the basis when $r_j  0$; the threshold at which it becomes marginally attractive is where $r_j = 0$.\n\nHere, the new variable $x_{+}$ has column $A_{+}(\\alpha) = \\alpha\\, a$ with $a = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$ and objective coefficient $c_{+} = 5$. Therefore its reduced cost as a function of $\\alpha$ is\n$$\nr_{+}(\\alpha) = c_{+} - y^{\\top} A_{+}(\\alpha) = c_{+} - \\alpha\\, y^{\\top} a.\n$$\n\nWe compute $y^{\\top}$ from the given $B$ and $c_B$. First find $B^{-1}$. With\n$$\nB = \\begin{pmatrix} 2  1 \\\\ 1  1 \\end{pmatrix},\n$$\nthe determinant is $\\det(B) = 2 \\cdot 1 - 1 \\cdot 1 = 1$, hence\n$$\nB^{-1} = \\begin{pmatrix} 1  -1 \\\\ -1  2 \\end{pmatrix}.\n$$\nThus,\n$$\ny^{\\top} = c_B^{\\top} B^{-1} = \\begin{pmatrix} 4  1 \\end{pmatrix} \\begin{pmatrix} 1  -1 \\\\ -1  2 \\end{pmatrix} = \\begin{pmatrix} 4 \\cdot 1 + 1 \\cdot (-1) \\quad,\\quad 4 \\cdot (-1) + 1 \\cdot 2 \\end{pmatrix} = \\begin{pmatrix} 3 \\quad,\\quad -2 \\end{pmatrix}.\n$$\n\nNext compute $y^{\\top} a$:\n$$\ny^{\\top} a = \\begin{pmatrix} 3  -2 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = 3 \\cdot 2 + (-2) \\cdot 1 = 6 - 2 = 4.\n$$\n\nTherefore,\n$$\nr_{+}(\\alpha) = c_{+} - \\alpha\\, (y^{\\top} a) = 5 - 4 \\alpha.\n$$\nThe critical value $\\alpha^{\\star}$ solves $r_{+}(\\alpha^{\\star}) = 0$:\n$$\n5 - 4 \\alpha^{\\star} = 0 \\quad \\Longrightarrow \\quad \\alpha^{\\star} = \\frac{5}{4}.\n$$\n\nSince $y^{\\top} a = 4  0$, for the maximization problem the new variable is attractive when $r_{+}(\\alpha)  0$, i.e., when $\\alpha  \\alpha^{\\star}$. The requested critical value where the reduced cost crosses zero is $\\alpha^{\\star} = \\frac{5}{4}$.", "answer": "$$\\boxed{\\frac{5}{4}}$$", "id": "3095291"}, {"introduction": "Often, new strategies are not entirely novel but are innovative combinations of existing activities. This problem [@problem_id:3095297] delves into the theory of adding such a 'composite' variable to a linear program. By analyzing its reduced cost, you will discover the fundamental economic principle that governs whether a blend of existing options can create new value in an optimal solution.", "problem": "Consider the linear program in inequality form\n$$\\min\\; c^\\top x \\quad \\text{subject to} \\quad A x \\le b,\\;\\; x \\ge 0,$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^m$, and $c \\in \\mathbb{R}^n$. Assume the problem is feasible, has a finite optimal value attained by some optimal basic feasible solution, and that no column of $A$ is the zero vector. Introduce slack variables $s \\in \\mathbb{R}^m_{\\ge 0}$ to write the equivalent equality form\n$$\\min\\; c^\\top x + 0^\\top s \\quad \\text{subject to} \\quad A x + s = b,\\;\\; x \\ge 0,\\; s \\ge 0.$$\nLet $a_i$ denote the $i$-th column of $A$, and let $r_i$ denote the reduced cost of $x_i$ with respect to some optimal dual solution.\n\nNow consider adding a new decision variable $x_{n+1}$ with column\n$$a_{n+1} \\;=\\; \\sum_{i=1}^n \\lambda_i\\, a_i,$$\nwhere $\\lambda_i \\ge 0$ for all $i$ and $\\sum_{i=1}^n \\lambda_i = 1$, and assign it a cost $c_{n+1} \\in \\mathbb{R}$. In words, $a_{n+1}$ is a convex combination of existing columns of $A$.\n\nWhich of the following statements are true?\n\nA. If $c_{n+1} \\ge \\sum_{i=1}^n \\lambda_i c_i$, then the current optimal basis of the original problem remains optimal after adding $x_{n+1}$. Moreover, if $c_{n+1}  \\sum_{i=1}^n \\lambda_i c_i$, then $x_{n+1}$ cannot be basic in any optimal basis of the expanded problem.\n\nB. If $c_{n+1} = \\sum_{i=1}^n \\lambda_i c_i$, then for any optimal dual solution $y^\\star$ of the original problem, the reduced cost of $x_{n+1}$ equals $\\sum_{i=1}^n \\lambda_i r_i(y^\\star)$; in particular, $x_{n+1}$ has zero reduced cost at $y^\\star$ if and only if every $r_i(y^\\star)$ with $\\lambda_i  0$ equals $0$.\n\nC. If $c_{n+1}  \\sum_{i=1}^n \\lambda_i c_i$, then for the optimal dual solution $y^\\star$ of the original problem, the reduced cost of $x_{n+1}$ is necessarily negative; hence the original optimum cannot remain optimal.\n\nD. Because $a_{n+1}$ is a convex combination of existing columns, $x_{n+1}$ can never be basic in any feasible basis of the equality-form linear program.", "solution": "The user wants me to validate and solve a linear programming problem concerning sensitivity analysis with the addition of a new variable.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n-   A linear program (LP) in inequality form: $\\min\\; c^\\top x \\quad \\text{subject to} \\quad A x \\le b,\\;\\; x \\ge 0$, where $A \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^m$, and $c \\in \\mathbb{R}^n$.\n-   The problem is feasible and has a finite optimal value attained by an optimal basic feasible solution.\n-   No column of $A$ is the zero vector.\n-   An equivalent equality form with slack variables $s \\in \\mathbb{R}^m_{\\ge 0}$: $\\min\\; c^\\top x + 0^\\top s \\quad \\text{subject to} \\quad A x + s = b,\\;\\; x \\ge 0,\\; s \\ge 0.$\n-   $a_i$ is the $i$-th column of $A$.\n-   $r_i$ is the reduced cost of $x_i$ with respect to some optimal dual solution.\n-   A new decision variable $x_{n+1}$ is added with a cost $c_{n+1} \\in \\mathbb{R}$.\n-   The column for the new variable is $a_{n+1} = \\sum_{i=1}^n \\lambda_i a_i$, where $\\lambda_i \\ge 0$ for all $i=1, \\dots, n$ and $\\sum_{i=1}^n \\lambda_i = 1$.\n\n**Step 2: Validate Using Extracted Givens**\n\n1.  **Scientifically Grounded:** The problem is set within the standard, well-established framework of linear programming and sensitivity analysis. All concepts, such as reduced costs, duality, and basic feasible solutions, are fundamental to optimization theory.\n2.  **Well-Posed:** The question asks to evaluate the truthfulness of several statements based on the provided setup. The setup is mathematically precise, allowing for a rigorous, unique deduction for each statement.\n3.  **Objective:** The problem is stated using formal mathematical language and is free from any subjective or ambiguous terminology.\n4.  **Completeness and Consistency:** The problem provides all necessary information to analyze the impact of adding the new variable. The assumption that an optimal basic feasible solution exists ensures that strong duality holds and optimal dual solutions exist. There are no contradictions in the givens.\n5.  **Other Flaws:** The problem is not trivial, unrealistic, or un-verifiable. It is a standard theoretical question in the field of optimization methods.\n\n**Step 3: Verdict and Action**\n\nThe problem statement is **valid**. I will proceed to derive the solution.\n\n### Derivation of Solution\n\nThe problem is stated in inequality form but converted to equality form for the analysis of bases. The dual of the equality-form problem\n$$ \\min\\; \\begin{pmatrix} c \\\\ 0 \\end{pmatrix}^\\top \\begin{pmatrix} x \\\\ s \\end{pmatrix} \\quad \\text{subject to} \\quad \\begin{pmatrix} A  I \\end{pmatrix} \\begin{pmatrix} x \\\\ s \\end{pmatrix} = b, \\;\\; \\begin{pmatrix} x \\\\ s \\end{pmatrix} \\ge 0 $$\nis given by\n$$ \\max\\; b^\\top y \\quad \\text{subject to} \\quad A^\\top y \\le c, \\;\\; y \\le 0. $$\nHere, the dual variables $y \\in \\mathbb{R}^m$ are associated with the $m$ equality constraints. An optimal basic feasible solution for the primal is optimal if and only if the reduced costs of all variables are non-negative. The reduced cost of a variable with cost coefficient $c_j$ and constraint column $a_j$ with respect to a dual solution $y$ is defined as $r_j(y) = c_j - y^\\top a_j$.\n\nLet $y^\\star$ be an optimal solution to the dual problem. The optimality conditions for the original problem ensure that the reduced costs for all original variables $x_i$ and slack variables $s_k$ are non-negative:\n1.  For $x_i$ ($i=1, \\dots, n$): $r_i(y^\\star) = c_i - (y^\\star)^\\top a_i \\ge 0$.\n2.  For $s_k$ ($k=1, \\dots, m$): $r_{s_k}(y^\\star) = 0 - (y^\\star)^\\top e_k = -y^\\star_k \\ge 0$, which is consistent with the dual constraint $y^\\star \\le 0$.\n\nNow, we add the new variable $x_{n+1}$ with cost $c_{n+1}$ and column $a_{n+1} = \\sum_{i=1}^n \\lambda_i a_i$. The current optimal basis (and its corresponding optimal dual solution $y^\\star$) remains optimal for the expanded problem if and only if the reduced cost of the new variable $x_{n+1}$ is also non-negative, i.e., $r_{n+1}(y^\\star) \\ge 0$.\n\nLet's compute the reduced cost of $x_{n+1}$ with respect to $y^\\star$:\n$$ r_{n+1}(y^\\star) = c_{n+1} - (y^\\star)^\\top a_{n+1} $$\nSubstitute $a_{n+1} = \\sum_{i=1}^n \\lambda_i a_i$:\n$$ r_{n+1}(y^\\star) = c_{n+1} - (y^\\star)^\\top \\left( \\sum_{i=1}^n \\lambda_i a_i \\right) $$\nBy linearity of the dot product:\n$$ r_{n+1}(y^\\star) = c_{n+1} - \\sum_{i=1}^n \\lambda_i (y^\\star)^\\top a_i $$\nFrom the definition of the reduced cost for $x_i$, we have $(y^\\star)^\\top a_i = c_i - r_i(y^\\star)$. Substituting this into the expression for $r_{n+1}(y^\\star)$:\n$$ r_{n+1}(y^\\star) = c_{n+1} - \\sum_{i=1}^n \\lambda_i (c_i - r_i(y^\\star)) $$\n$$ r_{n+1}(y^\\star) = \\left( c_{n+1} - \\sum_{i=1}^n \\lambda_i c_i \\right) + \\sum_{i=1}^n \\lambda_i r_i(y^\\star) $$\nThis central equation allows us to evaluate the given options.\n\n### Option-by-Option Analysis\n\n**A. If $c_{n+1} \\ge \\sum_{i=1}^n \\lambda_i c_i$, then the current optimal basis of the original problem remains optimal after adding $x_{n+1}$. Moreover, if $c_{n+1}  \\sum_{i=1}^n \\lambda_i c_i$, then $x_{n+1}$ cannot be basic in any optimal basis of the expanded problem.**\n\n*   **Part 1:** Assume $c_{n+1} \\ge \\sum_{i=1}^n \\lambda_i c_i$. This means the term $\\left( c_{n+1} - \\sum_{i=1}^n \\lambda_i c_i \\right) \\ge 0$. Since $y^\\star$ is an optimal dual solution, all reduced costs $r_i(y^\\star) \\ge 0$. We are also given $\\lambda_i \\ge 0$. Therefore, the sum $\\sum_{i=1}^n \\lambda_i r_i(y^\\star) \\ge 0$.\n    The reduced cost of the new variable is $r_{n+1}(y^\\star) = (\\text{non-negative}) + (\\text{non-negative})$, which implies $r_{n+1}(y^\\star) \\ge 0$. Since the reduced costs for all original variables were already non-negative, and the new one is as well, the optimality conditions are met. The current optimal solution remains optimal. The first part of the statement is correct.\n\n*   **Part 2:** Assume $c_{n+1}  \\sum_{i=1}^n \\lambda_i c_i$. The term $\\left( c_{n+1} - \\sum_{i=1}^n \\lambda_i c_i \\right)$ is strictly positive. As before, $\\sum_{i=1}^n \\lambda_i r_i(y^\\star) \\ge 0$. Thus, $r_{n+1}(y^\\star) = (\\text{strictly positive}) + (\\text{non-negative})  0$.\n    The set of optimal dual solutions for the expanded problem is a subset of the optimal dual solutions for the original problem. For any optimal dual solution $y^{opt}$ of the expanded problem, it must satisfy $A^\\top y^{opt} \\le c$ and $y^{opt} \\le 0$, hence $r_i(y^{opt}) \\ge 0$ for all $i=1, \\dots, n$.\n    Thus, for any optimal dual solution $y^{opt}$ of the expanded problem, its reduced cost for $x_{n+1}$ is $r_{n+1}(y^{opt}) = (c_{n+1} - \\sum \\lambda_i c_i) + \\sum \\lambda_i r_i(y^{opt})  0$.\n    For a variable to be included in an optimal basis, its reduced cost with respect to the dual solution associated with that basis must be zero. Since the reduced cost of $x_{n+1}$ is strictly positive for *all* optimal dual solutions, it can never be zero. Therefore, $x_{n+1}$ cannot be a basic variable in any optimal basis of the expanded problem. The second part of the statement is correct.\n\n**Verdict:** Statement A is **Correct**.\n\n**B. If $c_{n+1} = \\sum_{i=1}^n \\lambda_i c_i$, then for any optimal dual solution $y^\\star$ of the original problem, the reduced cost of $x_{n+1}$ equals $\\sum_{i=1}^n \\lambda_i r_i(y^\\star)$; in particular, $x_{n+1}$ has zero reduced cost at $y^\\star$ if and only if every $r_i(y^\\star)$ with $\\lambda_i  0$ equals $0$.**\n\n*   **Part 1:** Assume $c_{n+1} = \\sum_{i=1}^n \\lambda_i c_i$. The term $\\left( c_{n+1} - \\sum_{i=1}^n \\lambda_i c_i \\right) = 0$. Substituting this into our central equation gives:\n    $$ r_{n+1}(y^\\star) = 0 + \\sum_{i=1}^n \\lambda_i r_i(y^\\star) = \\sum_{i=1}^n \\lambda_i r_i(y^\\star) $$\n    This holds for any dual solution $y^\\star$, optimal or not. So the first part is correct.\n\n*   **Part 2:** We have $r_{n+1}(y^\\star) = \\sum_{i=1}^n \\lambda_i r_i(y^\\star)$.\n    We know $\\lambda_i \\ge 0$ and $r_i(y^\\star) \\ge 0$ for an optimal $y^\\star$. The sum is a sum of non-negative terms. Such a sum is zero if and only if every term in the sum is zero. That is, $\\lambda_i r_i(y^\\star) = 0$ for all $i=1, \\dots, n$.\n    This condition is met if, for each $i$, either $\\lambda_i = 0$ or $r_i(y^\\star) = 0$. This is equivalent to stating that for every index $i$ where $\\lambda_i  0$, the corresponding reduced cost $r_i(y^\\star)$ must be $0$. The second part of the statement is correct.\n\n**Verdict:** Statement B is **Correct**.\n\n**C. If $c_{n+1}  \\sum_{i=1}^n \\lambda_i c_i$, then for the optimal dual solution $y^\\star$ of the original problem, the reduced cost of $x_{n+1}$ is necessarily negative; hence the original optimum cannot remain optimal.**\n\n*   Assume $c_{n+1}  \\sum_{i=1}^n \\lambda_i c_i$. The term $\\left( c_{n+1} - \\sum_{i=1}^n \\lambda_i c_i \\right)$ is strictly negative.\n    The reduced cost is $r_{n+1}(y^\\star) = (\\text{strictly negative}) + \\sum_{i=1}^n \\lambda_i r_i(y^\\star)$.\n    The second term $\\sum_{i=1}^n \\lambda_i r_i(y^\\star)$ is non-negative. The sum of a negative number and a non-negative number is not necessarily negative.\n    For a counterexample, consider the LP $\\min\\; 10x_1$ subject to $x_1 \\le 1, x_1 \\ge 0$. The optimal solution is $x_1=0$ with optimal value $0$. The optimal dual solution for the equality form is $y^\\star=0$. The reduced cost of $x_1$ is $r_1(y^\\star) = 10 - 0 \\cdot 1 = 10$.\n    Now add a new variable $x_2$ with $a_2 = a_1$ (so $\\lambda_1=1$) and cost $c_2 = 5$.\n    This satisfies the condition $c_2  \\sum\\lambda_i c_i$, as $5  10$.\n    The reduced cost of $x_2$ is $r_2(y^\\star) = (c_2 - c_1) + \\lambda_1 r_1(y^\\star) = (5 - 10) + 1 \\cdot 10 = -5 + 10 = 5$.\n    Since $r_2(y^\\star)=5  0$, it is not negative. In this case, the original optimum remains optimal. The statement \"necessarily negative\" is false.\n\n**Verdict:** Statement C is **Incorrect**.\n\n**D. Because $a_{n+1}$ is a convex combination of existing columns, $x_{n+1}$ can never be basic in any feasible basis of the equality-form linear program.**\n\n*   This statement confuses the linear dependence of a set of vectors with the inability of one of those vectors to be part of a basis.\n    A basis for the equality-form LP is a set of $m$ linearly independent columns from the full constraint matrix $[A, a_{n+1}, I]$.\n    The relation $a_{n+1} = \\sum_{i=1}^n \\lambda_i a_i$ implies that the set of columns $\\{a_{n+1}\\} \\cup \\{ a_i \\mid \\lambda_i  0 \\}$ is linearly dependent. This means these specific columns cannot all be in a basis *simultaneously*. It does not prevent $a_{n+1}$ from being part of a basis with other linearly independent columns.\n    For a counterexample, let $m=2, n=2$, and $A = \\begin{pmatrix} 1  0 \\\\ 0  2 \\end{pmatrix}$. Let $\\lambda_1=0.5, \\lambda_2=0.5$. Then $a_1=\\begin{pmatrix}1\\\\0\\end{pmatrix}$, $a_2=\\begin{pmatrix}0\\\\2\\end{pmatrix}$, and $a_{n+1}=a_3=0.5a_1+0.5a_2 = \\begin{pmatrix}0.5\\\\1\\end{pmatrix}$.\n    Consider the set of columns $\\{a_1, a_3\\}$. The matrix formed by these columns is $\\begin{pmatrix} 1  0.5 \\\\ 0  1 \\end{pmatrix}$. Its determinant is $1 \\ne 0$. Thus, $\\{a_1, a_3\\}$ is a set of two linearly independent vectors in $\\mathbb{R}^2$ and can form a basis. So $x_3$ (our $x_{n+1}$) can be a basic variable. We can even choose a vector $b$ to make this basis feasible. For example, if $b=\\begin{pmatrix}1.5 \\\\ 2\\end{pmatrix}$, the basic solution is $x_B = \\begin{pmatrix}x_1\\\\x_3\\end{pmatrix} = \\begin{pmatrix} 1  0.5 \\\\ 0  1 \\end{pmatrix}^{-1}\\begin{pmatrix}1.5 \\\\ 2\\end{pmatrix} = \\begin{pmatrix} 1  -0.5 \\\\ 0  1 \\end{pmatrix}\\begin{pmatrix}1.5 \\\\ 2\\end{pmatrix} = \\begin{pmatrix}0.5 \\\\ 2\\end{pmatrix} \\ge 0$.\n    The premise of the statement is false.\n\n**Verdict:** Statement D is **Incorrect**.\n\n### Conclusion\n\nStatements A and B are true, while statements C and D are false.", "answer": "$$\\boxed{AB}$$", "id": "3095297"}]}