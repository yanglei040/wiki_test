{"hands_on_practices": [{"introduction": "To begin, we will tackle a foundational problem that walks through the complete analytical workflow for a nonlinear program. This practice ([@problem_id:3165953]) requires you to find candidate optimal points using the first-order Karush-Kuhn-Tucker (KKT) conditions. You will then verify the Linear Independence Constraint Qualification (LICQ) and apply the Second-Order Sufficient Conditions (SOSC) to rigorously classify the stationary point, solidifying your understanding of how these core theoretical pillars work in concert.", "problem": "Consider the equality-constrained nonlinear program with decision vector $x = (x_1, x_2) \\in \\mathbb{R}^2$:\n$$\\min_{x \\in \\mathbb{R}^2} \\; f(x) \\quad \\text{subject to} \\quad h(x) = 0,$$\nwhere $f(x) = x_1^2 + x_2^2 + x_1 x_2$ and $h(x) = x_1^2 - x_2$. Starting from the definitions of the Karush–Kuhn–Tucker (KKT) conditions for equality-constrained nonlinear programs, the Linear Independence Constraint Qualification (LICQ), and the Second-Order Sufficient Conditions (SOSC), perform the following:\n- Derive the first-order conditions from the Lagrangian and solve for all KKT stationary points.\n- Verify whether LICQ holds at each KKT stationary point.\n- Classify each KKT stationary point using SOSC as a local minimizer, local maximizer, or saddle point relative to the feasible manifold.\nFinally, report the exact minimum objective value attained by the nonlinear program. The final answer must be a single real number. No rounding is required.", "solution": "The problem is an equality-constrained nonlinear program (NLP) of the form:\n$$ \\min_{x \\in \\mathbb{R}^2} \\; f(x) \\quad \\text{subject to} \\quad h(x) = 0 $$\nwhere the decision vector is $x = (x_1, x_2)$, the objective function is $f(x) = x_1^2 + x_2^2 + x_1 x_2$, and the equality constraint is $h(x) = x_1^2 - x_2 = 0$.\n\nThe first step is to formulate the Lagrangian function $L(x, \\lambda)$, which is defined as $L(x, \\lambda) = f(x) + \\lambda h(x)$ for a Lagrange multiplier $\\lambda \\in \\mathbb{R}$.\n$$ L(x_1, x_2, \\lambda) = x_1^2 + x_2^2 + x_1 x_2 + \\lambda(x_1^2 - x_2) $$\n\nThe first-order Karush–Kuhn–Tucker (KKT) necessary conditions for optimality require that the gradient of the Lagrangian with respect to $x$ is zero, and that the original constraint is satisfied.\nThe gradient of the Lagrangian is $\\nabla_x L(x, \\lambda) = (\\frac{\\partial L}{\\partial x_1}, \\frac{\\partial L}{\\partial x_2})$. The KKT conditions are:\n1. $\\frac{\\partial L}{\\partial x_1} = 2x_1 + x_2 + 2\\lambda x_1 = 0$\n2. $\\frac{\\partial L}{\\partial x_2} = 2x_2 + x_1 - \\lambda = 0$\n3. $h(x_1, x_2) = x_1^2 - x_2 = 0$\n\nWe solve this system of three equations for the variables $x_1$, $x_2$, and $\\lambda$.\nFrom condition (3), we have $x_2 = x_1^2$.\nSubstitute this into condition (2) to express $\\lambda$ in terms of $x_1$:\n$$ 2(x_1^2) + x_1 - \\lambda = 0 \\implies \\lambda = x_1 + 2x_1^2 $$\nNow, substitute the expressions for $x_2$ and $\\lambda$ into condition (1):\n$$ 2x_1 + (x_1^2) + 2(x_1 + 2x_1^2)x_1 = 0 $$\n$$ 2x_1 + x_1^2 + 2x_1^2 + 4x_1^3 = 0 $$\n$$ 4x_1^3 + 3x_1^2 + 2x_1 = 0 $$\nFactor out $x_1$:\n$$ x_1(4x_1^2 + 3x_1 + 2) = 0 $$\nThis equation has solutions if $x_1=0$ or if $4x_1^2 + 3x_1 + 2 = 0$. The discriminant of the quadratic factor is $\\Delta = 3^2 - 4(4)(2) = 9 - 32 = -23$. Since $\\Delta  0$, the quadratic equation has no real roots. Therefore, the only real solution is $x_1 = 0$.\n\nWith $x_1 = 0$, we find the corresponding values for $x_2$ and $\\lambda$:\n- $x_2 = x_1^2 = 0^2 = 0$.\n- $\\lambda = x_1 + 2x_1^2 = 0 + 2(0)^2 = 0$.\n\nThus, there is a single KKT stationary point, $x^* = (0, 0)$, with the associated Lagrange multiplier $\\lambda^* = 0$.\n\nNext, we verify if the Linear Independence Constraint Qualification (LICQ) holds at $x^*$. LICQ requires that the gradients of the active constraints at the point are linearly independent. Here, we have one equality constraint $h(x) = 0$. LICQ holds if $\\nabla h(x^*) \\neq 0$.\nThe gradient of the constraint function is:\n$$ \\nabla h(x) = \\begin{pmatrix} \\frac{\\partial h}{\\partial x_1} \\\\ \\frac{\\partial h}{\\partial x_2} \\end{pmatrix} = \\begin{pmatrix} 2x_1 \\\\ -1 \\end{pmatrix} $$\nAt the KKT point $x^* = (0, 0)$:\n$$ \\nabla h(0, 0) = \\begin{pmatrix} 2(0) \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix} $$\nSince $\\nabla h(0, 0) \\neq \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$, the LICQ is satisfied at $x^*$.\n\nNow, we use the Second-Order Sufficient Conditions (SOSC) to classify this stationary point. A KKT point $(x^*, \\lambda^*)$ that satisfies LICQ is a strict local minimizer if for every non-zero vector $d$ in the critical cone $C(x^*)$, we have $d^T \\nabla_{xx}^2 L(x^*, \\lambda^*) d  0$. For equality constraints, the critical cone is the tangent space to the feasible manifold, defined as $C(x^*) = \\{ d \\in \\mathbb{R}^2 \\mid \\nabla h(x^*)^T d = 0 \\}$.\n\nFirst, we compute the Hessian of the Lagrangian, $\\nabla_{xx}^2 L(x, \\lambda)$:\n$$ \\nabla_{xx}^2 L(x, \\lambda) = \\begin{pmatrix} \\frac{\\partial^2 L}{\\partial x_1^2}  \\frac{\\partial^2 L}{\\partial x_1 \\partial x_2} \\\\ \\frac{\\partial^2 L}{\\partial x_2 \\partial x_1}  \\frac{\\partial^2 L}{\\partial x_2^2} \\end{pmatrix} = \\begin{pmatrix} 2 + 2\\lambda  1 \\\\ 1  2 \\end{pmatrix} $$\nEvaluate the Hessian at the KKT point $(x^*, \\lambda^*) = ((0, 0), 0)$:\n$$ \\nabla_{xx}^2 L(x^*, \\lambda^*) = \\begin{pmatrix} 2 + 2(0)  1 \\\\ 1  2 \\end{pmatrix} = \\begin{pmatrix} 2  1 \\\\ 1  2 \\end{pmatrix} $$\n\nNext, we determine the critical cone (tangent space) at $x^* = (0, 0)$. A vector $d = (d_1, d_2)$ is in the critical cone if $\\nabla h(x^*)^T d = 0$.\n$$ \\nabla h(0, 0)^T d = \\begin{pmatrix} 0  -1 \\end{pmatrix} \\begin{pmatrix} d_1 \\\\ d_2 \\end{pmatrix} = 0 \\cdot d_1 - 1 \\cdot d_2 = -d_2 $$\nThe condition is $-d_2 = 0$, so $d_2 = 0$. The critical cone is the set of vectors $d$ of the form $(d_1, 0)$ for any $d_1 \\in \\mathbb{R}$.\n\nFinally, we check the sign of the quadratic form $d^T \\nabla_{xx}^2 L(x^*, \\lambda^*) d$ for any non-zero vector $d = (d_1, 0)$ with $d_1 \\neq 0$:\n$$ d^T \\nabla_{xx}^2 L(x^*, \\lambda^*) d = \\begin{pmatrix} d_1  0 \\end{pmatrix} \\begin{pmatrix} 2  1 \\\\ 1  2 \\end{pmatrix} \\begin{pmatrix} d_1 \\\\ 0 \\end{pmatrix} $$\n$$ = \\begin{pmatrix} d_1  0 \\end{pmatrix} \\begin{pmatrix} 2d_1 \\\\ d_1 \\end{pmatrix} = 2d_1^2 $$\nSince $d$ is a non-zero vector in the critical cone, $d_1 \\neq 0$, which implies $d_1^2  0$. Therefore, $2d_1^2  0$.\n\nThe quadratic form is positive definite on the critical cone. The SOSC are satisfied for a strict local minimizer. Thus, the point $x^* = (0, 0)$ is a strict local minimizer. As this is the only KKT point found, it is the global minimizer for this problem.\n\nThe minimum objective value is attained at $x^* = (0, 0)$:\n$$ f(0, 0) = 0^2 + 0^2 + (0)(0) = 0 $$\nThe minimum value of the objective function is $0$.", "answer": "$$\\boxed{0}$$", "id": "3165953"}, {"introduction": "Building upon the standard procedure, our next exercise explores what happens when a key assumption—the Linear Independence Constraint Qualification (LICQ)—is not met. This problem ([@problem_id:3166015]) presents a scenario with a redundant equality constraint, leading to linearly dependent gradients. By analyzing this case, you will discover why LICQ is crucial for guaranteeing unique Lagrange multipliers and how its failure complicates the interpretation of the KKT conditions.", "problem": "Consider the nonlinear program with only equality constraints: minimize the objective function $f(\\mathbf{x}) = x_{1}^{2} + x_{2}^{2}$ subject to the constraints $h_{1}(\\mathbf{x}) = x_{1} + x_{2} - 1 = 0$ and $h_{2}(\\mathbf{x}) = 2 x_{1} + 2 x_{2} - 2 = 0$. The constraint $h_{2}$ is redundant because it is a scalar multiple of $h_{1}$. Work from first principles to analyze the following properties and derive quantitative conclusions:\n- Use core definitions to determine the unique primal optimizer $\\mathbf{x}^{\\star}$ of the problem.\n- Evaluate the gradients of the constraints at $\\mathbf{x}^{\\star}$ and explain why the Linear Independence Constraint Qualification (LICQ) fails when both $h_{1}$ and $h_{2}$ are imposed, and why LICQ is restored when only $h_{1}$ is imposed.\n- Starting from the Karush-Kuhn-Tucker (KKT) conditions for equality-constrained nonlinear programs, derive the stationarity and feasibility conditions for the case with both constraints and for the case with only $h_{1}$.\n- For the case with both constraints, characterize the set of Lagrange multiplier pairs $(\\lambda_{1}, \\lambda_{2})$ that satisfy the KKT stationarity condition at the primal optimizer. Among this family, determine the unique pair $(\\lambda_{1}^{\\star}, \\lambda_{2}^{\\star})$ of least Euclidean norm.\n- For the case with only $h_{1}$, determine the unique Lagrange multiplier $\\lambda^{\\star}$ that satisfies the KKT conditions at the primal optimizer.\n- Verify explicitly that the KKT conditions hold at $\\mathbf{x}^{\\star}$ for both the redundant and reduced constraint sets.\n\nReport your final answer as a single row matrix containing the least-norm multipliers $(\\lambda_{1}^{\\star}, \\lambda_{2}^{\\star})$ for the redundant-constraint case, followed by the unique multiplier $\\lambda^{\\star}$ for the reduced-constraint case. Provide exact values; no rounding is required. No units are needed.", "solution": "The analysis proceeds by addressing each part of the problem statement in order.\n\n**Primal Optimizer $\\mathbf{x}^{\\star}$**\n\nThe problem is to minimize $f(\\mathbf{x}) = x_{1}^{2} + x_{2}^{2}$ subject to the constraints. The feasible set is defined by $h_{1}(\\mathbf{x}) = x_{1} + x_{2} - 1 = 0$ and $h_{2}(\\mathbf{x}) = 2x_{1} + 2x_{2} - 2 = 0$. Since $h_2(\\mathbf{x}) = 2h_1(\\mathbf{x})$, the feasible region is simply the line described by $x_{1} + x_{2} = 1$.\n\nThe objective function $f(\\mathbf{x})$ represents the squared Euclidean distance of the point $\\mathbf{x}$ from the origin $(0, 0)$. Minimizing $f(\\mathbf{x})$ is equivalent to finding the point on the line $x_{1} + x_{2} = 1$ that is closest to the origin. This point is the orthogonal projection of the origin onto the line.\n\nWe can solve this by substitution. From the constraint, we have $x_{2} = 1 - x_{1}$. Substituting this into the objective function gives an unconstrained problem in one variable:\n$$g(x_{1}) = f(x_{1}, 1-x_{1}) = x_{1}^{2} + (1 - x_{1})^{2} = x_{1}^{2} + 1 - 2x_{1} + x_{1}^{2} = 2x_{1}^{2} - 2x_{1} + 1$$\nTo find the minimum, we take the derivative with respect to $x_{1}$ and set it to zero:\n$$\\frac{dg}{dx_{1}} = 4x_{1} - 2 = 0$$\nThis yields $x_{1} = \\frac{1}{2}$. The second derivative is $\\frac{d^{2}g}{dx_{1}^{2}} = 4  0$, which confirms that this is a local minimum. Since $g(x_1)$ is a convex quadratic, this is the global minimum.\nThe corresponding value for $x_{2}$ is $x_{2} = 1 - x_{1} = 1 - \\frac{1}{2} = \\frac{1}{2}$.\nThus, the unique primal optimizer is $\\mathbf{x}^{\\star} = \\begin{pmatrix} \\frac{1}{2}  \\frac{1}{2} \\end{pmatrix}^{T}$.\n\n**Analysis of Linear Independence Constraint Qualification (LICQ)**\n\nLICQ holds at a feasible point $\\mathbf{x}$ if the gradients of the active equality constraints at that point form a linearly independent set. At any feasible point, both equality constraints $h_1$ and $h_2$ are active. The gradients of the constraints are:\n$$\\nabla h_{1}(\\mathbf{x}) = \\begin{pmatrix} \\frac{\\partial h_{1}}{\\partial x_{1}} \\\\ \\frac{\\partial h_{1}}{\\partial x_{2}} \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$$\n$$\\nabla h_{2}(\\mathbf{x}) = \\begin{pmatrix} \\frac{\\partial h_{2}}{\\partial x_{1}} \\\\ \\frac{\\partial h_{2}}{\\partial x_{2}} \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}$$\nThese gradients are constant for all $\\mathbf{x}$.\n\n- **Case with both $h_{1}$ and $h_{2}$:** The set of constraint gradients at $\\mathbf{x}^{\\star}$ (or any other point) is $\\{\\nabla h_{1}(\\mathbf{x}^{\\star}), \\nabla h_{2}(\\mathbf{x}^{\\star})\\} = \\{\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}, \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}\\}$. This set is linearly dependent because $\\nabla h_{2}(\\mathbf{x}^{\\star}) = 2 \\nabla h_{1}(\\mathbf{x}^{\\star})$. Therefore, LICQ fails when both constraints are imposed.\n\n- **Case with only $h_{1}$:** The set of constraint gradients consists of a single vector, $\\{\\nabla h_{1}(\\mathbf{x}^{\\star})\\} = \\{\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\\}$. Since this vector is non-zero, the set is linearly independent by definition. Therefore, LICQ holds (is restored) when the redundant constraint $h_2$ is removed.\n\n**Karush-Kuhn-Tucker (KKT) Conditions**\n\nThe KKT conditions for an equality-constrained problem $\\min f(\\mathbf{x})$ subject to $h_i(\\mathbf{x})=0$ are primal feasibility and stationarity.\nStationarity requires $\\nabla f(\\mathbf{x}) + \\sum_{i} \\lambda_{i} \\nabla h_{i}(\\mathbf{x}) = \\mathbf{0}$. We first compute the gradient of the objective function:\n$$\\nabla f(\\mathbf{x}) = \\begin{pmatrix} 2x_{1} \\\\ 2x_{2} \\end{pmatrix}$$\n\n- **KKT conditions for both $h_1$ and $h_2$:**\nThe Lagrangian is $\\mathcal{L}(\\mathbf{x}, \\lambda_{1}, \\lambda_{2}) = f(\\mathbf{x}) + \\lambda_{1}h_{1}(\\mathbf{x}) + \\lambda_{2}h_{2}(\\mathbf{x})$.\n1.  **Stationarity:** $\\nabla f(\\mathbf{x}) + \\lambda_{1} \\nabla h_{1}(\\mathbf{x}) + \\lambda_{2} \\nabla h_{2}(\\mathbf{x}) = \\mathbf{0}$.\n    $$\\begin{pmatrix} 2x_{1} \\\\ 2x_{2} \\end{pmatrix} + \\lambda_{1} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} + \\lambda_{2} \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$$\n    This gives the system:\n    $2x_{1} + \\lambda_{1} + 2\\lambda_{2} = 0$\n    $2x_{2} + \\lambda_{1} + 2\\lambda_{2} = 0$\n2.  **Feasibility:**\n    $x_{1} + x_{2} - 1 = 0$\n    $2x_{1} + 2x_{2} - 2 = 0$\n\n- **KKT conditions for only $h_1$:**\nThe Lagrangian is $\\mathcal{L}(\\mathbf{x}, \\lambda) = f(\\mathbf{x}) + \\lambda h_{1}(\\mathbf{x})$.\n1.  **Stationarity:** $\\nabla f(\\mathbf{x}) + \\lambda \\nabla h_{1}(\\mathbf{x}) = \\mathbf{0}$.\n    $$\\begin{pmatrix} 2x_{1} \\\\ 2x_{2} \\end{pmatrix} + \\lambda \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$$\n    This gives the system:\n    $2x_{1} + \\lambda = 0$\n    $2x_{2} + \\lambda = 0$\n2.  **Feasibility:**\n    $x_{1} + x_{2} - 1 = 0$\n\n**Lagrange Multipliers for the Redundant Case**\n\nTo find the set of Lagrange multipliers $(\\lambda_{1}, \\lambda_{2})$ at the optimizer $\\mathbf{x}^{\\star} = (\\frac{1}{2}, \\frac{1}{2})$, we substitute these values into the stationarity conditions for the two-constraint case:\n$$2\\left(\\frac{1}{2}\\right) + \\lambda_{1} + 2\\lambda_{2} = 0 \\implies 1 + \\lambda_{1} + 2\\lambda_{2} = 0$$\n$$2\\left(\\frac{1}{2}\\right) + \\lambda_{1} + 2\\lambda_{2} = 0 \\implies 1 + \\lambda_{1} + 2\\lambda_{2} = 0$$\nBoth equations are identical: $\\lambda_{1} + 2\\lambda_{2} = -1$. This demonstrates that the multipliers are not unique, which is a consequence of LICQ failure. The set of valid multipliers is a line in the $(\\lambda_1, \\lambda_2)$-plane.\n\nTo find the unique pair $(\\lambda_{1}^{\\star}, \\lambda_{2}^{\\star})$ of least Euclidean norm, we minimize $\\|\\boldsymbol{\\lambda}\\|^{2} = \\lambda_{1}^{2} + \\lambda_{2}^{2}$ subject to $\\lambda_{1} + 2\\lambda_{2} = -1$.\nWe use substitution: $\\lambda_{1} = -1 - 2\\lambda_{2}$.\nThe norm-squared function becomes $N(\\lambda_{2}) = (-1 - 2\\lambda_{2})^{2} + \\lambda_{2}^{2} = 1 + 4\\lambda_{2} + 4\\lambda_{2}^{2} + \\lambda_{2}^{2} = 5\\lambda_{2}^{2} + 4\\lambda_{2} + 1$.\nTo minimize $N$, we set its derivative to zero:\n$$\\frac{dN}{d\\lambda_{2}} = 10\\lambda_{2} + 4 = 0 \\implies \\lambda_{2} = -\\frac{4}{10} = -\\frac{2}{5}$$\nThe corresponding $\\lambda_1$ is:\n$$\\lambda_{1} = -1 - 2\\left(-\\frac{2}{5}\\right) = -1 + \\frac{4}{5} = -\\frac{1}{5}$$\nSo, the least-norm multiplier pair is $(\\lambda_{1}^{\\star}, \\lambda_{2}^{\\star}) = (-\\frac{1}{5}, -\\frac{2}{5})$.\n\n**Lagrange Multiplier for the Reduced Case**\n\nTo find the unique multiplier $\\lambda^{\\star}$ for the reduced case (with only $h_1$), we use its stationarity conditions at $\\mathbf{x}^{\\star} = (\\frac{1}{2}, \\frac{1}{2})$:\n$$2\\left(\\frac{1}{2}\\right) + \\lambda = 0 \\implies 1 + \\lambda = 0 \\implies \\lambda = -1$$\nThe second equation gives the same result. Since LICQ holds, the multiplier is unique.\nThus, $\\lambda^{\\star} = -1$.\n\n**Verification of KKT Conditions**\n\n- **Redundant Case with $(\\lambda_{1}^{\\star}, \\lambda_{2}^{\\star}) = (-\\frac{1}{5}, -\\frac{2}{5})$ at $\\mathbf{x}^{\\star} = (\\frac{1}{2}, \\frac{1}{2})$:**\n    - Stationarity: $\\nabla f(\\mathbf{x}^{\\star}) + \\lambda_{1}^{\\star} \\nabla h_{1}(\\mathbf{x}^{\\star}) + \\lambda_{2}^{\\star} \\nabla h_{2}(\\mathbf{x}^{\\star}) = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} - \\frac{1}{5} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} - \\frac{2}{5} \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 1 - \\frac{1}{5} - \\frac{4}{5} \\\\ 1 - \\frac{1}{5} - \\frac{4}{5} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$. This holds.\n    - Feasibility: $h_{1}(\\mathbf{x}^{\\star}) = \\frac{1}{2} + \\frac{1}{2} - 1 = 0$. $h_{2}(\\mathbf{x}^{\\star}) = 2(\\frac{1}{2}) + 2(\\frac{1}{2}) - 2 = 0$. This holds.\n    The KKT conditions are satisfied.\n\n- **Reduced Case with $\\lambda^{\\star} = -1$ at $\\mathbf{x}^{\\star} = (\\frac{1}{2}, \\frac{1}{2})$:**\n    - Stationarity: $\\nabla f(\\mathbf{x}^{\\star}) + \\lambda^{\\star} \\nabla h_{1}(\\mathbf{x}^{\\star}) = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} + (-1) \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$. This holds.\n    - Feasibility: $h_{1}(\\mathbf{x}^{\\star}) = \\frac{1}{2} + \\frac{1}{2} - 1 = 0$. This holds.\n    The KKT conditions are satisfied.\n\nThe final answer is composed of the determined multipliers $(\\lambda_{1}^{\\star}, \\lambda_{2}^{\\star}, \\lambda^{\\star})$.\n$\\lambda_{1}^{\\star} = -\\frac{1}{5}$\n$\\lambda_{2}^{\\star} = -\\frac{2}{5}$\n$\\lambda^{\\star} = -1$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n-\\frac{1}{5}  -\\frac{2}{5}  -1\n\\end{pmatrix}\n}\n$$", "id": "3166015"}, {"introduction": "Our final practice delves into another subtle but critical property: strict complementarity. This exercise ([@problem_id:3165959]) is designed to show the practical consequences that arise when an active inequality constraint has a corresponding KKT multiplier $\\lambda^{\\star}$ equal to zero. You will investigate how this failure of strict complementarity is directly linked to the sensitivity of the optimal solution $x^{\\star}(\\mu)$ with respect to perturbations in the problem data, a key concept in advanced optimization and stability analysis.", "problem": "Consider the parametric nonlinear program (NLP): minimize the scalar function $f(x) = x^{2}$ subject to the single inequality constraint $g(x,\\mu) := -x + \\mu \\leq 0$, where $\\mu \\in \\mathbb{R}$ is a parameter. The terminology of Karush–Kuhn–Tucker (KKT) conditions is with respect to inequality constraints $g(x,\\mu) \\leq 0$ and multipliers $\\lambda \\geq 0$, and strict complementarity means $\\lambda_{i}^{\\star}  0$ for every inequality constraint that is active (i.e., equals zero) at a solution.\n\nTask:\n- At $\\mu = 0$, identify the optimal solution $x^{\\star}(0)$ and a KKT multiplier $\\lambda^{\\star}(0)$ that satisfy stationarity, primal feasibility, dual feasibility, and complementary slackness. Verify that strict complementarity fails because the active constraint has a zero multiplier.\n- Investigate implications for sensitivity and multiplier uniqueness at $\\mu = 0$: justify whether the KKT multiplier is unique and analyze the local behavior of the optimal solution mapping $\\mu \\mapsto x^{\\star}(\\mu)$ near $\\mu = 0$.\n- Compute the right-hand directional derivative of the optimal solution mapping at $\\mu = 0$ in the direction $d = 1$, defined by\n$$\nD^{+}x^{\\star}(0;1) := \\lim_{t \\downarrow 0} \\frac{x^{\\star}(0 + t \\cdot 1) - x^{\\star}(0)}{t}.\n$$\nExpress your final result as a single real number. No rounding is required.", "solution": "The problem is a parametric nonlinear program (NLP) defined as:\n$$\n\\text{minimize} \\quad f(x) = x^2\n$$\n$$\n\\text{subject to} \\quad g(x, \\mu) := -x + \\mu \\leq 0\n$$\nwhere $x \\in \\mathbb{R}$ is the decision variable and $\\mu \\in \\mathbb{R}$ is a parameter.\n\nThe Karush-Kuhn-Tucker (KKT) conditions are a set of first-order necessary conditions for a solution in nonlinear programming to be optimal. The Lagrangian for this problem is given by:\n$$\nL(x, \\lambda; \\mu) = f(x) + \\lambda g(x, \\mu) = x^2 + \\lambda(-x + \\mu)\n$$\nwhere $\\lambda$ is the KKT multiplier associated with the inequality constraint. The KKT conditions are:\n1.  **Stationarity**: $\\nabla_x L(x, \\lambda; \\mu) = 2x - \\lambda = 0$\n2.  **Primal Feasibility**: $-x + \\mu \\leq 0$\n3.  **Dual Feasibility**: $\\lambda \\geq 0$\n4.  **Complementary Slackness**: $\\lambda(-x + \\mu) = 0$\n\nFirst, we address the task for $\\mu = 0$. The problem becomes:\n$$\n\\text{minimize} \\quad f(x) = x^2\n$$\n$$\n\\text{subject to} \\quad -x \\leq 0 \\quad (\\text{or equivalently, } x \\geq 0)\n$$\nThe objective function $f(x) = x^2$ is a parabola with its minimum at $x=0$. The feasible region is the set of all non-negative real numbers, $x \\in [0, \\infty)$. Since the unconstrained minimizer $x=0$ lies within the feasible region, it is the optimal solution. Thus, the optimal solution at $\\mu=0$ is $x^{\\star}(0) = 0$.\n\nNow, we find the KKT multiplier $\\lambda^{\\star}(0)$ corresponding to $x^{\\star}(0)=0$ and $\\mu=0$. We use the KKT conditions with $\\mu=0$:\n1.  **Stationarity**: $2x - \\lambda = 0$. Substituting $x^{\\star}(0) = 0$, we get $2(0) - \\lambda = 0$, which implies $\\lambda = 0$. So, the KKT multiplier is $\\lambda^{\\star}(0) = 0$.\n2.  **Primal Feasibility**: $-x^{\\star}(0) \\leq 0 \\implies -0 \\leq 0$. This is satisfied.\n3.  **Dual Feasibility**: $\\lambda^{\\star}(0) \\geq 0 \\implies 0 \\geq 0$. This is satisfied.\n4.  **Complementary Slackness**: $\\lambda^{\\star}(0)(-x^{\\star}(0)) = 0 \\implies 0(-0) = 0$. This is satisfied.\n\nThe KKT pair is $(x^{\\star}(0), \\lambda^{\\star}(0)) = (0, 0)$.\nStrict complementarity requires that for any active constraint, the corresponding multiplier must be strictly positive. At the solution $x^{\\star}(0)=0$, the constraint $g(x,0) = -x \\leq 0$ is active, since $-x^{\\star}(0) = 0$. However, the corresponding multiplier is $\\lambda^{\\star}(0) = 0$. Since the multiplier is not strictly positive, the condition of strict complementarity fails.\n\nNext, we investigate the uniqueness of the multiplier and the local behavior of the solution mapping at $\\mu=0$. From the stationarity condition, $\\lambda = 2x$. Since the primal optimal solution $x^{\\star}(0)=0$ is unique, the multiplier is also uniquely determined as $\\lambda^{\\star}(0) = 2x^{\\star}(0) = 2(0) = 0$.\n\nTo analyze the local behavior of the optimal solution mapping $\\mu \\mapsto x^{\\star}(\\mu)$, we solve the NLP for a general parameter $\\mu$. The constraint is $x \\geq \\mu$.\n- If $\\mu \\leq 0$, the unconstrained minimizer $x=0$ is feasible since $0 \\geq \\mu$. Thus, the optimal solution is $x^{\\star}(\\mu) = 0$.\n- If $\\mu  0$, the unconstrained minimizer $x=0$ is not feasible. The objective function $f(x) = x^2$ is strictly increasing for $x0$. Therefore, for the feasible region $x \\in [\\mu, \\infty)$, the minimum value is attained at the boundary point $x=\\mu$. Thus, the optimal solution is $x^{\\star}(\\mu) = \\mu$.\n\nCombining these two cases, the optimal solution mapping is:\n$$\nx^{\\star}(\\mu) = \\begin{cases} 0  \\text{if } \\mu \\leq 0 \\\\ \\mu  \\text{if } \\mu  0 \\end{cases}\n$$\nThis function can also be written as $x^{\\star}(\\mu) = \\max\\{0, \\mu\\}$. This mapping is continuous, but it is not differentiable at $\\mu=0$. Its left-hand derivative is $0$ and its right-hand derivative is $1$. This lack of differentiability is characteristic of problems where strict complementarity fails.\n\nFinally, we compute the right-hand directional derivative of the optimal solution mapping at $\\mu=0$ in the direction $d=1$. The definition is:\n$$\nD^{+}x^{\\star}(0;1) := \\lim_{t \\downarrow 0} \\frac{x^{\\star}(0 + t \\cdot 1) - x^{\\star}(0)}{t}\n$$\nWe have already determined $x^{\\star}(0) = 0$. For the limit $t \\downarrow 0$, we consider $t  0$. The argument of the solution mapping is $0 + t \\cdot 1 = t$. Since $t0$, we are in the case where $\\mu  0$, so $x^{\\star}(t) = t$.\nSubstituting these into the limit definition:\n$$\nD^{+}x^{\\star}(0;1) = \\lim_{t \\downarrow 0} \\frac{x^{\\star}(t) - x^{\\star}(0)}{t} = \\lim_{t \\downarrow 0} \\frac{t - 0}{t}\n$$\n$$\nD^{+}x^{\\star}(0;1) = \\lim_{t \\downarrow 0} \\frac{t}{t} = \\lim_{t \\downarrow 0} 1 = 1\n$$\nThe right-hand directional derivative is $1$.", "answer": "$$\\boxed{1}$$", "id": "3165959"}]}